{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import sys\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas dataset by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../git_repo/Code-Generation/pickles/pandas_comments_filtered_data.pkl', 'rb') as fp:\n",
    "    pandas_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "for item in pandas_data:\n",
    "    raw_data.extend(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conala data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('../conala-baseline/conala-corpus/conala-train.snippet', 'r')\n",
    "raw_data = fp.readlines()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for line in raw_data:\n",
    "    try:\n",
    "        words = utils.tokenize_code(line, mode='canonicalize')\n",
    "        vocabulary = vocabulary.union(set(words))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10336"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = dict()\n",
    "id2char = dict()\n",
    "for i, char in enumerate(vocabulary):\n",
    "    char2id[char] = i\n",
    "    id2char[i] = char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-10, out=None):\n",
    "    \"\"\"\n",
    "    Sample from Gumbel(0, 1)\n",
    "    based on\n",
    "    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n",
    "    (MIT license)\n",
    "    \"\"\"\n",
    "    U = out.resize_(shape).uniform_() if out is not None else torch.rand(shape)\n",
    "    return - torch.log(eps - torch.log(U + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(data):\n",
    "    converted_data = []\n",
    "    for line in data:\n",
    "        try:\n",
    "            x = [char2id[char] for char in utils.tokenize_code(line)]\n",
    "            converted_data.extend(x)\n",
    "        except:\n",
    "            continue\n",
    "    return np.array(converted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = preprocess_inputs(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, data, batch_size=1):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = preprocess_inputs(self.data)\n",
    "        n = len(data) - 1\n",
    "        m = n // self.batch_size\n",
    "        data = data[:self.batch_size * m + 1]\n",
    "        inputs = data[:-1].reshape((self.batch_size, m)).T\n",
    "        targets = data[1:].reshape((self.batch_size, m)).T\n",
    "\n",
    "        pos = 0\n",
    "\n",
    "        while n - pos > 0:\n",
    "\n",
    "            l = np.random.random_integers(40, 60)\n",
    "            if pos + l >= m:\n",
    "                break\n",
    "\n",
    "            yield inputs[pos:pos+l], targets[pos: pos+l]\n",
    "            pos += l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(raw_data)\n",
    "permute = np.random.permutation(N)\n",
    "raw_data = np.array(raw_data)[permute]\n",
    "idx = int(0.7* len(raw_data))\n",
    "train_data = raw_data[0:idx]\n",
    "dev_data = raw_data[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TextDataLoader(train_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.hidden2word = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, forward):\n",
    "\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, h1 = self.lstm1(embeds)\n",
    "        lstm_out, h2 = self.lstm2(lstm_out)\n",
    "        lstm_out, h3 = self.lstm3(lstm_out)\n",
    "        h = self.hidden2word(lstm_out)\n",
    "\n",
    "        gumbel = Variable(sample_gumbel(shape=h.size(), out=h.data.new()))\n",
    "        h += gumbel\n",
    "        logits = h\n",
    "\n",
    "        if forward > 0:\n",
    "            outputs = []\n",
    "            logits = torch.transpose(logits, 0, 1)\n",
    "            h = torch.max(logits[:, -1:, :], dim = 2)[1].t()\n",
    "\n",
    "            for i in range(forward):\n",
    "                h = self.word_embeddings(h)\n",
    "                h, _ = self.lstm1(h)\n",
    "                h = self.hidden2word(h)\n",
    "                gumbel = Variable(sample_gumbel(shape=h.size(), out=h.data.new()))\n",
    "                h += gumbel\n",
    "                outputs.append(h)\n",
    "                h = torch.max(h, dim=2)[1]\n",
    "\n",
    "            logits = torch.transpose(logits, 0, 1)\n",
    "            logits = torch.cat([logits] + outputs, dim=0)\n",
    "            logits = torch.max(logits, dim=2)[1]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(64, 256, vocabsize, vocabsize)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (word_embeddings): Embedding(10336, 64)\n",
       "  (lstm1): LSTM(64, 256)\n",
       "  (lstm2): LSTM(256, 256)\n",
       "  (lstm3): LSTM(256, 256)\n",
       "  (hidden2word): Linear(in_features=256, out_features=10336, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spothara/anaconda3/envs/i_study/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: This function is deprecated. Please call randint(40, 60 + 1) instead\n",
      "/home/spothara/anaconda3/envs/i_study/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  tensor(5.7288)\n",
      "Loss is  tensor(5.7062)\n",
      "Loss is  tensor(5.6744)\n",
      "Loss is  tensor(5.6867)\n",
      "Loss is  tensor(5.8169)\n",
      "Loss is  tensor(5.6507)\n",
      "Loss is  tensor(5.6875)\n",
      "Loss is  tensor(5.5780)\n",
      "Loss is  tensor(5.9032)\n",
      "Loss is  tensor(5.7375)\n",
      "Loss is  tensor(5.7537)\n",
      "Loss is  tensor(5.5929)\n",
      "Loss is  tensor(5.7931)\n",
      "Loss is  tensor(5.7658)\n",
      "Loss is  tensor(5.8453)\n",
      "Loss is  tensor(5.9539)\n",
      "Loss is  tensor(5.8329)\n",
      "Loss is  tensor(5.7910)\n",
      "Loss is  tensor(5.7702)\n",
      "Loss is  tensor(5.7113)\n",
      "Loss is  tensor(5.8330)\n",
      "Loss is  tensor(5.6876)\n",
      "Loss is  tensor(5.7604)\n",
      "Loss is  tensor(5.7584)\n",
      "Loss is  tensor(5.8285)\n",
      "Loss is  tensor(5.8105)\n",
      "Loss is  tensor(5.8361)\n",
      "Loss is  tensor(5.6966)\n",
      "Loss is  tensor(5.7879)\n",
      "Loss is  tensor(5.8125)\n",
      "Loss is  tensor(6.0284)\n",
      "Loss is  tensor(5.8373)\n",
      "Loss is  tensor(5.7012)\n",
      "Loss is  tensor(5.6681)\n",
      "Loss is  tensor(5.6382)\n",
      "Loss is  tensor(5.7341)\n",
      "Loss is  tensor(5.6517)\n",
      "Loss is  tensor(5.6393)\n",
      "Loss is  tensor(5.5907)\n",
      "Loss is  tensor(5.6306)\n",
      "Loss is  tensor(5.8274)\n",
      "Loss is  tensor(5.8042)\n",
      "Loss is  tensor(5.8158)\n",
      "Loss is  tensor(5.5487)\n",
      "Loss is  tensor(5.7495)\n",
      "Loss is  tensor(5.7504)\n",
      "Loss is  tensor(5.7943)\n",
      "Loss is  tensor(5.8490)\n",
      "Loss is  tensor(5.9828)\n",
      "Loss is  tensor(5.8115)\n",
      "Loss is  tensor(5.7214)\n",
      "Loss is  tensor(5.8282)\n",
      "Loss is  tensor(5.7519)\n",
      "Loss is  tensor(5.7737)\n",
      "Loss is  tensor(5.6958)\n",
      "Loss is  tensor(5.8540)\n",
      "Loss is  tensor(5.7711)\n",
      "Loss is  tensor(5.7873)\n",
      "Loss is  tensor(5.8457)\n",
      "Loss is  tensor(5.6464)\n",
      "Loss is  tensor(5.7541)\n",
      "Loss is  tensor(5.8931)\n",
      "Loss is  tensor(5.9681)\n",
      "Loss is  tensor(5.8228)\n",
      "Loss is  tensor(5.7820)\n",
      "Loss is  tensor(5.7242)\n",
      "Loss is  tensor(5.6351)\n",
      "Loss is  tensor(5.6443)\n",
      "Loss is  tensor(5.7796)\n",
      "Loss is  tensor(5.6545)\n",
      "Loss is  tensor(5.6733)\n",
      "Loss is  tensor(5.6175)\n",
      "Loss is  tensor(5.8055)\n",
      "Loss is  tensor(5.7477)\n",
      "Loss is  tensor(5.8335)\n",
      "Loss is  tensor(5.6600)\n",
      "Loss is  tensor(5.5566)\n",
      "Loss is  tensor(5.7379)\n",
      "Loss is  tensor(5.7860)\n",
      "Loss is  tensor(5.9384)\n",
      "Loss is  tensor(5.8061)\n",
      "Loss is  tensor(5.8151)\n",
      "Loss is  tensor(5.7597)\n",
      "Loss is  tensor(5.7737)\n",
      "Loss is  tensor(5.7833)\n",
      "Loss is  tensor(5.8226)\n",
      "Loss is  tensor(5.7956)\n",
      "Loss is  tensor(5.8012)\n",
      "Loss is  tensor(5.8135)\n",
      "Loss is  tensor(5.8242)\n",
      "Loss is  tensor(5.7707)\n",
      "Loss is  tensor(5.7449)\n",
      "Loss is  tensor(5.8062)\n",
      "Loss is  tensor(5.8927)\n",
      "Loss is  tensor(5.9048)\n",
      "Loss is  tensor(6.0087)\n",
      "Loss is  tensor(5.7601)\n",
      "Loss is  tensor(5.7228)\n",
      "Loss is  tensor(5.7159)\n",
      "Loss is  tensor(5.6713)\n",
      "Loss is  tensor(5.7023)\n",
      "Loss is  tensor(5.6082)\n",
      "Loss is  tensor(5.5455)\n",
      "Loss is  tensor(5.6791)\n",
      "Loss is  tensor(5.7638)\n",
      "Loss is  tensor(5.7242)\n",
      "Loss is  tensor(5.7766)\n",
      "Loss is  tensor(5.7636)\n",
      "Loss is  tensor(5.4145)\n",
      "Loss is  tensor(5.8702)\n",
      "Loss is  tensor(5.7485)\n",
      "Loss is  tensor(5.8050)\n",
      "Loss is  tensor(5.9957)\n",
      "Loss is  tensor(5.8269)\n",
      "Loss is  tensor(5.7872)\n",
      "Loss is  tensor(5.6581)\n",
      "Loss is  tensor(5.7838)\n",
      "Loss is  tensor(5.7542)\n",
      "Loss is  tensor(5.7098)\n",
      "Loss is  tensor(5.7887)\n",
      "Loss is  tensor(5.8052)\n",
      "Loss is  tensor(5.7991)\n",
      "Loss is  tensor(5.8732)\n",
      "Loss is  tensor(5.8698)\n",
      "Loss is  tensor(5.6958)\n",
      "Loss is  tensor(5.8932)\n",
      "Loss is  tensor(5.9816)\n",
      "Loss is  tensor(5.7834)\n",
      "Loss is  tensor(6.0137)\n",
      "Loss is  tensor(5.7317)\n",
      "Loss is  tensor(5.6042)\n",
      "Loss is  tensor(5.6773)\n",
      "Loss is  tensor(5.6621)\n",
      "Loss is  tensor(5.6852)\n",
      "Loss is  tensor(5.6799)\n",
      "Loss is  tensor(5.6985)\n",
      "Loss is  tensor(5.6610)\n",
      "Loss is  tensor(5.7263)\n",
      "Loss is  tensor(5.7024)\n",
      "Loss is  tensor(5.7882)\n",
      "Loss is  tensor(5.5656)\n",
      "Loss is  tensor(5.8059)\n",
      "Loss is  tensor(5.6733)\n",
      "Loss is  tensor(5.8293)\n",
      "Loss is  tensor(5.9592)\n",
      "Loss is  tensor(5.8768)\n",
      "Loss is  tensor(5.8674)\n",
      "Loss is  tensor(5.7255)\n",
      "Loss is  tensor(5.8037)\n",
      "Loss is  tensor(5.7684)\n",
      "Loss is  tensor(5.6980)\n",
      "Loss is  tensor(5.7119)\n",
      "Loss is  tensor(5.8464)\n",
      "Loss is  tensor(5.8039)\n",
      "Loss is  tensor(5.8312)\n",
      "Loss is  tensor(5.7447)\n",
      "Loss is  tensor(5.7367)\n",
      "Loss is  tensor(5.7956)\n",
      "Loss is  tensor(5.9275)\n",
      "Loss is  tensor(5.8357)\n",
      "Loss is  tensor(5.9581)\n",
      "Loss is  tensor(5.6899)\n",
      "Loss is  tensor(5.7191)\n",
      "Loss is  tensor(5.5940)\n",
      "Loss is  tensor(5.7098)\n",
      "Loss is  tensor(5.7572)\n",
      "Loss is  tensor(5.6522)\n",
      "Loss is  tensor(5.6820)\n",
      "Loss is  tensor(5.5628)\n",
      "Loss is  tensor(5.7904)\n",
      "Loss is  tensor(5.7552)\n",
      "Loss is  tensor(5.8091)\n",
      "Loss is  tensor(5.6154)\n",
      "Loss is  tensor(5.7558)\n",
      "Loss is  tensor(5.6611)\n",
      "Loss is  tensor(5.8124)\n",
      "Loss is  tensor(5.9660)\n",
      "Loss is  tensor(5.8042)\n",
      "Loss is  tensor(5.8373)\n",
      "Loss is  tensor(5.6957)\n",
      "Loss is  tensor(5.8845)\n",
      "Loss is  tensor(5.6817)\n",
      "Loss is  tensor(5.7501)\n",
      "Loss is  tensor(5.7173)\n",
      "Loss is  tensor(5.8047)\n",
      "Loss is  tensor(5.8628)\n",
      "Loss is  tensor(5.8832)\n",
      "Loss is  tensor(5.8431)\n",
      "Loss is  tensor(5.7210)\n",
      "Loss is  tensor(5.7997)\n",
      "Loss is  tensor(5.9008)\n",
      "Loss is  tensor(5.7915)\n",
      "Loss is  tensor(5.8625)\n",
      "Loss is  tensor(5.7307)\n",
      "Loss is  tensor(5.6566)\n",
      "Loss is  tensor(5.6329)\n",
      "Loss is  tensor(5.6394)\n",
      "Loss is  tensor(5.7199)\n",
      "Loss is  tensor(5.6550)\n",
      "Loss is  tensor(5.6808)\n",
      "Loss is  tensor(5.6989)\n",
      "Loss is  tensor(5.7613)\n",
      "Loss is  tensor(5.8315)\n",
      "Loss is  tensor(5.6037)\n",
      "Loss is  tensor(5.7162)\n",
      "Loss is  tensor(5.6875)\n",
      "Loss is  tensor(5.8126)\n",
      "Loss is  tensor(5.9900)\n",
      "Loss is  tensor(5.8875)\n",
      "Loss is  tensor(5.8523)\n",
      "Loss is  tensor(5.7112)\n",
      "Loss is  tensor(5.8836)\n",
      "Loss is  tensor(5.7386)\n",
      "Loss is  tensor(5.9129)\n",
      "Loss is  tensor(5.7601)\n",
      "Loss is  tensor(5.7258)\n",
      "Loss is  tensor(5.8676)\n",
      "Loss is  tensor(5.8089)\n",
      "Loss is  tensor(5.8949)\n",
      "Loss is  tensor(5.7759)\n",
      "Loss is  tensor(5.7179)\n",
      "Loss is  tensor(5.8206)\n",
      "Loss is  tensor(5.8164)\n",
      "Loss is  tensor(5.8829)\n",
      "Loss is  tensor(5.9232)\n",
      "Loss is  tensor(5.6909)\n",
      "Loss is  tensor(5.7130)\n",
      "Loss is  tensor(5.6181)\n",
      "Loss is  tensor(5.6540)\n",
      "Loss is  tensor(5.6832)\n",
      "Loss is  tensor(5.6595)\n",
      "Loss is  tensor(5.5990)\n",
      "Loss is  tensor(5.6532)\n",
      "Loss is  tensor(5.9043)\n",
      "Loss is  tensor(5.6569)\n",
      "Loss is  tensor(5.8140)\n",
      "Loss is  tensor(5.6791)\n",
      "Loss is  tensor(5.6080)\n",
      "Loss is  tensor(5.7907)\n",
      "Loss is  tensor(5.7460)\n",
      "Loss is  tensor(6.0247)\n",
      "Loss is  tensor(5.8767)\n",
      "Loss is  tensor(5.8282)\n",
      "Loss is  tensor(5.6815)\n",
      "Loss is  tensor(5.7865)\n",
      "Loss is  tensor(5.7378)\n",
      "Loss is  tensor(5.7754)\n",
      "Loss is  tensor(5.8242)\n",
      "Loss is  tensor(5.8120)\n",
      "Loss is  tensor(5.9011)\n",
      "Loss is  tensor(5.7782)\n",
      "Loss is  tensor(5.8792)\n",
      "Loss is  tensor(5.7081)\n",
      "Loss is  tensor(5.8127)\n",
      "Loss is  tensor(5.8978)\n",
      "Loss is  tensor(5.8981)\n",
      "Loss is  tensor(5.9777)\n",
      "Loss is  tensor(5.7924)\n",
      "Loss is  tensor(5.5756)\n",
      "Loss is  tensor(5.6989)\n",
      "Loss is  tensor(5.6466)\n",
      "Loss is  tensor(5.7772)\n",
      "Loss is  tensor(5.5548)\n",
      "Loss is  tensor(5.5812)\n",
      "Loss is  tensor(5.5925)\n",
      "Loss is  tensor(5.8368)\n",
      "Loss is  tensor(5.6759)\n",
      "Loss is  tensor(5.8385)\n",
      "Loss is  tensor(5.7220)\n",
      "Loss is  tensor(5.5073)\n",
      "Loss is  tensor(5.8072)\n",
      "Loss is  tensor(5.7874)\n",
      "Loss is  tensor(5.9453)\n",
      "Loss is  tensor(5.8072)\n",
      "Loss is  tensor(5.8434)\n",
      "Loss is  tensor(5.7666)\n",
      "Loss is  tensor(5.7629)\n",
      "Loss is  tensor(5.6818)\n",
      "Loss is  tensor(5.7926)\n",
      "Loss is  tensor(5.8404)\n",
      "Loss is  tensor(5.7495)\n",
      "Loss is  tensor(5.8034)\n",
      "Loss is  tensor(5.8464)\n",
      "Loss is  tensor(5.8337)\n",
      "Loss is  tensor(5.6293)\n",
      "Loss is  tensor(5.8033)\n",
      "Loss is  tensor(5.8176)\n",
      "Loss is  tensor(5.9416)\n",
      "Loss is  tensor(5.8799)\n",
      "Loss is  tensor(5.7128)\n",
      "Loss is  tensor(5.7858)\n",
      "Loss is  tensor(5.6411)\n",
      "Loss is  tensor(5.7066)\n",
      "Loss is  tensor(5.7789)\n",
      "Loss is  tensor(5.6846)\n",
      "Loss is  tensor(5.5973)\n",
      "Loss is  tensor(5.6514)\n",
      "Loss is  tensor(5.6973)\n",
      "Loss is  tensor(5.7779)\n",
      "Loss is  tensor(5.7255)\n",
      "Loss is  tensor(5.7722)\n",
      "Loss is  tensor(5.5737)\n",
      "Loss is  tensor(5.7221)\n",
      "Loss is  tensor(5.7801)\n",
      "Loss is  tensor(5.8497)\n",
      "Loss is  tensor(5.9418)\n",
      "Loss is  tensor(5.8261)\n",
      "Loss is  tensor(5.7713)\n",
      "Loss is  tensor(5.8188)\n",
      "Loss is  tensor(5.6927)\n",
      "Loss is  tensor(5.7933)\n",
      "Loss is  tensor(5.8261)\n",
      "Loss is  tensor(5.6759)\n",
      "Loss is  tensor(5.7997)\n",
      "Loss is  tensor(5.7192)\n",
      "Loss is  tensor(5.9440)\n",
      "Loss is  tensor(5.7773)\n",
      "Loss is  tensor(5.6688)\n",
      "Loss is  tensor(5.8119)\n",
      "Loss is  tensor(5.8728)\n",
      "Loss is  tensor(5.9372)\n",
      "Loss is  tensor(5.9466)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = Variable(torch.LongTensor(inputs).cuda()), Variable(torch.LongTensor(targets).cuda())\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        word_scores = model(inputs, 0)\n",
    "\n",
    "        r, c, h = word_scores.shape\n",
    "        word_scores = word_scores.view(r * c, h)\n",
    "\n",
    "        targets = targets.contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(word_scores, targets)\n",
    "        print('Loss is ', loss.data.cpu()[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = TextDataLoader(dev_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (word_embeddings): Embedding(10336, 64)\n",
       "  (lstm1): LSTM(64, 256)\n",
       "  (lstm2): LSTM(256, 256)\n",
       "  (lstm3): LSTM(256, 256)\n",
       "  (hidden2word): Linear(in_features=256, out_features=10336, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spothara/anaconda3/envs/i_study/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: This function is deprecated. Please call randint(40, 60 + 1) instead\n"
     ]
    }
   ],
   "source": [
    "final_ip = []\n",
    "final_op = []\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(dev_loader):\n",
    "        inputs, targets = data\n",
    "        final_ip.append(inputs.transpose())\n",
    "        inputs, targets = Variable(torch.LongTensor(inputs).cuda()), Variable(torch.LongTensor(targets).cuda())\n",
    "\n",
    "        word_scores = model(inputs, 20)\n",
    "        outputs = word_scores.transpose(0, 1)\n",
    "        \n",
    "        final_op.append(outputs[:, -20:].cpu().data.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 57), (32, 20))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ip[0].shape, final_op[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('dummy_pandas_train_word_latest.txt', 'w')\n",
    "for i, item in enumerate(final_ip):\n",
    "    for j, line in enumerate(item):\n",
    "        chars = [id2char[x] for x in line]\n",
    "        fp.write(''.join(chars))\n",
    "        op_chars = [id2char[x] for x in final_op[i][j]]\n",
    "        fp.write(''.join(chars))\n",
    "        fp.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
