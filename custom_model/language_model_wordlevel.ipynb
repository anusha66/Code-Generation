{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import sys\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas dataset by us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/anushap/Code-Generation/data_processing/pickles/pandas_5yr_filtered_commented.pkl', 'rb') as fp:\n",
    "    pandas_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['r= requests.get(\"https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json\")\\n',\n",
       "  'json_data = r.json()\\n',\n",
       "  'json_data[\"dataset_data\"][\"data\"][0]'],\n",
       " ['# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse',\n",
       "  '# into the JSON structure that will be returned'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "for item in pandas_data:\n",
    "    raw_data.extend(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conala data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp = open('../../conala-baseline/conala-corpus/conala-train.snippet', 'r')\n",
    "#raw_data = fp.readlines()\n",
    "#fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for line in raw_data:\n",
    "    try:\n",
    "        words = utils.tokenize_code(line, mode='canonicalize')\n",
    "        pdb.set_trace()\n",
    "        vocabulary = vocabulary.union(set(words))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to_drop',\n",
       " 'clear_dest_dir',\n",
       " 'null_valls',\n",
       " 'NBClassifier',\n",
       " 'df_night',\n",
       " 'df_restaurants',\n",
       " 'rt_set',\n",
       " 'df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726',\n",
       " 'images',\n",
       " '165000',\n",
       " 'r_test',\n",
       " 'checkDataMatchBetweenVars',\n",
       " 'access_info',\n",
       " 'convereted_rate_old',\n",
       " 'complib',\n",
       " 'ticket_status',\n",
       " '#saves German stop words in a list',\n",
       " 'ddate',\n",
       " 'tmp_idx',\n",
       " 'SI6',\n",
       " 'usr_idx',\n",
       " 'twitter_archive_clean',\n",
       " 'files',\n",
       " 'df_freq_users',\n",
       " 'df_ll',\n",
       " 'check_rhum',\n",
       " 'tweets2',\n",
       " '683',\n",
       " 'rain_stats',\n",
       " 'gap',\n",
       " 'mismatch1',\n",
       " 'trump_tweets',\n",
       " 'Missoula',\n",
       " 'file_size',\n",
       " 'RDDTestScorees',\n",
       " 'ndvi_of_interest02',\n",
       " 'dfAnnualMGD',\n",
       " 'exc_data',\n",
       " 'sale_prod_sort',\n",
       " 'df_merge',\n",
       " 'column',\n",
       " 'simplefilter',\n",
       " 'until',\n",
       " 'WARNING',\n",
       " 'cisuabg7',\n",
       " 'make_corrections',\n",
       " 'you',\n",
       " 'dataSeries',\n",
       " 'nv',\n",
       " 'Print_hello_class',\n",
       " 'jupyter',\n",
       " 'json_data_format_records',\n",
       " 'regex',\n",
       " 'rhum_fine',\n",
       " 'options_frame',\n",
       " 'top_features',\n",
       " 'getLogger',\n",
       " 'df_mk',\n",
       " 'y_tmp',\n",
       " 'token_pattern',\n",
       " 'obo_parser',\n",
       " 'best_model_lr',\n",
       " 'words_only_sk_freq',\n",
       " 'gr_e1',\n",
       " 'output3',\n",
       " 'dic_inp',\n",
       " '# shortcut',\n",
       " 'capacity_gross_uba',\n",
       " 'image_df_clean',\n",
       " 'SlackLoader',\n",
       " 'count_cols',\n",
       " 'last_name',\n",
       " 'results_lm',\n",
       " 'get_datetime_from_cobburl',\n",
       " 'temp_close',\n",
       " 'X_tr',\n",
       " 'input_nodes_DF',\n",
       " '#result is dictionary. yay!',\n",
       " 'trip_data',\n",
       " 'load_headline_corpus',\n",
       " 'images_folder_name',\n",
       " 'RandomForestRegressor',\n",
       " 'html_page',\n",
       " 'unique_labels',\n",
       " 'data_numeric',\n",
       " 'cluster_pd',\n",
       " 'TripData2',\n",
       " 'GoogleMapPlotter',\n",
       " 'Precipitation',\n",
       " 'BallBerry_resistance_simulation_0_25',\n",
       " 'df_geo_insta',\n",
       " 'IBMpandas_df_subset',\n",
       " 'ylab',\n",
       " 'predict_class',\n",
       " 'orig_ct',\n",
       " 'createHydroShareResource',\n",
       " 'TRUE',\n",
       " '1000000000',\n",
       " 'bigtable',\n",
       " 'Base',\n",
       " 'old_p',\n",
       " 'min_count',\n",
       " '2461',\n",
       " 'read_gpickle',\n",
       " 'mapValues',\n",
       " '__get__',\n",
       " 'index_name',\n",
       " 'search_key_words',\n",
       " 'ibm_hr',\n",
       " 'a_df',\n",
       " 'day_later_timestamps',\n",
       " 'dateCreated',\n",
       " 'ms_df',\n",
       " 'mv_buy_hold',\n",
       " 'p4_result',\n",
       " 'df_man',\n",
       " 'validation_frame',\n",
       " 'train_ratio',\n",
       " 'to_rank_cols',\n",
       " 'ctd',\n",
       " 'subcols',\n",
       " 'mean_squared_error',\n",
       " 'words_scrape',\n",
       " '# remove null entries',\n",
       " 'store',\n",
       " 'testDataVecs',\n",
       " 'cat_flds',\n",
       " 'stats',\n",
       " 'autosklearn',\n",
       " 'BytesIO',\n",
       " 'df_transactions',\n",
       " 'cbs_df',\n",
       " 'exec',\n",
       " 'hdfs_conf',\n",
       " 'q_stations',\n",
       " 'sf',\n",
       " 'SparseMatrixSimilarity',\n",
       " 'y_pred_test',\n",
       " 'typesub2017',\n",
       " 'inner_list',\n",
       " 'read_sql_query',\n",
       " 'get_rating',\n",
       " 'df2_group',\n",
       " 'pystore',\n",
       " 'sprint_info',\n",
       " 'globalCityBytes',\n",
       " '261.8475',\n",
       " 'count_null',\n",
       " 'ftr_imp_rf',\n",
       " '10.0',\n",
       " 'df_json',\n",
       " 'thisyear',\n",
       " 'test_labels',\n",
       " 'popular',\n",
       " 'Austin',\n",
       " 'CISCIJ10_cycle2',\n",
       " 'X_new',\n",
       " 'tz_convert',\n",
       " 'new_w',\n",
       " 'data_links',\n",
       " 'twitter_df_wa',\n",
       " 'age_well_years',\n",
       " \"#This [:-1] means it does not add the 'Total' column we just created above\",\n",
       " 'RED',\n",
       " 'repeat_user',\n",
       " 'get_classifier',\n",
       " 'all_cards',\n",
       " 'lic_date',\n",
       " 'BPAIRED_GEN',\n",
       " 'table_results',\n",
       " 'get_cleaned_odds',\n",
       " 'train_file',\n",
       " 'telecom2',\n",
       " 'get_data_filter_object',\n",
       " 'game_id',\n",
       " 'emojis_db',\n",
       " 'tutorials',\n",
       " 'not_in_stops',\n",
       " '1000.0',\n",
       " 'results_distributedTopmodel',\n",
       " 'movies_df',\n",
       " 'simulFinsh',\n",
       " 'nb_pipe_2',\n",
       " 'purchases',\n",
       " '516',\n",
       " 'active_devices_list',\n",
       " 'top_five',\n",
       " 'Markdown',\n",
       " 'liquor_state_cost',\n",
       " '1469',\n",
       " 'fse',\n",
       " '## creating dummy',\n",
       " 'df_daily4',\n",
       " 'f_user',\n",
       " 'spmat',\n",
       " '# clean-up',\n",
       " 'page_interaction_result',\n",
       " 'Lon',\n",
       " '1534',\n",
       " 'gpByDate',\n",
       " 'y_tfidf',\n",
       " 'postgre_to_dataframe',\n",
       " 'XGBClassifier',\n",
       " 'awj_teacher_id',\n",
       " 'facilities',\n",
       " 'df8_lunch',\n",
       " 'wind_dir_columns',\n",
       " 'twitter_df',\n",
       " 'addicks',\n",
       " 'pop_df_3',\n",
       " 'twosample_sub',\n",
       " 'min_lat',\n",
       " 'num_to_name',\n",
       " 'QUIDS',\n",
       " 'AFX_X_data',\n",
       " '74',\n",
       " 'pipe_cv',\n",
       " 'baseball1_df',\n",
       " '#how many nulls were introduced?',\n",
       " 'num_status',\n",
       " 'net',\n",
       " 'cisnwh8_cycle1',\n",
       " 'd',\n",
       " 'wedate',\n",
       " 'bikes',\n",
       " 'val_labels',\n",
       " 'normalizedDf',\n",
       " 'preproc_titles',\n",
       " 'conversations',\n",
       " 'Riverside',\n",
       " 'ab_page',\n",
       " 'TERM2017',\n",
       " 'mf',\n",
       " 'unwrapped',\n",
       " 'tobs_df',\n",
       " 'ice_df',\n",
       " '76',\n",
       " 'df_first',\n",
       " 'hit_filter_df',\n",
       " 'ldamodel',\n",
       " 'crime_data',\n",
       " 'strptime',\n",
       " 'psycopg2',\n",
       " 'sample_split',\n",
       " 'election_data',\n",
       " 'reviews_sample',\n",
       " 'probarr2',\n",
       " 'url_votes',\n",
       " 'Quantile_95_disc_times_pay',\n",
       " 'extent',\n",
       " 'tweet_sentiment',\n",
       " 'search_booking',\n",
       " 'lastSeen',\n",
       " 'scaled_eth',\n",
       " 'userMovies',\n",
       " 'end',\n",
       " 'dly_frame',\n",
       " 'gross',\n",
       " 'word_freqs',\n",
       " 'postulaciones',\n",
       " 'df_pdb_metadata',\n",
       " \"# sets the column 'Draft_year' as the index'\",\n",
       " 'apt',\n",
       " 'dict_table',\n",
       " 'bin_vars',\n",
       " 'templates',\n",
       " 'np',\n",
       " '# SET YOUR LOGGING LEVEL HERE #',\n",
       " 'overallOutlet',\n",
       " 'most_busy',\n",
       " 'nan_rows',\n",
       " 'kind',\n",
       " 'direction',\n",
       " 'rPold',\n",
       " 'databasetype',\n",
       " 'merge_table_tc',\n",
       " 'orders',\n",
       " 'logreg',\n",
       " 'month_account_created',\n",
       " 'timestamp_left_df',\n",
       " 'mbti_text_collection_filler',\n",
       " 'merged_portfolio_sp_latest',\n",
       " 'df_cust_data',\n",
       " 'bbox_to_anchor',\n",
       " 'pm',\n",
       " '50000',\n",
       " 'myTimeZone',\n",
       " 'orders_df',\n",
       " 'my_model_q2',\n",
       " 'prior_2',\n",
       " '0.0506',\n",
       " 'get_real_types',\n",
       " 'GODag',\n",
       " 'go',\n",
       " 'test_errors',\n",
       " 'BUMatrix',\n",
       " 'prcp_df_flat',\n",
       " 'precip_data_df1',\n",
       " 'model_selection',\n",
       " 'df_teacher_behavior',\n",
       " 'replies_donald',\n",
       " 'r_client_previous',\n",
       " 'results_frame_b',\n",
       " 'cisuabd4_cycle6',\n",
       " 'cnf_matrix',\n",
       " 'summary_all',\n",
       " 'stop_words',\n",
       " 'classification_price',\n",
       " 'input_',\n",
       " '1.625',\n",
       " 'start_df',\n",
       " '#,encoding=\"cp1252\")#\"utf-16\")#',\n",
       " 'np_target',\n",
       " 'scraped_batch6_top',\n",
       " 'liquor_state_dollars',\n",
       " 'iso_join',\n",
       " 'pred_set',\n",
       " 'poparr2',\n",
       " 'reset_index',\n",
       " 'grid_lat',\n",
       " 'violations_trimmed',\n",
       " 'workspace_data',\n",
       " 'SHOPIFY_API_URL',\n",
       " 'dta_692',\n",
       " '__dict__',\n",
       " 'md_vals',\n",
       " 'plt',\n",
       " 'knn',\n",
       " 'apps_by_dist',\n",
       " 'avg_word_vec_features',\n",
       " 'todrop2',\n",
       " 'txt',\n",
       " 'labels_group',\n",
       " '1981',\n",
       " 'msft_piped',\n",
       " 'logisticRegr',\n",
       " 'df_converted',\n",
       " 'simResist_rootDistExp_1',\n",
       " 'integratedData',\n",
       " 'Muliple_columns',\n",
       " 'data_dict',\n",
       " 'no_name_list',\n",
       " 'ppt',\n",
       " 'findAll',\n",
       " 'blast',\n",
       " 'IMDB_dftouse_dict',\n",
       " '77.13',\n",
       " 'sent2tokens',\n",
       " '0.0118',\n",
       " 'gridfs',\n",
       " 'station_zipcode',\n",
       " 'as_index',\n",
       " 'total_treatment',\n",
       " '35180',\n",
       " 'final_rf_predictions',\n",
       " 'data_issues_transitions',\n",
       " 'convert_to_datetime',\n",
       " 'X_train_mean',\n",
       " 'trip_end_date',\n",
       " 'z',\n",
       " 'training_y',\n",
       " 'scores',\n",
       " 'ctrl_new',\n",
       " 'prob',\n",
       " 'StationCount',\n",
       " 'datAll',\n",
       " 'stock_daily',\n",
       " '#strip first 9 characters',\n",
       " 'pst',\n",
       " 'tuple_lst',\n",
       " 'msft',\n",
       " '# append the series to the data frame. The line 4 is appended',\n",
       " '2.75',\n",
       " 'scale_data',\n",
       " 'DEBUG',\n",
       " 'mean_age',\n",
       " 'support',\n",
       " 'get_ts',\n",
       " 'RegO',\n",
       " 'news',\n",
       " 'TMED',\n",
       " 'ymc',\n",
       " 'lastfm_model',\n",
       " 'raw_large_grid_df',\n",
       " 'left_index',\n",
       " '6000',\n",
       " 'targets',\n",
       " 'twentythree_delays_time',\n",
       " 'sl',\n",
       " 'non_na_df',\n",
       " 'n_user_days',\n",
       " 'top_apps',\n",
       " 'x_chart_ucl',\n",
       " 'fraud_country',\n",
       " 'text_list',\n",
       " 'twitter_archive_master',\n",
       " '# only want to visualise relationships between first 3 projections',\n",
       " 'st_time',\n",
       " 'Purchased',\n",
       " 'gs',\n",
       " 'newPage_df',\n",
       " 'city_pd',\n",
       " 'iso_gdf',\n",
       " 'abs_pg',\n",
       " 'route',\n",
       " 'wcPerf1_df',\n",
       " 'response_df',\n",
       " 'suburbanData_df',\n",
       " 'lq2015_combined',\n",
       " 'MILLESIME',\n",
       " '# train + items_categories join by item_id',\n",
       " 'last_unix',\n",
       " 'rural_merged_df',\n",
       " 'thirtyeight_delays_time',\n",
       " 'ArepaZone_id',\n",
       " 'jsondata',\n",
       " 'M_df',\n",
       " '# \"probs\":result_prob[:,1]})',\n",
       " 'Train',\n",
       " 'suburban',\n",
       " 'time_favorite',\n",
       " 'jd',\n",
       " 'eve_contr',\n",
       " '40.0',\n",
       " 'supersize',\n",
       " 'new_user_recommendations_rating_title_and_count_RDD',\n",
       " 'period',\n",
       " 'descending',\n",
       " 'Dense',\n",
       " ',',\n",
       " 'g_live_df',\n",
       " 'afx_17',\n",
       " 'feature_col',\n",
       " 'map_uniprot_to_pdb',\n",
       " 'Remaining_columns',\n",
       " '86400',\n",
       " 'usernodes',\n",
       " 'free_data',\n",
       " 'plot2',\n",
       " 'train_data_np',\n",
       " 'genus',\n",
       " 'top_places',\n",
       " 'factor_column',\n",
       " 'df_stock',\n",
       " 'correlations',\n",
       " 'station_availability_df',\n",
       " 'sales_update',\n",
       " 'thistweet',\n",
       " 'autoDf',\n",
       " 'rpartition',\n",
       " '0.04',\n",
       " 'get_subset_object',\n",
       " 'X_num',\n",
       " 't_cont_prob',\n",
       " 'df_fbase',\n",
       " 'stand_err',\n",
       " 'playlist',\n",
       " 'dir',\n",
       " 'results3',\n",
       " 'af',\n",
       " 'new_style_url',\n",
       " 'binarize',\n",
       " 'GamelogParser',\n",
       " 'dfQ3',\n",
       " 'createdtm',\n",
       " 'age_bins',\n",
       " 'nlargest',\n",
       " 'logit_multi_mod',\n",
       " 'Retweets',\n",
       " 'auth_key',\n",
       " 'bnb',\n",
       " 'y_sc',\n",
       " 'figsize',\n",
       " 'eval_data',\n",
       " 'split_data',\n",
       " 'registerTempTable',\n",
       " 'exp',\n",
       " 'closed',\n",
       " 'tables',\n",
       " 'iter',\n",
       " 'get_condition_df',\n",
       " 'billtargs',\n",
       " 'gdax_trans_btc',\n",
       " 'sentiments_df',\n",
       " 'block_geoids_2010',\n",
       " 'creds',\n",
       " 'tshift',\n",
       " '23',\n",
       " 'payload',\n",
       " 'original',\n",
       " 'prop_zn',\n",
       " 'brewery_bw',\n",
       " 'class_merged',\n",
       " 'hit_ratio_table',\n",
       " 'pook_url',\n",
       " 'Detroit',\n",
       " 'team_slugs_dict',\n",
       " 'etevents_grid',\n",
       " '1497',\n",
       " 'building_pa_specs',\n",
       " 'mpl',\n",
       " 'collect',\n",
       " 'avg_neighborhood_rt',\n",
       " 'bg_df',\n",
       " 'p2_table',\n",
       " 'signals',\n",
       " '23453634',\n",
       " 'GEMPRO',\n",
       " 'period_range',\n",
       " 'scen2xls',\n",
       " 'declump_emojis_in_text',\n",
       " 'sublist',\n",
       " 'classifier_container',\n",
       " 'Second_pokemon',\n",
       " 'crypto_combined',\n",
       " 'mvrs',\n",
       " 'B_INCR2',\n",
       " 'total_rows',\n",
       " 'by_time',\n",
       " 'tableId',\n",
       " 'San_Antonio',\n",
       " 'matt1',\n",
       " '.03',\n",
       " 'zip_to_down_ls',\n",
       " 'api_clean',\n",
       " 'commas',\n",
       " 'trn_lm',\n",
       " 'successful_campaigns',\n",
       " 'ci',\n",
       " 'date_first_booking',\n",
       " 'from_dict',\n",
       " 'new_x',\n",
       " 'int_and_tel',\n",
       " 'drop_first',\n",
       " 'percip_stats',\n",
       " 'close_deltas',\n",
       " \"#pd.merge(status_df, image_df_clean, on = 'tweet_id', how='inner')\",\n",
       " '# we got 88.6% r2_score with test/prediction',\n",
       " 'exploration_titanic',\n",
       " 'total_seconds',\n",
       " 'sen_dat',\n",
       " 'prob_group',\n",
       " 'FileCapture',\n",
       " 'season_id',\n",
       " 'SCN_BDAY',\n",
       " 'dummy_var',\n",
       " 'data_NL',\n",
       " 'dul_isbns',\n",
       " '0.001576',\n",
       " 'series',\n",
       " 'problem_id',\n",
       " 'oil_interpolation',\n",
       " 'test_RDD',\n",
       " 'maxCol',\n",
       " 'calls',\n",
       " 'contin_vars',\n",
       " 'endometrium_data',\n",
       " 'Largest_change_in_one_day',\n",
       " 'dfPost',\n",
       " 'cv_results',\n",
       " '.025',\n",
       " '# Only the value for new_page is relevant here as mentioned in the question above',\n",
       " 'flight',\n",
       " 'runtype',\n",
       " 'get_ticklabels',\n",
       " 'ticker',\n",
       " 'nanmax',\n",
       " 'requirements',\n",
       " 'feature_names',\n",
       " 'df_city',\n",
       " 'Predicted',\n",
       " 'model_target',\n",
       " 'service_day',\n",
       " 'ans_1',\n",
       " 'faulty_rating_id',\n",
       " 'download_quotes',\n",
       " 'airlines_sim',\n",
       " 'results_para',\n",
       " 'json_data_2017',\n",
       " 'simp_rxn_time_times',\n",
       " 'model_uid',\n",
       " 'geo_events',\n",
       " 'rootDistExp',\n",
       " 'pd_train_filtered',\n",
       " 'mergedLocationDF',\n",
       " 'X_train_1',\n",
       " 'tqdm',\n",
       " 'urban_table_df',\n",
       " 'contractor_id',\n",
       " 'dt',\n",
       " 'get_scoring_url',\n",
       " 'train_pp_path',\n",
       " '0.8',\n",
       " 'dataexpo',\n",
       " 'tuna_90',\n",
       " 'allclose',\n",
       " 'mt_df',\n",
       " 'REPLACE',\n",
       " '0.0211',\n",
       " '150000',\n",
       " '# Same as above: under the null, use both groups.',\n",
       " '# micro batch size is 3 seconds here . It will receive an RDD every 3 seconds',\n",
       " 'BinaryClassificationEvaluator',\n",
       " 'mapping',\n",
       " 'workspace_ids',\n",
       " 'pmean',\n",
       " 'rcParams',\n",
       " 'sessions_p_cuepoint',\n",
       " 'X_df',\n",
       " 'append',\n",
       " 'data_list',\n",
       " 'review',\n",
       " 'Cisnwf6_cycle3',\n",
       " 'dummies',\n",
       " 'demographics',\n",
       " 'rfc_features',\n",
       " 'search',\n",
       " 'cleaned_df',\n",
       " 'IntervalIndex',\n",
       " 'highest_temp_obs',\n",
       " 'tripAux',\n",
       " 'newaxis',\n",
       " 'num_portfolios',\n",
       " 'utility_patents_subset_df',\n",
       " 'loaded_mpg_estimation_model',\n",
       " 'df_first_days',\n",
       " '150',\n",
       " 'bymin',\n",
       " 'df_archive_csv',\n",
       " 'dict2',\n",
       " 'pOld',\n",
       " 'fin_r_monthly',\n",
       " 'grouped_dpt_city',\n",
       " 'urban_average_fare',\n",
       " 'df15stats',\n",
       " 'param',\n",
       " 'observation_data',\n",
       " 'xres3',\n",
       " 'save_model',\n",
       " 'city_dict',\n",
       " 'ArepaZone_date_array',\n",
       " 'female',\n",
       " 'HAlte',\n",
       " 'df_dates',\n",
       " 'a_tags',\n",
       " 'mismatch_index',\n",
       " 'drop_cols',\n",
       " 'max_sharpe_port_optim',\n",
       " 'model2db',\n",
       " 'chefdf',\n",
       " 'versicolor',\n",
       " 'Aussie_result',\n",
       " 'soup',\n",
       " 'Description',\n",
       " 'vectorized',\n",
       " 'assert',\n",
       " 'ans_3',\n",
       " 'axes_check',\n",
       " 'url2',\n",
       " 'euphoria',\n",
       " 'gym',\n",
       " 'tweeter_url',\n",
       " 'end_mask',\n",
       " 'bad_dates',\n",
       " 'old_page_sim',\n",
       " 'temperature_data',\n",
       " 'item_lookup',\n",
       " 'best_partition',\n",
       " 'forecast_out',\n",
       " 'feedbacks_stress',\n",
       " 'temp_us_full',\n",
       " 'dummy_country',\n",
       " 'df_pop_ceb',\n",
       " 'StdDev',\n",
       " 'turnstiles_df',\n",
       " 'city',\n",
       " '0.0674',\n",
       " 'min_vol_port_optim',\n",
       " 'wp_data',\n",
       " 'x_holdout',\n",
       " 'Dictionary',\n",
       " 'raw_data_df',\n",
       " 'my_tree_one',\n",
       " 'rtype',\n",
       " 'ignore_index',\n",
       " '170',\n",
       " 'time_fav',\n",
       " 'neu_tweets_rf',\n",
       " 'case',\n",
       " 'abandonded',\n",
       " 'year16',\n",
       " 'stoi',\n",
       " 'page',\n",
       " 'auth_endpoint',\n",
       " 'temp_8',\n",
       " 'ind_result_list',\n",
       " 'nr',\n",
       " 'concatenate',\n",
       " 'age_groups',\n",
       " 'nmf_tfidf_topic6_sample',\n",
       " 'Socrata',\n",
       " 'jobs_data2',\n",
       " '# Homeless',\n",
       " '1467',\n",
       " 'squared_errors_quadratic',\n",
       " 'results_jar_rootDistExp',\n",
       " 'x_axis',\n",
       " 'num_clusters',\n",
       " 'dfLookup',\n",
       " 'initial_cond',\n",
       " 'images_path',\n",
       " 'payments_NOT_common_all_yrs',\n",
       " 'mgxs_types',\n",
       " 'corrcoef',\n",
       " 'organization',\n",
       " 'WindSpeed',\n",
       " '2008',\n",
       " 'column_name',\n",
       " '88',\n",
       " 'year_precip',\n",
       " '18.5',\n",
       " 'README',\n",
       " 'mean_absolute_percentage_error',\n",
       " 'df_tot',\n",
       " 'val_avg_preds2',\n",
       " 'train_arrays',\n",
       " 'chmod',\n",
       " 'maxprcp',\n",
       " '81.25',\n",
       " 'company',\n",
       " 'typ',\n",
       " 'bmp_series',\n",
       " 'similarity',\n",
       " 'tweets_total',\n",
       " 'letters',\n",
       " 'recall_score',\n",
       " 'people_with_one_or_zero_collab',\n",
       " 'END',\n",
       " 'plot_LC_solar_Flare',\n",
       " 'cohorts_size',\n",
       " 'log_mod_int',\n",
       " 'weather_main',\n",
       " 'records3',\n",
       " 'df_data_1',\n",
       " 'domains',\n",
       " 'classify_natural',\n",
       " 'churn_df',\n",
       " 'year_to_date',\n",
       " 'borough_group',\n",
       " 'open_list',\n",
       " 'pdr',\n",
       " 'status',\n",
       " 'super',\n",
       " 'So',\n",
       " '# osx / Linux',\n",
       " 'train_x',\n",
       " 'row',\n",
       " 'exists',\n",
       " 'lon_li',\n",
       " 'tweets_clean',\n",
       " 'get_countries',\n",
       " 'brand_names',\n",
       " 'tweets_android',\n",
       " 'subset1',\n",
       " 'sanders',\n",
       " 'fixed',\n",
       " 'loan',\n",
       " 'The',\n",
       " 'all_tweets',\n",
       " 'img_url_valles',\n",
       " 'founddocs',\n",
       " 'span',\n",
       " 'df_master_select',\n",
       " 'to_pickle',\n",
       " '5563089830',\n",
       " 'addOne',\n",
       " 'ADNI_merge',\n",
       " 'yc_new3',\n",
       " 'mc_estimates',\n",
       " 'aibootcamp',\n",
       " 'r2_score',\n",
       " 'get_response',\n",
       " 'subject_id',\n",
       " 'MSUStats',\n",
       " 'logit_mod2',\n",
       " 'miss',\n",
       " 'input_folder',\n",
       " 'rsvp_df',\n",
       " 'user_table',\n",
       " '# 392,106,544 rows --> nealy 400 millions rows',\n",
       " 'active_list_pending_ratio_test',\n",
       " '#convert date to datetime dtype',\n",
       " 'xml_in',\n",
       " 'data_demo',\n",
       " 'TEMP',\n",
       " 'malebydatenew',\n",
       " 'movie1',\n",
       " 'SIGHTINGS',\n",
       " 'Aussie_df',\n",
       " 'plot_results',\n",
       " 'measure_val_2014_to_2017',\n",
       " 'tweets_ps',\n",
       " 'data3',\n",
       " '0.625',\n",
       " '18.01559',\n",
       " 'fa_index',\n",
       " '0.50',\n",
       " 'sumar_1',\n",
       " 'seaborn',\n",
       " 'bmtk',\n",
       " '0.95',\n",
       " 'ConsecutiveNPChunker',\n",
       " 'incident_df',\n",
       " 'idx_days',\n",
       " 'read_parquet',\n",
       " 'error_bad_lines',\n",
       " '0.135',\n",
       " 'X_testfinaltf',\n",
       " 'load_weights',\n",
       " 'cbg',\n",
       " 'df_not_cntrl_old',\n",
       " 'mask_page',\n",
       " 'error_treatment',\n",
       " 'recommendations',\n",
       " 'ser',\n",
       " 'group_to_list',\n",
       " 'reorder_customers',\n",
       " '_source',\n",
       " 'clients',\n",
       " 'current_img_url',\n",
       " '4.4',\n",
       " '0.001',\n",
       " 'figure_format',\n",
       " 'delays_geo',\n",
       " 'RN_PA_duration',\n",
       " 'o_data',\n",
       " 'date_min',\n",
       " 'ones',\n",
       " 'sqlite3',\n",
       " 'rc_2018_02',\n",
       " 'bds',\n",
       " 'filterwarnings',\n",
       " 'vol_vec',\n",
       " 'gb',\n",
       " 'wash_parkdf',\n",
       " '1800',\n",
       " '# start date for Sep 1st 2015',\n",
       " 'data_scientist',\n",
       " 'UNIT',\n",
       " 'average_for_each_calendar_month',\n",
       " 'sign',\n",
       " 'logit3',\n",
       " 'conversion_prob',\n",
       " 'VALUES',\n",
       " 'feats_dict',\n",
       " 'baby',\n",
       " 'df_enhanced',\n",
       " 'ibm_hr_target_small',\n",
       " 'postsQ',\n",
       " 'get_all_column_data_df',\n",
       " 'pulledTweets_df',\n",
       " 'score',\n",
       " 'dup_ent',\n",
       " 'df2_duplicated_row',\n",
       " 'dfGoles',\n",
       " 'digits',\n",
       " '#Printing the max value from January, 2015 to April, 2015',\n",
       " '# now we can visualize the duration and make sense of it',\n",
       " 'DATETIME',\n",
       " 'cercanasA1_11_14Entre50Y75mts',\n",
       " 'words_in_articles',\n",
       " 'test_batches',\n",
       " 'files_aapl',\n",
       " 'csvpath2',\n",
       " 'send_keys',\n",
       " 'body',\n",
       " 'ppt_date',\n",
       " 'md_keys',\n",
       " 'active_add_date',\n",
       " 'data_repo',\n",
       " 'modell',\n",
       " 'DummyDataframe',\n",
       " 'project',\n",
       " 'last_year',\n",
       " 'overall_df',\n",
       " 'output_notebook',\n",
       " 'common_words',\n",
       " '1061',\n",
       " 'extra',\n",
       " 'string',\n",
       " 'x_filtered',\n",
       " 'go_no_go_times',\n",
       " 'daily_returns',\n",
       " 'monthMask',\n",
       " 'tax_class2',\n",
       " 'dateCounts',\n",
       " 'clf_stack_rf',\n",
       " \"# it's a C array of python objects!\",\n",
       " 'solgenomics',\n",
       " 'find_elements_by_class_name',\n",
       " 'trains_fe1',\n",
       " 'sqladb',\n",
       " 'dd',\n",
       " 'pandas',\n",
       " 'picker',\n",
       " 'pairing_dict_names',\n",
       " 'get_dssp_annotations',\n",
       " 'scaler_M7',\n",
       " 'v_invoice_hub',\n",
       " 'tweets_data',\n",
       " 'todays_datetimes',\n",
       " 'savetxt',\n",
       " 'sorted_time',\n",
       " 'diagnosis_BLCHANGE',\n",
       " 'output_list',\n",
       " '860',\n",
       " 'artists_info',\n",
       " 'obamaSpeechRequest',\n",
       " 'test_cont_doc',\n",
       " 'kd915',\n",
       " 'df_ad_state_metro_1',\n",
       " 'address1',\n",
       " 'CC_records',\n",
       " 'glm',\n",
       " 'dist_df',\n",
       " 'c_',\n",
       " 'cuepoints',\n",
       " 'lumped_simulation',\n",
       " 'price_by_cluster',\n",
       " 'jup_url',\n",
       " 'dict_geo_count',\n",
       " 'pairofTransdf',\n",
       " 'grid_heatmap',\n",
       " 'filecounts',\n",
       " 'zipcodes',\n",
       " 'mongo_uri',\n",
       " 'doc_topic',\n",
       " 'lggs',\n",
       " 'transform',\n",
       " 'day_name',\n",
       " 'ridedata_df',\n",
       " 'df_search_cate_dummies',\n",
       " 'api_token',\n",
       " 'ProductPurchaseData',\n",
       " 'GeoDataFrame',\n",
       " 'add_to_list',\n",
       " 'y_axis_value',\n",
       " 'es_dicttype',\n",
       " 'loaded_traffic_flow_prediction_model',\n",
       " 'kaggle',\n",
       " 'splits',\n",
       " 'stops_heatmap',\n",
       " 'time_hour_for_file_name',\n",
       " '225',\n",
       " 'x_min',\n",
       " 'axlabel',\n",
       " 'merge_table',\n",
       " 'resample',\n",
       " 'two_digits',\n",
       " 'odm2rest_request',\n",
       " 'max_key',\n",
       " 'carto',\n",
       " 'df_green',\n",
       " 'ndvi_us',\n",
       " 'create_from_list',\n",
       " '0.64631688969744',\n",
       " 'grid_lon',\n",
       " 'analyse_sentiment',\n",
       " 'example1',\n",
       " 'grouped_months',\n",
       " 'exchange_id',\n",
       " 'input_image',\n",
       " 'np_diff',\n",
       " 'df_eng',\n",
       " 'pipe_dat',\n",
       " 'num_passengers',\n",
       " '0.4471',\n",
       " 'include',\n",
       " 'agents',\n",
       " 'lee_train_file',\n",
       " 'probs',\n",
       " 'colmns',\n",
       " 'slope',\n",
       " 'data_FCInspevnt',\n",
       " 'indata',\n",
       " 'reuters',\n",
       " 'small_movies_raw_data',\n",
       " 'words_only_scrape_freq',\n",
       " 'classify',\n",
       " 'invoice_link_dropper',\n",
       " 'createOrReplaceIndex',\n",
       " 'MODELE_CODE',\n",
       " 'df_sched',\n",
       " 'agent',\n",
       " 'crime_geo',\n",
       " 'y_data',\n",
       " 'overallQual',\n",
       " 'cycle2_MVI_0054',\n",
       " 'new_page_p',\n",
       " 'split_df',\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = dict()\n",
    "id2char = dict()\n",
    "for i, char in enumerate(vocabulary):\n",
    "    char2id[char] = i\n",
    "    id2char[i] = char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gumbel(shape, eps=1e-10, out=None):\n",
    "    \"\"\"\n",
    "    Sample from Gumbel(0, 1)\n",
    "    based on\n",
    "    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n",
    "    (MIT license)\n",
    "    \"\"\"\n",
    "    U = out.resize_(shape).uniform_() if out is not None else torch.rand(shape)\n",
    "    return - torch.log(eps - torch.log(U + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(data):\n",
    "    converted_data = []\n",
    "    for line in data:\n",
    "        try:\n",
    "            x = [char2id[char] for char in utils.tokenize_code(line)]\n",
    "            converted_data.extend(x)\n",
    "        except:\n",
    "            continue\n",
    "    return np.array(converted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = preprocess_inputs(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataLoader(DataLoader):\n",
    "\n",
    "    def __init__(self, data, batch_size=1):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = preprocess_inputs(self.data)\n",
    "        n = len(data) - 1\n",
    "        m = n // self.batch_size\n",
    "        data = data[:self.batch_size * m + 1]\n",
    "        inputs = data[:-1].reshape((self.batch_size, m)).T\n",
    "        targets = data[1:].reshape((self.batch_size, m)).T\n",
    "\n",
    "        pos = 0\n",
    "\n",
    "        while n - pos > 0:\n",
    "\n",
    "            l = np.random.random_integers(40, 60)\n",
    "            if pos + l >= m:\n",
    "                break\n",
    "\n",
    "            yield inputs[pos:pos+l], targets[pos: pos+l]\n",
    "            pos += l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(raw_data)\n",
    "permute = np.random.permutation(N)\n",
    "raw_data = np.array(raw_data)[permute]\n",
    "idx = int(0.7* len(raw_data))\n",
    "train_data = raw_data[0:idx]\n",
    "dev_data = raw_data[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = TextDataLoader(train_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.hidden2word = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, forward):\n",
    "\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, h1 = self.lstm1(embeds)\n",
    "        lstm_out, h2 = self.lstm2(lstm_out)\n",
    "        lstm_out, h3 = self.lstm3(lstm_out)\n",
    "        h = self.hidden2word(lstm_out)\n",
    "\n",
    "        gumbel = Variable(sample_gumbel(shape=h.size(), out=h.data.new()))\n",
    "        h += gumbel\n",
    "        logits = h\n",
    "\n",
    "        if forward > 0:\n",
    "            outputs = []\n",
    "            logits = torch.transpose(logits, 0, 1)\n",
    "            h = torch.max(logits[:, -1:, :], dim = 2)[1].t()\n",
    "\n",
    "            for i in range(forward):\n",
    "                h = self.word_embeddings(h)\n",
    "                h, _ = self.lstm1(h)\n",
    "                h = self.hidden2word(h)\n",
    "                gumbel = Variable(sample_gumbel(shape=h.size(), out=h.data.new()))\n",
    "                h += gumbel\n",
    "                outputs.append(h)\n",
    "                h = torch.max(h, dim=2)[1]\n",
    "\n",
    "            logits = torch.transpose(logits, 0, 1)\n",
    "            logits = torch.cat([logits] + outputs, dim=0)\n",
    "            logits = torch.max(logits, dim=2)[1]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(64, 256, vocabsize, vocabsize)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (word_embeddings): Embedding(14745, 64)\n",
       "  (lstm1): LSTM(64, 256)\n",
       "  (lstm2): LSTM(256, 256)\n",
       "  (lstm3): LSTM(256, 256)\n",
       "  (hidden2word): Linear(in_features=256, out_features=14745, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spothara/anaconda3/envs/i_study/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: This function is deprecated. Please call randint(40, 60 + 1) instead\n",
      "/home/spothara/anaconda3/envs/i_study/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is  tensor(5.7288)\n",
      "Loss is  tensor(5.7062)\n",
      "Loss is  tensor(5.6744)\n",
      "Loss is  tensor(5.6867)\n",
      "Loss is  tensor(5.8169)\n",
      "Loss is  tensor(5.6507)\n",
      "Loss is  tensor(5.6875)\n",
      "Loss is  tensor(5.5780)\n",
      "Loss is  tensor(5.9032)\n",
      "Loss is  tensor(5.7375)\n",
      "Loss is  tensor(5.7537)\n",
      "Loss is  tensor(5.5929)\n",
      "Loss is  tensor(5.7931)\n",
      "Loss is  tensor(5.7658)\n",
      "Loss is  tensor(5.8453)\n",
      "Loss is  tensor(5.9539)\n",
      "Loss is  tensor(5.8329)\n",
      "Loss is  tensor(5.7910)\n",
      "Loss is  tensor(5.7702)\n",
      "Loss is  tensor(5.7113)\n",
      "Loss is  tensor(5.8330)\n",
      "Loss is  tensor(5.6876)\n",
      "Loss is  tensor(5.7604)\n",
      "Loss is  tensor(5.7584)\n",
      "Loss is  tensor(5.8285)\n",
      "Loss is  tensor(5.8105)\n",
      "Loss is  tensor(5.8361)\n",
      "Loss is  tensor(5.6966)\n",
      "Loss is  tensor(5.7879)\n",
      "Loss is  tensor(5.8125)\n",
      "Loss is  tensor(6.0284)\n",
      "Loss is  tensor(5.8373)\n",
      "Loss is  tensor(5.7012)\n",
      "Loss is  tensor(5.6681)\n",
      "Loss is  tensor(5.6382)\n",
      "Loss is  tensor(5.7341)\n",
      "Loss is  tensor(5.6517)\n",
      "Loss is  tensor(5.6393)\n",
      "Loss is  tensor(5.5907)\n",
      "Loss is  tensor(5.6306)\n",
      "Loss is  tensor(5.8274)\n",
      "Loss is  tensor(5.8042)\n",
      "Loss is  tensor(5.8158)\n",
      "Loss is  tensor(5.5487)\n",
      "Loss is  tensor(5.7495)\n",
      "Loss is  tensor(5.7504)\n",
      "Loss is  tensor(5.7943)\n",
      "Loss is  tensor(5.8490)\n",
      "Loss is  tensor(5.9828)\n",
      "Loss is  tensor(5.8115)\n",
      "Loss is  tensor(5.7214)\n",
      "Loss is  tensor(5.8282)\n",
      "Loss is  tensor(5.7519)\n",
      "Loss is  tensor(5.7737)\n",
      "Loss is  tensor(5.6958)\n",
      "Loss is  tensor(5.8540)\n",
      "Loss is  tensor(5.7711)\n",
      "Loss is  tensor(5.7873)\n",
      "Loss is  tensor(5.8457)\n",
      "Loss is  tensor(5.6464)\n",
      "Loss is  tensor(5.7541)\n",
      "Loss is  tensor(5.8931)\n",
      "Loss is  tensor(5.9681)\n",
      "Loss is  tensor(5.8228)\n",
      "Loss is  tensor(5.7820)\n",
      "Loss is  tensor(5.7242)\n",
      "Loss is  tensor(5.6351)\n",
      "Loss is  tensor(5.6443)\n",
      "Loss is  tensor(5.7796)\n",
      "Loss is  tensor(5.6545)\n",
      "Loss is  tensor(5.6733)\n",
      "Loss is  tensor(5.6175)\n",
      "Loss is  tensor(5.8055)\n",
      "Loss is  tensor(5.7477)\n",
      "Loss is  tensor(5.8335)\n",
      "Loss is  tensor(5.6600)\n",
      "Loss is  tensor(5.5566)\n",
      "Loss is  tensor(5.7379)\n",
      "Loss is  tensor(5.7860)\n",
      "Loss is  tensor(5.9384)\n",
      "Loss is  tensor(5.8061)\n",
      "Loss is  tensor(5.8151)\n",
      "Loss is  tensor(5.7597)\n",
      "Loss is  tensor(5.7737)\n",
      "Loss is  tensor(5.7833)\n",
      "Loss is  tensor(5.8226)\n",
      "Loss is  tensor(5.7956)\n",
      "Loss is  tensor(5.8012)\n",
      "Loss is  tensor(5.8135)\n",
      "Loss is  tensor(5.8242)\n",
      "Loss is  tensor(5.7707)\n",
      "Loss is  tensor(5.7449)\n",
      "Loss is  tensor(5.8062)\n",
      "Loss is  tensor(5.8927)\n",
      "Loss is  tensor(5.9048)\n",
      "Loss is  tensor(6.0087)\n",
      "Loss is  tensor(5.7601)\n",
      "Loss is  tensor(5.7228)\n",
      "Loss is  tensor(5.7159)\n",
      "Loss is  tensor(5.6713)\n",
      "Loss is  tensor(5.7023)\n",
      "Loss is  tensor(5.6082)\n",
      "Loss is  tensor(5.5455)\n",
      "Loss is  tensor(5.6791)\n",
      "Loss is  tensor(5.7638)\n",
      "Loss is  tensor(5.7242)\n",
      "Loss is  tensor(5.7766)\n",
      "Loss is  tensor(5.7636)\n",
      "Loss is  tensor(5.4145)\n",
      "Loss is  tensor(5.8702)\n",
      "Loss is  tensor(5.7485)\n",
      "Loss is  tensor(5.8050)\n",
      "Loss is  tensor(5.9957)\n",
      "Loss is  tensor(5.8269)\n",
      "Loss is  tensor(5.7872)\n",
      "Loss is  tensor(5.6581)\n",
      "Loss is  tensor(5.7838)\n",
      "Loss is  tensor(5.7542)\n",
      "Loss is  tensor(5.7098)\n",
      "Loss is  tensor(5.7887)\n",
      "Loss is  tensor(5.8052)\n",
      "Loss is  tensor(5.7991)\n",
      "Loss is  tensor(5.8732)\n",
      "Loss is  tensor(5.8698)\n",
      "Loss is  tensor(5.6958)\n",
      "Loss is  tensor(5.8932)\n",
      "Loss is  tensor(5.9816)\n",
      "Loss is  tensor(5.7834)\n",
      "Loss is  tensor(6.0137)\n",
      "Loss is  tensor(5.7317)\n",
      "Loss is  tensor(5.6042)\n",
      "Loss is  tensor(5.6773)\n",
      "Loss is  tensor(5.6621)\n",
      "Loss is  tensor(5.6852)\n",
      "Loss is  tensor(5.6799)\n",
      "Loss is  tensor(5.6985)\n",
      "Loss is  tensor(5.6610)\n",
      "Loss is  tensor(5.7263)\n",
      "Loss is  tensor(5.7024)\n",
      "Loss is  tensor(5.7882)\n",
      "Loss is  tensor(5.5656)\n",
      "Loss is  tensor(5.8059)\n",
      "Loss is  tensor(5.6733)\n",
      "Loss is  tensor(5.8293)\n",
      "Loss is  tensor(5.9592)\n",
      "Loss is  tensor(5.8768)\n",
      "Loss is  tensor(5.8674)\n",
      "Loss is  tensor(5.7255)\n",
      "Loss is  tensor(5.8037)\n",
      "Loss is  tensor(5.7684)\n",
      "Loss is  tensor(5.6980)\n",
      "Loss is  tensor(5.7119)\n",
      "Loss is  tensor(5.8464)\n",
      "Loss is  tensor(5.8039)\n",
      "Loss is  tensor(5.8312)\n",
      "Loss is  tensor(5.7447)\n",
      "Loss is  tensor(5.7367)\n",
      "Loss is  tensor(5.7956)\n",
      "Loss is  tensor(5.9275)\n",
      "Loss is  tensor(5.8357)\n",
      "Loss is  tensor(5.9581)\n",
      "Loss is  tensor(5.6899)\n",
      "Loss is  tensor(5.7191)\n",
      "Loss is  tensor(5.5940)\n",
      "Loss is  tensor(5.7098)\n",
      "Loss is  tensor(5.7572)\n",
      "Loss is  tensor(5.6522)\n",
      "Loss is  tensor(5.6820)\n",
      "Loss is  tensor(5.5628)\n",
      "Loss is  tensor(5.7904)\n",
      "Loss is  tensor(5.7552)\n",
      "Loss is  tensor(5.8091)\n",
      "Loss is  tensor(5.6154)\n",
      "Loss is  tensor(5.7558)\n",
      "Loss is  tensor(5.6611)\n",
      "Loss is  tensor(5.8124)\n",
      "Loss is  tensor(5.9660)\n",
      "Loss is  tensor(5.8042)\n",
      "Loss is  tensor(5.8373)\n",
      "Loss is  tensor(5.6957)\n",
      "Loss is  tensor(5.8845)\n",
      "Loss is  tensor(5.6817)\n",
      "Loss is  tensor(5.7501)\n",
      "Loss is  tensor(5.7173)\n",
      "Loss is  tensor(5.8047)\n",
      "Loss is  tensor(5.8628)\n",
      "Loss is  tensor(5.8832)\n",
      "Loss is  tensor(5.8431)\n",
      "Loss is  tensor(5.7210)\n",
      "Loss is  tensor(5.7997)\n",
      "Loss is  tensor(5.9008)\n",
      "Loss is  tensor(5.7915)\n",
      "Loss is  tensor(5.8625)\n",
      "Loss is  tensor(5.7307)\n",
      "Loss is  tensor(5.6566)\n",
      "Loss is  tensor(5.6329)\n",
      "Loss is  tensor(5.6394)\n",
      "Loss is  tensor(5.7199)\n",
      "Loss is  tensor(5.6550)\n",
      "Loss is  tensor(5.6808)\n",
      "Loss is  tensor(5.6989)\n",
      "Loss is  tensor(5.7613)\n",
      "Loss is  tensor(5.8315)\n",
      "Loss is  tensor(5.6037)\n",
      "Loss is  tensor(5.7162)\n",
      "Loss is  tensor(5.6875)\n",
      "Loss is  tensor(5.8126)\n",
      "Loss is  tensor(5.9900)\n",
      "Loss is  tensor(5.8875)\n",
      "Loss is  tensor(5.8523)\n",
      "Loss is  tensor(5.7112)\n",
      "Loss is  tensor(5.8836)\n",
      "Loss is  tensor(5.7386)\n",
      "Loss is  tensor(5.9129)\n",
      "Loss is  tensor(5.7601)\n",
      "Loss is  tensor(5.7258)\n",
      "Loss is  tensor(5.8676)\n",
      "Loss is  tensor(5.8089)\n",
      "Loss is  tensor(5.8949)\n",
      "Loss is  tensor(5.7759)\n",
      "Loss is  tensor(5.7179)\n",
      "Loss is  tensor(5.8206)\n",
      "Loss is  tensor(5.8164)\n",
      "Loss is  tensor(5.8829)\n",
      "Loss is  tensor(5.9232)\n",
      "Loss is  tensor(5.6909)\n",
      "Loss is  tensor(5.7130)\n",
      "Loss is  tensor(5.6181)\n",
      "Loss is  tensor(5.6540)\n",
      "Loss is  tensor(5.6832)\n",
      "Loss is  tensor(5.6595)\n",
      "Loss is  tensor(5.5990)\n",
      "Loss is  tensor(5.6532)\n",
      "Loss is  tensor(5.9043)\n",
      "Loss is  tensor(5.6569)\n",
      "Loss is  tensor(5.8140)\n",
      "Loss is  tensor(5.6791)\n",
      "Loss is  tensor(5.6080)\n",
      "Loss is  tensor(5.7907)\n",
      "Loss is  tensor(5.7460)\n",
      "Loss is  tensor(6.0247)\n",
      "Loss is  tensor(5.8767)\n",
      "Loss is  tensor(5.8282)\n",
      "Loss is  tensor(5.6815)\n",
      "Loss is  tensor(5.7865)\n",
      "Loss is  tensor(5.7378)\n",
      "Loss is  tensor(5.7754)\n",
      "Loss is  tensor(5.8242)\n",
      "Loss is  tensor(5.8120)\n",
      "Loss is  tensor(5.9011)\n",
      "Loss is  tensor(5.7782)\n",
      "Loss is  tensor(5.8792)\n",
      "Loss is  tensor(5.7081)\n",
      "Loss is  tensor(5.8127)\n",
      "Loss is  tensor(5.8978)\n",
      "Loss is  tensor(5.8981)\n",
      "Loss is  tensor(5.9777)\n",
      "Loss is  tensor(5.7924)\n",
      "Loss is  tensor(5.5756)\n",
      "Loss is  tensor(5.6989)\n",
      "Loss is  tensor(5.6466)\n",
      "Loss is  tensor(5.7772)\n",
      "Loss is  tensor(5.5548)\n",
      "Loss is  tensor(5.5812)\n",
      "Loss is  tensor(5.5925)\n",
      "Loss is  tensor(5.8368)\n",
      "Loss is  tensor(5.6759)\n",
      "Loss is  tensor(5.8385)\n",
      "Loss is  tensor(5.7220)\n",
      "Loss is  tensor(5.5073)\n",
      "Loss is  tensor(5.8072)\n",
      "Loss is  tensor(5.7874)\n",
      "Loss is  tensor(5.9453)\n",
      "Loss is  tensor(5.8072)\n",
      "Loss is  tensor(5.8434)\n",
      "Loss is  tensor(5.7666)\n",
      "Loss is  tensor(5.7629)\n",
      "Loss is  tensor(5.6818)\n",
      "Loss is  tensor(5.7926)\n",
      "Loss is  tensor(5.8404)\n",
      "Loss is  tensor(5.7495)\n",
      "Loss is  tensor(5.8034)\n",
      "Loss is  tensor(5.8464)\n",
      "Loss is  tensor(5.8337)\n",
      "Loss is  tensor(5.6293)\n",
      "Loss is  tensor(5.8033)\n",
      "Loss is  tensor(5.8176)\n",
      "Loss is  tensor(5.9416)\n",
      "Loss is  tensor(5.8799)\n",
      "Loss is  tensor(5.7128)\n",
      "Loss is  tensor(5.7858)\n",
      "Loss is  tensor(5.6411)\n",
      "Loss is  tensor(5.7066)\n",
      "Loss is  tensor(5.7789)\n",
      "Loss is  tensor(5.6846)\n",
      "Loss is  tensor(5.5973)\n",
      "Loss is  tensor(5.6514)\n",
      "Loss is  tensor(5.6973)\n",
      "Loss is  tensor(5.7779)\n",
      "Loss is  tensor(5.7255)\n",
      "Loss is  tensor(5.7722)\n",
      "Loss is  tensor(5.5737)\n",
      "Loss is  tensor(5.7221)\n",
      "Loss is  tensor(5.7801)\n",
      "Loss is  tensor(5.8497)\n",
      "Loss is  tensor(5.9418)\n",
      "Loss is  tensor(5.8261)\n",
      "Loss is  tensor(5.7713)\n",
      "Loss is  tensor(5.8188)\n",
      "Loss is  tensor(5.6927)\n",
      "Loss is  tensor(5.7933)\n",
      "Loss is  tensor(5.8261)\n",
      "Loss is  tensor(5.6759)\n",
      "Loss is  tensor(5.7997)\n",
      "Loss is  tensor(5.7192)\n",
      "Loss is  tensor(5.9440)\n",
      "Loss is  tensor(5.7773)\n",
      "Loss is  tensor(5.6688)\n",
      "Loss is  tensor(5.8119)\n",
      "Loss is  tensor(5.8728)\n",
      "Loss is  tensor(5.9372)\n",
      "Loss is  tensor(5.9466)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = Variable(torch.LongTensor(inputs).cuda()), Variable(torch.LongTensor(targets).cuda())\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        word_scores = model(inputs, 0)\n",
    "\n",
    "        r, c, h = word_scores.shape\n",
    "        word_scores = word_scores.view(r * c, h)\n",
    "\n",
    "        targets = targets.contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(word_scores, targets)\n",
    "        print('Loss is ', loss.data.cpu()[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = TextDataLoader(dev_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (word_embeddings): Embedding(10336, 64)\n",
       "  (lstm1): LSTM(64, 256)\n",
       "  (lstm2): LSTM(256, 256)\n",
       "  (lstm3): LSTM(256, 256)\n",
       "  (hidden2word): Linear(in_features=256, out_features=10336, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spothara/anaconda3/envs/i_study/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: This function is deprecated. Please call randint(40, 60 + 1) instead\n"
     ]
    }
   ],
   "source": [
    "final_ip = []\n",
    "final_op = []\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(dev_loader):\n",
    "        inputs, targets = data\n",
    "        final_ip.append(inputs.transpose())\n",
    "        inputs, targets = Variable(torch.LongTensor(inputs).cuda()), Variable(torch.LongTensor(targets).cuda())\n",
    "\n",
    "        word_scores = model(inputs, 20)\n",
    "        outputs = word_scores.transpose(0, 1)\n",
    "        \n",
    "        final_op.append(outputs[:, -20:].cpu().data.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 57), (32, 20))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ip[0].shape, final_op[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('dummy_pandas_train_word_latest.txt', 'w')\n",
    "for i, item in enumerate(final_ip):\n",
    "    for j, line in enumerate(item):\n",
    "        chars = [id2char[x] for x in line]\n",
    "        fp.write(''.join(chars))\n",
    "        op_chars = [id2char[x] for x in final_op[i][j]]\n",
    "        fp.write(''.join(chars))\n",
    "        fp.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch0.4)",
   "language": "python",
   "name": "pytorch0.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
