# load the df_events file into a dataframe
# most missing values in Specialty are now filled
#fitfile = FitFile('fit_files/2912551983.fit') #cycling (example with good gps) #fitfile = FitFile('fit_files/2853423540.fit')#short swim
# Missing value due to disabled value
# save df to csv
# Overweight
#  toar() is for conversion to numpy array:
# We get descriptive statistics on a single column of our DataFrame
#Remove bad sensor data, where a value is >4 degrees different from the 2-hour median temp. 
#Show the data types of each column
# models trained using gensim implementation of word2vec
# The contractor table has been created locally before running this step #Using if_exists='append' can prevent the constraint of columns being changed 
# Use `engine.execute` to select and output our chosen vacation day-time period of around 15 days. # It picks the first 15 days of january for which data was collected.  (remember any missing days # that are not in sequence may have resulted from the 1400 est data points missing from prcp, etc)
#print the r^2 score of the linear regression model to determine the level of accuracy the contains
# all the estimateds_count in the file is >= 3.
# Information about the independent factor w?
#Check that the concatenation worked
#silence some warnings
#df= df.sort_values(['label'], ascending = True) #temp= df.sort_values(['label'])
#this is p tag looking for, but not for tweets with image
# Run OpenMC
# export the df_events as a csv to data output folder
# Set the X and y
#check the loaded file
#stories.drop('tags', axis=1)
# train and evaluate the chunker 
#plt.xlabel('') #plt.ylabel('')
# Lets scale everyone's score down by 10% because there was cheating # You have to call collect gather the results back into the driver to view it
# Let's see how many people were on each deck
# Perform a linear regression. #model_x.summary() # For non-categorical X.
#We cannot use rating, review and source hmm
# Drop duplicated user # Check the drop worked
#Save the table directly to a file.
# sends command
#convert dictionary to a dataframe.  #print(precip_data_df.count())
#select by day
# number of age-mates in different age values
# look for seasonal patterns using window=52 - RN/PAs
# let's define a list # we will append a new number in the list
# We display the total salary each employee received in all the years they worked for the company
# Split the data into training and test sets (30% held out for testing)
# if we just want floats in the entries, we do
# import modules and set up logging
#split names - in data wrangling
#Create a box and whiskers plot of our MeanFlow_cms values
# select all the non-numeric columns # get the list of all the non-numeric columns
#setting driver up
#ANSWER TO THE QUESTION take two:
# Checkpointing feature
# get release data
#tw_sample_df = pd.DataFrame(list(collection_reference.aggregate([{'$sample': {'size': 5}}])))
# Using the groupby method calculates aggregated DataFrame statistics
# And of course, we can easily save our new data
#Transforming the files3 dataset
# resample to minute intervals
# converting back to an h2o frame
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#converting to datetime format of pandas for age calculation 
# Make columns for seasons and terms
# For a quick check, let's also look at this where it is ascending # code here:
# Verify that an address is valid (i.e. in Google's system)
#Need to put tweet in df with time stamp, sentiment analysis and tweet. 
#identifying the number of times new_page and treatment line up
#'Seattle': '300bcc6e23a88361'
# How many stations are available in this dataset?
# 100 X 100
# load the source data
# Note: x-axis is the index.
# Let's try this out with "device model" first - only about 7k values
# add local python functions # add the 'src' directory as one where we can import modules
#au.plot_user_popularity(very_pop_df, day_list)
#tickers = companies['tweet_ticker'].tolist() #Tweets with the ticker in front come in very slowly. May take a while to build up. But these are official tweets
# Plot all "Categories" with an occurrence value of under 500.
# check option and selected method of (12) choice of hydraulic conductivity profile in Decision file
# # data = pd.read_csv('xclara.csv') # print(data.shape)
# converting the timestamp column # ts_to_datetime = lambda ts: datetime.datetime.fromtimestamp(int(ts)) # git_log['ts1'] = git_log['timestamp'].apply(ts_to_datetime)
# write to xlsx
# With effect modification # drop1(adj.glm,test="Chisq")
# checked, all tw ID has 18 digits
#What was the largest change between any two days (based on Closing Price)? # Note, this is interpreted to mean for two consecutive days # Why does it list in reverse order?
# Calculate the interquartile range (IQR).
# one time only
# interestingly, we can use either or both levels of a  # hierarchical index to pick out specific entries
# View all the classes mapped to the Base
#try two sample t test to see if the means of these distributions differ
# res.to_csv("/Users/monstar/Desktop/a.csv", encoding="gbk")
# export the dataframe
# goodreads_users_df['joined'].replace('None', np.nan, inplace=True) # goodreads_users_df['age'].replace('None', np.nan, inplace=True)
# Here it should have merged 2001 to 2001 index and so on because all the other valeus are the same but it did not.
#churns
# We print percentages:
# Saving the DataFrame as a .csv file.
#Example2: Import File from URL
# Theresa May indices
# change data type into readable "datatime" format.  # datetime64[s] shows date/time data by seconds. 
#Design a query to find the most active stations. #USC00519281 had the highest number of obs, at 2772
#merge the 2 df
#Pull data into a data frame
# Write to CSV
#Export the dataframe to a csv file. 
##### sort by label on column lables
# Merge the adj close pivot table with the adj_close table in order to grab the date of the Adj Close High (good to know).
# How many stations are available in this dataset?
# % of pos/neu/neg
#Importing movies dataset from Data folder
#plate_appearances.loc[plate_appearances.batter_name=='ryan howard',['is_shift','hit','successful_shift', #                                                                   'shift_lag_1', 'shift_lag_2']].head(20)
# Extract the scale factor
# hyperparameter combinations test
# Get the file name
#(Train,test)-store (checking if there was any unmatched value in right_t) ##Note: Level of store table is "Store"
# We create time series for data:
#Calculate the date one year from today
#create a min and a max date
## Saving our data # save our pca classifier # save our crosstab_transformed
# Build the topic assignments into a dataframe and merge it with the original dataframe
# Make output folder
#  Extract endpoint url and display it.
# A string variable. Strings are funny in NetCDF
# Set file names for train and test data
# Export the new CSV as "NewsMedia.csv"
#print(dir(np))
# Lenghts along time:
# DataFrame objects have a nice plot method
# Create a scatter plot of likes vs views
# set the index to date
# controls the session # closes session
# Importing the built-in logging module
# Open <country>_users.csv and print last 5 rows
# but it has recovered it up to some tiny epsilon
# We display the updated dataframe with the new column:
# Note: text is already a clean-cleaned description stored in series
# convert a sequence of objects to a DatetimeIndex
#percentage
# print(pd.Timestamp.min(df1['created_at']), pd.Timestamp.max(df2['created_at']))
# Set up logistic regression # Calculate results
# test_preds_df.head()
# regular expression aka wildcard pattern
# Let say you want to x in index 1
#Save News Tweets to csv
#Manchester has so many retweets! Let's see what that tweet is that makes it so good #Looks like it's a video
# Create linear regression object
#Plot the distribution
# log reg
# reindex to inlcude all claendar days, using forward fill #fin_r = fin_r.reindex(r_top10_mat.index, method='ffill')
#data = pd.read_csv('BCH_credid.csv', sep=',', header=None)
#Saves the final dataframe to a CSV to be compiled in another notebook
# Your code here
# reduce graph data size
# All Data
# split str of content into a list of words and insert as new column
#Example 6: Load a csv while specifying column names
#Summarize, using our own percentiles
#no of unique users in dataset
#print(scaled_eth)
#def drop_rows(index, df): #    tweet_text = df["Words"][index] #    df.drop(index, axis=0)
# Extract endpoint url and display it. #scoring_url = deployment_details['entity']['scoring_url']
# Save file
# create pyspark-dataFrame
# This shows, that two columns are the same, so take only one column
# .grep() allows you to search for string patterns within a column
#checking np.arange array for use on next cell
# df1 = pd.read_csv('../output/data/expr_2/expr_2_pcs_nth_0.1_div_101_03-20.csv', comment='#')
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# Plot all the positiv and negativ of the dummy data # This is an example of not choosing to save the image
# SENTIMENT ANALYSIS BY NEWS SOURCE # To create the plot, first we need to know the exact source names as reported in the file just exported out
### Step 15: Fit the model and then make predictions ## The labels will identify if we are in the 0 or 1 cluster
# Find users converted propertion
### Step 12: Review the two-dimensional array created from the PCA
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# get number of unique elements
# Change to text value to category 
#Number of tweets vs length
# Random Forest
# Transform percentage into float
### Step 22: Look deeper into the data for non-weekdays that look like non-commute days ## We see the a pattern of mostly holidays
# Nonsmoker descriptive statistics
# a two year range of daily data in a Series # only select those in 2013
# Helper function to add to spelling dictionary
# Replotting and seeing if the moments changed significantly
#uncomment the next line to install. #!conda install -y pandas-datareader
# Initialize the timing of computation
# df.loc[df['column_name'].isin(some_values)]
# compound dataframe
# Join on the bitcoin price information in euros
# let's try CPU (20 times slower!)
#Exploring the dataset #We can see thet there are high number of 5-star reviews
# Let's see a historical view of the closing price
# Accessing single elements is also pretty straight-forward
# Now we are justing picking up the randome code without the '/watch?v=' in the string so we are getting  # from the 9th index to the rest of it. 
#find the end date from first non zero observation for tobs at given station #assuming each month is 4 weeks, 4 weeks * 12 months = 48
# Hint: Use the same : notation, but use the state names listed above # Your code here:
#Export all features in a file to check correlations
# mean and standard deviaton of my ratings
# Also only pulling the ticker, date and adj. close columns for our tickers.
# SORT AND THEN DROP non numric values
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
# estructura bd
#url for mars image
# Verifying that all our shapes are as expected
#shows.to_pickle("ismyshowcancelled_raw_pull.pkl")
# Split off the HTTP headers
# drop 10 empty columns #drop empty last row
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# q_all_pathdep = c.submit_query(ptoclient.PTOQuerySpec() #                                .time("2014-01-01","2018-01-01") #                                .condition("ecn.multipoint.connectivity.path_dependent"))
# drop rows with missing value in specialty column
#Generate html table from df
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=API_KEY&start_date=2017-01-01&end_date=2017-01-02"
# backup our model # show summary of model
## 1. Sort traded_volumes
# Example 1 # Example 2
# create new column 'prosecution_period' as difference between grant date and filing date # convert timedelta to integer number of days # show summary statistics for prosecution period
# Translated role
# Filter outliers in Input Power
# 74 comments or less is encoded as 0, 75 comments or higher is encoded as 1
# cisnwh8
# Mongo is but collection in database of life....
#Clean up some bad data at index 11650
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Analyse dataset
# Print the external node types
# Both Pandas and Numpy are sufficiently clever to figure out that the mean of a  # series shall be the mean of its contents provided they are numerical
# Make sure cells are full width of the screen
# Initialize environment # Initialize experiment
#urls of the 10 top commented posts
# calculating or setting the year with the most commits to Linux
# count the gene id
# running a sqlalchemy query to get the date and tobs from  the measurement table.
# head climate_pred
# forward fill
# Create a column that describes the type of injury based on the notes column using # the function I created: extract_injury, df['Injury_Type']
# make the new column the index for bitcoin prices
#define colors and legend values
# Or, we can fill nulls with a default value
# Percent of true gains over all samples: compare to precision -- precision should be signficantly higher if this is a useful # prediction # (cnf_matrix[1,0] + cnf_matrix[1,1]) / cnf_matrix.sum() 
# I can change the level of the logger.
#Looking at new table
# read dataframe
# I will add both and see which makes the most sense
# Fit network. Currently set the batch_size=1; will add more relevant information on this later.
# Mean of the two probabilities
# Calculate the average chart lower control limit.
# Delete the item from the current version
#Removing'userTimezone' which corresponds to 'None'. # Let's also check how many records are we left with now #removed 41 tweets!
# Download 311 data
# march has most delays
# only have partial jan 2017
# Import the libraries.
# Just get rid of those annoying |
# This analysis shows that there are more nulls in last four years
# load the dataset containing explanation of the variables in the diagnosis datasets # downloaded from: https://ida.loni.usc.edu/pages/access/studyData.jsp?categoryId=16&subCategoryId=43
#fill cohort activation dates
# creating an explicit/distinct copy of the features
# Set data for 2018 & 2019
#  and save a reference to those classes called Station and Measurement.
#try to drop duplicates on the projects dataframe
# remove retweets (highly probable repetition on dataset) # TODO: remove really short tweets!!
# check build estimator model
''''$ user access to matplotlib functionality, which is another Python package that can allow us to visualize our results.'''' $ import pandas as pd
#Read tweets approval ratings from csv file
# the number of reports from the most active station
# data=stock_data["CLOSE"].iloc[-100:]
# Create a string variable which will be used to get all of the different years data
#Filtering out the movies from the input
# Export
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# un peu plus lisible : les CNI avant le 10 janvier, # en erreur ( status : False)
#search best parameters #%debug
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Inspect counts
# data.head()
# Here's where I apply the train test split
# NS --> Non Seoul 
# so roughly 25% of data should fall outside either quartile - note rougly, since we have categorical data here # below lower quartile
query = ''' SELECT tweets_info.created_at, user_mentions, tweets_info.id, tweets_info.user_id, tweets_users.name$             FROM tweets_info INNER JOIN tweets_users $             ON (tweets_info.user_id = tweets_users.id); '''$
# To access a particular row, we can use the index name:
# Let's group purchase quantities by Stock Code and CustomerID
# resample data to weekly
# media de tmax anual
#ignore all 2* tweets #positive sentiment = 4* tweets
# We can view all of the classes that automap found
# plot
# show last 5 rows of data frame
# 3.
# getting a single document:
#merge in the data
# create a Series of incremental values # index by hour through all of August 2014
# Reading data from the rest of csv files.
# drop previous index
# View the types of data you can download usign wiski SOS
# Step 10: Conduct PCA to limit features to 2 ## (optional) use svd_solver='dense'
# make dummies out of categorical variables
# default value of min_count=5
#import the Linear Regression package from SK Learn to create the linear regression model #import the train,test split package to succeessfully create a model that uses a proper testing and training set 
# Loading data
# load .csv files created by SCRAPING
#create a column to indicate most recent measurements #where the current measurement is the max date, and there is more than one measurement for the wpdx_id
# For inputting the password
# Set index to Date # Apply the count function # Seeing what DummyDataframe look like
#make a copy of dataframe #converting normal values to log values using numpy's log function to remove anamolies
# isMax (bigger value is better than small value)
# number of tweets labelled as mortality salience during missile threat crisis period
# Sort the dataframe by date
#Display shape of df_students
# Make predictions using the testing set
#appending data of 1st computer to 2nd computer 
# Extract Projection Information
# do some sessions show up in one or the other? # looks like it is the case, and some sessions that don't have either (quite a few in fact...)
#Got the run ID from Watson Environment.
# Your code here
# Using the transpose function to make the rows columns
# Instantiate an empty Tallies object
# end_time :: end time of 2016
#read in the schedule file
#Check via customer sample
#plot = df['6/27/2018':'6/28/2018'].plot(y='lux')
#List the unique values inthe CharacteristicName column
## Paste for 'construction_year' and plot ## Paste for 'gps_height' and plot
### Step 17: Dissect just a portion of the data ## The red cluster has a high peak commuter pattern displayed (Commute days)
### Step 18: Dissect just a portion of the data ## The purple cluster has a more even distribution (Non-commute days)
# Save to file
# Load sklearn and LDA (Latent Dirichlet Allocation): # Load word tokenisers and vectorisers:
#compared the syntax with p4merge - the data is correct #only the solution code is missing: #gender, email & visit date
# Create the pie chart based upon the values above # Automatically find the percentages of each part of the pie chart
# check the number of unique locations # should be 1 less than the calls DataFrame
# default value of workers=3 (tutorial says 1...)
# Set a starting and ending time. #end_t = '2017-12-31 23:59:59'
# created_at is likely to be parsed as text # We convert dates to a date/time format so we can do 'maths' on them # If ok, dtype should be datetime64[ns] or <M8[ns]
#Test to make sure connection is working.
# subset for only type 
# What was the median trading volume during this year?
# Create a bar chart based upon the above data
#grouped_by_year_DRG.get_group(level=1)
#Put the indeed and tia data together, then clean some more
#BuyingData = BData.parse('Set4-User Buying Behavior')
#'Durham': 'bced47a0c99c71d0',
#Dictionary of value averaged by half-year
#What does our index look like? 
# Load the data from the query into a dataframe
#Here are the single tweets
# Place map
# mode calculated on a Series
# Look at the head of the dataframe
#get the mean prediction across all 
# Keep only text in the text column
# get 8-16 week forecast new patients # keep only date in index, drop time
#eliminate duplicate measurments
# We extract the mean of lenghts:
#read real-time data output, change column names, and convert to GeoDataFrame for spatial analysis
#reset index so that we do not have issues since we loaded multiple files 
# creating authentication  # connecting to the Twitter API using the above authentication
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
#Other filtering examples
# find the non-numeric feature columns
# Assign the Measurement class to a variable called station
# convert location to numeric value
# rename id tp tweet_id
#Check the number of tables in the database
#Joined(train,test)-weather (checking if there was any unmatched value in right_t) ##Note: Level of weather data is "State-Date"
#135.675300,34.955205,135.795300,35.055205
#Greensboro': 'a6c257c61f294ec1'
# Get and display model uid.
# with the below the Confidence Intervals for the predicted values of revenue are calculated
# Create the necessary dummy variables
# print min and max date
#  Sanity check on array shape, expect: (16042,) #  from which we can see the population size:
# check option and selected method of (11) choice of groundwater parameterization in Decision file
#torch.manual_seed(args.seed) #if args.cuda: #    torch.cuda.manual_seed(args.seed)
# Now that we have performed some test, lets go back to our intital dataframe which is df
# load stop words
#Iterating through the companies data frame for the twitter usernames #printing every twitter username
#information_extraction_pdf = 'https://github.com/KnowledgeLab/content_analysis/raw/data/21.pdf'
# store the first paragraph
# store the first title
### Plotting
# Show first 10 records
# it has a name
#TRANSOFRM THE PARAMETERS BEFORE GOING TO SEARCH 
# Merge train and items and store it on the train variable
# todo compute rolling corr matrix and plot
#Inspect distribution of subjects by prior tweet history volumes
#creating 2 arrays: features and response #features will have all independent variables #response has the target variable
# How many stations are available in this dataset?
# Busqueda de valores nulos
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser' # Examine the results, then determine element that contains sought info
#save the map as a webapp
# plot autocorrelation function for first difference RN/PA data
# Plot time (x-axis) against length of tweets (y-axis)
# Before we merge the two datasets, let's ensure there are no duplicate rows in the user information dataset
# find historical data for 2015
#Complete a merge of the grouped dataframe and the schools to bring in the type of school (District or Charter) that #each school belongs to. #Presents the final dataframe.
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
# Get a response to the input text 'How are you?'
# load preprocessed data 2
#Save figure
# Create your stream object with authentication
# Get all unique artistId, and broadcast them
#Great. Let's just verify that there are only "complete" permits in this data frame.
#select the latest sprint that the stories are in and then we can filter the ones that sprints that are closed. #only after the above is done, we can filter the stories that have their latest sprints closed
# population or sample size of treatment group
# Call function and retrieve data
# grabs the first row of our data table
# better than get_hist_data
# C(w) refers to w as a categorical variable.
# Drop the unnecessary columns and rearrange columns
### Fit Your Linear Model And Obtain the Results
# the names of the columns have become the index # they have been 'pivoted'
# Naives Bayes with Kfolds on our Test Data
# Pulls the first 5 columns from the data
# use a right close
#add it to the csv
# Car KOL pred
# When convinced Raw and Processed data is good, upload to bucket. # Upload raw heat pump spreadsheet to the bucket.
# For getting all the text (paragraphs)
# Check out the actual counts within each wear day bin
# Spark SQL has the ability to infer the schema of JSON data and understand the structure of the data #once we have created the Dataframe, we can print out the schema to see the shape of the data
# printing first five rows of the data frame #df.head()  # 5 is the default value. we can ask for any number as shown below
## Remove Nones and print highest and lowest prices
# Pickle the 'data' dictionary using the highest protocol available.
#sample query to verify key works
# Lemmatize verbs by specifying pos
#check
# Now we use the Base class to reflect the database tables
# 271 is a memba production (not a photoz) #print "Number of rows with 'production_id'==271   : ", len(np.where(df3['production_id']==271)[0])
#extract the url of background-image 
#open and read the file into a pandas data frame
### Fit Your Linear Model And Obtain the Results # Instantiate the model # Fit the model
# first tweet of 9/20/2017
#Stem words
#Group by Target, aggregation average by Compound 
# examine the huge bar on top, consisting customers who made their the first and last purchase within 80 days
# RETRIVE TWIT IN SHORT MODE (as-is)
#remove new lines
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# copy Snapshot data
# Joining two data frames
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
## Check if there is 'None' in their 'userID' column ## Check how many tweets are we left with now
# Invoice data generator
#Now the crime data only has M and F
# Load Data Set # Display several random 'size' features
# adding prefix VIP_ to column names
# Convert created_utc to readable format
# Total hours
# Average all polarities by news source # View the polarities
# convert column into int type 
query_term = 'Disaster risk assessment'$ query = """SELECT distinct e.pullquery, e.pullby, a.optionalid01 as doi, c.title, d.abstracttext,c.journalname,c.pubyear, c.publicationdate from public.datapull_detail as a inner join public.datapull_uniqueid as b on a.pullsource = b.pullsource AND a.associatedid = b.associatedid inner join public.datapull_title as c on b.uniqueid = c.uniqueid inner join public.datapull_text as d on b.uniqueid = d.uniqueid inner join public.datapull_id as e on e.pullid = a.pullid where e.pullquery = '{}' limit 25""".format(query_term)$
#group["C"] # group['C'].shape # group.keys()
# group by blocks
# Check the rating of a known FA class article.
# Imports the Google Cloud Client Library. # The client must be created with admin=True because it will create a table.
# ROC-AUC curve is its own Class. That seems kinda weird.
# UN01.DE is Uniper SE, a recent spin-off
# read from the appl worksheet
# Split Data # To avoid Data leakage split the data
# We remove the watches and shoes columns # we display the modified DataFrame
# Show all characters belonging to the amount
# another example, list of countries
# import historical data
# Mars Weather URL to Scrape
# check shape of empty grid
# train word2vec on the two sentences
"""$ Print any single news title$ """$
#Create year values from the ActivityStartDate column
# Formatting the dataframe in decending order
# A:
# Reflect Database into ORM class ===save references
# get the top level key for the nested dictionary containing max. daily change
# Check for null values for each series
# check inserted records
# create an array of linearly separated values around m_true # create an array of linearly separated values ae
# Print each negative tweet
# Alternative way to get the date only 
# We can view all of the classes that automap found #Base.classes.keys()
# group the data by authorId and authorName and extract a number of stats from each group
# Create root universe
# we can get the first value in the date column
# This code was tested with TensorFlow v1.4 # The import statements will not work with earlier versions, because Keras is in tf.contrib in those versions
# can also just type 'results'
# does not recover exactly due to insufficient floating point precision
# add 5% of noise
#print the first two parts: overall introduction and timely publication
# This will print the most similar words present in the model
# Number of stations
# Show how to use Tally.get_values(...) with a CrossScore and CrossNuclide
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("spotify_merged.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
# concat grid id and temp
# If the error `No module named 'matplotlib'` happen, execute: # pip3 install matplotlib  # on terminal.
# Build Pie Chart
# Get the current date and time # Read the time and date now
# df_plot.sort()
# Set up the Auth Tweepy object. This takes your credentials from above # and does some authentication with Twitter
# looks like there aren't any posts with this topic as the max
# summary
# 200 X 200
#marvel_comics_save = 'wikipedia_marvel_comics.html'
#proportion of users who converted to other page
# Likes vs retweets visualization:
# most common tags
# !pip install scrapy
# P value
# Save the dataframe in a csv
# DATADIC contains 
#Construct time features
# create timestamp index
# Make vhd folder
# filter on Suburban # len(Suburban) # Suburban
# fancy indexing creates a copy
# Checking sample data
# Use Inspector to print the column names and types for measurement
# A:
# Sort according to EXT, y first, no latter
#exploring the categories csv file
# pull a subset of data for testing
#Converting timestamp to a true datetime64 format
# Count terms only once, equivalent to Document Frequency
#Check the combination of contractor_number and contractor_bus_name are unique #The combination is unique so far.  #Check duplicated rows by contractor_number 
# Remove the ".csv" from the file name # Set the index as the 'mail ID'
#Mars Images
# can only fit model once
# generate LDA model
# Copy data frame and drop rows containing null values
# display(flight.select("duration").show()) # display(flight2.select("duration_h", 'duration_m').show()) # flight2.toPandas().head()
# the connection/session
# Get rid of the duplicate entry
#A quick scan reveals this amazing user description:
# Export to "tallies.xml"
#Calculate the difference in NDVI from before to after cyclone
## Python will convert \n to os.linesep
# let's boolean for cat/funny is in the title
#set input/output paths #can eventually set this to the SOAPY API https://dev.socrata.com/foundry/data.waterpointdata.org/gihr-buz6 #DATA_PATH = "/Users/chandlermccann/Google Drive/Google Drive/Berkeley MIDS 2016/W210-Capstone_WaterProject"
# Create optimizer
# get sum of interceptions by game day
# Apply the percent function # Seeing what DummyDataframe look like
# Create time series for data:
# convert to categorical the AGE_groups collumn.
# subset us lon # subset us lat # print dimensions
# Supposedly this AppAuthHandler is faster since it doesn't authenticate
# Look at some of the tickets along with labeled tone
# first we get some 
# p_new under null 
# print_full(feedbacks_stress) # print_full(feedbacks_stress)
#Events per day
# groupBy function usage with the columns we group to become as the index.
#most referers are the mobile app
# The proportion of p_diffs that are greater than the actual observed difference.
#Mars Weather
# Convert 'total_bill' to a numeric dtype # Convert 'tip' to a numeric dtype # Print the info of tips
# Name of columns
# !dpkg -i libcudnn7_7.1.3.16-1+cuda8.0_amd64.deb
# choose a random point for the update  # update m # update c
# "Slice" the nu-fission data into a new derived Tally
#convert all columns
# compare the predicted and actual labels for a query
# of unique messages = len
#Get the rows has appointment only
# Transform the text to a vector, with the same shape of the trained data.
#gives you the class average
#results = Geocoder.reverse_geocode(df['latitude'][0], df['longitude'][0])
# Transform Burgers vector to axes
#Example 1:
# A:
# remove qstot_12 and qstot_14
#to get vectographic representation
# Create a new year column
# dicts to map numbers to words/labels
# Run the model with training set 
#Wait that's so cool! Emojis show up in jupyter notebook! #I'm noticing a lot of these tweets are retweets
#Calculate the highest opening prices for the stock during 2017
# Instantiate a lgb classifier: lgb
# Print the first 5 external nodes
# extract weekday and publish time from vacancies
# We wish to iterate over the number themselves
# dummying rate_experience 
# count by date in datetimes
#testing queries on it 
# a query to get the Twitter place_id (code) for each city
# Calculate probability of conversion for new page
# hyperparameter combinations test
#New Col added
# Insert distance2 into df as a col # loc=11 sets the location at index11 after end station longitude # value=distance2 sets values of the column to the pd series list created above
# Find out Shape of the data
# Define the Cosine Similarity function
# Headless Chrome #options.add_argument('headless') #options.add_argument('window-size=1920x1080')
# FileLink(str(FLASK_PATH/'df_parent.csv'))
#prenatal pairings, all time
## we will gather all topics from subreddit (limit=None)
#Task 1: Import 2017 daily data for AFX_X 
# reg_final = xgb.XGBRegressor(**best_model['params']).fit(X, y)#**params
#Display answer as a Series
# Using the input folder variable specified at the beginning of the code, and the string variable specified above # Create a variable containing the filename 
# See first 10 stations that are not shared
# retrieve life expectancy at birth for all countries # from 1980 to 2014
# unigram chunker 
'''Reviewing the Y Dataframe'''$
# credit card payments
# and mode
# Top 10 least active companies
# Numero de filas y columnas en el dataset
# The mean absolute percentage error
#Write out data to csv
# Plot ACF of first difference of each series
# it's important to realize that this is a series:
# get the top level key for the nested dictionary containing max. daily change
# convert crfa_f to numeric value
# Inspect master size
# add a notebook to the resource of summa output # check the resource id on HS that created.
# count occurrences of 'jewish cowbell' triple parenthesis '(((like this)))'
cur.execute("""SELECT title, journalname from public.datapull_title limit 10""")$
# Series what's the name for in Series # operation
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# check inserted records
# Drop all dupclicates so only one instance of each song remains
# Read the data.
#Create custom stop word list
# Get a list of column names and types # columns
# Express as a time delta in days using the NumPy function timedelta64
#ua is a dataframe containing all the united airline tweets
# change data type # ARR by industry QT
# run this cell
#Variable Time is now decimal day time.
# March 2016 was our start period, and the period frequency is months.
# Mean calculated on a DataFrame
# Can you tell me the average low for the week?
# anchor random seed (numpy)
# Rename index levels
# Cut postTestScore and place the scores into bins # Creating a group based off of the bins
#run a sample query
# your code here
## Feature importances
# Filter outliers in hspf
# number of iterations in simulation:  reccomended 20000
# Applying the function again to the column 1
#Any funny minumum values left?
#Storing the movie information into a pandas dataframe #Storing the user information into a pandas dataframe #Head is a function that gets the first N rows of a dataframe. N's default is 5.
# Import Naives Bayes and 10-fold cross validation # Prepare the kfold model # Leave one out cross validation model
# Lower value.
# Create a cumulative balance column in BTC for gdax
# examine the types of the columns in the DataFrame
# Transmission 2050 [GWh], marathon
# Stip off the year and save a list of %m-%d strings
# Assign company label to each dialogue
# resampling degree day data. The first line takes the average when a given hour has multiple entries. # The second line generates daily data.
# print(df_final['created_time'][0].split('T')[0])
##### now apply to rows
#We are sampling the movies into 5 bins based on ratings
# cannot validate a boolean as a string
# Check SFL stuff
# extract precip array # check shape
# set SUMMA executable file
#Switch to dict
# get the dataset we just uploaded
# plot histogram
# Create a linearly spaced array of 5 values from 0 to 100
# empty dictionary to append number of tweets to 
#The result is correct; 
# RN/PA decomposition
# make predictions for those x values and store them
### Step 20: Use the day of week attribure of the datetime64 object 
# dropping columns '[', ']', and ','
# list all scores that aren't null
# Import data
# sort posts by number of likes, and only return posts that contain a tattoo:
### Fitting the Linear Model And Obtain the Results
# check NA
# subset ndvi to us
### Fit Your Linear Model And Obtain the Results # Ab_page is used, which means, new_page and treatment is the baseline
# droping those rows where all the rows have na
#set up only display 10 rows c # can do this in environment
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
#create arima model
# draw the boxplot of total based on different types
# Applying function to "Stocks" 
#I don't need this #type(rptdt)         pandas.core.series.Series
# Random forest score:
# write your code here
# Pickle the 'data' dictionary using the highest protocol available.
# To make forecast for next 90 days
#Design a query to retrieve the last 12 months of precipitation data.
#computing the f1 score
## the reason we subset [0,1] value is that np.corrcoef returns correlation matrix between variables
#converts to dataframe
#2D correlation matrix plots scope vs type vs nusers
# Define the DataFrames with only a single "Category".
# Read the file into a DataFrame: df # Print the head of df
#'Memphis': 'f995a9bd45d4a867'
# get 8-16 week forecast existing patients # keep only date in index, drop time
# rename "value" field to "mmol/L" # convert mmol/L to mg/dL and create a new field # view the cgm mg/dL data
# Which learner had the most interactions and how many interactions did they have?
#happiness_df
# Combine ET for each rootDistExp # add label 
# Create Dataframe from Dictionary
# also we can specify the intersection of rows
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Examine the results, then determine element that contains sought info
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#view result
# Add a column level for our new measure # Concat it with our original data
# Import and Initialize Sentiment Analyzer
# Create BeautifulSoup object; parse with 'html.parser'
# join in information about occurring words, probabilities etc
#code below used to deal with special characters on the file path during read_csv()
# Plot time (x-axis) against number of retweets for each tweet (y-axis)
# So pretty much this just shows the annual return of each stock so that -> # you can see which stock has a higher annual rate of return.  # In this case you can see that MSFT is better than all of the others.
# analyze validtation between BallBerry simulation and observation data.
# read back binary arrow file from disk
# Create colors for the clusters.
#Constructing a model on a scaled dataset
# datetime conversion and created a new column NDATE
# Get the .txt files
# Design a query to find the most active stations.
# get numerical features
# approx cost for a year
# Use Pandas to calcualte the summary statistics for the precipitation data
#finding the important features
#set up reddit connection
# convert 'publish_time' to datetime format # the big 'Y' was key to making the 'to_datetime()' work
# We are joining the developing dataframe with the sp500 closes again, this time with the latest close for SP.
# Create a new dataframe for the (It does so by testing true/false on date equaling 2018-05-31)
## Sort by Date
#delete unknown gender type
#overallCond = pd.get_dummies(dfFull.OverallCond)
# Checking if quandl is working
# dropping front end of curve, dampens predictive power
# reflect an existing database into a new model --> creates a Base object # reflect the tables --> creates ORM objects for each table in the 'hawaii.sqlite' database
# 6. What was the average daily trading volume during this year?
#Save figure
# get the count of unique locations
# To check wheather our dummy variables are create or not
# json objects are interactable in the way python dicts are.
# make new column for month. 
# convert retrieved records to a dataframe
# resample data to weekly
#Find the index of opening price #Some opening prices were None
#convert to df so can change more times and also download as csv if wanted
# predict on the last two months
# Number of dialogue
#'Minneapolis': '8e9665cec9370f0f'
SQL = """$ SELECT rating, title FROM film WHERE rating = "PG" OR rating = "G";$ """$
# alternative: assign method (input arg must be a valid statement # that could be dumped on the prompt)
#### Test #### 
# have a peek at the data
#Reading all necessary CSV files from Scrapy in the DIR
# Movies with the highest RottenTomatoes rating
# Dummy subreddit
### Create the necessary dummy variables
# No null values, but from a visual check, there are definitely blank strings  # in the region and country series. We should check for those as well.
#modelD2V.wv.wmdistance(currentData[0].words,currentData[1].words)
#Save Test Dataset as pickle for later use # the test data in pkl format
#Understanding the total amount of time for the postings
#check
#cur.execute('UPDATE actor SET first_name = \'HARPO\' WHERE actor_id = 172;')
# num_comments along the time
# We print percentages:
#write to data frame
query = ''' SELECT t2.id, t2.created_at, t2.is_retweet$             FROM tweets_info t1 INNER JOIN tweets_info t2$             ON (t1.retweet_id = t2.id); '''$
# Create pivot table
# search for vendor number by name
# Create new column defining high: # comments > median and low: # comments < median 
#probably not necessary, but i'm going to drop unblended from Y and then re-add
# Sure enough, the read_csv method sucessfully converted our data to use a DatetimeIndex. # Unlike with plotting numpy arrays, where we had to provide a formatter for diplaying dates, # pandas is smart enough to do it for us
#Compare shapes of the two dataframes
# get the bitcoin prices
# It can sometimes be useful to know which column contains the maximum value for each row. #
# read in the advertising dataset
# plot histogram
# show dataframe first rows
# Just in case: close the reading of the LSST catalog file.
# Store the wavelength info into a key of the same name
# Importing data from the processed 'news' DataFrame. # Define current working DataFrame.
# Save file to csv
# Reading dataset after ignoring initial space by skipinitialspace=True
# Make sure to also output the intermediary steps
#from rules, perishable weighted as 1.25
# Variables:
# df.tail(2)
# UniProt mapping
# Concatenate ebola_melt and status_country column-wise: ebola_tidy # Print the shape of ebola_tidy # Print the head of ebola_tidy
# can give a Series a single data value,  # and it will be repeated # to be as long as the index
# To insert a document into a collection we can use the insert_one() method:
#REFLECT DATABASE TABLES INTO THE SQL ALCHEMY ORM # Create an engine to connect to the database.
# Repeating words like hurrrryyyyyy
# accediendo a filas y columnas por posiciones enteras
# Not really needed, but nicer plots
#title_alt = imgs.find('img')['alt'][2]
# Assign tie values the maximum rank in the group
# Retrieving the last 12 months of precipitation data.
# Read one site as a DataFrame
#Create BeautifulSoup object; parse with 'html.parser'
#face detection attributes
# Count terms only (no hashtags, no mentions)
# useful: .between
# there are 63,378 tweets from these influencers
#input_nodes_DF = nodes_table(nodes_file, 'FridayHarborBiophysics')
# Summary statistics for the precipitation data
# Save DF to json file
# Try Multinomial Naive Bayes model with TF-IDF # Define X and y # Train test split 
# To flatten after combined everything. 
#Accuracy on test data
# observe results
#tweetsPorFecha['tweetCreated']=tweetsPorFecha['tweetCreated'].map(lambda x: x.strftime('%Y/%m/%d') if x else '')
# Check if users could be in calibration for only some days
# does it return True or False from re.match
#2a. You need to find the ID number, first name, and last name of an actor, of whom you know only the first name, "Joe." What is one query would you use to obtain this information?
# Distinct Number of Users
# Create engine using hawaii.sqlite created in database_engineering steps
# we get 726 rows for our 2 years of data
# Insert new column at column index 5
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned # Use parameter limit = 1 to only return the latest date data
# sort in expiration order    
# cancatenate with duplicate indices
#check if there is duplicated name; #The name is unique per record
# get column names
"""$ Group news by day of news_collected_time and concatenate news_titles$ """$
# take only elements that belong to groups that have less than 5 members
# Print paragraph texts
#group data  #total fares for pie slice
#getting dummies for car models
# Even any other numpy unversal function can be applied 
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# np.where() is a vectorized if-else function, where a condition is checked for each component of a vector, and the first argument passed is used when the condition holds, and the other passed if it does not # We have 1's for bullish regimes and 0's for everything else. Below I replace bearish regimes's values with -1, and to maintain the rest of the vector, the second argument is apple["Regime"]
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Display first 10 elements from dataframe:
# join the output of the previos grouping action to the input data set  # the goal is to optain an additional column which will contain count of publications per author
#Convert 'M'/'F' to 1/0 # Converting binary variables from strings to numbers
# Retrieving statistical information
#print model.words # list of words in dictionary
# Add a row to the DF # First create a new DF with those rows
# Neat way to rename columns and order appropriately
# calculating number of commits # calculating number of authors # printing out the results
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
#== Series by passing list: Index is auto-created ==
#print head of type
#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'
# get categorical dataframe
# remove subjects with more than 50% of missing values
# create a safe copy #Category: Music
## The QTradableStocksUS universe generally contains a greater number of assets than previous iterations of the tradable universe. ## The resulting summary table displays the mean, std, min-max of daily median in addition to number of assets in this universe:
# Changing the type of the hashtags and user_mentions columns from string to list
# To know the value of country count
# Requirement #2: Add your code here
# calculating or setting the year with the most commits to Linux
#arima11= ARIMA(dta,[1,1,0],freq='Q').fit() #arima11.summary()
#target
# Specify a start point and how many periods should come after.
# how many unique products do we have listed in this purchase history? 
# we only need review after 2017-01-01
# When we dereference an instance method, we get a *bound method*; the instance method bound to the instance:
# output to .csv file
#res4.text
#!pip install pymysql
# group by 'A' column for the same value, and take sum to each group  # Column 'B' is ignored for sum() operation 
# Start of time estimation for the notebook.
# Query for last 12 months of precipitation
#make a copy of dataframe #converting normal values to log values using numpy's log function to remove anamolies
# Setting index
#Note : The time column should be of unix format
# resample to 1 min intervals, then back to 1 sec
# Convert columns from string to datetime
# define the vector w
# search for word in the SpaCy vocabulary and # change the is_stop attribute to True (default is False)
# save the tweets to disk
# 'Reno': '4b25aded08900fd8'
#Example2: Passing the sheetname method 
# get just the cgm data # look at the first 5 rows of data
# builtins.uclresearch_topic = 'HAWKING' # 4828104 entries # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# Since we do not need the 'count' lets drop it from the dataframe
# Convert the sex column to type 'category' # Convert the smoker column to type 'category' # Print the info of tips
#Youtube videos by country
## no.of unique cats taken in by the shelter 
# The base Dataframe's beginning.
# Make dataframe
# Build Pie Chart
#List the counts of records by month (easily changed to show year year)
# Create logit_countries object # Fit
#We can have a look at the different labels which were created in the VCF
#Create two new dataframe views: One with records before Falls Lake an one after
# Run the prediction
# Get a list of column names and types
# close the connection
# combination of education and purpose
#Make predictions using fit model
# we'll read the user file into a Pandas dataframe.
# evaluate the chunker
# Write data out to an excel file
# your code here
# import modules & set up logging
#%% Read dataframe 
# Process data using Natural Language Processing techniques: clean, remove stop words and lemmatize.
# to find columns for normalizing # looks like diff, trans, volatil, transpm
# bash environment
# Cycling distance # Create a tuple of data
# Joining categorical data back with Ideas DataFrame
#pd.DataFrame(content,columns=columns_name )
# Let's combine the original comments with the model predictions so we can more easily make sense of the results.
# Number of ratings
# Dataframe for alpha values (transparency)
# write to db
#ordered and paired after genesisco
# Dataframe of dates (contains 21 values)
# Reflect Database into ORM class
# Read/clean each DataFrame
#Join the tables
# The following command retrieves the symbols and price for all stocks with a price less than 10 and greater than 0
#Do Data Frame queries
#stats.normaltest(model_arima121)
#### Define #### # Capitalize the first letter of p1,p2,p3 columns to make it consistent #### Code ####
# Create a df from list of tweets  # Each row is a separate tweet
# Filter rows by considering "name" and maximum "capacity # Apply general template of columns
# pickle classified data.
## Dump Results
# load the query results into a dataframe and pivot on station
# by year and in-season vs. offseason
# Now let's clean up and change the index column to just be the actual data column, instead:
# myclient = InfluxDBClient(host, port, user, password, dbname) # myclient.create_database(dbname) # myclient.create_retention_policy('inf_policy', 'INF', 1, default=True)
# Clean up the 'brand' column.  Strip spaces and proper case strings.
# shape of coarse grid
# Create an OpenMOC Geometry from the OpenMC Geometry
#Initialize modules for plots
# Display the row's columns and data in dictionary format (first row)
#mistake here to just use priors_reordered for reorder =1 which makes all data reorder =1 only
# Construct pandas Series objects
#drop actual == NaN, since we don't know beat or not...
# Make predictions on test data using the Transformer.transform() method.
# current.head() # print(data_current) # data_current
# confirm review shape
# results are returned as an iterable list
# google
# Combine each stomatal resistance parameterizations # add label 
#df2 = df2.drop(["Symbol"], axis = 1) #df2.set_index("Date", inplace = True)
# read all data ignore bad lines
# x is a date in string format e.g. 2/7/14, 12:08 PM
# use cross_val_score() # ... #
# In the tweet_data dataset, Setphan and Bo are the top two dogs that have most favorite and retweets
#Have a look at a few random records
# Save png
# plt.plot(np.cumsum(pca.explained_variance_ratio_))  # plt.show() 
##### unless specified otherwise, head and tail are first and last 5 rows #### print(df.tail)
# NOOOO OOOLD
# convert 'C' to null
# Inspect dataframe
# Verifying that we have compatible shapes of correct dimensionality
# Add Rapid Watershed Delineation onto map
# .index.values,'created_at']='2010-11-11 03:59:09'
#forecast_range.head()
# count the number of complaints per month
# Put the data into HDFS - adjust path or filename as needed
# List of countries
# get day name from a date
# mean
# query for prcp data based on date range from most recent to a year before #make data from from sql query
# get today's date
# Delete the first column.
# print a single tweet json for reference
# separating new fans and returning fans
#remove columns with identifing info
"""$ Check shape and confirm that no NA values are present$ """$
#finds date of most recent and least recent tweet
# Tokenizing your data for use with the model
#Frequency count
# perform fit
# create a csv of the url list #print(df.head())
# try on the test data, however, the result not very good # reason might be overfitting
# export
# Cargamos hoja de calculo en un dataframe
# plot fixation counts
# Go to this link: https://www.quandl.com/data/BCHARTS/ITBITSGD-Bitcoin-Markets-itbitSGD # follow the instructions on quandl's python page, store the results in the coin_data variable
# Visualization of log odds - this needs some work as you cannot really visualize 10,000 words
# read sets into a DataFrame; index is chosen as the orient so the set names are the indices
# our main index is the 'Mail' column
#merge average fare and ride
# Perform a query to retrieve the data and precipitation scores
# Querying by Team by Game (shows both teams' stats)
# df_2016
# Read Measurements file with pandas and inspect the data
# First let's turn each title into a list of words # Then we'll use the lemmatizer to change plurals to singulars to group them # And rejoin the lists of words to one string for the count vectorizer
# See if you can select the high temperature for January 18
#register a data frame as table and run SQL statements against it
# create dataframe of matrix  and then inputting the confusion matrix
# Instantiate a new MGXS Library from the pickled binary file "mgxs/mgxs.pkl"
# Writing the file as csv
# now I want to find the first and third quartiles
# URL of page to be scraped # Retrieve page with the requests module # Create BeautifulSoup object; parse with 'lxml'
# build the same model, making the 2 steps explicit # can be a non-repeatable, 1-pass generator
# Transforma datetime para string # Sera utilizado a data em formato de string para aumentar a performance # de busca/insercao de dados do dataFrame usando .at[index, col]
# I want to use woba as a feature in this model. let's make sure these values are close to reality # looks good https://baseballsavant.mlb.com/expected_statistics
#Remove small tokens with less then 3 characters.
# instead of fitting a model, let us predict probabilities
#Bakersfield
# extend the stop-words list
# All of the steps above a needed to build a combined dataframe # Notice also that the date is a column
