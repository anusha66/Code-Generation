#how many wells that were good at time of measurement do we predict will fail within one year from today?
#Now, build the graph...
# make a time series of mean score per day
# Setting the columns names
#Make 5 recommendations to 2093760 # construct set of recommended artists
# First get the data # quick check to make sure no object dtypes in dataframe
#Design a query to calculate the total number of stations.
## Make csv file
# Count the number of unique values in column <landing_page>
# Store our sentiment analysis in a dataframe
#Looks like a lot more people from Costa Rica now. Let's try to aggregate all the locations with 'Costa Rica' in the name
# Get a random sample of 200 entries from the dataset. # Simple scatter plot
# Revenue has null values for some company. 'V' has been such identified company. # In this experiment, we remove the company from the dataset instead of interpolating. # Convert quarter_start field to datetime.
#etsamples_100hz = etsamples_100hz.reset_index() #etsamples_100hz.loc[:,"eyetracker"] = etsamples_100hz.level_0
# months: use the builtin calendar module
# We can use a factorplot to count categorical data
# Print percentages:
# Load the results into a pandas dataframe.  #df.set_index('prcp', inplace=True, )
# i need to kill 2 days with no data
#Use unique to show what the 4 unique values are
# Plot the scoring history to make sure you're not overfitting
# check if any values are present
# rtemoving unused columns
# To validate charmander #post_discover_sales[~(post_discover_sales['Email'].isin(pre_discover_sales['Email']))]
# 11. 
# Check the list of column
# First let's turn each title into a list of words # Then we'll use the lemmatizer to change plurals to singulars to group them # And rejoin the lists of words to one string for the count vectorizer
# foreach through all tweets pulled # printing the text stored inside the tweet object
# Take the date and time fields into a single datetime column
#274 is what our value of "missing" is for well age. Excluded that here
#plt.plot('date', 'tweets1', label='tweets')
# Get the maximum likely class
# How many stations are available in this dataset? # Print results of above count query# Print  
#  ENCOURAGE RE-RUNS of this cell #  showing one-year of simulated price histories... #  ... like playing with a kaleidoscope.
# To insert a document into a collection we can use the insert_one() method:
# View certain rows of the test dataset
# Sort the data by operator and part.
# shift using a DateOffset
#sort by [0] to agg same plan_permutations
# Convert columns to numeric values
# assign the cluster predictions to our customers #cust_clust.head()
# dropping columns '[', ']', and ','
# print out details of each variable
# merged1['Specialty'].isnull()
# Display the most commonly rated movies
#Last check for NA values
# columns
# To find  pnew  -  pold  for your simulated values from part (e) and (f).
# same data as before
# type(transaction_dates['Created at'][0])
# Count the total number of tweets
# intentionally not skipping rows here since we need the metadata too
#Task 7: Find the median trading volume
#Load engine created in database_engineering
#### Test ####
# LOADING LIBRARIES WE MIGHT NEED... # # statistics
# Save cleaned and encrypted dataset back to csv without indexes
# get 8-16 week forecast new patients # keep only date in index, drop time
# see an example for a given author (e.g. Zhou Shisheng)
# less than 500 # more than 500
# Create a dataframe with the minimum and maximum values of 'x'.
#Saving unique ratings into a numpy array
# Create an index transformer that calculates similarity based on  # our space # Return the sorted list of cosine similarities to the first document
#total3.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/objs_miss_fall2016.csv',index=False)
# make forecast for future dates created (pd df object)
#Check the conversion went through smoothly
# open all files
# Set up arrays for charting
# print some configuration details for future replicability. #hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE
# Spark sql will only work with table so register the dataframe access_logs_df as table #rename it as AccessLog as the query given earlier had the table mentioned as such.
# extract data # read in correct data
#Calculate average reading score
#array with 23 dataframes (one for each week) with the videos, tags and other information
# Running and Trainign LDA_3 model on the document term matrix.
#print_sentiment_scores(twitter_datadf['text'][1]) #analyser.polarity_scores(twitter_datadf['text'][1])['neu']
# Predict
# Create a set of boxplots of like_ratio by category
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#print df
# MultinomialNB with same three features above
# 10. Which of the vignette has the largest mean score for each education level? What about the median?
# # we'll filter out the non-represented classes, sort them, and plot it!
# Load the President Trump's tweets
# common_words
#read it into a dataframe
# We can resample the 15 minute data to make it hourly # this resamples a datetime index to hourly and averages each value within the hour 
# checking out their unique values, for a single level  # checking out their unique values, for combinations of multiple levels # See answer at https://stackoverflow.com/questions/39080555/pandas-get-level-values-for-multiple-columns
#format the trend date from string format of mm/dd/yy to date  #format mm-dd-yy, for correct datatype assignment
# counting missing electricity points # lots of missing data in 2017, this may help explain why R2 dropped when used on 2017 data
# read parquet file created with arrow with dask for compatibility check
#selecting by broadcasting: this selects everty 1000th row from my data, stating w row 0
# Summary statistics
"""$ Run preliminary test of classifier to get baseline accuracy$ """$
# find historical data for 2008
#accessing columns #or
# Use Pandas to calculate the summary statistics for the precipitation data
#df.loc[20]
# it seems the overal quality of estimate is not that high?
# Probability of user converting
#Checking for the null count
# Querying by Team by Game (only shows one team's stats)
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Checking spread of created vars
#'San Jose': '7d62cffe6f98f349'
# Save twitter data to CSV file
'''Remove duplicates'''$
#separate numeric and object variables
#Re-ordering the DataFrame
# filter out unassembled DNA sequences from the GRCh38 dataframe # drop unneccesary columns # sort the sub-gene dataframe by lenght
################# # Start here    # #################
#let's look at temperature across time by max temperature
# Print records in test.csv
#(print(len(a[0])) #The lenght of the feature vectore is equal to the number of different words in the test sample.
# Converting my tobs_date list to a dataframe.
# transform the rate into numeric variables
# For finding the first nav
"""$ Check number of news per day$ """$
#Check via customer sample
# First, import the relevant modules
# Check if there are any NaN values in the Test DF
# Take a peak at the data
# Calculate how many times people talked about IKEA by following function  # Applying same way, count data of other interested cities are saved as following dataset
#Select rows where the Confidence indicates estimated:
# check if the DB name exists     cursor.execute('''CREATE TABLE IF NOT EXISTS fib ($                             calculated_value INTEGER)''')$
#Show second row
# import utils.py to download TestCases from HS, unzip and installation
# for test set, we just fill y values with zeros (they won't be used anyway)
# For demo purposes we can look what our invoice looks like
#### cache during development
# Education KOL pred
# Printing the content of git_log_excerpt.csv
#Remove tokens with punctation.
# The number of times the new_page and treatment don't line up. # adding rows where <group> equals 'treatment' and <landing_page> does not equal 'new_page'
# Median calculated on a Series
#reading in csv files
# accediendo a varias filas por etiqueta
#Check via customer sample
# replace NaN with placeholder value, let's say 99999
#Determine the top five schools that have the lowest percent of students passing math and reading (overall passing rate)
# Checking out statistics for transaction order
# prototype only: save the light curve to disk
#Create predictions and evaluate #print("Number of features used: {}".format(np.sum(clf.coef_ != 0)))
# Make the output Dataframes
#Converts df slice to demographics sql table
# replace NaNs with 0
# Find all the tweets mentioning "SANTOS"
#import pattern
#Setting the date time as the index allows time slicing
# distribution plot without the KDE
# your code here # every 8th business end of month
# Convert df to html
# De paso recolectemos aquellas columnas que tengan o indiquen un formato  "date" # Estas las podriamos utilizar de ser necesario en la importacion.
# df_data=pd.DataFrame({'time':(times+utcoffset).value,'chips':target.values()})
# how outliers many below 1.625?
#Shuffle the dataset for using  the dataset  #shuffled = scratch #shuffled = shuffled.sample(frac=1).reset_index(drop=True)
# remove not useful variables
# Aggregate for a DataFrame. #
# Now lets encode the gender in a numpy array Y
# creating attend csv
# create list of columns to lag
# NA imputation
#Checking spread of created vars
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
# arquivo com os dados da planilha excel
# open the model
# get 8-16 week forecast new patients # keep only date in index, drop time
# send data to PostgreSQL 
#df_geo_insta.to_csv('instagram.csv')
#no aporta nada, solo tiene ids
#initial data 
# above upper quartile
# Reloading the used classifier
# Remove stop words from 'words'
# plotly does not work with "dates" in strict sense, therefore we need to chage it again into a string
# Example
# dataframe remains # view data descriptions
#print the final parameters
# reindex
# Give the chart a title, x label, and y label
# List comprehension for reddit total comments of post
# ...and as in the case of position-based indexing, using # slices or lists as col indexes results in dataframes
    '''$     Load in pickle for news data over selected period.$     '''$
#Show first row
# select rows using .loc
# highest
# loading data into pandas for inspection
#extract individual genres from list of genre, turn into dummy variables, drop duplicates
#inspect measurement table
#Read Excel file  #https://stackoverflow.com/questions/32591466/python-pandas-how-to-specify-data-types-when-reading-an-excel-file
# train again with the best rank 
# Inspect maximum number of comments in the recent scrape 
# Print all of the classes mapped to the Base
#Design a query to find the most active stations.
#import dataset
# write your code here
# A:
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
#groups the dataframe by the result list
# What kind of injuries are we looking at?
# 136 tweet ids are not exist in the tweet_archive_table
# determining the last sensible commit timestamp
#lets plot the time series
# data_air_visit_data['air_store_id'].drop_duplicates().count() # data_hpg_reserve.info
####STEP 1 # Dependencies # URL of page to be scraped
# Round
#Check duplicated rows by contractor_number since contactor_number is unique per contractor per jurisdiction
##Check the last 2 tweets in the dataset
# Cargamos hoja de calculo en un dataframe
# print out details of each variable
# class look up and as it is a frikin descriptor # then call it
# print min and max date
# filtering out wrong timestamps
# set the index of all_cards to be the name column, so we can search cards by name
# save the hashtags to disk
#random forrest classifier
# The correlations are the following
# np.mean() can be used to calculate the proportion of converted users
# example of a cross-listed dealer
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# Check if there are any NaN values in the Train DF
#re - upload complete file (processed in excel)
"""$ load data$ """$
#create a column with the result of the analysis:
# the length of the review
# remove colums with all na #psy_hx.columns
# Importar el modulo data del paquete pandas_datareader. La comunidad lo importa con el nombre de web
#flatfile generation
# Group df by source to get average polarity scores
# get ndarrays columns from the dataset we just uploaded
# split features and target
# plot the closing price of GOOG
#_source contains plenty of other fields we'd like to examine on their own- so can do the same thing to expand that json, this time into index
#plot_user_popularity(very_pop_df,day_list)
# Instantiate a Cell # Register bounding Surfaces with the Cell # Fill the Cell with the Material
# Extract title text
# read in data
# find the relative image url # img_url_rel = image_soup.select_one('figure.lede a img') # img_url_rel
# A:
# we now plot on the KaplanMeierFitter itself to plot both the KM estimate and its confidence intervals:
# Create a one-dimensional NumPy array from a range # arange returns evenly spaced values within a given interval, given a starting point (included) # and stopping point (excluded), and an optional step size and data type.
## Read the source html code ## Close the connection 
# create a counter to look at freqency of insert_id # filter list to only include ids that have a count greater than 1 # how many insert_ids are duplicated?
# Unique operator values in stops
# change appointmentduration to hours
# calculating number of commits # calculating number of authors # printing out the results
# Save file to csv
# market cap of fund using W matrix
# Check for missing values?
# how many values less than 6 in each row?
# AUROC Score
#show behaviour during sepcific time window #pdf.loc['2016-1-1':'2016-3-31',top_allocs[:10].columns].plot()
# Check how many tweets creatd every minute during the data collection period # At this point I just want to check the time trend of the tweets which can be done without time-zone conversion.
# Update emoji dict. Eventually unicode-escape this.
#Boston
#This puts data in order the frames list  #All nucleated data is shown then partial nuc then outside-out...as continues inside brackets
# Use tally arithmetic to ensure that the absorption- and scattering-to-total MGXS ratios sum to unity # The sum ratio is a derived tally which can generate Pandas DataFrames for inspection
# 3. Create a new Data Frame called uni containing only rows from free_data which indicate that  # the person attended university or graduate school. Print the value counts for each country.
# fit validation model
# these are the card layouts for "typical" Magic cards - the rest are the layouts we need to remove # the outer lambda defines an indexing by location to apply to each element in the cards column, and the # inner map/lambda defines that indexing as one that removes the given layouts/types
# Extract NoData value and store into dictionary
# Or one specific month
# Crate an instance of the d2ix post process class: # Post process for a specific scenario: model, scen, version
# Group and drop columns no longer meaningful
# Notes:  #      - x-axis is the index. #      - Must find a better way to represent these figures.
# Attach Measurements table to the database
# just short the gene dataframe by length and return the first few values
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# df=pd.read_csv("/home/midhu/DataScience/nlp/keras-quora-question-pairs/dataset/quora_test.csv")
# df.ix['600848']
#### dumping dict of data frame to pickle file
# Set the index to 'time' 
# Create BeautifulSoup object; parse with 'html.parser'
# responses
# List the stations and the counts in descending order.
# Save the query results as a Pandas DataFrame and set the index to the date column
#export df to csv
# Verify that an address is valid (i.e. in Google's system)
# We remove the store 3 row # we display the modified DataFrame
#number early subscribers
# dateutil
# view the start of the file just saved
#model.most_similar('yea.r.01',  topn=15)
### Number 1
# Sill have duplicate data, also it created extra columns
# Calculate the date 1 year ago from today
# 3.Calculate what the highest opening prices were for the stock in this period
# Design a query to calculate the total number of stations.
## Additional: look at the distribution of daily trading volume
#show head of INQ2016 same for 2017,2018
# model from previous logistic regression
# Topic 6 is a strong contender for 'eating'
#Importing the data sets
# Reflect Database into ORM classes
# Create logit_countries object # Fit
# replace vendor id with an equivalent hash function
#renaming second column 
# flight7 = flight7.na.drop()
##### ignore
#total4
# finds tobs for most active station
# Create a box plot.
# Dados de BO Roubo de Celular
# resample to weekly data
# Define a dummy model for a benchmark that predicts the majority class every time
# Retrieve Latest Date
#load data into dataframe
# merge with main df
# Display the client version number.
# either it opend at high are closed at low
#json.dump(fp, youtube_urls, separators=(',', ':'))
#USvideos['comment_intensity'] = USvideos['comment_count'] / USvideos['views']  #USvideos['thumbs'] = (USvideos['likes'] + USvideos['dislikes']) 
# find the non-numeric feature columns
#Convert datetime to an actual datetime object
# We use the `pd` alias to tell Python that we want to use a `pandas` function
#Export the data in the DataFrame into a CSV file
# mydata['new'] = dc2015['created_at'].dt.hour
# Save the created DataFrames
# Load raw hashrate data over the last two years # Downloaded from https://blockchain.info/charts/hash-rate
# if you want more fields brought through to the final final just add more lines here
#'Plano': '488da0de4c92ac8e'
#here is the data:
# the cost is the data gets copied
#So we do have some duplicated permit numbers. How many duplicates are there?
# We'll hold out 10% of our data for validation and leave 90% for training
#What about location since timezone was surprising?
# Remove stop words
# ML modules
# No duplicated tweet id
# apply the function to the dataframe rowwise
# group by combination of 'A' and 'B' for taking sum 
# Getting probs from logprobs #probs_test = softmax(log_preds_test); # trying this custom softmax. SAME RESULT.
# graph api call using requests get method.
## Summarize 
# See "Special Case: Conversion to List" section for more.
#Create TFIDF vectors for modeling
# Use the Base class to reflect the database tables\n",
# media de tmin anual
# cvec_2 top 10 words
#URL for NASA Mars News Site
# Read the csv file we just created into memory in order to downcast # We can pass type_map to pd.read_csv to specify the types we want
# looking at tags of some randomly chosen queries # notice that most cities after 'TO' are incorrectly tagged as VB
#group by blocks #drop extra columns
# Export
# create Series with index
# merge 'yc_merged_drop' with the departure zip codes
# find indiviual probabilty recievd new_page
#Get the last 100 tweets from BBC, CBS, CNN, Fox, and New York times.
# Display of first 20 elements from dataframe:
# run the model giving the output the suffix "lumpedTopmodel_docker_develop" and get "results_lumpedTopmodel" object
# Filter on Rural # len(Rural) # Rural
#Glance at utc_offset
# Create a GeoPandas GeoDataFrame from Sites DataFrame
# Printing multiple outputs per cell
# Separate response variables from predictors # Split the training data into training and validation sets for cross-validation
# resample to 1 sec intervals using forward fill
# create a new DataFrame using the time frame above to select rows from the whole DataFrame
# View column headings and index
# Inspect station data
# sort by column BG
#https://radimrehurek.com/gensim/tutorial.html # this makes process visible
#Get rid of 2018, since its not a complete year
# This is perhaps not 100% intuitive - the value given will determine the # number of ticks appearing between each major tick; it has no relation to  # the scale of the actual values being plotted.
## Fit/train the model (x,y need to be matrices)
# convert from MSE to RMSE
# Step 2: Conduct Pivot Table on Data
# try again with a tab separator # some errors occurred with missing values after the 600th row so I keep it small here, 100 rows #bg2 = pd.read_csv('/home/hbada/BGdata/Libre2018-01-03.txt', sep='\t', nrows=100) # pythonanywhere
# Retrieve Latest Date - This way we do not have to 'hardcode the date' later in our code...
# We don't want to join a group that is in a grace period!
# Plot tweet lengths against time:
#Merge every row into one paragraph, then split it by sentences ('.') #Use 'type' to check the data type of globalCitySentences
#split the dataset 
#iii. How much influence to my posts have? #Ans. Get the sum of all the retweeted tweets and group by users   #     The users with most counts is most influencial
# How many stations are available in this dataset?$ stations_df = pd.read_sql("""SELECT * FROM station""", engine.connect())$
#getting the summary of the above model
# Here's a pivot table instead - exact same output as a groupby
# We extract the mean of lenghts:
# import calendar # calendar.day_name['2016-01-13']
#avereage hourly rate
# similar to Matlab's tables, we can set a dataframe's column # as the index. Note that it's not done inline.
# Task 1
# game_winners
#finds date of most recent and least recent tweet
# print(rdc.feature_importances_[113:1000])
# tokenize words 
#Frequency count
#probability of conversion in treatment group
#provided dataset:
# Lets fix the population column real quick
#joined df_groups and df_events together after computing the number of events created over the past 7 sevens by groups
#The return here is the training set of features, testing set of features, #training set of labels, and testing set of labels
# rename column names for readability
# years since joinging
# convert pandas column into matrix to use in ARIMA model
# remove emojis
# Weekday
#Show the rows corresponding to index values 6 thru 10
# Adding ab_page column #The inv column is unnecessary and can be dropped
# Count the number of stations in the Measurement table
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# make new column for year. Then drop all rows from 2018
# drop dry because it is really common #df7.loc[df7['avg_dew_point Created'] == 'dry', 'avg_dew_point Created'] = np.nan
# rename the column names  # MAKE SURE THE RENAMING IS OK AS THE ORDER OR COLUMNS MIGHT CHANGE
# What class passenger was on each deck?
# used later to find coefPaths # used later to find the original location of the path from non one hot
# In our case bigram is enough
# Task 2
# Use tally arithmetic to compute the absorption-to-total MGXS ratio # The absorption-to-total ratio is a derived tally which can generate Pandas DataFrames for inspection
# figure points
# Predict stock prices
# Create the 'DateTime' column in the price2017 DataFrame
#We will start first by uploading json and pandas using the commands below:
#Great. This is what we wanted. Now let's convert the result, a grouped series, back into to a dataframe and rename the last column as "Completed_Permits". 
# Getting predictions for train from our training set
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
# delete canceled appointments from the training ? # TODO: put a boolean in a preprocessing to do so or not
#save most recent tweets
# ! cp -R keras-yolo3/model_data .
# Use Pandas to calcualte the summary statistics for the precipitation data
# Create a dataframe that just consists of the songs that make it to number one in at least one region
# Export to csv
# Test our trained SVM classifier with our test data
#Tweepy Cursor handles pagination .. 
# Read OGLE data table
# Getting the values and plotting it # plt.scatter(f1, f2, c='black', s=7)
# We can also delete elements using the drop method
# subset to included participants 
# combined_df5 merged all uncommon values into a single 'other' var; this is an alternative strategy
#y_train.head()
# Pivot the DataFrame
#set up geometry field for delay_geo.. right now, there are only lat/longs
# mean() along each row (axis=1)
# cannot validate when a values type doesn't match the provided type_map
# count by event
## Accuracy is the proportion of true positives and true negatives vs false positives and false negatives. # In your confusin matrix, it's top-left + bottom-right / total.
# beginning & end of the data collection in Pacific time
#firebase = firebase.FirebaseApplication('', None)
### Genrate mapping of numerical values to Grape names
# read in the csv data into a pandas data frame and set the date as the index # execute the describe() function and transpose the output so that it doesn't overflow the width of the screen
# Merge the overall dataframe with the adj close start of year dataframe for YTD tracking of tickers. # Should not need to do the outer join; # , how='outer'
# stacking predicted probabilities by mean
# New ARR Closed Won by Industry and Quarter
# Reset index
# Convert sentiments to DataFrame
# Identify game state # Call it a win for away if away has same or higher win percentage
#Example 7: Import File from URL
#Trading volume occurs at [6] #Using the statistics library
# Merge does not care about the index # As in this example even though the  # index were different, it created only 4 rows
# pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Followers','Following','TweetCount'])
# groupby in conjunction with aggregation functions like mean: # fast way to summarize data; a hierarchical index is by default  # generated
# import Gaussian Naive Bayes
# Create an engine using the 'hawaii.sqlite' database
#So it looks like 1014-885=129 of the original images were deleted in the deduplication, I think...
#Show first 5 tweets
#overlay the points of delay with a map of Chicago
# total number of datapoints
# TODO: overall validation score in one number.
#ind_result
# Set up logistic regression # Calculate results
# Calculate a set of basic statistics.
# calculate a one day offset from 2014-8-29
## YOUR CODE HERE
# id is just a identifier, so generate features of data
#identifying unique instances where old_page does not line up with control
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
#Save current featured image of mars in var featured_img_url
# Convert the pd df to HTML table and clean up. 
#specifying the huerestic and the maxlen yourself makes this run faster the utility doesn't have to calculate these for you.
# Initialize a 2-group MGXS Library for OpenMOC
# to get the last 12 months of data, last date - 365
# 1. Collect data from FSE for AFX_X for the whole year 2017
#Drop the water_year2 column, now that the demonstration is over...
#importing the libraries
#create arima model
# - GICS
#Grabs the last date entry in the data table
# Urban cities ride totals
# Now to import an example I made
# top 5 breweries by total beers drank
#clean up the raw data 
# ANSWER TASK G CODE FINAL
q ='''SELECT nameLast as last_name, nameFirst as firstName, birthYear as birth_year  $     from people where $     nameLast='Williams' and birthYear > '1920';'''$
# contamos cuantos elementos tenemos con valor
# We get descriptive statistics on our stock data
# Veamos algunas muestras
# Number of positive target values
# City
# 6. What was the average daily trading volume during this year?
# its reserved for columns
# df.head()
# find appropriate contract at the moment of the call
#gives all the row as a ndarray
#determine which factors have the gratest impact on trip duration by creating a coefficient matrix and calling the #tripduration column and put it in descending order
# Score the predictions
# Create a staging bucket required to write staging files # When creating a model from local files
#We check a column to see the auto renewal status of users license
# n_new = new page count
# What are the most active stations? # List the stations and the counts in descending order. #tobs temperature observations
#Check the dimensions of the dataframe: it's BIG with close to 100k records!
#create a function to normalize the data #apply that function to each column of the dataframe
# customer_emails.Email.dropna(inplace=True)
# 10-fold cross-validation with logistic regression
# check Initial condition data in file manager file
# Split the data into features and target label # Visualize skewed continuous features of original data
#run for August users
#show all the geometry levels we can use
# A:
#Like Vs retweets visualization:
# important: 'parse_dates=True' when reading docs with dates!
# search vs. booking
# Read the records from test data
# Set up Twitter workspace
# converting to date to pd date time
# Use the session to query Demographics table and display the first 5 locations ### BEGIN SOLUTION ### END SOLUTION
# Import studies text file
# load the data 
# worst 5 breweries - expanded to ignore the "garbage beer" breweries at the very bottom
#I did not know that the ".values" is what outputs #the values in a numpy form !
#sorting by an axis
# Create a mask True/False of which stopids are shared by both dataframes # trips.loc[trips['tripid'] == 4093258]
# check if any values are present
# NASA Mars News - title
#trainDataVecs.shape #testDataVecs.shape #np.any(np.isnan(trainDataVecs))
# Load the President Trump's tweets
# posts[posts['PostTypeId'] == 2].groupby('CreationDate')['Body'].count().plot(alpha = 0.3, color = 'r') # posts[posts['PostTypeId'] == 1].groupby('CreationDate')['Body'].count().plot(alpha = 0.3, color = 'g')
# Select only the date and prcp values.
#Size of rc_2018_02.csv is 20 GB
# Segun se indica en DataDictionaryBuildingPermit.csv permit number es del tipo number pero vemos que lo interpreta como object # De la simple revision se puede observar casos donde el permit number es MXXXXXX y parece estar relacionado con permit_creation_Date
## Initial Apple Negs
# print min & max dates for each datetime column
#create a new label call HL_PCt which is percent volatility and add feture
# let's check the best estimator
# Come back the next day skip last 2 blocks and resume here
# converting date to datetime
# create a new DataFrame # drop columns we don't plan to use to reduce the overall DataFrame size
# Show all chars that are predicted to belong to the amount
#check how many values are missing from the data
# URL of page to be scraped # Retrieve page with the requests module # Create BeautifulSoup object; parse with 'lxml'
# drop the 1st row (which is the column names): 
# And this is a plot of these:
# load data description and see samples
#read in the group file
# Lenghts along time:
# remove stop and other words, links
# Observing the numeric details...
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
#'San Diego': 'a592bd6ceb1319f7'
# how many unique authors do we have NOW?
#Rearranging/ Renaming data for graph
# the most popular words which might contain useful information in the reviews # apperantly we need to update the stopwords list # here I update 'aa' in the stopwords list
# extract data values from datatable # add column names # view first couple rows
#to see what the visititems methods does, type ? at the end:
filter_special_characters('Hi//)(&(/%( \n\n\t)))""""""!!!.')
#statistics_table.to_csv('compiled_data/statistics_table', index=False)
#### dumping dict of data frame to pickle file
#Plot the min, avg, and max temperature from your previous query as a bar chart. #Use the average temperature as the bar height. #Use the peak-to-peak (tmax-tmin) value as the y error bar (yerr).
#Outputing dataframe as excel sheet #writer = pd.ExcelWriter('TITLE.xlsx') #dataframe.to_excel(writer, 'Sheet#)
# A dataframe is an RDD of rows plus information on the schema. # performing **collect()* on either the RDD or the DataFrame gives the same result.
# Create the 'type' column
# your code here
# read-in created csv file
### Use Sklearn TfidfVectorizer to convert words into vectors
#Indianapolis': '018929347840059e'
# drop scratched
# Export CSV
# Gives us general information about the porfolio
# monthly data
# describe num_words field
#Example1:
# defind observation data
# Fit the model
#Boxplot functions
# Import BeautifulSoup into your workspace # Initialize the BeautifulSoup object on a single movie review     
# Here we are adding a "date" column that can be used for grouping operations
# number of labels
#Which station has the highest number of observations?
# Arrays for Bar Charts
# merge with QUIDS
#final.loc[np.argmin(np.abs((final['RA0']-target15[0]))+np.abs((final['DEC0']-target15[1])))]
# NASA Mars News - paragraph
# df_lib_con.title = df_lib_con.title.str.replace('\d+', '')
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# fraction of volume for every coin
# Make a new dataframe with the tag lists expanded into columns of Series.
# check your resources id and copy it, then post simulation results of BallBerry back to HS
# Apply np.cumsum() to each column 
# ...and plot it against time
#get the sum of a column
# Change directory # Create a list with all the files
#quandl.get("NASDAQOMX/COMP", authtoken="yEFb5f6a7oQL91qzEsvg", start_date="2003-01-20")
# nan = 0
#list of the 100 major U.S. cities taken from this Wikipedia page:   https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population
# Get station count, has been checked with measurement station count
# Using logistic regression because it outputs values between 0 and 1
#change the type of columns to integer
# Reset index
# Compute thermal flux utilization factor using tally arithmetic
## convert the variable to numerical values with LabelEncoder() funstion
# 
# ciscid4
# how many columns and rows do we have in data?
#Create a box and whiskers plot of our MeanFlow_cms values, broken by status
# Display the unique values of column L2
# loading the data # new_pw(data)
# write the excel data to a JSON file
# deployment
# at first I didn't realize this produced the lunch data I wanted, so... # I tried to find another way to extract lunch data # a long story follows but it worked too
# create pySUMMA Plotting Object #Val_eddyFlux = Plotting(hs_path + '/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
#'Madison': 'e67427d9b4126602'
# Print model.summary.
#r= average_daily_return(data,portfolio=True) #print(r)
#Converting it to Pandas df for better view
# Create an object to transform the data to fit minmax processor
#'San Francisco': '5a110d312052166f'
# Install the boto library.
#'Tampa': 'dc62519fda13b4ec'
# reset the index to the date
#Joined(train,test)-googletrend for germany (checking if there was any unmatched value in right_t) ##Note: Level of googletrend table for germany is "Year-Week"
#sample 1000 events at random
# Query all tobs values # Convert tuples into list
# word2vec expects a list of lists. # Using punkt tokenizer for better splitting of a paragraph into sentences. #nltk.download('popular')
# Creating the authentication object # Setting your access token and secret # Creating the API object while passing in auth information
#equal footing
# future = m.make_future_dataframe(periods=52*3, freq='w') # future_temp = np.random.uniform(df.TempC.min(), df.TempC.max(), size=future.shape) # future['TempC'] = future_temp
#added the year 2018 in sublime and excel. created 2 separate files because the date was different format. Next I'll convert the dates to datetimeformat and then merge the to datarames back together
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that  # the date format is YYYY-MM-DD).
# RE.FINDITER
# this is an inner join by default
# MAC
#Create predictions and evaluate
# The ~same simple calculation in Tensorflow
# How many stations are available in this dataset?
# minimum age
# Pull the data from remote location: here is my github "Data" folder # Note: if you read the http files than read the file content and not the html because you'll get issue and not knowing where comes from. # docs source: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
# vemos los tipos de datos de cada columna ( los que puso pandas por defecto )
# to make plots appear in the notebook
#Show rows 100 thru 105
# Ignore me for now
# Check for null values to confirm drop
# Probability an individual recieved new page
# Converting your dataframe to a matrix # Displaying rows 1-10 and all columns. Note: rows indices start at 0.
# cisuabd4
# Requirement #2: Add your code here
# Different entries for each index
# grid = sns.FacetGrid(train_df, col='Embarked')
# Replace NaN with blank string
## Initially, think of this as similar to a pivot in excel - but more flexible
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON stru+cture that will be returned
# Fitting a model for Country with US as baseline
# We print percentages:
# Construct and display pivot table
#plot histogram of the 'Amazon Customer' # these customers never update images of the products.
#Plot Histogram of residuals
################################################## # Load transaction  ##################################################
# Example
# Look for any remaining missing values per column
# determining the first real commit timestamp
# Instantiate the count vectorizer with an NGram Range from 1 to 3 and english for stop words. # Fit the text and transform it into a vector. This will return a sparse matrix.
# get shape of Series
# load workspace configuratio from ./aml_config/config.json file
#Converting date time to numeric for calculation 
#query data 
#cek print
#startdate = "2018-01-02"
# create from dictionary where keys are column names, values are Series
# Let's try this out with "device model" first - only about 7k values
#Prettify indents the parsed page in a 'pretty' and readable format
# find the relative image url
#The date format is correct
# 'itineraries' holds a lot more information, let's start with how many itineraries were returned # and list what keys exit for the first itinerary
# The amount of unique user-entries:
# Test score is better than the train score - model generalizes well 
# printing first five rows of the data frame #df.head()  # 5 is the default value. we can ask for any number as shown below
#df_series.index = pd.DatetimeIndex(train_frame.index, freq="N")
# general information about the file
#Chesapeake
# Load total market cap data
# No need to add ab_page, as new_page will always have treatment, we'll end up having singular matrix
#Create InstanceType column from Usage Type col
#table.div(table.sum(1), axis=0).plot(kind='bar', stacked=True)
# Transform all words to lower case
# X will be a pandas dataframe of all columns except meantempm, feutures
# End of time estimation for the notebook.
#quandl database query through API
# A:
# We'll explore the English to Spanish file to get a feel for it
# We display the updated dataframe with the new column:
### export to chinese_vessels_CLAV.csv file
# Education KOL
# count the frequency of the types of sub-gene elements
#All formatting is the same so just cut the wordings #RUN ONCE ONLY
# before merging, both zip columns need to be the same datatype
#Remove unnecessary columns
# Array of 4 features # Corresponding array of 4 labels
#print(scaled_rip)
# Printing the content of git_log_excerpt.csv
# Access local file
# tweet ID, retweet count, and favorite count
# A tweet with just two characters? 
#Laredo': '4fd63188b772fc62'
# group by names  # show the replicated names
# setting index to date
#festivals['Name']='' #festivals['latitude']='' #festivals['longitude']=''
# Rejoin this set of ids into the pax_raw dataframe
#print(highlight(json.dumps(jsummaries, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter()))  # remove the comment from the beginning of the line above to see the full results
#The new contractor_bus_name is correctly created.
#Houston': '1c69a67ad480e1b1'
# Import curren stops table into dataframe
#Sort our recommendations in descending order #Just a peek at the values
#Making a copy of the dictionary and showing all the possible campaign stage names options
#Save
# Vectorize the articles into a matrix # predict whether or not economically relevant based on previously fit model
# one time if for converison of list
# Predict the rating of a known B class article.
# Clean up the merge results
# This will apply the removal of punctuation and stopwords to a string.$ def text_stripper(string):$     '''Takes a string and removes punctuation, capitalisation and stopwords.'''$
#6  When I want to replace my values with NaN
# send request # print status
# accediendo a una fila por etiqueta y obteniendo un dataframe
# View unique causes of death
# loan_stats["revol_util"].describe()
# default is freq='B' for bdate_range
# Save a reference to the stations table as `Station`
# finds station activity and sorts from most active to least active.
#get station count, has been checked with measurement station count
#Drop all of the unnecessary columns for thinking about reliance on conspiracy
# Create the Dataset object # Map features and labels with the parse function
#mentions[0].text
# Show datatypes
# subset for only amount
# get 5000 random ids
#merge with itself? or drop -- are these aliases or distinct babies
#zt = r1_test[r1_test == False].index.tolist()
## How many tables are there in the database
## collect oppening prices in a list
#Select the flow data and confidence values for the 100th to 110th records
# Same for values in 'E' column 
# For finding all the paragraph tags
# Aggregate for a Series. #
# resetting index to eliminate user id's
#stopword_list.remove("ich")    #Remove "ich" from the list.
# The index of our data frame is the first column (how the data is organized)
# longest_date_interval.columns = longest_date_interval.columns.droplevel(level=1) # longest_date_interval = longest_date_interval # .remove_unused_levels().droplevel(level=1)
# Convert datatype of created_at column for better manipulation
# make the date series into a dataframe with the key 
# Information about the independent factor x.
# No missing values
# use read_table with sep=',' to read a csv
# For finding all the texts enclosed by links
#df2
# start_date = input('Vacation Start Date: Year-Date-Month format ') # end_date = input('Vacation End Date: Year-Date-Month format ')
#Merge dates of interest with the Landsat scenes to match dates
# Example on how to access and edit parameters manually if neccessary
## Retrieve all of our original tweet textx, tweet id, user who tweeted, and tweet hashtags. Ignore _id which is  ## MongoDb's id of each record, ie. a 'primary key'.
# Print the street name
# create lookup key in crime dataset
# analyze validtation between BallBerry simulation and observation data.
#Plot using Pandas
#Distinct
# analyze validtation between BallBerry simulation and observation data.
## convert disctionary to pandas Data Frame for easier manipulation
# Hacemos un display del dataframe:
# Create time series for the data:
# bad request use Response.raise_for_status() to track
# analyze validtation between BallBerry simulation and observation data.
# use pandas to_datetime() function to convert string column to datetime
#For example, if we drop the NaN points for this release, can we genearate a quality indicator for all the estimates?
##### the index which was created in the dataframe
# imports
#del df.index.name
#Create a series with 100 random numbers
# Export CSV
# get the index of the high values in the data # get the index of the low values in the data # get the list of change in values of the stock (exclude entries that has either empty high values or empty low values)
#Compute accuracy on our training set
# Construct all tallies needed for the multi-group cross section library
#truncate format to Year-month-day
# This is its own cell because it takes a while to load this thing # takes a little bit. increase limit at own risk. # model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  
#reindex the table
# with start- or end-date * periods:
# define user ratings
# create a dataframe
#Display the dataframe to view the output
#create arima model
# Articles are contained in a <div class="article--container"> element.
# how many unique authors do we have?
# If you prefer sweets... setting "axis=1" tells the function to operate on columns. # "axis=0" is rows.
# find historical data for 2011
#sns.lmplot(data=aa,x='weekofyear',y='message',aspect=2,scatter=True,fit_reg=False,markers='x')
# This means that I can use series-methods like .head():
# load model if saved during a previous session
#'St. Louis': '0570f015c264cbd9',
# Display first five lines of combined DataFrame
# cleaning the vote column into two columns 'vote' and the 'ballot_type' used
# testing if my functions work #they do!
# df.groupby([df.created_at.dt.month,'product_version']).count()['Id'].reset_index(1)
# Read in potential data model during initialization
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Sort by various ways
# joined_hist.info()
# Check for null values
# Check everything  has worked
# Glance at r_close
# Merge with library_df
# set index as the date.  #precip_data_df.reset_index(inplace=True,drop=True)
# dict get # returns value if key is in dict, otherwise returns a value of your choice
# Loading in the pandas module. # Reading in the log file # Printing out the first 5 rows
# Set legendary False = 0, True = 1
## Total early-pairings , since 3-01-2018
#over the whole month (includes entire data range) total number of page views 
#Visit the url for JPL's Featured Space Image here.
# apply the tweet extend function # prepared for run # mancano 900 - 1058
#Imports
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#only want tech groups #our category column is now redundant
#check the dataset 
# Break out the single polygon into the individual polygons.  This produces about 7375 total rows.$ sql = """CREATE TABLE unjoined_sideys as SELECT (ST_Dump(geom)).geom AS geom FROM union_sideys;"""$
# Calculate total days covered by the tweet file to determine criteria for ignoring tweets
# Replies of Trump
# Nasa Mars News URL to Scrape
# Your code here # df['pct_over'] =  # df['pct_under'] = 
# write to db
# Find the div that will allow you to retrieve the news paragraph # Use this html, do not go back to the website to look for it # Latest news paragraph
# All numeric data is now int64
# initialize the twitter API object
# first print the Dataset object to see what we've got # close the Dataset.
#true == churned, false == active #bug- will return user who 'canceled' and 'retrialed' same day which is latest in scn timeline
# # CREATE DUMMIES # # Month
# Mars Facts
#we also don't really care about how the members join
# Plot time (x-axis) against number of favorites for each tweet
# OSX Users can run this to open the file in a browser,  # or you can manually find the file and open it in the browser
# Baseline score:
# Performing a one tail test using statsmodel
#reddit_ids, data = load_data('../data/worldnews_2017_1-2017_1_flatcomments.txt', dates_D)
# One last downcasting check...
#eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality, failed to converge # Assign each to an attribute in your network #nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')
# This will actually build the network and determine which nodes are connected to which.  # Until now, only the rules were given and stored.
# Now mash them together into a DataFrame
# convert dictionary to dataframe
##Reduce DataFrame as the transform function is very expensive on the whole dataset
# I could drop all the prior day info, because I saw no autocorrelation, but it just seems... odd.
# model.wv.syn0 consists of a feature vector for each work # with a min word count of 30, a vocab of 6,793 words as created # shape of wv.syn0 should be 6793, 200
#If you input a dictionary, it will parse the key as the column header
#We need to define Required = Y mandatory to run experiment
#we need to add another feature and lable
# Use a colour-blind friendly colormap, "Paired".
# Instantiate a Materials collection # Export to "materials.xml"
# read from google
# identify nulls
#'Riverside': '6ba08e404aed471f',
# Clean up
# Plot ACF of first difference of each series
# view the leaderboard
#Selecting the interesting features #Adding a column containing the target
# non unique userid
#Rename the last two fields
# What about intersection between KBest and elnet only?
    """Take in a vector of input values and return the design matrix associated $     with the basis functions."""$
#use prettify to analyze
# response.status_code, response.url
#Apply text cleaning function to data frame
# testing = pd.read_excel('SHARE-UCSD-export_reformatted.xlsx')
#dfg = dfg.reset_index(drop=True)
# Now we need to grab 1 year back, from the last data available (above).. # ... iE we need to get the last 12 months of data, last date - 365:
# Estimate 'y' for these 'x' values and store them in 'estimates'.
#Get my document sample
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# go to FDIC site
# get artists whose name contains "Aerosmith" # show two examples
# Replace the API_KEY and API_SECRET with your application's key and secret. # Use AppAuthHandler instead of OAuthHandler to get higher limits.
#stop_words
# creating vip reason df
# Explore the action space # Generate some samples from the action space
# You can build a DataFrame from a dictionary:
#query tables to get count of daily report, all temp data is complete for each record, so the count#query t  #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
#2D correlation matrix plots scope vs type vs site
#train_y = np_utils.to_categorical(train_y, 3) #val_y = np_utils.to_categorical(val_y, 3)
#Kansas City': '9a974dfc8efb32a0'
# The column names are taken alphabetically from the keys of the dictionary # The rows are taken from the union of the indices. It will use numerical row indices if there are now labels given. # NaN stands for not a number
#redshift_admin
SQL = """SELECT * FROM github_pull_requests_2"""$
#extract title text
#Assign the column names to dataframe
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Observation counts per station in descending order
# 7. Bar plot visualizing the _overall_ sentiments
#Convert from cfs to cms (1 CFS = 0.028316847 CMS)
# Info method shows the number of NaN records - more than 1400 # PD.read_csv command - parse_dates was used to convert date raw date into datetime format
# separate out target column, rename columns w/ generic title  |  [target = 't']
# URL of page with latest Mars weather tweet to be scraped
# Use the loc method for selection by label
"""Merge LDA output and DF"""$ #Make LDA corpus of our Data #make dense numpy array of the topic proportions for each document
# Convert completed time from UTC to EST
#Chula Vista
# first, plot the observed data # then, plot the least squares line
# select column needed
#housing prices
# calculate time between AppointmentCreated and AppointmentDate
# total time of active projects
# read the contents of the file into a DataFrame
# plots key elements of the universe selected
# prada styled info tweeted by gucci 
# df_2010
#'Lubbock': '3f3f6803f117606d'
# Reading from pickle # read the data as binary data stream
# pie chart aesthetics 
#if 5668 birthdays recorded, why only 5552 scn_age generated...oh, NaT #count/len(SCN_BDAY) users provide their baby's age
# distribution plot of temp_celcius
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# bob's shopping cart
# Performance of the testing model 
# is our date column datetime?
# Convert titles to string type
# converting the timestamp column # summarizing the converted timestamp column
#full_stats = df2.groupby('group') #full_stats.describe()
#applying nupy math functions without losing datastructure
SQL = """$     SELECT category_id FROM film_category WHERE film_id = 2;$ """$
# last 12 months qty received
# Change row & col labels
# set index as datetimeindex
# SpaCy pipeline
# Plot just the Dew Point data
### Step 16: Reproduce the scatter plot with divider of labels ## Provides clear separation of clusters
# TODO : load data 'AMZN.csv'
# Save model.
# TASK G ANSWER CHECK
#Select only one topic per group
#check the loaded file
# Find the probability indiviual control group they converted
# last word category to emoji
#finally 3 hours!
### END SOLUTION
#Autoscaling can increase the number of instances.
# Design a query to calculate the total number of stations.
# plt.xticks(rotation='45')
# check if still missing 
# df.drop_duplicates(subset=['A', 'C'], keep=False) # df.total_idle.value_counts()
# plot means by month:
# PRs opened in the last month
# Make spp folder
#Check Current working directory
# plot number of comments distribution
# total days
# Drop the rows with NAN values 
# list(.zscan_iter(key))
# Configure how graphs will show up in this notebook
#Create data frame with stations and counts
# weird but possible
#raw_data = pd.read_csv("ENTER_YOUR_CSV_FILE_NAME_HERE.csv") # Now simply run the entire Jupyter notebook!
# get the overall sentiment per media and store it in a dataframe
# Generate auth tokens for Colab
#.apply(myutilObj.my_tokenizer)
#!/usr/bin/python ## dd/mm/yyyy format
#Show the slice of rows spanning september 10th thru 15th, 1998
# Past 10 days
# loading data into pandas for inspection
# create a DatetimeIndex using a time zone
# convert the 3 date columns from str to datetime
# Apply the fix_timestamp function to each timestamp row in the gdax_trans DF
# Shuffle the training and validation data
# convert to datetime objects
# let's find the median and mode
#The coulumns are our features currently,and adding new column which will predict by shifting to the specified period
# Okay, let's try a second example.
# The Frequency_score column should have been inferred as a numeric, so it may contain some unwanted non-numeric data
# load json twitter data # Convert to pandas dataframe
# fig.savefig('toma.png', dpi=300)
# save data to CSV
#branch point path length vs radius
#create arima model
# Add stops which were not in oz_stops but were in stops
#https://stackoverflow.com/questions/29722704/fastest-way-to-calculate-average-of-datetime-rows-using-pandas
#the count for new and old
#Check for non null values
# change to out_path directory
# seriously, what's up with this table # for row in table_rows: #     print(row.text)
##   Creating and working with Temp Tables
#Apply this mask to create a view of just records from Jan thru Sept
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Author: Eelco Hoogendoorn
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Create SOUP
# The index values are DatetimeIndex objects. # The names of columns in the data frame
# dependent variable #y = df.rating
# Which GPU am I using?
#First convert mapInfo to a string #see what the split method does
# create a Python list of three feature names # use the list to select a subset of the DataFrame (X) # select the Sales column as the response (y)
#%%timeit ### %timeit seems to be having issues. no time to figure out why.
#full window width
# Simulate conversion rates under null hypothesis
# Using the apply() function, compute the sum and mean for each column in scores.
# drop any potential rows w/o valid Reorder_Interval_Group
# find the countries that have over 1 billion people. (thats kind of a lot)
# download the text
# results 
# Ensure latitude is within bounds (-90 to 90) # Have to do this because grid file has 90.000001
# Spreadsheet
## filtering out the cats
# Likes vs retweets visualization:
# Combine ET for model representation of the lateral flux of liquid water # add label 
# dummying purchase
# get the first index
# Decode the JSON data into a dictionary: json_data # Print each key-value pair in json_data
#let's plot the hour of the post against how many retweets to see the optimal time of tweeting to reach others #it looks like, from this sample, the best time to tweet if you want to reach more people is from 11pm to 1am
## get core and xtra lists into dataframes for merging into eppd
#Group data by year and compute the total flow and count of records #Remove records with fewer than 350 records #Rename the 'sum' column
# y_pred = knn_reg.predict(x_test)
########################## Check Models Folder #################################
#Problem 1
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
#Review the dataframe that has all of the merged data (schools and students)
# Array with Interval index of the weeks
# BTC-EUR price from May 2015
# Calculate difference in p under the null hypothesis
# Apply standardizing function to "Cause of death" column
# default value of size=100
#!pip install -U jupyter #from jupyter_core.paths import jupyter_data_dir jupyter_data_dir()
# For Displaying last six Data points # For Displaying first six Data points #pd.DataFrame(dummy_var["_Source"][Company_Name]['Close']['Forecast'])[:6]
#dfall.tweet_time = dfall.tweet_time.dt.date
# Start the experiment run.
# Time Conversion
#read in the mean rsvp count data collected on Jan 27, 2018
# converting to date to pd date time
# replacing spaces with underscore
#r_clean[::int(len(r_clean)*.1)].sum(axis=1) #hist_alloc[::int(len(r_clean)*.1)].sum(axis=1)
# Store the map_info into a key of the same name
# add a new column # plot model error by bid-ask spread # plot model error against strike, many expirations included
#Determine number of students with a passing reading score (assume that a passing reading score is >69)
#resample transit and cab data to a lower frequency (1 Month) #select by day
# Inspecting tables in the sqlite database
#standardizing the CIK column by padding 0's using zfill function to make it a 10 digit string which is unique for every company
#Finding the correlation
# option 2 (finds unique rows, duplicates columns)
# Initialize the dataframe to hold all the data # Initialize an empty list
# Reflect Database into ORM classes #do some reflection and export our schema
# creating experience csv
# Read dataset csv file downloaded from Kaggle website # Check the basic structure for all features
# from numpy arrays
# What is the average?
# df = pd.read_csv("../2016-17_teamBoxScore.csv")
#http://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.proportions_ztest.html #Help to calculate p_value easily using statsmodelapi #zstat_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old])
#Example 2:
# Wide display
#df_weather.Date = pd.to_datetime(df_weather.Date).dt.date
# drop duplicates and unnecessary column # set `keep='last'` to keep the last duplicate.
# calculating single correlation between two columns is also available, however quite difficult to viz by then
# convert ISO string to datetime
# Viewing the edge_types file
#'Raleigh': '161d2f18e3a0445a'
# add features to test # check wether the join operations where right
# Get the url for the data by following the instruction at https://docs.quandl.com/docs/in-depth-usage.
# query to pull the last 12 months of precipitation data
#Takes like 60 seconds
# Create sentiments data frame
# drop ceil_15min from data
# Now, we are interested in looking closer at "Likes". We can use Pandas describe function (for DataFrames and Series): # (normalize=True) gives percentages of total volume. 
#We have scaled the data inside -1 to 1 to again -1 to 1. May not make much sense in this example, but for other real data we might want to transform and re-transform.
# because pandas is built on numpy, series have numpy functions
#We notice an the max date of 2044-06-01. This date is in the future and seems to be an anamoly.  #Let us check # Bingo!! This is defintely an outlier. It only has one value. Let us drop it
# In the next code lines we will define some variables, what are they types?
# Urban cities average fare
# Summary of results
#ACF and PACF plots:
#we execute the query statement and we get a DataFrame as a result.
# Setup tfidfvectorizer with english stop words # Fit the SOUP data into TfidfVectorizer to get a tfidf matrix
#df_concat_2["message_likes_dummy"] = pd.get_dummies(df_concat_2.message_likes_rel)
#Random Forest Result 
#  Select the last 12 months of precipitation data: # Grabs the last date entry in the data table.   
#Read first row: In Document Nr 0, word nr x appears 1 time.
# Make five reccommendations to user 1059637 who is the first among the top-10 users in term of number of playcounts # construct set of recommended artists
# defind simulation data
# We replace all NaN values with 0
# Requires MSMS installation # See the ssbio wiki for more information: https://github.com/SBRG/ssbio/wiki/Software-Installations
# reading csv file with first column as index  # Here we have dates as the index unlike numbers starting from zero as in previous case
# Checking the datatypes of our submission sample
# import modules:
#how many rows do we have with nan in a given column?
# Fill in the values from this weeks pace
# We change the column label bikes to hats # we display the modified DataFrame
# print bottom three (least consistent)
# Looks like a lot of proper nouns
# check the frequency is BusinessDay
# Convert to numpy array for later operations
# Add two columns A and B.
# Create a feature DataFrame for this week's games.
#https://plot.ly/python/linear-fits/
# CHECKING TOTALS # bands.sum(axis=0).sum()
#df['money_csum_cent'] = 100*df.money_csum/df.groupby(['program_id'])['money_collected'].sum()
# Position: row 1, column 4
#Categorical variables
# Convert release_date column to datetime object
#plt.axis("equal")
# histogram in pandas
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Deleting date again as it's an unnecessary column.  Explaining that new column is the Ticker Start of Year Close.
# pass the url and the season to the passing method in the GameLog class.
# The Dataframe
#change the case of the text in tweets to consistently search for key terms
# This pivot table will index on the Ticker and Acquisition Date, and find the max adjusted close.
#join the dataframes together from drugs we used chembl_id vs name 
#disconnect to the sql database
# Plot all columns as subplots
# file path, mode, and encoding can be seen when file object is called
#dates_list = [datetime.strptime(date, '%Y-%m-%d').date() for date in month] #eth['close'] = eth['close'].astype('float64') 
# Setting up plotting in Jupyter notebooks # plot the data # ... YOUR CODE FOR TASK 8 ...
# We create a column with the result of the analysis:
#append the old and new csv
# define directory where you save SUMMA TestCases
"""$ Use this code to test your classifier after each round of confidence sampling and plot your results$ """$
#sort by nytimes in order to create different color scatter plot
# Group the data by media outlet
#Ah predict already how? I dunno how to put back :X
# Write hdf5 to current directory
# select a strike to plot # get the call options # set the strike as the index so pandas plots nicely
#Outliers
# Sampling the Recommendations provide using the item:desc dictionary we had created earlier
# shift forward by 5 hours
# Neural net predictions
# create a length column on the gene dataframe # calculate basic statistics about length values
# plt.show()
# count number of fire  per weekday in May 2018
# let's compare the three legs of the first itinerary, similarly as we compared the itineraries
# Initialise with a dictionary. #
#We parse the date to have a uniform 
# How many stations are available in this dataset?
#==============================================================================
# df.at['Row', 'Column'] = 10
# current.head()
# # Create a DataFrame containing the average compound score of each media sources's tweeets #df2.head()
#Peek into the data
#del festivals['lat_long']
# Dropping categorical columns so that applymap() can run # Applying functionn
# split features and target
# Clean up zones column.  Reduce to either 'Single' or 'Multi'.
#import json back into jupyter  #print(op_ed_articles['full_text'][2])
#Create a content_short field for Tf-idf vectorization
# Get best partition
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Convert the returned JSON object into a Python dictionary
#Split the DataSet
#Sharpe Ratio #Menor SD
#look at the support vectors
# write data to csv
# days from today until a given day
# save transformed data
#Find the lowest count of movies for a specific rating to get a downsampling size
# Create RSV only dataframe
#Get data for the Haw River (siteNo = '02096960') using the function above
# Use the session to query measurments table and display the first 5 locations
#Add a query to a dataframe #View data
# Save df_TempJams to csv
# selecting just the vendor id column
# Specify where the dataset is saved and the name of the file
# Or count all the entries (not including nulls or empty) in OBJECT_TYPE:
#     print(screen_name)
# Use Pandas to calcualte the summary statistics for the precipitation data
# count the size of the class
# get the index of the high values in the data
#recall: (X_train, y_train), (X_test, y_test) = mnist.load_data()
# Save all Valid Weather Data in CSV file
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#return the last 12 months
# genre_ids is a combination of differents types # We are going to get value that are not numbers
#Let's get rid of None values for the timezone just like in the tutorial
# run a file to set creds as environment variables
# The above query returns a list of tuples from the measurement 'table' object.  We want to import these tuples into a pandas # dataframe which extracts the values from the tuples for input to the dataframe. # Add column labels to the df.
# How many stations are available in this dataset?  Method #1 using a query from the database:
# Summarize capacity of suspect data by data_source # Summarize capacity of suspect data by energy source
# Instantiate a Materials object # Export to "materials.xml"
# finally save to preprocessed folder
# LOADING LIBRARIES WE MIGHT NEED... # # system
# open precip nc
# Train the Naive Bayes model
# Print a tweet
# set SUMMA executable file
#strip spaces #or: turnstiles_df.columns = [column.strip() for column in turnstiles_df.columns]
#Count the number of stations
# Export file as a csv, without the Pandas index, but with the header
# Describe the categorical features
#Export dataframe to csv:
# If python2 is not loaded kill the hypervisor # ! kill -9 -1
# Which features play the biggest role in predicting campaign success:
# Note that a single FITS file might contain different tables in different HDUs # You can load a `fits.HDUList` and check the extension names # Then you can load by name or integer index via the `hdu` option
# `make_pipeline` - credit goes to Harsha
# Printing the content of git_log_excerpt.csv
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Another sample text
# Isolate comment_body
#admissions api connection
## So only retrive the tweet text, id, user info (id and screen name), and hashtags. Not sure if we're going to use them ## all but ... # Expand the cursor and construct the DataFrame
# show first few rows
# df_2008
# demo interpolating the NaN values
# since we no longer need the urlname, let's drop it and free up some clutter
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
#dimension #shape
# Add interaction between the pages and countries
# days
# Import the GEM-PRO class
# common observations in history and train ?
#By Zainab Calcuttawala - May 22, 2017
# create new timestamp column in the bird_data, pass the data form timestamps list and match its index columns 
#table.to_csv("filenamehere.csv") # Write table to CSV
# use BIC to confirm best number of AR components # plot information criteria for different orders
# create the prefetch column
# Load the query results into a DataFrame and set the date column as an index
# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number
# Numeric Columns # scale the columns back to eliminate skew using log function exception - identity column
#vectorizing words #vect = CountVectorizer(stop_words=stop_words, ngram_range=(2,7), max_df=.2, min_df=.0001)
#df_sentiments_means.plot(kind='bar', legend=False)
# Calling r.json() does indeed produce a dictionary as requested
# Removed outliers for visualization purposes
# the cleanest way, in words: take all rows and the column named "A"
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Create x, where x the 'scores' column's values as floats
# Import required libraries
#tweak the 'minimum probability' argument to expand/contract the list
# find all indexs with nan # find all columns that have nan in them and present them to with the meta-data
# Add the figure to a layout # Show the plot. This will create a warning as the figure contains no data.
# Find number of rows with 0 in M_pay_3d column.  # We may put change other NaN values to zero if this number is low.
#x=dataframe['Date']
# With .loc, you can select multiple rows and columns by using lists
# creates a subset of the original DF w/ only certain times represented
# And recommendations:
# How many values are there?
# Tokenize tweet data in the dataframe
# AUC 
#KNN classification
# month time period # freq M :: month
# Change our units to 000s for funsies
#load data into pandas dataframe
# Data type of each column. #
# We can also do a crosstab (not sure how much sense there is in this but...):
# We convert the Date type:
# plot histogram
# logistic regression without tfidf #%%time
# convert date columns to datetime 
#timeStart = datetime.strptime(timeStart, "2015-06-09 00:00:00")
## refresh .Renviron variables
# day with max return 
"""$ Check relative distribution of data features$ """$
# Connect to the database hawaii.sqlite # Import the data from the measurement table into a dataframe
# !cd .. # !curl -O http://www.grouplens.org/system/files/ml-100k.zip # !unzip ml-100k.zip
# Merge the two datasets # The lengths of all 3 datasets should be equal
# Open/create a file to append data
# Load the last statepoint file
#Wrapped with pd.DataFrame() for pretty printing
# remove na values # remove duplicate index values
# ??
# -> report error: Index does not support mutable operations
# Display confusion matrix
# to stop TensorBoard
# check group size
# in fact this is the same as looking at the group by on count_publications columns and count authors
# Modify instance with multiple thermal distresses
# convert to dict
# Write tweets data frame into a CSV file for later use #dfTweets.to_csv("tweets.csv", encoding='utf-8', index=False) # Read tweets from CSV file into a data frame
# finding the timezone of the max retweets (cats) #the max is 18857 retweets
# set up logging for gensim training
# Convert discrete value to indictor feature
# we can devise hierarchical indexes (which by itself just pulls # several columns out as indexes but does not sort or in any other  # way rearrange the rows)
# Train, test split data_woe
# contamos cuantos elementos tenemos sin valor
# msft calls expiring on 2015-01-05
# Extract title text
# one-hot encoding of categorical variables
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
    '''Function creates a new data frame with date as the index and the sentiment score $        as a column.$     '''$
# Resample
# This is the column I was interested in creating.  It should contain the same information that I would get by dummying # all of the lead_mgr values.
# Concatenate first two and last two rows
# Replace NaN values with 0
# ACCURACY of the model
# Author: Stefan van der Walt
#l4exc_l4exc
# subset us lon # subset us lat # print dimensions
# remove 'O' from the labels # labels.remove('O') # labels[:5]
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned # print(r.json())
# Create an object to transform the data to fit minmax processor
# Add chanel column to users table
# Verifying correct tweets from each source
# To check the size of the dataset
# calculate the next month end by # rolling forward from a specific date
# Getting phenotypes for huIDs that have associated genotypes
# a date without time can be represented # by creating a date using a datetime object
# split data into training set and a temporary set using sklearn.model_selection.traing_test_split
#pivot table showing the Age and affair status
# only one file because all the json files were the same {"hello": "world"}
# Get json object with the intraday data and another with  the call's metadata
# new_discover_sale_transaction.groupby(by=['Email', 'Created at'])['Total'].sum()
# creates 1-hot encoded matrix with 1 at the position indicated by betas_argmax
# Import Dependencies
# We print the index and data of Groceries
#additional features from goal, pledge and backers columns #The above field will be used to compute another metric # In backers column, impute 0 with 1 to prevent undefined division.
#pulling artist information
# In jupyter notebooks you can prefice commands with a ! to run shell commands # here we remove any files with the name of the file we are going to download # then download the file
# keep the empty lines for now # if len(line) == 0: # continue
# Take a peak at the data
# Rename the unnamed column to 'time'
# concat dummy/indicator variables to dataframe
# I'm assuming that - unlike weekly - I've never gone a full month without drinking at least one beer
# Merge csvs (3)
# better if the data set included a lot more days
# read csv file
# Suburban cities ride totals
# Check on the duplicates
# Apply the NoData value  # Convert raw reflectance into type float (was integer, see above)
# Histograms can be crated from Pandas
# favorite table # for row in table_rows: #     print(row.text)
# connect to local
# Another way to see relevant information is to use the `describe` feature of DataFrames
#datatype of the columns
# Create a dummy column that is 1 if the row represents an injury  # or a 0 if the row represents a player reactivated.
# For some reason stats.percentileofscore() throws an error if it's not imported right before being called:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #r = requests.get('https://www.quandl.com/?apikey=2WWxsMC5KvrYAngyPH-W.json')
# migrating table info # for row in table_rows: #     print(row.text)
# Build orders based on this threshold
# start the Chrome driver
# Sanity check: sort with commandline
# Clean the dataframe using clean_dataset() function
# make predictions 
#created ticker dataframe (to add column 'ticker' to databreach_2017 dataframe)
# The climate data for Hawaii is provided through two CSV files. Start by using Python  # and Pandas to inspect the content of these files and clean the data.
#Create date columns #Create month_year columns
#Refernce paths for data files
#Set the index to be the bus names
# https://blog.mafr.de/2012/04/15/scikit-learn-feature-extractio/
# check out how many years out it's been since the measurement. Really two groups, 1 year and 6 years
#Search some tweets! Let's look at hashtag #H2P (Hail to Pitt)
# data we'll be adding to a test DB: # single shop example:
#fraq_fund_volume_m.plot() #plt.show()
# Using ARMA model
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Removing the records with number of reactions less than 10 # Total number of reactions includes the likes so they have to be removed
# Cargamos hoja de calculo en un dataframe
# # Perform a query to retrieve the data and precipitation scores # Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date
# Check the result with built in function as cross-validation - passed
# Columnas
# Delete a collection snapshot # To delete all snapshots, use: # collection.delete_snapshots()
# accediendo a varias columnas por le label
#see if wards are in same bounds as delay points
# For getting the table
# convert the text column from the dataframe to a string
# predict fire size
# check it
# save our pca classifier
# we're at a good stage to store the dataset
# for porability 
# input
# you can also' take a dictionary of lists and then pass it into the data frame
# save csv file as megmfurr_tweets
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Tell Bokeh to display plots inside the notebook.
# Optional: Create an output folder to store your predictions
#Sort our recommendations in descending order #Just a peek at the values
# What are the most active stations?
# determine the order of the AR(p) model w/ partial autocorrelation function, alpha=width of CI
# Convert a matrix to a dataframe # Display the top 20 rows
# if the file path does not exist, it will be created
# Choose 1 column and save that image
# read from Kenneth French fama global factors data set
# probability of conversion regardless of page
# The tags field is another compound field.
# Fill in missing topic values with 0
# get station count,
# Add work day to df # # Encode user type # start_df.UserType = start_df.UserType.map({'Member': 1, 'Casual': 2})
#split the strings using the separator "," 
# read in data to dataframe
# Add labels to the x and y axes
# ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration'] # groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show() # flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')).agg(F.mean('price_will_drop_num')).show()
#Analyzing distributions
# tagging a sample sentence (first validation sentence)
# pivot dataframe for plotting
#Now, let's modify the dataframe to show only those rows that have a "complete" in the Current Status column, since we want to include only those permit applications that are marked as "complete" in our analysis. 
# Transmission 2040 [GWh], late sprint
# get_dummies function gives us 1s and 0s
#Extract trading volume from the list of lists, index 6
# The user "Rezeptesammlerin" seems to have many duplicate recipes # How many recipes does she have in total?
# a = 4.05 angstroms (Al fcc lattice constant) # Create cubic box (alpha, beta, gamma angles default to 90)
# output full forecast dataframes to csv
# remove unwanted fields
#Check via customer sample
# Explore state (observation) space
# Cleaning the speech text
#== Sorting by values of a column 
#term_freq_df .head()
# assemble features
# Saving data
# the money each user spends on renting
# Plot in Bar Chart the total number of issues closed every day for every Category
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Remove all rows with NaN birthyear
# Concatenate Coinbase and GDAX BTC wallets
#Which categories are most popular?
#Which Tasker has been shown the least?
# Which tweets received zero likes?
#del df2['building_use']
#create a new column called month of year, and convert the type of data to string variables to prep for dummies
# loading in test data as well as the Sample Submission file
# open r humidity nc
# concat df and coming_next_reason
# join tables - method 1, but hard to do c
#load data #resampling data
#plt.ylabel('No. de Tweets') #ax.yaxis.set_ticks_position('none')
# minimum you need to pass to upload a dataset
# if user doesn't have executable file or executable file don't work on your local computer, use run_option ='docker_develop' #results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'docker_develop')
"""$ """$
# look at the count of values for the 3 categories.
# how significant z score is
# Examine applications here
# by repeated tweet
# Enter code to look at datatypes
# stops_heatmap.save("heatmap.html") # stops_heatmap.render()
# let's visualize our boxplot
#Display the dimensions of the data frame
# train and evaluate the decision tree chunker
#Load the json file #Print the parameters    
#Henderson': '0e2242eb8691df96'
# insert a test post:
# scatter plot: x-cordinate = index, and y-cordinate = Age
#check values substituted
# Identify incomplete rows
# Retrieve data from nasa # Create BeautifulSoup object; parse with 'html.parser'
# write arrow table to a single parquet file, just to test it
# First, import the relevant modules
# show last 2 rows of data frame
# double check that the bus data doesn't have any weird spaces in it. # Any extra spaces in this string will hinder operations of retrieval by key
# append the heading row to price_data and set column names
#El Paso': '6a0a3474d8c5113c'
# extract full monthly grid
# Save twitter data to CSV file
# timestamp with just a time # which adds in the current local date
#### Test ####
# use iPython's functions to print data frames prettier # OR: HTML(result_df.to_html())
# Read downloaded file into a pandas dataframe.  Note there are weird characters requiring the ISO encoding
#gente con mejores estudios busca mas alto nivel
#41% of those who pair at <3m, subscribe <3
# create a trip duration column where the duration is in hours using pandas datetime package
# Load the query results into a Pandas DataFrame
## Fit/train the model (x,y need to be matrices)
# Check the data types for the remaining columns
# specifying the index column
# plot a scatter plot of all errors > 1.0e-4
# remove the unwanted columns # what does our data look like
# Now query to pull the last YEAR of precipitation data
# for our single-sides test, assumed at 95% confidence level: # Here, we take the 95% values as specified in PartII.1
#make ints to simplify
#df['LinkedAccountId'].apply(lambda x: x in acct_dict print(acct_dict[x])) #df['LinkedAccountId'].apply(lambda x:x in acct_dict acct_dict[x])
#### Test ####
# Utilize JSON dumps to generate a pretty-printed json
# Create a pandas dataframe as follows: # Display the first 10 elements of the dataframe
# start_date = input('Vacation Start Date: Year-Date-Month format ') # end_date = input('Vacation End Date: Year-Date-Month format ')
#Export Canadian Tire file to local machine
# create an empty list to store the query results
# City and address columns are correctly filled
# comments = pd.read_csv('seekingalpha_top_comments.csv')
#creating a metric to see number of competitors for a given project #number of participants in a given category, that launched in the same year and quarter and in the same goal bucket #since the above table has all group by columns created as index, converting them into columns
#Split the Data in training and testing dataset
# Filter down to the set with valid licenses #df_valid = pd.read_csv('/home/bmcfee/data/cc_tracks.csv.gz', usecols=[0])['track_id']
# Gather the data collected into a dataframe and print it.
# a function that cleans the text of each tweet by removing removing links and special characters using regex.
#Size of RC_2018-02 is 56 GB
# Need to convert the created_at to a time stamp
# output to .csv file
#Example 7: Import File from URL
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# pairings < 0m / all pairings, since 3-01-2018
#subset by row index
# converting the timestamp column # summarizing the converted timestamp column
# This step can take 5 to 20 minutes # NOTE: Currently, recreate=False is not supported for deploying to ACS clusters
# your code here #Let's briefly look at your result
#My local key:
# defind simulation data
#build query for last 12 months of prcp data #using the last_date datime variable and relativedelta in filter to get last 12 months
#count vectorizer has extracted 38058 words out of the corpus.
# summary results
# RE.SPLIT
# listing all of the collections in our database:
#Find average HP by Brand #Add a count 1 to each record and then reduce to find totals of HP and counts
#Show last row
# Inspect numeric feature summary statistics 
# get all puts at strike price of $80 (first four columns only)
# Put the data into HDFS - adjust path or filename if needed
# unique identification of project
# import data # show top rows
#pernan is a filtration level - scenes with more nans than this per scene are removed
#cek print head - zona waktu as index
# I DON'T KNOW WHY THIS ERROR IS HAPPENING
#df_all_payments = df_all_payments.reset_index(drop=True)
# We display the total amount of money spent in salaries each year
#lastly, let's convert our datetime columns for later use. (We need to tell pandas these are dates)
# find each of the inconsistent rows in this horrid table
# true flag (T/F): positive class # predicted flag (T/F): negative class
#get station count, has been checked with measurement station count
# split features and target
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Time series for retweet:
#  numpy.setdiff1d() returns the sorted, unique values in ar1 that are not in ar2. #genco ids post-GEN from cameras paired post-genesisco which do not correspond to post-genesisco shopify orders
#create a dataframe of a random samplesize of the master dataframe to determine which variables impact the target variable most
# load in api key
# Check every in_response_to_tweet_id only contains one float value 
# Allign tweets in df and add column names and labels
#seaborn settings
# Averaged predictions
# If we plot all 3 graphs together, we get: # Note that the length appears to be a horizontal line because it's scale (max length = 144) in comparison to  
# Split into training and test data: 66% and 33%
# find the max of IMDB column
# String formatting, to be visited later
#List the stations and observation counts in descending order
# Use the "short_id' field as the index # Show the first few rows
#Add MGD columns
# cur.execute("""SELECT distinct pullid,pullquery from public.datapull_id limit 25""")$ # returns = cur.fetchall()
#Step 2 in filtering: Applying the masks using logical combinations
# Create model # Compile model
# only US, CAN and MEX are returned by default
# determining the first real commit timestamp (by Linus)
# creating a matrix after dropping nulls
# Set file names for train and test data
# Find the div that will allow you to retrieve mars weather # Print the results to verify it is correct
# This command should output closing values for our three # stocks from March 22nd to March 30th.
# write out new master json file
#cisuabn14
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
###YOUR CODE HERE###
#active
#Colorado Springs': 'adc95f2911133646
# Read the variation of heat demand from Data.xlsm
# so what just happened # well m calls # same as looking up m.__repr__ and the calling
# Sort the dataframe by date
# To flatten after combined everything. 
#Selecting the input file to be read
# We extract the mean of lenghts:
#Data Frame Summary (its a class based on 'pandas_summary' library)
# Step 9: Convert pivoted into numpy array ## Use fillna(0) to fill in missing data
#Set some global options
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# cisnwe5
# sentiment analysis
#Bin (categorize) the data in the dataframe and return the head.
# calculate all Wednesdays between 2014-06-01 # and 2014-08-31
# proportion of p_diffs greater than the actual difference observed in ab_data.csv is computed as:
# ddf.to_csv('new_ddf.csv', index=False)
# For Research purpose: 
# Store the best model
# Plot of tasks completed each week by energy level # cust = itemTable.groupby(['Energy'], as_index=True) # cust.get_group('Normal-energy')
###Make sure you have run get_weather.ipynb to pull this csv below
# we're hoping to predict, is actually the future price. As such, our features are actually:  # current price, high minus low percent, and the percent  # change volatility. The price that is the label shall be the price at some determined point the future
# get columns by name
# Converting my station_temp_tobs to a dataframe.
# Access a row
# Column >> result #display head again
# load credentials
# checkout cand_id and see if there are duplicates, i.e. if there are differences in how cand_name is formatted
# validate inputs to the RNN
# Cria um dataframe vazio e adiciona os arquivos de cada candidato
#full_image =  #featured_image_url = soup.find('https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16715_hires.jpg') #for image in images:
# Structures data library # Columnar data library
#remove the row that contains a negative value for initiation days
# Find top-10 tweeters:
# Create an array of 1s
#Chandler
# confirm that this is a new array
# load back in the file # get data from loaded in array
#Example 7: Import File from URL
# 6. If we define the "Tasker conversion rate" as the number of times a #    Tasker has been hired, out of the number of times the Tasker has #    been shown, how many Taskers have a conversion rate of 100%
# reuse the created period range as index:
#Strip unwanted newlines to clean up the table.
# set date as datetime
# Categorize Gender Column by Sex # use map method
# Create fuel assembly Lattice
# post simulation results of Jarvis back to HS
# summary of information about the data frame 
# what is the duration of author's publication career?
#coefficient between delay instance and crime instance
# get countries and show the 3 digit code and name # show a subset of the country data
# Setup plot style
#try using dendrogram by parsing texts into words
# the cards column contains the cards of each set in json format, so each set of cards can be # converted from a json object into a DataFrame
# Getting polarith and subjectivity as well as assigning an Sentiment to each of the tweet. 
#client.create_database(dbname)
# opening json file #     pprint(data)
# Determine post age odds ratio 
# To check missing value and any rows in csv file 
# checking shapes before fitting a model
# Predict probability estimates instead of 0/1 class labels
# Run OpenMC!
# Location; use labeled index # Integer location; use numerical index
#1b. Display the first and last name of each actor in a single column in upper case letters. Name the column Actor Name
# Create engine using the `hawaii.sqlite` database file
# sort dates in descending order # calculate daily logarithmic return
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'lxml'
# Specify the input folder and the type of files to load in (glob_string): # The input folder will change depending on which computer/dataset analysing # If analysing data on windows make sure that there is a 'r' before the file name e.g. r'folder/subfolder'
#Show entire description column
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
# https://movie.naver.com/movie/sdb/rank/rmovie.nhn?sel=cur&date=20170806
# State
# the number of reports from the most active station# the nu 
#Convert tobs list to data frame #tobs_values_df.rename(columns={'0':'Temperature (Deg. Fah.)'}, inplace=True)
# Output a csv file for analyze
# Create a Dataframe #tweet_info_pd.to_csv('NewsMood.csv')
# it is a string
#convert html syntax to humanly readable characters
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-04-24&end_date=2018-04-24&api_key=" + API_KEY)
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
#Arlington
#Removing rows with null values
# Check is Data is imbalanced
#import bible text
# map vip to 1's and 0's
#print(highlight(json.dumps(jscores, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter()))  # remove the comment from the beginning of the line above to see the full results # Convert to a data frame and show all products owned by this client
# specify resume path.
#dealing with json and wrangling
#can put two datasets back together, and remove "_source" column (which was expanded into expanded_data)
#Select top 100 values using 'nlargest'
# URL of page to be scraped
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#We need to forecast the stock price - here we need to forecast out the 1 percent of the valu #0.01 says next 1day prediction into the future
#We see some null values in the gender, city bd registerd_via column, we replace those either with mean(numeric) or mode(categorical)
# Categorize time labels
# Create: vader df
# select all the non-numeric columns # get the list of all the non-numeric columns
#save new title in a variable
# ciscij10
#sns.violinplot(data=corr_df, palette="Set3", bw=.2, cut=1, linewidth=1)
# Let's find the highest weighted words
# instantiating weather feature dataframe
# preview the data
# Summarize the scrapped documents from hot.json
# We create a column with the result of the analysis:
# Task C answer check
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# create a column with the result of the analysis: # We display the updated dataframe with the new column:
#take a peek at playlist variable
# Name
#grouped['C'].agg([np.sum, np.mean, np.std]) Quantile_95_disc_times_pay.agg([np.min,np.max,np.sum,quantile[0.1]])
#Return a list of all of the columns in the new dataframe.
# In the following we are just picking up the first link that shows up. 
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# creat a new column in our DataFrame with the sentiment
# read data in from JSON
#List the unique values inthe HydrologicEvent column AND COUNT THE NUMBER OF RECORDS IN EACH
#empty becuse the duplicated row is dropped
# grab the subreddit
# Use SQLAlchemy create_engine to connect to your sqlite database.
#create and display foxnews sentiments dataframe
# split the data into training and test
# Define a dictionary with the filename, path, and dbase type. `echo` suppresses output.
# We can look at the available keys:
#Return a json list of stations from the dataset.
# From the docs: "Max id returns results with an ID less than (older than) or equal to the # specified ID. 
# n_old transactions conversion sample
#UTC is 5 hours earlier than EST. So in the graph below, tweets were peaked around 15:30pm
# Compute fast fission factor factor using tally arithmetic
#'Louisville': '095534ad3107e0e6'
# Create the hawaii sqlite engine created in database_engineering.ipynb file # Declare the base as automap_base # Use the Base class to reflect the database tables
# 'counter' is a standard python package. not unique to DOTCE. # it's a container that keeps track of how many times equivalent values are added
#dropping extra column  #avi_data.drop([avi_data.columns[-1]],axis=1, inplace=True)
# the logistic regression accuracy score
# EXPORT FILE # Export the dataframe into an output CSV file located in the folder "output", placed in the same folder as the code # The file name, using a clever time stamp, will show as MM-DD-YY-Output.csv
#Convert the date column into a datetime
#create a data frame
#Last 5 Rows
# Reading packets from pre-captured file
# create an isntance of the ModifyModel class 
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Number of unique movies in the ratings data file
# Lets get the data for which we need values #Get all rows and only columns 6 through 54
# Calculate basic statistics for y by group within w.
# raw_df = pd.concat([raw_df, pd.DataFrame(np.arange(avg_da).reshape(-1))], axis = 1)
#read the 2 df
# Scrape JPL Mars Space Images - Featured Image # URL of page to be scraped
# Fit only to the training data
#Convert to dictionary/json format
#save model
# Percentage Change Column is created
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#Add a new columns that calculats the money per student ratio.
# Read the csv data file. It is encoded in UTF-8. # File has three columns: moisture (float), part (int), and operator (int).
#accounts for paid subscribers, ie those who did not churn immediately following free month
#Features #Targets
# concat df and coming_next_reason
#Visualize the frequency of #shoolshooting was tweeted
# Remove emojis #df.text = df.text.apply(removeEmoj)
#train.drop('timePeriodStart',1,inplace=True)
# use the same HTTP request... AntiNex will do the rest
#Create predictions and evaluate
#Reading Pickle File 
# Display dataframe types and usage statistics
# Re-fit the model with fewer features
# results are returned as an iterable list
#Review all column names from dataframe once the drop has occured
# What are the most active stations? # List the stations and the counts in descending order.
# Creating lambda function to map datetime transformation to datetime columns
# define the length of period #print 'Accumulated number of tweets: ' + str(accumulated_num) #print 'Frequency of tweets every perid: ' + str(frequency)
#select all rows for a specific column
# Load the second file into a DataFrame: csv2 # Print the head of csv2
# use defaults, let primary key auto-increment, just supply material ID
# what is bins=100?
#Describe dates
# Count the number of stations in the Measurement table
# analyze validtation between BallBerry simulation and observation data.
#df.loc[df.age < 0]
# one way to tokenize:
#Check shape first
# continue training with the loaded model!
# adding prefix VIP_ to column names
# This step is needed to get the date into the index # This is required if you want to easily plot the data
# Train model
# Print the tuned parameters and score
#data_comparision
# get 8-16 week forecast existing patients # keep only date in index, drop time
query = ''' SELECT tweets_info.created_at, tweets_info.id, tweets_info.user_id, tweets_users.name$             FROM tweets_info INNER JOIN tweets_users $             ON (tweets_info.user_id = tweets_users.id); '''$
# Create symbolic link in ~/data/libs to use site-packages SystemML jar as opposed to DSX platform SystemML jar
# Print out AUC, the percentage of the ROC plot that is underneath the curve
# merge with council data #df7 = pd.merge(df6,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE',suffixes=('_created','_closed'))
# These are only hashtags that have value_counts greater than 4
# Calculating  how representative the 2nd df is of the 1st df through the proportion of their count volumes 
#Print the raw comment and output of get_text to compare: #It's supposed to remove tags and markup
# Top 10 words associations  TM (negatives), DC (positives) # To save the full set to file, uncomment the following line # logodds.drop_duplicates().sort_values(by=['count']).to_csv('logsodd.csv')
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure ## Structure of the JSON:
# the `self` function of Engine has an attribute to return the data as a pandas df.
#'Tucson': '013379ee5729a5e6'
# Extract url from text column
#2.Convert the returned JSON object into a Python dictionary.
#final_member_pivot
# another messy  file with mess at the end
# convert crfa_c to ASCII equivalents
# year 2019 and up is clearly wrong
##### Can referencce column name without quotes
### Retain Only the Top 12 most common wine types
# Value of old_page_converted
#print(tmp)
# Delete it from the original series
#1 #data= quandl.get("FSE/AFX_X",start_date="2017-01-01",end_date="2017-12-31") #data
#6875 Outlier, Depth = 675
# dropping columns '[', ']', and ','
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
# tweets
# Must have an HSPF of at least 10 to make list.  For now use 18 as an # upper limit, but later that may not be high enough.
# analysis_df.count() # analysis_df.count # analysis_df['compound'].mean()
## Ideal max_features was found to be 2,000, but this took too long to run
# Print summary statistics for the precipitation data.
# Task 4
# my fire size model doesn't predict any fires over 100; or size class C.  # this is a limitation
# Author: Evgeni Burovski
# Need to run this first, apparently: # http://forums.fast.ai/t/understanding-code-error-expected-more-than-1-value-per-channel-when-training/9257/10
### Step 14: Import Gaussian Mixture model to investigate the PCA plot ## Specify 2 -> for two clusters
#Adding a new column to the dataframes that contain the polarity value.
# The DataFrame is created from the RDD or Rows # Infer schema from the first row, create a DataFrame and print the schema
# scatter_matrix
# Graficar precios de cierre y precios de cierre ajustados
#to confirm join acted as expected and 'missing orders' were placed pre-genesisco
# df=load_df('rolled_filled_elapsed_events_df.pkl')
# Generate new feature in our DataFrame for the label
# Set just like the index for a DataFrame... # ...except we give a list of column names instead of a single string column name
# Now, we're ready to start streaming!  We'll look for recent tweets which use the word "data". # You can pause the display of tweets by interrupting the Python kernel (use the menu bar at the top)
# For binary classification, response should be a factor
# A:
# concatenate dataframes with different columns
#Download the dataset. Takes a while, file is 800MB. Due to a slight complication with jupyter notebooks, #the progress bar is not displayed, so be patient.
#reindex the DataFrame
# drop rows with missing value in specialty column
### Sample of clean descriptions
# msft calls expiring on 2015-01-17
# Design a query to retrieve the last 12 months of precipitation data and plot the results
# Divide each number of each countries columns by it's annual maximum 
# OSX Users can run this to open the file in a browser,  # or you can manually find the file and open it in the browser
#List columns being omitted from saved data
#Display the first 5 rows
# This here is what we want!
# as it can be seen, the star should be in the stop word list
#count of converted users in treatment group
# calculating number of commits # calculating number of authors # printing out the results
# API Information
#creating tweet summary for each news agency
# path length for each branch point
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
#Import Random Forest libraries 
# check some of those features
# Create a Prophet object and fit the data
#add delay_time to dataframe since it was acting funny when  #we tried directly adding it to the dictionary earlier
# Print percentages:
#Calculate vector-average wind direction if not already present
#Return a list of all of the columns in the dataframe as a list
# Show the clean description of the ticket
# Parse the dimension2 (which contains the customer id and name) and split it into two columns
# It seems 1st user will like her profile whereas 2nd one won't.
#Review data in dataframe.
# Remove stop words from "words"
### Create the necessary dummy variables
# lets turn that list into a spreadsheet
#ax = plt.bar(temp_list,frequency,color='b',alpha=.05,align="center") #plt.bar(x_axis, users, color='r', alpha=0.5, align="center")
# There are 2 '68' columns. # So, adding value to original '68' column and dropping 2nd col
# Defenders
#making dataframe from python data dictionary 
# have a peak at data that occurred after 2018-04-01 23:59:59
# Using agency 'METRA' from first feed
# Most of dogs' names are missing # There are some mispelled names in the name column, like 'a', 'the', 'an'
## YOUR CODE HERE
# covert the query results to a pandas dataframe
#Uno los dataframes para hacer clustering de TODO #all_df.info()
# check Basin Parameter info data in file manager file
#Remove rows which contain words from blacklist #Remove Duplicates (don't keep any posts which appear more than once)
# test data predictions # make predictions on validation data
# How many complaints actually contain the words "injured" or "hurt"?
# Double Checking if all of the correct rows were removed - this should come out to be 0
# Simulate conversion rates under null hypothesis
# Set DateTime as index
# caution!
#Next we will read the data in into an array that we call tweets.
#Filter and create a new RDD
# 24 hour for training same hour for validation
# Import countries.csv data
# take subset to develop algorithm with it 
# create a new table which encorporates the necessary values for the bubble chart
# create subjid column by removing videoname # create video name column by removing subjid # change column name to specify video name
# get one column by attribute, return a Series object
# Mapped classes are now created with names by default # matching that of the table name.
# Importing data from the processed 'news' DataFrame.
#### Define #### # Convert id column from a int to a string # #### Code ####
# set Date column to index
# convert crfa_f to numeric value
# Therapists decomposition
# All we are doing is importing libraries that have already been created by the community. Note! That we # have defined np here. np could have been defined under any name. Industry standard is np. Same with plt.
# convert column of dataframe to categorical
# Use IFrame to display pyLDAvis results to prevent pyLDAvis from overriding # other formatting of the Jupyter notebook
# Calculate total stock price change within the date specified
#'Fremont': '30344aecffe6a491',
#df_tte_all['UsageType'].fillna('not applicable',inplace=True)
#now pass that series to the datafram to subset it
# Dropping the Id column from the dataframe # Checking the shape of the dataframe
# remove data that do not have place object
#os.listdir(A1.paths['directory_paths']['indicator_settings'])
# Step 5: Time to review data.py file and review where function went wrong... ## Review the first 24 entries of the fremont.csv file ## This displays the time is 12 hours and not 24, which means our strftime; We need to use I instead of H
# Few tests: This will print the odd word among them 
# read in the iris data # create X (features) and y (response)
#Date
# Print sentiment percentages
# how many rows, columns do we have now?
#show result
# Define URL and get request from Reddit
# Split the label column out from the features dataframe # Sample the indexed DataFrame
# Median trading volume
# Find all 'a' tags (which define hyperlinks): a_tags # Print the URLs to the shell
# Abosulte value # Round # help() # type q to quit
#Import Data from pickle files.
#will login to synapse
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#We use tfidf vectorizer with german stop word list (modified) from below. #df.message_vec.head()
# this dataframe was created with the 'queries.ipynb' script
# Plot toy dataset per feature #sns.pairplot(df, hue='labels');
# Visualize the learned Q-table
# Will go ahead and fill in null values with empty strings so that we can aggregate cleanly
# Combine each stomatal resistance parameterizations # add label 
# the number of reports from the most active station
# Export collected weather data to csv file
# Surveys by day
# Initialize reader object: df_reader # Print two chunks
# Create tf-idf model from corpus # num_nnz is the number of tokens # num_docs here is the number of users which is 10000
# commit changes
# create a column with the result of the analysis # display the updated dataframe with the new column
#Frequency_score is float data type, but should be integer
# 3.2.B OUTPUT/ANSWER
# read csv directly from Yahoo! Finance from a URL
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Logistic Regression model
#print(cv2)
# Extract mean of tweet length
#You should also convert month from object to datatime!
#df['name':50].groupby('name').count()
# drop duplicates
# adding prefix BOUGHT_ to column names
# Loading a pickle file
#Corpus Christi': 'a3d770a00f15bcb1'
#TODO
# gpByDate.plot()
# Logging functionality
# failing to comprehend this migrating table location # for row in table_rows: #     print(row.text)
# Save your regex in punct_re # YOUR CODE HERE # raise NotImplementedError()
#Analyze outputs for new config
# creating month csv
#access_logs_parsed.takeSample(False, 10)
# We create an extractor object: ## https://developer.twitter.com/en/docs/api-reference-index
# Use a session query to find the first row in the database
# LDA Model 
# Get the tables in the Hawaii Database
# Import the model we are using # Instantiate model with 1000 decision trees # Train the model on training data
# Check to see if the above worked.
# Print scores from various models
# Find the div that will allow you to retrieve the latest mars images
#Compute 1000 lbs/day from MGD and TotalN
# Get a list of column names and types # columns
# get indices for rows where the L2 headers are present # these will indicate the beginning of data 
# this data is now sent out to the executors.
# drop all null values
# .logical_negation() flips 0 to 1 (Trues => False)
# import pyLDAvis # import pyLDAvis.gensim # import warnings
#import plotly.tools as tls #tls.set_credentials_file(username='drozen', api_key='GTP8SX2KBqr3loYdTVb6')
#retrieve page with the requests module
#preprocess('DirectRunner',BUCKET, bigquery_dataset)
###YOUR CODE HERE###
## Request page ## See the html for the whole page # https://www.purdueexponent.org/
## EXAMPLE # Plotting DataFrames
# combined_df5 merged all uncommon values into a single 'other' var; this is an alternative strategy
#Show the first 5 rows of that array
# convert the result proxy to a pandas dataframe
# Match Email pattern: 
# save data to csv
# We display the updated dataframe with the new column:
# Example
#== interactive browsing 
# Call method 'conf_matrix' from class "Classifier"
# Read the filtered tweets from the .txt files
# get values
##Create an engine to connect to sqlite database created in previous step
#Removing one negative value in registred via
# Education KOL pred
# Download the dataset # Split the dataset
# could call out to OS and run this but sometimes I work across systems and it is easier to just cut and paste command line #  exiftool -csv="FILE GENERATED ABOVE" /path/from/root/to/your/images
#drop rows where all columns are NaN
# In order of most delays: # Blue (199), Red (149), Green (110), Yellow (80), Orange (66)
# Creaate a new column that is product of both dimensions # print a few rows & columns
# replace 0's, since they cause potential blow-ups
#Store original primary sensor data for later comparrison #Identify if primary sensor is passive or aspirated
# create pandas dataframe for conversion to arrow
# Merge speeches and metadata
# read a few columns of the tweet data
# request data from the API
# load data
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# ! cd /home/ubuntu/s3/flight_1_5/extracted # ! mv /home/ubuntu/s3/flight_1_5/extracted/final_results/*.txt . # -exec runs any command,  {} inserts the filename found, \; marks the end of the exec command.
# the off-season.
#data.dropna(inplace=True)
#Dictionary of Outliers
## Loading the data created
# what are all of the business quarterly end # dates in 2014?
    """$     Return a new SoundCloud Client object.$     """$
# Load the last statepoint file
## create a database (if it doesn't exist)
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Write to CSV
# 6. What was the average daily trading volume during this year?
# load CSV files
# Basic search for '#Australia'
# create a copy
# Precipitation Analysis # - Design a query to retrieve the last 12 months of precipitation data.
#find average by dividing HP total by count total
# Take all vets younger than 38 
# Largest change between any two days
# Make a copy of the game data to work from for EDA
# groupby on the ID. # SO_reference  #https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python
# Create a session
# Step1:  get rsIDs for the variants
# Task 1 : Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD) #Collect data from the Frankfurt Stock Exchange, for the ticker AFX_X for the whole year 2017
#Great. Again, let's do a sanity check one more time. Let's show all rows with missing data:
# here is where I filter only for ones (residential tax class)
# count number of deaths in 2014:
# Strip local time to include date only #itemTable["Date"] = itemTable["Date"].map(parse_full_date) # Categorize energy labels
# Raw table showing all the tasks completed each week this year # gDate_vProject
# Object oriented approach
# Step 3: Plot pivot table data ## Here we see the data doesn't have the two peaks, but instead just one...This is what we need to fix
# Setup 
'''$     3 - Number of SQLs - Sales Sourced vs. Marketing Sourced$ '''$
#Create Twitter basemap specifying map center, zoom level, and using the CartoDB Positron tiles
# now we need to rebuild the prior parameter covariance matrix
# And now load the data. Not needed if it is all run in one piece, but useful  # if the notebook needs to be re-run/debugged in multiple iterations.
#reorganize data set
# should we need to load the model
#plot histogram
# change date to datetime # sentiment_df.head(100)
# m.add_regressor('TempC')
# Confusion Matrix
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
#import statsmodels.api as sm
# costumise dates with time info to midnight convention:
# Use Pandas to calcualte the summary statistics for the precipitation data
#Examine the relationship between score and number of comments 
# Plot the daily normals as an area plot with `stacked=False`
# Use the Base class to reflect the database tables ### BEGIN SOLUTION ### END SOLUTION
#Oh only one database :/ well too bad about database then #But I lazy to drop
# create df
# Merge and simplify the mini lines to the rest of the lines one final time. sql = """CREATE TABLE mthrice as $ SELECT ST_Simplify((ST_Dump(ST_LineMerge(ST_Union(linestring)))).geom,.00001) as linestring FROM mtwice;"""$
### Create the necessary dummy variables for the country
# import utils.py to download TestCases from HS, unzip and installation
# split features and target
#Convert the returned JSON object into a Python dictionary.
#import Twitter data
# Get movies not rated by new user # keep just those not on the ID list # Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies
#Urban cities driver totals
# Calculates how many people DID NOT indicate ageRange
# Country map to plot the transmission arrows on
#Load the saved csv file, reading the site_no column as a string, not a number.
# Take the raw data and filter out a single person just to check if the data looks reasonable.
# Validate the zip codes # Manually fix the zip codes for the 7 stations with 4 digit zip codes by checking gmaps.geocode(station 'station, New York, NY')
# wide to long # view head of final row-wise dataset
#Creates Dummy from post likes at threshold 500.
# Return Apple stock price
##### index, series pairs
# Make columns for seasons and terms
# Pring filtered df for station_distance for : # start station name, end station name,  # start coordinates, end coordinates
# what does the dataframe look like now?
# in order to aggregte data from non-unique index:
# read local csv
# zona waktu as index
#making python dict
# this method works, to extract the hour of each datetime value
# cinema meta-data
# recode school_type
# Find meaningless columns (<=1 categories)
# Initialization
#Clean Text
# to find unique user_id
# Requires SCRATCH installation, replace path_to_scratch with own path to script # See the ssbio wiki for more information: https://github.com/SBRG/ssbio/wiki/Software-Installations
# analyze validtation between Simple resistance simulation and observation data.
#one more filtering criteria:  no missing values.  The last_event column is a datetime object, #and if there were any groups with no past events, this column will be a NaN (missing value).
# Control the default size of figures
# Creating a Series is easy - just pass in a list of values
# And we come full circle! We encode the list we created in  # a json string. We could then provide that over the internet # in our own API!!
# Python's SQLite let's you use either '==' or '=', but I think SQL only allows '=', okay?
### unhide the strings in jupyter notebook #new_df.clean_text[new_df['clean_text']==997055707471003653]
## train-test split
#test = pd.read_csv("test.csv", header=0, sep=",", encoding="latin_1")
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# get specific group
# showing plottings via terminal rather than window popup
# first 5 rows
# Iterate over all cells and cross section types
# builtins.uclresearch_topic = 'GIVENCHY' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
#Import the data set into dataframe
#To get the average per student then you need group the data by student
# The protocol version used is detected automatically, so we do not # have to specify it.
# Rows can be retrieved by location using .iloc
# only consider regular season games
# statistical information related to the data frame
# Test & extract results 
# Extracting CNN news sentiment from the dataframe
# Creating a df of just the nullse
# looking up 'new york' in the airports dataframe # fs refers to 'flightstats code' of the airport
# first column [0] is target (outcome variable) # second column onward [1:] are features  # we keep the feature name in X_header
# print percentages
#2. Running averages - First counting events/flags of interest in bwd and fwd diretions
# get life expectancy at birth for all countries from 1980 to 2014
# Get metadata documents
# download .swc and marker files
# Read Stations file with pandas and inspect the data
# Show GMT to EDT
#Save Test Dataset as pickle for later use # the test data in pkl format
# Display of first 10 elements from dataframe:
# Can also visulize entitie detection given a ticket
# 24 + 4984 = 5008 # found 8 dealers listed with both HYB and STD
#view the header names for files
# are there any values less than zero?
#Now fill index with the previous values
# your code here
# boolean indexing: note that with .iloc we *must* take  # the .values # or iris.iloc[iris.iloc[:,0].values>7.5,4]
# of unique users = len
# histogram of score
# The "doesnt_match" function will try to deduce which word in a set is most dissimilar from the others:
## filtering out the cats
# read the oracle 10k documents 
# you can convert date time index back to period index
# CREATED A NEW COLUMN WITH THE RESULT OF ANALYSIS:
# Using without Request$ from requests_html import HTML$ doc="""<a href='https://httpbin.org'>"""$
# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')
#conn = sqlite3.connect("/tmp/coindesk.sqlite3")
#Assign records collected before 1980 a status of "Before FL" #Assign records collection after 1984 a status of "After FL" #Show a random sample of the dataframe
#Create a new dataframe from the Flow and Confidence columns
# We replace NaN values with the previous value in the row
# the set names form an index, and the columns are the attributes of each set (set name, release date, cards, etc)
# Pure Scorers
# final dataframe
# print the row at the fifth position
# We drop any rows with NaN values
# Be a scatter plot of sentiments of the last **100** tweets sent out by each news organization, ranging from -1.0 to 1.0, where a score of 0 expresses a neutral sentiment, -1 the most negative sentiment possible, and +1 the most positive sentiment possible. # Each plot point will reflect the _compound_ sentiment of a tweet. # Sort each plot point by its relative timestamp.
# Get the average Compound sentiment
# masked["RT"] = [1 if "RT" in ele else 0 for ele in masked["text"]]
#Merging the second train file with train dataframe in the dictionary
# In _theory_ in preview dataproc Spark UI is force disabled but history fills the gap, except history server isn't started by default :(
# fig, ax = plt.subplots(2)
#join elo table with main table
# Print all of the classes mapped to the Base
# Imported chromedriver, had to redownload the latest chromedriver and insert the file into project file path # because the current chrome browser wouldnt work with the earlier version of chromedriver.
#df['age'] = 
# largest basket:
# remove cancelled, suspended and live
# 7. Print the mean for all columns for each age_cat using groupby.
# flight_to_brisbane = flight.where(col("price") > 0 & col("to_city_name") == "brisbane").groupby(flight.search_date_x).count()
#using StringIndexer for cluster graph 
# ???? why this one does not work???
#Create a folder called network.  #Within this folder, we create a folder'recurrent_network' where the nodes and edges in our example will be saved
# Initializing the query structure: transform corpus to LSI space and index it
# Now remove the sample rows from the main dataset
# Export CSV
#Sort INT by Contact_ID and Interaction Name
# Predict
# Prep I-TASSER model folders
#File from which input needs to be taken
# print the R-squared value for the model # R-squared: proportion of variance explained, meaning the proportion of variance in the  # observed data that is explained by the model
# saving model
# accediendo a una columna por el atributo
# Mars Facts URL to Scrape
def lookup_project_git(org, project):$     """Returns the project github for a specific project. Assumes project is git hosted"""$
# Double Check all of the correct rows were removed - this should be 0
# Grabbing the files from the download 
# reorganize df
# scrs < 0m / all scrs . 
# The mean squared error
#rename state_abbrev from state_lookup table to state_code
#Example 5: Specify dot (.) values as missing values
# test datetime object
# Ensure that the Date column is in datetime format to facilitate plotting
#Lets see what Melt does
# Use Inspector to print the column names and types for station
# Apply unix_to_datetime function to every row in the coinbase_btc_eur dataframe
#convert json to dict with .json()
#............................................................................ ##   Advanced Spark : Partitions #............................................................................
#from collections import Counter
# Describe function shows all statistical information at once
# We can also use the "most_similar" function to get insight into the model's word clusters:
# calculating number of commits # calculating number of authors # printing out the results
# array to dataframe # add name
# take a quick look at the DataFrame that returned
# discretizing data also results in categoricals
# see the histogram of the possible fake review.
#This creates a Panda Series object which first calculates the difference between the times, and then the mean of those differences #This takes some time to calculate
# list of words, punctuation and emoticons to get rid of when tokenizing a tweet
# extract lonlat grid # plot
# select fileid with the category filter
# Scrape NASA Mars News # URL of page to be scraped
#Add the MGD column to the dfHaw table
# Some care is needed when removing warnings. But for the final version of this notebook it should be safe.
# today_
#split the dataset by gender
# plt.savefig('Days Between Purchases Chart.png')
# 6.
# Creating a y_train with our click information # From the Kaggle site, we know 'id' is just a record indicator, so we can drop it # along with the "click" column, which is our target
# Our classifier is now trained. Wow that was easy. Now we can test it!
#### ###
# Create a list of lists of the data
# take the first value of each bucket
#'Newark': 'fa3435044b52ecc7'
# Use Pandas to calcualte the summary statistics for the precipitation data
# Utilities
# get mongodb params from environment
# UniProt mapping
# users
# intentionlly set to display the entire data-frame
# top 3 styles, minimum of x beers drank
# Highest & Lowest opening price
# Preview the ratings dataframe
# read the file to test
#read it into a dataframe
# payment distributed
#unique_urls = group_on_field_with_metrics('url') # add domain
# i band
#Cuts instances with more than 10000.
# Random Forest Model 
# David Cameron indices
# few days ago it was 758
# Here are the topics that Gensim found ** BASED ON THE TRAIN DATA**
#Aurora
# user=userid # userid=2605
# exist Net_Value_item_level = 0, delete them
# merge onto weather features
# X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)
# Determine a monthly profile to be used by cities that are not covered. # Because the above data came from PCE, non-covered cities are primarily Urban. # Take the average of the Hub cities that have annual usages > 500 kWh/month
#'Rochester': '2409d5aabed47f79'
#appleNegs.
#run for March users
# We create a Pandas Series that stores a grocery list of just fruits # We display the fruits Pandas Series
# 3.2.C OUTPUT/ANSWER 
#save as csv
# Random Forest Model 
## File created in Google colaboratory so need to download libraries and data on begin 
#this is India meri jaan, yaha twitter pe Likes se jada Retweets hote hai,  #So we will do seperate visualization
# Define X and y
# We first name the file that contains our data of interest
# Set an option so we can display the full tweet text # have a look at a few of the tweets (remember your subsetting)
#### Test ####
#preview output of first element
#'St. Petersburg': '5d231ed8656fcf5a'
# need to get rid of that empty value in score; subbing in an average of two bracketing scores # 0.187218571
# aprovechamos y salvamos este nuevo set # y recargamos la informacion nuevamente
#reset index to create date as a column 
#Every genre is separated by a | so we simply have to call the split function on |
#### Test ####
# commit changes
## Solare Flare 2012-03-07   highest sigma event 
# To call (instantiate) the class
# join all messages by the same candidate
# Renaming the columns to station, name, tobs
# extract data from tweets that match search terms 
# path to saved figures
# We Create a DataFrame that only has selected items for Alice # We display alice_sel_shopping_cart
# check if any values are present
# Use the forest's predict method on the test data
#Tweepy Cursor handles pagination .. 
#== Check if NaN or not  #print(pd.isna(dfx)) <== depreted?? 
# RE.SUB
# How many stations are available in this dataset?
# Ordeno de mayor a menor la temperatura media
# reflect an existing database into a new model # reflect the tables
#this is the plot for the last year
# get the one_way column to show if the search is one way 
# drop tweets which startswith 'RT' --> get only original tweets, without Retweets
# df2 = pd.read_feather('street_seg_id')
# Designed a query to retrieve the last 12 months of temperature observation data (tobs).
# Return descriptive statistics on a single column
#1 reset index so that we do not have issues since we loaded multiple files 
# Read the Dataset
# check shape of DataFrame
# Filter outliers in Max Capacity
# instantiate model object
# fill shipping method names with shipping method ids if there is no shipping method name
# teacher_behavior
# percentage of ratings above 4.0
# summary() function failed to work. Used summary2 instead
# Print all of the classes mapped to the Base by the automap function.
# Design a query to calculate the total number of stations.
# Obtain the source names for reference
#Reassign name to remove the unrecognized symbol '?';
#combine the two months of data
# Check for unique values in each non-id series
# Grouped by date 
#by default, the row index is used for abscissa values, which comes in handy for  # time series data
#'Nashville': '00ab941b685334e3'
# bind parameters to the stmt value, specifying the date_filter to be 10 days \ # before today
# 17. Create a new Data Frame with columns named cat and score from your results in part (16),  # for the column with the largest score and the actual score respectively.
#Save figure
# add features to train_reduced # check wether the join operations where right
# LENGTH AND SENTIMENT
# show the oversampled dataset. Now, we have balanced sample.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#print (c)
#converts to dataframe
# Retire column was infered as a float data type
# get dummies for country and category type
#convert pandas DF to HTML file
### Number 2
#Save News Tweets to csv
# For perspective, guessing the training mean return in the val set results in the following MAE:
# split features and target
#sort by BBC in order to create different color scatter plot
# R squared.
# Timeline for the dataframe
# Create a legend for the chart
# Display of first 10 elements from dataframe:
# .loc can deal with a boolean series
# find historical data for 2010
#change the directory
# Split the Data to avoid Leakage #splitting into training and test sets
# Exporting as csv
#col_labels = ['Date',,'Lang', 'Author','Stars','Title','Review','Reply',]
# -a show attributes, -v verbose
# generate a Gaussian distribution
# Collect the names of tables within the database
# Create Geometry and set root Universe
#Albuquerque
# Lets set a style
# Average polarity by News Source # View Polarities
# open temp nc
# The protocol version used is detected automatically, so we do not # have to specify it.
# write to csv the cluster selection
#veamos los dtypes
# Tag parts of speech (PoS)
# result = {'customerID': avg_interval_between_reorders}
# Retrieve the NuFissionXS object for the fuel cell from the library
# Get data
# Set the index to date and sort by date # See what it looks like
# adding back to df
# Create invalid data
# Get feature names and class labels
# pie chart showing percent of total rides by city type
# a simple logistic benchmark. only knows how often defense shifts and how often batter is shifted against. 
#print head of capa
# run multiple times to see samples # randomly chosen sample IOB tagged queries from training data
## The model was created above: ## It should be noted that they do not accurately specify the time constraints for these models!!!!!
# the best parameters for the logistic regression is:
# TODO: check if all 4X people actually did give more data
# Load pre-calculated stacking fault energy data
# Sort the dataframe by date
# cvec_3 top 10 words
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Suburban cities average fare
# output2 = output.select('label', 'features').rdd
# TASK B ANSWER CHECK
# To find convert rate for p_new
# Compute cross sections on a nuclide-by-nuclide basis
#Select all the non-null comment fields from the table
# Perform a query to retrieve the data and precipitation scores
# A:
# Dataframe of all events created by a user within a 30 day period
#The train stations names need to be cleaned up
# Reduce words to their root form
#Converting Data Into Numpy
#disable warnings
# summary statistics
# Create timeseries from scenario results in preperation to post processing
#tweetdf[pd.isnull(output)]
#del festivals['Name']
# We append store 3 to our store_items DataFrame # We display the modified DataFrame
#estatisticas basicas # print(feedbacks_stress.loc[7, ['incomodo', 'interesse1', 'interesse2', 'num_video_preferido', 'justificativa']])
#https://plot.ly/python/linear-fits/
#example_text_file = 'sometextfile.txt' #stringToWrite = 'A line\nAnother line\nA line with a few unusual symbols \u2421 \u241B \u20A0 \u20A1 \u20A2 \u20A3 \u0D60\n'
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
#Show the first 4 rows. 
#x=dataframe['Date']
# Sample five random ratings
# client.repository.delete(experiment_uid)
# Station and Measurement
# Scrape the 99bitcoins.com website
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#The final recommendation table
# shift so that we're trying to predict tomorrow
# Copy data_df and rename columns
cur.execute("""SELECT created_at, text, location, time_zone, geo, lang from tweet_dump$                order by id DESC limit 10;""")$
# Authorize the repository client
# Returned as a python dictionary# Return 
# Counting the no. commits per year # Listing the first rows
# from h2o.estimators.gbm import H2OGradientBoostingEstimator # hf = h2o.H2OFrame(flight_pd)
# URL of page to be scraped
# import the data and concatenate into a single pandas dataframe
# `hawaii.sqlite` database file created in database_engineering steps
# Prepare Results Dataframe to output to CSV file and for the scatter plot ######media_user_results_df = pd.DataFrame.from_dict(results_list)
# same as above
#3 # the R^2 was lower on the test data compared to the training data
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
## read and print token
# Build models
# Now we'll try "device_ip" - 2MM values in the reduced training set
#fill the missing millesime by year of creation date
#4c. Oh, no! The actor HARPO WILLIAMS was accidentally entered in the actor table as GROUCHO WILLIAMS,  #the name of Harpo's second cousin's husband's yoga teacher. Write a query to fix the record.
# Function to only select the upper case characters from a string
#1: Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD) #2: Convert the returned JSON object into a Python dictionary
# the attributes it have
# Creating the term dictionary of our courpus, where every unique term is assigned an index. # Found the output from models is more close to what tickets mean about when not using bigram phrases
# Set a starting and ending time. #end_t = '2017-12-31 23:59:59'
# check available info
#df['price'].map(normalizePrice)
# parse_dates=True, means we let Panda to "understand" the date format for us
#probability of conversion in cntrol group
#Make sure no missing geometry
# Check data type and missing value for each column to make sure they meet constraint of destination table 
# get a list of the unique data types
# Combine columns into one stage column
# change the name for the convenience
# Use Pandas to import a csv file into a dataframe # Check the dataframe to see how many rows and columns it contains
#taking log of the column 
# extract full monthly grid
#  Let's document the numerical values of (mean, sigma) for the record:
# REMOVE THIS LINE IF YOU ARE GOING TO PUT IN PRODUCTION
# Let's have a look
# unigram chunker 
# set the target site as a variable
# Find total number of stations
# Fill unknown zones with 'unknown'
# If you want to drop the redundant column:
# cost per day
# Create a list of filenames for all the years data 
# The beginning of the first 100 comment items...
# reflect an existing database into a new model # reflect the tables
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# read data and convert to date-time
#Since there are two weather data files, I first merged them into 1 file #Check to see if they have the same shape
# 6.What was the average daily trading volume during this year?
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# 'itos': 'int-to-string'
#format date into dateformat 
#works! returns shit for set add #all plans
# Create BeautifulSoup object; parse with 'html.parser'
#That cut our number of tweets nearly in half!
# generate a list of all order intervals
# dataframe of positive examples
# dataframe with just the most retweeted tweet for each week # dataframe with the number of tweets for each # dataframe with averages
# 7.What was the median daily trading volume during this year?
#first cohort
# TASK D ANSWER CHECK
# We'll also rename some of the columns so that the output is cleaner.
# Stripping From StarClinch from the names. # Now, we only have the names.
# your code here
# Show Figure
#Example 3:
#import stats models to check the R^2 value of the model
# group by user
#Save figure
#huh = soup.find_all('header',class_='gallery_header') #huh
# Create BeautifulSoup object; parse with 'html.parser'
#Show the row with the index matching Jan 1st, 1975
# Rural cities average fare
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Read the Excel files. # The "new" file is the newest, unchanged file from SAP. # The "old" file is the older, modified the previous "new" file.
# look for seasonal patterns using window=52
# execute the info() function
# set max row and random seed
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
# first, let's check the 5 styles that have the most checkins
# Lets check the tokenizing function
#sns.distplot(x)
# Leemos el archivo. Veamos si tenemos algun inconveniente...
# clean up whitespace # clean up spelling
#To shows columns within dataframe
# Drop users who don't have at least 4 days # Keep only seqn for joining purposes
# can sort order using index
# Put in dataframe and view selected rows/columns 
# Remove rows with missing values in column 'driver_id'. The order was not accepted. there was no trip
#check that arrays have the same shape
# preserve
# common words from the reviews
# To use the saved classifier # The above 2 cells can be commeneted once the classsifier has been trained and saved to a file
#for i in range(5): #    print (i, X_vect[i])
# sentiment analysis
# validate this is what we expect, missing values may throw off pandas types
#Execute zhunt with the arguments windowsize, minsize, maxsize and datafile...
# model= model.load_weights("model")
#Cuts time and keeps date.
# And then you can customize these new coulmns using the same method as above. 
# When convinced Raw and Processed data is good, upload to bucket. # Upload raw TMY3 files to bucket.
# Vader
# Load the data from the query into a dataframe
# Copy original features for use in Pipeline
#Describe the set of all sessions where a printer was used #Important features are mean, 50%, and 75%
# Print data from nasa_url to make sure there is data 
# ntree_limits the number of trees in the prediction; defaults to 0 (use all trees)
# Make distance_list into a series # Set to var 'distance2' which will become a new col 'distance' added to df
# get the third and fourth indexes of an Index object
# sample data: note that Series auto-generate index  # histogram by value_counts
# 'startdate': '2017-10-05+19:19:00', # 'enddate': '2017-10-27+13:17:26',
# Store the experiment, and display the experiments uid.
# apply function to get stats
# replace vendor id with an equivalent hash function
# your code here
#Store the final contents into a DataFrame:
# manually add the intercept
# Fix incorrectly geocoded coordinates
# Verifying that our new submission has the correct columns
# perform a search # print the number of items returned
# First, get a list of all of the hemispheres
# Let's create two Indexes for experimentation
# columns in table x
# Shuffle the data set
# clean up nulls
#write the above dataframe, with cleaned up lists for contributors, affiliations, publishers, and tags, to a new excel doc # testing.to_excel('SHARE_cleaned_lists.xlsx', index=False)
#Determine number of students with a passing math score (assume that a passing math score is >69)
# tweets by location
##### concatenate along rows
cur_b.execute('''$     FROM room r INNER JOIN hotel@FIT5148A h ON h.hotel_id=r.hotel_id''')
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# Gather the brkeys with duplicate entries from Inspection_duplicates # Convert to list
#inspect station data
# It seems younger people mostly performed activity.
# Explore our target variable.
# Read the previously-saved Excel file. # List worksheets within the workbook.
# create monthly data #fin_r_monthly = fin_r.loc[fin_r.index.day == 10]
#And let's try this again:
# Pledge and backers # Select the product and average of backers and pledged # Average of th number of backers and amount pledged
# Convert age into hours and inspect head of scrape 
# Evaluate model
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
# write the numeric features back to a table
#file_for_writing = open('life.txt','w',encoding='cp949') #file_for_appending = open('life.txt','a',encoding='cp949')
# export
# print the text of the first result.
# fit the model
# set timeseries data
#Review summary stats for the data set
#Downloading the file to a local folder
# Use len() and unique() to calculate unique user_ids in df2
# top beers
# check shape
#creating new dataset using query function
# Create model # Compile model
# filter data yang berbahasa Indonesia
# define the feature set and response variable # drop the response variable from the dataset # drop the identify index column from progresql
# use 'last', so obtain closing price of the month
# Data info
# adding prefix ATTEND_ to column names
#Summarize the MeanFlow_cfs data, using default outputs
# show that types were conserved even through export
# Look at 4 features: title, subreddit, num_comments, and created_utc
# Set representative structures
# Look for Retweets
#Review column names in dataframe
# 4.What was the largest change in any one day (based on High and Low price)?
# data_folder = '/Users/alex/Documents/ODS/oct_4_2016_dump'
#Jersey City': 'b046074b1030a44d'
# We'll also remove the double "_" in DATE__OF_OCCURENCE
# Para el caso de zip code y location y dado que son pocos (en virtud de lo observado anteriormente) vamos a desprendernos de aquellos casos en los cuales # no tengamos ni uno ni otro. Puesto que si tenemos uno podriamos llegar a tener el otro
# Add a "minute of day" column as a plotting index
# gDateEnergy_content.count()
# Save references to each table
# Initialize reader object: urb_pop_reader # Get the first DataFrame chunk: df_urb_pop # Check out the head of the DataFrame
#import FB data
# or  ... s[['c']]
#drop some columns from data
# Issues open
# display(flightv1_1.select("flight_leg1.carrierSummary").show(100, truncate=False))
# set image path
# Now we'll try "device_ip" - 2MM values in the reduced training set
# 3. 
# which neighborhoods had an average response time of at least two weeks?
# Run openmc in plotting mode
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#just_coordinates = wash_parkdf.coordinates
# drop Nan if it is in specific column
# using the default parameters of the logistic regression
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser'
# TODO for now, just delete the song
# One hot encode features with boolean values 
# predict on validation set
#month pandas.DataFrame(data['11']['data'])
#WARNING: This cell will take a while to execute - up to a couple of hours
# Summation - NaNs are zero. # If everything is NaN - the result is NaN as well. # pandas' cumsum and cumprod ignore NaNs but preserve them in the resulting arrays.
#Make dataframe a datetime index
#build url  #go to facebook get the GameOfThrone page name and then get the id.
# 8. 
#=== By Label: Selecting multi-axis by column labels 
# pie chart showing percent of total drivers by city type
# Instantiate the model. # Train the model, passing the x values, and the target (y) # the vectorized variable contains all the test data.
# separately save the meta_data for each order # then we can add it back later
#we will arrange it in a assending order with a new label
# Converting column to date format
#Concat two original contractor_bus_name and contractor_number column  #into a unique contractor_bus_name per record
# Transforming dataset values # vetor de frequencia de palavras = repos freq. # Calcula matriz esparsa de similaridades User_LangsxUser_Langs
# Time series comparing Likes vs Retweets 
# Convert to Spark Dataframe # Create a Spark DataFrame from Pandas
# Add week of the year
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
#It is easy to do a plot on this:
# We display the total amount of money spent in salaries each year #We display the average salary per year
## strip whitespace in column headers
# Use the base url to create an absolute url
# set datetime
# Euclidean Distance Caculator
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# messy file
# predict against the test set # preds = aml.predict(test) # or:
# pull out isPorn values for training data (first 2000 youtube title values and first porn title values)
# convert back to spark dataframe
# Renaming column
# Checking the columns of our submission sample
#review saved data
# Show a sample of the data
#Run this command to debug if Service failed
# Dataframe for alpha values (transparency)
# Extraemos el promedio:
# Let's check the baseline accuracy for the model. 
# count the gene names
#ff6042c59f3870e2 -- original hash #fe6046c59f3870e2 -- new hash, off by 1
# Group by day
# resample to weekly data
# Check for duplicate col names 
#let's look at 1 timezone.. washington, dc
#Save figure
#Use the REST API for a static search #Our example finds recent tweets using the hashtag #datascience #tweet_list = api.search(q='#%23datascience') #%23 is used to specify '#'
# create a mosaic graph 
# create PCA # fit oru data
# storing cities, states and counties as sets
# Create the Dataset object. # Map features and labels with the parse function.
# first 25 shared domains
# merging zipincome with zip_depart
# let's check unique count of contractor_number per name  #Display the contractor id which have more than one name.
"""$ Count token frequency and print$ """$
#check how many values are missing
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# calculating or setting the year with the most commits to Linux
##### sort by label on index
# Make country folders
# Show the node_types file. Note the common column is node_type_id
#5mm records as result of query, but no data transferred #Set device_id = 2 to use open GPUs memory; MapD using device_id = 0 for its own caching
# write out csv file of all data
# check Basin Parameter info data in file manager file
# Step 4: Investigate the data # This shows that there are now only 12 unique times b/c the AM and PM are no longer there
#calculating the proportion of p_diffs greater than actual differce
# Groupby the new year and month columns
#Save figure
#turn weekly series into dataframe and add prediction column
# Extract unkown users' log
# Filter Twitter Streams to capture data by the keywords:
#7.)
# recode column names
#Save this model to a pickle file
# Simulating conversion rate times for NULL Hypothesis 
# frequency of chunk types/labels # from collections import Counter
#Show rows which address is null
#words = jieba.cut(data.content[286], cut_all=False) #for word in words: #    print(word)
# Filter dataframe without undefined state and assign it back to data
# train file is an extract from history file with target variable added
# describe the imported data: mean, median, max, min
# Request content from web page # Set as Beautiful Soup Object
# How many stations are available in this dataset?
# Removing observations at end of dataframe with na values
# Read the dow_jones_data.
# Create a column with the result of the analysis: # Display the updated dataframe with the new column:
# convert series to categorical, then back to series  # because categoricals lack many of the useful methods of series
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
"""$ Test recover df pickle$ """$
# play with FileDataStream
# Transform the list of series into one series # Merge the series to free_data dataframe
## display the highest-traffic stations for this week (all entries+ all exits)
# Check for NaN values in CAND_NAME
# Create our tokenizer # We will tokenize on character level! # We will NOT remove any characters
#Get the top 10 locations of the tweet authors (include no location listing)
# NLTK's punkt includes a pre-trained tokenizer for english which can # be used to transform (split) new paragraph observations into sentences
# sort from the youngest to the oldest
# Creating another lamba function to find spaces and split the each value
# Assign the stations class to a variable called `stations`
# When both left and right has unique key values # The 'inner' is taken. key='zoo' in the right is not adapted into merge result.  
# Only fetchs historical data from a desired symbol # When we are not dealing with equity, we must use the generic method # or qb.History[QuoteBar]("EURUSD", timedelta(30), Resolution.Daily)
# Columns with results
# Use the append() function to join together free_sub and ed_level.  # Are the results the same as in part (4)?  # If not, how could you reproduce the result append() by using concat() or merge()?
#'New Orleans': 'dd3b100831dd1763'
# fit the classifier to the data
# if we'd rather have the grouping variables as columns # (not as index):
# You can compute the class GPA by #Show pipeline maps i.e. the cluster tracks how the data was created #rddScaledScores?
#finds date of most recent and least recent tweet
# Create logit_countries object # Fit
# We should make sure there are no collisions in column names.
#export and load the df_sample csv
# update new category names with new (interpolated) category names 
# recuperar los tipos de datos de cada columna
# Conert the numeric columns to integer
# Number of Stations
#Create a df from table extracted from webpage
# let's get every tweet with hashtag "#digitalgc" in for the past week # twitter api only allows searches for the past week # api.search
#CC: 4.2.5: Calculating Daily Mean Speed
# mean US precipitation grid time series # mean plot
# Compute metrics
# get the unique number of games Jimmy did not play in
# Counting the no. commits per year # Listing the first rows
#models trained using gensim implementation of word2vec
# A:
#inspect station data
# using crawler
# Create a histogram.
# cannot pass on non-date-like strings
# A cleaner one line solution
# reset the index to the date
#probability of individual with new_page
# Next, compute the duration of the campain, in weeks
# Convert the dataframe into proper format which fbprophet library can understand
# pickle.dump(ab_groups, open("ab_groups.p", "wb"))
#shape of the data
# tokenize the text
# get the gene names # show to value of a random selection of the attribute column of 10 records
# load the .env file
#### Work with dates
#save the results in excel or show it #df_p.to_excel(str(keyword)+'20.xlsx', sheet_name='sheet1', index=False)
## 'engine' is a connection to a database ## Here, we're using postgres, but sqlalchemy can connect to other things too.
#All the tweets are combined first
# integridad del DF
#Get the text of the tweet
# check score.py and main.py
#Lincoln': '3af2a75dbeb10500'
# p_new under null and p_old under null and p_mean
# Importing the built-in logging module
# Facilitators
# for parsing bad formats # caution: recognizes some strs as dates that you might not prefer
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# cisrol12 # this subject had mp4s for each activity
# return the requested page into a json object
#*--------Merge Command to merge Studies and sponsors--------------------* #studies_c=studies_b.merge(countries,on='nct_id')
# add response variable
#El numero de oficina cual es ?
#Perform a basic search query with '#puravida' in the tweets (Costa Rica's national motto) # Print the number of items returned by the search query to verify our query ran. Its 15 by default.
# Step 8: import PCA
# really, answer come so quick
# From a list with an implicit index
# build the DOM Tree # print the parsed DOM Tree #print lxml.html.tostring(tree)
#now drop extra columns
# Use list comprehension to create new DataFrame column 'Total Urban Population' # Plot urban population data
# create a very simple time-series with two index labels # and random values
# Creatinng lambda function to map week number to datetime columns
# score summary
cur_b.execute('''$ ''')$ conn_b.commit()
# check Forcing list data in file manager file
#We create and authenticate an instance of our new ```PrintingStreamListener``` class
# check fineness of feature_col
# 2. Convert the returned JSON object into a Python dictionary.
# Process a single status
# 10-fold cross-validation with two features (excluding Newspaper)
# plug in to df_tick
#plt.ylim(200,400);
# Are the results only positive? # ANSWER: NO
# Plot all "Categories" and their occurrence count.
# pick the top n games based on either leverage_ratio or confidence
# Grouping by "grade" category and count the number of occurences 
# from lists
# Convert JSON tree to a Python dict #data = json.loads(geocode_result) # print to stderr so we can verfiy that the tree is correct.
# shape of coarse grid
# have a peak at data that occurred before 2018-04-01 23:00
# reshape the data
# This has reduced the dataset by approx. a third:
#TODO
# Create corpus matrix
# show topics
# Save references to each table
# replacing all 'N,0"' values in the country column with 'NZERO' to avoid discrepancies while one hot encoding
# We'll should also remove the double occurence of "_" in DATE__OF_OCCURENCE. Do so below: # New Code here # crimes.columns
# Use SQLAlchemy create_engine to connect to your sqlite database. # Create an engine for the hawaii.sqlite database (created in part 2) #
# df['CreatedDate2'] = pd.to_datetime(df['CreatedDate'], unit='ms')
# Looking at the information saved within a gene
# the 'ensemble', 'havana', and 'ensemble_havana' annotation sources describe sub-gene elements. # use logical indexing to filter the dataframe for sub-gene element annotations
# filter SwissProt proteins to remove species for which a proteome was downloaded
#Stores file in the same directory as the iPython notebook
#probs_test.detach().data.cpu().numpy()[:, 1].mean()
# Use DataFrame.plot() and sub in the new dataframe containing predictions made by the model you used  # to generate a graphical comparison of the predicted prices. # YOUR CODE HERE
#Example3: Passing the sheetname method by their order
# Perform date transformations
# 1. free_data = my_df_free1
# 6 blocks x 20s
#Save figure
# Reading csv file and specifying the index as column 0 of the file
# Get the raw data values for the reflectance data
# Creating a lamba function to read each row and find the comma...then split the field
# run the model giving the output the suffix "1dRichards_docker_develop" and get "results_1dRichards" object
# Get Guido's text: guido_text # Print Guido's text to the shell
#check the loaded file #Remove unnecessary columns
# Only using data with labels # Splitting that subset into train and val
# load the data to a .csv
# Drop the rows for last date
# TODO: Tune Parameters
# Doctors Decomposition
# Let's fit& score with Logistic Regression
# cvec_4 top 10 words
#Tells us non-null types
# From the original merged table, count the values by type of city
#lets look at the first 5
# Rebuild network and run simulation using new config parameters 
# find the table # for row in table_rows: #     print(row.text)
# group by order_num, then sum up number of tubes per order # lets peek at the result
# create from list of Series
"""$ Number of unique timepoints$ """$
#downloading dataset
# It would be nicer if those states were capitalized. We can easily do that
# Save to DB, name taken directly from search #hashtag # DataSet.to_sql(SEARCHHASH,engine,if_exists='append',index=False)
# -f = to ignore non-existent files, never prompt # -r = to remove directories and their contents recursively # -v = to explain what is being done
# Build a classifier # k is chosen to be square root of number of training example
# Facebook
"""$ Make tmp sr pickle$ """$
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Cuidado, para cada columna son las filas con elementos
# Clear out the proc directory
# 14. Create and print a list containing the mean and the value counts of each column in the data frame except the country column.
#faamg
# read in the dataset
# TASK 2
# export latest elms_all
#Convert the returned JSON object into a Python dictionary.
# Create the plot
# Import studies text file
# Change the Date column to the index to facilitate plotting
# The index name of the previous row can be recovered with:
# Change the plot size.
#pre-3 mo pairs
# We print some information about Groceries # attributes: shape, ndim, size
# select all rows for a specific column
# Language / culture can have a huge effect on NPS responses; does it here?
# Instantiate a 2-group EnergyGroups object
# reset - the row index will be made a column again
#Retrieve the title for all NASA articles
# Only needed so this notebook can be run repeatedly
# This gives a list BS ResultSets for all articles. #h2_content = [item.find_all('h2') for item in article_divs]
# function to show the yearly intervals
#Use Pandas to print the summary statistics for the precipitation data.
# Export to CSV
# Sorting is necessary after parallel execution.
# Save data to Excel
# We replace NaN values with the previous value in the column
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Drop all rows with missing information
# load the
# shift forward one day
# Most favorited
#Multiply the genres by the weights and then take the weighted average
#Pregnant(kittyIndexToOwner[_matronId], _matronId, _sireId, matron.cooldownEndBlock);
# prediction on validation set
# Dict of all co_occurences
# Count hashtags only
# We extract the mean of lenghts:
#start random forest
# Calcular rendimientos diarios y graficarlos
# Count number of lines in ratings
# faster way with pandas 'to_datetime' method:
#df_concat.drop(["comments.data", , 1, inplace = True)
#payments_all_yrs_ZERO_discharge_rank = payments_all_yrs_ZERO_discharge_rank.reset_index() #payments_all_yrs = payments_all_yrs.reset_index(drop=True)
# retweets
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# the cursor - holds a result
#new_messages.timestamp = pd.to_datetime(new_messages.timestamp, unit='ms') #new_messages.watermark = pd.to_datetime(new_messages.watermark, unit='ms') #new_messages.datetime_created = pd.to_datetime(new_messages.datetime_created)
#model.most_similar('yea.r.01',  topn=15)
# alternatives:
# now it would be nice if they were assigned to a few different days and the usual three meals # aka a DataFrame with 3 columns, and 7 rows # NumPy's ndarray can be reshaped - here's how it's done with a pandas DataFrame
#gbm_predictions.loc[ada_predictions['Predictions'] == 1]
# READ DATA
#Set style
# Your code goes here #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # pass the loss function that optimizer should optimize on.
# Prepare scoring payload using these random values
# Compute the 10-day mean difference between the daily high and low prices
# We replace NaN values by using linear interpolation using row values
# Create second-level index for df based on begining and end or data range
# %load ../solutions/lobsters_dropped.py #stories['github_username'].isna().sum()
# group data frame by time and number of tweets per each hour
#dfg.loc[('drg3', 'Red'), :] #dfg.loc[('drg3'), :5]
# Make columns with country sums
# What are the most active stations? # List the stations and the counts in descending order.
# Load the query results into a Pandas DataFrame and set the index to the date column.
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe: # We display the last 10 elements of the dataframe:
# Combine ET for each rootDistExp # add label 
# get number of non-NaN elements
# sneak peak at data
cur.execute("""CREATE OR REPLACE VIEW empvw_20$                 AS SELECT * FROM employees WHERE department_id=20""")$
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#coefficient between delay time and crime instance
# HC01_VC36 is mean travel time for workers (census tract level) in minutes
# let's check
# Number of tweets by company
# one time if for converison of list
# Requirement #2: Add your code here
# normalization to put everything in float and convert "N" to nan
# Creates a dictionary based on the sentence tokens
# check tail
# tweets_df = pd.read_csv(tweets_filename)
# Evaluate the model and print results (optional, only useful when there's a metric to compare against)
# make a calculation directly # or create a function and apply it accross rows of data # save data to file
#dataSeries = pd.Series(df_Q123['Outside Temperature'])
# Just to have the sample dataframe here for comparison
#from nltk download premade stopwords list
# import and retrieve portuguese stop words # stop words are not considered as token as usually they carry no meaning (!)
# create our kmeans model & predict clusters
bulk_data = [(row['url'],row['title'],row['author'],row['tstamp']) for row in res]$ cur.executemany("""INSERT INTO coindesk VALUES (%s, %s, %s, %s)""", bulk_data)$
#url for mars table
# Count the number of tweets in each time zone and get the first 10 #pretty crazy that Central America (where Costa Rica is) is in 10th place
#coverting a csv file into a dataframe
# Checking Correlations # Cost and Sessions highly correlated r = .89 # Margin & Bookings approaching multicolinearity colinear r = .92
# create new dataframe with profile_ids # add feature value column to existing dataframe
# convert bytes to string!
# Once the column has been created, I want to sample parts of the database to ensure integrity in the matching.
# saved each to its own date and then loaded/brought them together to one df
# might be affected by timezones, so look at just US. # looks like there's a spike around lunch hour (1-2 pm) and around after work 
#ubico por columnas #df['field1']
# analyze validtation between BallBerry simulation and observation data.
# Your code here # df = 
# Normalize creates normal daily times and will set the default time for each day as midnight.
# Look at one ticket sepecificly
# do the rest here (potentially the next day when quotas are refreshed) #print('title:{} artist:{} url:{}'.format(title, artist, youtubeurl))
## Read in the data set
#payload = "elec,id=500 value=24 2018-03-05T19:31:00.000Z\n"
#Create df with EC2 ProductName, BoxUsage, which eliminates 1 time RI charges, and then filter on TTE_DEV #df_ec2_instance_ri = df_ec2_instance[df_ec2_instance['ReservedInstance']=='Y']
# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')
# Double Check all of the correct rows were removed - this should be 0
# Identify the abbreviations, as these are not relevant to the SDA corpus.
# for game-state variables we care about the beginning of the PA, since that mostly determines strategy
# Encontrar los registros que se encuentran entre las posiciones [10mo - 20vo] de mi DataFrame
#Mars News #print(soup.find_all('a', class_='content_title'))
# Largest change in any one day
#one can ignore this cell if ANN is not required
# convert s1 from US/Eastern to US/Pacific
# This is the fastest way to swap a dictionary key / value order.   # This is the format gensim LDA expects it's vocabulary.
# We first observe the head data.
# Display shape of Projects, Participants and the merged DataFrame fp7 # The number of rows in the merged DataFrame FP7 must be the same as the # number of rows in participants
## ------ Avergae Polarities of each news source ------ ##
# labels dict contains IOB (inside-out-beginning) labelled entities # printing some randomg k:v pairs 
# pitcher_throws # stand (batter_bats)
# Compute neutrons produced per absorption (eta) using tally arithmetic
# URL of NASA
#nltk.download('tagsets')
## Check the top 5 tweets in the dataset
# accediendo a una columna por el label y obteniendo un dataframe
# TODO: Create a new agent with a different state space grid
#Printing the schema of the dataframe
# drop rows where our value of interest is unknown # if you drop the original column you'll lose partial atbats. sometimes it's null for only some pitches. dropping it loses event data.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# load ADNIMERGE.csv, the main dataset (we will add our own variables to it). 
# Examine visits here
#== By Label: row range by data index, column range by column names 
# tips.tail() # tips.sample()
# Say we only want to compare fails and success, we can create a separate view of the dataset that # contains only those states
#API setup
# Plot the results as a histogram with bins=12.
# Create a sentiment model and store in data_df
# series
# Total number of stations
# 3. How many total unique Taskers are there in this data sample?
# This should read datetime64 for Date, and float for everything else.
# Add a normalizing column for each song in each region to convert to streams per million of populace
# how big is the largest collection?
# In single line
# maybe a pie chart showing the different users and the magnitude of their contributions is the total number of lines changed/removed/added??
#using . notation
# Next step: only select the RIDs that overlap across all 3 diagnosis datasets # first make a list containing all the unique RIDs in each dataset
# Subset to only Level 1, and those where days_baseline is small (let's say below 7)
# Select for just the graffiti removal entries
#checking the shape of the age_up70 dataframe
# read in the file from matlab # read in the file from our edftocsv
# Lets create a copy of the dataframe so that we can run some tests on it
# check types
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#################################################### # LOADING THE DATA ####################################################
# shift the signal, so trade at end of month and avoid lookahead bias
# Find and click the full image button
# Add two new columns to dataframe
# I created a csv with: # a binary for whether a day is an offical DC gov workday (per their website), which should inform occupant electricity use, # and the minutes of daylight in a given day (from NOAA), which should inform day v night electricity use and capture seasonality
# largest decrease in gamma / best to stop the logspace around 300, can do 10 steps, accuracy vs. speed
# add columns with automatic alignment
# How many tickets in each topic
#X_train.append(training_active_listing_dummy)
#movies['year']=movies['title'].str.extract('(\s+\(\d\d\d\d$)',expand=True)
"""Start Logging"""$ # Some of the code below takes a long time to run, the logging helps knwo that it's still working
# Use the Base class to reflect the database tables
# Group data by company to normalize it accordingly.
# Merge the station data with census income group data
# Graph of the Polarity by hour for a specific day
#normaliza os dados da versao 2 para range de 10
# Predicting the  y values on the testing  Random Forest Classifier #fit on the training, predict on the testing data
# create an isntance of the ModifyModel class 
#tweets = pd.read_csv('tweets_mentioning_candidates.csv')
#looking for all AZs
# Train the classifier Naive Bayes Classifier
# add intercept column # add dummy variables
# flight6 = spark.read.parquet("C:\\s3\\20170503_jsonl\\flight6.parquet")
#now its time for forcasting the data
# Make predictions. # Select example rows to display.
#Temperature - fill short gaps < 1hr in all temperature records
# 12. Print rows 100 to 110 of the free1 Data Frame
# df_search_cate_dummies.iloc/[1]['user_id']
# Doing some basic descriptives
# Now combine them, dropping the original compound column that we are expanding.
#read in level1 CSV
# Initialize map
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser'
# same logic here:
# looks like someone just manually entered the same datapoint twice
#Design a query to calculate the total number of stations.
#date_str = date.strftime("%Y/%m/%d %H:%M:%S")
# a timestamp representing a specific date
# Merge rows to summing along rows of multidex
# save to new csv
#hierarchical Indexing
# Create a new dataframe for the Airport ID (It does so by testing true/false on date equaling 2018-05-31)
# Table info to check NULL values
# We replace NaN values by using linear interpolation using column values
# lgb1.feature_importances_
#with open('../notebooks/subsample_idx_greedy_balanced.json', 'w') as fd: #    json.dump(list(idx_set), fd, indent=2)
# create list stores deep cleaned text
# write your code here
# A:
### read in IOTC csv file scraped from iccat site (for Chinese vessels only)
# Read the data file. # y is the column of response values.
# A one liner as I wanted to explore using islice and thought it should be possible
# array to dataframe # add name
# view the processed dataset
# analyze validtation between BallBerry simulation and observation data.
# Checking How big the Data set is
# treatment group converting mean
# compile model
#Garland': '7c01d867b8e8c494'
#training the dataset
# Create pivot table with mean age at death by year
# Extract the 8-group transport cross section for the fuel # Condense to the 2-group structure
# 8. Print the mean education for each age_cat using groupby.
# Creating the list of files in the folder
# check current directory
# load the data # this file has 'numVoters', 'numDems', 'numRepubs' calculated from the voter tables and populated
# Pull up the same value in the "cleaned" dataset
# Loading avro-issues.csv
#Drops the unneeded part of the URL in the feature "Date" 
#get the mean prediction across all 
#Select the flow data (4th column) for the 100th record
# convert dictionary to dataframe
#Change Dates to string fo manipulation
# Load the list of topics created manually
# Setup the hyperparameter grid #,'max_depth':[4,6,8,10]
#session query
#Remove rows which contain words from blacklist
# the maximum age
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
#Creating a (pandas) DataFrame # We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
"""$ Print all news title docs$ """$
# Save the data, overwriting the old 2018 file
# Many of these are just MAGA related # Let's see how many
#converting size in KB to size in MB
# common observations in history and train ?
# Likes vs retweets visualization
# Create BeautifulSoup object; parse with 'html.parser'
# results are returned as an iterable list
#called effectiveness function 
# Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date
# save data to CSV
# classify the device type # remove remaining device columns
# 9. Print summary statistics for each column for those with an education greater than or equal to 5, grouped by age_cat.
# Table values are from 'D' column  # Indexed by combination of 'A' & 'B' columns  # Result table has columns for each unique value of 'C' (i.e., 'bar' and 'foo')
# convert KOSDAQ ids & names lists to Pandas DF 
# Labels - row: 'CHARLOTTETOWN', column: 'Wind Spd (km/h)'
# Pipeline class # Create and return an empty Pipeline
# Examine fitness_tests here
#features
# merge the the newly imported "numberOneUnique_df" with the original "spotify_df" df one to id the songs that make it to # 1
# df.apply(arg) will apply the function arg to each column in df, and return a DataFrame with the result # Recall that lambda x is an anonymous function accepting parameter x; in this case, x will be a pandas Series object
# Here's where I apply the train test split
# and that the column labels
#Filter by the station with the highest number of observations.
# Make predictions
# convert from string (dtype='O') to float64
# OPTIONALLY, print the keys for each turnstile (in case we need to access it later) # for key, turnstile in turnstiles: #     print(key)
# get the current date and time (now)
# last 5 rows
# import ab_data.csv and display first 5 rows
#  Compute "geovolatility" per our definition.
# Strip/fix columns and index # Take a quick peek to make sure everything's fine
# Note: used reset_index() here so it's easier to use the 'time' and 'time_2' columns if needed
# Setup Tweepy API Authentication #, parser=tweepy.parsers.JSONParser()
## daily normal 
# post simulation results of simpleResistance back to HS
# need to fix format issue on 'lastest_consensus_created_date' column.
#'St. Paul': '60e2c37980197297'
#Check count of missing address1 value 
# each key of 'words' is a word, each value its index # printing some random key:value pairs of 'words'
# We would like to see what values we have for "Class". This snippet below shows that Class is a binary column.
# print(host)
#tab.to_csv('fit_files/processed.csv')
# List the primitives in a dataframe
# build new df from 'df_amznnews' so as to leave that available for other play.
#pivot table showing the years of married and affair status
# RE.COMPILE
# Loading new datasets # List for storing the group stage games
# Now let's encode the emojis so we can use it in the vectorizer as a feature. # create a new category 'emojis' with extracted emojis from tweets.
# subset predictor X and y
# Fit pipeline 
# Find stops which are in in stops but not in oz_stops
#get the ground altitude from the modal value of the rtk data
# convert pandas data frame to arrow table
# View the last five rows of data.
# to compare score between train / test set.  
# URL and get request from Reddit # Set `limit=100` for fetch 100 posts per attempt
# Eliminar los registros que tengan nulos en la columna "created_time"
#5 dropping the NA values
# Now let's see how this works in a base case, and with the labor # force growth rate boosted by 2% per year in the alternative case...
# Removing one of the duplicated rows from df2
# assign new category names to [a,b,e]
# Sort tweets by date
# select a column
# le_data_all.pivot(index='country',columns='year') # examine pivoted data
# tokenize all the tweet's text # apply hash tag function to text column # apply at_tag function to text column
# Apparent Magnitude limit # must be a float number # Band: i
#Dot produt to get weights #The user profile
# Use Pandas to calcualte the summary statistics for the precipitation data
#Visualize the frequency of #hermosabeach was tweeted during the data collection period
# Hmm...in this case, we want the Date column to be our index, so let's try again
#Sorting by values
# same reason as above to save the data...
# Split our data into random train and test subsets with specific proportion
#this weather station started in 2013
#globalCity_pdf = 'http://journals.sagepub.com/doi/pdf/10.1177/0042098007085099'
# Extract slice - 2
#  Invest at inprice, then #  assuming market conditions embedded in poparr: #  ... effectively simulates 5,000 years of bootstrapped daily data.
#converts to dataframe
# look for '&amp;'
# Test all of the assumptions
#Distribution of data across state
#split the data #train_b.summary()
# We create time series for data:
# Remove columns
# Visualise the data for the 100'th day in 2018
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Getting logs of the fake user
#payload = "water,id="+str(metric[0])+" value="+str(metric[2])+"\n"
# Random forest score:
# https://developers.google.com/maps/documentation/geocoding/intro
# Retrieve data from Mars Weather # Create BeautifulSoup object; parse with 'html.parser'
#print(tweet.created_at,tweet.text)
#creating a column = to the index to be used as function argument
# Our start period was March 2016 and our period frequency is months.
# Find the numeric values: matches # Print the matches
# subset to included participants  ## there are duplicates, weird
# Converting the results into a dataframe # View the Temperature DataFrame
# Retrieve weekday and weekday name
# mean and variance w/in each group
#Want to check table names -- returns the same as above
#range of cohorts
# create the path to the data file # the file is named 'test1.csv' # read in the data
#finding the important features
#make sure query is up-to-date #query to match BABYID_BABYSN_CHANNELID_PAIRDATE for all channels # API_KEY = 'LcuoHqjZLvxaSPDrhv5VMhcrJUyPVb88RJR69REq'
#Example1:
# r squared value
# We create a DataFrame  and provide the row index # We display the DataFrame
#################################### # remove invalid duluth isbns
# Assign reponse object to a variable r and check the data by calling method .json()
# comes with a generated readme
# lag two days
#3  to carry forward previous day value for a NA value, we use ffill
# Add a new column that we will use for groupby operations
# Prefixing the my_tag dict with ** passes all its items as separate arguments # which are then bound to the named parameters,with the remaining caught by # **attrs
# Create datetime Series #  number of created items per month over the timeline of the data
# Count the total number of tweets
# media
#Grabbing first occurrence of scns_created, argmin should provide startdate
#To find value of n_new
#store tables
# what does requirements.txt look like?
# Fit the model
# read grid id to subset # unique
# favorite table # for row in table_rows: #     print(row.text)
# the accuracy hasn't changed at all. # then, we are going to decide the best combination of the model
# Set environment variable + check # export instagram_client_secret=91664a8e599e42d2a6a824de6ea456ec # echo $instagram_client_secret
# Fill missing values with 0s
#df_hi_temps.describe()
# make new column for day. 
## drop a collection via pymongo #client['test_database'].drop_collection('posts')
# Replace NaN data with a high and invalid number which is -99999 # We're saying we want to forecast out 1% of the entire length of the dataset into the future
# What was the average daily trading volume during this year?
# Split text into sentences
# get the dataset we just uploaded
#clean the data
# fitting our reducer to the full dataset # NOTE: takes 18 minutes on my local 8GB machine
#Join list values
#https://docs.python.org/3.6/library/datetime.html#strftime-strptime-behavior
#pandas #transactions_pandas = pd.read_json('data/transactions.ndjson', lines=True)
#     df_tweets['polarity'] = TextBlob(tweet).sentiment.polarity #     df_tweets['subjectivity'] = TextBlob(tweet).sentiment.subjectivity
#take a look at the data- this is only first-level broken out json (note the _source column still contains many fields)
# Convert the url to df
# Use `engine.execute` to select and display the first 10 rows from the table
# lets find the most retweeted tweet
# Saving results to pickle # store the data as binary data stream
# plt.savefig('Actual number of days between purchases')
# this way, numpy can get around the "fixed" type of objects
###YOUR CODE HERE###
#'Wichita': '1661ada9b2b18024'
#filled in nulls for LinkedAccountId, may need to do entire file later, not sure why payer doesn't need it
# Read the data.
# Sort by source and date to get latest tweets at top
#split the data as described above #Prepare predictors and response columns
#This is what we would expect 
#let us find out the significance of z-score
# Golf KOL
# remove multiindex # users with only one prediction got NaN and it's OK
# Challenge 2 done
# set SUMMA executable file
# extract the price data only # extract the column headings and convert to a row:
# Checking who all pitch artist to the user.
# accediendo a una columna por el label y obteniendo una serie
# check inserted records
# Read data from csv into a Panda dataframe
#Add a column
# Group data by death year and cause of death
#drop columns 'text' & 'full_text' #Extract hashtag from 'tweet' and create a new cloumn 'hashtag'
# a useful variant: dropna
# Every row in 'beta_dist' is a simulation of all the 'arms' in the experiment. # This step identifies the winning arm in each iteration, by calculating the MAX per row
#Convert dataframe to list
#df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD')) #df = percipitation_2017_df.cumsum() #percipitation_2017_df.cumsum()
# make new dataframes with new and old data
# read the oracle 10k sections
#Calculate the average daily trading volume during this year
"""$ descriptive stats$ """$
#Football International Friendlies data
# Drop any obervations before 2005
# Load package # Instantiate and fit the classifier model
# Variables can be any type of data
### START CODE HERE ### ### END CODE HERE ###
#First let's load meta graph and restore weights
# Save the dataframe to CSV
# Separate response variables from predictors
# loading iris file
# number of unique users in the dataset
# Visualizing the distributions
# Create an engine for the hawaii.sqlite database
#create a timestamp to get values from the FAll 2018 term that are prior to Jan 1 2017
# read ratings of small dataset
#In order to do an inner join, there must be a common key in both datasets.  Base the common key off of the school name. #This cell changes the name of the school column so that the inner join can be completed 
# plt.subplot(2,2,4)
#Display shape of df_schools
# Access local file
# We can safely drop all events with a 1970 start date.
# We display the correlation between columns
# Netstat
#status: any includes archived orders
# Show results
# some chakcing to be sure that the number of authors which are left is correct
#Percentage missed for each tip level #Negative means that actual > predicted: 80% of time, predicted tip was too low
#Looks like the second file has less columns. Let's take a look at both files and see what we can drop
# The *self* parameter in *Customer* methods performs the given instructions. # For e.g. to withdraw
# Print all of the classes mapped to the Base\n",
#use requests builtin method .json() to parse JSON into a dictionary
# calculate weeks between apply and submit
#Dataframe with only required columns
#Q: there are some points with NaN values, if the point is NaN, how to handle? valid or not?
#### Test ####
# specify column type
# DISPLAYED THE UPDATED DATAFRAME WITH NEW COLUMN :
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Group by group_period as 7D, since market cap data points set at weekly
# Split the data
# calculating number of commits # calculating number of authors # printing out the results
# 2-3-08 belongs to the 2007 Season, including # the given dates, which were start of season and # the Superbowl.
# Get Facebook data and store in variable.
# the 2018 offseason.
# Make columns with country sums
#Save to a csv file
#Display each column's data type
# Where would trades be if only bought when predicted return > 0.01% and sold when < -0.01%
# Rename the column Region to Country
# Assigned the Measurement class to a variable called measurement in previous sections. # Here saving my query to a list
# Print all of the classes mapped to the Base
#Confirm it looks good by viewing the first 5 records using the "head()" function
# Put the columns to be predicted, at the end
# drop columns where reason for visit name = Follow up, returning patient # existing patient dataframe
# Use Pandas to calcualte the summary statistics for the precipitation data
# save our final DataFrame as a CSV file
# Inspect measurement table
# Removing rows with topics which have a total number of articles less than 25
# Assign the measurement and station classes to a variable called `Measurement` & `Station`
# pickle pulled data.
# rural
# calculating number of commits # calculating number of authors # printing out the results
#\xa0 is  non-breaking space in Latin1 (ISO 8859-1). replace with a space
# Retrieve page with the requests module
# Take a look at the words in the vocabulary #print(vocab) # Sum up the counts of each vocabulary word
#Using a for loop(s), compute the sum and mean for each column in scores.
# Load the weights from disk for our model object
# Add cleaned tokens as an attribute to by_tweeter:
# filter(function or None, iterable) --> filter object # Return an iterator yielding those items of iterable for which function(item) # is true. If function is None, return the items that are true.
# we don't need file name anymore
#Group by the status values, and then describe, and then TRANSPOSE for neater presentation
#install XGBoost
# extract full monthly grid
# may I know how to see attribute of every object in notebook? # . + Tab
# open file, read it, and tokenize:
# https://www.reddit.com/r/Bitcoin/comments/
# look at the example from above, what that ticket is actually talking about
#Twitter connector
# train.summary()
##Create temperature bins. #bins=[]
# First, replace NaN with NA in status_message
# Difference of simulated values
q="""select workclass,avg(age) Avg,min(age) Min,max(age) max from adultData group by workclass"""$ #q="""select count(*) from adultData where workclass='?'"""$
#total=out3
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Read Files_osha
# Now the Processed data
# 2. Sort free_data by country, educ, and then by age in decending order, modifying the original Data Frame.
#**Which Tasker has been hired the most?**
# tweets.apply(lambda row: print(row['tokens']), axis=1) # sorted_x
# Upload the processed files
#pivot table showing the Age and affair status
#Remove Duplicates (don't keep any posts which appear more than once)
# use a vectorized string operation over the email addresses
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# merge weather and 311 #df5 = pd.merge(df4,df2,how='left',left_on='Date Closed',right_on='date',suffixes=('_created','_closed'))
#drop NaN in bio* to make the data clean
# Next, our dictionary must be converted into a bag-of-words:
# appointments = pd.read_csv('Appointments.csv')
# how many rows and columns?
#Example 6: Load a txt file while specifying column names
# df_session_dummies.head(n=2) # df_session_dummies_drop=df_session_dummies.drop(['created_at','value',],1)
#Save figure
# for testing: # hp = houseprint.Houseprint(spreadsheet='unit and integration test houseprint')
# we can also name the columns
# We create a pandas dataframe as follows:# We cr  # We display the first 10 elements of the dataframe:
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# The result of loading, paring down, and renaming columns should # look something like this:
#Graph the results  # Avg Temperature on your trip \n2016-04-13 to 2016-04-13
# map the count function to each strike where there is a nan implied volatility
# concatenate with the set of resulting columns the intersection of the column names by specifying join='inner'
# Show plot
# Making a copy on which we will perform the transformations
# Get the count of total number of records in the dataframe
#== Describe : Show quick status summary (interactive tool)
# shift forward one business day
# creates a temp file # save the model
# Start Chrome with Selenium
# Now let's see how this works in a base case, and with the labor # force growth rate boosted by 2% per year in the alternative case...
# Sample user
#Merge 2 dataframe
#### Test ####
#R18df.rename({'Create_Date': 'Count-2018'}, axis = 'columns')
#limit dataframe to only original tweets
#session query
# Get marketdata from database...
# Initialize MGXS Library with OpenMC statepoint data
# get the feature column
#Read in data from source 
# extract extended tweets DB to csv
# df_4_test.dtypes
# put response data into the data dictionary
# URL of page with Mars hemisphere high resolution images to be scraped
#iterator returned over all documents
#highest # retweets
#Use Inspector to find the table names 
# Read in our data with more help from the read_csv parameters
#goal: 
# Get some general info on the dataset. # Int is for integer, float for floating point (number with decimal), object for string (text) or mixed types.
#find number of rows that has appointment - could be interesting number to get as well
# Extract data for a specific point using the coordinates 
# Import Dependencies
# Call sns.set() to change the default styles for matplotlib to use Seaborn styles.
# save matrix as csv.  # np.savetxt("foo.csv", g.featureMatrix , delimiter=",") # another way
# X will be a pandas dataframe of all columns except meantempm, feutures # y will be a pandas series of the meantempm, target
SQL = """SELECT * FROM github_issues_2"""$
# drop unused columns
#clean the data from NaN values
#mydf2.datetime = pd.to_datetime(mydf2.datetime)
#add create date in date format
# Look at Subreddit and Age features only  # Define X and y
# unfortunately the index is numeric, which makes # accessing data by date more complicated
# How many learners had more than 100 interactions (>100) ?
#Subset data to alternate temperature columns #Check if any secondary temp sensors exist
# plot heatmap of predicted probability for fires
# check NaNs
# Use `engine.execute` to select and display the first 10 rows from the table
# Append topic codes to dataframe 
#oh shize, did we ensure the OSes are covered too? will need to improve eventually, but as all are Linux, TTE is ok #doing D2MI_DEV, not as clean? nope - we have RHEL and Linux mixed, fix it later
# value of the accumulator - the number of even numbers in data
# in terminal, this will create a csv from the video
# print(type(df)) # print(df.shape) # print(df.head())
# Take a quick look at the data
# reading a file containing list of US cities, states and counties
# get terms most similar to cantonese
# Misc utility functions
# Print the first 5 external nodes # input_nodes_DF = pd.DataFrame.from_csv(input_nodes_file, sep = ' ')
# Make Predictions
#Extract the year from the datetime index
# dummy data
# creating DataFrame
# Create a 'tweets' DataFrame for each row of the 'news organizations' DataFrame # Would it be ok if the funtion get_df_for_user_tweets required a second argument, which was defined here?
#applying cleaner (now without reforming each title into a string)
'''Standardizing the Test Set with the Standard Scalar'''$ active_list_pending_ratio_test = x_test[['Active Listing Count ', 'Pending Ratio']].values$ '''Performing the fit and the transform method'''$
#Set index to date
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Create pivot table
# Now comes the training and testing
# check is document has been parsed (dependency parsing)
# read the 2017 data to see the original data structure
# Import transplant stemsoft data
# Display of first 10 elements from dataframe:
# Creating lenghts over time
## numbers of user_ids
# Reflect Database into ORM classes
#do the non RI credit charges contain all the instance types, can we get the prices from here? compare above^ #yes, so until we have a good way of pulling the billing data, we'll just pull if from this TTE billing
# this is the default, raise when unparseable
#x=dataframe['Date']
# Set the limits of the x axis # Set the limits of the y axis
# Put in the end points for the forecast
# mean() along each column 
#look at top 50 days with most complaints. what was the top complaint for each of these days?
#Let's remove all rows containing null values. 
#find all mentions of woman/women within the posts
thecmd = '''curl -X POST -H "Content-Type: application/json" -d '{$   "query": "'''+query+'''"$ }' "http://'''+USERNAME+'''.carto.com/api/v2/sql/job?api_key='''+APIKEY+'''"'''
# Export the data in the DataFrame into a CSV file.
#Clean the df
# query to pull the last year of precipitation data
# save to excel file in worksheet sheet1
#setting up 10 commandments chapter to see how the summaries compare
#'Tulsa': 'cb74aaf709812e0f'
#== Sorting axis by labels (axis=0:row, axis=1:column)
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#dfg = df_n.groupby('drg3').agg({'2014':[np.size, np.mean]})
# plot KDE 
# Instantiate the model
# Expand submitter_user into its own df 
#Test #The date format is correct
# grab the last date entry in the data table
#Create list of top10 topic_id from dataframe
## Converting json to dictionary
# focusing on the Black Friday Weekend
# Usar csv Writer
#Works with Pix4D capture october 2017
# Plot the temperatures for the week
#Remove local version of date
# Confusion Matrix
# Step 11: Try fitting the two-dimensional X value
#Display head of df_schools
# use an index vector to find genes that are longer than 2 million bases # sort the returned values by length # .iloc[::-1] reverses the sort to be max to min
#Setting the date time as the index allows time slicing
# Extracting CBS news sentiment from the dataframe
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY + "&start_date=2017-01-01&end_date=2017-12-31"
# plot autocorrelation function for doctors data
# Calculate Driver Percents
#construct and display cbs sentiments dataframe
# Or it could be access by the position of the column (as usual starting from 0)
"""$ Check all shed words$ """$
#Save to a local file. First collect the RDD to the master  #and then save as local file. # creating a new file, writing contents to the file, and saving.
# scrs < 0m / all scrs, since 3-01-2018
# Will convert to a numpy array
# remove white spaces
# helpers contain helper functions # main_functions contain the feature engineering code # mod_class contains the prediction class
# education
# Print the first 10 most frequent words found in tweets that mention 'gender'
# again, let's take a look
# exog = sm.tools.add_constant(exog)
# this yields the row labels
# Get a list of column names and types for Stations Table # columns
# A:
# check a single (or multiple) posts
# these columns tend to be set-dependent - for instance, the same card can have different borders in different # reprintings; all_cards is just a list of cards, so we remove these columns # casting the columns as sets makes writing the loc much easier, there could be a computational cost though? not sure
# remove those that don't have a week 0 value
# Drop these cases
# z score
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #"https://www.quandl.com/api/v3/datasets/FSE/AF_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY
# remember there are about 250 days of stock trading in a year.
# drow rows
#abc = abc.reset_index(level=[0,1]) #abc = abc.reset_index()
# Get our Mongo on
# count words
#Baltimore
# data=stock_data["CLOSE"].iloc[-100:]
#ADF(resid_6203.values)
# Create a separate data frame that holds the average park attendance for different parks.
# Model is very overfit since the test score is much lower than the train score 
# Converting the index as date
# Option 2 just switches two index levels (a more common need than you'd think) # Note: This time we're doing an inplace change, but there's no parameter for this method either.
# Mapping using BLAST
# Read the data from the vtc-cab repos
# your code
# Get sentiment for Costco tweet #Read costco tweets csv file
#Save figure
# emptyDF = spark.createDataFrame(aggDF, struct_v1_0)
# Use SQLAlchemy automap_base() to reflect your tables into classes # 
#drop guidance_high and guidance_low, always NaN
# Resource: https://stackoverflow.com/questions/33899369/ranking-order-per-group-in-pandas # Create 'Inspection_number' columns, populate with groupby function on brkey field and rank ascending # Convert Inspection number field to integer
# we'll filter out the non-represented classes, sort them, and plot it!
#Find Mean Sentiment by news org
# 
#print the summary statistics for the precipitation data
# Number of missing - You can investigate these more later
# Set crs to epsg:4326
#merging the particpants column into the base table
# then, we update the stop-words list to see the result
#Read the data into the notebook
#- Force the creation of the directory to save the outputs. #- "If the subdirectory does not exist then create it"
# Compute user activity # We are interested in how many playcounts each user has scored.
# Get the dataframe
# Creation of connection to our mlab instance
# convert jsonData to pandas dataframe
# General plotting settings
# Amount invested in BTC in GDAX
#text_noun = myutilObj.tag_noun_func_words(Osha_AccidentCases['Title_Summary_Case'])
# Retrieve data from Mars Facts # Put into pandas table
# Stations with the highest number of observations
#get data from the csv directly instead of running it till now
# Prepare data
# Insert 1 or 0 if the topics appear in the campaign
# Correlation coefficient for each field
# get current local time
# Demonstrating that the rows with NaN have been removed and the last row as seen is the row just above the last NaN value # Ths would work is we did df.dropna in the previous step, but thats not what we are going to do now
#Let's see if we can scrape the urls from 'Top 5 Data Science and Machine Learning Course for Programmers' by DZone
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#test the counts #B4JAN17['Contact_ID'].value_counts().sum() #B4JAN18['Contact_ID'].value_counts().sum()
'''Performing Polynomial Features on the Pending Ratio Column'''$
def to_date(s, year):$     '''Returns a date from the datetime library from a string like \'January 1\''''$
# We'll start with LSI and a 100D vector
# a path without the root package name does not always work (e.g., it works inPyCharm, but not in Jupyter) # If the subpackage will not be imported later with a separate import command, it can also be included in initial import  # import package_within_package 
#calculating differnece from orginal dataset
# id and sentiment are categoricals
# tokenize all the pdf text
##   Creating data frames directly from CSV
# Finding unique user_ids # Checking for non-unique users
#Check count of missing city value 
# We change the row index to be the data in the pants column # we display the modified DataFrame
#there are duplicate second entries for this well, just coercing to 1
# find overall sentiments
# Add to Pandas Dataframe
# get indices
#import libraries to display equations within the notebook
# Call the function with lambda_val 0.1, alpha 40, 30 iterations and 10 latent features
# .ix can be used to look up rows by either the index label or location,  # essentially combining .loc and .iloc in one.
#calling only 2017 from api
# extract matrix for fire heatmap
# Sample five random movies
# We print percentages:
# until ttk is installed, add parent dir to path
# test to see if this is right
#writing yearmonth and counting no of files into csv
# extract precip array # check shape
# Now we use Pandas to print the summary statistics for the precipitation data-- IE, the describe function.
# some basic plotting
# Query for precipitation data based on date range from most recent to a year before # Create data frame from sql query
# check added features
# Get help
# Print the value counts for 'Borough'
# Calculate the date 1 year ago from today # one year ago is 2016-08-24
cur.execute("""select text, geo, coordinates, location $             from tweet_dump where location != ''$             order by id DESC limit 5;""")$
# So this is just simple pandas!
# create an authentication # connect to Twitter API
#pd.set_option('display.max_columns', 999) #pd.set_option('display.max_colwidth', 999)
#Row wise concatenation
#Load Data
# Spot check example # Fliter data where name is one of the repeate values
# convert strings to dates using datetime.strptime:
# fig.savefig('toma.png', dpi=300)
#switching date of tweet dtype from object to date-time #setting handle as index
#Tells us the probablity of a Reddit will be correctly identified in the class its assigned
#As we can say that rating 1 ,2,3,4,5 are skewed towards right now we will se about outliers using box plots #as we can say there are outliers so text length as may not be as a best feature for predicting
# Ploting mention count and the hashtag count against eachother
# get sum of touchdowns by game day
# Display confusion matrix for the binary classification # example by using our helper function
# Create a cursor to do SQL queries. The cursor is the object that is used to do  # queries to the archive
# Extract the mean length of the tweets:
### list of column names
# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})
#import datetime
# Rows where magnitude is greater than 10.
# Prints only the number of rows # Prints on the number of columns
#Filter events which topic is in top10 #Compare shapes
# Give the chart a title, x label, and y label
# Download all mapped PDBs and gather the metadata
#Trying to read the csv file with the list of twitter accounts of the fortune 1000 companies #this is a csv file with the twitter accounts of every Forturne 1000 company #I am reading the csv file to a data frame called companies
# here, name-based index created above will be kicked out altogether and replaced by the  # default numerical index
# create the API
# En esta instancia guardamos una copia de la informacion. Dado que no utilizamos ninguna de las columnas actualmente como index, no guardamos el indice que  genera pandas al importar el csv
#Full graph #fullgraph = nx.read_gpickle('all_tweets_pickle') #Bigraph
# First let's turn each title into a list of words # Then we'll use the lemmatizer to change plurals to singulars to group them # And rejoin the lists of words to one string for the count vectorizer
# remove unnecessary columns(0 or 1 non-null value): contributors, coordinates, geo, place
# For getting all the texts under the div tag
# Preview the information available for the table
# preview the data
### Checking the number of dummy variables needed
#Read json with pandas and inspect
# Train-test split for accuracy metrics
#voters.loc[voters.LastVoted.notnull(),:]   # uncomment to see these 6 columns
# dummying coming_next_reason column
#YH_df["date"] = YH_df["created_at"].str.encode('utf-8').apply(parser.parse) #YH_df["date"] = pd.to_datetime(YH_df["date"])
# listings.dtypes # listings.head(2)
# The clipboard from Excel has the following:
# The url request must start 'https://www.quandl.com/api/v3/datasets/'  # then 'FSE' is for Frankfurt Stock Exchange, then the ticker, 'AFX_X' then .json? then any filters.
# convert crfa_a to numeric value
# drop NaNs
#doing business as name
# Save the file and run this block
# Append distance to an empty list # Iterate thru get_distance to return the distance between coordinates
# Create online deployment.
# train_ratio = 0.9
#Pass the tweets list to 'toDataFrame' to create the DataFrame
# df.columns.values
# Load the mazda datasets
# add a notebook to the resource of summa output # check the resource id on HS that created.
# To print the count of our vocab # For each, print the vocabulary word and the number of times it  # appears in the training set
#creating a spark context file and then reading the log file using textfile() method.
# check the first 5 cards in the Dataframe containing cards from the Ixalan set
# Instantiate a 2-group EnergyGroups object
#== Boolean Indexing 
#data['new_claps'] = data.claps.rank(method='dense').astype(int)
#randomdata.describe()
# same as above but as different return value type (as series) 
# do not fit with U.S variable as this will be the baseline
# how many are `Literacy & Language`?
#'Oklahoma City': '9531d4e3bbafc09d'
# calculating or setting the year with the most commits to Linux # print the result
# reset the index to the date
# Calculate the date 1 year ago from today
# All tweets's retweeted value is 0
#info df
#Use nunique on the column to list the number of unique values
# percentage of ratings in lower half (of possible scores, not lower half of distribution)
# Get column and types Station Table # columns
# Word frequency for terms only (no hashtags, no mentions)
# Add a new column to data file #Make sure to modify addFarm function on adding new column
#KNN Result
#select only the rows in files1 who had completed application
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Check that the largest observed value is <= step threshold
# Use tally arithmetic to compute the scattering-to-total MGXS ratio # The scattering-to-total ratio is a derived tally which can generate Pandas DataFrames for inspection
# df_2017
# Function inputs # Plot the matrix
# one hot encode the component and team columns
# make sure ES is up and running
# 68 original columns # 10 are empty # 
# Describe is the basic summary stats function for numerical values.
# create from ndarray
# Keep relevant columns only (23 columns of 49) #studies_a=pd.DataFrame(studies,columns=['nct_id','overall_status'])
# normalize the data attributes
# Cleaning up our column names using string functions:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# the number of features has been reduced significantly, now only 86 features
# Create index
# # # created a dataset that use postcode as index
# Numpy broadcasting to perform cartesian product of nxn euclidean distances # https://stackoverflow.com/a/37903795/230286 # (n x 1 x d) - (n x d) -> (n x n x d)
# Remove the seconds in the timestamp column
# recuerda que esto no cambia data en realidad, porque no lo hemos guardado
# label encoding block by their any_spot rankings
#print(preprocessor('</a>This :) is :( a test :-)!' )) #print(re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', '</a>This :) is :( a test :-)!'))
# regression model #fit the model
#Number of rows in training set  
# use BIC to confirm best number of AR components # plot information criteria for different orders
#Load CSV
#Get a count of all records from the dataframe's shape (rows, columns)
# How many stations are available in this dataset?
# we can see ~our~ most followed users,
#Do the same for male to get a male only dataset #split the dataset by gender
# show available waterbodies
#created 3 separate dataframes for urban, suburban, and rural
# How many stations are available in this dataset?
# re-assign variable value
# Create x, where x the 'scores' column's values as floats
#get average-to-date for each PA / player to avoid info leakage
#a computer can understand
# df_lib_con.title = df_lib_con.title.str.replace('\d+', '')
# Retrieve the NuFissionXS object for the fuel cell from the 1-group library # Show the Pandas DataFrame for the 1-group MGXS
# A:
#bins='log', cmap='inferno'
#Let's plot a subset of the data in order toget a better sense of it.
# the contents of the date column
# We replace NaN values with the next value in the column
# get rid of the attributes column # peek at the new columns
# Creamos series de tiempo para datos:
# Export to csv
# put into pandas table
# Call set_up_images
# rename non-target columns
# What are the most active stations? # List the stations and the counts in descending order.
#adopted_cats.reset_index()
## Get the No of Followers and retweets 
#To get an insight on the length of each review, we can create a new column in yelp called text length
# 4. What was the largest change in any one day (based on High and Low price)?
# rows only with emoji # df_emoji_rows.head(1)
# Plotting one of the users...but I have 5 total users
# create time series for data
# Confirm that you have 8 values for each column.
# Applying function to "Ideas" 
#Humboldt Park, Division Street converted to Humboldt Park and Division Street
#The number of times the new_page and treatment don't line up
# get the indices for the rows where the L1 headers are present
# read in the file 
# split into training and test
# Design a NEW query to retrieve the last 12 months of AVERAGE precipitation data for each day.
#Count and plot actor 1 Name
#find maximum of Sepal.Length by Species
#lsi_out[list(range(50))], reduced #result = clustering.k_means(reduced, n_clusters=5)
# The case of "1) variable (time, hru or gru)" #fig = result.get_figure() #fig.savefig('/media/sf_pysumma/pptrate.png')
# First Column
# drop un-needed columns
# At this point it's clear how to use mrec and what it does under the hood, but let's do # one more thing and use SLIM. # First, find regularization constants:
#Get rid of none values for the timezone
#Extract years and months into a new columns in our dataframe #Have a look...
# parse url request to json # pretty print json; suppressed print because it's a lot of lines #print(json.dumps(j, indent=2, sort_keys=True))
# read the original frame in from cache (pickle)
#Tweets containing references to Donald Trump
# Inspect number of rows and columns 
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
# Count the number of tweets in each time zone and get the first 10
#Import modules
#merged2.columns, merged2.index
#calcuate the age of the stories in the last state it is in
#print key["API_KEY"], key["API_SECRET"], key["ACCESS_TOKEN"], key["ACCESS_TOKEN_SECRET"]
#Using the statisics library#Using t 
# read geojson neighborhood data
# Load the tallies from the statepoint into each MGXS object
# keep only columns with more than # non-null rows
# frequency count
# result = customer_visitors.groupby('Yearcol').mean() # result.customers_visited = np.round(result.customers_visited) # result
# Top 10 most active companies
# print some configuration details for future replicability. #hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE
# To access a particular row, we use the index position:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#check
#== By Label: A parcular cell (scalar value)
# Combine ET for each rootDistExp # add label 
# creating dummy variables for DEVICES
# creates a Series with values (representing temperatures) # for each date in the index
# Visualize parse trees
# how many unique authors do we have?
#Sorting by date
# specify outer join
# A:
# filter on Urban # len(Urban) # Urban
# Create dataframe from a dictionary of Pandas Series
# Count number of missing values from each column
# integer index and .values field: the values proper (numpy array)
#select only that day
# ph = load_df('mergeable_holidays.pkl')
# drop columns where reason for visit name = Follow up, THerapy, returning patient existing patient dataframee
# Verify DataFrame:
# Create a list of links.
# Retrieve tone of each ticket
#### Assessing the final dataset
#df_insta.dropna()
# score on test set
# Checking the numbers of polarity, subjectivity and Sentiments of the first 5 tweet.
# Drop duplicated user
# Rezeptsammlerin now only has 679 recipes instead of 870:
# Find and list the most active stations
# create overall df to set up for mean scores (bar chart)
# load the query results into a dataframe and pivot on station
#create a temp df that has all the sentiments broken out by applying the get_polarity_scores function
# Adjust the two date columns.
# selecting data by col and inx:
# take a peak at the data
# the apply() function here should probably be split up; it's messy!
# The Duplicates have diffrent time fetched the file
# Full_text column was checked carefully # 22699 tweets were the exact same tweet, available at verizonprotests.com to be posted by one click
# Read the last 25 Million records from training data
##Plot histrogram of dates of birth
# Apply new function to the column entries
# We can save a reference to the bound method and call it later and it will use the right instance
#Calculating median trading volumme for the year
# selecting two columns # enter code to select two columns. It will look something like: dataframe[[column 1, column 2]]
# Set the X and y
# alternative: .sample and .difference
#non_nlp variables  # y=df_combined['subreddit'] # transform the label 
# Design a query to calculate the total number of stations.
# get cat sizes
# get the current local time and demonstrate there is no # timezone info by default
# Some stats of constructed dataset:
# sends command
# check shape
#count of converted values
# call install_test_cases_hs method to download TestCase from HS, unzip and install the TestCase.
# Ctrl+C the table output (say 0-3 records of the table above) # Run this...
#output.to_csv("result_modified_97rfcorrect.csv", index=False)
# Get all titles that have "SOFTWARE" or "PROGRAMMER" in them
# split out y size # split out predictors
# Instead of leaving it as NaN set it to 0, although that's not possible as event cannot have "0" gathering
# call download_executable_lubuntu_hs method to download SUMMA Executable from HS, and unzip.
#split the data as described above #Prepare predictors and response columns
#wikipedia_women_in_medicine = 'https://en.wikipedia.org/wiki/Women_in_medicine' #requests.get(wikipedia_women_in_medicine)
# print words without probability
# let's start with a few simple observations: # - as in the case of dataframes, indexing series with  # integers vs slices/list gives different types of outputs
# Create engine using the `hawaii.sqlite` database file
# count by date in datetimes
# df_2007
# Load the query results into a Pandas DataFrame.
# Train for one epoch, just the embedding layer
#alias the datacube to something more wieldy and pass a string for reporting purposes
#create arima model
# Filtrar solo las columnas que son FLG
#The train stations names need to be cleaned up
# 2. Each recommendation set shows from 1 to 15 Taskers, what is: # - average number of Taskers shown
# Find the key of the dict
# Groups already grouped dataframe by parent index (level=0) AND sums it up # gDate_content_count
# 7.
#Group male data by moon phase
#verify if connection is established #part 1- create session to connect to DB engine
# export # ca_de_xml.to_csv(export_path, index=False)
#find the shortest line
# Number of reports from the most active station
# dropping the "hour" column now that we no longer need it
# integer features scaling
# A:
# stack the various columns
# tables=np.asarray(list(Base.metadata.tables.keys())).astype(object)
#### Number of unique wells based on UWI
#read_hdf('store_tl.h5', 'table', where = ['index>2'])
# sentiment_df.head(100)
# Find user with most tweets:
# we cant work with the null value so we replace with them with a negative large number of value
# data scraped from http://www.unicode.org/emoji/charts/full-emoji-list.html
#generate nodes weekly dataframe
#sn.distplot(a.A.flatten()[:],norm_hist= True,bins=60 ) #sn.kdeplot(a.A.flatten()[:]) #sn.distplot(a.A.flatten()[:], hist=False);
#There are too many document types to plot on one chart so select the types with highest maximum
#Cleveland': '0eb9676d24b211f1
# find historical data for 2012
# Count mentions only
#run for March users
## repeated user_id
#Estoy levantando 100 posts
#Using the statisics library
#Clean the data
# Establish a new column for person's age at designation
# Just do demonstrate one of the useful DataFrame methods, # this is how you can count the number of sources in each class:
# Re-format dates in date column # Rename columns before final export
# analyze validtation between BallBerry simulation and observation data.
# 6. 
# Change some of the parameters to see how the model fits
#Before cleaning data process begin, I will copy the contractor table to contractor_clean table  #to check original data or roll back the operation. 
# Save the DataFrame as a csv
#### sort by value
#Read canadian tire tweets csv file
# Visualize results
# the first entry in this Dataframe's shape gives the number of cards in the set, and the second is the number of card attributes (name, mana cost, type, etc)
# The wikipedia URL that every article has in common
# Split into test and train
# Calculate various qunatiles.
# shuffling the df to avoid time dependencies for now
# Simulate conversion rates under null hypothesis
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
# The mean squared error
#sort by CBS in order to create different color scatter plot
# Line equation function # test 0.2634
#this reads the json files from a specified column. In this case, the gps files from the validation study (+a few others #that we'll have to filter out later by created on datetime.
# see data in tabular form!
# Take a look at the words in the vocabulary
# plot the sum by day for June 2015:
# We import Pandas as pd into Python # We create a Pandas Series that stores a grocery list # We display the Groceries Pandas Series
# instantitate an empty list called data to store the result proxy
# what is conversion rate for Pold under the null
# Show feature properties
# Pickle the 'data' dictionary using the highest protocol available.
# which simply looks up the dunder on the class i.e.
# Create a new DataFrame with an entry per sentence for the training set
# Creating folds
# Setup Tweepy API Authentication
# Read the JSON file into a list of dictionaries
# If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.
#1 we are going to put some other values instead of NA
# Make columns for seasons and terms
# compute the number of mislabeled articles
# get now, and now localized to UTC
# TASK F ANSWER CHECK
#save cleaned data out to csv
#au.plot_user_popularity(very_pop_df, day_list)
# events
# Get the shape, # of elements, and # of dimensions
# If Q1 = yes; add 0 years to Q1A answer # assigning each answer to unique Q list
#check to ensure all the dummies were created successfully
#Woo hoo, it worked!
# 12 months before LAST record entry.
#'Detroit': 'b463d3bd6064861b',
#Return select rows AND columns using loc
# Convert minutes_list into a series # set to 'tripduration_minutes' which will be new column 'tripduration_minutes' added to df
# delete cached result:
# We can also apply functions to each column or row of a DataFrame
# sen_data.to_msgpack('full_DataFrame/combined_data/master/sentiment_data_2min.msg') # sen_data2.to_msgpack('full_DataFrame/combined_data/master/sentiment_data_5min.msg')
# We drop columns which give us a score for personality type
# examine rolling volatility
#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 6)
# check Initial condition data in file manager file
# population or sample size of treatment group
# Heatmap
# creating ad_source df
# Store which indexes need to be removed #check total row count
# Get a list of column names and types # columns
#'San Antonio': '3df4f427b5a60fea'
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
#split the dataset 
# only process the first three rows
# Data key value is a dictionary with a children key -- where all the post information is 
##### first examine loc()
# Ben helped me with this lovely for loop...
# get conversation length
# It seems people recently joined have performed most of the activity in rating or not rating Paula's profile 
# Read data into a DataFrame
# Find duplicated user
# prefix an extra column of ones to the feature matrix (for intercept term)
####TEST # b['Compound Score'].mean()
# Convert this cell to markdown and write your answer here.
# sentiment_df[sentiment_df['name'] == "CBS"]
# take a look at the first two lines #!curl 'https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv' -o Consumer_Complaints.csv
# creating a copy of dataframe df # dropping rows where pages and group don't line up
## print culstered result 
#Get the top 10 timezones of the tweet authors
# grouped_by_letter.get_group("O")
# # convert date columns to datetime 
# Need to determine a way to keep track of the session_id along with words... # This quick and dirty will just go through all the sessions....
#duplicates #Credit:Hector Ian Martinez
#-------IMport Countries-------#
# Nodes files
## Simple enough - let's use a loop and our new understanding of dicts to print out the mode and route name:
# descending order
# Read gzipped file in a dataframe # compression="infer" tells pandas to look at file extension, which is "gz"
# tweetsIRMA = pd.read_sql("SELECT tc.tweet_id,tc.longitude, tc.latitude, i.lon, i.lat, sqrt(pow(tc.longitude - i.lon,2)+pow(tc.latitude - i.lat,2)) as 'distance', atan(tc.latitude - i.lat,tc.longitude - i.lon)*57.2958 as 'angle' FROM tweetIRMA ti JOIN irma i on ti.irmaTime = i.DateTime JOIN tweetCoords tc on tc.tweet_id = ti.tweet_id",Database().myDB)
#pprint(api_json['messages'])
# Use the create_engine function to create the connection to the target database...Note: this doesn't actually open the conn.
#sc.stop()
# What is the median?
#getting dummies for payment options
# since this took forever, better save the model to disk
# Calculates the column-wise standard deviation, ignoring NaNs
# Add it to original DF
# Requirement #1: Add your code here
# Actual Test
# number of events, articles and mention_date of the dataset
# To test the slack technologies we now add a demand that we do not have yet added a technology
# from data frames expects index 0,1, ...
#Data from SERI report, table 3 #Difference between the SERI report and our values .
# Load in the cruise data file, without the header names # Instead let's specify the column names for just the ones we need.   # Let's show a snippet of what we have.
# inspect the first 10 rows # the printSchema() method tells you the data type of each column
# Remove the bullet point # Save the first name as its own column # Save the last name as its own column
# subset temp to us
# What are the different species of Iris and how many rows are associated with each
# mean US temperature grid time series # mean plot
#let's look at the data! start with weather data: pull for 2010-2017 for the central park station
#Convert the ActivityStartDate to a datetime object (currently a string)
# get the top 1000
# Make predictions on test data using the Transformer.transform() method.
# merge dataframes
#### Change name and drop [Neighbors_Obj]
# Creates a list for sentence tokens
sql = """SELECT * FROM $     feed_fetcher_feeditem T1 LIMIT 100$     """$
# View dataset metadata (in different way)
## Count the number of tweets per userID and get the first 10
# to get the last 12 months of data, last date - 365
#Collecting data from the FSE for AFX_X for the year 2017
# Let's say you want to get the distinct count of unique entries in the OWNERS # table:
#Importing URLs from PC_World_Urls.pickle created by PC_World_Scraper_FindProductURLs.ipynb
# Where are the "outliers"?
## Writing to csv
#X_train['age'] = df_imputed.iloc[:,0]
# id and sentiment are categoricals
# Alternative way to get the date only 
# lets fit logistic model for the countries, baseline = CA
#  Invest at inprice, then #  assuming market conditions embedded in poparr2: #  ... effectively simulates 5,000 years of bootstrapped hybrid daily data.
# using numpy.random.choice(a, size=None, replace=True, p=None) we take output 
#Tally the number of unique sites
# Append results to 'results_list'
# Simplify the dataset into question answering pairs # TODO: Filter out some long conversations that appear to be open discussions instead question answering.
# Non-numeric Columns # bulk convert all non-numeric columns to equivalent hash values
#2. Convert the returned JSON object into a Python dictionary.
# Filtering only for successful and failed projects #converting 'successful' state to 1 and failed to 0
# are there any values greater than 8?
# Max FAVs
# Save references to each table
# Never forget to map
# take logarithm of goals
#ensure column names do not have spaces
# select an expiration to plot # get the call options # set the strike as the index so pandas plots nicely
# %reload_ext autoreload # %autoreload 2 # from DeepText.preprocess import preProcessor_in_memory
# Fixing the datatype 
#'Orlando': '55b4f9e5c516e0b6'
# align train and test frames
#read in the projects file
# Bayesian 0.71106382978723404 # Logistic 0.74416413373860191
#Compute annual Nitrate concentration
# Joining ab test table with countries table
# your code here # use a hyphen to remove zero-padding
# how many values less than 6?
# Settings for notebook
#code to work with a polygon input
# delete the 'to be deleted' columns # no way to do this, so create a duplicate table containing the columns we do want # now we have a time series with blood glucose
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# The "description" column has the text that we want to analyze 
# Create engine using the `demographics.sqlite` database file
#plot the histogram
# 2. Convert the returned JSON object into a Python dictionary
# Create engine using the `demographics.sqlite` database file ### BEGIN SOLUTION ### END SOLUTION
# Print best error rates achieved
# save as pipe delimited # check if it worked
cur.execute("""UPDATE empvw_20$                 SET salary = 1000""")$
# Fit model (will take some time)
#learner.load_cycle('adam3_10',2)
# latest_time_entries from last 9 days
# teacher_info
# Get all document texts and their corresponding IDs.
# find historical data for 2004
# Find an attribute of this page - the title of the comic.
# TASK 1
# We need to save y values as trained too even though they are not scaled to avoid confusion with previous split
#Starting by looking at up votes, going to set it up by looking at the differences between #top fifteen percent and those that received 0 up votes (4888 vs. 6910 observations).  #creating 'viral' dummy
#inspect measurement table
# Task F answers
# make sure each game as only one label # if there is drop the last the was given
# Create an intance of the class
# essential imports
# this is text processing required for topic modeling with Gensim
# Add the 95 % confidence interval of the median.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Merge omdb
# Suppresing warnings for a "pretty output."
#Read tweets approval ratings from csv file
#timeconsuming
#in descending order
#merge of ordered timeline-shards no longer ordered for these users (iloc USER_PLANS)
# Creating a document-term matrix
# Percentage of is_attributed == 1
#Find the highest number of returning stations
# Make a list of all seasons
#Put all bus names in lower case
# series have properties: name, dtype, values
# results are returned as an iterable list
# create activity prediction matrix
#Churns cohort x cohort
# introduce some condition, e.g. only users with 14 or more consecutive days
#Tells us the probablity of a Reddit will be correctly identified in the class its assigned
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
#'New York': '27485069891a7938'
# plot basic histogram 
'''Performing Polynomial Features on the Pending Ratio Column'''$
## load isochrone into a geopandas dataframe
#Load the voltage profiles
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# show ohlc resampling
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
# Distribution of daily changes 
# give tweepy access
### Create the necessary dummy variables for landing page
# create new column "age" by subtracting fetched time - created time. # To make it readable, timedelta64 is needed. [m] gives by minutes-long, [h] by hours. 
# Merge to our features df
#Converting values to float
#2 Convert the returned JSON object into a Python dictionary.
# Specify a "cell" domain type for the cross section tally filters # Specify the cell domains over which to compute multi-group cross sections
# df_2009
# plot Series obj
#forest_clf = RandomForestClassifier()
# View the polarities
#create and display cnn sentiments datafrmae
# Logistic regression confusion matrix
# create an HDFS directory for this assignment
"""$ Print complete news titles$ """$
# I realized through working with this data in the past that Per Seat Price they give is not always correct.
# List stopwords
#Convert to numpy array and calculate the p-value
#Dictionary of Outliers for Time
# plot autocorrelation function for first difference of doctors data
#Click here and press Shift+Enter
# -> Being more anxious leads to worse outcomes
#all plan combinations in this set of users # plans,counts = np.unique(BID_PLANS_df['subs_array'],return_counts=True) #add 'Free-Month-Trial' to start of trial timeline 
#Save figure
#'Montgomery': '7f061ded71fdc974',
# get number of elements
# df.loc[df['column_name'] == some_value] # Reset the index numbers
## Store the imputed data for further processing
#9.6 Modify Object Properties # You can modify properties on objects like this:
# 10. 
# Type in the search bar, and click 'Search'
# convert to daily frequency # many items will be dropped due to alignment
#precipitation descriptive statistics
# seeing sections of the table in turn
#creating test and train dependent and independent variables #Split the data into test and train (30-70: random sampling) #will be using the scaled dataset to split 
# This isn't right. There are no long positions. Take another look at the code and fix it.
# Copy dataframe # Remove incriminating rows
#%matplotlib notebook
# get the unique number of games that Jimmy did play in
#Temperature - fill gaps in primary temperature under 1 hour
# Display the entries where close is greater then 80
# Create a list from 0 to 40 with each step being 0.1 higher than the last
# and then reload it...
# Print all of the classes mapped to the Base ### BEGIN SOLUTION ### END SOLUTION
#Atlanta
# todos os concursos de 2017 # soma dos valores da sena distribuidos no ano de 2017
# Gets historical data from the subscribed assets, the last 360 datapoints with daily resolution
# Split data into train and test data
#reading the data #combinign the two tables into single table
# Create df from rf model predictions of test set data
#Most recent date by descending.first
# fit tfidf dataset
#df1.loc['2017']   'the label [2017] is not in the [index]' #df1.loc['year'==2017]        KeyError: False #df2017.head()
#Test to make sure connection is working.
# find historical data for 2013
# movie meta-data
#file = open(today + '-' + hour_minute + ' ' + site_name + ' 1 title_page.txt','w')
#how many wells that were good at time of measurement do we predict to fail now?
#client = MongoClient('localhost', 27017) ## dropping a database via pymongo #client.drop_database('test_database')
# Look in the utils library to see how we implemented the solution.
# data munging # satisfaction
# set the simulation start and finish times
# Frequency specifies the length of the periods
#In order to do an inner join, there must be a common key in both datasets.  Base the common key off of the school name. #This cell changes the name of the school column so that the inner join can be completed .
#### Define #### # Convert tweet_id column from int obejects to string objects# #### Code ####
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Use Pandas to calcualte the summary statistics for the precipitation data
# Using set math to find out which columns are common between both DataFrames
# Calculate the sample variance.
#checked values using (len(new_page_converted))
# Arrow positions per country combinations
# note that we use ascending = False becase we want people iwth largest number of things to get higher ranks
# show the first 5 rows of data frame
# Collect data for ticker AFX_X for whole year 2017
# Read the data file and save it to a dataframe called "df".
#percent_quarter = percent_quarter.apply(lambda x: x > 1)
# check inserted records
# Reading back in with chunks
# concatenate dataframes with different rows
# Train the model using the training sets
# Replace the negative values with average values
#1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD). # Use requests package
#Create a barplot for sentiment analysis and approval
# In Pandas DF # Average all polarities by news source # View the polarities
#Check via customer sample
# Reading data
# Let's make some recommendations
# we will create the indices from the  # product of these iterators
#tweetdf = pd.read_csv("tweets_w_lga.csv")
# We display the salary distribution per department per year.
# Concat All Events and Validation Events
# same if we enter a list as column index # (which can easily happen inadvertently)...
#Normalize nested data structure to a data frame format.
#LR.score(X_test, y_test1)
#Initializes the PixieDust monitor
# return NaT for input when unparseable
# We also load avro-transitions.csv
# as it can be seen, the star should be in the stop word list
# Replace each NaN with the value from the previous value along the given axis
# Define endpoint information.# Define endpoint information.
# Set storage path # show the new storage path
# Reshape the array into a 4x5 matrix
# make a GET request
#Trading volume occurs at [6] #Using the statistics library
# Your friendly tokenizer # Numpy
# Load and peek at your data. Change the file name as needed. 
# calculating number of commits # calculating number of authors # printing out the results
#Create empty df template #df = pd.DataFrame(columns=['Link','Title','Publisher','Journal','Authors','RDate','PDate']) #Or import df from pkl file
#Save node information. The columns to be saved in this file are set by the user. Note that node_type_id is needed here
# Show best number of trees
#### Define #### # Remove all tweet record that have a non-null retweeted_status # #### Code ####
# Plot causes of death by year
#join the data frames
# Total duplicates in name column
# removed string word comments
#process things
#will create percentile buckets for the goal amount in a category
# create a series to work with
# Add the average.
#Sharpe Ratio #Menor SD
#Change duration of simulation from 3 seconds to 0.5 seconds
# cisuabe5
#Chicago
# convert date from string to datetime
#Aggregate Compound sentiments
# How many stations are available in this dataset?
'''Ridge'''$
#Check shapes of array
#searching for href
# Break the text content apart for better formatting.
# 75 rows and 22 columns
## We can use the /agences resource of the Index API to get more information. ## Below, we combine the previous steps into one line and ask for the agency associated with the first feed:
# scrs < 3m / all scrs, since 3-01-2018 # 50% scrs start their scns too early
# remove outliers
# In this previous case the positional index is the same as the name of the index # To see the difference, let's create a new dataframe with just the last 5 rows:
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# count the NaN values in a columns
# Store the cross section data in an "mgxs/mgxs.h5" HDF5 binary file
# custom grids for every degree # vectors # mesh grid array
# len(xres3) # len(xres3.items()) # object.keys(xres).length
# bb type
# Renaming the columns to date, tobs
# compute the mean of complaints per quarter... # note this doesn't make sense, but works anyway
# Choose a start date and end date for your trip. Make sure that your vacation range is approximately 3-15 days total.
#getting summary of large dataset
# note the index added as a primary key # this is required to make certain calculations -- it will be added automatically
#inspect station data.
#Count and plot source url
# your code here
cur.execute("""select text, geo, coordinates, location $             from tweet_dump where coordinates->'coordinates' IS NOT NULL $             order by id DESC limit 5;""")$
# extracting environment variable using os.environ.get
# Stops that are in oz_stops but not in stops # Note: these stops do not exist when looked up online!
# Creating DF which can be used in models
## Libraries 
#compare summaries for Matthew 1
# get the days_plan_adead column using trip_start_date - timestamp
# read in a (large) convolutional neural network model # this will only work after the CNN model is downloaded (~800MB) # e.g. python -m spacy download en_core_web_lg
# set SUMMA executable file
# Setting up plotting in Jupyter notebooks # plot the data
# Look at #535, to see if the cleaner function got wrong stuff out
# Here it should have merged 2001 to 2001 index and so on because all the other valeus are the same but it did not.
# Import CSV file
# We create a dictionary from a list of Python dictionaries that will number of items at the new store # We create new DataFrame with the new_items and provide and index labeled store 3 # We display the items at the new store
# call edftocsv function
# Multiple Ensemble genes map to the same Hugo name. Each of these values has been normalized via log2(TPM+0.001) # so we convert back into TPM, sum, and re-normalize.
# Visualize
# Now we are picking up the random code that is used for every youtube video
# Device
# Assign the Measurement class to a variable called measurement
# split into training and test sets using dedicated wrapper
# Reactions anzeigen
# Create the grid widget
# Don't forget to drop nulls! 
# a Jupyter thing to send the encoding to Arvados via CWL.
#Variables to populate the metadata in the SB Item #Retrieve Acquisition Date
# y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag) # y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)
# Run this cell to download the data
#what is th eofficial Water Resources Station Catalogue ID of site '105001'?
#Investigate the semantic space
# a 10-row mini test df
# an attribute is changed within the string-like object
# create dataFrame from ndarray
# check inserted records
# save model as...
# create the training, test sets # we don't need the date columns anymore
# highest temperature recorded
# Apply the wrangling and cleaning function
# Create the boxplot # Display the plot
#one popular person in und... let's see this famous tweet #perhaps 'und' is just 'undefined'. The two heavily retweeted tweets are actually RTs themselves!
# set c to the minimum
#Access state lookup table
# We insert a new column with label shoes right before the column with numerical index 4 # we display the modified DataFrame
# Setup Tweepy API Authentication
# Find the max of the 'Average Fare' to plot on y axis
# roughly 7.5% of users are classified as active under the current definition
#merge datasets
# view vegetation for June 6th 2016
#### This adds a column that says whether a row is closer to the bottm or the top of the well #### This is useful for doing creation of features of rolling windows where you want to avoid going into another well stacked above.
# example of a STD customer's order history
# create range of indexes (by default dayly timestamps):
# Generate entries in column "technology" according to energy source "hydro"
#Plot the results as a histogram with bins=12
# Create a scatterplot matrix
#Select lang field and counting returned values from the table
#Long Beach': '01c060cf466c6ce3'
# favorite table # for row in table_rows: #     print(row.text)
# later on the first row will be an issue; 1.0 isn't a valid number
# Different way to plot
# concat df and coming_next_reason
# a Series can be added as a column to a DataFrame
# Show results
# Read .csv files from kaggle
# Summary statistics for the percipitation df. 
#Sharpe Ratio #Menor SD
## one way or another, by this time, we can merge in the list version, cpdi, to get ## a complete 'eppd' dataframe.
# 'None' is treated as null here, so I'll remove all the records having 'None' in their 'userTimezone' column # Let's also check how many records are we left with now
# There are not any null values
# Create a new dataframe that retains only songs that are in the number one position (for each region)
# approximation to avoid a second matrix # multiply the maxarg by factor, sum across and divide by same factor # all matrix elements lower than argmax will contribute very little
# Create a training DataFrame
# build dataframe # view column information
# Converting the index as date
# Get coordinates of FSRQs
# creating bands csv
# To get you started, here is how to test the 5th scanned value.
# Use the Inspector to explore the database and print the table names
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Converting date to a DateTime Format
#Check that the filters worked
# Read one site as a CSV
# Width # Color # Round values
# Writing to .csv file
#df_train = df_train.drop(columns=columns[19]) #df_test = df_test.drop(columns=columns[19])
#
# check shape
# Find unique users in the dataset.
# Thre seems some missing values filling by 0
# load panda dataframes into sqlite databases
#NB: mean accuracy is higher because it is choosing the best prediction row by row. then comparing with actual category.
# Baseline score:
#check if any null entries exist
# Remove duplicates
# the target 
# normalizing values
# suburban
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Create new column with clean text # Add a column nwords 
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data # Get the count of total number of records in the dataframe
# read in the data from a file # display the table of data
# sample queries 
# group by multiple columns
#using groupby function to check values
#### Test ####
# Add one column named "Total"
# 2. Convert the returned JSON object into a Python dictionary #this seems to easy... 
# This is what the table looks like
# tempsApr and tempsMay are valid Series objects
# Descriptive statistics
# get only the index of the Series
#x = df_stock1.filter(['Date'], axis =1 ) #y = df_stock1.filter(['Close'], axis = 1)
#id_str is meaningless, so we're getting rid of it. 
# Create a one-dimensional NumPy array from a range with a specified increment
# migrating table info # for row in table_rows: #     print(row.text)
#dates = sm.tsa.datetools.dates_from_range('1980m1', length=nobs)
# feature transformation
## Patch of the sky in the Milky way observed by OGLE:
#So it looks like we can go ahead and delete the duplicates (it's safe)
# Load model.
# Fit pipeline 
# Just my repos
# Run this cell
# these are the columns we'll base out calculations on.
# Count the total number of tweets
# Manipulation methods: #Apply
# fig1.show() # Pupper is the most populate stage
# View data attributes
# check our features
# Perform a query to retrieve the data and precipitation scores
# We won't use these dataframes anymore
# create some noise
# Save data to Excel
# get value counts
# Store link
# This will give the total number of words in the vocabolary created from this dataset
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("song_tracker.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
#Get the station_id and name for the station has the highest number of observations
#Retrieve the news paragraph for all NASA articles
#df_low_temps.head()
# step 1 - create boolean Series # step 2 - do boolean selection
# imprimir a soma de todos os ganhadores da sena  # imprimir a soma de todos os ganhadores da quina # imprimir a soma de todos os ganhadores da quadra
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("hit_tracker.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
#authentication #connecting to Twitter API
#joined_test[v] = joined_test[v].fillna(0).astype('float32')
# extracts newest and oldest tweet dates from dataframe for display in local timezone
# We replace NaN values with the next value in the row
# Stats
# Identify incomplete rows
# Use Pandas to calcualte the summary statistics for the precipitation data
# create a new column that we'll make the index so we can merge dataframes
# Turn the dummified df into two columns with PCA
#remove links
#Convert unix date format
# Setting up plotting in Jupyter notebooks # plot the data
# dummying month_bought 
# Taking home team to be the one we are interested in 
# Ok, but maybe a bar plot would be better.
# Create a copy dataframe of the origin # Collapse the dates # Showcase the new copy
# Pull in a brand new ticket # prepare(new_ticket)
#total=total.dropna()
# summarizing the corrected timestamp column
# reducing our "train_4" sparse matrix which has already been scaled # this crashes the kernel at 1000 and 100 components...
#'Scottsdale': '0a0de7bd49ef942d'
# Loading data
# We can display the message data in JSON format
# apply preprocessing.scale to your X variable # y has already been created, which is the label
# Dropping the id column from test # Now we can verify that "train" and "test" have the same number of columns, as expected # We can also verify that the "submit" dataframe has the same number of rows as "test"
#Try indexing the other way -- still only 950 matches
# are all values less than 10?
# skip 100 lines, then only process the next five
##### modify to use variable Year as index
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#Change date to datetime
# filter only domains that need to be shortenened
# how does our result look?  Should get something like this.
#Save a references for the ff tables:
# First, create new columns with the year of each date columns
# Give it a second to load the page
# Dicts can contain other dicts, like in the case of the plan:
# Push the sentiments DataFrame to a new CSV file for each vader and Watson
#Wow that is so small!
### Remove all Blends and Wine Types made with more then one Grape
# Calculate % change from quarter to quarter, find those with change greater than 100%
# Examine GDP Series
#res3.text
# Drop meaningless columns
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#### Define #### # Identify and remove erroneous dog names # #### Code ####
#Columns not being written to file
# Move this to text as an aside
#Read the first 5 records
# Display of first 10 elements from dataframe:
# reduce the dimension of the feature for the test data
"""$ Run preliminary test of classifier to get baseline accuracy$ """$
#Contains key search term
# by year and in-season vs. offseason
# Freeviewing
# Design a query to calculate the total number of stations.
# Separate response variables from predictors # Split the training data into training and validation sets for cross-validation
# rimuove tutte le interruzioni (spero)
# predic y 
# First option - the slice object # arr[slice(None), 1] is the same as arr[:, 1]
# Create dataframe
# copy the datasets for cleaning
# Using matplotlib, let's plot the data # By default, the indices will be on the x-axis, and values on the y-axis
# Creating dummy variables for country column
# We're plotting them all at once so... there are going to be some visual issues. # Volume distorts the visualization because those values are much larger than the prices. # Let's put all those other ones on a plot.
# Train test split 
#### Test ####
# Statistics on each column
# Information of duplicate row user_id
# newdf.get_group("2018-02-27 05:06:00")
# - median  number of Taskers shown
##### Your Code Here #### ###########################
#Reading CSV files in the DIR
# Put the data into HDFS - adjust path or filename if needed
# Read the newly created csv back in as "numberOneUnique_df" for later merging with the original "spotify_df"
# check out state abbreviations
# And so, it can be used with .loc to get the index by its name:
#rename columns #print head of load
# save the model to disk
#cost for the period
# created a dataframe with the text for each article and then combined the output with the metadata into a second dataframe  # dropped url, headline, type_material (all Op_Ed) from the dataset
#### Define #### # Replace underscores '_' between words in p1,p2,p3 columns with ' ' #### Code ####
#babies uid == BABY_UID
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# logistic regression model # fit model
# make sure all column names are the same and in the same order
# Station Analysis # - Design a query to calculate the total number of stations.
# defind simulation data
# correlation matrix
# examine the first 5 rows
#Stores file in the same directory as the iPython notebook
# How do those 54 recipes look?
#how many rows do we have with nan in a given column?
# Import inpatient census extract
# Display the data table for preview
# Display confusion matrix
# Get Sobeys for walmart tweet #Read Sobeys tweets csv file
# for better logging information
# sort df by label
# Create number purchases per user feature
#train on 20m data
# Mapping using BLAST
# coordinates to a 2 dimension array # check dimensions
# Creating a sentiment dataframe # Writing sentiment data to a csv file
# Mean Value of pnew and pold
# Count hashtags only
# print some sample training trees
# Since the underlying data of Fundamentals.exchange_id # is of type string, .latest return a Classifier
# apple_data.info
# builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# Load JSON as Python Dictionary, for data in 2017
# get one venue's information
# Save file to csv
#lv_workspace.set_data_filter(step=1, subset='B', filter_type='include_list', filter_name='SEA_AREA_NAME', data=include_WB)
# Solo Analizar el periodo 201701
# set the simulation start and finish times
# Load Data Set # Add column on which to build pivot table # Sample data
#app_ids
#checking most recent record date using the -1 indice
# calculating or setting the year with the most commits to Linux
#save the data df to json
# Convert sentiments to DataFrame
# Get a list of column names and types # columns
## So setup our graph so that we have unique users, tweet id's, and hashtags
# a timestamp with both date and time
# We can also select which data from a dictionary we want to put into the data frame # use the keyword column
# check Initial condition data in file manager file
#sc.stop()
# Anything less than 0 means that the stock close was prior to acquisition.
# get indices for the rows where the misc headers are present # these will indicate the end of data
#map titles to edge From column
# Create Database Connection
# A scratch cell used to pull up details of a table column as needed  #voters.E1_110816.value_counts(dropna=False)
# Test function
# Max RTs:
# text cleaning imports
#make a copy of dataframe #converting normal values to log values using numpy's log function to remove anamolies
#print first 10 lines of first shard of train.csv
# Store the API key as a string - according to PEP8, constants are always named in all upper case
# Naive Bayes
##### transpose the dataframe
# Note: Count can't be greater than 20
# find campaigns with under 50 users # remove users with those campaigns, and those with 'unknown'
#There are too many document types to plot on one chart so select the types with highest maximum
# check if shapes match
# concatenate by columns
# Imports the methods needed to abstract classes into tables # Allow us to declare column types
# your code here
# -r stands for 'recursive'
#add it to the csv
#Perform a basic search query where we search for the '#flu' in the tweets # Print the number of items returned by the search query to verify our query ran. Its 15 by default
# remove items shipped before our start date # remove items shipped after our end date
# export/ create the processed dataset
# Print the street number
# Apply pre-processing
#data
# accediendo a varias columnas por el nombre (label)
#**Which Tasker has been shown the most?**
#export Data Frame into csv
# remove URLs and twitter handles
# Import top50 datasets # Remove URL form top50 (optional)
# create category column # create single name column
# Generate some samples from the state space 
# add a notebook to the resource of summa output # check the resource id on HS that created.
# Write to CSV -- to insure code runs if there is no internet connectivity and API can't be called
# validation set score
# Count mentions only
# Sort the dataframe by date
# Iterate through all the columns # This is an example of not choosing to save the image
#Example 5: Import File from URL
# Collect data from FSE for AFX_X for 2017 with API call
#print(reviews) #print(reviews_vector)
#Export to CSV:
# add a EVA as a material
# and distance.
#try with a csv
res = sqlCtx.sql("""SELECT host, stddev(result)$             FROM tempTable$             GROUP BY host""")$
# A nice thing about Pandas is that series and dataframe methods output new series and dataframe objects # So we see that the type of value_counts is itself a series:
# Query to calculate the total number of stations # Counting and grouping operations in SQLAlchemy
# Load the model that we created in Part 2
# Calculate time laps in Eric's data: # created times variable containing timestamp data for Eric # Create elapsed_time list containing time elapsed between next timestamps 
# read the data and tell pandas the date column should be a date # in the resulting DataFrame
# instead of going through output piecemeal, let's look at a summary: # print a summary of the fitted model
# Convert this cell to markdown and write your answer here.
#Most commented posts
#Sort CON by Contact_ID and Opportunity Name
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# these are the important words in the title
# SAVE A CSV OF THE TABLE ("number_one_chart.csv") TO COMBINE WITH "song_tracker.csv" LATER
# Choosing only slected columns from
# Now make a masked array to find movies to recommend # values are the movie ids, mask is the movies the most # similar user liked.
# Find State according to Address1 and City and fill in 
"""$ Dates go until 2018 and are mostly unique.$ """$
#Apply functions to create new columns in dataframe 
# Reading the dataset in a dataframe using Pandas
# Check is Data is imbalanced
# check Basin variable meta data in file manager file
# Retrieve the parent divs for all articles
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#type(df_concat)
# Joining two data frames
# midToto is 13, therefore user can change each layer value from 0 to 12
#And to check the tail
# Create df with datetimes range as index
#from pandas.tools.plotting import autocorrelation_plot
# Write an aggregation function that finds the mean price per minute and the total volume per minute - output this to a new DF
#Read the last 5 records
# However, non-numerical columns are simply omitted:
# Instantiate the count vectorizer with an NGram Range from 1 to 3 and english for stop words. # Fit the text and transform it into a vector. This will return a sparse matrix.
# Loading a JSON file
# Show duplicate data entries
#read in the member file
# attempt to pull out any duplicates based on text data
# Column names on the raw data file
SQL = """$ SELECT rating, COUNT(*) FROM film WHERE rating = "PG" OR rating = "G" GROUP BY rating;$ """$
# Display Time-Series data price of Ethereum in Bitcoin
# URL of page to be scraped
# remove outliers
# December 31, 2017 at 7:03pm from Online Store...unix timestamp datetime funk?
#  set the index to the date column.
# Read the data file.
# check inserted records
# checking the null values in the DF , according to Ben this is the fancy one
# terminate the spark session
# Reflect Database into ORM class
#combine so all charges have a proper unblended rate
#check name for consistency
# Add year/week
#### Test ####
#file path
# Open specific netCDF file
# df.groupby([df.created_at.dt.month,'product_version']).count()['Id'].reset_index(1)
# import Quick Inventory of Depressive Symptomatology-Self Rating
# Collect data from 2017
#for post in posts.find({"author": "Mike"}):
#Las Vegas': '5c2b5e46ab891f07'
#Lexington-Fayette': '00ae272d6d0d28fe'
# View the game_data shape: (43727, 161)
# Assert that there are no missing values
# let's build a model with GPU
#Write to CSV
#### Results for 09-05-2015
# Perform a bsic analysis of variance (ANOVA). # C(x) refers to x as a categorical variable.
# logistic regression # fit model
#Saving the output to CleanedMovies.csv in an Output folder
# Average those cities together to get the default usage value.
# IQR
# All rounded players..
#### Test ####
# drop last row
# Saving our dataframe to file
#Drop rows with all NAN
# Transmission 2040 [GWh], marathon
# What does this look like with 15 topics (potentially the knee)
# The protocol version used is detected automatically, so we do not # have to specify it.
# rearrange column order to group releveant columns together
# remove selected workspace #ekos.delete_workspace(user_id = user_id, unique_id = workspace_uuid, permanently=True)
# set the index to date
# Return descriptive statistics on each column of the DF
# Fill the address from table [EDW].[SHIPTO_DIM]: CustomerID = '0000100273'
# Identifying the top 10 authors # value_counts returns object containing counts of unique values # Listing contents of 'top_10_authors'
# To talk to a database, pandas needs a connection object passed into its # methods. We'll call this conn. # Now conn is connected to a database file called geo.db
# a number (float)
# Visit URL
#df_unique_providers.drop('level_0',axis=1)
#Calculate average math score
# Clean up rct
#Melt with 2 variables
# check that this code works as intended
# Convert sentiment means dataset into DataFrame
# Attach tokens to the dataframe 
#count of converted users with new_page
# Average daily trading volume
# creating array for ARIMA modelling # ARIMA model #printing parameters
# Lets add a new column to the dataframe
# Load the previous query results into a Pandas DataFrame and add the `trip_dates` range as the `date` index
# Transmission 2030 [GWh], late sprint
# Perform a second groupby command on the 'data_FCInspevnt_latest' table # Filter Inspection_duplicates for those with an a sum of the Inspection Number greater than 1, i.e. a duplicate entry
#plt.hist(tag_degrees.values(), 50, normed=True) #Display histogram of node degrees in 100 bins
# Plot all columns (default)
#count of users with new_page
# Now, creating a Dataframe for the post: "FCC Announces Plan to Repeal Net Neutrality", # Note that the column named "index" describes each row's index from the source, aggregate Dataframe.
# instantitate an empty list called data to store the result proxy
# save the file to the output
# Put the data into HDFS - adjust path or filename as needed
#print(agg.fit_predict(locationDistanceMatrix_norm)
#count of converted users in control group
# create pySUMMA Plotting Object
# are all values in each row less than 8?
# Convert df to csv
# we can also change this in place, by setting the parameter inpace to True inside the drop method
# Create similarity measure object in tf-idf space
# find historical data for 2014
# convert date
#downloaded dataset for daily price
#Tweepy Cursor handles pagination .. 
#sort by CNN in order to create different color scatter plot
# describe dataframe
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
# instantiating and training the Word2Vec model # getting the training loss value
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
## Transaction: Title
# We load fake Company data in a DataFrame
# change default figure and font size # visualize the feature importance of every variable
# Use start_date and end_date parameters to retrun data for 2017 only 
# basenames = [os.path.basename(path) for path in cris.SpeechCollector.audio_collection['audio_path']] # basenames[0] in basenames
# 3.2.C OUTPUT/ANSWER
# Plot the histogram
# Create gensim dictionary object
# Query for most recent date in Measures # Get string value of recent date from tuple return by query # Get string value of 1 year prior to recent date
# take the publications whose publication date is yonger than x days
# standardize column names
# List the stations and the counts in descending order.
#by Kadhim Ajrash  and Khalid Al Ansary, 23 May 2017
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
#probabilities are skewed left, 
# load the model from disk
cursor.execute("""SELECT * FROM coindesk""")$
# Log to Database
#print the rows with index label 12
## Write pandas dataframe to a Google Sheet Using df2spread: # Insert ID of Google Spreadsheet # Insert Sheet Name
# change table formatting         #qt_convrates_toClosedWon.columns = convrates_to_closedWon
#weather_snow = weather_df['SNOW'].resample('1M').mean() #weather_snow = weather_df['SNOW'].resample('1M').mean()
# for each reprinted card, the first reprint has the completed printing/rarity dictionary, so we can get rid of every other duplicate
# summaries
#take a peek at each individual track
#Writing Pickle file 
#Twitter accounts in the decreasing order of frquency tweeting about Donald Trump
# Write the test results  #output = pd.DataFrame( data={"id":testdf["id"], "sentiment":result} )
# quick and dirty investigation for 'RegDateOriginal' and 'RegDate'
# freq='BM' stands for Businees end day of month # the last BM of each month in 2017
# run the model giving the output the suffix "rootDistExp"
#Example:
# train score
# Predictions for the first customer across the first 5 items
#(p_diffs<p_diff_abdata).mean()
#Temperature - fill gaps under 3 hours before aggregation
#  ENCOURAGE re-running this cell, like playing with a kaleidoscope: #  Visually see the increased "choppiness" due to GM(2) overlay. #  Local "downtrends" are more likely. 
#get count by month of each term
# take a look at the dataset
# Show data range
#query for precipitation data based on date range from most recent to a year before #create data frame from sql query
# Open Fermi 3FGL from the repo # Alternatively, one can grab it from the server. #table = Table.read("http://fermi.gsfc.nasa.gov/ssc/data/access/lat/4yr_catalog/gll_psc_v16.fit")
# Double check if our cleaner functions works well by looking at how many tickets by # of words
# make my acutal outcome column y
# affair =0 means there is no affair and rating are good (>=4) #and womens having affair have rated thier marriage <=3.5 on an average 
# Filter down to just Ductless units
# add ToT sum by student_program # drop duplicates on student_section, reduce columns
# Option 1 is the generalized solution to reorder the index levels # Note: We're not making an inplace change in this cell, #       but it's worth noting that this method doesn't have an inplace parameter.
# Format bill ids to match the bill ids taken in by the API
# elegant way of computing the sum columnwise (in case all are numeric): 
# Find and click the full image button
#5a. You cannot locate the schema of the address table. Which query would you use to re-create it? 
# asfreq :: you can convert one period to another eg. quater period to month period # following will change freq from quater to month  # how :: this will decide which month freqency needs to be created from quater weather it is start of month or end of month
# Display unique values with counts for ticket_status
# Split data into train and test datasets
# Assign the station class to a variable called station
# and average temperature most active station
# set up a new dataframe
#select URL to gather info
# summarizing the converted timestamp column
cur.execute("""DELETE FROM coindesk;""")$
# Retrieve last 12 months of data, last date - 365
# Convert JSON object into Python dict
# variables = arr[0].keys()
# Give our chart some labels and a tile
# Load data into dataframe
# Data correlation # corr method gives us a correlation between different columns
# where's my favorite beer at?!
# 10M word/token corpus  
# Sort data frame by target and tweets ago
# don't convert anything and return the original input when unparseable
# Give the file a meaningful name
# dataframe of negative examples
# 4 adding axis to the equation. It will copy vertically
# Save DF to csv file
# confirm which conda environment you are using - make sure it is one with SpaCy installed # if you have difficulty importing spacy try the following in git bash # conda install ipykernel --name Python3
# How many learners were in the housing trainings project?
#     print(screen_name)
#big_data_station_mean = big_data_station.EXIT.mean() #print('Min:', big_data_station_mean.min()) #print('Max:', big_data_station_mean.max())
# Create new dataframe that only contains rows with tipPC < 200
# provides info related to the dataframe df_CLEAN1A as follow: 1k observations, each column has datatype of numeric(int64) and also how much memory is using in our machine. 
# Accuracy: 48.86% for doc similarity # Accuracy: 39.39% for skills
# Print a sample of it:
#6d. How many copies of the film Hunchback Impossible exist in the inventory system?
# Find the probability indiviual treatment group they converted
# find historical data for 2017
# Average Order Value of _New_ Customers who made their first purchase after taking Discover # new_discover_sale_transaction.groupby(by=['Email', 'Created at'])['Total'].sum()
# Import the pandas package, then use the "read_csv" function to read # the labeled training data
# R-Blogger news #print(type(soup)) #print (soup.prettify)
# 4. What was the largest change in any one day (based on High and Low price)? # Alternative solution using list comprehension.
# We load Google stock data in a DataFrame # We print some information about Google_stock
# create a Series with a PeriodIndex
#get projection information from windfiled
# instantiate the algorithm, take the default settings # Convert indexed labels back to original labels.
# Merge files by station
# Match project id with project name
# drop useless categorical column (because we have dummy/indicator variables)
#Creating slice from the df for the second table
#2
# Create logger
#Export Canadian Tire file to local machine
# Use a colour-blind friendly colormap, "Paired".
# To find value of n_old
#request 2017 stock  #convert to dict 
#df.index.names = ['timestamp_ix', 'rank_ix']
#make a single dataframe with all 3 indicies #take a sample and show it
#create the x,y training and testing variables from the dataframes. The size of the training set will be 80 percent #the size of the testing set will be 20 percent the variables
#Save latest weather info in var mars_weather
# datetime of the tweet object is in UTC (Universal Time Coordinate) # so I generate a column for the time of the tweet in Pacific timezone
# games without events
# Date limebike started differentiating between bikes & scootscoots
#Estoy levantando 1000 posts
#del festivals.index.name
# Calculate the average chart upper control limit.
# Create a pandas dataframe from the model predictions (prev numpy array)
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# take only elements that belong to groups with a group sum greater than 100
# we simplify the purchase history to only include user_id & product_id
# Print the lat/long
# Using the station id from the previous query, calculate the lowest temperature recorded, 
# Convert from reviews to features #X = vectorizer.fit_transform(df.review)
# Here we are merging the new dataframe with the sp500 adjusted closes since the sp start price based on  # each ticker's acquisition date and sp500 close date. # .set_index('Ticker')
# Appending train and test to get full dataset for cross-validation
# first thought is to check out some trump tweets
#convert dictionary to dataframe
# rolling sum over data for each hour
#Read in the results.csv
# SVC with tfidf
# Calculate difference
# flight.describe().show()
#getting dummies for hand drive
# figuring out how to sort friends_n_followers by number of friends
#Save Date in Pickle and CSV format
# Probably want to create a datetime object for March 31st 2017 # create a logical column with the datetime object and a timedelta object
#Plot the results using the DataFrame plot method.
# Return the list of movies with the lowest score:
#  At this point, the user could specify their mean and sigma, #  but we shall use actual values computed from 51-year history:
#train test split, standardize data
# 5. 
# Normalization for non-object features - Numeric featue normalizatoin
# Specify multi-group cross section types to compute
# Example
# fit the model
#####list the datatypes 
# abs is actually a numpy function so it can also be implemented as follows
# for appending rows you can use .rbind()
#Now I need to index the dataframes by their datetime columns
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#write out the source data onto disk #however we want to write only the records which are duplicates. Better idea to remove the non duplicates.
# collecting data and converting to Python dictionary
#content_analysis_save = 'wikipedia_content_analysis.html'
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD). # https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-05-30&end_date=2018-05-30&api_key=sKXGJG7ybc76fKLMSfxc
#check missing value
#rename column headings
#Apply length function to the review column
#wav_version = pydub.AudioSegment.from_file("tests/test_data/acoustic/wave0.wav", "wav") #wav_version.frame_rate #wav_version.channels
# favorite table # for row in table_rows: #     print(row.text)
# using the rest of the training data
# Create DataFrame copy grouped by Categories
# write out our formatted sentence using an f-string
#Fort Worth': '42e46bc3663a4b5f'
# time between questions posing and first answer
# rename Tip_Amt to reflect new value: TipPC
#model_w.summary() # For non-categorical X.
# split and dummy bands column
# combined_df5
#We can now list the counts of records by confidence code
# load the trained model
#Perform a basic search query where we search for the #kingjames in the tweets #%23 is used to specify '#' # Print the number of items returned by the search query to verify our query ran. Its 15 by default
#pnew conversion rate:
#Create a data frame from the Row objects
# group by SN and find average of each group
# find each of the inconsistent rows in this horrid table, which is now in a new place # for row in table_rows: #     print(row.text)
# And it's tail:
# Requires DSSP installation # See the ssbio wiki for more information: https://github.com/SBRG/ssbio/wiki/Software-Installations
# Retrieve the first [0] record and store it in the latest date variable # Use strptime function to store the formatted date in the format_latest_date variable. We will use this in our code later
#Quandl_DF['Date_series'] = pd.to_datetime(Quandl_DF['Date'])
# change the data type of publicationDate column
# Summary statistics of overall data-columns (where the data-type can make such statistics to happen)
# I am giving our data a name and instantiating a SQL table
#print all paragraph texts
#Now let's get the genres of every movie in our original dataframe #And drop the unnecessary information
# Call method 'score_all_on_classifier_by_key' on our classifier object. # It calls method "score_all" from class 'Classify' # 'score on all the test and validation sets' 
# Check for missing data - none
# QUERY METHOD!!!
# Train our SVM classifier                  
#create GeoDataFrame for Chicago wards
#Testing % change calculations and trading logic
# Location outside US, ignore
# plt.scatter(f1, f2, c='black', s=7) # Data Normalization
# alldata # https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas # df.loc[df['column_name'] == some_value]
#create time series for market difference 
    """for every sample, calculate probability for every possible label$     you need to supply your RNN model and maxlen - the length of sequences it can handle$     """$
# How many usrs in df_log
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#'Philadelphia': 'e4a0d228eb6be76b'
# Save references to the table
# Make columns with country average
# We print the number of non-NaN values in our DataFrame
# import pickle # # Getting back the objects:
#This is what the table is going to look like.  #There are nine categories. Above is a short description of each column. 
#'Oakland': 'ab2f2fac83aa388d'
## Reading the conn logs for our analysis
#usage_400hz = add_plane_data
# Extract specific values - 1
# read the parameters of the character n-gram extraction module
# What is the minimum value?
# Load query data into a dataframe
#We must convert the Polarity to string to be used as a map attribute
#Have to run later, no national events in training set
# This graph is the right data but the x and y axis are not on the right scale. Notice the date span on the x axis # Notice the y axis labels
#TEST #results = Geocoder.reverse_geocode(df['latitude'][0], df['longitude'][0]) #31.3372728, -109.5609559
# identify wells with two measurements
# Encontrar el 20vo registro de mi DataFrame
# Write to CSV
#Read in data from source 
# delta represents the temporal difference between two datetime objects
"""this is what worked to create the existing file $ read in SHARE json records and set encoding to utf-8"""$
# Second option - the IndexSlice object
# chaining head onto the frequency count
#Use a lambda function to assign values to water_year2 based on datetime values
# Calculate Probability of old landing page
# For the rows where treatment is not aligned with new_page or control is not aligned with old_page # we cannot be sure if this row truly received the new or old page. # adding rows where <group> equals 'control' and <landing_page> does not equal 'old_page'
# DC Min and Max # Election periods: scottish referendum (pre premiership), pre 2015, and pre 2016 referendum
# 'stoi': 'string to int'
# Print the first Tweet as JSON:
## Using Dow 30 dividend data sourced from yahoo finance
# Access the first row by positional index:
#Changing the Winner with 0 and 1 #if winner is first Pokemon then Winner is 0 else 1
# Second Approach, using Doc2Vec and LSTM for predicting
# Using proportions_ztest(count, nobs, value=None, alternative='two-sided', prop_var=False)
#Testing tvec
# models trained using gensim implementation of word2vec
# Let's train popularity model
# Keep only top50 datasets in dict (and convert int to str)
# connect to MongoDB cluster
#Remove unnecessary columns
'''$ u.data     -- user id | item id | rating | timestamp. $               The time stamps are unix seconds since 1/1/1970 UTC  '''$
# Add code here # Get the accuracy of each model and the overlap between the model
# We create a column with the result of the analysis:
# Prepare the data
# Use the forest's predict method on the test data
# Import package # Build DataFrame of tweet texts and languages # Print head of DataFrame
# Find z-score and p-value
# Lenghts along time:
# http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/01_the_machine_learning_landscape.ipynb #x_train = np.float32(x_train) #x_train = x_train.reshape((len(x_train),1))
# plotting the 20 most frequently used tags
#task 6:  What was the average daily trading volume during this year? #Create an empty list named traded_volume to hold the 'Traded Volume' data for the whole year #Calculate the average daily trading volume #Calcula 
#a final check to make sure everything looks okay
# to get the last 12 months of data, last date - 365
# create train and test
# count = merge_table['S&P500 Open'].value_counts() # len(count)
# create lookup key in HCAD dataset
# Display min & max wavelengths
# Listing top 10 authors
# Define the columns and set the index.  
#validation set accuracy
#array created of every variable that is related to review scores #rows that are missing any of the above variables are dropped from the data frame
#fill NANs with 0s for Total Row
# Convert into log2(tpm+0.001)
#create a Poisson variable called observation. #we combine our data (count data), with our proposed data generation scheme (lambda_)
### when you are done, call conn.close()
# Which were the 3 most popular trainings? (Trainings with the most learners)
# Plot scores obtained per episode
# Compute the error for every 1000*10*10 generated datapoint. # Then flatten them all out into a list of length 100,000.
# most of the usernames are in 2 words or 3 words.
# Initialise with a scalar. #
#  This will totally remove all duplicates 
# print a single tweet json for reference #print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
# calculating number of commits # calculating number of authors # printing out the results
#No Clue
# how many columns and rows do we have in data?
# Make predictions using the testing set
#It's another video.
# OAuth process, using the keys and tokens
#find any numbers within the posts
# Calculate the date 1 year ago from today
# let's declare those files above as variables in Python.
#load the correction factor table from the jupyter notebook `MassFlowCFFinder`
# converting to date to pd date time
# Earliest Date
# Generate a dynamic list of tickers to pull from Yahoo Finance API based on the imported file with tickers.
# merge dataframes
# this model doesn't perform as well # i think additional variable will be needed to improve this
# Get the tweets above. 
# Note A1 is a counted allele and usually minor. (A2 is usually major)
# m['created_date'] = m['created_at'].dt.strftime('%Y-%m-%d')
# Printing the content of git_log_excerpt.csv
# Count hashtags only
# Baseline score:
# sort grid_pr fires by lat/lon to get projection right
# different than find_one
# Check to make sure everything worked out
# Checking our shapes. We see that "test" has an extra column, because it still has "id". We'll drop this column next  # after using it to prepare a submission dataframe, where we'll put our predictions
# now, we can map the numeric values v in a sentence with the k,v in the dict # train_x contains the list of training queries; train_x[0] is the first query # this is the first query
# Apply the count function # Seeing what DummyDataframe look like
# Subset to Enrollment Level only # remove colums with all na
# Flatten out the 'sentiment' dictionary column of the 'tweets' DataFrames
# praw object for script to use Reddit API
# plot new grid
#reading csv file created by webscraping
# print a sample tree in tuple format
# Count mentions only
# Retrieve the last date entry in the data table
#Example 3:
#  Large "repeat" values obviously increases computing time... #  Repeatly generate daily prices for one-year, record gmr each time.
# Find the link to more details about the Avengers movie and click it
# Query to find the count of stations 
# drop rows with missing specialty
# checking for duplicate entries
# I couldn't get the #s and the dates to match up (1 more line in the counts which is like the indexing) # saved them both to a .csv and then combined the csv worked though
# Create new Data Frame
# Count hashtags only
# Save dataframe to csv 
# Load the dataset
# Checking our shapes. We see that "test" has an extra column, because it still has "id". We'll drop this column next  # after using it to prepare a submission dataframe, where we'll put our predictions
# load the corpus
#create BeautifulSoup object and parse with html
# Creating dummy variables
#write your dataframes to a csv file, if you want to be able to use it later. # Write out the DF as a new CSV file
# Crear 2 nuevas columnas llamadas longitude y latitude, basados en el campo place
# Plot a run chart, using the index for x.
# series index by index #max of scns created
# drop the identity column from progresql
# Sort the morning traffic and observe a cutoff point to classify the extreme value as erroneous
# One hot encoding
#These orders do not exist in shopify
# Drop rows with duplicate names
# your code here
# Grabs the last date entry in the data table
# save df2 to a new csv file # also specify naming the index as date
# Setup Tweepy API Authentication
# Extract slice - 1
# drop low because it is really common #df7.loc[df7['avg_health_index Created'] == 'low', 'avg_health_index Created'] = np.nan
# Train model
# Coin DB # COIN_DB.to_csv('coin_list.csv')
#Check that you are using the correct version of Python (should be 3.5 for these tutorials)
# How many usres in df_users
# Create a directory to hold the data # Download the data
# correct joining
# remove unwanted fields
# You can use the axis argument to get the maximum per row instead. #
# Display the MongoDB records created above
# get the final train dataset
#transit_df_rsmpld = transit_df.reset_index().set_index('DATETIME').resample('1D',how='sum')
# Save the dataframe as a csv file to be used elsewhere
#'Stockton': 'd98e7ce217ade2c5'
# kfold cross validation
# Create an array of 10 random integers between 1 and 100
# Check to confirm distance was properly added to our list
# let's generate a series with a hierarchical index: 
# Calculate h_theta -- Predictionof a row
# Display name and importance
# freq='B' is business days
# most common hashtags
#create time series for data. Not usefule so far:
# And the iloc method for selection by position
# Create an engine to a SQLite database file called `hawaii.sqlite`
# define the data path # load json data # view the first 5 rows of data
# Armamos un dataframe con los tweets recortados #we will structure the tweets data into a pandas DataFrame
# unhide the strings in jupyter notebook # assign all new functions to df # new_df.head(1)
# Find quarterly average
# first attempt, try to include all of the punctuation on the title as the valid datasets # but beforehand, we need to remove the None and NaN val
# !git clone https://github.com/s0yamazaki/WallClassification.git
# # Getting back the objects:
# Read the data
#== By Label: Same as above, but faster 
#firebase.patch("Exhibitions/-LFlR_PhbP2eWNCGPZeu",new_data)
#cleaning the dataframe containing dates based on the data we saw 
#Importing rating dataset from Data folder
## expand full path to token ## create env variable TWITTER_PAT (with path to saved token) ## save as .Renviron file (or append if the file already exists)
# Find the max and min of the 'Average Fare' to plot on y axis
#Put all data into dataframe sentiments_df
# generate sample # Compute and display sample statistics
# set appoinmtemnt duration column to hours
# fig.savefig('toma.png', dpi=300)
# To get the total number of duplicated under user_id 
# return the dtypes
#identifying unqique instances where new_page does not line up with treatment
#autheticate
# Use a colour-blind friendly colormap, "Paired".
# a series' index
# Printing the content of git_log_excerpt.csv # ... YOUR CODE FOR TASK 1 ...
#renaming columns of the derived table
# confirm records were dropped
# Creating time stamp format column using the column 'Time stamp'
# Import the data from the station table into a dataframe
# Add code here
#Combining all three datasets(tweets1,tweets2,tweets3) together into a single dataframe named tweets.
# flight_pd.head() # help(pd.read_csv)
# - Design a query to retrieve the last 12 months of temperature observation data (tobs). #   - Filter by the station with the highest number of observations.
# export lm_withsubID
#origine incident is not independant from target
# From the docs: "Max id returns results with an ID less than (older than) or equal to the # specified ID. 
# Perform a query to retrieve the data and precipitation scores
# fig.savefig('toma.png', dpi=300)
# https://stackoverflow.com/questions/40894739/dataproc-jupyter-pyspark-notebook-unable-to-import-graphframes-package
# Copy the data of interest - with headers - into clipboard with Ctrl+C # Run this... ## Copy the Excel table to the clipboard first.
#df.parse(sheet_name)  # read a specific sheet to DataFrame
# Statistical Summary  of Precipitation. 
# Load uber data
#(r_forest.columns[['id','id_str','screen_name','location','description','url','name']], #r_forest.fillna(r_forest.mean())
# Scalar ranges can be set with a simple tuple: # Or more complex ranges can be set using one the of the ranges objects:
# Reflect Database into ORM class
q="""select capital_gain,capital_loss,(capital_gain+' '+capital_loss) Net_capital_Gain from adultData"""$
#This will give the total number of words in the vocabolary created from this dataset
#Honolulu': 'c47c0bc571bf5427'
# dish it out in snappy parquet for comparison
# convert an h2oframe to pandas frame (make sure H2OFrame fits in memory!)
# How many stations are available in this dataset?. Calculating # of stations in the full measurement table.
# calculating number of commits # calculating number of authors # printing out the results
# Use csv Writer
# 15. Create a Data Frame, called demographics, using only the columns sex, age, and educ from the free1 Data Frame.  # Also create a Data Frame called scores, using only the columns v1, v2, v3, v4, v5, v6 from the free1 Data Frame
# We extract the mean of lenghts:
#  numpy.setdiff1d() returns the sorted, unique values in ar1 that are not in ar2. # genco ids from paired cameras which do not correspond to shopify order ids
# combine these two equal sized frames
# Use `engine.execute` to select and display the first 10 rows from the table
# after comparing other models with the previous ones, # it turns out that reducing the number of features did not improve the result
# Drop non-normalized scores of Brokerage and Betweenness
# Create a dataframe named free_sub, consisting of the id, country, and y columns from free_data.
# create_database('mysql://root:root@localhost:3306/stocks_nse')
# plot de predictions.grade si predictions.prediction 
# If the condition is True then the row will be selected
#cascading operations
# best/worst styles, minimum of 5 beers drank
# now we have negative values, so we need one more transform before we can run MultinomialNB
# training 
# We Create a DataFrame that only has selected items for both Alice and Bob # We display sel_shopping_cart
# Here it can be seen that less popular people have liked Paula's profile and more polar people doesn't.
# Define random seed for reproducability # Define sample size # Generate sample with replacement
# summarize the data
# concatenate tc physcial and electronic dataframes into one
#tree_clf.fit(X,y)
# Get the shape and # of elements. shape() gives the number of rows (axis 0) # and the number of columns (axis 1). These axis indices are important later.
# check the area of each hru to calculate areal average ET # read the area of each hru
#df = pd.read_csv('statcast_0817.csv') # using a csv of statcast events April 2015 - Aug. 2018
# Print the index of airquality_pivot
# Not really able to tell with such few responses in so many categories. # I imagine we'll get something similar if we pull NPS scores by countries as well...
# read the csv from S3 # display the first 5 records to verify the import
# By default the max() method will calculate the maximum value in each column. #
#check for any null values in the trimmed dataset 
# Calculate n_new and n_old
#Create dataframes
# Find columns with null values #new_df.isnull().any()
# View the data type
# Incase the street length was wrong for any street, we take a median for street length for each street
#Line plot of log_AAPL
# Sampling the Recommendations provide using the item:desc dictionary we had created earlier
## Sort each tweet value by its relative timestamp
#Simpan data ke CSV
#== Filling missing value 
# que created_at sea tratada como fecha # La convertimos:
# Extracting FOX news sentiment from the dataframe
# Retain only traffic reading at 6AM and 12PM
# Save the query results as a Pandas DataFrame and set the index to the date column
## Time series
#calculate the mean absolute error
#Max and min time
#Convert to seconds
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# 1 quarter changes regression: later half of the sample
# Notebook customizations
#qrt = closePrice.resample('Q').mean().plot
#drop rows with null values in dirty columns: 27662565 records #drop rows with values in dirty columns: 26290342
# Export CSV
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
#url_votes = grouped['net_votes'].agg({'total_votes': lambda x: np.sum(x), 'avg_votes': lambda x: np.sum(x) / len(x)})    
# didn't have much luck removing columns # nor renaming the headers but we take care of these problems later
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# used to check that we have converted the objects into datetime rows from above properly
# coefficient of determination R^2 of the prediction.
# Make a pivot table containing ratings indexed by user id and movie id
# #read windfield geotiff
#  Population size should double, expect (32084,)
# Note: Count can't be greater than 200
# count how many values you have per column # We print the number of non-NaN values in our DataFrame
# read parquet file with arrow
# Create a Spark DataFrame from Pandas
#Read the Prometheus JSON BZip data
### Test ####
#find earliest create date
#Compute the sigmoid function
#this is the plot for the full data range which is the default value so have not given the start and end 
################################ # create law file and fix law account number
# remove zeros
# Global Variables for eacch file # Empty List to store the cumulated jams and irregualrities data
# inspecting df2
# Create a pandas dataframe # Display first 10 elements of the dataframe (ie. 10 most recent tweets)
#Do the same for male
# Add the topics columns back onto the original dataframe
# Examine Consumption Series
#active
# export the Geodataframe as shapefile
# Requirement #1: Add your code here
# Now we'll try "device_id" - about 700k values
# null value check
# Display the results of the Language Stats analysis.
# counts number of stations
#sum() #Returns the sum of the values for the requested axis. By default, axis is index (axis=0).
#df.head()
# review small df window after engineering lagged features/target vector(s)
#Diplay the top model parameters
## check to make sure NaN appears for the first row for an SCP
# Convert to pandas dataframe
#save data to a csv file
# Create Geometry and set root Universe # Export to "geometry.xml"
# removing un-activated users 
#1.Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017  #(keep in mind that the date format is YYYY-MM-DD)
# Create a pandas dataframe as follows: # Display the first 10 elements of the dataframe:
# logistic regression with tfidf #%%time
# remove invalid data points (negative intervals)
# Wait, a tomato isn't a vegetable! # This method will replace all instances of 'tomato' with 'pea'.
# mean deaths per year:
# we will remove a number in our array
# Merge libraries_df with metadata
# Add Distinct Users
# Check proportion of calibrated minutes vs not: 1 id calibrated, else 0
# Convert data into a DataFrame
# reset index to date
# inspect Meta Data
# process missing value
# Check column names # Change column name
# Build an LSI space from the input TFIDF matrix, mapping of row id to word, and num_topics # num_topics is the number of dimensions to reduce to after the SVD # Analagous to "fit" in sklearn, it primes an LSI space
#get station count, has been checked with measurement station count#get sta 
# Logistic Regression.
# Determine the column index for 'Close'
# result = pd.concat([Aust_result,Aussie_result]) # result.head(5)
# check option and selected method of (12) choice of hydraulic conductivity profile in Decision file
# combine lat and lon column to a shapely Point() object
# new_messages.watermark = new_messages.watermark.map(lambda x: x.replace(second=0))
# Print some info from the first tweet:
# Save our dataframe as a CSV
#### **Below is the screenshot of the MongoDB structures showing list of dataset**
# Create number purchases per product feature
# Create an extractor object: # Create a tweet list as follows: # Print the total number of extracted tweets
# Example
# For Displaying Data
# First, find a tweet with the data-name `Mars Weather`
# use the model to make predictions on a new value
# The case of "1) variable (time, hru or gru) and more than 2 hru
# without resample
# Find the dates
# subset temp to us
#dropping! the smallest number
#load objects
# pytz
# delete by row label
# Time format x-axis to 12-o'clock time
#we extract menas of the length
#we seem only to be interested in 'first_name'
# Create copies of original dataframes
#Histogram of tweet lengths
# set up tweepy
# Manually train a logistic regression classifier with the training set (supervised)
# Colors # Define x-axis and xlabels
# drop NaN values for purposes of analysis 
# Task 2
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# 10-fold cross-validation with the best KNN model
# read movies of small dataset
# Let's see the graph that our regression created. 
# default behavior is dayfirst=False
#see how many counts are negative 
# cumulative mean of the MC estimates
# line plot: x-cordinate = index, and y-cordinate = Age
#display(dflong) #dflong.Date.unique()
# get graph Facebook
# Printing the content of git_log_excerpt.csv
# Load the data from the query into a dataframe
# slice as column index: a dataframe
#las columnas ciudad y mapacalle tienen demasiados NaN como para aportar alguna infromacion relevante
# cisuabf6
# Query for finding the most active stations # List the stations and observation counts in descending order
#Any funny minumum values left?
#Using cursor to connect and query the newly created table
# here's the index
#Import Random Forest libraries 
# Define endpoint information.
# test the resample dataset
#train_df # all_train_df = pd.read_csv(slowdata + 'train.csv', index_col='id', parse_dates=['date'], dtype=dtypes) # train_df=all_train_df[-chunksize:]
#inspect df
# using seconds as unit for more precision
# Setting up plotting in Jupyter notebooks # plot the data
# Assign NaN to all entries in L3 equal to ' '
# 1 quarter changes regression: full sample
# Replace year of birth according to correction we received from the Portland Police Bureau.
# stock_data['TIMESTAMP'].iloc[1:10]
#abc = detailed_confirm_write_to_file_ALL_DRGs(50030,2014,6) # 50030
# The method .get_dtype_counts() will help me to see the number of # columns of each type in the DataFrame
# Create a new dataframe by dropping rows with NA data
# Use Pandas to calcualte the summary statistics for the precipitation data
# likes and retweeets
"""$ Check results$ """$
#### Test ####
# Create a minimum and maximum processor object
# Train it over a desired number of episodes and analyze scores # Note: This cell can be run multiple times, and scores will get accumulated
# Data
#post request to chart_data with chartParams, gets back data
# There is missing data again, so we drop them as well:
# get the column names
# Import two methods from the DOTCE Class `report`.
# elms_all are last two weeks' elms data
# Get indexes and values
# split one review into separate words # remove stop words from review text
# Read in reflectance hdf5 file
# Use Pandas Plotting with Matplotlib to plot the data # Rotate the xticks for the dates
#connecting to the sqlite table hawaii_hw that was created in data_engineering notebook, then connecting to it
# Remove rows with missing values in column 'driver_id'. The order was not accepted. there was no trip
#pd.to_datetime('11/12/2010', format='%m/%d/%Y')
# SMA: 6, 12, 20, 200
#url for mars news
# Grab a single record to get a sense of schema
#Which_Years_for_each_DRG.head()
# tensorboard = keras.callbacks.TensorBoard(log_dir="../logs", write_graph=True, write_images=True, histogram_freq=1)
#print first ten rows of theft data
# show first 5 rows of data frame
# display unique values with counts for zip_code
#check values substituted
# Retrieve the parent divs for all articles
# read in pickled emoji dictionary I created from emojis in the dataset. I want to use # each emoji as an individual feature.
# Divide each number of each countries columns by it's annual maximum 
# Make sure there are no duplicate keys
#bow = bow.rename(columns = {'fit': 'fit_'}) #X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:,2:], bow.iloc[:,1])                                                   #id_vals=X_test.index
# calculating or setting the year with the most commits to Linux
#using the count method to get the number of elements.
#You can load json, text and other files using sqlContext #unlinke an RDD, this will attempt to create a schema around the data #self describing data works really well for this
# Mentioning the dat range also
# split into conversation and last message dataframes # users_df = df[df.conv_flag==0].drop(['conversations','conv_flag'],axis=1)
# 1000010 is a standard id, so it haven't been considered as mispelled id in the dataset
#URL's of pages
#absolutely useless, must be a better format
# Drop new column
# driver = selenium.webdriver.Chrome(executable_path = "<path to chromedriver>") # This command opens a window in Chrome # driver = selenium.webdriver.Firefox(executable_path = "<path to geckodriver>") # This command opens a window in Firefox
# Export to csv file as backup file 
# What is the sample standard deviation?
# Count the number of stations in the Measurement table
# Set representative sequences
# The protocol version used is detected automatically, so we do not # have to specify it.
# Copying dataframe in df2 # Removing increasing rows
# index positions correspond to integer values stored in vocab dictionary
# Save submission
### Create the necessary dummy variables
# axis=1 apply function to each row
# Exogenous transmission capacities in 2025 per region [MW]
# Get list of messages
# Obtain the source names for reference
# Imprimimos porcentajes:
# Min and Max
# count occurences of uppercase words, which may indicate rage or anger
# remember from above that beers below 2.0 are very rare, so let's use that as a cutoff point
# Creating a dataframe to house the data
# Replace NaN in Field4 with "None"
# Provides useful statistcal data on the data set
# Get categorical codes for date columns
# periods will use the frequency as the increment
# write clean data to a csv to open in Rstudio
# save the file to the output
# read in our raw CSV to DataFrame
# check if any values are present
# Declare the constants. # Type I error, alpha level, significance level. #bootstrap_number_samples = 10000
#feature_set  # uncomment feature_set in order to illustrate.   May require notebook reboot depending on memory. 
# Create DataFrame
# Your Code Goes Here
# Another way to withdraw is by using the class name itself as follows:
# How many stations are available in this dataset?
#TODO: get qty rejected # get ncrs
# Word frequency for terms only (no hashtags, no mentions)
# parse the sentence
#add birthdate for subscribed-babies
# get multiple sections with the term fees
# adding prefix MONTH_ to column names
# print(i1)
#Fresno': '944c03c1d85ef480'
#Send the requests and translate the response
# Convert "date" data to datetime and set index on date
#idx = pd.IndexSlice
# get multiple sections with the term fees # use SpaCy to determine what type of fees
# Verifying 
# Creating new df with only a few wanted cloumns
#Show the first 4 rows (again): Note that if we omit the lower bound, it assumes it's zero
# default is freq='D' for date_range
#resets the index (for ease of use)
#the plot shows that there is almost no correlation between numeric variables
# monthly data #assert (fin_coins_r.index == fin_r_monthly[start_date:end_date].index).all()
#in descending order
#Male and crime only dataset
# rename the values column
# Assuming 0.2% of people that pass buy a crepe # Column "day_sales" in units 'Number of crepes sold per day in that location'
# key step!
#construct and display bbc sentiments dataframe
# A simple function for us to use in pandas' .apply() method.$ def stem_func(word):$     '''Stems a word'''$
# It appears that this stop comes up twice in oz's file. However, stop 7270 is not a real stop when looked up online.
# models trained using gensim implementation of word2vec
# 4. Which Tasker has been shown the most?
# export latest elms_all
# Get sentiment for Loblaws tweet #Read loblaws tweets csv file
# Fit the Pipeline on train subsets
dbquery = '''select * from mobileos$             join mobilebrowser on mobileos.id_os = mobilebrowser.id_brsw'''
#Creating new columns 
# ANSWER CODE CELL FOR TASK B INITIAL
# shows time stability
# fill values from backward
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# FutureWarning is OK: # https://stackoverflow.com/questions/40659212/futurewarning-elementwise-comparison-failed-returning-scalar-but-in-the-futur
# Checking whti a fake user whose craeted_on date is later
# how many unique authors do we have?
# Load the statepoint file
# TASK E ANSWER CHECK
#creating a backup copy of the input dataset
#transit_df = transit_df.reset_index().set_index('DATETIME') #transit_df_rsmpld = transit_df_rsmpld.reset_index().set_index(['DATETIME','STATION'])
# Baseline accuracy for this model is percentage of low vs high in the train set
# Double Check all of the correct rows were removed - this should be 0
# it took the mean of the columns, then the mean of those means # however, s1 and s2 have a different number of terms
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Compute and display sample statistics
# Describe the variable 'Number of people injured'
# compute difference from original dataset ab_data.csv
# Transform output into a classification task.
#That's even better! #ANSWER TO THE QUESTION take three: #Worse than before but a bit fairer.
# concatenates the individual files into a single df # csv files must be in your working directory
# tweets with hashtags or user mentions of apple or samsung
# Test
# We will join on the email so we need to make sure the email address column is formatted in the same way
# Average weight by position, from heaviest to lighest
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# flight2.dtypes # flight2.first()
# print(X_train.shape)
# Solution 2 : numpy diff function
# the most popular words which might contain useful information in the reviews
#create a dataframe with a random sample of the dataframe and run a test linear regression to determine accuracy
# plot autocorrelation function for therapist data
#checking that we are okay in terms of time sorting
# The base dataframe's end.
## combine data frame
#all_mapped = all_data.loc[all_data['rpc'].isin(mapping['rpc']) * all_data['store'].isin(mapping['store'])]
# Note 1: This Notebook is an exercise by following Walker Rower tutorial at https://www.bmc.com/blogs/amazon-sagemaker/
# identifying when new_page and treatment don't line up
#Question 2
# Verify
# query for the available stomatal resistance parameterizations
# change name of oz_stop id to stopid to allow merge
sql_query = """$ SELECT * FROM top_5_by_genre;$ """$
#the largest change between any two days (based on Closing Price)?
# export keys
# Save the workbook.
#run ML flow
# Initialise with a Numpy array. #
#INT is the contact information for INTERACTIONS #INT=pd.read_csv('C:/Users/sxh706/Desktop/Interactions.by.Month/2018/June/Interactions.Contacts.csv',skipfooter=5,encoding='latin-1',engine ='python')
# Step 7: Display the shape of the pivoted data
#los df que tienen la info del postulante (sin relacion con el aviso)
# points.iloc[0, 1]  # ERRORS
# Set the index to time, so when we save the data out we dont have an additional column with increasing numbers 
# ====== Reading files ======
#Create new dataframes in preparation of feature engineering
# save data to CSV
#inspect dataframe#inspect 
"""$ Take a small sample for plotting$ """$
# predict # round predictions
# The next line causes Pandas to display all the characters # from each tweet when the table is printed, for more # convenient reading.  Comment it out if you don't want it.
#ensure you have a full dataset, 100 per account  #len(organize_data) #save data to csv file 
# Send and catch the request: r
#merging the particpants column into the base table
# check shape of empty grid
# We extract the mean of lengths:
# bulk convert all non-numeric columns to equivalent hash values # X['ITEM_NO'] = X['ITEM_NO'].apply(lambda x: hash(x))
#Create a database and connect with sqlite3
#Saving Plot
#8b. How would you display the view that you created in 8a?
# Delete the collection
#**the 'tasker_id' column in the following dataframe represents the number of times the tasker_id has been shown**
### final list of chinese_vessels
# Initialize PyMongo to work with MongoDBs
# Read in 2018 data
#new_style_url='https://raw.githubusercontent.com/dunovank/jupyter-themes/master/jupyterthemes/styles/compiled/monokai.css'
##   Creating data frames directly from CSV
# Summary statistics of Precipitation Date
## create a list of top 25 users from 'users' DF
# verify the type now is date # in pandas, this is actually a Timestamp
# thats games without vectors, but with labels
# Total tobs
#giving the values and index itelf a label
# Remove row using drop ... axis=0
# 9. 
# true flag (T/F): negative class # predicted flag (T/F): positive class
# create with column names
# We count the number of NaN values in store_items # We print x
# Use Pandas to print the summary statistics for the precipitation data.
# to 45 minute frequency and forward fill
# save to hdf
# Find the max and min of the 'Total Number of Rides' to plot on x axis
#splinter
#print lxml.html.tostring(item)
# generate a Series at one minute intervals
# Create capture object for live capture
#Initialize the server 
#create the linear regression model using the lm.fit function on the x and y training sets
#We parse the date to have a uniform 
# removing NAs drastically drops count. Try imputation with mean value of sensor # check if operation needs to be performed. Losing out on valuable information
# Round capacity values as well as the efficiency estimate to five decimals-
################################################## # Load transaction  ##################################################
# day_map = {0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}
#using value counts in descending order. 
# Make output directory
# Use Pandas to print the summary statistics for the precipitation data.Base.classes.keys()
# print out details of each variable
# Run this cell to display all buckets.
#for prophet to work, columns should be in the format ds and y
#import vader for sentiment analysis
# pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Following','Followers','TweetCount'])
# Printing the content of git_log_excerpt.csv
# convert 'Date' from object to datetime data type
#'Virginia Beach': '84229b03659050aa'
# fill values from forward
# verify that we now have all the float columns we expected
# Delete rows/columns # pop -> columns # drop -> rows and columns by using axis
#retriveing data form bitcoinity.org
# your code here
# Creamos el dataframe con los tweets # Mostramos los primeros 5 tweets en nuestro dataframe
# Upload the raw Excel files exported from AkWarm Library.
# a series
# dropping some objects base on its index
# Access a collection (create it if not exist)
# create a new DataFrame from calls where the location below removed
#viz_1=sb.countplot(x=studies_b.enrollment, data=studies_b)
# create some noise with higher variance and add bias.
# shift just the index values
# end = datetime.datetime(2013, 1, 27)
## I need to create some files to use in the globing exercises
#Click here and press Shift+Enter
# Test
# option 1 (finds unique columns, duplicates rows)
#Reset the index from all the row dropping
# 3.2.C OUTPUT/ANSWER #bottom 10
#Qn 4 # What was the largest change in any one day (based on High and Low price)?
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# Returned as a python dictionary
# Set your x and y limits
#prob don't need # last_train = train_df.iloc[-1].name # last_train
# find historical data for 2009
#mktime(current_time.timetuple())
#### note the resulting index
#dftemp = df1.where(df1['Area'] == 'Iceland') ...... where command keeps array size same so not used here
# TASK E ANSWER CHECK
# Check if we are missing any classifier summaries
#get all the things you can run on this object #dir(festivals.index)
#info_final.drop('idpostulante', axis = 1, inplace = True)
# data science tweets
# purpose
# Custom Python Library Imports
# custom grids for every degree # vectors # mesh grid array
# Setup Tweepy API Authentication
# analyze validtation between BallBerry simulation and observation data.
# wide to long # view head of final row-wise dataset
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# set SUMMA executable file
#go to Mars weather's twitter page. 
# remove colums with all na
#task 3: Calculate what the highest and lowest opening prices were for the stock in this period #create a list to store the Opening prices for the stock #There are some None type values which we need to take care of.
## Sleep on rate limit is false because I check in my code
# Latest Date
# An efficient way of storing data to disk is in binary format.
# Retrieving the last 12 months of data, last date - 365
# Distribution of tweets per year
#2
#If you ran the upper code, the data can be directly converted to a Spark DF #Else, you can just use a pre-parsed .csv file available in the next codeblock
# add a notebook to the resource of summa output # check the resource id on HS that created.
#create arima model #model_701.forecast(5)
#Set up tweepy authentication
# Alternative way to get the date only 
# Merging actuals and backcast
#nt_price = pd.crosstab(nt.index.to_period("W"),nt.catfathername, values=nt["sq_price_value"], aggfunc='mean', margins=False).fillna(0.0)
# display unique values with counts for street_address
#Importing S&P Data
#Oh you pesky little one
# sice this took forever, better save the model to disk
#== Using isin() method 
#Check duplicated rows by contrator_id
# Positive Reaction ratio - Negative Reaction ratio  # Bi-variate relationship
#url for mars image
#read in the schedule file
#Renaming a column in files4  #Making  a jobcandidate column in files4 
#Creating more sensible and consumable features => time since promotion
# export 
# Capture shape of raw data
#### Define #### # Rename id column to keep consistency # #### Code ####
# Remove rows where df['Injury']==0
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Double Check all of the correct rows were removed - this should be 0
#Show the results; water_year and water_year2 are the same...
# To avoid ambiguity between index label and index, we can use the loc or iloc attributes. # loc means location and points to an index label # iloc mean integer location and points to a numeric index
# Dimensions. #
# Divide each number by each countries annual maximum
#Normalize the json into a DataFrame format
# check shape
# comparison to newer files (2017-10-15)
#Example2: Import File from URL
# Custom the color
#SQL Solution$ c.execute('''SELECT MAX(track_duration),artist_name, track_name from track_db''')$
#Combine The Data
# filter on term for 2017,2018,2019
# Define sentiment bins and groups # Add column "Sentiment group" to tweets data in sentiment_df
# Inside gamers
# get dr number of unique providers for each dataframe
# preprend to corpus
#Store Definitions in WML Repository
# Get avg compound sentiment
# print the p-values for the model coefficients
# Groupby average compound sentiments by Symbol
#check value by print(pdiff)
# Take a look at the submitter_user field; it is a dictionary itself.
# get max probability column name, this is final prediction for validation set
# Distinct authors
# Write to CSV
# pold == pnew
##Distribution of the proportion of companies with a corporate secret PSC at an address
# (d.year, d.month, d.day) >= (search_date.year,search_date.month,search_date.day)
# Pivot airquality_melt: airquality_pivot # Print the head of airquality_pivot
# TODO plot main sequence
## Find the stock with the minimum dividend yield
# This new column determines what SP 500 equivalent purchase would have been at purchase date of stock.
# Replace Nan's with values from the previous row or column with forward filling # Replace each NaN with the value from the previous value along the given axis
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Let's see how many years ago it started.
# sort values by date
# drop dry because it is really common
# load random forest classifier
#Create a mask of rows measuring NO3 #Convert the TotalN values in those rows from NO3 to N
# create the column timestamp with Hours for our dataframe
#@title Default title text
# look at unique regions that are missing data # this code might be wrong...
#verifying the counts equal the amount of rows of the dataset
# Keep relevant columns only (23 columns of 49)
# Empty search to ensure it is working # res["hits"]["hits"][-1]
# plot histogram with KDE curve
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# to get the last 12 months of data, last date - 365
#get highest interday change
# Making a test user dataframe and change [user_id = 2]'s created-on date
class Square(Rectangle):$     """ Simple circle, inheriting from Rectangle """$
### Substitute out replaced names in main dataframe under new column name ### And reduce dataframe to only include top 12 most common grapes
# n_old = old page count
# to 45 minute frequency and forward fill
## Top 10 Bloggers
# To flatten after combined everything. 
# read csv
# display unique values with counts for council_district
#proportion of p_diffs greater than act_diffs
#count of users in control group
# Change row & col labels
#import required modules from the scikit-learn metrics package
# Calculate the date 1 year ago from today
## create a new column 'score_str' which will hold a value about the score - is it below average or above average?
#Display the columns and their data types
#Fetching of data from yahoo finace
# outlier points
# converting the timestamp column # summarizing the converted timestamp column
# Delete items (returns a copy with deleted item -out of place!)
# Desktop with Development C++
#Dot produt to get weights #The user profile
# the randomly generated data from the normal distribution with a mean of 1 # should have a mean that's almost equal to 0, hence no error occurs
# Hacemos display de los sentimientos:
#Drop rows with invalid data
# Create average purchase per user feature
#Convert titles to a list for analysis
# Create a temp dataframe from our PCA projection data "x_9d"
# Check out the structure of the resulting DataFrame
#### Define #### # Change format in the 'time' column # #### Code ####
# Subset to only week 0 or over 12 # NOTE: week only available for self-rated, thus this selects self-rated implicitely # Subset to only those colums I will need 
# Job seems a good feature but needs to be converted into numbers, check how many distinct jobs DF has
# Preview the Data in the Measurement table
#Load the query results into a Pandas DataFrame and set the index to the date column.
# Identify possible issues with some columns
# data_air_visit_data[:3]
# Drop duplicates 2862 in user_id # Verify the drop action
# RTs AND SENTIMENT
#'Portland': 'ac88a4f17a51c7fc'
#now we create a pandas dataframe for the tweets #print out some examples 
#face detection attributes
# find all message with @someone
# tokenize the text
# Lengths along time:
### Fit Your Linear Model And Obtain the Results
# Open the dataset # Swap the dimensions
# find each of the inconsistent rows in this horrid table, which is now in a new place AGAIN # for row in table_rows: #     print(row.text)
# Test function
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# check inserted records
# Author: Steve Tjoa
# create a period representing a month of time # starting in August 2014
#select a row
# If Q4 = no; add n/a to Q4A # assigning each answer to unique Q list
#this is a little on-off switch for my various long run-time operations that are behind a check. #go_no_go = 0  #GO #runtime items will be necessary once each time after the kernel is reset.
# results = soup.find_all('li', class_='rollover_description_inner')
# create end date column from period index
#Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
# not only is Pandas much nicer, it also executes queries!
# Examining the paragraphs found in the body / the body data
# flight_pd.to_csv('C:\\s3\\20170503_jsonl\\flight_pd.csv', sep='\t')
# remove selected workspace #ekos.delete_workspace(user_id = user_id, unique_id = workspace_uuid, permanently=True)
#'Miami': '04cb31bae3b3af93'
# Write to CSV
#df.head()
#Or we can just show the count by a single column
# To ensure images in this notebook are displayed # properly, please execute this block of code.
# create grid id 1 to 1535 and save as type string
# flight = spark.read.parquet("C:\\s3\\20170503_jsonl\\flight.parquet")
# Remove rows with lingering null values
#2 drop duplicate records
# LOAD FROM HARD DRIVE
# Drop rows with label 0
#URL's of pages to be scraped
# Instances have a .__class__ attribute that points to their class.
# review small df window after engineering lagged features/target vector(s)
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
# The Directory to save the csv file.
# Export to "geometry.xml"
# 136 records removed
# Find the div that will allow you to retrieve the news title # Use this html, do not go back to the website to look for it # Latest news title
#Averages grouped by price
# what is the type of the index?
# Dump data to pickle # top_100_2016 = pickle.load( open( "top_movies_100_year2016_list.p", "rb" ))
#reorder+rename cols before saving
# Use Pandas to calculate the summary statistics for the precipitation data
# drop extra id column
# Novo Dado a ser rankeado # Transforming new_doc into vector (lang_idx, freq)
#Female and crime only dataset
# Fill nan with zeros
# Get genotypes that have associated blood type phenotype
#Remove na's
# make predictions for test data # Convert numpy array to list
# Convert into log2(tpm+0.001)
# get odds ratio for the age
# get final event of the PA, as this will say single/hr/k/walk/etc
# eliminar datos nulos
# Delete rows where 'Ov.transport status' is 5, 6, or 7.
# Of course we can aggregate the other way too
# list of workspace ids # workspace_ids
# pairings < 3m / all pairings
#  Map number of ratings to 0 or 1
# Probability of control group converting
# add back metadata for each order # make an additional column with just the year
#  The mean and sigma arguments are the gross descriptors of the GM(2). #  But note here that the sign of the mean here is REVERSED.
# Save corpus for later
# Save your regex in punct_re
# converting the timestamp column # summarizing the converted timestamp column
# drop low because it is really common
# .table() gives you the frequency count of each unique level
#Add list of nodes and edges
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# logging.getLogger().setLevel(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
# Requires GenePattern Notebook: pip install genepattern-notebook # Username and password removed for security reasons.
# Making df_test_index
# Find the longest name
# YOUR CODE HERE # raise NotImplementedError()
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
# customers
#final_df['mean'].values
#Reading all necessary CSV files from Scrapy in the DIR
# Print the tail of df
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
## Now try creating a dataframe that has the subset of crimes with the primary description CRIMINAL DAMAGE  # and the secondary description TO PROPERTY # Will need to google how to use & operator with pandas
# drop extra id column
# tweet date/times are reported in utc
# what aabout this 
# Get all of the SamplingFeatures from the ODM2 database that are Sites # Read Sites records into a Pandas DataFrame # ()"if sf.Latitude" is used only to instantiate/read Site attributes)
# this method displays the predictions on random rows of the holdout set
# Plot the Dew Point and Temperature data, but not the Pressure data
# Creating reference to CSV file # Importing the CSV into a pandas DataFrame # Looking at the top of the df to get a feel for the data
#Loading Data
# Note that this file contains 7 duplicates that are removed.
# Generate data
#change dtype to int64 #weird hack from H2o notebook
cursor.execute("""DROP TABLE coindesk""")$
# Perform a linear regression.
#Grouped by data in a dataframe groupby
#p['xx_1'] = p["xx"].shift(1) 
# withlocation[1]
# getting for each game the violance events level and the number of tickets #V.head(10)
# merge data
# We remove the new watches column # we display the modified DataFrame
# Run the testing model 
# Read the variation of electricity demand from Data.xlsm
# For retrieving all the URLs from the sitemap
# How's it looking?
# Split into test and train
#Read in csv file into two dataframes
# Correlation plots of all variable pair can also be made with seaborn
# SAVE A CSV WHICH NOW ONLY SHOWS DATA FOR SONGS THAT REACHED NUMBER ONE
# Define the authorization endpoint.
# output tells you index (row) 1 is Tuesday
# read in dataframe
# write making the worksheet name MSFT
# Save and show plot
#You can also use the offline_PSI_tweets() function to instatiate your data if its already downloaded ## tweets = offline_PSI_tweets(x)
# Now we'll try "device_id" - about 700k values
# We can even slice easily using the first MultiIndex (year in our case) # health_data_row.loc[1]  # doesn't work
# Save references to each table
# baseline accuracy
#print(dataframe.shape)
# Are the ids unique?
# control group converting mean
# distribution plot of the data
#print(highlight(json.dumps(jcustomer_info, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter())) # remove the comment from the beginning of this line to see the full results
# Make predictions on test data using the Transformer.transform() method.
# Remove these stops.
# separate out target column, rename w/ generic title
# set code, name, release date and # of cards for the 5 latest expansions
#group ride data by city # average fare per city # number of rides per city
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Extract specific values - 2
# dev4['rank'] = dev4['avg_rank'].rank()
# drop rows with missing value in specialty column
# converting the timestamp column # summarizing the converted timestamp column
#Check to see the change in Index.
# Search for '#Aussie'
# get the top model to use
# before_sherpa # after_sherpa
#import crime data by Chicago wards
#Removing intermediate columns created
#
# Take a look at our new dataframe
# Extract bad band windows
# set index of series c 
#join Shopify orders to Baby-camera Pair date 
# 181 retweet removed
# read, but skip rows 0,2 and 3
#Create BeautifulSoup object; parse with 'html' or 'lxml'
# simple pandas plot
# movie link
#Convert the date column into a datetime
#nyc open data API, you need apptoken to run this cell
# start_time :: start time of 2016
# Print the head of airquality_melt
#('J:\Financial Forecasting\Development\Electricity\Load Revisions\Q2_2017', index=False, encoding='utf-8')
# coordinates to a 2 dimension array # check dimensions
# perform update 
# Must set filter to subset for len(boolean) to work when calling get_filtered_data, if no filters are set, boolean is None
# Slice using both "outer" and "inner" index. #
# Importar el modulo data del paquete pandas_datareader. La comunidad lo importa con el nombre de web
# check the simulation start and finish times
# Likes vs retweets visualization:
# View the data and think about the transformations needed.
sql_query = """$ SHOW COLUMNS from sakila.address;$ """$
# setting X & y and tts 
#final.head()
#Export Canadian Tire file to local machine # Get sentiment for walmart tweet
# Counting the no. commits per year / without using the index # Listing the first rows
# check option and selected method of (11) choice of groundwater parameterization in Decision file
# df_2011
# Find the div that will allow you to retrieve the latest mars images
# again, default is to generate an error
#col.assign(rounded_dt=weather.datetime.dt.round('H'))
# Predict the values, using the test features for both vectorized data.
# Is is okay tha NaN in first_published_at converted to 1970-01-01 00:00:00.000009999 ?
"""$ Use this cell to run the preliminary test and calculate the baseline accuracy of the newly created classifer$ """$
#import into python
# Perform a MWW test # Print the p-value. Note I have specified I want to print a floating point decimal with 15 decimals after the period
#Use a lambda function to assign values to water_year2 based on datetime values
# Create a Beautiful Soup object
# TM Min and Max
# creating dataframe of matrix  and adding the confusion matrix
#compare summaries for Matthew 1-- one of the chapters describing birth of Christ
# instead of accepting the unwieldy labels we can define them
# ahora leamos nuestra copia. Todavia no incorporamos los dates en la importacion.
# Note: Count can't be greater than 200
# List comprehension for reddit titles
# busco duplicados en las fechas
# Linux / OSX # Windows     #! "c:/Program Files (x86)/GnuWin32/bin/wget.exe" --no-check-certificate https://pjreddie.com/media/files/yolov3.weights
# create a fitted model with three features # print the coefficients
# use average accuracy as an estimate of out-of-sample accuracy
# Calculate precision and recall scores with sklearn
# Requirement #2: Add your code here # updated by Huang, Silin ID: A20383068
# Setting up plotting in Jupyter notebooks # plot the data
# Get laste record entry date for temp
## total.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/target_winter2017.csv',index=False)
# plot autocorrelation function for first difference of therapist data
# Calculate Probability of new landing page
#builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
#Rename columns.
# You also can use the index to display any given time range.
# you can label the rows in the data frame
# Create the 'str_split' column
# Few tests: This will print the odd word among them 
# get 8-16 week forecast existing patients # keep only date in index, drop time
# ask what is the name of the country for each year # with the least life expectancy
# the file is a two-column csv: date, tweet
# mean_encoding_test(val, train,'DOW',"any_spot" )  # tbd #mean_encoding_test(val, train,'hour',"any_spot" ) # tbd # mean_encoding_test(val, train,'Block',"Real.Spots" ) # tbd
# Create data frame #Force column order
# creating ad df
# Looking at one tweet object, which has type Status: # example_tweets[0] # ...to get a more easily-readable view.
#CON is the CONTACT TERM information #CON=pd.read_csv('C:/Users/sxh706/Desktop/Interactions.by.Month/2018/June/Interactions.Opportunity.Term.csv',skipfooter=5,encoding='latin-1',engine ='python')
# get the local "now" (date and time) # can take a time zone, but thats not demonstrated here
#Most common title words 
qry = '''\$ DROP TABLE IF EXISTS lucace.disc_667_stage_00_list_of_users$ '''$
# use the keyword index
# Check for missing values # Extract data where is_attributed == 1 # Check NAs
# All only csv competitions
# Initialize Sentiment Analyzer
# sort value by grade column 
# create pca to explain 99% of data # fit our scaled data
#Make the df look nice.  
#Column clean-up
# Build Pie Chart
# View the precipitation table
# Listing top 30 most popular hash tags during the data collection period # "#guncontrol', '#guncontrol.', and '#guncontrolnow' should be count toghether. So should '#marchforourlives' and '#march4ourlives' etc
# Open the data file and read the contents into a dataframe #df=pd.read_csv("SFData5Users.csv")
# Drop NAs
#using groupby function to check values
#Example 6: Specify question mark (' ?') value as missing values
# import the data offset types # calculate one business day from 2014-8-31
#check the index and shape
# to force type of columns, use the dtypes parameter # following forces the column to be float64
# Find the station has the highest number of observations
# Drop columns with all NAN #c_df.head()
# Standard deviation of the residuals.
# First drop the maxtempm and mintempm from the dataframe # X will be a pandas dataframe of all columns except meantempm # y will be a pandas series of the meantempm
# midToto is 13, therefore user can change each layer value from 0 to 12
### Let's drop that silly row with huge snack expenses
# The number of different types of vote we are dealing with.
# describe dataframe
# Normal
# Rural cities ride totals
# Delete rows where 'Reason for rejection' is X1 or X8.
# Display a list of buckets.
# correlation between score and num_comments
# Search all the data from twitter related to given tag
#Limit data to those with more than 10 years data and still operating
# Smoker descriptive statistics
#Print accuracy
# Officer citations are wayyy skewed
# r squared value
# The correlations are the following
# Show rows with invalid data
##Extract year from Date
# Setting up dataframe
#hyperparam =[0.01, 0.015, 0.020, 0.025]
# The object type in the date column is a bunch of strings 
#Anchorage
#get large image
# determine the order of the AR(p) model w/ partial autocorrelation function, alpha=width of CI
#Remove header row
# lr.fit(features_normalized, paid_status_transf)
# this is the data per subject
# { "nameLast" : "Williams", "nameFirst" : "Ted" }
#Reading CSV files in the DIR
# get_data() is a function within Engine that makes a sql call to an API.
# show topics related to a word
'''based on what's seen in this column, there are about 862 null values for 'Footnote' but the rest are '*' values $ so it would probably be best to ignore this column for our analysis '''
# normalize original tweeters for 
# Calculate distance by coordinates using geopy.distance library
# find historical data for 2002
# Cleansing the Search Term for the records with SearchCategory of Plant, to only include plant id and removing everything else
# driver = selenium.webdriver.Chrome(executable_path = "<path to chromedriver>") # This command opens a window in Chrome # driver = selenium.webdriver.Firefox(executable_path = "<path to geckodriver>") # This command opens a window in Firefox # Get the xkcd website
# 4. 
# Find the max and min of the 'Total Number of Rides' to plot on x axis
# Replacing nulls with 0.
# instantitiate df object | conform to Prophet nomenclature
#rename the columns
# Apply smoothing function to each line, identifying transactions between Coinbase and GDAX
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data
# Collect the names of tables within the database
# Getting logs of usre 1
# Organize dataframe by date
## Fill NA values to avoid Null errors
# 5.What was the largest change between any two days (based on Closing Price)?
# Take a look at 10 observations with the following fields
# drop flagged indices
# data_store_id_relation.groupby('air_store_id').count().query('hpg_store_id!=1')
# Using our SQlite database created just prior, (hawaii), # ..  we now create the engine:
# Reading JSON
#install MXNet
q = """SELECT education, occupation, relationship FROM adultData """$
#Pull 2017 data
# Set up a collection name "test_database" in MongoDB
# New dataframe for rural data
# The Dataframe's text.
# check Forcing list data in file manager file
# why tweets contain "felicidades"?
### Create the necessary dummy variables for landing page
# Create a 2 x 1 date series
# Concat the two dataframes together columnwise
# check x distributions X train # distribution plot of temp_celcius
# or: # sales_df.groupby(['Product_Id', 'Country']).sum().loc[(5,), 'Quantity']
# Find the index of this maximum 'yob' (year of birth) row
# merge with main df
#Let's bring back our datetime values from index to a column in our dataframe
# Read the solar power production from Data.xlsm
# block_geoids_2010 = [row[0] for row in query_psql("SELECT geoid2010 FROM sf1_2010_block_p001 order by blockidx2010")]
# Replace key with Googlemap API developer key # https://github.com/googlemaps/google-maps-services-python
# Create Excel File
# Select row of data by integer position
# Set the x and y limits
# Set the index to the date column
# Let's check the top 100 records in the Data Set
# aprovechamos y salvamos este nuevo set # y recargamos la informacion nuevamente
# use cross_val_score() # ... #
# The mean squared error
#print(os.path.abspath(os.curdir)) #print(os.path.abspath(place))
# post simulation results of simpleResistance back to HS
# Initialize PyMongo to work with MongoDBs
#  setting a Target variable
# Create a Soup i.e  ## A new Column that combines Category, EventName and Location ## for applying TF-IDF
# initiate geoip client
#9.7 Delete Object Properties # You can delete properties on objects by using the del keyword:
#*--------Merge Command to merge Studies and sponsors--------------------* #studies_b=studies_a.join(sponsors,rsuffix='_other')
#replace all Nans with 0
#Find the largest change in any one day (based on High and Low price) #Verify new column is correct
# Calculate the rainfall per weather station for your trip dates using the previous year's matching dates. # Sort this in descending order by precipitation amount and list the station, name, latitude, longitude, and elevation
# create Series from dictionaries
# get mongodb params (using configparser)
# defines our session and launches graph # runs result
# perform count on dataframe composed only of non-crime criteria # but append that information to original dataset
# To check that indeed 500 tweets were examined, count them
# Author: Evgeni Burovski
# import a list of stop words from SpaCy
# hierarchical index in conjunction with sorting # (note that here we should have converted the day to ordinal) # tips.set_index(["sex","day"]).sort_values(["sex", "day"]) works the same
#Check graph styles available
#merge average fare data into city dataframe
# From the above plot, we can there are two outliers in the rating ratio: 177.6 and 42.0
# merge with council data #df7 = pd.merge(df6,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE',suffixes=('_created','_closed'))
# View dataset coordinates
# We get the trend for closed prs by month
# verify that 6 sets have been removed from all_sets
#get frequesncies of sms #get size of groups
#df_concat_2
# get the general model # predict with the model
# check shape
# we need to double check all tweets are actually in nsw #rough NSW bounding box
# 3.2.B OUTPUT/ANSWER
# From a list with an *explicit* index
# write the scenario to an excel workbook
# Compute the probability of failure for different temperatures
# Latest Date
# Convert each 'tweets' DataFrame to a list of dicts
# Retrieve latest full mars images
#Austin
# Specify an end point and how many periods should come before.
#=== By Label: row selection by 'data index'
# Getting Unique Values Across Multiple Columns in a Pandas Dataframe
# Create a list from 0 to 40 with each step being 0.1 higher than the last
# for l in range(0,20):
#  Importing another file for the coming events
#Make nodes generator, i.e. convert each row of dataframe into a list
#users who purchased multiple cameras and at least one of those were a replacement
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#pulling in data from synapse table (df_table) needed to merge with validation data google sheet
# put the data in the following form # | moveId | title |  # how many unique authors do we have?
# get features names
# print("Accuracy = %g" % accuracy) # print("Test Error = %g" % (1.0 - accuracy))
#Looks like we're good on missing values. Now let's get a summary of the data to see if there's any more issues with it
# run the model giving the output the suffix "distributedTopmodel_docker_develop" and get "results_distributedTopmodel" object
#installing pandas libraries #There is a bug in the latest version of html5lib so install an earlier version #Restart kernel after installing html5lib
#Let's check it out
# display specific number of rows from bottom of the file 5 is the default 
#uniformly format dates #recognize as dates for ordering/comparing #     ORDER_BPAIR_SCN_SHOPIFY[date_col] = pd.to_datetime(ORDER_BPAIR_SCN_SHOPIFY[date_col])
#the length of the data frame is too big and we need to predict  #1% of the data so lets narrow it down now
#results = results.append(nflcont.interest_over_time())
# Number of populate for old Page
# drop unwanted columns
## YOUR CODE HERE
# INSERT SCREENSHOT OF JOB TRACKER UI COUNTERS
# Create one dimensional NumPy array from a list
# this list is hard to read
# Split in features and target
#establish a conenction
#https://stackoverflow.com/questions/19851005/rename-pandas-dataframe-index
# Transmission 2030 [GWh], marathon
## Feature importances
#print(tmp)
# Download the data and remove the header
# trip_start_date
# plot the model error
# Making sure we have the right number of predictions
#Let's get the tweets from Youngstown, Ohio
#df = pd.read_csv('/home/bmcfee/data/vggish-likelihoods-a226b3-maxagg10.csv.gz', index_col=0)
# Grouping interests by user
# Encontrar los registros que se encuentran entre las posiciones [10mo - 20vo] de mi DataFrame
#Testing on a sample text
# Read the JSON file into a list of dictionaries
# reflect an existing database into a new model # reflect the tables
#That's our result! Now let's export this dataframe into a csv in case we want to work with it later. 
# Load the training set # Group the taxonomic classes
#Read the header of the dataframe
# shopify_data_merge_msrp.apply(lambda x: x['Discount Amount']/x['MSRP'])
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# characterize the data #  number of words, don't remove punctuations and other non-alphanumeric characters
# Now, we can save the new data set that includes the headers
# fire violation  # firevio.dtypes  
#pop_df_3.index
# take a look at your results
#remove columsn with identifing info
# Checking the columns data-types:
# let's use tab next to the "." and see the string propierties
# The 13 most liked tweets # ... Found by extracting a number from the Likes.max() number, providing a list of  # tweets with likes between that number and .max()
# Read the variation of wind generation from Data.xlsm
# TODO: SHOULD NOT HAVE TO DO THIS # CONVERT INDEX TO DATETIME
# converting the timestamp column # summarizing the converted timestamp column
# create target vector(s) | lookforward window
# Extracting BBC news sentiment from the dataframe # Use datetime library to convert Data stored in string to datetime format
#e6e6c9791516c2c3 -- original hash #e6e6c9791516c2e2 -- new hash, off by 1
# pytz
# new groupby object by department AND city
#Convert event time to datetime
# id any missing specialties
# Z-Scores mentioned in the article.
#preprocess 
#%% Do Blink Analysis
# Frequency counts using bracket notation 
#this will upsample:
# convert text to datetime object
# Optional: save model for future comparison
# Instantiate a Materials collection and export to XML
# accediendo a una columna por el nombre (label) y obteniendo un dataframe.
# tweets.apply(lambda row: print(row['tokens']), axis=1)
# Create a vector of random integers, then reshape into a 4x4 array. #
# from the non-active users, how many still visit the site (LastAccessDate)?
#model = ols("happy ~ age + income + np.power(age, 2) + np.power(income, 2)", training).fit()
# inspect duplicate userid
#Rename columns
# Which features play the biggest role in predicting campaign success:
# Show all Plants with invalid commisioning dates
# Grabbing the ticker close from the end of last year
#Find the highest number of retweets that a tweet received
### Step 13: Create a scatter plot to compare the two data sets ## The clusters suggests 2 unique type of days
# continue here to extract Median Listing Price and to split the data using train_test_split()
# visualize the importance of features
#s = "01/12/2011"
# convert it to pandas data frame
# Now apply the transformations to the data:
# Define model and loss
# load logistic model
# Create a new df copy of df2
## Test code on a reasonably small DF
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
# generate a list of all customerIDs
#### Define #### # Join all three tables using unique key 'tweet_id' # #### Code ####
# Here the benchmark is NASDAQ Biotechnology index 
# We display the updated dataframe with the new column:
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Interpolate from the current state to the forecasts
# load pickle file for dataframe
# Pick a single tweet to analyze
#df["X"] = df["order_type"].apply(lambda x: 0 if x == 'single' else 1)
# remove duplicate ID entries
# dropping values that match conditions in the query function
# get the trip_duration column using trip_end_date - trip_start_date
#Joined(train,test)-googletrend for states(checking if there was any unmatched value in right_t) ##Note: Level of googletrend table for states is "State-Year-Week"
# this_tweet.head()
#this is looking cleaner now!
#normalize the continuous variables that will impact tripduration
# Make ved folder
#estatisticas basicas
# Verify accuracy score through cross-validation of X and y data
# let's use 'StatusDate' to estimate time to request resolution # however, first need to convert to appropriate format to compare with 'CreatedDate' # compute resolution time in days for each request
# Count terms only (no hashtags, no mentions)
# drop known bots from analysis 
# for testing
##### key, value pairs
#df_trimmed.info() #df_trimmed.head() # df_trimmed.eventClassification.unique()
# Extract the uid of the saved model
#CHURNED SLICE, churned dates
# transdat = temp_data.loc[1:700,["OPEN", "HIGH", "LOW", "CLOSE"]] # transdat # temp_data.head(400)
# Count tweets per time zone for the top 10 time zones
def saveDataFrame(df,path):$     '''save DataFerame in csv format'''$
# convert crfa_f to numeric value
# Fitting K-means with 4 clusters
# Call the function
# store CV results in a DF
#'Milwaukee': '2a93711775303f90'
# NLTK Stop words
##Most Liked Posts 
#From the data table above, create an index to return all rows for which the  #Phylum name ends in "bacteria" and the value is greater than 1000.
# Approach 2: Creation of  #Class_frame.loc[Class_frame["amused"]>0]
#We need to load only the stuff between the curly braces
# This creates a list of BeautifulSoup objects.
# Locais no dataset de atendimentos
# Calculate the total number of stations 
## Amount paid
# first date where nr of coins are c
# RE.SEARCH
# Retrieving a total amount of dates
# selecting two columns
#Rainfall per weather station for previous year's matching dates (based on your trip dates)
# Critical value at 95% confidence
# Push table to database
# For getting all the links found in the nav bar
# take a look at the data
# watch out with label-based indexing when indices are scrambled: # we will get all values with indexes 1 and 7 and all that happen to be in between
## 3. print the median of traded_volumes
# examine some of the indicators
#charts.plot(df, stock=True, show='inline')
# show summary statistics for number of claims in utility patent
# Save the references to each table.
# add features to validation # check wether the join operations where right
# common words from the reviews
# Downloading the data
# printing parameters AIC,BIC and HQIC
#load a Parquet file
# Naive Bayes with LOOCV on our Test Data
# Display of first 10 elements from dataframe:
# New dataframe for suburban data
# create CustomBusinessDay object based on the federal calendar # now calc next business day from 2014-8-29
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# Divide each number of each countries columns by it's annual maximum 
# Creating a rate column with the extracted numerical rating  # WHAT AM I DOING?
#Or just show the rows, i.e., the first item in the shape result
# Tests submitted over time
# Increase the depth of the GBM on this new, reshaped data # We can increase the AUC a bit, perhaps not worth the problem
# OK. Now I want to look again at my dataset head to remind myself of the column names and values for each.
#Retrieve the parent divs for all NASA articles #Print results
# The article URLs provided for this project. # Note that this script will work with other articles that share the same HTML layout. Just add URLs to this list.
#print(dataframe.head())
# Transmission 2020 [GWh], marathon
# drop ceil_15min from data
# Number of unique rows
# Topic 6 is a strong contender for 'eating'
# best/worst breweries, min x beers
# limit the query using WHERE and LIMIT
# creating/opening database: # opening collection (instagram posts):
# Number of votes
# Renaming the columns to Date and Precipitation
# before I split I want to understand my distributions of my predictors
# create a fitted model with three features # print the coefficients
#Third Data Set:
# Show model training parameters
#Mars hemispheres # url of page to be scraped
#Los Angeles': '3b77caf94bfc81fe'
# Extracting user 1 for testing
# Push the sentiment DataFrame to a new CSV file
# Train model
# drop the null in is_enabled
# convert utc to est? Find out what tz it is first.
## create price changes for certain time periods in advance
# Save the updated spreadsheet
# Show the nodes file
# How many elements in each, how many sessions, how many elements per session
# or # 0 is Monday so 2 is wednesday
# Create the Estimator
# Multinomial NB model on age and subreddit 
# Preprocessing the Date (Created at)
#show the band widths between the first 2 bands and last 2 bands 
# Analyze the data in one tweet to see what we require #The key names beginning with an '_' are hidden ones and usually not required, so we'll skip themif not param.startswith("_"):print  "%s : %s\n" % 
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
##Prediction:
# Dispersion plot
# add porn column and remove extra headings
#reading in csv files
# Finally, export this file 
## url
# How many ticket in each type
# extract metadata values
# Gather benford's distribution percentages
#creating derived metrics/ features #converting the date columns from string to date format #will use it to derive the duration of the project
# Take a look at the words in the vocabulary
# create dataframe from list of tuples
# some numerical values do not show as such, clean missing records
# login_url = 'http://www.prisontalk.com/forums/login.php?do=login' # login_response = s.post(login_url, data=login_form)
#Add variable initializer.
# getting rid of null values and then providing description to show success in that endeavor
# Use Pandas to calcualte the summary statistics for the precipitation data
# read the json data into a pandas dataframe # set column 'created_at' to the index # convert timestamp index to a datetime index
# calculate the mean of the first minute of the walk
# Modified the dataframe being evaluated to look at highest close which occurred after Acquisition Date (aka, not prior to purchase).
# what's the lenght of the groups we're going to get
# review existing columns | points on yield curve
#print the first five rows of theft data
#ignore any warnings
# Read All 5M data points # Extract the bodies from this dataframe
# Replicate booking = 1 by 4 times
#dang, there are ~80k nulls in may
#order_details_train = pd.read_csv('../data/raw/order_products__train.csv')
# Observed contingency table of test_id against test results # p-value of the null hypothesis
# read excel file
# we store current working directory in cwd1, later we change working directory and use it to change it back
#reset the index to the date
# Create the Dataset object. # Map features and labels with the parse function.
# Write to disk to prevent us to have to hit the API again
# M = Month end frq and MS = month start freq
# PASS CONDITIONAL GROUPBY??
# Concatenating the two pd.series
# Clean, tokenize, and apply padding / truncating such that each document length = 70 #  also, retain only the top 8,000 words in the vocabulary and set the remaining words #  to 1 which will become common index for rare words 
# Assign the demographics class to a variable called `Demographics` ### BEGIN SOLUTION ### END SOLUTION
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
#see the values at those index # outliers values replace it with mean should be fine for now
# they do intersect, so create an overlay with a 'union'
# Places table
#Problem 2
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# R == 'regular season'
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Store tables
# try without extra squre bracket
# use reindex
# Structures data library # Columnar data library
# caution
# Read in csv file as a Pandas DataFrame
# plot the fixations as a heatmap # TODO annotation how many fixations from how many pictures are used for each eyetracker
# create target vector(s) | lookforward window
# List stores
# Import and Initialize Sentiment Analyzer
#print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']][-30:])
#Count All Tokens
# corr.to_csv("correlationmatrix.csv", sep='\t', encoding='utf-8')
# Total dates
# Let's use the Support Vector Regression from Scikit Learn  svm package # using the defaults only
# get compensation of employees: Wages and Salaries
# delete a column with pop
# Split arrays into "training set" (train_Features, train_species) and "test set" (test_Features, test_species)
#### dumping dict of data frame to pickle file
#Grouping by outlet then creating a new DF for the bar chart
#check the loaded file
# let's create the dataset of beers drank per month and sort it
# drop 'title' and dummy all of it.
# Number of tweets in a conversation on average
# Start by sorting data from oldest rides to newest
# new crossed columns are added in model.py # see first 20 line on codes starting from "INPUT_COLUMNS
# trading_days = fk.get_trading_days(start=start, end=end) # trading_days = list(set(fk.get_trading_days(start=start, end=end)).union(set(fk.get_report_days(start=start, end=end))))
# Count how many files in training folder
# analyze validtation between BallBerry simulation and observation data.
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
# Set operator as the index.
# adding prefix AD_ to column names
'''Validating the Score with the training set'''$
#women_in_medicine_save = 'wikipedia_women_in_medicine.html'
#Visualize results
# display first 10 elements of the dataframe
#segments
# Movies with the lowest RottenTomatoes rating
# histogram of num_comments
# important: please modify the path as your local spark location 
# investigate on the position and properties of detected fixations
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Create a TensorFlow session and register it with Keras. It will use this session to initialize all the variables
#Specify filename to save to #Save edge parameters #Ext_input.save_edge_types(filename=input_edge_types_file, opt_columns=['weight', 'delay', 'nsyns', 'params_file'])
# wide to long # view head of final row-wise dataset
#print(pred) #print(clf.predict_proba(x_test))
# URL of page to be scraped
#test set accuracy
# what percent is that?
# invoices creation distributed date
# Query and select 'date' and 'prcp' for the last 12 months
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Declare a base using 'automap_base()' #Use the base class to reflect the database tables
#the url for one dream from the user 'fataverde' posted five days ago (today's date: 1.9.18)
# Merge two dataframes, the common column is selected aotumatically
#WARNING: Long execution time - 20-30 mins
# Load the query results into a Pandas DataFrame and set the index to the date column.
# create a DataFrame with the minimum and maximum values of TV
#getting dummies for body types of cars
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
#df.dropna(how = 'all')
# Test score is better than the train score - model generalizes well 
#Repeat customers
# Simulated conversion rate of Pnew uner the null
# Convert all oz_stop ids to string format
# pipeline / fit / score for lr & hvec
# Get values # features # targets
## I am just interested in the cats that were adopted
#count of users in treatment group
#preds = model.predict_proba(X_test_mean)
#Subset columns
# numer of NaNs is sum of logical true
# Arithmetic operations between Pandas series and single numbers
# Count terms only (no hashtags, no mentions)
# check index have correct freq "<MonthEnd>"
#Excludes the "Cover_Type" column from the features provided
# Locais no dataset de postes
# # convert date columns to datetime 
# tensorboard = keras.callbacks.TensorBoard(log_dir="../logs", write_graph=True, write_images=True, histogram_freq=1)
#Set index to date
# get english stop words
# a series to demonstrate alignment
#replace '-'(hyphen) with '_' underscore to make database operation
# Index persistence
# Extracting NYT news sentiment from the dataframe
# combine all product_type
# New Pandas DataFrame with a new name of the field including the Sentiment Analysis results (SA)
# save data to CSV# save  
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# figure out what was the spike at 5 am (related to 5-5:59 am tweets) on Small Business Saturday 
# groupBy function usage if you want to not have the columns you are groupBy to become the index.
# Merging event_list and df_test_user whrere event_start_at > created_on
#Divide p-value by 2 since this a one tail test (upper tail test)  #Since this is a right tail test p-value for right tail is 1-p-value on left tail
# Make one big dataframe
# Store stops in csv for safekeeping
# Read csv filenames 
# Summary statistics for the movies data set
#move the lat and long columns to be next to the address #You can rearrange columns directly by specifying their order: #df = df[['a', 'y', 'b', 'x']]
#How many words have more than one translation?
#Remove duplicated rows by contractor_id #Remove duplicated rows by contractor_number
# Create an array of zeros of length n
#plt.ylim(200,400);
#concatenate the sentiments_df, then export it to csv, and show the completed dataframe
# Calculate the morning traffic that fall under the income group for every station
# plot autocorrelation function for RN/PA data
# .dt.strftime("%m-%d-%Y") # .dt.strftime("%m-%d-%Y")
#Save to a csv file 
#Let's get the average number of retweets and favorties per location #looks like even though we got rid of None values in userLocation, there are still empty Strings
# Now, call the Quandl API and pull out a small sample of the data (only 5 days) to get a glimpse # into the JSON structure that will be return
# check the confusion matrix # it seems that many rates are predicted as 5 point, which generates the most of errors.
# missing values check
# exact match
# Amount invested in BTC in Coinbase
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure #my_data = pd.DataFrame(data['dataset']['data'])
# Change the order of the columns
# get cat sizes # create embedding sizes # nconts
#challenge 2 #Create a new column that stores the date and time as a single pandas datetime object
# no output below means the conversion was successful
# dd = dd.drop(['name', 'location','screen_name','description','created_at','Unnamed: 0'], axis=1) # dd.head() # print(dd.iloc[0])
# Test dataframe
# creating new vs return fan csv
#Double-click on the file that is saved at the filepath shown below -  #This will launch your interactive pyldavis chart in a browser window!!
# check Basin variable meta data in file manager file
# Integer slicing
# each hour of 31 Dec, 2017
#average number of completed tasks
# Search for '#Australia'
# Temporarily save the data, to avoid having to run the first bits of code  # again in case this notebook needs to be re-run/debugged...
# Setup Tweepy API Authentication
# even tho aldaar is below average it has the most retweets as outlayers 
# Dimensions:
# Load the results into a pandas dataframe. Set the index to the `date`
sql_lagou = """$     select * from lagou_recruit_day$ """$
# Convert dictionary to a DataFrame
#getting mars tweet
# Drop duplicates from table of latest inspections
#Cincinnati
# rename treatment to ab_page and drop control column
# anchor random seed
# Printing the content of git_log_excerpt.csv
## We have to break this up into ~3000 size inserts, otherwise Neo4J will crash
# Missing 1 value for Q3 so filling with 'Other'
# proj WGS84
# Getting some preliminary descriptive statistics for the columns in the df
# evaluate predictions
#start_time = timeit.default_timer() #create a copy of the dataframe to label encode
# mean deaths per day:
#Convert into a Python dictionary
# 2. Convert the returned JSON object into a Python dictionary.
# Creating reference to CSV file # Importing the CSV into a pandas DataFrame # Looking at the top of the df to get a feel for the data
#print(data[:5])
# This will shift the column index up by the number of periods - here it is 1% of len(df) which rounded up is 35 and # the column values it shifts up will be replaced with NaN
# more properly, should do
# unemp rate for this april 2018 was 4.1
#Review resulting dataframe without the additional columns
#Use the iloc method 
# show available waterbodies
# Load needed Python libraries
# OVERALL SENTIMENT ANALYSIS BY NEWS SOURCE # Create dataframe that calculate the overall compounded score to place on the y axis
#proportions_ztest returns the z_score, p_value
# for rows, the same rules apply: integer indexing returns # a series, slice or list indexing returns a dataframe
# remove PAs with no event. these are probably PAs with pinch hitters.  (< 1% of observations)
#Now you can write your dataframe to hyper file, which you can open with tableau directly. Then have fun with your EDA!
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# KEGG mapping of gene ids
## Import Raw TTN Data from Google SpreadSheet
#read in old csv files
# leadsdf['simpleDate'] = pd.to_datetime(leadsdf["lastEnteredOn"],format="%Y-%m-%d") # leadsdf['simpleDate'] = leadsdf['simpleDate'].dt.strftime('%Y-%m-%d')
# Now we can set the new index. This is a destructive # operation that discards the old index, which is # why we saved it as a new column first.
# getting rid of columns and rows: drop
#Converting JSON object to a python dict
#'Pittsburgh': '946ccd22e1c9cda1'
# We want to expand these fields into our dataframe. First expand into its own dataframe.
# Let's try with our homes median list price data, which we will want to be time-indexed
# df_2001
#for prophet to work, columns should be in teh format ds and y
### Create the necessary dummy variables
## Uncomment and complete #trump = trump[trump['source'][:7] == 'Twitter']
###YOUR CODE HERE###
# check inserted records
#file_time = str(current_time.time())+'_'+str(current_time.date())
# Calculating in and out degrees
# due date distributed
# you can convert your data frame index to period index
#First, remind us what our columns are
# Calculate basic statistics.
# number of active authors per month
# index in the resultant contatinated df
# Spark Streaming #Create streaming context with latency of 1
# Prepare for Submission
# get the current date
#if running demo, uncomment this and change test['id'].vaues -> test['index'].values
# Save a reference to the measurements table as `Measurement`
#Select records for 2017 #Group by site & compute mean TotalN
# Define database and collection
# Deep copy 
# Split the Data to avoid Leakage #splitting into training and test sets
# Insert tripduration_minutes into df as a column # loc=1 will set location at index1 # value=tripduration_minutes - sets values of the column to pd series list made above
# execute query again # fetch the results and use them to create the dataframe
# Further increase confidence of accuracy score through cross-validation of X and y data
# create a variable with the path/name of the file that will contain your unique address list
# Remove mismatched  rows
# returning to numpy
#retrieve pages with request module
#Return the list of movies with the lowest score:
#Group by News Org  
# df.head().T.head(40)
# Calculate number of unique users in dataset
# Am I currently using a GPU?
# Populate the pandas dataframe with our JSON file
# Set up access credentials
# get the total number of collaborators
#Lets check out the languages used #I wonder what 'und' is!?
# let's see if removing some of those values near 0 gives me a more recognizable distribution:
# load the model from disk
# Graficamos
####TEST #a['Compound Score'].mean()
# Delete Rows # drop rows for mismatched treatment groups # drop rows for mismatched control groups
#check count of tweets per geography
#                                   .size())
# Function to load in the first ssize rows of pre-processed train data
#A DataFrame has a second index, representing the columns:
# List comprehension for subreddits of reddit posts
# pythonize column names
# checking the length of dataframe with number of entries # using len() and unique() to count the number of of unique rows
# My gamelogs include 3/29/2000 - 10/02/2016, so I'm going to delete those rows.
# index has freq None
#Import all data available at the hackathon
# merge weather and 311 #df5 = pd.merge(df4,df2,how='left',left_on='Date Closed',right_on='date',suffixes=('_created','_closed'))
# TODO High-Cardinality Categorical  Cluster # plot(df_train.iloc[:, 35])
# Saves an image of our chart so that we can view it in a folder
# KNN model 
#### Look at DEPT to make sure it has gone up, it has!
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
#Plot sites with an area > 25 sq mi
#act_diff = p_new - p_old # compute difference from original dataset ab_data.csv
# Get sentiment for walmart tweet #Read walmart tweets csv file
# First row from validation dataset
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# FA_indexs
# to get the last 12 months of data, last date - 365
# generate historgrams of the # returns of our portfolio
# Saves an image of our chart so that we can view it in a folder
# fig.savefig('toma.png', dpi=300)
#Dallas': '18810aa5b43e76c7'
# split X and y into training and testing sets
#Mars Weather
# check Basin variable meta data in file manager file
#Create Dataframe
#yearago_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() - dt.timedelta(days=7)
# Lista de palavras para ignorar usando os pacotes do NLTK
#verifying the dataframe contains 100 tweets from each of the news channels
# save the file to the output
# using .div maxes sure you divide the corresponding indices, in this case business names #ratio fillna??
#Remove unnecessary columns
# Predict_gnb
# Printing the content of git_log_excerpt.csv
# r.raise_for_status()
#Find the largest change between any two days (based on Closing Price)
# Converting to list
# Stats model logit() and fit() countries
# We display the average salary per year
# we will sort the information in the last array
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Podemos crear un dataframe como sigue: # Hacemos un display del dataframe:
# CHECK THAT LOGIN SUCCEEDED
# Training the model 
# re-arrange cols to original order
# Rural cities total drivers
# p_new under null 
# df is the dataframe where the content of the csv file is stored # note that with dataframes I can refer to variables as dictionary keys,  # i.e. df['starttime'] or as attributes: df.starttime. 
# drop NaNs created from lagged feature generation
# slice price DataFrame to show only Germany
# mean Closing price for specific month:
# https://spark.apache.org/docs/latest/ml-classification-regression.html#multinomial-logistic-regression
txt = """$ The sparks from the seemingly ordinary July 23 incident on a road near Redding set off one of the most destructive wildfires in state history and killed eight people, including three fire personnel. It also destroyed more than 1,000 homes and consumed 229,651 acres, according to California fire officials said.$ """$
#'Toledo': '7068dd9474ab6973'
# Scale back
# get indices for any other keys that are part of data
# old allocations
# Split the Data to avoid Leakage #splitting into training and test sets
#Display the last 5 rows in the contractor
#== Selective overwite 
#logistic regression to predict converted column using intercept and treatment columns
#Get info on the tweet's author
# get artists whose name contains "Aerosmith"
# get max probability column name, this is final prediction for test set
##select whether you want excel or csv
# We can view all of the classes (i.e. tables) that automap found
#info_final_gente = pd.merge(info_final, avisos_detalles, on = 'idaviso', how = 'left') #info_final_avisos = pd.merge(avisos_detalles, info_final, on = 'idaviso', how = 'left')
# Calculate probability of conversion for old page
# collecting the violence levels 
# Plot ACF of first difference of each series
# to calculate the total we only need to get the size (len) for each selection # Underweight
# Links anzeigen
#Saving that into a new csv
# on the occasion: when using multiple boolean indexes,  # ** make sure you get the parentheses right! **
# A:
# Turn team_slug_df into key:value pairs
# New dataframe for urban data
# how significant our z-score is # what our critical value at 95% confidence is
# most delays are on thursdays
#== Drop any rows having missing data 
# remove the reference to the original data frame by creating a copy # use vector subtraction to find the length of each chomosome/DNA sequence # peak at the first few rows of the datafram with a length column added
# Convert the returned JSON object into a Python dictionary
# Creamos la columna nueva SA y guardamos los valores # Mostramos el dataframe actualizado
#X.head() #abc = df.reset_index().values.tolist()
# fit network
# This is where we break the json file to a dataframe with its individual cols
# create search_weekday column
# Infer the schema, and register the DataFrame as a table.
#write this dataframe into csv  #due to the twitter content is not determined, the data form needs extre cleaning steps
# quadratic features to use in our estimation
# add prefix to column names
# concatenate first three rows of above dataframes
#We have derived important features and now we will dropped the columns to reduce memory usage
# check option and selected method of (12) choice of hydraulic conductivity profile in Decision file
# The highest rated dog is called Atticus, it has a rating of 177.6
#BUMatrix.plot(kind='scatter',x='', y='event_Observation') # from pandas.plotting import scatter_matrix # scatter_matrix(BUMatrix)
# Resource: https://stackoverflow.com/questions/38421170/cant-set-index-of-a-pandas-data-frame-getting-keyerror
#subset 2014 data #for predictions later
# Predict the rating of known stub article.
#Design a query to calculate the total number of stations. #New DF grouped by station with new column 'count'
# sanity check out environment is working
# 3.2.C OUTPUT/ANSWER #top 10
# Import census tool data pull
# example text
# how to create a class using type
# Cleaning up DataFrame for calculation  # Lagged total asset # Average total asset - "rolling" 
# validation 24 hours last day 
# Which were the 3 most popular trainings? (Trainings with the most learners)
# Getting chromedriver to retrieve html from reddit, not sure what the timer is for exactly need to get info # Giving it 2 seconds to completely load the reddit page before scraping
#qry = DBSession.query(User).filter( #        and_(User.birthday <= '1988-01-17', User.birthday >= '1985-01-17'))
# car break-in  #car=load_data('https://data.sfgov.org/resource/cuks-n6tp') #car.head(5)
# differ from ndarray
# Design a query to find the most active stations.  # Which station has the highest number of observations? # List the stations and observation counts in descending order
# Inspect compiled dictionary 
# Issues open
# Read the Train and Test files
# Read in csv file containing latitude and longitude information for each of the towns # Spliting each line so each town is a different element of a list
# Import qgrid library and set some options for optimal display of tables with # big number of columns. Also, do not allow the widget to edit the values of a # dataframe
# easily count the data rows now # note this data is continuous, every few minutes, so a count isn't really helpful but nice to know it can be done
# Moving on to the User Information csv file...
# 6/11/2017 ~ 6/18/2017
#time_rt.plot(figsize = (16,4), color = 'b')
# Convert sentiments to DataFrame
# Save data to Excel
# largest change in any one day (based on High and Low price)?
# iris file #Remove the header line
# read CSV
#ADD CODE for using cleanNewUserArtistDF and print
# Hint
# Eliminar los registros que tengan nulos en la columna "created_time"
#You should convert the user_id from object to integer!
#split zipcode columns into two columns by '-'
# Check out columns
# using random forest classifier
# Remove characters and redundant whitespaces
# Requests
#Store locations of long NAN gaps in primary sensor record to be filled
# Get the title of Guido's webpage: guido_title # Print the title of Guido's webpage to the shell
# money flow index (14 day) # mean-centered money flow index
# Stacking the lists column wise, so that they can fit in shape in the dataframe.
#for i in stopword_list: #   print (i)
# Import the Sample worksheet with acquisition dates and initial cost basis:
#cut out only low redshift objects for bad seeing
# Creating a 'index' so that plotting is easier
# Enter the density of the material that was sieved.
#creating home,Opp_Team,Win columns from Match_up
# aprovechamos y salvamos este nuevo set # y recargamos la informacion nuevamente
#Great. Now let's use the groupby function to count the number of  permits completed by year and month.
# Create the empty table that will contain the sidewalk linestrings.$ sql = """CREATE TABLE approx_medial (id bigserial, LINESTRING geometry); """$
# Add missing coordinates to dataframe
# Lenghts along time:
#agg([np.sum,np.count]) #sum() #agg({'col5':np.sum,'col6':np.sum})
#Akron
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Here's a example code snippet you might use to define the feature columns:
# Change this to 'jobs_raw.tsv' for subsequent readings # data-ca-180423.tsv
# Divide each number by each countries annual maximum
# Change the number of bins.
# Firts, create an array of booleans using the labels from db.
#df = pd.read_pickle('/home/jm/Documents/testingStuff/my_file.pkl')
# Run in test mode and analyze scores obtained
# Plot the histogram
#plt.figure()
# sort from the oldest down to the youngest
# Make a list of all Terms per season
#Seasonality calculation
#since we expect ~ 57% precision (true positive rate) how many wells do we expect to ACTUALLY fail this year?
#explore pandas dataframe work functions. 
# Reformatting date/time column
# select orders related to our selected users
# Create corpus # A corpus is a list of bags of words
#Export dataframe to csv
def similar(a, b):$     """Get a similarity metric for strings a and b"""$
# the finally logistic regression model:
# Confusion Matrix # https://mingchen0919.github.io/learning-apache-spark/decision-tree-classification.html
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
#calculate the average across each time slice #calcuate the Mean Daily Rainfall for each Month #calcuate the Total Rainfall for each Month
# analysis.iloc[0]['project_url']
# How many stations are available in this dataset?
# We print percentages:
# Predicting the sentiment values for test data and saving the results in a csv file 
# sparse_categorical_crossentropy is like categorical crossentropy but without converting targets to one hot
# set SUMMA executable file
#load data  
# Run seasonal decomposition and plot
#Create list of siteIDs
#Question 3
# wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'
#getting dummies for countries
# percentiles look right; most data should be fairly close to the value at the 50% mark
#beautifulsoup boilerplate code
# Here the benchmark is iShares Nasdaq Biotechnology ETF 
#Create data frame with stations, dates, and temperature
#print df.is_application.head(5)
# grab the index of the 50 largest abs(errors) # get the rest of the details from the frame # plot model error against strike
# get just a time from a datet ime
# What are the most active stations? # List the stations and the counts in descending order.
# Average chart statistics. # Calculate the average of averages.
# Initializing an LSI transformation
# or we can aggregate the data according to criteria # $sample is a mongo function
# in our case we have more than 2 dataframes so the following code will merge more than 
# Save file
# getting GEO ID for Philippines
# Even if you pause the display of tweets, your stream is still connected to Twitter! # To disconnect (for example, if you want to change which words you are searching for),  # use the disconnect() function.
# Read the data file and save it to a dataframe called "df".
#print randomdata2 #randomdata3 = randomdata1[(randomdata1 <=-3)] #print randomdata3
# URL for reddit
#More Manual Calls
# are all values equal to 6?
# load the string into a list, and remove duplcates by casting the list as a set.
#output
# fit with non-tfidf dataset
#show info/length of collected data
# We create a pandas dataframe as follows:
# Convert date in 'Timestamp' column from strftime to datetime
#  Illustrate the KEY IDEA by our helper function: #  which outputs an array of three random values from poparr. #  Rerun this cell, to see how resampling would work.
#[i for i in my_list if '91' not in i and '18' not in i]
#inspect measurement table#inspec 
# Counting the number of null values
# create a column with the result of the analysis: # display the updated dataframe with the new column:
# We can use the same objects on test now that they have been fitted on train
#fitting the model using logistic regression for preddicting converted column using intercept and ab_page
# transform spark dataframe to pandas to print the image
# 'y' for 1 and 'n' for 0
# Simulating for old page
# choose the columns we need 
# There is one user_id repeated in df2 # Locating duplicate id
# We extract the mean of likes:
#normaliza os dados da versao 1 para range de 5
# look at all models and topics learned in class
#\xa0 is  non-breaking space in Latin1 (ISO 8859-1). replace with a space
# Use Pandas to calcualte the summary statistics for the precipitation data
# To find convert rate for p_old 
#Keep only [2.5, 97.5] quantiles (word count)
# Read the filtered tweets from the .txt files
#7.(Optional) What was the median trading volume during this year.  #(Note: you may need to implement your own function for calculating the median.)
# How many stations are available in this dataset?
# Initialize empty dataframes
# Import packages using the import command.  # Chaning my working directory
#create arima model
# State of the git repo (deep-review submodule)
# pandas way
#Tokenize words in articles
# Add author information for model
#### Test ####
# create pySUMMA Plotting Object
#List the stories with their status, age and sprint age.
# visualisations
# Load the data
#appending 1st,2nd and 3rd computer into one system
# Prevents tables from being truncated.
#sort lastest_consensus_created_date to find the latest ones.
#Looks like all are active, but just in case you want to load in a new dataset lets be sure we get it right... #The status column is now redundant
# visualisations
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#### print a few wells to double check
# view the first five lines of data/msft.csv # !type ..\..\data\msft.csv     # on windows
res = sqlCtx.sql("""SELECT name, stddev(result) as std__of_result$             FROM tempTable$             GROUP BY name""")$
# Let's get Apple stock data; Apple's ticker symbol is AAPL # First argument is the series we want, second is the source ("yahoo" for Yahoo! Finance), third is the start date, fourth is the end date
## we see that the class ="ref-module-paid-bribe" contains information of all the reports in a page
#Inspect label balance
# read in a dataset to print columns
## Count the amount of sources, and display  the first 5
# top chatter over time
#Importing and instantiating Vader Sentiment
# Mapping using the PDBe best_structures service
# We will join on the email so we need to make sure the email address column is formatted in the same way
# calculate the average RMSE
# also useful: isin
#print ("Current date & time " + time.strftime("%c")) #list_of_idxes = [1454, 1852, 2801, 4545, 4759] #list_of_idxes = [3, 4, 56, 62, 63]
# path will need to be changed pending on where the repo is cloned to
## check whether the parsing succeeded for all the locations
# read matching list
# connect to twitter # write up about API object
# %matplotlib notebook
#df.drop_duplicates(subset=['last_name'])
# Find one reinsurer-AIG treaty:
# calculate mean average deviation with window of 5 intervals
# intersect the two lists (lists are tuples of words and their frequency, we need the first value only)
# Baseline confusion matrix
#x=dataframe['Date']
#URL's of pages to be scraped
#sort by Fox in order to create different color scatter plot
# Create a binary target variable (injured/not injured)
# group by News_Source # calculate mean on compound scores
# Looking at one tweet object, which has type Status: 
# 23 ST -> 14 ST-UNION SQ @ 11:30 am = 20 min # Print total crepe sales lost due to transit
# look for seasonal patterns using window=52 - Dr
#tidy_format = tidy_format.groupby(tidy_format.index.get_level_values(0)) #print(tidy_format.head)
#Convert list of past tweets to text for modeling.
# TASK C ANSWER CHECK
# let's use the model we created to make a prediction:
# Retrieving latest date available
#Create a list of metadata rows to skip; rows 1-29 and 31  #Append '30' to the list
# define "db" and "coll" variables for convenience
# Neat way to rename columns and order appropriately
# check the blob, is it really there? # yay! it works!
# Cleaning up columns so it's easier to reference them.
#### Test ####
# build the vectors that will be the pradicating features
# number of patients over time, new and existings
# See https://www.mediawiki.org/wiki/Manual:Namespace#Built-in_namespaces
## Views
#Filter our merged sites-results dataframe to contain only the sites identified above
# Simulate conversion rates under null hypothesis
# initialize authorization instance using credentials
# nuisance columns after groupby
#creating a serie from a dicionary #will be created in key sorted order
# %load solutions/pipe_results.py
#READ THE LIST OF NEWSPAPERS
#Upper code was used to produce a .csv file containing all parsed Twitter data #Was uploaded to Dropbox and made available via a public link, lower function downloads that file
# highest temperature recorded
# append new records to historical data
# We create a Pandas DataFrame by passing it a dictionary of Pandas Series # We display the DataFrame
# All columns except for 'label' will be considered as features
#concatanate the predictor variables with the dummies dataframe
# converting into datetime-format
# This merge results in duplicate columns, marked by _x and _y suffixes:
# Convert notebook to HTML # Upload the HTML # Upload the Actual Notebook
#cur_laurel = conn_laurel.cursor()
# dummy all the subreddits
# We extract the mean of lengths:
# Remove the 'Date' and 'Time' columns
#What was the largest change between any two days (based on Closing Price)
# idpath = "data/most_id.csv"
### read in IOTC csv file scraped from iotc site (for Chinese vessels only)
# If there are Nan values we can use inputer  
# results are returned as an iterable list
# Keys become column headers, indexes becomes row labels
# We Create a DataFrame that only has Bob's data # We display bob_shopping_cart
# donor_class.sort_values('Donation Amount count', ascending=False, inplace=True) # donor_class.reset_index(drop=True, inplace=True) # donor_class.loc[donor_class.index <= top_20]
#working with fakes
# deciding the cut off
#create an extractor object: #create a tweet list as follows:
#after getting the data we will fill the data
#Summarize the MeanFlow_cfs data, using default outputs
# What are the most active stations? # List the stations and the counts in descending order.
# Collect data from the Fankfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
# kill line breaks # coerce license date col to datetime and sort descending
# Check the date range to see if anything is fishy there...
# convert the price data to a dictionary
# The SUM produces the number of emails opened # The COUNT produces the total emails sent
#if '409' in str(d): #   print(properties)
# How good was our prediction? Calculate the total squared error!
# Add a blank column to flag number one hits
#tweetdf.to_csv("tweets_w_lga.csv")
# how many unique authors do we have?
# Suburban cities total drivers
# Continue clicking throught the comics
# and visually?
# Use tally arithmetic to compute the difference between the total, absorption and scattering # The difference is a derived tally which can generate Pandas DataFrames for inspection
#tweetdf.to_csv("tweets_w_lga.csv")
# Source Names
# we can do ?pd.read_csv or just check the  # documentation online since it usually looks nicer ...
# integer index: a series
#Example2:
# Create database connection
# Task 1
# load the model from disk
# show dataframe first rows
# favorite table # for row in table_rows: #     print(row.text)
# Find all the json zips that have been scraped.
#Delete that row of course
# Load the data from the query into a dataframe
# Use Pandas to calculate the summary statistics for the precipitation data. # The .describe function allows us to do this by showing us the main  # characteristics of the data contained in the dataframe.
# getting train and test
# palette # sns.set_palette("cubehelix")
# We want to extract 5 topics: # We fit the model to the textual data: 
#'Omaha': 'a84b808ce3f11719'
#plt.ylim(200,400); #plt.xlim(datetime.datetime(2018,2,7),datetime.datetime(2018,3,24))
# find the min of the IMDB column
# Take a single site, Confirm the correctness of rankings
#Determine the top five schools that have the highest percent of students passing math and reading (overall passing rate)
#We should remove the rows where treatment is not aligned with new_page, or control is not aligned with old_page #Append both the dfs to get the complete one
#s_etf.plot(ax=ax, legend='right')
# List the deployments
# split the data set into mentor-mentee based on the number of collaborators a person has
# change into datetime format
# Show results
# A:
# Find unique users # Check for not unique users
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
#New dataframe #They might be values that might still be sharing dates and department ID, #re-run similar loop to the one above with new created dataframe to make sure that that no values repeat.
#Drop two columns in the dataframe that shows the number of students that pass either math or reading for readability
#Examine the summary stats of the per_studen_budget column to create bins needed for next analysis. 
#create the url using database_code=FSE, dataset_code=AFX_X, start_date=2017-01-01 and end_date=2017-12-31 #use string formatter to format the API key
# Create Indices
# add up all the lengths
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data
# df_2004
# create dataframe of matrix and add to variable conmat
#need to conduct a Train/Test split before creating features
# 1 quarter changes regression: earlier half of the sample
# cv_score = cross_val_score(rfc, features_class_norm, paid_status_transf, scoring='roc_auc', cv=5)
#sentiment analyzer #pip install vaderSentiment==2.5
### Plot the important variables ###
# Creating a rate column with the extracted numerical rating  # WHAT AM I DOING?
# Add Text Data
# President Obama's 8th state of the union address
# Cast Frequency_score as integer
# Call the play text the same thing as the other dataframe # Convert date to datetime
# Create csv
# create lagged autoregressive features
##### ndarray of transposed
#Clean the title and date data
# boolean indexing: with .loc, we can input a boolean series  # as the row index; no need to take the values (in contrast to .iloc)
# Remove the missing values
# Design a query to calculate the total number of stations.
# Load part of the dataset to have a look
#we create the trainig and testing data from the data we get
# Density of data # % of seconds with trade
# 34 ST-PENN STA -> 34 ST-HERALD SQ @ 11:30 am = 10 min # Print total crepe sales lost due to transit
# print top three (most consistent)
#select the call option with the fifth expiry date
#Create list of top10 topics
# Drop created and time fetched columns - age takes their place 
# Merge the two DataFrames based on DateTime  # using inner join with price data as left (since price data is shorter)
# list all vendors with receipts
### train + items + item_categories + shops Join
# see an example for a given author
# We create time series for data:
# merge 'yc200902_short_with_duration' with 'yc_trimmed'
# get the index labels, aka row labels
# Drop any NaN values if it still exists # Now drop the NaN values, this will remove the last 35 columns from our data
# make DataFrame with an index but no columns
#same result 0.995
# Get a list of column names and types for Measurements Table # columns
# add a notebook to the resource of summa output # check the resource id on HS that created.
#create a new master dataframe by concatanating the original master and dummies dataframe
# get a list of all data fields
# Create average purchase per product feature
# extract temp grid as a 843 by n matrix # check shape
# We change the row label from store 3 to last store # we display the modified DataFrame
# sum the results
#Stories that in Code Review/Testing or Approval in reln need to be flagged
# df_graph.
#    time_left_for_this_task=120, per_run_time_limit=30, #    tmp_folder='/tmp/autoslearn_regression_house_tmp', #    output_folder='/tmp/autosklearn_regression_house_out'
# cuantil
#set variables to allow automatic X-axis on plot
# Upload files to the buckets.
#Example1:
# Merge the data # Class method?
# 3.Calculate what the lowest opening prices were for the stock in this period
# Showing that the last 35 values have been shifted up and replaced with NaN values
# Colors # Define x-axis and xlabels
# leave only uncanceled ride
# And in a similar manner, we get the trend by quarter
"""$ Check stop-words$ """$
#We create a column with the result of the analysis: #We display the updated dataframe with the new column:
#save data to a csv file
#Conver Receipts into dataframe
# Check all of the correct rows were removed - this should be 0
# Save the figure #plt.savefig("Sentiment_Analysis_of_Media_Tweets.png") # Show plot
## no.of unique cats given out by the shelter 
# Create an OpenMOC Geometry from the OpenMC Geometry
# What's the probability that this classification is accurate?
#write to a CSV file
# Apply the smoother cleaning function (part 2), by checking indexs in transfer_duplicates DF and then updating BTC DF
# Check that we don't have any null/nan at this point # Make sure they have identical hugo gene indexes
# content of the merge of 3 dataframes
# total size in GBs
# df2 = merged_pd.drop('air_store_id',1) # df3 = df2.drop('day_of_week',1)
#finding the important features
# can also show how many times we have scrapped  from this id of the reddit # However, this i found out there are multiple topics
# Let's now filter our data
# loop through all period objects in the index # printing start and end time of each
# 7. (Optional) What was the median trading volume during this year.
# Set the X and y
# Read dataset
# pw_attempt_check(data)
# headers of 'User-agent' is required if you don't # want a 429 error as it limits your returns.
# Save DF to a csv file.
#We extract the mean of lengths:
#df_en['text']
# Example # Get Apple Stock
# We can use the describe method to inspect our data easily
# the above p values 0.1291 and 0.4558 clearly states no significance
# take a look at your results
# KNN model 
# Print the info of df_subset
#comp.head()
# Sort data frame by target and tweets ago #sentiments_df.head()
# Extract the mean of lenghts:
#create and display nytimes sentiments into dataframe
# Get training run status.
# Distances table
#find all mentions of man/men within the posts
### we are intersted only in official posts
# check index
# probability of recieving new page # probability of recieving old page
# make a resource public
# Use the forest's predict method on the test data
# Remove NaN values
# y_pred = lgb1.predict(test_x) # r2_score(test_y, y_pred)
#wikipedia_marvel_comics = 'https://en.wikipedia.org/wiki/Marvel_Comics'
# dropping test rows
# easy to access data # efficiency wise, better (database)
# convert the string date fields into a date type so that we can make calculations on it
# drop rows with missing specialty
# Set "training='probaility'" to train on the labels # Display head of input data of stack layer : probability
#Remove header row
# YOUR CODE HERE # raise NotImplementedError()
#psy_prepro.head()
# let remove duplicates #checking the presence of Mary again #lets shuffle the data set
# Use Pandas to print the summary statistics for the precipitation data.
# Print the external node types
# count number of fire violation per month in  May 2018 
#Great. Now that all rows with nulls have been removed, we can see that we're now working with ~97k rows of data
# as we can see from above, some old pages were given to treatment group # and some new pages are given to control group, which was wrong # so lets drop all these 3893 rows, which are wrongly aligned
# to compare score between train / test set.  
# last day is -2
# examine the start and end times of this period
# We can export that to Excel
#'Denver': 'b49b3053b5c25bf5'
# Standard Deviation, variance, SEM and quantiles can also be calculate
# Print the RESULT LIST so far ###print(f"RESULTS LIST: '{results_list}'") # Prepare DataFrame for the bar plot
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
# Checking if it works
# show overall statistics of the dataframe
# reflect an existing database into a new model # reflect the tables
# Multiple calculations can be presented in DataFrame
# Calculate the actucl difference observed in ab_data
#R17df.rename({'Create_Date': 'Count-2017'}, axis = 'columns')
# cisuabg7
# generate train/test data
# Convert API response into a dictionary 
# Check the data
# Reflect Database into ORM class
# range of potential gammas using logspace
# df_2012
# 120 date/time series with 1second resolution
#Determine summary statistics for the school dataframe
# an empty model, no training yet
# RESET the TF defaults # Start a fresh graph with no configs -- TODO: get some config info for the graph
#killfile = killfile[:20000]
# Lets load the Concatenated DataFrame and the engine specification will help to delimit the outliers
# Convert Start/End Coordinates into lists
# Only fetchs historical data from a desired symbol # or qb.History("SPY", 360, Resolution.Daily)
### Step 19: Convert dates into days of the week  # a) convert object into DatetimeIndex
# how many rows, columns do we have now?
#Compute accuracy on our training set
#### we have already seen iloc(); here is another example
#model_713.forecast(5)
# # a list of dictionaries containing metadata for cells with reconstructions # a list of cell metadata for cells with reconstructions, download if necessary # cells = ctc.get_cells(require_reconstruction=True)
# Save weights from the model, allows prediction without retraining and sharing model with others.
# Predict_SVM
# Gradient Treee Boosting
#Checking the new csv
# how many records will we drop?
# Create ccp with percentage of total market cap
# This will load the cached model
# Having some issues saving to pickle, so I'll save to CSV, reupload, and do the downcasting again before I save the pkl #shows.to_csv("ismyshowcancelled_tmp_1.csv")
#drop all rows that have any NaN values # general observation: NaNs in name_of_conference columns
# Force the Frequency_score column to a numeric data type as it should be
#Check via customer sample
#Design a query to retrieve the last 12 months of temperature observation data (tobs). #Filter by the station with the highest number of observations.
#converts a list of JSON into Dataframe
# Upper-case the home.dest field
# merge the tables
# index can be obtained for an OrderedDict by using, for example, d.items()[0]
# create df copy for use w/ predictions # tail.tail(2)
# Use Pandas to print the summary statistics for the precipitation data.
# Divide each number by each countries annual maximum
#Jacksonville': 'd5dbaf62e7106dc4'
# define path to save model
#785 columns = 1 + 28*28 #reshape into numy array with shape (4199, 1, 28,28)
# Load in the csv data #headlines_df = pd.read_csv("../data/headlines/labeled_headlines.csv", index_col=0, parse_dates=[0])
# histogram in matplotlib
#perform delete
# Display the row's columns and data in dictionary format (first row)
# Get down do the acutal data
# a[y,x] = avg
# transformation primitives
# user_df
# create grid id 1 to 1534 and save as type string # head and tail
# manufacture some loss function # there are n_epochs * n_batchs * batch_size  # recorded values of the loss
# What are the most active stations? # List the stations and the counts in descending order.
# converting the timestamp column # summarizing the converted timestamp column
# From a dictionary so keys are the index and get sorded by keys
# read pickled data.
#Let's display the text of each tweet we found. #[tweet.text for tweet in tweet_list]
# check the structure of the dataframe
# import fire predictor # remove fires with missing values # greater than 5000 acres
# Examine purchases here
# The 'Balance' column actually stores the same value for every record - so delete it, and fix later
# Extract spatial extent from attributes
# show two examples
# calculating number of commits # calculating number of authors # printing out the results
# Copy data from the cloud to this instance
# (empirically derived mean sentiment intensity rating increase for booster words)
# Let's add the classified english data together with the non-english (unclassified)   
#part 2- print row from result
# List comprehension for reddit post timestamps
# Dummy subreddit values 
# import data
# Use your model to make prediction on our test set. 
# dateutil - utc special case
#review expanded data- note could further do the same thing for contributors, or affiliations
# heaviest connections in the graph
### Fit Your Linear Model And Obtain the Results
# assert clean.shape[0] == expected_index.shape[0]
# convert META_b when REF alleles are different
# use BIC to confirm best number of AR components # plot information criteria for different orders
# merge the master dataframe and the dataseries dataframe over the key and drop the key. 
#Return the list of movies with the highest score:
# cutting on too short questions because metrics are not working for those (yet)
#df.to_csv('df.csv')
# URL of page with Mars facts to be scraped
# Group by labels to display how the clusters were formed.
# run the model giving the output the suffix "rootDistExp"
# Concatenate column names from different levels in the hierarchy. #
# Create a graph of the monthly complaint counts
# no avail_docks outliers to get rid of
# data science retweets
############# Lest's Collect  US cities information ######################
# explore dict output
# Split PKG/A column into to and drop
#print df2.loc[['2016-09-18'], df2.columns.get_indexer_for('GrossIn')]
# SORT AND THEN DROP non numric values
#Example1:
# adjust the request format:
# converting the timestamp column # summarizing the converted timestamp column
# Using the station id from the previous query, calculate the lowest temperature recorded, 
# label-based slice indexing of columns is possible # (note both inclusive borders)
# we only need to "instantiate" once.  Then we can call mine_user_tweets as much as we want.
# calculate daily percentage change
# Expand into a 1-hot encoding
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Add a query to a dataframe #View data
#need df for on demand to creat price list
# accediendo a files y columnas por etiquetas
#Retun time and sensor information
#Simply use the Dataframe Object to create the table:
#y_test_array = y_test.values.reshape(-1, 1)
# the resulting mapping returns all sets that aren't in invalid_sets
#df_inventory_santaclara =df_inventory_santaclara.iloc[7:,]            ##remove unnecessary columns
# delete by axis
# Make system a 2x2x2 supercell of itself
#Split Data ###YOUR CODE HERE###
#We create time series for data:
# Get a list of column names and types # columns
# filter North American mammals whose genus is "Antilocapra" # query, loop over and print out animals.
#
#getting the summamry of the above fitted model
#Best Parameter by GridSearch
# ...and it does not matter whether indexing is position-based  # (iloc) or label-based (loc, see below for a more detailed expo)
# save fixed result to repo # prepared, models, recs
#delete duplicate receipts 80874 duplicate receipts
# from pyspark.sql import crosstab # from pyspark.sql.functions import * # flight.columns
# group your sources and take mean of compound
#Compute annual Nitrate concentration
# quick and dirty investigation for LTDate
# Want to leverage the Company name so need to create dummy variables. 
# Export to csv
# Common citation violations in residential over commercial areas
#'Washington': '01fbe706f872cb32'
# Can use any valid frequency for pd.date_range, such as 'D' or 'M'
#tweepy auth
# Add cleaned tokens as an attribute to by_tweeter:
#assigning meanngful indexes
# create grid id array
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# Name of timestamp to parse on the raw data file
# Inspect the table(s)
# training loss
#Next we will read the data in into an array that we call tweets.
# Divide each number by each countries annual maximum
#create date variables for title
# Convert pandas table into html
#Use the read_html function in Pandas to automatically scrape tabular data of mars facts from the page.
# A:
# Are there any outliers that would make this task difficult??
#dates_by_tweet_count
#churns 
# Visit URL
#Display a few beginning rows.
# Getting rid of null values is easy too
#https://stackoverflow.com/questions/546321/how-do-i-calculate-the-date-six-months-from-the-current-date-using-the-datetime
# Export all tallies to a "tallies.xml" file
# calculate the temperature difference between the two cities
# this period can be used as index in data frame # index which has period is called priod index
# Delete newline characters
# Downcast binary target column 
# check Basin Parameter info data in file manager file
### read in IATTC csv file created by using Scrapy crawler spider (for Chinese vessels only)
#importing from microsoft excel
# Design a query to retrieve the last 12 months of precipitation data and plot the results # Calculate the date 1 year ago from today
# your code
# from sklearn.feature_extraction.text import TfidfVectorizer
# The case of "1) variable (time, hru or gru)"
# masked['user_age'] = np.where(masked['id'], 0, 0)
# accediendo a una fila por etiqueta y obteniendo una serie
# Find number of rows in the dataset.
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# For finding all the links
# Call the 'LangStats' class from DOTCE.
# Reseting index to turn the multi-index dataframe to a single-index dataframe
#load most up to date data
#Find current featured image
# Make columns for seasons and terms
#check if projectId in schedules is also in projects and budgets
# check that it works
# Alternative solution, in Python 3.5 and above # Z = np.ones((5,3)) @ np.ones((3,2))
# Set index
# Drops the specified column from a dataframe # You'll be able to see that the volume column is no longer there
# Creating a new field called Sepal which is the product of sepal length and sepal width # Getting the Sepal values at the 25th, 50th, and 75th percentiles
# Look at first x rows in the table
# The protocol version used is detected automatically, so we do not # have to specify it.
# use pandas to get_dummies
# Look at our current features df
# distribution plot of X test
# Pickle Reader For Exported Data Frame File # /r/news, top 200 threads for big sample testing size. Data From November 6, 2017 # change directory depending on user
# Getting a list of column names and types # columns
# Create a new dataframe for the Airport ID (It does so by testing true/false on date equaling 2018-05-31)
# Now we can check if there are any significant differences, between the 2 labels,
# I did this for the entire set of lead managers before losing all of my work
# load workspace configuratio from ./aml_config/config.json file
# you can add arithmatic operations between two defined periods # here we have 2 quaters defined and if you subtract q2-q # result is how many quaters between q2 and q
# are classes balanced? #86.31% will be benchmark for our models
# Probability of treatment group converting
# set SUMMA executable file
# the attribute of the function (default parameters)
#transit_df['EXITS'] = transit_df.apply(fix_exits,axis=1,iqr=intqrange)
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# read csv
# Use Pandas to print the summary statistics for the precipitation data.
# feats_dict
# of course ignore unit_sales since missing from test
#Set site, parameter, and stat codes
#show the inferred schema SQL style
# calculating number of commits # calculating number of authors # printing out the results
# Treehouse samples are prefixed with TH (prospective patient) or THR (pediatric research study) # so filter for only these excluding the TCGA and TARGET that are already in our other dataset
# Store the Average Sentiments
# this ensures the plot appears in the web browser
#join in the weather for these days! then look at complaint types
# logging.basicConfig(filename=log_file_name, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Get a list of column names and types # columns
# Let's again use the inner join method we used above
# return the original input when unparseable
#Hialeah': '629f4a26fed69cd3'
'''Here we are creating a new column named new_page_converted with a length equal to the new_pages count$ calculated above and filling them with one's and zero's with the mean probability we calculated above for $ one's and remaing for zeros'''$
# Create an environment and set random seed
# scale data
# use dropna to eliminate rows or columns with NaN values # We drop any rows with NaN values
# Use a compound data type for structured arrays
# # This shows a chart of the count of observations by month # 
# A:
# Let's get the more detailed Titanic data set
#fmt date for YYYY-MM cohort
# And visualizing...
# Apply rule of thumbs to top10 
# index date
# Displaying all data from table "measures"
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
#plot weekly actual and forecast
#term_freq_df.head()
# Check the accuracy of the trained model with early stopping
#Training and testing dataset
#df_times.describe()
# Print out the number of movies we will recommend.
# train a simple logistic regression, and...
# output and submit to kaggle
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# To check details of duplicated row 
# save model to a file and restore # with open('house_regressor.pkl', 'rb') as f: #    restore = pickle.load(f)
# Check SFL stuff
# Check the accuracy of the trained model
# More data insepction...
# look for seasonal patterns using window=52 - therapists
# note: although the strs and the converter # have format m-d-y the output has format y-m-d-h-min
# target is the handle. # make trump 1 and sanders 0
#show the shape of dataframe
# set SUMMA executable file
# Frequency analysis for words of interest # Number of unique and total words in the text
# take a look at your results
# Strange data
# We display the first 10 elements of the dataframe: #display(data2.head(10))
#Plot bar charts of the three summary tables
# Saving data
# Model persistence: save(), load()
# Add data to Pandas DataFrame
# Final clean csv.  Other csv's below are for separating categories
#dfFull['OverallQualNorm'] = dfFull.OverallQual/dfFull.OverallQual.max()
#Data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
#.dropna(subset=['place_name_long'])
# Select and display the first 5 rows from the table
# How many stations are available in this dataset?
#import pandas as pd
#Check count of missing state value 
# Initialise with a list. #
# Retrieve page with the requests module
# concat grid id and temp
#lda_tfidf.print_topics(10)
#missing value treatment # Check for nulls.
#Complete a groupby to determine the average math score of each school by grade level.
# A look under the hood:
# show available waterbodies
#Charlotte
# Merging Twitter data frame and Stock market data frame
# FML floating points
#todo 1a. Display the first and last names of all actors from the table actor
#member_pivot
##### named tuple
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017. # 2. Convert the returned JSON object into a Python dictionary.
# Join the SP 500 start of year with current dataframe for SP 500 ytd comparisons to tickers.
# -> Having more severe depression leads to worse outcomes
##choose a 15 day vacation trip
### Create the necessary dummy variables
#how many rows do we have with nan in a given column?
##### ignore
# analyze validtation between BallBerry simulation and observation data.
# plot first 10 samples from training data # plot 10 random samples of test data 
# return a frame eliminating rows with NaN values
# check Forcing list data in file manager file
# TODO: plot discriminating over position taken
#writing year month and size in MB into CSV
# Show the node_types file. Note the common column is node_type_id
#Clipping outliers/wierd values (Conditional Imputation)
# Let's find the 'random' buttom
# Kind of a hack because of the Spark notebook serialization issues
# Create logit_countries object # Fit
# this is what infowars shows up as...
# Need to do this
# Clean up the dataset for viewing purposes
# merge the datasets to compare gene_count and chromosome length # convert a series to a dataframe # merge datasets
#r_train['has_extended_profile'].value_counts()
# Transmission 2050 [GWh], late sprint
# interactions between treatment page and country
# merge with main df
# Word frequency for terms only (no hashtags, no mentions)
# Concatenating the two pd.series
#check url links submitted by suspicious accounts
### Create the necessary dummy variables
# Display data for min and max age
# df_dummies.head() # df_value.head()
# check no more missing values
# filename2 = 'expr_3_qi_nmax_32_nth_1.0_g_101_04-17_opt_etgl_L-BFGS-B.csv' # df2 = pd.read_csv('../output/data/expr_3/' + filename2, comment='#')
'''Evaluating the model with the Test (Hold Out) Data'''$
# columns in table x
#List the unique values inthe HydrologicEvent column
# identify single column to target
#Find all images in original features df where neither the hash nor the filename match anything in the new database
# Put the data into HDFS - adjust path or filename as needed
# sanity check out environment is working
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# adding prefix RATE_ to column names
# drop rows if any col has NA
# Let's bring class in too:
#check that not all the values are 0
# creo columnas para agrupar
# -> Having more life enjoyment and satisfaction leads to better outcomes
#df.dropna(inplace=True)
# Create dictionary: AFX_X_2017
# final sise check
#### Define #### # Convert tweet_id column from int type to object type # #### Code ####
#loading the data frame df 
# Need to factor in that some positions were purchased much more recently than others. # Join adj_close dataframe with portfolio in order to have acquisition date.
# See how many disease labels overlap
# melihat hasil perubahan zona waktu di kolom index
# Add code here #my_model.fit(X_train, y_train)
# Drop three columns we don't use
# Export to csv
# Print the files on one line each.
# Backend
# checking the easist way of looking the files you have in the directory
# ,test_df=df_test #todo: add a test set
#To save excel file 
# create start date column from period index
# Create Data Frame
# Specify multiple aggregation functions as a list. #
# pickle data
# Accessing a column:
# Pandas provides a simple way to create data sets using DatetimeIndex. # An easy way to create one is using the pd.date_range method
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#convert the day of week column to strings, prepping to be dummified
#Make the graphs prettier (code in tutorial is for an older version) #Turn off pretty print while we're at it
# join for plotting purposes
# The proportion of p_diffs greater than the actual difference observed in ab_data.csv is: 89%
#Leitura do data set
# Get the tweets stored in MongoDB
#using value counts in descending order. 
# assert monthly data
# Import csv file from Oz
# lets find the number of rows we have now. We want to  # have a reasonable number to rows to train our deep learning model
# Apply to all movie reviews in dataframe
#Get data from Quandl
# RE.MATCH
# model for all model visualitation
# Veamos una descripcion que incluya todo los casos (numericos y no numericos)
#create arima model
#**Which Tasker has been hired the least?**
# obtain the value of the variable
#do the same for male
# Initialize dictionary # Create dictionary, with dates as the keys for the data in each column.
# check option and selected method of (11) choice of groundwater parameterization in Decision file
# Instantiate a "coarse" 2-group EnergyGroups object # Instantiate a "fine" 8-group EnergyGroups object
#  GIT_LOCATION is the path of the git executable
# datAll['month'] = datAll['Date'].map(lambda x: x.) # datAll['Year-Mon'] = datAll['Date'].dt.strftime('%b-%Y')
# Check that new_stops table contains stops which were in stops but not in oz_stops
#Overall statistics on the entire dataset
# this has been reduced to a Series
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# We see that there are two date/time columns, we'll tell pandas to parse them when loading the full file.
# Create test data for predictions with neural net
# rf.fit(features_class_norm, paid_status_transf)
#this is a little on-off switch for my various long run-time operations that are behind a check. # go_no_go = 0  #GO #runtime items will be necessary once each time after the kernel is reset.
## to get the summary of the counts ##
# we can use standard type __call__ see below
# sorting values
# Inner join pax_raw with the rows we want to keep
##### need this "magic" statement to display plots inside jupyter lab
# Establish a connection to your GIS.
# merge
#df.isnull().sum().sum()
# saving model
# Extract mean of length of tweets
### Create the necessary dummy variables
#Check duplcated rows by all columns at first.
#save preprocessed df #psy_prepro = psy_prepro.set_index('subjectkey', verify_integrity=True)
#Utility function for joining dfs more conveniently
# Total file size (compressed)
# Grabs the last date entry in the data table
# Look at some 1 word ticket
# option 4 (ignores and resets index)
#checking the dimension of the whole data frame
#not required
# Identifying the top 10 authors # top_10_authors = git_log.groupby('author').count().apply(lambda x: x.sort_values(ascending=False)).head(10) # # Listing contents of 'top_10_authors'
# Calculate basic statistics for y by group within x.
# find historical data for 2003
# Query to retrieve the last 12 months of Temperature Data # Use the start date and end date calculated before.
# Cretae the DateTime column from the TimeDelta column that we have in the energy generation data
# view the second isochrone
#In looking at unique values in DESC category, it does not appear to be anything we really care about.
# fit two standard deviations between -1 and 1
# calculating number of commits # calculating number of authors # printing out the results
# display unique values with counts for neighborhood_district
#Create dataframe of sentiments
# Make a new empty pandas data frame
# Save the edges and edge_types file.
# Reflect Database into ORM class
#4 convert all characters to lower case
#Cleaner display
# Run the normalizer on the dataframe
# We can set a MultiIndex while reading a csv by referencing columns to be used in the index by number
#The dtypes attribute reveals the data type for each column in our DataFrame.
# why tweets contain "instantaneamente"?
#Nasdaq 100 Company list
# notice the change in encoding in the output
# exercise (page 161) # extract the data highlighted, *as 2d arrays*
##print(r.json())
# removed string word comment
# Convert text to a Python object using the 'json' package # And now we have a Python list:
# Percent of true gains over all samples: compare to precision -- precision should be signficantly higher if this is a useful # prediction # (cnf_matrix[1,0] + cnf_matrix[1,1]) / cnf_matrix.sum() 
# Load in your own data
# Downcast Only # data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/ef3f532b-7570-43d9-b016-6b58c4429b15/dar24011.asc' # Down and Up Casts
# df_search_cate_dummies.head()
#Count unique topic_names within dataframe
# Load the list of daily normals into a Pandas DataFrame and set the index equal to the date
# create a period index representing # all monthly boundaries in 2013
# Qn 1. # Collect data from FSE for AFX_X for 2017 with API call
#we got two new feature .now update the dataframe with updated with new information
#Import file 'Sample_Superstore_Sales.xlsx' which is in FourFiles.zip
# use matplotlib to create a bar chart from the dataframe
# Create hourly data frame
# Fit pipeline 
#print (r) #print (data)
# OPTION_CONTEXT???
# to copy the next value in the row, we use bfill
# Select row of data by index name
#tweepy API Authentication
##categories can be considered counts over time. Which could then become ratios.
# Without effect modification # drop1(adj.glm,test="Chisq")
# reg_target_encoding(train,'Block',"Real.Spots")  # tbd # reg_target_encoding(train,'DOW',"any_spot")   #tbd # reg_target_encoding(train,'hour',"any_spot")  # tbd
# {0: -7.4305555555555545, 1: -15.097222222222221, 2: -7.263888888888888, 3: -5.097222222222222, 4: 3.402777777777778, 5: 8.069444444444445, 6: 16.569444444444446, 7: 9.736111111111112, 8: -0.7638888888888887, 9: 1.902777777777778, 10: -3.263888888888889, 11: -0.7638888888888887}
# Check how many countires there are to determine how many dummy variables are needed
# Define default tick locations for our plots
# show you the table after clean work 
#Concatenate (or join) the pre and post summary objects
# exercise (page ) # 1) extract the superdiagonal # 2) extract all the numbers divisible by 3 using a boolean mask
#replace outlier with nan for correct mean calculation #replace with mean for now. Todo find a better way to replace them. 
#Target city = Charlotte, NC
#create session and initialize variables
# drop duplicate row # To confirm if duplicated have been removed
# preview the data
from datetime import date, datetime$ def json_serial(obj):$     """JSON serializer for objects not serializable by default json code"""$
# Import the csv file with historical neo data and other variables
# load the endometrium vs. uterus tumor data
q = """SELECT avg(hours_per_week) Average FROM adultData where sex='Male' and workclass='Private'"""$
#read value into row, the same session created previously will be used
# Find customerID 
# Create time series for data:
# creating a new dataframe with just the columns I need
# roi['roi_14'] =  roi_on_day(14, users_df, orders_df) # roi['roi_30'] =  roi_on_day(30, users_df, orders_df) # roi
# shows time stability
# set input shape for ANN
#     print('Deleting the {} table.'.format(table_name)) #     connection.delete_table(table_name)
#Note the as_index option will enable the datasets to be joined. The default behaviour of the groupby method is to make the groupby variable an index.
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Load relevant libraries for tf-idf and k-means clustering
#getting dummies for custom
# cisnwf6
#users whose most recent scn-status is canceled followed by trialing and were mis-categorized as churn
# Create a dataframe with only the Adj Close column as that's all we need for this analysis.
#Question 1
# Assign directory paths and SQLite file name # sqlite_pth = os.path.join(dpth, os.path.pardir, "data", dbname_sqlite)
#df[df['message'].map(len) < 2] #any empty or very small messages?
# do note the freq
#Drop NaN.
#print(df3) #print(df_list) #print(df3.head())
# The database file we create will be called hawaii.sqlite
# Reassign data frame without null value rows in name column
# Display the new number of thermal distesses
# overall info
# HITS THE SERVER: downloads data from yahoo for selected EFT
# Model is very overfit since the test score is much lower than the train score 
# Create a scatter plot, assuming data are in time order.
# Data frame
# figure out what was the spike at 5-6 am on Small Business Saturday # more than 90% of the collected tweets were created between 5:30 am to 6 am Eastern time # So let's focus on those
# running in one Core
#Test the return values
# Convert Timestamp series into datetime format
# Print the head of airquality_pivot
# 1. How many recommendation sets are in this data sample?
# print(listings['price'])
# what are our unique tags?
# Referencing the adj_close dataframe from above
# Seasonal Difference by itself was not enough!
#Drop empty columns(axis=1 for columns and axis=0 for rows;  #how='all' to drop if all values are nan and how='any' to drop if any value is nan)
# Write data to excel file in sheet with labeled with exposure time
# count number of each unique values
# converting the timestamp column # summarizing the converted timestamp column
# proportion of p_diffs greater than the actual difference observed in ab_data.csv is computed as:
# Count tweets per time zone for the top 10 time zones
# Converting data type
# Let's pull top 5 subreddit in the most number of comments
#help
# add appropriate prefix and suffix to metadata keys  # get indices for rows where the metadata keywords are present
# Remove Dublicate user_id but keep data frame df2
#Quandl_DF.drop(['Date_series'], axis = 1,  inplace = True)
###
# return the rows where the temps for Missoula > 82
# what is one day from 2014-11-30?
#get rid of all the PPK data with no photos and the empty fields
# predictions = grid.predict(X) # print(classification_report(y, predictions))
# how many unique authors do we have?
# Take the mean of these two probabilities
#you can get this from https://www.kaggle.com/egrinstein/20-years-of-games #contains reviews about witcher 3
# For Displaying Data
# Use `engine.execute` to select and display the first 10 rows from the table
################################################## # Load train set user and test set user ##################################################
#Remove columns which are not necessary to answer the question I would like to answer
#ignore_index means the indices will not be reset after each append
# Reflect Database into ORM class
# This will print the most similar words present in the model
# convert array values to floats
# fit the model to trainng data
# np.where returns all the indices for which a condition is true
# print all the outputs in a cell
#import the prepped dataframe that contains weather data and start date and age and distance
#Lastly drop words that are only 1 character post stemming
#create a pandas dataframe to store tweets: #display the first 10 elements of the dataframe:
# A:
#dfs[25].head()
# According to 'column_names' # 2nd column = Open Price
#Converts time stamp to date and time.
#Saving variables
## Print top 10 tweets and notice Sentiment field added at the end of each record
# We'll hold out 10% of our data for validation and leave 90% for training
# Check whether the zipcode and city difference are correct.
# merge the labels with there games 
# plt.bar(Daily_Price[0],Daily_Price[1]) # plt.show()
## Note the type, its dict ## Dictionary have key dataset and nested key data. Lets see "column_names" details
#print(highlight(json.dumps(jevent_scores, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter()))  # remove the comment from the beginning of the line above to see the full results # Convert to a data frame and show all flagged life events for this client
# Preview the first 5 records of the DataFrame using `head()`. # There's also a `tail()` function!
#Resetting the index to avoid future issues #Dropping unnecessary issues due to save memory and to avoid issues
# Make the graphs prettier
# how many users
# We create a column with the result of the analysis:
# Print the shape of df
hotel_query = """ $         """$ booking_df = snowflake_query(hotel_query)
# check value counts # pd.value_counts(RNPA_new['ReasonForVisitName'])
# boxcox
# Start to extract the number out of the Notes column. # Replace the hyphen in '15-day' with a space to help splitting and extracting digits.
# Printing the content of git_log_excerpt.csv
# We change the row label from store 2 to last store # we display the modified DataFrame
#inspect dataframe
# Import file
# check if dataset is imbalanced
# Fill NaNs with zeros
# Keep only unique play text, keep the first timestamp
# creates nodes in a graph # "construction phase"
# Largest change in any one day (based on High and Low price)?
# write your code here
# caution
# DataViz libs
# check y
# Find the new_page and 
#Dropping the duplicates to then merge later
# Latest Date
# DEFAULT, MERGES ON ALL OVERLAPPING COLUMNS
# looks like a bunch of empty strings when a location isn't found, so that's fine.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned # Update link with start data filter, api key and data for the Frankfort stock exchange (FSE) 
# df_2002
#exportamos a un csv
# Annnnddd... now every column has no string datatypes... :) 
#droping the non unique id 
# Take a peak at the data
# We drop any columns with NaN values
# df_2014
#info_final = pd.merge(info_final, avisos_detalles, on = 'idpostulante', how = 'inner')
# check if there is any null value
# Decision Tree Model
#either way to find unique recSets (using groupby or unique count)
#show behaviour during sepcific time window #pdf.loc['2016-1-1':'2016-3-31',top_allocs[:10].columns].plot()
#Bar plot for overall sentiment
# Select desired columns
# Create logit_countries object # Fit
# write code to drop column below:
# we create a new table grouped by city type and driver count and count the number od rides in the city
# Fill NaNs with zeros
# Run the testing model 
#Plot using Pandas
# Verify that there aren't any null values
#CHURNED SLICE, activation dates
# calculate mean on compound scores
#Calculate the lowest opening prices for the stock during 2017
# Create the deployment
# printing the most likely IOB tags for each POS tag # extract the list of pos tags # for each tag, assign the most likely IOB label
#create dummy variables for the categories
# and we can see it is a collection of timestamps
# Train our classifier by giving it 1796 scanned digits!
# READ IT IN USING # df = pd.read_csv('citation-data-clean.csv')
# look for '&amp;'
# Convert sentiments to DataFrame
# Just Use DataFrame to have a clear view of the dataset
# builtins.uclresearch_topic = 'GIVENCHY' # builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'FLORIDA'
# concat df and coming_next_reason
# convert timestamp to a time value
#Example
# Load results for
# Tokenize the text 
# Build a KNN classifier # k is chosen to be square root of number of training example
## Now predict on the test set with the training model
# 5.
#Train Model and Predict  
# change type of int variables (from float to int, possible because no more nans)
# Change path delimiters to unix format if necessary
#changed the name of columns
# total unique ids of the new dataframe
# http://w2.weather.gov/climate/xmacis.php?wfo=mtr
#Display head of df_students
# A priori tenemos la misma cantidad de columnas, veamos que se corresponden los nombres 
#50th is 6, 80th is 38, 90th percentile is 183 up_votes, 95th 997 up_votes, 99th is 15170, max 192,674
# Rearrange the columns in the name of order
# Number of populate for new Page
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# create period range:
cur_b.execute('''$ ''')$ conn_b.commit()
#Summarize, **using our own percentiles**
# First, import the relevant modules
# Compute sentiment for each tweet and add the result into a new column: # Display the first 10 elements of the dataframe:
# Concatenate series # Index has to be ignored because the series 's' does not have the index column
# create a pandas dataframe # display first 10 elements of the dataframe
# Join both sources into a single dataframe
# n_new transcations conversion sample values
#joined_test.reset_index(inplace=True)
# To grab the other needed files, execute this code block command: 
# Run all sequence to structure alignments
#Use Pandas to plot an area plot (`stacked=False`) for the daily normals.
#and then read in this document to analyze # df = pd.read_excel('SHARE_cleaned_lists.xlsx', index=False)
# Convert OpenMC's funky ppm to png # Display the materials plot inline
# Take a peak of the data
# num_regex = re.compile('[-+]?[.]?[\d]+(?:,\d\d\d)*[\.]?\d*(?:[eE][-+]?\d+)?')
# Display the graph for performance comparison
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# First option - the slice object
# builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
#Get last year of precipitation data
# Concatinate each Data Frames 
# tweet_id is int type instead of object type. # Capitalize the first letter of p1,p2,p3 columns to make it consistent # Remove underscores '_' between words in p1,p2,p3 columns 
# Read the item's data
# Call the function to pull NWIS data
# Import the required libraries.
# The same notation that can be used to select a subset of rows,columns with  # lists or arrays can be used with .iloc:
# option 3 (uses only index from df1)
#let's use the function with our list #Pass the tweets list to the above function to create a DataFrame # sort by retweet count
# Remove columns using drop ... axis=1 for column
#query data
# current fire predictions for grid cells
#prints a column
# Merge mojo and youtube
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#  Quantile-Quantile plot of geovolatility
# Create a Word / Index dictionary, mapping each vocabulary word to # a cluster number                                                                                            
# number of tweets
#A different approach using apply() method and a lambda function:
# find a positive 606 value in original dataset
# df_2013
### Deal with the outliers of the data convert it to the statistical mean of the data check how to do that using other attributes  ### for the time being fill it with mean 
# print min and max date
# Gensim Library
#creating url for year 2017 by changing the start+_date and end_date parameter of query string. #Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 
# Merge CSV
# Mars facts # url of page to be scraped
# 15 topics seems like too many for donuts so let's go with 10 # train LDA using 6 cores (1 master + 5 workers)
# take a look at the dataset
# join information about first answer into the frame # RUN ONLY ONCE or results in duplicate rows
# lowest 
# To install package which finds arima order  # !pip install pyramid-arima
# close connection to SalesForce
# Store the API key as a string - according to PEP8, constants are always named in all upper case
#converting all the datetime #segments.st_time.apply(lambda d:datetime.strptime(d, '%m/%d/%y %H:%M'))
#test = raw_large_grid_df.query("subject in ['VP1', 'VP3']")
# Transmission 2020 [GWh], late sprint
# Adding a column for sentiment    
# also a series
# this is just for unscaling
# replace the indexes of the dataframe with the correct header columns by renaming them
#Select rows where the Mean flow was less than 50 cfs
# image does not display on github  # either download and run the notebook or navigate to the URL below to view the image
# building a dataframe with columns 'features' and 'relevance scores' # Since, the relevance score is compute over the embedding vector, we aggregate it by computing 'mean' # over the embedding to get scalar coefficient for the features
# Using the API object to get tweets from my timeline, and storing it in a variable called public_tweets
# plot the top 5 buzzing subreddit by # of posts
#Shuffling the data
# Run the model with training set 
# Create and display the first scatter plot
#overallYearRemodAdd = pd.get_dummies(dfFull.YearRemodAdd)
# # This doesn't give us an accurate base to compare stocks in our portfolio. It's all over the place.
#Create the Pandas GroupBy object
# retrieve stored API_KEY
#probability that user gonna convert
# backward fill
#delete duplicate receipts 80874 duplicate receipts
### WCPFC list to start and try to merge IOTC list into it
# Convert Y from 0/1/2 to one-hot format
##Load in officers data
#Save df to csv
#feature importance
# import a .csv file (data from blood testers usually exported as .csv or .txt) # bg2 = pd.read_csv('/home/hbada/BGdata/Libre2018-01-03.txt') # when using pythonanywhere.com # and the data is tab delimited!
# your code here # a DatetimeIndex was created
#use Pandas to convert data to HTML table string
#Design a query to calculate the total number of stations.
# Take a peak at the data
#### Define #### # Convert tweet_id column from int type to object type # #### Code ####
# Obtain the hour of the day and the day of the week for each observation and add them as features to the dataframe
# Once again need to delete the new Date column added as it's redundant to Latest Date.   # Modify Adj Close from the sp dataframe to distinguish it by calling it the SP 500 Latest Close.
## Q1. How to print out dictionary values using for loop?
#Getting right time
# Load dataset
# Print out single tweet
# 'Shreveport': '4ec71fc3f2579572'
# Create the design matrix and target vector
#Question 4
# create pandas DF
# create a list gruped by type from the alreay created table to find the number of drivers 
#Use tweepy.OAuthHandler to create an authentication using the given key and secret #Connect to the Twitter API using the authentication
# Concatenate multiple data frames into a single frame
# Define a date range #print(dates[0])
# view the entity pairs in descending order
#hierarchical Indexing
# if necessary delete the parquet file
# (i.e., if the memory is contiguous)
# Apply the function
#same result as before!
# write the scaled and unskewed data back to postgresql
# Query for 1 week before `2017-08-23` using the datetime library
#Downloads prehosted dataset from Dropbox
# Scoring
### Validate Pickle ###
# Retrieve latest full mars images
# integridad db
# (target: Series type)
# Time is stored in a raw computer format # But we can convert it to a datetime object so it's comprehensible. # note OTP returns raw time value with three extra zeros, divide by 1000 to get rid of them
# Create and display the second scatter plot
#convert to date objects
# Set some simple earthquake related keywords:  # Collect 100 tweets using the keywords:
#getting the unique count of all program codes
# view regrid humidity
# Create BeautifulSoup object; parse with 'html.parser' # Examine the results, then determine element that contains sought info
# Check that there are 500 tweets (100 per news source)
# query values in set:
# open Chrome
## make an array of midnight datetimes for slicing dataframe by days
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#count of words in messages (posts)    
# query to pull the last year of precipitation data
# 4.
# load data into H2O object
#`refit` raises an error, so modified
# Read the data
#Word frequency for all terms (including hashtags and mentions)
#Your code goes here # normalization factor # seting up the loss function
# Let's get some data; top stories from lobste.rs; populate a DataFrame with the JSON
# Display a list of all the models
#Total retweets, favorites
# Check if the word 'fell' exists in the vocabulary
# From the object 'report', list the most important features.
#Pivot table
#recarguemos el archivo parseando ahora issued_date como date
# Now we load the data from the query into a pandas DF: # call it "rain_df":
# Restart runtime to allow Jupyter to know the changes above
# Load a single column for rows 0 to 100
# Convert test_age to series; set to new variable 'age' we add to df
# ETHNICITY_LEVELS[0]=='Asian', so I'm interested in e==0.
#ii. How viral are my posts? #Ans. Get the sum of all the fav_cnt and group by users order by fav_cnt desc; #     The user with most counts has the most viral posts.
# Importing Libraies
# List comprehension for reddit post timestamps
# Optional: Create an output folder and write dataframe out to CSV
# R squared adjusted
#feature selection
# series with all NaN
# Reset the index of airquality_pivot: airquality_pivot # Print the new index of airquality_pivot
#Put all data into dataframe sentiments_df
# just trying different slices of the data
#lets authenticate with Twitter which means login via code
#Complete a groupby to determine the average reading score of each school by grade level.
# number of active authors per month
# Add sentiment as new column in dataframe # Display the first 10
#Complete the inner join of the school and students dataframes to reutrn a combined (merged) dataframe. #Drop the School ID and Student ID columns to reduce the columns in the merged dataframe for readability
#Create a column of flow in cubic meters per second
# Assign the measurements class to a variable called `measurements`
# initialize your estimator # train your model
#quantitative-categorical-categorical
# Define the demonstration data. #base_df.drop(['index'], inplace=True)
# Save figure
# Design a query to calculate the total number of stations.
# Take a peak at the first couple of rows in the dataframe
# Group by twitter account
# mapping from integer feature name to original token string
# Load the data of the scraped results 
#High on We and collective phrases
#out_columns=out_temp_columns+incremental_precip_columns+general_data_columns+ wind_dir_columns #columns to include in output
# Convert sentiments to DataFrame
# Mapping id -> slug # Mapping id -> slug (will be used for readability)
#Put all data into dataframe and export to csv file
# Inspect data types
#get rid of some unnecessary columns for the purpose of our investigation
# Convert'created_at' time data rounding to nearest minute 
# follow up @8/14/18 # The web framework gets post_id from the URL and passes it as a string # Convert from string to ObjectId:
# Make Predictions and output the results
#ignore_index means the indices will not be reset after each append
#Let's first create a new dataframe with only the Permit Number, Current Status, and Completed Date columns from the  #dataset. 
# do the two isochrones intersect?
# skip only two lines at the end # engine parameter to force python implementation rather than default c implementation
#reorganize data set
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Let's display what is contained in the node_types file.
# drop rows with NaN
# To get the total number of unique values for users
#query to calculate the total number of stations.
#sns.heatmap(corrmat, vmax=.8, square=True);
# the version iterates over the data
# FAVOURITE TWEET:
######################### Combine Data Frames ############################## # To flatten after combined everything. 
# merge in office name from offices df
# Show how to use Tally.get_values(...) with a CrossScore
# Extract projection information # metadata['projection'] = refl['Metadata']['Coordinate_System']['Proj4'].value
# merge with council data #df7 = pd.merge(df6,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE',suffixes=('_created','_closed'))
# Print the value counts for 'Site Fill'
# output.coalesce(2).write.parquet("C:\\s3\\20170503_jsonl\\output.parquet")
# I can find out if Katie Hopkins has tweeted more than one time:
#we need so we can later import for anaylis the whole dataset. 
# generate a series based upon business days
# Saving to csv to open again with chunks
# Inspect the hierarchy of the json dictionary 
# dropping columns '[', ']', and ','
# Counting the no. commits per year # Listing the first rows
# count the number of times each GPE appears
# Drop null values
# can also use
# find historical data for 2016
# poo
#Display all the columns
# create weekday vs weekend column for icu_intime 
### read in WCPFC csv file created by using Scrapy crawler spider (for Chinese vessels only)
# Convert categorical variable into dummy/indicator variables
# Join Dataframes
# Pandas series are mutable, i.e. we can change the elements of the series after it's been created
# check to see if there are any np.nans
# Range chart statistics. # Calculate the average range.
# Create ARIMA model
# coins that are out of top 50 today
#bikedataframe = bikedataframe.drop(['DATE', 'REPORTTPYE'], axis=1)
# transform all of the data using the ktext processor
# Copy data frame and apply function
# Plot the results using DataFrame.plot
# and the first ten records
# read in msft.csv into a DataFrame
# Golf KOL pred
#doc source https://seaborn.pydata.org/generated/seaborn.countplot.html
# indexer untuk tanggal pembuatan status - pas di tanggal tersebut
# which values in the Missoula column are > 82 ?
# create a set of users # randomly sample 20,000 users
# checking whether the data has any null values
# save the file to the output
# Drop missing values (Birth Year)
# create P attribute  # create Plot attribute with open_netcdf method
#.get_group((2015,871))
#date_splits
# Check is Data is imbalanced
#converting date to datetime format
#model = load_model('FC_weights')
# 13. Print rows 100 to 110 of only the first 3 columns in free1 using only indices.
# Score gridsearch logreg model on test data
# RETRIVE TWIT IN EXTENDED MODE (TO-BE)
# use logical indexing to return the dataframe rows where source is GRCh38
# Export the dataframe to a csv
# write your code here
### Fit Your Linear Model And Obtain the Results
#R16df.rename({'Create_Date': 'Count-2016'}, axis = 'columns')
#combining the new dummie variables table to the original table using user_id
# Stats about Tone
#number of unique projectIDs in projects
# results = soup.find_all('div', class_='img')
# specify a new set of names for the columns # all lower case, remove space in Adj Close # also, header = 0 skips the header row
# writing it to csv #pred = pd.DataFrame(predictions, columns=["rating"]).to_csv("24nov.csv", index=False) #predictions.to_csv("submission.csv", columns=["id","rating"], index_lable=tf.id)
# default DatetimeIndex and its Timestamps do not have # time zone information
# print the HTML for the first result.
# df.loc['2013-01-01'] # df.loc[:,['A','B']] # df
# If there was no accident, set value to 0, not NaN
# Upper value.
# Check whether columns have been created in dataframe
# Extract specific values - 2
# ciscih8
# pick your favorite classfication model
# Setup Tweepy API Authentication
# import and instantiate TF-IDF Vectorizer
# Import the built-in logging module and configure it so that Word2Vec creates nice output messages
# Format column AB (column 28) to two decimal places.
#see data distribution of active/inactive stations
# groupby by defaut creates a (hierarchical) index:
# set list of columns/points on yield curve used
# Drop the irrelevant columns
#checked values using (len(old_page_converted))
#the WOEID of 2459115 is NEW YORK #url = 'https://api.twitter.com/1.1/trends/place.json?id=2459115'
#JPL Mars Space Images - Featured Image
# Transform injury_df Date field to match the games_df date field:
#produce statistics for valid/invalid AC
#The number of events hosted by groups 
# REPLACE PATH TO THE FILE
#Upper and lower bollinger bands
# Lenghts along time
#resolve entity names to deduplicate
display(HTML('''Account with max following:<br><a href='https://github.com/{}' target="_blank">{}</a>'''.format(max_fwing.login, name)))
# Look at one of the ticket
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
## Get the contents of interest: all the p's # ** p means paragraph in html. Check more tag definitions on w3schools.org
# import matplotlib # matplotlib.pyplot.plot_date(df['newDT'], df['ED'])
#m1.diagnostics()
# What is the maximum value?
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#plt.xticks(['2017-07-01 00','2017-08-01 00','2017-09-01 00','2017-10-01 00','2017-11-01 00'],['7-1','8-1','9-1','10-1','11-1'])
# Shuffle the rows in search2
# Create the inspector and connect it to the engine # Collect the names of tables within the database
# Save the dataframe in a csv # Repeat the process for all tweets
# make predictions
#Join events and groups
# Create the ticks for our bar chart's x axis
# url to read # read it # examine a subset of the first table read
# Simulate conversion rates under null hypothesis
# doctors
# Assert that all values are >= 0
#print(dictionary.token2id['movie']) #verify if these words were in the stopwords list #print(dictionary.token2id['n\'t'])
# columns in data # first and last item in data confirming I have all 2017 information
# item_lookup = shopify_data_simple[['child_sku', 'child_name']].drop_duplicates()
#fetching the first 5 entry 
# pipeline / fit / score for lr & tvec
# Display location of saved model.
# open netCDF file
# Similarly for an instance, although this really is a dict, not a mappingproxy.
# output2.take(2)
# merging zipincome with zip_dest
# duplicates #intervention_train.drop_duplicates(inplace=True)
# view df info
# Write out the DF as a new CSV file
# dateutil
# wall time to train model
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
# For Displaying Data
# Edges files
#Column clean-up
#'Glendale': '389e765d4de59bd2'
#read in mike objects #mike.sort_values(by='RA0')
#Create series with just the closing price
# Common citation violations from parking during certain hours and expired meters
# dataframe is like a powerfull spreadsheet # We print the type of items to see that it is a dictionary
# Location outside US, ignore
# Width # Color # Values as integers
# convert ticket_closed_date_time to a pd.datetime field
#Replace '|' with ',' and then replace ',' with ',\n' to save the entire dataset into an excel
# get new and existing patient dataframes
# When using a Datetime Indexed data set, you get some pretty cool  # new methods for calculating different 'rolling' statistics
# pandas default datatype given
# directory
# Because of GitHub space limits (no files over 2GB), train data file was split into 5 pieces # Loading the first file with header row to use for column names # Checking the columns present
#Show the last 5 rows 
# return a frame eliminating columns with NaN values
# simple word algebra example:
#Activation cohort x cohort
# Get the columns (movieIds) for the current user
#weight_is_relevant = 1
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#== Transpose (like numpy)
# Most retweeted
#'Mesa': '44d207663001f00b'
#Read in the data into the 'sites' dataframe 
# subset temp to us
# Number of non-merge commits that modified the manuscript markdown source
# keys should be all matched keys up to date
# lg = pd.read_csv('awesome_go_logging.csv') # TODO: should have a function for that ....
# You can then convert the np array to a list
# Identify rows which are transfers from Coinbase to GDAX
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# check the simulation start and finish times
# data munging # split language and region
# analyze validtation between Jarvis simulation and observation data.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Define number of Training samples (70 %), Validation (15%) and Testsamples (15%)
# load the model from disk
# run the model giving the output the suffix "rootDistExp"
# Pandas DataFrame with the Sentiment Analysis results
# top 5 breweries
# Merge the morning traffic avg with census data
#### Read the updated dataframes for analysis
#hist_alloc.loc[::int(len(hist_alloc)*0.1)].plot(kind='hist')
#Buffalo
## a method to convert the response into JSON!
# How would we do the same, but groupby both primary and sedondary description?
#Remove unnecessary columns
#Exporting transactions with their associated orders
# note we just counted the length of the "legs" output, it contains the details of the actual route # here is what is included in a "leg"
# all data on June 3rd of 2015:
#DATA_ROOT = 'data/workshop-content18/5-cloudpbx/data/cloudpbx_sample_data_10k/' #CSV_FILE_PATH = os.path.join('locn-filtered.csv')
# view the head of our df
#read the data (cleaned and merged)
#df_csv.info()
# Save the query results as a Pandas DataFrame and set the index to the date column
# Show Figure
# select scalar values using at
