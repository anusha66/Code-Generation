# end month frequency from given quater
# save our crosstab
# Inspect column names 
# HACKER EXTRA - experiment with additional plots that help us understand this dataset
# Repeated part from morning track: impute with the mean 
# changing the name of a series:
#dropped the Sec filings column using .drop function
# Convert the returned JSON object to a Python Dict
#My routine to conceal root passwords
# Create dataframe from the .csv files or simply read in csv files. 
#Then I resampled by the hour, taking the mean duration and distance traveled for every hour,  #as well as the sum of all the trips and members.
# problem 6 solved.
# Calculate what the highest and lowest opening prices were for the stock in this period.
#How about average number of retweets and favorites per timezone?
#'Santa Ana': '0562e9e53cddf6ec'
# This compares the stocks as if they were all starting from the same price 100
#It looks like there must be duplicate rows since there are ~85k unique permits out of 97k permits in the dataframe. Let's first find out how many entire rows are duplicates of one another. 
#Checking to ensure I pulled all outlets
# create a Series with a PeriodIndex and which # represents all calendar month period in 2013 and 2014
# Veamos permit type y permit_type_definition
#Let's try using groupby to see what different language users were doing
#df = pd.read_csv(reviews_file_name, sep=',', quotechar='"', encoding = "ISO-8859-1")
# Step 7: Display the shape of the pivoted data transposed ## Think in terms of 2036 observations and each observation includes 24 hours (i.e features) ## Review the PCA - Principal Component Analysis from sklearn
#Example2: Import File from URL
# Importing data
# count at 0
#Import BeautifulSoup into workspace #Initialize BeautifulSoup object on first comment
# Select only the `date` and `prcp` values.
# don't forget to change url path to that of your device. Ex: url=("/Users/name/folder/detroit_census.csv") 
# df.at['Row', 'Column'] = 10 #note the funky name is still present as Index
# get head or tail elements
# We can append a column to our existing index
# area plot of their age distribution
#---Connecting to a MongoDB client---
# print "Loglikelihood:\n", "\n", loglik, "\n \n LL Calc\n", loglikA
# Construct the client object # Build the authorization url for your app
# Create the inspector and connect it to the engine # Collect the names of tables within the database
#cols = ['sentiment','id','date','query_string','user','text'] #df = pd.read_csv("F:/web mining/webmining project/crawler/annotation_data/Samsung_with date_Merged_final.",header=None) # above line will be different depending on where you saved your data, and your file name
    """$     Solar Incident Radiation Based on yearly guess & month.$     """$
# Check null value exists or not
# get a small_lst for demonstration purposes only
#Histogram
# Plot in Bar Chart the total number of issues created for every Phase based on thier priorites
# The two partitions have to be ordered by looking at the extra field in the first line: all A rows need to go before all B rows
# Fit the model
# convert id to long instead of string
# Pulls the last 5 columns from the data
# Now, train the model
# maybe a pie chart showing the different users and the magnitude of their contributions is the total number of lines changed/removed/added??
# Dataframe of user information, here we can determine the last month when they were seen 
# Dependencies # URL of page to be scraped
# if you want to conert period index to date time index # since this is quater so it will take starting date of quater
# search of life expectancy indicators
#show how to get the other comments of a user #print(submission.title)
#Save Data into CSV
# create a trial parameter object to check root distribution exponents
# fire report  #fire.head()
# Read csv
# your code here
# pickle final data
# adding prefix BAND_ to column names
# most top-N informative features
#happiness_df average by year
# Checking shapes to make sure our matrices are congruent
# There is missing data again, so we drop them as well:
# used the requests.json() method to convert the JSON object into a Python dictionary # I printed the column_names here for reference
#Save compound data in csv file 
#Baton Rouge
# foreach through all tweets pulled # printing the text stored inside the tweet object
#Now let's get the genres of every movie in our original dataframe #And drop the unnecessary information
# calculating or setting the year with the most commits to Linux
# Sorting the Dataframe based on DealID
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser'
# how many 0 and 1 are there in the affair column.
# Make a train/test split using 20% test size
# run ltv model
# find the best combination of the hyperparameters
# favorite table # for row in table_rows: #     print(row.text)
# target is the handle. # make trump 1 and sanders 0
# Extract just the BTC wallet transactions
# Load tips data set # Display several random rows
##1.4. Creating a (pandas) DataFrame # We create a pandas dataframe as follows:
# Print the info of df
# this is the default, raise when unparseable -- convert cell type to Code to see error
# create two Series, same start, same periods, same frequencies # each with a different time zone
# What are the most active stations? # List the stations and the counts in descending order.
#Birmingham
#get highest within-day change
# show summary statistics for number of claims
#### Results for 09-05-2016
# create a new dataframe with the indexes of the mismatched parings excluded from analysis
#word_counts.Tweets.astype(str)
# Lets load the Concatenated DataFrame and the engine specification will help to delimit the outliers
# Create BeautifulSoup object; parse with 'html.parser'
# Convert stock ids & names lists to Pandas DF 
#Reading complete data #Reading the marked as anomaly explicitly #testdata = pd.read_excel("D:\AML\AnomalyDetection\osgTestData.xls")
#type(rptdt)     pandas.core.series.Series
# Put in function to remove stop words
# query to pull the last year of precipitation data
#Example1:
#mongo returns a generator, which we call cursor
# save the model to disk for reloading later
# show a sample of the sub gene dataframe
#scrape just 5 of the threads
# Load data # Print shape
#df_TempJams['lineString'] = df_TempJams['lineString'].apply(lambda x: loads(x))
#Convert the returned JSON object into a Python dictionary.
# Example 1:
# categorizing fan zipcodes
# get repeats gff
# Create a list of filenames, this will be used to loop over each of the different years netCDF files
# transform the data
#Authorization
#confirm scns_created,associated scns_array in sequential order...should be null
# import my method from the source code, # which drops rows with 0 in them
### START CODE HERE ### ### END CODE HERE ###
# rebuild and display empty database
# df = pd.read_excel("msft.xls", sheetname='aapl) #sheetname
# Sort the data by time, so rows are in chronological order
# Importamos  matplotlib
# Encode day_of_year as cyclical
#grouping the function by the GICS Sector column using groupby
#upload csv containing data set provided by LA county
#### Create a column that has a number that symbolizes whether a row is close or not to the 'real' pick #### We'll do this first for Top McMurray and then top Paleozoic, which is basically base McMurray #### Top paleozoic version
# Create a "tallies.xml" file for the MGXS Library
# Plot toy dataset per feature
#Obtain all the column names for later calculations
# same as above
#Click here and press Shift+Enter
## Solar Flare 2017-09-10 
# generate the Word2Vec model
# Removing the MTU and MTU2 columns
## 2. Visualization and basic statistics
# T :: it will transform the table and make rows as columns and columns as rows
# print(address_df.shape)
# Calculate the exponential of all elements in the input array.
# To create a column for the intercept # To create a dummy variable column #df2[['new_page','old_page']] = pd.get_dummies(df2['landing_page'])
# your code here
#correlation of continuous variables
# Load gh_api_token & meetup_key & genderize_key
#Extract the month from the datetime column
# date
#Read data into a pandas DataFrame. Sorry this also takes a moment, approx half a minute
#Save
# drop nulls created by lag variables (5 nulls per batter. amounts to rougbly 5k observations # in other words, 0.8% of rows lost to creating 5 PA worth of lag columns
# get defects - had to read in excel as NCR tables not working for moment.
# Export data to csv
# break down specialty category by provider ID number
#Show all of the tables in database #We'll use the nyctaxi table, created using code similar to https://github.com/toddwschneider/nyc-taxi-data
# And, plotting the comment score distribution for this Dataframe...
#Group by the status values, and then describe..
# Functions to count the number of files per folder
# encodedlist is train_X
# Merge csvs (2)
# RE.FINDALL
### Station Analysis #Design a query to calculate the total number of stations. #I'm doing a group_by just in case there are duplicate stations
#Show 100th row
#total count of individuals
# View dataset dimensions
# Cyber Monday
#SAVING TO LOCAL DISK:
# and what is the minimum life expectancy for each year
#create a data frame of the actual values on second measurement and our probabilities from the first predictiojn
# The base URL where the OrgAPI is located.
#Auto ML
# first, value counts can show us in text form what we're looking at
# create new column 'figure_density' as difference between grant date and filing date # show summary statistics for figure density
# %load solutions/query.py
# Create a datetime variable from Create_Date this operation takes up to 3 min
#May-Nov has (31 + 30 + 31 + 31 + 30 + 31 + 30)(24) = 5136 hours in total. There should be 5136 entries
#Example 1: 
# selected items from alice's cart
#perform delete
# Divide each number of each countries columns by it's annual maximum 
# df_2015
#sort index by date (earliest tweet to most recent tweet)
#Review summary stats for the data set
# extract mean of lengths
# Add markers that are circles and of a certain size.
#festivals.rename(columns = header)
#output df_SP to a file
# Sort by co_occurence (max co_occurences in first row of df) # Keep only top10 co-occurence
# Load a single column from table
# load dataset
# Apply new function to the column entries
# Retrieve NASA page with the requests module #Display information from page being scraped
# save as csv file
#read in the cities file
# Likes vs retweets graph
# now I want to replace some scores into numbers
# Author: Warren Weckesser
#test the model using lm.predict and the x_test set of data
'''Here we are creating a new column named old_page_converted with a length equal to the old_pages count$ calculated above and filling them with one's and zero's with the mean probability we calculated above for $ one's and remaing for zeros'''$
# Total Number of rows
# Decoder: uses the same weights as encoder.
# saving cleaned files to CSV
# merge with main df
# Optional: Write dataframe out to CSV
# urban
#Set xgboost parameters and tree type #Use all the defaults for train here, as to not add excess detail #In actual usage, grid-search or other means of parameter tuning appropriate
# In total we are using for this model the following amount of coefficients
# set new column names
# determine the order of the AR(p) model w/ partial autocorrelation function, alpha=width of CI
#festivals.index = festivals['IndexT']
# Create a column with the result of the sentiment analysis # Display the updated dataframe with the new column:
# customer creation distributed date
# We display the first 10 elements of the dataframe:
#find outliers  #X = x['age'].astype('timedelta64[D]') ###Value counts to see any outliers for the data
# Setup Tweepy API Authentication
#convert to date format
# Make the output Dataframe
# Reference # https://radimrehurek.com/gensim/tut2.html
# Find Correlations
#df_insta.dropna()
#2018-04-03T23:53:00.726Z
# plot fixation durations
# Print the value_counts for 'State'
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
#Export Canadian Tire file to local machine
# Twitter credentials
# None and a are not dog names 
# Merge multiple rows
## Check if there is 'None' in 'tweetSource' column ## Check how many tweets are we left with now
#Storing the movie information into a pandas dataframe #Storing the user information into a pandas dataframe #Head is a function that gets the first N rows of a dataframe. N's default is 5.
# Creating Coming Next Reason CSV
# The eldery person in our dataset is 92
# Exchange DB # EXCHANGE_DB.to_csv('exchanges_list.csv')
c.execute('''SELECT * FROM artist_db''')$
#Check via customer sample
# Assign classes to a variable
#Now try the same thing, but for the end of the dataframe, called the "tail". 
# We still need to rename the columns
# Change the name of first row from "Unknown" to "rowID"
#'Sacramento': 'b71fac2ee9792cbe'
# Create a database session object
#Stack method rotates the data frame so that columns are represented in rows #To complement this, unstack pivots from rows back to columns.
#Regression - Forecasting and Predicting
#add the previously created series to the main ward dataframe
## Request page ## See the html for the whole page # https://www.purdueexponent.org/
# Concatenating the two pd.series with a pd.DataFrame
# sort dataframe (hint: use .sort_values(by="column name", inplace=True))
#!rm ../../output/prealignment/raw/{srx}/{srr}/{srr}*bam*
# Check out the distribution of days worn
# Make an empty dataframe for the transmission capacities between the countries
#Train Model and Predict  
# to read multiple files just add the names
# YOUR CODE HERE # raise NotImplementedError()
#Define a dictionary with the numpy functions needed for the groupby that will be completed in the next step.
# Example 2:
## Now, create a dataframe that only has the subset of crimes with the primary description 'CRIMINAL DAMAGE' # code here:
#Use SQLAlchemy create_engine to connect to your already created/saved data,  sqlite database. #engine = create_engine("sqlite:///hawaii.sqlite")
# Find the lat/long of a certain address
#data_spd['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_spd['tweets_spd'] ])
# Get all unique brands and models
#Saving the output to SampleRatings.csv in an Output folder
#creating new dataset using query function
# Use the Base class to reflect the database tables
# A lot of these variables are redundant, especially squared dummy variables # All of these variables listed next are 'binary' but only some are meaningful
    """$     create a new column: 'location'$     """$
# read csv into data frame
# are the observations in train in history ?
# Inspect data types 
# Merge the portfolio dataframe with the adj close dataframe; they are being joined by their indexes.
###last hour of values
# Find total number of stations
#Select NDVI for scene after cyclone
#Seccond Data Set:
# Group the data by media outlet #groupedNews.head()
# eliminate all zip code outside NYC and change to integer
#correlation of continuous variables with the dependent variable
# Calculate the date 1 year ago from today # to get the last 12 months of data, last date - 365
# Variables contained in this file:
# Show all output
#'Norfolk': 'b004be67b9fd6d8f'
#df_pd = df_pd.set_index("timestamp") #df_pd.set_index("timestamp")
# check to be sure formats match
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# show a sample of the data from the new dataframe 'gdf'
#aggdf = loctweetdf[['lat','lng','text','lga']].groupby(['lat','lng', 'lga']).agg('count').reset_index() #output
# Visualizing causes of death by year # This is hard to read because there are so many different causes of death, but left here for reference
# Requirement #1: Add your code here # updated by Huang, Silin ID: A20383068
#Top 5 Players by Points
#calculate the decision tree's mean absolute error
# That looks better # Let's inspect the index to make sure the dates are of the right type
# These are taken from arxiv paper JH.
# The header keywords are stored as a dict # table.meta
#gract.to_csv('gract.csv')
# we can make structs in numpy via compound data types
# analyze validtation between BallBerry simulation and observation data.
# fit (train) the vectorizer with the corpus from video text contents
#Initialize Sentiment Analyzer:
#Anaheim
#1 Collect data from the Franfurt Stock Exchange, for the ticker AFX_X,  # for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
#df_4_test.dtypes
#Mean and standar deviation returns
# We can create a time series of tweet length like this
# For retrieving the xml content from the sitemap
# population is a string because of the commas. humm
#Clear to see that the Monetary_score column contains data outliers
# Save an image of the chart and print it to the screen
#Number of rows in testing set 
# see an example for a given author
# TO BE CONTINUED...
# instantiate the algorithm, take the default settings # Convert indexed labels back to original labels. #pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,labelIndexer,OH1,OH2,OH3,OH4,assembler,rf])
#Return a list of all of the columns in the dataframe as a list
# Group by # Group data to get different types of information
# set the new url and go to the site
# convert crfa_r to numeric value
#Store-store_states (checking if there was any unmatched value in right_t) #Weather-state_names
#analysis_historical['Volitility_90_day'] = analysis_historical.groupby('coin_name')['close_price'].rolling(window=90).std().reset_index(drop=True) #analysis_historical['Volitility_365_day'] = analysis_historical.groupby('coin_name')['close_price'].rolling(window=365).std().reset_index(drop=True)  ## fyi, normally annual volitility is 255 instead of 365 since there are only 255 trading days in a year, but since crypto is all 365 then I leave at 365
#this will take a while
# favorite table # for row in table_rows: #     print(row.text)
# What are the most active stations? # List the stations and the counts in descending order.
#change date fields to date types in schedule tab #Convert all nan observations to 1/1/1900 so they can be easily  #identified and excluded
# Ensuring data is ordered by date
# Import dataset
    """$         create the target directory$     """$
#Create a new month column # code here
# Call the method 'generate_chart' from the Class 'report'  
# run the model giving the output the suffix "rootDistExp" # if user doesn't have executable file or executable file don't work on your local computer, use run_option ='docker_develop' #results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'docker_develop')
#Set pandas so that the dataframe column width will be expanded, so  #we can see more of what is happening in our cleaning steps
#ls_dataset displays the name, shape, and type of datasets in hdf5 file
# concatenate tc physcial and electronic dataframes into one
# Find all the plays mentioning Santos
#added layer of granularity in event customer churns within month...count the scn
# use multi-index by specifying keys
# Counting the no. commits per year # Listing the first rows
#Find Precipitation Data from the Last 12 months
# The ApproximateMedialAxis lines created point in different directions,$ # this step simplifies them.$ sql = '''CREATE TABLE merged_approx_medial as SELECT id, ST_LineMerge(linestring) AS linestring FROM approx_medial;'''$
#  toar() is for conversion to numpy array:
# add our new function to the class # test our function on the initialized instance
# set seed for reproducibility
# the number of reports from the most active station
# Assigns the close column from the dataframe to the series variable, close_series # prints the first five rows in the series
# Display of first 10 elements from dataframe:
# Load BTC data 
# Calculate the range for each operator by part combination.
#ARIMA model
# look at the count of values for the 3 categories.
# Number of unique products
#Fort Wayne': '3877d6c867447819'
# accediendo a una columna por el nombre (label) y obteniendo una serie
#B2.add_files_from_workstep(default_workspace.get_step_0_object())
# And add back to the original dataframe
#Drop removes a specified row or column from a dataframe
# creating purchase csv
# display the unique values with counts for issue_type
#ignore_index means the indices will not be reset after each append
# Instantiate an instance of the classifier we defined above. # This method, 'get_classifier', takes arguments 'self' and 'key' # It then returns 'self.config.classifiers[key]'
# This code saves the cleaned salary information back to the main data folder
#Saving the model to HDFS
# Which campaigns ended with positive results? 
# interaktif jupyter
# Remove rows for games that occur in 1999 season and in 2017 season
#split validation into 50% each of 10%
#make dictionary
# Need to change to API request to call for Franfurt Stock Exchange, FSE, AFX_X # Change response to readable JSON
# find historical data for 2007
# p.242 Processing documents into tokens # split the sentence/corpora into individual elements
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Dataframe from cutted tweets
# another way of importing the file
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Count number of stations in Measurement table
# create a data frame and reset the index
# Add this line if team made postseason:
# calculating number of commits # calculating number of authors # printing out the results
## ------ 500 tweets ------
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data
# plot histogram of the counts
# Tot number of stations are 9
# Plot all the positiv and negativ of the dummy data # This is an example of not choosing to save the image
# Most active stations
# Your code here # df['All'] = 
# 7. 
#The table I created above on the nulls in the dataset let's us know that Completed Date has 101,709 null values, and Permit Number and Current Status do not have any null values. Let's confirm that here. 
# Use **statsmodels** to import your regression model. # Instantiate the model, and fit the model using the two columns you created in part b.  # to predict whether or not an individual converts.
# Tells us how significant our z-score is # Tells us what our critical value at 95% confidence
# join the output of the previos grouping action to the input data set  # the goal is to optain an additional column which will contain all authors of that particulair publication
# Print all classes mapped to the Base
# Generate version number for built
#replaceing time under an hour with 1 hour 
# predict on test set
# Backward filling # To do in-place updating # store_items.fillna(method='backfill', axis=0, inplace=True)
#data massage newsMood_df #converts 'Timestamp' from string into datetime object # save newsMood_df to csv file in same folder
###YOUR CODE HERE###
# Plot in Bar Chart the total number of issues created every day for every Detaction Phase
#Peek in the dataframe
#preview
#Let's go grab those articles out of lists, then we roll
# Group ratings by movie ID # Select top rated movies
# convert Tip_Amt into Tip percentage
#Create predictions and evaluate
#### Define #### # Convert index into a column and reset the index # #### Code ####
#show result
# Package the request, send the request and catch the response: r
# mean deaths per month:
# Which learner had the most interactions and how many interactions did they have?
# means by month:
#NASA news site, pull latest article text and date
# called the stats_diff function that gives us the difference
# Total dates
## remove columns we don't need for model
# The above query returns a list of tuples from the measurement 'table' object.  We want to import these tuples into a pandas # dataframe which extracts the values from the tuples for input to the dataframe. # Add column labels to the df.
#CIA, ORGANISATION_CODE and L2_organization_id are the same
#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
# Use pandas to write the comma-separated output file
#rf_enc = OneHotEncoder() #rf_enc.fit(rf.apply(X_train))
# Print the columns of df
#'Columbus': '3df0e3eb1e91170b'
# Checking missing values
#index objects are immutable
#Change the index to be values in the datetime column and display them
#map the dictionary to the new col on index
#now pass that series to the datafram to subset it
# Use catplot() to combine a countplot() and a FaceGrid()
# Information extraction from dataloader and model # vcf_to_region will generate a variant-centered regions when presented a VCF record.
#Concatenate (or join) the pre and post summary objects
# Load the necessary data for the module
# set the X and y
# each 4h of 01.Jan, 2000
# Random forest score:
# questions.loc[questions['zipcode'] == 'Npahel17@gmail.com']
# 5. Which Tasker has been hired the most?	 # sample.groupby('tasker_id').size().sort_values(ascending=False) \ #   .reset_index(name='hired')
#checking distribution of projects across various main categories #kick_projects.groupby(['category','state']).size()
# The mean of lengths
#Read tweets from csv file
#population = np.random.randint(2, size=(36, 10, 29)) # random population of 10 binary genetic strings
# Apply lambda function to each column.  This example yilelds scalar value from function. # As seen below, NaN is ignored in calculation. 
# reading the dataset
# reference:
# check if any values are present
# Notice how the data did not contain headers - let's try again, this time passing  # in some keyword arguments
#tlen.plot(figsize=(16,4),label="Length", color='r');
# Car KOL
# predict on the same data
# get values, note that the result is a multidimensional array
# Converting to Timestamps
#flight.select(regexp_extract('arr_time', r'[+-][0-9]{2}:[0-9]{2}\b', 1)).alias('d').collect
#Display the row's columns and data in dictionary format
# What are the most active stations? # List the stations and the counts in descending order.
# Creating another lamba function to find spaces and split the each value
# this might run long
# Output File 
# view the fees by year|
# Loop Media Sources 
# Split PKG/A column into to and drop
# another example from Tesla with more data points
# Create a new dataframe for the Airport ID (It does so by testing true/false on date equaling 2018-05-31)
#types of data
# new_df = pd.DataFrame(columns = ['hour','minute','hourfloat','x','y'])
cur.execute("""INSERT INTO coindesk VALUES ('http://google.com', 'google', 'Ian Chen', '2018-11-11 11:11:11')""")$
# specify a column as index
# Store a Library and its MGXS objects in a pickled binary file "mgxs/mgxs.pkl"
# check json errors
# Get last tweet or mars weather
# Get the keys to understand the data structure and how data were nested.
# once we sort them we can create a data frame for the first 25 default Json files ready to be dowlaoded
#Monthly average
#Prepare our data for plotting 
# wall time to train model
# 3.
# missing value check
#Larger time period 
# Perform prediction on the Kaggle test set previously imported
# entity visualization # after you run this code, open another browser and go to http://localhost:5000 # when you are done (before you run the next cell in the notebook) stop this cell 
# went through the Quandl documentation and determined that I needed to find the Franfurt Stock Exchange dataset code # as well as add start_date and end_date parameters to make sure the API request via HTTP GET was done properly
# there are NAs
# popCon = popCon.set_index('contact')
# from tensorforce import Configuration
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Take 'full_text' when available, otherwise take 'text'
# merge 'yc_depart' with the destination zip codes
#Describe records before 1980 and after 1984 (using index slicing, as above)
# Write the file
# Period :: represents time span  it is creating series of time for given month or year # here it is 2016 months  # A-DEC :: Its Annual and ends in DECember
# plot the pie chart
# Checking class balancce in y_train
# Padding NAN's
#'Modesto': 'e41805d7248dbf1e'
# average the predictions, and then round to 0 or 1 # you can also do a weighted average, as long as the weight adds up to 1  # how accurate was the ensemble?
# We'll use gain first because that is the most useful for my purposes (saving more than $5 is necessary as it is cost of trade)
# Display a row that broke the assumptions
#Drop the redundant/useless columns
#Reformat variables
# For doing the same thing as above
# loading the processed csv containing the urls for all of the op-ed pieces
#type BI et type UT are the same
# Delete records that duplicate brand / outdoor unit / indoor unit model.
# Converting my list to a dataframe.
#hours.columns, hours.index
# Parse the resulting html with soup
#Example 5: Specify dot (.) values as missing values
# Load the spikes # Print the first 10 spikes
# Clean tweet_df, keep only useful columns
#Scraping the table and getting HTML string 
#== By label: only one row -> dimension reduction 
# City data
# read in again, now specify the data column as being the  # index of the resulting DataFrame
# cvec_1 top 10 words
# Convert all of the pushed_at dates to time # Normalizing to the earliest date.
# Print records in train.csv
#Example 2: If no header (title) in raw data file
#params
# Export to CSV
# Access local file
#Read csv file into a DataFrame #reviewing dataframe and loaded data
# change default figure and font size # visualize the feature importance of every variable
# %load ../solutions/sol_2311.py
# First, import the relevant modules # only need json_normalize from pandas
#searching for href
# valid
# in order to completely get rid of the original index # and have a fresh one, e.g. in a subsample:
#Resetting the index to avoid future issues #Dropping unnecessary issues due to save memory and to avoid issues
# Preview the Data in the station dataframe...as expected from the above...there are 9 unique stations
### Step 21: Plot the graph with day of week as coloration
# default inner join
# Find the more info button and click that
# Show best score
# AUC for our recommender system
# Locating row information for duplicate user_id
# Creates a subset dataframe from another dataframe
#selecting a subset of the rows so it fits in slide.
# convert rate for Pnew under the null
#preserve # Esta celda da el estilo al notebook
# Creating a list of items 'tobs_data' from our initial mulitdimensional array 'temperature_data' # tobs_data # Again, commented the tobs_data print as it was just needed to understand the layout
# df_2003
# add active_user column, and marketing_source column
# Applying the lamba function on each row
# pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
# Setting up plotting in Jupyter notebooks # plot the data
# Use Pandas to calcualte the summary statistics for the precipitation data
#print data
# Caption is having issues in TF-IDF  # it is bc of the numbers...
#### Define #### # Remove all tweet record that have a non-null retweeted_status # #### Code ####
#Convert all characters to lower case
# Write the lambda function using replace # Write the lambda function using regular expressions # Print the head of tips
#extract the time
#print(data[:5])
# simple pandas plot
# Get column and types for Measurement Table # columns
# fig.savefig('toma.png', dpi=300)
qry = '''\$ DROP TABLE IF EXISTS lucace.disc_667_stage_10_daily_active_users$ '''$
# make a GET request
# Create a 1-group structure # Create a new MGXS Library on the coarse 1-group structure
# read excel file # only reads first sheet
# We are embedding the video we searched into the following webpage using HTML
#Import the NetworkBuilder module from modelingsdk. This module allows us to create a network object #to which we can add nodes and edges respectively.  #Create network object
# Slice using just the "outer" index. #
# Rounding the numbers to two decimals
# Now, let's use kNN:
#checking the range of what we are about to input into the database.
# 300 X 300
# Gets rid of 'u/' so that it is easier to use the api
# results are returned as an iterable list
# Save final version here. #json.dump(fp, youtube_urls, separators=(',', ':'))
# Logistic Regression on age and subreddit 
# preprocessing and cleaning  # remove NaN and NA from dataframe
# Dummying attend_with
#create an index to compare against each iteration of tau #deterministic variable lambda
# entfernt Spalte "created_at"
# Volume    5923 non-null float64,  not 8885
#clean df...no nans
# your code here
# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)
# Least favorited 
# We remove the store 2 and store 1 rows # we display the modified DataFrame
# Or: sales_df['Country'].value_counts()
# %load shared_elements/system_info.py
#Compute counts and plot them as a bar chart
# Display a list of created buckets.
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# leadsdf = leadsdf[['_id','firstName','lastName','phone1','ingestionStatus','RVOwner','address','city','email','entryPerson', #                                    'submissionFile','filename', 'participant','program']] # leadsdf
# Use Pandas to calcualte the summary statistics for the precipitation data
# get index
######################### Combine Data Frames ##############################
# Run in python console
# Use Pandas Plotting with Matplotlib to plot the data # Rotate the xticks for the dates #df.plot(kind="bar", x="date", y="prcp", alpha=1.0)
# Reflect Database into ORM classes
# Create connection to the read-only DB
# For this, I will use the preprocessed df again.
# For finding the first paragraph tag
HTML("""<iframe width="560" height="315" src="https://www.youtube.com/embed/pPN8d0E3900" frameborder="0" allowfullscreen></iframe>""")
# Check for any missing data
# Make vwg folder
# Use the Base class to reflect the database tables
# Remove outliers if needed, replot and see if the moments changed significantly. # Checking how many values I have above 100% tipPC
## Find the stock with the maximum dividend yield
# Below is the list of default classifiers which were included at the beginning of the code # clf_base_default = [clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc] # Pass the clf_base_default value to the 'clf' parameter
#model['fell']
#using sqlalchemy engine #print(seng) #engine = create_engine('mysql+pymysql://root:'+spassword+''@localhost/animals_db')
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
# read in data only in the Date and close columns, # use Date as the inde
# What are the most active stations? # List the stations and the counts in descending order.
# Use the session to query measurments table and display the first 5 locations
#### write our earlier dataframe to csv file
# Normalized the data
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
# add results to our data frame
#reduce dim (240000,1) -> (240000,) to match y_train's dim
# Plug sentiment in to df_tick
# Data frame is ready
#Head shows top 5 rows of ingested data
# What is the number of backers that most success projects have?
# convert the boolean outproc_flag to numeric value Y/N == 1/0
# Backward filling
# Save to an Excel file with a single worksheet.
# data count
# details from https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words # lower case all text
# df_session_dummies.head(n=2) # df_session_dummies_drop=df_session_dummies.drop(['created_at','value',],1) # df_session_dummies.head()
#(player['events'] == 'triple') | #(player['events'] == 'double'), |
# check the simulation start and finish times
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# write scenario from xlsx to data base and run the sceanrio in GAMS
# check for unbalanced classes - the others should be balanced 
# Reduce words to their stems
# how many rows, columns do we have now?
# This is a comment. Anything after the # symbol will not be interpreted as code.
# Use statsmodels to verify results
# Create our data frame with features. This can take a while to run.
# dummy all the subreddits
# path for writing to csv the merged dataset: /home/sb0709/github_repos/bootcamp_ksu/
# Merge free_sub and ed_level together. Which column should the merge be performed on?  # Do this using both the concat() and merge() functions.
#file path
#Print number of deaths per year sorted by deaths
# take a look at the first two lines
#creating the model 
#print key["API_KEY"], key["API_SECRET"], key["ACCESS_TOKEN"], key["ACCESS_TOKEN_SECRET"]
# use pandas to view the data # We create a pandas dataframe as follow: # We display the first 10 elements of the dataframe
#Load the query results into a Pandas DataFrame and set the index to the date column.
# JH uses 14. 8.26: first 4, then 3, then 2. saved model, reloaded 8.27 and set up for 5 epochs. pred on this, 8.27.csv # DONE. fine for now.
# you can define period like starting year is 2011 and 10 quaters after it
#  each month end of 2017
# analyze validtation between BallBerry simulation and observation data.
#delete * gender type
# Show results
#to_datetime
#all_lum.loc[:,'plot_grouping'] = all_lum.block.map(str) + all_lum.lum.map(str)
# prediction on test set
# Summary statistics for the ratings data set
# Create a pandas dataframe: # Display the first 10 elements of the dataframe:
# proportion of users converted
#Checking the values of the df
#removes the 'b' infront of the tweets that indicate byte format
# Create a pandas data frame # We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# group by Symbol
# Create the 'country' column
# Only co-occurence > 1
# create new DataFrame to select a time range # remove NaN values that result in the new DataFrame
# record observation count for each sensor
# Group data by death year and cause of death
# This shows that we guessed 0 for each one of our rows.
# Third option - the cross-section - returns specific values
#Qn 2 #Convert the returned JSON object into a Python dictionary.
#DF
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Our "Date" looks like strings.
# Remove characters and redundant whitespaces
#high res image
##### return numpy ndarray
#eth['close'] = eth['close'].astype('float64') 
#, 'test'
# dr_new.index
#Alternate method for reading sql with pandas
# lets fit logistic model for the countries with ab_page, baseline = CA
# create CSV file of signals for each channel
# Let's get back to the data frames we had before
## creat a new DF with 2 columns - sum of score and sum of comments with post id as index
# Select columns of interest
# Check out the structure of the resulting DataFrame
# find historical data for 2001
# This is to force jupyter to display all rows when asked.
# Make sure no variable is the index
#under_sample_data['Diff'] = under_sample_data['Diff'].fillna(0)
# To produce pie_chart by city type #we group the original csv read file by "type"
# read in data from HDF5
#d = feedparser.parse('http://g1.globo.com/dynamo/rss2.xml')
# Implement simple regression: Result ~ Input # Display model fit results
#helper function
# 18. Using the Data Frame created in part (17),  # print the frequency of each column being the max score.
# Display coefficients for different features
# How many stations are available in this dataset?
# Delete the datastore # to delete all datastores in the path, use: # pystore.delete_stores()
# The Matcher identifies text from rules we specify
# Drop those two rows with those indices and you are saying inplace=True, to make sure you are not creating a copy. 
#Convert query to a DF
#setting unique ID as index of the table #this is because the ID column will not be used in the algorithm. yet it is needed to identify the project
#filecount
#inspect measurement table
# data munging # screen
# missing value check
# convert your "Date" objects into datetime objects.
# We can also use the "backfill" method to fill in values to the back
# get 5000 random ids
# 'supercontig' types columns represent unassembled sequences # (incomplete/total) * 100
# Predicting val_dl to verify that it works
# plot of the length of tweets vs period of time
#Saving features in csv
#most active stations over the entire data set. Defaults to descending order of count
#'Phoenix': '5c62ffb0f0f3479d'
# files8['Tenure']=files8['Tenure']/86400
#locate missing value record
#now the information is empty because i ran the command after dropping the duolicated row
# Filter outliers in Min Power
# freq='M' uses last day of month by default
# Setting up plotting in Jupyter notebooks # plot the data # ... YOUR CODE FOR TASK 8 ...
# Repeated part from morning track: fill with media or mean example so can match the orginal cleaned data. 
# overview of the class label column
# The youngest person in our dataset is 92
# view the resulting isochrone shape (can you guess why there are separated geographies?)
# We can rename the column to avoid the clash
# use SVM to classify
# Remove rows where 'Delivery block' is empty.
# Dealing with NaN
# View the first five rows of data.
#msft['2012-01-03'] while this will not work for dataframe, this syntax can work for series
# to get vector graphic plots # to get high resolution plots
# Timestamp conversion 
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
#bow_test=pd.concat([bow_test,data_test_model],axis=1) #bow_test.sort_index(axis = 1, ascending = False) #bow.sort_index(axis = 1, ascending = True)
# Calculate median number of comments 
# It is the same as:
#the below display is the list of company names and tickers that do not begin with same character
# Sumar una unidad a una columna
#setting up date
# Make columns with country sums
# first build an empty model, no training
# Inspect the data to determine values for an implicit enum
q = '''$     DROP VIEW customer_2;$ '''$
#now create another label for forcast data that we need
# Gets historical data from the subscribed assets, between two dates with daily resolution
#creating target
# word2vec expects a list of lists. # Using punkt tokenizer for better splitting of a paragraph into sentences. #nltk.download('popular')
# We create time series for data:
# Create subset dataframes by city
# print out details of each variable
# %load ../solutions/sol_2312.py
# Ensure to Convert df_TempIrregular.detectionTimeStamp to datetime
# filter out Twitter links
# Add stop name from oz_stops as column in stops.
# Let's get the IDs of the artists whose name contains "unknown"
# Extract title text
#Convert titles to a list for further scrubbing
# worst 3 styles, average rating, minimum of x beers drank
# Authentication
# these are the columns we'll base out calculations on.
# shape of coarse grid
# Grab the data
"""$ Load tmp sr pickle for news title docs$ """$
# return the Missoula column using a property
#Let's pick one date and show the first rows #These are cleaned up data and therefore need no further cleaning (I had to do some cleaning of strings, fill NaN, #and eliminate unnecessary columns)
# 181 tweet id in tweet_archive_clean do not exist in tweet_image table
#for prophet to work, columns should be in the format ds and y
# Let's plot this instead
