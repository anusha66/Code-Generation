cityID = '2409d5aabed47f79'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Rochester.append(tweet) 
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall()
df_user_info = df_user_info[df_user_info['activated']==1]$ df_user_info['marketing_source'] = \$     df_user_info['marketing_source'].loc[:].fillna("unknown")
import sqlalchemy as sa$ engine = sa.create_engine('redshift+psycopg2://admissionsapi:admissionsapi@admissions-api.cv90snkxh2gd.us-west-2.rds.amazonaws.com:5432/admissionsapi')$ pd.read_sql("select * from applicants limit 10",engine)
tweet_df["tweet_date"] = pd.to_datetime(tweet_df["tweet_date"])
display(data.head(4))
people.shape
ET_Combine = pd.concat([simResis_hour, BallBerry_hour, Jarvis_hour], axis=1)$ ET_Combine.columns = ['Simple resistance', 'Ball-Berry', 'Jarvis']
cityID = 'fa3435044b52ecc7'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Newark.append(tweet) 
df.median()
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ print(last_date)$
idx_close = r.json()['dataset']['column_names'].index('Close')
def tokenizer(text):$     return text.split()$ tokenizer('runners like running and thus they run')
df7.loc[df7['avg_health_index Created'] == 'low', 'avg_health_index Created'] = np.nan$ df7['avg_health_index Created'].value_counts(dropna=False)
mydata.iloc[0] 
indexed = base_df$ supreme_court_df = base_df.loc[indexed['parent_id'] == 't3_3b6zln'].reset_index()
woba_to_date = plate_appearances.sort_values(['batter','game_date','at_bat_number'], ascending=True).groupby('batter')['woba_value'].expanding(min_periods=1).mean()
top_10_authors = git_log['author'].value_counts()[:10]$ top_10_authors
data.TMED.head()
corrmat = blockchain_df.corr()$ f, ax = plt.subplots(figsize=(11, 9))$ sns.heatmap(corrmat)
tree = RandomForestClassifier(max_depth = 4)$ tree.fit(X_train, y_train)
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(RN_PA_duration, params=params, freq=31, title='RN/PA Decomposition')
siteInfo = siteInfo.query('Range >= 10 & EndYear >= 2017')$ siteInfo
if weights_flag:$     regridder.clean_weight_file()  # clean-up
train.drop('second',1,inplace=True)$ train.drop('minute',1,inplace=True)$ train.drop('action',1,inplace=True)$
pd.Timestamp("now")
table_rows = driver.find_elements_by_tag_name("tbody")[30].find_elements_by_tag_name("tr")$
a.iloc[3]
a = df.resample('1H').mean()$ a.plot()$ plt.show()
newcolumns = df1.columns.str.strip()$ df1.columns = newcolumns
test_id = pax_raw.seqn.values[1]$ pax_sample = pax_raw[pax_raw.seqn==test_id].copy()
df = df.T$ df
for_exiftool[['SourceFile','GPSLatitude','GPSLongitude','GPSAltitude']].to_csv(basedirectory+projectname+'/'+projectname+'_exiftool.csv',index=False)
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
list(festivals.columns.values)
purchase_history = pd.merge(orders_subset, $                             order_details_prior, $                             on=['order_id'])[['user_id', 'product_id']]
c_df = c_df.dropna(axis=1,how='all')$ c_df.size
df = pd.read_sql('SELECT * from customer', con=conn_b)$ df
Output_two = New_query.ss_get_results(sport='football',league='nfl', ep='team_game_logs', season_id='nfl-2017-2018',team_id='nyg')$ Output_two
f_user = os.path.join(data_dir, 'following_users.csv')$ f_term = os.path.join(data_dir, 'tracking_terms.csv')$ f_meta = os.path.join(data_dir, 'collection_meta.csv')
plt.show()
print("_____________________________________________")$ media_user_overall_df = pd.DataFrame.from_dict(media_user_overal_results)$ media_user_overall_df.head() 
df.in_response_to_tweet_id.isnull().sum()
pd.options.display.max_colwidth = 300$ data_df[['ticket_id','type','desc']].head(10)
a = np.array([1, 2, 3])$ a
grouped_authors_by_publication.tail()
tz_pytz = pytz.timezone('Europe/London')
df_subset.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70)$ plt.show()
result = api.search(q='%23Australia')  # "%23" == "#"$ len(result)
df.info()
fetch_measurements('http://archive.luftdaten.info/2016-05-09/')
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])$ df_new.head()
tweetsPorFecha=tweet_frame[['tweetCreated','userID']]$ tweetsPorFecha.loc[:,('tweetCreated')]=tweetsPorFecha['tweetCreated'].map(lambda x: x.strftime('%Y/%m/%d') if x else '')$ tweetsPorFecha
treaties = db.treaties$ treaty_id = treaties.insert_one(treaty).inserted_id$ treaty_id
df_ctr = df2[df2['group'] == 'control']['converted'].mean()$ print("{} is the probability they converted.Thus, given that an individual was in the control group.".format(df_ctr))
print('Total records {}'.format(len(non_na_df)))$ print('Start / End : {}, {}'.format(non_na_df.index.min(), non_na_df.index.max()))
lda_model = LatentDirichletAllocation(n_components=15, random_state=42, learning_method = 'batch')$ train_lda_features = lda_model.fit_transform(tfidf_vectorized)$ np.mean(np.max(train_lda_features, axis=1)) #15
%%time$ body_pp = processor(keep_n=8000, padding_maxlen=70)$ train_body_vecs = body_pp.fit_transform(train_body_raw)
sub_gene_df.sample(10)
df_sp_500 = df_sp_500.withColumn('IDX', func.lit('SP_500'))
df = pd.read_csv('../data/hash_rate_raw.csv', names=['Date', 'Hashrate'])
engine = create_engine("sqlite:///hawaii.sqlite")
r = pd.DataFrame(q, columns = ['cat','score'])$ r.head()
df['2015-06-03']['battle_deaths'].mean()
df_tick_sent = df_tick.join(df_amznnews_2tick)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY)
ctd_df.dtypes
xyz = json.dumps(youtube_urls, separators=(',', ':'))$ with open('youtube_urls.json', 'w') as fp:$     fp.write(xyz)$
big_change = max([day[2] - day[3] for day in afx_dict['dataset_data']['data']])$ print("The largest change in stock price was " + str(round(big_change,2)) + ".")
data.columns
views_data_clean.to_csv('data/views_data_clean.csv',index=False)
mbti_text_collection_filler.to_csv('Reddit_mbti_data_filler.csv',encoding='utf-8')
from sklearn.model_selection import train_test_split$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
experiment_run_details = client.experiments.get_run_details(experiment_run_uid)
df_3_test = pd.read_csv('Medicare_Hospital_Spending_by_Claim.csv')$ df_4_test = df_3_test.drop(df_3_test.columns[[11, 12]], 1)$
findNum = re.compile(r'\d+')$ for i in range(0, len(postsDF)):$ 	print(findNum.findall(postsDF.iloc[i,0]))
hexbin = sns.jointplot(x="item", y="sentiment", data=dta, kind="scatter")$
print('\n Row x Columns of imported data:')$ print(dfa.shape)$ print(type(dfa))
df_tte_all = pd.concat([df_tte_ri,df_tte_ondemand])
Osha_AccidentCases['Title_Summary_Case'] = Osha_AccidentCases['Title_Summary_Case']$ Osha_AccidentCases.head()
df_cs['Sentiment_class'] = df_cs.apply(conditions, axis=1)$ df_cs.to_csv("costco_senti_score.csv", encoding='utf-8', index=False)
info_final.head(1)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=ibBvKxUvyJJC1ziPSUPJ'$ r = requests.get(url)
!wget http://files.fast.ai/data/dogscats.zip && unzip dogscats.zip -d data/
mystem = Mystem()$ df['lemmas'] = df['description'].apply(lambda x: mystem.lemmatize(x))
artistDF[locate("Aerosmith", "name") > 0].show(20)
ymc=yearmonthcsv.coalesce(1)$ path=input("Enter the path where you wanna save your csv")$ ymc.write.format('com.databricks.spark.csv').save(path,header = 'true')
df_videos.head()
miner = TweetMiner(twitter_keys, api, result_limit=20)$ sanders = miner.mine_user_tweets(user='bernisanders', max_pages=10)
table = pd.read_html(url_mars_facts)$ table[0]
pd.concat([msftA01.head(3),msftA02.head(3)])
df_users =  pd.read_sql(SQL, db)$ print(df_users.head())$ print(df_users.tail())
us_grid_id = (pd.read_csv('../data/model_data/us_grid_id.csv', index_col = 0)$               .groupby('grid_id').count().reset_index())$
crime = crime[crime.Sex != '*']
label_and_pred = rfModel.transform(testData).select('label', 'prediction')$ label_and_pred.rdd.zipWithIndex().countByKey()$
red_4['created_utc'] = red_4['created_utc'].astype('datetime64[s]')$ red_4['time fetched'] = red_4['time fetched'].astype('datetime64[s]')$ red_4.head()
import requests$ import quandl$ quandl.ApiConfig.api_key = 'AnxQsp4CdfgzKqwfNbg8'
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
CSV_FILE_PATH = os.path.join('pims_cloudpbx_subset_201806051550_1million.csv')$ GEOLITE_ASN_PATH = os.path.join('GeoLite2-ASN.mmdb')$ GEOLITE_CITY_PATH = os.path.join('GeoLite2-City.mmdb')
df['Address1'].fillna("895 SABINE ST", inplace = True)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt"$ mydata = pd.read_table(path, sep= '\t')$ mydata.head(5)
words_only_sk_freq = FreqDist(words_only_sk)$ print('The 100 most frequent terms (terms only): ', words_only_sk_freq.most_common(30))
data = read_root('./preprocessed_files.root')$ data.head()
from h2o.estimators.glm import H2OGeneralizedLinearEstimator$ glm_model = H2OGeneralizedLinearEstimator(model_id = "GLM", family = "binomial")$ glm_model.train(x = predictor_columns, y = target, training_frame = train, validation_frame = valid)
selected_features=selected.index$ X_train_new=X_train[selected_features]$ X_test_new=X_test[selected_features]
html = requests.get(marsweath_url)$ soup = bs(html.text, 'html.parser')
with open(os.path.join(os.getcwd(),"data/credentials.json")) as data_file:    $     key = json.load(data_file)$
sales['date'] = pd.to_datetime(sales['date'],format="%d.%m.%Y")
df.info()
studies = pd.read_csv('C:/Users/akapoor/Music/01 Docs/HealthCare App/ctdb/studies.txt', sep="|")
sentiments_pd.to_excel("NewsMood.xlsx", encoding="UTF-8")$
df.select('station','year','measurement').show(5)
outfile = os.path.join("Resource_CSVs","Main_data_negative.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
fire_size = pd.read_csv('../data/model_data/1979-2016_fire_size.csv', index_col=0)$ fire_size.dropna(inplace=True)$
jpl_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars"$ browser.visit(jpl_url)
rng = pd.date_range('3/6/2012 00:00:00', periods=10,freq="D",tz="US/Mountain")$ rng.tz, rng[0].tz
very_pop_df = au.filter_for_support(popular_trg_df, min_times=7)$ au.plot_user_dominance(very_pop_df)
movies.sample(5)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2016-06-07&end_date=2016-06-09&api_key=yUSy9ms6sx2Ubk55dXdN")$
rhum_us = rhum_nc.variables['rhum'][1, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(rhum_us)
pd.to_datetime(['04-01-2012 10:00'])
tlen.plot(figsize = (16,4), color = 'r')
r2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=%s' % API_KEY)
df.index
    l = s.split(' ')$     return date(to_int(year), month_to_int[l[0]], to_int(l[1]))
df = df.drop_duplicates(subset='id', keep='last')$ df.drop(columns='Unnamed: 0', axis=1, inplace=True)$ print(df.shape)
mgxs_lib.load_from_statepoint(sp)
y_class = demo.get_class(y_pred)$ cm(y_test,y_class,['0','1'])
my_stream.disconnect()
data_2018 = data_2018.rename(columns={'Unnamed: 0':'time'})
df['Opp_Team']=df.Match_up.str[-5:-2]$ df['home']=df['Match_up'].apply(lambda x: x[2]=='v')$ df['win']=df['W_or_L'].apply(lambda x: x=='W')
hawaii_df = measure_df.merge(station_df, left_on='station', right_on='station', how='outer')$ hawaii_df.head()
df.iloc[(len(df)-lookforward_window)-2:(len(df)-lookforward_window),:]
reddit_info.to_csv('reddit_data_2.csv', encoding = 'utf-8', index = False)
res = sts.query(qry)
tweet_df_polarity = my_tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
df_reg=injuries_hour[['date_time','Rain','injuries','wet','low_vis']]$ df_reg['hour']=pd.to_datetime(df_reg.date_time).dt.hour$ df_reg.head()$
S_lumpedTopmodel.decision_obj.thCondSoil.options, S_lumpedTopmodel.decision_obj.thCondSoil.value
for post in posts.find():$     print(post)
iv = options_frame[options_frame['Strike'] == 130.0]$ iv_call = iv[iv['OptionType'] == 'call']$ iv_call[['Expiration', 'ImpliedVolatilityMid']].set_index('Expiration').plot()
data = aapl.get_call_data(expiry=aapl.expiry_dates[4])$ data.iloc[0:5:, 0:5]
pd.date_range('8/14/2017 14:41:31', periods=5)
for stat in session.query(Measurement.station).distinct():$      print(stat)
df['exp_name'].value_counts()
yc_new1 = yc_new1.merge(zipincome, left_on='zip_dest', right_on='ZIPCODE', how='inner')$ yc_new1.head()
model = pipeline.fit(train)
data_NL = data_NL.sort_values($     'capacity', ascending=False).groupby('name', as_index=False).first()$ data_NL = data_NL.reindex(columns=columns_sorted)
from sklearn import metrics$ print("Accuracy: %.3f" % # TODO$          )
    def __init__(self, point, x):$         super().__init__(point, x, x)
engine=create_engine("sqlite:///hawaii.sqlite")$ Base = automap_base()$ Base.prepare(engine, reflect=True)
punkt = nltk.data.load('tokenizers/punkt/english.pickle')
chk = joined.loc[joined['nat_event']==1].head()$ holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
ibm_hr = spark.read.csv("../data/WA_Fn-UseC_-HR-Employee-Attrition.csv", header=True, mode="DROPMALFORMED")$ ibm_hr.show(3)
merged2['AppointmentDuration'] = merged2['AppointmentDuration'] / 60.0
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
news_organizations_df['tweets'] = news_organizations_df['tweets'].map(df_to_list)
import os$ print(os.listdir())
jcomplete_profile = json.loads(profile)[0]$ jcustomer_info = jcomplete_profile['customer']$ dfbasic = json_normalize(jcustomer_info)
theft.iloc[0:10]
results = pd.read_csv('../data/result.csv',$                       low_memory=False      #This is required as it's a large file...$                      )
len(df[~(df.data == {})])
options_frame = pandas.read_pickle('options_frame.pickle')
! rm -rf recs1$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models1 --outdir recs1
print(model.aic,model.bic,model.hqic)
sp500[(sp500.Price < 10) & (sp500.Price > 0)][['Price']]
hashtags = df.text.str.extractall(r"#(\w+)")$ mask = hashtags.loc[:, 0].value_counts() > 4$ hashtags.loc[:,0].value_counts()[mask]
affair_age = pd.crosstab(data.age, data.affair.astype(bool))$ affair_age
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$
distance_list = []$ for i in range(0, len_start_coord_list):$     distance_list.append(get_distance())
fix_space = lambda x: pd.Series([i for i in reversed(x.split(' '))])
model.wv.most_similar("crot")
df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9])
df['Approximate Age at Designation'] = df['membership_date'].apply(lambda x: x.year)-df['yob']
!dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb
for v in d.variables:$     print(v)
cityID = '512a8a4a4c4b4be0'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Charlotte.append(tweet) 
plt.savefig(str(output_folder)+'NB01_2_landscape_image01_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice_str)
gene_df = sub_gene_df[sub_gene_df['type'] == 'gene']$ gene_df = gene_df.copy()$ gene_df.sample(10).attributes.values
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['NewWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']$ df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['LastBitWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(-1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']$
s4.value_counts()
output.to_csv( "randomforest.csv", index=False, quoting=3 )
col = pymongo.MongoClient()['Tweets']['Streaming']$ col.count()
print activity_df.loc['2018-05-08']
youtube_df= pd.read_csv("../Data/youtubeVid_main.csv",sep = ",")$ youtube_df["trending_date"] = pd.to_datetime(youtube_df["trending_date"] \$                                            , format = "%Y/%m/%d")
year_info = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date.between('2016-08-23', '2017-08-23')).all()$ year_info_df = pd.DataFrame(year_info)$ year_info_df.set_index("date").head()$
tips.sort_values(["sex", "day"]).set_index(["sex", "day"]).head(12)$
brewery_bw.tail(8)
print(pd.to_numeric(countdf['number_ratings']).sum())$ print(pd.to_numeric(countdf['number_ratings']).sum()-pd.to_numeric(count1df['number_ratings']).sum())$ print(pd.to_numeric(countdf['number_ratings']).sum()-pd.to_numeric(count6df['number_ratings']).sum())
cityID = '01fbe706f872cb32'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Washington.append(tweet) 
data2 = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$
odds = get_cleaned_odds(odds)$ odds.tail()
clusters_compare=pd.concat([final_df_copy,pd.DataFrame(data=kmeans3,columns=['kmeans3']),pd.DataFrame(data=kmeans5,columns=['kmeans5']),pd.DataFrame(data=kmeans6,columns=['kmeans6'])],axis=1)$ clusters_compare.head()$ clusters_compare.to_csv(encoding='utf-8',path_or_buf='clusters.csv')
test[['clean_text','user_id','predict']][test['user_id']==1497996114][test['predict']==9].shape[0]
paired_df[['dataset_1', 'dataset_2']] = paired_df.paired_datasets.apply(pd.Series)$ paired_df = paired_df.loc[:, ['dataset_1', 'dataset_2','co_occurence']]$ paired_df = paired_df.loc[paired_df.co_occurence > 1]
guido_title = soup.title$ print(guido_title)
df.loc['20180101':'20180103', ['A','B']]
df_group_by = df_group_by.reset_index()$
genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId'])$ genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ genreTable.head()
news_dict_df.to_json("Newschannel_tweets_df.json")
classification_df = encoded_df.query('(id == 5 or id == 14) and best != 0')$ classification_df.best.describe()
S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
tips["sex"].index
largest_collection_size = df_meta['collection_size_bytes'].max()$ largest_collection_size
kwargs = {'num_workers': 4, 'pin_memory': True} if args.cuda else {}$ kwargs
print('Highest opening price - {} \nLowest opening price - {}'.format(max(opening_price), min(opening_price)))
twitter_api = tweepy.API(authorization_instance)$ twitter_api
Y = 'label'$ dogscats_h2o[Y] = dogscats_h2o[Y].asfactor()$
from scipy.stats import norm$ print("The significance z-score (p-value) is: %.4f" %norm.cdf(z_score))$ print("The critical value at 95%% confidence is: %.4f" %(norm.ppf(1-(0.05/2))))
conn.execute(q)
reddit = praw.Reddit(client_id='yRkwuIWiThfXeQ',$                      client_secret='Kond5JUokaOkKKXu6LipwN_CtcM',$                      user_agent='weekdayjason')
two_day_sample.reset_index(inplace=True)
res = sm.tsa.seasonal_decompose(events_per_day['count_event_day'].values,freq=7,model='multiplicative')
all_zeros = 1 - val_y.mean()$ all_zeros
print(nc_file)$ nc_file.close(); print('Dataset is closed!')
rain_df = pd.DataFrame(rain)$ rain_df.head()
sgm_bgp_100yr_run(L0 = 1000, E0 = 1, Delta_n = 0.02)
bottom_views = doctype_by_day.loc[:,doctype_by_day.max() < 10]$ ax = bottom_views.plot()$ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
data.iloc[1]
dfD.to_sql('distances', conn)
for df in (joined, joined_test):$   df.loc[df['CompetitionDaysOpen']<0,'CompetitionDaysOpen']=0$   df.loc[df['CompetitionOpenSinceYear']<1900,'CompetitionDaysOpen']=0
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);$
tips.head(3) # default 5$
QLESQ = QLESQ.dropna(axis=1, how='all')$ QLESQ.columns
y_train = np_utils.to_categorical(y_train, 90)$ y_test = np_utils.to_categorical(y_test, 90)
df.index = pd.PeriodIndex(df.index,freq='Q-JAN')$ df.index
from zipline.pipeline import Pipeline$ def make_pipeline():$     return Pipeline()
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]
nasa_url = 'https://mars.nasa.gov/news/'
cl_ca.columns = ['XML_' + x if x!='APPLICATION_ID' else 'APP_APPLICATION_ID' for x in cl_ca.columns.values]
S_lumpedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_lumpedTopmodel.txt')
stock['daily_gain'] = stock.close - stock.open$ stock['daily_change'] = stock.daily_gain / stock.open
import pickle$ with open('180225_10slsqpGLD.pkl', 'rb') as f:  # Python 3: open(..., 'rb')$     rez_2 = pickle.load(f)
base_df = DataObserver.major_df$ base_df_body = DataObserver.major_df['body']
inspector = inspect(engine)$ inspector.get_table_names()$
random_crashes_upsample_df = pd.DataFrame(random.choices(random_crashes_df.values, k=sample_size), columns=list(random_crashes_df.columns))$ random_crashes_upsample_df
stock['target'] = stock.daily_gain.shift(-1)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])$ print(old_page_converted)
df.median() - 1.57 * (df.quantile(.75) - df.quantile(.25))/np.sqrt(df.count())
ttt = list(spp.term.unique())
containers[0].find("li", {"class":"paid-amount"}).span.contents[-1].split()[2]
results_para = soup.find_all("div", { "class" : "rollover_description_inner" })$ news_p = results_para[0].text$ news_p$
rain_df.set_index('date').head()
get_nps(combined_df, 'device').sort(columns='score', ascending=False)
np.random.randint(1,100, 10)
questions = pd.concat([questions.drop('vip_reason', axis=1), vip_reason], axis=1)
cursor = collection_reference.find()
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep$ lee_train_file = test_data_dir + 'lee_background.cor'$ print lee_train_file
for c in ccc[:2]:$     for i in spp[spp.columns[spp.columns.str.contains(c)==True]].columns:$         spp[i] /= spp[i].max()
users_chanel = pd.merge(Users, Relations, how = 'outer', on=['id_partner', 'name'])$ users_chanel.head()
rf_v2.hit_ratio_table(valid=True)
twitter_coll_reference.count()
scores[scores.RottenTomatoes == scores.RottenTomatoes.min()]
y=dataframe1['RSI_30']$ plt.plot(y)$ plt.show()
client.query("show databases")
data_other = tmpdf.index[tmpdf[tmpdf.isin(DATA_SUM2_KEYS)].notnull().any(axis=1)].tolist()$ data_other
df_vow.plot()$ df_vow[['Open','Close','High','Low']].plot()
name_df.sum(axis=0).sort_values(ascending=False).head(20)
avg_test_pred2 = np.stack(test_pred2).mean(axis=0)$ print ("type(avg_test_pred2):", type(avg_test_pred2), avg_test_pred2.shape)$ print(avg_test_pred2[0:10, :])
msftAC = msft['Adj. Close']$ msftAC['2012-01-03']
year_2017 = [info for info in r.json()['dataset']['data'] if info[0][:4]=='2017']$ print(year_2017)$
grid_id = np.arange(1, 1535,1)$ grid_id_array = np.reshape(grid_id, (26,59))
news_title = soup.find_all("div", class_="content_title")[0].text$ print(news_title)
%matplotlib inline$ commits_per_year.plot(kind="line", title="Commits Per Year", legend=False)
max_ch_ol1 = max(abs(v.close-next(islice(o_data.values(), i+1, i+2)).close) for i, v in enumerate(o_data.values()) if i < len(o_data)-1)$ print('A one liner using islice: {:.2f}'.format(max_ch_ol1))
ser = pd.DataFrame({'By': dates, 'key':[0] * len(dates)})$ ser
df.drop(["join_mode"], axis = 1, inplace = True)
from statsmodels.stats.diagnostic import acorr_ljungbox$ print('差分序列的白噪聲檢查结果為：', acorr_ljungbox(resid_6201.values, lags=1)) 
if 0 == go_no_go:$     all_top_vecs = [lda.get_document_topics(serial_corp[n], minimum_probability=0) \$                     for n in range(len(serial_corp))]
sum_ratio = absorption_to_total + scattering_to_total$ sum_ratio.get_pandas_dataframe()
output = pd.DataFrame(data={"id":test.id, "rating":predictions})$ output.to_csv( "new_naive.csv", index=False, quoting=3 )$
!dpkg -i cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64-deb
from gensim.corpora import Dictionary, MmCorpus$ from gensim.models.ldamulticore import LdaMulticore$ import cPickle as pickle
df_gnis_test = df_gnis.dropna(axis=0, subset=['PRIM_LAT_DEC','PRIM_LONG_DEC'],thresh=1)$ df_gnis_test.shape
body = soup.body$ for paragraph in body.find_all('p'):$     print (paragraph.text)
cpdi = pd.DataFrame(coreval); cpdi.rename(columns={0:'i', 1:'core'}, inplace=True)$ xpdi = pd.DataFrame(xtraval); xpdi.rename(columns={0:'i', 1:'xtra'}, inplace=True)
master_df_total=pd.concat([master_df,master_dummies],axis=1)
for i,x in top_likes.iteritems():$     print ('https://www.facebook.com/'+x )
xres3['hits']['hits']$ pd.DataFrame(xres3['hits']['hits']). ._source$
trump = api.user_timeline(id='realDonaldTrump') # last 20 tweets
neuron_no = 10$ source_indices_L23exc_L23fs = np.where(np.array(conn_L23exc_L23fs.i)==neuron_no)$ target_indices_L23exc_L23fs = np.array(conn_L23exc_L23fs.j)[source_indices_L23exc_L23fs]
knn = KNeighborsClassifier(n_neighbors=5)$ knn.fit(X_train_total, y_train)$ knn.score(X_test_total_checked, y_test)
group_by_kmeans = Cluster_df.groupby('KMeansLabels').mean()$ group_by_kmeans.head()
with open('./data/processed/X_train_age_imputed.pkl', 'wb') as picklefile:$     pickle.dump(X_train,picklefile)$
tweets_df = tweets_df.sort_values("Date", ascending=False)$ tweets_df
dpth = os.getcwd()$ dbname_sqlite = "ODM2_Example2.sqlite"$ sqlite_pth = os.path.join(dpth, os.path.pardir, "data/expectedoutput", dbname_sqlite)
cdata.loc[13].Number_TD
df['year'] = pd.DatetimeIndex(df['datetime']).year$ df['month'] = pd.DatetimeIndex(df['datetime']).month$ df.head()
sns.boxplot(calls_df["length_in_sec"],orient='v')
not_in_oz = stops.loc[~mask & (stops['operator'] == 'bac')].head(5)$ not_in_oz
r.summary2()
cp = nltk.RegexpParser(grammar)$ result = cp.parse(sentence)$ print(result)
mojog_df["unemp_rate"] = 4.1$ mojog_df.head()
for screen_name in napturalistamo_followers_timelines_grouped.index:$     with open("../output/napturalistamo_followers/text/{0}_timeline.txt".format(screen_name), "w") as text_file:$         text_file.write(napturalistamo_followers_timelines_grouped[screen_name])
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
df2.isnull().sum()
trip_month_day = trip_dates.strftime('%m-%d')$ trip_month_day
model_arma = sm.tsa.ARMA(AAPL_array, (2,2)).fit()$ print(model_arma.params)
dftemp = df1[(df1['Area'] == "Iceland")]$ dftemp.head(10)$
iris.drop(['Id'], inplace=True, axis=1)$ print(iris.shape)
np_diff = np.diff(close)$ max(np_diff)$
extract_deduped_cmp = extract_deduped[f_remove_extract_fields(extract_deduped.sample()).columns.values].copy()
df['LinkedAccountName'] = df['LinkedAccountId'].apply(num_to_name)$ df['PayerAccountName'] = df['PayerAccountId'].apply(num_to_name)
HOU_analysis = team_analysis.get_group("HOU").groupby("Category") # Pulls only team transactions from sample, then groups$
all_cards = pd.DataFrame(data = None, columns = all_cards_columns)$ all_cards.rename_axis("name", inplace = True)$ all_cards.head()
obj.rank(ascending=False, method='max')
response_df[response_df.isnull().any(axis=1)]
from fastai.fastai.structured import *$ from fastai.fastai.column_data import *$ from IPython.display import HTML
training_df = features[~features.gameId.isin(production_df.gameId)]
fcc_nn.head()
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])$
clean_merge_df.to_csv("Desktop/Project-2/spotify_merged_data.csv", index=False, header=True)
combined_df.to_csv('kaggle_results.csv', index=False)
np.array([1,2,3,4,5])$
from sklearn.metrics import mean_squared_error, mean_absolute_error$ mean_absolute_error(y_test, y_pred_lm)
cityID = '3b77caf94bfc81fe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Los_Angeles.append(tweet) 
df_pd.sort_values(by='timestamp')$ train_frame = df_pd[0 : int(0.7*len(df_pd))]$ test_frame = df_pd[int(0.7*len(df_pd)) : ]$
dataset['Created_at'] = pd.to_datetime(dataset.Created_at)$ dataset.head()
learner.save_encoder('adam3_10_enc')$
tweets_df.tail()
import numpy as np$ cs = np.log(df['Size'])$ cs = cs.reset_index()
graph_url = 'https://graph.facebook.com/v2.11/'$ req_url = '74133697733?fields=posts{message,created_time,comments.limit(0).summary(true), likes.limit(0).summary(true)}'$ final_url = graph_url + req_url
from sklearn.linear_model import RidgeCV$ rr = RidgeCV(alphas=alphas, fit_intercept=True, normalize=False)$ score_model(rr, alpha=True)
df.to_json('data\8oct_pre_processed_stemmed_polarity.json', orient='records', lines=True)
df.groupby(df.index.tz_localize('GMT').tz_convert('US/Eastern').hour).count().Tweets.plot(kind='barh')
contractor_merge.rename(index=str, columns={"state_abbrev" :"state_code"}, inplace =True)
X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})$ X_new.head()
avgPurchP = train.groupby(by='Product_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseP'})$ train = train.merge(avgPurchP, on='Product_ID', how='left')$ test = test.merge(avgPurchP, on= 'Product_ID', how='left')
%time train_4_reduced = tsvd.transform(train_4)
output= "Update user SET following=50 where user_id='@Pratik'"$ cursor.execute(output)$
FOX = news_df.loc[(news_df["Source Account"] == "FoxNews")]$ FOX.head(2)
df = pd.read_csv('anova_one_factor.csv')
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours Autocorrelation')
tsprior = pd.Timestamp('1/1/2016')$ tsprior2 = pd.Timestamp ('1/1/2017')$ tsprior3 = pd.Timestamp('1/1/2018')
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$     filter(Measurement.station == 'USC00519281').all()$
trump.info()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=YxSY4z-2vyxvJ15WjtFa&start_date=2017-01-01&end_date=2017-12-31"$ req = requests.get(url)$ req.text
pd.crosstab(data.userName, data.Likes.head())
results = requests.get(final_url,{'access_token':token})
gmaps = googlemaps.Client(key='xxxxxxxxx')
xml_in.dtypes
gs.score(X_test, y_test)
x = df_1.groupby('Lang').Review.count().sort_values(ascending = False)$
tmp = df[selected_features].join(outcome_scaled).reset_index().set_index('date')$ tmp.dropna().resample('Q').apply(lambda x: x.corr()).iloc[:,-1].unstack()\$ .iloc[:,:-1].plot(title='Correlation of Features to Outcome\n (by quarter)')$
idx = pd.IndexSlice$ df.loc[idx[:, :, 'x'], :]
IQR = scores_thirdq - scores_firstq$ print('The IQR of the ratings is {}.'.format(IQR))
questions['VIP_YES'] = questions['vip'].map({'Yes':1, 'No':0})$ questions['VIP_YES'].value_counts()
city_pd.replace('',np.nan,inplace=True)$ city_pd.dropna(axis=0,how='any')
df_lineup = df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')") $ print("{} times the new_page and treatment don't line up.".format((df_lineup.shape[0]) ))$
ad_source.columns = ['AD_'+str(col) for col in ad_source.columns]
seed = 2210$ (train_sample, validation) = modeling2.randomSplit([0.9,0.1], seed=seed)
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="vegan").items(max_tweets):$     print(tweet)
tranny.values
df1=df1[['Adj. Close','volatility','PCT_Change','Adj. Open','Adj. Volume']]$ df1.head()
df_subset2.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70)$ plt.show()
iso_join.fillna(99999, inplace=True)
np.clip(bag, 0, 1, out=bag)
!hdfs dfs -put Consumer_Complaints.csv {HDFS_DIR}/Consumer_Complaints.csv
dfrecent = dfdaycounts[dfdaycounts['created_date']> pd.to_datetime('2012-12-31')]
our_nb_classifier = engine.get_classifier("nhtsa_classifier")
(df.isnull().sum() / df.shape[0]).sort_values(ascending=False)   # credit Ben shaver
train_msk = ((train.click_timeDay == 8) & (train.click_timeHour >= 9)) | \$ ((train.click_timeDay == 9) & (train.click_timeHour <= 8))$ val_msk = ((train.click_timeDay == 9) & (train.click_timeHour >= 9) & (train.click_timeHour <= 15))
df_concat_2["time"].str.split(':')$ df_concat_2["time"] = df_concat_2["time"].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
r1 = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=")$
model.wv.most_similar('cost_function', topn=10)
pd.DataFrame(pairing_dict_names)
wrsids[np.where(ids=='105001')[0][0]]$ 
tweet_data = pd.read_json(twitter_json)$ tweet_data.set_index('created_at', drop=True, inplace= True)$ pd.to_datetime(tweet_data.index)
lrs = [0.0001, 0.0001, 0.0001, 0.0001, .001]
type(ts.index)
data['SA'] = np.array([analize_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
a = tips.loc[:,"tip"]$ a.head()
test_data.text
keep_vars = set(no_hyph.value_counts().head(12).index)
new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs$ new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))$ new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)$
totmcap = mcap_mat.T.sum()$ fund_fraq_mcap = fundmcap / totmcap
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])$
count_by_insertid = Counter(df.insert_id)$ insertid_freq = {x : count_by_insertid[x] for x in count_by_insertid if count_by_insertid[x] > 1 }$ print('Duplicated insert_ids: {}'.format(len(insertid_freq.keys())))
rhum_us_full = rhum_nc.variables['rhum'][:, lat_li:lat_ui, lon_li:lon_ui]
readerASN = geoip2.database.Reader(GEOLITE_ASN_PATH)$ readerCITY = geoip2.database.Reader(GEOLITE_CITY_PATH)
tweet_df.info()$
cur.close()$ con.close()
prcp_df.count()
prophet_model = Prophet(interval_width=0.95)  #default==0.8
msft = pd.read_csv("msft.csv", $                     dtype = { 'Volume' : np.float64})$ msft.dtypes
cityID =  '4ec71fc3f2579572'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $          Shreveport.append(tweet) 
print(preprocessor(df.loc[0, 'review'][:-500]), '\n')$ print(preprocessor("</a>This :) is :( a test :-)!"))
tmp = tmp.sort_values(by = 'meantempm', axis = 0, ascending = False)$
tmp_df = tmp_ratings.pivot(index='userId', columns='movieId', values='rating')
df_all_wells_wKNN_DEPTHtoDEPT.info()
random.shuffle(porn_ids)$ porn_bots = porn_ids[:6000]
from google.colab import auth$ auth.authenticate_user()
colors = ["lightblue", "green", "red", "blue", "orange", "maroon"]$ x_axis = np.arange(len(final_df))$ x_labels = diff_df.index
    df.to_csv(path,sep=',',index=False)
dfm = dfn.drop(['usd_pledged','goal','state','slug','currency','deadline','state_changed_at','created_at','backers_count','spotlight','period'], axis=1).copy()
df = pd.read_csv('data/test1.csv')$ df
print(airquality_pivot.index)
pred_cols = features$ df2 = sl.copy()$ df2=df2[pred_cols]
ds = tf.data.TFRecordDataset(train_path)$ ds = ds.map(_parse_function)$ ds
rf.score(X_train_all, y_train)
import pandas as pd$ dff = pd.DataFrame(g.featureMatrix)$ dff.to_csv("foo.csv")
from keras.models import Sequential$ from keras.layers import Dense$
Precipitation_DF.plot(rot=45,title="Precipitation from %s to %s"%(start_date,end_date),figsize=(8,5),grid=None,colormap="PuOr_r")$ plt.show()
df = pd.read_csv('df.csv',index_col=0,low_memory=False)$ df.shape
Not_same = df_3['user_id'].count() + df_43['user_id'].count() $ Not_same$
print(datetime.datetime.strftime(d2, "%Y-%m-%d"))$ type(datetime.datetime.strftime(d2, "%d-%b-%Y"))
!wget -O ChurnData.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/ChurnData.csv
pd.read_sql('SELECT * FROM experiments WHERE temperature = 375 ORDER BY irradiance DESC', conn, index_col='experiment_id')
p_diffs = np.array(p_diffs)$ (p_diffs > actual_diff).mean()
z_score, p_value = sm.stats.proportions_ztest([old_conversions, new_conversions], [n_old, n_new], alternative = 'smaller')$ print(z_score)$ print(p_value)$
plt.savefig("BarSentiment.png")$ plt.show()
df.plot()$ plt.show()
QLESQ.drop_duplicates(subset=["subjectkey"], keep='first', inplace=True)$ QLESQ.shape$ QLESQ.describe()
scores_df.to_csv('../Results/News_Tweets.csv', encoding="utf-8", index=False)
topics_data = pd.DataFrame(topics_dict)
df['Position'].plot(figsize=(20,10))$
store_items.fillna(method='ffill', axis=0) # filled with previous value from the column
public_tweets = api.user_timeline(target_user,count=1)
building_pa.sample(5)
titles_list = temp_df2['titles'].tolist()
from keras.preprocessing.text import Tokenizer$ import numpy as np
payload_scoring = {"fields":X_test.columns.tolist(), "values": values}$ print(payload_scoring)
data = pd.read_table('log.txt', names=['id','api','count','res_time_sum','res_time_min','res_time_max','res_time_avg','interval','created_at'])
news_title_docs_high_freq_words_df = pd.read_pickle(news_title_docs_high_freq_words_df_pkl)$ with pd.option_context('display.max_colwidth', 100):$     display(news_title_docs_high_freq_words_df)
np.count_nonzero(x < 6)
df.drop_duplicates(subset=['address', 'date'], inplace = True)
article_divs = [item.find('div',{'class':'article--container'}) for item in soups]
afx_1d = requests.get(("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json" +$                        "?start_date=2018-08-01&end_date=2018-08-01&api_key={}").format(API_KEY)).json()
sp500.ix[['MSFT', 'ZTS']]
population = evolve_new_generation(tp)$ floreano_experiment = FloreanoExperiment(population, 15)$ floreano_experiment.run_experiment()
sample.head()
scores[:1.625].sum()
combined_salaries.to_csv(directory+'03_cleaned_salaries_for_app.csv',index=False)
df_birth['Country '].value_counts(dropna=False).head()
reddit = reddit.drop_duplicates(subset=['Titles', 'Subreddits'], keep='last', inplace=False) #dropping those $ reddit.head()$ reddit.shape #now I see those specific rows have been dropped$
GenresString=Genres.replace('|',',')$ GenresString=GenresString.replace(',',',\n')
production_df = pd.merge(future_predictions, features, on=['Date', 'HomeTeam', 'AwayTeam', 'season'])
engine.execute('SELECT * FROM measurements LIMIT 5').fetchall()
df = df.dropna()$ df = df[list(df_symbols.columns) + targets]
merged_portfolio_sp['Equiv SP Shares'] = merged_portfolio_sp['Cost Basis'] / merged_portfolio_sp['SP 500 Initial Close']$ merged_portfolio_sp.head()
data.describe()
PCA(2).fit(X)
overall_df = pd.DataFrame(data= twitter_df.groupby('User')['Compound'].mean())$ overall_df
%matplotlib inline  $ import matplotlib.pyplot as plt
df = pd.read_excel('data/analysis2/Dow.xlsx')$ df.head(3)$ df.shape
urb_pop_reader = pd.read_csv(file_wb, chunksize = 1000)$ df_urb_pop = next(urb_pop_reader)$ print(df_urb_pop.head())
globalCityContent = readPDF(globalCityBytes)$ globalCitySentences = globalCityContent.replace('\n','').split('.')$ type(globalCitySentences)
d = feedparser.parse('http://rss.nytimes.com/services/xml/rss/nyt/InternationalHome.xml')
X.columns
df.loc[df.userLocation == 'Manchester, PA', 'tweetText']$
ts.get_k_data(code="sh",start="2016-08-02",end="2018-08-21")
for tweet in negativeTweets['Tweets'].iteritems():$     print (tweet)
browser.find_by_css("a.product-item")[i].click()$ hemisphere_image_urls = []$ links = browser.find_by_css("a.product-item")
tfidf = models.TfidfModel(corpus)
google['high'].apply(custome_roune).plot(kind='hist', bins=6)
index_name = df.iloc[0].name$ print(index_name)
vip_reason = questions['vip_reason'].str.get_dummies(sep="'")
for iter_x in np.arange(lookback_window)+1:$     df['{0}-{1}'.format(base_col,str(int(iter_x)))] = df[base_col].shift(iter_x)
from sklearn.metrics import mean_squared_error, mean_absolute_error$ mean_absolute_error(y_test, y_pred_dt)
inoroffseason = ALLbyseasons.groupby("InorOff") # This groups our sample by whether transactions took place in-season or during$
df['Mo'] = df['datetime'].map(lambda x: x.month)$ df.head()
news_df = pd.DataFrame(news_dict) 
price2017['DateTime'] = pd.to_datetime(price2017['Date'] + ' ' + price2017['Time'])
data_df.groupby('type')['ticket_id'].nunique()
datetime_columns = filter(lambda a: a[-3:] == "_at", df.columns)$ for column in datetime_columns:$     df[column] = pandas.to_datetime(df[column], unit='s')
open('data/wx/tmy3/proc/700197.csv').readlines()[:6]
findM = re.compile(r'wom[ae]n', re.IGNORECASE)$ for i in range(0, len(postsDF)):$ 	print(findM.findall(postsDF.iloc[i,0]))
pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])$
chained2 = itertools.chain.from_iterable([[1, 2, 3, 4], [5, 6, 7, 8]])$ for letter in chained2:$     print(letter)
BAL_analysis = team_analysis.get_group("BAL").groupby("Category") # Pulls only team transactions from sample, then groups$
day_order = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']$ days = sns.countplot(x='Day of Week', data=merge_df, palette='viridis',order=day_order)$ days.set_title('Surveys by Day of Week')
tweet_df_clean = tweet_df.rename(columns={'id':'tweet_id'})
df_users = pd.read_csv('../../data/march/users.csv')$ df_levels = pd.read_csv('../../data/march/levels.csv')$ df_events = pd.read_csv('../../data/march/events.csv', skiprows=1, names=event_header, error_bad_lines=False, warn_bad_lines=True)     
plotdf.loc[plotdf.index[-1], 'forecast'] = hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'meanRemainingTweets'].iloc[0] + plotdf['currentCount'].max()$ plotdf.loc[plotdf.index[-1], 'forecastPlus'] = plotdf.loc[plotdf.index[-1], 'forecast'] + hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0]$ plotdf.loc[plotdf.index[-1], 'forecastMinus'] = plotdf.loc[plotdf.index[-1], 'forecast'] - hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0]
words = lc_review.split(" ")$ words_no_stop = [w for w in words if w not in stopwords.words("english")]
max_sharpe_port_b = results_frame_b.iloc[results_frame_b['Sharpe'].idxmax()]$ min_vol_port_b = results_frame_b.iloc[results_frame_b['SD'].idxmin()]
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())$
print('With KNN (K=) accuracy is: ',knn.score(x_test,y_test)) $
mydata.head()
news_title = soup.title.text
re.search('\$\d+\.(\d+)?', price_tmp).group()$
s = pd.Series(np.random.randn(4))$ s
c.description
page_soup.body.div
sentiment_df=pd.DataFrame(sentiments)$ sentiment_df.head()
req = requests.get(url)$ html = req.text
receipts = pd.DataFrame.from_dict(Receipts)
test_tweet = api.user_timeline(newsOutlets[0])$
y = tweets['handle'].map(lambda x: 1 if x == 'Donald J. Trump' else 0).values$ print(np.mean(y))
events_filtered_1 = events_enriched_df[events_enriched_df['yes_rsvp_count']>50]$ events_filtered_2 = events_df[(events_df['yes_rsvp_count']>50) & (events_df['venue.city']=='Chicago')]
!hdfs dfs -cat {HDFS_DIR}/p32c-output/part-0000* > p32c_results.txt
data2['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data2['Tweets'] ])$ display(data2.head(10))
sns.distplot(data['Age'], kde = False, bins = 60)
df_small = raw_data[raw_data.num_tubes < sets_threshold]$ df_sets = raw_data[raw_data.num_tubes >= sets_threshold]
s3 = pd.Series(0,pd.date_range('2013-01-01','2014-12-31'))$ s3['2013']
data['Sentiments'] = np.array([ predict_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
i=random.randrange(len(train_trees))$ print(train_trees[i])
os.chdir("C:\\#Study\\2. Ryerson_all\\THE MRP\\Dataset\\test_pad")
print(trump.axes)$ print(trump.shape)
df = pd.read_excel("../../data/stocks.xlsx")$ df.head()
shows = pd.DataFrame(data={'title':titles,'status':statuses,'years':years,'network':networks,'genre':genres,\$                           'tagline':taglines,'link':links})
df.drop_duplicates(['title'],keep= 'first',inplace =True)
import pandas as pd$ git_log = pd.read_csv('datasets/git_log.gz',sep='#', header= None,encoding='latin-1',  names=['timestamp', 'author'])$ git_log.head()
instance.updateParameters(cannull,ranges,tests)$ instance.testassumptions()
tweets_raw["date"] = tweets_raw["date"].apply(lambda d: parse(d, ignoretz = True))
os.listdir(os.getcwd() + "/2018-05-26/")[0]
sent_groups_pivot_df = sent_groups_df.pivot(index="Sentiment group", columns="Twitter account", values="Group total")$ sent_groups_pivot_df
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
ldamodel_Tesla= models.ldamodel.LdaModel(corpus_Tesla, num_topics=3, id2word = dictionary, passes=20)
epochs = 50$ batch_size = 128$ keep_probability = 0.5
wget.download('https://cernbox.cern.ch/index.php/s/ibtnI2ESaFjIgSi/download')
sel_shopping_cart = pd.DataFrame(items, index = ['pants', 'book'])$ sel_shopping_cart
df_full['school_type'] = df_full['school_type'].map(DATA_L1_HDR_DICT)
df['created_at'] = pd.to_datetime(df['created_at'])
print(dfd.capacity_5F_max.describe())$ dfd.capacity_5F_max.hist()
!wget -O cell_samples.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/cell_samples.csv
time_length = pd.Series(df.len.values, index=df.Date)$ time_favorite = pd.Series(df.Likes.values, index=df.Date)$ time_retweet = pd.Series(df.Retweets.values, index=df.Date)
zipincome['ZIPCODE'] = zipincome['ZIPCODE'].astype(float)
sentiments_pd= sentiments_pd.sort_values("Date", ascending=False)
corr = newMergedDF.corr().fillna(0)$
nnew = df2.query('landing_page == "new_page"').count()[0]$ print ("The population of Newpage is : {}".format(nnew))
r6s.num_comments.mean()
station_df.head(10)
bikedataframe = pd.concat([dfbikes, snow_weather, df_wu, dfsunrise], axis=1)$ bikedataframe.head()
df_country=pd.get_dummies(data=countries_df, columns=['country'])$ df_country.head()
df_by_donor = donations[['Donor ID','Donation ID', 'Donation Amount', 'Donation Received Date']].groupby('Donor ID', as_index=False).agg({'Donation ID': 'count', 'Donation Received Date': 'max', 'Donation Amount': ['min', 'max', 'mean', 'sum']})
year17 = driver.find_elements_by_class_name('yr-button')[16]$ year17.click()
d1.mean().mean() # caution! this isn't the real mean$
df_activity_prediction = \$     df_active_user_prediction[df_active_user_prediction['user_id'].isin(active_users)]\$     .merge(df_active_user_metrics,on="user_id", how="left")
from sklearn.externals import joblib$ joblib.dump(pca, '../models/pca_20kinput_6858comp.pkl')
df_complete_agg =df_complete_set.loc[:,[1,'Tweets Ago','compound']]$ df_complete_agg.rename(columns={'compound' : 'Tweet Polarity'  ,1 : 'News Agency' },inplace=True)$ df_complete_agg.head()$
btc['2017-08'].plot(y='price')$ plt.show()
intervention_train.isnull().sum()
df['PCT_change']=(df['Adj. Close']-df['Adj. Open'])/df['Adj. Open']*100.0
df.info()
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
ved['season'] = ved.index.str.split('.').str[0]$ ved['term'] = ved.index.str.split('.').str[1]
C = pd.merge(A,B, on = 'team', how = 'left',left_index=True, right_index=True)$ C
ohlc = walk.resample("H").ohlc()$ ohlc
for i in range(len(review_df.rate)):$     review_df.iloc[i,4]=review_df.rate[i][:3]$     review_df.iloc[i,6]=review_df.review_format[i][7:]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print( "z_score: {} \np_value: {} ".format(z_score, p_value))$
grid_id = np.arange(1, 1535,1)$ grid_id_array = np.reshape(grid_id, (26,59))$ grid_id_flat = grid_id_array.flatten()
transit_df_rsmpld = transit_df_byday.reset_index().groupby('FUZZY_STATION').apply(lambda x: x.set_index('DATETIME').resample('1M').sum()).swaplevel(1,0)$ transit_df_rsmpld.info()$ transit_df_rsmpld.head()
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-05-23&end_date=2018-05-23&api_key='+API_KEY$ r1 = requests.get(url1)
cityID = '8173485c72e78ca5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Atlanta.append(tweet) 
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])
df["日時"] = pd.to_datetime(df["日時"])
df7.loc[df7['avg_dew_point Created'] == 'dry', 'avg_dew_point Created'] = np.nan$ df7['avg_dew_point Created'].value_counts(dropna=False)
cityID = 'b49b3053b5c25bf5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Denver.append(tweet) 
station_df['station'].count()
data_df.desc[15]
db = client.test_database$ collection = db.test_collection$ collection
ORDER_TO_PAIR_SHOPIFY = pd.merge(left=BPAIRED_SHOPIFY,right=ORDERS_SHOPIFY[['order_number','created_at']].astype(dtype),left_on='shopify_order_id',right_on='order_number')
df.index[0]
example1_df = spark.read.json("./world_bank.json.gz")
attend_with.columns = ['ATTEND_'+str(col) for col in attend_with.columns]
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
pd.isnull(r_forest).any(1).nonzero()[0]
before_date=str(fc_clean.sel(time =start_of_event, method = 'pad').time.values)[0:10]$ after_date=str(fc_clean.sel(time =start_of_event, method = 'backfill').time.values)[0:10]
apple['2017-07']['Close'].mean()
df2.drop(labels=2862, inplace=True)$ sum(df2['user_id'].duplicated())
df = pd.read_csv('data/cornwall_phones.csv')$ df.head()
mask = (df['message'].str.len() > 3) #Remove cases with message length < 3.$ df = df.loc[mask]
Measurements = Base.classes.measurements$ Stations = Base.classes.stations
column_list1 = ['DewPoint']$ df[column_list1].plot()$ plt.show()
from sklearn.model_selection import GridSearchCV$ param_grid = {'learning_rate': [0.05,0.1],'num_leaves': [40,60,80]}
c_df = new_df.dropna(how='all') $ c_df.size
train_df = replace(combats)$ print(train_df.head(5))
hit_filter_df.to_csv("Desktop/Project-2/numberOneUnique.csv", index=False, header=True)
import gcsfs$ import google.datalab.bigquery as bq$ import pandas as pd
wqYear = dfWQ.groupby('Year')['TotalN'].mean()$ dfWQ_annual = pd.DataFrame(wqYear)
gen = image.ImageDataGenerator()$ batches = gen.flow(X_train, y_train, batch_size=64)$ test_batches = gen.flow(X_test, y_test, batch_size=64)
pd.read_pickle('data/wx/tmy3/proc/703950.pkl', compression='bz2').head()
import pandas as pd$ dataset = pd.ExcelFile("basedados.xlsx")$ data = dataset.parse(0)
sims = gensim.similarities.docsim.Similarity( 'shard', tf_idf[corpus],$                                              num_features=len(dictionary), num_best= 10)$ print(sims)
df.cdescr.head()
data_spd = pd.DataFrame()$ data_spd['tweets'] = np.array(tweet_spd)$ data_spd.head(n=3)
data['Open'].min()
walk['2014-08-01 00:00'].mean()
df.isnull().sum().sum()
dfSummary[4:9].plot(kind='bar',$                     figsize=(20,6),$                     title="Data Summaries: Quantiles");
X_copy['crfa_a'] = X_copy['crfa_a'].apply(lambda x: int(x))
auth = tweepy.OAuthHandler(config.consumer_key, config.consumer_secret)$ auth.set_access_token(config.access_token, config.access_token_secret)$ api = tweepy.API(auth)
mean = np.mean(data['len'])$ print("The average length in tweets: {}".format(mean))
r = dict(r.json())
terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=2)$ feature_columns = [ terms_embedding_column ]
import numpy as np$ ok.grade('q02')
features.tail(3)
print(df.shape)$
public_tweets = api.search('StarWarsDay')
df3['timestamp'] = df3.timestamp.apply(lambda x: pd.datetime.strptime(x, DATETIME_FMT))$
results = Geocoder.reverse_geocode(41.9028805,-87.7035663)
ndvi_change= ndvi_of_interest02-ndvi_of_interest$ ndvi_change.attrs['affine'] = affine
print("Probability an individual recieved new page:", $       df2['landing_page'].value_counts()[0]/len(df2))
fin_r = fin_r.reindex(df.resample('D').index, method='ffill') #df.resample('D').index$ assert (fin_r.index == r_top10_mat.index).all()
r=requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=7up7a3-xNp8abz4VJGsq&start_date=2018-07-22')
subwaydf['DESC'].value_counts()
data.to_csv('cifar10-test-200.csv')$
df.set_index('Day') # this returns the new data frame with index set as 'Day' but does not modify the existing df dataframe$ df.head()
filtered_brewery = df.groupby('brewery_name').filter(lambda x: x.brewery_name.value_counts() >= 3)$ brewery_bw = filtered_brewery.groupby('brewery_name').rating_score.mean().sort_values(ascending=False)
df_tick = df_tick.loc['2017-08-24':'2017-08-30']
df.tail(8)
df_concensus_uaa = df_concensus_uaa.sort_index()
crimes.PRIMARY_DESCRIPTION.head()
exiftool -csv -createdate -modifydate MVI_0011.mp4 MVI_0012.mp4 > out.csv
shows = pd.read_pickle("ismyshowcancelled_raw_pull.pkl")
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
print(r.json()['dataset_data']['column_names'])
INSERT INTO payment (booking_id, payment_date, payment_method, payment_amount)$ VALUES (8, TO_DATE('2017-09-10', 'YYYY-MM-DD'), 'BPay', 741.96)$
cityID = '5d231ed8656fcf5a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Petersburg.append(tweet) 
df_con1 = df2.query("converted=='1'") $ x_ = df_con1["user_id"].count()$ x_$
%%time $ nlp = spacy.load('en_core_web_lg') 
trains_fe2_y= trains_fe1[['reordered']]$ trains_fe2_y.head()
siteNo = '02087500' #Neuse R. Near Clayton $ pcode = '00060'     #Discharge (cfs)$ scode = '00003'     #Daily mean
data_rf = pd.DataFrame(data=[tweet.text for tweet in tweets_rf], columns=['Tweets'])$ display(data_rf.head(10))
ibm_hr_cat_dum = spark.createDataFrame(pd_cat)$ ibm_hr_cat_dum.show(3)
ibm_hr_final2 = ibm_hr_final.join(ibm_hr.select("Attrition"))$ ibm_hr_final2.printSchema()
df_ncrs = pd.read_excel('ncr_data.xlsx', index='Notification')$ df_ncrs.head()
dj_df =  getTextFromThread(urls_df.iloc[0,0], urls_df.iloc[0,1])$ for i in range(1,5):$         dj_df.append(getTextFromThread(urls_df.iloc[i,0], urls_df.iloc[i,1]), ignore_index = True)
print(client.version)
SelectedHighLows = AAPL.loc["2017-06-20":"2017-07-20", ["high", "low"]]$ SelectedHighLows
cityID = '9531d4e3bbafc09d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oklahoma_City.append(tweet) 
rmse_scores = np.sqrt(mse_scores)$ print(rmse_scores)
api_key = '25bc90a1196e6f153eece0bc0b0fc9eb'$ units = 'Imperial'$ url = 'http://api.openweathermap.org/data/2.5/weather'
cityID = 'e444ecd51bd16ff3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cincinnati.append(tweet) 
total_ridepercity=pd.DataFrame(ride_percity)$ total_ridepercity=total_ridepercity.reset_index()$ total_ridepercity
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB.json?start_date=2014-01-01&end_date=2014-01-03&api_key=HL_EJeRkuQ-GFYyb_sVd')
tfav.plot(figsize=(16, 4), label="Likes", legend=True)$ tret.plot(figsize=(16, 4), label="Retweets", legend=True)
from sqlalchemy.orm import Session$ session = Session(bind=engine)
x = api.GetUserTimeline(screen_name="HillaryClinton", count=20, include_rts=False)$ x = [_.AsDict() for _ in x]
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
le_indicators = wb.search("life expectancy")$ le_indicators.iloc[:3,:2]
annual_returns = returns.mean() * 250 $
classifier = tf.estimator.Estimator($     model_fn=mobilenet_model_fn, model_dir=paths["Checkpoints"])
small_data = pd.concat([data.iloc[:2],$                       data.iloc[-2:]])
words_only_sk = [term for term in words_sk if not term.startswith('#') and not term.startswith('@')]$ corpus_tweets_streamed_keyword.append(('words', len(words_only_sk))) # update corpus comparison$ print('The number of words only (no hashtags, no mentions): ', len(words_only_sk))
list(cur.execute('SELECT * FROM experiments'))
df.dropna(how='any', inplace=True)$
df.columns
!wget https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt$ !head -n 5 ProductPurchaseData.txt
conn.execute(sql)
top10_topics_list = top10_topics_2.head(10)['topic_id'].values$ top10_topics_list
df[['text', 'favorite_count', 'date']][df.favorite_count == np.min(df.favorite_count)]
from ramutils.classifier.utils import reload_classifier$ classifier_container = reload_classifier('R1387E', 'catFR5', 1, mount_point='/Volumes/RHINO/')$ classifier_container.features.shape # n_events x n_features power matrix
output_test = output_list$ output_test.count()
print(r_test.json())
df_schools.describe()
qends = pd.date_range('2014-01-01','2014-12-31',freq='BQS-JUN')$ qends.values
least = pd.DataFrame(data.groupby('tasker_id').hired.sum())$ least.loc[least['hired']==0]
noise = [np.random.normal(0,noise_level*p,1) for p in weather.power_output]
Z = np.arange(10)$ np.add.reduce(Z)
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
payload = {'key1': 'Janney1', 'key2':['hello', 'world']}$ r = requests.get('http://httpbin.org/get', params=payload)$ r.url
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-04-27&end_date=2018-04-27&api_key=3w2gwzsRAMrLctsYLAzN')$ json_sample = r.json()
file_name = 'campaign_finance_clean_data.csv'$ dat.to_csv(path_or_buf=file_name,sep=',')
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
from pyspark.ml import Pipeline$ from pyspark.ml.classification import DecisionTreeClassifier$ from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer
cityID = '4fd63188b772fc62'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Laredo.append(tweet) 
print("Mean squared error: %.2f"$       % mean_squared_error(y_test, y_pred))
cityID = 'c7ef5f3368b68777'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baton_Rouge.append(tweet) 
metrics = client.experiments.get_latest_metrics(experiment_run_uid)$ all_metrics = client.experiments.get_metrics(experiment_run_uid)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']])$ results = logit_mod.fit()$ results.summary2()
age_up70.shape
metadata['data_ignore_value'] = float(refldata.attrs['Data_Ignore_Value'])$ metadata
dr_num_new_patients = dr_num_new_patients.astype('float')$ dr_num_existing_patients = dr_num_existing_patients.astype('float')
data.mean()
logmod = sm.Logit(ab_df2['converted'], ab_df2[['intercept','ab_page']])$ results = logmod.fit()
scratch['created_at'] = scratch['created_at'].map(lambda x: x.lstrip('"').rstrip('"'))
print('Median daily volume of 2017: ' + str(int(np.median(vol_vec))))
pd.crosstab(df_result.launched_year, df_result.State).plot()
df.to_csv('citation-data-clean.csv', index=False)$
np.where([min(BID_PLANS_df.iloc[i]['scns_created']) != BID_PLANS_df.iloc[i]['scns_created'][0] for i in range(len(BID_PLANS_df))])
pd.read_clipboard()
with pd.option_context('display.max_colwidth', 130):$     print(news_titles_sr)
news_df = guardian_data.copy()
from sqlalchemy import create_engine$ engine = create_engine('postgresql+psycopg2://postgres:admin@localhost:5432/DTML')$ X_copy.to_sql('dtml_featr_num', engine, if_exists='append')
predictions.show()
svc = SVC(random_state=20)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'decision_function_shape':['ovo', 'ovr'], 'kernel':['linear', 'rbf']}$ grid_svc = GridSearchCV(svc, param_grid=param_grid, cv=10, n_jobs=-1)
nitrodata['Month'].value_counts().sort_index()
data.resample('D',how='mean').head()$
print(r.json()['dataset']['data'][0])
result_3 = pd.concat([df1, df3], ignore_index = True) # same as option 1 but with reset index $ result_3
data.index = pd.to_datetime(data.index)$ data.interpolate(method='time',inplace=True)$
top_10_authors = git_log.author.value_counts(dropna=True).head(10)$ top_10_authors
mask = stops['stopid'].isin(shared_ids)
dfX = data.drop(['created_at','date','ooCost'], axis=1)$ dfY = data['ooCost']
data = pd.read_csv('./data/flowdata.csv',index_col = 0,parse_dates = True)$ data.head()
small_frame.rbind(small_frame)
iso_join = iso_join.dissolve(by=['time', 'time_2']).reset_index()$
spks = np.loadtxt('output/spikes.csv')$ print(spks[1:10, :])
cityID = '813a485b26b8dae2'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Albuquerque.append(tweet)  
projFile = "Projects.csv"$ schedFile = "Schedules.csv"$ budFile = "Budgets.csv"
y = list(train_1m_ag.is_attributed)$ X = train_1m_ag.drop(['is_attributed'],axis=1)
df_unique_providers = df_unique_providers.reset_index(drop=True)$ df_unique_providers.head()
SP_data = "..\Raw Data\S&P 500 Raw data_1-20-2017 ~ 3-21-2018.csv"$ SP_df = pd.read_csv(SP_data, encoding = "ISO-8859-1")$ SP_df.head()
X = [string.replace('\n', ' ') for string in X]
CREATE OR REPLACE VIEW all_rooms AS$     SELECT h.hotel_id, h.hotel_name, r.room_num, r.room_type_code, r.occupancy, r.rate $
nba_df.drop([30,31], axis=0, inplace= True)$ nba_df.loc[28:34]
lr.fit(features_class_norm, overdue_transf)$     $ print_feature_importance(vectorizer.feature_names_, lr.coef_)
model.fit([sources], targets, epochs=10, batch_size=512, validation_split=0.1)$
weather_x = weather_norm.drop('tavg', axis=1)$ weather_y = weather_norm['tavg'].shift(-1)
top_views = doctype_by_day.loc[:,doctype_by_day.min().sort_values(ascending=False)[:10]]$ ax = top_views.plot()$ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
df.sort_values('dealid', ascending = True, inplace = True)
mgxs_lib.build_library()
QLESQ = QLESQ[(QLESQ["level"]=="Level 1") & (QLESQ["days_baseline"] <= 7)]$ QLESQ.shape
frames = [df_tw, df_tw1]$ df = pd.concat(frames)
if not os.path.isdir('output/wind_generation'):$     os.makedirs('output/wind_generation')
pd.crosstab(train.CIA, train.L2_ORGANISATION_ID)
transit_df['DELEXITS']= transit_df['EXITS'].diff()$
engine = create_engine('sqlite:///hawaii.sqlite')
sentiment_df = pd.DataFrame(save_sentiment)$ sentiment_df.to_csv('sentiment.csv', sep=',', header=True, index=True, index_label=None)$ sentiment_df
d = json.loads(r.text[len('var tumblr_api_read = '):-2])$ print(d.keys())$ print(len(d['posts']))
leadsdf['simpleDate'] = pd.to_datetime(leadsdf["lastEnteredOn"],format="%Y-%m-%d")$ leadsdf['simpleDate'] = leadsdf['simpleDate'].dt.strftime('%Y-%m-%d')
url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars"$ base_url = "https://astrogeology.usgs.gov"$ hemisphere_image_urls = []
print((fit.shape, fit1_test.shape))$ print((fit2.shape, fit2_test.shape))$ print((fit3.shape, fit3_test.shape))
previous_month_date = end_date - timedelta(days=30)$ pr = PullRequests(github_index).get_cardinality("id").since(start=previous_month_date).until(end=end_date)$ get_aggs(pr)
ms_columns = inspector.get_columns('measurements')$ for c in ms_columns:$     print(c['name'], c["type"])$
data = pd.read_csv('Nairaland_user_age.csv')$ data
data.groupby(['Name'])['Salary'].sum()
match_results = pd.read_csv("data/afl_match_results.csv")$ odds = pd.read_csv("data/afl_odds.csv")$ player_stats = pd.read_csv("data/afl_player_stats.csv")
bgCan = [[ 5.6 , 7.8, 6.0 ], [ 12.2, 4.4, 6.7 ]] # same ndarray as the NumPy lesson$ df = pd.DataFrame(bgCan, index=['Monday', 'Tuesday'], columns=['Breakfast', 'Lunch', 'Dinner'])$ df
df = pd.read_csv("today.csv", index_col=False)$ df.head()
dat=pd.read_csv(pth)$ print(pth)
sort_df.head(10)
df = pd.read_json("data\8oct_pre_processed_stemmed.json", orient='records', lines=True)
data[data.index.duplicated()]
!wget https://data-ppf.github.io/labs/lab4/Residuals.jpeg$ !wget https://data-ppf.github.io/labs/lab4/Star.obs.jpeg
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
folium.GeoJson(watershed).add_to(m);
xml_in_merged['authorId'].nunique()
load2017 = load2017.dropna() 
print(r.json()['dataset_data']['data'][:4])
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, wait_on_rate_limit=True)
df = pd.read_csv('SHARE_cleaned_lists.csv')
int_and_tel.loc[int_and_tel['sentiment_magnitude'] >= 10]
dfn.to_csv('News.csv')
if pattern.search('AAbc') is not None:$     print('asdfdsf')
from sklearn.decomposition import PCA$ import sklearn
df_birth[df_birth.population > 1000000000]$
city_ride_data = clean_combined_city_df.groupby(["type"]).sum()["fare"]$ city_ride_data.head()$
my_df_free1.iloc[100:110,0:3]
tweet_length.plot(figsize=(20,5),color='r')
f.visititems?
load_dotenv('.env')
df_detail_download_total_time = df_detail.merge(df_mix_comment[['game_format_name','comment_time']], how='left', on='game_format_name')
artistByID.filter(lambda line: '[unknown]' in line).take(5)
cityID = '3f3f6803f117606d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lubbock.append(tweet) 
npath = out_file2$ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
ET_Combine = pd.concat([BallBerry_hour, Jarvis_hour, simResis_hour], axis=1)$ ET_Combine.columns = ['Ball-Berry', 'Jarvis', 'Simple resistance']
lengths = sent.Post.apply(len)$ print('Average character length of the posts are:')$ print (np.mean(lengths))
number_of_commits = git_log['timestamp'].count()$ number_of_authors = len(git_log['author'].dropna().unique().tolist())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']$ table_list = [pd.read_csv(f'{fname}.csv', low_memory=False) for fname in table_names]$ for table in table_list: display(table.head())
print(r.json())$
scores = cross_val_score(model, X, y, scoring='roc_auc', cv=5)$ print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))
pregnancies.data.loc[0]
(data_2017_12_14[data_2017_12_14['text'].str.contains("instantaneamente", case = False)])["text"].head()
data.head()
sts_c_model = logit('label ~ total_bill + tip + size + C(sex) + C(day) + C(time)', $                     data=tdf).fit()$ sts_c_model.summary()
rain_df.set_index('date').head()
automl = autosklearn.regression.AutoSklearnRegressor($ )
df.tail(50)
store_items.pop('new watches')$ store_items
ct = CellTypesApi()$ cells = ct.list_cells(require_reconstruction=True)$
plt.subplot(2,2,1)$ plt.subplot(2,2,2)$ plt.show()
data = pd.concat([data, df_cat], axis=1)
deployment_details = client.deployments.create(model_uid, 'Retail_Churn_with_XGBoost')
print(open('datasets/git_log_excerpt.csv'))
df = pd.read_sql('SELECT * from room', con=conn_b)$ df.head(10) # show 10 rows only
def line_eq(x, mm, bb):$     return  x*mm + bb$ print "# test: %s"%(line_eq(10, 0.4471, -4.2076))$
for col in list(df.columns) :$     k = sum(pd.isnull(df[col]))$     print(col, '{} nulls'.format(k))
dfPre = df.loc['1930-01-01':'1979-12-31']$ dfPost = df.loc['1984-01-01':'2017-12-31']
regr.score(X, y)  # when we fit all of the data points
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)$
tlen.plot(figsize=(16,4), color='r');
datos['AdS'] = np.array([ analiza_sentimiento(tweet) for tweet in datos['Tweets'] ])$ display(datos.head(10))
ORDER_BPAIR_POSTGEN = pd.merge(left=BPAIRED_GEN,right=ORDERS_GEN[['order_number','created_at']].astype(dtype),left_on='shopify_order_id',right_on='order_number')
df_wm.to_csv("walmart_senti_score.csv", encoding='utf-8', index=False)$
start_t = '2013-01-01 00:00:00'$ end_t=pd.to_datetime('today')- timedelta(days=1)$ end_t1=str(end_t)$
no_hyph = df_nona[df_nona['variety']\$                     .apply(lambda x: len(x.split('-')) < 2)]['variety'].str.lower()$ no_hyph = no_hyph[no_hyph.apply(lambda x: x.split()[-1] != 'blend')].replace(repl_dir)
import ibm_boto3$ from ibm_botocore.client import Config
df_raw = df_raw.loc[df_raw.artist.notnull(),:]
counts = combined_df5['vo_propdescrip'].value_counts()$ repl = counts[counts <= threshold].index$ combined_df5['vo_propdescrip']=combined_df5['vo_propdescrip'].replace(repl, 'Other')$
new_fp = 'data/brain2body_headers.txt'$ b2b_df.to_csv(new_fp, index=False) # Setting index to False will drop the index integers, which is ok in this case
train_set.columns = ['tweet_id', 'tweetText', 'polarity_value', 'polarity_type', 'topic','set']
df.loc[mask,:]
sentiments_pd = sentiments_pd[["Outlet", "Date", "Tweet", "Compound", "Positive", "Neutral", "Negative", "Tweets Ago"]]$ sentiments_pd.tail()
yc_new3 = yc_new2[yc_new2.tipPC < 100]
ranks = dev3[to_rank_cols].apply(lambda x: x.rank(ascending=False), axis=0)
sns.factorplot('SA','len',data = data, kind = 'point', size = 6)
df_geo_count = df_geo.groupby("geo_code").count()$ dict_geo_count = df_geo_count.to_dict()["id_str"]
tickers = companies['tweet_ticker'].tolist()$
mv_lens = pd.merge(movies, ratings)
lossprob = fe.bs.smallsample_loss(2560, poparr, yearly=256, repeat=500, level=0.90, inprice=1.0)$
y_pred = model.predict(X_test)$ predictions = y_pred.tolist()
pd.Series(42)
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))
artistDF[locate("Aerosmith", "name") > 0].show(20)$ artistDF[artistDF.artistID==1000010].show()$ artistDF[artistDF.artistID==2082323].show()$
print(commmon_intervention_train.values.shape)$ print(commmon_intervention_test.values.shape)
dictionary = Dictionary(messages_clean)$ corpus = [dictionary.doc2bow(text) for text in messages_clean]
df['water_year2'] = df['datetime'].apply(lambda x: x.year if x.month < 10 else x.year + 1)
chefdf.to_csv('exports/trend_data_chefkoch.csv')
df.to_csv('df_ancestry.csv')$ df_model = h2o.import_file(path='df_ancestry.csv')
list(map(range, a.shape))
np.any(x > 8)
list(r_ord.values())[2]$
html_page = browser.html$ JPL_soup = bs(html_page, "lxml")
blahblah = 'This string is stored in the variable on the left.'$ blahblah
print(list(cos.buckets.all()))
df.sheet_names # see all sheet names$
validation_features = bind_features(validation, train_test="train").cache()$ validation_features.count()
inspector = inspect(engine)$ inspector.get_table_names()
print(len(sessions_p_caption), len(sessions_p_cuepoint), sessions.id.count())$
for screen_name in napturalistamo_youtubers_timelines_grouped.index:$     with open("../output/black_hair_culture/text/{0}_timeline.txt".format(screen_name), "w") as text_file:$         text_file.write(napturalistamo_youtubers_timelines_grouped[screen_name])
test_ind["Pred_state_XGB"] = xgb_model.predict(test_ind[features])$ train_ind["Pred_state_XGB"] = xgb_model.predict(train_ind[features])$ kick_projects_ip["Pred_state_XGB"] = xgb_model.predict(kick_projects_ip_scaled_ftrs)
df.groupby(['weekday']).agg([sum])$
p_new = df2['converted'].mean()$ print ("convert rate for p_new under the null :{} ".format(round(p_new, 4)))
print(dfx.fillna(value=-999.25),'\n')$ print(dfx) # original data remains unchanged. 
encoded_df = pd.get_dummies(df, columns=['category', 'fileType'])$ encoded_df.shape
!gsutil cp gs://solutions-public-assets/smartenup-helpdesk/ml/issues.csv $CSV_FILE
experiment_details = client.repository.store_experiment(meta_props=experiment_metadata)$ experiment_uid = client.repository.get_experiment_uid(experiment_details)$ print(experiment_uid)
store_items = store_items.drop(['store 1'], axis=0)$ store_items
data['Age'].value_counts()
doc.is_parsed
url = "https://bittrex.com/api/v1.1/public/getmarketsummaries"$ r = requests.post(url)  $ data = json.loads(r.content.decode())$
df['conv_flag'] = df.conversations.apply(lambda x: 1 if isinstance(x, dict) else 0)$ df = df[df.conv_flag==1].drop(['users','conv_flag'],axis=1)$ print("... got just conversations")
step_counts = step_counts.fillna(0.)
plt.bar(1,dict["avg"],color="orange",yerr=dict["max"]-dict["min"])$ plt.title("Trip Avg Temp")$ plt.ylabel("Temperature")
print(data_tokenized.shape)$ print(label.shape)$ searchParameters(data_tokenized, np.ravel(label), MLPClassifier(max_iter=500, tol=0.00001), container, text='mlp')
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url)
empDf.join(deptDf, empDf.deptid == deptDf.id).show()
a=DataAPI.write.update_industry(industry="H_SWL1", trading_days=trading_days, override=False)
configure['run']['duration'] = 0.5
fuel_therm_abs_rate = sp.get_tally(name='fuel therm. abs. rate')$ therm_util = fuel_therm_abs_rate / therm_abs_rate$ therm_util.get_pandas_dataframe()
new_stops = new_stops.drop(new_stops.index[[4473,4474]])
sentiments_df = sentiments_df.sort_values(["Target","TweetsAgo"], ascending=[True, False])$ sentiments_df.head()
temp_us = temp_nc.variables['air'][1, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(temp_us)
if not database_exists(engine.url):$     create_database(engine.url)$ print(database_exists(engine.url))
df.head(5)
len(ibm_train.columns), len(feature_col)
dr_new.columns$
joined = joined.dropna(axis=0)$ print('Number of rows in joined = {}'.format(joined.CustomerID.count()))
STEPS = 365*10$ random_steps = pd.Series(np.random.randn(STEPS), index=pd.date_range('2000-01-01', periods=STEPS))
rddScaledScores = RDDTestScorees.map(lambda entry: (entry[1] * 0.9))$
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))$
dfd = dfh[dfh['Centrally Ducted or Ductless'].str.startswith('Ductless')]$ print(len(dfd))$ dfd['Centrally Ducted or Ductless'].unique()
p_old = df2['converted'].mean()$ print("{} is the convert rate for Pold under the null.".format(p_old))
x = plt.gca().xaxis$ for item in x.get_ticklabels():$     item.set_rotation(45)
set(list(building_pa_specs['Column name'].values))==set(list(building_pa.columns))
import pandas as pd$ companies = pd.read_csv('Fortune-1000-Company-Twitter-Accounts.csv')
symbol='IBB'$ benchmark2 = web.DataReader(symbol, 'yahoo' , start_date ,end_date)
from scipy.stats import norm$ print(norm.cdf(z_score)) #Z-score significance$ print(norm.ppf(1-(0.05))) # It tells us what our critical value at 95% confidence is $
tweet_favourite.plot(figsize=(20,8), label="Likes", legend=True)$ tweet_retweet.plot(figsize=(20,8), label="Retweets", legend=True )
plt.show()
df.columns=['created_at','id_str','in_reply_to_user_id_str','raw_text']$ df = df.drop(columns='created_at')
import matplotlib as mpl$ mpl.get_backend()
df2.drop(labels=1899, axis=0, inplace=True)$ df2[df2['user_id']==773192]
pax_path = 'paxraw_d.csv'$ %time pax_raw = pd.read_csv(os.path.join(data_dir, pax_path), dtype=type_map)
Google_stock.head()
pgh_311_data.resample("M").count()
df['Shipping Method name'] = df['Shipping Method name'].fillna(df['Shipping Method Id'])
max_IMDB = scores.IMDB.max()
Base.classes.keys()
spotify_df["Streams per 1 million"] = (spotify_df["Streams"]/spotify_df["Population"]*1000000).round(2)
my_url = "http://www.ipaidabribe.com/reports/all#gsc.tab=0"
transfer_duplicates = BTC.loc[BTC['Smoother'].isnull()==False,['Year','Month','Day','Smoother']];
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df.head()
small_frame = loan_stats[1:3,['loan_amnt','installment']]$ single_col_frame = loan_stats[1:3,'grade']$ small_frame.cbind(single_col_frame) 
t2['p1'] = t2['p1'].str.replace('_', ' ')$ t2['p2'] = t2['p2'].str.replace('_', ' ')$ t2['p3'] = t2['p3'].str.replace('_', ' ')
df.iloc[[1]]
for i in range(0,10):$     topics = model.show_topic(i, 10)$     print "%s: %s" %(i, (', '.join([str(word[0]) for word in topics])))
df_uv = df.query('landing_page != "new_page"') $ df_vu = df_uv.query('group == "treatment"')$ df_vu.count()$
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'],unit='s')$ git_log.describe()
df.sort_index(axis=1,ascending=False)
Highest_opening_price = mydata['Open'].max()$ Highest_opening_price
bands.at['2018-04-29 13:24:49', '68'] = 1$ bands = bands.drop(['drop', 'drop2', '68_2', 'drop3', 'drop4'], axis=1)
measurements_df.head()
bb.plot(y='volume')
mean = np.mean(data['len'])$ print("The average length of tweets: {}".format(mean))
cat_feats = ['Company']$ features = pd.get_dummies(features, columns=cat_feats, drop_first=True)$ features.head()
s_filled = s.fillna(0)$ s_filled
qrt = closePrice.resample('Q').mean()
if not os.path.isdir('output/pv_production'):$     os.makedirs('output/pv_production')
!wget https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv$ !head -n 2 Consumer_Complaints.csv
merged_portfolio_sp_latest = pd.merge(merged_portfolio_sp, sp_500_adj_close, left_on='Latest Date', right_on='Date')$ merged_portfolio_sp_latest.head()
import datetime as dt$ from dateutil.relativedelta import relativedelta
twelve_months_prcp.head()
grouped_publications_by_author.tail(10)
with open("Valid_events_eps0.7_5days_500topics","rb") as fp:$     Valid_events = pickle.load(fp)
options_frame['ModelError'].hist()$
results_image = soup_mars.find_all("div", { "class" : "img" })$ image = results_image[1]$ image$
prcp['date']=pd.to_datetime(prcp['date'])$ type(prcp.loc[0,'date'])
ideas.drop(['Authors', 'Link', 'Tickers', 'Title', 'Strategy'],axis=1,inplace=True)$ ideas = ideas.applymap(to_datetime)
station_count.sort_values(['Count'],ascending=False, inplace=False, kind='quicksort', na_position='last')
fig, ax = plt.subplots(figsize = (14.5,8.2))$ ax.hist(events_df['words_count'], 50, facecolor='green', alpha=0.75)$ plt.show()
sns.barplot(data=df.groupby('education').agg({'applicant_id':lambda x:len(set(x))}).reset_index(),$             x='education',y='applicant_id')
modelFCDL.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])$
all_data_merge = pd.merge(all_data, mapping, on = ['rpc'])$ all_data_merge.shape
crimes.dtypes
plt.rc('figure', figsize=(5, 5))$ mosaic(crosstabkw.stack(),gap=0.03 )$ plt.title('Mosaic graph: Successful / Contain Keyword')$
r_json = r.json()$
daily_df.reset_index(inplace=True)$
prcp.describe()
files_gps = []$ for x in range(771,797): #can change this range to pull any gps files; might be limited by API calls depending on #$     files_gps.append(syn.downloadTableFile(table_results, column='UnknownFile_1.json.items', rowId=x, versionNumber=1, ifcollision=u'keep.both'))
test.fillna(0, axis=1, inplace=True)$ train.fillna(0, axis=1, inplace=True)
pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')
paragraphs = soup.find_all('div', class_='rollover_description_inner')   $ print(paragraphs)
weather.info()
cityID = '00ab941b685334e3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Nashville.append(tweet) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
df1['forcast']=np.nan
(train.shape, test.shape)
sns.set_style('whitegrid')$ sns.distplot(data_final['countPublications'], kde=False,color="red") # bins=30, $
df.std()
import matplotlib.cm as cm$ dots_c, line_c, *_ = cm.Paired.colors
import matplotlib.pyplot as plt
pd.to_datetime(['2009/07/31', 'asd'], errors='coerce')
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'America/Los_Angeles').strftime('%D: %r')$ date_min = date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'America/Los_Angeles').strftime('%D: %r')
zstat_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(zstat_score, p_value)$ print("The Pvalue for null Hypothesis is {}".format(p_value))
df_cs.isDuplicated.value_counts()$ df_cs.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
new_doc = repos[4]$ new_vec = dictionary.doc2bow(new_doc)$ repos_langs.iloc[4, 0] # Para testar nossa hipotese vamos encontrar usuarios similares ao usuario 4
data.tail()
aldf = indeed1.append(tia1, ignore_index=True)
merged_portfolio_sp_latest_YTD_sp = pd.merge(merged_portfolio_sp_latest_YTD, sp_500_adj_close_start$                                              , left_on='Start of Year', right_on='Date')$ merged_portfolio_sp_latest_YTD_sp.head()
num_row = df.shape[0]$ print("{} rows in the dataset.".format(num_row))
df['end date'] = df.index.map(lambda x:x.end_time)$ df
my_gempro.genes.get_by_id('Rv1295').protein.representative_structure$ my_gempro.genes.get_by_id('Rv1295').protein.representative_structure.get_dict()
print(X.shape,y.shape)$ print(X_train.shape,y_train.shape)$ print(X_test.shape,y_test.shape)
test_ind["Pred_state_RF"] = trained_model_RF.predict(test_ind[features])$ train_ind["Pred_state_RF"] = trained_model_RF.predict(train_ind[features])$ kick_projects_ip["Pred_state_RF"] = trained_model_RF.predict(kick_projects_ip_scaled_ftrs)
x_train, x_test, y_train, y_test = train_test_split(bow_df, amazon_review['Sentiment'], shuffle= True, test_size=0.2)
print("The loglikelihood estimations are below. \n ")$ print("Loglikelihood:\n",  loglikA)$
rng=pd.period_range('1/1/2000', '6/30/2000', freq='M')$ rng
newdf.info()
df_lm.filter(regex='q_lvl_0|last_month|q_lvl_0_c').boxplot(by='last_month', figsize=(10,10),showfliers=False)
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
model = pd.get_dummies(auto_new.CarModel)$ model = model.ix[:, ["ToyotaCamry", "ToyotaCorolla","ToyotaRav4", "ToyotaLandCruiserPrado", "ToyotaIpsum", "ToyotaSienna", "Toyota4-Runner"]]$ model.head()
url_df_full=url_df[url_df['url'].isnull()==False]
today = datetime.datetime.today()$ start_date = str(datetime.datetime(today.year, today.month-1, 1).date())$ print('predictions for', start_date)
sublist = [BAL, CHI, HOU, PIT] # These are the four teams with an original Per Seat Price column uploaded.$ for team in sublist:$     team.drop(["Per Seat Price"], axis = 1, inplace = True) # Drops the column
fig1, ax1 = plt.subplots()$ ax1.pie(stage_summary['count'], labels = stage_summary['stage']) $ fig1.savefig('Stage Pie Plot')$
sub_gene_df['type'].value_counts()
sns.boxplot(data=sample)
all_tables_df.OWNER.nunique()
car = car[['dayofweek','date','incidntnum']].reset_index(drop=True)$ car.head()
pixiedust.enableJobMonitor()$ sqlContext = pixiedust.SQLContext(sc)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
df['SA'] = np.array([ analize_sentiment(tweet) for tweet in df['Tweets'] ])$ display(df.head(10))
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
from bs4 import BeautifulSoup             $ example1 = BeautifulSoup(train["review"][0], 'html.parser')  
fi = ed_level.append(free_sub)
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline',$                         xls_dir='scen2xls', file_name='data.xlsx', verbose=False)
pd.read_sql('SELECT * FROM experiments WHERE irradiance = 700 ORDER BY temperature', conn, index_col='experiment_id')
stations = df.station.unique()$ no_of_stations = len(stations)$ no_of_stations
df_test_index.iloc[6260, :]
dc.head(5)
clusters_kmeans_predict2 = clf_kmeans2.fit_predict(X_train2)$
file_names = []$ file_names = glob.glob('*.csv')
import pandas as pd$ tweets = pd.DataFrame(tweet_list)$ tweets.info()
dfTickets.to_csv('all_tickets.csv', index=False, index_label=False)
store_items = store_items.rename(columns={'bikes': 'hats'})$ store_items
releases.head()
go_no_go = 1 #NO_GO - ie do NOT run all the long run-time items.  Note that some of these long$
Series(np.random.randn(6), index=rng)
rain_df = pd.DataFrame(rain_score)$ rain_df.set_index('date').head()$ rain_df.head()
scoring_url = client.deployments.get_scoring_url(deployment_details)$ print(scoring_url)
ans = pd.pivot_table(df, values='D', index=['A','B'], columns=['C'])$ ans
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK', 'ab_page', 'CA_ab_page', 'UK_ab_page']])$ results = logit_mod.fit()$ results.summary2()
sp500.iloc[[0, 2]]
from helper_code import mlplots as ml$ ml.confusion(y_test.reshape(y_test.shape[0]), $              predicted, ['No Failure', 'Failure'], 2, 'O-Ring Thermal Distress')
df_reader = pd.read_csv(file_wb, chunksize = 10)$ print(next(df_reader))$ print(next(df_reader))$
with model:$     idx = np.arange(n_count_data)$     lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)
s.loc[s>4]
with open('key_phrases_rake.pickle', 'wb') as f:$     pickle.dump(key_phrases_rake, f, pickle.HIGHEST_PROTOCOL)
sample = pd.read_csv('../assets/sampleSubmission')$ test = pd.read_csv('../assets/test')
result = requests.get(url)$ c = result.content$ soup = BeautifulSoup(c, "lxml")
DATADIC = pd.read_csv('DATADIC.csv')$ list(DATADIC['FLDNAME'].unique())$ DATADIC[['FLDNAME', 'TEXT']].head()
t2.tweet_id=t2.tweet_id.astype(str)
companies = ["WIKI/ATVI.11","WIKI/ADBE.11","WIKI/AKAM.11","WIKI/ALXN.11","WIKI/GOOGL.11","WIKI/AMZN.11","WIKI/AAL.11","WIKI/AMGN.11","WIKI/ADI.11","WIKI/AAPL.11","WIKI/AMAT.11","WIKI/ADSK.11","WIKI/ADP.11","WIKI/BIDU.11","WIKI/BIIB.11","WIKI/BMRN.11","WIKI/CA.11","WIKI/CELG.11","WIKI/CERN.11","WIKI/CHKP.11","WIKI/CTAS.11","WIKI/CSCO.11","WIKI/CTXS.11","WIKI/CTSH.11","WIKI/CMCSA.11","WIKI/COST.11","WIKI/CSX.11","WIKI/XRAY.11","WIKI/DISCA.11","WIKI/DISH.11","WIKI/DLTR.11","WIKI/EBAY.11","WIKI/EA.11","WIKI/EXPE.11","WIKI/ESRX.11","WIKI/FAST.11","WIKI/FISV.11","WIKI/GILD.11","WIKI/HAS.11","WIKI/HSIC.11","WIKI/HOLX.11","WIKI/IDXX.11","WIKI/ILMN.11","WIKI/INCY.11","WIKI/INTC.11","WIKI/INTU.11","WIKI/ISRG.11","WIKI/JBHT.11","WIKI/KLAC.11","WIKI/LRCX.11","WIKI/LBTYA.11","WIKI/MAR.11","WIKI/MAT.11","WIKI/MXIM.11","WIKI/MCHP.11","WIKI/MU.11","WIKI/MDLZ.11","WIKI/MSFT.11","WIKI/MNST.11","WIKI/MYL.11","WIKI/NFLX.11","WIKI/NVDA.11","WIKI/ORLY.11","WIKI/PCAR.11","WIKI/PAYX.11","WIKI/PCLN.11","WIKI/QCOM.11","WIKI/REGN.11","WIKI/ROST.11","WIKI/STX.11","WIKI/SIRI.11","WIKI/SWKS.11","WIKI/SBUX.11","WIKI/SYMC.11","WIKI/TSLA.11","WIKI/TXN.11","WIKI/TSCO.11","WIKI/TMUS.11","WIKI/FOX.11","WIKI/ULTA.11","WIKI/VRSK.11","WIKI/VRTX.11","WIKI/VIAB.11","WIKI/VOD.11","WIKI/WBA.11","WIKI/WDC.11","WIKI/WYNN.11","WIKI/XLNX.11"]#"WIKI/PCLN.11",
three = df.pop('three')
lims_df.columns
all_tables_df.iloc[0]
train_4_reduced.shape, y_train.shape
tweets_by_user_mentions = pd.read_sql_query(query, conn, parse_dates=['created_at'])$ tweets_by_user_mentions.head()
list(filter(lambda num:num%2==0,seq))    # seq = [1,2,3,4,5]  you can see above just below section 8.2
df_roll = df.set_index("posted_date")$ df_roll = df_roll.resample("1h").sum().fillna(0).rolling(window=3, min_periods=1).mean()$ df_roll.reset_index(inplace=True)
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, prec_us)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata  = pd.read_csv(path, sep =' ')$ mydata.head(5)
transfer_duplicates.apply(lambda row: smoother_function_part2(row["Year"], row["Month"], row["Day"], row["Smoother"]), axis=1);
val_avg_preds2 = np.stack(val_pred2).mean(axis=0)$ print ("type(val_avg_preds2):", type(val_avg_preds2), val_avg_preds2.shape)$ print(val_avg_preds2[0:10, :])
df[12].plot()
rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]
loss = 10 / np.linspace(1, 100, a.size)$ loss.shape
from scipy.integrate import odeint$ def cap_continua(C, t, r):$     return r * C
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep$ lee_train_file = test_data_dir + 'lee_background.cor'
data.to_csv('/home/sb0709/github_repos/bootcamp_ksu/Data/data.csv', sep = ',')
config = {"features analyzed": ["Sex", "Pclass", "FamilySize", "IsAlone", "Embarked", "Fare", "Age", "Title"]}$ datmo.snapshot.create(message="EDA", config=config)
station_count.loc[(station_count['Count'] >= 2000)]
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new['country'].astype(str).value_counts()
tweets_df.iloc[3, 10]
del merged_portfolio_sp_latest_YTD['Date']$ merged_portfolio_sp_latest_YTD.rename(columns={'Adj Close': 'Ticker Start Year Close'}, inplace=True)$ merged_portfolio_sp_latest_YTD.head()
data.sort_values('TMED', inplace=True, ascending=False)$ data.head()
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')$ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
tweet = soup.find('div', class_="js-tweet-text-container")
df_artist.columns = df_artist.columns.str.replace(" ","_")$ df_artist.head(2)
grouped.size().unstack().fillna(0).plot(kind="bar", stacked=True, figsize=(10,6)).legend(loc='center left', bbox_to_anchor=(1, 0.5))$ plt.show()
if 'WindSpeed' and 'WindDir' in dat.columns:    $     dat.loc[dat.VecAvgWindDir.isnull(), 'VecAvgWindDir']=LVL1.vector_average_wind_direction_individual_timestep(WS=dat.WindSpeed[dat.VecAvgWindDir.isnull()], WD=dat.WindDir[dat.VecAvgWindDir.isnull()])$ 
pulledTweets_df.sentiment_predicted_lr.value_counts().plot(kind='bar', $                                                            title = 'Classification using Logistic Regression model')$ plt.savefig('data/images/Pulled_Tweets/'+'LR_class_hist.png')
vacancies['weekday'] = vacancies['created'].apply(lambda x: x.weekday() + 1)$ vacancies['hour'] = vacancies['created'].apply(lambda x: x.hour)
total_y = list(youTubeTitles.values[:2500,1]) + list(pornTitles.values[:2500,1])
df = actuals.merge(backcast, on='Gas_Date')
TEST_DATA_PATH = SHARE_ROOT + 'test_dataframe.pkl'$ test_df.to_pickle(TEST_DATA_PATH)
mike=pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/mike_previous_observing.txt',delim_whitespace=True,header=None,names=['index','name','ra','dec'])$ mike['RA0']=[float(Angle(i, u.hr).to_string(unit=u.degree, precision=5, decimal=True)) for i in mike.ra]$ mike['DEC0']=[float(Angle(i, u.deg).to_string(unit=u.degree, precision=5, decimal=True)) for i in mike.dec]$
cityID = '01c060cf466c6ce3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Long_Beach.append(tweet) 
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
df_user[df_user['user.name'] == 'Marco Rubio']
k = 3$ neigh = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)
year_to_date = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(year_to_date)
df = pd.DataFrame(data, columns = ['Rank','Cafe','Menu','URL'])$ df.head(5)
Temperature_DF=pd.DataFrame(results)$ Temperature_DF.head()
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Type"])
con,cur = helper.connect_to_db()$
aug2014.start_time, aug2014.end_time
print (quand_request.json())
orig_ticker = pd.DataFrame(ticker) $ orig_ticker['Ticker_Symbol'] = pd.Series(ticker, index=data_ticker.index)
x = tags['Count'][0:20]$ y = tags['TagName'][0:20]$ sns.barplot(x, y, color = 'g')
exiftool -csv -createdate -modifydate cisnwf6/Cisnwf6_cycle3.MP4 > cisnwf6.csv
cur.execute('INSERT INTO materials VALUES ("EVA", "ethylene vinyl acetate", 0.123, 4.56, "polymer")')$ conn.commit()  # you must commit for it to become permanent$ cur.rowcount  # tells you how many rows written, sometimes, it's quirky
close_idx = afx['dataset']['column_names'].index('Close')
injury_df.sort_values('Date', ascending=False).head()
grp = dta.groupby(dta.results.str.contains("Pass"))$ grp.groups.keys()
df_f2.loc[df_f2["CustID"].isin([customer])]
voters = pd.read_csv('data_raw_NOGIT/voters_district3.txt', sep='\t')$ households = pd.read_csv('data_raw_NOGIT/households_district3.txt', sep='\t')$ households_with_count = pd.read_csv('data_raw_NOGIT/AK_hhld_withVoterCounts.txt', sep='\t')
tcat_df.to_csv('cat_tweets.csv')$ tdog_df.to_csv('dog_tweets.csv')
cnx.commit()$ c.fetchall()
import numpy as np$ ok.grade('q04')
data_config = dict(path='tests/data/nhtsa_as_xml.zip',$                    databasetype="zipxml", # define that the file is a zip f$                    echo=False)
t2['p1'] = t2['p1'].str.title()$ t2['p2'] = t2['p2'].str.title()$ t2['p3'] = t2['p3'].str.title()
donor_groups = df3_obs[['Donor ID', 'Donation Received Date']].groupby('Donor ID')$ time_diff = donor_groups.apply(lambda df: df['Donation Received Date'].diff().mean().fillna(0))$ print(time_diff.head())
display(data.head(10))
df['range'] = (df.max(axis='columns') - df.min(axis='columns'))
datAll['year'] = datAll['Date'].map(lambda x: x.year)$ datAll['month'] = datAll['Date'].map(lambda x: x.month)$
alice_sel_shopping_cart = pd.DataFrame(items, index=['glasses', 'bike'], columns=['Alice'])$ alice_sel_shopping_cart
X_train, X_test, y_train, y_test = train_test_split(stock.drop(['target'], 1), stock['target'], test_size=0.3, random_state=42)
learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)
from pandas.tseries.offsets import *$ d + BusinessDay()
FTE = pd.DataFrame(requirements, index = ['FTE'])$ FTE
df[df.Predicted == 5]
ts.get_growth_data(2018,1)
q = pd.Period('2017Q1',freq='Q-JAN')$ q2 = pd.Period('2018Q2',freq='Q-JAN')$ q2-q
tweets_df = pd.DataFrame(tweets_all)$ tweets_df['retweet_ratio'] = (tweets_df['retweet_count']/tweets_df['followers_count'])*10000
print(prec_long_df['date'].min(), prec_long_df['date'].max())
pivoted.T[labels == 1].T.plot(legend=False, alpha=0.1)
res = sts.query(qry)
tweets_RDD = sc.textFile('./PSI_tweets.txt')$ tweets = tweets_RDD.collect() #or tweets = tweets_RDD.take(500) for some testing $
session.query(Station.name).count()
response = requests.get(url)$ response
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}]$ new_store = pd.DataFrame(new_items, index = ['store 3'])$ new_store
search.timestamp = pd.to_datetime(search.timestamp)$ search.trip_start_date = pd.to_datetime(search.trip_start_date)$ search.trip_end_date = pd.to_datetime(search.trip_end_date)
! ./data_tools/download.sh$ ! ./data_tools/split.sh
y_pred = rf.predict(X_test)
station_cnt = session.query(Measurement.station).distinct().count()$ station_cnt
print("{:.2f} GB".format(df.fileSizeMB.sum() / 1024))
targettraffic = dfs_morning.merge(census_zip, left_on=['STATION'], right_on=['station'], how='left')$ targettraffic['targettraffic'] = targettraffic['ENTRIES_MORNING'] * targettraffic['betw150kand200k']/100
plt.figure(0)$ source_counts = df['sourceurl'].value_counts().head(10)$ source_counts.plot.bar()
import sys$ import os$ from urllib.request import urlretrieve
from sklearn.neighbors import KNeighborsClassifier $ from sklearn.model_selection import train_test_split$ V=pd.merge(cards,game_vail,right_index=True, left_index=True)$
media_classes = [c for c in df_os.columns if c not in ['domain', 'notes']]$ breakdown = df_questionable[media_classes].sum(axis=0)$ breakdown.sort_values(ascending=False)
new_group=df.groupby(by=result)$ new_group
recommendation_sets = len(sample.index)$ recommendation_sets
filename = processed_dir+'pulledTweetsCleanedLemmaEmEnc_df'$ gu.pickle_obj(filename,pulledTweets_df)
mar_file = os.path.join(DATA_DIR, "addresses.xlsx")
most_freq = data[(data['longitude'] == 103.93700000000001) & (data['latitude'] == 1.34721)]
from IPython.core.display import display, HTML$ display(HTML("<style>.container { width:100% !important; }</style>"))
df5 = df4.set_index(pd.DatetimeIndex(df4['Date']))$ df5 = df5[['BG']].copy()$ df5
df3.to_csv('lyincomey_subset_v5_wseconds.csv')
agg_function = {'budget':np.mean, 'reading_score':np.mean, 'math_score': np.mean, 'size': np.mean, 'passing_reading': np.sum, 'passing_math': np.sum}
engine.execute('SELECT * FROM measurements LIMIT 15').fetchall()$
tsd = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with tsd.open('inpt-forecasting/Transplant Stemsoft data -040117 to 061518.xlsx') as tsd_f:$   tsd_df = pd.read_excel(tsd_f)
car18= car.groupby('dayofweek')['incidntnum'].agg({'numofinstance_car':'nunique'}).reset_index()$
doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]
def get_client():$     client = soundcloud.Client(client_id=CLIENT_ID)$     return client
sample_record="0,5,1,4,0,0,0,0,0,1,0,0,0,0,0,6,1,0,0,0.9,1.8,2.332648709,10,0,-1,0,0,14,1,1,0,1,104,2,0.445982062,0.879049073,0.40620192,3,0.7,0.8,0.4,3,1,8,2,11,3,8,4,2,0,9,0,1,0,1,1,1"$ label,payload = sample_record.split(',',maxsplit=1)
pd.DataFrame(dummy_var["_Source"][Company_Name]['Low']['Forecast'])[-4:]
fa_index = pd.read_excel(path + 'fa_indexs.xlsx')$ fa_index
hm = pd.read_csv('Resources/hawaii_measurements.csv')$ hs = pd.read_csv('Resources/hawaii_stations.csv')
ls_data = pd.read_excel(cwd+'\\LS_mail_0604_from_python.xlsx')$ ls_columns=ls_data.columns.values
for row in session.query(Measurements).limit(5).all():$     print(row)$
irisRDD = SpContext.textFile("iris.csv")$ irisData = irisRDD.filter(lambda x: "Sepal" not in x)$ print (irisData.count())
ans = df.groupby(['A','B']).sum()$ ans
s_nonulls = s.dropna()$ s_nonulls
Aussie_df = toDataFrame(aussie_results)$ Aussie_df.head(5)
data = data_all.dropna(subset=['has_odds', 'sigma_scaled'])$ data.tail(3)
intervention_train.index.duplicated()
serc_pixel_df = pd.DataFrame()
kickstarters_2017[cont_vars].corr()
empDf.select("name").show()
fitfile = FitFile('fit_files/2871238195.fit')#longer swim two way$ fitfile = FitFile('fit_files/2913114523.fit')#krumme lanke swim$
s = pd.to_datetime(data.created_at) $ s.head()$ data.index = s
display(data.head(10))
kick_projects.isnull().sum()
year11 = driver.find_elements_by_class_name('yr-button')[10]$ year11.click()
augur.GitHubAPI.code_reviews = code_reviews$ ld = github.code_reviews('rails', 'rails')$
cats_in = intake.loc[intake['Animal Type']=='Cat']$ cats_in.shape
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))$
xgb_learner.fit_best_model(dtrain)
out_temp_columns = [s for s in daily_dat.columns if primary_temp_column in s] #only save select temperature columns$ save_name=Glacier.lower()+ Station + "_daily_"+"LVL2.csv" #filename$ save_pth=os.path.join(save_dir, save_name) #location to save file
import re$ df.loc[:,"message"] = df.message.apply(lambda x : " ".join(re.findall('[\w]+',x))) $
results = session.query(Measurement.date, Measurement.prcp).\$     filter(and_(Measurement.date <= Curr_Date, Measurement.date >= One_year_ago_date)).\$     order_by(Measurement.date.desc()).all()$
import pandas_datareader.data as web  #Not 'import pandas.io.data as web' as in the book.
c = Counter(tree_labels)$ pprint.pprint(c.most_common(10))
plt.legend(handles=[Urban,Suburban,Rural], loc="best")
tweet_archive_clean['tweet_id'].isin(tweet_image_clean['tweet_id']).value_counts()$
print(abs(-2))$ print(round(3.234543,3))$
weather_mean.iloc[1, 4]
hit_tracker_df.to_csv("Desktop/Project-2/hit_songs_only.csv", index=False, header=True)
df.groupby("newsOutlet")["compound"].max()
url ='https://graph.facebook.com/v2.6/'+str(x[1])+'/posts/?fields=id,comments.limit(0).summary(true),shares.limit(0).summary(true),likes.limit(0).summary(true),reactions.limit(0).summary(true)'
snowshoe_df = pd.read_pickle('..\data\interim\df_for_snowshoe_classification')$ snowshoe_df.drop(columns = ['datetime'],inplace=True)$ snowshoe_df.head()
tlen.plot(figsize=(16,4), color='r')
data.iloc[[1,10,3], [2, 3]]
df=pd.read_csv("dataset_quora/quora_train_test.csv")
free_data.iloc[:5]
result3.summary2()
compound_final.set_index(['Date'], inplace=True)$ compound_final.head()
final_df = diff_df.merge(final_sentiment_df, how = "outer")$ final_df$ round(final_df, 3)
len(mod.coef_)
if not os.path.isdir('output'):$     os.makedirs('output')
model = os.path.join('files', '2008--Mendelev-M-I--Al--111-GSF.xml')$ gamma = am.defect.GammaSurface(model=model, box=box)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=zDPbq2QVaB7jFAEq5Tn6')$ print(r.json())
df['y'].plot()
msftA = msft[['Adj Close']] $ closes = pd.concat([msftA, aaplA], axis=1)$ closes[:3]
!head -n 2 ProductPurchaseData.txt
newsMood_df['Timestamp'] = pd.to_datetime(newsMood_df['Timestamp'])$ newsMood_df.to_csv('newsMood_dataframe.csv')$ newsMood_df.head()
%%time$ [upload_blob(bucket_name, source_file, source_file.replace('\\', '/')) for source_file in filepathlist]
df['id'] = df['id'].astype('category')$ df['sentiment'] = df['sentiment'].astype('category')$ df['created_at'] = pd.to_datetime(df['created_at'])
AVG_daily_trading_volume = mydata['Traded Volume'].mean()$ AVG_daily_trading_volume
pd.DataFrame(dummy_var["_Source"][Company_Name]['High']['Forecast'])[-6:]
bad_iv.groupby(['Strike']).count()['Expiration']
table = pd.read_html("https://en.wikipedia.org/wiki/List_of_sandwiches", header=0)[0]$
dfEtiquetas["city"] = dfEtiquetas["place"].apply(lambda p: p["location"]["city"] if "city" in p["location"].keys() else None)
apple = web.DataReader("AAPL", "morningstar", start, end)
plt.figure(figsize=(15,8))$ ax = sns.boxplot(x = 'Type 1', y = 'Total', data = pokemon )$ plt.show()
recortados = [recortar_tweet(t) for t in tweets_data]$ tweets = pd.DataFrame(recortados)
masked['user_age_days'] = [ele.days for ele in masked['user_age']]$ masked.head()
import datetime $ now = datetime.datetime.now()$ print now
%%time$ nodeInDegreeDict = network_friends.in_degree()$ nodeOutDegreeDict = network_friends.out_degree()
marsweath_url = 'https://twitter.com/marswxreport?lang=en'
rides_fare_average_max = rides_analysis["Average Fare"].max()$ rides_fare_average_max
cityID = 'd5dbaf62e7106dc4'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jacksonville.append(tweet) 
df1['forcast'].head() #allthe data will be nan $
md_vals = list(tmpdf_md.loc[:,METADATA_VAL_COLS].unstack().values)$ md_vals = [s.lower() if isinstance(s, str) else s for s in md_vals]$ md_vals
new.to_csv('dates.csv')$ test.to_csv('count.csv')
price2017 = price2017.drop(['Date','Time'], axis=1)
data.userScreen.nunique()
for c in ccc:$     vwg[c] = vwg[vwg.columns[vwg.columns.str.contains(c)==True]].sum(axis=1)
tokens = reddit.title.apply(process)
who_purchased = pd.get_dummies(questions['purchased'])
Station = Base.classes.station$ Measurement = Base.classes.measurement$
Desc_active_stations = session.query(Measurement.station, func.count(Measurement.prcp)).\$                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).all()$ Desc_active_stations
df['id'].value_counts()    # this will help us to see if there is repetition on the titles$
CM_s = pendulum.datetime(2017, 11, 27, 0, 0, 0, tzinfo='US/Eastern') $ CM_e = pendulum.datetime(2017, 11, 27, 23, 59, 59, tzinfo='US/Eastern') $ CM = tweets[(tweets['time_eastern'] >= CM_s) & (tweets['time_eastern'] <= CM_e)]
df1 = df1[df1['Title'].str.contains(blacklist) == False]$ df1.shape
new_table=original_merged_table.groupby(["city","type","driver_count"]).size().reset_index().rename(columns={0:"Number of ride"})$ new_table.head()
sns.set(style="white", color_codes=True)$ sns_plot = sns.jointplot(x = source_pos_df['Position_x'], y = source_pos_df['Position_y'], kind='kde', color="skyblue")$ sns_plot.savefig('kde_source.png')
soup = bs(response.text, 'html.parser')
print('input dim = {}'.format(input_image.ndim))$ print('input shape = {}'.format(input_image.shape))
df4['Date'] = pd.to_datetime(df4['Date'])$ df4.dtypes
train,test = iris.split_frame([0.8])
tlen = pd.Series(data=data['len'].values, index=data['Date'])
techDetails.to_pickle('PC_World_Latops_Scrape.pickle')$ techDetails.to_csv('PC_World_Latops_Scrape.csv')
grouped_by_year_DRG.groupby(level=0)$
json_dict=req.json()$ json_dict
df.isnull().sum()
data.resample('M').mean().head()  $
print(data.program_code.value_counts()[:3])
s.asfreq('8BM')
print("Min " + str(tm['created_at'].min()) + " Max " + str(tm['created_at'].max()))
pivoted.shape
df_result = df[df.State.isin(['Failed', 'Successful'])]$ pd.crosstab(df_result.launched_year, df_result.State)
file = open("datasets/git_log_excerpt.csv", "r")$ print(file.read())
data.isnull().sum()
md_keys = ['   ' + s + ':' for s in METADATA_KEYS]$ md_idx = tmpdf_md.index[tmpdf_md[tmpdf_md.isin(md_keys)].notnull().any(axis=1)].tolist()$ md_idx
officers = pd.read_csv('data/outputs/active_officers.csv')$ officers.company_number = officers.company_number.astype(str)
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_15min'], axis=1)$ dfY = data['corrCost']
df['Size'].sum()/(1024*1024*1024)
data['timestamp'] = pd.to_datetime(data['Date'], unit='s')
missing_millesime = intervention_train.MILLESIME.isnull()$ intervention_train.loc[missing_millesime, 'MILLESIME'] = intervention_train.loc[missing_millesime, 'CRE_DATE_GZL'].apply(lambda x: x.year)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/mydata.json"$ df = pd.read_json(path)$ df.head(5)
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
grouped_df = news_sentiment_df.groupby('News_Source')$ grouped_compound = grouped_df['compound'].mean()$ grouped_compound
future_dates = prophet_model.make_future_dataframe(periods=forecast_steps, freq='W')$ future_dates.tail(3)
pd.options.display.max_rows$ pd.set_option('display.max_colwidth', -1)$ new_df = df_filtered_by_RT.assign(clean_text = df_clean_text, extr_emojis = df_extr_emojis)$
y_pred = y_pred.argmax(axis=1)
usgs_temp_cols=hourly_dat.columns[hourly_dat.columns.str.contains("USGS")]$ hourly_dat[usgs_temp_cols]['2016':'2018'].plot()
np.r_[np.random.random(5), np.random.random(5)]
results.to_csv(path_or_buf=path + '/NFL_Fantasy_Search_2016_PreSeason.csv')
qt_ind_ARR_closed.applymap(lambda x: "{0:,.2f}".format(x))
calc_mean_auc(product_train, product_users_altered, $               [sparse.csr_matrix(item_vecs), sparse.csr_matrix(user_vecs.T)], product_test)$
for active_add_date in daterange:$     active_add_rows = active_df[active_df['start_date']==active_add_date]$     cohort_active_activated_df.loc[active_add_date,active_add_date:] = len(active_add_rows) 
stations_des=session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()$ stations_des
stock_data.index
building_pa_prc_shrink[building_pa_prc_shrink.columns[0:5]].head(10)
hawaii_station_df = pd.read_csv(r"\Users\Josue\Desktop\\hawaii_stations.csv")$
data_by_date_df["date"] = data_by_date_df["date"].dt.strftime("%Y-%m")
min(ORDER_BPAIR_POSTGEN['order_number'].astype(int))
openmc_geometry = openmc.Geometry(root_universe)$ openmc_geometry.export_to_xml()
options_frame.head()
master_df['Month of Year']=pd.to_datetime(master_df['startday']).dt.month.astype(str)
df['launched'] = pd.to_datetime(df.launched)$ df['deadline'] = pd.to_datetime(df.deadline)$
engine = create_engine("sqlite:///hawaii.sqlite")
dfjoined = dfrecent.merge(dfcounts, how = 'inner', on = ['created_date'])
plt.scatter(tfav,tret, marker='.', color='red')
coming_next_reason = coming_next_reason.drop(['[', ']'], axis=1)$ coming_next_reason = coming_next_reason.drop(coming_next_reason.columns[0], axis=1)
tweets ['apple'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'apple' in x else False)$ tweets ['samsung'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'samsung' in x else False)
most_active_df = most_active_df.loc[most_active_df['date'] > year_ago]$ most_active_df.plot.bar
X = [html.unescape(string) for string in X]
lgb.plot_importance(lgb1, max_num_features=30)$ plt.show()$
crimes.columns = crimes.columns.str.replace('__', '_')$ crimes.columns
print(type(df.groupby("grade").size()))  # as series (series values is count of each category)$ df.groupby("grade").size()
exiftool -csv -createdate -modifydate cisuabg7/cisuabg7_cycle1.MP4 cisuabg7/cisuabg7_cycle4.MP4 cisuabg7/cisuabg7_cycle6.MP4 > cisuabg7.csv
df = df.loc[df['d_first_name'] != 'Unknown']$ df = df.loc[df['d_birth_date'] != 'nan']$ df.set_value(df.index[11650], 'd_birth_date', '04/26/1940')
results = soup.find_all('div', class_="top-matter")$ results
news_df = pd.DataFrame(average_sentiment_list).set_index("User").round(3)$ news_df.head()
points.name="WorldCup"$ points.index.name="Previous Points"$ points$
joined.dtypes.filter(items=['Frequency_score'])
df_c = df.query('landing_page!= "old_page"')  $ df_cb = df_c.query('group == "control"')$ df_cb.nunique()$
mars_facts_df.columns = ['Characteristic','Data']$ mars_df_table = mars_facts_df.set_index("Characteristic")$ mars_df_table
from pyspark.sql.functions import col$
[tweet.lang for tweet in tweet_list]
auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)$ api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
texts = df[df['section_text'].str.contains('fees')][['filename','section_text']].values$ len(texts)
store_items.dropna(axis = 0)
df_valid = pd.read_csv('/home/bmcfee/data/cc_tracks.csv.gz', usecols=[0], nrows=1000000)['track_id']$ df_valid = df_valid.apply(lambda x: '{:06d}'.format(x))
job_requirements = pd.DataFrame(requirements)$ job_requirements$
QUIDS_wide.drop(labels=["qstot_12","qstot_14"], axis = 1, inplace=True)
keywords = ['earthquake', 'quake', 'magnitude', 'epicenter', 'magnitude', 'aftershock']$ search_results = api.search(q=' OR '.join(keywords), count=100)
KKK = str(time.strftime("%m-%d-%y")) + "-Output.csv"$ df.to_csv("output/" + KKK, encoding="utf-8")
print (r.json())
tallies_file = openmc.Tallies()$ mgxs_lib.add_to_tallies_file(tallies_file, merge=True)
x_min_max = pd.DataFrame({'x': [df['x'].min(), df['x'].max()]})$ x_min_max
year_with_most_commits = ... 
data['Close'].pct_change(periods=2).max()
df['text_extended'] = df.index1[898:1061].apply(tweet_extend)$
df = df[[target_column]].copy()$ base_col = 't'$ df.rename(columns={target_column: base_col}, inplace=True)
model_uid = client.repository.get_model_uid(saved_model_details)$ print("Saved model uid: " + model_uid)
label_and_pred = dtModel.transform(testData).select('label', 'prediction')$ label_and_pred.rdd.zipWithIndex().countByKey()$
building_pa_prc_fix_issued=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date','issued_date'])
data=requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-07-01&end_date=2018-07-31&api_key=%s" %API_KEY)
Z = np.arange(1,15,dtype=np.uint32)$ R = stride_tricks.as_strided(Z,(11,4),(4,4))$ print(R)
len(df_events), len(df_events.group_id.unique())
print('Largest daily range of 2017: €' + str(np.max(range_vec).round(2)))
not_numbers = data_read.genre_ids.astype(str).apply(lambda x: x.isnumeric()) == False$ data_read["genre_ids"][not_numbers.values].sample(10)
npath = save_filepath + '/uva_hpc/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
conf = SparkConf().setAll([('spark.executor.memory', '6g'), ('spark.executor.cores', '6'), ('spark.cores.max', '6'), ('spark.sql.session.timeZone', 'UTC')])$ sc = SparkContext("local", "ib", conf=conf)
secondary_temp_dat=dat[secondary_temp_columns].copy()$ secondary_temps_exist= not secondary_temp_dat.dropna(how='all').empty$
t1[t1['retweeted_status_id'].notnull()==True]
vacancies['created'] = vacancies['created'].apply(lambda x: dateutil.parser.parse(x))
logit_countries = sm.Logit(newset['converted'], $                            newset[['country_UK', 'country_US', 'intercept']])$ result_new = logit_countries.fit()
df_emoji_rows = new_df[new_df.extr_emojis != '']$
NS_active_2015_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2015-06-01']$ NS_active_2016_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2016-06-01']$ NS_active_2017_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2017-06-01']
df2[df2.duplicated(['user_id'], keep=False)]
my_tweet_df.count()
pd.Series({'a': 42, 'b': 13, 'c': 2})
DC_features = pd.read_csv('Historical_Work_Sunlight_Data.csv',index_col=0,parse_dates=True)
!tar -xzvf cudnn-8.0-linux-x64-v7.1.tgz
df_new['country'].value_counts()
train = pd.read_csv("../input/web-traffic-time-series-forecasting/train_1.csv")$ keys = pd.read_csv("../input/web-traffic-time-series-forecasting/key_1.csv")$ ss = pd.read_csv("../input/web-traffic-time-series-forecasting/sample_submission_1.csv")
bd.index
results = pd.read_csv('player_stats/{}_results.csv'.format(team_accronym), parse_dates=['Date'])$ results_postseason = pd.read_csv('player_stats/{}_results_postseason.csv'.format(team_accronym), parse_dates=['Date'])
prec_long_df.describe()
from pandas.plotting import autocorrelation_plot$ autocorrelation_plot(CH_electric['Total_Demand_KW'])
a = np.arange(25).reshape(5,5)$
y.start_time
score = model.evaluate(test_train_vecs_w2v, y_test_train, batch_size=128, verbose=2)$ print (score[1])
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)$ core_samples_mask[db.core_sample_indices_] = True$ core_samples_mask
raw_freeview_df, raw_fix_count_df = condition_df.get_condition_df(data=(etsamples_grid,etmsgs_grid,etevents_grid), condition='FREEVIEW')
contextFreeList = [i/10 for i in range(1,3,step = 0.5)]$ for i in contextFreeList:$     print(i,tuning(w_contextFree = i))
bar_plot = sns.barplot(x="News Organization", y="Mean Compound Score", hue="News Organization", $                        data=aggrigate_results_df, palette=news_colors)$ bar_plot.axes.set_title(f'Overall Media Sentiment, date: {today}', fontsize=14)$
mars_df.rename(columns={0: 'description', 1: 'value'}, inplace=True)$ mars_df.set_index('description', inplace=True)$ print(mars_df)
df.loc[dates[0]]
dfn['goal_log'] = np.log10(dfn['goal'].values)
import pandas as pd$ import numpy as np$ import matplotlib.pyplot as plt
exec(open ('../../config.py').read ()) # Load file where API keys are stored$ API_KEY = QUANDL_API_KEY
dictionary = corpora.Dictionary(text_list)$ dictionary.save('dictionary.dict') $ print dictionary
faa_data_phil_pandas = faa_data_pandas[faa_data_pandas['AIRPORT_ID'] == "KPHL"]$ print(faa_data_phil_pandas.shape)$ faa_data_phil_pandas.head()
red.shape
mean = np.mean(df.len)$ print('Tweet length average is: {}\n'.format(mean))
ward_df = pd.DataFrame(wards)
timeEnd = ("2017-06-07 23:59:59")$ timeStart = ("2015-06-09 00:00:00")$ timeNow = datetime.strptime(timeNow, "%Y-%m-%d %H:%M:%S")
sess = tf.Session()$ K.set_session(sess)
tmp = df.corr(method = 'pearson')[['meantempm']]$
data.Rateio_Sena.sum()/data.Acumulado.value_counts().NÃO
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S.executable = excutable +'/summa.exe'
driver.find_element_by_xpath('//*[@id="body"]/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[1]/b/font/a').click()
df = df.merge(pd.get_dummies(df['Date'].dt.strftime('%A')),left_index=True,right_index=True,how='left')$ print ('After Weekday',df.shape)$
measure_avg_prcp_year_df.set_index('Date',  inplace=True)$ measure_avg_prcp_year_df.head()$
df['datetime'] = df.index.values
len([premieScn for premieScn in SCN_BDAY_qthis.scn_age if premieScn < 0])/SCN_BDAY_qthis.scn_age.count()
style_bw.head(5)
data = pd.read_json('UCSD_records.json', orient = 'columns')
final_df = diff_df.merge(final_sentiment_df, how = "outer")$ final_df
measurements = "Resources/data/hawaii_measurements.csv"
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-06-25&end_date=2018-06-25&api_key={}'.format(API_KEY))
unique_words_sk = set(words_sk)$ corpus_tweets_streamed_keyword.append(('unique words', len(unique_words_sk))) # update corpus comparison$ print('Number of unique terms: ', len(unique_words_sk))
driver = selenium.webdriver.Safari() # This command opens a window in Safari$ driver.get('https://www.boxofficemojo.com')
analyser.polarity_scores(twitter_datadf['text'][1])$
datetime.now().time()
df = pd.read_csv("nyhto_twitter_edited.csv")$ df2018 = pd.read_csv("nyhto_twitter_edited2018.csv")
qs = qs.join(features, how="inner", rsuffix="_r")$ qs.head()
data_issues.columns
df1.tail(36)
df_plot=pd.DataFrame(df_recommended_menues['dates'].value_counts())$ df_plot.head()
reddit = praw.Reddit(client_id=keyVars[0], client_secret=keyVars[1], password=keyVars[2],$                      user_agent=keyVars[3], username=keyVars[4])
df_l2.loc[df_l2["CustID"].isin([customer])]
goodreads_users_df.replace('None', np.nan, inplace=True)
QUIDS_wide.dropna(subset =["qstot_0"], axis =0, inplace=True)
store_items.fillna(0)
results['HydrologicEvent'].value_counts()
y_size = fire_size['SIZECLASS']$ x_climate = fire_size.loc[:, 'glon':'rhum_perc_lag12']
df.ix[df.injured > 0, 'injured'] = 1 
spyder_etf = get_pricing(dwld_key, start_date.strftime(date_fmt))$ spyder_etf.name = dwld_key + ' ETF Index'$ s_etf = (spyder_etf.pct_change() + 1).cumprod()
cols = vip_df.columns.tolist()$ cols = cols[-1:] + cols[:-1]$ vip_df = vip_df[cols]
df['y'].plot.box(notch=True)
tweet_df.sort_values(by="date", ascending=True)$
list(pd.read_csv(projFile, nrows=1).columns), list(pd.read_csv(schedFile, nrows=1).columns), list(pd.read_csv(budFile, nrows=1).columns)
len(purchase_history.product_id.unique())
tmp_one.find(class_ = 'sammyListing').get_text()
del merged_portfolio_sp_latest['Date']$ merged_portfolio_sp_latest.rename(columns={'Adj Close': 'SP 500 Latest Close'}, inplace=True)$ merged_portfolio_sp_latest.head()
user_df = stories.submitter_user.apply(pd.Series)$ user_df.head()
from bs4 import BeautifulSoup$ import pandas as pd$ from urllib.request import urlopen
combined_city_df = pd.merge(city_data_df, ride_data_df,$                                  how='outer', on='city')$ combined_city_df.head(5)
df['log_AAPL']=np.log(df['NASDAQ.AAPL'])
new_array = np.concatenate((training_active_listing_dummy,training_pending_ratio),axis=1)
pd.melt(df, id_vars=['A'], value_vars=['B'])
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ r = requests.get(url)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\metrics.sas7bdat"$ df = pd.read_sas(path)$ df.head(5)
df.to_csv(r'C:\Users\LW130003\cgv.csv', index=False)
all_df = train_df.append(test_df)$
extract_deduped_with_elms_v2.shape, extract_deduped_with_elms.shape
avg_da = pd.DataFrame('AVG_DA':raw_df.groupby('Date')['DA-price'].mean().values)$ avg_da
df_dropVivo_detail_download_total_time = df_detail_download_total_time[~(df_detail_download_total_time['source'] == 'vivo_game')]$ df_dropVivo_detail_download_total_time.head(1)
stat_info_merge = pd.concat([stat_info_city[1], stat_info_st[[0,1]]], axis=1)
train, valid, test = covtype_df.split_frame([0.6, 0.2], seed=1234)$ covtype_X = covtype_df.col_names[:-1]     #last column is Cover_Type, our desired response variable $ covtype_y = covtype_df.col_names[-1]    
model=LogisticRegression(penalty='l2', C=1)$ model.fit(X_train_mean, y_train_mean)$ preds = model.predict_proba(X_test_mean)
params = {"objective": "reg:linear", "booster":"gblinear"}$ gbm = xgboost.train(dtrain=T_train_xgb, params=params)
np.random.seed(123456)$ bymin = pd.Series(np.random.randn(24*60*90),pd.date_range('2014-08-01','2014-10-29 23:59',freq='T'))$ bymin
my_data.shape$
crimes.PRIMARY_DESCRIPTION.value_counts()
print(r.json()['dataset_data']['column_names'])$ print(r.json()['dataset_data']['data'][0])$ print(r.json()['dataset_data']['data'][-1])
plt.pie(count_driver, labels=type_driver,explode=explode, colors=colors,$         autopct="%1.1f%%", shadow=False, startangle=140)$
df.head()$
results['HydrologicEvent'].unique()
metadata_df_all['type'] = [t.lower() for t in metadata_df_all['type']]$ metadata_df_all['bill_id'] = metadata_df_all['type'] + metadata_df_all['number'].map(int).map(str)$ metadata_df_all = metadata_df_all[['accessids', 'bill_id']]
cityID = 'a84b808ce3f11719'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Omaha.append(tweet) 
df_train = pd.merge(df_train, df_items, on='item_nbr', how='left')
from collections import Counter$ c = Counter([int(stats.coleman_liau(x['cdescr'])) for x in df.to_dict(orient='records')])$
raw_scores = df.rating_score[df.rating_score.notnull()]
merged = pd.DataFrame.merge(mojog_df, youtube_df,on='movie_name', how = 'inner')$ merged.head()
precip_data_df1=precip_data_df.copy()$ precip_data_df1.reset_index(inplace=True,drop=False)$ precip_data_df1.head(5)
comments = df.loc[:,'comment_body']$ print(comments[0])$ type(comments)
eth = pd.read_csv("Ether-chart.csv", sep=',')$ eth['date'] = ' '$ eth.info()$
y.name
data_folder = '../input/export_Feb_8_2018'$ ods = SlackLoader(data_folder)
free_data.groupby('age_cat').mean()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/EON_X.json?api_key='+API_KEY+'&start_date=2018-06-28&end_date=2018-06-28')
dfCr = pd.read_csv('data/CC2017.csv')$ dfCr = dfCr[dfCr['Transaction']=='CREDIT'] $ dfCR= dfCr[dfCr['Memo'].notnull()] # remove null entries
!arvados-cwl-runner --name "Encode Blood Types" encode.cwl --arr ./npy_data/train_data.npy --script just_encode.py
df.injured.value_counts()
xml_in_sample.shape
vocab = list(model.wv.vocab.keys())$ vocab[:25]
Daily_Price.tail()$
url = "https://www.dropbox.com/s/he5tvdxriqj9s9b/parsed_tweets.csv?dl=1"$ location = './'$ download_file(url, location)
Aussie_result=pd.concat([sydney_aussie,brisbane_aussie,melbourne_aussie])$
data = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY)
tweets_df = pd.read_csv(tweets_filename, $                         converters={'tweet_place': extract_country_code, $                                     'tweet_source': extract_tweet_source})
store_items = store_items.rename(index={'store 3': 'last store'})$ store_items
dr_new_8_to_16wk_arimax = dr_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ dr_new_8_to_16wk_arimax.index = dr_new_8_to_16wk_arimax.index.date
exchanges = ccw.get_exchanges_list()$ EXCHANGE_DB = pd.DataFrame.from_dict(exchanges, orient='index')$
print("Probability of user converting:", df2.converted.mean())
date_precipitation=(session.query(Measurement.date,Measurement.prcp)$ .filter(Measurement.date>=start_date)$ .order_by(Measurement.date.desc()).all())
lr = LogisticRegression(C=3.0, random_state=42)
df.describe()
df['timestamp'].dt
weather = pd.read_csv("weather.csv", parse_dates={"date" : ['Date']})$ weather = weather.drop(['HDD', 'CDD'], axis=1)
np.random.seed(123456)$ ts = Series([1,2,2.5,1.5,0.5],pd.date_range('2014-08-01',periods=5))$ ts
from bson.objectid import ObjectId$ def get(post_id):$     document = client.db.collection.find_one({'_id': ObjectId(post_id)})
small_movies_file = os.path.join(dataset, 'ml-latest-small', 'movies.csv')$ small_movies_raw_data, small_movies_raw_data_header = read_file(small_movies_file)$
lag_list = list(df.columns.values)
df_ratings.drop('video_watched_type', axis=1, inplace=True)$ df_ratings.drop('rating_date_creation', axis=1, inplace=True)$ df_ratings
((df-df_from_csv)**2 < 1e-25).all()
data_dict = json.loads(data.text)
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig8_right_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
f.groupby('Price').mean()
results_1dRichards, output_R = S_1dRichards.execute(run_suffix="1dRichards_hs", run_option = 'local')
CON=pd.read_csv('C:/Users/mjc341/Desktop/UMAN 1507 Monthly INQ summary Report/Interactions.Opportunity.Term.csv',skipfooter=5,encoding='latin-1',engine ='python')$
check_rhum = rhum_fine[1]$ sns.heatmap(check_rhum)
pd.set_option('display.max_colwidth',100)
old_page_converted = np.random.choice([1, 0], size=n_old,p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
articles['tokens'] = articles['tokens'].map(lambda s: [w for w in s if len(w)>1])
with open('DB_API_key.txt','r') as file:$     API_Key =  file.readline()
pd.crosstab(df_result.launched_year, df_result.State).plot.bar()
conn.get_tables()
S_1dRichards.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
from sklearn.linear_model import LogisticRegression
df_release = df_release.dropna(axis=0, subset=['actual'])$ df_release.shape
df_inventory_santaclara =df_santaclara.transpose()                     ##Transpose $ df_inventory_santaclara.reset_index(level=0,inplace=True)$ df_inventory_santaclara.columns=['Date','Units']
stops.to_csv('stops_all_ireland.csv', sep=';')
url_test = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \$       "&start_date=2017-01-01&end_date=2017-01-02&api_key=" + API_KEY$ req_test = requests.get(url_test)
n_user_days = pax_raw[['seqn', 'paxday']].drop_duplicates().groupby('seqn').size()
artistDF[artistDF.artistID==1000010].show()$ artistDF[artistDF.artistID==2082323].show()
s = pd.Series([1, 2, 3, 4, 5, 6])$ s
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)
output_SVM = model_SVM.predict(test[:, 1:5])$ rowID_SVM = [TEST.rowID for TEST in test_data.itertuples()]$ result_df_SVM = pandas.DataFrame({"rowID": rowID,"cOPN": list(output_SVM)})
df['device_class'] = df.device_type.apply(lambda d: classify_device_type(d))$ df.drop(['device_family', 'device_id', 'device_model', 'device_type'], axis=1, inplace=True)
tdf[tdf['smoker'] == 'Yes'].describe()
precip_data_df2=precip_data_df1[["date","Precipitation"]]$ precip_data_df2.describe()
fig,ax=plt.subplots(1,2,figsize=(15,3))$ ax[0].boxplot(joined['CompetitionOpenSinceYear'],vert=False)$ ax[1].boxplot(joined['CompetitionDaysOpen'],vert=False)
results = sm.OLS(gdp_cons_df.Delta_C1[:140], gdp_cons_df.Delta_Y1[:140]).fit()$ print(results.summary())
cursor.execute('SELECT * FROM fib')$ print(cursor.fetchall())
free_data.dtypes
hit_tracker_grouped_df.to_csv("Desktop/Project-2/hit_tracker.csv", index=False, header=True)
store_items.pop('glasses')$ store_items
plt = df2.plot(legend=False)$ plt.set_ylabel("complaint count")$ plt.set_xlabel("DOTCE predicted grade school level (high school 9-12)");
crimes.head()
daily_df = twitter_daily_df.join(stock_df, ["Day","Company"]).orderBy('Day','Company')
df = pd.read_json('moscow_vacs_2018-03-04.json')
html = browser.html$ soup = BeautifulSoup(html, 'html.parser')
for col_x in np.arange(len(col_list)):$     df.rename(columns={col_list[col_x]: rename_list[col_x]}, inplace=True)
dem = dem[dem["subjectkey"].isin(incl_Ss)]
from gensim import models$ models = models.Word2Vec.load_word2vec_format((r'C:\Users\User\Desktop\670\7_Topic_Modeling\data\text8.csv'))
import pprint$ pprint.pprint(vars(example_tweets[0]))
df6.mean() # mean of lunch values$
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_10min'], axis=1)$ dfY = data['corrCost']
gDate_content_count = gDateProject_content.groupby(level=0)$ gDate_content_count = gDate_content_count.sum()$
model_NB = MultinomialNB()$ model_NB.fit(count_vectorized, df_train.author)
r = requests.get ('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=4zX7CA93VkKSEygsbSLv')$ print (r.json()['dataset']['data'][0])
import matplotlib.pyplot as plt$ plt.matshow(ibm_hr_target_small.select("*").toPandas().corr())
AFX_dict = dict(r.json())
pd.date_range('12/31/2017', '01/01/2018', freq='H') $
combined = dfs[0].append(dfs[1], ignore_index=True)$ combined = combined.append(dfs[2], ignore_index=True)$ combined.head()
data_AFX_X.describe()['Traded Volume']
plot_compare_generator(['mention_count', 'hashtag_count'],"Comparing the two counts" ,DummyDataframe.index, DummyDataframe, intervalValue= 1, saveImage=True, fileName="compare")
X_train, X_test, y_train, y_test = train_test_split(features,classification_open,test_size=0.2)
data = r.json()$ print(data)$
results = nfl.interest_over_time()$
re.match('^\w+@[a-zA-Z_]+?\.[a-zA-Z]{2,3}$', 'abc@gmail.com').group()
backers = databackers.pivot_table(index = 'backers', columns='successful',aggfunc=np.size)$ backers = backers.rename(columns= {1: 'Successful', 0:'Failed'})$ backers[backers['Successful'] == backers['Successful'].max()]
trump.dtypes
merged = price2017.merge(typesub2017,how='inner',left_on='DateTime',right_on='DateTime')
df.dealowner.unique()
from pyspark.sql.functions import isnan, when, count, col$ ibm_hr.select([count(when(isnan(c), c)).alias(c) for c in ibm_hr.columns]).show()
titles_list = temp_df2['titles'].tolist()
back2sent = word_vecs.apply(lambda x: ' '.join(x))$ transform = TfidfVectorizer(lowercase=False, max_df=.1, max_features=2000, ngram_range=(1,3))$ tf_idf_matrix = transform.fit_transform(back2sent.values)
import tensorflow as tf$ tf.test.gpu_device_name()
%%writefile trainer/__init__.py$
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'lxml')
weights['0.encoder.weight'] = T(new_w)$ weights['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))$ weights['1.decoder.weight'] = T(np.copy(new_w))
big_data_station = big_data.groupby('STATION')$
screen_name="SKinnock"$ data.to_csv('%s_likes_rts.csv' % screen_name, sep=',', encoding='utf-8')$
country_dummies=pd.get_dummies(df_new['country'])$ df_new=df_new.join(country_dummies)$ df_new.head()
for route in routes:$     print('mode: {} | id: {} | route name: {}'.format(route['mode'], route['id'], route['longName']))
lossprob2 = fe.bs.smallsample_loss(2560, poparr2, yearly=256, repeat=500, level=0.90, inprice=1.0)$
ca_all_path = cwd + '\\LeadGen\\Ad hoc\\SubID\\CA_SubID_from_python.csv'$ ca_all.to_csv(ca_all_path, index=False)
mod_model.xls2model(new_version='new', annotation=None)$ scenario = mod_model.model2db()$ scenario.solve(model='MESSAGE', case='GHD_hospital')
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S_1dRichards.executable = excutable +'/summa.exe'
dict_urls['web']['project'].split('?')[0]
plt.bar(x_axis, bar_Compound, color=["gold","blue","green","red","lightblue"], align="edge")
from sklearn.model_selection import train_test_split$ from keras.utils import np_utils$ train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.2, stratify = y)#, stratify = y)$
cursor.fetchall()
jeff = Customer('Jeff Knupp', 1000.0)    #jeff is the object, which is an iinstance of the *Customer* class
drop_num.append(19)
df_new['CA_ab_page'] = df_new['CA'] * df_new['ab_page']$ df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page']$ df_new.head()
tweets['created_at'] = pd.to_datetime(tweets['created_at'])$ tweets.dtypes
data = np.zeros(4, dtype={'names':('name', 'age', 'weight'),$                           'formats':('U10', 'i4', 'f8')})$ print(data.dtype)
importances=model_rf_19_S.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
outfile = os.path.join("Resource_CSVs","Main_data_Likes.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
ts.get_debtpaying_data(2018,1)
b = R17df.rename({'Create_Date': 'Count-2017'}, axis = 'columns')$
from sklearn.ensemble import RandomForestClassifier$ rf_clf=RandomForestClassifier(max_depth=None)$ print(rf_clf)
review_df.date = pd.to_datetime(review_df.date)$ review_df=review_df[review_df['date']>='2017-01-01']
data['weekday'] = data.index.weekday
bday = datetime(1986, 3, 6).toordinal()$ now = datetime.now().toordinal()$ now - bday
import tensorflow as tf$ from tensorflow.contrib.tensorboard.plugins import projector
tobs_df = pd.DataFrame(Waihee_in_last_year, columns=['Temperature'])$ tobs_df.head(11)
index_vector = df.source == 'GRCh38'$ gdf = df[index_vector]$ gdf.shape
ice = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with ice.open('inpt-forecasting/Inpatient Census extract - PHSEDW 71118.csv') as ice_f:$   ice_df = pd.read_csv(ice_f)
data_ps['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_ps['Tweets'] ])$ display(data_ps.head)
print(plate_appearances.shape)$ plate_appearances = plate_appearances.dropna()$ print(plate_appearances.shape)
irisDF = SpSession.createDataFrame(irisMap)$ irisDF.show()$
last_12_precip = session.query(Measurement.date, Measurement.prcp).\$ filter(Measurement.date >= '2016-08-24').filter(Measurement.date <= '2017-08-23').order_by(Measurement.date).all()
df2 = df[df.fav >= 20]$ len(df2)
df.to_sql('places', conn)
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])
df = demo.make_df(35175,35180)$ df.head()
daily_change = [daily[2]-daily[3] for daily in json_data['dataset_data']['data'] if daily[2:3] != None]$ largest_change = str(round(max(daily_change), 2))$ print('The largest change in any one day was $' + largest_change + ' in 2017.')$
userByCountry_df  = youtube_df[youtube_df["channel_title"].isin(channel_namesC)]
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
data_2018= data_2018.set_index('time')
time_chart = vincent.Line(school_per_minute)$ time_chart.axis_titles(x='Time', y='Hashtag frequencies')$ time_chart.display()
yc_new = yc_depart.merge(destinationZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')$ yc_new.shape
soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())$
transactions = sql_context.read.json('data/transactions.ndjson')
%%bash$ cut -f 1,4-6 PPMI/logASYN/PHENO0_0.txt | sed 's/_/\t/g' | sed 's/(\//\t/g' | sed 's/)//g' | head
print "word    count"$ print "----------------------------\n"$ !hdfs dfs -cat /user/koza/hw3/3.2/issues/wordCount_part3/* | head -n 10
trump.index=pd.DatetimeIndex(trump['created_at'])$ trump.drop('created_at', axis = 1, inplace =True)$ trump.shape
calls_nocontact_simp = calls_nocontact.drop(columns=['ticket_id', 'issue_description', 'city', 'state', 'location', 'geom'])$ calls_nocontact_simp.head()
print(type(data.points))$ data[['points']]$
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
sp = openmc.StatePoint('statepoint.20.h5')
S_1dRichards.forcing_list.filename
posts = pd.DataFrame(columns = ['id', 'time', 'score', 'url', 'title'])$ comments = pd.DataFrame(columns = ['parent', 'text'])
data.count()
df = pd.read_csv('end-part2_df.csv').set_index('date')$ df.describe().T
max_sharpe_port_optim = results_frame_optim.iloc[results_frame_optim['Sharpe'].idxmax()]$ min_vol_port_optim = results_frame_optim.iloc[results_frame_optim['SD'].idxmin()]
url_img = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url_img)
trainData = trainData.filter(lambda line : line[1] != 1034635)
crime = crime[crime.Sex != 'U']
client = boto3.client('s3')$ with open(datafile, 'wb') as f:$     client.download_fileobj(bucket, prefix, f)
Base.classes.keys()
twelve_months_prcp.head()
import json$ with open('data_comparision/heremap.json', 'w') as outfile:$     json.dump(j_data, outfile)
print(rhum_nc)$ for v in rhum_nc.variables:$     print(rhum_nc.variables[v])
Latest_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ start_date = pd.to_datetime(Latest_date[0]).date()- timedelta(days=365)$ print(start_date)
contractor_clean[contractor_clean.city.isnull()]
classes = ['actif', 'churn', 'lost']$ conf_matrix = confusion_matrix(y_true, y_pred)$ plot_confusion_matrix(conf_matrix, classes, 'True label', 'Predicted label', title='Confusion matrix', cmap=plt.cm.Blues)
df = pd.read_pickle('all-RSS.pkl')
challange_1.shape
df_users =  pd.read_sql(SQL, db)$ print(df_users.head())$ print(df_users.tail())
offseason18 = ALL[(ALL.index > '2018-02-07')] # Finally, this says that everything after this past Superbowl is part of$
trump_df.text = trump_df.text.str.lstrip("b'")
import matplotlib.pyplot as plt$ import pandas as pd$ import numpy as np
data.info()
x=[0,1,2,3,4,5]$ network_simulation[network_simulation.generations.isin(x)]$
df2[df2['group']=='treatment']['converted'].mean()
brewery_bw.head(5)
seq2seq_Model.save('seq2seq_model_tutorial.h5')
for urlTuple in otherPAgeURLS[:3]:$     meritParagraphsDF = meritParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ meritParagraphsDF
def normalizePrice(price):$     return int(price.replace(',',''))$ df['price'].map(lambda price: int(price.replace(',','')) ).head()
got_data = pickle.load(file=open('got_100_data.data','rb'))
hourly_df['Price_Change'].value_counts()
vol = vol.replace(0, 1)$ vol.describe()
df2.rename(columns = {'treatment': 'ab_page'}, inplace=True)$ df2.drop('control', axis=1, inplace=True)$ df2.head()
pm_data = pd.concat([pm_data,$                      pd.DataFrame(pm_data.groupby('unit_number').cumcount(), columns = ['obs_count'])]$                     , axis = 1)
td_wdth = td_norm * 1.5$ td_alph = td_alpha$ td[:] = td[:].astype(int)
cityID = '249bc600a1b6bb6a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Aurora.append(tweet) 
new_df = df.fillna(method = 'bfill', axis = 'columns')$ new_df
conn.commit()
btc = pd.read_csv('/home/rkopeinig/workspace/Time-Series-Analysis/data/btc.csv')$ btc['date'] = pd.to_datetime(btc['date'])$ btc = btc.set_index('date')
df.source.value_counts()
results.head()
average_range = df['range'].mean()
df_movies.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/movies.csv', sep=',', encoding='utf-8', header=True)
ftr_imp_rf=zip(features,trained_model_RF.feature_importances_)$ for values in ftr_imp_rf:$     print(values)
df.median() + 1.57 * (df.quantile(.75) - df.quantile(.25))/np.sqrt(df.count())
MetaMetaclass.__call__(MetaClass,'Example', (), {})$
dta.query("(risk == 'Risk 1 (High)') | (risk == 'Risk 1 (Medium)')").head()
top_10_authors = git_log.groupby("author").count().sort_values(by="timestamp", ascending=False).head(10)$ top_10_authors
with open('united_list_fresh_lower.pickle', 'rb') as ff:$     united_list_fresh_lower = pickle.load(ff)
regr.fit(X_train, y_train)
gmap = gmplot.GoogleMapPlotter(28.68, 77.13, 13)
rf = pickle.load(open('../data/model_data/size_mod.sav', 'rb'))
df_sched = df_sched[~(df_sched.Initiation < 0)].copy()
cursor.execute("SHOW TABLES")$ cursor.fetchall()
year9 = driver.find_elements_by_class_name('yr-button')[8]$ year9.click()
mean = np.mean(data['len'])$ print("The length's average in tweets: {}".format(mean))
max_change = [ldf['High'] - ldf['Low']]$ largest_change = np.asarray(max_change).max()$ largest_change
assert mcap_mat.shape[0] < 100
(final_rf_predictions['predict']==test['Cover_Type']).as_data_frame(use_pandas=True).mean()
df.isnull().values.any()
df_ratings.head()
df_tte['ItemDescription'].unique()
spp = pd.read_excel('input/Data.xlsm', sheet_name='43', header=11, skipfooter=8793)
pandas_small_frame = small_frame.as_data_frame()$ print(type(pandas_small_frame))$ pandas_small_frame
A = np.arange(25).reshape(5,5)$ A[[0,1]] = A[[1,0]]$ print(A)
pd.DatetimeIndex(pivoted.columns)
hawaii_measurement_df.head(10)
import pandas as pd$ cs = pd.Series(cleaned)$ by_tweeter['cleaned'] = cs
lg = pd.read_csv('awesome_go_web.txt.csv')$ lg.head()
number_of_commits = len(git_log)$ number_of_authors = len(git_log.query("author != ''").groupby('author'))$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
closing_prices = (x[4] for x in data_table if x[4] is not None)$ inter_day_changes = (abs(i - j) for i, j in itertools.combinations(closing_prices, 2))$ print('Largest Change between days: {:.2f}'.format(max(inter_day_changes)))
mydict = r.json()['dataset']$ dict(list(mydict.items())[0:10])
rain = session.query(Measurement.date, Measurement.prcp).\$     filter(Measurement.date > last_year).\$     order_by(Measurement.date).all()$
new_model = gensim.models.Word2Vec(min_count=1)  # an empty model, no training$ new_model.build_vocab(sentences)                 # can be a non-repeatable, 1-pass generator     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)                       $
print(data.petal_length.median())
row_idx = (slice(None), slice(None), slice('Bob', 'Guido'))  # all years, all visits, Bob + Guido$ health_data_row.loc[row_idx, 'HR']
dfcsv = df.loc[df['fileType'] == 'csv']$ dfcsv['fileCount'].describe()
os.chdir("E:/Data/Client1/Excel Files")$ path = os.getcwd()$ files = os.listdir(path)
test[['clean_text','user_id','predict']][test['user_id']==1895520105].shape[0]
results = soup.find_all('li', class_="content_title")
for i, row in breakfastlunchdinner.iterrows():$     breakfastlunchdinner.loc[i, 'day_sales'] = sum(row[1:]) * .002 $ breakfastlunchdinner
subject_df = latest_df[['classification_id', 'subject_ids', 'subject_data']].copy()$ subject_df['subject_data'] = subject_df.subject_data.apply(lambda x: list(json.loads(x).values())[0]['Filename'])
confidence  = clf.score(X_test, y_test)$ print("Confidence our SVR classifier is: ", confidence)
roundedDF=np.around(overallDF, decimals=2)$ roundedDF["Compounded Score"]
two_day_sample['date'] = two_day_sample.timestamp.dt.date
import builtins$ builtins.uclresearch_topic = 'GIVENCHY' # 226984 entires$ from configuration import config
for numbData in collData.distinct().collect():$     print(numbData)
log_sgm_bgp_100yr_run(L0 = 1000, E0 = 1, Delta_n = 0.02)
station_num = session.query(func.count(Station.station)).all()$ station_num
np.random.seed(123)$ my_model_q1 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label')$ my_model_q1.fit(X_train, y_train)
for key, value in sample_dic.iteritems():$     print value
import gensim$ LabeledSentence = gensim.models.doc2vec.LabeledSentence
query = "SELECT DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) as created_date, temp, wdsp, mxpsd, gust, max, min, prcp, sndp, snow_ice_pellets FROM `bigquery-public-data.noaa_gsod.gsod20*` WHERE _TABLE_SUFFIX BETWEEN '10' AND '17' AND stn = '725053' AND wban = '94728'"$ weather = read_gbq(query=query, project_id='opendataproject-180502', dialect='standard')
cityID = '7068dd9474ab6973'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Toledo.append(tweet) 
cdata.describe()
url = "https://www.reddit.com/hot.json"
summary_df = sentiments_df.groupby('Media Sources', as_index=False).mean()$ summary_df = summary_df[['Media Sources','Compound']]$ summary_df
client.experiments.list_training_runs(experiment_run_uid)
for column in ideas:$     ideas[column] = ideas[column].dt.week
merged_table = merged_table.drop(columns=['host', 'last_update', $                                           'Unnamed: 0', 'queue', 'date', $                                           'weekday', 'strtime'])
import seaborn as sns$ sns.heatmap(fpr_a)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path, sheetname = 1)$ df.head(5)
compound_df = pd.DataFrame(save_compound_list)$ compound_df = compound_df.transpose()$ compound_df.head()
df = sf_permits[['Permit Number','Current Status','Completed Date']]$ df.head()
for v in data.values():$     if 'Q3' not in v['answers']:$         v['answers']['Q3'] = ['Other']
data = pd.read_csv('fatal-police-shootings-data.csv')$ print(data.shape)$ data.info()
merged_data['payment_day'] = merged_data['last_payment_date'].dt.day$ merged_data['payment_month'] = merged_data['last_payment_date'].dt.month$ merged_data['payment_year'] = merged_data['last_payment_date'].dt.year
y[y**2 - np.sin(y) >= np. cos(y)] # math function based mask!$
nasa_url = 'https://mars.nasa.gov/news/'$ jup_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
df = df.drop("a")$ df
learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr=(32,10)) $
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']])$ result=logit.fit()
msft = pd.read_csv("../../data/msft.csv", index_col=0)$ msft.head()
df_treat = df2.query('group=="treatment"')$ y_treat = df_treat["user_id"].count()$
engine = create_engine("sqlite:///hawaii.db", echo = True)$
df2 = pd.read_csv('../output/data/expr_2/expr_2_pcs_nth_1_div_51_04-18.csv', comment='#')
df1 = pd.DataFrame({'full_text': full_text})$ combined_df = pd.concat([df_urls, df1], axis=1, join_axes=[df_urls.index])$ data = combined_df.drop(['source','type_material','headline','url'],axis=1)
le = LabelEncoder()$ le.fit(data.Block)
MFThermalCorrDF=pd.read_pickle('MFThermalCorrDF.p')
def load_data(url):$     return pd.read_json(url, orient='columns') $
print()$ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
all_slices.sort(key = lambda slice: slice['expires'])$     $ print(f"Found {len(all_slices)} slices")    
places = extractor.geo_search(query="Philippines", granularity="country")$ place_id = places[0].id$ print("Philippines id is:", place_id)
data = r.json()$
precipitation_df['date'] = pd.to_datetime(precipitation_df["date"]).dt.date$ precipitation_df.set_index(["date"], inplace = True)
test = pd.read_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep = ',', $                    encoding='utf-8', low_memory=False)
sum(contractor.city.isnull())
size_pred = rf.predict(climate_vars)
df_SP.to_pickle('DDC_data_cleaned.pkl')
df_students1 = df_students.rename(columns={'school':'school_name'})$ df_students1.head()
for col in merge_df.columns: $     print(merge_df[col].describe())
df['tax_type'] = df['tax_type'].str.replace(u'\xa0', u' ')$
file_names = []$ file_names = glob.glob('../craigslist-data/final-data-multiple-cities/*.csv')
for row in selfharmm_topic_names_df.iloc[4]:$     print(row)
with open('all_data_vectorized.dpkl', 'wb') as f:$     dpickle.dump(all_data_vectorized, f)
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
df = pd.read_csv("../Files/371SpectraDeMeoColor.csv", index_col=0)$ df = shrink_classes(df)
url = "https://mars.nasa.gov/news/"
fh_3 = FeatureHasher(num_features=uniques.iloc[0, 1], input_type='string', non_negative=True)$ %time fit3 = fh_3.fit_transform(train.device_ip)
keep_RNPA_cols = ['follow up Telepsyche', 'Follow up', 'follow', 'Returning Patient']$ RNPA_existing = RNPA_existing[RNPA_existing['ReasonForVisitName'].isin(keep_RNPA_cols)]$ pd.value_counts(RNPA_existing['ReasonForVisitName'])
data.L2.unique()
addOne = cylHPData.mapValues(lambda x: (x, 1))$ print (addOne.collect())$
ts.shift(1,DateOffset(minutes=0.5))
s.describe()
edge_types_DF = pd.read_csv('network/recurrent_network/edge_types.csv', sep = ' ')$ edge_types_DF
new_stops.to_sql('main_stops', engine, if_exists='append', index=False)
st_columns = inspector.get_columns('stations')$ for c in st_columns:$     print(c['name'], c["type"])$
df_vow['Year'] = df_vow.index.year$ df_vow['Month'] = df_vow.index.month$ df_vow['Day'] = df_vow.index.day
p_new = df2[df2['landing_page']=='new_page']['converted'].mean()$ print("Probability of conversion for new page (p_new):", p_new)
extract_deduped_with_elms_v2.loc[(~extract_deduped_with_elms_v2.ACCOUNT_ID.isnull())$                              &(extract_deduped_with_elms_v2.LOAN_AMOUNT.isnull())].shape
station_count = station_df['Station ID'].nunique()$ print(station_count)
bigram = gensim.models.Phrases(text_list)$ bigram_text = [bigram[line] for line in text_list]
file1 = '/Users/gta/dev/hw-4/schools_complete.csv'$ file2 = '/Users/gta/dev/hw-4/students_complete.csv'
warnings.filterwarnings('ignore')$
vwg = pd.read_excel('input/Data.xlsm', sheet_name='53', usecols='A:Y', header=13, skipfooter=10)
df3[df3['group']=='treatment'].head()
df = pd.read_sql('SELECT * from payment', con=conn_b)$ df
lm=sm.Logit(df2['converted'],df2[['intercept','ab_page']])$ r=lm.fit()
tl_2030 = pd.read_csv('input/data/trans_2030_ls.csv', encoding='utf8', index_col=0)
df4.dtypes
len([baby for baby in BDAY_PAIR_df.pair_age if baby<3])
current_img = current_img_link[0]['style']$ print(current_img.find("('"))$ print(current_img.find("')"))
t2.tail(10)
access_logs_parsed = access_logs_raw.map(parse_apache_log_line).filter(lambda x: x is not None)$
reddit.describe()
inspector = inspect(engine)$ inspector.get_table_names()
print(model.summary())
df.query('Confidence == "A:e"')
sentiments_df = df.apply(get_polarity_scores, axis=1)$ sentiments_df.head()
n = 4$ km = KMeans(n_clusters=n, random_state=123)$ km_res = km.fit(finalDf)
len(cats_in['Animal ID'].unique())
X = reddit_master['title']$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
set(user.columns).intersection(raw.columns)
cityID = '3877d6c867447819'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Wayne.append(tweet) 
df = pd.DataFrame(pd.read_csv(filepath, header = None, index_col = False))
with open('100UsersResults.data', 'wb') as filehandle:  $     pickle.dump(results, filehandle)
df_game_comment =df_game_comment[~(df_game_comment['total'].isin(['']))]$ df_game_comment.head()
x_scaled = 0$ if len(x_normalized) > 1:$     x_scaled = min_max_scaler.fit_transform(x_normalized)
df['label'] = df[forecast_column].shift(-forecast_out)
QUIDS = QUIDS.loc[(QUIDS['week'] == 0) |(QUIDS['week'] == 12)  |(QUIDS['week'] == 14)]$ QUIDS = QUIDS[["subjectkey", "qstot", "week"]].sort_values(['subjectkey', 'week'])
print(regression_model.coef_)$ print(regression_model.intercept_)
cdata.loc[cdata['Number_TD'] > 1, 'Number_TD' ] = 1
df = pd.read_csv(location+'paula_ratings.csv.bz2', parse_dates=['date_of_birth', 'date_of_registration'])
df.sort_values(by="pickup", inplace=True)
adf_check(dfs['Seasonal Difference'].dropna())
with open('180219_10slsqpDM.pkl', 'rb') as f:  # Python 3: open(..., 'rb')$     rez_2 = pickle.load(f)
print(df_subset.info())
excutable = utils.download_executable_lubuntu_hs(save_filepath)
plt.savefig(str(output_folder)+'NB01_6_NDVI_change_'+str(cyclone_name)+'_'+str(location_name))
product_time = nbar_clean[['time', 'product']].to_dataframe() #Add time and product to dataframe$ product_time.index = product_time.index + pd.Timedelta(hours=10) #Roughly convert to local time$ product_time.index = product_time.index.map(lambda t: t.strftime('%Y-%m-%d')) #Remove Hours/Minutes Seconds by formatting into a string
filtered_styles = df.groupby('simple_style').filter(lambda x: x.simple_style.value_counts() >= 5)$ style_bw = filtered_styles.groupby('simple_style').rating_score.mean().sort_values(ascending=False)
df = pd.read_excel("msft.xls")$ df.head()
output_gnb = model_gnb.predict(test[:, 1:5])$ rowID_gnb = [TEST.rowID for TEST in test_data.itertuples()]$ result_df_gnb = pandas.DataFrame({"rowID": rowID,"cOPN": list(output_gnb)})
ts.tshift(-1,freq="H")
merge_df = pd.merge(customers_df, responses_df,left_on='RESPONSE_ID', right_on='ID')$ print(len(merge_df))$ merge_df.head()
def save_csv(k, v, directory):$     path = directory + k + '-matrix.csv'   $     v.to_csv(path, index=False, encoding='UTF-8')
f.loc[:1] # this slice no longer works!
d1.sum() # reduces over columns$
table_names = ['train']$ str('{PATH}{fname}.csv')
print('Highest daily high of 2017: €' + str(np.max(hi_vec)))$ print('Lowest daily low of 2017: €' + str(np.min(lo_vec)))
S.decision_obj.simulStart.value = "2006-07-01 00:00"$ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
y_pred = regr.predict(X_test)
day = datetime.now()$
temp_df2['titles'] = temp_df2['titles'].astype(str)
print(len(plan['plan']['itineraries']))$ print(plan['plan']['itineraries'][0].keys())
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
df = pd.read_csv('/Users/jledoux/Documents/projects/Saber/baseball-data/statcast_with_shifts.csv')
engine = create_engine("sqlite:///Resources/hawaii.sqlite")
np.shape(prec_fine)
model.load_weights('model')$ loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)$ print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))
game_data_all.shape
pd_review.shape
techmeme['nlp_text'] = techmeme.titles.apply(lambda x: tokenizer.tokenize(x.lower()))$ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: ' '.join(x))
x = api.GetUserTimeline(screen_name="berniesanders", count=20, include_rts=False)$ x = [_.AsDict() for _ in x]
import numpy as np$ ok.grade('q06')
driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[1]').send_keys('Avengers: Infinity War')$ driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[2]').click()
for col in df.select_dtypes(include='datetime64').columns:$     print_time_range(col)
t3.head(5)
df_imputed = pd.DataFrame(df_imput)$
stations = session.query(Measurement).group_by(Measurement.station).count()$ stations
table = soup.find('table')$ print (table)
ymc=yearmonthcountcsv.coalesce(1)$ path1=input("Enter the path where you wanna save your csv")$ ymc.write.format('').save(path1,header = 'true')
t1.head(5)$
inspector = inspect(engine)$ inspector.get_table_names()
data.index
data_year_df['Date'] = pd.to_datetime(data_year_df['Date'])$ data_year_df.head()$
test_classifier('c1', WATSON_CLASSIFIER_ID_2)$ plt.plot(classifier_stats['c1'], 'ro')$ plt.show()
data_path = os.path.join(os.getcwd(), 'datasets', 'cryptocurrencyhistoricaldata','ethereum.csv')$ train_A = pd.read_csv(data_path, delimiter = ';')
PYR['soup'] = PYR.apply(create_soup, axis = 1)
tweets['text'].str.lower().str.contains('donald trump|president trump').value_counts()
conn_str = "mongodb://127.0.0.1/clintrials"$ client = pm.MongoClient(conn_str)$ client["admin"].command("listDatabases")
wedate=data['Week Ending Date']$ wedate.head(10)
df = pd.read_csv("../../data/msft.csv",header=0,names=['open','high','low','close','volume','adjclose'])$ df.head()
dummies = pd.get_dummies(plate_appearances['bb_type']).rename(columns=lambda x: 'bb_type_' + str(x))$ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)$ plate_appearances.drop(['bb_type'], inplace=True, axis=1)
result2.summary2()
price_dict = price_data.to_dict()
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
S_1dRichards = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_1dRichards.txt')
dates = pd.date_range('2010-01-01', '2010-12-31')$ df1=pd.DataFrame(index=dates)$ print(df1.head())
tweet_data['text_tokenized'] = tweet_data['text'].apply(lambda x: word_tokenize(x.lower()))$ tweet_data['hash_tags'] = tweet_data['text'].apply(lambda x: hash_tag(x))$ tweet_data['@_tags'] = tweet_data['text'].apply(lambda x: at_tag(x))
store_items.fillna(0)
df_full["Field4"] = df_full.Field4.fillna("None")
plt.rcParams['figure.figsize'] = [16,4]$ plt.plot(pd.to_datetime(mydf2.datetime),mydf2.fuelVoltage, 'g.');$ plt.xlim(datetime.datetime(2018,2,8),datetime.datetime(2018,3,25))
prcp_query = session.query(func.strftime(measurements.date), (measurements.prcp)).\$ filter(measurements.date <= last_date, measurements.date>= last_date-relativedelta(months=12)).all()$ prcp_query
ds_issm = xr.open_dataset(data_url1)$ ds_issm = ds_issm.swap_dims({'obs': 'time'})$ ds_issm
data.index = pd.to_datetime(data['created_at'])
df_users = pd.read_csv('../data/august/users.csv')$ df_levels = pd.read_csv('../data/august/levels.csv')$ df_events = pd.read_csv('../data/august/events.csv', skiprows=1, names=event_header, error_bad_lines=False, warn_bad_lines=True)     
n = np.count_nonzero(e==0)$ k = np.count_nonzero(y[e==0])
precip_data = session.query(Measurements).first()$ precip_data.__dict__
print(df.shape)$ df = df[pd.notnull(df['is_shift'])]$ print(df.shape)
model_knn_vector = KNeighborsClassifier(n_neighbors=250)$ model_knn_vector = model_knn_vector.fit(train_arrays, train_labels)$ model_knn_vector.score(test_arrays, test_labels)
for col in Y_train_df.columns:$     nbsvm_models[col] = NbSvmClassifier(C=10, dual=True).fit(X_train_cont_doc, Y_train_df[col])
! python  keras-yolo3/yolo.py ../sonkey13.jpeg
data = data[data.state != 'undefined']$ data.info()
url_template.format(start,)
sp = openmc.StatePoint('statepoint.50.h5')
au.save_df(df_city, 'data/city-util/proc/city')$ au.save_df(df_util, 'data/city-util/proc/utility')$ au.save_df(misc_info, 'data/city-util/proc/misc_info')  # this routine works with Pandas Series as well
pvt.reset_index(inplace=True)$ pvt
!hdfs dfs -cat /user/koza/hw3/3.2.1/productFrequencies/* | wc -l
infoExtractionRequest = requests.get(information_extraction_pdf, stream=True)$ print(infoExtractionRequest.text[:1000])
accuracy = accuracy_score(y_test, y_pred)$ print('Accuracy: {:.1f}%.'.format(accuracy * 100.0))
human_genome_length = gdf.length.sum()$ print("The length of the human genome is {} bases long.".format(human_genome_length))
index = pd.date_range('2018-3-1', periods=1000)$ index
active_with_return.iloc[:,0] = active_with_return.iloc[:,0].astype("int")
processing_test.files
news_sentiments.to_csv('New_Source_sentiments.csv', encoding='utf-8', index=False)
date_ly = dt.date.today() - dt.timedelta(days = 2*365)$ print(date_ly)
open('test_data//write_test.txt', mode='w')
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=["Tweets"])$ display(data.tail(10))
fh_3 = FeatureHasher(input_type='string', non_negative=True)$ %time fit3 = fh_3.fit_transform(train.device_ip)
fires.dtypes
sp_500_adj_close = sp500[['Adj Close']].reset_index()
df = pd.read_csv('./Data/AAPL.csv', index_col=0)$ df.head()
plt.title(f"Overall Media Sentiment Based on Twitter as of {curDate}")$ plt.xlabel("Outlets")$ plt.ylabel("Tweet Polarity")
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);
print(r"'\xba\xba'.decode('mbcs'):",repr('\xba\xba'.decode('mbcs')))
pd.to_datetime(df.time.unique(), unit='s')
for columns in DummyDataframe[["Positiv", "Negativ"]]:$     basic_plot_generator(columns, "Graphing Dummy Data" ,DummyDataframe.index, DummyDataframe)
len(df_new.UWI.unique())
rf.score(clim_test, size_test)
date_df = pd.DataFrame(normals, columns=('tmin','tavg','tmax'))$ date_df['date'] = trip_dates$ date_df.set_index('date')
events_enriched_df = pd.merge(events_df[['event_id','group_id','yes_rsvp_count','waitlist_count','event_time']]$                               ,groups_topics_unique_df[['group_id','topic_name','topic_id']]$                               ,on ='group_id' , how='left' )
cursor = db.TweetDetils.aggregate([ {"$group" : {"_id":"$user_name", "score":{"$sum":"$fav_cnt"}}},  {"$sort":{"score" : -1}},{"$limit":5}])$ for rec in cursor:$     print(rec["_id"], rec["score"])
%time train = pd.read_csv("../assets/trainaa")$ train.head(1)
df1['volatility']=(df1['Adj. High']-df1['Adj. Close'])/df1['Adj. Close']$ df1['volatility'].head()
df.to_csv("newsOutletTweets.csv")
experience = pd.get_dummies(questions['rate_experience'])
most_recent_1yr_entry = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(most_recent_1yr_entry)
conn_hardy = psycopg2.connect("dbname='analytics' user='udacian' host='udacity-segment.c2zpsqalam7o.us-west-2.redshift.amazonaws.com' port='5439' password='AYEe&mtihMqtXQbWR2xgWrhmKzd6]F'")
data.head(4)
post_gc_launch_sales =  shopify_data_merge_msrp[shopify_data_merge_msrp['Created at'] >= '2018-06-08']$ post_gc_launch_sales[(post_gc_launch_sales['Email'] == 'vincent.brouwer1977@gmail.com')]$
tfav.plot(figsize = (16, 4), color ='b')$
data_path = 'Crimes_-_2001_to_present.csv'$ df = spark.read.csv(data_path, sep = ',', header = True)$ df.take(1)
df = json_normalize(j['datatable'], 'data')$ df.columns = col_names$ df.head()
lm3 = sm.Logit(ab_df_new['converted'], ab_df_new[['intercept','CA','UK','treatment_US','treatment_CA']])$ result3 = lm3.fit()$ result3.summary2()
!cat ProductPurchaseData.txt | head
todays_datetimes = [datetime.datetime.today() for i in range(100)]
p = fp7.append(h2020)$ fp7 = 0$ h2020 = 0
data_links = ['https://raw.githubusercontent.com/sinny777/hukam/master/machinelearning/tf-nlc-model/data/data.csv']
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=xhB-Ae2VRyMYmv2CRGtV")
if 1 == 1:$     news_titles_sr = pd.read_pickle(news_period_title_docs_pkl)
crimes.iloc[10:20,4:6]
raw_data = pd.read_csv('AMZN.csv',sep=',')
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'US', 'UK']])$ result = logm2.fit()$ result.summary2()
Measurement = Base.classes.hi_measurements$ Station = Base.classes.stations
df_daily.dropna(subset=["PREV_DATE"], axis=0, inplace=True)$ df_daily.head(5)
X = pd.get_dummies(X, columns=['subreddit'], drop_first = True)
concat_3 = pd.concat([df1, df2, df5], axis=1)$ concat_3
definition_details = client.repository.store_definition(filename_keras, model_definition_metadata)
with open('datasets/git_log_excerpt.csv', 'r') as file:$     file = file.read()$     print(file)$
x.dtype = 'int32' # (need to revert)$ x.astype(float) # creates a new array
url_weather = "https://twitter.com/marswxreport?lang=en"$ browser.visit(url_weather)
df.info()
bymin = walk.resample("1Min")$ bymin.resample('S').mean()
dfs.sort_values(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True, ascending=False)$ dfs.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True)
lr_pipe.fit(X_train, y_train)$ lr_pipe.score(X_test, y_test)
import pandas as pd$ weather = pd.DataFrame(weather_json['data'])$ weather['date']  = pd.to_datetime(weather['date'])
columns_name = ["content","creationTime","isTop","referenceId","userClientShow","isMobile"]$ content = re.findall(r'"showOrderComment".*?"content":"(.*?)","creationTime":"(.*?)","isTop":(\w*).*?"referenceId":"(\d*)".*?"userClientShow":"(.*?)".*?"isMobile":(\w*).*?',c)$ content$
td = td.fillna(0)
converted = ts.asfreq('1Min', method='pad')
TripData_merged3 = TripData_merged2.dropna(how='any')
new_group.get_group('N')
sorted_errors_idx = options_frame['ModelError'].map(abs).sort_values(ascending=False).head(50)$ errors_20_largest_by_strike = options_frame.ix[sorted_errors_idx.index]$ errors_20_largest_by_strike[['Strike', 'ModelError']].sort_values(by='Strike').plot(kind='bar', x='Strike')$
mlab_uri = os.getenv('MLAB_URI')$ mlab_collection = os.getenv('MLAB_COLLECTION')
expiry = datetime.date(2015,1,17)$ aapl_calls = aapl.get_call_data(expiry=expiry)$ aapl_calls.iloc[0:5,0:4]
crime_data = pd.read_csv("chicago_crimes_by_ward.csv")
df_pol_t = pd.concat([df_pol_matrix_df, df_pol_t.drop('title', axis=1)], axis=1)$
criteria = so['ans_name'].isin(['Scott Boston', 'Ted Petrou', 'MaxU', 'unutbu'])$ so.loc[criteria].head()
groupby_w = df['y'].groupby(df['w'])$ round(groupby_w.describe(), 3)
x = np.arange(8).reshape(4,2)$ y = x.flatten()$ y[[0, 1, 5]]
df = df.sort_values('label', ascending = True)$ df.head()
data = pandas.read_csv("MSFT.csv", index_col='Date')$ data.index = pandas.to_datetime(data.index)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
taxdata_blocks = pd.merge(taxdata,parcel_blocks, how='left', left_on=['pin'], right_on=['PIN'])$ taxdata_blocks = taxdata_blocks.drop(['pin','PIN'],axis=1)$ taxdata_blocks = taxdata_blocks.dropna(subset=['TRACTCE10','BLOCKCE10'])
mv_lens.title.value_counts().head()
df_students.describe()
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=180)
mask = y_test.index$ t_flag = y_test == 0$ p_flag = pred == 1
cityID = '9a974dfc8efb32a0'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Kansas_City.append(tweet) 
dul_isbns = dul['ISBN RegEx']$ dul_isbns.size
combats.Winner[combats.Winner == combats.First_pokemon] = 0$ combats.Winner[combats.Winner == combats.Second_pokemon] = 1$ print(combats.head(5))
rhum_nc = Dataset("../data/nc/rhum.mon.mean.nc")
bad_r = requests.get('http://httpbin.org/status/404')$ bad_r.status_code
dfLikes.groupby("date").agg({"id": "count"})
trip_prec_df = pd.DataFrame(sq.history_rainfall_trip(), columns=["Station", "Total prec"])$ trip_prec_df
paired_cameras_missing_from_shopify_orders = np.setdiff1d(BPAIRED_SHOPIFY['channel_order_id'],ORDER_TO_PAIR_SHOPIFY['channel_order_id'])
sc.getConf().get("spark.yarn.historyServer.address")
network_simulation[network_simulation.generations.isin([5])]$
ax = pdf.plot()$
df['date'] = pd.to_datetime(df['date'])
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])$
from h2o.automl import H2OAutoML
RIDs_DXSUM = list(diagnosis_DXSUM_PDXCONV_ADNIALL['RID'].unique())$ RIDs_ADSXLIST = list(diagnosis_ADSXLIST['RID'].unique())$ RIDs_BLCHANGE = list(diagnosis_BLCHANGE['RID'].unique())
df_schools = pd.read_csv(file1)$ df_students = pd.read_csv(file2)
joined[['Frequency_score']] = joined[['Frequency_score']].astype(int)$ joined.dtypes.filter(items=['Frequency_score'])
honeypot_df.drop(['time_stamp1','time_stamp2','time_stamp3'], axis = 1, inplace = True)
df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']]$ print(df.head())
df = pd.read_csv('estimating_quartiles.csv')
time_series.info()
df['y'].plot.hist(bins=15)
pipe_dat = df_pipe.copy()
end_date = (datetime.strptime(most_recent_tobs , '%Y-%m-%d') - timedelta(weeks=48)).\$     strftime('%Y-%m-%d') #converts datetime to string$ end_date
data['SA'] = np.array([ analyseSentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
festivals.at[2,'Location'] = "Humboldt Park and Division Street"$ festivals.head(3)
precip_df = history_df['precipitation']$ precip_df.describe()
cursor = collection_reference.aggregate([{'$sample': {'size': 5}}])$ tw_sample_df = pd.DataFrame(list(cursor))$ tw_sample_df
ISR_df = ISR_df.merge(ISR_df_agg, how='left', on='student_program')$ ISR_df.drop_duplicates('Student_Section__c', inplace=True)$ df = df.merge(ISR_df[['Section__c', 'Student__c'] + list(ISR_df_agg.columns)], on='Section__c')
save_n_load_df(ph, 'mergeable_holidays.pkl')$
for c in ccc:$     vhd[c] /= vhd[c].max()
if not os.path.isdir('output/heat_demand'):$     os.makedirs('output/heat_demand')
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']])$ lm.fit().summary2()
free_data.head(5)
latest_news_para = soup.find('div', class_ = 'article_teaser_body').get_text()$ print(latest_news_para)$
item_lookup = shopify_full_size_only[['child_sku', 'child_name']].drop_duplicates() # Only get unique item/description pairs$ item_lookup['child_sku'] = item_lookup['child_sku'].astype(str) # Encode as strings for future lookup ease
lr = LogisticRegression(random_state=20)$ lr.fit(X, y)
data.shape
df = df[(df['Age_Years'] <= 30) & (df['Age_Years'] >= 20)]$ df.head()
engine.return_as_panda_dataframe = True
df2 = df[df['Title'].str.contains(blacklist) == False]$ df2 = df2.drop_duplicates(subset = 'Title', keep = False)$ df2 = multi_manufacturer_designer(df2, 'Title')$
post_discover_sales[post_discover_sales['Email'] == 'Jennyann57@yahoo.com.sg']
grid_lat = np.arange(24, 50.0, 1)$ grid_lon = np.arange(-125.0, -66, 1)$ glons, glats = np.meshgrid(grid_lon, grid_lat)
bb['low'].plot()
sanders_df = pd.DataFrame(sanders)$ sanders_df.head()
masked.user_created_at = pd.to_datetime(masked.user_created_at)
import warnings$ warnings.simplefilter('ignore')
bymin.resample("S").bfill()
dtc_features = sorted(list(zip(test_features, dtc.feature_importances_)), key=lambda x: x[1], reverse=True)$ dtc_features
np_x = np.random.rand(1000)$ np_target = 0.96*np_x + 0.24
coarse_groups = mgxs.EnergyGroups([0., 0.625, 20.0e6])$ fine_groups = mgxs.EnergyGroups([0., 0.058, 0.14, 0.28,$                                  0.625, 4.0, 5.53e3, 821.0e3, 20.0e6])
df1.fillna(-999999,inplace=True)
data.duplicated().sum()
X_train, X_test, y_train, y_test = train_test_split(features,classification_price,test_size=0.2)
print("Probability of treatment group converting:", $       df2[df2['group']=='treatment']['converted'].mean())
Labels_majority = (((CurrentA1.iloc[:,3:14] + CurrentA2.iloc[:,3:14] + CurrentA3.iloc[:,3:14])/3)>0.3).astype("int32")$ Class_frame_majority = pd.concat([CurrentA1.iloc[:,0:2],Labels_majority],axis=1)$ Class_frame_majority.to_csv("union_voting_elisa.csv")
classifier = svm.SVC(kernel='linear')$ classifier.fit(X_train, y_train)$ y_prediction_SCM = classifier.predict(X_test)$
combined_data.to_excel('SHARE-UCSD-export_reformatted.xlsx',encoding="utf-8", index=False)$ combined_data.to_csv('SHARE-UCSD-export_reformatted.csv',encoding="utf-8", index=False)
Obama_raw = pd.read_csv('data/BarackObama.csv', encoding='latin-1')$ Trump_raw = pd.read_csv('data/realDonaldTrump.csv', encoding='latin-1')
p_new = df2['converted'].mean()$ print("{} is the convert rate for  Pnew under the null.".format(p_new))
import sys$ sys.executable$
df.columns
pd.read_csv("../../data/msft.csv",nrows=3)
tot = pd.merge(dailyplay, promotions, how = 'left', left_on = ['Date'], right_on = ['Date'])
X = reddit_master['title']$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys() # Retrieve the names of the tables in the database
tweet_info_pd = pd.DataFrame(tweet_info)$ tweet_info_pd = tweet_info_pd[['Account', 'Text', 'Date', 'Compound Score', 'Positive Score', 'Neutral Score', 'Negative Score','Tweets Ago']]$ tweet_info_pd.head()$
random_integers.idxmax(axis=1)
march_2016 = pd.Period('2016-03', freq='M')$ print march_2016.start_time$ print march_2016.end_time
print ("categorical accuracy from mean      :", float(categorical_accuracy(val_labels, val_avg_preds2).eval()))$ print ("best individual categorical accuracy:", np.max(cat_acc))$
model = word2vec.Word2Vec.load("200features_30minwords_10context")
url = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-06-25&end_date=2018-06-25&' + API_KEY$ r = requests.get(url)$ json_data = r.json()
print ("Data Frame with Forward Fill:")$ df2.reindex_like(df1,method='ffill')
tweet_summary = tweet_df.groupby(["Tweet Source"], as_index = False)["Compound Score"].mean().round(3)$ tweet_summary
plt.pie(drivers_totals, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()
client.experiments.get_status(experiment_run_uid)['state']
Google_stock.describe()
df1_normal = df1.copy()$     $ df1_normal['y'] = np.log(df1_normal['y'])
for g in my_gempro.genes:$     for s in g.protein.structures:$         g.protein.align_seqprop_to_structprop(seqprop=g.protein.representative_sequence, structprop=s)
rt_set = df2['text'].value_counts().index.tolist()$ rt_set_vals = df2['text'].value_counts().values.tolist()$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ data.head(10)
cityID = 'b463d3bd6064861b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Detroit.append(tweet) 
data = originaldata.copy()$ data = data.reset_index()$ del data["index"]
df_model = df[['height','profile_popularity','Age','Time_In_Badoo','job_ID','liked']]$ df_model.head()
weather1.head()
pc = decimal.Decimal(110) / decimal.Decimal(100)$ fee = decimal.Decimal(.003)$ pc> 1+fee
feature_col = ibm_hr_final.columns$ feature_col
rsvp_df = pd.read_csv("last_five_rsvp_means.csv")$ rsvp_df.head()
cursor_sampl = collection_reference.aggregate($     [{'$sample': {'size': 20}}]$ )
sel = [Measurements.station, func.count(Measurements.tobs)]$ active_stations_data = session.query(*sel).group_by(Measurements.station).order_by(desc(func.count(Measurements.tobs))).all()$ active_stations_data
print "https://twitter.com/AirbnbHelp/status/{}".format(tweets_df['09/20/2017'].iloc[0]['twitter_id'])
url = 'https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2015-01-01&end_date=2015-01-01&api_key='$
s_mountain = Series(np.arange(0,5),index=pd.date_range('2014-08-01', periods=5, freq="H",tz="US/Mountain"))$ s_eastern = Series(np.arange(0,5),index=pd.date_range('2014-08-01', periods=5, freq="H",tz="US/Eastern"))$ s_mountain
df_dummies = pd.get_dummies(df_value, columns=['value'])$ df_dummies.head()
df_links['link.domain'].value_counts().head(25)
query = 'CREATE TABLE new_york_new_york_points_int_ct10 AS (SELECT geoid, osm_id, latitude, longitude FROM beh_nyc_walkability, new_york_new_york_points WHERE ST_INTERSECTS(beh_nyc_walkability.the_geom, new_york_new_york_points.the_geom))'$
auth = tweepy.OAuthHandler(api_key, api_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, wait_on_rate_limit_notify=True, wait_on_rate_limit=True)
data.loc[(80, slice(None),'put'),:].iloc[0:5,0:4]
len(free_data.country.unique())
vars2 = [x for x in dfa.ix[:,6:54]]$ vars2
ks_particpants=kick_projects.groupby(['category','launched_year','launched_quarter','goal_cat_perc']).count()$ ks_particpants=ks_particpants[['name']]$ ks_particpants.reset_index(inplace=True)
Base.prepare(engine, reflect=True)$
df2 = df$ mismatch_index = mismatch_df.index$ df2 = df2.drop(mismatch_index)
last_unix=last_date.timestamp() #it is a function of the time module that can convert the time to$
testing.head()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key={}".format(API_KEY))$ print(r.json()['dataset']['data'][0])
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ print(commits_per_year.apply(lambda x: x.head())[0:5])
scores[3.5:].sum()/total
dul = pd.concat([dule2, dulp], ignore_index=True)$ dul
treehouse_expression_hugo_tpm = treehouse_expression.apply(np.exp2).subtract(1.0).add(0.001).apply(np.log2)
for edge in H.edges():$     if H[edge[0]][edge[1]]['weight'] > 3000:$         print(edge,partition[edge[0]],partition[edge[1]])
vhd = pd.read_excel('input/Data.xlsm', sheet_name='52', usecols='A:AO', header=6, skipfooter=16)
title = soup.find_all('div', class_="content_title")[1].text.strip()$ description=soup.find_all('div', class_="rollover_description_inner")[1].text.strip()
yc_new2 = yc_new1[['Fare_Amt', 'tripDurationHours', 'Trip_Distance',$        'Tip_Amt', 'income_departure', 'income_dest']]$ yc_new2.head()
temp_df.plot(kind="hist", bins=12)
len(soup.find_all('div', 'sammy'))$
pipeline.fit(X_train, y_train)
r= requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json")$ json_data = r.json()$ json_data["dataset_data"]["data"][0]
x_train, x_test, y_train, y_test = train_test_split(bow_df, amazon_review['Sentiment'], test_size=0.2)$ x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1.0/3.0)
def clean_tweet(tweet):$     return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())$
SGDC = SGDClassifier()$ model2 = SGDC.fit(x_train, y_train)
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(therapist_duration, params=params, freq=31, title='Therapist Decomposition')
tmp_df = ratings.pivot(index='userId', columns='movieId', values='rating')
df_features2.loc[df_features2["CustID"].isin([customer])]
ix = ((iris.Species == "setosa") | (iris.Species == "versicolor")) & (iris["Sepal.Length"]>6.5)$ iris.loc[ix.values,iris.columns[:2]]
df_full.rename(columns=DATA_L2_HDR_DICT,inplace=True)
df.groupby('domain').count().sort_values('id',ascending=False).head(25)
plt.xlim(-0.25, len(x_axis))$ plt.ylim(-.3, .1)
url = 'https://mars.nasa.gov/news/'
data.loc[pd.to_datetime("2016-09-02")]
malebyphase = malemoon.groupby(['Moon Phase']).sum().reset_index()$ malebyphase
building_pa_prc_shrink.to_csv("buildding_02.csv",index=False)$ building_pa_prc_shrink=pd.read_csv('buildding_02.csv',parse_dates=['permit_creation_date'])
psy_df4 = PDSQ.merge(psy_df3, on='subjectkey', how='right') # I want to keep all Ss from psy_df$ psy_df4.shape
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="Dreamers").items(max_tweets):$     print(tweet)
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))$
solar_wind_df.loc[3080:3085]
write_to_pickle(path+'/features.pkl', users)$ users=load_pickle('features.pkl')$
new_df = new_df.loc[df['ooCancelReason'] == 0]
Measurement = Base.classes.measurement$ Station = Base.classes.station
iso_join.head()
data['intercept'] = 1.0
rng_dateutil = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='dateutil/Europe/London')
data_compare['SA_mix'] = np.array([ analize_sentiment_multilingual(tweet) for tweet in data_compare['tweets_original'] ])
type(df_vow['Date'].loc[0])
news_df.head()
df.iloc[:4]
df_events.iloc[:,7].value_counts()
import matplotlib$ import matplotlib.pyplot as plt$ pd.plotting.scatter_matrix(newMergedDF, figsize=(88, 88))$
for row in df.itertuples():$     print(row)
conn = 'mongodb://localhost:27017'$ client = pymongo.MongoClient(conn)
top_songs[top_songs['Track Name'].isnull()]['Region'].unique()$
print(tweet_df.groupby(["Tweet Source"])["Tweet Source"].count())$ tweet_df.head()
p(sys.getdefaultencoding)
d = datetime(2014,8,29)$ do = pd.DateOffset(days = 1)$ d + do
comm.columns
re.sub(rpt_regex, rpt_repl, "Reppppeated characters in wordsssssssss" )
grid_id = pd.DataFrame(grid_id_flat).astype('str')$ grid_id.columns = ['grid_id'] $ print(grid_id.head(), grid_id.tail())
financial_crisis.drop('Spain defaults 7x')$
df2[df2['group']=='control']['converted'].mean()
longest_date_interval = longest_date_interval.reset_index()$ longest_date_interval.columns = longest_date_interval.columns.remove_unused_levels().droplevel(level=1)$ print(longest_date_interval.head(3))$
AAPL_array=df["NASDAQ.AAPL"].dropna().as_matrix()$ model_arima = ARIMA(AAPL_array, (2,2,2)).fit()$ print(model_arima.params)
hru_rootDistExp = rootDistExp.open_netcdf()
json_data = r.json()$ for k in json_data.keys():$     print(k + ': ', json_data[k])$
events = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).last().reset_index()$ events = events[['atbat_pk', 'events', 'woba_value']]
score_summary = sns.countplot(x='Score', data=merge_df)$ score_summary.set_title('Recommendation')
!wget https://storage.googleapis.com/aibootcamp/data/ortec_templates.zip
modelCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])$
list(tweets_total[0].keys())
branch_dist = [analyze_swcfile.get_dist_to_root(a,example_df) for a in branches]
df_merge.per_student_budget.describe()
datos = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(datos.head(10))
dr_new_data_plus_forecast.to_csv('./data/dr_new_patients_arimax_forecast.csv')$ dr_existing_data_plus_forecast.to_csv('./data/dr_existing_patients_arimax_forecast.csv')
DataSet = DataSet[DataSet.userName.notnull()]$ len(DataSet)
import nltk.data$ tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')$
autoDataFile = open("auto-data-saved.csv","w")$ autoDataFile.write("\n".join(autoData.collect()))$ autoDataFile.close()
res = es.search()$ print(res["_shards"])$
events_per_day = events_df[['event_day','event_id']].groupby('event_day').count()$ events_per_day.rename(columns={'event_id':'count_event_day'},inplace=True)$ events_per_day.reset_index(inplace=True)
sample.dtypes
S = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure08/summa_fileManager_riparianAspenPerturbRoots.txt')
nmf_tfidf_topic6_sample = mf.random_sample(selfharmmm_final_df, criterion1='non_lda_max_topic', value1='nmf_tfidf_topic6', use_one_criterion=True)
def ceil_dt(dt):$     delta = timedelta(minutes=15)$     return (dt + (datetime.min - dt) % delta) + timedelta(hours=2)
shopping_carts = pd.DataFrame(items)$ shopping_carts
n_estimator = 100$ rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator, n_jobs=3, verbose=2)$ rf.fit(X_train, y_train)$
sns.regplot(x="qstot_0", y="y", data=psy_native).set_title("Depressive Symptomatology")$
fig3 = df['zone'].value_counts().plot('barh', title='Citations by Zone Category')
rows=session.query(Adultdb).filter_by(native_country='Outlying-US(Guam-USVI-etc)')
tag_df = stories.tags.apply(pd.Series)$ tag_df.head()
xyz = json.dumps(youtube_urls, separators=(',', ':'))$ with open('youtube_urls.json', 'w') as fp:$     fp.write(xyz)$
df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium", "good", "very good"])$ df["grade"]
year_with_most_commits = commits_per_year.idxmax()[0].year$ year_with_most_commits
%%time$ data_demo["num_child"] = data_demo["comment_id"].apply(lambda x: data_demo[data_demo["parent_id"]==x].shape[0])
aliases = [b for b in BID_PLANS_df.index if '\n' in b]
locations = session.query(Measurement).group_by(Measurement.station).count()$ print("There are {} stations.".format(locations))$
from sklearn.feature_extraction.text import CountVectorizer # Import the library to vectorize the text$ count_vect = CountVectorizer(ngram_range=(1,3),stop_words='english')$ count_vectorized = count_vect.fit_transform(df_train.text)
pd.value_counts(Stockholm_data.tweet_created_at).reset_index()$ countData = pd.read_csv(path + "countData.csv")$ countData
cityID ='5c62ffb0f0f3479d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Phoenix.append(tweet) 
df = pd.read_csv('/home/bmcfee/data/vggish-likelihoods-a226b3-maxagg10.csv.gz', nrows=1000000, index_col=0)
from nltk.corpus import stopwords$ english = stopwords.words('english')
merged_portfolio_sp = pd.merge(merged_portfolio, sp_500_adj_close, left_on='Acquisition Date', right_on='Date')$ merged_portfolio_sp.head()
data['Week Ending Date'] = pd.to_datetime(data['Week Ending Date'])$ data['year'] = data['Week Ending Date'].dt.year$ data.head(5)
df['lead_mgr_counts'] = df.groupby(['lead_mgr'])['lead_mgr'].transform('count')$
max_name_length = (df['Name'].map(len).max())$ print("Longest name:", max_name_length)
total_stations = session.query(Stations).distinct().count()
graf=df.copy()
df.index.get_level_values('Subcategory').unique()$
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
AAPL_array=df["log_AAPL"].dropna().as_matrix() 
sns.factorplot(x='call_type',y='length_in_sec',col='call_day',data=calls_df,kind='bar')
! rm -rf /home/ubuntu/s3/flight_1_5/extracted/flight_1_5_price_2017-05-10_.pq$
review_df.head()
common_words_file = open('common_words.txt')$ common_words = common_words_file.read().split(',')$
dat=dat_before_fill.copy() #reset 'dat' to be un-interpolated data$ for temp_col in temp_columns:$     dat.loc[:,temp_col]=dat[temp_col].interpolate(method='linear', limit=12) #<= 3hrs;  at a 15 min interval, 3 hrs is 12 measurements
print(bag.toarray())
print(1/np.exp(-0.0408),1/np.exp(0.0099))
forecast_set = clf.predict(X_lately)$ len(forecast_set)
plot_autocorrelation(series=dr_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr hours new patients'))$ plot_autocorrelation(series=dr_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr hours existing patients'))
ltdate = pd.to_datetime(voters.LTDate.map(lambda x: x.replace(' 0:00', '')))$ print(ltdate.describe())$ ltdate.value_counts(dropna=False).head(5)
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
D2 = [(str(v), str(t).replace("|","")) for v,t in D2]$ D2[0:5]
df.describe()
y = w * x + b$ loss = K.mean(K.square(y-target))
twelve_months = session.query(Measurements.date, Measurements.prcp).filter(Measurements.date > year_before)$ twelve_months_prcp = pd.read_sql_query(twelve_months.statement, engine, index_col = 'date')
dfSummary = pd.concat([summary_all, summary_bystatus],axis=1)$ dfSummary.columns = ("1930-2017","1930-1980","1984-2017")$ dfSummary
save_filepath = '/media/sf_pysumma'$ hs_path = save_filepath+'/a0105d479c334764ba84633c5b9c1c01/a0105d479c334764ba84633c5b9c1c01/data/contents'$ S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
words = [w for w in words if not w in stopwords.words("english")]$ print(words)
news_sentiment_df.to_csv('news_sentiments.csv')
pd.core.common.is_list_like = pd.api.types.is_list_like$ import pandas_datareader.data as web
dollars_per_unit.columns = pd.MultiIndex.from_product([['Dollars per Unit'], dollars_per_unit.columns])$ pd.concat([multi_col_lvl_df, dollars_per_unit], axis='columns').head(3)
firewk18= fire[(fire['incident_date'].dt.year ==2018) &(fire['incident_date'].dt.month ==5)].groupby('dayofweek')['incident_number'].agg({'numofinstance_fire':'nunique'}).reset_index()$
numofstations=session.query(Station.station).count()$ numofstations
Net_Capital_Gain = pd.read_sql(q, connection)$ Net_Capital_Gain.head()
clusters_kmeans_predict0 = clf_kmeans0.fit_predict(X_train)$
X_know.shape
dtanswer = qs.CreationDate_a - qs.CreationDate
import matplotlib.pyplot as plt$ import seaborn as sns
first_row = session.query(Station).first()$ first_row.__dict__
df = pd.read_csv(datafile)$ print(df.as_matrix().shape)
y_pred = fit5.predict(X_test)
tweet_df.count()
df.index
reddit_master.shape
data.loc[data['hired']==1].groupby('category').hourly_rate.mean()
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", start='1980',end='2014')$ le_data_all
data.plot.area()
dataPath = os.path.join("..", "example-data")$ data = pd.read_csv(os.path.join(dataPath, "example-from-j-jellyfish.csv"))$ data.head()
df.loc['1975-01-01']
yhat_summore = modell.predict(X_dunno)$
retweets = pd.read_sql_query(query, conn)$ retweets.head()
hawaii_df.describe()
from pandas_datareader import wb$ all_indicators = wb.get_indicators()$ all_indicators.ix[:,0:1]
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
mars_images = requests.get('https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars')$ mars_soup = BeautifulSoup(mars_images.content, 'html.parser')$
cur.execute('SELECT LangCd ,count(*) FROM surveytabl WHERE LangCd="QB";')$ cur.fetchall()
data['Age'].min()
dfd = pd.get_dummies(dfm, columns=['country'], drop_first=True)$ dfd = pd.get_dummies(dfd, columns=['cat_type'], drop_first=True)
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')$ git_log['timestamp'].describe()
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2.drop(['Neighbors_Obj'], axis=1)$
match = results[2]$ print lxml.html.tostring(match)
validation.analysis(observation_data, Jarvis_resistance_simulation_1)
sentiment_df = ave_sentiment_by_company_df.reset_index(drop = False)$ sentiment_df
df.ix[:4]
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=uEEsBcH3CPhxJazrzGFz&start_date=2017-01-01&end_date=2017-12-31")$
display((data['Tweets'][fav]))$ print("Number of likes: {}".format(fav_max))$ print("{} characters.\n".format(data['len'][fav]))
!hdfs dfs -put Consumer_Complaints.csv Consumer_Complaints.csv
plot_mnist_sample(mnist_train.train_data, $                   sample_idx=[i for i in range(10)])$ plot_mnist_sample(mnist_test.test_data, size=10)$
import numpy as np$ ok.grade('q01')
pca = PCA(n_components=6858)$ pca.fit(crosstab)
scores = cross_val_score(model, X_top, y, scoring='roc_auc', cv=5)$ print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))
df_trimmed.event.unique()$
tencent_df.set_index('date').pivot_table(index='date', columns=['second_type', 'work_year']).fillna('--').rename(columns={'salary_mean':'平均薪资/K'})
parsed_locations[parsed_locations.country.isnull()]
data.Likes.value_counts(normalize=True).head().plot(kind='bar')
df_daily[df_daily["ENTRIES"] < df_daily["PREV_ENTRIES"]].head()
obs = df_gp_hr.groupby('level_0').mean()$ observation_data = obs['Observation (aspen)']
print (df.dtypes[df.dtypes == 'object'])$
print "Total number of rows of table 'photoz_bcnz': ", len(df3) $ print df3.columns.values$ df3
list = ['a','b','c','d','e']$ list.insert(1,'x')$ print(list)
from statsmodels.tsa.stattools import adfuller as ADF$ print('原始序列的ADF檢查結果為：', ADF(resid_713.values))$
df = df[df['Time_In_Badoo'] <= 1500]$ df.head()
learn.predict()
all_patents_df.number_of_claims.describe()
plt.style.use('ggplot')$ bb['close'].apply(rank_performance).value_counts().plot(kind='bar')
print(train.isnull().sum())$ train_att = train[train['is_attributed']==1]$ print(train_att.isnull().sum())
df.published_at.describe()
h = plt.hist(tag_degrees.values(), 100) #Display histogram of node degrees in 100 bins$ plt.loglog(h[1][1:],h[0]) #Plot same histogram in log-log space
df_campaigns['Open Rate'] = df_campaigns['Open Rate'].apply(lambda x: x[:-1]).astype('float')$ df_campaigns['Click Rate'] = df_campaigns['Click Rate'].apply(lambda x: x[:-1]).astype('float')
files = [f for f in listdir('Twitter_SCRAPING/scraped/') if f.endswith('.csv') and isfile(join('Twitter_SCRAPING/scraped/', f))]$ d_scrape = pd.concat([pd.read_csv('Twitter_SCRAPING/scraped/'+f, encoding='utf-8') for f in files], keys=files)$ print(d_scrape.head())
df.filter(like="FLG_", axis=1)
pd.isnull(r1_test).values.any()$
max_change_1day = max((v.High - v.Low) for v in data.values()) $ print('=>The maximum change in a day was {:.2f} ' .format(max_change_1day))
word_centroid_map = dict(zip( model.wv.index2word, idx ))
cityID = '00ae272d6d0d28fe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lexington_Fayette.append(tweet) 
zipincome = pd.DataFrame(jsonData)$ zipincome.head()
kick_projects['launched_date'] = pd.to_datetime(kick_projects['launched'], format='%Y-%m-%d %H:%M:%S')$ kick_projects['deadline_date'] = pd.to_datetime(kick_projects['deadline'], format='%Y-%m-%d %H:%M:%S')
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path, sheetname = 'Superstore2')$ df.head(5)
print(pd.unique(stops['operator'].ravel().tolist()))
stream = tweepy.Stream(auth, l)
t1.drop(t1[t1['retweeted_status_id'].notnull()==True].index, inplace=True)
timelog = timelog.drop(['Email', 'User', 'Amount ()', 'Client', 'Billable'], axis=1)
for i, row in companies.iterrows():$     print(companies['twitter'][i])
coll.distinct("overall_status")
plt.hist(review_length, bins =100)$ plt.show() # most reviews are short, only few reviews are very long.
walk.resample("1Min", closed="right")
r=[col for col in df if (col.startswith('רמת האלימות'))]$ game_vail=df.loc[:,r]$
if not os.path.isdir('output/electricity_demand'):$     os.makedirs('output/electricity_demand')
df3 = df2[df2['Current Status']=='complete']$ df3.head()
scores_df.to_csv('News_Tweets.csv', encoding="utf-8", index=False)
transformed_test_pending_ratio = \$ polynomial_features.transform(x_test['Pending Ratio'].values.reshape(-1,1))
pca = PCA().fit(X_std) $
data.loc[:, ['TMIN', 'TMED']].head()
flight.describe("from_city_name").show() 
!rm lazy_helpers.py*$ !wget https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
shows.dtypes
ds_info = ingest.upload_dataset(database=db, dataset=test)$ ds_info
yc_new4.head()
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots$  $ stock_data["CLOSE"].iloc[:].plot() # Plot the adjusted closing price of AAPL
df.reorder_levels(order=['Date', 'Store', 'Category', 'Subcategory', 'UPC EAN', 'Description']).head(3)
import requests$ urlkorbase='http://www.koreabaseball.com/Record/Player/HitterBasic/Basic1.aspx'$ data=requests.get(urlkorbase).text$
frame = pd.DataFrame(data)$ frame
wordsearch_dict = {}$ for word in order.keys():$     wordsearch_dict.update({word: df.text.str.extractall(r'({})'.format(word), re.IGNORECASE).size})
df=pd.read_csv("Monika__link_syn11_sel.csv")#,encoding="cp1252")#"utf-16")#$ df.head()$
events_df['event_time'] = events_df['event_time'].apply(lambda s: datetime.datetime.strptime(str(s),'%Y-%m-%d %H:%M:%S'))
feature_set = layer.query(out_sr={'wkid': 4326})$
df = DF.copy()$ df = df[24:]$ print("... created a copy and cropped odd convos from start")
precip_data_df.head(3)
mars_html_table = mars_table.to_html()$
df['Category']=df['Memo'].apply(returnCategory)$ df['Single Name']=df['Name'].apply(returnName)$ df.head()
df_comms = pd.DataFrame(columns=['comm_id','comm_msg','comm_date'])
months = months.dropna(subset = ['birth year'])$ print(months.shape)$ print(months.head(5))
b = news_df[news_df['Source Acc.'] == 'nytimes']$ b.head()$ print(b['Compound Score'].sum())
P_new = ab_df2['converted'].mean()$ print(P_new)
import time$ print (time.strftime("%Y%m%d"))
Largest_change_between_any_two_days = mydata['Close'].diff().max() $ Largest_change_between_any_two_days
predictions = lrmodel.transform(testData)
average_math_score = df_students['math_score'].mean()$ average_math_score
temp_df.groupby('reorder_interval_group')['Order_Qty'].mean()
def cosine_similarity(u, v):$     return(np.dot(u, v)/np.sqrt((np.dot(u, u) * np.dot(v, v))))
tweet_df["tweet_source"].unique()
d.weekday()
import csv$ data_with_header = list(csv.reader(open("test_data//askreddit_2015.csv", encoding="utf8")))$ data = data_with_header[1:len(data_with_header)]
tc = pd.concat([tce2, tcp], ignore_index=True)$ tc
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path)$ df.head(5)
print(sum(receipts.duplicated(['parseUser','reportingStatus','iapWebOrderLineItemId','iapPurchaseDate'])))$ print(sum(receipts.duplicated(['iapWebOrderLineItemId'])))
dfg = dfg.set_index(['drg3', 'discharges'])$
sum([1 for row in U_B_df.cameras if len(row) > 2])
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.tail(10)
a=contractor_clean.groupby(['contractor_id','contractor_bus_name'])['contractor_bus_name'].nunique()$ print a[a >1]$ contractor_clean[contractor_clean.contractor_number.duplicated() == True]
new_discover_sale_transaction = post_discover_sales[post_discover_sales['Email'].isin(new_customers_test['Post Launch Emails'].unique())]$ new_discover_sale_transaction['Total'].mean()
clf.predict(users_to_predict)$
df.resample('H').mean()
[x.text for x in html.find_all('a', {'class':'next '})]
top_songs['Year'] = top_songs['Date'].dt.year
p_diffs = np.array(p_diffs)$ p_diffs
df.head()
temporal_group = 'weekly'$ df = pd.read_csv('../data/historical_data_{0}.csv'.format(temporal_group))
topTweeters = data.groupby(by=['userScreen', 'userName'])['Tweets'].count()$ topTweeters.sort_values(ascending=[False]).head(10)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\mydata.json"$ df = pd.read_json(path)$ df.head(5)
ts.get_report_data(2018,1)
data2.SA = data2.SA.astype(str)
for leg in plan['plan']['itineraries'][0]['legs']:$     print('distance = {:,.2f} | duration = {:.0f} | mode = {} | route = {} | steps = {}'.\$ format(leg['distance'], leg['duration'], leg['mode'], leg['route'], len(leg['steps'])))
df_tobs.set_index('date',drop=True,inplace=True)
s = pd.Series([99,5,60], index = ['HPI','Int_rate','Low_tier_HPI'])$ df1.append(s,ignore_index=True) # append the series to the data frame. The line 4 is appended$
Base.classes.keys()
df.head()
workspace_uuid = ekos.get_unique_id_for_alias(user_id, 'lena_indicator')$
pax_raw.groupby('seqn').paxcal.mean().value_counts()
df_2017['bank_name'] = df_2017.bank_name.str.split(",").str[0]$
yc_new2.rename(columns={'Tip_Amt':'tipPC'}, inplace=True)$ yc_new2.head()
crosstab_transformed = pca.transform(crosstab)
!rm -fv .cache/runtimes.csv
data_df['clean_desc'] = data_df.apply(text_clean, axis=1)$ data_df['clean_desc'] = data_df['clean_desc'].astype(unicode)$ data_df['nwords'] = data_df['clean_desc'].str.count(' ').add(1)
df = df.drop_duplicates()$
df_con_treat = df_con1.query('group =="treatment"')$ x_treat = df_con_treat["user_id"].count()$ x_treat$
df_newpage = df2.query('landing_page =="new_page"')$ x_newpage = df_newpage["user_id"].count()$
aggregates['Email Address'] = aggregates['Email Address'].apply(lambda x: str(x.lower().strip()))$ aggregates['Email Address'] = aggregates['Email Address'].astype(str)
%matplotlib inline $ import matplotlib.pyplot as plt # this imports the plotting library in python}
bool(login_page.find_all(string=re.compile('redirect')))
stat_info_city = stat_info[1].apply(fix_space)$ print(stat_info_city)
Total_Number_of_Rides_max = rides_analysis["Total Number of Rides"].max()$ Total_Number_of_Rides_max
ride_df_suburban = suburban.groupby('city')$ city_df_suburban = suburban.set_index('city')
festivals_clean.info()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ request = requests.get(url)
output.coalesce(2).write.parquet("/home/ubuntu/parquet/output.parquet/output.parquet")
S_1dRichards.decision_obj.groundwatr.options, S_1dRichards.decision_obj.groundwatr.value
data['age'][data['age'] < 0] = 365+data['age']$ data.iloc[140:170,]$
temp_df2.shape
dict(list(result.items())[:20])
print(df.shape)$ print(df.describe())
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY$ r = requests.get(url)$ json_data = r.json()
suburban_avg_fare = suburban_type_df.groupby(["city"]).mean()["fare"]$ suburban_avg_fare.head()$
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
counts = review_df.groupby('author').size()$ df2 = pd.DataFrame(counts, columns = ['size'])$ df2[df2['size']>1]
filename = processed_dir+'pulledTweetsProcessedAndClassified_df'$ gu.pickle_obj(filename,pulledTweets_df)
   $ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=amQDZBVZxsNFXgn8Dmpo')$ r.json()['dataset']['data'][1]
S_distributedTopmodel.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
tot_station= session.query(station).count()$ for station in session.query(station.station):$     print(station)
print(open(os.path.join(PROJ_ROOT, 'requirements.txt')).read())
rain_df.describe()
df_urls = pd.read_csv('ny_times_url_dataframe.csv', encoding = 'utf-8')$ df_urls.shape$
top_10_authors = git_log.groupby('author').count().apply(lambda x: x.sort_values(ascending=False)).head(10)$ top_10_authors
datesStr=dates.strftime('%Y-%m-%d')
y = K.dot(x, W) + b$ loss = K.categorical_crossentropy(y, target)
!hdfs dfs -put ProductPurchaseData.txt ProductPurchaseData.txt
twitter_df_us=twitter_df[twitter_df.location=='United States']$ plt.plot(twitter_df_us['created_at_time'], twitter_df_us['retweet_count'], 'ro')$ plt.show()$
data.isnull().sum()
userMovies = userMovies.reset_index(drop=True)$ userGenreTable = userMovies.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ userGenreTable
df_subset['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=False, logy=False)$ plt.show()
validation.analysis(observation_data, BallBerry_resistance_simulation_1)
tweetdf = pd.read_csv('../../data/clean/tweets_w_lga.csv') # shortcut
my_stocks = ["WIKI/AAPL.1", "EIA/PET_RWTC_D.1", "NSE/OIL.1"]$ import quandl as q$ stock_open = q.get(my_stocks, start_date="2017-01-01", end_date="2018-02-01")
pd.to_datetime(segments.st_time[:])
data.plot(figsize=(16,6))$ plt.show()
mRF = H2ORandomForestEstimator()$ mRF.train(['sepal_len','sepal_wid','petal_len','petal_wid'],'class',train)
data['SA'] = np.array([analyze_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
for div in soup.find_all('div'):$     print (div.text)
stop_words = nltk.corpus.stopwords.words('portuguese')
df_tobs = pd.DataFrame(data)$ display(df_tobs.head())$ display(df_tobs.info())
chinese_vessels_iccat = pd.read_csv('iccat_china_active_13mar2017.txt', sep='\t', encoding='utf-8')
df_low_temps.describe()$
soup.find('div', class_='movie-add-info left').find_all('li')
model.wv.syn0.shape
newdf = pd.merge(bc, ts, left_index=True, right_index=True)
data.isnull().sum()
multi.handle.value_counts() / multi.shape[0]
df = pd.read_csv('./dataset/master_06-02-2018(hotposts).csv')$ df.drop(columns='Unnamed: 0', axis=1, inplace=True)
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(5))
get_nps(combined_df, 'role_en').sort(columns='score', ascending=False)
len(df_measurement['station'].unique())$
df['gathering'].fillna(0, inplace = True)
new.Purchased.value_counts()/len(new)*100$
sauce = urllib.request.urlopen('https://pythonprogramming.net/sitemap.xml').read()$ soup = bs.BeautifulSoup(sauce, 'xml')$ soup
!wget -nv https://www.py4e.com/code3/mbox.txt -O mbox.txt
player['event'] = np.where((player['events'] == 'home_run'), #|$                            1, 0)$ player['event'].sum()
log_user1 = df_log[df_log['user_id'].isin([df_test_user['user_id']])]$ print(log_user1)
manager.image_df['p_hash'].isin(tree_features_df['p_hash']).describe()
num_stations = session.query(func.count(hi_stations.STATION.distinct())).scalar()$ print("Number Of Stations: " + str(num_stations))
ad_source = questions['ad_source'].str.get_dummies(sep="'")
df = pd.read_csv("../../data/msft_with_footer.csv",skipfooter=2,engine='python')$ df
train_Features, test_Features, train_species, test_species = train_test_split(Features, species, train_size=0.5, random_state=0)
%matplotlib inline$ import matplotlib.pyplot as plt
C = np.bincount([1,1,2,3,4,4,6])$ A = np.repeat(np.arange(len(C)), C)$ print(A)
cityID = 'a592bd6ceb1319f7'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Diego.append(tweet) 
df.index = df.datetime
ADP_array=df["NASDAQ.ADP"].dropna().as_matrix()$
X = data.values$ from sklearn.preprocessing import StandardScaler$ X_std = StandardScaler().fit_transform(X)
pd.Series([42, 13, 2, 69])
Base = automap_base()$ Base.prepare(engine, reflect= True)$ Base.classes.keys()$
preds = gbm.predict(T_test_xgb)
model4 = MNB.fit(x_train, y_train)$ model5 = SGDC.fit(x_train, y_train)
feedbacks_stress.loc[feedbacks_stress['versao'] == 2, ['incomodo', 'interesse1', 'interesse2'] ]*=2$
ngrams_summaries = cvec_4.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
automl = pickle.load(open(filename, 'rb'))
s4.unique()
gdax_trans['Timestamp'] = gdax_trans.apply(lambda row: fix_timestamp(row["Timestamp"]), axis=1)
pulledTweets_df = gu.read_pickle_obj(processed_dir+'pulledTweetsProcessedAndClassified_df')
fed_reg_dataframe = pd.DataFrame.from_records(tuple_lst, columns=['date','str_text'], index = 'date')
df[df['text'].str.contains('appointment')]
df.mean()
reserve_tb[reserve_tb['customer_id'].isin(target)]
mentors = data_final[data_final['countCollaborators'] > 4]['authorId'].unique()$ mentees = data_final[data_final['countCollaborators'] <= 4]['authorId'].unique()
marsfacts = df_mfacts.to_html(bold_rows=True)$ marsfacts$
manager.image_df[manager.image_df['filename'] == 'image_sitka_spruce_71.png']$
B4JAN16['Contact_ID'].value_counts().sum()$
sp500.loc['MMM']
import tensorflow as tf$ x1 = tf.constant(5)$ x2 = tf.constant(6)
collection.delete_snapshot('snapshot_name')$
df.word_count.sum()
breakdown[breakdown != 0].sort_values().plot($     kind='bar', title='Russian Trolls Number of Links per Topic'$ );
results = pd.read_csv('datasets/results.csv')
tweets.dtypes
df_session_dummies = pd.get_dummies(df, columns=['action'])$
cur.execute('INSERT INTO experiments (material_id) VALUES ("EVA")')  # use defaults, $ conn.commit()
(tweets_df["Compound_Score"]<=0).value_counts()$
rain_df.describe()
stock_subset = stock_data[ [ 'open', 'close' ] ] $ print(display(stock_subset.head(5)))
df_=data[['QG_mult','QG_ptD','QG_axis2']].copy()$ df_['target']=data['isPhysG']$ df_.head()
extractor = connectToTwitterAPI()$ tweets = extractor.search(q="#BlackPanther", count=50)$ print("Number of tweets extracted: {}.\n".format(len(tweets)))
type(data.Likes.value_counts())
temp_nc = Dataset("../data/nc/air.mon.mean.nc")
nulls=(c_date.notnull()==False)$ nulls.value_counts()
print(soup.prettify())
s = pd.Series(np.random.randint(0,7,size=10))  #low,high,size $ print(s,'\n')$ s.value_counts()
model = KNeighborsClassifier(n_neighbors=250)$ model = model.fit(train[0:, 1:5], train[0:, 7])
df.dropna().shape == df.shape, df.shape
print("{} is the proportion of users converted.".format(df['converted'].mean()))
h5 = qb.History[QuoteBar](eur.Symbol, timedelta(30), Resolution.Daily)$
display(flight2.show(5))$
nb_topics = 5$ lda = LatentDirichletAllocation(n_components=nb_topics, learning_method='batch', random_state=42)$ lda.fit(vects)
qt_ind_ARR[industries] = qt_ind_ARR[industries].applymap(lambda x: np.float(x))$ qt_ind_ARR
cityID = '1661ada9b2b18024'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Wichita.append(tweet) 
featured_img_url = "https://www.jpl.nasa.gov" + current_img_url$ featured_img_url
fe.bs.csv2ret??
c.loc[c>0.7]
data.groupby(['Year'])['Salary'].mean()
total_tokens_sk = len(all_tokens_sk)$ corpus_tweets_streamed_keyword.append(('total tokens', total_tokens_sk)) # update corpus comparison$ print('Total number of words (including mentions, hashtags and links) in the collection: ', total_tokens_sk)
df.index
sites.shape
dfAnnualMGD = dfHaw_Discharge.groupby('Year')['flow_MGD'].agg(['sum','count'])$ dfAnnualMGD = dfAnnualMGD[dfAnnualMGD['count'] > 350]$ dfAnnualMGD.columns = ['AnnualFlow_MGD','Count']
movies_df = pd.read_csv('movies.csv')$ ratings_df = pd.read_csv('ratings.csv')$ movies_df.head()
S_lumpedTopmodel.basin_par.filename
questions = pd.concat([questions.drop('attend_with', axis=1), attend_with], axis=1)
df1[40:].head(5)
del_id = list(df_never_moved['id'])$ df['Timestamp +2'] = df['Timestamp'].apply(addtwo)
grouped_dpt_city = department_df.groupby(["Department", "City"])
sen_dat = pd.read_msgpack('full_DataFrame/combined_data/master/sentiment_data_2min.msg')$ sen_data2 = pd.read_msgpack('full_DataFrame/combined_data/master/sentiment_data_5min.msg')
vec1.get_feature_names() #feature names in test sample.$
street_median_length =data.groupby("Street")["Street.Length"].median()$ data = data.join(street_median_length, on="Street", rsuffix='_correct')
INT = INT.sort_values(by = ['Contact_ID','Int_Name'])
average_reading_score = df_students['reading_score'].mean()$ average_reading_score
mars_df = pd.read_html(mars_facts_url)$ mars_facts_df = pd.DataFrame(mars_df[0])
tweets_df.userLocation.value_counts()
from quantopian.pipeline.data import Fundamentals$ exchange = Fundamentals.exchange_id.latest
pathdataOH = np.repeat(newPaths[idxKeep], invals)$ oldpath = np.repeat(idxOP[idxKeep], invals)
df = pd.read_csv('local_newspapers.csv')$ df.head()
gb.agg(['sum', 'count'])    $
import pandas as pd$ df = pd.DataFrame(query_op)$
df_date = df_date['unique_date'].apply(lambda x: x.strftime("%Y-%m-%d %H:%M:%S"))$
with open('total_review_apps_eng_lower.pickle', 'rb') as d:$     total_review_apps_eng_lower = pickle.load(d)
print(loan_stats["revol_util"].na_omit().min())$ print(loan_stats["revol_util"].na_omit().max())$
y_hat = nb.predict(train_4)
df = pd.read_sql(SQL, db)$ df
dflong = pd.read_csv("SP500_Long_V4.CSV")$ print dflong.shape$
tweetsIn22Mar.head()$ tweetsIn1Apr.head()$ tweetsIn2Apr.head()
df["result"].plot()$ plt.show()
d_housing=detroit_census2.drop(detroit_census2.index[:24])$ d_housing=d_housing.drop(d_housing.index[5:])$ d_housing
for v in contin_vars:$     joined[v] = joined[v].fillna(0).astype('float32')$
img_url_valles = soup.find('div', 'downloads').a['href']
print('Total items received: {:,.0f}'.format(items_received(df_receipts, rpt_dt_start, rpt_dt_end, vendor_num)))
plantlist.capacity_net_bnetza = plantlist.capacity_net_bnetza.round(decimals=5)$ plantlist.capacity_gross_uba = plantlist.capacity_gross_uba.round(decimals=5)  $ plantlist.efficiency_estimate = plantlist.efficiency_estimate.round(decimals=5)
import test_package.print_hello_function_container$ import test_package.print_hello_class_container$ import test_package.print_hello_direct # note that  the paths should include root (i.e., package name)$
cityID = '27485069891a7938'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_York.append(tweet) 
DataAPI.write.update_secs_industry_sw(industry="A_SWL1", trading_days=trading_days, override=False)
df_CLEAN1A = pd.read_csv(url_CLEAN1A,sep=',')$ df_CLEAN1B = pd.read_csv(url_CLEAN1B,sep=',')$ df_CLEAN1C = pd.read_csv(url_CLEAN1C,sep=',')
deployment_details = client.deployments.create(model_uid, "My NLC Car model deployment")
Geocoder.geocode("4207 N Washington Ave, Douglas, AZ 85607").valid_address
tobs_date_df = pd.DataFrame.from_records(tobs_date)$ tobs_date_df.head()
!head -n 5 ProductPurchaseData.txt
import json$ list_of_issues_dict_data = [json.loads(line) for line in open('SPM587SP18issues.json')]
commonwords=set(list(zip(*wordfrequencylist))[0]).intersection(list(zip(*hashtaglistw))[0])$ frequentcommonwords=set(list(zip(*wordfrequencylist[:200]))[0]).intersection(list(zip(*hashtaglistw[:200]))[0])
data.loc[data.L2 == ' ', ['L2']] = np.nan$ data.L2.unique()
features = pd.merge(features, form_btwn_teams.drop(columns=['margin']), on=['game', 'team', 'opponent'])
tweetdf.loc[tweetdf['lat'].notnull(),'lga'] = np.asarray(output)$
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
com_eng_df['issues_closed_total'].plot()
get_nps(combined_df, 'language').sort(columns='score', ascending=False).head(10)
tweet_df = pd.DataFrame(tweet_data)$
psy_prepro = pd.read_csv("psy_prepro.csv")$ psy_prepro = psy_prepro.set_index('subjectkey', verify_integrity=True)$
ride_df_rural = rural.groupby('city')$ city_df_rural = rural.set_index('city')
plot_autocorrelation(series=dr_num_new_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of new patients'))$ plot_autocorrelation(series=dr_num_existing_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of existing patients'))
import pandas as pd$ df = pd.DataFrame(tweets_data, columns=['text', 'id'])$ print(df.head())
df.to_csv("NewsMood.csv")
logreg = LogisticRegression()$ logreg.fit(X_train, y_train)$ logreg.score(X_test, y_test)
summaries = "".join(df.title)$ ngrams_summaries = cvec_1.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
os.getcwd()$ model_path = 'task1_pm_model.h5'
len([earlyPr for earlyPr in BDAY_PAIR_df.pair_age if earlyPr < 3])/BDAY_PAIR_df.pair_age.count()
len(df[~(df.groups == {})])
stopwords = ['你','\n','的','也','是','在','讓','都','了','有','與']$ puncs = '✨！？｡"#+＃＄％＆＇>（）＊＋，－／!：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.。'
data_file_path = "/Users/gandalf/Documents/coding/do_not_commit/capstone/"$ website_file_path = '/Users/gandalf/Documents/coding/rczyrnik.github.io/capstone/'
df.sample(10)
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:10]
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
ts.tshift(5,freq="H")
predicted_values = model_NB.predict_proba(X_test)
json_data= response.json()['dataset_data']$ print(json_data)$
eval_data = pd.concat([predictions, actuals])$ eval_data['day'] = eval_data.index
a = tweets.apply(lambda row: countOcc(row['tokens']), axis=1)$ sorted_x = sorted(occ.items(), key=operator.itemgetter(1), reverse=True)$ sorted_x
plt.hist(p_diffs);
base_url = 'https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_'
DataSet.head()
edftocsv.edftocsv(inputFile, outputFileHeaders, outputChanHeaders, outputData, True)
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'ab_page','US', 'UK']])$ result = logm2.fit()$ result.summary2()
data.keys()
wqYear = dfHawWQ.groupby('Year')['TotalN'].mean()$ dfAnnualN = pd.DataFrame(wqYear)
unique, counts = np.unique(y_hat, return_counts=True)$ print(unique, counts)
tweet_group_df = tweet_df.groupby(["Target"])['Compound'].agg(['mean']).sort_index().reset_index()$ tweet_group_df = tweet_group_df.rename(columns={"mean":"Tweet Polarity"})$ tweet_group_df.head()
my_gempro.kegg_mapping_and_metadata(kegg_organism_code='mtu')$ print('Missing KEGG mapping: ', my_gempro.missing_kegg_mapping)$ my_gempro.df_kegg_metadata.head()
trn_lm, val_lm = sklearn.model_selection.train_test_split(np.array(df["numerized_tokens"]), test_size=0.1)$ len(trn_lm), len(val_lm)
df_user[df_user['user.name'].str.contains('marco rubio', case=False)]
precip = session.query(Measure.date, Measure.prcp).\$     filter(Measure.date > '2016-08-23').\$     order_by(Measure.date).all() 
high_sta_obs = session.query(measurements.station, func.count(measurements.stations)).\$                 group_by(measurements.station).order_by(measurements.tobs).all()$ high_sta_obs
df = pd.read_csv('jamesblanchard_tweets.csv')$ print(df)
r= requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json")$ json_data = r.json()$ json_data["dataset_data"]["data"][0]
!open resources/html_table_marsfacts.html
crimes.drop('LOCATION', axis=1, inplace=True)
df.date
npath = out_file3$ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
sites = pd.read_csv('../data/station.csv',$                    dtype={'HUCEightDigitCode':'str'})
df = DataObserver.build_simply(file_path= '/Users/admin/Documents/Work/AAIHC/AAIHC-Python/Program/DataMine/Reddit/json_data/Processed_DataFrames/r-news/DF-version_2/DF_v2.json')$ cw_df = df[['ups', 'downs', 'category', 'sentiment_score', 'sentiment_magnitude', 'date_created', 'time_created']]
df.info()$
X = df[['word_count','sentiment','subjectivity','domain_d','post_duration']]$ y_cm = LabelEncoder().fit_transform(df['com_label'])#Comments$ y_sc = LabelEncoder().fit_transform(df['score_label'])# Score$
category_count_df2 = category_count_df1.loc[category_count_df1["category"] >= 800 , :] $
time_series['count'].sum() == (nodes.shape[0] + ways.shape[0])
df_group_by.head(20)
ekos.load_workspace(user_id, alias = workspace_alias)$
databreach_2017 = pd.read_csv('databreach_2017.csv')
g = sns.FacetGrid(data=dataset, col='rating')$ g.map(plt.hist, 'text length', bins=50)$
news_df = news_df.sort_index()$ news_df.head()
 $ mfacts = pd.read_html(marsfacts_url)$ mfacts
df['text']=df['title'].str.replace('\d+', '')$
df[(df["compound"]==0)].groupby("newsOutlet").count()
precip_df.describe()
REDASH_AUTH_URL = 'https://nanit-bi:pcjxg72f3yat@redash.nanit.com/api/queries/154/results.json?api_key=LcuoHqjZLvxaSPDrhv5VMhcrJUyPVb88RJR69REq'$
predictions=model.predict(X)$ rounded =[round(x[0]) for x in predictions]$ print(rounded)
s4.count()
df.sort_values(by=['title','num_comments'],inplace =True)$ df.head()
f = e.instance_method$ e.instance_var = 'e\'s instance var'$ f()
from pyspark.ml.classification import LogisticRegression$ lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)$
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2014-01-01&end_date=2014-01-02&api_key=S9bQCgmEaGHM8HSDLBNo')$ r.status_code
plt.savefig("Scatter.jpg")$ plt.show()
store_items = store_items.drop(['store 1', 'store 2'], axis=0)$ store_items
(category_count_df2["category"].sum() / category_count_df1["category"].sum()).round(2)$
today_ = datetime.date.today().strftime("%Y_%m_%d")$ today_ = '2018_06_07'$ print("today is " + today_)
ts.get_cashflow_data(2018,1)
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'$ browser.visit(url)
load_weigths_into_target_network(agent, target_network) $ sess.run([tf.assert_equal(w, w_target) for w, w_target in zip(agent.weights, target_network.weights)]);$ print("It works!")
df.to_csv(save_dir)
html_table_marsfacts = html_table_marsfacts.replace('\n', ' ')$ html_table_marsfacts
print(datetime.now() - timedelta(hours=1))$ print(datetime.now() - timedelta(days=3))$ print(datetime.now() + timedelta(days=368, seconds=2))
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))
df_ratings.shape
type(ts.index[0])
sentiments_pd.to_csv("quickbook_competitors_tweet_data.csv")
X_test = count_vect.transform(df_test.text)
wordfreq = FreqDist(words_sk)$ print('The 100 most frequent terms, including special terms: ', wordfreq.most_common(100))
volt_prof_before.set_index('Bus', inplace=True)$ volt_prof_after.set_index('Bus', inplace=True)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-12-29&end_date=2017-12-29&api_key=' + API_KEY)$ r.json()
target_user = ("@BBC", "@CBSNews", "@CNN","@FoxNews", "@nytimes")$ total_tweets = pd.DataFrame(columns=["Name","Tweet Order","Text","Compound","Positive","Negative","Neutral","Time","Adj Time"])$
afx_x_oneday = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2014-12-01&end_date=2014-12-01&api_key=F57SPh3gyaXMHjLAp-pY')$
pickle_in = open('linearregression.pickle', 'rb')$ clf = pickle.load(pickle_in)
options_frame.info()
df = turnstile_df.reset_index()$ df.columns = [col.strip() for col in df.columns]$ df.sample(5)
to_plot = customer_emails[['Email', 'Days Between Int']]$ plt.title("Actual number of days between purchases")$ sb.distplot(to_plot['Days Between Int'].dropna())$
Bot_tweets = data.drop(['text','text_lower'],axis=1)$ Bot_tweets.head()
prev_year = dt.date.today() - dt.timedelta(days=365)$ prev_year
df.describe()
dictionary = corpora.Dictionary(texts)$ dictionary
precip_data_df.head(3)$
t1.head(5)
import pandas as pd$ import os$ os.chdir('C://Users/dane.arnesen/Documents/Projects/pytutorial/')
data.info()
dfprediction = pd.DataFrame(data={'y_hat': rf.predict(X_btc_test)}, index=X_btc_test.index)
tOverweight = len(df[(df['bmi'] >= 25.0) & (df['bmi'] < 30.0)])$ tOverweight
model = model_simple_nmt(len(human_vocab), len(machine_vocab), Tx)$ model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
df['receive_time'] = pd.to_datetime(df['receive_time'])$ df['created_at'] = pd.to_datetime(df['created_at'])
latest_version = str(d.id[0])$ print latest_version
adjmats, timepoints = pt1.runltvmodel(rawdata, numwins=10)
pprint.pprint(test_collection.find_one())
dataframe.drop(['Date'],inplace=True,axis=1)$ scaler = MinMaxScaler(feature_range=(0, 1))$ scaled = scaler.fit_transform(dataframe)
store_items.dropna(axis = 0)
df.set_index('UPC EAN', append=True, inplace=True)$ df.head(3)
stringToWrite = 'A line\nAnother line\nA line with a few unusual symbols ␡ ␛ ₠ ₡ ₢ ₣ ൠ\n'$ with open(example_text_file, mode = 'w', encoding='utf-8') as f:$     f.write(stringToWrite)
df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1]/100) for tup in pops_list]$ df_pop_ceb.plot(kind='scatter', x='Year', y='Total Urban Population')$ plt.show()
X2.time_since_meas_years.hist()
happiness_df=pd.DataFrame(columns=['dates','happiness'])$ for j in range(0,len(happiness)):$             happiness_df.loc[j]=[str(time[j])+"-12-31"+"T00:00:00Z",happiness[j]*33.33]$
series1['1975-12-31':'1980-12-31'].plot()$ plt.show()
data_file='data/airports.csv'
close_px_all = pd.read_csv('stock_px.csv', parse_dates=True, $                            index_col=0)$ close_px_all.head() # data 2003-2011, >2000 rows
df2['timestamp'] = pd.date_range('8/8/2018', periods=len(df2['MATCHKEY']), freq='D')
data_df.groupby('topic')['ticket_id'].nunique()
file = open("./data/page.txt", "w")$ file.write(html)$ file.close()
usecols=[0,6,21,30,31,32,33,34,58,59,60]$ nitrogen = nitrogen.iloc[:,usecols]$ nitrogen.columns
movie.info()$
co_occurence_on_top50 = {ds: [(el[0], str(el[1])) for el in datasets_co_occurence[ds]] for ds in top50.dataset_id.tolist()}
%%time$ pq.write_table(crime_geo_table, data_dir + file_name + '.parq')
mean = mc_estimates.expanding().mean()
measure_df.info()$
df1 = pd.DataFrame(np.random.randn(6,3),columns=['col1','col2','col3'])$ df2 = pd.DataFrame(np.random.randn(2,3),columns=['col1','col2','col3'])$ print(df2.reindex_like(df1))
session.query(Measurement.station).distinct().count()
lsi.save('trump.lsi')$ lsi = models.LsiModel.load('trump.lsi')
type.__new__(type,'A',(),{"a":1})
engine = create_engine("sqlite:///hawaii.sqlite")
data = r.json()
np.intersect1d(top_10_KBest, top_10_elnet)
day_change = [(daily[2]-daily[3]) for daily in afx_17['dataset']['data']]$ print('Largest change in 1 day based on High and Low price is $%.2f.' % max(day_change))
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'pure_scorers'
query.get_dataset(db, id=ds_info["DatasetId"][0], columns="ndarrays", get_info_items=True)
import sklearn.feature_extraction.text$ vctr2 = sklearn.feature_extraction.text.CountVectorizer(tokenizer = lambda key_phrases_rake: key_phrases_rake, preprocessor=lambda key_phrases_rake: key_phrases_rake)$
new_df = df.fillna(0)$ new_df
scoring_url = client.deployments.get_scoring_url(deployment_details)$ print(scoring_url)
station = Base.classes.stations
X_test[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
us_cities = pd.read_csv("us_cities_states_counties.csv", sep="|")$ us_cities.head()$
z_stat, p_val = stats.ranksums(virginica, versicolor)$ print('MWW RankSum p-value %0.15f' % p_val)
print("prediction: {0}, actual: {1}, 2018-03-31".format(y_pred_list[0], y_test_aapl[0,0]))
from pprint import pprint # ...to get a more easily-readable view.$ pprint(example_tweets[0])
data_kb['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_kb['Tweets'] ])$ display(data_kb.head)
twitter_df.to_csv("twitter_data.csv")
x_train,x_test,y_train,y_test=train_test_split(df_predictors,df_tripduration,test_size=.2, random_state=1)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(doc_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours Autocorrelation')
sorted(entity_relations.items(), key=lambda x: x[1], reverse=True)
weather_features = weather_features.resample('D').sum()
driver = selenium.webdriver.Safari() # This command opens a window in Safari$ driver.get("https://xkcd.com/")
files4= files4.rename(columns={'jobid':'jobId'})$ files4['jobcandidate']=files4['jobId'].astype(str)+'-'+files4['candidateid'].astype(str)
cgmStats = get_stats(cgm)$ cgmStats.T
[random_date(pd.datetime.now() - pd.offsets.Day(10), pd.datetime.now()) for _ in range(10)]
df['age'] = df['fetched time'] - df['created_utc']$ df['age'] = df['age'].astype('timedelta64[m]')$ df.head()
logit_countries = sm.Logit(df4['converted'], $                            df4[['country_UK', 'country_US', 'intercept']])$ result2 = logit_countries.fit()
ab_df_new['treatment_US'] = ab_df_new.ab_page * ab_df_new.US$ ab_df_new['treatment_CA'] = ab_df_new.ab_page * ab_df_new.CA$ ab_df_new.head()
plate_appearances = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).first().reset_index()
thai_province_list = set(df.loc[:,'province_in_thai'])$ len(thai_province_list) ## it have 77 province
store = pd.HDFStore("../../data/store.h5")$ df = store['df']$ df
test = vec1.fit_transform(df.message[1]) #takes 2. row in df for testing$ for i in test:$     print(i)$
df_game_download['game_format_name'] = df_game_download['game_name'].apply(remove_punctuation)$ df_download_num = df_game_download.groupby(['game_format_name','source'])['download_num'].max().reset_index()  # reset_index() 不能缺$ df_download_num[df_download_num['game_format_name'] == '王者荣耀'].head()
term_freq_df.sort_values(by='total', ascending=False).iloc[:10]
def create_soup(x):$     return ''.join(x['categoryname']) + ', ' + ''.join(x['eventname']) + ', ' + ''.join(x['location'])
! find /home/ubuntu/s3/flight_1_5/extracted/final_results -name '*.txt' -exec mv {} /home/ubuntu/s3/flight_1_5/extracted \;$ ! rmdir /home/ubuntu/s3/flight_1_5/extracted/final_results
new_df = df.fillna(method = 'ffill')$ new_df
bd.columns.name = "Data"$ bd
print(count_all.most_common(10))
clean_measure = measure.fillna(0)
AAPL.to_csv('/home/hoona/Python/mpug/directFromPandaDataFrame.csv')
free_data.mean(),free_data.count()
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'})$ station_count = station[['Count']]$ station_count $
daily_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner')$ daily_df.head()
file_name = "./data/train_preprocessed2.csv"$ train_df2 = pd.read_csv(file_name, low_memory = False)$ train_df2.head()
dr_existing_8_to_16wk_arimax = dr_existing_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ dr_existing_8_to_16wk_arimax.index = dr_existing_8_to_16wk_arimax.index.date
tdf = sns.load_dataset('tips')$ tdf.sample(5)
data.printSchema()
response = requests.get('http://www.reddit.com/r/aww')$ page_source = response.text$ print(page_source[:1000])
for url in soup.find_all('a'):$     print (url)
merged1['DaysFromAppointmentCreatedToVisit'] = (merged1['AppointmentDate'] - merged1['AppointmentCreated']).dt.days
df = pd.get_dummies(df, columns = ['new_home', 'hail_resistant_roof'],$                     drop_first=True)
df['label'] = df[forecast_col].shift(-forecast_out)
pystore.set_path('./pystore_demo')$ pystore.get_path()
count_authors_with_given_numer_publications = data_final.groupby('countPublications', as_index=False)['authorId'].count()$ count_authors_with_given_numer_publications.columns = ['number_publications', 'how_many_authors_with_given_publications']$ count_authors_with_given_numer_publications.head(20)
df.to_json('twitter_data_5aug.json')
gbm = GradientBoostingClassifier(max_depth = 8, n_estimators= 200, max_features = 0.3)$ gbm.fit(X_reshaped, y)$ getAUC(gbm, X_reshaped, y, X_test_reshaped, y_test)
df2.tail(2)
prcp_gb_year['prcp'].describe()
zf_train = zipfile.ZipFile('train.csv.zip', mode='r')$ zf_test = zipfile.ZipFile('test.csv.zip', mode='r')
sentiments_pd.to_csv("News-Tweet-Sentiments.csv")
CBS = news_df.loc[(news_df["Source Account"] == "CBSNews")]$ CBS.head(2)
for columns in DummyDataframe:$     basic_plot_generator(columns, "Graphing Dummy Data" ,DummyDataframe.index, DummyDataframe)
top_100_2016['release_date'] = pd.to_datetime(top_100_2016['release_date'], format = '%Y-%m-%d' )
contractor[contractor.contractor_number.duplicated() == True]
df6 = df5.Completed_Date.groupby([df5.Completed_Date.dt.year.rename('Completed_Year'), df5.Completed_Date.dt.month.rename('Completed_Month')]).agg('count')$ print(df6)
periods = 31 * 24$ hourly = Series(np.arange(0,periods),pd.date_range('08-01-2014',freq="2H",periods=periods))$ hourly
prec_long_df = pd.melt(prec_wide_df, id_vars = ['grid_id', 'glon', 'glat'],$                       var_name = "date", value_name = "prec_kgm2")$ prec_long_df.head()
importances=model_rf_14_b.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
df.columns = ['ID', 'name', 'category', 'Main category', 'currency', 'deadline',$               'goal', 'launched', 'pledged', 'State', 'Backers', 'country',$               'usd pledged', 'Pledged (USD)', 'Goal (USD)']
ps.sort_index().tail(600)
sentiments_pd.to_excel("NewsMood.xlsx", encoding="latin-1") 
out=final.sort_values(by=['priority','RA0'])$ out=out[['name','RA0','DEC0','redshift','min_air','best_time','priority']]$ out.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/all_objs_winter2017.csv',index=False)$
gmm.fit(X)$ labels = gmm.predict(X)$ labels
data[(data['author_flair'] == 'Saints') & (data['game_state'] == 'close')].comment_body.head(15)$
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots$  $ temp_data["CLOSE"].iloc[:].plot() # Plot the adjusted closing price of AAPL
year_prcp_df = pd.read_sql_query(year_prcp.statement, engine,index_col = 'date')$ year_prcp_df.head()
group_period = '7D'$ cc_df = cc_df.resample(group_period).mean()$ cc_df.head()
cityID =  '4b25aded08900fd8'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Reno.append(tweet) 
newfile.to_excel('most_excellent.xlsx', sheet_name='test_sheet')
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\olympics.dta"$ df = pd.read_stata(path)$ df.head(5)
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
dfLikes.groupby("año").agg({"id": "count"})
git_log = git_log.sort_index()$ git_log[git_log['author'] == 'Linus Torvalds'].head(10)
executable_path = {'executable_path': 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe'}$ browser = Browser('chrome', **executable_path, headless=False)$ url = 'https://mars.nasa.gov/news/'
train_features = bind_features(train_reduced, train_test="train").cache()$ train_features.count()
text = "Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers."$ print(text)
cur.fetchall()
result_df = pd.DataFrame({'profile_id':profile_ids})$ result_df['num_events'] = result_list
old_page_converted=np.random.choice([1,0],size=n_old,p=[pold,(1-pold)])$ old_page_converted.mean()
data_hpg_reserve['hpg_store_id'].drop_duplicates().count()
twitter_daily_df = twitter_daily_df.join(combined_text, ["Day","Company"]).orderBy('Day','Company')
import warnings$ warnings.simplefilter("ignore", UserWarning)$ warnings.simplefilter("ignore", FutureWarning)
latest_time_entries = toggl.request("https://www.toggl.com/api/v8/time_entries")
stories.submitter_user[0]
temp_data=stock_data[stock_data.SYMBOL == 'ABB'] $ temp_data=temp_data.rename(columns={"OPEN": "Open", "HIGH": "High", "LOW": "Low", "CLOSE": "Close", "TIMESTAMP": "Date"})$
%config InlineBackend.figure_format = 'svg'$ %config InlineBackend.figure_format = 'retina'
data_read = pd.read_csv("data_music_replay_train")$ data = data_read.copy()
bird_data['timestamp'] = pd.Series(timestamps, index = bird_data.index)$ print(bird_data.timestamp.head())
uber = pd.read_csv('https://assets.datacamp.com/production/course_2023/datasets/nyc_uber_2014.csv')$ print(uber.shape)$ print(uber.head())
display(data.head(10))$
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY$ data = requests.get(url)
news_title = soup.find('div', class_='content_title').text.strip()$ news_title
mean = np.mean(data['len'])$ print("the mean length of the tweets is: {}".format(mean))
sentiment_df.to_csv("sentiment.csv", encoding = "utf-8-sig", index = False)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-06-27&end_date=2018-06-27&apikey='+API_KEY)
traded_volume =[d[dt]['Traded Volume'] for dt in d.keys()]$ average_trading_volume = sum(traded_volume)/len(traded_volume)$ average_trading_volume$
int_and_tel.plot(y='sentiment_score', use_index= True, figsize=(18, 6))
season07 = ALL[(ALL.index >= '2007-09-06') & (ALL.index <= '2008-02-03')] # This means every transaction between 9-6-07 and$
ideas = categorical.join(ideas)
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);
uni = free_data[free_data['educ']<=5]$ uni.groupby('country')['country'].count()
activity_df = pd.DataFrame(joined_data)
plt.savefig(str(output_folder)+'NB01_7_windfield_vs_NDVIchange'+str(cyclone_name)+'_'+str(location_name))
df.iloc[:4]
%%bash$ sed 's/^chr//g' post/allV.tab | (head -n 1 - && tail -n +2 - | LANG=C sort) |\$     LANG=C join -t$'\t' --header - ../tools/rs_37_sort.txt > post/allV_RS.tab
prec_wide_df = pd.concat([grid_df, prec_df], axis = 1)$ prec_wide_df.head()
df7 = df6.rename('Completed_Permits').reset_index()
display(flightv1_1.select('timeline_leg1.getItem(0)').show(100, truncate=False))$
pax_raw.paxstep.max() <= step_threshold
model_w.summary2() # For categorical X.
client.experiments.list_runs()
from sklearn.mixture import GaussianMixture$ gmm = GaussianMixture(2)
data[['TMAX']].head()
for v in data.values():$     if v['answers']['Q4'] == 'No':$         v['answers']['Q4A'] = 'n/a'
client.repository.ExperimentMetaNames.show()
tcat_df = tcat_df.append(tcat)$ tdog_df = tdog_df.append(tdog)
zf = zipfile.ZipFile(path)$ df = pd.read_excel(zf.open('Sample_Superstore_Sales.xlsx'))$ df.head(5)
testurl = r"https://www.cobbgis.org/openimage/bravescam/Cam128/Cam_12818-06-27_21-00-09-03.jpg"$ get_datetime_from_cobburl(testurl)
Measurements = Base.classes.metDF$ Stations = Base.classes.logDF
data[(data['author_flair'] == 'Broncos') & (data['win_differential'] >= 0.9)].comment_body.head(15)$
store_items.dropna(axis=1) # or store_items.dropna(axis=1, inplace=True)
with model:$     observation = pm.Poisson("obs", lambda_, observed = summary['event'])
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid$ x = df30458.groupby('start_station_name').count()$ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
questions['zipcode'].unique()$
hours = df4['Date'].dt.hour$ hours$
print('{0:.2f}%'.format((scores[:1.625].sum() / total) * 100))
smooth = condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="SMOOTHPURSUIT")
dataset = member_pivot['Percent Purchase']*100$ print dataset.values
my_gempro.pdb_downloader_and_metadata()$ my_gempro.df_pdb_metadata.head(2)
calls_nocontact.neighborhood_district.value_counts()
headers=[x.replace('-','_') for x in headers]
df_events_sample = df_events.sample(n=1000)
!pip install -q xgboost==0.4a30$ import xgboost
json=request.json()
user = raw['submitter_user'].apply(pd.Series)$ user.head(3)
d.groupby('label')['tweet_id'].count()
pd.concat([msftA[:3], aaplA[:3]], ignore_index=True)
rain_df = pd.DataFrame(rain)$ rain_df.head()$
data["engagement"] = np.where(data["comms_num"]>500, 1, 0)
cityID =  '5a110d312052166f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Francisco.append(tweet) 
conn_b.commit()
fh_1 = FeatureHasher(num_features=uniques.iloc[2, 1], input_type='string', non_negative=True) # so we can use NaiveBayes$ %time fit = fh_1.fit_transform(train.device_model)
materials_file = openmc.Materials([inf_medium])$ materials_file.export_to_xml()
building_pa.to_csv("buildding_00.csv",index=False)
for c in ccc:$     for i in ved[ved.columns[ved.columns.str.contains(c)==True]].columns:$         ved[i] /= ved[i].max()
print(np.shape(b))$ print(np.size(b))$ print(np.ndim(b))
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s')$ git_log.timestamp.describe()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)
results.shape
sorted_posts = gram_collection.find({"contains_tattoo": 1}).sort([("likes", pymongo.DESCENDING)])
calls_nocontact.council_district.value_counts()
print (autoData.reduce(lambda x,y: x if len(x) < len(y) else y))
client = MongoClient()$ db = client.sample_1m$ db.sample_1m.find_one()
model = ols("happy ~ age + income", training).fit()$ model.summary()
plate_appearances.loc[plate_appearances.batter_name=='ryan howard',].head()
sns.pairplot(iris, hue='Species')
client.deployments.list()
response = requests.get("https://data.bitcoinity.org/markets/bidask_sum/24h/USD/bitfinex?bp=1&bu=c&r=minute&t=m")
print(Probit(y, X).fit().summary())
print(type(dicts))$ print(dicts.keys())
pax_raw = pax_raw.merge(keep_days, on=['seqn', 'paxday'], how='inner')
df.to_csv("fav.csv")
df['y'].plot.box(notch=True, showmeans=True)
df['job'] = df['job'].fillna(value=0)
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep=',', encoding='utf-8', header=True)
featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA22456_hires.jpg'$ featured_image_url
df['year'] = df.index.year
days = pd.date_range('2014-08-29','2014-09-05',freq='B')$ for d in days: print(d)
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
tobs_values_df=pd.DataFrame([tobs_values]).T$ tobs_values_df.head()
gdax_trans_btc = gdax_trans.loc[gdax_trans['Account_name'] == 'BTC',:]
aggregate_by_name = pd.concat(g for _, g in df_Providers.groupby("name") if len(g) > 1)$ aggregate_by_name.head()
df.info()
count_hash = Counter()$ count_hash.update(clean_hashlist3)$ print(count_hash.most_common(30))$
tret.plot(figsize=(16,4), color='r');
data['Subjectivity'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df_copy = df_groups.join(df_events.groupby(['group_id']).created.count(), how ='inner',on= 'group_id', lsuffix= '_left', rsuffix = '_count')$ df_copy = df_copy.sort_values(by = 'created_count', ascending = False)$ df_copy.head()
outfile = path + '../output/allData_NoRetweets_May27_Cleaned.csv'$ cleanedDataNoRetweets.to_csv(outfile, index=False, encoding='utf-8')
media_classes = [c for c in df_os.columns if c not in ['domain', 'notes']]$ media_classes
lm.pvalues
df = pd.read_sql('SELECT * from booked_room', con=conn_b)$ df.head(10) # show 10 rows only
f = open("datasets/git_log_excerpt.csv")$ print(f.read())
coin_data = quandl.get("BCHARTS/ITBITSGD")$
df_first_days['active_user'] = df_first_days['user_id'].apply(lambda user: 1 if user in active_users else 0)$ df_first_days = df_first_days.merge(df_user_info[['user_id', 'marketing_source']], on='user_id', how='left')
user_df = pd.read_csv('User_Information.csv')
data.RTs.describe()
fh_2 = FeatureHasher(input_type='string', non_negative=True)$ %time fit2 = fh_2.fit_transform(train.device_id)
mean = np.mean(data['len'])$ print ("The length's average in tweets: {}".format(mean))$
response_count = requests.get(SHOPIFY_API_URL+'/orders/count.json',params={'status':'any'}).json()['count']
date_df.plot.area(stacked=False)
df.zone.fillna('Unknown', inplace=True)$ df.county_name.fillna('Alameda', inplace=True)
[1] # <-- this syntax should be used when converting to a list.$
engine = create_engine("sqlite:///hawaii.sqlite")
containers = page_soup.find_all("section", {"class":"ref-module-paid-bribe"})
ds = tf.data.TFRecordDataset(train_path)$ ds = ds.map(_parse_function)$ ds
Google_stock = pd.read_csv('~/workspace/udacity-jupyter/GOOG.csv')$ print('Google_stock is of type:', type(Google_stock))$ print('Google_stock has shape:', Google_stock.shape)$
t1.tweet_id =t1.tweet_id.astype(str)$
df_tte['InstanceType'] = df_tte['UsageType'].apply(lambda usage: usage.split(':')[1]) #create column with instance type
sub_mean = df.groupby('subreddit').agg({'num_comments': 'mean'})$ top_com = sub_mean.sort_values('num_comments', ascending = False).head()$ top_com
print('raw time value: {}'.format(plan['plan']['date']))$ print('datetime formatted: {}'.format(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(plan['plan']['date']/1000))))
with open('faved_tweets.df', 'wb') as handle:$     pickle.dump(df, handle)
exiftool -csv -createdate -modifydate cisnwh8/cisnwh8_cycle1.MP4 cisnwh8/cisnwh8_cycle2.MP4 cisnwh8/cisnwh8_cycle3.MP4 cisnwh8/cisnwh8_cycle4.MP4 cisnwh8/cisnwh8_cycle5.MP4 cisnwh8/cisnwh8_cycle6.MP4 > cisnwh8.csv
os.chdir(os.path.expanduser('~/PycharmProjects/chat-room-recommendation/'))$ lines = open('cornell-movie-dialogs-corpus/movie_lines.txt','r').read().split('\n')$ movie_metadata = open('cornell-movie-dialogs-corpus/movie_titles_metadata.txt','r').read().split('\n')
logit = sm.Logit(df3['converted'],df3[['intercept' ,'treatment']])
cityID = '0562e9e53cddf6ec'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Santa_Ana.append(tweet) 
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['tweets'] ])$ display(data.head(5))
Urban = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Urban")]$
adj_close_pivot_merged = pd.merge(adj_close_pivot, adj_close$                                              , on=['Ticker', 'Adj Close'])$ adj_close_pivot_merged.head()
chunker = ConsecutiveNPChunker(train_trees)$ print(chunker.evaluate(valid_trees))
forecast_range = forecast1[::-1]$ forecast_range.info()$ forecast_range.head()
lr = LogisticRegression()$ lr.fit(X, y)
TEXT.vocab.stoi['the']
df = df.replace('tomato', 'pea')$ df
ctc = pd.DataFrame(columns=ccc, index=ccc)
X = reddit_master[['age', 'subreddit']].copy(deep=True)$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
df_test_user = df_users.iloc[0, :]$ df_test_user
data["hours"] = data["time_up_clean"].astype(str).str[0:2]$ data["hours"] = data["hours"].astype(int)$ data.loc[data["hours"] == 0, "hours"] = 1
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
response = requests.get('http://www.reddit.com/r/goodnews')$ page_source = response.text$ print(page_source[:1000])
plt.style.available
len(email_age_unique[pd.isnull(email_age_unique['request.ageRange']) == True])
df4 = df3.dropna(axis=0, inplace=False)$ df4.head()
BallBerry_ET_Combine = pd.concat([BallBerry_rootDistExp_1, BallBerry_rootDistExp_0_5, BallBerry_rootDistExp_0_25], axis=1)$ BallBerry_ET_Combine.columns = ['BallBerry(Root Exp = 1.0)', 'BallBerry(Root Exp = 0.5)', 'BallBerry(Root Exp = 0.25)']
df_20180113 = pd.read_csv('data/discovertext/hawaii_missile_crisis-export-20180221-113313.csv',$                           infer_datetime_format=True)$ df_20180113.head()
pcpData_df.prcp.describe()
plt.plot(scores); plt.title("Scores");
table = driver.find_element_by_xpath('//*[@id="body"]/table[2]')$ table.get_attribute('innerHTML').strip()
gDate_vEnergy = gDateEnergy_content.count().unstack()$ gDate_vEnergy = gDate_vEnergy.fillna(0)
lst = data_after_first_filter.WATER_BODY_NAME.unique()$ print('Waterbodies in workspace dataset:\n{}'.format('\n'.join(lst)))
np.shape(ndvi_coarse)
new_df = pd.read_sql_query('select * from film where title = "Hunchback Impossible"', engine)$ new_df.head()$
S_distributedTopmodel.decision_obj.groundwatr.options, S_distributedTopmodel.decision_obj.groundwatr.value
with open('components/pop_models/excitatory_pop.json') as exc_data:$     exc_prs = json.load(exc_data)$ pprint.pprint(exc_prs)
scattering_to_total = scattering.xs_tally / total.xs_tally$ scattering_to_total.get_pandas_dataframe()
import statsmodels.api as sm$ logit = sm.Logit(df['converted'],df[['intercept','treatment']])$
df_download_num = df_download_num.groupby(['game_format_name'])['download_num'].sum().reset_index()$ df_download_num.columns = ['game_format_name', 'download_sum_num']$ df_download_num[df_download_num['game_format_name'] == '王者荣耀'].head()
vect = CountVectorizer(stop_words=stop_words, ngram_range=(2,4), min_df=0.0001)$ X = vect.fit_transform(fashion.text)
df.rename(columns={'value':'lux'},inplace=True)
lm.fit(x_train, y_train)
df_gnis = pd.read_csv(file_name+'_20180601.txt', sep='|', encoding = 'utf-8')
posts.head()
data = fat.add_sma_columns(data, 'Close', [6,12,20,200])$ data.tail()
results_lumpedTopmodel, output_LT = S_lumpedTopmodel.execute(run_suffix="lumpedTopmodel_hs", run_option = 'local')
mgxs_lib.dump_to_file(filename='mgxs', directory='mgxs')
!head -n 2 Consumer_Complaints.csv
crypto_combined = pd.concat([crypto_data, crypto_ggtrends], axis=1).dropna(how='any')$ crypto_combined_s = crypto_combined.copy(deep=True)$ print(crypto_combined_s.head(10))
Val_eddyFlux = Plotting(hs_path + '/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
rural_driver_total = rural_type_df.groupby(["city"]).mean()["driver_count"]$ rural_driver_total.head()
donors_c.iloc[2097169, :]$
date = datetime.datetime.strptime(   )$ mask = 
gdax_trans_btc = pd.merge(gdax_trans_btc, coinbase_btc_eur_min.iloc[:,:2], on="Timestamp", how="left")
r =requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2010-01-01&end_date=2010-01-01')
predictions = lrModel.transform(testData)$ predictions.select("prediction", "label", "features").show(5)$ predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred7.csv')
from tensorforce.agents import PPOAgent$ from tensorforce.core.networks import LayeredNetwork, layers, Network, network
exiftool -csv -createdate -modifydate ciscid4/CISCID4_cycle2.mp4 ciscid4/CISCID4_cycle3.mp4 ciscid4/CISCID4_cycle4.mp4 ciscid4/CISCID4_cycle5.mp4 ciscid4/CISCID4_cycle6.mp4 > ciscid4.csv
news_paragraph = soup.find_all("div", class_="rollover_description_inner")[0].text$ print(news_paragraph)
print(dictionary.token2id['like'])
p_old = df2['converted'].mean()$ print ("convert rate for p_old under the null :{} ".format(round(p_old, 4)))
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','Status']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)$
bird_data = pd.read_csv('res/data/bird_tracking.csv')$ ix = bird_data.bird_name == 'Eric'$ x, y = bird_data.longitude[ix], bird_data.latitude[ix]
cityID = '55b4f9e5c516e0b6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Orlando.append(tweet) 
station_df =pd.read_sql('station',engine)$ station_unique_df = station_df['station']$ station_unique_df.nunique()$
df.head()
grouped_publications_by_author['countCollaborators'] = grouped_publications_by_author['authorCollaboratorIds'].apply(len)
print(groceries.index)$ print(groceries.values)
tdelta = pd.to_datetime(loctweetdf['created_at'].max()) - pd.to_datetime(loctweetdf['created_at'].min())$ tdelta = tdelta.days$ tdelta
matches = re.findall('\d+', $     'the recipe calls for 10 strawberries and 1 banana')$ print(matches)$
df3 = df3.reset_index(drop=True)$ df_list = df_list.reset_index(drop=True)$ combine = (df_list.join(df3))
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)
iso_gdf.plot();
Measurement = Base.classes.measurement$ Station = Base.classes.station
accuracy = accuracy_score(y_test, y_pred)$ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
df.injured.describe()
very_pop_df = au.filter_for_support(popular_trg_df, min_times=6)$ au.plot_user_dominance(very_pop_df)
calls_df["dial_type"].value_counts()
session.query(Measurement.date).order_by(Measurement.date.desc()).first()
coefs.loc['age', :]
statistics_table = win_rates_table.merge(pick_rates_table, left_index=True, right_index=True)$ statistics_table.head()
df2 = df2.drop_duplicates(['user_id'], keep='first')$ df2.info()
year_info_df.describe()
means = df_lg_hubs.mean()$ default_use = [means[str(i)] for i in range(1, 13)]$ default_use
df['date'] = df['date'].apply(lambda time: time.strftime('%m-%d-%Y'))$ df['time'] = df['time'].apply(lambda time: time.strftime('%H:%M'))
import _pickle as cPickle$ with open('tuned_crf_classifier.pkl', 'rb') as fid:$     crf = cPickle.load(fid)
pd.date_range(start='3/1/2016', periods=20)
kochdf.loc[kochdf['date'] == max_date]
cust_vecs, item_vecs = implicit_weighted_ALS(train_mat, lambda_val=0.1, alpha = 40, iterations = 30, rank_size = 20)
load2017.columns = ['time', 'day-ahead', 'actual'] $ load2017.head()
output_fn = "News_Sentiment_Analysis.csv"$ sentiment_df.to_csv(output_fn)
buckets_to_df(contributors.fetch_aggregation_results()['aggregations']['0']['buckets'])
store_items = store_items.drop(['store 3'], axis = 0)$ store_items
sentimentDf.to_csv("twitterSentiment.csv")
busy_stations = session.query(Measurement.station, func.count(Measurement.tobs)).filter(Station.station == Measurement.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()$ busy_stations
kickstarters_2017 = pd.read_csv("ks-projects-201801.csv")$ kickstarters_2017.head()
sdf.createOrReplaceTempView("tempTable")$ res.show()
import geopy.distance
temp_df2['titles'] = temp_df2['titles'].astype(str)
Measurement = Base.classes.measurement$ Station = Base.classes.station
results = session.query(Station.station, Station.name).count()$ print(f"There are {results} stations.")
df.mean(1)
menes_postulados.groupby()
not_in_oz = not_in_oz[['stopid', 'address', 'lat', 'lng', 'routes']]$ new_stops = new_stops.append(not_in_oz)$ new_stops.head(5)
import pandas as pd$ PATH='light_sensor-20180705-1304.csv'$ df=pd.read_csv(PATH)
result = Geocoder.geocode("7250 South Tucson Boulevard, Tucson, AZ 85756")
for col in response_df.columns:$     if response_df[col].dtype in (object, str):$         print col, pd.np.where(response_df[col] == '', 1, 0).sum()
plt.style.use('ggplot')$ bb['close'].apply(rank_performance).value_counts().plot(kind='pie', legend=True)
ngrams_summaries = cvec_3.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
data4.to_file('Twitters_FSGutierres.shp', driver='ESRI Shapefile')
x_test = np.array(data)
en_es = pd.read_csv(FPATH_ENES, sep=" ", header=None)$ en_es.columns = ["en", "es"]$ en_es.describe()
import re$ regex_1 = re.compile('\w[A-Z]+')
holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00102032') ]
census_zip = df_station.merge(wealthy, left_on=['zipcode'], right_on=['zipcode'],  how='left')$ nan_rows = census_zip[census_zip['betw150kand200k'].isnull()]$ census_zip.dropna(inplace = True)
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
value=ratings['rating'].unique()$ value
data = utils.load_data()
gene_df[gene_df['length'] > 2e6].sort_values('length').iloc[::-1]
data = data.loc[data['tmin'] >= -10]
new_page_converted=np.random.choice([1,0],size=n_new,p=[pnew,(1-pnew)])$ new_page_converted.mean()
ab_df2[((ab_df2['group'] == 'treatment') == (ab_df2['landing_page'] == 'new_page')) == False].shape[0]
words_only_scrape = [term for term in words_scrape if not term.startswith('#') and not term.startswith('@')]$ print('The number of words only (no hashtags, no mentions): ', len(words_only_scrape))
df.quantile([.01, .05, .1, .25, .5, .75, .9, .95, .99])
deltadf.to_csv('exports/trend_deltas.csv')
df.status.value_counts()
attend_with.to_csv('../data/attend.csv')
df = pd.merge(t1, t2, on='tweet_id', how='inner')$ df = pd.merge(df, t3, on='tweet_id', how='inner')
ctrl_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][0]$ print("P(ctrl_con) = %.4f" %ctrl_con)
!head ml-100k/u.data
ra_min_ogle_fix = 250.0; ra_max_ogle_fix = 283.0;$ dec_min_ogle_fix = -40.0; dec_max_ogle_fix = -15.0;
r1.keys()
sort_df.tail(10)
strs = 'NOTE: This event is EVERY FRIDAY!! Signup is a'$ result = re.split(r'[^0-9A-Za-z]+',strs)$ print(result)
p_new = df2['converted'].mean()
apple["Regime"] = np.where(apple['20d-50d'] > 0, 1, 0)$ apple["Regime"] = np.where(apple['20d-50d'] < 0, -1, apple["Regime"])$ apple.loc[dt.date(2016,8,7):dt.date(2016,1,1),:].plot(ylim = (-2,2)).axhline(y = 0, color = "black", lw = 2)
top_10_authors = git_log['author'].value_counts().head(10)$ top_10_authors
some_df = sqlContext.createDataFrame(some_rdd)$ some_df.printSchema()
kick_projects = kick_projects.replace({'country': 'N,0"'}, {'country': 'NZERO'}, regex=True)
path = 'c:\\windows\\fonts\\nanumgothiccoding.ttf'$ font_name = fm.FontProperties(fname=path, size=50).get_name()$ plt.rc('font', family=font_name)
%%time$ grid_svc.fit(X_tfidf, y_tfidf)
tips_train = tips.sample(frac=0.8, random_state=123)$ tips_test = tips.loc[tips.index.difference(tips_train.index),:]$ tips_train.shape, tips_test.shape, tips.shape
Base.classes.stations
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=YxSY4z-2vyxvJ15WjtFa&start_date=2017-01-01&end_date=2017-12-31"$ req = requests.get(url)
    $ eng = create_engine("sqlite:///hawaii.sqlite")
df.iloc[(len(df)-lookforward_window)-3:(len(df)-lookforward_window),:]
tmp = data_df.copy()$ tmp.columns = [x if re.search("size", x) else "data_{}".format(x) for x in tmp.columns]
times = bird_data.timestamp[bird_data.bird_name == "Eric"]$ elapsed_time = [time-times[0] for time in times]$ print(elapsed_time[:10])
kochdf.to_csv('exports/trend_data.csv')
dfd.zones.value_counts()
density = 1.32
model.save_weights('best.hdf5')
rdd.map(lambda x: x**2 + really_large_dataset.value).collect()
p_mean = np.mean([p_new, p_old])$ print("Probability of conversion udner null hypothesis (p_mean):", p_mean)
plt.xlim(100, 0)$ plt.ylim(-1, 1)
ab_df2.converted.mean()
tweets_l_scrape = d_scrape['text'].tolist() # create a list from 'text' column in d dataframe$ print(tweets_l_scrape[-1:])
soup = bs(html.text, 'html.parser')
old_page_converted = np.random.choice([1,0], size=df_oldlen, p=[pold,(1-pold)])$
training.index = range(608)$ test.index = range(153)$ training.head()
table_rows = driver.find_elements_by_tag_name("tbody")[16].find_elements_by_tag_name("tr")$
mlp_pc_fp = 'data/richmond_median_list_prices_percent_change.csv'$ mlp_pc.to_csv(mlp_pc_fp, index=True) # Pass index=True to ensure our DatetimeIndex remains in the output
print(groceries.loc[['eggs', 'apples']])$ print(groceries.iloc[[2, 3]])
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='API_KEY$ r = requests.get(url)
local.get_dataset(post_process_info["DatasetId"])
google_stock['Adj Close'].describe()
merged_data['cs_creation_day'] = merged_data['customer_creation_date'].dt.day$ merged_data['cs_creation_month'] = merged_data['customer_creation_date'].dt.month$ merged_data['cs_creation_year'] = merged_data['customer_creation_date'].dt.year
fulldf.to_csv('FullResults.csv', encoding='utf-8', index=False)
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf],$       labels=["bad", "ok", "yeah!"]).sample(10)
 print(r.text)
mydata.to_csv("Data-5year-2012-20180617.csv")
df_session_dummies = pd.get_dummies(df, columns=['action'])$ df_session_dummies.head()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df["Source"].unique()
movies_df.loc[movies_df['movieId'].isin(recommendationTable_df.head(20).keys())]
ny_df=pd.DataFrame.from_dict(foursquare_data_dict['response'])
probs_test[:, 1].mean()
doc_df = pd.read_sql("SELECT * FROM Documents", con=engine)$ doc_df.head(3)
contractor_clean['last_updated'] = pd.to_datetime(contractor_clean.last_updated)$ contractor_clean['updated_date'] =contractor_clean['last_updated'].dt.strftime('%m/%d/%Y')$ contractor_merge['month_year'] =contractor_merge['last_updated'].dt.to_period('M')
data = r.json()
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comments', ''))
X_trainset.shape$ y_trainset.shape$
days_alive = (datetime.datetime.today() - datetime.datetime(1981, 6, 11))$ days_alive.days
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.csv"$ df = pd.read_csv(path, sep =',', na_values=['.'])$ df.head(5)
measurements_df = measurements_df.dropna(how='any')
from sqlalchemy_utils.functions import create_database$
all_tables_df.loc[:, 'OBJECT_NAME']
plot = events_top10_df.boxplot(column=['yes_rsvp_count']\$                            , by='topic_name', showfliers = False, showmeans = False, figsize =(17,8.2)\$                            ,whis=[20, 80])
retweets = megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")]$ megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")]['text'].count() # 1,633
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
print("Url: " + client.repository.get_model_url(saved_model_details))
df.xs(key='x', level=2)
tweet_archive_clean['stage'] = tweet_archive_clean[['doggo', 'floofer','pupper', 'puppo']].apply(lambda x:''.join(x), axis= 1)
data = trends.interest_over_time()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
pantab.frame_to_hyper(active_with_return, 'Avtive User Analysis.hyper')
bucket.upload_dir('data/wx/tmy3/raw/', 'wx/tmy3/raw', clear_dest_dir=True)
import pandas as pd$ df_from_pd = pd.read_clipboard()$ df_from_pd
au.clear_dir('data/city-util/proc')
df.groupby([df.Date.dt.month, df.Date.dt.day]).count().Tweets
Base = automap_base()$ Base.prepare(engine, reflect=True)$
ripple_market_info.drop(['Date'],inplace=True,axis=1)$ scaler_rip = MinMaxScaler(feature_range=(0, 1))$ scaled_rip = scaler_rip.fit_transform(ripple_market_info)$
cityID = 'bced47a0c99c71d0'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Durham.append(tweet) 
vacation_data_df=pd.DataFrame(vacation_data)$ rain_per_station = pd.pivot_table(vacation_data_df,index=['station'],values=['prcp'], aggfunc=sum)$ rain_per_station$
s1 = pd.Series(np.arange(1, 6, 1)) $ s2 = pd.Series(np.arange(6, 11, 1)) $ pd.DataFrame({'c1': s1, 'c2': s2})
for i in range(len(df.columns)):$     df.rename(columns={i:df_header[i]}, inplace=True)
results = session.query(measurement.date, measurement.prcp).filter(measurement.date >= prev_year).all()
ensemble_preds = np.round((preds1 + preds2 + preds3 + preds4 + preds5) / 5).astype(int)$ print(ensemble_preds.mean())
openmc.run()
df.drop(df[['prospectid', 'ordernumber', 'ordercreatedate', 'dnatestactivationdayid', 'xsell_gsa', 'xsell_day_exact' ]], axis=1, inplace=True)
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv',$                  parse_dates=['deadline', 'launched'],$                  encoding = "ISO-8859-1")
merge_table = pd.merge(extreme_dates1, main_df, on= "Date", how = "left")$ merge_table
df['created_date'] = pd.to_datetime(df['created_date'])$ df.head()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()
def ls_dataset(name,node):$     if isinstance(node, h5py.Dataset):$         print(node)
seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)
df['time_detained'] = df['time_detained'] / np.timedelta64(1,'D')
iso_json = json.loads(iso_response.text)$ iso_gdf = gpd.GeoDataFrame.from_features(iso_json['features'])$ iso_gdf[:]
df_schools.shape
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df.head()
newfile = newfile.loc[newfile['Delivery block'].notnull()]
matchinglist = getmatchinglist()$ matchinglist.head()
tUnderweight = len(df[df['bmi']< 18.5])$ tUnderweight
pvt = pvt.drop(['ga:dimension2', 'customerId'], axis=1)$ pvt = pvt[['ga:transactionId', 'ga:date', 'customerName', 'productAndQuantity']]$ pvt
df.set_index('datetime',inplace=True)
honeypot_input_data = "2018-01-26-mhn.log"
data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/0ddd2680-e35d-46bc-ac1a-d350da4f409d/ar24011.asc'
rain_df.set_index('date').head()
session.query(func.count(Station.station)).all()
df1 = pd.merge(left=dfWQ_annual,right=dfQ1_annual,how='inner',left_index=True,right_index=True)$ df2 = pd.merge(left=dfWQ_annual,right=dfQ2_annual,how='inner',left_index=True,right_index=True)$ df3 = pd.merge(left=dfWQ_annual,right=dfQ3_annual,how='inner',left_index=True,right_index=True)
logger.info('Define Source')$ data = pd.DataFrame.from_csv('~\\neo.csv')$ logger.debug('df: %s', data)
store_items.fillna(method = 'backfill', axis = 0)
new_page_converted = np.random.choice([1,0], size = nNew, p=[pMean,oneMinusP])$ new_page_converted.mean()
ptgeom = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]$ gdf = gpd.GeoDataFrame(df, geometry=ptgeom, crs={'init': 'epsg:4326'})$ gdf.head(5)
access_logs_df.createOrReplaceTempView('AccessLog')$
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1?api_key="+API_KEY
churned_ordered = ordered_df.loc[churned_ord]
print('Columns:',len(free_data.columns), 'Rows:',len(free_data))
df2_no_outliers = df2.copy()$     $ df2_no_outliers['y'] = np.log(df2_no_outliers['y'])
outputs = {}$ for key in predictions:$     outputs[key] = pd.DataFrame({id_label:ids, target_label:predictions[key]})
network.rebuild()$ sim.run()
df_schoo11 = df_schools.rename(columns={'name':'school_name'})$ df_schoo11.head()
so.loc[so['favoritecount'].between(30, 40), 'title'::3].head()
cityID = 'ab2f2fac83aa388d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oakland.append(tweet) 
from scipy import stats$ instance.initialize(parameters)
precip_data = session.query(Measurements).first()$ precip_data.__dict__
crimes.columns = crimes.columns.str.strip()$ crimes.columns
file_names = []$ file_names = glob.glob('../craigslist-data/final-data-multiple-cities/*.csv')
bucket.upload_dir('data/city-util/proc', 'city-util/proc', clear_dest_dir=True)
plt.xlim(100, 0)$ plt.ylim(-1, 1)
csvfile ='results.csv'$ resdf = pd.read_csv(csvfile)$ resdf.info()
S_distributedTopmodel.meta_basinvar.filename
text = data_df['clean_desc'].apply(cleaning)$ text_list = [i.split() for i in text]$ len(text_list)
df = pd.read_csv(chat_file, error_bad_lines=False)
df2['converted'].mean()
sample.groupby('tasker_id').hired.count().sort_values(ascending=False).head(1)
len(chefdf.name)
FORMAT = '%(asctime)s : %(levelname)s : %(message)s'$ logging.basicConfig(format=FORMAT)$ logging.getLogger().setLevel(level=logging.INFO)$
data.plot.line(x = 'Unnamed: 0', y = 'Age', figsize = (15, 10))
year_dict = year_request.json()$ print (type(year_dict))
model_w = sm.formula.ols('y ~ C(w)',data=df).fit()$ anova_w_table = sm.stats.anova_lm(model_w, typ=1)$ anova_w_table.round(3)
pipe_lr = make_pipeline(cvec, lr)$ pipe_lr.fit(X_train, y_train)$ pipe_lr.score(X_test, y_test)
data.head()
Val_eddyFlux = Plotting(hs_path+'/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
QUIDS_wide["y"] = QUIDS_wide[['qstot_12','qstot_14']].apply(lambda x: x['qstot_14'] if np.isnan(x['qstot_12'])$                                                             else x['qstot_12'], axis=1)
emojis_db=pd.read_csv('emojis_db_csv.csv')$ emojis_db.head()
plt.hist(name_array)$ plt.title('The distribution of the names\' length')$ plt.show()$
plt.savefig(str(output_folder)+'NB01_4_NDVI01_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice_str)
tmp_one = soup.find_all('div', 'sammy')[0]$ type(tmp_one)$
grouped_mean = tweet_df.groupby(["source"]).mean()["compound"]
X.drop('title', axis=1, inplace=True)$ X = pd.get_dummies(X, drop_first=True)
paired_df_grouped['best_co_occurence'] = paired_df_grouped.apply(lambda x: x['all_co_occurence'][:rule_of_thumb(x['top10'])], axis=1)$ del paired_df_grouped['top10']$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
[(type(nd), nd.shape) for nd in read_in["ndarrays"]]
results.coordinates
import csv$ data = list(csv.reader(open("test_data//my_data.csv")))
model_x = sm.formula.ols('y ~ C(x)', data = df).fit()$ anova_x_table = sm.stats.anova_lm(model_x, typ = 1)$ anova_x_table.round(3)
print(data.index.value_counts())$ data=data.drop(data.index[-1])
df_concat_2["message_likes_dummy"] = np.where(df_concat_2.message_likes_rel > 500, 1, 0)
data.head()
inspector = inspect(engine)$ inspector.get_table_names()
df1=pd.read_csv("../Raw Data/approval data clean values only.csv")$ df1.head()
table.info('stats')
plt.scatter(branch_dist[1:],branch_r[1:])
twitter_df['location'].value_counts()
df_names = pd.read_csv('names.csv')$ df_names
norm.ppf(1-0.05/2)
test_df = allData[allData.lang=='es']$ print(test_df)
dr_test_data = dr_test_data.resample('W-MON').sum()$ RN_PA_test_data = RN_PA_test_data.resample('W-MON').sum()$ therapist_test_data = therapist_test_data.resample('W-MON').sum()
S_lumpedTopmodel.meta_basinvar.filename
cur.execute('SELECT count(Comments_Ratings) FROM surveytabl WHERE Comments_Ratings is not null;')$ cur.fetchall()
import nltk$ from nltk.corpus import stopwords$ print(stopwords.words('english'))
f = open('datasets/git_log_excerpt.csv', 'r')$ f.read()
x = tf.constant(1, name='x')$ y = tf.Variable(x+10, name='y')$ print(y)
index = pd.date_range('2018-3-1', periods=1000, freq='B')$ index
rows.describe()
ch.setLevel(logging.WARNING)
divs = body.find_all('div',class_='slide')$ divs
articles['content_short'] = articles['tokens'].map(lambda s: ' '.join(s))
options_frame['BidAskSpread'] = options_frame['Ask'] - options_frame['Bid']$ errors_20_largest_by_spread = options_frame.ix[sorted_errors_idx.index]$ errors_20_largest_by_spread[['BidAskSpread', 'ModelError']].sort_values(by='BidAskSpread').plot(kind='bar', x='BidAskSpread')
plt.savefig(str(output_folder)+'NB05_2_FC_before_and_after'+str(cyclone_name)+'_'+str(location_name))
plt.plot(glons, glats, marker='.', color='k', linestyle='none')$ plt.show()
temps_df.loc['2018-05-02'].index
train_frame = train_frame.reset_index()$ df_series = pd.Series(train_frame["values"])$
cityID = 'e67427d9b4126602'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Madison.append(tweet) 
%matplotlib inline$ df['log_AAPL'].plot(figsize=(12,8));
X_train_predict = model.predict(X_train)$ X_test_predict = model.predict(X_test)
import pandas as pd$ git_log['timestamp']=pd.to_datetime(git_log['timestamp'],unit='s')$ git_log.timestamp.describe()$
now = time.strftime("%c")$ todays_date = time.strftime("Current date & time" + time.strftime("%c"))$ print(now.split(':')[0].replace('  ',' ') + ':' + now.split(':')[1])$
forecast_set = clf.predict(X_lately) #selecting only the last few rows to predict$ print(forecast_set)$ df['Forecast'] = np.nan #Adding new column to store all the values
    no_punc = [char for char in string.lower() if char not in punctuation]$     no_punc = ''.join(no_punc)$     return [word for word in no_punc.split() if word not in stopwords.words('english')]
sns.heatmap(data_numeric.corr().abs().round(2),annot=True)
corpus_Tesla = [dictionary.doc2bow(text) for text in sws_removed_all_tweets]
df_2015['bank_name'] = df_2015.bank_name.str.split(",").str[0]$
trump_df.to_csv("test.csv", index=False, encoding="utf-8")
pd.crosstab(train.TYPE_BI, train.TYPE_UT)
print(ndvi_nc)$ for v in ndvi_nc.variables:$     print(ndvi_nc.variables[v])
%bash$ gsutil cat "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" | head
soup = bs(response.text, 'html.parser')
pax_raw.info()
freq_station = {'id':"",'name':""}$ freq_station['id'] = active_station_df.iloc[:1]['station'][0]$ freq_station['name'] = active_station_df.iloc[:1]['name'][0]
all_df.isnull().any()$
preci_df = pd.DataFrame(preci_data)$ preci_df.head()
seed = 2210$ (train1, dev1, modeling2) = (modeling1$                              .randomSplit([0.4, 0.1, 0.5], seed=seed))
data = pd.read_csv('dog_rates_tweets.csv', parse_dates=[1])$
ibm_hr_cat = ibm_hr_no_numerical.select(categorical_no_target)$ ibm_hr_cat.show(3)
plt.plot(weather['created_date'], weather['max'])$ plt.xticks(rotation='vertical')
dfClientes.iloc[20, :]
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=' + API_KEY + '&start_date=2017-1-1&end_date=2017-12-31')$ 
!hdfs dfs -cat /user/koza/hw3/3.2.1/productWordCount/* | tail -n 1
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score
small_lst = df.sample(100)$ location_small_lst, message_small_lst, date_small_lst = small_lst.locations,small_lst.messages, small_lst.posted_date
tips.columns
dr_new_patient_8_to_16wk_arima = dr_new_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']]$ dr_new_patient_8_to_16wk_arima.index = dr_new_patient_8_to_16wk_arima.index.date
new_doc = UnicodeDammit.detwingle(doc)$ print(new_doc)$ print(new_doc.decode('utf8'))$
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key="$ r = requests.get(url+API_KEY)$
len(kochdf.loc[kochdf['user'] == "Rezeptsammlerin"]['name'])
mmx = MinMaxScaler()$ %time train_4_reduced = mmx.fit_transform(train_4_reduced)
faa_data_minor_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "M"]$ print(faa_data_minor_damage_pandas.shape)$ faa_data_minor_damage_pandas.head()
temp_df = temp_df.join(metadata.set_index('order_num'), how='left', on='order_num')$ temp_df['year'] = temp_df.date.dt.year
preds = aml.leader.predict(test)
train_data, test_data = train_test_split(status_data, test_size=0.50)$ train = train_data.values$ test = test_data.values
train_df.head()
output_path = "/Users/kai.bernardini/Documents/MA575/stock_data/"
lat = 47.4797$ lon = 8.5358
merged[['CA', 'UK', 'US']] = pd.get_dummies(merged['country'])$ merged.head()
extract_nondeduped_cmp = extract_all[f_remove_extract_fields(extract_all.sample()).columns.values].copy()
merged1.drop('id_y', axis=1, inplace=True)
the_means = np.mean(the_sample, axis=1)$ print(f'Mean ({num_samples} samples) = {np.mean(the_means):5.3f}')$ print(f'Standard deviation ({num_samples} samples) = {np.std(the_means):5.3f}')
url_votes = grouped['net_votes'].agg({'total_votes': 'sum', 'avg_votes': 'mean'})    $
unigram_chunker = UnigramChunker(train_trees)$ print(unigram_chunker.evaluate(valid_trees))
autoDf1 = SpSession.read.csv("auto-data.csv",header=True)$ print (autoDf1.show())$
data.to_csv("...Twitter\\RAILWAYS.csv", sep =',')
subreddit = 'AskReddit'$ sr = reddit.subreddit(display_name = subreddit)
round((model_x.rsquared), 3)
exl=pd.read_excel('Book1.xlsx', converters={'item#':str,'part#':str, 'deliverydate':pd.to_datetime})$
tips.sex = tips.sex.astype('category')$ tips.smoker = tips.smoker.astype('category')$ print(tips.info())$
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_zh_wzh_2.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_wzh_2.w2v')
response = requests.post("https://data.bitcoinity.org/chart_data", data=chartParams)$ response.content
plt.style.use('ggplot')$ data.groupby('Año').mean()['tmin']
y_pred = gnb.predict(X_clf)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)])$ new_page_converted.mean()
ind_result_list=ind_result.tolist()$ ids=independent_dataset['id'].tolist()
id_of_tweet = 932626561966247936$ tweet = (api.get_status(id_of_tweet, tweet_mode='extended')._json['full_text'])$ print(tweet)
health_data_row.xs(key=(2013 , 1), level=('year', 'visit'))
treehouse_labels_pruned = treehouse_labels.filter(regex='\ATH|\ATHR', axis="index")
turnstiles = df.groupby(['STATION','C/A','UNIT','SCP'])$ print('There are {} unique turnstiles.'.format(len(turnstiles)))$
song_tracker_grouped_df.to_csv("Desktop/Project-2/song_tracker.csv", index=False, header=True)
tweets.dtypes
model_ft = FastText.load_fasttext_format('C:/Users/edwin/Desktop/FastText/wiki.zh.bin')$
if not os.path.isdir('output'):$     os.makedirs('output')
max_open = ldf['Open'].max()$ max_open
cand.CAND_ID.value_counts()$ cand.CAND_NAME.value_counts()
compound(px_etfs).plot(fontsize='small') # exclude from long strat the negative sectors?
newfile = pd.read_excel('export new.xlsx')$ oldfile = pd.read_excel('export old.xlsx')
poparr = fe.bs.csv2ret('boots_ndl_d4spx_1957-2018.csv.gz',$                        mean=fe.bs.SPXmean, sigma=fe.bs.SPXsigma,$                        yearly=256)
final_df = pd.concat([drugs_through_bp, other_drug])$ final_df = final_df.sort_values(by=['DWPC'], ascending = False)$ final_df
fakeNews = trump.loc[trump['text'].str.contains("fake"),:]$ ax = sns.kdeplot(fakeNews['year'], label="'Fake' word  usage")$
estimator.predict_proba(X1)
y_hat = linreg.predict(quadratic)$ plt.plot(y_hat,'-b')$ plt.show()
finals['type'] = "normal"$ finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'facilitator'
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(therapist_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Therapist Hours First Difference Autocorrelation')
sentiments_df = sentiments_df.sort_values(["Target","TweetsAgo"], ascending=[True, False])$
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Therapists Hours Partial Autocorrelation')
df.sort_values(['operator', 'part'], inplace=True)
sentiment_df['Timestamp'] = pd.to_datetime(sentiment_df.Timestamp)$ sentiment_df.sort_values(by='Timestamp')
history_df = pd.read_sql('measurement',engine)$ history_df['station'].value_counts()
X_test=tok.texts_to_sequences(X_test)$ x_test=sequence.pad_sequences(X_test,maxlen=maxlen)
analysis_historical =final_coin_data[['coin_name','date_hist','open_price','close_price']]$ a['Volitility_30_day'] = a['close_price'].rolling(window=30).std().reset_index(drop=True)$
data.sort_values(by = 'Age', ascending = False) #Really? What is a 95 yr old doing on NairaLand?
articles = db.get_sql(sql)$ articles.head()
output = pd.DataFrame(data={"Id":id, "bot":sub1})$ print(metrics.accuracy_score(output.bot, test_copy1.bot))$
for row in session.query(stations, stations.station).all():$     print(row)
scn_genesis = pd.to_datetime(min(USER_PLANS_df['scns_created']))
df.hastags = df.hastags.apply(getHasTags)
nbart_allsensors =nbart_allsensors.sortby('time')
tweets_raw = pd.read_csv(delimiter="\t",filepath_or_buffer='tweets_terr.txt', names=["lan","id","date", "user_name", "content"],encoding='utf-8',quoting=csv.QUOTE_NONE)
maxData = flowerKV.reduceByKey(lambda x, y: max(float(x),float(y)))$ print (maxData.collect())$
iso_gdf_2.plot();
most_likes = data_df_reduced.sort_values("likes", ascending=False).head(50)$ most_likes
data_l1 = tmpdf.index[tmpdf[tmpdf.isin(DATA_L1_HDR_KEYS)].notnull().any(axis=1)].tolist()$ data_l1
Featured_image = image_soup.find('img',class_="fancybox-image")$ print (Featured_image)$
output = model.predict(test[:, 1:5])$ rowID = [TEST.rowID for TEST in test_data.itertuples()]$ result_df = pandas.DataFrame({"rowID": rowID,"cOPN": list(output)})
csvpath2 = os.path.join('Desktop', 'Project-2', 'numberOneUnique.csv')$ import csv$ numberOneUnique_df = pd.read_csv(csvpath2, encoding="ISO-8859-1")
ndvi_of_interest02= ndvi.sel(time = time_slice02, method='nearest')$ ndvi_of_interest02
words_only_sp = [term for term in words_sp if not term.startswith('#') and not term.startswith('@')]$ corpus_tweets_streamed_profile.append(('words', len(words_only_sp))) # update corpus comparison$ print('The number of words only (no hashtags, no mentions): ', len(words_only_sp))
current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')$ path = 'Tweet_Frame_'+current_time$ print(path)
print(fee_types.get('user fees', 'return this if the key is not in the dict'))$ print(fee_types.get('not a value', 'return this if the key is not in the dict'))
bixi=pd.read_csv('OD_2018-07.csv')$ stations=pd.read_csv('Stations_2018.csv')
df = pd.get_dummies(df, columns=["component", "product"], prefix=["component", "product"])$ df.head(2)
df2[['ab_page', 'intercept']] = pd.get_dummies(df2['group'])$ df2['intercept'] = 1$ df2.head()
from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report$ from sklearn.model_selection import GridSearchCV, StratifiedKFold$ from sklearn import preprocessing
%%time$ grid_svc.fit(X, y)
datAll['blk_rng'] = datAll['Block_range'].map(str)+' '+datAll['Street_name'].map(str)
training, test = train_test_split(review_df, test_size=0.2, random_state=233)$ print(len(training), "train +", len(test), "test")
damage = crimes[crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE']$ damage.head()
convictions_df = convictions_df.replace(r'\n',' ', regex=True) $ convictions_df = convictions_df.replace(r'\t',' ', regex=True) $ convictions_df
cityID = 'a6c257c61f294ec1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Greensboro.append(tweet) 
print(r.text)
artists_info = [sp.artist(artist_id[i]) for i in range(0,num_songs)]
openmc.run()
indeed1 = indeed[['company','location','skills','title','salary_clean','category','duration_int','summary_clean']]$ indeed1.shape
regr = linear_model.LinearRegression()
chinese_vessels.to_csv('chinese_vessels_CLAV.csv', index=False, encoding='utf-8')
wards = gp.GeoDataFrame.from_file('Boundaries - Wards (2015-)/geo_export_e0d2c9f9-461f-4c6e-b5fd-24e123c74ee3.shp')
festivals.head(5)
client = foursquare.Foursquare(client_id='1KNECHOW4ALXKWS4OWU2TEUZMPW0WUN1NORS2OUMWWBBCV4C', client_secret='O4QOOLKGDZK44DTBRPQIUDGO2Z4XQYYJQOJ0LN5E5FAQASMM', redirect_uri='http://fondu.com/oauth/authorize')$ auth_uri = client.oauth.auth_url()
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['Retweets'].values, index=data['Date'])
model_data.to_hdf('..//data//model//model_data_12H.h5', key = 'xyz', complib = 'blosc')
sp500.Price
honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['Time stamp'].str.split("mhn").tolist(), columns = ['time_stamp1','time_stamp2','time_stamp3'])],axis = 1)$ honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['time_stamp3'].str.split("T").tolist(), columns = ['date','time'])], axis = 1)
model = gensim.models.Word2Vec(sentences, min_count=1)
with pd.option_context('display.max_rows', 150):$     print(news_period_df[news_period_df['news_entities'] == ''].groupby(['news_collected_time']).size())
df.loc['1998-09-10':'1998-09-15','MeanFlow_cfs':'Confidence']
bild = bild[bild.message != "NaN"]$ spon = spon[spon.message != "NaN"]
df_template = pd.DataFrame(index=datetimes)$ df_template.index.name = 'dt'
data = pd.read_csv('Dumping.csv', delimiter = ',', skiprows=0, squeeze=False, skip_blank_lines=True, index_col=None)$
df = pd.read_csv("FuelConsumption.csv")$ df.head()
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1 - p_new)])$ p_new_sim = new_page_converted.mean()$ p_new_sim
tobs_stn_281_df = pd.DataFrame.from_records(tobs_stn_281, columns=('Date','Station','Tobs'))$ tobs_stn_281_df.head()
with open("datasets/git_log_excerpt.csv", "r") as file:$     print(file.read())
last_tobs = session.query(Measurement.tobs, Measurement.station).order_by(Measurement.station.desc()).limit(365).all()$ last_tobs = pd.DataFrame(last_tobs)$ last_tobs.head()
status_data = pandas.read_csv("./Dataset Processed/mypersonality_final_classifiedByClass_onlyColumn.csv",encoding='cp1252')$
np.shape(rhum_fine)
ids = new_stops['stopid']$ new_stops[ids.isin(ids[ids.duplicated()])]
df["created"] = pd.to_datetime(df["created"])$ df["last_event"] = pd.to_datetime(df["last_event"])$ df.head()
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_hat), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
ibm_hr_int = ibm_hr_target.select(numerical)$ ibm_hr_int.show(3)
rain_df = pd.DataFrame(rain)$ rain_df.head()
recommendations = (model.recommendProducts(1059637, 5))$ recommendations[: 5]$ recArtist = set(list(elt[1] for elt in recommendations))$
df_concensus['esimates_count'].describe()$
le_data = le_data_all.reset_index().pivot(index='country',columns='year')$ le_data.iloc[:,0:3]
with open('/data/logs/kong/2018_07_03_access.log') as f:$     data_log = f.read()
criteria = so['ans_name'] == 'Scott Boston'$ so[criteria].head()
words_mention_scrape = [term for term in words_scrape if term.startswith('@')]$ corpus_tweets_scraped.append(('mentions', len(words_mention_scrape))) # update corpus comparison$ print('Total number of mentions: ', len(words_mention_scrape)) #, set(terms_mention_stream))
ws = Workspace.from_config()$ print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\n')
df_concat = pd.concat([bild, spon]) $
pd.to_datetime('11/12/2010', format='%d/%m/%Y')
merged1.isnull().sum()
X = X.drop(0, axis='index')
prcip_df.describe()$
initial_trend(series, 12)$ initial_seasonal_components(series, 12)$ triple_exponential_smoothing(series, 12, 0.716, 0.029, 0.993, 24)$
points_dic={"India":345,"Bangladesh":456,"Pakistan":789,"China":90}$ points=pd.Series(points_dic)$ points$
for col_cells in ws.iter_cols(min_col=28, max_col=28):$     for cell in col_cells:$         cell.number_format = '0.00'
table_rows = driver.find_elements_by_tag_name("tbody")[4].find_elements_by_tag_name("tr")$
df.loc[df['field1']>27,['created_at','field1']]
prcp_df = pd.DataFrame(prcp)$ prcp_df.head()
exiftool -csv -createdate -modifydate cisuabf6/cisuabf6_cycle1.MP4 cisuabf6/cisuabf6_cycle2.MP4 cisuabf6/cisuabf6_cycle3.MP4 cisuabf6/cisuabf6_cycle4.MP4  > cisuabf6.csv
post_number = len( niners[niners['Jimmy'] == 'yes']['GameID'].unique() )$ print post_number
s.loc['c'] $
joined = joined.astype(intDict)
extractor = twitter_setup()$ tweets = extractor.user_timeline(screen_name="realDonaldTrump", count=200)$ print("Number of tweets extracted: {}.\n".format(len(tweets)))
store_items.fillna(method='ffill', axis=1) # filled with previous value from that row
tmi = indices(tmaggr, 'text', 'YearWeek')$ tmi.head()
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Doctors', doc_duration, window=52)
len(calls.location.unique())
pd.merge(msftAR0_5, msftVR2_4)
for col in user_df.columns:$     print col, len(user_df[user_df[col].isnull()])
prophet_df = pd.DataFrame()$ prophet_df['y'] = df[target_column]$ prophet_df['ds'] = df['date']
df.to_excel("../../data/stocks_msft.xlsx", sheet_name='MSFT')
news_df['Timestamp'] = pd.to_datetime(news_df['Date'], infer_datetime_format=True)$ news_df.head()
df_sb.isDuplicated.value_counts()$ df_sb.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
ride_df_urban = urban.groupby('city')$ city_df_urban = urban.set_index('city')
prec_nc = Dataset("../data/nc/pr_wtr.mon.mean.nc")
userActivity = userArtistDF.groupBy("userID").sum("playCount").collect()$ pd.DataFrame(userActivity[0:5], columns=['userID', 'playCount'])
s = pd.Series(todays_datetimes)$ s.diff().mean()
symbol='^NBI'$ benchmark1 = web.DataReader(symbol, 'yahoo' , start_date ,end_date)
pgh_311_data.resample("Q").mean()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()$
year12 = driver.find_elements_by_class_name('yr-button')[11]$ year12.click()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-20&end_date=2018-08-20&api_key=' + API_KEY$ r = requests.get(url)
tmax_day_2018.coords
print(s1.head(3))$ print()$ print(s1.tail())
input_folder = '/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/minimum temperature'$ glob_string = '*.nc'
from bokeh.io import output_notebook$ output_notebook()
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
from IPython.display import HTML$ HTML('<iframe src="https://opendata.aemet.es/centrodedescargas/inicio" width="700" height="400"></iframe>')
df = pd.DataFrame.from_dict(tweet_info)$ df.head()
df.to_csv('Tableau-CitiBike/TripData_2017_Winter.csv', index=False)
annual_returns.head() $
countries_df = pd.read_csv('/Users/pra/Desktop/AnalyzeABTestResults 2/countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
df['up_votes'].quantile(q=.50)$ df.loc[df['up_votes'] == 192674]['title'].values
feeds = json.loads(response)$ print(type(feeds))$ print(feeds)
def getResults(JsonReponse):$     return JsonReponse.get('ResultSet').get('Result')
trump['text'] = trump['text'].str.lower()$
df = df[df.tax_class2 == '1']$
stations = session.query(Measurement).group_by(Measurement.station).count()$ print(f"{stations} stations")
clf.fit(X_train, y_train)
trump_originals = trump[trump.is_retweet == False]$ trump_originals.shape$
df = pd.read_csv("WaterUsageSample.csv")$ df.shape
avi_data = pd.read_csv('AviationData.csv', encoding='ISO-8859-1')$
groups = contract_history[['INSTANCE_ID', 'UPD_DATE']].merge(intervention_train[['INSTANCE_ID', 'CRE_DATE_GZL']])
Station = Base.classes.stations
k ['MONTH'] = k.index.map(dict)
model.save('/tmp/model.atmodel')$
INT['Create_Date'].min()
tweets_df = tweets_df[tweets_df.userTimezone.notnull()]$ len(tweets_df)$
from nltk.corpus import conll2000$ from nltk import conlltags2tree, tree2conlltags$ train_labels[3]
chinese_vessels = chinese_vessels_wcpfc.copy()$ chinese_vessels = mergetable(chinese_vessels, chinese_vessels_iotc)
combined_df4['split_llpg1']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(x.split(',')[1:2]))$ combined_df4['split_llpg2']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(x.split(',')[1:3]))$ combined_df4.head()
y_newpage = df2["user_id"].count()$ prob_newpage = x_newpage/y_newpage$ prob_newpage$
type(df), df.shape$
psy = psy_native.copy()$ psy = psy.dropna(thresh=(psy.shape[1]/2), axis=0)$ psy.shape  # 18 people removed
X = vectorizer.fit_transform(clean_train_reviews)
Base.prepare(engine, reflect=True)
df.to_csv('top40_processed.csv', date_format="%Y-%m-%d", index=False)
x = store_items.isnull()$ print(x)$
y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)$ y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)
step_counts.dtypes
show_data.plot(kind="bar")
store_items.fillna(0)
r_lol = r.json()['dataset']['data']
climate_vars.head()
model_df = topics_data.drop(['body', 'comms_num', 'id', 'title'], axis=1)
b[b.T.sum()==c].index.min()
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local')
pd.DataFrame.to_csv(receipt_data, 'receipt_data.csv')
scope_df['Invalid AC'] = scope_df['textinfo'].str.contains('Acceptance|AC', case = False, regex = True) == False$ scope_df[scope_df['Invalid AC']].to_excel(writer, index=False, sheet_name='Invalid AC', freeze_panes=(1,0), columns=['Team_story', 'key_story', 'reporter_story'])$
df_groups = pd.read_csv('groups.csv')$ df_groups.head()
local_tz = pytz.timezone('America/New_York')
nlp = spacy.load('en')$ op_ed_articles['full_text_tokenized'] = op_ed_articles['full_text'].apply(lambda x: nlp(x))$
contractor_merge = pd.merge(contractor_clean, state_lookup,$                             on=['state_id'], how='left')
status_data = status_data.drop(['STATUS', '#AUTHID', 'sEXT', 'sNEU', 'sAGR',$                                     'sCON', 'sOPN', 'DATE'], axis=1)
points=pd.Series([630,25,26,255],$     index=['India','Bangladesh','Pakistan','China'])$ print(points)
import pickle$ filename = 'automl_feat.sav'$ pickle.dump(automl_feat, open(filename, 'wb'))
jdfs.loc[~jdfs.fork]
subset_data = pd.merge(interest_dates, product_time, left_index=True,#left_on= 'date', $                        right_index=True, how='inner') #Match dates and merge
file = 'https://assets.datacamp.com/production/course_2023/datasets/dob_job_application_filings_subset.csv'$ df = pd.read_csv(file)$ print(df.head())
!hdfs dfs -cat {HDFS_DIR}/p32b-output/part-0000* > p32b_results.txt$
payload = {'api_key': 'apikey'}$ r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json',params=payload)$
i1 = pd.Index([1, 3, 5, 7, 9])$ i2 = pd.Index([2, 3, 5, 7, 11])
Which_Years_for_each_DRG.loc[345]$
segments.st_time.dtype$ datetime.strptime(segments.st_time.loc[0],'%m/%d/%y %H:%M')
from ditto.network.network import Network$ G=Network()$ G.build(m)
import pandas as pd$ the_dict = pd.read_clipboard().to_dict('records')$ the_dict
inter_by_date = niners.groupby('Date')['InterceptionThrown'].sum()$ inter_by_date;
x.loc[x.loc[:,"A"]>0.6,"A"]
columns = inspector.get_columns('clean_hawaii_measurements.csv')$ for c in columns:$     print(c['name'], c['type'])$
bigram_chunker = BigramChunker(train_trees)$ print(bigram_chunker.evaluate(valid_trees))
dataBloodType.human_id = dataBloodType.human_id.str.lower()$ df2 = df.merge(dataBloodType,left_on = 'Sample', right_on='human_id', how='inner')$ del dataBloodType$
data.loc[:, 'TMAX'].head()
commmon_intervention_train = intervention_train.index.intersection(intervention_history.index)
df_ct['cleaned_text'] = df_ct['text'].apply(lambda x : text_cleaners(x))
pd.concat([msftAV, aaplA], join='inner')
git_log['timestamp'].describe()
number_of_commits = len(git_log)$ number_of_authors = len(git_log.dropna().author.unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df = quandl.get('WIKI/GOOGL')
with open('key_phrases.pickle', 'wb') as f:$     pickle.dump(key_phrases, f, pickle.HIGHEST_PROTOCOL)
root_universe = openmc.Universe(name='root universe', cells=[cell])
ratings=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\ratings.csv')
opening_prices = [x for x in opening_prices if x is not None]$ print('Highest opening price - {} \nLowest opening price - {}'.format(max(opening_prices), min(opening_prices)))
search['search_weekday'] =  search.timestamp.dt.dayofweek+1$ search['trip_start_date_weekday'] =  search.trip_start_date.dt.dayofweek+1$ search['trip_end_date_weekday'] =  search.trip_end_date.dt.dayofweek+1
poparr2 = fe.bs.hybrid2ret(poparr,$                            mean=-fe.bs.SPXmean, sigma=fe.bs.SPXsigma,$                            yearly=256)
df = df.dropna(axis=0, how='any')$ print(str(datetime.datetime.now()) + ' Deleted rows with empty values;')
LabelsReviewedByDate = wrangled_issues_df.groupby(['Priority','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
g8_groups.agg(['mean', 'std'])
df_ll.head(2)$ df_ll.isDuplicated.value_counts()$ df_ll.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
company_count_df = pd.DataFrame.from_dict(company_count, orient='index')$ company_count_df.columns=['Count']$ company_count_df.sort_values(by='Count', ascending=False).plot(kind='bar', figsize=(16,8), cmap='Set3')$
userMovies = userMovies.reset_index(drop=True)$ userGenreTable = userMovies.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ userGenreTable
df3['Current Status'].unique()
rain_df.describe()
a1 = np.array([1, 2, 3, 4]) $ a2 = np.array([4, 3, 2, 1]) $ a1 + a2
rf = RandomForestClassifier(n_estimators=50, max_depth=50, n_jobs=4)$ rf.fit(X = clim_train, y = size_train)
node_types_DF = pd.read_csv('network/recurrent_network/node_types.csv', sep = ' ')$ node_types_DF
sts_model.pred_table()
!convert materials-xy.ppm materials-xy.png$ Image(filename='materials-xy.png')
yc_new2['Tip_Amt'] = yc_new['Tip_Amt'] / yc_new['Fare_Amt'] * 100$ yc_new2.head()
tfav.plot(figsize = (16,4), label = "Likes", legend = True)$ tret.plot(figsize = (16,4), label = "Retweets", legend = True);  $
!export HOST_COMPILER=gcc-5; export CUDAHOME=/usr/local/cuda; cd implicit; python3 setup.py install
plt.scatter(USvideos['likes'], USvideos['views'])
S_distributedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_distributedTopmodel.txt')
data[data.userScreen=='KTHopkins']
idx = df2['Number'].values$ Xtrain = Xtrain[idx,:] $ Xtrain.shape
sns.factorplot('sex', data=titanic3, hue='pclass', kind='count')
clf = LogisticRegression(fit_intercept=True).fit(X, y)
categorical = free_data.dtypes[free_data.dtypes == "object"].index$ free_data[categorical].describe()
month = pd.get_dummies(questions['month_bought'])
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/SCN_BDAY_qthis.scn_age.count()
crs = {'init': 'epsg:4326'}$ geometry = df_TempJams['lineString']$ geo_TempJams = gpd.GeoDataFrame(df_TempJams, crs=crs,geometry = geometry)
df1=pd.read_csv("approval data clean values only.csv")$ df1.head()
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')
tz_cat['tweetRetweetCt'].max()$ tz_cat.index[tz_cat['tweetRetweetCt'] == tz_cat['tweetRetweetCt'].max()].tolist()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)$ regr = LinearRegression()$ regr.fit(X_train, y_train)
df.resample('A').mean()
credentials = json.load(open('./apikey.json', 'r'), encoding='utf-8')
df = pd.read_csv('data/test1.csv', parse_dates=['date'], index_col='date')$ df
md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128)$
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()$
our_nb_classifier.last_probability
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Amount"])
FREEVIEW.plot_fixation_durations(raw_freeview_df)
invalid_ac_df.groupby(['reporter_story']).sum().sort_values(by=['Invalid AC'], ascending=False).head()
openmoc_geometry = get_openmoc_geometry(sp.summary.geometry)
df_variables.to_csv("../01_data preprocessing/data new/variables.csv", encoding="utf-8", sep=",", index=False)
from sklearn.naive_bayes import GaussianNB
car=pd.read_csv('http://data.sfgov.org/resource/cuks-n6tp.csv')$ car.dtypes
results = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ for row in results:$     print(f"The most recent date recorded is {row}.")
gs.score(X_test_total_checked, y_test)
knn = KNeighborsClassifier(n_neighbors=5)$ knn.fit(X_train, y_train)$ knn.score(X_test, y_test)
lv_workspace.get_subset_object('B').get_step_object('step_1').show_settings()$ lv_workspace.get_subset_object('B').get_data_filter_object(step=1).exclude_list_filter
user = user[user.is_enabled >= 0]
from nltk.corpus import stopwords$ print(stopwords.words('english'))
fg.savefig("Sentiment Analysis of Tweets.png")
df3 = df3.add_suffix(' Closed')$ df7 = pd.merge(df4,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE Closed')$
master_df['Distance in km']=std.fit_transform(master_df[['Distance in km']])$ master_df['age']=std.fit_transform(master_df[['age']])$ master_df['Temp high (F)']=std.fit_transform(master_df[['Temp high (F)']])
df["EDAD"].apply(sumar_1)
Base.classes.keys()
beta_dist[np.arange(mcmc_iters), betas_argmax] *= 10000$ sum(beta_dist)/1000
result.route
tokenizer = Tokenizer(char_level=True, filters=None)$ tokenizer.fit_on_texts(invoices)
import urllib3, requests, json, base64, time, os$ warnings.filterwarnings('ignore')
park_attendance_mean = games_df.groupby('park_id').mean()['attendance'].to_frame()$ park_attendance_mean.reset_index(inplace=True)
pattern = re.compile('AA')$ print(pattern.findall('AAbcAA'))$ print(pattern.findall('bcAA'))
for row_index, row in df.iterrows():$     print(row_index, row)
scores[scores.RottenTomatoes == scores.RottenTomatoes.max()]
(session.query(Measurement.station, Station.name, func.count(Measurement.station))$  .filter(Measurement.station==Station.station)$  .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).limit(1).all())
demand = model.get_parameter(par='demand')$ list(set(demand.commodity))$ demand.head()
json_data = r.json()$ print(type(json_data))
import seaborn as sns$ sns.set()$ sns.set_context("talk")
df.head()
tdf['label'] = (tdf['smoker'] == 'Yes').astype(float)$ tdf.sample(5)
USvideos['like_ratio'] = USvideos['likes'] / (USvideos['likes'] + USvideos['dislikes']) $ USvideos['like_ratio'].describe()$
grid.cv_results_['mean_test_score']
joined=join_df(joined,trend_de,["Year","Week"],suffix='_DE')$ joined_test=join_df(joined_test,trend_de,["Year","Week"],suffix='_DE')$ sum(joined['trend_DE'].isnull()),sum(joined_test['trend_DE'].isnull())
df[(abs(df['Open']-df['High'])<0.2 ) | (abs(df['Close']-df['Low'])<0.2)]
treatment_notAligned = ((df['group'] == 'treatment') & (df['landing_page'] != 'new_page'))$ treatment_notAligned.sum()
dfa.head()
df = df.sort_values(by='fav', ascending=False)
utility_patents_df.number_of_claims.describe()
df_tweets_sort.to_csv('News_Tweets_Data.csv',index=False)
stamp_name = '4. timestamp'
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
assert not tcga_target_gtex_expression_hugo_tpm.isnull().values.any()$ assert not treehouse_expression.isnull().values.any()$ assert np.array_equal(tcga_target_gtex_expression_hugo_tpm.index, treehouse_expression.index)
df.loc[0, 'review'][:-500]
shows = pd.read_csv("ismyshowcancelled_tmp_1.csv",index_col=0)
mammals = session.query(NA).filter(NA.genus == 'Antilocapra').all()$ for mammal in mammals:$     print("Family: {0}, Genus: {1}".format(mammal.family, mammal.genus))
reviews.info()$ reviews=pd.read_csv("ign.csv",index_col=['Unnamed: 0','score_phrase'])$ reviews.head()
prec_us = prec_nc.variables['pr_wtr'][1, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(prec_us)
today = datetime(2014,11,30)$ tomorrow = today + pd.Timedelta(days=1)$ tomorrow
excel=pd.rea_excel("File Name")
trt_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][1]$ print("P(trt_con) = %.4f" %trt_con)
reddit_master['Class_comments'].value_counts()/reddit_master.shape[0]
conn.close()
temp_df.head()
pvt['customerId'] = pvt['ga:dimension2'].str.rpartition('-')[0].str.strip()$ pvt['customerName'] = pvt['ga:dimension2'].str.rpartition('-')[2].str.strip()$ pvt
import pandas as pd$ import matplotlib.pyplot as plt
industry_group=df.groupby(by="GICS Sector")$ industry_group
df_members = pd.read_csv('members.csv', encoding = 'latin-1')$ df_members['joined'] = pd.to_datetime(df_members['joined'], yearfirst = True)$ df_members.head()
joined.dtypes
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
err = (actual - expected[np.newaxis,:,:]).reshape(-1)$ err.shape
df['len_convo'] = df.conversations.apply(len)$ print("... got conversation lengths")
logging.info('Dropping unnecessary variables')$ data.drop(['Date', 'Open', 'High', 'Low', 'Market Cap', 'Dependent Variable'], axis=1, inplace=True)
mydata.plot(figsize =(15 ,6)) $ plt.show()
import string$ punct_re = r'[^\s\w\d]'$ trump['no_punc'] = trump['text'].str.replace(re.compile(punct_re), ' ')$
stocks.loc['Apple']
data['Age'].max()
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']])$ results = logit_mod.fit()
items = {'Bob': pd.Series([245, 25, 55], index=['bike', 'pants', 'watch']),$          'Alice': pd.Series([40, 110, 500, 45], index=['book', 'glasses', 'bike', 'pants'])}
df = pd.read_csv('basic_statistics_single_column_data.csv')
! rm -rf splits1$ ! mrec_prepare --dataset ml-100k/u.data --outdir splits1 --rating_thresh 4 --test_size 0.5 --binarize
scaler=StandardScaler()$ scaler.fit(x_train)
df['MeanFlow_cfs'].describe()
pystore.delete_store('mydatastore')$
for col in temp_columns:$     print(col)$     dat.loc[:,col]=dat[col].interpolate(method='linear', limit=3)
pv1 = pd.pivot_table(df_cod, values="Age at death", index="Death year")$ pv1
print(len(alltrains))$ alltrains.to_csv('data/JY-HKI-dec17-mar18.csv')
happiness_df=happiness_df.groupby('dates').mean()$
df_concensus = pd.read_csv( pwd+'/consensus_shift_history.052317.csv') #consensus?
df.head()
grid_pr_fires.plot.scatter(x='glon',y='glat', c='pr_fire', $                            colormap = 'RdYlGn_r')
summary_bystatus = df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9]).T$ summary_bystatus
sl[sl.status_binary==0][(sl.today_preds==1)].shape
df.user_id.nunique()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', header=None)$ mydata.head(5)
import lxml.html$ from lxml.cssselect import CSSSelector$ tree = lxml.html.fromstring(r.text)$
iris.iloc[(iris.iloc[:,0]>7.5).values,4]$
sanne_data = bird_data[bird_data.bird_name == 'Sanne']$ print(sanne_data.timestamp.head())
for c in ccc:$     if not os.path.isdir('output/' + c):$         os.makedirs('output/' + c)
df = pd.read_csv("ab_data.csv")$ df.head()
validation.analysis(observation_data, distributed_simulation)
%%time$ model = AlternatingLeastSquares(use_gpu=False)$ model.fit(matrix_data)
station_count = session.query(Station.id).count()  $ print(f'There are {station_count} weather stations in Hawaii.')
Suburban = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Suburban")]$
a = np.arange(25).reshape(5,5) ; a$
sentiments_df = pd.DataFrame.from_dict(sentiments)$ unique_sentiments_df = sentiments_df.drop_duplicates("Tweet Text", keep = "first")$ unique_sentiments_df
twitter_daily_df = twitter_daily_df.join(distinct_users, ["Day","Company"]).orderBy('Day','Company')
charge = reader.select_column('charge')$ charge = charge.values # Convert from Pandas Series to numpy array$ charge
input_node_types_DF = pd.read_csv('network/source_input/node_types.csv', sep = ' ')$ input_node_types_DF
fe.bs.bootshow(256, poparr2, repeat=3)$
cityID = '6ba08e404aed471f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Riverside.append(tweet) 
new_df = df.replace(-99999, np.NAN)$ new_df
ts.get_profit_data(2018,1)
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
df = pd.read_csv('weather.csv')
style_bw.tail(5)
search_rating = np.vectorize(search_rating, otypes=[np.float])$ data['rating'] = search_rating(data['text'])$ data = data[pd.notnull(data['rating'])]
from collections import Counter$ words = set()$ word_counts = data['Tweets'].apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0)$
newsorgs_bar = mean_newsorg_sentiment["News Organization"]$ compound_bar = mean_newsorg_sentiment["Compound"]$ x_axis = np.arange(0, len(compound_bar), 1)
inspector.get_table_names()
list_to_merge = list(db.tweets.find({},{"id": 1, "user": 1,"text": 1,"hashtags":1, "_id": 0}))
DataAPI.write.update_secs_industry_gics(industry='A_GICSL1', trading_days=trading_days, override=False)
df_course_tags = pd.read_csv('course_tags.csv')  $ df_user_assess_scores = pd.read_csv('user_assessment_scores.csv')$ df_user_course_views = pd.read_csv('user_course_views.csv')
df = pd.read_csv("AAPL.csv",na_values = ["null"])$ df.dtypes
mfx = sts_model.get_margeff()$ print(mfx.summary())
data.sample(4)
stories.tags.head()
data.describe(include=['O'])
set(tcga_target_gtex_labels.disease).intersection(treehouse_labels_pruned.disease)
a = np.arange(0.5, 10.5, 0.5)$ a
from pymyinstall.fix import fix_scipy10_for_statsmodels08$ fix_scipy10_for_statsmodels08()
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read(), end='')
trading_volume = [x[6] for x in data_table if x[6] is not None]$ print('Average Trading Volume: {:f}'.format(sum(trading_volume) / len(trading_volume)))
df_tte_ri.drop(['UnBlendedRate'], axis=1, inplace=True)$ df_tte_ri.head(2)
from bs4 import BeautifulSoup$ example1 = BeautifulSoup(train["comment_text"][0], "html.parser")
results_jar_rootDistExp, output_jar_rootDistExp = S.execute(run_suffix="jar_rootDistExp", run_option = 'local')
cityID = '0570f015c264cbd9'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Louis.append(tweet) 
clean_merge_df = merge_table_df.drop(["Number One_x"], axis=1)$ clean_merge_df.rename(columns={'Number One_y': 'Reached Number One'}, inplace=True)
requests.get(saem_women)
 (merged_visits.pipe(lambda df: df['Fail'].div(df['Pass']))$   .dropna()$   .sort_values(ascending=False))$
cityID = 'b004be67b9fd6d8f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Norfolk.append(tweet) 
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=3w2gwzsRAMrLctsYLAzN')$ json_data = r.json()$ print(type(json_data))
ax = data[["TMAX", "TMIN", "TMED"]].plot()$ ax.set_xlabel('DATE')$ ax.set_title('Temperaturas')
pf.cost.sum()/100
print(rdc.feature_importances_[0:113])$
aapl = getStockPrice("AAPL",1982, 1, 1, 2018, 1, 23)$ aapl.head()$
!hdfs dfs -cat {HDFS_DIR}/p32cfr-output/part-00001 {HDFS_DIR}/p32cfr-output/part-00000 > p32cfr_results.txt
start = time()$ ldamodel_3 = Lda(doc_term_matrix, num_topics=16, id2word = dictionary, passes=25, chunksize=2000)  # with 16 topics$ print('CPU time for LDA_3: {:.2f}s'.format(time()-start))$
db.query("select min(created) from bike_locations where provider='limebike' and raw->'attributes'->>'vehicle_type'='scooter'").export('df')
analysis_historical.groupby('coin_name').apply(lambda x: x.sort_index(ascending=False, inplace=True))$ analysis_historical['daily_log_return'] = (np.log(analysis_historical['close_price'] /$     analysis_historical['close_price'].shift(-1)))$
df = pd.read_csv('first_17500_csv', index_col=0)
writer = pd.ExcelWriter('output.xlsx')$ frame.to_excel(writer,'Sheet1')$ writer.save()
df4 = df3.drop_duplicates()$ df4.describe()
stock_data.index=pd.to_datetime(stock_data['latestUpdate'])$ stock_data['latestUpdate'] = pd.to_datetime(stock_data['latestUpdate'])
json_data = r.json()$ json_data
days_range = pd.date_range(start=min_date, end=max_date, freq='D')$ idx_days = [str(s)[:10] for s in days_range]
loc = session.query(Stations.station_id).count()$ print("Total {} stations were used to gather data.".format(loc))
scores[scores.IMDB == min_IMDB]
end_date = pd.Series(pd.to_datetime(end_date).strftime('%Y-%m'),index=churned_ix)
df_schools.describe()
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
sns.distplot(data['Age'])
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=' + API_KEY)$
set(dat.columns) - set(temp_columns+ incremental_precip_columns+ general_data_columns+ wind_dir_columns)
filepath = os.path.join('input', 'input_plant-list_SI.csv')$ data_SI = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_SI.head()
df_new_conv = df_newpage.query('converted == "1"')$ x_new_conv = df_new_conv["user_id"].count()$
sentiment.to_csv("sentiment.csv", encoding="utf-8", index=False, header=True)
fox = news_sentiment('@FoxNews')$ fox['Date'] = pd.to_datetime(fox['Date'])$ fox.head()
df.head(10)
iv = options_frame[options_frame['Expiration'] == '2016-03-18']$ iv_call = iv[iv['OptionType'] == 'call']$ iv_call[['Strike', 'ImpliedVolatilityMid']].set_index('Strike').plot(title='Implied volatility skew')
files_txt = glob.glob("*.txt")
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
pulledTweets_df.sentiment_predicted_nb.value_counts().plot(kind='bar', $                                                            title = 'Classification using Naive Bayes model')$ plt.savefig('data/images/Pulled_Tweets/'+'NB_class_hist.png')
weather_norm = weather_features.apply(lambda c: 0.5 * (c - c.mean()) / c.std())
plot_results = actual_value_second_measure.join(predicted_probs_first_measure)$
cityID = '161d2f18e3a0445a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Raleigh.append(tweet) 
df4 = df.drop('Cabin', axis=1) \$     .loc[lambda x: pd.notnull(x['Embarked'])] \$     .fillna(30)
df3 = df2.copy()
predictions = dtModel.transform(testData)
useful_indeed = indeed[~indeed['salary_clean'].isnull()]$ useful_indeed.shape$
plt.plot(model_output.history['loss'],c='k',linestyle='--')$ plt.show;
lm.score(x_test,y_test)
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::1].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.'])$ ax.set_ylabel('Relative error')$
n_user_days.value_counts().sort_index()
plt.plot(model_output.history['loss'],c='k',linestyle='--')$ plt.plot(model_output.history['val_loss'],c='purple',linestyle='-')$ plt.show;
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)$
df = pd.read_csv(pump_data_path, index_col = 0)$ df.head(1)
html = browser.html$ soup = bs(html, 'html.parser')$ browser.click_link_by_partial_text('FULL IMAGE')
df_bud = pd.read_csv(budFile, usecols = budCols, $                  dtype = bud_dtypes)
movie['point'] = movie['point'].astype(float)$ movie.info()$
df.xs(key=('a', 'ii', 'z'))
X2 = now[[col for col in now.columns if col != 'bitcoinPrice_future_7']]
req = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY)$ json_data = req.json()
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$ model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)
scaler = MinMaxScaler()$ data[intFeatures] = scaler.fit_transform(data[intFeatures])
type(df.date[0])
np.linspace(0, 100, 5)
new.describe()
xml_in.head(5)
!hdfs dfs -put CC_records.csv {HDFS_DIR}/Consumer_Complaints.csv
%%time$ responded_all_jan23[fields].to_excel(export_filename_jan23, index=False)$ responded_all_feb5[fields].to_excel(export_filename_feb5, index=False)
search['prefetch'] = search['message'].apply(prefetch)
display(data_rf.head(10))
for c in ccc:$     vhd[c] = vhd[vhd.columns[vhd.columns.str.contains(c)==True]].sum(axis=1)
ndvi_grid = np.array(np.meshgrid(lon_us, lat_us)).reshape(2, -1).T$ np.shape(ndvi_grid)
learn.data.test_dl = test_dl$ log_preds_test = learn.predict(is_test=True);
df_piotroski = pd.DataFrame({'id' : stock_id_ls, 'name': stock_name_ls}, columns = ['id', 'name', 'market_type', 'quant', 'market_sum', 'property_total', 'debt_total', 'listed_stock_cnt', 'pbr', 'face_value',])
df_vndr_id = pd.DataFrame(X_copy['vndr_id'])
len(df[~(df.event_properties == {})])
close_px['AAPL'].ix['01-2011':'03-2011'].plot()
results_distributedTopmodel, output_DT = S_distributedTopmodel.execute(run_suffix="distributedTopmodel_hs", run_option = 'local')
df_cat.head()
import matplotlib.pyplot as plt$ plt.plot(twitter_df['created_at_time'], twitter_df['retweet_count'], 'ro')$ plt.show()$
S_1dRichards.initial_cond.filename
driver = webdriver.Chrome()
import csv$ data = pd.read_csv("McDonaldstweets.csv")$ display(data)
sp500.index
station_count = session.query(Stations.id).count()$ print ("Total Number of Stations are: "+ str(station_count))
s4 = pd.Series([10, 0, 1, 1, 2, 3, 4, 5, 6, np.nan])$ len(s4)
data.head()
Google_stock.corr()
key = 'o_r_mapper:91ala:ala103077070913625'$ rdb = c.srdb_new(0)$
label_and_pred = lrmodel.transform(testData).select('label', 'prediction')$ label_and_pred.rdd.zipWithIndex().countByKey()
data.iloc[1:10:3]
drop_columns = ['discount' , 'plan_list_price', 'actual_amount_paid', 'transaction_date','is_auto_renew' ]$ df_transactions = df_transactions.drop(drop_columns, 1)
from sklearn import linear_model$ lin_reg = linear_model.LinearRegression()$ lin_reg.fit(x_train,y_train)
final.to_csv('final.csv')
logistic_file = '../data/model_data/log_pred_mod.sav'$ pickle.dump(logreg, open(logistic_file, 'wb'))
ax = time_length.plot(figsize=(16,4), color='r', title='Tweet length vs. DateTime')$ ax.set_xlabel("DateTime")$ ax.set_ylabel("Tweet length")
print('Scanning for all greetings:')$ for key, row in table.scan():$     print('\t{}: {}'.format(key, row[column_name.encode('utf-8')]))$
TrainingSamples = int(MaxPoints * 0.7)$ ValidationSamples = int((MaxPoints-TrainingSamples)/2)$ TestSamples = MaxPoints - TrainingSamples - ValidationSamples
multi_col_lvl_df.applymap(lambda x: np.nan if np.isnan(x) else str(round(x/1000, 2)) + "k").head(10)
RegO = pd.to_datetime(voters.RegDateOriginal.map(lambda x: x.replace(' 0:00', '')))$ Reg = pd.to_datetime(voters.RegDate.map(lambda x: x.replace(' 0:00', '')))$ datecomp = pd.DataFrame({'OrigRegDate':RegO, 'RegDate':Reg})
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
df.dtypes
intervention_train['MILLESIME'] = intervention_train['MILLESIME'].astype(int)$ intervention_train['RESOURCE_ID'] = intervention_train['RESOURCE_ID'].astype(int)
market_cap_df = pd.read_csv('../data/total_market_cap.csv', index_col='Date', parse_dates=True)$ market_cap_df.head()
femalebydatenew  = femalebydate[['Sex','Offense']].copy()$ femalebydatenew.head(3)$
paired_df_grouped.sort_values('co_occurence', ascending=False, inplace=True)$ paired_df_grouped.loc[:, ['dataset_1', 'all_co_occurence']].head(20)$ paired_df_grouped['top10'] = paired_df_grouped.co_occurence.apply(lambda x: x[:10])$
vectorizer = TfidfVectorizer(max_df=0.1)  $ train_tweets_vector = vectorizer.fit_transform(train_tweets['pasttweets_text'])$ dev_tweets_vector = vectorizer.transform(dev_tweets['pasttweets_text'])
x_axis = np.arange(0, Total_Number_of_Rides_max+6, 5)$ x_axis
TripData_merged3.isnull().sum()
cvec.fit(X_train)$ X_train_matrix = cvec.transform(X_train)$ print(X_train_matrix[:5])
print(soup.prettify())
df3 = df3.add_suffix(' Created')$ df7 = pd.merge(df4,df3,how='left',left_on='Date Created',right_on='MEETING_DATE Created')$
dummy_df['price_change_1day'] = (dummy_df['price'].shift(-1440) - dummy_df['price']).fillna(method = 'ffill')$ dummy_df['price_change_2days'] = (dummy_df['price'].shift(-2880) - dummy_df['price']).fillna(method = 'ffill')$ dummy_df['price_change_3days'] = (dummy_df['price'].shift(-4320) - dummy_df['price']).fillna(method = 'ffill')
year_ago = datetime.now().date()$ query.filter(cast())
print(ser_obj.values)$ print(ser_obj.index)
future = m.make_future_dataframe(periods=52*3, freq='w')$ future.tail()
df = pd.DataFrame.from_dict(dst_cap, orient="index")$ df.columns = ['Fequency']
df.to_csv(DATAPATH+'submit_most_popular_category.csv', index=False)$
import re$ re.split('\.,', price_tmp)$
prcp_df['date'] = [dt.datetime.strptime(x, "%Y-%m-%d") for x in prcp_df['date']]
df_valid["Died"] = pd.to_datetime(df_valid['Died'], unit="ms")$ df_valid = df_valid[df_valid["Died"] < datetime.strptime("2018-01-01", "%Y-%m-%d")]
grouped_data = dj_df.groupby(by='company_ticker')$ cmp_to_scaler = {}$ norm_dj_df = pd.DataFrame(columns=dj_df.columns) # Dataframe with quarter_start, company_ticker, normalized-revenue information.
authorization_instance = tweepy.OAuthHandler(consumer_key,consumer_secret)$ authorization_instance.set_access_token(access_token, access_token_secret)$ authorization_instance.secure = True
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=5&column_names=[api_key=***')$ print(r.json())$
idx = pd.period_range('2011','2017',freq='Q')$ idx
mydata.info()
lons, lats = np.meshgrid(lon_us, lat_us)$ plt.plot(lons, lats, marker='.', color='k', linestyle='none')$ plt.show()
dic = dict(req.json())
r.json()['dataset_data']
display(data.tail(10))
for urlTuple in otherPAgeURLS[:3]:$     contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ contentParagraphsDF
df.to_csv('../output/releases_with_demographics.csv')
DataAPI.write.update_indicators(["HIGH", "LOW", "IPO_LISTDAYS"], trading_days, override=False, log=True)
bb['close'].apply(rank_performance).value_counts().plot()
!wget -O ChurnData.csv https://ibm.box.com/shared/static/8s8dn9gam7ipqb42cm4aehmbb26zkekl.csv
sns.set(font_scale=1.5, style='whitegrid')$ sns.set_palette(sns.cubehelix_palette(rot=-.4))
df_2004['bank_name'] = df_2004.bank_name.str.split(",").str[0]$
inspector = inspect(engine)$ inspector.get_table_names()
pd.merge(msftAR0_5, msftVR2_4, how='outer')
import pip$ pip.main(['install','quandl'])
auth = tweepy.OAuthHandler(consumer_key,consumer_secret)$ auth.set_access_token(access_token,access_token_secret)$ api = tweepy.API(auth) 
data = pd.Series([0.25,0.5,0.75,1])$ data
flight.crosstab('start_date','from_city_name').show()$
plt.style.use('default')
columns = inspector.get_columns('stations')$ for c in columns:$     print(c['name'], c["type"])$
Google_stock['Adj Close'].describe()
aggdf = tweetdf[['lat','lng','text']].loc[~pd.isnull(tweetdf['lat'])].groupby(['lat','lng']).agg('count').reset_index()$
filenames = glob.glob(os.path.join(input_folder, glob_string))
idx = pd.period_range('2011',periods=10,freq='Q')$ idx
jsonUrl = "s3a://DH-DEV-PROMETHEUS-BACKUP/prometheus-openshift-devops-monitor.1b7d.free-stg.openshiftapps.com/"+metric_name+"/"$ jsonFile = sqlContext.read.option("multiline", True).option("mode", "PERMISSIVE").json(jsonUrl)
for df in (joined, joined_test):$   df['Promo2Since']=pd.to_datetime(df.apply(lambda x: Week(x['Promo2SinceYear'],x['Promo2SinceWeek']).monday(),axis=1).astype(pd.datetime))$   df['Promo2Days']=df['Date'].subtract(df['Promo2Since']).dt.days
subreddit = reddit.subreddit('Bitcoin')$ for submission in reddit.subreddit('all').hot(limit=25):$     print(submission.title)
display(datos.head(10))
results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'local')$
mars_weather = soup.find_all('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text')[0].text$ print(mars_weather)
test_collection.insert_one(temp_dict)
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url)$
plt.style.use('default')$ %pprint
dates = pd.DatetimeIndex(pivoted.columns)$ dates[(labels == 0) & (dayofweek < 5)]
df_variables.loc[df_variables["CustID"].isin([customer])]
xml_in['publicationDate'] = pd.to_datetime(xml_in['publicationDate'], format='%Y-%m-%d', errors='coerce')
g8_groups.mean()
dev4['avg_rank'] = dev4[[c for c in dev4.columns if c.startswith('rank_')]].apply(np.mean, axis=1)$ dev4['rank'] = dev4.avg_rank.rank()
tweets_data_path = '../data/tweets_si.json'$ tweets_file = open(tweets_data_path, "r")$ tweets_data = json.load(tweets_file)
psy_df3 = QLESQ.merge(psy_df2, on='subjectkey', how='right') # I want to keep all Ss from psy_df
df_ll = pd.read_csv("loblaws_all.csv", encoding="latin-1")
ddf = pd.read_csv('new_ddf.csv')
df['full_text'] = df['full_text'].apply(lambda x: x.replace('\r', ' '))$ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\n', ' '))$ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\t', ' '))
crypto_combined = pd.concat([crypto_data, crypto_ggtrends], axis=1).dropna(how='any')   ### Remove NaN $ crypto_combined_s = crypto_combined.copy(deep=True)$ print(crypto_combined_s.head(5))
results = sm.OLS(gdp_cons_df.Delta_C1[:280], gdp_cons_df.Delta_Y1[:280]).fit()$ print(results.summary())
tmax_day_2018.tmax[100].plot();
new_model = gensim.models.Word2Vec(min_count=1)  $ new_model.build_vocab(sentences)                     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)
df = df.set_index('date')
x = topics_data.comms_num$ y = topics_data.score$ print("Correlation between Number of Comments and Total Score is:", round(np.corrcoef(x, y)[0,1], 2))$
temp_df=df_small.groupby('order_num').sum().reset_index()$ temp_df.head()
df_members = df_members[df_members.registered_via != -1]
events_df['utc_offset'].head(5)
df_newhouse.reset_index(level=0,inplace=True)$ df_newhouse.columns = ["Date","Average_Housing_permits"]$ df_newhouse.head(5)$
centers_df["latitude"] = pd.Series(lat)$ centers_df["longitude"] = pd.Series(lon)
msft = pd.read_csv('msft.csv', index_col=0)$ msft.head()
datetimes = pd.date_range(DATA_STARTTIME, DATA_ENDTIME, freq='min')$ datetimes[0:10]
btc.corr()
iris.head().loc[:,:"Sepal.Length"]$
import csv$ data.to_csv('Marvel.csv', encoding='utf-8', index=False)
import tm_assignment_util as util$ myutilObj = util.util()$ Osha_AccidentCases = util.accidentCases_Osha
evaluation_df = predictions_df.reset_index().sort_values(by=['unit', 'item', 'index'])$ evaluation_df['actual_value'] = test_df.sort_values(by=['unit', 'item', 'date'])['value'].values
df1 = df1.dropna()
data_2018.to_csv('/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/minimum temperature/extracted_data/tmin.2018.csv')
xml_in_merged.tail(2)
USvideos = pd.read_csv('data/USvideos_no_desc.csv', parse_dates=['trending_date', 'publish_time'])
graf['DETAILS']=graf['DETAILS'].str.replace('\n', ' ')
cityID = '1c69a67ad480e1b1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Houston.append(tweet) 
multi.handle.value_counts()/multi.shape[0]
trading_volume.sort()
drone_utc = drone2.set_index('drone_date_utc')
plot_BIC_AR_model(data=doc_duration.diff()[1:], max_order_plus_one=10)
year_prcp = session.query(Measurements.date, Measurements.prcp).order_by(Measurements.date).filter(Measurements.date > year_ago)
r_dict = r.json()$ print(r_dict['dataset_data']['column_names'])
ad_source.to_csv('../data/ad_source.csv')
df.info()$
daily_returns = calc_daily_returns(closes)$ daily_returns.plot(figsize=(8,6));
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt"$ mydata = pd.read_table(path, sep= '\t')$ mydata.head(5)
word_centroid_map = dict(zip(model.wv.index2word, idx))
retweet_and_count = sns.factorplot(data=tweets_df, x="name", y="retweet_count", kind="box")$ retweet_and_count.set(yscale="log")$ plt.xticks(rotation=60)     # alwatan have above average number of retweets and alseyassah have below average number of retweets$
year8 = driver.find_elements_by_class_name('yr-button')[7]$ year8.click()
!grep -A 20 "INPUT_COLUMNS =" taxifare/trainer/model.py
r_train, r_test, rl_train, rl_test = train_test_split(r_forest.ix[:,0:10], r_forest['bot'], test_size=0.2, random_state = 2)
results_df.to_csv("2018-04-09-Dow-NewsStats.csv", index=False)$
from statsmodels.tsa.stattools import acf, pacf
suburban_ride_total = suburban_type_df.groupby(["city"]).count()["ride_id"]$ suburban_ride_total.head()$
spacy_url = 'https://spacy.io/assets/img/pipeline.svg'$ iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url)$ HTML(iframe)
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df_train = pd.concat((df_train, pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/train_v2.csv',dtype={'is_churn' : np.int8} )), axis=0, ignore_index=True).reset_index(drop=True)
df_train = sales_by_storeitem(df_train)$ df_test['sales'] = np.zeros(df_test.shape[0])$ df_test = sales_by_storeitem(df_test)
jevent_scores = jcomplete_profile['event_scores']$ esdf = pd.DataFrame.from_dict(jevent_scores)$ print(esdf[['event_type_id','model_scope_forecast_horizon','effective_date', 'score_value']])
cityID = '52445186970bafb3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chandler.append(tweet) 
api_json = json.loads(api_call.text)$ for x in api_json['messages']:$     print(x['body'])$
all_sets.cards = all_sets.cards.apply(lambda x: pd.read_json(json.dumps(x), orient = "records"))
free_data.sort_values(by="country")$ free_data.sort_values(by="educ")$ free_data.sort_values(by="age", inplace=True, ascending = False)
y = df['comments']$ X = df[['subreddit', 'title']].copy(deep=True) 
foobar = np.dtype([('foo',int),('bar',float)])
results = session.query(Measurement.tobs).all()$ tobs_values = list(np.ravel(results))$ tobs_values
print(df.apply(np.cumsum))
data.plot(legend=True,figsize=(15,8))
lr2 = LinearRegression()$ lr2.fit(train_data, train_labels)
metadata['epsg'] = int(refl['Metadata']['Coordinate_System']['EPSG Code'].value)$ metadata['epsg']
top_score = df.rating_score.max()$ print('The highest rating is a {} out of 5.'.format(top_score))
df2 = pd.read_csv('comma_delim_clean.csv', index_col='id')$
import pickle$ with open('house_regressor.pkl', 'wb') as f:$     pickle.dump(automl, f)$
dates = pd.date_range('2018-05-01', '2018-05-06')$ temps1 = Series([80, 82, 85, 90, 83, 87], index = dates)$ temps1
df_merge.columns
pp = PostProcess(run_config='config/run_config_notebook.yml', $                  model='MESSAGE_GHD', scen='hospitals baseline', version=None)
LR_prob=logisticRegr.predict_proba(train_ind[features])$ roc=roc_auc_score(test_dep[response], best_model_lr.predict_proba(test_ind[features])[:,1])
from ssbio.pipeline.gempro import GEMPRO
volt_prof_before=pd.read_csv('../inputs/opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder))$ volt_prof_after=pd.read_csv('../outputs/from_opendss/to_opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder))
logreg = LogisticRegression()$ logreg.fit(X_train_all, y_train)$ logreg.score(X_test_all, y_test)
df.to_excel("../../data/msft2.xlsx")
s=r.json()$ type(s)
edu_gen_edad = pd.merge(educacion, genero_edad, on = 'idpostulante', how = 'inner')$ edu_gen_edad.head(1)
df = df.drop_duplicates()$ df.loc[df['Name'] == 'Mary']$ df = df.sample(frac=1).reset_index(drop=True)
metadata['bad_band_window1'] = refl.attrs['Band_Window_1_Nanometers']$ metadata['bad_band_window1']
d = x.T.flatten() # is this data preserved?$ print(d)$ d.base is x # numpy wants to be efficient, but the memory isn't contiguous
import numpy as np$ X_nonnum = X_copy.select_dtypes(exclude=np.number)
y_pred_mdl7 = mdl.predict(test_orders_prodfill)
req = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-7-04&end_date=2017-07-04&api_key=" + API_KEY)$ data = req.json()$ data
df=pd.read_csv("../Raw Data/Trump tweets 3 day groups jan-dec 2017.csv")$ df.head()
df.dropna(subset=['insert_id'], how='all')$ df = remove_duplicate_index(df)
random_integers = rng.randint(1, 10, size = 16).reshape(4, 4)
columns = inspector.get_columns('stations')$ for c in columns:$     print(c['name'], c["type"])$
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
company_df.to_csv('companies.csv')
studies_a = pd.DataFrame(studies, columns=['why_stopped','verification_date','target_duration','study_type','start_date_type','start_date','source','phase','overall_status','official_title','number_of_arms','nct_id','limitations_and_caveats','last_known_status','last_changed_date','is_unapproved_device','is_fda_regulated_drug','is_fda_regulated_device','enrollment_type','enrollment','completion_date','brief_title','baseline_population'])$ studies_a.head()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&start_date=2017-01-02&end_date=2017-01-02&api_key=nK_oDNyRo17zSF7LsAUb')$ print(r.json())$
shopping_carts = pd.DataFrame(items)$ shopping_carts
!wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/patches/2/cuda-repo-ubuntu1604-8-0-local-cublas-performance-update_8.0.61-1_amd64-deb
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
results_df = pd.DataFrame(dict_results).set_index("Username").round(2)$ results_df
df_teacher_info = df_teacher_info[df_teacher_info.awj_teacher_id.isin(pick_list)]$ df_teacher_info.drop(["country", "timezone", "degree_and_university", "child_exp"], axis=1, inplace=True)
tweet_group = tweet_df.groupby(['source']).mean().reset_index()$ tweet_group
inspector = inspect(engine)$ inspector.get_table_names()
meets_credit_policy = doesnt_meet_credit_policy.logical_negation()$ meets_credit_policy.head(rows=2)
average_vol = [i[5] for i in json_afx_data] $ print('Average trading volume is :', sum(average_vol) / len(average_vol)) #divide the sum by the length // count of entries.$
S_1dRichards.meta_basinvar.filename
for i in range(len(tweets['Tweet'])):$     tweets['Tweet'][i] = " ".join([word for word in tweets['Tweet'][i].split()$                                 if 'http' not in word and '@' not in word and '<' not in word and '#' not in word and '\\xf' not in word])
table_rows = driver.find_elements_by_tag_name("tbody")[8].find_elements_by_tag_name("tr")$
display(data[data['age'] == data['age'].min()])$ display(data[data['age'] == data['age'].max()])
step_counts.fillna(0., inplace=True)
pwd = os.getcwd()$ df_users = pd.read_csv( pwd+'/users.052317.csv', encoding='utf-8') #user, need to find a way to link them, since it is only individual record. $
with open('/Users/ianbury/Springboard/springboard/quandl_api_key','r') as file:$     API_KEY = file.read()$
from sklearn.externals import joblib$ joblib.dump(pca, '../models/pca_20kinput_6858comp.pkl')$ np.save('../models/crosstab_40937.pkl', crosstab_transformed)
df2.to_csv("newstweets.csv",index=False)
hawaii_measurement_df = hawaii_measurement_df.replace(np.nan, 0)
sumAll = df['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])$ sumAll
X_lowVar = df.copy()
import gp$ import genepattern$ genepattern.GPAuthWidget(genepattern.register_session("https://gp-beta-ami.genepattern.org/gp", "", ""))
uniqueArtists = newUserArtistDF.select("artistID").distinct().count()$ print("Total n. of artists: ", uniqueArtists)$
session.query(Measurement.id, func.max(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
data_AFX_X['Difference'] = data_AFX_X['High'] - data_AFX_X['Low']$ data_AFX_X.describe()
cityID = 'dc62519fda13b4ec'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tampa.append(tweet) 
max_fwing = df.loc[df.following.idxmax()] $ name = max_fwing['name'] if max_fwing['name'] is not None else max_fwing['login']$
df_features_checkcorr = df_features$ df_features_checkcorr.to_csv("data new/features.csv", encoding="utf-8", sep=",", index=False)
'plumber' in model.wv.vocab
intervention_train.isnull().sum()
classifiers = visual_recognition.list_classifiers()$ print(classifiers)
ts.head()
LARGE_GRID.plot_accuracy(raw_large_grid_df, option='dodge')
%%time$ df = table.to_pandas()
tweets_df = tweets_df[tweets_df['location'] != ''].reset_index(drop=True) # reset index from 0$ tweets_df = tweets_df.sort_values('timestamp')$ print('got locations for {} retweets'.format(len(tweets_df)))
ax = P.plot_1d('pptrate')$ ax.figure.savefig('/media/sf_pysumma/pptrate.png')$
from nltk.corpus import stopwords$ portuguese_stop_words = stopwords.words('portuguese')
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
with open(os.path.expanduser('~/.secrets/twitter_thebestcolor.yaml')) as f:$     creds =  yaml.load(f)
data_FCInspevnt_latest = data_FCInspevnt_latest.drop_duplicates(subset = 'brkey', keep='last')
p_diff = new_page_converted.mean() - old_page_converted.mean()$ print("{} is simulated value of pnew - pold".format(p_diff))
columns = ['day_period', 'weekday', 'category', 'is_self', 'is_video']$ le = LabelEncoder()$ model_df[columns] = model_df[columns].apply(lambda x: le.fit_transform(x))
transform = TfidfVectorizer(lowercase=False, min_df=.01)$ tf_idf_matrix = transform.fit_transform(back2sent.values)$ tf_idf_matrix.shape
autoDf.createOrReplaceTempView("autos")$ SpSession.sql("select * from autos where hp > 200").show()
events_df['event_day'] = events_df['event_time'].apply(lambda d: d.replace(hour=0,minute=0,second=0))$ events_df['event_week'] = events_df['event_day'].apply(lambda d: d - datetime.timedelta(d.weekday()))$ events_df['event_weekday'] = events_df['event_day'].apply(lambda d: d.weekday())
sns.distplot(utility_patents_subset_df.prosecution_period, color="orange")$ plt.show()
mapInfo_split = mapInfo_string.split(",") $ print(mapInfo_split)
import statsmodels.api as sm
df.var()
tst_lat_lon_df = pd.read_csv("testset_unique_lat_and_lon_vals.csv", index_col=0)
img_url_rel = img_soup.find('figure', class_='lede').find('img')['src']$ img_url_rel
df.head(100)
compound_df = compound_df.reset_index()
most_active_stations = session.query(Measurement.station, func.count(Measurement.station)).\$         group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()0$ print (most_active_stations)
import name_entity_recognition as ner$ ner.load_data()
grouped_authors_by_publication.rename(columns = {'authorName':'authorNames_in_given_publication'}, inplace = True)$ grouped_authors_by_publication.rename(columns = {'authorId':'authorIds_in_given_publication'}, inplace = True)
tlen = pd.Series(data['len'].values, index=data['Date'])$ tfav = pd.Series(data['Likes'].values, index=data['Date'])$ tret = pd.Series(data['RTs'].values, index=data['Date'])
(pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8") 
df = engine.get_data(**opts)
xml_in_merged = pd.merge(xml_in, grouped_df, on=['authorId', 'authorName'], how='left')
list(set(df.CustomerID[pd.isnull(df.State)]))$ df[pd.isnull(df.State)].head()
def day_of_week(date):$     days_of_week = {0: 'monday', 1: 'tuesday', 2: 'wednesday', 3: 'thursday', 4: 'friday', 5: 'saturday', 6: 'sunday'}$     return days_of_week[date.weekday()]
print('Total number of different vote types recorded: {}'.format(len(v['vote'].value_counts(dropna=False))))$ v['vote'].value_counts(dropna=False).nlargest(10)
studies_c = pd.merge(studies_b,countries[['nct_id','name']],on='nct_id', suffixes=('_sponsor', '_country'),how='left')$ studies_c.head()
tweet = tweets[1]$ pp.pprint([att for att in dir(tweet) if '__' not in att])
%matplotlib inline$ df_concat_2.boxplot(column="message_likes_rel",by="page")$
store_items.fillna(method='backfill', axis=0)$
outlet_mean_score=sentiments_df.groupby("Media_Source")["Compound"].mean()$ outlet_mean_score = round (outlet_mean_score,2) $ outlet_mean_score #check
tweet_df["tweet_source"].unique()
contractor[contractor.contractor_id.duplicated() == True]
df_hdf = dd.read_hdf(target, '/data')$ df_hdf.head()
print('The Jupyter notebook stores information in the "Kernel".\$       \nRestart the Kernel to clear noteook memory.')
free_data[free_data['educ']>5].groupby('age_cat').describe()
target_city = {"lat": 35.227724, "lng": -80.839699} $ target_coords = f"{target_city['lat']},{target_city['lng']}"$
wb = openpyxl.load_workbook('most_excellent.xlsx')$ wb.sheetnames
volume_weather=general_volume.merge(df, left_on='ds' , right_on='Date')$ volume_weather.head()$ volume_weather=volume_weather[~volume_weather.T_avg.isnull()]
df_amznnews_clsfd_2tick = df_amznnews_clsfd_2tick.set_index('publish_time')$ df_amznnews_clsfd_2tick.info()
itemTable["Energy"] = itemTable["Content"].map(energy_level)
tweets['hour']= dts.dt.hour$ tweets['date']= dts.dt.date$ tweets
plt.savefig(str(output_folder)+'NB01_3_landscape_image02_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice02_str) 
list(set(df['City'][df.CustomerID=='0000118196']))   
my_gempro.prep_itasser_modeling('~/software/I-TASSER4.4', '~/software/ITLIB/', runtype='local', all_genes=False)
stations_df.count()
kushy_prod_df.describe(include="all")$
df_mfacts = mfacts[0]$ df_mfacts.columns = ['Name', 'Values']$ df_mfacts
stringlike_instance.content = 'changed content'
data_set.to_csv("C://SHERYL MATHIAS/US Aug 2016/Fall 2017/INFM750/HillaryClintonTweets.csv", index=False, encoding='utf-8')
df_pop['pop_t-1'] = df_pop['population'].shift(1) $ df_pop['pop_change'] = df_pop['population']- df_pop['pop_t-1']$
store_items.fillna(method = 'ffill', axis = 1)
Mars_Weather_URL = 'https://twitter.com/MarsWxReport/status/1017925917065302016'$ Weather_response = requests.get(Mars_Weather_URL)$ Weather_soup = BeautifulSoup(Weather_response.text, 'html.parser')
coming_next_reason = questions['coming_next_reason'].str.get_dummies(sep="'")
items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']),$          'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])}$ print(type(items))
data_current = current.loc[(df['Date'] == '2018-05') | (df['Date'] == '2018-06')] $
for row in mw.itertuples():$     rtp = RichTextPage(content=row[4])$     rtp.save()$
df2[df2['group']=='control']['converted'].mean()$
df.index
[scores.apply(sum), scores.apply(np.mean)]
all_sets.cards["XLN"].shape
P = Plotting(output_path)$ Plot = P.open_netcdf()
index = pd.bdate_range('2018-3-1', periods=1000)$ index
ratings.sample(5)
df = pd.read_excel("F:/web mining/webmining project/crawler/annotation_data/samsung/Merged_final.xlsx")$
tl_2040 = pd.read_csv('input/data/trans_2040_ls.csv', encoding='utf8', index_col=0)
pd.concat([msftA[:5], aaplA[:3]], axis=1, join='inner', keys=['MSFT', 'AAPL'])
first_row = session.query(Measurement).first()$ first_row.__dict__
airlines_day_unstacked = airlines_day_unstacked[(airlines_day_unstacked != 0).all(1)]
r.text
n_new = df2[df2.group == "treatment"].count()[0]$ print("The population of user under treatment group: %d" %n_new)
logs['key'].value_counts().plot()$ plt.show()
population.apply(lambda val: val > 1000000)
print('Total minutes over time series: {}'.format(df_btc.shape[0]))$ print('% minutes in time series with {} trades: {}'.format('USD-BTC', df_btc['USD-BTC_low'].notna().sum()/df_btc.shape[0] ))$
Precipitation_DF.head(10)
S.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
Jarvis_ET_Combine = pd.concat([Jarvis_rootDistExp_1, Jarvis_rootDistExp_0_5, Jarvis_rootDistExp_0_25], axis=1)$ Jarvis_ET_Combine.columns = ['Jarvis(Root Exp = 1.0)', 'Jarvis(Root Exp = 0.5)', 'Jarvis(Root Exp = 0.25)']
export_path=cwd+'\\ca_simu_from_python.csv'$ ca_de.to_csv(export_path, index=False)
s_pacific = s_eastern.tz_convert("US/Pacific")$ s_pacific
headers= ({'User-agent': 'kiros Bot 0.1'})
with connection:$     cursor.executemany('INSERT INTO fib VALUES (?)',$                        [(str(x),) for x in fib(10)])$
query ="SELECT * FROM tdata_db_2.Weather_Log ORDER BY Log_Id DESC"$ df = pd.read_sql(query,session)$ df.head(10)
df.time.unique().shape
station_count = session.query(Stations.station).group_by(Stations.station).count()
centroids2 = clusters_kmeans2.cluster_centers_
y_ = df2["user_id"].count()$
df2017=df1.iloc[0:165,]  #line 165 is 2016 so we need 1 more than line 164 ie 165$ df2017.tail()
type(model.wv.syn0)$ len(model.wv.vocab)$ model.wv.syn0.shape
News_title = Mars_soup.find('div',class_="content_title").a.text$  $ print(News_title)
df_RSV = df_full[df_full.Field4 == "RSV"]
df.dtypes
df.reset_index().to_csv('final.csv',header=True, index=False)
active_station_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).filter(Measurement.date > last_year).filter(Measurement.station == most_active_station).all()$
cityID = '7f061ded71fdc974'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Montgomery.append(tweet) 
W.WISKI_CODES
y_pred = model.predict(X_test)$ y_test_rescaled = scaler.inverse_transform(y_test)$ y_pred_rescaled = scaler.inverse_transform(y_pred)
y_pred = logreg.predict(X_test)$ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))
stock_return = stocks.apply(lambda x: x / x[0])$ stock_return.head()
plt.title("Pyber Rides Sharing (2016)")$ plt.xlabel("Total Number of Rides Per City")$ plt.ylabel("Average Fare ($)")
df.head()
table_rows = driver.find_elements_by_tag_name("tbody")[18].find_elements_by_tag_name("tr")$
resumePath = "resumes/jacky.txt"
figure_density_df = utility_patents_subset_df.dropna()$ sns.distplot(figure_density_df.figure_density, color="red")$ plt.show()
speeches_metadata = speeches_cleaned.merge(metadata_df_all, left_on = 'id', right_on = 'accessids', how = 'inner')
contractor_clean['zip_part1'] = contractor_clean.zipcode.str.split('-').str[0]$ contractor_clean['zip_part2'] = contractor_clean.zipcode.str.split('-').str[1]
s = fixed.iloc[200000:205000,]$ plt.plot(s.unstack(level=0), alpha=0.2);
ranking = pd.read_csv('datasets/fifa_rankings.csv') # Obtained from https://us.soccerway.com/teams/rankings/fifa/?ICID=TN_03_05_01$ fixtures = pd.read_csv('datasets/fixtures.csv') # Obtained from https://fixturedownload.com/results/fifa-world-cup-2018$ pred_set = []$
pd.options.display.max_rows$ pd.set_option('display.max_colwidth', -1)$ type(df.iloc[15]['in_reply_to_user_id_str'])
cityID = 'dd3b100831dd1763'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_Orleans.append(tweet) 
accumulated_num, frequency = split_by_time(dates, periode= 60 * 60 * 20)$ print 'There are totally '+str(len(frequency)) + ' periods.'$
tweet_data_df.to_csv("tweet_data_df.csv",sep =",")
userProfile = userGenreTable.transpose().dot(inputMovies['rating'])$ userProfile
df2['user_id'][df2.duplicated(['user_id'], keep=False)]
baby_scn_created['BABY_CREATED'] = pd.to_datetime(baby_scn_created['BABY_CREATED'])$ baby_scn_created['SCN_CREATED'] = pd.to_datetime(baby_scn_created['SCN_CREATED'])$
G = nx.Graph() #creates empty graph, initiliasize a graph object$ G.add_nodes_from(node_names)$ G.add_edges_from(edges)
tzs = DataSet['userTimezone'].value_counts()[:10]$ print(tzs)
lr = LogisticRegressionCV()$ lr.fit(train_Features, train_species)
tl_2050 = pd.read_csv('input/data/trans_2050_ls.csv', encoding='utf8', index_col=0)
active_psc_records.month_year_birth.hist(figsize=(20,5),bins=50)
y.name = "huhu"$ y = y.rename("jsdfjkdsfhsdfhdsfs")$ y
train_df = effectiveness(train_df)$ print(train_df.head(5))
plidata_blocks = pd.merge(plidata, parcel_blocks, how='left', left_on=['PARCEL'], right_on=['PIN'])$ plidata_blocks = plidata_blocks.drop(['PARCEL','PIN'], axis=1)$ plidata_blocks=plidata_blocks.dropna(subset=['TRACTCE10','BLOCKCE10'])
news_p = news_items[0].find(class_='rollover_description_inner').text$ print (news_p)
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=featured#submit'$ browser.visit(url)$ time.sleep(3)  #allow time for page to load
rfc = RandomForestClassifier(n_estimators=1000, max_depth=100, max_features=2, n_jobs=-1)$ scores = cross_val_score(rfc, X, np.ravel(y,order='C'), cv=5)$ print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
tweetdf['lga'].unique()
df_con_control = df_con1.query('group =="control"')$ x_control = df_con_control["user_id"].count()$ x_control$
TEXT.vocab.itos[:12]
x = np.array([0,1,2,3,4,5,6,min])$ x.dtype # it's a C array of python objects!$
precip_df.rename(columns={'prcp': 'precipitation'}, inplace=True)$ precip_df.set_index('date', inplace=True)$ precip_df.head()
element = driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a')$ element.click()
BuyingData = pd.read_excel('/Users/itsjustme/Desktop/BuyingBehaviourTwoYrs.xlsx')
reddit.Comments.value_counts(ascending=False).head(25) #just seeing the distribution of the number of comments$
df_user = pd.read_csv(f_user)$ df_user.head(3)
url = 'https://twitter.com/marswxreport?lang=en'$ browser.visit(url)
r6s['score'].corr(r6s['num_comments'])
daily_unit_df = all_turnstiles.groupby(['STATION','C/A','UNIT','DATE'], as_index=False).sum().drop(['ENTRIES','EXITS'], axis=1)$ daily_unit_df.sample(5)
normalizedDf = finalDf$ normalizedDf = normalizedDf.drop('kmLabels', axis=1);
data[data['authorName'] == 'Lunulls A. Lima Silva']#['link_weight']#.loc[3]
df_subset.boxplot(column='Initial Cost', by='Borough', rot=90)$ plt.show()
search_term = input('Enter text to search for: ').upper()$ df_vendor_single = df_vendors[df_vendors['Vendor'].str.contains(search_term)]$ df_vendor_single
elms_all = elms_sl.append(elms_pl, ignore_index=True)$ elms_all = elms_all.append(elms_tl, ignore_index=True)$ elms_all['SSN'] = [float(x) for x in elms_all.SSN.values]
import sklearn.feature_extraction.text$ vctr = sklearn.feature_extraction.text.CountVectorizer(tokenizer = lambda key_phrases: key_phrases, preprocessor=lambda key_phrases: key_phrases)$
expiry = datetime.date(2015, 1, 5)$ msft_calls = Options('MSFT','yahoo').get_call_data(expiry=expiry)$ msft_calls.iloc[0:5,0:5]
store_items = store_items.rename(index = {'store 3': 'last store'})$ store_items
!jupyter nbconvert index.ipynb --to html$ bucket.upload_file('index.html', 'index.html')$ bucket.upload_file('index.ipynb', 'index.ipynb')
materials_file = openmc.Materials([fuel, water, zircaloy])$ materials_file.export_to_xml()
cgm = cgm.rename(columns={"value": "mmol_L"})$ cgm["mg_dL"] = (cgm["mmol_L"] * 18.01559).astype(int)$ cgm.mg_dL.head()
!hdfs dfs -mkdir hw3$ os.getcwd() 
trump['hours'] = trump.index.hour$ trump['weekday'] = trump.index.weekday_name
TripData_merged = pd.concat([TripData1, TripData2, TripData3])
median_trading_volume = statistics.median([day[6] for day in data])$ print ('Median trading volume for 2017:', median_trading_volume)
auth = tweepy.OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)$ api = tweepy.API(auth)
slicer.apply(np.mean, axis=1)
fps.pyldavis_fp
measure_df = pd.read_csv(measure, encoding="iso-8859-1", low_memory=False)$ measure_df.head()
index = pd.date_range('2018-3-1', periods=1000, freq='M')$ index 
tweets_by_user = pd.read_sql_query(query, conn, parse_dates=['created_at'])$ tweets_by_user.head()
html = requests.get(nasa_url)$ soup = bs(html.text, 'html.parser')
new_df = df.fillna(method = 'bfill')$ new_df
df=df.set_index('Date')$ bwd = df[['Store']+columns].sort_index().groupby("Store")[columns].rolling(7, min_periods=1).sum()$ fwd = df[['Store']+columns].sort_index(ascending=False).groupby('Store')[columns].rolling(7,min_periods=1).sum()
y_pred_rf = rf.predict(X_test)$ y_train_pred_rf=rf.predict(X_train)$ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(rf.score(X_test, y_test)))
dupes_to_delete = dfd.duplicated(subset=['brand', 'outdoor_model', 'indoor_model'])$ dupes_to_delete.value_counts()
df_TempIrregular.to_csv('data/All_Irregularities_20180601_to20180607.csv', sep=',')
import pandas as pd$ tweets = pd.read_csv ("./twitter.csv", header=None)$ tweets.head()
segments = pd.read_csv("transit_segments.csv", parse_dates=['st_time', 'end_time'])$
obamaSpeechRequest = requests.get('https://en.wikisource.org/wiki/Barack_Obama%27s_Eighth_State_of_the_Union_Address')$ print(obamaSpeechRequest.text[40000:41000])
weather['precip_total'] = weather['precip_total'].replace('NaN', None, regex=False).fillna(0)$ weather['pressure_avg'] = weather['pressure_avg'].replace('NaN', None, regex=False).fillna(0)$ weather['wind_speed_peak'] = weather['wind_speed_peak'].replace('NaN', None, regex=False).fillna(0)
display(data.head(10))
tail = df.copy()$
extract_deduped_with_elms = extract_deduped_with_elms_v2.copy()
for col in var_cat:$     taxi_sample[col] = taxi_sample[col].astype(np.int64)
import pickle$ filename = 'finalized_automl.sav'$ pickle.dump(automl, open(filename, 'wb'))
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
print(f"Number of stations is {sq.station_count()}")
print(dfd.in_pwr_5F_max.describe())$ dfd.in_pwr_5F_max.hist()
df = pd.merge(df,rsvp_df,how="left",on="urlname")$ df.head()
priors_product_reordered_spec= priors_reordered.groupby(["user_id","product_id"]).size().reset_index(name ='reordered_count_spec')$ priors_product_reordered_spec['userprod_id']=priors_product_reordered_spec['product_id'] + priors_product_reordered_spec['user_id'] *100000$ priors_product_reordered_spec.head(10)
median_comments = reddit['num_comments'].median()$ median_comments
airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')$ print(airquality_pivot.head())
df2.sort_values('total_likes',inplace=True,ascending=False)$ top_likes = df2.head(10)['id']
festivals.index
oz_stops['stopid'] = oz_stops['stopid'].apply(lambda x: str(x))
requests.get(wikipedia_content_analysis)
train_x = encodedlist$ print np.asarray(train_x).shape
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY)
data.tasker_id.value_counts().head()
data.iloc[:, 2].head()
directory_name = os.path.join('network', 'recurrent_network')
twitter.name.value_counts()$
contractor_clean.loc[contractor_clean['contractor_id'].isin([382,383,384,385,386,387]),$                      'contractor_bus_name'] ='Cahaba Government Benefit Administrators, LLC'
df_2003['bank_name'] = df_2003.bank_name.str.split(",").str[0]$
df.LinkedAccountId.fillna(value=1,inplace=True)
df['AppointmentDurationHours'] = df['AppointmentDuration'] / 60.0
df.to_excel("../../data/stocks2.xlsx")
!hdfs dfs -cat {HDFS_DIR}/hw3.1-output/* > complaintCounts.tsv$ !cat complaintCounts.tsv
ctd_df.head(10)
wa_sales_decompose = sm.tsa.seasonal_decompose(wa_sales, freq=12, model='additive')$ decmpose_plot = wa_sales_decompose.plot()
sales_df[sales_df['Product_Id'] == 5].groupby(['Country']).sum()['Quantity'].sort_values(ascending=False)$
for idx,row in df[:3].iterrows():$     print(row['URL'])$     $
yearmonthcsv=yearmonth_sumsizecsv.select(yearmonth_sumsizecsv['yearmonth'],(yearmonth_sumsizecsv['sum(size_in_KB)']/1000).alias("total_size_in_MB"))
s.index
trump.describe()
pred_labels = lr2.predict(test_data)$ print("Training set score: {:.2f}".format(lr2.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(lr2.score(test_data, test_labels)))
data.index[0] = 15
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)
print(y_test.mean(), y_train.mean())
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'$ 
df_train.loc[:, 'is_attributed'] = df_train.loc[:, 'is_attributed'].astype(np.int8)$ df_valid.loc[:, 'is_attributed'] = df_valid.loc[:, 'is_attributed'].astype(np.int8)$ df_test.loc[:, 'is_attributed'] = df_test.loc[:, 'is_attributed'].fillna(0).astype(np.int8)
popCon.sort_values(by='counts', ascending=False).head(9).plot(kind='bar')
plt.savefig(str(output_folder)+'NB01_5_NDVI02_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice02_str)
print(df,'\n')$ print(df.apply(lambda x: x.max()-x.min()))
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['Retweets'].values, index=data['Date'])
!hdfs dfs -cat 32ordered_results-output/part-0000* > 32ordered_results-output.txt$ !tail 32ordered_results-output.txt
df = pd.read_csv('data_stocks.csv')
sample.head(1)
np.unique(bdata.index.time)
outliers_timeDict = {key: df[abs(df['timePassed'] - np.mean(df['timePassed'])) > 3 * np.std(df['timePassed'])] for key, df in typesDict.items()}$ outliers_timeDict.keys()
df.groupby('category')['position','hourly_rate','num_completed_tasks'].agg({'median','mean','min','max'}).reset_index() \$     .rename(columns={'category': 'category', 'position': 'position_stats_overall','hourly_rate':'hourly_rate_stats_overall', \$                  'num_completed_tasks':'num_completed_tasks_stats_overall'})
dfClientes.loc[dfClientes["MES"] == 201701].head()
aaplA = aapl[['Adj Close']] $ pd.concat([msftAV, aaplA])
spp['DK'] = spp[spp.columns[spp.columns.str.contains('DK')==True]].sum(axis=1)$ spp['DE'] = spp[spp.columns[spp.columns.str.contains('DE')==True]].sum(axis=1)
df.drop(["city","lat","lon"], inplace = True, axis = 1)
df1 = tier1_df.reset_index()$ df1 = df1.rename(columns={'Date':'ds', 'Incidents':'y'})
ch_year = pd.DataFrame(ch['startDate'].dt.year.value_counts(sort=False))$ ch_year['SERI'] = pd.Series([10,606,560,689,654,683,745,312,10], index=[2007,2008,2009,2010,2011,2012,2013,2014,2015])$ ch_year['Delta'] = ch_year['startDate']-ch_year['SERI']
df.plot();
df.info()
password_str = ""$ with open("dbpass.txt", mode="r") as pass_f:$     password_str = pass_f.readline()
kmeans_model = KMeans(n_clusters=2, init='k-means++', random_state=42).fit(crosstab_transformed)$ c_pred = kmeans_model.predict(crosstab_transformed)
df.sum()
birth_dates.head()
chinese_vessels_wcpfc = pd.read_csv('chinese_vessels_wcpfc.csv')
tips.index
UserInsight = pd.DataFrame(columns=['_id','owner','source','category','type','message','data','upvote','downvote'])$ UserInsight
FREEVIEW.plot_heatmap(raw_freeview_df,raw_fix_count_df)
df_pix4D = pd.read_json(basedirectory+projectname+'/'+Pix4D_filename, typ = 'series')$ images_json = df_pix4D['actual']['photos']
dist = np.sum(bag, axis=0)$ dist
most_recent = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ most_recent_list = most_recent[0].split("-")#split on "-"$ most_recent_list#check
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$
predictions = model.transform(data)$ predictions.toPandas()$
session.query(Measurement.date).order_by(Measurement.date.desc()).first()$
X_copy['vndr_id'] = X_copy['vndr_id'].apply(lambda x: hash(x))
searched_tweets = [status for status in tweepy.Cursor(api.search, wait_on_rate_limit=True,                                        wait_on_rate_limit_notify=True, $ since='2018-08-05', until='2018-08-06', q=query).items(max_tweets)]$
S_1dRichards.decision_obj.thCondSoil.options, S_1dRichards.decision_obj.thCondSoil.value
results = sm.OLS(gdp_cons_df.Delta_C1[141:280], gdp_cons_df.Delta_Y1[141:280]).fit()$ print(results.summary())
fp7_proj.shape, fp7_part.shape, fp7.shape
ts_filter = ts_mean[ticker][ts_mean[euphoria].shift(1)<0.05]
from sklearn.metrics import f1_score$ f1_score(y_test, y_pred_lgr, average='macro')  
temp['c'] = temp['contents'].str.split()
block_geoids_2010 = json.load(open('block_geoids_2010.json'))$ print 'There are', len(block_geoids_2010), 'blocks'$ assert(len(block_geoids_2010) + 1 == len(block_populations))
%time nb.fit(train_4, y_train)
tweets_df_lang = tweets_df.groupby('language')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ tweets_df_lang$
len(cvec.get_feature_names())$
store_items.fillna(method='ffill', axis=0)
tweet_archive_clean.drop(['new'], axis= 1, inplace= True)
iris_mat = iris.as_matrix()$ print(iris_mat[0:9,:])
stoplist = stopwords.words('english')$ stoplist.append('free')$ print(stoplist)
station_df = pd.read_sql("SELECT * FROM station", conn)
engine = create_engine("sqlite:///hawaii.sqlite")$
turnstiles_df['DateTime'] = turnstiles_df['DATE'] + turnstiles_df['TIME']$ turnstiles_df['DateTime'] = pd.to_datetime(turnstiles_df['DateTime'], format='%m/%d/%Y%H:%M:%S')
coming_next_reason.columns = ['NEXT_'+str(col) for col in coming_next_reason.columns]
auth = tweepy.OAuthHandler(ckey, csecret)$ auth.set_access_token(atoken, asecret)
sql_query = "SELECT * FROM events WHERE actor_id= %s"$ df = pd.read_sql_query(sql_query, session.bind, params=[userid])$ df
print('Before deleting out of bounds game rows:',injury_df.shape)$ injury_df = injury_df[(injury_df['Date'] > '2000-03-28') & (injury_df['Date'] < '2016-10-03')]$ print('After deleting out of bounds game rows:',injury_df.shape)
year13 = driver.find_elements_by_class_name('yr-button')[12]$ year13.click()
workspace_ids = []$ for i in workspaces_list:$     workspace_ids.append(i['id'])$
datasets_ref = pd.read_csv('../list_of_all_datasets_dgfr/datasets-2017-12-13-18-23.csv', sep=';')$ datasets_slug_id = datasets_ref.set_index('slug')['id'].to_dict()$ datasets_id_slug = datasets_ref.set_index('id')['slug'].to_dict()
texts = df[df['section_text'].str.contains('fees')]['section_text'].values[0:5]
dfall.info()
rfc_features = sorted(list(zip(test_features, rfc.feature_importances_)), key=lambda x: x[1], reverse=True)$ rfc_features
print(svm_clf.support_vectors_.shape)$ print(svm_clf.support_.shape)$ print(svm_clf.n_support_ )$
Magic.__repr__
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key='$ url_api=url+API_KEY$ r = requests.get(url_api)
from sklearn.naive_bayes import MultinomialNB$ clf = MultinomialNB().fit(X_train_tfidf, tweet_train_target)
df.sort_values(['Likes', 'len'], ascending=False).head()
df = df[df["status"] == "active"]$ df.drop(["status"], axis = 1, inplace = True)$ df.shape
prcp_analysis_df.set_index(['Date'], inplace=True)$ prcp_analysis_df.head()
ab_df.shape[0]
model.most_similar('hand',  topn=100)$
df_cust_data = pd.read_excel('mazda_dataset.xlsx','Customer data')$ df_prospects_data = pd.read_excel('mazda_dataset.xlsx','Prospects data')
new_df = df.dropna(subset=['driver_id','pickup_lat','pickup_lon','dropoff_lat','dropoff_lon'],inplace=False)
pax_raw['minute_of_day'] = pax_raw.paxhour*60 + pax_raw.paxminut$ pax_raw.head()
data.name.isnull().sum()
analysis.iloc[686].Description$ analysis.iloc[686].project_url$
visits = sql_query('select * from visits')$ visits.head(3)
hours.dropna(subset=['Specialty'], how='all', inplace=True)
cityID = '44d207663001f00b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Mesa.append(tweet) 
conf_matrix = confusion_matrix(training_test_labels, lr.predict(preproc_training_test), labels=[1,2,3,4,5])$ conf_matrix
from sklearn.feature_selection import VarianceThreshold$ sel = VarianceThreshold(threshold=(0.8 * (1 - 0.8)))
html_table = df.to_html()$ html_table
lookforward_window = 1$ for iter_x in np.arange(lookforward_window)+1:$     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata  = pd.read_csv(path, sep ="\s+")$ mydata.head(5)$
S_lumpedTopmodel.decision_obj.groundwatr.options, S_lumpedTopmodel.decision_obj.groundwatr.value
data_all = data.learner_id.value_counts()$ print(len(data_all[data_all > 100]))
xml_in[xml_in['publicationDate'].isnull()].count()
soup.find_all('div', class_='schedule-container')[0].select('.active')
print(data.head(10))
measurement_df.describe()
train_ind, test_ind, train_dep, test_dep = train_test_split(kick_projects_ip_scaled_ftrs, kick_projects_ip[response], test_size=0.3, random_state=0)
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings_v1.csv', sep=',', encoding='utf-8', header=True)
results=session.query(Measurements.Station,Measurements.Date,Measurements.Tobs).\$         filter(Measurements.Date>=start_date, Measurements.Date<=end_date, Measurements.Station == station_mostobs).\$         order_by(Measurements.Date.desc()).all()
f = open('..\\Output\\GenreString.csv','w')$ f.write(GenresString) #Give your csv text here.$ f.close()
appended = ad_data1.union(ad_data2)
store_items = store_items.drop(['watches', 'shoes'], axis = 1)$ store_items
engine = create_engine("sqlite:///hawaii.sqlite")$
lr2 = LogisticRegression(solver='saga', multi_class='multinomial', random_state=20, max_iter=1000)$ lr2.fit(X, y)
fsrq = np.where( np.logical_or(table['CLASS1']=='fsrq ',table['CLASS1']=='FSQR '))
np.ones(5)
data.head()
df_2018.dropna(subset=['Specialty'], how='all', inplace=True)
DummyDataframe2 = DummyDataframe2.apply(lambda x: calPercentage(x, "Token_Count", ["Positiv", "Negativ"]), axis=1)$ DummyDataframe2
url_yr2017='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=MY_API_KEY&start_date=2017-01-01&end_date=2017-12-31'$ response=requests.get(url_yr2017)
myTimeZone = pytz.timezone('US/Eastern')$ itemTable["Date"] = itemTable["Date"].apply(localize_time, args=(myTimeZone,))
trump = pd.read_csv('data/trump_tweets.csv', header=0, index_col=0)$ trump.head()
dfx = df.copy()$ dfx[dfx < 0] = -dfx  # abs$ print(dfx)
new = df.sort_values("dates")$ new.reset_index()$
import quandl$ quandl.ApiConfig.api_key = 'RGYoyz3FAs5xbhtGVAcc'
df_vow.plot()
data.loc[pd.Timestamp('2012-01-01 09:00'):pd.Timestamp('2012-01-01 19:00')]$
def dist(a, b, ax=1):$     return np.linalg.norm(a - b, axis=ax)
scores_by_org = news_df.groupby('Screen Name')['Compound Score'].mean()$ scores_by_org.head()
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"$ date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"
df.resample('D').mean()
email_sku_count = pd.DataFrame(transaction_dates.groupby(['Email', 'Lineitem sku'])['Lineitem quantity'].sum()).reset_index()$ email_sku_count
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read())
bbc_df = constructDF("@BBC")$ display(constructDF("@BBC").head())
BTC['Smoother'] = BTC.apply(lambda row: smoother_function(row["Transfer_method"], row["Type"], row["BTC_amount"]), axis=1)
options_frame[abs(options_frame['ModelError']) >= 1.0e-4].plot(kind='scatter', x='BidAskSpread', y='ModelError')
dfMeanFlow.head()
df_emoji_rows.extr_emojis.value_counts().head(20)
df[df.Target == 5]
print('First tweet recorded  @ {}'.format(tweets['time_eastern'].min()))$ print('Last tweet recorded @ {}'.format(tweets['time_eastern'].max()))
scraped_batch6_top.to_csv('batch6_top.csv', encoding='utf-8')$ scraped_batch6_sec.to_csv('batch6_sec.csv', encoding='utf-8')
endometrium_data = pd.read_csv('data/small_Endometrium_Uterus.csv', sep=",")  # load data$ endometrium_data.head(n=5)  # adjust n to view more data
summary_all = df['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9])$ summary_all
trump_twitter = pd.read_json('/home/data_scientist/data/misc/trump_tweets_2017.json', encoding='utf8')$ trump_twitter.head()
import pandas as pd$ git_log = pd.read_csv('datasets/git_log.gz', sep='#', header=None, encoding='latin-1', names=['timestamp','author'])$ print(git_log.head(5))
df['profarea'].value_counts()
table_rows = driver.find_elements_by_tag_name("tbody")[26].find_elements_by_tag_name("tr")$
number_of_commits = len(git_log)$ number_of_authors = git_log.loc[:, 'author'].dropna().nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
frames = [data_BBC, data_CBS, data_CNN, data_FoxNews, data_nytimes]$ big_data = pd.concat(frames)$ big_data.to_csv('SentimentAnalysisData.csv')
import pandas as pd$ import matplotlib.pyplot as plt$ import seaborn as sns
scores_mode = scores.sort_values(ascending=False).index[0]$ print('The mode is {}.'.format(scores_mode))
LSST_sample_filename = 'LSST_ra_250_283_dec_-40_-15.dat'$ LSST_data = np.genfromtxt(DirSaveOutput+LSST_sample_filename, usecols=[5])
store_items.fillna(method = 'ffill', axis = 0)
Magic.__dict__['__repr__'].__get__(None, Magic)$
model.init_sims(replace=True)
soups = [soup(requests.get(url).text, 'html.parser') for url in article_urls]
tlen.plot(figsize=(16,4), color='b');
cityID = '53b67b1d1cc81a51'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Birmingham.append(tweet) 
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
df = pd.read_csv("https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv")$ df[:5]
user_df['bus_id'].values
y_axis = np.arange(math.floor( rides_fare_average_min ) - 5, math.floor(rides_fare_average_max) + 6, 5)$ y_axis
idx = pd.IntervalIndex.from_arrays(df2.Start, df2.End, closed='both')$ idx
y = x.astype(float)$ y.base is x
metadata['reflectance_scale_factor'] = float(refldata.attrs['Scale_Factor'])$ metadata
model = AuthorTopicModel.load('/tmp/model.atmodel')
df_r2.loc[df_r2["CustID"].isin([customer])]
oz_stops.loc[oz_stops['stopid'] == '7270']
df.loc[df['last_name']=='Copening', 'age'] = df.age.median()$ df.loc[df['last_name'] == 'Copening']
words_hash_scrape = [term for term in words_scrape if term.startswith('#')]$ corpus_tweets_scraped.append(('hashtags', len(words_hash_scrape))) # update corpus comparison$ print('Total number of hashtags: ', len(words_hash_scrape)) #, set(terms_hash_stream))
rets.hist()$ plt.show()
S_lumpedTopmodel.forcing_list.filename
y_pred = pipeline.predict(X_test)$ print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))$
crime = pd.read_csv('clean_data/KCPD_Crime_Data_2017_clean.csv')$ moon = pd.read_csv('clean_data/Moon_Data_2017_cleaned.csv')
df[['Open','Close']]  # we need to pass the list of columns that we want access. Not just the individual column names $
res3 = rs.post('http://bsr.twse.com.tw/bshtm/bsMenu.aspx', headers = headers, data = payload)$
hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),$                  callbacks=[RocAuc], verbose=1)
df.to_csv(LM_PATH/'df.csv', index=False)
tweet_list = api.search(q='#%23mallorca')
vhd['season'] = vhd.index.str.split('.').str[0]$ vhd['term'] = vhd.index.str.split('.').str[1]
df = $ df.head()
firebase.post("Exhibitions/",new_data)$
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="ivanka").items(max_tweets):$     print(tweet)
table_rows = driver.find_elements_by_tag_name("tbody")[6].find_elements_by_tag_name("tr")$
total_stations = session.query(Station).group_by(Station.station).count()$ print('Number of stations: ' + str(total_stations))
psy_prepro = psy$ psy_prepro.to_csv("psy_prepro.csv")
features=list(kick_projects_ip)$ features.remove('state')$ response= ['state']
len(df[~(df.user_properties == {})])
df = web.get_data_yahoo("SPY",start ="2000-01-01",end = "20-03-2018")
afx_17 = json.loads(afx_x_2017.text)$ type(afx_17)
new_page_p = ab_df2.landing_page.value_counts()[0]/ab_df2.shape[0]$ old_page_p = ab_df2.landing_page.value_counts()[1]/ab_df2.shape[0]$ print(('new page probability', new_page_p),('old page probability', old_page_p))
data_= pd.read_sql(q,connection)$ data_.head()
table.meta['TSMIN']
import numpy as np$ ok.grade('q04')
expanded_data = pd.read_json( (data['_source']).to_json(), orient='index')
corr_matrix = dftotal.corr()$ corr_matrix["tripduration"].sort_values(ascending=False)
info_df.plot(kind='area', stacked=False, alpha=0.5, colormap='Spectral')$ plt.show()
bd.index.name
h2o.init()
tallies_file.export_to_xml()
joined_hist.joined = pd.to_datetime(joined_hist.joined)$ joined_hist = joined_hist.sort_values(ascending=True, by='joined')$ joined_hist.head()$
contractor = pd.read_csv('contractor.csv')$ state_lookup = pd.read_csv('state_lookup.csv')
with open(saem_women_save, mode='w', encoding='utf-8') as f:$     f.write(SAEMRequest.text)
TERM2017 = INT.loc[(INT.Term == 'Fall 2017')]$ TERM2018 = INT.loc[(INT.Term == 'Fall 2018')]$ TERM2019 = INT.loc[(INT.Term == 'Fall 2019')]$
iris.groupby('Species')['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'].describe()
vocab = vect.get_feature_names()
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
df=json_normalize(data["dataset"], "data")$ df.columns = col_names$ df.head(3)
model.wv.most_similar(positive=['pasta','chinese'], negative=['italian'])$
covar=np.cov(x,y)$ covar
df.head(20)
results = model_selection.cross_val_score(gnb, X_test, Y_test, cv=kfold)$ results.mean()
r.json()['dataset']
x['age'] = x['age'].astype('timedelta64[D]')
light_curve_fit_df.to_hdf('/Users/jmason86/Dropbox/Research/Postdoc_NASA/Analysis/Coronal Dimming Analysis/Example Fit Dimming Light Curve.hdf', 'light_curve_df')
red_4.drop(['created_utc', 'time fetched'], axis = 1, inplace = True)
fin_p.index
index = similarities.MatrixSimilarity(lsi[corpus])$ index.save('bible.index')
expectancy_for_least_country = le_data.min(axis=0)$ expectancy_for_least_country
measure_val_2014_to_2017.count()
df1 = add_percentiles(df)$ df1.head()
layout_row = bokeh.layouts.row(fig_out)$ bokeh.plotting.show(layout_row)
S_lumpedTopmodel.decision_obj.bcLowrSoiH.options, S_lumpedTopmodel.decision_obj.bcLowrSoiH.value
data.groupby('SA')['RTs'].sum().plot(kind = 'barh')
base_df.tail()
negative = '/Users/EddieArenas/desktop/Capstone/negative-words.txt'$ negative = pd.read_table(negative, encoding = "ISO-8859-1")
(fe.bs.SPXmean, fe.bs.SPXsigma)
data.show()$ data.printSchema()$
import pandas as pd       $ train = pd.read_csv("movie-data/labeledTrainData.tsv", header=0, \$                     delimiter="\t", quoting=3)
dfs_morning.sort_values(by='ENTRIES_MORNING', ascending = False).head(50)$ threshold = 100000$ dfs_morning.loc[dfs_morning['ENTRIES_MORNING']>threshold, 'ENTRIES_MORNING'] = dfs_morning.ENTRIES_MORNING.quantile(.5)
engine = create_engine('sqlite:///results.db')$ pd.read_sql_query('SELECT * FROM demotabl LIMIT 5;',engine)
plot_BIC_AR_model(data=therapist_duration.diff()[1:], max_order_plus_one=10)
deck = pd.DataFrame(titanic3['cabin'].dropna().str[0])$ deck.columns = ['deck']  # Get just the deck column$ sns.factorplot('deck', data=deck, kind='count')
mean_encoding_test(val, train,'Block',"any_spot" ) $ mean_encoding_test(val, train,'DOW',"Real.Spots" ) $ mean_encoding_test(val, train,'hour',"Real.Spots" ) 
rf.score(X_train, y_train)
donors.loc[donors['Donor Zip'] == 606 , 'Donor City'].value_counts()
df['screen_name'].value_counts()
pd.to_datetime(['2009/07/31', 'asd'], errors='raise')
fNames = dfX.columns
tvec = TfidfVectorizer(stop_words='english')$ X_train_counts = tvec.fit_transform(X_train)$ X_test_counts = tvec.transform(X_test)
output = pd.DataFrame(data={"id":test["id"], "sentiment":result,})# "probs":result_prob[:,1]})$ output.to_csv(os.path.join(outputs,'Word2Vec_AverageVectors.csv'), index=False, quoting=3)
plt.boxplot(raw_scores, vert=False)$ plt.xticks(xlocs, xlocs)$ plt.xlabel('Beer Ratings');
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
collection_posts.delete_many({})$ collection_comments.delete_many({})$
day_markers = np.arange(df1['DATETIME'].min(), df1['DATETIME'].max(), timedelta(days=1))$ day_markers = np.append(day_markers, df1['DATETIME'].max())$ day_markers
import pandas as pd$ reviews=pd.read_csv("ign.csv")$ reviews.head()
messages=df.status_message.tolist()$ messages[:5]
spark_path = "D:/spark-2.3.1-bin-hadoop2.7"
with open('test.csv') as f:$     size=len([0 for _ in f])$     print("Records in test.csv => {}".format(size))
name = "binding_2018-04-03_no1"$ data.to_csv("models/comparison/"+name+".csv")
msft = pd.read_csv("../../data/msft.csv", dtype={'Volume': np.float64})$ msft.dtypes
plans_set = set()$ [plans_set.add(plan) for plans_combination in np.unique(USER_PLANS_df['scns_array']) for plan in plans_combination ]$ plans_set
tweet_df.to_csv("output/Sentiment_Analysis_Media_Data.csv",$                      encoding="utf-8", index=False)
mask = (df['tweet_created'] > "Thu Dec 14 00:00:00 +0000 2017") & (df['tweet_created'] <= "Thu Dec 14 23:59:59 +0000 2017")$ data_2017_12_14 = df.loc[mask]
daily_trading_volume = [daily[6] for daily in json_data['dataset_data']['data'] if daily[6] != None]$ average = str(round(sum(daily_trading_volume)/len(daily_trading_volume),1))$ print('The average daily trading volume during 2017 was ' + average + '.')$
from scipy.cluster.hierarchy import dendrogram, linkage, set_link_color_palette
for img in featured_image_url:$     link = img.a['data-fancybox-href']$ link
adj_close_start = adj_close[adj_close['Date']==end_of_last_year]$ adj_close_start.head()
(y_hat.shape, y_train.shape)
collection.delete_item('AAPL')
y_pred = model.predict(x_test, batch_size=1024, verbose=1)
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')$ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
data.plot.scatter(x = 'Unnamed: 0', y = 'Age', figsize = (15, 10))
df.dropna()
svg2 = displacy.render(parsed4, style='ent', jupyter=True)
clf.fit(digits.data[:-2], digits.target[:-2])
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
X_copy['item_no'] = X_copy['item_no'].apply(lambda x: hash(x))
sentimentDf.sort_values("date", inplace=True, ascending=True)$ sentimentDf.head()
print(dfd.hspf.describe())$ dfd.hspf.hist()
df.sample(5)
xticks = pd.date_range('00:00', '23:00', freq='H', tz='US/Eastern').map(lambda x: pd.datetime.strftime(x, '%I %p'))$ xticks
staging_bucket = 'gs://' + google.datalab.Context.default().project_id + '-dtlb-staging-resolution'$ !gsutil mb -c regional -l {storage_region} {staging_bucket}
significance_level = 0.05$ confidence_level = 1 - significance_level$
flights2 = flights.set_index(["year", "month"])["passengers"]$ flights2.head()
mv_lens.head()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)])$ new_page_converted.mean()
tweetdf = pd.read_csv('../../data/clean/tweets_w_lga.csv') # shortcut$ tweetdf['latlng'] = list(zip(tweetdf.lat, tweetdf.lng))
ebola_melt['type'] = ebola_melt.str_split.str.get(0)$ ebola_melt.head()
TensorBoard().stop(23002)$ print 'stopped TensorBoard'$ TensorBoard().list()
df = df[df["category"]=="Tech"].reset_index(drop=True)$ df.drop(["category"], axis = 1, inplace = True)
thistweet=pd.read_csv("../output/tweet_with_location.csv")$
pipe_lr_3 = make_pipeline(tvec, lr)$ pipe_lr_3.fit(X_train, y_train)$ pipe_lr_3.score(X_test, y_test)
ts.get_operation_data(2018,1)
print(json.dumps(geocode_result, indent=4))$
P.plot_1d('scalarCanopyTranspiration')
health_data_row.loc[2013:2017]  # 2017 doesn't exist, but Python's slicing rules prevent an exception here$
num_dr_new = dr_new['Provider'].resample('W-MON', lambda x: x.nunique())$ num_dr_existing = dr_existing['Provider'].resample('W-MON', lambda x: x.nunique())
nltk.help.upenn_tagset()
df.info()
df_tidied = df_tidied.sort_values(by=["name2"])$ df_tidied = df_tidied.drop_duplicates()$ print("Note that close to half rows are removed by removing the duplicates. New shape is", df_tidied.shape)
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))
print('Average daily volume of 2017: ' + str(np.mean(vol_vec).round(1)))
for key,value in deaths_sorted.items():$     print( "Deaths in " + str(key) + " = " + str(value))
year3 = driver.find_elements_by_class_name('yr-button')[2]$ year3.click()
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df['datetime'] = pd.to_datetime(df['datetime'],format=('%Y-%m-%d'))$ df.dtypes
print(re.split(('\n|\r\n'), tmp_string)[0])$ print(re.split(('\n|\r\n'), tmp_string)[1])$
bands.sum(axis=0)$
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
num_names = df.shape[0]$ print ('Number of names in the training dataset', num_names)
station_count = session.query(func.count(Station.id)).all()$ station_count
df1_after_df2 = df2.append(df1)$ df1_after_df2
pi_year10lambapoint9_PS11taskG = 2.02
json_data['dataset'].keys()
df.iloc[0:4]
def split_data(x,y):$     x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, shuffle=False)$     return x_train, x_test, y_train, y_test
lsi = models.LsiModel(corpus_tfidf, id2word=bag, num_topics=10)$ corpus_lsi = lsi[corpus_tfidf]
os.chdir(root_dir + "data/")$ df_fda_drugs_reported.to_csv("filtered_fda_drug_reports.csv", index=False)
rng.day
building_pa_prc_zip_loc.to_csv("buildding_03.csv",index=False)$ building_pa_prc_zip_loc=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date'])
v_today = datetime.datetime.now()$ today_str  = '('+v_today.strftime("%m/%d/%y")+')'$ today_str_plot  = v_today.strftime("%m-%d-%y")$
num_id = df.nunique()['user_id']$ print("{} unique users in the dataset.".format(num_id))
hyperparam =[0.01, 0.1, 1, 10, 100]$
pattern = re.compile('AA')$ print(pattern.match('AAbc'))$ print(pattern.match('bcAA'))
!hdfs dfs -cat 31results-output/part-0000* > 31results-output.txt$ !head 31results-output.txt
overallQual = pd.get_dummies(dfFull.OverallQual)$
sns.distplot(virginica)$ sns.distplot(versicolor)
woba_leaderboard = df.groupby(['batter_name', 'batter'])['woba_value'].agg(['mean', 'count'])$ woba_leaderboard.loc[woba_leaderboard['count']>100,].sort_values('mean', ascending=False).head()
grid_lat = np.arange(np.min(lat_us), np.max(lat_us), 1)$ grid_lon = np.arange(np.min(lon_us), np.max(lon_us), 1)$ glons, glats = np.meshgrid(grid_lon, grid_lat)
df_raw = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ df = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ print df_raw.head()
df2 = df[['MeanFlow_cfs','Confidence']]$ df2.head()
plt.figure()$ precip_df.plot(kind = 'line', x = 'Date', y = 'Precip')$ plt.legend(loc='best')$
df_CLEAN1A['AGE'].max()
session.query(Measurement.id, func.avg(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
url_1day='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=TaaJzC_if3-1gX5Wty4D&start_date=2017-04-10&end_date=2017-04-10'$ response=requests.get(url_1day)
X[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
words_only_sp_freq = FreqDist(words_only_sp)$ print('The 100 most frequent terms (terms only): ', words_only_sp_freq.most_common(20))
model_info = kipoi_veff.ModelInfoExtractor(model, Dataloader)$ vcf_to_region = kipoi_veff.SnvCenteredRg(model_info)
df1.tail(5)
from sqlalchemy import create_engine$ engine = create_engine('mysql+pymysql://root:kobi5555@0.0.0.0/proddb')
len(vk_download('friends.getMutual','target_uid=10592581')['response'])
df_TempIrregular['timeStamp'] = pd.to_datetime(df_TempIrregular.pubTimeStamp)$
num_words=["roussia","jayin","page","chance"]$ print (words_df[ words_df['Word'].isin(num_words) ])
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
r = requests.get(url1)$ json_data = r.json()
n_old = df2.query(('landing_page == "old_page"')).count()[0]$ n_old
df = df.set_index('email')
average_trading = statistics.mean([day[6] for day in data])$ print ('Average daily trading volume for 2017:', round(average_trading,2))
info_final.drop('idAviso', axis = 1, inplace = True)$
df_merge = pd.merge(df_schoo11, df_students1, on='school_name')$ df_merge.drop(['School ID', 'Student ID'], axis = 1, inplace=True)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \$       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY$ request = requests.get(url)
df_Diff=df_Modified-df_Created$ df_Diff.head(9)
emb, summ = seq2seq_inf.generate_issue_title(txt)$ summ
feature_names = vectorizer.get_feature_names()$ if feature_names:$     feature_names = np.asarray(feature_names)
np.shape(temp_fine)
building_pa_prc_shrink.to_csv("buildding_01.csv",index=False)$
train_labels = one_hot_matrix(dflong[dflong['Date'].isin(train_dates)]['Y'].as_matrix(), 3) $ validation_labels = one_hot_matrix(dflong[dflong['Date'].isin(validation_dates)]['Y'].as_matrix(), 3)$ test_labels = one_hot_matrix(dflong[dflong['Date'].isin(test_dates)]['Y'].as_matrix(), 3)
x.drop([0, 1])
pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)$ tweets = api.search(q='Deloitte', rpp=1)$ pp.pprint([att for att in dir(tweets) if '__' not in att])
shown = pd.DataFrame(data.tasker_id.value_counts())$ shown.loc[shown['tasker_id']==1]
ab_df_new[['CA','UK','US']] = pd.get_dummies(ab_df_new.country)
df = pd.read_csv("../../data/msft2.csv",skiprows=[0,2,3])$ df
print(voters.PermCategory.unique())$ voters.PermCategory.value_counts(dropna=False)
reverseAAPL = AAPL.sort_index(ascending=False)$ reverseAAPL.head()
data = data.dropna(subset=['name'], how='any')$ data.info()
file_attrs_string = str(list(hdf5_file.items()))$ file_attrs_string
linear_predictor = linear.deploy(initial_instance_count=1, #Initial number of instances. $                                  instance_type='ml.m4.xlarge') # instance type
df = df.drop('vegetables', axis=1)$ df
html = open("./data/page.txt", 'r').read()
df_first_days['active_user'].value_counts()
if 1 == 1:$     token_counter = collections.Counter(tokens)$     print(str(token_counter.most_common(50)))
df_prec = df_prec.set_index('date',drop=True)$ display(df_prec.head())$ display(df_prec.shape)
population.loc['California':'Illinois']
df_merge.groupby(['school_name', 'grade']).math_score.mean().unstack()
afxx_data = r.json()
beginDT = '2017-02-01T00:00:00.000Z'$ endDT = '2017-09-01T00:00:00.000Z'$
hot_df.to_csv('data_redditv2.csv')
mlab_uri = os.environ['MLAB_URI']$ mlab_collection = os.environ['MLAB_COLLECTION']
for item in result_set:$     print(item.index,item.relationship)
sn.distplot(a.A.flatten()[:],kde=False,norm_hist=True,bins=900)$ plt.xlim((0.,0.5))
df_from_json = pd.read_json("../../data/stocks.json")$ df_from_json.head(5)
list(set(df['City'][df.CustomerID=='0000119007']))   
rain_df.set_index('date').head()
fpr_a = (grid_pr_fires.sort_values(['glat', 'glon'], ascending=[False,True])['pr_fire']$          .values.reshape(26,59))
file = open('tweets/tweets_Trump.csv', 'w')$ for tweet in results:$     file.write("%s\n" % tweet)
polynomial_features = PolynomialFeatures(2, include_bias=False)$ polynomial_features.fit(x_train['Pending Ratio'].values.reshape(-1,1))$ training_pending_ratio = polynomial_features.transform(x_train['Pending Ratio'].values.reshape(-1,1))
adj_glm = smf.glm('hospital_expire_flag ~ C(inday_icu_wkd) + C(admission_type)', $                      data=data, family=sm.families.Binomial()).fit()$ adj_glm.summary2()$
logit_countries = sm.Logit(df4['converted'], $                            df4[['country_UK', 'country_US', 'intercept']])$ result2 = logit_countries.fit()
BPAIRED_SHOPIFY['shopify_order_id'] = [a.split('-')[0] for a in BPAIRED_SHOPIFY['channel_order_id']]
dfm = (filtered_df['l_req_3d'] - filtered_df['l_req_3d_num_tutors'])$ filtered_df.loc[dfm > 0, :]
data['sepal_area'] = data.sepal_length * data.sepal_width$ print(data.iloc[:5, -3:])
import pandas as pd$ git_log = pd.read_csv('datasets//git_log.gz', sep='#', encoding='latin1', header=None,  names=["timestamp", "author"], compression='infer')$ git_log.head(5)
store_items = pd.DataFrame(items2, index = ['store 1', 'store 2'])$ store_items
df = df.drop(['Count'], axis=1)
iris.groupby('Species')['Species'].count()
dfLikes["date"] = dfLikes["created_time"].apply(lambda d: datetime.strptime(d, '%Y-%m-%dT%H:%M:%S+%f').strftime('%Y%m%d'))
grouped_publications_by_author['authorId'].nunique()
def tempF2C(x): return (x-32.0)*5.0/9.0$ def tempC2F(x): return (x*9.0/5.0)+32.0
%%time$ reader = pa.RecordBatchFileReader(data_dir + file_name + '.arrow')$ read_table = reader.read_all()
tweet_data.sort_values('rating', ascending= False).head()$
daily_df = pd.concat([predictions_daily, stock_daily], axis=1, join='inner')$ daily_df.head()
s3.reindex(np.arange(0,7), method='bfill')
tallies_file = openmc.Tallies()
churn_df.shape$
df_birth.Continent.value_counts(dropna=False)
dfClientes.iloc[1, 3] = 50
wednesdays = pd.date_range('2014-06-01','2014-08-31', freq='W-WED')$ wednesdays.values
RunSQL(sql_query)$ actor = pd.read_sql_query(sql_query, engine)$ actor.head()
USERNAME = ''$ TOKEN = ''$ API_BASE_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'
df_vow.describe()
commits_per_year = corrected_log.resample('AS',on='timestamp')['author'].agg('count')$ print(commits_per_year.head())
https://graph.facebook.com/v2.12/https://www.facebook.com/DoctorKoWJ/?access_token=EAACEdEose0cBAD3Gl8ZAKfLburye6ZAoDst9aoTkFmqZBdyGnJyz5PrrgizU3sZAAL0EFLlZBrZB2EZBTEWrZAfiRhudbHZBwAfMiNvy21DUZBo9HJHgJpBrnyGeFZCUt9MlGEfZBukCZC38GyuaKpSm8qx0f8tZCGjBAHUJ36bJjikdB4PsuSFQv9gizFwOI2vXd2wNYZD&debug=all&format=json&method=get&pretty=0&suppress_http_code=1$ 觀察後發現即是'https://graph.facebook.com/v2.9/{}/posts?limit=100&access_token={}'.format(fanpage_id, token)$ http://web.jobbole.com/89106/
X = df[[col for col in df.columns if col != 'bitcoinPrice_future_7']]$ y = df['bitcoinPrice_future_7']  
abc = abc.reset_index(level=[0,1])$ abc.columns
test_features = bind_features(test, train_test="test").cache()$ test_features.count()
cur.execute('SELECT EmpGradeRank, count(EmpGradeRank) FROM demotabl WHERE EmpGradeRank="SENIOR";')$ cur.fetchone()
who_purchased.columns = ['BOUGHT_'+str(col) for col in who_purchased.columns]
geometry.export_to_xml()
%matplotlib inline$ commits_per_year.plot(figsize=(12,7), kind='bar', legend=None, title='Commits per year')
mlp_df.plot(figsize=(10, 10))$ plt.show()
import nltk$ nltk.download('punkt')$ from nltk.tokenize import word_tokenize
cityID = '42e46bc3663a4b5f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Worth.append(tweet) 
all_sets.loc[:, ["name", "releaseDate", "setSize"]].sort_values(["releaseDate"]).tail()
test_df = pd.read_csv("test.csv", dtype=dtypes)$ test_df.head()
df['x'] = df.index$ df.plot.scatter(x='x', y='y')
print(df['State'].value_counts(dropna=False))
dtc = DecisionTreeClassifier(max_leaf_nodes=1002)$ scores = cross_val_score(dtc, X, np.ravel(y,order='C'), cv=10)$ print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
google_stock.describe()
sentiments_df.to_csv("recentTweets.csv", encoding="utf-8", index=False)
yearago_date = dt.date(2016, 8 , 22)$ print(yearago_date)
plt.style.available
from_6_cdf.plot(kind='barh', x='category', y='occurrence_count', figsize=(12, 10), title= 'Categories', label= "Occurrence Count")$ plt.gca().invert_yaxis()$ plt.legend(loc= 4, borderpad= True)
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['acc'])
supreme_court_df.head()
data['inday_icu_wkd'] = np.where(data.intime_icu.dt.weekday <= 4, $                                  'weekday','weekend')$ data['inday_icu_wkd'].value_counts()
uber1.to_csv('uber1.csv', index=False)$ uber2.to_csv('uber2.csv', index=False)$ uber3.to_csv('uber3.csv', index=False)
edge_types_file = directory_name + 'edge_types.csv'   # Contains info. about every edge type$ edges_file = directory_name + 'edges.h5'             # Contains info. about every edge created
df.index.tz_localize('GMT').tz_convert('US/Eastern')
data.loc[[pd.to_datetime("2016-12-01"), pd.to_datetime("2016-12-03")]]
precp_df = pd.DataFrame(results, columns=['date', 'prcp'])$ precp_df.set_index('date', inplace=True, )$ precp_df.head(10)
hi_iday_delta_index = close_deltas.index(max(close_delta for close_delta in close_deltas if close_delta is not None))$ ans_5 = ('%s had greatest between-day difference: %s' % (dates[hi_iday_delta_index], close_deltas[hi_iday_delta_index]))
import numpy as np$ result=results.home_score$ result.apply(np.median)
prcp_df = pd.DataFrame(prcp_data, columns=['Date', 'Precipitation'])$ prcp_df.set_index('Date', inplace=True) $ prcp_df.head()
word_vecs.sample(10)
soup.find('div', class_="poster-section left").find('img')['src']
exclude_year = [1985, 1986, 1987] $ lv_workspace.get_subset_object('A').set_data_filter(step=1, filter_type='exclude_list', filter_name='YEAR', data=exclude_year) 
engine = create_engine("sqlite:///hawaii.sqlite")
mean_pr = prec_long_df.groupby(['date'])['prec_kgm2'].mean()$ mean_pr.plot(x='date', y='prec_kgm2')
with open('united_total_list_rake.pickle', 'wb') as f:$     pickle.dump(united_total_list_rake, f, pickle.HIGHEST_PROTOCOL)
table = Table.read("../datasets/catalogs/fermi/gll_psc_v16.fit.gz")$
databreach_2017 = databreach_2017.dropna(axis=0,how='any')
input_col = ['msno','plan_list_price','actual_amount_paid','payment_plan_days']$ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions',input_col)$
df = pd.read_csv('score_data_set.csv', index_col='createdAt', parse_dates=['createdAt'])$ df.head()
import dotce$ stats = dotce.LanguageStats()
sns.distplot(df['num_comments'])$ plt.title("Distribution - Number of comments");
data = df.values
questions.set_index('createdAt', inplace=True)
pd.DataFrame(sanders)
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))
sentiments_df = pd.DataFrame(sentiment_array)$ sentiments_df = sentiments_df[["TweetsAgo","Target","User","Date","Positive","Neutral","Negative","Compound","Text"]]$ sentiments_df.head()
gene_df.drop('attributes', axis=1, inplace=True)$ gene_df.head()
X_top = X[top_features]$ model.fit(X_top, y)
prcp_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation'])$ prcp_df.set_index('Precipitation Date', inplace=True) # Set the index by date
logit2 = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']])$ result = logit2.fit()$ result.summary2()
train_df = pd.read_csv("train.csv", skiprows=range(1,159903891), nrows=25000000, dtype=dtypes)$ train_df.head()
text_classifier.get_step_params_by_name("text1_char_ngrams")
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
stat_info_st = stat_info[0].apply(fix_space)$ print(stat_info_st)
tranny = df.T$ tranny
S_distributedTopmodel.basin_par.filename
y_pred = pipe_nb.predict(pulledTweets_df.emoji_enc_text)$ y_proba = pipe_nb.predict_proba(pulledTweets_df.emoji_enc_text)$ pulledTweets_df['sentiment_predicted_nb']=[classes[y_pred[i]] for i in range(len(y_pred))]
churned_unordered = unordered_df.loc[churned_unord]
schedId_in_proj = [x for x in df_sched.ProjectId if x in list(df_proj.ProjectId)]$ schedId_in_bud = [x for x in df_proj.FMSID if x in list(df_bud.project_id)]
total=extraD$ total.loc[:,'exposure_time']=total.apply(exposure_redshift, args=('normal',), axis=1)
np.all(x == 6)
df_teacher_behavior = df_teacher_behavior[df_teacher_behavior.awj_teacher_id.isin(pick_list)]$ df_teacher_behavior.drop("教材", axis=1, inplace=True)$ df_teacher_behavior["机构"] = "com"
for c in ccc:$     ved[c] = ved[ved.columns[ved.columns.str.contains(c)==True]].sum(axis=1)
df.loc[:,'A']
from bmtk.builder.networks import NetworkBuilder$ net = NetworkBuilder('V1')
scores_median = np.median(sorted(raw_scores))$ print('The median is {}.'.format(scores_median))
urban_summary_table = pd.DataFrame({"Average Fare": urban_avg_fare,$                                    "Total Rides": urban_ride_total})$ urban_summary_table.head()
dfg = dfg.sort_values(['discharges'], ascending=[False])$ dfg = dfg.reset_index(['drg3']) $ dfg.head()
soup.p
df.plot()$ plt.show()
!cat data/kaggle_data/features.txt
df_ct.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID','Unnamed: 18'],axis=1,inplace=True) 
month_year_crimes = crimes.groupby(['year', 'month']).size()
txt_tweets = txt_tweets[~txt_tweets.str.startswith('RT')]$ txt_tweets_position = tweets[~tweets['text'].str.startswith('RT')]['position']$ txt_tweets_position.value_counts()
len(df.index)$
flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')). \$     agg(F.mean('price_will_drop_num')).orderBy(['trip', 'stay_days', 'lead_time'], ascending=[1, 1, 0]).show(1000)$
sentiments_df = pd.DataFrame(sentiment_array)
data = pd.read_csv('./fake_company.csv')$ data
gdax_trans_btc.plot(kind='line',x='Timestamp',y='BTC_balance_GBP', grid=True);
print('{0:.2f}%'.format((scores[0:2.5].sum() / total) * 100))
questions = pd.concat([questions.drop('coming_next_reason', axis=1), coming_next_reason], axis=1)
psy_df5 = HAMD.merge(psy_df4, on='subjectkey', how='right') # I want to keep all Ss from psy_df$ psy_df5.shape
race_vars.columns = race_vars.columns.str.replace(' ', '_')$ race_vars.columns = race_vars.columns.str.replace('/', '_')$ race_vars.columns = race_vars.columns.str.lower()
output= "Create view ViewDemo as select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc;"$ cursor.execute(output)$
ctc_alpha = ctc ** (1/3)$ ctc_alpha = ctc_alpha / ctc_alpha.max().max()
data.dtypes$
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$ print(new_page_converted)
tobs_df=pd.DataFrame(date_tobs, columns=['date','tobs'])$ tobs_df.head()
timezones = DataSet['userTimezone'].value_counts()[:10]$ print(timezones)
y = api.GetUserTimeline(screen_name="HillaryClinton", count=20, max_id=935706980643147777, include_rts=False)$ y = [_.AsDict() for _ in y]
model.doesnt_match("man tiger woman child ".split())$
tree = DecisionTreeClassifier(criterion='gini')$ model = tree.fit(X_train_total, y_train)$ model.score(X_test_total_checked, y_test)
viz_1=sb.countplot(x=studies_b.enrollment, data=studies_b)$
import gensim, logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
clean_sentiments = pd.DataFrame(clean_sentiments, columns = ['Target', 'Date', 'Tweet Ago', 'Compound',$                                                              'Positive', 'Negative', 'Neutral', 'Text', 'Source'])$ select_data = clean_sentiments
np.zeros(5)
AFX_X_data = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=ZfFwbzzp8_Rsbi_mGznR&start_date=2017-01-01&end_date=2017-12-31')
ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')$ ebola_melt.head()
rpt_regex = re.compile(r"(.)\1{1,}", re.IGNORECASE);$ def rpt_repl(match):$     return match.group(1)+match.group(1)
guido_text = soup.text$ print(guido_text[:500])
CNN = news_df.loc[(news_df["Source Account"] == "CNN")]$ CNN.head(2)
 $ session.query(Measurement.id, func.min(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
merge_df['Primary Language'] = merge_df['Language'].str.split('-').str[0]$ merge_df['Language Location'] = merge_df['Language'].str.split('-').str[1]
Measurements = Base.classes.measurement$ Stations = Base.classes.station
conn.execute(sql)
iso_gdf.intersects(iso_gdf_2)
price_data = heading.append(price_data)$ price_data.columns = price_data.iloc[0]
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-02-02&end_date=2018-02-03&api_key=' + API_KEY$ r = requests.get(url)$ json_data = r.json()
fire_size_file = '../data/model_data/size_mod.sav'$ pickle.dump(rf, open(fire_size_file, 'wb'))
df['new_column'] = df['column_1'] + df['column_2']$ df['new_column'] = df.apply(my_function, axis = 1)$ df.to_csv['my_data.csv']$
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours Partial Autocorrelation')
closes.plot(figsize=(8,6));
import pandas as pd$ pd.core.common.is_list_like = pd.api.types.is_list_like$ import pandas_datareader.data as web
log_file_name = 'shopee_add_and_cancel_fans_log\\shopee_add_and_cancel_fans_log_' + today_date_string + '.txt'$ logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
import pymysql$ pymysql.install_as_MySQLdb()$ import MySQLdb
model = app.models.get("Textures & Patterns")$ model.predict_by_url(url='https://assets.vogue.com/photos/595a6e6ca236912379c91ccf/master/pass/_ARC0003.jpg')
tags = tags.sum(level=0)$ tags.head()
for tweet in public_tweets:$     print (tweet.text + " : " + str(tweet.created_at) + " : " + tweet.user.screen_name + " : " + tweet.user.location) $     print (" ")
useful_indeed.isnull().sum()$
df_max = df.groupby('date').head(1)$ df_count = df.groupby(['date'] ,as_index=False).count()$ df_mean = df.groupby(['date'], as_index=False).mean()
import cv2$ cv2.imread("smallgray.png",1)$
i = random.randrange(len(train_pos))$ train_pos[i]
clfgtb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train, y_train)$ clfgtb.score(x_test, y_test)
sns.kdeplot(utility_patents_subset_df.number_of_claims, shade=True, color="purple")$ plt.show()
from nltk import pos_tag$ sentence = word_tokenize('I always lie down to tell a lie.')$ pos_tag(sentence)
def day_of_week(times):$     datetime_df = pd.DataFrame(times)
env = Environment.from_studio("ticketing", "http://10.29.28.147:8090",$                               studio_api_token="dXNlci1kZXY6UGFzc3dvcmQ0REVW")$ exp = env.create_experiment('imdb-ratings')
year4 = driver.find_elements_by_class_name('yr-button')[3]$ year4.click()
from arcgis.gis import GIS$ from IPython.display import display$ gis = GIS() # anonymous connection to www.arcgis.com
for col in X_nnumcols:$     X[col] = X[col].apply(lambda l: hash(l))$     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
cdf.plot(kind='barh', x='category', y='occurrence_count', figsize=(12, 10), title= 'Categories', label= "Occurrence Count")$ plt.gca().invert_yaxis()$ plt.legend(loc= 4, borderpad= True)
words_mention_sp = [term for term in words_sp if term.startswith('@')]$ corpus_tweets_streamed_profile.append(('mentions', len(words_mention_sp))) # update corpus comparison$ print('List and total number of mentions: ', len(set(words_mention_sp))) #, set(terms_mention_stream))
data.loc[[pd.to_datetime("2016-09-02")]]
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid$ x = df30458.groupby('end_station_name').count()$ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
gnb.fit(X_clf, y_clf)
news_organizations_df['tweets'] = news_organizations_df.handle.map(get_df_for_user_tweets)$
temp_df2['titles'] = temp_df2['titles'].str.lower()
move_1_herald = sale_lost(breakfastlunchdinner.iloc[1, 1], 10)$ print('Adjusted total for route: ' + str(move_34p34h34h - move_1_herald))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2014-01-01&end_date=2014-01-02')$
df2.drop('building_use', axis = 1, inplace=True)
from sklearn.cluster import KMeans$ kmeans_model = KMeans(n_clusters=7) $ kmeans_model.fit(df)
df_categories = pd.read_csv('categories.csv')$ df_categories.head()
airline_df['sentiment'] = airline_df['tweet'].apply(lambda tweet: NBClassifier.classify(extract_features(getFeatureVector(processTweet2(tweet)))))$
datetime.now().toordinal() - datetime(1987, 1, 4).toordinal()
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(doc_duration, params=params, freq=12, title='Doctors Decomposition')
client.repository.list_models()
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))$
    return "https://github.com/{0}/{1}.git".format(org, project)$ 
tips.groupby(["sex","size"]).mean().loc[:,"total_bill"].loc["Female",3:5]
td_wdth = td_norm * 5$ td_alph = td_alpha$ td = td.round(1)
req = requests.request('GET', 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
nyt = news_sentiment('@nytimes')$ nyt['Date'] = pd.to_datetime(nyt['Date'])$ nyt.head()
df.iloc[-1]
reflClean = reflRaw.astype(float)$ reflClean
pd.to_datetime(['2009/07/31', 'asd'], errors='ignore')
df.reset_index(inplace=True)$ df.head()
df_cod2["Cause of death"].unique()
tw['tweet_id'].map(lambda x: len(str(x).strip())).value_counts()
r.json() 
from sklearn.metrics import confusion_matrix$ cmx = confusion_matrix(answers, preds, labels=labels)
total3=total.ix[(total['RA0']<335) & (total['RA0']>225)]$ total3$
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
start_of_event=datetime.datetime.strptime(start_of_event,'%Y-%m-%d') #Convert to datetime$ end_of_event=datetime.datetime.strptime(end_of_event,'%Y-%m-%d') #Convert to datetime$ location_name=location_name.replace(" ","_") #replace spaces with underscore
es.get(index="test-index", doc_type='tweet', id=1)
Base = automap_base()$ Base.prepare(engine, reflect=True)
from nltk.stem.wordnet import WordNetLemmatizer$ lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]$ print(lemmed)
df.to_html('resources/html_table_marsfacts.html')
lm = smf.ols(formula='y ~ x', data=df).fit()$ lm.params
df.set_index('datetime',inplace=True)$ df.index
g8_groups['area'].mean()
max(dif_dict, key=dif_dict.get) 
model_filename = 'models/finalized_traffic_flow_prediction_model.sav'$ loaded_traffic_flow_prediction_model = pickle.load(open(model_filename, 'rb'))$
questions.to_csv('../data/clean.csv')
reddit.to_csv('data_reddit.csv')
nitrodata['Year'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).year$ nitrodata['Month'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).month
breakdown[breakdown != 0].sort_values().plot($     kind='bar', title='Sean Hannity Number of Links per Topic'$ )
cleanedData = allData$ cleanedData[cleanedData['text'].str.contains("\&amp;")].shape[0]
geovol = fe.std(ss)$ geovol
data_final.shape
S_1dRichards.decision_obj.simulStart.value, S_1dRichards.decision_obj.simulFinsh.value
now_start = datetime.datetime.now()$ time_start = now_start.strftime("%Y-%m-%d (yyyy-mm-dd); %H:%M hrs.")$ print "# Starting time of computing: %s"%time_start
overall_df[overall_df['News Outlet'] == 'BBC']['Compound Score'].values[0]$
centroids0 = clusters_kmeans0.cluster_centers_
driver = webdriver.Chrome(executable_path="./chromedriver")
sentiment=sentiment.sort_values(by='Tweet_date', ascending = False)$ sentiment.head()
print("Mean squared error: %.2f"% mean_squared_error(y_test, y_pred))
df['launched_year'] = df['launched'].dt.year$ df['deadline_year'] = df['deadline'].dt.year
plt.pie(totalDrivers, labels = labels, explode = explode, $         colors = colors, autopct="%1.1f%%", shadow=True, startangle=180)$ plt.show()
mgxs_lib = openmc.mgxs.Library(geometry)$ mgxs_lib.energy_groups = groups
df['Non-Crime Criteria Met'] = df_criteria.apply(lambda x: y_count(x),axis=1)
preci_df.set_index('date').head()$
df_ratings.describe()
combined_df = test.join(prediction_df)$ combined_df.head(15)
rddScaledScores.reduce(lambda s1,s2: s1 + s2) / rddScaledScores.count()
bob_shopping_cart = pd.DataFrame(items, columns=['Bob'])$ bob_shopping_cart
image = soup.find('a',class_="button fancybox")['data-fancybox-href']
station_count = session.query(func.count(distinct(Measurement.station))).all()$ station_count$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ print("Last 10 tweets:")$ display(data.head(10))
mars_html_table = mars_df_table.to_html(classes='marsdata')$ mars_table = mars_html_table.replace('\n', ' ')$ mars_table
q1_url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31?api_key="+API_KEY$ r1 = requests.get(q1_url)$ r1 = r1.json()$
ADBC = AdaBoostClassifier(n_estimators=50)$ ADBC.fit(X, y) 
mft = most_followed_tweeters.followers$ normalized_followers = np.round((np.abs(np.array(mft)-np.array(mft).mean())/mft.std())*10, 0)
precip_data_df = pd.DataFrame(precip_data_dict)$ precip_data_df = precip_data_df.rename(columns={"prcp":"Precipitation"})$ precip_data_df.head()$
df1['K_lbs-Site1'] = df1["AnnualFlow_MGD"] * df1['TotalN'] * 8.34 / 100000$ df2['K_lbs-Site2'] = df2["AnnualFlow_MGD"] * df2['TotalN'] * 8.34 / 100000$ df3['K_lbs-Site3'] = df3["AnnualFlow_MGD"] * df3['TotalN'] * 8.34 / 100000
demographics = my_df_free1.iloc[:,0:3]$ scores = my_df_free1.iloc[:,5:]
corrected_log.sort_values('timestamp').describe()
trump_twitter = pd.read_json('trump_tweets_2017.json', encoding='utf8')$ trump_twitter.head()
number_one_df = spotify_df.loc[spotify_df["Position"] == 1,:]
ab_df2.loc[ab_df2.user_id.duplicated(),:]
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-05-08&end_date=2018-05-08&api_key=' + API_KEY)$
now = datetime.now()$ print(now)
merge_df['Satisfaction'] = pd.qcut(x=merge_df['Score'],labels=[False, True],q=2)
x.drop(["sum"], axis=1)
plt.figure(figsize=(15,5))$ sns.barplot(data=aa,x='weekofyear',y='message')
data = pd.read_sql("SELECT salary FROM empvw_20",xedb)$ print(data)
merged2.dropna(subset=['Specialty'], how='all', inplace=True)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
matt1 = dta.t[(dta.b==40) & (dta.c==1)]$
print 'All IDs are unique' if len(response_df) == len(response_df.id.unique()) else 'IDs not all unique'
session.query(func.count(Measurement.date)).all()
prcp_query = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > year_to_date).statement$ prcp_year_df = pd.read_sql_query(prcp_query, session.bind)$ prcp_year_df.set_index('date').head()
df = df.dropna().copy()
calls_nocontact.street_address.value_counts()
rf = RandomForestClassifier()$ rf.fit(X_train, y_train)$ rf.score(X_test, y_test)
period_df.iloc[0]['url']$ period_df.iloc[0]['time']$ test_image, test_image_url, test_datetime = download_single_snapshot(period_df.iloc[0]['url'], period_df.iloc[0]['time'], workspace_dir)
bins = [70, 72, 74, 76, 78, 80]$ temp_freq_dict={"Temperature":temp_list, "Frequency":frequency }$ temp_freq_df=pd.DataFrame(temp_freq_dict)
X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.2, random_state=23)
relevance_scores_df = pd.DataFrame(relevance_scores[0]).mean(axis=1)$ relevance_scores_df.describe()
news_sentiments = pd.DataFrame(sentiments_df, columns= ["News Source", "Date/Time", "Compound","Positive",$                                                         "Neutral", "Negative", "Tweet", "Tweets Ago"])$ news_sentiments.head()
df.iloc[0]
abc = df_providers.groupby(['year','drg3']) orange$ abc.head()
weather_features.iloc[:6]
studies = pd.read_csv('C:/Users/akapoor/Music/01 Docs/HealthCare App/ctdb/studies.txt', sep="|")$ studies.head()
march_2016 = pd.Period('2016-03', freq='M')$ print march_2016.start_time$ print march_2016.end_time
customers_arr = np.array(cust_list)$ items_arr = np.array(item_list)
%matplotlib inline$ env_test.unwrapped.render('notebook', close=True)$ env_test.unwrapped.render('notebook')
data.sort_values(by = 'Age', ascending = True)
df3 = df8.add_suffix(' Created')$ df7 = pd.merge(df,df3,how='left',left_on='Date Created',right_on='MEETING_DATE Created')$
pipe_lr_2 = make_pipeline(hvec, lr)$ pipe_lr_2.fit(X_train, y_train)$ pipe_lr_2.score(X_test, y_test)
colors = ["green", "red", "blue", "orange", "maroon"]$ x_axis = np.arange(len(final_df))$ x_labels = diff_df.index
sl[sl.status_binary==0][(sl.today_preds<sl.one_year_preds)].shape
conn = psycopg2.connect("dbname='pgsdwh' user='502689880' host='alpgpdbgp2prd.idc.ge.com' password='pass66824w'")
print "Logistic : %.2f%%" %(Logistic_scrore*100)$ print "Decision Tree : %.2f%%" %(DT_Score*100)$ print "Randomforest : %.2f%%" %(RF_Score*100)
rf.score(X_train, y_train)
!wget https://raw.githubusercontent.com/jacubero/ColabEnv/master/awk_netstat.sh -O awk_netstat.sh$ !chmod +x awk_netstat.sh$ !./awk_netstat.sh
tweets.query("text.str.contains('repúblicadominicana',False)").shape$
frames = [nuc, part_nuc, out_out, noSeal, entirecell]$ ps = pd.concat(frames)$ ps.head(10)
import os$ os.environ["instagram_client_secret"] = "91664a8e599e42d2a6a824de6ea456ec"$
tips.sample(5).reset_index(drop=True)
sl[sl.status_binary==0][(sl.today_preds==1)].shape[0]*.57
Measurement = Base.classes.measurements$ Station = Base.classes.stations
df_test_index = pd.DataFrame()$
tobs = session.query(Measure.station, Measure.date, Measure.tobs).order_by(Measure.date).\$ filter(Measure.date > '2016-08-23').all()$ tobs
results[results['type']=='Other']['tone'].value_counts()
term_tops = lda.get_term_topics(rand_word, minimum_probability=.0001)$ term_tops, get_topic_desig(term_tops)
Base = automap_base()$ Base.prepare(engine, reflect=True)$
inspector = inspect(engine)$ inspector.get_table_names()
sns.factorplot('pclass', data=titanic3, hue='sex', kind='count')
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']])$ res = lm.fit()$ res.summary2()
df.dropna(inplace=True) #because we are prediction 30 days extra empty values are created in other rows.$
coins = ccw.get_coin_list()$ COIN_DB = pd.DataFrame.from_dict(coins, orient='index')$
assembler = VectorAssembler(inputCols = feature_col, outputCol = "features")$ assembled = assembler.transform(ibm_train)
features_df.plot.bar(x='features',y='importance')
Session=sessionmaker(bind=engine)$ session=Session()$ result_set=session.query(Adultdb).first()$
df.to_csv('Tableau-CitiBike/TripData_2017_Fall.csv', index=False)
state = env.reset()$ state, reward, done, info=env.step(env.action_space.sample())$ state.shape
print(df.dtypes)
year_with_most_commits = commits_per_year['commits'].idxmax().year
pd.options.display.max_colwidth = 200$ data_df[['ticket_id','type','clean_desc','nwords']].head(30)
table_rows = driver.find_elements_by_tag_name("tbody")[28].find_elements_by_tag_name("tr")$
feature_names = vectorizer.get_feature_names()$ class_labels = clf.classes_$ print(class_labels)
df['created_at'] = pd.to_datetime(df['created_at'])$ df.head()
check_null = df.isnull().sum(axis=0).sort_values(ascending=False)/float(len(df))$ print(np.sum(check_null.values) == 0.0)
df['created_at'] = pd.to_datetime(df['created_at'])
contractor_final.to_csv('contractor_final.csv')
df2.columns
df = titanic3[['cabin', 'pclass']].dropna()$ df['deck'] = df.apply(lambda row: ord(row.cabin[0]) -64, axis=1)$ sns.regplot(x=df["pclass"], y=df["deck"])
training_set, test_set = newdf[newdf['date']<split_date], newdf[newdf['date']>=split_date]$ training_set = training_set.drop('date', 1)$ test_set = test_set.drop('date', 1)
from sightengine.client import SightengineClient$ client = SightengineClient("737618018", "bstrJ5VzARavYy5FsELN")$ output = client.check('face-attributes').set_url('http://cache.magazine-avantages.fr/data/photo/w1000_c18/4j/hommebrunyeuxverts.jpg')
df_df = pd.DataFrame(df1)$ df_df
merged_data = youTubeTitles.append(pornTitles, ignore_index=True)$ title_matrix= merged_data.loc[:,["title"]]$ target_array=merged_data.loc[:, ["isPorn"]]
post_gen_paired_cameras_missing_from_join = np.setdiff1d(BPAIRED_GEN['shopify_order_id'],ORDER_BPAIR_POSTGEN['shopify_order_id'].astype(str))
import sklearn.model_selection as model_select$ results = model_select.cross_val_score(classifier, X, y, cv=10, scoring='accuracy')$
data['SA'] = np.array([analize_sentiment(tweet) for tweet in data['Tweets']])
top_words = pd.DataFrame(X.toarray()).sum().sort_values(ascending=False).head(10)$ for word in top_words.index:$     print 'Feature: {}, Token: {}'.format(word, tfidf.get_feature_names()[word])
test_tweet = api.user_timeline(newsOutlets[0])$ print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
iex_coll_reference.count()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X'$ r = requests.get(url)$ data = r.json()
data_year_df = pd.DataFrame.from_records(data_year, columns=('Date','Station','Prcp'))$ data_year_df.head()
trump_origitnals["lText"] = trump_o["text"].map(lambda x: x if type(x)!=str else x.lower())
datetime.date(datetime(2014,12,14))
content = [item.find_all(['p','h2']) for item in article_divs]$
raw = pd.read_csv('jobs_raw.tsv', sep='\t')
station_count = session.query(Measurement.station).distinct().count()$ print('There are',station_count,'stations')
for cell in openmc_cells:$     for rxn_type in xs_library[cell.id]:$         xs_library[cell.id][rxn_type].load_from_statepoint(sp)
documents = [x.split(' ') for x in documents]
np.shape(rhum_us_full)
auth_key = 'yZVrH8Esn8zs7vGPN2zJ'$ main_df = pd.DataFrame # Lets create a blank dataframedf = quandl.get("FMAC/HPI_KOKIN", authtoken=auth_key)
import numpy as np$ import matplotlib.pyplot as plt
df_cod3 = df_cod2.copy()$ df_cod3["Cause of death"] = df_cod3["Cause of death"].apply(classify_natural)$ df_cod3
print(type(some_rdd),type(some_df))$ print('some_df =',some_df.collect())$ print('some_rdd=',some_rdd.collect())
pd.options.display.max_colwidth = -1$ df[['Text', 'PP Text']]$
tweet_df = pd.DataFrame(twitter_list)$ tweet_df.head()
print(scratch.shape)$ scratch = scratch.dropna(subset=['age'])$ print(scratch.shape)
utility_patents_subset_df['prosecution_period'] = utility_patents_subset_df.grant_date - utility_patents_subset_df.filing_date$ utility_patents_subset_df.prosecution_period = utility_patents_subset_df.prosecution_period.apply(lambda x: x.days)$ utility_patents_subset_df.prosecution_period.describe()
data['win_differential'] = abs(data.homeWinPercentage - data.awayWinPercentage)$ data['win_team'] = np.where(data.awayWinPercentage >= data.homeWinPercentage, 'away', 'home')$ data['game_state'] = np.where(data.win_differential < 0.6, 'close', 'notclose')$
t = input("Input a template: ")$ t = json.loads(t)$ DBfindByTemplate(t)$
df1['PCT_Change']=(df1['Adj. Close']-df1['Adj. Open'])/df1['Adj. Open'] $ df1['PCT_Change'].head()
df2 = tier1_df.reset_index()$ df2 = df2.rename(columns={'Date':'ds', 'Incidents':'y'})
session.query(Measurements.date).order_by(Measurements.date).first()
pred.head(10)$
file_names = []$ file_names = glob.glob('*.csv')
all_sets = pd.read_json("AllSets.json", orient = "index")$ all_sets.head()
bg3 = bg2.drop(bg2.index[0]) # drop first row$ bg3
ml.confusion(dep_test.reshape(dep_test.shape[0]), $              predicted, ['No', 'Yes'], 2, 'Smoker Classification [Numeric & Categoric]')
xtrain,xtest,ytrain,ytest=cross_validation.train_test_split(X,Y,test_size=0.2)
executable_path = {'executable_path': "browser = Browser('executable_path', C:\Users\Brittney_Joyce\AppData\Local\Temp\Temp1_chromedriver_win32.zip)"}$ browser = Browser('chrome', **executable_path)$ browser.visit(url)
df_events = pd.read_csv("data_output/df_events.csv",low_memory=False)
trainpath = "./data/train.csv"$ testpath = "./data/test.csv"$
idx = np.random.permutation(train_data.index)$ train_data = train_data.reindex(idx)$ train_labels = train_labels.reindex(idx)
plt.xlim(0, 1.0)$ _ = plt.barh(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.values()), align='center')$ _ = plt.yticks(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.keys()))
df_new[['action_new_page','action_old_page']] = pd.get_dummies(df_new['landing_page'])$ df_new = df_new.drop('action_old_page', axis=1)$ df_new.head()
true_file = pd.read_csv(filepath_or_buffer=os.path.join(edfDir, 'pt1sz2_eeg.csv'), header=None)$ test_file = pd.read_csv(filepath_or_buffer=outputData, header=None)
loan_stats["loan_status"].table()
experience.columns = ['RATE_'+str(col) for col in experience.columns]
lm.predict(x_test)
intervention_train.reset_index(inplace=True)$ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
columns = ['DepartmentId', "Date", "Volume"]$ df_ = pd.DataFrame(data = new_df, columns = columns)$ df_.to_csv("Task1.csv", sep='\t')
tweet_df_polarity = tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
print([col for col in data.columns if data[col].dtype == object]) # 字符数据的处理
rng = np.random.RandomState(23)$ sample_size = 50$ rng.choice(sizes, sample_size, replace=True)
test_url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-08-21&end_date=2018-08-21&api_key=' + API_KEY$ test_data = requests.get(url)
walkmin = walk.resample("1Min")
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-09&end_date=2018-08-09&api_key=' + key)$ r.headers['content-type']$ r.json()
!git clone https://github.com/benfred/implicit/
np.any(x < 0)
df = pd.read_csv('data/test1.csv', parse_dates=['date'])$ df
churned_bool = pd.Series([USER_PLANS_df.loc[uid]['status'][np.argmax(USER_PLANS_df.loc[uid]['scns_created'])] =='canceled' for uid in USER_PLANS_df.index],index=USER_PLANS_df.index)
df.T
styles = df.groupby('simple_style').size().sort_values(ascending=False)$ styles.head(5)
print(df.head())
print ('\nThere are {} rows in this dataframe.'.format(len(df)))$ uniqueRows = len(pd.unique(df['user_id']))$ print ('\nThere are {} unique rows in this dataframe.'.format(uniqueRows))
import pandas as pd$ df = pd.read_csv("noshit.csv", index_col=0)$ df= df.sort_values(by=["rate"], ascending=False)
df.rename(columns={'nm':'city','countryCode':'countrycode'},inplace=True)$ df.set_index("id", inplace=True)$ df.head()
s3 = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5})$ s3
import numpy as np$ import matplotlib.pyplot as plt$ import pandas as pd
X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(df_ml_features_scaled, \$                                                     df_ml_target, test_size = 0.2, random_state = 10,\$                                                     stratify = df_ml_target)
logit_countries2 = sm.Logit(df4['converted'], $                            df4[['ab_page', 'country_UK', 'country_US', 'intercept']])$ result3 = logit_countries2.fit()
s.plot(figsize=(12,10), label="Actual")$ s_median.plot(label="Median")$ plt.legend();
df_5['weeks_between'] = df_5.submitted_at - df_5.application_created_at$ df_5['weeks_between'] = np.ceil(df_5.weeks_between.dt.days/7)
print 'RF: %s' % rf.score(X_test, y_test)$ print 'KNN: %s' % knn.score(X_test, y_test)
mgxs_lib.domain_type = 'cell'$ mgxs_lib.domains = geometry.get_all_material_cells().values()
RatingSampledf.to_csv("..\\Output\\SampleRatings.csv")
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'game_winners'
film_category_df = pd.read_sql(SQL, db)
cnn_compound=df.loc[df.company=='CNN']['compound']$ cnn_when=df.loc[df.company=='CNN']['tweets_ago']$ print(cnn_when.values)
for name, val in zip(x.columns, adult_model.feature_importances_):$     print(f'{name} importance = {100.0*val:5.2f}%')
pMean = np.mean([pNew,pOld])$ pMean
f = v.reset_index().rename(columns={'level_0':'ID'})$ f['ballot_type'] = f['vote'].str.extract('\((.*?)\)', expand=True).fillna('')$ f['vote'] = f['vote'].replace('\(.*\)', '', regex=True)
df_sum=pd.DataFrame(data=sum_row).T$ df_sum 
logit_pageCountry = sm.Logit(merged['converted'],merged[['ab_page', 'intercept', 'UK', 'US']])$ result_pageCountry = logit_pageCountry.fit()
snap_data = df_snap.copy()
news_dict_df.to_csv("Newschannel_tweets_df.csv")
eresolve = pd.Series(dfENTed.Entity.values,index=dfENTed.InText.values).to_dict()$ EEdgeDF['To'] = EEdgeDF['To'].map(eresolve)$ EEdgeDF.head(7)
dfFull['YearRemodAddNorm'] = dfFull.YearRemodAdd/dfFull.YearRemodAdd.max()$
dat['orig_T1']=dat[primary_temp_column].copy()$ primary_temp_aspirated='aspirated' in primary_temp_column.lower()
plt.bar(x_axis, np.sqrt(np.square(deltas_df['Delta Approval']*.8)), color="blue")$ plt.bar(x_axis, np.sqrt(np.square(deltas_df['Delta Compound']*2)), color="green")$ plt.show()
tfidf.fit(text)
postags = sorted(set([pos for sent in train_trees for (word, pos) in sent.leaves()]))$ print(unigram_chunker.tagger.tag(postags))
df['y'].plot.box()
df_from_csv.equals(df)
ts1 = pd.Series(np.arange(len(df)), df_Created)$ df_Month = ts1.resample('M').count()$ df_Month.head(9)
import numpy as np$ data = np.random.normal(0.0, 1.0, 1000000)$ np.testing.assert_almost_equal(np.mean(data), 0.0, decimal = 2)
openmc.plot_geometry(output=False)
df_movies.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/movies_v1.csv', sep=',', encoding='utf-8', header=True)
AFX_X_2017_dict = AFX_X_2017_r.json()
for title in soup.find_all('a'):$     print(title.text)$
active_stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()$ active_stations
mgxs_lib.by_nuclide = True
ts.shift(-2)
fraq_volume_m_sel = b_mat.as_matrix() * fraq_volume_m_coins$ fraq_fund_volume_m = fraq_volume_m_sel.sum(axis=1)$
search.groupby(['search_type','booking']).size()
X = pd.get_dummies(X, drop_first=True)
targettraffic['weekday'] = targettraffic['DATE_TIME'].apply(lambda x:x.weekday())$ targettraffic['weekdayname'] = targettraffic['DATE_TIME'].apply(lambda x:x.weekday_name)
playlist = sp.user_playlist(spotify_url.split('/')[4],spotify_url.split('/')[6])$ pd.io.json.json_normalize(playlist)
numPurchP = train.groupby(by='Product_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesP'})$ train = train.merge(numPurchP, on='Product_ID', how='left')$ test = test.merge(numPurchP, on= 'Product_ID', how='left')
accuracy = accuracy_score(y_test, predictions)$ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
%%time$ max_key = max( r_dict.keys(), key = get_nextday_chg )$ print('largest change in price between any two days: '+ str( get_nextday_chg(max_key) ) )
model.wv.syn0.shape
conn.commit()
df_search_cate_dummies[df_search_cate_dummies['user_id']== 18].index$
num_clusters = 3$ km = KMeans(n_clusters=num_clusters, random_state=0).fit(tfidf_matrix)#TODO$ clusters = km.labels_.tolist()
genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId'])$ genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ genreTable.head()
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comment', ''))
a=unique_users[unique_users.created_at.isin(['0000-00-00 00:00:00'])].index.values$ unique_users.loc[a,'created_at']='2010-11-11 03:59:09'$
columns = inspector.get_columns('measurements')$ for c in columns:$     print(c['name'], c["type"])
caption_text_clean = [review_to_words(doc, True) for doc in captions.caption]$ caption_text_clean[:10]
del(df_obj2['G'] )$ print(df_obj2.head())
res4 = rs.get('http://bsr.twse.com.tw/bshtm/bsContent.aspx', headers = headers)$
merged_data['inv_creation_day'] = merged_data['invoices_creation_date'].dt.day$ merged_data['inv_creation_month'] = merged_data['invoices_creation_date'].dt.month$ merged_data['inv_creation_year'] = merged_data['invoices_creation_date'].dt.year
result=results.set_index(['date','home_team','away_team'])$ result.head()
bands.to_csv('../data/bands.csv')
containers[0].find("li", {"class":"name"}).a['title']
df = df.drop(['mintempm', 'maxtempm'], axis=1)$ X = df[[col for col in df.columns if col != 'meantempm']]$ y = df['meantempm']
print(scores.mean())
c = (y - m*x).mean()$ print(c)
df.set_index(['operator', 'part'], inplace=True)
print('min wavelength:', np.amin(wavelengths),'nm')$ print('max wavelength:', np.amax(wavelengths),'nm')
sns.heatmap(ndvi_us, vmin = -.1, vmax=1)
fakes.comment[0]
stories = pd.concat([stories.drop(['submitter_user'], axis=1), user_df], axis=1)$ stories.head()
tips.sort_values(["tip","size"],ascending=False).head(10)
def jprint(j):$     print(json.dumps(j,indent=4,sort_keys=True))
number_of_commits = len(git_log)$ number_of_authors = len(git_log.dropna()['author'].unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
input_nodes_DF=nodes_table('network/source_input/nodes.h5', 'inputNetwork')$ input_nodes_DF[:5]
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_zh_200_wzh.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_200_wzh.w2v')
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd[:10]
request_data_2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31')
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ SA1 = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ]) #store data in its own vector $ display(data.head(10))
d = requests.post(link, json=contact_form, params={'hapikey': hubspot_write_key})$ d.status_code$
coin_data.columns
days_alive = (datetime.datetime.today() - datetime.datetime(1981, 6, 11))$ days_alive.days$ days_alive
sns.heatmap(df.corr())$ plt.show()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-01-02&api_key=" + API_KEY)
df_filtered_by_RT = df[~df.raw_text.str.startswith('RT')]$ df_filtered_by_RT.head(1)
data_AFX_X['diff_rows'] = data_AFX_X['Close'].diff(periods=-1)$ data_AFX_X.describe()$ min(data_AFX_X['diff_rows'])
locations = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\$ filter(Measurement.station==Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))#considering 95 percent confidance interval
mean_absolute_error(Y_btc_val, [Y_btc_train.mean() for i in range(Y_btc_val.shape[0])])
X_new.shape$
with pd.option_context('display.max_rows', 150):$     print(news_period_df.groupby(['news_collected_time']).size())
set(stories.columns) - set(stories.dropna(thresh=9, axis=1).columns)$
recent_date = session.query(Measures.date).order_by('Measures.date desc').first()$ recent_date = recent_date[0]$ prior_date = get_prior_years_date(recent_date,1)
df_birth.population.value_counts(dropna=False).head()
df_vow.head()
df3_holidays = df3.copy()$ df3_holidays['y'] = np.log(df3_holidays['y'])
nasa_url = 'https://mars.nasa.gov/news/'$ jup_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
response = requests.get('https://www.jconline.com/search/gun%20control/')$ soupresults2 = BeautifulSoup(response.text,'lxml')$
tweet_df_polarity = tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
precip_df = precip_df.sort_values(by = 'Date')
csvFile = open('ua.csv', 'a')
from sqlalchemy.ext.declarative import declarative_base$ from sqlalchemy import Column, Integer, String, Float 
sites2 = sites[sites['DrainageAreaMeasure/MeasureValue'] > 25]$ sites2.shape
financial_crisis.loc['Wall Street Crash / Great Depression'] = '1929 - 1932'$ print(financial_crisis)
building_pa_prc.shape
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_hat), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
from kipoi_veff.parsers import KipoiVCFParser$ vcf_reader = KipoiVCFParser("example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf")$ print(list(vcf_reader.kipoi_parsed_colnames.values()))
df_json = pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')
average_trading = statistics.mean([day[6] for day in data])$ print ('Average daily trading volume for 2017:', average_trading)
for col in X_nnumcols:$     X[col] = X[col].apply(lambda l: hash(l))$     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
sentiment_df = pd.DataFrame(results_list)$ sentiment_df
sns.boxplot(x='rating', y='text length', data=dataset)$
df.loc['1930-01-01':'1979-12-31','status'] = "Before FL"$ df.loc['1984-01-01':'2017-12-31','status'] = "After FL"$ df.sample(10)
s = df_sms.groupby(['group','ShopperID'])['ID'].count().reset_index()$ s.groupby(['ID','group']).size();
rain_score = session.query(Measurement.prcp, Measurement.date)\$                .filter(Measurement.date > past_year).\$                 order_by(Measurement.date).all()
STATION_traffic_weektotals = (SCP_ENTRY_weektotals + SCP_EXIT_weektotals).groupby(['STATION']).sum()$ STATION_traffic_weektotals.sort_values(ascending=False).head(10)
scores['test_accuracy'].mean()$
cityID = 'b71fac2ee9792cbe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Sacramento.append(tweet) 
news_p = soup.find("div", class_="rollover_description_inner").text
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 0), 'type'] = 'inside_gamers'
data.columns = ['Tweets','len', 'ID','Date', 'Source', 'Likes', 'RTs', 'SA']$ data.to_csv('experiment1.csv', index_label='Index_name')
df.printSchema()
number = 100$ factor = 1.1$ text =  "hello world"
for tweet in tweepy.Cursor(api.search, q='#GES2017',$                           lang='en',since='2017-11-28',max='2017-11-30').items(200):$     csv_writter.writerow([tweet.text.encode("utf-8")])
for i in range(3):$     dfs[i]['SA'] = np.array([analyze_sentiment(tweet) for tweet in dfs[i]['Tweets']])
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]
pure_final_df=pure_final_df.sort_values(by='מועד המשחק') $ pure_final_df=pure_final_df._get_numeric_data()$
for model_name in nbsvm_models.keys():$     test_probs[model_name] = nbsvm_models[model_name].predict_proba(test_cont_doc)
breast_cancer_df = pd.read_csv('data/breast_cancer.csv')$ breast_cancer_df.head()
s4g.groupby('Symbol')
desc_stats = desc_stats.transpose().unstack(level=0).transpose()$ desc_stats
engine.execute("SELECT count(station), station FROM measurement GROUP BY station ORDER BY count(station) DESC").fetchall()
grouped_df = news_sentiment_df.groupby('News_Source')$ grouped_compound = grouped_df['compound'].mean()$ grouped_compound
states = pd.DataFrame({'population': population,$                        'area': area}   )$ states
hi_day_delta_index = day_deltas.index(max(delta_val for delta_val in day_deltas if delta_val is not None))$ ans_4 = ('%s had greatest within day difference: %s' % (dates[hi_day_delta_index], day_deltas[hi_day_delta_index]))
combined_data = pd.concat([data, expanded_data], axis=1)$ combined_data = combined_data.drop('_source', axis=1)
feat = ['categoryname', 'eventname', 'location']$ for f in feat:$     PYR[f] = PYR[f].apply(clean_dataset)
X_copy['score_edi_instability_avg'] = X_copy['score_edi_instability_avg'].apply(lambda x: float(x))
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLAday.csv"$ tesla_days = pd.read_csv(url, index_col=0, parse_dates=True)$ tesla_days
teama_merge = my_elo_df.merge(final_elo, how='left', left_on=['Date', 'Team A'], right_on=['Date', 'Team'])$ teama_merge[teama_merge['Team A'] == 'Cloud9'].tail(7)
num_benfords = 10$ benfords = [np.log10(1+1/i) for i in range(1, num_benfords + 1)]$ x_ben = [x for x in range(0, num_benfords)]
faa_data_substantial_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "S"]$ print(faa_data_substantial_damage_pandas.shape)$ faa_data_substantial_damage_pandas.head()
ldamodel.print_topics()
en_translation_counts = en_es.groupby(by='en').size()$ en_translation_counts[en_translation_counts > 1].hist(bins=10)
sq83= "CREATE TEMPORARY TABLE  newtable_22222 ( SELECT * FROM Facebook_NBA order by 0.2*likes+0.4*Comments+0.4*shares DESC limit 150)"$ sq84="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Id, SUBSTRING_INDEX(SUBSTRING_INDEX(message,' ',i+1),' ',-1) word FROM newtable_22222, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 0 ORDER BY total DESC, word;"
input_edge_types_file  = input_directory_name + 'input_edge_types.csv'$ Ext_input.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=input_directory_name)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2017-11-01&end_date=2017-11-02')
df = pd.read_json('vacs_2018-03-04.json')
idx = pd.IndexSlice$ health_data_row.loc[idx[:, :, 'Bob'], :]  # very close to the naive implementation
r.encoding
from keras.models import load_model$ y_pred = model.predict_classes(val_x)$ y_true = val_y
df7['avg_health_index Closed'].value_counts(dropna=False)
df = pd.DataFrame()$ df
Z = np.random.randint(0,3,(3,10))$ print((~Z.any(axis=0)).any())
prediction_df = pd.DataFrame(predictions, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"])$ combined_df = comment_df.join(prediction_df) # join the comment dataframe with the results dataframe$ combined_df.head(15)
tree_features_df['p_hash'].isin(manager.image_df['p_hash']).describe()$
plt.title('Burberry ngram', fontsize=18)$ burberry_ng.plot(kind='barh', figsize=(20,16));$ plt.savefig('../visuals/Burberry_ngram.jpg')
free_data.head(10)
gs.score(X_test_all, y_test)
sentiments_pd.count()
store.delete_collection('NASDAQ.EOD')
Stations = Base.classes.Stations$ Measurements = Base.classes.Measurements
pd.read_sql(q, connection) # Avergae working hours of Private Sector Men
cityID = 'af2a75dbeb10500'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lincoln.append(tweet) 
print(df.columns.values)
import pprint$ summary = learn.summary()$ print(str(pprint.pformat(summary))[:1000])
df2[df2.duplicated(['user_id'])].user_id
df_kosdaq = pd.DataFrame({'id' : kosdaq_id_ls, 'name' : kosdaq_name_ls, 'market_type' : 2}, columns = ['id', 'name','market_type', 'quant', 'market_sum', 'property_total', 'debt_total', 'listed_stock_cnt', 'pbr', 'face_value',])
payments_NOT_common_all_yrs = (df_sites_NOT_common_DRGs.groupby(['id_num','year'])[['disc_times_pay']].sum())$ payments_NOT_common_all_yrs = payments_NOT_common_all_yrs.reset_index()$ payments_NOT_common_all_yrs.head()
data = requests.get ("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key="+API_KEY ).json()
pop_cat = timecat_df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ pop_cat.head()
df_raw_tweet = pd.read_csv('./Datasets/Twitter_Training_Data2.csv', encoding='latin1')$ print (df_raw_tweet.head())
tl_2020 = pd.read_csv('input/data/trans_2020_ls.csv', encoding='utf8', index_col=0)
data = data.set_index('time')
SelectedOpenClose = AAPL.iloc[200:210, [1,3]]$ SelectedOpenClose
S_lumpedTopmodel.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df =sentiments_df[['Date','Compound','Count']]$ sentiments_df.tail()
data_kb = pd.DataFrame(data=[tweet.text for tweet in tweets_kb], columns=['Tweets'])$ display(data_kb.head(10))
dc['created_at'].hist(color="blue") # blue, David Cameron$ tm['created_at'].hist(color="orange") # orange, Theresa May
! head -n 5 ../../data/msft.csv # OS/Linux$
mean = np.mean(data['len'])$ print("The length's average in tweets: {}".format(mean))
%matplotlib inline$ AAPL.plot()
eth = pd.read_csv("Ether-chart.csv", sep=',')$ eth['date'] = ' '$ eth.info()$
weekly_tagsVideosGB = weekly_dataframe(TagsVideosGB)$ weekly_tagsVideosGB[0].head()
volt_prof_before['Bus']=volt_prof_before['Bus'].apply(lambda x:x.lower())$ volt_prof_after['Bus'] =volt_prof_after['Bus'].apply(lambda x:x.lower())
joined=join_df(joined,googletrend,["State","Year","Week"])$ joined_test=join_df(joined_test,googletrend,["State","Year","Week"])$ sum(joined['trend'].isnull()),sum(joined_test['trend'].isnull())
df_combined['won'] = (df_combined['home_score'] - df_combined['away_score']) > 1$ df_combined.drop(['date', 'city', 'country','home_score','away_score'], axis=1, inplace=True)
vocab = vectorizer.get_feature_names()$ print(vocab)
pm_data.dropna(inplace=True)$ pm_data.status.value_counts()
final_test_pred_nbsvm1 = test_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
g = sns.catplot(x="AGE_groups", col="goodbad",$                  data=data, kind="count",$                  height=5, aspect=.9);
df_log.user_id.nunique()
measurements_df.count()
np.all(x < 10)
scores = raw_scores.value_counts().sort_index()$ scores
haw.to_csv('./haw_python.csv',index=True)
who_purchased.to_csv('../data/purchase.csv')
buyer=patch.groupby(['buyer_name'])$ repeat=buyer.count()
nb = MultinomialNB()$ nb.fit(X_train_total, y_train)$ nb.score(X_test_total_checked, y_test)
df_cat = pd.get_dummies(data[catFeatures])
adj_close.head()
DataSet[['userName','userFollowerCt','tweetRetweetCt']].sort_values('tweetRetweetCt',ascending=False).head(10)
cand.CAND_ST.value_counts()
contractor_merge['contractor_bus_name'].head()$
engine = create_engine("sqlite:///hawaii.sqlite")
logger = logging.getLogger()$ logger.setLevel(logging.INFO)  # SET YOUR LOGGING LEVEL HERE #
df2 = pd.DataFrame(df.groupby("media source").mean()['compound score'])$
pd.set_option('display.max_colwidth', -1)$ df.loc[df.Sentiment==1, ['description','Sentiment']].head(10)
btc['2017-09-12':'2017-09-22'].plot(y='price')$ plt.show()
from pandas.plotting import scatter_matrix$ axes = scatter_matrix(data.loc[:, "TMAX":"TMIN"])
store_items.fillna(method = 'backfill', axis = 1)
session.query(Measurement.tobs).order_by(Measurement.tobs).first()
all_tables_df.iloc[:, 1]
times_zone = pd.DataFrame(df_table['createdOnTimeZone'][771:797])$ times_created = pd.DataFrame(df_table['createdOn'][771:797])$ appV = pd.DataFrame(df_table['appVersion'][771:797])
rootDistExp = Plotting(S.setting_path.filepath+S.para_trial.value)
tweets['location'] = tweets['location'].str.strip()$ tweets.groupby(tweets.location).count()['id'].sort_values(ascending=False)$
stops_heatmap = folium.Map(location=[39.0836, -77.1483], zoom_start=11)$ stops_heatmap.add_child(heatmap_full)$
Lowest_opening_price = mydata['Open'].min()$ Lowest_opening_price
s = pd.Series([1,3,5,np.nan,6,8])$ print(s)
df.iloc[99:104]
groupby_x = df['y'].groupby(df['x'])$ round(groupby_x.describe(), 3)
coarse_groups = openmc.mgxs.EnergyGroups(group_edges=[0., 20.0e6])$ coarse_mgxs_lib = mgxs_lib.get_condensed_library(coarse_groups)
price=data.json()
type2017.head()
crimes_by_yr_month = pd.DataFrame(datAll.groupby([datAll['year'],datAll['month']])$                                .agg({'Offense_count':'sum'}))
np.shape(prec_fine)
import sys$ src_dir = os.path.join('..', 'src')$ sys.path.append(src_dir)
df_page1 = len(df2.query("landing_page == 'new_page'")) / df2.shape[0]$ print("{} is the probability that an individual received the new page.".format(df_page1))
newfile = newfile[~newfile['Reason for rejection'].isin({'X1', 'X8'})]
model.load_weights('best.hdf5')
temp=pd.read_json("./jsondata/condensed_2017.json")$ temp.head()
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Therapist Hours Autocorrelation')
!curl -L -O  https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv$ !head -n 1 Consumer_Complaints.csv > CC_header.csv$ !tail -n +2 Consumer_Complaints.csv > CC_records.csv
a=DataAPI.write.update_secs_industry_sw(industry="H_SWL1", trading_days=['2017-12-29'], override=False)
qw = qgrid.show_grid(all_tables_df, show_toolbar=True)
import pandas as pd$ users_df = pd.read_csv('data/{country}_users.csv'.format(country=country).replace(' ', '+'))$ users_df.sample(5)
data = TwitterData_Initialize()$ data.initialize("data/train.txt", is_spain = False)$ data.processed_data.head(10)
chefdf = chefdf.dropna()
df.loc[monthMask, 'water_year'] = df['year'] + 1
for post in posts.find({"reinsurer": "AIG"}):$     pprint.pprint(post)
df['fetched time'] = df['fetched time'].astype('datetime64[s]')$ df['created_utc'] = df['created_utc'].astype('datetime64[s]')
data[(data['author_flair'] == 'Bears') & (data['win_differential'] >= 0.9)].comment_body.head(15)$
last_year = today.year + 1$ years = list(range(join_date.year, last_year))$ years
from bmtk.analyzer import nodes_table$ nodes_table('network/recurrent_network/nodes.h5', 'Cortical')
responses_df = pd.read_json('./data/Responses2.json', lines=True)$ print(len(responses_df))$ responses_df.head()
pd.options.display.max_colwidth = 100$ data_df[data_df.nwords == 1]['clean_desc'].head(15)
search['trip_duration'] = (search['trip_end_date'] - search['trip_start_date']).dt.days
version = str(int(time.time()))
exiftool -csv -createdate -modifydate cisuabn14/cisuabn14_cycle2.MP4 cisuabn14/cisuabn14_cycle3.MP4 cisuabn14/cisuabn14_cycle4.MP4 cisuabn14/cisuabn14_cycle5.MP4 cisuabn14/cisuabn14_cycle6_part1.MP4 cisuabn14/cisuabn14_cycle6_part2.MP4 > cisuabn14.csv
tweet_pickle_path = r'data/twitter_01_20_17_to_3-2-18.pickle'$ tweet_data.to_pickle(tweet_pickle_path)
typesub2017['MTU2'] = typesub2017.MTU.str[:16]$ typesub2017['DateTime'] = pd.to_datetime(typesub2017['MTU2'])
df_merge['per_student_budget'] = df_merge['budget'] / df_merge['size']$ df_merge.head()
df.boxplot('MeanFlow_cms',by='status');
print(f"lowest_temp = {min(tobs_data)} \$         highest_temp = {max(tobs_data)}\$         avg_temp_of_most_active_station = {np.mean(tobs_data)}")
data.groupby(['Year'])['Salary'].sum()$ data.groupby(['Year'])['Salary'].mean()
from bmtk.analyzer import nodes_table$ nodes_table('network/recurrent_network/nodes.h5', 'V1')
from IPython.display import display$ pd.options.display.max_columns = None
plt.hist(y_res)$ plt.hist(y)$ plt.show()
df.replace('\n', ' ', inplace=True, regex=True)$ df.lic_date = pd.to_datetime(df.lic_date, errors='coerce')$ df = df.sort_values('lic_date', ascending=False)
print(df.Col_1)
AAPL.info()
values = [4, 56, 2, 45.6, np.nan, 23] # np.nan returns a null object (Not a Number)$ s = pd.Series(values)$ s
df3[['inv', 'ab_page']] = pd.get_dummies(df3['group'])$ df3.tail()$
df.count()
access_logs_raw.count()
df_20180113_filtered.to_csv('data/hawaii_missile_crisis-20180113.csv')
cols = ['GP', 'GS', 'MINS', 'G', 'A', 'SHTS', 'SOG', 'GWG', 'HmG', 'RdG', \$         'G/90min', 'SC%', 'Year', 'PKG', 'PKA']$ goals_df[cols] = goals_df[cols].apply(pd.to_numeric)
station_data = session.query(Stations).first()$ station_data.__dict__
iris.loc[iris["Species"].isin(["setosa","versicolor"]),:].sample(10)
crimes.groupby(['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']).size()
S_1dRichards.basin_par.filename
df3=df3.sort_values(by=["Net_Value_item_level"],ascending=[False])$ len(df3[df3.Net_Value_item_level==0])
count_pages=df2.groupby(['landing_page']).size()# this gives the count of both new and old pafes together$ n_new=count_pages[0]# this gives the count for the new pages$ n_new
data.Likes.value_counts(normalize=True).head()
df.shape
! rm -rf models1$ ! mrec_train -n4 --input_format tsv --train "splits1/u.data.train.*" --outdir models1 --model=popularity
stations = Base.classes.stations$ stations
ctd = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with ctd.open('inpt-forecasting/Census Tool Data Pull CY2017- May 2018CONFIDENTIAL.xls.xlsx') as ctd_f:$   ctd_df = pd.read_excel(ctd_f)
aapl = pd.read_csv(file_name, index_col='Date')$ aapl
df.drop(df.query("group =='treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group =='control' and landing_page == 'new_page'").index, inplace=True)$
mbti_text_collection.to_csv('Reddit_mbti_data_2.csv',encoding='utf-8')
np.shape(temp_us_full)
numPurchU = train.groupby(by='User_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesU'})$ train = train.merge(numPurchU, on='User_ID', how='left')$ test = test.merge(numPurchU, on= 'User_ID', how='left')
datetime.now().date()
tweets.sort_values(by="retweets", ascending=False).head()
expx=np.mean(x)$ expy=np.mean(y)$ expx, expy
Base.prepare(engine, reflect=True)$ 
full_image_elem = browser.find_by_id('full_image')$ full_image_elem.click()
!hdfs dfs -cat {HDFS_DIR}/p32a-output/part-0000* > p32a_results.txt
gs_lr_tfidf.fit(X_train, y_train) 
total.load_from_statepoint(sp)$ absorption.load_from_statepoint(sp)$ scattering.load_from_statepoint(sp)
random_integers.max(axis=1)
print('RF: {}'.format(rf.score(X_test, y_test)))$ print('KNN: {}'.format(knn.score(X_test, y_test)))
ctc = ctc.round(1)
quandl.ApiConfig.api_key = 'gTmSNjbQR-8Q5U9pukHX'$ quandl.get('BITFINEX/BTCEUR',collapse="monthly")$
set_option("display.max_colwidth",280)$
data.drop(['ceil_15min'], axis=1,inplace=True)
temp_df = df.copy()$ temp_df.index = df.index.set_names('Desc.', level='Description')$ temp_df.head(3)
saved = savedict.copy()$ ex = saved['2017-11-15']$ print("Choose Campaign Name: ",ex['Campaign Name'].unique())$
pd.read_csv("data.csv", index_col=[0, 1, 2, 3, 4, 5], skipinitialspace=True, parse_dates=['Date']).head(3)
data.head()$
len(cats_out['Animal ID'].unique())
USvideos.info()
gbm_predictions = pd.concat([pd.Series(gbm_pred, index=y_test.index, name='Predictions'), y_test], axis=1)$ gbm_predictions.head()
file_cap = pyshark.FileCapture('captures/botnet-sample.pcap')
tickerdf = tickerdf.dropna(axis=0,how='any')
round(max(multi.handle.value_counts(normalize=True)),4)
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ last_date
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
news_organizations_df['tweets'] = news_organizations_df['tweets'].map(normalize_df)
df_cs.head(2)
page_size = 200$ response = client.get('/tracks', q='footwork', limit=page_size,$                     linked_partitioning=1)$
req_test.text
ARStores = $ ARStores.head()
saveTweetDataToCSV(input_hash_tag.value)
pax_raw = pax_raw.merge(n_user_days, on='seqn', how='inner')
data = pd.DataFrame(data=[tweet.text for tweet in searched_tweets], columns=['Tweets'])$ display(data.tail(10))
airquality_pivot = airquality_pivot.reset_index()$ print(airquality_pivot.index)
df.set_index('Parameter', inplace=True)$ df.head()
from sodapy import Socrata$ client = Socrata("data.cityofnewyork.us", os.getenv("apptoken"))
groupby_location = df1['location'].groupby($     df1['location']).count().sort_values(ascending=False)$ print(groupby_location.head())$
non_na_df = df.dropna()
df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopMcMrNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TMcM_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))$ df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopPalNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TPal_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))$
ps = pst.to_period()$ ps
df = pd.read_sql(SQL, db)$ df
pd.Series([1,2,9])
twelve_months_prcp.head()
df_dn.to_csv('data/DayorNight.csv', date_format="%d/%m/%Y %H:%M:%S",index=False)
title_sum = preproc_titles.sum(axis=0)$ title_counts_per_word = list(zip(pipe_cv.get_feature_names(), title_sum.A1))$ sorted(title_counts_per_word, key=lambda t: t[1], reverse=True)[:]$
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period()$ print("Trend for month: ", get_trend(get_timeseries(closed_pr)))
from bs4 import BeautifulSoup as soup ##BeautifulSoup$$ from urllib.request import urlopen as uReq$ import requests
mlp_fp = 'data/richmond_median_list_prices.csv'$ mlp_df = pd.read_csv(mlp_fp)$ mlp_df.head()
law = tc_final$ law['YBP sub-account'].replace("195099", "590099", inplace= True)$ law
stories = pd.read_json('https://lobste.rs/hottest.json')$ stories.head()
data.info()
forecast_column = 'Adj. Close'$ df.fillna(value=-99999, inplace=True)$ forecast_out = int(math.ceil(len(df)*0.01))
active_users.shape # there are only 7479 active users out of 22884 signed users$
for c, f in zip(tips_model.coef_[0], features.columns.values):$     print(f'{c:5.2f} * {f}')
joined=join_df(train,store,"Store")$ joined_test=join_df(test,store,"Store")$ sum(joined['State'].isnull()),sum(joined_test['State'].isnull())
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ data.head(10)
cursor = db.TweetDetils.aggregate([ {"$group" : {"_id":"$user_name", "score":{"$sum":"$retweet_cnt"}}},  {"$sort":{"score" : -1}},{"$limit":5}])$ for rec in cursor:$     print(rec["_id"], rec["score"])
data_final['authorId'].nunique()
    if isinstance(obj, (datetime, date)):$         return obj.isoformat()$     raise TypeError ("Type %s not serializable" % type(obj))
last_date = dt.date(2017, 8, 23)$ last_date
from sklearn.model_selection import cross_val_score$ from sklearn.ensemble import GradientBoostingClassifier$ forest_clf.score(X_test, Y_test) # test socre$
compared_resuts = ka.predict(test_data, results, 'Logit')$ compared_resuts = Series(compared_resuts)  # convert our model to a series for easy output
!wget https://raw.githubusercontent.com/sunilmallya/mxnet-notebooks/master/python/tutorials/data/p2-east-1b.csv
DataSet = DataSet[DataSet.tweetSource.notnull()]$ len(DataSet)
w.index.freq, b.index.freq, r.index.freq
arr_size = reflClean.shape$ arr_size
validation.analysis(observation_data, simple_resistance_simulation_1)
pd.date_range('1/1/2017', '12/1/2017', freq='BM')$
print('RF:', rf.score(X_test, y_test))$ print('KNN:', knn.score(X_test, y_test))
table_rows = driver.find_elements_by_tag_name("tbody")[12].find_elements_by_tag_name("tr")$
plt.rcParams['figure.figsize'] = [18.0, 10.0]
tfav.plot(figsize=(16,4), label="Likes", legend=True);$ tret.plot(figsize=(16,4), label="Retweets", legend=True)
df2.loc['2016-09-18', ['GrossIn', 'NetIn']]$
import warnings$ warnings.filterwarnings('ignore')
active_with_return.iloc[:,1] = pd.to_datetime(active_with_return.iloc[:,1])
dfLikes["periodo"] = dfLikes["created_time"].apply(lambda d: datetime.strptime(d, '%Y-%m-%dT%H:%M:%S+%f').strftime('%Y%m'))
pd.merge(df1,df2, on=['HPI','Int_rate','US_GDP_Thousands']) # merge on list and on various columns$ 
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)$ os.chdir(str(today))$ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)$
df['state'] = df['state'].str.capitalize()$ df.groupby('state')['ID'].count()
cityID = '04cb31bae3b3af93'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Miami.append(tweet) 
import matplotlib.cm as cm$ proudlove_c, perry_c, all_c, regression_c, *_ = cm.Paired.colors
Base.classes.keys()
pokemon.drop(["id"],inplace=True,axis=1)$ pokemon.drop(["Type 2"],inplace=True,axis=1)$ pokemon.head()
df_corr = result.groupby(['type', 'scope'])['site'].sum().reset_index()$ display(df_corr.sort_values('site',ascending=False).head(10))$ plot2D(df_corr, 'type', 'scope','site')
print('Best Score: {:.3f}'.format(XGBClassifier.best_score))$ print('Best Iteration: {}'.format(XGBClassifier.best_iteration))
df['HIGH_LOW']=(df['num_comments']>df['num_comments'].median()).astype(int)$ print(df.shape)$ df.head()
import datetime as dt$ mydata = dc2015.copy()$ mydata['new'] = np.where((mydata['created_at'].dt.time >= '0:00') & (mydata['created_at'].dt.time < '12:00'), 'morning', 'evening')
!grep -A 15 "add_engineered(" taxifare/trainer/model.py
INSERT INTO payment (booking_id, payment_date, payment_method, payment_amount)$ VALUES (8, TO_DATE('2017-09-10', 'YYYY-MM-DD'), 'BPay', -741.96)$
x_minor_ticks = 10 # Note that this doesn't work for datetime axes.$ y_minor_ticks = 10
sns.set_style('whitegrid')$ sns.set_context("talk", font_scale=1.5, rc={"lines.linewidth": 2.5})
segs = ga3.management().segments().list().execute()$ df = pd.DataFrame([x for x in segs['items']])$ df.sort_values("created", ascending=False).head(5)
df.corr().round(2)$
print(df_sentiments)$ df_sentiments = df_sentiments['score'].apply(pandas.to_numeric, errors='ignore')$ df_sentiments_means = df_sentiments.transpose().apply(numpy.mean, axis=1)$
mlp_pc = mlp_df.pct_change()$ mlp_pc.head()
base_df_body.head()
station_data = session.query(Stations).first()$ station_data.__dict__
print data_df.clean_desc[26]
url = 'https://www.reddit.com/r/Python/'
from bmtk.analyzer import nodes_table$ input_nodes_DF = nodes_table(nodes_file, 'FridayHarborBiophysics')$ input_nodes_DF[:5]
g8_aggregates.columns = ['_'.join(col) for col in g8_aggregates.columns]$ g8_aggregates
for c in ccc:$     vwg[c] /= vwg[c].max()
min_div_stock=df.iloc[df["Dividend Yield"].idxmin()]$ min_div_stock$ print("The stock with the minimum dividend yield is %s with yield %s" % (min_div_stock['Company Name'],min_div_stock['Dividend Yield']))
stero_grid_file = E.obs['NSIDC_0051']['grid']$ obs_grid = import_data.load_grid_info(stero_grid_file, model='NSIDC')$ obs_grid['lat_b'] = obs_grid.lat_b.where(obs_grid.lat_b < 90, other = 90)
merged_portfolio_sp_latest_YTD = pd.merge(merged_portfolio_sp_latest, adj_close_start, on='Ticker')$ merged_portfolio_sp_latest_YTD.head()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ session = Session(engine)$
dummy = pd.get_dummies(furniture, columns=['category'],drop_first=True)
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv', nrows=10)
pook_bytes = io.BytesIO(pook_dl.content)$ print(readPDF(pook_bytes)[:1000])
df_subset['diff'] = df_subset.apply(diff_money, axis = 1, pattern = pattern)$ print(df_subset.head())$ 
from time import mktime$ current_time.timetuple()$ mktime(current_time.timetuple())
top_10_authors = pd.value_counts(git_log['author'])$ top_10_authors = top_10_authors.head(10)$ print(top_10_authors)
DataSet['tweetSource'].value_counts()[:5]$
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
df_raw[df_raw.list_date.isnull()]
data['Open'].max()
ADNI_merge = pd.read_csv('ADNIMERGE.csv')$ ADNI_merge.head()
rf.score(X_train_total, y_train)
len([premiePair for premiePair in BDAY_PAIR_qthis.pair_age if premiePair < 0])/BDAY_PAIR_qthis.pair_age.count()
chinese_vessels.isnull().sum()
df3.describe()
(p_diffs>p_diff_abdata).mean()
emails_dataframe['address'].str.split("@")
for urlTuple in otherPAgeURLS[:3]:$     contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ contentParagraphsDF.head()
regression_estimation(X,final_df['mean'])$ classification_estimation(X,final_df['mean'].map(lambda x: x*10))$
print(df['Borough'].value_counts(dropna=False))
print(response.text)
sns.set(color_codes=True)$ sns.distplot(utility_patents_subset_df.number_of_claims, bins=40, kde=False)$ plt.show()
style.use('ggplot')
random.sample(labels.items(), 25)
s.index[[2,3]]
df4.describe() # basic stats all from one method$
avisos_online.head(1)$
empDf.createOrReplaceTempView("employees")$ SpSession.sql("select * from employees where salary > 4000").show()
psy_hx = psy_hx.dropna(axis=1, how='all')$
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
def get_duration_career(input_):$     return max(input_) - min(input_)$ grouped_publications_by_author['duration_career'] = grouped_publications_by_author['publicationDates'].apply(get_duration_career)$
df.drop_duplicates(subset=['first_name', 'last_name'], keep='last')$
workspace_data = w.data_handler.get_all_column_data_df()$ lst = workspace_data.WATER_BODY_NAME.unique()$ print('WATER_BODY_NAME in workspace:\n{}'.format('\n'.join(lst)))
distance = pd.Series(distance_list)
events_enriched_df['topic_name'].drop_duplicates().count()
top_songs.rename(columns={'Region': 'Country'}, inplace=True)
x_mean, up, low = bollinger_band(DataFrame(values), 20, 1.5)
df.to_csv('Tableau-CitiBike/TripData_2018_Winter.csv', index=False)
df_null_acct_name = df[df['LinkedAccountName'].isnull()]
cur.execute("SELECT name FROM sqlite_master WHERE type='table';")$ print(cur.fetchall())
pd.DataFrame(dummy_var["_Source"][Company_Name]['Close']['Forecast'])[-6:]$
session.query(Station).count()$
reorder_customers = np.fromiter(result.keys(), dtype=int)$ reorder_customers.size
df['MeanFlow_mps'] = df['MeanFlow_cfs'] * 0.028316847
%matplotlib inline$ word_freqs.plot(30)
res = requests.get('http://elasticsearch:9200')$ r=json.loads(res.content)$ r
groupby_user = df1['user'].groupby(df1['user']).count()$ groupby_user.describe()$
lr = LogisticRegressionCV(n_jobs=3)$ lr.fit(X_train[['avg_shifted_against', 'def_shift_pct']], y_train)
S_1dRichards.decision_obj.bcLowrSoiH.options, S_1dRichards.decision_obj.bcLowrSoiH.value
%%time$ max_key = max( r_dict.keys(), key = get_daily_chg )$ print('largest change in price in any one day: '+ str( get_daily_chg(max_key) ) )
merged = merged.set_index('DateTime')
y = x.loc[:,"A"]$ y.mean()$ np.mean(y)
data['new_claps'] = buckets$ data.head()
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)
colmns=['category', 'launched_year', 'launched_quarter', 'goal_cat_perc', 'participants']$ ks_particpants.columns=colmns
soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
reg_target_encoding(train,'Block',"any_spot") $ reg_target_encoding(train,'DOW',"Real.Spots") $ reg_target_encoding(train,'hour',"Real.Spots") $
import pickle$ with open('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.pckl', 'rb') as f:$     my_saved_gempro = pickle.load(f)
print(dfd.in_pwr_47F_min.describe())$ dfd.in_pwr_47F_min.hist()
pd.set_option('max_colwidth',150)$ df_en.head()
forecast = prophet_model.predict(future_dates)
import pandas as pd$ bild = pd.io.json.json_normalize(data=bild)$ spon = pd.io.json.json_normalize(data=spon)
from h2o.estimators.deeplearning import H2ODeepLearningEstimator$ help(h2o.model.model_base.ModelBase)
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))$
ave_sentiment_by_company_df = pd.DataFrame(ave_sentiment_by_company)$ ave_sentiment_by_company_df
print(train["comment_text"][0])$ print(example1.get_text())$
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
df_students.columns
author_commits = git_log.groupby('author').count().sort_values(by='timestamp', ascending=False)$ top_10_authors = author_commits.head(10)$ top_10_authors
spark.sql('create database if not exists my_analysis_work')$ spark.sql('drop table if exists my_analysis_work.example_timeseries')$ joined_patient.write.saveAsTable('my_analysis_work.example_timeseries')
faa_data_destroyed_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "D"]$ print(faa_data_destroyed_damage_pandas.shape)$ faa_data_destroyed_damage_pandas
S_lumpedTopmodel.initial_cond.filename
df = pd.read_csv('ZILLOW-Z49445_ZRISFRR.csv',index_col=0)$ df.columns=['Price'] # Changing the name of the column. (Index is not treated as a column so in our df we have only 1 column)$ df.head()
mean_length = np.mean([len(i) for i in data.text])$ print("The lenght's average in tweets: %.2f" % mean_length)
pattern = re.compile(' +')$ print(pattern.split('AA   bc'))$ print(pattern.split('bcA A'))
customers_arr = np.array(cust_list)$ items_arr = np.array(item_list)
last_entry_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ last_entry_date
free_data.index.name = 'id'$ free_data.head(2)
from scrapy.selector import Selector
results=session.query(func.count(Stations.Index)).all()$ station_count=results[0][0]$ print("Count of Stations: %d"%station_count)
sortHighThenVol = AAPL.sort_values(by=["high", "volume"], ascending=False)$ sortHighThenVol.head()
questions = questions.drop(questions.index[[9,22]])
y_train = train.click$ train = train.drop(columns=['click', 'id'])$
betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality$ nx.set_node_attributes(G, betweenness_dict, 'betweenness')$
from sklearn.ensemble import RandomForestClassifier
file = 'data/pickled/Emoticon_NB4/full_emoji_dict.obj'$ emoji_dict = gu.read_pickle_obj(file)
author_email = 'xxx@ibm.com'
new_stops.loc[new_stops['stopid'] == '7567']
plt.style.use('seaborn-darkgrid')$ bb.plot(y='close')
db.collection_names(include_system_collections=False)
mlp_df.index
all_tweets = df_Tesla['text'].values$ print(all_tweets[2])
Base = automap_base()$ Base.prepare(engine, reflect=True)$
df_2016['bank_name'] = df_2016.bank_name.str.split(",").str[0]$
automl.fit(X_train, y_train, dataset_name='psy_prepro')
reflRaw = refl['Reflectance_Data'].value$ reflRaw
df_data=pd.DataFrame({'time':(times+utcoffset).value[:-sa],'chips':final.chips.values})$
print("State space samples:")$ print(np.array([env.observation_space.sample() for i in range(10)]))
df.head(5)
from sklearn.model_selection import train_test_split$ tips_train, tips_test = train_test_split(tips, test_size=0.2, random_state=123)$ tips_train.shape, tips_test.shape, tips.shape
df[df['bmi']< 18.5] # this is the way to select data, by using filters. $
number_of_commits = git_log.timestamp.count()$ number_of_authors = git_log.author.value_counts(dropna=True).count()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
print(pd.isnull(dfx))
print df.shape$ ab_counts = df.groupby('ab_test_group').count().reset_index()$ ab_counts$
crypto_data.tail(5)
df.isnull().sum()$
news_period_df.loc[3, 'news_title']
%timeit articles['tokens'] = articles['content'].map(nltk.tokenize.RegexpTokenizer('[A-Za-z]+').tokenize)
auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)$ auth.set_access_token(acc_token, acc_secret)$ api = tweepy.API(auth)
tripduration_minutes = pd.Series(minutes_list)
pd.Series({2:'a', 1:'b', 3:'c'})
ndvi_us = ndvi_nc.variables['NDVI'][0, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(ndvi_us)
df_new.head()
fb_data = graph.get_object(id='DonutologyKC/', fields=req_fields)$ type(fb_data)
from h2o.estimators.random_forest import H2ORandomForestEstimator
display(data.head(20))
with open('hashtags/hashtags.csv', 'w') as f:$     [f.write('{0},{1}\n'.format(tag, val)) for tag, val in tag_cloud.items()]
from sklearn.dummy import DummyClassifier$ dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X, y)$ y_predict_dummy = dummy_majority.fit(X, y)
list = [1,3,4,30]$ list.append(21)$ print(list)
apple.resample('M').mean()
columns = inspector.get_columns('measurements')$ for c in columns:$     print(c['name'], c["type"])$
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1&api_key='+API_KEY$ r = requests.get(url)$
pax_raw[pax_raw.paxstep > step_threshold].tail(20)
t = pd.read_hdf('../t.hdf','table')$ dd = pd.read_hdf('../dd.hdf','table')
twitter_page = requests.get("https://twitter.com/marswxreport?lang=en")$ twitter_soup = BeautifulSoup(twitter_page.content, 'html.parser')$ mars_weather = twitter_soup.find('p', class_="tweet-text").string
import pandas as pd$ df = pd.read_csv(datafile)$ df.head(5)
INQ2016.Create_Date.dt.month.value_counts().sort_index()
df.groupby("PredictedIntent").agg({'Notes':'count'})
import os, os.path$ def filecount(dir_name):$     return len([f for f in os.listdir(dir_name) if os.path.isfile(dir_name +f)])
dfLikes["año"] = dfLikes["created_time"].apply(lambda d: datetime.strptime(d, '%Y-%m-%dT%H:%M:%S+%f').strftime('%Y'))
df1 = pd.read_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext_all.csv", encoding='ANSI', $                  index_col='tweet_id', $                  sep=',')$
h3 = qb.History(spy.Symbol, datetime(2014,1,1), datetime.now(), Resolution.Daily)
%matplotlib inline$ commits_per_year.plot(kind='line', title='History of Linux', legend=False)
mergde_data.loc[mergde_data['max']==0]
partition = community.best_partition(G)$ print('Modularity: ', community.modularity(partition, G))
df_inventory_santaclara['Year']=(df_inventory_santaclara['Date'].str.split('-').str[0])
logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']])$ results = logit_mod.fit()
dict_r = r.json()$ type(dict_r)
svm_classifier.score(X_test, Y_test)
store_items.interpolate(method='linear', axis=0)
pd.date_range('1/1/2016', '12/1/2016', freq='BM')
df_date_precipitation=pd.DataFrame(date_precipitation, columns=['date','precipitation'])$ df_date_precipitation.set_index('date', inplace=True)$ df_date_precipitation.head()
urbanData_df = data_df[data_df.type == "Urban"]$ suburbanData_df = data_df[data_df.type == "Suburban"]$ ruralData_df = data_df[data_df.type == "Rural"]
df.iloc[]
print(max(sessions.groupby('id').title.nunique()))
ts / ts.shift(1)
combined_df.to_csv('manual_comment_results.csv', index=False)
plot_autocorrelation(series=RNPA_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of RNPA hours new patients'))$ plot_autocorrelation(series=RNPA_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of RNPA hours new patients'))
amznnews_matrix = count_vectorizer.transform(df_amznnews.headline_text)$ amznnews_pred = nb_classifier.predict(amznnews_matrix)
driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a').click()
svc.score(X_tfidf_test, y_tfidf_test)
df.dropna(subset=['Specialty'], how='all', inplace=True)
tag_df = tag_df.sum(level=0)$ tag_df.head()
candidate_data['messages'] = data.groupby('from_name')['message'].apply(' '.join)
df = pd.read_csv('Reddit06022018.csv',index_col ='Unnamed: 0' , engine='python')
from pyspark.streaming import StreamingContext$  $ streamContext = StreamingContext(SpContext,3) # micro batch size is 3 seconds here . It will receive an RDD every 3 seconds
x=[i for i in range(11)]$ x$ network_simulation[network_simulation.generations.isin(x)]$
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print("z-score:", z_score,$      "\np-value:", p_value)
ad_source = ad_source.drop(['[', ']'], axis=1)$ ad_source = ad_source.drop(ad_source.columns[0], axis=1)
sentiments_pd["Outlet"].unique()
total_students_with_passing_math_score = len(df_students.loc[df_students['math_score'] > 69])$ total_students_with_passing_math_score
irisRDD = SpContext.textFile("iris.csv")$ print (irisRDD.count())
Let's get started. First, let's import the Pandas package to use for the analysis. The Pandas package also allows the$
test_array = np.concatenate([test_active_listing_dummy, test_pending_ratio_dummy], axis=1)$ print(test_array.shape)
import sys$ sys.version
weather_cols = ['Weather', 'TempF', 'TempC', 'Humidity', 'WindSpeed', 'WindDirection', 'Pressure', 'Precip', ]$ weather_df = weather_df[weather_cols]$ weather_df.head()
flights2.passengers.plot()
questions = questions.reset_index(drop=True)
ratings.describe()
store_items.insert(5, 'shoes', [8, 5, 0])$ store_items
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);
import ta # technical analysis library: https://technical-analysis-library-in-python.readthedocs.io/en/latest/$ features['f13'] = ta.momentum.money_flow_index(prices.high, prices.low, prices.close, prices.volume, n=14, fillna=False)$ features['f14'] = features['f13'] - features['f13'].rolling(200,min_periods=20).mean()$
bins = [-1,-0.5,0,0.5,1]$ group_names = ["Between -1 and -0.5", "Between -0.5 and 0", "Between 0 and 0.5", "Between 0.5 and 1"]$ sentiment_df["Sentiment group"] = pd.cut(sentiment_df["Compound score"], bins, right = False, labels=group_names)
qrtSurge = ((qrt.shift(-3)- qrt) / qrt )$ surge = qrtSurge[qrtSurge>1]$ surge.sort_values(ascending=False)$
collection_reference.count_documents({})$
exiftool -csv -createdate -modifydate cisuabd4/cisuabd4_cycle1.MP4 cisuabd4/cisuabd4_cycle2.MP4 cisuabd4/cisuabd4_cycle3.MP4 cisuabd4/cisuabd4_cycle4.MP4 cisuabd4/cisuabd4_cycle5.MP4 cisuabd4/cisuabd4_cycle6.MP4 > cisuabd4.csv
sns.jointplot(x = "positive_ratio", y = "negative_ratio", data = news_df)
df['message_vec'] = vec2.fit_transform(df['message'])$
year16 = driver.find_elements_by_class_name('yr-button')[15]$ year16.click()
session.query(Measurement.tobs).order_by(Measurement.tobs.desc()).first()
len(pd.unique(df2['user_id']))
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', na_values=['.'], names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'class'])$ mydata.head(5)
plt = r6s.score.hist(bins = 10000)$ plt.set_xlim(0,50)
rng_pytz = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='Europe/London')
from sklearn.neighbors import KNeighborsClassifier$ from sklearn.model_selection import train_test_split
df_master.info()
data['intra_day_diff'] = data.High - data.Low$ data.head()
url_mars_facts = "https://space-facts.com/mars/"$ browser.visit(url_mars_facts)
contract_history['NUM_CAMPAGNE'] = contract_history['NUM_CAMPAGNE'].map(lambda x: float(x) if x not in ['N', ''] else np.nan)
last_year = dt.date(2017, 4, 15) - dt.timedelta(days=365)$ print(last_year)
from IPython.display import IFrame$ IFrame('data/lda.html', width=1220, height=860)
seventeen = r.json()['dataset_data']$ seventeen
sentiment.to_csv("sentiment_by_vader.csv", index=False, header=True)$ watson_df.to_csv("sentiment_by_watson.csv",index=False,header=True)$  $
x_min=np.around(np.amin(windfield_matched_array),decimals=-1)-10$ x_max=np.around(np.amax(windfield_matched_array),decimals=-1)+10$ x_num= np.around(np.amax(windfield_matched_array)-np.amin(windfield_matched_array))
ss = fe.bs.smallsample_gmr(256, poparr, yearly=256, repeat=300)$
for n in df[:3].index:$     print(df['URL'][n])$     $
month.to_csv('../data/month.csv')
dfFull['OverallCondNorm'] = dfFull.OverallCond/dfFull.OverallCond.max()
print(r.json()['dataset']['column_names'])
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])$
countries = wb.get_countries()$ countries.iloc[0:10].ix[:,['name','capitalcity','iso2c']]
new_ticket = 'It seems like I have followed the set up directions but I see the set up WEPAY \$                 and am not sure if I have completed everything..thank you'$
driver.get("http://www.reddit.com")$ time.sleep(2)$ html = driver.page_source
plt.rcParams['figure.figsize'] = [16,4]$ plt.plot(pd.to_datetime(mydf1.datetime),mydf1.fuelVoltage,'g.', markersize = 2);$
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-24&end_date=2018-08-24&api_key=" + API_KEY)
HAMD = HAMD[HAMD["level"]=="Enrollment"]$ HAMD = HAMD.dropna(axis=1, how='all')$ HAMD.columns
df.max()
user_df.columns = ['response_id', 'id', 'customer_id', 'crm_tier', $                    'sales_tier', 'mktg_tier', 'addons',$                    'screen_size', 'role', 'language']
Measurements = Base.classes.hawaii_measurement$ Stations = Base.classes.hawaii_station$
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=uEEsBcH3CPhxJazrzGFz&start_date=2017-01-01&end_date=2017-12-31")$ r.json()
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
CON = CON.drop(columns =['Full Name'])
ctc = ctc.fillna(0)
movies.describe()
import pandas as pd$ import numpy as np$ np.random.seed(1234) 
def calc_daily_returns(closes):$     return np.log(closes/closes.shift(1))[1:]
my_model_q3_proba = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='probability')$ my_model_q3_proba.fit(X_train, y_train)$ my_model_q3_proba.stackData.head()
adjusted_width = 25$ ws.column_dimensions['L'].width = adjusted_width$ ws.column_dimensions['Q'].width = adjusted_width
fcc_nn.tail()
us_prec = prec_fine.reshape(844,1534).T #.T is for transpose$ np.shape(us_prec)
feature_cols = ['TV', 'radio']$ X = data[feature_cols]$ print(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')).mean())
X_train.to_csv('X_train.csv')$ X_test.to_csv('X_test.csv')
opener = r.build_opener()$ opener.addheaders = [('User-agent', 'Mozilla/5.0')]$
scratch['created_at'] = pd.to_datetime(scratch['created_at'], coerce=True)
kick_projects['goal_cat_perc'] =  kick_projects.groupby(['category'])['goal'].transform($                      lambda x: pd.qcut(x, [0, .35, .70, 1.0], labels =[1,2,3]))
ADBC = AdaBoostClassifier(n_estimators=50)$ ADBC.fit(X_resampled, y_resampled) 
tm_2030 = pd.read_csv('input/data/trans_2030_m.csv', encoding='utf8', index_col=0)
df_stock1 = df_stock.filter(['Date', 'Close'], axis=1)$ df_test = df_test.filter(['Date', 'Close'], axis=1)$
df = pd.read_parquet('training_data.parquet')
df_plot = df_recommended_menues.sort_values(by=['dates'])
flights2.loc[[1950, 1951]]
df[0].plot()
aapl = pd.read_excel("../../data/stocks.xlsx", sheetname='aapl')$ aapl.head()
df2 = df$ mismatch_index = mismatch_df.index$ df2 = df2.drop(mismatch_index)
yc_new2[yc_new2.tipPC > 100]
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True)
spark.stop()
df_pos_emo_end = df_pos_emo_end[df_pos_emo_end.last_word_category.str.len() == 1]$ df_pos_emo_end.last_word_category.value_counts().head(20)
c.loc["c":"e"]
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd.head()
latest_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()[0]$ format_latest_date = dt.strptime(latest_date,"%Y-%m-%d")$ format_latest_date # Display the date
female = crime.loc[crime['Sex']=='F']$ female.head(3)
print total.shape[0], total.dropna().shape[0]$ total.head()
dfLikes.groupby("periodo").agg({"id": "count"})
grouped.size()
data['Age'].hist(bins = 10)
X = pivoted.fillna(0).T.values$ X.shape
free_data.describe()
experiment_details = client.experiments.get_details(experiment_uid)$ print(json.dumps(experiment_details, indent=2))
Mars_soup = BeautifulSoup(html, 'html.parser')
hired = data.loc[data['hired']==1].tasker_id.value_counts()$ hired[:5]
header_names = {'1. symbol':'sym',$                 '2. price':'price_str', $                 '3. volume':'vol'}
df_CLEAN1A.info()$
for field_name, dtype in df.select_dtypes(include=categories).items():$     print(field_name)$     df[field_name] = pd.Series(pd.Categorical(df[field_name]).codes)
data.drop(['ceil_10min'], axis=1,inplace=True)
import tweepy$ import pandas as pd$ import matplotlib.pyplot as plt
S_distributedTopmodel.forcing_list.filename
tmax_day_2018.values
for obj in bucket_obj.objects.all():$     print('Object key: {}'.format(obj.key))$     print('Object size (kb): {}'.format(obj.size/1024))
list(t2.p1)
datetime.now()
probs_test = F.softmax(V(torch.Tensor(log_preds_test)));$
print(df.tail())
groupby_time = df1['time'].groupby(df1['time']).count()$ groupby_time$
posts = get_posts_as_df(polpages[0])$ for seite in polpages[1:]:$     posts = posts.append(get_posts_as_df(seite))
comments = [x.text for x in soup.find_all('a', {'class':'bylink comments may-blank'})]
print('band width between first 2 bands =',(wavelengths.value[1]-wavelengths.value[0]),'nm')$ print('band width between last 2 bands =',(wavelengths.value[-1]-wavelengths.value[-2]),'nm')
print('{}index/agencies/{}'.format(base_url, '1')) $ print(json.loads(requests.get('{}index/agencies/{}'.format(base_url, '1')).text))
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
avgComp = groupedNews["Compound"].mean()$ avgComp.head()
npath = '/glade/u/home/ydchoi/sopron_2018/notebooks/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
sns.violinplot(x=df['liked'],y=df['profile_popularity'],data=df,whis=np.inf)$
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_jp_200_wzh.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_200_wzh.w2v')
df_2002['bank_name'] = df_2002.bank_name.str.split(",").str[0]$
test_classifier('c1', WATSON_CLASSIFIER_ID_2)$ plt.plot(classifier_stats['c1'], 'ro')$ plt.show()
aml_deployment.deploy()
import pandas_datareader $ faamg = pandas_datareader.get_data_google(['FB','AAPL', 'AMZN','MSFT', 'GOOGL' ])['Close']$
wash_parkdf.coordinates.dropna(inplace=True)
sum(insertid_freq.values())
iris.head().iloc[:,0]
train_b, valid_b, test_b = df_b.split_frame([0.7, 0.15], seed=1234)$ valid_b.summary()
all_sets.cards["XLN"].head()
sum(contractor.state_id.isnull()) #the count of missing state_id value is 0$ contractor.state_id.value_counts() #The state_id columns do not have missing data
df2 = pd.read_csv("../../data/msft.csv",usecols=['Date','Close'],index_col=['Date'])$ df2.head()
combined_df = user_df.merge(response_df, how='inner', left_on='response_id', right_on='id')$ print len(combined_df), len(user_df), len(response_df)
x,y=X[0:train],Y[0:train]$ print (x.shape,y.shape)$ model.fit(x,y,epochs=150,batch_size=10,shuffle=True)
users = pd.read_csv('LaManada_new/tbluserinfo.csv',sep=SEP)$ users.shape
QUIDS = pd.read_table("qids01.txt", skiprows = [1])
print(sum(receipts.duplicated(['parseUser','reportingStatus'])))$ print(sum(receipts.duplicated(['iapWebOrderLineItemId'])))
data.registerTempTable("my_data")
n_neg = n_pos * 10$ train_neg = train_sample.filter(col('is_attributed')==0).orderBy(func.rand(seed=seed)).limit(n_neg).cache()$ print("number of negative examples:", n_neg)
from sqlalchemy import create_engine, func, inspect$ inspector = inspect(engine)$ inspector.get_table_names()$
DataAPI.write.update_index_contents(index_code="A", trading_days=trading_days, override=False, log=False)
studies_b=studies_a.merge(sponsors,on='nct_id', how='left')$ studies_b.head()
data_l2_begin = tmpdf.index[tmpdf[tmpdf.isin(DATA_L2_HDR_KEYS)].notnull().any(axis=1)].tolist()$ data_l2_begin
for column in ['Announced At','Created At','Shipped At','Updated At']: $     df[column] = pd.to_datetime(df[column])
appleInitialNegs = neg_tweets[neg_tweets.author_id_y == 'AppleSupport']
pd.Series(feature_names).sample(20)
old_prob = gb.count().values / sum(gb.count().values)
orders_subset = orders[orders.user_id.isin(selected_users)]
open('test_data//open_close_test.txt')
mydata.tail()
df['money_csum'] = df.groupby(['program_id'])['money_collected'].cumsum()$ df['cum_pct'] = df.groupby('program_id')['money_csum'].transform(lambda x: x * 100/ x.iloc[-1])$ df
contractor_clean.head() #these columns have been dropped
primary_temp_null_indx=dat[primary_temp_column].isnull()
temp_df['reorder_interval_group'].replace('', np.nan, inplace=True)$ temp_df.dropna(subset=['reorder_interval_group'], inplace=True)
import requests, json$ data = requests.get(url).json()
url_hemispheres = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars"$ browser.visit(url_hemispheres)
DataSet.tail(2)
tzs = DataSet['userTimezone'].value_counts()[:10]$ print tzs
dc['created_at'] = pd.to_datetime(dc['created_at'], format='%Y-%m-%d %H:%M:%S')$ tm['created_at'] = pd.to_datetime(tm['created_at'], format='%Y-%m-%d %H:%M:%S')$ dc.created_at.dtype #tm.created_at.dtype
import numpy as np$ step_counts = step_counts.astype(np.float)$ print step_counts.dtypes
y = df['hospitalmortality'].values$ X = df.drop(['hospitalmortality'], axis=1).values$ X_header = df.drop(['hospitalmortality'],axis=1).columns
pd.io.json.json_normalize(playlist['tracks']['items'][2])
df1.append(df2).append(df3)$
fashion['created'] = pd.to_datetime(fashion.created)$ fashion.set_index('handle', inplace=True)$ fashion.dtypes
for table in table_list: display(DataFrameSummary(table).summary())
user_df.fillna(value='', inplace=True)
mov_ids = tmp_df[tmp_df.tmp_idx == usr_idx].columns
merged = df2.merge(dfCountry, on='user_id')$ merged.head()
import re$ import string$ punch=set(string.punctuation)
experiment_df.drop(['email_opened'], axis=1, inplace=True)$ experiment_df.drop(['date'], axis=1, inplace=True)$ experiment_df.drop(['UUID_hash2'], axis=1, inplace=True)
doc = coll.find_one()$ doc
sentiments_pd = pd.DataFrame.from_dict(sentiments).round(3)$ sentiments_pd
LR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)$ yhat_prob2 = LR2.predict_proba(X_test)$ print ("LogLoss: : %.2f" % log_loss(y_test, yhat_prob2))$
df.to_csv("noshit.csv")$
high12 = session.query(Measurement.tobs).\$ filter(Measurement.station == "USC00519281", Measurement.station == Station.station, Measurement.date >="2016-08-23", Measurement.date <="2017-08-23").\$ all()
delays_time =  [one_delays_time, two_delays_time, three_delays_time, four_delays_time, five_delays_time, six_delays_time, seven_delays_time, eight_delays_time, nine_delays_time, ten_delays_time, eleven_delays_time, twelve_delays_time, thirteen_delays_time, fourteen_delays_time, fifteen_delays_time, sixteen_delays_time, seventeen_delays_time, eighteen_delays_time, nineteen_delays_time, twenty_delays_time, twentyone_delays_time, twentytwo_delays_time, twentythree_delays_time, twentyfour_delays_time, twentyfive_delays_time, twentysix_delays_time, twentyseven_delays_time, twentyeight_delays_time, twentynine_delays_time, thirty_delays_time, thirtyone_delays_time, thirtytwo_delays_time, thirtythree_delays_time, thirtyfour_delays_time, thirtyfive_delays_time, thirtysix_delays_time, thirtyseven_delays_time, thirtyeight_delays_time, thirtynine_delays_time, fourty_delays_time, fourtyone_delays_time, fourtytwo_delays_time, fourtythree_delays_time, fourtyfour_delays_time, fourtyfive_delays_time, fourtysix_delays_time, fourtyseven_delays_time, fourtyeight_delays_time, fourtynine_delays_time, fifty_delays_time]$ idx = 0$ ward_df.insert(loc = idx, column = 'Delay Time', value=delays_time)
feed_ids = feeds[feeds['publication_id'] == pubs[pubs['name'] == 'Fox News'].id.values[0]]['id']$ print(feed_ids) # these can be used as indices for feeds_items
city_avg_fare_renamed = city_avg_fare.rename(columns={"fare": "average_fare"})$ citydata_avg_fare_work = city_avg_fare_renamed[['city', 'driver_count', 'type', 'average_fare']]$
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Therapists', therapist_duration, window=52)
all_data_vectorized = body_pp.transform_parallel(all_data_bodies)
festivals.at[2,'longitude'] = -87.7035663$ festivals.head(3)$
df=pd.read_csv("../UserData/1000ShareAllColumns.csv")$ df.dtypes
r = requests.get("https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2017-01-01&end_date=2017-01-03&api_key="", auth=(''))$
Features = iris.values[:, :4]$ species = iris.values[:, 4]
positive = '/Users/EddieArenas/desktop/Capstone/positive-words.txt'$ positive = pd.read_table('/Users/EddieArenas/desktop/Capstone/positive-words.txt')
models.save('models')
before_sherpa = df.loc[df["index"] <= 1685.0]$ after_sherpa = df.loc[df["index"] > 1685.0]$
df['language'] = [np.nan if l == 'C' else l for l in df.language]
universe = ["SAP.DE", "UN01.DE", "BAS.DE"]$ price_data = yahoo_finance.download_quotes(universe)
my_gempro.get_msms_annotations()
print(psy_hx.shape)$ psy_hx = psy_hx[psy_hx["subjectkey"].isin(incl_Ss)]$ psy_hx.shape$
movies_df['genres'] = movies_df.genres.str.split('|')$ movies_df.head()
pd.Series(np.random.randn(5))
data = data.loc[data['tmin'] <= 15]
sentiment_overall = news_sentiments.groupby('News Source').agg({'Compound': np.mean}).reset_index() $ sentiment_overall
containers[0].find("div",{"class":"key"}).a['title'].split()[0].replace(',',"")
df.info(null_counts=True, memory_usage='deep')
lc_review = pd_review["text"][0].lower()$
coin_data.describe()
engine = create_engine("sqlite:///hawaii_hw.sqlite")$ conn = engine.connect()
feedbacks_stress.describe()$
def class_size(Y):$     unique, counts = np.unique(Y, return_counts=True)$     return dict(zip(unique, counts))$
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
killfile = [i[0] for i in [i for i in list(pgn2value.items()) if i[1] == 1]]$ np.random.shuffle(killfile)$ killfile = killfile[:20572 - 15286 ]$
temps_df.Missoula - temps_df.Philadelphia
print(gs.best_estimator_)
data = []$ for row in result_proxy:$     data.append({'date': row[0], 'prcp': row[1]})
demand.loc[:,'value'] = demand.loc[:,'value'].copy() *1.5$ demand.head()
traded_volumes.sort()
convRate = pd.concat([hired,shown],axis=1)$ convRate['rate'] = convRate['hired']/convRate['tasker_id']
stmt = text("SELECT * FROM states where last_changed>=:date_filter")$ stmt = stmt.bindparams(date_filter=datetime.now()-timedelta(days=20))
!sed -i -e 's/if self.max_leaf_nodes == "None":/if self.max_leaf_nodes == "None" or not self.max_leaf_nodes:/' \$   /usr/local/lib/python3.5/dist-packages/autosklearn/pipeline/components/regression/gradient_boosting.py
results_df.describe()
average_chart_lower_control_limit = average_of_averages - 3 * d_three * average_range / \$                                     (d_two * math.sqrt(subgroup_size))
!ls -l ./cnn/questions/training/ | grep -v ^l | wc -l
tf.reset_default_graph()$ P("Reseting TF graph")$ the_graph = tf.Graph()
cbd = CustomBusinessDay(holidays=cal.holidays())$ datetime(2014,8,29) + cbd
print(data_all[data_all < 101].describe())
response=r.json()$ print(response)
card_layouts = ["double-faced", "flip", "leveler", "meld", "normal", "split"]$ all_sets.cards = all_sets.cards.apply(lambda x: x.loc[x.layout.map(lambda y: y in card_layouts)])$ all_sets.cards = all_sets.cards.apply(lambda x: x.loc[x.types.map(lambda y: y != ["Conspiracy"])])
df['dealowner'] = df['dealowner'].str.split(expand = True)[0]
test.iloc[40:100]
gene_df['gene_name'].unique().shape
a = np.arange(1, 11)$ a
plt.scatter(X2[:, 0], X2[:,1], c=labels, cmap='rainbow')$ plt.colorbar()
babies.head()
jd = r1.json()$ print(jd)
ma_mov_idx = ma.array(mov_ids, mask = mov_vec)$ mov_idx = ma_mov_idx[~ma_mov_idx.mask]        
geocode_endpoint = 'https://maps.googleapis.com/maps/api/geocode/json?'
twitter_df = spark.createDataFrame(delimited_twitter_df)
df.head()
digitalgc_tweets = api.search(q="#digitalgc", $                               count=100, lang="en", $                               since="2018-01-01")$
print("Min " + str(dc['created_at'].min()) + " Max " + str(dc['created_at'].max()))$ print("Min " + str(tm['created_at'].min()) + " Max " + str(tm['created_at'].max()))
rain_df.describe()$
df = df.drop(['Unnamed: 0', 'id', 'created_at', 'crawl_at'], axis=1)
master_df=pd.read_csv("CitiBike_Data_for_Machine1.csv")$ master_df.head()
users = users.sort_values(by=['avg_score'], ascending=False)$ top_25_users = users.index[1:50].tolist()
import time$ ctime = obj['openTime']/1000$ time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ctime))
table_rows = driver.find_elements_by_tag_name("tbody")[14].find_elements_by_tag_name("tr")$
nt_price = nt.groupby(pd.Grouper(freq='W'))["sq_price_value"].mean().to_frame().rename(index=str, columns={"sq_price_value":"nt_sq_price_value"})
new_page_converted = np.random.binomial(1,P_new,n_new)$ new_page_converted
all_sets.shape
int_and_tel = cw_df.loc[cw_df['category'] == 'Internet & Telecom']$ int_and_tel.sort_index(axis= 0, inplace= True)$ int_and_tel.reset_index(inplace= True, drop= True)
print("Probability of control group converting:", $       df2[df2['group']=='control']['converted'].mean())
income_raw = data['income']$ features_raw = data.drop('income', axis = 1)$ vs.distribution(data)
df = ts.get_stock_basics()$ date = df.ix['600848']['timeToMarket'] #上市日期YYYYMMDD
desc_stats.columns = ['Count', 'Mean', 'Std. Dev.', 'Min.', '25th Pct.', 'Median', '75th Pct.', 'Max']$ desc_stats
year_with_most_commits = commits_per_year["author"].idxmax().year$ print(year_with_most_commits)
df.index
tweet_df["date"] = pd.to_datetime(tweet_df["date"])$
delimited_hourly = delimited_twitter_df.groupby([pd.Grouper(freq="H"), 'company']).count()['text'].to_frame()$ delimited_hourly.columns = ['Number_of_Tweets']$
int_tel_sentiment[:100].plot(figsize=(20, 20))
routes = json.loads(requests.get('{0}index/agencies/{1}/{2}/routes'.format(base_url, '1', '1')).text)$ print(routes)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
with open(marvel_comics_save, mode='w', encoding='utf-8') as f:$     f.write(wikiMarvelRequest.text)
even_counter.value
pd.date_range('3/7/2012 12:56:31', periods=6, normalize=True)
pystore.list_stores()
NS_2017 = NS_active_2017_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']]$ NS_2016 = NS_active_2016_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']]$ NS_2015 = NS_active_2015_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']]
airports_df[airports_df['city'].str.lower() =='new york']
path_to_token <- normalizePath("data_sci_8001_token.rds")$ envvar <- paste0("TWITTER_PAT=", path_to_token)$ cat(envvar, file = "~/.Renviron", fill = TRUE, append = TRUE)
data = pd.read_csv("MSFT.csv", index_col='Date')$ data.index = pd.to_datetime(data.index)
codes = pd.read_csv('data/icd-main.csv')$ codes = codes[(codes['code'] != codes['code'].shift())].set_index('code')
data = pd.read_csv("ml-100k/u.data", sep="\t", header=None, index_col=0)$ data.columns = ["item id" , "rating" , "timestamp"]
yc_new1 = yc_new.merge(zipincome, left_on='zip_depart', right_on='ZIPCODE', how='inner')$ yc_new1.head()
rain_df.set_index('date').head()$
marsfacts_url = 'https://space-facts.com/mars/'$
newfile = newfile[~newfile['Ov.transport status'].isin({5, 6, 7})]
plate_appearances = plate_appearances.loc[plate_appearances.events.isnull()==False,]
data = pd.read_csv('dog_rates_tweets-e7.csv', parse_dates=[1])$
a_tags = soup.find_all('a')$ for link in a_tags:$     print(link.get('href'))$
session.query(tobs.station, func.count(tobs.tobs)).group_by(tobs.station).\$                order_by(func.count(tobs.tobs).desc()).all() 
station_count = session.query(Stations.id).count()$ print (f"Station Count = {station_count}")
cityID = '629f4a26fed69cd3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Hialeah.append(tweet) 
user = DataSet['userName'].value_counts()[:10]$ print(user)
tweet_df = tweet_df.reset_index()[['created_at','id','favorite_count','favorited','retweet_count','retweeted','retweeted_status','text']]
fitness_tests = sql_query("select * from fitness_tests")$ fitness_tests.head(3)
df.user_mentions = df.user_mentions.apply(usersMentions)
df[pd.isnull(df.Address1)]
sentiment_df["date"] = pd.to_datetime(sentiment_df["date"])$
waihee_tobs = session.query(Measurement.tobs).\$ filter(Measurement.station == "USC00519281", Measurement.station == Station.station, Measurement.date >="2017-07-29", Measurement.date <="2018-07-29").\$ all()
festivals.head(3)
idx = pd.IndexSlice$ df.loc[idx['a', 'ii', 'z'], :]
iso_join.plot();
print(station_availability_df.info())$
df2.converted.mean()
poparr.shape
message_filename = data_file_path + "raw_data_messages.json"$ DF = pd.read_json(message_filename)$ print("... read in dataframe")
df_clean['body_length'].hist(range = (0,100))
probarr = fe.toar(lossprob)$ fe.plotn(fe.np.sort(probarr), title="tmp-SORTED-prob")
df1 = df.groupby("Symbol")["Percent_Diff"].sum()$ df1
sr_top = sr.top(limit = 1000) # limit is 1000 -- won't return any more than that
df = pd.concat([df.title, df.type], axis=1, keys=["title", "type"])$
soup.find_all('p')
bucket.upload_dir('data/heat-pump/raw/', 'heat-pump/raw', clear_dest_dir=True)
data = session.query(hi_measurement).first()$ data.__dict__
with open('key_phrases.pickle', 'rb') as f:$     key_phrases = pickle.load(f)
data.Likes.value_counts(normalize=True)
df['reviw'] = df['review'].apply(preprocessor)
mid = (len(trading_volume) - 1) // 2 + 1$ median_volume = sorted(trading_volume)[mid]$ print('Median Trading Volume: {:.0f}'.format(median_volume))
soup = bs(response.text, 'html.parser')
active_ordered = ordered_df.loc[~churned_ord]$
results_ball_rootDistExp, output_ball_rootDistExp = S.execute(run_suffix="ball_rootDistExp", run_option = 'local')
metadata['wavelength'] = refl['Metadata']['Spectral_Data']['Wavelength'].value$ metadata
engine.score_all_on_classifier_by_key("nhtsa_classifier")
df7.to_csv('Completed_Permits_by_Month.csv')
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s')$ print(git_log.timestamp.describe())
df_hi_temps.head()
x_train.head()$
from sklearn.model_selection import train_test_split$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)
url_CLEAN1A = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1A.csv"$ url_CLEAN1B = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1B.csv"$ url_CLEAN1C = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1C.csv"$
w = np.zeros(shape=(2, 1))$ w[0] = m$ w[1] = c
test_id_ctable = df.groupby(['test_id', 'pass_test']).size().unstack('test_id').fillna(0)$ _, p, _, _ = chi2_contingency(test_id_ctable)$ p
prcp_query = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > last_year).all()
prec_us_full = prec_nc.variables['pr_wtr'][:, lat_li:lat_ui, lon_li:lon_ui]
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt"$ df = pd.read_csv(path, sep ='\s+', na_values=['.'])$ df.head(5)
Val_eddyFlux = Plotting('/glade/u/home/ydchoi/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')$
auth = HydroShareAuthBasic(username='****', password='****')$ hs = HydroShare(auth=auth)$ resource_id = hs.createResource(rtype, title, resource_file=fpath, keywords=keywords, abstract=abstract, metadata=metadata, extra_metadata=extra_metadata)
reddit.Upvotes.value_counts(ascending=False).head(25) #just seeing the number of upvotes for each Reddit post$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(2))
most_active_station_tobs = session.query(Measurement.tobs).\$ filter(Measurement.station == most_active_station, Measurement.station == Station.station,\$        Measurement.date >="2017-08-01", Measurement.date <="2018-07-31").all()
cumret['monthly_cumret'].iloc[0:12].prod()
for row in cursor.columns(table='TBL_FCInspevnt'):$     print(row.column_name)
if search_results is not []:$     print(json.dumps(search_results[0]._json, indent=2))
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt"$ mydata = pd.read_csv(path, sep= '\t')$ mydata.head(5)
lr = LinearRegression()$ lr.fit(train_data, train_labels)
train.nrows
false_churn_bool = [np.any([USER_PLANS_df.loc[unchurn]['status'][i] !='canceled' for i,j in enumerate(USER_PLANS_df.loc[unchurn]['scns_created']) if j==max(USER_PLANS_df.loc[unchurn]['scns_created'])]) for unchurn in churned_ix] 
year_dict = year_request.json()$ print (type(year_dict))
scraped_batch6_top['Date'] = scraped_batch6_top['Date'].str.split('/')
LR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)$ yhat_prob2 = LR2.predict_proba(X_test)$ print ("LogLoss: : %.2f" % log_loss(y_test, yhat_prob2))$
print(chunker.evaluate(valid_trees))
allItemIDs = np.array(allData.map(lambda x: x[1]).distinct().collect())$ bAllItemIDs = sc.broadcast(allItemIDs)
contractor_final.to_sql('contractor', con=engine,if_exists='append',index = False)
model = RandomForestClassifier(n_estimators = 10)$ model.fit(X, y)
cityID = '30344aecffe6a491'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fremont.append(tweet) 
df = pd.read_csv('../Datasets/4 keywords.csv', index_col='Symbol')
with open('API_KEY.txt') as file:$     API_KEY = file.readline()
eppd = eppd.merge(cpdi, on='i')$ eppd = eppd.merge(xpdi, on='i')$ eppd.head(40)
pokemon['Total']= pokemon['HP']+pokemon['Attack']+pokemon['Defense']+pokemon['Sp. Atk']+pokemon['Sp. Def']+pokemon['Speed']$ pokemon.head()
response = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?column_index=4&start_date=2018-02-01&end_date=2018-02-01&collapse=daily&transform=rdiff&api_key={}'.format(API_KEY))$ print(response.json())
cats_out = outcome.loc[outcome['Animal Type']=='Cat']$ cats_out.shape
chance = cnf_matrix[1:].sum() / cnf_matrix.sum()
results = Geocoder.reverse_geocode(31.3372728, -109.5609559)$ results.coordinates
tokens = nltk.word_tokenize(content)$ fdist = nltk.FreqDist(tokens)$ print(fdist)
df_temp = df_measurement[['station','tobs']].groupby(['tobs']).min()$ df_temp.head(1)$ df_temp.tail(1)
row_slice = (slice(None), slice(None), 'Bob')  # all years, all visits, of Bob$ health_data_row.loc[row_slice, 'HR']$
mergde_data=pd.merge(game_meta,labels_group , right_index=True, left_index=True)$ merge_val=pd.merge(game_meta,game_vail,right_index=True, left_index=True)$
ts.shift(1)
df.sort_values(by='B', ascending=True)
grouped_dpt["Revenue"].filter(lambda x: x.sum() > 1000)
scores_firstq = np.percentile(raw_scores, 25)$ scores_thirdq = np.percentile(raw_scores, 75)$ print('The first quartile is {} and the third quartile is {}.'.format(scores_firstq, scores_thirdq))
sherpa = current.loc[df["By Name"] ==  "Sherpa "] $ sherpa
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall()$
dfEtiquetas["latitude"] = dfEtiquetas["place"].apply(lambda p: p["location"]["latitude"])$ dfEtiquetas["longitude"] = dfEtiquetas["place"].apply(lambda p: p["location"]["longitude"])
pattern = re.compile('AA')$ print(pattern.search('AAbcAA'))$ print(pattern.search('bcAA'))
df.iloc[::20].plot.bar(title="Percipitation")$ plt.tight_layout()$ plt.show()$
df_cod2["Cause of death"] = df_cod2["Cause of death"].apply(standardize_cod)$ df_cod2["Cause of death"].value_counts()
validation.analysis(observation_data, lumped_simulation)
df['exp_name'] = df['experience'].apply(lambda x: x['name'])
tmax_day_2018.dims
SAMPLES = 100; gamma_vals = np.logspace(-2, 3, num=SAMPLES)$ gamma_val, gamma_sr = quick_gamma(gamma_vals, consol_px, hist_window, lb, frequency, min_gross, max_gross, min_w, max_w)$ gamma_val, gamma_sr
keys = tweepy.OAuthHandler(consumer_key, consumer_secret)$ keys.set_access_token(access_token, access_token_secret)$ api = tweepy.API(keys, parser=tweepy.parsers.JSONParser())
results2 = model_selection.cross_val_score(gnb, X_test, Y_test, cv=loocv)$ results2.mean()
X = [re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', string) for string in sample]
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))$
date = dsolar_df['Date']$ date = strptime(string, "%d %b %Y  %H:%M:%S.%f")$
heap = [h for h in heap if len(set([t.author_id for t in h.tweets if t.author_id in names])) == 1]
df_tte_all[df_tte_all['UsageType'].str.contains('UGW')]['ItemDescription'].unique()
os.chdir(root_dir + "data/")$ df_fda_drugs_reported = pd.read_csv("filtered_fda_drug_reports.csv", header=0)
df_Tesla = pd.DataFrame.from_dict(dataset)$ df_Tesla[['created_at','text','hashtags','username','user_followers_count','topic']].head()
df = pd.read_csv("FuelConsumption.csv")$ df.head()$
indeed[indeed['summary'].isnull()]$
bar_prep = sentiments_pd.groupby(['Outlet'],as_index=False).mean()[['Outlet','Compound']]$ bar_prep.head()
dfbreakfast = df[(df['TIME'] == '11:00:00') | (df['TIME'] == '12:00:00')]$ dfbreakfast.head(2)
conn.execute(sql)
file_name = str(time.strftime("%m-%d-%y")) + "-NewsMoodTweets.csv"$ sentiments_pd.to_csv(file_name, encoding="utf-8")
df = pd.concat([df_en,df_other])
df.drop(remove_cols, axis=1, inplace=True)
logodds.drop_duplicates().sort_values(by=['count']).plot(kind='barh')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)$ fse = r.json()$ fse
festivals.rename_axis('')$ festivals.columns.names = ['Index']
r6s.groupby('created')['num_comments'].sum().plot()
subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})]
df_grp1=df.groupby('Dates').mean()$ display(df_grp1.head())$ df_grp1.plot(y="prcps",title="Precipitation 2016-08-22 to 2017-08-23")$
from sqlalchemy import create_engine$ engine = create_engine('postgresql+psycopg2://postgres:admin@localhost:5432/DTML')$ X.to_sql('dtml_mstr_scld', engine, if_exists='append')
stocks.loc[('Apple', '2017-12-29')]
now = Timestamp("now")$ local_now = now.tz_localize('UTC')$ now, local_now
factors = []$ DataAPI.write.update_factors_return(['GROWTH'], trading_days)
pd.read_pickle('data/wx/tmy3/proc/tmy3_meta.pkl', compression='bz2').head()
tweet.text
dr_ID = [7.0, 10.0, 16.0]$ RNPA_ID = [3.0, 9.0, 12.0, 13.0, 14.0, 15.0, 19.0, 25.0, 27.0, 30.0]$ ther_ID = [11.0, 17.0, 18.0, 23.0, 24.0, 26.0, 28.0, 29.0]
sample = sample[sample['polarity'] != 2]$ sample['sentiment'] = (sample['polarity'] ==4).astype(int)
tick_locations = [value+0.4 for value in x_axis]$ plt.xticks(tick_locations, bar_outlets)
measure_df.to_sql(name='Measurements', con=engine, index=False)
len(df.user_id.unique())
user = reddit.redditor('Shadow_Of_')$ for comment in user.comments.new():$     print(comment.body)
for model_name in nbsvm_models.keys():$     valid_probs[model_name] = nbsvm_models[model_name].predict_proba(X_valid_cont_doc)
!wc -l $ml_data_dir/ratings.csv
price_data = price_data.iloc[1:]
last_year = dt.date(2018, 6, 2) - dt.timedelta(days=365)$ print(last_year)
measurements_df=pd.read_csv(measurements, dtype=object)
columns = inspector.get_columns('measures')$ for c in columns:$     print(c['name'], c["type"])$
data.learner_id.value_counts()[:1]
nav = soup.nav$ for url in nav.find_all('a'):$     print (url.get('href'))
airlines_sim = [d for d in airlines if len(d.tweets) == 2]$ len(airlines_sim)
sat_5am = pendulum.datetime(2017, 11, 25, 5, 30, 0, tzinfo='US/Eastern')$ sat_6am = pendulum.datetime(2017, 11, 25, 6, 0, 0, tzinfo='US/Eastern')$ sat_spike = tweets[(tweets['time_eastern'] >= sat_5am) & (tweets['time_eastern'] <= sat_6am)]
results = soup.find('div', class_="rollover_description_inner")$ results2 = soup.find('div', class_="content_title")
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')$ git_log.describe()
list.sort()$ print(list)
number_of_commits = len(git_log)$ number_of_authors = len(git_log["author"].dropna().unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df_Tesla['topic_codes']=topic_codes_Tesla_tweets$ df_Tesla.head()
temp_df = pd.DataFrame(data = us_temp)$ temp_df.columns = ts.dt.date[:843]
RE_EMOJI = re.compile('[\U00010000-\U0010ffff]', flags=re.UNICODE)$ def strip_emoji(text):$     return RE_EMOJI.sub(r'', text)$
plt.title(f"Overall Media Sentiment Based on Twitter as of {curDate}")$ plt.xlabel("Outlets")$ plt.ylabel("Tweet Polarity")
df_users.user_id.nunique()
print contractor_clean['zip_part1'].head(); contractor_clean['zip_part2'].head()$
df_vets = df_vets.loc[df_vets['d_birth_date'] > '1980-01-01']$ drop_cols = ['d_suffix', 'section_id', 'row_num', 'site_num', 'cem_url', 'cem_phone', 'cem_addr_two', 'cem_addr_one', 'city', 'state', 'zip', 'v_first_name', 'v_mid_name', 'v_last_name', 'v_suffix']$ df_vets = df_vets.drop(drop_cols, axis=1)
n_old = df2[df2['group'] == 'control'].shape[0]$ n_old
mydf1.info()
x_scaled = min_max_scaler.fit_transform(x_normalized)
result = forest.predict(testDataVecs)$ output = pd.DataFrame(data={"id":test["id"], "sentiment":result})$ output.to_csv( "output.csv", index=False, quoting=3 )
status_data = status_data.drop(['BROKERAGE', 'BETWEENNESS', 'NBROKERAGE',$                                     'NBETWEENNESS', 'DENSITY', 'TRANSITIVITY', 'NETWORKSIZE'], axis=1)
y_pred = logreg.predict(X_train)$ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_train, Y_train)))
from jira import JIRA$ jira = JIRA(cred['host_jira'], basic_auth=(cred['email'], cred['password_jira']))$ issues = jira.search_issues('issuetype = "Bug de Tracking"')
!h5ls -r 'data/my_pytables_file.h5'
(training,testing) = clean_data.randomSplit([0.7,0.3])
X_copy['crfa_r'] = X_copy['crfa_r'].apply(lambda x: int(x))
index=pd.date_range('8/14/2017', '12/31/2017')
rf=RandomForestClassifier(labelCol="label", featuresCol="features")$ labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels)$ pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,labelIndexer, OH1, OH2, OH3, OH4, OH5, OH6, assembler, rf, labelConverter])
df[df.Likes == 0].tail()
page = requests.get('https://www.r-bloggers.com')$ soup = BeautifulSoup(page.text, "html5lib")$
!hdfs dfs -put ProductPurchaseData.txt {HDFS_DIR}/ProductPurchaseData.txt
data.keys()
master_df_rand = master_df.sample(frac = 0.067, random_state = 1)$ master_df_rand = master_df_rand.reset_index(drop=True)$ master_df_rand
!python model.py -h
outliersDict = {key: df[abs(df['value'] - np.mean(df['value'])) > 3 * np.std(df['value'])] for key, df in typesDict.items()}$ outliersDict.keys()
malemoon = pd.concat([moon, malebydatenew], axis=1)$ malemoon.head(3)
tm_2040 = pd.read_csv('input/data/trans_2040_m.csv', encoding='utf8', index_col=0)
index.save('trump.index')$ index = similarities.MatrixSimilarity.load('trump.index')
engine = create_engine("sqlite:///hawaii.sqlite")$
ldf['Traded Volume'].mean()
df.columns = df.columns.str.replace('[', '')$ df.columns = df.columns.str.replace(']', '')$ df.columns = df.columns.str.replace(' ', '_')
rfc = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=0)$ cv_score = cross_val_score(rfc, features_class_norm, overdue_transf, scoring='roc_auc', cv=5)$ 'Mean ROC_AUC score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
gbm = H2OGradientBoostingEstimator()$ gbm.train(['sepal_len','sepal_wid','petal_len','petal_wid'],'class',train)
checking['age'].iloc[z]
filename = 'Daily_Stock_Prediction_latest.pk'$ with open('./Models/'+filename, 'rb') as f:$     model_test = pickle.load(f)
cityID = '0eb9676d24b211f1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cleveland.append(tweet) 
engine = create_engine('mysql+pymysql://root:@localhost/sawi_tweets?charset=utf8mb4', encoding='utf8', echo = False)$ df = pd.read_sql_table("sawi_tweets_historical", con = engine)
rng = pd.date_range(end='2018-01-19', periods=200, freq='BM')$ type(rng)
df = build_dataframe('../data/raw/')$ df.info()
oz_stops = oz_stops.rename(columns={'stop_id': 'stopid'})$ oz_stops.head(1)
df[['text', 'retweet_count', 'date']][df.retweet_count == np.max(df.retweet_count)]
df = pd.DataFrame(data=fruits_and_veggies)$ df
print(type(coin_data.index))$ coin_data.columns
words = [w for w in words if w not in stopwords.words('english')]$ print(words)
data.info()
df.info()$ df.isnull().sum()$
from gensim import models$ model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  
events_top10_df = events_enriched_df[events_enriched_df['topic_id'].isin(top10_topics_list)].copy()$ events_enriched_df.shape[0], events_top10_df.shape[0]
html = browser.html$ soup = bs(html, 'html.parser')$
pbptweets = pbptweets.drop_duplicates(subset='text', keep='first')
X = pd.get_dummies(X, drop_first = True)$ X.head()
plt.plot(df[base_col].rolling(window=12).std(),color='green',lw=2.0,alpha=0.4)$ plt.plot(df[base_col].rolling(window=3).std(),color='purple',lw=0.75)$ plt.show;
np.random.seed(1) $ model = models.LdaMulticore(corpus, id2word=dictionary, num_topics=10, workers=5,passes=200, eval_every = 1)
df.loc[df['lead_mgr'].str.contains('Stanl'), 'lead_mgr'] = 'Morgan Stanley'$
for tweet in df_tweets:$     print(TextBlob(tweet).sentiment)$     print(TextBlob(tweet).sentiment.subjectivity)$
checking.iloc[z,0] = np.nan$ checking['age'] = checking['age'].fillna(checking['age'].mean())
print login_response.status_code$ login_page = BeautifulSoup(login_response.content, 'lxml')
df.isnull().values.any()
for f in read_in["files"]:$     fp = getattr(processing_test.files, f)$     print(json.load(open(fp.load())))
print(list(cos.buckets.all()))
df.groupby([df.index.month, df.index.day]).size().plot()$ plt.show()
contractor_merge['contractor_bus_name'] = contractor_merge['contractor_bus_name']+" - "+contractor_merge['contractor_number'].astype(str)
festivals['Index'] = range(1, len(festivals) + 1)$ list(festivals.columns.values)$ festivals.head(3)$
type(df.date[0])
shape_file = ('/g/data/r78/vmn547/GWandDEA_bex_ness/Little_GW_AOI_for_demo/kEEP_ord/KEEP_AOI.shp')
lr1 = LogisticRegression(random_state=20, max_iter=10000, C=0.5, multi_class= 'ovr', solver= 'saga')$ lr1.fit(X, y)$ lr1.score(X_test, y_test)
xml_in_sample = xml_in[xml_in['authorName'].isin(random_authors_final)]
result = cur.fetchall()$
!wget http://files.grouplens.org/datasets/movielens/ml-latest.zip
weather.info()
cabs_df_byday = cabs_df.loc[cabs_df.index.weekday == weekday]$ cabs_df_byday.info()$ cabs_df_byday.head()
df = pd.read_csv(project_path + '/data/raw/data.csv', index_col=0)$ df.head()
def calc_tmps(spec_date):$     return session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$ filter(Measurement.date == spec_date).all()
cycling_data = [10.7, 0, None, 2.4, 15.3, 10.9, 0, None]$ joined_data = list(zip(step_counts, cycling_data))$ print joined_data$
tz_dateutil = dateutil.tz.gettz('Europe/London')
t3[t3['retweeted_status'].notnull()== True]
unknown_users_log = df_log[~df_log['user_id'].isin(df_users['user_id'])]
ex=savedict['2017-11-15']$ ex.head()
import pickle$ filename = 'models/sentiment_model.sav'$ pickle.dump(sentiment_model, open(filename, 'wb'))
print('Best score for data:',np.mean(forest_clf.feature_importances_))
tia['date'] = tia['date'].apply(lambda x: x[14:])$ tia['date'][0]
X = np.array(df1.drop(['label'], axis=1))$ y = np.array(df1['label'])
knn_reg.score(x_test,y_test)
stn_cnt_df=pd.DataFrame(stations_des,columns=['Station','Counts'])$ stn_cnt_df.head()
for i, row in problems.iterrows():$     srx, srr = row.srx, row.srr$
mean = np.mean(data['Likes'])$ print("The Likes' average in tweets: {}".format(mean))
import numpy as np$ np.sqrt(s)
Base.classes.keys()$
full_image_elem = browser.find_by_id('full_image')
yc200902_short['Trip_Pickup_DateTime'] = pd.to_datetime(yc200902_short['Trip_Pickup_DateTime'])
questions = pd.concat([questions.drop('bands', axis=1), bands], axis=1)
import test_package.print_hello_class_container$ my_instance = test_package.print_hello_class_container.Print_hello_class()$ my_instance
df.set_index(['Date', 'Store', 'Category', 'Subcategory', 'Description'], inplace=True)$ df.head(3)
ibm_train = ibm_hr_final.join(ibm_hr_target.select("Attrition_numerical"))$ ibm_train.printSchema()
words = [w for w in words if not w in stopwords.words('english')]$ print(words[:100])
filt_zip_loc=~np.logical_and(building_pa_prc_shrink[['zipcode']].isna().values ,building_pa_prc_shrink[['location']].isna().values)
station_availability_df.loc[station_availability_df['status_key']==1,'status_value'] = "In Service"$ station_availability_df.loc[station_availability_df['status_key']==3,'status_value'] = "Not In Service"$ station_availability_df.status_value.value_counts()
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href']
df.loc['a', 'ii', 'z']
sns.set_style('whitegrid')$ sns.distplot(data_final['countCollaborators'], kde=False,color="red")#, bins=20)$
print(rmse_scores.mean())
df_track.to_sql('track_db', cnx)$ df_artist.loc[:,:].to_sql('artist_db', cnx)
df_pop = df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ df_pop$
colors = ('blue', 'red', 'gold', 'green', 'c')$ lgd = zip(tweet_df["Tweet Source"].unique(),colors)$
contrast = df[(df['up_votes'] > df['up_votes'].quantile(q=.85)) | (df['up_votes'] == 0)].reset_index(drop=True)$ contrast['viral'] = (contrast['up_votes'] != 0).astype(int)$ contrast.shape
result_1 = pd.concat([df1, df3], axis = 1) # concatenate one dataframe on another along columns$ result_1
df["text"] =  df.text.str.replace('[^\x00-\x7F]','')$ df.text = df.text.apply(removeEmoj)
data_ar = np.array(data_ls)$ data_df = pd.DataFrame(data_ar, columns = ["ticket_id","time_utc","type", "desc"])
model.wv.doesnt_match("man woman dog child kitchen".split())$
output = pd.DataFrame({id_label:ids, target_label:predictions})$ print_and_log(output)
months = pd.concat([nov, dec])$ print nov.shape, dec.shape, months.shape
stories = stories.set_index('short_id')$ stories.head()
from statsmodels.stats.diagnostic import acorr_ljungbox$ print('差分序列的白噪聲檢查结果為：', acorr_ljungbox(resid_6203.values, lags=1)) 
validation.analysis(observation_data, simple_resistance_simulation_0_5)
payload = {'key1': 'janney1', 'key2': None}$ r = requests.get('http://httpbin.org/get', params=payload)$ r.url
DATA_PATH = "../data/query.csv"$ df = pd.read_csv(DATA_PATH)
from Lightcurve import plot_LC_solar_Flare$ % matplotlib inline $ plot_LC_solar_Flare('FERMI/SolarFlares/LAT_Flares/lat_LC_20120307.fits', 'Flare20120307')
AFX_X_dict = json.loads(AFX_X_data.text)
info_final = pd.merge(edu_gen_edad, visitas, on = 'idpostulante', how = 'inner')$ info_final = pd.merge(info_final, postulaciones, on = 'idpostulante', how = 'inner')$ info_final.head()
sql = "SELECT * FROM main_stops;"$ stops = pd.read_sql(sql, engine)
print('if we spend 50k on TV ads, we predict to sell:', round(lm.predict(X_new)[0],2), ' thousand units')
nitrodata['MonitoringLocationIdentifier'].nunique()
X_testset.shape$ y_testset.shape
results = pp.get_results()
fire=load_data('https://data.sfgov.org/resource/wbb6-uh78.json')$ fire.dtypes$
logreg = LogisticRegression(penalty='l1', solver='liblinear')$ y_pred = cross_validation.cross_val_predict(logreg, X, y, cv=5)$ print(metrics.accuracy_score(y, y_pred))
tweet_image_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']).value_counts()$
df_geo_insta['hour']=hours$ df_geo_insta['hour'].unique()$
cleansed_search_df['SearchTerm'] = np.where(cleansed_search_df['SearchCategory'] == "Plant", cleansed_search_df['SearchTerm'].str.rpartition('-')[2].str.strip() , cleansed_search_df['SearchTerm'])$ cleansed_search_df.loc[cleansed_search_df['SearchCategory'] == "Plant"]
processed_tweets.sample(4)
abc = Grouping_Year_DRG_discharges_payments.groupby(['year','drg3']).get_group((2015,871))$ abc.head()
open_list = [d[dt]['Open'] for dt in d.keys() if d[dt]['Open'] is not None]$
dat_dow.vgplot.line(value_name='Hospital mortality rate')
new_model =  gensim.models.KeyedVectors.load_word2vec_format(path_database+'lesk2vec.bin', binary=True)
dates = [datetime(2014,8,1),datetime(2014,8,2)]$ ts = pd.Series(np.random.randn(2),dates)$ ts
req = requests.request('GET', 'https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2017-02-01&end_date=2017-02-01&api_key='+API_KEY)$ req.json()
(ggplot(all_lum_binned.query("lum>0&subject=='VP3'"),aes(x="td",y="gy"))+geom_smooth(method='loess'))+facet_wrap("~eyetracker")+xlim(-1,4)
deserialized = json.loads(serialized)$ if "data science" in deserialized["topics"]:$     print (deserialized)
go_no_go = 1 #NO_GO - ie do NOT run all the long run-time items.  Note that some of these long$
df_chunks = pd.read_csv(LM_PATH/'df.csv', chunksize=chunksize)
len_plot = t_len.plot(figsize=(16,4), label="Length", color='r', legend=True, title='Length of tweets over time')$ len_vs_time_fig = len_plot.get_figure()$ len_vs_time_fig.savefig('len_vs_time.png')
dfHaw_Discharge['flow_MGD'] = dfHaw_Discharge['meanflow_cfs'] * 0.64631688969744
df_events.to_csv("data_output/df_events.csv", encoding="utf-8", index=False)
search['one_way'] = search.apply(lambda x: 0 if x['trip_end_loc'] == x['trip_start_loc'] else 1, axis=1)
tree_features_df['filename'].isin(manager.image_df['filename']).describe()$
cityID = '960993b9cfdffda9'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Bakersfield.append(tweet) 
(train.shape, test.shape, y_train.shape)
new_style_url='https://raw.githubusercontent.com/neilpanchal/spinzero-jupyter-theme/master/custom.css'$ print("Will be using css from {}".format(new_style_url))
pdf = pd.read_csv('training_data/utah_positive_examples.csv')$ ndf = pd.read_csv('training_data/utah_negative_examples.csv')$ wdf = pd.read_csv('utah_weather_2010-2018_grouped.csv')
winpct.loc[winpct['text'].apply(lambda x: any(re.findall('Santos',x)))][['playId','homeWinPercentage','playtext','date']]
eta = therm_fiss_rate / fuel_therm_abs_rate$ eta.get_pandas_dataframe()
lr_y_score = model.predict_proba(X_test)[:, 1] #[:,1] is formatting the output$ lr_y_score
mlp = df[['Median Listing Price']].values$ print('shape:\n', mlp.shape, '\n',$      '1st 5 rows:\n', mlp[0:5, :])
from sklearn.preprocessing import Imputer$ trainDataVecs = Imputer().fit_transform(trainDataVecs)
labeled_docs = LabeledLineSentence(candidate_data['messages'], candidate_data.index)
from sklearn.linear_model import LogisticRegression$ logreg = LogisticRegression()$ print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())
print("P(converted) = %.4f" %df2.converted.mean())
qs.columns
submit.head(1)
pred_labels = lr.predict(test_data)$ print("Training set score: {:.2f}".format(lr.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(lr.score(test_data, test_labels)))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=vq_k_-sidSPNHLeBVV8a')
cityID = 'adc95f2911133646'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Colorado_Springs.append(tweet) 
reduced = merged_data.loc[merged_data['time_sec'].notnull()]$ compacted = reduced[['new_time','drone_rtk_lat','drone_rtk_lon','drone_rtk_alt','drone_lat','drone_lon','rel_alt','evnt']]$
print(ser_obj.head(3))
features = ['cEXT', 'cNEU', 'cOPN', 'cAGR', 'cCON']$ for feature in features:$     status_data[feature] = status_data[feature].map({'y': 1.0, 'n': 0.0}).astype(int)
df_CLEAN1A['AGE'].min()
cities_list = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose', 'Detroit', 'Jacksonville', 'Indianapolis', 'San Francisco', 'Columbus', 'Austin', 'Memphis', 'Fort Worth', 'Baltimore', 'Charlotte', 'El Paso', 'Boston', 'Seattle', 'Washington', 'Milwaukee', 'Denver', 'Louisville', 'Las Vegas', 'Nashville', 'Oklahoma City', 'Portland', 'Tucson', 'Albuquerque', 'Atlanta', 'Long Beach', 'Fresno', 'Sacramento', 'Mesa', 'Kansas City', 'Cleveland', 'Virginia Beach', 'Omaha', 'Miami', 'Oakland', 'Tulsa', 'Honolulu', 'Minneapolis', 'Colorado Springs', 'Arlington', 'Wichita', 'Raleigh', 'St. Louis', 'Santa Ana', 'Anaheim', 'Tampa', 'Cincinnati', 'Pittsburgh', 'Bakersfield', 'Aurora', 'Toledo', 'Riverside', 'Stockton', 'Corpus Christi', 'Newark', 'Anchorage', 'Buffalo', 'St. Paul', 'Lexington-Fayette', 'Plano', 'Fort Wayne', 'St. Petersburg', 'Glendale', 'Jersey City', 'Lincoln', 'Henderson', 'Chandler', 'Greensboro', 'Scottsdale', 'Baton Rouge', 'Birmingham', 'Norfolk', 'Madison', 'New Orleans', 'Chesapeake', 'Orlando', 'Garland', 'Hialeah', 'Laredo', 'Chula Vista', 'Lubbock', 'Reno', 'Akron', 'Durham', 'Rochester', 'Modesto', 'Montgomery', 'Fremont', 'Shreveport', 'Arlington', 'Glendale']
oz_stops = pd.read_csv('../../Data/all_bus_stops.csv', sep=',')
tweet_df = pd.DataFrame(tweet_data)$ tweet_df.shape
df['incident_zip'] = df.incident_zip.astype(int)$ df = df[df['incident_zip'] < 12000 ]
time6MDict = averagebytime(typesDict, '6M')$ time6MDict.keys()
calls_df.groupby(["dial_type"])["length_in_sec"].mean()
engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))$ print(engine.url)
crimes.columns = crimes.columns.str.replace(' ', '_')$ crimes.columns
px = pd.read_csv(dwld_key + '-hold-pricing.csv', index_col='Date', parse_dates=True)
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY)$ r=requests.get(url)
model.wv.most_similar("man")$
temps_mosact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == most_activity[0], Measurements.date > year_ago).all()
gdax_trans['Timestamp'] = pd.to_datetime(gdax_trans['Timestamp'], format="%d/%m/%Y %H:%M:%S")
print("p_new under the null is: %.4f\np_old under the null is: %.4f" %(p_new, p_old))
ebola_melt['country'] = ebola_melt.str_split.str.get(1)$ ebola_melt.head()
df.iloc[99,3]
mpiv=pd.pivot_table(movie,index=['Date'],columns=['Name'],values=['Point'])$ mpiv.head()
df_full = pd.DataFrame()$ df_list = []
n_bandits = gb.size().shape[0]$ test_labels = [i for i in gb.size().index]$ mcmc_iters = 25000
df_final = df_grouped.merge(df_schoo11, on='school_name', how='left')$ df_final.drop(['budget_x', 'size_x', 'School ID','size_y', 'budget_y'], axis = 1, inplace=True)$ df_final
df[df.client_event_time < datetime.datetime(2018,4,1,23,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
in_path = os.path.join(os.getcwd(), in_filename)$ df = pd.read_csv(in_path, encoding='utf-8', parse_dates=['created_at'])$ df.drop_duplicates('beer_name', inplace=True)
x_train, x_test, y_train, y_test = train_test_split(x, y , test_size=0.4, random_state=2)
pd.Series(HAMD.min(axis=1)<0).value_counts()  # no
recommendations = model.recommendProducts(2093760, 5)$ recArtist = set([r[1] for r in recommendations])
people.dtypes
daily_constituent_count = QTU_pipeline.groupby(level=0).sum()$ QTU_pipeline.groupby(level=0).median().describe()
print(sample_data.json())
client.experiments.monitor_logs(experiment_run_uid)
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
df.head()
articles = db.articles.find()$ for article in articles:$     print(article)
msft = pd.read_csv("../../data/msft.csv")$ msft.head()
random.shuffle(porn_ids)$ porn_bots = porn_ids[:5000]
data = data.loc[data['Estación'] == 'Invierno']
groups = openmc.mgxs.EnergyGroups()$ groups.group_edges = np.array([0., 0.625, 20.0e6])
plt.pie(total_ride, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()
idx_trade = r.json()['dataset']['column_names'].index('Traded Volume')$ idx_trade
vocab = {v: k for k, v in vectorizer.vocabulary_.items()}$ vocab
import statsmodels$ import statsmodels.api as sm$ import statsmodels.formula.api as smf
frames=[NameEvents,ValidNameEvents]$ TotalNameEvents = pd.concat(frames)
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)$ sns.boxplot(x="frcar", y="y", data = psy_prepro, ax=ax1).set_title("Anxiety driving/riding in a car")$ sns.boxplot(x="jmp2w", y="y", data = psy_prepro, ax=ax2).set_title("Feel jumpy and restless in past 2 weeks")$
year7 = driver.find_elements_by_class_name('yr-button')[6]$ year7.click()
model_filename = 'models/finalized_mpg_estimation_model.sav'$ loaded_mpg_estimation_model = pickle.load(open(model_filename, 'rb'))$
!head ../../data/msft_modified.csv
df.rename(columns={'85235_00060_00003':'MeanFlow_cfs','85235_00060_00003_cd':'Confidence'},inplace=True)$ df.head()
url = 'https://mars.nasa.gov/news/'$ response = requests.get(url)$ soup = bs(response.text, 'html.parser')
convert_me = "2016bbb12---15"$ datetime.datetime.strptime(convert_me, "%Ybbb%m---%d")
test[['clean_text','user_id','predict']][test['user_id']==5563089830][test['predict']==12].shape[0]
df.index
np.mean([len(h.tweets) for h in heap])
n_trees = 10$ y_pred = model.named_steps['XGBClassifier'].predict(mapper.fit_transform(X_test), ntree_limit= n_trees)
bucket.upload_dir('data/wx/tmy3/proc/', 'wx/tmy3/proc', clear_dest_dir=True)
percent_success = round(kickstarters_2017["state"].value_counts() / len(kickstarters_2017["state"]) * 100,2)$ print("State Percent: ")$ print(percent_success)
model.save('model/my_model_current.h5')$ model.summary()
t_len = pd.Series(data=data['len'].values, index=data['Date'])$ t_fav = pd.Series(data=data['Likes'].values, index=data['Date'])$ t_ret = pd.Series(data=data['RTs'].values, index=data['Date'])
actualDiff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean()$ (actualDiff < pDiffs).mean()
team_slugs_mini = team_slugs_df[['new_slug','nickname']]$ team_slugs_mini.set_index('nickname', inplace=True)$ team_slugs_dict = team_slugs_mini.to_dict()['new_slug']
daily_change = [(daily_p[2]-daily_p[3]) for daily_p in afx_17['dataset']['data'] ]$ dc= (max(daily_change))$ print('The largest change in any one day is $%.2f.'% dc)
from sightengine.client import SightengineClient$ client = SightengineClient("737618018", "bstrJ5VzARavYy5FsELN")$ output = client.check('face-attributes').set_url('https://instagram.fprg2-1.fna.fbcdn.net/vp/aa7a6811e2fbc814c91bb94e92aa8467/5B197538/t51.2885-19/s150x150/11875444_527830867370128_1019973931_a.jpg')
from pyramid.arima import auto_arima
start = datetime.datetime(2010, 1, 1)$ df = web.DataReader("GOOG", 'yahoo', start)$ df.head()
df = df.swaplevel('Description', 'UPC EAN')$ df.head(3)
data_df = data_df[data_df["from.id"]==("368759307733")]
result.street_number
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Doctors', doc_duration, window=52)
stocks.to_sql('stocks', engine, index = False, if_exists = 'append')
x.mean(axis=0)
import os$ os._exit(0)
vendors = df_receipts.vendor_search.unique()$ vendors
delta=datetime(2011,1,7)-datetime.now();$ delta
fname = '/Users/kylefrankovich/Desktop/insight/list_test_data/deluxetattoochicago/deluxetattoochicago.json'$ data = json.load(open(fname))$ len(data)
df_sentiment_means = pd.DataFrame(news_sentiment_means)$ df_sentiment_means.head(10)
df_tick_clsfd_sent = df_tick.join(df_amznnews_clsfd_2tick)$ df_tick_clsfd_sent.info()
import os$ files = os.listdir("/data/measurements")$ print('\n'.join(files))
x.dropna()
with client_hdfs.read('/mydata/helloworld.csv', encoding = 'utf-8') as reader:$     readdf=pd.read_csv(reader,index_col=0)
import re$ trump['source'] = trump['source'].str.replace(re.compile('<.*?>'), '')$ trump['source'].unique()
q_all_pathdep = c.retrieve_query('https://v3.pto.mami-project.eu/query/8da2b65bd4f7cd8d56d90ddfcd85297e8aac54fcd0e04f0a0fa51b2937b3dc62')$ q_all_pathdep.metadata()
print(data.program_code.value_counts())
df_tte_ondemand = df_tte[df_tte['ReservedInstance'] == 'N']$ df_tte_ondemand['UsageType'].unique()
filterdf = result.query("0 <= best <= 1 and fileType == 'csv' and teamCount < 1000")$ filterdf.corr()['best'].sort_values()
df.shape[0]
from dotce.report import generate_chart
data = pd.read_csv('./fake_company.csv')$ data
df.sort_values(by="grade")
tobs_df = pd.DataFrame.from_records(station_tobs)$ tobs_df.head()
application_month_range = ['2017-10','2017-11','2017-12','2018-01','2018-02','2018-03','2018-04']$ man_export_filename = cwd + '\\Manual UW\\Weekly\\Manual UW Tracking.csv'    $
chunker.tagger.classifier.show_most_informative_features(15)
print(tweets[0].id)$ print(tweets[0].created_at)
len(df_proj.ProjectId.unique())
voters.LastVoted.value_counts(dropna=False)$
print("Mean absolute percentage error: %.3f" % mean_absolute_percentage_error(y_test, y_pred) + '%')
w_train, w_test=train_test_split(df3, test_size=.33, random_state=42)
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG RN/PAs', RN_PA_duration, window=52)
df_raw_fb = pd.read_csv('./Datasets/Facebook_Training_Data.csv', encoding='latin1')$ print (df_raw_fb.head())
store_items.interpolate(method = 'linear', axis = 0)
workspace_uuid = ekos.get_unique_id_for_alias(user_id, 'lena_newdata')$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])
joined.dtypes.filter(items=['Frequency_score'])
all_tables_df.OBJECT_TYPE.count()
c>0.7
df_students.shape
pd.Series(np.r_[1,2,9])
compound_final.head()
fig = ax.get_figure()$ fig.savefig("Overall Sentiment.png")
Output_three = New_query.ss_get_results(sport='football',league='nfl', ep='team_game_logs', season_id='nfl-2017-2018', game_id='nfl-2017-2018-ari-det-2017-09-10-1300')$ Output_three
my_gempro.blast_seqs_to_pdb(all_genes=True, seq_ident_cutoff=.7, evalue=0.00001)$ my_gempro.df_pdb_blast.head(2)
conf_matrix = confusion_matrix(new_y, lr2.predict(new_x), labels=[1,2,3,4,5])$ conf_matrix # most of the results are classified as 5 point$
df['comments'].value_counts()/len(df['comments'])
quandl.ApiConfig.api_key = API_KEY$ quandl.get('FSE/AFX_X', start_date='2018-08-08', end_date='2018-08-08')
tempsApr = Series([29, 30, 28, 31, 32], index = pd.date_range('2018-04-20', '2018-04-24'))$ tempsMay = Series([26, 24, 22, 22, 19], index = pd.date_range('2018-05-20', '2018-05-24'))$ tempsMay - tempsApr
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL')$                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1))$                           &(extract_nondeduped_cmp.app_branch_state=='OH')].groupby('APP_PROMO_CD').size()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ hawaii = Base.classes.measurement
media_user_results_df = pd.DataFrame.from_dict(results_list)$ media_user_results_df.head(10)
new_messages = new_messages[new_messages.user_id == 42966]
df_ec2 = df_cols[df_cols.ProductName == 'Amazon Elastic Compute Cloud'] # narrow down to EC2 charges$ df_ec2_instance = df_ec2[df_ec2.UsageType.str.contains('BoxUsage:')] #narrow down to instance charges$ df_tte = df_ec2_instance[df_ec2_instance['LinkedAccountName'] == target_account]$
df.groupby(['line']).agg([sum])$
unique_Taskers_shown_most = sample['tasker_id'].value_counts().head(1)$ unique_Taskers_shown_most
x_normalized = pd.DataFrame(x_scaled)
df_sb = pd.read_csv("sobeys_all.csv", encoding="latin-1")
results = logit.fit()$ results.summary2()
y_pred = model.predict(x_test)
from selenium import webdriver$ driver = webdriver.Chrome()
train, test = dogscats_h2o.split_frame(ratios=[0.7])
movies=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\movies.csv')
from spacy.matcher import Matcher
tzs = tweets_df['userTimezone'].value_counts()[:10]$ print(tzs)$
struct_v1_0 = sp_df_test.schema$ emptyDF = spark.createDataFrame(sc.emptyRDD(), struct_v1_0)$ emptyDF.write.parquet(os.path.join(comb_fldr, "flight_v1_0.pq"))
temp_df['date'].describe()
(mydata / mydata.iloc[0] * 100).plot(figsize = (15, 8)); # 15 deals with the width and 8 deals with the price.$ plt.show() # this function always plots the price using matplotlib functions.$
Total_Number_of_Rides_min = rides_analysis["Total Number of Rides"].min()$ Total_Number_of_Rides_min
with open("life.txt",'r') as f:$     data = f$ process(data)
tweetnet = nx.read_gpickle('twitter_graph_data/bigraph_full_pickle')
output.printSchema()$ output2 = output.select('label', 'features')$
data = pd.read_sql("SELECT * FROM empvw_20",xedb)$ print(data)
measure_df.to_csv("Hawaii_measurements_clean.csv", index=False, sep='\t', encoding='utf-8')$ station_df.to_csv("Hawaii_stations_clean.csv", index=False, sep='\t', encoding='utf-8')$ hawaii_df.to_csv("Hawaii_merged_clean.csv", index=False, sep='\t', encoding='utf-8')
created_date = [to_datetime(t).date() for t in tweets_df['created_at']]$ tweets_df['created_date'] = created_date$ tweets_df.head()
ratings_df = ratings_df.drop('timestamp', 1)$ ratings_df.head()
my_tag={'name':'img','title':'Sunset boulevard',$        'src':'sunset.jpg','cls':'framed'}$ tag(**my_tag)
print('AUC using XGBoost = {:.2f}'.format(roc_auc_score(y_test, y_pred)))
data_libraries_df = pd.merge(left=libraries_df, right=tmp, on="asset_name", how="outer")
table = pd.crosstab(df["grade"], df["loan_status"], normalize=True)$
cris = CrisAI()$ cris.collect_cris_datasets(["tests/test_data/acoustic.zip", "tests/test_data/acoustic2.zip"])$ cris.SpeechCollector.audio_collection$
aci_service.get_logs()
csvFile = open('ua.csv', 'a')$ csvWriter = csv.writer(csvFile)
data.info()
data['len'].hist(bins=14)
calls_nocontact_simp['ticket_closed_date_time'] = pd.to_datetime(calls_nocontact_simp['ticket_closed_date_time'])
df.get_dtype_counts()
graph.run(tweet_query,param=list_to_merge[23000::])$
type(t1.tweet_id.iloc[3])
top_10_authors = git_log.author.value_counts().head(10)$ top_10_authors
import builtins$ builtins.uclresearch_topic = 'HAWKING'$ from configuration import config
scaler = preprocessing.StandardScaler().fit(X_train)$ X_train_scaled = scaler.transform(X_train)$ X_test_scaled = scaler.transform(X_test)
print("Err rate Model")$ for rate, name in sorted((rate, name) for name, rate in best_error.items()):$     print("%f %s" % (rate, name))$
grouped=dup_ts.groupby(level=0)$ grouped.mean()
data = grouped_publications_by_author.copy()
tweet_df = pd.DataFrame.from_dict(tweet_ls)$ tweet_df.sort_values(by='Date', ascending=False)$ tweet_df.head()
import random$ sample=movie1['ceiled_ratings'].tolist()$ x = np.linspace(1,5,100)
dforders = ml4t.build_orders(dfprediction, abs_threshold=0.01, startin=False, symbol='USD-BTC')$ dforders
print(soup.prettify())
Base.classes.keys()
xml_in[xml_in['venueName'].isnull()].count()
log_mod = pickle.load(open('../data/model_data/log_pred_mod.sav', 'rb'))
with open('/Users/annalisasheehan/Dropbox/Climate_India/Data/data_child/lat_long_younglives_AS_cut.csv') as f:$     latlon = f.read().splitlines()
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key='$ url_api=url+API_KEY$ r = requests.get(url_api)
df_countries = pd.read_csv('countries.csv')$ df_countries.head()
url = 'https://twitter.com/marswxreport?lang=en'$ response = requests.get(url)$ soup = bs(response.text, 'lxml')
BallBerry_resistance_simulation_1 = BallBerry_ET_Combine['BallBerry(Root Exp = 1.0)']$ BallBerry_resistance_simulation_0_5 = BallBerry_ET_Combine['BallBerry(Root Exp = 0.5)']$ BallBerry_resistance_simulation_0_25 = BallBerry_ET_Combine['BallBerry(Root Exp = 0.25)']
stopword_list = stopwords.words("german")   #saves German stop words in a list$ print(len(stopword_list),"stop words in the list.")   #Prints number (len()) of elements in a list.$
qs = qs.merge(answers, how="left", left_on="Id", right_on="ParentId", suffixes=("", "_a"))$ print qs.head()
df_master_select = df_master_select.dropna()$ df_master_select.head()
gdax_trans_btc['Balance'] = gdax_trans_btc['Trade_amount'].cumsum();
sqlCtx = pyspark.SQLContext(sc)$ sdf = sqlCtx.createDataFrame(df.astype(str))$ sdf.show(5)
RDDTestScorees.map(lambda entry: (entry[0], entry[1] * 0.9)).collect()
check_null = df.isnull().sum(axis=0).sort_values(ascending=False)/float(len(df))$ np.sum(check_null.values) == 0
INQ2016.head(1)
tallies_file.export_to_xml()
adj_close_acq_date['Date Delta'] = adj_close_acq_date['Date'] - adj_close_acq_date['Acquisition Date']$ adj_close_acq_date['Date Delta'] = adj_close_acq_date[['Date Delta']].apply(pd.to_numeric)  $ adj_close_acq_date.head()
with tf.device('/gpu:0'):$     history = model.fit(X_train, Y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1, shuffle=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)
@app.route("/api/v1.0/stations")$ def stations():$     return jsonify(session.query(hi_stations.STATION.distinct()).all())
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-05-22&end_date=2018-05-22&api_key=zDPbq2QVaB7jFAEq5Tn6')
fix_space_0 = lambda x: pd.Series([i for i in reversed(x.split(' '))])
print(df.tail()) 
df.head()
c = dif.resample('2H').sum()$ c.plot()$ plt.show()
scores[scores.IMDB == scores.IMDB.min()]
res.summary2()
sample=list(db.tweetcollection.find({'_id':994759019909726208}))$ sample
accuracy = metrics.accuracy_score(predictions,y_test)$ print ("Accuracy : %s" % "{0:.3%}".format(accuracy))$
data.head(10)
adj_close_pivot = adj_close_acq_date_modified.pivot_table(index=['Ticker', 'Acquisition Date'], values='Adj Close', aggfunc=np.max)$ adj_close_pivot.reset_index(inplace=True)$ adj_close_pivot
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
from statsmodels.tsa.arima_model import ARIMA$ arima10 = ARIMA(dta_713,(1,0,0),freq='Q').fit()$ arima10.summary()$
ten = pd.merge(left=free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].mean().idxmax(1).to_frame(), right = free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].median().idxmax(1).to_frame(), left_index=True, right_index=True)$ ten.columns = ['mean', 'median']$ ten
roi = pd.DataFrame({'roi_0': roi_on_day(0, users_costs_df, orders_df)})$
total_fare=pd.DataFrame(city_fare)$ totol_fare=total_fare.reset_index()$ totol_fare$
media = np.mean(datos['len'])$ print("El promedio de caracteres en tweets: {}".format(media))
weight_is_relevant = 2*1/(np.sum(article_isRelevant)/ len(article_isRelevant))$ weight_is_not_relevant = 1$ weights = {0:weight_is_not_relevant, 1:weight_is_relevant}
with open('united_list_lower.pickle', 'rb') as f:$     united_list_lower = pickle.load(f)
!hdfs dfs -cat {HDFS_DIR}/p31-output/part-0000* > p31_results.txt
df2.to_csv("highRTrate.csv")$ df3.to_csv("lowRTrate.csv")
df = pd.read_csv(CSV_FILE_PATH,delim_whitespace=True)
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=10, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
AFX = r.json()$ print(type(AFX))
coinbase_btc_eur_min.plot(kind='line',x='Timestamp',y='Coin_price_EUR', grid=True);
target_column = 'DGS30'$ col_list.remove(target_column)
df["timePeriodStart"] = pd.to_datetime(df.timePeriodStart)
t = 0.25$ c_y = 0.6607/(1-t)$ print(c_y)
jsondata = df3.to_json(orient='records')$ with open("_df_subset_v5_wseconds.json", "w") as outfile:$     json.dump(jsondata, outfile)
df_time = df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ df_time
t3=t3.rename(columns={"id": "tweet_id"})
events_popularity_summary = events_top10_df[events_top10_df['yes_rsvp_count']>5][['event_id','popularity','topic_name']].pivot_table($                                 index='topic_name', columns='popularity', aggfunc='count')
pd.Timestamp('2014-12-14 17:30')
stn_tempobs_df=pd.DataFrame(stn_tempobs,columns=['Station','Temperature (Deg. Fah.)'])$ stn_tempobs_df
pd.read_csv(r'C:\Users\Patrik\Downloads\webrobots.iokickstarter-datasets\Kickstarter_2017-10-15T10_20_38_271Z\Kickstarter016.csv').info()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key='YOUR_API_KEY'&start_date=2018-07-04&end_date=2018-07-05")
y_pred_mdl = mdl.predict(X_test)$ y_train_pred_mdl=mdl.predict(X_train)$ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(mdl.score(X_test, y_test)))
with open('youtube_urls.json', 'r') as fp:$     youtube_urls = json.load(fp)
news_title = soup.title.text$ print(news_title)
temp_df2['timestamp'] = pd.to_datetime(temp_df2['timestamp'],infer_datetime_format=True)
ws.delete_cols(1)
store_items = store_items.drop(['watches', 'shoes'], axis=1)$ store_items
plt.savefig('Sentiment_Analysis_News_Organizations_Tweets.png')
stations_df=pd.read_csv(stations, dtype=object)
suburban_summary_table = pd.DataFrame({"Average Fare": suburban_avg_fare,$                                     "Total Rides": suburban_ride_total})$ suburban_summary_table.head()
data_FR[data_FR.duplicated(subset='name', keep=False)]
twelve_months_prcp.head()
df1.columns$ df2=df1.drop(['creation_day','object_id','name', 'email','last_session_creation_time','invited_by_user_id','Referral_create_time','creation_time'],axis=1)$ df2.columns
train, valid, test = covtype_df.split_frame([0.7, 0.15], seed=1234)$ covtype_X = covtype_df.col_names[:-1]     #last column is Cover_Type, our desired response variable \n",$ covtype_y = covtype_df.col_names[-1]    
df.tail(5)
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period(period="quarter")$ print("Trend for quarter: ", get_trend(get_timeseries(closed_pr)))
slope, intercept, r_value, p_value, std_err = stats.linregress(data['timestamp'],data['rating'])
np.where(y > 10)
np.array(df[['Visitors','Bounce_Rate']])
print(afx_x_oneday.json())
predictions_count = model_count_NB.predict(X_test_count)$ predictions_tfidf = model_tfidf_NB.predict(X_test_tfidf)
avisos_detalles.drop('ciudad', axis = 1, inplace = True)$ avisos_detalles.drop('mapacalle', axis = 1, inplace = True)$ avisos_detalles.head(1)
test_df[["id", "labels"]].to_csv("submission_8.27.csv", index=False)
exiftool -csv -createdate -modifydate cisnwe5/Cisnwe5_cycle4.MP4 > cisnwe5.csv
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets if tweet.lang == 'en'], columns=['Tweets'])$ display(data.head(10))
op_ed_articles = pd.read_json('nytimes_oped_articles.json')$ op_ed_articles = op_ed_articles.reset_index(drop=True)$ op_ed_articles.head()
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
volume.sort()$ volume[len(volume)/2]
data.to_csv('prepared_data_with_cuts.csv', index = True)$
indeed.dropna(subset=['summary'], inplace=True)$ indeed.isnull().sum()
print()$ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
sampleSize=ratings.groupby("rating").count().reset_index()['userId'].min()$ sampleSize
data.plot(kind='scatter', x='TV', y='sales')$ plt.plot(X_new, preds, c='red', linewidth=2)
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%s" % (x.year, str(x.week).zfill(2)))$ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%s" % (x.year, str(x.week).zfill(2)))
os.remove(fileName)
nu_fission_rates = fuel_rxn_rates.get_slice(scores=['nu-fission'])$ nu_fission_rates.get_pandas_dataframe()
from sklearn import model_selection$ kfold = model_selection.KFold(n_splits=10, shuffle=True)$ loocv = model_selection.LeaveOneOut()
j = r.json()$
top100ratings=ratings.groupby('movieId').count().nlargest(100,'rating').reset_index()
building_pa_prc_shrink.dtypes
node_types_DF = pd.read_csv(node_models_file, sep = ' ')$ node_types_DF
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()
result = clustering.mean_shift(reduced, bandwidth=3.8)$ labels = np.unique(result['labels'])$ print(len(labels))
state_grid_new = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(21, 21))$ q_agent_new = QLearningAgent(env, state_grid_new)$ q_agent_new.scores = []  # initialize a list to store scores for this agent
m = pd.Period("2011-01",freq='M')$ print(m.start_time)$ print(m.end_time)
apple_data = google_stocks('AAPL')$ apple_data.columns$ apple_data.index
results = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= prev_year).all()$ results[0:10]
x.iloc[:3,:]
measurements = Base.classes.measurements$ measurements
ab_groups = pickle.load(open("ab_groups.p", "rb"))
df.dropna(inplace=True)$ df.shape
listings.loc[0]$
n_new, n_old = df2['landing_page'].value_counts()$ print("new:", n_new, "\nold:", n_old)
with open("TestUser1.json","r") as fh:$     data = json.load(fh)$
(token <- readRDS("data_sci_8001_token.rds"))
mydata.head(3)
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
(fit.shape, fit2.shape, fit3.shape, fit4.shape)
result['subjid'] = result['SourceFile'].apply(textsplitter)$ result['SourceFile'] = result['SourceFile'].apply(reversetextsplitter)$ result = result.rename(index=str, columns={'SourceFile':'videoname'})
drop_list = ['FEDFUNDS','DGS1MO','DGS3MO','DGS6MO','DGS1','DGS2','DGS3']$ for drop_x in drop_list:$     df.drop(drop_x, axis=1, inplace=True)
sales_df.groupby('Country').count()['Revenue'].sort_values(ascending=False)$
injury_df['Date'] = injury_df['Date'].map(lambda x: x.replace('-',''))$ injury_df['Date'].head()
year_with_most_commits = commits_per_year[commits_per_year == commits_per_year.max()].sort_values(by='num_commits').head(1).reset_index()['timestamp'].dt.year$ print(year_with_most_commits[0])
df = df[df.year >2005]
from utils import write_output$ output_path = os.path.join(output_dir, 'prediction.csv')$ write_output(ids, ids_col, y_pred, label_col, output_path)
df['DETAILS']=df['DETAILS'].fillna("")$
import IPython$ print (IPython.sys_info())$ !pip freeze
page_size = 200$ response = client.get('/users', q='footwork', limit=page_size,$                     linked_partitioning=1)$
path = "http://www.principlesofeconometrics.com/stata/consumption.dta"$ df = pd.read_stata(path)$ df.head(5)
class StdDev(CustomFactor):$     def compute(self, today, asset_ides, out, values):$         out[:] = np.nanstd(values, axis=0)
winpct = pd.read_csv('All_Games_Win_Pct.csv')$ winpct['text'] = winpct['playtext']$ winpct['date'] = pd.to_datetime(winpct['Game Date'])
%%time$ ddf = dd.read_parquet(data_dir + file_name + '.parq', index='Date')
bg_df2 = pd.DataFrame(bg3) # create new variable explicitly for it being a DataFrame in pandas$ bg_df2$
inspector = inspect(engine)$ inspector.get_table_names()
simple_resistance_simulation_1 = sim_ET_Combine['simResist(Root Exp = 1.0)']$ simple_resistance_simulation_0_5 = sim_ET_Combine['simResist(Root Exp = 0.5)']$ simple_resistance_simulation_0_25 = sim_ET_Combine['simResist(Root Exp = 0.25)']
client = pymongo.MongoClient()$ tweets = client['twitter']['tweets'].find()$ latest_tweet = tweets[200]
df.loc[df.userLocation == 'Youngstown, Ohio', :]
np.save('../models/crosstab_40937.pkl', crosstab)
from sklearn.model_selection import train_test_split as tts$ X_train, X_test, Y_train, Y_test = tts(X,Y, test_size=0.2, random_state = 56)
df3['tweet'] = df3['full_text'].where(pd.notna(df3['full_text']), other = df3['text'])
print("The tweet with more retweets is: \n{}".format(data['Tweets'][rt]))$ print("Number of retweets: {}".format(rt_max))$ print("{} characters.\n".format(data['len'][rt]))
predictions = np.array([item['classes'] for item in classifier.predict(input_fn=eval_input_fn)])$ ids = np.array([i + 1 for i in range(len(predictions))])$ output = pd.DataFrame({id_label:ids, target_label:predictions}, dtype=np.int32)
df['Year'] = df['created_date'].dt.strftime('%Y')$ df['YearMonth'] = df['created_date'].dt.strftime('%Y/%m')$ df['Month'] = df['created_date'].dt.strftime('%b')
high_low_diff = TenDayMeanDifference(inputs=[USEquityPricing.high, USEquityPricing.low])
avg_Task = sample['num_completed_tasks'].mean()   $ avg_Task
from app.util import data_range$ data_range()
df_weekly = df_mean.merge(df_count[['date', 'id']], suffixes=('_average','_count'), on='date').merge($     df_max[['date', 'week', 'text', 'polarity', 'negative', 'retweets']], on='date', suffixes=('_average','_max'))
print(type(data.values))$ data.values
close_series = stock_data['close']$ display(close_series.head(5)) 
df.isnull().sum()
plt.show()
for temp_col in temp_columns:$     dat[temp_col]=LVL1.hampel(dat[temp_col], k=7) #remove outliers with hampel filter$     dat[temp_col]=LVL1.basic_median_outlier_strip(dat[temp_col], k=8, threshold=4, min_n_for_val=3) #remove remaining outliers; spikes where val exceeds 2-hr rolling median by 4C    
df_merge['budget_binned'] = pd.cut(df_merge['per_student_budget'], bins, labels = bin_names)$ df_merge.head()
for col in X_numcols:    $     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
cityID = '3df4f427b5a60fea'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Antonio.append(tweet) 
hist(df.pre_clean_len,100)$ grid()
Delta_r = +0.04$ Delta_Y_PS12taskB = -1
plt.savefig(str(output_folder)+'NB01_1_imagery_avaliable_cyclone_'+str(cyclone_name)+'_'+str(location_name))
metrics.accuracy_score(y_test, predicted)
vec1 = modelD2V.infer_vector(currentData[0].words)$ vec2 = modelD2V.infer_vector(currentData[1].words)$
df_users_test = df_users.iloc[:2, :]$ df_users_test.iloc[1, -1] = '2017-09-20'$ df_users_test
lr = LogisticRegression(random_state=20, max_iter=10000)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr', 'multinomial'], 'solver':['saga','newton-cg', 'lbfgs']}$ grid_tfidf = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
import numpy as np$ ok.grade('q02')
results = []$ for line in file_handle:$     results.append(line.replace('foo', 'bar'))
joined['dcoilwtico']=joined['dcoilwtico'].astype(np.int8)
df['index1'] = df.index
df_2010['bank_name'] = df_2010.bank_name.str.split(",").str[0]$
lb = articles['tokens'].map(len).quantile(.025)$ ub = articles['tokens'].map(len).quantile(.975)$ articles = articles.loc[(articles['tokens'].map(len)>lb) & (articles['tokens'].map(len)<ub),:]
s = '2016-01-01 00:00:00'$ s = '2018-07-01 00:00:00'$ int(time.mktime(datetime.datetime.strptime(s, "%Y-%m-%d %H:%M:%S").timetuple()))
emails_dataframe['address'].str.split("@").str.get(1)
filter_commands = ['module load python/python-3.3.0']$ filter_commands.append("python %s/filter_fasta.py %s/swissProt_emb_proteins.fasta -out_fasta %s/swissProt_emb_proteins_filtered.fasta -v %s" %(PY_PATH, proteins_evidence_dir, proteins_evidence_dir, "'Arabidopsis thaliana' 'Solanum lycopersicum' 'Oryza sativa' 'Glycine max' 'Vitis vinifera'"))$ send_commands_to_queue('filter_swissprot', filter_commands, queue_conf)
ds_info = ingest.upload_dataset(database=db,$                                 dataset=test,$                                 type_map={"bools": float})
df.iloc[1]
ldf.Close.diff().max()
measure.info()
data_ps = pd.DataFrame(data=[tweet.text for tweet in tweets_ps], columns=['Tweets'])$ display(data_ps.head(10))
plans_set = set()$ plans,counts = np.unique([(['Free-Month-Trial'] + p if p[0] != 'Free-Month-Trial' else p) for p in BID_PLANS_df['scns_array']  ],return_counts = True)
load2017.isnull().sum() 
lm.summary()
price2017 = price2017[['Date', 'Time', 'Germany/Austria/Luxembourg[Euro/MWh]']]$ price2017.columns = ['Date', 'Time', 'DE-AT-LUX']$ price2017.head()
df2.sort_values('total_comments',inplace=True,ascending=False)$ top_comments=df2.head(10)$ top_comments
!open table.html
import pandas as pd$ import numpy as np$ data = pd.read_json('trump_tweets_2009~2018.json')
xml_in.shape
import pandas as pd$ import matplotlib.pyplot as plt
newfile.insert(0, 'CSCA - Add notes here of customer requirement', '')$ newfile.insert(1, 'Direction from Diana', '')
month.columns = ['MONTH_'+str(col) for col in month.columns]
def esol(time, a, datetime):$     pesol = a * 0.000015 * (1 + 0.5 * np.cos(((datetime[int(time)].month - 1) * 3.14) / 6.0))$     return pesol
regr2.score(X2, y)  # when we fit all of the data points
df=df.drop('SEC filings',axis=1)
d1=Series(pd.to_datetime(datestrs));  $ d1
Twitter_map = folium.Map([45.955263, 8.935129], tiles='cartodbdark_matter', zoom_start = 5)$ Twitter_map
df.loc[:,"Date"] = 
news = pd.DataFrame([(div.h3.text.rsplit(' - ')[0],str(datetime.strptime(div.h3.text.rsplit(' - ')[1],'%B %d, %Y')),str(div.p).rsplit('<br/>')[0].replace('<p>','')) $         for div in soup.find_all('div',{'class':'bitcoin_history'})])$ news.columns=['Headline','Date','News']
data['affair'].value_counts()
Average_Compounds.to_csv("Average_Compound_Sentiments.csv")
my_gempro.get_scratch_predictions(path_to_scratch='scratch', $                                   results_dir=my_gempro.data_dir,$                                   num_cores=4)
year_with_most_commits = commits_per_year.sort_values(by='author', ascending=False).head(1) $ year_with_most_commits = 2017
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','Category']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)
tweet.author
findM = re.compile(r'm[ae]n', re.IGNORECASE)$ for i in range(0, len(postsDF)):$ 	print(findM.findall(postsDF.iloc[i,0]))
len([b for b in BDAY_PAIR_df.pair_age if b<0 ])
story_sentence = f'Of {record_count:,} licensed debt collectors in Colorado, {action_count:,} ({pct_whole:0.2f}%) have been subject to some form of legal or administrative action, according to an analysis of Colorado Secretary of State data.'$ print(story_sentence)
delays = pd.read_csv('output.csv', header=None)$ delays.columns = ['Date','ID','Delay','Latitude','Longitude']$ delays_geo = GeoDataFrame(delays)
import pandas as pd$ tweets = pd.read_csv('tweets.csv')
df = pd.read_csv(reviews_file_name, encoding = "ISO-8859-1")$ df.head(n = 10)
print(df.columns)$ df.rename(columns = {'usd pledged':'usd_pledged'}, inplace=True)$
np.random.seed(1)
engine = create_engine("sqlite:///./Resources/hawaii.sqlite", echo=False)
from sklearn.ensemble import RandomForestClassifier$ rf = RandomForestClassifier(n_estimators = 10, random_state = 42)$ rf.fit(X_train,y_train)
kickstarter_req = requests.get("https://webrobots.io/kickstarter-datasets/")$ zip_to_down_ls = re.findall("href=\"(.*s3\.amazonaws.*\.json\.gz)\"", kickstarter_req.text)$ zip_to_down_ls
df_estimates_false['points'].hist(bins=50, figsize=(10,5))$ plt.show()$
contrast['title'] = contrast['title'].apply(cleaner)
cityID = 'a3d770a00f15bcb1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Corpus_Christi.append(tweet) 
c = df.groupby(['education', 'purpose']).agg({'applicant_id': lambda x: len(set(x))}).reset_index()$ c['education+purpose'] = c['education']+c['purpose']$ sns.barplot(data=c, x='education+purpose', y='applicant_id')
pd.date_range('1/1/2017', '12/1/2017', freq='M') $
a = np.random.randn(50, 600, 100)$ a.shape
cityID = '0e2242eb8691df96'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Henderson.append(tweet) 
print(trump.favorite_count.describe())$ print("   ")$ print(trump.retweet_count.describe())
conn = engine.connect()$ measure_df = pd.read_sql("SELECT * FROM Measurement", conn)
stemmer = nltk.stem.porter.PorterStemmer()$ %timeit articles['tokens'] = articles['tokens'].map(lambda s: [stemmer.stem(w) for w in s])
df.loc[:, topics[0]:topics[-1]] = df.apply(lambda x: \$                                            pd.Series([t in x['tags'] for t in topics], index=topics), axis=1)
df = pd.DataFrame(daily_norm, columns = ["date","min_temp","avg_temp","max_temp"])$ df["date"] = pd.to_datetime(df["date"]).dt.date$ df.set_index(["date"], inplace = True)
for row in session.query(measurements, measurements.tobs, stations.station).limit(5).all():$     print(row)
Measurements = Base.classes.measurement$ Stations = Base.classes.station$ Hawaii = Base.classes.hawaii
df = df[df.userTimezone.notnull()]$ len(df)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
type(df_vow['Date'].loc[0])
todays_date = datetime.datetime.now()
datascience_tweets[datascience_tweets['text'].str.contains("RT")]['text'].count() # 322
df['upper'] = df['body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))$ df[['body','upper']].head()
def predict_row(row, theta):$     hx = sigmoid(np.dot(row, theta))$     return hx
np.setdiff1d(np.arange(6567,8339),ORDERS_GEN['order_number'])
table_rows = driver.find_elements_by_tag_name("tbody")[20].find_elements_by_tag_name("tr")$
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week))$ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week))
print(df['Confidence'].unique())
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json')
train_df = pd.read_csv(slowdata + 'train.csv', index_col='id', parse_dates=['date'], dtype=dtypes)$ save_n_load_df(train_df, "training_dev.pkl")$ train_df.name = "TRAINING"
from pandas.io.json import json_normalize$ df_new = json_normalize(list(df_json['user']))
store_items.insert(4, 'shoes', [8,5,0])$ store_items
from bs4 import BeautifulSoup$ import requests$ url = 'https://www.reddit.com/r/Python/'
data_sample['value'] = [i.replace(',', '') for i in data_sample['value']]$ data_sample['value'] = pd.to_numeric(data_sample['value'])
print d.variables['trajectory']
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]$
merge_table1=merge_table.dropna(axis=0)$ merge_table1.head(20)
RNPA_new_8_to_16wk_arima = RNPA_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ RNPA_new_8_to_16wk_arima.index = RNPA_new_8_to_16wk_arima.index.date
jscores = jcomplete_profile['scores']$ sdf = pd.DataFrame.from_dict(jscores)$ print(sdf[['score_code','model_scope_forecast_horizon','effective_date', 'score_value']])
df_rs = master_df_total.sample(frac = 0.085, random_state = 1)
sp500 = pd.read_csv('sp500.csv', index_col='Symbol', usecols=[0,2,3,7])$ sp500.head()
engine = create_engine("sqlite:///hawaii.sqlite")
test_tweet = api.user_timeline(newsOutlets[0])$ print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
df4 = bg_df2[['Date', 'BG']].copy()$ df4 # whew, this way works fine$
print(r.json())
df.date[0]
psa_proudlove = pd.read_csv('psa_proudlove.csv', parse_dates=True, index_col='Date')$ psa_perry = pd.read_csv('psa_perry.csv', parse_dates=True, index_col='Date')$ psa_all = pd.read_csv('psa_all.csv', parse_dates=True, index_col='Date')
from pysumma.utils import utils$ import os
nb = MultinomialNB()$ nb.fit(X_train, y_train)$ nb.score(X_test, y_test)
dict(list(r_close.items())[0:10])
a = a.reshape(4, 5)$ a
closes = pd.concat([msftA01[:3], aaplA01[:3]], keys=['MSFT', 'AAPL'])$ closes
p_old = df2['converted'].mean()
engine.execute('SELECT * FROM measures LIMIT 5').fetchall()
training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=42)
import sys$ sys.path.append('../')$ sys._enablelegacywindowsfsencoding() 
DataSet.to_csv("file_name_final.csv", sep=",")
df2[((df2['group'] == 'treatment') ==(df2['landing_page'] == 'new_page')) == False].shape[0]
df_merge.groupby(['school_name','grade']).reading_score.mean().unstack()
temp_df['reorder_interval_group'] = temp_df['reorder_interval_group'].astype(float)
import datetime as dt$ start_time = dt.datetime.now()
print(train_trees[random.randrange(len(train_trees))])
len(calls_nocontact.location.unique())
X_copy['avg_six_mnth_cnt'] = X_copy['avg_six_mnth_cnt'].apply(lambda x: float(x))
city_dict = {}$ for city in cities_list:$     city_dict[city] = api.geo_search(query = city, wait_on_rate_limit=True, granularity = 'city')[0].id
df = pd.read_excel(workbook_name, sheetname=1)
image_processor.set_up_images()
print(lgb_cv.best_params_)$ print(lgb_cv.best_score_)
jobs=df['job'].unique()
mean_temp = temp_long_df.groupby(['date'])['temp_c'].mean()$ mean_temp.plot(x='date', y='temp_c')
df['CIK']=df['CIK'].map(lambda x:str(x).zfill(10))
def prepare_data_directory():$     if not os.path.exists(data_directory):$         os.makedirs(data_directory)
t_len.plot(figsize=(16,4), label="Length", color='r', legend=True)$ t_fav.plot(figsize=(16,4), label="Likes", legend=True)$ t_ret.plot(figsize=(16,4), label="Retweets", color="g", legend=True)$
successful_campaigns = results[results['tone']=='Positive']['campaign_id'].unique()$ print('first five positive campaigns (unordered): \n{0}'.format(successful_campaigns[:5]))
X = endometrium_data.drop(['ID_REF', 'Tissue'], axis=1).values$ y = pd.get_dummies(endometrium_data['Tissue']).values[:,1]
data = pd.read_csv('dog_rates_tweets.csv', parse_dates=[1])
df3.groupby('created_at').count()['tweet'].plot()
len(pd.unique(ratings['movieId'].ravel()))
df_day['Forecast'] = bound_prediction(sarima_mod.predict())$ df_day$ df_day.plot(figsize=(14, 6));
results_measurement = session.query(Measurements.station,Measurements.date,Measurements.prcp, Measurements.tobs).all()$ results_measurement
from IPython.display import Image$ Image('/Users/jdchipox/Desktop/SV40table.png')
rowsToSkip = list(range(28))$ rowsToSkip.append(29)
coins_mcap_today = mcap_mat.iloc[-2]$ coins_mcap_today = coins_mcap_today.sort_values(ascending=False)
i=random.randrange(len(train_x))$ print_query(i)
print(address_df['nndr_prop_ref'].value_counts().head(3))
t3.info()
feedbacks_stress.loc[feedbacks_stress['versao'] == 1, ['incomodo', 'interesse1', 'interesse2'] ] //= 2$
tobs_date_df = tobs_date_df.rename(columns={0: "date", 1: "tobs" })$ tobs_date_df.head()
no_test_df = df[df["dataset"]=="train"] #.drop_duplicates(subset="text") actually can't do this w out changing vocab$ trn_df, val_df = sklearn.model_selection.train_test_split(no_test_df, test_size=0.1)$ len(no_test_df), len(df), len(trn_df), len(val_df)
df.loc[:,"message"] = df['message'].str.findall('\w{3,}').str.join(' ') $ df.head()
red.columns
df = pd.read_csv("detected.csv")$ df['META_b_cov'] = df['META_b'].where(df['metaREF']!=df['glmmREF'].str.lower(), -df['META_b'])
trains_fe2_x= trains_fe1.drop(['days_since_prior_order','add_to_cart_order','reordered','eval_set'], axis=1)$ trains_fe2_x.head() #12 features as the predictor variables
fine_xs = xs_library[fuel_cell.id]['transport']$ condensed_xs = fine_xs.get_condensed_xs(coarse_groups)
counts = df2['landing_page'].value_counts()$ counts
f_regex = re.compile('([^a-z]|^|$)f([^a-z]|^|$)')$ m_regex = re.compile('([^a-z]|^|$)m([^a-z]|^|$)')$ num_regex = re.compile('[0-9]+')
data.head()
new_fan = questions.loc[questions['years_attend'] == 0]$ return_fan = questions.loc[questions['years_attend'] != 0]
P.plot_1d_hru(0,'airtemp')
df = pd.read_csv('cleaned_and_merged.csv').drop('Unnamed: 0', axis = 1)$ df.head()
col_eliminar = ['bbox_coords','coords_coords','country_code','ext_media_t_co','ext_media_url','symbols']$ tweetsDf = tweetsDf.drop(columns=col_eliminar)
def calc_tmps(start_date, end_date):$     return session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$     filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all()
df = pd.DataFrame(data)$ df= df[['station','RainFall']]$ df.style.bar(subset=['RainFall'], align='mid', color=['#5fba7d'])
def quadratic(x, **kwargs):$     return np.hstack([np.ones((x.shape[0], 1)), x, x**2])
grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)$ grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)$ grid.add_legend()
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
from sklearn.model_selection import cross_val_score$ print(cross_val_score(model4, x_val, y_val))$ print(cross_val_score(model5, x_val, y_val))
resultDS.to_csv(fileName, sep=delimiter, header=True, index = False)
c = y.ravel() # returns a reference to the same array if possible $ c.base is x
import scipy.stats as stats$ t_stat, p_val = stats.ttest_ind(dftop['temp'],weather['temp'], equal_var=False)
bin_vars = [col for col in Xtrain_pf.columns if Xtrain_pf[col].nunique() == 2]$ bin_vars
station_data = session.query(Stations).first()$ station_data.__dict__
from pandas.tools.plotting import lag_plot$ dataSeries = pd.Series(Q3['Average Temperature'])
b2b_df = pd.read_csv(data_fp, header=None, names=['brain_weight', 'body_weight'])$ b2b_df.head()
pd.set_option('display.max_colwidth', 100)$ clinton_df.head()
cust_clust = crosstab.copy()$ cust_clust['cluster'] = c_pred$
grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)$ grid.map(plt.hist, 'Age', alpha=.5, bins=20)$ grid.add_legend()
print(r.json()['dataset_data']['column_names'])#['start_date'])#
print(soup.find_all('div', 'sammy'))$
word_counts = bow_features(sentences, words_in_articles)$ word_counts.shape$
y = tweets['handle'].map(lambda x: 1 if x == 'realDonaldTrump' else 0).values$ print max(pd.Series(y).value_counts(normalize=True))
print(df['Site Fill'].value_counts(dropna=False))
z = indx[indx == False].index.tolist()$ scratch['created_at'].iloc[z]
tlen.plot(figsize=(16,4), color='r')
stop_words = nltk.corpus.stopwords.words('english')$ print(stop_words)$ print('Count: {}'.format(len(stop_words)))
large_image_url = browser.find_by_xpath('//*[@id="page"]/section[1]/div/article/figure/a/img')["src"]$ print(large_image_url)
df = pd.read_csv('~/mids/w266/w266_final_project/Combined_Comments.csv', delimiter=',')
for date_col in ['BABY_CREATED','SCN_CREATED','PURCHASED','PAIRED']:$     ORDER_BPAIR_SCN_SHOPIFY[date_col] = pd.to_datetime(ORDER_BPAIR_SCN_SHOPIFY[date_col]).dt.strftime("%m-%d-%Y")$
json_data_2017 = request_data_2017.json()
iris_new['Sepal'] = iris_new['SepalLength'] * iris_new['SepalWidth']$ iris_new['Sepal'].quantile([0.25, 0.5, 0.75])
print(df2,'\n')$ print(df2[df2['E'].isin(['test'])])  # select E column=='test' only 
filename = "../datasets/catalogs/fermi/gll_psc_v16.fit.gz" $ print([_.name for _ in fits.open(filename)])$ extended_source_table = Table.read(filename, hdu='ExtendedSources')
bigquery_dataset = "{}:{}.hygiene".format(PROJECT,PROJECT)$ job = preprocess('Dataflow', BUCKET, bigquery_dataset)
print(type(plan["plan"]))$ print(plan['plan'].keys())
tweetsIn22Mar.index = pd.to_datetime(tweetsIn22Mar['created_at'], utc=True)$ tweetsIn1Apr.index = pd.to_datetime(tweetsIn1Apr['created_at'], utc=True)$ tweetsIn2Apr.index = pd.to_datetime(tweetsIn2Apr['created_at'], utc=True)
news_p = soup.find('div', class_='rollover_description').text.strip()$ news_p
df = pd.DataFrame([tweet.text for tweet in tweets], columns=['Tweets'])$ df.head(n=10)
cityID = '013379ee5729a5e6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tucson.append(tweet) 
all_data_merge= all_data_merge[True - all_data_merge['review_text'].str.contains('系统默认好评')]$ all_data_merge= all_data_merge[True - all_data_merge['review_text'].str.contains('此用户')]$ all_data_merge.shape
df3[pd.isnull(df3).any(axis=1)]
tweet_df.count()
y = df['comments']$ X = df[['subreddit', 'title', 'age']].copy(deep=True)
t3.tweet_id=t3.tweet_id.astype('str')
merged_portfolio = pd.merge(portfolio_df, adj_close_latest, left_index=True, right_index=True)$ merged_portfolio.head()
media_user_results_df.to_csv("MediaTweetsData.csv", encoding="utf-8", index=False)
lampposts_location_id = '441a59c4-44d7-4428-8918-0560fdf145c8'$ df_lampposts_loc = pd.DataFrame(get_public_data_sp(lampposts_location_id))$ df_lampposts_loc.head()
test_input_fn = create_predict_input_fn(test_data, DEFAULT_BS)$ stats = classifier.evaluate(input_fn=test_input_fn)
adj_glm_int = smf.glm('hospital_expire_flag ~ C(inday_icu_wkd) * C(admission_type)', $                      data=data, family=sm.families.Binomial()).fit()$ adj_glm_int.summary2()$
X_train, X_test, y_train, y_test = train_test_split(features,regression_price,test_size=0.2)
ser_obj2.name = 'temp'$ ser_obj2.index.name = 'year'$ print(ser_obj2.head())
json_data = r.json()$ json_limit = r_limit.json()$ pp.pprint(json_limit)
Quandl_DF.info()$ Quandl_DF.tail(5)
for row in selfharmm_topic_names_df.iloc[3]:$     print(row)
print("Percentage of positive Federer's tweets: {}%".format(len(pos_tweets_rf)*100/len(data_rf['Tweets'])))$ print("Percentage of neutral Federer's tweets: {}%".format(len(neu_tweets_rf)*100/len(data_rf['Tweets'])))$ print("Percentage de negative Federer's tweets: {}%".format(len(neg_tweets_rf)*100/len(data_rf['Tweets'])))
svm_classifier = GridSearchCV(estimator=estimator, cv=kfold, param_grid=svm_parameters)$ svm_classifier.fit(X_train, Y_train)
RDDTestScorees.groupByKey().collect()
from sqlalchemy import distinct$ number_of_stations = session.query(func.count(distinct(Measurement.station))).scalar()$ print(f'The total number of stations : {number_of_stations}')
ss_scaler.fit(active_list_pending_ratio_test)$ active_list_pending_ratio_test_transform = ss_scaler.transform(active_list_pending_ratio_test)$ active_list_pending_ratio_transform[0:5,:]
tweet_df.to_csv('recent_news_tweets.csv')
r.status_code
plot_price(f,'Close')$ plt.legend("full range")
td_by_date = niners.groupby('Date')['Touchdown'].sum()$ td_by_date;
df2 = df.dropna()$ df2.isnull().values.any()
vect = Seq2WordVecTransformer()$ X_vect = vect.fit_transform(X, verbose='debug')$ print ('len(X_vect):', len(X_vect))$
posts.to_excel('digitalreport.xlsx')
df.shape
RNPA_existing_8_to_16wk_arima = RNPA_existing_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ RNPA_existing_8_to_16wk_arima.index = RNPA_existing_8_to_16wk_arima.index.date
y_hat = model.predict(X_test)
data = []$ for row in result_proxy:$     data.append({'date': row.measurements.date, 'tobs': row.measurements.tobs})
dataset.User.value_counts()
flight_pd.to_csv('/home/ubuntu/parquet/flight_pd.csv', sep='\t')
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
page_size = 200$ response = client.get('/tracks', tags='footwork', limit=page_size,$                     linked_partitioning=1)$
sns.set_palette("deep",desat=.6)$ sns.set_context(rc={"figure:figsize":(8,4)})
df_station = pd.DataFrame(list(station_zipcode.items()), columns = ['station', 'zipcode'])$ df_station['zipcheck']=df_station.zipcode.apply(lambda x:len(x))$ df_station[df_station['zipcheck']!=5]$
act['bin'] = pd.cut(act.activity_count, bins=[-1,0,1,2,5,100])$ gract = act.groupby(['month', 'bin']).activity_count.agg(['size'])$
df_zillow.to_csv('cleaned_Zillow.csv', index = False)$ df_zillow = pd.read_csv('cleaned_Zillow.csv')
df = all_tables_df.tail()
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'),how='inner')
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df_concat['date'] = pd.DatetimeIndex(df_concat.date_series).normalize()$ df_concat.date.head()
word2vec = Word2VecProvider()$ word2vec.load("data/embedding_file")
log.info("starting job")$ new_predictions_response = client.run_job(body=req_body)$ log.info("done with job")
explode = [0.1,0,0]$ colors = ["gold", "lightblue", "lightcoral"]$ labels = ["Urban", "Suburban","Rural"]
daily_returns=portfolio_func.calc_daily_returns(closes)$ huber = sm.robust.scale.Huber()$ returns_av, scale = huber(daily_returns)
t1.stage.value_counts()
all_sessions = cuepoints['session_id'].unique()$ all_sessions
tips.set_index(["sex","day"]).head(5)
conn_a.commit()
df.tail()$
forcast_set=clf.predict(X_lately)
xml_in_merged = pd.merge(xml_in_sample, grouped_authors_by_publication, on=['publicationKey'], how='left')
print(plan['plan']['itineraries'][0]['legs'][0].keys())
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))$ colors
grid_pr_size.plot.scatter(x='glon',y='glat', c='size_val', $                            colormap = 'RdYlGn_r')
pyber_df.head()$
with gzip.GzipFile('data/cleaned_df.pkl.gz', 'wb') as file:  $     joblib.dump(df, file)
import sys$ sys.path.insert(0, '..')
print([x for x in df.columns])
time_open_days_issues = Issues(github_index)$ time_open_days_issues.is_open()$ time_open_days_issues.fetch_results_from_source('time_open_days', 'id_in_repo', dataframe=True)
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, temp_us)
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()$
df_sb.head(2)
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
cohort_activated_df = pd.DataFrame(index=daterange,columns=daterange)
chinese_vessels_iattc = pd.read_csv('chinese_vessels_iattc.csv')
import calendar $ month2int = {v.lower():k for k,v in enumerate(calendar.month_name)}$ month2int    
requests.get(wikipedia_marvel_comics)
df = quandl.get('NASDAQOMX/COMP', api_key='yEFb5f6a7oQL91qzEsvg',start_date = '1960-01-01',end_date = '2016-12-26')$ a=df$ a.rename(columns={'Index Value' : 'index_value'}, inplace=True)
version = str(int(time.time()))$ database_log('Daily_Stock_Prediction', version, float(auc), float(build_time))
df_amznnews_clsfd_2tick = df_amznnews_clsfd[['publish_time','textblob_sent', 'vs_compound']]$ df_amznnews_clsfd_2tick.head()
my_stream_listener = PrintingStreamListener()$ my_stream = tweepy.Stream(auth = api.auth, listener=my_stream_listener)
os.chdir(out_path)$ os.getcwd()
plt.rcParams['figure.figsize'] = 8, 6 $ plt.rcParams['font.size'] = 12$ viz_importance(rf_reg, wine.columns[:-1])
fetch_measurements('http://archive.luftdaten.info/2015-05-09/')
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
corpus = [dictionary.doc2bow(text) for text in texts]$ corpus
y_cat.value_counts()$
red_4 = red[['title', 'subreddit', 'num_comments', 'created_utc', 'id', 'time fetched']].copy(deep = True)$ red_4.head()
urls = pd.read_pickle('PC_World_Urls.pickle')$ headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}$ shop_web_addr = 'https://www.pcworld.co.uk'
rtc = rtc.set_index('Export Region')$ rtc = rtc.drop(['Year'], axis=1)
url = "http://space-facts.com/mars/"
full_act_data.plot(figsize=(20,8));
table_rows = driver.find_elements_by_tag_name("tbody")[22].find_elements_by_tag_name("tr")$
s3 = pd.Series(np.arange(12, 14), index=[1, 2])$ pd.DataFrame({'c1': s1, 'c2': s2, 'c3': s3})
t1.name.value_counts()
lda_corpus = model[corpus]$ doc_topic_matrix = matutils.corpus2dense(lda_corpus, num_terms=n_topics).transpose()$ df = df.reset_index(drop=True).join(pd.DataFrame(doc_topic_matrix))
stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\$ order_by(desc(func.count(Measurement.station))).all()$ stations
median_Task = sample['num_completed_tasks'].median()$ median_Task
calls_nocontact_2017 = calls_nocontact_simp.loc[mask]
df3['created_at'] = df3['created_at'].apply(lambda x: x.round('min'))
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))
data = pd.DataFrame(data = [x.text for x in tweets], columns = ['Tweets'])$ print(data.head(10))
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
print(sp500['Price'].head(3))$ print(sp500[['Price','Sector']].head(3))
diff = [abs(list(r_close.values())[i] - list(r_close.values())[i-1]) for i in range (1,len(r_close))]$     $
number_of_commits = len(git_log)$ number_of_authors = git_log['author'].nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df.num_comments = df.num_comments.astype(int)
df = pd.read_sql('SELECT * from room_type', con=conn_b)$ df
import pandas as pd$ with pd.option_context("max.rows", 10):$     print(dta.results.value_counts())
TripData_merged = pd.concat([TripData1, TripData2])
from dateutil.parser import parse $
!wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb
def top_n_tweet_days(df,n):$     return df.head(n)$ top_20_tweet_days = top_n_tweet_days(dates_by_tweet_count,20)
sns.catplot(x="Opening_year", y="HC01_VC36", hue="lic_code",$             col="isClosed", aspect=.6,$             kind="swarm", data=df);$
stop = np.floor(len(X)/4).astype(int)$ model.fit(X[:stop], y[:stop], epochs=200, batch_size=128*2)$
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df =sentiments_df[['Date','Compound','Count']]$ sentiments_df.head()
df['name'].value_counts()
X_train.isnull().sum()
prec_df = pd.DataFrame(data = us_prec)$ prec_df.columns = ts.dt.date
fav_plot = t_fav.plot(figsize=(16,4), label="Favorites", legend=True, title='Number of favorites for tweets over time')$ fav_vs_time_fig = fav_plot.get_figure()$ fav_vs_time_fig.savefig('num_favs_over_time.png')
precip_data_df.set_index("date",drop=True,inplace=True)$ precip_data_df.columns$ precip_data_df.tail()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','ca','uk']])$ results = logit_mod.fit()
plotdf['forecast'] = plotdf['forecast'].interpolate()$ plotdf['forecastPlus'] = plotdf['forecastPlus'].interpolate()$ plotdf['forecastMinus'] = plotdf['forecastMinus'].interpolate()
from datetime import datetime$ datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')$ datetime.strptime('2017-12-25', '%Y-%m-%d').weekday()
raw_data_df = pd.read_csv('Daisy Debt.csv', parse_dates=[0]) $ raw_data_df.head()
df_nott = df.query('landing_page == "old_page"')$ df_4 = df_nott.query('group != "control"')$ df_4.nunique()$
!cat ../../data/msft_with_footer.csv # osx / Linux
exiftool -csv -createdate -modifydate MVI_0011.mp4 > out.csv
newdf['score'].fillna(0.187218571, inplace=True)
sentiments_grp = sentiments_pd.groupby("Source")$ aggr_comp_sentiments = sentiments_grp["Compound"].mean()$ aggr_comp_sentiments
grid.fit(Xtrain, ytrain)
head = pd.Timestamp('20150101')$ tail = pd.Timestamp('20160101')$ df = hp.get_data(sensortype='water', head=head, tail=tail, diff=True, resample='min', unit='l/min')$
Base.classes.keys()
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S_lumpedTopmodel.executable = excutable +'/summa.exe'
daterange = pd.date_range(scn_genesis[0],datetime.today(),freq='1M')
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&API_KEY'$ r = requests.get(url)
plt.show()$
tweetsIn22Mar = tweets22Mar[tweets22Mar.lang == 'in']$ tweetsIn1Apr = tweets1Apr[tweets1Apr.lang == 'in'] $ tweetsIn2Apr = tweets2Apr[tweets2Apr.lang == 'in']
hourly_df['Open_Price_Change'].value_counts()
corpus = [dictionary.doc2bow(interests_token) for interests_token in interests_tokens]$ print(interests_tokens[1])$ print(corpus[1])
import statistics$ statistics.median(v)
%pycat bikescore.py$
pd.set_option('display.max_columns', 23)$ movies.head(5)
printer_data = pd.read_csv('http://ocf.io/shichenh/ocf_datathon_ds/printing.csv')$ session_data = pd.read_csv('https://www.ocf.berkeley.edu/~shichenh/ocf_datathon_ds/sessions.csv')$ staff_data = pd.read_csv('https://www.ocf.berkeley.edu/~shichenh/ocf_datathon_ds/s_sessions.csv')
def toDateTime(x):$     parsedStringDate = x[0:x.index(',')]$     return datetime.strptime(parsedStringDate, '%m/%d/%y').date()
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit = 's')$ git_log['timestamp'].describe()
S_distributedTopmodel.initial_cond.filename
df3 = df / df.iloc[0, :]$ df3.plot()$ plt.show()
job_a_requirements = pd.DataFrame(requirements, columns = ['Job_A'])$ job_a_requirements
n = 1$ selection = 'confidence'$ topn = summary.sort_values(by=['week_id', selection], ascending=[True, False]).groupby('week_id').head(n)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-08-10&end_date=2018-08-10&api_key=xhB-Ae2VRyMYmv2CRGtV")
rural_avg_fare = rural_type_df.groupby(["city"]).mean()["fare"]$ rural_avg_fare.head()$
testdf = getTextFromThread(urls_df.iloc[2,0], urls_df.iloc[2,1])$ testdf.head()$
total.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_3/target_winter2017_night2.csv',index=False)
display(data.head(20))
lagou_df = pd.read_sql(sql_lagou, conn)
df_chapters_read['referer'].value_counts()
x_train, x_test, y_train, y_test = train_test_split(data_woe, df['y'], test_size=0.2, stratify=df['y'], random_state=17)$ print(str(datetime.datetime.now()) + ' train_test_shapes:', x_train.shape, x_test.shape)$
df_new.groupby('country').count()
(act_diff < p_diffs).mean()
print(data.json()['dataset']['column_names'])
utils.serialize_data(data)
groceries.drop('apples', inplace=True)
print(mbti_text_collection.info())
sqladb.columns=headers
from IPython.display import display, Math, Latex
analysis_df.head()$
words_hash_sk = [term for term in words_sk if term.startswith('#')]$ corpus_tweets_streamed_keyword.append(('hashtags', len(words_hash_sk))) # update corpus comparison$ print('List and total number of hashtags: ', len(words_hash_sk)) #, set(terms_hash_stream))
pickle.dump(final_xgb, open(base_filename + '_model.sav', 'wb'))
d = datetime.date(2016, 7, 8)$ d.strftime("On %A %B the %-dth, %Y it was very hot.")
result = api.search(q='%23Australia')  # "%23" == "#"$ len(result)
tdf[tdf['smoker'] == 'No'].describe()
miner = TweetMiner(api, result_limit=200)$ trump_tweets = miner.mine_user_tweets("realDonaldTrump", max_pages=14)
proj_df['Project Subject Category Tree'].value_counts()
stock_data.describe()
gDate_vProject = gDateProject_content.unstack(level=1, fill_value=0)$
new_df = df.dropna(how = 'all')$ new_df
sqladb.head()
np.random.seed(123)$ np.random.shuffle(raw_data)$
tweets.sort_values(by="frequency", ascending=True).head()
testing = pd.read_csv('SHARE-UCSD-export_reformatted.csv')
print(plan.keys())
vip_reason = vip_reason.drop(['[', ']'], axis=1)$ vip_reason = vip_reason.drop(vip_reason.columns[0], axis=1)
example_sent = "April 2018 JAX Magazine is Out: Machine Learning. #BigData #DeepLearning #MachineLeaning #DataScience #AI #Python #RStats @filtration \\ppo."$ word_tokens = tokenize_tweet_text(example_sent, Qye_words = ['BigData'])$ print(word_tokens)$
svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf')$ svc.fit(x_res, y_res)$ scores = cross_validate(svc, x_res, y_res, cv=10, n_jobs=-1, return_train_score=True)
x_normalized = intersections[for_normalized_columns].values.astype(float)
df["grade"] = df["raw_grade"].astype("category")$ df["grade"]
movies_df = pd.read_csv('movies.csv')$ ratings_df = pd.read_csv('ratings.csv')$ movies_df.head()
BMonthEnd().rollforward(datetime(2014,9,15))
final_csv.sentiment_score.idxmax()$
carts_clean.to_csv('data/carts_clean.csv',index=False)
applications = sql_query("select * from applications")$ applications.head(3)
df=dbobject.postgre_to_dataframe('select * from tripadvisor')$ df.head(200)
for tweet in tw.Cursor(api.home_timeline).items(10):$     print(tweet.text) 
print data_df.clean_desc[15]
p1.age= 45$ print(p1.age)
YH_df["date"] = pd.to_datetime(YH_df["created_at"], errors='coerce')
df.plot(x="newDT", y=["ED",'DD','CVX','FL','CAT'], kind="line")$
pca_full = PCA()$ pca_full.fit(crosstab) ## note: This takes 1:20 minutes to complete 20,000 records
year_string = 'extracted_data/*.csv'
print(data.first_name + " " +data.last_name)
from IPython.core.display import display, HTML$ display(HTML("<style>.container { width:100% !important; }</style>"))
output.shape
plt.figure(1)$ actor_counts = df['Actor1Name'].value_counts().head(10)$ actor_counts.plot.bar()
top10_topics_2 = top10_topics_1.sort_values(by='count_event_per_topic', ascending=False)$ top10_topics_2.head(10)
nitrogen = results[mediaMask & hydroMask & charMask & sampFracMask] $ nitrogen.shape
start_coord_list = station_distance['Start Coordinates'].tolist()$ end_coord_list = station_distance['End Coordinates'].tolist()
Z = np.arange(11)$ Z[(3 < Z) & (Z <= 8)] *= -1$ print(Z)
url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'$ browser.visit(url)
df_atends['Prefeitura Regional'].value_counts()
measurements_year = session.query(Measurements.date,Measurements.prcp).limit(10).all()$ for mammal in measurements_year:$     print(mammal)
plt.scatter(X2[:, 0], X2[:,1], c=dayofweek, cmap='rainbow')$ plt.colorbar()
session.query(Measurement.station, func.count(Measurement.station)).\$     group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
drop_num.append(35)$
s2 = s.reindex(['a', 'c', 'e', 'g']) $ s2['a'] = 0 $ s2
query ="SELECT * FROM tddb_00.Weather_Log ORDER BY Log_Id DESC"$ df = pd.read_sql(query,session)$ df.head(10)
ibm_hr_target_small.stat.corr("Age", "DailyRate")
system = system.supersize(2, 2, 2)$ print(system)
for item in bottom_three:$     print('{} has a std of {}.'.format(item[0], item[1]))
newdf.loc[newdf['score'] == 0.187218571]
tlen = pd.Series(data=datos['len'].values, index=datos['Creado'])$ tfav = pd.Series(data=datos['Likes'].values, index=datos['Creado'])$ tret = pd.Series(data=datos['RTs'].values, index=datos['Creado'])
df.drop("water_year2",axis='columns',inplace=True)
station_distance.insert(loc=11, column='Distance(Miles)', value=distance)
bixi_hourly=bixi_hourly.resample('1H', how={'duration_sec': np.mean,$                                             'distance_traveled': np.mean, 'is_member':np.sum,$                                             'number_of_trips':np.sum})
comp = reduce(lambda x, y: pd.merge(df_test, forecast_range, on = 'ds'), df_test.append(forecast_range))$ comp.describe()$
import random$ sample = data.iloc[list(random.sample(range(data.shape[0]), 200)), :]$ sample.plot.scatter(x='WRank', y='WPts')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-05-15&api_key=' + API_KEY)$ r.json()
pdiff = (new_page_converted.mean()) - (old_page_converted.mean())$
df1.loc[0:8,['SerNo', 'By']]
station_count = session.query(func.count(Station.id)).all()$ print(station_count)
SandP = ['MMM', 'ABT', 'ABBV', 'ACN', 'ATVI', 'AYI', 'ADBE', 'AAP', 'AES', 'AET', 'AMG', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ALXN', 'ALLE', 'AGN', 'ADS', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'APC', 'ADI', 'ANTM', 'AON', 'APA', 'AIV', 'AAPL', 'AMAT', 'ADM', 'ARNC', 'AJG', 'AIZ', 'T', 'ADSK', 'ADP', 'AN', 'AZO', 'AVB', 'AVY', 'BHI', 'BLL', 'BAC', 'BCR', 'BAX', 'BBT', 'BDX', 'BBBY', 'BRK.B', 'BBY', 'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BF.B', 'CHRW', 'CA', 'COG', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CAT', 'CBOE', 'CBG', 'CBS', 'CELG', 'CNC', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHTR', 'CHK', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CME', 'CMS', 'COH', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'CXO', 'COP', 'ED', 'STZ', 'GLW', 'COST', 'COTY', 'CCI', 'CSRA', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DLPH', 'DAL', 'XRAY', 'DVN', 'DLR', 'DFS', 'DISCA', 'DISCK', 'DG', 'DLTR', 'D', 'DOV', 'DOW', 'DPS', 'DTE', 'DD', 'DUK', 'DNB', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EVHC', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'EXC', 'EXPE', 'EXPD', 'ESRX', 'EXR', 'XOM', 'FFIV', 'FB', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC', 'FTI', 'FL', 'F', 'FTV', 'FBHS', 'BEN', 'FCX', 'FTR', 'GPS', 'GRMN', 'GD', 'GE', 'GGP', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GS', 'GT', 'GWW', 'HAL', 'HBI', 'HOG', 'HAR', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HP', 'HSIC', 'HES', 'HPE', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HPQ', 'HUM', 'HBAN', 'IDXX', 'ITW', 'ILMN', 'INCY', 'IR', 'INTC', 'ICE', 'IBM', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IRM', 'JBHT', 'JEC', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KHC', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LEG', 'LEN', 'LUK', 'LVLT', 'LLY', 'LNC', 'LLTC', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MNK', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MJN', 'MDT', 'MRK', 'MET', 'MTD', 'KORS', 'MCHP', 'MU', 'MSFT', 'MAA', 'MHK', 'TAP', 'MDLZ', 'MON', 'MNST', 'MCO', 'MS', 'MSI', 'MUR', 'MYL', 'NDAQ', 'NOV', 'NAVI', 'NTAP', 'NFLX', 'NWL', 'NFX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'PCAR', 'PH', 'PDCO', 'PAYX', 'PYPL', 'PNR', 'PBCT', 'PEP', 'PKI', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'RL', 'PPG', 'PPL', 'PX', 'PCLN', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'QCOM', 'PWR', 'DGX', 'RRC', 'RTN', 'O', 'RHT', 'REG', 'REGN', 'RF', 'RSG', 'RAI', 'RHI', 'ROK', 'COL', 'ROP', 'ROST', 'RCL', 'R', 'SPGI', 'CRM', 'SCG', 'SLB', 'SNI', 'STX', 'SEE', 'SRE', 'SHW', 'SIG', 'SPG', 'SWKS', 'SLG', 'SNA', 'SO', 'LUV', 'SWN', 'SWK', 'SPLS', 'SBUX', 'STT', 'SRCL', 'SYK', 'STI', 'SYMC', 'SYF', 'SYY', 'TROW', 'TGT', 'TEL', 'TGNA', 'TDC', 'TSO', 'TXN', 'TXT', 'BK', 'CLX', 'COO', 'HSY', 'MOS', 'TRV', 'DIS', 'TMO', 'TIF', 'TWX', 'TJX', 'TMK', 'TSS', 'TSCO', 'TDG', 'RIG', 'TRIP', 'FOXA', 'FOX', 'TSN', 'USB', 'UDR', 'ULTA', 'UA', 'UAA', 'UNP', 'UAL', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'URBN', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'WM', 'WAT', 'WEC', 'WFC', 'HCN', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WFM', 'WMB', 'WLTW', 'WYN', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XL', 'XYL', 'YHOO', 'YUM', 'ZBH', 'ZION', 'ZTS'] $ for company in SandP:$     qb.AddEquity(company)
data.learner_id.value_counts()
df2.shape, op_before.shape, op_after.shape,  op_a2.shape$
with open('train.csv') as f:$     size=len([0 for _ in f])$     print("Records in train.csv => {}".format(size))
hp = houseprint.Houseprint()$
test = old_test.append(new_test).reset_index()$
df_students.columns.tolist()
closes.plot(figsize=(8,6));
soup.nav
conn = 'mongodb://localhost:27017'$ client = pymongo.MongoClient(conn)
S_distributedTopmodel.decision_obj.bcLowrSoiH.options, S_distributedTopmodel.decision_obj.bcLowrSoiH.value
X_copy['loc'] = X_copy['loc'].apply(lambda x: int(x))
display(data.head(10))
df_new['intercept']=1$ df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
qs[dtanswer.dt.total_seconds() < 60][["Id", "CreationDate", "CreationDate_a"]].head()
print(most_informative_features(our_nb_classifier.classifier, n=10))
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
for tweet in public_tweets:$     print(json.dumps(tweet, sort_keys=True, indent=4))
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','country_UK','country_US']])$ r=lm.fit()
information_ratio = pd.Series([40,50], ['Manager_A', 'Manager_B'])$ print(information_ratio)
api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)
year14 = driver.find_elements_by_class_name('yr-button')[13]$ year14.click()
a = 4.5$ b = 2$ print 'a is %s, b is %s' % (type(a), type(b))
pd.date_range(end='6/1/2016', periods=20)
from sqlalchemy.orm import Session$ session = Session(engine)
stationActive_df = pd.read_sql("SELECT station, count(*) as `Station Count` FROM measurement group by station order by `Station Count` DESC", conn)$ stationActive_df
X_nonnum = X_copy.select_dtypes(exclude=np.number)$ X_nnumcols = list(X_nonnum)$ print(X_nnumcols)
next(stream_docs(path='./movie_data.csv'))
usage_400hz = hc.table('asm_wspace.usage_400hz_2017_Q4') $
np.exp(results.params)
fit1.resid.hist();
df2.to_csv("SandP.csv")$ df2 = pd.read_csv("SandP.csv", parse_dates = True, index_col = 1)$ df2.head(15)$
logodds[logodds.index.str.contains('you')]
ny_df=pd.DataFrame.from_dict(foursquare_data_dict)
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_new, (1 - p_new)])$ p_old_sim = old_page_converted.mean()$ p_old_sim
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]
df_raw = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ df = df_raw.copy()$ print(df_raw.head())
LSST_catalog_data.close();LSST_catalog_data.close();
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3.to_hdf('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.h5', key='df', mode='w')
import seaborn as sns$ sns.set(style="darkgrid")$ ax = sns.countplot(x="AGE_groups", data=df_CLEAN1A)$
df1.shape
df['Injury'] = [1 if 'placed' in text else 0 for text in df.Notes]
fin_p.dropna(inplace=True)
station_distance.insert(loc=1, column='Trip Duration(Minutes)', value=tripduration_minutes)
df['gender']=df['gender'].replace({'Male': 0, 'Female': 1})$ for col in ['Partner','Dependents','PhoneService','PaperlessBilling','Churn']:$      df[col].replace({'Yes': 1, 'No': 0}, inplace=True)
ET_Combine = pd.concat([hour_1dRichards, hour_lumpedTopmodel, hour_distributedTopmodel_average], axis=1)$ ET_Combine.columns = ["Baseflow = 1D Richards'", 'Baseflow = Topmodel(lumped)', 'Baseflow = Topmodel(distributed)']
chrome_driver_path = "D://Program Files (x86)//百度云同步盘//我的软件//chromedriver.exe"$
rf=RandomForestClassifier(labelCol="label", featuresCol="features")$ labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels)$ pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,labelIndexer,OH1,OH2,OH3,OH4,assembler,rf,labelConverter])$
train_topics_df=train_topics_df.fillna(0)$ test_topics_df=test_topics_df.fillna(0)
precip_data = session.query(Measurements).first()$ precip_data.__dict__
tobs_data = []$ for row in temperature_data:$     tobs_data.append(row[0])$
contractor[contractor.duplicated() == True]
cwd1 = os.getcwd()$ print cwd1$
shots_df[['PKG', 'PKA']] = shots_df['PKG/A'].str.split('/', expand=True)$ shots_df.drop('PKG/A', axis=1, inplace=True)
adopted_cats = cats_merge.loc[cats_merge['Outcome Type']=='Adoption']
df.info()
dif.plot.hist()$ plt.show()
prcp_df = pd.DataFrame(prcp_data, columns=['Date', 'Precipitation'])$ prcp_df.set_index('Date', inplace=True) # Set the index by date
grpConfidence = df.groupby(['Confidence'])
X = fires.loc[:, 'glon':'rhum_perc_lag12']$ X = X.drop(['date'], axis=1)$ y = fires['fire']
pattern = re.compile('AA')$ print([x for x in pattern.finditer('AAbcAA')])$ print([x for x in pattern.finditer('bcAA')])
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']['authorCollaborators']#.loc[97545]
!apt install libnvrtc8.0$ !pip install mxnet-cu80$ import mxnet as mx
sp = openmc.StatePoint('statepoint.082.h5')
db = client["clintrials"]$ coll = db["clinical_studies"]$ client["clintrials"].command("listCollections")
afx_17 = json.loads(afx_x_2017.text)$ type(afx_17)
manager.image_df[manager.image_df['filename'] == 'image_picea_sitchensis_in_winter_11.png']$
final_valid_pred_nbsvm1 = valid_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
def location_column(dataframe_list):$     for df in dataframe_list:$         df['location'] = df.user.map(extract_location)
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
read_in = pd.read_pickle(processing_test.data())$ read_in
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local')$
for node in nodes:$     print node.text_content()
dataset = final_member_pivot['Percent Purchase'].values*100$ print dataset
df.to_sql('fb_posts', engine, schema=schema, if_exists='append')
df=df.drop_duplicates(subset=["שם קבוצה מארחת","שם קבוצה אורחת","תאריך המשחק"], keep="first")$ assert(df.duplicated().sum()==0)
y_range = (0, 200)$ x_range = bokeh.models.ranges.Range1d(start = datetime.datetime(2017,1,1),$                                       end = datetime.datetime(2017,1,31))
df_newpage = df.query('landing_page == "new_page"')$ df_2 = df_newpage.query('group == "treatment"')$ df_2.nunique()$
result = pd.concat(six)$ free_data = pd.merge(left=free_data, right = result.to_frame(), left_index=True, right_index=True)
table_rows = driver.find_elements_by_tag_name("tbody")[2].find_elements_by_tag_name("tr")
data_df.groupby('tone')['ticket_id'].nunique()
preds = lm.predict(X_new)$ preds
contractor_clean[contractor_clean.city.isnull()] # The result is empty. $ contractor_clean.loc[contractor_clean['contractor_id'] == 139]$
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', na_values=['.'])$ mydata.head(5)
us_temp = temp_fine.reshape(843,1534).T #.T is for transpose$ np.shape(us_temp)
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S_distributedTopmodel.executable = excutable +'/summa.exe'
final=final[(final.redshift<0.3) & (final.redshift>0.)]$ print final.shape[0]
print('Total null values in name column: ', data.name.isnull().sum())$ print('\nInconsistent data for the missing name rows\n')$
ts = pd.Timestamp(datetime.datetime(2016,5, 12, 3, 42, 56))$ ts
assembly = openmc.RectLattice(name='1.6% Fuel Assembly')$ assembly.pitch = (1.26, 1.26)$ assembly.lower_left = [-1.26 * 17. / 2.0] * 2
X_copy['crfa_f'] = X_copy['crfa_f'].apply(lambda x: int(x))
index = similarities.MatrixSimilarity(doc_vecs, $                                       num_features=topics)$ sims = sorted(enumerate(index[doc_vecs[6]]), key=lambda item: -item[1])
digits.target[5]$ clf.predict(digits.data[5:6])
val_df.reset_index(drop=True, inplace=True)$ val_y.reset_index(drop=True, inplace=True)$ test_df.reset_index(drop=True, inplace=True)
X = pd.get_dummies(X, columns=['subreddit'], drop_first=True)
test = tmax_day_2018.sel(lat=16.75, lon=81.25).to_dataframe()
soup.get_text()
dates = pd.date_range('20180114', periods=7)$ dates
df_concat_2.message_likes_rel = np.where(df_concat_2.message_likes_rel > 10000, 10000, df_concat_2.message_likes_rel)
median_trading_volume = statistics.median([day[6] for day in data])$ print ('Median trading volume for 2017:', median_trading_volume)
news_df = pd.DataFrame(news_dict)$ news_df.head()
base = wards.plot(column = 'geometry', figsize = (10,10), color = 'grey', edgecolor = 'black')$ delays_geo.plot(ax = base, marker='o', color='blue', markersize = 5)
%%time$ model = AlternatingLeastSquares(use_gpu=True)$ model.fit(matrix_data)
grads = K.gradients(loss, [w,b])$ updates = [(w, w-lr*grads[0]), (b, b-lr*grads[1])]
net.build()
deut5 = dta.t[(dta.b==5) & (dta.c==5)]
model.wv.most_similar("cantonese")
train=mydf.sample(frac=0.9,random_state=200)$ test=mydf.drop(train.index)
ser = pd.Series(['Tues','Wed'], index=days)$ ser
iotc = pd.read_excel('IOTC_positive_list_2017-03-09.xls')
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep =' ')$ df.head(5)
first_row=session.query(meas).first()$ first_row.__dict__
train_body_raw = traindf.body.tolist()$ train_title_raw = traindf.issue_title.tolist()$ train_body_raw[0]
(train_4.shape, y_train.shape)
df1=files3.pivot_table(index='candidateid', columns='dimensiontype', values='finalscore')$ df1new = pd.DataFrame(df1.to_records())$ df1new.head()
test_preds_df = pd.DataFrame(test_preds,index=test_target.index,columns=['kwh_pred'])$
data = pd.read_csv('data/analisis_invierno_5.csv')
data.show()
grouped_publications_by_author.columns = grouped_publications_by_author.columns.droplevel(0)$ grouped_publications_by_author.columns = ['authorId', 'authorName','publicationTitles','authorCollaboratorIds','authorCollaborators','countPublications','publicationKeys','publicationDates']
rhum_long_df = pd.melt(rhum_wide_df, id_vars = ['grid_id', 'glon', 'glat'],$                       var_name = "date", value_name = "rhum_perc")$ rhum_long_df.head()
url = 'https://space-facts.com/mars/'
plt.plot('date', "posts", label='FB posts')$ plt.legend(loc='best')$ plt.show()
year10 = driver.find_elements_by_class_name('yr-button')[9]$ year10.click()
s = BeautifulSoup(doc,'html.parser')$ print(s.prettify())
Base = automap_base()$ Base.prepare(engine, reflect=True)
df_2014['bank_name'] = df_2014.bank_name.str.split(",").str[0]$
salida_all = getFacebookPageFeedData(page_id, access_token, 100)$ columns = ['post_from', 'post_id', 'post_name', 'post_type', 'post_message', 'post_link', 'post_shares', 'created_time']$ df_posts = pd.DataFrame(columns=columns)
station_distance['Sex'] = station_distance.Gender.map({0:'unknown', 1:'male', 2:'female'})
sortedprecip_12mo_df.describe()
df.head()
measure_nan.count()
r2017_dict = r2017.json()
df_pol['text']=df_pol['title'].str.replace('\d+', '')$
df_CLEAN1A['AGE_groups'] = df_CLEAN1A['AGE_groups'].astype('category')
dev3['type_first_engagement'] = dev3.apply(get_first_engagement, axis=1)$ dev3['type_last_engagement'] = dev3.apply(get_last_engagement, axis=1)$ dev3['days_involved'] = dev3.apply(get_time_engaged, axis=1)
iris.head()
prcp_df = pd.DataFrame(prcp_query).set_index('date')$ prcp_df.head()$ prcp_df_flat = pd.DataFrame(prcp_query)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data2.csv"$ mydata = pd.read_csv(path, sep =',', header=None)$ mydata.head(5)
fe.bs.bootstrap(3, poparr)$
joined=join_df(joined,weather,["State","Date"])$ joined_test=join_df(joined_test,weather,["State","Date"])$ sum(joined['Max_TemperatureC'].isnull()),sum(joined_test['Max_TemperatureC'].isnull())
google = web.DataReader('MSFT','google',start,end)$ google.head()
metadata_documents = [x.name() for x in query.element("asset/meta").elements() if x.name() != "mf-revision-history"]$ metadata_documents
table.colnames
test_classifier('c2', WATSON_CLASSIFIER_ID_3)$ plt.plot(classifier_stats['c2'], 'ro')$ plt.show()
url = 'https://space-facts.com/mars/'
retweets['id'].groupby(pandas.to_datetime(retweets['created_at']).dt.date).count().mean() # 2.86
campaigns_under_50 = marketing_sources_active_users[marketing_sources_active_users['user_id']<50].index.tolist()$ df_first_days = df_first_days[~df_first_days['marketing_source'].isin(campaigns_under_50 )]
dfClientes.iloc[10:20, :]
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ commits_per_year.columns = ['commits']$ commits_per_year.head()
df_sched = pd.read_csv(schedFile, usecols = schedCols, $                  dtype = sched_dtypes)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY)$
error = train_ALS(training_RDD, test_RDD, best_rank, seed, iterations, lambda_=0.01)$ print('For testing data the RMSE is %s' + str(error))
c = watershed.unary_union.centroid # GeoPandas heavy lifting$ m = folium.Map(location=[c.y, c.x], tiles='CartoDB positron', zoom_start=12)
reframed = series_to_supervised(scaled, 1, 1)$ reframed.drop(reframed.columns[[8,9,10,11,12,13]], axis=1, inplace=True)$ print(reframed.head())
import pandas as pd$ cs = pd.Series(cleaned)$ by_lga['cleaned'] = cs
count_by_Confidence = grpConfidence['MeanFlow_cfs'].count()$ count_by_Confidence.plot(kind='pie');
user_ratings = df_ratings[(df_ratings.user_id==9)]$ user_ratings
tweet_image_clean.shape$
cohort_activated_df = pd.DataFrame(index=daterange,columns=daterange)
tweets['SA'] = tweets['full_text'].apply(lambda x: analize_sentiment(x))
df1 = df.loc[:, ('Close')].reset_index().rename(index=str, columns={"Date": "ds", 'Close': 'y'})
print ('Before Dummies',df.shape)$ df = df.merge(pd.get_dummies(df['Date'].dt.strftime('%B')),left_index=True,right_index=True,how='left')$ print ('After Month',df.shape)$
import os$ os.environ["THEANO_FLAGS"] = "mode=FAST_RUN,device=cuda0,floatX=float32"
for c in ccc:$     for i in vhd[vhd.columns[vhd.columns.str.contains(c)==True]].columns:$         vhd[i] /= vhd[i].max()
type2017.isnull().sum() 
df.iloc[0:3, [0, 2]]
files = glob.glob(os.path.join(input_folder, year_string))
!rm world_bank.json.gz -f$ !wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz
mapInfo = refl['Metadata']['Coordinate_System']['Map_Info'].value$ mapInfo
t1= twitter.copy()$ t2 = pred.copy()$ t3 = tweet_df.copy()
mean = np.mean(data['len'])$ print("The length's average in tweets: {}", format(mean))$
new_df = pd.read_sql_query('select * from actor where first_name = "Groucho"', engine)$ new_df.head()$
prcp = session.query(prcp.date, prcp.prcp).\$     filter(prcp.date > first_date).\$     order_by(prcp.date).all()
combine_Xtrain_ytrain = pd.concat([X,y],axis=1)$ combine_Xtrain_ytrain = combine_Xtrain_ytrain.drop(['date','subjects','word_count','full_text'],axis=1)$ combine_Xtrain_ytrain.head()
lb = aml.leaderboard$ lb
e.__class__
temp_data_query = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
df['MeanFlow_cms'].describe()
Irregularities_data = []$ time_hour_for_file_name = 0 #datetime.datetime.now().time().hour$
df_daily4=df_daily.groupby(["C/A", "UNIT", "STATION"]).DAILY_ENTRIES.sum().reset_index()$ df_daily4.head(5)$
df = pd.read_csv('ab_data.csv')$ df.head()
fuel_mgxs = mgxs_lib.get_mgxs(fuel_cell, 'nu-fission')
ride_level_data = ridedata_df.groupby(ridedata_df['city'])$ city_avg_fare = ride_level_data.mean().reset_index()$ city_nbr_rides = ride_level_data.count().reset_index()$
weather_df_byday = weather_df.loc[weather_df.index.weekday == weekday]$ weather_df_byday.info()$ weather_df_byday.head()
driver.get("http://www.reddit.com")$ time.sleep(1)
dfs_morning.loc[dfs_morning['ENTRIES_MORNING']<=0, 'ENTRIES_MORNING'] = dfs_morning.ENTRIES_MORNING.quantile(.5)
str(containers[0].find("li", {"class":"transaction"}).a["title"])
df = pd.read_sql("SELECT * FROM Sections ", con=engine)$ df.head(3)
s_median = s.resample('5BM').median()$ s_median
KKK = df.groupby(["Source"]).mean()["Compounded Score"]$ overallDF = pd.DataFrame.from_dict(KKK)$ overallDF["Compounded Score"]
df = df.merge(pd.get_dummies(df['Device']),left_index=True,right_index=True,how='left')$ print ('Device',df.shape)
y_prob = gnb.predict_proba(X_clf)$ print(y_prob.shape)
stock_dict = {}$ for data in r.json()['dataset']['data']:$     stock_dict[data[0]] = dict(zip(r.json()['dataset']['column_names'][1:], data[1:]))
h1 = qb.History(360, Resolution.Daily)$ h1;
df_cs = pd.read_csv("costco_all.csv", encoding="latin-1")
sample = df.sample(n=1000, replace=False)
ans = pd.pivot_table(df, values='E', index=['A','B'], columns=['C'])$ ans
print("Today's Date is:", dt.date.today())$ query_date = dt.date.today() - dt.timedelta(days=365)$ print("One year ago today is:", query_date)$
data_AFX_X.describe()['High']
utils.fix_coordinates(data)
train_data, test_data, train_labels, test_labels = train_test_split(spmat, labels, test_size=0.10, random_state=42)  
df[['Footnote']].isnull().sum()$
for user in targets:$     clean_sentiments = clean_sentiments.append(news(user))
user_df = pd.read_csv('verified_user.csv')$ print(len(user_df))$ print(user_df.created_at.min())$
mentions = api.GetMentions()$ print([m.text for m in mentions])
outname = 'user_dataset_results.csv' # You can rename this file to whatever you'd like. Using the same output filename multiple times could cause overwriting, be careful!$ fullpath = os.path.join('./output', outname)  $ combined_df.to_csv(fullpath, index=False)
data_df['tone'] = data_df.apply(tone, axis=1)
data_2018 = pd.read_csv(filename_2018)
df.price_doc.hist(bins=100)$
df = pd.read_sql('SELECT * from booking', con=conn_b)$ df
created_table=pd.merge(new_table,y_axis_value)$ created_table.head()
model.score(combined_transformed_array, y_test)
mars_table = table[0]$ mars_table.columns = ["Parameter", "Values"]$ mars_table.set_index(["Parameter"])$
pd.set_option('display.max_columns', 100)$ tmpdf
temps_df[temps_df.Missoula > 82]
print(len(distance_list))$ station_distance.shape
files = os.listdir(os.getenv('PUIDATA') + '/archives')
import builtins$ builtins.uclresearch_topic = 'GIVENCHY' # 226984 entires$ from configuration import config
import nltk.data$ tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
MEDIAN_daily_trading_volume = mydata['Traded Volume'].median()$ MEDIAN_daily_trading_volume
df['sales_type'] = df['sales_type'].str.replace(u'\xa0', u' ')$
y_clf = np.where(y_tr >= 1800, 1, 0)$ print(np.where(y_clf==0)[0].shape)$ print(np.where(y_clf==1)[0].shape)
data['Traded Volume'].mean()
len(kochdf.loc[kochdf['user'] == "Rezeptsammlerin"]['name'])
Jarvis_resistance_simulation_1 = Jarvis_ET_Combine['Jarvis(Root Exp = 1.0)']$ Jarvis_resistance_simulation_0_5 = Jarvis_ET_Combine['Jarvis(Root Exp = 0.5)']$ Jarvis_resistance_simulation_0_25 = Jarvis_ET_Combine['Jarvis(Root Exp = 0.25)']
print("# of files in unsubscribed :", filecount(os.fspath(master_folder + lists + "unsubscribed/")))$ print("# of files in members :", filecount(os.fspath(master_folder + lists + "members/")))$ print("# of files in cleaned :", filecount(os.fspath(master_folder + lists + "cleaned/")))
google_stock.corr()
rural_ride_total = rural_type_df.groupby(["city"]).count()["ride_id"]$ rural_ride_total.head()
print([X.shape[0],y.shape[0]])
df_2013['bank_name'] = df_2013.bank_name.str.split(",").str[0]$
calls_nocontact_2017.to_csv("311_calls_new.csv")
words_hash_scrape = [term for term in words_scrape if term.startswith('#')]$ corpus_tweets_scraped.append(('hashtags', len(words_hash_scrape))) # update corpus comparison$ print('Total number of hashtags: ', len(words_hash_scrape)) #, set(terms_hash_stream))
FREEVIEW.plot_histogram(raw_fix_count_df)
exiftool -csv -createdate -modifydate ciscij10/CISCIJ10_cycle2.mp4 ciscij10/CISCIJ10_cycle3.mp4 ciscij10/CISCIJ10_cycle4.mp4 ciscij10/CISCIJ10_cycle5.mp4 ciscij10/CISCIJ10_cycle6.mp4 > ciscij10.csv
etsamples_100hz.iloc[0:1]
p_value = scipy.stats.chi2_contingency(full_contingency)[1]$ print(p_value)
df_reindexed = df.reindex(index=[0,2,5], columns=['A', 'C', 'B'])$ df_reindexed
DummyDataframe2 = DummyDataframe[["Tokens","Token_Count"]].copy()$ DummyDataframe2 = DummyDataframe2[["Tokens","Token_Count"]].groupby('Date').agg({'Tokens': 'sum', 'Token_Count': 'sum'})$ DummyDataframe2
df1=data.rename(columns={'SA':'Polarity'})$ df1.head()
y.mean()
df.shape$ df.to_csv('ny_times_url_dataframep3.csv', index=False,encoding = 'utf-8')$
test_pred_svm = lin_svc_clf.predict(test_cont_doc)
data.describe()
s = gp.GeoSeries([Point(x,y) for x, y in zip(delays_geo['Longitude'], delays_geo['Latitude'])])$ delays_geo['geometry'] = s$ delays_geo.crs = {'init': 'epsg:4326', 'no_defs': True}
optimizer = tf.train.AdagradOptimizer(0.01 )$ train = optimizer.minimize(loss)$
!mv 9 blast.txt
topLikes = data[(data.Likes >= data.Likes.max()-1200)]$ topLikes$
import pandas as pd$ d = pd.read_json('testtweets.json')$ d.head()
grouped_date = merged_table.groupby(['Date']).agg({"Likes": "sum","Retweets": "sum","Compound":"mean",$                                                       "Negative":"mean","Neutral":"mean","Positive":"mean"}).apply(list).reset_index()$ grouped_date
metadata = df_small[['order_num', 'date', 'country']].drop_duplicates()$ metadata.head()
df_2011['bank_name'] = df_2011.bank_name.str.split(",").str[0]$
train_pos = train_sample.filter(col('is_attributed')==1)$ n_pos = train_pos.count()$ print("number of positive examples:", n_pos)
airline_training_set = nltk.classify.util.apply_features(extract_features, airline_tweets)$ NBClassifier = nltk.NaiveBayesClassifier.train(airline_training_set)
uniq_sorted_churned_plans_counts = sorted(uniq_churned_plans_counts,key=lambda x:x[0].tolist())
ml_repository_client = MLRepositoryClient(service_path)$ ml_repository_client.authorize(username, password)
columns = ['id', 'datetime', 'created_at', 'text', 'file', 'twitter_handle']$ df = pd.DataFrame(columns=columns)
con = cx_Oracle.connect(connection_string)
plt.pie(totalRides, labels = labels, explode = explode, $         colors = colors, autopct="%1.1f%%", shadow=True, startangle=180)$ plt.show()
typesub2017['Solar'].sum() 
at_messages = non_na_df[non_na_df.message_text.str.startswith('@')]
[[scores.loc[:,[row]].mean(),scores.loc[:,[row]].sum()]for row in scores]
%matplotlib inline$ commits_per_year.plot(kind='line', legend=False, title='commits per year' )
import os$ place = os.chdir("..")$
reqs.count()
df['start date'] = df.index.map(lambda x:x.start_time)$ df
a.
miss = df.isnull().values.any()$ print ("Missing Values : {}".format(miss))
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY$ r = requests.get(url)
tweets_df = pd.read_csv('C:\\Users\\ericr\\Google Drive\\schoolwork\\current classes\\CS 230\\Project\\230_crypto\\data\\twitter\\hourly\\labeled_tweets_hourly.csv', encoding='ISO-8859-1')$ print("There are {} tweets".format(tweets_df.shape[0]))$ tweets_df.head()
import matplotlib.pyplot as plt$ import seaborn as sns$ sns.set()
news_p = soup.find_all('p')$ for paragraph in news_p:$     print(paragraph.text)
val_idx = np.flatnonzero($     (df.index<=datetime.datetime(2018,4,3)) & (df.index>=datetime.datetime(2018,3,1)))$
reg_traffic_with_flags = search_algo(regular_traffic,honeypot_df,['id.orig_h','src'],['ts','ts_unix'])
cityID = '84229b03659050aa'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Virginia_Beach.append(tweet) 
df = df[df.fav >= 10]$ len(df)$
conn.commit()
top10_df_pd=top10_df.toPandas()$ top10_df_pd.head(10)
keep_day_threshold = 4$ n_user_days = n_user_days.loc[n_user_days >= keep_day_threshold].reset_index()$ n_user_days = n_user_days[['seqn']]
iris.head().iloc[:,:1]
cur = con.cursor()
tweets = megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False]$ megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False]['text'].count() # 895
%matplotlib inline$ import seaborn as sns$ sns.heatmap(pd.DataFrame(years).T.fillna(0))
measurements_df.to_csv('clean_hawaii_measurements.csv', index=False)
active_with_return.dropna(inplace = True)
print(spike_tweets.iloc[5000, 2])
! rm -rf recs3$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models3 --outdir recs3
sns.boxplot(x=df['liked'],y=df['Age_Years'],data=df,whis=np.inf)$
Top_tweets.groupby('sentiment').count()
df1.num_words.describe()$
dti = pd.to_datetime(['Aug 1,2014','2014-08-2','2014.8.3',None])$ for l in dti: print(l)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(therapist_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Therapists Hours First Difference Partial Autocorrelation')
flight6 = spark.read.parquet("/home/ubuntu/parquet/flight6.parquet")$
dfs_resample.reset_index(inplace = True)$ dfs_resample['TIME'] = pd.to_datetime(dfs_resample['DATE_TIME']).dt.hour$ dfs_morning = dfs_resample[(dfs_resample['TIME'] == 6) | (dfs_resample['TIME'] == 12)]
df.rename(columns={'Price':'House_Price'},inplace=True) # renaming inplace i.e. in that same data frame$ df.head()
Genres=",".join(Genres).join(("",""))
final_df.corr()["ground_truth_adjusted"][names]
predict.predict_score('Wikipedia')
df2.user_id.nunique()
hm_sub = grid_heatmap[grid_heatmap['pr_fire'] >= 0.25]$ fire_hm = hm_sub[['glat', 'glon', 'pr_fire']].values
outfile = os.path.join("Resource_CSVs","Main_data_revised.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
mgxs_lib.build_hdf5_store(filename='mgxs.h5', directory='mgxs')
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \$       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY$ req = requests.get(url)
cityID = 'cb74aaf709812e0f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tulsa.append(tweet) 
query.get_dataset(db, id=ds_info["DatasetId"][0])
alice_sel_shopping_cart = pd.DataFrame(items, index = ['glasses', 'bike'], columns = ['Alice'])$ alice_sel_shopping_cart
sc = spark.sparkContext$ access_logs_raw = sc.textFile('data/apache.log')
raw_data = raw_data.loc[raw_data['date'] >= pd.to_datetime(start_date)]$ raw_data = raw_data.loc[raw_data['date'] <= pd.to_datetime(end_date)]
result = pd.merge(df, ser, on = 'key').drop('key', axis = 1)$ result
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep ='\s+') # Index in the first column "col=0" and header on the first "row"$ df.head(5)$
def filter_special_characters(text):$     return re.sub('[^A-Za-z0-9\s;,.?!]+', '', text)$
connection = sqlite3.connect(':memory:')$ cursor = connection.cursor()$
trump_o.drop("text", axis = 1)$ trump_con = trump_o.drop(["favorite_count", "text", "id_str", "is_retweet", "retweet_count", "source", "created_at"], axis = 1)$ trump_con.shape
string_2018 = 'extracted_data/tmin.2018.csv'
json.dumps(letters)[:1000]
conn = pymysql.connect(host='localhost', port=3306, user='root', password='pythonetl', db='pythonetl')
session.query(func.count(Measurements.date)).all()
print("Exporting to %s"%(man_export_filename))$ extract_man_uw.to_csv(man_export_filename, index=False)
aapl = pd.read_csv(file_name, index_col='Date', usecols=['Date', 'Adj Close'])$ aapl.columns = ['AAPL']$ aapl
age = pd.Series(test_age)
forest_clf = RandomForestClassifier(random_state=0)$ forest_clf.fit(X_train, Y_train)
cdate=[x for x in building_pa.columns if 'date' in x]$ cdate$
collection = store.collection('NASDAQ.EOD')$ collection
datetime.time(datetime(2015,12,14,15,17,30))
for v in data.values():$     if v['answers']['Q1'] == 'yes':$         v['answers']['Q1A'] = 0
kmf.plot()$ plt.title('Kaplan Meier Fitter estimates')
group = {}$ for letters in list_of_letter:$     group[letters] = grouped_by_letter.get_group(letters)$
nodes_file = directory_name + 'nodes.h5'              # Contains information about every node$ node_models_file = directory_name + 'node_types.csv'   # Contains information about models
print(data["Ganhadores_Sena"].sum())$ print(data["Ganhadores_Quina"].sum())$ print(data["Ganhadores_Quadra"].sum())
tweets = pd.read_csv('tweets_mentioning_candidates.csv')$ tweets['set'] = 'test'$ tweets['polarity_value'] = np.NaN
detroit_census2=detroit_census.drop("Fact Note", axis=1)$ detroit_census2=detroit_census2.drop("Value Note for Detroit city, Michigan", axis=1)
vip_reason.columns = ['VIP_'+str(col) for col in vip_reason.columns]
events.loc['2017-01-10':'2017-01-11'].head(4)
df2017 = dfSubSites[dfSubSites.Year == 2017]$ df2017sites = df2017.groupby('MonitoringLocationIdentifier').mean()$ df2017sites.TotalN
def create_df_grouped_by_date( tweets_df ):$     return tweets_df.groupby( Grouper( 'date' )).mean()
aaplA01 = aapl['2012-01'][['Adj Close']]$ withDups = pd.concat([msftA01[:3], aaplA01[:3]])$ withDups
df_daily5=df_daily.groupby(["STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index()$ df_daily5.head(5)$
gpByDate = data_air_visit_data.groupby('visit_date').agg(np.sum)$
import numpy as np$ raw_data = df.as_matrix()$ print(raw_data.shape)
forecast_col = 'Adj. Close'$ df.fillna(value=-99999, inplace=True)$ forecast_out = int(math.ceil(0.01 * len(df))) #round off to the nearest value $
df2['user_id'].nunique()$ print ("Total Number of Unique row : {}".format(df2['user_id'].nunique()))
plt.savefig(str(output_folder)+'NB05_1_FC_'+str(cyclone_name)+'_'+str(location_name))
for_plt = tweets.set_index('created_at', drop=False, inplace=False)$ for_plt.created_at.groupby(pd.Grouper(freq='H')).count().plot(kind='bar', figsize=(20, 6))$
print('number of deaths in 2014:',df['2014']['battle_deaths'].sum())
mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % ('lead-reader', 'lead-reader', 'ds025991-a0.mlab.com', '25991', 'tra-ingestion')$ con = MongoClient(mongo_uri)$ db = con['tra-ingestion']
file = "../Data/output/sampleDataWithSentiment.csv" $ allData = pd.read_csv(file,index_col=None, header=0, engine="c", encoding='utf-8', low_memory=False)
import string$ from nltk.corpus import stopwords$ from nltk.tokenize import wordpunct_tokenize
result_merged.summary2()
games_df = pd.DataFrame(game_data_all).copy()
df.dtypes
df.head().to_json("../../data/stocks.json")$ !cat ../../data/stocks.json
result_set=session.query(Adultdb).filter_by(relationship='Not-in-family').all()
df_measurement.describe()
total_fit_time = (end_fit - start_fit)$ print(total_fit_time/60.0)
words_sk = [term for term in all_tokens_sk if term not in stop and not term.startswith('http') and len(term)>2]$ corpus_tweets_streamed_keyword.append(('meaningful words', len(words_sk))) # update corpus comparison$ print('Total number of meaningful words (without stopwords and links): ', len(words_sk))
Base = automap_base()$ Base.prepare(engine, reflect=True)
featured_image_url = 'https://www.jpl.nasa.gov/images/cassini/20170809/cassini20170809-16.jpg'
itemTable["Time"] = itemTable["Content"].map(time_effort)
mars_weather_tweet = weather_soup.find('div', attrs={"class": "tweet", "data-name": "Mars Weather"})
prcp_df.describe()
df.brewery_name.value_counts().head(5)
plt.savefig("SentimentScatterPlot.png")$ plt.show()
display(data.head(10))
sorted_results.describe()
df_tweets = pd.DataFrame(tweets)$ df_tweets
df_merge.head()
df_features2.to_csv("data new/featureslrfmp.csv", encoding="utf-8", sep=",", index=False)
! cp Observatory_Sauk_Incubator.ipynb /home/jovyan/work/notebooks/data/Incubating-a-DREAM/Sauk_JupyterNotebooks
stops.loc[~mask].head(10)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM$ pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724, open( "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p", "wb" ) )
df.head()
dfn = df.loc[df.period].copy()$ dfo = df.loc[~df.period].copy()$ dfn.shape, dfo.shape
g_goodbad_not_index = sl_data.groupby(['goodbad','AGE_groups'], as_index=False).sum()$ g_goodbad_not_index
df.text[df.text.str.len() == df.text.str.len().min()]
posts.groupby('from').aggregate(sum)
df['date'] = pd.to_datetime(df.date)$ df.set_index('date', inplace=True)
print ('Model Score Using the Training Data:\n',model.score(x1, y_train))
MICROSACC.plot_default(microsaccades,subtype="count/(6*20)")+ylab("microsaccaderate [1/s]")
from sklearn.feature_extraction.text import TfidfVectorizer # Import the library to vectorize the text$ tfidf_vect = TfidfVectorizer(ngram_range=(1,3), stop_words='english')$ tfidf_vectorized = tfidf_vect.fit_transform(df_train.text)
td_norm = td ** (10/11)$ td_norm = td_norm.round(1)
print("Number of mislabeled points out of a total %d points : %d" % \$       (X_clf.shape[0], (y_clf != y_pred).sum()))
type(t2.tweet_id.iloc[2])
base_df.describe()
validation.analysis(observation_data, Jarvis_resistance_simulation_0_25)
outfile = os.path.join("Resource_CSVs","Main_data_positive.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
cityID = '6a0a3474d8c5113c'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         El_Paso.append(tweet) 
url = "https://mars.nasa.gov/api/v1/news_items/?order=publish_date+desc"$ response = requests.get(url).json()$ body = response.get('items')[0]$
locations = session.query(Measurement).group_by(Measurement.station).count()$ print(f"There are",locations,"stations.")
us_grid = np.array(np.meshgrid(grid_lon, grid_lat)).reshape(2, -1).T$ np.shape(us_grid)
header = flowerData.first()$ flowerKV= flowerData.filter(lambda line: line != header)$ print (flowerKV.collect())
df2.drop(labels=1899, axis=0, inplace=True)
station_count = session.query(func.count(Station.station)).all()$ station_count[0]
print('Outliers are points below {} or above {}.'.format((scores_firstq - (1.5 * IQR)), (scores_thirdq + (1.5 * IQR))))
graf_train=pd.concat([graf_train, train_topics_df], axis=1)$ graf_test=pd.concat([graf_test, test_topics_df], axis=1)
np.array(df[['Visitors','Bounce_Rate']]).tolist()
to_datetime = lambda x: pd.to_datetime(x)
from scipy.optimize import curve_fit$ popt, pcov = curve_fit(sigmoid, xdata, ydata)$ print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
network_simulation[network_simulation.generations.isin([])]$
search_booking = search1[search1.booking == 1]$ search2 = search1.append([search_booking]*4,ignore_index=True)
df.info()
print ab_counts.first_name.values$ print type(ab_counts.first_name.values)$
np.random.seed(123456)$ ps = pd.Series(np.random.randn(12),mp2013)$ ps
df_R['Month'] = df_R['Date'].apply(lambda s:s.split('-')[1])
df = stories_df[((stories_df['fixVersions'].isin([reln, 'Backlog']) | pd.isnull(stories_df['fixVersions'])) & $                         (stories_df['status'].isin(['Code Review', 'In Progress', 'Approval', 'Closed'])) )]$ df[['key', 'status', 'fixVersions', 'summary']]
sorteios_2017 = data[data["Data Sorteio"] > "2017-01-01"]$ sorteios_2017.Ganhadores_Sena.sum()$
r_test = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-04-30&end_date=2018-04-30&api_key=YOURAPIKEY')$
team.isnull().sum()
data_df.clean_desc[15]
probarr2 = fe.toar(lossprob2)$ fe.plotn(fe.np.sort(probarr2), title="tmp-SORTED-prob")
ms_df = mt_df[mt_df['mortality_salience'] == 1]$ print("HI tweets between 18:07-18:21 AM 2019-01-13 labelled as mortality_salience: {}".format(len(ms_df)))$ print("percent mortality salience: {}".format(len(ms_df)/len(mt_df)))
data.dtypes
df[df.brewery_name == 'To Øl']
data_numeric = auto_new.select_dtypes(exclude="object")
df_insta=pd.read_csv('Instagram_Mobility_20_fixed.csv')$ df_insta=df_insta.dropna(subset=['coords'])$ df_insta.head()
data_issues_transitions=pd.read_csv('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-transitions.csv')
projects.actual_hours.sum()
def tweet_extend (tweet_id):$     $     return api.get_status(tweet_id, tweet_mode='extended')._json['full_text']
X_model = df_model.drop('liked',axis=1)$ Y_model = df_model.liked
precipitation = measure_avg_prcp_year_df['Avg Prcp'].describe()$ pd.DataFrame(precipitation, columns=['Avg Prcp'])
df_1 = df.drop(['AppID','AppName', '1', '2', '3', 'Device','DeviceType','Tags','Updated'], axis =1)$
lda_model = models.LdaModel(corpus=corpus, id2word=id2token, num_topics=20, update_every=0, passes=20)$ lda_model.save('/tmp/model.lda')$ lda_model = models.LdaModel.load('/tmp/model.lda ')$
article_urls = ['http://www.nhm.ac.uk/discover/the-cannibals-of-goughs-cave.html','http://www.nhm.ac.uk/discover/how-we-became-human.html','http://www.nhm.ac.uk/discover/the-origin-of-our-species.html']$
title_alt = imgs.find_all("img", class_="thumb")$ title_alt
grouper = df.groupby(['source account'])$ overall_score = grouper['compound score'].mean()$ overall_score
executable_path = {'executable_path': 'chromedriver.exe'}$ browser = Browser('chrome', **executable_path, headless=False)
from sqlalchemy.ext.automap import automap_base$ from sqlalchemy import create_engine
import ssbio.core.io$ my_saved_gempro = ssbio.core.io.load_json('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.json', decompression=False)
best_parameters, score, _ = max(GSCV.grid_scores_, key=lambda x: x[1])$ print('best parameters:', best_parameters)
%%time$ corpus = load_headline_corpus(verbose=True)$ print ('Headlines:', len(corpus.sents()))
df3 = tier1_df.reset_index()$ df3 = df3.rename(columns={'Date':'ds', 'Incidents':'y'})
(keys.shape, keys_0611.shape)
mRF.model_performance(test)
pd.DataFrame(hillary)
df_cities_clean = df_cities.loc[df_cities['lon']!='']$ df_cities_clean.to_csv('Cities_Data.csv',index=False)$ df_cities_clean.head()
percipitation_2017_df.plot()$ plt.show()
df2.head()$
print(df.shape)
df = pd.read_csv(data_location)$ df.head(5)
zz = z["uri"].groupby(pandas.Grouper(freq='M')).count()$ zz.plot()
ExtractData = twitter_api()$ tweets = ExtractData.user_timeline(screen_name="chintkap", count=200)$
total = scores.sum()$ scores[:2.75].sum()/total
cityID = 'e41805d7248dbf1e'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Modesto.append(tweet) 
service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'
recommendationTable_df = recommendationTable_df.sort_values(ascending=False)$ recommendationTable_df.head()
url_df=pd.DataFrame({'url':urls})
nitrodata['ActivityStartDate'] = pd.to_datetime(nitrodata['ActivityStartDate'],format="%Y-%m-%d")
finals[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 1)].PLAYER_NAME.value_counts()
temp_df['date'] = [dt.datetime.strptime(x, "%Y-%m-%d") for x in temp_df['date']]
theft.iloc[4]
active_unordered = unordered_df.loc[~churned_unord]
dir(friends_n_followers)$ help(friends_n_followers.sort_values)
X_new = pd.DataFrame({'TV': [40]})$ X_new.head()
predictions = forest.predict(test_data_features)$ output = pd.DataFrame( data={"id":test["id"], "rating":predictions} )
df = df[(df['state'] == 'successful') | (df['state'] == 'failed')]$ df['state'] = df['state'].replace({"failed":0,'successful':1})$ df['spotlight'] = df['spotlight'].replace({False:0,True:1})
payments_total_yrs.tail()$ payments_total_yrs.to_csv('Top Total Payees More than 1 Million Total.csv')$
most_temp_info = session.query(Measurement.station, Measurement.tobs).filter(Measurement.station == most_busy).all()$ most_temp_info
fig, ax = plt.subplots()$ df.groupby(['epoch', 'batch']).mean().plot(ax=ax)$ plt.show()
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf]).sample(10)
plot_q_table(q_agent_new.q_table)
weather1=pd.read_csv('data/Crime/weather1.csv')$ weather2=pd.read_csv('data/Crime/weather2.csv')$ print(weather1.shape, weather2.shape)
repeat_customer_purchase_timing['first last gap'].hist()
start_df['WorkDay'] = (start_df.index.weekday < 6) * 1$
utils.plot_user_steps(pax_raw, None, 2, 15)
df7['avg_dew_point Closed'].value_counts(dropna=False)
df_test = pd.read_hdf('bittrex.h5', 'bittrex_tb', where = ["mn_ix= 'BTC-VTC'"])$ df_test$
powers = np.linspace(0, -65, 66) # длинный скан
store_items.interpolate(method = 'linear', axis = 1)
station_distance.loc[:, ['Start Station Name', 'End Station Name',$                          'Start Coordinates', 'End Coordinates']].head(2)
df['profarea'] = df['specializations'].apply(lambda x: x[0]['profarea_name'])
result = grouper.dba_name.value_counts()
client.repository.ExperimentMetaNames.show()
pprint.pprint(treaties.find_one({"reinsurer": "AIG"}))
print("Unique users:", len(df2.user_id.unique()))$ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
web.DataReader("A576RC1A027NBEA","fred",datetime.date(1929,1,1),datetime.date(2013,1,1))
cityID = '018929347840059e'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Indianapolis.append(tweet) 
ppm_body = preProcessor_in_memory(hueristic_pct=.61, keep_n=6000, maxlen=60)$ vectorized_body = ppm_body.fit_transform(data_to_clean_body)
lon_us = lon[lon_li:lon_ui]-360$ lat_us = lat[lat_li:lat_ui]$ print(np.min(lon_us), np.max(lon_us), np.min(lat_us), np.max(lat_us))
nold = df2.query('landing_page == "old_page"').count()[0]$ print ("The population of Oldpage is : {}".format(nold))
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
print(df.iloc[0])
df.loc[6:10]
pd.read_csv("../../data/msft.csv", skiprows=100, nrows=5, header=0,names=['open','high','low','close','vol','adjclose'])
x = store_items.isnull().sum().sum()$ print(x)
X_train.shape
!head ../../data/msft2.csv  # Linux
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall()
titles = soup.find_all('div', class_='content_title')   $ print(titles)
precision = float(precision_score(y, gbc.predict(X)))$ recall = float(recall_score(y, gbc.predict(X)))$ print("The precision is {:.1f}% and the recall is {:.1f}%.".format(precision * 100, recall * 100))
df2= df2.sort_values(by=["rate"], ascending=False)$
all_sets.shape
y = api.GetUserTimeline(screen_name="berniesanders", count=20, max_id=935706980643147777, include_rts=False)$ y = [_.AsDict() for _ in y]
data["time_up_sec"] = pd.to_datetime(data["time_up_clean"], format= "%H:%M:%S")
from google.cloud import bigtable$ client     = bigtable.Client.from_service_account_json(JSON_SERVICE_KEY,project=project_id, admin=True)$ instance   = client.instance(instance_id)$
data['intra_day_diff'].max()
xml_in[xml_in['publicationKey'].isnull()].count()
ws = Workspace.from_config()$ print(ws.name, ws.resource_group, ws.location, sep = '\n')
primitives = ft.list_primitives()$ print(primitives.shape)$ primitives[primitives['type'] == 'aggregation'].head(10)
round((timelog.seconds.sum() / 60 / 60), 1)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
games_df.head()
new_stock_data = stock_data.drop('volume', axis = 1)$ print(display(new_stock_data.head()))
help(h2o.estimators.glm.H2OGeneralizedLinearEstimator)$ help(h2o.estimators.gbm.H2OGradientBoostingEstimator)
len(df[~(df.group_properties == {})])
status_data = status_data.dropna()
!ptdump -av 'data/my_pytables_file.h5'
oz_mask = oz_stops['stop_id'].isin(shared_ids)$ not_in_stops = oz_stops.loc[~oz_mask]$ not_in_stops
date = session.query(prcp.date).order_by(prcp.date.desc()).first()$ print(date)
a = []$ b = [item for item in new_tweets if any(term in item.text.lower() for term in search_terms)]$
df = pd.DataFrame(X)$ df['labels'] = pd.Series(y)$
wb.save('most_excellent.xlsx')
gDateEnergy = itemTable.groupby([pd.Grouper(freq='1W', key='Date'),'Energy'], as_index=True)$ gDateEnergy_content = gDateEnergy['Content']$
for i1 in range(2):$     writer = pd.ExcelWriter('Bitfinex/Bitfinex_2017.xlsx')$     df_table_Bitfinex.to_excel(writer, 'Bitfinex_2017', index = False, header = True)
twelve_months = session.query(Measurements.date, Measurements.prcp).filter(Measurements.date > year_before)$ twelve_months_prcp = pd.read_sql_query(twelve_months.statement, engine, index_col = 'date')
ride_percity=original_merged_table["type"].value_counts()$
(r_clean * hist_alloc).sum(axis=1).hist(bins=50)
df_final.columns
pst = ps.to_timestamp()$ pst.index
rng = pd.date_range('1/1/2018', periods=120, freq='S')$ len(rng)
DataSet = DataSet[DataSet.userTimezone.notnull()]$ len(DataSet)
url = 'https://mars.nasa.gov/news/'
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,100], ylim=[0,100], title="Favorites and Retweets:100x100")
precip_data = session.query(measurment).first()$ precip_data.__dict__
print('The data in Groceries is:', groceries.values)$ print('The index of Groceries is:', groceries.index)
df.info()
pd.value_counts(RNPA_existing['ReasonForVisitName'])
df4.sort_values(by='BG')
df_lampposts_loc['LOCAL'].value_counts()
filename = '../data/DadosBO_2017_1(FURTO DE CELULAR).csv'$ df_BOs = pd.read_csv(filename, sep=';', encoding='utf-8')$ df_BOs.info()
station_names = session.query(Station.station).all()$ station_names
list.remove(3)$ print(list)
new_page_converted.mean() - old_page_converted.mean()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new], nobs=[n_old, n_new], alternative='smaller' )$ z_score, p_value
data.map(add_one).collect()
kochdf = kochdf.dropna()
coinbase_btc_eur['Timestamp'] = coinbase_btc_eur["Time"].apply(lambda row: unix_to_datetime(row))
df_teacher_monitoring = df_teacher_monitoring[df_teacher_monitoring.awj_teacher_id.isin(pick_list)]$ df_teacher_monitoring.drop("comment", axis=1, inplace=True)
data = res.json()   
calls_nocontact.issue_type.value_counts()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_table(path, sep ='\s+', na_values=['.'], names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'class'])$ mydata.head(5)$
df.reset_index(inplace=True, drop=True)
importances=model_tree_6_b.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
df_concat["date_series"] = pd.to_datetime(df_concat["created_time"])$ df_concat["date_series"].head()
live_capture = pyshark.LiveCapture(interface='wlan1')
pd.read_csv("test.csv", encoding="utf-8").head()
title = soup.title.text$ print(title)
GamelogParser.GameLog().passing(urls[:1][0], season)
dbcon = sqlite3.connect("mobiledata.db")$
all_sets.describe()
pvt['ga:date'] = pd.to_datetime(pvt['ga:date'], format='%Y%m%d', errors='coerce')$ pvt.rename(columns={'ga:transactionId': 'transactionId', 'ga:date': 'date'}, inplace=True)$ pvt
experiment_run_uid = client.experiments.get_run_uid(experiment_run_details)$ print(experiment_run_uid)
import requests$ import json$ from pandas.io.json import json_normalize
pd.set_option('display.max_colwidth', -1)$ df[['text', 'favorite_count', 'date']][df.favorite_count == np.max(df.favorite_count)]
from gensim import models, similarities$ lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=100)
now = datetime.now() + pd.Timedelta('010:30:00')$ stories_df['Age In Days'] = stories_df.apply(lambda x: (now - x[x['status'] + ' Set To Date']).days, axis = 1)
therm_fiss_rate = sp.get_tally(name='therm. fiss. rate')$ fast_fiss = fiss_rate / therm_fiss_rate$ fast_fiss.get_pandas_dataframe()
grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)$ grid.map(plt.hist, 'Age', alpha=.5, bins=20)$ grid.add_legend();
files = glob.glob('../data/bases-completas/*.csv')
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']#['link_weight'].loc[97528]
groups_topics_df.shape, groups_topics_unique_df.shape
appointments['AppointmentCreated'] = pd.to_datetime(appointments['AppointmentCreated'], errors='coerce')$ appointments['AppointmentDate'] = pd.to_datetime(appointments['AppointmentDate'], errors='coerce')
tweets.head()
data = pd.read_csv('barbsList99.csv')$ data['order'] = data.apply(lambda x : 0,axis=1)$ data.to_csv('barbsList99.csv',index=False,index_label=False)
scaler_fit.inverse_transform(predict_actual_df).shape
P = type.__new__(LetsGetMeta,'S',(),{})$ P.__class__.__class__
iris = pd.read_csv('data/iris.csv')$ print(iris.shape)
import statsmodels.api as sm$ df2['intercept'] = 1$ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']$
my_gempro.uniprot_mapping_and_metadata(model_gene_source='ENSEMBLGENOME_ID')$ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping)$ my_gempro.df_uniprot_metadata.head()
import re$ pbptweets.loc[pbptweets['text'].apply(lambda x: any(re.findall('Santos',x)))][['date','screen_name','text']]
last_commit_timestamp = git_log[git_log['timestamp'] $                                 < pd.to_datetime('today')].sort_values('timestamp', ascending=False).head(1)$ last_commit_timestamp
cust_vecs[0:,].dot(item_vecs).toarray()[0,:5]
closePrice = aapl['Close']$ closePrice.head(5)
cityID = '944c03c1d85ef480'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fresno.append(tweet) 
com_eng_df = ghtorrent.community_engagement('rails', 'rails')$ com_eng_df = com_eng_df.set_index(['date'])
df.head(3) # only 3 data rows
from h2o.estimators.gbm import H2OGradientBoostingEstimator
model_x.summary2() # For categorical X.
!pip install pandas-datareader$ !pip install --upgrade html5lib==1.0b8$
BTC = pd.concat([btc_wallet,gdax_trans_btc])
df_detail_download_total_time.shape
[t for t in df.in_response_to_tweet_id.tolist() if not isinstance(t, float)]
features = weather_features.merge(DC_features,left_index=True,right_index=True)
merged_data['due_day'] = merged_data['due'].dt.day$ merged_data['due_month'] = merged_data['due'].dt.month$ merged_data['due_year'] = merged_data['due'].dt.year
marketSeries = pd.Series(sp500["Dif"].values, index=sp500["Date"])$ marketSeries.plot();
ser7 = pd.Series([9,8,7,6,5,34,2,98])$ ser7.head(7) $ ser7.tail(2)
os.chdir(root_dir + "data/")$ filtered_df_fda_drugs.to_csv("filtered_fda_drug_reports.csv", index=False)
store_items.pop('new watches')$ store_items
predict.predict_score('Water_fluoridation')
df_wm = pd.read_csv("walmart_all.csv", encoding="latin-1")
day2int = {v.lower():k for k,v in enumerate(calendar.day_name)}$ day2int
df = pd.merge(applications,questions,on='applicant_id')$ df['response_'] = np.where(df['response'].isnull(),'No Response',df['response'])$ df.head()
df.sort_values (by='J')
my_stream.filter(track=['data'])
df2 = df2.drop_duplicates(subset=['user_id'], keep='first')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=ZxFGsokp-2_XeaAAkjne&start_date=2017-01-01&end_date=2017-12-31')
toyotaData=autoData.filter(lambda x: "toyota" in x)$ print (toyotaData.count())$
def words_at_start(index, df):$     tweet_text = df["Words"][index]$     return tweet_text.split()[:3]$
df.shape
np.log(points)
merge_table_df = pd.merge(spotify_df, numberOneUnique_df, on=['URL','Region'], how='left')
my_gempro.blast_seqs_to_pdb(all_genes=True, seq_ident_cutoff=.9, evalue=0.00001)$ my_gempro.df_pdb_blast.head(2)
test[['clean_text','user_id','predict']][test['user_id']==5563089830].shape[0]
iris.loc[:,"Species"] = iris.loc[:,"Species"].astype("category")
df.label.value_counts()
np.sum(x < 6, axis=1)
my_df_free1.iloc[100:110]
HYB, STD = set(HYB_customer_order_intervals), set(STD_customer_order_intervals)$ for customerID in HYB.intersection(STD):$     print(customerID)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
DummyDataframe2 = DummyDataframe2.apply(lambda x: update_values_category(x, "Tokens"), axis=1)$ DummyDataframe2
from sklearn import preprocessing$ min_max_scaler = preprocessing.MinMaxScaler()
start_t = '2018-05-01 00:00:00'$ end_t=pd.to_datetime('today')- timedelta(days=1)$ end_t1=str(end_t)$
urban_driver_total = urban_type_df.groupby(["city"]).mean()["driver_count"]$ urban_driver_total.head()$
df = DataObserver.build_simply(file_path= '/Users/admin/Documents/Work/AAIHC/AAIHC-Python/Program/DataMine/Reddit/json_data/Processed_DataFrames/r-news/DF-version_2/DF_v2.json')
from nltk.corpus import stopwords$ from sklearn.feature_extraction.text import CountVectorizer
Stations = Base.classes.stations$ Measurements = Base.classes.measurements
speeches_df3['text'] = [text.replace('\n', '').replace('  ', '') for text in speeches_df3['text']]
s.asfreq('3B', method='bfill').head(15)
conn.execute("SELECT * FROM posts WHERE id=?", (158743,)).fetchall()
sentiments_df.to_csv("NewsMood.csv")
df_selection = df_selection.dropna(how='any') 
from dotce.visual_roc import roc_curve, precision_recall_curve
df_m.loc[df_m["CustID"].isin([customer])]
current_weather=soup.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text').text$ current_weather
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
df.groupby(['year','month']).agg([sum])$
hs.setAccessRules(C_resource_id, public=True)
resource_id = hs.createHydroShareResource(title=title, content_files=files, keywords=keywords, abstract=abstract, resource_type='genericresource', public=False)
temp_long_df = pd.melt(temp_wide_df, id_vars = ['grid_id', 'glon', 'glat'],$                       var_name = "date", value_name = "temp_c")$ temp_long_df.head()
df_new['country'].value_counts()
Xfull = Xtrain.append(Xtest)$ yfull = ytrain.append(ytest)$ print(Xfull.shape, yfull.shape)
print("The tweet with more likes is: \n{}".format(data['Tweets'][fav]))$ print("Number of likes: {}".format(fav_max))$ print("{} characters.\n".format(data['len'][fav]))
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-04-24&end_date=2018-04-24&api_key=" + API_KEY)
print(new_df.isnull().sum())$
df.num_comments= df.num_comments.apply(lambda x:0 if x <= 74 else 1)
final.index=range(final.shape[0])
df.status_message.fillna('NA', inplace=True)$
events_df['event_time'].max(),events_df['event_time'].min()
print('Groceries has shape:', groceries.shape)$ print('Groceries has dimension:', groceries.ndim)$ print('Groceries has a total of', groceries.size, 'elements')
SCN_BDAY.scn_age.describe()
df['w'].unique()
sentimentDf["date"] = pd.to_datetime(sentimentDf["date"])$
BBC = news_df.loc[(news_df["Source Account"] == "BBCNews")]$ BBC.head(2)
X = dataset[dataset.columns[1:]]$ Y = np.array(dataset["label"])
primitives[primitives['type'] == 'transform'].head(10)
endog = data_df.Count$ exog = data_df[['TempC','WindSpeed','Precip']]$
amazon_review = pd.read_csv('amazon_cells_labelled.tsv',sep='\t') $ amazon_review
clean_sentiments['Date'] = pd.to_datetime(clean_sentiments['Date'])
print('Median trading volume during 2017 - {}'.format(list_median(traded_volumes)))
session.query(Adultdb).filter_by(education="9th").delete(synchronize_session='fetch')
tweetDF = sqlContext.createDataFrame(data, filtered_check_list)
df.describe(percentiles=[.5]).round(3).transpose()
requests.get(wikipedia_content_analysis)
unique_Taskers = len(sample['tasker_id'].value_counts())$ unique_Taskers
sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df.loc[sprintsWithStoriesAndEpics_df.groupby("key_story")["startDate"].idxmax()]$ sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[pd.notnull(sprintsWithStoriesAndEpics_df.index)]$
import exampleModule # This has one function called ex$ exampleModule.ex([4,5,6])
cat_sz = [(c, len(full_data[c].unique())) for c in cats]
for item in result_set:$     print(item.index,item.education)
affair_yrs_married = pd.crosstab(data.yrs_married, data.affair.astype(bool))$ affair_yrs_married
prcp_analysis_df.plot(figsize = (18,8), color='blue', rot = 340 )$ plt.show()$ 
store_items = store_items.append(new_store, sort=False)$ store_items
df_4_test.columns =  ['Hospital', 'Provider ID', 'State', 'Period', 'Claim Type', 'Avg Spending Hospital', 'Avg Spending State', 'Avg Spending Nation', 'Percent Spending Hospital', 'Percent Spending State', 'Percent Spending Nation']$ print df_4_test
theft.iloc[0:5]
x_vals2 = [item + 0.3 for item in x_vals]$ plt.bar(x_vals2, quadratic_data, width=0.3)$ 
tweet_archive_clean = tweet_archive.copy()$ tweet_image_clean = tweet_image.copy()$ tweet_df_clean = tweet_df.copy()
if (kaggle|sim): test = test.reset_index(drop=True)
predictSVR = svr_rbf.predict(np.concatenate((X_train,X_validation,X_test),axis=0))$ predictANN = ANNModel.predict(np.concatenate((X_train,X_validation,X_test),axis=0))#one can ignore this line if ANN is $
print(reg1.score(X_train, y_train))$ print(reg1.score(X_test, y_test))
top_chater = non_na_df['sender_card'].value_counts()$ top_chater.nlargest(20).plot(kind='bar', figsize=(14,6))
gene_df['gene_id'].unique().shape
n_old = df2[df2.group == "control"].count()[0]$ print("The population of user under treatment group: %d" %n_old)
data_df.iloc[535]
store_items = store_items.rename(index = {'store 2': 'last store'})$ store_items
flight_hex = h2o.H2OFrame(flight_pd)
afx_volume = [day[6] for day in afx_dict['dataset_data']['data']]$ afx_avg_vol = sum(afx_volume)/len(afx_volume)$ print("The average daily trading volume for AFX_X in 2017 was " + str(round(afx_avg_vol,2)) + ".")
coinbase_btc_eur_min=coinbase_btc_eur.groupby('Timestamp', as_index=False).agg({'Coin_price_EUR':'mean', 'Coin_volume':'sum'})
trump_month_distri.plot(kind='bar', figsize=(10,5), rot= 45,title="# of Twitters of Donald Trump")$ plt.savefig('fig/trump_month.png');
df = pd.DataFrame()$ variables = ['text','created_at','source','user']$ df = pd.DataFrame([[getattr(i,j) for j in variables] for i in tweets], columns = variables)
lm.rsquared
data_FCInspevnt_latest = data_FCInspevnt.loc[data_FCInspevnt['Inspection_number'] == 1]$ data_FCInspevnt_latest = data_FCInspevnt_latest.reset_index(drop=True)$ data_FCInspevnt_latest.head(15)
df['log_price']=np.log(df['price_doc'].values)
raw.columns
print('number of contributions with missing candidate name: ',len(dat[dat.CAND_NAME.isnull()==True]))$ print('number of candidate ids for contributions with missing candidate name: ',len(pd.unique(dat[dat.CAND_NAME.isnull()==True].CAND_ID)))
import pandas as pd$ loctweetdf = tweetdf.loc[~pd.isnull(tweetdf['lat'])]
twitter_url = "https://twitter.com/marswxreport?lang=en"$ browser.visit(twitter_url)
dfDay = dfDay[(dfDay['Date'].dt.year == 2018) | (dfDay['Date'].dt.year == 2019)]
cityID = '2a93711775303f90'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Milwaukee.append(tweet) 
url = 'https://www.dropbox.com/s/yz5biypudzjpc12/PSI_tweets.txt?dl=1'$ location = './' #relative location for Linux, saves it in the same folder as this script$ download_file(url, location)
engine = create_engine('mysql+mysqldb://spmb:appoloodatavag@cw-spmb.ct4csmrmjrt7.us-west-2.rds.amazonaws.com/spmb_101')$
kayla['SA'] = np.array([ analyze_sentiment(tweet) for tweet in kayla['Tweets'] ])$ kelsey['SA'] = np.array([ analyze_sentiment(tweet) for tweet in kelsey['Tweets'] ])
cityID = '389e765d4de59bd2'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Glendale.append(tweet) 
api = twitter.Api(consumer_key = consumer_key, consumer_secret=consumer_secret,$                  access_token_key=access_token,access_token_secret = access_token_secret,$                   sleep_on_rate_limit=False)
dfDir = '/home/parkkeo1/reddit_unlocked/Isaac/DataFrames/RedditData_Nov-06-2017'$ print_df = pd.read_pickle(dfDir)$ print_df$
print(type(df_final['created_time'][0]))$ pd.__version__
a.iloc[:3]
bucket.upload_dir('data/city-util/raw', 'city-util/raw', clear_dest_dir=True)
ts.dt.weekday
os.environ.get('FOO')
print data_df.clean_desc[20]$ print 'becomes this ---'$ print text_list[20]
Base.prepare(engine, reflect=True)
df2 = df2.add_suffix(' Created')$ df4 = pd.merge(df,df2,how='left',left_on='Date Created',right_on='date Created')$
%sc$ !wget 'https://s3.amazonaws.com/crimexyz/crime.csv'
end_time = dt.datetime.now()$ (end_time - start_time).total_seconds()
df2['low_tempf'].mean()
df_full = df_full[list(DATA_L2_HDR_DICT.values()) + ['school_type']]
import qgrid$ qgrid.set_grid_option('forceFitColumns', False)$ qgrid.set_grid_option('editable', False)
table_rows = driver.find_elements_by_tag_name("tbody")[24].find_elements_by_tag_name("tr")$
import gspread$ from oauth2client.service_account import ServiceAccountCredentials$ from operator import itemgetter
rng2017 = pd.date_range('2017 Jan 1 00:00', periods = 12, freq = 'MS')$ rng2018 = pd.date_range('2018 Jan 1 00:00', periods = 12, freq = 'MS')$ rng2019 = pd.date_range('2019 Jan 1 00:00', periods = 12, freq = 'MS')
df = pd.read_csv('../nba-enhanced-stats/2016-17_teamBoxScore.csv')$ df.head()
data = data.drop_duplicates(subset=['name'], keep=False)
tlen = pd.Series(data = data.len.values, index = data.Date)$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
csvWriter = csv.writer(csvFile)
matt2 = dta.t[(dta.b==40) & (dta.c==2)]
tweet_df_polarity = tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
cityID = '0a0de7bd49ef942d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Scottsdale.append(tweet) 
measurements_df.count()
precip = session.query(Precip.date, Precip.prcp, Precip.station).filter(Precip.date >= date_ly)
year_prcp = session.query(Measurements.date, Measurements.prcp).order_by(Measurements.date).filter(Measurements.date > year_ago)$
col_names = list(zip(df_test.columns, df_train.columns))$ for cn in col_names:$     assert cn[0] == cn[1]
from pysumma.utils import utils
sub_data = data[data["place"].notnull()]$ sub_data.head()
exiftool -csv -createdate -modifydate cisuabe5/cisuabe5_cycle1.MP4 cisuabe5/cisuabe5_cycle2.MP4 cisuabe5/cisuabe5_cycle3.MP4 cisuabe5/cisuabe5_cycle4.MP4 > cisuabe5.csv
%matplotlib inline$ commits_per_year.plot(kind='line', title='Commits per year', legend=False)
transform = am.tools.axes_check(np.array([x_axis, y_axis, z_axis]))$ b = transform.dot(burgers)$ print('Transformed Burgers vector =', b)
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]$
df_imputed_median_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.median())
tfav2 = tfav.values$ SA2 = SA1 * 100000
scores_mean = np.mean(raw_scores)$ scores_std = np.std(raw_scores)$ print('The mean is {:.5} and the standard deviation is {:.5}.'.format(scores_mean, scores_std))
c = b[b == 7].index.get_level_values('user_id').tolist()$
filename_2018 = os.path.join(input_folder, string_2018)
print("Unique users:", len(df2.user_id.unique()))$ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
print(airquality_melt.head())
mp2013 = pd.period_range('1/1/2013','12/31/2013',freq='M')$ mp2013
pax_raw.paxcal.value_counts() / len(pax_raw)
data.groupby(['Year'])['Salary'].sum()
Measurement = Base.classes.measurement$ Station = Base.classes.station$
t1.
search_key_words = "\"" + search_key_words.replace(" ","+") + "\""$ df['newspaper_search'] = df['newspaper'].apply(lambda x: x.replace("/","%2F"))
data = allocate_equities(allocs=[  0.25 , 0.25,  0.25 , 0.25],dates=dates)$ data.plot(),$ plt.show()$
days_alive / pd.Timedelta(nanoseconds=1)
df.plot(subplots=True)$ plt.show()
tweets['time_eastern'] = tweets['created_at'].apply(lambda x: x.tz_localize('UTC').tz_convert('US/Eastern'))
predict_day = weather_x.index[-60]
store_items.loc[['store 1']]
stories = pd.concat([stories.drop('tags', axis=1), tag_df], axis=1)$ stories.head()
data = data.sort_values(by=['time'])
df_en.to_csv('bitcoin_eng.csv') # this has all 4 bitcoins$
doc_duration = doc_duration.resample('W-MON').sum()$ RN_PA_duration = RN_PA_duration.resample('W-MON').sum()$ therapist_duration = therapist_duration.resample('W-MON').sum()
d = datetime.date(1492, 10, 12)$ d.strftime('%A')
resdf.iloc[:,114:129].to_sql('demotabl',conn)
df.sort_index(axis=0, ascending=True)
data=requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=%s" %API_KEY)
def my_scaler(col):$   return (col - np.min(col))/(np.max(col)-np.min(col))$ data_scaled = data_numeric.apply(my_scaler)
def tokenizer(text):$     return text.split()
cwd = os.getcwd()$ cwd
y = list(train_10m_ag.is_attributed)$ X = train_10m_ag.drop(['is_attributed'],axis=1)$ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt"$ df = pd.read_table(path, sep ='\s+', na_values=['.'])$ df.head(5)
gene_count_df = chromosome_gene_count.to_frame(name='gene_count').reset_index()$ merged_df = gdf.merge(gene_count_df, on='seqid')$ merged_df
building_pa = pd.read_csv('Building_Permits.csv')$ building_pa.head(5)
r=pd.date_range(start='8/14/2017', periods=5)$ r
from pyspark.sql.types import DoubleType, IntegerType$ for col_name in data.columns:$     data = data.withColumn(col_name, data[col_name].cast(DoubleType()))
ave_sentiment_by_company = unique_sentiments_df.groupby("Symbol")["Compound"].mean()$ ave_sentiment_by_company
indx_weather=pd.read_csv(working_folder+indx_stn)
tdf = pd.read_csv('ksdata.csv', index_col=0)
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)$
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ req = requests.get(url)
leadsdf.to_csv('Leads App Report From 30-08-2017 to 02-09-2017.csv')$
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=JVTZ9kPgnsnq9oFbym2s&start_date=2018-05-17'$ r = requests.get(url)$ print(r.text)
cnn_df = constructDF("@CNN")$ display(constructDF("@CNN").head())
df_members['bd_c'] = pd.cut( df_members['bd'] , age_bins, labels=age_groups)
%time tsvd = tsvd.fit(train_4)
sites.dtypes
!pwd #location of the current working directory. $
predictions = model_rf.transform(test_data)$ evaluatorRF = MulticlassClassificationEvaluator(labelCol="trips", predictionCol="prediction", metricName="accuracy")$ accuracy = evaluatorRF.evaluate(predictions)$
df_times.head()$
!wget ftp://ftp.solgenomics.net/tomato_genome/annotation/ITAG3.2_release/ITAG3.2_RepeatModeler_repeats_light.gff -P $DATA_PATH
display(data.head(20))
unordered_timelines = list(chain(*np.where([min(USER_PLANS_df.iloc[i]['scns_created']) != USER_PLANS_df.iloc[i]['scns_created'][0] for i in range(len(USER_PLANS_df))])))
df_detail = df_detail.merge(df_sum_total, how='left', on='game_format_name')
list(bow_test.columns).index("word2vec_90")$ print((bow_test.columns))$
pickle_in = open("neuralNet.pickle","rb")$ ANNModel = pickle.load(pickle_in)
kick_projects = pd.merge(kick_projects, ks_ppb, on = ['category', 'launched_year','goal_cat_perc'], how = 'left')
df_day = endog.to_frame()$ df_day.rename(columns={"BikeID" : "Count"}, inplace=True)
id_of_tweet = 932626561966247936$ tweet = api.get_status(id_of_tweet)$ print(tweet.text)
p_x = x.copy()
df_release = pd.read_csv( pwd+'/releases.052317.csv', encoding='utf-8') #earning release?
all_tables_df.loc[0]
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt"$ mydata = pd.read_csv(path, sep= '\t')$ mydata.head(5)
pd.Timestamp('2014-12-15')
pd.pivot_table(bb_df, values = ["Wt"], index = ['Pos'], aggfunc = np.mean).sort_values(by = ['Wt'], ascending = False)
cityID = '946ccd22e1c9cda1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Pittsburgh.append(tweet) 
print(len(free_data.country.unique()))
siteFeatures = read.getSamplingFeatures(type='Site')$ df = pd.DataFrame.from_records([vars(sf) for sf in siteFeatures if sf.Latitude])
json_data = r.json()$ print(type(json_data))$
df['water_year2'] = df[['year','month']].apply($     lambda row: row['year'] if row['month'] < 10 else row['year'] + 1, $     axis = "columns")
for c in ccc:$     for i in vwg[vwg.columns[vwg.columns.str.contains(c)==True]].columns:$         vwg[i] /= vwg[i].max()
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)$ tf.summary.scalar("loss", loss)$ merged_summary_op = tf.summary.merge_all()
top50 = pd.read_csv('../top50visited.csv', sep=';')$ top50['dataset_slug'] = [x.split('www.data.gouv.fr/datasets/')[1] for x in top50.dataset]$ top50['dataset_id'] = top50.dataset_slug.map(datasets_slug_id)
print(df[df.B>0],'\n')$ print(df[df>0],'\n')
print(cons_df.head())$ print(cons_df.PCEC96.head())$ cons_df.PCEC96.plot()
img_path = os.getcwd()+ '/images/'
store_items = store_items.drop(['bikes'], axis=1)  # bikes column$ store_items
window = pdf.loc['2008-1-1':'2009-3-31']$ portfolio_metrics(window)$ window.plot();$
bc.set_index('newdate', inplace=True)
randomdata1 = randomdata.describe()$ print randomdata1
store_items.fillna(method='backfill', axis=0)
df.describe()
Station = Base.classes.station$ Measurements = Base.classes.measurements
tdf = sns.load_dataset('tips')$ tdf['size'].sample(5)
summed.fillna(method='pad')  # The NaN column remained the same, but values were propagated forward$
assert len(target_docs) == 200000, 'target_docs should be truncated to the first 200k rows to use the cached model.'$ fname = get_file(fname='kdd_lm_v2.h5', origin='https://storage.googleapis.com/kdd-seq2seq-2018/kdd_lm_v2.h5', )$ model = load_model(fname)
vocab = vectorizer.get_feature_names()$ dist = np.sum(train_data_features, axis=0)
import builtins$ builtins.uclresearch_topic = 'GIVENCHY'$ from configuration import config
kushy_prod_data_path = "products-kushy_api.2017-11-14.csv"$ kushy_prod_df = pd.read_csv(kushy_prod_data_path, low_memory=False)$ kushy_prod_df.head(10)
data_issues_csv.head()
type(t1.tweet_id.iloc[3])$
df.boxplot('MeanFlow_cms');
def yearMarkers(axis_obj, x_pos, **kwargs):$     axis_obj.axvline(x_pos, linestyle='--', color='w', alpha=.4, **kwargs)$ years = np.arange(0,len(grouped), 51)
data.recommendation_id.nunique()
x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)
portfolio_df.reset_index(inplace=True)$ adj_close_acq_date = pd.merge(adj_close, portfolio_df, on='Ticker')$ adj_close_acq_date.head()
cols_to_remove = ["multiverseid", "imageName", "border", "mciNumber", "foreignNames",$                   "originalText", "originalType", "source"]$ all_sets.cards = all_sets.cards.apply(lambda x: x.loc[:, list(set(x.columns) - set(cols_to_remove))])
df.isnull().sum()
psy_df2 = psy_hx.merge(psy_df, on='subjectkey', how='right') # I want to keep all Ss from psy_df$ psy_df2.shape
X = df.drop(ls_head.index('sr_flag'), axis='columns')$ X = X.drop(0, axis='columns')$ y = df[ls_head.index('sr_flag')]
df.loc['2017-02-01': '2018-03-01',['Adj. Close', 'Forecast']].plot(figsize=(20,10))
iris.head().iloc[:,[0]]
df[['beer_name', 'rating_score', 'simple_style', 'brewery_name', 'brewery_country']][df.rating_score < 2].sort_values('rating_score')
tweetsIn22Mar.head()$ tweetsIn1Apr.head()$ tweetsIn2Apr.head()
sns.violinplot(x=df['liked'],y=df['Time_In_Badoo'],data=df,whis=np.inf)$ plt.title('By Time In Badoo')$
df['StatusDate'] = pd.to_datetime(df['StatusDate'], format='%m/%d/%Y %I:%M:%S %p +0000') $ df['ResolutionTime'] = (df['StatusDate']-df.index).astype('timedelta64[h]') / 24
df3= df2.sort_values(by=["rate"], ascending=True)$ df3.head(100)
print(doc)$ print(doc.decode('windows-1252'))$ print(doc.decode('utf8'))$
donors[donors['Donor Zip'] == 606 ].head()
buckets_to_df(contributors.fetch_aggregation_results()['aggregations']['0']['buckets']).head()
X.shape
ward_df['Delays'].corr(ward_df['Crime Count'])
tfidf_matrix = tfidf.fit_transform(df_videos['video_desc'])$ tfidf_matrix.toarray()
LARGE_GRID.display_fixations(raw_large_grid_df, option='fixations')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY)
interests_groupby_user = df_interests['interest_tag'].groupby(df_interests['user_handle'])$ lst_user_interests = [[name, group.tolist()] for name, group in interests_groupby_user]$ lst_user_interests[1]
model.save('ubs_model.h5')  
S_distributedTopmodel.decision_obj.simulStart.value, S_distributedTopmodel.decision_obj.simulFinsh.value
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726 = test5result$ pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726, open( "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p", "wb" ) )
from dotce.report import (most_informative_features,$                           ascii_confusion_matrix)
feedbacks_stress.describe()
df2.to_csv('ab_data_new.csv', index=False)
top_songs['Month'] = top_songs['Date'].dt.month
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).order_by(Measurements.date).all()
pd.Timestamp('17:55')
tweets = pd.concat([tweets1,tweets2,tweets3])$ tweets.shape
%%time$ crime_geo.to_parquet(data_dir + file_name + '.parquet', compression='SNAPPY')
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False)$
from nltk.tokenize import sent_tokenize$ sentence = sent_tokenize(text)$ print(sentence)
sig_sq = np.var(d_3)$ mu = np.mean(d_3)$ scipy.stats.norm.interval(0.95, loc=mu, scale=sig_sq)
knn = KNeighborsClassifier(n_neighbors=20)$ print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())
r =requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2017-01-01&end_date=2017-12-31')$ print(r.json())
pd.options.display.max_colwidth = 400$ data_df[['clean_desc','sent_pola','sent_subj', 'tone']][data_df.tone == 'impolite']
df['x'].unique()
fin_coins_r.shape
options_frame.info()
df_movies = pd.merge(df, xml_in_sample, left_on=['movieId'], right_on=['authorId'],  how='left')[['movieId', 'authorName']]$ df_movies.drop_duplicates(subset=None, keep='first', inplace=True)
((pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days)*365
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets], columns=['Tweets'])$ display(data.head(10))
!ln -s -f ~/.local/lib/python2.7/site-packages/systemml/systemml-java/systemml-1.0.0-SNAPSHOT-extra.jar ~/data/libs/systemml-1.0.0-SNAPSHOT-extra.jar$ !ln -s -f ~/.local/lib/python2.7/site-packages/systemml/systemml-java/systemml-1.0.0-SNAPSHOT.jar ~/data/libs/systemml-1.0.0-SNAPSHOT.jar
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=As4uh8SmqPWoJ1s9XXDT&start_date=2017-01-01&end_date=2017-01-01"$ r = requests.get(url)
stations = session.query(Measurement).group_by(Measurement.station).count()$ print("There are {} stations.".format(stations))
x["sum"]=x[list(x.columns)].sum()$ x
X2 = PCA(2).fit_transform(X)
url = "https://twitter.com/marswxreport?lang=en"$ response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')
cityID = 'c47c0bc571bf5427'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Honolulu.append(tweet) 
titanic3['home.dest'].str.upper().head()
print('Before removing reactivations:',df.shape)$ df = df[df.Injury != 0]$ print('With only placements onto the Disabled List:',df.shape)
data['SA'] = np.array([analyse_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
sess = tf.Session()$ print(sess.run(result))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY+'&limit=1')
ward_df['Delay Time'].corr(ward_df['Crime Count'])
grid_df.tail()
old_page_converted = np.random.binomial(1, P_old, n_old)$ old_page_converted
pmean = np.mean([p_new,p_old])$ round (pmean, 4)
for tweet in results:$    print (tweet.text)
crimes[(crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE')&(crimes['SECONDARY_DESCRIPTION']=='TO PROPERTY')].head()$
dd.to_csv("processed_users_verified.csv", sep=',', encoding='utf-8')$
df.drop(['Unnamed:_13', 'addressee', 'delivery_line_2', 'ews_match', 'suitelink_match', 'urbanization', 'extra_secondary_number', 'extra_secondary_designator', 'pmb_designator', 'pmb_number'], axis = 1, inplace=True)$ df.drop([326625], axis=0, inplace=True)
tweet_data.sort_values(['favorite_count','retweet_count'], ascending=[False, False]).head(5)$
cabs_df_rsmpld = cabs_df_byday.resample('1M')['passenger_count'].count()$ cabs_df_rsmpld.head()
red_4.info()
result = cur.fetchall()$
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(doc_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours First Difference Partial Autocorrelation')
query = 'SELECT * FROM testdataset.wdbc ORDER BY index'$ dataset = pd.read_gbq(project_id='PROJECTID', query=query)$ dataset.head()
dbData.head(7)  # NaN's show up when the field has no data.  Need both masses, eccentricity, semimajor axis, $
X_new = test_tfidf.loc[:207]$ new_x = X_new.iloc[:,index_smote]$ new_y = test_tfidf['y']
FIGURE_PREFIX = '../figures/'
lr2 = LogisticRegression(random_state=20, max_iter=10000, C= 1, multi_class='ovr', solver='saga')$ lr2.fit(X_tfidf, y_tfidf)$ lr2.score(X_tfidf_test, y_tfidf_test)
merged_data['tax'].fillna(0, inplace=True)
df_2012['bank_name'] = df_2012.bank_name.str.split(",").str[0]$
x = datetime.strptime(inner_list[0][0], '%Y-%m-%d')$ type(x.year)
stations = session.query(Station.name).count()$ print(f'There are total of {stations} stations in Hawaii'.format())
festivals['lat_long'] = festivals[['latitude', 'longitude']].apply(tuple, axis=1)
df_Tesla['tokens'] =stemmed$ df_Tesla.head(2)
sl['mindate'] = sl.groupby('wpdx_id')["new_report_date"].transform('min')$ sl['maxdate'] = sl.groupby('wpdx_id')["new_report_date"].transform('max')
with pd.option_context('display.max_colwidth', 100):$     display(news_period_df[['news_collected_time', 'news_title']])
url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRlXVQ6c3fKWvtQlFRSRUs5TI3soU7EghlypcptOM8paKXcUH8HjYv90VoJBncuEKYIZGLq477xE58C/pub?gid=0&single=true&output=csv'$ df_hourly = pd.read_csv(url,parse_dates = ['time'],infer_datetime_format = True,usecols = [0,3])$ df_hourly.head()
cityID = '67b98f17fdcf20be'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Boston.append(tweet) 
news_df=pd.DataFrame({'Source Account':[],'Text':[],'Date':[],'Compound Score':[],'Positive Score':[],'Neutral Score':[],'Negative Score':[]})$ news_df.head()
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','OriginationPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['orange','green', 'blue', 'red', 'black'], grid=False)$
Station_id = session.query(Station.station).filter(Station.name == 'WAIHEE 837.5, HI US').all()$ Station_id
dr_existing_patient_8_to_16wk_arima = dr_existing_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']]$ dr_existing_patient_8_to_16wk_arima.index = dr_existing_patient_8_to_16wk_arima.index.date
model_artifact = MLRepositoryArtifact(model, training_data=train.select('ATM_POSI','POST_COD_Region','DAY_OF_WEEK','TIME_OF_DAY_BAND','FRAUD'),\$                                       name="Predict ATM Fraud")$ model_artifact.meta.add("authorName", "Data Scientist");
le_data_all.index.levels[0]
df_ct.head(2)
validation.analysis(observation_data, simple_resistance_simulation_0_25)
difference = total.xs_tally - absorption.xs_tally - scattering.xs_tally$ difference.get_pandas_dataframe()
aussie_search = api.search(q='%23Aussie')$ len(aussie_search)
pd.Series(0, index=days)
for columns in DummyDataframe2[["Positiv", "Negativ"]]:$     basic_plot_generator(columns, "Graphing Dummy Data using Percent" ,DummyDataframe2.index, DummyDataframe2)
df_birth['Continent'].value_counts(dropna=False)
df.groupby("newsOutlet")["compound"].min()
mean = np.mean(data['len'])$ print("The average length of all tweets is: {}".format(mean))
tweet_archive_clean['url'] = tweet_archive_clean.text.str.extract('(?P<url>https://.*/+[a-zA-Z0-9]+)', expand=True)
calls_nocontact = calls[calls.location != '(530187.70942, 3678691.8167)']
for id in other_classifier_ids:$     response = visual_recognition.delete_classifier(classifier_id=id)
state_lookup.info()
my_df[my_df.isnull().any(axis=1)].head()
blink= condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="BLINK")
miner = TweetMiner(twitter_keys, api, result_limit=400)$ trump_tweets = miner.mine_user_tweets("realDonaldTrump")
counts,x,y = np.histogram2d(theta[:,0], theta[:,1], bins=[50,50], range=[[0,1],[0,1]])$ plt.imshow(counts, extent=(x[0],x[-1],y[0],y[-1]), origin='lower');
sdf.createOrReplaceTempView("tempTable")$ res.show()
INT = INT.drop(columns= ['Contact: First Name', 'Contact: Middle Name',$        'Contact: Last Name'])
frames = [bbc, cbs, cnn, fox, nyt]$ result = pd.concat(frames)$ result.head()
dta.query(("facility_type == 'Restaurant' and "$            "inspection_type == 'Complaint Re-Inspection'"))$
train, test = data.randomSplit([0.8,0.2], seed=6)$ train.cache()$ test.cache()
classif_varieties = set(ndf[y_col].unique())$ label_map = {val: idx for idx, val in enumerate(ndf[y_col].unique())}
rf = RandomForestClassifier(n_estimators = 30)$ rf.fit(X_train, y_train)$ rf.score(X_test, y_test)
gnb.__dict__
print('\n{0} Movie Recommendations for User = {1}' \$       .format(mov_vec[mov_vec == 0].shape[0], $               tmp_df[tmp_df.tmp_idx == usr_idx].index[0]))
predict.predict_score('Stuart_Bithell')
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
newdf.describe()
from config import config$ CONFIGS = config.Config.get(env='prod', caller_info=False)
df.shape    
min_IMDB = scores.IMDB.min()
ds_train = FileDataStream.read_csv(train_file, numeric_dtype=np.float32, sep='\t')$ print(repr(ds_train.Schema))$ print(ds_train.Schema)
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost'], axis=1)$ dfY = data['ooCost']
temp_df = active_psc_records[active_psc_records.company_number.isin(secret_corporate_pscs.company_number)]$ len(temp_df[temp_df.groupby('company_number').secret_base.transform(all)].company_number.unique())
df2.columns = ['c','d']$ df2
grpConfidence.count()
print(AFX_X_06082017_r.json())
conn = sqlite3.connect("geo.db")$
m.plot(forecast);
tobs_date = session.query(measurement.date, measurement.tobs).all()$ tobs_date                   
conn_SF.close()
cityID = '7c01d867b8e8c494'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Garland.append(tweet) 
data.loc[data.PRECIP.isnull()]
months['date'] = pd.to_datetime(months['starttime'])$ months.head()
%matplotlib inline$ commits_per_year['author'].plot( kind='line', title='Commits per Year', grid=True)
X=final_df.drop(['max', 'mean','std','sum'], axis=1)$ print("number of samples are= "+str(X.shape[0]) +" with number of features= "+str(X.shape[1]))
print (collData.getNumPartitions()) # this is the number of CPU cores$
result_set =session.query(Adultdb).filter(Adultdb.education.in_(['Masters', '11th'])).all()
df_t1 = df.query('landing_page == "new_page"').query('group == "treatment"')$ df_t2 = df.query('landing_page == "old_page"').query('group == "control"')$ df2 = df_t1.append(df_t2, ignore_index=True)
data[data.name == 'New EP/Music Development'].head()
xml_in_sample['authorId'].nunique()
news_t = soup.find("div", class_="content_title").text
text = nltk.Text(tokens)$ text.dispersion_plot([token for token, frequency in text_nostopwords])
import json$ list_of_issues_dict_data = [json.loads(line) for line in open('SPM587FA17issues.json')]
com_eng_df['issues_open'].plot()
ab_df.user_id.nunique()
from sklearn.decomposition import LatentDirichletAllocation$ from sklearn.feature_extraction.text import CountVectorizer
session.query(Measure.date).order_by(Measure.date.desc()).first()
raw_data = pd.read_csv('./activities_201802011009.csv')
import sqlite3$ conn = sqlite3.connect("database.db")$ cursor = conn.cursor()$
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,200], ylim=[0,200], title="Favorites and Retweets:200x200")
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
capa2017.head()
vac_start_date = '2015-09-01' # start date for Sep 1st 2015$ vac_end_date = '2015-09-16'$ data_start_date = dt.date(2015,9,1) - dt.timedelta(days=365)$
dfs[25].describe()
if 1 == 1:$     news_period_df = pd.read_pickle(config.NEWS_PERIOD_DF_PKL)
indices = ml.show_feature_importance(df, '3M')
df_users = pd.read_csv('../data/march/users.csv')$ df_levels = pd.read_csv('../data/march/levels.csv')$ df_events = pd.read_csv('../data/march/events.csv', skiprows=1, names=event_header, error_bad_lines=False, warn_bad_lines=True)     
filename1 = 'expr_3_nmax_32_nth_0.1_ns_0.01_04-19.csv'$ df = pd.read_csv('../output/data/expr_3/' + filename1, comment='#')$
displacy.serve(doc, style='ent')
client.repository.list_definitions()
mean_abs_dev = lambda x: np.fabs(x-x.mean()).mean()$ pd.rolling_apply(hlw,5,mean_abs_dev).plot();
import tweepy$ import pandas as pd$ import matplotlib.pyplot as plt
reddit = pd.read_csv(filename)$ reddit.drop('Unnamed: 0', axis = 1, inplace = True)$ reddit.head()
base_df.head()
sel = [Measurements.date, func.avg(Measurements.prcp)]$ initial_date = format_latest_date - timedelta(days=365) # This will be start date from 12 months before final date of 8/23/17$ prcp_data = session.query(*sel).filter((Measurements.date >= initial_date)).group_by(Measurements.date).order_by(Measurements.date).all()
bigdf = pd.concat(dfs)
most = dfrecent.head(50)
dfEtiquetas["date"] = dfEtiquetas["created_time"].apply(lambda d: datetime.strptime(d, '%Y-%m-%dT%H:%M:%S+%f').strftime('%Y%m%d'))
dti2 = pd.to_datetime(['Aug 1, 2014', 'foo']) $ type(dti2)
df_user = pd.read_csv(user_source)$ df_user.head()
tweets_df.head()
fundret.idxmax(), fundret[fundret.idxmax()]
%run '../forecasting/helpers.py'$ %run '../forecasting/main_functions.py'$ %run '../forecasting/ForecastModel.py'
type2017 = type2017.dropna() 
ts.dt.hour
for metric in a[:]:$     payload = "elec,id="+str(metric[0])+" value="+str(metric[2])+" "+str(pd.to_datetime(metric[3]).value // 10 ** 9)+"\n"$     r = requests.post(url, params=params, data=payload)
fit1_test = fh_1.transform(test.device_model)$ fit2_test = fh_2.transform(test.device_id)$ fit3_test = fh_3.transform(test.device_ip)
%matplotlib inline$ seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)
PRE_PATH = PATH/'models'/'wt103'$ PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'
plt.hist(review_df.fake_review)$ plt.show()
test[['clean_text','user_id','predict']][test['user_id']==1497996114].shape[0]
X['is_cat'] = X['title'].map(lambda x: 1 if 'cat' in x else 0)$ X['is_funny'] = X['title'].map(lambda x: 1 if 'funny' in x else 0)
final_rf_predictions = rf_v2.predict(test[:-1])
t1.tweet_id =t1.tweet_id.astype(str)
Precipitation_DF.describe()
Google_stock.tail()
all_df = test_set.append(properties)$ len(all_df)
file_name = str(time.strftime("%m-%d-%y")) + "-tweets.csv"$ tweet_df.to_csv("analysis/" + file_name, mode = 'w',encoding="utf-8",index = False)$ 
%matplotlib inline$ %pylab inline$ pylab.rcParams['figure.figsize'] = (20, 9)   # Change the size of plots
table = soup.table$ print (table)
dataset['text length'] = dataset['text'].apply(len)
svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf')$ svc.fit(X_tfidf, y_tfidf)$ svc.score(X_tfidf_test, y_tfidf_test)
my_gempro.set_representative_structure()$ my_gempro.df_representative_structures.head()
df_z= df_cb.groupby(["landing_page","group"]).count()$ df_z$
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
tcat = pd.read_csv(filepath + 'cat_tweets.csv')$ tdog = pd.read_csv(filepath + 'dog_tweets.csv')$ tcat.head()
merge_df['Screen A'] = pd.to_numeric(merge_df['Screen Size'].str.split('x').str[0])*pd.to_numeric(merge_df['Screen Size'].str.split('x').str[1])$ merge_df['Screen Q'] = pd.qcut(x=merge_df['Screen A'],labels=['small','medium','large'],q=3)
df = pd.DataFrame.from_records(mylist)$ df.head()
data_df.info()
hours.shape$
result = api.search(q='%23HarryPotter')$ len(result)
datecols = ["CreationDate"]$ for datecol in datecols:$     qs[datecol] = pd.to_datetime(qs[datecol], origin="julian", unit="D")
diff_df = df_df.reset_index(drop = False)$ diff_df
male = crime.loc[crime['Sex']=='M']$ male.head(3)
yc_new2.dropna()$ yc_new2.isnull().sum()
df_2008['bank_name'] = df_2008.bank_name.str.split(",").str[0]$
metrics = pd.read_json('../metadata/metrics.json')$ metrics
year1 = driver.find_element_by_name('2001')$ year1.click()
m = pd.read_json('njdevils_gr_posts.json',orient='records')$ m['created_date'] = m['created_time'].dt.strftime('%Y-%m-%d')$
df_defects = pd.read_excel('ncr_data.xlsx', index='Notification')$ df_defects
$sudo wget http://files.pushshift.io/reddit/comments/RC_2018-02.xz$
_delta=_delta.to_dict()
for dim in d.dimensions:$     print('%s:\t%s' % (dim, d.dimensions[dim]))
trial_parameter_nc = Plotting(S_distributedTopmodel.setting_path.filepath+S_distributedTopmodel.local_attr.value)$ trial_parameter = trial_parameter_nc.open_netcdf()$ trial_parameter['HRUarea']
pd.Series(PDSQ.min(axis=1)<0).value_counts()  # no
!wget -nv -O /resources/data/PierceCricketData.csv https://ibm.box.com/shared/static/reyjo1hk43m2x79nreywwfwcdd5yi8zu.csv$ df = pd.read_csv("/resources/data/PierceCricketData.csv")$ df.head()
avgvolume.sort()$ mediankey = int((len(avgvolume)/2))$ print('Median trading volume for the year was: ',avgvolume[mediankey])
active_stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\$                                 order_by(func.count(Measurement.station).desc()).all()$ active_stations$
df_grouped.drop(['passing_reading', 'passing_math'], axis = 1, inplace = True)$ df_grouped.columns
print("prediction: {0}, actual: {1}, 2018-03-31".format(y_pred_list[0], y_test_aapl[0,0]))$
duplicates = df_cust_data[df_cust_data['Email Address'].isin(df_cust_data['Email Address'].\$                     value_counts()[df_cust_data['Email Address'].value_counts()>1].index)]$ len(duplicates[duplicates['Email Address'] != "no email"])/2
top_10_authors = git_log.loc[:, 'author'].dropna().value_counts().head(10)$ top_10_authors
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)
crime_geo_df = crime_geo[geo_data_columns].compute()$ crime_geo_df.info()
data[['TMAX', 'TMED']].head()
sl['two_measures'] = np.where((sl.mindate!=sl.maxdate),1,0)
data = pd.read_csv("BreastCancer.csv")$ data.head()
req.text
fulldf['sentiment'] = np.array([sentiment(tweet) for tweet in fulldf['tweetText'] ])
df['Injury_Type'] = df.Notes.map(extract_injury)
arraycontainer = loadarray('test.hdf')$ adjmats = arraycontainer.data
df_select_cats = df_select.copy()$ df_select_cats = df_select_cats.groupby(['Categories'], as_index=False).mean()$ df_select_cats
raw_data_df.loc[raw_data_df['Income'] >= 1, 'Decrease_debt_Y_N'] = "Y"$ raw_data_df.head()
workclass = pd.read_sql(q, connection)$ workclass.head(10)
stacked=football.stack()$ stacked.head()$ stacked.unstack().head()
topLikes.sort_values(by='Likes', ascending=[False])
control_notAligned = ((df['group'] == 'control') & (df['landing_page'] != 'old_page'))$ control_notAligned.sum()
Z = np.random.randint(0,10,(3,3))$ print(Z)$ print(Z[Z[:,1].argsort()])
last_12_precip_df = pd.DataFrame(data=last_12_precip)$ last_12_precip_by_date_df  = last_12_precip_df.set_index("date")$ last_12_precip_by_date_df.head(2500)
df_u= df_vu.groupby(["landing_page","group"]).count()$ df_u$
number_of_commits =git_log['timestamp'].size$ number_of_authors = git_log.dropna(how='any')['author'].unique().size$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
member=pd.read_sql_table('flag_member_all', engine)
round((timelog.seconds.sum() / 60 / 60 / 24), 1)
added_series = pd.Series(index=daterange)$
Base.classes.keys()
songLink = 'https://www.youtube.com/embed/'+soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href'][9:]$ HTML('<iframe width="560" height="315" src='+songLink+' frameborder="0" allowfullscreen></iframe>')
help(web.DataReader)
basic_plot_generator("mention_count", "Saving an Image Graph" ,DummyDataframe.index, DummyDataframe,saveImage=True, fileName="dummyGraph")
predictions = rfModel.transform(testData)
csvHead = pd.read_csv('Data/TripData/JC-201706-citibike-tripdata.csv')
bots = df[df['author'].str.contains('Bot')]['author'].unique()
households_with_count.iloc[30:40, 15:]
tweets = pd.read_csv('df_tweets.csv', parse_dates=['created_at'], infer_datetime_format= True, low_memory=False,\$                     usecols=['id', 'created_at', 'hashtags'])
posts = db.posts # cria a coleção$ post_id = posts.insert_one(post) #insere dados$ post_id$
sentiments_pd.to_csv("Twitter_News_Mood.csv", index=False)
np.mean(df['converted'] == 1)
b.loc[1:7]
df_grouped.columns.tolist()
ts = df.groupby(pd.Grouper(key='created_at', freq='D')).mean()
a = tweets.apply(lambda row: countOcc(row['tokens']), axis=1)$ sorted_x = sorted(occ.items(), key=operator.itemgetter(1), reverse=True)$
GIT_LOCATION = \$ 'C:\\Users\\donrc\\AppData\\Local\\GitHub\\PortableGit_f02737a78695063deace08e96d5042710d3e32db\\cmd\\git.exe'$
topics = lda.get_term_topics('network')$ for t in topics:$     print(t)
pred_labels = rdg.predict(test_data)$ print("Training set score: {:.2f}".format(rdg.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(rdg.score(test_data, test_labels)))$
import seaborn as sns$ sns.factorplot('sex', data=titanic3, kind='count')
test_scores = run(q_agent_new, env, num_episodes=100, mode='test')$ print("[TEST] Completed {} episodes with avg. score = {}".format(len(test_scores), np.mean(test_scores)))$ _ = plot_scores(test_scores)
station_data = session.query(Stations).first()$ station_data.__dict__
df_2009['bank_name'] = df_2009.bank_name.str.split(",").str[0]$
close_price=[row[4] for row in daily_price if row[4] is not None]$ close_price.sort()$ print(close_price[-1]-close_price[0])$
sns.regplot(x=df['score'], y=df["comms_num"], $             line_kws={"color":"r","alpha":0.7,"lw":5});
daily_mcap_mat.resample('M').asfreq().index
log_user2 = df_log[df_log['user_id'].isin([df_test_user_2['user_id']])]$ print(log_user2)
cityID = 'c0b8e8dc81930292'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baltimore.append(tweet) 
df.query('MeanFlow_cfs < 50')
merged1.columns
float(df.converted.sum())/df_length
dates = pd.date_range('8/1/2017', periods=100, freq='W-MON')$ dates
result_2 = pd.concat([df1, df3], axis = 1, join_axes=[df1.index]) # concatenate one dataframe on another along columns$ result_2
%cd drive/CloudAI/nmt-chatbot$ !python utils/prepare_for_deployment.py
my_gempro.set_representative_sequence()$ print('Missing a representative sequence: ', my_gempro.missing_representative_sequence)$ my_gempro.df_representative_sequences.head()
import builtins$ builtins.uclresearch_topic = 'GIVENCHY'$ from configuration import config
typesub2017['Solar'] = typesub2017['Solar'].astype(int)$ typesub2017['Wind Offshore'] = typesub2017['Wind Offshore'].astype(int)$ typesub2017['Wind Onshore'] = typesub2017['Wind Onshore'].astype(int)
pd.set_option("max.rows", 10)$ result
df = pd.read_csv('weather_data_austin_2010 (1).csv')$ cols = ['Temperature','DewPoint','Pressure']$ df = df[cols]
Base.classes.keys()$
df_tweets.reset_index().to_pickle("../tweets_extended.pkl")
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Pacific').strftime('%D: %r') + " (PST)"$ date_min = date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Pacific').strftime('%D: %r') + " (PST)"
df_transactions['not_auto_renew'] = df_transactions.is_auto_renew.apply(lambda x: 1 if x == 0 else 0)
y_class_baseline = demo.get_class(y_pred_baseline)$ cm(y_test,y_class_baseline,['0','1'])
Quantile_95_disc_times_pay = df.groupby(['drg3','year']).agg([np.sum, np.mean, np.std])$ Quantile_95_disc_times_pay.head(8)$
test.nrows
db = client.nhl_db$ collection = db.articles
y_pred_test = crf.predict(X_test)$ metrics.flat_f1_score(y_test, y_pred_test,$                       average='weighted', labels=labels)
pax_raw.columns = [x.lower() for x in pax_raw.columns]
cursor = db.tweets.find({}, {'text':1, 'id':1,'user':1, 'hashtags':1,'_id': 0})$ df =  pd.DataFrame(list(cursor))$ df.head(3)
page = pb.Page(commons_site, 'Motín de Aranjuez 2016 06.jpg', ns=6)$ page.exists()
libraries_df = libraries_df.merge(libraries_metadata_df, on="asset_id")
%matplotlib inline$ cat_group_counts = df.groupby("category").size().sort_values(ascending=False)[:10]$ cat_group_counts.plot(kind="bar", title="Top 10 Meetup Group Categories")
tweet_df.count()
    return stemmer.stem(word)
avgPurchU = train.groupby(by='User_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseU'})$ train = train.merge(avgPurchU, on='User_ID', how='left')$ test = test.merge(avgPurchU, on= 'User_ID', how='left')
tNormal = len(df[ (df['bmi'] > 18.5) & (df['bmi'] < 25.0) ])$ tNormal
df = df.drop_duplicates(subset='id')
concat4 = pd.concat([free_sub,ed_level], ignore_index=True)$ merge4 = pd.merge(left=free_sub, right=ed_level, how='outer' ,left_index=True, right_index=True)
df_concensus_uaa['latest_consensus_created_date'] = pd.to_datetime(df_concensus_uaa['latest_consensus_created_date'], dayfirst=True)
twitter_df_wa=twitter_df[twitter_df.location=='Washington, DC']$ plt.plot(twitter_df_wa['created_at_time'], twitter_df_wa['retweet_count'], 'ro')$ plt.show()
df = pd.read_excel('weather_nan.xlsx')$ df
df['State'][df.CustomerID=='0000114883']='MA'
df.head(100)
df.loc[index_name]
top_songs['Date'] = pd.to_datetime(top_songs['Date'])
test_df = df[df["dataset"]=="test"]; test_df.shape
X_train1 = vctr1.fit_transform(united_list_good) # векторизируем весь текст - преобразованные тексты $
cityID = 'b046074b1030a44d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jersey_City.append(tweet) 
print(rhum_long_df['date'].min(), rhum_long_df['date'].max())
file_name='precios/AAPL.csv'$ aapl = pd.read_csv(file_name)$ aapl
fire_pred = log_mod.predict_proba(climate_vars)
new_comments_df = pd.read_csv('input/test.csv') # Replace 'test.csv' with your dataset$ X_test = test["comment_text"].str.lower() # Replace "comment_text" with the label of the column containing your comments
dc = datacube.Datacube(app='dc-nbart')
number_of_commits = git_log['timestamp'].count()$ number_of_authors = git_log["author"].nunique(dropna = True)$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
final_df.corr()["ground_truth_crude"][names]
file = open(resumePath, "r")$ resume = file.read()$ corpus.insert(0, resume)
b = 2. * np.random.randn(*a.shape) + 1.$ b.shape
model = gensim.models.Word2Vec(sentences, min_count=10)
output_fn = "NewsMedia.csv"$ result.to_csv(output_fn)
s2.mean(), s2.std(), s2.unique() # numpy functions ignore nan's
vacation_data_df=pd.DataFrame(vacation_data)$ rain_per_station = pd.pivot_table(vacation_data_df,index=['station'],values=['prcp'], aggfunc=sum)$ rain_per_station
df = pd.DataFrame(recentd, columns=['prcp'])$ df.head(20)
numero = json.loads(ejemplo_json).get('phoneNumbers')[1].get('number')$ print(numero)
tlen = pd.Series(data=df['len'].values, index=df['Date'])$ tfav = pd.Series(data=df['Likes'].values, index=df['Date'])$ tret = pd.Series(data=df['RTs'].values, index=df['Date'])
pd.merge(d1, d3, left_on='city', right_on='place').drop('place', axis=1)
engine.execute("SELECT  * FROM contractor").fetchall()
for title, artist in unique_title_artist[current_len:min(current_len+batch_size, len_unique_title_artist)]:$     youtubeurl = urllib.parse.quote_plus(YOUTUBE_URL_TEMPLATE.format(gc.search(title +' '+artist)), safe='/:?=')$     youtube_urls[str((title, artist))] = youtubeurl
desc_stats.to_excel('DescStats_v1.xlsx')
def join_df(left, right, left_on, right_on=None, suffix="_y"):$   if right_on is None: right_on=left_on$   return left.merge(right, left_on=left_on, right_on=right_on, suffixes=("",suffix))
h2o.init()$
df_corr = result.groupby(['type', 'scope'])['user'].sum().reset_index()$ display(df_corr.sort_values('user',ascending=False).head(10))$ plot2D(df_corr, 'type', 'scope','user')
prcp_df.describe()
pd.date_range('2014-08-01 12:10:01',freq='S',periods=10)
dfd['brand'] = [b.strip().title() for b in dfd.brand]$ dfd.brand.unique()
date_collected = datetime.strftime(nytimes_df['Date'][0], "%B %d, %Y")$
document_matrix = cvec.transform(my_df.text)$ my_df[my_df.target == 0].tail()
for row in my_df_large.itertuples():$     pass$
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(doc_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours First Difference Autocorrelation')
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
y = x.loc[:,"A"]$ y
totalfare_drivers_by_city= cityfare_driver.groupby(cityfare_driver['type']).sum().reset_index()$ citylabels = totalfare_drivers_by_city['type']$ totalarea_fares = totalfare_drivers_by_city['fare']
df.A
for item in rows:$     item.native_country='United-States'$ session.commit()
df.min()
precipitation_df = pd.DataFrame(precipitation_data, columns=['Date', 'Precipitation'])$ precipitation_df.set_index('Date', inplace=True, )$ precipitation_df
x_train = scaler.transform(x_train)$ x_test = scaler.transform(x_test)
datasets_co_occurence = paired_df_grouped[['dataset_1', 'best_co_occurence']].set_index('dataset_1').to_dict()['best_co_occurence']
from pyspark.sql import functions as F$ access_logs_df.select(min('contentSize'), avg('contentSize')).show()$ access_logs_df.describe(["contentSize"]).show()
clean_stations = pd.concat([stations, stat_info_merge], axis=1)
session.query(Measurement.station, func.count(Measurement.station).label('count')).\$                                   group_by(Measurement.station).\$                                   order_by('count DESC').all()
df2 = merged_pd.loc[:,['visitors', 'weekday', 'holiday_flg']]$ df2.groupby(['weekday','holiday_flg']).agg(['count',np.sum])
tweeter_url = 'https://twitter.com/marswxreport?lang=en'$ browser.visit(tweeter_url)
btc_wallet.plot(kind='line',x='Timestamp',y='BTC_balance_GBP', grid=True);
%run twitter_creds.py
df = pd.read_csv(SRC_FILE)$ df['CreatedDate'] = pd.to_datetime(df['CreatedDate'], format='%m/%d/%Y %I:%M:%S %p +0000')$ df.set_index('CreatedDate', inplace=True)
number_of_commits = git_log['timestamp'].count()$ number_of_authors = len(git_log['author'].dropna().unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
input_node_types_DF = pd.read_csv(input_models_file, sep = ' ')$ input_node_types_DF
import pandas as pd$ commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ print(commits_per_year.head())
tweet = result[0] #Get the first tweet in the result$ for param in dir(tweet):$ (param, eval('tweet.'+param))
alpha = 0.05$ np.random.seed(999)$ dist_n = (np.random.randn(10000) + 5) * 4 # +5 fixes mean, *4 fixes stdev
lgb_mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', $           objective = 'binary')$
for k,v in results.items():$     save_csv(k, v, '../data/matrix-geral/')
top_10_authors = git_log.loc[:, 'author'].dropna().value_counts().head(10)$ top_10_authors
(gamma_chart - gamma_chart.shift(1))['Risk'].plot()
tweets_original['full_text'] = tweets_original['full_text'].str.decode('utf-8')$ tweets_original['created_at'] = tweets_original['created_at'].str.decode('utf-8')
city_avg_fare = pd.merge(citydata_df, city_avg_fare, on='city', how='left')$
tweet_archive_clean.shape$
stocks['Date'] = stocks['Date'].dt.week
ax = plt.gca()$ ax.set_title('Quadratic ($x^2$) vs. Linear ($x$)')
prophet_model.fit(prophet_df)
dict_c1 = defaultdict(list)$ for i, elem in enumerate(clusters_kmeans_predict1):$     dict_c1[elem].append(united_list_good[i])
globalCityRequest = requests.get(globalCity_pdf, stream=True)$ print(globalCityRequest.text[:1000])
validation.analysis(observation_data, BallBerry_resistance_simulation_0_5)
train_df.columns[train_df.isnull().any()].tolist()
df.drop_duplicates(subset=['last_name'], keep='last')$
y=dataframe1['CCI_30']$ plt.plot(y)$ plt.show()
s = pd.Series(np.random.randn(len(rng)).cumsum(), index=rng)$ s.head()
data['count'].hist(bins=30)$ plt.show()
from fastai.fastai.structured import *$ from fastai.fastai.column_data import *
df['time'] = df['text'].str.extract(r'(\d?\d:\d\d\s?[a-z]{2})')
df2['intercept'] = 1$ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])$ df2.head()
crime['Sex'].value_counts()
data['TMAX'].head()
df2.loc['2018-01-18', 'high_tempf']
nytimes_df = constructDF("@nytimes")$ display(constructDF("@nytimes").head())
df3 = df3.dropna(how='all')$ print(df3.sample(5))
Base.prepare(engine, reflect=True)$
elms_all_0611 = elms_all_0604.append(elms_all, ignore_index=True)$ elms_all_0611.drop_duplicates('ACCOUNT_ID',inplace=True)
x=[0,1,2,3,4,5]$ network_simulation[network_simulation.generations.isin([5])]$
rain_df = pd.DataFrame(rain)$ rain_df.head()
y = df['comments']$ X = df['title']$ X = X.apply(lambda x: PorterStemmer().stem(x))
Inspection_duplicates = data_FCInspevnt_latest.loc[data_FCInspevnt_latest['brkey'].isin(vals)]$ Inspection_duplicates
zipcode = "nyc_zip.geojson"$ zc = gpd.read_file(zipcode)$ zc.shape
(data_2017_12_14[data_2017_12_14['text'].str.contains("felicidades", case = False)])["text"].head()
userMovies = moviesWithGenres_df[moviesWithGenres_df['movieId'].isin(inputMovies['movieId'].tolist())]$ userMovies
data.head()
cv_results = pd.DataFrame(rs.cv_results_)$ cv_results
f1 = df2['sizec'].values$ f2 = df2['breakfastc'].values$ X = np.array(list(zip(f1, f2)))$
df2.shape[0]$ print ("Total Number of row : {}".format(df2.shape[0]))
result = modelk.predict(X_test )$
bbc = news_sentiment('@BBCNews')$ bbc['Date'] = pd.to_datetime(bbc['Date'])$ bbc.head()$
df = df.dropna()
unique_users = set(orders.user_id.unique())$ selected_users = random.sample(unique_users, 20000)
model_ADP = ARIMA(ADP_array, (2,2,1)).fit()$
spp['season'] = spp.index.str.split('.').str[0]$ spp['term'] = spp.index.str.split('.').str[1]
ans = df.groupby('A').sum()$ ans
isinstance(df.date[1],datetime)
P_old = ab_df2['converted'].mean()$ print(P_old)
building_pa_prc_zip_loc['permit_type'].unique()
len([earlyScn for earlyScn in SCN_BDAY_qthis.scn_age if earlyScn < 3])
mapInfo_string = str(serc_mapInfo.value) #convert to string$ mapInfo_string.split?
measurement = Base.classes.measurements
feats_dict = dict()$ for i, name in enumerate(dummies_df.keys()):$     feats_dict[i] = name $
mngr = dsdb.ConnectionManager(dsdb.LOCAL, user="jacksonb")$ local = mngr.connect(dsdb.LOCAL)$ local._deep_print()
malebydate = male.groupby(['Date','Sex']).count().reset_index()$ malebydate.head(3)
qs = qs[qs.nwords > 5]
df_R['Year']=df_R['Date'].str.slice(0,4)$
def keras_rnn_predict(samples, empty=human_vocab["<pad>"], rnn_model=m, maxlen=30):$     data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty)$     return rnn_model.predict(data, verbose=0)
for item in top_three:$     print('{} has a std of {}.'.format(item[0], item[1]))
model.fit(x, ynum, epochs=25, batch_size=32,verbose=2)
bag = vect.fit_transform(df['lemmas']).toarray()$ bag
model.fit(X_train, y.labels, epochs=5, batch_size=32,verbose=2)
qt_convrates_toClosedWon.applymap(lambda x: "{0:.2f}%".format(x * 100.00))$
d.groupby(d['pasttweets'].str.len())['tweet_id'].count()
df.describe()
y_pred = crf.predict(X_valid)$ metrics.flat_f1_score(y_valid, y_pred,$                       average='weighted', labels=labels)
dreamophone_url = 'https://dreamophone.com/dream/2186'
df_proj_agg = df_proj[['ProjectId','DivisionName','UnitName','Borough','Sponsor','PhaseName',$  'ProjectType', 'Priority', 'DesignContractType', 'ConstructionContractType',$  'SourceSystem', 'MultipleFMSIds', 'DesignFiscalYear','ConstructionFiscalYear']].drop_duplicates()
tdf = sns.load_dataset('tips')$ tdf['data']= 1$ tdf.sample(5)
data.loc[data['hired']==1].groupby('category').num_completed_tasks.mean()
ved = pd.read_excel('input/Data.xlsm', sheet_name='51', usecols='A:W', header=12, skipfooter=4)
dist = np.sum(X, axis=0)$ for tag, count in zip(vocab, dist):$     print (count,tag)
df2 = df$ mismatch_index = mismatch_df.index$ df2 = df2.drop(mismatch_index)
df.data.head()
S_lumpedTopmodel.decision_obj.hc_profile.options, S_lumpedTopmodel.decision_obj.hc_profile.value
word_count = np.array([len(d.split(' ')) for d in docs])$ print('Done.')
from IPython.core.display import display, HTML$ display(HTML("<style>#notebook-container { margin-left:-14px; width:calc(100% + 27px) !important; }</style>"))
cleansed_search_df['Date'] = pd.to_datetime(cleansed_search_df['Date'], format="%Y%m%d", errors="coerce")$ cleansed_search_df
number_one_charts_df.to_csv("Desktop/Project-2/number_one_chart.csv", index=False, header=True)
first_id_time = datetime.datetime.utcfromtimestamp(last_id_time) - dateutil.relativedelta.relativedelta(hours=21)$ first_id_time = int(time.mktime(first_id_time.timetuple()))
xlfile = os.path.join(DATADIR,DATAFILE)$ xl = pd.ExcelFile(xlfile)                $ tmpdf = xl.parse(xl.sheet_names[0])       
sumTable = tips.groupby(["sex","day"]).mean()$ sumTable
compared_resuts.to_csv("data/output/logitregres.csv")
df_members.isnull().sum()  
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig8_left_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
pred = predict_class(np.array(theta), X_test_1)$ print ('Test Accuracy: %f' % ((y_test[(pred == y_test)].size / float(y_test.size)) * 100.0))
graph.run("CREATE CONSTRAINT ON (u:User) ASSERT u.user_name IS UNIQUE;")$ graph.run("CREATE CONSTRAINT ON (t:Tweet) ASSERT t.tweet_id IS UNIQUE;")$ graph.run("CREATE CONSTRAINT ON (h:Hashtag) ASSERT h.tag IS UNIQUE;")
fe.plots.plotqq(ss)
really_large_dataset = sc.broadcast(100)
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd.head()
!grep -A 50 "build_estimator" taxifare/trainer/model.py
r = requests.get(url)$ r.json()
df.isnull().sum()
q_agent_new.scores += run(q_agent_new, env, num_episodes=50000)  # accumulate scores$ rolling_mean_new = plot_scores(q_agent_new.scores)
y.end_time
pca = PCA(2)$ X = pca.fit_transform(new_df.ix[:,1:20])$
x.loc[:,["B","A"]]
plt.savefig('overall_media.png')$ plt.show()
daily_df['Price_Change'].value_counts()
corrplot(corr=r[coins_infund].loc[start_date:end_date].corr())$ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date)$ plt.show()
data.info()
contractor_clean['last_updated'].head()$
h4 = qb.History(spy.Symbol, 360, Resolution.Daily)$
from statsmodels.stats.diagnostic import acorr_ljungbox$ print('差分序列的白噪聲檢查结果為：', acorr_ljungbox(resid_713.values, lags=1)) 
kick_projects.loc[:,'goal_reached'] = kick_projects['pledged'] / kick_projects['goal'] # Pledged amount as a percentage of goal.$ kick_projects.loc[kick_projects['backers'] == 0, 'backers'] = 1 $ kick_projects.loc[:,'pledge_per_backer'] = kick_projects['pledged'] / kick_projects['backers'] # Pledged amount per backer.
data.iloc[:, [2, 3]].head()
posts.groupby(['from', 'hostname_clean'])['post'].aggregate(sum)
(elms_all.shape, elms_all_0611.shape)
data_FCInspevnt["Inspection_number"] = data_FCInspevnt.groupby("brkey")["in_modtime"].rank(ascending=False)$ data_FCInspevnt['Inspection_number'] = data_FCInspevnt['Inspection_number'].astype('int64')$ data_FCInspevnt.head(15)
!wget https://pjreddie.com/media/files/yolov3.weights$
stream.filter(track=['clinton','trump','sanders','cruz'])
plotdf.loc[:thisWeekHourly['hourNumber'].max(), :] = plotdf.loc[:thisWeekHourly['hourNumber'].max(), :].fillna(method='ffill')$ plotdf.loc[:thisWeekHourly['hourNumber'].max(), :] = plotdf.loc[:thisWeekHourly['hourNumber'].max(), :].fillna(0)
plt.plot(losses[:])$
materials_file = openmc.Materials([fuel, water, zircaloy])$ materials_file.export_to_xml()
print('Best Score: {}'.format(XGBClassifier.best_ntree_limit))
bd.reset_index(drop=True)
f_lr_hash_modeling2 = f_lr_hash_modeling2.withColumn('id', col('id').cast('long'))$ f_lr_hash_test = f_lr_hash_test.withColumn('id', col('id').cast('long'))
    example1_df.registerTempTable("world_bank")
df['age'].fillna(df.groupby(['gender'])['age'].transform(mean))
ab_df2.user_id.nunique()
pd.to_datetime(['2009/07/31', 'asd'], errors='ignore')
conn_laurel = psycopg2.connect("dbname='analytics' user='analytics' host='analytics.cv90snkxh2gd.us-west-2.rds.amazonaws.com' password='!TgP$Ol9Z&6QhKW0tmn9mOW5rYT2J8'")$
df_ml_features = df_reg.drop('isClosed',axis = 1)$ df_ml_target = df_reg['isClosed']
data['data'].keys()
daily_change={row[0]:row[2]-row[3] for row in price_list if row[1]!=None}$ max_daily_change=max(daily_change.items(), key=lambda k: k[1])[1]$ print("The largest change in any one day is "+str(round(max_daily_change,2)))
validation.analysis(observation_data, BallBerry_resistance_simulation_0_25)
df[['beer_name', 'brewery_name', 'rating_score']][(df.brewery_name.str.contains('Arcadia')) & (df.beer_name.str.startswith('IPA'))]
arrows = pd.read_csv('input/data/arrow_positions.csv', encoding='utf8', index_col=[0,1])
print(pd.to_numeric(countdf['number_votes']).sum())$ print(pd.to_numeric(countdf['number_votes']).sum()-pd.to_numeric(count1df['number_votes']).sum())$ print(pd.to_numeric(countdf['number_votes']).sum()-pd.to_numeric(count6df['number_votes']).sum())
groupedNews = sentiments_df.groupby(["User"], as_index=False)$ groupedNews.head()
tmax = tmax_day_2018.tmax[0]$ tmax.plot()
measure_df.head()$
cnf_matrix[1:].sum() / cnf_matrix.sum()
auth_endpoint = 'https://iam.bluemix.net/oidc/token'
plt.rcParams['figure.figsize'] = [16,4]$ plt.plot(pd.to_datetime(mydf3.datetime),mydf3.fuelVoltage, 'g.', markersize = 2);$ plt.xlim(datetime.datetime(2017,11,15),datetime.datetime(2018,3,28))
from nltk.stem.porter import PorterStemmer$ stemmed = [PorterStemmer().stem(w) for w in words]$ print(stemmed)
df2 = pd.DataFrame(np.array([[10, 11], [20, 21]]), columns=['a', 'b'])$ df2
mean = np.mean(data['len'])$ print("The average length of all tweets: {}".format(mean))
survey = resdf.iloc[:,:113]$ survey.insert(2,'LangCd',resdf.iloc[:,120])$ survey.to_sql('surveytabl',conn)
text_df.head()
df_test_index = pd.merge(df_test_index[event_list['event_start_at'] > df_test_user['created_on']],$                             log_user1[event_list['event_start_at'] > df_test_user['created_on']], on='event_id', how='left')$ df_test_index
print(len(labels.keys()))
le_data_all = wb.download(indicator="SP.DYN.LE00.IN",country=countries['iso2c'],start='1980',end='2012')$ le_data_all
img_url = f'https://www.jpl.nasa.gov{img_url_rel}'$ img_url
ecxels[['מארחת','אורחת']]=ecxels[['מארחת','אורחת']].applymap(fixer)$
data.columns
test_classifier('c0', WATSON_CLASSIFIER_ID_1)$ plt.plot(classifier_stats['c0'], 'ro')$ plt.show()
grid.grid_scores_
np.random.seed(1)$ rs = 1
twelve_months = session.query(Measurements.date, Measurements.prcp).filter(Measurements.date > year_before)$ twelve_months_prcp = pd.read_sql_query(twelve_months.statement, engine, index_col = 'date')
data_air_visit_data.loc[:,['air_store_id', 'visitors']].groupby('air_store_id').size()$
kick_projects_ip_scaled_ftrs = pd.DataFrame(preprocessing.normalize(kick_projects_ip[features]))$ kick_projects_ip_scaled_ftrs.columns=list(kick_projects_ip[features])
df = pd.read_csv(source_csv)$ df.head()
test = test.drop(columns="id")$ (train.shape, test.shape, submit.shape)
df['duration'] = np.round((df['deadline'] - df['launched']).dt.days / 7)
r6s = r6s[['created_utc', 'num_comments', 'score', 'title', 'selftext']]$ r6s['created'] = pd.to_datetime(r6s['created_utc'],unit='s')$ r6s = r6s[r6s['created']>datetime.date(2017,12,1)]
random_integers.max()
grouped2.size().unstack().plot(kind="bar", stacked=True, figsize=(8,6))$ plt.show()
df_2007['bank_name'] = df_2007.bank_name.str.split(",").str[0]$
temp_df = pd.DataFrame(temps)
x["A"]$ x.get("A")$ x.A # works only for column names that are valid Python variable names
vwg['season'] = vwg.index.str.split('.').str[0]$ vwg['term'] = vwg.index.str.split('.').str[1]
fit_time = (end_fit - start_fit)$ print(fit_time/60.0)
plt.style.use('ggplot')$ bb['close'].apply(rank_performance).value_counts().plot(kind='barh')
r_train, r_test, rl_train, rl_test = train_test_split(r_forest.ix[:,col], r_forest['bot'], test_size=0.2, random_state = 2)$ r1_train, r1_test, rl1_train, rl1_test = train_test_split(r_forest.ix[:,col1], r_forest['bot'], test_size=0.2, random_state = 2)$
typesub2017 = typesub2017.drop(['MTU','MTU2'],axis=1)
testing.to_csv('SHARE_cleaned_lists.csv', index=False)
DummyDataframe = DummyDataframe.set_index("Date").sort_index()$ DummyDataframe = DummyDataframe.groupby("Date").sum()
df1['Hour'] = pd.to_datetime(df1['Date'], format='%H:%M').dt.hour # to create a new column with the hour information$ df1.head()
exiftool -csv -createdate -modifydate cisrol12/cycle1_MVI_0032.mp4 cisrol12/cycle1_MVI_0033.mp4 cisrol12/cycle1_MVI_0034.mp4 cisrol12/cycle2_MVI_0035.mp4 cisrol12/cycle2_MVI_0036.mp4 cisrol12/cycle2_MVI_0037.mp4 cisrol12/cycle2_MVI_0038.mp4 cisrol12/cycle2_MVI_0039.mp4 cisrol12/cycle2_MVI_0042.mp4 cisrol12/cycle2_MVI_0043.mp4 cisrol12/cycle2_MVI_0044.mp4 cisrol12/cycle2_MVI_0045.mp4 cisrol12/cycle2_MVI_0046.mp4 cisrol12/cycle2_MVI_0047.mp4 cisrol12/cycle2_MVI_0048.mp4 cisrol12/cycle2_MVI_0049.mp4 cisrol12/cycle2_MVI_0050.mp4 cisrol12/cycle2_MVI_0051.mp4 cisrol12/cycle2_MVI_0052.mp4 cisrol12/cycle2_MVI_0053.mp4 cisrol12/cycle2_MVI_0054.mp4 cisrol12/cycle2_MVI_0055.mp4 cisrol12/cycle4_MVI_0075.mp4 cisrol12/cycle4_MVI_0076.mp4 cisrol12/cycle4_MVI_0078.mp4 cisrol12/cycle4_MVI_0079.mp4 cisrol12/cycle4_MVI_0080.mp4 cisrol12/cycle4_MVI_0081.mp4 cisrol12/cycle4_MVI_0082.mp4 cisrol12/cycle4_MVI_0083.mp4 cisrol12/cycle4_MVI_0084.mp4 cisrol12/cycle4_MVI_0085.mp4 cisrol12/cycle4_MVI_0086.mp4 cisrol12/cycle4_MVI_0089.mp4 cisrol12/cycle5_MVI_0095.mp4 cisrol12/cycle5_MVI_0096.mp4 cisrol12/cycle5_MVI_0097.mp4 cisrol12/cycle5_MVI_0098.mp4 cisrol12/cycle5_MVI_0100.mp4 cisrol12/cycle5_MVI_0101.mp4 cisrol12/cycle5_MVI_0102.mp4 cisrol12/cycle5_MVI_0106.mp4 > cisrol12.csv
windfield = gdal.Open(input_folder+windfield_name, gdal.GA_ReadOnly)$ windfield
exec(open("./secrets.literal").read())$ gh_user = "holdenk"$ fs_prefix = "gs://boo-stuff/"
russians_df = pd.read_csv('users.csv')
df = pd.concat([sanders_df, dtrump_df, jimcramer_df], ignore_index=True) # ignore index = True it will reset all the$
y_train_hat = model.predict(X_train)$ y_train_hat = squeeze(y_train_hat)$ CheckAccuracy(y_train, y_train_hat)
iris.loc[:,"Species"].cat.categories
store_items = store_items.drop(['store 2', 'store 1'], axis = 0)$ store_items
lda_tfidf.print_topics(num_topics=10, num_words=7)$
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)$
DF1 = pd.DataFrame(datePrecip, columns=["Date","Precipitations"])$ DF1.head()
mean_of_tweets = data.mean()
df_master=pd.read_csv('cleaned data/final_master.csv')
tesla['nlp_text'] = tesla.text.apply(lambda x: tokenizer.tokenize(x.lower()))$ tesla.nlp_text = tesla.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ tesla.nlp_text = tesla.nlp_text.apply(lambda x: ' '.join(x))
fig = ax.get_figure()$ fig.savefig('n5-exercise.svg')
tm_2050 = pd.read_csv('input/data/trans_2050_m.csv', encoding='utf8', index_col=0)
dfClientes.loc[dfClientes["CODIGOCLIENTE"] == "00028227742", "EDAD"] = 26
(obs_diff-np.mean(p_diffs))/np.std(p_diffs)
compound_df = pd.DataFrame([avg_compound]).round(3)$ compound_df
data.iloc[:, [2]].head()
df["mail ID"] = df["mail ID"].apply(lambda x: x.split('.')[0])$ df = df.set_index("mail ID")
tobs_df.hist(column='Temperature', bins=12)$ plt.ylabel("Frequency",fontsize=12)$
Measurements = Base.classes.measurements$ Stations = Base.classes.stations
potential = lmp.Potential('2015--Pascuet-M-I--Al--LAMMPS--ipr1.json')
r.json()
