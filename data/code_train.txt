sl[sl.status_binary==0][(sl.today_preds<sl.one_year_preds)].shape
from ditto.network.network import Network$ G=Network()$ G.build(m)
ts = df.groupby(pd.Grouper(key='created_at', freq='D')).mean()
compound_df.columns = target_terms$ compound_df.head()
recommendations = model.recommendProducts(2093760, 5)$ recArtist = set([r[1] for r in recommendations])
df = PredClass.df_model$ print df.dtypes.loc[df.dtypes == 'object']
session.query(stations).count()
DataSet.to_csv("file_name_final.csv", sep=",")
counts = df2['landing_page'].value_counts()$ counts
news_df = pd.DataFrame(average_sentiment_list).set_index("User").round(3)$ news_df.head()
[x for x in tweets_df.userLocation if 'Costa Rica' in x]$
import random$ sample = data.iloc[list(random.sample(range(data.shape[0]), 200)), :]$ sample.plot.scatter(x='WRank', y='WPts')
dj_df = dj_df[dj_df['company_ticker'] != 'V'] $ dj_df['quarter_start'] = pd.to_datetime(dj_df['quarter_start'])$ print(dj_df.info())
etsamples_100hz.iloc[0:1]
import calendar $ month2int = {v.lower():k for k,v in enumerate(calendar.month_name)}$ month2int    
import seaborn as sns$ sns.factorplot('sex', data=titanic3, kind='count')
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df = pd.DataFrame(recentd, columns=['prcp'])$ df.head(20)
volume_weather=general_volume.merge(df, left_on='ds' , right_on='Date')$ volume_weather.head()$ volume_weather=volume_weather[~volume_weather.T_avg.isnull()]
print(df['Confidence'].unique())
gbm_model.plot()
len(df[~(df.group_properties == {})])
df_ratings.drop('video_watched_type', axis=1, inplace=True)$ df_ratings.drop('rating_date_creation', axis=1, inplace=True)$ df_ratings
post_discover_new_customers_validate = sales_data_clean[~(sales_data_clean['Email'].isin(post_discover_new_customers['Email']))]$ post_discover_new_customers_validate[post_discover_new_customers_validate['Created at'] >= "2017-09-09"]
categorical = free_data.dtypes[free_data.dtypes == "object"].index$ free_data[categorical].describe()
ctd_df.dtypes
techmeme['nlp_text'] = techmeme.titles.apply(lambda x: tokenizer.tokenize(x.lower()))$ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: ' '.join(x))
for tweet in public_tweets:$     print (tweet.text + " : " + str(tweet.created_at) + " : " + tweet.user.screen_name + " : " + tweet.user.location) $     print (" ")
df["DATE_TIME"] = pd.to_datetime(df.DATE + " " + df.TIME, format="%m/%d/%Y %H:%M:%S")$ df.head(5)
fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 7,8], feature_names=X_test.columns, grid_resolution=70, n_cols=7)$ fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 10], feature_names=X_test.columns, grid_resolution=70, n_cols=7)$
plt.plot('date', "posts", label='FB posts')$ plt.legend(loc='best')$ plt.show()
y_pred = y_pred.argmax(axis=1)
total_num_stations = session.query(func.count(Station.station)).first()$ print(f"Total number of stations: {str(total_num_stations[0])}")
fe.bs.bootshow(256, poparr, repeat=3)$
treaties = db.treaties$ treaty_id = treaties.insert_one(treaty).inserted_id$ treaty_id
test.iloc[40:100]
df.sort_values(['operator', 'part'], inplace=True)
ts.shift(1,DateOffset(minutes=0.5))
uniq_sorted_churned_plans_counts = sorted(uniq_churned_plans_counts,key=lambda x:x[0].tolist())
cols = ['GP', 'GS', 'MINS', 'G', 'A', 'SHTS', 'SOG', 'GWG', 'HmG', 'RdG', \$         'G/90min', 'SC%', 'Year', 'PKG', 'PKA']$ goals_df[cols] = goals_df[cols].apply(pd.to_numeric)
cust_clust = crosstab.copy()$ cust_clust['cluster'] = c_pred$
coming_next_reason = coming_next_reason.drop(['[', ']'], axis=1)$ coming_next_reason = coming_next_reason.drop(coming_next_reason.columns[0], axis=1)
print(temp_nc)$ for v in temp_nc.variables:$     print(temp_nc.variables[v])
merged1.isnull().sum()
mv_lens.title.value_counts().head()
team.isnull().sum()
st_columns = inspector.get_columns('stations')$ for c in st_columns:$     print(c['name'], c["type"])$
p_diff = new_page_converted.mean() - old_page_converted.mean()$ print("{} is simulated value of pnew - pold".format(p_diff))
df_hdf = dd.read_hdf(target, '/data')$ df_hdf.head()
email_sku_count = pd.DataFrame(transaction_dates.groupby(['Email', 'Lineitem sku'])['Lineitem quantity'].sum()).reset_index()$ email_sku_count
my_tweet_df.count()
xlfile = os.path.join(DATADIR,DATAFILE)$ xl = pd.ExcelFile(xlfile)                $ tmpdf = xl.parse(xl.sheet_names[0])       
import statistics$ statistics.median(trading_vol)
engine = create_engine("sqlite:///hawaii.sqlite")
t3[t3['retweeted_status'].notnull()== True]
import statsmodels$ import statsmodels.api as sm$ import statsmodels.formula.api as smf
dfTickets.to_csv('all_tickets.csv', index=False, index_label=False)
RNPA_new_8_to_16wk_arima = RNPA_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ RNPA_new_8_to_16wk_arima.index = RNPA_new_8_to_16wk_arima.index.date
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']['authorCollaborators']#.loc[97545]
df_small = raw_data[raw_data.num_tubes < sets_threshold]$ df_sets = raw_data[raw_data.num_tubes >= sets_threshold]
x_min_max = pd.DataFrame({'x': [df['x'].min(), df['x'].max()]})$ x_min_max
value=ratings['rating'].unique()$ value
index = similarities.MatrixSimilarity(doc_vecs, $                                       num_features=topics)$ sims = sorted(enumerate(index[doc_vecs[6]]), key=lambda item: -item[1])
total3=total.ix[(total['RA0']<335) & (total['RA0']>225)]$ total3$
forecast = prophet_model.predict(future_dates)
c_date = scratch['created_at']$ c_date.shape
for f in read_in["files"]:$     fp = getattr(processing_test.files, f)$     print(json.load(open(fp.load())))
bar_outlets = avgComp["Target"]$ bar_Compound = avgComp["Compound"]$ x_axis = np.arange(0, len(bar_Compound), 1)
print 'Python Version: %s' % (sys.version.split('|')[0])$ hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER$ print 'HDFS filesystem running at: \n\t %s' % (hdfs_conf[0])
access_logs_df.createOrReplaceTempView('AccessLog')$
testdata = np.load(outputFile)$ data = pd.read_csv(inputFile, delimiter=',', header=None)
average_reading_score = df_students['reading_score'].mean()$ average_reading_score
weekly_tagsVideosGB = weekly_dataframe(TagsVideosGB)$ weekly_tagsVideosGB[0].head()
start = time()$ ldamodel_3 = Lda(doc_term_matrix, num_topics=16, id2word = dictionary, passes=25, chunksize=2000)  # with 16 topics$ print('CPU time for LDA_3: {:.2f}s'.format(time()-start))$
analyser.polarity_scores(twitter_datadf['text'][1])$
! rm -rf recs3$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models3 --outdir recs3
bp = USvideos.boxplot(column='like_ratio', by='category_name', vert = False)$
print(data.json()['dataset']['column_names'])
df.drop_duplicates(subset=['first_name', 'last_name'], keep='last')$
nb = MultinomialNB()$ nb.fit(X_train_total, y_train)$ nb.score(X_test_total_checked, y_test)
ten = pd.merge(left=free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].mean().idxmax(1).to_frame(), right = free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].median().idxmax(1).to_frame(), left_index=True, right_index=True)$ ten.columns = ['mean', 'median']$ ten
breakdown[breakdown != 0].sort_values().plot($     kind='bar', title='Russian Trolls Number of Links per Topic'$ );
trump_twitter = pd.read_json('/home/data_scientist/data/misc/trump_tweets_2017.json', encoding='utf8')$ trump_twitter.head()
common_words_file = open('common_words.txt')$ common_words = common_words_file.read().split(',')$
measurements_df=pd.read_csv(measurements, dtype=object)
Shoal_Ck_hr = Shoal_Ck_15min.resample('h', on=str('DateTime')).mean() $ Shoal_Ck_hr.head(10)$
df.index.get_level_values('Subcategory').unique()$
youtube_df= pd.read_csv("../Data/youtubeVid_main.csv",sep = ",")$ youtube_df["trending_date"] = pd.to_datetime(youtube_df["trending_date"] \$                                            , format = "%Y/%m/%d")
bldg_data_0 = bldg_data[bldg_data['255_elec_use']==0]$ bldg_data_0.groupby([bldg_data_0.index.year,bldg_data_0.index.month]).agg('count').head(100)$
%%time$ ddf = dd.read_parquet(data_dir + file_name + '.parq', index='Date')
yc200902_short = yc[::1000]$ yc200902_short.shape
df.describe(percentiles=[.5]).round(3).transpose()
test_classifier('c2', WATSON_CLASSIFIER_ID_3)$ plt.plot(classifier_stats['c2'], 'ro')$ plt.show()
year8 = driver.find_elements_by_class_name('yr-button')[7]$ year8.click()
data['team']$ data.team
pcpData_df.prcp.describe()
pd.options.display.max_colwidth = -1$ df[['Text', 'PP Text']]$
df_estimates_false['points'].hist(bins=50, figsize=(10,5))$ plt.show()$
print("Probability of user converting:", df2.converted.mean())
df_members.isnull().sum()  
Output_two = New_query.ss_get_results(sport='football',league='nfl', ep='team_game_logs', season_id='nfl-2017-2018',team_id='nyg')$ Output_two
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json')
fig,ax=plt.subplots(1,2,figsize=(15,3))$ ax[0].boxplot(joined['CompetitionOpenSinceYear'],vert=False)$ ax[1].boxplot(joined['CompetitionDaysOpen'],vert=False)
cityID = '7d62cffe6f98f349'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Jose.append(tweet) 
news_sentiment_df.to_csv("twitter_news_org_sentiment.csv", encoding="utf-8", index=False)
df_tidied = df_tidied.sort_values(by=["name2"])$ df_tidied = df_tidied.drop_duplicates()$ print("Note that close to half rows are removed by removing the duplicates. New shape is", df_tidied.shape)
data_numeric = auto_new.select_dtypes(exclude="object")
sentiments_pd = sentiments_pd[["Outlet", "Date", "Tweet", "Compound", "Positive", "Neutral", "Negative", "Tweets Ago"]]$ sentiments_pd.tail()
gdf = gdf[gdf['seqid'].isin(chromosomes_list)]$ gdf.drop(['start', 'end', 'score', 'strand', 'phase', 'attributes'], axis=1, inplace=True)$ gdf.sort_values('length').iloc[::-1]
pwd = os.getcwd()$ df_users = pd.read_csv( pwd+'/users.052317.csv', encoding='utf-8') #user, need to find a way to link them, since it is only individual record. $
plt.plot(weather['created_date'], weather['max'])$ plt.xticks(rotation='vertical')
with open('test.csv') as f:$     size=len([0 for _ in f])$     print("Records in test.csv => {}".format(size))
vec1.get_feature_names() #feature names in test sample.$
tobs_date_df = pd.DataFrame.from_records(tobs_date)$ tobs_date_df.head()
for i in range(len(review_df.rate)):$     review_df.iloc[i,4]=review_df.rate[i][:3]$     review_df.iloc[i,6]=review_df.review_format[i][7:]
soup.nav
with pd.option_context('display.max_rows', 150):$     print(news_period_df.groupby(['news_collected_time']).size())
df_f2.loc[df_f2["CustID"].isin([customer])]
import quandl$ quandl.ApiConfig.api_key = 'RGYoyz3FAs5xbhtGVAcc'
test_df.columns[test_df.isnull().any()].tolist()
!head -n 5 ProductPurchaseData.txt
pd.value_counts(Stockholm_data.tweet_created_at).reset_index()$ countData = pd.read_csv(path + "countData.csv")$ countData
df.query('Confidence == "A:e"')
with connection:$     cursor.executemany('INSERT INTO fib VALUES (?)',$                        [(str(x),) for x in fib(10)])$
df.iloc[1]
from pysumma.utils import utils$ import os
df_train = sales_by_storeitem(df_train)$ df_test['sales'] = np.zeros(df_test.shape[0])$ df_test = sales_by_storeitem(df_test)
df = pd.DataFrame({'Char':chars,'Target':y_true})
temporal_group = 'weekly'$ df = pd.read_csv('../data/historical_data_{0}.csv'.format(temporal_group))
test[['clean_text','user_id','predict']][test['user_id']==5563089830][test['predict']==12].shape[0]
with open("datasets/git_log_excerpt.csv", "r") as file:$     print(file.read())
import re$ df.loc[:,"message"] = df.message.apply(lambda x : " ".join(re.findall('[\w]+',x))) $
treatment_notAligned = ((df['group'] == 'treatment') & (df['landing_page'] != 'new_page'))$ treatment_notAligned.sum()
print(data.petal_length.median())
hawaii_station_df = pd.read_csv(r"\Users\Josue\Desktop\\hawaii_stations.csv")$
data.loc[[pd.to_datetime("2016-12-01"), pd.to_datetime("2016-12-03")]]
df_p3.loc[df_p3["CustID"].isin([customer])]
iso_join.fillna(99999, inplace=True)
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).tail(5)
transactions['items_total'].describe()
light_curve_fit_df.to_hdf('/Users/jmason86/Dropbox/Research/Postdoc_NASA/Analysis/Coronal Dimming Analysis/Example Fit Dimming Light Curve.hdf', 'light_curve_df')
pred_labels = rdg.predict(test_data)$ print("Training set score: {:.2f}".format(rdg.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(rdg.score(test_data, test_labels)))$
outputs = {}$ for key in predictions:$     outputs[key] = pd.DataFrame({id_label:ids, target_label:predictions[key]})
resdf.iloc[:,114:129].to_sql('demotabl',conn)
store_items.fillna(0)
import re$ pbptweets.loc[pbptweets['text'].apply(lambda x: any(re.findall('Santos',x)))][['date','screen_name','text']]
import nltk.data$
df.set_index('datetime',inplace=True)
sns.distplot(data['Age'], kde = False, bins = 60)
s.asfreq('8BM')
html_table = df.to_html()$ html_table
cdate=[x for x in building_pa.columns if 'date' in x]$ cdate$
df_data=pd.DataFrame({'time':(times+utcoffset).value[:-sa],'chips':final.chips.values})$
scores[:1.625].sum()
print(scratch.shape)$ scratch = scratch.dropna(subset=['age'])$ print(scratch.shape)
df.drop(df[['prospectid', 'ordernumber', 'ordercreatedate', 'dnatestactivationdayid', 'xsell_gsa', 'xsell_day_exact' ]], axis=1, inplace=True)
g8_groups.mean()
Y = np.ones((num_names,1))$ Y[df['Gender'] == 'F',0] = 0
attend_with.to_csv('../data/attend.csv')
lag_list = list(df.columns.values)
merged_data['tax'].fillna(0, inplace=True)
fig,ax=plt.subplots(1,2,figsize=(15,3))$ ax[0].boxplot(joined['Promo2SinceYear'],vert=False)$ ax[1].boxplot(joined['Promo2Days'],vert=False)
most_active_df = most_active_df.loc[most_active_df['date'] > year_ago]$ most_active_df.plot.bar
import pandas as pd$ dataset = pd.ExcelFile("basedados.xlsx")$ data = dataset.parse(0)
new_model = gensim.models.Word2Vec.load(temp_path)  
dr_new_8_to_16wk_arimax = dr_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ dr_new_8_to_16wk_arimax.index = dr_new_8_to_16wk_arimax.index.date
stocks.to_sql('stocks', engine, index = False, if_exists = 'append')
df_geo_insta['hour']=hours$ df_geo_insta['hour'].unique()$
avisos_online.head(1)$
data = TwitterData_Initialize()$ data.initialize("data/train.txt", is_spain = False)$ data.processed_data.head(10)
scores[3.5:].sum()/total
from ramutils.classifier.utils import reload_classifier$ classifier_container = reload_classifier('R1387E', 'catFR5', 1, mount_point='/Volumes/RHINO/')$ classifier_container.features.shape # n_events x n_features power matrix
words = [w for w in words if not w in stopwords.words('english')]$ print(words[:100])
data_by_date_df["date"] = data_by_date_df["date"].dt.strftime("%Y-%m")
data_df.clean_desc[22]
rows.describe()
from scipy.optimize import curve_fit$ popt, pcov = curve_fit(sigmoid, xdata, ydata)$ print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
training.index = range(608)$ test.index = range(153)$ training.head()
plt.title(f"Overall Media Sentiment Based on Twitter as of {curDate}")$ plt.xlabel("Outlets")$ plt.ylabel("Tweet Polarity")
comments = [x.text for x in soup.find_all('a', {'class':'bylink comments may-blank'})]
x.loc[:,["B","A"]]
if 1 == 1:$     news_period_df = pd.read_pickle(config.NEWS_PERIOD_DF_PKL)
df.iloc[0]
sp500.loc['MMM']
df.groupby("newsOutlet")["compound"].max()
import pandas as pd$ df = pd.read_csv(datafile)$ df.head(5)
df_artist.artist_genres = df_artist.artist_genres.apply(lambda x:",".join(map(str,x)))$ df_artist = pd.concat([df_artist.iloc[:,:-1],df_artist.artist_genres.str.get_dummies(sep = ',')], axis = 1)$ df_artist = df_artist.drop_duplicates(subset = 'artist_id').reset_index(drop = True)
precip_data = session.query(measurment).first()$ precip_data.__dict__
exl=pd.read_excel('Book1.xlsx', converters={'item#':str,'part#':str, 'deliverydate':pd.to_datetime})$
error = train_ALS(training_RDD, test_RDD, best_rank, seed, iterations, lambda_=0.01)$ print('For testing data the RMSE is %s' + str(error))
red_4['num_comments'].max()
Base.classes.keys()$
station_count.loc[(station_count['Count'] >= 2000)]
df = pd.read_csv('data_stocks.csv')
churn_df.shape$
datetime.now().toordinal() - datetime(1987, 1, 4).toordinal()
network_simulation[network_simulation.generations.isin([])]$
new_group=df.groupby(by=result)$ new_group
df['Injury_Type'].value_counts()
tweet_image_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']).value_counts()$
last_commit_timestamp = git_log[git_log['timestamp'] $                                 < pd.to_datetime('today')].sort_values('timestamp', ascending=False).head(1)$ last_commit_timestamp
tlen.plot(figsize = (16,4), color = 'r')
data_hpg_reserve['hpg_store_id'].drop_duplicates().count()
from bs4 import BeautifulSoup$ import requests$ url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
ctc = ctc.round(1)
contractor[contractor.contractor_number.duplicated() == True]
DataSet.tail(2)
file_name='precios/AAPL.csv'$ aapl = pd.read_csv(file_name)$ aapl
print(ndvi_nc)$ for v in ndvi_nc.variables:$     print(ndvi_nc.variables[v])
Magic.__dict__['__repr__'].__get__(None, Magic)$
print(temp_long_df['date'].min(), temp_long_df['date'].max())
greater_first = git_log[git_log['timestamp'] >= str(first_commit_timestamp.iloc[0]['timestamp'])]$ corrected_log = greater_first[greater_first['timestamp'] <= str(last_commit_timestamp.iloc[0]['timestamp'])]$ corrected_log.sort_values('timestamp')                 
all_cards = pd.DataFrame(data = None, columns = all_cards_columns)$ all_cards.rename_axis("name", inplace = True)$ all_cards.head()
with open('hashtags/hashtags.csv', 'w') as f:$     [f.write('{0},{1}\n'.format(tag, val)) for tag, val in tag_cloud.items()]
clf = RandomForestClassifier()$ clf.fit(x_train, y_train)
final_df.corr()["ground_truth_adjusted"][names]
np.mean(df['converted'] == 1)
df.loc[df['Sold_to_Party'] == '0000101663'].sample(10)['SalesOffice']
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ print("Last 20 tweets:")$ display(data.head(20))
train_df.columns[train_df.isnull().any()].tolist()
df1 = pd.read_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext_all.csv", encoding='ANSI', $                  index_col='tweet_id', $                  sep=',')$
df = pd.read_csv(project_path + '/data/raw/data.csv', index_col=0)$ df.head()
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])
plt.hist(review_length, bins =100)$ plt.show() # most reviews are short, only few reviews are very long.
psy_hx = psy_hx.dropna(axis=1, how='all')$
pd.core.common.is_list_like = pd.api.types.is_list_like$ import pandas_datareader.data as web
resultDS.to_csv(fileName, sep=delimiter, header=True, index = False)
tweet_group = tweet_df.groupby(['source']).mean().reset_index()$ tweet_group
query.get_dataset(db, id=ds_info["DatasetId"][0], columns="ndarrays", get_info_items=True)
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_15min'], axis=1)$ dfY = data['corrCost']
goog.plot(y='Close')
expanded_data = pd.read_json( (data['_source']).to_json(), orient='index')
very_pop_df = au.filter_for_support(popular_trg_df, max_times=5, min_times=3)$ au.plot_user_dominance(very_pop_df)
cell = openmc.Cell(cell_id=1, name='cell')$ cell.region = +min_x & -max_x & +min_y & -max_y$ cell.fill = inf_medium
News_title = Mars_soup.find('div',class_="content_title").a.text$  $ print(News_title)
kickstarters_2017 = pd.read_csv("ks-projects-201801.csv")$ kickstarters_2017.head()
Featured_image = image_soup.find('img',class_="fancybox-image")$ print (Featured_image)$
pd.DataFrame(hillary)
kmf.plot()$ plt.title('Kaplan Meier Fitter estimates')
a = np.arange(1, 11)$ a
page_html= uClient.read()$ uClient.close()
count_by_insertid = Counter(df.insert_id)$ insertid_freq = {x : count_by_insertid[x] for x in count_by_insertid if count_by_insertid[x] > 1 }$ print('Duplicated insert_ids: {}'.format(len(insertid_freq.keys())))
print(pd.unique(stops['operator'].ravel().tolist()))
merged2['AppointmentDuration'] = merged2['AppointmentDuration'] / 60.0
number_of_commits = git_log['timestamp'].count()$ number_of_authors = len(git_log['author'].dropna().unique().tolist())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df.to_csv('Tableau-CitiBike/TripData_2018_Winter.csv', index=False)
totmcap = mcap_mat.T.sum()$ fund_fraq_mcap = fundmcap / totmcap
df.isnull().values.any()
np.sum(x < 6, axis=1)
prediction_proba = grid.predict_proba(X_test)$ prediction_proba = [p[1] for p in prediction_proba]$ print(roc_auc_score(y_test, prediction_proba))
pdf.loc['2016-1-1':'2016-3-31'].plot()$
df3.groupby('created_at').count()['tweet']
file = 'data/pickled/Emoticon_NB4/full_emoji_dict.obj'$ gu.pickle_obj(file, emoji_dict)
cityID = '67b98f17fdcf20be'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Boston.append(tweet) 
frames = [nuc, part_nuc, out_out, noSeal, entirecell]$ ps = pd.concat(frames)$ ps.head(10)
sum_ratio = absorption_to_total + scattering_to_total$ sum_ratio.get_pandas_dataframe()
uni = free_data[free_data['educ']<=5]$ uni.groupby('country')['country'].count()
xgb_learner.fit_best_model(dtrain)
card_layouts = ["double-faced", "flip", "leveler", "meld", "normal", "split"]$ all_sets.cards = all_sets.cards.apply(lambda x: x.loc[x.layout.map(lambda y: y in card_layouts)])$ all_sets.cards = all_sets.cards.apply(lambda x: x.loc[x.types.map(lambda y: y != ["Conspiracy"])])
metadata['data_ignore_value'] = float(refldata.attrs['Data_Ignore_Value'])$ metadata
btc['2017-08'].plot(y='price')$ plt.show()
pp = PostProcess(run_config='config/run_config_notebook.yml', $                  model='MESSAGE_GHD', scen='hospitals baseline', version=None)
daily_unit_df = all_turnstiles.groupby(['STATION','C/A','UNIT','DATE'], as_index=False).sum().drop(['ENTRIES','EXITS'], axis=1)$ daily_unit_df.sample(5)
int_tel_sentiment[:100].plot(figsize=(20, 20))
measure_df.to_sql(name='Measurements', con=engine, index=False)
gene_df.sort_values('length').head()
engine = create_engine("sqlite:///hawaii.sqlite")$ engine
df=pd.read_csv("dataset_quora/quora_train_test.csv")
df.index
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM$ pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724, open( "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p", "wb" ) )
data = data.set_index('time')
Mars_soup = BeautifulSoup(html, 'html.parser')
responses_df = pd.read_json('./data/Responses2.json', lines=True)$ print(len(responses_df))$ responses_df.head()
stations_des=session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()$ stations_des
df = pd.DataFrame(results, columns=['date', 'precipitation'])$ df.set_index(df['date'], inplace=True)$ df.head()
twitter_df.to_csv("twitter_data.csv")
Geocoder.geocode(festivals["Location"][9]).valid_address
store_items = store_items.drop(['store 3'], axis = 0)$ store_items
len([earlyScn for earlyScn in SCN_BDAY_qthis.scn_age if earlyScn < 3])
tz_dateutil = dateutil.tz.gettz('Europe/London')
!head ../../data/msft_modified.csv
new_model =  gensim.models.KeyedVectors.load_word2vec_format(path_database+'lesk2vec.bin', binary=True)
r1 = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=")$
pd.merge(df1,df2, on='HPI')$
year_ago = datetime.now().date()$ query.filter(cast())
Highest_opening_price = mydata['Open'].max()$ Highest_opening_price
station_count = session.query(Stations.id).count()$ print ("Total Number of Stations are: "+ str(station_count))
import matplotlib.pyplot as plt$ %matplotlib inline
INQ2016.head(1)
lr2 = LogisticRegression(random_state=20, max_iter=10000, C= 1, multi_class='ovr', solver='saga')$ lr2.fit(X_tfidf, y_tfidf)
for row in selfharmm_topic_names_df.iloc[3]:$     print(row)
bixi=pd.read_csv('OD_2018-07.csv')$ stations=pd.read_csv('Stations_2018.csv')
Base = automap_base()$ Base.prepare(engine, reflect=True)$
logit_countries = sm.Logit(df4['converted'], $                            df4[['country_UK', 'country_US', 'intercept']])$ result2 = logit_countries.fit()
X_copy['vndr_id'] = X_copy['vndr_id'].apply(lambda x: hash(x))
df_newhouse.reset_index(level=0,inplace=True)$ df_newhouse.columns = ["Date","Average_Housing_permits"]$ df_newhouse.head(5)$
output = pipeline.fit(flight7).transform(flight7)$ output = output.withColumnRenamed('price_will_drop_num', 'label')$ output.cache()
covar=np.cov(x,y)$ covar
total4=total.ix[(total['RA0']<50) & (total['RA0']>15)]$ total4.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/objs_miss_fall2016_ra_gt_1.csv',index=False)
most_active_station_tobs = session.query(Measurement.tobs).\$ filter(Measurement.station == most_active_station, Measurement.station == Station.station,\$        Measurement.date >="2017-08-01", Measurement.date <="2018-07-31").all()
df['y'].plot.box()
filename = '../data/DadosBO_2017_1(FURTO DE CELULAR).csv'$ df_BOs = pd.read_csv(filename, sep=';', encoding='utf-8')$ df_BOs.info()
dr = dr.resample('W-MON').sum()$ RNPA = RNPA.resample('W-MON').sum()$ ther = ther.resample('W-MON').sum()
from sklearn.dummy import DummyClassifier$ dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X, y)$ y_predict_dummy = dummy_majority.fit(X, y)
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
df = pd.DataFrame(got_data)
psy_df2 = psy_hx.merge(psy_df, on='subjectkey', how='right') # I want to keep all Ss from psy_df$ psy_df2.shape
print(client.version)
df[(abs(df['Open']-df['High'])<0.2 ) | (abs(df['Close']-df['Low'])<0.2)]
xyz = json.dumps(youtube_urls, separators=(',', ':'))$ with open('youtube_urls.json', 'w') as fp:$     fp.write(xyz)$
USvideos['like_ratio'] = USvideos['likes'] / (USvideos['likes'] + USvideos['dislikes']) $ USvideos['like_ratio'].describe()$
import numpy as np$ X_nonnum = X_copy.select_dtypes(exclude=np.number)
df['datetime'] = pd.to_datetime(df['datetime'],format=('%Y-%m-%d'))$ df.dtypes
releases = pd.read_csv('../input/jail_releases.csv')$ bookings = pd.read_csv('../input/jail_bookings.csv')
sentiments_pd.to_csv("News-Tweet-Sentiments.csv")
import datetime as dt$ mydata = dc2015.copy()$ mydata['new'] = np.where((mydata['created_at'].dt.time >= '0:00') & (mydata['created_at'].dt.time < '12:00'), 'morning', 'evening')
au.save_df(df_city, 'data/city-util/proc/city')$ au.save_df(df_util, 'data/city-util/proc/utility')$ au.save_df(misc_info, 'data/city-util/proc/misc_info')  # this routine works with Pandas Series as well
df = pd.read_csv('../data/hash_rate_raw.csv', names=['Date', 'Hashrate'])
merged_data['drone_rtk_lat'] = merged_data['rtk_lat'].interpolate(method='linear')$ merged_data['drone_rtk_lon'] = merged_data['rtk_lon'].interpolate(method='linear')$ merged_data['drone_rtk_alt'] = merged_data['rtk_alt'].interpolate(method='linear')$
cityID = '488da0de4c92ac8e'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Plano.append(tweet) 
df = pd.read_csv('first_17500_csv', index_col=0)
y[y**2 - np.sin(y) >= np. cos(y)] # math function based mask!$
df3['Permit Number'].duplicated().sum()
seed = 2210$ (train1, dev1, modeling2) = (modeling1$                              .randomSplit([0.4, 0.1, 0.5], seed=seed))
tweets_df.userLocation.value_counts()
words = [w for w in words if w not in stopwords.words('english')]$ print(words)
from keras.models import Sequential$ from keras.layers import Dense$
twitter[twitter.tweet_id.duplicated()]$
options_frame['ImpliedVolatilityMid'] = options_frame.apply(_get_implied_vol_mid, axis=1)
ans = df.groupby(['A','B']).sum()$ ans
probs_test = F.softmax(V(torch.Tensor(log_preds_test)));$
results = requests.get(final_url,{'access_token':token})
all_res = pd.read_csv("/Users/sdas/GoogleDrive/Papers/DeepLearning/DLinFinance/SP_Data_shared/DLIndex_Random_results_30_10000_30.csv")$ all_res.head()$ all_res.describe()
[1] # <-- this syntax should be used when converting to a list.$
vectorizer = TfidfVectorizer(max_df=0.1)  $ train_tweets_vector = vectorizer.fit_transform(train_tweets['pasttweets_text'])$ dev_tweets_vector = vectorizer.transform(dev_tweets['pasttweets_text'])
Base.prepare(engine, reflect=True)$ 
plt.style.use('ggplot')$
ngrams_summaries = cvec_2.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
url = "https://mars.nasa.gov/news/"
pax_path = 'paxraw_d.csv'$ %time pax_raw = pd.read_csv(os.path.join(data_dir, pax_path), dtype=type_map)
i = random.randrange(len(train_pos))$ train_pos[i]
plidata_blocks = pd.merge(plidata, parcel_blocks, how='left', left_on=['PARCEL'], right_on=['PIN'])$ plidata_blocks = plidata_blocks.drop(['PARCEL','PIN'], axis=1)$ plidata_blocks=plidata_blocks.dropna(subset=['TRACTCE10','BLOCKCE10'])
aTL[['system.record_id','system.created_at','APP_LOGIN_ID','APP_PRODUCT_TYPE','BRANCH','AREA_MANAGER','REGIONAL_MANAGER']].to_excel(cwd + '\\CA TL 0521-0526.xlsx', index=False)$ aSL[['system.record_id','system.created_at','APP_LOGIN_ID','APP_PRODUCT_TYPE','BRANCH','AREA_MANAGER','REGIONAL_MANAGER']].to_excel(cwd + '\\CA SL 0521-0526.xlsx', index=False)
s2 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])$ s2
yc_depart = yc_merged_drop.merge(departureZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')$ yc_depart.shape
df_page1 = len(df2.query("landing_page == 'new_page'")) / df2.shape[0]$ print("{} is the probability that an individual received the new page.".format(df_page1))
target_user = ("@BBC", "@CBSNews", "@CNN","@FoxNews", "@nytimes")$ total_tweets = pd.DataFrame(columns=["Name","Tweet Order","Text","Compound","Positive","Negative","Neutral","Time","Adj Time"])$
display(data.head(20))
results_lumpedTopmodel, output_LT = S_lumpedTopmodel.execute(run_suffix="lumpedTopmodel_hs", run_option = 'local')
Rural = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Rural")]$
events_df['utc_offset'].head(5)
ptgeom = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]$ gdf = gpd.GeoDataFrame(df, geometry=ptgeom, crs={'init': 'epsg:4326'})$ gdf.head(5)
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
y = list(train_10m_ag.is_attributed)$ X = train_10m_ag.drop(['is_attributed'],axis=1)$ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)
bymin.resample("S").bfill()
calls_nocontact_2017 = calls_nocontact_simp.loc[mask]
mydict = r.json()['dataset']$ dict(list(mydict.items())[0:10])
station_data = session.query(Stations).first()$ station_data.__dict__
df4.sort_values(by='BG')
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)
df_valid["Died"] = pd.to_datetime(df_valid['Died'], unit="ms")$ df_valid = df_valid[df_valid["Died"] < datetime.strptime("2018-01-01", "%Y-%m-%d")]
x_minor_ticks = 10 # Note that this doesn't work for datetime axes.$ y_minor_ticks = 10
model.fit(X_train, y.labels, epochs=5, batch_size=32,verbose=2)
rmse_scores = np.sqrt(mse_scores)$ print(rmse_scores)
pivoted = bdata.pivot_table('Total', index=bdata.index.time, columns=bdata.index.date)
bg2 = pd.read_csv('Libre2018-01-03.txt', sep='\t', nrows=100) # local$ print(bg2)$ print(type(bg2))
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
df.status.value_counts()
tlen.plot(figsize=(16,4), color='r')
globalCityContent = readPDF(globalCityBytes)$ globalCitySentences = globalCityContent.replace('\n','').split('.')$ type(globalCitySentences)
r_train, r_test, rl_train, rl_test = train_test_split(r_forest.ix[:,0:10], r_forest['bot'], test_size=0.2, random_state = 2)
cursor = db.TweetDetils.aggregate([ {"$group" : {"_id":"$user_name", "score":{"$sum":"$retweet_cnt"}}},  {"$sort":{"score" : -1}},{"$limit":5}])$ for rec in cursor:$     print(rec["_id"], rec["score"])
stations_df.count()
r.summary2()
df.source.value_counts()
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))$
from datetime import datetime$ datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')$ datetime.strptime('2017-12-25', '%Y-%m-%d').weekday()
data.loc[data['hired']==1].groupby('category').hourly_rate.mean()
birth_dates.head(3)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \$       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY$ request = requests.get(url)
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'game_winners'
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'America/Los_Angeles').strftime('%D: %r')$ date_min = date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'America/Los_Angeles').strftime('%D: %r')
print(rdc.feature_importances_[0:113])$
documents = [x.split(' ') for x in documents]
Bot_tweets.groupby('sentiment').count()
df2[df2['group']=='treatment']['converted'].mean()
data.head()
df_birth.population = pd.to_numeric(df_birth.population.str.replace(',',''))
df_copy = df_groups.join(df_events.groupby(['group_id']).created.count(), how ='inner',on= 'group_id', lsuffix= '_left', rsuffix = '_count')$ df_copy = df_copy.sort_values(by = 'created_count', ascending = False)$ df_copy.head()
df.dropna(inplace=True) #because we are prediction 30 days extra empty values are created in other rows.$
raw.columns
last_year = today.year + 1$ years = list(range(join_date.year, last_year))$ years
ADP_array=df["NASDAQ.ADP"].dropna().as_matrix()$
RE_EMOJI = re.compile('[\U00010000-\U0010ffff]', flags=re.UNICODE)$ def strip_emoji(text):$     return RE_EMOJI.sub(r'', text)$
df = df.merge(pd.get_dummies(df['Date'].dt.strftime('%A')),left_index=True,right_index=True,how='left')$ print ('After Weekday',df.shape)$
df.loc[6:10]
df3[['inv', 'ab_page']] = pd.get_dummies(df3['group'])$ df3.tail()$
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))$
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2016-06-07&end_date=2016-06-09&api_key=yUSy9ms6sx2Ubk55dXdN")$
top_songs['Year'] = top_songs['Date'].dt.year
df7['avg_dew_point Closed'].value_counts(dropna=False)
grouped_publications_by_author.columns = grouped_publications_by_author.columns.droplevel(0)$ grouped_publications_by_author.columns = ['authorId', 'authorName','publicationTitles','authorCollaboratorIds','authorCollaborators','countPublications','publicationKeys','publicationDates']
df = titanic3[['cabin', 'pclass']].dropna()$ df['deck'] = df.apply(lambda row: ord(row.cabin[0]) -64, axis=1)$ sns.regplot(x=df["pclass"], y=df["deck"])
pathdataOH = np.repeat(newPaths[idxKeep], invals)$ oldpath = np.repeat(idxOP[idxKeep], invals)
bigram = gensim.models.Phrases(text_list)$ bigram_text = [bigram[line] for line in text_list]
json=request.json()
absorption_to_total = absorption.xs_tally / total.xs_tally$ absorption_to_total.get_pandas_dataframe()
%matplotlib inline$ import matplotlib.pyplot as plt
forecast_set = clf.predict(X_lately)$ len(forecast_set)
price2017['DateTime'] = pd.to_datetime(price2017['Date'] + ' ' + price2017['Time'])
import json$ import pandas as pd$ import matplotlib.pyplot as plt
df7 = df6.rename('Completed_Permits').reset_index()
y_hat = nb.predict(train_4)
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, temp_us)
intervention_history.sort_values(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
a = []$ b = [item for item in new_tweets if any(term in item.text.lower() for term in search_terms)]$
! python  keras-yolo3/yolo.py ../sonkey13.jpeg
df_measurement.describe()
hit_tracker_df = clean_merge_df.loc[clean_merge_df["Reached Number One"] == "Number One Hit",:]
reddit_info.to_csv('reddit_data_2.csv', encoding = 'utf-8', index = False)
svm_classifier.score(X_test, Y_test)
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="Dreamers").items(max_tweets):$     print(tweet)
OGLE_file = 'tl.txt'$ Dir_OGLE_file = '/Users/arturo/Documents/Research/LSST/OGLE/'$ OGLE_ra_dec_data = np.genfromtxt(Dir_OGLE_file+OGLE_file, usecols=[9,10])
f1 = df2['sizec'].values$ f2 = df2['breakfastc'].values$ X = np.array(list(zip(f1, f2)))$
financial_crisis.drop('Spain defaults 7x')$
dem = dem[dem["subjectkey"].isin(incl_Ss)]
combined_df4['split_llpg1']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(str(x).split(',')[1:2]))$ combined_df4['split_llpg2']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(str(x).split(',')[1:3]))$ combined_df4.head()
x_train.head()$
tag_df = tag_df.stack()$ tag_df
s = gp.GeoSeries([Point(x,y) for x, y in zip(delays_geo['Longitude'], delays_geo['Latitude'])])$ delays_geo['geometry'] = s$ delays_geo.crs = {'init': 'epsg:4326', 'no_defs': True}
df.mean(1)
ds_info = ingest.upload_dataset(database=db,$                                 dataset=test,$                                 type_map={"bools": float})
logs['key'].value_counts().plot()$ plt.show()
y_pred = logreg.predict(X_train)$ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_train, Y_train)))
print('First tweet recorded  @ {}'.format(tweets['time_eastern'].min()))$ print('Last tweet recorded @ {}'.format(tweets['time_eastern'].max()))
from firebase import firebase$ firebase = firebase.FirebaseApplication('', None)$ firebase.get("Exhibitions/-LFlR_PhbP2eWNCGPZeu",None)
classif_varieties = set(ndf[y_col].unique())$ label_map = {val: idx for idx, val in enumerate(ndf[y_col].unique())}
df = pd.read_csv('end-part2_df.csv').set_index('date')$ df.describe().T
merged_portfolio_sp_latest_YTD = pd.merge(merged_portfolio_sp_latest, adj_close_start, on='Ticker')$ merged_portfolio_sp_latest_YTD.head()
new_pred = pred1.join(pred2 , on='id', rsuffix='_2').join(pred3 , on='id', rsuffix='_3')$ new_pred['pred']=new_pred[['any_spot','any_spot_2','any_spot_3']].mean(axis=1).astype(int)$ new_pred = new_pred.drop(['any_spot','any_spot_3'], axis=1).rename(columns={'pred': 'any_spot'})
qt_ind_ARR_closed.applymap(lambda x: "{0:,.2f}".format(x))
sentiment_df = ave_sentiment_by_company_df.reset_index(drop = False)$ sentiment_df
tweet_df = pd.DataFrame.from_dict(tweet_ls)$ tweet_df.sort_values(by='Date', ascending=False)$ tweet_df.head()
data['win_differential'] = abs(data.homeWinPercentage - data.awayWinPercentage)$ data['win_team'] = np.where(data.awayWinPercentage >= data.homeWinPercentage, 'away', 'home')$ data['game_state'] = np.where(data.win_differential < 0.6, 'close', 'notclose')$
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt"$ df = pd.read_csv(path, sep ='\s+', na_values=['.'])$ df.head(5)
average_trading = statistics.mean([day[6] for day in data])$ print ('Average daily trading volume for 2017:', round(average_trading,2))
pd.merge(df1,df2, on=['HPI','Int_rate','US_GDP_Thousands']) # merge on list and on various columns$ 
output= "Delete from user where user_id='@Pratik'"$ cursor.execute(output)$
sumTable = tips.groupby(["sex","day"]).mean()$ sumTable
from sklearn.naive_bayes import GaussianNB
engine = create_engine("sqlite:///hawaii.sqlite")
tree_features_df['filename'].isin(manager.image_df['filename']).describe()$
DataSet.head()
base = wards.plot(column = 'geometry', figsize = (10,10), color = 'grey', edgecolor = 'black')$ delays_geo.plot(ax = base, marker='o', color='blue', markersize = 5)
ab_df.shape[0]
def validation_score(score_series):$     return score_series.mean()
ind_result_list=ind_result.tolist()$ ids=independent_dataset['id'].tolist()
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']])$ result=logit.fit()
df.describe()
d = datetime(2014,8,29)$ do = pd.DateOffset(days = 1)$ d + do
[x.text for x in html.find_all('a', {'class':'next '})]
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))
df_nott = df.query('landing_page == "old_page"')$ df_4 = df_nott.query('group != "control"')$ df_4.nunique()$
print("P(converted) = %.4f" %df2.converted.mean())
featured_img_url = "https://www.jpl.nasa.gov" + current_img_url$ featured_img_url
mars_html_table = mars_df_table.to_html(classes='marsdata')$ mars_table = mars_html_table.replace('\n', ' ')$ mars_table
ppm_body = preProcessor_in_memory(hueristic_pct=.61, keep_n=6000, maxlen=60)$ vectorized_body = ppm_body.fit_transform(data_to_clean_body)
mgxs_lib = openmc.mgxs.Library(geometry)$ mgxs_lib.energy_groups = groups
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key="$ r = requests.get(url+API_KEY)$
df.drop("water_year2",axis='columns',inplace=True)
import re$ import string$ punch=set(string.punctuation)
from statsmodels.tsa.arima_model import ARIMA$ model_6203 = ARIMA(dta_6204, (8, 1, 1)).fit() $ model_6203.forecast(5)[:1] 
DataAPI.write.update_secs_industry_gics(industry='A_GICSL1', trading_days=trading_days, override=False)
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
urban_ride_total = urban_type_df.groupby(["city"]).count()["ride_id"]$ urban_ride_total.head()
import exampleModule # This has one function called ex$ exampleModule.ex([4,5,6])
df.brewery_name.value_counts().head(5)
retail_data = raw_data.loc[pd.isnull(raw_data.accountid) == False]
pi_year10lambapoint9_PS11taskG = 2.02
result = DBRunQuery(q)$ print("Result = ")$ print(json.dumps(result, indent=2))
data.count(axis=0)
Google_stock.describe()
building_pa.sample(5)
number_pos = data_df[data_df['is_high_val'] == 1].shape[0]$ number_all = data_df.shape[0]$ print(f'Target labels of class \'1\': {number_pos} or {(number_pos/number_all)*100:.2f}% over all.')
containers[0].find("div",{"class":"key"}).a['title'].split()[0].replace(',',"")
daily_trading_volume = [daily[6] for daily in json_data['dataset_data']['data'] if daily[6] != None]$ average = str(round(sum(daily_trading_volume)/len(daily_trading_volume),1))$ print('The average daily trading volume during 2017 was ' + average + '.')$
df['2018-05-21'] # Try to understand the error ... Its searching in columns !!! $
df = pd.read_excel("Data/Moments Report.xls")$
groups = contract_history[['INSTANCE_ID', 'UPD_DATE']].merge(intervention_train[['INSTANCE_ID', 'CRE_DATE_GZL']])
print(type(data.values))$ data.values
corr_matrix = dftotal.corr()$ corr_matrix["tripduration"].sort_values(ascending=False)
from sklearn import metrics$ print("Accuracy: %.3f" % # TODO$          )
staging_bucket = 'gs://' + google.datalab.Context.default().project_id + '-dtlb-staging-resolution'$ !gsutil mb -c regional -l {storage_region} {staging_bucket}
df_transactions['not_auto_renew'] = df_transactions.is_auto_renew.apply(lambda x: 1 if x == 0 else 0)
n_new = df2.query(('landing_page == "new_page"')).count()[0]$ n_new
locations = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\$ filter(Measurement.station==Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()
results.shape
def my_scaler(col):$   return (col - np.min(col))/(np.max(col)-np.min(col))$ data_scaled = data_numeric.apply(my_scaler)
customer_emails = sales_data_clean[['Email', 'Paid at']].drop_duplicates()$ customer_emails.dropna(inplace=True)
from sklearn.linear_model import LogisticRegression$ logreg = LogisticRegression()$ print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())
S_1dRichards.initial_cond.filename
income_raw = data['income']$ features_raw = data.drop('income', axis = 1)$ vs.distribution(data)
df_users = pd.read_csv('../data/august/users.csv')$ df_levels = pd.read_csv('../data/august/levels.csv')$ df_events = pd.read_csv('../data/august/events.csv', skiprows=1, names=event_header, error_bad_lines=False, warn_bad_lines=True)     
df_final.columns
print(datetime.now() - timedelta(hours=1))$ print(datetime.now() - timedelta(days=3))$ print(datetime.now() + timedelta(days=368, seconds=2))
tfav.plot(figsize = (16,4), label = "Likes", legend = True)$ tret.plot(figsize = (16,4), label = "Retweets", legend = True);  $
close_px_all = pd.read_csv('stock_px.csv', parse_dates=True, $                            index_col=0)$ close_px_all.head() # data 2003-2011, >2000 rows
search.groupby(['search_type','booking']).size()
test_df = pd.read_csv("test.csv", dtype=dtypes)$ test_df.head()
keys = tweepy.OAuthHandler(consumer_key, consumer_secret)$ keys.set_access_token(access_token, access_token_secret)$ api = tweepy.API(keys, parser=tweepy.parsers.JSONParser())
df['created_at'] = pd.to_datetime(df['created_at'])
for row in session.query(Measurements).limit(5).all():$     print(row)$
studies = pd.read_csv('C:/Users/akapoor/Music/01 Docs/HealthCare App/ctdb/studies.txt', sep="|")$ studies.head()
got_data = pickle.load(file=open('got_100_data.data','rb'))
brewery_bw.tail(8)
print ab_counts.first_name.values$ print type(ab_counts.first_name.values)$
df.sort_index(axis=0, ascending=True)
mask = stops['stopid'].isin(shared_ids)
len(df[~(df.user_properties == {})])
news_title = soup.find('div', class_='content_title').text.strip()$ news_title
from sklearn.preprocessing import Imputer$ trainDataVecs = Imputer().fit_transform(trainDataVecs)$ testDataVecs = Imputer().fit_transform(testDataVecs)
trump_twitter = pd.read_json('trump_tweets_2017.json', encoding='utf8')$ trump_twitter.head()
users.groupby('CreationDate')['LastAccessDate'].count().plot()$
prcp = session.query(prcp.date, prcp.prcp).\$     filter(prcp.date > first_date).\$     order_by(prcp.date).all()
$hadoop fs -put /data/tg_cg18_bigdata/rc_2018_02.csv /user/sohom/$
building_pa_prc_shrink[building_pa_prc_shrink.columns[0:5]].head(10)
appleInitialNegs = neg_tweets[neg_tweets.author_id_y == 'AppleSupport']
for col in df.select_dtypes(include='datetime64').columns:$     print_time_range(col)
df1['volatility']=(df1['Adj. High']-df1['Adj. Close'])/df1['Adj. Close']$ df1['volatility'].head()
print(gs.best_estimator_)
with open('youtube_urls.json', 'r') as fp:$     youtube_urls = json.load(fp)
questions['createdAt'] = pd.to_datetime(questions['createdAt'])
calls_nocontact_simp = calls_nocontact.drop(columns=['ticket_id', 'issue_description', 'city', 'state', 'location', 'geom'])$ calls_nocontact_simp.head()
df[df.Predicted == 5]
type2017.isnull().sum() 
url = 'https://mars.nasa.gov/news/'$ response = requests.get(url)$ soup = bs(response.text, 'html.parser')
price_data = price_data.iloc[1:]
trump_month_distri.plot(kind='bar', figsize=(10,5), rot= 45,title="# of Twitters of Donald Trump")$ plt.savefig('fig/trump_month.png');
df_data_desc = pd.read_csv(filepaths['data_desc'], encoding='latin1')$ df_data_desc = df_data_desc[df_data_desc.columns[1:]].fillna('')$ df_data_desc.head(10)
df_groups = pd.read_csv('groups.csv')$ df_groups.head()
tlen.plot(figsize=(16,4), color='r');
words_sk = [term for term in all_tokens_sk if term not in stop and not term.startswith('http') and len(term)>2]$ corpus_tweets_streamed_keyword.append(('meaningful words', len(words_sk))) # update corpus comparison$ print('Total number of meaningful words (without stopwords and links): ', len(words_sk))
base_df.describe()
top_10_authors = git_log['author'].value_counts().head(10)$ top_10_authors
cityID = 'a592bd6ceb1319f7'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Diego.append(tweet) 
xml_in_sample1['authorId'].nunique()
df_complete_agg =df_complete_set.loc[:,[1,'Tweets Ago','compound']]$ df_complete_agg.rename(columns={'compound' : 'Tweet Polarity'  ,1 : 'News Agency' },inplace=True)$ df_complete_agg.head()$
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]$
df = json_normalize(j['datatable'], 'data')$ df.columns = col_names$ df.head()
f.visititems?
def filter_special_characters(text):$     return re.sub('[^A-Za-z0-9\s;,.?!]+', '', text)$
statistics_table = win_rates_table.merge(pick_rates_table, left_index=True, right_index=True)$ statistics_table.head()
dict_wells_df_and_Nofeatures_20180707 = dict_of_well_df$ pickle.dump(dict_wells_df_and_Nofeatures_20180707, open( "dict_of__wells_df_No_features_class3_20180707.p", "wb" ) )
plt.plot(kind='bar')$ plt.show()
writer = pd.ExcelWriter('my_dataframe.xlsx')$ merged_df.to_excel(writer, 'Sheet1')$ writer.save()
print(type(some_rdd),type(some_df))$ print('some_df =',some_df.collect())$ print('some_rdd=',some_rdd.collect())
ebola_melt['type'] = ebola_melt.str_split.str.get(0)$ ebola_melt.head()
days_alive = (datetime.datetime.today() - datetime.datetime(1981, 6, 11))$ days_alive.days$ days_alive
data = pd.read_csv('Nairaland_user_age.csv')$ data
transform = TfidfVectorizer(lowercase=False, min_df=.01)$ tf_idf_matrix = transform.fit_transform(back2sent.values)$ tf_idf_matrix.shape
cityID = '018929347840059e'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Indianapolis.append(tweet) 
data = data_all.dropna(subset=['has_odds', 'sigma_scaled'])$ data.tail(3)
kochdf.to_csv('exports/trend_data.csv')
mydata.info()
volume_m = volumes.resample('M').sum()
df1.num_words.describe()$
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path)$ df.head(5)
obs = df_gp_hr.groupby('level_0').mean()$ observation_data = obs['Observation (aspen)']
automl_feat.fit(X_train, y_train,$            dataset_name='psy_native',$            feat_type=feat_type)
plot = events_top10_df.boxplot(column=['yes_rsvp_count']\$                            , by='topic_name', showfliers = False, showmeans = False, figsize =(17,8.2)\$                            ,whis=[20, 80])
from bs4 import BeautifulSoup             $ example1 = BeautifulSoup(train["review"][0], 'html.parser')  
two_day_sample['date'] = two_day_sample.timestamp.dt.date
print(len(labels.keys()))
station_count.iloc[:,0].idxmax()$
newsorgs_bar = mean_newsorg_sentiment["News Organization"]$ compound_bar = mean_newsorg_sentiment["Compound"]$ x_axis = np.arange(0, len(compound_bar), 1)
psy_df = dem.merge(QUIDS_wide, on='subjectkey', how='right') # I want to keep all Ss from QUIDS_wide$ psy_df.shape
final.loc[(((final.loc[:,'RA']-261.8475)**2+(final.loc[:,'DEC']-55.1813888889)**2)**0.5).idxmin(),:]
news_p = soup.find('div', class_='rollover_description').text.strip()$ news_p
df_pol['text']=df_pol['title'].str.replace('\d+', '')$
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=ZxFGsokp-2_XeaAAkjne&start_date=2017-01-01&end_date=2017-12-31')
fraq_volume_m_coins = volume_m.div(volume_m.sum(axis=1), axis=0)
tag_df = stories.tags.apply(pd.Series)$ tag_df.head()
npath = out_file2$ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
print(df.apply(np.cumsum))
tlen.plot(figsize=(16,4), color='r')
df["qty"].sum()
os.chdir("E:/Data/Client1/Excel Files")$ path = os.getcwd()$ files = os.listdir(path)
df = quandl.get('NASDAQOMX/COMP', api_key='yEFb5f6a7oQL91qzEsvg',start_date = '1960-01-01',end_date = '2016-12-26')$ a=df$ a.rename(columns={'Index Value' : 'index_value'}, inplace=True)
df_detail = df_detail.fillna(0)
cities_list = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose', 'Detroit', 'Jacksonville', 'Indianapolis', 'San Francisco', 'Columbus', 'Austin', 'Memphis', 'Fort Worth', 'Baltimore', 'Charlotte', 'El Paso', 'Boston', 'Seattle', 'Washington', 'Milwaukee', 'Denver', 'Louisville', 'Las Vegas', 'Nashville', 'Oklahoma City', 'Portland', 'Tucson', 'Albuquerque', 'Atlanta', 'Long Beach', 'Fresno', 'Sacramento', 'Mesa', 'Kansas City', 'Cleveland', 'Virginia Beach', 'Omaha', 'Miami', 'Oakland', 'Tulsa', 'Honolulu', 'Minneapolis', 'Colorado Springs', 'Arlington', 'Wichita', 'Raleigh', 'St. Louis', 'Santa Ana', 'Anaheim', 'Tampa', 'Cincinnati', 'Pittsburgh', 'Bakersfield', 'Aurora', 'Toledo', 'Riverside', 'Stockton', 'Corpus Christi', 'Newark', 'Anchorage', 'Buffalo', 'St. Paul', 'Lexington-Fayette', 'Plano', 'Fort Wayne', 'St. Petersburg', 'Glendale', 'Jersey City', 'Lincoln', 'Henderson', 'Chandler', 'Greensboro', 'Scottsdale', 'Baton Rouge', 'Birmingham', 'Norfolk', 'Madison', 'New Orleans', 'Chesapeake', 'Orlando', 'Garland', 'Hialeah', 'Laredo', 'Chula Vista', 'Lubbock', 'Reno', 'Akron', 'Durham', 'Rochester', 'Modesto', 'Montgomery', 'Fremont', 'Shreveport', 'Arlington', 'Glendale']
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
from sklearn.linear_model import LogisticRegression
typesub2017['Solar'] = typesub2017['Solar'].astype(int)$ typesub2017['Wind Offshore'] = typesub2017['Wind Offshore'].astype(int)$ typesub2017['Wind Onshore'] = typesub2017['Wind Onshore'].astype(int)
diff_df = df_df.reset_index(drop = False)$ diff_df
fuel_therm_abs_rate = sp.get_tally(name='fuel therm. abs. rate')$ therm_util = fuel_therm_abs_rate / therm_abs_rate$ therm_util.get_pandas_dataframe()
columns = ['day_period', 'weekday', 'category', 'is_self', 'is_video']$ le = LabelEncoder()$ model_df[columns] = model_df[columns].apply(lambda x: le.fit_transform(x))
emails_dataframe['address'].str.split("@").str.get(1)
exiftool -csv -createdate -modifydate ciscid4/CISCID4_cycle2.mp4 ciscid4/CISCID4_cycle3.mp4 ciscid4/CISCID4_cycle4.mp4 ciscid4/CISCID4_cycle5.mp4 ciscid4/CISCID4_cycle6.mp4 > ciscid4.csv
data = grouped_publications_by_author.copy()
df.boxplot('MeanFlow_cms',by='status');
data.L2.unique()
with open("TestUser3.json","r") as fh:$     data = json.load(fh)$
df.head().to_json("../../data/stocks.json")$ !cat ../../data/stocks.json
%cd drive/CloudAI/nmt-chatbot$ !python utils/prepare_for_deployment.py
df4.dtypes
Val_eddyFlux = Plotting('/glade/u/home/ydchoi/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')$
cityID = 'e67427d9b4126602'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Madison.append(tweet) 
print(model.summary())
data = allocate_equities(allocs=[  0.25 , 0.25,  0.25 , 0.25],dates=dates)$ data.plot(),$ plt.show()$
top10_df_pd=top10_df.toPandas()$ top10_df_pd.head(10)
x_scaled = 0$ if len(x_normalized) > 1:$     x_scaled = min_max_scaler.fit_transform(x_normalized)
cityID =  '5a110d312052166f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Francisco.append(tweet) 
import ibm_boto3$ from ibm_botocore.client import Config
cityID = 'dc62519fda13b4ec'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tampa.append(tweet) 
rain_df.set_index('date').head()
joined=join_df(joined,trend_de,["Year","Week"],suffix='_DE')$ joined_test=join_df(joined_test,trend_de,["Year","Week"],suffix='_DE')$ sum(joined['trend_DE'].isnull()),sum(joined_test['trend_DE'].isnull())
df_events_sample = df_events.sample(n=1000)
results = session.query(Measurement.tobs).all()$ tobs_values = list(np.ravel(results))$ tobs_values
import nltk.data$ tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
df3 = df / df.iloc[0, :]$ df3.plot()$ plt.show()
future = m.make_future_dataframe(periods=52*3, freq='w')$ future.tail()
df = pd.read_csv("nyhto_twitter_edited.csv")$ df2018 = pd.read_csv("nyhto_twitter_edited2018.csv")
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY$ r = requests.get(url)
pattern = re.compile('AA')$ print([x for x in pattern.finditer('AAbcAA')])$ print([x for x in pattern.finditer('bcAA')])
newdf = pd.merge(bc, ts, left_index=True, right_index=True)
driver = webdriver.Chrome(executable_path="/Users/dale/Downloads/chromedriver")
pred_labels = rdg2.predict(test_data)$ print("Training set score: {:.2f}".format(rdg2.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(rdg2.score(test_data, test_labels)))
x = tf.constant(1, name='x')$ y = tf.Variable(x+10, name='y')$ print(y)
engine.execute('SELECT * FROM Station').fetchall()
data['Age'].min()
url_CLEAN1A = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1A.csv"$ url_CLEAN1B = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1B.csv"$ url_CLEAN1C = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1C.csv"$
tweets.dtypes
%matplotlib inline  $ import matplotlib.pyplot as plt
df.iloc[99:104]
df['is_rank_1'] = False$ df.loc[df['Rank'] == 1, 'is_rank_1'] = True
data.name.isnull().sum()
print("Probability an individual recieved new page:", $       df2['landing_page'].value_counts()[0]/len(df2))
iris_mat = iris.as_matrix()$ print(iris_mat[0:9,:])
exiftool -csv -createdate -modifydate cisuabd4/cisuabd4_cycle1.MP4 cisuabd4/cisuabd4_cycle2.MP4 cisuabd4/cisuabd4_cycle3.MP4 cisuabd4/cisuabd4_cycle4.MP4 cisuabd4/cisuabd4_cycle5.MP4 cisuabd4/cisuabd4_cycle6.MP4 > cisuabd4.csv
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False)$
reqs.count()
grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)$ grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')$ grid.add_legend()
df['DETAILS']=df['DETAILS'].fillna("")$
df_sum=pd.DataFrame(data=sum_row).T$ df_sum 
req = requests.request('GET', 'https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2017-02-01&end_date=2017-02-01&api_key='+API_KEY)$ req.json()
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']])$ res = lm.fit()$ res.summary2()
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
pd.pivot_table(tdf, values='data', columns='day', index='time', $                margins=True, fill_value=0, aggfunc='count')
df_new.hist(figsize=(10,5))$ plt.show()$
fit1.resid.hist();
input_col = ['msno','payment_plan_days','transaction_date', 'membership_expire_date',]$ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions',input_col)
df.loc[0, 'review'][:-500]
TripData_merged3.isnull().sum()
first_commit_timestamp = git_log[git_log['author'] == 'Linus Torvalds'].sort_values('timestamp').head(1)$ first_commit_timestamp
from sklearn.feature_extraction.text import TfidfVectorizer # Import the library to vectorize the text$ tfidf_vect = TfidfVectorizer(ngram_range=(1,3), stop_words='english')$ tfidf_vectorized = tfidf_vect.fit_transform(df_train.text)
s4.shape
ws = Workspace.from_config()$ print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\n')
x['age'] = x['age'].astype('timedelta64[D]')
result_set =session.query(Adultdb).filter(Adultdb.education.in_(['Masters', '11th'])).all()
tweetsIn22Mar.head()$ tweetsIn1Apr.head()$ tweetsIn2Apr.head()
yourstartdate=datetime.strptime(input('Enter End date in the format %Y-%m-%d'), '%Y-%m-%d')$ yourenddate=datetime.strptime(input('Enter End date in the format %Y-%m-%d'), '%Y-%m-%d')$ calc_temps(yourstartdate,yourenddate)
s1 = pd.Series(np.arange(1, 6, 1)) $ s2 = pd.Series(np.arange(6, 11, 1)) $ pd.DataFrame({'c1': s1, 'c2': s2})
fh_1 = FeatureHasher(input_type='string', non_negative=True) # so we can use NaiveBayes$ %time fit = fh_1.fit_transform(train.device_model)
s = BeautifulSoup(doc,'html.parser')$ print(s.prettify())
img_url_rel = img_soup.find('figure', class_='lede').find('img')['src']$ img_url_rel
contractor_clean['updated_date'].head()$
print(len(plan['plan']['itineraries']))$ print(plan['plan']['itineraries'][0].keys())
data.userScreen.nunique()
gs.score(X_test, y_test)
df.head(10)
train_frame = train_frame.reset_index()$ df_series = pd.Series(train_frame["values"])$
data.info()
cityID = '73d1c1c11b675932'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chesapeake.append(tweet) 
market_cap_df = pd.read_csv('../data/total_market_cap.csv', index_col='Date', parse_dates=True)$ market_cap_df.head()
lm = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']])$ res = lm.fit()
df_tte['InstanceType'] = df_tte['UsageType'].apply(lambda usage: usage.split(':')[1]) #create column with instance type
table = pd.crosstab(df["grade"], df["loan_status"], normalize=True)$
dfWordsEn['Line'] = dfWordsEn['Line'].str.lower()$ dfFirstNames['Line'] = dfFirstNames['Line'].str.lower()$ dfBlackListWords['Line'] = dfBlackListWords['Line'].str.lower()
X2 = now[[col for col in now.columns if col != 'bitcoinPrice_future_7']]
end_time = dt.datetime.now()$ (end_time - start_time).total_seconds()
quandl.ApiConfig.api_key = 'gTmSNjbQR-8Q5U9pukHX'$ quandl.get('BITFINEX/BTCEUR',collapse="monthly")$
df_vow.head()
en_es = pd.read_csv(FPATH_ENES, sep=" ", header=None)$ en_es.columns = ["en", "es"]$ en_es.describe()
display(data.head(10))
chinese_vessels.to_csv('chinese_vessels_CLAV.csv', index=False, encoding='utf-8')
test[['clean_text','user_id','predict']][test['user_id']==5563089830].shape[0]
sub_gene_df['type'].value_counts()
tia['date'] = tia['date'].apply(lambda x: x[14:])$ tia['date'][0]
zipincome['ZIPCODE'] = zipincome['ZIPCODE'].astype(float)
df_sb.isDuplicated.value_counts()$ df_sb.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
Features = iris.values[:, :4]$ species = iris.values[:, 4]
ripple_market_info.drop(['Date'],inplace=True,axis=1)$ scaler_rip = MinMaxScaler(feature_range=(0, 1))$ scaled_rip = scaler_rip.fit_transform(ripple_market_info)$
file = open("datasets/git_log_excerpt.csv", "r")$ print(file.read())
filepath = os.path.join('input', 'input_plant-list_SE.csv')$ data_SE = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_SE.head()
tweet_df.head()$
df.text[df.text.str.len() == df.text.str.len().min()]
cityID = '4fd63188b772fc62'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Laredo.append(tweet) 
counts = review_df.groupby('author').size()$ df2 = pd.DataFrame(counts, columns = ['size'])$ df2[df2['size']>1]
questions.set_index('createdAt', inplace=True)
festivals.head(5)
pax_raw = pax_raw.merge(n_user_days, on='seqn', how='inner')
jsummaries = jcomplete_profile['summaries']$ recent = pd.DataFrame.from_dict(jsummaries)$ print(recent[['start_date','type','number_of_active_accounts', 'log_ins_market_downturn']][-5:])
contractor_merge['contractor_bus_name'].head()$
cityID = '1c69a67ad480e1b1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Houston.append(tweet) 
sql = "SELECT * FROM main_stops;"$ stops = pd.read_sql(sql, engine)
recommendationTable_df = recommendationTable_df.sort_values(ascending=False)$ recommendationTable_df.head()
saved = savedict.copy()$ ex = saved['2017-11-15']$ print("Choose Campaign Name: ",ex['Campaign Name'].unique())$
compacted.to_csv(basedirectory+projectname+'/'+projectname+'_images.csv')
amznnews_matrix = count_vectorizer.transform(df_amznnews.headline_text)$ amznnews_pred = nb_classifier.predict(amznnews_matrix)
df.user_mentions = df.user_mentions.apply(usersMentions)
predict.predict_score('Wikipedia')
clean_merge_df = merge_table_df.drop(["Number One_x"], axis=1)$ clean_merge_df.rename(columns={'Number One_y': 'Reached Number One'}, inplace=True)
    no_punc = [char for char in string.lower() if char not in punctuation]$     no_punc = ''.join(no_punc)$     return [word for word in no_punc.split() if word not in stopwords.words('english')]
new_df = df.replace(-99999, np.NAN)$ new_df
r = requests.get(url_api)$ print(r.status_code)
data.loc[[pd.to_datetime("2016-09-02")]]
df_cod2["Cause of death"].unique()
print(loan_stats["revol_util"].na_omit().min())$ print(loan_stats["revol_util"].na_omit().max())$
index = pd.bdate_range('2018-3-1', periods=1000)$ index
Station = Base.classes.stations
station_activity = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\$ filter(Measurement.station == Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
trump_o.drop("text", axis = 1)$ trump_con = trump_o.drop(["favorite_count", "text", "id_str", "is_retweet", "retweet_count", "source", "created_at"], axis = 1)$ trump_con.shape
ds = tf.data.TFRecordDataset(train_path)$ ds = ds.map(_parse_function)$ ds
mentions = api.GetMentions()$ print([m.text for m in mentions])
df.info(null_counts=True, memory_usage='deep')
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Amount"])
random.shuffle(porn_ids)$ porn_bots = porn_ids[:5000]
aliases = [b for b in BID_PLANS_df.index if '\n' in b]
pd.isnull(r1_test).values.any()$
inspector = inspect(engine)$ inspector.get_table_names()
opening_prices = []$ for ele in r.json()['dataset_data']['data']:$     opening_prices.append(ele[1])
df.iloc[99:110,3:]
ans = pd.pivot_table(df, values='E', index=['A','B'], columns=['C'])$ ans
soup.find_all('p')
g8_groups['area'].mean()
questions = questions.reset_index(drop=True)
stopword_list.extend(["dass", "wer", "wieso", "weshalb", "warum", "gerade"]) #Add the words ["dass"] to the list.$ print(len(stopword_list))
coin_data.columns
longest_date_interval = longest_date_interval.reset_index()$ longest_date_interval.columns = longest_date_interval.columns.remove_unused_levels().droplevel(level=1)$ print(longest_date_interval.head(3))$
tweets['created_at'] = pd.to_datetime(tweets['created_at'])$ tweets.dtypes
ser = pd.DataFrame({'By': dates, 'key':[0] * len(dates)})$ ser
df['x'].unique()
df2.info()
df=pd.read_table("../../data/msft.csv",sep=',')$ df.head()
for url in soup.find_all('a'):$     print (url.get('href'))
dataBloodType.human_id = dataBloodType.human_id.str.lower()$ df2 = df.merge(dataBloodType,left_on = 'Sample', right_on='human_id', how='inner')$ del dataBloodType$
def calc_tmps(spec_date):$     return session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$ filter(Measurement.date == spec_date).all()
subset_data = pd.merge(interest_dates, product_time, left_index=True,#left_on= 'date', $                        right_index=True, how='inner') #Match dates and merge
demand = model.get_parameter(par='demand')$ list(set(demand.commodity))$ demand.head()
list_to_merge = list(db.tweets.find({},{"id": 1, "user": 1,"text": 1,"hashtags":1, "_id": 0}))
result.route
datAll['blk_rng'] = datAll['Block_range'].map(str)+' '+datAll['Street_name'].map(str)
validation.analysis(observation_data, simple_resistance_simulation_0_5)
df['MeanFlow_cfs'].plot();
for numbData in collData.distinct().collect():$     print(numbData)
validation.analysis(observation_data, Jarvis_resistance_simulation_0_25)
topics_data = pd.DataFrame(topics_dict)
display(datos.head(10))
tlen = pd.Series(data['len'].values, index=data['Date'])$ tfav = pd.Series(data['Likes'].values, index=data['Date'])$ tret = pd.Series(data['RTs'].values, index=data['Date'])
bad_r = requests.get('http://httpbin.org/status/404')$ bad_r.status_code
validation.analysis(observation_data, BallBerry_simulation)
tweet_df["tweet_date"] = pd.to_datetime(tweet_df["tweet_date"])
df_estimates_false = df_estimates_false.dropna(axis=0, subset=['points'])$ print(df_estimates_false)$
df.index
import pandas as pd$ import matplotlib.pyplot as plt
df = pd.concat(frames, axis=1)$
s = pd.Series(np.random.randn(4))$ s
deltadf.to_csv('exports/trend_deltas_chefkoch.csv')
high_idx = afx['dataset']['column_names'].index('High')$ low_idx = afx['dataset']['column_names'].index('Low')$ change_values = [entry[high_idx] - entry[low_idx] for entry in afx['dataset']['data'] if entry[high_idx] and entry[low_idx]]
pred = predict_class(np.array(theta), X_train_1)$ print ('Train Accuracy: %f' % ((y_train[(pred == y_train)].size / float(y_train.size)) * 100.0))
mgxs_lib.build_library()
df_sched.iloc[:,1:] = df_sched.iloc[:,1:].apply(lambda x: x.str[:10])
from gensim import models$ model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  
final.index=range(final.shape[0])
r=pd.date_range(start='8/14/2017', periods=5)$ r
user_ratings = df_ratings[(df_ratings.user_id==9)]$ user_ratings
data_ar = np.array(data_ls)$ data_df = pd.DataFrame(data_ar, columns = ["ticket_id","time_utc","type", "desc"])
resdf=resdf.drop(['Unnamed: 0'], axis=1)$ resdf.head(3)$
from statsmodels.tsa.arima_model import ARIMA$ model_6203 = ARIMA(dta_6203, (6, 1, 0)).fit() $ model_6203.forecast(5)[:1] 
article_divs = [item.find('div',{'class':'article--container'}) for item in soups]
data_final['authorId'].nunique()
df = df.drop('vegetables', axis=1)$ df
year11 = driver.find_elements_by_class_name('yr-button')[10]$ year11.click()
plt.figure(figsize=(15,5))$ sns.barplot(data=aa,x='weekofyear',y='message')
data.Likes.value_counts(normalize=True).head()
from gensim.models import Doc2Vec$ model = Doc2Vec.load('/tmp/movie_model.doc2vec')
cityID = '0570f015c264cbd9'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Louis.append(tweet) 
mv_lens.head()
f = v.reset_index().rename(columns={'level_0':'ID'})$ f['ballot_type'] = f['vote'].str.extract('\((.*?)\)', expand=True).fillna('')$ f['vote'] = f['vote'].replace('\(.*\)', '', regex=True)
testdf = getTextFromThread(urls_df.iloc[2,0], urls_df.iloc[2,1])$ testdf.head()$
tweets['location'] = tweets['location'].str.strip()$ tweets_loc = tweets.groupby(tweets.location).count()['id'].sort_values(ascending=False)$ tweets_loc$
potential = lmp.Potential('2015--Pascuet-M-I--Al--LAMMPS--ipr1.json')
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df.sort_values(['Likes', 'len'], ascending=False).head()
joined_hist.joined = pd.to_datetime(joined_hist.joined)$ joined_hist = joined_hist.sort_values(ascending=True, by='joined')$ joined_hist.head()$
print('Total null values in name column: ', data.name.isnull().sum())$ print('\nInconsistent data for the missing name rows\n')$
df3[df3['group']=='treatment'].head()
dict(list(r_close.items())[0:10])
data_libraries_df = pd.merge(left=libraries_df, right=tmp, on="asset_name", how="outer")
precip_data_df.set_index("date",drop=True,inplace=True)$ precip_data_df.columns$ precip_data_df.tail()
print(fee_types.get('user fees', 'return this if the key is not in the dict'))$ print(fee_types.get('not a value', 'return this if the key is not in the dict'))
import pandas as pd$ git_log = pd.read_csv('datasets//git_log.gz', sep='#', encoding='latin1', header=None,  names=["timestamp", "author"], compression='infer')$ git_log.head(5)
pokemon['Legendary'] = np.where(pokemon['Legendary'] == True, 1, 0)$
len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
x = content_performance_bytime.groupby(['document_type', pd.Grouper(freq='M')])['pageviews'].sum()
url_img = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url_img)
df['text_extended'] = df.index1[898:1061].apply(tweet_extend)$
import tweepy$ import pandas as pd$ import matplotlib.pyplot as plt
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))$
df = df[df["category"]=="Tech"].reset_index(drop=True)$ df.drop(["category"], axis = 1, inplace = True)
engine.execute("SELECT  * FROM contractor").fetchall()
conn.execute(sql)
tdelta = pd.to_datetime(loctweetdf['created_at'].max()) - pd.to_datetime(loctweetdf['created_at'].min())$ tdelta = tdelta.days$ tdelta
data_donald_replies = pd.read_csv("trump_replies.csv", dtype={'reply_id':str})$ data_donald_replies.head()
nasa_url = 'https://mars.nasa.gov/news/'
df.head()
df.to_sql('tw_posts', engine, schema=schema, if_exists='append')
news_paragraph = soup.find_all("div", class_="rollover_description_inner")[0].text$ print(news_paragraph)
joined.dtypes
twitter_api = tweepy.API(authorization_instance)$ twitter_api
print(nc_file)$ nc_file.close(); print('Dataset is closed!')
churned_bool = pd.Series([USER_PLANS_df.loc[uid]['status'][np.argmax(USER_PLANS_df.loc[uid]['scns_created'])] =='canceled' for uid in USER_PLANS_df.index],index=USER_PLANS_df.index)
print ('Before Dummies',df.shape)$ df = df.merge(pd.get_dummies(df['Date'].dt.strftime('%B')),left_index=True,right_index=True,how='left')$ print ('After Month',df.shape)$
url = 'https://space-facts.com/mars/'
df.drop(["join_mode"], axis = 1, inplace = True)
fav_plot = t_fav.plot(figsize=(16,4), label="Favorites", legend=True, title='Number of favorites for tweets over time')$ fav_vs_time_fig = fav_plot.get_figure()$ fav_vs_time_fig.savefig('num_favs_over_time.png')
!open table.html
multi.handle.value_counts() / multi.shape[0]
z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new], nobs=[n_old, n_new], alternative='smaller' )$ z_score, p_value
dates_D = load_dates('../data/worldnews_2016_10-2017_9_submissiondates.txt')$
shows.dtypes
betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality$ nx.set_node_attributes(G, betweenness_dict, 'betweenness')$
net.build()
states = pd.DataFrame({'population': population,$                        'area': area}   )$ states
tweet_df = pd.DataFrame(tweet_data)$ tweet_df.shape
temp_df = active_psc_records[active_psc_records.company_number.isin(secret_corporate_pscs.company_number)]$ len(temp_df[temp_df.groupby('company_number').secret_base.transform(all)].company_number.unique())
X_train, X_test, y_train, y_test = train_test_split(stock.drop(['target'], 1), stock['target'], test_size=0.3, random_state=42)
type(model.wv.syn0)$ len(model.wv.vocab)$ model.wv.syn0.shape
url_df=pd.DataFrame({'url':urls})
client.repository.ExperimentMetaNames.show()
df1['PCT_Change']=(df1['Adj. Close']-df1['Adj. Open'])/df1['Adj. Open'] $ df1['PCT_Change'].head()
import matplotlib.cm as cm$ dots_c, vhlines_c, *_ = cm.Paired.colors
materials_file = openmc.Materials([fuel, water, zircaloy])$ materials_file.export_to_xml()
google = web.DataReader('MSFT','google',start,end)$ google.head()
data.isnull().sum()
cityID = '6ba08e404aed471f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Riverside.append(tweet) 
if weights_flag:$     regridder.clean_weight_file()  # clean-up
plot_autocorrelation(series=RNPA_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of RNPA hours new patients'))$ plot_autocorrelation(series=RNPA_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of RNPA hours new patients'))
lb = aml.leaderboard$ lb
df_=data[['QG_mult','QG_ptD','QG_axis2']].copy()$ df_['target']=data['isPhysG']$ df_.head()
ab_df2.loc[ab_df2.user_id.duplicated(),:]
df.rename(columns={'85235_00060_00003':'MeanFlow_cfs','85235_00060_00003_cd':'Confidence'},inplace=True)$ df.head()
np.intersect1d(top_10_KBest, top_10_elnet)
def quadratic(x, **kwargs):$     return np.hstack([np.ones((x.shape[0], 1)), x, x**2])
print(soup.prettify())
import requests$ base_url = 'http://www.mlssoccer.com/stats/season'$ response = requests.get(base_url)$
df_ct['cleaned_text'] = df_ct['text'].apply(lambda x : text_cleaners(x))
testing = pd.read_csv('SHARE-UCSD-export_reformatted.csv')
dfg = dfg.sort_values(['discharges'], ascending=[False])$ dfg = dfg.reset_index(['drg3']) $ dfg.head()
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)
y_estimates = lm.predict(x_min_max)$ y_estimates
client = MongoClient()$ db = client.sample_1m$ db.sample_1m.find_one()
print(sample_data.json())
url = "https://www.fdic.gov/bank/historical/bank/"$ driver.get(url)
artistDF[locate("Aerosmith", "name") > 0].show(20)$ artistDF[artistDF.artistID==1000010].show()$ artistDF[artistDF.artistID==2082323].show()$
auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)$ api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
stop_words = set(stopwords.words('english'))$
vip_reason = questions['vip_reason'].str.get_dummies(sep="'")
print("Action space:", env.action_space)$ print("Action space samples:")$ print(np.array([env.action_space.sample() for i in range(10)]))
df = pd.DataFrame(data=fruits_and_veggies)$ df
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
df_corr = result.groupby(['type', 'scope'])['site'].sum().reset_index()$ display(df_corr.sort_values('site',ascending=False).head(10))$ plot2D(df_corr, 'type', 'scope','site')
from sklearn.model_selection import train_test_split$ from keras.utils import np_utils$ train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.2, stratify = y)#, stratify = y)$
cityID = '9a974dfc8efb32a0'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Kansas_City.append(tweet) 
job_requirements = pd.DataFrame(requirements)$ job_requirements$
conn_hardy = psycopg2.connect("dbname='analytics' user='udacian' host='udacity-segment.c2zpsqalam7o.us-west-2.redshift.amazonaws.com' port='5439' password='AYEe&mtihMqtXQbWR2xgWrhmKzd6]F'")
df_users =  pd.read_sql(SQL, db)$ print(df_users.head())$ print(df_users.tail())
news_title = soup.title.text$ print(news_title)
sqladb.columns=headers
dict_1=request.json()$ dict_1.get('dataset_data')
station_obs_df = pd.DataFrame(sq.station_obs(), columns = ["Station name", "Observation counts"])$ station_obs_df
outlet_mean_score=sentiments_df.groupby("Media_Source")["Compound"].mean()$ outlet_mean_score = round (outlet_mean_score,2) $ outlet_mean_score #check
df['MeanFlow_mps'] = df['MeanFlow_cfs'] * 0.028316847
measure.info()
base_col = 't'$ df.rename(columns={target_column: base_col}, inplace=True)
url_weather = "https://twitter.com/marswxreport?lang=en"$ browser.visit(url_weather)
df.loc[0:2, ['A', 'C']]
lda_corpus = model[corpus]$ doc_topic_matrix = matutils.corpus2dense(lda_corpus, num_terms=n_topics).transpose()$ df = df.reset_index(drop=True).join(pd.DataFrame(doc_topic_matrix))
myTimeZone = pytz.timezone('US/Eastern')$ itemTable["Date"] = itemTable["Date"].apply(localize_time, args=(myTimeZone,))
cityID = '28ace6b8d6dbc3af'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chula_Vista.append(tweet) 
data.plot(kind='scatter', x='TV', y='sales')$ plt.plot(X_new, preds, c='red', linewidth=2)
car = car[['dayofweek','date','incidntnum']].reset_index(drop=True)$ car.head()
d_housing=detroit_census2.drop(detroit_census2.index[:24])$ d_housing=d_housing.drop(d_housing.index[5:])$ d_housing
merged1['DaysFromAppointmentCreatedToVisit'] = (merged1['AppointmentDate'] - merged1['AppointmentCreated']).dt.days
projects.actual_hours.sum()
df = pd.read_csv('data/test1.csv')$ df
compound(px_etfs).plot(fontsize='small') # exclude from long strat the negative sectors?
fashion[fashion.index == 'gucci'].sort_values("PRADA-proba", ascending=False).head(10)
df_2010['bank_name'] = df_2010.bank_name.str.split(",").str[0]$
cityID = '3f3f6803f117606d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lubbock.append(tweet) 
with open('100UsersResults.data', 'rb') as filehandle:  $     result = pickle.load(filehandle)
explode = [0.1,0,0]$ colors = ["gold", "lightblue", "lightcoral"]$ labels = ["Urban", "Suburban","Rural"]
SCN_BDAY.scn_age.describe()
X[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
S = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure08/summa_fileManager_riparianAspenPerturbRoots.txt')
bob_shopping_cart = pd.DataFrame(items, columns=['Bob'])$ bob_shopping_cart
mRF.model_performance(test)
isinstance(df.date[1],datetime)
temp_df2['titles'] = temp_df2['titles'].astype(str)
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'],unit='s')$ git_log.describe()
df2[df2['group']=='control']['converted'].mean()
np.log(points)
film_category_df = pd.read_sql(SQL, db)
print('Total items received: {:,.0f}'.format(items_received(df_receipts, rpt_dt_start, rpt_dt_end, vendor_num)))
store_items = store_items.rename(index={'store 3': 'last store'})$ store_items
df_amznnews_clsfd_2tick = df_amznnews_clsfd_2tick.set_index('publish_time')$ df_amznnews_clsfd_2tick.info()
spacy_url = 'https://spacy.io/assets/img/pipeline.svg'$ iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url)$ HTML(iframe)
column_list1 = ['DewPoint']$ df[column_list1].plot()$ plt.show()
plt.scatter(X2[:, 0], X2[:,1], c=labels, cmap='rainbow')$ plt.colorbar()
raw_data = pd.read_csv('AMZN.csv',sep=',')
model.save('/tmp/model.atmodel')$
import numpy as np$ ok.grade('q06')
groups_topics_unique_df = groups_topics_df.drop_duplicates(subset=['group_id'])
df_cs.head(2)
df_ctr = df2[df2['group'] == 'control']['converted'].mean()$ print("{} is the probability they converted.Thus, given that an individual was in the control group.".format(df_ctr))
df_pos_emo_end = df_pos_emo_end[df_pos_emo_end.last_word_category.str.len() == 1]$ df_pos_emo_end.last_word_category.value_counts().head(20)
data['age'][data['age'] < 0] = 365+data['age']$ data.iloc[140:170,]$
results_measurement = session.query(Measurements.station,Measurements.date,Measurements.prcp, Measurements.tobs).all()$ results_measurement
linear_predictor = linear.deploy(initial_instance_count=1, #Initial number of instances. $                                  instance_type='ml.m4.xlarge') # instance type
stations = df.station.unique()$ no_of_stations = len(stations)$ no_of_stations
x = plt.gca().xaxis$ for item in x.get_ticklabels():$     item.set_rotation(45)
extract_deduped_with_elms_v2.loc[(~extract_deduped_with_elms_v2.ACCOUNT_ID.isnull())$                              &(extract_deduped_with_elms_v2.LOAN_AMOUNT.isnull())].shape
df = df.drop_duplicates()$
apple.resample('M').mean().plot(grid=True)
previous_month_date = end_date - timedelta(days=30)$ pr = PullRequests(github_index).get_cardinality("id").since(start=previous_month_date).until(end=end_date)$ get_aggs(pr)
if not os.path.isdir('output/pv_production'):$     os.makedirs('output/pv_production')
cwd = os.getcwd()$ cwd
sns.distplot(df['num_comments'])$ plt.title("Distribution - Number of comments");
round((timelog.seconds.sum() / 60 / 60 / 24), 1)
df = df.dropna(axis=0, how='any')$ df = df.loc[df['fuel_litre'] < 40 ]$ df = df.loc[df['fuel_litre'] > 4 ]
key = 'o_r_mapper:91ala:ala103077070913625'$ rdb = c.srdb_new(0)$
%matplotlib inline$ seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)
stn_cnt_df=pd.DataFrame(stations_des,columns=['Station','Counts'])$ stn_cnt_df.head()
so.loc[so['favoritecount'].between(30, 40), 'title'::3].head()
raw_data = pd.read_csv("kickstarter.csv") #Example$ raw_data.head()$ print ("The provided data set consists of",raw_data.shape[0],"rows and",raw_data.shape[1],"columns (features.")
sentiment_overall = news_sentiments.groupby('News Source').agg({'Compound': np.mean}).reset_index() $ sentiment_overall
from google.colab import auth$ auth.authenticate_user()
Osha_AccidentCases['Title_Summary_Case'] = Osha_AccidentCases['Title_Summary_Case']$ Osha_AccidentCases.head()
import time$ print (time.strftime("%Y%m%d"))
df.loc['1998-09-10':'1998-09-15']
[random_date(pd.datetime.now() - pd.offsets.Day(10), pd.datetime.now()) for _ in range(10)]
df = pd.read_csv(datafile)$ print(df.as_matrix().shape)
rng = pd.date_range('3/6/2012 00:00:00', periods=10,freq="D",tz="US/Mountain")$ rng.tz, rng[0].tz
search.timestamp = pd.to_datetime(search.timestamp)$ search.trip_start_date = pd.to_datetime(search.trip_start_date)$ search.trip_end_date = pd.to_datetime(search.trip_end_date)
gdax_trans['Timestamp'] = gdax_trans.apply(lambda row: fix_timestamp(row["Timestamp"]), axis=1)
idx = np.random.permutation(train_data.index)$ train_data = train_data.reindex(idx)$ train_labels = train_labels.reindex(idx)
datecols = ["CreationDate"]$ for datecol in datecols:$     qs[datecol] = pd.to_datetime(qs[datecol], origin="julian", unit="D")
scores_median = np.median(sorted(raw_scores))$ print('The median is {}.'.format(scores_median))
df['label'] = df[forecast_col].shift(-forecast_out)
our_nb_classifier.predict("The car lights turned off and THROTTLE did not work when driving for a long time")
joined.dtypes.filter(items=['Frequency_score'])
twitter_json = r'data/twitter_01_20_17_to_3-2-18.json'$ tweet_data = pd.read_json(twitter_json)
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
plt.scatter(branch_dist[1:],branch_r[1:])
from statsmodels.tsa.arima_model import ARIMA$ model_6201 = ARIMA(dta_6201, (5, 1, 2)).fit() $ model_6201.forecast(5)[:1] 
not_in_oz = not_in_oz[['stopid', 'address', 'lat', 'lng', 'routes']]$ new_stops = new_stops.append(not_in_oz)$ new_stops.head(5)
donor_groups = df3_obs[['Donor ID', 'Donation Received Date']].groupby('Donor ID')$ time_diff = donor_groups.apply(lambda df: df['Donation Received Date'].diff().mean().fillna(0))$ print(time_diff.head())
count_pages=df2.groupby(['landing_page']).size()# this gives the count of both new and old pafes together$ n_new=count_pages[0]# this gives the count for the new pages$ n_new
nulls=(c_date.notnull()==False)$ nulls.value_counts()
os.chdir(out_path)$ os.getcwd()
table_rows = driver.find_elements_by_tag_name("tbody")[14].find_elements_by_tag_name("tr")$
autoDf.createOrReplaceTempView("autos")$ SpSession.sql("select * from autos where hp > 200").show()
df.loc[monthMask, 'water_year'] = df['year'] + 1
r= requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json")$ json_data = r.json()$ json_data["dataset_data"]["data"][0]
A = np.arange(25).reshape(5,5)$ A[[0,1]] = A[[1,0]]$ print(A)
url_1day='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=TaaJzC_if3-1gX5Wty4D&start_date=2017-04-10&end_date=2017-04-10'$ response=requests.get(url_1day)
PYR['soup'] = PYR.apply(create_soup, axis = 1)
print(type(coin_data.index))$ coin_data.columns
y = train.rating
from tensorflow.python.client import device_lib$ device_lib.list_local_devices()
mapInfo_string = str(serc_mapInfo.value) #convert to string$ mapInfo_string.split?
feature_cols = ['TV', 'radio', 'newspaper']$ X = data[feature_cols]$ y = data.sales
for row in my_df_large.itertuples():$     pass$
from IPython.core.display import display, HTML$ display(HTML("<style>.container { width:100% !important; }</style>"))
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)])$ new_page_converted.mean()
[scores.apply(sum), scores.apply(np.mean)]
temp_df['reorder_interval_group'].replace('', np.nan, inplace=True)$ temp_df.dropna(subset=['reorder_interval_group'], inplace=True)
df_birth[df_birth.population > 1000000000]$
pook_url = "http://www.djbible.classicalgasemissions.com/book_of_pook.pdf"$ pook_dl = requests.get(pook_url, stream = True)
data.map(add_one).collect()
stero_grid_file = E.obs['NSIDC_0051']['grid']$ obs_grid = import_data.load_grid_info(stero_grid_file, model='NSIDC')$ obs_grid['lat_b'] = obs_grid.lat_b.where(obs_grid.lat_b < 90, other = 90)
filename = "HubProcessDurations.xlsx"$ xl = pd.ExcelFile(filename)$ tabs = xl.sheet_names
cats_out = outcome.loc[outcome['Animal Type']=='Cat']$ cats_out.shape
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);
ET_Combine = pd.concat([hour_1dRichards, hour_lumpedTopmodel, hour_distributedTopmodel_average], axis=1)$ ET_Combine.columns = ["Baseflow = 1D Richards'", 'Baseflow = Topmodel(lumped)', 'Baseflow = Topmodel(distributed)']
who_purchased = pd.get_dummies(questions['purchased'])
s.index[0]
json_data = r.json()$ for k in json_data.keys():$     print(k + ': ', json_data[k])$
import matplotlib.pyplot as plt$ plt.plot(twitter_df['created_at_time'], twitter_df['retweet_count'], 'ro')$ plt.show()$
cpdi = pd.DataFrame(coreval); cpdi.rename(columns={0:'i', 1:'core'}, inplace=True)$ xpdi = pd.DataFrame(xtraval); xpdi.rename(columns={0:'i', 1:'xtra'}, inplace=True)
dfAnnualMGD = dfHaw_Discharge.groupby('Year')['flow_MGD'].agg(['sum','count'])$ dfAnnualMGD = dfAnnualMGD[dfAnnualMGD['count'] > 350]$ dfAnnualMGD.columns = ['AnnualFlow_MGD','Count']
knn_reg.score(x_test,y_test)
directory = './Models'$ if not os.path.exists(directory):$     os.makedirs(directory)
req = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=HL_EJeRkuQ-GFYyb_sVd&start_date=2017-01-01&end_date=2017-12-31')
engine = create_engine("sqlite:///hawaii.sqlite")
df_merge.head()
idx = pd.IntervalIndex.from_arrays(df2.Start, df2.End, closed='both')$ idx
coinbase_btc_eur_min.plot(kind='line',x='Timestamp',y='Coin_price_EUR', grid=True);
new_page_converted.mean()-old_page_converted.mean()
df_cod2["Cause of death"] = df_cod2["Cause of death"].apply(standardize_cod)$ df_cod2["Cause of death"].value_counts()
model = gensim.models.Word2Vec(sentences, size=200)
import os$ import gmaps$ gmaps.configure(api_key='AIzaSyC-TiBcF4pJGqxRyg0G3q7tCajyHYnf98E') # Your Google API key$
pd.DataFrame(dummy_var["_Source"][Company_Name]['Close']['Forecast'])[-6:]$
dfall.info()
experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)
def ceil_dt(dt):$     delta = timedelta(minutes=15)$     return (dt + (datetime.min - dt) % delta) + timedelta(hours=2)
rsvp_df = pd.read_csv("last_five_rsvp_means.csv")$ rsvp_df.head()
masked.user_created_at = pd.to_datetime(masked.user_created_at)
crimes.columns = crimes.columns.str.replace(' ', '_')$ crimes.columns
(r_clean * hist_alloc).sum(axis=1).hist(bins=50)
metadata['map_info'] = refl['Metadata']['Coordinate_System']['Map_Info'].value$ metadata
options_frame['BidAskSpread'] = options_frame['Ask'] - options_frame['Bid']$ errors_20_largest_by_spread = options_frame.ix[sorted_errors_idx.index]$ errors_20_largest_by_spread[['BidAskSpread', 'ModelError']].sort_values(by='BidAskSpread').plot(kind='bar', x='BidAskSpread')
total_students_with_passing_reading_score = len(df_students.loc[df_students['reading_score'] > 69])$ total_students_with_passing_reading_score
cabs_df_byday = cabs_df.loc[cabs_df.index.weekday == weekday]$ cabs_df_byday.info()$ cabs_df_byday.head()
inspector = inspect(engine)$ inspector.get_table_names()
df['CIK']=df['CIK'].map(lambda x:str(x).zfill(10))
stars = dataset.groupby('rating').mean()$ stars.corr()
result_1 = pd.concat([df1, df3], axis = 1) # concatenate one dataframe on another along columns$ result_1
df_full = pd.DataFrame()$ df_list = []
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()$
experience.to_csv('../data/experience.csv')
df = pd.read_csv(r"C:\Users\Adi\Desktop\Data_Science\Capstone1\DataSet.csv")$ df.info()
pd.Series(np.r_[1,2,9])
df.mean()
df = pd.read_csv('../nba-enhanced-stats/2016-17_teamBoxScore.csv')$ df.head()
zstat_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(zstat_score, p_value)$ print("The Pvalue for null Hypothesis is {}".format(p_value))
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep =' ')$ df.head(5)
from IPython.core.display import display, HTML$ display(HTML("<style>#notebook-container { margin-left:-14px; width:calc(100% + 27px) !important; }</style>"))
df_reg=injuries_hour[['date_time','Rain','injuries','wet','low_vis']]$ df_reg['hour']=pd.to_datetime(df_reg.date_time).dt.hour$ df_reg.head()$
df = df.drop_duplicates(subset='id', keep='last')$ df.drop(columns='Unnamed: 0', axis=1, inplace=True)$ print(df.shape)
ibm_hr_target_small.stat.corr("Age", "DailyRate")
vacancies['created'] = vacancies['created'].apply(lambda x: dateutil.parser.parse(x))
edge_types_DF = pd.read_csv('network/recurrent_network/edge_types.csv', sep = ' ')$ edge_types_DF
cityID = '161d2f18e3a0445a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Raleigh.append(tweet) 
test_features = bind_features(test, train_test="test").cache()$ test_features.count()
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&collapse=null&api_key=my_API_KEY'
preci_data = session.query(Measurement.date, Measurement.prcp).\$     filter(Measurement.date > last_year).\$     order_by(Measurement.date).all()
gs_lr_tfidf.fit(X_train, y_train) 
sentiments_df = pd.DataFrame(sentiment_array)
data.drop(['ceil_15min'], axis=1,inplace=True)
data.Likes.value_counts(normalize=True)
scaler_fit.inverse_transform(predict_actual_df).shape
s2.mean(), s2.std(), s2.unique() # numpy functions ignore nan's
print(data.index.value_counts())$ data=data.drop(data.index[-1])
number = 100$ factor = 1.1$ text =  "hello world"
urban_avg_fare = urban_type_df.groupby(["city"]).mean()["fare"]$ urban_avg_fare.head()
result_merged.summary2()
from statsmodels.tsa.stattools import acf, pacf
df=dbobject.postgre_to_dataframe('select * from tripadvisor')$ df.head(200)
tfidf_vect = TfidfVectorizer(stop_words="english")$ tfidf_mat = tfidf_vect.fit_transform(PYR['soup'].values)
%matplotlib inline$ df_concat_2.boxplot(column="message_likes_rel",by="page")$
from sklearn.model_selection import cross_val_score$ from sklearn.ensemble import GradientBoostingClassifier$ forest_clf.score(X_test, Y_test) # test socre$
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
test = vec1.fit_transform(df.message[1]) #takes 2. row in df for testing$ for i in test:$     print(i)$
recommendations = (model.recommendProducts(1059637, 5))$ recommendations[: 5]$ recArtist = set(list(elt[1] for elt in recommendations))$
Jarvis_resistance_simulation_1 = Jarvis_ET_Combine['Jarvis(Root Exp = 1.0)']$ Jarvis_resistance_simulation_0_5 = Jarvis_ET_Combine['Jarvis(Root Exp = 0.5)']$ Jarvis_resistance_simulation_0_25 = Jarvis_ET_Combine['Jarvis(Root Exp = 0.25)']
store_items.fillna(0)
my_gempro.get_msms_annotations()
df = pd.read_csv('./Data/AAPL.csv', index_col=0)$ df.head()
sample.dtypes
import numpy as np$ import matplotlib.pyplot as plt$ import pandas as pd
xml_in[xml_in['venueName'].isnull()].count()
plotdf.loc[:thisWeekHourly['hourNumber'].max(), :] = plotdf.loc[:thisWeekHourly['hourNumber'].max(), :].fillna(method='ffill')$ plotdf.loc[:thisWeekHourly['hourNumber'].max(), :] = plotdf.loc[:thisWeekHourly['hourNumber'].max(), :].fillna(0)
store_items = store_items.rename(columns = {'bikes': 'hats'})$ store_items
for item in bottom_three:$     print('{} has a std of {}.'.format(item[0], item[1]))
inflex_words = en_translation_counts[en_translation_counts < 2]$ print("Total: {} unique words".format(len(inflex_words)))$ inflex_words.sample(10)
dti.freq
brands_np = np.asarray(brands)$ models_np = np.asarray(models)
newfile.insert(0, 'CSCA - Add notes here of customer requirement', '')$ newfile.insert(1, 'Direction from Diana', '')
production_df = pd.merge(future_predictions, features, on=['Date', 'HomeTeam', 'AwayTeam', 'season'])
slope, intercept, r_value, p_value, std_err = stats.linregress(data['timestamp'],data['rate'])
bands.sum(axis=0)$
df['money_csum'] = df.groupby(['program_id'])['money_collected'].cumsum()$ df['cum_pct'] = df.groupby('program_id')['money_csum'].transform(lambda x: x * 100/ x.iloc[-1])$ df
weather_mean.iloc[1, 4]
calls_df["dial_type"].value_counts()
top_100_2016['release_date'] = pd.to_datetime(top_100_2016['release_date'], format = '%Y-%m-%d' )
plt.pie(count_driver, labels=type_driver,explode=explode, colors=colors,$         autopct="%1.1f%%", shadow=False, startangle=140)$
data['Age'].hist(bins = 10)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key='YOUR_API_KEY'&start_date=2018-07-04&end_date=2018-07-05")
del merged_portfolio_sp_latest_YTD['Date']$ merged_portfolio_sp_latest_YTD.rename(columns={'Adj Close': 'Ticker Start Year Close'}, inplace=True)$ merged_portfolio_sp_latest_YTD.head()
GamelogParser.GameLog().passing(urls[:1][0], season)
activity_df = pd.DataFrame(joined_data)
trump_origitnals["lText"] = trump_o["text"].map(lambda x: x if type(x)!=str else x.lower())
adj_close_pivot = adj_close_acq_date_modified.pivot_table(index=['Ticker', 'Acquisition Date'], values='Adj Close', aggfunc=np.max)$ adj_close_pivot.reset_index(inplace=True)$ adj_close_pivot
final_df = pd.concat([drugs_through_bp, other_drug])$ final_df = final_df.sort_values(by=['DWPC'], ascending = False)$ final_df
conn.close()
df.plot(subplots=True)$ plt.show()
open('test_data//open_close_test.txt')
eth = pd.read_csv("Ether-chart.csv", sep=',')$ eth['date'] = ' '$ eth.info()$
%matplotlib inline$ commits_per_year.plot(kind='line', legend=False, title='commits per year' )
data_compare['SA_mix'] = np.array([ analize_sentiment_multilingual(tweet) for tweet in data_compare['tweets_original'] ])
tcat_df = tcat_df.append(tcat)$ tdog_df = tdog_df.append(tdog)
save_filepath = os.path.expanduser("~")
test_classifier('c1', WATSON_CLASSIFIER_ID_2)$ plt.plot(classifier_stats['c1'], 'ro')$ plt.show()
NYT = news_df.loc[(news_df["Source Account"] == "nytimes")]$ NYT.head(2)
groupedNews = sentiments_df.groupby(["User"], as_index=False)$ groupedNews.head()
yhat_summore = modell.predict(X_dunno)$
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3.to_hdf('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.h5', key='df', mode='w')
iv = options_frame[options_frame['Strike'] == 130.0]$ iv_call = iv[iv['OptionType'] == 'call']$ iv_call[['Expiration', 'ImpliedVolatilityMid']].set_index('Expiration').plot()
sns.boxplot(calls_df["length_in_sec"],orient='v')
customers_arr = np.array(cust_list)$ items_arr = np.array(item_list)
ts.tshift(5,freq="H")
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score
gene_df['length'] = gene_df.end - gene_df.start + 1$ gene_df['length'].describe()
import matplotlib$ import matplotlib.pyplot as plt$ pd.plotting.scatter_matrix(newMergedDF, figsize=(88, 88))$
firewk18= fire[(fire['incident_date'].dt.year ==2018) &(fire['incident_date'].dt.month ==5)].groupby('dayofweek')['incident_number'].agg({'numofinstance_fire':'nunique'}).reset_index()$
for leg in plan['plan']['itineraries'][0]['legs']:$     print('distance = {:,.2f} | duration = {:.0f} | mode = {} | route = {} | steps = {}'.\$ format(leg['distance'], leg['duration'], leg['mode'], leg['route'], len(leg['steps'])))
pd.Series({'a': 42, 'b': 13, 'c': 2})
tweets_raw["date"] = tweets_raw["date"].apply(lambda d: parse(d))
station_count = session.query(Measurement.station).distinct().count()$ print('There are',station_count,'stations')
import os$ os.environ["THEANO_FLAGS"] = "mode=FAST_RUN,device=cuda0,floatX=float32"
festivals.at[2,'latitude'] = 41.9028805$ festivals.head(3)
sherpa = current.loc[df["By Name"] ==  "Sherpa "] $ sherpa
df2 = pd.DataFrame(df.groupby("media source").mean()['compound score'])$
table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']$ table_list = [pd.read_csv(f'{fname}.csv', low_memory=False) for fname in table_names]$ for table in table_list: display(table.head())
festivals['lat_long'] = festivals[['latitude', 'longitude']].apply(tuple, axis=1)
ideas.drop(['Authors', 'Link', 'Tickers', 'Title', 'Strategy'],axis=1,inplace=True)$ ideas = ideas.applymap(to_datetime)
dfX = data.drop(['created_at','date','ooCost'], axis=1)$ dfY = data['ooCost']
dfd.zones.value_counts()
op_ed_articles = pd.read_json('nytimes_oped_articles.json')$ op_ed_articles = op_ed_articles.reset_index(drop=True)$ op_ed_articles.head()
articles['content_short'] = articles['tokens'].map(lambda s: ' '.join(s))
partition = community.best_partition(G)$ print('Modularity: ', community.modularity(partition, G))
url = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=xQWdR8uvz6QJeQ6gqN5g&start_date=2017-01-01&end_date=2017-12-31')$ json_data = url.json()$ data = json.dumps(json_data['dataset'])$
data_dict = json.loads(data.text)
train,test = iris.split_frame([0.8])
max_sharpe_port_optim = results_frame_optim.iloc[results_frame_optim['Sharpe'].idxmax()]$ min_vol_port_optim = results_frame_optim.iloc[results_frame_optim['SD'].idxmin()]
print(svm_clf.support_vectors_.shape)$ print(svm_clf.support_.shape)$ print(svm_clf.n_support_ )$
df.to_csv("NewsMood.csv")
pd.Period('1/1/21', 'D') - pd.Period(pd.datetime.today(), 'D')
with open('all_data_vectorized.dpkl', 'wb') as f:$     dpickle.dump(all_data_vectorized, f)
sampleSize=ratings.groupby("rating").count().reset_index()['userId'].min()$ sampleSize
df_RSV = df_full[df_full.Field4 == "RSV"]
dfHaw_Discharge = getNWISData('02096960')$ dfHaw_Discharge.head()
for row in session.query(stations, stations.station).all():$     print(row)
query ="SELECT * FROM tdata_db_2.Weather_Log ORDER BY Log_Id DESC"$ df = pd.read_sql(query,session)$ df.head(10)
df_TempIrregular.to_csv('data/All_Irregularities_20180601_to20180607.csv', sep=',')
df_vndr_id = pd.DataFrame(X_copy['vndr_id'])
string_2018 = 'extracted_data/tmin.2018.csv'
all_tables_df.OBJECT_TYPE.count()
for screen_name in napturalistamo_youtubers_timelines_grouped.index:$     with open("../output/black_hair_culture/text/{0}_timeline.txt".format(screen_name), "w") as text_file:$         text_file.write(napturalistamo_youtubers_timelines_grouped[screen_name])
precip_df.describe()
def class_size(Y):$     unique, counts = np.unique(Y, return_counts=True)$     return dict(zip(unique, counts))$
close_idx = afx['dataset']['column_names'].index('Close')
gen = image.ImageDataGenerator()$ batches = gen.flow(X_train, y_train, batch_size=64)$ test_batches = gen.flow(X_test, y_test, batch_size=64)
media_user_results_df.to_csv("MediaTweetsData.csv", encoding="utf-8", index=False)
data_kb = pd.DataFrame(data=[tweet.text for tweet in tweets_kb], columns=['Tweets'])$ display(data_kb.head(10))
past_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ past_year
not_numbers = data_read.genre_ids.astype(str).apply(lambda x: x.isnumeric()) == False$ data_read["genre_ids"][not_numbers.values].sample(10)
df = df[df.userTimezone.notnull()]$ len(df)
%run twitter_creds.py
tobs_stn_281_df = pd.DataFrame.from_records(tobs_stn_281, columns=('Date','Station','Tobs'))$ tobs_stn_281_df.head()
session.query(Station.name).count()
dfs['DE'].groupby(['comment', 'data_source'])['electrical_capacity'].sum().to_frame()$ dfs['DE'].groupby(['comment', 'energy_source_level_2'])['electrical_capacity'].sum().to_frame()
materials_file = openmc.Materials([fuel, water, zircaloy])$ materials_file.export_to_xml()
df.to_csv("data/processed/" + "processed.csv", sep=',', encoding='utf-8', index=False)
import sys$ import os$ from urllib.request import urlretrieve
prec_nc = Dataset("../data/nc/pr_wtr.mon.mean.nc")
from sklearn.naive_bayes import GaussianNB$ gnb = GaussianNB()$ gnb.fit(X_train, Y_train)
test_tweet = api.user_timeline(newsOutlets[0])$ print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
S_1dRichards.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
turnstiles_df = turnstiles_df.rename(columns=lambda x: x.strip())$
num_stations = session.query(func.count(hi_stations.STATION.distinct())).scalar()$ print("Number Of Stations: " + str(num_stations))
hit_filter_df.to_csv("Desktop/Project-2/numberOneUnique.csv", index=False, header=True)
data.describe(include=['O'])
sentiments_pd.to_csv("Twitter_News_Mood.csv", index=False)
import sys$ sys.version
rfc_features = sorted(list(zip(test_features, rfc.feature_importances_)), key=lambda x: x[1], reverse=True)$ rfc_features
filename = "../datasets/catalogs/fermi/gll_psc_v16.fit.gz" $ print([_.name for _ in fits.open(filename)])$ extended_source_table = Table.read(filename, hdu='ExtendedSources')
pipe_lr = make_pipeline(cvec, lr)$ pipe_lr.fit(X_train, y_train)$ pipe_lr.score(X_test, y_test)
with open('datasets/git_log_excerpt.csv', 'r') as file:$     file = file.read()$     print(file)$
jd = r1.json()$ print(jd)
text = "Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers."$ print(text)
comments = df.loc[:,'comment_body']$ print(comments[0])$ type(comments)
import sqlalchemy as sa$ engine = sa.create_engine('redshift+psycopg2://admissionsapi:admissionsapi@admissions-api.cv90snkxh2gd.us-west-2.rds.amazonaws.com:5432/admissionsapi')$ pd.read_sql("select * from applicants limit 10",engine)
cursor = db.tweets.find({}, {'text':1, 'id':1,'user':1, 'hashtags':1,'_id': 0})$ df =  pd.DataFrame(list(cursor))$ df.head(3)
grouped_publications_by_author.tail(10)
df_2008['bank_name'] = df_2008.bank_name.str.split(",").str[0]$
interpolated = bymin.resample("S").interpolate()$ interpolated
df.drop(["urlname"], axis = 1, inplace=True)$ top_rsvp.drop(["urlname"], axis = 1, inplace=True)$ top_rsvp.head(5)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(doc_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours First Difference Partial Autocorrelation')
print('input dim = {}'.format(input_image.ndim))$ print('input shape = {}'.format(input_image.shape))
df_new['CA_ab_page'] = df_new['CA'] * df_new['ab_page']$ df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page']$ df_new.head()
day2int = {v.lower():k for k,v in enumerate(calendar.day_name)}$ day2int
from ssbio.pipeline.gempro import GEMPRO
intervention_train.reset_index(inplace=True)$ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL'], inplace=True)
'Similarly, militant groups in the Niger Delta attacked Nigeria oil facilities to the point that production fell to roughly half of its former levels at times.'$ 'But Nigeria still expects that its exemption will continue for at least six months, according to oil minister Ibe Kachikwu.'$ 'Libya also has plans to raise output to 1.32 million bpd by the end of the year, up from an earlier target of 1.1 million bpd.')
bird_data['timestamp'] = pd.Series(timestamps, index = bird_data.index)$ print(bird_data.timestamp.head())
table = pd.read_html("https://en.wikipedia.org/wiki/List_of_sandwiches", header=0)[0]$
plot_BIC_AR_model(data=therapist_duration.diff()[1:], max_order_plus_one=10)
search['prefetch'] = search['message'].apply(prefetch)
df_date_precipitation=pd.DataFrame(date_precipitation, columns=['date','precipitation'])$ df_date_precipitation.set_index('date', inplace=True)$ df_date_precipitation.head()
word_centroid_map = dict(zip(model.wv.index2word, idx))
for col in X_numcols:    $     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
vect = CountVectorizer(stop_words=stop_words, ngram_range=(2,4), min_df=0.0001)$ X = vect.fit_transform(fashion.text)
print(df_sentiments)$ df_sentiments = df_sentiments['score'].apply(pandas.to_numeric, errors='ignore')$ df_sentiments_means = df_sentiments.transpose().apply(numpy.mean, axis=1)$
resp_dict = r.json()$ type(resp_dict)
sns.distplot(questions_scores[:int(len(questions_scores)*0.99)])
y = x.loc[:,"A"]$ y
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-04-27&end_date=2018-04-27&api_key=3w2gwzsRAMrLctsYLAzN')$ json_sample = r.json()
x_normalized = intersections_irr[for_normalized_columns].values.astype(float)
import pandas as pd$ import numpy as np$ import matplotlib.pyplot as plt
term_tops = lda.get_term_topics(rand_word, minimum_probability=.0001)$ term_tops, get_topic_desig(term_tops)
containNaN=final_df[final_df.isnull().any(axis=1)]$
layout_row = bokeh.layouts.row(fig_out)$ bokeh.plotting.show(layout_row)
filtered_df[filtered_df['M_pay_3d'] == 0].shape[0]
y=dataframe1['CCI_30']$ plt.plot(y)$ plt.show()
all_tables_df.loc[[2, 3, 4, 10, 2000], ['OBJECT_NAME', 'OBJECT_TYPE']]
dfbreakfast = df[(df['TIME'] == '11:00:00') | (df['TIME'] == '12:00:00')]$ dfbreakfast.head(2)
! rm -rf recs2$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models2 --outdir recs2
df.count()
df4['preprocess_tweet'] = df4['tweet'].apply(lambda x : preprocess(x))$ df4.head()
classification_df = encoded_df.query('(id == 5 or id == 14) and best != 0')$ classification_df.best.describe()
from sklearn.neighbors import KNeighborsClassifier$ from sklearn.model_selection import train_test_split
m = pd.Period("2011-01",freq='M')$ print(m.start_time)$ print(m.end_time)
multi_col_lvl_df.applymap(lambda x: np.nan if np.isnan(x) else str(round(x/1000, 2)) + "k").head(10)
DATA_PATH = "../data/query.csv"$ df = pd.read_csv(DATA_PATH)
people.dtypes
pd.crosstab(data.userName, data.Likes.head())
data['timestamp'] = pd.to_datetime(data['Date'], unit='s')
sns.set_style('whitegrid')$ sns.distplot(data_final['countCollaborators'], kde=False,color="red", bins=25) $
lr1 = LogisticRegression(random_state=20, max_iter=10000, C=0.5, multi_class= 'ovr', solver= 'saga')$ lr1.fit(X, y)$ lr1.score(X_test, y_test)
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')$ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
timeEnd = ("2017-06-07 23:59:59")$ timeStart = ("2015-06-09 00:00:00")$ timeNow = datetime.strptime(timeNow, "%Y-%m-%d %H:%M:%S")
readRenviron("~/.Renviron")
fundret.idxmax(), fundret[fundret.idxmax()]
sns.boxplot(data=sample)
conn = engine.connect()$ measure_df = pd.read_sql("SELECT * FROM Measurement", conn)
!head ml-100k/u.data
combined_df = user_df.merge(response_df, how='inner', left_on='response_id', right_on='id')$ print len(combined_df), len(user_df), len(response_df)
csvFile = open('ua.csv', 'a')
sp = openmc.StatePoint('statepoint.082.h5')
tbl_detail = conn.get_table_details("nyctaxi")$ pd.DataFrame(tbl_detail)
df.dropna(subset=['insert_id'], how='all')$ df = remove_duplicate_index(df)
birth_dates.set_index("BirthDate_dt").loc["2014-01-01":,:]
df_obj2.index[0] = 2$
sts_model.pred_table()
TensorBoard().stop(23002)$ print 'stopped TensorBoard'$ TensorBoard().list()
grouped.size()
count_authors_with_given_numer_publications = data_final.groupby('countPublications', as_index=False)['authorId'].count()$ count_authors_with_given_numer_publications.columns = ['number_publications', 'how_many_authors_with_given_publications']$ count_authors_with_given_numer_publications.head(20)
cdata.loc[cdata['Number_TD'] > 1, 'Number_TD' ] = 1
json_data_2017 = request_data_2017.json()
import csv$ data = pd.read_csv("McDonaldstweets.csv")$ display(data)
tz_cat['tweetRetweetCt'].max()$ tz_cat.index[tz_cat['tweetRetweetCt'] == tz_cat['tweetRetweetCt'].max()].tolist()
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$ model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)
all_features = pd.get_dummies(all_features, dummy_na=True) $ all_features.shape
tips.set_index(["sex","day"]).head(5)
x_train, x_test, y_train, y_test = train_test_split(data_woe, df['y'], test_size=0.2, stratify=df['y'], random_state=17)$ print(str(datetime.datetime.now()) + ' train_test_shapes:', x_train.shape, x_test.shape)$
data.isnull().sum()
expiry = datetime.date(2015, 1, 5)$ msft_calls = Options('MSFT','yahoo').get_call_data(expiry=expiry)$ msft_calls.iloc[0:5,0:5]
news_items = soup.find_all(class_='slide')$ news_title = news_items[0].find(class_='content_title').text$ print (news_title)
encoded_df = pd.get_dummies(df, columns=['category', 'fileType'])$ encoded_df.shape
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2017-11-01&end_date=2017-11-02')
def create_df_grouped_by_date( tweets_df ):$     return tweets_df.groupby( Grouper( 'date' )).mean()
ADBC = AdaBoostClassifier(n_estimators=50)$ ADBC.fit(X_resampled, y_resampled) 
df['lead_mgr_counts'] = df.groupby(['lead_mgr'])['lead_mgr'].transform('count')$
small_data = pd.concat([data.iloc[:2],$                       data.iloc[-2:]])
store_items.fillna(0)
accuracy=100*metrics.accuracy_score(y_test, y_hat)$ print(accuracy)
Z = np.arange(1,15,dtype=np.uint32)$ R = stride_tricks.as_strided(Z,(11,4),(4,4))$ print(R)
neuron_no = 10$ source_indices_L23exc_L23fs = np.where(np.array(conn_L23exc_L23fs.i)==neuron_no)$ target_indices_L23exc_L23fs = np.array(conn_L23exc_L23fs.j)[source_indices_L23exc_L23fs]
lon_us = lon[lon_li:lon_ui]-360$ lat_us = lat[lat_li:lat_ui]$ print(np.min(lon_us), np.max(lon_us), np.min(lat_us), np.max(lat_us))
labels = list(crf.classes_)$
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=' + API_KEY)$
x_scaled = min_max_scaler.fit_transform(x_normalized)
users_chanel = pd.merge(Users, Relations, how = 'outer', on=['id_partner', 'name'])$ users_chanel.head()
df.label.value_counts()
df.shape
BMonthEnd().rollforward(datetime(2014,9,15))
results = [i.lower() for i in names]$ df = pd.DataFrame(results,columns={'Sample'})$ df['Number'] = df.index
datetime.date(datetime(2014,12,14))
X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.2, random_state=23)
affair_children = pd.crosstab(data.children, data.affair.astype(bool))$ affair_children
processing_test.files
import alpha_vantage$ from alpha_vantage.timeseries import TimeSeries$ ts = TimeSeries(key='1250F9WWA3Z77BIK')$
new_discover_sale_transaction = post_discover_sales[post_discover_sales['Email'].isin(new_customers_test['Post Launch Emails'].unique())]$ new_discover_sale_transaction['Total'].mean()
betas_mask = np.zeros(shape=(mcmc_iters, n_bandits))$ betas_mask[np.arange(mcmc_iters), betas_argmax] = 1
import matplotlib.pyplot as plt$ import pandas as pd$ import numpy as np
print('The data in Groceries is:', groceries.values)$ print('The index of Groceries is:', groceries.index)
kick_projects.loc[:,'goal_reached'] = kick_projects['pledged'] / kick_projects['goal'] # Pledged amount as a percentage of goal.$ kick_projects.loc[kick_projects['backers'] == 0, 'backers'] = 1 $ kick_projects.loc[:,'pledge_per_backer'] = kick_projects['pledged'] / kick_projects['backers'] # Pledged amount per backer.
artists_info = [sp.artist(artist_id[i]) for i in range(0,num_songs)]
!rm world_bank.json.gz -f$ !wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz
results = []$ for line in file_handle:$     results.append(line.replace('foo', 'bar'))
!head -n 2 ProductPurchaseData.txt
data_2018 = data_2018.rename(columns={'Unnamed: 0':'time'})
data = pd.concat([data, df_cat], axis=1)
avg_monthly = np.mean(df.month.value_counts())$ std_monthly = np.std(df.month.value_counts())$ print('The average beers drank per month is {:.2f} beers and the standard deviation is {:.2f} beers.'.format(avg_monthly, std_monthly))
TripData_merged = pd.concat([TripData1, TripData2, TripData3])
df6.mean() # mean of lunch values$
data = spark.read.csv('sensor_data.csv',header=True)
suburban_ride_total = suburban_type_df.groupby(["city"]).count()["ride_id"]$ suburban_ride_total.head()$
duplicates = df_cust_data[df_cust_data['Email Address'].isin(df_cust_data['Email Address'].\$                     value_counts()[df_cust_data['Email Address'].value_counts()>1].index)]$ len(duplicates[duplicates['Email Address'] != "no email"])/2
reflClean = reflRaw.astype(float)$ reflClean
plt.hist(data.sepal_length, bins=25)
table_rows = driver.find_elements_by_tag_name("tbody")[30].find_elements_by_tag_name("tr")$
local = mngr.connect(dsdb.LOCAL)$ local
coin_data.describe()
df.dtypes
df['Injury'] = [1 if 'placed' in text else 0 for text in df.Notes]
from scipy import stats$ instance.initialize(parameters)
r= requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json")$ json_data = r.json()$ json_data["dataset_data"]["data"][0]
table_rows = driver.find_elements_by_tag_name("tbody")[10].find_elements_by_tag_name("tr")$
dforders = ml4t.build_orders(dfprediction, abs_threshold=0.01, startin=False, symbol='USD-BTC')$ dforders
from selenium import webdriver$ driver = webdriver.Chrome()
!hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part3/* | sort -k1,1nr -k3,3 | head -n 20
feat = ['categoryname', 'eventname', 'location']$ for f in feat:$     PYR[f] = PYR[f].apply(clean_dataset)
y_pred = crf.predict(X_valid)$ metrics.flat_f1_score(y_valid, y_pred,$                       average='weighted', labels=labels)
orig_ticker = pd.DataFrame(ticker) $ orig_ticker['Ticker_Symbol'] = pd.Series(ticker, index=data_ticker.index)
hm = pd.read_csv('Resources/hawaii_measurements.csv')$ hs = pd.read_csv('Resources/hawaii_stations.csv')
contractor_clean['last_updated'] = pd.to_datetime(contractor_clean.last_updated)$ contractor_clean['updated_date'] =contractor_clean['last_updated'].dt.strftime('%m/%d/%Y')$ contractor_merge['month_year'] =contractor_merge['last_updated'].dt.to_period('M')
file1 = '/Users/gta/dev/hw-4/schools_complete.csv'$ file2 = '/Users/gta/dev/hw-4/students_complete.csv'
volt_prof_before.set_index('Bus', inplace=True)$ volt_prof_after.set_index('Bus', inplace=True)
vocab = vect.get_feature_names()
X2.time_since_meas_years.hist()
result = api.search(q='%23H2P')
fname = '/Users/kylefrankovich/Desktop/insight/list_test_data/deluxetattoochicago/deluxetattoochicago.json'$ data = json.load(open(fname))$ len(data)
fraq_volume_m_sel = b_mat.as_matrix() * fraq_volume_m_coins$ fraq_fund_volume_m = fraq_volume_m_sel.sum(axis=1)$
model_arma = sm.tsa.ARMA(AAPL_array, (2,2)).fit()$ print(model_arma.params)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=JVTZ9kPgnsnq9oFbym2s&start_date=2018-05-17'$ r = requests.get(url)$ print(r.text)
news_df = (news_df$            .loc[news_df['num_reactions'] - $                 news_df['num_likes'] >= 10,:])
aapl = pd.read_csv(file_name, index_col='Date')$ aapl
year_info = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date.between('2016-08-23', '2017-08-23')).all()$ year_info_df = pd.DataFrame(year_info)$ year_info_df.set_index("date").head()$
import statistics$ statistics.median(v)
dfGoles.columns
collection.delete_snapshot('snapshot_name')$
data.loc[:, ['TMIN', 'TMED']].head()
delays_geo.to_crs(wards.crs, inplace=True)$ delays_geo.geometry.total_bounds
table = soup.table$ print (table)
tweets_l_scrape = d_scrape['text'].tolist() # create a list from 'text' column in d dataframe$ print(tweets_l_scrape[-1:])
size_pred = rf.predict(climate_vars)
newdf.loc[newdf['score'] == 0.187218571]
from sklearn.externals import joblib$ joblib.dump(pca, '../models/pca_20kinput_6858comp.pkl')
with open('faved_tweets.df', 'wb') as handle:$     pickle.dump(df, handle)
df2['converted'].mean()
application_month_range = ['2017-10','2017-11','2017-12','2018-01','2018-02','2018-03','2018-04']$ man_export_filename = cwd + '\\Manual UW\\Weekly\\Manual UW Tracking.csv'    $
numbers = {'integers': [1,2,3], 'floats': [1.1, 2.1, 3.1]}$ numbers_df = pd.DataFrame(numbers)$ numbers_df
megmfurr_tweets = pandas.read_csv('@megmfurr_tweets.csv')$ megmfurr_tweets
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-12-29&end_date=2017-12-29&api_key=' + API_KEY)$ r.json()
from bokeh.io import output_notebook$ output_notebook()
outdir = './output'$ if not os.path.exists(outdir):$     os.mkdir(outdir)
recommendationTable_df = recommendationTable_df.sort_values(ascending=False)$ recommendationTable_df.head()
most_active_stations = session.query(Measurement.station, func.count(Measurement.station)).\$         group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()0$ print (most_active_stations)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(doc_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours Partial Autocorrelation')
iris_new = pd.DataFrame(iris_mat, columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','Species'])$ iris_new.head(20)
open('test_data//write_test.txt', mode='w')
basic_plot_generator("mention_count", "Saving an Image Graph" ,DummyDataframe.index, DummyDataframe,saveImage=True, fileName="dummyGraph")
factors = web.DataReader("Global_Factors","famafrench")$ factors
ab_df2.converted.mean()
stories.tags.head()
train_topics_df=train_topics_df.fillna(0)$ test_topics_df=test_topics_df.fillna(0)
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
start_df['WorkDay'] = (start_df.index.weekday < 6) * 1$
mapInfo_split = mapInfo_string.split(",") $ print(mapInfo_split)
in_path = os.path.join(os.getcwd(), in_filename)$ df = pd.read_csv(in_path, encoding='utf-8', parse_dates=['created_at'])$ df.drop_duplicates('beer_name', inplace=True)
plt.title("Pyber Rides Sharing (2016)")$ plt.xlabel("Total Number of Rides Per City")$ plt.ylabel("Average Fare ($)")
flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')). \$     agg(F.mean('price_will_drop_num')).orderBy(['trip', 'stay_days', 'lead_time'], ascending=[1, 1, 0]).show(1000)$
time_series.describe()
i = 0$ sample_sent = valid_labels[i]$ print(' '.join(sent2tokens(sample_sent)), end='\n')
pivoted_df = news_sentiment_df.pivot(index='Tweet_Number', columns='News_Source', values='compound')$ pivoted_df.head()
df3 = df2[df2['Current Status']=='complete']$ df3.head()
tl_2040 = pd.read_csv('input/data/trans_2040_ls.csv', encoding='utf8', index_col=0)
df['intercept']=1$ df[['control', 'treatment']] = pd.get_dummies(df['group'])$
idx_trade = r.json()['dataset']['column_names'].index('Traded Volume')$ idx_trade
len(kochdf.loc[kochdf['user'] == "Rezeptsammlerin"]['name'])
a = uc.set_in_units(4.05, 'angstrom')$ box = am.Box(a=a, b=a, c=a)
dr_new_data_plus_forecast.to_csv('./data/dr_new_patients_arimax_forecast.csv')$ dr_existing_data_plus_forecast.to_csv('./data/dr_existing_patients_arimax_forecast.csv')
extract_nondeduped_cmp = extract_all[f_remove_extract_fields(extract_all.sample()).columns.values].copy()
df_variables.loc[df_variables["CustID"].isin([customer])]
print("State space:", env.observation_space)$ print("- low:", env.observation_space.low)$ print("- high:", env.observation_space.high)
speeches_df3['text'] = [text.replace('\n', '').replace('  ', '') for text in speeches_df3['text']]
df.sort_values(by='B', ascending=True)
term_freq_df.sort_values(by='total', ascending=False).iloc[:10]
assembler = VectorAssembler(inputCols = feature_col, outputCol = "features")$ assembled = assembler.transform(ibm_train)
logging.info('Saving')$ df_final.to_csv('~\\df_final.csv')
f = {'total_cost':['sum','count'], 'date_of_birth':['first']}$ total_spending = df.groupby(['uid'])['total_cost','date_of_birth'].agg(f)$
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','Category']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)
mydata = quandl.get("FSE/AFX_X", start_date="2017-01-01", end_date="2017-12-31")
months = months.dropna(subset = ['birth year'])$ print(months.shape)$ print(months.head(5))
BTC = pd.concat([btc_wallet,gdax_trans_btc])
%matplotlib inline$ cat_group_counts = df.groupby("category").size().sort_values(ascending=False)[:10]$ cat_group_counts.plot(kind="bar", title="Top 10 Meetup Group Categories")
shown = pd.DataFrame(data.tasker_id.value_counts())$ shown.loc[shown['tasker_id']==1]
df[df.Likes == 0].tail()
df2.drop('building_use', axis = 1, inplace=True)
master_df['Month of Year']=pd.to_datetime(master_df['startday']).dt.month.astype(str)
sample = pd.read_csv('../assets/sampleSubmission')$ test = pd.read_csv('../assets/test')
rhum_nc = Dataset("../data/nc/rhum.mon.mean.nc")
questions = pd.concat([questions.drop('bands', axis=1), bands], axis=1)
df = pd.merge(applications,questions,on='applicant_id')$ df['response_'] = np.where(df['response'].isnull(),'No Response',df['response'])$ df.head()
labeled_news = pd.read_csv('./labeled_news.csv', encoding='cp1252', header=None, names = ["class", "discription", "source", "title"])$ labeled_news = resample(labeled_news)$ labeled_news.head()
%matplotlib inline$ TpFDroped.plot(kind='bar', legend=True)$
ds_info = ingest.upload_dataset(database=db, dataset=test)$ ds_info
results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'local')$
with pd.option_context('display.max_rows', 150):$     print(news_period_df[news_period_df['news_entities'] == ''].groupby(['news_collected_time']).size())
pulledTweets_df.sentiment_predicted_lr.value_counts().plot(kind='bar', $                                                            title = 'Classification using Logistic Regression model')$ plt.savefig('data/images/Pulled_Tweets/'+'LR_class_hist.png')
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))
applications = sql_query("select * from applications")$ applications.head(3)
tmp = tweets.groupby(['snsuserid','text']).size().reset_index()$ tmp.rename(columns={0:'counts'},inplace=True)$ tmp.sort_values(by=['counts'],ascending=False).query("counts > 2").counts.sum()
crimes.dtypes
stops_heatmap = folium.Map(location=[39.0836, -77.1483], zoom_start=11)$ stops_heatmap.add_child(heatmap_full)$
plt.boxplot(raw_scores, vert=False)$ plt.xticks(xlocs, xlocs)$ plt.xlabel('Beer Ratings');
sites.shape
tree_chunker = ConsecutiveNPChunker(train_trees)$ print(tree_chunker.evaluate(valid_trees))
with open('components/pop_models/excitatory_pop.json') as exc_data:$     exc_prs = json.load(exc_data)$ pprint.pprint(exc_prs)
cityID = '0e2242eb8691df96'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Henderson.append(tweet) 
test_collection.insert_one(temp_dict)
data.plot.scatter(x = 'Unnamed: 0', y = 'Age', figsize = (15, 10))
print(checking['age'].iloc[z])$ print(test_checking['age'].iloc[zt])
measurements_df.count()
html = requests.get(nasa_url)$ soup = bs(html.text, 'html.parser')
%%time$ pq.write_table(crime_geo_table, data_dir + file_name + '.parq')
import requests$ import quandl$ quandl.ApiConfig.api_key = 'AnxQsp4CdfgzKqwfNbg8'
xml_in_merged.tail(2)
user_df['bus_id'].values
price_data = heading.append(price_data)$ price_data.columns = price_data.iloc[0]
cityID = '6a0a3474d8c5113c'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         El_Paso.append(tweet) 
temp_us_full = temp_nc.variables['air'][:, lat_li:lat_ui, lon_li:lon_ui]
sentiments_df.to_csv("recentTweets.csv", encoding="utf-8", index=False)
pd.Timestamp('17:55')
type(df_master.tweet_id[1])
result_df, total_count = search_scopus(key, 'TITLE-ABS-KEY(gender in science)')$ from IPython.display import display, HTML$ display(result_df)$
df_gnis = pd.read_csv(file_name+'_20180601.txt', sep='|', encoding = 'utf-8')
menes_postulados.groupby()
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
yc200902_short['Trip_Pickup_DateTime'] = pd.to_datetime(yc200902_short['Trip_Pickup_DateTime'])
tobs_df = pd.DataFrame(Waihee_in_last_year, columns=['Temperature'])$ tobs_df.head(11)
model.fit(x, ynum, epochs=25, batch_size=32,verbose=2)
trump.dtypes
msft = pd.read_csv("../../data/msft.csv", index_col=0)$ msft.head()
options_frame[abs(options_frame['ModelError']) >= 1.0e-4].plot(kind='scatter', x='BidAskSpread', y='ModelError')
df = $ df.head()
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
from scipy.stats import norm$ print(norm.cdf(z_score)) #Z-score significance$ print(norm.ppf(1-(0.05))) # It tells us what our critical value at 95% confidence is $
joined['dcoilwtico']=joined['dcoilwtico'].astype(np.int8)
df['LinkedAccountName'] = df['LinkedAccountId'].apply(num_to_name)$ df['PayerAccountName'] = df['PayerAccountId'].apply(num_to_name)
t1[t1['retweeted_status_id'].notnull()==True]
for tweet in public_tweets:$     print(json.dumps(tweet, sort_keys=True, indent=4))
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
def calc_tmps(start_date, end_date):$     return session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$     filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all()
df_sb['Sentiment_class'] = df_sb.apply(conditions, axis=1)$ df_sb.to_csv("sobeys_senti_score.csv", encoding='utf-8', index=False)$
data =[]      $ for row in result_proxy:$     data.append({'station':row.stations.name,'RainFall':row.rainfall})
contractor_clean[contractor_clean.city.isnull()] # The result is empty. $ contractor_clean.loc[contractor_clean['contractor_id'] == 139]$
comments = pd.concat([comments_df, comments_df_middle])$ comments = comments.drop_duplicates()$ comments.to_csv('seekingalpha_top_comments.csv')$
ks_particpants=kick_projects.groupby(['category','launched_year','launched_quarter','goal_cat_perc']).count()$ ks_particpants=ks_particpants[['name']]$ ks_particpants.reset_index(inplace=True)
from sklearn.model_selection import train_test_split as tts$ X_train, X_test, Y_train, Y_test = tts(X,Y, test_size=0.2, random_state = 56)
df_valid = pd.read_csv('/home/bmcfee/data/cc_tracks.csv.gz', usecols=[0], nrows=1000000)['track_id']$ df_valid = df_valid.apply(lambda x: '{:06d}'.format(x))
DF1 = pd.DataFrame(datePrecip, columns=["Date","Precipitations"])$ DF1.head()
def clean_tweet(tweet):$     return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())$
$sudo wget http://files.pushshift.io/reddit/comments/RC_2018-02.xz$
stock_data.index=pd.to_datetime(stock_data['latestUpdate'])$ stock_data['latestUpdate'] = pd.to_datetime(stock_data['latestUpdate'])
from utils import write_output$ output_path = os.path.join(output_dir, 'prediction.csv')$ write_output(ids, ids_col, y_pred, label_col, output_path)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.csv"$ df = pd.read_csv(path, sep =',', na_values=['.'])$ df.head(5)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-24&end_date=2018-08-24&api_key=" + API_KEY)
len([premiePair for premiePair in BDAY_PAIR_qthis.pair_age if premiePair < 0])/BDAY_PAIR_qthis.pair_age.count()
crimes.PRIMARY_DESCRIPTION[3:10]
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')$ git_log.describe()
aml_deployment.deploy()
ARStores = $ ARStores.head()
with open('DB_API_key.txt','r') as file:$     API_Key =  file.readline()
BallBerry_resistance_simulation_1 = BallBerry_ET_Combine['BallBerry(Root Exp = 1.0)']$ BallBerry_resistance_simulation_0_5 = BallBerry_ET_Combine['BallBerry(Root Exp = 0.5)']$ BallBerry_resistance_simulation_0_25 = BallBerry_ET_Combine['BallBerry(Root Exp = 0.25)']
prcp_query = session.query(func.strftime(measurements.date), (measurements.prcp)).\$ filter(measurements.date <= last_date, measurements.date>= last_date-relativedelta(months=12)).all()$ prcp_query
len(cvec.get_feature_names())$
logit_pageCountry = sm.Logit(merged['converted'],merged[['ab_page', 'intercept', 'UK', 'US']])$ result_pageCountry = logit_pageCountry.fit()
pattern = re.compile(' +')$ print(pattern.split('AA   bc'))$ print(pattern.split('bcA A'))
db.collection_names(include_system_collections=False)
addOne = cylHPData.mapValues(lambda x: (x, 1))$ print (addOne.collect())$
df.iloc[-1]
reddit.describe()
data.loc[(80, slice(None),'put'),:].iloc[0:5,0:4]
!hdfs dfs -put ProductPurchaseData.txt {HDFS_DIR}/ProductPurchaseData.txt
dict_urls['web']['project'].split('?')[0]
df = pd.read_csv('ab_data.csv')$ df.head()
pernan = 0.80$ nbart_allsensors = nbart_allsensors.dropna('time',  thresh = int(pernan*len(nbart_allsensors.x)*len(nbart_allsensors.y)))
tweetsIn22Mar.head()$ tweetsIn1Apr.head()$ tweetsIn2Apr.head()
df_users_test = df_users.iloc[:2, :]$ df_users_test.iloc[1, -1] = '2017-09-20'$ df_users_test
df_all_payments  = ( df_providers.groupby(['id_num','year'])[['disc_times_pay']].sum())$ df_all_payments.head()$
data.groupby(['Year'])['Salary'].sum()
df["created"] = pd.to_datetime(df["created"])$ df["last_event"] = pd.to_datetime(df["last_event"])$ df.head()
table_rows = driver.find_elements_by_tag_name("tbody")[2].find_elements_by_tag_name("tr")
mask = y_test.index$ t_flag = y_test == 1$ p_flag = pred == 0
station_count = session.query(Stations.station).group_by(Stations.station).count()
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_10min'], axis=1)$ dfY = data['corrCost']
r.json()
tret.plot(figsize=(16,4), color='r');
post_gen_paired_cameras_missing_from_join = np.setdiff1d(BPAIRED_GEN['shopify_order_id'],ORDER_BPAIR_POSTGEN['shopify_order_id'].astype(str))
master_df_rand = master_df.sample(frac = 0.067, random_state = 1)$ master_df_rand = master_df_rand.reset_index(drop=True)$ master_df_rand
credentials = json.load(open('./apikey.json', 'r'), encoding='utf-8')
[t for t in df.in_response_to_tweet_id.tolist() if not isinstance(t, float)]
df = pd.DataFrame([x._json for x in tweets])[['text', 'created_at', 'user']]$ df['label'] = df.user.map(lambda x: x.get('name'))$ df.head()
sns.set_palette("deep",desat=.6)$ sns.set_context(rc={"figure:figsize":(8,4)})
test['visitors'] = 0.2*preds1+0.2*preds2+0.3*preds3+0.1*preds4+0.2*preds5$ test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)$ sub1 = test[['id','visitors']].copy()
t_len.plot(figsize=(16,4), label="Length", color='r', legend=True)$ t_fav.plot(figsize=(16,4), label="Likes", legend=True)$ t_ret.plot(figsize=(16,4), label="Retweets", color="g", legend=True)$
train_data, test_data = train_test_split(status_data, test_size=0.50)$ train = train_data.values$ test = test_data.values
max_IMDB = scores.IMDB.max()
a = 4.5$ b = 2$ print 'a is %s, b is %s' % (type(a), type(b))
station_count.sort_values(['Count'],ascending=False, inplace=False, kind='quicksort', na_position='last')
stories = stories.set_index('short_id')$ stories.head()
dfQ1['flow_MGD'] = dfQ1['meanflow_cfs'] * 0.64631688969744$ dfQ2['flow_MGD'] = dfQ2['meanflow_cfs'] * 0.64631688969744$ dfQ3['flow_MGD'] = dfQ3['meanflow_cfs'] * 0.64631688969744
con,cur = helper.connect_to_db()$
nitrogen = results[mediaMask & hydroMask & charMask & sampFracMask] $ nitrogen.shape
model_att = model_attention_nmt(len(human_vocab), len(machine_vocab))$ model_att.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
le_data_all.index.levels[0]
git_log = git_log.sort_index()$ git_log[git_log['author'] == 'Linus Torvalds'].head(10)
AAPL_array=df["log_AAPL"].dropna().as_matrix() 
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep$ lee_train_file = test_data_dir + 'lee_background.cor'
mars_weather = soup.find_all('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text')[0].text$ print(mars_weather)
df.loc[mask,:]
jsondata = df3.to_json(orient='records')$ with open("_df_subset_v5_wseconds.json", "w") as outfile:$     json.dump(jsondata, outfile)
exiftool -csv -createdate -modifydate cisuabn14/cisuabn14_cycle2.MP4 cisuabn14/cisuabn14_cycle3.MP4 cisuabn14/cisuabn14_cycle4.MP4 cisuabn14/cisuabn14_cycle5.MP4 cisuabn14/cisuabn14_cycle6_part1.MP4 cisuabn14/cisuabn14_cycle6_part2.MP4 > cisuabn14.csv
req = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-7-04&end_date=2017-07-04&api_key=" + API_KEY)$ data = req.json()$ data
from sklearn.model_selection import cross_val_score$ print(cross_val_score(model4, x_val, y_val))$ print(cross_val_score(model5, x_val, y_val))
active_ordered = ordered_df.loc[~churned_ord]$
cityID = 'adc95f2911133646'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Colorado_Springs.append(tweet) 
vhd = pd.read_excel('input/Data.xlsm', sheet_name='52', usecols='A:AO', header=6, skipfooter=16)
m.__repr__()$
sortedprecip_12mo_df=precip_12mo_df.sort_values('date',ascending=True)$ sortedprecip_12mo_df.head()
daily_df.reset_index(inplace=True)$
honeypot_input_data = "2018-01-26-mhn.log"
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
for table in table_list: display(DataFrameSummary(table).summary())
X = pivoted.fillna(0).T.values$ X.shape
pd.set_option('max_colwidth', 100) #Its nice to see all columns
r.json()['dataset_data']
data_rf = pd.DataFrame(data=[tweet.text for tweet in tweets_rf], columns=['Tweets'])$ display(data_rf.head(10))
exiftool -csv -createdate -modifydate cisnwe5/Cisnwe5_cycle4.MP4 > cisnwe5.csv
a['SA'] = np.array([ analyze_sentiment(tweet) for tweet in a['Tweets'] ])$ b['SA'] = np.array([ analyze_sentiment(tweet) for tweet in b['Tweets'] ])
df_merge['budget_binned'] = pd.cut(df_merge['per_student_budget'], bins, labels = bin_names)$ df_merge.head()
wednesdays = pd.date_range('2014-06-01','2014-08-31', freq='W-WED')$ wednesdays.values
(act_diff < p_diffs).mean()
ddf = pd.read_csv('new_ddf.csv')
dfn.to_csv('News.csv')
with open('./data/model/age_prediction_sk.pkl', 'wb') as picklefile:$     pickle.dump(grid, picklefile)
gDateEnergy = itemTable.groupby([pd.Grouper(freq='1W', key='Date'),'Energy'], as_index=True)$ gDateEnergy_content = gDateEnergy['Content']$
pdf = pd.read_csv('training_data/utah_positive_examples.csv')$ ndf = pd.read_csv('training_data/utah_negative_examples.csv')$ wdf = pd.read_csv('utah_weather_2010-2018_grouped.csv')
df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']]$ print(df.head())
print(sp500['Price'].head(3))$ print(sp500[['Price','Sector']].head(3))
tobs_df = pd.DataFrame.from_records(station_tobs)$ tobs_df.head()
store_items.loc[['store 1']]
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
with open(os.path.expanduser('~/.secrets/twitter_thebestcolor.yaml')) as f:$     creds =  yaml.load(f)
cand.CAND_ID.value_counts()$ cand.CAND_NAME.value_counts()
weather_features.iloc[:6]
columns = ['id', 'datetime', 'created_at', 'text', 'file', 'twitter_handle']$ df = pd.DataFrame(columns=columns)
featured_image_url = soup.select_one("figure.lede img").get("src") $ print(featured_image_url)
from fastai.fastai.structured import *$ from fastai.fastai.column_data import *$ from IPython.display import HTML
df_sched = df_sched[~(df_sched.Initiation < 0)].copy()
topTweeters = data.groupby(by=['userScreen', 'userName'])['Tweets'].count()$ topTweeters.sort_values(ascending=[False]).head(10)
np.ones(5)
cityID = '52445186970bafb3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chandler.append(tweet) 
y = x.astype(float)$ y.base is x
arraycontainer = loadarray('test.hdf')$ adjmats = arraycontainer.data
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt"$ df = pd.read_table(path, sep ='\s+', na_values=['.'])$ df.head(5)
sample.head()
Series(np.random.randn(6), index=rng)
html_table_marsfacts = html_table_marsfacts.replace('\n', ' ')$ html_table_marsfacts
prcp['date']=pd.to_datetime(prcp['date'])$ type(prcp.loc[0,'date'])
station_distance['Sex'] = station_distance.Gender.map({0:'unknown', 1:'male', 2:'female'})
assembly = openmc.RectLattice(name='1.6% Fuel Assembly')$ assembly.pitch = (1.26, 1.26)$ assembly.lower_left = [-1.26 * 17. / 2.0] * 2
npath = out_file3$ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
df.info()
def get_duration_career(input_):$     return max(input_) - min(input_)$ grouped_publications_by_author['duration_career'] = grouped_publications_by_author['publicationDates'].apply(get_duration_career)$
ward_df['Delays'].corr(ward_df['Crime Count'])
countries = wb.get_countries()$ countries.iloc[0:10].ix[:,['name','capitalcity','iso2c']]
%matplotlib notebook$ plt.style.use('seaborn-paper')$ mpl.rcParams['figure.facecolor'] = (0.8, 0.8, 0.8, 1)
from scipy.cluster.hierarchy import dendrogram, linkage, set_link_color_palette
all_sets.cards = all_sets.cards.apply(lambda x: pd.read_json(json.dumps(x), orient = "records"))
df[['polarity', 'subjectivity']] = df['text'].apply(lambda text: pd.Series(TextBlob(text).sentiment))$ df['SA'] = np.array([ analize_sentiment(tweet) for tweet in df['text'] ])
client.query("show databases")
with open('../data/rockville_unclean.json') as f:$     data = json.load(f)$     $
coefs.loc['age', :]
df.info()$ df.isnull().sum()$
train_4_reduced.shape, y_train.shape
y_prob = gnb.predict_proba(X_clf)$ print(y_prob.shape)
openmc.run()
print(groceries.loc[['eggs', 'apples']])$ print(groceries.iloc[[2, 3]])
engine=create_engine(seng)$ data['Actor Name'] = pd.read_sql_query('select UPPER(concat(first_name," ", last_name))  from actor', engine)$ data
engine = create_engine("sqlite:///hawaii.sqlite")
analysis_historical.groupby('coin_name').apply(lambda x: x.sort_index(ascending=False, inplace=True))$ analysis_historical['daily_log_return'] = (np.log(analysis_historical['close_price'] /$     analysis_historical['close_price'].shift(-1)))$
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'lxml')
input_folder = '/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/minimum temperature'$ glob_string = '*.nc'
pd.set_option('display.max_colwidth', -1)$ df.loc[df.Sentiment==1, ['description','Sentiment']].head(10)
S_lumpedTopmodel.decision_obj.thCondSoil.options, S_lumpedTopmodel.decision_obj.thCondSoil.value
from bs4 import BeautifulSoup$ import pandas as pd$ from urllib.request import urlopen
containers[0].find("div",{"class":"key"}).a['title'].split()[1].replace(',',"")
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
tobs_values_df=pd.DataFrame([tobs_values]).T$ tobs_values_df.head()
sentiments_pd.to_csv("quickbook_competitors_tweet_data.csv")
tweet_info_pd = pd.DataFrame(tweet_info)$ tweet_info_pd = tweet_info_pd[['Account', 'Text', 'Date', 'Compound Score', 'Positive Score', 'Neutral Score', 'Negative Score','Tweets Ago']]$ tweet_info_pd.head()$
type(df.date[0])
X = [html.unescape(string) for string in X]
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=JyyZ2KQSdpCZHjbk1E9x"$ req  = requests.get(url)$
S_lumpedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_lumpedTopmodel.txt')
cityID = '319ee7b36c9149da'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Arlington.append(tweet) 
tickerdf = tickerdf.dropna(axis=0,how='any')
hourly_df['Price_Change'].value_counts()
directory = 'C://Users//Joan//OneDrive//capstone//'$ dta = pd.read_csv(directory + 't_asv.csv')
questions['VIP_YES'] = questions['vip'].map({'Yes':1, 'No':0})$ questions['VIP_YES'].value_counts()
jscores = jcomplete_profile['scores']$ sdf = pd.DataFrame.from_dict(jscores)$ print(sdf[['score_code','model_scope_forecast_horizon','effective_date', 'score_value']])
resumePath = "resumes/jacky.txt"
df_json = pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')
combined_data = pd.concat([data, expanded_data], axis=1)$ combined_data = combined_data.drop('_source', axis=1)
top100ratings=ratings.groupby('movieId').count().nlargest(100,'rating').reset_index()
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=featured#submit'$ browser.visit(url)$ time.sleep(3)  #allow time for page to load
url =  'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=Tr4sDaRC5eTmZYfipkh3&start_date=2017-01-01&end_date=2017-12-31'$ r = requests.get(url)$ afxdata = r.json()['dataset']
forecast_col = 'Adj. Close'$ df.fillna(value=-99999, inplace=True)$ forecast_out = int(math.ceil(0.01 * len(df))) #round off to the nearest value $
df_group_by.head(20)
itemTable["Time"] = itemTable["Content"].map(time_effort)
vader_df = twitter_df.groupby(["user"]).mean()["compound"]
import numpy as np$ X_nonnum = X.select_dtypes(exclude=np.number)$ X_num = X.select_dtypes(include=np.number)
news_title = soup.title.text
exiftool -csv -createdate -modifydate ciscij10/CISCIJ10_cycle2.mp4 ciscij10/CISCIJ10_cycle3.mp4 ciscij10/CISCIJ10_cycle4.mp4 ciscij10/CISCIJ10_cycle5.mp4 ciscij10/CISCIJ10_cycle6.mp4 > ciscij10.csv
%matplotlib inline$ sns.violinplot(data=october, inner="box", orient = "h", bw=.03)$
top_words = pd.DataFrame(X.toarray()).sum().sort_values(ascending=False).head(10)$ for word in top_words.index:$     print 'Feature: {}, Token: {}'.format(word, tfidf.get_feature_names()[word])
weather_features = pd.DataFrame(index=weather_data.index)
train_df.head()
df.groupby('domain').count().sort_values('id',ascending=False).head(25)
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
import numpy as np$ ok.grade('q02')
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-20&end_date=2018-08-20&api_key=' + API_KEY$ r = requests.get(url)
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
playlist = sp.user_playlist(spotify_url.split('/')[4],spotify_url.split('/')[6])$ pd.io.json.json_normalize(playlist)
containers[0].find("li", {"class":"name"}).a['title']
Quantile_95_disc_times_pay = df.groupby(['drg3','year']).agg([np.sum, np.mean, np.std])$ Quantile_95_disc_times_pay.head(8)$
df_grouped.columns.tolist()
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-06-25&end_date=2018-06-25&api_key={}'.format(API_KEY))
fulldf['sentiment'] = np.array([sentiment(tweet) for tweet in fulldf['tweetText'] ])
df_from_json = pd.read_json("../../data/stocks.json")$ df_from_json.head(5)
results['HydrologicEvent'].value_counts()
df2.drop_duplicates('user_id',keep='first',inplace=True)$ df2[df2.duplicated('user_id',keep=False)]$
subreddit = 'AskReddit'$ sr = reddit.subreddit(display_name = subreddit)
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
foxnews_df = constructDF("@FoxNews")$ display(constructDF("@FoxNews").head())
training, test = train_test_split(review_df, test_size=0.2, random_state=233)$ print(len(training), "train +", len(test), "test")
data_config = dict(path='tests/data/nhtsa_as_xml.zip',$                    databasetype="zipxml", # define that the file is a zip f$                    echo=False)
print(plan.keys())
@app.route("/api/v1.0/stations")$ def stations():$     return jsonify(session.query(hi_stations.STATION.distinct()).all())
y = api.GetUserTimeline(screen_name="berniesanders", count=20, max_id=935706980643147777, include_rts=False)$ y = [_.AsDict() for _ in y]
old_page_converted = np.random.binomial(1, P_old, n_old)$ old_page_converted
df3.groupby('created_at').count()['tweet'].plot()
therm_fiss_rate = sp.get_tally(name='therm. fiss. rate')$ fast_fiss = fiss_rate / therm_fiss_rate$ fast_fiss.get_pandas_dataframe()
cityID = '095534ad3107e0e6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Louisville.append(tweet) 
engine=create_engine("sqlite:///hawaii.sqlite")$ Base = automap_base()$ Base.prepare(engine, reflect=True)
from collections import Counter$ c = Counter([int(stats.coleman_liau(x['cdescr'])) for x in df.to_dict(orient='records')])$
avi_data = pd.read_csv('AviationData.csv', encoding='ISO-8859-1')$
lr.score(preproc_training_test, training_test_labels)
KKK = str(time.strftime("%m-%d-%y")) + "-Output.csv"$ df.to_csv("output/" + KKK, encoding="utf-8")
temp_df['date'] = [dt.datetime.strptime(x, "%Y-%m-%d") for x in temp_df['date']]
news_sentiments = pd.DataFrame.from_dict(sentiments)$ news_sentiments.head()
print(df.tail()) 
file_cap = pyshark.FileCapture('captures/botnet-sample.pcap')
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline',$                         xls_dir='scen2xls',$                         file_name='data.xlsx', verbose=False)
r = requests.get(url)$ json_data = r.json()
len(pd.unique(ratings['movieId'].ravel()))
vars2 = [x for x in dfa.ix[:,6:54]]$ vars2
groupby_w = df['y'].groupby(df['w'])$ round(groupby_w.describe(), 3)
avg_da = pd.DataFrame('AVG_DA':raw_df.groupby('Date')['DA-price'].mean().values)$ avg_da
dailyplay = pd.read_excel('data.xlsx', sheetname = 'dailyplay')$ promotions = pd.read_excel('data.xlsx', sheetname = 'promotions')$
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
scaler=StandardScaler()$ scaler.fit(x_train)
r_json = r.json()$
seq2seq_Model.save('seq2seq_model_tutorial.h5')
df['PCT_change']=(df['Adj. Close']-df['Adj. Open'])/df['Adj. Open']*100.0
req.text
df_merge['per_student_budget'] = df_merge['budget'] / df_merge['size']$ df_merge.head()
df = pd.read_csv('msa.csv')
SCR_PLANS_df = USER_PLANS_df.drop(free_mo_churns)
X_model = df_model.drop('liked',axis=1)$ Y_model = df_model.liked
questions = pd.concat([questions.drop('vip_reason', axis=1), vip_reason], axis=1)
time_chart = vincent.Line(school_per_minute)$ time_chart.axis_titles(x='Time', y='Hashtag frequencies')$ time_chart.display()
df["text"] =  df.text.str.replace('[^\x00-\x7F]','')$ df.text = df.text.apply(removeEmoj)
train.drop('second',1,inplace=True)$ train.drop('minute',1,inplace=True)$ train.drop('action',1,inplace=True)$
log.info("starting job")$ new_predictions_response = client.run_job(body=req_body)$ log.info("done with job")
pred_labels = lr.predict(test_data)$ print("Training set score: {:.2f}".format(lr.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(lr.score(test_data, test_labels)))
with open(output_folder+Company_Name+"_Forecast_"+datetime.datetime.today().strftime("%m_%d_%Y")+".pkl",'rb') as fp:$     dummy_var = pickle.load(fp)
pax_raw.info()
X_top = X[top_features]$ model.fit(X_top, y)
results = soup.find_all('div', class_="top-matter")$ results
df_merge.columns
stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\$ order_by(desc(func.count(Measurement.station))).all()$ stations
to_datetime = lambda x: pd.to_datetime(x)
accumulated_num, frequency = split_by_time(dates, periode= 60 * 60 * 20)$ print 'There are totally '+str(len(frequency)) + ' periods.'$
df.loc[:,'A']
csv2 = pd.read_csv(csv_files[1])$ print(csv2.head())
cur.execute('INSERT INTO experiments (material_id) VALUES ("EVA")')  # use defaults, $ conn.commit()
df.price_doc.hist(bins=100)$
temp_df['date'].describe()
locations = session.query(Measurement).group_by(Measurement.station).count()$ print("There are {} stations.".format(locations))$
validation.analysis(observation_data, BallBerry_resistance_simulation_0_25)
df.loc[df['last_name']=='Copening', 'age'] = df.age.median()$ df.loc[df['last_name'] == 'Copening']
def tokenizer(text):$     return text.split()$ tokenizer('runners like running and thus they run')
X_know.shape
from gensim import models$ models = models.Word2Vec.load_word2vec_format((r'C:\Users\User\Desktop\670\7_Topic_Modeling\data\text8.csv'))
coming_next_reason.columns = ['NEXT_'+str(col) for col in coming_next_reason.columns]
compound_final.set_index(['Date'], inplace=True)$ compound_final.head()
lr2 = LinearRegression()$ lr2.fit(train_data, train_labels)
print(lgb_cv.best_params_)$ print(lgb_cv.best_score_)
import json$ with open('data_comparision/heremap.json', 'w') as outfile:$     json.dump(j_data, outfile)
RNPA_existing_8_to_16wk_arima = RNPA_existing_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ RNPA_existing_8_to_16wk_arima.index = RNPA_existing_8_to_16wk_arima.index.date
tweets_by_user = pd.read_sql_query(query, conn, parse_dates=['created_at'])$ tweets_by_user.head()
!ln -s -f ~/.local/lib/python2.7/site-packages/systemml/systemml-java/systemml-1.0.0-SNAPSHOT-extra.jar ~/data/libs/systemml-1.0.0-SNAPSHOT-extra.jar$ !ln -s -f ~/.local/lib/python2.7/site-packages/systemml/systemml-java/systemml-1.0.0-SNAPSHOT.jar ~/data/libs/systemml-1.0.0-SNAPSHOT.jar
print('AUC using XGBoost = {:.2f}'.format(roc_auc_score(y_test, y_pred)))
df3 = df8.add_suffix(' Created')$ df7 = pd.merge(df,df3,how='left',left_on='Date Created',right_on='MEETING_DATE Created')$
hashtags = df.text.str.extractall(r"#(\w+)")$ mask = hashtags.loc[:, 0].value_counts() > 4$ hashtags.loc[:,0].value_counts()[mask]
(category_count_df2["category"].sum() / category_count_df1["category"].sum()).round(2)$
print(train["comment_text"][0])$ print(example1.get_text())$
logodds.drop_duplicates().sort_values(by=['count']).head(10)$ logodds.drop_duplicates().sort_values(by=['count']).tail(10)$
 print(r.text)
engine.return_as_panda_dataframe = True
cityID = '013379ee5729a5e6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tucson.append(tweet) 
tweet_archive_clean['url'] = tweet_archive_clean.text.str.extract('(?P<url>https://.*/+[a-zA-Z0-9]+)', expand=True)
AFX_X_dict = json.loads(AFX_X_data.text)
dataset = final_member_pivot['Percent Purchase'].values*100$ print dataset
!cat ../../data/msft_with_footer.csv # osx / Linux
X_copy['crfa_c'] = X_copy['crfa_c'].apply(lambda x: ord(x))
git_log.head(10)
print(df.Col_1)
keep_vars = set(no_hyph.value_counts().head(12).index)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])$ print(old_page_converted)
tmp = df.corr(method = 'pearson')[['meantempm']]$
groceries.drop('apples', inplace=True)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=z-6V67L2Lei8_zy742pd&start_date=2017-01-01&end_date=2018-01-01', auth=('user', 'pass'))$
ps.sort_index().tail(600)
attend_with = attend_with.drop(['[', ']'], axis=1)$ attend_with = attend_with.drop(attend_with.columns[0], axis=1)
test_ind["Pred_state_LR"] = best_model_lr.predict(test_ind[features])$ train_ind["Pred_state_LR"] = best_model_lr.predict(train_ind[features])$ kick_projects_ip["Pred_state_LR"] = best_model_lr.predict(kick_projects_ip_scaled_ftrs)
tweets = megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False]$ megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False]['text'].count() # 895
orig_ct = len(dfd)$ dfd = dfd.query('hspf >= 10.0 and hspf <= 18')$ print(len(dfd) - orig_ct, 'eliminated')
analysis_df.head()$
back2sent = word_vecs.apply(lambda x: ' '.join(x))$ transform = TfidfVectorizer(lowercase=False, max_df=.1, max_features=2000, ngram_range=(1,3))$ tf_idf_matrix = transform.fit_transform(back2sent.values)
rain_df.describe()
daily_change={row[0]:row[2]-row[3] for row in price_list if row[1]!=None}$ max_daily_change=max(daily_change.items(), key=lambda k: k[1])[1]$ print("The largest change in any one day is "+str(round(max_daily_change,2)))
grid_pr_size.describe()
Z = np.arange(11)$ Z[(3 < Z) & (Z <= 8)] *= -1$ print(Z)
learn.predict()
from sklearn.mixture import GaussianMixture$ gmm = GaussianMixture(2)
for i in range(3):$     dfs[i]['SA'] = np.array([analyze_sentiment(tweet) for tweet in dfs[i]['Tweets']])
some_df = sqlContext.createDataFrame(some_rdd)$ some_df.printSchema()
from pandas.plotting import scatter_matrix$ axes = scatter_matrix(data.loc[:, "TMAX":"TMIN"])
import matplotlib.pyplot as plt$ %matplotlib inline$ aapl[['Close', 'Adj Close']].plot(figsize=(8,6));
post_gen_paired_cameras_missing_from_shopify_orders = np.setdiff1d(BPAIRED_GEN['shopify_order_id'],ORDERS_GEN['order_number'].astype(str))$ np.array_equal(post_gen_paired_cameras_missing_from_shopify_orders,post_gen_paired_cameras_missing_from_join)
df.info()
tdf['label'] = (tdf['smoker'] == 'Yes').astype(float)$ tdf.sample(5)
df.set_index(['Date', 'Store', 'Category', 'Subcategory', 'Description'], inplace=True)$ df.head(3)
my_stream.filter(track=['data'])
Y = 'label'$ dogscats_h2o[Y] = dogscats_h2o[Y].asfactor()$
df_vow.describe()
aaplA = aapl[['Adj Close']] $ pd.concat([msftAV, aaplA])
wget.download('https://cernbox.cern.ch/index.php/s/ibtnI2ESaFjIgSi/download')
df_reindexed = df.reindex(index=[0,2,5], columns=['A', 'C', 'B'])$ df_reindexed
merged2.dropna(subset=['Specialty'], how='all', inplace=True)
word_vecs.sample(10)
expiry = datetime.date(2015,1,17)$ aapl_calls = aapl.get_call_data(expiry=expiry)$ aapl_calls.iloc[0:5,0:4]
results = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ for row in results:$     print(f"The most recent date recorded is {row}.")
for c in ccc:$     for i in vwg[vwg.columns[vwg.columns.str.contains(c)==True]].columns:$         vwg[i] /= vwg[i].max()
!open resources/html_table_marsfacts.html
set(dat.columns) - set(temp_columns+ incremental_precip_columns+ general_data_columns+ wind_dir_columns)
df.head()
desc_stats = desc_stats.transpose().unstack(level=0).transpose()$ desc_stats
title_sum = preproc_titles.sum(axis=0)$ title_counts_per_word = list(zip(pipe_cv.get_feature_names(), title_sum.A1))$ sorted(title_counts_per_word, key=lambda t: t[1], reverse=True)[:20]$
df_con_treat = df_con1.query('group =="treatment"')$ x_treat = df_con_treat["user_id"].count()$ x_treat$
number_of_commits = git_log['timestamp'].count()$ number_of_authors = len(git_log['author'].dropna().unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
USERNAME = ''$ TOKEN = ''$ API_BASE_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'
tweet_summary = tweet_df.groupby(["Tweet Source"], as_index = False)["Compound Score"].mean().round(3)$ tweet_summary
branch_dist = [analyze_swcfile.get_dist_to_root(a,example_df) for a in branches]
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, ndvi_us)
from h2o.estimators.gbm import H2OGradientBoostingEstimator
pd.Series(feature_names).sample(20)
m = Prophet()$ m.fit(df1);
delays_time =  [one_delays_time, two_delays_time, three_delays_time, four_delays_time, five_delays_time, six_delays_time, seven_delays_time, eight_delays_time, nine_delays_time, ten_delays_time, eleven_delays_time, twelve_delays_time, thirteen_delays_time, fourteen_delays_time, fifteen_delays_time, sixteen_delays_time, seventeen_delays_time, eighteen_delays_time, nineteen_delays_time, twenty_delays_time, twentyone_delays_time, twentytwo_delays_time, twentythree_delays_time, twentyfour_delays_time, twentyfive_delays_time, twentysix_delays_time, twentyseven_delays_time, twentyeight_delays_time, twentynine_delays_time, thirty_delays_time, thirtyone_delays_time, thirtytwo_delays_time, thirtythree_delays_time, thirtyfour_delays_time, thirtyfive_delays_time, thirtysix_delays_time, thirtyseven_delays_time, thirtyeight_delays_time, thirtynine_delays_time, fourty_delays_time, fourtyone_delays_time, fourtytwo_delays_time, fourtythree_delays_time, fourtyfour_delays_time, fourtyfive_delays_time, fourtysix_delays_time, fourtyseven_delays_time, fourtyeight_delays_time, fourtynine_delays_time, fifty_delays_time]$ idx = 0$ ward_df.insert(loc = idx, column = 'Delay Time', value=delays_time)
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
if 'WindSpeed' and 'WindDir' in dat.columns:    $     dat.loc[dat.VecAvgWindDir.isnull(), 'VecAvgWindDir']=LVL1.vector_average_wind_direction_individual_timestep(WS=dat.WindSpeed[dat.VecAvgWindDir.isnull()], WD=dat.WindDir[dat.VecAvgWindDir.isnull()])$ 
df_schools.columns.tolist()
data_df.clean_desc[15]
pvt['customerId'] = pvt['ga:dimension2'].str.rpartition('-')[0].str.strip()$ pvt['customerName'] = pvt['ga:dimension2'].str.rpartition('-')[2].str.strip()$ pvt
clf.predict(users_to_predict)$
precip_data_df.head(3)$
words = [w for w in words if not w in stopwords.words("english")]$ print(words)
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.tail(10)
import pandas as pd$ tweets = pd.DataFrame(tweet_list)$ tweets.info()
temp_freq_df.plot(x="date",y="Precipitation",kind="bar",ax=None,legend=True,$                      title="Hawaii - Temperature  vs Frequency ")$ plt.show()
bands.at['2018-04-29 13:24:49', '68'] = 1$ bands = bands.drop(['drop', 'drop2', '68_2', 'drop3', 'drop4'], axis=1)
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 1), 'type'] = 'defenders'
ny_df=pd.DataFrame.from_dict(foursquare_data_dict)
df[df.client_event_time >= datetime.datetime(2018,4,2,0,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
routes = json.loads(requests.get('{0}index/agencies/{1}/{2}/routes'.format(base_url, '1', '1')).text)$ print(routes)
twitter.name.value_counts()$
Xs = pd.get_dummies(df.subreddit, drop_first = True)
df = pd.DataFrame(data)$ df= df[['station','RainFall']]$ df.style.bar(subset=['RainFall'], align='mid', color=['#5fba7d'])
all_df = test_set.append(properties)$ len(all_df)
S_1dRichards.basin_par.filename
df2 = df[df['Title'].str.contains(blacklist) == False]$ df2 = df2.drop_duplicates(subset = 'Title', keep = False)$ df2 = multi_manufacturer_designer(df2, 'Title')$
y_pred_test = crf.predict(X_test)$ metrics.flat_f1_score(y_test, y_pred_test,$                       average='weighted', labels=labels)
print("Identify Injured by Keyword")$ print(df.cdescr.str.contains('INJUR|HURT').sum())$ print(len(df))
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
merged = merged.set_index('DateTime')
f.loc[:1] # this slice no longer works!
tweets_data_path = '../data/tweets_si.json'$ tweets_file = open(tweets_data_path, "r")$ tweets_data = json.load(tweets_file)
toyotaData=autoData.filter(lambda x: "toyota" in x)$ print (toyotaData.count())$
train_small_data = pd.read_feather("../../../data/talking/train_small_data.feather")$ val_small_data = pd.read_feather("../../../data/talking/val_small_data.feather")$ test = pd.read_feather("../../../data/talking/test_small_data.feather")
dfCountry = pd.read_csv('countries.csv')$ dfCountry.head()
xml_in_sample = xml_in[xml_in['authorName'].isin(random_authors_final)]
created_table=pd.merge(new_table,y_axis_value)$ created_table.head()
result['subjid'] = result['SourceFile'].apply(textsplitter)$ result['SourceFile'] = result['SourceFile'].apply(reversetextsplitter)$ result = result.rename(index=str, columns={'SourceFile':'videoname'})
sp500.Price
Base.classes.keys()
df = DataObserver.build_simply(file_path= '/Users/admin/Documents/Work/AAIHC/AAIHC-Python/Program/DataMine/Reddit/json_data/Processed_DataFrames/r-news/DF-version_2/DF_v2.json')
t3.tweet_id=t3.tweet_id.astype('str')
df5 = df4.set_index(pd.DatetimeIndex(df4['Date']))$ df5 = df5[['BG']].copy()$ df5
X_copy['score_edi_instability_avg'] = X_copy['score_edi_instability_avg'].apply(lambda x: float(x))
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(therapist_duration, params=params, freq=31, title='Therapist Decomposition')
import numpy as np$ import matplotlib.pyplot as plt
iris.loc[:,"Species"] = iris.loc[:,"Species"].astype("category")
from IPython.display import IFrame$ IFrame('data/lda.html', width=1220, height=860)
df1 = df.groupby("Symbol")["Percent_Diff"].sum()$ df1
cityID = '30344aecffe6a491'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fremont.append(tweet) 
df_tte_all[df_tte_all['UsageType'].str.contains('UGW')]['ItemDescription'].unique()
theft = crimes[theft_bool]$ theft.head()
iris.drop(['Id'], inplace=True, axis=1)$ print(iris.shape)
sub_data = data[data["place"].notnull()]$ sub_data.head()
B2.paths['directory_paths']['indicator_settings']$
!head -24 fremont.csv
model.wv.doesnt_match("man woman dog child kitchen".split())$
iris = load_iris()$ X = iris.data$ y = iris.target
containers[0].find("span",{"class":"date"}).contents[0]
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
xml_in_sample1.shape
for item in result_set:$     print(item.index,item.relationship)
URL = "http://www.reddit.com/hot.json"$ res = requests.get(URL, headers = {'User-agent':'Caitlin Bot 0.1'})
X = joined.drop('CHURN', axis = 1)$ y = joined['CHURN']$ X.sample(n=5, random_state=2)
mid = (len(trading_volume) - 1) // 2 + 1$ median_volume = sorted(trading_volume)[mid]$ print('Median Trading Volume: {:.0f}'.format(median_volume))
a_tags = soup.find_all('a')$ for link in a_tags:$     print(link.get('href'))$
print(abs(-2))$ print(round(3.234543,3))$
import pickle$ bild = pickle.load(file=open("facebookposts_bild.pickle", "rb"))$ spon = pickle.load(file=open("facebookposts_spon.pickle", "rb"))
syn = synapseclient.login()$ syn.login()
r_test = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-04-30&end_date=2018-04-30&api_key=YOURAPIKEY')$
df['message_vec'] = vec2.fit_transform(df['message'])$
park = load_data('../../static/parkinson_1960tonow.csv')
df = pd.DataFrame(X)$ df['labels'] = pd.Series(y)$
plot_q_table(q_agent_new.q_table)
user_df.fillna(value='', inplace=True)
ET_Combine = pd.concat([simResis_hour, BallBerry_hour, Jarvis_hour], axis=1)$ ET_Combine.columns = ['Simple resistance', 'Ball-Berry', 'Jarvis']
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
weather_df.to_csv("Weather data.csv", encoding = "utf-8-sig", index = False)
day_order = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']$ days = sns.countplot(x='Day of Week', data=merge_df, palette='viridis',order=day_order)$ days.set_title('Surveys by Day of Week')
df_reader = pd.read_csv(file_wb, chunksize = 10)$ print(next(df_reader))$ print(next(df_reader))$
tf_idf = gensim.models.TfidfModel(corpus)$ print(tf_idf)
conn_b.commit()
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
joined.dtypes.filter(items=['Frequency_score'])
!hdfs dfs -cat {HDFS_DIR}/p32b-output/part-0000* > p32b_results.txt$
df = pd.read_csv("https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv")$ df[:5]
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X'$ r = requests.get(url)$ data = r.json()
logreg = LogisticRegression(penalty='l1', solver='liblinear')$ y_pred = cross_validation.cross_val_predict(logreg, X, y, cv=5)$ print(metrics.accuracy_score(y, y_pred))
import cv2$ cv2.imread("smallgray.png",1)$
mean = np.mean(data['len'])$ print("The average length of all tweets: {}".format(mean))
active_with_return.iloc[:,1] = pd.to_datetime(active_with_return.iloc[:,1])
df.loc[:50,'name'].value_counts()
QLESQ.drop_duplicates(subset=["subjectkey"], keep='first', inplace=True)$ QLESQ.shape$ QLESQ.describe()
who_purchased.columns = ['BOUGHT_'+str(col) for col in who_purchased.columns]
import pickle$ with open('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.pckl', 'rb') as f:$     my_saved_gempro = pickle.load(f)
cityID = 'a3d770a00f15bcb1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Corpus_Christi.append(tweet) 
num_clusters = 3$ km = KMeans(n_clusters=num_clusters, random_state=0).fit(tfidf_matrix)#TODO$ clusters = km.labels_.tolist()
gpByDate = data_air_visit_data.groupby('visit_date').agg(np.sum)$
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')$ logging.debug('Start of program')
table_rows = driver.find_elements_by_tag_name("tbody")[8].find_elements_by_tag_name("tr")$
punct_re = '[^\w\s]'$ trump['no_punc'] = trump['text'].str.replace(punct_re, " ")$
cells_file = 'network/recurrent_network/node_types.csv'$ rates_file = configure['output']['rates_file']$ plot_rates_popnet(cells_file,rates_file,model_keys='pop_name')
month.to_csv('../data/month.csv')
access_logs_parsed = access_logs_raw.map(parse_apache_log_line).filter(lambda x: x is not None)$
ExtractData = twitter_api()$ tweets = ExtractData.user_timeline(screen_name="chintkap", count=200)$
data = session.query(hi_measurement).first()$ data.__dict__
ldamodel_Tesla= models.ldamodel.LdaModel(corpus_Tesla, num_topics=3, id2word = dictionary, passes=20)
inspector = inspect(engine)$ inspector.get_table_names()
from sklearn.ensemble import RandomForestClassifier$ rf = RandomForestClassifier(n_estimators = 10, random_state = 42)$ rf.fit(X_train,y_train)
print(response.text)
print "Logistic : %.2f%%" %(Logistic_scrore*100)$ print "Decision Tree : %.2f%%" %(DT_Score*100)$ print "Randomforest : %.2f%%" %(RF_Score*100)
mhemi_image = soup.find_all('div',class_="item")
df1['K_lbs-Site1'] = df1["AnnualFlow_MGD"] * df1['TotalN'] * 8.34 / 100000$ df2['K_lbs-Site2'] = df2["AnnualFlow_MGD"] * df2['TotalN'] * 8.34 / 100000$ df3['K_lbs-Site3'] = df3["AnnualFlow_MGD"] * df3['TotalN'] * 8.34 / 100000
columns = inspector.get_columns('clean_hawaii_stations.csv')$ for c in columns:$     print(c['name'], c['type'])$
data_l2_begin = tmpdf.index[tmpdf[tmpdf.isin(DATA_L2_HDR_KEYS)].notnull().any(axis=1)].tolist()$ data_l2_begin
really_large_dataset = sc.broadcast(100)
non_na_df = df.dropna()
meets_credit_policy = doesnt_meet_credit_policy.logical_negation()$ meets_credit_policy.head(rows=2)
from gensim.corpora import Dictionary, MmCorpus$ from gensim.models.ldamulticore import LdaMulticore$ import cPickle as pickle
import plotly.plotly as py$ import plotly.graph_objs as go$
url = "https://mars.nasa.gov/news/"$ response = requests.get(url)
bigquery_dataset = "{}:{}.hygiene".format(PROJECT,PROJECT)$ job = preprocess('Dataflow', BUCKET, bigquery_dataset)
SGDC = SGDClassifier()$ model2 = SGDC.fit(x_train, y_train)
response = requests.get('https://www.purdueexponent.org/search/?f=html&q=gun+control&d1=2018-02-14&d2=2018-06-25&sd=desc&l=100&t=article%2Ccollection&nsa=eedition')$ soupresults1 = BeautifulSoup(response.text,'lxml')$
df = pd.read_csv('weather_data_austin_2010 (1).csv')$ cols = ['Temperature','DewPoint','Pressure']$ df = df[cols]
combined_df4['split_llpg1']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(x.split(',')[1:2]))$ combined_df4['split_llpg2']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(x.split(',')[1:3]))$ combined_df4.head()
dfMeanFlow.head()
df_tobs = pd.DataFrame(data)$ display(df_tobs.head())$ display(df_tobs.info())
re.match('^\w+@[a-zA-Z_]+?\.[a-zA-Z]{2,3}$', 'abc@gmail.com').group()
news_sentiment_df.to_csv('news_sentiments.csv')
display(data.head(10))
print data_df.clean_desc[15]
df.head(3) # only 3 data rows
cm = our_nb_classifier.conf_matrix(our_nb_classifier.data_results['out_y'],$                             our_nb_classifier.data_results['out_n'])$ print(ascii_confusion_matrix(cm))
tweets_raw = pd.read_csv(delimiter="\t",filepath_or_buffer='tweets_terr.txt', names=["lan","id","date", "user_name", "content"],encoding='utf-8',quoting=csv.QUOTE_NONE)
s1.values
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
df_members = df_members[df_members.registered_via != -1]
test[['clean_text','user_id','predict']][test['user_id']==5563089830][test['predict']==11].shape[0]
! ./data_tools/download.sh$ ! ./data_tools/split.sh
EXIFtool_command = 'exiftool'+' -csv="'+basedirectory+projectname+'_exiftool.csv" '+imagepath$ EXIFtool_command$
df3 = df3.dropna(how='all')$ print(df3.sample(5))
df.groupby(['line']).agg([sum])$
data['sepal_area'] = data.sepal_length * data.sepal_width$ print(data.iloc[:5, -3:])
vol = vol.replace(0, 1)$ vol.describe()
dat['orig_T1']=dat[primary_temp_column].copy()$ primary_temp_aspirated='aspirated' in primary_temp_column.lower()
crime_geo_df = crime_geo[geo_data_columns].compute()$ crime_geo_df.info()
speeches_metadata = speeches_cleaned.merge(metadata_df_all, left_on = 'id', right_on = 'accessids', how = 'inner')
tweets = pd.read_csv('df_tweets.csv', parse_dates=['created_at'], infer_datetime_format= True, low_memory=False,\$                     usecols=['id', 'created_at', 'hashtags'])
import requests, json$ data = requests.get(url).json()
sp500 = pd.read_csv('sp500.csv', index_col='Symbol', usecols=[0,2,3,7])$ sp500.head()
print(r.text)
! find /home/ubuntu/s3/flight_1_5/extracted/final_results -name '*.txt' -exec mv {} /home/ubuntu/s3/flight_1_5/extracted \;$ ! rmdir /home/ubuntu/s3/flight_1_5/extracted/final_results
inoroffseason = ALLbyseasons.groupby("InorOff") # This groups our sample by whether transactions took place in-season or during$
data.index = pd.to_datetime(data.index)$ data.interpolate(method='time',inplace=True)$
outliersDict = {key: df[abs(df['value'] - np.mean(df['value'])) > 3 * np.std(df['value'])] for key, df in typesDict.items()}$ outliersDict.keys()
df.head(20)
qends = pd.date_range('2014-01-01','2014-12-31',freq='BQS-JUN')$ qends.values
def get_client():$     client = soundcloud.Client(client_id=CLIENT_ID)$     return client
sp = openmc.StatePoint('statepoint.50.h5')
if not database_exists(engine.url):$     create_database(engine.url)$ print(database_exists(engine.url))
df['SA'] = np.array([ analize_sentiment(tweet) for tweet in df['Tweets'] ])$ display(df.head(10))
outfile = os.path.join("Resource_CSVs","Main_data_negative.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
print('Average daily volume of 2017: ' + str(np.mean(vol_vec).round(1)))
%cd "C:\Users\Safaa\Desktop\PythonStuff\10-16-2017-Gw-Arlington-Class-Repository-DATA\Homework\11-Adv-Data-Storage-Retrieval\Instructions\Resources"$ measure = "hawaii_measurements.csv"$ station = "hawaii_stations.csv"
result = api.search(q='%23Australia')  # "%23" == "#"$ len(result)
df = DF.copy()$ df = df[24:]$ print("... created a copy and cropped odd convos from start")
for stat in session.query(Measurement.station).distinct():$      print(stat)
brandValues.mapValues(lambda x: int(x[0])/int(x[1])). \$     collect()$
df_vets = df_vets.loc[df_vets['d_birth_date'] > '1980-01-01']$ drop_cols = ['d_suffix', 'section_id', 'row_num', 'site_num', 'cem_url', 'cem_phone', 'cem_addr_two', 'cem_addr_one', 'city', 'state', 'zip', 'v_first_name', 'v_mid_name', 'v_last_name', 'v_suffix']$ df_vets = df_vets.drop(drop_cols, axis=1)
closing_prices = (x[4] for x in data_table if x[4] is not None)$ inter_day_changes = (abs(i - j) for i, j in itertools.combinations(closing_prices, 2))$ print('Largest Change between days: {:.2f}'.format(max(inter_day_changes)))
games_df = pd.DataFrame(game_data_all).copy()
aggregate_by_name = pd.concat(g for _, g in df_Providers.groupby("name") if len(g) > 1)$ aggregate_by_name.head()
from sqlalchemy.orm import Session$ session = Session(bind=engine)
%%bash$ sed 's/^chr//g' post/allV.tab | (head -n 1 - && tail -n +2 - | LANG=C sort) |\$     LANG=C join -t$'\t' --header - ../tools/rs_37_sort.txt > post/allV_RS.tab
year_2017 = [info for info in r.json()['dataset']['data'] if info[0][:4]=='2017']$ print(year_2017)$
df3[pd.isnull(df3).any(axis=1)]
df = df[df.tax_class2 == '1']$
print('number of deaths in 2014:',df['2014']['battle_deaths'].sum())
itemTable["Energy"] = itemTable["Content"].map(energy_level)
gDate_vProject = gDateProject_content.unstack(level=1, fill_value=0)$
print activity_df.Walking
pivoted.plot(legend=False, alpha=0.1)
!wget https://storage.googleapis.com/aibootcamp/data/ortec_templates.zip
pipe_dat = df_pipe.copy()
Twitter_map = folium.Map([45.955263, 8.935129], tiles='cartodbdark_matter', zoom_start = 5)$ Twitter_map
mfp_boss.build_prior()
os.chdir(root_dir + "data/")$ df_fda_drugs_reported = pd.read_csv("filtered_fda_drug_reports.csv", header=0)
city_avg_fare_renamed = city_avg_fare.rename(columns={"fare": "average_fare"})$ citydata_avg_fare_work = city_avg_fare_renamed[['city', 'driver_count', 'type', 'average_fare']]$
model = word2vec.Word2Vec.load("200features_30minwords_10context")
df.hist(bins=50, figsize=(15,15));$
sentimentDf["date"] = pd.to_datetime(sentimentDf["date"])$
m = Prophet(interval_width=0.95)$ m.fit(df);
label_and_pred = rfModel.transform(testData).select('label', 'prediction')$ label_and_pred.rdd.zipWithIndex().countByKey()$
x=[i for i in range(11)]$ x$ network_simulation[network_simulation.generations.isin(x)]$
logmod = sm.Logit(ab_df2['converted'], ab_df2[['intercept','ab_page']])$ results = logmod.fit()
pd.date_range('8/14/2017 14:41:31', periods=5)
sorted_results.describe()
sns.regplot(x=df['score'], y=df["comms_num"], $             line_kws={"color":"r","alpha":0.7,"lw":5});
date_df.plot.area(stacked=False)
Base.prepare(engine, reflect=True)$
y_cat.value_counts()$
twitter_df = pd.DataFrame(results_dict)$ twitter_df.head()
conn.execute(sql)
df_country=pd.get_dummies(data=countries_df, columns=['country'])$ df_country.head()
from pysumma.utils import utils
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date'], axis=1)$ dfY = data['corrCost']
json_data2017=response.json()['dataset_data']$
df_raw_tweet = pd.read_csv('./Datasets/Twitter_Training_Data2.csv', encoding='latin1')$ print (df_raw_tweet.head())
new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs$ new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))$ new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)$
urban_driver_total = urban_type_df.groupby(["city"]).mean()["driver_count"]$ urban_driver_total.head()$
len(email_age_unique[pd.isnull(email_age_unique['request.ageRange']) == True])
img = mpimg.imread('input/c(DE-DK-NO-SE).png')
df = pd.read_csv('GageData.csv', dtype={'site_no':'str'}) 
test_id = pax_raw.seqn.values[1]$ pax_sample = pax_raw[pax_raw.seqn==test_id].copy()
df_station = pd.DataFrame(list(station_zipcode.items()), columns = ['station', 'zipcode'])$ df_station['zipcheck']=df_station.zipcode.apply(lambda x:len(x))$ df_station[df_station['zipcheck']!=5]$
rhum_long_df = pd.melt(rhum_wide_df, id_vars = ['grid_id', 'glon', 'glat'],$                       var_name = "date", value_name = "rhum_perc")$ rhum_long_df.head()
df_concat_2["message_likes_dummy"] = np.where(df_concat_2.message_likes_rel > 500, 1, 0)
aapl = getStockPrice("AAPL",1982, 1, 1, 2018, 1, 23)$ aapl.head()$
for row_index, row in df.iterrows():$     print(row_index, row)
spp['season'] = spp.index.str.split('.').str[0]$ spp['term'] = spp.index.str.split('.').str[1]
station_distance.loc[:, ['Start Station Name', 'End Station Name',$                          'Start Coordinates', 'End Coordinates']].head(2)
iso_join.head()
grouped=dup_ts.groupby(level=0)$ grouped.mean()
df = pd.read_csv('df.csv',index_col=0,low_memory=False)$ df.shape
tweetsIn22Mar.index = pd.to_datetime(tweetsIn22Mar['created_at'], utc=True)$ tweetsIn1Apr.index = pd.to_datetime(tweetsIn1Apr['created_at'], utc=True)$ tweetsIn2Apr.index = pd.to_datetime(tweetsIn2Apr['created_at'], utc=True)
json_data = r.json()$ print(type(json_data))
hours = df4['Date'].dt.hour$ hours$
soup.find_all('div', class_='schedule-container')[0].select('.active')
df_full['school_type'] = df_full['school_type'].map(DATA_L1_HDR_DICT)
for col in data_df.columns:$     if np.unique(data_df[col].dropna().astype(str)).shape[0] <= 1:$         print(col)
tfidf = models.TfidfModel(corpus)
df.head(10)
print("{} is unique user_id are in dataset df2.".format(df2['user_id'].nunique()))
my_gempro.get_scratch_predictions(path_to_scratch='scratch', $                                   results_dir=my_gempro.data_dir,$                                   num_cores=4)
validation.analysis(observation_data, Simple_resistance_simulation)
df.dropna(axis = 0, inplace = True)$ df.reset_index(inplace=True, drop=True)$ df.shape
%matplotlib inline$ %pylab inline$ pylab.rcParams['figure.figsize'] = (20, 9)   # Change the size of plots
values = [4, 56, 2, 45.6, np.nan, 23] # np.nan returns a null object (Not a Number)$ s = pd.Series(values)$ s
json.dumps(letters)[:1000]
pd.read_sql('SELECT * FROM experiments WHERE irradiance = 700 ORDER BY temperature', conn, index_col='experiment_id')
pd.options.display.max_rows$ pd.set_option('display.max_colwidth', -1)$ type(df.iloc[15]['in_reply_to_user_id_str'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)
print (test.shape)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-05-08&end_date=2018-05-08&api_key=' + API_KEY)$
grouped.get_group('MSFT')
%matplotlib inline$ word_freqs.plot(30)
Google_stock.head()
for cell in openmc_cells:$     for rxn_type in xs_library[cell.id]:$         xs_library[cell.id][rxn_type].load_from_statepoint(sp)
import builtins$ builtins.uclresearch_topic = 'HAWKING'$ from configuration import config
tdf = pd.read_csv('ksdata.csv', index_col=0)
RDDTestScorees.groupByKey().collect()
with open('total_review_apps_eng_lower.pickle', 'rb') as d:$     total_review_apps_eng_lower = pickle.load(d)
sp500.iloc[[0, 2]]
df = df.loc[df['game_type'] == 'R',]
df.describe()
result = modelk.predict(X_test )$
cnn = news_sentiment('@CNN')$ cnn['Date'] = pd.to_datetime(cnn['Date'])$ cnn.head()
measure_nan = measure[measure.isnull().any(axis=1)]
airports_df[airports_df['city'].str.lower() =='new york']
y = df['hospitalmortality'].values$ X = df.drop(['hospitalmortality'], axis=1).values$ X_header = df.drop(['hospitalmortality'],axis=1).columns
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df=df.set_index('Date')$ bwd = df[['Store']+columns].sort_index().groupby("Store")[columns].rolling(7, min_periods=1).sum()$ fwd = df[['Store']+columns].sort_index(ascending=False).groupby('Store')[columns].rolling(7,min_periods=1).sum()
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", start='1980',end='2014')$ le_data_all
metadata_documents = [x.name() for x in query.element("asset/meta").elements() if x.name() != "mf-revision-history"]$ metadata_documents
for a in cells: $     ct.save_reconstruction(a['id'], file_name='allen_morpho/ai'+str(a['id'])+'.swc')$     ct.save_reconstruction_markers(a['id'],file_name='allen_morpho/ai'+str(a['id'])+'_marker.swc')
station_df = pd.read_csv(station, encoding="iso-8859-1", low_memory=False)$ station_df.head()
df.index.tz_localize('GMT').tz_convert('US/Eastern')
TEST_DATA_PATH = SHARE_ROOT + 'test_dataframe.pkl'$ test_df.to_pickle(TEST_DATA_PATH)
display(data.head(20))
svg2 = displacy.render(parsed4, style='ent', jupyter=True)
HYB, STD = set(HYB_customer_order_intervals), set(STD_customer_order_intervals)$ for customerID in HYB.intersection(STD):$     print(customerID)
list(pd.read_csv(projFile, nrows=1).columns), list(pd.read_csv(schedFile, nrows=1).columns), list(pd.read_csv(budFile, nrows=1).columns)
np.any(x < 0)
print ("Data Frame with Forward Fill:")$ df2.reindex_like(df1,method='ffill')
days_alive / pd.Timedelta(nanoseconds=1)
iris.iloc[(iris.iloc[:,0]>7.5).values,4]$
df['screen_name'].value_counts()
plt = r6s.score.hist(bins = 10000)$ plt.set_xlim(0,50)
model.doesnt_match("man tiger woman child ".split())$
cats_in = intake.loc[intake['Animal Type']=='Cat']$ cats_in.shape
doc_df = pd.read_sql("SELECT * FROM Documents", con=engine)$ doc_df.head(3)
ps = pst.to_period()$ ps
data['SA'] = np.array([analize_sentiment(tweet) for tweet in data['Tweets']])
html=HTML(html=doc)$ html.links
grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)$ grid.map(plt.hist, 'Age', alpha=.5, bins=20)$ grid.add_legend()
conn = pymysql.connect(host='localhost', port=3306, user='root', password='pythonetl', db='pythonetl')
df.loc['1930-01-01':'1979-12-31','status'] = "Before FL"$ df.loc['1984-01-01':'2017-12-31','status'] = "After FL"$ df.sample(10)
df2 = df[['MeanFlow_cfs','Confidence']]$ df2.head()
store_items.fillna(method = 'ffill', axis = 1)
all_sets.shape
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'pure_scorers'
fed_reg_dataframe[fed_reg_dataframe.index > '2017-01-20']
theft.iloc[4]
store_items.dropna(axis = 0)
cnn_compound=df.loc[df.company=='CNN']['compound']$ cnn_when=df.loc[df.company=='CNN']['tweets_ago']$ print(cnn_when.values)
avgComp = groupedNews["Compound"].mean()$ avgComp.head()
masked['user_age_days'] = [ele.days for ele in masked['user_age']]$ masked.head()
df_train = pd.concat((df_train, pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/train_v2.csv',dtype={'is_churn' : np.int8} )), axis=0, ignore_index=True).reset_index(drop=True)
sc.getConf().get("spark.yarn.historyServer.address")
df_clean['body_length'].hist(range = (0,100))
teama_merge = my_elo_df.merge(final_elo, how='left', left_on=['Date', 'Team A'], right_on=['Date', 'Team'])$ teama_merge[teama_merge['Team A'] == 'Cloud9'].tail(7)
Base.classes.keys()
driver = webdriver.Chrome(executable_path="./chromedriver")
df['age'].fillna(df.groupby(['gender'])['age'].transform(mean))
!hdfs dfs -cat /user/koza/hw3/3.2.1/productWordCount/* | tail -n 1
df = df[(df['state'] == 'successful') | (df['state'] == 'failed')]$ df['state'] = df['state'].replace({"failed":0,'successful':1})$ df['spotlight'] = df['spotlight'].replace({False:0,True:1})
free_data.groupby('age_cat').mean()
from pyspark.sql.functions import col$
from pyspark.ml.feature import StringIndexer, VectorIndexer
latest_news_para = soup.find('div', class_ = 'article_teaser_body').get_text()$ print(latest_news_para)$
directory_name = os.path.join('network', 'recurrent_network')
index = similarities.MatrixSimilarity(lsi[corpus])
pokemon_train = pokemon[~pokemon['Name'].isin(pokemon_test['Name'])]
deltadf.to_csv('exports/trend_deltas.csv')
INT = INT.sort_values(by = ['Contact_ID','Int_Name'])
output = model.predict(test[:, 1:5])$ rowID = [TEST.rowID for TEST in test_data.itertuples()]$ result_df = pandas.DataFrame({"rowID": rowID,"cOPN": list(output)})
my_gempro.prep_itasser_modeling('~/software/I-TASSER4.4', '~/software/ITLIB/', runtype='local', all_genes=False)
url  = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv"$ iris = h2o.import_file(url)
lm.rsquared
fire_size_file = '../data/model_data/size_mod.sav'$ pickle.dump(rf, open(fire_size_file, 'wb'))
data.TMED.head()
marsfacts_url = 'https://space-facts.com/mars/'$
    return "https://github.com/{0}/{1}.git".format(org, project)$ 
ab_df2[((ab_df2['group'] == 'treatment') == (ab_df2['landing_page'] == 'new_page')) == False].shape[0]
PRE_PATH = PATH/'models'/'wt103'$ PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'
tweet_df = tweet_df[['tweet', 'source', 'created_at', 'compound', 'positive', 'neutral', 'negative']]$ tweet_df.head()
len([premieSn for premieSn in SCN_BDAY.scn_age if premieSn < 0])/SCN_BDAY.scn_age.count()
print("Mean squared error: %.2f"% mean_squared_error(y_test, y_pred))
contractor_merge.rename(index=str, columns={"state_abbrev" :"state_code"}, inplace =True)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep ='\s+', na_values=['.'])$ df.head(5)
x = datetime.strptime(inner_list[0][0], '%Y-%m-%d')$ type(x.year)
data_year_df['Date'] = pd.to_datetime(data_year_df['Date'])$ data_year_df.head()$
pd.melt(df, id_vars=['A'], value_vars=['B'])
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])
coinbase_btc_eur['Timestamp'] = coinbase_btc_eur["Time"].apply(lambda row: unix_to_datetime(row))
AFX_X_2017_dict = AFX_X_2017_r.json()
print (collData.getNumPartitions()) # this is the number of CPU cores$
from jira import JIRA$ jira = JIRA(cred['host_jira'], basic_auth=(cred['email'], cred['password_jira']))$ issues = jira.search_issues('issuetype = "Bug de Tracking"')
btc.describe()
model.most_similar("man")$
number_of_commits = git_log['timestamp'].count()$ number_of_authors = git_log['author'].nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
prec_df = pd.DataFrame(data = us_prec)$ prec_df.columns = ts.dt.date
options_frame.info()
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf]).sample(10)
plt.hist(review_df.fake_review)$ plt.show()
df_survival_by_donor = df_survival[['Donor ID', 'Donation Received Date']].groupby('Donor ID')$ mean_time_diff = df_survival_by_donor.apply(lambda df: df['Donation Received Date'].diff().mean())$
punctuation = list(string.punctuation)$ stop = stopwords.words('english')+stopwords.words('spanish') + stopwords.words('french') + punctuation + other 
lons, lats = np.meshgrid(lon_us, lat_us)$ plt.plot(lons, lats, marker='.', color='k', linestyle='none')$ plt.show()
doc_id_list = np.array(reuters.fileids(category_filter))$ doc_id_list = doc_id_list[doc_id_list != 'training/3267']
url = 'https://mars.nasa.gov/news/'
dfHaw_Discharge['flow_MGD'] = dfHaw_Discharge['meanflow_cfs'] * 0.64631688969744
import warnings$ warnings.simplefilter("ignore", UserWarning)$ warnings.simplefilter("ignore", FutureWarning)
today_ = datetime.date.today().strftime("%Y_%m_%d")$ today_ = '2018_06_07'$ print("today is " + today_)
female = crime.loc[crime['Sex']=='F']$ female.head(3)
to_plot.plot.hist(by='Days Between Int', bins=100);$ plt.title("Actual number of days between purchases")$
ldf['Traded Volume'].mean()
y_train = train.click$ train = train.drop(columns=['click', 'id'])$
confidence  = clf.score(X_test, y_test)$ print("Confidence our SVR classifier is: ", confidence)
payments_total_yrs.tail()$ payments_total_yrs.to_csv('Top Total Payees More than 1 Million Total.csv')$
r_lol = r.json()['dataset']['data']
walk.resample("1Min").first()
cityID = 'fa3435044b52ecc7'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Newark.append(tweet) 
new.describe()
from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report$ from sklearn.model_selection import GridSearchCV, StratifiedKFold$ from sklearn import preprocessing
mlab_uri = os.getenv('MLAB_URI')$ mlab_collection = os.getenv('MLAB_COLLECTION')
my_gempro.uniprot_mapping_and_metadata(model_gene_source='TUBERCULIST_ID')$ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping)$ my_gempro.df_uniprot_metadata.head()
write_to_pickle(path+'/features.pkl', users)$ users=load_pickle('features.pkl')$
pd.set_option('display.max_columns', 100)$ tmpdf
style_bw.head(5)
print('Highest opening price - {} \nLowest opening price - {}'.format(max(opening_price), min(opening_price)))
print(raw_data.head())$ print(raw_data.shape)
test = pd.read_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep = ',', $                    encoding='utf-8', low_memory=False)
stations_df=pd.read_csv(stations, dtype=object)
merged_data['payment_day'] = merged_data['last_payment_date'].dt.day$ merged_data['payment_month'] = merged_data['last_payment_date'].dt.month$ merged_data['payment_year'] = merged_data['last_payment_date'].dt.year
url_domains = grouped['domain'].agg({'domain': lambda x: np.unique(x)})$ unique_urls = pd.merge(unique_urls, url_domains)$ unique_urls.head()
LSST_sample_filename = 'LSST_ra_250_283_dec_-40_-15.dat'$ LSST_data = np.genfromtxt(DirSaveOutput+LSST_sample_filename, usecols=[5])
df_concat_2.message_likes_rel = np.where(df_concat_2.message_likes_rel > 10000, 10000, df_concat_2.message_likes_rel)
rf = RandomForestClassifier(n_estimators = 30)$ rf.fit(X_train, y_train)$ rf.score(X_test, y_test)
dci = indices(dcaggr, 'text', 'YearWeek')$ dci.head()
latest_version = str(d.id[0])$ print latest_version
ldamodel.print_topics()
cityID = '249bc600a1b6bb6a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Aurora.append(tweet) 
sql_query = "SELECT * FROM events WHERE actor_id= %s"$ df = pd.read_sql_query(sql_query, session.bind, params=[userid])$ df
df3=df3.sort_values(by=["Net_Value_item_level"],ascending=[False])$ len(df3[df3.Net_Value_item_level==0])
features = weather_features.merge(DC_features,left_index=True,right_index=True)
df_pol_t = pd.concat([df_pol_matrix_df, df_pol_t.drop('title', axis=1)], axis=1)$
df_hubs = df_avg_use.query('city != "non hub"').copy()$ df_lg_hubs = df_hubs.query('annual_avg > 500')$ df_lg_hubs
cityID = '2409d5aabed47f79'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Rochester.append(tweet) 
appleNegs.groupby('group_id').tweet_id.count().reset_index()$
df_users = pd.read_csv('../../data/march/users.csv')$ df_levels = pd.read_csv('../../data/march/levels.csv')$ df_events = pd.read_csv('../../data/march/events.csv', skiprows=1, names=event_header, error_bad_lines=False, warn_bad_lines=True)     
fruits= pd.Series(data = [10, 6, 3,], index = ['apples', 'oranges', 'bananas'])$ fruits
!hdfs dfs -cat {HDFS_DIR}/p32cf-output/part-0000* > p32cf_results.txt
mydata.to_csv("Data-5year-2012-20180617.csv")
rf = RandomForestClassifier()$ rf.fit(X_train, y_train)$ rf.score(X_test, y_test)
!wget https://download.pytorch.org/tutorial/faces.zip$ !unzip faces.zip
tfav.plot(figsize = (16, 4), color ='b')$
X = reddit_master['title']$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
data_file='data/airports.csv'
set_option("display.max_colwidth",280)$
t1.info()
train_body_raw = traindf.body.tolist()$ train_title_raw = traindf.issue_title.tolist()$ train_body_raw[0]
cityID = '5d231ed8656fcf5a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Petersburg.append(tweet) 
newdf['score'].fillna(0.187218571, inplace=True)
building_pa_prc_zip_loc.to_csv("buildding_03.csv",index=False)$ building_pa_prc_zip_loc=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date'])
precip_data_df1=precip_data_df.copy()$ precip_data_df1.reset_index(inplace=True,drop=False)$ precip_data_df1.head(5)
movies_df['genres'] = movies_df.genres.str.split('|')$ movies_df.head()
t1.head(5)$
conn_a.commit()
from Lightcurve import plot_LC_solar_Flare$ % matplotlib inline $ plot_LC_solar_Flare('FERMI/SolarFlares/LAT_Flares/lat_LC_20120307.fits', 'Flare20120307')
jeff = Customer('Jeff Knupp', 1000.0)    #jeff is the object, which is an iinstance of the *Customer* class
candidate_data['messages'] = data.groupby('from_name')['message'].apply(' '.join)
station_analysis_df = tobs_df.rename(columns={0: "station", 1: "name", 2: "date", 3: "tobs"})$ station_analysis_df.head()
searched_tweets = [status for status in tweepy.Cursor(api.search, wait_on_rate_limit=True,                                        wait_on_rate_limit_notify=True, $ since='2018-08-05', until='2018-08-06', q=query).items(max_tweets)]$
FIGURE_PREFIX = '../figures/'
alice_sel_shopping_cart = pd.DataFrame(items, index = ['glasses', 'bike'], columns = ['Alice'])$ alice_sel_shopping_cart
len(df[~(df.event_properties == {})])
y_pred_mdl = mdl.predict(X_test)$ y_train_pred_mdl=mdl.predict(X_train)$ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(mdl.score(X_test, y_test)))
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="ivanka").items(max_tweets):$     print(tweet)
print(pd.isnull(dfx))
pattern = re.compile('AA')$ print(pattern.sub('BB', 'AAbcAA'))$ print(pattern.sub('BB', 'bcAA'))
stations = session.query(Measurement).group_by(Measurement.station).count()$ print(stations)
data.sort_values('TMED', inplace=True, ascending=False)$ data.head()
Base = automap_base()$ Base.prepare(engine, reflect=True)$
plot_price(f,'Close',start='Jan 01, 2017',end='Dec 31, 2017')$ plt.legend("last year")
search['one_way'] = search.apply(lambda x: 0 if x['trip_end_loc'] == x['trip_start_loc'] else 1, axis=1)
df_filtered_by_RT = df[~df.raw_text.str.startswith('RT')]$ df_filtered_by_RT.head(1)
df1 = pd.read_feather('new_sensor_data2') $ df1.head(10)
last_tobs = session.query(Measurement.tobs, Measurement.station).order_by(Measurement.station.desc()).limit(365).all()$ last_tobs = pd.DataFrame(last_tobs)$ last_tobs.head()
google_stock['Adj Close'].describe()
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
df = pd.read_csv('artist-pitch.csv')$ df.head()
df1.shape
print(dfd.capacity_5F_max.describe())$ dfd.capacity_5F_max.hist()
prophet_model = Prophet(interval_width=0.95)  #default==0.8
df['Shipping Method name'] = df['Shipping Method name'].fillna(df['Shipping Method Id'])
df_teacher_behavior = df_teacher_behavior[df_teacher_behavior.awj_teacher_id.isin(pick_list)]$
print('{0:.2f}%'.format((scores[4.0:5.0].sum()/total) * 100))
res.summary2()
Base.classes.keys()
results = session.query(Station.station, Station.name).count()$ print(f"There are {results} stations.")
my_tweet_df["tweet_source"].unique()
contractor_clean.loc[contractor_clean['contractor_id'].isin([382,383,384,385,386,387]),$                      'contractor_bus_name'] ='Cahaba Government Benefit Administrators, LLC'
months = pd.concat([nov, dec])$ print nov.shape, dec.shape, months.shape
for col in user_df.columns[3:]:$     print col, user_df[col].unique()
grouped_date = merged_table.groupby(['Date']).agg({"Likes": "sum","Retweets": "sum","Compound":"mean",$                                                       "Negative":"mean","Neutral":"mean","Positive":"mean"}).apply(list).reset_index()$ grouped_date
flights2.passengers.plot()
cityID = '00ab941b685334e3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Nashville.append(tweet) 
stmt = text("SELECT * FROM states where last_changed>=:date_filter")$ stmt = stmt.bindparams(date_filter=datetime.now()-timedelta(days=20))
r = pd.DataFrame(q, columns = ['cat','score'])$ r.head()
plt.savefig(str(output_folder)+'NB01_4_NDVI01_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice_str)
train_features = bind_features(train_reduced, train_test="train").cache()$ train_features.count()
sns.factorplot('SA','len',data = data, kind = 'point', size = 6)
plt.hist(y_res)$ plt.hist(y)$ plt.show()
url = 'https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2015-01-01&end_date=2015-01-01&api_key='$
c = b[b == 7].index.get_level_values('user_id').tolist()$
news_df = pd.DataFrame(news_dict)$ news_df.head()
joined['Retire'].dtypes
dfd = pd.get_dummies(dfm, columns=['country'], drop_first=True)$ dfd = pd.get_dummies(dfd, columns=['cat_type'], drop_first=True)
mars_html_table = mars_table.to_html()$
json = r1.json()$ print(type(json)) #result is dictionary. yay!$ print(json)
scores_df.to_csv('News_Tweets.csv', encoding="utf-8", index=False)
mean_absolute_error(Y_btc_val, [Y_btc_train.mean() for i in range(Y_btc_val.shape[0])])
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost'], axis=1)$ dfY = data['ooCost']
BBC = news_df.loc[(news_df["Source Account"] == "BBCNews")]$ BBC.head(2)
round((model_x.rsquared), 3)
df["result"].plot()$ plt.show()
plt.legend(handles=[Urban,Suburban,Rural], loc="best")
display(data.head(10))$
c.loc[c>0.7]
year10 = driver.find_elements_by_class_name('yr-button')[9]$ year10.click()
os.chdir("C:\\#Study\\2. Ryerson_all\\THE MRP\\Dataset\\test_pad")
X_train, X_test, y_train, y_test = train_test_split(features,classification_price,test_size=0.2)
final.to_csv('final.csv')
df_1 = df.drop(['AppID','AppName', '1', '2', '3', 'Device','DeviceType','Tags','Updated'], axis =1)$
!ptdump -av 'data/my_pytables_file.h5'
alpha = 0.05$ np.random.seed(999)$ dist_n = (np.random.randn(10000) + 5) * 4 # +5 fixes mean, *4 fixes stdev
inspector = inspect(engine)$ inspector.get_table_names()
geometry = openmc.Geometry(root_universe)
cityID = '813a485b26b8dae2'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Albuquerque.append(tweet)  
style.use('ggplot')
tweet_df_polarity = tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
temp_nc = Dataset("../data/nc/air.mon.mean.nc")
with open('united_list_fresh_lower.pickle', 'rb') as ff:$     united_list_fresh_lower = pickle.load(ff)
clusters_compare=pd.concat([final_df_copy,pd.DataFrame(data=kmeans3,columns=['kmeans3']),pd.DataFrame(data=kmeans5,columns=['kmeans5']),pd.DataFrame(data=kmeans6,columns=['kmeans6'])],axis=1)$ clusters_compare.head()$ clusters_compare.to_csv(encoding='utf-8',path_or_buf='clusters.csv')
building_pa_prc_shrink.dtypes
from nltk import pos_tag$ sentence = word_tokenize('I always lie down to tell a lie.')$ pos_tag(sentence)
dict(list(result.items())[:20])
fuel_mgxs = mgxs_lib.get_mgxs(fuel_cell, 'nu-fission')
request_data_2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31')
DummyDataframe = DummyDataframe.set_index("Date").sort_index()$ DummyDataframe = DummyDataframe.groupby("Date").sum()
questions = pd.concat([questions.drop(['month_bought'], axis=1), month], axis=1)
step_counts[1:3] = np.NaN
feature_names = vectorizer.get_feature_names()$ class_labels = clf.classes_$ print(class_labels)
plt.pie(totalRides, labels = labels, explode = explode, $         colors = colors, autopct="%1.1f%%", shadow=True, startangle=180)$ plt.show()
lr = LogisticRegressionCV(n_jobs=3)$ lr.fit(X_train[['avg_shifted_against', 'def_shift_pct']], y_train)
capa2017.head()
i=random.randrange(len(train_x))$ print_query(i)
print(fitB_Cl12l21.summary2())
grid.best_params_
atdist_4x_positive = atdist_4x[atdist_4x['emaResponse'].isin(["Yes! This info is useful, I'm going now.", "Yes. This info is useful but I'm already going there."])]$ atdist_4x_positive.merge(atloc_4x, how='left', on=['vendorId', 'taskLocationId'])
model = os.path.join('files', '2008--Mendelev-M-I--Al--111-GSF.xml')$ gamma = am.defect.GammaSurface(model=model, box=box)
new = df.sort_values("dates")$ new.reset_index()$
ngrams_summaries = cvec_3.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
data = r.json()
suburban_avg_fare = suburban_type_df.groupby(["city"]).mean()["fare"]$ suburban_avg_fare.head()$
output.printSchema()$ output2 = output.select('label', 'features')$
import numpy as np$ ok.grade('q01')
p_new = df2['converted'].mean()$ print("{} is the convert rate for  Pnew under the null.".format(p_new))
mgxs_lib.by_nuclide = True
cur.execute('SELECT count(Comments_Ratings) FROM surveytabl WHERE Comments_Ratings is not null;')$ cur.fetchall()
results = session.query(measurement.date, measurement.prcp).filter(measurement.date >= prev_year).all()
def day_of_week(times):$     datetime_df = pd.DataFrame(times)
df = pd.read_csv(source_csv)$ df.head()
df_daily4=df_daily.groupby(["C/A", "UNIT", "STATION"]).DAILY_ENTRIES.sum().reset_index()$ df_daily4.head(5)$
from nltk.stem.wordnet import WordNetLemmatizer$ lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]$ print(lemmed)
import numpy as np$ raw_data = df.as_matrix()$ print(raw_data.shape)
import warnings; warnings.simplefilter('ignore')
all_sets.describe()
model.create_timeseries(scenario)$ model.close_db()
tweetdf.loc[tweetdf['lat'].notnull(),'lga'] = np.asarray(output)$
festivals.head(5)
store_items = store_items.append(new_store, sort=False)$ store_items
feedbacks_stress.describe()$
slope, intercept, r_value, p_value, std_err = stats.linregress(data['timestamp'],data['rating'])
with open(example_text_file, mode = 'w', encoding='utf-8') as f:$     f.write(stringToWrite)
stn_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).filter(Measurement.station == busiest_stn).filter(Measurement.date > data_oneyear).order_by(Measurement.date).all()$ stn_temp
df.iloc[0:4]
y=dataframe1['RSI_30']$ plt.plot(y)$ plt.show()
ratings.sample(5)
client.repository.delete('50017ab0-237c-451b-befe-ef435c1b5d86')
Base = automap_base()$ Base.prepare(engine, reflect= True)$ Base.classes.keys()$
news = pd.DataFrame([(div.h3.text.rsplit(' - ')[0],str(datetime.strptime(div.h3.text.rsplit(' - ')[1],'%B %d, %Y')),str(div.p).rsplit('<br/>')[0].replace('<p>','')) $         for div in soup.find_all('div',{'class':'bitcoin_history'})])$ news.columns=['Headline','Date','News']
print(r.json())$
movies_df.loc[movies_df['movieId'].isin(recommendationTable_df.head(20).keys())]
weather_x = weather_norm.drop('tavg', axis=1)$ weather_y = weather_norm['tavg'].shift(-1)
tmp = data_df.copy()$ tmp.columns = [x if re.search("size", x) else "data_{}".format(x) for x in tmp.columns]
result = cur.fetchall()$
ml_repository_client = MLRepositoryClient(service_path)$ ml_repository_client.authorize(username, password)
year_dict = year_request.json()$ print (type(year_dict))
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ print(commits_per_year.apply(lambda x: x.head())[0:5])
h2o.init()$
url = 'https://www.reddit.com/r/Python/'
file = "../Data/output/sampleDataWithSentiment.csv" $ allData = pd.read_csv(file,index_col=None, header=0, engine="c", encoding='utf-8', low_memory=False)
engine = create_engine("sqlite:///hawaii.sqlite")$
media_user_results_df = pd.DataFrame.from_dict(results_list)$ media_user_results_df.head(10)
criteria = so['ans_name'].isin(['Scott Boston', 'Ted Petrou', 'MaxU', 'unutbu'])$ so.loc[criteria].head()
lr.score(test_array, y_test)$
stats = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$ filter(Measurement.station=="USC00519281").group_by(Measurement.station).all()$ stats
(token <- readRDS("data_sci_8001_token.rds"))
model = pipeline.fit(train)
fh_3 = FeatureHasher(input_type='string', non_negative=True)$ %time fit3 = fh_3.fit_transform(train.device_ip)
missing_millesime = intervention_train.MILLESIME.isnull()$ intervention_train.loc[missing_millesime, 'MILLESIME'] = intervention_train.loc[missing_millesime, 'CRE_DATE_GZL'].apply(lambda x: x.year)
new_df = pd.read_sql_query('select * from actor where first_name = "Groucho"', engine)$ new_df.head()$
def only_upper(s):$     return "".join(c for c in s if c.isupper())
seventeen = r.json()['dataset_data']$ seventeen
list(tweets_total[0].keys())
dictionary = corpora.Dictionary(text_list)$ dictionary.save('dictionary.dict') $ print dictionary
start_t = '2013-01-01 00:00:00'$ end_t=pd.to_datetime('today')- timedelta(days=1)$ end_t1=str(end_t)$
qs.columns
def normalizePrice(price):$     return int(price.replace(',',''))$ df['price'].map(lambda price: int(price.replace(',','')) ).head()
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLA.csv"$ tesla = pd.read_csv(url, index_col=0, parse_dates=True) # index_col = 0, means that we want date to be our index$ tesla
df2[df2['group']=='control']['converted'].mean()$
df_gnis_test = df_gnis.dropna(axis=0, subset=['PRIM_LAT_DEC','PRIM_LONG_DEC'],thresh=1)$ df_gnis_test.shape
contractor_final.info()
data["type"].unique()
tweet_archive_clean['stage'] = tweet_archive_clean[['doggo', 'floofer','pupper', 'puppo']].apply(lambda x:''.join(x), axis= 1)
grouped_authors_by_publication.rename(columns = {'authorName':'authorNames_in_given_publication'}, inplace = True)$ grouped_authors_by_publication.rename(columns = {'authorId':'authorIds_in_given_publication'}, inplace = True)
iris = pd.read_csv('data/iris.csv')$ print(iris.shape)
df['log_AAPL']=np.log(df['NASDAQ.AAPL'])
prec_us_full = prec_nc.variables['pr_wtr'][:, lat_li:lat_ui, lon_li:lon_ui]
(fe.bs.SPXmean, fe.bs.SPXsigma)
df = pd.read_parquet('training_data.parquet')
df.head()
unigram_chunker = UnigramChunker(train_trees)$ print(unigram_chunker.evaluate(valid_trees))
executable_path = {'executable_path': 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe'}$ browser = Browser('chrome', **executable_path, headless=False)$ url = 'https://mars.nasa.gov/news/'
station_num = session.query(func.count(Station.station)).all()$ station_num
df.zone.fillna('Unknown', inplace=True)$ df.county_name.fillna('Alameda', inplace=True)
pd.merge(d1, d3, left_on='city', right_on='place').drop('place', axis=1)
(pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days
files = glob.glob(os.path.join(input_folder, year_string))
supreme_court_df.head()
Base = automap_base()$ Base.prepare(engine, reflect=True)
print(r.json()['dataset']['data'][0])
birth_dates.head()
weather1=pd.read_csv('data/Crime/weather1.csv')$ weather2=pd.read_csv('data/Crime/weather2.csv')$ print(weather1.shape, weather2.shape)
AVG_daily_trading_volume = mydata['Traded Volume'].mean()$ AVG_daily_trading_volume
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ data.head(10)
TEXT.vocab.itos[:12]
date_collected = datetime.strftime(nytimes_df['Date'][0], "%B %d, %Y")$
plans_set = set()$ [plans_set.add(plan) for plans_combination in np.unique(USER_PLANS_df['scns_array']) for plan in plans_combination ]$ plans_set
soup = bs(response.text, 'html.parser')
df.head()
avg_order_intervals = np.fromiter(result.values(), dtype=float)$ avg_order_intervals.size
train_pos = train_sample.filter(col('is_attributed')==1)$ n_pos = train_pos.count()$ print("number of positive examples:", n_pos)
df_max = df.groupby('date').head(1)$ df_count = df.groupby(['date'] ,as_index=False).count()$ df_mean = df.groupby(['date'], as_index=False).mean()
MEDIAN_daily_trading_volume = mydata['Traded Volume'].median()$ MEDIAN_daily_trading_volume
scn_genesis = pd.to_datetime(min(USER_PLANS_df['scns_created']))
import numpy as np$ ok.grade('q03')
df.columns = ['ID', 'name', 'category', 'Main category', 'currency', 'deadline',$               'goal', 'launched', 'pledged', 'State', 'Backers', 'country',$               'usd pledged', 'Pledged (USD)', 'Goal (USD)']
df['dealowner'] = df['dealowner'].str.split(expand = True)[0]
s.asfreq('3B', method='bfill').head(15)
plt.show()$
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', header=None)$ mydata.head(5)
import statsmodels.api as sm
groupby_user = df1['user'].groupby(df1['user']).count()$ groupby_user.describe()$
plt.savefig(str(output_folder)+'NB01_6_NDVI_change_'+str(cyclone_name)+'_'+str(location_name))
divs = body.find_all('div',class_='slide')$ divs
soup = bs(response.text, "html.parser")
df.loc['1975-01-01']
rural_avg_fare = rural_type_df.groupby(["city"]).mean()["fare"]$ rural_avg_fare.head()$
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key='$ url_api=url+API_KEY$ r = requests.get(url_api)
newfile = pd.read_excel('export new.xlsx')$ oldfile = pd.read_excel('export old.xlsx')
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Doctors', doc_duration, window=52)
df.info()
pd.set_option("display.max_rows", 20)$ np.random.seed(12)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY$ r = requests.get(url)$ json_data = r.json()
styles = df.groupby('simple_style').size().sort_values(ascending=False)$ styles.head(5)
example_sent = "April 2018 JAX Magazine is Out: Machine Learning. #BigData #DeepLearning #MachineLeaning #DataScience #AI #Python #RStats @filtration \\ppo."$ word_tokens = tokenize_tweet_text(example_sent, Qye_words = ['BigData'])$ print(word_tokens)$
x = df_1.groupby('Lang').Review.count().sort_values(ascending = False)$
building_pa = pd.read_csv('Building_Permits.csv')$ building_pa.head(5)
datAll['Offense Type'] = datAll['Offense Type'].str.strip()$ datAll['Offense Type'] = np.where(datAll['Offense Type']=="AutoTheft",'Auto Theft',datAll['Offense Type'])
lims_df.columns
keep_day_threshold = 4$ n_user_days = n_user_days.loc[n_user_days >= keep_day_threshold].reset_index()$ n_user_days = n_user_days[['seqn']]
pd.DataFrame(d, index=['d', 'b', 'a']) # uses d, not df, as data input
df_Tesla = pd.DataFrame.from_dict(dataset)$ df_Tesla[['created_at','text','hashtags','username','user_followers_count','topic']].head()
new_df = df.dropna(subset=['driver_id','pickup_lat','pickup_lon','dropoff_lat','dropoff_lon'],inplace=False)
windfield_matched_array=windfield_matched.ReadAsArray()$ print('windfield shape = '+ str(shape(windfield_matched_array)))$ print('ndvi_change shape = '+ str(shape(ndvi_change.values)))
from IPython.display import HTML$ HTML('<iframe src="https://opendata.aemet.es/centrodedescargas/inicio" width="700" height="400"></iframe>')
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]
pickle_in = open('linearregression.pickle', 'rb')$ clf = pickle.load(pickle_in)
vect = Seq2WordVecTransformer()$ X_vect = vect.fit_transform(X, verbose='debug')$ print ('len(X_vect):', len(X_vect))$
kayla['SA'] = np.array([ analyze_sentiment(tweet) for tweet in kayla['Tweets'] ])$ kelsey['SA'] = np.array([ analyze_sentiment(tweet) for tweet in kelsey['Tweets'] ])
weather.info()
import subprocess$ out = subprocess.check_output(["./zhunt3", '24', '16', '16', 'SimianVirus40.txt'])
model.load_weights('model')$ loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)$ print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))
df_concat['date'] = pd.DatetimeIndex(df_concat.date_series).normalize()$ df_concat.date.head()
raw_data_df.loc[raw_data_df['Income'] >= 1, 'Decrease_debt_Y_N'] = "Y"$ raw_data_df.head()
bucket.upload_dir('data/wx/tmy3/raw/', 'wx/tmy3/raw', clear_dest_dir=True)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyze = SentimentIntensityAnalyzer()
rain_df = pd.DataFrame(rain)$ rain_df.head()$
p_x = x.copy()
print_sessions = printer_usage.query('total_printed >= 1')$ print_sessions.describe()
print(soup.prettify())
n_trees = 10$ y_pred = model.named_steps['XGBClassifier'].predict(mapper.fit_transform(X_test), ntree_limit= n_trees)
distance = pd.Series(distance_list)
s.index[[2,3]]
s = pd.Series(np.random.randint(0,7,size=10))  #low,high,size $ print(s,'\n')$ s.value_counts()
exportparams = urllib.parse.urlencode({$     'action': 'tweet-export',$     'format': 'csv'})
experiment_details = client.repository.store_experiment(meta_props=experiment_metadata)$ experiment_uid = client.repository.get_experiment_uid(experiment_details)$ print(experiment_uid)
cgmStats = get_stats(cgm)$ cgmStats.T
X_copy['item_no'] = X_copy['item_no'].apply(lambda x: hash(x))
s.plot(figsize=(12,10), label="Actual")$ s_median.plot(label="Median")$ plt.legend();
sentiment_df = pd.DataFrame(results_list)$ sentiment_df
data['intercept'] = 1.0
utils.fix_coordinates(data)
submit.head(1)
result = api.search(q='%23HarryPotter')$ len(result)
browser.find_by_css("a.product-item")[i].click()$ hemisphere_image_urls = []$ links = browser.find_by_css("a.product-item")
i1 = pd.Index([1, 3, 5, 7, 9])$ i2 = pd.Index([2, 3, 5, 7, 11])
for row in cursor.columns(table='TBL_FCInspevnt'):$     print(row.column_name)
shuffled = data.sample(frac=1)$
data.iloc[:,10:14] = data.iloc[:,10:14].fillna("0")  # Overall_Credit_Status, Delivery_Block, Billing_Block, Block_flag$ data.iloc[:,26] = data.iloc[:,26].fillna("0")   # state$ data.dropna(how='any',axis='rows',inplace=True) # district
testing.to_csv('SHARE_cleaned_lists.csv', index=False)
total_students_with_passing_math_score = len(df_students.loc[df_students['math_score'] > 69])$ total_students_with_passing_math_score
groupby_location = df1['location'].groupby($     df1['location']).count().sort_values(ascending=False)$ print(groupby_location.head())$
concat_3 = pd.concat([df1, df2, df5], axis=1)$ concat_3
CREATE OR REPLACE VIEW all_rooms AS$     SELECT h.hotel_id, h.hotel_name, r.room_num, r.room_type_code, r.occupancy, r.rate $
network_simulation[network_simulation.generations.isin([2])]$
vals = Inspection_duplicates.index.values$ vals = list (vals)$ vals
station_data = session.query(Stations).first()$ station_data.__dict__
sns.boxplot(x=df['liked'],y=df['Age_Years'],data=df,whis=np.inf)$
df.injured.value_counts()
wb = openpyxl.load_workbook('most_excellent.xlsx')$ wb.sheetnames
fin_r_monthly = fin_r.resample('M').asfreq()
manager.image_df['p_hash'].isin(tree_features_df['p_hash']).describe()
df['pb_prod'] = df['pledged'] * df['backers']$ df['pb_avg'] = df[['pledged', 'backers']].mean(axis=1)
red_4['age'] = red_4['age'].astype('timedelta64[h]')$ red_4.head()
from pyspark.ml.evaluation import BinaryClassificationEvaluator$ evaluator = BinaryClassificationEvaluator(rawPredictionCol="prediction", labelCol="label", metricName="areaUnderROC")$ print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(RN_PA_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours First Difference Partial Autocorrelation')
from sqlalchemy import create_engine$ engine = create_engine('postgresql+psycopg2://postgres:admin@localhost:5432/DTML')$ X_copy.to_sql('dtml_featr_num', engine, if_exists='append')
file_for_reading = open('life.txt','r',encoding='cp949')$ file_for_reading.close()
print("Exporting to %s"%(man_export_filename))$ extract_man_uw.to_csv(man_export_filename, index=False)
print match.text
stop = np.floor(len(X)/4).astype(int)$ model.fit(X[:stop], y[:stop], epochs=200, batch_size=128*2)$
data = df.values
df_students.describe()
client = boto3.client('s3')$ with open(datafile, 'wb') as f:$     client.download_fileobj(bucket, prefix, f)
len(pd.unique(df2['user_id']))
top_score = df.rating_score.max()$ print('The highest rating is a {} out of 5.'.format(top_score))
np.shape(temp_us_full)
df_uv = df.query('landing_page != "new_page"') $ df_vu = df_uv.query('group == "treatment"')$ df_vu.count()$
model = model_simple_nmt(len(human_vocab), len(machine_vocab), Tx)$ model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
tweetsIn22Mar = tweets22Mar[tweets22Mar.lang == 'in']$ tweetsIn1Apr = tweets1Apr[tweets1Apr.lang == 'in'] $ tweetsIn2Apr = tweets2Apr[tweets2Apr.lang == 'in']
X = df.drop(ls_head.index('sr_flag'), axis='columns')$ X = X.drop(0, axis='columns')$ y = df[ls_head.index('sr_flag')]
data_tickers = data_tickers.resample(sampling, how='last') $ data_tickers.head()
data.info()
attend_with.columns = ['ATTEND_'+str(col) for col in attend_with.columns]
df['MeanFlow_cms'].describe()
[(type(nd), nd.shape) for nd in read_in["ndarrays"]]
red_4 = red[['title', 'subreddit', 'num_comments', 'created_utc', 'id', 'time fetched']].copy(deep = True)$ red_4.head()
my_gempro.set_representative_structure()$ my_gempro.df_representative_structures.head()
DataSet[['userName','tweetRetweetCt']].sort_values('tweetRetweetCt',ascending=False).head(10)
df_students.columns
Largest_change_in_one_day = (mydata['High'] - mydata['Low']).max()$ Largest_change_in_one_day
data_folder = '../input/export_Feb_8_2018'$ ods = SlackLoader(data_folder)
cityID = 'b046074b1030a44d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jersey_City.append(tweet) 
crimes.columns = crimes.columns.str.replace('__', '_')$ crimes.columns
filt_zip_loc=~np.logical_and(building_pa_prc_shrink[['zipcode']].isna().values ,building_pa_prc_shrink[['location']].isna().values)
pax_raw['minute_of_day'] = pax_raw.paxhour*60 + pax_raw.paxminut$ pax_raw.head()
gDate_vEnergy = gDateEnergy_content.count().unstack()$ gDate_vEnergy = gDate_vEnergy.fillna(0)
Measurement = Base.classes.measurement$ Station = Base.classes.station
urb_pop_reader = pd.read_csv(file_wb, chunksize = 1000)$ df_urb_pop = next(urb_pop_reader)$ print(df_urb_pop.head())
df_raw_fb = pd.read_csv('./Datasets/Facebook_Training_Data.csv', encoding='latin1')$ print (df_raw_fb.head())
s.loc['c'] $
print("Before drop these columns, data table has shape: ", data.shape)$ data = data.drop(to_drop, axis=1)$ print("After dropping these columns, data table has shape: ", data.shape)
com_eng_df['issues_open'].plot()
display(flightv1_1.select('timeline_leg1.getItem(0)').show(100, truncate=False))$
img_path = os.getcwd()+ '/images/'
fh_3 = FeatureHasher(num_features=uniques.iloc[0, 1], input_type='string', non_negative=True)$ %time fit3 = fh_3.fit_transform(train.device_ip)
free_data.head(10)
avg_neighborhood_rt[avg_neighborhood_rt>14].sort_values()
openmc.plot_geometry(output=False)
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2018-03-27&end_date=2018-03-27&api_key='+API_KEY)
wash_parkdf.coordinates.dropna(inplace=True)
ddp = dfd.dropna(axis=0, subset=['in_reply_to_screen_name'], how='any')
lr = LogisticRegression(random_state=20)$ lr.fit(X, y)
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')
df_raw = df_raw.loc[df_raw.artist.notnull(),:]
df = pd.get_dummies(df, columns = ['new_home', 'hail_resistant_roof'],$                     drop_first=True)
val_pred_svm = lin_svc_clf.predict(X_valid_cont_doc)
df = pandas.DataFrame(data['11']['data']['month']['data'])
if 0 == go_no_go:$     lda_vis_serialized = pyLDAvis.gensim.prepare(lda, serial_corp, d)$     pyLDAvis.save_html(lda_vis_serialized, fps.pyldavis_fp)
(a + b).describe()$
trump.index=pd.DatetimeIndex(trump['created_at'])$ trump.drop('created_at', axis = 1, inplace =True)$ trump.shape
graph_url = 'https://graph.facebook.com/v2.11/'$ req_url = '74133697733?fields=posts{message,created_time,comments.limit(0).summary(true), likes.limit(0).summary(true)}'$ final_url = graph_url + req_url
len(free_data.country.unique())
df.loc[:,['A','B']]
plt.pie(totalDrivers, labels = labels, explode = explode, $         colors = colors, autopct="%1.1f%%", shadow=True, startangle=180)$ plt.show()
model_NB = MultinomialNB()$ model_NB.fit(count_vectorized, df_train.author)
metadata = df_small[['order_num', 'date', 'country']].drop_duplicates()$ metadata.head()
df1['label'] = df[forcast_col].shift(-forcast_out)$ df1['label'].head()
prcp_year_df['date'] = pd.to_datetime(prcp_year_df['date'])
contractor_merge['contractor_bus_name'] = contractor_merge['contractor_bus_name']+" - "+contractor_merge['contractor_number'].astype(str)
bow_corpus  = [dictionary.doc2bow(text) for text in list(repos)]$ index = SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12418)
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['Retweets'].values, index=data['Date'])
twitter_df = spark.createDataFrame(delimited_twitter_df)
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%s" % (x.year, str(x.week).zfill(2)))$ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%s" % (x.year, str(x.week).zfill(2)))
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, prec_us)
data.Likes.value_counts(normalize=True).head().plot(kind='bar')
data.groupby(['Year'])['Salary'].sum()$ data.groupby(['Year'])['Salary'].mean()
newcolumns = df1.columns.str.strip()$ df1.columns = newcolumns
img_url = f'https://www.jpl.nasa.gov{img_url_rel}'$ img_url
df['Year'] = df['created_date'].dt.strftime('%Y')$ df['YearMonth'] = df['created_date'].dt.strftime('%Y/%m')$ df['Month'] = df['created_date'].dt.strftime('%b')
def dist(a, b, ax=1):$     return np.linalg.norm(a - b, axis=ax)
print(afx_x_oneday.json())
!head ../../data/msft2.csv  # Linux
preds = aml.leader.predict(test)
total_y = list(youTubeTitles.values[:2500,1]) + list(pornTitles.values[:2500,1])
ibm_hr_cat_dum = spark.createDataFrame(pd_cat)$ ibm_hr_cat_dum.show(3)
df.rename(columns={'Price':'House_Price'},inplace=True) # renaming inplace i.e. in that same data frame$ df.head()
sample.head(1)
testing.head()
data.sample(4)
aci_service.get_logs()
ctc_alpha = ctc ** (1/3)$ ctc_alpha = ctc_alpha / ctc_alpha.max().max()
media = np.mean(datos['len'])$ print("El promedio de caracteres en tweets: {}".format(media))
df['comments'].value_counts()/len(df['comments'])
gene_df['gene_name'].unique().shape
manager.image_df[manager.image_df['filename'] == 'image_sitka_spruce_71.png']$
df.groupby([df.Date.dt.month, df.Date.dt.day]).count().Tweets
dr_2018 = dr_2018.resample('W-MON').sum()$ RNPA_2018 = RNPA_2018.resample('W-MON').sum()$ ther_2018 = ther_2018.resample('W-MON').sum()
set(user.columns).intersection(raw.columns)
twitter_df_wa=twitter_df[twitter_df.location=='Washington, DC']$ plt.plot(twitter_df_wa['created_at_time'], twitter_df_wa['retweet_count'], 'ro')$ plt.show()
plt.savefig(str(output_folder)+'NB01_3_landscape_image02_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice02_str) 
tweet_list = api.search(q='#%23mallorca')
plt.rc('figure', figsize=(5, 5))$ mosaic(crosstabkw.stack(),gap=0.03 )$ plt.title('Mosaic graph: Successful / Contain Keyword')$
pca_full = PCA()$ pca_full.fit(crosstab) ## note: This takes 1:20 minutes to complete 20,000 records
cities = set(us_cities['City'].str.lower())$ states = set(us_cities['State full'].str.lower())$ counties = set(us_cities['County'].str.lower())
ds = tf.data.TFRecordDataset(train_path)$ ds = ds.map(_parse_function)$ ds
df_links['link.domain'].value_counts().head(25)
yc_new1 = yc_new.merge(zipincome, left_on='zip_depart', right_on='ZIPCODE', how='inner')$ yc_new1.head()
name =contractor.groupby('contractor_bus_name')['contractor_number'].nunique() $ print(name[name>1])
if 1 == 1:$     token_counter = collections.Counter(tokens)$     print(str(token_counter.most_common(50)))
load2017.isnull().sum() 
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-08-10&end_date=2018-08-10&api_key=xhB-Ae2VRyMYmv2CRGtV")
year_with_most_commits = commits_per_year.sort_values(by='author', ascending=False).head(1) $ year_with_most_commits = 2017
reverseAAPL = AAPL.sort_index(ascending=False)$ reverseAAPL.head()
for c in ccc:$     if not os.path.isdir('output/' + c):$         os.makedirs('output/' + c)
node_types_DF = pd.read_csv('network/recurrent_network/node_types.csv', sep = ' ')$ node_types_DF
taxi_sample = conn.select_ipc_gpu(query, device_id=2)
df3.to_csv('lyincomey_subset_v5_wseconds.csv')
S_distributedTopmodel.basin_par.filename
np.unique(bdata.index.time)
(a_diff<np.array(p_diffs)).mean()
month_year_crimes = crimes.groupby(['year', 'month']).size()
plt.savefig(str(output_folder)+'NB01_2_landscape_image01_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice_str)
df_day = endog.to_frame()$ df_day.rename(columns={"BikeID" : "Count"}, inplace=True)
unknown_users_log = df_log[~df_log['user_id'].isin(df_users['user_id'])]
stream.filter(track=['clinton','trump','sanders','cruz'])
volume.sort()$ volume[len(volume)/2]
df_full.rename(columns=DATA_L2_HDR_DICT,inplace=True)
with gzip.GzipFile('data/cleaned_df.pkl.gz', 'wb') as file:  $     joblib.dump(df, file)
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1 - p_new)])$ p_new_sim = new_page_converted.mean()$ p_new_sim
c = Counter(tree_labels)$ pprint.pprint(c.most_common(10))
df[pd.isnull(df.Address1)]
data = pd.read_csv('Eplusfanpage.csv')$ data$
data = data[data.state != 'undefined']$ data.info()
print(commmon_intervention_train.values.shape)$ print(commmon_intervention_test.values.shape)
data.describe()
result = requests.get(url)$ c = result.content$ soup = BeautifulSoup(c, "lxml")
station_count = session.query(func.count(Station.id)).all()$ station_count
logging.info('Finishing dataset')$ df_final['Close: t'] = df_final['Close: t'].shift(1)$ df_final = df_final[2:-15]
dj_df = pd.read_table(data_file_path)$ print(dj_df.head())$ print(dj_df.info())
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
pd.Series(pd.Categorical(iris["Species"])).sample(5)
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
%%time$ if 1 == 1:$     news_period_df = pd.read_pickle(config.NEWS_PERIOD_DF_PKL)
ds_train = FileDataStream.read_csv(train_file, numeric_dtype=np.float32, sep='\t')$ print(repr(ds_train.Schema))$ print(ds_train.Schema)
result = pd.concat(six)$ free_data = pd.merge(left=free_data, right = result.to_frame(), left_index=True, right_index=True)
STATION_traffic_weektotals = (SCP_ENTRY_weektotals + SCP_EXIT_weektotals).groupby(['STATION']).sum()$ STATION_traffic_weektotals.sort_values(ascending=False).head(10)
print('number of contributions with missing candidate name: ',len(dat[dat.CAND_NAME.isnull()==True]))$ print('number of candidate ids for contributions with missing candidate name: ',len(pd.unique(dat[dat.CAND_NAME.isnull()==True].CAND_ID)))
tokenizer = Tokenizer(char_level=True, filters=None)$ tokenizer.fit_on_texts(invoices)
locations = DataSet['userLocation'].value_counts()[:10]$ print(locations)
punkt = nltk.data.load('tokenizers/punkt/english.pickle')
data.sort_values(by = 'Age', ascending = True)
fix_space = lambda x: pd.Series([i for i in reversed(x.split(' '))])
stations = Base.classes.stations$ stations
left = pd.DataFrame({'key':['foo','boo'], 'qval': [1,2]})$ right = pd.DataFrame({'key': ['foo','boo','zoo'], 'rval': [4,5,100]})$ print( pd.merge(left,right,on='key'))
h5 = qb.History[QuoteBar](eur.Symbol, timedelta(30), Resolution.Daily)$
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
fi = ed_level.append(free_sub)
cityID = 'dd3b100831dd1763'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_Orleans.append(tweet) 
gnb.fit(X_clf, y_clf)
tips.groupby(["sex", "day"]).mean().reset_index()
rddScaledScores = RDDTestScorees.map(lambda entry: (entry[1] * 0.9))$
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"$ date_min = date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"
logit_countries2 = sm.Logit(df4['converted'], $                            df4[['ab_page', 'country_UK', 'country_US', 'intercept']])$ result3 = logit_countries2.fit()
set(user_df.columns).intersection(stories.columns)
filepath = os.path.join("data_output","df_sample.csv")$ df_sample = pd.read_csv(filepath)$ df_sample.head()
df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium", "good", "very good"])$ df["grade"]
data.dtypes
joined = joined.astype(intDict)
stations = session.query(Station.name).count()$ print(f'There are total of {stations} stations in Hawaii'.format())
df = table[0]$ df.columns = ['Parameter', 'Values']$ df.head()
digitalgc_tweets = api.search(q="#digitalgc", $                               count=100, lang="en", $                               since="2018-01-01")$
sanne_data = bird_data[bird_data.bird_name == 'Sanne']$ print(sanne_data.timestamp.head())
mean_pr = prec_long_df.groupby(['date'])['prec_kgm2'].mean()$ mean_pr.plot(x='date', y='prec_kgm2')
mean = np.mean(df.len)$ print('Tweet length average is: {}\n'.format(mean))
pre_number = len( niners[niners['Jimmy'] == 'no']['GameID'].unique() )$ print pre_number
import pandas as pd$ commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ print(commits_per_year.head())
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_jp_200_wzh.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_200_wzh.w2v')
miner = TweetMiner(twitter_keys, api, result_limit=20)$ sanders = miner.mine_user_tweets(user='bernisanders', max_pages=10)
station_data = session.query(Stations).first()$ station_data.__dict__
from app.crawler import Crawler$ crawler = Crawler()$ crawler.pull_save_data()
df['y'].plot.hist()
dti2 = pd.to_datetime(['Aug 1, 2014', 'foo']) $ type(dti2)
max_ch_ol2 = max(abs(u.close-v.close) for u,v in zip(list(o_data.values()),list(o_data.values())[1:]))$ print('Another one liner using islice: {:.2f}'.format(max_ch_ol2))
rain_df.set_index('date').head()
y_newpage = df2["user_id"].count()$ prob_newpage = x_newpage/y_newpage$ prob_newpage$
df['duration'] = np.round((df['deadline'] - df['launched']).dt.days / 7)
df1 = df.loc[:, ('Close')].reset_index().rename(index=str, columns={"Date": "ds", 'Close': 'y'})
ab_groups = pickle.load(open("ab_groups.p", "rb"))
df.shape
nlp = spacy.load('en')$ op_ed_articles['full_text_tokenized'] = op_ed_articles['full_text'].apply(lambda x: nlp(x))$
gene_df = sub_gene_df[sub_gene_df['type'] == 'gene']$ gene_df = gene_df.copy()$ gene_df.sample(10).attributes.values
load_dotenv('.env')
data_date_df = pd.concat([pd.DataFrame(pd.to_datetime(data_df_reduced.created_time)).rename(columns={"created_time": "date"}), data_df_reduced.drop("created_time", 1)], axis=1)$ data_date_df.head()$
df_p = pd.DataFrame({'Date': list_date, 'Clique Words': list_clique, 'Tweets':list_whole})$ df_p = df_p.sort_values('Date')$ df_p$
engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))$ print(engine.url)
combined = dfs[0].append(dfs[1], ignore_index=True)$ combined = combined.append(dfs[2], ignore_index=True)$ combined.head()
data.head()
tweet.text
%pycat bikescore.py$
cityID = 'af2a75dbeb10500'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lincoln.append(tweet) 
print("p_new under the null is: %.4f\np_old under the null is: %.4f" %(p_new, p_old))
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$
finals['type'] = "normal"$ finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'facilitator'
from dateutil.parser import parse $
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-06-01&end_date=2018-06-01&api_key=")$
exiftool -csv -createdate -modifydate cisrol12/cycle1_MVI_0032.mp4 cisrol12/cycle1_MVI_0033.mp4 cisrol12/cycle1_MVI_0034.mp4 cisrol12/cycle2_MVI_0035.mp4 cisrol12/cycle2_MVI_0036.mp4 cisrol12/cycle2_MVI_0037.mp4 cisrol12/cycle2_MVI_0038.mp4 cisrol12/cycle2_MVI_0039.mp4 cisrol12/cycle2_MVI_0042.mp4 cisrol12/cycle2_MVI_0043.mp4 cisrol12/cycle2_MVI_0044.mp4 cisrol12/cycle2_MVI_0045.mp4 cisrol12/cycle2_MVI_0046.mp4 cisrol12/cycle2_MVI_0047.mp4 cisrol12/cycle2_MVI_0048.mp4 cisrol12/cycle2_MVI_0049.mp4 cisrol12/cycle2_MVI_0050.mp4 cisrol12/cycle2_MVI_0051.mp4 cisrol12/cycle2_MVI_0052.mp4 cisrol12/cycle2_MVI_0053.mp4 cisrol12/cycle2_MVI_0054.mp4 cisrol12/cycle2_MVI_0055.mp4 cisrol12/cycle4_MVI_0075.mp4 cisrol12/cycle4_MVI_0076.mp4 cisrol12/cycle4_MVI_0078.mp4 cisrol12/cycle4_MVI_0079.mp4 cisrol12/cycle4_MVI_0080.mp4 cisrol12/cycle4_MVI_0081.mp4 cisrol12/cycle4_MVI_0082.mp4 cisrol12/cycle4_MVI_0083.mp4 cisrol12/cycle4_MVI_0084.mp4 cisrol12/cycle4_MVI_0085.mp4 cisrol12/cycle4_MVI_0086.mp4 cisrol12/cycle4_MVI_0089.mp4 cisrol12/cycle5_MVI_0095.mp4 cisrol12/cycle5_MVI_0096.mp4 cisrol12/cycle5_MVI_0097.mp4 cisrol12/cycle5_MVI_0098.mp4 cisrol12/cycle5_MVI_0100.mp4 cisrol12/cycle5_MVI_0101.mp4 cisrol12/cycle5_MVI_0102.mp4 cisrol12/cycle5_MVI_0106.mp4 > cisrol12.csv
data = res.json()   
studies_c = pd.merge(studies_b,countries[['nct_id','name']],on='nct_id', suffixes=('_sponsor', '_country'),how='left')$ studies_c.head()
r6s.num_comments.mean()
numero = json.loads(ejemplo_json).get('phoneNumbers')[1].get('number')$ print(numero)
result = api.search(q='%23puravida') #%23 is used to specify '#'$ len(result)
from sklearn.decomposition import PCA$ import sklearn
qs[dtanswer.dt.total_seconds() < 60][["Id", "CreationDate", "CreationDate_a"]].head()
pd.Series([2, 4, 6])
import lxml.html$ from lxml.cssselect import CSSSelector$ tree = lxml.html.fromstring(r.text)$
femalebydatenew  = femalebydate[['Sex','Offense']].copy()$ femalebydatenew.head(3)$
df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1]/100) for tup in pops_list]$ df_pop_ceb.plot(kind='scatter', x='Year', y='Total Urban Population')$ plt.show()
dates = [datetime(2014,8,1),datetime(2014,8,2)]$ ts = pd.Series(np.random.randn(2),dates)$ ts
to_week = lambda x: x.dt.week
score_summary = sns.countplot(x='Score', data=merge_df)$ score_summary.set_title('Recommendation')
INSERT INTO payment (booking_id, payment_date, payment_method, payment_amount)$ VALUES (8, TO_DATE('2017-09-10', 'YYYY-MM-DD'), 'BPay', 741.96)$
S_1dRichards.forcing_list.filename
my_stream_listener = PrintingStreamListener()$ my_stream = tweepy.Stream(auth = api.auth, listener=my_stream_listener)
len(ibm_train.columns), len(feature_col)
json_data = r.json()
for tweet in tw.Cursor(api.home_timeline).items(10):$     print(tweet.text) 
feature_cols = ['TV', 'radio']$ X = data[feature_cols]$ print(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')).mean())
df_tick_clsfd_sent = df_tick.join(df_amznnews_clsfd_2tick)$ df_tick_clsfd_sent.info()
plt.rcParams['figure.figsize'] = [16,4]$ plt.plot(pd.to_datetime(mydf3.datetime),mydf3.fuelVoltage, 'g.', markersize = 2);$ plt.xlim(datetime.datetime(2017,11,15),datetime.datetime(2018,3,28))
results[results['type']=='Other']['tone'].value_counts()
cdf.plot(kind='barh', x='category', y='occurrence_count', figsize=(12, 10), title= 'Categories', label= "Occurrence Count")$ plt.gca().invert_yaxis()$ plt.legend(loc= 4, borderpad= True)
n = 1$ selection = 'confidence'$ topn = summary.sort_values(by=['week_id', selection], ascending=[True, False]).groupby('week_id').head(n)
print(type(df.groupby("grade").count())) # as data frame ('id' column and 'raw_grade' column both contained)$ df.groupby("grade").count()
pd.Series([1,2,9])
print(json.dumps(geocode_result, indent=4))$
np.shape(temp_fine)
df[df.client_event_time < datetime.datetime(2018,4,1,23,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
df_geo_count = df_geo.groupby("geo_code").count()$ dict_geo_count = df_geo_count.to_dict()["id_str"]
len(chefdf.name)
amazon_review = pd.read_csv('amazon_cells_labelled.tsv',sep='\t') $ amazon_review
corpus = [dictionary.doc2bow(text) for text in texts]$ corpus
lda.print_topics()
Measurements = Base.classes.measurement$ Stations = Base.classes.station
kick_projects = kick_projects.replace({'country': 'N,0"'}, {'country': 'NZERO'}, regex=True)
crimes.columns = crimes.columns.str.replace('__', '_')$
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)$
sample=train.head(100).copy()$
my_gempro.genes.get_by_id('Rv1295').protein.representative_structure$ my_gempro.genes.get_by_id('Rv1295').protein.representative_structure.get_dict()
sub_gene_logical_vector = df.source.isin(['ensembl', 'havana', 'ensembl_havana'])$ sub_gene_df = df[sub_gene_logical_vector]$ sub_gene_df.shape
filter_commands = ['module load python/python-3.3.0']$ filter_commands.append("python %s/filter_fasta.py %s/swissProt_emb_proteins.fasta -out_fasta %s/swissProt_emb_proteins_filtered.fasta -v %s" %(PY_PATH, proteins_evidence_dir, proteins_evidence_dir, "'Arabidopsis thaliana' 'Solanum lycopersicum' 'Oryza sativa' 'Glycine max' 'Vitis vinifera'"))$ send_commands_to_queue('filter_swissprot', filter_commands, queue_conf)
data.to_csv('TwitterData.csv')
probs_test[:, 1].mean()
df.plot()$ plt.show()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path, sheetname = 1)$ df.head(5)
cleansed_search_df['Date'] = pd.to_datetime(cleansed_search_df['Date'], format="%Y%m%d", errors="coerce")$ cleansed_search_df
free_data.head(5)
MICROSACC.plot_default(microsaccades,subtype="count/(6*20)")+ylab("microsaccaderate [1/s]")
plt.savefig(str(output_folder)+'NB01_1_imagery_avaliable_cyclone_'+str(cyclone_name)+'_'+str(location_name))
df = pd.read_csv('ZILLOW-Z49445_ZRISFRR.csv',index_col=0)$ df.columns=['Price'] # Changing the name of the column. (Index is not treated as a column so in our df we have only 1 column)$ df.head()
reflRaw = refl['Reflectance_Data'].value$ reflRaw
fix_comma = lambda x: pd.Series([i for i in reversed(x.split(','))])
results_1dRichards, output_R = S_1dRichards.execute(run_suffix="1dRichards_hs", run_option = 'local')
guido_text = soup.text$ print(guido_text[:500])
df_ll.head(2)$ df_ll.isDuplicated.value_counts()$ df_ll.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
no_test_df = df[df["dataset"]=="train"] #.drop_duplicates(subset="text") actually can't do this w out changing vocab$ trn_df, val_df = sklearn.model_selection.train_test_split(no_test_df, test_size=0.1)$ len(no_test_df), len(df), len(trn_df), len(val_df)
top_songs.to_csv('top_songs_clean.csv')
df_daily.dropna(subset=["PREV_DATE"], axis=0, inplace=True)$ df_daily.head(5)
epochs = 50$ batch_size = 128$ keep_probability = 0.5
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(doc_duration, params=params, freq=12, title='Doctors Decomposition')
logreg = LogisticRegression()$ logreg.fit(X_train_all, y_train)$ logreg.score(X_test_all, y_test)
ngrams_summaries = cvec_4.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
trump.info()
ride_percity=original_merged_table["type"].value_counts()$
tweets.head()
network.rebuild()$ sim.run()
table_rows = driver.find_elements_by_tag_name("tbody")[16].find_elements_by_tag_name("tr")$
temp_df=df_small.groupby('order_num').sum().reset_index()$ temp_df.head()
df1 = pd.DataFrame([pd.Series(np.arange(10, 15)), pd.Series(np.arange(15, 20))])$ df1
df.time.unique().shape
!wget -nv -O /resources/data/PierceCricketData.csv https://ibm.box.com/shared/static/reyjo1hk43m2x79nreywwfwcdd5yi8zu.csv$ df = pd.read_csv("/resources/data/PierceCricketData.csv")$ df.head()
df['state'] = df['state'].str.capitalize()$ df.groupby('state')['ID'].count()
DataSet.tail()$
! rm -rf /home/ubuntu/s3/flight_1_5/extracted/flight_1_5_price_2017-05-10_.pq$
model = KNeighborsClassifier(n_neighbors=250)$ model = model.fit(train[0:, 1:5], train[0:, 7])
sq83= "CREATE TEMPORARY TABLE  newtable_22222 ( SELECT * FROM Facebook_NBA order by 0.2*likes+0.4*Comments+0.4*shares DESC limit 150)"$ sq84="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Id, SUBSTRING_INDEX(SUBSTRING_INDEX(message,' ',i+1),' ',-1) word FROM newtable_22222, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 0 ORDER BY total DESC, word;"
if 0 == 1:$     news_titles_sr.to_pickle(news_period_title_docs_pkl)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)$
data.info()
au.clear_dir('data/city-util/proc')
free_data.mean(),free_data.count()
import pandas_datareader $ faamg = pandas_datareader.get_data_google(['FB','AAPL', 'AMZN','MSFT', 'GOOGL' ])['Close']$
read_in = pd.read_pickle(processing_test.data())$ read_in
fse2017 = dict(r.json())
elms_all_0611.iloc[1048575:].to_excel(cwd+'\\ELMS-DE backup\\elms_all_0611_part2.xlsx', index=False)
s=r.json()$ type(s)
prcp_df.plot()$ plt.show()
studies = pd.read_csv('C:/Users/akapoor/Music/01 Docs/HealthCare App/ctdb/studies.txt', sep="|")
measure_avg_prcp_year_df.set_index('Date',  inplace=True)$ measure_avg_prcp_year_df.head()$
index_name = df.iloc[0].name$ print(index_name)
plt.rcParams['figure.figsize'] = [18.0, 10.0]
len([baby for baby in BDAY_PAIR_df.pair_age if baby<3])
print('Groceries has shape:', groceries.shape)$ print('Groceries has dimension:', groceries.ndim)$ print('Groceries has a total of', groceries.size, 'elements')
df.iloc[:4]
get_nps(combined_df, 'language').sort(columns='score', ascending=False).head(10)
groups = mgxs.EnergyGroups()$ groups.group_edges = np.array([0., 0.625, 20.0e6])
bd.reset_index()
titles = soup.find_all('div', class_='content_title')   $ print(titles)
spark.sql('create database if not exists my_analysis_work')$ spark.sql('drop table if exists my_analysis_work.example_timeseries')$ joined_patient.write.saveAsTable('my_analysis_work.example_timeseries')
content = [item.find_all(['p','h2']) for item in article_divs]$
def yearMarkers(axis_obj, x_pos, **kwargs):$     axis_obj.axvline(x_pos, linestyle='--', color='w', alpha=.4, **kwargs)$ years = np.arange(0,len(grouped), 51)
prcp_df.describe()
file_name = str(time.strftime("%m-%d-%y")) + "-NewsMoodTweets.csv"$ sentiments_pd.to_csv(file_name, encoding="utf-8")
evaluation_df = predictions_df.reset_index().sort_values(by=['unit', 'item', 'index'])$ evaluation_df['actual_value'] = test_df.sort_values(by=['unit', 'item', 'date'])['value'].values
sentiments_pd.to_excel("NewsMood.xlsx", encoding="UTF-8")
store_items.fillna(method = 'ffill', axis = 0)
top_10_authors = git_log['author'].value_counts()[:10]$ top_10_authors
measurements_df = measurements_df.dropna(how='any')
!wget https://raw.githubusercontent.com/sunilmallya/mxnet-notebooks/master/python/tutorials/data/p2-east-1b.csv
ts.shift(1)
pd.set_option('display.max_colwidth', -1)$ df[['text', 'favorite_count', 'date']][df.favorite_count == np.max(df.favorite_count)]
recommendationTable_df = ((genreTable*userProfile).sum(axis=1))/(userProfile.sum())$ recommendationTable_df.head()
pregnancies.data.loc[0]
for model_name in nbsvm_models.keys():$     valid_probs[model_name] = nbsvm_models[model_name].predict_proba(X_valid_cont_doc)
datasets_co_occurence = paired_df_grouped[['dataset_1', 'best_co_occurence']].set_index('dataset_1').to_dict()['best_co_occurence']
words_hash_sk = [term for term in words_sk if term.startswith('#')]$ corpus_tweets_streamed_keyword.append(('hashtags', len(words_hash_sk))) # update corpus comparison$ print('List and total number of hashtags: ', len(words_hash_sk)) #, set(terms_hash_stream))
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))$
from sklearn.ensemble import RandomForestClassifier
daily_returns = calc_daily_returns(closes)$ daily_returns.plot(figsize=(8,6));
!wc -l $ml_data_dir/ratings.csv
d1=Series(pd.to_datetime(datestrs));  $ d1
df_concat.drop(df_concat.columns[[0,1,2]], axis=1, inplace= True)
payments_all_yrs_ZERO_discharge_rank = (df_providers.loc[idx_ZERO_discharge_rank,:].groupby(['id_num','year'])[['discharge_rank']].sum())$ payments_all_yrs_ZERO_discharge_rank = payments_all_yrs_ZERO_discharge_rank.sort_values(['discharge_rank'], ascending=[False])$ print('payments_all_yrs_ZERO_discharge_rank.shape',payments_all_yrs_ZERO_discharge_rank.shape)
retweets = megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")]$ megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")]['text'].count() # 1,633
test_url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-08-21&end_date=2018-08-21&api_key=' + API_KEY$ test_data = requests.get(url)
cur = conn.cursor()
new_messages = messages.copy() $ new_messages.head()
new_model = gensim.models.Word2Vec.load('lesk2vec')
x["A"]$ x.get("A")$ x.A # works only for column names that are valid Python variable names
arr = bg_df.values.reshape(7, 3) # reshape our 21 values into 3 columns; becomes ndarray$ bg_df2 = pd.DataFrame(arr) # convert back to DataFrame$ bg_df2
gbm_predictions = pd.concat([pd.Series(gbm_pred, index=y_test.index, name='Predictions'), y_test], axis=1)$ gbm_predictions.head()
experiment_df = pd.read_csv('multi_arm_bandit_example_distrib.csv')$ experiment_df.drop(['Unnamed: 0'], axis=1, inplace=True)$ experiment_df.head(2)
plt.style.use('default')
optimizer = tf.train.AdagradOptimizer(0.01 )$ train = optimizer.minimize(loss)$
payload_scoring = {"fields":X_test.columns.tolist(), "values": values}$ print(payload_scoring)
high_low_diff = TenDayMeanDifference(inputs=[USEquityPricing.high, USEquityPricing.low])
store_items.interpolate(method = 'linear', axis = 1)
datetimes = pd.date_range(DATA_STARTTIME, DATA_ENDTIME, freq='min')$ datetimes[0:10]
set(stories.columns) - set(stories.dropna(thresh=9, axis=1).columns)$
groupby_time = df1['time'].groupby(df1['time']).count()$ groupby_time$
dfg = dfg.set_index(['drg3', 'discharges'])$
for c in ccc:$     vhd[c] = vhd[vhd.columns[vhd.columns.str.contains(c)==True]].sum(axis=1)
stationActive_df = pd.read_sql("SELECT station, count(*) as `Station Count` FROM measurement group by station order by `Station Count` DESC", conn)$ stationActive_df
prcp_df = pd.DataFrame(prcp_query).set_index('date')$ prcp_df.head()$ prcp_df_flat = pd.DataFrame(prcp_query)
data = pd.DataFrame(data=[tweet.text for tweet in results], columns=['Tweets'])$ display(data.head(10))$ display(data.tail(10))
Jarvis_ET_Combine = pd.concat([Jarvis_rootDistExp_1, Jarvis_rootDistExp_0_5, Jarvis_rootDistExp_0_25], axis=1)$ Jarvis_ET_Combine.columns = ['Jarvis(Root Exp = 1.0)', 'Jarvis(Root Exp = 0.5)', 'Jarvis(Root Exp = 0.25)']
s4.count()
df.head()
data = pd.read_sql("SELECT * FROM empvw_20",xedb)$ print(data)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-05-22&end_date=2018-05-22&api_key=zDPbq2QVaB7jFAEq5Tn6')
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
ward_df['Delay Time'].corr(ward_df['Crime Count'])
sns.catplot(x="Opening_year", y="HC01_VC36", hue="lic_code",$             col="isClosed", aspect=.6,$             kind="swarm", data=df);$
df_cat.head()
company_count_df = pd.DataFrame.from_dict(company_count, orient='index')$ company_count_df.columns=['Count']$ company_count_df.sort_values(by='Count', ascending=False).plot(kind='bar', figsize=(16,8), cmap='Set3')$
df.hastags = df.hastags.apply(getHasTags)
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','OriginationPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['orange','green', 'blue', 'red', 'black'], grid=False)$
contract_history['NUM_CAMPAGNE'] = contract_history['NUM_CAMPAGNE'].map(lambda x: float(x) if x not in ['N', ''] else np.nan)
dict_tokens = corpora.Dictionary(tokens)
grid_df.tail()
tweets_df = pd.read_csv(tweets_filename, $                         converters={'tweet_place': extract_country_code, $                                     'tweet_source': extract_tweet_source})
test_input_fn = create_predict_input_fn(test_data, DEFAULT_BS)$ stats = classifier.evaluate(input_fn=test_input_fn)
df['new_column'] = df['column_1'] + df['column_2']$ df['new_column'] = df.apply(my_function, axis = 1)$ df.to_csv['my_data.csv']$
from pandas.tools.plotting import lag_plot$ dataSeries = pd.Series(Q3['Average Temperature'])
dc.head(5)
from nltk.corpus import stopwords$ import string$ punc = list(string.punctuation)$
from nltk.corpus import stopwords$ portuguese_stop_words = stopwords.words('portuguese')
kmeans_model = KMeans(n_clusters=2, init='k-means++', random_state=42).fit(crosstab_transformed)$ c_pred = kmeans_model.predict(crosstab_transformed)
conn.commit()
url = 'https://space-facts.com/mars/'
tzs = tweets_df['userTimezone'].value_counts()[:10]$ print(tzs)$
databreach_2017 = pd.read_csv('databreach_2017.csv')
df.corr().round(2)$
result_df = pd.DataFrame({'profile_id':profile_ids})$ result_df['num_events'] = result_list
tweets_original['full_text'] = tweets_original['full_text'].str.decode('utf-8')$ tweets_original['created_at'] = tweets_original['created_at'].str.decode('utf-8')
injury_df.iloc[:40,:]
df.to_json('twitter_data_5aug.json')
twitter_df_us=twitter_df[twitter_df.location=='United States']$ plt.plot(twitter_df_us['created_at_time'], twitter_df_us['retweet_count'], 'ro')$ plt.show()$
df.loc[df['field1']>27,['created_at','field1']]
validation.analysis(observation_data, Jarvis_resistance_simulation_0_5)
df.head()
pd.date_range('3/7/2012 12:56:31', periods=6, normalize=True)
data_df.desc[17]
for title, artist in unique_title_artist[current_len:min(current_len+batch_size, len_unique_title_artist)]:$     youtubeurl = urllib.parse.quote_plus(YOUTUBE_URL_TEMPLATE.format(gc.search(title +' '+artist)), safe='/:?=')$     youtube_urls[str((title, artist))] = youtubeurl
data = pd.read_csv("BreastCancer.csv")$ data.head()
payload = "elec,id=500 value=24 "#+str(pd.to_datetime('2018-03-05T19:29:00.000Z\n').value // 10 ** 9)$ r = requests.post(url, params=params, data=payload)
df_ec2 = df_cols[df_cols.ProductName == 'Amazon Elastic Compute Cloud'] # narrow down to EC2 charges$ df_ec2_instance = df_ec2[df_ec2.UsageType.str.contains('BoxUsage:')] #narrow down to instance charges$ df_tte = df_ec2_instance[df_ec2_instance['LinkedAccountName'] == target_account]$
grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)$ grid.map(plt.hist, 'Age', alpha=.5, bins=20)$ grid.add_legend();
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]$
import re$ regex_1 = re.compile('\w[A-Z]+')
plate_appearances = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).first().reset_index()
dfClientes.iloc[10:20, :]
for title in soup.find_all('a'):$     print(title.text)$
day_change = (x[2] - x[3] for x in data_table if x[2] is not None and x[3] is not None)$ print('Largest Day Change: {:.2f}'.format(max(day_change)))
pickle_in = open("neuralNet.pickle","rb")$ ANNModel = pickle.load(pickle_in)
s_pacific = s_eastern.tz_convert("US/Pacific")$ s_pacific
vocab = {v: k for k, v in vectorizer.vocabulary_.items()}$ vocab
fcc_nn.head()
fp7_proj.shape, fp7_part.shape, fp7.shape
tweet_df_polarity = tweet_df.groupby(["Source"]).mean()["Vader_score"]$ pd.DataFrame(tweet_df_polarity)
random.sample(labels.items(), 25)
plate_appearances['pitcher_throws_left'] = np.where(plate_appearances['p_throws'] == 'L', 1, 0)$ plate_appearances['left_handed_batter'] = np.where(plate_appearances['stand'] == 'L', 1, 0)
eta = therm_fiss_rate / fuel_therm_abs_rate$ eta.get_pandas_dataframe()
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'$ 
nltk.help.upenn_tagset()
DataSet.head(2)
data.loc[:, ['TMAX']].head()
state_grid_new = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(21, 21))$ q_agent_new = QLearningAgent(env, state_grid_new)$ q_agent_new.scores = []  # initialize a list to store scores for this agent
df.printSchema()
print(df.shape)$ df = df[pd.notnull(df['is_shift'])]$ print(df.shape)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ request = requests.get(url)
ADNI_merge = pd.read_csv('ADNIMERGE.csv')$ ADNI_merge.head()
visits = sql_query('select * from visits')$ visits.head(3)
df.loc['20180101':'20180103', ['A','B']]
tips.head(3) # default 5$
df_result = df[df.State.isin(['Failed', 'Successful'])]$ pd.crosstab(df_result.launched_year, df_result.State)
api_key = '25bc90a1196e6f153eece0bc0b0fc9eb'$ units = 'Imperial'$ url = 'http://api.openweathermap.org/data/2.5/weather'
tobs_df.hist(column='Temperature', bins=12)$ plt.ylabel("Frequency",fontsize=12)$
from textblob import TextBlob$ data_df['sent_pola'] = data_df.apply(lambda x: TextBlob(x['clean_desc']).sentiment.polarity, axis=1)$ data_df['sent_subj'] = data_df.apply(lambda x: TextBlob(x['clean_desc']).sentiment.subjectivity, axis=1)
s = pd.Series([1, 2, 3, 4, 5, 6])$ s
print(f"Number of stations is {sq.station_count()}")
unique_Taskers = len(sample['tasker_id'].value_counts())$ unique_Taskers
df.dtypes
spotify_df["Streams per 1 million"] = (spotify_df["Streams"]/spotify_df["Population"]*1000000).round(2)
largest_collection_size = df_meta['collection_size_bytes'].max()$ largest_collection_size
step_counts.fillna(0., inplace=True)
buckets_to_df(contributors.fetch_aggregation_results()['aggregations']['0']['buckets']).head()
crimes.PRIMARY_DESCRIPTION.head()
RIDs_DXSUM = list(diagnosis_DXSUM_PDXCONV_ADNIALL['RID'].unique())$ RIDs_ADSXLIST = list(diagnosis_ADSXLIST['RID'].unique())$ RIDs_BLCHANGE = list(diagnosis_BLCHANGE['RID'].unique())
QLESQ = QLESQ[(QLESQ["level"]=="Level 1") & (QLESQ["days_baseline"] <= 7)]$ QLESQ.shape
graf=df.copy()
age_up70.shape
true_file = pd.read_csv(filepath_or_buffer=os.path.join(edfDir, 'pt1sz2_eeg.csv'), header=None)$ test_file = pd.read_csv(filepath_or_buffer=outputData, header=None)
df1 = df.copy()
fires.dtypes
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ data.head(10)
r.json() 
users = pd.read_csv('LaManada_new/tbluserinfo.csv',sep=SEP)$ users.shape
ts_filter = ts_mean[ticker][ts_mean[euphoria].shift(1)<0.05]
full_image_elem = browser.find_by_id('full_image')
df2[['ab_page', 'intercept']] = pd.get_dummies(df2['group'])$ df2['intercept'] = 1$ df2.head()
DC_features = pd.read_csv('Historical_Work_Sunlight_Data.csv',index_col=0,parse_dates=True)
(gamma_chart - gamma_chart.shift(1))['Risk'].plot()
s3 = pd.Series(np.arange(12, 14), index=[1, 2])$ pd.DataFrame({'c1': s1, 'c2': s2, 'c3': s3})
data_df.groupby('topic')['ticket_id'].nunique()
new_array = np.concatenate((training_active_listing_dummy,training_pending_ratio),axis=1)
movies['year']=movies['title'].str.extract('(\(\d\d\d\d\)$)',expand=True)$ movies['year'] = movies['year'].str.replace('(', '')$ movies['year'] = movies['year'].str.replace(')', '')
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$
Base.prepare(engine, reflect=True)
grouped_data = dj_df.groupby(by='company_ticker')$ cmp_to_scaler = {}$ norm_dj_df = pd.DataFrame(columns=dj_df.columns) # Dataframe with quarter_start, company_ticker, normalized-revenue information.
census_zip = df_station.merge(wealthy, left_on=['zipcode'], right_on=['zipcode'],  how='left')$ nan_rows = census_zip[census_zip['betw150kand200k'].isnull()]$ census_zip.dropna(inplace = True)
df1['Hour'] = pd.to_datetime(df1['Date'], format='%H:%M').dt.hour # to create a new column with the hour information$ df1.head()
feedbacks_stress.loc[feedbacks_stress['versao'] == 2, ['incomodo', 'interesse1', 'interesse2'] ]*=2$
y_hat = model.predict(X_test)
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline',$                         xls_dir='scen2xls', file_name='data.xlsx', verbose=False)
tweets = pd.read_csv('tweets_mentioning_candidates.csv')$ tweets['set'] = 'test'$ tweets['polarity_value'] = np.NaN
df_ec2[df_ec2['AvailabilityZone'].isnull()]['UsageType'].unique()
airline_training_set = nltk.classify.util.apply_features(extract_features, airline_tweets)$ NBClassifier = nltk.NaiveBayesClassifier.train(airline_training_set)
df2['intercept'] = 1$ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])$ df2.head()
flight6 = spark.read.parquet("/home/ubuntu/parquet/flight6.parquet")$
forcast_set=clf.predict(X_lately)
predictions = lrModel.transform(testData)$ predictions.select("prediction", "label", "features").show(5)$ predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred7.csv')
for col in temp_columns:$     print(col)$     dat.loc[:,col]=dat[col].interpolate(method='linear', limit=3)
my_df_free1.iloc[100:110]
df_search_cate_dummies[df_search_cate_dummies['user_id']== 18].index$
iris.groupby('Species')['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'].describe()
stories = pd.concat([stories.drop(['submitter_user'], axis=1), user_df], axis=1)$ stories.head()
dat=pd.read_csv(pth)$ print(pth)
c = watershed.unary_union.centroid # GeoPandas heavy lifting$ m = folium.Map(location=[c.y, c.x], tiles='CartoDB positron', zoom_start=12)
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
c.loc["c":"e"]
holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00102032') ]
station_count = session.query(Station).count()$ station_count
df_date = df_date['unique_date'].apply(lambda x: x.strftime("%Y-%m-%d %H:%M:%S"))$
pd.Timestamp('2014-12-15')
tags = tags.sum(level=0)$ tags.head()
df_20180113_filtered.to_csv('data/hawaii_missile_crisis-20180113.csv')
reviews.info()$ reviews=pd.read_csv("ign.csv",index_col=['Unnamed: 0','score_phrase'])$ reviews.head()
faa_data_substantial_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "S"]$ print(faa_data_substantial_damage_pandas.shape)$ faa_data_substantial_damage_pandas.head()
data_df.info()
store_items.interpolate(method = 'linear', axis = 0)
lgb.plot_importance(lgb1, max_num_features=30)$ plt.show()$
for inst in idx_set:$     with open('../notebooks/subsample_idx_{}.json'.format(inst), 'w') as fd:$         json.dump(list(idx_set[inst]), fd, indent=2)$
text = data_df['clean_desc'].apply(cleaning)$ text_list = [i.split() for i in text]$ len(text_list)
churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]$ churn_df['churn'] = churn_df['churn'].astype('int')$ churn_df.head()$
estimator.predict_proba(X1)
chinese_vessels_iccat = pd.read_csv('iccat_china_active_13mar2017.txt', sep='\t', encoding='utf-8')
df = pd.read_csv('estimating_quartiles.csv')
max_ch_ol1 = max(abs(v.close-next(islice(o_data.values(), i+1, i+2)).close) for i, v in enumerate(o_data.values()) if i < len(o_data)-1)$ print('A one liner using islice: {:.2f}'.format(max_ch_ol1))
temp_df = pd.DataFrame(data = us_temp)$ temp_df.columns = ts.dt.date[:843]
local.get_dataset(post_process_info["DatasetId"])
validation.analysis(observation_data, simple_resistance_simulation_0_25)
print(df.shape)$ print(df.describe())
ab_df2.query('group == "treatment"')['converted'].mean()
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
cityID = '7c01d867b8e8c494'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Garland.append(tweet) 
train_df = replace(combats)$ print(train_df.head(5))
pv1 = pd.pivot_table(df_cod, values="Age at death", index="Death year")$ pv1
fine_xs = xs_library[fuel_cell.id]['transport']$ condensed_xs = fine_xs.get_condensed_xs(coarse_groups)
free_data.groupby('age_cat')['educ'].mean()
files = os.listdir(os.getenv('PUIDATA') + '/archives')
cwd = os.getcwd()$ cwd
voters = pd.read_csv('data_raw_NOGIT/voters_district3.txt', sep='\t')$ households = pd.read_csv('data_raw_NOGIT/households_district3.txt', sep='\t')$ households_with_count = pd.read_csv('data_raw_NOGIT/AK_hhld_withVoterCounts.txt', sep='\t')
donors_c.iloc[2097169, :]$
data_issues_csv.head()
scraped_batch6_top['Date'] = scraped_batch6_top['Date'].str.split('/')
avg_test_pred2 = np.stack(test_pred2).mean(axis=0)$ print ("type(avg_test_pred2):", type(avg_test_pred2), avg_test_pred2.shape)$ print(avg_test_pred2[0:10, :])
df.iloc[99,3]
tweet_df = pd.DataFrame(tweet_data)$
datesStr=dates.strftime('%Y-%m-%d')
df = pd.read_excel("mails taggen.xlsx")
from sklearn.model_selection import GridSearchCV$ param_grid = {'learning_rate': [0.05,0.1],'num_leaves': [40,60,80]}
year_prcp = session.query(Measurements.date, Measurements.prcp).order_by(Measurements.date).filter(Measurements.date > year_ago)$
df1 = df1[df1['Title'].str.contains(blacklist) == False]$ df1.shape
data['Age'].max()
j = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&start_date=2014-01-01&end_date=2014-01-02&api_key=' + API_KEY)$
output= "Create view ViewDemo as select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc;"$ cursor.execute(output)$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
with pd.option_context('display.max_colwidth', 130):$     print(news_titles_sr)
data_2018.to_csv('/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/minimum temperature/extracted_data/tmin.2018.csv')
df.text.str.extractall(r'(MAKE AMERICA GREAT AGAIN)|(MAGA)').index.size
yearmonthcsv=yearmonth_sumsizecsv.select(yearmonth_sumsizecsv['yearmonth'],(yearmonth_sumsizecsv['sum(size_in_KB)']/1000).alias("total_size_in_MB"))
intervention_train.reset_index(inplace=True)$ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
tfav.plot(figsize=(16,4), label="Likes", legend=True);$ tret.plot(figsize=(16,4), label="Retweets", legend=True)
html_page = browser.html$ JPL_soup = bs(html_page, "lxml")
results = soup.find_all('li', class_="content_title")
train_df = effectiveness(train_df)$ print(train_df.head(5))
last_12_precip_df = pd.DataFrame(data=last_12_precip)$ last_12_precip_by_date_df  = last_12_precip_df.set_index("date")$ last_12_precip_by_date_df.head(2500)
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8") 
df['device_class'] = df.device_type.apply(lambda d: classify_device_type(d))$ df.drop(['device_family', 'device_id', 'device_model', 'device_type'], axis=1, inplace=True)
free_data[free_data['educ']>5].groupby('age_cat').describe()
ans = pd.pivot_table(df, values='D', index=['A','B'], columns=['C'])$ ans
df_kosdaq = pd.DataFrame({'id' : kosdaq_id_ls, 'name' : kosdaq_name_ls, 'market_type' : 2}, columns = ['id', 'name','market_type', 'quant', 'market_sum', 'property_total', 'debt_total', 'listed_stock_cnt', 'pbr', 'face_value',])
weather_mean.loc['CHARLOTTETOWN', 'Wind Spd (km/h)']
from zipline.pipeline import Pipeline$ def make_pipeline():$     return Pipeline()
fitness_tests = sql_query("select * from fitness_tests")$ fitness_tests.head(3)
trains_fe2_x= trains_fe1.drop(['days_since_prior_order','add_to_cart_order','reordered','eval_set'], axis=1)$ trains_fe2_x.head() #12 features as the predictor variables
merge_table_df = pd.merge(spotify_df, numberOneUnique_df, on=['URL','Region'], how='left')
stock_return = stocks.apply(lambda x: x / x[0])$ stock_return.head()
w_train, w_test=train_test_split(df3, test_size=.33, random_state=42)
tips.columns
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'})$ station_count = station[['Count']]$ station_count 
y_pred = model.predict(x_test)
temp_df['reorder_interval_group'] = temp_df['reorder_interval_group'].astype(float)
turnstiles = df.groupby(['STATION','C/A','UNIT','SCP'])$ print('There are {} unique turnstiles.'.format(len(turnstiles)))$
pd.Timestamp("now")
Google_stock.tail()
df = pd.read_csv('./ab_data.csv')$ df.head()
geovol = fe.std(ss)$ geovol
df = turnstile_df.reset_index()$ df.columns = [col.strip() for col in df.columns]$ df.sample(5)
iso_join = iso_join.dissolve(by=['time', 'time_2']).reset_index()$
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)$
start_date = "2016-03-24"$ end_date = "2016-04-09"$ calc_temps(start_date,  end_date).mean(axis = 1)
resource_id = hs.createHydroShareResource(title=title, content_files=files, keywords=keywords, abstract=abstract, resource_type='genericresource', public=False)
df_concensus = pd.read_csv( pwd+'/consensus_shift_history.052317.csv') #consensus?
cityID = '60e2c37980197297'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Paul.append(tweet) 
sum(contractor.address1.isnull())
random.sample(words.items(), 10)
print()$ print(df.groupby('Class').size())
engine = create_engine('mysql+mysqldb://spmb:appoloodatavag@cw-spmb.ct4csmrmjrt7.us-west-2.rds.amazonaws.com/spmb_101')$
plt.axis('equal')$ plt.plot(  np.nancumsum(tab.delta_long), np.nancumsum(tab.delta_lat))$ plt.show()$
primitives = ft.list_primitives()$ print(primitives.shape)$ primitives[primitives['type'] == 'aggregation'].head(10)
df_amznnews_clsfd_2tick = df_amznnews_clsfd[['publish_time','textblob_sent', 'vs_compound']]$ df_amznnews_clsfd_2tick.head()
affair_yrs_married = pd.crosstab(data.yrs_married, data.affair.astype(bool))$ affair_yrs_married
pattern = re.compile('AA')$ print(pattern.match('AAbc'))$ print(pattern.match('bcAA'))
ranking = pd.read_csv('datasets/fifa_rankings.csv') # Obtained from https://us.soccerway.com/teams/rankings/fifa/?ICID=TN_03_05_01$ fixtures = pd.read_csv('datasets/fixtures.csv') # Obtained from https://fixturedownload.com/results/fifa-world-cup-2018$ pred_set = []$
pulledTweets_df['emoji_enc_text'] = pulledTweets_df['Processed_tweet'].apply(declump_emojis_in_text)$ pulledTweets_df['emoji_enc_text'] = pulledTweets_df['emoji_enc_text'].apply(encode_emojis)$ pulledTweets_df['emojis'] = pulledTweets_df['Processed_tweet'].apply(extract_emojis)
X = fires.loc[:, 'glon':'rhum_perc_lag12']$ X = X.drop(['date'], axis=1)$ y = fires['fire']
nb_pipe_2.fit(X_train, y_train)$ nb_pipe_2.score(X_test, y_test)
not_in_oz = stops.loc[~mask & (stops['operator'] == 'bac')].head(5)$ not_in_oz
ground_alt = merged_data['rtk_alt'].mode()[0]$ ground_alt
crime_geo_table = pa.Table.from_pandas(crime_geo_df)$ crime_geo_table
df.tail(5)
rf.score(X_train_all, y_train)
URL = "http://www.reddit.com/hot.json?limit=100"$ res = requests.get(URL, headers={'User-agent': 'KH'})$ data = res.json()
dfEtiquetas.dropna(subset=["created_time"], inplace=True)
new_df = df.dropna()$ new_df
log_sgm_bgp_100yr_run(L0 = 1000, E0 = 1, Delta_n = 0.02)
df2 = df2.drop_duplicates(subset=['user_id'], keep='first')
df["grade"].cat.categories = ["very good", "good", "very bad"]$ df["grade"]
tweets_df = tweets_df.sort_values("Date", ascending=False)$ tweets_df
df.A
le_data = le_data_all.reset_index().pivot(index='country',columns='year')$ le_data.iloc[:,0:3]
tweet_data['text_tokenized'] = tweet_data['text'].apply(lambda x: word_tokenize(x.lower()))$ tweet_data['hash_tags'] = tweet_data['text'].apply(lambda x: hash_tag(x))$ tweet_data['@_tags'] = tweet_data['text'].apply(lambda x: at_tag(x))
appmag_lim = 21.0$
userProfile = userGenreTable.transpose().dot(inputMovies['rating'])$ userProfile
year_info_df.describe()
time_chart = vincent.Line(per_minute)$ time_chart.axis_titles(x='Time', y='Hashtag frequencies')$ time_chart.display()
mlp_df = pd.read_csv(mlp_fp, index_col='Date', parse_dates=True)$ mlp_df.head()
df.sort_values (by='J')
os.chdir(root_dir + "data/")$ df_fda_drugs_reported.to_csv("filtered_fda_drug_reports.csv", index=False)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)
dfrecent = dfdaycounts[dfdaycounts['created_date']> pd.to_datetime('2012-12-31')]
globalCityRequest = requests.get(globalCity_pdf, stream=True)$ print(globalCityRequest.text[:1000])
df.xs(key='x', level=2)
lossprob = fe.bs.smallsample_loss(2560, poparr, yearly=256, repeat=500, level=0.90, inprice=1.0)$
news_df = pd.DataFrame(news_dict) 
cleanedData = allData$ cleanedData[cleanedData['text'].str.contains("\&amp;")].shape[0]
instance.updateParameters(cannull,ranges,tests)$ instance.testassumptions()
percent_success = round(kickstarters_2017["state"].value_counts() / len(kickstarters_2017["state"]) * 100,2)$ print("State Percent: ")$ print(percent_success)
train_b, valid_b, test_b = df_b.split_frame([0.7, 0.15], seed=1234)$ valid_b.summary()
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
df.drop(remove_cols, axis=1, inplace=True)
tmax_day_2018.tmax[100].plot();
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2014-01-01&end_date=2014-01-02')$
log_user2 = df_log[df_log['user_id'].isin([df_test_user_2['user_id']])]$ print(log_user2)
for metric in a[:]:$     payload = "elec,id="+str(metric[0])+" value="+str(metric[2])+" "+str(pd.to_datetime(metric[3]).value // 10 ** 9)+"\n"$     r = requests.post(url, params=params, data=payload)
print 'RF: %s' % rf.score(X_test, y_test)$ print 'KNN: %s' % knn.score(X_test, y_test)
geocode_endpoint = 'https://maps.googleapis.com/maps/api/geocode/json?'
html = requests.get(marsweath_url)$ soup = bs(html.text, 'html.parser')
for tweet in tweepy.Cursor(api.search, q='#GES2017',$                           lang='en',since='2017-11-28',max='2017-11-30').items(200):$     csv_writter.writerow([tweet.text.encode("utf-8")])
df['index1'] = df.index
march_2016 = pd.Period('2016-03', freq='M')$ print march_2016.start_time$ print march_2016.end_time
matches = re.findall('\d+', $     'the recipe calls for 10 strawberries and 1 banana')$ print(matches)$
print(psy_hx.shape)$ psy_hx = psy_hx[psy_hx["subjectkey"].isin(incl_Ss)]$ psy_hx.shape$
Temperature_DF=pd.DataFrame(results)$ Temperature_DF.head()
targettraffic['weekday'] = targettraffic['DATE_TIME'].apply(lambda x:x.weekday())$ targettraffic['weekdayname'] = targettraffic['DATE_TIME'].apply(lambda x:x.weekday_name)
temp_df.groupby('reorder_interval_group')['Order_Qty'].mean()
inspector = inspect(engine)$ inspector.get_table_names()
daterange = pd.date_range(scn_genesis[0],datetime.today(),freq='1M')
import pandas as pd$ PATH='light_sensor-20180705-1304.csv'$ df=pd.read_csv(PATH)
importances=model_rf_14_b.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
REDASH_AUTH_URL = 'https://nanit-bi:pcjxg72f3yat@redash.nanit.com/api/queries/154/results.json?api_key=LcuoHqjZLvxaSPDrhv5VMhcrJUyPVb88RJR69REq'$
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\mydata.json"$ df = pd.read_json(path)$ df.head(5)
regr.score(X, y)  # when we fit all of the data points
store_items = pd.DataFrame(items2, index = ['store 1', 'store 2'])$ store_items
dul_isbns = dul['ISBN RegEx']$ dul_isbns.size
r = requests.get(url)$ r.json()
readme = processing_test.README()$ readme = open(readme, "r")$ print(readme.read())
ts.shift(-2)
new_df = df.fillna(method = 'ffill')$ new_df
two_day_sample.reset_index(inplace=True)
my_tag={'name':'img','title':'Sunset boulevard',$        'src':'sunset.jpg','cls':'framed'}$ tag(**my_tag)
ts1 = pd.Series(np.arange(len(df)), df_Created)$ df_Month = ts1.resample('M').count()$ df_Month.head(9)
tweet_df.count()
data.mean()
USER_PLANS_df['start_date'] = pd.to_datetime(USER_PLANS_df['scns_created'].apply(lambda x:x[np.argmin(x)])).dt.strftime('%Y-%m')
n_new = df2[df2['group'] == 'treatment'].shape[0]$ n_new
Measurements = Base.classes.measurements$ Stations = Base.classes.stations
print(open(os.path.join(PROJ_ROOT, 'requirements.txt')).read())
automl.fit(X_train, y_train, dataset_name='psy_prepro')
us_grid_id = (pd.read_csv('../data/model_data/us_grid_id.csv', index_col = 0)$               .groupby('grid_id').count().reset_index())$
table_rows = driver.find_elements_by_tag_name("tbody")[18].find_elements_by_tag_name("tr")$
scores['test_accuracy'].mean()$
import os$ os.environ["instagram_client_secret"] = "91664a8e599e42d2a6a824de6ea456ec"$
test.fillna(0, axis=1, inplace=True)$ train.fillna(0, axis=1, inplace=True)
df_hi_temps.head()
top_songs['Day'] = top_songs['Date'].dt.day
post_id = posts.insert_one(post) #insere dados$ post_id$
forecast_column = 'Adj. Close'$ df.fillna(value=-99999, inplace=True)$ forecast_out = int(math.ceil(len(df)*0.01))
data_AFX_X.describe()['Traded Volume']
from nltk.tokenize import sent_tokenize$ sentence = sent_tokenize(text)$ print(sentence)
out = query.get_dataset(db, id=ds_info["DatasetId"][0])
type2017 = type2017.dropna() 
%time tsvd = tsvd.fit(train_4)
Genres=",".join(Genres).join(("",""))
print(datetime.datetime.strftime(d2, "%Y-%m-%d"))$ type(datetime.datetime.strftime(d2, "%d-%b-%Y"))
transactions = sql_context.read.json('data/transactions.ndjson')
for tweet in df_tweets:$     print(TextBlob(tweet).sentiment)$     print(TextBlob(tweet).sentiment.subjectivity)$
data.head()
mars_df = pd.read_html(mars_facts_url)$ mars_facts_df = pd.DataFrame(mars_df[0])
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall()
tweets.sort_values(by="retweets", ascending=False).head()
with open('100UsersResults.data', 'wb') as filehandle:  $     pickle.dump(results, filehandle)
to_plot = customer_emails[['Email', 'Days Between Int']]$ plt.title("Actual number of days between purchases")$ sb.distplot(to_plot['Days Between Int'].dropna())$
x = np.array([0,1,2,3,4,5,6,min])$ x.dtype # it's a C array of python objects!$
x_train, x_test, y_train, y_test = train_test_split(bow_df, amazon_review['Sentiment'], shuffle= True, test_size=0.2)
cityID = '1661ada9b2b18024'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Wichita.append(tweet) 
df.LinkedAccountId.fillna(value=1,inplace=True)
impact_effort = pd.read_csv('impact_effort.csv')
tweet_df = tweet_df.sort_values(['source', 'created_at'], ascending=False).reset_index()$ del tweet_df['index']$ tweet_df.head()
train, valid, test = covtype_df.split_frame([0.6, 0.2], seed=1234)$ covtype_X = covtype_df.col_names[:-1]     #last column is Cover_Type, our desired response variable $ covtype_y = covtype_df.col_names[-1]    
plt.scatter(tfav,tret, marker='.', color='red')
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))#considering 95 percent confidance interval
test[['clean_text','user_id','predict']][test['user_id']==1497996114].shape[0]
user_corrs = df.groupby('user_id')[['user_answer', 'question_answer']].corr()$ user_corrs = user_corrs.iloc[0::2, -1].reset_index(level=[1])['question_answer']$
all_turnstiles.head()
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S_distributedTopmodel.executable = excutable +'/summa.exe'
price_data = json_normalize(data, [['dataset', 'data']])$ heading = json_normalize(data_test, [['dataset', 'column_names']]).transpose()
df.dealowner.unique()
data.loc[:, 'TMAX'].head()
df = pd.read_sql('SELECT * from room', con=conn_b)$ df.head(10) # show 10 rows only
df_data = pd.read_csv(CSV_FILE, dtype=str)$ print '%d rows' % len(df_data)$ df_data.head()
df_sp_500 = df_sp_500.withColumn('IDX', func.lit('SP_500'))
grouped = df_cod2.groupby(["Death year", "Cause of death"])$ grouped.size()
df3 = df3.drop(['text', 'full_text'], axis=1)$ df3['hashtag'] = df3['tweet'].str.findall(r'#.*?\s')  # .findall returns list which causes issues later$ df3.reindex(columns = ['created_at','location','time_zone','tweet', 'hashtag'])
x.dropna()
betas_argmax = np.zeros(shape=mcmc_iters)$ betas_argmax = beta_dist.argmax(1)
Genres=movie_rating['genres'].values.tolist()
percipitation_2017_df.plot()$ plt.show()
dfn = df.loc[df.period].copy()$ dfo = df.loc[~df.period].copy()$ dfn.shape, dfo.shape
df = pd.read_sql("SELECT * FROM Sections ", con=engine)$ df.head(3)
data['Traded Volume'].mean()
df.describe()
results=pd.read_csv("results.csv")$ results.tail()$
df = df[df.year >2005]
from sklearn.naive_bayes import MultinomialNB$ clf = MultinomialNB().fit(X_train_tfidf, tweet_train_target)
blahblah = 'This string is stored in the variable on the left.'$ blahblah
model.fit([sources], targets, epochs=10, batch_size=512, validation_split=0.1)$
import tensorflow as tf$ sess=tf.Session()    $ saver = tf.train.import_meta_graph('/tmp/testing/stock_prediction_12_21/model.ckpt-1000.meta')$
print(len(alltrains))$ alltrains.to_csv('data/JY-HKI-dec17-mar18.csv')
y = list(train_1m_ag.is_attributed)$ X = train_1m_ag.drop(['is_attributed'],axis=1)
irisRDD = SpContext.textFile("iris.csv")$ print (irisRDD.count())
ab_df.user_id.nunique()
sns.distplot(virginica)$ sns.distplot(versicolor)
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
tsprior = pd.Timestamp('1/1/2016')$ tsprior2 = pd.Timestamp ('1/1/2017')$ tsprior3 = pd.Timestamp('1/1/2018')
small_ratings_file = os.path.join(dataset, 'ml-latest-small', 'ratings.csv')$ print('small_ratings_file: '+ small_ratings_file)$ small_ratings_raw_data , small_ratings_raw_data_header = read_file(small_ratings_file)
df_students1 = df_students.rename(columns={'school':'school_name'})$ df_students1.head()
plt.subplot(2,2,1)$ plt.subplot(2,2,2)$ plt.show()
df_schools.shape
filepath = os.path.join('input', 'input_plant-list_NO.csv')$ data_NO = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_NO.head()
df = df[df.launched_year != 1970]
Google_stock.corr()
!wget https://raw.githubusercontent.com/jacubero/ColabEnv/master/awk_netstat.sh -O awk_netstat.sh$ !chmod +x awk_netstat.sh$ !./awk_netstat.sh
response_count = requests.get(SHOPIFY_API_URL+'/orders/count.json',params={'status':'any'}).json()['count']
result_final.summary2()
people_with_one_or_zero_collab['authorId'].nunique()
plt2 = results["diff"].hist(range=[-0.5, .5], density=True, cumulative=True, figsize=(8, 4))
weather1.head()
jeff.withdraw(100.0)   # Instruction to withdraw$ jeff.balance           # Shows 900.0
Base.classes.keys()
response=r.json()$ print(response)
df_5['weeks_between'] = df_5.submitted_at - df_5.application_created_at$ df_5['weeks_between'] = np.ceil(df_5.weeks_between.dt.days/7)
df = df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]
df_estimates_false.groupby('metric').count()$
type(t1.tweet_id.iloc[3])
msft = pd.read_csv("msft.csv", $                     dtype = { 'Volume' : np.float64})$ msft.dtypes
display(data.head())
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key="+API_KEY)
group_period = '7D'$ cc_df = cc_df.resample(group_period).mean()$ cc_df.head()
from sklearn.model_selection import train_test_split$ x_train, x_test, y_train, y_test = train_test_split($     data, targets, test_size=0.25, random_state=23)
number_of_commits = len(git_log)$ number_of_authors = len(git_log.query("author != ''").groupby('author'))$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
season07 = ALL[(ALL.index >= '2007-09-06') & (ALL.index <= '2008-02-03')] # This means every transaction between 9-6-07 and$
fb_data = graph.get_object(id='DonutologyKC/', fields=req_fields)$ type(fb_data)
offseason18 = ALL[(ALL.index > '2018-02-07')] # Finally, this says that everything after this past Superbowl is part of$
for c in ccc:$     vwg[c] = vwg[vwg.columns[vwg.columns.str.contains(c)==True]].sum(axis=1)
df.to_csv('GageData.csv',index=False)
df.dtypes
to_trade = np.abs(dfprediction['y_hat']) > 0.01$ to_trade.sum() # will result in 29 trades$ dfprediction[to_trade]
top_songs.rename(columns={'Region': 'Country'}, inplace=True)
mylist = session.query(measurement.date, measurement.prcp).filter(measurement.date.between('2016-08-23', '2017-08-23')).all() $ 
Base.classes.keys()
df.head()
cols = status_data.columns.tolist()$ cols = cols[:5] + cols[5:10]$ status_data = status_data[cols]
keep_RNPA_cols = ['follow up Telepsyche', 'Follow up', 'follow', 'Returning Patient']$ RNPA_existing = RNPA_existing[RNPA_existing['ReasonForVisitName'].isin(keep_RNPA_cols)]$ pd.value_counts(RNPA_existing['ReasonForVisitName'])
rain_df.describe()
calls_nocontact_2017.to_csv("311_calls_new.csv")
precip_data = session.query(Measurements).first()$ precip_data.__dict__
news_df = (news_df.groupby(['topic'])$               .filter(lambda x: len(x) >= 25))
Measurement = Base.classes.measurement$ Station = Base.classes.station
filename = processed_dir+'pulledTweetsCleanedLemmaEmEnc_df'$ gu.pickle_obj(filename,pulledTweets_df)
ride_df_rural = rural.groupby('city')$ city_df_rural = rural.set_index('city')
number_of_commits = len(git_log)$ number_of_authors = len(git_log.dropna()['author'].unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df['sales_type'] = df['sales_type'].str.replace(u'\xa0', u' ')$
response = requests.get(url)
vocab = vectorizer.get_feature_names()$ dist = np.sum(train_data_features, axis=0)
[[scores.loc[:,[row]].mean(),scores.loc[:,[row]].sum()]for row in scores]
model.load_weights('best.hdf5')
import pandas as pd$ cs = pd.Series(cleaned)$ by_tweeter['cleaned'] = cs
list(filter(lambda num:num%2==0,seq))    # seq = [1,2,3,4,5]  you can see above just below section 8.2
import numpy as np$ cs = np.log(df['Size'])$ cs = cs.reset_index()
summary_bystatus = df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9]).T$ summary_bystatus
!pip install -q xgboost==0.4a30$ import xgboost
rhum_us_full = rhum_nc.variables['rhum'][:, lat_li:lat_ui, lon_li:lon_ui]
t1.
import csv$ data = list(csv.reader(open("test_data//my_data.csv")))
subreddit = reddit.subreddit('Bitcoin')$ for submission in reddit.subreddit('all').hot(limit=25):$     print(submission.title)
data_df.desc[15]
auth = tweepy.OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)$ api = tweepy.API(auth)
from h2o.estimators.deeplearning import H2ODeepLearningEstimator$ help(h2o.model.model_base.ModelBase)
bins = [70, 72, 74, 76, 78, 80]$ temp_freq_dict={"Temperature":temp_list, "Frequency":frequency }$ temp_freq_df=pd.DataFrame(temp_freq_dict)
df.status_message.fillna('NA', inplace=True)$
new_page_converted.mean() - old_page_converted.mean()
workclass = pd.read_sql(q, connection)$ workclass.head(10)
total=extraD$ total.loc[:,'exposure_time']=total.apply(exposure_redshift, args=('normal',), axis=1)
top_10_authors = git_log.loc[:, 'author'].dropna().value_counts().head(10)$ top_10_authors
import tm_assignment_util as util$ myutilObj = util.util()$ Osha_AccidentCases = util.accidentCases_Osha
bucket.upload_dir('data/wx/tmy3/proc/', 'wx/tmy3/proc', clear_dest_dir=True)
free_data.sort_values(by="country")$ free_data.sort_values(by="educ")$ free_data.sort_values(by="age", inplace=True, ascending = False)
hired = data.loc[data['hired']==1].tasker_id.value_counts()$ hired[:5]
a = tweets.apply(lambda row: countOcc(row['tokens']), axis=1)$ sorted_x = sorted(occ.items(), key=operator.itemgetter(1), reverse=True)$
bucket.upload_dir('data/city-util/proc', 'city-util/proc', clear_dest_dir=True)
affair_age = pd.crosstab(data.age, data.affair.astype(bool))$ affair_age
df1 = df1.drop_duplicates(subset = 'Title', keep = False)$
emails_dataframe['address'].str.split("@")
x=[0,1,2,3,4,5]$ network_simulation[network_simulation.generations.isin([5])]$
df2 = df2.add_suffix(' Created')$ df4 = pd.merge(df,df2,how='left',left_on='Date Created',right_on='date Created')$
df_users = df_users.dropna(axis=0)$ print(df_users.shape)$ df_users.head(10)
corpus_Tesla = [dictionary.doc2bow(text) for text in sws_removed_all_tweets]
appointments = pd.read_csv('./data/AppointmentsSince2015.csv')
df.shape    
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_table(path, sep ='\s+', na_values=['.'], names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'class'])$ mydata.head(5)$
df_session_dummies = pd.get_dummies(df, columns=['action'])$ df_session_dummies.head()
plt.savefig(str(output_folder)+'NB05_1_FC_'+str(cyclone_name)+'_'+str(location_name))
hp = houseprint.Houseprint()$
bd.columns.name = "Data"$ bd
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
engine = create_engine("sqlite:///hawaii.sqlite")
df.head()
plt.bar(1,dict["avg"],color="orange",yerr=dict["max"]-dict["min"])$ plt.title("Trip Avg Temp")$ plt.ylabel("Temperature")
bad_iv.groupby(['Strike']).count()['Expiration']
pd.concat([msftAV, aaplA], join='inner')
plt.show()
news_df = guardian_data.copy()
prcp_df.count()
df.describe()
ts.shift(1,freq="B")
from tempfile import mkstemp$ fs, temp_path = mkstemp("gensim_temp")  $ model.save(temp_path)  
driver = webdriver.Chrome('/Users/daesikkim/Downloads/chromedriver', chrome_options=options) # chrome_options=options$ driver.implicitly_wait(3)
sgm_bgp_100yr_run(L0 = 1000, E0 = 1, Delta_n = 0.02)
new_messages = new_messages[new_messages.user_id == 42966]
contractor_merge = pd.merge(contractor_clean, state_lookup,$                             on=['state_id'], how='left')
t3.info()
c = R18df.rename({'Create_Date': 'Count-2018'}, axis = 'columns')
trump_originals = trump[trump.is_retweet == False]$ trump_originals.shape$
year_prcp = session.query(Measurements.date, Measurements.prcp).order_by(Measurements.date).filter(Measurements.date > year_ago)
from sqlalchemy import create_engine$ engine = create_engine('mysql+pymysql://root:kobi5555@0.0.0.0/proddb')
mgxs_lib.load_from_statepoint(sp)
feature_col = ibm_hr_final.columns$ feature_col
df_raw = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ df = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ print df_raw.head()
df.to_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext.csv", sep=',')
df_3_test = pd.read_csv('Medicare_Hospital_Spending_by_Claim.csv')$ df_4_test = df_3_test.drop(df_3_test.columns[[11, 12]], 1)$
ny_df=pd.DataFrame.from_dict(foursquare_data_dict['response'])
url_hemispheres = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars"$ browser.visit(url_hemispheres)
labeled_docs = LabeledLineSentence(candidate_data['messages'], candidate_data.index)
print(trump.favorite_count.describe())$ print("   ")$ print(trump.retweet_count.describe())
inspector = inspect(engine)$ inspector.get_table_names()
df = pd.read_csv("AAPL.csv",na_values = ["null"])$ df.dtypes
featured_image_url = 'https://www.jpl.nasa.gov/images/cassini/20170809/cassini20170809-16.jpg'
df.info()
len(df[df['text'].str.contains('appointment')])
test = tmax_day_2018.sel(lat=16.75, lon=81.25).to_dataframe()
from sqlalchemy.ext.automap import automap_base$ from sqlalchemy import create_engine
import matplotlib.pyplot as plt$ import seaborn as sns$ sns.set()
import pandas as pd$ dff = pd.DataFrame(g.featureMatrix)$ dff.to_csv("foo.csv")
X = df[[col for col in df.columns if col != 'bitcoinPrice_future_7']]$ y = df['bitcoinPrice_future_7']  
df_users =  pd.read_sql(SQL, db)$ print(df_users.head())$ print(df_users.tail())
timelog = timelog.drop(['Email', 'User', 'Amount ()', 'Client', 'Billable'], axis=1)
load2017 = load2017.dropna() 
mydf1.info()
created_date = [to_datetime(t).date() for t in tweets_df['created_at']]$ tweets_df['created_date'] = created_date$ tweets_df.head()
X = reddit_master[['age', 'subreddit']].copy(deep=True)$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
df.index
data_all = data.learner_id.value_counts()$ print(len(data_all[data_all > 100]))
secondary_temp_dat=dat[secondary_temp_columns].copy()$ secondary_temps_exist= not secondary_temp_dat.dropna(how='all').empty$
import seaborn as sns$ sns.heatmap(fpr_a)
df.isnull().sum()
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall()$
df_Tesla['topic_codes']=topic_codes_Tesla_tweets$ df_Tesla.head()
df_tte['ItemDescription'].unique()
even_counter.value
exiftool -csv -createdate -modifydate MVI_0011.mp4 > out.csv
import pandas as pd$ df = pd.DataFrame(query_op)$
print(df.iloc[0])
us_cities = pd.read_csv("us_cities_states_counties.csv", sep="|")$ us_cities.head()$
model.wv.most_similar("cantonese")
def tempF2C(x): return (x-32.0)*5.0/9.0$ def tempC2F(x): return (x*9.0/5.0)+32.0
input_nodes_DF = nodes_table('network/source_input/nodes.h5', 'inputNetwork')$ input_nodes_DF[1:5]
predictions = np.array([item['classes'] for item in classifier.predict(input_fn=test_input_fn)])$ predictions = [reverse_lookup[x] for x in predictions]
df['year'] = df.index.year
theta = np.minimum(1,np.maximum(0,np.random.normal(loc=0.75, scale=0.1, size=(10000,5))))
df = pd.DataFrame(pd.read_csv(filepath, header = None, index_col = False))
news_organizations_df['tweets'] = news_organizations_df.handle.map(get_df_for_user_tweets)$
contrast['title'] = contrast['title'].apply(cleaner)
ss_scaler.fit(active_list_pending_ratio_test)$ active_list_pending_ratio_test_transform = ss_scaler.transform(active_list_pending_ratio_test)$ active_list_pending_ratio_transform[0:5,:]
temp_df=temp_df.set_index('date')$ temp_df.head()
top_10_authors = git_log['author'].value_counts().head(10).to_frame()$ top_10_authors
tmp_df = tmp_ratings.pivot(index='userId', columns='movieId', values='rating')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
doc.is_parsed
temp=pd.read_json("./jsondata/condensed_2017.json")$ temp.head()
tsd = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with tsd.open('inpt-forecasting/Transplant Stemsoft data -040117 to 061518.xlsx') as tsd_f:$   tsd_df = pd.read_excel(tsd_f)
data.head(10)
tlen.plot(figsize=(16,4), color='r');
df2.user_id.nunique()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys() # Retrieve the names of the tables in the database
df_tte[df_tte['ReservedInstance'] == 'N']['InstanceType'].unique() 
pd.to_datetime(['2009/07/31', 'asd'], errors='raise')
y=dataframe1['Chaikin']$ plt.plot(y)$ plt.show()
plt.xlim(-0.25, len(x_axis))$ plt.ylim(-.3, .1)
plotdf.loc[plotdf.index[-1], 'forecast'] = hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'meanRemainingTweets'].iloc[0] + plotdf['currentCount'].max()$ plotdf.loc[plotdf.index[-1], 'forecastPlus'] = plotdf.loc[plotdf.index[-1], 'forecast'] + hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0]$ plotdf.loc[plotdf.index[-1], 'forecastMinus'] = plotdf.loc[plotdf.index[-1], 'forecast'] - hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0]
df.mean() # same as df.mean(0)
most = dfrecent.head(50)
df2 = df.dropna()$ df2.isnull().values.any()
findM = re.compile(r'wom[ae]n', re.IGNORECASE)$ for i in range(0, len(postsDF)):$ 	print(findM.findall(postsDF.iloc[i,0]))
query = 'CREATE TABLE new_york_new_york_points_int_ct10 AS (SELECT geoid, osm_id, latitude, longitude FROM beh_nyc_walkability, new_york_new_york_points WHERE ST_INTERSECTS(beh_nyc_walkability.the_geom, new_york_new_york_points.the_geom))'$
sentiments_df.to_csv("NewsMood.csv")
df.set_index('Parameter', inplace=True)$ df.head()
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).order_by(Measurements.date).all()
df.to_excel("../../data/stocks2.xlsx")
deut5 = dta.t[(dta.b==5) & (dta.c==5)]
cityID = 'cb74aaf709812e0f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tulsa.append(tweet) 
df.sort_index(axis=1,ascending=False)
payload = {'api_key': 'apikey'}$ r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json',params=payload)$
abc = Grouping_Year_DRG_discharges_payments.groupby(['year','drg3']).get_group((2015,871))$ abc.head()
sns.kdeplot(utility_patents_subset_df.number_of_claims, shade=True, color="purple")$ plt.show()
logit = sm.Logit(df3['converted'],df3[['intercept' ,'treatment']])
user = raw['submitter_user'].apply(pd.Series)$ user.head(3)
contractor_clean['last_updated'].head()$
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
top10_topics_2 = top10_topics_1.sort_values(by='count_event_per_topic', ascending=False)$ top10_topics_2.head(10)
AFX_dict = dict(r.json())
s = pendulum.datetime(2017, 11, 23, 0, 0, 0, tzinfo='US/Eastern') # Thanksgiving day$ e = pendulum.datetime(2017, 11, 25, 23, 59, 59, tzinfo='US/Eastern') # Small Businees Saturday$ Th_BF_Sa = tweets[(tweets['time_eastern'] >= s) & (tweets['time_eastern'] <= e)]
csvFile = open('ua.csv', 'a')$ csvWriter = csv.writer(csvFile)
df_pix4D = pd.read_json(basedirectory+projectname+'/'+Pix4D_filename, typ = 'series')$ images_json = df_pix4D['actual']['photos']
df2.plot()$ plt.show()
os.remove(fileName)
label_and_pred = lrmodel.transform(testData).select('label', 'prediction')$ label_and_pred.rdd.zipWithIndex().countByKey()
X2 = PCA(2).fit_transform(X)
df_schools.head()
gene_df[gene_df['length'] > 2e6].sort_values('length').iloc[::-1]
df.index = df.datetime
cbs = news_sentiment('@CBSNews')$ cbs['Date'] = pd.to_datetime(cbs['Date'])$ cbs.head()
test_data.text
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \$       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY$ req = requests.get(url)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(doc_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours Autocorrelation')
drivers = data_merge.groupby("type")$ drivers_total = drivers['driver_count'].sum()$ drivers_totals = [drivers_total['Urban'], drivers_total['Suburban'], drivers_total['Rural']]
cbs_df = constructDF("@CBS")$ display(constructDF("@CBS").head())
all_tables_df.iloc[:, 1]
if 1 == 1:$     ind_shed_word_dict = pd.read_pickle(config.IND_SHED_WORD_DICT_PKL)$     print(ind_shed_word_dict.values())
autoDataFile = open("auto-data-saved.csv","w")$ autoDataFile.write("\n".join(autoData.collect()))$ autoDataFile.close()
len([premieScn for premieScn in SCN_BDAY_qthis.scn_age if premieScn < 0])/SCN_BDAY_qthis.scn_age.count()
np.array(df[['Visitors','Bounce_Rate']])
crimes.columns = crimes.columns.str.strip()$ crimes.columns
%run '../forecasting/helpers.py'$ %run '../forecasting/main_functions.py'$ %run '../forecasting/ForecastModel.py'
sns.barplot(data=df.groupby('education').agg({'applicant_id':lambda x:len(set(x))}).reset_index(),$             x='education',y='applicant_id')
print(count_all.most_common(10))
options_frame.info()
endog = data_df.Count$ exog = data_df[['TempC','WindSpeed','Precip']]$
tips.index
columns = inspector.get_columns('stations')$ for c in columns:$     print(c['name'], c["type"])$
now = datetime.now()$ print(now)
conn.execute("SELECT * FROM posts WHERE id=?", (158743,)).fetchall()
cols_to_remove = ["multiverseid", "imageName", "border", "mciNumber", "foreignNames",$                   "originalText", "originalType", "source"]$ all_sets.cards = all_sets.cards.apply(lambda x: x.loc[:, list(set(x.columns) - set(cols_to_remove))])
QUIDS_wide.dropna(subset =["qstot_0"], axis =0, inplace=True)
heap = [h for h in heap if len(set([t.author_id for t in h.tweets if t.author_id in names])) == 1]
(obs_diff-np.mean(p_diffs))/np.std(p_diffs)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=uEEsBcH3CPhxJazrzGFz&start_date=2017-01-01&end_date=2017-12-31")$
annual_returns = returns.mean() * 250 $
x.drop([0, 1])
abc = abc.reset_index(level=[0,1])$ abc.columns
conn = 'mongodb://localhost:27017'$ client = pymongo.MongoClient(conn)
df['word_count'] = df['body'].apply(lambda x: len(str(x).split(" ")))$ df[['body','word_count']].head()
cityID = 'c0b8e8dc81930292'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baltimore.append(tweet) 
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots$  $ stock_data["CLOSE"].iloc[:].plot() # Plot the adjusted closing price of AAPL
from statsmodels.tsa.stattools import adfuller as ADF$
park_attendance_mean = games_df.groupby('park_id').mean()['attendance'].to_frame()$ park_attendance_mean.reset_index(inplace=True)
rf.score(X_train_total, y_train)
data = pd.read_csv("MSFT.csv", index_col='Date')$ data.index = pd.to_datetime(data.index)
df = df.swaplevel('Description', 'UPC EAN')$ df.head(3)
my_gempro.blast_seqs_to_pdb(all_genes=True, seq_ident_cutoff=.9, evalue=0.00001)$ my_gempro.df_pdb_blast.head(2)
raw_data = pd.read_csv('./activities_201802011009.csv')
X_trainset.shape$ y_trainset.shape$
df_cs = pd.read_csv("costco_all.csv", encoding="latin-1")
plt.savefig('Sentiment_Analysis_News_Organizations_Tweets.png')
struct_v1_0 = sp_df_test.schema$ emptyDF = spark.createDataFrame(sc.emptyRDD(), struct_v1_0)$ emptyDF.write.parquet(os.path.join(comb_fldr, "flight_v1_0.pq"))
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()$
df_release = df_release.dropna(axis=1, how='all') $ df_release.shape
data_FCInspevnt["Inspection_number"] = data_FCInspevnt.groupby("brkey")["in_modtime"].rank(ascending=False)$ data_FCInspevnt['Inspection_number'] = data_FCInspevnt['Inspection_number'].astype('int64')$ data_FCInspevnt.head(15)
breakdown[breakdown != 0].sort_values().plot($     kind='bar', title='Sean Hannity Number of Links per Topic'$ )
mean_newsorg_sentiment = grouped_newsorgs['Compound'].mean()$ mean_newsorg_sentiment.head()
%%writefile trainer/__init__.py$
df_date_precipitation.describe()
for col in list(df.columns) :$     k = sum(pd.isnull(df[col]))$     print(col, '{} nulls'.format(k))
df_geo_segments.crs = {'init': 'epsg:4326'}
kick_projects = pd.merge(kick_projects, ks_particpants, on = ['category', 'launched_year', 'launched_quarter','goal_cat_perc'], how = 'left')
stop_words_update.append('star')$ pipeline.set_params(posf__stop_words=stop_words_list, cv__stop_words=stop_words_update)
Obama_raw = pd.read_csv('data/BarackObama.csv', encoding='latin-1')$ Trump_raw = pd.read_csv('data/realDonaldTrump.csv', encoding='latin-1')
import os # To use command line like instructions$ if not os.path.exists(DirSaveOutput): os.makedirs(DirSaveOutput)
userActivity = userArtistDF.groupBy("userID").sum("playCount").collect()$ pd.DataFrame(userActivity[0:5], columns=['userID', 'playCount'])
com_eng_df = ghtorrent.community_engagement('rails', 'rails')$ com_eng_df = com_eng_df.set_index(['date'])
mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % ('lead-reader', 'lead-reader', 'ds025991-a0.mlab.com', '25991', 'tra-ingestion')$ con = MongoClient(mongo_uri)$ db = con['tra-ingestion']
zipincome = pd.DataFrame(jsonData)$ zipincome.head()
sns.set_style('whitegrid')$ sns.set_context("talk", font_scale=1.5, rc={"lines.linewidth": 2.5})
gdax_trans_btc.plot(kind='line',x='Timestamp',y='BTC_balance_GBP', grid=True);
text_noun = Osha_AccidentCases['Title_Summary_Case'].apply(myutilObj.tag_noun_func_words)
 $ mfacts = pd.read_html(marsfacts_url)$ mfacts
tobs_df=pd.DataFrame(date_tobs, columns=['date','tobs'])$ tobs_df.head()
df_en.to_csv('bitcoin_eng.csv') # this has all 4 bitcoins$
import csv$ data_with_header = list(csv.reader(open("test_data//askreddit_2015.csv", encoding="utf8")))$ data = data_with_header[1:len(data_with_header)]
df.loc[:, topics[0]:topics[-1]] = df.apply(lambda x: \$                                            pd.Series([t in x['tags'] for t in topics], index=topics), axis=1)
btc.corr()
datetime.now().time()
df1.tail(5)
url='https://dzone.com/articles/top-5-data-science-and-machine-learning-course-for'
json_data = r.json()$ json_data
B4JAN16['Contact_ID'].value_counts().sum()$
polynomial_features = PolynomialFeatures(2, include_bias=False)$ polynomial_features.fit(x_train['Pending Ratio'].values.reshape(-1,1))$ training_pending_ratio = polynomial_features.transform(x_train['Pending Ratio'].values.reshape(-1,1))
    l = s.split(' ')$     return date(to_int(year), month_to_int[l[0]], to_int(l[1]))
from gensim import models, similarities$ lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=100)
import test_package.print_hello_function_container$ import test_package.print_hello_class_container$ import test_package.print_hello_direct # note that  the paths should include root (i.e., package name)$
act_diff = df[df["group"]=='treatment']["converted"].mean() - df[df["group"]=='control']["converted"].mean()$ act_diff
df['id'] = df['id'].astype('category')$ df['sentiment'] = df['sentiment'].astype('category')$ df['text'] = df['text'].astype('string')
fed_reg_dataframe['token_text'] = fed_reg_dataframe['str_text'].apply(lambda x: word_tokenize(x.lower()))
irisDF1 = SpSession.read.csv("iris.csv",header=True)$ print (irisDF1.show())
print("Unique users:", len(df2.user_id.unique()))$ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
sum(contractor.city.isnull())
store_items = store_items.set_index('pants')$ store_items
print(actual_value_second_measure[actual_value_second_measure==2])$ holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00063550') ]
overall_df = pd.DataFrame(data= twitter_df.groupby('User')['Compound'].mean())$ overall_df
ave_sentiment_by_company_df = pd.DataFrame(ave_sentiment_by_company)$ ave_sentiment_by_company_df
s1.index
from IPython.display import display, Math, Latex
cust_vecs, item_vecs = implicit_weighted_ALS(train_mat, lambda_val=0.1, alpha = 40, iterations = 30, rank_size = 20)
sp500.ix[['MSFT', 'ZTS']]
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?\&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY)$ print(r.json())
hm_sub = grid_heatmap[grid_heatmap['pr_fire'] >= 0.25]$ fire_hm = hm_sub[['glat', 'glon', 'pr_fire']].values
movies.sample(5)
print("Percentage of positive Federer's tweets: {}%".format(len(pos_tweets_rf)*100/len(data_rf['Tweets'])))$ print("Percentage of neutral Federer's tweets: {}%".format(len(neu_tweets_rf)*100/len(data_rf['Tweets'])))$ print("Percentage de negative Federer's tweets: {}%".format(len(neg_tweets_rf)*100/len(data_rf['Tweets'])))
import sys$ sys.path.insert(0, '..')
cumret['monthly_cumret'].iloc[0:12].prod()
ymc=yearmonthcountcsv.coalesce(1)$ path1=input("Enter the path where you wanna save your csv")$ ymc.write.format('').save(path1,header = 'true')
us_prec = prec_fine.reshape(844,1534).T #.T is for transpose$ np.shape(us_prec)
rain_df.describe()
fig, ax = plt.subplots()$ df.groupby(['epoch', 'batch']).mean().plot(ax=ax)$ plt.show()
twelve_months = session.query(Measurements.date, Measurements.prcp).filter(Measurements.date > year_before)$ twelve_months_prcp = pd.read_sql_query(twelve_months.statement, engine, index_col = 'date')
!grep -A 15 "add_engineered(" taxifare/trainer/model.py
help(h2o.estimators.glm.H2OGeneralizedLinearEstimator)$ help(h2o.estimators.gbm.H2OGradientBoostingEstimator)
print(df['Borough'].value_counts(dropna=False))
session.query(Measurement.date).order_by(Measurement.date.desc()).first()$
result = cur.fetchall()$
df.head()
auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret)$ api = tweepy.API(auth)
all_text.head()
np.r_[np.random.random(5), np.random.random(5)]
import pandas as pd$ tweets = pd.read_csv('tweets.csv')
data[data.name == 'New EP/Music Development'].head()
val='2017-08-14-23' # string$ datetime.strptime(val, '%Y-%m-%d-%H') # date
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
fashion['created'] = pd.to_datetime(fashion.created)$ fashion.set_index('handle', inplace=True)$ fashion.dtypes
clf_y_score = rfc.predict_proba(X_test)[:, 1] #[:,1] is formatting the output$ clf_y_score
sns.boxplot(x='rating', y='text length', data=dataset)$
plot_compare_generator(['mention_count', 'hashtag_count'],"Comparing the two counts" ,DummyDataframe.index, DummyDataframe, intervalValue= 1, saveImage=True, fileName="compare")
td_by_date = niners.groupby('Date')['Touchdown'].sum()$ td_by_date;
from helper_code import mlplots as ml$ ml.confusion(y_test.reshape(y_test.shape[0]), $              predicted, ['No Failure', 'Failure'], 2, 'O-Ring Thermal Distress')
cur = con.cursor()
mean = np.mean(data['len'])$ print("The average length of tweets: {}".format(mean))
table.colnames
grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)$ grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)$ grid.add_legend()
import datetime as dt$ from dateutil.relativedelta import relativedelta
int_and_tel.loc[int_and_tel['sentiment_magnitude'] >= 10]
print(stock_data.shape)$ print("The number of rows in the dataframe is: ", stock_data.shape[0])$ print("The number of columns in the dataframe is: ", stock_data.shape[1])$
events_top10_df = events_enriched_df[events_enriched_df['topic_id'].isin(top10_topics_list)].copy()$ events_enriched_df.shape[0], events_top10_df.shape[0]
plt.title(f"Overall Media Sentiment Based on Twitter as of {curDate}")$ plt.xlabel("Outlets")$ plt.ylabel("Tweet Polarity")
my_gempro.pdb_downloader_and_metadata()$ my_gempro.df_pdb_metadata.head(2)
import pandas as pd$ companies = pd.read_csv('Fortune-1000-Company-Twitter-Accounts.csv')
bd.reset_index(drop=True)
api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)
building_pa.to_csv("buildding_00.csv",index=False)
tweetnet = nx.read_gpickle('twitter_graph_data/bigraph_full_pickle')
elon['nlp_text'] = elon.text.apply(lambda x: tokenizer.tokenize(x.lower()))$ elon.nlp_text = elon.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ elon.nlp_text = elon.nlp_text.apply(lambda x: ' '.join(x))
tweet_df.info()$
for div in soup.find_all('div'):$     print (div.text)
measure_df.info()$
temp_df.head()
df_new.groupby('country').count()
import pandas as pd$ d = pd.read_json('testtweets.json')$ d.head()
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)
voters.LastVoted.value_counts(dropna=False)$
coming_next_reason = questions['coming_next_reason'].str.get_dummies(sep="'")
YH_df["date"] = pd.to_datetime(YH_df["created_at"], errors='coerce')
listings.loc[0]$
pd.read_clipboard()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=", auth=('', ''))$
X_copy['crfa_a'] = X_copy['crfa_a'].apply(lambda x: int(x))
df.dropna(inplace=True)$ df.shape
result = grouper.dba_name.value_counts()
config = {"features analyzed": ["Sex", "Pclass", "FamilySize", "IsAlone", "Embarked", "Fare", "Age", "Title"]}$ datmo.snapshot.create(message="EDA", config=config)
distance_list = []$ for i in range(0, len_start_coord_list):$     distance_list.append(get_distance())
deployment_details = client.deployments.create(model_uid, "My NLC Car model deployment")
train_ratio = 0.75$ train_size = int(samp_size * train_ratio); train_size$ val_idx = list(range(train_size, len(df)))
Aussie_df = toDataFrame(aussie_results)$ Aussie_df.head(5)
colnames = ['log***','gfhdfxh']$ df.columns = colnames
df_cust_data = pd.read_excel('mazda_dataset.xlsx','Customer data')$ df_prospects_data = pd.read_excel('mazda_dataset.xlsx','Prospects data')
npath = '/glade/u/home/ydchoi/sopron_2018/notebooks/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
dist = np.sum(X, axis=0)$ for tag, count in zip(vocab, dist):$     print (count,tag)
sc = spark.sparkContext$ access_logs_raw = sc.textFile('data/apache.log')
all_sets.cards["XLN"].head()
groups = openmc.mgxs.EnergyGroups()$ groups.group_edges = np.array([0., 0.625, 20.0e6])
print(df[df.B>0],'\n')$ print(df[df>0],'\n')
data['new_claps'] = buckets$ data.head()
randomdata1 = randomdata.describe()$ print randomdata1
print(type(df.groupby("grade").size()))  # as series (series values is count of each category)$ df.groupby("grade").size()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']])$ results = logit_mod.fit()$ results.summary2()
proj_df['Project Subject Category Tree'].value_counts()
cityID = '9531d4e3bbafc09d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oklahoma_City.append(tweet) 
year_with_most_commits = commits_per_year.idxmax()[0].year$ year_with_most_commits
rain_df.set_index('date').head()$
date_ly = dt.date.today() - dt.timedelta(days = 2*365)$ print(date_ly)
tweet_df.retweeted.value_counts()$
data.info()
print(df['Confidence'].nunique())
print('{0:.2f}%'.format((scores[0:2.5].sum() / total) * 100))
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])$
words_only_sp_freq = FreqDist(words_only_sp)$ print('The 100 most frequent terms (terms only): ', words_only_sp_freq.most_common(20))
data = pd.read_csv('barbsList99.csv')$ data['order'] = data.apply(lambda x : 0,axis=1)$ data.to_csv('barbsList99.csv',index=False,index_label=False)
print('With KNN (K=) accuracy is: ',knn.score(x_test,y_test)) $
files1= files1.loc[files1['Shortlisted']==1]$ files1.shape
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
pax_raw.paxstep.max() <= step_threshold
scattering_to_total = scattering.xs_tally / total.xs_tally$ scattering_to_total.get_pandas_dataframe()
df_2017['bank_name'] = df_2017.bank_name.str.split(",").str[0]$
classes = ['actif', 'churn', 'lost']$ conf_matrix = confusion_matrix(y_true, y_pred)$ plot_confusion_matrix(conf_matrix, classes, 'True label', 'Predicted label', title='Confusion matrix', cmap=plt.cm.Blues)
df = pd.get_dummies(df, columns=["component", "product"], prefix=["component", "product"])$ df.head(2)
res = requests.get('http://elasticsearch:9200')$ r=json.loads(res.content)$ r
df.info()
df.describe()
pd.DataFrame(np.array([[10, 11], [20, 21]]))
studies_a = pd.DataFrame(studies, columns=['why_stopped','verification_date','target_duration','study_type','start_date_type','start_date','source','phase','overall_status','official_title','number_of_arms','nct_id','limitations_and_caveats','last_known_status','last_changed_date','is_unapproved_device','is_fda_regulated_drug','is_fda_regulated_device','enrollment_type','enrollment','completion_date','brief_title','baseline_population'])$
kick_projects_ip_scaled_ftrs = pd.DataFrame(preprocessing.normalize(kick_projects_ip[features]))$ kick_projects_ip_scaled_ftrs.columns=list(kick_projects_ip[features])
race_vars.columns = race_vars.columns.str.replace(' ', '_')$ race_vars.columns = race_vars.columns.str.replace('/', '_')$ race_vars.columns = race_vars.columns.str.lower()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-04-24&end_date=2018-04-24&api_key=" + API_KEY)
X_new.shape$
index = similarities.MatrixSimilarity(lsi[corpus])$ index.save('bible.index')
thai_province_list = set(df.loc[:,'province_in_thai'])$ len(thai_province_list) ## it have 77 province
squared_distances_i_j_k = np.power(y_i[:, np.newaxis, :] - y_i, 2)$ pairwise_squared_distances_i_j = squared_distances_i_j_k.sum(axis=2)$ pairwise_squared_distances_i_j
gdax_trans_btc['Timestamp'] = gdax_trans_btc['Timestamp'].map(lambda x: x.replace(second=0))
data.fillna(0).describe()
le = LabelEncoder()$ le.fit(data.Block)
tmp = 'is ;) :) seven.<br /><br />Title (Brazil): Not Available'$ print(preprocessor(tmp))$
logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']])$ results = logit_mod.fit()
train.nrows
plot_BIC_AR_model(data=RN_PA_duration.diff()[1:], max_order_plus_one=8)
df = pd.read_csv("WaterUsageSample.csv")$ df.shape
df.shape
session.query(Measurement.station).distinct().count()
df_user['user.name'].value_counts()[:10]
male = crime.loc[crime['Sex']=='M']$ male.head(3)
lst = data_after_subset_filter.SEA_AREA_NAME.unique()$ print('Waterbodies in subset:\n{}'.format('\n'.join(lst)))
city_group_df_merge_urban = city_group_df_merge.loc[city_group_df_merge['City Type']=='Urban',:]$ city_group_df_merge_suburban = city_group_df_merge.loc[city_group_df_merge['City Type']=='Suburban',:]$ city_group_df_merge_rural = city_group_df_merge.loc[city_group_df_merge['City Type']=='Rural',:]$
stations = session.query(Measurement).group_by(Measurement.station).count()$ print(f"{stations} stations")
extract_deduped_with_elms = extract_deduped_with_elms_v2.copy()
x_normalized = intersections[for_normalized_columns].values.astype(float)
woba_to_date = plate_appearances.sort_values(['batter','game_date','at_bat_number'], ascending=True).groupby('batter')['woba_value'].expanding(min_periods=1).mean()
last_unix=last_date.timestamp() #it is a function of the time module that can convert the time to$
df['text']=df['title'].str.replace('\d+', '')$
coarse_fuel_mgxs = coarse_mgxs_lib.get_mgxs(fuel_cell, 'nu-fission')$ coarse_fuel_mgxs.get_pandas_dataframe()
type(df_vow['Date'].loc[0])
hexbin = sns.jointplot(x="item", y="sentiment", data=dta, kind="scatter")$
series1['1975-12-31':'1980-12-31'].plot()$ plt.show()
df.date
store_items.fillna(method = 'backfill', axis = 0)
gene_df.drop('attributes', axis=1, inplace=True)$ gene_df.head()
tlen = pd.Series(data=datos['len'].values, index=datos['Creado'])$ tfav = pd.Series(data=datos['Likes'].values, index=datos['Creado'])$ tret = pd.Series(data=datos['RTs'].values, index=datos['Creado'])
red_4.to_csv(filename)
df_mfacts = mfacts[0]$ df_mfacts.columns = ['Name', 'Values']$ df_mfacts
image_processor.set_up_images()
for col_x in np.arange(len(col_list)):$     df.rename(columns={col_list[col_x]: rename_list[col_x]}, inplace=True)
active_stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\$                                 order_by(func.count(Measurement.station).desc()).all()$ active_stations$
to_pickle('Data files/clean_dataframe.p',adopted_cats)$ adopted_cats=from_pickle('Data files/clean_dataframe.p')
DataSet[['userName','userFollowerCt','tweetRetweetCt']].sort_values('tweetRetweetCt',ascending=False).head(10)
dataset['text length'] = dataset['text'].apply(len)
daily_change = [daily[2]-daily[3] for daily in json_data['dataset_data']['data'] if daily[2:3] != None]$ largest_change = str(round(max(daily_change), 2))$ print('The largest change in any one day was $' + largest_change + ' in 2017.')$
df_emoji_rows = new_df[new_df.extr_emojis != '']$
compound_df.plot(kind='scatter', x='index', y='@BBCWorld', subplots=False)$ plt.show()
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
portfolio_df.info()
for column in ideas:$     ideas[column] = ideas[column].dt.week
festivals.at[2,'Location'] = "Humboldt Park and Division Street"$ festivals.head(3)
mismatch1 = (ab_df['landing_page'] == "new_page")&(ab_df['group'] == "control")$ mismatch2 = (ab_df['landing_page'] == "old_page")&(ab_df['group'] == "treatment")$ print(ab_df[mismatch1].shape[0]+ab_df[mismatch2].shape[0])
data_l1 = tmpdf.index[tmpdf[tmpdf.isin(DATA_L1_HDR_KEYS)].notnull().any(axis=1)].tolist()$ data_l1
indx_weather=pd.read_csv(working_folder+indx_stn)
train, test = dogscats_h2o.split_frame(ratios=[0.7])
sel = [Measurements.date, func.avg(Measurements.prcp)]$ initial_date = format_latest_date - timedelta(days=365) # This will be start date from 12 months before final date of 8/23/17$ prcp_data = session.query(*sel).filter((Measurements.date >= initial_date)).group_by(Measurements.date).order_by(Measurements.date).all()
plt.figure(1)$ actor_counts = df['Actor1Name'].value_counts().head(10)$ actor_counts.plot.bar()
maxData = flowerKV.reduceByKey(lambda x, y: max(float(x),float(y)))$ print (maxData.collect())$
result = clustering.mean_shift(reduced, bandwidth=3.8)$ labels = np.unique(result['labels'])$ print(len(labels))
ax = P.plot_1d('pptrate')$ ax.figure.savefig('/media/sf_pysumma/pptrate.png')$
print activity_df.iloc[:,0]
experiment_df.drop(['email_opened'], axis=1, inplace=True)$ experiment_df.drop(['date'], axis=1, inplace=True)$ experiment_df.drop(['UUID_hash2'], axis=1, inplace=True)
! mrec_tune -d 'splits1/u.data.train.0' --input_format tsv --l1_min 0.001 --l1_max 1.0 --l2_min 0.0001 --l2_max 1 --max_sims 200 --min_sims 1 --max_sparse 0.3
pop_cat = timecat_df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ pop_cat.head()
df['year'] = pd.DatetimeIndex(df['datetime']).year$ df['month'] = pd.DatetimeIndex(df['datetime']).month$ df.head()
j = r.json()$
options_frame = pandas.read_pickle('options_frame.pickle')
tweets['text'].str.lower().str.contains('donald trump|president trump').value_counts()
red.shape
page_size = 200$ response = client.get('/tracks', tags='footwork', limit=page_size,$                     linked_partitioning=1)$
tzs = DataSet['userTimezone'].value_counts()[:10]$ print tzs
import pandas as pd$ import matplotlib.pyplot as plt$ import seaborn as sns
merged2.shape$
now = datetime.now() + pd.Timedelta('010:30:00')$ stories_df['Age In Days'] = stories_df.apply(lambda x: (now - x[x['status'] + ' Set To Date']).days, axis = 1)
screen_name = 'cdvel'$ with open(os.path.join(os.getcwd(),"data/credentials.json")) as data_file:    $     key = json.load(data_file)$
median_trading_volume = statistics.median([day[6] for day in data])$ print ('Median trading volume for 2017:', median_trading_volume)
zipcode = "nyc_zip.geojson"$ zc = gpd.read_file(zipcode)$ zc.shape
total.load_from_statepoint(sp)$ absorption.load_from_statepoint(sp)$ scattering.load_from_statepoint(sp)
df_drug_counts.dropna(axis = 1, thresh = 20).plot(kind = 'bar',$                                                  figsize = (10,6))
df_birth.Continent.value_counts(dropna=False)
result = customer_visitors.groupby('Yearcol').mean().astype(int)$ result$
sort_df.head(10)
print ('Python Version: %s' % (sys.version.split('|')[0]))$ hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER$ print ('HDFS filesystem running at: \n\t %s' % (hdfs_conf[0]))
all_tables_df.iloc[0]
url = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-06-25&end_date=2018-06-25&' + API_KEY$ r = requests.get(url)$ json_data = r.json()
processed_tweets.sample(4)
df.loc['20180103','A']
BallBerry_ET_Combine = pd.concat([BallBerry_rootDistExp_1, BallBerry_rootDistExp_0_5, BallBerry_rootDistExp_0_25], axis=1)$ BallBerry_ET_Combine.columns = ['BallBerry(Root Exp = 1.0)', 'BallBerry(Root Exp = 0.5)', 'BallBerry(Root Exp = 0.25)']
df = df.merge(pd.get_dummies(df['Device']),left_index=True,right_index=True,how='left')$ print ('Device',df.shape)
dates = pd.date_range('2018-05-01', '2018-05-06')$ temps1 = Series([80, 82, 85, 90, 83, 87], index = dates)$ temps1
import IPython  # for displaying parse trees inline$ for tree in parser.parse(sentence):$     IPython.display.display(tree)  # instead of tree.draw()
xml_in_merged['authorId'].nunique()
sentiments_pd= sentiments_pd.sort_values("Date", ascending=False)
pd.merge(msftAR0_5, msftVR2_4, how='outer')
pp = rf.predict_proba(X_train)$ pp = pd.DataFrame(pp, columns=['The_Onion_Prob', 'VICE_Prob', 'GoldenStateWarriors_Prob'])
Urban = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Urban")]$
items = {'Bob': pd.Series([245, 25, 55], index=['bike', 'pants', 'watch']),$          'Alice': pd.Series([40, 110, 500, 45], index=['book', 'glasses', 'bike', 'pants'])}
TripData_merged2.isnull().sum()
iris.head().iloc[:,0].values
mask = (df['tweet_created'] > "Thu Dec 14 00:00:00 +0000 2017") & (df['tweet_created'] <= "Thu Dec 14 23:59:59 +0000 2017")$ data_2017_12_14 = df.loc[mask]
save_n_load_df(ph, 'mergeable_holidays.pkl')$
keep_cols = ['Follow up Telepsychiatry', 'Follow up', 'Therapy Telepsychiatry', 'Returning Patient', 'Returning Patient MD Adult']$ dr_existing = dr_existing[dr_existing['ReasonForVisitName'].isin(keep_cols)]
DataSet.head(10)$ DataSet.tail(5)
data_links = ['https://raw.githubusercontent.com/sinny777/hukam/master/machinelearning/tf-nlc-model/data/data.csv']
data_df['tone'] = data_df.apply(tone, axis=1)
df_master.info()
frames = [df_tw, df_tw1]$ df = pd.concat(frames)
gs.score(X_test_all, y_test)
df.head()
df2.drop(labels=1899, axis=0, inplace=True)
len(kochdf.loc[kochdf['user'] == "Rezeptsammlerin"]['name'])
(session.query(Measurement.station, Station.name, func.count(Measurement.station))$  .filter(Measurement.station==Station.station)$  .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all())
overallDf = pd.DataFrame({$     "News Outlet": overallOutlet,$     "Compound Score" : overallCompound})
vacation_data_df=pd.DataFrame(vacation_data)$ rain_per_station = pd.pivot_table(vacation_data_df,index=['station'],values=['prcp'], aggfunc=sum)$ rain_per_station$
sentiments_df = df.apply(get_polarity_scores, axis=1)$ sentiments_df.head()
adjusted_width = 25$ ws.column_dimensions['L'].width = adjusted_width$ ws.column_dimensions['Q'].width = adjusted_width
close_px['AAPL'].ix['01-2011':'03-2011'].plot()
df.data.head()
subject_df = latest_df[['classification_id', 'subject_ids', 'subject_data']].copy()$ subject_df['subject_data'] = subject_df.subject_data.apply(lambda x: list(json.loads(x).values())[0]['Filename'])
df.sort_values(by=['title','num_comments'],inplace =True)$ df.head()
print(spike_tweets.iloc[5000, 2])
train_df = pd.read_csv("train.csv", skiprows=range(1,159903891), nrows=25000000, dtype=dtypes)$ train_df.head()
active_psc_records.month_year_birth.hist(figsize=(20,5),bins=50)
stat_info_st = stat_info[0].apply(fix_space_0)$ print(stat_info_st)
f = e.instance_method$ e.instance_var = 'e\'s instance var'$ f()
avgvolume.sort()$ mediankey = int((len(avgvolume)/2))$ print('Median trading volume for the year was: ',avgvolume[mediankey])
crimes[['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']].head()
y = df['comments']$ X = df[['subreddit', 'title', 'age']].copy(deep=True)
tips_train = tips.sample(frac=0.8, random_state=123)$ tips_test = tips.loc[tips.index.difference(tips_train.index),:]$ tips_train.shape, tips_test.shape, tips.shape
X = df[['word_count','sentiment','subjectivity','domain_d','post_duration']]$ y_cm = LabelEncoder().fit_transform(df['com_label'])#Comments$ y_sc = LabelEncoder().fit_transform(df['score_label'])# Score$
session.query(func.count(Sta.name)).all()
cat_sz = [(c, len(full_data[c].unique())) for c in cats]
now = pd.Timestamp('now')$ now, now.tz is None
print('There are {} news articles'.format(len(news)))$ print('Timewise, we have news from {} to {}'.format(min(news.date), max(news.date)))
cur.execute("SELECT * FROM pgsdwh.pgsparts.geps_distinct_order_lines_dw_t")
np.shape(rhum_us_full)
df_con1 = df2.query("converted=='1'") $ x_ = df_con1["user_id"].count()$ x_$
hs_path = utils.install_test_cases_hs(save_filepath)
import pandas as pd$ df_from_pd = pd.read_clipboard()$ df_from_pd
output = pd.DataFrame(data={"Id":id, "bot":sub1})$ print(metrics.accuracy_score(output.bot, test_copy1.bot))$
sdsw = sd[(sd['JOB_TITLE'].str.contains('SOFTWARE')) | (sd['JOB_TITLE'].str.contains('PROGRAMMER')) | (sd['WAGE_UNIT_OF_PAY'] == 'Year')]$ sdsw.sample(50)
y_size = fire_size['SIZECLASS']$ x_climate = fire_size.loc[:, 'glon':'rhum_perc_lag12']
df['gathering'].fillna(0, inplace = True)
excutable = utils.download_executable_lubuntu_hs(save_filepath)
train, valid, test = covtype_df.split_frame([0.7, 0.15], seed=1234)$ covtype_X = covtype_df.col_names[:-1]     #last column is Cover_Type, our desired response variable \n",$ covtype_y = covtype_df.col_names[-1]    
requests.get(saem_women)
for i in range(0,10):$     topics = model.show_topic(i, 10)$     print "%s: %s" %(i, (', '.join([str(word[0]) for word in topics])))
a = tips.loc[:,"tip"]$ a.head()
engine = create_engine("sqlite:///hawaii.db", echo = True)$
retweets['id'].groupby(pandas.to_datetime(retweets['created_at']).dt.date).count().mean() # 2.86
df_2007['bank_name'] = df_2007.bank_name.str.split(",").str[0]$
prcp_df = pd.DataFrame(prcp_in_last_year, columns=['date', 'prcp'])$ prcp_df.head(11)
learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)
dc = datacube.Datacube(app='dc-nbart')
from statsmodels.tsa.arima_model import ARIMA$ model_713 = ARIMA(dta_713, (2, 0, 1)).fit() $ model_713.forecast(5)[:1] 
df.filter(like="FLG_", axis=1)
df_daily5=df_daily.groupby(["STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index()$ df_daily5.head(5)$
avg_Task = sample['num_completed_tasks'].mean()   $ avg_Task
r1.keys()
gDate_content_count = gDateProject_content.groupby(level=0)$ gDate_content_count = gDate_content_count.sum()$
ldf['Traded Volume'].median()
malebyphase = malemoon.groupby(['Moon Phase']).sum().reset_index()$ malebyphase
Session=sessionmaker(bind=engine)$ session=Session()$ result_set=session.query(Adultdb).first()$
export_path=cwd+'\\ca_simu_from_python.csv'$ ca_de.to_csv(export_path, index=False)
print (autoData.reduce(lambda x,y: x if len(x) < len(y) else y))
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
train = train.drop(columns=["hour"])$ test = test.drop(columns=["hour"])
scaler = MinMaxScaler()$ data[intFeatures] = scaler.fit_transform(data[intFeatures])
day = datetime.now()$
l=len(col)$ for i in range(1,l):$     X=np.column_stack((X,col[i]))
tables=db_engine.table_names()$
len(df_new.UWI.unique())
df_test = pd.read_hdf('bittrex.h5', 'bittrex_tb', where = ["mn_ix= 'BTC-VTC'"])$ df_test$
sentiment_df["date"] = pd.to_datetime(sentiment_df["date"])$
data.userScreen.max() # calls the user that appears most times under the userScreen column$
df1.fillna(-999999,inplace=True)
emojis_db=pd.read_csv('emojis_db_csv.csv')$ emojis_db.head()
weekly_videosGB = weekly_dataframe(nodesGB)$ weekly_videosGB[0].head()
sn.distplot(a.A.flatten()[:],kde=False,norm_hist=True,bins=900)$ plt.xlim((0.,0.5))
top_views = doctype_by_day.loc[:,doctype_by_day.min().sort_values(ascending=False)[:10]]$ ax = top_views.plot()$ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
cityID = '0eb9676d24b211f1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cleveland.append(tweet) 
year12 = driver.find_elements_by_class_name('yr-button')[11]$ year12.click()
words_mention_sp = [term for term in words_sp if term.startswith('@')]$ corpus_tweets_streamed_profile.append(('mentions', len(words_mention_sp))) # update corpus comparison$ print('List and total number of mentions: ', len(set(words_mention_sp))) #, set(terms_mention_stream))
df_users = pd.read_csv('../data/march/users.csv')$ df_levels = pd.read_csv('../data/march/levels.csv')$ df_events = pd.read_csv('../data/march/events.csv', skiprows=1, names=event_header, error_bad_lines=False, warn_bad_lines=True)     
df2[df2.duplicated(['user_id'])].user_id
salida_all = getFacebookPageFeedData(page_id, access_token, 100)$ columns = ['post_from', 'post_id', 'post_name', 'post_type', 'post_message', 'post_link', 'post_shares', 'created_time']$ df_posts = pd.DataFrame(columns=columns)
median_trading_volume = statistics.median([day[6] for day in data])$ print ('Median trading volume for 2017:', median_trading_volume)
printer_usage.rename(columns={'SUM(pages)' : 'total_printed'}, inplace=True)$ printer_usage.drop(columns=['pages'])$ print('')
df['Approximate Age at Designation'] = df['membership_date'].apply(lambda x: x.year)-df['yob']
data_frame['CLASS1'].value_counts()
pvt['ga:date'] = pd.to_datetime(pvt['ga:date'], format='%Y%m%d', errors='coerce')$ pvt.rename(columns={'ga:transactionId': 'transactionId', 'ga:date': 'date'}, inplace=True)$ pvt
validation.analysis(observation_data, lumped_simulation)
free_data.dtypes
lr2 = LogisticRegression(solver='saga', multi_class='multinomial', random_state=20, max_iter=1000)$ lr2.fit(X, y)
contractor_clean = contractor.copy()
tweet_df.to_csv("output/Sentiment_Analysis_Media_Data.csv",$                      encoding="utf-8", index=False)
sortHighThenVol = AAPL.sort_values(by=["high", "volume"], ascending=False)$ sortHighThenVol.head()
df_ct = pd.read_csv("can_tire_all_final.csv", encoding="latin-1")
prediction_df = pd.DataFrame(predictions, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"])$ combined_df = comment_df.join(prediction_df) # join the comment dataframe with the results dataframe$ combined_df.head(15)
all_sets.cards["XLN"].shape
base_url = 'https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_'
train_data, test_data, train_labels, test_labels = train_test_split(spmat, labels, test_size=0.10, random_state=42)  
df.quantile([.01, .05, .1, .25, .5, .75, .9, .95, .99])
from sklearn.utils import shuffle$ qs = shuffle(qs)$ print len(qs)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)])$ new_page_converted.mean()
ctrl_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][0]$ print("P(ctrl_con) = %.4f" %ctrl_con)
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))
CBS = news_df.loc[(news_df["Source Account"] == "CBSNews")]$ CBS.head(2)
def line_eq(x, mm, bb):$     return  x*mm + bb$ print "# test: %s"%(line_eq(10, 0.4471, -4.2076))$
files_gps = []$ for x in range(771,797): #can change this range to pull any gps files; might be limited by API calls depending on #$     files_gps.append(syn.downloadTableFile(table_results, column='UnknownFile_1.json.items', rowId=x, versionNumber=1, ifcollision=u'keep.both'))
data.show()
vocab = vectorizer.get_feature_names()$ print(len(vocab))$ print(vocab[:10])
df['2015-06'].resample('D').sum().plot()
import pandas as pd$ groceries = pd.Series(data = [30, 6, 'Yes', 'No'], index = ['eggs', 'apples', 'milk', 'bread'])$ groceries
data = []$ for row in result_proxy:$     data.append({'date': row[0], 'prcp': row[1]})
P_old = ab_df2['converted'].mean()$ print(P_old)
data.describe().transpose()
with open('united_total_list_rake.pickle', 'wb') as f:$     pickle.dump(united_total_list_rake, f, pickle.HIGHEST_PROTOCOL)
Magic.__repr__
combine_Xtrain_ytrain = pd.concat([X,y],axis=1)$ combine_Xtrain_ytrain = combine_Xtrain_ytrain.drop(['date','subjects','word_count','full_text'],axis=1)$ combine_Xtrain_ytrain.head()
skf = StratifiedKFold(n_splits=5)$ skf.get_n_splits(X, y)$ folds = [(tr,te) for (tr,te) in skf.split(X, y)]
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
import json$ list_of_issues_dict_data = [json.loads(line) for line in open('SPM587SP18issues.json')]
model.init_sims(replace=True)
new_df = df.fillna(0)$ new_df
vhd['season'] = vhd.index.str.split('.').str[0]$ vhd['term'] = vhd.index.str.split('.').str[1]
print("Number of mislabeled points out of a total %d points : %d" % \$       (X_clf.shape[0], (y_clf != y_pred).sum()))
now = Timestamp("now")$ local_now = now.tz_localize('UTC')$ now, local_now
import numpy as np$ ok.grade('q05')
df_zillow.to_csv('cleaned_Zillow.csv', index = False)$ df_zillow = pd.read_csv('cleaned_Zillow.csv')
very_pop_df = au.filter_for_support(popular_trg_df, min_times=7)$ au.plot_user_dominance(very_pop_df)
dummies = pd.get_dummies(plate_appearances['events']).rename(columns=lambda x: 'event_' + str(x))$ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)
print(np.shape(b))$ print(np.size(b))$ print(np.ndim(b))
for v in data.values():$     if v['answers']['Q1'] == 'yes':$         v['answers']['Q1A'] = 0
df1_dummies.shape
from src.image_manager import ImageManager$
most_recent_1yr_entry = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(most_recent_1yr_entry)
cityID = 'b463d3bd6064861b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Detroit.append(tweet) 
df.loc['1998-09-10':'1998-09-15','MeanFlow_cfs':'Confidence']
tripduration_minutes = pd.Series(minutes_list)
!rm -fv .cache/runtimes.csv
import numpy as np$ result=results.home_score$ result.apply(np.median)
sen_dat = pd.read_msgpack('full_DataFrame/combined_data/master/sentiment_data_2min.msg')$ sen_data2 = pd.read_msgpack('full_DataFrame/combined_data/master/sentiment_data_5min.msg')
status_data = status_data.drop(['STATUS', '#AUTHID', 'sEXT', 'sNEU', 'sAGR',$                                     'sCON', 'sOPN', 'DATE'], axis=1)
plt.plot(df[base_col].rolling(window=12).std(),color='green',lw=2.0,alpha=0.4)$ plt.plot(df[base_col].rolling(window=3).std(),color='purple',lw=0.75)$ plt.show;
finalData=dat.append(Stockholm_data)$ X.f = vectorizer.fit_transform(finalData['tweet_text'])$ y.f = finalData['class']$
S_lumpedTopmodel.initial_cond.filename
n_new = df2[df2.group == "treatment"].count()[0]$ print("The population of user under treatment group: %d" %n_new)
counts,x,y = np.histogram2d(theta[:,0], theta[:,1], bins=[50,50], range=[[0,1],[0,1]])$ plt.imshow(counts, extent=(x[0],x[-1],y[0],y[-1]), origin='lower');
ad_source = questions['ad_source'].str.get_dummies(sep="'")
remove_index = treat_oldp.append(ctrl_newp).index$ remove_index.shape
columns = inspector.get_columns('stations')$ for c in columns:$     print(c['name'], c["type"])$
cityID = '3df4f427b5a60fea'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Antonio.append(tweet) 
import pandas as pd$ git_log = pd.read_csv("datasets/git_log.gz", sep='#', encoding='latin-1', header=None, names=['timestamp', 'author'], compression='gzip')$ git_log.head(5)
r_train, r_test, rl_train, rl_test = train_test_split(r_forest.ix[:,col], r_forest['bot'], test_size=0.2, random_state = 2)$ r1_train, r1_test, rl1_train, rl1_test = train_test_split(r_forest.ix[:,col1], r_forest['bot'], test_size=0.2, random_state = 2)$
pd.read_csv("../../data/msft.csv",nrows=3)
data['data'].keys()
SelectedHighLows = AAPL.loc["2017-06-20":"2017-07-20", ["high", "low"]]$ SelectedHighLows
techmeme['sources'] = techmeme.extra_sources.copy()$ for i, list_ in enumerate(techmeme.sources):$     list_.append(techmeme.original_source[i])
df['len_convo'] = df.conversations.apply(len)$ print("... got conversation lengths")
sns.violinplot(x=df['liked'],y=df['Time_In_Badoo'],data=df,whis=np.inf)$ plt.title('By Time In Badoo')$
price_data_df = quandl.get('BCHARTS/KRAKENUSD', start_date="2018-04-18", end_date="2018-04-20")
df2[df2.duplicated('user_id')]
theta_0 = 0.1* np.random.randn(X_train_1.shape[1])$ theta = gradient_descent(X_train_1, y_train, theta_0, 0.1, 100)
b = news_df[news_df['Source Acc.'] == 'nytimes']$ b.head()$ print(b['Compound Score'].sum())
X_train.isnull().sum()
overall_df[overall_df['News Outlet'] == 'BBC']['Compound Score'].values[0]$
!wget https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv$ !head -n 2 Consumer_Complaints.csv
df2 = df.copy()$ df2 = df.drop(df[(df.group == "control") & (df.landing_page == "new_page") | (df.group == "treatment") & (df.landing_page == "old_page")].index)
plt.scatter(dataframe['lat'], dataframe['lon'],c = dataframe['label'],cmap=plt.cm.Paired)$ plt.scatter(center['lat'],center['lon'],marker='s')$ plt.show()
timezones = DataSet['userTimezone'].value_counts()[:10]$ print(timezones)
grouped_by_letter = df1.groupby(['tactic_letter'])$ list_of_letter = df1['tactic_letter'].unique()$
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')$ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
all_sessions = cuepoints['session_id'].unique()$ all_sessions
reddit = reddit.drop_duplicates(subset=['Titles', 'Subreddits'], keep='last', inplace=False) #dropping those $ reddit.head()$ reddit.shape #now I see those specific rows have been dropped$
countries = pd.read_csv('C:/Users/akapoor/Music/01 Docs/HealthCare App/ctdb/countries.txt', sep="|")$ countries.head()
nodes_file = directory_name + 'nodes.h5'              # Contains information about every node$ node_models_file = directory_name + 'node_types.csv'   # Contains information about models
for route in routes:$     print('mode: {} | id: {} | route name: {}'.format(route['mode'], route['id'], route['longName']))
pd.read_sql('SELECT * FROM experiments WHERE temperature = 375 ORDER BY irradiance DESC', conn, index_col='experiment_id')
LT906474 = pd.read_table("GCA_900186905.1_49923_G01_feature_table.txt.gz", compression="infer")$ CP020543 = pd.read_table("GCA_002079225.1_ASM207922v1_feature_table.txt.gz", compression="infer")
tweetsIRMA = pd.read_sql("SELECT tc.tweet_id, i.DateTime, tc.text, tc.longitude as 'tweet_long', tc.latitude as 'tweet_lat', i.lon as 'irma_long', i.lat as 'irma_lat' FROM tweetIrmaTimes ti JOIN irmaFeatures i on ti.irmaTime = i.DateTime JOIN tweetCoords tc on tc.tweet_id = ti.tweet_id",Database().myDB)
api_json = json.loads(api_call.text)$ for x in api_json['messages']:$     print(x['body'])$
engine = create_engine("sqlite:///Resources/hawaii.sqlite")
conf = SparkConf().setAll([('spark.executor.memory', '6g'), ('spark.executor.cores', '6'), ('spark.cores.max', '6'), ('spark.sql.session.timeZone', 'UTC')])$ sc = SparkContext("local", "llite", conf=conf)
df.median()
payment = pd.get_dummies(auto_new.Payment_Option)$ payment.head()
import pickle$ filename = 'finalized_automl.sav'$ pickle.dump(automl, open(filename, 'wb'))
class StdDev(CustomFactor):$     def compute(self, today, asset_ides, out, values):$         out[:] = np.nanstd(values, axis=0)
store_items = store_items.append(new_store)$ store_items
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False)$
test_df = df[df["dataset"]=="test"]; test_df.shape
print('#events :', len(all_data.ID.unique()), '#sources :', len(all_data.Source.unique()), '#articles', len(all_data.Article.unique()), '#mention_date', len(all_data.MentionDate.unique()))
demand.loc[:,'value'] = demand.loc[:,'value'].copy() *1.5$ demand.head()
val_df.reset_index(drop=True, inplace=True)$ val_y.reset_index(drop=True, inplace=True)$ test_df.reset_index(drop=True, inplace=True)
ch_year = pd.DataFrame(ch['startDate'].dt.year.value_counts(sort=False))$ ch_year['SERI'] = pd.Series([10,606,560,689,654,683,745,312,10], index=[2007,2008,2009,2010,2011,2012,2013,2014,2015])$ ch_year['Delta'] = ch_year['startDate']-ch_year['SERI']
cruise_data = pd.read_table(data_file, delim_whitespace=True, header=None, skiprows=1)$ cruise_data = cruise_data.rename( columns={ 0:'Pressure', 1:'Temperature', 13:'Salinity' } )$ cruise_data[0:5]
data.show()$ data.printSchema()$
injury_df['First_Name'] = injury_df['Relinquished'].apply(lambda y: y.split()[0])$ injury_df['Last_Name'] = injury_df['Relinquished'].apply(lambda y: y.split()[1] if len(y.split())>1 else 'Unknown')
rhum_us = rhum_nc.variables['rhum'][1, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(rhum_us)
iris.groupby('Species')['Species'].count()
mean_temp = temp_long_df.groupby(['date'])['temp_c'].mean()$ mean_temp.plot(x='date', y='temp_c')
query = "SELECT DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) as created_date, temp, wdsp, mxpsd, gust, max, min, prcp, sndp, snow_ice_pellets FROM `bigquery-public-data.noaa_gsod.gsod20*` WHERE _TABLE_SUFFIX BETWEEN '10' AND '17' AND stn = '725053' AND wban = '94728'"$ weather = read_gbq(query=query, project_id='opendataproject-180502', dialect='standard')
nitrodata['ActivityStartDate'] = pd.to_datetime(nitrodata['ActivityStartDate'],format="%Y-%m-%d")
sr_top = sr.top(limit = 1000) # limit is 1000 -- won't return any more than that
predictions = lrmodel.transform(testData)
final_df = diff_df.merge(final_sentiment_df, how = "outer")$ final_df
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2.drop(['Neighbors_Obj'], axis=1)$
tokens = reddit.title.apply(process)
articles = db.get_sql(sql)$ articles.head()
tmax_day_2018.values
user = DataSet['userName'].value_counts()[:10]$ print(user)
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)$
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=zDPbq2QVaB7jFAEq5Tn6')$ print(r.json())
all_tables_df.OWNER.nunique()
urls = pd.read_pickle('PC_World_Urls.pickle')$ headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}$ shop_web_addr = 'https://www.pcworld.co.uk'
pax_raw[pax_raw.paxstep > step_threshold].tail(20)
output.to_csv( "randomforest.csv", index=False, quoting=3 )
df_imputed = pd.DataFrame(df_imput)$
df['id'] = df['id'].astype('category')$ df['sentiment'] = df['sentiment'].astype('category')$ df['created_at'] = pd.to_datetime(df['created_at'])
max(close, key=close.get)
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'US', 'UK']])$ result = logm2.fit()$ result.summary2()
lossprob2 = fe.bs.smallsample_loss(2560, poparr2, yearly=256, repeat=500, level=0.90, inprice=1.0)$
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$ print(new_page_converted)
nitrodata['MonitoringLocationIdentifier'].nunique()
sentiments_pd = pd.DataFrame.from_dict(sentiments).round(3)$ sentiments_pd
airlines_sim = [d for d in airlines if len(d.tweets) == 2]$ len(airlines_sim)
for col in X_nnumcols:$     X[col] = X[col].apply(lambda l: hash(l))$     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
dict_r = r.json()$ type(dict_r)
kick_projects = df_kick[(df_kick['state'] == 'failed') | (df_kick['state'] == 'successful')]$ kick_projects['state'] = (kick_projects['state'] =='successful').astype(int)
np.any(x > 8)
print("The tweet with more likes is: \n{}".format(data['Tweets'][fav]))$ print("Number of likes: {}".format(fav_max))$ print("{} characters.\n".format(data['len'][fav]))
Measurement = Base.classes.measurements$ Station = Base.classes.stations
s =[1,2,2,3]$ list(map(lambda x:(s.count(x)),s))
dfn['goal_log'] = np.log10(dfn['goal'].values)
df_artist.columns = df_artist.columns.str.replace(" ","_")$ df_artist.head(2)
iv = options_frame[options_frame['Expiration'] == '2016-03-18']$ iv_call = iv[iv['OptionType'] == 'call']$ iv_call[['Strike', 'ImpliedVolatilityMid']].set_index('Strike').plot(title='Implied volatility skew')
ppm_title = preProcessor_in_memory(hueristic_pct=.99, append_indicators=True, padding='post', keep_n=4000, maxlen=12)$ vectorized_title = ppm_title.fit_transform(data_to_clean_title)
df['receive_time'] = pd.to_datetime(df['receive_time'])$ df['created_at'] = pd.to_datetime(df['created_at'])
cityID = '55b4f9e5c516e0b6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Orlando.append(tweet) 
df = df.dropna()$ df = df[list(df_symbols.columns) + targets]
df_proj = pd.read_csv(projFile, usecols = projCols, $                  dtype = proj_dtypes)
X_train.to_csv('X_train.csv')$ X_test.to_csv('X_test.csv')
wqYear = dfHawWQ.groupby('Year')['TotalN'].mean()$ dfAnnualN = pd.DataFrame(wqYear)
merged = df2.merge(dfCountry, on='user_id')$ merged.head()
d = datetime.date(2016, 7, 8)$ d.strftime("On %A %B the %-dth, %Y it was very hot.")
np.count_nonzero(x < 6)
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
shape_file = ('/g/data/r78/vmn547/GWandDEA_bex_ness/Little_GW_AOI_for_demo/kEEP_ord/KEEP_AOI.shp')
df4 = bg_df2[['Date', 'BG']].copy()$ df4 # whew, this way works fine$
r.json()['dataset']
df.cdescr.head()
engine = create_engine("sqlite:///hawaii.sqlite")
plt.hist(p_diffs);
afxx_data = r.json()
engine = create_engine("sqlite:///hawaii.sqlite")$
print("Err rate Model")$ for rate, name in sorted((rate, name) for name, rate in best_error.items()):$     print("%f %s" % (rate, name))$
df.to_csv("../../data/msft_piped.txt",sep='|')$ !head -n 5 ../../data/msft_piped.txt
data = pd.read_sql("SELECT salary FROM empvw_20",xedb)$ print(data)
hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),$                  callbacks=[RocAuc], verbose=1)
learner.save_encoder('adam3_10_enc')$
latest_time_entries = toggl.request("https://www.toggl.com/api/v8/time_entries")
df_teacher_info = df_teacher_info[df_teacher_info.awj_teacher_id.isin(pick_list)]$ df_teacher_info.drop(["country", "timezone", "degree_and_university", "child_exp"], axis=1, inplace=True)
des2doc = dict()$ for index,row in temp.iterrows():$     des2doc[index] = row['description']
year4 = driver.find_elements_by_class_name('yr-button')[3]$ year4.click()
element = driver.find_element_by_xpath('//*[@id="comic"]/img')$ element.get_attribute("title")
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=xhB-Ae2VRyMYmv2CRGtV")
X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(df_ml_features_scaled, \$                                                     df_ml_target, test_size = 0.2, random_state = 10,\$                                                     stratify = df_ml_target)
contrast = df[(df['up_votes'] > df['up_votes'].quantile(q=.85)) | (df['up_votes'] == 0)].reset_index(drop=True)$ contrast['viral'] = (contrast['up_votes'] != 0).astype(int)$ contrast.shape
precip_data = session.query(Measurements).first()$ precip_data.__dict__
t = 0.25$ c_y = 0.6607/(1-t)$ print(c_y)
assert(df.duplicated().sum()==0)
import test_package.print_hello_class_container$ my_instance = test_package.print_hello_class_container.Print_hello_class()$ my_instance
from pyspark.ml import Pipeline$ from pyspark.ml.classification import DecisionTreeClassifier$ from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer
dictionary = Dictionary(messages_clean)$ corpus = [dictionary.doc2bow(text) for text in messages_clean]
df['y'].plot.box(notch=True)
API_CALL = "https://www.quandl.com/api/v3/datasets/WIKI/FB.json?column_index=4&start_date=2014-01-15&end_date=2014-01-16&collapse=daily&transform=rdiff&api_key="$ r = requests.get(API_CALL + API_KEY)
merged2 = pd.DataFrame.merge(merged,omdb_df,on="movie",how="inner")$ merged2.head()
import warnings$ warnings.simplefilter('ignore')
df1=pd.read_csv("approval data clean values only.csv")$ df1.head()
segments = pd.read_csv("transit_segments.csv", parse_dates=['st_time', 'end_time'])$
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid$ x = df30458.groupby('start_station_name').count()$ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
unordered_timelines = list(chain(*np.where([min(USER_PLANS_df.iloc[i]['scns_created']) != USER_PLANS_df.iloc[i]['scns_created'][0] for i in range(len(USER_PLANS_df))])))
doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]
p = len(train_att)/len(train)$ print(len(train_att))$ print('The percentage of converted clicks is {num:.10%}'.format(num=p))
big_exit_mask = big_data.EXIT > 1500000000$ big_data_masked = big_data[big_exit_mask]$ big_data_masked.STATION.value_counts()$
sss = list(spp.season.unique())
volt_prof_before['Bus']=volt_prof_before['Bus'].apply(lambda x:x.lower())$ volt_prof_after['Bus'] =volt_prof_after['Bus'].apply(lambda x:x.lower())
y.name
tweet = soup.find('div', class_="js-tweet-text-container")
df_activity_prediction = \$     df_active_user_prediction[df_active_user_prediction['user_id'].isin(active_users)]\$     .merge(df_active_user_metrics,on="user_id", how="left")
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
fixed.head()
lr_y_score = model.predict_proba(X_test)[:, 1] #[:,1] is formatting the output$ lr_y_score
test_ind["Pred_state_RF"] = trained_model_RF.predict(test_ind[features])$ train_ind["Pred_state_RF"] = trained_model_RF.predict(train_ind[features])$ kick_projects_ip["Pred_state_RF"] = trained_model_RF.predict(kick_projects_ip_scaled_ftrs)
cityID = '27485069891a7938'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_York.append(tweet) 
sns.set(color_codes=True)$ sns.distplot(utility_patents_subset_df.number_of_claims, bins=40, kde=False)$ plt.show()
transformed_test_pending_ratio = \$ polynomial_features.transform(x_test['Pending Ratio'].values.reshape(-1,1))
iso_json = json.loads(iso_response.text)$ iso_gdf = gpd.GeoDataFrame.from_features(iso_json['features'])$ iso_gdf[:]
volt_prof_before=pd.read_csv('../inputs/opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder))$ volt_prof_after=pd.read_csv('../outputs/from_opendss/to_opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder))
df_temp = df_measurement[['station','tobs']].groupby(['tobs']).min()$ df_temp.head(1)$ df_temp.tail(1)
ohlc = walk.resample("H").ohlc()$ ohlc
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(therapist_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Therapists Hours First Difference Partial Autocorrelation')
(df.xs(symbol,level='symbol')['2011':].flow*100.).rename(symbol).plot.hist(bins=61,$ title='{}: Daily Change in Shares (%)'.format(symbol),figsize=(10,4),xlim=(-10,10),alpha=0.75)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
df['age'] = df['fetched time'] - df['created_utc']$ df['age'] = df['age'].astype('timedelta64[m]')$ df.head()
features = pd.merge(features, form_btwn_teams.drop(columns=['margin']), on=['game', 'team', 'opponent'])
data_sample['value'] = [i.replace(',', '') for i in data_sample['value']]$ data_sample['value'] = pd.to_numeric(data_sample['value'])
dict_data = r.json()$ print(type(dict_data))$ print(dict_data)
mgxs_lib.domain_type = 'cell'$ mgxs_lib.domains = geometry.get_all_material_cells().values()
df_2009['bank_name'] = df_2009.bank_name.str.split(",").str[0]$
ax = time_length.plot(figsize=(16,4), color='r', title='Tweet length vs. DateTime')$ ax.set_xlabel("DateTime")$ ax.set_ylabel("Tweet length")
forest_clf = RandomForestClassifier(random_state=0)$ forest_clf.fit(X_train, Y_train)
tweet_df_polarity = tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
cnn_df = constructDF("@CNN")$ display(constructDF("@CNN").head())
y_class = demo.get_class(y_pred)$ cm(y_test,y_class,['0','1'])
!hdfs dfs -mkdir hw3$ os.getcwd() 
with pd.option_context('display.max_colwidth', 100):$     display(news_period_df[['news_collected_time', 'news_title']])
sublist = [BAL, CHI, HOU, PIT] # These are the four teams with an original Per Seat Price column uploaded.$ for team in sublist:$     team.drop(["Per Seat Price"], axis = 1, inplace = True) # Drops the column
from nltk.corpus import stopwords$ print(stopwords.words('english'))
p_diffs = np.array(p_diffs)$ (p_diffs > actual_diff).mean()
outliers_timeDict = {key: df[abs(df['timePassed'] - np.mean(df['timePassed'])) > 3 * np.std(df['timePassed'])] for key, df in typesDict.items()}$ outliers_timeDict.keys()
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(doc_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours First Difference Autocorrelation')
!wget -O ChurnData.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/ChurnData.csv
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)$ sns.boxplot(x="frcar", y="y", data = psy_prepro, ax=ax1).set_title("Anxiety driving/riding in a car")$ sns.boxplot(x="jmp2w", y="y", data = psy_prepro, ax=ax2).set_title("Feel jumpy and restless in past 2 weeks")$
plans_set = set()$ plans,counts = np.unique([(['Free-Month-Trial'] + p if p[0] != 'Free-Month-Trial' else p) for p in BID_PLANS_df['scns_array']  ],return_counts = True)
plt.savefig(str(output_folder)+'NB05_2_FC_before_and_after'+str(cyclone_name)+'_'+str(location_name))
cityID = '7f061ded71fdc974'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Montgomery.append(tweet) 
s4 = pd.Series([10, 0, 1, 1, 2, 3, 4, 5, 6, np.nan])$ len(s4)
data_FCInspevnt_latest = data_FCInspevnt.loc[data_FCInspevnt['Inspection_number'] == 1]$ data_FCInspevnt_latest = data_FCInspevnt_latest.reset_index(drop=True)$ data_FCInspevnt_latest.head(15)
with open('./data/processed/X_train_age_imputed.pkl', 'wb') as picklefile:$     pickle.dump(X_train,picklefile)$
p1.age= 45$ print(p1.age)
free_data.describe()
driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[1]').send_keys('Avengers: Infinity War')$ driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[2]').click()
daily = hourly.asfreq('D')$ daily
prcp_gb_year['prcp'].describe()
households_with_count.iloc[30:40, 15:]
train_ind, test_ind, train_dep, test_dep = train_test_split(kick_projects_ip_scaled_ftrs, kick_projects_ip[response], test_size=0.3, random_state=0)
df['Position'].plot(figsize=(20,10))$
df2 = df$ mismatch_index = mismatch_df.index$ df2 = df2.drop(mismatch_index)
from matplotlib import style$ style.use('fivethirtyeight')$ import matplotlib.pyplot as plt
post_number = len( niners[niners['Jimmy'] == 'yes']['GameID'].unique() )$ print post_number
dat_before_fill=dat.copy()$ for temp_col in temp_columns:$     dat.loc[:,temp_col]=dat[temp_col].interpolate(method='linear', limit=3)
stock_data.loc[stock_data['close'] > 80]
y_axis = np.arange(math.floor( rides_fare_average_min ) - 5, math.floor(rides_fare_average_max) + 6, 5)$ y_axis
os.chdir(root_dir + "data/")$ df_fda_drugs_reported = pd.read_csv("filtered_fda_drug_reports.csv", header=0)
Base.classes.keys()$
cityID = '8173485c72e78ca5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Atlanta.append(tweet) 
sorteios_2017 = data[data["Data Sorteio"] > "2017-01-01"]$ sorteios_2017.Ganhadores_Sena.sum()$
h1 = qb.History(360, Resolution.Daily)$ h1;
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)$ regr = LinearRegression()$ regr.fit(X_train, y_train)
countries_df = pd.read_csv('/Users/pra/Desktop/AnalyzeABTestResults 2/countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
dfprediction = pd.DataFrame(data={'y_hat': rf.predict(X_btc_test)}, index=X_btc_test.index)
most_recent = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ most_recent_list = most_recent[0].split("-")#split on "-"$ most_recent_list#check
%%time$ grid_svc.fit(X_tfidf, y_tfidf)
df2017=df1.iloc[0:165,]  #line 165 is 2016 so we need 1 more than line 164 ie 165$ df2017.tail()
twitter_coll_reference.count()
year13 = driver.find_elements_by_class_name('yr-button')[12]$ year13.click()
soup.find('div', class_='movie-add-info left').find_all('li')
payments_NOT_common_all_yrs = (df_sites_NOT_common_DRGs.groupby(['id_num','year'])[['disc_times_pay']].sum())$ payments_NOT_common_all_yrs = payments_NOT_common_all_yrs.reset_index()$ payments_NOT_common_all_yrs.head()
sl[sl.status_binary==0][(sl.today_preds==1)].shape
client = MongoClient()$ db = client.test_database #acessa ou cria o banco$
utils.plot_user_steps(pax_raw, None, 2, 15)
merge_df['Satisfaction'] = pd.qcut(x=merge_df['Score'],labels=[False, True],q=2)
S.decision_obj.simulStart.value = "2007-07-01 00:00"$ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
pd.date_range('1/1/2016', '12/1/2016', freq='BM')
df_schoo11 = df_schools.rename(columns={'name':'school_name'})$ df_schoo11.head()
t2.tweet_id=t2.tweet_id.astype(str)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-09&end_date=2018-08-09&api_key=' + key)$ r.headers['content-type']$ r.json()
sortedprecip_12mo_df.describe()
set(bookings.columns).intersection(set(releases.columns))
df.var()
new_page_converted = np.random.choice([1,0], size=df_newlen, p=[pnew,(1-pnew)])$
arrows = pd.read_csv('input/data/arrow_positions.csv', encoding='utf8', index_col=[0,1])
ranks = dev3[to_rank_cols].apply(lambda x: x.rank(ascending=False), axis=0)
xml_in.head(5)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY$ data = requests.get(url)
df = pd.read_csv('basic_graphics_single_column_data.csv')
mask = percent_quarter.abs().apply(lambda x: x > 1)$ percent_quarter[mask].nlargest(4)
df = pd.read_sql('SELECT * from membership', con=conn_b)$ df
df_chunks = pd.read_csv(LM_PATH/'df.csv', chunksize=chunksize)
pd.concat([msftAV[:5], aaplAV[:3]], axis=1, keys=['MSFT', 'AAPL'])
regr.fit(X_train, y_train)
dfs_morning.loc[dfs_morning['ENTRIES_MORNING']<=0, 'ENTRIES_MORNING'] = dfs_morning.ENTRIES_MORNING.quantile(.5)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY)
plt.bar(x_axis, np.sqrt(np.square(deltas_df['Delta Approval']*.8)), color="blue")$ plt.bar(x_axis, np.sqrt(np.square(deltas_df['Delta Compound']*2)), color="green")$ plt.show()
tweet_df_polarity = my_tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
df_l2.loc[df_l2["CustID"].isin([customer])]
data_read = pd.read_csv("data_music_replay_train")$ data = data_read.copy()
! rm -rf recs1$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models1 --outdir recs1
list(map(range, a.shape))
import pandas as pd$ loctweetdf = tweetdf.loc[~pd.isnull(tweetdf['lat'])]
data.groupby(['Year', 'Department'])['Salary'].sum()
frames=[NameEvents,ValidNameEvents]$ TotalNameEvents = pd.concat(frames)
iris.head().iloc[:,[0]]
import pandas as pd$ bild = pd.io.json.json_normalize(data=bild)$ spon = pd.io.json.json_normalize(data=spon)
from sklearn.metrics import r2_score$ r2_score(y_test, pred)$
pixiedust.enableJobMonitor()$ sqlContext = pixiedust.SQLContext(sc)
pd.to_datetime(['2009/07/31', 'asd'], errors='coerce')
data_issues_transitions=pd.read_csv('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-transitions.csv')
title_sum = preproc_titles.sum(axis=0)$ title_counts_per_word = list(zip(pipe_cv.get_feature_names(), title_sum.A1))$ sorted(title_counts_per_word, key=lambda t: t[1], reverse=True)[:]$
store_items.fillna(method='ffill', axis=1) # filled with previous value from that row
service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'$
pystore.set_path('./pystore_demo')$ pystore.get_path()
a = a.reshape(4, 5)$ a
response = requests.get('http://www.reddit.com/r/aww')$ page_source = response.text$ print(page_source[:1000])
average_trading = statistics.mean([day[6] for day in data])$ print ('Average daily trading volume for 2017:', average_trading)
from keras.preprocessing.text import Tokenizer$ import numpy as np
raw_data_df = pd.read_csv('Daisy Debt.csv', parse_dates=[0]) $ raw_data_df.head()
number_of_commits = len(git_log)$ number_of_authors = git_log.loc[:, 'author'].dropna().nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df = pd.read_pickle('all-RSS.pkl')
net.save_nodes(nodes_file_name='nodes.h5', node_types_file_name='node_types.csv', output_dir=directory_name)
print('Best Score: {}'.format(XGBClassifier.best_ntree_limit))
t1.drop(t1[t1['retweeted_status_id'].notnull()==True].index, inplace=True)
grouped2.size().unstack().plot(kind="bar", stacked=True, figsize=(8,6))$ plt.show()
empDf.join(deptDf, empDf.deptid == deptDf.id).show()
data.name.duplicated(keep=False).sum()
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comments', ''))
start = datetime.now()$ print((datetime.now() - start).seconds)
kick_projects['goal_cat_perc'] =  kick_projects.groupby(['category'])['goal'].transform($                      lambda x: pd.qcut(x, [0, .35, .70, 1.0], labels =[1,2,3]))
np.random.seed(123456)$ ts = Series([1,2,2.5,1.5,0.5],pd.date_range('2014-08-01',periods=5))$ ts
df['y'].plot.box(notch=True, showmeans=True)
max_sharpe_port = results_frame.iloc[results_frame['Sharpe'].idxmax()]$ min_vol_port = results_frame.iloc[results_frame['SD'].idxmin()]
configure['run']['duration'] = 0.5
exiftool -csv -createdate -modifydate cisuabe5/cisuabe5_cycle1.MP4 cisuabe5/cisuabe5_cycle2.MP4 cisuabe5/cisuabe5_cycle3.MP4 cisuabe5/cisuabe5_cycle4.MP4 > cisuabe5.csv
cityID = '1d9a5370a355ab0c'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chicago.append(tweet) 
tweet_df["date"] = pd.to_datetime(tweet_df["date"])$
sentiments_grp = sentiments_pd.groupby("Source")$ aggr_comp_sentiments = sentiments_grp["Compound"].mean()$ aggr_comp_sentiments
numofstations=session.query(Station.station).count()$ numofstations
from sklearn.linear_model import RidgeCV$ rr = RidgeCV(alphas=alphas, fit_intercept=True, normalize=False)$ score_model(rr, alpha=True)
output.shape
for names in asdf:$     name = names.div['data-name']$ name
words = latest_tweet['full_text'].split(' ')
df = pd.read_csv('../Data/bidsmerged_update__2_.csv') $ df.shape$
print('{}index/agencies/{}'.format(base_url, '1')) $ print(json.loads(requests.get('{}index/agencies/{}'.format(base_url, '1')).text))
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/SCN_BDAY_qthis.scn_age.count()
df = df[df['Time_In_Badoo'] <= 1500]$ df.head()
df = all_tables_df.tail()
engine = create_engine("sqlite:///hawaii.sqlite")$
data.isnull().sum()
mgxs_lib.build_hdf5_store(filename='mgxs.h5', directory='mgxs')
grid_lat = np.arange(24, 50.0, 1)$ grid_lon = np.arange(-125.0, -66, 1)$ glons, glats = np.meshgrid(grid_lon, grid_lat)
xres3['hits']['hits']$ pd.DataFrame(xres3['hits']['hits']). ._source$
dummies = pd.get_dummies(plate_appearances['bb_type']).rename(columns=lambda x: 'bb_type_' + str(x))$ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)$ plate_appearances.drop(['bb_type'], inplace=True, axis=1)
tobs_date_df = tobs_date_df.rename(columns={0: "date", 1: "tobs" })$ tobs_date_df.head()
pgh_311_data.resample("Q").mean()
start_date = dt.date(2018, 3, 7)$ end_date = dt.date(2018, 3, 17)
data.info()
df.index
station_data = session.query(Stations).first()$ station_data.__dict__
plt.figure(0)$ source_counts = df['sourceurl'].value_counts().head(10)$ source_counts.plot.bar()
s = pd.Series(np.random.randn(len(rng)).cumsum(), index=rng)$ s.head()
result = cur.fetchall()$
import os$ KAGGLE_USERNAME = os.environ.get("KAGGLE_USERNAME")$ print(KAGGLE_USERNAME)
oz_mask = oz_stops['stop_id'].isin(shared_ids)$ not_in_stops = oz_stops.loc[~oz_mask]$ not_in_stops
df_model = df[['height','profile_popularity','Age','Time_In_Badoo','job_ID','liked']]$ df_model.head()
from bs4 import BeautifulSoup as soup ##BeautifulSoup$$ from urllib.request import urlopen as uReq$ import requests
matt1 = dta.t[(dta.b==40) & (dta.c==1)]$
search['days_plan_ahead'] = (search['trip_start_date'] - search['timestamp']).dt.days+1
%%time $ nlp = spacy.load('en_core_web_lg') 
S_lumpedTopmodel.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
%matplotlib inline$ commits_per_year['author'].plot( kind='line', title='Commits per Year', grid=True)
data_df.iloc[535]
df1.append(df2).append(df3)$
crime = pd.read_csv('clean_data/KCPD_Crime_Data_2017_clean.csv')$ moon = pd.read_csv('clean_data/Moon_Data_2017_cleaned.csv')
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}]$ new_store = pd.DataFrame(new_items, index = ['store 3'])$ new_store
edftocsv.edftocsv(inputFile, outputFileHeaders, outputChanHeaders, outputData, True)
%%time$ tcga_target_gtex_expression_hugo_tpm = tcga_target_gtex_expression_hugo \$     .apply(np.exp2).subtract(0.001).groupby(level=0).aggregate(np.sum).add(0.001).apply(np.log2)
city_pd.replace('',np.nan,inplace=True)$ city_pd.dropna(axis=0,how='any')
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href']
get_nps(combined_df, 'device').sort(columns='score', ascending=False)
measurement = Base.classes.measurements
from sklearn.model_selection import train_test_split$ tips_train, tips_test = train_test_split(tips, test_size=0.2, random_state=123)$ tips_train.shape, tips_test.shape, tips.shape
posts.groupby('from').aggregate(sum)
qw = qgrid.show_grid(all_tables_df, show_toolbar=True)
active_with_return.dropna(inplace = True)
!arvados-cwl-runner --name "Encode Blood Types" encode.cwl --arr ./npy_data/train_data.npy --script just_encode.py
import datetime$ dNow = datetime.datetime.now()$ AcqDate = dNow.strftime("%Y-%m-%d")
y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)$ y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)
!wget -nv https://www.py4e.com/code3/mbox.txt -O mbox.txt
wrsids[np.where(ids=='105001')[0][0]]$ 
vocab = list(model.wv.vocab.keys())$ vocab[:25]
mni =raw.iloc[50610:50619]
stringlike_instance.content = 'changed content'
bgCan = [[ 5.6 , 7.8, 6.0 ], [ 12.2, 4.4, 6.7 ]] # same ndarray as the NumPy lesson$ df = pd.DataFrame(bgCan, index=['Monday', 'Tuesday'], columns=['Breakfast', 'Lunch', 'Dinner'])$ df
df = pd.read_sql('SELECT * from hotel', con=conn_a)$ df
model.save('ubs_model.h5')  
training_set, test_set = newdf[newdf['date']<split_date], newdf[newdf['date']>=split_date]$ training_set = training_set.drop('date', 1)$ test_set = test_set.drop('date', 1)
session.query(Measurement.id, func.max(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
odds = get_cleaned_odds(odds)$ odds.tail()
df_subset.boxplot(column='Initial Cost', by='Borough', rot=90)$ plt.show()
tweets_df.loc[tweets_df.language == 'und', :]$
c = (y - m*x).mean()$ print(c)
state_lookup.info()
store_items.insert(4, 'shoes', [8,5,0])$ store_items
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, wait_on_rate_limit = True)
rides_fare_average_max = rides_analysis["Average Fare"].max()$ rides_fare_average_max
df_first_days['active_user'].value_counts()
merged_data = youTubeTitles.append(pornTitles, ignore_index=True)$ title_matrix= merged_data.loc[:,["title"]]$ target_array=merged_data.loc[:, ["isPorn"]]
sns.heatmap(ndvi_us, vmin = -.1, vmax=1)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closerToBotOrTop'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], 'FromTopWell', 'FromBotWell')
std_df = choose_local_df('STD')$ std_df.loc[std_df['Sold_to_Party']=='0000101348'][['Sold_to_Party','Sales_document','Material_Description','Unit_Price','Document_Date']]
index=pd.date_range('8/14/2017', '12/31/2017')
data_PL.loc[data_PL['energy_source'] == 'Hydro', 'technology'] = 'Pumped storage'
temp_df.plot(kind="hist", bins=12)
sns.pairplot(iris, hue='Species')
cur.execute('SELECT LangCd ,count(*) FROM surveytabl WHERE LangCd="QB";')$ cur.fetchall()
cityID = '01c060cf466c6ce3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Long_Beach.append(tweet) 
table_rows = driver.find_elements_by_tag_name("tbody")[28].find_elements_by_tag_name("tr")$
bg3 = bg2.drop(bg2.index[0]) # drop first row$ bg3
tmax = tmax_day_2018.tmax[0]$ tmax.plot()
questions = pd.concat([questions.drop('attend_with', axis=1), attend_with], axis=1)
ser = pd.Series(['Tues','Wed'], index=days)$ ser
result_new.summary2()
results = pd.read_csv('datasets/results.csv')
precipitation_totals.describe()
max_sharpe_port_b = results_frame_b.iloc[results_frame_b['Sharpe'].idxmax()]$ min_vol_port_b = results_frame_b.iloc[results_frame_b['SD'].idxmin()]
eppd = eppd.merge(cpdi, on='i')$ eppd = eppd.merge(xpdi, on='i')$ eppd.head(40)
DataSet = DataSet[DataSet.userTimezone.notnull()]$ len(DataSet)
df.info()
number_one_df = spotify_df.loc[spotify_df["Position"] == 1,:]
beta_dist[np.arange(mcmc_iters), betas_argmax] *= 10000$ sum(beta_dist)/1000
training_df = features[~features.gameId.isin(production_df.gameId)]
df = build_dataframe('../data/raw/')$ df.info()
data = pandas.read_csv("MSFT.csv", index_col='Date')$ data.index = pandas.to_datetime(data.index)
fsrq = np.where( np.logical_or(table['CLASS1']=='fsrq ',table['CLASS1']=='FSQR '))
bands.to_csv('../data/bands.csv')
digits.target[5]$ clf.predict(digits.data[5:6])
inspector = inspect(engine)$ inspector.get_table_names()
pp.pprint(r.json())
df['date'] = pd.to_datetime(df['date'])
nitrogen['ActivityMediaSubdivisionName'].unique()
open('data/wx/tmy3/proc/700197.csv').readlines()[:6]
td_wdth = td_norm * 5$ td_alph = td_alpha$ td = td.round(1)
data.columns = ['Tweets','len', 'ID','Date', 'Source', 'Likes', 'RTs', 'SA']$ data.to_csv('experiment1.csv', index_label='Index_name')
drop_num.append(19)
col_eliminar = ['bbox_coords','coords_coords','country_code','ext_media_t_co','ext_media_url','symbols']$ tweetsDf = tweetsDf.drop(columns=col_eliminar)
np.shape(prec_us_full)
num_id = df.nunique()['user_id']$ print("{} unique users in the dataset.".format(num_id))
df['job'] = df['job'].fillna(value=0)
df_track.to_sql('track_db', cnx)$ df_artist.loc[:,:].to_sql('artist_db', cnx)
print ("categorical accuracy from mean      :", float(categorical_accuracy(val_labels, val_avg_preds2).eval()))$ print ("best individual categorical accuracy:", np.max(cat_acc))$
multi.handle.value_counts()/multi.shape[0]
my_df[my_df.isnull().any(axis=1)].head()
columnsToDropDuplicates = ['body']$ dfTickets = dfTickets.drop_duplicates(columnsToDropDuplicates)$ print(dfTickets.shape)
data["engagement"] = np.where(data["comms_num"]>500, 1, 0)
normalizedDf = finalDf$ normalizedDf = normalizedDf.drop('kmLabels', axis=1);
ride_df_suburban = suburban.groupby('city')$ city_df_suburban = suburban.set_index('city')
   $ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=amQDZBVZxsNFXgn8Dmpo')$ r.json()['dataset']['data'][1]
data_df['clean_desc'] = data_df.apply(text_clean, axis=1)$ data_df['clean_desc'] = data_df['clean_desc'].astype(unicode)$ data_df['nwords'] = data_df['clean_desc'].str.count(' ').add(1)
prcp_1_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation'])$ prcp_1_df.set_index('Precipitation Date', inplace=True) # Set the index by date$ prcp_1_df.count()
trump = pd.read_csv('data/trump_tweets.csv', header=0, index_col=0)$ trump.head()
print(train_trees[random.randrange(len(train_trees))])
mcg = s4g.groupby(['Symbol', 'Year', 'Month'])
df_z= df_cb.groupby(["landing_page","group"]).count()$ df_z$
df.head(5)
pokemon['Total']= pokemon['HP']+pokemon['Attack']+pokemon['Defense']+pokemon['Sp. Atk']+pokemon['Sp. Def']+pokemon['Speed']$ pokemon.head()
json_data = r.json()$ print(type(json_data))$
table = driver.find_element_by_xpath('//*[@id="body"]/table[2]')$ table.get_attribute('innerHTML').strip()
tempsApr = Series([29, 30, 28, 31, 32], index = pd.date_range('2018-04-20', '2018-04-24'))$ tempsMay = Series([26, 24, 22, 22, 19], index = pd.date_range('2018-05-20', '2018-05-24'))$ tempsMay - tempsApr
cdata.describe()
s.index
df_stock1 = df_stock.filter(['Date', 'Close'], axis=1)$ df_test = df_test.filter(['Date', 'Close'], axis=1)$
trump = trump.drop(["id_str"], axis = 1)
a = np.arange(0.5, 10.5, 0.5)$ a
table_rows = driver.find_elements_by_tag_name("tbody")[12].find_elements_by_tag_name("tr")$
y = df_series#pd.Series(y, index=dates)$ arma_mod = sm.tsa.ARMA(y, order=(2,2))$ arma_res = arma_mod.fit(trend='nc', disp=-1)
from sklearn.feature_selection import VarianceThreshold$ sel = VarianceThreshold(threshold=(0.8 * (1 - 0.8)))
ra_min_ogle_fix = 250.0; ra_max_ogle_fix = 283.0;$ dec_min_ogle_fix = -40.0; dec_max_ogle_fix = -15.0;
df4 = df3.drop_duplicates()$ df4.describe()
model = AuthorTopicModel.load('/tmp/model.atmodel')
nb_pipe.fit(X_train, y_train)$ nb_pipe.score(X_test, y_test)
jdfs.loc[~jdfs.fork]
df = pd.read_csv('data/cornwall_phones.csv')$ df.head()
media_classes = [c for c in df_os.columns if c not in ['domain', 'notes']]$ media_classes
sentiments_pd.count()
population.apply(lambda val: val > 1000000)
fig1, ax1 = plt.subplots()$ ax1.pie(stage_summary['count'], labels = stage_summary['stage']) $ fig1.savefig('Stage Pie Plot')$
tmax_day_2018.attrs
print([x for x in df.columns])
results = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= prev_year).all()$ results[0:10]
p = fp7.append(h2020)$ fp7 = 0$ h2020 = 0
a = np.random.randn(50, 600, 100)$ a.shape
sentiments_pd.to_excel("NewsMood.xlsx", encoding="latin-1") 
crimes.PRIMARY_DESCRIPTION.value_counts()
img_url_valles = soup.find('div', 'downloads').a['href']
model.wv.syn0.shape
data_kb['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_kb['Tweets'] ])$ display(data_kb.head)
song_tracker_grouped_df.to_csv("Desktop/Project-2/song_tracker.csv", index=False, header=True)
freq_station = {'id':"",'name':""}$ freq_station['id'] = active_station_df.iloc[:1]['station'][0]$ freq_station['name'] = active_station_df.iloc[:1]['name'][0]
paragraphs = soup.find_all('div', class_='rollover_description_inner')   $ print(paragraphs)
df_low_temps.describe()$
criteria = so['ans_name'] == 'Scott Boston'$ so[criteria].head()
print(data["Ganhadores_Sena"].sum())$ print(data["Ganhadores_Quina"].sum())$ print(data["Ganhadores_Quadra"].sum())
hit_tracker_grouped_df.to_csv("Desktop/Project-2/hit_tracker.csv", index=False, header=True)
auth = tweepy.OAuthHandler(consumerKey, consumerSecret)$ api = tweepy.API(auth)
for v in contin_vars:$     joined[v] = joined[v].fillna(0).astype('float32')$
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"$ date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"
store_items.fillna(method = 'backfill', axis = 1)
print(data_all[data_all < 101].describe())
stations_df.count()
hawaii_df.describe()
bc['newdate'] = pd.DatetimeIndex(bc.date).normalize()
pca = PCA(2)$ X = pca.fit_transform(new_df.ix[:,1:20])$
X = [re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', string) for string in sample]
weather_df.is_copy = False$ weather_df["Time of retrieval"] = [datetime.fromtimestamp(d) for d in weather_df["Time of retrieval"]]$ weather_df.head()
%matplotlib inline$ commits_per_year.plot(kind='line', title='History of Linux', legend=False)
month = pd.get_dummies(questions['month_bought'])
df_combined['won'] = (df_combined['home_score'] - df_combined['away_score']) > 1$ df_combined.drop(['date', 'city', 'country','home_score','away_score'], axis=1, inplace=True)
pd.crosstab(df_result.launched_year, df_result.State).plot.bar()
DummyDataframe2 = DummyDataframe[["Tokens","Token_Count"]].copy()$ DummyDataframe2 = DummyDataframe2[["Tokens","Token_Count"]].groupby('Date').agg({'Tokens': 'sum', 'Token_Count': 'sum'})$ DummyDataframe2
new_ticket = 'It seems like I have followed the set up directions but I see the set up WEPAY \$                 and am not sure if I have completed everything..thank you'$
print total.shape[0], total.dropna().shape[0]$ total.head()
corrected_log.sort_values('timestamp').describe()
%time train_4_reduced = tsvd.transform(train_4)
cityID = '0a0de7bd49ef942d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Scottsdale.append(tweet) 
data = utils.load_data()
pp.pprint(tweet._json)
X = preprocessing.scale(X)$
test = test.drop(columns="id")$ (train.shape, test.shape, submit.shape)
tree_features_df['p_hash'].isin(manager.image_df['p_hash']).describe()$
np.all(x < 10)
pd.read_csv("../../data/msft.csv", skiprows=100, nrows=5, header=0,names=['open','high','low','close','vol','adjclose'])
modCrimeData = crimeData$ modCrimeData.set_index("Year", inplace=True)$ modCrimeData.head()
mydata.head(3)
news_df['Timestamp'] = pd.to_datetime(news_df['Date'], infer_datetime_format=True)$ news_df.head()
urls_to_shorten = [link for link in urls if ux.is_short(link)]$ urls_to_shorten
df1 = add_percentiles(df)$ df1.head()
Measurements = Base.classes.measurement$ Stations = Base.classes.station$ Hawaii = Base.classes.hawaii
df['launched_year'] = df['launched'].dt.year$ df['deadline_year'] = df['deadline'].dt.year
driver.get("http://www.reddit.com")$ time.sleep(1)
print(type(plan["plan"]))$ print(plan['plan'].keys())
sentiment.to_csv("sentiment_by_vader.csv", index=False, header=True)$ watson_df.to_csv("sentiment_by_watson.csv",index=False,header=True)$  $
useful_indeed = indeed[~indeed['salary_clean'].isnull()]$ useful_indeed.shape$
no_hyph = df_nona[df_nona['variety']\$                     .apply(lambda x: len(x.split('-')) < 2)]['variety'].str.lower()$ no_hyph = no_hyph[no_hyph.apply(lambda x: x.split()[-1] != 'blend')].replace(repl_dir)
qrtSurge = ((qrt.shift(-3)- qrt) / qrt )$ surge = qrtSurge[qrtSurge>1]$ surge.sort_values(ascending=False)$
print(gdp_df.head())$ print(gdp_df.GDPC1.head())$ gdp_df.GDPC1.plot()
res3 = rs.post('http://bsr.twse.com.tw/bshtm/bsMenu.aspx', headers = headers, data = payload)$
filtered_df = data_df.drop(labels=['id', 'fst_subject', 'fst_tutor_type'], axis=1)
print(AFX_X_06082017_r.json())
pre_name=list(twitter[twitter.name==twitter.name.str.lower()].name)$ for item in pre_name:$     t1['name'].replace(item, 'None', inplace=True)
set(hourly_dat.columns) - set(out_temp_columns+incremental_precip_columns+general_data_columns+ wind_dir_columns)
mfx = sts_model.get_margeff()$ print(mfx.summary())
crypto_data.head(5)
display(data.head(4))
X_new = test_tfidf.loc[:207]$ new_x = X_new.iloc[:,index_smote]$ new_y = test_tfidf['y']
test_classifier('c1', WATSON_CLASSIFIER_ID_2)$ plt.plot(classifier_stats['c1'], 'ro')$ plt.show()
df1 = df[df['Title'].str.contains(search_terms)]$ df1.shape
HOU_analysis = team_analysis.get_group("HOU").groupby("Category") # Pulls only team transactions from sample, then groups$
raw_freeview_df, raw_fix_count_df = condition_df.get_condition_df(data=(etsamples_grid,etmsgs_grid,etevents_grid), condition='FREEVIEW')
from sqlalchemy import distinct$ number_of_stations = session.query(func.count(distinct(Measurement.station))).scalar()$ print(f'The total number of stations : {number_of_stations}')
y = list(train_50m_ag.is_attributed)$ X = train_50m_ag.drop(['is_attributed'],axis=1)$ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)
df['full_text'] = df['full_text'].apply(lambda x: x.replace('\r', ' '))$ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\n', ' '))$ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\t', ' '))
y_pred = logreg.predict(X_test)$ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))
row_slice = (slice(None), slice(None), 'Bob')  # all years, all visits, of Bob$ health_data_row.loc[row_slice, 'HR']$
df = pd.DataFrame([tweet.text for tweet in tweets], columns=['Tweets'])$ df.head(n=10)
t1= twitter.copy()$ t2 = pred.copy()$ t3 = tweet_df.copy()
s_filled.plot()$ plt.show()
merged[['CA', 'UK', 'US']] = pd.get_dummies(merged['country'])$ merged.head()
df_vow.plot()$ df_vow[['Open','Close','High','Low']].plot()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
type(t1.tweet_id.iloc[3])$
table.info('stats')
df2[df2.duplicated(['user_id'], keep=False)]
text_df.head()
median_Task = sample['num_completed_tasks'].median()$ median_Task
def split_data(x,y):$     x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, shuffle=False)$     return x_train, x_test, y_train, y_test
file_names = []$ file_names = glob.glob('*.csv')
!hdfs dfs -put ProductPurchaseData.txt ProductPurchaseData.txt
csvpath2 = os.path.join('Desktop', 'Project-2', 'numberOneUnique.csv')$ import csv$ numberOneUnique_df = pd.read_csv(csvpath2, encoding="ISO-8859-1")
cand.CAND_ST.value_counts()
df.loc[index_name]
load2017.columns = ['time', 'day-ahead', 'actual'] $ load2017.head()
import pickle$ filename = 'models/sentiment_model.sav'$ pickle.dump(sentiment_model, open(filename, 'wb'))
pf.cost.sum()/100
df1 = pd.DataFrame({'full_text': full_text})$ combined_df = pd.concat([df_urls, df1], axis=1, join_axes=[df_urls.index])$ data = combined_df.drop(['source','type_material','headline','url'],axis=1)
t2['p1'] = t2['p1'].str.replace('_', ' ')$ t2['p2'] = t2['p2'].str.replace('_', ' ')$ t2['p3'] = t2['p3'].str.replace('_', ' ')
babies.head()
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['tweets'] ])$ display(data.head(5))
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','ca','uk']])$ results = logit_mod.fit()
col_names = list(zip(df_test.columns, df_train.columns))$ for cn in col_names:$     assert cn[0] == cn[1]
station_count = session.query(func.count(Station.id)).all()$ print(station_count)
simple_resistance_simulation_1 = sim_ET_Combine['simResist(Root Exp = 1.0)']$ simple_resistance_simulation_0_5 = sim_ET_Combine['simResist(Root Exp = 0.5)']$ simple_resistance_simulation_0_25 = sim_ET_Combine['simResist(Root Exp = 0.25)']
sns.heatmap(df.corr())$ plt.show()
calls.head()
data.to_csv('TwitterSentimentData.csv')
kochdf.loc[kochdf['date'] == max_date]
xml_in[xml_in['publicationDate'].isnull()].count()
ice = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with ice.open('inpt-forecasting/Inpatient Census extract - PHSEDW 71118.csv') as ice_f:$   ice_df = pd.read_csv(ice_f)
pyber_df.head()$
ml.confusion(dep_test.reshape(dep_test.shape[0]), $              predicted, ['No', 'Yes'], 2, 'Smoker Classification [Numeric & Categoric]')
df_sb = pd.read_csv("sobeys_all.csv", encoding="latin-1")
import gensim, logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
df = df.sort_values('label', ascending = True)$ df.head()
numPurchU = train.groupby(by='User_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesU'})$ train = train.merge(numPurchU, on='User_ID', how='left')$ test = test.merge(numPurchU, on= 'User_ID', how='left')
training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=42)
my_gempro.blast_seqs_to_pdb(all_genes=True, seq_ident_cutoff=.7, evalue=0.00001)$ my_gempro.df_pdb_blast.head(2)
us_grid = np.array(np.meshgrid(grid_lon, grid_lat)).reshape(2, -1).T$ np.shape(us_grid)
sentiment_df = pd.DataFrame(save_sentiment)$ sentiment_df.to_csv('sentiment.csv', sep=',', header=True, index=True, index_label=None)$ sentiment_df
pmean = np.mean([p_new,p_old])$ round (pmean, 4)
words_hash_scrape = [term for term in words_scrape if term.startswith('#')]$ corpus_tweets_scraped.append(('hashtags', len(words_hash_scrape))) # update corpus comparison$ print('Total number of hashtags: ', len(words_hash_scrape)) #, set(terms_hash_stream))
i=random.randrange(len(train_trees))$ print(train_trees[i])
from quantopian.pipeline.data import Fundamentals$ exchange = Fundamentals.exchange_id.latest
apple_data = google_stocks('AAPL')$ apple_data.columns$ apple_data.index
import builtins$ builtins.uclresearch_topic = 'GIVENCHY' # 226984 entires$ from configuration import config
data = r.json()
foursquare_data_dict['response'].items()[2][1][0]
df.to_csv('Tableau-CitiBike/TripData_2017_Fall.csv', index=False)
lv_workspace.get_subset_object('B').get_step_object('step_1').show_settings()$ lv_workspace.get_subset_object('B').get_data_filter_object(step=1).exclude_list_filter
dfClientes.loc[dfClientes["MES"] == 201701].head()
S.decision_obj.simulStart.value = "2006-07-01 00:00"$ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
tdf = sns.load_dataset('tips')$ tdf['data']= 1$ tdf.sample(5)
ls_data = pd.read_excel(cwd+'\\LS_mail_0604_from_python.xlsx')$ ls_columns=ls_data.columns.values
session.query(measurements.date)[-1]
year_with_most_commits=commits_per_year[commits_per_year == commits_per_year.max()].sort_values(by='num_commits').head(1).reset_index()['timestamp'].dt.year$ year_with_most_commits='2016'$ print(year_with_most_commits)
data.to_json('nytimes_oped_articles.json')
sentiments_df = pd.DataFrame.from_dict(sentiments)$ unique_sentiments_df = sentiments_df.drop_duplicates("Tweet Text", keep = "first")$ unique_sentiments_df
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])$
graph.run("CREATE CONSTRAINT ON (u:User) ASSERT u.user_name IS UNIQUE;")$ graph.run("CREATE CONSTRAINT ON (t:Tweet) ASSERT t.tweet_id IS UNIQUE;")$ graph.run("CREATE CONSTRAINT ON (h:Hashtag) ASSERT h.tag IS UNIQUE;")
pd.Timestamp('2014-12-14 17:30')
job_a_requirements = pd.DataFrame(requirements, columns = ['Job_A'])$ job_a_requirements
S_distributedTopmodel.initial_cond.filename
conf = SparkConf().setAll([('spark.executor.memory', '6g'), ('spark.executor.cores', '6'), ('spark.cores.max', '6'), ('spark.sql.session.timeZone', 'UTC')])$ sc = SparkContext("local", "ib", conf=conf)
adj_close_acq_date['Date Delta'] = adj_close_acq_date['Date'] - adj_close_acq_date['Acquisition Date']$ adj_close_acq_date['Date Delta'] = adj_close_acq_date[['Date Delta']].apply(pd.to_numeric)  $ adj_close_acq_date.head()
data_l2_end = tmpdf.index[tmpdf[tmpdf.isin(DATA_SUM1_KEYS)].notnull().any(axis=1)].tolist()$ data_l2_end
cnct = pd.Series(df.Title.values,index=df.index).to_dict()$ EEdgeDF['From'] = EEdgeDF['From'].map(cnct)$ EEdgeDF.head(7)
engine = create_engine("sqlite:///./Resources/hawaii.sqlite", echo=False)
print(voters.PermCategory.unique())$ voters.PermCategory.value_counts(dropna=False)
period_df.iloc[0]['url']$ period_df.iloc[0]['time']$ test_image, test_image_url, test_datetime = download_single_snapshot(period_df.iloc[0]['url'], period_df.iloc[0]['time'], workspace_dir)
print("The tweet with more retweets is: \n{}".format(data['Tweets'][rt]))$ print("Number of retweets: {}".format(rt_max))$ print("{} characters.\n".format(data['len'][rt]))
import nltk$ nltk.download('punkt')$ from nltk.tokenize import word_tokenize
df1_normal = df1.copy()$     $ df1_normal['y'] = np.log(df1_normal['y'])
%bash$ gsutil cat "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" | head
with open('/Users/ianbury/Springboard/springboard/quandl_api_key','r') as file:$     API_KEY = file.read()$
y_pred = pipe_nb.predict(pulledTweets_df.emoji_enc_text)$ y_proba = pipe_nb.predict_proba(pulledTweets_df.emoji_enc_text)$ pulledTweets_df['sentiment_predicted_nb']=[classes[y_pred[i]] for i in range(len(y_pred))]
tranny = df.T$ tranny
x = api.GetUserTimeline(screen_name="berniesanders", count=20, include_rts=False)$ x = [_.AsDict() for _ in x]
campaigns_under_50 = marketing_sources_active_users[marketing_sources_active_users['user_id']<50].index.tolist()$ df_first_days = df_first_days[~df_first_days['marketing_source'].isin(campaigns_under_50 )]
bottom_views = doctype_by_day.loc[:,doctype_by_day.max() < 10]$ ax = bottom_views.plot()$ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
print(X.shape,y.shape)$ print(X_train.shape,y_train.shape)$ print(X_test.shape,y_test.shape)
msftA = msft[['Adj Close']] $ closes = pd.concat([msftA, aaplA], axis=1)$ closes[:3]
from sqlalchemy.ext.declarative import declarative_base$ from sqlalchemy import Column, Integer, String, Float 
days_alive = (datetime.datetime.today() - datetime.datetime(1981, 6, 11))$ days_alive.days
!h5ls -r 'data/my_pytables_file.h5'
for tweet in query:$     if replies_donald.get(tweet.in_reply_to_status_id_str) != None:$
result = api.search(q='%23flu') #%23 is used to specify '#'$ len(result)
raw_data = raw_data.loc[raw_data['date'] >= pd.to_datetime(start_date)]$ raw_data = raw_data.loc[raw_data['date'] <= pd.to_datetime(end_date)]
local.export_to_quilt(post_process_info["DatasetId"])
result.street_number
graf['DETAILS2']=graf['DETAILS'].progress_apply(text_process)
data = pd.read_csv('Dumping.csv', delimiter = ',', skiprows=0, squeeze=False, skip_blank_lines=True, index_col=None)$
data[['TMAX', 'TMED']].head()
data.tasker_id.value_counts().head()
news_sentiments.to_csv("News_Sentiments.csv")
for i in range(len(tweets['Tweet'])):$     tweets['Tweet'][i] = " ".join([word for word in tweets['Tweet'][i].split()$                                 if 'http' not in word and '@' not in word and '<' not in word and '#' not in word and '\\xf' not in word])
top50 = pd.read_csv('../top50visited.csv', sep=';')$ top50['dataset_slug'] = [x.split('www.data.gouv.fr/datasets/')[1] for x in top50.dataset]$ top50['dataset_id'] = top50.dataset_slug.map(datasets_slug_id)
df['Category']=df['Memo'].apply(returnCategory)$ df['Single Name']=df['Name'].apply(returnName)$ df.head()
print("State space samples:")$ print(np.array([env.observation_space.sample() for i in range(10)]))
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig8_right_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
trump_tweets=pd.read_csv('Resource_CSVs/Twitter_RawData.csv')$ type(trump_tweets)$ trump_tweets
f1_score(Y_valid_lab, val_pred_svm, average='weighted', labels=np.unique(val_pred_svm))
words_mention_sk = [term for term in words_sk if term.startswith('@')]$ corpus_tweets_streamed_keyword.append(('mentions', len(words_mention_sk))) # update corpus comparison$ print('List and total number of mentions: ', len(set(words_mention_sk))) #, set(terms_mention_stream))
precip_df = precip_df.sort_values(by = 'Date')
for columns in DummyDataframe:$     basic_plot_generator(columns, "Graphing Dummy Data" ,DummyDataframe.index, DummyDataframe)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path)$ df.head(5)
afx_x_2017 = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY)
reviews = np.array(tf.review)$ reviews_vector = vectorizer.transform(reviews)$ predictions = clf.predict(reviews_vector)$
output_fn = "News_Sentiment_Analysis.csv"$ sentiment_df.to_csv(output_fn)
cur.execute('INSERT INTO materials VALUES ("EVA", "ethylene vinyl acetate", 0.123, 4.56, "polymer")')$ conn.commit()  # you must commit for it to become permanent$ cur.rowcount  # tells you how many rows written, sometimes, it's quirky
dbData.head(7)  # NaN's show up when the field has no data.  Need both masses, eccentricity, semimajor axis, $
df = pd.read_csv('jamesblanchard_tweets.csv')$ print(df)
sdf.createOrReplaceTempView("tempTable")$ res.show()
type(data.Likes.value_counts())
locations = session.query(Measurement).group_by(Measurement.station).count()$ print(f"There are",locations,"stations.")
from gensim.models import Word2Vec$ model = Word2Vec.load("300features_40minwords_10context")
times = bird_data.timestamp[bird_data.bird_name == "Eric"]$ elapsed_time = [time-times[0] for time in times]$ print(elapsed_time[:10])
df = pd.read_csv('data/test1.csv', parse_dates=['date'])$ df
lm.summary()
df.isnull().sum()[df.isnull().sum() > 0]
df2.sort_values('total_comments',inplace=True,ascending=False)$ top_comments=df2.head(10)$ top_comments
CON = CON.sort_values(by = ['Contact_ID','OppName'])
print(data.json())
title_sum = preproc_titles.sum(axis=0)$ title_counts_per_word = list(zip(pipe_cv.get_feature_names(), title_sum.A1))$ sorted(title_counts_per_word, key=lambda t: t[1], reverse=True)[:]$
number_one_charts_df.to_csv("Desktop/Project-2/number_one_chart.csv", index=False, header=True)
columns_chosen = ['Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close','Adj. Volume']$ df = df[columns_chosen]
ma_mov_idx = ma.array(mov_ids, mask = mov_vec)$ mov_idx = ma_mov_idx[~ma_mov_idx.mask]        
df['State'][df.CustomerID=='0000114883']='MA'
pd.to_datetime(df.time.unique(), unit='s')
df_students['passing_reading'] = df_students.apply(passing_reading, axis = 1) $ df_students['passing_math'] = df_students.apply(passing_math, axis=1)$ df_students.head()
df = pd.read_csv("contact.csv", index_col=None) 
daily_df['Price_Change'].value_counts()
S_1dRichards.meta_basinvar.filename
locations = soup.find_all(class_='sl-spot-details__name')$ locations
data_ps = pd.DataFrame(data=[tweet.text for tweet in tweets_ps], columns=['Tweets'])$ display(data_ps.head(10))
df_concat = pd.concat([bild, spon]) $
crypto_combined = pd.concat([crypto_data, crypto_ggtrends], axis=1).dropna(how='any')   ### Remove NaN $ crypto_combined_s = crypto_combined.copy(deep=True)$ print(crypto_combined_s.head(5))
df[0].plot()
tweets_df.tail()
df_template = pd.DataFrame(index=datetimes)$ df_template.index.name = 'dt'
from pandas.plotting import autocorrelation_plot$ autocorrelation_plot(CH_electric['Total_Demand_KW'])
coinbase_btc_eur_min=coinbase_btc_eur.groupby('Timestamp', as_index=False).agg({'Coin_price_EUR':'mean', 'Coin_volume':'sum'})
crypto_data.tail(5)
x.mean(axis=0)
from sklearn.feature_extraction.text import CountVectorizer # Import the library to vectorize the text$ count_vect = CountVectorizer(ngram_range=(1,3),stop_words='english')$ count_vectorized = count_vect.fit_transform(df_train.text)
import ssbio.core.io$ my_saved_gempro = ssbio.core.io.load_json('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.json', decompression=False)
data_FR[data_FR.duplicated(subset='name', keep=False)]
df_members = pd.read_csv('members.csv', encoding = 'latin-1')$ df_members['joined'] = pd.to_datetime(df_members['joined'], yearfirst = True)$ df_members.head()
rt_set = df2['text'].value_counts().index.tolist()$ rt_set_vals = df2['text'].value_counts().values.tolist()$
header_names = {'1. symbol':'sym',$                 '2. price':'price_str', $                 '3. volume':'vol'}
df = pd.read_sql(SQL, db)$ df
btc.plot(y=['price'])$ plt.show()
url = 'https://mars.nasa.gov/news/'
df = df[(df['Age_Years'] <= 30) & (df['Age_Years'] >= 20)]$ df.head()
min(ORDER_BPAIR_POSTGEN['order_number'].astype(int))
prcp_analysis_df.set_index(['Date'], inplace=True)$ prcp_analysis_df.head()
df = pd.read_csv('anova_one_factor.csv')
df = pd.read_sql('SELECT * from room_type', con=conn_b)$ df
(df.isnull().sum() / df.shape[0]).sort_values(ascending=False)   # credit Ben shaver
spark.stop()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()
df_tte_all = pd.concat([df_tte_ri,df_tte_ondemand])
! cp Observatory_Sauk_Incubator.ipynb /home/jovyan/work/notebooks/data/Incubating-a-DREAM/Sauk_JupyterNotebooks
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week))$ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week))
t1.head(5)
measurements = "Resources/data/hawaii_measurements.csv"
tmax_day_2018 = xr.open_dataset('/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/maximum temperature/tmax.2018.nc', decode_times = False)
tweets['location'] = tweets['location'].str.strip()$ tweets.groupby(tweets.location).count()['id'].sort_values(ascending=False)$
QUIDS = pd.read_table("qids01.txt", skiprows = [1])
r2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=%s' % API_KEY)
for post in posts.find():$     print(post)
cityID = '5c2b5e46ab891f07'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Las_Vegas.append(tweet) 
cityID = '00ae272d6d0d28fe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lexington_Fayette.append(tweet) 
game_data_all.shape
assert pd.notnull(ebola).all().all()
%%time$ model = AlternatingLeastSquares(use_gpu=True)$ model.fit(matrix_data)
company_df.to_csv('companies.csv')
fetch_measurements('http://archive.luftdaten.info/2015-05-09/')
model_x = sm.formula.ols('y ~ C(x)', data = df).fit()$ anova_x_table = sm.stats.anova_lm(model_x, typ = 1)$ anova_x_table.round(3)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ca','uk']])$ results = logit_mod.fit()
movies.to_csv('..\\Output\\CleanedMovies.csv')
means = df_lg_hubs.mean()$ default_use = [means[str(i)] for i in range(1, 13)]$ default_use
IQR = scores_thirdq - scores_firstq$ print('The IQR of the ratings is {}.'.format(IQR))
finals[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 1)].PLAYER_NAME.value_counts()
t2.tail(10)
fin_r_monthly = fin_r_monthly.iloc[:-1]
combined_df.to_csv('kaggle_results.csv', index=False)
c_df = new_df.dropna(how='all') $ c_df.size
tm_2040 = pd.read_csv('input/data/trans_2040_m.csv', encoding='utf8', index_col=0)
lda_model = LatentDirichletAllocation(n_components=15, random_state=42, learning_method = 'batch')$ train_lda_features = lda_model.fit_transform(tfidf_vectorized)$ np.mean(np.max(train_lda_features, axis=1)) #15
with open('united_list_lower.pickle', 'rb') as f:$     united_list_lower = pickle.load(f)
merged1.columns
workspace_uuid = ekos.get_unique_id_for_alias(user_id, 'lena_newdata')$
df_prec = df_prec.set_index('date',drop=True)$ display(df_prec.head())$ display(df_prec.shape)
google_stock.describe()
df['Address1'].fillna("895 SABINE ST", inplace = True)
top_10_authors = pd.value_counts(git_log['author'])$ top_10_authors = top_10_authors.head(10)$ print(top_10_authors)
conn = sqlite3.connect("geo.db")$
a.iloc[3]
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url)$
df_unique_providers = df_unique_providers.reset_index(drop=True)$ df_unique_providers.head()
average_math_score = df_students['math_score'].mean()$ average_math_score
rtc = rtc.set_index('Export Region')$ rtc = rtc.drop(['Year'], axis=1)
pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])$
test = all_sets.cards["XLN"]$ test.loc["Search for Azcanta", ["manaCost", "types", "printings"]]
df_sentiment_means = pd.DataFrame(news_sentiment_means)$ df_sentiment_means.head(10)
df_Tesla['tokens'] =stemmed$ df_Tesla.head(2)
df_new_conv = df_newpage.query('converted == "1"')$ x_new_conv = df_new_conv["user_id"].count()$
trading_volume = [x[6] for x in data_table if x[6] is not None]$ print('Average Trading Volume: {:f}'.format(sum(trading_volume) / len(trading_volume)))
AAPL_array=df["NASDAQ.AAPL"].dropna().as_matrix()$ model_arima = ARIMA(AAPL_array, (2,2,2)).fit()$ print(model_arima.params)
df['Forecast'] = np.nan
date_df = pd.DataFrame(normals, columns=('tmin','tavg','tmax'))$ date_df['date'] = trip_dates$ date_df.set_index('date')
tl_2030 = pd.read_csv('input/data/trans_2030_ls.csv', encoding='utf8', index_col=0)
Inspection_duplicates = data_FCInspevnt_latest.groupby(['brkey'])[['Inspection_number']].sum()$ Inspection_duplicates = Inspection_duplicates.loc[Inspection_duplicates['Inspection_number'] > 1]$ Inspection_duplicates
h = plt.hist(tag_degrees.values(), 100) #Display histogram of node degrees in 100 bins$ plt.loglog(h[1][1:],h[0]) #Plot same histogram in log-log space
df.plot()$ plt.show()
df_newpage = df2.query('landing_page =="new_page"')$ x_newpage = df_newpage["user_id"].count()$
fcc_nn = indexed.loc[indexed['parent_id'] == 't3_7ej943'].reset_index()$
data = []$ for row in result_proxy:$     data.append({'date': row.measurements.date, 'tobs': row.measurements.tobs})
df_movies.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/movies_v1.csv', sep=',', encoding='utf-8', header=True)
!hdfs dfs -put Consumer_Complaints.csv {HDFS_DIR}/Consumer_Complaints.csv
from sklearn.cluster import AgglomerativeClustering$ agg = AgglomerativeClustering(n_clusters=3, affinity='precomputed',linkage='average')$
df_con_control = df_con1.query('group =="control"')$ x_control = df_con_control["user_id"].count()$ x_control$
Val_eddyFlux = Plotting(hs_path+'/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
np.all(x < 8, axis=1)
tweet_df.to_csv('recent_news_tweets.csv')
financial_crisis.drop('Spain defaults 7x', inplace = True)$ print(financial_crisis)
sims = gensim.similarities.docsim.Similarity( 'shard', tf_idf[corpus],$                                              num_features=len(dictionary), num_best= 10)$ print(sims)
year14 = driver.find_elements_by_class_name('yr-button')[13]$ year14.click()
date = dsolar_df['Date']$ date = strptime(string, "%d %b %Y  %H:%M:%S.%f")$
data_path = os.path.join(os.getcwd(), 'datasets', 'cryptocurrencyhistoricaldata','ethereum.csv')$ train_A = pd.read_csv(data_path, delimiter = ';')
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="vegan").items(max_tweets):$     print(tweet)
CNN = news_df.loc[(news_df["Source Account"] == "CNN")]$ CNN.head(2)
temp_long_df.describe()
waihee_tobs = session.query(Measurement.tobs).\$ filter(Measurement.station == "USC00519281", Measurement.station == Station.station, Measurement.date >="2017-07-29", Measurement.date <="2018-07-29").\$ all()
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)$ training_loss = model_with_loss.get_latest_training_loss()$ print(training_loss)
afx_1d = requests.get(("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json" +$                        "?start_date=2018-08-01&end_date=2018-08-01&api_key={}").format(API_KEY)).json()
str(containers[0].find("li", {"class":"transaction"}).a["title"])
data = pd.read_csv('./fake_company.csv')$ data
plt.rcParams['figure.figsize'] = 8, 6 $ plt.rcParams['font.size'] = 12$ viz_importance(rf_reg, wine.columns[:-1])
q1_url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31?api_key="+API_KEY$ r1 = requests.get(q1_url)$ r1 = r1.json()$
cris = CrisAI()$ cris.collect_cris_datasets(["tests/test_data/acoustic.zip", "tests/test_data/acoustic2.zip"])$ cris.SpeechCollector.audio_collection$
!hdfs dfs -cat {HDFS_DIR}/p32c-output/part-0000* > p32c_results.txt
Temperature_DF.plot.hist(by="Tobs",bins=12,title="Observed Temperature from %s to %s"%(start_date,end_date))$ plt.ylabel("Frequency of Temperature")$ plt.show()
dictionary = corpora.Dictionary(texts)$ dictionary
recent_date = session.query(Measures.date).order_by('Measures.date desc').first()$ recent_date = recent_date[0]$ prior_date = get_prior_years_date(recent_date,1)
from datetime import datetime$ xml_in['days_diff_to_publication'] = (datetime.now() - xml_in['publicationDate']).astype('timedelta64[D]')
df.columns = df.columns.str.replace('[', '')$ df.columns = df.columns.str.replace(']', '')$ df.columns = df.columns.str.replace(' ', '_')
engine.execute("SELECT count(station), station FROM measurement GROUP BY station ORDER BY count(station) DESC").fetchall()
news9= ('Iraq backed a proposal from Saudi Arabia and Russia to extend output cuts for nine months, removing one of the last remaining obstacles to an agreement at the OPEC meeting in Vienna this week.'$ 'Iraq has the worst record of compliance with its pledged cuts, pumping about 80,000 more barrels of oil a day than permitted during the first quarter. If that deal gets extended to 2018, the nation will have even less incentive to comply because capacity at key southern fields is expanding and three years of fighting Islamic State has left it drowning in debt.'$
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()$
predicted_probs_first_measure.hist(bins=50)
model_filename = 'models/finalized_mpg_estimation_model.sav'$ loaded_mpg_estimation_model = pickle.load(open(model_filename, 'rb'))$
cursor.fetchall()
version = str(int(time.time()))$ database_log('Daily_Stock_Prediction', version, float(auc), float(build_time))
theft.loc[12]
spreadsheet = '1LTXIPNb7MX0qEOU_DbBKC-OwE080kyRvt-i_ejFM-Yg'$ wks_name = 'CleanedData'$ d2g.upload(df_dn,spreadsheet,wks_name,col_names=True,clean=True)
qt_convrates_toClosedWon.applymap(lambda x: "{0:.2f}%".format(x * 100.00))$
weather_df_rsmpld = weather_df_byday.resample('1M').mean()$ weather_df_rsmpld.head()
all_cards = all_cards[~all_cards.index.duplicated(keep = "first")]
for col in merge_df.columns: $     print(merge_df[col].describe())
pd.io.json.json_normalize(playlist['tracks']['items'][2])
with open(output_folder+Company_Name+"_Forecast_"+datetime.datetime.today().strftime("%m_%d_%Y")+".pkl",'wb') as fp:$     pickle.dump(final_data,fp)
donald_trump_tweets['screen_name'].value_counts()
d = {'id': ids, 'sentiment': solution}$ output = pd.DataFrame(data=d)$ output.to_csv( "Word2Vec_Keras_AverageVectors.csv", index=False, quoting=3 )
RegO = pd.to_datetime(voters.RegDateOriginal.map(lambda x: x.replace(' 0:00', '')))$ Reg = pd.to_datetime(voters.RegDate.map(lambda x: x.replace(' 0:00', '')))$ datecomp = pd.DataFrame({'OrigRegDate':RegO, 'RegDate':Reg})
pd.date_range('1/1/2017', '12/1/2017', freq='BM')$
results_ball_rootDistExp, output_ball_rootDistExp = S.execute(run_suffix="ball_rootDistExp", run_option = 'local')
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\metrics.sas7bdat"$ df = pd.read_sas(path)$ df.head(5)
print('Best score for data:',np.mean(forest_clf.feature_importances_))
cust_vecs[0:,].dot(item_vecs).toarray()[0,:5]
(p_diffs>p_diff_abdata).mean()
dat=dat_before_fill.copy() #reset 'dat' to be un-interpolated data$ for temp_col in temp_columns:$     dat.loc[:,temp_col]=dat[temp_col].interpolate(method='linear', limit=12) #<= 3hrs;  at a 15 min interval, 3 hrs is 12 measurements
fe.bs.bootshow(256, poparr2, repeat=3)$
INQ2016.Create_Date.dt.month.value_counts().sort_index()
df = pd.read_csv("FuelConsumption.csv")$ df.head()$
from app.util import data_range$ data_range()
twelve_months = session.query(Measurements.date, Measurements.prcp).filter(Measurements.date > year_before)$ twelve_months_prcp = pd.read_sql_query(twelve_months.statement, engine, index_col = 'date')
table = Table.read("../datasets/catalogs/fermi/gll_psc_v16.fit.gz")$
data_df.groupby('nwords')['ticket_id'].nunique()
QUIDS_wide["y"] = QUIDS_wide[['qstot_12','qstot_14']].apply(lambda x: x['qstot_14'] if np.isnan(x['qstot_12'])$                                                             else x['qstot_12'], axis=1)
data.groupby('affair').mean()$
dfd = dfh[dfh['Centrally Ducted or Ductless'].str.startswith('Ductless')]$ print(len(dfd))$ dfd['Centrally Ducted or Ductless'].unique()
ISR_df = ISR_df.merge(ISR_df_agg, how='left', on='student_program')$ ISR_df.drop_duplicates('Student_Section__c', inplace=True)$ df = df.merge(ISR_df[['Section__c', 'Student__c'] + list(ISR_df_agg.columns)], on='Section__c')
df.reorder_levels(order=['Date', 'Store', 'Category', 'Subcategory', 'UPC EAN', 'Description']).head(3)
metadata_df_all['type'] = [t.lower() for t in metadata_df_all['type']]$ metadata_df_all['bill_id'] = metadata_df_all['type'] + metadata_df_all['number'].map(int).map(str)$ metadata_df_all = metadata_df_all[['accessids', 'bill_id']]
x["sum"]=x[list(x.columns)].sum()$ x
full_image_elem = browser.find_by_id('full_image')$ full_image_elem.click()
engine=create_engine(seng)$ data3 = pd.read_sql_query('describe actor;', engine)$ data3
q = pd.Period('2017Q1',freq='Q-JAN')$ q=q.asfreq('M',how="start")$ q
calls_nocontact.ticket_status.value_counts()
train, test = data.randomSplit([0.8,0.2], seed=6)$ train.cache()$ test.cache()
Base.classes.stations
session.query(Measurement.id, func.avg(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
df_geo = pd.DataFrame(sub_data["id_str"]).reset_index(drop=True)$ df_geo["geo_code"] = geo_code$ df_geo.head()
url = "https://www.reddit.com/hot.json"
git_log['timestamp'].describe()
conn.commit()
last_year = dt.date(2017, 4, 15) - dt.timedelta(days=365)$ print(last_year)
afx_17 = json.loads(afx_x_2017.text)$ type(afx_17)
df = pd.DataFrame()$ variables = ['text','created_at','source','user']$ df = pd.DataFrame([[getattr(i,j) for j in variables] for i in tweets], columns = variables)
plt.title("Aggregate Media Sentiment baed on Twitter")$ plt.ylabel("Tweet Polarity")
df = pd.read_excel(workbook_name, sheetname=1)
google_stock.corr()
df[['beer_name', 'brewery_name', 'rating_score']][(df.brewery_name.str.contains('Arcadia')) & (df.beer_name.str.startswith('IPA'))]
df.word_count.sum()
sentiments_df = sentiments_df.sort_values(["Target","TweetsAgo"], ascending=[True, False])$ sentiments_df.head()
pd.to_datetime(['2009/07/31', 'asd'], errors='ignore')
!mv 9 blast.txt
n_neg = n_pos * 10$ train_neg = train_sample.filter(col('is_attributed')==0).orderBy(func.rand(seed=seed)).limit(n_neg).cache()$ print("number of negative examples:", n_neg)
new_df = df.fillna(method = 'bfill', axis = 'columns')$ new_df
news_dict_df.to_csv("Newschannel_tweets_df.csv")
import sys$ sys.executable$
print(data.learner_id.nunique())
for screen_name in napturalistamo_followers_timelines_grouped.index:$     with open("../output/napturalistamo_followers/text/{0}_timeline.txt".format(screen_name), "w") as text_file:$         text_file.write(napturalistamo_followers_timelines_grouped[screen_name])
big_data_station = big_data.groupby('STATION')$
yc_new3 = yc_new2[yc_new2.tipPC < 100]
df_CLEAN1A.info()$
y_pred = pipeline.predict(X_test)$ print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))$
import pprint$ summary = learn.summary()$ print(str(pprint.pformat(summary))[:1000])
new_df = pd.read_sql_query('select * from film where title = "Hunchback Impossible"', engine)$ new_df.head()$
df_treat= df2[df2['group'] == 'treatment']['converted'].mean()$ print("{} is the probability they converted.Thus, given that an individual was in the treatment group.".format(df_treat))
year17 = driver.find_elements_by_class_name('yr-button')[16]$ year17.click()
new_discover_sale_transaction = post_discover_sales[post_discover_sales['Email'].isin(new_customers_test['Post Launch Emails'].unique())]$ new_discover_sale_transaction['Total'].mean()$
import pandas as pd       $ train = pd.read_csv("movie-data/labeledTrainData.tsv", header=0, \$                     delimiter="\t", quoting=3)
page = requests.get('https://www.r-bloggers.com')$ soup = BeautifulSoup(page.text, "html5lib")$
big_change = max([day[2] - day[3] for day in afx_dict['dataset_data']['data']])$ print("The largest change in stock price was " + str(round(big_change,2)) + ".")
Google_stock = pd.read_csv('~/workspace/udacity-jupyter/GOOG.csv')$ print('Google_stock is of type:', type(Google_stock))$ print('Google_stock has shape:', Google_stock.shape)$
np.random.seed(123456)$ ps = pd.Series(np.random.randn(12),mp2013)$ ps
windfield_proj = windfield.GetProjection()$ windfield_proj
rf=RandomForestClassifier(labelCol="label", featuresCol="features")$ labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels)$ pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,labelIndexer, OH1, OH2, OH3, OH4, OH5, OH6, assembler, rf, labelConverter])
hawaii_df = measure_df.merge(station_df, left_on='station', right_on='station', how='outer')$ hawaii_df.head()
itemTable["Project"] = itemTable["Project_Id"].map(project_link)
data.drop(catFeatures, axis=1,inplace=True)
survey = resdf.iloc[:,:113]$ survey.insert(2,'LangCd',resdf.iloc[:,120])$ survey.to_sql('surveytabl',conn)
test_array = np.concatenate([test_active_listing_dummy, test_pending_ratio_dummy], axis=1)$ print(test_array.shape)
logger = logging.getLogger()$ logger.setLevel(logging.INFO)  # SET YOUR LOGGING LEVEL HERE #
df_ct.to_csv("can_tire_senti_score.csv", encoding='utf-8', index=False)
import matplotlib.cm as cm$ dots_c, line_c, *_ = cm.Paired.colors
n_old = df2[df2['group'] == 'control'].shape[0]$ n_old
r_2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY+'&start_date=2017-01-01&end_date=2017-12-31')$ data_2017 = r_2017.json()
df.head()
df_indices = df_sp_500.unionAll(df_nasdaq).unionAll(df_russell_2k)$ df_indices.sample(False, 0.01).show()
x_train,x_test,y_train,y_test=train_test_split(df_predictors,df_tripduration,test_size=.2, random_state=1)
mars_weather = current_weather_info[0].text$ mars_weather
tweets['time_eastern'] = tweets['created_at'].apply(lambda x: x.tz_localize('UTC').tz_convert('US/Eastern'))
mergde_data.loc[mergde_data['max']==0]
db.query("select min(created) from bike_locations where provider='limebike' and raw->'attributes'->>'vehicle_type'='scooter'").export('df')
salida_all = getFacebookPageFeedData(page_id, access_token, 1000)$ columns = ['post_from', 'post_id', 'post_name', 'post_type', 'post_message', 'post_link', 'post_shares', 'created_time']$ df_posts = pd.DataFrame(columns=columns)
festivals['Index'] = range(1, len(festivals) + 1)$ list(festivals.columns.values)$ festivals.head(3)$
average_chart_upper_control_limit = average_of_averages + 3 * d_three * average_range / \$                                     (d_two * math.sqrt(subgroup_size))
prediction_df = pd.DataFrame(y_pred, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"])$ prediction_df.head(15)
x=[0,1,2,3,4,5]$ network_simulation[network_simulation.generations.isin(x)]$
grouped_dpt["Revenue"].filter(lambda x: x.sum() > 1000)
purchase_history = pd.merge(orders_subset, $                             order_details_prior, $                             on=['order_id'])[['user_id', 'product_id']]
results.coordinates
 $ session.query(Measurement.id, func.min(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
X = vectorizer.fit_transform(clean_train_reviews)
merged_portfolio_sp = pd.merge(merged_portfolio, sp_500_adj_close, left_on='Acquisition Date', right_on='Date')$ merged_portfolio_sp.head()
Xfull = Xtrain.append(Xtest)$ yfull = ytrain.append(ytest)$ print(Xfull.shape, yfull.shape)
trump = api.user_timeline(id='realDonaldTrump') # last 20 tweets
ward_df = pd.DataFrame(wards)
df_roll = df.set_index("posted_date")$ df_roll = df_roll.resample("1h").sum().fillna(0).rolling(window=3, min_periods=1).mean()$ df_roll.reset_index(inplace=True)
results = pd.read_csv('../data/result.csv',$                       low_memory=False      #This is required as it's a large file...$                      )
svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf')$ svc.fit(X_tfidf, y_tfidf)$ svc.score(X_tfidf_test, y_tfidf_test)
df_Diff=df_Modified-df_Created$ df_Diff.head(9)
flight.describe("from_city_name").show() 
hand = pd.get_dummies(auto_new.Hand_Drive)$ hand.head()
dir(friends_n_followers)$ help(friends_n_followers.sort_values)
techDetails.to_pickle('PC_World_Latops_Scrape.pickle')$ techDetails.to_csv('PC_World_Latops_Scrape.csv')
date = datetime.datetime.strptime(   )$ mask = 
prcp_analysis_df.plot(figsize = (18,8), color='blue', rot = 340 )$ plt.show()$ 
scores[scores.IMDB == min_IMDB]
poparr = fe.bs.csv2ret('boots_ndl_d4spx_1957-2018.csv.gz',$                        mean=fe.bs.SPXmean, sigma=fe.bs.SPXsigma,$                        yearly=256)
x_train, x_test, y_train, y_test = train_test_split(x, y , test_size=0.4, random_state=2)
print('Columns:',len(free_data.columns), 'Rows:',len(free_data))
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index$ all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))$ all_features = all_features.fillna(all_features.mean())
mgxs_lib.mgxs_types = ['nu-transport', 'nu-fission', 'fission', 'nu-scatter matrix', 'chi']
next(stream_docs(path='./movie_data.csv'))
clf.fit(X_train, y_train)
print(df.dtypes)
np.abs(df2['Change'])
small_frame.rbind(small_frame)
drone_utc = drone2.set_index('drone_date_utc')
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
scope_df['Invalid AC'] = scope_df['textinfo'].str.contains('Acceptance|AC', case = False, regex = True) == False$ scope_df[scope_df['Invalid AC']].to_excel(writer, index=False, sheet_name='Invalid AC', freeze_panes=(1,0), columns=['Team_story', 'key_story', 'reporter_story'])$
url = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2017-01-01&end_date=2017-12-31&' + API_KEY$ r = requests.get(url)$ json_data = r.json()
with open(content_analysis_save, mode='w', encoding='utf-8') as f:$     f.write(wikiContentRequest.text)
req = requests.request('GET', 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
df.isnull().sum()
mars_df.rename(columns={0: 'description', 1: 'value'}, inplace=True)$ mars_df.set_index('description', inplace=True)$ print(mars_df)
lengths = sent.Post.apply(len)$ print('Average character length of the posts are:')$ print (np.mean(lengths))
fname = "tests/test_data/acoust.ic/wa.ve0.wav"$ re.findall(r"^.*\.(.+)",fname)[0]$
table_rows = driver.find_elements_by_tag_name("tbody")[22].find_elements_by_tag_name("tr")$
automl.refit(X_test.copy(), y_test.copy())$ print(automl.show_models())
df_select_cats = df_select.copy()$ df_select_cats = df_select_cats.groupby(['Categories'], as_index=False).mean()$ df_select_cats
story_sentence = f'Of {record_count:,} licensed debt collectors in Colorado, {action_count:,} ({pct_whole:0.2f}%) have been subject to some form of legal or administrative action, according to an analysis of Colorado Secretary of State data.'$ print(story_sentence)
cityID = '42e46bc3663a4b5f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Worth.append(tweet) 
dtanswer = qs.CreationDate_a - qs.CreationDate
yc_new2.rename(columns={'Tip_Amt':'tipPC'}, inplace=True)$ yc_new2.head()
model_w.summary2() # For categorical X.
bands = questions['bands'].str.get_dummies(sep="'")
counts = combined_df5['vo_propdescrip'].value_counts()$ repl = counts[counts <= threshold].index$ combined_df5['vo_propdescrip']=combined_df5['vo_propdescrip'].replace(repl, 'Other')$
grpConfidence.count()
import _pickle as cPickle$ with open('tuned_crf_classifier.pkl', 'rb') as fid:$     crf = cPickle.load(fid)
result = api.search(q='%23arena') $ len(result)
pnew=df2.converted.mean()$ pnew
irisDF = SpSession.createDataFrame(irisMap)$ irisDF.show()$
scores_by_org = news_df.groupby('Screen Name')['Compound Score'].mean()$ scores_by_org.head()
table_rows = driver.find_elements_by_tag_name("tbody")[4].find_elements_by_tag_name("tr")$
fcc_nn.tail()
my_gempro.get_dssp_annotations()
latest_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()[0]$ format_latest_date = dt.strptime(latest_date,"%Y-%m-%d")$ format_latest_date # Display the date
Quandl_DF['Month'] = Quandl_DF['Date'].dt.month$ Quandl_DF['Year'] = Quandl_DF['Date'].dt.year$ Quandl_DF['WeekNo'] = Quandl_DF['Date'].dt.week
xml_in['publicationDate'] = pd.to_datetime(xml_in['publicationDate'], format='%Y-%m-%d', errors='coerce')
data.describe()
data.registerTempTable("my_data")
news_p = soup.find_all('p')$ for paragraph in news_p:$     print(paragraph.text)
genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId'])$ genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ genreTable.head()
engine.score_all_on_classifier_by_key("nhtsa_classifier")
red_4.isna().sum()
dta.query("(risk == 'Risk 1 (High)') | (risk == 'Risk 1 (Medium)')").head()
svm_classifier = GridSearchCV(estimator=estimator, cv=kfold, param_grid=svm_parameters)$ svm_classifier.fit(X_train, Y_train)
wards = gp.GeoDataFrame.from_file('Boundaries - Wards (2015-)/geo_export_e0d2c9f9-461f-4c6e-b5fd-24e123c74ee3.shp')
pc = decimal.Decimal(110) / decimal.Decimal(100)$ fee = decimal.Decimal(.003)$ pc> 1+fee
list(set(df['City'][df.CustomerID=='0000118196']))   
X = data.values$ from sklearn.preprocessing import StandardScaler$ X_std = StandardScaler().fit_transform(X)
alldata = pd.merge(data_FCBridge, data_FCInspevnt_latest, on='brkey')$ alldata.loc[alldata['bridge_id'] == 'LMY-S-FOSL']$
marketSeries = pd.Series(sp500["Dif"].values, index=sp500["Date"])$ marketSeries.plot();
def keras_rnn_predict(samples, empty=human_vocab["<pad>"], rnn_model=m, maxlen=30):$     data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty)$     return rnn_model.predict(data, verbose=0)
df_log.user_id.nunique()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=' + API_KEY + '&start_date=2017-1-1&end_date=2017-12-31')$ 
cityID = 'e4a0d228eb6be76b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Philadelphia.append(tweet) 
Stations = Base.classes.Stations$ Measurements = Base.classes.Measurements
spp['DK'] = spp[spp.columns[spp.columns.str.contains('DK')==True]].sum(axis=1)$ spp['DE'] = spp[spp.columns[spp.columns.str.contains('DE')==True]].sum(axis=1)
print()$ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
with open('180219_10slsqpDM.pkl', 'rb') as f:  # Python 3: open(..., 'rb')$     rez_2 = pickle.load(f)
UserInsight = pd.DataFrame(columns=['_id','owner','source','category','type','message','data','upvote','downvote'])$ UserInsight
cityID = 'ab2f2fac83aa388d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oakland.append(tweet) 
os.listdir(os.getcwd() + "/2018-05-26/")[0]
usage_400hz = hc.table('asm_wspace.usage_400hz_2017_Q4') $
df.loc['a', 'ii', 'z']
text_classifier.get_step_params_by_name("text1_char_ngrams")
df.min()
rain_df = pd.DataFrame(rain)$ rain_df.head()
data2.SA = data2.SA.astype(str)
chk = joined.loc[joined['nat_event']==1].head()$ holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
plt.scatter(compound_final.index, compound_final.Compound)$ plt.show()
results = Geocoder.reverse_geocode(31.3372728, -109.5609559)$ results.coordinates
sl['two_measures'] = np.where((sl.mindate!=sl.maxdate),1,0)
dfClientes.iloc[20, :]
outfile = os.path.join("Resource_CSVs","Main_data.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
df_raw = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ df = df_raw.copy()$ print(df_raw.head())
delta=datetime(2011,1,7)-datetime.now();$ delta
data = pd.read_json('UCSD_records.json', orient = 'columns')
idx = pd.IndexSlice$ health_data_row.loc[idx[:, :, 'Bob'], :]  # very close to the naive implementation
df_birth['Country '].value_counts(dropna=False).head()
df['water_year2'] = df['datetime'].apply(lambda x: x.year if x.month < 10 else x.year + 1)
p_old = df2['converted'].mean()$ print ("convert rate for p_old under the null :{} ".format(round(p_old, 4)))
control_notAligned = ((df['group'] == 'control') & (df['landing_page'] != 'old_page'))$ control_notAligned.sum()
print("Min " + str(dc['created_at'].min()) + " Max " + str(dc['created_at'].max()))$
TEXT.vocab.stoi['the']
if search_results is not []:$     print(json.dumps(search_results[0]._json, indent=2))
df = pd.read_excel('data/analysis2/Dow.xlsx')$ df.head(3)$ df.shape
df.iloc[0]
combats.Winner[combats.Winner == combats.First_pokemon] = 0$ combats.Winner[combats.Winner == combats.Second_pokemon] = 1$ print(combats.head(5))
import gensim$ LabeledSentence = gensim.models.doc2vec.LabeledSentence
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print( "z_score: {} \np_value: {} ".format(z_score, p_value))$
tvec = TfidfVectorizer(stop_words='english')$ X_train_counts = tvec.fit_transform(X_train)$ X_test_counts = tvec.transform(X_test)
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_zh_200_wzh.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_200_wzh.w2v')
! rm -rf models1$ ! mrec_train -n4 --input_format tsv --train "splits1/u.data.train.*" --outdir models1 --model=popularity
co_occurence_on_top50 = {ds: [(el[0], str(el[1])) for el in datasets_co_occurence[ds]] for ds in top50.dataset_id.tolist()}
conn_str = "mongodb://127.0.0.1/clintrials"$ client = pm.MongoClient(conn_str)$ client["admin"].command("listDatabases")
df_cs.isDuplicated.value_counts()$ df_cs.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
data = pd.read_csv("ml-100k/u.data", sep="\t", header=None, index_col=0)$ data.columns = ["item id" , "rating" , "timestamp"]
my_model_q9 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label')$ my_model_q9.fit(X_train, y_train)$ base_model_relation, base_accuracy_comparison = my_model_q9.base_model_eval()
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])
! rm -rf splits1$ ! mrec_prepare --dataset ml-100k/u.data --outdir splits1 --rating_thresh 4 --test_size 0.5 --binarize
y_pred_mdl7 = mdl.predict(test_orders_prodfill)
import pandas as pd$ df = pd.DataFrame(tweets_data, columns=['text', 'id'])$ print(df.head())
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print("z-score:", z_score,$      "\np-value:", p_value)
tlen.plot(figsize=(16, 4), color='r')
from sklearn import linear_model$ lin_reg = linear_model.LinearRegression()$ lin_reg.fit(x_train,y_train)
x = tags['Count'][0:20]$ y = tags['TagName'][0:20]$ sns.barplot(x, y, color = 'g')
traded_volume =[d[dt]['Traded Volume'] for dt in d.keys()]$ average_trading_volume = sum(traded_volume)/len(traded_volume)$ average_trading_volume$
elements.head(5)
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)$
train=mydf.sample(frac=0.9,random_state=200)$ test=mydf.drop(train.index)
merge_table1=merge_table.dropna(axis=0)$ merge_table1.head(20)
dat_hcad['blk_lower'] = dat_hcad['0'] - dat_hcad['0'] % 100$ dat_hcad['blk_upper'] = dat_hcad['blk_lower'] + 99$ dat_hcad['blk_range'] = dat_hcad['blk_lower'].map(str)+'-'+dat_hcad['blk_upper'].map(str)+' '+dat_hcad['COMMERCE'].map(str)
print('min wavelength:', np.amin(wavelengths),'nm')$ print('max wavelength:', np.amax(wavelengths),'nm')
top_10_authors = git_log.groupby('author').count().apply(lambda x: x.sort_values(ascending=False)).head(10)$ top_10_authors
mars_facts_df.columns = ['Characteristic','Data']$ mars_df_table = mars_facts_df.set_index("Characteristic")$ mars_df_table
rf_v2.hit_ratio_table(valid=True)
ratings = ['review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value']$ filtered_df.dropna(axis=0, subset=[ratings], inplace=True)
cohort_retention_df.fillna(0,inplace=True)
treehouse_expression_hugo_tpm = treehouse_expression.apply(np.exp2).subtract(1.0).add(0.001).apply(np.log2)
with model:$     observation = pm.Poisson("obs", lambda_, observed = summary['event'])
import sqlite3$ conn = sqlite3.connect("database.db")$ cursor = conn.cursor()$
print(data.program_code.value_counts())
plt.plot(scores); plt.title("Scores");
err = (actual - expected[np.newaxis,:,:]).reshape(-1)$ err.shape
plt.hist(name_array)$ plt.title('The distribution of the names\' length')$ plt.show()$
pd.Series(42)
df.drop_duplicates(['title'],keep= 'first',inplace =True)
test_tweet = api.user_timeline(newsOutlets[0])$
number_of_commits =git_log['timestamp'].size$ number_of_authors = git_log.dropna(how='any')['author'].unique().size$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
sig_sq = np.var(d_3)$ mu = np.mean(d_3)$ scipy.stats.norm.interval(0.95, loc=mu, scale=sig_sq)
data_final.shape
y_pred = rf.predict(X_test)
df.loc[df.userTimezone == 'Mountain Time (US & Canada)', 'tweetText']$
auth = tweepy.OAuthHandler(ckey, csecret)$ auth.set_access_token(atoken, asecret)
findNum = re.compile(r'\d+')$ for i in range(0, len(postsDF)):$ 	print(findNum.findall(postsDF.iloc[i,0]))
last = session.query(Measurement.date).order_by(Measurement.date.desc()).first()[0]$ last_date = datetime.strptime(last, '%Y-%m-%d')$ year_ago = last_date - dt.timedelta(days=365)
f_user = os.path.join(data_dir, 'following_users.csv')$ f_term = os.path.join(data_dir, 'tracking_terms.csv')$ f_meta = os.path.join(data_dir, 'collection_meta.csv')
MFThermalCorrDF=pd.read_pickle('MFThermalCorrDF.p')
df['created_at_time'] = pd.to_datetime(df.created_at)
session.query(Measurements.date).order_by(Measurements.date).first()
tickers = portfolio_df['Ticker'].unique()$ tickers
final_df = diff_df.merge(final_sentiment_df, how = "outer")$ final_df$ round(final_df, 3)
rf.score(clim_test, size_test)
data.loc[9323,'Tweets']$
%%bash$ cut -f 1,4-6 PPMI/logASYN/PHENO0_0.txt | sed 's/_/\t/g' | sed 's/(\//\t/g' | sed 's/)//g' | head
m = pd.read_json('njdevils_gr_posts.json',orient='records')$ m['created_date'] = m['created_time'].dt.strftime('%Y-%m-%d')$
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read())
words_hash_sp = [term for term in words_sp if term.startswith('#')]$ corpus_tweets_streamed_profile.append(('hashtags', len(words_hash_sp))) # update corpus comparison$ print('List and total number of hashtags: ', len(words_hash_sp)) #, set(terms_hash_stream))
round(max(multi.handle.value_counts(normalize=True)),4)
fpr_a = (grid_pr_fires.sort_values(['glat', 'glon'], ascending=[False,True])['pr_fire']$          .values.reshape(26,59))
for post in posts.find({"reinsurer": "AIG"}):$     pprint.pprint(post)
two_day_sample.head()
(train.shape, test.shape, y_train.shape)
[k for val in train_x[0] for k,v in words.items() if v==val]
DummyDataframe2 = DummyDataframe2.apply(lambda x: update_values_category(x, "Tokens"), axis=1)$ DummyDataframe2
HAMD = HAMD[HAMD["level"]=="Enrollment"]$ HAMD = HAMD.dropna(axis=1, how='all')$ HAMD.columns
news_organizations_df['tweets'] = news_organizations_df['tweets'].map(normalize_df)
reddit = praw.Reddit(client_id='yRkwuIWiThfXeQ',$                      client_secret='Kond5JUokaOkKKXu6LipwN_CtcM',$                      user_agent='weekdayjason')
plt.plot(glons, glats, marker='.', color='k', linestyle='none')$ plt.show()
df=pd.read_csv('2017-2018_NBA_Player_stats.csv')$ df.info()
from nltk.corpus import conll2000$ from nltk import conlltags2tree, tree2conlltags$ train_labels[3]
words_mention_scrape = [term for term in words_scrape if term.startswith('@')]$ corpus_tweets_scraped.append(('mentions', len(words_mention_scrape))) # update corpus comparison$ print('Total number of mentions: ', len(words_mention_scrape)) #, set(terms_mention_stream))
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep ='\s+', header=None)$ df.head(5)
ss = fe.bs.smallsample_gmr(256, poparr, yearly=256, repeat=300)$
driver.find_element_by_xpath('//*[@id="body"]/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[1]/b/font/a').click()
results=session.query(func.count(Stations.Index)).all()$ station_count=results[0][0]$ print("Count of Stations: %d"%station_count)
merged2.dropna(subset=['Specialty'], how='all', inplace=True)
df2[df2.duplicated('user_id')]$
new.to_csv('dates.csv')$ test.to_csv('count.csv')
clean_sentiments = pd.DataFrame(clean_sentiments, columns = ['Target', 'Date', 'Tweet Ago', 'Compound',$                                                              'Positive', 'Negative', 'Neutral', 'Text', 'Source'])$ select_data = clean_sentiments
words_hash_scrape = [term for term in words_scrape if term.startswith('#')]$ corpus_tweets_scraped.append(('hashtags', len(words_hash_scrape))) # update corpus comparison$ print('Total number of hashtags: ', len(words_hash_scrape)) #, set(terms_hash_stream))
tweet_data_df.to_csv("tweet_data_df.csv",sep =",")
d = nc.Dataset('data/otn200_20170802T1937Z_a755_2845_bd51_e1ca_6d36_6133.nc', 'r')
(train.shape, test.shape)
%%time$ corpus = load_headline_corpus(verbose=True)$ print ('Headlines:', len(corpus.sents()))
soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
a_df.to_csv('a_tweets.csv')$ b_df.to_csv('b_tweets.csv')
dfEtiquetas["latitude"] = dfEtiquetas["place"].apply(lambda p: p["location"]["latitude"])$ dfEtiquetas["longitude"] = dfEtiquetas["place"].apply(lambda p: p["location"]["longitude"])
df['y'].plot()
unordered_df = USER_PLANS_df.iloc[unordered_timelines]
X = X.drop(0, axis='index')
dfs_morning.sort_values(by='ENTRIES_MORNING', ascending = False).head(50)$ threshold = 100000$ dfs_morning.loc[dfs_morning['ENTRIES_MORNING']>threshold, 'ENTRIES_MORNING'] = dfs_morning.ENTRIES_MORNING.quantile(.5)
y_train = np_utils.to_categorical(y_train, 90)$ y_test = np_utils.to_categorical(y_test, 90)
np.setdiff1d(np.arange(6567,8339),ORDERS_GEN['order_number'])
data = data.drop_duplicates(subset=['name'], keep=False)
s_median = s.resample('5BM').median()$ s_median
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
df2.to_csv("../../data/msft_modified.csv",index_label='date')
auth = tweepy.OAuthHandler(config.consumer_key, config.consumer_secret)$ auth.set_access_token(config.access_token, config.access_token_secret)$ api = tweepy.API(auth)
idx = pd.IndexSlice$ df.loc[idx[:, :, 'x'], :]
df7['avg_health_index Closed'].value_counts(dropna=False)
lgreg = LogisticRegression()$ lgreg.fit(train_data, train_labels)
coins = ccw.get_coin_list()$ COIN_DB = pd.DataFrame.from_dict(coins, orient='index')$
import sys$ sys.version
df_users.user_id.nunique()
! mkdir census_data$ ! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data --output census_data/adult.data$ ! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test --output census_data/adult.test
breakdown[breakdown != 0].sort_values().plot($     kind='bar', title='Russian Trolls Number of Links per Topic'$ );
extract_deduped_cmp = extract_deduped[f_remove_extract_fields(extract_deduped.sample()).columns.values].copy()
random_integers.max(axis=1)
articles = db.articles.find()$ for article in articles:$     print(article)
ibm_train = ibm_hr_final.join(ibm_hr_target.select("Attrition_numerical"))$ ibm_train.printSchema()
transit_df_rsmpld = transit_df_byday.reset_index().groupby('FUZZY_STATION').apply(lambda x: x.set_index('DATETIME').resample('1M').sum()).swaplevel(1,0)$ transit_df_rsmpld.info()$ transit_df_rsmpld.head()
df.to_csv(save_dir)
cityID = 'd98e7ce217ade2c5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Stockton.append(tweet) 
import sklearn.model_selection as model_select$ results = model_select.cross_val_score(classifier, X, y, cv=10, scoring='accuracy')$
np.random.randint(1,100, 10)
print(len(distance_list))$ station_distance.shape
flights2 = flights.set_index(["year", "month"])["passengers"]$ flights2.head()
def predict_row(row, theta):$     hx = sigmoid(np.dot(row, theta))$     return hx
for name, val in zip(x.columns, adult_model.feature_importances_):$     print(f'{name} importance = {100.0*val:5.2f}%')
index = pd.date_range('2018-3-1', periods=1000, freq='B')$ index
counts = Counter(l_hashtags)$ df = pd.DataFrame(counts.most_common(20), columns=['Hashtag', 'Count'])$ df.to_csv('hashtag_counts.csv')
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
df.iloc[0:3, [0, 2]]
engine = create_engine("sqlite:///hawaii.sqlite", echo = False)
dataPath = os.path.join("..", "example-data")$ data = pd.read_csv(os.path.join(dataPath, "example-from-j-jellyfish.csv"))$ data.head()
recortados = [recortar_tweet(t) for t in tweets_data]$ tweets = pd.DataFrame(recortados)
pd.options.display.max_rows$ pd.set_option('display.max_colwidth', -1)$ new_df = df_filtered_by_RT.assign(clean_text = df_clean_text, extr_emojis = df_extr_emojis)$
qrt = closePrice.resample('Q').mean()
df = pd.concat([df.title, df.type], axis=1, keys=["title", "type"])$
!git clone https://github.com/u110/WallClassification
import pickle$ with open('180225_10slsqpGLD.pkl', 'rb') as f:  # Python 3: open(..., 'rb')$     rez_2 = pickle.load(f)
USvideos = pd.read_csv('data/USvideos.csv', parse_dates=['trending_date', 'publish_time'])
df.at[dates[2],'A']
firebase.post("Exhibitions/",new_data)$
scratch['created_at'] = scratch['created_at'].map(lambda x: x.lstrip('"').rstrip('"'))
ratings=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\ratings.csv')
path_to_token <- normalizePath("data_sci_8001_token.rds")$ envvar <- paste0("TWITTER_PAT=", path_to_token)$ cat(envvar, file = "~/.Renviron", fill = TRUE, append = TRUE)
rides_fare_average_min = rides_analysis["Average Fare"].min()$ rides_fare_average_min
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df =sentiments_df[['Date','Compound','Count']]$ sentiments_df.head()
sample = rng.choice(sizes, sample_size, replace=True)$ print(f'Mean (one sample) = {np.mean(sample):5.3f}')$ print(f'Standard deviation (one sample) = {np.std(sample):5.3f}')
df['AppointmentDurationHours'] = df['AppointmentDuration'] / 60.0
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=10, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
df2.user_id.duplicated().sum()
xml_in.dtypes
df_notnew = df.query('landing_page != "new_page"')$ df_3 = df_notnew.query('group == "treatment"')$ df_3.nunique()$
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
import matplotlib.cm as cm$ proudlove_c, perry_c, all_c, regression_c, *_ = cm.Paired.colors
tips["sex"].index
f = open("datasets/git_log_excerpt.csv")$ print(f.read())
colmns=['category', 'launched_year', 'launched_quarter', 'goal_cat_perc', 'participants']$ ks_particpants.columns=colmns
temp_df2.shape
honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['Time stamp'].str.split("mhn").tolist(), columns = ['time_stamp1','time_stamp2','time_stamp3'])],axis = 1)$ honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['time_stamp3'].str.split("T").tolist(), columns = ['date','time'])], axis = 1)
station_df = pd.read_sql("SELECT * FROM station", conn)
X = dataset[dataset.columns[1:]]$ Y = np.array(dataset["label"])
tweets = pd.concat([tweets1,tweets2,tweets3])$ tweets.shape
flight_hex = h2o.H2OFrame(flight_pd)
Station_id = session.query(Station.station).filter(Station.name == 'WAIHEE 837.5, HI US').all()$ Station_id
lm_withsubID_export_path = cwd+'\\LeadGen\\Ad hoc\\SubID\\LM Sig Loans with SubID.xlsx'$ lm_withsubID.to_excel(lm_withsubID_export_path, index=False)
p_value = scipy.stats.chi2_contingency(full_contingency)[1]$ print(p_value)
y = api.GetUserTimeline(screen_name="HillaryClinton", count=20, max_id=935706980643147777, include_rts=False)$ y = [_.AsDict() for _ in y]
precip = session.query(Precip.date, Precip.prcp, Precip.station).filter(Precip.date >= date_ly)
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
import os$ sc.addPyFile(os.path.expanduser('~/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar'))$ from graphframes import *
import pandas as pd$ the_dict = pd.read_clipboard().to_dict('records')$ the_dict
df.sheet_names # see all sheet names$
prcp_df.describe()
uber = pd.read_csv('https://assets.datacamp.com/production/course_2023/datasets/nyc_uber_2014.csv')$ print(uber.shape)$ print(uber.head())
r_forest['has_extended_profile'] = r_forest['has_extended_profile'].fillna(0)
y_range = (0, 200)$ x_range = bokeh.models.ranges.Range1d(start = datetime.datetime(2017,1,1),$                                       end = datetime.datetime(2017,1,31))
Stations = Base.classes.stations$ Measurements = Base.classes.measurements
Net_Capital_Gain = pd.read_sql(q, connection)$ Net_Capital_Gain.head()
model.wv.syn0.shape
cityID = 'c47c0bc571bf5427'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Honolulu.append(tweet) 
%%time$ crime_geo.to_parquet(data_dir + file_name + '.parquet', compression='SNAPPY')
pandas_small_frame = small_frame.as_data_frame()$ print(type(pandas_small_frame))$ pandas_small_frame
station_cnt = session.query(Measurement.station).distinct().count()$ station_cnt
number_of_commits = git_log.timestamp.count()$ number_of_authors = git_log.author.value_counts(dropna=True).count()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
csvWriter = csv.writer(csvFile)
demographics = my_df_free1.iloc[:,0:3]$ scores = my_df_free1.iloc[:,5:]
mean = np.mean(data['len'])$ print("The average length in tweets: {}".format(mean))
paired_cameras_missing_from_shopify_orders = np.setdiff1d(BPAIRED_SHOPIFY['channel_order_id'],ORDER_TO_PAIR_SHOPIFY['channel_order_id'])
small_frame = loan_stats[1:3,['loan_amnt','installment']]$ single_col_frame = loan_stats[1:3,'grade']$ small_frame.cbind(single_col_frame) 
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall()
conf_matrix = confusion_matrix(new_y, lr2.predict(new_x), labels=[1,2,3,4,5])$ conf_matrix # most of the results are classified as 5 point$
status_data = status_data.drop(['BROKERAGE', 'BETWEENNESS', 'NBROKERAGE',$                                     'NBETWEENNESS', 'DENSITY', 'TRANSITIVITY', 'NETWORKSIZE'], axis=1)
free_sub = free_data.loc[:,['country','y']]
from sqlalchemy_utils.functions import create_database$
predictions = model.transform(data)$ predictions.toPandas()$
df[df['bmi']< 18.5] # this is the way to select data, by using filters. $
empDf.filter(empDf["age"] >30).join(deptDf, empDf.deptid == deptDf.id).\$         groupBy("deptid").\$         agg({"salary": "avg", "age": "max"}).show()
filtered_styles = df.groupby('simple_style').filter(lambda x: x.simple_style.value_counts() >= 5)$ style_bw = filtered_styles.groupby('simple_style').rating_score.mean().sort_values(ascending=False)
mmx = MinMaxScaler()$ %time train_4_reduced = mmx.fit_transform(train_4_reduced)
for col in Y_train_df.columns:$     nbsvm_models[col] = NbSvmClassifier(C=10, dual=True).fit(X_train_cont_doc, Y_train_df[col])
sel_shopping_cart = pd.DataFrame(items, index = ['pants', 'book'])$ sel_shopping_cart
sns.violinplot(x=df['liked'],y=df['profile_popularity'],data=df,whis=np.inf)$
rng = np.random.RandomState(23)$ sample_size = 50$ rng.choice(sizes, sample_size, replace=True)
df.describe()
tc = pd.concat([tce2, tcp], ignore_index=True)$ tc
from sklearn.ensemble import RandomForestClassifier$ rf_clf=RandomForestClassifier(max_depth=None)$ print(rf_clf)
print(np.shape(a))$ print(np.size(a))
trial_parameter_nc = Plotting(S_distributedTopmodel.setting_path.filepath+S_distributedTopmodel.local_attr.value)$ trial_parameter = trial_parameter_nc.open_netcdf()$ trial_parameter['HRUarea']
df = pd.read_csv('/Users/jledoux/Documents/projects/Saber/baseball-data/statcast_with_shifts.csv')
print(airquality_pivot.index)
get_nps(combined_df, 'country').sort(columns='score', ascending=False).head(10)
df = pd.read_csv(data_location)$ df.head(5)
random_integers.max()
pd.isnull(r_forest).any(1).nonzero()[0]
n_new, n_old = df2['landing_page'].value_counts()$ print("new:", n_new, "\nold:", n_old)
tweets_df = pd.DataFrame(tweets_all)$ tweets_df['retweet_ratio'] = (tweets_df['retweet_count']/tweets_df['followers_count'])*10000
print(new_df.isnull().sum())$
step_counts.dtypes
street_median_length =data.groupby("Street")["Street.Length"].median()$ data = data.join(street_median_length, on="Street", rsuffix='_correct')
%matplotlib inline$ df['log_AAPL'].plot(figsize=(12,8));
customers_arr = np.array(cust_list)$ items_arr = np.array(item_list)
sentiment_df['Timestamp'] = pd.to_datetime(sentiment_df.Timestamp)$ sentiment_df.sort_values(by='Timestamp')
import csv$ data.to_csv('Marvel.csv', encoding='utf-8', index=False)
print(dfx.fillna(value=-999.25),'\n')$ print(dfx) # original data remains unchanged. 
tweets['created_at'] = pd.to_datetime(tweets['created_at'])$ tweets.dtypes
fox = news_sentiment('@FoxNews')$ fox['Date'] = pd.to_datetime(fox['Date'])$ fox.head()
dfs_resample.reset_index(inplace = True)$ dfs_resample['TIME'] = pd.to_datetime(dfs_resample['DATE_TIME']).dt.hour$ dfs_morning = dfs_resample[(dfs_resample['TIME'] == 6) | (dfs_resample['TIME'] == 12)]
rain_df = pd.DataFrame(rain_score)$ rain_df.set_index('date').head()$ rain_df.head()
tlen = pd.Series(data = data.len.values, index = data.Date)$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
from sklearn.metrics import mean_squared_error, mean_absolute_error$ mean_absolute_error(y_test, y_pred_lm)
events_df['event_time'].max(),events_df['event_time'].min()
import time$ ctime = obj['openTime']/1000$ time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ctime))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=6QBbyTowfjjBzvYS8nXF')
results = sm.OLS(gdp_cons_df.Delta_C1[141:280], gdp_cons_df.Delta_Y1[141:280]).fit()$ print(results.summary())
from IPython.display import display$ pd.options.display.max_columns = None
from matplotlib.dates import MonthLocator, WeekdayLocator, DateFormatter$ %pylab inline$ pylab.rcParams['figure.figsize'] = (15, 9)$
print(station_availability_df.info())$
chefdf.to_csv('exports/trend_data_chefkoch.csv')
S_1dRichards.decision_obj.thCondSoil.options, S_1dRichards.decision_obj.thCondSoil.value
url_votes = grouped['net_votes'].agg({'total_votes': 'sum', 'avg_votes': 'mean'})    $
bg_df2 = pd.DataFrame(bg3) # create new variable explicitly for it being a DataFrame in pandas$ bg_df2$
page_size = 200$ response = client.get('/tracks', q='footwork', limit=page_size,$                     linked_partitioning=1)$
top_10_authors = git_log.groupby("author").count().sort_values(by="timestamp", ascending=False).head(10)$ top_10_authors
festivals_clean.info()
print(reg1.score(X_train, y_train))$ print(reg1.score(X_test, y_test))
tmp_df = ratings.pivot(index='userId', columns='movieId', values='rating')
windfield = gdal.Open(input_folder+windfield_name, gdal.GA_ReadOnly)$ windfield
poparr2.shape
x = api.GetUserTimeline(screen_name="HillaryClinton", count=20, include_rts=False)$ x = [_.AsDict() for _ in x]
print()$ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
%%time$ table = pq.read_table(data_dir + file_name + '.parq')
stock_df = spark.createDataFrame(stock_delimited_daily)
jsonUrl = "s3a://DH-DEV-PROMETHEUS-BACKUP/prometheus-openshift-devops-monitor.1b7d.free-stg.openshiftapps.com/"+metric_name+"/"$ jsonFile = sqlContext.read.option("multiline", True).option("mode", "PERMISSIVE").json(jsonUrl)
list(t2.p1)
INT['Create_Date'].min()
def sigmoid(z):$     s =  1.0 / (1.0 + np.exp(- z))$     return s
plot_price(f,'Close')$ plt.legend("full range")
law = tc_final$ law['YBP sub-account'].replace("195099", "590099", inplace= True)$ law
airlines_day_unstacked = airlines_day_unstacked[(airlines_day_unstacked != 0).all(1)]
Irregularities_data = []$ time_hour_for_file_name = 0 #datetime.datetime.now().time().hour$
df2.info()
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
malemoon = pd.concat([moon, malebydatenew], axis=1)$ malemoon.head(3)
graf_train=pd.concat([graf_train, train_topics_df], axis=1)$ graf_test=pd.concat([graf_test, test_topics_df], axis=1)
print(cons_df.head())$ print(cons_df.PCEC96.head())$ cons_df.PCEC96.plot()
active_unordered = unordered_df.loc[~churned_unord]
data4.to_file('Twitters_FSGutierres.shp', driver='ESRI Shapefile')
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)$
fh_2 = FeatureHasher(num_features=uniques.iloc[1, 1], input_type='string', non_negative=True)$ %time fit2 = fh_2.fit_transform(train.device_id)
df.isnull().sum()
plt = df2.plot(legend=False)$ plt.set_ylabel("complaint count")$ plt.set_xlabel("DOTCE predicted grade school level (high school 9-12)");
station_count = session.query(Station.id).count()  $ print(f'There are {station_count} weather stations in Hawaii.')
df.sum()
df["Diff"] = df["Close"] - df["Open"]$ df["Percent_Diff"] = (df["Diff"]/df["Open"])*100$ df.head()
df.iloc[(len(df)-lookforward_window)-2:(len(df)-lookforward_window),:]
regGridSearch.best_estimator_.coef_
df1[40:].head(5)
precipitation_df = pd.DataFrame(sq.prec_last_12_months())
mbti_text_collection_filler.to_csv('Reddit_mbti_data_filler.csv',encoding='utf-8')
openmc_geometry = openmc.Geometry(root_universe)$ openmc_geometry.export_to_xml()
df_user_info = df_user_info[df_user_info['activated']==1]$ df_user_info['marketing_source'] = \$     df_user_info['marketing_source'].loc[:].fillna("unknown")
AFX_X_data = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=ZfFwbzzp8_Rsbi_mGznR&start_date=2017-01-01&end_date=2017-12-31')
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
lr2 = LogisticRegression(random_state=20, max_iter=10000, C= 1, multi_class='ovr', solver='saga')$ lr2.fit(X_tfidf, y_tfidf)$ lr2.score(X_tfidf_test, y_tfidf_test)
df.loc[df['waiting_days']>=0]['waiting_days'].describe()
df = df.replace('tomato', 'pea')$ df
df.resample('A').mean()
list.remove(3)$ print(list)
libraries_df = libraries_df.merge(libraries_metadata_df, on="asset_id")
twitter_daily_df = twitter_daily_df.join(distinct_users, ["Day","Company"]).orderBy('Day','Company')
pax_raw.paxcal.value_counts() / len(pax_raw)
frame = pd.DataFrame(data)$ frame
rain_df.set_index('date').head()
pd.read_pickle('data/wx/tmy3/proc/tmy3_meta.pkl', compression='bz2').head()
df4 = df.drop('Cabin', axis=1) \$     .loc[lambda x: pd.notnull(x['Embarked'])] \$     .fillna(30)
print(df.columns)$ df.rename(columns = {'usd pledged':'usd_pledged'}, inplace=True)$
lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics = topics)
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
y_pred = pipe_lr.predict(pulledTweets_df.emoji_enc_text)$ y_proba = pipe_lr.predict_proba(pulledTweets_df.emoji_enc_text)$ pulledTweets_df['sentiment_predicted_lr']=[classes[y_pred[i]] for i in range(len(y_pred))]
idx_close = r.json()['dataset']['column_names'].index('Close')
Aussie_result=pd.concat([sydney_aussie,brisbane_aussie,melbourne_aussie])$
S_lumpedTopmodel.decision_obj.hc_profile.options, S_lumpedTopmodel.decision_obj.hc_profile.value
from shapely.geometry import Point$ data3['geometry'] = data3.apply(lambda x: Point((float(x.lon), float(x.lat))), axis=1)
new_messages['sub'] = (new_messages.timestamp - new_messages.watermark)$ new_messages['day'] = pd.to_datetime(new_messages.watermark, unit='ms').dt.date$
print(tweets[0].id)$ print(tweets[0].created_at)
df.to_csv('../output/releases_with_demographics.csv')
<img src="/images/MongoDB1.png" alt="[img: MongoDB view]" title="MongoDB View" />
numPurchP = train.groupby(by='Product_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesP'})$ train = train.merge(numPurchP, on='Product_ID', how='left')$ test = test.merge(numPurchP, on= 'Product_ID', how='left')
extractor = connectToTwitterAPI()$ tweets = extractor.search(q="#BlackPanther", count=50)$ print("Number of tweets extracted: {}.\n".format(len(tweets)))
print data_df.clean_desc[26]
pd.DataFrame(dummy_var["_Source"][Company_Name]['Open']['Forecast'])[:12]$
mars_weather_tweet = weather_soup.find('div', attrs={"class": "tweet", "data-name": "Mars Weather"})
print('if we spend 50k on TV ads, we predict to sell:', round(lm.predict(X_new)[0],2), ' thousand units')
P.plot_1d_hru(0,'airtemp')
ADBC = AdaBoostClassifier(n_estimators=50)$ ADBC.fit(X, y) 
Latest_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ start_date = pd.to_datetime(Latest_date[0]).date()- timedelta(days=365)$ print(start_date)
prec_us = prec_nc.variables['pr_wtr'][1, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(prec_us)
df.drop_duplicates(subset=['address', 'date'], inplace = True)
prices=pickle.load(open('Q://LB2//dump//prices.p', 'rb'))$ prices=prices.reindex_axis(sorted(prices.columns), axis=1)$ returns=Factor.prices_to_returns(prices, replace_missing=True)
rng_pytz = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='Europe/London')
store_items = store_items.drop(['store 1'], axis=0)$ store_items
xticks = pd.date_range('00:00', '23:00', freq='H', tz='US/Eastern').map(lambda x: pd.datetime.strftime(x, '%I %p'))$ xticks
mean = np.mean(data['len'])$ print ("The length's average in tweets: {}".format(mean))$
print df.shape$ ab_counts = df.groupby('ab_test_group').count().reset_index()$ ab_counts$
tweet_archive_clean = tweet_archive.copy()$ tweet_image_clean = tweet_image.copy()$ tweet_df_clean = tweet_df.copy()
hist(df.pre_clean_len,100)$ grid()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, wait_on_rate_limit=True)
lr = LogisticRegressionCV()$ lr.fit(train_Features, train_species)
colors = ["green", "red", "blue", "orange", "maroon"]$ x_axis = np.arange(len(final_df))$ x_labels = diff_df.index
figure_density_df = utility_patents_subset_df.dropna()$ sns.distplot(figure_density_df.figure_density, color="red")$ plt.show()
json_dict=req.json()$ json_dict
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets], columns=['Tweets'])$ display(data.head(10))
knn = KNeighborsClassifier(n_neighbors=20)$ print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())
small_movies_file = os.path.join(dataset, 'ml-latest-small', 'movies.csv')$ small_movies_raw_data, small_movies_raw_data_header = read_file(small_movies_file)$
y_hat = linreg.predict(quadratic)$ plt.plot(y_hat,'-b')$ plt.show()
pd.to_datetime(['04-01-2012 10:00'])
(tweets_df["Compound_Score"]<=0).value_counts()$
mean = mc_estimates.expanding().mean()
data.plot.line(x = 'Unnamed: 0', y = 'Age', figsize = (15, 10))
dflong = pd.read_csv("SP500_Long_V4.CSV")$ print dflong.shape$
graph = facebook.GraphAPI(access_token=access_token, version='2.7')
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read(), end='')
rain_df = pd.DataFrame(rain)$ rain_df.head()
iris.head().iloc[:,:1]
avisos_detalles.drop('ciudad', axis = 1, inplace = True)$ avisos_detalles.drop('mapacalle', axis = 1, inplace = True)$ avisos_detalles.head(1)
exiftool -csv -createdate -modifydate cisuabf6/cisuabf6_cycle1.MP4 cisuabf6/cisuabf6_cycle2.MP4 cisuabf6/cisuabf6_cycle3.MP4 cisuabf6/cisuabf6_cycle4.MP4  > cisuabf6.csv
sel = [Measurements.station, func.count(Measurements.tobs)]$ active_stations_data = session.query(*sel).group_by(Measurements.station).order_by(desc(func.count(Measurements.tobs))).all()$ active_stations_data
pd.Series(PDSQ.min(axis=1)<0).value_counts()  # no
cur.execute('SELECT EmpGradeRank, count(EmpGradeRank) FROM demotabl WHERE EmpGradeRank="SENIOR";')$ cur.fetchone()
bd.index
from h2o.estimators.random_forest import H2ORandomForestEstimator
service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'
svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf')$ svc.fit(x_res, y_res)$ scores = cross_validate(svc, x_res, y_res, cv=10, n_jobs=-1, return_train_score=True)
train_df = pd.read_csv(slowdata + 'train.csv', index_col='id', parse_dates=['date'], dtype=dtypes)$ save_n_load_df(train_df, "training_dev.pkl")$ train_df.name = "TRAINING"
twelve_months_prcp.head()
data["time_up_sec"] = pd.to_datetime(data["time_up_clean"], format= "%H:%M:%S")
%matplotlib inline$ commits_per_year.plot(kind='line', title='Commits per year', legend=False)
data.loc[data.L2 == ' ', ['L2']] = np.nan$ data.L2.unique()
results = sm.OLS(gdp_cons_df.Delta_C1[:280], gdp_cons_df.Delta_Y1[:280]).fit()$ print(results.summary())
df.set_value(306, 'yob',1998)
stock_data.index
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)$ os.chdir(str(today))$ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)$
df.get_dtype_counts()
df4 = df3.dropna(axis=0, inplace=False)$ df4.head()
precip_data_df2=precip_data_df1[["date","Precipitation"]]$ precip_data_df2.describe()
tweet_favourite.plot(figsize=(20,8), label="Likes", legend=True)$ tweet_retweet.plot(figsize=(20,8), label="Retweets", legend=True )
news_title_docs_high_freq_words_df = pd.read_pickle(news_title_docs_high_freq_words_df_pkl)$ with pd.option_context('display.max_colwidth', 100):$     display(news_title_docs_high_freq_words_df)
t3.head(5)
from sklearn import preprocessing$ min_max_scaler = preprocessing.MinMaxScaler()
q_agent_new.scores += run(q_agent_new, env, num_episodes=50000)  # accumulate scores$ rolling_mean_new = plot_scores(q_agent_new.scores)
data = trends.interest_over_time()
response = requests.post("https://data.bitcoinity.org/chart_data", data=chartParams)$ response.content
kochdf = kochdf.dropna()
df.columns
from dotce.report import (most_informative_features,$                           ascii_confusion_matrix)
(elms_all.shape, elms_all_0611.shape)
print(groceries.index)$ print(groceries.values)
words = lc_review.split(" ")$ words_no_stop = [w for w in words if w not in stopwords.words("english")]
hdf5_file = h5py.File(refl_filename,'r')$ hdf5_file
plt.figure()$ precip_df.plot(kind = 'line', x = 'Date', y = 'Precip')$ plt.legend(loc='best')$
engine = create_engine("sqlite:///hawaii_hw.sqlite")$ conn = engine.connect()
new_df = df.dropna(subset=['driver_id'],inplace=False)
pd.to_datetime('11/12/2010', format='%d/%m/%Y')
data = fat.add_sma_columns(data, 'Close', [6,12,20,200])$ data.tail()
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'$ browser.visit(url)
doc = coll.find_one()$ doc
Which_Years_for_each_DRG.loc[345]$
modelCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])$
theft.iloc[0:10]
data.head(10)
calls_nocontact.zip_code.value_counts()
checking['age'].iloc[z]
height = soup.find_all(class_='quiver-surf-height')$ height.text
file = 'data/pickled/Emoticon_NB4/full_emoji_dict.obj'$ emoji_dict = gu.read_pickle_obj(file)
for c in ccc[:2]:$     for i in spp[spp.columns[spp.columns.str.contains(c)==True]].columns:$         spp[i] /= spp[i].max()
ids = new_stops['stopid']$ new_stops[ids.isin(ids[ids.duplicated()])]
y_train=bow["label"]$ X_train=bow.drop({"ID","label"},axis=1)$
year_with_most_commits = commits_per_year["author"].idxmax().year$ print(year_with_most_commits)
access_logs_raw.count()
example1_df = spark.read.json("./world_bank.json.gz")
df.loc['2017-02-01': '2018-03-01',['Adj. Close', 'Forecast']].plot(figsize=(20,10))
df['conv_flag'] = df.conversations.apply(lambda x: 1 if isinstance(x, dict) else 0)$ df = df[df.conv_flag==1].drop(['users','conv_flag'],axis=1)$ print("... got just conversations")
artistAliasDF[ artistAliasDF.mispelledID==1000010 ].show()$ artistAliasDF[ artistAliasDF.mispelledID==2082323].show()$
nasa_url = 'https://mars.nasa.gov/news/'$ jup_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
pd.DataFrame(pairing_dict_names)
tweet_archive_clean.drop(['new'], axis= 1, inplace= True)
driver = selenium.webdriver.Safari() # This command opens a window in Safari$ driver.get('https://www.boxofficemojo.com')
contractor_final.to_csv('contractor_final.csv')
df.std()
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))
my_gempro.set_representative_sequence()$ print('Missing a representative sequence: ', my_gempro.missing_representative_sequence)$ my_gempro.df_representative_sequences.head()
with open('key_phrases_rake.pickle', 'rb') as f:$     key_phrases_rake = pickle.load(f)
df2 = df$ mismatch_index = mismatch_df.index$ df2 = df2.drop(mismatch_index)
print(bag.toarray())
test_df[["id", "labels"]].to_csv("submission_8.27.csv", index=False)
country_dummies=pd.get_dummies(df_new['country'])$ df_new=df_new.join(country_dummies)$ df_new.head()
dev4['avg_rank'] = dev4[[c for c in dev4.columns if c.startswith('rank_')]].apply(np.mean, axis=1)$ dev4['rank'] = dev4.avg_rank.rank()
rtc = pd.read_excel('input/data/ExogenousTransmissionCapacity.xlsx',$                     pars_cols='B:R',$                     header=3)
messages=df.status_message.tolist()$ messages[:5]
tweet_df["tweet_source"].unique()
print("Porcentaje de tweets positivos: {}%".format(len(tweets_positivos)*100/len(datos['Tweets'])))$ print("Porcentaje de tweets neutros: {}%".format(len(tweets_neutros)*100/len(datos['Tweets'])))$ print("Porcentaje de tweets negativos: {}%".format(len(tweets_negativos)*100/len(datos['Tweets'])))
print("Min " + str(dc['created_at'].min()) + " Max " + str(dc['created_at'].max()))$ print("Min " + str(tm['created_at'].min()) + " Max " + str(tm['created_at'].max()))
df['upper'] = df['body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))$ df[['body','upper']].head()
df[['beer_name', 'rating_score', 'simple_style', 'brewery_name', 'brewery_country']][df.rating_score < 2].sort_values('rating_score')
shows = pd.DataFrame(data={'title':titles,'status':statuses,'years':years,'network':networks,'genre':genres,\$                           'tagline':taglines,'link':links})
df_full["Field4"] = df_full.Field4.fillna("None")
stock_data.describe()
for field_name, dtype in df.select_dtypes(include=categories).items():$     print(field_name)$     df[field_name] = pd.Series(pd.Categorical(df[field_name]).codes)
pd.date_range('2014-08-01 12:10:01',freq='S',periods=10)
file_name = 'campaign_finance_clean_data.csv'$ dat.to_csv(path_or_buf=file_name,sep=',')
df_movies.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/movies.csv', sep=',', encoding='utf-8', header=True)
calls = pd.read_csv("311_Calls__2012-Present_.csv")
len(df[~(df.groups == {})])
significance_level = 0.05$ confidence_level = 1 - significance_level$
feature_set = layer.query(out_sr={'wkid': 4326})$
results_df = pd.DataFrame(dict_results).set_index("Username").round(2)$ results_df
plt.plot(losses[:])$
Customer.withdraw(jeff, 200.0)$ jeff.balance           # Shows 700.0
station_df['station'].count()
df_ncrs = pd.read_excel('ncr_data.xlsx', index='Notification')$ df_ncrs.head()
words_only_sk_freq = FreqDist(words_only_sk)$ print('The 100 most frequent terms (terms only): ', words_only_sk_freq.most_common(30))
cp = nltk.RegexpParser(grammar)$ result = cp.parse(sentence)$ print(result)
SCN_BDAY = pd.merge(BID_PLANS_df,pd.to_datetime(BDAY_PAIR_df['birthdate']).dt.strftime('%Y-%m-%d').to_frame(),how='left',left_index=True,right_index=True)
texts = df[df['section_text'].str.contains('fees')][['filename','section_text']].values$ len(texts)
month.columns = ['MONTH_'+str(col) for col in month.columns]
for i1 in range(2):$     writer = pd.ExcelWriter('Bitfinex/Bitfinex_2017.xlsx')$     df_table_Bitfinex.to_excel(writer, 'Bitfinex_2017', index = False, header = True)
cityID = '944c03c1d85ef480'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fresno.append(tweet) 
response_raw = requests.get(url,params)$ response_clean = response_raw.content.decode('utf-8')
precipitation_df['date'] = pd.to_datetime(precipitation_df["date"]).dt.date$ precipitation_df.set_index(["date"], inplace = True)
traffic_df_rsmpld = traffic_df_rsmpld.sort_index().loc[idx['2016-01-01':'2017-12-31',:],:].sort_index()$ traffic_df_rsmpld.info()$ traffic_df_rsmpld.head()
texts = df[df['section_text'].str.contains('fees')]['section_text'].values[0:5]
page_soup.body.div
df = df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]
df.iloc[:4]
index = pd.date_range('2018-3-1', periods=1000)$ index
data = originaldata.copy()$ data = data.reset_index()$ del data["index"]
sns.heatmap(data_numeric.corr().abs().round(2),annot=True)
fin_coins_r.shape
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid$ x = df30458.groupby('end_station_name').count()$ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
malebydate = male.groupby(['Date','Sex']).count().reset_index()$ malebydate.head(3)
df.rename(columns={'value':'lux'},inplace=True)
for i, row in breakfastlunchdinner.iterrows():$     breakfastlunchdinner.loc[i, 'day_sales'] = sum(row[1:]) * .002 $ breakfastlunchdinner
df.index = df['Date']
bbc_df = constructDF("@BBC")$ display(constructDF("@BBC").head())
    return stemmer.stem(word)
oz_stops.loc[oz_stops['stopid'] == '7270']
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_jp_wzh_2.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_wzh_2.w2v')
unique_Taskers_shown_most = sample['tasker_id'].value_counts().head(1)$ unique_Taskers_shown_most
elms_all_0611.loc[range(1048575)].to_excel(cwd+'\\ELMS-DE backup\\elms_all_0611_part1.xlsx', index=False)
df_ll = pd.read_csv("loblaws_all.csv", encoding="latin-1")
pipeline.fit(X_train, y_train)
dbcon = sqlite3.connect("mobiledata.db")$
df['HL_PCT']=(df['Adj. High']-df['Adj. Close'])/df['Adj. Close']*100.0
Delta_r = +0.04$ Delta_Y_PS12taskB = -1
tmp = df[selected_features].join(outcome_scaled).reset_index().set_index('date')$ tmp.dropna().resample('Q').apply(lambda x: x.corr()).iloc[:,-1].unstack()\$ .iloc[:,:-1].plot(title='Correlation of Features to Outcome\n (by quarter)')$
s3.reindex(np.arange(0,7), method='bfill')
r = requests.get ('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=4zX7CA93VkKSEygsbSLv')$ print (r.json()['dataset']['data'][0])
df2 = pd.read_csv('comma_delim_clean.csv', index_col='id')$
df_test_user_2 = df_test_user.copy()$ df_test_user_2['created_on'] = '2017-09-20 00:00:00'
xml_in['authorId'].nunique()
sp = openmc.StatePoint('statepoint.20.h5')
import numpy as np$ ok.grade('q04')
kick_projects_ip_copy= kick_projects_ip.copy()
cabs_df_rsmpld = cabs_df_byday.resample('1M')['passenger_count'].count()$ cabs_df_rsmpld.head()
reddit_master['Class_comments'].value_counts()/reddit_master.shape[0]
df2[((df2['group'] == 'treatment') ==(df2['landing_page'] == 'new_page')) == False].shape[0]
d1.mean().mean() # caution! this isn't the real mean$
print (quand_request.json())
the_means = np.mean(the_sample, axis=1)$ print(f'Mean ({num_samples} samples) = {np.mean(the_means):5.3f}')$ print(f'Standard deviation ({num_samples} samples) = {np.std(the_means):5.3f}')
df.injured.describe()
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
y_clf = np.where(y_tr >= 1800, 1, 0)$ print(np.where(y_clf==0)[0].shape)$ print(np.where(y_clf==1)[0].shape)
print("prediction: {0}, actual: {1}, 2018-03-31".format(y_pred_list[0], y_test_aapl[0,0]))$
df = pd.concat(map(pd.read_csv, glob.glob("*.csv")))
tweets ['apple'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'apple' in x else False)$ tweets ['samsung'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'samsung' in x else False)
re.sub(rpt_regex, rpt_repl, "Reppppeated characters in wordsssssssss" )
df_all_users['Email Address'] = df_all_users['Email Address'].apply(lambda x: str(x.lower().strip()))$ df_all_users['Email Address'] = df_all_users['Email Address'].astype(str)
pd.pivot_table(bb_df, values = ["Wt"], index = ['Pos'], aggfunc = np.mean).sort_values(by = ['Wt'], ascending = False)
r.json()['dataset']['data'][0]$
from pyspark.sql.functions import *$ display(flight2.select(max("start_date")).show())$ display(flight2.select(min("start_date")).show())
cvec.fit(X_train)$ X_train_matrix = cvec.transform(X_train)$ print(X_train_matrix[:5])
np_diff = np.diff(close)$ max(np_diff)$
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]
df_rs = master_df_total.sample(frac = 0.085, random_state = 1)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Therapist Hours Autocorrelation')
nbart_allsensors =nbart_allsensors.sortby('time')
base_df.tail()
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'),how='inner')
all_data_merge = pd.merge(all_data, mapping, on = ['rpc'])$ all_data_merge.shape
%sc$ !wget 'https://s3.amazonaws.com/crimexyz/crime.csv'
Not_same = df_3['user_id'].count() + df_43['user_id'].count() $ Not_same$
dictionary=json.loads(json_string) #Convert string to dictionary
measurements_df.count()
S.decision_obj.stomResist.options
oz_stops = oz_stops.rename(columns={'stop_id': 'stopid'})$ oz_stops.head(1)
RunSQL(sql_query)$ actor = pd.read_sql_query(sql_query, engine)$ actor.head()
data_AFX_X['diff_rows'] = data_AFX_X['Close'].diff(periods=-1)$ data_AFX_X.describe()$ min(data_AFX_X['diff_rows'])
keys_0611 = keys.copy()$ keys_0611.to_excel(cwd + '\\ELMS-DE backup\\keys_0611.xlsx', index=False)
wb.save('most_excellent.xlsx')
ml.run_ml_flow(df1)
pd.Series(np.random.randn(5))
INT=pd.read_csv('C:/Users/mjc341/Desktop/UMAN 1507 Monthly INQ summary Report/Interactions.Contacts.csv',skipfooter=5,encoding='latin-1',engine ='python')$
pivoted.shape
edu_gen_edad = pd.merge(educacion, genero_edad, on = 'idpostulante', how = 'inner')$ edu_gen_edad.head(1)
points.iloc[6]$
data_2018= data_2018.set_index('time')
with client_hdfs.read('/mydata/helloworld.csv', encoding = 'utf-8') as reader:$     readdf=pd.read_csv(reader,index_col=0)
indeed1 = indeed[['company','location','skills','title','salary_clean','category','duration_int','summary_clean']]$ indeed1.shape
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
twelve_months_prcp.head()
sample = df.sample(n=1000, replace=False)
predictions=model.predict(X)$ rounded =[round(x[0]) for x in predictions]$ print(rounded)
pd.set_option('display.max_colwidth', 100)$ clinton_df.head()
organize_data.to_csv("NewsAccountData.csv")
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='API_KEY$ r = requests.get(url)
kick_projects = pd.merge(kick_projects, ks_ppb, on = ['category', 'launched_year','goal_cat_perc'], how = 'left')
np.shape(temp_fine)
mean = np.mean(data['len'])$ print("The length's average in tweets: {}".format(mean))
for col in X_nnumcols:$     X[col] = X[col].apply(lambda l: hash(l))$     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
DBPATH = 'results.db'$ conn = sqlite3.connect(DBPATH)$ cur = conn.cursor()
plt.savefig("Scatter.jpg")$ plt.show()
top_genre= pd.read_sql_query('select * from top_genre', engine)$ top_genre.head()
store.delete_collection('NASDAQ.EOD')
convRate = pd.concat([hired,shown],axis=1)$ convRate['rate'] = convRate['hired']/convRate['tasker_id']
chinese_vessels.isnull().sum()
conn = 'mongodb://localhost:27017'$ client = pymongo.MongoClient(conn)$
data_2018 = pd.read_csv(filename_2018)
new_style_url='https://raw.githubusercontent.com/neilpanchal/spinzero-jupyter-theme/master/custom.css'$ print("Will be using css from {}".format(new_style_url))
autoDf1 = SpSession.read.csv("auto-data.csv",header=True)$ print (autoDf1.show())$
Precipitation_DF.describe()
users = users.sort_values(by=['avg_score'], ascending=False)$ top_25_users = users.index[1:50].tolist()
type(df.date[0])
left.head()$
tobs = session.query(Measure.station, Measure.date, Measure.tobs).order_by(Measure.date).\$ filter(Measure.date > '2016-08-23').all()$ tobs
points.name="WorldCup"$ points.index.name="Previous Points"$ points$
store_items = store_items.drop(['store 1', 'store 2'], axis=0)$ store_items
print(len(free_data.country.unique()))
mask = y_test.index$ t_flag = y_test == 0$ p_flag = pred == 1
df2 = pd.DataFrame(np.array([[10, 11], [20, 21]]), columns=['a', 'b'])$ df2
x =  store_items.isnull().sum().sum()$ print('Number of NaN values in our DataFrame:', x)
rain_df.describe()$
converted = ts.asfreq('45Min', method='pad')
model_data.to_hdf('..//data//model//model_data_12H.h5', key = 'xyz', complib = 'blosc')
Total_Number_of_Rides_max = rides_analysis["Total Number of Rides"].max()$ Total_Number_of_Rides_max
executable_path = {'executable_path': "browser = Browser('executable_path', C:\Users\Brittney_Joyce\AppData\Local\Temp\Temp1_chromedriver_win32.zip)"}$ browser = Browser('chrome', **executable_path)$ browser.visit(url)
for node in nodes:$     print node.text_content()
np.random.seed(123456)$ bymin = pd.Series(np.random.randn(24*60*90),pd.date_range('2014-08-01','2014-10-29 23:59',freq='T'))$ bymin
live_capture = pyshark.LiveCapture(interface='wlan1')
h2o.init()
lm.fit(x_train, y_train)
tweets_raw["date"] = tweets_raw["date"].apply(lambda d: parse(d, ignoretz = True))
pm_data.dropna(inplace = True)$ pm_data.shape
plantlist.capacity_net_bnetza = plantlist.capacity_net_bnetza.round(decimals=5)$ plantlist.capacity_gross_uba = plantlist.capacity_gross_uba.round(decimals=5)  $ plantlist.efficiency_estimate = plantlist.efficiency_estimate.round(decimals=5)
input_col = ['msno','plan_list_price','actual_amount_paid','payment_plan_days']$ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions',input_col)$
dat_dow.vgplot.line(value_name='Hospital mortality rate')
reddit.Upvotes.value_counts(ascending=False).head(25) #just seeing the number of upvotes for each Reddit post$
if not os.path.isdir('output'):$     os.makedirs('output')
prcp_df.describe()
print(prec_nc)$ for v in prec_nc.variables:$     print(prec_nc.variables[v])
print(list(cos.buckets.all()))
df3 = tier1_df.reset_index()$ df3 = df3.rename(columns={'Date':'ds', 'Incidents':'y'})
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
output= "Update user SET following=50 where user_id='@Pratik'"$ cursor.execute(output)$
print(open('datasets/git_log_excerpt.csv'))
df4['Date'] = pd.to_datetime(df4['Date'])$ df4.dtypes
cityID = '84229b03659050aa'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Virginia_Beach.append(tweet) 
s3.reindex(np.arange(0,7), method='ffill')
weather.info()
store_items.pop('new watches')$ store_items
response = requests.get("https://data.bitcoinity.org/markets/bidask_sum/24h/USD/bitfinex?bp=1&bu=c&r=minute&t=m")
s = pd.Series(todays_datetimes)$ s.diff().mean()
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(5))
bucket.upload_dir('data/city-util/raw', 'city-util/raw', clear_dest_dir=True)
a.iloc[:3]
final_rm=final.drop(target0.index)$ X=create_time_list(final_rm)
collection = store.collection('NASDAQ.EOD')$ collection
calls_nocontact = calls[calls.location != '(530187.70942, 3678691.8167)']
viz_1=sb.countplot(x=studies_b.enrollment, data=studies_b)$
b = 2. * np.random.randn(*a.shape) + 1.$ b.shape
ts.tshift(-1,freq="H")
start = datetime.datetime(2010, 1, 1)$ df = web.DataReader("GOOG", 'yahoo', start)$ df.head()
uber1.to_csv('uber1.csv', index=False)$ uber2.to_csv('uber2.csv', index=False)$ uber3.to_csv('uber3.csv', index=False)
!wget -O ChurnData.csv https://ibm.box.com/shared/static/8s8dn9gam7ipqb42cm4aehmbb26zkekl.csv
contractor_clean.head() #these columns have been dropped
result = pd.concat([df1, df3], axis = 0) # concatenate one dataframe on another along rows$ result
df.reset_index(inplace=True, drop=True)
!hdfs dfs -cat 32ordered_results-output/part-0000* > 32ordered_results-output.txt$ !tail 32ordered_results-output.txt
daily_change = [(daily_p[2]-daily_p[3]) for daily_p in afx_17['dataset']['data'] ]$ dc= (max(daily_change))$ print('The largest change in any one day is $%.2f.'% dc)
S_1dRichards = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_1dRichards.txt')
year_dict = year_request.json()$ print (type(year_dict))
plt.xlim(100, 0)$ plt.ylim(-1, 1)
all_df = train_df.append(test_df)$
year9 = driver.find_elements_by_class_name('yr-button')[8]$ year9.click()
from time import mktime$ current_time.timetuple()$ mktime(current_time.timetuple())
df1_after_df2 = df2.append(df1)$ df1_after_df2
dftemp = df1[(df1['Area'] == "Iceland")]$ dftemp.head(10)$
import numpy as np$ ok.grade('q04')
combined_df[combined_df['classifier_summary'].isnull() == True]
print(festivals.dtypes)$ print(festivals.info())$
info_final.drop('idAviso', axis = 1, inplace = True)$
datascience_tweets[datascience_tweets["text"].str.contains("RT")==False]['text'].count() # 895
sns.barplot(data=df.groupby('purpose').agg({'applicant_id':lambda x:len(set(x))}).reset_index(),$             x='purpose',y='applicant_id')
from config import config$ CONFIGS = config.Config.get(env='prod', caller_info=False)
grid_lat = np.arange(np.min(lat_us), np.max(lat_us), 1)$ grid_lon = np.arange(np.min(lon_us), np.max(lon_us), 1)$ glons, glats = np.meshgrid(grid_lon, grid_lat)
auth = tweepy.OAuthHandler(consumer_key,consumer_secret)$ auth.set_access_token(access_token,access_token_secret)$ api = tweepy.API(auth) 
validation.analysis(observation_data, richard_simulation)
temp_long_df = pd.melt(temp_wide_df, id_vars = ['grid_id', 'glon', 'glat'],$                       var_name = "date", value_name = "temp_c")$ temp_long_df.head()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-01-02&api_key=" + API_KEY)
S.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
twitter_url = "https://twitter.com/marswxreport?lang=en"$ browser.visit(twitter_url)
QLESQ = QLESQ.dropna(axis=1, how='all')$ QLESQ.columns
open_list = [d[dt]['Open'] for dt in d.keys() if d[dt]['Open'] is not None]$
api = twitter.Api(consumer_key = consumer_key, consumer_secret=consumer_secret,$                  access_token_key=access_token,access_token_secret = access_token_secret,$                   sleep_on_rate_limit=False)
session.query(Measure.date).order_by(Measure.date.desc()).first()
results.to_csv("mb.csv")$ results.to_pickle("football_pickle")$ pd.read_pickle("football_pickle")
year_to_date = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(year_to_date)
dc['created_at'].hist(color="blue") # blue, David Cameron$ tm['created_at'].hist(color="orange") # orange, Theresa May
data = r.json()$
tweetDF = sqlContext.createDataFrame(data, filtered_check_list)
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig8_left_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
from statsmodels.tsa.arima_model import ARIMA$ model_701 = ARIMA(dta_701, (4, 0, 0)).fit() $ model_701.forecast(5)[:1] 
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
max(dif_dict, key=dif_dict.get) 
df = actuals.merge(backcast, on='Gas_Date')
nt_price = nt.groupby(pd.Grouper(freq='W'))["sq_price_value"].mean().to_frame().rename(index=str, columns={"sq_price_value":"nt_sq_price_value"})
calls_nocontact.street_address.value_counts()
SP_data = "..\Raw Data\S&P 500 Raw data_1-20-2017 ~ 3-21-2018.csv"$ SP_df = pd.read_csv(SP_data, encoding = "ISO-8859-1")$ SP_df.head()
indeed[indeed['summary'].isnull()]$
import pickle$ filename = 'automl_feat.sav'$ pickle.dump(automl_feat, open(filename, 'wb'))
print(df2,'\n')$ print(df2[df2['E'].isin(['test'])])  # select E column=='test' only 
contractor[contractor.contractor_id.duplicated() == True]
sns.jointplot(x = "positive_ratio", y = "negative_ratio", data = news_df)
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url)
df_sched = pd.read_csv(schedFile, usecols = schedCols, $                  dtype = sched_dtypes)
files4= files4.rename(columns={'jobid':'jobId'})$ files4['jobcandidate']=files4['jobId'].astype(str)+'-'+files4['candidateid'].astype(str)
for df in (joined, joined_test):$   df['Promo2Since']=pd.to_datetime(df.apply(lambda x: Week(x['Promo2SinceYear'],x['Promo2SinceWeek']).monday(),axis=1).astype(pd.datetime))$   df['Promo2Days']=df['Date'].subtract(df['Promo2Since']).dt.days
ca_all_path = cwd + '\\LeadGen\\Ad hoc\\SubID\\CA_SubID_from_python.csv'$ ca_all.to_csv(ca_all_path, index=False)
arr_size = reflClean.shape$ arr_size
t3=t3.rename(columns={"id": "tweet_id"})
print('Before removing reactivations:',df.shape)$ df = df[df.Injury != 0]$ print('With only placements onto the Disabled List:',df.shape)
r = requests.get("https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2017-01-01&end_date=2017-01-03&api_key="", auth=(''))$
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df.sample(10)
financial_crisis.loc['Tulip Mania']
people.shape
for c in ccc:$     vhd[c] /= vhd[c].max()
df=json_normalize(data["dataset"], "data")$ df.columns = col_names$ df.head(3)
extract_deduped_with_elms_v2.shape, extract_deduped_with_elms.shape
pd.read_csv(r'C:\Users\Patrik\Downloads\webrobots.iokickstarter-datasets\Kickstarter_2017-10-15T10_20_38_271Z\Kickstarter016.csv').info()
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/mydata.json"$ df = pd.read_json(path)$ df.head(5)
sns.set(style="white", color_codes=True)$ sns_plot = sns.jointplot(x = source_pos_df['Position_x'], y = source_pos_df['Position_y'], kind='kde', color="skyblue")$ sns_plot.savefig('kde_source.png')
cnx.commit()$ c.fetchall()
daily_df = twitter_daily_df.join(stock_df, ["Day","Company"]).orderBy('Day','Company')
TERM2017 = INT.loc[(INT.Term == 'Fall 2017')]$ TERM2018 = INT.loc[(INT.Term == 'Fall 2018')]$ TERM2019 = INT.loc[(INT.Term == 'Fall 2019')]$
bins = [-1,-0.5,0,0.5,1]$ group_names = ["Between -1 and -0.5", "Between -0.5 and 0", "Between 0 and 0.5", "Between 0.5 and 1"]$ sentiment_df["Sentiment group"] = pd.cut(sentiment_df["Compound score"], bins, right = False, labels=group_names)
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 0), 'type'] = 'inside_gamers'
num_dr_new = dr_new['Provider'].resample('W-MON', lambda x: x.nunique())$ num_dr_existing = dr_existing['Provider'].resample('W-MON', lambda x: x.nunique())
file = open(resumePath, "r")$ resume = file.read()$ corpus.insert(0, resume)
definition_details = client.repository.store_definition(filename_keras, model_definition_metadata)
avgcomp = groupedNews['Compound'].mean()$ avgcomp.head()
lm.pvalues
ave_sentiment_by_company = unique_sentiments_df.groupby("Symbol")["Compound"].mean()$ ave_sentiment_by_company
pdiff = (new_page_converted.mean()) - (old_page_converted.mean())$
stories.submitter_user[0]
final_valid_pred_nbsvm1 = valid_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
print(len(countdf['user'].unique()))$ print(len(countdf['user'].unique())-len(count1df['user'].unique()))$ print(len(countdf['user'].unique())-len(count6df['user'].unique()))
outfile = os.path.join("Resource_CSVs","Main_data_revised.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
p_old = df2['converted'].mean()$ p_old
sns.distplot(temp_df[temp_df.total_companies > 100].proportion_no_psc)
gram_collection.find_one({"account": "deluxetattoochicago"})['date_added']$
airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')$ print(airquality_pivot.head())
FREEVIEW.plot_main_sequence(raw_freeview_df)
min_div_stock=df.iloc[df["Dividend Yield"].idxmin()]$ min_div_stock$ print("The stock with the minimum dividend yield is %s with yield %s" % (min_div_stock['Company Name'],min_div_stock['Dividend Yield']))
merged_portfolio_sp['Equiv SP Shares'] = merged_portfolio_sp['Cost Basis'] / merged_portfolio_sp['SP 500 Initial Close']$ merged_portfolio_sp.head()
store_items.fillna(method='ffill', axis=0) # filled with previous value from the column
req_test.text
datetime.now().toordinal()/365$
sentimentDf.sort_values("date", inplace=True, ascending=True)$ sentimentDf.head()
df7.loc[df7['avg_dew_point Created'] == 'dry', 'avg_dew_point Created'] = np.nan$ df7['avg_dew_point Created'].value_counts(dropna=False)
rf = pickle.load(open('../data/model_data/size_mod.sav', 'rb'))
no3Mask = nitrogen['DetectionQuantitationLimitMeasure/MeasureUnitCode'] == 'mg/l NO3'$ nitrogen.loc[no3Mask,'TotalN'] = nitrogen['TotalN'] * 0.2259
df2['timestamp'] = pd.date_range('8/8/2018', periods=len(df2['MATCHKEY']), freq='D')
!wget http://files.fast.ai/data/dogscats.zip && unzip dogscats.zip -d data/
top_songs[top_songs['Track Name'].isnull()]['Region'].unique()$
time_series['count'].sum() == (nodes.shape[0] + ways.shape[0])
studies_a = pd.DataFrame(studies, columns=['why_stopped','verification_date','target_duration','study_type','start_date_type','start_date','source','phase','overall_status','official_title','number_of_arms','nct_id','limitations_and_caveats','last_known_status','last_changed_date','is_unapproved_device','is_fda_regulated_drug','is_fda_regulated_device','enrollment_type','enrollment','completion_date','brief_title','baseline_population'])$ studies_a.head()
res = es.search()$ print(res["_shards"])$
sns.distplot(utility_patents_subset_df.prosecution_period, color="orange")$ plt.show()
json_data = r.json()$ print(json_data)
last_year = dt.date(2018, 6, 2) - dt.timedelta(days=365)$ print(last_year)
hi_iday_delta_index = close_deltas.index(max(close_delta for close_delta in close_deltas if close_delta is not None))$ ans_5 = ('%s had greatest between-day difference: %s' % (dates[hi_iday_delta_index], close_deltas[hi_iday_delta_index]))
df_users_test = df_users.iloc[:2, :]$ df_users_test.created_on[1] = '2017-09-20'$ df_users_test
    def __init__(self, point, x):$         super().__init__(point, x, x)
df[y_col] = df['variety'].str.lower().replace(repl_dir)$ df_noblends = df[df[y_col].replace(repl_dir).str.lower().isin(keep_vars)]$ df_noblends[y_col].unique().size
n_old = df2.query(('landing_page == "old_page"')).count()[0]$ n_old
converted = ts.asfreq('1Min', method='pad')
DataSet[['userName','tweetFavoriteCt']].sort_values('tweetFavoriteCt',ascending=False).head(10)
delimited_hourly.reset_index(inplace=True)$ delimited_hourly.head()
data = pd.read_csv('data/analisis_invierno_3.csv')
calls_nocontact.council_district.value_counts()
(act_diff < p_diffs).mean()
df_control = df2.query('group=="control"')$ y_control = df_control["user_id"].count()$
store_items = store_items.rename(columns={'bikes': 'hats'})$ store_items
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
print("Today's Date is:", dt.date.today())$ query_date = dt.date.today() - dt.timedelta(days=365)$ print("One year ago today is:", query_date)$
model_df['score_str'] = "x"$ model_df.score_str[model_df.score <= model_df.score.quantile(.5)] = "below_avg"$ model_df.score_str[model_df.score > model_df.score.quantile(.5)] = "above_avg"
sites.dtypes
df = web.get_data_yahoo("SPY",start ="2000-01-01",end = "20-03-2018")
print('Outliers are points below {} or above {}.'.format((scores_firstq - (1.5 * IQR)), (scores_thirdq + (1.5 * IQR))))
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')$ git_log['timestamp'].describe()
groceries.drop('apples')
Image("../../raw_data/images/visual_studio_community.png", width=1000)
userProfile = userGenreTable.transpose().dot(inputMovies['rating'])$ userProfile
import numpy as np$ data = np.random.normal(0.0, 1.0, 1000000)$ np.testing.assert_almost_equal(np.mean(data), 0.0, decimal = 2)
datos['AdS'] = np.array([ analiza_sentimiento(tweet) for tweet in datos['Tweets'] ])$ display(datos.head(10))
joined = joined.dropna(axis=0)$ print('Number of rows in joined = {}'.format(joined.CustomerID.count()))
avgPurchU = train.groupby(by='User_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseU'})$ train = train.merge(avgPurchU, on='User_ID', how='left')$ test = test.merge(avgPurchU, on= 'User_ID', how='left')
titles_list = temp_df2['titles'].tolist()
df = pd.DataFrame(x_9d)$ df = df[[0,1,2]] # only want to visualise relationships between first 3 projections$ df['X_cluster'] = X_clustered
USvideos.describe()$ USvideos.head()
df['date'] = df['date'].apply(lambda time: time.strftime('%m-%d-%Y'))$ df['time'] = df['time'].apply(lambda time: time.strftime('%H:%M'))
QUIDS = QUIDS.loc[(QUIDS['week'] == 0) |(QUIDS['week'] == 12)  |(QUIDS['week'] == 14)]$ QUIDS = QUIDS[["subjectkey", "qstot", "week"]].sort_values(['subjectkey', 'week'])
jobs=df['job'].unique()
measure_df.head()$
temp_df = pd.DataFrame(temps)
data.dtypes
data_air_visit_data.loc[:,['air_store_id', 'visitors']].groupby('air_store_id').size()$
df2.drop(labels=2862, inplace=True)$ sum(df2['user_id'].duplicated())
data.groupby('SA')['RTs'].sum().plot(kind = 'barh')
cityID = 'ac88a4f17a51c7fc'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Portland.append(tweet) 
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=["Tweets"])$ display(data.tail(10))
from sightengine.client import SightengineClient$ client = SightengineClient("737618018", "bstrJ5VzARavYy5FsELN")$ output = client.check('face-attributes').set_url('http://cache.magazine-avantages.fr/data/photo/w1000_c18/4j/hommebrunyeuxverts.jpg')
at_messages = non_na_df[non_na_df.message_text.str.startswith('@')]
df_writer_combined_text['combined_text_tokenized'] = df_writer_combined_text['combined_text'].apply(lambda x: nlp(x))$
tlen.plot(figsize=(16,4), color='r');
lm3 = sm.Logit(ab_df_new['converted'], ab_df_new[['intercept','CA','UK','treatment_US','treatment_CA']])$ result3 = lm3.fit()$ result3.summary2()
ds_issm = xr.open_dataset(data_url1)$ ds_issm = ds_issm.swap_dims({'obs': 'time'})$ ds_issm
table_rows = driver.find_elements_by_tag_name("tbody")[6].find_elements_by_tag_name("tr")$
testurl = r"https://www.cobbgis.org/openimage/bravescam/Cam128/Cam_12818-06-27_21-00-09-03.jpg"$ get_datetime_from_cobburl(testurl)
data['Sentiments'] = np.array([ predict_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df = pd.read_sql('SELECT * from customer', con=conn_b)$ df
Z = np.random.randint(0,10,(3,3))$ print(Z)$ print(Z[Z[:,1].argsort()])
aug2014 = pd.Period('2014-08',freq='M')$ aug2014
df.iloc[[1]]
for v in data.values():$     if v['answers']['Q4'] == 'No':$         v['answers']['Q4A'] = 'n/a'
go_no_go = 1 #NO_GO - ie do NOT run all the long run-time items.  Note that some of these long$
results_para = soup.find_all("div", { "class" : "rollover_description_inner" })$ news_p = results_para[0].text$ news_p$
df['end date'] = df.index.map(lambda x:x.end_time)$ df
data=requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=%s" %API_KEY)
pd.read_sql('SELECT * FROM experiments', conn, index_col='experiment_id')
body = soup.body$ for paragraph in body.find_all('p'):$     print (paragraph.text)
flight_pd.to_csv('/home/ubuntu/parquet/flight_pd.csv', sep='\t')
workspace_uuid = ekos.get_unique_id_for_alias(user_id, 'lena_indicator')$
cityID = '04cb31bae3b3af93'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Miami.append(tweet) 
outfile = os.path.join("Resource_CSVs","Main_data_positive.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
print ("The number Unique userid in dataset is {}".format(df.user_id.drop_duplicates().count()))
grpConfidence['MeanFlow_cfs'].count()
%matplotlib inline$ from IPython.display import Image$ from IPython.core.display import HTML
grid_id = np.arange(1, 1535,1)$ grid_id_array = np.reshape(grid_id, (26,59))$ grid_id_flat = grid_id_array.flatten()
flight = spark.read.parquet("/home/ubuntu/parquet/flight.parquet")
df_master_select = df_master_select.dropna()$ df_master_select.head()
temp_df2 = temp_df.drop_duplicates()
px = pd.read_csv(dwld_key + '-hold-pricing.csv', index_col='Date', parse_dates=True)
df = df.drop("a")$ df
nasa_url = 'https://mars.nasa.gov/news/'$ jup_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
e.__class__
df.iloc[(len(df)-lookforward_window)-3:(len(df)-lookforward_window),:]
S_distributedTopmodel.decision_obj.bcLowrSoiH.options, S_distributedTopmodel.decision_obj.bcLowrSoiH.value
thisDir = os.getcwd()$ csvDir = thisDir + '/../dbases/'
geometry.export_to_xml()
tweet_image_clean.shape$
news_title = soup.find_all("div", class_="content_title")[0].text$ print(news_title)
f.groupby('Price').mean()
type(ts.index)
os.chdir('/Users/AlexandraDing/Desktop')$ pickle.dump( top_movies_list, open( "top_movies_100_year2016_list.p", "wb" ) )$
ORDER_BPAIR_POSTGEN.columns
precipitation = measure_avg_prcp_year_df['Avg Prcp'].describe()$ pd.DataFrame(precipitation, columns=['Avg Prcp'])
merged1.drop('Id', axis=1, inplace=True)
new_doc = repos[4]$ new_vec = dictionary.doc2bow(new_doc)$ repos_langs.iloc[4, 0] # Para testar nossa hipotese vamos encontrar usuarios similares ao usuario 4
femalebydate = female.groupby(['Date','Sex']).count().reset_index()$ femalebydate.head(3)
step_counts = step_counts.fillna(0.)
idx = df2['Number'].values$ Xtrain = Xtrain[idx,:] $ Xtrain.shape
url_df_full=url_df[url_df['url'].isnull()==False]
y_pred = model.predict(X_test)$ predictions = y_pred.tolist()
%%time$ treehouse_expression_hugo_tpm = treehouse_expression.apply(np.exp2).subtract(1.0).add(0.001).apply(np.log2)
coefs.loc['age', :]
events = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).last().reset_index()$ events = events[['atbat_pk', 'events', 'woba_value']]
data = data.dropna()
newfile = newfile[~newfile['Ov.transport status'].isin({5, 6, 7})]
sns.factorplot('sex', data=titanic3, hue='pclass', kind='count')
workspace_ids = []$ for i in workspaces_list:$     workspace_ids.append(i['id'])$
len([earlyPr for earlyPr in BDAY_PAIR_df.pair_age if earlyPr < 3])/BDAY_PAIR_df.pair_age.count()
the_data = tmp_df.applymap(lambda x: 1 if x > 3 else 0).as_matrix()$ print(the_data.shape)
print("Probability of control group converting:", $       df2[df2['group']=='control']['converted'].mean())
temp_df = temp_df.join(metadata.set_index('order_num'), how='left', on='order_num')$ temp_df['year'] = temp_df.date.dt.year
poparr2 = fe.bs.hybrid2ret(poparr,$                            mean=-fe.bs.SPXmean, sigma=fe.bs.SPXsigma,$                            yearly=256)
corpus = [dictionary.doc2bow(text) for text in texts]$ corpora.MmCorpus.serialize('bible.mm', corpus)$ print(corpus)
import string$ punct_re = r'[^\s\w\d]'$ trump['no_punc'] = trump['text'].str.replace(re.compile(punct_re), ' ')$
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit = 's')$ git_log['timestamp'].describe()
df7.loc[df7['avg_health_index Created'] == 'low', 'avg_health_index Created'] = np.nan$ df7['avg_health_index Created'].value_counts(dropna=False)
loan_stats["loan_status"].table()
G = nx.Graph() #creates empty graph, initiliasize a graph object$ G.add_nodes_from(node_names)$ G.add_edges_from(edges)
quandl.ApiConfig.api_key = API_KEY$ quandl.get('FSE/AFX_X', start_date='2018-08-08', end_date='2018-08-08')
FORMAT = '%(asctime)s : %(levelname)s : %(message)s'$ logging.basicConfig(format=FORMAT)$ logging.getLogger().setLevel(level=logging.INFO)$
import gp$ import genepattern$ genepattern.GPAuthWidget(genepattern.register_session("https://gp-beta-ami.genepattern.org/gp", "", ""))
df_test_index = pd.DataFrame()$
max_name_length = (df['Name'].map(len).max())$ print("Longest name:", max_name_length)
fakeNews = trump.loc[trump['text'].str.contains("fake"),:]$ ax = sns.kdeplot(fakeNews['year'], label="'Fake' word  usage")$
most_temp_info = session.query(Measurement.station, Measurement.tobs).filter(Measurement.station == most_busy).all()$ most_temp_info
customers_df = pd.read_csv('./data/User_Information.csv')$ print(len(customers_df))$ customers_df.head()
regression_estimation(X,final_df['mean'])$ classification_estimation(X,final_df['mean'].map(lambda x: x*10))$
file_names = []$ file_names = glob.glob('../craigslist-data/final-data-multiple-cities/*.csv')
print(df.tail())
r = requests.get("https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2014-06-01&end_date=2014-06-02&api_key="+API_KEY)$ print(r.json())
crimes[(crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE')&(crimes['SECONDARY_DESCRIPTION']=='TO PROPERTY')].head()$
merged1.drop('id_y', axis=1, inplace=True)
local_tz = pytz.timezone('America/New_York')
type.__new__(type,'A',(),{"a":1})
siteFeatures = read.getSamplingFeatures(type='Site')$ df = pd.DataFrame.from_records([vars(sf) for sf in siteFeatures if sf.Latitude])
seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)
column_list2 = ['Temperature','DewPoint']$ df[column_list2].plot()$ plt.show()
kushy_prod_data_path = "products-kushy_api.2017-11-14.csv"$ kushy_prod_df = pd.read_csv(kushy_prod_data_path, low_memory=False)$ kushy_prod_df.tail(10)
data_issues=pd.read_json('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-issues.json',lines=True)
codes = pd.read_csv('data/icd-main.csv')$ codes = codes[(codes['code'] != codes['code'].shift())].set_index('code')
np_x = np.random.rand(1000)$ np_target = 0.96*np_x + 0.24
for col in var_cat:$     taxi_sample[col] = taxi_sample[col].astype(np.int64)
cursor.execute("SHOW TABLES")$ cursor.fetchall()
lm = smf.ols(formula='y ~ x', data=df).fit()$ lm.params
df_group_by = df_group_by.reset_index()$
df_pop['pop_t-1'] = df_pop['population'].shift(1) $ df_pop['pop_change'] = df_pop['population']- df_pop['pop_t-1']$
sample=list(db.tweetcollection.find({'_id':994759019909726208}))$ sample
from sklearn.neighbors import KNeighborsClassifier $ from sklearn.model_selection import train_test_split$ V=pd.merge(cards,game_vail,right_index=True, left_index=True)$
df_weekly = df_mean.merge(df_count[['date', 'id']], suffixes=('_average','_count'), on='date').merge($     df_max[['date', 'week', 'text', 'polarity', 'negative', 'retweets']], on='date', suffixes=('_average','_max'))
store_items.pop('new watches')$ store_items
mRF.predict(test)
ved = pd.read_excel('input/Data.xlsm', sheet_name='51', usecols='A:W', header=12, skipfooter=4)
for url in soup.find_all('loc'):$     print (url.text)
full_act_data.plot(figsize=(20,8));
train_data, test_data, train_labels, test_labels = train_test_split(spmat, y_data, test_size=0.10, random_state=42)  
df_schools = pd.read_csv(file1)$ df_students = pd.read_csv(file2)
sns.pairplot(data, hue='species', size=3)
hit_tracker_df.to_csv("Desktop/Project-2/hit_songs_only.csv", index=False, header=True)
auth_endpoint = 'https://iam.bluemix.net/oidc/token'
df.iloc[1] # select row by integer location 1$
message_filename = data_file_path + "raw_data_messages.json"$ DF = pd.read_json(message_filename)$ print("... read in dataframe")
df.to_excel("../../data/stocks_msft.xlsx", sheet_name='MSFT')
plt.savefig("SentimentScatterPlot.png")$ plt.show()
tweets_RDD = sc.textFile('./PSI_tweets.txt')$ tweets = tweets_RDD.collect() #or tweets = tweets_RDD.take(500) for some testing $
fh_2 = FeatureHasher(input_type='string', non_negative=True)$ %time fit2 = fh_2.fit_transform(train.device_id)
health_data_row.loc[2013:2017]  # 2017 doesn't exist, but Python's slicing rules prevent an exception here$
Measurement = Base.classes.hi_measurements$ Station = Base.classes.stations
all_zeros = 1 - val_y.mean()$ all_zeros
reframed = series_to_supervised(scaled, 1, 1)$ reframed.drop(reframed.columns[[8,9,10,11,12,13]], axis=1, inplace=True)$ print(reframed.head())
print 'All IDs are unique' if len(response_df) == len(response_df.id.unique()) else 'IDs not all unique'
ab_df2.query('group == "control"')['converted'].mean()
sns.distplot(data['Age'])
jcomplete_profile = json.loads(profile)[0]$ jcustomer_info = jcomplete_profile['customer']$ dfbasic = json_normalize(jcustomer_info)
predictions = dtModel.transform(testData)
new_stops = new_stops.drop(new_stops.index[[4473,4474]])
df = df[[target_column]].copy()$ base_col = 't'$ df.rename(columns={target_column: base_col}, inplace=True)
all_sets.loc[:, ["name", "releaseDate", "setSize"]].sort_values(["releaseDate"]).tail()
ride_level_data = ridedata_df.groupby(ridedata_df['city'])$ city_avg_fare = ride_level_data.mean().reset_index()$ city_nbr_rides = ride_level_data.count().reset_index()$
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY\$ +'&start_date=2018-05-18&end_date=2018-05-18'$ r = requests.get(url)
df.xs(key=('a', 'ii', 'z'))
dev3['type_first_engagement'] = dev3.apply(get_first_engagement, axis=1)$ dev3['type_last_engagement'] = dev3.apply(get_last_engagement, axis=1)$ dev3['days_involved'] = dev3.apply(get_time_engaged, axis=1)
df.dropna(subset=['Specialty'], how='all', inplace=True)
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s')$ print(git_log.timestamp.describe())
data.info()
aussie_search = api.search(q='%23Aussie')$ len(aussie_search)
best_model = h2o.get_model(gbm_grid_cart.model_ids[0])$ best_model
before_sherpa = df.loc[df["index"] <= 1685.0]$ after_sherpa = df.loc[df["index"] > 1685.0]$
crime_data = pd.read_csv("chicago_crimes_by_ward.csv")
honeypot_df.drop(['time_stamp1','time_stamp2','time_stamp3'], axis = 1, inplace = True)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['NewWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']$ df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['LastBitWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(-1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']$
display(data.head(10))
metadata['bad_band_window1'] = refl.attrs['Band_Window_1_Nanometers']$ metadata['bad_band_window1']
c.index = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j"]$ c
ORDER_TO_PAIR_SHOPIFY = pd.merge(left=BPAIRED_SHOPIFY,right=ORDERS_SHOPIFY[['order_number','created_at']].astype(dtype),left_on='shopify_order_id',right_on='order_number')
tweet_archive_clean.shape$
df = pd.read_csv("../../data/msft2.csv",skiprows=[0,2,3])$ df
soup = bs(response.text, 'html.parser')
grid_pr_fires.plot.scatter(x='glon',y='glat', c='pr_fire', $                            colormap = 'RdYlGn_r')
soup.find('div', class_="poster-section left").find('img')['src']
prcp_df['date'] = [dt.datetime.strptime(x, "%Y-%m-%d") for x in prcp_df['date']]
from sodapy import Socrata$ client = Socrata("data.cityofnewyork.us", os.getenv("apptoken"))
y.start_time
print(airquality_melt.head())
test = old_test.append(new_test).reset_index()$
ndvi_grid = np.array(np.meshgrid(lon_us, lat_us)).reshape(2, -1).T$ np.shape(ndvi_grid)
for item in rows:$     item.native_country='United-States'$ session.commit()
exclude_year = [1985, 1986, 1987] $ lv_workspace.get_subset_object('A').set_data_filter(step=1, filter_type='exclude_list', filter_name='YEAR', data=exclude_year) 
stocks.loc[('Apple', '2017-12-29')]
import pandas as pd$ pd.core.common.is_list_like = pd.api.types.is_list_like$ import pandas_datareader.data as web
S_distributedTopmodel.decision_obj.simulStart.value, S_distributedTopmodel.decision_obj.simulFinsh.value
tfav.plot(figsize=(16, 4), label="Likes", legend=True)$ tret.plot(figsize=(16, 4), label="Retweets", legend=True)
games_df.head()
RunSQL(sql_query)$ actor = pd.read_sql_query(sql_query, engine)$ actor.head()
X = reddit['title'].values$ y = reddit['engagement']$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
out=final.sort_values(by=['priority','RA0'])$ out=out[['name','RA0','DEC0','redshift','min_air','best_time','priority']]$ out.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/all_objs_winter2017.csv',index=False)$
df_wm.to_csv("walmart_senti_score.csv", encoding='utf-8', index=False)$
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ commits_per_year.columns = ['commits']$ commits_per_year.head()
S_lumpedTopmodel.decision_obj.groundwatr.options, S_lumpedTopmodel.decision_obj.groundwatr.value
df_2011['bank_name'] = df_2011.bank_name.str.split(",").str[0]$
image = soup.find('a',class_="button fancybox")['data-fancybox-href']
pd.to_datetime(['2009/07/31', 'asd'])
col['date_time'] = col.index.map(str) + " " + col["TIME"]$ col['date_time'] = pd.to_datetime(col['date_time'])$ col['date_time']=pd.to_datetime(col.date_time.dt.date) + pd.to_timedelta(col.date_time.dt.hour, unit='H')
predictions_count = model_count_NB.predict(X_test_count)$ predictions_tfidf = model_tfidf_NB.predict(X_test_tfidf)
df_events.iloc[:,7].value_counts()
test_classifier('c0', WATSON_CLASSIFIER_ID_1)$ plt.plot(classifier_stats['c0'], 'ro')$ plt.show()
pook_bytes = io.BytesIO(pook_dl.content)$ print(readPDF(pook_bytes)[:1000])
z_stat, p_val = stats.ranksums(virginica, versicolor)$ print('MWW RankSum p-value %0.15f' % p_val)
df['water_year2'] = df[['year','month']].apply($     lambda row: row['year'] if row['month'] < 10 else row['year'] + 1, $     axis = "columns")
soup = bs(html.text, 'html.parser')
print("Min " + str(tm['created_at'].min()) + " Max " + str(tm['created_at'].max()))
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_hat), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
matt2 = dta.t[(dta.b==40) & (dta.c==2)]
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf],$       labels=["bad", "ok", "yeah!"]).sample(10)
building_pa_prc=pd.read_csv("buildding_00.csv")
x = api.GetUserTimeline(screen_name="berniesanders", count=20, include_rts=False)$ x = [_.AsDict() for _ in x]
rtitle = [x.text for x in soup.find_all('a', {'data-event-action':'title'})]$ rtitle.pop(0)
data[data.index.duplicated()]
!wget https://pjreddie.com/media/files/yolov3.weights$
lm = smf.ols(formula='sales ~ TV + radio + newspaper + TV*radio', data=data).fit()$ lm.params
print(scores.mean())
precision = float(precision_score(y, gbc.predict(X)))$ recall = float(recall_score(y, gbc.predict(X)))$ print("The precision is {:.1f}% and the recall is {:.1f}%.".format(precision * 100, recall * 100))
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status', 'OriginationPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'green', 'red', 'yellow'], grid=False)$
%matplotlib inline$ commits_per_year.plot(figsize=(12,7), kind='bar', legend=None, title='Commits per year')
last_entry_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ last_entry_date
total.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_3/target_winter2017_night2.csv',index=False)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(therapist_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Therapist Hours First Difference Autocorrelation')
p_new = df2['converted'].mean()$ print ("convert rate for p_new under the null :{} ".format(round(p_new, 4)))
import builtins$ builtins.uclresearch_topic = 'GIVENCHY'$ from configuration import config
df_concat.rename(columns={"likes.summary.total_count" : "likes_total",$                           "comments.summary.total_count" : "comments_total" }, inplace = True)
btc['2017-09-12':'2017-09-22'].plot(y='price')$ plt.show()
numbers_df = pd.DataFrame(numbers, index = ['number_1', 'number_2','number_3'])$ numbers_df
ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')$ ebola_melt.head()
model.wv.doesnt_match("aku ngentot anda sex".split())
dr_existing_8_to_16wk_arimax = dr_existing_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ dr_existing_8_to_16wk_arimax.index = dr_existing_8_to_16wk_arimax.index.date
country_with_least_expectancy = le_data.idxmin(axis=0)$ country_with_least_expectancy
import pandas as pd$ tweets = pd.read_csv ("./twitter.csv", header=None)$ tweets.head()
mean_encoding_test(val, train,'Block',"any_spot" ) $ mean_encoding_test(val, train,'DOW',"Real.Spots" ) $ mean_encoding_test(val, train,'hour',"Real.Spots" ) 
sentiments_df = pd.DataFrame(sentiment_array)$ sentiments_df = sentiments_df[["TweetsAgo","Target","User","Date","Positive","Neutral","Negative","Compound","Text"]]$ sentiments_df.head()
ad_source.to_csv('../data/ad_source.csv')
import pprint$ pprint.pprint(vars(example_tweets[0]))
CON=pd.read_csv('C:/Users/mjc341/Desktop/UMAN 1507 Monthly INQ summary Report/Interactions.Opportunity.Term.csv',skipfooter=5,encoding='latin-1',engine ='python')$
datetime.now()
name_df.sum(axis=0).sort_values(ascending=False).head(20)
res = sts.query(qry)
FTE = pd.DataFrame(requirements, index = ['FTE'])$ FTE
print(train.isnull().sum())$ train_att = train[train['is_attributed']==1]$ print(train_att.isnull().sum())
dfcsv = df.loc[df['fileType'] == 'csv']$ dfcsv['fileCount'].describe()
from textblob import TextBlob$ from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
df.sort_values(by="grade")
pca = PCA(n_components=6858)$ pca.fit(crosstab)
news_sentiments = pd.DataFrame(sentiments_df, columns= ["News Source", "Date/Time", "Compound","Positive",$                                                         "Neutral", "Negative", "Tweet", "Tweets Ago"])$ news_sentiments.head()
Bot_tweets = data.drop(['text','text_lower'],axis=1)$ Bot_tweets.head()
plt.pie(total_ride, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()
Precipitation_DF.head(10)
count_hash = Counter()$ count_hash.update(clean_hashlist3)$ print(count_hash.most_common(30))$
df=pd.read_csv("../UserData/1000ShareAllColumns.csv")$ df.dtypes
status_data = status_data.dropna()
df_u= df_vu.groupby(["landing_page","group"]).count()$ df_u$
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv"$ df = pd.read_table(path, sep =',', na_values=[' ?'])$ df.head(5)
from pandas.tseries.offsets import *$ d + BusinessDay()
print(trump.axes)$ print(trump.shape)
msft = pd.read_csv("../../data/msft.csv", dtype={'Volume': np.float64})$ msft.dtypes
(session.query(Measurement.station, Station.name, func.count(Measurement.station))$  .filter(Measurement.station==Station.station)$  .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).limit(1).all())
c_df = c_df.dropna(axis=1,how='all')$ c_df.size
round(np.sqrt(model_x.scale), 3)
df = df.drop(['mintempm', 'maxtempm'], axis=1)$ X = df[[col for col in df.columns if col != 'meantempm']]$ y = df['meantempm']
df[12].plot()
expenses_df.drop(expenses_df.index[-1], inplace = True)$ expenses_df
print('Total number of different vote types recorded: {}'.format(len(v['vote'].value_counts(dropna=False))))$ v['vote'].value_counts(dropna=False).nlargest(10)
prec_long_df.describe()
tNormal = len(df[ (df['bmi'] > 18.5) & (df['bmi'] < 25.0) ])$ tNormal
rural_ride_total = rural_type_df.groupby(["city"]).count()["ride_id"]$ rural_ride_total.head()
newfile = newfile[~newfile['Reason for rejection'].isin({'X1', 'X8'})]
for obj in bucket_obj.objects.all():$     print('Object key: {}'.format(obj.key))$     print('Object size (kb): {}'.format(obj.size/1024))
r6s['score'].corr(r6s['num_comments'])
saveTweetDataToCSV(input_hash_tag.value)
siteInfo = siteInfo.query('Range >= 10 & EndYear >= 2017')$ siteInfo
tdf[tdf['smoker'] == 'Yes'].describe()
accuracy = metrics.accuracy_score(predictions,y_test)$ print ("Accuracy : %s" % "{0:.3%}".format(accuracy))$
sns.countplot(x='badge_#', data=df)
regr2.score(X2, y)  # when we fit all of the data points
final_df.corr()["ground_truth_crude"][names]
print('Number of rows with invalid values = {}'.format(len(joined[joined.isnull().any(axis=1)])))$ joined[joined.isnull().any(axis=1)]
df_inventory_santaclara['Year']=(df_inventory_santaclara['Date'].str.split('-').str[0])
logging.info('Dropping unnecessary variables')$ data.drop(['Date', 'Open', 'High', 'Low', 'Market Cap', 'Dependent Variable'], axis=1, inplace=True)
hyperparam =[0.01, 0.1, 1, 10, 100]$
type(df_vow['Date'].loc[0])
cityID = '7a863bb88e5bb33c'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anchorage.append(tweet) 
large_image_url = browser.find_by_xpath('//*[@id="page"]/section[1]/div/article/figure/a/img')["src"]$ print(large_image_url)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours Partial Autocorrelation')
header = cylData.first()$ cylHPData= cylData.filter(lambda line: line != header)$ print (cylHPData.collect())
lr.fit(features_class_norm, overdue_transf)$     $ print_feature_importance(vectorizer.feature_names_, lr.coef_)
smooth = condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="SMOOTHPURSUIT")
t = input("Input a template: ")$ t = json.loads(t)$ DBfindByTemplate(t)$
file_names = []$ file_names = glob.glob('../craigslist-data/final-data-multiple-cities/*.csv')
df = engine.get_data(**opts)
topics = lda.get_term_topics('network')$ for t in topics:$     print(t)
df[['Footnote']].isnull().sum()$
mft = most_followed_tweeters.followers$ normalized_followers = np.round((np.abs(np.array(mft)-np.array(mft).mean())/mft.std())*10, 0)
import geopy.distance
year2 = driver.find_elements_by_class_name('yr-button')[1]$ year2.click()
cleansed_search_df['SearchTerm'] = np.where(cleansed_search_df['SearchCategory'] == "Plant", cleansed_search_df['SearchTerm'].str.rpartition('-')[2].str.strip() , cleansed_search_df['SearchTerm'])$ cleansed_search_df.loc[cleansed_search_df['SearchCategory'] == "Plant"]
driver = selenium.webdriver.Safari() # This command opens a window in Safari$ driver.get("https://xkcd.com/")
free_data.index.name = 'id'$ free_data.head(2)
Total_Number_of_Rides_min = rides_analysis["Total Number of Rides"].min()$ Total_Number_of_Rides_min
clean_measure = measure.fillna(0)
prophet_df = pd.DataFrame()$ prophet_df['y'] = df[target_column]$ prophet_df['ds'] = df['date']
typesub2017 = typesub2017.rename(index=str, columns={"Solar  - Actual Aggregated [MW]": "Solar", "Wind Offshore  - Actual Aggregated [MW]": "Wind Offshore", "Wind Onshore  - Actual Aggregated [MW]" : "Wind Onshore" })$ typesub2017.head()
BTC['Smoother'] = BTC.apply(lambda row: smoother_function(row["Transfer_method"], row["Type"], row["BTC_amount"]), axis=1)
prcp_df = pd.DataFrame(prcp_data, columns=['Date', 'Precipitation'])$ prcp_df.set_index('Date', inplace=True) $ prcp_df.head()
inspector.get_table_names()
log_user1 = df_log[df_log['user_id'].isin([df_test_user['user_id']])]$ print(log_user1)
df_grp1=df.groupby('Dates').mean()$ display(df_grp1.head())$ df_grp1.plot(y="prcps",title="Precipitation 2016-08-22 to 2017-08-23")$
df_nona = df.fillna('NA')
Largest_change_between_any_two_days = mydata['Close'].diff().max() $ Largest_change_between_any_two_days
most_yards[['Date','PlayType','Yards.Gained','qtr','desc','Rusher','Receiver', 'Jimmy']][:10]
df.drop(bad_indices, inplace=True)
data_store_id_relation.count()
engine = create_engine("sqlite:///hawaii.sqlite")
pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')
!apt install libnvrtc8.0$ !pip install mxnet-cu80$ import mxnet as mx
data_= pd.read_sql(q,connection)$ data_.head()
r =requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2017-01-01&end_date=2017-12-31')$ print(r.json())
db = client.test_database$ collection = db.test_collection$ collection
rural_summary_table = pd.DataFrame({"Average Fare": rural_avg_fare, $                                    "Total Rides": rural_ride_total})$ rural_summary_table.head()
base_df_body.head()
S_distributedTopmodel.forcing_list.filename
(data_2017_12_14[data_2017_12_14['text'].str.contains("felicidades", case = False)])["text"].head()
df_new[['action_new_page','action_old_page']] = pd.get_dummies(df_new['landing_page'])$ df_new = df_new.drop('action_old_page', axis=1)$ df_new.head()
dates = pd.date_range(date.today(), periods=2)$ dates
ibm_hr_final2 = ibm_hr_final.join(ibm_hr.select("Attrition"))$ ibm_hr_final2.printSchema()
X_train[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
sales_df[sales_df['Product_Id'] == 5].groupby(['Country']).sum()['Quantity'].sort_values(ascending=False)$
df['yob'].idxmax(axis=1)
psy_df4 = PDSQ.merge(psy_df3, on='subjectkey', how='right') # I want to keep all Ss from psy_df$ psy_df4.shape
df['datetime'] = df.index.values
spp = pd.read_excel('input/Data.xlsm', sheet_name='43', header=11, skipfooter=8793)
block_geoids_2010 = json.load(open('block_geoids_2010.json'))$ print 'There are', len(block_geoids_2010), 'blocks'$ assert(len(block_geoids_2010) + 1 == len(block_populations))
gmaps = googlemaps.Client(key='xxxxxxxxx')
writer = pandas.ExcelWriter(os.path.join(output_dir,$                                          '{}_example_stats.xlsx'.format(cur_experiment)))
print activity_df.iloc[-3]
plt.xlim(100, 0)$ plt.ylim(-1, 1)
precip_df.rename(columns={'prcp': 'precipitation'}, inplace=True)$ precip_df.set_index('date', inplace=True)$ precip_df.head()
DataSet.head(100)
building_pa_prc_shrink.to_csv("buildding_01.csv",index=False)$
scores = cross_val_score(model, X_top, y, scoring='roc_auc', cv=5)$ print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))
print("Mean squared error: %.2f"$       % mean_squared_error(y_test, y_pred))
import os$ place = os.chdir("..")$
auth = HydroShareAuthBasic(username='****', password='****')$ hs = HydroShare(auth=auth)$ resource_id = hs.createResource(rtype, title, resource_file=fpath, keywords=keywords, abstract=abstract, metadata=metadata, extra_metadata=extra_metadata)
conn = 'mongodb://localhost:27017'$ client = pymongo.MongoClient(conn)
df['HIGH_LOW']=(df['num_comments']>df['num_comments'].median()).astype(int)$ print(df.shape)$ df.head()
def create_soup(x):$     return ''.join(x['categoryname']) + ', ' + ''.join(x['eventname']) + ', ' + ''.join(x['location'])
readerASN = geoip2.database.Reader(GEOLITE_ASN_PATH)$ readerCITY = geoip2.database.Reader(GEOLITE_CITY_PATH)
del p1.age$ print(p1.age)   # It will give you error
studies_b=studies_a.merge(sponsors,on='nct_id', how='left')$ studies_b.head()
hawaii_measurement_df = hawaii_measurement_df.replace(np.nan, 0)
data['intra_day_diff'] = data.High - data.Low$ data.head()
stn_rainfall = session.query(Measurement.station, func.sum(Measurement.prcp)).filter(Measurement.date >= data_oneyear).group_by(Measurement.station).order_by(func.sum(Measurement.prcp).desc()).all()$ stn_rainfall$
s3 = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5})$ s3
mlab_uri = os.environ['MLAB_URI']$ mlab_collection = os.environ['MLAB_COLLECTION']
sess = tf.Session()$ print(sess.run(result))
df['Non-Crime Criteria Met'] = df_criteria.apply(lambda x: y_count(x),axis=1)
df.count()
Z = np.arange(10)$ np.add.reduce(Z)
from spacy.lang.en.stop_words import STOP_WORDS$ print('Example stop words: {}'.format(list(STOP_WORDS)[0:10]))
tips.sort_values(["sex", "day"]).set_index(["sex", "day"]).head(12)$
plt.style.available
city_avg_fare = pd.merge(citydata_df, city_avg_fare, on='city', how='left')$
rate_change['rating'].sort_values(ascending=False)[0:2]
df3 = df3.add_suffix(' Closed')$ df7 = pd.merge(df4,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE Closed')$
tmax_day_2018.coords
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period()$ print("Trend for month: ", get_trend(get_timeseries(closed_pr)))
all_sets.shape
s = df_sms.groupby(['group','ShopperID'])['ID'].count().reset_index()$ s.groupby(['ID','group']).size();
df_concat_2 = pd.concat([df_bild, df_spon]) #concats a list of dfs to one df.$
model = app.models.get("Textures & Patterns")$ model.predict_by_url(url='https://assets.vogue.com/photos/595a6e6ca236912379c91ccf/master/pass/_ARC0003.jpg')
np.shape(rhum_fine)
nsw_bb = Polygon([[-35.52052802079999,140.999279200001],[-28.1570199879999,140.999279200001],$                   [-28.1570199879999,159.105444163417],[-37.5052802079999,159.105444163417]])
!hdfs dfs -cat 32B_results-output/part-0000* > 32B_results-output.txt$ !head 32B_results-output.txt
pd.Series([2, 4, 6], index=['a','b','c'])
mod_model.scen2xls(version=None)
for t in [36, 58, 65, 72, 88]:$     str = f'P(Fail) of O-Ring at {t} deg. = {lgt.predict([t, 1])[0]:4.2f}%'$     print(str)
max_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ print("Max Date is ", max_date)
news_organizations_df['tweets'] = news_organizations_df['tweets'].map(df_to_list)
html = browser.html$ soup = bs(html, 'html.parser')$
cityID = 'c3f37afa9efcf94b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Austin.append(tweet) 
pd.date_range(end='6/1/2016', periods=20)
df.loc[dates[0]]
teams=pd.unique(results[['home_team','away_team']].values.ravel())$ teams
x_axis = np.arange(0, Total_Number_of_Rides_max+6, 5)$ x_axis
df=pd.read_csv("Monika__link_syn11_sel.csv")#,encoding="cp1252")#"utf-16")#$ df.head()$
df = pd.read_excel('weather_nan.xlsx')$ df
nodes = nodereader.values.tolist() 
sum([1 for row in U_B_df.cameras if len(row) > 2])
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ r = requests.get(url)
times_zone = pd.DataFrame(df_table['createdOnTimeZone'][771:797])$ times_created = pd.DataFrame(df_table['createdOn'][771:797])$ appV = pd.DataFrame(df_table['appVersion'][771:797])
df_movies = pd.merge(df, xml_in_sample, left_on=['movieId'], right_on=['authorId'],  how='left')[['movieId', 'authorName']]$ df_movies.drop_duplicates(subset=None, keep='first', inplace=True)
fNames = dfX.columns
predictions = model_rf.transform(test_data)$ evaluatorRF = MulticlassClassificationEvaluator(labelCol="trips", predictionCol="prediction", metricName="accuracy")$ accuracy = evaluatorRF.evaluate(predictions)$
df3.describe()
results_distributedTopmodel, output_DT = S_distributedTopmodel.execute(run_suffix="distributedTopmodel_hs", run_option = 'local')
!pip install pandas-datareader$ !pip install --upgrade html5lib==1.0b8$
tweets_df.head()
df.tail(8)
for date_col in ['BABY_CREATED','SCN_CREATED','PURCHASED','PAIRED']:$     ORDER_BPAIR_SCN_SHOPIFY[date_col] = pd.to_datetime(ORDER_BPAIR_SCN_SHOPIFY[date_col]).dt.strftime("%m-%d-%Y")$
forcast_out=int(math.ceil(.01*len(df1)))$ forcast_out$
results = nfl.interest_over_time()$
nold = df2.query('landing_page == "old_page"').count()[0]$ print ("The population of Oldpage is : {}".format(nold))
dfm = dfn.drop(['usd_pledged','goal','state','slug','currency','deadline','state_changed_at','created_at','backers_count','spotlight','period'], axis=1).copy()
url_template.format(start,)
!hdfs dfs -cat {HDFS_DIR}/p32a-output/part-0000* > p32a_results.txt
a = np.array([1, 2, 3])$ a
list(cur.execute('SELECT * FROM experiments'))
df_ml_features = df_reg.drop('isClosed',axis = 1)$ df_ml_target = df_reg['isClosed']
connection = sqlite3.connect(':memory:')$ cursor = connection.cursor()$
festivals.head(3)
tm_2030 = pd.read_csv('input/data/trans_2030_m.csv', encoding='utf8', index_col=0)
ftr_imp=zip(features,xgb_model.feature_importances_)
tmp = tmp.sort_values(by = 'meantempm', axis = 0, ascending = False)$
!curl -L -O  https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv$ !head -n 1 Consumer_Complaints.csv > CC_header.csv$ !tail -n +2 Consumer_Complaints.csv > CC_records.csv
def trip_start_date(x):$     return re.search(r'(\d{4})-(\d{2})-(\d{2})', x).group(0)
options_frame['ModelError'].hist()$
(y_hat.shape, y_train.shape)
df.loc[df.userLocation == 'Youngstown, Ohio', :]
df = pd.read_csv('/home/bmcfee/data/vggish-likelihoods-a226b3-maxagg10.csv.gz', nrows=1000000, index_col=0)
interests_groupby_user = df_interests['interest_tag'].groupby(df_interests['user_handle'])$ lst_user_interests = [[name, group.tolist()] for name, group in interests_groupby_user]$ lst_user_interests[1]
dfClientes.iloc[10: 20, [0, 2, 3]]
sample_text = "Hey there! This is a sample review, which happens to contain punctuations."$ print(text_process(sample_text))
import json$ list_of_issues_dict_data = [json.loads(line) for line in open('SPM587FA17issues.json')]
Base = automap_base()$ Base.prepare(engine, reflect=True)$ session = Session(engine)$
df7.to_csv('Completed_Permits_by_Month.csv')
df = pd.read_csv("../Files/371SpectraDeMeoColor.csv", index_col=0)$ df = shrink_classes(df)
df.head()
post_gc_launch_sales =  shopify_data_merge_msrp[shopify_data_merge_msrp['Created at'] >= '2018-06-08']$ post_gc_launch_sales[(post_gc_launch_sales['Email'] == 'vincent.brouwer1977@gmail.com')]$
print(r.json()['dataset_data']['column_names'])
word_count = np.array([len(d.split(' ')) for d in docs])$ print('Done.')
new_fp = 'data/brain2body_headers.txt'$ b2b_df.to_csv(new_fp, index=False) # Setting index to False will drop the index integers, which is ok in this case
firevio=load_data('https://data.sfgov.org/resource/x75j-u3wx.json')$ firevio.head(5)$
pop_df_3 = df2[(df2.index >= '2016-01-01') & (df2.index <= '2016-12-31')]$ pop_df_3['GrossOut'].plot(kind='line', color='g')$ pop_df_3['GrossIn'].plot(kind='line', color='r')
!hdfs dfs -cat {HDFS_DIR}/hw3.1-output/* > complaintCounts.tsv$ !cat complaintCounts.tsv
INT = INT.drop(columns= ['Contact: First Name', 'Contact: Middle Name',$        'Contact: Last Name'])
data.dtypes$
a.
topLikes = data[(data.Likes >= data.Likes.max()-1200)]$ topLikes$
vwg = pd.read_excel('input/Data.xlsm', sheet_name='53', usecols='A:Y', header=13, skipfooter=10)
df.index = pd.to_datetime(df.index)
git_log['timestamp']=pd.to_datetime(git_log['timestamp'], unit='s')$ print(git_log['timestamp'].describe())
lookforward_window = 1$ for iter_x in np.arange(lookforward_window)+1:$     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
bbc = news_sentiment('@BBCNews')$ bbc['Date'] = pd.to_datetime(bbc['Date'])$ bbc.head()$
manager.image_df[manager.image_df['filename'] == 'image_picea_sitchensis_in_winter_11.png']$
tz_pytz = pytz.timezone('Europe/London')
grouped_dpt_city = department_df.groupby(["Department", "City"])
events_df['event_time'] = events_df['event_time'].apply(lambda s: datetime.datetime.strptime(str(s),'%Y-%m-%d %H:%M:%S'))
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
from IPython.display import Image$ Image('/Users/jdchipox/Desktop/SV40table.png')
BPAIRED_SHOPIFY['shopify_order_id'] = [a.split('-')[0] for a in BPAIRED_SHOPIFY['channel_order_id']]
blink= condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="BLINK")
df_birth['Continent'].value_counts(dropna=False)
random_crashes_upsample_df = pd.DataFrame(random.choices(random_crashes_df.values, k=sample_size), columns=list(random_crashes_df.columns))$ random_crashes_upsample_df
df['created_at'] = pd.to_datetime(df['created_at'])
name = "binding_2018-04-03_no1"$ data.to_csv("models/comparison/"+name+".csv")
materials_file = openmc.Materials([inf_medium])$ materials_file.export_to_xml()
data[['TMAX']].head()
a = tweets.apply(lambda row: countOcc(row['tokens']), axis=1)$ sorted_x = sorted(occ.items(), key=operator.itemgetter(1), reverse=True)$ sorted_x
random_integers = rng.randint(1, 10, size = 16).reshape(4, 4)
active_users.shape # there are only 7479 active users out of 22884 signed users$
model = ols("happy ~ age + income", training).fit()$ model.summary()
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
train_set.columns = ['tweet_id', 'tweetText', 'polarity_value', 'polarity_type', 'topic','set']
dtc_features = sorted(list(zip(test_features, dtc.feature_importances_)), key=lambda x: x[1], reverse=True)$ dtc_features
plantlist[plantlist['commissioned'].isnull()]
adj_close_start = adj_close[adj_close['Date']==end_of_last_year]$ adj_close_start.head()
max_retweet = DataSet['tweetRetweetCt'].max()$ max_retweet
plt.scatter(X2[:, 0], X2[:,1])
mlp = df[['Median Listing Price']].values$ print('shape:\n', mlp.shape, '\n',$      '1st 5 rows:\n', mlp[0:5, :])
features_df.plot.bar(x='features',y='importance')
s = '2016-01-01 00:00:00'$ s = '2018-07-01 00:00:00'$ int(time.mktime(datetime.datetime.strptime(s, "%Y-%m-%d %H:%M:%S").timetuple()))
%%time$ df = table.to_pandas()
x_train = scaler.transform(x_train)$ x_test = scaler.transform(x_test)
y = K.dot(x, W) + b$ loss = K.categorical_crossentropy(y, target)
log_mod = pickle.load(open('../data/model_data/log_pred_mod.sav', 'rb'))
df3 = df2.copy()
tst_lat_lon_df = pd.read_csv("testset_unique_lat_and_lon_vals.csv", index_col=0)
S_1dRichards.decision_obj.bcLowrSoiH.options, S_1dRichards.decision_obj.bcLowrSoiH.value
reorder_customers = np.fromiter(result.keys(), dtype=int)$ reorder_customers.size
df = pd.merge(t1, t2, on='tweet_id', how='inner')$ df = pd.merge(df, t3, on='tweet_id', how='inner')
symbol='^NBI'$ benchmark1 = web.DataReader(symbol, 'yahoo' , start_date ,end_date)
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
json_data= response.json()['dataset_data']$ print(json_data)$
plotdf['forecast'] = plotdf['forecast'].interpolate()$ plotdf['forecastPlus'] = plotdf['forecastPlus'].interpolate()$ plotdf['forecastMinus'] = plotdf['forecastMinus'].interpolate()
df = pickle.load(open( "lyincomey.p", "rb" ))
tweet = tweets[1]$ pp.pprint([att for att in dir(tweet) if '__' not in att])
df.tail()
df = df.drop_duplicates(subset='id')
df.drop(df.query("group =='treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group =='control' and landing_page == 'new_page'").index, inplace=True)$
search['trip_duration'] = (search['trip_end_date'] - search['trip_start_date']).dt.days
joined=join_df(joined,googletrend,["State","Year","Week"])$ joined_test=join_df(joined_test,googletrend,["State","Year","Week"])$ sum(joined['trend'].isnull()),sum(joined_test['trend'].isnull())
thistweet=pd.read_csv("../output/tweet_with_location.csv")$
df.tail(50)
master_df['Distance in km']=std.fit_transform(master_df[['Distance in km']])$ master_df['age']=std.fit_transform(master_df[['age']])$ master_df['Temp high (F)']=std.fit_transform(master_df[['Temp high (F)']])
if not os.path.isdir('output/electricity_demand'):$     os.makedirs('output/electricity_demand')
feedbacks_stress.describe()
rfc = RandomForestClassifier(n_estimators=1000, max_depth=100, max_features=2, n_jobs=-1)$ scores = cross_val_score(rfc, X, np.ravel(y,order='C'), cv=5)$ print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
df['StatusDate'] = pd.to_datetime(df['StatusDate'], format='%m/%d/%Y %I:%M:%S %p +0000') $ df['ResolutionTime'] = (df['StatusDate']-df.index).astype('timedelta64[h]') / 24
words_only_scrape = [term for term in words_scrape if not term.startswith('#') and not term.startswith('@')]$ print('The number of words only (no hashtags, no mentions): ', len(words_only_scrape))
bots = df[df['author'].str.contains('Bot')]['author'].unique()
solar_wind_df.loc[3080:3085]
for key, value in df.iteritems():$     x = {key : value}$     print(x)
df_trimmed.event.unique()$
model_uid = client.repository.get_model_uid(model_details)$ print(model_uid)
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
temp_data=stock_data[stock_data.SYMBOL == 'ABB'] $ temp_data=temp_data.rename(columns={"OPEN": "Open", "HIGH": "High", "LOW": "Low", "CLOSE": "Close", "TIMESTAMP": "Date"})$
DataSet['userTimezone'].value_counts()
    df.to_csv(path,sep=',',index=False)
X_copy['crfa_f'] = X_copy['crfa_f'].apply(lambda x: int(x))
n = 4$ km = KMeans(n_clusters=n, random_state=123)$ km_res = km.fit(finalDf)
ax = trump_histogram(trump_df)$ plt.show()
cv_results = pd.DataFrame(rs.cv_results_)$ cv_results
cityID = '2a93711775303f90'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Milwaukee.append(tweet) 
from nltk.corpus import stopwords$ stop_words = stopwords.words(['english','danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish'])$ stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
df2.sort_values('total_likes',inplace=True,ascending=False)$ top_likes = df2.head(10)['id']
data[(data.phylum.str.endswith('bacteria')) & (data.value>1000)]
Labels_majority = (((CurrentA1.iloc[:,3:14] + CurrentA2.iloc[:,3:14] + CurrentA3.iloc[:,3:14])/3)>0.3).astype("int32")$ Class_frame_majority = pd.concat([CurrentA1.iloc[:,0:2],Labels_majority],axis=1)$ Class_frame_majority.to_csv("union_voting_elisa.csv")
d = json.loads(r.text[len('var tumblr_api_read = '):-2])$ print(d.keys())$ print(len(d['posts']))
soups = [soup(requests.get(url).text, 'html.parser') for url in article_urls]
df_atends['Prefeitura Regional'].value_counts()
station_count = session.query(func.count(Station.station)).all()$ station_count[0]
containers[0].find("li", {"class":"paid-amount"}).span.contents[-1].split()[2]
b[b.T.sum()==c].index.min()
pattern = re.compile('AA')$ print(pattern.search('AAbcAA'))$ print(pattern.search('bcAA'))
session.query(func.count(Measurement.date)).all()
crimes[['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']].head()
trip_prec_df = pd.DataFrame(sq.history_rainfall_trip(), columns=["Station", "Total prec"])$ trip_prec_df
norm.ppf(1-0.05/2)
new_stops.to_sql('main_stops', engine, if_exists='append', index=False)
nav = soup.nav$ for url in nav.find_all('a'):$     print (url.get('href'))
review_df.head()
b.loc[1:7]
print('Median trading volume during 2017 - {}'.format(list_median(traded_volumes)))
from pandas_datareader import wb$ all_indicators = wb.get_indicators()$ all_indicators.ix[:,0:1]
head = pd.Timestamp('20150101')$ tail = pd.Timestamp('20160101')$ df = hp.get_data(sensortype='water', head=head, tail=tail, diff=True, resample='min', unit='l/min')$
utility_patents_df.number_of_claims.describe()
Measurements = Base.classes.metDF$ Stations = Base.classes.logDF
validation_features = bind_features(validation, train_test="train").cache()$ validation_features.count()
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:10]
!curl -L -O  https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt
print(model.aic,model.bic,model.hqic)
print(parquet_file)$ df = sqlContext.read.load(parquet_file)$ df.show()
results2 = model_selection.cross_val_score(gnb, X_test, Y_test, cv=loocv)$ results2.mean()
display(data.head(10))
suburban_summary_table = pd.DataFrame({"Average Fare": suburban_avg_fare,$                                     "Total Rides": suburban_ride_total})$ suburban_summary_table.head()
cbd = CustomBusinessDay(holidays=cal.holidays())$ datetime(2014,8,29) + cbd
network_simulation[network_simulation.generations.isin([5])]$
for c in ccc:$     for i in ved[ved.columns[ved.columns.str.contains(c)==True]].columns:$         ved[i] /= ved[i].max()
search_rating = np.vectorize(search_rating, otypes=[np.float])$ data['rating'] = search_rating(data['text'])$ data = data[pd.notnull(data['rating'])]
df.shape[0]
df.groupby([df.index.month, df.index.day]).size().plot()$ plt.show()
gbm = GradientBoostingClassifier(max_depth = 8, n_estimators= 200, max_features = 0.3)$ gbm.fit(X_reshaped, y)$ getAUC(gbm, X_reshaped, y, X_test_reshaped, y_test)
yc_new4.head()
results = soup.find_all('div', class_='slide')$ print(results)
article_urls = ['http://www.nhm.ac.uk/discover/the-cannibals-of-goughs-cave.html','http://www.nhm.ac.uk/discover/how-we-became-human.html','http://www.nhm.ac.uk/discover/the-origin-of-our-species.html']$
dataframe.drop(['Date'],inplace=True,axis=1)$ scaler = MinMaxScaler(feature_range=(0, 1))$ scaled = scaler.fit_transform(dataframe)
tm_2020 = pd.read_csv('input/data/trans_2020_m.csv', encoding='utf8', index_col=0)
data.drop(['ceil_10min'], axis=1,inplace=True)
df2['user_id'].nunique()$ print ("Total Number of Unique row : {}".format(df2['user_id'].nunique()))
for row in selfharmm_topic_names_df.iloc[4]:$     print(row)
filtered_brewery = df.groupby('brewery_name').filter(lambda x: x.brewery_name.value_counts() >= 3)$ brewery_bw = filtered_brewery.groupby('brewery_name').rating_score.mean().sort_values(ascending=False)
cur.execute('SELECT material_id, long_name FROM materials WHERE alpha < 1 LIMIT 2')$ for c in cur: print('{} is {}'.format(*c))  # user the cursor as an iterator
db = client.insight_database$ collection = db.posts
print(pd.to_numeric(countdf['number_votes']).sum())$ print(pd.to_numeric(countdf['number_votes']).sum()-pd.to_numeric(count1df['number_votes']).sum())$ print(pd.to_numeric(countdf['number_votes']).sum()-pd.to_numeric(count6df['number_votes']).sum())
prcp_analysis_df = df.rename(columns={0: "Date", 1: "precipitation"})$ prcp_analysis_df.head()
y.mean()
lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()$ lm.params
negative = '/Users/EddieArenas/desktop/Capstone/negative-words.txt'$ negative = pd.read_table(negative, encoding = "ISO-8859-1")
model.get_params()
url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars"$ base_url = "https://astrogeology.usgs.gov"$ hemisphere_image_urls = []
cityID = '3b77caf94bfc81fe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Los_Angeles.append(tweet) 
df_test_user = df_users.iloc[0, :]$ df_test_user
sentiment.to_csv("sentiment.csv", encoding="utf-8", index=False, header=True)
lr = LinearRegression()$ lr.fit(train_data, train_labels)
user = user[user.is_enabled >= 0]
df.groupby(df.index.tz_localize('GMT').tz_convert('US/Eastern').hour).count().Tweets.plot(kind='barh')
dummy_df['price_change_1day'] = (dummy_df['price'].shift(-1440) - dummy_df['price']).fillna(method = 'ffill')$ dummy_df['price_change_2days'] = (dummy_df['price'].shift(-2880) - dummy_df['price']).fillna(method = 'ffill')$ dummy_df['price_change_3days'] = (dummy_df['price'].shift(-4320) - dummy_df['price']).fillna(method = 'ffill')
df2.to_csv('ab_data_new.csv', index=False)
from bmtk.analyzer import nodes_table$ nodes_table('network/recurrent_network/nodes.h5', 'Cortical')
print(max(sessions.groupby('id').title.nunique()))
d.weekday()
classifier = tf.estimator.Estimator($     model_fn=mobilenet_model_fn, model_dir=paths["Checkpoints"])
nb = MultinomialNB()$ nb.fit(X_train, y_train)$ nb.score(X_test, y_test)
dataset['Created_at'] = pd.to_datetime(dataset.Created_at)$ dataset.head()
print('band width between first 2 bands =',(wavelengths.value[1]-wavelengths.value[0]),'nm')$ print('band width between last 2 bands =',(wavelengths.value[-1]-wavelengths.value[-2]),'nm')
tweet = result[0] #Get the first tweet in the result$ for param in dir(tweet):$ (param, eval('tweet.'+param))
data_ps['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_ps['Tweets'] ])$ display(data_ps.head)
predictions = forest.predict(test_data_features)$ output = pd.DataFrame( data={"id":test["id"], "rating":predictions} )
text = nltk.Text(tokens)$ text.dispersion_plot([token for token, frequency in text_nostopwords])
youtube_df["isPorn"] = pd.Series()$ youtube_df=youtube_df.fillna(0)$ youTubeTitles = youtube_df.loc[:, ["title", "isPorn"]]$
hawaii_measurement_df = pd.read_csv(r"\Users\Josue\Desktop\\hawaii_measurements.csv")
results_df.to_csv("2018-04-09-Dow-NewsStats.csv", index=False)$
my_url = "http://www.ipaidabribe.com/reports/all#gsc.tab=0"
data_df.groupby('type')['ticket_id'].nunique()
md_vals = list(tmpdf_md.loc[:,METADATA_VAL_COLS].unstack().values)$ md_vals = [s.lower() if isinstance(s, str) else s for s in md_vals]$ md_vals
num_benfords = 10$ benfords = [np.log10(1+1/i) for i in range(1, num_benfords + 1)]$ x_ben = [x for x in range(0, num_benfords)]
kick_projects['launched_date'] = pd.to_datetime(kick_projects['launched'], format='%Y-%m-%d %H:%M:%S')$ kick_projects['deadline_date'] = pd.to_datetime(kick_projects['deadline'], format='%Y-%m-%d %H:%M:%S')
vocab = vectorizer.get_feature_names()$ print(vocab)
fed_reg_dataframe = pd.DataFrame.from_records(tuple_lst, columns=['date','str_text'], index = 'date')
weather['precip_total'] = weather['precip_total'].replace('NaN', None, regex=False).fillna(0)$ weather['pressure_avg'] = weather['pressure_avg'].replace('NaN', None, regex=False).fillna(0)$ weather['wind_speed_peak'] = weather['wind_speed_peak'].replace('NaN', None, regex=False).fillna(0)
print login_response.status_code$ login_page = BeautifulSoup(login_response.content, 'lxml')
init = tf.global_variables_initializer()$ sess.run(init)
yc_new2.dropna()$ yc_new2.isnull().sum()
df.describe()
tweet_data = pd.read_json(twitter_json)$ tweet_data.set_index('created_at', drop=True, inplace= True)$ pd.to_datetime(tweet_data.index)
walk['2014-08-01 00:00'].mean()
adj_close_acq_date_modified = adj_close_acq_date[adj_close_acq_date['Date Delta']>=0]$ adj_close_acq_date_modified.head()
result = dta.groupby((dta.results, dta.dba_name)).size()
print(df.columns.values)
theft.iloc[0:5]
df_R['Year']=df_R['Date'].str.slice(0,4)$
all_data_df = pd.read_csv('github_issues.csv')$ all_data_bodies = all_data_df['body'].tolist()
search_booking = search1[search1.booking == 1]$ search2 = search1.append([search_booking]*4,ignore_index=True)
df_null_acct_name = df[df['LinkedAccountName'].isnull()]
orders = pd.read_csv('../data/raw/orders.csv')$ products = pd.read_csv('../data/raw/products.csv')$ order_details_prior = pd.read_csv('../data/raw/order_products__prior.csv')$
test_id_ctable = df.groupby(['test_id', 'pass_test']).size().unstack('test_id').fillna(0)$ _, p, _, _ = chi2_contingency(test_id_ctable)$ p
df = pd.read_excel('accounts-annotations.xlsx', encoding='cp1252')
cwd1 = os.getcwd()$ print cwd1$
rain_df.set_index('date').head()
ds = tf.data.TFRecordDataset(train_path)$ ds = ds.map(_parse_function)$ ds
full_act_data.to_csv(os.path.join(data_dir, 'bbradshaw_fbml_data.csv'))
rng2017 = pd.date_range('2017 Jan 1 00:00', periods = 12, freq = 'MS')$ rng2018 = pd.date_range('2018 Jan 1 00:00', periods = 12, freq = 'MS')$ rng2019 = pd.date_range('2019 Jan 1 00:00', periods = 12, freq = 'MS')
grp = dta.groupby(dta.results.str.contains("Pass"))$ grp.groups.keys()
stat_info_merge = pd.concat([stat_info_city[1], stat_info_st[[0,1]]], axis=1)
%%time$ body_pp = processor(keep_n=8000, padding_maxlen=70)$ train_body_vecs = body_pp.fit_transform(train_body_raw)
Measurements = Base.classes.hawaii_measurement$ Stations = Base.classes.hawaii_station$
S_distributedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_distributedTopmodel.txt')
x.iloc[z]
iso_join = gpd.overlay(iso_gdf, iso_gdf_2, how='union')
df.to_sql('places', conn)
req_dict = dict(req.json())$
S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
df.game_type.value_counts()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&start_date=2017-01-02&end_date=2017-01-02&api_key=nK_oDNyRo17zSF7LsAUb')$ print(r.json())$
Measurements = Base.classes.measurements$ Stations = Base.classes.stations
df[['Open','Close']]  # we need to pass the list of columns that we want access. Not just the individual column names $
s2 = s.reindex(['a', 'c', 'e', 'g']) $ s2['a'] = 0 $ s2
from fastai.fastai.structured import *$ from fastai.fastai.column_data import *
d = x.T.flatten() # is this data preserved?$ print(d)$ d.base is x # numpy wants to be efficient, but the memory isn't contiguous
contractor = pd.read_csv('contractor.csv')$ state_lookup = pd.read_csv('state_lookup.csv')
FREEVIEW.plot_heatmap(raw_freeview_df,raw_fix_count_df)
for iter_x in np.arange(lookforward_window)+1:$     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
pystore.list_stores()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
m.plot(forecast);
total_tokens_sk = len(all_tokens_sk)$ corpus_tweets_streamed_keyword.append(('total tokens', total_tokens_sk)) # update corpus comparison$ print('Total number of words (including mentions, hashtags and links) in the collection: ', total_tokens_sk)
corr = newMergedDF.corr().fillna(0)$
session.query(func.count(Measurements.date)).all()
clf = svm.SVR()$
web.DataReader("A576RC1A027NBEA","fred",datetime.date(1929,1,1),datetime.date(2013,1,1))
store_items.pop('glasses')$ store_items
train_Features, test_Features, train_species, test_species = train_test_split(Features, species, train_size=0.5, random_state=0)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726 = test5result$ pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726, open( "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p", "wb" ) )
bar_prep = sentiments_pd.groupby(['Outlet'],as_index=False).mean()[['Outlet','Compound']]$ bar_prep.head()
df_ct.head(2)
month_counts = df.month.value_counts().sort_index()
X.drop('title', axis=1, inplace=True)$ X = pd.get_dummies(X, drop_first=True)
np.mean([len(h.tweets) for h in heap])
df.sort_values(by="pickup", inplace=True)
!grep -A 20 "INPUT_COLUMNS =" taxifare/trainer/model.py
start = "2005-01-01"$ end = "2018-06-31"$ trading_days = fk.get_monthly_last_trading_days(start=start, end=end)$
!ls -l ./cnn/questions/training/ | grep -v ^l | wc -l
validation.analysis(observation_data, simple_resistance_simulation_1)
trt_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][1]$ print("P(trt_con) = %.4f" %trt_con)
df.set_index(['operator', 'part'], inplace=True)
ad_source.columns = ['AD_'+str(col) for col in ad_source.columns]
print ('Model Score Using the Training Data:\n',model.score(x1, y_train))
with open(saem_women_save, mode='w', encoding='utf-8') as f:$     f.write(SAEMRequest.text)
results = pd.concat([pd.Series(preds).reset_index(drop=True), Y_test.reset_index(drop=True)], axis = 1)$ results.columns = ["predicted", "actual"]$ results["diff"] = (results["predicted"] - results["actual"])/results["actual"]
display(data.tail(10))
segs = ga3.management().segments().list().execute()$ df = pd.DataFrame([x for x in segs['items']])$ df.sort_values("created", ascending=False).head(5)
scores[scores.RottenTomatoes == scores.RottenTomatoes.min()]
plt = r6s.num_comments.hist(bins = 1000)$ plt.set_xlim(0,50)
spark_path = "D:/spark-2.3.1-bin-hadoop2.7"
LARGE_GRID.display_fixations(raw_large_grid_df, option='fixations')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-07-13&end_date=2018-07-13&api_key=%s' % API_KEY)
sess = tf.Session()$ K.set_session(sess)
input_edge_types_file  = input_directory_name + 'input_edge_types.csv'$ Ext_input.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=input_directory_name)
prec_long_df = pd.melt(prec_wide_df, id_vars = ['grid_id', 'glon', 'glat'],$                       var_name = "date", value_name = "prec_kgm2")$ prec_long_df.head()
pred = clf.predict(x_test)$ print(metrics.accuracy_score(y_test, pred))
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
(final_rf_predictions['predict']==test['Cover_Type']).as_data_frame(use_pandas=True).mean()
print('{0:.2f}%'.format((scores[:1.625].sum() / total) * 100))
merged_data['inv_creation_day'] = merged_data['invoices_creation_date'].dt.day$ merged_data['inv_creation_month'] = merged_data['invoices_creation_date'].dt.month$ merged_data['inv_creation_year'] = merged_data['invoices_creation_date'].dt.year
date_precipitation=(session.query(Measurement.date,Measurement.prcp)$ .filter(Measurement.date>=start_date)$ .order_by(Measurement.date.desc()).all())
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=YxSY4z-2vyxvJ15WjtFa&start_date=2017-01-01&end_date=2017-12-31"$ req = requests.get(url)
Base = automap_base()$ Base.prepare(engine, reflect=True)$
dreamophone_url = 'https://dreamophone.com/dream/2186'
mv_lens = pd.merge(movies, ratings)
if 0 == go_no_go:$     all_top_vecs = [lda.get_document_topics(serial_corp[n], minimum_probability=0) \$                     for n in range(len(serial_corp))]
prcp_df =pd.read_sql_query(prcp_data, session.bind)$ prcp_df.set_index('date',inplace=True)$ prcp_df.head()
X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})$ X_new.head()
body = pd.get_dummies(auto_new.Body_Type)$ body.head()
S_lumpedTopmodel.decision_obj.bcLowrSoiH.options, S_lumpedTopmodel.decision_obj.bcLowrSoiH.value
df['salary'] = np.nan$ df$
gs.score(X_test_total, y_test)
buyer=patch.groupby(['buyer_name'])$ repeat=buyer.count()
new_page_converted = np.random.choice([1,0], size = nNew, p=[pMean,oneMinusP])$ new_page_converted.mean()
oz_stops['stopid'] = oz_stops['stopid'].apply(lambda x: str(x))
pipe_lr_2 = make_pipeline(hvec, lr)$ pipe_lr_2.fit(X_train, y_train)$ pipe_lr_2.score(X_test, y_test)
X = dfX[fNames].values$ y = dfY.values
adopted_cats = cats_merge.loc[cats_merge['Outcome Type']=='Adoption']
df_treat = df2.query('group=="treatment"')$ y_treat = df_treat["user_id"].count()$
model=LogisticRegression(penalty='l2', C=1)$ model.fit(X_train_mean, y_train_mean)$ preds = model.predict_proba(X_test_mean)
usecols=[0,6,21,30,31,32,33,34,58,59,60]$ nitrogen = nitrogen.iloc[:,usecols]$ nitrogen.columns
x = store_items.isnull().sum()$ print(x)
information_ratio = pd.Series([40,50], ['Manager_A', 'Manager_B'])$ print(information_ratio)
words_only_sp = [term for term in words_sp if not term.startswith('#') and not term.startswith('@')]$ corpus_tweets_streamed_profile.append(('words', len(words_only_sp))) # update corpus comparison$ print('The number of words only (no hashtags, no mentions): ', len(words_only_sp))
w.index.freq, b.index.freq, r.index.freq
final_rf_predictions = rf_v2.predict(test[:-1])
df_lampposts_loc['LOCAL'].value_counts()
appointments['AppointmentCreated'] = pd.to_datetime(appointments['AppointmentCreated'], errors='coerce')$ appointments['AppointmentDate'] = pd.to_datetime(appointments['AppointmentDate'], errors='coerce')
modelFCDL.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])$
news_df = news_df.set_index('Timestamp')$ news_df.head()
from nltk.corpus import stopwords$ english = stopwords.words('english')
s2 = pd.Series([10,100,1000,10000],subset.index)$ s2
headers=[x.replace('-','_') for x in headers]
index.save('trump.index')$ index = similarities.MatrixSimilarity.load('trump.index')
nyt = news_sentiment('@nytimes')$ nyt['Date'] = pd.to_datetime(nyt['Date'])$ nyt.head()
elms_all = elms_sl.append(elms_pl, ignore_index=True)$ elms_all = elms_all.append(elms_tl, ignore_index=True)$ elms_all['SSN'] = [float(x) for x in elms_all.SSN.values]
df1=data.rename(columns={'SA':'Polarity'})$ df1.head()
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
data = pd.DataFrame(data=[tweet.text for tweet in searched_tweets], columns=['Tweets'])$ display(data.tail(10))
sat_spike.info()
g_goodbad_not_index = sl_data.groupby(['goodbad','AGE_groups'], as_index=False).sum()$ g_goodbad_not_index
df_test_index = pd.merge(df_test_index[event_list['event_start_at'] > df_test_user['created_on']],$                             log_user1[event_list['event_start_at'] > df_test_user['created_on']], on='event_id', how='left')$ df_test_index
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])$ from scipy.stats import norm$ z_score,1-p_value/2$
bigdf = pd.concat(dfs)
stops.to_csv('stops_all_ireland.csv', sep=';')
files = [name for name in listdir() if 'csv' in name]$ fileName = files[0]$ fileName = '866192035974276_AXIO.csv'
movies.describe()
list(festivals.columns.values)
en_translation_counts = en_es.groupby(by='en').size()$ en_translation_counts[en_translation_counts > 1].hist(bins=10)
contractor_clean=contractor_clean[contractor_clean['contractor_id'].isin([139,140,228,236,238]) & $     contractor_clean['contractor_version']!=1 ]$ contractor_clean=contractor_clean.loc[~contractor_clean.contractor_id.isin([373,374,378])]
np.zeros(5)
plt.rcParams['figure.figsize'] = [16,4]$ plt.plot(pd.to_datetime(mydf2.datetime),mydf2.fuelVoltage, 'g.');$ plt.xlim(datetime.datetime(2018,2,8),datetime.datetime(2018,3,25))
df = pd.concat([df, sentiments_df], axis=1)$ df.to_csv('file_output\\news_mood.csv')$ df.head()
targettraffic = dfs_morning.merge(census_zip, left_on=['STATION'], right_on=['station'], how='left')$ targettraffic['targettraffic'] = targettraffic['ENTRIES_MORNING'] * targettraffic['betw150kand200k']/100
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours Autocorrelation')
baby_scn_created['BABY_CREATED'] = pd.to_datetime(baby_scn_created['BABY_CREATED'])$ baby_scn_created['SCN_CREATED'] = pd.to_datetime(baby_scn_created['SCN_CREATED'])$
pd.DataFrame.to_csv(receipt_data, 'receipt_data.csv')
df_pop = df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ df_pop$
r=requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=7up7a3-xNp8abz4VJGsq&start_date=2018-07-22')
conf_matrix = confusion_matrix(training_test_labels, lr.predict(preproc_training_test), labels=[1,2,3,4,5])$ conf_matrix
df.isnull().sum()
df_user[df_user['user.name'] == 'Marco Rubio']
btc_wallet.plot(kind='line',x='Timestamp',y='BTC_balance_GBP', grid=True);
data = r.json()$ print(data)$
print(list(festivals.columns.values))$ festivals = festivals[['Index', 'Date', 'Location', 'latitude', 'longitude', 'Website']]$ festivals.head(3)
cat_sz = [(c, len(full_data[c].unique())) for c in cats]$ emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]$ n_conts = len(full_data.columns) - len(cats)
turnstiles_df['DateTime'] = turnstiles_df['DATE'] + turnstiles_df['TIME']$ turnstiles_df['DateTime'] = pd.to_datetime(turnstiles_df['DateTime'], format='%m/%d/%Y%H:%M:%S')
df_raw[df_raw.list_date.isnull()]
dd = pd.read_csv("processed_users_verified.csv")$ dd.head()$
test_df = allData[allData.lang=='es']$ print(test_df)
new_fan.to_csv('../data/new_fan.csv')$ return_fan.to_csv('../data/return_fan.csv')
fps.pyldavis_fp
S_lumpedTopmodel.meta_basinvar.filename
df.ix[:4]
pd.date_range('12/31/2017', '01/01/2018', freq='H') $
data.loc[data['hired']==1].groupby('category').num_completed_tasks.mean()
result = api.search(q='%23Australia')  # "%23" == "#"$ len(result)
os.chdir(root_dir + "data/")$ filtered_df_fda_drugs.to_csv("filtered_fda_drug_reports.csv", index=False)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())$
retweet_and_count = sns.factorplot(data=tweets_df, x="name", y="retweet_count", kind="box")$ retweet_and_count.set(yscale="log")$ plt.xticks(rotation=60)     # alwatan have above average number of retweets and alseyassah have below average number of retweets$
for dim in d.dimensions:$     print('%s:\t%s' % (dim, d.dimensions[dim]))
precipitation_df = pd.DataFrame(precipitation_data, columns=['Date', 'Precipitation'])$ precipitation_df.set_index('Date', inplace=True, )$ precipitation_df
lagou_df = pd.read_sql(sql_lagou, conn)
df = pd.DataFrame.from_dict(tweet_info)$ df.head()
html = browser.html$ soup = BeautifulSoup(html, 'html.parser')
data_FCInspevnt_latest = data_FCInspevnt_latest.drop_duplicates(subset = 'brkey', keep='last')
cityID = 'e444ecd51bd16ff3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cincinnati.append(tweet) 
df2.rename(columns = {'treatment': 'ab_page'}, inplace=True)$ df2.drop('control', axis=1, inplace=True)$ df2.head()
np.random.seed(1)$ rs = 1
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read())$     f.close()
graph.run(tweet_query,param=list_to_merge[23000::])$
for v in data.values():$     if 'Q3' not in v['answers']:$         v['answers']['Q3'] = ['Other']
data4.crs= "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
kushy_prod_df.describe(include="all")$
accuracy = accuracy_score(y_test, predictions)$ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
pred_cols = features$ df2 = sl.copy()$ df2=df2[pred_cols]
df.resample('D').mean()
r = dict(r.json())
dic = dict(req.json())
kushy_prod_data_path = "products-kushy_api.2017-11-14.csv"$ kushy_prod_df = pd.read_csv(kushy_prod_data_path, low_memory=False)$ kushy_prod_df.head(10)
data = pd.read_csv('dog_rates_tweets.csv', parse_dates=[1])$
df['label'] = df[forecast_column].shift(-forecast_out)
s.loc[s>4]
mojog_df["unemp_rate"] = 4.1$ mojog_df.head()
df_merge.head()
crimes.iloc[10:20,4:6]
workspace_data = w.data_handler.get_all_column_data_df()$ lst = workspace_data.WATER_BODY_NAME.unique()$ print('WATER_BODY_NAME in workspace:\n{}'.format('\n'.join(lst)))
import pandas as pd$ import matplotlib.pyplot as plt
KKK = df.groupby(["Source"]).mean()["Compounded Score"]$ overallDF = pd.DataFrame.from_dict(KKK)$ overallDF["Compounded Score"]
z_score, p_value = sm.stats.proportions_ztest([old_conversions, new_conversions], [n_old, n_new], alternative = 'smaller')$ print(z_score)$ print(p_value)$
x.iloc[:3,:]
plate_appearances = plate_appearances.loc[plate_appearances.events.isnull()==False,]
pantab.frame_to_hyper(active_with_return, 'Avtive User Analysis.hyper')
r.text
my_gempro.kegg_mapping_and_metadata(kegg_organism_code='mtu')$ print('Missing KEGG mapping: ', my_gempro.missing_kegg_mapping)$ my_gempro.df_kegg_metadata.head()
url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRlXVQ6c3fKWvtQlFRSRUs5TI3soU7EghlypcptOM8paKXcUH8HjYv90VoJBncuEKYIZGLq477xE58C/pub?gid=0&single=true&output=csv'$ df_hourly = pd.read_csv(url,parse_dates = ['time'],infer_datetime_format = True,usecols = [0,3])$ df_hourly.head()
tcat = pd.read_csv(filepath + 'cat_tweets.csv')$ tdog = pd.read_csv(filepath + 'dog_tweets.csv')$ tcat.head()
leadsdf['simpleDate'] = pd.to_datetime(leadsdf["lastEnteredOn"],format="%Y-%m-%d")$ leadsdf['simpleDate'] = leadsdf['simpleDate'].dt.strftime('%Y-%m-%d')
staff = staff.set_index('Name')$ staff
x.drop(["sum"], axis=1)
data = json.loads(r.text)
cityID = '946ccd22e1c9cda1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Pittsburgh.append(tweet) 
user_df = stories.submitter_user.apply(pd.Series)$ user_df.head()
mlp_fp = 'data/richmond_median_list_prices.csv'$ mlp_df = pd.read_csv(mlp_fp)$ mlp_df.head()
df_2001['bank_name'] = df_2001.bank_name.str.split(",").str[0]$
df1 = tier1_df.reset_index()$ df1 = df1.rename(columns={'Date':'ds', 'Incidents':'y'})
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new['country'].astype(str).value_counts()
import re$ trump['source'] = trump['source'].str.replace(re.compile('<.*?>'), '')$ trump['source'].unique()
model4 = MNB.fit(x_train, y_train)$ model5 = SGDC.fit(x_train, y_train)
df = pd.read_sql('SELECT * from booked_room', con=conn_b)$ df.head(10) # show 10 rows only
current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')$ path = 'Tweet_Frame_'+current_time$ print(path)
%%time$ nodeInDegreeDict = network_friends.in_degree()$ nodeOutDegreeDict = network_friends.out_degree()
merged_data['due_day'] = merged_data['due'].dt.day$ merged_data['due_month'] = merged_data['due'].dt.month$ merged_data['due_year'] = merged_data['due'].dt.year
df.index = pd.PeriodIndex(df.index,freq='Q-JAN')$ df.index
df.columns
df.describe()
authors = EQCC(git_index)$ authors.get_cardinality("author_uuid").by_period()$ print(pd.DataFrame(authors.get_ts()))
df = pd.concat([sanders_df, dtrump_df, jimcramer_df], ignore_index=True) # ignore index = True it will reset all the$
from pyspark.streaming import StreamingContext$  $ streamContext = StreamingContext(SpContext,3) # micro batch size is 3 seconds here . It will receive an RDD every 3 seconds
sub = pd.DataFrame()$ sub['click_id'] = test_df['click_id']$ print("Sub dimension "    + str(sub.shape))
datetime.now().date()
if (kaggle|sim): test = test.reset_index(drop=True)
Measurement = Base.classes.measurements
df2017 = dfSubSites[dfSubSites.Year == 2017]$ df2017sites = df2017.groupby('MonitoringLocationIdentifier').mean()$ df2017sites.TotalN
db = client.nhl_db$ collection = db.articles
df2=df.copy()$ print(id(df),sep='\n')$ print(id(df2),sep='\n')
X_train, X_test, y_train, y_test = train_test_split(features,regression_price,test_size=0.2)
station_distance.insert(loc=1, column='Trip Duration(Minutes)', value=tripduration_minutes)
cur.execute(sql_all_tables)$ all_tables_df = pd.DataFrame($     cur.fetchall(), columns=[rec[0] for rec in cur.description])
dtc = DecisionTreeClassifier(max_leaf_nodes=1002)$ scores = cross_val_score(dtc, X, np.ravel(y,order='C'), cv=10)$ print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
mar_file = os.path.join(DATA_DIR, "addresses.xlsx")
df2 = df$ mismatch_index = mismatch_df.index$ df2 = df2.drop(mismatch_index)
import numpy as np$ x = np.array([0,1,2,3,4,5,6,7])$ x.dtype
response = requests.get(url)
scores[scores.IMDB == scores.IMDB.min()]
grouped_newsorgs = news_sentiment_df.groupby(["News Organization"], as_index = False)
df.index[0]
len(df.user_id.unique())
import tensorflow as tf$ tf.test.gpu_device_name()
yxe_tweets = pop_tweets('stoonTweets.json')
consumer_info = [line.strip() for line in open("df_twitter_consumer_auth.txt")]$ access_info = [line.strip() for line in open("df_twitter_access_auth.txt")]$
grouped_publications_by_author['countCollaborators'] = grouped_publications_by_author['authorCollaboratorIds'].apply(len)
tweets_df.language.value_counts()$
yc_new4 = yc_new3[yc_new3.tipPC > 1]
model_filename = 'models/finalized_traffic_flow_prediction_model.sav'$ loaded_traffic_flow_prediction_model = pickle.load(open(model_filename, 'rb'))$
closes.plot(figsize=(8,6));
a = news_df[news_df['Source Acc.'] == 'BBC']$ a.head()$ print(a['Compound Score'].sum())
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
twitter_df['location'].value_counts()
crimes_by_yr_month = pd.DataFrame(datAll.groupby([datAll['year'],datAll['month']])$                                .agg({'Offense_count':'sum'}))
def load_train_pp(data_path=DATA_PATH,ssize):$     train_pp_path = os.path.join(data_path, "train_pp.csv")$     return pd.read_csv(train_pp_path,nrows=ssize)
data.columns
subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})]
pax_raw.columns = [x.lower() for x in pax_raw.columns]
print ('\nThere are {} rows in this dataframe.'.format(len(df)))$ uniqueRows = len(pd.unique(df['user_id']))$ print ('\nThere are {} unique rows in this dataframe.'.format(uniqueRows))
injury_df.sort_values('Date', ascending=False).head()
fin_p.index
printer_data = pd.read_csv('http://ocf.io/shichenh/ocf_datathon_ds/printing.csv')$ session_data = pd.read_csv('https://www.ocf.berkeley.edu/~shichenh/ocf_datathon_ds/sessions.csv')$ staff_data = pd.read_csv('https://www.ocf.berkeley.edu/~shichenh/ocf_datathon_ds/s_sessions.csv')
df2 = df2.add_suffix(' Closed')$ df4 = pd.merge(df,df2,how='left',left_on='Date Closed',right_on='date Closed')$
drop_num.append(35)$
fig = ax.get_figure()$ fig.savefig("Overall Sentiment.png")
knn = KNeighborsClassifier(n_neighbors=5)$ knn.fit(X_train, y_train)$ knn.score(X_test, y_test)
df_all_wells_wKNN_DEPTHtoDEPT.info()
import pandas as pd$ git_log = pd.read_csv('datasets/git_log.gz',sep='#', header= None,encoding='latin-1',  names=['timestamp', 'author'])$ git_log.head()
sites2 = sites[sites['DrainageAreaMeasure/MeasureValue'] > 25]$ sites2.shape
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
df_wm = pd.read_csv("walmart_all.csv", encoding="latin-1")
sample_record="0,5,1,4,0,0,0,0,0,1,0,0,0,0,0,6,1,0,0,0.9,1.8,2.332648709,10,0,-1,0,0,14,1,1,0,1,104,2,0.445982062,0.879049073,0.40620192,3,0.7,0.8,0.4,3,1,8,2,11,3,8,4,2,0,9,0,1,0,1,1,1"$ label,payload = sample_record.split(',',maxsplit=1)
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ SA1 = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ]) #store data in its own vector $ display(data.head(10))
fa_index = pd.read_excel(path + 'fa_indexs.xlsx')$ fa_index
last_year = dt.date(2017, 6, 2) - dt.timedelta(days=365)$ print(last_year)
rets.hist()$ plt.show()
fg.savefig("Sentiment Analysis of Tweets.png")
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::1].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.'])$ ax.set_ylabel('Relative error')$
cityID = '18810aa5b43e76c7'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Dallas.append(tweet) 
from sklearn.model_selection import train_test_split$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)
tweeter_url = 'https://twitter.com/marswxreport?lang=en'$ browser.visit(tweeter_url)
S_distributedTopmodel.meta_basinvar.filename
news_df=pd.DataFrame({'Source Account':[],'Text':[],'Date':[],'Compound Score':[],'Positive Score':[],'Neutral Score':[],'Negative Score':[]})$ news_df.head()
yearago_date = dt.date(2016, 8 , 22)$ print(yearago_date)
stop_words = nltk.corpus.stopwords.words('portuguese')
print(tweet_df.groupby(["Tweet Source"])["Tweet Source"].count())$ tweet_df.head()
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings_v1.csv', sep=',', encoding='utf-8', header=True)
ratio = result["Fail"].div(result["Pass"])$ ratio.sort_values(ascending=False, inplace=True)$ ratio$
df_ct.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID','Unnamed: 18'],axis=1,inplace=True) 
output_gnb = model_gnb.predict(test[:, 1:5])$ rowID_gnb = [TEST.rowID for TEST in test_data.itertuples()]$ result_df_gnb = pandas.DataFrame({"rowID": rowID,"cOPN": list(output_gnb)})
temp = open('datasets/git_log_excerpt.csv')$ print(temp)
r.status_code
data['Close'].pct_change(periods=2).max()
df.Visitors.tolist()
logit_country = sm.Logit(merged['converted'],merged[['intercept','UK', 'US']])$ result_merged = logit_country.fit()
data.groupby(['Year'])['Salary'].mean()
list.sort()$ print(list)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=5&column_names=[api_key=***')$ print(r.json())$
datos = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(datos.head(10))
bool(login_page.find_all(string=re.compile('redirect')))
%time nb.fit(train_4, y_train)
df_full = df_full[list(DATA_L2_HDR_DICT.values()) + ['school_type']]
rural_driver_total = rural_type_df.groupby(["city"]).mean()["driver_count"]$ rural_driver_total.head()
p_old = df2['converted'].mean()
months['date'] = pd.to_datetime(months['starttime'])$ months.head()
df = df.dropna().copy()
price2017 = price2017[['Date', 'Time', 'Germany/Austria/Luxembourg[Euro/MWh]']]$ price2017.columns = ['Date', 'Time', 'DE-AT-LUX']$ price2017.head()
apple['2017-07']['Close'].mean()
from pyspark.ml.classification import LogisticRegression$ lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)$
emb, summ = seq2seq_inf.generate_issue_title(txt)$ summ
cityID = '7068dd9474ab6973'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Toledo.append(tweet) 
y_pred = model.predict(X_test)$ y_test_rescaled = scaler.inverse_transform(y_test)$ y_pred_rescaled = scaler.inverse_transform(y_pred)
data_other = tmpdf.index[tmpdf[tmpdf.isin(DATA_SUM2_KEYS)].notnull().any(axis=1)].tolist()$ data_other
old_prob = gb.count().values / sum(gb.count().values)
X_train, X_test, y_train, y_test = train_test_split(features,classification_open,test_size=0.2)
contractor.tail()
dfx = df.copy()$ dfx[dfx < 0] = -dfx  # abs$ print(dfx)
import statsmodels.api as sm$ logit = sm.Logit(df['converted'],df[['intercept','treatment']])$
tweet.author
artistDF[locate("Aerosmith", "name") > 0].show(20)
final_test_pred_nbsvm1 = test_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
combined_data.to_excel('SHARE-UCSD-export_reformatted.xlsx',encoding="utf-8", index=False)$ combined_data.to_csv('SHARE-UCSD-export_reformatted.csv',encoding="utf-8", index=False)
Base.classes.keys()
info_final.head(1)
p_old = df2[df2['landing_page']=='old_page']['converted'].mean()$ print("Probability of conversion for old page (p_old):", p_old)
game_vail=df.loc[:,r]$
plot_autocorrelation(series=dr_num_new_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of new patients'))$ plot_autocorrelation(series=dr_num_existing_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of existing patients'))
tUnderweight = len(df[df['bmi']< 18.5])$ tUnderweight
posts.groupby(['from', 'hostname_clean'])['post'].aggregate(sum)
trump_df.to_csv("test.csv", index=False, encoding="utf-8")
ix = ((iris.Species == "setosa") | (iris.Species == "versicolor")) & (iris["Sepal.Length"]>6.5)$ iris.loc[ix.values,iris.columns[:2]]
sanders_df = pd.DataFrame(sanders)$ sanders_df.head()
team_slugs_mini = team_slugs_df[['new_slug','nickname']]$ team_slugs_mini.set_index('nickname', inplace=True)$ team_slugs_dict = team_slugs_mini.to_dict()['new_slug']
urban_summary_table = pd.DataFrame({"Average Fare": urban_avg_fare,$                                    "Total Rides": urban_ride_total})$ urban_summary_table.head()
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))$
df.groupby(['weekday']).agg([sum])$
print(dfx.dropna(how='any'))
gdf = gdf.copy()$ gdf['length'] = gdf['end'] - gdf['start'] + 1$ gdf.head()
json_data = r.json()
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
X.shape
with tf.device('/gpu:0'):$     history = model.fit(X_train, Y_train, epochs=epochs_num, batch_size=16, validation_split=0.1, verbose=1, shuffle=True)
from pandas.io.json import json_normalize$ exportOI = json_normalize(ODResult)
search['search_weekday'] =  search.timestamp.dt.dayofweek+1$ search['trip_start_date_weekday'] =  search.trip_start_date.dt.dayofweek+1$ search['trip_end_date_weekday'] =  search.trip_end_date.dt.dayofweek+1
autoDf = SpSession.createDataFrame(autoMap)$ print (autoDf.show())$
screen_name="SKinnock"$ data.to_csv('%s_likes_rts.csv' % screen_name, sep=',', encoding='utf-8')$
quadratic = [[x ** 2, x, 1] for x in np.arange(0, len(y))] 
cl_ca.columns = ['XML_' + x if x!='APPLICATION_ID' else 'APP_APPLICATION_ID' for x in cl_ca.columns.values]
pd.concat([msftA01.head(3),msftA02.head(3)])
drop_columns = ['discount' , 'plan_list_price', 'actual_amount_paid', 'transaction_date','is_auto_renew' ]$ df_transactions = df_transactions.drop(drop_columns, 1)
S_distributedTopmodel.decision_obj.hc_profile.options, S_distributedTopmodel.decision_obj.hc_profile.value
tweet_data.sort_values('rating', ascending= False).head()$
BUMatrix.plot(x='eventOccurredDate', y=['event_Observation','event_Incident'],style=".",figsize=(15,15))$ plt.show()$
data = pd.read_csv('dog_rates_tweets.csv', parse_dates=[1])
train2014 = data[np.logical_and(data['date'] > '2013-12-31',data['date'] < '2015-01-01')]$ print(train2014.shape)
predict.predict_score('Stuart_Bithell')
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'})$ station_count = station[['Count']]$ station_count $
state = env.reset()$ state, reward, done, info=env.step(env.action_space.sample())$ state.shape
!hdfs dfs -cat 32ordered_results-output/part-0000* > 32ordered_results-output.txt$ !head 32ordered_results-output.txt
ctd = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with ctd.open('inpt-forecasting/Census Tool Data Pull CY2017- May 2018CONFIDENTIAL.xls.xlsx') as ctd_f:$   ctd_df = pd.read_excel(ctd_f)
text = df.section_text[2461]$ text
P = type.__new__(LetsGetMeta,'S',(),{})$ P.__class__.__class__
df_fbase.l_total_asset = df_fbase.total_asset.shift(1)$ df_fbase.l_total_asset = df_fbase.l_total_asset.where(df_fbase.year != "2012")$ df_fbase.avg_total_asset = df_fbase.total_asset.where(df_fbase.year != "2012").rolling(2).mean()
train_msk = ((train.click_timeDay == 8) & (train.click_timeHour >= 9)) | \$ ((train.click_timeDay == 9) & (train.click_timeHour <= 8))$ val_msk = ((train.click_timeDay == 9) & (train.click_timeHour >= 9) & (train.click_timeHour <= 15))
print(data.program_code.value_counts()[:3])
driver.get("http://www.reddit.com")$ time.sleep(2)$ html = driver.page_source
results = session.query(Measurement.date, Measurement.prcp).\$     filter(and_(Measurement.date <= Curr_Date, Measurement.date >= One_year_ago_date)).\$     order_by(Measurement.date.desc()).all()$
car=pd.read_csv('http://data.sfgov.org/resource/cuks-n6tp.csv')$ car.dtypes
a1 = np.array([1, 2, 3, 4]) $ a2 = np.array([4, 3, 2, 1]) $ a1 + a2
session.query(Measurement.station, func.count(Measurement.station)).\$     group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
red.shape
com_eng_df['issues_closed_total'].plot()
zf_train = zipfile.ZipFile('train.csv.zip', mode='r')$ zf_test = zipfile.ZipFile('test.csv.zip', mode='r')
with open('/Users/annalisasheehan/Dropbox/Climate_India/Data/data_child/lat_long_younglives_AS_cut.csv') as f:$     latlon = f.read().splitlines()
import qgrid$ qgrid.set_grid_option('forceFitColumns', False)$ qgrid.set_grid_option('editable', False)
df8_lunch.count()
user_df = pd.read_csv('User_Information.csv')
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
time_length.plot(figsize = (16, 4), color = 'r')$ time_fav.plot(figsize = (16,4), color = 'g')$
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd[:10]
sentiments_pd.to_excel("NewsMood.xlsx", encoding="UTF-8")$
data_AFX_X['Difference'] = data_AFX_X['High'] - data_AFX_X['Low']$ data_AFX_X.describe()
irisRDD = SpContext.textFile("iris.csv")$ irisData = irisRDD.filter(lambda x: "Sepal" not in x)$ print (irisData.count())
df = pd.read_csv('./dataset/master_06-02-2018(hotposts).csv')$ df.drop(columns='Unnamed: 0', axis=1, inplace=True)
uniqueArtists = newUserArtistDF.select("artistID").distinct().count()$ print("Total n. of artists: ", uniqueArtists)$
gnb.__dict__
dfLikes.dropna(subset=["created_time"], inplace=True)
active_with_return.iloc[:,0] = active_with_return.iloc[:,0].astype("int")
contractor_clean['zip_part1'] = contractor_clean.zipcode.str.split('-').str[0]$ contractor_clean['zip_part2'] = contractor_clean.zipcode.str.split('-').str[1]
comm.columns
rf = RandomForestClassifier(n_estimators=50, max_depth=50, n_jobs=4)$ rf.fit(X = clim_train, y = size_train)
for i, words in enumerate(kochdf['name']):$         words = words.replace(char,' ')$     words = words.replace('  ',' ')
req = requests.get(url)$ html = req.text
primary_temp_null_indx=dat[primary_temp_column].isnull()
guido_title = soup.title$ print(guido_title)
import ta # technical analysis library: https://technical-analysis-library-in-python.readthedocs.io/en/latest/$ features['f13'] = ta.momentum.money_flow_index(prices.high, prices.low, prices.close, prices.volume, n=14, fillna=False)$ features['f14'] = features['f13'] - features['f13'].rolling(200,min_periods=20).mean()$
df_details = pd.DataFrame(np.column_stack([id_list, followers_count_list, friends_count_list, statuses_count_list, created_at_list, location_list]),\$                  columns = ['id', 'followers_count', 'friends_count', 'statuses_count', 'created_at', 'location'])
stopword_list = stopwords.words("german")   #saves German stop words in a list$ print(len(stopword_list),"stop words in the list.")   #Prints number (len()) of elements in a list.$
portfolio_df = pd.read_excel('Sample stocks acquisition dates_costs.xlsx', sheetname='Sample')$ portfolio_df.head(10)
final=final[(final.redshift<0.3) & (final.redshift>0.)]$ print final.shape[0]
compound_df = compound_df.reset_index()
density = 1.32
df['Opp_Team']=df.Match_up.str[-5:-2]$ df['home']=df['Match_up'].apply(lambda x: x[2]=='v')$ df['win']=df['W_or_L'].apply(lambda x: x=='W')
building_pa_prc_shrink.to_csv("buildding_02.csv",index=False)$ building_pa_prc_shrink=pd.read_csv('buildding_02.csv',parse_dates=['permit_creation_date'])
df6 = df5.Completed_Date.groupby([df5.Completed_Date.dt.year.rename('Completed_Year'), df5.Completed_Date.dt.month.rename('Completed_Month')]).agg('count')$ print(df6)
conn.execute(sql)
utils.add_coordinates(data)
tlen.plot(figsize=(16,4), color='r');$
payments_all_yrs = \$ df_providers.groupby(['id_num','name','year'])[['disc_times_pay']].agg(['sum', 'count'])$ payments_all_yrs.head()
cityID = 'fef01a8cb0eacb64'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Akron.append(tweet)   
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-06-27&end_date=2018-06-27&apikey='+API_KEY)
terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=2)$ feature_columns = [ terms_embedding_column ]
raw = pd.read_csv('jobs_raw.tsv', sep='\t')
for c in ccc:$     vwg[c] /= vwg[c].max()
df['y'].plot.hist(bins=15)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)$ core_samples_mask[db.core_sample_indices_] = True$ core_samples_mask
engine = create_engine('mysql+pymysql://root:@localhost/sawi_tweets?charset=utf8mb4', encoding='utf8', echo = False)$ df = pd.read_sql_table("sawi_tweets_historical", con = engine)
test_scores = run(q_agent_new, env, num_episodes=100, mode='test')$ print("[TEST] Completed {} episodes with avg. score = {}".format(len(test_scores), np.mean(test_scores)))$ _ = plot_scores(test_scores)
df_subset['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=False, logy=False)$ plt.show()
usgs_temp_cols=hourly_dat.columns[hourly_dat.columns.str.contains("USGS")]$ hourly_dat[usgs_temp_cols]['2016':'2018'].plot()
data.sort_values(by = 'Age', ascending = False) #Really? What is a 95 yr old doing on NairaLand?
ttt = list(spp.term.unique())
res = sm.tsa.seasonal_decompose(events_per_day['count_event_day'].values,freq=7,model='multiplicative')
sl[sl.status_binary==0][(sl.today_preds==1)].shape[0]*.57
df_final.columns
df['created_at'] = pd.to_datetime(df['created_at'])$ df.head()
orders_subset = orders[orders.user_id.isin(selected_users)]
corpus = [dictionary.doc2bow(interests_token) for interests_token in interests_tokens]$ print(interests_tokens[1])$ print(corpus[1])
df_tweets_sort.to_csv('News_Tweets_Data.csv',index=False)
    spacy_a = nlp(a)$     spacy_b = nlp(b)$     return spacy_a.similarity(spacy_b)
grid.cv_results_['mean_test_score']
label_and_pred = dtModel.transform(testData).select('label', 'prediction')$ label_and_pred.rdd.zipWithIndex().countByKey()$
high12 = session.query(Measurement.tobs).\$ filter(Measurement.station == "USC00519281", Measurement.station == Station.station, Measurement.date >="2016-08-23", Measurement.date <="2017-08-23").\$ all()
rf_average = rf.rainfall.mean(dim =('longitude','latitude'))$ rf_mnthy_mean = rf_average.resample('1M', dim='time',how='mean')$ rf_mnthy_tot = rf_average.resample('1M', dim='time',how='sum')$
final_csv.sentiment_score.idxmax()$
len(df_measurement['station'].unique())$
print("Percentage of positive tweets: {}%".format(data_spd.query('SA>1').shape[0]*100/len(data_spd['tweets'])))$ print("Percentage of neutral tweets: {}%".format(data_spd.query('SA==1').shape[0]*100/len(data_spd['tweets'])))$ print("Percentage of negative tweets: {}%".format(data_spd.query('SA<1').shape[0]*100/len(data_spd['tweets'])))
result = forest.predict(testDataVecs)$ output = pd.DataFrame(data={"id":test["id"], "sentiment":result})$ output.to_csv( "output.csv", index=False, quoting=3 )
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['acc'])
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S_lumpedTopmodel.executable = excutable +'/summa.exe'
def load_data(url):$     return pd.read_json(url, orient='columns') $
wa_sales_decompose = sm.tsa.seasonal_decompose(wa_sales, freq=12, model='additive')$ decmpose_plot = wa_sales_decompose.plot()
siteIDs = dfSubSites['MonitoringLocationIdentifier'].unique().tolist()$ siteIDs
high = max(v.Open for v in data.values() )$ low = min(v.Open for v in data.values())$ print('=>The high and low opening prices for this stock in 2017 were {:.2f} and {:.2f} '.format(high, low) + 'respectively.')
requests.get(wikipedia_content_analysis)
country = pd.get_dummies(auto_new.Country)$ country.head()
df4.describe() # basic stats all from one method$
opener = r.build_opener()$ opener.addheaders = [('User-agent', 'Mozilla/5.0')]$
symbol='IBB'$ benchmark2 = web.DataReader(symbol, 'yahoo' , start_date ,end_date)
stn_tempobs_df=pd.DataFrame(stn_tempobs,columns=['Station','Temperature (Deg. Fah.)'])$ stn_tempobs_df
df['is_application'] = df.application_date.apply(lambda x: 'No Application' if x == None else 'Application')$ df.head(5)
sorted_errors_idx = options_frame['ModelError'].map(abs).sort_values(ascending=False).head(50)$ errors_20_largest_by_strike = options_frame.ix[sorted_errors_idx.index]$ errors_20_largest_by_strike[['Strike', 'ModelError']].sort_values(by='Strike').plot(kind='bar', x='Strike')$
datetime.time(datetime(2015,12,14,15,17,30))
busy_stations = session.query(Measurement.station, func.count(Measurement.tobs)).filter(Station.station == Measurement.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()$ busy_stations
average_of_averages = df['average'].mean()
lsi = models.LsiModel(corpus_tfidf, id2word=bag, num_topics=10)$ corpus_lsi = lsi[corpus_tfidf]
cursor_sampl = collection_reference.aggregate($     [{'$sample': {'size': 20}}]$ )
from functools import reduce$ dfs = [df_CLEAN1A, df_CLEAN1B, df_CLEAN1C] # lift of the dataframes$ data = reduce(lambda left,right: pd.merge(left,right,on='MATCHKEY', how='inner'), dfs)$
measurements_df.to_csv('clean_hawaii_measurements.csv', index=False)
places = extractor.geo_search(query="Philippines", granularity="country")$ place_id = places[0].id$ print("Philippines id is:", place_id)
my_stream.disconnect()
df = pd.read_csv('basic_statistics_single_column_data.csv')
randomdata2 = randomdata1[(randomdata1 >=3) | (randomdata1 <=-3)]$ randomdata2.describe()$
URL = "http://www.reddit.com"
calls_df.groupby(["dial_type"])["length_in_sec"].mean()
np.all(x == 6)
df_tweets['expanded_urls'] = df_tweets['expanded_urls'].apply(lambda x : set(json.loads(x)))
aggdf = tweetdf[['lat','lng','text']].loc[~pd.isnull(tweetdf['lat'])].groupby(['lat','lng']).agg('count').reset_index()$
%%time$ grid_svc.fit(X, y)
print(mbti_text_collection.info())
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])
coinbase_btc_eur_min['Timestamp'] = pd.to_datetime(coinbase_btc_eur_min['Timestamp'], format="%Y/%m/%d %H:%M")
fe.bs.bootstrap(3, poparr)$
part_of_site_name = input('What are some of the letters in site name?')$ part_of_site_name = part_of_site_name.upper()$ matching = [s for s in Site_names if (part_of_site_name in s )]$
precip_data = session.query(Measurements).first()$ precip_data.__dict__
measure_nan.count()
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
fit1_test = fh_1.transform(test.device_model)$ fit2_test = fh_2.transform(test.device_id)$ fit3_test = fh_3.transform(test.device_ip)
lm=sm.Logit(df2['converted'],df2[['intercept','ab_page']])$ r=lm.fit()
import matplotlib.pyplot as plt$ plt.matshow(ibm_hr_target_small.select("*").toPandas().corr())
features = ['cEXT', 'cNEU', 'cOPN', 'cAGR', 'cCON']$ for feature in features:$     status_data[feature] = status_data[feature].map({'y': 1.0, 'n': 0.0}).astype(int)
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_new, (1 - p_new)])$ p_old_sim = old_page_converted.mean()$ p_old_sim
r6s = r6s[['created_utc', 'num_comments', 'score', 'title', 'selftext']]$ r6s['created'] = pd.to_datetime(r6s['created_utc'],unit='s')$ r6s = r6s[r6s['created']>datetime.date(2017,12,1)]
df2['user_id'][df2.duplicated(['user_id'], keep=False)]
mean = np.mean(data['Likes'])$ print("The Likes' average in tweets: {}".format(mean))
feedbacks_stress.loc[feedbacks_stress['versao'] == 1, ['incomodo', 'interesse1', 'interesse2'] ] //= 2$
model.wv.most_similar('cost_function', topn=10)
df['tax_type'] = df['tax_type'].str.replace(u'\xa0', u' ')$
preci_df.describe()$
p_old = df2['converted'].mean()$ print("{} is the convert rate for Pold under the null.".format(p_old))
lb = articles['tokens'].map(len).quantile(.025)$ ub = articles['tokens'].map(len).quantile(.975)$ articles = articles.loc[(articles['tokens'].map(len)>lb) & (articles['tokens'].map(len)<ub),:]
tweets_raw = pd.read_table(filepath_or_buffer='tweets_terror2.txt', names=["lan","id","date", "user_name", "content"])
trading_volume.sort()
session.query(Station).count()$
cities_df = pd.DataFrame()$ weather_df = pd.DataFrame()
import pandas as pd$ import os$ os.chdir('C://Users/dane.arnesen/Documents/Projects/pytutorial/')
from statsmodels.tsa.arima_model import ARIMA$ model_6203 = ARIMA(dta_692, (5, 1, 0)).fit() $ model_6203.forecast(5)[:1] 
stats['commit'] = commit_df.commit.iloc[-1]
feed_ids = feeds[feeds['publication_id'] == pubs[pubs['name'] == 'Fox News'].id.values[0]]['id']$ print(feed_ids) # these can be used as indices for feeds_items
%timeit articles['tokens'] = articles['content'].map(nltk.tokenize.RegexpTokenizer('[A-Za-z]+').tokenize)
model_artifact = MLRepositoryArtifact(model, training_data=train.select('ATM_POSI','POST_COD_Region','DAY_OF_WEEK','TIME_OF_DAY_BAND','FRAUD'),\$                                       name="Predict ATM Fraud")$ model_artifact.meta.add("authorName", "Data Scientist");
t1.stage.value_counts()
Val_eddyFlux = Plotting(hs_path + '/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[(sprintsWithStoriesAndEpics_df['Sprint Age In Days'] > 3) | (sprintsWithStoriesAndEpics_df['Age In Days'] > 3)]$ sprintsWithStoriesAndEpics_df[['key_story', 'Team_story', 'fixVersions_story', 'summary_story', 'status_story', 'Age In Days', 'Sprint Age In Days', 'Open Set To Date']].sort_values(by=['Age In Days', 'Sprint Age In Days'], ascending = False)
time_open_days_issues = Issues(github_index)$ time_open_days_issues.is_open()$ time_open_days_issues.fetch_results_from_source('time_open_days', 'id_in_repo', dataframe=True)
train = pd.read_csv("../input/web-traffic-time-series-forecasting/train_1.csv")$ keys = pd.read_csv("../input/web-traffic-time-series-forecasting/key_1.csv")$ ss = pd.read_csv("../input/web-traffic-time-series-forecasting/sample_submission_1.csv")
ad_data=appended.union(ad_data3)
import pandas as pd$ pd.set_option('display.max_columns', None)
df_concensus_uaa = df_concensus_uaa.sort_index()
df = df[df["status"] == "active"]$ df.drop(["status"], axis = 1, inplace = True)$ df.shape
closed_issue_age = Issues(github_index).is_closed()\$                                        .fetch_results_from_source('time_to_close_days', 'id_in_repo', dataframe=True)$ print(closed_issue_age.head())
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-02-02&end_date=2018-02-03&api_key=' + API_KEY$ r = requests.get(url)$ json_data = r.json()
df_all_wells_wKNN_DEPTHtoDEPT.tail()
! head -n 5 ../../data/msft.csv # OS/Linux$
sdf.createOrReplaceTempView("tempTable")$ res.show()
apple = web.DataReader("AAPL", "morningstar", start, end)
containers = page_soup.find_all("section", {"class":"ref-module-paid-bribe"})
d.groupby('label')['tweet_id'].count()
csvHead = pd.read_csv('Data/TripData/JC-201706-citibike-tripdata.csv')
DataSet['tweetSource'].value_counts()[:5]$
top_chater = non_na_df['sender_card'].value_counts()$ top_chater.nlargest(20).plot(kind='bar', figsize=(14,6))
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
my_gempro.map_uniprot_to_pdb(seq_ident_cutoff=.3)$ my_gempro.df_pdb_ranking.head()
aggregates['Email Address'] = aggregates['Email Address'].apply(lambda x: str(x.lower().strip()))$ aggregates['Email Address'] = aggregates['Email Address'].astype(str)
print(rmse_scores.mean())
iris.loc[iris["Species"].isin(["setosa","versicolor"]),:].sample(10)
now = time.strftime("%c")$ todays_date = time.strftime("Current date & time" + time.strftime("%c"))$ print(now.split(':')[0].replace('  ',' ') + ':' + now.split(':')[1])$
os.chdir(os.path.expanduser('~/PycharmProjects/chat-room-recommendation/'))$ lines = open('cornell-movie-dialogs-corpus/movie_lines.txt','r').read().split('\n')$ movie_metadata = open('cornell-movie-dialogs-corpus/movie_titles_metadata.txt','r').read().split('\n')
parsed_locations[parsed_locations.country.isnull()]
matchinglist = getmatchinglist()$ matchinglist.head()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
from matplotlib import style$ style.use('fivethirtyeight')$ import matplotlib.pyplot as plt
df.drop_duplicates(subset=['last_name'], keep='last')$
pprint.pprint(treaties.find_one({"reinsurer": "AIG"}))
mean_abs_dev = lambda x: np.fabs(x-x.mean()).mean()$ pd.rolling_apply(hlw,5,mean_abs_dev).plot();
commonwords=set(list(zip(*wordfrequencylist))[0]).intersection(list(zip(*hashtaglistw))[0])$ frequentcommonwords=set(list(zip(*wordfrequencylist[:200]))[0]).intersection(list(zip(*hashtaglistw[:200]))[0])
y_class_baseline = demo.get_class(y_pred_baseline)$ cm(y_test,y_class_baseline,['0','1'])
y=dataframe1['Close']$ plt.plot(y)$ plt.show()
nasa_url = 'https://mars.nasa.gov/news/'$ jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
FOX = news_df.loc[(news_df["Source Account"] == "FoxNews")]$ FOX.head(2)
df.ix[df.injured > 0, 'injured'] = 1 
grouped_df = news_sentiment_df.groupby('News_Source')$ grouped_compound = grouped_df['compound'].mean()$ grouped_compound
from pprint import pprint # ...to get a more easily-readable view.$ pprint(example_tweets[0])
move_3_union = sale_lost(breakfastlunchdinner.iloc[3, 1], 20)$ adjustment_2 = move_1_union + move_2_union$ print('Adjusted total for route: ' + str(move_34p14u14u - move_3_union))
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Doctors', doc_duration, window=52)
fixey = tidy_format.merge(sent, how='left', right_index=True, left_on='word').groupby(['id']).sum()$ trump['polarity'] = fixey['polarity']$
d['pasttweets_text']=d['pasttweets'].apply(lambda x: ', '.join(x))
import numpy as np$ ok.grade('q02')
X_new = pd.DataFrame({'TV': [40]})$ X_new.head()
session.query(Measurement.date).order_by(Measurement.date.desc()).first()
rowsToSkip = list(range(28))$ rowsToSkip.append(29)
db = client["clintrials"]$ coll = db["clinical_studies"]$ client["clintrials"].command("listCollections")
clean_stations.columns = ['station', 'name', 'latitude', 'longitude', 'elevation', 'city', 'country', 'state']
cur.execute('SELECT results FROM trials WHERE trial_id = 1')$ trial1_results = cur.fetchone()$ pd.read_csv(io.StringIO(trial1_results[0]), index_col=0)$
user_df.columns = ['response_id', 'id', 'customer_id', 'crm_tier', $                    'sales_tier', 'mktg_tier', 'addons',$                    'screen_size', 'role', 'language']
type(t2.tweet_id.iloc[2])
X=final_df.drop(['max', 'mean','std','sum'], axis=1)$ print("number of samples are= "+str(X.shape[0]) +" with number of features= "+str(X.shape[1]))
dr_num_new_patients = dr_new['id'].resample('W-MON', lambda x: x.nunique())$ dr_num_existing_patients = dr_existing['id'].resample('W-MON', lambda x: x.nunique())
page.exists()
views = containers[0].find("li", {"class":"views"}).contents[1][0:3]$ views
siteMask = nitrodata['MonitoringLocationIdentifier'].isin(siteInfo.index)$ dfSubSites = nitrodata[siteMask]$ dfSubSites.shape
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
authorization_instance = tweepy.OAuthHandler(consumer_key,consumer_secret)$ authorization_instance.set_access_token(access_token, access_token_secret)$ authorization_instance.secure = True
df2.shape, op_before.shape, op_after.shape,  op_a2.shape$
points_dic={"India":345,"Bangladesh":456,"Pakistan":789,"China":90}$ points=pd.Series(points_dic)$ points$
 (merged_visits.pipe(lambda df: df['Fail'].div(df['Pass']))$   .dropna()$   .sort_values(ascending=False))$
df = pd.read_csv('local_newspapers.csv')$ df.head()
url = "https://www.dropbox.com/s/he5tvdxriqj9s9b/parsed_tweets.csv?dl=1"$ location = './'$ download_file(url, location)
session.query(Measurement.tobs).order_by(Measurement.tobs.desc()).first()
elms_all_0611 = elms_all_0604.append(elms_all, ignore_index=True)$ elms_all_0611.drop_duplicates('ACCOUNT_ID',inplace=True)
shopping_carts = pd.DataFrame(items)$ shopping_carts
X = np.array(df1.drop(['label'], axis=1))$ y = np.array(df1['label'])
dftotal=pd.concat([df1,df1_dummies],axis=1)$ dftotal.head()
top_songs['Date'] = pd.to_datetime(top_songs['Date'])
df.head()
!jupyter nbconvert index.ipynb --to html$ bucket.upload_file('index.html', 'index.html')$ bucket.upload_file('index.ipynb', 'index.ipynb')
conn_laurel = psycopg2.connect("dbname='analytics' user='analytics' host='analytics.cv90snkxh2gd.us-west-2.rds.amazonaws.com' password='!TgP$Ol9Z&6QhKW0tmn9mOW5rYT2J8'")$
X = pd.get_dummies(X, columns=['subreddit'], drop_first=True)
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
price2017 = price2017.drop(['Date','Time'], axis=1)
close_price=[row[4] for row in daily_price if row[4] is not None]$ close_price.sort()$ print(close_price[-1]-close_price[0])$
trainpath = "./data/train.csv"$ testpath = "./data/test.csv"$
iotc = pd.read_excel('IOTC_positive_list_2017-03-09.xls')
from sklearn.preprocessing import Imputer$ trainDataVecs = Imputer().fit_transform(trainDataVecs)
title = soup.find_all('div', class_="content_title")[1].text.strip()$ description=soup.find_all('div', class_="rollover_description_inner")[1].text.strip()
shopping_carts = pd.DataFrame(items)$ shopping_carts
bob_shopping_cart = pd.DataFrame(items, columns=['Bob'])$ bob_shopping_cart
df3_obs.loc['Churn Date'] = obs_end_date - dt.timedelta(df3_obs.loc['Frequency of donations'])
fakes.comment[0]
category_count_df2 = category_count_df1.loc[category_count_df1["category"] >= 800 , :] $
extractor = twitter_setup()$ tweets = extractor.user_timeline(screen_name="realDonaldTrump", count=200)$ print("Number of tweets extracted: {}.\n".format(len(tweets)))
df1['forcast'].head() #allthe data will be nan $
df['MeanFlow_cfs'].describe()
Desc_active_stations = session.query(Measurement.station, func.count(Measurement.prcp)).\$                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).all()$ Desc_active_stations
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=ibBvKxUvyJJC1ziPSUPJ'$ r = requests.get(url)
df.replace('\n', ' ', inplace=True, regex=True)$ df.lic_date = pd.to_datetime(df.lic_date, errors='coerce')$ df = df.sort_values('lic_date', ascending=False)
print 'Date range: %s - %s' % (response_df.created.min(), response_df.created.max())
price_dict = price_data.to_dict()
gb.agg(['sum', 'count'])    $
d = requests.post(link, json=contact_form, params={'hapikey': hubspot_write_key})$ d.status_code$
squared_errors_quadratic = [(y_hat[i] - y[i]) ** 2 for i in range(len(y))]$ print("Total sq error is {0}".format(sum(squared_errors_quadratic)))$ print("Average sq error is {0}".format(sum(squared_errors_quadratic)/len(squared_errors_quadratic)))
spotify_df['Number One']=np.nan
tweetdf = pd.read_csv('../../data/clean/tweets_w_lga.csv') # shortcut
xml_in_sample['authorId'].nunique()
suburban_driver_total = suburban_type_df.groupby(["city"]).mean()["driver_count"]$ suburban_driver_total.head()
driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a').click()
iso_join.plot();
difference = total.xs_tally - absorption.xs_tally - scattering.xs_tally$ difference.get_pandas_dataframe()
tweetdf = pd.read_csv('../../data/clean/tweets_w_lga.csv') # shortcut$ tweetdf['latlng'] = list(zip(tweetdf.lat, tweetdf.lng))
tweet_df["tweet_source"].unique()
df = pd.read_csv(pump_data_path, index_col = 0)$ df.head(1)
iris.head().iloc[:,0]
path = "http://www.principlesofeconometrics.com/stata/consumption.dta"$ df = pd.read_stata(path)$ df.head(5)
engine = create_engine('sqlite:///hawaii.sqlite')
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=YxSY4z-2vyxvJ15WjtFa&start_date=2017-01-01&end_date=2017-12-31"$ req = requests.get(url)$ req.text
automl_feat = pickle.load(open(filename, 'rb'))
df_videos.head()
table_rows = driver.find_elements_by_tag_name("tbody")[24].find_elements_by_tag_name("tr")$
kickstarter_req = requests.get("https://webrobots.io/kickstarter-datasets/")$ zip_to_down_ls = re.findall("href=\"(.*s3\.amazonaws.*\.json\.gz)\"", kickstarter_req.text)$ zip_to_down_ls
indeed.dropna(subset=['summary'], inplace=True)$ indeed.isnull().sum()
preci_df = pd.DataFrame(preci_data)$ preci_df.head()
DF1.describe()
trn_lm, val_lm = sklearn.model_selection.train_test_split(np.array(df["numerized_tokens"]), test_size=0.1)$ len(trn_lm), len(val_lm)
sns.set(font_scale=1.5, style='whitegrid')$ sns.set_palette(sns.cubehelix_palette(rot=-.4))
nb_topics = 5$ lda = LatentDirichletAllocation(n_components=nb_topics, learning_method='batch', random_state=42)$ lda.fit(vects)
cityID = 'a84b808ce3f11719'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Omaha.append(tweet) 
plt.rcParams['figure.figsize'] = [16,4]$ plt.plot(pd.to_datetime(mydf1.datetime),mydf1.fuelVoltage,'g.', markersize = 2);$
min_IMDB = scores.IMDB.min()
payments_total_yrs.head(2)
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).head(5)
df_t1 = df.query('landing_page == "new_page"').query('group == "treatment"')$ df_t2 = df.query('landing_page == "old_page"').query('group == "control"')$ df2 = df_t1.append(df_t2, ignore_index=True)
ax = pdf.plot()$
client.deployments.list()
mentors = data_final[data_final['countCollaborators'] > 4]['authorId'].unique()$ mentees = data_final[data_final['countCollaborators'] <= 4]['authorId'].unique()
df['created_date'] = pd.to_datetime(df['created_date'])$ df.head()
result2.summary2()
df_vow['Year'] = df_vow.index.year$ df_vow['Month'] = df_vow.index.month$ df_vow['Day'] = df_vow.index.day
print("Unique users:", len(df2.user_id.unique()))$ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
data[(data['author_flair'] == 'Saints') & (data['game_state'] == 'close')].comment_body.head(15)$
columns = ['DepartmentId', "Date", "Volume"]$ df_ = pd.DataFrame(data = new_df, columns = columns)$ df_.to_csv("Task1.csv", sep='\t')
df_grouped.drop(['passing_reading', 'passing_math'], axis = 1, inplace = True)$ df_grouped.columns
df_merge.per_student_budget.describe()
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY)$ r=requests.get(url)
if create_sql_indexes:$     conn.execute('create index left_ix on left(key, key2)')$     conn.execute('create index right_ix on right(key, key2)')$
human_genome_length = gdf.length.sum()$ print("The length of the human genome is {} bases long.".format(human_genome_length))
prcp_df = pd.DataFrame(prcp_data, columns=['Date', 'Precipitation'])$ prcp_df.set_index('Date', inplace=True) # Set the index by date
df_2004['bank_name'] = df_2004.bank_name.str.split(",").str[0]$
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
from sklearn.model_selection import train_test_split $ X_train,X_test,y_train,y_test = train_test_split(X,y, random_state=42)
results = sm.OLS(gdp_cons_df.Delta_C1[:140], gdp_cons_df.Delta_Y1[:140]).fit()$ print(results.summary())
rfc = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=0)$ cv_score = cross_val_score(rfc, features_class_norm, overdue_transf, scoring='roc_auc', cv=5)$ 'Mean ROC_AUC score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
fig, ax = plt.subplots(figsize=(12,12))$ xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)$ plt.show()
search_rating = np.vectorize(search_rating, otypes=[np.float])$ data['rate'] = search_rating(data['text'])$ data = data[pd.notnull(data['rate'])]
twitter_daily_df = twitter_daily_df.join(combined_text, ["Day","Company"]).orderBy('Day','Company')
obamaSpeechRequest = requests.get('https://en.wikisource.org/wiki/Barack_Obama%27s_Eighth_State_of_the_Union_Address')$ print(obamaSpeechRequest.text[40000:41000])
joined[['Frequency_score']] = joined[['Frequency_score']].astype(int)$ joined.dtypes.filter(items=['Frequency_score'])
winpct = pd.read_csv('All_Games_Win_Pct.csv')$ winpct['text'] = winpct['playtext']$ winpct['date'] = pd.to_datetime(winpct['Game Date'])
news_sentiment_df.to_csv('news_sentiment.csv', sep=',')
for iter_x in np.arange(lookback_window)+1:$     df['{0}-{1}'.format(base_col,str(int(iter_x)))] = df[base_col].shift(iter_x)
tranny.values
convictions_df = convictions_df.replace(r'\n',' ', regex=True) $ convictions_df = convictions_df.replace(r'\t',' ', regex=True) $ convictions_df
x.loc[x.loc[:,"A"]>0.6,"A"]
df1 = df1.dropna()
stations = session.query(Measurement).group_by(Measurement.station).count()$ stations
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv', nrows=10)
xtrain,xtest,ytrain,ytest=cross_validation.train_test_split(X,Y,test_size=0.2)
print('Total minutes over time series: {}'.format(df_btc.shape[0]))$ print('% minutes in time series with {} trades: {}'.format('USD-BTC', df_btc['USD-BTC_low'].notna().sum()/df_btc.shape[0] ))$
move_1_herald = sale_lost(breakfastlunchdinner.iloc[1, 1], 10)$ print('Adjusted total for route: ' + str(move_34p34h34h - move_1_herald))
for item in top_three:$     print('{} has a std of {}.'.format(item[0], item[1]))
data = aapl.get_call_data(expiry=aapl.expiry_dates[4])$ data.iloc[0:5:, 0:5]
top10_topics_list = top10_topics_2.head(10)['topic_id'].values$ top10_topics_list
red_4.drop(['created_utc', 'time fetched'], axis = 1, inplace = True)
merged = price2017.merge(typesub2017,how='inner',left_on='DateTime',right_on='DateTime')
vendors = df_receipts.vendor_search.unique()$ vendors
sales = sales.join(shops.set_index('shop_id'))  # train + shops join by shop_id $ items_categories = item_categories.join(items.set_index('item_category_id'))    # item_categories + items join by item_category_id$ sales = sales.join(items_categories.set_index('item_id'))    # train + items_categories join by item_id
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']#['link_weight'].loc[97528]
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['Retweets'].values, index=data['Date'])
yc_merged = yc_trimmed.merge(yc_sd, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')
df.index
df1.dropna(inplace=True)
days = ['day1', 'day2']$ nocol=pd.DataFrame(index=days)$ nocol
df.to_csv(DATAPATH+'submit_most_popular_category.csv', index=False)$
columns = inspector.get_columns('measurements')$ for c in columns:$     print(c['name'], c["type"])$
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
master_df_total=pd.concat([master_df,master_dummies],axis=1)
data.columns
avgPurchP = train.groupby(by='Product_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseP'})$ train = train.merge(avgPurchP, on='Product_ID', how='left')$ test = test.merge(avgPurchP, on= 'Product_ID', how='left')
us_temp = temp_fine.reshape(843,1534).T #.T is for transpose$ np.shape(us_temp)
store_items = store_items.rename(index = {'store 3': 'last store'})$ store_items
x = store_items.isnull().sum().sum()$ print(x)
df = stories_df[((stories_df['fixVersions'].isin([reln, 'Backlog']) | pd.isnull(stories_df['fixVersions'])) & $                         (stories_df['status'].isin(['Code Review', 'In Progress', 'Approval', 'Closed'])) )]$ df[['key', 'status', 'fixVersions', 'summary']]
df_a = pd.DataFrame(df_graph.groupby('dates')['click_rec_menues'].value_counts())
automl = autosklearn.regression.AutoSklearnRegressor($ )
data.quantile(0.25)
x_min=np.around(np.amin(windfield_matched_array),decimals=-1)-10$ x_max=np.around(np.amax(windfield_matched_array),decimals=-1)+10$ x_num= np.around(np.amax(windfield_matched_array)-np.amin(windfield_matched_array))
bucket_name = buckets[0]$ bucket_obj = cos.Bucket(bucket_name)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\olympics.dta"$ df = pd.read_stata(path)$ df.head(5)
merged = pd.merge(train,properties,how='left',on='parcelid')
Lowest_opening_price = mydata['Open'].min()$ Lowest_opening_price
df1.tail(36)
colors = ["lightblue", "green", "red", "blue", "orange", "maroon"]$ x_axis = np.arange(len(final_df))$ x_labels = diff_df.index
new_df = new_df.loc[df['ooCancelReason'] == 0]
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period(period="quarter")$ print("Trend for quarter: ", get_trend(get_timeseries(closed_pr)))
stop_words = nltk.corpus.stopwords.words('english')$ print(stop_words)$ print('Count: {}'.format(len(stop_words)))
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
mbti_text_collection.to_csv('Reddit_mbti_data_2.csv',encoding='utf-8')
receipts = pd.DataFrame.from_dict(Receipts)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
plt.show()
len(cats_out['Animal ID'].unique())
openmoc_geometry = get_openmoc_geometry(sp.summary.geometry)
our_nb_classifier.last_probability
sd.to_csv("media_sentiment.csv",header=True)
transfer_duplicates.apply(lambda row: smoother_function_part2(row["Year"], row["Month"], row["Day"], row["Smoother"]), axis=1);
assert not tcga_target_gtex_expression_hugo_tpm.isnull().values.any()$ assert not treehouse_expression.isnull().values.any()$ assert np.array_equal(tcga_target_gtex_expression_hugo_tpm.index, treehouse_expression.index)
data.head()$
df['Size'].sum()/(1024*1024*1024)
df2 = merged_pd.loc[:,['visitors', 'weekday', 'holiday_flg']]$ df2.groupby(['weekday','holiday_flg']).agg(['count',np.sum])
importances=model_rf_19_S.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
df['id'].value_counts()    # this will help us to see if there is repetition on the titles$
trainData = trainData.filter(lambda line : line[1] != 1034635)
for p in mp2013:$     print("{0} {1}".format(p.start_time,p.end_time))
print('Median daily volume of 2017: ' + str(int(np.median(vol_vec))))
y = df['comments']$ X = df['title']$ X = X.apply(lambda x: PorterStemmer().stem(x))
df = pd.read_csv('ab_data.csv')$ df.head()
with open("TestUser1.json","r") as fh:$     data = json.load(fh)$
headers= ({'User-agent': 'kiros Bot 0.1'})
sentiment_pd.to_csv('Senitment_On_Tweets.csv')
mean = np.mean(data['len'])$ print("The length's average in tweets: {}", format(mean))$
pd.set_option('max_colwidth',150)$ df_en.head()
my_stocks = ["WIKI/AAPL.1", "EIA/PET_RWTC_D.1", "NSE/OIL.1"]$ import quandl as q$ stock_open = q.get(my_stocks, start_date="2017-01-01", end_date="2018-02-01")
s.describe()
print(1/np.exp(-0.0408),1/np.exp(0.0099))
!hdfs dfs -cat {HDFS_DIR}/p31-output/part-0000* > p31_results.txt
knn = KNeighborsClassifier(n_neighbors=5)$ knn.fit(X_train_total, y_train)$ knn.score(X_test_total_checked, y_test)
print(df_subset.info())
comp = reduce(lambda x, y: pd.merge(df_test, forecast_range, on = 'ds'), df_test.append(forecast_range))$ comp.describe()$
sentiments_df = sentiments_df.sort_values(["Target","TweetsAgo"], ascending=[True, False])$
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
nytimes_df = constructDF("@nytimes")$ display(constructDF("@nytimes").head())
status = client.training.get_status(training_run_guid_async)$ print(json.dumps(status, indent=2))
dfD.to_sql('distances', conn)
findM = re.compile(r'm[ae]n', re.IGNORECASE)$ for i in range(0, len(postsDF)):$ 	print(findM.findall(postsDF.iloc[i,0]))
data_df = data_df[data_df["from.id"]==("368759307733")]
sp500.index
new_page_p = ab_df2.landing_page.value_counts()[0]/ab_df2.shape[0]$ old_page_p = ab_df2.landing_page.value_counts()[1]/ab_df2.shape[0]$ print(('new page probability', new_page_p),('old page probability', old_page_p))
hs.setAccessRules(C_resource_id, public=True)
y_pred_rf = rf.predict(X_test)$ y_train_pred_rf=rf.predict(X_train)$ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(rf.score(X_test, y_test)))
df = df.drop(['Ticket','Cabin'], axis=1)$ df = df.dropna() 
np.mean(cross_val_score(lgb1, train_X, train_Y, scoring='r2', cv=5, verbose=5))
requests.get(wikipedia_marvel_comics)
questions = questions.drop(questions.index[[9,22]])
np.array([1,2,3,4,5])$
for column in ['Announced At','Created At','Shipped At','Updated At']: $     df[column] = pd.to_datetime(df[column])
hours.dropna(subset=['Specialty'], how='all', inplace=True)
my_model_q3_proba = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='probability')$ my_model_q3_proba.fit(X_train, y_train)$ my_model_q3_proba.stackData.head()
header = flowerData.first()$ flowerKV= flowerData.filter(lambda line: line != header)$ print (flowerKV.collect())
temp = pd.read_table('vader_lexicon.txt', names=('word', 'polarity', 'idk', 'idk1'))$ sent = pd.DataFrame({'polarity':temp['polarity']})$ sent.index = temp['word']$
psy_prepro = pd.read_csv("psy_prepro.csv")$ psy_prepro = psy_prepro.set_index('subjectkey', verify_integrity=True)$
df = df.drop_duplicates()$ df.loc[df['Name'] == 'Mary']$ df = df.sample(frac=1).reset_index(drop=True)
rain_df.describe()
input_node_types_DF = pd.read_csv('network/source_input/node_types.csv', sep = ' ')$ input_node_types_DF
car18= car.groupby('dayofweek')['incidntnum'].agg({'numofinstance_car':'nunique'}).reset_index()$
df2.shape
index_remove = list(ab_df[mismatch1].index) + list(ab_df[mismatch2].index)$ ab_df2 = ab_df.drop(labels=index_remove,axis=0)
rf.score(X_train, y_train)
coins_mcap_today = mcap_mat.iloc[-2]$ coins_mcap_today = coins_mcap_today.sort_values(ascending=False)
aug2014.start_time, aug2014.end_time
desc_stats.to_excel('DescStats_v1.xlsx')
cityID = 'b49b3053b5c25bf5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Denver.append(tweet) 
print(data.petal_length.std(),$       data.petal_length.var(),$       data.petal_length.sem())
print("_____________________________________________")$ media_user_overall_df = pd.DataFrame.from_dict(media_user_overal_results)$ media_user_overall_df.head() 
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
df_test_index.iloc[6260, :]
df_ratings.describe()
Base = automap_base()$ Base.prepare(engine, reflect=True)
print data.describe()
actual_diff = (df2[df2['group'] == "treatment"]['converted'].mean()) - (df2[df2['group'] == "control"]['converted'].mean())$ actual_diff
b = R17df.rename({'Create_Date': 'Count-2017'}, axis = 'columns')$
exiftool -csv -createdate -modifydate cisuabg7/cisuabg7_cycle1.MP4 cisuabg7/cisuabg7_cycle4.MP4 cisuabg7/cisuabg7_cycle6.MP4 > cisuabg7.csv
split_pct=0.75$ X_train, y_train, X_test, y_test, scaler = train_test_split(df, split_pct=split_pct, scale_data=True)
data = res.json()
pd.read_pickle('data/city-util/proc/city.pkl', compression='bz2').head()
Station = Base.classes.station$ Measurement = Base.classes.measurement$
SAMPLES = 100; gamma_vals = np.logspace(-2, 3, num=SAMPLES)$ gamma_val, gamma_sr = quick_gamma(gamma_vals, consol_px, hist_window, lb, frequency, min_gross, max_gross, min_w, max_w)$ gamma_val, gamma_sr
df_2012['bank_name'] = df_2012.bank_name.str.split(",").str[0]$
rng = pd.date_range('1/1/2018', periods=120, freq='S')$ len(rng)
df_schools.describe()
models.save('models')
tf.reset_default_graph()$ P("Reseting TF graph")$ the_graph = tf.Graph()
killfile = [i[0] for i in [i for i in list(pgn2value.items()) if i[1] == 1]]$ np.random.shuffle(killfile)$ killfile = killfile[:20572 - 15286 ]$
df = pd.read_csv('Reddit06022018.csv',index_col ='Unnamed: 0' , engine='python')
start_coord_list = station_distance['Start Coordinates'].tolist()$ end_coord_list = station_distance['End Coordinates'].tolist()
h4 = qb.History(spy.Symbol, 360, Resolution.Daily)$
pd.DatetimeIndex(pivoted.columns)
xml_in.shape
pred = predict_class(np.array(theta), X_test_1)$ print ('Test Accuracy: %f' % ((y_test[(pred == y_test)].size / float(y_test.size)) * 100.0))
SelectedOpenClose = AAPL.iloc[200:210, [1,3]]$ SelectedOpenClose
from statsmodels.tsa.arima_model import ARIMA$ model_713 = ARIMA(dta_713, (2, 2, 0)).fit() $ model_713.forecast(5)[:1] 
ct = CellTypesApi()$ cells = ct.list_cells(require_reconstruction=True)$
model.save_weights('best.hdf5')
output_SVM = model_SVM.predict(test[:, 1:5])$ rowID_SVM = [TEST.rowID for TEST in test_data.itertuples()]$ result_df_SVM = pandas.DataFrame({"rowID": rowID,"cOPN": list(output_SVM)})
clfgtb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train, y_train)$ clfgtb.score(x_test, y_test)
pd.read_csv("test.csv", encoding="utf-8").head()
sum(insertid_freq.values())
ccp_df = cc_df[coins].div(market_cap_df['Total Market Cap'], axis=0)$ ccp_df = ccp_df[::-1]    # Reverse order of df$ ccp_df.head()
assert len(target_docs) == 200000, 'target_docs should be truncated to the first 200k rows to use the cached model.'$ fname = get_file(fname='kdd_lm_v2.h5', origin='https://storage.googleapis.com/kdd-seq2seq-2018/kdd_lm_v2.h5', )$ model = load_model(fname)
shows = pd.read_csv("ismyshowcancelled_tmp_1.csv",index_col=0)
xml_in.dropna(subset = ['venueName', 'publicationKey', 'publicationDate'], inplace = True) $
joined[['Frequency_score']] = joined[['Frequency_score']].apply(pd.to_numeric, errors='coerce')
df_features2.loc[df_features2["CustID"].isin([customer])]
active_station_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).filter(Measurement.date > last_year).filter(Measurement.station == most_active_station).all()$
from pandas.io.json import json_normalize$ df_new = json_normalize(list(df_json['user']))
titanic3['home.dest'].str.upper().head()
print("customer_transaction.shape",customer_transaction.shape)$ print("data.shape",data.shape)
list(r_ord.values())[2]$
tail = df.copy()$
prcp.describe()
for c in ccc:$     ved[c] /= ved[c].max()
cityID = 'd5dbaf62e7106dc4'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jacksonville.append(tweet) 
os.getcwd()$ model_path = 'task1_pm_model.h5'
type(df), df.shape$
tweets_df = pd.read_csv('C:\\Users\\ericr\\Google Drive\\schoolwork\\current classes\\CS 230\\Project\\230_crypto\\data\\twitter\\hourly\\labeled_tweets_hourly.csv', encoding='ISO-8859-1')$ print("There are {} tweets".format(tweets_df.shape[0]))$ tweets_df.head()
plt.hist(data['Age'])
session.query(Adultdb).filter_by(occupation="?").delete(synchronize_session='fetch')$ session.commit()
first_row = session.query(Station).first()$ first_row.__dict__
def getResults(JsonReponse):$     return JsonReponse.get('ResultSet').get('Result')
a = temps.read_array()$ plt.imshow(a)
primitives[primitives['type'] == 'transform'].head(10)
dd.to_csv("processed_users_verified.csv", sep=',', encoding='utf-8')$
grid_id = pd.DataFrame(grid_id_flat).astype('str')$ grid_id.columns = ['grid_id'] $ print(grid_id.head(), grid_id.tail())
loss = 10 / np.linspace(1, 100, a.size)$ loss.shape
active_station = session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all()$ active_station
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s')$ git_log.timestamp.describe()
pd.Series({2:'a', 1:'b', 3:'c'})
pulledTweets_df = gu.read_pickle_obj(processed_dir+'pulledTweetsProcessedAndClassified_df')
[tweet.lang for tweet in tweet_list]
df_NOTCLEAN1A.shape
fire_size = pd.read_csv('../data/model_data/1979-2016_fire_size.csv', index_col=0)$ fire_size.dropna(inplace=True)$
purchases = sql_query('select * from purchases')$ purchases.head(3)
gdax_trans['Balance']= 0.00
metadata['spatial_extent'] = refldata.attrs['Spatial_Extent_meters']$ metadata
artistDF[artistDF.artistID==1000010].show()$ artistDF[artistDF.artistID==2082323].show()
number_of_commits = git_log['timestamp'].count()$ number_of_authors = git_log["author"].nunique(dropna = True)$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
!gsutil cp gs://solutions-public-assets/smartenup-helpdesk/ml/issues.csv $CSV_FILE
B_INCR2 = 0.293$ B_DECR2 = -0.293
df = pd.concat([df_en,df_other])
print("Age:-",result_set.age,"workclass:-",result_set.workclass)
rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]
X = pd.get_dummies(X, drop_first = True)$ X.head()
bird_data = pd.read_csv('res/data/bird_tracking.csv')$ ix = bird_data.bird_name == 'Eric'$ x, y = bird_data.longitude[ix], bird_data.latitude[ix]
compared_resuts = ka.predict(test_data, results, 'Logit')$ compared_resuts = Series(compared_resuts)  # convert our model to a series for easy output
rng_utc = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz=dateutil.tz.tzutc())
expanded_data.head()
for edge in H.edges():$     if H[edge[0]][edge[1]]['weight'] > 3000:$         print(edge,partition[edge[0]],partition[edge[1]])
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK', 'ab_page', 'CA_ab_page', 'UK_ab_page']])$ results = logit_mod.fit()$ results.summary2()
expected_index = pd.DatetimeIndex(start=raw.index[0], end=raw.index[-1], freq='1H')$ ideal_index_df = pd.DataFrame(index=expected_index)$ clean = pd.concat([ideal_index_df, raw], axis=1)$
df = pd.read_csv("detected.csv")$ df['META_b_cov'] = df['META_b'].where(df['metaREF']!=df['glmmREF'].str.lower(), -df['META_b'])
plot_BIC_AR_model(data=doc_duration.diff()[1:], max_order_plus_one=10)
result = pd.merge(df, ser, on = 'key').drop('key', axis = 1)$ result
scores[scores.IMDB == max_IMDB]
qs = qs[qs.nwords > 5]
df.sample(5)  # To check the data$
url_mars_facts = "https://space-facts.com/mars/"$ browser.visit(url_mars_facts)
group_by_kmeans = Cluster_df.groupby('KMeansLabels').mean()$ group_by_kmeans.head()
results_jar_rootDistExp, output_jar_rootDistExp = S.execute(run_suffix="jar_rootDistExp", run_option = 'local')
g8_aggregates.columns = ['_'.join(col) for col in g8_aggregates.columns]$ g8_aggregates
pgh_311_data['REQUEST_ID'].resample("M").count().plot()
station_availability_df['avail_docks'].plot(kind='hist', rot=70, logy=True)
datascience_tweets[datascience_tweets['text'].str.contains("RT")]['text'].count() # 322
df.rename(columns={'nm':'city','countryCode':'countrycode'},inplace=True)$ df.set_index("id", inplace=True)$ df.head()
plt.plot(model_output.history['loss'],c='k',linestyle='--')$ plt.show;
goals_df[['PKG', 'PKA']] = goals_df['PKG/A'].str.split('/', expand=True)$ goals_df.drop('PKG/A', axis=1, inplace=True)
df2.loc['2016-09-18', ['GrossIn', 'NetIn']]$
final_df_copy=final_df.copy()$ final_df=final_df._get_numeric_data()$ final_df.head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt"$ mydata = pd.read_table(path, sep= '\t')$ mydata.head(5)
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key='$ url_api=url+API_KEY$ r = requests.get(url_api)
git_log.timestamp = pd.to_datetime(git_log['timestamp'], unit='s')$ git_log.timestamp.describe()
session.query(Measurement.tobs).order_by(Measurement.tobs).first()
x.loc[:,"B":"C"]
miner = TweetMiner(twitter_keys, api, result_limit=400)$ trump_tweets = miner.mine_user_tweets("realDonaldTrump")
ts / ts.shift(1)
tag_df = pd.get_dummies(tag_df)$ tag_df.head()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ req = requests.get(url)
query ="SELECT * FROM tddb_00.Weather_Log ORDER BY Log_Id DESC"$ df = pd.read_sql(query,session)$ df.head(10)
df_tte_ondemand = df_tte[df_tte['ReservedInstance'] == 'N']$ df_tte_ondemand['UsageType'].unique()
data.loc[[pd.to_datetime("2016-12-01"), pd.to_datetime("2016-12-03")], ["TMAX", "TMIN"]]
product_time = nbar_clean[['time', 'product']].to_dataframe() #Add time and product to dataframe$ product_time.index = product_time.index + pd.Timedelta(hours=10) #Roughly convert to local time$ product_time.index = product_time.index.map(lambda t: t.strftime('%Y-%m-%d')) #Remove Hours/Minutes Seconds by formatting into a string
    example1_df.registerTempTable("world_bank")
y_test_array = y_test.as_matrix()
invalid_sets = ["UGL", "UNH", "UST", "pCEL", "pHHO", "VAN"]$ all_sets = all_sets.loc[~all_sets.code.map(lambda x: x in invalid_sets)]
df_inventory_santaclara =df_santaclara.transpose()                     ##Transpose $ df_inventory_santaclara.reset_index(level=0,inplace=True)$ df_inventory_santaclara.columns=['Date','Units']
store_items = store_items.drop(['bikes'], axis=1)  # bikes column$ store_items
system = system.supersize(2, 2, 2)$ print(system)
x_train, x_test, y_train, y_test = train_test_split(bow_df, amazon_review['Sentiment'], test_size=0.2)$ x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1.0/3.0)
tlen = pd.Series(data = data['len'].values, index = data['Date'])$ tfav = pd.Series(data = data['Likes'].values, index = data['Date'])$ tret = pd.Series(data = data['RTs'].values, index = data['Date'])$
columns = inspector.get_columns('clean_hawaii_measurements.csv')$ for c in columns:$     print(c['name'], c['type'])$
mammals = session.query(NA).filter(NA.genus == 'Antilocapra').all()$ for mammal in mammals:$     print("Family: {0}, Genus: {1}".format(mammal.family, mammal.genus))
STEPS = 365*10$ random_steps = pd.Series(np.random.randn(STEPS), index=pd.date_range('2000-01-01', periods=STEPS))
r.summary2()
best_parameters, score, _ = max(GSCV.grid_scores_, key=lambda x: x[1])$ print('best parameters:', best_parameters)
iris.head().loc[:,:"Sepal.Length"]$
! rm -rf ../../results/mrec-4-2 && mkdir -p ../../results/mrec-4-2$ ! cp -R tmp/* ../../results/mrec-4-2
print(sum(receipts.duplicated(['parseUser','reportingStatus'])))$ print(sum(receipts.duplicated(['iapWebOrderLineItemId'])))
flight.crosstab('start_date','from_city_name').show()$
grouped_mean = tweet_df.groupby(["source"]).mean()["compound"]
wqYear = dfWQ.groupby('Year')['TotalN'].mean()$ dfWQ_annual = pd.DataFrame(wqYear)
ltdate = pd.to_datetime(voters.LTDate.map(lambda x: x.replace(' 0:00', '')))$ print(ltdate.describe())$ ltdate.value_counts(dropna=False).head(5)
cat_feats = ['Company']$ features = pd.get_dummies(features, columns=cat_feats, drop_first=True)$ features.head()
hot_df.to_csv('data_redditv2.csv')
fig3 = df['zone'].value_counts().plot('barh', title='Citations by Zone Category')
cityID = '01fbe706f872cb32'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Washington.append(tweet) 
future_dates = prophet_model.make_future_dataframe(periods=forecast_steps, freq='W')$ future_dates.tail(3)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
import pandas as pd$ cs = pd.Series(cleaned)$ by_lga['cleaned'] = cs
points=pd.Series([630,25,26,255],$     index=['India','Bangladesh','Pakistan','China'])$ print(points)
grid_id = np.arange(1, 1535,1)$ grid_id_array = np.reshape(grid_id, (26,59))
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(2))
stamp_name = '4. timestamp'
inspector = inspect(engine)$ inspector.get_table_names()
plt.plot(model_output.history['loss'],c='k',linestyle='--')$ plt.plot(model_output.history['val_loss'],c='purple',linestyle='-')$ plt.show;
tweets_data_path = '../data/tweets_no.json'$ tweets_file = open(tweets_data_path, "r")$ tweets_data = json.load(tweets_file)
for c in ccc[:2]:$     spp[c] /= spp[c].max()
before_date=str(fc_clean.sel(time =start_of_event, method = 'pad').time.values)[0:10]$ after_date=str(fc_clean.sel(time =start_of_event, method = 'backfill').time.values)[0:10]
marsfacts = df_mfacts.to_html(bold_rows=True)$ marsfacts$
table = pd.read_html(url_mars_facts)$ table[0]
pd.DataFrame(sanders)
yc_new2.describe()
def top_n_tweet_days(df,n):$     return df.head(n)$ top_20_tweet_days = top_n_tweet_days(dates_by_tweet_count,20)
churned_ordered = ordered_df.loc[churned_ord]
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url)
precip_data_df.head(3)
s_nonulls = s.dropna()$ s_nonulls
last_date = dt.date(2017, 8, 23)$ last_date
tallies_file.export_to_xml()
temps_df.Missoula - temps_df.Philadelphia
idx = pd.period_range('2011','2017',freq='Q')$ idx
graf['DETAILS']=graf['DETAILS'].str.replace('\n', ' ')
df_train.loc[:, 'is_attributed'] = df_train.loc[:, 'is_attributed'].astype(np.int8)$ df_valid.loc[:, 'is_attributed'] = df_valid.loc[:, 'is_attributed'].astype(np.int8)$ df_test.loc[:, 'is_attributed'] = df_test.loc[:, 'is_attributed'].fillna(0).astype(np.int8)
S_lumpedTopmodel.basin_par.filename
chinese_vessels_iattc = pd.read_csv('chinese_vessels_iattc.csv')
excel=pd.rea_excel("File Name")
prev_year = dt.date.today() - dt.timedelta(days=365)$ prev_year
X_testset.shape$ y_testset.shape
from nltk.corpus import stopwords$ from sklearn.feature_extraction.text import CountVectorizer
P.plot_1d('scalarCanopyTranspiration')
masked['user_age'] = masked['created_at'] - pd.to_datetime(masked['user_created_at'])
data.loc[pd.to_datetime("2016-09-02")]
num_row = df.shape[0]$ print("{} rows in the dataset.".format(num_row))
network_simulation[network_simulation.generations.isin([8])]$
for url in soup.find_all('a'):$     print (url)
import dotce$ stats = dotce.LanguageStats()
pvt.reset_index(inplace=True)$ pvt
df = pd.read_csv('../Datasets/4 keywords.csv', index_col='Symbol')
featured_image_url = browser.find_by_xpath('//*[@id="page"]/section[1]/div/div/article')
vwg['season'] = vwg.index.str.split('.').str[0]$ vwg['term'] = vwg.index.str.split('.').str[1]
schedId_in_proj = [x for x in df_sched.ProjectId if x in list(df_proj.ProjectId)]$ schedId_in_bud = [x for x in df_proj.FMSID if x in list(df_bud.project_id)]
load_weigths_into_target_network(agent, target_network) $ sess.run([tf.assert_equal(w, w_target) for w, w_target in zip(agent.weights, target_network.weights)]);$ print("It works!")
Z = np.dot(np.ones((5,3)), np.ones((3,2)))$ print(Z)$
df = df.set_index('date')
new_stock_data = stock_data.drop('volume', axis = 1)$ print(display(new_stock_data.head()))
iris_new['Sepal'] = iris_new['SepalLength'] * iris_new['SepalWidth']$ iris_new['Sepal'].quantile([0.25, 0.5, 0.75])
pd.options.display.max_colwidth = 300$ data_df[['ticket_id','type','desc']].head(10)
with open('key_phrases.pickle', 'rb') as f:$     key_phrases = pickle.load(f)
import pandas as pd$ pd_cat = pd.get_dummies(ibm_hr_cat.select("*").toPandas())$ pd_cat.head(3)
features.tail(3)
X_test[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
dfDir = '/home/parkkeo1/reddit_unlocked/Isaac/DataFrames/RedditData_Nov-06-2017'$ print_df = pd.read_pickle(dfDir)$ print_df$
columns = inspector.get_columns('measures')$ for c in columns:$     print(c['name'], c["type"])$
faa_data_phil_pandas = faa_data_pandas[faa_data_pandas['AIRPORT_ID'] == "KPHL"]$ print(faa_data_phil_pandas.shape)$ faa_data_phil_pandas.head()
df_lm.filter(regex='q_lvl_0|last_month|q_lvl_0_c').boxplot(by='last_month', figsize=(10,10),showfliers=False)
df.loc[df['lead_mgr'].str.contains('Stanl'), 'lead_mgr'] = 'Morgan Stanley'$
ws = Workspace.from_config()$ print(ws.name, ws.resource_group, ws.location, sep = '\n')
q = pd.Period('2017Q1',freq='Q-JAN')$ q2 = pd.Period('2018Q2',freq='Q-JAN')$ q2-q
new.Purchased.value_counts()/len(new)*100$
print("Probability of treatment group converting:", $       df2[df2['group']=='treatment']['converted'].mean())
S_distributedTopmodel.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
lr = LogisticRegression()$ lr.fit(X, y)
transit_df['DELEXITS']= transit_df['EXITS'].diff()$
top_10_authors = git_log.author.value_counts(dropna=True).head(10)$ top_10_authors
data = pd.read_csv('data/analisis_invierno_5.csv')
results_df.describe()
feats_dict = dict()$ for i, name in enumerate(dummies_df.keys()):$     feats_dict[i] = name $
all_df.isnull().any()$
siteNo = '02087500' #Neuse R. Near Clayton $ pcode = '00060'     #Discharge (cfs)$ scode = '00003'     #Daily mean
data.printSchema()
number_of_commits = len(git_log)$ number_of_authors = len(git_log.dropna().author.unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
treehouse_labels_pruned = treehouse_labels.filter(regex='\ATH|\ATHR', axis="index")
summary_df = sentiments_df.groupby('Media Sources', as_index=False).mean()$ summary_df = summary_df[['Media Sources','Compound']]$ summary_df
%matplotlib inline $ import matplotlib.pyplot as plt # this imports the plotting library in python}
dfjoined = dfrecent.merge(dfcounts, how = 'inner', on = ['created_date'])
log_file_name = 'shopee_add_and_cancel_fans_log\\shopee_add_and_cancel_fans_log_' + today_date_string + '.txt'$ logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])$
n_user_days = pax_raw[['seqn', 'paxday']].drop_duplicates().groupby('seqn').size()
pd.to_datetime(['2009/07/31', 'asd'], errors='ignore')
cityID = '629f4a26fed69cd3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Hialeah.append(tweet) 
new_page_converted=np.random.choice([1,0],size=n_new,p=[pnew,(1-pnew)])$ new_page_converted.mean()
env = gym.make('MountainCar-v0')$ env.seed(505);
scaler = preprocessing.StandardScaler().fit(X_train)$ X_train_scaled = scaler.transform(X_train)$ X_test_scaled = scaler.transform(X_test)
store_items.dropna(axis = 0)
data = np.zeros(4, dtype={'names':('name', 'age', 'weight'),$                           'formats':('U10', 'i4', 'f8')})$ print(data.dtype)
zz = z["uri"].groupby(pandas.Grouper(freq='M')).count()$ zz.plot()
bday = datetime(1986, 3, 6).toordinal()$ now = datetime.now().toordinal()$ now - bday
titanic3 = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')$ titanic3.head()
end_date = pd.Series(pd.to_datetime(end_date).strftime('%Y-%m'),index=churned_ix)
indexed = base_df$ supreme_court_df = base_df.loc[indexed['parent_id'] == 't3_3b6zln'].reset_index()
paired_df_grouped['best_co_occurence'] = paired_df_grouped.apply(lambda x: x['all_co_occurence'][:rule_of_thumb(x['top10'])], axis=1)$ del paired_df_grouped['top10']$
df['date'] = pd.to_datetime(df.date)$ df.set_index('date', inplace=True)
engine.execute('SELECT * FROM measures LIMIT 5').fetchall()
author_commits = git_log.groupby('author').count().sort_values(by='timestamp', ascending=False)$ top_10_authors = author_commits.head(10)$ top_10_authors
df_day['Forecast'] = bound_prediction(sarima_mod.predict())$ df_day$ df_day.plot(figsize=(14, 6));
document_matrix = cvec.transform(my_df.text)$ my_df[my_df.target == 0].tail()
accuracy = accuracy_score(y_test, y_pred)$ print('Accuracy: {:.1f}%.'.format(accuracy * 100.0))
from sklearn.model_selection import train_test_split$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
df_times.head()$
print('\n{0} Movie Recommendations for User = {1}' \$       .format(mov_vec[mov_vec == 0].shape[0], $               tmp_df[tmp_df.tmp_idx == usr_idx].index[0]))
clf = LogisticRegression(fit_intercept=True).fit(X, y)
compared_resuts.to_csv("data/output/logitregres.csv")
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB.json?start_date=2014-01-01&end_date=2014-01-03&api_key=HL_EJeRkuQ-GFYyb_sVd')
df2[df2.duplicated(['user_id'], keep=False)]
import pickle$ with open('house_regressor.pkl', 'wb') as f:$     pickle.dump(automl, f)$
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL')$                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1))$                           &(extract_nondeduped_cmp.app_branch_state=='CA')].groupby('APP_PROMO_CD').size()
accuracy = accuracy_score(y_test, y_pred)$ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
data.RTs.describe()
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Therapists', therapist_duration, window=52)
d=[datetime.strptime(x, '%m/%d/%Y') for x in datestrs]$
y = tweets['handle'].map(lambda x: 1 if x == 'Donald J. Trump' else 0).values$ print(np.mean(y))
df_ratings.shape
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S_1dRichards.executable = excutable +'/summa.exe'
tokens = nltk.word_tokenize(content)$ fdist = nltk.FreqDist(tokens)$ print(fdist)
!hdfs dfs -cat 31results-output/part-0000* > 31results-output.txt$ !head 31results-output.txt
df_daily[df_daily["ENTRIES"] < df_daily["PREV_ENTRIES"]].head()
data2 = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$
dfSummary[4:9].plot(kind='bar',$                     figsize=(20,6),$                     title="Data Summaries: Quantiles");
utils.serialize_data(data)
lsi.save('trump.lsi')$ lsi = models.LsiModel.load('trump.lsi')
df_df = pd.DataFrame(df1)$ df_df
questions.to_csv('../data/clean.csv')
overallQual = pd.get_dummies(dfFull.OverallQual)$
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=3w2gwzsRAMrLctsYLAzN')$ json_data = r.json()$ print(type(json_data))
output_test = output_list$ output_test.count()
engine.execute('SELECT * FROM measurements LIMIT 5').fetchall()
stations = session.query(Measurement).group_by(Measurement.station).count()$ print("There are {} stations.".format(stations))
df_comms = pd.DataFrame(columns=['comm_id','comm_msg','comm_date'])
sum(contractor.state_id.isnull()) #the count of missing state_id value is 0$ contractor.state_id.value_counts() #The state_id columns do not have missing data
pd.Series([42, 13, 2, 69])
response = requests.get(url)$ response
prec_wide_df = pd.concat([grid_df, prec_df], axis = 1)$ prec_wide_df.head()
lda_tfidf.print_topics(num_topics=10, num_words=7)$
kick_projects.isnull().sum()
df_merge.groupby(['school_name', 'grade']).math_score.mean().unstack()
fe.bs.csv2ret??
lst = data_after_first_filter.WATER_BODY_NAME.unique()$ print('Waterbodies in workspace dataset:\n{}'.format('\n'.join(lst)))
cityID = '512a8a4a4c4b4be0'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Charlotte.append(tweet) 
merge_table = pd.merge(extreme_dates1, main_df, on= "Date", how = "left")$ merge_table
most_freq = data[(data['longitude'] == 103.93700000000001) & (data['latitude'] == 1.34721)]
print(data.first_name + " " +data.last_name)
dataset = member_pivot['Percent Purchase']*100$ print dataset.values
for row in df.itertuples():$     print(row)
url2 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2017-01-02&end_date=2017-12-31&api_key=mVyTTi52QUPLHnmV-tx_'$ r2 = requests.get(url2)$ jd = r2.json()$
merged_portfolio_sp_latest_YTD_sp = pd.merge(merged_portfolio_sp_latest_YTD, sp_500_adj_close_start$                                              , left_on='Start of Year', right_on='Date')$ merged_portfolio_sp_latest_YTD_sp.head()
sns.regplot(x="qstot_0", y="y", data=psy_native).set_title("Depressive Symptomatology")$
vac_start_date = '2015-09-01' # start date for Sep 1st 2015$ vac_end_date = '2015-09-16'$ data_start_date = dt.date(2015,9,1) - dt.timedelta(days=365)$
ab_df_new[['CA','UK','US']] = pd.get_dummies(ab_df_new.country)
xml_in[xml_in['publicationKey'].isnull()].count()
expx=np.mean(x)$ expy=np.mean(y)$ expx, expy
validation.analysis(observation_data, BallBerry_resistance_simulation_1)
plot_mnist_sample(mnist_train.train_data, $                   sample_idx=[i for i in range(10)])$ plot_mnist_sample(mnist_test.test_data, size=10)$
store_items.dropna(axis=0) # or store_items.dropna(axis=0, inplace=True) 
S_lumpedTopmodel.forcing_list.filename
for_plt = tweets.set_index('created_at', drop=False, inplace=False)$ for_plt.created_at.groupby(pd.Grouper(freq='H')).count().plot(kind='bar', figsize=(20, 6))$
ymc=yearmonthcsv.coalesce(1)$ path=input("Enter the path where you wanna save your csv")$ ymc.write.format('com.databricks.spark.csv').save(path,header = 'true')
node_types_DF = pd.read_csv(node_models_file, sep = ' ')$ node_types_DF
for df in (joined, joined_test):$   df.loc[df['CompetitionDaysOpen']<0,'CompetitionDaysOpen']=0$   df.loc[df['CompetitionOpenSinceYear']<1900,'CompetitionDaysOpen']=0
element = driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a')$ element.click()
!rm lazy_helpers.py*$ !wget https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py
logit2_countries = sm.Logit(newset['converted'], $                            newset[['ab_page', 'country_UK', 'country_US', 'intercept']])$ result_final = logit2_countries.fit()
df_os[df_os['domain'] == 'infowars.com']
SandP = ['MMM', 'ABT', 'ABBV', 'ACN', 'ATVI', 'AYI', 'ADBE', 'AAP', 'AES', 'AET', 'AMG', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ALXN', 'ALLE', 'AGN', 'ADS', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'APC', 'ADI', 'ANTM', 'AON', 'APA', 'AIV', 'AAPL', 'AMAT', 'ADM', 'ARNC', 'AJG', 'AIZ', 'T', 'ADSK', 'ADP', 'AN', 'AZO', 'AVB', 'AVY', 'BHI', 'BLL', 'BAC', 'BCR', 'BAX', 'BBT', 'BDX', 'BBBY', 'BRK.B', 'BBY', 'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BF.B', 'CHRW', 'CA', 'COG', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CAT', 'CBOE', 'CBG', 'CBS', 'CELG', 'CNC', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHTR', 'CHK', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CME', 'CMS', 'COH', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'CXO', 'COP', 'ED', 'STZ', 'GLW', 'COST', 'COTY', 'CCI', 'CSRA', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DLPH', 'DAL', 'XRAY', 'DVN', 'DLR', 'DFS', 'DISCA', 'DISCK', 'DG', 'DLTR', 'D', 'DOV', 'DOW', 'DPS', 'DTE', 'DD', 'DUK', 'DNB', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EVHC', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'EXC', 'EXPE', 'EXPD', 'ESRX', 'EXR', 'XOM', 'FFIV', 'FB', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC', 'FTI', 'FL', 'F', 'FTV', 'FBHS', 'BEN', 'FCX', 'FTR', 'GPS', 'GRMN', 'GD', 'GE', 'GGP', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GS', 'GT', 'GWW', 'HAL', 'HBI', 'HOG', 'HAR', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HP', 'HSIC', 'HES', 'HPE', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HPQ', 'HUM', 'HBAN', 'IDXX', 'ITW', 'ILMN', 'INCY', 'IR', 'INTC', 'ICE', 'IBM', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IRM', 'JBHT', 'JEC', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KHC', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LEG', 'LEN', 'LUK', 'LVLT', 'LLY', 'LNC', 'LLTC', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MNK', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MJN', 'MDT', 'MRK', 'MET', 'MTD', 'KORS', 'MCHP', 'MU', 'MSFT', 'MAA', 'MHK', 'TAP', 'MDLZ', 'MON', 'MNST', 'MCO', 'MS', 'MSI', 'MUR', 'MYL', 'NDAQ', 'NOV', 'NAVI', 'NTAP', 'NFLX', 'NWL', 'NFX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'PCAR', 'PH', 'PDCO', 'PAYX', 'PYPL', 'PNR', 'PBCT', 'PEP', 'PKI', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'RL', 'PPG', 'PPL', 'PX', 'PCLN', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'QCOM', 'PWR', 'DGX', 'RRC', 'RTN', 'O', 'RHT', 'REG', 'REGN', 'RF', 'RSG', 'RAI', 'RHI', 'ROK', 'COL', 'ROP', 'ROST', 'RCL', 'R', 'SPGI', 'CRM', 'SCG', 'SLB', 'SNI', 'STX', 'SEE', 'SRE', 'SHW', 'SIG', 'SPG', 'SWKS', 'SLG', 'SNA', 'SO', 'LUV', 'SWN', 'SWK', 'SPLS', 'SBUX', 'STT', 'SRCL', 'SYK', 'STI', 'SYMC', 'SYF', 'SYY', 'TROW', 'TGT', 'TEL', 'TGNA', 'TDC', 'TSO', 'TXN', 'TXT', 'BK', 'CLX', 'COO', 'HSY', 'MOS', 'TRV', 'DIS', 'TMO', 'TIF', 'TWX', 'TJX', 'TMK', 'TSS', 'TSCO', 'TDG', 'RIG', 'TRIP', 'FOXA', 'FOX', 'TSN', 'USB', 'UDR', 'ULTA', 'UA', 'UAA', 'UNP', 'UAL', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'URBN', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'WM', 'WAT', 'WEC', 'WFC', 'HCN', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WFM', 'WMB', 'WLTW', 'WYN', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XL', 'XYL', 'YHOO', 'YUM', 'ZBH', 'ZION', 'ZTS'] $ for company in SandP:$     qb.AddEquity(company)
detroit_census2=detroit_census.drop("Fact Note", axis=1)$ detroit_census2=detroit_census2.drop("Value Note for Detroit city, Michigan", axis=1)
gene_count_df = chromosome_gene_count.to_frame(name='gene_count').reset_index()$ merged_df = gdf.merge(gene_count_df, on='seqid')$ merged_df
test_copy1.bot.value_counts()
tl_2050 = pd.read_csv('input/data/trans_2050_ls.csv', encoding='utf8', index_col=0)
ab_df_new['treatment_US'] = ab_df_new.ab_page * ab_df_new.US$ ab_df_new['treatment_CA'] = ab_df_new.ab_page * ab_df_new.CA$ ab_df_new.head()
psy_df5 = HAMD.merge(psy_df4, on='subjectkey', how='right') # I want to keep all Ss from psy_df$ psy_df5.shape
words_only_scrape_freq = FreqDist(words_only_scrape)$ print('The 20 most frequent terms (terms only): ', words_only_scrape_freq.most_common(20))
stat_info_merge = pd.concat([stat_info[1], stat_info_st[[0,1]]], axis=1)
for index, row in susp.iterrows():$     print(row.link)
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])$ df_new.head()
display(data[data['age'] == data['age'].min()])$ display(data[data['age'] == data['age'].max()])
df_dummies = pd.get_dummies(df_value, columns=['value'])$ df_dummies.head()
intervention_train.isnull().sum()
filename1 = 'expr_3_nmax_32_nth_0.1_ns_0.01_04-19.csv'$ df = pd.read_csv('../output/data/expr_3/' + filename1, comment='#')$
model.score(combined_transformed_array, y_test)
for row in cursor.columns(table='TBL_FCBridge'):$     print(row.column_name)
results['HydrologicEvent'].unique()
target_column = 'DGS30'$ col_list.remove(target_column)
tree_features_df[~(tree_features_df['p_hash'].isin(manager.image_df['p_hash']) | tree_features_df['filename'].isin(manager.image_df['filename']))]
!hdfs dfs -put Consumer_Complaints.csv Consumer_Complaints.csv
state = environment.reset()$ state, reward, done=environment.execute(env.action_space.sample())$ state.shape
print(f"lowest_temp = {min(tobs_data)} \$         highest_temp = {max(tobs_data)}\$         avg_temp_of_most_active_station = {np.mean(tobs_data)}")
experience.columns = ['RATE_'+str(col) for col in experience.columns]
fin_p.dropna(inplace=True)
sns.factorplot('pclass', data=titanic3, hue='sex', kind='count')
typesub2017['Solar'].sum() 
tweets['hour']= dts.dt.hour$ tweets['date']= dts.dt.date$ tweets
sns.regplot(x="totqlesq", y="y", data=psy_native).set_title("Quality of Life Enjoyment and Satisfaction")$
df.dropna()
AFX_X_2017 = r.json()
print([X.shape[0],y.shape[0]])
t1.tweet_id =t1.tweet_id.astype(str)$
df = quandl.get('WIKI/GOOGL')
portfolio_df.reset_index(inplace=True)$ adj_close_acq_date = pd.merge(adj_close, portfolio_df, on='Ticker')$ adj_close_acq_date.head()
set(tcga_target_gtex_labels.disease).intersection(treehouse_labels_pruned.disease)
tweetsIn22Mar.head()$ tweetsIn1Apr.head()$ tweetsIn2Apr.head()
my_model_q2 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_rf, training='label')$ cross_validation.cross_val_score(my_model_q2, X_test, y_test, cv=10, scoring='accuracy')
pokemon.drop(["id"],inplace=True,axis=1)$ pokemon.drop(["Type 2"],inplace=True,axis=1)$ pokemon.head()
reddit.to_csv('data_reddit.csv')
import os$ files = os.listdir("/data/measurements")$ print('\n'.join(files))
import matplotlib as mpl$ mpl.get_backend()
import os$ print(os.listdir())
md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128)$
writer=pd.ExcelWriter('output.xlsx')$ kk.to_excel(writer,'Sheet1')$ writer.save()
df['start date'] = df.index.map(lambda x:x.start_time)$ df
twitter_data = pd.DataFrame(list(twitter_coll_reference.find()))$ twitter_data.head()
g8_groups.agg(['mean', 'std'])
tweet_pickle_path = r'data/twitter_01_20_17_to_3-2-18.pickle'$ tweet_data.to_pickle(tweet_pickle_path)
all_tables_df.OBJECT_NAME
dates = pd.date_range('20180114', periods=7)$ dates
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY+'&limit=1')
master_df['day of week']=master_df['day of week'].astype(str)
plt.style.use('default')$ %pprint
eval_data = pd.concat([predictions, actuals])$ eval_data['day'] = eval_data.index
(act_diff < p_diffs).mean()
import pandas as pd$ dataset = pd.ExcelFile("basedados.xlsx")$ data = dataset.parse(0)
client = pymongo.MongoClient()$ tweets = client['twitter']['tweets'].find()$ latest_tweet = tweets[200]
reddit.Comments.value_counts(ascending=False).head(25) #just seeing the distribution of the number of comments$
assert mcap_mat.shape[0] < 100
oz_stops = pd.read_csv('../../Data/all_bus_stops.csv', sep=',')
num_names = df.shape[0]$ print ('Number of names in the training dataset', num_names)
df['reviw'] = df['review'].apply(preprocessor)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY)$
print(re.match('AA', 'AAbc'))$ print(re.match('AA', 'bcAA'))
import tensorflow as tf$ from tensorflow.contrib.tensorboard.plugins import projector
building_pa_prc.describe(include='all')
from statsmodels.tsa.arima_model import ARIMA$ model_6203 = ARIMA(dta_6208, (2, 1, 2)).fit() $ model_6203.forecast(5)[:1] 
least = pd.DataFrame(data.groupby('tasker_id').hired.sum())$ least.loc[least['hired']==0]
os.environ.get('FOO')
malebydatenew  = malebydate[['Sex','Offense']].copy()$ malebydatenew.head(3)
stock_dict = {}$ for data in r.json()['dataset']['data']:$     stock_dict[data[0]] = dict(zip(r.json()['dataset']['column_names'][1:], data[1:]))
S_distributedTopmodel.decision_obj.groundwatr.options, S_distributedTopmodel.decision_obj.groundwatr.value
coarse_groups = mgxs.EnergyGroups([0., 0.625, 20.0e6])$ fine_groups = mgxs.EnergyGroups([0., 0.058, 0.14, 0.28,$                                  0.625, 4.0, 5.53e3, 821.0e3, 20.0e6])
GIT_LOCATION = \$ 'C:\\Users\\donrc\\AppData\\Local\\GitHub\\PortableGit_f02737a78695063deace08e96d5042710d3e32db\\cmd\\git.exe'$
datAll['year'] = datAll['Date'].map(lambda x: x.year)$ datAll['month'] = datAll['Date'].map(lambda x: x.month)$
new_stops.loc[new_stops['stopid'] == '7567']
df.groupby('category')['position','hourly_rate','num_completed_tasks'].agg({'median','mean','min','max'}).reset_index() \$     .rename(columns={'category': 'category', 'position': 'position_stats_overall','hourly_rate':'hourly_rate_stats_overall', \$                  'num_completed_tasks':'num_completed_tasks_stats_overall'})
d1.sum() # reduces over columns$
top_10_authors = git_log['author'].value_counts().iloc[:10]$ print(top_10_authors)
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv',$                  parse_dates=['deadline', 'launched'],$                  encoding = "ISO-8859-1")
x_test = np.array(data)
rfc.fit(features_class_norm, overdue_transf)
go_no_go = 1 #NO_GO - ie do NOT run all the long run-time items.  Note that some of these long$
url ='https://graph.facebook.com/v2.6/'+str(x[1])+'/posts/?fields=id,comments.limit(0).summary(true),shares.limit(0).summary(true),likes.limit(0).summary(true),reactions.limit(0).summary(true)'
MetaMetaclass.__call__(MetaClass,'Example', (), {})$
tips.sort_values(["tip","size"],ascending=False).head(10)
pax_raw = pax_raw.merge(keep_days, on=['seqn', 'paxday'], how='inner')
%matplotlib inline$ AAPL.plot()
from arcgis.gis import GIS$ from IPython.display import display$ gis = GIS() # anonymous connection to www.arcgis.com
merge_df = pd.merge(customers_df, responses_df,left_on='RESPONSE_ID', right_on='ID')$ print(len(merge_df))$ merge_df.head()
miss = df.isnull().values.any()$ print ("Missing Values : {}".format(miss))
logistic_file = '../data/model_data/log_pred_mod.sav'$ pickle.dump(logreg, open(logistic_file, 'wb'))
mean = np.mean(data['len'])$ print("The average tweet length is: {}".format(mean))
df_new.head()
contractor[contractor.duplicated() == True]
psy_prepro = psy$ psy_prepro.to_csv("psy_prepro.csv")
def join_df(left, right, left_on, right_on=None, suffix="_y"):$   if right_on is None: right_on=left_on$   return left.merge(right, left_on=left_on, right_on=right_on, suffixes=("",suffix))
print("{:.2f} GB".format(df.fileSizeMB.sum() / 1024))
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ print(last_date)$
pd.options.display.max_colwidth = 100$ data_df[data_df.nwords == 1]['clean_desc'].head(15)
result_3 = pd.concat([df1, df3], ignore_index = True) # same as option 1 but with reset index $ result_3
challange_1.shape
predictSVR = svr_rbf.predict(np.concatenate((X_train,X_validation,X_test),axis=0))$ predictANN = ANNModel.predict(np.concatenate((X_train,X_validation,X_test),axis=0))#one can ignore this line if ANN is $
top_10_authors = git_log.loc[:, 'author'].dropna().value_counts().head(10)$ top_10_authors
groupby_x = df['y'].groupby(df['x'])$ round(groupby_x.describe(), 3)
year3 = driver.find_elements_by_class_name('yr-button')[2]$ year3.click()
results=session.query(Measurements.Station,Measurements.Date,Measurements.Tobs).\$         filter(Measurements.Date>=start_date, Measurements.Date<=end_date, Measurements.Station == station_mostobs).\$         order_by(Measurements.Date.desc()).all()
typesub2017['MTU2'] = typesub2017.MTU.str[:16]$ typesub2017['DateTime'] = pd.to_datetime(typesub2017['MTU2'])
iso_gdf_2.plot();
subwaydf['DESC'].value_counts()
weather_norm = weather_features.apply(lambda c: 0.5 * (c - c.mean()) / c.std())
number_of_commits = len(git_log)$ number_of_authors = len(git_log["author"].dropna().unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
calls_nocontact.neighborhood_district.value_counts()
sentiment_df=pd.DataFrame(sentiments)$ sentiment_df.head()
serc_pixel_df = pd.DataFrame()
net.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=directory_name)
Base = automap_base()$ Base.prepare(engine, reflect=True)$ hawaii = Base.classes.measurement
temp_df2['titles'] = temp_df2['titles'].str.lower()
df_tweets = pd.DataFrame(tweets)$ df_tweets
x_normalized = pd.DataFrame(x_scaled)
pd.read_csv("data.csv", index_col=[0, 1, 2, 3, 4, 5], skipinitialspace=True, parse_dates=['Date']).head(3)
data.dtypes
(data_2017_12_14[data_2017_12_14['text'].str.contains("instantaneamente", case = False)])["text"].head()
companies = ["WIKI/ATVI.11","WIKI/ADBE.11","WIKI/AKAM.11","WIKI/ALXN.11","WIKI/GOOGL.11","WIKI/AMZN.11","WIKI/AAL.11","WIKI/AMGN.11","WIKI/ADI.11","WIKI/AAPL.11","WIKI/AMAT.11","WIKI/ADSK.11","WIKI/ADP.11","WIKI/BIDU.11","WIKI/BIIB.11","WIKI/BMRN.11","WIKI/CA.11","WIKI/CELG.11","WIKI/CERN.11","WIKI/CHKP.11","WIKI/CTAS.11","WIKI/CSCO.11","WIKI/CTXS.11","WIKI/CTSH.11","WIKI/CMCSA.11","WIKI/COST.11","WIKI/CSX.11","WIKI/XRAY.11","WIKI/DISCA.11","WIKI/DISH.11","WIKI/DLTR.11","WIKI/EBAY.11","WIKI/EA.11","WIKI/EXPE.11","WIKI/ESRX.11","WIKI/FAST.11","WIKI/FISV.11","WIKI/GILD.11","WIKI/HAS.11","WIKI/HSIC.11","WIKI/HOLX.11","WIKI/IDXX.11","WIKI/ILMN.11","WIKI/INCY.11","WIKI/INTC.11","WIKI/INTU.11","WIKI/ISRG.11","WIKI/JBHT.11","WIKI/KLAC.11","WIKI/LRCX.11","WIKI/LBTYA.11","WIKI/MAR.11","WIKI/MAT.11","WIKI/MXIM.11","WIKI/MCHP.11","WIKI/MU.11","WIKI/MDLZ.11","WIKI/MSFT.11","WIKI/MNST.11","WIKI/MYL.11","WIKI/NFLX.11","WIKI/NVDA.11","WIKI/ORLY.11","WIKI/PCAR.11","WIKI/PAYX.11","WIKI/PCLN.11","WIKI/QCOM.11","WIKI/REGN.11","WIKI/ROST.11","WIKI/STX.11","WIKI/SIRI.11","WIKI/SWKS.11","WIKI/SBUX.11","WIKI/SYMC.11","WIKI/TSLA.11","WIKI/TXN.11","WIKI/TSCO.11","WIKI/TMUS.11","WIKI/FOX.11","WIKI/ULTA.11","WIKI/VRSK.11","WIKI/VRTX.11","WIKI/VIAB.11","WIKI/VOD.11","WIKI/WBA.11","WIKI/WDC.11","WIKI/WYNN.11","WIKI/XLNX.11"]#"WIKI/PCLN.11",
open('test_data//open_close_test.txt', encoding='utf8')
a = np.arange(25).reshape(5,5) ; a$
import json$ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=apikey')$
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comment', ''))
feeds = json.loads(response)$ print(type(feeds))$ print(feeds)
cnf_matrix[1:].sum() / cnf_matrix.sum()
new_comments_df = pd.read_csv('input/test.csv') # Replace 'test.csv' with your dataset$ X_test = test["comment_text"].str.lower() # Replace "comment_text" with the label of the column containing your comments
data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/0ddd2680-e35d-46bc-ac1a-d350da4f409d/ar24011.asc'
df_search_cate = df_session_dummies[df_session_dummies.action_search_cate == 1 ]$ df_search_cate = df_search_cate.dropna()$ df_search_cate_dummies = pd.get_dummies(df_search_cate, columns=['value'])$
events_enriched_df['topic_name'].drop_duplicates().count()
df = pd.DataFrame(daily_norm, columns = ["date","min_temp","avg_temp","max_temp"])$ df["date"] = pd.to_datetime(df["date"]).dt.date$ df.set_index(["date"], inplace = True)
mp2013 = pd.period_range('1/1/2013','12/31/2013',freq='M')$ mp2013
afx_x_2017 = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY)$ type(afx_x_2017)
df1=df1[['Adj. Close','volatility','PCT_Change','Adj. Open','Adj. Volume']]$ df1.head()
zf = zipfile.ZipFile(path)$ df = pd.read_excel(zf.open('Sample_Superstore_Sales.xlsx'))$ df.head(5)
show_data.plot(kind="bar")
delimited_hourly = delimited_twitter_df.groupby([pd.Grouper(freq="H"), 'company']).count()['text'].to_frame()$ delimited_hourly.columns = ['Number_of_Tweets']$
lr_pipe.fit(X_train, y_train)$ lr_pipe.score(X_test, y_test)
url = "https://bittrex.com/api/v1.1/public/getmarketsummaries"$ r = requests.post(url)  $ data = json.loads(r.content.decode())$
import pandas as pd$ with pd.option_context("max.rows", 10):$     print(dta.results.value_counts())
new_df = df.fillna(method = 'bfill')$ new_df
print activity_df.loc['2018-05-08']
auth = tweepy.OAuthHandler(consumer_key,consumer_secret)$ auth.set_access_token(access_token,access_token_secret)$ api = tweepy.API(auth,parser=tweepy.parsers.JSONParser())
categories = ['event','eventClassification']#,'companyInvolved','operationOrDevelopment','jobTypeObserved','stopJob','immediateActionsTaken','rigInvolved']$ BUMatrix = pd.get_dummies(df_trimmed,columns = categories)
adj_glm = smf.glm('hospital_expire_flag ~ C(inday_icu_wkd) + C(admission_type)', $                      data=data, family=sm.families.Binomial()).fit()$ adj_glm.summary2()$
reg_target_encoding(train,'Block',"any_spot") $ reg_target_encoding(train,'DOW',"Real.Spots") $ reg_target_encoding(train,'hour',"Real.Spots") $
initial_trend(series, 12)$ initial_seasonal_components(series, 12)$ triple_exponential_smoothing(series, 12, 0.716, 0.029, 0.993, 24)$
df_new['country'].value_counts()
min_lat = weather_df["Lat"].min()//10*10$ max_lat = weather_df["Lat"].max()//10*10+15$ tick_locations = np.arange(min_lat -10, max_lat +10, 10)
pd.options.display.max_colwidth = 200$ data_df[['ticket_id','type','clean_desc','nwords']].head(30)
dfSummary = pd.concat([summary_all, summary_bystatus],axis=1)$ dfSummary.columns = ("1930-2017","1930-1980","1984-2017")$ dfSummary
a = np.arange(25).reshape(5,5)$
checking.iloc[z,0] = np.nan$ checking['age'] = checking['age'].fillna(checking['age'].mean())
target_city = {"lat": 35.227724, "lng": -80.839699} $ target_coords = f"{target_city['lat']},{target_city['lng']}"$
session = tf.Session()$ session.run(tf.global_variables_initializer())
df2.drop(df2.index[2893])$ df2.user_id.duplicated().sum()
raw_data.head()
    if isinstance(obj, (datetime, date)):$         return obj.isoformat()$     raise TypeError ("Type %s not serializable" % type(obj))
logger.info('Define Source')$ data = pd.DataFrame.from_csv('~\\neo.csv')$ logger.debug('df: %s', data)
endometrium_data = pd.read_csv('data/small_Endometrium_Uterus.csv', sep=",")  # load data$ endometrium_data.head(n=5)  # adjust n to view more data
pd.read_sql(q, connection) # Avergae working hours of Private Sector Men
rows=session.query(Adultdb).filter_by(native_country='Outlying-US(Guam-USVI-etc)')
list(set(df.CustomerID[pd.isnull(df.State)]))$ df[pd.isnull(df.State)].head()
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
yc_new2 = yc_new1[['Fare_Amt', 'tripDurationHours', 'Trip_Distance',$        'Tip_Amt', 'income_departure', 'income_dest']]$ yc_new2.head()
roi = pd.DataFrame({'roi_0': roi_on_day(0, users_costs_df, orders_df)})$
tmp = df[selected_features].join(outcome_scaled).reset_index().set_index('date')$ tmp.dropna().resample('Q').apply(lambda x: x.corr()).iloc[:,-1].unstack().iloc[:,:-1].plot()$
input_shape_param = (X_train.shape[1],X_train.shape[2])$ input_shape_param
print('Scanning for all greetings:')$ for key, row in table.scan():$     print('\t{}: {}'.format(key, row[column_name.encode('utf-8')]))$
df_by_donor = donations[['Donor ID','Donation ID', 'Donation Amount', 'Donation Received Date']].groupby('Donor ID', as_index=False).agg({'Donation ID': 'count', 'Donation Received Date': 'max', 'Donation Amount': ['min', 'max', 'mean', 'sum']})
data['Subjectivity'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
from sklearn.feature_extraction.text import TfidfVectorizer$ from sklearn.cluster import KMeans$ from sklearn.metrics import silhouette_samples, silhouette_score
custom = pd.get_dummies(auto_new.Custom)$ custom.head()
exiftool -csv -createdate -modifydate cisnwf6/Cisnwf6_cycle3.MP4 > cisnwf6.csv
false_churn_bool = [np.any([USER_PLANS_df.loc[unchurn]['status'][i] !='canceled' for i,j in enumerate(USER_PLANS_df.loc[unchurn]['scns_created']) if j==max(USER_PLANS_df.loc[unchurn]['scns_created'])]) for unchurn in churned_ix] 
sp_500_adj_close = sp500[['Adj Close']].reset_index()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=a2rusGHVqV67tgCdY38x')$ r.headers['content-type']  #Type of data format queried for.  In this case, json. $ json_string=r.text  #Convert object to text so that it can be converted to a dictionary.
dpth = os.getcwd()$ dbname_sqlite = "ODM2_Example2.sqlite"$ sqlite_pth = os.path.join(dpth, os.path.pardir, "data/expectedoutput", dbname_sqlite)
mask = (df['message'].str.len() > 3) #Remove cases with message length < 3.$ df = df.loc[mask]
daily_mcap_mat.resample('M').asfreq().index
bild = bild[bild.message != "NaN"]$ spon = spon[spon.message != "NaN"]
df3 = df3.reset_index(drop=True)$ df_list = df_list.reset_index(drop=True)$ combine = (df_list.join(df3))
engine = create_engine('sqlite:///hawaii.sqlite')
data = data.dropna(subset=['name'], how='any')$ data.info()
cdata.loc[13].Number_TD
print('Total records {}'.format(len(non_na_df)))$ print('Start / End : {}, {}'.format(non_na_df.index.min(), non_na_df.index.max()))
spyder_etf = get_pricing(dwld_key, start_date.strftime(date_fmt))$ spyder_etf.name = dwld_key + ' ETF Index'$ s_etf = (spyder_etf.pct_change() + 1).cumprod()
rf.score(X_train, y_train)
df['x'] = df.index$ df.plot.scatter(x='x', y='y')
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
sat_5am = pendulum.datetime(2017, 11, 25, 5, 30, 0, tzinfo='US/Eastern')$ sat_6am = pendulum.datetime(2017, 11, 25, 6, 0, 0, tzinfo='US/Eastern')$ sat_spike = tweets[(tweets['time_eastern'] >= sat_5am) & (tweets['time_eastern'] <= sat_6am)]
start = time.time()$ print(list(map(sum_prime, [300000, 600000, 900000])))$ print("Time taken = {0:.5f}".format(time.time() - start))
measurements_year = session.query(Measurements.date,Measurements.prcp).limit(10).all()$ for mammal in measurements_year:$     print(mammal)
gdax_trans['Timestamp'] = pd.to_datetime(gdax_trans['Timestamp'], format="%d/%m/%Y %H:%M:%S")
print(airquality_pivot.head())
recommendation_sets = len(sample.index)$ recommendation_sets
listings.loc[0]$
pd.unique(tag_df.values.ravel())
adj_close.head()
adf_check(dfs['Seasonal Difference'].dropna())
databreach_2017 = databreach_2017.dropna(axis=0,how='any')
sorted_measurements_df.to_excel(writer, '{}'.format(settings['ExposureTime']))$ writer.save()
s4.value_counts()
import pandas as pd$ git_log['timestamp']=pd.to_datetime(git_log['timestamp'],unit='s')$ git_log.timestamp.describe()$
p_val = (p_diffs > act_diff).mean()$ print("Proportion greater than actual difference: %.4f" %p_val)
tzs = DataSet['userTimezone'].value_counts()[:10]$ print(tzs)
import numpy as np$ step_counts = step_counts.astype(np.float)$ print step_counts.dtypes
sub_mean = df.groupby('subreddit').agg({'num_comments': 'mean'})$ top_com = sub_mean.sort_values('num_comments', ascending = False).head()$ top_com
!cd .. && python -m scripts.retrain -h
md_keys = ['   ' + s + ':' for s in METADATA_KEYS]$ md_idx = tmpdf_md.index[tmpdf_md[tmpdf_md.isin(md_keys)].notnull().any(axis=1)].tolist()$ md_idx
df2 = df2.drop_duplicates(['user_id'], keep='first')$ df2.info()
Quandl_DF.info()$ Quandl_DF.tail(5)
df_providers.head()$ idx = df_providers[ (df_providers['id_num']==50030)].index.tolist()$ df_providers.loc[idx[:3],:]$
temps_df[temps_df.Missoula > 82]
today = datetime(2014,11,30)$ tomorrow = today + pd.Timedelta(days=1)$ tomorrow
reduced = merged_data.loc[merged_data['time_sec'].notnull()]$ compacted = reduced[['new_time','drone_rtk_lat','drone_rtk_lon','drone_rtk_alt','drone_lat','drone_lon','rel_alt','evnt']]$
grid.grid_scores_
grouped_publications_by_author['authorId'].nunique()
p_mean = np.mean([p_new, p_old])$ print("Probability of conversion udner null hypothesis (p_mean):", p_mean)
import pandas as pd$ reviews=pd.read_csv("ign.csv")$ reviews.head()
pd.DataFrame(dummy_var["_Source"][Company_Name]['High']['Forecast'])[-6:]
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall()$
train.info()
merged_table = merged_table.drop(columns=['host', 'last_update', $                                           'Unnamed: 0', 'queue', 'date', $                                           'weekday', 'strtime'])
for urlTuple in otherPAgeURLS[:3]:$     contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ contentParagraphsDF
Station = Base.classes.station$ Measurements = Base.classes.measurements
model.wv.most_similar("crot")
dr_num_new_patients = dr_num_new_patients.astype('float')$ dr_num_existing_patients = dr_num_existing_patients.astype('float')
x,y=X[0:train],Y[0:train]$ print (x.shape,y.shape)$ model.fit(x,y,epochs=150,batch_size=10,shuffle=True)
np.where(y > 10)
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
master_df=pd.read_csv("CitiBike_Data_for_Machine1.csv")$ master_df.head()
articles['tokens'] = articles['tokens'].map(lambda s: [w for w in s if len(w)>1])
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
df_vow.plot()
dfs[25].describe()
open_price = [x[1] for x in data2 if x[1]!=None]  # needs to exclude None. $ print('The highest opening price during 2017 is : ',max(open_price))$ print('The lowest opening price during 2017 is : ',min(open_price))
df_concat["date_series"] = pd.to_datetime(df_concat["created_time"])$ df_concat["date_series"].head()
df_variables.to_csv("../01_data preprocessing/data new/variables.csv", encoding="utf-8", sep=",", index=False)
print(data.head(10))
seed = 2210$ (train_sample, validation) = modeling2.randomSplit([0.9,0.1], seed=seed)
donors.loc[donors['Donor Zip'] == 606 , 'Donor City'].value_counts()
mergde_data=pd.merge(game_meta,labels_group , right_index=True, left_index=True)$ merge_val=pd.merge(game_meta,game_vail,right_index=True, left_index=True)$
Daily_Price.tail()$
print(type(r.json()))$ json_dict = r.json()$
jevent_scores = jcomplete_profile['event_scores']$ esdf = pd.DataFrame.from_dict(jevent_scores)$ print(esdf[['event_type_id','model_scope_forecast_horizon','effective_date', 'score_value']])
releases.head()
userMovies = userMovies.reset_index(drop=True)$ userGenreTable = userMovies.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ userGenreTable
pd.set_option('display.mpl_style', 'default')
collection_reference.count_documents({})$
data_compare['SA_textblob_de'] = np.array([ analize_sentiment_german(tweet) for tweet in data_compare['tweets_original'] ])$ data_compare['SA_google_translate'] = np.array([ analize_sentiment(tweet) for tweet in data_compare['tweets_translated'] ])
print(df.shape)
        select *$         from public.bookings b$         where b.CREATED_AT BETWEEN '2018-08-01' AND '2018-08-10' $
pd.value_counts(RNPA_existing['ReasonForVisitName'])
df['log_price']=np.log(df['price_doc'].values)
df.Notes = df.Notes.apply(lambda x: x.replace('-',' '))
f = open('datasets/git_log_excerpt.csv', 'r')$ f.read()
store_items = store_items.rename(index = {'store 2': 'last store'})$ store_items
twelve_months_prcp.head()
df = pd.read_csv('~/mids/w266/w266_final_project/Combined_Comments.csv', delimiter=',')
pm_data.dropna(inplace=True)$ pm_data.status.value_counts()
ctc = ctc.fillna(0)
pbptweets = pbptweets.drop_duplicates(subset='text', keep='first')
import tensorflow as tf$ x1 = tf.constant(5)$ x2 = tf.constant(6)
day_change = [(daily[2]-daily[3]) for daily in afx_17['dataset']['data']]$ print('Largest change in 1 day based on High and Low price is $%.2f.' % max(day_change))
LR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)$ yhat_prob2 = LR2.predict_proba(X_test)$ print ("LogLoss: : %.2f" % log_loss(y_test, yhat_prob2))$
f.index = [1,3,2,1,5] ; f
import matplotlib.pyplot as plt$ import seaborn as sns$ %matplotlib inline
print(y_test.mean(), y_train.mean())
df_lineup = df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')") $ print("{} times the new_page and treatment don't line up.".format((df_lineup.shape[0]) ))$
CON=CON.drop_duplicates(['Contact_ID'], keep='first')
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
merged_visits = visited.merge(dta)$ merged_visits.head()
tweets_df = tweets_df[tweets_df['location'] != ''].reset_index(drop=True) # reset index from 0$ tweets_df = tweets_df.sort_values('timestamp')$ print('got locations for {} retweets'.format(len(tweets_df)))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-05-15&api_key=' + API_KEY)$ r.json()
df_2002['bank_name'] = df_2002.bank_name.str.split(",").str[0]$
df.reset_index().to_csv('final.csv',header=True, index=False)
print (df.dtypes[df.dtypes == 'object'])$
ab_df2 = ab_df2.drop(labels=2893)
!wget https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt$ !head -n 5 ProductPurchaseData.txt
store_items.dropna(axis = 1)
df_2014['bank_name'] = df_2014.bank_name.str.split(",").str[0]$
info_final = pd.merge(edu_gen_edad, visitas, on = 'idpostulante', how = 'inner')$ info_final = pd.merge(info_final, postulaciones, on = 'idpostulante', how = 'inner')$ info_final.head()
from pyspark.sql.functions import isnan, when, count, col$ ibm_hr.select([count(when(isnan(c), c)).alias(c) for c in ibm_hr.columns]).show()
tree = DecisionTreeClassifier(criterion='gini')$ model = tree.fit(X_train_total, y_train)$ model.score(X_test_total_checked, y_test)
data.recommendation_id.nunique()
window = pdf.loc['2008-1-1':'2009-3-31']$ portfolio_metrics(window)$ window.plot();$
bar_plot = sns.barplot(x="News Organization", y="Mean Compound Score", hue="News Organization", $                        data=aggrigate_results_df, palette=news_colors)$ bar_plot.axes.set_title(f'Overall Media Sentiment, date: {today}', fontsize=14)$
new_stops = merged_stops[['stopid', 'stop_name', 'lat', 'lng', 'routes']]$ new_stops = new_stops.rename(columns={'stop_name': 'address'})$ new_stops.head(5)
logit_countries = sm.Logit(df4['converted'], $                            df4[['country_UK', 'country_US', 'intercept']])$ result2 = logit_countries.fit()
crimes.drop('LOCATION', axis=1, inplace=True)
new_table=original_merged_table.groupby(["city","type","driver_count"]).size().reset_index().rename(columns={0:"Number of ride"})$ new_table.head()
td = td.fillna(0)
gbm.predict(test)
df.plot();
response_df[response_df.isnull().any(axis=1)]
cohort_activated_df = pd.DataFrame(index=daterange,columns=daterange)
grouped_df = news_sentiment_df.groupby('News_Source')$ grouped_compound = grouped_df['compound'].mean()$ grouped_compound
data['Open'].min()
deployment_details = client.deployments.create(model_uid, 'Retail_Churn_with_XGBoost')
postags = sorted(set([pos for sent in train_trees for (word, pos) in sent.leaves()]))$ print(unigram_chunker.tagger.tag(postags))
dummy = pd.get_dummies(furniture, columns=['category'],drop_first=True)
type(ts.index[0])
clf.fit(digits.data[:-2], digits.target[:-2])
df.to_csv('citation-data-clean.csv', index=False)$
cleanedData[cleanedData['text'].str.contains("&amp;")].text
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd.head()
import pandas as pd$ df = pd.DataFrame(dataset)
import builtins$ builtins.uclresearch_topic = 'NYC'$ from configuration import config
questions = pd.concat([questions.drop('coming_next_reason', axis=1), coming_next_reason], axis=1)
df3['timestamp'] = df3.timestamp.apply(lambda x: pd.datetime.strptime(x, DATETIME_FMT))$
strs = 'NOTE: This event is EVERY FRIDAY!! Signup is a'$ result = re.split(r'[^0-9A-Za-z]+',strs)$ print(result)
results = pp.get_results()
all_tweets = df_Tesla['text'].values$ print(all_tweets[2])
model_knn_vector = KNeighborsClassifier(n_neighbors=250)$ model_knn_vector = model_knn_vector.fit(train_arrays, train_labels)$ model_knn_vector.score(test_arrays, test_labels)
y_pred = fit5.predict(X_test)
ldf.Close.diff().max()
k = 4$ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)$ neigh
intervention_train['MILLESIME'] = intervention_train['MILLESIME'].astype(int)$ intervention_train['RESOURCE_ID'] = intervention_train['RESOURCE_ID'].astype(int)
%%time$ [upload_blob(bucket_name, source_file, source_file.replace('\\', '/')) for source_file in filepathlist]
CON = CON.rename(columns={  'Contact ID (18-digit)':'Contact_ID', 'Opportunity Name':'OppName', $        'Term: Term Name':'Term', 'Opportunity Record Type':'Record_Type', 'Inquiry':'Inquiry', 'Inquiry Date':'Inquiry_Date',$        'Opportunity ID (18-digit)':'Opp_ID', 'Empl ID':'EMPL', 'Application Number':'App_Number'})
ab_df2.user_id.nunique()
weather = pd.read_csv("weather.csv", parse_dates={"date" : ['Date']})$ weather = weather.drop(['HDD', 'CDD'], axis=1)
df_students.head()
set(list(building_pa_specs['Column name'].values))==set(list(building_pa.columns))
df['up_votes'].quantile(q=.50)$ df.loc[df['up_votes'] == 192674]['title'].values
cols = vip_df.columns.tolist()$ cols = cols[-1:] + cols[:-1]$ vip_df = vip_df[cols]
nnew = df2.query('landing_page == "new_page"').count()[0]$ print ("The population of Newpage is : {}".format(nnew))
json_data = r.json()$ json_limit = r_limit.json()$ pp.pprint(json_limit)
rng=pd.period_range('1/1/2000', '6/30/2000', freq='M')$ rng
INSERT INTO payment (booking_id, payment_date, payment_method, payment_amount)$ VALUES (8, TO_DATE('2017-09-10', 'YYYY-MM-DD'), 'BPay', -741.96)$
summary_all = df['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9])$ summary_all
import pip$ pip.main(['install','quandl'])
data['SA'] = np.array([ analyseSentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
s = pd.Series([99,5,60], index = ['HPI','Int_rate','Low_tier_HPI'])$ df1.append(s,ignore_index=True) # append the series to the data frame. The line 4 is appended$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
full_act_data = steps.join(heart, how='left')
new_page_converted = np.random.binomial(1,P_new,n_new)$ new_page_converted
df.reset_index(inplace=True)$ df.head()
!wget https://data-ppf.github.io/labs/lab4/Residuals.jpeg$ !wget https://data-ppf.github.io/labs/lab4/Star.obs.jpeg
for g in my_gempro.genes:$     for s in g.protein.structures:$         g.protein.align_seqprop_to_structprop(seqprop=g.protein.representative_sequence, structprop=s)
info_df.plot(kind='area', stacked=False, alpha=0.5, colormap='Spectral')$ plt.show()
df = pd.read_csv('SHARE_cleaned_lists.csv')
!convert materials-xy.ppm materials-xy.png$ Image(filename='materials-xy.png')
ctd_df.head(10)
f_regex = re.compile('([^a-z]|^|$)f([^a-z]|^|$)')$ m_regex = re.compile('([^a-z]|^|$)m([^a-z]|^|$)')$ num_regex = re.compile('[0-9]+')
plt.xlim(0, 1.0)$ _ = plt.barh(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.values()), align='center')$ _ = plt.yticks(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.keys()))
data = requests.get ("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key="+API_KEY ).json()
row_idx = (slice(None), slice(None), slice('Bob', 'Guido'))  # all years, all visits, Bob + Guido$ health_data_row.loc[row_idx, 'HR']
import builtins$ builtins.uclresearch_topic = 'GIVENCHY'$ from configuration import config
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
frames = [data_BBC, data_CBS, data_CNN, data_FoxNews, data_nytimes]$ big_data = pd.concat(frames)$ big_data.to_csv('SentimentAnalysisData.csv')
pred.head(10)$
item = collection.item('AAPL')$ item
parameter = '00060'$ Shoal_Ck_15min = pull_nwis_data(parameter=parameter, site_number='08156800', start_date='2018-02-01', end_date='2018-02-18', site_name='08156800')$ Shoal_Ck_15min.head(10)$
import tweepy$ import pandas as pd$ import matplotlib.pyplot as plt
all_tables_df.iloc[2:4, 1:]
result_2 = pd.concat([df1, df3], axis = 1, join_axes=[df1.index]) # concatenate one dataframe on another along columns$ result_2
tweet_frame = toDataFrame(results)$ tweet_frame = tweet_frame.sort_values(by='tweetRetweetCt', ascending=0)$ print(tweet_frame.shape)
store_items = store_items.drop(['watches', 'shoes'], axis=1)$ store_items
result_set=session.query(Adultdb).filter_by(relationship='Not-in-family').all()
fire_pred = log_mod.predict_proba(climate_vars)
print(type(data.points))$ data[['points']]$
merged = pd.DataFrame.merge(mojog_df, youtube_df,on='movie_name', how = 'inner')$ merged.head()
print (r.json())
fe.plots.plotqq(ss)
word_centroid_map = dict(zip( model.wv.index2word, idx ))
len(df.index)$
df_R['Month'] = df_R['Date'].apply(lambda s:s.split('-')[1])
donors[donors['Donor Zip'] == 606 ].head()
df_2013['bank_name'] = df_2013.bank_name.str.split(",").str[0]$
z = indx[indx == False].index.tolist()$ scratch['created_at'].iloc[z]
print(rhum_long_df['date'].min(), rhum_long_df['date'].max())
from gensim import corpora, models$ dictionary = corpora.Dictionary(sws_removed_all_tweets)
url_yr2017='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=MY_API_KEY&start_date=2017-01-01&end_date=2017-12-31'$ response=requests.get(url_yr2017)
combined_city_df = pd.merge(city_data_df, ride_data_df,$                                  how='outer', on='city')$ combined_city_df.head(5)
url = "http://space-facts.com/mars/"
np.random.seed(1) $ model = models.LdaMulticore(corpus, id2word=dictionary, num_topics=10, workers=5,passes=200, eval_every = 1)
df = pd.read_csv("FuelConsumption.csv")$ df.head()
qs = qs.merge(answers, how="left", left_on="Id", right_on="ParentId", suffixes=("", "_a"))$ print qs.head()
df.groupby("newsOutlet")["compound"].min()
from pyramid.arima import auto_arima
conn_SF.close()
exec(open ('../../config.py').read ()) # Load file where API keys are stored$ API_KEY = QUANDL_API_KEY
pd.to_datetime(segments.st_time[:])
LARGE_GRID.plot_accuracy(raw_large_grid_df, option='dodge')
tl_2020 = pd.read_csv('input/data/trans_2020_ls.csv', encoding='utf8', index_col=0)
tweets['SA'] = tweets['full_text'].apply(lambda x: analize_sentiment(x))
a.iloc[[3]]
template_df = pd.concat([X_valid, y_valid], axis=1)$ template_df['is_test'] = np.repeat(True, template_df.shape[0])
for i in range(len(df.columns)):$     df.rename(columns={i:df_header[i]}, inplace=True)
df.query('MeanFlow_cfs < 50')
%%html$ <img src = "https://data.globalchange.gov/assets/e5/ee/9329d76bf3f5298dad58d85887bd/cs_ten_indicators_of_a_warming_world_v6.png", width=700, height=700>
relevance_scores_df = pd.DataFrame(relevance_scores[0]).mean(axis=1)$ relevance_scores_df.describe()
public_tweets = api.home_timeline()$
sns.barplot(x=top_sub['id'], y=top_sub.index) # challenge: annotate values in the plot$ plt.xlabel("number of posts")$ plt.title("Top 5 active subreddits by # of posts");
np.random.seed(123)$ np.random.shuffle(raw_data)$
mRF = H2ORandomForestEstimator()$ mRF.train(['sepal_len','sepal_wid','petal_len','petal_wid'],'class',train)
df_subset.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70)$ plt.show()
dfFull['YearRemodAddNorm'] = dfFull.YearRemodAdd/dfFull.YearRemodAdd.max()$
mydata.plot(figsize =(15 ,6)) $ plt.show()
grpConfidence = df.groupby(['Confidence'])
with open('API_KEY.txt') as file:$     API_KEY = file.readline()
df2.converted.mean()
store_items.fillna(method='backfill', axis=0)
print(sum(receipts.duplicated(['parseUser','reportingStatus','iapWebOrderLineItemId','iapPurchaseDate'])))$ print(sum(receipts.duplicated(['iapWebOrderLineItemId'])))
chinese_vessels = chinese_vessels_wcpfc.copy()$ chinese_vessels = mergetable(chinese_vessels, chinese_vessels_iotc)
train_labels = one_hot_matrix(dflong[dflong['Date'].isin(train_dates)]['Y'].as_matrix(), 3) $ validation_labels = one_hot_matrix(dflong[dflong['Date'].isin(validation_dates)]['Y'].as_matrix(), 3)$ test_labels = one_hot_matrix(dflong[dflong['Date'].isin(test_dates)]['Y'].as_matrix(), 3)
officers = pd.read_csv('data/outputs/active_officers.csv')$ officers.company_number = officers.company_number.astype(str)
df2.to_csv("newstweets.csv",index=False)
indices = ml.show_feature_importance(df, '3M')
bg2 = pd.read_csv('Libre2018-01-03.txt') # when saved locally$ print(bg2)$ type(bg2) # at bottom we see it's a DataFrame$
rng = pd.date_range(end='2018-01-19', periods=200, freq='BM')$ type(rng)
mars_table = mars_df.to_html()$ print(mars_table)
loc = session.query(Stations.station_id).count()$ print("Total {} stations were used to gather data.".format(loc))
!cat ProductPurchaseData.txt | head
t1.tweet_id =t1.tweet_id.astype(str)
trump['hours'] = trump.index.hour$ trump['weekday'] = trump.index.weekday_name
del merged_portfolio_sp_latest['Date']$ merged_portfolio_sp_latest.rename(columns={'Adj Close': 'SP 500 Latest Close'}, inplace=True)$ merged_portfolio_sp_latest.head()
for key, value in sample_dic.iteritems():$     print value
del_id = list(df_never_moved['id'])$ df['Timestamp +2'] = df['Timestamp'].apply(addtwo)
df = pd.read_csv('score_data_set.csv', index_col='createdAt', parse_dates=['createdAt'])$ df.head()
pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)$ tweets = api.search(q='Deloitte', rpp=1)$ pp.pprint([att for att in dir(tweets) if '__' not in att])
cityID =  '4ec71fc3f2579572'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $          Shreveport.append(tweet) 
X = endometrium_data.drop(['ID_REF', 'Tissue'], axis=1).values$ y = pd.get_dummies(endometrium_data['Tissue']).values[:,1]
max_change_1day = max((v.High - v.Low) for v in data.values()) $ print('=>The maximum change in a day was {:.2f} ' .format(max_change_1day))
mars_table = table[0]$ mars_table.columns = ["Parameter", "Values"]$ mars_table.set_index(["Parameter"])$
dfa=new_table.groupby("type")
auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)$ auth.set_access_token(acc_token, acc_secret)$ api = tweepy.API(auth)
frames = [bbc, cbs, cnn, fox, nyt]$ result = pd.concat(frames)$ result.head()
dates = pd.date_range('2010-01-01', '2010-12-31')$ df1=pd.DataFrame(index=dates)$ print(df1.head())
sorted(entity_relations.items(), key=lambda x: x[1], reverse=True)
result=results.set_index(['date','home_team','away_team'])$ result.head()
! rm -rf ~/s3/comb/flight_v1_0.pq
c = y.ravel() # returns a reference to the same array if possible $ c.base is x
df_subset['diff'] = df_subset.apply(diff_money, axis = 1, pattern = pattern)$ print(df_subset.head())$ 
rdd.map(lambda x: x**2 + really_large_dataset.value).collect()
from sqlalchemy import create_engine$ engine = create_engine('postgresql+psycopg2://postgres:admin@localhost:5432/DTML')$ X.to_sql('dtml_mstr_scld', engine, if_exists='append')
query_date = dt.date(2017, 8, 23) - dt.timedelta(days=7)$ print("Query Date: ", query_date)
url = 'https://www.dropbox.com/s/yz5biypudzjpc12/PSI_tweets.txt?dl=1'$ location = './' #relative location for Linux, saves it in the same folder as this script$ download_file(url, location)
metrics.accuracy_score(y_test, predicted)
filename = 'Daily_Stock_Prediction_latest.pk'$ with open('./Models/'+filename, 'rb') as f:$     model_test = pickle.load(f)
html = browser.html$ soup = bs(html, 'html.parser')$ browser.click_link_by_partial_text('FULL IMAGE')
data.head()
reserve_tb[reserve_tb['customer_id'].isin(target)]
print('raw time value: {}'.format(plan['plan']['date']))$ print('datetime formatted: {}'.format(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(plan['plan']['date']/1000))))
df_subset2.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70)$ plt.show()
df["timePeriodStart"] = pd.to_datetime(df.timePeriodStart)
keywords = ['earthquake', 'quake', 'magnitude', 'epicenter', 'magnitude', 'aftershock']$ search_results = api.search(q=' OR '.join(keywords), count=100)
popular_programs = challange_1["program_code"].value_counts()$ popular_programs
check_rhum = rhum_fine[1]$ sns.heatmap(check_rhum)
soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())$
tweet_df.count()
iris.loc[:,"Species"].cat.categories
driver = webdriver.Chrome()
day_markers = np.arange(df1['DATETIME'].min(), df1['DATETIME'].max(), timedelta(days=1))$ day_markers = np.append(day_markers, df1['DATETIME'].max())$ day_markers
data['SA'] = np.array([analize_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
num_words=["roussia","jayin","page","chance"]$ print (words_df[ words_df['Word'].isin(num_words) ])
rain = session.query(Measurement.date, Measurement.prcp).\$     filter(Measurement.date > last_year).\$     order_by(Measurement.date).all()$
max_change = [ldf['High'] - ldf['Low']]$ largest_change = np.asarray(max_change).max()$ largest_change
df.to_csv('df_ancestry.csv')$ df_model = h2o.import_file(path='df_ancestry.csv')
!sed -i -e 's/if self.max_leaf_nodes == "None":/if self.max_leaf_nodes == "None" or not self.max_leaf_nodes:/' \$   /usr/local/lib/python3.5/dist-packages/autosklearn/pipeline/components/regression/gradient_boosting.py
USvideos = pd.read_csv('data/USvideos_no_desc.csv', parse_dates=['trending_date', 'publish_time'])
wordfreq = FreqDist(words_sk)$ print('The 100 most frequent terms, including special terms: ', wordfreq.most_common(100))
nf = 1e-1$ loss = tf.reduce_mean(tf.squared_difference(Ypred*nf,Y*nf))$
stories = pd.read_json('https://lobste.rs/hottest.json')$ stories.head()
client.repository.list_models()
print(trump.favorite_count.sum())$ print(" ")$ print(trump.retweet_count.sum())
'plumber' in model.wv.vocab
print(most_informative_features(our_nb_classifier.classifier, n=10))
events_popularity_summary = events_top10_df[events_top10_df['yes_rsvp_count']>5][['event_id','popularity','topic_name']].pivot_table($                                 index='topic_name', columns='popularity', aggfunc='count')
building_pa_prc_fix_issued=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date','issued_date'])
rain_df = pd.DataFrame(rain)$ rain_df.head()
import os$ os._exit(0)
charge = reader.select_column('charge', start=0, stop=100)$ charge = charge.values # Convert from Pandas Series to numpy array$ charge
age = pd.Series(test_age)
n = np.count_nonzero(e==0)$ k = np.count_nonzero(y[e==0])
cursor = db.TweetDetils.aggregate([ {"$group" : {"_id":"$user_name", "score":{"$sum":"$fav_cnt"}}},  {"$sort":{"score" : -1}},{"$limit":5}])$ for rec in cursor:$     print(rec["_id"], rec["score"])
import gcsfs$ import google.datalab.bigquery as bq$ import pandas as pd
rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]$
outname = 'user_dataset_results.csv' # You can rename this file to whatever you'd like. Using the same output filename multiple times could cause overwriting, be careful!$ fullpath = os.path.join('./output', outname)  $ combined_df.to_csv(fullpath, index=False)
round((model_x.rsquared_adj), 3)
df1 = ml.select_features(indices.shape[0], indices, df)
empty_sample.iloc[1000:1010]
airquality_pivot = airquality_pivot.reset_index()$ print(airquality_pivot.index)
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df =sentiments_df[['Date','Compound','Count']]$ sentiments_df.tail()
s = fixed.iloc[200000:205000,]$ plt.plot(s.unstack(level=0), alpha=0.2);
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
df_merge.groupby(['school_name','grade']).reading_score.mean().unstack()
authors = Query(git_index).get_cardinality("author_name").by_period()$ print(get_timeseries(authors, dataframe=True).tail())
data['SA'] = np.array([analyse_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
df_merge = pd.merge(df_schoo11, df_students1, on='school_name')$ df_merge.drop(['School ID', 'Student ID'], axis = 1, inplace=True)
df['MeanFlow_cms'] = df['MeanFlow_cfs'] * 0.028316847
measurements = Base.classes.measurements$ measurements
from h2o.estimators.glm import H2OGeneralizedLinearEstimator$ glm_model = H2OGeneralizedLinearEstimator(model_id = "GLM", family = "binomial")$ glm_model.train(x = predictor_columns, y = target, training_frame = train, validation_frame = valid)
sns.factorplot(x='call_type',y='length_in_sec',col='call_day',data=calls_df,kind='bar')
base_df = DataObserver.major_df$ base_df_body = DataObserver.major_df['body']
fig = ax.get_figure()$ fig.savefig('n5-exercise.svg')
station_count = session.query(Stations.id).count()$ print (f"Station Count = {station_count}")
iris.head()
grouper = df.groupby(['source account'])$ overall_score = grouper['compound score'].mean()$ overall_score
feature_names = vectorizer.get_feature_names()$ if feature_names:$     feature_names = np.asarray(feature_names)
reddit = pd.read_csv(filename)$ reddit.drop('Unnamed: 0', axis = 1, inplace = True)$ reddit.head()
analysis.iloc[686].Description$ analysis.iloc[686].project_url$
out_temp_columns = [s for s in daily_dat.columns if primary_temp_column in s] #only save select temperature columns$ save_name=Glacier.lower()+ Station + "_daily_"+"LVL2.csv" #filename$ save_pth=os.path.join(save_dir, save_name) #location to save file
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd.head()
datasets_ref = pd.read_csv('../list_of_all_datasets_dgfr/datasets-2017-12-13-18-23.csv', sep=';')$ datasets_slug_id = datasets_ref.set_index('slug')['id'].to_dict()$ datasets_id_slug = datasets_ref.set_index('id')['slug'].to_dict()
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df.head()
red_4.info()
df.drop(["city","lat","lon"], inplace = True, axis = 1)
df3['created_at'] = df3['created_at'].apply(lambda x: x.round('min'))
from bson.objectid import ObjectId$ def get(post_id):$     document = client.db.collection.find_one({'_id': ObjectId(post_id)})
predictions = np.array([item['classes'] for item in classifier.predict(input_fn=eval_input_fn)])$ ids = np.array([i + 1 for i in range(len(predictions))])$ output = pd.DataFrame({id_label:ids, target_label:predictions}, dtype=np.int32)
for urlTuple in otherPAgeURLS[:3]:$     contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ contentParagraphsDF.head()
df = sf_permits[['Permit Number','Current Status','Completed Date']]$ df.head()
iso_gdf.intersects(iso_gdf_2)
df = pd.read_csv("../../data/msft_with_footer.csv",skipfooter=2,engine='python')$ df
citydata_nbr_rides_renamed = citydata_with_nbr_rides.rename(columns={"ride_id": "nbr_rides"})$ citydata_work = citydata_nbr_rides_renamed[['city', 'driver_count', 'type', 'average_fare', 'nbr_rides']]$
print(r_test.json())
from bmtk.analyzer import nodes_table$ nodes_table('network/recurrent_network/nodes.h5', 'V1')
df_selection = df_selection.dropna(how='any') 
df.user_id.nunique()
total_stations = session.query(Stations).distinct().count()
corrmat = blockchain_df.corr()$ f, ax = plt.subplots(figsize=(11, 9))$ sns.heatmap(corrmat)
es.get(index="test-index", doc_type='tweet', id=1)
display((data['Tweets'][fav]))$ print("Number of likes: {}".format(fav_max))$ print("{} characters.\n".format(data['len'][fav]))
hourly_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner')$ hourly_df.reset_index(inplace=True)$ hourly_df.head()
merged1 = pd.merge(left=merged1, right=offices, how='left', left_on='OfficeId', right_on='id')
nu_fiss_xs = fuel_xs.get_values(scores=['(nu-fission / flux)'])$ print(nu_fiss_xs)
metadata['epsg'] = int(refl['Metadata']['Coordinate_System']['EPSG Code'].value)$ metadata['epsg']
df3 = df3.add_suffix(' Created')$ df7 = pd.merge(df4,df3,how='left',left_on='Date Created',right_on='MEETING_DATE Created')$
print(df['Site Fill'].value_counts(dropna=False))
output.coalesce(2).write.parquet("/home/ubuntu/parquet/output.parquet/output.parquet")
data[data.userScreen=='KTHopkins']
data.to_csv('/home/sb0709/github_repos/bootcamp_ksu/Data/data.csv', sep = ',')
days = pd.date_range('2014-08-29','2014-09-05',freq='B')$ for d in days: print(d)
df.to_csv(LM_PATH/'df.csv', index=False)
data.keys()
vip_reason = vip_reason.drop(['[', ']'], axis=1)$ vip_reason = vip_reason.drop(vip_reason.columns[0], axis=1)
commits_per_year = corrected_log.resample('AS',on='timestamp')['author'].agg('count')$ print(commits_per_year.head())
print(Counter(ent.text for ent in doc.ents if 'GPE' in ent.label_))
df.dropna(how='any', inplace=True)$
from pyspark.sql import functions as F$ access_logs_df.select(min('contentSize'), avg('contentSize')).show()$ access_logs_df.describe(["contentSize"]).show()
year16 = driver.find_elements_by_class_name('yr-button')[15]$ year16.click()
three = df.pop('three')
pd.set_option('display.max_columns', 23)$ movies.head(5)
data['inday_icu_wkd'] = np.where(data.intime_icu.dt.weekday <= 4, $                                  'weekday','weekend')$ data['inday_icu_wkd'].value_counts()
chinese_vessels_wcpfc = pd.read_csv('chinese_vessels_wcpfc.csv')
df_cat = pd.get_dummies(data[catFeatures])
daily_df = pd.concat([predictions_daily, stock_daily], axis=1, join='inner')$ daily_df.head()
financial_crisis.loc['Wall Street Crash / Great Depression'] = '1929 - 1932'$ print(financial_crisis)
bad_iv_post = options_frame[np.isnan(options_frame['ImpliedVolatilityMid'])]
average_range = df['range'].mean()
model = ARIMA(AAPL_array, (2,2,1)).fit()$
coins_mcap_today[50:].index
bikedataframe = pd.concat([dfbikes, snow_weather, df_wu, dfsunrise], axis=1)$ bikedataframe.head()
all_data_vectorized = body_pp.transform_parallel(all_data_bodies)
df_cod3 = df_cod2.copy()$ df_cod3["Cause of death"] = df_cod3["Cause of death"].apply(classify_natural)$ df_cod3
Precipitation_DF.plot(rot=45,title="Precipitation from %s to %s"%(start_date,end_date),figsize=(8,5),grid=None,colormap="PuOr_r")$ plt.show()
options_frame.head()
msft = pd.read_csv("../../data/msft.csv")$ msft.head()
test[['clean_text','user_id','predict']][test['user_id']==1497996114][test['predict']==9].shape[0]
import seaborn as sns$ sns.set(style="darkgrid")$ ax = sns.countplot(x="AGE_groups", data=df_CLEAN1A)$
tweetsIn22Mar = tweetsIn22Mar.loc['2018-03-22']$ tweetsIn1Apr = tweetsIn1Apr.loc['2018-04-1']$ tweetsIn2Apr = tweetsIn2Apr.loc['2018-04-2']
temps_df.Missoula > 82
unique_users = set(orders.user_id.unique())$ selected_users = random.sample(unique_users, 20000)
ab_df2.isna().sum()
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep=',', encoding='utf-8', header=True)
TripData_merged3 = TripData_merged2.dropna(how='any')
P = Plotting(output_path)$ Plot = P.open_netcdf()
abc = df_providers.groupby(['year','drg3']) orange$ abc.head()
date_splits = sorted(list(mentions_df["date"].unique()))$
hourly_df['Open_Price_Change'].value_counts()
segments.st_time.dtype$ datetime.strptime(segments.st_time.loc[0],'%m/%d/%y %H:%M')
from keras.models import load_model$ y_pred = model.predict_classes(val_x)$ y_true = val_y
my_df_free1.iloc[100:110,0:3]
gs.score(X_test_total_checked, y_test)
id_of_tweet = 932626561966247936$ tweet = (api.get_status(id_of_tweet, tweet_mode='extended')._json['full_text'])$ print(tweet)
index_vector = df.source == 'GRCh38'$ gdf = df[index_vector]$ gdf.shape
tweets_df.to_csv("CityData.csv", encoding='utf-8')
LR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)$ yhat_prob2 = LR2.predict_proba(X_test)$ print ("LogLoss: : %.2f" % log_loss(y_test, yhat_prob2))$
df_new.head()
a =R16df.rename({'Create_Date': 'Count-2016'}, axis = 'columns')$
df_new =df_country.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
data_df.groupby('tone')['ticket_id'].nunique()
len(df_proj.ProjectId.unique())
results_image = soup_mars.find_all("div", { "class" : "img" })$ image = results_image[1]$ image$
df = pd.read_csv("../../data/msft.csv",header=0,names=['open','high','low','close','volume','adjclose'])$ df.head()
output = pd.DataFrame(data={"id":test.id, "rating":predictions})$ output.to_csv( "new_naive.csv", index=False, quoting=3 )$
rng = pd.date_range('3/6/2012 00:00', periods=15, freq='D')$ rng.tz is None, rng[0].tz is None
match = results[2]$ print lxml.html.tostring(match)
dates = pd.date_range('20130101', periods=6)$ df = pd.DataFrame(np.arange(24).reshape((6,4)),index=dates, columns=['A','B','C','D'])$ df.A.max
rdf.loc[pd.isna(rdf.accident_counts),'accident_counts'] = 0.0
df.median() + 1.57 * (df.quantile(.75) - df.quantile(.25))/np.sqrt(df.count())
df.head()
idx = pd.IndexSlice$ df.loc[idx['a', 'ii', 'z'], :]
exiftool -csv -createdate -modifydate ciscih8/CISCIH8_cycle1.mp4 ciscih8/CISCIH8_cycle2.mp4 ciscih8/CISCIH8_cycle3.mp4 ciscih8/CISCIH8_cycle4.mp4 ciscih8/CISCIH8_cycle5.mp4 ciscih8/CISCIH8_cycle6.mp4 > ciscih8.csv
tree = RandomForestClassifier(max_depth = 4)$ tree.fit(X_train, y_train)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
from sklearn.feature_extraction.text import TfidfVectorizer$ tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=2, max_df=0.5, stop_words=portuguese_stop_words)
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
for col_cells in ws.iter_cols(min_col=28, max_col=28):$     for cell in col_cells:$         cell.number_format = '0.00'
station_availability_df.loc[station_availability_df['status_key']==1,'status_value'] = "In Service"$ station_availability_df.loc[station_availability_df['status_key']==3,'status_value'] = "Not In Service"$ station_availability_df.status_value.value_counts()
tips.groupby(["sex","size"]).mean().loc[:,"total_bill"].loc["Female",3:5]
col_list = list(df.columns.values)$ print(col_list)
cols_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'splitseed']$ X_train.drop(cols_to_drop, axis=1, inplace=True)$ X_age_notnull.drop(cols_to_drop, axis=1, inplace=True)
old_page_converted = np.random.choice([1,0], size=df_oldlen, p=[pold,(1-pold)])$
url='https://api.twitter.com/1.1/trends/place.json?id=2459115'$ parameters={'q':'Trump'}$ topics=requests.get(url,auth=auth,params=parameters)
featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA22456_hires.jpg'$ featured_image_url
injury_df['Date'] = injury_df['Date'].map(lambda x: x.replace('-',''))$ injury_df['Date'].head()
invalid_ac_df.groupby(['reporter_story']).sum().sort_values(by=['Invalid AC'], ascending=False).head()
len(df_events), len(df_events.group_id.unique())
word2vec = Word2VecProvider()$ word2vec.load("data/embedding_file")
x_mean, up, low = bollinger_band(DataFrame(values), 20, 1.5)
tlen.plot(figsize=(16,4), color='r')
eresolve = pd.Series(dfENTed.Entity.values,index=dfENTed.InText.values).to_dict()$ EEdgeDF['To'] = EEdgeDF['To'].map(eresolve)$ EEdgeDF.head(7)
max_fwing = df.loc[df.following.idxmax()] $ name = max_fwing['name'] if max_fwing['name'] is not None else max_fwing['login']$
print data_df.clean_desc[20]$ print 'becomes this ---'$ print text_list[20]
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-05-23&end_date=2018-05-23&api_key='+API_KEY$ r1 = requests.get(url1)
p_tags = sample_soup.find_all("p")$ for p in p_tags:$     print(p.text)
df.plot(x="newDT", y=["ED",'DD','CVX','FL','CAT'], kind="line")$
m1.fit()$ m1.forecast()$ m1.plot(plot_type = 'autocorrelation' ,lag = 2)$
df.max()
r =requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2010-01-01&end_date=2010-01-01')
plt.tick_params(axis='x',width=2,colors='k')$ plt.tick_params(axis='y',width=2,colors='k')$
search2 = search2.sample(frac=1)
inspector = inspect(engine)$ inspector.get_table_names()
data_set.to_csv("hiv_antibody_found.csv", index=False, encoding='utf-8')$
X_train_predict = model.predict(X_train)$ X_test_predict = model.predict(X_test)
events_enriched_df = pd.merge(events_df[['event_id','group_id','yes_rsvp_count','waitlist_count','event_time']]$                               ,groups_topics_unique_df[['group_id','topic_name','topic_id']]$                               ,on ='group_id' , how='left' )
tick_locations = [value+0.4 for value in x_axis]$ plt.xticks(tick_locations, bar_outlets)
url = "http://www.fdic.gov/bank/individual/failed/banklist.html"$ banks = pd.read_html(url)$ banks[0][0:5].ix[:,0:4]
old_page_converted = np.random.choice([1, 0], size=n_old,p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
dr_new = doctors[doctors['ReasonForVisitDescription'].str.contains('New')]$ dr_existing = doctors[~doctors['ReasonForVisitDescription'].str.contains('New')]
assert (ebola >= 0).all().all()
print(dictionary.token2id['like'])
print(r.json()['dataset_data']['column_names'])$ print(r.json()['dataset_data']['data'][0])$ print(r.json()['dataset_data']['data'][-1])
item_lookup = shopify_full_size_only[['child_sku', 'child_name']].drop_duplicates() # Only get unique item/description pairs$ item_lookup['child_sku'] = item_lookup['child_sku'].astype(str) # Encode as strings for future lookup ease
ser7 = pd.Series([9,8,7,6,5,34,2,98])$ ser7.head(7) $ ser7.tail(2)
pipe_lr_3 = make_pipeline(tvec, lr)$ pipe_lr_3.fit(X_train, y_train)$ pipe_lr_3.score(X_test, y_test)
print("Url: " + client.repository.get_model_url(saved_model_details))
hru_rootDistExp = rootDistExp.open_netcdf()
e = Example()$ print(e.__dict__)$ print(e.__dict__.__class__)
(trainingData, testData) = output2.randomSplit([0.7, 0.3])
yc_new1 = yc_new1.merge(zipincome, left_on='zip_dest', right_on='ZIPCODE', how='inner')$ yc_new1.head()
intervention_train.index.duplicated()
df.info()
tcat_df.to_csv('cat_tweets.csv')$ tdog_df.to_csv('dog_tweets.csv')
rng_dateutil = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='dateutil/Europe/London')
total_fit_time = (end_fit - start_fit)$ print(total_fit_time/60.0)
data[(data['author_flair'] == 'Bears') & (data['win_differential'] >= 0.9)].comment_body.head(15)$
pd.DataFrame(dummy_var["_Source"][Company_Name]['Low']['Forecast'])[-4:]
edge_types_file = directory_name + 'edge_types.csv'   # Contains info. about every edge type$ edges_file = directory_name + 'edges.h5'             # Contains info. about every edge created
Top_tweets = data.drop(['text','text_lower'],axis=1)$ Top_tweets.head()
cityID = '389e765d4de59bd2'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Glendale.append(tweet) 
mike=pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/mike_previous_observing.txt',delim_whitespace=True,header=None,names=['index','name','ra','dec'])$ mike['RA0']=[float(Angle(i, u.hr).to_string(unit=u.degree, precision=5, decimal=True)) for i in mike.ra]$ mike['DEC0']=[float(Angle(i, u.deg).to_string(unit=u.degree, precision=5, decimal=True)) for i in mike.dec]$
closePrice = aapl['Close']$ closePrice.head(5)
fig = df['violation_desc_long'].value_counts()[:10].plot('barh') #title='Top Citations by Violation')$ plt.savefig("by_violation_desc.png")
items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']),$          'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])}$ print(type(items))
list(set(df['City'][df.CustomerID=='0000119007']))   
td_wdth = td_norm * 1.5$ td_alph = td_alpha$ td[:] = td[:].astype(int)
calls_nocontact_simp['ticket_closed_date_time'] = pd.to_datetime(calls_nocontact_simp['ticket_closed_date_time'])
GenresString=Genres.replace('|',',')$ GenresString=GenresString.replace(',',',\n')
RNPA_new = RNPA[RNPA['ReasonForVisitDescription'].str.contains('New')]$ RNPA_existing = RNPA[~RNPA['ReasonForVisitDescription'].str.contains('New')]
mlp_pc = mlp_df.pct_change()$ mlp_pc.head()
tweets.dtypes
data_file_path = "/Users/gandalf/Documents/coding/do_not_commit/capstone/"$ website_file_path = '/Users/gandalf/Documents/coding/rczyrnik.github.io/capstone/'
%time train = pd.read_csv("../assets/trainaa")$ train.head(1)
df.iloc[]
store_items.dropna(axis=1) # or store_items.dropna(axis=1, inplace=True)
model.wv.most_similar(positive=['pasta','chinese'], negative=['italian'])$
cohort_activated_df = pd.DataFrame(index=daterange,columns=daterange)
mov_ids = tmp_df[tmp_df.tmp_idx == usr_idx].columns
weight_is_relevant = 2*1/(np.sum(article_isRelevant)/ len(article_isRelevant))$ weight_is_not_relevant = 1$ weights = {0:weight_is_not_relevant, 1:weight_is_relevant}
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ print("Last 10 tweets:")$ display(data.head(10))
df.T
df[['text', 'retweet_count', 'date']][df.retweet_count == np.max(df.retweet_count)]
cityID = '44d207663001f00b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Mesa.append(tweet) 
sites = pd.read_csv('../data/station.csv',$                    dtype={'HUCEightDigitCode':'str'})
temp_us = temp_nc.variables['air'][1, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(temp_us)
writing_commit_df = commit_df.query("(characters_added > 0 or characters_deleted > 0) and merge == 0")$ stats['manuscript_commits'] = len(writing_commit_df)
(keys.shape, keys_0611.shape)
lg = pd.read_csv('awesome_go_web.txt.csv')$ lg.head()
np.array(df[['Visitors','Bounce_Rate']]).tolist()
transfer_duplicates = BTC.loc[BTC['Smoother'].isnull()==False,['Year','Month','Day','Smoother']];
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY)
S_lumpedTopmodel.decision_obj.simulStart.value, S_lumpedTopmodel.decision_obj.simulFinsh.value
merge_df['Primary Language'] = merge_df['Language'].str.split('-').str[0]$ merge_df['Language Location'] = merge_df['Language'].str.split('-').str[1]
validation.analysis(observation_data, Jarvis_simulation)
response = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?column_index=4&start_date=2018-02-01&end_date=2018-02-01&collapse=daily&transform=rdiff&api_key={}'.format(API_KEY))$ print(response.json())
TrainingSamples = int(MaxPoints * 0.7)$ ValidationSamples = int((MaxPoints-TrainingSamples)/2)$ TestSamples = MaxPoints - TrainingSamples - ValidationSamples
automl = pickle.load(open(filename, 'rb'))
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local')
data.head()
brewery_bw.head(5)
mta_avg = dfs_morning.groupby(by = 'STATION').ENTRIES_MORNING.mean().reset_index()$ mta_census = mta_avg.merge(census_zip, left_on=['STATION'], right_on=['station'], how='left')$ mta_census.drop('station', axis = 1, inplace = True)
df_master=pd.read_csv('cleaned data/final_master.csv')
top_allocs = hist_alloc.loc[pd.to_datetime(intervals)].sum(axis=0).sort_values(ascending=False)$ top_allocs[:10], top_allocs[-10:]
cityID = 'a307591cd0413588'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Buffalo.append(tweet) 
resp = r.json()$ print(resp)
crimes.groupby(['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']).size()
df_wm.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
pvt.to_csv('./output/ganalytics_transactions_and_associated_products.txt', sep='|', index=False, quoting=csv.QUOTE_NONE)
print(plan['plan']['itineraries'][0]['legs'][0].keys())
df['2015-06-03']['battle_deaths'].mean()
CSV_FILE_PATH = os.path.join('pims_cloudpbx_subset_201806051550_1million.csv')$ GEOLITE_ASN_PATH = os.path.join('GeoLite2-ASN.mmdb')$ GEOLITE_CITY_PATH = os.path.join('GeoLite2-City.mmdb')
df.head(3)
df = pd.read_csv('cleaned_and_merged.csv').drop('Unnamed: 0', axis = 1)$ df.head()
df_csv = pd.read_csv(datafile)$ df_csv.head()$
df = pd.DataFrame(results, columns=['date', 'precipitation'])$ df.set_index(df['date'], inplace=True)$ df.tail()
plt.show()
sp500.at['MMM', 'Price']
