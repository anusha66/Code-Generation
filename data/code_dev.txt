df_events = pd.read_csv("data_output/df_events.csv",low_memory=False)
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
fitfile = FitFile('fit_files/2871238195.fit')#longer swim two way$ fitfile = FitFile('fit_files/2913114523.fit')#krumme lanke swim$
df2.isnull().sum()
df.to_csv("newsOutletTweets.csv")
tOverweight = len(df[(df['bmi'] >= 25.0) & (df['bmi'] < 30.0)])$ tOverweight
probarr = fe.toar(lossprob)$ fe.plotn(fe.np.sort(probarr), title="tmp-SORTED-prob")
Google_stock['Adj Close'].describe()
for temp_col in temp_columns:$     dat[temp_col]=LVL1.hampel(dat[temp_col], k=7) #remove outliers with hampel filter$     dat[temp_col]=LVL1.basic_median_outlier_strip(dat[temp_col], k=8, threshold=4, min_n_for_val=3) #remove remaining outliers; spikes where val exceeds 2-hr rolling median by 4C    
df.dtypes
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_zh_wzh_2.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_wzh_2.w2v')
contractor_final.to_sql('contractor', con=engine,if_exists='append',index = False)
engine.execute('SELECT * FROM measurements LIMIT 15').fetchall()$
lm.score(x_test,y_test)
df_concensus['esimates_count'].describe()$
df['w'].unique()
nbar_clean.time
import warnings$ warnings.filterwarnings('ignore')
temp['c'] = temp['contents'].str.split()
current_weather=soup.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text').text$ current_weather
openmc.run()
df_events.to_csv("data_output/df_events.csv", encoding="utf-8", index=False)
y = df['comments']$ X = df[['subreddit', 'title']].copy(deep=True) 
df_sb.head(2)
stories = pd.concat([stories, tag_df], axis=1)
chunker = ConsecutiveNPChunker(train_trees)$ print(chunker.evaluate(valid_trees))
plt.title('Burberry ngram', fontsize=18)$ burberry_ng.plot(kind='barh', figsize=(20,16));$ plt.savefig('../visuals/Burberry_ngram.jpg')
RDDTestScorees.map(lambda entry: (entry[0], entry[1] * 0.9)).collect()
deck = pd.DataFrame(titanic3['cabin'].dropna().str[0])$ deck.columns = ['deck']  # Get just the deck column$ sns.factorplot('deck', data=deck, kind='count')
model_x.summary2() # For categorical X.
useful_indeed.isnull().sum()$
df2.drop(labels=1899, axis=0, inplace=True)$ df2[df2['user_id']==773192]
df.to_html('resources/html_table_marsfacts.html')
cur.execute("SELECT * FROM pgsdwh.pgsparts.geps_distinct_shipping_lines_dw_t")
precip_data_df = pd.DataFrame(precip_data_dict)$ precip_data_df = precip_data_df.rename(columns={"prcp":"Precipitation"})$ precip_data_df.head()$
weather_df_byday = weather_df.loc[weather_df.index.weekday == weekday]$ weather_df_byday.info()$ weather_df_byday.head()
data['Age'].value_counts()
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG RN/PAs', RN_PA_duration, window=52)
list = [1,3,4,30]$ list.append(21)$ print(list)
data.groupby(['Name'])['Salary'].sum()
(trainingData, testData) = data.randomSplit([0.7, 0.3])$
x.dtype = 'int32' # (need to revert)$ x.astype(float) # creates a new array
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
df_names = pd.read_csv('names.csv')$ df_names
df.boxplot('MeanFlow_cms');
import numpy as np$ X_nonnum = X.select_dtypes(exclude=np.number)
executable_path = {'executable_path': 'chromedriver.exe'}$ browser = Browser('chrome', **executable_path, headless=False)
print("prediction: {0}, actual: {1}, 2018-03-31".format(y_pred_list[0], y_test_aapl[0,0]))
stc.checkpoint("checkpoint10")
df_release = pd.read_csv( pwd+'/releases.052317.csv', encoding='utf-8') #earning release?
cursor = collection_reference.aggregate([{'$sample': {'size': 5}}])$ tw_sample_df = pd.DataFrame(list(cursor))$ tw_sample_df
group_sizes = (data.$               groupby('species')$               .size())
mlp_pc_fp = 'data/richmond_median_list_prices_percent_change.csv'$ mlp_pc.to_csv(mlp_pc_fp, index=True) # Pass index=True to ensure our DatetimeIndex remains in the output
df1=files3.pivot_table(index='candidateid', columns='dimensiontype', values='finalscore')$ df1new = pd.DataFrame(df1.to_records())$ df1new.head()
walkmin = walk.resample("1Min")
back_to_h2o_frame = h2o.H2OFrame(pandas_small_frame)$ print(type(back_to_h2o_frame))$ back_to_h2o_frame
print(r.json())
scratch['created_at'] = pd.to_datetime(scratch['created_at'], coerce=True)
ved['season'] = ved.index.str.split('.').str[0]$ ved['term'] = ved.index.str.split('.').str[1]
theft.sort_values('DATE_OF_OCCURRENCE', inplace=True, ascending=True)$
Geocoder.geocode("4207 N Washington Ave, Douglas, AZ 85607").valid_address
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df.head()
df_newpage = df.query('landing_page == "new_page"')$ df_2 = df_newpage.query('group == "treatment"')$ df_2.nunique()$
cityID = '300bcc6e23a88361'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Seattle.append(tweet) 
session.query(func.count(Station.station)).all()
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,100], ylim=[0,100], title="Favorites and Retweets:100x100")
df = pd.read_csv(SRC_FILE)$ df['CreatedDate'] = pd.to_datetime(df['CreatedDate'], format='%m/%d/%Y %I:%M:%S %p +0000')$ df.set_index('CreatedDate', inplace=True)
int_and_tel.plot(y='sentiment_score', use_index= True, figsize=(18, 6))
fh_1 = FeatureHasher(num_features=uniques.iloc[2, 1], input_type='string', non_negative=True) # so we can use NaiveBayes$ %time fit = fh_1.fit_transform(train.device_model)
import sys$ src_dir = os.path.join('..', 'src')$ sys.path.append(src_dir)
very_pop_df = au.filter_for_support(popular_trg_df, min_times=6)$ au.plot_user_dominance(very_pop_df)
tickers = companies['tweet_ticker'].tolist()$
from_6_cdf.plot(kind='barh', x='category', y='occurrence_count', figsize=(12, 10), title= 'Categories', label= "Occurrence Count")$ plt.gca().invert_yaxis()$ plt.legend(loc= 4, borderpad= True)
S_1dRichards.decision_obj.hc_profile.options, S_1dRichards.decision_obj.hc_profile.value
df2.head()$
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')
df.to_excel("../../data/msft2.xlsx")
adj_glm_int = smf.glm('hospital_expire_flag ~ C(inday_icu_wkd) * C(admission_type)', $                      data=data, family=sm.families.Binomial()).fit()$ adj_glm_int.summary2()$
tw['tweet_id'].map(lambda x: len(str(x).strip())).value_counts()
diff = [abs(list(r_close.values())[i] - list(r_close.values())[i-1]) for i in range (1,len(r_close))]$     $
df.quantile(.75) - df.quantile(.25)
df = df.drop(['Unnamed: 0', 'id', 'created_at', 'crawl_at'], axis=1)
flights2.loc[[1950, 1951]]
Base.classes.keys()
import scipy.stats as stats$ t_stat, p_val = stats.ttest_ind(dftop['temp'],weather['temp'], equal_var=False)
df_C = df_A.copy()$ columns = df_C.columns.tolist()$ res = pd.concat([pd.DataFrame(df_A.values, columns=columns), pd.DataFrame(df_B.values, columns=columns)], keys=["A", "B"])$
df.to_csv(r'C:\Users\LW130003\cgv.csv', index=False)
goodreads_users_df.replace('None', np.nan, inplace=True)
pd.concat([df1,df2,df3])$
churned_unordered = unordered_df.loc[churned_unord]
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
fulldf.to_csv('FullResults.csv', encoding='utf-8', index=False)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt"$ mydata = pd.read_table(path, sep= '\t')$ mydata.head(5)
tmi = indices(tmaggr, 'text', 'YearWeek')$ tmi.head()
df['fetched time'] = df['fetched time'].astype('datetime64[s]')$ df['created_utc'] = df['created_utc'].astype('datetime64[s]')
session.query(tobs.station, func.count(tobs.tobs)).group_by(tobs.station).\$                order_by(func.count(tobs.tobs).desc()).all() 
tot = pd.merge(dailyplay, promotions, how = 'left', left_on = ['Date'], right_on = ['Date'])
precp_df = pd.DataFrame(results, columns=['date', 'prcp'])$ precp_df.set_index('date', inplace=True, )$ precp_df.head(10)
outfile = os.path.join("Resource_CSVs","Main_data_Likes.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
news_sentiments.to_csv('New_Source_sentiments.csv', encoding='utf-8', index=False)
horizAAPL = AAPL.sort_index(axis=1)$ horizAAPL.head()
adj_close_pivot_merged = pd.merge(adj_close_pivot, adj_close$                                              , on=['Ticker', 'Adj Close'])$ adj_close_pivot_merged.head()
station_count = station_df['Station ID'].nunique()$ print(station_count)
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
movies=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\movies.csv')
plate_appearances.loc[plate_appearances.batter_name=='ryan howard',].head()
metadata['reflectance_scale_factor'] = float(refldata.attrs['Scale_Factor'])$ metadata
lr = LogisticRegression(random_state=20, max_iter=10000)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr', 'multinomial'], 'solver':['saga','newton-cg', 'lbfgs']}$ grid_tfidf = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
file_attrs_string = str(list(hdf5_file.items()))$ file_attrs_string
joined=join_df(train,store,"Store")$ joined_test=join_df(test,store,"Store")$ sum(joined['State'].isnull()),sum(joined_test['State'].isnull())
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])$
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ last_date
sl['mindate'] = sl.groupby('wpdx_id')["new_report_date"].transform('min')$ sl['maxdate'] = sl.groupby('wpdx_id')["new_report_date"].transform('max')
from sklearn.externals import joblib$ joblib.dump(pca, '../models/pca_20kinput_6858comp.pkl')$ np.save('../models/crosstab_40937.pkl', crosstab_transformed)
train_topics_df=pd.DataFrame([dict(ldamodel[item]) for item in train_corpus], index=graf_train.index)$ test_topics_df=pd.DataFrame([dict(ldamodel[item]) for item in test_corpus], index=graf_test.index)$
if not os.path.isdir('output'):$     os.makedirs('output')
scoring_url = client.deployments.get_scoring_url(deployment_details)$ print(scoring_url)
print d.variables['trajectory']
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep$ lee_train_file = test_data_dir + 'lee_background.cor'$ print lee_train_file
output_fn = "NewsMedia.csv"$ result.to_csv(output_fn)
import numpy as np$ np.sqrt(s)
tlen.plot(figsize=(16,4), color='b');
data.plot(figsize=(16,6))$ plt.show()
plt.scatter(USvideos['likes'], USvideos['views'])
df_tobs.set_index('date',drop=True,inplace=True)
conn.commit();$ conn.close();
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
import pandas as pd$ users_df = pd.read_csv('data/{country}_users.csv'.format(country=country).replace(' ', '+'))$ users_df.sample(5)
((df-df_from_csv)**2 < 1e-25).all()
data2['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data2['Tweets'] ])$ display(data2.head(10))
tfidf.fit(text)
dti = pd.to_datetime(['Aug 1,2014','2014-08-2','2014.8.3',None])$ for l in dti: print(l)
print("Percentage of positive tweet= {}".format(len(pos_tweet)*100/len(data['tweets'])))$ print("Percentage of negative tweet= {}".format(len(neg_tweet)*100/len(data['tweets'])))$ print("Percentage of neutral tweet= {}".format(len(neu_tweet)*100/len(data['tweets'])))
user_df = pd.read_csv('verified_user.csv')$ print(len(user_df))$ print(user_df.created_at.min())$
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']])$ result=logit.fit()$
test_preds_df = pd.DataFrame(test_preds,index=test_target.index,columns=['kwh_pred'])$
df_user[df_user['user.name'].str.contains('marco rubio', case=False)]
list = ['a','b','c','d','e']$ list.insert(1,'x')$ print(list)
scores_df.to_csv('../Results/News_Tweets.csv', encoding="utf-8", index=False)
df.loc[df.userLocation == 'Manchester, PA', 'tweetText']$
regr = linear_model.LinearRegression()
plt.hist(np.nan_to_num(_delta.values()), bins=10)$ plt.tight_layout()
lr = LogisticRegression(C=3.0, random_state=42)
fin_r = fin_r.reindex(df.resample('D').index, method='ffill') #df.resample('D').index$ assert (fin_r.index == r_top10_mat.index).all()
file_obj = open('BCH_person.csv', 'r')$ data = list(file_obj)
scraped_batch6_top.to_csv('batch6_top.csv', encoding='utf-8')$ scraped_batch6_sec.to_csv('batch6_sec.csv', encoding='utf-8')
from sklearn.cluster import KMeans$ kmeans_model = KMeans(n_clusters=7) $ kmeans_model.fit(df)
df_tick = df_tick.loc['2017-08-24':'2017-08-30']
outfile = path + '../output/allData_NoRetweets_May27_Cleaned.csv'$ cleanedDataNoRetweets.to_csv(outfile, index=False, encoding='utf-8')
temp['c'] = temp['contents'].str.split()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', na_values=['.'], names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'class'])$ mydata.head(5)
sumAll = df['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])$ sumAll
len(df.user_id.unique())$
eth_market_info.drop(['Date'],inplace=True,axis=1)$ scaler_eth = MinMaxScaler(feature_range=(0, 1))$ scaled_eth = scaler_eth.fit_transform(eth_market_info)$
def words_at_start(index, df):$     tweet_text = df["Words"][index]$     return tweet_text.split()[:3]$
scoring_url = client.deployments.get_scoring_url(deployment_details)$ print(scoring_url)
stations_df.to_csv('clean_hawaii_stations.csv', index=False)
sqlCtx = pyspark.SQLContext(sc)$ sdf = sqlCtx.createDataFrame(df.astype(str))$ sdf.show(5)
dfm = (filtered_df['l_req_3d'] - filtered_df['l_req_3d_num_tutors'])$ filtered_df.loc[dfm > 0, :]
doesnt_meet_credit_policy = loan_stats['loan_status'].grep(pattern = "Does not meet the credit policy.  Status:",$                                                      output_logical = True)
x_axis = np.arange(0,len(target_users))$ x_axis
df2 = pd.read_csv('../output/data/expr_2/expr_2_pcs_nth_1_div_51_04-18.csv', comment='#')
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$     filter(Measurement.station == 'USC00519281').all()$
for columns in DummyDataframe[["Positiv", "Negativ"]]:$     basic_plot_generator(columns, "Graphing Dummy Data" ,DummyDataframe.index, DummyDataframe)
df["Source"].unique()
gmm.fit(X)$ labels = gmm.predict(X)$ labels
print("{} is the proportion of users converted.".format(df['converted'].mean()))
X2.shape
print(r.json()['dataset_data']['data'][:4])
s4.unique()
df["grade"] = df["raw_grade"].astype("category")$ df["grade"]
data['len'].hist(bins=14)
rf = RandomForestClassifier()$ rf.fit(X_train, y_train)$ rf.score(X_test, y_test)
df_campaigns['Open Rate'] = df_campaigns['Open Rate'].apply(lambda x: x[:-1]).astype('float')$ df_campaigns['Click Rate'] = df_campaigns['Click Rate'].apply(lambda x: x[:-1]).astype('float')
dates = pd.DatetimeIndex(pivoted.columns)$ dates[(labels == 0) & (dayofweek < 5)]
tdf[tdf['smoker'] == 'No'].describe()
s3 = pd.Series(0,pd.date_range('2013-01-01','2014-12-31'))$ s3['2013']
def add_to_list(word_list, dictionary):$     for each in word_list:$         dictionary.append(each)
yc_new3.describe()
import pandas_datareader.data as web  #Not 'import pandas.io.data as web' as in the book.
now_start = datetime.datetime.now()$ time_start = now_start.strftime("%Y-%m-%d (yyyy-mm-dd); %H:%M hrs.")$ print "# Starting time of computing: %s"%time_start
Inspection_duplicates = data_FCInspevnt_latest.loc[data_FCInspevnt_latest['brkey'].isin(vals)]$ Inspection_duplicates
compound_df = pd.DataFrame([avg_compound]).round(3)$ compound_df
gdax_trans_btc = pd.merge(gdax_trans_btc, coinbase_btc_eur_min.iloc[:,:2], on="Timestamp", how="left")
%%time$ model = AlternatingLeastSquares(use_gpu=False)$ model.fit(matrix_data)
g = sns.FacetGrid(data=dataset, col='rating')$ g.map(plt.hist, 'text length', bins=50)$
data.plot(legend=True,figsize=(15,8))
health_data_row.loc[2013, 1, 'Guido']  # index triplet
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href'][9:]
end_date = (datetime.strptime(most_recent_tobs , '%Y-%m-%d') - timedelta(weeks=48)).\$     strftime('%Y-%m-%d') #converts datetime to string$ end_date
population.loc['California':'Illinois']
df_features_checkcorr = df_features$ df_features_checkcorr.to_csv("data new/features.csv", encoding="utf-8", sep=",", index=False)
scores_mean = np.mean(raw_scores)$ scores_std = np.std(raw_scores)$ print('The mean is {:.5} and the standard deviation is {:.5}.'.format(scores_mean, scores_std))
adj_close = all_data[['Adj Close']].reset_index()$ adj_close.head()
pure_final_df=pure_final_df._get_numeric_data()$
test_ind["Pred_state_XGB"] = xgb_model.predict(test_ind[features])$ train_ind["Pred_state_XGB"] = xgb_model.predict(train_ind[features])$ kick_projects_ip["Pred_state_XGB"] = xgb_model.predict(kick_projects_ip_scaled_ftrs)
data.head()
url = 'https://twitter.com/marswxreport?lang=en'$ browser.visit(url)
print((fit.shape, fit1_test.shape))$ print((fit2.shape, fit2_test.shape))$ print((fit3.shape, fit3_test.shape))
shows = pd.read_pickle("ismyshowcancelled_raw_pull.pkl")
val = val.split(b'\r\n\r\n',1)[1]$ print(val.decode('utf-8'))
df.drop(['Unnamed:_13', 'addressee', 'delivery_line_2', 'ews_match', 'suitelink_match', 'urbanization', 'extra_secondary_number', 'extra_secondary_designator', 'pmb_designator', 'pmb_number'], axis = 1, inplace=True)$ df.drop([326625], axis=0, inplace=True)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=As4uh8SmqPWoJ1s9XXDT&start_date=2017-01-01&end_date=2017-01-01"$ r = requests.get(url)
q_all_pathdep = c.retrieve_query('https://v3.pto.mami-project.eu/query/8da2b65bd4f7cd8d56d90ddfcd85297e8aac54fcd0e04f0a0fa51b2937b3dc62')$ q_all_pathdep.metadata()
df_2018.dropna(subset=['Specialty'], how='all', inplace=True)
html_table_marsfacts = df.to_html()$ html_table_marsfacts
url_test = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \$       "&start_date=2017-01-01&end_date=2017-01-02&api_key=" + API_KEY$ req_test = requests.get(url_test)
model.save('model/my_model_current.h5')$ model.summary()
traded_volumes.sort()
print(preprocessor(df.loc[0, 'review'][:-500]), '\n')$ print(preprocessor("</a>This :) is :( a test :-)!"))
utility_patents_subset_df['prosecution_period'] = utility_patents_subset_df.grant_date - utility_patents_subset_df.filing_date$ utility_patents_subset_df.prosecution_period = utility_patents_subset_df.prosecution_period.apply(lambda x: x.days)$ utility_patents_subset_df.prosecution_period.describe()
get_nps(combined_df, 'role_en').sort(columns='score', ascending=False)
print(dfd.in_pwr_5F_max.describe())$ dfd.in_pwr_5F_max.hist()
df.num_comments= df.num_comments.apply(lambda x:0 if x <= 74 else 1)
exiftool -csv -createdate -modifydate cisnwh8/cisnwh8_cycle1.MP4 cisnwh8/cisnwh8_cycle2.MP4 cisnwh8/cisnwh8_cycle3.MP4 cisnwh8/cisnwh8_cycle4.MP4 cisnwh8/cisnwh8_cycle5.MP4 cisnwh8/cisnwh8_cycle6.MP4 > cisnwh8.csv
db = client.Mars_db$ collection = db.mars_news
df = df.loc[df['d_first_name'] != 'Unknown']$ df = df.loc[df['d_birth_date'] != 'nan']$ df.set_value(df.index[11650], 'd_birth_date', '04/26/1940')
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2016-07-01&end_date=2016-07-01&api_key={}'.format(API_KEY)$ r=requests.get(url)$ print(r.json())
df.info()
input_node_types_DF = pd.read_csv(input_models_file, sep = ' ')$ input_node_types_DF
y = x.loc[:,"A"]$ y.mean()$ np.mean(y)
from IPython.core.display import display, HTML$ display(HTML("<style>.container { width:100% !important; }</style>"))
env = Environment.from_studio("ticketing", "http://10.29.28.147:8090",$                               studio_api_token="dXNlci1kZXY6UGFzc3dvcmQ0REVW")$ exp = env.create_experiment('imdb-ratings')
for i,x in top_likes.iteritems():$     print ('https://www.facebook.com/'+x )
year_with_most_commits = ... 
gene_df['gene_id'].unique().shape
tobs_date = session.query(measurement.date, measurement.tobs).all()$ tobs_date                   
climate_vars.head()
store_items.fillna(method='ffill', axis=0)
df['Injury_Type'] = df.Notes.map(extract_injury)
bc.set_index('newdate', inplace=True)
colors = ('blue', 'red', 'gold', 'green', 'c')$ lgd = zip(tweet_df["Tweet Source"].unique(),colors)$
s_filled = s.fillna(0)$ s_filled
chance = cnf_matrix[1:].sum() / cnf_matrix.sum()
ch.setLevel(logging.WARNING)
time_series.info()
df = pd.read_csv(CSV_FILE_PATH,delim_whitespace=True)
stock['daily_gain'] = stock.close - stock.open$ stock['daily_change'] = stock.daily_gain / stock.open
history = model.fit(train_X, train_Y, epochs=num_epochs, batch_size=1, verbose=2, shuffle=False) $
pMean = np.mean([pNew,pOld])$ pMean
average_chart_lower_control_limit = average_of_averages - 3 * d_three * average_range / \$                                     (d_two * math.sqrt(subgroup_size))
collection.delete_item('AAPL')
tweets_df = tweets_df[tweets_df.userTimezone.notnull()]$ len(tweets_df)$
!wget -nv https://data.wprdc.org/datastore/dump/40776043-ad00-40f5-9dc8-1fde865ff571 -O 311.csv
df.groupby(['month']).agg([sum])$
df.groupby(['year','month']).agg([sum])$
import urllib3, requests, json, base64, time, os$ warnings.filterwarnings('ignore')
D2 = [(str(v), str(t).replace("|","")) for v,t in D2]$ D2[0:5]
measure_val_2014_to_2017.count()
ADNI_diagnosis_data_description = pd.read_csv('ADNIMERGE_DICT.csv')$ print(ADNI_diagnosis_data_description['FLDNAME'].unique())$ ADNI_diagnosis_data_description[['FLDNAME','TEXT']]
for active_add_date in daterange:$     active_add_rows = active_df[active_df['start_date']==active_add_date]$     cohort_active_activated_df.loc[active_add_date,active_add_date:] = len(active_add_rows) 
X_lowVar = df.copy()
dfDay = dfDay[(dfDay['Date'].dt.year == 2018) | (dfDay['Date'].dt.year == 2019)]
Measurement = Base.classes.measurement$ Station = Base.classes.station$
df_proj_agg = df_proj[['ProjectId','DivisionName','UnitName','Borough','Sponsor','PhaseName',$  'ProjectType', 'Priority', 'DesignContractType', 'ConstructionContractType',$  'SourceSystem', 'MultipleFMSIds', 'DesignFiscalYear','ConstructionFiscalYear']].drop_duplicates()
txt_tweets = txt_tweets[~txt_tweets.str.startswith('RT')]$ txt_tweets_position = tweets[~tweets['text'].str.startswith('RT')]['position']$ txt_tweets_position.value_counts()
!grep -A 50 "build_estimator" taxifare/trainer/model.py
Let's get started. First, let's import the Pandas package to use for the analysis. The Pandas package also allows the$
df1=pd.read_csv("../Raw Data/approval data clean values only.csv")$ df1.head()
temps_mosact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == most_activity[0], Measurements.date > year_ago).all()
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots$  $ temp_data["CLOSE"].iloc[:].plot() # Plot the adjusted closing price of AAPL
year_string = 'extracted_data/*.csv'
userMovies = moviesWithGenres_df[moviesWithGenres_df['movieId'].isin(inputMovies['movieId'].tolist())]$ userMovies
results.to_csv(path_or_buf=path + '/NFL_Fantasy_Search_2016_PreSeason.csv')
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets if tweet.lang == 'en'], columns=['Tweets'])$ display(data.head(10))
(events.query('doc_type=="CNI" & index < "20170110" & ~status')$       .head())
contextFreeList = [i/10 for i in range(1,3,step = 0.5)]$ for i in contextFreeList:$     print(i,tuning(w_contextFree = i))
r = requests.get('https://www.quandl.com/api/v3/datasets/OPEC/ORB.json?start_date=2013-01-01&end_date=2013-01-01&api_key='+API_KEY)
print "word    count"$ print "----------------------------\n"$ !hdfs dfs -cat /user/koza/hw3/3.2/issues/wordCount_part3/* | head -n 10
data.to_csv('prepared_data_with_cuts.csv', index = True)$
graf_train, graf_test=train_test_split(graf, test_size=.33, random_state=42)
NS_active_2015_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2015-06-01']$ NS_active_2016_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2016-06-01']$ NS_active_2017_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2017-06-01']
total = scores.sum()$ scores[:2.75].sum()/total
tweets_by_user_mentions = pd.read_sql_query(query, conn, parse_dates=['created_at'])$ tweets_by_user_mentions.head()
all_tables_df.loc[0]
retail_data = retail_data[['key', 'quantity', 'accountid']]$ retail_grouped = retail_data.groupby(['accountid', 'key']).sum().reset_index()
doc_duration = doc_duration.resample('W-MON').sum()$ RN_PA_duration = RN_PA_duration.resample('W-MON').sum()$ therapist_duration = therapist_duration.resample('W-MON').sum()
data.head()
sample = sample[sample['polarity'] != 2]$ sample['sentiment'] = (sample['polarity'] ==4).astype(int)
Base.classes.keys()
%matplotlib inline$ env_test.unwrapped.render('notebook', close=True)$ env_test.unwrapped.render('notebook')
grouped_authors_by_publication.tail()
min_open = ldf['Open'].min()$ min_open
pprint.pprint(test_collection.find_one())
df = pd.merge(df,rsvp_df,how="left",on="urlname")$ df.head()
periods = 31 * 24$ hourly = Series(np.arange(0,periods),pd.date_range('08-01-2014',freq="2H",periods=periods))$ hourly
df_course_tags = pd.read_csv('course_tags.csv')  $ df_user_assess_scores = pd.read_csv('user_assessment_scores.csv')$ df_user_course_views = pd.read_csv('user_course_views.csv')
pd.concat([msftA[:3], aaplA[:3]], ignore_index=True)
W.WISKI_CODES
PCA(2).fit(X)
psy = pd.get_dummies(psy, columns = cat_vars, drop_first=False)
model = gensim.models.Word2Vec(sentences, min_count=10)
from sklearn.linear_model import LinearRegression$ from sklearn.model_selection import train_test_split
messages = pd.read_csv('message_read.csv')
files = [f for f in listdir('Twitter_SCRAPING/scraped/') if f.endswith('.csv') and isfile(join('Twitter_SCRAPING/scraped/', f))]$ d_scrape = pd.concat([pd.read_csv('Twitter_SCRAPING/scraped/'+f, encoding='utf-8') for f in files], keys=files)$ print(d_scrape.head())
sl['second_measurement'] = np.where((sl.new_report_date==sl.maxdate) & (sl.mindate!=sl.maxdate),1,0)
password_str = ""$ with open("dbpass.txt", mode="r") as pass_f:$     password_str = pass_f.readline()
DummyDataframe = DummyDataframe.set_index("Date")$ DummyDataframe = DummyDataframe.apply(lambda x: update_values_category(x, "Tokens"), axis=1)$ DummyDataframe
df2_no_outliers = df2.copy()$     $ df2_no_outliers['y'] = np.log(df2_no_outliers['y'])
metrics = pd.read_json('../metadata/metrics.json')$ metrics
ms_df = mt_df[mt_df['mortality_salience'] == 1]$ print("HI tweets between 18:07-18:21 AM 2019-01-13 labelled as mortality_salience: {}".format(len(ms_df)))$ print("percent mortality salience: {}".format(len(ms_df)/len(mt_df)))
preci_df.set_index('date').head()$
df_students.shape
y_pred = regr.predict(X_test)
appended = ad_data1.union(ad_data2)
mapInfo = refl['Metadata']['Coordinate_System']['Map_Info'].value$ mapInfo
print(len(sessions_p_caption), len(sessions_p_cuepoint), sessions.id.count())$
client.training.get_details('training-WWmHAB5ig')
print(regression_model.coef_)$ print(regression_model.intercept_)
compound_df = pd.DataFrame(save_compound_list)$ compound_df = compound_df.transpose()$ compound_df.head()
tallies_file = openmc.Tallies()
y.end_time
df_bud = pd.read_csv(budFile, usecols = budCols, $                  dtype = bud_dtypes)
df_r2.loc[df_r2["CustID"].isin([customer])]
fig = plot2.get_figure()$ fig.savefig("output_figure.svg")
results['CharacteristicName'].value_counts()
plot_data = df['amount_tsh']$ sns.kdeplot(plot_data, bw=100)$ plt.show()
pivoted.T[labels == 1].T.plot(legend=False, alpha=0.1)
pivoted.T[labels == 0].T.plot(legend=False, alpha=0.1)
df.to_json('data\8oct_pre_processed_stemmed_polarity.json', orient='records', lines=True)
from sklearn.decomposition import LatentDirichletAllocation$ from sklearn.feature_extraction.text import CountVectorizer
df.to_csv('dataframe-kevin.csv')$
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=180)
len(calls_nocontact.location.unique())
model = gensim.models.Word2Vec(sentences, workers=4)
start_t = '2018-05-01 00:00:00'$ end_t=pd.to_datetime('today')- timedelta(days=1)$ end_t1=str(end_t)$
dc['created_at'] = pd.to_datetime(dc['created_at'], format='%Y-%m-%d %H:%M:%S')$ tm['created_at'] = pd.to_datetime(tm['created_at'], format='%Y-%m-%d %H:%M:%S')$ dc.created_at.dtype #tm.created_at.dtype
iex_coll_reference.count()
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Type"])
data_AFX_X.median()['Traded Volume']
plt.bar(x_axis, bar_Compound, color=["gold","blue","green","red","lightblue"], align="edge")
grouped_by_year_DRG.groupby(level=0)$
aldf = indeed1.append(tia1, ignore_index=True)
BuyingData = pd.read_excel('/Users/itsjustme/Desktop/BuyingBehaviourTwoYrs.xlsx')
cityID = 'bced47a0c99c71d0'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Durham.append(tweet) 
time6MDict = averagebytime(typesDict, '6M')$ time6MDict.keys()
df.index
rain_df = pd.DataFrame(rain)$ rain_df.head()
tweets.sort_values(by="frequency", ascending=True).head()
gmap = gmplot.GoogleMapPlotter(28.68, 77.13, 13)
print(data.petal_length.mode())
crimes.head()
val_avg_preds2 = np.stack(val_pred2).mean(axis=0)$ print ("type(val_avg_preds2):", type(val_avg_preds2), val_avg_preds2.shape)$ print(val_avg_preds2[0:10, :])
tweet_archive_clean['text'] = tweet_archive_clean['text'].apply(lambda x: x.split('https')[0])
dr_new_patient_8_to_16wk_arima = dr_new_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']]$ dr_new_patient_8_to_16wk_arima.index = dr_new_patient_8_to_16wk_arima.index.date
actual_value_second_measure=pd.DataFrame(actual_value_second_measure)$ actual_value_second_measure.replace(2,1, inplace=True)
mean = np.mean(data['len'])$ print("The average length of all tweets is: {}".format(mean))
delays = pd.read_csv('output.csv', header=None)$ delays.columns = ['Date','ID','Delay','Latitude','Longitude']$ delays_geo = GeoDataFrame(delays)
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
S_distributedTopmodel.decision_obj.thCondSoil.options, S_distributedTopmodel.decision_obj.thCondSoil.value
events_filtered_1 = events_enriched_df[events_enriched_df['yes_rsvp_count']>50]$ events_filtered_2 = events_df[(events_df['yes_rsvp_count']>50) & (events_df['venue.city']=='Chicago')]
X_nonnum = X_copy.select_dtypes(exclude=np.number)$ X_nnumcols = list(X_nonnum)$ print(X_nnumcols)
station = Base.classes.stations
X_copy['loc'] = X_copy['loc'].apply(lambda x: int(x))
tweet_df_clean = tweet_df.rename(columns={'id':'tweet_id'})
cur.execute("SELECT name FROM sqlite_master WHERE type='table';")$ print(cur.fetchall())
joined=join_df(joined,weather,["State","Date"])$ joined_test=join_df(joined_test,weather,["State","Date"])$ sum(joined['Max_TemperatureC'].isnull()),sum(joined_test['Max_TemperatureC'].isnull())
kyt_lat = 34.955205 #35.005205$ kyt_long = 135.675300 #135.7353$ diff_wid = (135.795300 - 135.675300)/grid_size$
cityID = 'a6c257c61f294ec1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Greensboro.append(tweet) 
model_uid = client.repository.get_model_uid(saved_model_details)$ print("Saved model uid: " + model_uid)
inter = est.get_prediction(X_tab.astype(float))$ inter = inter.summary_frame(alpha=0.05)[['mean_ci_lower','mean_ci_upper']]$ inter.rename(columns={ 'mean_ci_lower':'CI lower', 'mean_ci_upper':'CI upper'})
df_new[['US','UK']] = pd.get_dummies(df_new['country'])[['US','UK']]$ df_new.tail()
print(prec_long_df['date'].min(), prec_long_df['date'].max())
poparr.shape
S_1dRichards.decision_obj.groundwatr.options, S_1dRichards.decision_obj.groundwatr.value
kwargs = {'num_workers': 4, 'pin_memory': True} if args.cuda else {}$ kwargs
df.shape$ df.tail()
import nltk$ from nltk.corpus import stopwords$ print(stopwords.words('english'))
for i, row in companies.iterrows():$     print(companies['twitter'][i])
infoExtractionRequest = requests.get(information_extraction_pdf, stream=True)$ print(infoExtractionRequest.text[:1000])
news_p = soup.find("div", class_="rollover_description_inner").text
news_t = soup.find("div", class_="content_title").text
t = pd.read_hdf('../t.hdf','table')$ dd = pd.read_hdf('../dd.hdf','table')
df.head()
bd.index.name
search_key_words = "\"" + search_key_words.replace(" ","+") + "\""$ df['newspaper_search'] = df['newspaper'].apply(lambda x: x.replace("/","%2F"))
df_train = pd.merge(df_train, df_items, on='item_nbr', how='left')
corrplot(corr=r[coins_infund].loc[start_date:end_date].corr())$ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date)$ plt.show()
d.groupby(d['pasttweets'].str.len())['tweet_id'].count()
features=list(kick_projects_ip)$ features.remove('state')$ response= ['state']
station_count = session.query(func.count(distinct(Measurement.station))).all()$ station_count$
data.loc[data.PRECIP.isnull()]
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
Twitter_map2.save('Twitter_FSGutierres_map.html') #save HTML
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(RN_PA_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours First Difference Autocorrelation')
len_plot = t_len.plot(figsize=(16,4), label="Length", color='r', legend=True, title='Length of tweets over time')$ len_vs_time_fig = len_plot.get_figure()$ len_vs_time_fig.savefig('len_vs_time.png')
print 'No duplicate IDs' if len(user_df.id.unique()) == len(user_df) else 'Duplicate IDs exist'
year15 = driver.find_elements_by_class_name('yr-button')[14]$ year15.click()
df_final = df_grouped.merge(df_schoo11, on='school_name', how='left')$ df_final.drop(['budget_x', 'size_x', 'School ID','size_y', 'budget_y'], axis = 1, inplace=True)$ df_final
import pandas as pd$ git_log = pd.read_csv('datasets/git_log.gz', sep='#', header=None, encoding='latin-1', names=['timestamp','author'])$ print(git_log.head(5))
response = chatbot.get_response("Where are you")$ print(response)
file_name = "./data/train_preprocessed2.csv"$ train_df2 = pd.read_csv(file_name, low_memory = False)$ train_df2.head()
plt.savefig(str(output_folder)+'NB01_5_NDVI02_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice02_str)
stream = tweepy.Stream(auth, l)
allItemIDs = np.array(allData.map(lambda x: x[1]).distinct().collect())$ bAllItemIDs = sc.broadcast(allItemIDs)
df3['Current Status'].unique()
sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df.loc[sprintsWithStoriesAndEpics_df.groupby("key_story")["startDate"].idxmax()]$ sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[pd.notnull(sprintsWithStoriesAndEpics_df.index)]$
n_old = df2[df2.group == "control"].count()[0]$ print("The population of user under treatment group: %d" %n_old)
btc = pd.read_csv('/home/rkopeinig/workspace/Time-Series-Analysis/data/btc.csv')$ btc['date'] = pd.to_datetime(btc['date'])$ btc = btc.set_index('date')
mydata.iloc[0] 
ts.get_k_data(code="sh",start="2016-08-02",end="2018-08-21")
model_w = sm.formula.ols('y ~ C(w)',data=df).fit()$ anova_w_table = sm.stats.anova_lm(model_w, typ=1)$ anova_w_table.round(3)
pvt = pvt.drop(['ga:dimension2', 'customerId'], axis=1)$ pvt = pvt[['ga:transactionId', 'ga:date', 'customerName', 'productAndQuantity']]$ pvt
df_new['intercept']=1$ df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
temps_df.loc['2018-05-02'].index
results = model_selection.cross_val_score(gnb, X_test, Y_test, cv=kfold)$ results.mean()
mydata.head()
walk.resample("1Min", closed="right")
for tweet in query2:$     if replies_blm.get(tweet.in_reply_to_status_id_str) != None:$
test[['clean_text','user_id','predict']][test['user_id']==1895520105][test['predict']==10].shape[0]
bucket.upload_dir('data/heat-pump/raw/', 'heat-pump/raw', clear_dest_dir=True)
soup.get_text()
n_user_days.value_counts().sort_index()
print example1_df.printSchema()
df.head(10)
opening_prices = [x for x in opening_prices if x is not None]$ print('Highest opening price - {} \nLowest opening price - {}'.format(max(opening_prices), min(opening_prices)))
with open('key_phrases.pickle', 'wb') as f:$     pickle.dump(key_phrases, f, pickle.HIGHEST_PROTOCOL)
req = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY)$ json_data = req.json()
lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]$ print(lemmed)
measurements_df.head()
Base.prepare(engine, reflect=True)
print "Total number of rows of table 'photoz_bcnz': ", len(df3) $ print df3.columns.values$ df3
current_img = current_img_link[0]['style']$ print(current_img.find("('"))$ print(current_img.find("')"))
projFile = "Projects.csv"$ schedFile = "Schedules.csv"$ budFile = "Budgets.csv"
logit2 = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']])$ result = logit2.fit()$ result.summary2()
print "https://twitter.com/AirbnbHelp/status/{}".format(tweets_df['09/20/2017'].iloc[0]['twitter_id'])
stemmer = nltk.stem.porter.PorterStemmer()$ %timeit articles['tokens'] = articles['tokens'].map(lambda s: [stemmer.stem(w) for w in s])
tweet_group_df = tweet_df.groupby(["Target"])['Compound'].agg(['mean']).sort_index().reset_index()$ tweet_group_df = tweet_group_df.rename(columns={"mean":"Tweet Polarity"})$ tweet_group_df.head()
repeat_customer_purchase_timing['first last gap'].hist()
id_of_tweet = 932626561966247936$ tweet = api.get_status(id_of_tweet)$ print(tweet.text)
X = [string.replace('\n', ' ') for string in X]
path = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=fr89z8vqESGWrVzvNFxC&start_date=2017-01-01&end_date=2018-01-01?api_key=API_KEY'$ r = requests.get(path).json()
snap_data = df_snap.copy()
crypto_combined = pd.concat([crypto_data, crypto_ggtrends], axis=1).dropna(how='any')$ crypto_combined_s = crypto_combined.copy(deep=True)$ print(crypto_combined_s.head(10))
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key={}".format(API_KEY))$ print(r.json()['dataset']['data'][0])
DataSet = DataSet[DataSet.userName.notnull()]$ len(DataSet)
from templates.invoicegen import create_invoice
crime['Sex'].value_counts()
tdf = sns.load_dataset('tips')$ tdf['size'].sample(5)
vip_reason.columns = ['VIP_'+str(col) for col in vip_reason.columns]
red_4['created_utc'] = red_4['created_utc'].astype('datetime64[s]')$ red_4['time fetched'] = red_4['time fetched'].astype('datetime64[s]')$ red_4.head()
round((timelog.seconds.sum() / 60 / 60), 1)
tweet_df_polarity = tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
df.num_comments = df.num_comments.astype(int)
cur.execute(query)$ cur.fetchall()
group = {}$ for letters in list_of_letter:$     group[letters] = grouped_by_letter.get_group(letters)$
taxdata_blocks = pd.merge(taxdata,parcel_blocks, how='left', left_on=['pin'], right_on=['PIN'])$ taxdata_blocks = taxdata_blocks.drop(['pin','PIN'],axis=1)$ taxdata_blocks = taxdata_blocks.dropna(subset=['TRACTCE10','BLOCKCE10'])
predict.predict_score('Water_fluoridation')
from google.cloud import bigtable$ client     = bigtable.Client.from_service_account_json(JSON_SERVICE_KEY,project=project_id, admin=True)$ instance   = client.instance(instance_id)$
from dotce.visual_roc import roc_curve, precision_recall_curve
universe = ["SAP.DE", "UN01.DE", "BAS.DE"]$ price_data = yahoo_finance.download_quotes(universe)
aapl = pd.read_excel("../../data/stocks.xlsx", sheetname='aapl')$ aapl.head()
(training,testing) = clean_data.randomSplit([0.7,0.3])
store_items = store_items.drop(['watches', 'shoes'], axis = 1)$ store_items
df[df.Target == 5]
url_c = "https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"$ c = pd.read_csv(url_c)$ c
elms_all_0604 = pd.read_excel(cwd+'\\ELMS-DE backup\\elms_all_0604.xlsx')$ elms_all_0604['ORIG_DATE'] = [datetime.date(int(str(x)[0:4]),int(str(x)[5:7]),int(str(x)[8:10]))$                              for x in elms_all_0604.ORIG_DATE.values]
marsweath_url = 'https://twitter.com/marswxreport?lang=en'
np.shape(prec_fine)
sentences = [['first', 'sentence'], ['second', 'sentence']]$ model = gensim.models.Word2Vec(sentences, min_count=1)
news_period_df.loc[3, 'news_title']
nitrodata['Year'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).year$ nitrodata['Month'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).month
topLikes.sort_values(by='Likes', ascending=[False])
miner = TweetMiner(api, result_limit=200)$ trump_tweets = miner.mine_user_tweets("realDonaldTrump", max_pages=14)
prcp = Base.classes.prcp$ tobs = Base.classes.tobs$ stations = Base.classes.stations
%%time$ max_key = max( r_dict.keys(), key = get_nextday_chg )$ print('largest change in price between any two days: '+ str( get_nextday_chg(max_key) ) )
for col in user_df.columns:$     print col, len(user_df[user_df[col].isnull()])
df = pd.read_sql('SELECT * from payment', con=conn_b)$ df
m_vals = np.linspace(m_true-3, m_true+3, 100) $ c_vals = np.linspace(c_true-3, c_true+3, 100)
for tweet in negativeTweets['Tweets'].iteritems():$     print (tweet)
min(close, key=close.get)
df = pd.DataFrame()$ df
grouped_df = xml_in.groupby(['authorId', 'authorName'], as_index=False)['publicationKey'].agg({'countPublications': 'count'})
root_universe = openmc.Universe(name='root universe', cells=[cell])
df.date[0]
!pip install --upgrade tensorflow==1.4
grouper = dta.groupby(dta.results)
df_from_csv.equals(df)
noise = [np.random.normal(0,noise_level*p,1) for p in weather.power_output]
marvelPTags = wikiMarvelSoup.body.findAll('p')$ for pTag in marvelPTags[:8]:$     print(pTag.text)
model.wv.most_similar("man")$
session.query(func.count(Station.station)).all()
u235_scatter_xs = fuel_xs.get_values(nuclides=['(U235 / total)'], $                                 scores=['(scatter / flux)'])$ print(u235_scatter_xs)
clean_merge_df.to_csv("Desktop/Project-2/spotify_merged_data.csv", index=False, header=True)
temp_wide_df = pd.concat([grid_df, temp_df], axis = 1)$ temp_wide_df.head()
import matplotlib.pyplot as plt
plt.pie(total_fare, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()$
import datetime $ now = datetime.datetime.now()$ print now
df_plot = df_recommended_menues.sort_values(by=['dates'])
auth = tweepy.OAuthHandler(api_key, api_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, wait_on_rate_limit_notify=True, wait_on_rate_limit=True)
nmf_tfidf_topic6_sample = mf.random_sample(selfharmmm_final_df, criterion1='non_lda_max_topic', value1='nmf_tfidf_topic6', use_one_criterion=True)
results.summary2()
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,200], ylim=[0,200], title="Favorites and Retweets:200x200")
with open(marvel_comics_save, mode='w', encoding='utf-8') as f:$     f.write(wikiMarvelRequest.text)
float(df.converted.sum())/df_length
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);$
Counter(tag_df.values.ravel()).most_common(5)
from scrapy.selector import Selector
p_diffs = np.array(p_diffs)$ p_diffs
data_set.to_csv("C://SHERYL MATHIAS/US Aug 2016/Fall 2017/INFM750/HillaryClintonTweets.csv", index=False, encoding='utf-8')
DATADIC = pd.read_csv('DATADIC.csv')$ list(DATADIC['FLDNAME'].unique())$ DATADIC[['FLDNAME', 'TEXT']].head()
events_df['event_day'] = events_df['event_time'].apply(lambda d: d.replace(hour=0,minute=0,second=0))$ events_df['event_week'] = events_df['event_day'].apply(lambda d: d - datetime.timedelta(d.weekday()))$ events_df['event_weekday'] = events_df['event_day'].apply(lambda d: d.weekday())
index = pd.to_datetime(non_na_df['created_at'])$ non_na_df.index = index
if not os.path.isdir('output/heat_demand'):$     os.makedirs('output/heat_demand')
Suburban = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Suburban")]$
x = np.arange(8).reshape(4,2)$ y = x.flatten()$ y[[0, 1, 5]]
df.head()
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])
def day_of_week(date):$     days_of_week = {0: 'monday', 1: 'tuesday', 2: 'wednesday', 3: 'thursday', 4: 'friday', 5: 'saturday', 6: 'sunday'}$     return days_of_week[date.weekday()]
status_data = pandas.read_csv("./Dataset Processed/mypersonality_final_classifiedByClass_onlyColumn.csv",encoding='cp1252')$
df_categories = pd.read_csv('categories.csv')$ df_categories.head()
post_discover_sales[post_discover_sales['Email'] == 'Jennyann57@yahoo.com.sg']
temp_df2['timestamp'] = pd.to_datetime(temp_df2['timestamp'],infer_datetime_format=True)
unique_words_sk = set(words_sk)$ corpus_tweets_streamed_keyword.append(('unique words', len(unique_words_sk))) # update corpus comparison$ print('Number of unique terms: ', len(unique_words_sk))
a=contractor_clean.groupby(['contractor_id','contractor_bus_name'])['contractor_bus_name'].nunique()$ print a[a >1]$ contractor_clean[contractor_clean.contractor_number.duplicated() == True]
df["mail ID"] = df["mail ID"].apply(lambda x: x.split('.')[0])$ df = df.set_index("mail ID")
mars_images = requests.get('https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars')$ mars_soup = BeautifulSoup(mars_images.content, 'html.parser')$
prophet_model.fit(prophet_df)
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)
df_cod2 = df_cod.copy()$ df_cod2 = df_cod2.dropna()
display(flight2.show(5))$
conn = psycopg2.connect("dbname='pgsdwh' user='502689880' host='alpgpdbgp2prd.idc.ge.com' password='pass66824w'")
dfs.sort_values(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True, ascending=False)$ dfs.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True)
tweets_df.iloc[3, 10]
tallies_file.export_to_xml()
ndvi_change= ndvi_of_interest02-ndvi_of_interest$ ndvi_change.attrs['affine'] = affine
f = open('..\\Output\\GenreString.csv','w')$ f.write(GenresString) #Give your csv text here.$ f.close()
X['is_cat'] = X['title'].map(lambda x: 1 if 'cat' in x else 0)$ X['is_funny'] = X['title'].map(lambda x: 1 if 'funny' in x else 0)
DATA_PATH = "~chandler.mccann/Downloads/"$ INPUT_FILE = os.path.join(DATA_PATH, "cleaned_water_data2.csv") #after running prep_water_data.py$
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)$ tf.summary.scalar("loss", loss)$ merged_summary_op = tf.summary.merge_all()
inter_by_date = niners.groupby('Date')['InterceptionThrown'].sum()$ inter_by_date;
DummyDataframe2 = DummyDataframe2.apply(lambda x: calPercentage(x, "Token_Count", ["Positiv", "Negativ"]), axis=1)$ DummyDataframe2
t_len = pd.Series(data=data['len'].values, index=data['Date'])$ t_fav = pd.Series(data=data['Likes'].values, index=data['Date'])$ t_ret = pd.Series(data=data['RTs'].values, index=data['Date'])
df_CLEAN1A['AGE_groups'] = df_CLEAN1A['AGE_groups'].astype('category')
lon_us = lon[lon_li:lon_ui]$ lat_us = lat[lat_li:lat_ui]$ print(np.min(lon_us), np.max(lon_us), np.min(lat_us), np.max(lat_us))
auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)$ auth.secure = True$ api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
pd.options.display.max_colwidth = 400$ data_df[['clean_desc','sent_pola','sent_subj', 'tone']][data_df.tone == 'impolite']
!wget http://files.grouplens.org/datasets/movielens/ml-latest.zip
p_new = df2['converted'].mean()
print(data_tokenized.shape)$ print(label.shape)$ searchParameters(data_tokenized, np.ravel(label), MLPClassifier(max_iter=500, tol=0.00001), container, text='mlp')
events_per_day = events_df[['event_day','event_id']].groupby('event_day').count()$ events_per_day.rename(columns={'event_id':'count_event_day'},inplace=True)$ events_per_day.reset_index(inplace=True)
g_goodbad_index = sl_data.groupby(['goodbad','AGE_groups']).sum()$ g_goodbad_index
df_chapters_read['referer'].value_counts()
actualDiff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean()$ (actualDiff < pDiffs).mean()
twitter_page = requests.get("https://twitter.com/marswxreport?lang=en")$ twitter_soup = BeautifulSoup(twitter_page.content, 'html.parser')$ mars_weather = twitter_soup.find('p', class_="tweet-text").string
tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')$ tips['tip'] = pd.to_numeric(tips['tip'], errors = 'coerce')$ print(tips.info())$
data_issues.columns
!tar -xzvf cudnn-8.0-linux-x64-v7.1.tgz
i = np.random.randint(x.shape[0]-1)$ m_star = m_star + 2*learn_rate*(x[i]*(y[i]-m_star*x[i] - c_star))$ c_star = c_star + 2*learn_rate*(y[i]-m_star*x[i] - c_star)
nu_fission_rates = fuel_rxn_rates.get_slice(scores=['nu-fission'])$ nu_fission_rates.get_pandas_dataframe()
from pyspark.sql.types import DoubleType, IntegerType$ for col_name in data.columns:$     data = data.withColumn(col_name, data[col_name].cast(DoubleType()))
print("Predicted:", "\n",' '.join(y_pred[0]))$ print('\n')$ print("Correct:", "\n" ,' '.join(sent2labels(sample_sent)))
df['messages'].value_counts()
df[df['text'].str.contains('appointment')]
X_test = count_vect.transform(df_test.text)
rddScaledScores.reduce(lambda s1,s2: s1 + s2) / rddScaledScores.count()
results = Geocoder.reverse_geocode(41.9028805,-87.7035663)
transform = am.tools.axes_check(np.array([x_axis, y_axis, z_axis]))$ b = transform.dot(burgers)$ print('Transformed Burgers vector =', b)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep ='\s+') # Index in the first column "col=0" and header on the first "row"$ df.head(5)$
from sklearn.model_selection import train_test_split$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
QUIDS_wide.drop(labels=["qstot_12","qstot_14"], axis = 1, inplace=True)
%config InlineBackend.figure_format='svg'$ plt.plot(x, np.sin(x)/x)
crimes['year'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.year)
print(type(dicts))$ print(dicts.keys())
gbm = H2OGradientBoostingEstimator()$ gbm.train(['sepal_len','sepal_wid','petal_len','petal_wid'],'class',train)
df.tail()$
data['Open'].max()
lgb_mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', $           objective = 'binary')$
input_nodes_DF=nodes_table('network/source_input/nodes.h5', 'inputNetwork')$ input_nodes_DF[:5]
vacancies['weekday'] = vacancies['created'].apply(lambda x: x.weekday() + 1)$ vacancies['hour'] = vacancies['created'].apply(lambda x: x.hour)
chained2 = itertools.chain.from_iterable([[1, 2, 3, 4], [5, 6, 7, 8]])$ for letter in chained2:$     print(letter)
experience = pd.get_dummies(questions['rate_experience'])
tweets['id'].groupby(pandas.to_datetime(tweets['created_at']).dt.date).count().mean()
engine.execute('SELECT * FROM station LIMIT 10').fetchall()
city_dict = {}$ for city in cities_list:$     city_dict[city] = api.geo_search(query = city, wait_on_rate_limit=True, granularity = 'city')[0].id
p_new = df2[df2['landing_page']=='new_page']['converted'].mean()$ print("Probability of conversion for new page (p_new):", p_new)
lr = LogisticRegression(random_state=20, max_iter=10000)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr','multinomial'], 'solver':['saga','newton-cg', 'lbfgs']}$ grid = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
df_members['bd_c'] = pd.cut( df_members['bd'] , age_bins, labels=age_groups)
station_distance.insert(loc=11, column='Distance(Miles)', value=distance)
print('\n Row x Columns of imported data:')$ print(dfa.shape)$ print(type(dfa))
def cosine_similarity(u, v):$     return(np.dot(u, v)/np.sqrt((np.dot(u, u) * np.dot(v, v))))
options = webdriver.ChromeOptions()$ options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome')
from IPython.display import FileLink$ FileLink(str(FLASK_PATH/'final_df.csv'))
len([b for b in BDAY_PAIR_df.pair_age if b<0 ])
top_subreddit = subreddit.new(limit=None)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=uEEsBcH3CPhxJazrzGFz&start_date=2017-01-01&end_date=2017-12-31")$ r.json()
params =  {'reg_alpha': 0.1, 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'min_child_weight': 1$           , 'n_estimators': 300, 'reg_lambda': 1.0, 'random_state': 1, 'max_depth': 4} $ reg_final = xgb.XGBRegressor(**params).fit(X, y)#**params$
data['intra_day_diff'].max()
filename_2018 = os.path.join(input_folder, string_2018)
stops.loc[~mask].head(10)
le_data_all = wb.download(indicator="SP.DYN.LE00.IN",country=countries['iso2c'],start='1980',end='2012')$ le_data_all
bigram_chunker = BigramChunker(train_trees)$ print(bigram_chunker.evaluate(valid_trees))
y.head()
dfCr = pd.read_csv('data/CC2017.csv')$ dfCr = dfCr[dfCr['Transaction']=='CREDIT'] $ dfCR= dfCr[dfCr['Memo'].notnull()] # remove null entries
scores_mode = scores.sort_values(ascending=False).index[0]$ print('The mode is {}.'.format(scores_mode))
sort_df.tail(10)
data.shape
print("Mean absolute percentage error: %.3f" % mean_absolute_percentage_error(y_test, y_pred) + '%')
haw.to_csv('./haw_python.csv',index=True)
plot_autocorrelation(series=dr_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr hours new patients'))$ plot_autocorrelation(series=dr_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr hours existing patients'))
c>0.7
%%time$ max_key = max( r_dict.keys(), key = get_daily_chg )$ print('largest change in price in any one day: '+ str( get_daily_chg(max_key) ) )
X_copy['avg_six_mnth_cnt'] = X_copy['avg_six_mnth_cnt'].apply(lambda x: float(x))
reddit_master.shape
npath = save_filepath + '/uva_hpc/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
df['cowbell'] = df['body'].apply(lambda x: len([x for x in x.split() if '(((' in x]))
cur.fetchall()
slist = [s1, s2, s3]$ for item in slist:$     item.name == 'NAME':$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
df = pd.read_sql('SELECT * from booking', con=conn_b)$ df
hits_df = number_one_df.drop_duplicates(subset=['URL','Region'], keep ='first')
psa_proudlove = pd.read_csv('psa_proudlove.csv', parse_dates=True, index_col='Date')$ psa_perry = pd.read_csv('psa_perry.csv', parse_dates=True, index_col='Date')$ psa_all = pd.read_csv('psa_all.csv', parse_dates=True, index_col='Date')
stoplist = stopwords.words('english')$ stoplist.append('free')$ print(stoplist)
ms_columns = inspector.get_columns('measurements')$ for c in ms_columns:$     print(c['name'], c["type"])$
df['time_detained'] = df['time_detained'] / np.timedelta64(1,'D')
airline_df['sentiment'] = airline_df['tweet'].apply(lambda tweet: NBClassifier.classify(extract_features(getFeatureVector(processTweet2(tweet)))))$
qt_ind_ARR[industries] = qt_ind_ARR[industries].applymap(lambda x: np.float(x))$ qt_ind_ARR
breast_cancer_df = pd.read_csv('data/breast_cancer.csv')$ breast_cancer_df.head()
df_concat_2["time"].str.split(':')$ df_concat_2["time"] = df_concat_2["time"].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
march_2016 = pd.Period('2016-03', freq='M')$ print march_2016.start_time$ print march_2016.end_time
print data.mean()
df2['low_tempf'].mean()
np.random.seed(1)
temp_df = df.copy()$ temp_df.index = df.index.set_names('Desc.', level='Description')$ temp_df.head(3)
df2["Temperature Groups"] = pd.cut(df2["tobs"], bins, labels=bin_names)$ df2_grp1=df2.groupby("Temperature Groups").count()$ df2_grp1["tobs"]
cursor.execute('SELECT * FROM fib')$ print(cursor.fetchall())
ts = pd.Timestamp(datetime.datetime(2016,5, 12, 3, 42, 56))$ ts
ftr_imp_rf=zip(features,trained_model_RF.feature_importances_)$ for values in ftr_imp_rf:$     print(values)
print(dfd.hspf.describe())$ dfd.hspf.hist()
n_bandits = gb.size().shape[0]$ test_labels = [i for i in gb.size().index]$ mcmc_iters = 25000
stat_info_city = stat_info[1].apply(fix_space)$ print(stat_info_city)
pd.Series(HAMD.min(axis=1)<0).value_counts()  # no
movies_df = pd.read_csv('movies.csv')$ ratings_df = pd.read_csv('ratings.csv')$ movies_df.head()
from sklearn import model_selection$ kfold = model_selection.KFold(n_splits=10, shuffle=True)$ loocv = model_selection.LeaveOneOut()
df.median() - 1.57 * (df.quantile(.75) - df.quantile(.25))/np.sqrt(df.count())
gdax_trans_btc['Balance'] = gdax_trans_btc['Trade_amount'].cumsum();
msft.dtypes
tm_2050 = pd.read_csv('input/data/trans_2050_m.csv', encoding='utf8', index_col=0)
trip_month_day = trip_dates.strftime('%m-%d')$ trip_month_day
for h in heap:$     h.company = [t.author_id for t in h.tweets if t.author_id in names][0]
weather_features = weather_features.resample('D').sum()
print(type(df_final['created_time'][0]))$ pd.__version__
slicer.apply(np.mean, axis=1)
import random$ sample=movie1['ceiled_ratings'].tolist()$ x = np.linspace(1,5,100)
ds_info = ingest.upload_dataset(database=db,$                                 dataset=test,$                                 validation_map={"bools": lambda x: isinstance(x, str)})
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL')$                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1))$                           &(extract_nondeduped_cmp.app_branch_state=='OH')].groupby('APP_PROMO_CD').size()
us_rhum = rhum_fine.reshape(844,1534).T #.T is for transpose$ np.shape(us_rhum)
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S.executable = excutable +'/summa.exe'
_delta=_delta.to_dict()
query.get_dataset(db, id=ds_info["DatasetId"][0])
sns.set_style('whitegrid')$ sns.distplot(data_final['countPublications'], kde=False,color="red") # bins=30, $
np.linspace(0, 100, 5)
wordsearch_dict = {}$ for word in order.keys():$     wordsearch_dict.update({word: df.text.str.extractall(r'({})'.format(word), re.IGNORECASE).size})
print contractor_clean['zip_part1'].head(); contractor_clean['zip_part2'].head()$
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(RN_PA_duration, params=params, freq=31, title='RN/PA Decomposition')
preds = lm.predict(X_new)$ preds
dayofweek = pd.DatetimeIndex(pivoted.columns).dayofweek
ad_source = ad_source.drop(['[', ']'], axis=1)$ ad_source = ad_source.drop(ad_source.columns[0], axis=1)
raw_scores = df.rating_score[df.rating_score.notnull()]
df = pd.read_json("data\8oct_pre_processed_stemmed.json", orient='records', lines=True)
sorted_posts = gram_collection.find({"contains_tattoo": 1}).sort([("likes", pymongo.DESCENDING)])
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','country_UK','country_US']])$ r=lm.fit()
intervention_train.isnull().sum()
ndvi_us = ndvi_nc.variables['NDVI'][0, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(ndvi_us)
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']])$ lm.fit().summary2()
new_df = df.dropna(how = 'all')$ new_df
pd.set_option("max.rows", 10)$ result
save_filepath = '/media/sf_pysumma'$ hs_path = save_filepath+'/a0105d479c334764ba84633c5b9c1c01/a0105d479c334764ba84633c5b9c1c01/data/contents'$ S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
from statsmodels.tsa.arima_model import ARIMA$ model_6203 = ARIMA(dta_690, (3, 1, 0)).fit() $ model_6203.forecast(5)[:1] 
plt.figure(figsize=(15,8))$ ax = sns.boxplot(x = 'Type 1', y = 'Total', data = pokemon )$ plt.show()
stocks['Date'] = stocks['Date'].dt.week
wedate=data['Week Ending Date']$ wedate.head(10)
print('RF:', rf.score(X_test, y_test))$ print('KNN:', knn.score(X_test, y_test))
my_data.shape$
with open('key_phrases_rake.pickle', 'wb') as f:$     pickle.dump(key_phrases_rake, f, pickle.HIGHEST_PROTOCOL)
future = m.make_future_dataframe(periods=90)$ future.tail()
date = session.query(prcp.date).order_by(prcp.date.desc()).first()$ print(date)
from sklearn.metrics import f1_score$ f1_score(y_test, y_pred_lgr, average='macro')  
x = topics_data.comms_num$ y = topics_data.score$ print("Correlation between Number of Comments and Total Score is:", round(np.corrcoef(x, y)[0,1], 2))$
news_df = pd.DataFrame(news_dict)$ news_df.head()
df_corr = result.groupby(['type', 'scope'])['user'].sum().reset_index()$ display(df_corr.sort_values('user',ascending=False).head(10))$ plot2D(df_corr, 'type', 'scope','user')
int_and_tel = cw_df.loc[cw_df['category'] == 'Internet & Telecom']$ int_and_tel.sort_index(axis= 0, inplace= True)$ int_and_tel.reset_index(inplace= True, drop= True)
file = 'https://assets.datacamp.com/production/course_2023/datasets/dob_job_application_filings_subset.csv'$ df = pd.read_csv(file)$ print(df.head())
cityID = 'f995a9bd45d4a867'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Memphis.append(tweet) 
dr_existing_patient_8_to_16wk_arima = dr_existing_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']]$ dr_existing_patient_8_to_16wk_arima.index = dr_existing_patient_8_to_16wk_arima.index.date
cgm = cgm.rename(columns={"value": "mmol_L"})$ cgm["mg_dL"] = (cgm["mmol_L"] * 18.01559).astype(int)$ cgm.mg_dL.head()
data.learner_id.value_counts()[:1]
happiness_df=pd.DataFrame(columns=['dates','happiness'])$ for j in range(0,len(happiness)):$             happiness_df.loc[j]=[str(time[j])+"-12-31"+"T00:00:00Z",happiness[j]*33.33]$
sim_ET_Combine = pd.concat([simResist_rootDistExp_1, simResist_rootDistExp_0_5, simResist_rootDistExp_0_25], axis=1)$ sim_ET_Combine.columns = ['simResist(Root Exp = 1.0)', 'simResist(Root Exp = 0.5)', 'simResist(Root Exp = 0.25)']
df = pd.DataFrame.from_dict(dst_cap, orient="index")$ df.columns = ['Fequency']
pd.concat([msftA[:5], aaplA[:3]], axis=1, join='inner', keys=['MSFT', 'AAPL'])
afx_x_oneday = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2014-12-01&end_date=2014-12-01&api_key=F57SPh3gyaXMHjLAp-pY')$
print(soup.prettify())
json_data = r.json()$ json_data
predictions.show()
dollars_per_unit.columns = pd.MultiIndex.from_product([['Dollars per Unit'], dollars_per_unit.columns])$ pd.concat([multi_col_lvl_df, dollars_per_unit], axis='columns').head(3)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()$
Mars_Weather_URL = 'https://twitter.com/MarsWxReport/status/1017925917065302016'$ Weather_response = requests.get(Mars_Weather_URL)$ Weather_soup = BeautifulSoup(Weather_response.text, 'html.parser')
qs = qs.join(features, how="inner", rsuffix="_r")$ qs.head()
import sys$ sys.path.append('../')$ sys._enablelegacywindowsfsencoding() 
retweet_plot = t_ret.plot(figsize=(16,4), label="Retweets", color="g", legend=True, title='Number of retweets for tweets over time')$ retweet_vs_time_fig = retweet_plot.get_figure()$ retweet_vs_time_fig.savefig('num_ret_over_time.png')
annual_returns.head() $
validation.analysis(observation_data, Jarvis_resistance_simulation_1)
%%time$ reader = pa.RecordBatchFileReader(data_dir + file_name + '.arrow')$ read_table = reader.read_all()
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))$ colors
selected_features=selected.index$ X_train_new=X_train[selected_features]$ X_test_new=X_test[selected_features]
df['NDATE']=pd.to_datetime(df['DATE'],unit='s')
files_txt = glob.glob("*.txt")
from sqlalchemy import create_engine, func, inspect$ inspector = inspect(engine)$ inspector.get_table_names()$
ibm_hr_int = ibm_hr_target.select(numerical)$ ibm_hr_int.show(3)
((pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days)*365
prcip_df.describe()$
importances=model_tree_6_b.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
reddit = praw.Reddit(client_id=keyVars[0], client_secret=keyVars[1], password=keyVars[2],$                      user_agent=keyVars[3], username=keyVars[4])
df_amznnews['publish_time'] = df_amznnews['publish_time'].astype(str)$ df_amznnews['publish_time'] = pd.to_datetime(df_amznnews['publish_time'], format='%Y%m%d%H%M')$ df_amznnews.info()$
merged_portfolio_sp_latest = pd.merge(merged_portfolio_sp, sp_500_adj_close, left_on='Latest Date', right_on='Date')$ merged_portfolio_sp_latest.head()
faa_data_destroyed_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "D"]$ print(faa_data_destroyed_damage_pandas.shape)$ faa_data_destroyed_damage_pandas
sentiment=sentiment.sort_values(by='Tweet_date', ascending = False)$ sentiment.head()
crime = crime[crime.Sex != 'U']
dfFull['OverallCondNorm'] = dfFull.OverallCond/dfFull.OverallCond.max()
auth_key = 'yZVrH8Esn8zs7vGPN2zJ'$ main_df = pd.DataFrame # Lets create a blank dataframedf = quandl.get("FMAC/HPI_KOKIN", authtoken=auth_key)
drop_list = ['FEDFUNDS','DGS1MO','DGS3MO','DGS6MO','DGS1','DGS2','DGS3']$ for drop_x in drop_list:$     df.drop(drop_x, axis=1, inplace=True)
Base = automap_base()$ Base.prepare(engine, reflect=True)
afx_volume = [day[6] for day in afx_dict['dataset_data']['data']]$ afx_avg_vol = sum(afx_volume)/len(afx_volume)$ print("The average daily trading volume for AFX_X in 2017 was " + str(round(afx_avg_vol,2)) + ".")
plt.savefig(str(output_folder)+'NB01_7_windfield_vs_NDVIchange'+str(cyclone_name)+'_'+str(location_name))
len(calls.location.unique())
df2.tail(2)
data.keys()
top_songs['Month'] = top_songs['Date'].dt.month
import pandas as pd$ weather = pd.DataFrame(weather_json['data'])$ weather['date']  = pd.to_datetime(weather['date'])
dr_test_data = dr_test_data.resample('W-MON').sum()$ RN_PA_test_data = RN_PA_test_data.resample('W-MON').sum()$ therapist_test_data = therapist_test_data.resample('W-MON').sum()
idx = data['dataset_data']['column_names'].index('Open')$ open_price = [day[idx] for day in data['dataset_data']['data'] if day[idx]]$ print('Highest and lowest opening prices in 2017 were {} and {}'.format(max(open_price), min(open_price)))
overall_gps = pd.DataFrame.from_dict(dist_df,orient='columns')
predict_day = weather_x.index[-60]
df.in_response_to_tweet_id.isnull().sum()
cityID = '8e9665cec9370f0f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Minneapolis.append(tweet) 
df = pd.read_sql(SQL, db)$ df
x = x.assign(A2 = x["A"]**2)$ x
t1.name.value_counts()
data_path = 'Crimes_-_2001_to_present.csv'$ df = spark.read.csv(data_path, sep = ',', header = True)$ df.take(1)
file_names = []$ file_names = glob.glob('*.csv')
scores[scores.RottenTomatoes == scores.RottenTomatoes.max()]
X = pd.get_dummies(X, columns=['subreddit'], drop_first = True)
df_new[['ca','uk','us']] = pd.get_dummies(df_new['country'])
for col in response_df.columns:$     if response_df[col].dtype in (object, str):$         print col, pd.np.where(response_df[col] == '', 1, 0).sum()
vec1 = modelD2V.infer_vector(currentData[0].words)$ vec2 = modelD2V.infer_vector(currentData[1].words)$
TEST_DATA = SHARE_ROOT + 'test_dataframe.pkl'$ df.to_pickle(TEST_DATA)
temp_df2['timestamp'].max() - temp_df2['timestamp'].min()
stations_df.head()
cur = conn.cursor()$ cur.execute('UPDATE actor SET first_name = CASE WHEN first_name = \'HARPO\' THEN \'GROUCHO\' ELSE \'MUCHO GROUCHO\' END WHERE actor_id = 172;')$
r6s.groupby('created')['num_comments'].sum().plot()
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))$
year_prcp_df = pd.read_sql_query(year_prcp.statement, engine,index_col = 'date')$ year_prcp_df.head()
retweets = pd.read_sql_query(query, conn)$ retweets.head()
sent_groups_pivot_df = sent_groups_df.pivot(index="Sentiment group", columns="Twitter account", values="Group total")$ sent_groups_pivot_df
search_term = input('Enter text to search for: ').upper()$ df_vendor_single = df_vendors[df_vendors['Vendor'].str.contains(search_term)]$ df_vendor_single
reddit['Class_comments'] = reddit.apply(lambda x: 'High' if x['num_comments'] > median_comments else 'Low', axis = 1)$ reddit.head()
df_tte_ri.drop(['UnBlendedRate'], axis=1, inplace=True)$ df_tte_ri.head(2)
mlp_df.plot(figsize=(10, 10))$ plt.show()
groups_topics_df.shape, groups_topics_unique_df.shape
bc = pd.read_csv("bitcoinity_all.csv")
random_integers.idxmax(axis=1)
data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)
sns.set_style('whitegrid')$ sns.distplot(data_final['countCollaborators'], kde=False,color="red")#, bins=20)$
df_ratings.head()
LSST_catalog_data.close();LSST_catalog_data.close();
metadata['wavelength'] = refl['Metadata']['Spectral_Data']['Wavelength'].value$ metadata
df = DataObserver.build_simply(file_path= '/Users/admin/Documents/Work/AAIHC/AAIHC-Python/Program/DataMine/Reddit/json_data/Processed_DataFrames/r-news/DF-version_2/DF_v2.json')$ cw_df = df[['ups', 'downs', 'category', 'sentiment_score', 'sentiment_magnitude', 'date_created', 'time_created']]
df.to_csv('Tableau-CitiBike/TripData_2017_Winter.csv', index=False)
sqladb=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',header=None,skipinitialspace=True)
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
sample_weight = np.array([1 if not p else 1.25 for p in joined_train.perishable])
for v in d.variables:$     print(v)
print(df.shape)$
my_gempro.uniprot_mapping_and_metadata(model_gene_source='ENSEMBLGENOME_ID')$ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping)$ my_gempro.df_uniprot_metadata.head()
ebola_tidy = pd.concat([ebola_melt_1, status_country], axis = 1)$ print(ebola_tidy.shape)$ print(ebola_tidy.head())$
pd.Series(0, index=days)
stock_data = db.stock_data$ stock_data_id = stock_data.insert_one(realtime_stock_data).inserted_id$ stock_data_id
    $ eng = create_engine("sqlite:///hawaii.sqlite")
rpt_regex = re.compile(r"(.)\1{1,}", re.IGNORECASE);$ def rpt_repl(match):$     return match.group(1)+match.group(1)
data.iloc[[1,10,3], [2, 3]]
import seaborn as sns$ sns.set()$ sns.set_context("talk")
title_alt = imgs.find_all("img", class_="thumb")$ title_alt
obj.rank(ascending=False, method='max')
prcp_query = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > year_to_date).statement$ prcp_year_df = pd.read_sql_query(prcp_query, session.bind)$ prcp_year_df.set_index('date').head()
pd.read_pickle('data/wx/tmy3/proc/703950.pkl', compression='bz2').head()
soup = bs(response.text, 'html.parser')
from sightengine.client import SightengineClient$ client = SightengineClient("737618018", "bstrJ5VzARavYy5FsELN")$ output = client.check('face-attributes').set_url('https://instagram.fprg2-1.fna.fbcdn.net/vp/aa7a6811e2fbc814c91bb94e92aa8467/5B197538/t51.2885-19/s150x150/11875444_527830867370128_1019973931_a.jpg')
words_only_sk = [term for term in words_sk if not term.startswith('#') and not term.startswith('@')]$ corpus_tweets_streamed_keyword.append(('words', len(words_only_sk))) # update corpus comparison$ print('The number of words only (no hashtags, no mentions): ', len(words_only_sk))
iris.iloc[iris.iloc[:,1].between(3.5, 3.6).values,1]
df.info()$
from bmtk.analyzer import nodes_table$ input_nodes_DF = nodes_table(nodes_file, 'FridayHarborBiophysics')$ input_nodes_DF[:5]
precipitation_df.describe()
news_dict_df.to_json("Newschannel_tweets_df.json")
X = reddit_master['title']$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
daily_df.reset_index(inplace=True)
score = model.evaluate(test_train_vecs_w2v, y_test_train, batch_size=128, verbose=2)$ print (score[1])
results.head()
tweetsPorFecha=tweet_frame[['tweetCreated','userID']]$ tweetsPorFecha.loc[:,('tweetCreated')]=tweetsPorFecha['tweetCreated'].map(lambda x: x.strftime('%Y/%m/%d') if x else '')$ tweetsPorFecha
pax_raw.groupby('seqn').paxcal.mean().value_counts()
if pattern.search('AAbc') is not None:$     print('asdfdsf')
engine=create_engine(seng)$ Joe = pd.read_sql_query('select actor_id, first_name, last_name  from actor where first_name = "Joe"', engine)$ Joe
dataset.User.value_counts()
engine = create_engine("sqlite:///hawaii.sqlite")
newdf.info()
store_items.insert(5, 'shoes', [8, 5, 0])$ store_items
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1?api_key="+API_KEY
all_slices.sort(key = lambda slice: slice['expires'])$     $ print(f"Found {len(all_slices)} slices")    
aaplA01 = aapl['2012-01'][['Adj Close']]$ withDups = pd.concat([msftA01[:3], aaplA01[:3]])$ withDups
contractor_merge[contractor_merge.contractor_bus_name.duplicated() == True]$
df2.columns
news_titles_sr = news_period_df.resample('D', on='news_collected_time')['news_title'].apply(lambda x: '\n'.join(x))
grouped_dpt["Revenue"].filter(lambda x: len(x) < 5)
news_p = news_items[0].find(class_='rollover_description_inner').text$ print (news_p)
totalfare_drivers_by_city= cityfare_driver.groupby(cityfare_driver['type']).sum().reset_index()$ citylabels = totalfare_drivers_by_city['type']$ totalarea_fares = totalfare_drivers_by_city['fare']
model = pd.get_dummies(auto_new.CarModel)$ model = model.ix[:, ["ToyotaCamry", "ToyotaCorolla","ToyotaRav4", "ToyotaLandCruiserPrado", "ToyotaIpsum", "ToyotaSienna", "Toyota4-Runner"]]$ model.head()
df2['Change'].abs()  
data=requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-07-01&end_date=2018-07-31&api_key=%s" %API_KEY)
apple["Regime"] = np.where(apple['20d-50d'] > 0, 1, 0)$ apple["Regime"] = np.where(apple['20d-50d'] < 0, -1, apple["Regime"])$ apple.loc[dt.date(2016,8,7):dt.date(2016,1,1),:].plot(ylim = (-2,2)).axhline(y = 0, color = "black", lw = 2)
print(r.json()['dataset_data']['column_names'])#['start_date'])#
display(data.head(10))
xml_in_merged = pd.merge(xml_in, grouped_df, on=['authorId', 'authorName'], how='left')
df['gender']=df['gender'].replace({'Male': 0, 'Female': 1})$ for col in ['Partner','Dependents','PhoneService','PaperlessBilling','Churn']:$      df[col].replace({'Yes': 1, 'No': 0}, inplace=True)
prcp_year_df.describe()
model_ft = FastText.load_fasttext_format('C:/Users/edwin/Desktop/FastText/wiki.zh.bin')$
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}]$ new_store = pd.DataFrame(new_items, index=['store 3'])$ new_store
clean_stations.columns = ['station', 'latitude', 'longitude', 'elevation', 'name', 'country', 'state']
number_of_commits = len(git_log)$ number_of_authors = git_log['author'].nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
page_size = 200$ response = client.get('/users', q='footwork', limit=page_size,$                     linked_partitioning=1)$
s = pd.Series([1,3,5,np.nan,6,8])$ print(s)
type2017.head()
requests.get(wikipedia_content_analysis)
ibm_hr_cat = ibm_hr_no_numerical.select(categorical_no_target)$ ibm_hr_cat.show(3)
psy = psy_native.copy()$ psy = psy.dropna(thresh=(psy.shape[1]/2), axis=0)$ psy.shape  # 18 people removed
dfname2 = dfname.copy()$ dfmusic = dfname2[dfname2.main_category == 'Music']$ dfmusic.head()
daily_constituent_count = QTU_pipeline.groupby(level=0).sum()$ QTU_pipeline.groupby(level=0).median().describe()
tweets['hashtags'] = tweets['hashtags'].apply(lambda x: x.strip('[]').lower().replace("'",'').split(', '))$ tweets['user_mentions'] = tweets['user_mentions'].apply(lambda x: x.strip('[]').lower().replace("'",'').split(', '))
df_new['country'].value_counts()
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','Status']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)$
year_with_most_commits = commits_per_year['commits'].idxmax().year
from statsmodels.tsa.arima_model import ARIMA$ arima10 = ARIMA(dta_713,(1,0,0),freq='Q').fit()$ arima10.summary()$
trains_fe2_y= trains_fe1[['reordered']]$ trains_fe2_y.head()
pd.date_range(start='3/1/2016', periods=20)
len(purchase_history.product_id.unique())
review_df.date = pd.to_datetime(review_df.date)$ review_df=review_df[review_df['date']>='2017-01-01']
e.instance_method
output_path = os.path.join(output_dir, 'prediction.csv')$ write_output(ids, ids_col, y_pred, label_col, output_path)
res4 = rs.get('http://bsr.twse.com.tw/bshtm/bsContent.aspx', headers = headers)$
import pymysql$ pymysql.install_as_MySQLdb()$ import MySQLdb
ans = df.groupby('A').sum()$ ans
import datetime as dt$ start_time = dt.datetime.now()
last_12_precip = session.query(Measurement.date, Measurement.prcp).\$ filter(Measurement.date >= '2016-08-24').filter(Measurement.date <= '2017-08-23').order_by(Measurement.date).all()
df3_holidays = df3.copy()$ df3_holidays['y'] = np.log(df3_holidays['y'])
df.set_index('Day') # this returns the new data frame with index set as 'Day' but does not modify the existing df dataframe$ df.head()
reg_traffic_with_flags = search_algo(regular_traffic,honeypot_df,['id.orig_h','src'],['ts','ts_unix'])
bymin = walk.resample("1Min")$ bymin.resample('S').mean()
df['launched'] = pd.to_datetime(df.launched)$ df['deadline'] = pd.to_datetime(df.deadline)$
w = np.zeros(shape=(2, 1))$ w[0] = m$ w[1] = c
for word in STOP_WORDS:$     nlp.vocab[word].is_stop = True
file = open('tweets/tweets_Trump.csv', 'w')$ for tweet in results:$     file.write("%s\n" % tweet)
cityID =  '4b25aded08900fd8'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Reno.append(tweet) 
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path, sheetname = 'Superstore2')$ df.head(5)
cgm = data[data["type"] == "cbg"].copy()$ cgm.head()
import builtins$ builtins.uclresearch_topic = 'GIVENCHY' # 226984 entires$ from configuration import config
df = df.drop(['Count'], axis=1)
tips.sex = tips.sex.astype('category')$ tips.smoker = tips.smoker.astype('category')$ print(tips.info())$
userByCountry_df  = youtube_df[youtube_df["channel_title"].isin(channel_namesC)]
len(cats_in['Animal ID'].unique())
base_df.head()
df = demo.make_df(35175,35180)$ df.head()
plt.pie(drivers_totals, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()
nitrodata['Month'].value_counts().sort_index()
logit_countries = sm.Logit(newset['converted'], $                            newset[['country_UK', 'country_US', 'intercept']])$ result_new = logit_countries.fit()
from kipoi_veff.parsers import KipoiVCFParser$ vcf_reader = KipoiVCFParser("example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf")$ print(list(vcf_reader.kipoi_parsed_colnames.values()))
dfPre = df.loc['1930-01-01':'1979-12-31']$ dfPost = df.loc['1984-01-01':'2017-12-31']
predicted_values = model_NB.predict_proba(X_test)
columns = inspector.get_columns('measurements')$ for c in columns:$     print(c['name'], c["type"])
cur.close()$ con.close()
c = df.groupby(['education', 'purpose']).agg({'applicant_id': lambda x: len(set(x))}).reset_index()$ c['education+purpose'] = c['education']+c['purpose']$ sns.barplot(data=c, x='education+purpose', y='applicant_id')
preds = gbm.predict(T_test_xgb)
df_user = pd.read_csv(f_user)$ df_user.head(3)
print(chunker.evaluate(valid_trees))
writer = pd.ExcelWriter('output.xlsx')$ frame.to_excel(writer,'Sheet1')$ writer.save()
d = datetime.date(1492, 10, 12)$ d.strftime('%A')
import gensim, logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
snowshoe_df = pd.read_pickle('..\data\interim\df_for_snowshoe_classification')$ snowshoe_df.drop(columns = ['datetime'],inplace=True)$ snowshoe_df.head()
pulledTweets_df['Processed_tweet'] = pulledTweets_df['text'].apply(cleaner)$ pulledTweets_df['Processed_tweet'] = pulledTweets_df['Processed_tweet'].apply(remove_stopwords)$ pulledTweets_df['Processed_tweet'] = pulledTweets_df['Processed_tweet'].apply(lemmatize)
newdf.describe()
os.environ['PROJECT'] = PROJECT$ os.environ['BUCKET'] = BUCKET$ os.environ['REGION'] = REGION
cycling_data = [10.7, 0, None, 2.4, 15.3, 10.9, 0, None]$ joined_data = list(zip(step_counts, cycling_data))$ print joined_data$
ideas = categorical.join(ideas)
columns_name = ["content","creationTime","isTop","referenceId","userClientShow","isMobile"]$ content = re.findall(r'"showOrderComment".*?"content":"(.*?)","creationTime":"(.*?)","isTop":(\w*).*?"referenceId":"(\d*)".*?"userClientShow":"(.*?)".*?"isMobile":(\w*).*?',c)$ content$
combined_df = test.join(prediction_df)$ combined_df.head(15)
print(pd.to_numeric(countdf['number_ratings']).sum())$ print(pd.to_numeric(countdf['number_ratings']).sum()-pd.to_numeric(count1df['number_ratings']).sum())$ print(pd.to_numeric(countdf['number_ratings']).sum()-pd.to_numeric(count6df['number_ratings']).sum())
td_alpha = td ** (1/3)$ td_alpha = td_alpha / td_alpha.max().max()
df.to_sql('fb_posts', engine, schema=schema, if_exists='append')
ORDER_BPAIR_POSTGEN = pd.merge(left=BPAIRED_GEN,right=ORDERS_GEN[['order_number','created_at']].astype(dtype),left_on='shopify_order_id',right_on='order_number')
df1.loc[0:8,['SerNo', 'By']]
Base.prepare(engine, reflect=True)$ Base.classes.keys()
match_results = pd.read_csv("data/afl_match_results.csv")$ odds = pd.read_csv("data/afl_odds.csv")$ player_stats = pd.read_csv("data/afl_player_stats.csv")
df1 = pd.merge(left=dfWQ_annual,right=dfQ1_annual,how='inner',left_index=True,right_index=True)$ df2 = pd.merge(left=dfWQ_annual,right=dfQ2_annual,how='inner',left_index=True,right_index=True)$ df3 = pd.merge(left=dfWQ_annual,right=dfQ3_annual,how='inner',left_index=True,right_index=True)
sp500[(sp500.Price < 10) & (sp500.Price > 0)][['Price']]
empDf.select("name").show()
from scipy import stats$ resid = model_arima121.resid$
t2['p1'] = t2['p1'].str.title()$ t2['p2'] = t2['p2'].str.title()$ t2['p3'] = t2['p3'].str.title()
tweet_df = pd.DataFrame(twitter_list)$ tweet_df.head()
data_NL = data_NL.sort_values($     'capacity', ascending=False).groupby('name', as_index=False).first()$ data_NL = data_NL.reindex(columns=columns_sorted)
filename = processed_dir+'pulledTweetsProcessedAndClassified_df'$ gu.pickle_obj(filename,pulledTweets_df)
df_tweets.reset_index().to_pickle("../tweets_extended.pkl")
vacation_data_df=pd.DataFrame(vacation_data)$ rain_per_station = pd.pivot_table(vacation_data_df,index=['station'],values=['prcp'], aggfunc=sum)$ rain_per_station
BAL_analysis = team_analysis.get_group("BAL").groupby("Category") # Pulls only team transactions from sample, then groups$
rain_df.set_index('date').head()
challenge_dbname = "challenge"$ pos_series_name = "idx.position"$ challenge_client = DataFrameClient(host, port, user, password, challenge_dbname)
dfd['brand'] = [b.strip().title() for b in dfd.brand]$ dfd.brand.unique()
np.shape(prec_fine)
openmoc_geometry = get_openmoc_geometry(mgxs_lib.geometry)
import matplotlib.pyplot as plt$ import seaborn as sns
first_row = session.query(Measurement).first()$ first_row.__dict__
priors_product_reordered_spec= priors_reordered.groupby(["user_id","product_id"]).size().reset_index(name ='reordered_count_spec')$ priors_product_reordered_spec['userprod_id']=priors_product_reordered_spec['product_id'] + priors_product_reordered_spec['user_id'] *100000$ priors_product_reordered_spec.head(10)
time_length = pd.Series(df.len.values, index=df.Date)$ time_favorite = pd.Series(df.Likes.values, index=df.Date)$ time_retweet = pd.Series(df.Retweets.values, index=df.Date)
df_release = df_release.dropna(axis=0, subset=['actual'])$ df_release.shape
predictions = rfModel.transform(testData)
data_current = current.loc[(df['Date'] == '2018-05') | (df['Date'] == '2018-06')] $
pd_review.shape
results = soup.find_all('div', class_="slide")$ print(results)
import gspread$ from oauth2client.service_account import ServiceAccountCredentials$ from operator import itemgetter
ET_Combine = pd.concat([BallBerry_hour, Jarvis_hour, simResis_hour], axis=1)$ ET_Combine.columns = ['Ball-Berry', 'Jarvis', 'Simple resistance']
df2.to_csv("SandP.csv")$ df2 = pd.read_csv("SandP.csv", parse_dates = True, index_col = 1)$ df2.head(15)$
df = pd.read_csv(chat_file, error_bad_lines=False)
def toDateTime(x):$     parsedStringDate = x[0:x.index(',')]$     return datetime.strptime(parsedStringDate, '%m/%d/%y').date()
scores = cross_val_score(model, X, y, scoring='roc_auc', cv=5)$ print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))
tweet_data.sort_values(['favorite_count','retweet_count'], ascending=[False, False]).head(5)$
df.sample(5)
plt.savefig('overall_media.png')$ plt.show()
pca = PCA().fit(X_std) $
df.head()$
def tweet_extend (tweet_id):$     $     return api.get_status(tweet_id, tweet_mode='extended')._json['full_text']
df['language'] = [np.nan if l == 'C' else l for l in df.language]
twelve_months_prcp.head()
(train_4.shape, y_train.shape)
folium.GeoJson(watershed).add_to(m);
a=unique_users[unique_users.created_at.isin(['0000-00-00 00:00:00'])].index.values$ unique_users.loc[a,'created_at']='2010-11-11 03:59:09'$
forecast_range = forecast1[::-1]$ forecast_range.info()$ forecast_range.head()
pgh_311_data.resample("M").count()
!hdfs dfs -put CC_records.csv {HDFS_DIR}/Consumer_Complaints.csv
ccc = td.columns
datetime.now().strftime('%A')
mean = np.mean(data['len'])$ print("the mean length of the tweets is: {}".format(mean))
twelve_months = session.query(Measurements.date, Measurements.prcp).filter(Measurements.date > year_before)$ twelve_months_prcp = pd.read_sql_query(twelve_months.statement, engine, index_col = 'date')
todays_date = datetime.datetime.now()
ws.delete_cols(1)
test_tweet = api.user_timeline(newsOutlets[0])$ print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
new_fan = questions.loc[questions['years_attend'] == 0]$ return_fan = questions.loc[questions['years_attend'] != 0]
CON = CON.drop(columns =['Full Name'])
df.dropna().shape == df.shape, df.shape
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Pacific').strftime('%D: %r') + " (PST)"$ date_min = date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Pacific').strftime('%D: %r') + " (PST)"
X_test=tok.texts_to_sequences(X_test)$ x_test=sequence.pad_sequences(X_test,maxlen=maxlen)
Top_tweets.groupby('sentiment').count()
grid.fit(Xtrain, ytrain)
df.shape$ df.to_csv('ny_times_url_dataframep3.csv', index=False,encoding = 'utf-8')$
svc.score(X_tfidf_test, y_tfidf_test)
%%time$ responded_all_jan23[fields].to_excel(export_filename_jan23, index=False)$ responded_all_feb5[fields].to_excel(export_filename_feb5, index=False)
aapl = pd.read_csv(file_name, index_col='Date', usecols=['Date', 'Adj Close'])$ aapl.columns = ['AAPL']$ aapl
FREEVIEW.plot_number_of_fixations(raw_fix_count_df, option=None)
coin_data = quandl.get("BCHARTS/ITBITSGD")$
logodds.drop_duplicates().sort_values(by=['count']).plot(kind='barh')
all_sets = pd.read_json("AllSets.json", orient = "index")$ all_sets.head()
df = df.set_index('email')
citydata_with_nbr_rides = pd.merge(citydata_avg_fare_work, city_nbr_rides, on='city', how='left')$ citydata_with_nbr_rides.head()
rain_score = session.query(Measurement.prcp, Measurement.date)\$                .filter(Measurement.date > past_year).\$                 order_by(Measurement.date).all()
Output_three = New_query.ss_get_results(sport='football',league='nfl', ep='team_game_logs', season_id='nfl-2017-2018', game_id='nfl-2017-2018-ari-det-2017-09-10-1300')$ Output_three
df_2016['bank_name'] = df_2016.bank_name.str.split(",").str[0]$
measure_df = pd.read_csv(measure, encoding="iso-8859-1", low_memory=False)$ measure_df.head()
tesla['nlp_text'] = tesla.text.apply(lambda x: tokenizer.tokenize(x.lower()))$ tesla.nlp_text = tesla.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ tesla.nlp_text = tesla.nlp_text.apply(lambda x: ' '.join(x))
df2.loc['2018-01-18', 'high_tempf']
empDf.createOrReplaceTempView("employees")$ SpSession.sql("select * from employees where salary > 4000").show()
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_hat), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
mgxs_lib = openmc.mgxs.Library.load_from_file(filename='mgxs', directory='mgxs')
df_dn.to_csv('data/DayorNight.csv', date_format="%d/%m/%Y %H:%M:%S",index=False)
scores_firstq = np.percentile(raw_scores, 25)$ scores_thirdq = np.percentile(raw_scores, 75)$ print('The first quartile is {} and the third quartile is {}.'.format(scores_firstq, scores_thirdq))
url = 'https://twitter.com/marswxreport?lang=en'$ response = requests.get(url)$ soup = bs(response.text, 'lxml')
new_model = gensim.models.Word2Vec(min_count=1)  # an empty model, no training$ new_model.build_vocab(sentences)                 # can be a non-repeatable, 1-pass generator     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)                       $
days_range = pd.date_range(start=min_date, end=max_date, freq='D')$ idx_days = [str(s)[:10] for s in days_range]
woba_leaderboard = df.groupby(['batter_name', 'batter'])['woba_value'].agg(['mean', 'count'])$ woba_leaderboard.loc[woba_leaderboard['count']>100,].sort_values('mean', ascending=False).head()
df.loc[:,"message"] = df['message'].str.findall('\w{3,}').str.join(' ') $ df.head()
LR_prob=logisticRegr.predict_proba(train_ind[features])$ roc=roc_auc_score(test_dep[response], best_model_lr.predict_proba(test_ind[features])[:,1])
cityID = '960993b9cfdffda9'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Bakersfield.append(tweet) 
stop_words_update = list(pipe_cv.get_stop_words())$ stop_words_update.append('pron')$ stop_words_update.append('aa')
compound_final.head()
