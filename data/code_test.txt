q = pd.Period('2017Q1',freq='Q-JAN')$ q=q.asfreq('M',how="end")$ q
np.save('../models/crosstab_40937.pkl', crosstab)
red.columns
plt.scatter(USvideos['dislikes'], USvideos['views'])$
df_imputed_mean_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.mean())
y.name = "huhu"$ y = y.rename("jsdfjkdsfhsdfhdsfs")$ y
df=df.drop('SEC filings',axis=1)
r2017_dict = r2017.json()
pass_file_name = "C:\KUBootCamp\\passw.json"$ data = json.load(open(pass_file_name))$ spassword = data['mysql_root']
df_CLEAN1A = pd.read_csv(url_CLEAN1A,sep=',')$ df_CLEAN1B = pd.read_csv(url_CLEAN1B,sep=',')$ df_CLEAN1C = pd.read_csv(url_CLEAN1C,sep=',')
bixi_hourly=bixi_hourly.resample('1H', how={'duration_sec': np.mean,$                                             'distance_traveled': np.mean, 'is_member':np.sum,$                                             'number_of_trips':np.sum})
average_vol = [i[5] for i in json_afx_data] $ print('Average trading volume is :', sum(average_vol) / len(average_vol)) #divide the sum by the length // count of entries.$
data_AFX_X.describe()['High']
df_time = df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ df_time
cityID = '0562e9e53cddf6ec'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Santa_Ana.append(tweet) 
(mydata / mydata.iloc[0] * 100).plot(figsize = (15, 8)); # 15 deals with the width and 8 deals with the price.$ plt.show() # this function always plots the price using matplotlib functions.$
df3.duplicated().sum()
sentiments_pd["Outlet"].unique()
np.random.seed(123456)$ ps = pd.Series(np.random.randn(24),pd.period_range('1/1/2013','12/31/2014',freq='M'))$ ps
building_pa_prc_zip_loc['permit_type'].unique()
tweets_df_lang = tweets_df.groupby('language')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ tweets_df_lang$
df = pd.read_csv(reviews_file_name, encoding = "ISO-8859-1")$ df.head(n = 10)
pivoted.T.shape
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt"$ mydata = pd.read_csv(path, sep= '\t')$ mydata.head(5)
df_countries = pd.read_csv('countries.csv')$ df_countries.head()
df[(df["compound"]==0)].groupby("newsOutlet").count()
from bs4 import BeautifulSoup$ example1 = BeautifulSoup(train["comment_text"][0], "html.parser")
prcp_query = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > last_year).all()
url=("/Users/maggiewest/Projects/detroit_census.csv")$ detroit_census = pd.read_csv(url)
festivals.at[2,'longitude'] = -87.7035663$ festivals.head(3)$
print(s1.head(3))$ print()$ print(s1.tail())
df.set_index('UPC EAN', append=True, inplace=True)$ df.head(3)
data.plot.area()
col = pymongo.MongoClient()['Tweets']['Streaming']$ col.count()
print("The loglikelihood estimations are below. \n ")$ print("Loglikelihood:\n",  loglikA)$
client = foursquare.Foursquare(client_id='1KNECHOW4ALXKWS4OWU2TEUZMPW0WUN1NORS2OUMWWBBCV4C', client_secret='O4QOOLKGDZK44DTBRPQIUDGO2Z4XQYYJQOJ0LN5E5FAQASMM', redirect_uri='http://fondu.com/oauth/authorize')$ auth_uri = client.oauth.auth_url()
inspector = inspect(engine)$ inspector.get_table_names()$
df = pd.read_excel("F:/web mining/webmining project/crawler/annotation_data/samsung/Merged_final.xlsx")$
def esol(time, a, datetime):$     pesol = a * 0.000015 * (1 + 0.5 * np.cos(((datetime[int(time)].month - 1) * 3.14) / 6.0))$     return pesol
df.isnull().sum()
small_lst = df.sample(100)$ location_small_lst, message_small_lst, date_small_lst = small_lst.locations,small_lst.messages, small_lst.posted_date
fig, ax = plt.subplots(figsize = (14.5,8.2))$ ax.hist(events_df['words_count'], 50, facecolor='green', alpha=0.75)$ plt.show()
LabelsReviewedByDate = wrangled_issues_df.groupby(['Priority','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
!hdfs dfs -cat {HDFS_DIR}/p32cfr-output/part-00001 {HDFS_DIR}/p32cfr-output/part-00000 > p32cfr_results.txt
results = logit.fit()$ results.summary2()
f_lr_hash_modeling2 = f_lr_hash_modeling2.withColumn('id', col('id').cast('long'))$ f_lr_hash_test = f_lr_hash_test.withColumn('id', col('id').cast('long'))
mydata.tail()
! rm -rf models3$ ! mrec_train -n4 --input_format tsv --train "splits1/u.data.train.*" --outdir models3 --model=slim \$     --l1_reg=0.001 --l2_reg=0.1
buckets_to_df(contributors.fetch_aggregation_results()['aggregations']['0']['buckets'])
df_user = pd.read_csv(user_source)$ df_user.head()
from bs4 import BeautifulSoup$ import requests$ url = 'https://www.reddit.com/r/Python/'
pst = ps.to_timestamp()$ pst.index
le_indicators = wb.search("life expectancy")$ le_indicators.iloc[:3,:2]
user = reddit.redditor('Shadow_Of_')$ for comment in user.comments.new():$     print(comment.body)
df_cities_clean = df_cities.loc[df_cities['lon']!='']$ df_cities_clean.to_csv('Cities_Data.csv',index=False)$ df_cities_clean.head()
rootDistExp = Plotting(S.setting_path.filepath+S.para_trial.value)
fire=load_data('https://data.sfgov.org/resource/wbb6-uh78.json')$ fire.dtypes$
csvpath = os.path.join('Desktop', 'Project-2', 'spotify_data.csv')$ import csv$ spotify_df = pd.read_csv(csvpath, encoding="ISO-8859-1")
todays_datetimes = [datetime.datetime.today() for i in range(100)]
fed_reg_data = r'data/fed_reg_data.pickle'$ final_df.to_pickle(fed_reg_data)
bands.columns = ['BAND_'+str(col) for col in bands.columns]
chunker.tagger.classifier.show_most_informative_features(15)
happiness_df=happiness_df.groupby('dates').mean()$
(fit.shape, fit2.shape, fit3.shape, fit4.shape)
chefdf = chefdf.dropna()
r_dict = r.json()$ print(r_dict['dataset_data']['column_names'])
Average_Compounds.to_csv("Average_Compound_Sentiments.csv")
cityID = 'c7ef5f3368b68777'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baton_Rouge.append(tweet) 
for tweet in results:$    print (tweet.text)
genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId'])$ genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ genreTable.head()
year_with_most_commits = commits_per_year[commits_per_year == commits_per_year.max()].sort_values(by='num_commits').head(1).reset_index()['timestamp'].dt.year$ print(year_with_most_commits[0])
df.sort_values('dealid', ascending = True, inplace = True)
url = "https://twitter.com/marswxreport?lang=en"$ response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')
data['affair'].value_counts()
X_train_valid, X_test, y_train_valid, y_test = train_test_split(pm_final.drop('status', axis = 1)$                                                     ,pm_final[['status']],$                                                     test_size=0.20)
adjmats, timepoints = pt1.runltvmodel(rawdata, numwins=10)
svc = SVC(random_state=20)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'decision_function_shape':['ovo', 'ovr'], 'kernel':['linear', 'rbf']}$ grid_svc = GridSearchCV(svc, param_grid=param_grid, cv=10, n_jobs=-1)
table_rows = driver.find_elements_by_tag_name("tbody")[26].find_elements_by_tag_name("tr")$
y = tweets['handle'].map(lambda x: 1 if x == 'Donald J. Trump' else 0).values$ print(np.mean(y))
gdax_trans_btc = gdax_trans.loc[gdax_trans['Account_name'] == 'BTC',:]
tdf = sns.load_dataset('tips')$ tdf.sample(5)
data = pd.DataFrame(data = [x.text for x in tweets], columns = ['Tweets'])$ print(data.head(10))
print(df.info())
pd.to_datetime(['2009/07/31', 'asd'], errors='raise')
s_mountain = Series(np.arange(0,5),index=pd.date_range('2014-08-01', periods=5, freq="H",tz="US/Mountain"))$ s_eastern = Series(np.arange(0,5),index=pd.date_range('2014-08-01', periods=5, freq="H",tz="US/Eastern"))$ s_mountain
history_df = pd.read_sql('measurement',engine)$ history_df['station'].value_counts()
cityID = '53b67b1d1cc81a51'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Birmingham.append(tweet) 
hi_day_delta_index = day_deltas.index(max(delta_val for delta_val in day_deltas if delta_val is not None))$ ans_4 = ('%s had greatest within day difference: %s' % (dates[hi_day_delta_index], day_deltas[hi_day_delta_index]))
all_patents_df.number_of_claims.describe()
fetch_measurements('http://archive.luftdaten.info/2016-05-09/')
df2 = df.drop(remove_index)$ print(df2.shape)  # This should be 294478 - 3893$ df2.head()
from collections import Counter$ words = set()$ word_counts = data['Tweets'].apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0)$
df = pd.read_csv('Pro3ExpFinal0602.csv',index_col ='Unnamed: 0' , engine='python')
soup = BeautifulSoup(response.text, 'html.parser')
df_piotroski = pd.DataFrame({'id' : stock_id_ls, 'name': stock_name_ls}, columns = ['id', 'name', 'market_type', 'quant', 'market_sum', 'property_total', 'debt_total', 'listed_stock_cnt', 'pbr', 'face_value',])
data = pd.read_excel("D:\ML\AnomalyDetection\datasetFun_cleaned.xls")$
data['Week Ending Date'] = pd.to_datetime(data['Week Ending Date'])$ data['year'] = data['Week Ending Date'].dt.year$ data.head(5)
file = open('chinese_stopwords_2.txt','rb').read().decode('utf8').split('\n')$ stop_words = list(set(file))
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt"$ mydata = pd.read_csv(path, sep= '\t')$ mydata.head(5)
cursor = collection_reference.find()
pickle.dump(final_xgb, open(base_filename + '_model.sav', 'wb'))
sub_gene_df.sample(10)
dj_df =  getTextFromThread(urls_df.iloc[0,0], urls_df.iloc[0,1])$ for i in range(1,5):$         dj_df.append(getTextFromThread(urls_df.iloc[i,0], urls_df.iloc[i,1]), ignore_index = True)
data = pd.read_csv('fatal-police-shootings-data.csv')$ print(data.shape)$ data.info()
crs = {'init': 'epsg:4326'}$ geometry = df_TempJams['lineString']$ geo_TempJams = gpd.GeoDataFrame(df_TempJams, crs=crs,geometry = geometry)
price=data.json()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata  = pd.read_csv(path, sep =' ')$ mydata.head(5)
fan_zip = [zipcode.isequal(str(zc)) for zc in questions['zipcode']]$ questions['zipcode'] = fan_zip
!wget ftp://ftp.solgenomics.net/tomato_genome/annotation/ITAG3.2_release/ITAG3.2_RepeatModeler_repeats_light.gff -P $DATA_PATH
filenames = glob.glob(os.path.join(input_folder, glob_string))
crosstab_transformed = pca.transform(crosstab)
auth = tweepy.OAuthHandler(api_key, api_secret)$ auth.set_access_token(access_token, access_secret)$ api = tweepy.API(auth)
np.where([min(BID_PLANS_df.iloc[i]['scns_created']) != BID_PLANS_df.iloc[i]['scns_created'][0] for i in range(len(BID_PLANS_df))])
from features.build_features import remove_invalid_data$ df = remove_invalid_data(pump_data_path)$ df.shape
m.fit([X], Y, epochs=10, batch_size=16, validation_split=0.1)$
mngr = dsdb.ConnectionManager(dsdb.LOCAL, user="jacksonb")$ local = mngr.connect(dsdb.LOCAL)$ local._deep_print()
df = pd.read_excel("msft.xls")$ df.head()
data = data.sort_values(by=['time'])
%matplotlib inline$ import matplotlib.pyplot as plt
df['sin_day_of_year'] = np.sin(2*np.pi*df.day_of_year/365)$ df['cos_day_of_year'] = np.cos(2*np.pi*df.day_of_year/365)
industry_group=df.groupby(by="GICS Sector")$ industry_group
la_inspections = pd.read_csv('/Users/salilketkar/thinkful/LA_health_score_inspections.csv')
df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopMcMrNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TMcM_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))$ df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopPalNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TPal_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))$
tallies_file = openmc.Tallies()$ mgxs_lib.add_to_tallies_file(tallies_file, merge=True)
df1 = pd.DataFrame(X_selected_features)$ df1['labels'] = pd.Series(y)$ sns.pairplot(df1, hue='labels')
print(r.json()['dataset']['column_names'])
so.loc[(so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')]
!wget -O cell_samples.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/cell_samples.csv
plot_LC_solar_Flare('FERMI/SolarFlares/LAT_Flares/lat_LC_20170910.fits','Flare20170910')
model = gensim.models.Word2Vec(sentences, min_count=1)
typesub2017 = typesub2017.drop(['MTU','MTU2'],axis=1)
mean_of_tweets = data.mean()
df = df.T$ df
print(address_df['nndr_prop_ref'].value_counts().head(3))
np.exp(results.params)
import statsmodels.api as sm$ df2['intercept'] = 1$ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']$
convert_me = "2016bbb12---15"$ datetime.datetime.strptime(convert_me, "%Ybbb%m---%d")
kickstarters_2017[cont_vars].corr()
exec(open("./secrets.literal").read())$ gh_user = "holdenk"$ fs_prefix = "gs://boo-stuff/"
df['Mo'] = df['datetime'].map(lambda x: x.month)$ df.head()
df.Date = pd.to_datetime(df.Date_Hour)$ df = df.set_index('Date_Hour').sort_index(ascending=True)
data = read_root('./preprocessed_files.root')$ data.head()
for_exiftool[['SourceFile','GPSLatitude','GPSLongitude','GPSAltitude']].to_csv(basedirectory+projectname+'/'+projectname+'_exiftool.csv',index=False)
print(plate_appearances.shape)$ plate_appearances = plate_appearances.dropna()$ print(plate_appearances.shape)
df_defects = pd.read_excel('ncr_data.xlsx', index='Notification')$ df_defects
sentiment_df.to_csv("sentiment.csv", encoding = "utf-8-sig", index = False)
dr_ID = [7.0, 10.0, 16.0]$ RNPA_ID = [3.0, 9.0, 12.0, 13.0, 14.0, 15.0, 19.0, 25.0, 27.0, 30.0]$ ther_ID = [11.0, 17.0, 18.0, 23.0, 24.0, 26.0, 28.0, 29.0]
conn.get_tables()
fcc_nn.plot(y='score', use_index=True)
df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9])
import os, os.path$ def filecount(dir_name):$     return len([f for f in os.listdir(dir_name) if os.path.isfile(dir_name +f)])
train_x = encodedlist$ print np.asarray(train_x).shape
TripData_merged = pd.concat([TripData1, TripData2])
pattern = re.compile('AA')$ print(pattern.findall('AAbcAA'))$ print(pattern.findall('bcAA'))
total_stations = session.query(Station).group_by(Station.station).count()$ print('Number of stations: ' + str(total_stations))
df.iloc[]
y_ = df2["user_id"].count()$
tmax_day_2018.dims
CM_s = pendulum.datetime(2017, 11, 27, 0, 0, 0, tzinfo='US/Eastern') $ CM_e = pendulum.datetime(2017, 11, 27, 23, 59, 59, tzinfo='US/Eastern') $ CM = tweets[(tweets['time_eastern'] >= CM_s) & (tweets['time_eastern'] <= CM_e)]
data.to_csv("...Twitter\\RAILWAYS.csv", sep =',')
expectancy_for_least_country = le_data.min(axis=0)$ expectancy_for_least_country
plot_results = actual_value_second_measure.join(predicted_probs_first_measure)$
BASE_URL = 'https://orgdemo.blastmotion.com'
from h2o.automl import H2OAutoML
scores = raw_scores.value_counts().sort_index()$ scores
utility_patents_subset_df['figure_density'] = utility_patents_subset_df['number-of-figures'] / utility_patents_subset_df['number-of-drawing-sheets']$ utility_patents_subset_df['figure_density'].describe()
dta.query(("facility_type == 'Restaurant' and "$            "inspection_type == 'Complaint Re-Inspection'"))$
INT['Create_Date']= pd.to_datetime(INT.Create_Date)
df.info()$
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv"$ df  = pd.read_csv(path, sep =',') # df  = pd.read_csv(path) may be too. sep =',' is by deffect.$ df.head(5)
alice_sel_shopping_cart = pd.DataFrame(items, index=['glasses', 'bike'], columns=['Alice'])$ alice_sel_shopping_cart
session.query(Adultdb).filter_by(education="9th").delete(synchronize_session='fetch')
for c in ccc:$     for i in vhd[vhd.columns[vhd.columns.str.contains(c)==True]].columns:$         vhd[i] /= vhd[i].max()
df_2015['bank_name'] = df_2015.bank_name.str.split(",").str[0]$
news_df = news_df.sort_index()$ news_df.head()
df_schools.describe()
mean = np.mean(data['len'])$ print("The length's average in tweets: {}".format(mean))
df['y'].plot(marker='o', markersize=5)
festivals.index
df_SP.to_pickle('DDC_data_cleaned.pkl')
paired_df_grouped.sort_values('co_occurence', ascending=False, inplace=True)$ paired_df_grouped.loc[:, ['dataset_1', 'all_co_occurence']].head(20)$ paired_df_grouped['top10'] = paired_df_grouped.co_occurence.apply(lambda x: x[:10])$
charge = reader.select_column('charge')$ charge = charge.values # Convert from Pandas Series to numpy array$ charge
ibm_hr = spark.read.csv("../data/WA_Fn-UseC_-HR-Employee-Attrition.csv", header=True, mode="DROPMALFORMED")$ ibm_hr.show(3)
stat_info_st = stat_info[0].apply(fix_space)$ print(stat_info_st)
response = requests.get(nasa_url)$ print(response.text)
sentimentDf.to_csv("twitterSentiment.csv")
df_cities = pd.read_csv('cities.csv')$ df_cities
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);
new_df = df.replace(['poor','average','good','exceptional'], [1,2,3,4])$ new_df
Z = np.random.randint(0,3,(3,10))$ print((~Z.any(axis=0)).any())
lm.predict(x_test)
old_page_converted=np.random.choice([1,0],size=n_old,p=[pold,(1-pold)])$ old_page_converted.mean()
df2.shape[0]$ print ("Total Number of row : {}".format(df2.shape[0]))
weights['0.encoder.weight'] = T(new_w)$ weights['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))$ weights['1.decoder.weight'] = T(np.copy(new_w))
measure_df.to_csv("Hawaii_measurements_clean.csv", index=False, sep='\t', encoding='utf-8')$ station_df.to_csv("Hawaii_stations_clean.csv", index=False, sep='\t', encoding='utf-8')$ hawaii_df.to_csv("Hawaii_merged_clean.csv", index=False, sep='\t', encoding='utf-8')
psy_df3 = QLESQ.merge(psy_df2, on='subjectkey', how='right') # I want to keep all Ss from psy_df
combined_df.to_csv('manual_comment_results.csv', index=False)
ride_df_urban = urban.groupby('city')$ city_df_urban = urban.set_index('city')
params = {"objective": "reg:linear", "booster":"gblinear"}$ gbm = xgboost.train(dtrain=T_train_xgb, params=params)
len(mod.coef_)
df2.columns = ['c','d']$ df2
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Therapists Hours Partial Autocorrelation')
festivals.rename_axis('')$ festivals.columns.names = ['Index']
data['SA'] = np.array([analyze_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
merged_data['cs_creation_day'] = merged_data['customer_creation_date'].dt.day$ merged_data['cs_creation_month'] = merged_data['customer_creation_date'].dt.month$ merged_data['cs_creation_year'] = merged_data['customer_creation_date'].dt.year
display(data.head(10))
X = x['age'].copy()$ X['C_sign'] = np.sign(X)$ X['C_sign'].value_counts()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
df_sched2 = df_sched.iloc[:,1:].apply(pd.to_datetime,format='%Y-%m-%d')
output = pd.DataFrame({id_label:ids, target_label:predictions})$ print_and_log(output)
lda_model = models.LdaModel(corpus=corpus, id2word=id2token, num_topics=20, update_every=0, passes=20)$ lda_model.save('/tmp/model.lda')$ lda_model = models.LdaModel.load('/tmp/model.lda ')$
filterdf = result.query("0 <= best <= 1 and fileType == 'csv' and teamCount < 1000")$ filterdf.corr()['best'].sort_values()
df_insta=pd.read_csv('Instagram_Mobility_20_fixed.csv')$ df_insta=df_insta.dropna(subset=['coords'])$ df_insta.head()
val_idx = np.flatnonzero($     (df.index<=datetime.datetime(2018,4,3)) & (df.index>=datetime.datetime(2018,3,1)))$
FREEVIEW.plot_fixation_durations(raw_freeview_df)
print(df['State'].value_counts(dropna=False))
top_10_authors = git_log.author.value_counts().head(10)$ top_10_authors
df_cs['Sentiment_class'] = df_cs.apply(conditions, axis=1)$ df_cs.to_csv("costco_senti_score.csv", encoding='utf-8', index=False)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
tweet_archive.name.value_counts().head(5)$
tag_df = tag_df.sum(level=0)$ tag_df.head()
DataSet = DataSet[DataSet.tweetSource.notnull()]$ len(DataSet)
movies_df = pd.read_csv('movies.csv')$ ratings_df = pd.read_csv('ratings.csv')$ movies_df.head()
coming_next_reason.to_csv('../data/coming_next_reason.csv')
df_CLEAN1A['AGE'].max()
exchanges = ccw.get_exchanges_list()$ EXCHANGE_DB = pd.DataFrame.from_dict(exchanges, orient='index')$
c.description
df_m.loc[df_m["CustID"].isin([customer])]
Measurement = Base.classes.measurement$ Station = Base.classes.station
crimes.tail()
desc_stats.columns = ['Count', 'Mean', 'Std. Dev.', 'Min.', '25th Pct.', 'Median', '75th Pct.', 'Max']$ desc_stats
new_columns = status_data.columns.values$ new_columns[0] = "rowID"$ status_data.columns = new_columns
cityID = 'b71fac2ee9792cbe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Sacramento.append(tweet) 
from sqlalchemy.orm import Session$ session = Session(engine)
stacked=football.stack()$ stacked.head()$ stacked.unstack().head()
forecast_set = clf.predict(X_lately) #selecting only the last few rows to predict$ print(forecast_set)$ df['Forecast'] = np.nan #Adding new column to store all the values
ward_df['Crime Count'] = ser.values
response = requests.get('https://www.jconline.com/search/gun%20control/')$ soupresults2 = BeautifulSoup(response.text,'lxml')$
clean_stations = pd.concat([stations, stat_info_merge], axis=1)
tweet_df.sort_values(by="date", ascending=True)$
for i, row in problems.iterrows():$     srx, srr = row.srx, row.srr$
n_user_days.hist()$ plt.axvline(x=4, c='r', linestyle='--')
ctc = pd.DataFrame(columns=ccc, index=ccc)
k = 3$ neigh = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)
exiftool -csv -createdate -modifydate MVI_0011.mp4 MVI_0012.mp4 > out.csv
trump['text'] = trump['text'].str.lower()$
agg_function = {'budget':np.mean, 'reading_score':np.mean, 'math_score': np.mean, 'size': np.mean, 'passing_reading': np.sum, 'passing_math': np.sum}
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata  = pd.read_csv(path, sep ="\s+")$ mydata.head(5)$
damage = crimes[crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE']$ damage.head()
engine = create_engine("sqlite:///dropna_hawaii.sqlite")
result = Geocoder.geocode("7250 South Tucson Boulevard, Tucson, AZ 85756")
data_spd = pd.DataFrame()$ data_spd['tweets'] = np.array(tweet_spd)$ data_spd.head(n=3)
brand_names = np.unique(np.asarray(data.brand[:]))$ model_names = np.unique(np.asarray(data.model[:]))
RatingSampledf.to_csv("..\\Output\\SampleRatings.csv")
df_c = df.query('landing_page!= "old_page"')  $ df_cb = df_c.query('group == "control"')$ df_cb.nunique()$
Base.prepare(engine, reflect=True)$
bin_vars = [col for col in Xtrain_pf.columns if Xtrain_pf[col].nunique() == 2]$ bin_vars
def location_column(dataframe_list):$     for df in dataframe_list:$         df['location'] = df.user.map(extract_location)
df_20180113 = pd.read_csv('data/discovertext/hawaii_missile_crisis-export-20180221-113313.csv',$                           infer_datetime_format=True)$ df_20180113.head()
commmon_intervention_train = intervention_train.index.intersection(intervention_history.index)
reddit.info()
merged_portfolio = pd.merge(portfolio_df, adj_close_latest, left_index=True, right_index=True)$ merged_portfolio.head()
df.groupby("PredictedIntent").agg({'Notes':'count'})
station_names = session.query(Station.station).all()$ station_names
ndvi_of_interest02= ndvi.sel(time = time_slice02, method='nearest')$ ndvi_of_interest02
positive = '/Users/EddieArenas/desktop/Capstone/positive-words.txt'$ positive = pd.read_table('/Users/EddieArenas/desktop/Capstone/positive-words.txt')
groupedNews = sentiments_df.groupby(["Target"], as_index=False)$
df['incident_zip'] = df.incident_zip.astype(int)$ df = df[df['incident_zip'] < 12000 ]
kick_projects[['goal', 'pledged', 'backers','usd_pledged','usd_pledged_real','usd_goal_real','state']].corr()
last_year = dt.date(2018, 7, 29) - dt.timedelta(days=365)$ print(last_year)
for v in d.variables:$     print(v)
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
cityID = 'b004be67b9fd6d8f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Norfolk.append(tweet) 
df_pd.sort_values(by='timestamp')$ train_frame = df_pd[0 : int(0.7*len(df_pd))]$ test_frame = df_pd[int(0.7*len(df_pd)) : ]$
ts.head()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2014-01-01&end_date=2014-01-02&api_key=S9bQCgmEaGHM8HSDLBNo')$ r.status_code
gdf.sample(10)
tweetdf['lga'].unique()
grouped.size().unstack().fillna(0).plot(kind="bar", stacked=True, figsize=(10,6)).legend(loc='center left', bbox_to_anchor=(1, 0.5))$ plt.show()
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'green', 'red', 'yellow'], grid=False)$
players=df.groupby('Player').agg({'Pts':np.average,'REB':np.average,'AST':np.average})$ top_five=players.nlargest(5,'Pts').index.tolist()$ players.nlargest(5,'Pts')
from sklearn.metrics import mean_squared_error, mean_absolute_error$ mean_absolute_error(y_test, y_pred_dt)
mlp_df.index
lrs = [0.0001, 0.0001, 0.0001, 0.0001, .001]
table.meta['TSMIN']
act['bin'] = pd.cut(act.activity_count, bins=[-1,0,1,2,5,100])$ gract = act.groupby(['month', 'bin']).activity_count.agg(['size'])$
foobar = np.dtype([('foo',int),('bar',float)])
validation.analysis(observation_data, BallBerry_resistance_simulation_0_5)
tfidf_matrix = tfidf.fit_transform(df_videos['video_desc'])$ tfidf_matrix.toarray()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
cityID = '0c2e6999105f8070'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anaheim.append(tweet)  
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY)
df_4_test.columns =  ['Hospital', 'Provider ID', 'State', 'Period', 'Claim Type', 'Avg Spending Hospital', 'Avg Spending State', 'Avg Spending Nation', 'Percent Spending Hospital', 'Percent Spending State', 'Percent Spending Nation']$ print df_4_test
daily_returns=portfolio_func.calc_daily_returns(closes)$ huber = sm.robust.scale.Huber()$ returns_av, scale = huber(daily_returns)
tlen = pd.Series(data=data['len'].values, index=data['Date'])
sauce = urllib.request.urlopen('https://pythonprogramming.net/sitemap.xml').read()$ soup = bs.BeautifulSoup(sauce, 'xml')$ soup
df_birth.population.value_counts(dropna=False).head()
joined.describe()$
plt.savefig("BarSentiment.png")$ plt.show()
test.nrows
data[data['authorName'] == 'Lunulls A. Lima Silva']#['link_weight']#.loc[3]
logodds[logodds.index.str.contains('you')]
rf=RandomForestClassifier(labelCol="label", featuresCol="features")$ labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels)$ pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,labelIndexer,OH1,OH2,OH3,OH4,assembler,rf,labelConverter])$
df_students.columns.tolist()
data = pd.read_csv('./fake_company.csv')$ data
jpl_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars"$ browser.visit(jpl_url)
X_copy['crfa_r'] = X_copy['crfa_r'].apply(lambda x: int(x))
store=join_df(store,store_states,"Store")$ weather=join_df(weather,state_names,'file','StateName')$ sum(store['State'].isnull()),sum(weather['State'].isnull())
analysis_historical =final_coin_data[['coin_name','date_hist','open_price','close_price']]$ a['Volitility_30_day'] = a['close_price'].rolling(window=30).std().reset_index(drop=True)$
for row in mw.itertuples():$     rtp = RichTextPage(content=row[4])$     rtp.save()$
table_rows = driver.find_elements_by_tag_name("tbody")[20].find_elements_by_tag_name("tr")$
activity = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\$ filter(Measurement.station == Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()$ activity
df_sched.fillna(value= '1900-01-01',inplace=True)
o_data = OrderedDict(sorted(data.items(), key=lambda t:t[0]))
df = pd.read_csv(location+'paula_ratings.csv.bz2', parse_dates=['date_of_birth', 'date_of_registration'])
def prepare_data_directory():$     if not os.path.exists(data_directory):$         os.makedirs(data_directory)
crimes['month'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.month)
from dotce.report import generate_chart
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local')$
pd.set_option('display.max_colwidth',100)
def ls_dataset(name,node):$     if isinstance(node, h5py.Dataset):$         print(node)
dul = pd.concat([dule2, dulp], ignore_index=True)$ dul
winpct.loc[winpct['text'].apply(lambda x: any(re.findall('Santos',x)))][['playId','homeWinPercentage','playtext','date']]
added_series = pd.Series(index=daterange)$
closes = pd.concat([msftA01[:3], aaplA01[:3]], keys=['MSFT', 'AAPL'])$ closes
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ commits_per_year.rename(columns={'author': 'num_commits'}, inplace=True)$ commits_per_year.head(5)
precip = session.query(Measure.date, Measure.prcp).\$     filter(Measure.date > '2016-08-23').\$     order_by(Measure.date).all() 
conn.execute(sql)
probarr2 = fe.toar(lossprob2)$ fe.plotn(fe.np.sort(probarr2), title="tmp-SORTED-prob")
augur.GitHubAPI.code_reviews = code_reviews$ ld = github.code_reviews('rails', 'rails')$
import pandas as pd$ import numpy as np$ np.random.seed(1234) 
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
close_series = stock_data['close']$ display(close_series.head(5)) 
display(data_rf.head(10))
btc = pd.read_csv('/home/rkopeinig/workspace/Time-Series-Analysis/data/btc.csv')$ btc['date'] = pd.to_datetime(btc['date'])$ btc = btc.set_index('date')
df['range'] = (df.max(axis='columns') - df.min(axis='columns'))
model_ADP = ARIMA(ADP_array, (2,2,1)).fit()$
pulledTweets_df.sentiment_predicted_nb.value_counts().plot(kind='bar', $                                                            title = 'Classification using Naive Bayes model')$ plt.savefig('data/images/Pulled_Tweets/'+'NB_class_hist.png')
!hdfs dfs -cat /user/koza/hw3/3.2.1/productFrequencies/* | wc -l
cityID = '3877d6c867447819'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Wayne.append(tweet) 
data['TMAX'].head()
B2.print_all_paths()$
stories = pd.concat([stories.drop('tags', axis=1), tag_df], axis=1)$ stories.head()
ratings_df = ratings_df.drop('timestamp', 1)$ ratings_df.head()
who_purchased.to_csv('../data/purchase.csv')
calls_nocontact.issue_type.value_counts()
for urlTuple in otherPAgeURLS[:3]:$     meritParagraphsDF = meritParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ meritParagraphsDF
our_nb_classifier = engine.get_classifier("nhtsa_classifier")
combined_salaries.to_csv(directory+'03_cleaned_salaries_for_app.csv',index=False)
! hdfs dfs -rm -R -f -skipTrash lastfm_model.spark$ model.save(sc,"lastfm_model.spark")$
successful_campaigns = results[results['tone']=='Positive']['campaign_id'].unique()$ print('first five positive campaigns (unordered): \n{0}'.format(successful_campaigns[:5]))
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
print('Before deleting out of bounds game rows:',injury_df.shape)$ injury_df = injury_df[(injury_df['Date'] > '2000-03-28') & (injury_df['Date'] < '2016-10-03')]$ print('After deleting out of bounds game rows:',injury_df.shape)
x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)
AFX = r.json()$ print(type(AFX))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)$ fse = r.json()$ fse
year7 = driver.find_elements_by_class_name('yr-button')[6]$ year7.click()
def tokenizer(text):$     return text.split()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1&api_key='+API_KEY$ r = requests.get(url)$
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=vq_k_-sidSPNHLeBVV8a')
recortados = [recortar_tweet(t) for t in tweets_data_all]$ tweets = pd.DataFrame(recortados)
df = pd.read_csv('weather.csv')
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-07-11&end_date=2018-07-11&api_key={}".format(API_KEY), $                  auth=('user', 'pass'))
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))
total_fare=pd.DataFrame(city_fare)$ totol_fare=total_fare.reset_index()$ totol_fare$
results = pd.read_csv('player_stats/{}_results.csv'.format(team_accronym), parse_dates=['Date'])$ results_postseason = pd.read_csv('player_stats/{}_results_postseason.csv'.format(team_accronym), parse_dates=['Date'])
number_of_commits = len(git_log.index)$ number_of_authors = git_log.author.nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
tweet_df.count()
prcp_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation'])$ prcp_df.set_index('Precipitation Date', inplace=True) # Set the index by date
FREEVIEW.plot_histogram(raw_fix_count_df)
tot_station= session.query(station).count()$ for station in session.query(station.station):$     print(station)
for columns in DummyDataframe2[["Positiv", "Negativ"]]:$     basic_plot_generator(columns, "Graphing Dummy Data using Percent" ,DummyDataframe2.index, DummyDataframe2)
active_stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()$ active_stations
df.head()
free_data.iloc[:5]
df.isnull().values.any()
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']])$ results = logit_mod.fit()
from scipy.stats import norm$ print("The significance z-score (p-value) is: %.4f" %norm.cdf(z_score))$ print("The critical value at 95%% confidence is: %.4f" %(norm.ppf(1-(0.05/2))))
xml_in_merged = pd.merge(xml_in_sample, grouped_authors_by_publication, on=['publicationKey'], how='left')
Base.classes.keys()
version = str(int(time.time()))
data["hours"] = data["time_up_clean"].astype(str).str[0:2]$ data["hours"] = data["hours"].astype(int)$ data.loc[data["hours"] == 0, "hours"] = 1
test_pred_svm = lin_svc_clf.predict(test_cont_doc)
store_items.fillna(method='backfill', axis=0)$
newsMood_df['Timestamp'] = pd.to_datetime(newsMood_df['Timestamp'])$ newsMood_df.to_csv('newsMood_dataframe.csv')$ newsMood_df.head()
MNB = MultinomialNB()$ model3 = MNB.fit(x_train, y_train)
LabelsReviewedByDate = wrangled_issues_df.groupby(['created_at','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
sqladb.head()
news_df.head()
nasa_articles = soup.find_all("div", class_="slide")
mvrs = ratings.groupby('movieId').size().sort_values(ascending=False)$ tmp_ratings = ratings.ix[mvrs[mvrs > rating_count].index].dropna()
yc_new2['Tip_Amt'] = yc_new['Tip_Amt'] / yc_new['Fare_Amt'] * 100$ yc_new2.head()
pred_labels = lr2.predict(test_data)$ print("Training set score: {:.2f}".format(lr2.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(lr2.score(test_data, test_labels)))
t3['timestamp'] = t3.index$ t3.reset_index(drop=True,inplace=True)
for item in result_set:$     print(item.index,item.education)
r = requests.get(url1)$ json_data = r.json()
df.resample('M').mean()
data.learner_id.value_counts()
apple.resample('M').mean()
url = "https://mars.nasa.gov/api/v1/news_items/?order=publish_date+desc"$ response = requests.get(url).json()$ body = response.get('items')[0]$
train_df = stats_diff(train_df)$ print(train_df.head(5))
session.query(func.count(Measurement.date)).all()
model_df = topics_data.drop(['body', 'comms_num', 'id', 'title'], axis=1)
data_year_df = pd.DataFrame.from_records(data_year, columns=('Date','Station','Prcp'))$ data_year_df.head()
pd.crosstab(train.CIA, train.L2_ORGANISATION_ID)
warnings.filterwarnings('ignore')$
output = pd.DataFrame(data={"id":test["id"], "sentiment":result,})# "probs":result_prob[:,1]})$ output.to_csv(os.path.join(outputs,'Word2Vec_AverageVectors.csv'), index=False, quoting=3)
n_estimator = 100$ rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator, n_jobs=3, verbose=2)$ rf.fit(X_train, y_train)$
print(df.columns)
cityID = '3df0e3eb1e91170b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Columbus.append(tweet) 
df.isnull().sum().sum()
data.index[0] = 15
df.set_index('datetime',inplace=True)$ df.index
k ['MONTH'] = k.index.map(dict)
theft = crimes[crimes['PRIMARY_DESCRIPTION']=='THEFT']$ theft.head()
g = sns.catplot(x="AGE_groups", col="goodbad",$                  data=data, kind="count",$                  height=5, aspect=.9);
model_info = kipoi_veff.ModelInfoExtractor(model, Dataloader)$ vcf_to_region = kipoi_veff.SnvCenteredRg(model_info)
dfSummary = pd.concat([sumAll,sumPre,sumPost],axis=1)$ dfSummary.columns = ("all","before","after")
import name_entity_recognition as ner$ ner.load_data()
y = df['comments']$ X = df[['title', 'age', 'subreddit']].copy(deep=True)
pd.date_range('1/1/2000', '1/1/2000 23:59', freq='4h')$
print('RF: {}'.format(rf.score(X_test, y_test)))$ print('KNN: {}'.format(knn.score(X_test, y_test)))
questions['zipcode'].unique()$
sample.groupby('tasker_id').hired.count().sort_values(ascending=False).head(1)
kick_projects.groupby(['main_category','state']).size()$
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
df=pd.read_csv("../Raw Data/Trump tweets 3 day groups jan-dec 2017.csv")$ df.head()
population = evolve_new_generation(tp)$ floreano_experiment = FloreanoExperiment(population, 15)$ floreano_experiment.run_experiment()
print(df,'\n')$ print(df.apply(lambda x: x.max()-x.min()))
df = pd.read_csv("ab_data.csv")$ df.head()
Regex to get the Facebook Page ID from a given URL$ https://gist.github.com/marcgg/733592$ http://bhan0507.logdown.com/posts/1291544-python-facebook-api
len(df[~(df.data == {})])
b2b_df = pd.read_csv(data_fp, header=None, names=['brain_weight', 'body_weight'])$ b2b_df.head()
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True)
test[['clean_text','user_id','predict']][test['user_id']==1895520105].shape[0]
y_pred = gnb.predict(X_clf)
df3.values
df2['timestamp'] = pd.to_datetime(df2['timestamp'], format='%d/%b/%Y:%H:%M:%S +0000', utc=True)
from pyspark.sql.functions import regexp_extract, col$
first_row=session.query(meas).first()$ first_row.__dict__
session.query(Measurement.station,func.count(Measurement.station)).\$         group_by(Measurement.station).\$         order_by(func.count(Measurement.station).desc()).all()
fix_space_0 = lambda x: pd.Series([i for i in reversed(x.split(' '))])
%%time$ data_demo["num_child"] = data_demo["comment_id"].apply(lambda x: data_demo[data_demo["parent_id"]==x].shape[0])
output_path = "/Users/kai.bernardini/Documents/MA575/stock_data/"
%matplotlib inline$ import seaborn as sns$ sns.heatmap(pd.DataFrame(years).T.fillna(0))
for user in targets:$     clean_sentiments = clean_sentiments.append(news(user))
shots_df[['PKG', 'PKA']] = shots_df['PKG/A'].str.split('/', expand=True)$ shots_df.drop('PKG/A', axis=1, inplace=True)
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLAday.csv"$ tesla_days = pd.read_csv(url, index_col=0, parse_dates=True)$ tesla_days
faa_data_minor_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "M"]$ print(faa_data_minor_damage_pandas.shape)$ faa_data_minor_damage_pandas.head()
trump.describe()
df_plot=pd.DataFrame(df_recommended_menues['dates'].value_counts())$ df_plot.head()
conn.commit()
msft = pd.read_csv('msft.csv', index_col=0)$ msft.head()
mgxs_lib.dump_to_file(filename='mgxs', directory='mgxs')
with open('./data/Responses.json') as file_in:$     for line in file_in:$         is_json(line)
public_tweets = api.user_timeline(target_user,count=1)
json_data['dataset'].keys()
pd.DataFrame(data['data']['children'])    # will give us the kind of data as well as the subreddit id
df.resample('H').mean()
tfav2 = tfav.values$ SA2 = SA1 * 100000
fit_time = (end_fit - start_fit)$ print(fit_time/60.0)
max_open = ldf['Open'].max()$ max_open
check_null = df.isnull().sum(axis=0).sort_values(ascending=False)/float(len(df))$ np.sum(check_null.values) == 0
beginDT = '2017-02-01T00:00:00.000Z'$ endDT = '2017-09-01T00:00:00.000Z'$
y_pred = model.predict(x_test, batch_size=1024, verbose=1)
displacy.serve(doc, style='ent')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
fin_coins_r.isnull().sum()
popCon.sort_values(by='counts', ascending=False).head(9).plot(kind='bar')
from tensorforce.agents import PPOAgent$ from tensorforce.core.networks import LayeredNetwork, layers, Network, network
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/EON_X.json?api_key='+API_KEY+'&start_date=2018-06-28&end_date=2018-06-28')
df3['tweet'] = df3['full_text'].where(pd.notna(df3['full_text']), other = df3['text'])
yc_new = yc_depart.merge(destinationZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')$ yc_new.shape
sumPre = dfPre['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])$ sumPost = dfPost['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])
df.to_csv('top40_processed.csv', date_format="%Y-%m-%d", index=False)
import pandas as pd$ y= pd.Period('2016')$ y
total_ridepercity=pd.DataFrame(ride_percity)$ total_ridepercity=total_ridepercity.reset_index()$ total_ridepercity
y_train.value_counts(normalize=True)
df1 = pd.DataFrame(np.random.randn(6,3),columns=['col1','col2','col3'])$ df2 = pd.DataFrame(np.random.randn(2,3),columns=['col1','col2','col3'])$ print(df2.reindex_like(df1))
cityID = 'e41805d7248dbf1e'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Modesto.append(tweet) 
ensemble_preds = np.round((preds1 + preds2 + preds3 + preds4 + preds5) / 5).astype(int)$ print(ensemble_preds.mean())
stock['target'] = stock.daily_gain.shift(-1)
instance.assumptionbreaker()
df1.columns$ df2=df1.drop(['creation_day','object_id','name', 'email','last_session_creation_time','invited_by_user_id','Referral_create_time','creation_time'],axis=1)$ df2.columns
start_of_event=datetime.datetime.strptime(start_of_event,'%Y-%m-%d') #Convert to datetime$ end_of_event=datetime.datetime.strptime(end_of_event,'%Y-%m-%d') #Convert to datetime$ location_name=location_name.replace(" ","_") #replace spaces with underscore
table = soup.find('table')$ print (table)
df_urls = pd.read_csv('ny_times_url_dataframe.csv', encoding = 'utf-8')$ df_urls.shape$
pd.crosstab(train.TYPE_BI, train.TYPE_UT)
dupes_to_delete = dfd.duplicated(subset=['brand', 'outdoor_model', 'indoor_model'])$ dupes_to_delete.value_counts()
df = pd.DataFrame.from_records(mylist)$ df.head()
hours.shape$
html = browser.html$ img_soup = BeautifulSoup(html, 'html.parser')
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', na_values=['.'])$ mydata.head(5)
spks = np.loadtxt('output/spikes.csv')$ print(spks[1:10, :])
tweet_df = tweet_df.reset_index()[['created_at','id','favorite_count','favorited','retweet_count','retweeted','retweeted_status','text']]
url = "https://space-facts.com/mars/"$ table = pd.read_html(url)$ print(table)
df.loc['20180103', ['A','B']]
city_ride_data = clean_combined_city_df.groupby(["type"]).sum()["fare"]$ city_ride_data.head()$
df = pd.read_csv('data/test1.csv', parse_dates=['date'], index_col='date')$ df
summaries = "".join(df.title)$ ngrams_summaries = cvec_1.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
pt = jdfs.pushed_at.apply(lambda x: time.mktime(x.timetuple()))$ npt = pt - pt.min()
with open('train.csv') as f:$     size=len([0 for _ in f])$     print("Records in train.csv => {}".format(size))
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data2.csv"$ mydata = pd.read_csv(path, sep =',', header=None)$ mydata.head(5)
today = datetime.datetime.today()$ start_date = str(datetime.datetime(today.year, today.month-1, 1).date())$ print('predictions for', start_date)
file_name = str(time.strftime("%m-%d-%y")) + "-tweets.csv"$ tweet_df.to_csv("analysis/" + file_name, mode = 'w',encoding="utf-8",index = False)$ 
filepath = os.path.join('input', 'input_plant-list_SI.csv')$ data_SI = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_SI.head()
csvfile ='results.csv'$ resdf = pd.read_csv(csvfile)$ resdf.info()
plt.rcParams['figure.figsize'] = 8, 6 $ plt.rcParams['font.size'] = 12$ viz_importance(rf_reg, wine.columns[:-1])
y = w * x + b$ loss = K.mean(K.square(y-target))
import requests$ import json$ from pandas.io.json import json_normalize
for img in featured_image_url:$     link = img.a['data-fancybox-href']$ link
with open("Valid_events_eps0.7_5days_500topics","rb") as fp:$     Valid_events = pickle.load(fp)
tips.sample(5).reset_index(drop=True)
userMovies = userMovies.reset_index(drop=True)$ userGenreTable = userMovies.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ userGenreTable
station_df.head(10)
plt.scatter(X2[:, 0], X2[:,1], c=dayofweek, cmap='rainbow')$ plt.colorbar()
pd.merge(msftAR0_5, msftVR2_4)
more_info_elem = browser.find_link_by_partial_text('more info')$ more_info_elem.click()
print('Best Score: {:.3f}'.format(XGBClassifier.best_score))$ print('Best Iteration: {}'.format(XGBClassifier.best_iteration))
calc_mean_auc(product_train, product_users_altered, $               [sparse.csr_matrix(item_vecs), sparse.csr_matrix(user_vecs.T)], product_test)$
df2[df2.duplicated(['user_id'], keep=False)]
stock_subset = stock_data[ [ 'open', 'close' ] ] $ print(display(stock_subset.head(5)))
df.select('station','year','measurement').show(5)
P_new = ab_df2['converted'].mean()$ print(P_new)
from IPython.core.display import HTML$ css_file = '../style/style.css'$ HTML(open(css_file, "r").read())
tobs_data = []$ for row in temperature_data:$     tobs_data.append(row[0])$
df_2003['bank_name'] = df_2003.bank_name.str.split(",").str[0]$
df_first_days['active_user'] = df_first_days['user_id'].apply(lambda user: 1 if user in active_users else 0)$ df_first_days = df_first_days.merge(df_user_info[['user_id', 'marketing_source']], on='user_id', how='left')
stat_info = stations['name'].apply(fix_comma)$ print(stat_info)
output= "CREATE TEMPORARY TABLE ABC AS select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc"$ cursor.execute(output)$
%matplotlib inline$ commits_per_year.plot(kind='bar',title='Linux Commits by Year',legend=False)
measurement_df.describe()
import requests$ urlkorbase='http://www.koreabaseball.com/Record/Player/HitterBasic/Basic1.aspx'$ data=requests.get(urlkorbase).text$
caption_text_clean = [review_to_words(doc, True) for doc in captions.caption]$ caption_text_clean[:10]
t3.drop(t3[t3['retweeted_status'].notnull()== True].index,inplace=True)
temp_df2['titles'] = temp_df2['titles'].str.lower()
tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))$ tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0])$ print(tips.head())
df['time'] = df['text'].str.extract(r'(\d?\d:\d\d\s?[a-z]{2})')
data = pd.read_csv('dog_rates_tweets-e7.csv', parse_dates=[1])$
grid_pr_size.plot.scatter(x='glon',y='glat', c='size_val', $                            colormap = 'RdYlGn_r')
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])$
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
res = sts.query(qry)
response = requests.get('http://www.reddit.com/r/goodnews')$ page_source = response.text$ print(page_source[:1000])
coarse_groups = openmc.mgxs.EnergyGroups(group_edges=[0., 20.0e6])$ coarse_mgxs_lib = mgxs_lib.get_condensed_library(coarse_groups)
df = pd.read_excel("../../data/stocks.xlsx")$ df.head()
songLink = 'https://www.youtube.com/embed/'+soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href'][9:]$ HTML('<iframe width="560" height="315" src='+songLink+' frameborder="0" allowfullscreen></iframe>')
from bmtk.builder.networks import NetworkBuilder$ net = NetworkBuilder('V1')
stocks.loc['Apple']
roundedDF=np.around(overallDF, decimals=2)$ roundedDF["Compounded Score"]
! rm -rf models2$ ! mrec_train -n4 --input_format tsv --train "splits1/u.data.train.*" --outdir models2 --model=knn
print(discharge_list[0].date)$ print(discharge_list[len(discharge_list)-1].date)
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,300], ylim=[0,300], title="Favorites and Retweets:300x300")$
russians_df = pd.read_csv('users.csv')
results = soup.find('div', class_="rollover_description_inner")$ results2 = soup.find('div', class_="content_title")
xyz = json.dumps(youtube_urls, separators=(',', ':'))$ with open('youtube_urls.json', 'w') as fp:$     fp.write(xyz)$
logreg = LogisticRegression()$ logreg.fit(X_train, y_train)$ logreg.score(X_test, y_test)
df.head()
attend_with = questions['attend_with'].str.get_dummies(sep="'")
with model:$     idx = np.arange(n_count_data)$     lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)
df.columns=['created_at','id_str','in_reply_to_user_id_str','raw_text']$ df = df.drop(columns='created_at')
AAPL.info()
hawaii_measurement_df.head(10)
rng.day
new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))$ new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)$ new_user_recommendations_rating_title_and_count_RDD.take(3)
df[['text', 'favorite_count', 'date']][df.favorite_count == np.min(df.favorite_count)]
store_items = store_items.drop(['store 2', 'store 1'], axis = 0)$ store_items
sales_df.groupby('Country').count()['Revenue'].sort_values(ascending=False)$
import IPython$ print (IPython.sys_info())$ !pip freeze
count_by_Confidence = grpConfidence['MeanFlow_cfs'].count()$ count_by_Confidence.plot(kind='pie');
print(list(cos.buckets.all()))
top_10_authors = git_log.author.value_counts().nlargest(10)$ top_10_authors
leadsdf.to_csv('Leads App Report From 30-08-2017 to 02-09-2017.csv')$
precip_df = history_df['precipitation']$ precip_df.describe()
df3.index
daily_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner')$ daily_df.head()
import nltk; nltk.download('stopwords')
df.iloc[::20].plot.bar(title="Percipitation")$ plt.tight_layout()$ plt.show()$
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()
con = cx_Oracle.connect(connection_string)
X_prepro = psy_prepro.drop(labels=["y"], axis =1)$ y_prepro = psy_prepro["y"]
soup.p
from IPython.display import HTML$
google_stock.isnull().any()
if not os.path.isdir('output/wind_generation'):$     os.makedirs('output/wind_generation')
Base.prepare(engine, reflect=True)
yc_new2[yc_new2.tipPC > 100]
max_div_stock=df.iloc[df["Dividend Yield"].idxmax()]$ max_div_stock$ print("The stock with the max dividend yield is %s with yield %s" % (max_div_stock['Company Name'],max_div_stock['Dividend Yield']))
np.random.seed(123)$ my_model_q1 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label')$ my_model_q1.fit(X_train, y_train)
model.most_similar('hand',  topn=100)$
seng = 'mysql+pymysql://root:'+spassword+'@localhost/sakila'$ engine=create_engine(seng)$ data = pd.read_sql_query('select * from actor', engine)$
temp_data_query = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
df2 = pd.read_csv("../../data/msft.csv",usecols=['Date','Close'],index_col=['Date'])$ df2.head()
session.query(Measurement.station, func.count(Measurement.station).label('count')).\$                                   group_by(Measurement.station).\$                                   order_by('count DESC').all()
for row in session.query(measurements, measurements.tobs, stations.station).limit(5).all():$     print(row)
AAPL.to_csv('/home/hoona/Python/mpug/directFromPandaDataFrame.csv')
td_norm = td ** (10/11)$ td_norm = td_norm.round(1)
data[(data['author_flair'] == 'Broncos') & (data['win_differential'] >= 0.9)].comment_body.head(15)$
centers_df["latitude"] = pd.Series(lat)$ centers_df["longitude"] = pd.Series(lon)
y_train_hat = model.predict(X_train)$ y_train_hat = squeeze(y_train_hat)$ CheckAccuracy(y_train, y_train_hat)
df_tick_sent = df_tick.join(df_amznnews_2tick)
display(data.head(20))
dfa.head()
backers = databackers.pivot_table(index = 'backers', columns='successful',aggfunc=np.size)$ backers = backers.rename(columns= {1: 'Successful', 0:'Failed'})$ backers[backers['Successful'] == backers['Successful'].max()]
X_copy['outproc_flag'] = X_copy['outproc_flag'].apply(lambda x: np.where(x=='N',0,1))
store_items.interpolate(method='linear', axis=0)
newfile.to_excel('most_excellent.xlsx', sheet_name='test_sheet')
data.count()
lc_review = pd_review["text"][0].lower()$
df_session_dummies = pd.get_dummies(df, columns=['action'])$
player['event'] = np.where((player['events'] == 'home_run'), #|$                            1, 0)$ player['event'].sum()
S_1dRichards.decision_obj.simulStart.value, S_1dRichards.decision_obj.simulFinsh.value
data = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY)
mod_model.xls2model(new_version='new', annotation=None)$ scenario = mod_model.model2db()$ scenario.solve(model='MESSAGE', case='GHD_hospital')
df_active_user_metrics['group_code_activations'].value_counts()
from nltk.stem.porter import PorterStemmer$ stemmed = [PorterStemmer().stem(w) for w in words]$ print(stemmed)
xml_in_sample.shape
print('The Jupyter notebook stores information in the "Kernel".\$       \nRestart the Kernel to clear noteook memory.')
print(Probit(y, X).fit().summary())
word_counts = bow_features(sentences, words_in_articles)$ word_counts.shape$
X = pd.get_dummies(X, drop_first=True)
!pwd #location of the current working directory. $
concat4 = pd.concat([free_sub,ed_level], ignore_index=True)$ merge4 = pd.merge(left=free_sub, right=ed_level, how='outer' ,left_index=True, right_index=True)
stations = "Resources/data/hawaii_stations.csv"
for key,value in deaths_sorted.items():$     print( "Deaths in " + str(key) + " = " + str(value))
!head -n 2 Consumer_Complaints.csv
model = RandomForestClassifier(n_estimators = 10)$ model.fit(X, y)
with open(os.path.join(os.getcwd(),"data/credentials.json")) as data_file:    $     key = json.load(data_file)$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
prcp_df = pd.DataFrame(prcp)$ prcp_df.head()
learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr=(32,10)) $
idx = pd.period_range('2011',periods=10,freq='Q')$ idx
pd.date_range('1/1/2017', '12/1/2017', freq='M') $
validation.analysis(observation_data, distributed_simulation)
crime = crime[crime.Sex != '*']
result3.summary2()
dts = list(map(pd.to_datetime, observation_dates))$ dts
(ggplot(all_lum_binned.query("lum>0&subject=='VP3'"),aes(x="td",y="gy"))+geom_smooth(method='loess'))+facet_wrap("~eyetracker")+xlim(-1,4)
for model_name in nbsvm_models.keys():$     test_probs[model_name] = nbsvm_models[model_name].predict_proba(test_cont_doc)
ratings.describe()
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
ab_df.converted.mean()
print(df.head())
trump_df.text = trump_df.text.str.lstrip("b'")
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))$
s4g.groupby('Symbol')
ebola_melt['country'] = ebola_melt.str_split.str.get(1)$ ebola_melt.head()
paired_df[['dataset_1', 'dataset_2']] = paired_df.paired_datasets.apply(pd.Series)$ paired_df = paired_df.loc[:, ['dataset_1', 'dataset_2','co_occurence']]$ paired_df = paired_df.loc[paired_df.co_occurence > 1]
df6 = df4.where( (hours > 10) & (hours < 13)) # show lunch data rows only$ df6 = df6[df6['BG'].notnull()]$ df6 # got same data as previous technique
pm_data = pd.concat([pm_data,$                      pd.DataFrame(pm_data.groupby('unit_number').cumcount(), columns = ['obs_count'])]$                     , axis = 1)
grouped2 = df_cod3.groupby(["Death year", "Cause of death"])$ grouped2.size()
unique, counts = np.unique(y_hat, return_counts=True)$ print(unique, counts)
health_data_row.xs(key=(2013 , 1), level=('year', 'visit'))
afx_17 = json.loads(afx_x_2017.text)$ type(afx_17)
dates = pd.date_range('8/1/2017', periods=100, freq='W-MON')$ dates
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1&api_key='API_KEY$ r = requests.get(url)
df.loc[1:4,"Date"]
for i, words in enumerate(chefdf['name']):$         words = words.replace(char,' ')$     words = words.replace('  ',' ')
url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'$ browser.visit(url)
ndExample = df.values$ ndExample
eth = pd.read_csv("Ether-chart.csv", sep=',')$ eth['date'] = ' '$ eth.info()$
table_names = ['train']$ str('{PATH}{fname}.csv')
dr_new.columns$
engine = create_engine('sqlite:///results.db')$ pd.read_sql_query('SELECT * FROM demotabl LIMIT 5;',engine)
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'ab_page','US', 'UK']])$ result = logm2.fit()$ result.summary2()
eegRaw_df.to_csv(outputData, index=False, header=False) 
import geopandas as gpd$ df = gpd.pd.read_csv('data/Places_Full.csv')$ dfD = gpd.pd.read_csv('data/Dist_Out.csv')
ids = topics_data.groupby('id')[['score', 'comms_num']].sum()
weather_cols = ['Weather', 'TempF', 'TempC', 'Humidity', 'WindSpeed', 'WindDirection', 'Pressure', 'Precip', ]$ weather_df = weather_df[weather_cols]$ weather_df.head()
USvideos.info()
year1 = driver.find_element_by_name('2001')$ year1.click()
pd.set_option('display.max_rows', 500)
data_2018 = data_2018.reset_index()
X.columns
table_grouped_type=original_merged_table.groupby("type")$
store = pd.HDFStore("../../data/store.h5")$ df = store['df']$ df
d = feedparser.parse('http://rss.nytimes.com/services/xml/rss/nyt/InternationalHome.xml')
sts_c_model = logit('label ~ total_bill + tip + size + C(sex) + C(day) + C(time)', $                     data=tdf).fit()$ sts_c_model.summary()
def jprint(j):$     print(json.dumps(j,indent=4,sort_keys=True))
r.groupby('cat').count()
for c, f in zip(tips_model.coef_[0], features.columns.values):$     print(f'{c:5.2f} * {f}')
station_df =pd.read_sql('station',engine)$ station_unique_df = station_df['station']$ station_unique_df.nunique()$
pystore.delete_store('mydatastore')$
from spacy.matcher import Matcher
nba_df.drop([30,31], axis=0, inplace= True)$ nba_df.loc[28:34]
precip_df = pd.DataFrame(precip)$ date_precip_df = precip_df.set_index("date")$ date_precip_df.head()$
df_kick= kickstarters_2017.set_index('ID')
print("# of files in unsubscribed :", filecount(os.fspath(master_folder + lists + "unsubscribed/")))$ print("# of files in members :", filecount(os.fspath(master_folder + lists + "members/")))$ print("# of files in cleaned :", filecount(os.fspath(master_folder + lists + "cleaned/")))
precip_data = session.query(Measurements).first()$ precip_data.__dict__
merge_df['Screen A'] = pd.to_numeric(merge_df['Screen Size'].str.split('x').str[0])*pd.to_numeric(merge_df['Screen Size'].str.split('x').str[1])$ merge_df['Screen Q'] = pd.qcut(x=merge_df['Screen A'],labels=['small','medium','large'],q=3)
check_null = df.isnull().sum(axis=0).sort_values(ascending=False)/float(len(df))$ print(np.sum(check_null.values) == 0.0)
df.loc[:,"Date"] = 
summed.fillna(method='pad')  # The NaN column remained the same, but values were propagated forward$
random.shuffle(porn_ids)$ porn_bots = porn_ids[:6000]
unassembled_human_genome_length = gdf[gdf['type'] == 'supercontig'].length.sum()$ percentage_incomplete = (unassembled_human_genome_length / human_genome_length)*100$ print("{}% of the human genome is incomplete.".format(round(percentage_incomplete, 4)))
learn.data.test_dl = test_dl$ log_preds_test = learn.predict(is_test=True);
tweet_length.plot(figsize=(20,5),color='r')
df_features2.to_csv("data new/featureslrfmp.csv", encoding="utf-8", sep=",", index=False)
high_sta_obs = session.query(measurements.station, func.count(measurements.stations)).\$                 group_by(measurements.station).order_by(measurements.tobs).all()$ high_sta_obs
cityID ='5c62ffb0f0f3479d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Phoenix.append(tweet) 
files8= files8.drop('EndDate',axis=1)$ files8= files8.drop('StartDate',axis=1)$ files8.head()$
contractor_clean[contractor_clean.city.isnull()]
df2[df2.duplicated('user_id',keep=False)]$
print(dfd.in_pwr_47F_min.describe())$ dfd.in_pwr_47F_min.hist()
index = pd.date_range('2018-3-1', periods=1000, freq='M')$ index 
%matplotlib inline$ commits_per_year.plot(kind="line", title="Commits Per Year", legend=False)
df_imputed_median_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.median())
labeled_news[0].describe()
df_CLEAN1A['AGE'].min()
iso_gdf.plot();
user_df = user_df.rename(columns={'created_at': 'user_created_at'})
classifier = svm.SVC(kernel='linear')$ classifier.fit(X_train, y_train)$ y_prediction_SCM = classifier.predict(X_test)$
newfile = newfile.loc[newfile['Delivery block'].notnull()]
x = store_items.isnull()$ print(x)$
df.head(5)
msftAC = msft['Adj. Close']$ msftAC['2012-01-03']
%config InlineBackend.figure_format = 'svg'$ %config InlineBackend.figure_format = 'retina'
clean_sentiments['Date'] = pd.to_datetime(clean_sentiments['Date'])
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&API_KEY'$ r = requests.get(url)
list(bow_test.columns).index("word2vec_90")$ print((bow_test.columns))$
median_comments = reddit['num_comments'].median()$ median_comments
all_tables_df.loc[:, 'OBJECT_NAME']
new_group.get_group('N')
df["EDAD"].apply(sumar_1)
v_today = datetime.datetime.now()$ today_str  = '('+v_today.strftime("%m/%d/%y")+')'$ today_str_plot  = v_today.strftime("%m-%d-%y")$
for c in ccc:$     ved[c] = ved[ved.columns[ved.columns.str.contains(c)==True]].sum(axis=1)
new_model = gensim.models.Word2Vec(min_count=1)  $ new_model.build_vocab(sentences)                     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)
coll.distinct("overall_status")
conn.execute(q)
df1['forcast']=np.nan
h3 = qb.History(spy.Symbol, datetime(2014,1,1), datetime.now(), Resolution.Daily)
y = tweets['handle'].map(lambda x: 1 if x == 'realDonaldTrump' else 0).values$ print max(pd.Series(y).value_counts(normalize=True))
import nltk.data$ tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')$
tlen = pd.Series(data=df['len'].values, index=df['Date'])$ tfav = pd.Series(data=df['Likes'].values, index=df['Date'])$ tret = pd.Series(data=df['RTs'].values, index=df['Date'])
urbanData_df = data_df[data_df.type == "Urban"]$ suburbanData_df = data_df[data_df.type == "Suburban"]$ ruralData_df = data_df[data_df.type == "Rural"]
print(rhum_nc)$ for v in rhum_nc.variables:$     print(rhum_nc.variables[v])
grads = K.gradients(loss, [w,b])$ updates = [(w, w-lr*grads[0]), (b, b-lr*grads[1])]
df_TempIrregular['timeStamp'] = pd.to_datetime(df_TempIrregular.pubTimeStamp)$
df_links = df_links[df_links['link.domain'] != 'twitter.com']
merged_stops = pd.merge(stops, oz_stops, on='stopid', how='inner')
artistByID.filter(lambda line: '[unknown]' in line).take(5)
title = soup.title.text$ print(title)
titles_list = temp_df2['titles'].tolist()
style_bw.tail(5)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
media_classes = [c for c in df_os.columns if c not in ['domain', 'notes']]$ breakdown = df_questionable[media_classes].sum(axis=0)$ breakdown.sort_values(ascending=False)
np.shape(ndvi_coarse)
r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))$ data = r.json()
if 1 == 1:$     news_titles_sr = pd.read_pickle(news_period_title_docs_pkl)
temps_df.Missoula
ex=savedict['2017-11-15']$ ex.head()
tweet_archive_clean['tweet_id'].isin(tweet_image_clean['tweet_id']).value_counts()$
df2 = tier1_df.reset_index()$ df2 = df2.rename(columns={'Date':'ds', 'Incidents':'y'})
pd.crosstab(df_result.launched_year, df_result.State).plot()
