#Look at the first few lines of the new dataframe
# quick and dirty investigation for LTDate
# If we plot all 3 graphs together, we get: # Note that the length appears to be a horizontal line because it's scale (max length = 144) in comparison to  
# split features and target
#admissions api connection
# ciscij10
# Histograms
#globalCity_pdf = 'http://journals.sagepub.com/doi/pdf/10.1177/0042098007085099'
# Import Airbnb user data.
#2. Get the row names from the above files.
# The inverted exponential of the negativ coefficient defines the relationship to the base variable. 
#http://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64
#convert dictionary to dataframe
# Load relevant libraries for tf-idf and k-means clustering
#split zipcode columns into two columns by '-'
# The row info of the repeated user_id
# This is the fastest way to swap a dictionary key / value order.   # This is the format gensim LDA expects it's vocabulary.
# connect to MongoDB cluster
# first year a ship sailed (check if 2003 when data begins or shortly after)
# Plot toy dataset per feature #sns.pairplot(df, hue='labels');
# The user "Rezeptesammlerin" seems to have many duplicate recipes # How many recipes does she have in total?
# Http response Headers is a dictionary
# Let's see our classes numbers. Not bad, 312 vs. 2165. Unbalanced, but not insignificant
# Finding difference between values of consecutive datetimes
# SORT AND THEN DROP non numric values
# In the Data Scientist Workbench (DSWB) you can prefice commands with a ! to run shell commands # here we remove any files with the name of the file we are going to download # then download the file
# Apparently the Date column is lying about its real name.  Going to have to legally change it.
# add up all the lengths
#using internal id to remove the student with two pres,one in a good session with logs, one not #id = 17931169
# Create a Pandas Data Frame from a list of Series.
# March 2016 was our start period, and the period frequency is months.
# just get the rows that don't have a dog stage
#returns all indices where treatment and new page don't match
#Check via customer sample
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Run Mood's test for male vs unknown
#Find out rows where treatment and new page don't line up
# Display first five lines of combined DataFrame
# run the model giving the output the suffix "rootDistExp"
#create dataframe
# Take the mean of the two probabilities for the probability of conversion under the null hypothesis
#writing yearmonth and counting no of files into csv
# write the downloaded file in to image.predictions.tsv
# Not really able to tell with such few responses in so many categories. # I imagine we'll get something similar if we pull NPS scores by countries as well...
# Show the row details for the duplicated user_id
# read the file into a single string
#Create a new dataset where we only get the observations for USA. Made sure to reset the index of new dataframe
# save model as...
# Apply rule of thumbs to top10 
# creo columnas para agrupar
# of unique users = len
# df_2001
# Predict the rating of known stub article.
# Get the average popularity by decades
# What are the dimensions # What are the column names # What do the first few rows look like
# Rural cities total drivers
#Number of samples of New page data
# Import data
#recarguemos el archivo parseando ahora issued_date como date
### male users are more likely to post about Michael Kors
# Calculate probability of conversion for new page
#'Durham': 'bced47a0c99c71d0',
# watch out with label-based indexing when indices are scrambled: # we will get all values with indexes 1 and 7 and all that happen to be in between
# Compute proportion of users converted without any further data investigation
#impute first_affiliate_tracked missing values with the mode
#gmap = gmplot.GoogleMapPlotter(40.7128, 74.0059, 8)
# Stops that are in oz_stops but not in stops # Note: these stops do not exist when looked up online!
#using iloc to fetch the 11,24 & 37 row ids
# find the non-numeric feature columns
# make a HUGE horizontal bar chart so we can see the distribution of 311 complaints # it took me a bunch of guesses to figure out the right figure size
#create a other category on auteur incident (feature engineering mb, like more than 1000 vs less ?) #same on RESOURCE_ID
#p_new = df2.converted.mean()
# use reindex
# A few of the biggest ones
# Use the session to query Demographics table and display the first 5 locations
### Create the necessary dummy variables
# Export to CSV
# Instantiate a "coarse" 2-group EnergyGroups object # Instantiate a "fine" 8-group EnergyGroups object
# For demo purposes we can look what our invoice looks like
# Create a new dataset in which group and landing_page is aligned correctly
#select the latest sprint that the stories are in and then we can filter the ones that sprints that are closed. #only after the above is done, we can filter the stories that have their latest sprints closed
# manufacture some loss function # there are n_epochs * n_batchs * batch_size  # recorded values of the loss
# Create fuel assembly Lattice
# print the text of the first result.
# prediction on test set
# Reopen
#merging the two dataframes
# Create a feature to indicate whether it is weekend.
#confirm duplicate row dropped
# Look for indexes which should be changes for treatment group # Update values # Change datatype
# checking that the duplicate has been removed - this should 0
## limit the period to the current time
# reset the index to the date
# http://w2.weather.gov/climate/xmacis.php?wfo=mtr
# Re-format dates in date column # Rename columns before final export
# read, but skip rows 0,2 and 3
# A:
# deleting duplicate record 
# cutting on too short questions because metrics are not working for those (yet)
#run for March users
#Save the new changes
# We can also select which data from a dictionary we want to put into the data frame # use the keyword column
# # To create a pandas dataframe from the events list # To preview the dataframe by first 5 rows.
# Save your regex in punct_re
#load pickled dataframe
# To check details of duplicated row 
# favorite table # for row in table_rows: #     print(row.text)
# Reference # https://radimrehurek.com/gensim/tut2.html
# since values are 1 and 0, we can calculate mean to get probability of an individual converting 
#For our analysis we will choose top 20 brands
# created in 5. Tidy Data
# Instantiate a Materials object # Export to "materials.xml"
# Data is saved in *master.csv* at source, allowing it to be reloaded for future uses.
# Replicate booking = 1 by 4 times
# what does the dataframe look like now?
### total number of posts and unique users mentioning Michael Kors
#plt.axis("equal")
#Print out the schema to see the shape of the data
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
#Dropping the duplicates to then merge later
#monthly variance of the portfolio
# Display of first 10 elements from dataframe:
# your code here
## add necessary libraries #%pylab inline #Populating the interactive namespace from numpy and matplotlib
# let's look at descriptive statistics of the twitter dataframe
# we can use info function to check number of values in each row
#df[['units_purchased','order_date','cust_id']].groupby(['cust_id']).agg('sum') #each_day.to_csv('each_day.csv')
# calculating number of commits # calculating number of authors # printing out the results
# Split the data into training and test sets (30% held out for testing)
# Find the row information for the repeat user_id
#There are no rows with missing values
# dataframe with rows where control is not aligned with old_page # treatment with rows where treatment is not aligned with new_page #sum up these number of rows 
# Which tweets received zero likes?
# get tfidf feature vector for the new document
## 25
# Lists of dictionaries can be made into a DataFrame.
#preview
# Two sample Hypothesis test. We typically use a 't-test' when our number of samples is low (< 30). The t-test assumes a wider  #distribution (more spread). We typically use a 'z-test' when we have >= 30 samples. The 'z-test' assumes a narrower distribution #(less spread).----`proportions_ztest(count, nobs, value=None, alternative='two-sided', prop_var=False)`
# get summary statistics
#The amount of converted users is found
### create time series data frames
#Kansas City': '9a974dfc8efb32a0'
# drop duplicates across all columns
# Finding the number of unique users
# add intercept column # add dummy variables
#save the tweet_id's of the above result
### Step 16: Reproduce the scatter plot with divider of labels ## Provides clear separation of clusters
# import new package 
# Which were the 3 most popular trainings? (Trainings with the most learners)
# remove subjects with no simband times recorded
#creating the binomial distribution for the New page
# convert an h2oframe to pandas frame (make sure H2OFrame fits in memory!)
# This will print the most similar words present in the model
# some numerical values do not show as such, clean missing records
# instead of going through output piecemeal, let's look at a summary: # print a summary of the fitted model
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#read the 2 df
# get unique user ids
# RETRIVE TWIT IN EXTENDED MODE (TO-BE)
#appleNegs.
# Use Inspector to print the column names and types
#df[['interest_level','interest_level_codes']].head(2) #print(df['interest_level_codes'].hist()) #df.to_csv('generated-data-with-codes.csv',sep=',', encoding='utf-8')
# Gives us general information about the porfolio
#split train and test data, based on rounds data, train on everything before 2015-08-01
# Codes can be combined with numbers to specify other frequencies. # For a frequency of 2 hours 30 mins,
# show a sample of the sub gene dataframe
# initiate geoip client
# Percent of true gains over all samples: compare to precision -- precision should be signficantly higher if this is a useful # prediction # (cnf_matrix[1,0] + cnf_matrix[1,1]) / cnf_matrix.sum() 
#Compute the probablity of new page converted rate
#nltk.download('tagsets')
# Counting the no. commits per year # Listing the first rows
# Feature Types
# only control group lined up with old_page and treatment group lined up with new_page  
#since I changed the range of the lat lngs returned from the lambda function, what I want is all the data after ~ 4th Sept 2016
# Setup Tweepy API Authentication
#find unique countries
#del festivals['Name']
# We extract the mean of lengths:
# logging.getLogger().setLevel(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
# you can see that more drivers leave rider_rating as opposed to riders leaving driver_rating
# Raw table showing all the tasks completed each week this year # gDate_vProject
# problem 6 solved.
#Remove columns which are not necessary to answer the question I would like to answer
#Group by the status values, and then describe, and then TRANSPOSE for neater presentation
# so what just happened # well m calls # same as looking up m.__repr__ and the calling
#Calculating the mean conversion rate 
# export
#Writing Pickle file 
# Read the data
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser'
# Let's see the correlation matrix 
#nyc open data API, you need apptoken to run this cell
#Rob #Initial Master Merging Merging by IDs between cal and "features" df
# Check the drop worked
# We can use iloc indexer to access data, maintaining dataframe index and column lavels
#Review data in dataframe.
# Show the node_types file. Note the common column is node_type_id
#This shows how many entries are null and what should we do with them
# read newly created dataset into another dataframe
#proportions_ztest returns the z_score, p_value
#converting eng columns to timestamp format
# Make one big dataframe
# features_to_use.remove()
#Getting the row information about the row which contains duplicate user_id
# Step 16: We generated anomaly scores for 10 records. let's look at them.
# Append distance to an empty list # Iterate thru get_distance to return the distance between coordinates
# split into training and test
#Sort by Product #Finds the column in dataframe, then searches each row for value specified.
#check for the column names and their datatypes for a particular table 'station'
# Check if there is a repeated entry
# post simulation results of Jarvis back to HS
#Let's pick one date and show the first rows #These are cleaned up data and therefore need no further cleaning (I had to do some cleaning of strings, fill NaN, #and eliminate unnecessary columns)
# interesting, "-H" is considered an adjective
# copy the datasets for cleaning
# Define URL and get request from Reddit
#Read in data from source 
# Define endpoint information.# Define endpoint information.
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#checking most recent record date using the -1 indice
# after determining the number of odds changes, we may want to determine when they generally occur # columns were added to the dataframe to help with this determination # adding 42,369 days to each date was necessary, as the original .csv file did not contain the year
# Numero de filas y columnas en el dataset
# How's it looking?
# find number of tweets without an image
# variables = arr[0].keys()
# Latest Date
#Variables to populate the metadata in the SB Item #Retrieve Acquisition Date
#showing the summary of the model
q8a_answer = r"""$ """$ display(Markdown(q8a_answer))
# your code here # every 8th business end of month
#Probability of converting given the group is treatment 
# Number of unique rows
# How many usres in df_users
# Save dataframe to csv 
# Convert pandas table into html
#tree_clf.fit(X,y)
#Autoscaling can increase the number of instances.
#cisuabn14
# Import dataset # Show the 6 first rows
# read excel file
# A:
# this feature does not seem to make sense, perhaps 'date_created' is the date the ad record was created - # not when it was literally created -- therefore going to to drop this created column
#We can isolate a product by his ID:
# Location outside US, ignore
# Now let's clean up and change the index column to just be the actual data column, instead:
# Extract mean of length of tweets
#REGISTRATION YEAR OF THE CAR
# Find the div that will allow you to retrieve mars weather # Print the results to verify it is correct
#create date variables for title
# Plot the scoring history to make sure you're not overfitting
#### SOMETHING IS WRONG WITH ME OR THE QUIZ #### the quiz says its more thank 3k's
# View the head 
# Read the records from test data
#model_w.summary() # For non-categorical X.
#Change null values to the median, of values > 0, because T, I think, means True.  #Therefore we want to find the median amount of precipitation on days when it rained.
# show summary statistics for number of claims
# Create a minimum and maximum processor object
#type BI et type UT are the same
# (d.year, d.month, d.day) >= (search_date.year,search_date.month,search_date.day)
# test data predictions # make predictions on validation data
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
#compared the syntax with p4merge - the data is correct #only the solution code is missing: #gender, email & visit date
# Nodes files
# Create a pandas dataframe as follows: # Display the first 10 elements of the dataframe:
## Make csv file
# format datetime field which comes in as string
# model.wv.syn0 consists of a feature vector for each work # with a min word count of 30, a vocab of 6,793 words as created # shape of wv.syn0 should be 6793, 200
#Line plot of log_AAPL
#export and load the df_sample csv
#verify if number of rows have been drop by one
# Find the max and min of the 'Total Number of Rides' to plot on x axis
# Interpolate from the current state to the forecasts
# total occurances of the old landing page
#Great. Let's just verify that there are only "complete" permits in this data frame.
# using the same logic from e.
# 7. 
#print head of capa
# Creating Coming Next Reason CSV
# Create a scatter plot of your predicted values vs. their true values # Describe anything you observe ## Appears to be a relatively straight line with slope of 1, indicating a solid prediction most of the time.
# label encoding block by their any_spot rankings
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# now, load it into pandas
##Read in again, now specify the data column as being the index of the resulting DataFrame
#run model
# Save the dataframe to CSV
# create dataFrame from ndarray
# summer : 5,6,7
#Optimal model found in the Gridsearch
#looks great
#number of reviews by product 
#Get index values of df1
#Converting Data Into Numpy
#data on individual borrowers
#this shows the level at which useful data is containted #the meta stuff in this case is pretty useless - the real info is in the history.
# Extracting NYT news sentiment from the dataframe
## Import Raw TTN Data from Google SpreadSheet
# can give a Series a single data value,  # and it will be repeated # to be as long as the index
# Check the column's data types.
#Change the x axis to get better results
# Add a new column to data file #Make sure to modify addFarm function on adding new column
# confirming work
# On price there seems to be a lot of 0's and the max is obviously not reliable
#Check to see the data type of the dataframe
# Inspect data types 
# Find z-score and p-value
#cascading operations
# liquor = liquor.dropna() # liquor.isnull().sum() # remove spaces
# Find number of rows with 0 in M_pay_3d column.  # We may put change other NaN values to zero if this number is low.
#Suburban
# Display a row that broke the assumptions
# put zero for null values 
# The case of "1) variable (time, hru or gru)" #fig = result.get_figure() #fig.savefig('/media/sf_pysumma/pptrate.png')
#Peaks are on Friday and Saturday
# probability of an individual converting regardless of the page they receive
#alphas = np.linspace(.001,100,5000)
# look at the count of values for the 3 categories.
# Apply the fix_timestamp function to each timestamp row in the gdax_trans DF
# The correlations are the following
# adding prefix BAND_ to column names
#Confirming if the number of rows are removed 
#'Tucson': '013379ee5729a5e6'
# When convinced Raw and Processed data is good, upload to bucket. # Upload raw heat pump spreadsheet to the bucket.
# Find the link to more details about the Avengers movie and click it
# loading iris file
# checking result from above code 
# import seaborn as sns
# y_pred = knn_reg.predict(x_test)
#creating two dataframes for each possibility to count the times  #the two values are not lining up
# Table for Bar graph - August q3a1="""$ select Ymonth,count(uid) as user_cnt,printf("%.2f",count(uid)*100/14116.0) as per_usr_cnt from res3a group by 1"""$
# only created events
#Create a barplot for sentiment analysis and approval
# top chatter over time
#timeStart = datetime.strptime(timeStart, "2015-06-09 00:00:00")
#copy the 3 datasets
#To check if there are any duplicate values in the user_id column
# group by SN and find average of each group
# Inspect the hierarchy of the json dictionary 
# Checking if quandl is working
# Row Sum of the original jArr
#Reassign name to remove the unrecognized symbol '?';
#probability of individual received the new page
# If the condition is True then the row will be selected
# Train the model using the training sets
# We change the column label bikes to hats # we display the modified DataFrame
## remove stopwords from plot summaries
# print(rdc.feature_importances_[113:1000])
# set directory/folder for file to  be loaded into 
# read from Kenneth French fama global factors data set
#Strange that there are dates in the dataset beyong the election day.  #Probably because of other non presidential races #This will need to be filtered.
##Changing column types
# find the relative image url
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
#count number of pages of data I have
### Create the necessary dummy variables
#autodf.powerPS.hist(bins=100)
# test to see if this is right
# Count number of stations in Measurement table
# Fit a logistic regression model with 'converted' as the response variable and 'ab_page' as the explanatory variable.
#specifying the huerestic and the maxlen yourself makes this run faster the utility doesn't have to calculate these for you.
#print(new_page_converted)
# this is an inner join by default
# Use the session to query Demographics table and display the first 5 locations ### BEGIN SOLUTION ### END SOLUTION
# Check if there are any 'Avenue' terms in street.
#Count the number of NaN in the dataframe
# make sure these numbers match DataFrame below for '2016-08-07'
#Find if data has missing values? #Find missing values by each column
# Save the query results as a Pandas DataFrame and set the index to the date column
#unique user ids in df2
# On which news the comment-count differences maximized?
# Has other attributes familiar from NumPy
#compare to old version
# terminate the spark session
# Print all of the classes mapped to the Base #THIS STEP ISN'T WORKING
# I could drop all the prior day info, because I saw no autocorrelation, but it just seems... odd.
# we create a new table grouped by city type and driver count and count the number od rides in the city
# to get the last 12 months of data, last date - 365
# We will join on the email so we need to make sure the email address column is formatted in the same way
#create clean job title data #special character removal, stemming and stop word removal #seperate new stemmed words with space only
# plot null distribution # plot line for observed statistic
# Identifying the top 10 authors # top_10_authors = git_log.groupby('author').count().apply(lambda x: x.sort_values(ascending=False)).head(10) # # Listing contents of 'top_10_authors'
# Checked if all of the correct rows were removed - this should be 0
# https://stackoverflow.com/questions/4527942/comparing-two-dictionaries-in-python
# Get the y values from DataFram # y.head()
# Plot the results as a histogram
# Numpy broadcasting to perform cartesian product of nxn euclidean distances # https://stackoverflow.com/a/37903795/230286 # (n x 1 x d) - (n x d) -> (n x n x d)
# path will need to be changed pending on where the repo is cloned to
#total count of individuals
#checking group treatment and control by country
# read in the iris data # create X (features) and y (response)
# actual differences
# Average daily trading volume
#create the logistic regression model
# Write your answer here 
# remove selected workspace #ekos.delete_workspace(user_id = user_id, unique_id = workspace_uuid, permanently=True)
#Show last row
#Create the pandas dataframe using the ID array to merge the other features into
# Lets look at the mean number of briths by the day of the year # Lets group by month and day seperately
# Simply save the data where 'retweeted_status_id' is null
# check if any values are present
# Remove 1 of 2 rows where user_id: 773192 and check to see it occurred
#saving it in dataframe
#create dummy variables for the categories
# print percentages
# Query the last 12 months of temperature observation data for this station and plot the results as a histogram
#r.status_code # print the first 500 characters of the HTML
#boxplot for renta feature
# Set the index to the date column
# Convert the above two-tailed test p-value to a one-test p-value
#pickling the results for flask integration
# Requirement #2: Add your code here # updated by Huang, Silin ID: A20383068
#== By Label: row range by data index, column range by column names 
# check x distributions X train # distribution plot of temp_celcius
# Take a look at rows with lowercase values in 'name' field 
#Create new dataframes in preparation of feature engineering
# Search all the data from twitter related to given tag
#It looks like there must be duplicate rows since there are ~85k unique permits out of 97k permits in the dataframe. Let's first find out how many entire rows are duplicates of one another. 
### Create the necessary dummy variables
# df['Date'].str[-4:].astype(int)
# Number of Stations
# Count terms only once, equivalent to Document Frequency
#dropping outliers
#Convert rate under the null is to be calculated as same as the average conversion rate for the page.
#merge jobs dataframes
#compute actual coverted rete for new page observed in ab_data.cs #compute actual coverted rete for old page observed in ab_data.cs #compute the difference
# lets add a decade column and look at males and female births as a function of it
# Tweet image predictions, what breed of dog (or other object, animal, etc.) is present in each tweet according to neural network
# fit the Logist model
#Jersey City': 'b046074b1030a44d'
# The number of rows in ins # The number of unique IDs in ins. # What does this tell you?
# save the model
# Look at #535, to see if the cleaner function got wrong stuff out
# Results
# is our date column datetime?
#reset index so that we do not have issues since we loaded multiple files 
# This results in a mutli-index set # lets turn them into a date using a dummy year # handle the leap year!
# customers
### Validate Pickle ###
# get the count of unique locations
# Check SFL stuff
# Use the "short_id' field as the index # Show the first few rows
# Check whether columns have been created in dataframe
### getting the odds by exponentiating coef
#create a linear regression model
#Reading image predictions into Panda's dataframe 
## Uncomment and complete #trump = trump[trump['source'][:7] == 'Twitter']
# Keep relevant columns only (23 columns of 49)
# replace the indexes of the dataframe with the correct header columns by renaming them
## Feature importances
# drop scratched
# look for '&amp;'
# What are the most active stations? # List the stations and the counts in descending order.
# Let calculate Accuracy Model #  Note: In binary and multiclass classification, this function is equal to the jaccard_similarity_score function.
# cisuabe5
# weird but possible
# iris file #Remove the header line
#This code block should indicate that there are no more missing values, except in the 'renta' feature
# Probability of receiving new page
# 5. 
# count the frequency of the types of sub-gene elements
# First, get a list of all of the hemispheres
# Perform a basic search query for '#data' -- grabs 15 entries by default
# check Forcing list data in file manager file
#based on output nr_of_pictures can be dropped #checking for nans
# install_rate is higher for smaller districts # variance in install_rate is slightly lower for smaller districts
# Standard Deviation, variance, SEM and quantiles can also be calculate
#Describe the set of all sessions where a printer was used #Important features are mean, 50%, and 75%
# building permit datasets
# Import second Funded Apps file
# using merge function make one final dataframe
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
#join in the weather for these days! then look at complaint types
#Check duplicated rows by contrator_id
# Drop records where target feature is null
# Import libs 
# total number of rows with missing values
#load most up to date data
# Merge the portfolio dataframe with the adj close dataframe; they are being joined by their indexes.
# Put the data into HDFS - adjust path or filename as needed
#qry = DBSession.query(User).filter( #        and_(User.birthday <= '1988-01-17', User.birthday >= '1985-01-17'))
# odd that they had ads after election day (runoff elections e.g., Louisiana) -- will filter out
# Train data is the dataframe that is continously modified while train dataframe is static
# Merge libraries_df with metadata
# What are the most active stations?
#our critical value at 95% confidence is
#df['money_csum_cent'] = 100*df.money_csum/df.groupby(['program_id'])['money_collected'].sum()
#Create Logit regression model for conveted variable and  ab_page, and us control as baseline
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Count the number of NaNs in each column
#Per the question 'proportion of the p_diffs are greater than the actual difference observed in ab_data.csv' ,then: # ie (1-0.090999999999999998)
#Prepare to plot using timestamp data
# convert the date column into something matplotlib can read and then plot it. 
#joined df_groups and df_events together after computing the number of events created over the past 7 sevens by groups
# create a list of lists for the x values that will be used for the OLS function.
# Alternatively: #pt_weekly['WoW'] = pt_weekly['num_listings'] - pt_weekly['num_listings'].shift(1)
# First, create new columns with the year of each date columns
# Initialize empty dataframes
#print summary
# joined_hist.info()
# Colors # Define x-axis and xlabels
# Different way to plot
# Use start_date and end_date parameters to retrun data for 2017 only 
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
## Request page ## See the html for the whole page # https://www.purdueexponent.org/
# Extract Projection Information
# excelDF =excelDF.drop("Calc",axis=1)
# Training the model 
# Read in the dataset and take a look at the top few rows here:
# Set up Twitter workspace
#get station count, has been checked with measurement station count#get sta 
### Fit Your Linear Model And Obtain the Results
#Convert titles to a list for analysis
#check if the columns are deleted
#The difference in the simulated values is found
# value of the accumulator - the number of even numbers in data
# note: although the strs and the converter # have format m-d-y the output has format y-m-d-h-min
# leads are done with negative integers
#pivot table showing the years of married and affair status
# Create colors for the clusters.
# Sort the dataframe by date
# data science retweets
# copy Snapshot data
# Check for unique values in each non-id series
# Calculate gameless pregame ridership.
'''$ Data Cleaning$ '''$
# Split into training and test data: 66% and 33%
#simulate n_new transactions with probability p_new using random.choice function
# Logistic Model with the response variable 'ab_page' and the countries as the explanatory variables
#check
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram #df_temp_by_USC00519281 = engine.execute("select count(*) from measurement where station = 'USC00519281' AND date > '2016-08-22';").fetchall()
# 68 original columns # 10 are empty # 
### Fit Your Linear Model And Obtain the Results
#Let's display the text of each tweet we found. #[tweet.text for tweet in tweet_list]
# Formatting the dataframe in decending order
###YOUR CODE HERE###
# Write your answer here #Idea is to hierarchically look for b2 starting with root index, before diving in data
# view df info
print("""Dataset shape: {}$         \nDatasets type: {}""".format(df.shape,$
# get numerical features
# Set the X and y
#Verify whether the latest account was created before the startDate
# for this we group by column 'group' # then we compute the statistics using describe function # as conversions are assigned boolean values, we can use mean to find probability of conversion
#Do the same for male to get a male only dataset #split the dataset by gender
# create the combined data frame # little bit of clean up
# Join the datasets
# First get the data # quick check to make sure no object dtypes in dataframe
# Fill nan with zeros
# The climate data for Hawaii is provided through two CSV files. Start by using Python  # and Pandas to inspect the content of these files and clean the data.
# so far, I dropped 15 rows 
# compute actual difference from original dataset ab_data.csv #p_diff
# Change directory # Create a list with all the files
# adding contractions...
# Check every in_response_to_tweet_id only contains one float value 
# Gridsearch incorporates k-folds validation # You do not have to create training/testing splits
## Create a logit model based on countries taking country_CA as the baseline and abs_page
# Fit your model using all of your training data
# loan_stats["revol_util"].describe()
#P-New
# create range of indexes (by default dayly timestamps):
#create BeautifulSoup object and parse with html
# shopify_data_merge_msrp.apply(lambda x: x['Discount Amount']/x['MSRP'])
#pnew conversion rate:
# Find all the tweets mentioning "SANTOS"
# Reflect Database into ORM class
# Use len() and unique() to calculate unique user_ids in df2
# Use the session to query measurments table and display the first 5 locations
# Device
# find the average attendance for the 2015 MIL season to impute the missing values
#If you input a dictionary, it will parse the key as the column header
# Stars counts
# Display first 5 rows from station table
# Take the raw data and filter out a single person just to check if the data looks reasonable.
# so roughly 25% of data should fall outside either quartile - note rougly, since we have categorical data here # below lower quartile
# mean_encoding_test(val, train,'DOW',"any_spot" )  # tbd #mean_encoding_test(val, train,'hour',"any_spot" ) # tbd # mean_encoding_test(val, train,'Block',"Real.Spots" ) # tbd
# Double Check all of the correct rows were removed - this should be 0 - was preset but does not work
#The return here is the training set of features, testing set of features, #training set of labels, and testing set of labels
### Create the necessary dummy variables, assigning the proper labels.
# examine the types of the columns in the DataFrame
#stop_words = set(stopwords.words('english')) #print(nltk.FreqDist(filtered_tokens).most_common(200)) #return filtered_tokens
# Investigate what type of religion are the places for worship??
# Define the columns and set the index.  
#create the x,y training and testing variables from the dataframes. The size of the training set will be 80 percent #the size of the testing set will be 20 percent the variables
#1 #data= quandl.get("FSE/AFX_X",start_date="2017-01-01",end_date="2017-12-31") #data
#Converting values to float
# Combine columns into one stage column
# drop the unnecessary dummy column
# Reviewing the summary data of the query returned
# Read the data from the vtc-cab repos
# 2. Convert the returned JSON object into a Python dictionary
# Find the number of unique values
#checking when treatment team lands incorrectly on the old page
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# {0: -7.4305555555555545, 1: -15.097222222222221, 2: -7.263888888888888, 3: -5.097222222222222, 4: 3.402777777777778, 5: 8.069444444444445, 6: 16.569444444444446, 7: 9.736111111111112, 8: -0.7638888888888887, 9: 1.902777777777778, 10: -3.263888888888889, 11: -0.7638888888888887}
# Extracting CNN news sentiment from the dataframe
# there are no rows with missing values
#current data frame
# Convert'created_at' time data rounding to nearest minute 
#user.iloc[1]
# pnew and real pold not under null # different not under null
# Conert the numeric columns to integer
# wide to long # view head of final row-wise dataset
# analyze validtation between BallBerry simulation and observation data.
#creating intercept column #creating dummy variable using ab_page insted of treatment column name.
#final.head()
#sales_agg.groupby('Store Number').max()
#  Sanity check on array shape, expect: (16042,) #  from which we can see the population size:
# Initializing the query structure: transform corpus to LSI space and index it
# Total number of data recorded
# Here are the topics that Gensim found ** BASED ON THE TRAIN DATA**
# Since our alternative hypothesis is p_new > p_old, the alternative parameter must be 'larger'
# Number of votes
# Create the design matrix and target vector
# view the values of the new dog_name column
#Count the number of lines where new_page and control are aligned, also old page and treatment, and add them up
# Print the info of df
# show summary statistics for number of claims in utility patent
# Show the head of the dataset
#now drop redundant timestamp column # inspect the dataframe
# create df
# info
# Removing outliers
# 0.9999999383005862 # Tells us how significant our z-score is
#autos.dropna(axis=0,inplace=True)
# Create columns with dummy variables in dataframe # drop on column (here: UK) since the predictor must be full rank
# Run the normalizer on the dataframe
# create list stores deep cleaned text
# Running and Trainign LDA_3 model on the document term matrix.
# 7. Print the mean for all columns for each age_cat using groupby.
#join the dataframes together from drugs we used chembl_id vs name 
# common_words
#Importing the cleaned file into new data frame (df2)
#we are using a sample size equal to the one of new_page in df2
#get 2015 and 2016 and predict 2016
# treatment group in UK # treatment group in Canada
# get now, and now localized to UTC
# get the general model # predict with the model
#We parse the date to have a uniform 
# Setting up plotting in Jupyter notebooks # plot the data
# We get the trend for closed prs by month
# Group data by death year and cause of death
#proportion of p_diffs greater than actual difference diff
#Garland': '7c01d867b8e8c494'
# Convert to Spark Dataframe # Create a Spark DataFrame from Pandas
#Divide the dataset into input variables and output  # (I get a memory error if I work with the whole set, so I'm reducing it to 1.000.000 rows)
#Insructor Note: Walk through creating the dummy variable.  # Two possible solutions: #OR
# To validate charmander #post_discover_sales[~(post_discover_sales['Email'].isin(pre_discover_sales['Email']))]
# To visualize avg duration and no. of trips from each station
# step 3: Instantiate the model # step 4: fit the model
# We load fake Company data in a DataFrame
# Exchange DB # EXCHANGE_DB.to_csv('exchanges_list.csv')
# Solution 2 : numpy diff function
# testing if tweet_id format changed
# Looking at one tweet object, which has type Status: # example_tweets[0] # ...to get a more easily-readable view.
# Display number of unique dates for each column
### create a new dataframe that contains the entries for the treatment group only ### the probability of conversion for the treatment group
# Just do demonstrate one of the useful DataFrame methods, # this is how you can count the number of sources in each class:
#% of customers who contribute to 80% of revenue
# Remove stop words from 'words'
# Most retweeted
'''$ DO NOT CHANGE ANYTHING IN THIS CELL$ '''$
#print_sentiment_scores(twitter_datadf['text'][1]) #analyser.polarity_scores(twitter_datadf['text'][1])['neu']
# Show Position Information
# Find z-score and p-value
# Simulate conversion rates
# simulate n_new transactions of the p_new  # proportion of the n_new transactions of the p_new 
# Calculate the simulated transactions' conversion rates difference
# Dummy subreddit values 
# significant of z-score # for our single-sides test, assumed at 95% confidence level, we calculate: 
# there is a 0.118808 probability they converted if they were in the treatment group
#'New Orleans': 'dd3b100831dd1763'
# Show metadata for database file
#finds date of most recent and least recent tweet
#9. Get the column array using a variable
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
#What about location since timezone was surprising?
# pd.to_datetime(df_train["date_account_created"]) # pd.to_datetime(df_train["timestamp_first_active"])
# we will sort the information in the last array
# check shape
# Dataframe of dates (contains 21 values)
# Display first 10 elements from dataframe:
# Check to see if the above worked.
#Getting the count of NaNs in location column #isnull() gives boollean values and sum() counts the True values
#Let's remove all rows containing null values. 
# My gamelogs include 3/29/2000 - 10/02/2016, so I'm going to delete those rows.
#loading the data frame df 
# its reserved for columns
#checking result from above code
#split the data as described above #Prepare predictors and response columns
#Drop rows with all NAN
# Checking if there are any values of Fare_amount that equals to 0
# Get the weekday 
### Use the binomial function to perform simulation
# The result of loading, paring down, and renaming columns should # look something like this:
# Annotation file for the CLIXO terms
# Get a list of column names and types
# Choose 'seller', 'price', 'yearOfRegistration', 'gearbox', 'powerPS', 'kilometer' and 'notRepairedDamage' for analysis
# df_2016
# Tells us how significant our z-score is and what it would have to exceed to reject the null:
#fatalities
# check option and selected method of (11) choice of groundwater parameterization in Decision file
# Calculate the proportion
# put response data into the data dictionary
### Check country of project location (as opposed to creator location) distribution
# comments = pd.read_csv('seekingalpha_top_comments.csv')
# TASK F ANSWER CHECK
#Group data by year and compute the total flow and count of records #Remove records with fewer than 350 records #Rename the 'sum' column
# A:
#Add dataframe values to csv
# check number of lines
#### Add a new column to combine RFM score: 111 is the highest score 
# Create new dataframe that only contains rows with tipPC < 200
#create time series for data. Not usefule so far:
# proportion of differences
#\xa0 is  non-breaking space in Latin1 (ISO 8859-1). replace with a space
#create dummy variables with US as baseline
#     print('Deleting the {} table.'.format(table_name)) #     connection.delete_table(table_name)
# Assigning table classes
# order dataframe by launchpad_issue
# Copy the data of interest - with headers - into clipboard with Ctrl+C # Run this... ## Copy the Excel table to the clipboard first.
# check the confusion matrix # it seems that many rates are predicted as 5 point, which generates the most of errors.
# The resulting object from value_counts() is sorted in descending order  # so that the first element is the most frequently-occurring element.  # The repeated user_id is 773192.
# Strip local time to include date only #itemTable["Date"] = itemTable["Date"].map(parse_full_date) # Categorize energy labels
# We need 'Date' as a column for a bit
# Set up logistic regression # Calculate results
#Number of rows in training set  
q8d_answer = r"""$ """$ display(Markdown(q8d_answer))
# Import rebuild of Site Visit file
# Filter to days store 1 was open
# Let's create a new "dog_type" variable for the four columns
#new dataframe with only tweets from users who tweeted before
# Merge the station data with census income group data
#n old is equal to the total number in csv
# Show plot
# Double Check all of the correct rows were removed - this should be 0
#Get last year of precipitation data
#Calculates actual difference between conversion rates #Calculates p_value
# Stats
# Simulate n_old size transactions with a convert rate of p_old  under the null # Calculate simulated transactions conversion rate
# Or: sales_df['Country'].value_counts()
# check if more than one user using same device
#mydf2.datetime = pd.to_datetime(mydf2.datetime)
# decompose 
##print(r.json())
# Links anzeigen
# shuffling the df to avoid time dependencies for now
# not only is Pandas much nicer, it also executes queries!
# reduce the dimension of the feature for the test data
# all numbers here - Summary stats on the census data section
# Import, convert 'Date' col to datetime
#mostRecentTweets.reset_index(drop=False, inplace=True)
# drop rows for mismatched treatment groups # drop rows for mismatched control groups
# Perform a second groupby command on the 'data_FCInspevnt_latest' table # Filter Inspection_duplicates for those with an a sum of the Inspection Number greater than 1, i.e. a duplicate entry
#Calculating the control group mean by filtering data using query.
# Filter rows by considering "name" and maximum "capacity # Apply general template of columns
#I don't need this #type(rptdt)         pandas.core.series.Series
#Assign each date to a different group
# Check the dataset a second time
# calculate the AUC on the new data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Check which values to extract
# Ben helped me with this lovely for loop...
### For cleaner results, drop non-US-located projects
#Vemos cuantas filas y columnas tenemos
#This is the syntax to be used to read in a SQL file
# Attach tokens to the dataframe 
# Copy data frame and drop rows containing null values
#Accuracy on test data
# Display a list of buckets.
# again, default is to generate an error
#The logit model is fitted to the data
# Create dataframe to every movie with the genre 'Western'
#calculate n-new and n-old
# simulate the conversion treatment for old page  # with sample size equal to the old page user in dataset  # and with probability of convert rate under the null
#Taking Canada as our base variable  # Fit
# Use table manager... throws an error #tables = table_manager.all()
# 2. Convert the returned JSON object into a Python dictionary.
# pull out isPorn values for training data (first 2000 youtube title values and first porn title values)
##Design a query to retrieve the last 12 months of temperature observation data (tobs). #Filter by the station with the highest number of observations.
# Parse the resulting html with soup
# Find unique users
# concat grid id and temp
# printing the most likely IOB tags for each POS tag # extract the list of pos tags # for each tag, assign the most likely IOB label
#Load Data
# Define a dictionary with the filename, path, and dbase type. `echo` suppresses output.
# Saving model to use in future legislation.
#gract.to_csv('gract.csv')
# find the difference of a 2d array and one of it's rows
# Create engine using the `demographics.sqlite` database file
#computing the f1 score
# merge onto weather features
# add response variable
#These orders do not exist in shopify
# Creating dummy variables for country column
# Write team schedules.
#using to_csv to write master dataframe to CSV format
# convert ISO string to datetime
# This shows that we guessed 0 for each one of our rows.
### Sample of clean descriptions
#builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# Make a list of all Terms per season
# Creating a new intercept column # Creating a new ab_page column
#Grouped by data in a dataframe groupby
# it can be useful to pass a dictionary mapping column names to operations applied
# people usually complain about potholes on or after 02 PM and before 03 PM
# open the model
# Let's identify the 10 most common complaints:
# ...and it does not matter whether indexing is position-based  # (iloc) or label-based (loc, see below for a more detailed expo)
# Dataframe of all events created by a user within a 30 day period
# Replace Default with Charged Off so there are only 2 factors
#Check the dimensions of the dataframe: it's BIG with close to 100k records!
# Decode the JSON data into a dictionary: json_data # Print each key-value pair in json_data
#Convert rate for  pnew  under the null
# Read countries.csv file into a dataframe
# plot fixation durations
# midToto is 13, therefore user can change each layer value from 0 to 12
# Check out the structure of the resulting DataFrame
# Show GMT to EDT
# drop one of duplicated id
#Great. Again, let's do a sanity check one more time. Let's show all rows with missing data:
# Overview of unique values in these categorical columns, we clean these strings  # before we make them dummies, to avoid that M1B and M1b e.g. become two different # columns
#Click here and press Shift+Enter
# price divided by (bedrooms + bathrooms)
# import utils.py to download TestCases from HS, unzip and installation
# Creating a lamba function to read each row and find the comma...then split the field
#Show rows 100 thru 105
#see that yearOfRegistration and ageCol
# Probability of conversion for new page
#do the non RI credit charges contain all the instance types, can we get the prices from here? compare above^ #yes, so until we have a good way of pulling the billing data, we'll just pull if from this TTE billing
# Utilities
# Join Dataframes
# Fornecido no enunciado: # "Assume under the null hypothesis,  pnew and  pold both have "true" success rates # equal to the converted success rate regardless of page - that is  pnew and  pold are equal"
#create a date (set date for each month as 1st of the month)
# Import CSV file as a pandas df (Data Frame) # Take a look at the first entries
# default value of workers=3 (tutorial says 1...)
# Use regulal expressions to extract contents from source and replace it # Assign appripriate category # Check the outcome
#Calculate the mean of 'converted' to get the probability of an individual converting 
#Again we are going to grab only what we want out of the CSV and check it out
# get the final train dataset
# Convert datatype of created_at column for better manipulation
# convert crfa_a to numeric value
# Read the data file and save it to a dataframe called "df".
# add a notebook to the resource of summa output # check the resource id on HS that created.
# count the gene names
#df[(df['group'] == 'control') != (df['landing_page'] == 'old_page')].shape[0]
# favorite table # for row in table_rows: #     print(row.text)
#m1.diagnostics()
# Urban cities ride totals
#run a query to count the number of dates when the corresponding rank was below 10
#Saving the new dataset
# Dictionary with brand name keys, values are corresponding proportion
# Create RSV only dataframe
# Verify the columns and values were created as expected
# To find convert rate for p_new
#CALCULATES THE MEAN OF CONVERTED VALUE = PROPORTION
# show topics
#13. Which transactions have a UserID not in users?
#  At this point, the user could specify their mean and sigma, #  but we shall use actual values computed from 51-year history:
# We extract the mean of lenghts:
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# Check if western movies are available in every decade and count their number
# 3.2.C OUTPUT/ANSWER
# Median calculated on a Series
# WIth replacement
## Filled in missing hours with the data available for previous hours
# create a Pandas datetime for today # use Pandas date string indexing to retrieve all rows for this today's date
cur.execute("""DELETE FROM coindesk;""")$
# Gets rid of 'u/' so that it is easier to use the api
# Read csv filenames 
# We can use the same objects on test now that they have been fitted on train
################################ # create law file and fix law account number
# Design a query to calculate the total number of stations.
## creat a new DF with 2 columns - sum of score and sum of comments with post id as index
# Percentage Change Column is created
# read in the file from matlab # read in the file from our edftocsv
# Just in case: close the reading of the LSST catalog file.
# Store the best model
#Peaks are from sunday to wednesday
#prints a column
# write to db
# How many stations are available in this dataset?
# feature transformation
#Reading from the saved file
# Backward filling
# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')
#Display the last 5 rows in the contractor
# For some odd reason, we can't just pass `list` into `.agg` so we define this function: # You may or may not use it
#run the string_01 in command prompt to generate the markdown files
# Viewing dataframe length to compare with it after cleaning
# Create logit_countries object # Fit
# pandas way
# Look for correlations among input variables (not including date columns)
#opening the countries.csv file
# Call the play text the same thing as the other dataframe # Convert date to datetime
# Make sure there are no duplicate keys
#And let's try this again:
# people usually complain on or after 11 PM and before 12 PM
#Churns cohort x cohort
# 4. What was the largest change in any one day (based on High and Low price)? # Alternative solution using list comprehension.
# Extracting CBS news sentiment from the dataframe
# Save the query results as a Pandas DataFrame and set the index to the date column
# we can devise hierarchical indexes (which by itself just pulls # several columns out as indexes but does not sort or in any other  # way rearrange the rows)
#Find average HP by Brand #Add a count 1 to each record and then reduce to find totals of HP and counts
# Ratings distribution across top 10 categories grouped by neighbourhood
#https://pandas.pydata.org/pandas-docs/stable/merging.html #result = pd.merge(left, right, on=['key1', 'key2'])
#Store Definitions in WML Repository
# create a new DataFrame to contain new columns date, month, year # data integrity check:  check how many years exist in df_convs_master
#Grouping the data based on the hour value and calculating the average speed in that hour
# ssh variables
#pd.read_csv("data/Chapter03/data/msft.csv")
#checked values using (len(old_page_converted))
# df_dummies.head() # df_value.head()
# Getting the dates for date crawled
# Reset the index of airquality_pivot: airquality_pivot # Print the new index of airquality_pivot
###Make sure you have run get_weather.ipynb to pull this csv below
# Calculate the date 1 year ago from today
# print the first 500 characters of the HTML
# scrs < 0m / all scrs, since 3-01-2018
# p_old is the mean of converted users over entire dataframe
#viewing the countries data
# remaining rows with null values
# Load the 311 data into a dataframe
# How many complaints actually contain the words "injured" or "hurt"?
## Python will convert \n to os.linesep
# calculate variance of p_diffs
# Trimming whitespace # Also see rstrip, lstrip
#Now this should work
# df_goog.index.month() + relativedelta(months=+6)
#print the summary statistics for the precipitation data
# emptyDF = spark.createDataFrame(aggDF, struct_v1_0)
## Initially, think of this as similar to a pivot in excel - but more flexible
### count the frequency of each user_id and list the results in decreasing order of frequencies
#calculate the mean absolute error
# Tells us what our critical value at 95% confidence is
# 12 months before LAST record entry.
# create a clone of your dataframe df2 in case of any errors
# failed campaigns by name length (sorted by name_length)
# people usually complain about loud parties on or after 11 PM and before 12 PM
# HC01_VC36 is mean travel time for workers (census tract level) in minutes
'''Earlier we were unable to identify country for so many rows from IP address,but we can not simply ignore $ create new Country/Category for these users as UNKNOWN'''
# checking the denominator values for df1
# Merge mojo and youtube
## strip whitespace in column headers
# check Basin Parameter info data in file manager file
# creating purchase csv
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Requirement #1: Add your code here
# dev4['rank'] = dev4['avg_rank'].rank()
#final_df['mean'].values
### Join shape with data
#feature importance
# divide the number of 'new_page' values by the total number of rows in the dataframe
# how many rows, columns do we have now?
# Dependencies # URL of page to be scraped
# 0.90505831275902449 # Tells us how significant our z-score is
# To Read the dataset
### Fit Your Linear Model And Obtain the Results ## Checking results
# show the data types of the columns
#first_month
# create pyspark-dataFrame
# Predict the rating of a known B class article.
# Make columns for seasons and terms
### when you are done, call conn.close()
#equal footing
# Add season.
# Sort the dataframe by date
#@title Default title text
#proportion of p_diffs greater than act_diffs
# Sumar una unidad a una columna
## Ideal max_features was found to be 2,000, but this took too long to run
# inplace =  True - make the changes and save it back to object
# NOTE - LOST A VENDOR
# check to see if rows were removed
# create a column with the result of the analysis # display the updated dataframe with the new column
# Concatinate each Data Frames 
# example of a payload
#creating the model 
# Joins df2 with countries on 'user_id'
#dropping variables
# Fitting logistic regression model with country and ab_page
# models trained using gensim implementation of word2vec
#probability of an individual converting given that they are in the treatment group
# get mongodb params from environment
# Run Mood's test for female vs unknown
#probability that user gonna convert
#URL's of pages to be scraped
## numbers of user_ids
#checking if we have missing values
#Get rid of variables that won't match up
#probability of conversion under the null
# data source : https://coinmetrics.io/data-downloads/ # trans_data = pd.read_csv('transaction_data/btc_transaction_data.csv', index_col=[0])
# 7. (Optional) What was the median trading volume during this year.
#with open('../notebooks/subsample_idx_greedy_balanced.json', 'w') as fd: #    json.dump(list(idx_set), fd, indent=2)
# m['created_date'] = m['created_at'].dt.strftime('%Y-%m-%d')
#cats.loc[cats['SexuponOutcome'] == 'Neutered Male'].describe()
# saving model
# horizontal bar graph is the best
#festivals['Name']='' #festivals['latitude']='' #festivals['longitude']=''
# Import image-predictions.tsv to a dataframe:
# Pring filtered df for station_distance for : # start station name, end station name,  # start coordinates, end coordinates
# Summary statistics
# integridad del DF
# Store a Library and its MGXS objects in a pickled binary file "mgxs/mgxs.pkl"
#contest_cr.where(F.col('party_id')== 265607419).take(2)#.select('party_id','party_name','address1','city','county','state','postal_code','start_date').take(5)
#joined all newly created variables into one table with primary key being date:
#The min date is before any in the train data frame, therefore stations were installed before the first trips (good). #The max date is before the end of the train data frame, therefore the service has not been adding new stations recently.
# 'Reno': '4b25aded08900fd8'
# The object type in the date column is a bunch of strings 
#Match the lines with the duplicate id found above
# There are 290584 unique users
#Displaying the data that we will feed to compute the z_score and p-value using statsmodels # Computing z_score and p-value
#using the CSV file from earlier, we can create a Pandas Dataframe:
#check they have joined properly
#in descending order
# creating folder 'wrangling_project_files'
# Take the date and time fields into a single datetime column
# method to write back to a csv file, you can use to_csv method for the same
#Sharpe Ratio #Menor SD
# hierarchical index in conjunction with sorting # (note that here we should have converted the day to ordinal) # tips.set_index(["sex","day"]).sort_values(["sex", "day"]) works the same
# Save the query results as a Pandas DataFrame 
#sentiment analyzer #pip install vaderSentiment==2.5
# create monthly data #fin_r_monthly = fin_r.loc[fin_r.index.day == 10]
#Set site, parameter, and stat codes
#we will exponentiate control
#segments
#quantitative-categorical-categorical
# validate inputs to the RNN
#url for mars news
#register a data frame as table and run SQL statements against it
# hpg.to_csv("data/kaggle/out1.csv")
# Standardize the  features, because there are so many "one hot" columns, i wanted to be sure to dampen the impact # of the other numeric columns that had larger values(ie. created date, and complaint type )
# calculating the probality of an individual in the control group #converting and store it in a variable # printing the probality of conversion
# Fitting the logistic regression model using country data
#Create a content_short field for Tf-idf vectorization
#Store the final cleaned dataframe
# join information about first answer into the frame # RUN ONLY ONCE or results in duplicate rows
# Load and peek at the Yahoo data.
#gmap.scatter(hotels_lat, hotels_lng, '#BB1F5C', size=20, marker=False)
#Problem 1
#old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) #len(old_page_converted)
# create a data frame and reset the index
# Now download all the pages
# set the simulation start and finish times
# Compute p-value # h_alternative : p_new > p_old
### top 10 users with Kate Spade mentions
# replace wrong ratings with the right ones 
# Format bill ids to match the bill ids taken in by the API
# Plot of tasks completed each week by energy level # cust = itemTable.groupby(['Energy'], as_index=True) # cust.get_group('Normal-energy')
# old page and new page shall be equal in the converted rate
#'Orlando': '55b4f9e5c516e0b6'
# import data # show top rows
# And in a similar manner, we get the trend by quarter
# take the publications whose publication date is yonger than x days
# how many 0 and 1 are there in the affair column.
# IP address of your PHYSICAL MACHINE (NOT VM)
# Calculate probability of conversion for old page
# Save references to the table
#make a copy of dataframe #converting normal values to log values using numpy's log function to remove anamolies
# Convert this cell to markdown and write your answer here.
# transformation primitives
# Train model
# Create dictionary: AFX_X_2017
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# You can also check it's data type separately
### fit the logistic model for periods of time ### print the results
# What's the probability that this classification is accurate?
#Plots the incremental entries for a particular station
# Add it to original DF
#CALCULATES THE MEAN OF CONVERTED VALUE = PROPORTION
#I will drop'id' as it's redundant
# number only
#find group == control and find probability of conversion
# wise to remove misaligned data since impact is small relative to total rows ## identify first set of rows containing misaligned data
# A:
# Use Pandas to calcualte the summary statistics for the precipitation data
# Groups already grouped dataframe by parent index (level=0) AND sums it up # gDate_content_count
# getting rid of null values and then providing description to show success in that endeavor
# export DataFrame - values option (list of lists without indeices)
#Getting Number of rows by shape method
#find the tweet with highest retweet count
#Read in data from source 
#Filter out duplicate rows
#Total Number of Total Fares Per City Type
# P value
#re-load data for further practices
#Starting by looking at up votes, going to set it up by looking at the differences between #top fifteen percent and those that received 0 up votes (4888 vs. 6910 observations).  #creating 'viral' dummy
#read data in from pickle file
# new_eng
#pair plotting for checking for collinearity
#Finds the minimum value for Discount %'s within a given band.
# TASK 1
#bikedataframe = bikedataframe.drop(['DATE', 'REPORTTPYE'], axis=1)
# confirm which conda environment you are using - make sure it is one with SpaCy installed # if you have difficulty importing spacy try the following in git bash # conda install ipykernel --name Python3
# Creation of connection to our mlab instance
# total time of active projects
# create a very simple time-series with two index labels # and random values
# Anything less than 0 means that the stock close was prior to acquisition.
#Also checking that loca
# Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date
# concatenate first three rows of above dataframes
##Create a series whose index is a series of dates ##between the two specified dates, (inclusive)
#deaths_Sl3.head()
#residuals of the model;
# Import file we just created as a data frame
# just trying different slices of the data
# create some noise with higher variance and add bias.
# look for seasonal patterns using window=52 - Dr
#Total retweets, favorites
#Simulate  n_old  transactions with a convert rate of  p_old  under the null #Display old_page_converted
# Check dataframe
# Missing 1 value for Q3 so filling with 'Other'
# cisnwf6
################################################## # Convert string to datetime format ##################################################
# Read city_dta with pandas
# one hot encode the component and team columns
# of course ignore unit_sales since missing from test
# Change the plot size.
#set the intercept to 1 # produce a dummy variables. control shall be 0 and treatment shall be 1 # test if the dummy variables are there. 
#ab_page coefficient interpretation #similar value to the result obtained  #when looking at the treatment/control group individually
# drop all duplicates across ALL columns
# now creating 'split_location_tmp'
#read back in and alias the dataframe
# quick consistency check
# Initialize the timing of computation
# length of unique users 
# the logistic regression accuracy score
#joining two tables
# Testing the plot function 
## Feature importances
# check out unique brand values
# Deleting date again as it's an unnecessary column.  Explaining that new column is the Ticker Start of Year Close.
# Create time series for data:
# Split PKG/A column into to and drop
# Note A1 is a counted allele and usually minor. (A2 is usually major)
# Calculate the sample variance.
# X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)
#We can have a look at the different labels which were created in the VCF
#year of registration
# Calculate the average chart upper control limit.
#create dummy variables
#Make p1,p2,p3 all caps
# check the datatype of the column
# foreach through all tweets pulled # printing the text stored inside the tweet object
# Creating the term dictionary of our courpus, where every unique term is assigned an index. # Found the output from models is more close to what tickets mean about when not using bigram phrases
# Here's where I apply the train test split
# read movies of small dataset
# instantiating and training the Word2Vec model # getting the training loss value
# merge
#adding the intercept to my dataframe #creating dummy variables
# parse a string date and output day of week
#It's the same as the success conversion rate regardless of the page
# Importamos  matplotlib
# Total recalls  # Recall percentage # Unique recall types
#check which values appear in the country column
#Reading from the saved file
# check option and selected method of (12) choice of hydraulic conductivity profile in Decision file
# save the file to the output
# Compute Age of the passenger at time of booking
# Crear 2 nuevas columnas llamadas longitude y latitude, basados en el campo place
#DF
# .loc can deal with a boolean series
#\xa0 is  non-breaking space in Latin1 (ISO 8859-1). replace with a space
#checking the result
# At frist some standards
# clean odometer
#Save the query results as a Pandas DataFrame and set the index to the date column
#create a dataframe with a random sample of the dataframe and run a test linear regression to determine accuracy
# Sample user
#apply random forest
# have a look at the beginning of one of the files to see what they look like
# convert to undirected # get the parties of the polticians # calculate the best partition of the parties
# remove both left and right parentheses
# Longuest sequence
# setting the rating_10_scale column to float
# Read the data file and save it to a dataframe called "df".
# read in a (large) convolutional neural network model # this will only work after the CNN model is downloaded (~800MB) # e.g. python -m spacy download en_core_web_lg
# Read the JSON file into a list of dictionaries
#  number of times the new_page and treatment don't line up is sum of above two values
# Fill NaNs with zeros
# We extract the mean of lenghts:
# Replies of Trump
# use dropna to eliminate rows or columns with NaN values # We drop any rows with NaN values
## adds release day of the week as int
# for this we group by column 'group' # then we compute the statistics using describe function # as conversions are assigned boolean values, we can use mean to find probability of conversion
# get indices for rows where the L2 headers are present # these will indicate the beginning of data 
# calculate the average RMSE
#Last 5 Rows
#create dummy variables
# convert crfa_f to numeric value
#. You can also get the result form the above cell. I did this method too.
# Pandas DataFrame with the Sentiment Analysis results
# Convert the url to df
# How many tickets in each topic
#content_analysis_save = 'wikipedia_content_analysis.html'
# Make columns with country sums
# beta_pop.tag.test_value.reshape((-1,1)).T.dot(X[:,8:9].T)
# let remove duplicates #checking the presence of Mary again #lets shuffle the data set
# Getting basic general info from the three data sets.  # Assessing  tweet image predictions archive
#df[['control', 'treatment']] = pd.get_dummies(df['group'])
#List the stories with their status, age and sprint age.
#create a dummy variable column for which page each user received
# roughly 7.5% of users are classified as active under the current definition
# In the next code lines we will define some variables, what are they types?
# Apply lambda function to each column.  This example yilelds scalar value from function. # As seen below, NaN is ignored in calculation. 
# in order to aggregte data from non-unique index:
# retrieve the Missoula column through property syntax
# # Execlude tweets with no images
#Since 1 is considered True, we don't need to specify the condition "converted == 1". 
#Reference: http://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.proportions_ztest.html
#Copy dataframe and drop ENTRIES and EXITS columns #df3.columns
# First option - the slice object
#msft['2012-01-03'] while this will not work for dataframe, this syntax can work for series
# fitting our reducer to the full dataset # NOTE: takes 18 minutes on my local 8GB machine
# Let's see how data distributed along with day of week mean
#### DataFrame as a 2d array
# read a few columns of the tweet data
# No. of unique users in the dataset
# Graph the feature importances
# bin & average data into low, middle, high polarities
#By Zainab Calcuttawala - May 22, 2017
# we can use the datetime object to print the day of the week
# total size in GBs
# Load the results into a pandas dataframe.  #df.set_index('prcp', inplace=True, )
# table below shows few datapoints along with all our features
q="""select workclass,avg(age) Avg,min(age) Min,max(age) max from adultData group by workclass"""$ #q="""select count(*) from adultData where workclass='?'"""$
#we will also drop all the rows where records are not matched from meter location file:
# create the intercept # create the dummies
# identifying when new_page and treatment don't line up
# Sorting the values by highest value count first
# selected items from alice's cart
# Some care is needed when removing warnings. But for the final version of this notebook it should be safe.
# n old: 145274
# convert rate for Pnew under the null
# Understand the various Marketing Channels in the data
# Find the div that will allow you to retrieve the news title # Use this html, do not go back to the website to look for it # Latest news title
# reading the new csv file and storing the data in 'df2'
#Compute proportion of the p_diffs are greater than the actual difference observed in ab_data.csv
# get the days_plan_adead column using trip_start_date - timestamp
#Data Frame Summary (its a class based on 'pandas_summary' library)
# We replace NaN values with the next value in the column
# Read in csv file as a Pandas DataFrame
# Use Inspector to print the column names and types
#print(scaled_rip)
# last word category to emoji
#retrieve page with the requests module
# the cleanest way, in words: take all rows and the column named "A"
# Eliminar los registros que tengan nulos en la columna "created_time"
# first build an empty model, no training
# Handle case where there are no churns
# Use mean() to compute conversion rate
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
'''$ This program finds the date on which highest value occurred for each group of four consecutive calendar months in series 's'.$ '''$
#count of converted users in treatment group
# train again with the best rank 
# Get an actual number for count(*) by using scalar()
# train
#club numerator and denominator rating as one value called rating #drop unecessary columns of numerator and denominator
# only process the first three rows
# alternative: assign method (input arg must be a valid statement # that could be dumped on the prompt)
# In the column 'raw', extract xxxx-xx-xx in the strings
#making python dict
# return a frame eliminating columns with NaN values
# Sort the dataframe by date
#Use tweepy.OAuthHandler to create an authentication using the given key and secret #Connect to the Twitter API using the authentication
# make a GET request
#construct and display bbc sentiments dataframe
#Let's plot a subset of the data in order toget a better sense of it.
#Determine number of students with a passing math score (assume that a passing math score is >69)
# pickle final data
#List the counts of records by month (easily changed to show year year)
# train
# Create Indices
# Use Inspector to print the column names and types
# load the data # this file has 'numVoters', 'numDems', 'numRepubs' calculated from the voter tables and populated
#Checking for nulls
# Calculate probability
# create a Series holding the total size of each CohortGroup
# another example, list of countries
#is any row NULL ?
# Get a list of column names and types # columns
# Now lets day we only wanted to see level 3 and greater and get the top ten
### the number of times an user in the treatment group is assigned the old page ### the number of times an user in the control group is assigned the new page ### the total number of times the treatment and new page do not line up
# or # 0 is Monday so 2 is wednesday
# products DataFrame
# To get you started, here is how to test the 5th scanned value.
# print(df_final['created_time'][0].split('T')[0])
# Write answer here
# The protocol version used is detected automatically, so we do not # have to specify it.
# Education KOL pred
# Split arrays into "training set" (train_Features, train_species) and "test set" (test_Features, test_species)
#we use shape function to see number of rows [first element]
#this is the plot for the last year
# r squared value
# Convert sentiments to DataFrame
#join_d=join_c.groupby('party_id_orig').agg({"name_similairity": "max"})
# We print percentages:
# copies of original dataframes
### Create the necessary dummy variables
# look for rows with missing values
# inspect the first 10 rows # the printSchema() method tells you the data type of each column
# cannot validate a boolean as a string
#check for duplicates
# Create connection to the read-only DB
#to illustrate what one of these entries llok like
# Comparing the mean loan between both sets
#Create a box and whiskers plot of our MeanFlow_cms values
#Using the statisics library#Using t 
# tokenize the text
# since pnew <= pold. Our Alternative Hypothesis is one sided 'smaller' #zscore = 1.3109241984234394 #pvalue = 0.90505831275902449
# Probability user converted(Conv) given that  user in treatment group(Trt) # Can also be written as P(Conv and Trt)/P(Trt)  #Users who converted and were in treatment group
#re - upload complete file (processed in excel)
#display status dataframe from tweepy API
# Create sentiments data frame
# day based datetime
# For Displaying Data
# Reflect Database into ORM classes
#What does our index look like? 
#Create dict2 from the TUPLEKEY and DATETIMEENTRIESVAL #Print the value of this particular turnstile
#Chesapeake
### Step 18: Dissect just a portion of the data ## The purple cluster has a more even distribution (Non-commute days)
# Row where user_id is repeated
# Export file as a csv, without the Pandas index, but with the header
# Now we drop the rows that won't be required for any analyis
thecmd = '''curl -X POST -H "Content-Type: application/json" -d '{$   "query": "'''+query+'''"$ }' "http://'''+USERNAME+'''.carto.com/api/v2/sql/job?api_key='''+APIKEY+'''"'''
# take a look at your results
#'Sacramento': 'b71fac2ee9792cbe'
# Use tally arithmetic to ensure that the absorption- and scattering-to-total MGXS ratios sum to unity # The sum ratio is a derived tally which can generate Pandas DataFrames for inspection
# Sort tweets by date
# remove outliers
# See the column data types and non-missing values
#take the exponential of each of the coefficients to determine how 1 unit increase or decrease by country affects the odds of being converted
# Change some of the parameters to see how the model fits
# Lowercase Weather Types
#Stem words
# Setup Tweepy API Authentication
# Getting basic general info from the three data sets.  # Assessing archive get with Tweepy library and saved as tweet_json.txt
#Import sklearn model to split, test and score data,and fit data model 
#print(preprocessor('</a>This :) is :( a test :-)!' )) #print(re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', '</a>This :) is :( a test :-)!'))
#checking duplicated ids #Display duplicated user
# change table formatting         #qt_convrates_toClosedWon.columns = convrates_to_closedWon
# just short the gene dataframe by length and return the first few values
#train_df["num_description"]= train_df["description"].apply(lambda x: len(word.findall(x)))
#Calculate the mean of the control group's 'converted' to get the probability of an individual converting 
# Run in test mode and analyze scores obtained
    """Take in a vector of input values and return the design matrix associated $     with the basis functions."""$
#simulating the old_page_converted 
# Plot the temperatures for the week
# Find the dates
# Upload the processed files
# We replace NaN values by using linear interpolation using column values
# Setup plot style
#Take 'full_text' when available, otherwise take 'text'
# https://blog.mafr.de/2012/04/15/scikit-learn-feature-extractio/
#Converted Sale (Dollars) to float type and removed $
### Create the necessary dummy variables
# create dummy columns for country
#print (c)
#'Raleigh': '161d2f18e3a0445a'
#Complete a merge of the grouped dataframe and the schools to bring in the type of school (District or Charter) that #each school belongs to. #Presents the final dataframe.
# Creating time stamp format column using the column 'Time stamp'
# We display the updated dataframe with the new column:
# Extract data for a specific point using the coordinates 
# try alpha=0.01 and examine coefficients
## Total early-pairings , since 3-01-2018
# Putting new DataSeries into a list -- need to do so in order to use pd.concat to join them together in a DataFrame
# Lendo o ab_data.csv
#It's another video.
# Largest change between any two days
# find the rows that had nan values
#read the data (cleaned and merged)
# 2357 Unique values in Column 'price'
# Download the dataset # Split the dataset
def lookup_project_git(org, project):$     """Returns the project github for a specific project. Assumes project is git hosted"""$
# Run in python console
# Upload the raw Excel files exported from AkWarm Library.
# tweets with hashtags or user mentions of apple or samsung
# reflect an existing database into a new model # reflect the tables
# Visualize
# set code, name, release date and # of cards for the 5 latest expansions
### Plot distribution of fix times # plt.hist(np.log(threeoneone_geo['fix_time_sec']+1),bins=100) ### Is this Poisson distributed?
# For getting all the text (paragraphs)
#Example 7: Import File from URL
# add a new column # plot model error by bid-ask spread # plot model error against strike, many expirations included
# Instantiate the model. # Train the model, passing the x values, and the target (y) # the vectorized variable contains all the test data.
# creating new dataframe by marging d2 and countries
# Checking the datatypes of our submission sample
# compound dataframe
# multiply dummy variables with coefficient and apply e/(1+e) function # no new objected created
#create a column for the intercept and a dummy variable column for which page each user received.
# confirming work
#Check via customer sample
# Return the unique user_ids in df2
# Probability of treatment group converting
# import table of countries
# From a cartesian product of single indices
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#std deviation 
# How many stations are available in this dataset?$ stations_df = pd.read_sql("""SELECT * FROM station""", engine.connect())$
#draw the histogramm. as to see the distribution is normal due to the central limit theorem. 
#merge in the data
# and what is the minimum life expectancy for each year
# Simplify the dataset into question answering pairs # TODO: Filter out some long conversations that appear to be open discussions instead question answering.
# Set the index name in the dataset to `wines`
# Retrieving statistical information
##Taking data out from inside an object and choosing only specific columns 
## refresh .Renviron variables
#CALCULATES THE PROBABILITY OF GETTING A NEW PAGE 
# Following this we can calculate Career length per season 
# Drop all rows in which any null value is present
#inspect dataframe
# analyze validtation between BallBerry simulation and observation data.
# preview the data
# Removing supspended and limiting dataset to recent years
# seriously, what's up with this table # for row in table_rows: #     print(row.text)
# what our critical value at 95% confidence is
## adds release day of the month as int
#Houston': '1c69a67ad480e1b1'
# count the size of the class
# rename "value" field to "mmol/L" # convert mmol/L to mg/dL and create a new field # view the cgm mg/dL data
# Fit the model the show the result summary
#UTC is 5 hours earlier than EST. So in the graph below, tweets were peaked around 15:30pm
# Implement dataset
#### Test ####
#read-in new dataset and look at the top few rows
# By default, frequency is one day
"""this is what worked to create the existing file $ read in SHARE json records and set encoding to utf-8"""$
# Rows where magnitude is greater than 10.
# convert the result proxy to a pandas dataframe
# Remove duplicates
# Make output folder
#plt.plot(range(11,16),range(11,16),'r-')
### Creating the dummy variables
# are there live datasets that have multiple submissions?
#eliminate dollar sign from price values. This will allow us to coerce values later and not get 'nan'
# Saving data
#sample size for the old page aka the control group, is the same as the size of the control group
#plt.xticks(['2017-07-01 00','2017-08-01 00','2017-09-01 00','2017-10-01 00','2017-11-01 00'],['7-1','8-1','9-1','10-1','11-1'])
# A cleaner one line solution
# Checking our shapes. We see that "test" has an extra column, because it still has "id". We'll drop this column next  # after using it to prepare a submission dataframe, where we'll put our predictions
#Convert the date column into a datetime
# Rows and Cols per group
#Create dataframes
#d = feedparser.parse('http://g1.globo.com/dynamo/rss2.xml')
#we can drop control column and use only treatment column as ab_page #df.drop #df.rename
# Exponentiate the coefficient to interoret them
#find average by dividing HP total by count total
# Get the summary for our model
#df_model['intercept'] = 1
# Convert timestamp to datetime object
#Removing rows with null values
#remove all rows with any na values to plot data
# read in csv and remove any remaining unreadable characters
#visual examination first
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
#Or just show the rows, i.e., the first item in the shape result
#using groupby function to check values
# Join the SP 500 start of year with current dataframe for SP 500 ytd comparisons to tickers.
##### concatenate along rows
#Change the invalid names to None
#Calculate the number of unique users in the dataset using nunique()
# Evaluate on test set
# Left Join # The output table has all the key values from the left table, and matching ones from the right
# How many stations are available in this dataset?  Method #1 using a query from the database:
# Use Pandas to calcualte the summary statistics for the precipitation data
#read a csv file 
# Use Pandas to calcualte the summary statistics for the precipitation data
#sales for top 10 cities
# indexer untuk tanggal pembuatan status - pas di tanggal tersebut
#Remove rows with onnet_mou_6, onnet_mou_7, onnet_mou_8 blanks
# Lenghts along time:
# Here's a global summary of our DataFrame. # There are about 15000 entries. Our features of interest don't have null values.
#e6e6c9791516c2c3 -- original hash #e6e6c9791516c2e2 -- new hash, off by 1
# OPTION_CONTEXT???
#clean up the raw data 
# Collect data from 2017
# if you want to conert period index to date time index # since this is quater so it will take starting date of quater
# groupBy function usage if you want to not have the columns you are groupBy to become the index.
#renaming columns of the derived table
# do a left merge on the index (date info)
# Import first Funded Apps file
# Calculate n_new and n_old
# Step 18: add scores to bitcoin data frame and print first few values
# Positive Reaction ratio - Negative Reaction ratio  # Bi-variate relationship
#Sort CON by Contact_ID and Opportunity Name
# are all values equal to 6?
#Before merging
##### now apply to rows
# Get a list of column names and types
#Rename the last two fields
# %load solutions/pipe_results.py
#initial data 
# get index of lowest value
# Check missing value
# post simulation results of simpleResistance back to HS
# removing a row from the dataframe by index
# google
# Train, test split data_woe
# URL of NASA
#use requests builtin method .json() to parse JSON into a dictionary
# group data frame by time and number of tweets per each hour
#USvideos['comment_intensity'] = USvideos['comment_count'] / USvideos['views']  #USvideos['thumbs'] = (USvideos['likes'] + USvideos['dislikes']) 
#since it's only 5% of the data, drop it
# we do the same for the remaining two sets of data #First for the prediction tsv file
# Adds distinctive colour using c=labels and cmap and also a colour bar.
# users DataFrame
# Convert the returned JSON object into a Python dictionary
# 1. free_data = my_df_free1
# recommendataion results
#     print(screen_name)
# get commentary tweets of US newsmakers and opinion-shapers #tweets.dt = pd.to_datetime(tweets.dt,unit='D')
# PD will will missing values with NaN
#Measurement # take result as dictunary 
# Use a colour-blind friendly colormap, "Paired".
# Probability of an individual in treatment group being converted
# Topic 6 is a strong contender for 'eating'
#Print out each turnstiles incremental entries by day
# simulate under null for n_new
## Transpose
# Get the title of Guido's webpage: guido_title # Print the title of Guido's webpage to the shell
# Can apply the .rolling() to a whole dataframe -- which is convenient -- versus doing one series at a time # Only time you get a NaN when there is not enough observations at the beginning of the dataset # hence there will be 6 NaNs for each column -- see .info() below
# Set location of file
# use this cell for scratch work # 
# weird...
# Store stops in csv for safekeeping
#finding the number of unique countries in our country data
# It seems 1st user will like her profile whereas 2nd one won't.
# sentiment analysis
# calculate loading scores and variation each principle compenent acount for 
# calculate a one day offset from 2014-8-29
#And to check the tail
# What are the most active stations? # List the stations and the counts in descending order.
# Using our SQlite database created just prior, (hawaii), # ..  we now create the engine:
#Save figure
# Group by station and month-day with average percipitation of years available
# same logic here:
# percent-point function, tells us what our critical value at 95% confidence is
#We would like both the value to be >1K and the name to end with a specified string. We return the entries of data that match the 2. # Write your answer here 
# After being fitted, the model can then be used to predict new values:
# import the data and concatenate into a single pandas dataframe
# New Pandas DataFrame with a new name of the field including the Sentiment Analysis results (SA)
# see an example for a given author
# There are special aggregations that ignore NaN
#overlay the points of delay with a map of Chicago
#destination
# read the parameters of the character n-gram extraction module
# last day is -2
# assume p_new is equal to the conversion rate regardless of landing page
#guinea.head()
# Number of folds you wish to train # Number of rows in your dataframe
# Unstacking long data into wide
# create my X variable
#tweepy API Authentication
# The proportion of null values which are greater than the observation from out dataset. Note that this proportion is equal to the p-value of our null hypothesis.
# Columns with results
# simulate 10,000 values
#Check count of missing state value 
# Create a cumulative balance column in BTC for gdax
#investigate details of rows with duplicate user ids
# filter all clientTimestamps which are out of appropriate time interval
#saving dataframe in csv
# create a DatetimeIndex using a time zone
# Check the duplicates user_id in df2
# import data # show top rows
# 'None' is treated as null here, so I'll remove all the records having 'None' in their 'userTimezone' column # Let's also check how many records are we left with now
# get a list of the unique data types
# We can also use the "most_similar" function to get insight into the model's word clusters:
# n_old
# current.head()
# Focus on the top prices in the dataset
# Make sure cells are full width of the screen
# print The proportion of users converted
# replace 0's, since they cause potential blow-ups
# calculate the two conversion rate on the for the old and new age for later
# retirado do link fornecido no enunciado. # Tells us how significant our z-score is # Tells us what our critical value at 95% confidence is
# zona waktu as index
#find conversion rate per country
# Setting up plotting in Jupyter notebooks # plot the data
#%% Read dataframe 
# now column-wise
# get the row at array position 1
#There are too many document types to plot on one chart so select the types with highest maximum
#lsi_out[list(range(50))], reduced #result = clustering.k_means(reduced, n_clusters=5)
#now we need to drop the columns that we don't want in our training data  #set that we feed into the algorithm;
#creating column "day" with new variable 1=day and 0=night
#let's look at 1 timezone.. washington, dc
# you are left with 31 columns
# probablity = 34753/(34753 + 255831) = 0.1196 (approx)
# Create a Prophet object and fit the data
# the off-season.
# Initiate and fit the model
#removed duplicates, we used retweets from the twitter_archive data set.
# Scale back
# flight_to_brisbane = flight.where(col("price") > 0 & col("to_city_name") == "brisbane").groupby(flight.search_date_x).count()
#Groupby to elimiate duplicate songs  
# join two dataframes: df2 and countries
# calculate length of array where values greater than cutoff (from ab_data.csv, not from simulation) # and divide by length of total array
# Distribution of tweets per year
# Prep I-TASSER model folders
# Perform a linear regression.
# Build orders based on this threshold
# .table() gives you the frequency count of each unique level
#Twitter connector
#now we can register the function for use in SQL
# from data frames expects index 0,1, ...
#Using cursor to connect and query the newly created table
#Read in the file containing our flu incidence outbreaks and check it out.
#types of data
#there are 2771 people with ages over 80 -- looks like there are some 2014s too
# We display the updated dataframe with the new column:
# Test the timestamp column datatype
# plt.savefig('Actual number of days between purchases')
# Revenue has null values for some company. 'V' has been such identified company. # In this experiment, we remove the company from the dataset instead of interpolating. # Convert quarter_start field to datetime.
# Invoice data generator
# Set representative structures
# Split PKG/A column into to and drop
# Clean up rct
#This returns the first twenty items in your dataframe
# The protocol version used is detected automatically, so we do not # have to specify it.
#droping the non unique id 
# plot null distribution # plot line for observed statistic
#sc.stop()
# Plug sentiment in to df_tick
# send data to PostgreSQL 
#Remove small tokens with less then 3 characters.
# print Player df shape, column names and top 3 rows ( what we really need is id and 'overall_rating')
# using map and 'party_dict'
# print(X_train.shape)
# How many ticket in each type
#View only object columns
# line plot: x-cordinate = index, and y-cordinate = Age
# exercise (page ) # 1) extract the superdiagonal # 2) extract all the numbers divisible by 3 using a boolean mask
# check the simulation start and finish times
# Lenghts along time:
# Previewing data
# Add year/week
# load data description and see samples
# Import the data from the station table into a dataframe
# CHECK THAT LOGIN SUCCEEDED
# remove data that do not have place object
# read the file to test
# df = df.drop('table_id', axis=1) # df = df.drop('robot_id', axis = 1)
#calculate the total number of stations. #station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'})
#subset 2014 data #for predictions later
#descriptive statistics for all columns
# creating a logistic regression model
#print(cv2)
# archive_df
# p_new under null 
# Data size too small
# find the countries that have over 1 billion people. (thats kind of a lot)
#probability converted given that that the individual is from the control group
# Pure Scorers
#Get the summary of the regression model
# We can use df.eval() to create a new column 'D' and assign to it a value computed from the other columns
#See types and options for country
#x=dataframe['Date']
# Display the duplicate row entry 
# Check out user age. # Note that there are clearly users with invalid age data--range goes from 1-2014!
# the number of reports from the most active station
# What are the most active stations? # List the stations and the counts in descending order.
# Create BeautifulSoup object; parse with 'html.parser' # Examine the results, then determine element that contains sought info
# Print each negative tweet
'''Here we are creating a new column named new_page_converted with a length equal to the new_pages count$ calculated above and filling them with one's and zero's with the mean probability we calculated above for $ one's and remaing for zeros'''$
# df.head()
#find latest data point
#YelpPymongo.py  #This lets pymongo about the connection we want to use, local host in our case #Setting context to new yelp database 
# Get all of the Variables from the ODM2 database then read the records # into a Pandas DataFrame to make it easy to view and manipulate
# to set this date to index. 
# Gets historical data from the subscribed assets, between two dates with daily resolution
# Read data from csv to a dataframe # data_df
# Use Pandas to calcualte the summary statistics for the precipitation data
# Get a response to the input text 'How are you?'
#14. Join users to transactions, keeping only rows from transactions and users that match #via UserID (inner join) Expected Output:
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Comparison operators and chained expressions
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
#merging the particpants column into the base table
### Create the necessary dummy variables
# accediendo a filas y columnas por posiciones enteras
# Save the dataframe in a csv
#inspect station data
#Change the data type of the 'Postal Code' field from an integer to an object #Show the column name and data type for the column
# Step 4: Investigate the data # This shows that there are now only 12 unique times b/c the AM and PM are no longer there
# The resulting array contains the unioni of indices. # Could be found with
# Defenders
# https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings/forums/t/18882/script-to-cook-sessions-csv
#df_weather.Date = pd.to_datetime(df_weather.Date).dt.date
### 4a. # show which country has highest conversion
# summary of information about the data frame 
# read newly created dataset into another dataframe
# Histogram of times a dog name is used
# double check all of the correct rows were removed - this should be 0
# Lets remove outliers caused by mistyped dates and missing values
# index is an array-like object of type pd.Index
# Note that this file contains 7 duplicates that are removed.
# preview the data
# drop retweet columns
#Store-store_states (checking if there was any unmatched value in right_t) #Weather-state_names
# range index allow us to use the series like a list
# Hmm...in this case, we want the Date column to be our index, so let's try again
# Run statistical model, note that the results cannot be interpreted straight foward because # the distribution of tripduration is not normal.
# convert erroneous datetimes to NaT
#df.describe()
# builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# This isn't right. There are no long positions. Take another look at the code and fix it.
#next we drop the Hand value
#use unique() function
#grabamos el resultado en un nuevo archivo
#combine so all charges have a proper unblended rate
# Show results
# Number of reports from the most active station
# Sample size for new_page 
#Calculate the sample mean difference
# loading data 
# What is the number of backers that most success projects have?
# generate evaluation metrics #print metrics.roc_auc_score(y_test, probs[:, 1])
# Reflect Database into ORM classes
# put all control AND old_page into one dataframe
# obtain the value of the variable
# Take a look at the words in the vocabulary
# Before we can do the sentiment analysis, we need to load the Loughran and MacDonald dictionary. # See https://www3.nd.edu/~mcdonald/Word_Lists.html
# appointments = pd.read_csv('Appointments.csv')
#how many wells that were good at time of measurement do we predict will fail within one year from today?
#Lets see what Melt does
#find number of rows in the dataset
# result = customer_visitors.groupby('Yearcol').mean() # result.customers_visited = np.round(result.customers_visited) # result
# have a peak at data that occurred after 2018-04-01 23:59:59
# Dropping unneeded columns
# Showing newly created columns
# and finally, to simply return to your normal, unindexed dataframe # you can use reset_index()
# Sort the dataframe by date
# Writing to .csv file
# collecting data and converting to Python dictionary
#Find the proportion of converted rate assuming p_new and p_old are equal
# Converting data type
#Create InstanceType column from Usage Type col
# fit the model
# Removing one of the duplicate rows and verify duplicate removed
# use the to_datetime function instead of the parser function
# Change to text value to category 
# validate this is what we expect, missing values may throw off pandas types
# Query all tobs values # Convert tuples into list
# We can modify data with them too
# Read/clean each DataFrame
#inspect measurement table
#Show rows which address is null
# A tweet with just two characters? 
# Code
### Create the necessary dummy variables
#Vemos si existen fechas con valor Nulo
# input_col = ['msno','date']
# best/worst breweries, min x beers
# take result as dictunary 
# make sure all column names are the same and in the same order
# Alternative solution, in Python 3.5 and above # Z = np.ones((5,3)) @ np.ones((3,2))
# file 1
# wall time to train model
# sum the results
## To determine the proportion of users converted in control group
# Create a one-dimensional NumPy array from a range # arange returns evenly spaced values within a given interval, given a starting point (included) # and stopping point (excluded), and an optional step size and data type.
## Always fill values after reindex
# heaviest connections in the graph
#!pip install -U jupyter #from jupyter_core.paths import jupyter_data_dir jupyter_data_dir()
#before
#df_series.index = pd.DatetimeIndex(train_frame.index, freq="N")
# Check for null values to confirm drop
# and at the bottom
# drop it since it has just one value for all the columns
# METAC_SITE_NM1 is a categorical column we want to keep
# People that has a bio line, searching for who is not shy
# Express as a time delta in days using the NumPy function timedelta64
#order by date
#Created a separate dataframe that holds only frequent users 
#Proportion of p_diff values that are greater than obs_diff from original dataset
# the probability of converting for treatment group
#cost for the period
# Fit Linear Model + Obtain Results 
# Identify incomplete rows
# Who are the most common wine reviewers in the dataset? Create a `Series`  # whose index is the `taster_twitter_handle` category from the dataset,  # and whose values count how many reviews each person wrote
# plot the sampling distribution
# Firts, create an array of booleans using the labels from db.
# Decoder: uses the same weights as encoder.
#delete the rows that didn't lineup
# Import and Initialize Sentiment Analyzer
# pip install engarde
# Get indexes and values
# Double Check all of the correct rows were removed - this should be 0
#new_messages.timestamp = pd.to_datetime(new_messages.timestamp, unit='ms') #new_messages.watermark = pd.to_datetime(new_messages.watermark, unit='ms') #new_messages.datetime_created = pd.to_datetime(new_messages.datetime_created)
# Probability an individual recieved new page
#use alternatively df.groupby("group").nunique("user_id") in order to get unique users by group
#Sampling distribution.Updated per review:replaced for loop for sampling distribution with in-built function's parameter,size.
# total number of missing values 
# Write the test results  #output = pd.DataFrame( data={"id":testdf["id"], "sentiment":result} )
#This gives you the top 15 places in user_location 
# Plot time (x-axis) against number of favorites for each tweet
#tweet_df_clean.drop('level_0', axis=1, inplace=True)
### list of column names
#checking if there are any that don't match fully
# Veamos una descripcion que incluya todo los casos (numericos y no numericos)
#df_inventory_santaclara =df_inventory_santaclara.iloc[7:,]            ##remove unnecessary columns
# Are all properties located in King County?
## Amount paid
# Calculating the difference between mean of each scenarios probability
# create an engine for hawaii.sqlite
# and if we want it for both levels - average age of  # reported event victim per bin by industry
#arranging 'Year' column in ascending order #before sorting #after sorting
# Check if all statezips are valid. # Select all statezips that have less than five instances in the Data Frame.
# PRs opened in the last month
# Plot the Dew Point and Temperature data, but not the Pressure data
# Load data.
# add local python functions # add the 'src' directory as one where we can import modules
#creating target
# Reading packets from pre-captured file
# load workspace configuratio from ./aml_config/config.json file
# tensorboard = keras.callbacks.TensorBoard(log_dir="../logs", write_graph=True, write_images=True, histogram_freq=1)
# merge data
#https://github.com/statsmodels/statsmodels/issues/3931#issuecomment-343810211
qry = '''\$ DROP TABLE IF EXISTS lucace.disc_667_stage_00_list_of_users$ '''$
# Create variable 'Tip_percentage_of_total_fare' to store the calculations of tip as a percentage of the total fare # by dividing each row's "Tip_amount" with "Fare_amount" data value
# accediendo a varias columnas por le label
# df=load_df('rolled_filled_elapsed_events_df.pkl')
# join for plotting purposes
#This is what we would expect 
# Check for missing values
# Call the function to pull NWIS data
#provided dataset:
# outlier points
#https://stackoverflow.com/questions/30222533/create-a-day-of-week-column-in-a-pandas-dataframe-using-python
# show all the information on the duplicated user_id rows
# Calculate the date 1 year ago from today # Perform a query to retrieve the data and precipitation scores # Save the query results as a Pandas DataFrame and set the index to the date column
#Save node information. The columns to be saved in this file are set by the user. Note that node_type_id is needed here
#Initialize Sentiment Analyzer:
# Reads the countries csv into a Dataframe
# create an empty list to store the query results
# first 10 days average. 
# concat df and coming_next_reason
# exog = sm.tools.add_constant(exog)
# get dummies for country and category type
# get errors from each of the models
# the cost is the data gets copied
# Start Chrome with Selenium
#info
# How many stations are available in this dataset?
#modelD2V.wv.wmdistance(currentData[0].words,currentData[1].words)
# describe dataframe
#Design a query to find the most active stations. #USC00519281 had the highest number of obs, at 2772
# approx cost for a year
# Combine dataframes
### Create the necessary dummy variables
#Before a merge, the column names must have the same name
#Dropping duplicate rows
#count number of unique cases per column 
# concatenating the DataFrames together for content matching operations
#duplicates #Credit:Hector Ian Martinez
# dummy data
#cleaned NYC 2012-2013 traffic data with coordinates and total traffic #convert the cleaned data to csv for use elsewhere
# Some stats of constructed dataset:
# drop zeros in all 
# run this cell
# Sample 10 random rows from the dataframe
# Find z-score and p-value
# Let's check which days have a traffic higher than 1%
# Checking duplicate userid
# res.to_csv("/Users/monstar/Desktop/a.csv", encoding="gbk")
# Print the RESULT LIST so far ###print(f"RESULTS LIST: '{results_list}'") # Prepare DataFrame for the bar plot
#print model.words # list of words in dictionary
#To get the average per student then you need group the data by student
# since this took forever, better save the model to disk
# Examine GDP Series
# make a hold out set of 1k? 
#learner.load_cycle('adam3_10',2)
# %load shared_elements/system_info.py
#the plot shows that there is almost no correlation between numeric variables
#df_adopted = df_adopted.reset_index()
# Information of duplicate row user_id
#review expanded data- note could further do the same thing for contributors, or affiliations
# isMax (bigger value is better than small value)
#Clear the frame so that a new plot can be plotted properly
### Keep 'name' (creator), 'is_registered'
# Create Excel File
# use SVM to classify
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-04-24&end_date=2018-04-24&api_key=" + API_KEY)
#Creating slice from the df for the second table
#replacing column name 'id' with 'tweet_id' so that the column name will be same across all three data frames
# typical workflow # perform some actions on database: create table, insert rows to tables, reterieve data, etc.
# Glance at r_close
# Can get the set of trades created by this signal
# dateutil
#transit_df['EXITS'] = transit_df.apply(fix_exits,axis=1,iqr=intqrange)
'''This cell is meant to be a workaround for the following error:$ module 'scipy.stats' has no attribute 'chisqprob'$ '''$
# Optional: save model for future comparison
#analysis_historical['Volitility_90_day'] = analysis_historical.groupby('coin_name')['close_price'].rolling(window=90).std().reset_index(drop=True) #analysis_historical['Volitility_365_day'] = analysis_historical.groupby('coin_name')['close_price'].rolling(window=365).std().reset_index(drop=True)  ## fyi, normally annual volitility is 255 instead of 365 since there are only 255 trading days in a year, but since crypto is all 365 then I leave at 365
#It seems we have one entry per zip code
# Run the testing model 
# the proportion of the p_diffs that are greater than the actual difference observed 
# Dropping the rows for mismatched treatment groups # Dropping the rows for mismatched control groups
#check how many values are missing from the data
#  Sample get all using sqlite3
# Check if there are any 'North/Northeast/Northwest' terms in street.
# filter SwissProt proteins to remove species for which a proteome was downloaded
# check Basin Parameter info data in file manager file
# Find the probability indiviual treatment group they converted
#using sqlalchemy engine #print(seng) #engine = create_engine('mysql+pymysql://root:'+spassword+''@localhost/animals_db')
# We're plotting them all at once so... there are going to be some visual issues # Volume distorts the viz because those values are much larger than the prices. # lets put all those other ones on a plot.
#the row names in a dataframe correspond to the index values which can be retieved using .index command
# we'll read the user file into a Pandas dataframe.
# use np.random.binomial to draw samples from binomial distribution. using p_new for their probability as the probability of conversion. # new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new]) # preview to confirm simulation
#df.iloc[1,] #drop 'adwordsClickInfo' feature because its embeded features have been extracted 
# Reshape the array into a 4x5 matrix
### Fit Your Linear Model And Obtain the Results
# Remove characters and redundant whitespaces
# dropping front end of curve, dampens predictive power
### Create the necessary dummy variables
# Instantiate an empty Tallies object
# lets get the average age of each agebin
#To determine the probability of the pages
#Remove rows which contain words from blacklist
#2.Convert the returned JSON object into a Python dictionary.
# Remove rows with missing values in column 'driver_id'. The order was not accepted. there was no trip
# Add stops which were not in oz_stops but were in stops
# suburban
# Save the created DataFrames
# We reused the first cell in part i to make sure there's no more duplicates. 
# Number of user_ids repeated # Find duplicate user
# for above random dataset checking the sum of all the numbers occured on Wednesday
# make recommendation for a single user # select favored restuarants # select similar items for user favored restaurants, and sort them to get the top 5
##nan treat as [0,0....0]
# Not really needed, but nicer plots
#first create a new column called "body_tokens" and transform to lowercase by applying the string function str.lower()
# Dataframe from cutted tweets
#print head of type
#Extract the year, month, and day from start_date
# Remove rows with lingering null values
# Use Pandas to print the summary statistics for the precipitation data.
#save data to a csv file
# Show how to use Tally.get_values(...) with a CrossScore and CrossNuclide
#huh = soup.find_all('header',class_='gallery_header') #huh
#A new variable: 
# Quickly open just the first 10,000 rows of the CSV file # We will store the result of the function in a variable called "preview"
# The correlations are the following
# Given that this is less than 4% of our data, we will remove these rows. # It appears that most of the vehicles were first registered in the past 20 years.
# How many stations are available in this dataset?
#If you ran the upper code, the data can be directly converted to a Spark DF #Else, you can just use a pre-parsed .csv file available in the next codeblock
# We use pprint as it makes our output easier to read 
# define columns for the interaction of country and landing page
# List comprehension for subreddits of reddit posts
# concat df and coming_next_reason
#from pandas.io import data, wb # becomes #from pandas_datareader import data, wb
#model 1 is accurate .74 out of 1
#mean speed group by hours
### Fit Your Linear Model And Obtain the Results
# reflect an existing database into a new model --> creates a Base object # reflect the tables --> creates ORM objects for each table in the 'hawaii.sqlite' database
# 2. Clean user id in tweets data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Write to CSV
# Read the blight violation file to avoid a warning we read everything as strings # remove duplicate tickets # rename lng as long
#number of new pages with conversion
# Dealing with NaN
# converting the one-star and five-star counts into frequencies
# filter only domains that need to be shortenened
# Create the pie chart based upon the values above # Automatically find the percentages of each part of the pie chart
# your code here
# Next we look at the different values for each column
# Perform a linear regression. #model_x.summary() # For non-categorical X.
#df2.index[146678] #146678th element of the index?
# examine posts that contain missing 'url' values
# merge two properly aligned dataframes together
# import modules & set up logging
#check the loaded file
# Test - note: this worked, I commented it out to save space. #get_real_types(tweets)
# to read multiple files just add the names
# Convert JSON object into Python dict
# generate the Word2Vec model
# read csv file # get the first 5 rows
def sort_dict_values(d, reverse=False):$     """sort dictionary by their values"""$
# Successful conversion/parsing from string to int
# And add back to the original dataframe
#Show 100th row
#prob don't need # last_train = train_df.iloc[-1].name # last_train
# Computes the z-score and the p-value for our null hypothesis, defined as the alternative being p_new > p_old:
# split into training and test sets using dedicated wrapper
#Converting date time to numeric for calculation 
#Find all of the business days in our time span
# Example # Get Apple Stock
# Print best error rates achieved
# faster way with pandas 'to_datetime' method:
### Fit Your Linear Model And Obtain the Results # I don't believe there is a difference between page and the group since the dataset was wrangled # to make sure these match.  However, per the instructions I will do it anyway:
#exponentiate the coefficients to get the muliplicative change in the odds
# Transaction Level Profit & Loss
#Probability of any individual receiving the new page
## 3. print the median of traded_volumes
# No need to add ab_page, as new_page will always have treatment, we'll end up having singular matrix
#count vectorizer has extracted 38058 words out of the corpus.
#col.assign(rounded_dt=weather.datetime.dt.round('H'))
#Wow that is so small!
### we do not need all three new columns, we drop 'wk1'
# get logarithm of price, powerPS for possible better apparent data distribution 
# For doing the same thing as above
# To count the number of objects in which 'timestamp' 1 day later than 'create_time' 
# Now we can check if there are any significant differences, between the 2 labels,
##Convert rate irrespective of the landing page
# Create the inspector and connect it to the engine # Collect the names of tables within the database
# Compute fast fission factor factor using tally arithmetic
# Read in 2018 data
## YOUR CODE HERE
# generate a series based upon business days
# assign newly created column names list to dataframe columns
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
# To use the saved classifier # The above 2 cells can be commeneted once the classsifier has been trained and saved to a file
# Starting the pickle file from the ETR-All notebook # Use the logit model fit parameters from the pums notebook # At the end of the pums notebook, the logit.params.index labels are cleaned to remove prefixed and brackets before pickling 
# sort the index 
# Calculate how many times people talked about IKEA by following function  # Applying same way, count data of other interested cities are saved as following dataset
# And of course, we can easily save our new data
# file path, mode, and encoding can be seen when file object is called
#Next we will read the data in into an array that we call tweets.
# Reflect Database into ORM classes
#Setting the datetime format while importing data
# show dataframe first rows
#ubico por columnas #df['field1']
#the column array values for WHO Region column is retieved using values method and stored in a variable WHOregion
## the number of cancelled vs not cancelled shows
# answer
#now pass that series to the datafram to subset it
#1 reset index so that we do not have issues since we loaded multiple files 
# Simulate conversion rates
# Largest change in any one day (based on High and Low price)?
#info 
## Initial Apple Negs
#1.Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017  #(keep in mind that the date format is YYYY-MM-DD)
# This will load the cached model
# Merge the morning traffic avg with census data
# Converting the results into a dataframe # View the Temperature DataFrame
# Calculate predictions: predictions
## create price changes for certain time periods in advance
# convert Tip_Amt into Tip percentage
# change default figure and font size # visualize the feature importance of every variable
# We can analyze categorical values with value_counts()
#we seem only to be interested in 'first_name'
# The index of our data frame is the first column (how the data is organized)
# group by
# Retrieve page with the requests module
#Exponentiate the coefficient to see the influence of the country variable on the conversion rate
# Average chart statistics. # Calculate the average of averages.
#checking the number of rows in the dataset
#ignore_index means the indices will not be reset after each append
# Visit URL
#Change this feature from a string to numeric. #Use errors = 'coerce' because some values currently equal 'T' and we want them to become NAs.
#Number of users landing on new page
#propiedades entre 25 y 50 metros cuadrados
# Load and peek at the AMD data.
## check the number of warning related features
# Handle Snow Weather Type
#we need to add another feature and lable
# Create a temp dataframe from our PCA projection data "x_9d"
# find each of the inconsistent rows in this horrid table, which is now in a new place # for row in table_rows: #     print(row.text)
# The proportion of users converted
# helpers contain helper functions # main_functions contain the feature engineering code # mod_class contains the prediction class
# Establish a connection to your GIS.
## sanity check
# print all the outputs in a cell
# doctors
# find all indexs with nan # find all columns that have nan in them and present them to with the meta-data
#tweetsPorFecha['tweetCreated']=tweetsPorFecha['tweetCreated'].map(lambda x: x.strftime('%Y/%m/%d') if x else '')
# simulate n_old transactions with a convert rate of p_old under the null
# drop rows
# Use numpy random.binomial to simulate n_new trials and store in new_page_converted
##Create an engine to connect to sqlite database created in previous step
# Control the default size of figures
# create target vector(s) | lookforward window
# percentiles look right; most data should be fairly close to the value at the 50% mark
# concat dummy/indicator variables to dataframe
#counts null values in all columns 
q="""select capital_gain,capital_loss,(capital_gain+' '+capital_loss) Net_capital_Gain from adultData"""$
# also useful: isin
# query to pull the last year of precipitation data
# plot of the length of tweets vs period of time
#find the duplicate user id
# Gensim Library
# get the current date
# Create the ticks for our bar chart's x axis
# You must save your regex for retweets in this variable # You must save your regex for hashtags, links, or pictures in this variable
# Converting the results of the query into a pandas dataframe
#df_final_10 contains tweets of rating_denominator value of 10
# masked['user_age'] = np.where(masked['id'], 0, 0)
## I need to create some files to use in the globing exercises
#Find out number of rows and columns
#store the data from 'tweet_json' to tweet_df dataframe
# March 2016 was our start period, and the period frequency is months.
# today_
# Create a new dataframe for the Airport ID (It does so by testing true/false on date equaling 2018-05-31)
# read in train, test, and sampel submission files #sample_sub_df = pd.read_csv('sample_submission_2.csv')
#### Test ####
# Load the results into a dataframe and set the index to the date #calc_temp.set_index('date', inplace=True, )
##Specific rows in a DataFrame object can be selected using a list of integer positions.
# find the tweets with more than one occurences of ##/##
# TODO: Create a new agent with a different state space grid
### Create the necessary dummy variables
# show the oversampled dataset. Now, we have balanced sample.
#working with fakes
# For delay/cancel count, fill NaN values with 0 (i.e. No delay/cancel flights)
# can pass row numbers to .loc if the row ix are numbers
#series_d
# Pivot airquality_melt: airquality_pivot # Print the head of airquality_pivot
# Find user with most tweets:
# Needs `lxml`, `beautifulSoup4` and `html5lib` python packages # Local backup in data/sea_levels/Obtaining Tide Gauge Data.html
#removing the rows in which 
# Check the rating of a known FA class article.
#How many rows with NULL values?
# probability an individual got the new page
#forecast_range.head()
# Take a peak at the data
# Use a session query to find the first row in the database
# showing plottings via terminal rather than window popup
# Show the node_types file. Note the common column is node_type_id
#Calculating old_page_converted
#group by breeds and then sort them by rating numerator
# Choose the station with the highest number of temperature observations.
# So we will aggregate by the top 6 brands as they more than 5 percent of the total values.
# at what index duplicated user_id is
# Train the classifier Naive Bayes Classifier
# Further data work and saving to .csv
# Create dummy variables
#transit_df = transit_df.reset_index().set_index('DATETIME') #transit_df_rsmpld = transit_df_rsmpld.reset_index().set_index(['DATETIME','STATION'])
# number of rows in the dataset
#df is the dataframe where the content of the csv file is stored # dropping NaN values
#Change Var1 to string so it won't treat categorical variable as numeric
# No. of unique user_ids after cleaning our dataset.
# Concatenating Strings
# For Displaying Data
# Pickle the 'data' dictionary using the highest protocol available.
# bind parameters to the stmt value, specifying the date_filter to be 10 days \ # before today
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser' # Examine the results, then determine element that contains sought info
# Convert this cell to markdown and write your answer here.
#Check to see if the function is working properly
# and mode
#Dot produt to get weights #The user profile
# replace NaNs with 0
# write your code here
# Create BeautifulSoup object; parse with 'html.parser'
# Load data.
# computes the average sales, from the first date _until_ the date specified.
# Run Kruskal-Wallis test for male vs unknown
# Describe de la duracion de los viajes con outliers
# old allocations
# Combine ET for each rootDistExp # add label 
#removing 'Ehail_fee' column from the data set #checking the columns in the data (checking whether 'Ehail_fee' is removed)
# Get the mean values of the airports in NYC with the index number of 2, 3 and 4.
#Total the amounts in the support subset
# Right Join # The output table has all the key values from the right table, and matching ones from the left
# 7. Bar plot visualizing the _overall_ sentiments
# Push table to database
#full_stats = df2.groupby('group') #full_stats.describe()
# what is that date's month?
# Merge rows to summing along rows of multidex
# Examine the results, then determine element that contains sought info
# write out csv file of all data
# import and retrieve portuguese stop words # stop words are not considered as token as usually they carry no meaning (!)
# Cleaning up DataFrame for calculation  # Lagged total asset # Average total asset - "rolling" 
# lag two days
#creating copy of the predictions dataframe where we will store the cleaned dataframe
# sum the bills
#Export Canadian Tire file to local machine
# drop columns where reason for visit name = Follow up, returning patient # existing patient dataframe
# create a dataframe to illustrate GroupBy
#save the results in excel or show it #df_p.to_excel(str(keyword)+'20.xlsx', sheet_name='sheet1', index=False)
# Need to factor in that some positions were purchased much more recently than others. # Join adj_close dataframe with portfolio in order to have acquisition date.
# Remove duplicates
# add features to test # check wether the join operations where right
#save clean dataframe to disk
#using groupby function to check values
#filter out the above #verify #crime_data["OFNS_DESC"].sort_values().unique()
# can also just type 'results'
#Creating a (pandas) DataFrame # We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#'Seattle': '300bcc6e23a88361'
# Add a new column that we will use for groupby operations
#Probability of a user converted in treatment group
# Create a scatterplot matrix
# What does this look like with 15 topics (potentially the knee)
# Golf KOL
#Create a week column that represents the week number in a year
#find the transgenic lines for each cell
#Creating new column monthly_savings = MonthlyIncome * 0.15
# Information of Passenger_count based on credit card usage
# get one column by attribute, return a Series object
#conversion rate
#ax.set_ylim([0, 100])
# mean US precipitation grid time series # mean plot
# Check shape 
# Create variable of columns that are no longer needed and drop them from the DataFrame 
# examine the first 5 rows
# Train model
# step 9: a graph of a smaller date range within the dataset. # the range to zoom in on will depend on the selected dataset (month, year, 4-year, 6-year)
# Nasa Mars News URL to Scrape
# how to create a class using type
# Here the benchmark is NASDAQ Biotechnology index 
# Example 1:
# convert date columns to datetime 
#That's even better! #ANSWER TO THE QUESTION take three: #Worse than before but a bit fairer.
#exponentiated the CV to inteprete the result
# We can use the describe method to inspect our data easily
### Create the necessary dummy variables
# inspect the dataframe
# fit tfidf dataset
# population or sample size of treatment group
# We'll explore the English to Spanish file to get a feel for it
#title_alt = imgs.find('img')['alt'][2]
#cur_laurel = conn_laurel.cursor()
# Print records in train.csv
# one row NaN for retweet and favorite count
# similar to Matlab's tables, we can set a dataframe's column # as the index. Note that it's not done inline.
# So we create new dataframe "baseball1_df" with only the needed columns and rows with missing data removed. Furthermore  # we reset the dataframe index so that we have and unbroken chain of index.
##Reduce DataFrame as the transform function is very expensive on the whole dataset
# Calculate Probability of new landing page
# Decision tree
# read back binary arrow file from disk
#F_hc = m_hc[score_variable].var() / a_hc[score_variable].var()
# Keep only the rows when control lines up with old_page and treatment lines up with new_page
# check Basin variable meta data in file manager file
#r_train['has_extended_profile'].value_counts()
# Get the mean of these probabilities
# set path where files are located  # (used \\ because it is a windows environment and \ is an escape character)
#file path
# Further cleaning on the data to standardize lowercase, and add remove gaps
# results = soup.find_all('li', class_='rollover_description_inner')
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Conversion of Training data into all Numerics 
# Find the numeric values: matches # Print the matches
# Make columns with country average
# Slice out specific rows and/or columns with iloc
# add a notebook to the resource of summa output # check the resource id on HS that created.
#get frequesncies of sms #get size of groups
##The Iowa zip codes all begin with numbers between 50 and 52 anything that began with anything above that is obviously  # a mistake. The real zipcode is 52601
# Convert stock ids & names lists to Pandas DF 
#Old page #p_old = df_old[df_old['converted'] == 1].mean()
# Come back the next day skip last 2 blocks and resume here
# Convert Timestamp series into datetime format
# check build estimator model
# Looking how long was the longest trip recorded in the dataset to see if it makes sense
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# add features to train_reduced # check wether the join operations where right
# Define endpoint information.
# change the data type of publicationDate column
# Also works on Dataframes # works axis=0 be default
# instantiate the algorithm, take the default settings # Convert indexed labels back to original labels.
#Users by cohorts
# Display data for min and max age
# Qn 1. # Collect data from FSE for AFX_X for 2017 with API call
#tweetdf = pd.read_csv("tweets_w_lga.csv")
# mean() along each column 
# instead of accepting the unwieldy labels we can define them
### Creating dummy variables
#shape of the data
# Unique users
# Find z-score and p-value
# path for writing to csv the merged dataset: /home/sb0709/github_repos/bootcamp_ksu/
# YOUR CODE HERE #raise NotImplementedError()
# extract categorical features which grouped by business_id
# make copy of the dataframe: # DEFINE: Convert tweet_id as string  #TEST:
#stats.pearsonr(ng['Volume'], lll['Volume']) #.24, 2.59e-18
#converting tweet_id datatype to string
### Checking the number of dummy variables needed
#check to make sure columns were dropped as expected.
#Importing Tweet Data
# Dimensions:
#get highest interday change
# Create an object to transform the data to fit minmax processor
# check for duplicates again
# This creates a list of BeautifulSoup objects.
# Count number of users with nodes input for each
#Verify data was populated into measurments table within database
# we will create the indices from the  # product of these iterators
# Calculate basic statistics for y by group within x.
# TASK 2
# Extract endpoint url and display it. #scoring_url = deployment_details['entity']['scoring_url']
#Remember this RDD:
# Keep only the year that a project or user was created
# points.iloc[0, 1]  # ERRORS
#df_insta.dropna()
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
# Calculate simulated transactions conversion rate
#details of duplicated row
# What's the longest ingredients list?
# The contractor table has been created locally before running this step #Using if_exists='append' can prevent the constraint of columns being changed 
# plot autocorrelation function for therapist data
### Fit Your Linear Model And Obtain the Results
#the below display is the list of company names and tickers that do not begin with same character
#plot histogram
# dummy variables for the landing_page feature
#get value count for each country to get idea of size
#high res image
# Check for duplicates
# datAll['month'] = datAll['Date'].map(lambda x: x.) # datAll['Year-Mon'] = datAll['Date'].dt.strftime('%b-%Y')
# Assert that all values are >= 0
# Read in the countries csv file
# Example
# try on the test data, however, the result not very good # reason might be overfitting
# load dataset
# add 5% of noise
#    time_left_for_this_task=120, per_run_time_limit=30, #    tmp_folder='/tmp/autoslearn_regression_house_tmp', #    output_folder='/tmp/autosklearn_regression_house_out'
# user_id 773192 is duplicated
#creating another subset to excluding the rows mentioned above #Verifying to ensure that the subset created is excluding the rows mentioned above
# Check the result with built in function as cross-validation - passed
# inspect the dataframes
#Sorts the table by Profit. #Since 'pro_result' is a pre-sorted table, we can use just .head() to display the top 5 values.
# Requires SCRATCH installation, replace path_to_scratch with own path to script # See the ssbio wiki for more information: https://github.com/SBRG/ssbio/wiki/Software-Installations
# Delete rows where 'Reason for rejection' is X1 or X8.
### Fit Your Linear Model And Obtain the Results
# We'll hold out 10% of our data for validation and leave 90% for training
# Drop one row with the duplicate user_id
#Loading JSON data into pandas dataframe  #Save dataframe to csv for visual assessment
# hyperparameter combinations test
# our main index is the 'Mail' column
#change the directory
# You can then convert the np array to a list
# tweets by location
#Show the results; water_year and water_year2 are the same...
#p-value calculation
# Random sample of 5 twitter entries
#initial visual assessment 
# Show our results
#Sorting by values
# We can resample the 15 minute data to make it hourly # this resamples a datetime index to hourly and averages each value within the hour 
# df_session_dummies.head(n=2) # df_session_dummies_drop=df_session_dummies.drop(['created_at','value',],1)
# KNN model 
### Fit Your Linear Model And Obtain the Results
# Ratio of the number of user of the new landing_page to the total number of landing_pages viewed
# R == 'regular season'
# I can change the level of the logger.
# Get filenames
## Transaction: Title
#the below amount of times that they do match up #this will take away the total from the above match
# Calculate number of unique users in dataset
# Delete all nodes
# Get the unique number of users in the dataset # Print out the unique number of users
# Only co-occurence > 1
#survivors
# Use of LIMIT and OFFSET: # "offset 5" truncated 1st 5 records and shows next 3; 6,7,8
# duplicate user_id entries
# Checking if all the nulls have been filled or dropped
# read all data ignore bad lines
#provide a summary of your data. 
#pd.read_csv('autos.csv', sep='|', names=m_cols , encoding='latin-1')
# Calculate z-score and p-value
#Sort INT by Contact_ID and Interaction Name
#Setting the date time as the index allows time slicing
# Retrieve the first [0] record and store it in the latest date variable # Use strptime function to store the formatted date in the format_latest_date variable. We will use this in our code later
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Load and peek at the Google data.
# note: this was necessary because the by parameter of the box plot only accepts sequences, i.e. tuples and other such things # if you try to use the original complaint column it returns an error saying that "by" only accepts sequences, not groupby sequences 
#Let's see if we can scrape the urls from 'Top 5 Data Science and Machine Learning Course for Programmers' by DZone
# make new column for year. Then drop all rows from 2018
# Show results
# Import Data and Read CSV file # Initial inspection
# Distributions of mean sean level globally and per hemisphere?
#'Minneapolis': '8e9665cec9370f0f'
#remove rows where control is not aligned with old_page and store it as df2 #remove rows where treatment is not aligned with new_page in df2
# merge with council data #df7 = pd.merge(df6,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE',suffixes=('_created','_closed'))
# select all the non-numeric columns # get the list of all the non-numeric columns
# show overall statistics of the dataframe
# check the df3 with the new columns added
#writing year month and size in MB into CSV
# Save and show plot
#Displaying repeated user_id 
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Save the query results as a Pandas DataFrame and set the index to the date column
#Here is the join using Python
# inspect the dataframes
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #14. Join users to transactions, keeping only rows from transactions and users that match via UserID (inner join) #Use Join and display only matched records.
# check shape
# print a single tweet json for reference #print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
# Ajustando o modelo
# You can build a DataFrame from a dictionary:
#removing the line from data frame by specyfing the index value under labels.
#home_dest.value_counts()
# Creating another lamba function to find spaces and split the each value
# Simulate n_new transactions with a convert rate of p_new by using random.choice() to fill the array with 1s or 0s
# as it is observed that the data is verys skewed and maybe has outliers, we will remove outliers and then plot # keep only the ones that are within +3 to -3 standard deviations in the column 'duration' using pandas indexing
# Retrieve NASA page with the requests module #Display information from page being scraped
# Use a colour-blind friendly colormap, "Paired".
#put data file 3 into a pandas df #parent node is 'page' #print first 5 rows
# returns no of Null values in each column
#Cuts instances with more than 10000.
# Create a new dataframe for the (It does so by testing true/false on date equaling 2018-05-31)
# 6375 unique names. That is quite a few names. Let's generalize into a binary feature called has_name instead: # per Eric's feedback, use pandas functionality instead and do this more efficiently vs pure python list comps: # Now, remove the Name as we don't actually use this subsequently.
#Export to CSV:
# print(len(old_page_converted)) 
# A number of ways this can be done... lambda function, pd.get_dummies for example
# Inspect master size
# prototype only: save the light curve to disk
#extract the whole rating, e.g 12.75/10  #extract the numerator, e.g 12.75 #update column values in the data
# weekday name
# we simplify the purchase history to only include user_id & product_id
#6875 Outlier, Depth = 675
# List the deployments
# create a set of users # randomly sample 20,000 users
# group by ticket
# use this cell for scratch work # consider using groupby or value_counts() on the 'name' or 'business_id'
# cannot pass on non-date-like strings
# Converting date to a DateTime Format
# Converting to list
# Import the csv file with historical neo data and other variables
#Show the last 5 rows 
#import libraries to display equations within the notebook
# To find value of n_old
# Create a regressor model # Calculate results
# 'supercontig' types columns represent unassembled sequences # (incomplete/total) * 100
#Eliminamos la columna de indice
# Split data into train and test datasets
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Group by languages
# Read dataset # Print a few lines
# Save the query results as a Pandas DataFrame and set the index to the date column
# posts[posts['PostTypeId'] == 2].groupby('CreationDate')['Body'].count().plot(alpha = 0.3, color = 'r') # posts[posts['PostTypeId'] == 1].groupby('CreationDate')['Body'].count().plot(alpha = 0.3, color = 'g')
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #10. Get the subset rows 11, 24, 37 #Pass index values using ISIN fuction to subset the values
# create_database('mysql://root:root@localhost:3306/stocks_nse')
# Option 1 is the generalized solution to reorder the index levels # Note: We're not making an inplace change in this cell, #       but it's worth noting that this method doesn't have an inplace parameter.
# shift forward by 5 hours
# new_page/unique user_id
#get all the things you can run on this object #dir(festivals.index)
# alternatively
#printing first few lines of the dataframe
#The decision rule is: Reject H0 if Z > 1.645.
#dropping columns #cust_data.drop('monthly_savings', inplace=True, axis=1) # it modify the existing data
# Compute p-value
# Plot histogram to check distribution
#So it looks like we can go ahead and delete the duplicates (it's safe)
# search vs. booking
# Display summary statistics for numerical and categorical columns.
# Drop all dupclicates so only one instance of each song remains
# Return the total number of tobs for USC00519281
"""$ Check results$ """$
# The ward, district, and community area nans go together
# Victim normal plot
# Can I select by a time?
# I create a new column with 2 possible values weekend or weekday
# ejemplo, La hora actual del servidor
# Display the row's columns and data in dictionary format (first row)
#changing the column name of dataframe df from indicator to indicator_id using the rename method
### total number of posts and unique users mentioning Kate Spade
# Removing duplicate record & selecting the first one with timestamp as "2017-01-09 05:37:58.781806"
# Check that we don't have any null/nan at this point # Make sure they have identical hugo gene indexes
#drop the variable column to leave just the stages #rename value column stages
# Drop one column to get full rank
#find the latest data point to determine starting date for the following query
#this would get the unique values so that we can create the cols
# Instantiate the model by using sm.Logit # Target variable is 'converted' # Predictors: using the 'intercept' column and three predictor column 'ab_page_new_page', 'country_UK', 'country_US'
## The QTradableStocksUS universe generally contains a greater number of assets than previous iterations of the tradable universe. ## The resulting summary table displays the mean, std, min-max of daily median in addition to number of assets in this universe:
## monthly medians
#Seems much better now
# load data from CSV
#Baltimore
#ANOVA F Test
# printing first five rows of the data frame #df.head()  # 5 is the default value. we can ask for any number as shown below
# add prefix to column names
# Display confusion matrix
#This is Kolmogorov Smirnoff for comparing tripduration across day and night BUT FOR A SAMPLE
# create a dataframe using cols and correlations
###### read file ##########
c.execute('''SELECT * FROM artist_db''')$
###this value tells us our critical value at 95 confidence is
#converting timestamp column to datetime datatype
# Simulates one test for the average of n_new binomial flips with a convert rate of p_null:
# Loading a JSON file
# The p-value for a one-sided hypothesis test # Reviewed and corrected as per reviewer comments
#altering the dataset with the required conditions
#Youtube videos by country
#now drop redundant 'doggo', floofer','pupper','puppo' columns #twitter_Archive.drop(['doggo','floofer','pupper','puppo'], axis=1,inplace=True) # inspect the dataframe
#Number of rows in dataset
## Libraries 
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
# Drop price outliers from the DataFrame
# Save file to csv
# We display the updated dataframe with the new column:
# calculate the conversion rate for treatment group.
#consider the data excluding any one of the duplicated ids
# Reading dataset after ignoring initial space by skipinitialspace=True
# Extract spatial extent from attributes
# URL of page to be scraped # Retrieve page with the requests module # Create BeautifulSoup object; parse with 'lxml'
# Creaate a new column that is product of both dimensions # print a few rows & columns
#users = pd.read_json("C:/users/Jared Chung/Desktop/D-Matrix/users.json") #Laptop
# join two to generate feature data for each business_id
# Verifying that our new submission has the correct columns
# open the file just to check it has been downloaded ok
#Loading the dataset in new dataframe df2
#Simulates Nold with Pold using np.random.binomial function and stores the values in variable old_page_converted
#Experiment Conversion Rate
#select bea.EPISODE_NO, beat.* from boepisodeaccount bea, boepisodeaccounttxn beat where bea.BOEPISODEACCOUNT_ID = beat.BOEPISODEACCOUNT_ID order by bea.EPISODE_NO, beat.TXN_DATETIME desc;
# Exclude accounts created 1 week before the march
#Fill the NaN values in the 'Returned' column with the value 'No'
# Move 'Date' back to the index
# A:
#Laredo': '4fd63188b772fc62'
#true == churned, false == active #bug- will return user who 'canceled' and 'retrialed' same day which is latest in scn timeline
# Load and peek at your data. Change the file name as needed. 
#additional features from goal, pledge and backers columns #The above field will be used to compute another metric # In backers column, impute 0 with 1 to prevent undefined division.
# Verify all rows were removed 
#s_etf.plot(ax=ax, legend='right')
# Concatenate first two and last two rows
# Random forest score:
# create a sub-dataframe that only included Friday and Monday data
# ['neutral'].plot(subplots=True)
# Creating a timedelta
# check if shapes match
### START CODE HERE ### ### END CODE HERE ###
# Double Check all of the correct rows were removed for old pages - this should be 0
# df.head(2)
# See the number of stars for each review
#charts.plot(df, stock=True, show='inline')
# Convert Start/End Coordinates into lists
# filter out people who joined prior to date
### Fit Your Linear Model And Obtain the Results
# Find values less than the lower control limit.
# There are more homeless people in the city in the summer
# 
# gather tweets about something everyone likes: #puppy #Get the first 2000 items based on the search query
# Cuidado, para cada columna son las filas con elementos
#Excludes the "Cover_Type" column from the features provided
# read the data and tell pandas the date column should be # a date in the resulting DataFrame
# Get MD_ID from Booth_ID
# Step 3: Plot pivot table data ## Here we see the data doesn't have the two peaks, but instead just one...This is what we need to fix
# Convert columns from string to datetime
# combination of education and purpose
# get max probability column name, this is final prediction for test set
# Set the limits of the x axis # Set the limits of the y axis
#load into products dataframe#load in 
# using random forest classifier
# Create the Estimator
### Create the necessary dummy variables
#  setting a Target variable
#voters.loc[voters.LastVoted.notnull(),:]   # uncomment to see these 6 columns
# Now, call the Quandl API and pull out a small sample of the data (only 5 days) to get a glimpse # into the JSON structure that will be return
#calculate the number of users landed and new_page
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# Check number of duplicates
# Here is the commnd to do it.  # But the parameter `index=False` will prevent  # the index column from being exported to the csv file.
# total experiment time
# Use Pandas to calcualte the summary statistics for the precipitation data
#Convert to numpy array and calculate the p-value
#df_1.to_csv('Xarr.csv', sep=',')
#self outer join of transaction table to get all possible pair of transaction by users 
# df.columns.values
#Subset data to alternate temperature columns #Check if any secondary temp sensors exist
# There are very high ratings, might want to address this manualy, as it will skew the data
# Store tables
# View the shape of the datagrame to determine there are 294478 rows
"""$ Dates go until 2018 and are mostly unique.$ """$
#kick_data = k_var_state.drop(['goal', 'static_usd_rate','currency','category'], axis=1)
### final list of chinese_vessels
#Testing on a sample text
#giving the values and index itelf a label
# loading the csv file into a dataframe
# Distribution of Breeds:
# 0.9999999383005862 # Tells us how significant our z-score is
# Create new Data Frame
# merge the dataframes into one big one
# Export
#df_insta.dropna()
#Reading the files as stated in the Problem Statement
# the conversion rate is 0.11959
# Find unique users # Check for not unique users
# What are the most active stations? List the stations and the counts in descending order.
# In order to calculate tip as a percentage of the total fare, we need to divide each row's "Tip_amount" with the  # "Fare_amount" data value. Thus, we need to check the "Fare_amount" values to make sure if any of it equals zero (0).
# the number of people that received the old page
# find a positive 606 value in original dataset
# find historical data for 2011
#P-Value
#Mars Weather
#Bar plot for overall sentiment
# Unique operator values in stops
#Do the regression
# merge commiter and author time info
#number early subscribers
# DABNEY CODE ADDITION
# 1-2 (1) Using Request to get all the tsv data
# Array with Interval index of the weeks
#Let's get the average number of retweets and favorties per location #looks like even though we got rid of None values in userLocation, there are still empty Strings
# The mean squared error
# Determine a monthly profile to be used by cities that are not covered. # Because the above data came from PCE, non-covered cities are primarily Urban. # Take the average of the Hub cities that have annual usages > 500 kWh/month
# add column for day of week
# user_df
#creating the dummy variables
# Display progress logs on stdout
#Airbnb_df= pd.read_csv('listings.csv') #Creating a dataframe for manipulation
#df_tweet_json_clean['created_at'].astype(datetime)
# Lista de palavras para ignorar usando os pacotes do NLTK
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
# Array of 4 features # Corresponding array of 4 labels
# Find out how long the test has been going on
# load the 311 data code book
# Get station count, has been checked with measurement station count
# 0.9999999383005862 # Tells us how significant our z-score is # 1.959963984540054 # Tells us what our critical value at 95% confidence is
# %load ../solutions/lobsters_dropped.py #stories['github_username'].isna().sum()
# output to .csv file
# Print the lat/long
#Testing tvec
# 'counter' is a standard python package. not unique to DOTCE. # it's a container that keeps track of how many times equivalent values are added
# save df2 to a new csv file # also specify naming the index as date
# Shows the unique values in the country column
# Date columns was converted to Datetime in order to use it to convert the index 
#Checking the 25 rows with NaNs in location to see if there is anything that catches the eye
# Calculate the conversion rates of control group and treatment group in different countries
# This compares the stocks as if they were all starting from the same price 100
#Renames a single column. Old name on left, new name on right. #Drops the column by finding value along headers. #Removing the column moves the columns to the right of it one over to the left.
# # This shows a chart of the count of observations by month # 
#Load the voltage profiles
# Group df by source to get average polarity scores
# creates 1-hot encoded matrix with 1 at the position indicated by betas_argmax
# dataframe of positive examples
# Padding NAN's
#print(highlight(json.dumps(jcustomer_info, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter())) # remove the comment from the beginning of this line to see the full results
# critical value at 95% confidence interval (Q1)
#  ENCOURAGE re-running this cell, like playing with a kaleidoscope: #  Visually see the increased "choppiness" due to GM(2) overlay. #  Local "downtrends" are more likely. 
#A 'business_day' or 'holiday' is a date within either of the respected lists.
### Fit Your Linear Model And Obtain the Results
# create the dummy variables per the instructions
# Presents the top 5 rows
# Check that there are 500 tweets (100 per news source)
# imprimir a soma de todos os ganhadores da sena  # imprimir a soma de todos os ganhadores da quina # imprimir a soma de todos os ganhadores da quadra
# size of the new group
# aprovechamos y salvamos este nuevo set # y recargamos la informacion nuevamente
# Use Pandas to calcualte the summary statistics for the precipitation data
#read in the schedule file
# use unique method
# Use SQLAlchemy create_engine to connect to your sqlite database. # Create an engine for the hawaii.sqlite database (created in part 2) #
# accediendo a una columna por el nombre (label) y obteniendo una serie
### top 10 locations with Kate Spade mentions
# We are adding value NDF to elements where the country destination was not filled for the test data #test_users["country_destination"] = "NDF" # Merge train and test users
# Dataframe for alpha values (transparency)
# Extract unkown users' log
# how many unique authors do we have?
# saving cleaned files to CSV
# Make five reccommendations to user 1059637 who is the first among the top-10 users in term of number of playcounts # construct set of recommended artists
#userproduct which is the output of previous problem and transactions df are merged to determine quantity of each product purchased by each user
#The amount of users for the control group
# Display counts for month in chronological order
# read data and view
#non_nlp variables  # y=df_combined['subreddit'] # transform the label 
# get the last investment an investor made
#perform delete
# .dt.strftime("%m-%d-%Y") # .dt.strftime("%m-%d-%Y")
#Mars Weather
#Design a query to calculate the total number of stations.
# Create the necessary dummy variables
####75% of the observations changed price in 3:32 or less, use us '3T' (3 minutes) when resampling) 
# alternatively
# %load solutions/query.py
# Keep only top50 datasets in dict (and convert int to str)
# unemp rate for this april 2018 was 4.1
#Apply functions to create new columns in dataframe 
# Use Pandas to calcualte the summary statistics for the precipitation data
#Question 4
# there are 290,584 unique user_ids in df2
# Now we'll try "device_id" - about 700k values
# creating dataframe of matrix  and adding the confusion matrix
# index can be obtained for an OrderedDict by using, for example, d.items()[0]
# Submission dataframe
# Show first 10 records
# confirm records were dropped
# messy file
# Take a peak at the data
# train file is an extract from history file with target variable added
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
#
# checking the different sizes of commits
# Display first 5 rows from measurements table
#change date fields to date types in schedule tab #Convert all nan observations to 1/1/1900 so they can be easily  #identified and excluded
# Next for the dataframe with data from api
# Importing data and checking for top 5 rows using the header
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# converting strings to dates
# Convert date_first_booking to datetime. As seen above, many users did not book at all.
#There are 7,206,411 rows in weights
# releaseDateDataValues = pd.to_datetime(releaseDateData) # print(releaseDateDataValues)
#See correlation with actual: SHIPPING & RECEIVING, BLDG 1856
# similarly .endswith()
# create a copy
# tweet_id to string type
# dataframe of negative examples
#df2.drop(df2.index[2893],inplace=True)
# print(len(old_page_converted))  #code to check values
# Run all sequence to structure alignments
# game_winners
#now we can convert rdd_example3 to a Dataframe
# By default .head() method displays the first 5 rows.
# Copy parts table and rename columns
#Copying the datasets for cleaning purposes
#Get rid of none values for the timezone
# Number of incomplete series generated for each  complete series
# get the converting probability
#now the information is empty because i ran the command after dropping the duolicated row
# let's look at descriptive statistics of the image predictions dataframe
# drop the name column, axis=1 means axis='col', which is confusing #pd.merge(df1, df3, left_on="employee", right_on="name" ).drop(0, axis=0) #axis=0 is row
# create a period index representing # all monthly boundaries in 2013
#Find out dublicated user id
#Hialeah': '629f4a26fed69cd3'
#great no null values duplicate users?
## Read the source html code ## Close the connection 
#Create a column of flow in cubic meters per second
#Rename columns
#propiedades entre 25 y 50 metros cuadrados
# Removing rows with topics which have a total number of articles less than 25
#def drop_rows(index, df): #    tweet_text = df["Words"][index] #    df.drop(index, axis=0)
# Appending train and test to get full dataset for cross-validation
# data_air_visit_data['air_store_id'].drop_duplicates().count() # data_hpg_reserve.info
#Clean Text
# prob new page receivers
### top mention date for each brand
#standardizing the CIK column by padding 0's using zfill function to make it a 10 digit string which is unique for every company
# summary results
# norm.cdf(z_score) tells us how significant our z-score is # norm.ppf(1-(0.05/2)) tells us what our critical value at 95% confidence is
# First data set is contained in one .csv file
#save final new dataframe to disk and free memory
#coefficient between delay time and crime instance
# from tensorforce import Configuration
#now use Geopandas to parse the locations into the london census output areas #now trim the londonGeo stuff to may it smaller... hopefully this will make any op faster
#now drop redundant timestamp column # inspect the dataframe
# Design a query to calculate the total number of stations.
# Group by day
#Example2: Import File from URL
# Get the average Compound sentiment
### This is data from wikipedia3 archive in data folder
# client.repository.delete(experiment_uid)
#Find out n old
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# CHECKING TOTALS # bands.sum(axis=0).sum()
# Calculate the difference 
# SampleBy returns a stratified sample without replacement based on the fraction given on each stratum
## Solar Flare 2017-09-10 
    '''$     Make sure github links point to downloadable files.$     '''$
# So there are roughly 85k word in there. What are some positive words?
#conn = sqlite3.connect("/tmp/coindesk.sqlite3")
# check inserted records
# 1-2 (4) read tsv file using Pandas
#This should work but not getting it to work at the moment, for now I'll just do each step manually #table1['country']= table1['ip_address'].map(lambda x: table2[table2['lower_bound_ip_address']== low[low < x].min()])
#Import plot libraries
#Tells us what our critical value at 95% confidence is (one-tailed)
# pipeline / fit / score for lr & tvec
### Create the necessary dummy variables for landing page
# convert s1 from US/Eastern to US/Pacific
# drop the name column, axis=1 means axis='col', which is confusing
#propiedades entre 50 y 75 metros cuadrados
#Change DATE from a string to a date
#display twitter archive
# replacing all 'N,0"' values in the country column with 'NZERO' to avoid discrepancies while one hot encoding
# favorite table # for row in table_rows: #     print(row.text)
#Import the dataset as a pandas dataframe. #Imports Excel file. Pandas tries to retain the format of the sheet.
# - GICS
# Check everything  has worked
# Affirmative. Drop the county column.
# drop all duplicates across ALL columns
# export latest elms_all
# note that these actions occurred on different days at different times # not 100% certain that 'unique user' definition should include same user on different day
#Remove Duplicates (don't keep any posts which appear more than once)
##The results from the above expression can then be applied to the [] operator of the dataframe which then ##results in only the rows where the expression evaluated to True being returned
#actual difference from ab_data.csv
#test_df["description_score"] = test_df['description'].apply(process) #test_score = test_df["description_score"].to_frame() #test_score.to_csv("data/test_score.csv")
# Set index to Date # Apply the count function # Seeing what DummyDataframe look like
# sanity check out environment is working
#Convert rate irrespective of the landing page
# check score.py and main.py
# Create a new dataframe for the Airport ID (It does so by testing true/false on date equaling 2018-05-31)
#As per assumption above
#Probability of conversion, given inclusion in the treatment group.
#In Dataframes we can reference the columns names for example:
# Fitting K-means with 4 clusters
#shows.to_pickle("ismyshowcancelled_raw_pull.pkl")
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# check for missing values
## Downloading file programmatically ##
# To know the value of country count
# correct joining
# calculate the conversion rate for control group.
#observed difference
#new page converted
# In the tweet_data dataset, Setphan and Bo are the top two dogs that have most favorite and retweets
# rebuild and display empty database
#find maximum of Sepal.Length by Species
# movie link
# start the Chrome driver
#now convert to Spark Dataframe
#Show the first 4 rows. 
#calculating the z-score where z=(x-mean)/std
# Assign the measurements class to a variable called `measurements`
# Column names
#concatenate the sentiments_df, then export it to csv, and show the completed dataframe
# The Frequency_score column should have been inferred as a numeric, so it may contain some unwanted non-numeric data
# Number of Names Used Only Once
# This graph is the right data but the x and y axis are not on the right scale. Notice the date span on the x axis # Notice the y axis labels
# calculate mean conversion rates on each day of week
# change to out_path directory
#Retrieve the title for all NASA articles
# Creating dummy variables for country column and joining them
#funding_sum
#find the number of unique users in the dataset
# Extract title text
#View files in bucket
# run the model giving the output the suffix "rootDistExp" # if user doesn't have executable file or executable file don't work on your local computer, use run_option ='docker_develop' #results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'docker_develop')
#final_member_pivot
#Now I need to index the dataframes by their datetime columns
# 14, 15, 16
#Change the x axis to get better results
# How many stations are available in this dataset?
# are all values less than 10?
# Model is very overfit since the test score is much lower than the train score 
# cleaning the vote column into two columns 'vote' and the 'ballot_type' used
# Check for null values for each series
#Test:
# Import the csv file and store it in a dataframe # Check the outcome
# Check for NaN values in CAND_NAME
# We can create a time series of tweet length like this
#number of unique projectIDs in projects
#print bthDF.loc(axis=0)[1,:][1].values #print bthDF.loc(axis=0)[3,:]
#sort by date in ascending order & also use method chaining
# take a look at the dataset
# However, non-numerical columns are simply omitted:
# create an authentication # connect to Twitter API
# calculate the distribution of values in the date_crawled, ad_created, and last_seen columns (all string columns) as percentages.
# Similarly transform the test reviews with the same fitted stfvect
# Extracts the minimum and maximum date. By the fact that PubDate is organized in format:  # YEAR-MONTH-DAY HOUR:MINUTE:SECOND, the lexicographic order corresponds to the chronological order.
# SORT AND THEN DROP non numric values
#print(os.path.abspath(os.curdir)) #print(os.path.abspath(place))
# Parse the date column as Python Date.
# plt.savefig('2-sided.tif')
#Extract the month from the datetime column
##Check the last 2 tweets in the dataset
##Create a series of 10 numbers
# Initialize reader object: df_reader # Print two chunks
# get columns by name
#load a Parquet file
#query to calculate the total number of stations.
# extract the number of closures only
# create a Python list of three feature names # use the list to select a subset of the DataFrame (X) # select the Sales column as the response (y)
# Renaming column to capture lost data
# SVC with tfidf
# Are there any outliers that would make this task difficult??
# a = 4.05 angstroms (Al fcc lattice constant) # Create cubic box (alpha, beta, gamma angles default to 90)
## Looking at results
# Replace NaN in Field4 with "None"
#load pickled dataframe
# Review CA conversion rates with new page
# Build Pie Chart
# Tells us how significant our z-score is # Tells us what our critical value with alpha 5%.
# Get the data from the database.
#will login to synapse
# make a bar chart of the categories for the merged data
# running a sqlalchemy query to get the date and tobs from  the measurement table.
# get the local "now" (date and time) # can take a time zone, but thats not demonstrated here
#calculate number of queries when landing_page is equal to new_page #print n_new
#make dictionary
# extracting the rows that have a timedelta less than zero.
# let's build a model with GPU
# Confusion Matrix
#We want to look at only one direction
# TODO: check if all 4X people actually did give more data
# the probability of an individual converting in treatment group
#### cache during development
# Import data
# find number of retweets
# Test the stored data
# Populate the pandas dataframe with our JSON file
# taking a look at how long the AB test was run - 3 weeks which is enough time \ # to overcome 'novelty effect' and or 'change aversion'
#Plot Histogram of residuals
# Create a dataframe of all the rows with the either a combination of 'treatment' and 'new_page' or 'control' and 'old_page.
#Review resulting dataframe without the additional columns
#============================================================================== #==============================================================================
# Replace row 296's zipcode with 98011
#[i for i in my_list if '91' not in i and '18' not in i]
# pull a subset of data for testing
# how many are `Literacy & Language`?
#load data into pandas dataframe df #display first two rows from the dataframe df
# Amount invested in BTC in Coinbase
# Converting to Timestamps
# merge weather and 311 #df5 = pd.merge(df4,df2,how='left',left_on='Date Closed',right_on='date',suffixes=('_created','_closed'))
# At this point it's clear how to use mrec and what it does under the hood, but let's do # one more thing and use SLIM. # First, find regularization constants:
# dropping columns '[', ']', and ','
# save the model
# create dataframe of matrix and add to variable conmat
# add active_user column, and marketing_source column
# For data without stage info, # add 'stage column and drop 4 columns ('doggo','floofer','pupper','puppo')
#group by user and product id. Take sum of quantity #reset index to flatten the df
# shift forward one business day
#holdout set = 2016
# Code # Recreate dataset twdf.clean with excluding tweets that could not be rad
#propiedades entre 100 y 125 metros cuadrados
# Set the start and end date of the trip
# Let's see how data distributed along with year
#Calculating nnew
# Count mentions only
# Ran the model with Recursive Feature Elimination in an attempt to find an optimal number of features to use.
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# output.coalesce(2).write.parquet("C:\\s3\\20170503_jsonl\\output.parquet")
# Now remove the sample rows from the main dataset
# This merge results in duplicate columns, marked by _x and _y suffixes:
# What is the minimum value?
# Show model training parameters
# !pip install scrapy
# describe num_words field
# show general information and statistics of complete dataset
### Create the necessary dummy variables
# your code
# Histogram with the range of 3 ~ 7 miles
# Resample and compute the monthly totals for the popular community centers
# I was receiving the error: # AttributeError: module 'scipy.stats' has no attribute 'chisqprob' # so I found this workaroud from the internet, please ignore this cell
#Dictionary of value averaged by half-year
# take subset to develop algorithm with it 
#### Create a column that has a number that symbolizes whether a row is close or not to the 'real' pick #### We'll do this first for Top McMurray and then top Paleozoic, which is basically base McMurray #### Top paleozoic version
# extracting environment variable using os.environ.get
#Check and plot the 50 first predictions
#### Define #### # Identify and remove erroneous dog names # #### Code ####
#mean conversion rate by country and landing_page -  #checking for possible interactions (whether the influence of landing_page #is different for the countries)
# ffill null values
# Check if new column names is assigned
# Checking the recency for every user # recency is measurement of the time elapsed since the last purchase was made by a customer
#number of rows
# What are the most expensive wine varieties?  # Create a `DataFrame` whose index is wine varieties and whose values are columns with the `min` and the `max`  # price of wines of this variety. Sort in descending order based on `min` first, `max` second.
# convert notebook to HTML for universal sharing ...
### Fit Your Linear Model And Obtain the Results
# identify nulls
#Resets the table and reformats into a more desireable dataframe structure.
# We can view all of the classes that automap found #Base.classes.keys()
# write your code here
# Add row with number of missing values
# The repeated user_id
# DEFINE: remove retweets # CODE:
# opening and closing time for odds added to each row
# Create a `Series` from entries in the `price` column, but convert the entries to strings.  # Hint: strings are `str` in native Python.
# Testing api authentication
# PandaSQL does not like column names with spaces. So we will rename (some of) them.
### Create the necessary dummy variables
#now getting the odds ratio by dividing the odds of conversion of the treatment group by  # the odds of conversion of the control group, this value indicates a tiny difference  #between p_new and p_old
## Count the amount of sources, and display  the first 5
# clean up nulls
# successful campaigns (sorted by counts)
# went through the Quandl documentation and determined that I needed to find the Franfurt Stock Exchange dataset code # as well as add start_date and end_date parameters to make sure the API request via HTTP GET was done properly
# Create a csv backup as well: # Note, code works, but is commented out so it doesn't get re-run accidentally
# approximation to avoid a second matrix # multiply the maxarg by factor, sum across and divide by same factor # all matrix elements lower than argmax will contribute very little
#start random forest
#merge with itself? or drop -- are these aliases or distinct babies
# Check if there are any 'East' terms in street.
#df_countries.info() #df3 = df2.set_index('user_id').join(df_countries.set_index('user_id'))
#compute the standard deviation of the sampling distribution of the difference in means
#Determine the top five schools that have the highest percent of students passing math and reading (overall passing rate)
# Now let's find the date
# If there are Nan values we can use inputer  
#Collecting data from the FSE for AFX_X for the year 2017
# What are the most active stations? # List the stations and the counts in descending order.
# plot the fixations as a heatmap # TODO annotation how many fixations from how many pictures are used for each eyetracker
# Actual difference #Proportion 
### Create the necessary dummy variables
# Use Pandas to calcualte the summary statistics for the precipitation data
# kd915 is the full dataset, containing projects that have been submitted multiple times and  # have live, successful, failed, suspended projects. # Combining tables to look for overlaps
# Original Age Distribution
#remove all cases from dataframe except those with top 200 job titles 
# true flag (T/F): negative class # predicted flag (T/F): positive class
# Check for missing values?
#'Washington': '01fbe706f872cb32'
# to get the last 12 months of data, last date - 365
# use inspector to get column names/types for station table
#Drop duplicated user_id
#create ab_page column
# Convert data type as 'category'
# Conversion rate GIVEN an individual was in "treatment" group
# now I can convert the column "Measurement_date" to a DateTime form, instead of a string form.
# DataViz libs
#Good, each stations is only listed once
#Utilizing the z test to get the z score and p value
# really, answer come so quick
# load the data to a .csv
#Events per day
# applying subject_count function to `subject_count_tmp`
# dataframe with just the most retweeted tweet for each week # dataframe with the number of tweets for each # dataframe with averages
### Dataframe with details on creator of the project  ### Grab 'name' (creator/company name(s)),  and 'is_registered'
# reflect an existing database into a new model # reflect the tables
## setup Checkpoint at each epoch, #keras #callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1) # does not give me a model
# Run Mood's test for male vs female
# have a peak at data that occurred before 2018-04-01 23:00
# To convert from a dataframe to a mutliindex Series
# find historical data for 2002
## make an array of midnight datetimes for slicing dataframe by days
## remove columns we don't need for model
# droping those rows where all the rows have na
#reset the index to the date
# dropping values that match conditions in the query function
#                                                    (taxi_hourly_df.index < "2016-07-01") &
# Remove outliers if needed, replot and see if the moments changed significantly. # Checking how many values I have above 100% tipPC
#get only rounds from companies in cleaned up orgs
# Convert date in 'Timestamp' column from strftime to datetime
# Displaying the z_score and p_value
# keep the empty lines for now # if len(line) == 0: # continue
# Load the weights from disk for our model object
#gbm_predictions.loc[ada_predictions['Predictions'] == 1]
#As we can say that rating 1 ,2,3,4,5 are skewed towards right now we will se about outliers using box plots #as we can say there are outliers so text length as may not be as a best feature for predicting
# checking for duplicate user_id.
# Get rid of the duplicate entry
# Create your connection.
#Click here and press Shift+Enter
# Using the station id from the previous query, calculate the lowest temperature recorded, highest temperature recorded, and average temperature most active station?
#firebase.patch("Exhibitions/-LFlR_PhbP2eWNCGPZeu",new_data)
### Create the necessary dummy variables
# Let's plot this instead
# Reset index
# Check if there are any 'South/Southeast/Southwest' terms in street.
# how many rows have "x" for race
# Export to CSV
# Grabs the last date entry in the data table
# Convert date from object to date
# concatenate tc physcial and electronic dataframes into one
# however the slice object isn't ideal - we can use pandas IndexSlice to perform  # in a more compact way # lets perform the same query as above with our new slicer
# Dataframes have a method, groupby(), that takes a column name be be the grouping key
#4. date ===> month, to see monthly 
####STEP 1 # Dependencies # URL of page to be scraped
# get 5000 random ids
# Find z-score and p-value
# view vegetation for June 6th 2016
# take a peak at the data
# Distances table
#this will take a while
################################################## # Load   ##################################################
#Cleaner display
#check for any null values in the trimmed dataset 
# converting back to an h2o frame
# http://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.proportions_ztest.html
# bb type
#Store original primary sensor data for later comparrison #Identify if primary sensor is passive or aspirated
#try with a csv
# because pandas is built on numpy, series have numpy functions
#Print first 2 rows
# print some configuration details for future replicability. #hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE
# Prepare data
#check they have joined properly
# 62 of the 65 monitored Congress members are found to be in the same group as their co-party members # 3 congresspeople not in the same group as their co-party members are circled
# Create "game ID" based on index.
# bob's shopping cart
# can use lambda function also
#Example 6: Load a txt file while specifying column names
# Check null value exists or not
# Get the keys to understand the data structure and how data were nested.
# Use Pandas to calcualte the summary statistics for the precipitation data
# Create the matrix of features that we will predict on
#Retieve the column list for the dataframe Users_first_tran created at previous step
# Double Check all of the correct rows were removed - this should be 0
# Save references to each table
## AVERAGE TIME TAKEN TO DEAL WITH A COMPLAINT : 
# Use Pandas to print the summary statistics for the precipitation data.
# Getting basic general info from the three data sets.  # Assessing WeRateDogs Twitter archive
# retrieve life expectancy at birth for all countries # from 1980 to 2014
#print list(label_encoder.inverse_transform([0,1])) # blurb_count = 30, backer_count = 50, goal_USD = 1100
# See https://www.mediawiki.org/wiki/Manual:Namespace#Built-in_namespaces
#tweetsOverall.reset_index(drop=False, inplace=True)
#Access state lookup table
# give me today's date
### getting the odds by exponentiating coef
#import country data and join to existing dataframe
# Calculate the observed difference between the new and old page:
#Lexington-Fayette': '00ae272d6d0d28fe'
# default is freq='D' for date_range
# Let's train popularity model
# get the third and fourth indexes of an Index object
# we can change frequecy
# Inspect the columns with NaN before Aggregating
## predict
#Mean and standar deviation returns
# replacing 'nan' with '' for improved legibility
# df.head().T.head(40)
# execute the info() function
# Explore the data - All the Funding Type counts
# merge ships and locations #df_l = pd.merge(df_l,df_s, left_on='ship_name', right_on='name') ## end up with callsign_x/y   and major_cruise_line_x/y (MORE CLEANUP)
##parse_dates parameter of the pd.read_csv() function guides pandas on how to convert data directly into a pandas date object
#alias the datacube to something more wieldy and pass a string for reporting purposes
#Read the first 5 records
# get one venue's information
# Store our sentiment analysis in a dataframe
#take a look to the 5 first rows
# print(host)
#### sort by value
#new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) #len(new_page_converted)
#exponentiate treatment
### Fit Your Linear Model And Obtain the Results #df['intercept'] = 1
# Display confusion matrix for the binary classification # example by using our helper function
#since we expect ~ 57% precision (true positive rate) how many wells do we expect to ACTUALLY fail this year?
# Nomes Errados
# Number of null values in each column
#now all the null values have been removed. great ! now i can test with a dataframe.
# Create a dataframe than contains only the trip distance values
# To make forecast for next 90 days
#importing seaborn and matplot lib for data visualization 
# Replace all null content_size values with 0. # Ensure that there are no nulls left.
#plt.xlabel('') #plt.ylabel('')
# check current directory
# Create root universe
# regular expression aka wildcard pattern
# A:
# get summary output
# Reflect Database into ORM classes #do some reflection and export our schema
# generate a list of all customerIDs
# the accuracy hasn't changed at all. # then, we are going to decide the best combination of the model
# outer join on key column
# Variables:
#filtering dataframe by indexes that are in the top 10 most retweets
# Counts the number of users that landed on the new page, and then divides by the number of new users to give the probability # of a user landing on the new page. Around half!
#autheticate
#Import test CSV (from Kaggle competition) as pandas dataframe
# Plotting one of the users...but I have 5 total users
# Get marketdata from database...
# create weekday vs weekend column for icu_intime 
# Downloading the data
# put your code here # ------------------
# creating authentication  # connecting to the Twitter API using the above authentication
# create new timestamp column in the bird_data, pass the data form timestamps list and match its index columns 
#Sets the columns as WEEK #Sets index as DAY
#Export the dataframe to a csv file. 
# Get movies not rated by new user # keep just those not on the ID list # Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies
# where the actual difference lies on null distribution
#number of unique converted users #Proportion of unique users converted
#additional cell: just checking to see if this duplicated row is omitted
#Tells us how significant our z-score is #Tells us what our critical value at 95% confidence 
# Perform a bsic analysis of variance (ANOVA). # C(x) refers to x as a categorical variable.
# masked["RT"] = [1 if "RT" in ele else 0 for ele in masked["text"]]
# Create a new dataframe by dropping rows with NA data
# get the one_way column to show if the search is one way 
#put data file 2 into a pandas df #parent node is 'page' #print first 5 rows
# What is the best wine I can buy for a given amount of money? Create a `Series` whose index  # is wine prices and whose values is the maximum number of points a wine costing that much was given in a review.  # Sort the valeus by price, ascending (so that `4.0` dollars is at the top and `3300.0` dollars is at the bottom).
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #15. Join users to transactions, displaying all matching rows AND all non-matching rows (full outer join) #Use outer Join and display both  matched and non matching records from user and tansactions table
# generate train/test data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# It seems younger people mostly performed activity.
# Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date # inplace = True to use date as the index
#Number of users (already known...)
# To flatten after combined everything. 
# You also can use the index to display any given time range.
# Load data into dataframe
#removing irrelevant columns for model
# Probability of treatment group converting
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# the number of features has been reduced significantly, now only 86 features
# for porability 
# List of indices of the rows with price outliers
# So the fact that the top user is 8 events tells me that when we break it down by day its gonna be very sad. # lets see what the distribution looks likes.
# Push the sentiments DataFrame to a new CSV file for each vader and Watson
# most common hashtags
# Filter down to the set with valid licenses #df_valid = pd.read_csv('/home/bmcfee/data/cc_tracks.csv.gz', usecols=[0])['track_id']
#getting dummies for custom
# Load the necessary data for the module
# Generate a dataframe from the value counts of number of complaints on each day: 
# Clean the dataframe using clean_dataset() function
## Dropping the tables that are still categorical because they're no use to us.
# Plot histogram to check the distribution
# Display a list of created buckets.
#'Nashville': '00ab941b685334e3'
# Create timeseries from scenario results in preperation to post processing
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
# set index of series c 
# Get the times the new_page and treatment don't line up
# We can view all of the classes (i.e. tables) that automap found
# import Gaussian Naive Bayes
# check if duplicates in user_id # we know that one user id is repeated due to difference between #userids and #unique ids
# we can confirm by checking unique values of user ids
#ADF(resid_6203.values)
#### dumping dict of data frame to pickle file
#Read the Prometheus JSON BZip data
# cvec_3 top 10 words
#Standard Deviation
# remove '$' sign from the three columns with them to change them from objects to floats.
# check and print players table shape
# Read the data  # View the top five rows
# entfernt Spalte "created_at"
#np.sqrt(-cross_val_score(model, X_tr, y_tr, scoring = "mean_squared_error"))
#Importing rating dataset from Data folder
# Number of leads per month per source
# create a DataFrame of the test dataset, so we can compare our predictions to the actual values
#Saving the dataset in a file
# the finally logistic regression model:
# Create logit_countries object # Fit
# Set file names for train and test data
#checking contents
# look at the ingredients string
# map df_twitter_extract to df_twitter
# Checking the numbers of polarity, subjectivity and Sentiments of the first 5 tweet.
## Accuracy is the proportion of true positives and true negatives vs false positives and false negatives. # In your confusin matrix, it's top-left + bottom-right / total.
# parse the sentence
#new_page_converted = np.random.choice(df2['converted'], 145310, replace = False)
# Select desired columns
# Create a Soup i.e  ## A new Column that combines Category, EventName and Location ## for applying TF-IDF
#query tables to get count of daily report, all temp data is complete for each record, so the count#query t  #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
# we only need to "instantiate" once.  Then we can call mine_user_tweets as much as we want.
# poo
#nota Hago esto para renombrar place_with_parent_names' con 'state_name' !!
# Remove the breed prection column that are no longer needed
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# calculating or setting the year with the most commits to Linux
#Solo me quedo con CAP FED y GBA
# to get the last 12 months of data, last date - 365
#statistics_table.to_csv('compiled_data/statistics_table', index=False)
#Getting rid of rows with nans in location field
#Join ab dataset with country dataset
#Probability of converting regardless of the page
#Call prediction on JSON data
#printing first few lines after the cleaning process
# The base Dataframe's beginning.
# Note: Count can't be greater than 200
# Convert dictionary to a DataFrame
# high = 0, low = 1, medium = 2
# corr.to_csv("correlationmatrix.csv", sep='\t', encoding='utf-8')
# Create a pandas dataframe as follows: # Display the first 10 elements of the dataframe
# Get the convert rate for the old page
#import matplotlib
# check y
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Predict_SVM
# UniProt mapping
# delete duplicate record  # we choose one with timestamp as "2017-01-09 05:37:58.781806"
# Joining two data frames
# write the numeric features back to a table
# these are the columns we'll base out calculations on.
# the following df will be imported to r
# Use pandas to create a range of dates
# Group by # Group data to get different types of information
#df_dup
# Read the data
#The final recommendation table
# Run Kruskal-Wallis test for female vs unknown
# What are the most active stations? # List the stations and the counts in descending order.
# check added features
#origine incident is not independant from target
#create intercept column
#fit a regression model including country variables
# Drop all rows with missing information
#print(data[:5])
# How many stations are available in this dataset? # Perform a query to retrieve the number os precipitation stations
# add intercept
# item_lookup = shopify_data_simple[['child_sku', 'child_name']].drop_duplicates()
#Convert to seconds
# reindex to inlcude all claendar days, using forward fill #fin_r = fin_r.reindex(r_top10_mat.index, method='ffill')
# Retrieving the last 12 months of data, last date - 365
#print(dataframe.head())
# add our new function to the class # test our function on the initialized instance
#Return a json list of stations from the dataset.
# check for NaN-values in dataframe and show additional information
# cross-validate the entire pipeline
#finding the important features
# We drop any rows with NaN values
#User_id 773192 is repeated
#Join events and groups
# Create Geometry and set root Universe # Export to "geometry.xml"
# Put the data into HDFS - adjust path or filename as needed
#sort by nytimes in order to create different color scatter plot
#listings['zipcode'].value_counts().plot(kind = 'bar')
#Make the df look nice.  
# run the model giving the output the suffix "lumpedTopmodel_docker_develop" and get "results_lumpedTopmodel" object
#Show the data types of each column
# kill line breaks # coerce license date col to datetime and sort descending
# What are the most active stations? # List the stations and the counts in descending order.
# are there any values greater than 8?
## shows the correlations among all existing features
#We can load the dataframe and take a look at the 
# create period range:
# Download image.predictions.tsv file using request library
# Checking if the values are correctly cleaned. 
#This simple look up details tweet_id is incorrectly a integer.  #It should be, like the zip code example we studied in the text, a string.
#Read the entire dataset #Using date parser to convert all dates into datetime
#Re-reading in JSON data
# Top 10 words associations  TM (negatives), DC (positives) # To save the full set to file, uncomment the following line # logodds.drop_duplicates().sort_values(by=['count']).to_csv('logsodd.csv')
# How many stations are available in this dataset?
# Create a mask True/False of which stopids are shared by both dataframes # trips.loc[trips['tripid'] == 4093258]
#Check shape first
# Create directory for new data files # Generate a new data file
'''proportion of users converted$ using mean() since converted column has binary values'''$
# Create regression model to look at interaction between page and country # Fit our regression model
#using the mean to fin the proportion of users converted #check the result
# To test the slack technologies we now add a demand that we do not have yet added a technology
# return the requested page into a json object
#Prepping the data for the list
# Convert to lowercase and remove spaces in borough names
# 0.094941687240975514 # Tells us how significant our z-score is
# get column names
# URL of page to be scraped # Retrieve page with the requests module # Create BeautifulSoup object; parse with 'lxml'
# we do not use 'CA' because its proportion is too weak. 
# Write your answer here #We'll use the axis = 1 as previously to change default row behaviour to column
# write code to drop column below:
# 2. Convert the returned JSON object into a Python dictionary #this seems to easy... 
# Cast Frequency_score as integer
#converting to datetime format of pandas for age calculation 
# Read 'contries_csv' ##Create dummies for two contries CA and US using UK as a guide ##Perfofm join to create new df
# probablity under null
#ual
# prada styled info tweeted by gucci 
#Join list values
#Looking at the features # dask dataframe  : # https://github.com/dask/dask-tutorial/blob/master/07_dataframe.ipynb
# setting response and testing request
#Create a column DATETIME which is a timestamp
# Returned as a python dictionary# Return 
# Baseline accuracy for this model is percentage of low vs high in the train set
# we only need review after 2017-01-01
# Calculate the date 2 year ago from today
#df.head()
# We can safely drop all events with a 1970 start date.
# Merge the overall dataframe with the adj close start of year dataframe for YTD tracking of tickers. # Should not need to do the outer join; # , how='outer'
# The database file we create will be called hawaii.sqlite
#This is Kolmogorov Smirnoff for comparing tripduration acrross day and night
# merged1['Specialty'].isnull()
# Add city variable. All rows are given the value of the current city name # Add TransformDescr metadata field
# Probability of user converting
# doing some housekeepin on the DataFrame by dropping the 'split_location_tmp' column
#remove some noise #ulimit = np.percentile(train_test.price.values, 99)
# Create a histogram.
#load the correction factor table from the jupyter notebook `MassFlowCFFinder`
# How many stations are available in this dataset?
#view the header names for files
# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number
# Creating a dataframe to house the data
#Monthly average
# define path to save model
# Count the total number of tweets
# calaculating the p-value.
# How many stations are available in this dataset? # Print results of above count query# Print  
# create new feature : "time_online" by differences in days among lastseen and datesreated
# A:
# Assume that errors and feature are both numpy arrays of the same length (number of data points)
# get trip
# Locate a substring
# 120 date/time series with 1second resolution
# total unique ids of the new dataframe
#Show the column name and data type for the column
#map titles to edge From column
#dropping the duplicate user
#the noHandReliable dataset has 1005692 records and  #the nan set had 3600. 1005692 + 3600 = 1009292 so we're good.
# normalize the data attributes
# concat with axis=1 (non-overlapping index)
'''$ This program finds the avarage of each calendar month in series 's'.$ '''$
# reading a csv file and loading it into a dataframe # displaying first five rows of the dataframe
#member_pivot
# pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Following','Followers','TweetCount'])
#tweepy auth
# Use the session to query Measurements table and display the first 5 rows
# Removed 'shuffle'. Received the following error: TypeError: Invalid parameters passed: {'shuffle': True}  # training_data,holdout = train_test_split(lq2015_combined,shuffle=True,test_size=0.10,random_state=123) # without the shuffle attribute
q = """SELECT avg(hours_per_week) Average FROM adultData where sex='Male' and workclass='Private'"""$
#Overall statistics on the entire dataset
#proportion of the p_diffs are greater than the actual difference observed in ab_data.csv
#creating new dataset using query function
# Add intercept column # Implement dummy variable and columns # Check implementation
# Plots the null normal and the observation from our dataset:
# no output below means the conversion was successful
#Save figure
#Example 3:
### Create the necessary dummy variables
#string slicing based on first word #but it remove numbers 
# check if there are any missing retweets
### Fit Your Linear Model And Obtain the Results
# use BIC to confirm best number of AR components # plot information criteria for different orders
# Tweet_info DataFrame
# finds station activity and sorts from most active to least active.
# change data type # ARR by industry QT
# Query to calculate the total number of stations # Counting and grouping operations in SQLAlchemy
# Take a peak at the data
# No rows have missing values
# "end_year" is the last year of data we want to pull, so we loop to end_year+1
# Create pivot table
# Take a look at 'text' column that contains #.#/# 
# probability of control group
#Error: wrong way to convert price to int (numeric) from string/text (non numeric)
#Now you can write your dataframe to hyper file, which you can open with tableau directly. Then have fun with your EDA!
#how many wells that were good at time of measurement do we predict to fail now?
# Full Outer Join users to transactions (UserID)
### Fit Your Linear Model And Obtain the Results
# Here we've created a date range for business date range, i.e. it excludes weekend holidays. 
## show the distribution of label "0" and "1"
### Fit Your Linear Model And Obtain the Results
# create time series for data
# Concating the dummies to the response variable and bill text,  # we have our pre-vectorized final dataset
# Print the shape of df
#pd.merge(cust_data, cust_demo, how='left', on='ID').head(3)
# save data to CSV# save  
#Calculate the mean of the treatment group's 'converted' to get the probability of an individual converting 
def saveDataFrame(df,path):$     '''save DataFerame in csv format'''$
# Print all of the classes mapped to the Base
# !git clone https://github.com/s0yamazaki/WallClassification.git
#quick check of the unique values that we will be looping through
# Education KOL
## Fitting the logistic model ## Writing to results variable
#Most common title words 
#read in the countries.csv dataset  #merge together your datasets on the approporiate rows
#drop all rows that have any NaN values # general observation: NaNs in name_of_conference columns
# Test
# excelDF.Region.unique()
# checking for duplicate entries
# get 8-16 week forecast existing patients # keep only date in index, drop time
# what is the type of the index?
# proportion of p_diffs that are greater than p_diff(difference observed in ab_data.csv)
# Calculate the range for each operator by part combination.
# Create a dummy variable
# Convert columns to numeric values
# Check the data types of the fields in the beirut data frame
#Importing the statsmodel library and creating a logistic regression model
### Arrange multiple colums values in ascending order
# check NaNs
# countries = drive.CreateFile({'id': '18FfElioS2-jPsXGf4yIY5hPCJb6bqrps'}) # countries.GetContentFile('countries.csv')  
# Multiple calculations can be presented in DataFrame
# Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date
# fraction of volume for every coin
#we are using a sample size equal to the one of old_page in df2
# look for '&amp;'
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# In the column 'raw', extract ####.## in the strings
#train_df # all_train_df = pd.read_csv(slowdata + 'train.csv', index_col='id', parse_dates=['date'], dtype=dtypes) # train_df=all_train_df[-chunksize:]
# The 13 most liked tweets # ... Found by extracting a number from the Likes.max() number, providing a list of  # tweets with likes between that number and .max()
# add an intercept # using pd.get_dummies to convert the 'landing_page' and 'group' to 1s and 0s
# replace the CREATED_ON column with parsed dates
# Reflecting an existing database into a new model # Reflecting the tables
# rows where the treatment group lands on old_page
# Drop three columns we don't use
# plotting time delata in days bucket
print("""Average victim age: {}$         \nYoungest victim age: {} years,$         \nOldest victim age: {} years""".format(df['AgeNormed'].mean(),$
# Distributions of temperature in each month since 1880
# Read the data.
# Read and display country dataset
#============================================================================== #==============================================================================
# Unique Volumes
# Read in the countries.csv # Take a look at the top few rows of the dataset
# Import the libraries.
# Show the type of Series we selected (which is a float in this case)
# Manually train a logistic regression classifier with the training set (supervised)
# visualisations
# check inserted records
# Variable to store max of "mean_odometer_km" # Display row for brand with max
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Create list of the invalid names from name column # Replace those names with 'None'
# 4. Object attributes and indices
#dicttagger_beverages = DictionaryTagger(['C:/Users/Connor Fitzmaurice/Documents/COMP47350/DataAnalyticsProject/beverages.yml'])
# drop nulls created by lag variables (5 nulls per batter. amounts to rougbly 5k observations # in other words, 0.8% of rows lost to creating 5 PA worth of lag columns
#inspecting the data set 
#calculare the number of unique user_id 
#Compute 1000 lbs/day from MGD and TotalN
#create dummy variables for the country column
# Count of different types of column_type # new_df_left.dtypes.count() #  Almost all are float variables except few int and object
# read json file
# View data attributes
#loading countries csv file
#create a list with specified row numbers and pass that to loc function
#df.head to minimize scrolling
#Tally the number of unique sites
### Fit Your Linear Model And Obtain the Results
# np.mean() can be used to calculate the proportion of converted users
# Delete the retweets # Delete tweets with no pictures #make copy
#Select the weather parameters which affect flight status: Visibility, Temperature, Wind Speed, Precipitation
#Problem 2
#Keep the first instance
#Columns not being written to file
# NOTE: add 7 for the 7 leap years between 1989 and 2019
## this corrects the order of the dates
# write your code here
# Checking datatypes
#Put all data into dataframe sentiments_df
# To answer oldest player we call .idxmax on the "ageAtFinal" column 
#Import Random Forest libraries 
#Example 7: Import File from URL
# a series' index
#Prices wrt to the number of bedrooms # count of words present in description column
# creates a Series with values (representing temperatures) # for each date in the index
### Michael Kors related comments and reposts count per day 
#created 3 separate dataframes for urban, suburban, and rural
# Infer the schema, and register the DataFrame as a table.
#**Which Tasker has been shown the most?**
# convert id to long instead of string
# Looking at irregular registration years
# Take a look at rows with lowercase values in 'name' field and 'text' field has value 'named'
# How long the costumer using our system # {now - first puchase}
# We drop columns which give us a score for personality type
# Make system a 2x2x2 supercell of itself
# A look under the hood:
#Grouping the data based on the week value and calculating the average speed in that week
# Let's see how many years ago it started.
#how many rows do we have with nan in a given column?
# Mars Weather URL to Scrape
#exportamos a un csv
# the duplicated user id is 773192
# this dataframe was created with the 'queries.ipynb' script
# Display results
#Clipping outliers/wierd values (Conditional Imputation)
# we can use standard type __call__ see below
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #12. Join users to transactions, keeping all rows from transactions and only matching rows from users (left join) #Left Join to display all transactions with left join with Users
# save the smaller file to disk to save time
#because coef are negative, I exp them.
#CREATES THE NEW CLEANED DF2 BY CONCATINATING THE QUERIED VALUES FROM THE MAIN DATA SET
#creating and fitting the Logistic Regression model
# fit the model by using country variables and provide the summary
# your code here # a DatetimeIndex was created
# merged the df_enhanced and df_tweets # Drop the unecessary id and retweeted columns. # Merging df_breed dataset with the df_clean dataset using left join
# looking at number of subjects per ad -- going to split and use same methodology as 'location' to 'metro/state' # proof of concept
# Read the dataset # Look at the top few rows
#2 drop duplicate records
#coefficient between delay instance and crime instance
# UniProt mapping
# Define function that returns a subset of words from our candidate set of words obtained from  # the edit functions, based on whether they occur in our vocabulary dictionary WORD_COUNTS. # This gives us a list of valid words from our set of candidate words.
# Collect data for ticker AFX_X for whole year 2017
##   Creating data frames directly from CSV
## Get the Column array using a variable
# calculating number of commits # calculating number of authors # printing out the results
#create the linear regression model using the lm.fit function on the x and y training sets
#df.categories.value_counts()
#Question 2
# Capitalize gender column strings
# Creating a 'index' so that plotting is easier
#Creating new dataset df2 which meets specifications
# Design a query to calculate the total number of stations.
#Read in new vacc file with added column.  We also added in a separate date column in Excel so that we can work with other data.
# testing
#Exporting transactions with their associated orders
# FileLink(str(FLASK_PATH/'df_parent.csv'))
# Pickle the 'data' dictionary using the highest protocol available.
# Determining % of listings with possibly inaccurate values # i.e. before 1900 and after 2016
#export Data Frame into csv
#predictions.size
# download .swc and marker files
# Fill missing values- CompetitionDistance
#making a copy of the data set #checking the number of missing values in each column
# Initialise with a dictionary. #
#Verificamos los nombres de las columnas
# Assigns the close column from the dataframe to the series variable, close_series # prints the first five rows in the series
# peak at events
#check for missing values
#filter elements in transaction table which have UserID present in transactions table but not registered in users table
# Check centers (means) of features for each cluster
# The mean of lengths
# 0.9999999383005862 # Tells us how significant our z-score is # 1.959963984540054 # Tells us what our critical value at 95% confidence is
#Determine number of students with a passing reading score (assume that a passing reading score is >69)
#(Train,test)-store (checking if there was any unmatched value in right_t) ##Note: Level of store table is "Store"
# What we've done above
#Find number of unique users in the dataset
# Total Nitrogen data at this site
#Display head of df_schools
# create temp helper df
# deleting the duplicate record with the oldest data
#getting the summary of the logistic regression model
# Scalar ranges can be set with a simple tuple: # Or more complex ranges can be set using one the of the ranges objects:
# Train it over a desired number of episodes and analyze scores # Note: This cell can be run multiple times, and scores will get accumulated
# 300 X 300
#this weather station started in 2013
# subset to included participants  ## there are duplicates, weird
# removing the tweets without images
# TASK C ANSWER CHECK
# I created a csv with: # a binary for whether a day is an offical DC gov workday (per their website), which should inform occupant electricity use, # and the minutes of daylight in a given day (from NOAA), which should inform day v night electricity use and capture seasonality
# drop un-needed columns
# Using without Request$ from requests_html import HTML$ doc="""<a href='https://httpbin.org'>"""$
"""$ Print complete news titles$ """$
# Double Check all of the correct rows were removed - this should be 0
#Probability of converting given the group is control 
##The first half of the week seems busier than the second, maybe because stores want to restock before the weekend?
# Drop this column from the training data # Since it is highly correlated with hotel continent, little information will be lost
# Set an option so we can display the full tweet text # have a look at a few of the tweets (remember your subsetting)
# total occurances of the new landing page
# get the indices for the rows where the L1 headers are present
#build url  #go to facebook get the GameOfThrone page name and then get the id.
# adding the 'match_strength' column
# Take a quick look at the data
# number of one should be same as the number of data observations and it can be verified as below
#== Selective overwite 
# same as above but as different return value type (as series) 
#save model
#Clear to see that the Monetary_score column contains data outliers
#Sorts the table by Profit. 'Ascending=False' puts the highest value at the top. #Since 'pro_result' is a pre-sorted table, we can use just .head() to display the top 5 values.
#proportion of users converted
# forward fill
#I'd like to de-duplicate the data in sql now as there is crossover between each import #First I need to extract data into a dataframe
# dataframe of records missing any values
# Export to csv file as backup file 
# Your code here # df['pct_over'] =  # df['pct_under'] = 
# one time if for converison of list
# Initialize reader object: urb_pop_reader # Get the first DataFrame chunk: df_urb_pop # Check out the head of the DataFrame
# teacher_behavior
#For consistency reasons, we should drop these rows if we want to analyze the ratings at a later stage.
# Note that a single FITS file might contain different tables in different HDUs # You can load a `fits.HDUList` and check the extension names # Then you can load by name or integer index via the `hdu` option
# Using set math to find out which columns are common between both DataFrames
# check the structure of Watch Event
# identify the total page counts for the treatment group
#removing one of the duplicated_id
# check inserted records
# Details of 1000+ exoplanets up to 2014
# The two columns before cleaning
#Count unique topic_names within dataframe
#lets look at the first 5
# numpy doesn't provide vectorized string operations
#dfall.tweet_time = dfall.tweet_time.dt.date
# Importar el modulo data del paquete pandas_datareader. La comunidad lo importa con el nombre de web
# Test converting the Text feature of the reviews into a text file 
# dr_new.index
# To carry out statistical operations we want to convert "price" and "odometer" columns to float type
# Load data # Print shape
"""$ Load tmp sr pickle for news title docs$ """$
# check unique countries values
# idpath = "data/most_id.csv"
# filter dataframe by rows that don't have values in in_reply_to_status_id
#fill cohort activation dates
# load pickle file for dataframe
#set(age_gender["year"])
# open all files
# A more professional way (using numpy instead of for loop): 
# insert a test post:
# Obtain the top 10 interests in the Big Data Group
# Request content from web page # Set as Beautiful Soup Object
#removing the processed columns
# Remove the missing values
#Assign the column names to dataframe
# use the parser function in the datautil library to parse human dates
# What are the most active stations? # List the stations and the counts in descending order.
# Use Pandas to calcualte the summary statistics for the precipitation data
#Saving the dataset in a file
# convert to datetime
# Setting up plotting in Jupyter notebooks # plot the data
#url_votes = grouped['net_votes'].agg({'total_votes': lambda x: np.sum(x), 'avg_votes': lambda x: np.sum(x) / len(x)})    
# fig.savefig('toma.png', dpi=300)
#Diplay the top model parameters
# probability of converting regardless of page #print('The probability of converting regardless of the page is: {}'.format(df2_converting))
# Get all document texts and their corresponding IDs.
# area plot of their age distribution
# Starts with a letter
# using .div maxes sure you divide the corresponding indices, in this case business names #ratio fillna??
# RESET the TF defaults # Start a fresh graph with no configs -- TODO: get some config info for the graph
# View the game_data shape: (43727, 161)
#Read Excel file  #https://stackoverflow.com/questions/32591466/python-pandas-how-to-specify-data-types-when-reading-an-excel-file
#reset the index to date
# Look at the head of the dataframe
# find the non-numeric feature columns
# Define column names for portfolio dataframe # Create a target distribution dataframe
# duplicates #intervention_train.drop_duplicates(inplace=True)
# Find the max and min of the 'Average Fare' to plot on y axis
# Extract mean of tweet length
# run the model giving the output the suffix "distributedTopmodel_docker_develop" and get "results_distributedTopmodel" object
# Is there correlations between the northern and southern sea level timeseries we loaded?
#Look at the columns to review the data for stations #Oh, I get it now. We need to grouby precipitation data, I think
# Let's us see all the contents of 'source'
# common words from the reviews
# The above query returns a list of tuples from the measurement 'table' object.  We want to import these tuples into a pandas # dataframe which extracts the values from the tuples for input to the dataframe. # Add column labels to the df.
#churns 
#What was the largest change between any two days (based on Closing Price)
# Statistical Summary  of Precipitation. 
# Compute and display sample statistics
#Put all data into dataframe sentiments_df
#the count for new and old
# Read the variation of wind generation from Data.xlsm
### Fit Your Linear Model And Obtain the Results
##### ignore
#shows quick statistic summary of data
# Plot the histogram
##The DataFrame and Series objects can be sliced to retrieve specific rows. #Slice the temp differences columns for rows at location 1 throgh 4
# Let's see a historical view of the closing price
# query to pull the last year of precipitation data
#---Connecting to a MongoDB client---
# Set the x and y limits
#Get a count of all records from the dataframe's shape (rows, columns)
# Plot observed statistic with the null distibution
#We create and authenticate an instance of our new ```PrintingStreamListener``` class
# Let's see if dataframe index is sorted?
# figure out what was the spike at 5-6 am on Small Business Saturday # more than 90% of the collected tweets were created between 5:30 am to 6 am Eastern time # So let's focus on those
# We won't use these dataframes anymore
# Series
# Assess the twitter archive
# create a Series with a PeriodIndex
# convert the source column to a categorical column
# cisrol12 # this subject had mp4s for each activity
# grabs the first row of our data table
# print out details of each variable
# print the column names
# Treatment group individual conversion rate 
# Load gh_api_token & meetup_key & genderize_key
#initialize an empty table with a date for each day of the year (336 rows)
# open r humidity nc
# A simple function for us to use in pandas' .apply() method.$ def stem_func(word):$     '''Stems a word'''$
#Pull 2017 data
# Location Function launched on 2017-04-03?
# Give the file a meaningful name
#dfHBOR3.drop('Unnamed: 0',axis=1)
# Drop rows with label 0
# check out state abbreviations
# From the original merged table, count the values by type of city
# test to double check the function worked correctly.
# write your code here 
#Greensboro': 'a6c257c61f294ec1'
# Double Check all of the correct rows were removed - this should be 0
# Let's combine the original comments with the model predictions so we can more easily make sense of the results.
# make a GET request
#trying the above but trying to convert the unhashable status object into a json object #parsed_json=json.loads(json_str) #type(parsed_json)
#drop the column unnamed 
#there are duplicate second entries for this well, just coercing to 1
# Simulate conversion rates under null hypothesis
# Save file
#number of rows in which the new_page and treatment do not match 
# Store filepath in a variable
# print the classification report
# delete rows where treatment is not aligned with new_page or control is not aligned with old_page
#tweak the 'minimum probability' argument to expand/contract the list
# count by event
# We'll need an additional column with just the hour
# simple pandas plot
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Write to CSV
# Precipitation Analysis # - Design a query to retrieve the last 12 months of precipitation data.
#groups.get_group('Marble Hill-Inwood').describe()
# check value counts # pd.value_counts(RNPA_new['ReasonForVisitName'])
# run a ztest 
#probability of an 
# Create binary indicator of whether the response time takes more than 1 day
# running results.summary() throws an error, specifically 'scipy.stats' has no attribute 'chisqprob' # based on this post https://github.com/statsmodels/statsmodels/issues/3931, we can add this for a workaround
# define your X (features) and y (target) # hint - make sure your y is not in your X!
# Finding the best parameters to build my final random forest
# network file for pnet
# figuring out how to sort friends_n_followers by number of friends
# Let's create a simple dataframe from a range of numbers with column names
### Fit Your Linear Model And Obtain the Results
# failed campaigns (sorted by counts)
# A lot of values missing
#count of converted users with new_page
# calculating the convert rate for the old page
# conversion rate of treatment group
#default is ascending meaning if sort_index() is not provided any value as a parameter, it will default to ascending. So its not mandatory to provide argument/parameter to sort_index() of Series object. 
# Places table
# convert text to datetime object
#lets explore the high end of prices # we have ferraris which seems to be correct, the rest is false data
# delete by axis
# flight7 = flight7.na.drop()
#postsY.reset_index(inplace=True) #fig, Ax = plt.subplots(2,2,figsize=(10,12))
# find a recipe with an ingredient
#distribution of favorite_count and retweet_count after a log transformation has been applied
#Removing observations with duration=0
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
## Reset indexing of the df
#Convert all characters to lower case
#List the stations and observation counts in descending order
# Download all mapped PDBs and gather the metadata
# List comprehension for reddit post timestamps
### Create the necessary dummy variables
### Create the necessary dummy variables
# Check if there are any 'Drive' terms in street.
# Sort output (according to decreasing order)
# basic step
# Read in potential data model during initialization
# Which learner had the most interactions and how many interactions did they have?
# drop known bots from analysis 
# Removing first duplicated row with row id 1899
#sm.stats.proportions_ztest([# of success old,new],[# of trials of old,new])
# Create engine using the `demographics.sqlite` database file
# Your code here
# now let's see how many of the tickets are distinct
# from sklearn.feature_extraction.text import TfidfVectorizer
# transform string to timestamp 
#dropping thos values except  for ferraris
# making pickup bins and grouping by pickup cluster and pickup bins
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Clear out the proc directory
# Self joining the Transaction Tables #printing the desired resultset
# proportion of users who viewed the new landing page
#this is a little on-off switch for my various long run-time operations that are behind a check. # go_no_go = 0  #GO #runtime items will be necessary once each time after the kernel is reset.
# Run this cell to display all buckets.
# Display a list of all the models
# 22 row is 2018 
#kind can be 'hist', 'scatter'
#Code: #Test:
# got lots of VW, bmw, opel merced  audi. i.e. we are in germany
#Compare shapes of the two dataframes
# Create Data Frame
# Drop null values
#Register the Spark Dataframe as a table #Now a SQL statement
# check shape
# Perform a query to retrieve the data and precipitation scores
# check for unbalanced classes - the others should be balanced 
#Getting the z value and p-value related to the model created above
#Convert to numeric
# Identifying the top 10 authors # value_counts returns object containing counts of unique values # Listing contents of 'top_10_authors'
# WARNING: Be careful when uncommenting the line below, it will set the entire column to NaN unless you  # put something to the right of the ellipses. # bus['postal_code_5'] = ... 
# Import countries.csv data
## stemmed keywords
# Calculate the interquartile range (IQR).
# join the country code based on user's id.
# Can also visulize entitie detection given a ticket
# Divide each number of each countries columns by it's annual maximum 
#over the whole month (includes entire data range) total number of page views 
#data massage newsMood_df #converts 'Timestamp' from string into datetime object # save newsMood_df to csv file in same folder
#stationweekday.head()
#Test # Recreate dataset twdf.clean with excluding invalid rating_numerator
# TASK E ANSWER CHECK
# 3. Create a new Data Frame called uni containing only rows from free_data which indicate that  # the person attended university or graduate school. Print the value counts for each country.
# peep at team table and print shape
# We can also convert all data into their exponents using the NumPy function
# create a new variable to make it easier to work with the json
#plt.colorbar()
# Requirement #2: Add your code here
#== Check if NaN or not  #print(pd.isna(dfx)) <== depreted?? 
#Anaheim
# 1. Collect data from FSE for AFX_X for the whole year 2017
#Prints out the top 15 places in user_location after clean up has been used on the dataframe
#inspecting nrOfPictures
# it's the same thing 
#create a column with the result of the analysis:
# How many stations are available in this dataset?
# Identify most active station
#Simulate the old transactions using random choice function with 1's and 0's
#the row names in a dataframe correspond to the index values which can be retieved using .index command#the ro 
# Build a new model # Calculate results
#Run a for loop across the list of columns with missing variables. Use .loc to locate the row and column that #I want to be replaced with the mean of each column as it goes throguh the loop.
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that  # the date format is YYYY-MM-DD).
#This returns only the duplicated rows. 
#create arima model
# df_2012
#Using the .hist() method
#Make a subset  #Check the latest date
#z_score = 1.3109241984234394, p_value = 0.18988337448195103 # 0.90505831275902449 # Tells us how significant our z-score is # 1.959963984540054 # Tells us what our critical values at 95% confidence is
# Perform a query to retrieve the data and precipitation scores
# Getting polarith and subjectivity as well as assigning an Sentiment to each of the tweet. 
# Retrive data frame from Measurement tabel
# p_new = p_old, copied from above , should give 0.12
# For finding all the paragraph tags
#checking for unique user id.
#Calculating the conversion rate of the old page
# Convert data into a DataFrame
# Custom the color
#Plot the distribution
# Investigating number of unique countries in the dataset
# See the data types of columns
# Need to export CSV with added weather data 
# parse_dates=True, means we let Panda to "understand" the date format for us
# create the intercept column #df2['intercept'] = 1 # create the ab_page column and fill the column data
# Converting the index as date
##### sort by label on index
# kfold cross validation
# Testing data merging
#change rating_numerator column data type to float.
#READING THE FILE #lOOKING AT THE HEAD
# Get column and types for Measurement Table # columns
### Fit Your Linear Model And Obtain the Results
# df_goog.plot() # this considers volume, which blows up graph and isn't proper unit
# pie chart showing percent of total drivers by city type
# Let's group purchase quantities by Stock Code and CustomerID
# bad request use Response.raise_for_status() to track
# Dropping categorical columns so that applymap() can run # Applying functionn
# find overall sentiments
# Printing the content of git_log_excerpt.csv
# Write team schedules.
#arima11= ARIMA(dta,[1,1,0],freq='Q').fit() #arima11.summary()
# Put in dataframe and view selected rows/columns 
#we compute the statistics for numerical variables
# remove white spaces
# print bottom three (least consistent)
# Investigating unique values in columns of interest
#Reading Pickle File 
# open and read the downloaded image.predictions.tsv file
#Male and crime only dataset
# need to fix format issue on 'lastest_consensus_created_date' column.
# get day name from a date
#inner join on UserID
# GPA mean for graduates
#wikipedia_marvel_comics = 'https://en.wikipedia.org/wiki/Marvel_Comics'
#resample transit and cab data to a lower frequency (1 Month) #select by day
#Dataframe with only required columns
# fitting linear model
# Abosulte value # Round # help() # type q to quit
#help(jp.sprints) #jp.boards()
#Save figure
# choose a random point for the update  # update m # update c
#Convert date to the important features, year, month, weekday (0 = Monday, 1 = Tuesday...) #We don't need day because what it represents changes every year.
# ...and as in the case of position-based indexing, using # slices or lists as col indexes results in dataframes
# perform a search # print the number of items returned
#Leitura do data set
#Check that you are using the correct version of Python (should be 3.5 for these tutorials)
# Group data by store name(number)
# Sort by co_occurence (max co_occurences in first row of df) # Keep only top10 co-occurence
# Creating a datetime index
# drop NaNs created from lagged feature generation
# Let's plot the whole history of the temperature (field4)
#full_data = train_data.append(test_data, ignore_index=True)
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
# Use Inspector to print the column names and types for table measurement
#Columns in constant values
# make sure tweet_id in all 3 dfs are of same datatype 'str'
# Read the Train and Test files
# Analyze the odometer and price columns for outliers
#Store locations of long NAN gaps in primary sensor record to be filled
# collecting the violence levels 
# merge
#get highest within-day change
# Create a TensorFlow session and register it with Keras. It will use this session to initialize all the variables
# checking the numebr of rows in the data set and storing # number of rows in  variable # printing the number of rows
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
### fit the logistic model for countries ### print the results
# renaming the grade cateogry to have more meaningful names using .cat.categories method
# Without Square Root
# get graph Facebook
# save as pipe delimited # check if it worked
# Import transplant stemsoft data
# BTC-EUR price from May 2015
#https://stackoverflow.com/questions/29722704/fastest-way-to-calculate-average-of-datetime-rows-using-pandas
#check the dataset 
#check for NaN to make sure
#checking that number of rows has been redced by 25
#number of users that received the old page; where group = control
# merge into temp
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#new_page_converted = df2[df2['landing_page'] == 'new_page']
#define colors and legend values
### Fit Your Linear Model And Obtain the Results
#Combine The Data
# Join both sources into a single dataframe
# transform all of the data using the ktext processor
# read newly created dataset into another dataframe
# which ship traveled the most distance? #df_l['ship_name'].groupby(df_l['major_cruise_line_x']).value_counts() #df_l['meters_traveled'].groupby(df_l['ship_name']).sum()
# simulating n_new transactions
# daily
# remove outliers above 350,000 and below 10
# Task F answers
# Two possible solutions: #OR #df_goog['Closed_Higher'] = pd.get_dummies(df_goog.Open > df_goog.Close).values
#happiness_df average by year
# my fire size model doesn't predict any fires over 100; or size class C.  # this is a limitation
# load stop words
#first , check the unique values for landing_page and group columns 
# Get the tables in the Hawaii Database
# import modules:
# 74 comments or less is encoded as 0, 75 comments or higher is encoded as 1
#use the converted rate in ab_data.csv regardless of the page
# From the simulation # finding the values in our sample that are greater than actual difference p_diff # how many less than zero
# Run OpenMC
#pd.to_datetime('11/12/2010', format='%m/%d/%Y')
#Get the top 10 timezones of the tweet authors
# Save references to each table
# Number of rows
#Group by News Org  
# Define the Cosine Similarity function
# Make the output Dataframe
# Simulate distribution under the null hypothesis # Plot the null distribution
#hyperparam =[0.01, 0.015, 0.020, 0.025]
#df['LinkedAccountId'].apply(lambda x: x in acct_dict print(acct_dict[x])) #df['LinkedAccountId'].apply(lambda x:x in acct_dict acct_dict[x])
# Scrape the 99bitcoins.com website
# Joining the classes again and checking that they are now balanced.
# sample proportion of new page conversion under null # sample proportion of old page conversion under null # difference
# Use .info on df to see if we are missing any values
# integer index: a series
# How many breakfasts?
# Display the data table for preview
# list_1 = list(df.m_y_d.value_counts().index) ... How list_1 was created... these are the days I have data for 
#Calculates number of users with new_page
# adding the 'match_strength' column
#Renaming a column in files4  #Making  a jobcandidate column in files4 
# Build an LSI space from the input TFIDF matrix, mapping of row id to word, and num_topics # num_topics is the number of dimensions to reduce to after the SVD # Analagous to "fit" in sklearn, it primes an LSI space
### save the entries in the timestamp column as Python datetime objects ### check for success
# Take a look at the words in the vocabulary
# Therapists decomposition
#readig csv
# Backend
# The styling '-' and '--' is just to make the figure # readable in the black & white printed version of this book.
#5a. You cannot locate the schema of the address table. Which query would you use to re-create it? 
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# date 1 year ago from today
# num_regex = re.compile('[-+]?[.]?[\d]+(?:,\d\d\d)*[\.]?\d*(?:[eE][-+]?\d+)?')
#Retieve the column list for the dataframe df_ created in problem statement 20
# combine these two equal sized frames
# number of users who reamined with old page (i.e. group = control)
#Load the json file #Print the parameters    
#Make sure no missing geometry
#marvel_comics_save = 'wikipedia_marvel_comics.html'
# Open Fermi 3FGL from the repo # Alternatively, one can grab it from the server. #table = Table.read("http://fermi.gsfc.nasa.gov/ssc/data/access/lat/4yr_catalog/gll_psc_v16.fit")
# query the max, min, and avg of the most active weather station
# add columns with automatic alignment
##### Fix time density also log-normal-ish
# Create an engine to a SQLite database file called `hawaii.sqlite`
# Get the file name
#Visualize the frequency of #shoolshooting was tweeted
# Now that all the values that may mislead us may cause an error are dealt with, we can go along to # build a derived variable, named "Tip_percentage_of_total_fare", for tip as a percentage of the total fare  # by dividing each row's "Tip_amount" with "Fare_amount" data value.
#Make predictions using fit model
#engine = create_engine("sqlite:///../Resources/hawaii.sqlite")
# Remove Dublicate user_id but keep data frame df2
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#, hue=ss.inverse_transform(X_test)[:,5]) # jitter=True  is still unreadable for the stripplot
#Split data into training and testing sets
#### This adds a column that says whether a row is closer to the bottm or the top of the well #### This is useful for doing creation of features of rolling windows where you want to avoid going into another well stacked above.
#Firstly, we call the asfreq method #We pass it the Daily argument which does include weekends #pad is the method that we use to carry forward the prices from 2nd to 3rd & 4th June
# this_tweet.head()
#!/usr/bin/python ## dd/mm/yyyy format
# Calculating p-value
# group by Symbol
# How many stations are available in this dataset?
#Merging all three dfs
# So pretty much this just shows the annual return of each stock so that -> # you can see which stock has a higher annual rate of return.  # In this case you can see that MSFT is better than all of the others.
# Remove stop words from "words"
#which are the variables that contain Nan values : 
# Direct masking operations are row-wise
# result = pd.concat([sample, c_df], axis=1, join_axes=[sample.index],join='left')
#read the csv file into a dataframe
# mql 1 vs 0 closed won as a percentage of total using apply
# Get all of the SamplingFeatures from the ODM2 database that are Sites # Read Sites records into a Pandas DataFrame # ()"if sf.Latitude" is used only to instantiate/read Site attributes)
# Checking unique user ids count
# How many stations are available in this dataset?
# Using the station id from the previous query, calculate the lowest temperature recorded, 
# predict the probability of film project with 50 blurb count, 50 backers and a goal of 500 dollars.
#getting value of cells with null values
#"lpep_pickup_datetime" and "Lpep_dropoff_datetime" have indexes 1 and 2 respectively #Passing errors='coerce' to convert invalid data to NaT (not a time) #Creating the "Trip_duration" column
# an empty model, no training yet
#All formatting is the same so just cut the wordings #RUN ONCE ONLY
# extract full monthly grid
# Create a logistic regression model with baselines as US and old_page
# The amount of unique user-entries:
# normalize the dataset
# find how many periods ? #start_idx + pd.tseries.offsets.DateOffset(months=1)
# fig.savefig('toma.png', dpi=300)
# 2. Remove the reply retweets and drop retweeted_status_id,  # retweeted_status_user_id and retweeted_status_timestamp column # only keep the tweets that retweeted_status_id is NA then drop the two columns
#save clean dataframe to disk and free memory
# Retrieve tone of each ticket
#the largest change between any two days (based on Closing Price)?
# creating DataFrame
# Load the mazda datasets
# Getting phenotypes for huIDs that have associated genotypes
#query or 2 variables to figure out new_page and treatment
# Vectorize the articles into a matrix # predict whether or not economically relevant based on previously fit model
# Looking at unique values # Looking at general statistics for the series # Looking at the number of each of the values
#before the data source as category, it was int.
#the missing renta values in the data dataset are filled in here
# All tweets's retweeted value is 0
#called effectiveness function 
#Visualize the frequency of #hermosabeach was tweeted during the data collection period
# .ix can be used to look up rows by either the index label or location,  # essentially combining .loc and .iloc in one.
#Average conversion rate for the whole dataset
# We change the row label from store 3 to last store # we display the modified DataFrame
# A:
#Now merge it back to the london OA code DF
# Load the second file into a DataFrame: csv2 # Print the head of csv2
#let's change the index to something more interesting
# predict on the last two months
#dicttagger_food = DictionaryTagger(['C:/Users/Connor Fitzmaurice/Documents/COMP47350/DataAnalyticsProject/DataAnalytics/food.yml'])
# Drop duplicated user
# The info of the values of each row
# Export the new CSV as "NewsMedia.csv"
### Number 1
# now we need to rebuild the prior parameter covariance matrix
#Plot using Pandas
# Get rid of the database we created
#upload csv containing data set provided by LA county
# define user ratings
#no nulls/missing values in any rows
# Import, convert 'Date' col to datetime
##Inspecting the index of the series by calling the index parameter
#looking shape 
#using value counts in descending order. 
# your code here
# check whether the author of each post in 'train' is in 'frequent_authors'
# Enter the density of the material that was sieved.
# merge player1 and player2 table
#initial visual assement
# create mask to select rows that 'group' and 'landing_page' lines up
# checking for duplicate entries for the same image in the df2
# Remove these stops.
#if we try to request from this url, it fails and says we need to include http.
# using crawler
#### Test ####
# df1 = pd.read_csv('../output/data/expr_2/expr_2_pcs_nth_0.1_div_101_03-20.csv', comment='#')
#get data from the csv directly instead of running it till now
#merge average fare data into city dataframe
# logistic regression without tfidf #%%time
# p_new under null 
# login_url = 'http://www.prisontalk.com/forums/login.php?do=login' # login_response = s.post(login_url, data=login_form)
# n_new = new page count
#Importing data
# Display a description of the dataset
# Import inpatient census extract
#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
# Cria um dataframe vazio e adiciona os arquivos de cada candidato
#ua is a dataframe containing all the united airline tweets
# get 8-16 week forecast new patients # keep only date in index, drop time
# praw object for script to use Reddit API
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# extend the stop-words list
#Call to the metadata of the Base class then generates the Schema
# Probability of an individual in the control group converting is 12.04%.
## Checking for the one duplicate row ##
# foreach through all tweets pulled # printing the text stored inside the tweet object
#for values less than 1, lets reciprocal it for better explanation
### Creating the necessary dummy variables
#read in old csv files
#Setting the date time as the index allows time slicing
# Using pd.concat to join individual series in a list in one DataFrame - harmonize start dates
#Add MGD columns
## we will gather all topics from subreddit (limit=None)
# We would like to see what values we have for "Class". This snippet below shows that Class is a binary column.
# Try Multinomial Naive Bayes model with TF-IDF # Define X and y # Train test split 
# drop any duplicates across ALL columns (not sure why they would be there though???)
#Since the return value coefficient for ab_page is less than one, we take the reciprocal. This returns the value 1.02  #as mentioned above
# How would we do the same, but groupby both primary and sedondary description?
# smh
# convert to ndarray # Second way
# finding rows where seller is not labels 'privat'
#calculate magnitude at which variables have effect over response variable
# LOADING LIBRARIES WE MIGHT NEED... # # system
# Calculating the p-value here. # I have to turn p_diffs to an array, if I do the following with p_diffs as a list, it won't work.
# Here's an example file!
#CON is the CONTACT TERM information #CON=pd.read_csv('C:/Users/sxh706/Desktop/Interactions.by.Month/2018/June/Interactions.Opportunity.Term.csv',skipfooter=5,encoding='latin-1',engine ='python')
# your code here #Let's briefly look at your result
#for post in posts.find({"author": "Mike"}):
# This is one tail test.
# Alternative way to get the date only 
# Create the plot
# Mapping id -> slug # Mapping id -> slug (will be used for readability)
#retriveing data form bitcoinity.org
#Training and testing dataset
#remember that your imports should all be at the top. I leave it here to hightlight that this package is needed at this point of the workflow
# data based on index location
#  GIT_LOCATION is the path of the git executable
# The travel with duration less or equal to 60 seconds. 
# y axis for funding_average_per_type_bar_chart 
# storing only the original ratings in the df_enhanced dataframe
# the first entry in this Dataframe's shape gives the number of cards in the set, and the second is the number of card attributes (name, mana cost, type, etc)
# teacher_info
# LENGTH AND SENTIMENT
# Create a list of links.
#Then generate a heatmap using the latitudes and longitudes
# we store current working directory in cwd1, later we change working directory and use it to change it back
#Look at the columns to review the data
# Subset to only Level 1, and those where days_baseline is small (let's say below 7)
# Helper function to add to spelling dictionary
# Top 10 Names
# create a query mask for rows where the Category is equal to the value "Road/Street Issues" # find the rows matching the query, select the Issue column and count the unique values 
# We first observe the head data.
#'St. Louis': '0570f015c264cbd9',
#new_page_converted = new_page_converted[:145274]
#read value into row, the same session created previously will be used
# If there was no accident, set value to 0, not NaN
# Extract the scale factor
#user_id column is reduntant # replace null-values in visited column with zero
# nan = 0
# cisuabf6
# For finding the first paragraph tag
#print(dictionary.token2id['movie']) #verify if these words were in the stopwords list #print(dictionary.token2id['n\'t'])
# List indices of rows with month 0 # Pass the list as the index parameter to drop these rows
# use both 'title' and 'author' as the input text
#au.plot_user_popularity(very_pop_df, day_list)
# use the parser function in the datautil library to parse human dates #date.weekday()
# Datetime in pure Python
# p_old = 0.1196
#List columns being omitted from saved data
# use pandas to get_dummies
#displaying proportion of p_diffs greater than obs_diff
# Assign the demographics class to a variable called `Demographics`
# Read the twitter_archive_enhanced.csv file as pandas dataframe
# One last downcasting check...
# get ndarrays columns from the dataset we just uploaded
# the probability that an individual received the new page
# Merging Twitter data frame and Stock market data frame
#Import modules
#Make the graphs prettier (code in tutorial is for an older version) #Turn off pretty print while we're at it
# Use a colour-blind friendly colormap, "Paired".
# Articles are contained in a <div class="article--container"> element.
# Country map to plot the transmission arrows on
#read stations csv
# All we are doing is importing libraries that have already been created by the community. Note! That we # have defined np here. np could have been defined under any name. Industry standard is np. Same with plt.
cur_b.execute('''$     FROM room r INNER JOIN hotel@FIT5148A h ON h.hotel_id=r.hotel_id''')
# What kind of injuries are we looking at?
#'San Jose': '7d62cffe6f98f349'
# once we sort them we can create a data frame for the first 25 default Json files ready to be dowlaoded
# Plot a run chart, using the index for x.
# prediction on validation set
#The number of rows in the dataset. #print(len(df))
# your code here
# a 10-row mini test df
## This may take a few moments
#Merge the other feature dataframes into the main dataframe
# cumulative mean of the MC estimates
#'Tulsa': 'cb74aaf709812e0f'
#Repeating the process on the support dataset, and only showing the first 5 results using .head()
# Read csv into pandas dataframe and store as countries_df # Merge df2 and countries_df and make user_id the index (as they match on index)
# Using the inplace option results in a SettingWithCopyWarning
#double check the correct row was removed
# 12. Print rows 100 to 110 of the free1 Data Frame
#new_cases_Sl_concat['Total_new_cases_Sl'] = new_cases_Sl_concat['Total_new_cases_Sl'].astype(int) #grouped_months_Sl.head()
#Add variable initializer.
# Setting up plotting in Jupyter notebooks # plot the data
# reflect an existing database into a new model # reflect the tables
#create a timestamp to get values from the FAll 2018 term that are prior to Jan 1 2017
# Simulating distribution under null hypothesis # Line is actual difference observed from df2 data -0.001578
# The case of "1) variable (time, hru or gru) and more than 2 hru
# Let's look at the types of values for 'onpromotion'
# display first 10 elements of the dataframe
#we will arrange it in a assending order with a new label
# control group converting mean
#plot the normal dsitribution for the null hypothesis
#Looks like a lot more people from Costa Rica now. Let's try to aggregate all the locations with 'Costa Rica' in the name
# Can be constructed from a dictionary of Series objects
q ='''SELECT nameLast as last_name, nameFirst as firstName, birthYear as birth_year  $     from people where $     nameLast='Williams' and birthYear > '1920';'''$
# train a simple logistic regression, and...
# under null, we disregard the type of page(based on the assumptions in Q2). So conversion rate will the mean of converted regardless of page
#US #For a unit decrease in US,ab_page is 1.01 times likely holding all else is constant
#Tells us the probablity of a Reddit will be correctly identified in the class its assigned
# Sort the dataframe by date
#  group time object, calculate mean sentiment if tweets are created at the same time # reformat the data into dataframe # group every 5 minutes, and caculate average
# The candidates
# Apply to all movie reviews in dataframe
#knowing the unique values in the country columns
# Check how many countries are in the dataset
#new_cases_liberia_concat.drop(new_cases_liberia_concat.columns[[1, 2]], axis=1, inplace=True)
#Let us see if we have any null values 
#Understanding the total amount of time for the postings
#converting tweet_id column to string datatype
#simuated p_old  #simulated p_new
# Assuming 0.2% of people that pass buy a crepe # Column "day_sales" in units 'Number of crepes sold per day in that location'
##### ndarray of transposed
# We display the total amount of money spent in salaries each year #We display the average salary per year
# slice as column index: a dataframe
#Set style
# Write your answer here
# filter data where p1_dog is True
# feature matrix
# any string that is also in the list gets turned into "None"
#'revenue': tmdb_movies['revenue']
# Ok, but maybe a bar plot would be better.
# Dataset import and minimal datset exploration
# check to be sure formats match
# Replace missing values with 0 # Fill with median # dropping the observations
cur_b.execute('''$ ''')$ conn_b.commit()
# 11.9% conversion rate
# count occurrences of 'jewish cowbell' triple parenthesis '(((like this)))'
# Fills the null values, in the first column, with the last non-null value
#View results
### Michael Kors related comments and reposts count per day 
#table.div(table.sum(1), axis=0).plot(kind='bar', stacked=True)
# using seconds as unit for more precision
# read data in from JSON
#Making a copy of the dictionary and showing all the possible campaign stage names options
# Use Partial sorting on a sorted MultiIndex
# most of the usernames are in 2 words or 3 words.
# accediendo a varias filas por etiqueta
# what are our unique tags?
# examine some of the indicators
# Replacing German words with English words
#Export table to sql
# Movies with the highest RottenTomatoes rating
txt = """$ The sparks from the seemingly ordinary July 23 incident on a road near Redding set off one of the most destructive wildfires in state history and killed eight people, including three fire personnel. It also destroyed more than 1,000 homes and consumed 229,651 acres, according to California fire officials said.$ """$
# Apply the percent function # Seeing what DummyDataframe look like
# calculating the actual success conversion rate for old page
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
#import bible text
#original data set differences 
#Drop the duplicates
# Use `engine.execute` to select and output our chosen vacation day-time period of around 15 days. # It picks the first 15 days of january for which data was collected.  (remember any missing days # that are not in sequence may have resulted from the 1400 est data points missing from prcp, etc)
# Or, using the dateutil module, you can parse dates from a variety of string formats
# We are embedding the video we searched into the following webpage using HTML
# get head or tail elements
#Load the query results into a Pandas DataFrame and set the index to the date column.
# Check that the largest observed value is <= step threshold
#Create predictions and evaluate #print("Number of features used: {}".format(np.sum(clf.coef_ != 0)))
#This is just to test if the connection was established. Should only have 15 items in this. 
# view the leaderboard
# which neighborhoods had an average response time of at least two weeks?
#convert to date objects
# assemble features
# Load uber data
# Perform vectorer operations
# The mean absolute percentage error
# before
#from pandas.tools.plotting import autocorrelation_plot
#convert the day of week column to strings, prepping to be dummified
### Fit Your Linear Model And Obtain the Results
# pie chart showing percent of total rides by city type
#I used this zip code for my other report as well. It is missing only a bit of data and is formatted rather well.
# fig.savefig('toma.png', dpi=300)
# read csv
# Grabbing the ticker close from the end of last year
# Turn the dummified df into two columns with PCA
# open the "json_example.json" file for writing, and write in it the json object "obj"
#logic to drop duplcates and validat the record count
# creating attend csv
# Top 10 negative news
#Filter for bettable races by chaining criteria
# Call method 'score_all_on_classifier_by_key' on our classifier object. # It calls method "score_all" from class 'Classify' # 'score on all the test and validation sets' 
# Save the query results as a Pandas DataFrame and set the index to the date column
# create dummy variable columns 
# q_all_pathdep = c.submit_query(ptoclient.PTOQuerySpec() #                                .time("2014-01-01","2018-01-01") #                                .condition("ecn.multipoint.connectivity.path_dependent"))
#To determine the probability of the pages
# Look to see if boroughs had different types of complaints, 
# reg_final = xgb.XGBRegressor(**best_model['params']).fit(X, y)#**params
# Let's move on iwth data exploration
# adding pd.DataFrame did not help runtime # took 1 min when the Zero values removed # need to drop the zero value, since the distribution has many of them
#exponentiate the coefficient of ab_page
#Inspect label balance
## mean year in dataset
#Display summary of results
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
# now let's remove all zeros and see the distribution
# Get Sobeys for walmart tweet #Read Sobeys tweets csv file
#Example: scrape description
# joining the DataFrames together # removing rows that are only present in the CHGIS file
# Control Conversion Rate
#Set panda options so whole lines for tips can be view
#Read tips csv into dataframe  #tipsDF = pd.read_csv('C:/Users/ray/Documents/ConnorStuff/DataAnalytics/CSV_Files/yelp_tips.csv', index_col=False, encoding='UTF-8')
# get release data
# set up tweepy
# Describe function shows all statistical information at once
# new_page and treatment don't line up in the cases of new_page&control + old_page&treatment:
#Again, I don't have enough data to see the whole year...
# Get the length of the old page group
# interprete the coefficients
#for registration yer we have some outliers  near min and max
# Now let's encode the emojis so we can use it in the vectorizer as a feature. # create a new category 'emojis' with extracted emojis from tweets.
# Create copy of dataframe
# Write answer here # we are not interested in the correlation between year and year, and this allows us to use max meaningfully
# Probability of individual user converting
# all data on June 3rd of 2015:
# match colun 55 to 66 : home player ids #match.columns[55 :66]
# Change categorical landing_page to dummy variables and drop the old page. # The ab_page column, is 1 when an individual receives the treatment and 0 if control. # Add an intercept column.
# Save model.
# Run the model with training set 
# Store link
# Increase the depth of the GBM on this new, reshaped data # We can increase the AUC a bit, perhaps not worth the problem
#Number of rows and features after removal
# And the iloc method for selection by position
#train test split, standardize data
#cars_updated = dedups.apply(preprocessing.LabelEncoder().fit_transform)
# df_2010
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# to get the last 12 months of data, last date - 365
#already computed in d.
# logistic regression # fit model
# Check the results
# step 5: summary of the model
#df = train[columns]
#Save in feather format
# proportion of p_diffs great than the actual difference observed
## check each column as series  #autos["num_photos"].head()
# The base URL where the OrgAPI is located.
# Create a dataframe that just consists of the songs that make it to number one in at least one region
#### Number of unique wells based on UWI
# valid
# Count where developer and ocmmiter are different
# see the histogram of the possible fake review.
### Create the necessary dummy variables
# Generate dummy variables on 'landing_page' by using prefix 'ab_page'  and save as a new dataset df2_dummy
#making a copy of the original dataframe where we will store the cleaned data
# Get values # features # targets
# Create BeautifulSoup object; parse with 'html.parser'
#Your code goes here # normalization factor # seting up the loss function
## We have to break this up into ~3000 size inserts, otherwise Neo4J will crash
################################################## # Load   ##################################################
#creating tweet summary for each news agency
# Drop the unnecessary columns
#Return the list of movies with the highest score:
# converting the timestamp column # summarizing the converted timestamp column
# convert those two columns from float to int
# fitting logistic model
# Fitting logistic regression model with only country variable
# format datetime field which comes in as string
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #2. Get the row names from the above files. # print index values as explained  the project sheet- 1st Data frame.
#Normalize the json into a DataFrame format
#It gives nearly = to 0 value
# FML floating points
# calculate n_old
# simple word algebra example:
# We can also use the "backfill" method to fill in values to the back
# Past 10 days
# Keep only text in the text column
# events
# if result is anything but zero, then the dataframe contains missing values
# tweets.apply(lambda row: print(row['tokens']), axis=1) # sorted_x
### Create the necessary dummy variables
#Use binomial because there are only two possible outcomes #Code = np.random.binomial(n, p, size) with n being possible outcomes, p being the convert rate of p_new under the null  #hypothesis and size the number of times an individual landed on the new page. 
# set SUMMA executable file
# use matplotlib to create a bar chart from the dataframe
#Total Number of Rides Per City Type
#Summarize the MeanFlow_cfs data, using default outputs
#concatanate the predictor variables with the dummies dataframe
# New dataframe for suburban data
# Define the authorization endpoint.
# Use statsmodels to import the regression model. Instantiate the model, and fit the model using the intercept  # and ab_page columns to predict whether or not an individual converts.
# Which learner had the most interactions and how many interactions did they have?
# compute the cumulative sum for every column and make a chart
## create a database (if it doesn't exist)
# need to use the lowercase version of the company name
### Retain Only the Top 12 most common wine types
# Displaying the result
# 7.
# looking at individual tweet polarities
# Export to "geometry.xml"
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#vio2016['num_vio'] = ins2016[]
# number of events, articles and mention_date of the dataset
#take the count of the rows with missing data in pop_2012 and divide it by the length of the column
# This is a workaround on an error discussed on github here: https://github.com/statsmodels/statsmodels/issues/3931 # Show results
# Create datetime Series #  number of created items per month over the timeline of the data
# Verify that an address is valid (i.e. in Google's system)
# For finding the first nav
#Here we found all the rows that doesn't have the consistent pair of (group,landing_page) which are (treatment,new_page) OR #(Control,old_page)
# get image-predictions.tsv with requests library
#groupby and count, save as dataframe
# Once you have a datetime object, you can do things like printing the day of the week
# Load in the cruise data file, without the header names # Instead let's specify the column names for just the ones we need.   # Let's show a snippet of what we have.
# manually add the intercept
# testing if above code works
# import dataset in dataframe object
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data # Get the count of total number of records in the dataframe
# calculate n_new
#write your dataframes to a csv file, if you want to be able to use it later. # Write out the DF as a new CSV file
# get number of elements
# Only select booths for which we have data
# Plot graph of seller # Total count of unique values
# Find all 'a' tags (which define hyperlinks): a_tags # Print the URLs to the shell
### Create the necessary dummy variables
# Find all names that start and end with a consonant
#Add list of nodes and edges
# dummy variables for landing page
#Perform a basic search query where we search for the '#flu' in the tweets # Print the number of items returned by the search query to verify our query ran. Its 15 by default
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
# Seasonal Difference by itself was not enough!
#merge the 2 df
#Urban
# standardize column names
# count the NaN values in a columns
# save the model to disk
### Define negative threshold for latter use
# Urban cities average fare
# Counting activity by station data to determine most active station
# Use Pandas to calcualte the summary statistics for the precipitation data #for the last 12 whole months
#create intercept column and dummy column for ab_page
#pvalue calculation
# write your code here
# test to see any duplicate
#create a dog_stage column and delete the each columns for dog stages
#cek print
#compute test statistic and p-value for two-sided hypothesis test in which #conversion rate difference is not equal to observed difference
#Simulate the new page converted on the size of the new pages
# Write out the DF as a new CSV file
# get the total number of collaborators
# plot sampling distribution
# first we get some 
#CHURNED SLICE, activation dates
#Convert to datetime so that it can be manipulated more easily
#do rows having missing values or not
# default is freq='B' for bdate_range
#ax = sns.swarmplot(y=price)
# large districts adopt more apps and with higher variance/std
#Check if all NULLS are gone ?
# Neat way to rename columns and order appropriately
## i'm pretty sure something is wrong here.
# create a query mask for rows where the Category is equal to the value "Road/Street Issues" # find the rows matching the query, select the Issue column and count the unique values and make a bar chart
# this feature does not seem to make sense, perhaps 'date_created' is the date the ad was created - not when it was # literally created -- therefore going to to drop this created column
#Storing the movie information into a pandas dataframe #Storing the user information into a pandas dataframe #Head is a function that gets the first N rows of a dataframe. N's default is 5.
#creating a column = to the index to be used as function argument
#Sort by Product #Finds the column in dataframe, then searches each row for value specified.
# Create a bar chart based upon the above data
# YOUR CODE HERE #raise NotImplementedError()
# df2.T
# Create columns of Date and Time to be used for Time of Day motor vehicle crashes
# Topic 6 is a strong contender for 'eating'
#write out the source data onto disk #however we want to write only the records which are duplicates. Better idea to remove the non duplicates.
# how many values less than 6 in each row?
# We extract the mean of lenghts:
# draw a vertical line for difference between treatment and control conversion rates calculated in section `g` # draw a vertical line for 95th percentile for our directional alternative with 0.05 p critical value
# get ready to analyze hurricane relief loans
# Import studies text file
#result1
# From a list of tuples
# make a HUGE horizontal bar chart so we can see the distribution of 311 complaints # it took me a bunch of guesses to figure out the right figure size
# show available waterbodies
# Setup the hyperparameter grid #,'max_depth':[4,6,8,10]
#Create predictions and evaluate
#agg([np.sum,np.count]) #sum() #agg({'col5':np.sum,'col6':np.sum})
# submit the request
# Model 3: xgboost
# empty dictionary to append number of tweets to 
# Check that there is only one duplicate user_id in df2
# Total duplicates in name column
#randomdata.describe()
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# set SUMMA executable file
# converting to seconds for readability
# Change the name of first row from "Unknown" to "rowID"
# place parameters into X and labels into y
# reset index
#Creating transaction list array with value of 1's and 0's with probability rate of Pnew which is 12%
##### named tuple
#df_regression = pd.concat([intercept, treatment], axis=1)
# number of unique users
# the length of the review
#sorting and sampling day toa smaller size
### This is data from wikipedia3 archive in data folder
# Rows and columns
# Create an empty figure
# Convert titles to string type
# years since joinging
# The index name of the previous row can be recovered with:
#test
#Checking spread of created vars
# SAVE A CSV OF THE TABLE ("number_one_chart.csv") TO COMBINE WITH "song_tracker.csv" LATER
# load the model from disk
#df = pd.get_dummies(df,drop_first=True)
# Now query to pull the last YEAR of precipitation data
### Create the necessary dummy variables
# Take a look at the words in the vocabulary
#Find out the proportion of converted in the treatment group
################################################## # Load   ##################################################
# remove the reference to the original data frame by creating a copy # use vector subtraction to find the length of each chomosome/DNA sequence # peak at the first few rows of the datafram with a length column added
# proportion of users converted
# merging countries.csv with AB_data.csv
# day_map = {0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'}
# Make target integer, one hot encoded, calculate target priors
# Dropping duplicate row
# Supply the zipcode found in Wikipedia.
#Before cleaning data process begin, I will copy the contractor table to contractor_clean table  #to check original data or roll back the operation. 
# Lemmatize verbs by specifying pos
# Display number of unique users 
#data = pd.read_csv('BCH_credid.csv', sep=',', header=None)
# Plot graph of abtest # Total count of unique values
# total # of rows in the dataset
# can also rename columns inplace
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser'
# proportion of P_diffs greater than the actual difference observed in ab_data.csv is computed as:
# Find unique users # Check for not unique users
# missing value check
# and visually?
# Check if there are any 'Court' terms in street.
#Simulation of n_old transactions with a convert rate of  p_old  under the null
# Verify accuracy score through cross-validation of X and y data
#Under the null hypothesis, we assume the converted rates for the old and new pages to be the same
# RE.COMPILE
# determine number of unique users using nunique()
#merged_left['taxa'] = merged_left['taxa'].fillna('Unidentified')
# rows only with emoji # df_emoji_rows.head(1)
# get the convert rate of p_old under the null
#Vemos los duplicados
#Take a look at the shape of the dataset
#Select all the non-null comment fields from the table
# dummying coming_next_reason column
# assign the cluster predictions to our customers #cust_clust.head()
# Information of Hour_of_day based on credit card usage
#Create a copy of all the gathered dataframes
# to make plots appear in the notebook
# Step 13: another reality check. The name isn't important to know, but it's nice to know things are working so far.
##Station analysis  #Design a query to calculate the total number of stations
# take a look at the data
#db setup-- the posts collection is the only one for stashing everything returned from that api endpoint #note to self-- use mongoDatabase2
#Negate those user IDs which are present in user table
#create new columns for dummy values #drop one country column to ensure a full matrix
#Get the station_id and name for the station has the highest number of observations
# fig, ax = plt.subplots(2)
#check all duplicated jpg_url from the image predictions data set
#Use SQLAlchemy create_engine to connect to your already created/saved data,  sqlite database. #engine = create_engine("sqlite:///hawaii.sqlite")
# Structures data library # Columnar data library
# read newly created dataset into another dataframe
# Create average purchase per product feature
# What are the most active stations? # List the stations and the counts in descending order.
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("song_tracker.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
## Sort by Date
#unique users
# Viewing dog_stage values
#displaying the p_new
# 10. Which of the vignette has the largest mean score for each education level? What about the median?
# failing to comprehend this migrating table location # for row in table_rows: #     print(row.text)
# no duplicated cols 
# SMA: 6, 12, 20, 200
#Merge every row into one paragraph, then split it by sentences ('.') #Use 'type' to check the data type of globalCitySentences
# Display the information of the repeated user_id
# df_2014
# Drop new column
# evaluate predictions
# generate historgrams of the # returns of our portfolio
# drop Nan if it is in specific column
# URL of page to be scraped
# Replace NaN with blank string
# change appointmentduration to hours
### top 10 locations for total posts
# Create an OpenMOC Geometry from the OpenMC Geometry
#check that file has been imported successfully
#_source contains plenty of other fields we'd like to examine on their own- so can do the same thing to expand that json, this time into index
#Probability  of an individual converting regardless of the page they receive
# Checking the result
#Check if there's any row missing values
# changing the python list into a numpy array
# Query for precipitation data based on date range from most recent to a year before # Create data frame from sql query
# Import gatecount data.
# df_2002
# for appending rows you can use .rbind()
# Fit and summarize OLS model
#Simulating the old_page converted for the old probabilty for number of times as defined in n_old
#create logit regression
# This will shift the column index up by the number of periods - here it is 1% of len(df) which rounded up is 35 and # the column values it shifts up will be replaced with NaN
# Take a look at the last row of df2_dummy
# count of words present in description column #
# We can see that the number of repos created was the highest in the middle of 2016
# Let's try this out with "device model" first - only about 7k values
# 0.905058312759 # Tells us how significant our z-score is # 1.959963984540054 # Tells us what our critical value at 95% confidence is
# Columnas
# looking at data to check for outliers
# 1.959963984540054 # Tells us what our critical value at 95% confidence is
# create a DataFrame of tokens with their separate one-star and five-star counts
# 2 # Remove three observations ratings for which are outliers
# Remap the `price` column by subtracting the median price
# len(df2)
# Import some packages to help parse the dates
# store the vocabulary of X_train
# the most popular words which might contain useful information in the reviews # apperantly we need to update the stopwords list # here I update 'aa' in the stopwords list
# Final clean csv.  Other csv's below are for separating categories
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# Row where user_id is repeated
# create a period representing a month of time # starting in August 2014
# by year and in-season vs. offseason
# merge this with stacked_image_predictions table 
# Create an engine using the 'hawaii.sqlite' database
# print out details of each variable
# Dropping of duplicates
#df.info()
# write your code here 
    '''$     Load in pickle for news data over selected period.$     '''$
# checking for NaNs
# Use Pandas Plotting with Matplotlib to plot the data
#Finding the duplicated row in df2
# create the response variable
## YOUR CODE HERE
# perform an exponential function to the coefficient to get the probability between 0 and 1
#lOOKS FOR THE NUMBER OF UNIQUE USERS
#Query 12 months of precipiation data. 8/23/16-8/23/17
# Create time series for the data:
# create from ndarray
# Simulating for old page
# Finding the different types of dtypes
# sort dataframe (hint: use .sort_values(by="column name", inplace=True))
#identifying the number of times new_page and treatment line up
#Cuts time and keeps date.
# passing a date to pd.to_datetime() yields Timestamp. # passing a series of dates yields a DatetimeIndex
# let's check which colums we have
# drop low because it is really common #df7.loc[df7['avg_health_index Created'] == 'low', 'avg_health_index Created'] = np.nan
# Remove any games with bad dates that must be excluded. # Don't need to worry about 2012 games because of the lockout. # Remove anything after traffic time range.
#Subsetting out the total counts from the table and  #Transposting the row to a column for plotting
# By creating dummy variables to codify the type of bill as a variable, # we can use it alongside densed count vectorized text. # Here we also convert the bill text to all lower case in order to better match word instances
#dfX_hist.set_index('race_id','runner_id','name', inplace=True)
#### Test ####
# Change the order of the columns
# Preview the first 5 records of the DataFrame using `head()`. # There's also a `tail()` function!
### Create the necessary dummy variables
# means by month:
# How many stations are available in this dataset?
# Reading a csv file with the read_csv function
#Reading back from the pickle file
# 24 hour for training same hour for validation
# Access a collection (create it if not exist)
# Summary statistics for the precipitation data
# transform key dates to correct data type
# To get the frequency of each component failure 
# check some of those features
# query to pull the last 12 months of precipitation data
# Describe the variable 'Number of people injured'
# get the head of the dataset
# The number of unique users in the dataset df2
# Imports the Google Cloud Client Library. # The client must be created with admin=True because it will create a table.
# Create new column defining high: # comments > median and low: # comments < median 
#converstion rate
#res3.text
# create new feature : "car_age" , how old car is 
# Returns a string containing previous collection date in EpiCollect table
# Viewing the edge_types file
#words = jieba.cut(data.content[286], cut_all=False) #for word in words: #    print(word)
# load panda dataframes into sqlite databases
#get metadata info
# Import library # Implement and fit logistic regression model
#  find the number of rows in the dataset
# Create an engine for the hawaii.sqlite database
"""$ Check shape and confirm that no NA values are present$ """$
# Final Plot 2
#Users by cohorts
# extract mean of lengths
# Temporarily save the data, to avoid having to run the first bits of code  # again in case this notebook needs to be re-run/debugged...
# Different entries for each index
#Example 3:
#row with missing values #checking the results
# p.242 Processing documents into tokens # split the sentence/corpora into individual elements
#simulating the Pnew. 
# initialize the twitter API object
# Get some general info on the dataset. # Int is for integer, float for floating point (number with decimal), object for string (text) or mixed types.
#Check duplicated rows by contractor_number since contactor_number is unique per contractor per jurisdiction
# probability of new page
#Read the header of the dataframe
# cfs_df.Zip.unique() # cfs_df.groupby([cfs_df.Zip,cfs_df.Beat]).size().nlargest(5)
# Define number of Training samples (70 %), Validation (15%) and Testsamples (15%)
# call edftocsv function
# Get count of all sources
#mktime(current_time.timetuple())
## Simple enough - let's use a loop and our new understanding of dicts to print out the mode and route name:
#create new column Key with value as 1 for both the dataframe as this would become the common key to be merged
# Apply 'get_gender' function to all names with missing gender 
# selecting just the vendor id column
#make a copy of dataframe #converting normal values to log values using numpy's log function to remove anamolies
#converting entire dataframe in string
#Read first few rows
# Backward filling # To do in-place updating # store_items.fillna(method='backfill', axis=0, inplace=True)
# Create a "tallies.xml" file for the MGXS Library
# Make a train/test split using 20% test size
# Create control group subset as df2_control
# When the completed dates are missing, the complaint is still open
#clean the data
# Segun se indica en DataDictionaryBuildingPermit.csv permit number es del tipo number pero vemos que lo interpreta como object # De la simple revision se puede observar casos donde el permit number es MXXXXXX y parece estar relacionado con permit_creation_Date
#checking for duplicated tweet_ids / observations 
#We use the isin function as was asked to return baseball's entries that match with desired statement # Write your answer here
## Dump Results
#We create a column with the result of the analysis: #We display the updated dataframe with the new column:
# Replace NaN data with a high and invalid number which is -99999 # We're saying we want to forecast out 1% of the entire length of the dataset into the future
# Use the pivot function to make rows into columns with only the popular community centers
# AUC 
# Drop the original name column
#filter: the total history of commits for nova, cinder, horizon, fuel, neutron, keystone, heat, glance, tempest projects - just have to change project name in the code below
#Looks like a typo 
# Creating a new dataframe, df3 for this part to avoid any confusion 
# Replace stage with `doggo` for the records whose text contains the term `doggos`
# Calculate the date 1 year ago from today
# -> Having more life enjoyment and satisfaction leads to better outcomes
#Exploring the dataset #We can see thet there are high number of 5-star reviews
# here, name-based index created above will be kicked out altogether and replaced by the  # default numerical index
# Combine ET for each rootDistExp # add label 
#f3.columns #Yes, they have been 
# Next step: only select the RIDs that overlap across all 3 diagnosis datasets # first make a list containing all the unique RIDs in each dataset
# Retrieve last 12 months of data, last date - 365
#merge sessions with test data
#create a groupby dataframe grouped by genre #calculate the mean score by genre, print out the results
# Checking for both country and  Page together
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# Combine ET for model representation of the lateral flux of liquid water # add label 
# Take a look at the words in the vocabulary #print(vocab) # Sum up the counts of each vocabulary word
#los df que tienen la info del postulante (sin relacion con el aviso)
#code to import the json objects from the .txt file into a dataframe: #read .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count.
# The standard deviation for the sampling distribution for difference in means
#check the value, in order to determine which is a duplicate.
# Then sends this new processed data frame table to a csv in the Data folder 
#Get the original input matrix
# Retire column was infered as a float data type
# Show rows with invalid data
# Remove a column by label # Check to see if df was modified (if not how would we modify it inplace?)
# cisuabd4
# Stats model logit() and fit() countries
#query for having only the rows that have group and landing page are consistent, (treatment,new_page) OR (control,old_page)
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
# grouped_by_letter.get_group("O")
# the cursor - holds a result
# import matplotlib # matplotlib.pyplot.plot_date(df['newDT'], df['ED'])
#Now try the same thing, but for the end of the dataframe, called the "tail". 
# Create: vader df
# % of pos/neu/neg
#Test
# This fills them with the previous year. See also temp3.interpolate
# odd that they had ads after election day (runoff elections e.g., Louisiana) -- will filter out post 11/8/16
# rename 'id' column in tweet_json_clean to 'tweet_id'
#authentication #connecting to Twitter API
# check tail
# Notice how the data did not contain headers - let's try again, this time passing  # in some keyword arguments
# Let us just impute the missing values with mean values for the time being
# read csv directly from Yahoo! Finance from a URL
# Find the proportion of users converted
#== Drop any rows having missing data 
# set index for team_games df
# And negative ones?
#kind can be 'hist', 'scatter'
# Count proportion of p_diffs greater than the real difference
# Create dummy variables
# Or, we can fill nulls with a default value
# apply function to get stats
# Read data into a DataFrame
#Simulate Nnew transactions with convert rate of Pnew
# Verify data
# Total size in GB
# Only needed so this notebook can be run repeatedly
# Example
# Write Python object to string
# Can you count something interesting?  # Printing number of successful, canceled, live and suspended projects and  # relative numb of backers per year per state.
# fill na with previous value (to fix empty gps and empty accelerometer)
#searching for href
# Graph of the Polarity by hour for a specific day
##Replacing days of week numbers with day names
#rows of the repeated user_id
# analyze validtation between BallBerry simulation and observation data.
# return the original input when unparseable
# Drop users who don't have at least 4 days # Keep only seqn for joining purposes
# Read the newly created csv back in as "numberOneUnique_df" for later merging with the original "spotify_df"
# Display the results
# The x, y, lat, and long nans go together
#groups the dataframe by the result list
#Remove local version of date
#renaming name column values which starts with lower case characters as 'none' because those are not actually names
# proportion of p_diffs greater than the actual difference observed in ab_data.csv is computed as:
'''Evaluating the model with the Test (Hold Out) Data'''$
#the min is 0, there are 20952 of them
# The protocol version used is detected automatically, so we do not # have to specify it.
# function takes in the datetime string of the cell and returns the datetime object
# quick and dirty investigation for 'RegDateOriginal' and 'RegDate'
#delete * gender type
#take a peek at each individual track
# Recheck duplicated row
#ANOVA F Test
# drop the identity column from progresql
# Write data out to an excel file
# Summary statistics for the movies data set
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
# flight = spark.read.parquet("C:\\s3\\20170503_jsonl\\flight.parquet")
# Author: Evgeni Burovski
# CREATED A NEW COLUMN WITH THE RESULT OF ANALYSIS:
#Lets remove all javascript and css.  Printing each piece for educational purposes.
# labels dict contains IOB (inside-out-beginning) labelled entities # printing some randomg k:v pairs 
# Create second-level index for df based on begining and end or data range
#Distribution under null hypothesis
# Total hours
#Make dataframe a datetime index
# The highest rated dog is called Atticus, it has a rating of 177.6
# load the endometrium vs. uterus tumor data
# act_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() # New conversion rate of control group
# lq2015_q1.sort_values(by='Date')
# loading the processed csv containing the urls for all of the op-ed pieces
#import pattern
# check the blob, is it really there? # yay! it works!
# print the confusion matrix
# Create a dataframe from the first series object using the dataframe constructor
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# gpByDate.plot()
### Create the necessary dummy variables #df_new['country_uk']=df_new['country'].apply(lambda x: 1if x =='UK' else 0)
#the url for one dream from the user 'fataverde' posted five days ago (today's date: 1.9.18)
# We should make sure there are no collisions in column names.
# output and submit to kaggle
# count the number of rows
#etsamples_100hz = etsamples_100hz.reset_index() #etsamples_100hz.loc[:,"eyetracker"] = etsamples_100hz.level_0
# Understand the various Marketing Channels in the data
#Let's check it out
# removing in_reply_to_status_id and in_reply_to_user_id as it only has 75 non null values
#convert to arrray
# Search for '#Aussie'
# We replace NaN values with the previous value in the row
#Show movies with language 'th'
#Find Precipitation Data from the Last 12 months
# Here the benchmark is iShares Nasdaq Biotechnology ETF 
# Plot histogram
# now reset the index
# select fileid with the category filter
# Set location of file
# We display the updated dataframe with the new column:
#read data in from pickle file
# We should remove the 3893 rows.
# Print a tweet
#The date format is correct
# Generate some samples from the state space 
# create df copy for use w/ predictions # tail.tail(2)
## ratimg numerator ##
# Checking for any duplicate user_id values
# Just get rid of those annoying |
# write data to csv
# Can scale teh columns in percent_columns.keys() + '_HOME' or '_AWAY' using StandardScaler
# Or it could be access by the position of the column (as usual starting from 0)
# Visualise the data for the 100'th day in 2018
#version2
#### Visualizing the data
### Plot the important variables ###
## Retrieve all of our original tweet textx, tweet id, user who tweeted, and tweet hashtags. Ignore _id which is  ## MongoDb's id of each record, ie. a 'primary key'.
# Proportion of users converted: 0.12104
#Check values for a STATION
#grouped_months_liberia.head()
# Load the query results into a Pandas DataFrame.
# for testing: # hp = houseprint.Houseprint(spreadsheet='unit and integration test houseprint')
# subset to included participants 
#Check for non null values
# save to new csv file
# check how the features are correlated with the overall ratings
#Join weaher and crime dataframes to create feature matrix
#Displaying row information for the repeat user_id
# sort in expiration order    
# shows time stability
# Assign NaN to all entries in L3 equal to ' '
# ix index is a hybrid approach
#gmap = gmplot.GoogleMapPlotter(40.7128,74.0059,8)
# Add season column.
# Use the create_engine function to create the connection to the target database...Note: this doesn't actually open the conn.
#P-New
#trainR.iloc[:,:101]
# tl_svl and mass_svl
# New dataframe
# Use the Inspector to explore the database and print the table names
#no missing value in df_eng
# from lists
# load data into H2O object
# How would you modify the above cell to do the same reordering, #   but at the same time, remove one, say the one labeled 'e' # Write your code here...
# load the .env file
## There seems to be only one row with an invalid zip code. Let's drop it.
# We create an extractor object: ## https://developer.twitter.com/en/docs/api-reference-index
# TWO-WAY SPLIT # group by season and team
# select a talk with multi-speakers as test
# there are 3,893 rows where new_page and treatment do not line up
#Do Data Frame queries
# This means that I can use series-methods like .head():
# create a trial parameter object to check root distribution exponents
# let's check if there's data
# Let's have a look
# Make the random forest classifier
# Separate response variables from predictors
#proportion of users who converted to other page
# Subset to Enrollment Level only # remove colums with all na
#### Test ####
#135.675300,34.955205,135.795300,35.055205
#Row info
# convert ticket_closed_date_time to a pd.datetime field
# I DON'T KNOW WHY THIS ERROR IS HAPPENING
# Dataframe for alpha values (transparency)
# We can use dateutil to parse dates from string formats
# que no introduzcan ruido en nuestro analisis
# Look at some 1 word ticket
#result_liberia
# Get all of the Variables from the ODM2 database then read the records # into a Pandas DataFrame to make it easy to view and manipulate
# loading the first file into a data frame, and visualising it
##Plot histrogram of dates of birth
# add a notebook to the resource of summa output # check the resource id on HS that created.
# Show the weighted mean polarities of the extremes # Take average polarity and multiply by # of tweets in that polarity bucket
#simulate under normal
# To provide the results summary
#Add intercept column #Create dummy variables for "group" #Drop one dummy variable column 
# Sort the data by time, so rows are in chronological order
# How many stations are available in this dataset?
# Conversion probability regardless of landing page
# unigram chunker 
#check for the column names and their datatypes for a particular table 'measurement'
# Printing the content of git_log_excerpt.csv
# Obtain the source names for reference
# take a look at the first two lines #!curl 'https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv' -o Consumer_Complaints.csv
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# created data frame with start stop times and dates for all tests
# df_goog.Open.asfreq('M', method='backfill').plot()
## check to make sure NaN appears for the first row for an SCP
#Code
#Select records for 2017 #Group by site & compute mean TotalN
# Define random seed for reproducability # Define sample size # Generate sample with replacement
# check if there is any null value
# Local backup: data/sea_levels/sl_nh.txt
#Now we will break the data up by country and see what effect the page has on conversion
# rotation number is utilized to determine which days and which games are unique # start times for contests are often pushed back to due delays (rain, etc.) # but these are not unique games; needed a way to group all odds for particular games together
# drop any duplicates across ALL columns (not sure why they would be there though???)
# might be affected by timezones, so look at just US. # looks like there's a spike around lunch hour (1-2 pm) and around after work 
#determine 50 most frequent job titles
# Deleting mismatching rows: # drop rows for mismatched treatment groups # drop rows for mismatched control groups
# Counts of non-null values for each attribute.
# Finally, let's get the content
# draw the boxplot of total based on different types
# we can do ?pd.read_csv or just check the  # documentation online since it usually looks nicer ...
# Name of timestamp to parse on the raw data file
# Find pathways involving cancer
# Busqueda de valores nulos
# plots the histogram
# lets fit logistic model for the countries with ab_page, baseline = CA
# Mean calculated on a DataFrame
# Example 1 # Example 2
# Rename the rating numerator 
### Fit Your Linear Model And Obtain the Results
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#interpreting coefficients
#That cut our number of tweets nearly in half!
# NLTK has a lot of functions for analysis, for simple ones like this to very complex. # Let's look at the 20 most frequent words
# The standard deviation of the sampling distribution for old_page
################################################## # Load   ##################################################
#We choose top 20 brands for further analysis.
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# Import statsmodels
#replace outlier with nan for correct mean calculation #replace with mean for now. Todo find a better way to replace them. 
# View value counts of customers
# Access data
# Combine each stomatal resistance parameterizations # add label 
# add date column
# Convert to numpy array # Plot sampling distribution of the simulated difference in means  # As we would expect from the central limit theorem centered at 0 and normally distributed
# and here it is! # warning about column 45 - yep the original weather events  # with categories and NaN, new column was made into a string and then gouped as seen above
#deal with oil dataframe in pandas with backfill, then transform it to spark dataframe
# We can even slice easily using the first MultiIndex (year in our case) # health_data_row.loc[1]  # doesn't work
# urban
# extract full monthly grid
# We still need to rename the columns
# merge with main df
#How many of the image predictions actually predict objects other than dogs?
#pd.set_option('display.max_columns', 999) #pd.set_option('display.max_colwidth', 999)
#Get the rows has appointment only
# Add 'd' days to calculate a new datetime
# Create an authentication using the given key and secret # Connect to the Twitter API using the authentication
# print some sample training trees
## check how many different users ## Should match the target non-cold-users aprox.
'''$ DO NOT CHANGE ANYTHING IN THIS CELL$ '''$
# Load the list of daily normals into a Pandas DataFrame and set the index equal to the date
# print(pd.Timestamp.min(df1['created_at']), pd.Timestamp.max(df2['created_at']))
#Tomo una muestra con el 80% de los elementos del set de entrenamiento
#merged2.columns, merged2.index
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#list unique values in offense description
# this is just for unscaling
#'Stockton': 'd98e7ce217ade2c5'
# Get the count from each source
# Double Check 
# using infos consistency from DataExploration 
# Below is the list of default classifiers which were included at the beginning of the code # clf_base_default = [clf_base_lr, clf_base_rf, clf_base_nb, clf_base_knn, clf_base_dt, clf_base_svc] # Pass the clf_base_default value to the 'clf' parameter
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned # Update link with start data filter, api key and data for the Frankfort stock exchange (FSE) 
# convert KOSDAQ ids & names lists to Pandas DF 
# split features and target
# Calculate probability of conversion for new page
# dropping NaN values
# train score
#See correlation with actual: CVS
# remove invalid data points (negative intervals)
#9.6 Modify Object Properties # You can modify properties on objects like this:
# Here's a pivot table instead - exact same output as a groupby
# set the new url and go to the site
#This puts data in order the frames list  #All nucleated data is shown then partial nuc then outside-out...as continues inside brackets
### combine the two dataframes into a new one  ### preview the combined dataframe
# Probability an individual received new page
#index objects are immutable
# calculating number of commits # calculating number of authors # printing out the results
## Solare Flare 2012-03-07   highest sigma event 
# Are the results only positive? # ANSWER: NO
# Export the data in the DataFrame into a CSV file.
# 6. What was the average daily trading volume during this year?
# total # of rows
# probability of an individual converting regardless of the page they receive
### Create the necessary dummy variables
# reading in cleaned up dataset
# Drop the numerator and denominator columns
# Partition the world cup data and the club data. We're only going to train our model using club data. # Show the features latest game in competition id 4, which is the world cup.
# https://www.reddit.com/r/Bitcoin/comments/
# The input data has the latest zip code for each donor. So we cannot observe any movement even if there was any since # all donations by a given donor will only have the same exact zipcode.
#Create a new dataset by filtering the rows where the group is associated with the right landing page
#abc = detailed_confirm_write_to_file_ALL_DRGs(50030,2014,6) # 50030
# View dataset coordinates
# set index to title
# Netstat
#Calculate the date 1 year ago from today
# Transmission 2030 [GWh], marathon
#Agrupamos por place_country_code
#Computing z_score and p_value #display z_score and p_value
# create train and test
# select an expiration to plot # get the call options # set the strike as the index so pandas plots nicely
#given in countries dataset with the ab_data dataset.  
# First overview of dataset
#This generates a series
# We load Google stock data in a DataFrame # We print some information about Google_stock
# Find the p-value and z-score by using the stats ztest built in
# We change the row label from store 2 to last store # we display the modified DataFrame
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Check the status. 200 means OK
# columns common 
#sum()
#== Series by passing list: Index is auto-created ==
# drop rows with missing value in specialty column
# Look at total events per day of the week # yay no need to group - looks good # Friday #1
#Convert Rate for Pnew under Null is simply 11.96% because for the Null hypothesis we are looking at the probability of converting regardless of page
# revise "gear_box"
#checking rows were removed successfully
#Combines the values from both tables, sorted and filtered by product. #prod_sort #sale_prod_sort
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#create a groupby dataframe grouped by genre #calculate the mean score by genre, print out the results
# dummying rate_experience 
# If you want more specific primitive :
# Stations with the highest number of observations
# Create dummy variables for landing page : ab_page
# example text
#18. Determine how much quantity of each product was purchased by each user
# load countries csv file # join df2 to countries dataset #preview
# rob
# determine number of converted users using mean(), since the column only contains 0 or 1 as values
# population is a string because of the commas. humm
# Create an engine for the `hawaii.sqlite` database
# given an individual received the control page, the probability of converting #print('The probability that an individual in the control group converted is: {}'.format(df2_control))
# Instances have a .__class__ attribute that points to their class.
# summary
#`refit` raises an error, so modified
#Importing movies dataset from Data folder
# Calculate the rainfall per weather station for your trip dates using the previous year's matching dates. # Sort this in descending order by precipitation amount and list the station, name, latitude, longitude, and elevation
#total=total.dropna()
# Compute p-value
#how many rows do we have with nan in a given column?
# 
#How does it look like
# Count the number of stations in the Measurement table
#checking the range of what we are about to input into the database.
# this ensures the plot appears in the web browser
# accuracy_score(actual, pred)
# unique names of the countries
# Create our data frame with features. This can take a while to run.
# Loading previously saved data
#Let's join our tables
#Drop empty columns(axis=1 for columns and axis=0 for rows;  #how='all' to drop if all values are nan and how='any' to drop if any value is nan)
# Read dataset csv file downloaded from Kaggle website # Check the basic structure for all features
# Hacemos display de los sentimientos:
ref_vars = '''id, totalDocks, city, altitude, stAddress2, longitude, $     postalCode, testStation, stAddress1, stationName, landMark, $     latitude, location'''.replace('\n','').split(',')$
#5mm records as result of query, but no data transferred #Set device_id = 2 to use open GPUs memory; MapD using device_id = 0 for its own caching
#fitfile = FitFile('fit_files/2912551983.fit') #cycling (example with good gps) #fitfile = FitFile('fit_files/2853423540.fit')#short swim
# remove emojis
# Distinct Number of Users
# Replace each NaN with the value from the previous value along the given axis
# grab the subreddit
#Tokenize words in articles
# find historical data for 2001
# rows have missing values
# load the data into the ipynb
### some general settings, common to all plots...
# Option 2 just switches two index levels (a more common need than you'd think) # Note: This time we're doing an inplace change, but there's no parameter for this method either.
# define 'id' as the index and convert 'created_at' to datetime format
#copy = train3 #copy['duration'] = duration 
# df.head()
# Convert timestamps
# same as above
# Probability an individual recieved new page
# There seem to be 27 police districts, 0 through 25 plus 31
#query to calculate the total number of stations.
# rounding the .5, 4.5 ratings for drivers
# As discussed earlier, this is much faster # than doing the addition via a Python loop or comprehension
# set up dictionary index # index name city name for uniqueness, and *dictionary* to allow ES queries across all datasets
#Add the MGD column to the dfHaw table
#Fit new model
#assign the column names back to the datafram
# get today's date
# Write the predictions DataFrame to a .csv file
# Word frequency for terms only (no hashtags, no mentions)
#Calculating z-score # Calculation with 95% confidence level
#print the field which have more thatn 90# not null values
# DATE CRAWLED
#alternative method to avoid workaround : results.summary2()
# from pyspark.sql import crosstab # from pyspark.sql.functions import * # flight.columns
#babies uid == BABY_UID
#This time distance weighting scored worse #Let's try now adding some more dummies #Recall which columns we have
#view data from that has a decimal value from the "text". The reviewer pointed out that the the regular expression made an error and only extrate one number. 
# read the dataset # show the top few rows
#Merge 2 dataframe
# number confirmed NOT foreign service
#full_image =  #featured_image_url = soup.find('https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16715_hires.jpg') #for image in images:
# return NaT for input when unparseable
#since new_page_converted and old_page_converted have different sizes,  #I use the mean to get the difference
# your code here
# Store the cross section data in an "mgxs/mgxs.h5" HDF5 binary file
#another way to do it, by using list comprehensions
#big_data_station_mean = big_data_station.EXIT.mean() #print('Min:', big_data_station_mean.min()) #print('Max:', big_data_station_mean.max())
# tweets.apply(lambda row: print(row['tokens']), axis=1)
##Get the row at array position 1
# checking the number of rows with new page # checking the number of rows with old page # calculating the convert rate
# NLTK's punkt includes a pre-trained tokenizer for english which can # be used to transform (split) new paragraph observations into sentences
cur.execute("""SELECT created_at, text, location, time_zone, geo, lang from tweet_dump$                order by id DESC limit 10;""")$
#checking contents of dataframe
# Reassign data frame without null value rows in name column
# Find the max of the 'Average Fare' to plot on y axis
#sanity check to make sure the timestamps of our records are what #we'd expect; the below selection looks good, the Timestamps on  #the right are an hour before the Timestamps on the left...great!
# 
# df_data=pd.DataFrame({'time':(times+utcoffset).value,'chips':target.values()})
# accuracy_score(actual, bm1)
# Verify the observed conversion rate by country
# Re-split data
# and then reload it...
# Converting my tobs_date list to a dataframe.
# final sise check
# Make an empty dataframe for the transmission capacities between the countries
#scrape just 5 of the threads
#get the second element of the list in metro_area_state to be state
#data.dropna(inplace=True)
#Example 2:
# Remove rows where df['Injury']==0
# selecting two columns # enter code to select two columns. It will look something like: dataframe[[column 1, column 2]]
#Create data frame with stations and counts
#Look for duplicates
# output full forecast dataframes to csv
# Locais no dataset de atendimentos
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Number of missing - You can investigate these more later
# tweets
#Plot histogram
# with the below the Confidence Intervals for the predicted values of revenue are calculated
# read the oracle 10k sections
# Getting chromedriver to retrieve html from reddit, not sure what the timer is for exactly need to get info # Giving it 2 seconds to completely load the reddit page before scraping
#Create custom stop word list
### Income is log-normal
#Number of rows from new page are higher than the ones on old page, therefore we truncate new page up to the numbers of old  #page and compute the difference
# confirm that 'train' and 'new' together contain all the rows from 'hn'
#CHALLENGE 5 Answer #serieschallenge5[0:1000]
#obeserved difference  #pdiff > obs diff proportions
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#describe
# Check if all cities are valid. # Select all cities that have less than five instances in the Data Frame.
# Cretae the DateTime column from the TimeDelta column that we have in the energy generation data
# Read in the ACS DataFrame, dropping the meaningless 'Unnamed: 0' column that is an artifact of writing the # DataFrame to a csv file
# which row has the longest ingredients string
## Creating series form dictionary
# filter out Twitter links
# 1.959963984540054 # Tells us what our critical value at 95% confidence is
# Controlling for a common coding error when you try to fit a model that also # has a variable "fit"
# arquivo com os dados da planilha excel
#Plot bar charts of the three summary tables
# Scrape NASA Mars News # URL of page to be scraped
# Mapping using BLAST
# Remove any rows with null values in the property_description
# Rejoin this set of ids into the pax_raw dataframe
#'Pittsburgh': '946ccd22e1c9cda1'
# a simple logistic benchmark. only knows how often defense shifts and how often batter is shifted against. 
# We display the salary distribution per department per year.
# this is the default, raise when unparseable
#https://stackoverflow.com/questions/16729574/how-to-get-a-value-from-a-cell-of-a-dataframe
# Reads the CSV file using Pandas
# Now the Processed data
#model 2 is accurate .75 out of 1
# Find all the plays mentioning Santos
#Create a database and connect with sqlite3
# num_comments along the time
# first attempt, try to include all of the punctuation on the title as the valid datasets # but beforehand, we need to remove the None and NaN val
# Writing to CSV files
#return the last 12 months
#converting time_stamp to date time
#we extract menas of the length
#Number of users who have been assigned with old page
# We print the number of non-NaN values in our DataFrame
## print culstered result 
# 1 quarter changes regression: later half of the sample
### Fit Your Linear Model And Obtain the Results
# Make Predictions and output the results
# Upper-case the home.dest field
# Checking tail for data verification
#Selecting the interesting features #Adding a column containing the target
"""Fill missing age values with median age. I decided to create a $ new column for this in case I wanted to come back later and regroup original $ age into categorical buckets with a 'None' category.""" $
# check rows with shape method
# remove those that don't have a week 0 value
### Create the necessary dummy variables to perform regression in terms of the US
# we can calculate the proportion  of users converted by taking mean 
#Nasdaq 100 Company list
# Save the updated spreadsheet
# Drop the address column.
# Use Pandas to calcualte the summary statistics for the precipitation data
# divide numerator column by denominator column
cur.execute("""UPDATE empvw_20$                 SET salary = 1000""")$
# save the model
# change the datatype of tweet_id # change the datatype of tweet_id column in status_df as well
# slice the dataframe and only keep some columns
# The number of rows in ins # The number of unique business IDs in ins.
# the set names form an index, and the columns are the attributes of each set (set name, release date, cards, etc)
# get just the first day in Feburary
#total=out3
# either it opend at high are closed at low
#create a dataframe of a random samplesize of the master dataframe to determine which variables impact the target variable most
# workaround for a missing function problem in scipy.stats # https://github.com/statsmodels/statsmodels/issues/3931#issuecomment-343810211
# read CSV
# Drop unused columns
# plot the model error
#getting summary of large dataset
# Count node types using the aggregate function
# normalize original tweeters for 
#dropping some data I no longer need
# Let's use padas dt.dayofweek (Monday=0 to Sunday=6) and add it as a column called 'DAY_OF_WEEK'
#check if column is deleted
# 5.What was the largest change between any two days (based on Closing Price)?
#find out which column has any missing values
#Wait that's so cool! Emojis show up in jupyter notebook! #I'm noticing a lot of these tweets are retweets
# Creating the list of files in the folder
#fraud_data_updated = pickle.load(open('preprocess_second.p', mode='rb'))
#sn.distplot(a.A.flatten()[:],norm_hist= True,bins=60 ) #sn.kdeplot(a.A.flatten()[:]) #sn.distplot(a.A.flatten()[:], hist=False);
# a query to get the Twitter place_id (code) for each city
# read in a dataset to print columns
# que created_at sea tratada como fecha # La convertimos:
# What proportion of the p_diffs are greater than the actual difference observed in ab_data.csv?
# shift just the index values
# Number of missing observations per element.
# plt.subplot(2,2,4)
# There are 30471 rows and 292 columns in the data.
# Caption is having issues in TF-IDF  # it is bc of the numbers...
# Plotting the number of events per waterway
# calculate the converison rate of whole dataframe2, regardless the group type and landing_page.
# Plot toy dataset per feature
'''$ u.data     -- user id | item id | rating | timestamp. $               The time stamps are unix seconds since 1/1/1970 UTC  '''$
#Confirm removal of one of the lines
# characterize the data #  number of words, don't remove punctuations and other non-alphanumeric characters
#fit the linear model
# Get times that control and new_page line up
# outer join be default
# statistical information related to the data frame
# show you the table after clean work 
# A trivial plot, easy to generate thanks to GeoPandas
### Fit Your Linear Model And Obtain the Results
#119998 
# make my acutal outcome column y
#import FB data
# Probability user converted(Conv) given that  user in control group(Ctrl) # Can also be written as P(Conv and Ctrl)/P(Ctrl)  #Users who converted and were in control group
# store the first title
# df.head()
#Test
# We change the row index to be the data in the pants column # we display the modified DataFrame
# Tell pandas to add up all of the values for each key
# create copies of the dataframes to work on:
#renaming second column 
# last 12 months of temperature data. 2016-8 - 2017-8 # Select only the date and prcp values.
### Remove all Blends and Wine Types made with more then one Grape
# Import libs and read dataset
### 3a. # show which country has highest number of visits in entire dataset
#Reading file ab_data.csv
#Design a query to retrieve the last 12 months of temperature observation data (tobs). #Filter by the station with the highest number of observations.
# new_df = pd.DataFrame(columns = ['hour','minute','hourfloat','x','y'])
# calculate the temperature difference between the two cities
# Get json object with the intraday data and another with  the call's metadata
#cars2.head()
# Reduce words to their root form
# first I must extract the hashtags from the text
#Got the run ID from Watson Environment.
# which values in the Missoula column are > 82 ?
# The Observation of differences in conversion, grouped by 'treatment' and 'control' groups:
# Passing keys= gives names to columns when using axis=1
#Removing one negative value in registred via
#add query1 rows count + query2 rows count Ans (1965+1928 = 3893)
# proportion of p_diffs greater than the actual difference observed in ab_data.csv is computed as:
# Now we are justing picking up the randome code without the '/watch?v=' in the string so we are getting  # from the 9th index to the rest of it. 
# https://developers.google.com/maps/documentation/geocoding/intro
# Train for one epoch, just the embedding layer
#strong correlation between favorite_count and retweet_count
### Step 19: Convert dates into days of the week  # a) convert object into DatetimeIndex
# # convert date columns to datetime 
# Print the value_counts for 'State'
### Create the necessary dummy variables
# Get a list of column names and types # columns
# We can use non-contiguous or non-sequential indices:
#verifying the change
# Earliest Date
# 10. 
#train_data.shape #test_data.shape
# do the rest here (potentially the next day when quotas are refreshed) #print('title:{} artist:{} url:{}'.format(title, artist, youtubeurl))
#no_description = df[df.description==''] #df = df.drop(no_description.index) #pp_bold('Ignoring {} listings, {} remain'.format(len(no_description), len(df)))
#converting tweet_id column to string datatype
# save data to CSV
# To produce pie_chart by city type #we group the original csv read file by "type"
# Find unique users in the dataset.
# Verification that the columns "price" and "odometer"  # (also we changed his name to odometer_km) has changed  # to type in64
# Simulate n_new size transactions with a convert rate of p_new under the null
#Remove the NaN from the dataframe
# List stores
# Settings the "station" as the index
# continue here to extract Median Listing Price and to split the data using train_test_split()
# 1-2 (2) using the `with open() as file` to store http response into a file
# Leemos el archivo. Veamos si tenemos algun inconveniente...
# Step1 : read from the given csv file and store it
#Group by Target, aggregation average by Compound 
#pivot table showing the Age and affair status
# Save the query results as a Pandas DataFrame and set the index to the date column
#Convert to array
#displaying n_new which is equal to the number of rows with control group
# convert date columns to datetime format
# print some configuration details for future replicability. #hdfs_conf = !hdfs getconf -confKey fs.default.name ### UNCOMMENT ON ALTISCALE
# import package for datetime  # datetime parser #creating range of dates for 201
# p_new = p_old, copied from above , should give 0.12
#probability converted given that that the individual is from the treatment group
# Explained variance score: 1 is perfect prediction
#change 'tweet_id' column type to object type (i.e. strings)
#assigning meanngful indexes
# it can be useful to convert flat data into a multiindex
# Partition the world cup data and the club data. We're only going to train our model using club data. # Show the features latest game in competition id 4, which is the world cup.
# accediendo a una fila por etiqueta y obteniendo un dataframe
#Convert "Month" column to same date-time format as the other so they can be merged
# Rounding the numbers to two decimals
# plotting different realtionship between vaibles
#converting date to datetime format
#Create dataframes for the other features so they are able to be merged
#load pickled dataframe
# Create logit_countries object # Fit
#Step 2 in filtering: Applying the masks using logical combinations
########################## Check Models Folder #################################
# the `self` function of Engine has an attribute to return the data as a pandas df.
#So it looks like 1014-885=129 of the original images were deleted in the deduplication, I think...
# Assign the class to a variable 
#adding new numeric columns
# Drop NAs
# Display name and importance
# Stip off the year and save a list of %m-%d strings
# Take a look at the top few rows of df2
# Check dataset with one removed row
# Return Apple stock price
# New Age Distribution, replace lots of NaN's with median, which is 34.
#1 we are going to put some other values instead of NA
# mean Closing price for specific month:
# Process data using Natural Language Processing techniques: clean, remove stop words and lemmatize.
# Calculates how many people DID NOT indicate ageRange
# check if there is any minus number between dates
## WE can do plotting according to Close volume
# To calculate this we check difference from original dataset ab_data.csv 
# Try out other methods and bound variables!
#Draw samples from a binomial distribution
# concat grid id and temp
#Next, production_countries contains JSON data, so load it as a column of dicts
# URL for reddit
# outliers?
## makes plot text lowercase ## removes punctuation from plot text
# Read the variation of heat demand from Data.xlsm
# Here it should have merged 2001 to 2001 index and so on because all the other valeus are the same but it did not.
# select all rows for a specific column
# Access local file
#utilize the description column to count
# RETRIVE TWIT IN SHORT MODE (as-is)
# Create a date value # Syntax: pd.datetime(year, month, day, hour, mins)
# I encoutered this specific issue with missing function chisqprob  # in my local environment this is why I run the code below
#Save News Tweets to csv
#Export Canadian Tire file to local machine
# It looks like the "Created Date" and "Closed Date" columns contain strings, not actual dates.
#Dictionary of Outliers
# a Series can be added as a column to a DataFrame
# if necessary delete the parquet file
# Assign a whole column
# df_2008
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#Let's see the bottom values as well.
# apply linear regressor
# Exporting to be read by Gephi for better visualization # nx.write_gml(G,"data/theplazz_politics.gml") # export for R-ergm
#Converted State Bottle Retail to float type and removed $
# Users and organisations
# In the column 'raw', extract single digit in the strings
#plot Null distribution and vertical line for observed distribution
#Find out the proportion of converted in the control group
# Abbreviate 'Southeast' and 'South' to 'SE' and 'S' respectively.
# now I want to replace some scores into numbers
# LOAD FROM HARD DRIVE
## Some columns are too spare to calculate bivariate stats out
#Urban cities driver totals
# Given that an individual was in the control group, what is the probability they converted
# does not recover exactly due to insufficient floating point precision
# Load data.
# compute difference from original dataset (df2)
# Note: x-axis is the index.
# Add stop name from oz_stops as column in stops.
#sanity check for NaNs
# Create a feature DataFrame for this week's games.
#Normalize nested data structure to a data frame format.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Making sure we have the right number of predictions
#Converting JSON object to a python dict
#joined_test[v] = joined_test[v].fillna(0).astype('float32')
# prepare for merge with temp df
# simulating n_new transaction that have probabiliy of receiving new page, using binomial gives the outout as the number of 1's
# Design a query to retrieve the last 12 months of precipitation data and plot the results # Calculate the date 1 year ago from today
# export DataFrame - columns option (similar to index with primary mapping being column name)
######################## Data Prep & Test codes - START ###########################
#plt.plot() #autodf.price.plot(kind='hist',bins=100) #plt.show()
# Get people in organization
#determine unique countries
#calculate daily total number of transactions, average transaction value
#Simulate the new transactions using random choice function with 1's and 0's
# How many stations are available in this dataset?
#plot weekly actual and forecast
# Summary Statistics for Station - USC00519397 WAIKIKI 717.2, HI US
# Tells us how significant our z-score is # Tells us what our critical value at 95% confidence is
# Locating row information for duplicate user_id
# drop rows with missing value in specialty column
# Print some info from the first tweet:
#Checking the new csv
# Set data for 2018 & 2019
# significance of z-score
# Import CSV file
#create dummy variables #df_new['country'].astype(str).value_counts()
#Save compound data in csv file 
# comes with a generated readme
# Count the NaNs
cursor.execute("""SELECT * FROM coindesk""")$
#this reads the json files from a specified column. In this case, the gps files from the validation study (+a few others #that we'll have to filter out later by created on datetime.
# Logistic regression confusion matrix
# Plotting the total duration of events by waterway (have to convert type for plotting)
# Calculate basic statistics.
# difference in average conversions for above simulations between control and treatment group
#Most commented posts
# Check
# Load packages
#Convert the column headers to a list, then rearrange the list. #Makes the last column (ID) be the first column
# columns in table x
# distribution plot without the KDE
#5 dropping the NA values
# instantiating weather feature dataframe
#Select the flow data (4th column) for the 100th record
#16. Determine which sessions occurred on the same day each user registered
# Let's make some recommendations
# All Tuesdays that behave like Weekends 
# Plot the distribution of response time again
# The histogram plots with trip distance values ranging from 50 to 150 miles
# Encode day_of_year as cyclical
# Setting up dataframe
#Read canadian tire tweets csv file
## EXAMPLE # Plotting DataFrames
# Choose the station with the highest number of temperature observations.
# Logging functionality
# Using the transpose function to make the rows columns
# classify the device type # remove remaining device columns
# Using inplace raises a warning, so used assignment here instead.
#Find the largest change in any one day (based on High and Low price) #Verify new column is correct
#Need to rename column for processing here because it has a space
#Calculating the treatment group mean by filtering data using query.
#Quitamos los lang en Null ya que no se puede identificar la localizacion del usuario
#!head data/goog.csv #On non-windows systems
# How many stations are available in this dataset?
# Use Inspector to print the column names and types for table measurement
# From a dictionary
#Football International Friendlies data
#df2['landing_page'].mean() #df2['landing_page'].value_counts()[0]/len(df2) #df.converted.sum()/len(df)
#now we create a pandas dataframe for the tweets #print out some examples 
# Checking our shapes. We see that "test" has an extra column, because it still has "id". We'll drop this column next  # after using it to prepare a submission dataframe, where we'll put our predictions
# Display Time-Series data price of Ethereum in Bitcoin
# number of unique items per feature # review print how to get the numbers lined up -- see Course-Syllabus 9.6
# WalMart Kaggle Competition evaluation metric #This basically shows the error rate #We want it to be as low as possible
### Fit Your Linear Model And Obtain the Results
# Export all tallies to a "tallies.xml" file
#Combining all three datasets(tweets1,tweets2,tweets3) together into a single dataframe named tweets.
# Run Cross-validation
#drop rows where all columns are NaN
# calculate the total number of stations. #station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'})
#generally greater favorite count count with greater rating numerator 
# impute missing attendance values 
#============================================================================== #==============================================================================
# ab_data = drive.CreateFile({'id': '1OWPen9Pzgmnr-pZ63pvv6t93oWbpooEW'}) # ab_data.GetContentFile('ab_data.csv')  
# Get avg compound sentiment
## Check if there is 'None' in 'tweetSource' column ## Check how many tweets are we left with now
# checking the last viewing in the experiement
# Inner join pax_raw with the rows we want to keep
# answer
# Compute difference from original dataset ab_data.csv # Displaying the actual difference
#preprocess 
#Trading volume occurs at [6] #Using the statistics library
# 2015 so far
# define the feature set and response variable # drop the response variable from the dataset # drop the identify index column from progresql
#data['Created Date'] = data['Created Date'].apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))
#data = pd.read_json('comment.json') #data_m = pd.read_csv('movie.csv')
# eliminar datos nulos
#Let's bring back our datetime values from index to a column in our dataframe
# Printing multiple outputs per cell
#Pulling in state victory margins data  #df_state_victory_margins = pd.read_csv('./state_victory_margins_data.csv')
# turning weather table date field to date field -  # although they look the same - weather date column (EST) is Object type # adding same column name "dateShort" good to keep schema the same when possible
## The model was created above: ## It should be noted that they do not accurately specify the time constraints for these models!!!!!
# rows where the control group lands on new_page
# read the csv file  # print the first 5 rows of the dataset
# Convert a matrix to a dataframe # Display the top 20 rows
# How many users were not invited by another user = 5583/12000
# media
#replace '-'(hyphen) with '_' underscore to make database operation
#lal
# Standard deviation of the residuals.
#'Columbus': '3df0e3eb1e91170b'
# Apply changes to each column
# using the rest of the training data
## set these weather conditions to NaN and ffill them with previous hours data
#Change duration of simulation from 3 seconds to 0.5 seconds
# check for missing values
#count of words in messages (posts)    
# Optional: Create an output folder and write dataframe out to CSV
# fit with non-tfidf dataset
# assert clean.shape[0] == expected_index.shape[0]
# use this cell for scratch work # 
# Let's get the IDs of the artists whose name contains "unknown"
#Using the .plot() method
#Example2: Import File from URL
# load the dataset containing explanation of the variables in the diagnosis datasets # downloaded from: https://ida.loni.usc.edu/pages/access/studyData.jsp?categoryId=16&subCategoryId=43
# Export CSV
# try again with a tab separator # some errors occurred with missing values after the 600th row so I keep it small here, 100 rows #bg2 = pd.read_csv('/home/hbada/BGdata/Libre2018-01-03.txt', sep='\t', nrows=100) # pythonanywhere
# sample data: note that Series auto-generate index  # histogram by value_counts
# double Check all of the correct rows were removed - this should be 0
# A:
# Use Pandas to calcualte the summary statistics for the precipitation data
#Sharpe Ratio #Menor SD
# as seen above, 290584 entries now as entry with index 1876 is deleted # we can confirm by checking unique values of user ids
#cust_group_date.sort_values(['count'], ascending=False) #date=pd.to_datetime(cust_group_date['order_date']).date()
# Initialize a 2-group MGXS Library for OpenMOC
# read in the csv and join to the previous df2
### Fit Your Linear Model And Obtain the Results
# Here Alternative is taken as the larger as we are intersted in null_propotions > Value
# Import the csv file and store it in a dataframe # Check the result
#print (r) #print (data)
## Inspecting the curious case of 'BECHTEL MARINE PROPULSION CORPORATION'
#Getting the countries data and joining it with the existing dataframe df2
# investigate last_seen
#Binomial Dstribution
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #r = requests.get('https://www.quandl.com/?apikey=2WWxsMC5KvrYAngyPH-W.json')
## save keras model, see above where callback was created, but not working for this project/model ## 17504 Sep  6 15:07 weights.{epoch:02d}-{val_loss:.2f}.hdf5
# Upper line means not a dog
#  Importing another file for the coming events
# To create a pandas dataframe from the duplicate_check_events list # To preview the dataframe by first 5 rows.
#Store twitter_archive and image_prediction to CSV
## daily normal 
# months: use the builtin calendar module
# index date
# create new dataframe # successful campaigns by name length (sorted by name_length)
# checking number of rows in dataframe df2
#df_c_merge['country']['converted'].mean()
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# convert from MSE to RMSE
# initialize your estimator # train your model
# Number of unique users in df2
# in terminal, this will create a csv from the video
# fig.savefig('toma.png', dpi=300)
#Inspect the data frame
# For Displaying last six Data points # For Displaying first six Data points #pd.DataFrame(dummy_var["_Source"][Company_Name]['Close']['Forecast'])[:6]
#pd.DataFrame(content,columns=columns_name )
# save data to CSV
# adding the "market_cap_usd_billions" column to the dataframe by manipulating the "market_cap_usd" column
#for i in range(5): #    print (i, X_vect[i])
#Estoy levantando 1000 posts
# pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Followers','Following','TweetCount'])
# dropping the duplicate and keeping dataframe df2
# reference:
### Fit Your Linear Model And Obtain the Results
# we just borrowed 'std'! '0-mean' is already assumed because it's the Null !
#merge tables (this is not particularly efficient, as we do not need all the columns)
# getting the WHO Region column as array
# In single line
#Now fill index with the previous values
# Get categorical codes for date columns
# ciscih8
#Drop removes a specified row or column from a dataframe
### getting the odds by exponentiating coef
# one more look
# Generate auth tokens for Colab
#feature selection
#Let's do a quick count of Gender. # Note that less than half the hosts are Female.
# What are the most active stations? # List the stations and the counts in descending order. #stationCounts =  session.query(Measurement).group_by(Measurement.station).all()
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# dfincident["CasesWithin8hours"] = 0 # from sklearn.utils import resample # dfincident = resample(dfincident, n_samples=100, random_state=1)
# Collect the names of tables within the database
# create dataframe from list of tuples
# 'stoi': 'string to int'
# This merge results in duplicate columns, marked by _x and _y suffixes--but that's OK for now!
# Plot ACF of first difference of each series
#load into transactions dataframe
#Any funny minumum values left?
#happiness_df
# Now I need to forward fill in the draft years, so I need to create new rows for missing years and then forward  # fill
# FIRST
# skip 100 lines, then only process the next five
# read in tab seperated folder using requests library # set url variable  #create requests object to work with 
# weights array
# Double Check all of the correct rows were removed - this should be 0  
# Summary statistics of Precipitation Date
#unrealistic rows with high registration_year date
# resDir  = "basic regression test.csv"
#Frequency count
## no.of unique cats taken in by the shelter 
# Double Check all of the correct rows were removed
# Generate a dynamic list of tickers to pull from Yahoo Finance API based on the imported file with tickers.
# Suburban cities average fare
# Creating new df with only a few wanted cloumns
#Change missing values in 'name' from 'None' to NaN 
# Do any misspell cinamon?
# pairings < 3m / all pairings
# write clean data to a csv to open in Rstudio
# perform update 
# Replace the ??? with the write string format code
# Verify that an address is valid (i.e. in Google's system)
#Simulate  nold transactions with a convert rate of  pnewpnew  under the null
# Check SFL stuff
# Rename odometer to odometer_km to be more descriptive
# How many data samples do we have all together?
# No, how many NaNs in the entire dataset?
# How many stations are available in this dataset?
#del df.index.name
# bash environment
#Number of unique users in our new dataset 
#Re-ordering the DataFrame
#Inspect distribution of subjects by prior tweet history volumes
# DataFrame has an index attribute to access index labels
# sort df by label
# backward fill
# ensure destination directory exists
# Printing the content of git_log_excerpt.csv
# creating copies of df's to ensure I wont lose original material from any mistakes cleaning
# grap spx ts
# how many are breakfast foods?
# Missing Values
#Since there are two weather data files, I first merged them into 1 file #Check to see if they have the same shape
#Removing data that is not adhering to the expected values
# from the non-active users, how many still visit the site (LastAccessDate)?
#dicttagger_location = DictionaryTagger(['C:/Users/Connor Fitzmaurice/Documents/COMP47350/DataAnalyticsProject/location.yml'])
# Provide descriptive statistics for all columns of this data frame
#There are too many document types to plot on one chart so select the types with highest maximum
# 3. Correct format of timestamp  # use to_datetime function to clean timestamp column
# Create a dataframe with the minimum and maximum values of 'x'.
# Convert the pd df to HTML table and clean up. 
# ...and plot it against time
# Check for missing values # Extract data where is_attributed == 1 # Check NAs
# this might run long
#Concatenate (or join) the pre and post summary objects
# Removing observations at end of dataframe with na values
# get cat sizes # create embedding sizes # nconts
# From a list with an implicit index
# verify sum
# count how many values you have per column # We print the number of non-NaN values in our DataFrame
# With .loc, you can select multiple rows and columns by using lists
# segment districts by size
# Inspect measurement table
# The number of rows and columns in the dataset # Print the number of rows in the dataset
# Demonstrating that the rows with NaN have been removed and the last row as seen is the row just above the last NaN value # Ths would work is we did df.dropna in the previous step, but thats not what we are going to do now
# We create a pandas dataframe as follows:
# Mean Value of pnew and pold
# Number of users converted is mean of column converted
#delete the images folder that is OUTSIDE the git_page folder #delete the markdonw file that is OUTSIDE the git_page folder
# Convert minutes_list into a series # set to 'tripduration_minutes' which will be new column 'tripduration_minutes' added to df
# URL of page with latest Mars weather tweet to be scraped
# Checking transaction status
# ??
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#creating and joining dummies for the group column
# html scrape
# credit card payments
# Create linear regression object
#calculate the decision tree's mean absolute error
# merge the the newly imported "numberOneUnique_df" with the original "spotify_df" df one to id the songs that make it to # 1
# check option and selected method of (12) choice of hydraulic conductivity profile in Decision file
#7.)
# Check the data types. Are the two DataFrame's data types compatible?
# Create a pandas dataframe: # Display the first 10 elements of the dataframe:
#everything not between the two numbers
# select disasters declared in past 10 years
# Check if all bedrooms, bathrooms, and floors are numeric.
# Create a Word / Index dictionary, mapping each vocabulary word to # a cluster number                                                                                            
#drop NaN in bio* to make the data clean
# Negative tsla <=> not active before D0 # tsla_neg_DF = tsla_neg.toDF(["cid","ssd","num_ssd","tsla","tuna"]) # tsla_neg_pd = tsla_neg_DF.toPandas()
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# TODO: Tune Parameters
#Reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html #Compare z-score to one-tailed test critical value at 5% significance level
#pivot table showing the Age and affair status
# taking a look at the counts by country
#set up a new dataframe
# get proportion of converted users
# target is the handle. # make trump 1 and sanders 0
# Here's a example code snippet you might use to define the feature columns:
# Load the last statepoint file
# backup our model # show summary of model
#sns.distplot(x)
#Proportion of control group converted
# there are NAs
#, 'test'
# Print the street number
#Export dataframe to csv
# Incase the street length was wrong for any street, we take a median for street length for each street
#reading the csv file into a datafram
# Read the data file.
# show prediction results
# Simulate n_old transacrions with a convert rates under null hypothesis
#df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD')) #df = percipitation_2017_df.cumsum() #percipitation_2017_df.cumsum()
# Clarify playoffs. Only needed for 2013.
## Now, create a dataframe that only has the subset of crimes with the primary description 'CRIMINAL DAMAGE' # code here:
#fbdata[['Close']].plot() #goog[['Close']].plot(ax=ax) #goog[['Close']].plot()
# api url
# df2 has 290584 unique users
# How many stations are available in this dataset?
#Compute accuracy on our training set
# replacing 'nan' with '' for improved legibility # print(df_final.columns)
#the data frame has 11 rows and 15 columns
# write your code here
# Ensure latitude is within bounds (-90 to 90) # Have to do this because grid file has 90.000001
#Example1:
#one more filtering criteria:  no missing values.  The last_event column is a datetime object, #and if there were any groups with no past events, this column will be a NaN (missing value).
# Frequency table to show the times where new_page and treatment records don't align
#Add a query to a dataframe #View data
#Plot the total_ridership counts in ascending order # @Grader: How do I get the station that ties with each of these counts?
# elegant way of computing the sum columnwise (in case all are numeric): 
# We may want to keep all gorups in which the standard deviation is larger than some critical value # Return data only if that key's std in column 2 is greater than 4
### Create the necessary dummy variables
#due to 'converted' are 1 and 0 we can use the mean to know the probability.
# unique users in total
#find the number of rows in the dataset
#save results
#R18df.rename({'Create_Date': 'Count-2018'}, axis = 'columns')
# Use Pandas to calcualte the summary statistics for the precipitation data
### create dummy variables for the country column
# a date without time can be represented # by creating a date using a datetime object
# looks like someone just manually entered the same datapoint twice
#Example 6: Load a csv while specifying column names
# Simulation under null for new_page
#getting dummies for car models
# For getting the table
#remove duplicate job postings based on clean descriptions
# drop columns where all values are null values
# Calculate the morning traffic that fall under the income group for every station
#checking statistical values in the twitter_Archive dataframe for the different variables
# Count hashtags only
# predict fire size
# Display changes made
# Create x, where x the 'scores' column's values as floats
# Test function
# Set just like the index for a DataFrame... # ...except we give a list of column names instead of a single string column name
# removing useless columns
#Woo hoo, it worked!
# Cycling distance # Create a tuple of data
#Using unique() function
# by repeated tweet
# Fill in missing topic values with 0
#use this below to split at @@@ and then save it to a md file
# head climate_pred
# Getting just the date
# resample to 1 sec intervals using forward fill
# show total conversions by datestamp
# check our features
# recuerda que esto no cambia data en realidad, porque no lo hemos guardado
# Process a single status
#now making the chages using inplace=True to make the changes permanent
# Check the possible values of country column
# Exogenous transmission capacities in 2025 per region [MW]
#joblib.dump(km, './data/KMeans.pkl')
# since happy with subject_count - rename the column - get rid of the tmp tag
# Verify missing values
# inspect number of entries in df2 after deleting duplicate record
## load isochrone into a geopandas dataframe
# last 5 rows
# Print the info of df_subset
# Create the first DataFrame from the given URL
#Actual difference of converted rates
# Reconstructing the original object hierarchy! 
# Inside gamers
#make ints to simplify
#converted users without dropping the duplicates - note this is not the correct value as it includes duplicates
#To find value of n_new
# this has been reduced to a Series
# read the data into a DataFrame # we want the Date as an index and it to be automatically parsed
# using *2 // 3 to take month number and convert it into season bucket ([1,2,3][4,5,6][7,8,9][10,11,12])
#Check the unique user_id
# MS means "month start"
# accediendo a una columna por el atributo
# lets turn that list into a spreadsheet
#After merging
# Add the figure to a layout # Show the plot. This will create a warning as the figure contains no data.
# calculating the actual success conversion rate for new page
# Using the inspector to print the column names within the measurement table and its types
#Create list of top10 topic_id from dataframe
#individual was in the treatment group, what is the probability they converted
# Number of unique users: 290584
# Repeated part from morning track: fill with media or mean example so can match the orginal cleaned data. 
# Use IFrame to display pyLDAvis results to prevent pyLDAvis from overriding # other formatting of the Jupyter notebook
#create a new dataframe by merging users and transactions #print the rows
# Column names on the raw data file
# Segment customers into clusters
# Issues open
# Make ved folder
### Fit our Linear Model And Obtain the Results
#Compute the probablity of old page converted rate
# interaktif jupyter
#print ("Current date & time " + time.strftime("%c")) #list_of_idxes = [1454, 1852, 2801, 4545, 4759] #list_of_idxes = [3, 4, 56, 62, 63]
# retrieve row by index label using .loc
# initiate a kojak database and collection with documents related to crunch_ppl
# Calculate probability of converted in control
#Temperature - fill short gaps < 1hr in all temperature records
#drop the 'unnamed 0:' column
# Sort movies by budget in descending order
# read the 2017 data to see the original data structure
#### Define #### # Convert tweet_id column from int type to object type # #### Code ####
# Subjectivity
# Displaying the mean of the result
# Import curren stops table into dataframe
#New dataframe #They might be values that might still be sharing dates and department ID, #re-run similar loop to the one above with new created dataframe to make sure that that no values repeat.
# Number of unique user_ids
# set threshold to drop NAs
#print(reviews) #print(reviews_vector)
# Load the data
# exercise (page 161) # extract the data highlighted, *as 2d arrays*
# first, let's check the 5 styles that have the most checkins
### Create the necessary dummy variables
# Assign the station class to a variable called station
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
# Checking work
#check if projectId in schedules is also in projects and budgets
# this method works, to extract the hour of each datetime value
# key step!
#Reading all necessary CSV files from Scrapy in the DIR
#Column clean-up
# TTL
# Sort the dataframe by date
# DEFINE: drop the unwanted columns #CODE: #TEST:
# Creating lenghts over time
# convert the chosen lines to a single json string and read it into a pandas dataframe
# number of unique users
# analyze validtation between BallBerry simulation and observation data.
# Looking at this data, I see correlations between sales and bottle # bottle sold # I looked at sales data where there was a high correlation. I saw that there was a high degree of correlation between bottles sold and volume sold so opted to go through one simple regression
# create a dataframe using cols and correlations
#preprocess('DirectRunner',BUCKET, bigquery_dataset)
# join two dataframes: df2 and countries
# combined_df5 merged all uncommon values into a single 'other' var; this is an alternative strategy
# Assign new column names to our DataFrame
# The method .get_dtype_counts() will help me to see the number of # columns of each type in the DataFrame
# easy to access data # efficiency wise, better (database)
# df_2011
# finding index of your Dataframe
# calculate mean of p_diffs
# Tells us how significant our z-score is
# set avg_monthly_google_search_volumes #for country, volume in avg_monthly_search_volumes.items(): #    data_countries[data_countries.country_destination==country]["avg_monthly_search_volume"] = volume
#old page convert rate
#s = "01/12/2011"
# Open the dataset # Swap the dimensions
# Declare a base using 'automap_base()' #Use the base class to reflect the database tables
cursor.execute("""DROP TABLE coindesk""")$
#Example 5: Import File from URL
###
#Fort Worth': '42e46bc3663a4b5f'
# Word frequency for terms only (no hashtags, no mentions)
# load ADNIMERGE.csv, the main dataset (we will add our own variables to it). 
# writing it to csv #pred = pd.DataFrame(predictions, columns=["rating"]).to_csv("24nov.csv", index=False) #predictions.to_csv("submission.csv", columns=["id","rating"], index_lable=tf.id)
# turn timestamp into DateTime
# calculate age based on admission date # calculate length of stay in days
# Capture shape of raw data
#print(tmp)
# save new clean dataset which contains no duplicates or records with missing or mismatched values # we will use this dataset in next sections
# save p_diffs for later use
# Most active stations
#if running demo, uncomment this and change test['id'].vaues -> test['index'].values
# Let's pull top 5 subreddit in the most number of comments
#  This will totally remove all duplicates 
#For test samples, please follow the following two cells as feature transformation procedures #transform time string to float
#os.listdir(A1.paths['directory_paths']['indicator_settings'])
# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})
#==============================================================================
# Use `engine.execute` to select and display the first 10 rows from the table
# plot histogram of null distribution # plot actual statistic over null distribution
### Fit Your Linear Model And Obtain the Results
# Correlation between all pairs of ETFs
# Print out the number of movies we will recommend.
# Again, to limit the number of calls to Twitter API, just do lookups on followers that connect to those in our user group. # We are not interested in "friends" that are not part of this community.
# probablity = 145310 / 290584 = 0.5001 (approx)
# Create a list from 0 to 40 with each step being 0.1 higher than the last
# with a dataframe
#Creating transaction list array with value of 1's and 0's with probability rate of Pold which is 12%
### fit the logistic model for page,  countries and periods of time ### print the results
# Find columns with null values #new_df.isnull().any()
# adding the subject group to the all sim band data set
# there are 63,378 tweets from these influencers
# join in information about occurring words, probabilities etc
# how much did hurricane relief make up the total SBA disaster loan in the past decade? # answer: 52%
#Probability of a user landing on new_page
# creating a dummy variables for the column -country # deleting one column from the dataframe
#data_comparision
# Convert text to a Python object using the 'json' package # And now we have a Python list:
#set variables to allow automatic X-axis on plot
# peak at score
#register the Spark Dataframe as a table
# load country data
#print df2.loc[['2016-09-18'], df2.columns.get_indexer_for('GrossIn')]
##### the index which was created in the dataframe
# Terminate the session
# the maximum age
# Removing the records with number of reactions less than 10 # Total number of reactions includes the likes so they have to be removed
# are there any values less than zero?
# Fix Data Types
# favorite table # for row in table_rows: #     print(row.text)
#count
# To print the count of our vocab # For each, print the vocabulary word and the number of times it  # appears in the training set
#Features #Targets
## To determine the number of records 
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
# set SUMMA executable file
# this yields the row labels
#A quick observation: As seen below, user 2 is an adopted users (login 3 times between 2014-02-03 and 2014-02-10. # User 11991 (at the tail of the data) is not an adopted users although he used the system more than 3. 
#A different approach using apply() method and a lambda function:
#find number of rows that has appointment - could be interesting number to get as well
# calculate the next month end by # rolling forward from a specific date
# from - http://knowledgetack.com/python/statsmodels/proportions_ztest/
# Alternative way to get the date only 
# Scorepoints range is fro 4 - 200 points 
#print randomdata2 #randomdata3 = randomdata1[(randomdata1 <=-3)] #print randomdata3
#Check min/max dates
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#columns to delete #remove rows and columns from twitter_archive
#replace values that should not exist with what they were supposed to be, or with a median of the feature
#Create the Pandas GroupBy object
#Authorization
# Make a copy of each data frame so we can still access the original data
# Display of first 10 elements from dataframe:
#Read the clean data file and load into the dataframe
## Find the stock with the minimum dividend yield
#get station count, has been checked with measurement station count
# Combining tables to look for overlaps
# Pandas will type-case when NA values are present
# creating 'time_from_creation' feature (days)
    '''$     Do a HEAD request of downloadable datasets to check if they're still there.$     '''$
# Explore the unique values in the brand column, and decide on which brands to aggregate by
#if you want to analyze other factors, you just need to change 'ScoreLabel'
# 181 retweet removed
# melihat hasil perubahan zona waktu di kolom index
# provides info related to the dataframe df_CLEAN1A as follow: 1k observations, each column has datatype of numeric(int64) and also how much memory is using in our machine. 
# grab the feature names, ie the vocabulary, or the feature space
#output df_SP to a file
#Chandler
# We display the average salary per year
# Testing the merging data function with dataframes
# Convert categorical variable into dummy/indicator variables
# concatenate by columns
#Testing the feature mapper
# UN01.DE is Uniper SE, a recent spin-off
# getting a single document:
# find all message with @someone
#3 # the R^2 was lower on the test data compared to the training data
#grouping the function by the GICS Sector column using groupby
#Test data
# landing page classifications
# As shown in http://knowledgetack.com/python/statsmodels/proportions_ztest/, the significance and the critical value  # can be determined by calculating the CDF of the z-score as well as the comparison with the critical value at 95% confidence # for a one-sided proportion hypothesis test
# !cd .. # !curl -O http://www.grouplens.org/system/files/ml-100k.zip # !unzip ml-100k.zip
# 1.959963984540054 # Cut off limit for 95% confidence
#weight_is_relevant = 1
# Checking that all of the correct rows were removed - this should be 0
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#One nice feature of Jupyter is that we can show it in a table via Pandas
# Probability of individual converting that is in treatment group
# append the heading row to price_data and set column names
# As per the instruction above, p_old = p_new = converted rate in ab_data.csv regardless of the page
# Read .csv files from kaggle
#Twitter accounts in the decreasing order of frquency tweeting about Donald Trump
# To find  pnew  -  pold  for your simulated values from part (e) and (f).
#Show all of the tables in database #We'll use the nyctaxi table, created using code similar to https://github.com/toddwschneider/nyc-taxi-data
# analyze validtation between BallBerry simulation and observation data.
# determine number of rows using shape()
# you can define period like starting year is 2011 and 10 quaters after it
# convert python list to numpy array
# First row from validation dataset
#get 2015 and 2016 and predict 2016
### Fit Your Linear Model And Obtain the Results
# doesn't matter which row is dropped
#crews
#Create series with just the closing price
# count number of fire violation per month in  May 2018 
#checking contents of dataframe
#### Change name and drop [Neighbors_Obj]
# %load ../solutions/sol_2312.py
# Save the figure #plt.savefig("Sentiment_Analysis_of_Media_Tweets.png") # Show plot
#cars22 = cars2.drop(['price'],1)
# tokenize words 
#Check the conversion went through smoothly
# Create archive
#replacing _ in between names in p1,p2 & p3 columns
# Arithmetic operations between Pandas series and single numbers
#pulling from s3
#print(d)
# Join on the bitcoin price information in euros
#copy the clean dataframe to the csv file
# inspect measurement
# get the diff between max and min age by agebin in the index
# Import the built-in logging module and configure it so that Word2Vec creates nice output messages
#creating a dataframe df_inner_users_sess by  #df_inner_users_sess #filtering out rows from dataframe df_inner_users_sess having the same Registered Date and SessionDate 
# Suburban cities total drivers
# alternatively #cpi_sdmx.translate_expression('',component='TimeDimension')
# df is the dataframe where the content of the csv file is stored # note that with dataframes I can refer to variables as dictionary keys,  # i.e. df['starttime'] or as attributes: df.starttime. 
# generate random 0,1 array based on the convert rate of old page group.
#Create Logit regression model for converted on intercept,ab-page, and country with CA as baseline
# Create a 'tweets' DataFrame for each row of the 'news organizations' DataFrame # Would it be ok if the funtion get_df_for_user_tweets required a second argument, which was defined here?
# score on test set
# Take mean amoung columns
# plot autocorrelation function for RN/PA data
#Display a few beginning rows.
# .logical_negation() flips 0 to 1 (Trues => False)
#can put two datasets back together, and remove "_source" column (which was expanded into expanded_data)
# proportion of users converted
# We get descriptive statistics on a single column of our DataFrame
# Resample
# more properly, should do
# display the unique values with counts for issue_type
## I am just interested in the cats that were adopted
# Group percentage
# Strips Columns of white spaces - the indentation above is from spaces in the column names # strips the white space and also replaces any other white space with an underscore and also made the names UPPER case
# download the text
# this action has no effect on tree but could be usefull to avoid false relation of order in regression
#################################### # remove invalid duluth isbns
# Check all of the correct rows were removed - this should be 0
# create category column # create single name column
# Dropping the Id column from the dataframe # Checking the shape of the dataframe
# Extract title text
# Review_count distribution across categories
## this is ready to use
# what is that date's day?
#Search some tweets! Let's look at hashtag #H2P (Hail to Pitt)
#Create logistic regression for the intereaction variable between new page and country using dummy variable
# Make predictions using the testing set
# tweet date/times are reported in utc
### Create the necessary dummy variables
# URL of page to be scraped
# create the necessary dummy variables
# Create the 'country' column
# Investigating the unique countries in the dataset
#Investigate the semantic space
#Stores file in the same directory as the iPython notebook
#'Scottsdale': '0a0de7bd49ef942d'
# Plot a histogram of these 10000 p_diffs
#Due to limited processing power, I have taken random sample of 100 instead of the complete 19001 dataset. 
# Challenge 1
#We will simply execute every command
# Location # Removing stopwords # Finding most common words
# Cyber Monday
#Mean Revenue
#6. Arrange values of a particular column in ascending order.
# check json errors
# create pySUMMA Plotting Object #Val_eddyFlux = Plotting(hs_path + '/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
# Drop the irrelevant columns
# get the treatment group converting probability
# 0 accounts_provisioned and 0 district_size... maybe bad data capture?
#Checking dataframe info for missing data
#df.head()
#rename state_abbrev from state_lookup table to state_code
#data record con price < 100
#find any numbers within the posts
#Try kNN classifier
# save the model to disk for reloading later
#cut out only low redshift objects for bad seeing
# Retrieve the NuFissionXS object for the fuel cell from the 1-group library # Show the Pandas DataFrame for the 1-group MGXS
# data = pd.read_json('/Users/y.kashnitsky/Downloads/train.json')
# Using binomial distribution to find samples with probability p_old
# since converted is a boolean (0 and 1), the mean will give the probability of conversion
# sort grid_pr fires by lat/lon to get projection right
# aggregations of python objects will result in an error
#Number of unique users
#Create logistic regression for the interaction between ab_page and country
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#most active stations. #stations and observation counts in descending order
#print(len(old_page_converted))  #code to check values
# Instantiate the count vectorizer with an NGram Range from 1 to 3 and english for stop words. # Fit the text and transform it into a vector. This will return a sparse matrix.
# DataFrame objects have a nice plot method
# Retrive data frame from Measurement tabel
"""$ Print any single news title$ """$
#Compute the number of rows containing users with new page
# for each reprinted card, the first reprint has the completed printing/rarity dictionary, so we can get rid of every other duplicate
## convert disctionary to pandas Data Frame for easier manipulation
#reading csv file created by webscraping
#read_hdf('store_tl.h5', 'table', where = ['index>2'])
# top 5 breweries
# Create histogram here
# Plot some stuff
#Read in FLu Incidents file and check for accuracy
#4. Change the column name from any of the above file and store the changes made #permanently.
#show the inferred schema SQL style
# create some random data # put that data into a dataframe
# Lower case columns.
# Removing some 'scraping debris'
### Step 12: Review the two-dimensional array created from the PCA
# The protocol version used is detected automatically, so we do not # have to specify it.
# define the vector w
# put data into a data frame for charting
#Read tweets approval ratings from csv file
### Fit Your Linear Model And Obtain the Results considering both country and page
#Read in clean measurements csv file
# Create a month column # Create a log of income column
# convert in_port bool
# create column for "Days to Resolution"
# mean() along each row (axis=1)
# join the country data to df2
#New page #p_old = df_old[df_old['converted'] == 1].mean()
#I convert Timedelta value to float (number of seconds of the trip)
# Select for just the graffiti removal entries
#Validating the dropped values
#Merge the other feature dataframes into the main dataframe
# Double Check all of the correct rows were removed - this should be 0
#Next, genres contains JSON data, so load it as a column of dicts
# import and instantiate MultinomialNB
# Creates a list for sentence tokens
#read in the member file
# 18. Using the Data Frame created in part (17),  # print the frequency of each column being the max score.
#Let's try using groupby to see what different language users were doing
#df['age'] = 
# find historical data for 2015
#Only Relevant  user Id
# get metadata info
## Always fill values after reindex
# creating a logistic regerssion model # displaying results
# We can compute the same result via pd.eval as follows
#You can load json, text and other files using sqlContext #unlinke an RDD, this will attempt to create a schema around the data #self describing data works really well for this
# Calculate the date 1 year ago from today
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned # Use parameter limit = 1 to only return the latest date data
# convert the 3 date columns from str to datetime
# Pulls the last 5 columns from the data
# Cleaning Data  #We shall first drop the duplicate values , if any
# extract weekday and publish time from vacancies
# Reading from pickle # read the data as binary data stream
#replace all Nans with 0
#we used the built in .rank method to rank the scores and the gross, stroing the values in new columns.
#Distinct
# Make output directory
#This is Pearson R for comparing tripduration acrros day and night
# Load data and print out a few lines # Read csv into pandas dataframe and store as df # View the first five lines of the dataframe
# concatinate two series together
# find the indices of headers # drop the indicies
#Import the data set into dataframe
# fill shipping method names with shipping method ids if there is no shipping method name
# common words from the reviews
# Check the out-of-bag score, which is very low (40% accuracy)
# Question 3
# regression model #fit the model
# word2vec expects a list of lists. # Using punkt tokenizer for better splitting of a paragraph into sentences. #nltk.download('popular')
#let us find out the significance of z-score
# Make predictions on test data using the Transformer.transform() method.
# answer
# Simulate conversion rates under null hypothesis
# No.of conversions / total no. of individuals
#Eliminamos las filas duplicadas tomando como referencia la columna id
# axis=1 apply function to each row
# 2015 Only Subset
# Get sentiment for walmart tweet #Read walmart tweets csv file
# Source Names
# Overall 11.97% of the users converted.
# 3.Calculate what the lowest opening prices were for the stock in this period
#print lxml.html.tostring(item)
# Remove columns using drop ... axis=1 for column
#merge datasets
# Counting repeated values
# Applying function to "Ideas" 
# To flatten after combined everything. 
# predict the new values 
#6  When I want to replace my values with NaN
# sends command
#reading csv file #printing out first few lines
# Look at 4 features: title, subreddit, num_comments, and created_utc
#split the strings using the separator "," 
# get station count,
# accuracy_score(actual, bm2)
#The time it takes to process. Cleaned up
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# number of unique user id's in the dataset
# Compute thermal flux utilization factor using tally arithmetic
# Normalize summary
# Use Pandas Plotting with Matplotlib to plot the data
# remove the replies from the dataset by selecting only records with in_reply_to_status_id is null:  # Test - we expect the numer of records to be 2278 as there are 2356 records of which 78 are replies
# First, check if every user has the only one `converted` value
# Probability user converted(Conv) given that  user in treatment group(Trt) # Can also be written as P(Conv and Trt)/P(Trt)  #Users who converted and were in treatment group
# Pickle Reader For Exported Data Frame File # /r/news, top 200 threads for big sample testing size. Data From November 6, 2017 # change directory depending on user
# To answer this we can group by column 'group' & then we compute the statistics using describe function as conversions are assigned boolean values, we can use mean to find probability of conversion
# use describe or shape to find number of rows
# Challenge 2 done
# Create log scale response time instead for modeling
#Calculate for pnew # Display convert rate
##baseline is the average of the training scores: ## Note this what is used to calculate Total sum of sqaures
# summary of model
# Renaming the columns to date, tobs
#Create a Spark Dataframe with the createDataFrame function #Print the first two rows to check
# Create new df "f2"
# didn't have much luck removing columns # nor renaming the headers but we take care of these problems later
# Probability of individual received new page  
#for k,v in sorted(sentimentCounter.iteritems(),key=lambda x:x[0]): #    print k,v
#load into users dataframe
#treatment in group will be called A and new_page in landing_page will  #be called B #calculate thenumber of time new_page and treatment don't line up
# Let's take a look into our data to check for missing values and data types
#Read the entire dataset #Using date parser to convert all dates into datetime
# Add cleaned tokens as an attribute to by_tweeter:
# The probability of individual in the treatment group converting #df2.where(df['group'] == 'treatment')['converted'].mean()
# close 
# Date limebike started differentiating between bikes & scootscoots
# Replace the ??? with the write string format code
#Filter our merged sites-results dataframe to contain only the sites identified above
# check available info
# These are only hashtags that have value_counts greater than 4
#Convert into a Python dictionary
#Design a query to calculate the total number of stations.
# Convert "date" data to datetime and set index on date
# save new clean dataset which contains no duplicates or records with missing or mismatched values # we will use this dataset in next sections
# let's start with a few simple observations: # - as in the case of dataframes, indexing series with  # integers vs slices/list gives different types of outputs
#subo el dataframe
# calculating single correlation between two columns is also available, however quite difficult to viz by then
# Probability of conversion for new page
#Plot the results as a histogram with bins=12
# To create a new dataframe for all the launch events of product-b # To count the unique device number as first-time launches
#show the shape of dataframe
# filtering out wrong timestamps
# Drop the 'True' column.
#was a first booking made
# proportion of p_diffs greater than the actual difference observed in ab_data.csv is calculated as per below:
# Range chart statistics. # Calculate the average range.
#Tweets = AllTweets['username'=="b'SFBARTalert '"]
# explicitly copy 'hn' when creating 'train' and 'new'
# simulate transactions using random.choice
#Apply length function to the review column
# adding prefix BOUGHT_ to column names
#only here in in the ttarc.
# create a database (if it doesn't exist)
# first take a copy of the datasets as we will be manipulaing them
# Printing the content of git_log_excerpt.csv
#using the shape function to count the rows #check the result
# series index by index #max of scns created
# inspect duplicate userid
# load preprocessed data 2
# Here we can get date range for some days and freq can be multiple also. 
# https://movie.naver.com/movie/sdb/rank/rmovie.nhn?sel=cur&date=20170806
# the number of reports from the most active station
# maybe a pie chart showing the different users and the magnitude of their contributions is the total number of lines changed/removed/added??
#Number of rows in testing set 
# getting rid of unnecessary whitespace in the labels - could be a gotcha later on
# what's the lenght of the groups we're going to get
# Display the result of the regression model
# Give the chart a title, x label, and y label
# The Directory to save the csv file.
# There are 290584 unique users identified by a user_id.
#idx = pd.IndexSlice
#Shape #Type #Summary
#to confirm join acted as expected and 'missing orders' were placed pre-genesisco
#GETTING THE NON UNIQUE USER INFO
# replace NaN values with zero in abs_page or else the regression model will not work
# data scraped from http://www.unicode.org/emoji/charts/full-emoji-list.html
# Now to import an example I made
#'diff_abs_logprob_final_tote_morning_line' is like log-return of one runner, we wanted to get 'sum return', so we weighted 'diff_abs_logprob_final_tote_morning_line' and got the sum of abs.
# change the name for the convenience
# Viewing all values of name after cleaning
# generate sample # Compute and display sample statistics
#  numpy.setdiff1d() returns the sorted, unique values in ar1 that are not in ar2. # genco ids from paired cameras which do not correspond to shopify order ids
# or: # sales_df.groupby(['Product_Id', 'Country']).sum().loc[(5,), 'Quantity']
# Join on columns
# Pull in a brand new ticket # prepare(new_ticket)
# Count the total number of tweets
# Load a single column from table
# The youngest person in our dataset is 92
#calculate the average across each time slice #calcuate the Mean Daily Rainfall for each Month #calcuate the Total Rainfall for each Month
# There are multiple dogs in single tweet
# Sort the data by operator and part.
# train the RandomForestRegressor on our training data
# fill in NaN values
# Assign the Measurement class to a variable called measurement
# Read the ab_data.csv file and set it to df # Show the first 5 rows of df
##Prediction:
# first number is one-star reviews, second number is five-star reviews
#Check via customer sample
# Get sentiment for Loblaws tweet #Read loblaws tweets csv file
# adding back to df
# Create interaction variables
# test using visual - check columns
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #7. Arrange multiple column values in ascending order. #Arranged muliple columns values in accending order (using index and output as per exected output)
# Random Forest
#print(agg.fit_predict(locationDistanceMatrix_norm)
# mapped classes are now created with names by default # matching that of the table name.
# Shape function to see the number of rows
# Min and Max
# To avoid ambiguity between index label and index, we can use the loc or iloc attributes. # loc means location and points to an index label # iloc mean integer location and points to a numeric index
# better than get_hist_data
# load random forest classifier
# what is one day from 2014-11-30?
#pull data from downloaded csv
# Get down do the acutal data
# Double Check all of the correct rows were removed for new pages - this should be 0
#df = pd.read_csv(reviews_file_name, sep=',', quotechar='"', encoding = "ISO-8859-1")
# Querying by Team by Game (only shows one team's stats)
# get averaged word vectors for our training CORPUS
#reset index to create date as a column 
# Separate response variables from predictors # Split the training data into training and validation sets for cross-validation
# Replace any unvalid name with None
# In total we are using for this model the following amount of coefficients
## Ensure that all amounts are dollar figures
#Sorting by date
# check the simulation start and finish times
# create new DataFrame to select a time range # remove NaN values that result in the new DataFrame
#To get information if there are any attributes with missing values in the dataset
# determine the convert rate using the mean() on the converted column
#gives all the row as a ndarray
# Select only rows where NEIGHBORHOOD equals "Greenfield" and then count how many complaints came from each source
#creating flag for active users
## Creating a file named image-predicitons.tsv ##
# Using matplotlib, let's plot the data # By default, the indices will be on the x-axis, and values on the y-axis
#.apply(myutilObj.my_tokenizer)
# Handling only the date, not the time
# Print all of the classes mapped to the Base\n",
# back-fill to propagate the next values backward
# Create subset dataframes by city
# Tells us how significant our z-score is #Assuming 95% confidence # Tells us what our critical value at 95% confidence is
# Design a query to retrieve the last 12 months of precipitation data and plot the results
# Print the files on one line each.
# use an inner join
#freq = Counter([g[1] for g in genres]) #freq
# drop useless categorical column (because we have dummy/indicator variables)
# Grouping interests by user
#.get_group((2015,871))
# read the csv from S3 # display the first 5 records to verify the import
# Find unique users # Find non unique users
# create html file
# confirming we got the right row
#del festivals.index.name
# Understand the distribution of accounts by created time
# convert the dictionary in to a dataframe and save.
# Use Pandas to calcualte the summary statistics for the precipitation data
# Isolate comment_body
# print(df_new1.review_scores_rating)
# proportion of users converted
# A priori tenemos la misma cantidad de columnas, veamos que se corresponden los nombres 
# Creates a subset dataframe from another dataframe
#Try K-NEAREST NEIGHBORS classifier on reduced feature space.
# number of noms for foreign service
# boxcox
# Earliest Date
# remove retweets (highly probable repetition on dataset) # TODO: remove really short tweets!!
#install MXNet
# Renaming the column
# session.query(measurements.date).order_by(measurements.date.desc()).first()
#Austin
# read csv file
# drop rows with missing specialty
# get the top model to use
# Create the 'str_split' column
# Count terms only (no hashtags, no mentions)
#Initializes the PixieDust monitor
# set the index to date
# 'mean' represents the percentage of popular posts for each user
#applying nupy math functions without losing datastructure
# give me today's date
# New dataframe for rural data
#!pip install pymysql
##### Can referencce column name without quotes
#Total no of users
#The train stations names need to be cleaned up
# grep spx ts
# Save DF to json file
# A:
# 85% of SBA loan applicants in Florida had to wait more than three months for approval
# unigram chunker 
# probability of conversion regardless of page
# First, import the relevant modules
# get repeats gff
# Now I'm going to create the draft year column for the first year that each GM drafted.  # Now filling the rest of 'Draft_year' with just the year column
# Creates a Logistic Regression with converted as the response variable and ab_page as the explanatory variable:
# Colors # Define x-axis and xlabels
#format the trend date from string format of mm/dd/yy to date  #format mm-dd-yy, for correct datatype assignment
# force nansecond-based time
##choose a 15 day vacation trip
# Create a sentiment model and store in data_df
#'Denver': 'b49b3053b5c25bf5'
# you can create a boolean mask to pass from analysis to analysis as well # select age for all children
#remove first duplicate, remain last
# compute difference from original dataset ab_data.csv
# Split data frame into two different df's based on year. 
# Save references to each table
#df_low_temps.head()
## filtering out the cats
#create word stemming object #create stop words object
#To get an insight on the length of each review, we can create a new column in yelp called text length
#Eliminate batch jobs
# get the column names
# Accessing single elements is also pretty straight-forward
# start_time = pd.tslib.Timestamp('2017-04-29 00:00:00')
# Let's re-import the tweets_i_master dataframe and make a copy to clean:
#setting driver up
# Tells us how significant our z-score is # Tells us what our critical value at 95% confidence is
# I'm assuming that - unlike weekly - I've never gone a full month without drinking at least one beer
#take a peek at playlist variable
# here is where I filter only for ones (residential tax class)
# Select row of data by integer position
# Probability of an individual in the treatment group converting is 11.88%.
#read data as Spark dataframe
# Save the dataframe as a csv file to be used elsewhere
#Plot histogram
# Verify
# analyze validtation between BallBerry simulation and observation data.
# 3.
# Make a copy of the original dataset
# TTL
#Test
# Create dataframe from a dictionary of Pandas Series
#Created a mask to extract all the transactions in 2015 alone
# Printing the content of git_log_excerpt.csv
# Set the day column to integers # null values made it a string datatype previously
#  Invest at inprice, then #  assuming market conditions embedded in poparr2: #  ... effectively simulates 5,000 years of bootstrapped hybrid daily data.
#for prophet to work, columns should be in the format ds and y
# Just one more column with missing data to fill!
# Build Pie Chart
#Getting dummies
# create columns that combine MatchTime with rotation number # this is a first step in creating unique columns with game number by team
# Generating  submission
# display results
# test the resample dataset
# Groupby client id and calculate mean, max, min previous loan size
# Your code here # parse two comlums(1 and 2) together inside a column
# probability that an individual received the new page #print('The probability that an individual received the old page is: {}.'.format(df2_old_page))
# Assigned the Measurement class to a variable called measurement in previous sections. # Here saving my query to a list
#simulate n_oldtransactions with probability p_old using random.choice function
#Import the NetworkBuilder module from modelingsdk. This module allows us to create a network object #to which we can add nodes and edges respectively.  #Create network object
# A:
# What are the most active stations? # List the stations and the counts in descending order.
# data.head()
# tweet_id (int -> str) # timestamp (-> str)
#Bakersfield
# extracts newest and oldest tweet dates from dataframe for display in local timezone
# Printing the content of git_log_excerpt.csv
# Extract the 8-group transport cross section for the fuel # Condense to the 2-group structure
# Initialise with a scalar. #
#Drop rows with invalid data
# Save file
# Proportion of users converted
# read in the csv data into a pandas data frame and set the date as the index # execute the describe() function and transpose the output so that it doesn't overflow the width of the screen
#treatment_df=df2.query('group=="treatment"') #treatment_ctr
# Calculate difference in p under the null hypothesis
# compare the predicted and actual labels for a query
# Count values of new dataset
# All only csv competitions
#merge the tweets data with the new merged data
# Get a list of column names and types # columns
# create some random time series data and create a default plot
#number of rows for a dataframe
# How many contain cinnamon
#import statsmodels.api as sm
#We should remove the rows where treatment is not aligned with new_page, or control is not aligned with old_page #Append both the dfs to get the complete one
# Design a query to retrieve the last 12 months of precipitation data and plot the results
# Renaming metac site column to match oncology dummy column later
# Creating a y_train with our click information # From the Kaggle site, we know 'id' is just a record indicator, so we can drop it # along with the "click" column, which is our target
#pulling artist information
# Take the mean of these two probabilities
# Importing libraries
# 3.2.C OUTPUT/ANSWER 
# Convert to pandas dataframe
# cinema meta-data
#correct rating denominator greater than 10
#open the file and import it into a dataframe
# check the dimensions of the data
## removes punctuation
# creating experience csv
#Setup trackobot
# import ab_data.csv and look at the first five rows in the dataframe # look at the first five rows of the dataset
### Fit the New Model And Obtain the Results
# The case of "1) variable (time, hru or gru)"
#4c. Oh, no! The actor HARPO WILLIAMS was accidentally entered in the actor table as GROUCHO WILLIAMS,  #the name of Harpo's second cousin's husband's yoga teacher. Write a query to fix the record.
# applying a NumPy ufunc on either of these object will result in another pandas object with the indices preserved
# check inserted records
#index is now the date.  It is the primary key. #we created a year column and a month column #since the date is now the index, we can easily get the month and day
# Design a query to retrieve the last 12 months of precipitation data and plot the results
# Position: row 1, column 4
# threeoneone_geo_sample = threeoneone_geo.sample(5000)
# row info
#7.(Optional) What was the median trading volume during this year.  #(Note: you may need to implement your own function for calculating the median.)
# Series what's the name for in Series # operation
## change name to dog_name
# Test dataframe
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Import required libraries
## combine data frame
# Store filepath in a variable
# Merge 2010 to 2016 weather data
#compute the test statistic notetet by z_score and the p_value
# dropping duplicates jpg_url
### export to chinese_vessels_CLAV.csv file
# (empirically derived mean sentiment intensity rating increase for booster words)
# Extract specific values - 2
#(player['events'] == 'triple') | #(player['events'] == 'double'), |
# Write text column to text file to allow NLtk text analysis
# FA_indexs
# Addition
# Checking which bills my scrape was unable to find text for. Noticed it was small enough to drop.
#Drawing samples from a binomial distribution of probability p_old
# NASA Mars News - paragraph
# A quick lookthrough of the first and last 5 rows of data and its values
#Load the dataset and take a look of the first few lines
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#appends a new column that tells us if this tweet is by a new user # with no other tweets in our dataset
# longest_date_interval.columns = longest_date_interval.columns.droplevel(level=1) # longest_date_interval = longest_date_interval # .remove_unused_levels().droplevel(level=1)
# Remove observations containing ANY
# Read dataset
#turn weekly series into dataframe and add prediction column
# For further analysis, saving the file as ab_data_cleaned)
# few days ago it was 758
# Inspect column names 
#'Norfolk': 'b004be67b9fd6d8f'
# Remove surveys which are empty
# 3. Bitwise Operators
#bow = bow.rename(columns = {'fit': 'fit_'}) #X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:,2:], bow.iloc[:,1])                                                   #id_vals=X_test.index
#used stackoverflow for this #https://stackoverflow.com/questions/22904523/select-rows-with-duplicate-observations-in-pandas
#grouped_by_year_DRG.get_group(level=1)
# Left Join on train_df
#Gettign the list of Columns in DataFrame
#  create columns for the dummy variables
#Ah predict already how? I dunno how to put back :X
# Print a sample of it:
#I did not know that the ".values" is what outputs #the values in a numpy form !
# 271 is a memba production (not a photoz) #print "Number of rows with 'production_id'==271   : ", len(np.where(df3['production_id']==271)[0])
# You can get basic properties on all numeric columns with describe(). In this case, there's just one -- AQI. # You can get individual values as well with mean(), std(), etc.
# Viewing all of the classes that automap found
# Probability of an individual in any group converting is 11.96%.
#Export dataframe to csv:
# todo compute rolling corr matrix and plot
#testing queries on it 
# Time Conversion
#Create DataFrame
# Now mash them together into a DataFrame
##### transpose the dataframe
#looking for all AZs
'''Standardizing the Test Set with the Standard Scalar'''$ active_list_pending_ratio_test = x_test[['Active Listing Count ', 'Pending Ratio']].values$ '''Performing the fit and the transform method'''$
# remove duplicate by specifying user_id as argument for the subset parameter
query = ''' SELECT tweets_info.created_at, user_mentions, tweets_info.id, tweets_info.user_id, tweets_users.name$             FROM tweets_info INNER JOIN tweets_users $             ON (tweets_info.user_id = tweets_users.id); '''$
# drop extra id column
#Perform a query to retrieve the data and precipitation scores
# Read one site as a CSV
# Show Profit & Loss Report
# making dataframe where group and landing_page is allinged properly.
#iterate through the list and append the traded volume to the list
# Change path delimiters to unix format if necessary
# the attributes it have
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
# Don't forget to drop nulls! 
# Subset to work on, Living in Valencia,Spain and active
# Round VisitTime, which is Index, to minutes # This didn't work # R code I have: sitevisits$cleanTime<- round_date(sitevisits$visitTime, "minute")
# There were some tweet ids which we could not scrape using the API
SQL = """$ SELECT rating, title FROM film WHERE rating = "PG" OR rating = "G";$ """$
## Read in the data set
# asfreq :: you can convert one period to another eg. quater period to month period # following will change freq from quater to month  # how :: this will decide which month freqency needs to be created from quater weather it is start of month or end of month
#normaliza os dados da versao 1 para range de 5
#subsetting data with trip distance greater than 100 miles #selecting only Trip_distance and Trip_duration columns
#New page and treatment are the criteria we should have
# num_photos, seller and offer_type can be dropped
# validating the changes
# Compute metrics
#perform delete
# Reopen
#url_cleansing_udf = F.udf(cleanse_url,types.StringType()) #smpl_new = smpl_join.withColumn("predicted_acct", predict_acct_id_udf(smpl_join["party_id"],smpl_join["decision_date_time"],smpl_join["party_name"],smpl_join["postal_code"],smpl_join["address1"],smpl_join["street_name"]))
#examining the structure of the dataframe
#total3.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/objs_miss_fall2016.csv',index=False)
# Sampling the Recommendations provide using the item:desc dictionary we had created earlier
# compute p-value
# sen_data.to_msgpack('full_DataFrame/combined_data/master/sentiment_data_2min.msg') # sen_data2.to_msgpack('full_DataFrame/combined_data/master/sentiment_data_5min.msg')
# filtering out all all dates before 8/1/2016
# Import the Sample worksheet with acquisition dates and initial cost basis:
#disable warnings
#df2['rx_requested'] = df2['rx_requested'].map(lambda x: re.sub(r'[^\w]', ' ', x))
# Group data by company to normalize it accordingly.
#Summary of results
# Replotting and seeing if the moments changed significantly
#Load the country data and join the country data into the df2 for further analysis
# Even any other numpy unversal function can be applied 
# Thre seems some missing values filling by 0
# Looking at the price column 
# Assign the other series as a new column in this dataframe
#checking user_id values
#Count the number of columns
#Shuffle the dataset for using  the dataset  #shuffled = scratch #shuffled = shuffled.sample(frac=1).reset_index(drop=True)
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#The country dataset is read and joined with the existing dataset
# Create optimizer
#simulate n_old with convert rate of p_old
# Get the cheapest movies with ratings
#query to find the most active stations.
# Lets check the tokenizing function
# logging.basicConfig(filename=log_file_name, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Let's see how data distributed along with day of week mean
#summer complaints
# We can also fill values # lets use the mean value of A, computed by stacking rows of A
# Full_text column was checked carefully # 22699 tweets were the exact same tweet, available at verizonprotests.com to be posted by one click
#fit the model with the dategroup dummie variables
# export lm_withsubID
# Simulated conversion rate of Pnew uner the null
#'Phoenix': '5c62ffb0f0f3479d'
# Quick preview of the data frame
# Misc utility functions
# change type of int variables (from float to int, possible because no more nans)
#tidy_format = tidy_format.groupby(tidy_format.index.get_level_values(0)) #print(tidy_format.head)
# find the critical value for 95% confidence interval
#count of total number of missing values in the DataFrame
#### Remove outlier
# first check how many dates we need to look up
# Normalize reviewText
#y_train.head()
# use pandas to_datetime() function to convert string column to datetime
# Number of unique users in df2 # or len(df2['user_id'].unique())
#Find the lowest count of movies for a specific rating to get a downsampling size
# make a copy of df_twitter to alter it later
# Load Data Set # Add column on which to build pivot table # Sample data
# get the top level key for the nested dictionary containing max. daily change
# create a Series with an arbitrary list
#check results
#making a copy of the original dataframe where the cleaned data will be stored
#preview output of first element
# Load the results into a dataframe and set the index to the date
# check the iowa file location for your computer 
# TEST :
# Define the baseline as (group=control/country=CA)
#2D correlation matrix plots scope vs type vs site
# Delete rows where 'Ov.transport status' is 5, 6, or 7.
# categorizing fan zipcodes
# Filter on Rural # len(Rural) # Rural
# how many unique authors do we have NOW?
# Perform a query to retrieve the data and precipitation scores
#checking the result 
# to calculate the total we only need to get the size (len) for each selection # Underweight
# Parse the page content
# Store the API key as a string - according to PEP8, constants are always named in all upper case
#Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result.
# Remove stop words
# Uniformizando
# make DataFrame with an index but no columns
## three CSVs ## captured 7/20/2017 ## represents data from 2003 to 1H2017 ## see https://data.world/brandon-telle/cruise-ship-locations/
### Fit Your Linear Model And Obtain the Results
# Predicting the sentiment values for test data and saving the results in a csv file 
# Number of tweets in a conversation on average
#Agrupamos por el campo user_location
# import utils.py to download TestCases from HS, unzip and installation
# How many stations are available in this dataset?
# execute query again # fetch the results and use them to create the dataframe
#switching date of tweet dtype from object to date-time #setting handle as index
#Repeat customers
# loading data into pandas for inspection
#how many unique user used the page
#calculate the number of users landed and new_page
# Drop duplicates with drop_duplicates fn 
# create the shapes based off the coordinates of each census tract in the dataset; these shapes will be used # for calculating which census tract a given crime took place in
#== By Label: A parcular cell (scalar value)
# Summary statistics for the percipitation df. 
# probablity = 17489 / 145274 = 0.1204 (approx)
# Checking the scrape results
#### write our earlier dataframe to csv file
# Count user_id of new_page # Calculate probability
# which simply looks up the dunder on the class i.e.
## Put those values as 0 and set missing_dt as False
# Looking at one tweet object, which has type Status: 
#from experimental.portfolio_rl.clone. #   Note since ipynb runs here, the import should be relative to this file #   Ignore the import error created by the editor
# Step 7: Display the shape of the pivoted data
# We replace NaN values by using linear interpolation using row values
# Creating dummy variables for landing page
#first determine unique column list putting Country at 1st position
#First plot #Number of ads by state
# Plot in Bar Chart the total number of issues created for every Phase based on thier priorites
# separately save the meta_data for each order # then we can add it back later
# Get the url for the data by following the instruction at https://docs.quandl.com/docs/in-depth-usage.
#probability that an individual received the new page
# get english stop words
# now I want to find the first and third quartiles
# checking an printing the number of null values
# Just to have the sample dataframe here for comparison
#Pivot table
# ..consequently: # Pold = Pnew is basically the farthermost level at which the Null Hp stands true.
#n new is equal to the total number in csv
#inspect df
# Visualizing the distributions
# NumPy-style vectorized operations
#Pull data into a data frame
# load the model from disk
# Doctors Decomposition
#Made sure there are no null values
#Display shape of df_students
# Using ARMA model
#Example1:
# check the earliest and latest times in the dataset
# SENTIMENT ANALYSIS BY NEWS SOURCE # To create the plot, first we need to know the exact source names as reported in the file just exported out
# Get a number for count(*) by using scalar()
# proportion of p_diffs greater than the actual difference is computed as:
#Create a new month column # code here
# import fire predictor # remove fires with missing values # greater than 5000 acres
#Compute the difference between p_diffs and ab_Data.csv # proportion of p_diffs greater than the actual difference observed in ab_data.csv is computed as:
# 5. Which Tasker has been hired the most?	 # sample.groupby('tasker_id').size().sort_values(ascending=False) \ #   .reset_index(name='hired')
# data we'll be adding to a test DB: # single shop example:
# Checking whether duplicate parcelids have different log error or not #  Yes they do
#1: Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD) #2: Convert the returned JSON object into a Python dictionary
# converting the timestamp column # summarizing the converted timestamp column
#R16df.rename({'Create_Date': 'Count-2016'}, axis = 'columns')
#below csv file created after manual cleaning of few values in the rating_numerator and rating_denominator columns using ratings #from the 'text' column.
#cleaning the dataframe containing dates based on the data we saw 
# Use the Base class to reflect the database tables
# Example
#reset trip data
# Load the data of the scraped results 
# Correct way to do conversion/parsing datatype from text/string to int  #replace([list of things to match],[if yes,do this],[enable "regex" by typing "True]) #modified price values by removing $ & , like $5,000 & then converting/parsing it to int or float (float is like int but with decimal support)
# Histograms can be crated from Pandas
#show the head lines of the data frame
# getting for each game the violance events level and the number of tickets #V.head(10)
# set index as the date.  #precip_data_df.reset_index(inplace=True,drop=True)
#first_values
#now verify changes has reflected in df
# Look at one of the ticket
# add these dummies into df2
# to find columns for normalizing # looks like diff, trans, volatil, transpm
# tips.tail() # tips.sample()
# Give it a second to load the page
#Tweepy Cursor handles pagination .. 
# Get_dummies splits-out these indicator vars into a dataframe
# Divide each number by each countries annual maximum
# Double Check all of the correct rows were removed - this should be 0
#Find all of the holidays during our time span
#1. The difference in the probability of converting for the old and new pages, while other variables remain constant #2. The difference in the probability of converting for CA and US, while other variables remain constant #3. The difference in the probability of converting for UK and US, while other variables remain constant
# merge country and df2 tables
# import statistics package # total number of conversions in the dataset # n_old and n_new were calculated previously
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
# All Data
# Check what countries are in the new dataset
# Saving the file
# this code was borrowed from https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python
# Stats about Tone
#WARNING: Long execution time - 20-30 mins
#Save Date in Pickle and CSV format
# Performance of the testing model 
# using as type function changing format type from number to string on all three dataframe
# get all messages (11,000+)
# read geojson neighborhood data
# Loop through the list of %m-%d strings and calculate the normals for each date
#Let's check for Duplicated before proceeding
#Pull in most recent 20 tweets
# 1-1: WeRateDogs Twitter Archive
# sneak peak at data
# Let's see how many people were on each deck
#copy from the check below
# import statsmodels and perform a logistic regression for columns converted as response and group as explanatory (control is baseline) # fit the model
# calculate RMSE (for alpha=0.001)
# ciscid4
# Generate dummy variables on column 'country'
#Check to see the data type
#Removing trips with trip distance greater than 100 miles #Looking at the summary statistics of the Average speed
# Second Approach, using Doc2Vec and LSTM for predicting
### Use Sklearn TfidfVectorizer to convert words into vectors
#Proportion of treatment group converted
# Dropping UK so it can be our baseline
# Clean up
# Creating a sentiment dataframe # Writing sentiment data to a csv file
# For the purpose of analysis I created a DataFrame Copy as dfc
# Now let's see how this works in a base case, and with the labor # force growth rate boosted by 2% per year in the alternative case...
# test if 'None is replaced with nan
#plt.ylabel('No. de Tweets') #ax.yaxis.set_ticks_position('none')
# Calculate total stock price change within the date specified
# In order to make easy of feature extraction we change ddate dtypes from object to pandas datetime # So, we use to_datetime() function.
# concatenate tc physcial and electronic dataframes into one
# droping duplicate row
# CityName Column has fixed City Names # better!
# check team df shape and peek df (just team names and id)
# # Create a DataFrame containing the average compound score of each media sources's tweeets #df2.head()
# Estimate 'y' for these 'x' values and store them in 'estimates'.
# The MEAN of the fare amount the RateCodeID values equal to 2 or 3 or 4,  # each representing the airports JFK, Newark, Nassau or Westchester
# Exchanging the index to Datetime to allow for future timeseries analysis
### Fit Your Linear Model And Obtain the Results #fit the model #Summary of the model
# df_lib_con.title = df_lib_con.title.str.replace('\d+', '')
q5d_answer = r"""$ Years are covered at 2013 to 2016. The number of inspections each year varies widely.$ """$
# delete the 'to be deleted' columns # no way to do this, so create a duplicate table containing the columns we do want # now we have a time series with blood glucose
# aggregations wont result in errors but aren't very useful
#calculating the number of unique users 
# Calculate % change from quarter to quarter, find those with change greater than 100%
#drop some columns
# Simulate n_new transactions with a convert rate of p_new by using random.choice() to fill the array with 1s or 0s
# Get the .txt files
#create clean role description data #special character removal, stemming and stop word removal #seperate new stemmed words with space only
# Sort output (increasing by year, then decreasing by total pageviews)
# Predict probability estimates instead of 0/1 class labels
# looks like a bunch of empty strings when a location isn't found, so that's fine.
##CSV file contents can be imported into a DataFrame using the pd.read_csv() function
# check index have correct freq "<MonthEnd>"
# 10-fold cross-validation with two features (excluding Newspaper)
# df['CreatedDate2'] = pd.to_datetime(df['CreatedDate'], unit='ms')
# Create the deployment
# Average those cities together to get the default usage value.
#db.drop_collection('fs.files') #db.drop_collection('fs.chunks')
## check the number of rolling features
# Save the query results as a Pandas DataFrame 
# We extract the mean of lenghts:
# convert your "Date" objects into datetime objects.
""" count of goup landing on treatment group"""$
# ~ means 'not', thus it reverses True and False
# Saves an image of our chart so that we can view it in a folder
# Perform date transformations
# chaining head onto the frequency count
# determine n_new by counting the elements of the treatment group
# Set the X and y
# Calculate the user numbers of control group and treatment group in different countries
# Design a query to retrieve the last 12 months of precipitation data and plot the result # Need to find the last date in data
# Group the data by media outlet #groupedNews.head()
# Sort the DataFrame by date
#Select all from mesurements table
# Split the string
# Import census tool data pull
# tweetsIRMA = pd.read_sql("SELECT tc.tweet_id,tc.longitude, tc.latitude, i.lon, i.lat, sqrt(pow(tc.longitude - i.lon,2)+pow(tc.latitude - i.lat,2)) as 'distance', atan(tc.latitude - i.lat,tc.longitude - i.lon)*57.2958 as 'angle' FROM tweetIRMA ti JOIN irma i on ti.irmaTime = i.DateTime JOIN tweetCoords tc on tc.tweet_id = ti.tweet_id",Database().myDB)
# 3. 
#Albuquerque
#Sorting Data with multiple columns #cust_demo.sort_values(by=['Location', 'Gender'], ascending= False).head(5)
# Tell us what our critical value at 95% confidence is
#Verifying the DataSets Created
# Q2
# Looks like districts tend to not provision accounts for their secondary apps?
#Number of Control Users
# fastest way I know to do this
#Test the return values
# Find the probability indiviual control group they converted
#Plot null distribution
#Or we can just show the count by a single column
# Checking out statistics for transaction order
#model 3 is accurate .72 out of 1
# analyze validtation between Jarvis simulation and observation data.
### output to local tsv file: image-predictions.tsv
#Random Forest Result 
#Get the top 10 locations of the tweet authors (include no location listing)
##categories can be considered counts over time. Which could then become ratios.
#using the value_counts() method to get the number of values in the group column. 
# ask what is the name of the country for each year # with the least life expectancy
# create a Series with a PeriodIndex and which # represents all calendar month period in 2013 and 2014
# Checking sample data
# create new column "age" by subtracting fetched time - created time. # To make it readable, timedelta64 is needed. [m] gives by minutes-long, [h] by hours. 
# note the index added as a primary key # this is required to make certain calculations -- it will be added automatically
#info df
#Calculate the number of unique users in the dataset using nunique()
#Read tweets from csv file
# Clean up the 'brand' column.  Strip spaces and proper case strings.
# Duplicates and Dropping
#added the year 2018 in sublime and excel. created 2 separate files because the date was different format. Next I'll convert the dates to datetimeformat and then merge the to datarames back together
#getting dummies for countries
# Check is Data is imbalanced
#fitting the model
#view if there is any missing value
# at first I didn't realize this produced the lunch data I wanted, so... # I tried to find another way to extract lunch data # a long story follows but it worked too
# create pca to explain 99% of data # fit our scaled data
#Saving the model to HDFS
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# prices could have been sorted how to do ibra
# Print sentiment percentages
# set index as datetimeindex
#plot
#Too wide
# drop the duplicate #recheck  # as to see we got now only 290584 rows and we deleted one row
# How many stations are available in this dataset? # Design a query to calculate the total number of stations.
# extract full monthly grid
# List the stations and the counts in descending order.
# construct a date by hand
# Read in the dataset and take a look at the top few rows 
#distribution of retweet_count
#n_new would be where group is equal to  'control'
# Create df from rf model predictions of test set data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# convert strings to dates using datetime.strptime:
# Build models
#significant level of z-score is
#This will give the total number of words in the vocabolary created from this dataset
#Counting frequency of values in a dataframe column #Counting frequency in a series variable
# Now lets specify the column name 
# loop through all period objects in the index # printing start and end time of each
# time between questions posing and first answer
# Save references to each table
# answer
#Extract years and months into a new columns in our dataframe #Have a look...
# remove subject with only one time recorded
# Name
# pass the url and the season to the passing method in the GameLog class.
# Next, we will fit the data with the training feature matrix X and training response vector y
#Add a query to a dataframe #View data
#renaming multiple columns. changes are permanent
# create P attribute  # create Plot attribute with open_netcdf method
#worked
# Counting the no. commits per year # Listing the first rows
#session query
#Summarize, using our own percentiles
# YOUR CODE HERE #raise NotImplementedError()
# check dataset
#ACF and PACF plots:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# RE.SPLIT
#verify if connection is established #part 1- create session to connect to DB engine
### 5b. # Step 9: count how many months each marketing channel had lowest conversions
# Read the new data set
# a Jupyter thing to send the encoding to Arvados via CWL.
# Inspecting tables in the sqlite database
#Displaying number of unique users in the dataset
#...
# results are returned as an iterable list
# df.head() to avoid scrolling
# Now that all of the states have the same start dates -- fillna with zero then take the rolling averages
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#'San Francisco': '5a110d312052166f'
# Group by groups to answer the question
#reading in csv files
# Slice using just the "outer" index. #
# Multinomial NB model on age and subreddit 
# we should only use the rows that we can feel confident in the accuracy of the data. so remove them. and define the new df
# Using pandas value_counts function to aggregate types
# plot the sum of the columns
# Print the columns of df
#Checking for linear relationship
# len(xres3) # len(xres3.items()) # object.keys(xres).length
## Non-Values snip that will be removed ##
# Examining the paragraphs found in the body / the body data
# -a show attributes, -v verbose
#db.fs.files.createIndex({ "text": "text" } ) # need to implement this in pymongo, works in mongo shell
# Round capacity values as well as the efficiency estimate to five decimals-
# jupyterworkflow.data is a created package to act as a module, which contains a function that helps with getting the data only if the data is needed and other operations. # the data is assigned to Fremont
#uncomment the next line to install. #!conda install -y pandas-datareader
# Now comes the training and testing
#Last check for NA values
#check how many values are missing
# most missing values in Specialty are now filled
# Collect data from the Fankfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
# plot all of the columns
# No missing values because of :" RangeIndex: 294478 entries, 0 to 294477 "
# Look at Subreddit and Age features only  # Define X and y
#. Checking all correct rows are removed
# Podemos crear un dataframe como sigue: # Hacemos un display del dataframe:
# To include missing values in the distribution and to use percentages # instead of counts, chain value_counts(normalize = True, dropna = False)
#getting the number of old page landings. 
# step 1 - create boolean Series # step 2 - do boolean selection
# Drop duplicates 2862 in user_id # Verify the drop action
# Use the session to query measurments table and display the first 5 locations
# create a list containing Pearson's correlation between 'overall_rating' with each column in cols
# Select only required columns
# i need to kill 2 days with no data
#write to data frame
# The MEDIAN trip distance grouped by hour of day
# drop duplicates
# This analysis shows that there are more nulls in last four years
# Reactions anzeigen
# shortcut: GridSearchCV automatically refits the best model with the entire dataset, and can be used to make predictions
#join the data frames
# Check for any missing data
#Probability of receiving the new_page
#calculate the difference of simulated converted proportion  # between old and new page user
# without resample
# Load BTC data 
# To create a column for the intercept # To create a dummy variable column #df2[['new_page','old_page']] = pd.get_dummies(df2['landing_page'])
# converting to date to pd date time
# fill values from forward
# Preview data.  Credit Line Age (CLAge) has missing values. The head function is a CAS function that mimics the python head
# Data frame is ready
# Get the count of total number of records in the dataframe
# df['no_cumulative'] = df.groupby(['name'])['no'].apply(lambda x: x.cumsum())
#This also reveals something erroneous about the data. THe denominator should be out of 10. #But the mean is 10.45. It's also odd that the max is 2356.000000 and the min is 0.
#nunique fuction to extract the number of users:
# Referencing the adj_close dataframe from above
### Fit Your Linear Model And Obtain the Results
#Mars Images
# Let's save this clean dataset having no duplicates or records with missing or mismatched values & use this dataset in next sections
# Location normal plot
# There is no missing values
#create and display foxnews sentiments dataframe
#result summary
# replacing 'nan' with '' for improved legibility # print(df_final.columns)
# Critical Value at 95% confidence interval
#the dates are initially strings --> convert them to datetime format 
# How many stations are available in this dataset?
#Use the iloc method 
#merging the particpants column into the base table
# To check that indeed 500 tweets were examined, count them
# ANSWER CODE CELL FOR TASK B INITIAL
# Display results
# logistic regression # results
# filter on Suburban # len(Suburban) # Suburban
# print lxml.html.tostring(item)
"""$ Use this cell to run the preliminary test and calculate the baseline accuracy of the newly created classifer$ """$
# Split off the HTTP headers
# Read the csv file we just created into memory in order to downcast # We can pass type_map to pd.read_csv to specify the types we want
# save as csv file
# View the first five rows of data.
# Plot all "Categories" and their occurrence count.
# plot number of comments distribution
#users whose most recent scn-status is canceled followed by trialing and were mis-categorized as churn
# count words
# have a peek at the data
# look at summary stats for ds[123]
#Initialize the server 
# Find the more info button and click that
# The column names are taken alphabetically from the keys of the dictionary # The rows are taken from the union of the indices. It will use numerical row indices if there are now labels given. # NaN stands for not a number
# A little EDA:
#CALCULATES THE NUMBER OF MISMATCHING VALUES UTILIZING .GROUPBY FUNCTIONS
#You can load json, text and other files using sqlContext #unlinke an RDD, this will attempt to create a schema around the data #self describing data works really well for this
# remove zeros
# Read sql data
#Looks like we're good on missing values. Now let's get a summary of the data to see if there's any more issues with it
#reading in csv files
#2. Running averages - First counting events/flags of interest in bwd and fwd diretions
#x=dataframe['Date']
#print(tweet.created_at,tweet.text)
# replace vendor id with an equivalent hash function
# Pivoted data gets all the na fields filled with 0 and it transposes the the data/matrix/arrays getting all the values from it. # prints the shape of the data which is 2159 crossings in 24 hours
#use the training set to create a standard scaler for validation  #and test sets, finds the mean and standard deviation of the train set
#starting with copying df_arch
#generate nodes weekly dataframe
# join the output of the previos grouping action to the input data set  # the goal is to optain an additional column which will contain all authors of that particulair publication
# This pivot table will index on the Ticker and Acquisition Date, and find the max adjusted close.
#import the prepped dataframe that contains weather data and start date and age and distance
# wordjes = 
# Design a query to retrieve the last 12 months from 08-23-2017 of precipitation data and plot the results. Select only # the date and prcp values
#rf_enc = OneHotEncoder() #rf_enc.fit(rf.apply(X_train))
# gDateEnergy_content.count()
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
### Exponentiate 
#Use tweepy.OAuthHandler to create an authentication using the given key and secret #Connect to the Twitter API using the authentication
# Merging data of "twitter-archive-enhanced.csv" and data from twitter api # Drop 'id' column
# files8['Tenure']=files8['Tenure']/86400
# All rounded players..
# Test score is better than the train score - model generalizes well 
# Correcting an issue of two variables called the same.
# Setup the logistic regression model
#Note : The time column should be of unix format
# Find conversion rate for p old under the null 
#It seems to be difficult to convert this column to int, so I use string comparision instead
# We Create a DataFrame that only has Bob's data # We display bob_shopping_cart
#Subset columns
# Unique users
# how significant our z-score is
# In order of most delays: # Blue (199), Red (149), Green (110), Yellow (80), Orange (66)
# view the resulting isochrone shape (can you guess why there are separated geographies?)
# See first 10 stations that are not shared
# 3.2.C OUTPUT/ANSWER #bottom 10
#The z-score, p-value and the significance of the z-score is computed.  
# Specify where the dataset is saved and the name of the file
# Create an array of zeros of length n
# To check wheather our dummy variables are create or not
#instantiate, fit and run summary of new regression model with 'ab_page', 'UK', 'US', and Canada as the baseline
'''Reviewing the Y Dataframe'''$
## Get the contents of interest: all the p's # ** p means paragraph in html. Check more tag definitions on w3schools.org
# Note here we are initializing a dataframe with a dict of 1D ndarrays (numpy arrays)
#old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])
# Making a test user dataframe and change [user_id = 2]'s created-on date
#Divide p-value by 2 since this a one tail test (upper tail test)  #Since this is a right tail test p-value for right tail is 1-p-value on left tail
# probability of converting
#2012-2013 NYC traffic before cleaning
# Read in dataset
# OK. Now I want to look again at my dataset head to remind myself of the column names and values for each.
# Change Indicator name to Indicator_Id temporarily, there will not be any change to actual df
# Let's join the column to the rest of the data
# convert crfa_c to ASCII equivalents
#convert rate for p_old is the same as convert rate for dataset regardless of ab group
# Task 2
# Probability of control group converting
# delta represents the temporal difference between two datetime objects
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#appends a new column that tells us if this tweet is a repeat # we need to filter these tweets out! #new dataframe df_filtered with repeat tweets filtered out (this keeps first version but not repeated version)
# make a time series of mean score per day
# decompose
#!/usr/bin/env python3 # -*- coding: UTF-8 -*-
# groupby Funding Type and count states within each type
# Fill the address from table [EDW].[SHIPTO_DIM]: CustomerID = '0000100273'
# add ToT sum by student_program # drop duplicates on student_section, reduce columns
# drop ceil_15min from data
#Get rid of unamed column in tips
# checking out their unique values, for a single level  # checking out their unique values, for combinations of multiple levels # See answer at https://stackoverflow.com/questions/39080555/pandas-get-level-values-for-multiple-columns
#Test
class Square(Rectangle):$     """ Simple circle, inheriting from Rectangle """$
# Predict_gnb
# Data importing (we will use data from Pandas Case Study)
#So saving and reading as a pickle file
# view the head of our df
# Now, creating a Dataframe for the post: "FCC Announces Plan to Repeal Net Neutrality", # Note that the column named "index" describes each row's index from the source, aggregate Dataframe.
#drop guidance_high and guidance_low, always NaN
# Display confusion matrix
# Use your model to make prediction on our test set. 
#we will exponentiate treatment
# reciprocals of the coeffiecients
# response.status_code, response.url
# Percent of true gains over all samples: compare to precision -- precision should be signficantly higher if this is a useful # prediction # (cnf_matrix[1,0] + cnf_matrix[1,1]) / cnf_matrix.sum() 
#     df_tweets['polarity'] = TextBlob(tweet).sentiment.polarity #     df_tweets['subjectivity'] = TextBlob(tweet).sentiment.subjectivity
# Export data to csv
# So, are they aligned?
# How significant is the z-score  # What is the critical value with alpha 5%.
# Calcular rendimientos diarios y graficarlos
# From a list or numpy array
#sorting Values in DataSet by Year
#dealing with json and wrangling
#housing prices
# values count for countries.
#converting users columns to timestamp format
#Export all features in a file to check correlations
#'Madison': 'e67427d9b4126602'
#read data into pandas with Latin-1 encoding to prevent errors
# Replace key with Googlemap API developer key # https://github.com/googlemaps/google-maps-services-python
# listing all of the collections in our database:
#group data  #total fares for pie slice
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# import data
#now store the cleaned up dataframes as csv files
# Convert both dictionaries to series objects, using the series constructor
# Drop meaningless columns
#id_str is meaningless, so we're getting rid of it. 
#create dummy variables for the 'country' column and join to df2
### Fit Your Linear Model And Obtain the Results
# Save references to each table
#Used this to simulate 10000 iterations of binomial choices bases on the probability CRnew,CRold which are equal (P_null) #we put the difference in an list called p_diffs
# And recommendations:
# Switch to an integer index by moving 'Date' back to being a column
# Split text into sentences
# How many users whose last session creation time is null = 3177/12000
# Declare the constants. # Type I error, alpha level, significance level. #bootstrap_number_samples = 10000
# summarizing the converted timestamp column
#Select top 100 values using 'nlargest'
#under null
#read data
# is there a difference in the age distribution # between classes
# to find unique user_id
### Deal with the outliers of the data convert it to the statistical mean of the data check how to do that using other attributes  ### for the time being fill it with mean 
#Refernce paths for data files
# initialize preprocessing 
#That's our result! Now let's export this dataframe into a csv in case we want to work with it later. 
# Creating the authentication object # Setting your access token and secret # Creating the API object while passing in auth information
### Fit Your Linear Model And Obtain the Results #Fit and show the results
#  Yearly interest to be paid off:
# delete rows with index 416,562,689,748,848,897, Do not reset indexes until the cleaning operation has happened
#inspect station data.
# Save the dataset in csv file
## download the image-predictions.tsv
# Bar plot of gender
#Check duplicates
# also we can specify the intersection of rows
# List the stations and the counts in descending order.
# Remove mismatched  rows
# create a new DataFrame to contain new columns date, month, year # data integrity check:  check how many years of data exist in df_visits_master
#Left join the sales_data to the returns_data on Order ID
# Fitting the regression model
# In our case bigram is enough
#by Kadhim Ajrash  and Khalid Al Ansary, 23 May 2017
#Checking the values of the df
#one nice feature of the notebooks and python is that we can show it in a table via Pandas
## Reading file as tab delimited file ##
# okay: re_json['statuses']
# Results
#converting tweet_id, 'in_reply_to_status_id', and 'in_reply_to_user_id' to object  #converting timestamp to datetime type  #converting tweet_id to object 
# First let's turn each title into a list of words # Then we'll use the lemmatizer to change plurals to singulars to group them # And rejoin the lists of words to one string for the count vectorizer
# converting the timestamp column # summarizing the converted timestamp column
# Converting the data types so that the "tobs" column will return a value
# calculating number of commits # calculating number of authors # printing out the results
# save new cleaned dataset #read new dataset in new dataframe df2
# ! cd /home/ubuntu/s3/flight_1_5/extracted # ! mv /home/ubuntu/s3/flight_1_5/extracted/final_results/*.txt . # -exec runs any command,  {} inserts the filename found, \; marks the end of the exec command.
# we can get the first value in the date column
#CALCULATES THE PROPORTION of converted users within the treated group reading new_page
# create PCA # fit oru data
# lets look at brands that make up at least 2% of the observations
# Setup Tweepy API Authentication
# controls the session # closes session
# Let's check what types of theft we have and how many
## drop a collection via pymongo #client['test_database'].drop_collection('posts')
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure #my_data = pd.DataFrame(data['dataset']['data'])
# 9302 records with Status == "Operating" #print(len(oper_df.index))
#Read csv
#make sure user ids in each dataframe match up
#'Oakland': 'ab2f2fac83aa388d'
# Import data from Azure Blob Storage
### find the duration of the experiment
# Get feature names and class labels
# we just need one value of the groupby object for unique turnstile total daily entry count # this is one way but I later realized there were two better methods 
# query values in set:
# Put in function to remove stop words
# make predictions for test data # Convert numpy array to list
# Load JSON as Python Dictionary, for data in 2017
# Create DataFrames from list of dictionaries # Save the dataFrame in file
# each key of 'words' is a word, each value its index # printing some random key:value pairs of 'words'
# What companies run the rank 5/10 most delayed?
'''$ '''$ # get investors with high centrality
# Python's SQLite let's you use either '==' or '=', but I think SQL only allows '=', okay?
# 0.1212626 users are converted
#Analyze outputs for new config
#opening the ab_data.csv file
#find unqiue users 
# Load the query results into a Pandas DataFrame
#Researching ads where both candidates are there 
# Set up a collection name "test_database" in MongoDB
## Remove Nones and print highest and lowest prices
# can also use
#new dataframe size
# Combine each stomatal resistance parameterizations # add label 
# Encontrar los registros que se encuentran entre las posiciones [10mo - 20vo] de mi DataFrame
#Every genre is separated by a | so we simply have to call the split function on |
# leave only uncanceled ride
# forward-fill a dataframe by specficing the axis # null values remain if there is no previous value
# we can see ~our~ most followed users,
# Theresa May indices
#checking the average conversion rate
# group by combination of 'A' and 'B' for taking sum 
# Look for null or duplicated values
# set the X and y
# The convert rate is the same for p old under the null
#Unique user ids
# Since the underlying data of Fundamentals.exchange_id # is of type string, .latest return a Classifier
# checking whether the data has any null values
# Now, let's read this dataset into another dataframe
# The "doesnt_match" function will try to deduce which word in a set is most dissimilar from the others:
# Read in reflectance hdf5 file
# Sure enough, the read_csv method sucessfully converted our data to use a DatetimeIndex. # Unlike with plotting numpy arrays, where we had to provide a formatter for diplaying dates, # pandas is smart enough to do it for us
# find historical data for 2007
# people can resubmit their projects a 2nd, 3rd etc. time...So there are repeates. # Therefore, for this dataset maybe we should only analyze the 1st submissions.
# dropping rows where 'location' NaNs
# Ensure to Convert df_TempIrregular.detectionTimeStamp to datetime
# write file to path
# filter the df with new_page in landing_page # count the number of user recieved the new page
# Second option - the IndexSlice object
#pd.scatter_matrix(cust_data._get_numeric_data()) #pd.scatter_matrix(cust_demo.select_dtypes(include = ['number']))
# We create a column with the result of the analysis:
# Step 7: Display the shape of the pivoted data transposed ## Think in terms of 2036 observations and each observation includes 24 hours (i.e features) ## Review the PCA - Principal Component Analysis from sklearn
# instatiate the model and then assign to the country variable the fit model
#this are the columns not present in the table and yes in view
#now pass that series to the datafram to subset it
# Now we use the Base class to reflect the database tables
# Get a slice with IndexSlice
#== Transpose (like numpy)
# Plot line for observed statistic
# creating a model
# Instantiating and fitting the model.
# get the current local time and demonstrate there is no # timezone info by default
# histogram of num_comments
#9.7 Delete Object Properties # You can delete properties on objects by using the del keyword:
### Step 13: Create a scatter plot to compare the two data sets ## The clusters suggests 2 unique type of days
# Renaming the columns to station, name, tobs
#Sorts the table by Profit. #Since 'pro_result' is a pre-sorted table, we can use just .head() to display the top 5 values.
#Search for missing values
#find no of columns and non-empty fields in dataset
### Fit Your Linear Model And Obtain the Results
#Create a folder called network.  #Within this folder, we create a folder'recurrent_network' where the nodes and edges in our example will be saved
# look at the distinct values for 'CRIME_CATEGORY_DESCRIPTION'
# import new package 
# create end date column from period index
# compute p value
# analysis_df.count() # analysis_df.count # analysis_df['compound'].mean()
#df_times.describe()
# Double check whether all of the correct rows were removed ( this should be 0 )
# Filter customers above 70th percentile
# TASK B ANSWER CHECK
### top 10 locations with Michael Kors mentions
#'St. Paul': '60e2c37980197297'
# Station and Measurement
# as in get_child_column_names(observations_node)
#According to the null hypotheses, there is no difference in conversion based on the page, which means the conversions for each page are the same.
# adding an intercept column to the dataframe # creating a new columns with 0,1 values depending  #wether user is from the control or treatment group
# Check the data types for the remaining columns
# Save corpus for later
# convert column into int type 
# Perform a MWW test # Print the p-value. Note I have specified I want to print a floating point decimal with 15 decimals after the period
# rename cols and change data type
#a computer can understand
# and the first ten records
# Find highly correlated columns 
# Fit only to the training data
#Find the number of rows 
#plt.figure(figsize=(12,6)) #plt.legend(loc='best', frameon=False)
# (c)
#we assumed Pnew and Pold are equal
#drop actual == NaN, since we don't know beat or not...
#print df
#Train Model and Predict with the best k we founded in the train X data set
# money flow index (14 day) # mean-centered money flow index
#Chula Vista
#Honolulu precipitation data for 12 months
# creating a copy of dataframe df # dropping rows where pages and group don't line up
# Probability of of recieving new page 
# Default actions is a left join on the indexes
#distribution of favorite_count
# Construct all tallies needed for the multi-group cross section library
#Which is missing?
# Histogram with the range of 7 ~ 10 miles
# check if any values are present
# Confusion Matrix
# sentiment_df[sentiment_df['name'] == "CBS"]
#print zscore and pvalue.
#Get rid of columns we won't use in logistic regression
# To filter these active devices in product-a
# Step 22: add the outlier anomalies to the plot
### Number 2
#Examine the relationship between score and number of comments 
## Read file of organizations 6,696 records - was organization.csv 497,523 #unique_org.head(5)   #print(len(unique_org.index)) 
## convert occured_at to datetime
#get rid of all the PPK data with no photos and the empty fields
# make copy of the dataframe
#read it into a dataframe
# Create corpus # A corpus is a list of bags of words
# try another plot type # move the cursor to the right of the period and hit tab
# count by date in datetimes
# Assign directory paths and SQLite file name
# Read in our data with more help from the read_csv parameters
# Is a wine more likely to be "tropical" or "fruity"? Create a `Series` counting how many times each of these  # two words appears in the `description` column in the dataset.
#since the size of the whole data is huge, I will just leave random 20% as test set, and rest 80% as training set #get training and testing data
# Check if all countries are United States of America.
#daily counts for turnstile (df["C/A"] == 'A002')&(df.UNIT == 'R051')&(df.SCP=='02-00-00')&(df.STATION=='59 ST')
# train_ratio = 0.9
# print("Accuracy = %g" % accuracy) # print("Test Error = %g" % (1.0 - accuracy))
# create a column for all of the country codes  # c1 is US, c2 is UK, and c3 is CA
# retrain and reevaluate the model
#set up reddit connection
#Save to a csv file
# Save DF to csv file
# converting all fields to strings for ease of comparison
# NASA Mars News - title
# Your friendly tokenizer # Numpy
# Define default tick locations for our plots
# A:
#transfer to dataframe
#  Compute "geovolatility" per our definition.
# Removing 2018 because it's not a full year
#Calculate average math score
# Create Geometry and set root Universe
# split one review into separate words # remove stop words from review text
#check the datatypes
# Requests
# group by names  # show the replicated names
# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)
# Neural net predictions
#Concatenate (or join) the pre and post summary objects
# default value of size=100
# y axis for funding_average_per_type_bar_chart  #fund_avgs
# Set file names for train and test data
#downloading dataset
# push each tuple of calculations into a list called `normals`
# Show the nodes file
# compute actual difference in conversion rate
# To flatten after combined everything. 
#get the first transaction per user
# output2.take(2)
# your code here
#trainDataVecs.shape #testDataVecs.shape #np.any(np.isnan(trainDataVecs))
# Scoring
# current.head() # print(data_current) # data_current
# Design a query to calculate the total number of stations.
# group by blocks
"""$ Check number of news per day$ """$
# cisuabg7
#Since, there is consistency with this id,  #we can probably just choose either and remove.  #We shouldn't be counting the same user more than once.
# check size
# Check for null values
#random.randint - Allows you to generate random numbers #We are generating random numbers between 1 & 10.  #We are generating the number of numbers in the range rng i.e. 72 numbers in our example
# Package the result as dataframe
# To find the active devices from duplicate_check_df # put active devices (more than once in device_id) in a list # To show how many devices are active
#Reset the index of the new sorted dataframe, change the column name, and check it out
# nanosecond based time
# Transmission 2020 [GWh], late sprint
# Dump data to pickle # top_100_2016 = pickle.load( open( "top_movies_100_year2016_list.p", "rb" ))
# run a file to set creds as environment variables
# query total number of stations
    """$     Return a new SoundCloud Client object.$     """$
#training the dataset
# save fixed result to repo # prepared, models, recs
# 75 rows and 22 columns
#load data into pandas dataframe
# lets look at the summary
#verifying the dataframe contains 100 tweets from each of the news channels
#Diff of signup to purchase #fraud_country['purchase_delta'] = fraud_country['purchase_time'] - fraud_country['signup_time'] #Bucketing purchase value
# Importing data from the processed 'news' DataFrame. # Define current working DataFrame.
#numero di data record con price nullo
# id is just a identifier, so generate features of data
# you can overwrite the former df_restaurants
# inspect the tables in the database before establishing connection
# write frame to csv
#Compute the number of unique users who has old page  using df2 dataframe #display the number of unique users who has new page
# Choosing only slected columns from
#compute test statistic and p-value for one-sided hypothesis test in which #conversion rate difference is greater than observed difference
# pairings < 0m / all pairings, since 3-01-2018
# Desktop with Development C++
# Run the prediction
#Use this to setup sql imports/exports later
# Update emoji dict. Eventually unicode-escape this.
# find historical data for 2016
# To get the total number of duplicated under user_id 
# One hot encode features with boolean values 
#User Id that appears twice in df2
#need df for on demand to creat price list
# group modalities in FORMULE
# Fill in the list_files variable with a list of all the names of the files in the zip file # YOUR CODE HERE #raise NotImplementedError()
#Reformat variables
# Make the model with the specified regularization parameter # Train on the training data
#load into transactions dataframe#load in 
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
#Concat two original contractor_bus_name and contractor_number column  #into a unique contractor_bus_name per record
# create a new DataFrame from calls where the location below removed
# split the data set into mentor-mentee based on the number of collaborators a person has
#write to a CSV file
# load the source data
# ntree_limits the number of trees in the prediction; defaults to 0 (use all trees)
# What is k_var? Entire cleaned dataset with all outcomes
#Read the last 5 records
# Choose a start date and end date for your trip. Make sure that your vacation range is approximately 3-15 days total.
#create copy of dataframe and sort by rating numerator
# convert the price data to a dictionary
# check shape
#Create predictions and evaluate
# Check the variables in column country
# Because NumPy evaluates each subexpression, this is roughly equivalent to the following
#Example 5: Specify dot (.) values as missing values
"""$ descriptive stats$ """$
# merge with main df
# Calculate n_new and n_old
# Get metadata documents
#Investigating sponsor
# Lets set a style
# Compute the proportion of users converted # Convert to a percentage # Print the proportion of users convereted
# summaries
# Step 4: quick reality check.
# build the vectors that will be the pradicating features
# as expected, weekday has higher usage # Lets look at the hourly trend on weekday versus weekends # group by a flag marking the weekend and time of day
# revise "fuel_type"
# Tells us how significant our z-score is # for our single-sides test, assumed at 95% confidence level, we calculate: 
# successful campaigns (sorted by counts)
# get multiple sections with the term fees # use SpaCy to determine what type of fees
# Add Text Data
# Let's try with our homes median list price data, which we will want to be time-indexed
#Calculate the P -value
# Create directory for new data files # Generate a new data file
#We create time series for data:
# Print information about the dataframe to see if any rows contain null data
# let's explore
#  set the index to the date column.
# interpreting the summary
# create distribution under the null hypothesis
# pandas default datatype given
#find vs index
# The protocol version used is detected automatically, so we do not # have to specify it.
# (5.288388269523054, 1.2339882755533999e-07)
# We first name the file that contains our data of interest
#Print the raw comment and output of get_text to compare: #It's supposed to remove tags and markup
# check option and selected method of (11) choice of groundwater parameterization in Decision file
#abc = abc.reset_index(level=[0,1]) #abc = abc.reset_index()
# Print the value counts for 'Site Fill'
# Delete a table
# Further Hacking (the origine of timestamp)
#sqlContext is used for defining Dataframes and working with SparkSQL #use sc to create our sqlContext, sc has the connection information for the #Spark enviroment
# Create dataframe from the .csv files or simply read in csv files. 
# Index persistence
# Likewise for dates # you can also use .astype('datetime64[ns]')
# Make a new dataframe with only 1 star and 5 star reviews
#Create Dataframe
## Coverted Rate with respect to control group 
# Be careful with arithmetic operations on series # whose indices don't match up!
#('J:\Financial Forecasting\Development\Electricity\Load Revisions\Q2_2017', index=False, encoding='utf-8')
#Simulate  nnew  transactions with a convert rate of  pnewpnew  under the null
#r= average_daily_return(data,portfolio=True) #print(r)
# Add the 95 % confidence interval of the median.
# should we need to load the model
#probability of conversion for new page
# correlation matrix
# Arrays for Bar Charts
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("hit_tracker.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
# 4. What was the largest change in any one day (based on High and Low price)?
# are the observations in train in history ?
#install XGBoost
# run query, convert results to a local pandas dataframe, and display as an HTML table.
# First overview of dataset
# first tweet of 9/20/2017
#hours.columns, hours.index
#Drawing samples from a binomial distribution of probability p_new
# print(i1)
# introduce some condition, e.g. only users with 14 or more consecutive days
# Here's how we rename them # Here's how we add new categories # Here's how to reorder
# identify wells with two measurements
## What is the total amount of the negative?
# Homework Basic Regex 2
# add country data to previous data
#model_713.forecast(5)
'''$ Number of appointments by student$ '''$
# find the names of the recipes
# Read in the data
# verify that 6 sets have been removed from all_sets
#Like Vs retweets visualization:
## To determine the proportion of users converted
#### Define #### # Convert id column from a int to a string # #### Code ####
### Fit Your Linear Model And Obtain the Results
# quick check
# Average polarity by News Source # View Polarities
#propiedades entre 75 y 100 metros cuadrados
# Density of data # % of seconds with trade
# Resample (grouping) by month ("M") and counting the number of complaints
# media de tmax anual
# create a Series of incremental values # index by hour through all of August 2014
### Create the necessary dummy variables
# # Perform a query to retrieve the data and precipitation scores # Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
#Create a column "is_lasvegas" where 0 is no, 1 is yes
#Loading Data
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# current fire predictions for grid cells
# treatment group not receiving the new page # control group not receiving the old page # total
# how does our result look?  Should get something like this.
#Convert datetime to an actual datetime object
# export the Geodataframe as shapefile
# Becasue we assume under the null that p_new = p_old = p_converted # We get p_old = 0.1196
# Tells us how significant our z-score is #Tells us what our critical value at 95% confidence is
# simulate under null for n_new
#Full graph #fullgraph = nx.read_gpickle('all_tweets_pickle') #Bigraph
# When we dereference an instance method, we get a *bound method*; the instance method bound to the instance:
# remove items shipped before our start date # remove items shipped after our end date
# exclude tweets shorter than 3 words
# Save references to each table 
# Check the incoming schema, we want to convert datetime to the correct type. # format datetime field which comes in as string
# Calcuate the difference
# Drop unused columns from the dataframe
# Conversion rate for treatment group: 0.1188
#remove punctuation. Let's talk about that lambda x. #view output
#todo 1a. Display the first and last names of all actors from the table actor
#'Lubbock': '3f3f6803f117606d'
## no.of unique cats given out by the shelter 
# how many columns and rows do we have in data?
# Can be constructed from a structed array
# first 5 rows of the data
# Add sentiment as new column in dataframe # Display the first 10
#  numpy.setdiff1d() returns the sorted, unique values in ar1 that are not in ar2. #genco ids post-GEN from cameras paired post-genesisco which do not correspond to post-genesisco shopify orders
#checked that all dates are datetime 
# Task 1
## Try some of the different string format codes and see what happens
# get cat sizes
# run the commented out code to see list of all column names # cursor.fetchall()
#df = pd.read_pickle('/home/jm/Documents/testingStuff/my_file.pkl')
# Create dataframe with all treatment records # Compute the probability they converted # Display the probability they converted
# Stip off the year and save a list of %m-%d strings
# A:
#dropped the Sec filings column using .drop function
# The aggregate percentage of non-autoconfirmed creations over the entire period
# Get the statistical summary of age at booking by gender
# Calculate the mean
#Transforming the files3 dataset
# Find the number of unique users in the dataset
#PLace results into DataFrame #Set Index to Date
# your code here
# Create a dog type variable  
'''$     3 - Number of SQLs - Sales Sourced vs. Marketing Sourced$ '''$
# Split data into flights that were cancelled vs not cancelled
# Here it can be seen that less popular people have liked Paula's profile and more polar people doesn't.
# wide to long # view head of final row-wise dataset
# What about intersection between KBest and elnet only?
# wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'
# Check if drop was successful
# Because of GitHub space limits (no files over 2GB), train data file was split into 5 pieces # Loading the first file with header row to use for column names # Checking the columns present
# Convert rate under the null where Pnew = Pold
#getting dummies for body types of cars
# Lendo nosso conjunto de dados # Verificando as primeiras linhas do dataset
# Allign tweets in df and add column names and labels
#Reviewing the datatypes again to notice change.
#remove the row that contains a negative value for initiation days
# Probability of conversion 
#Show the histogram
# updated: proportion of p_diffs greater than the actual observed difference in original dataset (p_diff calculated above)
# Checking who all pitch artist to the user.
# Restart runtime to allow Jupyter to know the changes above
# pickle.dump(ab_groups, open("ab_groups.p", "wb"))
# Load in your own data
### Create the necessary dummy variables
# Counts of users that received the new page.
# estructura bd
# Write to CSV -- to insure code runs if there is no internet connectivity and API can't be called
# Number of rows 294478
# Explore the action space # Generate some samples from the action space
# copy cleaned dataframe
# define the date range for imagery
# determining the last sensible commit timestamp
# Inspect maximum number of comments in the recent scrape 
# find the average attendance for the 2015 BOS season to impute the missing values
#change the timestamp datatype to datetime object
#plt.ylim(200,400); #plt.xlim(datetime.datetime(2018,2,7),datetime.datetime(2018,3,24))
### Fit Your Linear Model And Obtain the Results
#Read dataset
### since we do not need all three new columns we drop `CA`
# resample to minute intervals
# write making the worksheet name MSFT
# df_4_test.dtypes
#Change Dates to string fo manipulation
# And it's tail:
# removing NAs drastically drops count. Try imputation with mean value of sensor # check if operation needs to be performed. Losing out on valuable information
# City data
#Print accuracy
#df_csv.info()
# To create a new dataframe which has the right combination of entries for group and landing page : #  treatment/newpage or control/old_page
# Largest change in any one day
# Final Plot 1
#test_count_vect = CountVectorizer(vocabulary=unique_feature_list, tokenizer=lambda x: x.split(',')) #test_count_vect = CountVectorizer(vocabulary=count_vect.vocabulary_, tokenizer=lambda x: x.split(','))
# Import qgrid library and set some options for optimal display of tables with # big number of columns. Also, do not allow the widget to edit the values of a # dataframe
# The mean squared error
#List the unique values inthe HydrologicEvent column AND COUNT THE NUMBER OF RECORDS IN EACH
# check Forcing list data in file manager file
#Show the first 4 rows (again): Note that if we omit the lower bound, it assumes it's zero
# Reset index
#THIS CODE IS INTENDED TO FIND THE REVENUE 
#create arima model #model_701.forecast(5)
# year 2019 and up is clearly wrong
# Check how many tweets creatd every minute during the data collection period # At this point I just want to check the time trend of the tweets which can be done without time-zone conversion.
"""$ Group news by day of news_collected_time and concatenate news_titles$ """$
# create date time fields for sorting # drop duplicate records, which greatly reduces the size of the data frame
#### Read the updated dataframes for analysis
# Then we can compute covariance with Series, excluding missing values if present
# Fit the logistic model
# Transmission 2040 [GWh], marathon
#as above
#we create the trainig and testing data from the data we get
#doc source https://seaborn.pydata.org/generated/seaborn.countplot.html
# Small pieces of .csv documents can help me saving running time when I was restarted Kernel.
#Other lab courses taken by students
#get average-to-date for each PA / player to avoid info leakage
# how many rows, columns do we have now?
#Difference by 1 and view time series and resulting auto correlation
# Now let's see how this works in a base case, and with the labor # force growth rate boosted by 2% per year in the alternative case...
# Number of companies complained about
# ROC-AUC curve is its own Class. That seems kinda weird.
# Define parameters to classify outliers in "odometer_km" # IQR = Q3 - Q1 # LF = Q1 - (1.5 * IQR)
# Use the nunique func to get unique number
#create new dataset that meets the specifications
# Probability of control group converting
#now we will inspect the dataframe to inspect what the values look like
# How many stations are available in this dataset?
# checking the type first element in series
# testing proof of concept -- going to split 'location' and then access separately
#Since under the null hypothesis , both have the same convert rate, then we will calculate it regardless the page
# train and evaluate the chunker 
# Establish a new column for person's age at designation
# save data to csv
#Test whether the retweets are deleted
# REMOVE THIS LINE IF YOU ARE GOING TO PUT IN PRODUCTION
# Create and populate 'intercept' and 'ab_page' columns with zeros (control group)
# Sample five random ratings
### Complaint density also log-normal
### Fit Your Linear Model And Obtain the Results
# Rescale to 1 to 10 - Polarity is float which lies in the range of [-1,1] 
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# top 3 styles, minimum of x beers drank
#join_a.cache()
# get specific group
#Displaying unique users of new datset
# { "nameLast" : "Williams", "nameFirst" : "Ted" }
# results.summary() causing issue: https://github.com/statsmodels/statsmodels/issues/3931 # Workaround for issue
#Regression - Forecasting and Predicting
# print the row at the fifth position
#rename accounts with function name embedded to avoid confusion:
# load in the file using numpyloadtext
# Renaming the gender column
# Difference of simulated values
# make the resutls as a pandas dataframe
# create lookup key in HCAD dataset
# assign all of df to df2 # determine index of rows where treatment doesn't lined up with new_page
# groupby by defaut creates a (hierarchical) index:
# Convert completed time from UTC to EST
# We create time series for data:
# Number of populate for old Page
#URL for NASA Mars News Site
## Find the number of missing hours
## Now predict on the test set with the training model
# To visualize the number of each bike type used in al the trips
#list(df.brand)
# Test all of the assumptions
#Bin (categorize) the data in the dataframe and return the head.
## this function removes punctuation from a string
# Write the lambda function using replace # Write the lambda function using regular expressions # Print the head of tips
#most active stations over the entire data set. Defaults to descending order of count
# Selects only records with WordCount equals to 0 and count the number
# rimuove tutte le interruzioni (spero)
#request 2017 stock  #convert to dict 
## YOUR CODE HERE
# show the first 5 rows of data frame
# determine n_old by counting the elements of the control group
# write the scenario to an excel workbook
# input_col = ['msno','date']
# Add one column named "Total"
# export # ca_de_xml.to_csv(export_path, index=False)
#Now let's get the genres of every movie in our original dataframe #And drop the unnecessary information
#import additional libraries #calculate VIFs for variables in the model
#Create our first variable to hold the Air Passenger totals #Create a second variable to hold our Flu totals #Check out the data types so that we can make sure they aren't naughty when around one another.
#clean df...no nans
# Exponentiate results
# returning to numpy
# Let's again use the inner join method we used above
# Check that one row has been removed
#Importing Poll Data
#Copy the a temporary dataset for the cleaning purpose.
# Fitting a model for Country with US as baseline
#timeconsuming
# Database Setup
#index 0 is treatment i.e. new, 1 is control i.e. old # The sample size n-old is: 145274.
query="""SELECT dataid, count(*) FROM university.electricity_egauge_15min$ WHERE local_15min$ BETWEEN '01-01-2015' AND '01-01-2017' GROUP BY dataid """$
# we need to double check all tweets are actually in nsw #rough NSW bounding box
# Read 85% of the lines and filter to keep only those which are in the US
# We'll use gain first because that is the most useful for my purposes (saving more than $5 is necessary as it is cost of trade)
# Check the number of times the landing page not match the group # supposely 'new_page' should match to 'treatment', and 'old_page' should match to 'control'
# Identify rows which are transfers from Coinbase to GDAX
# export keys
# change NaN values in new column with "Other_ml
# trip_start_date
# population or sample size of treatment group
# To create a pandas dataframe from the events list # To preview the dataframe for first 5 rows.
# Simulate conversion rates under null hypothesis
#1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD). # Use requests package
# Concatenate above two dataframes and replace twitter_archive_clean
# cur.execute("""SELECT distinct pullid,pullquery from public.datapull_id limit 25""")$ # returns = cur.fetchall()
#Convert from cfs to cms (1 CFS = 0.028316847 CMS)
#In looking at unique values in DESC category, it does not appear to be anything we really care about.
# Spark SQL has the ability to infer the schema of JSON data and understand the structure of the data #once we have created the Dataframe, we can print out the schema to see the shape of the data
#Number of users landing on old page
# view the processed dataset
# Categorize time labels
#establish a conenction
# Get a list of column names and types # columns
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data
#performing an inner join on dataframe users and sessions for all matching Userid and matching Registered and SessionDate
# open netCDF file
# create my y variable.
# drop columns containing a null value
### Create the necessary dummy variables
#50% probability to receive new page
# Convert from reviews to features #X = vectorizer.fit_transform(df.review)
#ff6042c59f3870e2 -- original hash #fe6046c59f3870e2 -- new hash, off by 1
### Kate Spade related comments and reposts count per day 
# User number of treatment group n_new
#How about average number of retweets and favorites per timezone?
# create grid id 1 to 1535 and save as type string
# December 31, 2017 at 7:03pm from Online Store...unix timestamp datetime funk?
### Station Analysis #Design a query to calculate the total number of stations. #I'm doing a group_by just in case there are duplicate stations
#Getting count of NaNs in subjects column
# Counting the no. commits per year # Listing the first rows
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Split data
# We didn't set a column number of the index of giss_temp, we can do that afterwards:
#Create two new dataframe views: One with records before Falls Lake an one after
#checking results
# Encontrar el 20vo registro de mi DataFrame
# Add labels to X and Y axes :: Add title
# to compare score between train / test set.  
#plate_appearances.loc[plate_appearances.batter_name=='ryan howard',['is_shift','hit','successful_shift', #                                                                   'shift_lag_1', 'shift_lag_2']].head(20)
# Removing supspended state and limiting dataset to recent years
# User comments counts
#The sum of users that converted over the total number of users. 
# Read the file into a DataFrame: df # Print the head of df
#plot histogram of the 'Amazon Customer' # these customers never update images of the products.
#Simulating the new_page converted for the new probabilty for number of times as defined in n_new
#File from which input needs to be taken
# - Design a query to retrieve the last 12 months of temperature observation data (tobs). #   - Filter by the station with the highest number of observations.
# tokenize the text
#Los precios reales de las propiedades que uso para entrenar el algoritmo
# dateutil
#### Define #### # Convert tweet_id column from int type to object type # #### Code ####
## convert occured_at to datetime
# check size
# Checking the duplicate rows joined or not after left join # Note -- Some parcelid >2 hence not 125*2 row
# 6/11/2017 ~ 6/18/2017
#The critical level for a Type I error of 5% for a one-sided z-test
# %load ../solutions/sol_2311.py
# To prep the voting record for our merge, we drop columns we will no longer use
#Review number of users
#Reemplazamos los . por _ en los nombres de las columnas
# Display top 5 rows
# education
# perform logistic regression
# Train-test split for accuracy metrics
# summarize the data
# Logistic fitting
# Settings for plots
# Let's fit& score with Logistic Regression
### Fit Your Linear Model And Obtain the Results
# Sanity check: sort with commandline
# build the same model, making the 2 steps explicit # can be a non-repeatable, 1-pass generator
# Faltando dados
#Using get_dummies()
# Read data
# no longer necessary but it is a good idea to select only instance within the area and overwrite the file ## blight_in = blight_inc.apply(within_area,axis=1) ## blight_inc.loc[blight_in].to_csv(processed_path+"blight_incident_count.csv",index=False)
#Look at top browsers within OS w.r.t. Page views
#Instead of using Excel, we found a way to convert our month and year data to datetime format to make this column play nice.
# generate random 0,1 array based on the convert rate of new page group.
#'Newark': 'fa3435044b52ecc7'
# drop rows for mismatched treatment groups # drop rows for mismatched control groups
# larger means: prop > value (observed value), this is also justified by the graph above
# Score the predictions
#Stack method rotates the data frame so that columns are represented in rows #To complement this, unstack pivots from rows back to columns.
#Example1:
# For inputting the password
# row index of speaker gender-unidentified talks
# make class predictions for X_test_dtm # calculate accuracy of class predictions
# read tables
#*--------Merge Command to merge Studies and sponsors--------------------* #studies_c=studies_b.merge(countries,on='nct_id')
# Load total market cap data
# name is 'the'
# %matplotlib notebook
# Find the most recent date# 
#original calculation
# remove duplicate ID entries
# calculate probability of converting regardless of the page
# Fit Your Linear Model And Obtain the Results
# How many have cinnamon as an ingredient?
# Query for finding the most active stations # List the stations and observation counts in descending order
# check the simulation start and finish times
# Save a reference to the measurements table as `Measurement`
##   Creating and working with Temp Tables
# this list is hard to read
### START CODE HERE ### ### END CODE HERE ###
# create from list of Series
# How many stations are available in this dataset?
# Filter to only 2015: # hint: liquor.Date.dt._______
# Concatenate series # Index has to be ignored because the series 's' does not have the index column
# Save the results to a variable.
#open and read the file into a pandas data frame
# Plotting
#show actual locations
# Where are the NaNs?
#12. Merge two dataframes trasnactions and users showing data for all records of transaction and showing corresponding data for userIDs
# general information about the file
#get the info of the dataset
# We want to get plain text only, so we need to remove all the non-visible charaters  # such as `\xa0` and `\n` and replace them by a space. # The '\n' (return) is easy as it's a specific case
#and then read in this document to analyze # df = pd.read_excel('SHARE_cleaned_lists.xlsx', index=False)
### Step 20: Use the day of week attribure of the datetime64 object 
#Creating a new dataframe only for the treatment group
# Can use any valid frequency for pd.date_range, such as 'D' or 'M'
# how many records will we drop?
# Dropping column. Use axis = 1 to indicate columns and inplace = True to 'commit' the transaction
# READ DATA
# extract the price data only # extract the column headings and convert to a row:
# Export to csv
# Create index
#convert to integer 
# Get the unique geo_codes.
#fitting a logistic regression model for the same
# list all vendors with receipts
# Aggregate for a DataFrame. #
# Fit Your Linear Model  # Fit the model
#check that dataframe has been created and data has been imported
# Query for most recent date in Measures # Get string value of recent date from tuple return by query # Get string value of 1 year prior to recent date
#Corpus Christi': 'a3d770a00f15bcb1'
# Looking at the odometer column
# remove unwanted fields
#Stories that in Code Review/Testing or Approval in reln need to be flagged
# We display the total salary each employee received in all the years they worked for the company
# Check how many crimes per type
# add game number to expand table
# try without extra squre bracket
# Twitter credentials
# Cleaning up columns so it's easier to reference them.
#Using mean since data is in 1 and 0.
#investigate the distributrion of year of registration again
# convert crfa_r to numeric value
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# frequency count
#  The mean and sigma arguments are the gross descriptors of the GM(2). #  But note here that the sign of the mean here is REVERSED.
# Boolean variable that returns rows of outliers below the Lower Fence # Array of indices of the rows that are outliers
#this is looking cleaner now!
# saved each to its own date and then loaded/brought them together to one df
'''based on what's seen in this column, there are about 862 null values for 'Footnote' but the rest are '*' values $ so it would probably be best to ignore this column for our analysis '''
# I am giving our data a name and instantiating a SQL table
# Read the filtered tweets from the .txt files
# Use the pivot function to make column values into columns
# Let's see how data distributed along with year
# how significant our z-score is # what our critical value at 95% confidence is
# To count how many objects have 'timestamp'
# create odds columns in team df
# Only fetchs historical data from a desired symbol # When we are not dealing with equity, we must use the generic method # or qb.History[QuoteBar]("EURUSD", timedelta(30), Resolution.Daily)
# Apply the selector to the DOM tree.
# Change our units to 000s for funsies
# training loss
#Method 2: Using Dictionary
# We create time series for data:
##712 is the area code for Dunlap, the proper zip code is 51529.
#Vista del dataframe
# use average accuracy as an estimate of out-of-sample accuracy
# Merge two dataframes, the common column is selected aotumatically
# Create the 'DateTime' column in the price2017 DataFrame
#Create a list of metadata rows to skip; rows 1-29 and 31  #Append '30' to the list
# Shows row at index 2893 (duplicate user)
# Count the number of unique values in column <landing_page>
### Create the necessary dummy variables
# Calculate a set of basic statistics.
# type(transaction_dates['Created at'][0])
#Convert list of past tweets to text for modeling.
# useful: .between
# To check missing value and any rows in csv file 
# Reading from CSV file. #   breaches.csv is all of the breaches in the haveibeenpwnded.com database #   This was retrieved from their API, and turned into a CSV (using Pandas of course :) )
# # a list of dictionaries containing metadata for cells with reconstructions # a list of cell metadata for cells with reconstructions, download if necessary # cells = ctc.get_cells(require_reconstruction=True)
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
#inspect measurement table
#  Large "repeat" values obviously increases computing time... #  Repeatly generate daily prices for one-year, record gmr each time.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Proportion of users who have converted
# we can also create a regular sequency of periods
# df[df.columns[1:] ] # First Section Incidents data
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# calculating number of commits # calculating number of authors # printing out the results
# check if repeat row drop or not
# alternative: .sample and .difference
#unique user_ids 
### Create the necessary dummy variables
#setting up the intercept and dummy variables
# Save cleaned and encrypted dataset back to csv without indexes
# We create time series for data:
# simple pandas plot
## quarterly means
# Reduce df_api to the necessary columns
#create a new label call HL_PCt which is percent volatility and add feture
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# media de tmin anual
# Just Use DataFrame to have a clear view of the dataset
#Peaks are between 10pm and 1am
# the 'ensemble', 'havana', and 'ensemble_havana' annotation sources describe sub-gene elements. # use logical indexing to filter the dataframe for sub-gene element annotations
# Calculate the date 1 year ago from today # one year ago is 2016-08-24
# df_search_cate_dummies.head()
# Checking the dataframe for the changes made
# Reading the twitter_archive that's provided
# Retrieve page with the requests module
#First we need to extract the two columns we want
#load data  
# First, we want to extract all the words in the text. # The regexp_tokenize line will convert everything to lower cap, keep only words (i.e. drop numbers and # ponctuation) and split everything in tokens.
#shape of old control group
# Convert df to html
# pandas DataFrame
# get 8-16 week forecast existing patients # keep only date in index, drop time
# Joining the country dataframe and the page dataframe on user_id
# Convert topics to a DataFrame for further analysis
#double check ab_page
# complete rework of option (extract real options if possible)
### Now dropping test column
#use unique() function to find the number of unique users
# a series
# The difference in the real data # Convert p_diffs to numpy array # Proportion of p_diffs greater than real data
# predictions = grid.predict(X) # print(classification_report(y, predictions))
# PASS CONDITIONAL GROUPBY??
# Summation - NaNs are zero. # If everything is NaN - the result is NaN as well. # pandas' cumsum and cumprod ignore NaNs but preserve them in the resulting arrays.
#Jacksonville': 'd5dbaf62e7106dc4'
# URL of page to be scraped
#from rules, perishable weighted as 1.25
#np.apply_along_axis(datetime.datetime.combine(), 0, weather_dt, )
# Check if we are missing any classifier summaries
#Saving that into a new csv
# rename Tip_Amt to reflect new value: TipPC
# plot the closing price of GOOG
# Find the div that will allow you to retrieve the news paragraph # Use this html, do not go back to the website to look for it # Latest news paragraph
# check to see if the countries dataframe has the same amount of rows as the df2 dataframe
# What are the most active stations? # List the stations and the counts in descending order.
# calculating difference in convert rate for the new page and old page  # under the null hypothesis
# these are held in memory so it is intensive when they are large # numexpr allows this without intermediate arrays
# Headless Chrome #options.add_argument('headless') #options.add_argument('window-size=1920x1080')
# Merge the two datasets # The lengths of all 3 datasets should be equal
# before I split I want to understand my distributions of my predictors
# get 8-16 week forecast existing patients # keep only date in index, drop time
# Provides useful statistcal data on the data set
# Looks like install rate exploded
# Checking Correlations # Cost and Sessions highly correlated r = .89 # Margin & Bookings approaching multicolinearity colinear r = .92
#table.to_csv("filenamehere.csv") # Write table to CSV
# Take a peak of the data
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# pold == pnew
#removes the 'b' infront of the tweets that indicate byte format
#Exponentiate the coefficient of each variable
# dropping columns '[', ']', and ','
#Compute a simple cross-tabulation of two columns 
# Display of first 10 elements from dataframe:
### Fit Your Linear Model And Obtain the Results
#wav_version = pydub.AudioSegment.from_file("tests/test_data/acoustic/wave0.wav", "wav") #wav_version.frame_rate #wav_version.channels
# now, we can map the numeric values v in a sentence with the k,v in the dict # train_x contains the list of training queries; train_x[0] is the first query # this is the first query
#Converting timestamp to a true datetime64 format
#Plot using Pandas
# collaborations stats # for each author, I want to keep track of who commits their diffs # who else works on the same files as them
# Checking the columns of our submission sample
# Read the dataset
#'Portland': 'ac88a4f17a51c7fc'
# Investment types
#remove columsn with identifing info
## there seem to be a few shows that started prior to 1980
#identifying unqique instances where new_page does not line up with treatment
#Size of rc_2018_02.csv is 20 GB
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD). # https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-05-30&end_date=2018-05-30&api_key=sKXGJG7ybc76fKLMSfxc
# dummy variables for the country column
# To make the copy of all dataframe
# we now plot on the KaplanMeierFitter itself to plot both the KM estimate and its confidence intervals:
# Now apply the transformations to the data:
# histogram in matplotlib
# What is the probability that an individual received the old page?
# compute p-value
# reference: http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.html
# each 4h of 01.Jan, 2000
# Calculating the difference in mean of the probabilities (p_new and p_old) for the sampling distribution # (for the alternative hypothesis i.e not for H_0)
# Number of times new_page and treatment don't line up: 3893 # This was found by querying the dataframe for all rows where we found combinations of new_page & control or old_page & treatment
# look to makes sure intercept, control, and treatment columns created
# Conversion rate independent of page: 0.1196
#df.loc[20]
# rename treatment to ab_page and drop control column
# drop original label columns
# for l in range(0,20):
## drop row with invalid data
# Calculating probability of an individual convertion rate regardless of the page
# .index.values,'created_at']='2010-11-11 03:59:09'
# Inspect dataframe
# end_time :: end time of 2016
# Shuffle the training and validation data
# a useful variant: dropna
# the target 
# 2012 created
# Clean all strings
#p_old = df2.converted.mean()
# filter North American mammals whose genus is "Antilocapra" # query, loop over and print out animals.
# Show Topics
#And look at summary statistics
#Create dict1 from the TUPLEKEY and VALUE
# Fit model (will take some time)
# number of users with old page
# We can compute totals amoung groups
#Row wise concatenation
# There are 290584 unique users identified by a user_id.
#tweets.shape
# Let's find the highest weighted words
# create dataframe of matrix  and then inputting the confusion matrix
# we use shape function to see number of rows [first element]
# results are returned as an iterable list
# Joined df_countries and df2_dummy on index
# Drop duplicated user # Check the drop worked
# Keys become column headers, indexes becomes row labels
#== Using isin() method 
# Gives no. of rows, columns as a tuple
#Investigate the user nodes, for any user that is suspicious
# me aseguro de haber tomado todos los datos correctos
# (b) 
#converted rate for p_old
# Define model and loss
# We can see that the values in this field are rounded, which might indicate that sellers had to choose from pre-set  # options for this field. Additionally, there are more high mileage than low mileage vehicles.
#bins='log', cmap='inferno'
#accounts for paid subscribers, ie those who did not churn immediately following free month
#display the picture of the dog with the most rating
cur.execute("""select text, geo, coordinates, location $             from tweet_dump where location != ''$             order by id DESC limit 5;""")$
# movie meta-data
#Group by the status values, and then describe..
# Polarity
# Once the column has been created, I want to sample parts of the database to ensure integrity in the matching.
# filter out all all dates before 8/1/2016
# convert date from string to datetime
# Apply the NoData value  # Convert raw reflectance into type float (was integer, see above)
#Tweepy Cursor handles pagination .. 
# empty 'autor' means the company blog
# Evaluate model
# remove the holidays where the days have been transfered to only leave the days where they were actually celebrated.
#plt.savefig("date_Fre.png")
# Save the query results as a Pandas DataFrame and set the index to the date column  
#Simulate distribution under the null hypothesis
# I"m an economical wine buyer. Which wine in is the "best bargain", e.g., which wine has the highest points-to-price ratio in the dataset?
# below is the average number of changes in odds for each category: moneyline, runline, total
# Okay, let's try a second example.
# Random forest score:
# cuantil
# Plot graph of offerType # Total count of unique values
#vemos cuantas filas tienen place_country_code en Nulo y contienen en user_location la palabra argentina
#time_rt.plot(figsize = (16,4), color = 'b')
#Contains key search term
#Check via customer sample
# Get a list of column names and types # columns
#trans = trans.set_index("SurveyResultId")
# Applying the function to the bill text
# AUROC Score
# trading_days = fk.get_trading_days(start=start, end=end) # trading_days = list(set(fk.get_trading_days(start=start, end=end)).union(set(fk.get_report_days(start=start, end=end))))
# Write to CSV
# For Displaying Data
## To determine number of times the new_page and treatment don't line up.
# todos os concursos de 2017 # soma dos valores da sena distribuidos no ano de 2017
#The amount of unique user id's is investigated
#Create a series with 100 random numbers
# Heatmap
# calculating or setting the year with the most commits to Linux
#Convert rate under the null is to be calculated as same as the average conversion rate for the page.
#Create two user-item matrices, one for training and another for testing
# set new column names
# I only want to consier for 1980 to 2017
# Converting my list to a dataframe.
# coordinates to a 2 dimension array # check dimensions
#importing the pandas library for data analysis #downloading the data from the url
# analyze validtation between BallBerry simulation and observation data.
# We also load avro-transitions.csv
# how significant our z-score is
# calculating number of commits # calculating number of authors # printing out the results
# remove observations which have price less than $500  # and greater than $350,000 
#checking np.arange array for use on next cell
# Cleaning up our column names using string functions:
# ML modules
# Show a return report # Ability to added custom groupps
### Create the necessary dummy variables
# Correlation Matrix
# First and foremost - make copies
# Get laste record entry date for temp
#============================================================================== #==============================================================================
# Show available caslibs
# total number of columns with missing values
# Setup Tweepy API Authentication #, parser=tweepy.parsers.JSONParser()
#Need to rename column for processing here because it has a space #This generates a series
#reorganize data set
# Default action is to append the data
# If you want to drop the redundant column:
### Create the necessary dummy variables
#Query to calculate the total number of stations
#create a dummy variable column for which page each user received
# Extracting user 1 for testing
# First we load the page
##plt.axvline(obs_diff, color='r');
# plot the data
##### unless specified otherwise, head and tail are first and last 5 rows #### print(df.tail)
# Let's see how data distributed along with day of week sum
# Perform a query to retrieve the data and precipitation scores
# check Forcing list data in file manager file
#Vemos cuantas filas tienen datos en place_country
# concat 2 series
#reading the csv file into a dataframe
# NaN operations will result in NaN
##Cleaning the data
# create a fitted model with three features # print the coefficients
# checking if anything interesting comes out after looking at 1-app districts
cur.execute("""SELECT table_name FROM information_schema.tables$        WHERE table_schema = 'public'""")$
# shape of coarse grid
# find historical data for 2017
# show ohlc resampling
#Saving Plot
### Create the necessary dummy variables
#Find out convert rate P new
#Import file 'Sample_Superstore_Sales.xlsx' which is in FourFiles.zip
# check a single (or multiple) posts
#to get vectographic representation
# if you want more fields brought through to the final final just add more lines here
# drop unused columns
# show available waterbodies
#before cleaning
# create the dummies for each country
### Fit Your Linear Model And Obtain the Results
##Averages of basic stats for each store
# Armamos un dataframe con los tweets recortados #we will structure the tweets data into a pandas DataFrame
#https://stackoverflow.com/questions/19851005/rename-pandas-dataframe-index
#selecting a subset of the rows so it fits in slide.
#display image data
# Use Pandas to calculate the summary statistics for the precipitation data
# Get training run status.
#Check size of the df
# 'Shreveport': '4ec71fc3f2579572'
#B2.add_files_from_workstep(default_workspace.get_step_0_object())
# Delete it from the original series
# resetting index to eliminate user id's
#Sample submission
# let's visualize our boxplot
# DISPLAYED THE UPDATED DATAFRAME WITH NEW COLUMN :
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
# Print out single tweet
# Nonsmoker descriptive statistics
#df_times['landing_page'].unique()
#show behaviour during sepcific time window #pdf.loc['2016-1-1':'2016-3-31',top_allocs[:10].columns].plot()
# Print the tail of df
#null_vals = np.random.normal(0, diff.std(), diff.size) #Actual difference observed in population
# your code here
# What are the minimum and maximum prices for each `variety` of wine?  # Create a `DataFrame` whose index is the `variety` category from the dataset and  # whose values are the `min` and `max` values thereof.
#Set some global options
# look at the count of values for the 3 categories.
# total days
#read in the group file
#torch.manual_seed(args.seed) #if args.cuda: #    torch.cuda.manual_seed(args.seed)
#no missing values are seen
# examine the huge bar on top, consisting customers who made their the first and last purchase within 80 days
# get feature matrix and dependent variable column from the DataFrame
# Drop rows containing outliers from the DataFrame
# Reading the autos.csv CSV file into pandas # and assigning it to the variable name autos. 
# created_index = pd.DatetimeIndex(housing['Created Date']) # closed_index = pd.DatetimeIndex(housing['Closed Date'])
# Read the data set
#Example2: Passing the sheetname method 
# add match time ROT column
# Use the append() function to join together free_sub and ed_level.  # Are the results the same as in part (4)?  # If not, how could you reproduce the result append() by using concat() or merge()?
# read in the file 
# visualize the relationship between the features and the response using scatterplots
# Now we'll try "device_id" - about 700k values
# Drop duplicate row
##Just checking out the 425 dollar bottle, it looks ok
#count of converted users in control group
## So only retrive the tweet text, id, user info (id and screen name), and hashtags. Not sure if we're going to use them ## all but ... # Expand the cursor and construct the DataFrame
# .to_csv('Hasil5.csv')
# load logistic model
# confirming work
# Find Correlations
# List all the denominators
#see if wards are in same bounds as delay points
#Show first row
# the connection/session
# Display unique values of names and extract invalid names
#Delete that row of course
# create merged data frame # convert to datetime
#avereage hourly rate
# By default the max() method will calculate the maximum value in each column. #
# Use Inspector to print the column names and types for station
# 1. Remove the reply tweets and drop in_reply_to_status_id and in_reply_to_user_id column # only keep the tweets that in_reply_to_user_id is NA then drop the two columns
#This cleans up the one big dataframe that we now have and outputs the top 15 places 
# let's create the dataset of beers drank per month and sort it
# The reintroduced data is raw, so transforming the data with the instatiated # Count Vecotirzer will be needed.
# Let's try this out with "device model" first - only about 7k values
#The dtypes attribute reveals the data type for each column in our DataFrame.
# by open pr count
# apple_data.info
# take logarithm of goals
## Showing all the indexed tables
# read in dataframe
# merge weather and 311 #df5 = pd.merge(df4,df2,how='left',left_on='Date Closed',right_on='date',suffixes=('_created','_closed'))
#Instantiate a Randomorest regressor: rfreg
#%% Do Blink Analysis
# 34 ST-PENN STA -> 34 ST-HERALD SQ @ 11:30 am = 10 min # Print total crepe sales lost due to transit
# setting index to date
# Slice with names using loc
# filters the columns with labels of array of 0s and 1s
##### index, series pairs
# Transforming dataset values # vetor de frequencia de palavras = repos freq. # Calcula matriz esparsa de similaridades User_LangsxUser_Langs
# Make spp folder
# show last 5 rows of data frame
# Delete the first column.
# and average temperature most active station
# unique user ids count is
# read csv into data frame
# for our single-sides test, assumed at 95% confidence level: # Here, we take the 95% values as specified in PartII.1
#calculation of actual differences
#df2
# 2. Sort free_data by country, educ, and then by age in decending order, modifying the original Data Frame.
# Filter high-value customers by taking average of total reacharge amount (voice) and total recharge amount (data) for June and July # Calculate 70 percentile
# Now that we have performed some test, lets go back to our intital dataframe which is df
#Complete a groupby to determine the average reading score of each school by grade level.
# To ensure images in this notebook are displayed # properly, please execute this block of code.
# Most of dogs' names are missing # There are some mispelled names in the name column, like 'a', 'the', 'an'
# Create BeautifulSoup object; parse with 'html.parser'
# unhide the strings in jupyter notebook # assign all new functions to df # new_df.head(1)
# first let's compute the observed difference (from actual dataset) of conversion rates \ # between the treatment and control group
# Need to run this first, apparently: # http://forums.fast.ai/t/understanding-code-error-expected-more-than-1-value-per-channel-when-training/9257/10
# It appears that this stop comes up twice in oz's file. However, stop 7270 is not a real stop when looked up online.
# Create the 'type' column
# Initialise with a Numpy array. #
#Then I resampled by the hour, taking the mean duration and distance traveled for every hour,  #as well as the sum of all the trips and members.
##### ignore
# sice this took forever, better save the model to disk
#https://stackoverflow.com/questions/546321/how-do-i-calculate-the-date-six-months-from-the-current-date-using-the-datetime
# Import csv file from Oz
# scrs < 3m / all scrs, since 3-01-2018 # 50% scrs start their scns too early
# used later to find coefPaths # used later to find the original location of the path from non one hot
# Read the last 25 Million records from training data
# we can make use of groupby function to group the diff types of groups and find mean of each group's converted column to find the  #probability of each type of group 
# Probability of an individual converting regardless of the page they receive
#Select first row from mesurements table
#Again a simple sql example:
#### Results for 09-05-2016
# record observation count for each sensor
# number of users who got the new page (i.e. group = treatment)
#Reference columns by new names
# merging shifted data set back into our origional  # data set to create the lagged data set
# Go to this link: https://www.quandl.com/data/BCHARTS/ITBITSGD-Bitcoin-Markets-itbitSGD # follow the instructions on quandl's python page, store the results in the coin_data variable
# return the Missoula column using a property
# check the area of each hru to calculate areal average ET # read the area of each hru
#Creating more sensible and consumable features => time since promotion
# Read All 5M data points # Extract the bodies from this dataframe
# These are taken from arxiv paper JH.
# Add a column level for our new measure # Concat it with our original data
# checkout cand_id and see if there are duplicates, i.e. if there are differences in how cand_name is formatted
# create a business day offset
#apply more
#now its time for forcasting the data
#Merge 2 dataframes above into X
# check shape of empty grid
#Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
# drop duplicate row # To confirm if duplicated have been removed
# export
#For each User fetch the pair of pair trnsactionIDs
#The number of unique users in the dataset
# NOOOO OOOLD
#pernan is a filtration level - scenes with more nans than this per scene are removed
# set the simulation start and finish times
# Plot causes of death by year
# the number of people that received the new page
# accediendo a varias columnas por el nombre (label)
# conbine two tables # reset index
#print lxml.html.tostring(item)
#.dropna(subset=['place_name_long'])
# Preprocessing the Date (Created at)
#handling case where we only have one file for month 10 in Guinea
# Match Email pattern: 
# create overall df to set up for mean scores (bar chart)
#exponentiate variables attached to explanatory variable
# n_old transactions conversion sample
## Creating a Spark Dataframe from the Pandas Dataframe in order to use it with the Decision Tree model.
# Dicts can contain other dicts, like in the case of the plan:
# set the index of all_cards to be the name column, so we can search cards by name
# Display value counts
# block_geoids_2010 = [row[0] for row in query_psql("SELECT geoid2010 FROM sf1_2010_block_p001 order by blockidx2010")]
# import my method from the source code, # which drops rows with 0 in them
#confirming if the dummies inherit correct values by comparing dummy column values to country column
# get just the cgm data # look at the first 5 rows of data
# figure points
# Filter Twitter Streams to capture data by the keywords:
# number of unique users
# Manipulation methods: #Apply
# Make a new empty pandas data frame
# Create DataFrame
#Dot produt to get weights #The user profile
# Pandas series are mutable, i.e. we can change the elements of the series after it's been created
#checking when control team lands incorrectly on the new page
# pitcher_throws # stand (batter_bats)
# keep rows where a min number of non-null values are
#'Rochester': '2409d5aabed47f79'
# Generate data
#Find the duplicate id 
#Check graph styles available
# unique country values
### create a new column `day` which contains only the day from timestamp
# Match project id with project name
# Outlier data
#convert mileage to a series
# z-score is 1.31, p-value is 0.905
#after
#  toar() is for conversion to numpy array:
#Get the keys of the max and min values
#l4exc_l4exc
# plotting each day of week for the data using a different colour
#Use a lambda function to assign values to water_year2 based on datetime values
# Read gzipped file in a dataframe # compression="infer" tells pandas to look at file extension, which is "gz"
# answer
# slice the temp differences column for the rows at # location 1 through 4 (as though it is an array)
#try using dendrogram by parsing texts into words
#Colorado Springs': 'adc95f2911133646
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Check the new data frame
# adding prefix MONTH_ to column names
# deleting the group column from the dataframe # dislplaying first two rows of the updated dataframe
#now create another label for forcast data that we need
# Summary Statistics for Station - USC00519397 WAIKIKI 717.2, HI US
# read dataset
# We create a column with the result of the analysis:
# Initialize the dataframe to hold all the data # Initialize an empty list
# df = database dump # dfU = urls scraped
# already failing 
# it is a string
# From internal encodings
#disable warnings
# calculate mean of p_diffs
# set the target site as a variable
# from h2o.estimators.gbm import H2OGradientBoostingEstimator # hf = h2o.H2OFrame(flight_pd)
# note that upper-case version does NOT work
# Neat way to rename columns and order appropriately
# Apparent Magnitude limit # must be a float number # Band: i
# Initialization
# lr.fit(features_normalized, paid_status_transf)
# Return types... # What type is returned from loc and iloc - check here...
#Convert TransactionDate to datetime field
#country_size = ac.groupby(["Country"]).size()
### compute the observed difference of conversion probabilities
#### Total amount for locations outside USA:  30710.63
# Prepare for Submission
# isolate city boundaries for Leiden
# Apply smoothing function to each line, identifying transactions between Coinbase and GDAX
# Create a Spark DataFrame from Pandas
#Show the row with the index matching Jan 1st, 1975
# Instantiate an instance of the classifier we defined above. # This method, 'get_classifier', takes arguments 'self' and 'key' # It then returns 'self.config.classifiers[key]'
#this gives the columns to be dropped with mean etc close to  -- nrOfPictures
#=== By Label: row selection by 'data index'
# We can look at the page title. That might be an easier way to extract the news title
# can only fit model once
#create a matrix where each row is a round and each column is an investor from the cleaned up investor list
#create an intercept and a dummy variable column for which page each user received
# car break-in  #car=load_data('https://data.sfgov.org/resource/cuks-n6tp') #car.head(5)
# confirm that this is a new array
# Extract specific values - 1
# All numeric data is now int64
# remove unessecary columns
# Summary statistics
### Fit Your Linear Model And Obtain the Results
# Convert into a pandas dataframe & rename column
# all the estimateds_count in the file is >= 3.
# create the odds column, this will be needed to import into r
# check Basin Parameter info data in file manager file
# Inspect the dataframe and Pandas automatic data type detection
# distribution plot of temp_celcius
#Group by product then sum, then sort, then output top 5 results in a single box. #Sorts the table by Profit. 'Ascending=False' puts the highest value at the top. #Since 'pro_result' is a pre-sorted table, we can use just .head() to display the top 5 values.
# To create a column of longest_activity_time for each device # based on the first event time and latest event time
# Proportion of simulated differences greater than actual difference
# Importing a joining a new variable (country) to add to the fit
#CHURNED SLICE, churned dates
# 6. What was the average daily trading volume during this year?
# analysis.iloc[0]['project_url']
# differences computed in from p_new and p_old
#fetching the first 5 entry 
# could call out to OS and run this but sometimes I work across systems and it is easier to just cut and paste command line #  exiftool -csv="FILE GENERATED ABOVE" /path/from/root/to/your/images
# upper line name should be 'Quizno'
# Prepare Results Dataframe to output to CSV file and for the scatter plot ######media_user_results_df = pd.DataFrame.from_dict(results_list)
# How many stations are available in this dataset?. Calculating # of stations in the full measurement table.
#save cleaned data out to csv
# Create the necessary dummy variables
#sub setting time series # Between June 1951 to Jan 1952
# Calculate the date 1 year ago from today
# 6.What was the average daily trading volume during this year?
# Make the graphs a bit prettier, and bigger
# print out details of each variable
# reflect an existing database into a new model # reflect the tables, means start mappig from engine
# Only fetchs historical data from a desired symbol # or qb.History("SPY", 360, Resolution.Daily)
# tokenize reviewText
# Split the label column out from the features dataframe # Sample the indexed DataFrame
#print(highlight(json.dumps(jevent_scores, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter()))  # remove the comment from the beginning of the line above to see the full results # Convert to a data frame and show all flagged life events for this client
#info #181 observations are retweets 
#Click here and press Shift+Enter
# Design a query to calculate the total number of stations.
# calculating number of commits # calculating number of authors # printing out the results
#The below gets agency with highest number of job postings, but doesn't account for postings with # Of Positions > 1; we should include these in the total count
#correlation of continuous variables with the dependent variable
#change times from object to pandas datetime
#Estoy levantando 100 posts
"""$ Number of unique timepoints$ """$
# converting to date to pd date time
# Find stops which are in in stops but not in oz_stops
# Show us only the data where 'Date-Fri' is 1
# descending order
# 'startdate': '2017-10-05+19:19:00', # 'enddate': '2017-10-27+13:17:26',
# Finally, export this file 
# Put the data into HDFS - adjust path or filename as needed
# gives sums of the columns
#Mars hemispheres # url of page to be scraped
### both brands receive more postings from females
# scetter plot, s = size of each point, c = color of each point # It can be observed that highest mean_duration per trip, total_duration  # and no. of trips correspond to different start station IDs
# treatment group converting mean
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# This is the tradeoff to decide morning and afternoon
# Inspect compiled dictionary 
# Distribution of daily changes 
# Compute cross sections on a nuclide-by-nuclide basis
# read in dataframe
#Get the unique user_id in df2
## To determine the number of unique users
# Use the Base class to reflect the database tables
#Create a list of zeroes, to initially set the "is_service_focused" column
#save dataframe
# Squre Root
# Create average purchase per user feature
# Csv file was uploaded to amazon aws rds mysql server as a database table # Here, we use the connection string and user details to connect to the database # In the following lines, we will look at the normalization operations done on the table. 
# choose either 0 or 1 with a probability of p_new, a total of n_new times
#ordered and paired after genesisco
#Aurora
# Import data # Check the data
#dfg = df_n.groupby('drg3').agg({'2014':[np.size, np.mean]})
# use the best set of parameters
#Joined(train,test)-googletrend for germany (checking if there was any unmatched value in right_t) ##Note: Level of googletrend table for germany is "Year-Week"
#You should also convert month from object to datatime!
# Check the first three columns we are interested in.
# (i.e., if the memory is contiguous)
# Return the list of movies with the lowest score:
# map data to the twitter archive
# Find top-10 tweeters:
#adding intercept column #creating dummy variable #dropping one dummy variable so that the matrix should be full rank.
# Print all classes mapped to the Base
# checking that there is indeed one duplicate
# check dataset
# Information about the independent factor x.
#Convert rate new under the null
# Check out the structure of the resulting DataFrame
# 2011
# This here is what we want!
# Can be constructed from a 2d array of data.
#Takes like 60 seconds
# reuse the created period range as index:
# shift using a DateOffset
# save the results so far
#Add the intercept #Fill in the dummy variables for control and treatment
# Find the station has the highest number of observations
# Save to file
# Resource: https://stackoverflow.com/questions/33899369/ranking-order-per-group-in-pandas # Create 'Inspection_number' columns, populate with groupby function on brkey field and rank ascending # Convert Inspection number field to integer
# for rows, the same rules apply: integer indexing returns # a series, slice or list indexing returns a dataframe
# Most favorited
# Create a date range # Syntax: pd.date_range(start, stop, freq=)
# read matching list
#Group male data by moon phase
# aprovechamos y salvamos este nuevo set # y recargamos la informacion nuevamente
# Requirement #2: Add your code here
# How many usrs in df_log
# put all treatment AND new_page into one dataframe
# Access a row
# Number of unique users in the dataset #unique_users = df.groupby('user_id').count()
# percentage of ratings above 4.0
# info helps to get an overview as well
# OVERALL SENTIMENT ANALYSIS BY NEWS SOURCE # Create dataframe that calculate the overall compounded score to place on the y axis
# Number of Unique Users
# games without events
# Here it should have merged 2001 to 2001 index and so on because all the other valeus are the same but it did not.
# merge dataframes
#============================================================================== #==============================================================================
# We can append a column to our existing index
# Solve for Tue
#Takes a very long time
# does it return True or False from re.match
#Calculate the proportion of users converted
# Before this test began, we would have picked a significance level. Let's just say it's 95%. According to the Hypothesis setting #, it's a one-tail test so a z-score past 1.64 will be significant. (if two-tail, then -1.96 to 1.96)
#check for null values by column
#exponentiate US and CA
# Calculate the actucl difference observed in ab_data
#group ride data by city # average fare per city # number of rides per city
# Histogram of time to close a ticket
#Joined(train,test)-googletrend for states(checking if there was any unmatched value in right_t) ##Note: Level of googletrend table for states is "State-Year-Week"
### Fit Your Linear Model And Obtain the Results
# Create BeautifulSoup object; parse with 'html.parser'
#select the call option with the fifth expiry date
# loading in test data as well as the Sample Submission file
# write to xlsx
#Rural
#faamg
# HACKER EXTRA - experiment with additional plots that help us understand this dataset
# number of rows in dataset
# print out details of each variable
#Looks like the second file has less columns. Let's take a look at both files and see what we can drop
# We create a pandas dataframe as follows:# We cr  # We display the first 10 elements of the dataframe:
## we see that the class ="ref-module-paid-bribe" contains information of all the reports in a page
# We remove the store 3 row # we display the modified DataFrame
# Create BeautifulSoup object; parse with 'html.parser'
#find distance between element
# use iloc to fetch that specific row from the dataframe
#### print a few wells to double check
### Create the necessary dummy variables
#Proportions of browser usage within OS
# we get 726 rows for our 2 years of data
#printing first few columns of the cleaned dataframe
#forest_clf = RandomForestClassifier()
# we need the critical value for our z-score at a 95% confidence # we use scipy stats.norm for that
# Pledge and backers # Select the product and average of backers and pledged # Average of th number of backers and amount pledged
# a number (float)
# The wikipedia URL that every article has in common
#gente con mejores estudios busca mas alto nivel
# Find n old
# What are the most active stations? # List the stations and the counts in descending order.
# Dropping all unaligned treatment and page rows and putting them in a new dataframe called df2
# read parquet file with arrow
# Create new column to fill in with the following scrape...
# do some sessions show up in one or the other? # looks like it is the case, and some sessions that don't have either (quite a few in fact...)
# Open specific netCDF file
# Create DataFrame copy grouped by Categories
### Create the necessary dummy variables
# please note - these lines throw a warning if they are executed twice
# import the model # instantiate the model # fit the model
### perform a two sample proportion hypothesis testing, where the alternative is p_new < p_old  ### result is used below in the Answer
# READ IT IN USING # df = pd.read_csv('citation-data-clean.csv')
# check the structure of a push event
# stkinf - Database # music - Collection
#Split Data ###YOUR CODE HERE###
# Oh that's not good. Let's see what the last available date is
# Create lists of unique values for each column, excluding missing values (NaN)
#info_final_gente = pd.merge(info_final, avisos_detalles, on = 'idaviso', how = 'left') #info_final_avisos = pd.merge(avisos_detalles, info_final, on = 'idaviso', how = 'left')
#pivot the Fremont DataFrame where the value total appears and the index.time in fremont plus the columns date and store it in a variable pivoted # plot the pivoted data havint the Total value of passings from East and West side and the Time at which they happen
# Looking for unrealistic values
#### Test ####
# Creating lambda function to map datetime transformation to datetime columns
# Remove the row with duplicate user_id # Checking that the duplicated row has been removed
#The test-set R^2 is defined as 1-SSE/SST, where SSE is the sum of squared errors of the model on the test set and SST is the sum of squared errors of the baseline model. For this model, the R^2 is then computed to be 1-5762082/7802354.
# changing the name of a series:
#movies['year']=movies['title'].str.extract('(\s+\(\d\d\d\d$)',expand=True)
# creates a temp file # save the model
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# Calculate the date 1 year ago from today
# Contains multiple levels of indexing and multiple labels per datapoint
# Calculate the date 1 year ago from today
# Replace '-unknown-' and 'OTHER' gender with 'UNKNOWN' to simplify data.
#unique user_ids in df2
# dependent variable #y = df.rating
# Set up the Auth Tweepy object. This takes your credentials from above # and does some authentication with Twitter
# Predict
#Updated the dataframe to take out the city
# df_2009
# plug in to df_tick
#Converting timestamp column to actual timestamp
#pprint(api_json['messages'])
#Code: #I decided to keep the index so that I could identify right away which row was removed.
# Design a query to retrieve the last 12 months 08-23-201
#count of users in control group
#pandas function to calculate unique users #OR #Display the number 
# used to check that we have converted the objects into datetime rows from above properly
#list(np.log(df2.btime*df2.Da*df2.dt))
# Notes:  #      - x-axis is the index. #      - Must find a better way to represent these figures.
# image does not display on github  # either download and run the notebook or navigate to the URL below to view the image
# range of potential gammas using logspace
#ind_result
# tick_labels = ax.xaxis.major.formatter.seq # ax.xaxis.major.formatter.seq = [label.split(':')[-1] if label else "" for label in tick_labels]
## Fitting using intercept, country(UK as baseline), and ab_page (old_page as baseline)
# merge the datasets to compare gene_count and chromosome length # convert a series to a dataframe # merge datasets
# Create a linearly spaced array of 5 values from 0 to 100
# drop duplicates across ALL columns
# result = {'customerID': avg_interval_between_reorders}
#BUMatrix.plot(kind='scatter',x='', y='event_Observation') # from pandas.plotting import scatter_matrix # scatter_matrix(BUMatrix)
#Storing the movie information into a pandas dataframe #Storing the user information into a pandas dataframe #Head is a function that gets the first N rows of a dataframe. N's default is 5.
# resample to 1 min intervals, then back to 1 sec
# Creating dummy variables
# examine the score and parameters of the best model
# Calculate the sum of sales for each store in 2015 by grouping the full year data # hint: what columns do you need? what is your aggregating function? 
# find the min of the IMDB column
# Convert Y from 0/1/2 to one-hot format
#results summary was not working, found solution here https://github.com/statsmodels/statsmodels/issues/3931
#Replacing the NaN values with unknwown #Reseting the index #Showing the result
#file = open(today + '-' + hour_minute + ' ' + site_name + ' 1 title_page.txt','w')
#calculate magnitude at which baseline variable has effect over other categorical variables
# We can look at the available keys:
#read real-time data output, change column names, and convert to GeoDataFrame for spatial analysis
# drop unwanted columns
# get 8-16 week forecast new patients # keep only date in index, drop time
# generate a Gaussian distribution
##  For the sake of this exercise, let's just get a subsample so spatial joins are faster
# Save the precipitation query results as a Pandas DataFrame and set the index to the date column
#'Montgomery': '7f061ded71fdc974',
# number of unique users in the dataset
# COnvert rate for pold
# show which specific days have the highest conversions (sort in descending order)
# group by News_Source # calculate mean on compound scores
# dropping row
#API setup
# Import the required libraries.
# Importing JSON files to DataFrames
# printing first five rows of the data frame #df.head()  # 5 is the default value. we can ask for any number as shown below
#write this dataframe into csv  #due to the twitter content is not determined, the data form needs extre cleaning steps
# How many stations are available in this dataset?
#Mars News #print(soup.find_all('a', class_='content_title'))
# Drop duplicated rows based on user id
#locate missing value record
#proportion of users converted
#Drop all other combination that are not asked for
# Check for streets that start a capital letter.
# Count the number of tweets in each time zone and get the first 10 #pretty crazy that Central America (where Costa Rica is) is in 10th place
# Retrieve data from Mars Weather # Create BeautifulSoup object; parse with 'html.parser'
#Variable Time is now decimal day time.
# exponentiate the ab_page, UK(c2), CA(c3)
# sessions DataFrame
#df = df.sort_values('date', ascending=True) #plt.plot(df['date'], df['count']) #plt.xticks(rotation='vertical')
# Count how many files in training folder
# drop ceil_15min from data
# find historical data for 2004
# Summarize the length of the ingredients string
#import the Green Taxi data for Sept.2015 # take a peak
# for our single-sides test, assumed at 95% confidence level, we calculate: 
# Load tips data set # Display several random rows
# Arrange Data set based on the value of Year in ascending order (Show first 5 Records)
# m.add_regressor('TempC')
#Review column names in dataframe
# Testing the linear model function to print a table of the summary of the analysis
##Drop missing values
#using pandas frunction from link below #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html #For each subject string in the Series, extract groups from the first match of regular expression pattern
# using a starting point and periods
#save the map as a webapp
# To explain the z-score, find the z-score for 95% confidence level # The critical value for 95% confidence level
#df.isnull()
# amazon_gp['total_amount_cents'] = amazon_gp.total_amount_cents / 100
# Create an index transformer that calculates similarity based on  # our space # Return the sorted list of cosine similarities to the first document
#Create Twitter basemap specifying map center, zoom level, and using the CartoDB Positron tiles
#Combine date and time columns to get date_time
# A:
# Merging multiple DFs with the same index by passing a list of names to .join
# Negative tuna <=> not active after D0 # tuna_neg_DF = tuna_neg.toDF(["cid","ssd","num_ssd","tsla","tuna"]) # tuna_neg_pd = tuna_neg_DF.toPandas()
# Use Pandas to calcualte the summary statistics for the precipitation data
# Choose the station with the highest number of temperature observations.
#Data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
#correct rating denominator less than 10
# Frist, make indexs aligements # get id list
# To calculate the differences value between 'create_timestamp' and 'timestamp', # then convert them to seconds format # To preview the new dataframe by first 5 rows
#Acquire and show data
# proportion of users converted
# tables=np.asarray(list(Base.metadata.tables.keys())).astype(object)
### 7a. # extract only the conversion_rate (conversions/user_visits) - this will be used as training and testing data
#p['xx_1'] = p["xx"].shift(1) 
#Check end of file
# Overweight
#groupby city
# extract data from tweets that match search terms 
# index and user_id of the duplicate 
#!rm ../../output/prealignment/raw/{srx}/{srr}/{srr}*bam*
# Sampling the Recommendations provide using the item:desc dictionary we had created earlier
# Let's get some data; top stories from lobste.rs; populate a DataFrame with the JSON
#One fewer column than training data
# Create a datetime variable from Create_Date this operation takes up to 3 min
#Double-click on the file that is saved at the filepath shown below -  #This will launch your interactive pyldavis chart in a browser window!!
# remove selected workspace #ekos.delete_workspace(user_id = user_id, unique_id = workspace_uuid, permanently=True)
# Changing frequency from 2W to 1 W, without a method, will get NaNs
# end = datetime.datetime(2013, 1, 27)
# How many stations are available in this dataset?
# rtemoving unused columns
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Prepare scoring payload using these random values
# convert to numpy array
# Get a list of column names and types for Stations Table # columns
# Show the position summary
# Also only pulling the ticker, date and adj. close columns for our tickers.
# I realize from watching walk-through videos that the first two lines could \ # be combined into one statement, but I am leaving my original un-refactored code
#User ID for non-unique user_id
#Downloading the file to a local folder
#payload = "elec,id=500 value=24 2018-03-05T19:31:00.000Z\n"
#we drop all of the columns except tripduration and date
#save df
#check for the single row of data from the measurement table
## Read file and then sort - was organization.csv #neworg.head(5)  
# We may want to use some colours etc from other libraries
# the given CSV file
# after dropping the duplicate data, I still have 290585.  :( It should be 290584. Please help
# Station Analysis # - Design a query to calculate the total number of stations.
# Filter outliers in hspf
#plt.ylim(200,400);
#probability of an individual converting regardless of the page 
# Index 0 is the treatment i.e. new, 1 is the control i.e. old # The sample size n-new is: 145310.
# Convert 'total_bill' to a numeric dtype # Convert 'tip' to a numeric dtype # Print the info of tips
#Histogram
# YOUR CODE HERE #raise NotImplementedError()
#find the number of null values in the dataset
# this yields 2097 observations for hour of the day 
#Create a pandas dataframe for JSON data #Save dataframe to csv for visual assessment
#read in ab_data.csv and store in df, taking a look at the first few rows
#Design a query to retrieve the last 12 months of precipitation data.
# Missing values statistics
# Finding the repeated user_id
# adding prefix VIP_ to column names
# answer to the exercise
# df2.loc[df2['user_id'].duplicated(), :]
#2
# the observed conversion rate of the new page # the observed conversion rate of the old page
# Likes vs retweets visualization:
# Finally, let's select midday and midnight complaints only
# create a counter to look at freqency of insert_id # filter list to only include ids that have a count greater than 1 # how many insert_ids are duplicated?
# An efficient way of storing data to disk is in binary format.
#one popular person in und... let's see this famous tweet #perhaps 'und' is just 'undefined'. The two heavily retweeted tweets are actually RTs themselves!
# We create a column with the result of the analysis:
# Ignore me for now
# 311 + HPD com + HPD pro
# use get_dummies for categorical variables, since the output columns are in alphabetical order
#find rows with landing page as new page #find total no of rows
# and distance.
#dfg.loc[('drg3', 'Red'), :] #dfg.loc[('drg3'), :5]
# Use Pandas to print the summary statistics for the precipitation data.
## Summarize 
#Merge dates of interest with the Landsat scenes to match dates
## drop duplicates as suggested by Joe
#Finding null values 
#Create a list for indices, and a list for total_ridership
#eVALUATES THE DATA SET WITH .INFO FUNCTION
# Move the 'Date' field back into the index
# open Chrome
#p_value
# create activity prediction matrix
#== Sorting axis by labels (axis=0:row, axis=1:column)
# use BIC to confirm best number of AR components # plot information criteria for different orders
# Tweets without Names
# instantiate the model with 'intercept' and 'ab_page' # fit the model
# Load the data from the query into a dataframe
#finds date of most recent and least recent tweet
# Add Distinct Users
#generate numerical category labels for job titles to be used during classification
# Let's say we'll generate some random numbers  # Here we got 72 random integers.  # we can use it to generate a pandas Series. 
#82% of the original datset is left to be analysed
# Add code here
# Load pre-calculated stacking fault energy data
#set threshold to drop NAs
# In order to answer question 2 we can again use a factorplot on 'lgID' using hue='divID'
#Atlanta
# determine number of converted users using mean(), since the column only contains 0 or 1 as values
# Find quarterly average
# Structure of the final DataFrame
# 86.53333333333333
#Code
    """Convert text to lowercase"""$     return text.lower()
# Reading data
# Instantiate a lgb classifier: lgb
# drop columns where reason for visit name = Follow up, THerapy, returning patient existing patient dataframee
# What is the median?
'''$ DO NOT CHANGE ANYTHING IN THIS CELL$ '''$
# shows time stability
# Testing tweeter api
#Limit data to those with more than 10 years data and still operating
# Test function
# df2 = merged_pd.drop('air_store_id',1) # df3 = df2.drop('day_of_week',1)
# There is missing data again, so we drop them as well:
# read the oracle 10k documents 
#tried using melt and didn't work. Value got doubled...so I had to get creative with strings. 
# Create final csv backup of cleaned and tidied data!
# we can quickly glimps through some of the problems in csv, faster than in jupyter # df_complete_a.query("name=='None' or name=='an' or name=='a' or name=='the' or name=='just'")['text'].to_csv('problem_names.csv')
# df.ix['600848']
#MEN | Converting | datetime
#by default, the row index is used for abscissa values, which comes in handy for  # time series data
# using numpy.random.choice(a, size=None, replace=True, p=None) we take output 
# How many stations are available in this dataset?
# Preview the Data in the Measurement table
#Set the DATE column to a date
#Set the names of the columns
#model = ols("happy ~ age + income + np.power(age, 2) + np.power(income, 2)", training).fit()
#Create predictions and evaluate
# Reading provided file from UDACITY
#Vacation Range is 8/1/17 - 8/7/17 # Create engine using the `hawaii.sqlite` database file
#Conversion rate of new page
# Calculate mean to get probability of a user converting
# Predict the values, using the test features for both vectorized data.
# get top 10 likers
# Creating reference to CSV file # Importing the CSV into a pandas DataFrame # Looking at the top of the df to get a feel for the data
# Check how many crimes per hour
# Setup Tweepy API Authentication
# Observation counts per station in descending order
# Normalize summary
#Now let's get the genres of every movie in our original dataframe #And drop the unnecessary information
# Gridsearch incorporates k-folds validation # You do not have to create training/testing splits
# place data in a DataFrame
# Show Figure
# now we have a time series reflecting the average number of births by date of thyear # lets plot
# Get our Mongo on
# create new dataset that haven't any uncertain data
## this funciton cleans runtime
# Calculate the difference
#show result
#len(apple.index[apple.index.duplicated()])
# merge dataframes
# Load Data Set # Display several random 'size' features
# Normalize reviewText
#get rid of some unnecessary columns for the purpose of our investigation
#loading csv files
# print(type(df)) # print(df.shape) # print(df.head())
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #6. Arrange values of a particular column in ascending order. #Sory by year and also in Ascending Order
# How many stations are available in this dataset?
#============================================================================== #==============================================================================
# repeated user
# drop columns related to retweets
#create and display cnn sentiments datafrmae
#Create the values for the first dictionary by zipping columns together and convet to list
# Pick a single tweet to analyze
# displaying results
#checking the difference between the two calculated values.
#Group by station and month-day with average percipitation of years available
#size of the old group
# get multiple sections with the term fees
# drop the null in is_enabled
# Find duplicated user
# get max probability column name, this is final prediction for validation set
#number of unique ids
# create from dictionary where keys are column names, values are Series
# Fill in missing values of 'Days_missed' with its class-conditional mean (graduated or not)
# dropping the 'split_location_tmp' column
#Retrieve the parent divs for all NASA articles #Print results
# Plot graph of monthOfRegistration
# Variable to store min of "mean_odometer_km" # Display row for brand with max
# displaying results of the logistic regression model
## To determine the proportion of users converted
#Calculate the date one year from today
#Upper and lower bollinger bands
#getting the number of new page landings. 
# proportion of users converted
#n = 145274
# read parquet file created with arrow with dask for compatibility check
#Check count of missing city value 
# load model if saved during a previous session
# remove URLs and twitter handles
#............................................................................ ##   Advanced Spark : Partitions #............................................................................
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #16. Determine which sessions occurred on the same day each user registered #Join User ID and Session Date with Registed date and locate the matching rows.
# import the data offset types # calculate one business day from 2014-8-31
#Add newly created Age feature to the list of features
# Use the forest's predict method on the test data
# convert the CohortPeriod as indices instead of OrderPeriod
#date_str = date.strftime("%Y/%m/%d %H:%M:%S")
# Import dataset
#reading and storing the csv file in a dataframe
# unique league names
#The single duplicate is shown
# Grouping by "grade" category and count the number of occurences 
# How many values are there?
# Include missing values in gender variable
#Visualizing new dataset
#First let's load meta graph and restore weights
# get value counts
# create a list gruped by type from the alreay created table to find the number of drivers 
# Create a categorical variable from a numeric and then compute dummies
#unique_urls = group_on_field_with_metrics('url') # add domain
# control group proportion of conversion
#add the previously created series to the main ward dataframe
# Save the query results as a Pandas DataFrame and set the index to the date column
# Fill NaNs with zeros
#Create two user-item matrices, one for training and another for testing
# remove colums with all na
#Anchorage
# Set crs to epsg:4326
# convert the boolean outproc_flag to numeric value Y/N == 1/0
# Average Order Value of _New_ Customers who made their first purchase after taking Discover # new_discover_sale_transaction.groupby(by=['Email', 'Created at'])['Total'].sum()
#proportion of the p_diffs are greater than the actual difference observed in ab_data.csv
# set list of columns/points on yield curve used
#checking the number of unique users
#Plots the data obtained from simulation
# This will apply the removal of punctuation and stopwords to a string.$ def text_stripper(string):$     '''Takes a string and removes punctuation, capitalisation and stopwords.'''$
# read in data
#filter these from 1900 to 29=016 to determine how much data will be dropped
# ====== Reading files ======
#filter: commits for nova, cinder, horizon, fuel, neutron, keystone, heat, glance, tempest projects for the year 2015 - 2014 -2013 -2012 -2011 - just change the year and the project name in the code below
# Width # Color # Values as integers
# Get row information for user_id: 773192
# mean deaths per month:
# check option and selected method of (12) choice of hydraulic conductivity profile in Decision file
# Rename "odometer" to describe units
# read in dataset
#Importing and instantiating Vader Sentiment
# Name of columns
# Lenghts along time:
# create a new column that we'll make the index so we can merge dataframes
# reading csv file with first column as index  # Here we have dates as the index unlike numbers starting from zero as in previous case
# What is the maximum value?
## Writing to csv
#old page converted
#More details on this file at http://www.firstpythonnotebook.org/dataframe/index.html
#Describe dates
#Conversion is 1.04 times likely in US holding all else in constant
# Use your previous function `calc_temps` to calculate the tmin, tavg, and tmax  # for your trip using the previous year's data for those same dates.
#ign.rename(columns={'platform':'Platform'}, inplace=True)
# when the plot will be created, save it to a file
#select only the rows in files1 who had completed application
# builtins.uclresearch_topic = 'GIVENCHY' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# Read the dow_jones_data.
#now we can register the function for use in SQL
# Expore the data # 286 records with Status == "Closed" #print(len(closed_df.index))
# Let's see how data distributed along with month of year
##### sort by label on column lables
# df=pd.read_csv("/home/midhu/DataScience/nlp/keras-quora-question-pairs/dataset/quora_test.csv")
# Visualization of df_user_count
# Renaming column
"""$ Print all news title docs$ """$
# The number of unique users in the dataset
# Load results for
# how outliers many below 1.625?
# To insert a document into a collection we can use the insert_one() method:
# label-based slice indexing of columns is possible # (note both inclusive borders)
#Conver Receipts into dataframe
#It worked!
# Grabbing the files from the download 
#where x is square footage
# loc indexer can be used with explicit index and coluimn names
# Drops the specified column from a dataframe # You'll be able to see that the volume column is no longer there
#Drop two columns in the dataframe that shows the number of students that pass either math or reading for readability
# Examine purchases here
# Use the engine and connection string to create a database called hawaii.sqlite. # reflect an existing database into a new model # reflect the tables
#Remove duplicated rows by contractor_id #Remove duplicated rows by contractor_number
#Calculating nold
#To compute the logistic regression model
# imports
# Creating a rate column with the extracted numerical rating  # WHAT AM I DOING?
#Count and plot actor 1 Name
# create team-specific df
# Load the previous query results into a Pandas DataFrame and add the `trip_dates` range as the `date` index
# Probability of conversion for old page
## Build a Data Set with every possible (UserID and ProductID ) pair (Cross Join)
# Now lets encode the gender in a numpy array Y
# Describe is the basic summary stats function for numerical values.
# Here I should create something that looks for file renames and maps old name to new name and creates a new column (stable filename)
## Number of missing hour values in the first 6 hours
# Read image predictions of tsv file into DataFrame 
# Extract the features from the users set: timestamp_first_active, sign_flow, age
# accediendo a una columna por el nombre (label) y obteniendo un dataframe.
# Renaming metac site column to match oncology dummy column later
# Print out AUC, the percentage of the ROC plot that is underneath the curve
# save our pca classifier
# Gather benford's distribution percentages
#Count Rows Where Landing Page and Treatment Dont Align
# start_time :: start time of 2016
# Frequency specifies the length of the periods
#15 fold cross validation. Multiply by -1 to make values positive. #Used median absolute error to learn how many trips my predictions are off by.
#look at top 50 days with most complaints. what was the top complaint for each of these days?
# Two variable Scatterplot
#Create a new dataframe and drop the ENTRIES and EXITS, so that we only have INCR_ENTRIES left
# Exporting as csv
#Plot the results as a histogram with bins=12.
# subset for only type 
# extract metadata values
# get conversation length
# split into conversation and last message dataframes # users_df = df[df.conv_flag==0].drop(['conversations','conv_flag'],axis=1)
#Set the index correctly
# number of new-page users
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# day with max return 
# Create a Dataframe #tweet_info_pd.to_csv('NewsMood.csv')
# Let's find the 'random' buttom
#(r_forest.columns[['id','id_str','screen_name','location','description','url','name']], #r_forest.fillna(r_forest.mean())
# Your code goes here #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # pass the loss function that optimizer should optimize on.
# make sure each game as only one label # if there is drop the last the was given
# if we sent a valid request to the API then we should get a Reponse [200] status code # this is the URL that we sent to the api - note I didn't show in the output below bc it has my API key embedded! # this is some additional information about the response that the API sent to us
# Take a look at the submitter_user field; it is a dictionary itself.
# Look for any remaining missing values per column
# Approx 12% of users converted
#reddit_data = reddit_data.set_index('date')
#https://stackoverflow.com/questions/13851535/how-to-delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression
#Calculate Actual Difference in observed CTR
#read from feather
# Look at our current features df
# RE.FINDITER
# Creating folds
# No. of rows in the dataset
# Find the number of times of two variables do not line up
# how many unique authors do we have?
#new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])
#Create list of siteIDs
# concat with overlapping index (default join type is outer)
### pretty useless (blurb is captured in 'df_parsed')
# create with column names
# print the whole dataframe using pandas
#Pretty good score on that one - let's try 1000
################################################## # Load user_logs  ##################################################
# construct an R-Tree for the census tract shapes 
#subset of rows containing "-" in id column. These are errors. #violations_dash #new df with rows containing "-" in id column removed
# Are they aligned?
# Inner join the country data with the ab_test_results page dataframe -df2
# To derive the number of unique users
# Are there significant differences in the average scores assigned by the various reviewers?  # Create a `Series` whose index is reviewers and whose values is the average review score given out by that reviewer. # Hint: you will need the `taster_name` and `points` columns
# Verify DataFrame:
# create the pd DataFrame
#use DatetimeIndex to extract the year from each month of the dates in my data. I created a new column for year.
# setting up second data frame
#fig.set_size_inches(13.5, 9)
# I will add both and see which makes the most sense
#Top 5 Players by Points
#show head of INQ2016 same for 2017,2018
# If I calculated things correctly, there should be only unique user_ids per each group/landing_page, # So I can go this way..
# cars.query('20 > mpg >= 15')       # more complex query # cars.query('mpg > 20 and cyl < 6') # multiple booleans # cars.query('index == mpg')         # Use index in query ***
# # Do a pivot to get a matrix #
#split the data #train_b.summary()
# Descriptive statistics
# transdat = temp_data.loc[1:700,["OPEN", "HIGH", "LOW", "CLOSE"]] # transdat # temp_data.head(400)
#mentions[0].text
# column for year
# check when did the test start and end
#Describe data
# people can resubmit their projects a 2nd, 3rd etc. time... i.e. there are repeates. # Therefore, for this dataset we should only analyze the 1st submission (launched at the most recent date). # This table has projects that have been submitted multiple times:
#Doing a quick test to see if this works
# Drop any obervations before 2005
# Use csv Writer
# Our start period was March 2016 and our period frequency is months.
# Conversion Rate Approx ~12%
# delete a column with pop
# How many stations are available in this dataset?
#Verficamos el reemplazo
#downloaded dataset for daily price
# Fit the model
#I group by year then take the average of the rank for the US team
# Train test split 
# create a column for sales from January through March for 2016.
# import seaborn as sns # sns.lmplot(x='hour', y='start', data=hours, aspect=1.5, scatter_kws={'alpha':0.2})
# Will go ahead and fill in null values with empty strings so that we can aggregate cleanly
# grab the last date entry in the data table
# 2011 created
#'Tampa': 'dc62519fda13b4ec'
#test code, no longer in use #type(ArepaZone_timeline)
#finding the difference in mean between the two pages conversion rate
# converted rate for p_new
#find outliers  #X = x['age'].astype('timedelta64[D]') ###Value counts to see any outliers for the data
# created a dataframe with the text for each article and then combined the output with the metadata into a second dataframe  # dropped url, headline, type_material (all Op_Ed) from the dataset
#exponentiate the coefficients to get the muliplicative change in the odds
# Look for Retweets
# table = pivot_table(df, values='D', index=['A', 'B'], # ...                     columns=['C'], aggfunc=np.sum)
#array created of every variable that is related to review scores #rows that are missing any of the above variables are dropped from the data frame
# Number of positive target values
# Median trading volume
# For perspective, guessing the training mean return in the val set results in the following MAE:
# Sort according to EXT, y first, no latter
#apply Random Forest
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
# Given that an individual was in the treatment group, what is the probability they converted
#post request to chart_data with chartParams, gets back data
# look for seasonal patterns using window=52
#qrt = closePrice.resample('Q').mean().plot
#transform all text features into counts #print features_counts.vocabulary_.get(u'allowed')
#My routine to conceal root passwords
# add a notebook to the resource of summa output # check the resource id on HS that created.
# Examine visits here
# categorize into council meetings, hearings, committees
# Train model
# pickle classified data.
#old_page_converted
# any DatetimeIndex can be converted to a PeriodIndex with to_period() # 'D' indicates daily frequency
#overallCond = pd.get_dummies(dfFull.OverallCond)
# Test
#check it
# read the dataset and take a look at the top few rows
# returns a boolean series indicating which values aren't NULL
# Para el caso de zip code y location y dado que son pocos (en virtud de lo observado anteriormente) vamos a desprendernos de aquellos casos en los cuales # no tengamos ni uno ni otro. Puesto que si tenemos uno podriamos llegar a tener el otro
# we check number of values in each rows 
#Find the highest number of retweets that a tweet received
# copy data frame to only use train set
# Call the method 'generate_chart' from the Class 'report'  
# Keep one of the generated dummy variables # Given old_page as our analysis baseline, we only keep ab_page_new_page column in the dataset
# More data insepction...
#inspect measurement table
#we need so we can later import for anaylis the whole dataset. 
# caution!
#under_sample_data['Diff'] = under_sample_data['Diff'].fillna(0)
# Put the columns to be predicted, at the end
#Apply this mask to create a view of just records from Jan thru Sept
# Create a scatter plot, assuming data are in time order.
#Convert to dictionary/json format
# this data is now sent out to the executors.
# Take a single site, Confirm the correctness of rankings
# first, load data from CSV
# Convert to numpy array
'''$ This program finds the sum of the values in series 's' for every Wednesday$ '''$
# as a final step, we shall drop the rows where there is null value for rating #also, we shall remove records with extremley high value of rating, else we get max rating as infinites
#here is the data:
## Re-Ording Columns ##
# RE.SUB
#test cell:
# Load a single column for rows 0 to 100
# # convert date columns to datetime 
# plot autocorrelation function for first difference RN/PA data
# given that an individual received the treatment, the probability of converting #print('The probability that an individual in the treatment group converted is: {}'.format(df2_treatment))
# TODO: Add your Quandl API Key
### Use conditionals to split the timestamps to three weeks
# Verified that these remaining entries are for non-US location
# -> Having more severe depression leads to worse outcomes
#Drop all of the unnecessary columns for thinking about reliance on conspiracy
#Visualize results
# None of the rows are missing values
# Delete Rows # drop rows for mismatched treatment groups # drop rows for mismatched control groups
# an attribute is changed within the string-like object
# Percentage of is_attributed == 1
# Specify a "cell" domain type for the cross section tally filters # Specify the cell domains over which to compute multi-group cross sections
# Compute the error for every 1000*10*10 generated datapoint. # Then flatten them all out into a list of length 100,000.
# train_data_np = x_train.as_matrix() # train_vehicles_np = y_train.as_matrix()
# Which GPU am I using?
# The following command retrieves the symbols and price for all stocks with a price less than 10 and greater than 0
# dateutil - utc special case
# just an option to change as we'll be looking at text and don't want it truncated in the view
##Adding in some columns that might be useful later on.
# Where repos are both owned and starred
# roi['roi_14'] =  roi_on_day(14, users_df, orders_df) # roi['roi_30'] =  roi_on_day(30, users_df, orders_df) # roi
# Simulate conversion rates under null hypothesis
#some ratings were incorrectly extracted. For example ID: 681340665377193984 should be 9.5 but only 5 was picked up.
# Return descriptive statistics on each column of the DF
# Tweets by Hillary are indicated with a "-H" at the end.
#filtering dataframe by indexes that are in the top 10 most favorite
# better if the data set included a lot more days
#Glance at utc_offset
# create list of columns to lag
# Clean up zones column.  Reduce to either 'Single' or 'Multi'.
#Check that the concatenation worked
#You can also use the offline_PSI_tweets() function to instatiate your data if its already downloaded ## tweets = offline_PSI_tweets(x)
# Define X and y
# create dummies from categorical variable 'country'
#Have to run later, no national events in training set
#Convert the returned JSON object into a Python dictionary.
#plt.xticks(('English','Tamil','Hindi','Telugu','Marathi','Malayalam','Kannada','Bengali'))
# check Initial condition data in file manager file
# ETHNICITY_LEVELS[0]=='Asian', so I'm interested in e==0.
# intentionlly set to display the entire data-frame
# What percentage of violations are Chinese products?
# We replace NaN values with the next value in the row
# Predict_proba returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 1,  # P(Y=1|X), and second column is probability of class 0, P(Y=0|X):
#Finding the mean of the simulated datapoints that are higher than the observed difference in ab_data.csv 
#combine the two months of data
#Changing the Winner with 0 and 1 #if winner is first Pokemon then Winner is 0 else 1
#Get the n_new and n_old
#Checking data types of the columns in dataframe
# Display the MongoDB records created above
# See dataframe
# How many stations are available in this dataset? # Count the number of stations in the Measurement table
#df_concat.drop(["comments.data", , 1, inplace = True)
# Many ways to select rows in a dataframe that fall within a value range for a column. # Using `Series.between()` is one way.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #"https://www.quandl.com/api/v3/datasets/FSE/AF_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY
#Create a intercept column #Create a dummy column ab_page
#Removing rows with a duplicate user_id
#load into products dataframe
#read in the projects file
# since values are 1 and 0, we can calculate mean to get probability
#Tells us non-null types
# drop dry because it is really common #df7.loc[df7['avg_dew_point Created'] == 'dry', 'avg_dew_point Created'] = np.nan
#weather_snow = weather_df['SNOW'].resample('1M').mean() #weather_snow = weather_df['SNOW'].resample('1M').mean()
# note that we use ascending = False becase we want people iwth largest number of things to get higher ranks
# newdf.get_group("2018-02-27 05:06:00")
### Create the necessary dummy variables
# Create a list of filenames, this will be used to loop over each of the different years netCDF files
# Start by sorting data from oldest rides to newest
# y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag) # y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)
#Finding n_new and n_old
# Percentage of users who opted in to mailing list #Only about 25% of users prefer to receive marketing emails.
# Use Pandas Plotting with Matplotlib to plot the data # Rotate the xticks for the dates #df.plot(kind="bar", x="date", y="prcp", alpha=1.0)
#creating a metric to see number of competitors for a given project #number of participants in a given category, that launched in the same year and quarter and in the same goal bucket #since the above table has all group by columns created as index, converting them into columns
# Fill in missing values of 'GPA' with its class-conditional mean (graduated or not)
# Simulations done
# Requires MSMS installation # See the ssbio wiki for more information: https://github.com/SBRG/ssbio/wiki/Software-Installations
# Per instructions, plot a histogram of p_diffs.  As expected, this is a normal distribution:
# Proportion of users coverted. We use the mean.
#check value by print(pdiff)
# check the number of unique locations # should be 1 less than the calls DataFrame
# # CREATE DUMMIES # # Month
# Create Dataframe from Dictionary
#me suena raro que la duracion no tenga que ver con el inicio y el fin.. por lo que lo revisamos. #y no tamos que precisamanete no tiene una relacion direacta.
# index has freq None
### Step 21: Plot the graph with day of week as coloration
# write your code here
#print first five rows of the dataset
# find the number of rows in the data set 
#sa = str(ab_data.group) #print(sa.count('control'))
# Create a dataframe with only the Adj Close column as that's all we need for this analysis.
# opens webpage for use in BeautifulSoup    
# create a set of tups of all filename matches (old, new)
#probability of individual received new page
# Exclude the Tip_amounts that are 0 and calculate the mean of those that are not 0.
# Filtrar solo las columnas que son FLG
# 1000010 is a standard id, so it haven't been considered as mispelled id in the dataset
# remove data that do not have place object
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
# YOUR CODE HERE # raise NotImplementedError()
# Classify budgets to ranges # movies.budget.transform(lambda x: x < 1000000).aggregate(True)
#Check dataset features
# change "odometer" column to numeric # rename "odometer"
# drop rows for treatment group land on old_page # drop rows for control group land on new_page
# Create SOUP
# create the prefetch column
# View the data type
# Rows can be retrieved by location using .iloc
# Inspect data types
# Tot number of stations are 9
#last date available in our dataset
# run multiple times to see samples # randomly chosen sample IOB tagged queries from training data
#Simulation of n_new  transactions with a convert rate of  p_new  under the null
# Take a look at our new dataframe
# build X and Y from dfAll
# What are the most active stations? # List the stations and the counts in descending order. # most_active_id
# Using the groupby method calculates aggregated DataFrame statistics
# Instantiate a 2-group EnergyGroups object
# What are the most active stations? # List the stations and the counts in descending order.
# Show duplicate data entries
# percentage of ratings in lower half (of possible scores, not lower half of distribution)
#checking the shape of the age_up70 dataframe
#Find the index of opening price #Some opening prices were None
#============================================================================== #==============================================================================
SQL = """$ SELECT rating, COUNT(*) FROM film WHERE rating = "PG" OR rating = "G" GROUP BY rating;$ """$
#Interquartile Range: middle 50% of the data
# Import statsmodels and create variable to store the conversionr ates between the old and new pages
# there are 290,584 unique users in the dataset
#Stores file in the same directory as the iPython notebook
# Testing data features
    """for every sample, calculate probability for every possible label$     you need to supply your RNN model and maxlen - the length of sequences it can handle$     """$
#next element is state
#Check via customer sample
# create Series with index
#check the loaded file
# Exploring the distribution of brands
# Examining column names
#average number of completed tasks
#print the rows with index label 12
# Display the results of the Language Stats analysis.
#pass list of columns to df to get the values
#== By Label: Same as above, but faster 
# sort values by date
#Checking the last ad in the dataset is before the end of elections
# Total Number of rows
# Adjust the two date columns.
# We print some information about Groceries # attributes: shape, ndim, size
#transform the features for all sets using the scaler created from the  #train set, this will subtract the mean to center the data and  #divide by the standard deviation to scale it
# First step is to create the groupby summary and extract the first record
### peak hour for each brand
# Variables can be any type of data
# plot of the proportion of p_diffs greater than the actual difference observed
# Save the query results as a Pandas DataFrame and set the index to the date column
# Add the topics columns back onto the original dataframe
# create pySUMMA Plotting Object
# Retrieve Latest Date - This way we do not have to 'hardcode the date' later in our code...
# Without effect modification # drop1(adj.glm,test="Chisq")
# Combine ET for each rootDistExp # add label 
#subset by row index
#### Define #### # Capitalize the first letter of p1,p2,p3 columns to make it consistent #### Code ####
# In order to answer question 4 we slice out of the dataframe rows where 'Scores' equal 4 points. 
#beautifulsoup boilerplate code
# Sort the dataframe by date
#This is Spearman R for comparing tripduration acrross day and night
# There is one user_id repeated in df2 # Locating duplicate id
# Final list with unique links # Remove the lone '/news'/ link # Converting the set into a list
#Split the data as train and test
#sc.stop()
# use 'last', so obtain closing price of the month
# find all sounds of footwork' #for tracks in tracks.collection: #    print(track.title)
# Total tobs
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
# merge the master dataframe and the dataseries dataframe over the key and drop the key. 
# check whether the issue of df.show() is fixed
# Get centered.
# Perform a query to retrieve the data and precipitation scores
# qty_left_1c_store = table_store[0:1]
#import datetime
#first genearte a boolean array generating true values for index or row numbers in the list#first g  #filter out only the false values using ~ operator which will retrieve values corresponding  #to all index values except for 5,12,23,56
# rename column names for readability
# Because the tips that are 0 means they are not automatically generated due to the passengers not using credit card # as a payment. Thus, in order to get a more accurate mean of the tips, we exclude the Tip_amounts that are 0 (Which # also means that the Tip_percentage_of_total_fare would be 0 as well) and calculate the mean of those that are not 0.
#psy_prepro.head()
## Time series
# favorite table # for row in table_rows: #     print(row.text)
# make sure ES is up and running
# Read dataset. While reading make date column dataframe index and parse date column as timestamp
#nt_price = pd.crosstab(nt.index.to_period("W"),nt.catfathername, values=nt["sq_price_value"], aggfunc='mean', margins=False).fillna(0.0)
# look at the example from above, what that ticket is actually talking about
# Install the boto library.
#put it into a panda dataframe.
# can also rename columns inplace
#compare summaries for Matthew 1
#Extract the year from the datetime index
# accediendo a una fila por etiqueta y obteniendo una serie
# verify the type now is date # in pandas, this is actually a Timestamp
# Only look in column 'one' for NaNs and drop a row if any
# make class predictions for X_test_dtm # calculate accuracy of class predictions
# Number of unique entries in "odometer_km" column
# Delete rows/columns # pop -> columns # drop -> rows and columns by using axis
#extract individual genres from list of genre, turn into dummy variables, drop duplicates
# importing statsmodel #number of old pages with conversion 
# inspect the dataframes
#calculare the number of unique user_id 
# Some users are inveted by another users. Who invited most users?
#Check that they have joined properly
# Over Rows
#Remove unnecessary columns
#Device information
# New ARR Closed Won by Industry and Quarter
#Design a query to find the most active stations.
# -> report error: Index does not support mutable operations
# Now there is only one row left with that user id:
# convert to daily frequency # many items will be dropped due to alignment
#Verify we removed outliers
# probabilty of new and old page based upon simulated values in e. and f.
# Cut postTestScore and place the scores into bins # Creating a group based off of the bins
# get index
# # we'll filter out the non-represented classes, sort them, and plot it!
# Loading a pickle file
# number of labels
#Importing URLs from PC_World_Urls.pickle created by PC_World_Scraper_FindProductURLs.ipynb
# Normal
# df.tail(2)
#examining the struncture of the dataframe df
# check how many unique countries are on the country column
# Local backup: data/sea_levels/sl_sh.txt
# builtins.uclresearch_topic = 'GIVENCHY' # builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'FLORIDA'
# scatter_matrix
# get final event of the PA, as this will say single/hr/k/walk/etc
# I want to use woba as a feature in this model. let's make sure these values are close to reality # looks good https://baseballsavant.mlb.com/expected_statistics
#drop_duplicates will remove the rows with same value in specified columns
# rolling sum over data for each hour
#Passing errors='coerce' to convert invalid data to NaT (not a time) #Creating the "Trip_duration" column
#Use ratings to divide into categories #Drop the unwanted columns
# remove 'O' from the labels # labels.remove('O') # labels[:5]
# Get last tweet or mars weather
#Count All Tokens
#trunc_df = trunc_df.reset_index()
# test size of merged file
# 3.
# it's important to realize that this is a series:
# the end of the data frame - has the kept weather columns and the some new grouped up columns
#Pass the tweets list to 'toDataFrame' to create the DataFrame
# Utilize JSON dumps to generate a pretty-printed json
# Model is very overfit since the test score is much lower than the train score 
#Preseving full visitor ID and date since it will be required lated #But first lets check if there are missing values in fullvisitorID and date #Ideally there should be no missing values
#Example 1:
# Make dataframe
# Initialize environment # Initialize experiment
#clean_numerator values test
# Creating Precipitation Analysis pandas dataframe to house the "date" and "prcp" values from the "measurements" table
# Tokenizing your data for use with the model
# using merge to join tables on user_id
#import vader for sentiment analysis
# Remove NaN values
#df_prediction_trial.groupby().count()
#Indianapolis': '018929347840059e'
# What are the most active stations? # List the stations and the counts in descending order.
# We create a dictionary from a list of Python dictionaries that will number of items at the new store # We create new DataFrame with the new_items and provide and index labeled store 3 # We display the items at the new store
#select all rows for a specific column
# apply an aggregate on the DataFrameGroupBy object
# use an index vector to find genes that are longer than 2 million bases # sort the returned values by length # .iloc[::-1] reverses the sort to be max to min
#The weather data frame is 5 times as long as the train data frame, therefore there are 5 entries per date.
# Likes vs retweets graph
# if we specify a join type, this will be equivalent to a merge
# set c to the minimum
#df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') #df_new['country'].value_counts()
## stemming the plot descriptions
### Fit Your Linear Model And Obtain the Results
# Show datatypes
# march has most delays
#group by breeds and then sort them by rating numerator
# Statistics on file length in bytes
#calcuate the age of the stories in the last state it is in
# save new cleaned dataset with no duplicates or records with missing or mismatched values # we will use this dataset in next sections
# sort value by grade column 
# http://www.tutorialspoint.com/python/python_reg_expressions.htm)
#fig, ax = plt.subplots(nrows=1,ncols=2)
# get the exponential value of each coefficients.
# take a look at your results
# For example, this file is part of assignment 1
### Fit Your Linear Model And Obtain the Results
#Sorts the table by Profit. #Since 'pro_result' is a pre-sorted table, we can use just .head() to display the top 5 values.
# Drop rows with duplicate names
#the last 12 months of temperature observation data (tobs). #Filter for Date and temperature column and select rows from 2016
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
# inspect df2
#checking and displating the duplicated user
# Gather the data collected into a dataframe and print it.
# Add a blank column to flag number one hits
#Cincinnati
## Additional: look at the distribution of daily trading volume
#I prefer to choose old one to delete by its index
# A dataframe is an RDD of rows plus information on the schema. # performing **collect()* on either the RDD or the DataFrame gives the same result.
# Call the 'LangStats' class from DOTCE.
# Load sample dataset
# We print percentages:
# columns in data # first and last item in data confirming I have all 2017 information
# create the column timestamp with Hours for our dataframe
# Checking head for data verification
# Print the tuned parameters and score
# the 2018 offseason.
# a timestamp with both date and time
#reindex the table
# Get rid of the duplicate entry
# Use Pandas Plotting with Matplotlib to plot the data # Rotate the xticks for the dates
#Select only one topic per group
#We need to forecast the stock price - here we need to forecast out the 1 percent of the valu #0.01 says next 1day prediction into the future
#will create percentile buckets for the goal amount in a category
# Create a feature to indicate whether it is a holiday
# Random Forest Model 
#Now I am ready to merge
# Save df_TempJams to csv
# Import data from Azure Blob Storage # Import data from the home directory on your machine  # dataFile = '/home/katlin/Desktop/PysparkExample/sampledata.csv'
# np.where() is a vectorized if-else function, where a condition is checked for each component of a vector, and the first argument passed is used when the condition holds, and the other passed if it does not # We have 1's for bullish regimes and 0's for everything else. Below I replace bearish regimes's values with -1, and to maintain the rest of the vector, the second argument is apple["Regime"]
# Convert df to csv
# No date column here. It has int index. Now create a date column
# Convert JSON tree to a Python dict #data = json.loads(geocode_result) # print to stderr so we can verfiy that the tree is correct.
#To include missing values in the distribution and to use percentages instead of counts.  #And to rank by date in ascending order(earliest to latest)
# Info function to check missing values
# Vader
# Sample size for old_page 
# Most prolific tweeter (by days) according to our dataset #df[df['userID'] == 710145000000000000.00] # Most prolific tweeter (by num tweets)
# transform test data (using fitted vocabulary in stfvec) into a document-term matrix
# Print paragraph texts
# logic: query the converted column where group column value is control and take mean
# Add interaction terms for both page AND 
# drop the is_dog column, as they're all true now
#calculating differnece from orginal dataset
#Calculating median trading volumme for the year
# ranking according to every factor, descending #df_factors_PILOT.to_csv('./df_factors_PILOT_rank.csv')
# Fixing the datatype 
#In Dataframes we can reference the columns names for example:
# Optional: Write dataframe out to CSV
# there is a 0.120386 probability they converted if they were in the control group
# Now we are picking up the random code that is used for every youtube video
#TRANSOFRM THE PARAMETERS BEFORE GOING TO SEARCH 
# save our final DataFrame as a CSV file
# r.raise_for_status()
#Use Pandas to plot an area plot (`stacked=False`) for the daily normals.
#49352 rows and 15 columns #bathrooms
# if check for the 50 clusters you can observe that there are two clusters with only 0.3 miles apart from each other # so we choose 40 clusters for solve the further problem # Getting 40 clusters using the kmeans 
# reset the index to the date
#df.survey.unique()
# Print data from nasa_url to make sure there is data 
##1.4. Creating a (pandas) DataFrame # We create a pandas dataframe as follows:
# 1.959963984540054 # Tells us what our critical value at 95% confidence is
# Print the external node types
#remove incorrect numerators
#lv_workspace.set_data_filter(step=1, subset='B', filter_type='include_list', filter_name='SEA_AREA_NAME', data=include_WB)
# Run Kruskal-Wallis test for male vs female
#y.to_csv('./data/y.csv', index=False)
# there are many ways to find the # of rows with NaN or missing values # in this case, we have 110572 car sale entries with NaN values
#Try indexing the other way -- still only 950 matches
#Load data
#output.to_csv("result_modified_97rfcorrect.csv", index=False)
# Merge free_sub and ed_level together. Which column should the merge be performed on?  # Do this using both the concat() and merge() functions.
# Loop Media Sources 
# count the number of complaints per moneth
#Before we do any aggregations we need to get rid of the timestamps
# define the data path # load json data # view the first 5 rows of data
#check how complete each column is. VIOLATION_DESC seems to be most complete with only 9 null objects (compared to range index)
# note that some locations may not have community leagues df.loc[df['Community League'].isnull()]
#out_columns=out_temp_columns+incremental_precip_columns+general_data_columns+ wind_dir_columns #columns to include in output
# As the values for converted are one of the two {0, 1} mean can be taken # to determine the probability
# Fix incorrectly geocoded coordinates
#create an extractor object: #create a tweet list as follows:
# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.
# Saving the model for use in future legislation
# number of unique items per feature # review how to print to get the numbers lined up nicely-- see Course-Syllabus 9.6
# Convert sentiments to DataFrame
#drop some columns from data
#The result shows there is no missing values
# generate class probabilities
"""$ Run preliminary test of classifier to get baseline accuracy$ """$
# show first few rows
# Plot time (x-axis) against number of retweets for each tweet (y-axis)
# Configure how graphs will show up in this notebook
# 3.Calculate what the highest opening prices were for the stock in this period
# calculate means by country and group for comparison
# We can finally combine our two columns into one DataFrame, which we will be joining with the ACS Data  # cleaned in another file.
#Get the text of the tweet
#reading in my api key saved in censusAPI.py as #myAPI = 'XXXXXXXXXXXXXXX' #from censusAPI import myAPI
# To show the shape of the data we selected (number of rows and columns)
# The beginning of the first 100 comment items...
# new groupby object by department AND city
#vectors of unhelpful double characters will be created, so we need to remove them
# Save file to csv
#help
# find indiviual probabilty recievd new_page
# Create new dataset
# examine the coefficients
# Any DatetimeIndex can be converted to a PeriodIndex
#Test:
# Creating dummy variables for country # Dropping US column to consider it as baseline
#Convert Category and County Number to int
# Create df with datetimes range as index
#Create the pandas dataframe using the ID array to merge the other features into
#**Which Tasker has been hired the most?**
# creating new vs return fan csv
# scale data
#When using period, instead of end, and a frequency of business days,  #Pandas will calculate 72 business days starting from 1/1/2017 #This will work for any period and freq you specify eg we could have had 72 hour elements
# check for all rows which have an entry for in_reply_to_status_id or in_reply_to_user_id
# Load needed Python libraries
# Preview the ratings dataframe
# look at unique regions that are missing data # this code might be wrong...
#convert objects to datetime for date column
#bow_test=pd.concat([bow_test,data_test_model],axis=1) #bow_test.sort_index(axis = 1, ascending = False) #bow.sort_index(axis = 1, ascending = True)
# Find the most active stations # List stations and observation counts in descending order
# another example from Tesla with more data points
#  and save a reference to those classes called Station and Measurement.
##The id of the user has been dropped for now
# sort the DataFrame by five_star_ratio (descending order), and examine the first 10 rows
# check if any values are present
# converting p_diffs into numpy array
# Add season.
#  we actually don't need new columns because per our earlier work,  # 'ab_page' matches the new_page value exactly.
# M = Month end frq and MS = month start freq
#NB: mean accuracy is higher because it is choosing the best prediction row by row. then comparing with actual category.
# Another way to see relevant information is to use the `describe` feature of DataFrames
# Latest Date
#inspect dataframe
# Change row & col labels
# z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])
# take only elements that belong to groups that have less than 5 members
# view the fees by year|
# Correlation plots of all variable pair can also be made with seaborn
# From the docs: "Max id returns results with an ID less than (older than) or equal to the # specified ID. 
# isolate weekends
# Sort index and assign dataframe again
# check abnormality, such as NaN and 0 value # if the min value contains 0, drop 0 value # if the count does not match with other variables, drop NaN
# creates the intercept
# reset - the row index will be made a column again
# Create a series for month in which account was created.
#Do we have any missing values? Apparently no.
# put into pandas table
#if using gaussian density as an argument for position #using the shortest distance on long and lat to find the density new X_test
# examine the highest rated posts
# cost per day
# concatenate dataframes with different rows
# How many NaN's -- pretty good case to drop observations with location NaN's as only 25 out of 85K observations
# group classifications
## Print top 10 tweets and notice Sentiment field added at the end of each record
#Find out how much of the data is after 2016
# use pandas to view the data # We create a pandas dataframe as follow: # We display the first 10 elements of the dataframe
#0th element in the list is metropolitan area
### Since order of rows has been intact on all 4 dataframes, ### joining them row by row into one final 
# Construct the client object # Build the authorization url for your app
# Prevents tables from being truncated.
#Split the Data in training and testing dataset
# dropping columns '[', ']', and ','
# Show all output
# Save the dataframe in a csv # Repeat the process for all tweets
# instantitate an empty list called data to store the result proxy
# or  ... s[['c']]
# Number of rows: 294478
# How good was our prediction? Calculate the total squared error!
# Double Check all of the correct rows were removed - this should be 0
#import the Linear Regression package from SK Learn to create the linear regression model #import the train,test split package to succeessfully create a model that uses a proper testing and training set 
# And we come full circle! We encode the list we created in  # a json string. We could then provide that over the internet # in our own API!!
# Double check if there are no more negative values in dataframe's "Tip_amount".
# Apply np.cumsum() to each column 
#5 random lines
#set input/output paths #can eventually set this to the SOAPY API https://dev.socrata.com/foundry/data.waterpointdata.org/gihr-buz6 #DATA_PATH = "/Users/chandlermccann/Google Drive/Google Drive/Berkeley MIDS 2016/W210-Capstone_WaterProject"
# data munging # satisfaction
#Diference between group:
# Calculating propotion of users that converted using old page
# Check on the duplicates
# replace NaN with placeholder value, let's say 99999
# new data frame
#calculate diffrence in p under null hypothesis
# creating bands csv
# fig.savefig('toma.png', dpi=300)
# # # created a dataset that use postcode as index
# request data from the API
# the last 12 months of temperature observation data (tobs). # Filter for Date and temperature column and select rows from 2016
# Change the Date column to the index to facilitate plotting
## Now try creating a dataframe that has the subset of crimes with the primary description CRIMINAL DAMAGE  # and the secondary description TO PROPERTY # Will need to google how to use & operator with pandas
# Set the index to date and sort by date # See what it looks like
# OSX Users can run this to open the file in a browser,  # or you can manually find the file and open it in the browser
# this method displays the predictions on random rows of the holdout set
# show the repated user_id
# most delays are on thursdays
# sort from the oldest down to the youngest
# YOUR CODE HERE #raise NotImplementedError()
#Proportions in when looking at sum of revenue
# Load raw hashrate data over the last two years # Downloaded from https://blockchain.info/charts/hash-rate
# path to saved figures
# checking the number the unique values in the user_id column and storing  # it in the variable # pritnig the number of unique users
# creating a matrix after dropping nulls
# Note: Count can't be greater than 200
# Dict of all co_occurences
# outliers?
# Fixing for the aforementioned problem with variable named "fit"
"""$ Make tmp sr pickle$ """$
#filecount
# what percent is that?
#since we are referring to the null in this argument, this is simply the conversion rate for the entire  #data set since the null argument is that there is no difference in conversion rate.
#Calculating the conversion rate of the new page
# Store the wavelength info into a key of the same name
#print(df['text'].values[i],classify_tweet(df['text'].values[i], classifier))
#comparing geocoded and sctual location
# 8. 
#this will upsample:
#Convert the data type to datetime and then parse the date
### Fit Your Linear Model And Obtain the Results
# Location; use labeled index # Integer location; use numerical index
# Calculate n_old
# Information of Tip_amount based on credit card usage
# Extract NoData value and store into dictionary
# Convert each 'tweets' DataFrame to a list of dicts
# Transforma datetime para string # Sera utilizado a data em formato de string para aumentar a performance # de busca/insercao de dados do dataFrame usando .at[index, col]
# So now dataframe is loaded with score per WC team per year, and we can answer question 1, we do this via a factorplot
# integridad db
#getting dummies for hand drive
# Convert sentiments to DataFrame
#Converting existing list to a Pandas dataframe
# caution
#Qn 2 #Convert the returned JSON object into a Python dictionary.
#number of users that received the new page; where group = treatment
# List sources
# retweets
# User number of control group n_old
#flight.select(regexp_extract('arr_time', r'[+-][0-9]{2}:[0-9]{2}\b', 1)).alias('d').collect
#2. Convert the returned JSON object into a Python dictionary.
#Show first 5 tweets
#finds date of most recent and least recent tweet
#use_data = use_data[(use_data['win_differential'] <= 0.2) | (use_data['win_differential'] >= 0.9)]
#compute descriptive statistics for all the available columns
# 4. 
#since we are referring to the null in this argument, this is simply the conversion rate for the entire  #data set since the null argument is that there is no difference in conversion rate.
# some chakcing to be sure that the number of authors which are left is correct
# creating new columns reperenting interactions between countries and pages # displaying first three rows of the updated dataframe
#df_concat_2
#df_pd = df_pd.set_index("timestamp") #df_pd.set_index("timestamp")
# DEFINE :Convert tweet_id to be string as it is categorical. # CODE: #TEST
# import any additional libraries which will be needed
def to_date(s, year):$     '''Returns a date from the datetime library from a string like \'January 1\''''$
# this way, numpy can get around the "fixed" type of objects
#Return a list of all of the columns in the new dataframe.
#clean the data from NaN values
# Now decompress
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# read the file saved for modeling
# Step1:  get rsIDs for the variants
#Got nothing back! it worked!
#Drop any columns with too many unique values or too less values - seller, offerType, nrOfPictures
# Filter dataframe without undefined state and assign it back to data
# Import the pandas package, then use the "read_csv" function to read # the labeled training data
# Transform output into a classification task.
# read ratings of small dataset
# visit_diff_from_rsrv = air_store_t[air_store_t.air_rsrv_visitors != air_store_t.air_rsrv_visitors_y] # len(visit_diff_from_rsrv)
# Difference between original data set and ab_dat.csv 
#before 2011
# districts tend to adopt apps in Aug each year, makes sense to prepare for the new school year
# Inspect the table(s)
#All dates are now datetime
# What happened to the nan? They are still nan, because our Series didn't contain them.  # Therefore, when we put the Series back into the Dataframe, those cells stayed empty.
# Sometimes you'll run into an OAuth bug when you run ok.auth(). If so, # uncomment this line of code and run this cell again. #os.remove(os.path.join(os.path.expanduser('~'), '.config', 'ok', 'auth_refresh'))
# first, find the number of users with pets # then, use this and num_unique_users to compute the percentage
# Import the initial, required libraries and modules.
# Approach 2: Creation of  #Class_frame.loc[Class_frame["amused"]>0]
#new column ratings string
# Create a set of boxplots of like_ratio by category
# Concatenate column names from different levels in the hierarchy. #
# Load query data into a dataframe
#Head shows top 5 rows of ingested data
# create our kmeans model & predict clusters
# compute the p-value based on single tail test  #compute the p-value based on two tailed test
# Set the index to the date column
# Check any of the rows have missing values
# The DataFrame is created from the RDD or Rows # Infer schema from the first row, create a DataFrame and print the schema
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON stru+cture that will be returned
# a[y,x] = avg
# To call (instantiate) the class
# query to pull the last year of precipitation data
# get the shape of control group
#mistake here to just use priors_reordered for reorder =1 which makes all data reorder =1 only
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #20. Join each user to his/her first occuring transaction in the transactions table #Use Group by and Frist functions to locate first occuring transaction from Transactions and users dataframe.
# creating ad df
# compute z-score and p-value using sm.stats.proportions_ztest
# merge 'yc_depart' with the destination zip codes
# Make sure no variable is the index
# driver = selenium.webdriver.Chrome(executable_path = "<path to chromedriver>") # This command opens a window in Chrome # driver = selenium.webdriver.Firefox(executable_path = "<path to geckodriver>") # This command opens a window in Firefox # Get the xkcd website
# Create engine using the `hawaii.sqlite` database file
# Am I currently using a GPU?
#Create an intercept for the regressions to come. A dummy variable for the 'landing_page' variable.
# plt.show()
# probability than an individual in the experiment received the new page is 0.5000619442226688
# Encontrar los registros que se encuentran entre las posiciones [10mo - 20vo] de mi DataFrame
#boolean indexing #query
#new_page_converted
# Concatenate multiple data frames into a single frame
# remember there are about 250 days of stock trading in a year.
#seaborn settings
# If everything is working properly, this should print out # a Status object (a single tweet).  clinton_tweets should # contain around 3000 tweets.
# Load the model that we created in Part 2
# Function inputs # Plot the matrix
# add intercept and dummy column # df.head to check output
#Find out number of unique users
# TODO : load data 'AMZN.csv'
# Regex to use in detecting unvalid names
# assert monthly data
# Print all of the classes mapped to the Base ### BEGIN SOLUTION ### END SOLUTION
# check size again
# Get the convert rate for the new page
#plot the distribution under the null #plot line for observed statistic
# Plot just the Dew Point data
# and we can see it is a collection of timestamps
# NS --> Non Seoul 
#List of stations and observation counts in descending order
# if we'd rather have the grouping variables as columns # (not as index):
# All of the steps above a needed to build a combined dataframe # Notice also that the date is a column
# Write to disk to prevent us to have to hit the API again
#Convert True to 1 and False to 0
# dummy all the subreddits
# Create a copy dataframe of the origin # Collapse the dates # Showcase the new copy
# split the data into training and test
#Calculate the proportion of users converted by using mean()
# show dataframe first rows
#array with 23 dataframes (one for each week) with the videos, tags and other information
# Function defaults to 2-sided test, but we want a 1-sided test ie where # alternative hypothesis is new_page > old_page therefore we must # pass extra parameter to function to ensure 1-sided test
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#Challenge 1
### Fit Your Linear Model And Obtain the Results
# Time series for retweet:
# probablity = 17264/145310 = 0.1188 (approx)
#Array of unique user_id values
# The null distribution we've created with bootstrapping has a normal bell-curved shape, as expected.
# Our classifier is now trained. Wow that was easy. Now we can test it!
#Merging the second train file with train dataframe in the dictionary
# Convert sentiments to DataFrame
# discretizing data also results in categoricals
#Display the columns and their data types
# investigate date_crawled
### perform a two sample proportion hypothesis testing, with the alternative p_new > p_old
#ADD CODE for using cleanNewUserArtistDF and print
### Race density bimodal?
#need to rotate x tick labels return to this
# Open/create a file to append data
# retain the first occurrence of each row (drop dups)
#WARNING: This cell will take a while to execute - up to a couple of hours
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
#3(a)
#row info for the repeat user id
#(df2['converted'] == 1).mean()
# interactions between treatment page and country
# transform the data
# Grouped by date 
# fitting the logistic model.
#split the dataset 
# fit validation model
# And, plotting the comment score distribution for this Dataframe...
# Creamos series de tiempo para datos:
#use describe to give a quick summary of the data,  #include = 'all' to get also categorical columns
# sorting values
# plot means by month:
# even tho aldaar is below average it has the most retweets as outlayers 
# specify which columns to keep
# The Conversion rate regardless of the web page
# train word2vec on the two sentences
# Summary Statistics for Station - USC00519397 WAIKIKI 717.2, HI US
# plot the sum of the columns as bars
### remove the rows for users in the treatment group that were assigned the old page ### remove the rows for users in the control group that were assigned the new page ### preview the new dataframe
# query to pull the last year of precipitation data for the busiest station
#Save figure
# Retrieve page with the requests module # Create BeautifulSoup object; parse with 'html.parser'
# Run this cell
# Correlation coefficient for each field
#Filter events which topic is in top10 #Compare shapes
# print the p-values for the model coefficients
#Load the query results into a Pandas DataFrame and set the index to the date column
# Copy data from the cloud to this instance
# We're plotting them all at once so... there are going to be some visual issues. # Volume distorts the visualization because those values are much larger than the prices. # Let's put all those other ones on a plot.
### Create the necessary dummy variables
# The p-value for a two-sided hypothesis test
# use the logistic regression model # fit the model
# Take a look at 10 observations with the following fields
#build query for last 12 months of prcp data #using the last_date datime variable and relativedelta in filter to get last 12 months
#use the sampling distribution to simulate the distribution under the null hypothesis
# Read the csv data file. It is encoded in UTF-8. # File has three columns: moisture (float), part (int), and operator (int).
### Create the necessary dummy variables
# Convert to URLs.  This can be done with for loop, but for simplicity, we use map function. # Extract ID portion of entries # Make it consumable by KEGG API (Convert to list of URLs)
# Get the number of rows from the shape
# tsla > 30 <=> last time user was active was more than T0 (30) days before D0
# calculating the probality of an individual in the treatment group #converting and store it in a variable # printing the probality of conversion
# Retrieve the jpg file url  
#print unique value of country column
#Save
# convert time
# lgb1.feature_importances_
# but it has recovered it up to some tiny epsilon
# setting X & y and tts 
#added layer of granularity in event customer churns within month...count the scn
# Create the grid widget
# get countries and show the 3 digit code and name # show a subset of the country data
#Show the rows corresponding to index values 6 thru 10
#We must create a new dataframe with the country codes added by uder_id
# check the class distribution
# Add the average.
#create a function to normalize the data #apply that function to each column of the dataframe
# Settings for notebook
# Let's check that we actually normalized the data - before
# df_lib_con.title = df_lib_con.title.str.replace('\d+', '')
# Sort the dataframe by date
# Shows the significance of the z_score
# get indices
# Create an authentication using the given key and secret # Connect to the Twitter API using the authentication
# just plot the dataframe and see what happens
#Test to make sure connection is working.
# Set 'user_id' as index in df2_dummy
# check to see if there are any np.nans
# Double check if our cleaner functions works well by looking at how many tickets by # of words
#most referers are the mobile app
#Using the requests function to download file from HTTP
### Create the necessary dummy variables
#number of unique users are stored in a variable named unique_users
#Insructor Note: Walk the students through creating the dummy variable.  # Two possible solutions: #OR
# How many stations are available in this dataset? #Design a query to calculate the total number of stations.
# Array of probability of converting, the probability of not convetring 
# read csv file
# make recommendation for a single item/restaurant
# Inspect the data to determine values for an implicit enum
# Save it to a csv
## fill NA in 'genres'
# see data in tabular form!
# costumise dates with time info to midnight convention:
# Create new dataframe of containing total congressional tweet count for each day in the data set
# columns in table x
# weird... 
# RN/PA decomposition
# Add author information for model
# No duplicated tweet id
# Must set filter to subset for len(boolean) to work when calling get_filtered_data, if no filters are set, boolean is None
# The convert rate is the same for p old under the null
# finding the minimum and maximum and minimum temps by day of the year over the period 2005-2014 # minimum_temp.head()
# T :: it will transform the table and make rows as columns and columns as rows
# additional: question posed by previous reviewer # how to get p-value as if it were a one-tailed test
'''Performing Polynomial Features on the Pending Ratio Column'''$
#A reminder of the range of values in number of daily trips.
# mean US temperature grid time series # mean plot
# it has a name
# Create a data frame with two columns: UsersID & country_destination. country_destination contains the predictions
#Total_entried=df.shape[0]
#Check how many NA #Type #Print the 3 first line
#Activation cohort x cohort
#Prettify indents the parsed page in a 'pretty' and readable format
# Given that an individual was in the control group, what is the probability they converted
## Get the No of Followers and retweets 
# sum of counts is 10278
# Apply the function
# look for seasonal patterns using window=52 - RN/PAs
# Instantiate a 2-group EnergyGroups object
# Show a feature with a depth of 2
#etage quality is not usable
#Outputing dataframe as excel sheet #writer = pd.ExcelWriter('TITLE.xlsx') #dataframe.to_excel(writer, 'Sheet#)
## this works because we set the index to time
# need to find out what the data type of tweet id is:
# deployment
# Get the responsible city agency and complaint type for the first ten 311 calls in the data
#Plot Null distribution #Plot vertical line for observed statistic
# filename2 = 'expr_3_qi_nmax_32_nth_1.0_g_101_04-17_opt_etgl_L-BFGS-B.csv' # df2 = pd.read_csv('../output/data/expr_3/' + filename2, comment='#')
#train = train.loc[lambda x: (x['time'] > 6) ,:].loc[lambda x: (x['time'] < 21) ,:]
#validation set accuracy
# Create a new dataframe that removes the rows where the landing_page and group columns don't align
# https://stackoverflow.com/questions/40894739/dataproc-jupyter-pyspark-notebook-unable-to-import-graphframes-package
# Drop all NA values and reorganize the columns to have our predicted variable as the last column
# Plot age - looks like average user is around 30, which makes sense.
# For this, I will use the preprocessed df again.
# examine column headers
# concatenate with the set of resulting columns the intersection of the column names by specifying join='inner'
# Your code here # df['All'] = 
## not listed is datetime[ns]
# 3.2.B OUTPUT/ANSWER
#strip spaces #or: turnstiles_df.columns = [column.strip() for column in turnstiles_df.columns]
# defines our session and launches graph # runs result
# We use the `pd` alias to tell Python that we want to use a `pandas` function
# Probability == 0.5001
# plt.xticks(rotation='45')
# find the best combination of the hyperparameters
#Test
# Use the Base class to reflect the database tables
# return 'train' and 'new' to their original contents
# calculating number of commits # calculating number of authors # printing out the results
# we want only the companies
#Remove header row
#users who purchased multiple cameras and at least one of those were a replacement
# Dropping the entries we failed to find bill text. 
# cumFrame['Date'] = pd.to_datetime(cumFrame.Date)
# Find out Shape of the data
#drop one of the duplicate user id rows
# Import the labeled data
### Create the necessary dummy variables
#Instantiate model 
# datetime of the tweet object is in UTC (Universal Time Coordinate) # so I generate a column for the time of the tweet in Pacific timezone
# Fit pipeline 
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# pick the top n games based on either leverage_ratio or confidence
# Plot bar chart with retweet count and favorite count by stage
# View unique causes of death
# NLTK Stop words
# Function to mahe the csv utf-8
### Step 14: Import Gaussian Mixture model to investigate the PCA plot ## Specify 2 -> for two clusters
# pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
# baseline accuracy
# remove any non-numeric characters and convert the column to a numeric dtype
# first 25 shared domains
#final.insert(0, 'ID', mid) #final.head()
#Convert dataframe to list
#df_geo_insta.to_csv('instagram.csv')
# join image_prediction dataset
# Shows the index and user_id of the duplicated ID:
#Seccond Data Set:
# check size again
# Show each of the individial transactions
# baseball_df is larger than can be viewed in jupyter, so instead we call .info on it to see some more details.
# Verifying value types
# Fill of the 'unknown' value for NaNs # Final cleaning up of the table schema
# create search_weekday column
# Check column names # Change column name
# rural
#Method-2 # df.assign will give you new data frame with old and new variables.  # You can create as many as variables
#creating the binomial distribution for the Old page
# Loading the model we created
# Get Facebook data and store in variable.
# do note the freq
# examine the start and end times of this period
#Check that the filters worked
# Get the dataframe
# plot null distribution and line at our observed differece
#Plot histogram for p_diffs. The red line shows the observed difference between the mean conversion rate for the #new page and the conversion rate for the old page. 
# Mars Facts URL to Scrape
#Calculates number of users with old_page
# search for a tag by name # shorter alternative: access it like an attribute
# print(len(yuniq),max(yuniq))
# removing leap days
#df1.loc['2017']   'the label [2017] is not in the [index]' #df1.loc['year'==2017]        KeyError: False #df2017.head()
#save the clean data set in a new file called ab_cleandata
cur.execute("""SELECT title, journalname from public.datapull_title limit 10""")$
# created_at is likely to be parsed as text # We convert dates to a date/time format so we can do 'maths' on them # If ok, dtype should be datetime64[ns] or <M8[ns]
# Break the text content apart for better formatting.
#face detection attributes
# leadsdf = leadsdf[['_id','firstName','lastName','phone1','ingestionStatus','RVOwner','address','city','email','entryPerson', #                                    'submissionFile','filename', 'participant','program']] # leadsdf
# Find z-score and p-value
# Tells us how significant our z-score is
# Removing the rows where  the landing_page and group columns don't align.
# Counting the number of rows with NaN values
#tlen.plot(figsize=(16,4),label="Length", color='r');
#change the type of columns to integer
# Find the div that will allow you to retrieve the latest mars images
#The number of events hosted by groups 
# plot first 10 samples from training data # plot 10 random samples of test data 
# Choose the station with the highest number of temperature observations.
# URL of page with Mars hemisphere high resolution images to be scraped
# calculate actual difference using original data set # place p_diffs into an array # proportion of p_diffs greater than actual_diff
#To save excel file 
# compute the exponential to interpret the results
# This shows, that two columns are the same, so take only one column
# check to make sure the total number of rows changed after drop
# mydata['new'] = dc2015['created_at'].dt.hour
# Get list of messages
# explore price and odometer
# Remove columns
# re-assign variable value
# Export to "tallies.xml"
#just a quick look at the count stats:
# Drop the now-redundant columns
# get the shape of treatment group
# First let's turn each title into a list of words # Then we'll use the lemmatizer to change plurals to singulars to group them # And rejoin the lists of words to one string for the count vectorizer
# load json twitter data # Convert to pandas dataframe
# test_preds_df.head()
# Mean of p_new and p_old
# take away "," and "km" from "odometer" column
#Return a list of all of the columns in the dataframe as a list
# you can also' take a dictionary of lists and then pass it into the data frame
# the version iterates over the data
# It seems people recently joined have performed most of the activity in rating or not rating Paula's profile 
# create accelerometer value regardless of axis (the absolute max of x,y, z at any given time)
# specify column type
#create arima model
#DATA_ROOT = 'data/workshop-content18/5-cloudpbx/data/cloudpbx_sample_data_10k/' #CSV_FILE_PATH = os.path.join('locn-filtered.csv')
#find the one user_id that is repeated
# Copy data frame and apply function
#  each month end of 2017
# List of columns to remove # Pass the list of columns as a parameter in the drop method
# creating month csv
# reshape the data to be plotted
# making a dataframe with all features (called predictor_df) #predictor_ df = X
#df.loc[df.age < 0]
# Table values are from 'D' column  # Indexed by combination of 'A' & 'B' columns  # Result table has columns for each unique value of 'C' (i.e., 'bar' and 'foo')
# create the path to the data file # the file is named 'test1.csv' # read in the data
#use prettify to analyze
#dfg = dfg.reset_index(drop=True)
# create a mosaic graph 
# Predicting the  y values on the testing  Random Forest Classifier #fit on the training, predict on the testing data
# Joining two data frames
#Construct time features
# Initial Table Head
## get core and xtra lists into dataframes for merging into eppd
# how many rows, columns do we have now?
# Import csv file # Merge datasets with an inner join # Check dataset
# Checking class balancce in y_train
# create target vector(s) | lookforward window
#Put the indeed and tia data together, then clean some more
# shift so that we're trying to predict tomorrow
# Variables created for tweepy query
# Max RTs:
# Filter down to just Ductless units
# Set up logistic regression # Calculate results
# read in msft.csv into a DataFrame
#creating my day column #creating my dataframe for converted users
# check Initial condition data in file manager file
#new dataframe with only tweets by users who haven't tweeted yet
## Check the top 5 tweets in the dataset
#Check rows with missing values
# Ordeno de mayor a menor la temperatura media
# data count
# subset temp to us
# Conversion probability regardless of landing page
#mongo returns a generator, which we call cursor
# class look up and as it is a frikin descriptor # then call it
# notice the small number of changes per contest - less than 4, compared to over four times this many for regular season contests
#Arlington
#Create empty df template #df = pd.DataFrame(columns=['Link','Title','Publisher','Journal','Authors','RDate','PDate']) #Or import df from pkl file
# Remove the ".csv" from the file name # Set the index as the 'mail ID'
# Remove some problematic data (non-ascii characters need to be cleaned)
# use a right close
# concatenates the individual files into a single df # csv files must be in your working directory
# To find the first event time for each device
# selecting the repeat user and store in a varible
# checking the number of unique value in user_id column and storing it in  #a variable # printing the number of unique user_ids
#show info/length of collected data
# Score gridsearch logreg model on test data
# merge in author information
# what percentage...are not state 0? came from example where affairs = 1 and no affairs = 0?
# Survival rate by geneder
#inspect dataframe
# Generate the statistical summary of the Trip_distance variable.
### store p_diffs as a numpy.array ### compute the p-value
# Creates a dictionary based on the sentence tokens
# Drop any row with NA/NaN # how = 'all' will drop only rows with ALL nan
# what is the duration of author's publication career?
# Computation of the mean of conversions, which consists of 1s for True and 0s for False, as the proportion:
#Doing info to check for NaN values #Info reveals that location and subjects do not have 85152 values.
#'Oklahoma City': '9531d4e3bbafc09d'
#reorganize data set
#get projection information from windfiled
#shape of all three dataframes
# Read xls file
# Calculate the date 1 year ago from today
# convert from string (dtype='O') to float64
#corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi
#Sort by Country #Finds the column in dataframe, then searches each row for value specified.
# Number of unique products
#Strip unwanted newlines to clean up the table.
# dummying purchase
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
#reduce dim (240000,1) -> (240000,) to match y_train's dim
# Prepare the data
# Compute the average age ate booking by gender for Febuary 1st, 2017
# and we should also look at what the weights for the different features are
# Getting a list of column names and types # columns
# Showing that the last 35 values have been shifted up and replaced with NaN values
# Set storage path # show the new storage path
#what is th eofficial Water Resources Station Catalogue ID of site '105001'?
# Retrieve the parent divs for all articles
# We start by finding the youngest player to play in the MLB, calling .idxmin on the "ageAtDebut" column
# Import package # Build DataFrame of tweet texts and languages # Print head of DataFrame
"""$ Check relative distribution of data features$ """$
# number of unique projects included in both classification dump and API results
#find number of rows in dataset #294478 rows
#subset records in month 07, 08 into file2
# find the proportion of users converted
# n new: 145310 
# Create a list from 0 to 40 with each step being 0.1 higher than the last
#converting  ad_created and last_seen to datetime
#view the probability of the converted rate for both control and treatment  #by applying method
# How many stations are available in this dataset?
# We can rename the column to avoid the clash
#test set accuracy
# Z-scor significance # Critical value at 95% confidence 
# check Basin variable meta data in file manager file
## Covert Rate with respect  to experiment group 
# The convert rate for p-new under the null is: 11.96%.
# View all the classes mapped to the Base
# Cleansing the Search Term for the records with SearchCategory of Plant, to only include plant id and removing everything else
#Create TFIDF vectors for modeling
# msft calls expiring on 2015-01-17
# drop duplicate row # confirm drop of duplicate row
#== Describe : Show quick status summary (interactive tool)
# commit changes
# write treated telecom file
# we cant work with the null value so we replace with them with a negative large number of value
#'Philadelphia': 'e4a0d228eb6be76b'
#Sort by Country #Finds the column in dataframe, then searches each row for value specified.
# Merge the tweets from 2016 (post 8th Nov) and 2017 to one data frame
# p_new-p_old for the simulated values will be very close to 0.
#Count the number of stations
# concatenate dataframes with different columns
# Plot the histogram
#df_review = r.copy()
# The "description" column has the text that we want to analyze 
#get the contents from the response object returned from requests
# calculate RMSE
# get the p-value
# lower case strings
# Count hashtags only
# df.loc['2013-01-01'] # df.loc[:,['A','B']] # df
### Fit Your Linear Model And Obtain the Results
#simulation of Nold transactions with Pold conversion chance
# check shape of DataFrame
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #19. For each user, get each possible pair of pair transactions (TransactionID1, TransacationID2) #Merge Transaction usng USer ID.
#Collecting review
#15. Join users to transactions, displaying all matching rows AND all non-matching rows #(full outer join)
# How do those 54 recipes look?
# groupby on the ID. # SO_reference  #https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python
# check if dataset is imbalanced
# accediendo a una columna por el label y obteniendo un dataframe
#Creating a column "Hour", which is the hour extracted from the pickup time
# Sill have duplicate data, also it created extra columns
# coefficient of determination R^2 of the prediction.
#Same as above
hotel_query = """ $         """$ booking_df = snowflake_query(hotel_query)
# A:
# check shape
# tweet does not exist anymore, remove it
### female users are much more active in terms of posting than male users
# Linux / OSX # Windows     #! "c:/Program Files (x86)/GnuWin32/bin/wget.exe" --no-check-certificate https://pjreddie.com/media/files/yolov3.weights
#Remove unnecessary columns
##Replacing missing category number with name from the ILB website
#Imprimimos la cantidad de columnas y filas
# Read Measurements file with pandas and inspect the data
#(print(len(a[0])) #The lenght of the feature vectore is equal to the number of different words in the test sample.
#Loading the Data and displaying first 5 rows
#created ticker dataframe (to add column 'ticker' to databreach_2017 dataframe)
# Convert the sex column to type 'category' # Convert the smoker column to type 'category' # Print the info of tips
# SQL Connection and attributes
### Create the necessary dummy variables
# Since this is such a small fraction I wam going to drop anything with na's # If you drop everything with this
# instead of df.groupby('key'), we can:
#dates for 1 station (df.STATION=='59 ST')
# set up logging for gensim training
# scatter plot: x-cordinate = index, and y-cordinate = Age
#df2.groupby('group')['converted'].mean()
# Create number purchases per user feature
# see an example for a given author (e.g. Zhou Shisheng)
# Calculating the probability of conversion for new page
# les mois, jours, etc. sont dans donnees.index (ou df[i].index) # on peut faire des masques :
# Use Pandas to import a csv file into a dataframe # Check the dataframe to see how many rows and columns it contains
#Get data for the Haw River (siteNo = '02096960') using the function above
#festivals.index = festivals['IndexT']
### Fit Your Linear Model And Obtain the Results
#checking the results
# Group the data by media outlet
### Fit Your Linear Model And Obtain the Results
# Count mentions only
# Iterate over all cells and cross section types
# Remove all rows with NaN birthyear
#1b. Display the first and last name of each actor in a single column in upper case letters. Name the column Actor Name
# the first 10 characters represent the day
#Verify tables exist
# types of events
# combined_df5 merged all uncommon values into a single 'other' var; this is an alternative strategy
### Substitute out replaced names in main dataframe under new column name ### And reduce dataframe to only include top 12 most common grapes
# number of rows and columns
# Push the sentiment DataFrame to a new CSV file
# Define the DataFrames with only a single "Category".
# returns an array
# This should train within a minute or so
# replacing 'nan' with '' for improved legibility
# create a temporary column called filename_groupby that includes other filename when one value is dev/null
# check Initial condition data in file manager file
### Looking at the range of weeknum ajust manually past year weeknum
#creating a serie from a dicionary #will be created in key sorted order
#drop the second row with a duplicate user_id
# p-value as if it were a one-tailed test
# Car KOL pred
# The proportion of users converted
#calculate the distribution of values in the date_crawled, ad_created, and last_seen #normalization is a term used to make all values standardized. eg, student marks <=100 while subjects <=7 so to normalize either make all standard to percentage like 100% so 85/100 marks= 85% for 5 subjects= 5/7 = 71%, so both columns got % values out of 100%, this is normalization of columns.  
# normalize # standardize
#Double Check all of the correct rows were removed - this should be 0
# data=stock_data["CLOSE"].iloc[-100:]
# Obtaining class information
# Max FAVs
#shows that we don't have null values
# count the gene id
# From here and for a while I work on methods to clean U.S. House bills specfically. # This includes decisions of where to begin considered text.
#app_ids
# With the list of team abbreviations, I can now rip through a URL template
#converting tweet_id datatype to string
# View the polarities
#df2.query('user_id == 773192').iloc[0]
#importing from microsoft excel
# since values are 1 and 0, we can calculate mean to get probability of an individual converting 
# number of times new_page and treatment don't line up
# Design a query to retrieve precipitation data and plot the results
# To insert a document into a collection we can use the insert_one() method:
# Table info to check NULL values
# convert rate is how many users converted (i.e. converted = 1)
#Review histrogram of customer actions
# rename id tp tweet_id
# check the structure of Watch Event
#Using the CSV file from earlier, we can create a Pandas Dataframe:
# KNN model 
# predict on validation set
# check Basin variable meta data in file manager file
# Slice using both "outer" and "inner" index. #
# get the summary of the model
# convert dictionary to dataframe
#%matplotlib notebook
# list of strings that begin with a lower case in the name column
#test the column order
# Delete duplicates
# migrating table info # for row in table_rows: #     print(row.text)
# check if duplicates in user_id
# for example, show me the BBLs where probability of a low-income household is above 90%
#access_logs_parsed.takeSample(False, 10)
#drop all rows where treatment and new_page don't line up and assign that new dataframe to df2
# last year a ship sailed
# merge_table = pd.merge(info_pd, items_pd, on="customer_id")
# identify single column to target
# create a boolean mask to isolate the duplicate user_id row
# ph = load_df('mergeable_holidays.pkl')
# Plot the cumulative sum of a Series
# Becasue we assume under the null that p_new = p_old = p_converted # We get p_new = 0.1196
# Fit Your Linear Model And Obtain the Results.
# store CV results in a DF
# SAVE A CSV WHICH NOW ONLY SHOWS DATA FOR SONGS THAT REACHED NUMBER ONE
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# removed string word comments
#retrieve pages with request module
#caller.join(other, lsuffix='_caller', rsuffix='_other') #df3.set_index('user_id', inplace=True)
# Daily Profit & Loss Frame
#Number of missing data rows in each column
#first genearte a boolean array generating true values for index or row numbers in the list #filter out only the false values using ~ operator which will retrieve values corresponding  #to all index values except for 5,12,23,56
#join_c.sort(F.col("party_id_orig").desc()).sort(F.col("part").desc()))
# Inner Join users to transactions (UserID)
# LDA Model 
# show last 2 rows of data frame
#Check For missing values
# list of workspace ids # workspace_ids
# Save the query results as a Pandas DataFrame and set the index to the date column # Sort the dataframe by date
# different than find_one
# What are the most active stations? # List the stations and the counts in descending order.
# Use the Inspector to explore the database and print the table names
# normalization to put everything in float and convert "N" to nan
# start_date = input('Vacation Start Date: Year-Date-Month format ') # end_date = input('Vacation End Date: Year-Date-Month format ')
#creating test and train dependent and independent variables #Split the data into test and train (30-70: random sampling) #will be using the scaled dataset to split 
# Fill in missing values of 'Age' with its class-conditional mean (graduated or not)
#Check there are no duplicates
#Test
# Also plot the actual observed difference in conversion in our sample
# I couldn't get the #s and the dates to match up (1 more line in the counts which is like the indexing) # saved them both to a .csv and then combined the csv worked though
# read dataset # inspect dataset
# correlation between score and num_comments
#Read in Seattle open data on Airbnb from Kaggle
#preds = model.predict_proba(X_test_mean)
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Get indices for rows to be removed # Drop rows
# We display the first 10 elements of the dataframe: #display(data2.head(10))
#Size of RC_2018-02 is 56 GB
#Given the individual received the control page, what is the probability of converting?
# Check to make sure everything worked out
# df_plot.sort()
# set SUMMA executable file
#Plot histogram
# Create a modality other for brands with low occurance
# plot histogram of the counts
# Group by group and calualate the mean of converted
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned #url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=API_KEY&start_date=2017-01-01&end_date=2017-01-02"
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
# Load the data #df = pd.read_csv("/Users/Stav/Desktop/Iowa_Liquor_Sales_reduced.csv")
# Specify multiple aggregation functions as a list. #
# fig = month_plot.get_figure()
# Merge companies_pf and funding_pf on Company # companies_pf = 8,664  funding_pf =  10,901  merged_data = 9,069 #merged_data.head(5)
#Investigating if any of the rows have missing values
# drop one row with row number index
# Extract projection information # metadata['projection'] = refl['Metadata']['Coordinate_System']['Proj4'].value
################################################## # Load transaction  ##################################################
# Delete records that duplicate brand / outdoor unit / indoor unit model.
# review small df window after engineering lagged features/target vector(s)
# Get the statistical summary of age at booking by gender for Febuary 1st, 2017
# Apply the smoother cleaning function (part 2), by checking indexs in transfer_duplicates DF and then updating BTC DF
#results = results.append(nflcont.interest_over_time())
# view the second isochrone
## How many tables are there in the database
# Insert distance2 into df as a col # loc=11 sets the location at index11 after end station longitude # value=distance2 sets values of the column to the pd series list created above
### Parse "data" into a data frame reading it as JSON
# train data-frame before adding the new feature
# Extract all noise-related complaints with a filter # Save the results to a new data frame
# plotly does not work with "dates" in strict sense, therefore we need to chage it again into a string
# abs is actually a numpy function so it can also be implemented as follows
# Export collected weather data to csv file
# an age is at 0, it's due to premature babies according to the symptoms - lets leave these
# Now we can drop the columns we used to derive the dog_type column: # We should also drop the source column - it contains the same value for nearly every row and doesn't tell us much: # Test
#The logit model is fitted
#plot the histogram
# The number of times the new_page and treatment don't line up. # adding rows where <group> equals 'treatment' and <landing_page> does not equal 'new_page'
# Calculate the proportion of users converted
# first date where nr of coins are c
# describe the imported data: mean, median, max, min
# Functions to count the number of files per folder
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
# initialize authorization instance using credentials
# determine the order of the AR(p) model w/ partial autocorrelation function, alpha=width of CI
#creeate a column for the  intercept  #create a dummy variable column
# Removing duplicated user_id
# There is missing data again, so we drop them as well:
# Around 90.38% of p_diffs are greater than the actual difference observed.
# OPTIONALLY, print the keys for each turnstile (in case we need to access it later) # for key, turnstile in turnstiles: #     print(key)
# make sure these numbers match DataFrame below for '2016-08-07'
#sns.lmplot(data=aa,x='weekofyear',y='message',aspect=2,scatter=True,fit_reg=False,markers='x')
# Lenghts along time
## ratimg denominator ##
#Example 1: 
# Converting the "date" field into the date data-type
#select URL to gather info
# specify outer join
# 1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017. # 2. Convert the returned JSON object into a Python dictionary.
# resample data to weekly
# 1. Unuseful dog tag doggo floofer pupper puppo, remove those columns
# A:
# Convert created_utc to readable format
# Remove 'user_id = Null' from dataframe  #df = df.dropna(how='any',axis=0)
# Create column to represent day of experiment not day of month
#Henderson': '0e2242eb8691df96'
#The .toDF method will turn an RDD into a dataframe
#finding the missing values
# view the first five lines of data/msft.csv # !type ..\..\data\msft.csv     # on windows
# read dataframe
# drop 'title' and dummy all of it.
# Run me
#find the row information for the repeat user_id
#Created a margin and price per liter column 
# Location outside US, ignore
#Boxplot functions
# This is a comment. Anything after the # symbol will not be interpreted as code.
#append the old and new csv
# When checking the signup_time & purchase_time, it is found that # the time difference is very correlated with the class (fraud) #add time difference as one more feature (change to float)
#determining what the oldest tweet accessible is 
# write out new master json file
# Every row in 'beta_dist' is a simulation of all the 'arms' in the experiment. # This step identifies the winning arm in each iteration, by calculating the MAX per row
# model for all model visualitation
##For each user, get each possible pair of pair transactions (TransactionID1, TransacationID2)
# obtaining unique number of values ofr user_id column
# Use Pandas to calcualte the summary statistics for the precipitation data
# the cards column contains the cards of each set in json format, so each set of cards can be # converted from a json object into a DataFrame
#odds ratio
#Draw graph #Run graph
# Default plot
# Check the length after removing the retweeted tweets
# check inserted records
#extracting variables of B28002 that contain "broadband"
#### Test ####
# Try logistic regression for classification
# Write your answer here
#appending 1st,2nd and 3rd computer into one system
#Create a column DAY that will label the days 0:6, 0 for Sunday 6 for Saturday
# here we look for duplicated user_id, but we show both of those in our result by keep=False
#dataSeries = pd.Series(df_Q123['Outside Temperature'])
# although faster than python loops or comprehensions, # less effient when computing compound expressions
#checking distribution of projects across various main categories #kick_projects.groupby(['category','state']).size()
# retain the last occurrence of each row (drop dups)
#keeping rows with sensible registration date 
#Convert the returned JSON object into a Python dictionary.
# calculating or setting the year with the most commits to Linux
# Save the cleaned master dataframe into csv file
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure ## Structure of the JSON:
#Akron
# Creating intercept column # Creating ab_page dummy variable column
# breed_predict 
# 0.9999999383005862 # Tells us how significant our z-score is # 1.959963984540054 # Tells us what our critical value at 95% confidence is
#'Wichita': '1661ada9b2b18024'
#read data values from the provided data.
# are there any NaN's
# image predictions imported from website address in .tsv format using requests library
#params['class_weight'] = ['balanced']
# Next, compute the duration of the campain, in weeks
#Given that all the values are either 1 or 0, the mean will be the number of 1's divided by the total number of rows.
# New conversion rate of treatment group
# create an HDFS directory for this assignment
# Generate new feature in our DataFrame for the label
#convert top 20 brands and their mean prices to a dataframe
#utilize the description column to count
# Get help
# Now we'll try "device_ip" - 2MM values in the reduced training set
####TEST # b['Compound Score'].mean()
#once we have created the Dataframe, we can print out the schema to see the shape of the data
#More Manual Calls
# stops_heatmap.save("heatmap.html") # stops_heatmap.render()
# feats_dict
# filter(function or None, iterable) --> filter object # Return an iterator yielding those items of iterable for which function(item) # is true. If function is None, return the items that are true.
# print index values as explained  the project sheet - 2nd Data frame.
# Use Pandas to calcualte the summary statistics for the precipitation data
#combined.head(600000) #combined.to_csv("data/kaggle/combined.csv")
#ohlc_BRML3.reset_index(inplace=True)
# just for sanity chaeck
# We display the first 10 elements of the dataframe:
# From arrays
#Compute proportion of the p_diffs are greater than the actual difference observed in ab_data.csv
#Plot histogram
# make a resource public
# Show the clean description of the ticket
# Create a new dataframe that retains only songs that are in the number one position (for each region)
# Find the duplicate user_id. 
#quandl database query through API
'''$ Ayush Jain$ '''$
# Load the data from the query into a dataframe
# give tweepy access
#find the end date from first non zero observation for tobs at given station #assuming each month is 4 weeks, 4 weeks * 12 months = 48
# get the unique number of games that Jimmy did play in
# What class passenger was on each deck?
# Design a query to retrieve the last 12 months of precipitation data and plot the results
# sort posts by number of likes, and only return posts that contain a tattoo:
# Read both CSV files and mearge them # Print out first few lines
# Using the inspector to print the column names within the station table and its types
# Visualize the topics # vis
#converts to dataframe
# find campaigns with under 50 users # remove users with those campaigns, and those with 'unknown'
# getting GEO ID for Philippines
# copy the dataframe to a new one, for clarity
# accediendo a una columna por el label y obteniendo una serie
#sessions_nonUS_purchase = sessions[sessions['user_id'].isin(train_nonUS_purchase)]
'''Ridge'''$
# let's use tab next to the "." and see the string propierties
#the length of the data frame is too big and we need to predict  #1% of the data so lets narrow it down now
# Now would be a good time to back up the data frame to a csv:
# Construct pandas Series objects
#now we will append our created data set to the current data set  #we are working with;  We'll then check the size of the data  #set to make sure it's what we'd expect; 
# Both Pandas and Numpy are sufficiently clever to figure out that the mean of a  # series shall be the mean of its contents provided they are numerical
# Gonna quickly check what the distro looks like here
### Step 17: Dissect just a portion of the data ## The red cluster has a high peak commuter pattern displayed (Commute days)
#Probability of converting regardless of page
#copy the data from dictionary to 'tweet_json.txt'
# result = pd.concat([Aust_result,Aussie_result]) # result.head(5)
#code to work with a polygon input
# non unique userid
# Number of each type of column
# Choose the station with the highest number of temperature observations. # Query the last 12 months of temperature observation data for this station and plot the results as a histogram
# Top 5 violations by product/type combination overall
# convert tweet_id to string datatype
# check it
# Simulate 10,000 times n_new size transactions with a convert rate of p_new under the null # Simulate 10,000 times n_old size transactions with a convert rate of p_old under the null # Calculate the conversion rates difference
# SpaCy pipeline
#CALCULATES THE PROBABILITY OF GETTING A OLD PAGE 
# merge 'yc_merged_drop' with the departure zip codes
# save the model
# Car KOL
# adjust local path.
# Perform a query to retrieve the data and precipitation scores
#example_text_file = 'sometextfile.txt' #stringToWrite = 'A line\nAnother line\nA line with a few unusual symbols \u2421 \u241B \u20A0 \u20A1 \u20A2 \u20A3 \u0D60\n'
# Boolean variable to return rows with max price
#Frequency count
### Fit Your Linear Model And Obtain the Results
# Variables contained in this file:
# testing = pd.read_excel('SHARE-UCSD-export_reformatted.xlsx')
#oh shize, did we ensure the OSes are covered too? will need to improve eventually, but as all are Linux, TTE is ok #doing D2MI_DEV, not as clean? nope - we have RHEL and Linux mixed, fix it later
#truncate format to Year-month-day
#Read Train and Test Data (stored in current directory)
#determine what percentage of our data has invalid values
# Type in the search bar, and click 'Search'
# The mean squared error
# Counts of users that received the old page.
# extract matrix for fire heatmap
#Average monthly returns of the portfolio
# 2014 created
# Clarify late games.
# Drop Rows with Missing Values # Make Year, Month Columns
# Assign a whole row
#read files
# Making a copy before setting index to `start_date`
# Data type of each column. #
# Use Pandas to calculate the summary statistics for the precipitation data
# Importar el modulo data del paquete pandas_datareader. La comunidad lo importa con el nombre de web
#model['fell']
## url
#Fetching of data from yahoo finace
#one can ignore this cell if ANN is not required
# now we have negative values, so we need one more transform before we can run MultinomialNB
#df.columns = df['Created Date'.replace'created_at]
#Date
#How many words have more than one translation?
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Checked the shape of the stores opened
# To read dataset # inspect dataset
# Use **statsmodels** to import your regression model. # Instantiate the model, and fit the model using the two columns you created in part b.  # to predict whether or not an individual converts.
#Creating Another DataFrame to get the Column Orders as per requirement #Verifying the Change as per requirement
# There are not any null values
# Creating an new dataframe that holds all mismatched rows
# get the probability of receiveing a new page
# Extracting BBC news sentiment from the dataframe # Use datetime library to convert Data stored in string to datetime format
# Baseline score:
# Inspect number of rows and columns 
#client = MongoClient('localhost', 27017) ## dropping a database via pymongo #client.drop_database('test_database')
#List the unique values inthe HydrologicEvent column
# Logistic Regression.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# 2013
#sqlContext is used for defining DataFrames and working with SparkSQL #Use sc to create our sqlContext, sc has the connection information for the #Spark enviroment
# df_2015
# Change type to category # Here's the initial categories, set automatically
#Check records where objecttype == null
# Mapping using BLAST
# check that all of the correct rows were removed 
res = sqlCtx.sql("""SELECT name, stddev(result) as std__of_result$             FROM tempTable$             GROUP BY name""")$
# Show our results
# Generate all the column names of the data so we would not need to constantly scroll upwards
#we join meter transaction table file2 with meter location file loc by meter pole id:
#test if the names have been replaced
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Convert to date time object
#unique user_ids
# summary of the regression when the country is US
# Need to do this
# display feature matrix dimension
# Alternatively you can join within Python as well (or Scala of course) # Here again, the column names are case sensitive and ID is not the same as id. Pay attention to the case of the column names !!
# TODO High-Cardinality Categorical  Cluster # plot(df_train.iloc[:, 35])
#Reading all necessary CSV files from Scrapy in the DIR
# remove the unwanted columns # what does our data look like
#removing non-numeric characters and renaming the variables
#inspect dataframe#inspect 
#session query
# rf.fit(features_class_norm, paid_status_transf)
# calculating number of commits # calculating number of authors # printing out the results
#5. Change the names of multiple columns.
# new_discover_sale_transaction.groupby(by=['Email', 'Created at'])['Total'].sum()
# Calculate the date 1 year ago from today # Need to figure out today then subtract 365 days - note that it says TODAY in instructions!
# Check whether the zipcode and city difference are correct.
# For some reason, the data types are being reset. So setting them back to their expected data types.
# Ensuring data is ordered by date
# show two examples
# flight.describe().show()
# Retrieve data from nasa # Create BeautifulSoup object; parse with 'html.parser'
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
# join the output of the previos grouping action to the input data set  # the goal is to optain an additional column which will contain count of publications per author
# creating a DataFrame from df3_list
# View certain rows of the test dataset
#- Force the creation of the directory to save the outputs. #- "If the subdirectory does not exist then create it"
#get the ground altitude from the modal value of the rtk data
# We need to save y values as trained too even though they are not scaled to avoid confusion with previous split
# Seperate data
# make a calculation directly # or create a function and apply it accross rows of data # save data to file
'''$ Number of appointments by coach$ '''$
# set date as datetime
# Manually download and open the file
#n_new would be where group is equal to  'treatment'
# Create a function, "getHour", that will be applied to all values of the 'lpep_pickup_datetime' variable # to extract the "Hours" values of each row of data
# Critical Z score value for a one tailed test at confidence level of 95%
#plt.hist([x[1] for x in Counter(commenters).items()])
#Keep only [2.5, 97.5] quantiles (word count)
#Making change Permanently using Inplace parameter #Verifying the change done above
# It is the same as:
#active
#renaming it as per given conditions #confirming the change
# lg = pd.read_csv('awesome_go_logging.csv') # TODO: should have a function for that ....
# we're hoping to predict, is actually the future price. As such, our features are actually:  # current price, high minus low percent, and the percent  # change volatility. The price that is the label shall be the price at some determined point the future
# Print the head of airquality_melt
# OSX Users can run this to open the file in a browser,  # or you can manually find the file and open it in the browser
# Pickle the 'data' dictionary using the highest protocol available.
# save matrix as csv.  # np.savetxt("foo.csv", g.featureMatrix , delimiter=",") # another way
# save to new csv
#df[df['group']==treatment].converted.count()
# fire report  #fire.head()
# dates are the first date of the old API to the last date of the new API
# this is what i saved..... # well, crap.
# Summary of the results
# updated actual observed difference using original dataset- responding to Udacity reviewers comments
# 2013 created
#get good trainers
# get artists whose name contains "Aerosmith"
#Re-scale #We keep this value to re-scale the predicted unit_sales values in the following lines of code.
# Delete a collection snapshot # To delete all snapshots, use: # collection.delete_snapshots()
# Display min & max wavelengths
# Load the last statepoint file
# Evaluate the results of the predictions: Range 1-5
#NASA news site, pull latest article text and date
# Load the query results into a DataFrame and set the date column as an index
# look at the unique values for country
# change columns to lowercase so easier to deal with # class_codes.columns = map(str.lower, class_codes.columns)
#Conversion rate for the treatment group
# images table # ASSESS: check if any tweets do not have images # looks like all tweets have images 
# Access the first row by positional index:
# fix missing chisqprob issue, see https://github.com/statsmodels/statsmodels/issues/3931
# Create a staging bucket required to write staging files # When creating a model from local files
#Converts time stamp to date and time.
# What are the most active stations? # List the stations and the counts in descending order.
# freq='B' is business days
#simulating the new_page_converted 
# Replace the null values in the columns with 'unknown'
# print(len(new_page_converted)) 
# We don't want to join a group that is in a grace period!
#killfile = killfile[:20000]
# Print the street name
# index in the resultant contatinated df
#Divide the amounts given to supporters by the total of all
# Design a query to find the most active stations.  # Which station has the highest number of observations? # List the stations and observation counts in descending order
# df_2007
### Create the necessary dummy variables
#need to rotate x tick labels return to this
#rows
# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html #list(set(df[:1000]['brand']))
### Fit Your Linear Model And Obtain the Results
# Returns the percentage of users in a single country
#'Virginia Beach': '84229b03659050aa'
# loading data into pandas for inspection
# mask = (combined.state=='United States') & (combined.color==0)
# Investigate negative response time values:
# 14. Create and print a list containing the mean and the value counts of each column in the data frame except the country column.
# column for month-date
# Graficar precios de cierre y precios de cierre ajustados
# Calling r.json() does indeed produce a dictionary as requested
# check option and selected method of (16) type of lower boundary condition for soil hydrology in Decision file
## We can use the /agences resource of the Index API to get more information. ## Below, we combine the previous steps into one line and ask for the agency associated with the first feed:
# Assign tie values the maximum rank in the group
# oauth authentication
# A:
# Export to CSV
# Saving the DataFrame as a .csv file.
# accediendo a files y columnas por etiquetas
# now creating 'metro_area' and 'state' features # testing proof of concept -- going to split 'location' and then access separately
# Number of stations
#find distance between element
# Days_missed mean for graduates
#checking if columns are removed
#Since all our coefficients are in negative, to convert them to exponent values I have taken the inverse of their values
#Convert genre from a column of dicts to a column of strings by retrieving the name attribute from each dict. #compute one hot encodings using str.get_dummies #tmdb_movies_vote_revenue.set_index('id').genre.str.get_dummies().sum(level=0).head(10)
## Add company to my df
# Task 4
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# identify the total page counts for the control group
# Annnnddd... now every column has no string datatypes... :) 
#tickers = companies['tweet_ticker'].tolist() #Tweets with the ticker in front come in very slowly. May take a while to build up. But these are official tweets
# creating 'length_of_ad' feature
# Define outliers as (Q3 + 1.5 x IQR)
#Read in csv file into two dataframes
# Double check all of the correct rows were removed.
# returns the number of rows and columns
#replaceing time under an hour with 1 hour 
##Read in data
## To determine the proportion of users converted in treatment group
# Create test data for predictions with neural net
# Calculate the average chart lower control limit.
#test if dog breed column is updated
#dropping the original 4 columns of dog stages.
# Save the temperature query results as a Pandas DataFrame and set the index to the date column
# the resulting mapping returns all sets that aren't in invalid_sets
# get training corpus feature vectors
# map the count function to each strike where there is a nan implied volatility
# n-old is:
SQL = """$     SELECT category_id FROM film_category WHERE film_id = 2;$ """$
# Ignore the index
# Fix for non pyspark start notebook 
# date
###plot with category
#Check to see the change in Index.
#Boston
# Plot tweet lengths against time:
# Slice with index using iloc
#df2.head()
# specifying the index column
# Lower case columns.
#simulating the Pold. 
#Put all bus names in lower case
# get average of numerical features which grouped by business_id
# we can also change this in place, by setting the parameter inpace to True inside the drop method
# select orders related to our selected users
# change tweet_id to string
# We can display all the data in the Series that exceeds the median of the Series
# Create a second logistic regression model with baselines as US and old_page
#converted users dropping the duplicates
# counts number of stations
#sum() #Returns the sum of the values for the requested axis. By default, axis is index (axis=0).
### To keep it clean and simple for NLP analysis, stick to one language only, English ### Given that population of English-speaking countries is dominated by the U.S. and Canada,  ### the second largest host of projects, is dual-speaking, data set will be limited to US only
# Statistics on each column
# add edges # print number of nodes and edges
# changing format to datetime
# ,test_df=df_test #todo: add a test set
# train the model # make the prediction
# Fit the model # Obtain the results
#you can get this from https://www.kaggle.com/egrinstein/20-years-of-games #contains reviews about witcher 3
# Combine the state and zipcode columns together to form the statezip column. # taken from https://stackoverflow.com/questions/19377969/combine-two-columns-of-text-in-dataframe-in-pandas-python # Drop the two columns.
#delete columns
# we will remove a number in our array
# How many stations are available in this dataset?
# Subset to only week 0 or over 12 # NOTE: week only available for self-rated, thus this selects self-rated implicitely # Subset to only those colums I will need 
# Evaluate bestModel found from Cross Validation
#### Assessing the final dataset
# Use Pandas to print the summary statistics for the precipitation data.
#Reciprocal value for ab_page.
#for abs_page
# Count mentions only
# Calculate median number of comments 
#model.fit(X_tr, np.log1p(y_tr))
# Plot time (x-axis) against length of tweets (y-axis)
#Round down to closest value actually found in data set - "2 years". Replace NaNs with this value
# write your code here 
# Fitting the grid search
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# This is to force jupyter to display all rows when asked.
#Looking at the components of the time (of the trip duration)
#Creat dictionary for FaultDesc_Corpus and Req_Corpus
# before_sherpa # after_sherpa
# df.at['Row', 'Column'] = 10
#number of times the new page appears 
#Number of Treatment Users
# import google closing price history
# Getting the table names for each table
#fare.value_counts()
# Print original review and beautifulsoup modified review with html tags removed
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
#world = world.to_crs({'init': 'epsg:3395'}) #world.plot();
# Printing the content of git_log_excerpt.csv
# Apply new function to the column entries
# make predictions
# data_folder = '/Users/alex/Documents/ODS/oct_4_2016_dump'
# Create a scatter plot of likes vs views
# create new dataframe with profile_ids # add feature value column to existing dataframe
#Removing one of the identified duplicate rows
# based upon the documentation
# We can cut a part of the data
# Merge tweet_archive and info
#For each day in train.date, find the number of docks (parking spots for individual bikes) that were installed  #on or before that day.
# Compute p-value
#Import BeautifulSoup into workspace #Initialize BeautifulSoup object on first comment
# TM Min and Max
# unfortunately the index is numeric, which makes # accessing data by date more complicated
# When both left and right has unique key values # The 'inner' is taken. key='zoo' in the right is not adapted into merge result.  
# Assign the measurement and station classes to a variable called `Measurement` & `Station`
# load workspace configuratio from ./aml_config/config.json file
# The row count has reduced by 4
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
#The amount of users for the treatment group
# As usual, it is good practice to work with copies.
#see the head of the dataframe again
# Find the possible values for the country column
# Fit Your Linear Model And Obtain the Results
# training 
# Getting logs of the fake user
# df.loc[df['column_name'].isin(some_values)]
#Here is in "unit increase", though the value is below 1... However, I find it more intuitive this way, without calculating the reciprocal #If an individual lives in the US, it is 0.99 times as likely to convert than the baseline (UK). #If an individual lives in Canada, it is 0.95 times as likely to convert than the baseline (UK).
# And on the test set.
# Split data into train and test data
# series
# group by multiple columns
# Export the dataframe to a csv
# The base dataframe's end.
#Create the data frame that will be used for training, with the dictionary we just created.
# how many unique authors do we have?
# import class codes to sort housing by broad category # class_codes = pd.read_csv('../../datasets/san_francisco/san_francisco/assessor_office/assessor_class_use_key.csv')
# create a Series from a list with implicit index
# Print the first 5 external nodes # input_nodes_DF = pd.DataFrame.from_csv(input_nodes_file, sep = ' ')
#zt = r1_test[r1_test == False].index.tolist()
# flight_pd.head() # help(pd.read_csv)
#rename column headings
# Final categories and number of associated projects in each
# index all variables in data dataframe
#compute test statistic and p-value
# We'll hold out 10% of our data for validation and leave 90% for training
# find the earliest and latest tweets in the set
# 4.What was the largest change in any one day (based on High and Low price)?
#Save returned_orders_data to a csv file.
# df.set_index('id')['value'].to_dict()
#act_diff = p_new - p_old # compute difference from original dataset ab_data.csv
# select a column
# check if any null values
# add intercept column # add control and treatment columns as dummy variables
# Visualize parse trees
# For a class of cars, has the mileage increased over model years 
#p_old = df2.query('group == "control"')['converted'].mean()
# Using ravel, and a string join, we can create better names for the columns:
#plot = df['6/27/2018':'6/28/2018'].plot(y='lux')
#using value counts in descending order. 
# number of times 'new_page' and 'treatment' do not align # two scenarios where this occurs # add the occurances of both scenarios together
# Prefixing the my_tag dict with ** passes all its items as separate arguments # which are then bound to the named parameters,with the remaining caught by # **attrs
#pd.DataFrame(pred).head(10) #pred[:10] same data, just array ## use DataFrame to drop zero
#Iterating through the companies data frame for the twitter usernames #printing every twitter username
# Use `engine.execute` to select and display the first 10 rows from the table
#lastly, let's convert our datetime columns for later use. (We need to tell pandas these are dates)
"""Start Logging"""$ # Some of the code below takes a long time to run, the logging helps knwo that it's still working
# Checkpointing feature
# Save your regex in punct_re # YOUR CODE HERE # raise NotImplementedError()
# In order to set the values that are originally 0 or with negative values to 1 in "Fare_amount", we need to first  # find their dataframe indexes within the "Fare_amount" column. Then select the "Fare_amount" column, which each  # value would only be 0 or negative then assign those values as 1.
#No duplicates between merging owns and stars on repo-user id combos
# Finding unique user_ids # Checking for non-unique users
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# feature importance for Random Forest
#http://knowledgetack.com/python/statsmodels/proportions_ztest/ # Tells us how significant our z-score is # 1.959963984540054 # Tells us what our critical value at 95% confidence is
# Are the ids unique?
# your code
#df_en['text']
# Calculating probability of landing on new page
# create a chart, and we might be tempted to # paste the code for 'construction_year' # paste the code for 'gps_height'
# Sort data frame by target and tweets ago
# throwaway_df.select(throwaway_df.count).show()
q6b_answer = r"""$ """$ display(Markdown(q6b_answer))
# simulating n_new transaction that have probabiliy of having old page, using binomial gives the outout as the number of 1's
#Displaying metadata for dataframe df
# install rate has deterioriated with more recent adoptions
# find historical data for 2009
#prenatal pairings, all time
# List the primitives in a dataframe
# Check how records not matching group/landing_page alignment compare with duplicate user_ids:
# Before processing delay_time statistics, remove cancalled entries first # Translate delay_time to float # Note: groupby preserves the order of rows within each group
# We can set a MultiIndex while reading a csv by referencing columns to be used in the index by number
# Design a query to retrieve the last 12 months of precipitation data and plot the results
# 4. Which Tasker has been shown the most?
# calculate mean on compound scores
# print summary
# 2012
#Sample IP address
#50th is 6, 80th is 38, 90th percentile is 183 up_votes, 95th 997 up_votes, 99th is 15170, max 192,674
# Double Check all of the correct rows were removed - this should be 0
# Now let's see how many bills per session in our data
# Plot all the positiv and negativ of the dummy data # This is an example of not choosing to save the image
# Summary of the image prediction data
# Create the Word2Vec model.
#face detection attributes
# This is bad news!
# Replace year of birth according to correction we received from the Portland Police Bureau.
# Timestamp conversion 
#plt.plot('date', 'tweets1', label='tweets')
# Create table with the number of precip observations per station
# How many stations are available in this dataset?
#check to ensure all the dummies were created successfully
# cols and rows information
# take a look at your results
#remove tweets with no image
# Remove duplicates
# Plot the results using DataFrame.plot
#print df.is_application.head(5)
# convert the "CREATED_ON" column to dates
#Convert the ActivityStartDate to a datetime object (currently a string)
#Saving variables
# convert to dict
# merge df1 and df2 into a new dataframe df3
# look at the ingredients
# number confirmed foreign service
# Extract the mean of lenghts:
# from experiments.fairvalue.models.history import History, LabelHistory;
# To create a pandas dataframe from the events list # To create a new dataframe for the objects that containing 'timestamp' variables # To preview the dataframe by first 5 rows.
# we find the rows with missing values with isnull
    '''Function creates a new data frame with date as the index and the sentiment score $        as a column.$     '''$
#test = pd.read_csv("test.csv", header=0, sep=",", encoding="latin_1")
# extract data # read in correct data
# List comprehension for reddit titles
#full outer join on user id
# Compute the fraction of people unser 18 by year
# Train our classifier by giving it 1796 scanned digits!
# true flag (T/F): positive class # predicted flag (T/F): negative class
# Merge df3 from above with the supervisor info in df4
#we have to pick a significance level. So, this level will be 95%:  #The critical value at 95% confidence is:
#Word frequency for all terms (including hashtags and mentions)
# save the file to the output
# Calculate the odds ratio of the variables compare to the baseline
# Solo Analizar el periodo 201701
# Number of trials for old_page: 145274
# the new dataframe drops all misalinged treatment/control groups with landing_page
# Explore state (observation) space
#check
# The number of different types of vote we are dealing with.
# Use SQLAlchemy automap_base() to reflect your tables into classes  # i.e. an existing database into a new model # reflect the tables
# convert 'publish_time' to datetime format # the big 'Y' was key to making the 'to_datetime()' work
### Fit Your Linear Model And Obtain the Results
#check if any null entries exist
# How many stations are available in this dataset?
# store the first paragraph
#Import Random Forest libraries 
#converting all the datetime #segments.st_time.apply(lambda d:datetime.strptime(d, '%m/%d/%y %H:%M'))
#
# for game-state variables we care about the beginning of the PA, since that mostly determines strategy
# Need to join Weather table info to Incidents table by the time of day column
#Rename columns.
# The header keywords are stored as a dict # table.meta
# Simulate binomial of Nold
"""$ Check stop-words$ """$
#drop rows where treatment is not aligned with new_page #drop rows where control is not aligned with old_page
# Sometimes you'll run into an OAuth bug when you run ok.auth(). If so, # uncomment this line of code and run this cell again.
# default inner join
#printin first few lines of the dataframe just loaded
#df_titanic['cabin'] = df_titanic.cabin.astype('category')
# nicer column name
#load first json object
# create a fitted model with three features # print the coefficients
# Check how many countires there are to determine how many dummy variables are needed
#check file is in directory 
# Adding a column for sentiment    
#Results summary
# 2014 created
# this is a gotcha - have to put dropna=False in order to get the NaN's in value_counts # want to keep the NaN's b/c can do math on the series (if convert NaNs to strings cannot do so)
# There are plenty of variables, but I'm only interested in id,dates,comments,shares,likes, so I'm deleting the other columns.
#create a new column called month of year, and convert the type of data to string variables to prep for dummies
# keys should be all matched keys up to date
#checking the results
# Getting some preliminary descriptive statistics for the columns in the df
# Determine the column index for 'Close'
# alternatively
# Merge with library_df
# Create an array of 1s
#'Memphis': 'f995a9bd45d4a867'
### read data from json file and load into dataframe: tweet_json
#dropping records for which the retweet status id is not null that those records that are retweets
#read the dataset from CSV file
#take a lot at all variable and their types
# Drop the zipcode_initial (for privacy reasons)
# Example of category distribution for Park Fercility Name
# What the full data set looks like in a pandas DataFrame. # Each row is an observation (a complaint) # Each column is a feature (a quality of that complaint)
#Copy the previous dataframe and drop DAY column
# reindex
#For RMLSE
#cargo los archivos
# Loading data
# n-new is: 
# To find convert rate for p_old 
#topUserItemDocs.columns=['item_index_corpus','score','user_id','source_item_id','score_weight'] #print topUserItemDocs.shape #print topUserItemDocs.columns
#Test #The date format is correct
# Find and click the full image button
# Set up logistic regression # Calculate results
#check values substituted
# create timestamp index
#Train Model and Predict  
# Use average price dictionary to build Series
#### **Below is the screenshot of the MongoDB structures showing list of dataset**
#info_final.drop('idpostulante', axis = 1, inplace = True)
#read in dataset
#2 Convert the returned JSON object into a Python dictionary.
# Sort the dataframe by date
# describe dataframe
# En esta instancia guardamos una copia de la informacion. Dado que no utilizamos ninguna de las columnas actualmente como index, no guardamos el indice que  genera pandas al importar el csv
#Setting index to state in both dataframes for join 
# Using idxmax() to find out the index of the max value
# search for multiple tags by name and attribute # shorter alternative: if you don't specify a method, it's assumed to be find_all() # even shorter alternative: you can specify the attribute as if it's a parameter
# Simulate conversion rates under null hypothesis
#Number of rows
## To determine the proportion of users converted regardless of the page - which is Pnew
# Build Pie Chart
#check if one duplicate is gone
# Setup 
#Visitors in test and train data
#air["reserve_visitors"].plot(figsize=(20,10), linewidth=5, fontsize=20) #plt.xlabel('reserve_datetime', fontsize=20); #plt.plot(air.index,air["reserve_visitors"])
#group by blocks #drop extra columns
# Find number of times which don't line up
# top beers
# Check that all of the correct rows were removed - this should be 0
### Create the necessary dummy variables #dfnew country
#incorrect values in denominator
#z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new, convert_old]), np.array([n_new, n_old]), alternative='larger')
#Vemos cuantas filas contienen la palabra argentina
# add appropriate prefix and suffix to metadata keys  # get indices for rows where the metadata keywords are present
#checking contents
# We replace all NaN values with 0
# convert to datetime datatype # check timestamp datatype
### Plotting
# series with all NaN
#under the null, we assume there is no different between two groups. Therefore, we don't need to calculate the p in two groups.
#only want tech groups #our category column is now redundant
### train + items + item_categories + shops Join
### Create the necessary dummy variables # Removing Canada and other Unnecessary variables
# proof of concept
## Probability of True Negative
#convert a column into an array variable and print that
#create and display nytimes sentiments into dataframe
##The names of the columns in a DataFrame are accessible via the .columns property
# Get the columns (movieIds) for the current user
#iterator returned over all documents
# R squared.
# winter : 11,12,1
# looking up 'new york' in the airports dataframe # fs refers to 'flightstats code' of the airport
#z-score significance #critical value at 95% confidance
# Find State according to Address1 and City and fill in 
# Exponentiating the coefficient for ab_page
# Total file size (compressed)
#ls ..\input\ #Windows command
# Read data from csv into a Panda dataframe
# In _theory_ in preview dataproc Spark UI is force disabled but history fills the gap, except history server isn't started by default :(
# why tweets contain "felicidades"?
# 9. Print summary statistics for each column for those with an education greater than or equal to 5, grouped by age_cat.
# this is a kind of sql "group by.. order by .. desc limit 10"
#store Countries.csv data in dataframe
# scree plot # generate percentage that each PCA accounts for # generate labels for scree plot
# Initialize PyMongo to work with MongoDBs
# pick your favorite classfication model
# recode column names
# Split our data into random train and test subsets with specific proportion
# Read our Data file with the pandas library # Not every CSV requires an encoding, but be aware this can come up
# define the index of each df subset as the timestamp column
# change date to datetime # sentiment_df.head(100)
#Oh you pesky little one
# plot Series obj
#6d. How many copies of the film Hunchback Impossible exist in the inventory system?
### Create the necessary dummy variables
# fix \n problem in text field # NO DON'T
# we can remove duplicates in multiple ways # removing by index, removing by first or last entry as well as by unique column timestamp
### Create the necessary dummy variables
# your code here
#cols='average_stars'
#Example 5: Specify dot (.) values as missing values
# Drop duplicated user
# Identify possible issues with some columns
# number of times treatment didn't with new_page
# let's remove any with no ATAR and zero marks # now look at those students with no ATAR # plot their distribution
# Avoid a warning that incorrecly appears when performing the assignment below. # Create WordCountBins with 7 bins
#  Now let predict using our test set:
# example of writing description to a file 
# Count the number of stations in the Measurement table
# 7.What was the median daily trading volume during this year?
# now use boolean selection to limit observations to presidential race only
# now we need to norm the age - lets inspect what age units we have
# Number of converted users: 35237 # Proportion of users converted: 12%
#use groupby to find mean of converted group
### top 10 users with Michael Kors mentions
#Removing intermediate columns created
# Internal nature of the object
#Find best features using RFE selector or RandomForest selector, use only those to see accuracy now.
# Take the mean of these two probabilities #apparanetly both are equal 
#We need to define Required = Y mandatory to run experiment
# Information about the repeat user_id
# Let's remove all these extra column names (Year  Jan ...). They all correspond to the index "Year"
### Fitting model and obtaining results
# Reading in the ab data
#engineroute = "C:/Users/Brooke/Downloads/"
#So we need query the rows have the treatment group with old_page  #or the control group with the new_page
#df.AGREEMENT_REQUIRED_NEW_POSITIONS.str.contains("^No, na=) 
# The probability of an individual converting
#old_page_converted = df2[df2['landing_page'] == 'old_page']
## Write pandas dataframe to a Google Sheet Using df2spread: # Insert ID of Google Spreadsheet # Insert Sheet Name
# subset us lon # subset us lat # print dimensions
# subset for only amount
# fit the classifier to the data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# New file from Chenda
#Manchester has so many retweets! Let's see what that tweet is that makes it so good #Looks like it's a video
# write to csv the cluster selection
#Resetting the index to avoid future issues #Dropping unnecessary issues due to save memory and to avoid issues
# find historical data for 2014
#stop_words
# build new df from 'df_amznnews' so as to leave that available for other play.
# make data suitable for classification - categories rather than numbers
# Coin DB # COIN_DB.to_csv('coin_list.csv')
def text_to_seq(text):$     '''Prepare the text for the model'''$
#Inspecting abtest
# Analyze the data in one tweet to see what we require #The key names beginning with an '_' are hidden ones and usually not required, so we'll skip themif not param.startswith("_"):print  "%s : %s\n" % 
# counting missing electricity points # lots of missing data in 2017, this may help explain why R2 dropped when used on 2017 data
# 11. 
# df_2013
# Add a "minute of day" column as a plotting index
# Using obs_diff (actual difference) calculated in part 1 to compare with calculated p_diffs.
# Round
# Initiate, fit and summarize the model
# Displaying results of our model
#Determine how significiant z score is
# Simulate conversion rates under null hypothesis
# Retrieve Latest Date
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Adjust signup_time, purchase_time to gmt offset
# midToto is 13, therefore user can change each layer value from 0 to 12
# Calculating propotion of users that converted using new page
# Use Inspector to print the column names and types for measurement
# take a look at the first two lines
# First let's turn each title into a list of words # Then we'll use the lemmatizer to change plurals to singulars to group them # And rejoin the lists of words to one string for the count vectorizer
# timestamp with just a time # which adds in the current local date
# of unique messages = len
#ab_page column, which is 1 when an individual receives the treatment and 0 if control. #add intercept column
# mean deaths per year:
#display(dflong) #dflong.Date.unique()
## train-test split
# Many ways to select rows in a dataframe that fall within a value range for a column. # Using `Series.between()` is one way.
# Show the first five items in the DataFrame
#import the dataset #show the first 5 rows
# Information of Trip_distance based on credit card usage
# Top 5 countries by violation count
# Creating a randomly made-up Series 
#compute the difference between new page and old page
# What are the most common wine-producing regions? Create a `Series` counting the number of times  # each value occurs in the `region_1` field. This field is often missing data,  # so replace missing values with `Unknown`. Sort in descending order.  
# intersect the two lists (lists are tuples of words and their frequency, we need the first value only)
# Save the data to a dataframe
#joblib.dump(ward, './data/Ward.pkl')
# To talk to a database, pandas needs a connection object passed into its # methods. We'll call this conn. # Now conn is connected to a database file called geo.db
### 5a. # Step 2: create a new DataFrame to show monthly visits by marketing channel and country_code
# this is what infowars shows up as...
# double check
# Probability == 0.1196
##select whether you want excel or csv
# Dump the list in to a text file tweet_json.txt
# Get and display model uid.
# We create a Pandas DataFrame by passing it a dictionary of Pandas Series # We display the DataFrame
# create a json file from response
# get number of non-NaN elements
# Calculate probability of converted in treatment
q = '''$     DROP VIEW customer_2;$ '''$
### Create the necessary dummy variablesdf_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]
# Hint
# in fact this is the same as looking at the group by on count_publications columns and count authors
# Look at start and end of experiment to understand frequency and duration of experiment # Experiment runs over 23 days with clicks every couple of seconds
# check if there are any duplicate user_ids
#pickle.dump(posts_transformed, open("posts.pkl", 'wb'))
# Length of strings
# Build a KNN classifier # k is chosen to be square root of number of training example
# We wish to iterate over the number themselves
# create a column with the result of the analysis: # We display the updated dataframe with the new column:
# plot a scatter graph of 0 and 1 grouped values
#df[df['message'].map(len) < 2] #any empty or very small messages?
#mapping = pd.read_csv("Mapping_csv.csv") #reports = pd.read_csv("AnomalyReportData.csv")
#Reading CSV files in the DIR
# lets create multiple indices - agebins and industry # we can select all adults that had an event related to cosmetics with the  # following:
#ab_page coefficient interpretation
#query data 
# What was the median trading volume during this year?
# donor_class.sort_values('Donation Amount count', ascending=False, inplace=True) # donor_class.reset_index(drop=True, inplace=True) # donor_class.loc[donor_class.index <= top_20]
# Inspecting class representation to determine how much should I balance the # unevenness of their frequency.
# look at first 2 rows
#Save the twitter_archive_master dataframe
# Get average time to resolution as a check 
#split the dataset by gender
# stack the various columns
#get dummies
#comment = pd.read_csv('a.csv', encoding='utf-8')
# We can also apply functions to each column or row of a DataFrame
#Histogram of tweet lengths
# Use DataFrame.describe() to look at descriptive statistics for all columns.
# Unique Lows
# Transform the list of series into one series # Merge the series to free_data dataframe
# What combination of countries and varieties are most common?
#col_labels = ['Date',,'Lang', 'Author','Stars','Title','Review','Reply',]
# use this cell for scratch work # consider using groupby or value_counts() on the 'name' or 'business_id' 
# creates nodes in a graph # "construction phase"
#applying cleaner (now without reforming each title into a string)
# Calculate probability of conversion for new page
#making dataframe from python data dictionary 
#Show the slice of rows spanning september 10th thru 15th, 1998
# note that the mean is much greater than the median due to extreme outliers
#  match colun 66 to 77 : home player ids #match.columns[66 :77]
# aucune trajectoire qui commence et finit par un arret , on va donc essayer le nombre de correspondance avec un arret / taille 
# make predictions 
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
## Fill NA values to avoid Null errors
# Find count of number of results being analyzed.
# Saving to csv to open again with chunks
#delete unknown gender type
### Fit Your Linear Model And Obtain the Results
#print(df3) #print(df_list) #print(df3.head())
# About 74% of active users used the system more than 3.
# if we just want floats in the entries, we do
#Fit the model
# Create "game ID" based on index.
"""$ """$
#== interactive browsing 
# View column headings and index
#query data
#tweetdf[pd.isnull(output)]
# change into datetime format
# Reflect Database into ORM class ===save references
#preserve # Esta celda da el estilo al notebook
# probability that the portfoloi will return more than 5% 
#show result
# -> Being more anxious leads to worse outcomes
# Setup tfidfvectorizer with english stop words # Fit the SOUP data into TfidfVectorizer to get a tfidf matrix
# scrs < 0m / all scrs . 
#keep_cols = ['id', 'created_at', 'display_text_range', 'favorite_count', 'retweet_count', 'full_text']
# Group by, a set of operations on a set of columns.  ## In this case you name the dictionary keys in agg to be the output column of the operation which is value to the key ## argument level in group by, used to group by a level of multi-level index
# If Q4 = no; add n/a to Q4A # assigning each answer to unique Q list
# can sort order using index
### Create the necessary dummy variables
#crime_df['Date'][0]
#ensure you have a full dataset, 100 per account  #len(organize_data) #save data to csv file 
#exponentiate variables attached to explanatory variable
# Instead of leaving it as NaN set it to 0, although that's not possible as event cannot have "0" gathering
#We can now list the counts of records by confidence code
# Clean up the merge results
#Reading Twitter archive into Panda's dataframe
#fmt date for YYYY-MM cohort
# forest_area-50mu	forest_area-50mu_100mu	forest_area-100mu_150mu	forest_area-150mu
# merge with main df
# for each filename, groupby filename
#Aggregate by customer
# user=userid # userid=2605
#checking dtypes
# To check how significant our z-score is # we calculate critical value for our single-sides test, assuming at 95% confidence level : 
#needed to run Scrapy API
# remove cancelled, suspended and live
# until ttk is installed, add parent dir to path
# TASK D ANSWER CHECK
#url for mars table
#For each value of max_wind, find the median max_gust and use that to fill the null values.
# convert the int64 datatype of date to meaningful dates of typedatetime64
#Use pd.to_datetime to convert the column from strings to DateTime objects.
# save the model
# Get the current date and time # Read the time and date now
# Get the times the new_page and treatment don't line up throw: # Get times that treatment and old_page line up
# Create new column with clean text # Add a column nwords 
#drop one of the duplicated user_id rows from the datafra #check the number of rows for the duplicated user_id 
#how many rows do we have with nan in a given column?
# Drop Exits and Desc Column.  To prevent errors in multiple run of cell, errors on drop is ignored
display(HTML('''Account with max following:<br><a href='https://github.com/{}' target="_blank">{}</a>'''.format(max_fwing.login, name)))
# checking for missing values
### Fitting the Linear Model And Obtain the Results
# get mongodb params (using configparser)
# separating new fans and returning fans
# Size is the output shape - set to old sample size to collect array of 1's and 0's.
# warnings.filterwarnings("ignore")
# on the occasion: when using multiple boolean indexes,  # ** make sure you get the parentheses right! **
# target is the handle. # make trump 1 and sanders 0
# Add Rapid Watershed Delineation onto map
# Return descriptive statistics on a single column
#Example 6: Specify question mark (' ?') value as missing values
# Get the shape, # of elements, and # of dimensions
# display unique values with counts for zip_code
#sorting the values in Year column in ascending order of year and making the sort operation permanent using inplace =True option.
# Numeric Columns # scale the columns back to eliminate skew using log function exception - identity column
#BuyingData = BData.parse('Set4-User Buying Behavior')
# Instantiate a Materials collection # Export to "materials.xml"
# Load in the csv data #headlines_df = pd.read_csv("../data/headlines/labeled_headlines.csv", index_col=0, parse_dates=[0])
# David Cameron indices
# This is the column I was interested in creating.  It should contain the same information that I would get by dummying # all of the lead_mgr values.
# slice price DataFrame to show only Germany
#Let's look at what the page.text value looks like before we clean it up with Beautiful Soup.
# Converting column to date format
# examine relevant results
# This code was tested with TensorFlow v1.4 # The import statements will not work with earlier versions, because Keras is in tf.contrib in those versions
#generally greater retweet count with greater rating numerator 
# Data frame
# save train and test sets as csvs
################################################## # Convert string to datetime format ##################################################
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
#filter by year and check that distribution across the boroughs is still comparable
#seeing if any value is missing or not
#Find the user_id repeated in df2
# To count the number of objects in which 'timestamp' earlier than 'create_time'
# set max row and random seed
# checking the easist way of looking the files you have in the directory
# Save data to Excel
# Load the previous query results into a Pandas DataFrame and add the `trip_dates` range as the `date` index
# Print the results of statsmodels
##### Your Code Here #### ###########################
# Use Pandas to calculate the summary statistics for the precipitation data
# Dropping inplace leads to confusion. Confusion leads to fear.  # Fear leads to anger. Anger leads to hate. Hate leads to suffering.
#Calculate the average daily trading volume during this year
# Initialize MGXS Library with OpenMC statepoint data
# From the docs: "Max id returns results with an ID less than (older than) or equal to the # specified ID. 
# top 5 breweries by total beers drank
# highest price values
# what are all of the business quarterly end # dates in 2014?
# Age mean for graduates
# Create online deployment.
#estatisticas basicas # print(feedbacks_stress.loc[7, ['incomodo', 'interesse1', 'interesse2', 'num_video_preferido', 'justificativa']])
# Now lets specify the column name 
#lets plot the time series
# revise "vehicle_type"
# show summary of results
#Access first row
# Double Check all of the correct rows were removed - this should be 0
# print_full(feedbacks_stress) # print_full(feedbacks_stress)
# Step 15: Here we take the original data and reformat it according to the format we just specified in step 14.  # And we peak at the first several values. We're only sending the 'value' data element for anomaly scoring. Is that right?
#Check for missing values
# save the tweets to disk
# analyze validtation between BallBerry simulation and observation data.
# apply the tweet extend function # prepared for run # mancano 900 - 1058
# Access local file
# HITS THE SERVER: downloads data from yahoo for selected EFT
#Outliers
# show topics related to a word
# Task 2
#We'll drop observations that are null across all columns that do have null variables except for 'lat, long, orig-dest'
# quick check that key is defined
# The probability that an individual received the new page #df2[df2['landing_page'] == 'new_page'].mean()
#Conversion rate
#First convert mapInfo to a string #see what the split method does
# your code here
# Dummy subreddit
#target
# investigate on the position and properties of detected fixations
# get 8-16 week forecast new patients # keep only date in index, drop time
# Normalize creates normal daily times and will set the default time for each day as midnight.
# get unique country list.
# look at all models and topics learned in class
# Check changes
# Load the query results into a Pandas DataFrame and set the index to the date column.
#### Look at DEPT to make sure it has gone up, it has!
# Graficamos
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
# most active stations. # stations and observation counts in descending order
# Call method 'conf_matrix' from class "Classifier"
#Remove bad sensor data, where a value is >4 degrees different from the 2-hour median temp. 
# add back metadata for each order # make an additional column with just the year
# look for missing names
# drop unnamed column
#file_time = str(current_time.time())+'_'+str(current_time.date())
# Simulating n_new 1's and 0's with P_new probability of success
# assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array
#Finds the maximum value for Discount %'s within a given band.
# Insert 1 or 0 if the topics appear in the campaign
# Count tweets per time zone for the top 10 time zones
# merage tables on species id # merged_df = pd.merge(left=surveys_df, right=species_df, left_on='species_id', right_on='species_id')
#creating decision tree target and features using numpy arrays: target, features_one
# explore odometer_km values
# Check out the actual counts within each wear day bin
# compare results to original dataset convert rates
# setting an index in pandas is quite easy - lets use AgeBins to start # lets select all rows that are related to children
query_term = 'Disaster risk assessment'$ query = """SELECT distinct e.pullquery, e.pullby, a.optionalid01 as doi, c.title, d.abstracttext,c.journalname,c.pubyear, c.publicationdate from public.datapull_detail as a inner join public.datapull_uniqueid as b on a.pullsource = b.pullsource AND a.associatedid = b.associatedid inner join public.datapull_title as c on b.uniqueid = c.uniqueid inner join public.datapull_text as d on b.uniqueid = d.uniqueid inner join public.datapull_id as e on e.pullid = a.pullid where e.pullquery = '{}' limit 25""".format(query_term)$
# Show a sample of the data
# or we can aggregate the data according to criteria # $sample is a mongo function
# converting the timestamp column # summarizing the converted timestamp column
# calculating or setting the year with the most commits to Linux
'''Let us normalize time_spent'''$
# Find duplicated user_id
# people usually complain about potholes on Tuesday
# Count of unique values for each column
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Convert API response into a dictionary 
#dimension #shape
# df2.nunique()
#rename columns #print head of load
# The number of new_page entries in the dataset
#checking if duplicated column is deleted or not 
# Print model.summary.
#Method 1 #sum/len
# show available waterbodies
# 10-fold cross-validation with logistic regression
# for TFIDF DTM
# set a common index, then merge this country column into df2
#First, let's download it from quandle.  
#input_nodes_DF = nodes_table(nodes_file, 'FridayHarborBiophysics')
# make dummies out of categorical variables
# Add two new columns to dataframe
# calculating the convert rate for the new page
#'San Diego': 'a592bd6ceb1319f7'
# Create a database session object
# Split the Data to avoid Leakage #splitting into training and test sets
#write the above dataframe, with cleaned up lists for contributors, affiliations, publishers, and tags, to a new excel doc # testing.to_excel('SHARE_cleaned_lists.xlsx', index=False)
# How many movies do not have a homepage?
# lowest price values
# save the model
# Calculate total days covered by the tweet file to determine criteria for ignoring tweets
# Collect data from FSE for AFX_X for 2017 with API call
#Row can be used to specify the name of the columns with a map() transformation
# calculate the ratio of five-star to one-star for each token
#Let's explore 'price' column
#Concatenates Month Name and Year together to generate our y-value for a plot. #Profit will be our x-value. #We also have to convert the Year value to a string.
# Export CSV
# find unique filenames, then loop through all unique filenames, if part of a rename,  # append to the dict associated with that filename. if entries in that dictionary, if a rename to one of those, associate with that
# Number of unique movies in the ratings data file
# display(flightv1_1.select("flight_leg1.carrierSummary").show(100, truncate=False))
#### Test ####
#feature_set  # uncomment feature_set in order to illustrate.   May require notebook reboot depending on memory. 
#Use the REST API for a static search #Our example finds recent tweets using the hashtag #datascience #tweet_list = api.search(q='#%23datascience') #%23 is used to specify '#'
# Use Pandas to calculate the summary statistics for the precipitation data
# Use Pandas to calcualte the summary statistics for the precipitation data
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#check for the single row of data from the station table
# Which features play the biggest role in predicting campaign success:
# read the csv and assign to the variable countries
# TASK G ANSWER CHECK
#Connect to database
#sample query to verify key works
# We can view all of the classes that automap found
# 2356 unique tweet ids # has 2075 unique
# process missing value
#Create and view new df
# Get the shape and # of elements. shape() gives the number of rows (axis 0) # and the number of columns (axis 1). These axis indices are important later.
# of unique user ids
# load data from CSV
# groupby in conjunction with aggregation functions like mean: # fast way to summarize data; a hierarchical index is by default  # generated
# Find the max and min of the 'Total Number of Rides' to plot on x axis
# Now make a masked array to find movies to recommend # values are the movie ids, mask is the movies the most # similar user liked.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Turn everything into relative percentages of the whole column (i.e. normalization)
#Select first row from stations
# Look at total events per hour of the day # yay no need to group - looks good # noon #1
# Set up access credentials
# Rural cities average fare
### Dataframe with location of the project (doesn't always coincide with 'country' of project creator) ### Grab 'country', 'name' (city),  and 'state' (state)
# get defects - had to read in excel as NCR tables not working for moment.
# Take a look at rows with lowercase values in 'name' field and the text field has value 'name is' 
#Change the order of columns. ID will become the first column.
#Only grab specific columns from the CSV that we want to work with.
# Print out the shape of the dataframe to show the number of rows in a (x, y) format
# we can find proportion of users converted by taking mean since values are 1 and 0
#'Louisville': '095534ad3107e0e6'
# print shape of Match df (25979 rows) and first 3 rows
# How significant is our z-score
# Export CSV
# Output File 
# What are the most active stations? # List the stations and the counts in descending order.
#add it to the csv
#absolutely useless, must be a better format
# save the hashtags to disk
# Make predictions on test data using the Transformer.transform() method.
# Predict stock prices
# No null values, but from a visual check, there are definitely blank strings  # in the region and country series. We should check for those as well.
# best/worst styles, minimum of 5 beers drank
# export 
#show the total number of rows
#Dallas': '18810aa5b43e76c7'
# Reflect Database into ORM class
# 13. Print rows 100 to 110 of only the first 3 columns in free1 using only indices.
# Find the transacations that fit the criteria of trips that terminated at one of the NYC airports, identified  # by the RateCodeID values equal to 2 or 3 or 4, each representing the airports JFK, Newark, Nassau or Westchester
#df = pd.read_csv('/home/bmcfee/data/vggish-likelihoods-a226b3-maxagg10.csv.gz', index_col=0)
#Convert the series into a dataframe
#Drop the 'white' as well as the original 'raceeth' column. Note: all those rows where all columns are=0 correspond to white=1
# Plot the data using the plot method
# In the Data Science Experience you can prefice commands with a ! to run shell commands. # Here we remove any files with the name of the file we are going to download and # then download the file
# See if you can select the high temperature for January 18
#propiedades entre 125 y 150 metros cuadrados
#import required modules from the scikit-learn metrics package
#to_datetime
# Create histogram here # YOUR CODE HERE #raise NotImplementedError()
# Create another column called 'default' to help with EDA
"""$ Take a small sample for plotting$ """$
# let's use 'StatusDate' to estimate time to request resolution # however, first need to convert to appropriate format to compare with 'CreatedDate' # compute resolution time in days for each request
# check inserted records
# plotting the 20 most frequently used tags
# Import data
# We extract the mean of lengths:
# Remove stop words from "words"
# The index values are DatetimeIndex objects. # The names of columns in the data frame
#repeat (e) for n_old # old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old]) # preview to confirm simulation
# Use Pandas to calcualte the summary statistics for the precipitation data
######################### Combine Data Frames ############################## # To flatten after combined everything. 
# Select only the date and prcp values.
# create a Series from a list with explicit index
# Calculate the number of conversions for each page
#search best parameters #%debug
# Importing the data
#How many times each user log-in in 7 days period.eg. user id 2 used the system 3 times starting the day 2014-02-06,
# create a list containing Pearson's correlation between 'overall_rating' with each column in cols
# wall time to train model
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Retain only traffic reading at 6AM and 12PM
#Load the dataset into a dataframe 
# Set negative values for Tip_amount to 0
# We remove the store 2 and store 1 rows # we display the modified DataFrame
# Prepare ford date filtering. Adding an EST time column since chat hosted by people in that time zone.
# plot histogram
# Use your previous function `calc_temps` to calculate the tmin, tavg, and tmax  # for your trip using the previous year's data for those same dates.
#Use the read_html function in Pandas to automatically scrape tabular data of mars facts from the page.
# Simulate conversion rates under null hypothesis
#  Select the last 12 months of precipitation data: # Grabs the last date entry in the data table.   
# Which rows of df['raw'] contain 'xxxx-xx-xx'?
# We was numpy arithmetic ops
# What are the most active stations? # List the stations and the counts in descending order.
# the row numbers specified can be retrieved using loc method which uses index values or row numbers as input
# to get the last 12 months of data, last date - 365
# Counting the no. commits per year # Listing the first rows
# Validate the zip codes # Manually fix the zip codes for the 7 stations with 4 digit zip codes by checking gmaps.geocode(station 'station, New York, NY')
# dang, so fast. 
#number of times the landing_page and group column didn't line up
# largest decrease in gamma / best to stop the logspace around 300, can do 10 steps, accuracy vs. speed
# Loading data
# look for seasonal patterns using window=52 - therapists
# Importing the built-in logging module
### Create the necessary dummy variables
### Fit Your Linear Model And Obtain the Results # Instantiate the model # Fit the model
# alternate method to find number of converted users 
# Create a 1-group structure # Create a new MGXS Library on the coarse 1-group structure
#The number of times the new_page and treatment don't line up. #treatment_new_pg = df[df(['group']=='treatment') & df(['landing_page']=='new_page')] #control_old_page = df[df(['group']=='control') & df(['landing_page']=='old_page')]
#Incluimos los campos del tipo Object
# Basic search for '#Australia'
# Compute the difference between Age at Booking and current age
#Number of rows and features
# Show all chars that are predicted to belong to the amount
#Keep in mind twitter_archive is original. Our twitter_archive_clean hasn't already had these #duplicates.
# We can view all of the classes that automap found
# Reflect hawaii database table station into its ORM class
# Show the dimension of the loaded data
#let's look at the duplicated values
# How many stations are available in this dataset? # measurementDF["station"].nunique()  <- this query is looking at a one year subset of the data
# For each user, each possible pair of pair transactions
# return the dtypes
# This command should output closing values for our three # stocks from March 22nd to March 30th.
# Summary statistics of overall data-columns (where the data-type can make such statistics to happen)
#simulating from the NULL
# Calculate n_new
# Size of old page samples
# Load the previous query results into a Pandas DataFrame and add the `trip_dates` range as the `date` index
#creating derived metrics/ features #converting the date columns from string to date format #will use it to derive the duration of the project
# Query for last 12 months of precipitation
# Check missing patterns
#X_train = sparse.hstack([train[simple_features], X_train_feature_counts, X_train_desc_vect]).tocsr() #X_test = sparse.hstack([test[simple_features], X_test_feature_counts, X_test_desc_vect]).tocsr()
#since under the null hypothesis,true success rates are equal to the converte success rate for both p new and p old,
#Downloads prehosted dataset from Dropbox
# Samples obtained per tectonic setting
# Create logit_countries object # Fit
# Save final version here. #json.dump(fp, youtube_urls, separators=(',', ':'))
# Finding unique countries in dataset
# check the structure of Watch Event
#tip_sample.to_csv('Documents/COMP47350/DataAnalyticsProject/Code/yelp_tips_prepared.csv')
#get the sum of a column
#Convert rate for  pold  under the null
# Convert notebook to HTML # Upload the HTML # Upload the Actual Notebook
# removing un-activated users 
# proportion of users converted in the 'dataset is 0.12126269856564711
#New Col added
# Set legendary False = 0, True = 1
# Merging data sets on bill_id column # Only introduction of the bill carries sponsorship data, so previous versions were dropped as duplicates.
# rows
# cvec_1 top 10 words
# Create ARIMA model
# overview of the class label column
# NA imputation
#since values are 0 and 1 we can take mean to find proportions #print(df.converted.sum()/df_length)
# Custom Python Library Imports
#checking and displating the number of unique users
# calculate weeks between apply and submit
# use the model to make predictions on a new value
# Randomly choose from [0,1], with probability [(1-p_old), p_old] and size n_old
#companies[companies['name'] == "Morgan Stanley"]
#'St. Petersburg': '5d231ed8656fcf5a'
# plotting p value
#Non Zero Revenue
# Instantiate a Cell # Register bounding Surfaces with the Cell # Fill the Cell with the Material
# Get a row
# 200 X 200
#Which categories are most popular?
# What are the most active stations? # List the stations and the counts in descending order. # measurementDF["station"].value_counts()  <- this query is looking at a one year subset of the data
#Clean the df
#selecting by broadcasting: this selects everty 1000th row from my data, stating w row 0
# rename heading columns
#Printing the schema of the dataframe
#It is easy to do a plot on this:
# Summary of results
# Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters of a Gaussian mixture distribution. # fit() method will find the best weights by training the algorithm using the data and labels provided
# Isolates weekdays
# The url request must start 'https://www.quandl.com/api/v3/datasets/'  # then 'FSE' is for Frankfurt Stock Exchange, then the ticker, 'AFX_X' then .json? then any filters.
# Create number purchases per product feature
# boolean indexing: with .loc, we can input a boolean series  # as the row index; no need to take the values (in contrast to .iloc)
# Averaged predictions
#df_train = df_train.drop(columns=columns[19]) #df_test = df_test.drop(columns=columns[19])
# predict against the test set # preds = aml.predict(test) # or:
# pickle pulled data.
# From a dictionary so keys are the index and get sorded by keys
# create a new table which encorporates the necessary values for the bubble chart
#Display the row's columns and data in dictionary format
## Sorting the data # ascending=False - get the sort in desc order
# join all messages by the same candidate
#CHECKING THE SHAPE
# Training data
#Creating another dataframe here... #We can use this same table for both parts.
#Cluster 15 #General topic is Fake News
# Now, we're ready to start streaming!  We'll look for recent tweets which use the word "data". # You can pause the display of tweets by interrupting the Python kernel (use the menu bar at the top)
# evaluate the chunker
# set input shape for ANN
# users can can change their login name so there are duplicate user_id's  # or login name can be from the user referenced as an organization they're part of
# group by 'group' # the means under 'converted' will show the probability of conversion for the 'control' and 'treatment' groups
# Split Data # To avoid Data leakage split the data
# first print the Dataset object to see what we've got # close the Dataset.
#Plot a histogram of the p_diffs
#Regardless of the page, compute the converted success rate
# Calculating in and out degrees
# Fit toLinear Model And Obtain the Results
# load the 311 data directly from the WPRDC
# Loading in the pandas module. # Reading in the log file # Printing out the first 5 rows
### Create the necessary dummy variables
#count
# df_session_dummies.head(n=2) # df_session_dummies_drop=df_session_dummies.drop(['created_at','value',],1) # df_session_dummies.head()
# box-plot of monthly returns for all stock in one plot.
# Return the duplicated user_id
# Convert data types to int
# RE.MATCH
# Likes vs retweets visualization:
#creating url for year 2017 by changing the start+_date and end_date parameter of query string. #Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 
#converstion rate
# basic cleaning delete constant columns 
#Set the index correctly
# print a single tweet json for reference
# Pipeline class # Create and return an empty Pipeline
# ['table_name', 'trip', 'stay_days', 'start_date', 'company', 'dep_time_local', 'stop_info', 'duration'] # groupby(flight2.trip, flight2.stay_days, flight.dep_time_group).agg(func.mean('price')).show() # flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')).agg(F.mean('price_will_drop_num')).show()
# get_dummies function gives us 1s and 0s
#propiedades entre 125 y 150 metros cuadrados #Hay una sola casa para la distancia entre 1500 y 2000mts. No vamos a tener en cuenta este dato por falta de propiedades.
#getting mars tweet
# Rename the odometer column to odometer_km
# Fill in the values from this weeks pace
#Describe records before 1980 and after 1984 (using index slicing, as above)
# keep only top 5 likers' data
#Use nunique on the column to list the number of unique values
# import modules and set up logging
#use real_diff obtained in Part I probability question 4(c) and 4(b)
# mean and standard deviaton of my ratings
# 100 X 100
# Collecting the brands with over 5 % of the ads
# Make a copy of twitter_archive_enhanced
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# freq='M' uses last day of month by default
# list all scores that aren't null
# Query to retrieve the last 12 months of Temperature Data # Use the start date and end date calculated before.
# make another dataframe about the supervisor for each group
# Requirement #1: Add your code here # updated by Huang, Silin ID: A20383068
# note we just counted the length of the "legs" output, it contains the details of the actual route # here is what is included in a "leg"
# Check the dataframe
#Given the individual received the treatment page, what is the probability of converting?
#get count by month of each term
#eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality, failed to converge # Assign each to an attribute in your network #nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')
# Turn DateCreated into a timestamp
#View first 5 rows
#Convert Categorical features to numerical values # let check gender
# copy a new df to this part
# Users pbls
#Get my document sample
# Start a spark session
# In the column 'raw', extract the word in the strings
# predict on test set
#No Clue
# Replace 'None' with Nan
# store the number of times each token appears across each class
#part 2- print row from result
#create arima model
# Let say you want to x in index 1
# write your code here
# alternatively
# print a sample tree in tuple format
# Read the JSON file into a list of dictionaries
# Checking if there are duplicate user_id
# Drop duplicate # Confirm no more duplicates
# get the top 1000
# Number of Different (Unique) Names
# Convert to ndarray (TWO ways) # first way
# Count hashtags only
# Spot check example # Fliter data where name is one of the repeate values
# count the number of listings with cars that were registered  # outside of the 1900 - 2016 interval
# Convert into log2(tpm+0.001)
# single out first 19 digits of the timestamp
# Probability of control group converting
# plot fixation counts
#in descending order
# Create the inspector and connect it to the engine; this is a step to check what's going on # Collect the names of tables within the database #NOTE - I accidentally created two extra tables 'measurements_df' and 'stations_df'; ignore these
# Find the div that will allow you to retrieve the latest mars images
# View dataset dimensions
#let's Mis breed will be 1 and pure breeds will be 0
# We have to enter the number of successes for the count parameter, and number of trials for the nobs parameter.
#061 NYC, 081 QN, 085 SI, 047 BK, 005 BX
#convert type to date type
# Plot ACF of first difference of each series
# Use statsmodels to verify results
# print both parts of team df
# later on the first row will be an issue; 1.0 isn't a valid number
#data_archie[data_archie['user_id'].isnull()].head(5)
# create new column 'figure_density' as difference between grant date and filing date # show summary statistics for figure density
# Import the model we are using # Instantiate model with 1000 decision trees # Train the model on training data
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
# print(len(new_page_converted)) #code to check values
# Create x, where x the 'scores' column's values as floats
#startdate = "2018-01-02"
#Example 7: Import File from URL
# turn tweet_id into string type
#create the url using database_code=FSE, dataset_code=AFX_X, start_date=2017-01-01 and end_date=2017-12-31 #use string formatter to format the API key
#Quandl_DF['Date_series'] = pd.to_datetime(Quandl_DF['Date'])
# Optional: Create an output folder to store your predictions
#Get the subset rows excluding 5,8, 12, 23, and 56
# get artists whose name contains "Aerosmith" # show two examples
# Mongo is but collection in database of life....
# Can perform vectorize operations on it
#counting rows where treatment and new page don't line up
#hpg
#setting up date
#converts to dataframe
# values are a numpy array
# Design a query to calculate the total number of stations.
# analyze validtation between BallBerry simulation and observation data.
#Creating the average speed variable #Looking at the summary statistics of the Average speed
# Extract the last name of each entry
# Save references to each table
# confirming that mask worked
#cassession.tableinfo() #cassession.columninfo('nn_scored')
# Tells how significant z_score is:
# 'itos': 'int-to-string'
### Fit Your Linear Model And Obtain the Results
#number of times treatment group member does not receive the new page
# start_mask
# returns data types for each attribute of the dataset
# What are the different species of Iris and how many rows are associated with each
#Read in clean stations csv file
# Start the experiment run.
### use the ratio of each users unqiue post on Michael Kors over the number of unqiue post on Kate Spade 
# remove entries created after October 31, 2017
# Create the empty table that will contain the sidewalk linestrings.$ sql = """CREATE TABLE approx_medial (id bigserial, LINESTRING geometry); """$
### Test ####
# And visualizing...
# check data schema
# sort by column BG
# Use tally arithmetic to compute the difference between the total, absorption and scattering # The difference is a derived tally which can generate Pandas DataFrames for inspection
## Reading the conn logs for our analysis
# Pull up the same value in the "cleaned" dataset
# And so, it can be used with .loc to get the index by its name:
# 2
# how many values less than 6?
# Sorting the Dataframe based on DealID
# Explore our target variable.
# There seem to be 78 community areas, 0 through 77
# Total numbers of recorded data = 1494926
### Take a look at the first 5 rows of the data
# We want to extract 5 topics: # We fit the model to the textual data: 
# train_df.clicked.isnull().sum() # np.unique(train_df['clicked'])
### Create the necessary dummy variables
# option 1 (finds unique columns, duplicates rows)
# Plot p_diffs as well as the observed difference (red line)
# convert rate for Pold  under the null # proportion of users who converted to old_page #print('The convert rate for old_page under the null is: {}.'.format(p_old))
#doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED)
#We will start first by uploading json and pandas using the commands below:
#Vemos las cantidad de registros por cada columna
# Save the dataframe in a csv # Repeat the process for all tweets
##Concat the new cols to the original dfs
# load CSV
# data munging # split language and region
# We need to insert dummy variable inplace of country names
# Show Figure
# convert crfa_f to numeric value
# get sum of interceptions by game day
# I can find out if Katie Hopkins has tweeted more than one time:
# call download_executable_lubuntu_hs method to download SUMMA Executable from HS, and unzip.
# A:
# Write an aggregation function that finds the mean price per minute and the total volume per minute - output this to a new DF
#Now use the optimal model's parameters to run random forest
#convert timestamp to date time format
# return a frame eliminating rows with NaN values
### Fit Your Linear Model And Obtain the Results
# Bitwise operators
# df_graph.
# pivot dataframe for plotting
# I realized through working with this data in the past that Per Seat Price they give is not always correct.
# Random forest score:
#Switch to dict
# tokenize all the pdf text
# Probably want to create a datetime object for March 31st 2017 # create a logical column with the datetime object and a timedelta object
# inspect dataset using head()
# Column >> result #display head again
#fitting the model #result summary
#print sentence.index('west') #it will throw an error
#put data file 4 into a pandas df #parent node is 'page' #print first 5 rows
# Number of times new_page and treatment don't line up
#cust_data['Dups']=cust_data.duplicated() # Creates a boolean series to indicate which rows have dups
#answer
#double check
# Pull the data from remote location: here is my github "Data" folder # Note: if you read the http files than read the file content and not the html because you'll get issue and not knowing where comes from. # docs source: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
# This is to check that I am calling tyopes and printing them out correctly
#Normalized the dataframe to find any correlations
# Let's see how data distributed along with month of year
# Use Pandas to calcualte the summary statistics for the precipitation data
# Groupby average compound sentiments by Symbol
# explore dict output
# add intercept # add ab_page column as the treatment column being 1
# Note: clipped at 180 # Note: seasonality corresponds to weekday and weekend effect
# output tells you index (row) 1 is Tuesday
# Number of unique entries in "price" column
# Query newly created table
#creating a spark context file and then reading the log file using textfile() method.
# Print the value counts for 'Borough'
# freq='BM' stands for Businees end day of month # the last BM of each month in 2017
#All the tweets are combined first
# The Julia array will have its dimensions reversed when passed to a Python function
# Predict
# Visualize results
# Check duplication
# Line equation function # test 0.2634
# Divide each number by each countries annual maximum
#First five rows are viewed
# Take a look at the values in each series.
#Find  pnewpnew  -  poldpold  for your simulated values from part (e) and (f).
# Group by twitter account
#s_new_rates = df_borrowers_time.groupby([lambda x: pd.datetime(x.year, x.month, 0, 0)])['id'].count() #s_new_rates = pd.DataFrame({s_new_rates})
#remove all rows where treatment is not aligned with new_page or control is not aligned with old_page  
# same data as before
# OutliersDetection is a subclass of DataExploration
# Export to CSV
#Lists the unique 'Segment' values within the table.
#Complete the inner join of the school and students dataframes to reutrn a combined (merged) dataframe. #Drop the School ID and Student ID columns to reduce the columns in the merged dataframe for readability
#type(df_concat)
# Let's create a backup here just in case!
#take only investors that have been active in the past year and have made more than 5 investments
# Show feature properties
# create CustomBusinessDay object based on the federal calendar # now calc next business day from 2014-8-29
# path length for each branch point
# Get a number for count(*) by using scalar()
## create a list of top 25 users from 'users' DF
#274 is what our value of "missing" is for well age. Excluded that here
# Apply the wrangling and cleaning function
# Converting my station_temp_tobs to a dataframe.
# only 31% New York applicants waited that long
# Find users converted propertion
#SGD classifier attempt
#merge this into the dtm_tfidf_df #view result
# Saving the dataset that has contains no duplicates or records with missing or mismatched values # Read the dataset again
# distribution plot of X test
# inspect Meta Data
# model from previous logistic regression
# Create the inspector and connect it to the engine # Collect the names of tables within the database
# Some wines do not list a price. How often does this occur?  # Generate a `Series`that, for each review in the dataset, states  # whether the wine reviewed has a null `price`
# Load the data to check the structure and columns
# are classes balanced? #86.31% will be benchmark for our models
# To find the latest event time for each device
# Baseline score:
# Calculate difference
# dropping some objects base on its index
# choose the columns we need 
# pytz
#Convert DateTime to integer month and append to list of test features
# print min and max date
# Author: Warren Weckesser
# Fit the model
# Using scipy to test significance of our z_score
#print(highlight(json.dumps(jscores, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter()))  # remove the comment from the beginning of the line above to see the full results # Convert to a data frame and show all products owned by this client
# tensorboard = keras.callbacks.TensorBoard(log_dir="../logs", write_graph=True, write_images=True, histogram_freq=1)
# Reading data from the rest of csv files.
# preview the data
#Wrapped with pd.DataFrame() for pretty printing
# Fit pipeline 
#calculating and printing the p and z values
# Save the workbook.
#Plot a station's incremental entries by date #df2[df2['TUPLEKEY']==("A002", "R051", "02-00-00", "LEXINGTON AVE")].groupby(['DATE']).sum().plot(figsize=(10,3))
# drop row of index 1899
#Clean up the the mismatched rows #Delete the mismatched rows
# let's use the model we created to make a prediction:
#sort lastest_consensus_created_date to find the latest ones.
# Create hourly data frame
### Create the necessary dummy variables for the country
### Fit Your Linear Model And Obtain the Results
# Create tf-idf model from corpus # num_nnz is the number of tokens # num_docs here is the number of users which is 10000
# load the df_events file into a dataframe
# value counts for individual columns using the original 
# Rename a single column
# defind observation data
# Use a colour-blind friendly colormap, "Paired".
# drop all rows with unknown coordinates or unknown permit issue dates
# reduce graph data size
# Create a new year column
# Multiplication
#You can load json, text, and other files using sqlContext. #Unlike an RDD, this will attempt to create a schema around the data. #Self describing data works really well for this.
# adding prefix AD_ to column names
# Check if the row I chose was removed
# Requirement #1: Add your code here
# Display of first 10 elements from dataframe:
#df_countries.head() #df_countries.shape
#Find outcritical value at 95% confidence 
#  ENCOURAGE RE-RUNS of this cell #  showing one-year of simulated price histories... #  ... like playing with a kaleidoscope.
##Rows can be explicitly accessed via index label using the .loc property.
# Only Select Milk Chilling Units
#create a new data set adding the old dataset using join
#highlighting the number of rows and columns
### Create the necessary dummy variables
#ignore_index means the indices will not be reset after each append
# get the bitcoin prices
#groupby state and then take the mean of labor force in 2013
# Take mean of two probabilities
# Here we will create our regression model with Canada and Old_page as our baselines
# word2vec expects a list of lists. # Using punkt tokenizer for better splitting of a paragraph into sentences. #nltk.download('popular')
# specify data directory and file # read csv file of classifications # create a project dataframe that contains the total number of classifications per project
# this is text processing required for topic modeling with Gensim
# (b)
# Check the predicyion confidence values
# Load the training set # Group the taxonomic classes
### This is data from wikipedia3 archive in data folder
# Print records in test.csv
#Simply use the DataFrame Object to create the table.
# There are 2 '68' columns. # So, adding value to original '68' column and dropping 2nd col
#lastly we move on to df_tweet_json. 
# Probability == 0.1204
#remove new lines
# To check the size of the dataset
## total.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/target_winter2017.csv',index=False)
#PLotting histogram for the differnces of conversion values generated using simulations
# the number of reports from the most active station# the nu 
# No missing values
# Use Pandas to print the summary statistics for the precipitation data.
#We notice an the max date of 2044-06-01. This date is in the future and seems to be an anamoly.  #Let us check # Bingo!! This is defintely an outlier. It only has one value. Let us drop it
# Sort the dataframe by date
#converted p_new
# Printing the content of git_log_excerpt.csv # ... YOUR CODE FOR TASK 1 ...
# Period
#disconnect to the sql database
#Checking that only presidential race ads are left in the dataset
# Remove one of the rows with a duplicate user_id, reset index and keep dataframe as df2
# Create the boxplot # Display the plot
#Compute annual Nitrate concentration
# names of the 4 groups
# None and a are not dog names 
# Compute neutrons produced per absorption (eta) using tally arithmetic
#exploring the categories csv file
#https://radimrehurek.com/gensim/tutorial.html # this makes process visible
#Extract test-id from dataframe for later use
# Double Checking if all of the correct rows were removed - this should come out to be 0
# import modules & set up logging
### Create the necessary dummy variables
#Get rid of columns we won't use in logistic regression
# read the data # display the data # show the number of data points in trip.csv
# read the json data into a pandas dataframe # set column 'created_at' to the index # convert timestamp index to a datetime index
# Simulating n_new 1's and 0's with P_old probability of success
# Save the query results as a Pandas DataFrame and set the index to the date column
# Total dates
#lda_tfidf.print_topics(10)
# Calculate basic statistics for y by group within w.
# Add to Pandas Dataframe
# df_count = str(df.converted).count('1') # print(df_count)
# Use Pandas Plotting with Matplotlib to plot the data
#== Filling missing value 
#prcp_summ_stats
# rearrange column order to group releveant columns together
# Split the Data to avoid Leakage #splitting into training and test sets
# Set some simple earthquake related keywords:  # Collect 100 tweets using the keywords:
#green taxi data before cleaning
# convert to lower-case
# you can convert date time index back to period index
#checking for duplicate observations 
#Create INCR_ENTRIES, which is the incremental entries per time period grouped by the TUPLEKEY
# R-Blogger news #print(type(soup)) #print (soup.prettify)
# Import BeautifulSoup into your workspace # Initialize the BeautifulSoup object on a single movie review     
# Create a GeoPandas GeoDataFrame from Sites DataFrame
# z score
# Calculating null accuracy
#del salesdec['Platform']
# Number of people that does not have a Bio line
## extrapolation, backfilling ('bfill') by day ('D')
# Cargamos hoja de calculo en un dataframe
### daily post by gender & brand
# We noticed the model search turned out as our best max_depth to be 14.
# dataframe is like a powerfull spreadsheet # We print the type of items to see that it is a dictionary
### Create the necessary dummy variables
# List of countries
# We can specify the axis along which the result will be concat
#summarizing the results
# add intercept # add dummies for landing page and put them in ab_page column
# Summary statistics for the ratings data set
# What are the most active stations? # List the stations and the counts in descending order.
# Determine the probability of an individual converting regardless of landing page by finding the mean of the converted column
# get the overall sentiment per media and store it in a dataframe
# Data key value is a dictionary with a children key -- where all the post information is 
#filled in nulls for LinkedAccountId, may need to do entire file later, not sure why payer doesn't need it
# Officer citations are wayyy skewed
#Again, I don't have enough data to see the whole year...
#Confirm it looks good by viewing the first 5 records using the "head()" function
# Reflect Database into ORM class
#scipy version 0.17.1
#first cohort
#check for missing values
#big gap between 75% of rating numerator and max value of rating numerator 
# Calculate the exponential of all elements in the input array.
# check whether the author of each post in 'train' is in 'smart_authors'
# leadsdf['simpleDate'] = pd.to_datetime(leadsdf["lastEnteredOn"],format="%Y-%m-%d") # leadsdf['simpleDate'] = leadsdf['simpleDate'].dt.strftime('%Y-%m-%d')
#Load the data  #display top rows
# check index
# Eliminar los registros que tengan nulos en la columna "created_time"
# check if still missing 
## To determine missing values in the dataset.
#Frequency_score is float data type, but should be integer
# check if any duplicates exist
# Run an aggregate operation # Gives us a general scale of orbital periods in days that each method is sensitive to
# thats games without vectors, but with labels
#Imports
# probability that an individual received the new page #print('The probability that an individual received the new page is: {}.'.format(df2_new_page))
# Merge the data # Class method?
#Check and plot the 50 first predictions
# the number of reports from the most active station
# retweeted_status_id can be removed as if it exists then it is a retweet, drop the rows
#delete duplicate receipts 80874 duplicate receipts
#dftemp = df1.where(df1['Area'] == 'Iceland') ...... where command keeps array size same so not used here
# Set a starting and ending time. #end_t = '2017-12-31 23:59:59'
#Show second row
#sort two dataframes
# Reduce words to their stems
# Sort the morning traffic and observe a cutoff point to classify the extreme value as erroneous
# Looking for unrealistic values
#myIP[3] in low
# Display coefficients for different features
#Solo dejo las propiedades que estan a menos de 600mts
# Assign the Measurement class to a variable called station
# Find values greater than the upper control limit.
# find the test statistic and p-value
### plot total converstation for each brand over the entire time frame
# Convert the string to a datetime object
# convert rate for Pnew  under the null # proportion of users who converted to new_page #print('The convert rate for new_page under the null is: {}.'.format(p_new))
#quick data type check
#Run this command to debug if Service failed
# 2. Specifying which columns to merge on (if keys have different names in datasets) # still an inner join!
# how many unique authors do we have?
# Probability of user converting
# looking at the first five rows of the dataframe
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #3. Change the column name from any of the above file. #Rename the columns Indicator to Indicaator ID
#I want to remove major outliers from the data; trips longer than 6 hours. This will remove less than 0.5% of the data.
#create column with the number of positive words
# Hacemos un display del dataframe:
#Read csv file into a DataFrame #reviewing dataframe and loaded data
# The following shows the time as 1 hour 30 minutes 5 seconds and 2 microseconds
#checking the average conversion rate in treatment group
#store tables
# iloc allows indexing and slicing with implicit index
#Max and min time
# instantitate an empty list called data to store the result proxy
# Define sentiment bins and groups # Add column "Sentiment group" to tweets data in sentiment_df
# Compute the average age ate booking by gender
# YOUR CODE HERE #raise NotImplementedError()
#checking that we are okay in terms of time sorting
# Probability of conversion regardless of page.
#convert dictionary to a dataframe.  #print(precip_data_df.count())
#dropping null TEMPs and TEMP_1s in the next two cells #...3600 records are removed
# Implement dummy variable and columns
# first, plot the observed data # then, plot the least squares line
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# Finding proportion of users converted by taking mean
#Selecting rows with combination as treatment/new or control/old #New dataframe DF2 is created with the corresponding changes
#KNN Result
#map the dictionary to the new col on index
# Abbreviate the country into USA.
# convert categorical data into string
#Display all the columns
# predict the new housing values
#'Detroit': 'b463d3bd6064861b',
# Read in countries dataframe and join it with our previous dataframe
# fig.savefig('toma.png', dpi=300)
# create lagged autoregressive features
# read the original frame in from cache (pickle)
#determine which factors have the gratest impact on trip duration by creating a coefficient matrix and calling the #tripduration column and put it in descending order
#OA11CD is the output area code, popden is population density 
# Count hashtags only
# add two features for the number of times a particular device and ip address are used
# We import Pandas as pd into Python # We create a Pandas Series that stores a grocery list # We display the Groceries Pandas Series
# As we can see the expected score should be around 6.5 # which suggests that a good movie should be above at least 7.5
# Make country folders
# Word frequency for terms only (no hashtags, no mentions)
# We can also use attributes
#El Paso': '6a0a3474d8c5113c'
# Drop MET_DATE1 as it is identical to index date
# EXPORT FILE # Export the dataframe into an output CSV file located in the folder "output", placed in the same folder as the code # The file name, using a clever time stamp, will show as MM-DD-YY-Output.csv
#Remove one of the duplicate lines
# Save file to csv
# Get a list of column names and types for Measurements Table # columns
#Convert Rate for Pold under Null is simply 11.96% because Pnew and Pold must be the same values under the null
#ax.set_xlim(-20,50) #ax.set_ylim(0,5000)
# create date column, NOTE the end range will change every week when new odds become available
# Using the apply() function, compute the sum and mean for each column in scores.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# We drop any columns with NaN values
# Merge omdb
# generate LDA model
#Prices wrt to the number of bedrooms # count of words present in description column
# Setting up plotting in Jupyter notebooks # plot the data # ... YOUR CODE FOR TASK 8 ...
#Humboldt Park, Division Street converted to Humboldt Park and Division Street
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Now we use Pandas to print the summary statistics for the precipitation data-- IE, the describe function.
# Print the first 5 external nodes
#checking and displating the number of duplicate users
# Abbreviate these terms.
#Find the proportion of converted rate assuming p_new and p_old are equal
## 23
# Make your prediction using the test set
#Now, let's modify the dataframe to show only those rows that have a "complete" in the Current Status column, since we want to include only those permit applications that are marked as "complete" in our analysis. 
#estatisticas basicas
#20. Get the first occuring Transaction for every user.
# Load the data from the query into a dataframe
# Reflect hawaii database table weather into it's ORM class
#next it would be interesting to look at the spread of population densities
#El promedio del precio por m2 se dispara a 2800USD en Abril de 2017
# now extract as date and time separate columns 
# YOUR CODE HERE #raise NotImplementedError()
# Get all unique artistId, and broadcast them
# !dpkg -i libcudnn7_7.1.3.16-1+cuda8.0_amd64.deb
# The probability of an individual converting regardless of the page they receive
#Return the list of movies with the lowest score:
# cisnwe5
# fit multinomial NB
#Print number of deaths per year sorted by deaths
# Display the row's columns and data in dictionary format (first row)
# close connection to SalesForce
# 09/04/2015 10:55:25 PM
# Select columns of interest
# So I need to do some exploration of the events table. For what ever reason user_id is a decimal.
# using the default parameters of the logistic regression
# Your code here # df = 
#KNN classification
# find historical data for 2003
# Calculate various qunatiles.
# merge
## test id connection
# What organization has the most users? # There are 416 unique organizations and most users belong to organization id 1, 2, 3 and 4
# We will now create an html version of this report 
# OR
# Use Pandas Plotting with Matplotlib to plot the data # Rotate the xticks for the dates # plt.xticks(rotation=70)
# drop all null values
#del festivals['lat_long']
# Use Pandas to calcualte the summary statistics for the precipitation data
# Extract the uid of the saved model
# convert series to categorical, then back to series  # because categoricals lack many of the useful methods of series
### Create the necessary dummy variables
#Let's see which zip code has the cleanest date.
# This is the format pd.to_datetime needs:
# Plot ACF of first difference of each series
# faster way to simulate the 10000 trials instead of using for loops.
#initiate SQLContext instance on Spark
# more helper data frames # temp_8 is 8 hours prior to contest start - the nth value can be changed to evaluate a different number of hours
#viz_1=sb.countplot(x=studies_b.enrollment, data=studies_b)
#absolute value #length of a list (which we'll cover shortly) #Set of unique values in a list 
# Probability == 0.1188
# Load the statepoint file
#(p_diffs<p_diff_abdata).mean()
#After using function
# Info method shows the number of NaN records - more than 1400 # PD.read_csv command - parse_dates was used to convert date raw date into datetime format
# convert to datetime datatype # check timestamp datatype
# Create Dataframe
# convert the text column from the dataframe to a string
#Count how many objects are in each
# Facilitators
# We Create a DataFrame that only has selected items for both Alice and Bob # We display sel_shopping_cart
#nmf = joblib.load('./data/NMF.pkl')
# Testing random binomial.
# columns
# search for word in the SpaCy vocabulary and # change the is_stop attribute to True (default is False)
# plot histogram
# Get a bag of words for the test set, and convert to a numpy array # Use the random forest to make sentiment label predictions
# 6. If we define the "Tasker conversion rate" as the number of times a #    Tasker has been hired, out of the number of times the Tasker has #    been shown, how many Taskers have a conversion rate of 100%
# Import file
# data frameas and series have a number of plot functions # here, we use a histogram
# Use Pandas Plotting with Matplotlib to plot the data # Rotate the xticks for the dates
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# sum of all comments
#Create year values from the ActivityStartDate column
# created data frame with start stop times and dates for all tests
# Merge the suburban created data frames on the name of the city
#Creating another dataframe here... #Displays first five results. Don't worry if the Discounts column is all zeroes right now; that can happen.
# As HOUR is a float data type, I'm filling with a dummy value of '99'. For others, filling with 'N/A'
# Take all vets younger than 38 
##Most Liked Posts 
##what you see here is largely a train of thought, when i figoure out how to automate these tasks theyll be at part 1 #aaplhv['pc_change']=(aaplhv['Open'] - aaplhv['Close'])/aaplhv['Open']  #aaplhv['abspc_change']=abs((aaplhv['Open'] - aaplhv['Close'])/aaplhv['Open'] )
# plot histogram
#Task 1: Import 2017 daily data for AFX_X 
#Dropped null/bad values
# display unique values with counts for council_district
# preprend to corpus
## Generating random sample for inspection ##
#Save df to csv
# above upper quartile
#Select NDVI for scene after cyclone
# Let's use the Support Vector Regression from Scikit Learn  svm package # using the defaults only
# number of age-mates in different age values
# export/ create the processed dataset
#Take the index values which need to be excluded # Negate the ex_index values by ~ operator
# create df2 by dropping a subset out of df # further refine df2 by dropping a subset out of itself
#.  Probability that an individual received the new page
# Counting the number of null values
# drop last row
# Make Predictions
#we join meter transaction table file2 with meter location file loc by meter pole id:
# Display dataframe types and usage statistics
#Simulate Nold transactions with convert rate of Pold
# Remove columns that will not be used in the model # Create new df "drop"
#Use unique to show what the 4 unique values are
## Top 10 Bloggers
# r squared value
#read the csv file to a dataframe
# same reason as above to save the data...
#this data frame's index is "Month" in the spreadsheet.
#"original datafarme df  column changes being made permanent by using inplace=True option.
#branch point path length vs radius
#The amount of unique users are found
#Compute the number of rows containing users with old page
#read in the dataset
#merge of ordered timeline-shards no longer ordered for these users (iloc USER_PLANS)
# %reload_ext autoreload # %autoreload 2 # from DeepText.preprocess import preProcessor_in_memory
#goal: 
# Top 10 least active companies
# Add data to Pandas DataFrame
# Suppresing warnings for a "pretty output."
#inspect table values
# create pandas DF
#model.fit(X_tr, np.log1p(y_tr))
#df['state'].value_counts().sort_values(ascending=False).head().plot()
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
#run for March users
# Number of unique user ids: 290584
### read in WCPFC csv file created by using Scrapy crawler spider (for Chinese vessels only)
# Convert to numpy array for later operations
# genre_ids is a combination of differents types # We are going to get value that are not numbers
#we also don't really care about how the members join
# look at the first five rows of the df2 dataset
#'Glendale': '389e765d4de59bd2'
# Importing Libraies
# Normalization for non-object features - Numeric featue normalizatoin
# First, replace NaN with NA in status_message
#Display answer as a Series
# get basic information about data
#explore pandas dataframe work functions. 
# Make predictions using the testing set
# 9. 
# clean up whitespace # clean up spelling
### Create the necessary dummy variables and drop the CA column to create a full rank matrix
# Transmission 2050 [GWh], late sprint
# combined_df5
# FAVOURITE TWEET:
#19. For each user, get each possible pair of pair transactions (TransactionID1,TransacationID2)
#origin
# Notice the mixed cases in the complaints - this will complicate string operations. # Let's format all the complaint types to lowercase for convenience when searching # Now identify the 25 most common complaints:
# SVR # run linear SVR first ( kernel = linear) #reg_analysis(model,X_train, X_test, y_train, y_test)
# Descriptive Statistics
# identify null values
#Create logistic regression for the intereaction variable between new page and country using dummy variable
# Set the index to 'time' 
# to force type of columns, use the dtypes parameter # following forces the column to be float64
# subset ndvi to us
# Split dataset into training and testing sets
# Check for missing data - none
#sort from largest to smallest
# resample to weekly data
# use 'title' as the input text
# load the query results into a dataframe and pivot on station
# creates a subset of the original DF w/ only certain times represented
# plot new grid
# Second step is to merge this new dataframe with the 'transactions' dataframe to get the required records
#Create dummy variables for the countries
#params
#calcualte the one-tail test z score of 95%
#iii. How much influence to my posts have? #Ans. Get the sum of all the retweeted tweets and group by users   #     The users with most counts is most influencial
# format date in order to split data frame later on.
# Specify the input folder and the type of files to load in (glob_string): # The input folder will change depending on which computer/dataset analysing # If analysing data on windows make sure that there is a 'r' before the file name e.g. r'folder/subfolder'
#changing multiple column names PUBLISH STATES to Publication Status, WHO region to WHO Region
# Use isin function with the list lst_exclude then show the Records except the list
# from sklearn.cross_validation import KFold
# Check how many unique user_id and countries in the dataset
# Total number of unique users can be counted using nunique function
#**Which Tasker has been hired the least?**
#compute the p-value
# Save the edges and edge_types file.
# Plot in Bar Chart the total number of issues created every day for every Detaction Phase
# index positions correspond to integer values stored in vocab dictionary
# now let's find a term that is not characteristic of either class...
# we add two more columns 'pickup_cluster'(to which cluster it belogns to)  # and 'pickup_bins' (to which 10min intravel the trip belongs to)
# read in the downloaded data to file predictions aliased pred
#The coefficients
# this is the data per subject
# array to dataframe # add name
# 0.0.50131155217 # Tells us how significant our z-score is # 1.959963984540054 # Tells us what our critical value at 95% confidence is
# Create database connection
# Lets fix the population column real quick
#### Test ####
#resetting index again and doing final sanity check
#REMOVES NON UNIQUE USERS AND KEEPING AT LEAST ONE ROW WITH THIS USER #CHECK TO CONFIRM IF THERE IS ANY DUPLICATES LEFT
#display the metadata for dataframe df1
### Create the necessary dummy variables
# Accuracy: 48.86% for doc similarity # Accuracy: 39.39% for skills
# creating an explicit/distinct copy of the features
# read some data for grades vs ATAR 
#Joined(train,test)-weather (checking if there was any unmatched value in right_t) ##Note: Level of weather data is "State-Date"
### find the unique values in the country column
#Conversion rate for the control group, where the mean can be used as the conversion rate
# Compute the 10-day mean difference between the daily high and low prices
#joined_test.reset_index(inplace=True)
# if the file path does not exist, it will be created
# Find z-score and p-value using statsmodels.api built in function # define alternative='larger' as our alternative hypothesis is one directional and only cares if new_page is better than old_page # z-score: -1.31, p-value: 0.905
# We see that there are two date/time columns, we'll tell pandas to parse them when loading the full file.
# simply using string.replace function to repalce _ 
#checking which different countries available
#Convert the returned JSON object into a Python dictionary.
## Store the imputed data for further processing
#Shuffle the rows of df so we get a distributed sample when we display top few rows
# p_new = 0.1196
# Veamos algunas muestras
#read data set and take a look
# only consider regular season games
# Scatterplot retweet_count over favorite_count
# The *self* parameter in *Customer* methods performs the given instructions. # For e.g. to withdraw
#reg = linear_model.Lasso(alpha = 0.1)
# read table #  summarize the number of plots by plot type #plots_df.groupby(["plot_type"])["plot_id"].nunique()
#Checking the dataframe for any missing values
# We Create a DataFrame that only has selected items for Alice # We display alice_sel_shopping_cart
# Tells us what our critical value at 95% confidence is
#Add a column
# use BIC to confirm best number of AR components # plot information criteria for different orders
# create a trip duration column where the duration is in hours using pandas datetime package
# Assign directory paths and SQLite file name # sqlite_pth = os.path.join(dpth, os.path.pardir, "data", dbname_sqlite)
# create SFrame
# compute the mean of complaints per quarter...note this doesn't make sense, but works anyway
# to compare score between train / test set.  
# Import packages using the import command.  # Chaning my working directory
#comp.head()
### Create the necessary dummy variables
#   total number of new page / total number of users
#Save News Tweets to csv
#fitting the model using logistic regression for preddicting converted column using intercept and ab_page
#Great. This is what we wanted. Now let's convert the result, a grouped series, back into to a dataframe and rename the last column as "Completed_Permits". 
#Looks like all are active, but just in case you want to load in a new dataset lets be sure we get it right... #The status column is now redundant
#redshift_admin
#X.head() #abc = df.reset_index().values.tolist()
# Test our trained SVM classifier with our test data
# Copy farebox. # Get aggregates by DOW, time.
# Logistic Regression model
# How many stations are available in this dataset?
# index of df_users
## returns the data for which year==2015
#Walk Forward Validation RMSE when the window is 12
# merge datasets to approporiate rows
# pw_attempt_check(data)
# Which campaigns ended with positive results? 
# Empty search to ensure it is working # res["hits"]["hits"][-1]
#Read in Boston open data on Airbnb from Kaggle
# perform logistic regression
# Use `engine.execute` to select and display the first 10 rows from each table
#Find the difference in Pnew and Pold mean conversion rates
# fitting logistic model.
# Top 20 complained against companies
# delete cached result:
# groupBy function usage with the columns we group to become as the index.
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#Check shapes of array
# Check how many crimes per neighbourhood
#load objects
# make new dataframes with new and old data
# Show best number of trees
# Count number of unique users in df2
#dates = sm.tsa.datetools.dates_from_range('1980m1', length=nobs)
# Check if all the price values are numeric.
# Log into OkPy. You may have to change this to ok.auth(force=True) if you run into # an OAuthError
# creating 'day_of_week' - day of week that ad aired
#To find out which user_id has duplicate values
#yearago_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() - dt.timedelta(days=7)
# AND
# Preview the information available for the table
# plot the sum by day for June 2015:
# row information for the repeat user_id
# Save references to each table
# Normalized the data
# merge with council data #df7 = pd.merge(df6,df3,how='left',left_on='Date Closed',right_on='MEETING_DATE',suffixes=('_created','_closed'))
# Compute the average age for each day
# convert group values to 1 or 0 depending on whether it's 'treatment' or 'control' # drop no_ab_page column
# I change the name of the columns so they are easier to work with # then I change row 546 to the correct date
#locating the duplicated user id  
# How many stations are available in this dataset?
# confirming the change
# Probability of control group converted
### create a new dataframe that contains the entries for the control group only ### the probability of conversion for the control group
# the file is a two-column csv: date, tweet
# Number of trials for new_page: 145310
q7c_answer = r"""$ """$ display(Markdown(q7c_answer))
# Create an entity from the client dataframe # This dataframe already has an index and a time index
# Use Pandas to print the summary statistics for the precipitation data.Base.classes.keys()
# Let's check that we actually normalized the data - after
#split names - in data wrangling
#total number of individuals who received the 'new_page' #total number of pages #probability of an individual received the new page
# API Information
#Number of samples of Old page data
# to sort our indices lexicographically (alphabetically) (which pandas will look for in more complex queries) # we can use: # check that both levels were sorted
# Top 10 positive news
# Suburban cities ride totals
# Number of members which country of residence is Spain
# For those tweet_ids that have more than one stage, it seems to be apropriate to retain only `doggo` records.
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Take note of where the NaNs appear
# concatenating the DataFrames together
# reading the dataset
# explore patterns and special cases
# get averaged word vectors for our test new_doc
# Create a series for month in which first booking was made and fill NAs.
#Probability of a user converted in control group
# (c) 
#load csv file and filter by date
# Convert dates
# TODO plot main sequence
# Saves an image of our chart so that we can view it in a folder
# there is no more retwets or replies
# Import library # Compute test statistic and p-value, see: http://knowledgetack.com/python/statsmodels/proportions_ztest/
# load the model from disk
# Combining into one Dataset
## filtering out the cats
# Missing values statistics
# another messy  file with mess at the end
# compute the number of mislabeled articles
# Double Check all of the correct rows were removed - this should be 0
#from collections import Counter
#check that not all the values are 0
# visualize the number of inmates
# Check if we can use clean_string without creating *unwanted* duplicates
# How many stations are available in this dataset?
#usage_400hz = add_plane_data
# We create a column with the result of the analysis: # We display the updated dataframe with the new column:
# Avg. 2010 pop by state
#verification - mean of p_diffs that are larger than the actual difference
#Display each column's data type
#Save latest weather info in var mars_weather
#data
# df.drop_duplicates(subset=['A', 'C'], keep=False) # df.total_idle.value_counts()
# find the relative image url # img_url_rel = image_soup.select_one('figure.lede a img') # img_url_rel
# Verifying the results 
#save preprocessed df #psy_prepro = psy_prepro.set_index('subjectkey', verify_integrity=True)
# print words without probability
#============================================================================== #==============================================================================
# mortality rate
#Next we will read the data in into an array that we call tweets.
# Only about 1% have a negative sentiment
# do not fit with U.S variable as this will be the baseline
# option 4 (ignores and resets index)
# convert p_diffs as numpy array # p_diffs simulation distribution
# set game from team_games df in team_odds df
# read sets into a DataFrame; index is chosen as the orient so the set names are the indices
# 8. Print the mean education for each age_cat using groupby.
# Concatenating the two pd.series
#giss_temp[giss_temp == "****"] = np.nan
#Total the amounts in the oppose subset
#Exponentiate to interpret
#read in level1 CSV
#Cleveland': '0eb9676d24b211f1
# Note: used reset_index() here so it's easier to use the 'time' and 'time_2' columns if needed
#Sort the dataframe by date
# Groups records by NewsDesk and Popular and counts the number of values
# Use Pandas to calcualte the summary statistics for the precipitation data
#Ponemos en minusculas el valor del campo user_location
# When convinced Raw and Processed data is good, upload to bucket. # Upload raw TMY3 files to bucket.
#Now the crime data only has M and F
# Session 14 - Project by Sreedhara Jagatagar  Sreenivasa #1. Get the Metadata from the above files. #print metadata of 2nd Dataframe (berlin_weather_oldest.csv')
# here is a minute-based datetime
#this are the columns not present in the table and yes in view
# Export to csv
# proj WGS84
#transform text into array
# Display unique values with counts for ticket_status
# Clean up column names
#Tried saving and reading from csv and datetime is lost
#Send the requests and translate the response
##Create a second series of values using the same index ##Calculate the difference between temps1 and temps2
# .head to avoid scrolling
# Then I create a column for the year that the GM started which will be used later when determining what drafts the GM was in charge of
#Convert genre from a column of dicts to a column of strings by retrieving the name attribute from each dict. #compute one hot encodings using str.get_dummies #tmdb_movies_genre_revenue.set_index('id').genre.str.get_dummies().sum(level=0).head(10)
# Check dataset with one removed row
# Calculate Driver Percents
# For the rows where treatment is not aligned with new_page or control is not aligned with old_page # we cannot be sure if this row truly received the new or old page. # adding rows where <group> equals 'control' and <landing_page> does not equal 'old_page'
#*--------Merge Command to merge Studies and sponsors--------------------* #studies_b=studies_a.join(sponsors,rsuffix='_other')
# (d)
# Now we'll try "device_ip" - 2MM values in the reduced training set
# Show results
# one-hot encoding of categorical variables
# How many stations are available in this dataset?
# n_new
#importing  combined dataframe #df=df1.head(120) #loc_df[40:60]
#Importing the data sets
#El numero de oficina cual es ?
# Will convert to a numpy array
# migrating table info # for row in table_rows: #     print(row.text)
#Draw samples from a binomial distribution
#Checking to ensure I pulled all outlets
# Setup Tweepy API Authentication
# as it can be seen, the star should be in the stop word list
# generate class probabilities
# 10M word/token corpus  
# grab the index of the 50 largest abs(errors) # get the rest of the details from the frame # plot model error against strike
#look as the shape of the dataframe
#Check our data frame
#display the picture of the most favorited dog
# Format column AB (column 28) to two decimal places.
# Looking for duplicated Tweet IDs
# Invalid denominator values
#fill the missing millesime by year of creation date
#features
# What are the most active stations? # List the stations and the counts in descending order.
#Download the dataset. Takes a while, file is 800MB. Due to a slight complication with jupyter notebooks, #the progress bar is not displayed, so be patient.
# drop rows with missing specialty
#extract urls from the texts
# Extract slice - 1
#Complete a groupby to determine the average math score of each school by grade level.
# Simulate with H0 assumption old_page data
# fit the model
# how many unique products do we have listed in this purchase history? 
#create time series for market difference 
# create a new DataFrame # drop columns we don't plan to use to reduce the overall DataFrame size
# customer_emails.Email.dropna(inplace=True)
#These are the two categories we created
#Create Dummy Variables
# get a small_lst for demonstration purposes only
# Make a submission dataframe # Save the submission dataframe
# More complete numeric summary than describe()
# determining the first real commit timestamp
# may I know how to see attribute of every object in notebook? # . + Tab
# Generate a full list of column names, in order to create a list of features to select X and y modeling variables
#X_train.append(training_active_listing_dummy)
# Once we drop duplicates, we end up with a number of bills similar to what we started. We scored almost all our bills!
#read in mike objects #mike.sort_values(by='RA0')
#df2.landing_page.count() #row_count=df2.shape[0]
# Number of ratings
# drop 10 empty columns #drop empty last row
#stopword_list.remove("ich")    #Remove "ich" from the list.
# lq2015_date.sort_values(by='Date')
# what does requirements.txt look like?
# parse JSON file using json.load
# data_store_id_relation.groupby('air_store_id').count().query('hpg_store_id!=1')
#Let's go grab those articles out of lists, then we roll
# Retrieve the last date entry in the data table
# No more Twitter API lookups are necessary. Create a lookup table that we will use to get the verify the userToName #dfCommunity = pd.DataFrame() #dfCommunity = pd.merge(dfLookup, on='userToId')
# displaying results
#Check the combination of contractor_number and contractor_bus_name are unique #The combination is unique so far.  #Check duplicated rows by contractor_number 
# identify different sources
# Iterate through all the columns # This is an example of not choosing to save the image
# new stacked table should be 3 x size of the old
#Set up tweepy authentication
# read the twitter archive from archive dataset
#Q: there are some points with NaN values, if the point is NaN, how to handle? valid or not?
# add a column that combines date and time in the datetime format
# checking the length of dataframe with number of entries # using len() and unique() to count the number of of unique rows
#looking at the first 10 characters and viewing their relative frequencies
# since values are 1 and 0, we can calculate mean to get probability of an individual converting 
## Some funds have zero amounts associated with them. ## They mostly look like costs - expense fees, transaction fees, administrative fees ## Let us examine if we can safely drop them from our analysis
## 'engine' is a connection to a database ## Here, we're using postgres, but sqlalchemy can connect to other things too.
#We can count this using the value_counts() function
#CHECKING THE UNIQUE USERS
#we got two new feature .now update the dataframe with updated with new information
# Retrieve latest full mars images
#df_business.attributes = df_business.attributes.apply(string_to_dict) #print type(df_business['attributes'][0])
# pytz
# drop extra id column
# if user doesn't have executable file or executable file don't work on your local computer, use run_option ='docker_develop' #results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'docker_develop')
# These are the features we care about.
# Count terms only (no hashtags, no mentions)
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data
# Create new features using specified primitives
# Create predictions using your holdout set (x_holdout)
# What is kick_data_state? ... # kick_data_state = kick_data_state[kick_data_state.state != 'canceled'][kick_data_state['launched_at'].dt.year >= 2014]
# compute difference from original dataset ab_data.csv
#get a list of only image names in sub directory #image_links = [f for f in glob.glob("markdown\*.png")]
# from numpy arrays
# printing parameters AIC,BIC and HQIC
# Actual Test
# How many stations are available in this dataset?
# Make predictions on testData. cvModel uses the bestModel.
# Divide each number of each countries columns by it's annual maximum 
# As per the instruction above, p_old = p_new = converted rate in ab_data.csv regardless of the page
# popCon = popCon.set_index('contact')
#calculating the proportion of p_diffs greater than actual differce
# basenames = [os.path.basename(path) for path in cris.SpeechCollector.audio_collection['audio_path']] # basenames[0] in basenames
#remember to add these in alphabetical order, not in the order listed above
# Period :: represents time span  it is creating series of time for given month or year # here it is 2016 months  # A-DEC :: Its Annual and ends in DECember
# fillna
# companies = pd.read_excel(io="/Users/DanShin/Downloads/Data/crunchbase_export.xlsx", sheetname="Companies")
# choose either 0 or 1 with a probability of p_old, a total of n_old times
# Save data to Excel
#13. Fetch the records from Transactions Df which are not in the dfTemp df which has matching records
# What is the average?
#Select rows where the Confidence indicates estimated:
# read grid id to subset # unique
# go to FDIC site
# This will give the total number of words in the vocabolary created from this dataset
# Source: https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python
# number of users with new page
#check to confirm the row was dropped
# import the csv file
#test
# One hot encoding
# calculating or setting the year with the most commits to Linux
# print(logit.fit().summary())
# First Column
# get the columns, which is also an Index object
# 1. Arithmetic Operators
# Observed contingency table of test_id against test results # p-value of the null hypothesis
# The article URLs provided for this project. # Note that this script will work with other articles that share the same HTML layout. Just add URLs to this list.
# Create dummy variables 
#now we remove the records where NaN is the value for TEMP or TEMP_1 #to replicate the select node;  We need to keep track of the size of #the dataframe to make sure this happens correctly;
# model= model.load_weights("model")
### Making the model, fitting
# For binary classification, response should be a factor
# Run OpenMC!
# 2010 created
#INT is the contact information for INTERACTIONS #INT=pd.read_csv('C:/Users/sxh706/Desktop/Interactions.by.Month/2018/June/Interactions.Contacts.csv',skipfooter=5,encoding='latin-1',engine ='python')
# Concatenate ebola_melt and status_country column-wise: ebola_tidy # Print the shape of ebola_tidy # Print the head of ebola_tidy
# 3.2.C OUTPUT/ANSWER #top 10
# Create dataframe
#Convert p_diffs to an array #Get the actual difference in "ab_data.csv" #Calculate the difference
#### Define #### # Rename id column to keep consistency # #### Code ####
#festivals.rename(columns = header)
# Users list
# Plot the distribution of response time again, with log transformation
# Outer Join # The output table has a UNION of the keys
# We'll should also remove the double occurence of "_" in DATE__OF_OCCURENCE. Do so below: # New Code here # crimes.columns
# Another way to withdraw is by using the class name itself as follows:
# x axis for funding_average_per_type_bar_chart 
#Compute counts and plot them as a bar chart
# Accessing a column:
##Calculate the temperature difference between the two cities using the temps_df DataFrame
#get rid of organizations that were founded before 1990 and after 2016
# created in 4. Tidy Data
#Vemos las columnas resultantes
# Or show a histogram for each region
#The coulumns are our features currently,and adding new column which will predict by shifting to the specified period
#a = np.array([0,1])
#Since there are ads with varying duration, will have to include duration 
# Resolution Status Distribution
#Clean the data
#Import some different but related data #Now let's see the headings:
#new_page_converted = np.random.binomial(1,p_new,n_new) #new_page_converted = \ #np.random.choice([0,1], size=n_new, p=[(1-p_new), p_new])
#We use tfidf vectorizer with german stop word list (modified) from below. #df.message_vec.head()
# shift the signal, so trade at end of month and avoid lookahead bias
# number noms NOT foreign service
# add porn column and remove extra headings
#Review the dataframe that has all of the merged data (schools and students)
#To find the proportion of users converted in the dataset
#Compute the sigmoid function
# highest temperature recorded
# remove the rows where the treatment group doesn't line up with the correct landing_page value # remove the rows where the control group doesn't line up with the correct landing_page value
# Count occurrences
#storing the combined dataframe into a csv file
# get current local time
# x axis for funding_average_per_type_bar_chart  #funding_types
# Setting the columns names
# Plot the results as a histogram with bins=12.
#census = pd.read_csv('New_York_City_Population_By_Boroughs.csv')
# only grab 2953 minor dataset for fast run SVR 
#Save Data into CSV
# the best parameters for the logistic regression is:
# Make columns for seasons and terms
# specify resume path.
#Calculate the total rows with mispalced labels
# this is a count of how many odds changes occurred for each contest, which can also be exported if need be
#JPL Mars Space Images - Featured Image
#checking that spaces are gone
# covert the query results to a pandas dataframe
#number of times old page appears 
# What was the average daily trading volume during this year?
#As shown below, # Of Positions prints anywhere from 1 to 100 #[  1   3   5  52   4   2  50  11  30   6   8  15  10  12  20  16   9   7 #  25  69  75  67  14  17  71  13 100 107  39  18  28]
# get all puts at strike price of $80 (first four columns only)
# Make vhd folder
## turns stemmed plot back into single strings
# minimum you need to pass to upload a dataset
#drop rows with null values in dirty columns: 27662565 records #drop rows with values in dirty columns: 26290342
# limit the returned query to 2 rows
"""$ Count token frequency and print$ """$
# Use the Inspector to explore the database
# Count the number of tweets in each time zone and get the first 10
# Baseline score:
# combine all product_type
#Input list of unique station keys for each station.  #This list is the same for all stations
# First, find a tweet with the data-name `Mars Weather`
# Calculate what the highest and lowest opening prices were for the stock in this period.
# Look at first x rows in the table
# Read csv
#MODEL SCORE WITH GRID SEARCH BEST PARAMS on 3CV folds/14 params {'C': 500, 'n_jobs': -1, 'penalty': 'l2'}
#analysis the timestamp column
# Use NumPys binomial to simulate n_old transactions with a convert rate of p_old under the null
# alldata # https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas # df.loc[df['column_name'] == some_value]
# Determine d2 and d3 constants.
# Store the API key as a string - according to PEP8, constants are always named in all upper case
### Fit the Linear Model And Obtain the Results
#Los Angeles': '3b77caf94bfc81fe'
# Siginificance of the z-score # Critical value at 95% confidence
# Check the list of column
# Define a date range #print(dates[0])
# Check the results by comparing the calculations.
# Sort by various ways
#First, remind us what our columns are
#df_state_victory_margins.plot(x='state', secondary_y=True, y='Percent margin', kind='bar')
#show how to get the other comments of a user #print(submission.title)
#logistic regression to predict converted column using intercept and treatment columns
HTML("""<iframe width="560" height="315" src="https://www.youtube.com/embed/pPN8d0E3900" frameborder="0" allowfullscreen></iframe>""")
# view the start of the file just saved
# 23 ST -> 14 ST-UNION SQ @ 11:30 am = 20 min # Print total crepe sales lost due to transit
# Getting the value of Rows as per requirement using iloc to get selected rows and all columns.
# Setting index
# converting the timestamp column # summarizing the converted timestamp column
#Gathering Udacity's Twitter image predictions 
# Connecting to MC database
#b. Each variable on its own plot
# converting into datetime-format
# Tells us the percentage of the significance of z-score is # Tells us what our critical value at 95% confidence is, #that the type I error rate equal to 5%
#Plot distribution of difference in CTR
# Saving results to pickle # store the data as binary data stream
# taking a look at the columns and decide which I want value counts for
# one time if for converison of list
# Double Check all of the correct rows were removed - this should be 0
# a UDF to delete spaces from the strings
### Check distribution by country (associtated with creator's location)
# This table has projects that have only been submitted once: # 95978 + 1012 = 96990
#Explore Database Columns
# From my previous study of the data, I drop the columns I know wont be useful in our analysis
# find the number of rows in the dataset
# fit the model
# Step 5: Time to review data.py file and review where function went wrong... ## Review the first 24 entries of the fremont.csv file ## This displays the time is 12 hours and not 24, which means our strftime; We need to use I instead of H
# Creating the object for LDA model using gensim library # Running and Trainign LDA model on the document term matrix.
# write the scaled and unskewed data back to postgresql
# Incidents table stored in Amazon Web Services s3 bucket
#An SQL example:
# change name of oz_stop id to stopid to allow merge
#Comenzamos a trabajar con el campo id para detectar filas duplicadas
# Rename columns where necessary
#twitter_df_final = pd.concat([archive_df_clean, status_df, image_df_clean], axis=1, join='inner') #create a copy of image_df which has only tweet_id and related data on prediction 1.
#read in the mean rsvp count data collected on Jan 27, 2018
# remove subjects with more than 50% of missing values
#str.time
# Check if the DataFrame looks how we want it to, continued.
# We get descriptive statistics on our stock data
# filter on Urban # len(Urban) # Urban
# Merge and simplify the mini lines to the rest of the lines one final time. sql = """CREATE TABLE mthrice as $ SELECT ST_Simplify((ST_Dump(ST_LineMerge(ST_Union(linestring)))).geom,.00001) as linestring FROM mtwice;"""$
# drop rows for mismatched treatment groups # drop rows for mismatched control groups
# Faltando valores para Puppers, Floofers, Doggos and Puppos
# Requirement #2: Add your code here
# Mapping using the PDBe best_structures service
#Sort our recommendations in descending order #Just a peek at the values
# re-arrange cols to original order
# read data withpandas.read_csv with delimiter and encoding specified
#Saving the dataset in a file
# fill null with 0
#Find out the average 
# unique identification of project
#reading csv file from twitter-arvhive-enhanced.csv to the dataframe df'
# check group size
# your code here
#Adding intercept variable and dummy variables for the group
# this shows that the index is not right.  # I need a datetime index for texas_city, like for example addicks.
#Drop unnamed index column as Panda adds its own one. Run this once only
# separate out target column, rename columns w/ generic title  |  [target = 't']
#checking contents of dataframe
# Let's say you want to get the distinct count of unique entries in the OWNERS # table:
## Simulation of N_New ##
# Create an object to transform the data to fit minmax processor
# Integer slicing
# write to file
#Print the columns which are having atleast one missed values and store in a variable
# reading in data
#Let's get the tweets from Youngstown, Ohio
#setting up 10 commandments chapter to see how the summaries compare
# df_train.shape
## N_Old mean ##$ '''plt.hist(old_page_converted);'''$
"""$ Use this code to test your classifier after each round of confidence sampling and plot your results$ """$
# split features and target
#Find current featured image
# data points separated into days of week
# text cleaning imports
# split features and target
#How many statuses do we have? Should be same as row number of extext.shape above
# Force the Frequency_score column to a numeric data type as it should be
#  Extracting the Transaction month out
# read in data only in the Date and close columns, # use Date as the inde
# Should be used on reloads to produce faster results.
# How many learners were in the housing trainings project?
############ check NA #############
#proportions of users converted
#use Pandas to convert data to HTML table string
# This step can take 5 to 20 minutes # NOTE: Currently, recreate=False is not supported for deploying to ACS clusters
# Ensure the file exists
# select all the non-numeric columns # get the list of all the non-numeric columns
#Created a Year and Month columns to extract the month and year of transactions
# minimum age
# Normalize Data # Data Standardization give data zero mean and unit variance (technically should be done after train test split )
#checking for duplicate observations 
# merge cities  (no need) #df_l = pd.merge(df_l,df_c.rename(columns={'port_city_id': 'id', 'id': 'city_id'}), on='city_id')
# String Splitting
# bus['zip_code'] = 
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# BE SURE TO RUN THIS CELL BEFORE ANY OF THE OTHER CELLS
#### ###
# drop the 1st row (which is the column names): 
# Simulation descriptive statistics 
# dataframe provides a more succinct evalucation
#first, we convert the visitStartTime stamp from POSIX time format to regular time format
#The number of unique users in the dataset.
# Check the final result of the dog_name column
# Two possible solutions: # If the comparison statement is True it will append a True bool to the new column.  Otherwise it will be a false. #Converts the Bool True False column to 1s and 0s
#Now merge it with the londonGeoDF using a spatial join to find which OA each location is in
# Compute the probability of failure for different temperatures
# Fill in missing values of 'Age' with its mean # Fill in missing values of 'GPA' with its mean # Fill in missing values of 'Days_missed' with its mean
# make new column for month. 
# Baseline Borough Accuracy  (Most commonly 311 serviced borough / Total 311 service calls ) in millions
# convert location to numeric value
# Series objects can be assigned a name  # The index can also be assigned directly.
# Use the republican convention as the start date - July 19th
#Create the necessary dummy variables
# set SUMMA executable file
# Retrieve the parent divs for all articles
#del df2['building_use']
#'Fremont': '30344aecffe6a491',
''''$ user access to matplotlib functionality, which is another Python package that can allow us to visualize our results.'''' $ import pandas as pd
# merge predictions and archive
# Set DateTime as index
#Reading CSV files in the DIR
# inspecting df2
# Simulate n_new transacrions with a convert rates under null hypothesis
# 2. Comparison Operators
# Create additional columns specifying what user/country converted
# We display the total amount of money spent in salaries each year
# read json from text
#using StringIndexer for cluster graph 
# Filter outliers in Min Power
#The dataset is read and the first few rows are shown
# run the model giving the output the suffix "1dRichards_docker_develop" and get "results_1dRichards" object
#Read in the data into the 'sites' dataframe 
####TEST #a['Compound Score'].mean()
# Calculate the difference in p under the null hypothesis
# First we will grab the data of interest
# alternatives:
#Select rows where the Mean flow was less than 50 cfs
# Calculating z_score and p-value.
#Example:
# use scipy.stats to compute the cumulative probability density (>z_score) and the critical value given alpha equal to 0.05
# /api/v1.0/tobs # Return a JSON list of Temperature Observations (tobs) for the previous year
#last_values
# Double check if there are no more values of 0 or negative values in dataframe's "Fare_amount".
# reset index
#we want data that is less than 5 MB per chunk (each compressed data chunk is stored in the list compressed data)
#http://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.proportions_ztest.html #Help to calculate p_value easily using statsmodelapi #zstat_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old])
# create two Series, same start, same periods, same frequencies # each with a different time zone
#last 12 months of temperature data. 2016-8 - 2017-8 #Select only the date and prcp values.
#checking the number of users in the dataset
# Examining data - top 10 states by amount and number of donors
#sorting by an axis
# Copying dataframe in df2 # Removing increasing rows
# calculate RMSE (for alpha=0.01)
# Examine violations where a product recall was requested
#checking the average conversion rate in control group
# proportion of users converted
# generate coordinates based on loading scores and scaled data
#this explains that there are only four sources:iPhone, Vine, Twitter Web, and TweetDeck. 
# Create the Dataset object. # Map features and labels with the parse function.
# search of life expectancy indicators
#subsetting data
#max_wind and max_gust are well correlated, so we can use max_wind to help fill the null values of max_gust
# Drop the unnecessary columns and rearrange columns
#ignore_index means the indices will not be reset after each append
# Your code here
# handle missing values # check the results
# save df to csv
# use the actual_diff I computed before  # and compute proportion of the p_diffs are greater than the actual difference
#Set index to date
#Investigating sponsor_types
# rescue code!
# converting to date to pd date time
#propiedades entre 150 y 200 metros cuadrados
# -f = to ignore non-existent files, never prompt # -r = to remove directories and their contents recursively # -v = to explain what is being done
# flight6 = spark.read.parquet("C:\\s3\\20170503_jsonl\\flight6.parquet")
# save a reference to those classes/each table called Station & Measurement.
#plot_user_popularity(very_pop_df,day_list)
# The above query returns a list of tuples from the measurement 'table' object.  We want to import these tuples into a pandas # dataframe which extracts the values from the tuples for input to the dataframe. # Add column labels to the df.
# Dataframe agg
#### Define #### # Convert index into a column and reset the index # #### Code ####
# confirming work
# merge two datasets 
# Place map
# Plot the daily normals as an area plot with `stacked=False`
#sort by Fox in order to create different color scatter plot
#age distribution #train['date_first_booking']
# Assign the demographics class to a variable called `Demographics` ### BEGIN SOLUTION ### END SOLUTION
# split out y size # split out predictors
# Import data # Display the top rows
#Load dictionaries from Yml files #dicttagger_sentiment = DictionaryTagger(['C:/Users/ray/Documents/ConnorStuff/DataAnalytics/positive.yml','C:/Users/ray/Documents/ConnorStuff/DataAnalytics/negative.yml']) 
### Arrange Values of Particular column in Asending Order.
# convert rate of P under new null
# Use Shape Function To get a count of rows
# same logic as above, here we do it for treatment instead of control
# Create a vector of random integers, then reshape into a 4x4 array. #
# verify that we now have all the float columns we expected
# Saving the model for use in future legislation
# Lenghts along time:
# Remove rows where 'Delivery block' is empty.
# Create a new df copy of df2
# A feature combining cities the flight is between. It is a substitute for distance since distance is not provided.
# fig1.show() # Pupper is the most populate stage
#We already have all the other variables so drop the ones that don't start with "Source" or "Customer_Existing" #Also keep ID
# Create a df from list of tweets  # Each row is a separate tweet
# Open <country>_users.csv and print last 5 rows
#convert rate for p_new is the same as convert rate for dataset regardless of ab group
# Flitering to the tracking tag that you are interested in
# DEFINE :timestamp must be a datetime object #CODE: # TEST :
#dfs[25].head()
# pickling pres_df
# Alternatively we can also use
#now save it to csv:
# Sample five random movies
# Get all titles that have "SOFTWARE" or "PROGRAMMER" in them
# Import and Initialize Sentiment Analyzer
# IQR
# less than 500 # more than 500
# Check rows left after removal
# Test converting the Text feature of the table into a text file 
# Obtain summary statistics for two groups
#!ls ..\input\ #Windows command
# Load the list of topics created manually
#Target city = Charlotte, NC
# Get info about the database to check missing values and data types.
# calculate the mean of the first minute of the walk
# select the closing price
# Let's look at the tweet dataframe
#Vemos cuantas filas tienen place_country en Nulo
# create an intercept column and fill it in with ones.
# Check if all the zip codes are valid. # Logic: zipcodes that appear less frequently tend to be erroneous, so check zipcodes used less than 5 times.
#we execute the query statement and we get a DataFrame as a result.
#training dataset got 69.37% accuracy
#load data for scientists_1.csv and figure out the columns
# Save references to each table
# Another way to do it
#Examine the summary stats of the per_studen_budget column to create bins needed for next analysis. 
##Create temperature bins. #bins=[]
#vectorizing words #vect = CountVectorizer(stop_words=stop_words, ngram_range=(2,7), max_df=.2, min_df=.0001)
# joining df2 and countries dataframe.
### Fit Your Linear Model And Obtain the Results
#compare summaries for Matthew 1-- one of the chapters describing birth of Christ
# Print the index of airquality_pivot
# Extract slice - 2
# update tweets_clean['user'] to id only
# Dropping duplicates based on index
# Save references to each table # Create our session (link) from Python to the DB
# rename non-target columns
# get_data() is a function within Engine that makes a sql call to an API.
#count of users in treatment group
# using a copy to be safe
# Spark Streaming #Create streaming context with latency of 1
# creating dummy variables for DEVICES
#r_clean[::int(len(r_clean)*.1)].sum(axis=1) #hist_alloc[::int(len(r_clean)*.1)].sum(axis=1)
### Fit Your Linear Model And Obtain the Results
# Python is row-major order # https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html
#df.isnull().sum().sum()
# creating vip reason df
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
#### Define #### # Replace underscores '_' between words in p1,p2,p3 columns with ' ' #### Code ####
#As per assumption that "All have equal convertion rate"
#reference syntax - proportions_ztest(count, nobs, value=None, alternative='two-sided', prop_var=False)
#Let's keep it
#'Toledo': '7068dd9474ab6973'
#get a sense of indvidual data types
#Now impute 0 to missing values
#Histogram plot of Sample Distribution
# run ltv model
# View the last five rows of data.
# save the dataframe as a csv file #df_3.to_csv('LBDLidentitefabriquee_2016_day_media.csv') #df_3.to_csv('LBDLplussecret_2016_day_media.csv')
##Create a Series using the DatetimeIndex created in the above cell
#Resets the table and reformats into a more desireable dataframe structure.
#Dataset has all kinds of races. We will need to filter on 'PRES'
#View the rows with dublicated user id 
# Dummying attend_with
#creat dataset with document index and genre
# Create a directory to hold the data # Download the data
# creating a new column for name length # storing name lengths for corresponding campaign
#count of converted values
# Simulate with H0 assumption new_page data
#move the lat and long columns to be next to the address #You can rearrange columns directly by specifying their order: #df = df[['a', 'y', 'b', 'x']]
# Issues open
#A query is used to investigate when new_page and treatment does not line up
# converting the timestamp column to dateime object and then extracting out the date
#Calculate the P -value
# checking if format change by above code
# Find and list the most active stations
#Created a scatter plot using the Volume Sold (LIters) sum and 2015 Sales
# df_new1.to_csv("cleanlisting.csv", index=False, encoding='utf-8') # print("csv created") # print(df_new1.shape)
# Calculate the log age of the account in weeks (+1 for smoothing)
## Sleep on rate limit is false because I check in my code
#reading predictions file data into a data frame
### Create the necessary dummy variables
## Patch of the sky in the Milky way observed by OGLE:
#Fit into the regression model
# --- check the feilds that are unique amoung the students 
# RE.SEARCH
# paid_success_with_data
# Create copies of original dataframes
# focusing on the Black Friday Weekend
# Check if there are any NaN values in the Train DF
# Because the shape value for n_new and n_old are different ther will any comparison will cause an error. # To circumvent this error we will use the mean. This should not change our output since we are still using  # probabilities as in the previous case. 
#left join transactions with users
# Samples contained per file
# Check amount of data post removal
#cur.execute string, attaching the station_ids list 
#not required
# Write hdf5 to current directory
# Create predictor and target variables as X and y
# Copy data_df and rename columns
# let's see if removing some of those values near 0 gives me a more recognizable distribution:
# Read the solar power production from Data.xlsm
#total number of rows are stored in the variable named total_rows
#creating another Dataframe with lesser no. of columns
#Station with Highest Number of Observations
# De paso recolectemos aquellas columnas que tengan o indiquen un formato  "date" # Estas las podriamos utilizar de ser necesario en la importacion.
# Add column and convert treat / control to BOOL
# Select only WordCount and use describe function to extract the main statistics from values.
# before merging, both zip columns need to be the same datatype
# for parsing bad formats # caution: recognizes some strs as dates that you might not prefer
# Note, for just counting the groupby and value_counts are equivalent # There is more than one way to skin the cat (or panda)
#churns
# Create a separate data frame that holds the average park attendance for different parks.
#create new columns: 1 = both in that country and in treatment group, 0 = in that country and in control group
# Listing top 30 most popular hash tags during the data collection period # "#guncontrol', '#guncontrol.', and '#guncontrolnow' should be count toghether. So should '#marchforourlives' and '#march4ourlives' etc
# grid = sns.FacetGrid(train_df, col='Embarked')
# ddf.to_csv('new_ddf.csv', index=False)
# datetime conversion and created a new column NDATE
#What was the largest change between any two days (based on Closing Price)? # Note, this is interpreted to mean for two consecutive days # Why does it list in reverse order?
# We create time series for data:
# Mapped classes are now created with names by default # matching that of the table name.
# STEP 3: Check how often the values are negative # This returns 7861, which is about 1% of the data.
#create arima model
# prepare for merge with temp df
# Design a NEW query to retrieve the last 12 months of AVERAGE precipitation data for each day.
# interestingly, we can use either or both levels of a  # hierarchical index to pick out specific entries
# Setting up plotting in Jupyter notebooks # plot the data # ... YOUR CODE FOR TASK 8 ...
#Save Test Dataset as pickle for later use # the test data in pkl format
#last_values
# removing the unnecessary columns
# SAVE A CSV OF THE NEW SPOTIFY TABLE ("spotify_merged.csv") WHICH NOW FLAGS SONGS THAT REACH NUMBER ONE IN A REGION
#veamos los dtypes
#caluculating the p_value with respect to the actual difference observed (4b&4c) 
# fit (train) the vectorizer with the corpus from video text contents
#To shows columns within dataframe
# Create a training DataFrame
#Vemos los tipos de datos
### Fit Your Linear Model And Obtain the Results
# Removendo o id duplicado mantendo o registro mais recente
# Use `engine.execute` to select and display the first 10 rows from the measurement table
## Saving our data # save our pca classifier # save our crosstab_transformed
# Checking for redundant rows
# graph api call using requests get method.
#find earliest create date
# subset temp to us
#total4
#importing data
#Uno los dataframes para hacer clustering de TODO #all_df.info()
# square city values
# Let's drop it:
#construct and display cbs sentiments dataframe
# only US, CAN and MEX are returned by default
#hago lo mismo pero ahora con la villa Fuerte Apache
# Delete the lines with no comment, therefore no sentiment - about 7% dropped 260217/278884
# Make a transform with first and last date per customer #p = data_s[data_s['isvalid']>0].groupby('customer')['date'].agg({"first": lambda x: x.iloc[0],"last": lambda x: x.iloc[-1]}).reset_index()
# Simulate conversion rates under null hypothesis
#remove tweets with null jpg_url values
# Let's display what is contained in the node_types file.
# What are the most active stations? # Create a query to find the most active stations. # List the stations and observation counts in descending order
#calculate the proportion of p_diffs greater than the observe difference
# Find the number of rows
# aggregate data # filter for dates # explicit typecast
# unify index column names
# Merge multiple rows
#create separate dataframe containing current dog stages
#proportion of p_diffs greater than the actual difference observed in ab_data.csv is:
# Using the station id from the previous query, calculate the lowest temperature recorded, 
# Print all of the classes mapped to the Base by the automap function.
# 2015 so far
# Printing the content of git_log_excerpt.csv
# finding the duplicated value # locating the duplicate row in dataframe df2
# days from today until a given day
### Find the range of dates 
# Count number of lines in ratings
### Create the necessary dummy variables
#Determine summary statistics for the school dataframe
# Load the query results into a Pandas DataFrame and set the index to the date column.
# merge the labels with there games 
#REFLECT DATABASE TABLES INTO THE SQL ALCHEMY ORM # Create an engine to connect to the database.
# Drop those two rows with those indices and you are saying inplace=True, to make sure you are not creating a copy. 
### Create the necessary dummy variables
# Merge sentiments back to data
#sorting Values in DataFrame by Year and Display Value
# Set a starting and ending time. #end_t = '2017-12-31 23:59:59'
# let's get every tweet with hashtag "#digitalgc" in for the past week # twitter api only allows searches for the past week # api.search
#Print out new dataframe # @Grader: How can I turn this into a dictionary??
#run model on the training set to produce the backtest
# here we create instantiate the CreateDataFrame class and assign the df attribute # print out the first 20 obs
#Difference in Conversion Rate , very close to 0
# Create engine using hawaii.sqlite created in database_engineering steps
#the first number is a z score value and the second is a p score value
# Make columns with country sums
#save data to a csv file
#The US column will be our baseline
# Use Pandas to calcualte the summary statistics for the precipitation data
#quiero buscar las propiedades que esten en Flores
# Create column and apply hours of the day from function
cur_b.execute('''$ ''')$ conn_b.commit()
#convert json to dict with .json()
# find the z-score and the p-value of a two-sided hypothesis test
# get the control group converting probability
# Put in the end points for the forecast
#Dropping where treatment is not aligned with new_page or control is not aligned with old_page
#let's double check to confirm it is dropped or not
# Define the demonstration data. #base_df.drop(['index'], inplace=True)
# Generate a new dataframe by selecting the features you just defined
##I'm going to create a correlation heat map with all of my quantitative data to try to find  parameters to put into the model.
#remove one of the duplicated rows
#converting size in KB to size in MB
# Novo Dado a ser rankeado # Transforming new_doc into vector (lang_idx, freq)
# The MEAN trip distance grouped by hour of day
# Count the total number of tweets
#plt.hist(tag_degrees.values(), 50, normed=True) #Display histogram of node degrees in 100 bins
# there are only users added
#adopted_cats.reset_index()
# read the entire file into a python array # remove the trailing "\n" from each line
#Looking data format and types
# only have partial jan 2017
# I used a "left" join to avoid removing rows if no match is found in the dests_small dataframe # I want to ensure the merge doesn't introduce too many nulls
#Melt with 2 variables
#TODO
# scenario_dates = df_exp.groupBy('date').sum() # var_per_portfolio.toPandas().plot()
# array to dataframe # add name
# Create a logit model based on countries taking country_CA as the baseline
# Let's get the more detailed Titanic data set
# load CSV files
#Test: # I wanted to test if the 'doggo' stage prevailed on 'pupper'.
#Rearranging/ Renaming data for graph
# Get the SamplingFeature object for a particular SamplingFeature by passing its SamplingFeatureCode
# Find out average 
# http://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/01_the_machine_learning_landscape.ipynb #x_train = np.float32(x_train) #x_train = x_train.reshape((len(x_train),1))
# Now we load the data from the query into a pandas DF: # call it "rain_df":
#Number of conversions for each page #n_old and n_new previously calculated
#Then read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count.
#read in the schedule file
#import excel spread sheet of list of violations
#identifying unique instances where old_page does not line up with control
#train_df["description_score"] = train_df['description'].apply(process) #train_score = train_df["description_score"].to_frame() #train_score.to_csv("data/train_score.csv")
# alternative='smaller' means p_old < p_new
# For convenience replaced the acceptance and response rates, which were mostly high 90s or NaNs with random values.
#3 possible ways [new_page + control, old_page + treatment, old_page + control]
# Create a dataframe with all old page records from df2
# inspect the datatypes for each column in the data
# in our case we have more than 2 dataframes so the following code will merge more than 
#Mean monthly returns
# Create symbolic link in ~/data/libs to use site-packages SystemML jar as opposed to DSX platform SystemML jar
#dates_list = [datetime.strptime(date, '%Y-%m-%d').date() for date in month] #eth['close'] = eth['close'].astype('float64') 
# Merge csvs (3)
# Merge companies_pf (8,664) and funding_pf (10,901)on Company to create merged_data = 9,069 #merged_data.head(5) #print(len(merged_data.index))
#all_mapped = all_data.loc[all_data['rpc'].isin(mapping['rpc']) * all_data['store'].isin(mapping['store'])]
# .order_by(Measurement.date.desc())
#reading data set, view the first several rows of it
# checks the length of the list for 0 (ok), which will be populated if columns that should have been deleted are present.
#Compute annual Nitrate concentration
# removing retweets
#set up geometry field for delay_geo.. right now, there are only lat/longs
# You can use the axis argument to get the maximum per row instead. #
# define the length of period #print 'Accumulated number of tweets: ' + str(accumulated_num) #print 'Frequency of tweets every perid: ' + str(frequency)
# Package the request, send the request and catch the response: r
# hyperparameter combinations test
# Be a scatter plot of sentiments of the last **100** tweets sent out by each news organization, ranging from -1.0 to 1.0, where a score of 0 expresses a neutral sentiment, -1 the most negative sentiment possible, and +1 the most positive sentiment possible. # Each plot point will reflect the _compound_ sentiment of a tweet. # Sort each plot point by its relative timestamp.
# Load the tallies from the statepoint into each MGXS object
#Convert rate new under the null
#probability of conversion in cntrol group
#download the file.
# saving as a pickle object to retain datetime object # to read in a pickled file [e.g., var_name = pd.read_pickle('data/FILENAME.pickle')]
# How many stations are available in this dataset?
#do a outer join on df_userid and df_Tran dataframe
# Retrieving a total amount of dates
#Use Pandas to print the summary statistics for the precipitation data.
# this is the default data, so let's refamiliarize ourselves with # what it looks like
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#hierarchical Indexing
# view regrid humidity
#removing the duplicated row
# Looks like it's working, now I have to do it for each id in the tweet_id column
# Extract just the BTC wallet transactions
# Rename the unnamed column to 'time'
# Print all of the classes mapped to the Base
# create an isntance of the ModifyModel class 
#Tweepy Cursor handles pagination .. 
#Test:
#You should convert the user_id from object to integer!
# Test
# set the index of pgh_311_data to be the parsed dates in the "CREATED_ON" column
# Access local file
#The result is correct; 
# air.head() # hpg.head() # hpg.head()
## check whether the parsing succeeded for all the locations
#dang, there are ~80k nulls in may
# Applying the function again to the column 1
# We can display the message data in JSON format
#Initialize modules for plots
#load second json object
#Determine the top five schools that have the lowest percent of students passing math and reading (overall passing rate)
# Now extract the first thousand lines to save time
#Calculate the lowest opening prices for the stock during 2017
# Task 1
#Saving the output to CleanedMovies.csv in an Output folder
# TODO: SHOULD NOT HAVE TO DO THIS # CONVERT INDEX TO DATETIME
#check for the data in the table 'station'
#10. Get the subset rows 11, 24, 37
# Drop the rows for last date
## 2. Visualization and basic statistics
# Check proportion of calibrated minutes vs not: 1 id calibrated, else 0
# Manual calculation of the z-score
"""$ load data$ """$
# create the training, test sets # we don't need the date columns anymore
sql_query = """$ SELECT * FROM top_5_by_genre;$ """$
# Looks like a lot of proper nouns
# Display the most commonly rated movies
#Retieve the column list for the dataframe df_ created in problem statement 20
# Double Check all of the correct rows were removed - this should be 0
#month pandas.DataFrame(data['11']['data'])
# Create engine using the `demographics.sqlite` database file ### BEGIN SOLUTION ### END SOLUTION
#!pip install  pytrends
# define "db" and "coll" variables for convenience
# get categorical dataframe
#Show entire description column
#probs_test.detach().data.cpu().numpy()[:, 1].mean()
#Agregnado las paradas de colectivos al dataframe de dataFiltrada
### Checking distribution of project outcomes ('state')
#print the final parameters
# Drop offending rows
# Joining categorical data back with Ideas DataFrame
# shape of country df
#drop #confirm
# storing cities, states and counties as sets
# take the list of [(date1, count1), (date2, count2), ...],  # for the turnstile and turn it into two lists: dates and counts #dates for turnstile (df["C/A"] == 'A002')&(df.UNIT == 'R051')&(df.SCP=='02-00-00')&(df.STATION=='59 ST')
#'Mesa': '44d207663001f00b'
# Value of old_page_converted
#https://stackoverflow.com/questions/11285613/selecting-columns-in-a-pandas-dataframe?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa #Splitting up the tweet_df to show all columns here
#bus.count # use value_counts() or unique() to determine if the business_id field is unique
#Get the proporation of the p_diffs greater than actual difference
# Information about the independent factor w?
#5. Changing the multiple column names
#export df to csv
# links to photos are stored in array # taking the length of the array provides the count of photos per observation
# warnings.filterwarnings("ignore")
# Print the first Tweet as JSON:
#4 convert all characters to lower case
# cvec_2 top 10 words
#plot the normal dsitribution for the null hypothesis
# Rezeptsammlerin now only has 679 recipes instead of 870:
# the probability that an individual received the new page
# integer features scaling
# Inner Join users to sessions on (UserID=UserID) and (Registered=SessionDate)
# Write your answer here # find the frame that satisfy the condition # print the number of records
#Adding a new column to the dataframes that contain the polarity value.
# Probability of individual converting that is in control group
# one way to tokenize:
# Strange data
# Variance in Monthly Returns
# Run seasonal decomposition and plot
# these are the change columns - only the moneyline changes during spring training, there is no runline or total odds
# creating a new dataframe with just the columns I need
# Create a list of filenames for all the years data 
# proportion of p_diffs greater than the actual difference observed in ab_data.csv is computed as:
### read in IOTC csv file scraped from iotc site (for Chinese vessels only)
#...or ranges
# count occurences of uppercase words, which may indicate rage or anger
# create new test df with col names already created
#create array
# Decision Tree Model
#UK #For a unit decrease in UK,ab_page is 1.02 times likely holding all else is constant
# display specific number of rows from bottom of the file 5 is the default 
# get a count of how many countries are in the country column
# Must have an HSPF of at least 10 to make list.  For now use 18 as an # upper limit, but later that may not be high enough.
# Evaluate the model and print results (optional, only useful when there's a metric to compare against)
## Using Dow 30 dividend data sourced from yahoo finance
#converted p_new
# add a notebook to the resource of summa output # check the resource id on HS that created.
# Not sure!!
#print the first two parts: overall introduction and timely publication
res = sqlCtx.sql("""SELECT host, stddev(result)$             FROM tempTable$             GROUP BY host""")$
# Calculate predicted probability of survival: predicted_prob_true # print predicted_prob_true
# drop the desired value from the index directly ??
### we are intersted only in official posts
# series have properties: name, dtype, values
# getting rid of columns and rows: drop
#get the mean prediction across all 
# plot sampling distribution
# load data
# checked, all tw ID has 18 digits
# Look at one ticket sepecificly
# Group the values together based on the RateCodeID values and assign the dataframe back to the 'bufferdf' variable.
# Get count of the stations by using scalar()
# np.where returns all the indices for which a condition is true
# Create a dataframe from the query above
# Show some sample data
# Parse the dimension2 (which contains the customer id and name) and split it into two columns
# Design a query to find the most active stations. List the stations and observation counts in descending order.
# Design a query to retrieve the last 12 months of precipitation data and plot the results # Calculate the date 1 year ago from today
# Merging event_list and df_test_user whrere event_start_at > created_on
# Fit your first decision tree: my_tree_one
# Reflect Database into ORM classes
# find historical data for 2010
#unique user_id = number of rows = 290584 => no duplicate users #how many countries do we have?
# Train the Naive Bayes model
# Download the data and remove the header
# We can fill NaN with another number
#Load the saved csv file, reading the site_no column as a string, not a number.
# sdks
#Return Series as ndarray or ndarray-like
# The columns of the local_sea_level_stations aren't clean: they contain spaces and dots.
#TODO
#highest # retweets
# Take a look at the series in the dataframe.
#Example: scrape link
# busco duplicados en las fechas
# Convert both dictionaries to series objects, using the series constructor.
# Insert new column at column index 5
# extract lonlat grid # plot
# Calculate the conversion rates by country
# Visualize the learned Q-table
# City and address columns are correctly filled
# Display location of saved model.
# a path without the root package name does not always work (e.g., it works inPyCharm, but not in Jupyter) # If the subpackage will not be imported later with a separate import command, it can also be included in initial import  # import package_within_package 
# Non-numeric Columns # bulk convert all non-numeric columns to equivalent hash values
# plt.rcParams['figure.figsize'] = (10, 6)
#import stats models to check the R^2 value of the model
#Get the sorted series of date to see the sample size for each date
#Declare an x-axis for our plot that runs the length of the data.
# put the data in the following form # | moveId | title |  # how many unique authors do we have?
# let's check the best estimator
#calculate the p-value
# Function to load in the first ssize rows of pre-processed train data
#A quick scan reveals this amazing user description:
# loading the data # new_pw(data)
# get just a time from a datet ime
#values = ','.join(str(v) for v in value_list)
#Question 3
# The 2015 version of the global dataset: # Local backup: data/sea_levels/sl_ns_global.txt
#convert html syntax to humanly readable characters
# Create a graph of the monthly complaint counts
# See how many disease labels overlap
# Tells us how significant our z-score is # Tells us what our critical value at 95% confidence is 
# get the convert rate of p_new under the null
# TODO: plot discriminating over position taken
# drop original ratings columns
# Save twitter data to CSV file
#import json back into jupyter  #print(op_ed_articles['full_text'][2])
# Author: Evgeni Burovski
#Get the sorted array of date
# create team games 2nd df with only unique games
# drop rows if any col has NA
# none_converted = df.groupby(['converted']).nunique().user_id[0] # Number of none converted users
# We'll also rename some of the columns so that the output is cleaner.
#X_train['age'] = df_imputed.iloc[:,0]
#The proportion of users converted.
# Read the item's data
# Instantiate the model
# Add values for CA, UK, and US
# bulk convert all non-numeric columns to equivalent hash values # X['ITEM_NO'] = X['ITEM_NO'].apply(lambda x: hash(x))
# drop tweets which startswith 'RT' --> get only original tweets, without Retweets
# The mean squared error
# This will actually build the network and determine which nodes are connected to which.  # Until now, only the rules were given and stored.
#not sure this showed much.  A little as expected.
#tokenize #view output
# Predicting val_dl to verify that it works
#Merge into X by using userid as a key
# Simulation under null for old_page
# 999=GPS geolocation, 1=country, 2=region
## adds release month as int ## I probably should have done this in one line ## or do I want to leave it in datetime format?
# check output
#Split the DataSet
# Tokenize the text 
# Read in csv file containing latitude and longitude information for each of the towns # Spliting each line so each town is a different element of a list
# The file I'm interested in parsing for cleanup:
# A scratch cell used to pull up details of a table column as needed  #voters.E1_110816.value_counts(dropna=False)
# If Q1 = yes; add 0 years to Q1A answer # assigning each answer to unique Q list
#'Omaha': 'a84b808ce3f11719'
### Fit Your Linear Model And Obtain the Results
# determine the order of the AR(p) model w/ partial autocorrelation function, alpha=width of CI
# Print percentages:
# A nice thing about Pandas is that series and dataframe methods output new series and dataframe objects # So we see that the type of value_counts is itself a series:
# compute difference from original dataset ab_data.csv
# import Quick Inventory of Depressive Symptomatology-Self Rating
# Take a peak at the data
# Spark sql will only work with table so register the dataframe access_logs_df as table #rename it as AccessLog as the query given earlier had the table mentioned as such.
# Add intercept column
## need EDA conversion to get this format, also datetime is UTC # example: df_movies['Released'] = pd.to_datetime(df_movies['Released'], infer_datetime_format=True) ## unit='D' instead of default ns
# examine the removed stop words including the data driven stop words
# manually renaming one exception to the following pattern # renaming the CHGIS fields in-place to conform to output specifications
# Create ParamGrid and Evaluator for Cross Validation
# Write data to excel file in sheet with labeled with exposure time
# Save all postgame data.
# eliminate all zip code outside NYC and change to integer
# We can use loc for masking and fancy indexing
# Read the Excel files. # The "new" file is the newest, unchanged file from SAP. # The "old" file is the older, modified the previous "new" file.
# Add two columns A and B.
# create a other modality for rare codes
# Histogram of dog_stage
# Specify an end point and how many periods should come before.
# plot de predictions.grade si predictions.prediction 
#                                   .size())
# unique users in total
#Works with Pix4D capture october 2017
#Define a dictionary with the numpy functions needed for the groupby that will be completed in the next step.
# similar API to scikit-learn and lifelines.
# join two datasets
# Pivot the DataFrame
#sample size for the new page aka the treatment group, is the same as the size of the treatment group
# Create engine using the `hawaii.sqlite` database file
# get 5000 random ids
# looking at tags of some randomly chosen queries # notice that most cities after 'TO' are incorrectly tagged as VB
# KEGG mapping of gene ids
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Look what the error is :
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
#sort by [0] to agg same plan_permutations
# convert 'C' to null
# Use engine.execute to select and display first 10 rows
# Log to Database
# Census Tract coordinates dataset
# Calculate the odds ratio of the variables compare to the baseline
#helper function
# Clean, tokenize, and apply padding / truncating such that each document length = 70 #  also, retain only the top 8,000 words in the vocabulary and set the remaining words #  to 1 which will become common index for rare words 
#                                    name it will be given in the file
# Describtive stats for tips
# get the dataset we just uploaded
# Select all from mesurements table
# Nearzerovariance function inspired from caret 
# find the table # for row in table_rows: #     print(row.text)
#the Null hypothesis states there is not difference between the  #conversion rates of old and new page. Thus, the whole dataset is used to  #calculate the convertion rate for the new page (pnew).
# 1 column is redundant with the index: 
# print (new_df_left[new_df_left['unitcnt'] == 70]) # print (new_df_left['landtaxvaluedollarcnt'].mean())
# Alternative way to get the date only 
# Summary of results
# When the completed dates are present, the complaint is closed
# Get all games. # Get set of all game dates.
# Create a histogram of your residuals. Describe anything you observe
#try two sample t test to see if the means of these distributions differ
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'NYC' # builtins.uclresearch_topic = 'FLORIDA'
# Assuming 95% CI for one-sided test, as stated in part II.1
# reevaluate the pipeline
# Here we are adding a "date" column that can be used for grouping operations
# Three weeks of Data
#Temperature - fill gaps under 3 hours before aggregation
# check inserted records
# rename the columns
# function to show the yearly intervals
# log reg
# Creating a new column for year and month
# one time only
# Assign classes to a variable
# simulating n_old transactions
#Converting CamelCase to snake_case
# figure out what was the spike at 5 am (related to 5-5:59 am tweets) on Small Business Saturday 
# Use Pandas to calcualte the summary statistics for the precipitation data
# 8 rows removed
# Get best partition
# final dataframe
#boxplof for antiguedad feature
# Here's where I apply the train test split
# Print the head of airquality_pivot
#cols = ['sentiment','id','date','query_string','user','text'] #df = pd.read_csv("F:/web mining/webmining project/crawler/annotation_data/Samsung_with date_Merged_final.",header=None) # above line will be different depending on where you saved your data, and your file name
# Retrieve the brand names as a list
#Check count of missing address1 value 
# read dataset # inspect dataset
#df['name':50].groupby('name').count()
# get values
# Create a string variable which will be used to get all of the different years data
# use the keyword index
# notice the change in encoding in the output
# Do this last - it's easy. Don't forget to create a copy.
#join the all the element sin lines list witn @@@ at the end of each line to perform operations #use this below to split at @@@ and then save it to a md file #content_joined.split('@@@')
# Create a box plot.
#df = df.drop(['Lon', 'Lat'], axis=1)
# number of unique users
#Create a data frame from the Row objects
# Find total number of stations
# get indices for any other keys that are part of data
#payments_all_yrs_ZERO_discharge_rank = payments_all_yrs_ZERO_discharge_rank.reset_index() #payments_all_yrs = payments_all_yrs.reset_index(drop=True)
# Reflect Database into ORM class
# Since the error kept happening that the chisqprob attribute does not exist, I implemented a workaround.
# Instantiate your read-only reddit instance using PRAW # You should have these set as environment variables
# Current directory
# plt.savefig('Days Between Purchases Chart.png')
#Find z-score and p-value
# RE.FINDALL
# Insert tripduration_minutes into df as a column # loc=1 will set location at index1 # value=tripduration_minutes - sets values of the column to pd series list made above
# Show us Twitter prices on Fridays in Q2 # Note we use the boolean '&' here, not the logical 'and' which will throw # a complicated error message.
#List the unique values inthe CharacteristicName column
# print summary of regression parameters
#Probablity that an individual received new page
# Show the row information for the repeat user_id
# predict # round predictions
# Loading the CSV
# Grabs the last date entry in the data table
# Merge csvs (2)
#For example, if we drop the NaN points for this release, can we genearate a quality indicator for all the estimates?
# Calculate the baseline accuracy. This is the probabiblity of predicting the city correctly, by always # guessing the majority case i.e., not Las Vegas.
# n_new transcations conversion sample values
#The model without the new factors of UK_ind_ab_page and US_ind_ab_page included for comparison with the model above
# Set environment variable + check # export instagram_client_secret=91664a8e599e42d2a6a824de6ea456ec # echo $instagram_client_secret
## expand full path to token ## create env variable TWITTER_PAT (with path to saved token) ## save as .Renviron file (or append if the file already exists)
#data file ####Using Pandas to load data #xls_file = pd.ExcelFile(filepath)
# Total dates
# display unique values with counts for neighborhood_district
# Read the text file tweet_json.txt
#Lists the unique 'Country' values within the table.
# Get first Transaction of each user, use it for left join with user
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# the probability of an individual converting in control group
# 13 files were converted as follows # where the filename 'mlbcontests6.26.2017-7.2.2017' was changed for each file
# p-value
# Pass the Sampling Feature ID of the specimen, and the relationship type
# Checking whti a fake user whose craeted_on date is later
# Call sns.set() to change the default styles for matplotlib to use Seaborn styles.
# json objects are interactable in the way python dicts are.
# checking shapes before fitting a model
# Negative tsla and tuna <=> only active on D0 # tsla_tuna_neg_DF = tsla_tuna_neg.toDF(["cid","ssd","num_ssd","tsla","tuna"]) # tsla_tuna_neg_pd = tsla_tuna_neg_DF.toPandas()
#Specify filename to save to #Save edge parameters #Ext_input.save_edge_types(filename=input_edge_types_file, opt_columns=['weight', 'delay', 'nsyns', 'params_file'])
# check fineness of feature_col
# Which news got the most attention by the politicians ?
# Dropping the rows where mismatch occurs
# Determine post age odds ratio 
# Use the Inspector to explore the database and print the table names
#check url links submitted by suspicious accounts
#Alternate method for reading sql with pandas
# read in data from HDF5
# reset the index to the date
#initial visual assessment 
# Labels for the legends of the scatter plots below
# Visit URL
# plot autocorrelation function for first difference of doctors data
# Extract the first name by asking for a contigous group of characters at the meginning of each element
#Analyzing distributions
# Using Arctic database to store # Create the library - defaults to VersionStore # Access the library
# Divide each number by each countries annual maximum
# now, just re-order the columns and re-assign to same df
#Convert event time to datetime
# plots key elements of the universe selected
# save data to csv for reloading
# Upper value.
# skip only two lines at the end # engine parameter to force python implementation rather than default c implementation
# Save the file and run this block
# check the first 5 cards in the Dataframe containing cards from the Ixalan set
# C.Given individual in treatment probability of their conversion  # D. Probability an individual recieved new page
# create dummy variables for country columns
# Create an extractor object: # Create a tweet list as follows: # Print the total number of extracted tweets
#Find out if there are some null values
#We must convert the Polarity to string to be used as a map attribute
#Design a query to calculate the total number of stations. #New DF grouped by station with new column 'count'
# Save the DataFrame as a csv
### Creating dummy variables
# specify a new set of names for the columns # all lower case, remove space in Adj Close # also, header = 0 skips the header row
#old df value counts before cleaning in wide format
# merge 'yc200902_short_with_duration' with 'yc_trimmed'
#Displaying the proportion of users converted
### Create the necessary dummy variables
# custom grids for every degree # vectors # mesh grid array
#Visit the url for JPL's Featured Space Image here.
#Access first column
#Lincoln': '3af2a75dbeb10500'
# Let's clean them up a bit:
#station_id_col
# removed string word comment
# Instantiate a new MGXS Library from the pickled binary file "mgxs/mgxs.pkl"
# sklearn version: 0.18.1 # build a variable as tip_percentage, filter out $0-fare trips
#join the country data frame with the converted rate data frame
# How many movies do not have a homepage? # .count() conta il numero di righe che non sono NaN per ogni colonna, quindi nel caso di homepage darebbe 0
# customer creation distributed date
# filter dataframe by rows that don't have values in retweeted_status_id
#tweet_ID should be a string, and hashtags return a "[]" instead of Null.  #also I am seeing that created_at time should be seperated out to date and time and day of the week.
# Merge the two DataFrames based on DateTime  # using inner join with price data as left (since price data is shorter)
# group your sources and take mean of compound
# Charts: Histogram for time for closing a ticket
#pandas #transactions_pandas = pd.read_json('data/transactions.ndjson', lines=True)
######################### Combine Data Frames ##############################
# how many new page
#mean speed group by weeks
#Assuming success rate of P_n and P_o are equal
# building a dataframe with columns 'features' and 'relevance scores' # Since, the relevance score is compute over the embedding vector, we aggregate it by computing 'mean' # over the embedding to get scalar coefficient for the features
#Number of unique users in the dataset
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
#################################################### # LOADING THE DATA ####################################################
# The Dataframe
# append new records to historical data
#delete the retweet rows keeping the original tweets
# first thought is to check out some trump tweets
# set datetime
#import dataset
#Using the statisics library
# TODO: overall validation score in one number.
#the WOEID of 2459115 is NEW YORK #url = 'https://api.twitter.com/1.1/trends/place.json?id=2459115'
#Converting the duration equal to total seconds
#Check duplcated rows by all columns at first.
# convert string to timestamp # calculate the time difference in hours between signup and purchase
#copy dataframes to preserve original while cleaning 
# id and sentiment are categoricals
# now creating 'metro_area'
# All the Outliers seem to have the following properties: state == YY and specific donorid. # Plot the remaining data outside of these to check that we caught all the outliers.
# first, value counts can show us in text form what we're looking at
# Calculate summary statistics for each variable
# Evaluate DataFrame after changes
#Check via customer sample
# Define X (features) and y (target variable) for modeling: # Train/Test Split!
# Which features play the biggest role in predicting campaign success:
# First, import the relevant modules
#Makes sense
# P1 misses: # P2 misses:
#probability of an individual converting regardless of page they receive
# calculate convert rate p_new
# Double Check all of the correct rows were removed - this should be 0
#Import regression model library
"""Assuming  the null hypothesis,  P(new) and  P(old) both have "true" success rates equal to the converted success $ rate regardless of page"""$
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Insert column for Month Ticket was Created, Month Ticket was Closed # Insert column for Calendar Year ticket was Closed #df['Calendar Year Closed'] = df['Date Closed'].dt.year
# Am I doing coreectly until here?
#Delete the duplicates
# move the cursor to the right of the period and hit tab
# To access a particular row, we use the index position:
# Can you tell me the average low for the week?
#task 3: Calculate what the highest and lowest opening prices were for the stock in this period #create a list to store the Opening prices for the stock #There are some None type values which we need to take care of.
# train_ratio = 0.9
#Created a scatter plot between 2015 Sales Q1 and 2015 Sales
# the sum of misalinged groups
# convert it to pandas data frame
# drop rows where our value of interest is unknown # if you drop the original column you'll lose partial atbats. sometimes it's null for only some pitches. dropping it loses event data.
# entity visualization # after you run this code, open another browser and go to http://localhost:5000 # when you are done (before you run the next cell in the notebook) stop this cell 
## one way or another, by this time, we can merge in the list version, cpdi, to get ## a complete 'eppd' dataframe.
# Processed
# opening json file #     pprint(data)
#Import all data available at the hackathon
# Dados de BO Roubo de Celular
#Load CSV
#### Define #### # Join all three tables using unique key 'tweet_id' # #### Code ####
#create arima model
# resampling degree day data. The first line takes the average when a given hour has multiple entries. # The second line generates daily data.
# convert date from string to date
# filter the df with old_page in landing_page # count the number of user recieved the old page
#Save figure
#Drop unnamed index column as Panda adds its own one. Run this once only
#drop the columns which are not having values atleast in 4 rows
#import plotly.tools as tls #tls.set_credentials_file(username='drozen', api_key='GTP8SX2KBqr3loYdTVb6')
#create a temp df that has all the sentiments broken out by applying the get_polarity_scores function
#Simulating the amount of conversions given the amount of users and the possibility of conversion
# The 'Balance' column actually stores the same value for every record - so delete it, and fix later
# seeing sections of the table in turn
#Save figure
# drop dry because it is really common
# make a temporal index by setting it equal to CREATED_ON
# Convert p_diffs to a Numpy array and plot a historgram of the data
# summary() function failed to work. Used summary2 instead
# A one liner as I wanted to explore using islice and thought it should be possible
#Convert time_stamp2 to day of the week
# Adding Multiindex keys
# Making a copy on which we will perform the transformations
#Save the table directly to a file.
#Plot of favorite_count and retweet_count over time
# Select a particular Series group from the original DataFrame group by referencing it's column name
#print key["API_KEY"], key["API_SECRET"], key["ACCESS_TOKEN"], key["ACCESS_TOKEN_SECRET"]
# max miles: 26,735 miles, let's investigate # Data error/discrepancy : total fare of $20.36 for traveling 26.7K miles 
#Drop NaN.
# Reading the dataset in a dataframe using Pandas
# load the data
### find the difference between the simulated proportions
# reg_target_encoding(train,'Block',"Real.Spots")  # tbd # reg_target_encoding(train,'DOW',"any_spot")   #tbd # reg_target_encoding(train,'hour',"any_spot")  # tbd
# get the index of the high values in the data
#convert all columns
# quick sanity check
#### Define #### # Change format in the 'time' column # #### Code ####
# custom grids for every degree # vectors # mesh grid array
#Store the final contents into a DataFrame:
# Groups the records by NewsDesk, SectionName and Popular and counts the number of values
#store the data in the image_file dataframe
# drop the rows where the control group is incorrectly aligned with the new page
#quandl.get("NASDAQOMX/COMP", authtoken="yEFb5f6a7oQL91qzEsvg", start_date="2003-01-20")
# generate df with pca coordinates, variables are presented as rows, thus the index should be variable names, the columns represent the different PCA axis 
# Connect to the database hawaii.sqlite # Import the data from the measurement table into a dataframe
# WIthout replacement
#Simulates Nnew with Pnew using np.random.binomial function and stores the values in variable new_page_converted
#Write out data to csv
# the attribute of the function (default parameters)
# Global Variables for eacch file # Empty List to store the cumulated jams and irregualrities data
# Transform percentage into float
# Reflect Database into ORM class
# find historical data for 2008
# Convert the dataframe into proper format which fbprophet library can understand
#Converts df slice to demographics sql table
#Convert ages to integer number of days, and append to list of test features
#restrict the data to age between 18 and 105
# the probability of converting regardless of the page
# If python2 is not loaded kill the hypervisor # ! kill -9 -1
#Test=pd.melt(All_tweet_data_v2, Remaining_columns)
# Replace missing gender values 
# print the newly created file
#Renaming the Column Name
# create a column with the result of the analysis: # display the updated dataframe with the new column:
# This new column determines what SP 500 equivalent purchase would have been at purchase date of stock.
# Convert the returned JSON object into a Python dictionary
#print first 5 rows to show clean role description data
# Checking the frequency of the user_id column values
# Our "Date" looks like strings.
# various options in pandas
# maybe a pie chart showing the different users and the magnitude of their contributions is the total number of lines changed/removed/added??
#models trained using gensim implementation of word2vec
# Find duplicated user
#Display the dimensions of the data frame
# save transformed data
## Reproducing R example given in Business Context Document
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
"""Use 'to_datetime' function to convert date_account_created, date_first_booking, $ and timestamp_first_active to datetime format. I suspect that month might be a useful feature $ for exploring seasonality, so once converted I'll extract the month from each."""$
# this model doesn't perform as well # i think additional variable will be needed to improve this
# Add labels to the x and y axes
# export the df_events as a csv to data output folder
# Step 9: Convert pivoted into numpy array ## Use fillna(0) to fill in missing data
# proportion of the p_diffs > actual difference observed
# TODO for now, just delete the song
# don't forget to change url path to that of your device. Ex: url=("/Users/name/folder/detroit_census.csv") 
# The convert rate for p-old under the null is: 11.96%.
# Creating a summary by month
#put data file 1 into a pandas df #parent node is 'page' #print first 5 rows
#Getting rid of spaces before the state name
# this period can be used as index in data frame # index which has period is called priod index
# Perform regression on countries # Fit our regression model
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#let's plot the hour of the post against how many retweets to see the optimal time of tweeting to reach others #it looks like, from this sample, the best time to tweet if you want to reach more people is from 11pm to 1am
### END SOLUTION
#limit dataframe to only original tweets
#Example2:
#Create a mask of rows measuring NO3 #Convert the TotalN values in those rows from NO3 to N
#removing doggo, floofer, pupper and puppo columns
#extract the url of background-image 
# reflect an existing database into a new model # reflect the tables
# Use Pandas to calcualte the summary statistics for the precipitation data
#We need to compute the critical z-scores for a 95% confidence interval
#Also checking another way that no NaNs remain in location
sql_lagou = """$     select * from lagou_recruit_day$ """$
# similar items per item
# example of a cross-listed dealer
# The standard deviation of the sampling distribution for new_page
#provide the summary for aur model  #stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
#Change date to datetime
#3. Run `python capture-tweets.py <topic>` to save tweets to file called `captured-tweets.txt` related to  #`<topic>`. I.e.: `python capture-tweets.py Google` or `python capture-tweets.py Iran`
#Need to put tweet in df with time stamp, sentiment analysis and tweet. 
#####list the datatypes 
# determining the first real commit timestamp (by Linus)
# DC Min and Max # Election periods: scottish referendum (pre premiership), pre 2015, and pre 2016 referendum
# Handling `\xa0` and all the other characters like that that might appear need more machinery. # For that, we use regular expressions: https://docs.python.org/3/library/re.html
### Dataframe containing primary (most important) features: df_us_ ### Drop columns that will be replaced with 
# train.summary()
### Create the necessary dummy variables
# important: 'parse_dates=True' when reading docs with dates!
#Convert unix date format
# Drop non-normalized scores of Brokerage and Betweenness
# Fill missing values with 0s
# lets examine how things change based on day of the week
# simulate distribution under the null hypothesis
# perform count on dataframe composed only of non-crime criteria # but append that information to original dataset
#This section prepares the dataset into a binary model which can then be used for logistic regression modelling
#This is what the table is going to look like.  #There are nine categories. Above is a short description of each column. 
# Getting the values and plotting it # plt.scatter(f1, f2, c='black', s=7)
# Extracting FOX news sentiment from the dataframe
# For getting all the links found in the nav bar
#tweetdf.to_csv("tweets_w_lga.csv")
# by year and in-season vs. offseason
# What are the most active stations? # List the stations and the counts in descending order. # This is the SQL Alchemy way to do the same thing
# reshape the data
#Third Data Set:
#displaying the p_old
#deaths
# use defaults, let primary key auto-increment, just supply material ID
# create grid id array
#len(df2[df2['converted'] == 1])
# treatment group proportion of conversion
# export DataFrame - records option (list of records)
#we do the same as we did above, but covert it into a Pandas dataframe. Note this takes quite a bit more memory, so will not be good for bigger data. #view the dtm dataframe
# Baseline (time_of_day = morning)
# mean and variance w/in each group
# In Pandas DF # Average all polarities by news source # View the polarities
#Convert 'M'/'F' to 1/0 # Converting binary variables from strings to numbers
# get dr number of unique providers for each dataframe
# Save weights from the model, allows prediction without retraining and sharing model with others.
# put data into a data frame for charting
# Get genotypes that have associated blood type phenotype
# count the converted rate regardless of the page and store it as p_undernull 
# Spatial distribution of complaints
# Summarize the scrapped documents from hot.json
#transit_df_rsmpld = transit_df.reset_index().set_index('DATETIME').resample('1D',how='sum')
#df.drop_duplicates(subset=['last_name'])
# load car csv 
# one dataset that is really interesting is the FDA adverse events dataset  # this can be downloaded straight from Kaggle: https://www.kaggle.com/fda/adverse-food-events.  # I have loaded this directly in the GitHub though. 
# save to excel file in worksheet sheet1
# Store the experiment, and display the experiments uid.
#Compute the number of unique users who has new page using df2 dataframe #display the number of unique users who has new page
# calculate all Wednesdays between 2014-06-01 # and 2014-08-31
# Set negative values in Tip_amount to 0
# Reading the URL into a DataFrame
# Delete the datastore # to delete all datastores in the path, use: # pystore.delete_stores()
#Created an empty column UserID #Arrange the column name as desired by using reindex
## this freezes the datetime of when the object was created
# number of tweets labelled as mortality salience during missile threat crisis period
# Creating an X for final analysis
# Conversion rate GIVEN an individual was in "control" group
# Naives Bayes with Kfolds on our Test Data
#Lets check out the languages used #I wonder what 'und' is!?
# df['Descriptor'].groupby(by=df.index.hour).value_counts().plot() # df[df['Descriptor'] == 'Pothole'].groupby(by=df.index.hour).count().plot()
# observe results
# The number of old_page entries in the dataset
# Since the coefficients for US and CA are negative, it is better to calculate a decrease by providing the reciprocal
'''$      using Dataframe.drop with our 2 conditions$ '''$
# Euclidean Distance Caculator
# Time series comparing Likes vs Retweets 
"""Merge LDA output and DF"""$ #Make LDA corpus of our Data #make dense numpy array of the topic proportions for each document
# Use catplot() to combine a countplot() and a FaceGrid()
# YOUR CODE HERE #raise NotImplementedError()
#Only numeric columns
## to get the summary of the counts ##
#checking if there are any nulls in countries_df
# Fitting the logistic regression model
#backlog
# This is perhaps not 100% intuitive - the value given will determine the # number of ticks appearing between each major tick; it has no relation to  # the scale of the actual values being plotted.
#Replace '|' with ',' and then replace ',' with ',\n' to save the entire dataset into an excel
# Train our SVM classifier                  
# Convert the "created_at" field into data/time stamp
# write the DataFrame to disk
#Using a for loop(s), compute the sum and mean for each column in scores.
#Proportion of users converted
# For retrieving the xml content from the sitemap
# How many stations are available in this dataset?
# Get a list of column names and types # columns
# How many stations are available in this dataset?
# as seen above, 290584 entries now as entry with index 1876 is deleted # we can confirm by checking unique values of user ids
# Find the number of times a treatment was not associated with with landing on the new_page.
# Create a list of lists of the data
# commentator group polarity distribution of news
# 6. 
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
### output json file
#eliminate duplicate measurments
#Read Reviews csv into dataframe
# extract temp grid as a 843 by n matrix # check shape
# Total number of stations
# reducing our "train_4" sparse matrix which has already been scaled # this crashes the kernel at 1000 and 100 components...
# Select only the `date` and `prcp` values.
#Save figure
# Eliminate replies , i.e. only chose the records with `in_reply_to_status_id_x` is Null # Eliminate retweets, i.e. retweeted_status is Null
# directory
# convert utc to est? Find out what tz it is first.
# Check for strings that doesn't end in any abbreviations
# Number of unique users
# Export to CSV
# sortedDF = dateCountsCloudyQB1.sort_values('dates')
# what are the newest ships #df_l[['ship_name','year_built']].sort_values('year_built').drop_duplicates('ship_name').tail(10) # gives NaN
# convert pandas data frame to arrow table
# Volume    5923 non-null float64,  not 8885
# Display the unique values of column L2
# Create a column with the result of the sentiment analysis # Display the updated dataframe with the new column:
# the apply() function here should probably be split up; it's messy!
#Training data with extra dummies
#propiedades entre 100 y 125 metros cuadrados
# Take a peak at the first couple of rows in the dataframe
# pandas can handle these issues
# sentiment_df.head(100)
#Check dataset features
# Filtering the Dataframe so that it only includes the correct allignments:
# define a parameter grid of the "important" tuning parameters
#Filter and create a new RDD
# Merge the urban created data frames on the name of the city
# EXTRA COLUMNS IN V_INVOICE_LINK
# import and instantiate TF-IDF Vectorizer
# Create the necessary dummy variables
# Let's create two Indexes for experimentation
##### key, value pairs
# Let's see the correlation matrix 
# Initial Table Head
# examine rolling volatility
# the probability of converting for control group
# Convert sentiments to DataFrame
#Remove header row
#now drop extra columns
# Create a pandas dataframe # Display first 10 elements of the dataframe (ie. 10 most recent tweets)
# list(.zscan_iter(key))
# Fit the Pipeline on train subsets
#probability that an individual received the new page
# checking the number of rows where new_page and treatment do not line up # storing it in a variable
# Check is Data is imbalanced
#Plot the min, avg, and max temperature from your previous query as a bar chart. #Use the average temperature as the bar height. #Use the peak-to-peak (tmax-tmin) value as the y error bar (yerr).
# df_2017
# Creating a X for when the model being analyzed is TF-IDF
#df2
# MAC
#sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') # instead of the variables.
#Calculate the highest opening prices for the stock during 2017
# Reading the file    
# dicts to map numbers to words/labels
# Principal component analysis (PCA). Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. # run exact full SVD - Singular Value Decomposition calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing
#Because gender and age of customers are static infomation which might not related to our case, we exclude them out
# Remove any games with bad dates that must be excluded. # Also get rid of 2012 games. # Clean up index.
# exact match
# Read the Dataset
# Verifying correct tweets from each source
#  Population size should double, expect (32084,)
#method array to pandas dataframe
#remove repetitive columns
#Determine ciritcal value at 95% confidence
#total number of users
#test
#Find Mean Sentiment by news org
# convert jsonData to pandas dataframe
# 15. Create a Data Frame, called demographics, using only the columns sex, age, and educ from the free1 Data Frame.  # Also create a Data Frame called scores, using only the columns v1, v2, v3, v4, v5, v6 from the free1 Data Frame
#showing actual difference in observed CTR
# apply Linear Regression
# select column needed
#Read from another csv file at the given location & display first two records
# replace vendor id with an equivalent hash function
# use statsmodels to import regression model #import statsmodels.api as sm
# Arrow positions per country combinations
# check if more than one user using same ip
# Upload files to the buckets.
#Checking what proportions of values are greater than the observed difference in the simulated values of differences
# write to db
# connecting to image predictions tsv file via requests library
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
# Using agency 'METRA' from first feed
# need to get rid of that empty value in score; subbing in an average of two bracketing scores # 0.187218571
# now let's plot that obs_diff on the histogram of simulated p_diffs
#Create data frame with stations, dates, and temperature
#     print(screen_name)
#load the dataset json into a dict
# Save the clean dataframe to csv 
# Fetching datasets...  # Still unclear what this gives besides dataset names # Warns that this is "non-public" CARTO API and may change in future
# the most popular words which might contain useful information in the reviews
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Import first Site Visit file
# the money each user spends on renting
# get only the index of the Series
# Add a normalizing column for each song in each region to convert to streams per million of populace
#Sort our recommendations in descending order #Just a peek at the values
# use this cell for scratch work
# The unix epoch is the number of nanoseconds that have elapsed  # since the universe was created # i.e. January 1, 1970 (midnight UTC/GMT), not counting leap seconds)
# want to run live rows that have an outcome in 9_15 dataset through the algorithem to see if it predicts the # correct outcome # filtering for lives rows:
## ------ Avergae Polarities of each news source ------ ##
# lets find the most retweeted tweet
# the above p values 0.1291 and 0.4558 clearly states no significance
# dtypes shows the type of each column in the data.
#exponentiate the coefficient of the variables
#creating interaction terms columns for the interaction between page and country
### Step 15: Fit the model and then make predictions ## The labels will identify if we are in the 0 or 1 cluster
#count of treatment group that is our sample
# Use SQLAlchemy automap_base() to reflect your tables into classes # 
#hierarchical Indexing
# Find all the json zips that have been scraped.
# Modified the dataframe being evaluated to look at highest close which occurred after Acquisition Date (aka, not prior to purchase).
# use string format codes to get the weekday
### Create the necessary dummy variables
# There seem to be 51 wards, 0 through 50
# To set the data path of json dataset
#adding dummy coloumn
# Use the Base class to reflect the database tables ### BEGIN SOLUTION ### END SOLUTION
#Converting it to Pandas df for better view
#Find out n new
# Find n_new by counting querying the number of times the landing_page is the new_page
# Construct new columns for all country/page combinations 
#model.most_similar('yea.r.01',  topn=15)
#create an empty list to put all the closed values #go through the data list and append the closed value
# shorten the names and add a total column
# Latest Date for station 'USC00519281'
# Get a random sample of 200 entries from the dataset. # Simple scatter plot
# stock_data['TIMESTAMP'].iloc[1:10]
# join the datasets and then check the resulting dataframe
#create a min and a max date
#### Define #### # Remove all tweet record that have a non-null retweeted_status # #### Code ####
#getting inflation data and creating a new dataframe 
# Translated role
# Sample Series object
#take the mean
# Analyse dataset
# scan through all the rows/tweets which did not have a dog_stage assigned # append the result in the dictionary
# With effect modification # drop1(adj.glm,test="Chisq")
#G = nx.Graph() #G.add_edges_from(zip(df['userFromId'],df['userToId']))
# Using binomial distribution to find samples with probability p_new
# create CSV file of signals for each channel
# quick check before diving in
# Remove the oldest row by timestamp and keep the latest one # see the info on our new df2
# create a column for sales from January through March for 2015.
# last 12 months qty received
#url for mars image
'''Performing Polynomial Features on the Pending Ratio Column'''$
# Convert release_date column to datetime object
#Verifying the DataSets Created
# the contents of the date column
#the first parameter is the number of successes and the second is the number of trials
#flatfile generation
# Remove rows with missing values in column 'driver_id'. The order was not accepted. there was no trip
# flatten y into a 1-D array
# common observations in history and train ?
# Surveys by day
# merge the tables
# Use Pandas to calcualte the summary statistics for the precipitation data
# with start- or end-date * periods:
# le_data_all.pivot(index='country',columns='year') # examine pivoted data
# Importing data
# Create a Beautiful Soup object
# Removed outliers for visualization purposes
# Find conversion rate for p new under the null 
# lets fit logistic model for the countries, baseline = CA
# plot the results to get a visual idea of their relationship and frequency # Also, it would seem they give the secretary of state a lot of directions.
# Preview the Data in the station dataframe...as expected from the above...there are 9 unique stations
#group by breed and store the means of retweet_count and favorite_count. #order by retweet_count and favorite_count. #plot the top 15 average counts.
# 24 + 4984 = 5008 # found 8 dealers listed with both HYB and STD
####only have 4, 5 ,6, three months totally
# read in again, now specify the data column as being the  # index of the resulting DataFrame
# Calcuate z score and p value
# Create copy of dataframe
#plt.axvline(x=obs_diff, color='red');
# Let's check number of entries in df2 after deleting duplicate record
    """$     Solar Incident Radiation Based on yearly guess & month.$     """$
# Create ccp with percentage of total market cap
# Load the dataset # Check data size
#pd.to_datetime(citi_df['starttime'], format="%m/%d/%Y %H:%M:%S")
#Need to remove the extra date columns, otherwise good!
# Function to only select the upper case characters from a string
# due date distributed
# OAuth process, using the keys and tokens
# Concatenating the two pd.series with a pd.DataFrame
# Save our dataframe as a CSV
# Create new columns to see an interaction between page and country 
# unfortunately the index is numeric, which makes # accessing data by date more complicated
#Design a query to calculate the total number of stations.
# Perform a query to retrieve the data and precipitation scores
#simulating and displaying the results
# add intercept and ab_page columns with values based on the conditions above # check if it worked
#Review summary stats for the data set
## create a new column 'score_str' which will hold a value about the score - is it below average or above average?
##Extract year from Date
# Downcast Only # data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/ef3f532b-7570-43d9-b016-6b58c4429b15/dar24011.asc' # Down and Up Casts
# Save the query results as a Pandas DataFrame # df = pd.read_sql(session.query(Complaint).filter(Complaint.id == 2).statement,session.bind) #precipitation_df = pd.read_sql(precipitation_data, engine)
#Add newly created Month feature to the list of features
# Add code here # Get the accuracy of each model and the overlap between the model
# create Series from dictionaries
# Having some issues saving to pickle, so I'll save to CSV, reupload, and do the downcasting again before I save the pkl #shows.to_csv("ismyshowcancelled_tmp_1.csv")
# ResultDateTime range (min & max)
#11. Get the subset rows excluding 5, 12, 23, and 56
# See if new features has properly created
#final_df.dropna(inplace = True)
#word = re.compile('\w{3,}') #test_df["num_description"]= test_df["description"].apply(lambda x: len(word.findall(x)))
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Compute sentiment for each tweet and add the result into a new column: # Display the first 10 elements of the dataframe:
# Creating new columns for interaction between page and country
### Create the necessary dummy variables
# Import library # Tells us how significant our z-score is; see: http://knowledgetack.com/python/statsmodels/proportions_ztest/ # Tells us what our critical value at 95% confidence is; see: http://knowledgetack.com/python/statsmodels/proportions_ztest/
#Sort the DataFrame values by `date`
# We convert the Date type:
#df.parse(sheet_name)  # read a specific sheet to DataFrame
# Search for '#Australia'
#3.created time #from lastest to earliest, lastest should have more interest
#There's only one numeric column in our data so we only get one column for output. #If there were multiple numeric columns we would get more columns.
# check shape of empty grid
#Tienen columnas diferentes #print(sells_primer_2015.keys().size) #print(sells_segundo_2015.keys().size)
# Plot scores obtained per episode
# Count the new pages
# Add chanel column to users table
# another way of importing the file
# Import the modules
# prob_treatment_converted - prob_control_converted is the actualy difference observed
# Find meaningless columns (<=1 categories)
# Durations
# Replace NaN with a scalar
#next we move on to the df_image_algo_clean data #first, copy the data. 
#Training data #Test data #Sample Sumbmission
# Note the differing start dates between the 'OH_R_con' Series and 'OH_D_con' Series # That is why have to put all in the same dataframe to harmonize dates for taking the rolling mean
#pd_exec_time.strftime('%Y-%m-%dT%H:%M:%S')
# Remove the seconds in the timestamp column
#finding the important features
# create a timestamp of July 1 at midnight
# The proportion of p_diffs that are greater than the actual observed difference.
# The next line causes Pandas to display all the characters # from each tweet when the table is printed, for more # convenient reading.  Comment it out if you don't want it.
# Read the variation of electricity demand from Data.xlsm
# get life expectancy at birth for all countries from 1980 to 2014
# copy an empty database file "empty_db.sqlite" to a copy named "db"
# Save the dataframe to csv
# Change the number of bins.
# Creating a new dataframe df2 such that it holds all values except mismatched values # Dropping all mismatched values from the df2 dataframe
# Spreadsheet
# Get Guido's text: guido_text # Print Guido's text to the shell
#Chicago
# Total number of files for each assignment
# display unique values in precipitation_inches column
#getting rid of $ and comma signs and type cast the column to integer
# Calculating  how representative the 2nd df is of the 1st df through the proportion of their count volumes 
# Applying function to "Stocks" 
# first column [0] is target (outcome variable) # second column onward [1:] are features  # we keep the feature name in X_header
#Select lang field and counting returned values from the table
# Show results
# take a look at the dataset
# Simulate conversion rates under null hypothesis
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# If the error `No module named 'matplotlib'` happen, execute: # pip3 install matplotlib  # on terminal.
#propiedades entre 150 y 200 metros cuadrados
# df = pd.read_excel("msft.xls", sheetname='aapl) #sheetname
# City
#Multiply the genres by the weights and then take the weighted average
# calculating or setting the year with the most commits to Linux # print the result
#get the number of weeks in September 2016, so 9/1 - 9/7 are week one, etc..
#Save to a csv file 
#Creating and fitting the regression model 
# What countries are in the country column?
# finding the timezone of the max retweets (cats) #the max is 18857 retweets
# Create a histogram of all of them
# This gives a list BS ResultSets for all articles. #h2_content = [item.find_all('h2') for item in article_divs]
# read in the country table
# Fit Model
# To get the total number of unique values for users
#get the top 20 brands
# preview of the data
# at the moment needed because the comments are overflowing on following line
#It's the same as the success conversion rate regardless of the page
# little bit of clean up
#Saves the final dataframe to a CSV to be compiled in another notebook
# proportions of user who converted (i.e. converted = 1) # alternatively
#calculate the recipricol
# View the precipitation table
# convert retrieved records to a dataframe
# Import two methods from the DOTCE Class `report`.
#view part of the pos_sent variable, to see how it's formatted.
#Test to make sure connection is working.
# check the renamed dataframe
# predic y 
# Initializing an LSI transformation
# Create engine using the `hawaii.sqlite` database file
#print first ten rows of theft data
# check and display names with lower cases
# Check the accuracy of the trained model with early stopping
#Which Tasker has been shown the least?
#Export Canadian Tire file to local machine
# get values, note that the result is a multidimensional array
#women_in_medicine_save = 'wikipedia_women_in_medicine.html'
# change rating columns to floats
# easily count the data rows now # note this data is continuous, every few minutes, so a count isn't really helpful but nice to know it can be done
#Removing the duplicate record which has the same user_id
# Retrieve the NuFissionXS object for the fuel cell from the library
# check option and selected method of (11) choice of groundwater parameterization in Decision file
## So setup our graph so that we have unique users, tweet id's, and hashtags
# Checking for missing data
# Find the key of the dict
#41% of those who pair at <3m, subscribe <3
# since we no longer need the urlname, let's drop it and free up some clutter
# Checking missing values
# And this is a plot of these:
#Number of samples of old page
# Test:
# driver = selenium.webdriver.Chrome(executable_path = "<path to chromedriver>") # This command opens a window in Chrome # driver = selenium.webdriver.Firefox(executable_path = "<path to geckodriver>") # This command opens a window in Firefox
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
### Create the necessary dummy variables
# Send and catch the request: r
#check that there is an observation for all 132 months(12 months * 11 years)
#Review summary stats for the data set
# Data Understanding
# make sure tweet_id in all 3 dfs are of same datatype 'str'
# Probability of treatment group converted
# In the following we are just picking up the first link that shows up. 
# by districts covered
# Even if you pause the display of tweets, your stream is still connected to Twitter! # To disconnect (for example, if you want to change which words you are searching for),  # use the disconnect() function.
# df.groupby([df.created_at.dt.month,'product_version']).count()['Id'].reset_index(1)
# get the current date and time (now)
# Call the function
# Plot data in a Series with the plot method
#This puts hotel_cluster back with user ID and hotel market to I can calculate the most popular clusters
# Start of time estimation for the notebook.
# A:
# Show all Plants with invalid commisioning dates
# use this data to visualize sentiment!
# press shift + enter when inside parenthesis followed by hist to see the  # possible arguments of the hist attribute
#Getting the Desired Values
#process things
# Delete Rows
# generate a list of all order intervals
# YOUR CODE HERE # raise NotImplementedError()
#Check to ensure the duplicate is dropped and there are no longer any duplicates
# default behavior is dayfirst=False
# Determine the probability that an individual in the treatment group converted
#We extract the mean of lengths:
#x=dataframe['Date']
# Load part of the dataset to have a look
# create a copy # only keep tweet whose retweeted_status_id is NAn
##Displaying the probability of the individual which is in treatment group and they converted #The probability of the individual which is in treatment group and they converted is is 0.118808
# Make predictions on test data using the Transformer.transform() method.
# missing value check
# create a pySUMMA simulation object using the SUMMA 'file manager' input file 
# Initialize map
# We can call capitalize on all entires and skip missing values
# Number of members which country of residence is Spain and city Valencia
# Visualizing the data
# loc and iloc can be passed a tuple of multiple indices
# For Research purpose: 
#Our final database will be stored in 'startups_USA'
# Dataframe of user information, here we can determine the last month when they were seen 
#Grabs the last date entry in the data table
# dropping test rows
#challenge 2 #Create a new column that stores the date and time as a single pandas datetime object
# histogram of p_diffs # where the actual difference observed in **ab_data.csv** falls on null distribution
# create an isntance of the ModifyModel class 
#Retrieve the news paragraph for all NASA articles
## Views
#getting total number of users by country
# JH uses 14. 8.26: first 4, then 3, then 2. saved model, reloaded 8.27 and set up for 5 epochs. pred on this, 8.27.csv # DONE. fine for now.
# Use duplicated function to view duplicate ID
# Check out language used on site--this may be a useful parameter. # See language list at https://www.loc.gov/standards/iso639-2/php/code_list.php for reference # Top languages used: English, Chinese, French, Spanish, Korean, German, Italian, Russian, Portuguese, Japanese, Swedish, Dutch
# Chart: Histogram for time for closing a ticket in 2013
#old_page_converted = np.random.choice([0,1],n_old,convert_prob)
#List of False/True for duplicated #Sum the values to get the the number of repeated user ids
#Create a new dataframe from the Flow and Confidence columns
#appending data of 1st computer to 2nd computer 
## Count the number of tweets per userID and get the first 10
##If the name of a column doesn't have spaces, it can be accessed using property-style
# common observations in history and train ?
# print the HTML for the first result.
# Save your regex in punct_re
#view part of the pos_sent variable, to see how it's formatted.
# Make a copy of the game data to work from for EDA
#a final check to make sure everything looks okay
# looking at this table, we see that the majority of planets have been discovered by Radial Velocity and Transit methods # some were only used in recent years
# Note: text is already a clean-cleaned description stored in series
# The ApproximateMedialAxis lines created point in different directions,$ # this step simplifies them.$ sql = '''CREATE TABLE merged_approx_medial as SELECT id, ST_LineMerge(linestring) AS linestring FROM approx_medial;'''$
#create engine to establish connection with the database
#read in in the variables available. the info you need is in the 1year ACS data
#see data distribution of active/inactive stations
# set models to run in pipeline
#checking if both data frames have the same amount of unique user ids and rows before joining to avoid nulls
#Casting the date column as a datetime object, instead of a string
## 2nd run 88.0%, suspect overfitting
### fit the logistic model for page and countries ### print the results
#Check missing values for dates
# another sanity check, if True, go ahead
#probability of individual converting given they are in the control group
#Grouping by outlet then creating a new DF for the bar chart
## Sort each tweet value by its relative timestamp
# Rename columns #funding_pf.head(5)
#To find the number of duplicated rows #cust_data.duplicated('ID').value_counts()
# We append store 3 to our store_items DataFrame # We display the modified DataFrame
# Author: Stefan van der Walt
# convert the country column into three columns.
# parse the string into a composition of dictionaries and lists
# Drop the rows where mismatch occur # Assign the dataframe to df2
# vocabulary list from the unique feature list # custom tokenizer to split by comma
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
# load data # conver string to datetime
#2D correlation matrix plots scope vs type vs nusers
# run the model giving the output the suffix "rootDistExp"
#Calculating new_page_converted
# Series' index has explicitly defined indexs.
# compute the average age at booking by race for Febuary 1st, 2017
#from nltk download premade stopwords list
#additional data on retweet count and favorite count extracted from tweepy API
# vemos los tipos de datos de cada columna ( los que puso pandas por defecto )
# dataframe remains # view data descriptions
#wikipedia_content_analysis = 'https://en.wikipedia.org/wiki/Content_analysis'
# Split the data into features and target label # Visualize skewed continuous features of original data
# Importing matplotlib and seaborn
## Loading the data created
# Evaluate the results of the predictions: Range 1-5
# Unique Closings
# How many stations are available in this dataset?
# Create capture object for live capture
#train.drop('timePeriodStart',1,inplace=True)
# Renaming the columns to Date and Precipitation
#  Let's document the numerical values of (mean, sigma) for the record:
# Read one site as a DataFrame
# Transmission 2020 [GWh], marathon
# drop NaNs
# Compute old converted success rate, which equals to the converted success rate regardless of page # Display old converted success rate
# dropping columns '[', ']', and ','
#Put all data into dataframe and export to csv file
#Save Test Dataset as pickle for later use # the test data in pkl format
# According to 'column_names' # 2nd column = Open Price
# Number of unique classes in each object column
#showing where obs_diffs is from the p_diffs
# Move this to text as an aside
# extract extended tweets DB to csv
#print(cities[cities.isnull()])
# Give our chart some labels and a tile
# Perform a query to retrieve the data and precipitation scores
#Utility function for joining dfs more conveniently
# Set 'user_id' as index in df_countries
#range
# Latest Date
### Fit Your Linear Model And Obtain the Results
# Check the outcome
# create new column 'prosecution_period' as difference between grant date and filing date # convert timedelta to integer number of days # show summary statistics for prosecution period
# Filter outliers in Input Power
# split DataFrames into Republican and Democrat
# finds tobs for most active station
# Returned as a python dictionary
# Enter code to look at datatypes
# remove na values # remove duplicate index values
#result.to_csv(userdatapath1)
# Results of regression analysis
# For getting all the texts under the div tag
# drop duplicates and unnecessary column # set `keep='last'` to keep the last duplicate.
#eth['close'] = eth['close'].astype('float64') 
# remember from above that beers below 2.0 are very rare, so let's use that as a cutoff point
# Calculate the date 1 year ago from today
# analyze validtation between BallBerry simulation and observation data.
#  Quantile-Quantile plot of geovolatility
# Design a query to retrieve the last 12 months of precipitation data and plot the results
# The coefficients
# End of time estimation for the notebook.
#Use Pandas to print the summary statistics for the precipitation data
#What is our critical value at 95% confidence
# Save the query results as a Pandas DataFrame and set the index to the date column
# This table has projects that have only been submitted once and are successes or failures:
# why tweets contain "instantaneamente"?
# plt.scatter(f1, f2, c='black', s=7) # Data Normalization
# create an array of linearly separated values around m_true # create an array of linearly separated values ae
#after getting the data we will fill the data
qry = '''\$ DROP TABLE IF EXISTS lucace.disc_667_stage_10_daily_active_users$ '''$
# Lets scale everyone's score down by 10% because there was cheating # You have to call collect gather the results back into the driver to view it
# Number of new listings per day
# fit the model and show the summary
### Create the necessary dummy variables
# binominal calculation 
# Change the index to the colum 'DATE'
#installing pandas libraries #There is a bug in the latest version of html5lib so install an earlier version #Restart kernel after installing html5lib
#create an index to compare against each iteration of tau #deterministic variable lambda
# Delete items (returns a copy with deleted item -out of place!)
    """$         create the target directory$     """$
# Convert the returned JSON object to a Python Dict
# -r stands for 'recursive'
#Importing S&P Data
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Bar plot of States
# remove not useful variables
#get the key of the max value(change in price of each day)
# compute difference from original dataset ab_data.csv
############# Lest's Collect  US cities information ######################
# find the number of rows in the dataset
## cleans the runtime column
#removing HTML tags from source and plotting source of tweets
# Z-Scores mentioned in the article.
# No missing values
# is there a difference in the no show rate based on gender
#Select the flow data and confidence values for the 100th to 110th records
# I create a function I can apply on each of the dataframes.  # This function tells it to drop any entries with DateTimeIndexes before July, 1980.
# Merge speeches and metadata
#finding the important features
#df_train.info()
#simulate 10,000 new page conversions of trial size n_new #simulate 10,000 old page conversions of trial size n_old #calculate the difference in proportion of conversions between all the simulations above
# Need to determine a way to keep track of the session_id along with words... # This quick and dirty will just go through all the sessions....
# change default figure and font size # visualize the feature importance of every variable
# Using a criterion to fill in missing values by assignment
# read in our raw CSV to DataFrame
# create pySUMMA Plotting Object
# plot
# print df
# recuperar los tipos de datos de cada columna
# Once we have this date formatted, we can quickly do vectorized operations on it
#def capitalizer(x): #    return x.upper
### Clean census data ## Create a tract GEOID
#Tweepy Cursor handles pagination .. 
# Group by group_period as 7D, since market cap data points set at weekly
# join on user id product id table with transaction table
# convert to categorical the AGE_groups collumn.
###calculation to determine the significance of the z_score
# check that it works
# Lets add a new column to the dataframe
# Grab the data
# look for rows with missing values
#print all paragraph texts
###last hour of values
#We have scaled the data inside -1 to 1 to again -1 to 1. May not make much sense in this example, but for other real data we might want to transform and re-transform.
#Improting the PCA module #Doing the PCA on the train data
# 15 topics seems like too many for donuts so let's go with 10 # train LDA using 6 cores (1 master + 5 workers)
# Delete the unecessary rating denominator column
#Read in new vacc file with added column
# instead we'll just use the convenience data set
# The rows do not have any missing values
# remove unnecessary columns(0 or 1 non-null value): contributors, coordinates, geo, place
# Reading JSON
# use json_normalize to extract a tidy form of tweets_clean['user']
#Source: https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python
# each hour of 31 Dec, 2017
# filter data yang berbahasa Indonesia
# What are the most active stations? # List the stations and the counts in descending order.
#print(parsed_json['text'])
#group races by num_starters
#df1.drop(df1.columns[[1,2,3,4,5,6]],1)
# We print percentages:
# pickle data
# headers of 'User-agent' is required if you don't # want a 429 error as it limits your returns.
# New dataframe df3 with the rows where new page and treatment do not line up is created. #Since they are errors, they are dropped from the dataframe df2 (duplicate of the df)
#creating dummy variables for group
# load in api key
# Create the necessary dummy variables. # Select as baseline the country with the most data points: the US. # Add an intercept column.
SQL = """SELECT * FROM github_issues"""$ #print(df_users)
# simulate the conversion treatment for new page  # with sample size equal to the new page user in dataset  # and wiht probability of convert rate under the null
# Grouping events by months
# Check the data
# Test
#active
# Find duplicates
#Calculate for pnew # Display convert rate
# Apply the "getHour" function to all values of the 'lpep_pickup_datetime' variable,  # then assign those values to a new column in the new pandas dataframe, "newdf",  # with a new variable name, "Hour_of_day".
# read pickled data.
#HISTOGRAM FOR P_DIFFS
# Now merge both with images
# sentiment analysis
# In order to analyze the Performance of Wild Card teams in post season we first have to load the "SeriesPost.csv" as dataframe # will be loaded as seriesPost_df # Let's see a preview of the data
# Doing some basic descriptives
# print(tweet)
# Kind of a hack because of the Spark notebook serialization issues
# Sorting is necessary after parallel execution.
# Is the denominator always 10? # No, it isn't... so we'll keep this column
# dropping some unecessray columns
# Identify the abbreviations, as these are not relevant to the SDA corpus.
#Averages grouped by price
#Lastly drop words that are only 1 character post stemming
# Merge the two Funded Apps data frames
# How to index into a particular date's adjusted closing price
# merge them into the user_id field # check if it went well
# write your code here
# Ensure that the Date column is in datetime format to facilitate plotting
# Make predictions. # Select example rows to display.
#Saving the dataset in a file
# Tag parts of speech (PoS)
###YOUR CODE HERE###
# plot the popular community centers
# Print the external node types
# Highest retweet count
# plot autocorrelation function for first difference of therapist data
### create dummy variables for the `period` column
# Naive Bayes with LOOCV on our Test Data
# Now I'm loading the csv into a pandas data frame and give it a look.Starting with Trump.
#test the model using lm.predict and the x_test set of data
#Series is an object in Pandas library, its built-in, & has special functions just like Dataframe object has.  #Simply whenever 1-d array is discussed in pandas, its known as Series.
# read in the dataset
# same if we enter a list as column index # (which can easily happen inadvertently)...
# using rename function changing column name from id to tweet_id
# Display shape of Projects, Participants and the merged DataFrame fp7 # The number of rows in the merged DataFrame FP7 must be the same as the # number of rows in participants
# get the month
#Read first row: In Document Nr 0, word nr x appears 1 time.
# Attach Measurements table to the database
#Trying to read the csv file with the list of twitter accounts of the fortune 1000 companies #this is a csv file with the twitter accounts of every Forturne 1000 company #I am reading the csv file to a data frame called companies
# Output Frame
# Create BeautifulSoup object; parse with 'html.parser'
#aggdf = loctweetdf[['lat','lng','text','lga']].groupby(['lat','lng', 'lga']).agg('count').reset_index() #output
# Load sklearn and LDA (Latent Dirichlet Allocation): # Load word tokenisers and vectorisers:
# preprocessing and cleaning  # remove NaN and NA from dataframe
# save csv file as megmfurr_tweets
# play with FileDataStream
# yearly resample to monthly, compute the totals, and plot
#Trading volume occurs at [6] #Using the statistics library
# running in one Core
#merge transactions dataframe performing a outer join to derive all possible pair of transactions
# name is 'the'
#create a new master dataframe by concatanating the original master and dummies dataframe
# Wide display
## Add them to my df
#saving query results as a Pandas Dataframe setting the date column as the index
# Resource: https://stackoverflow.com/questions/38421170/cant-set-index-of-a-pandas-data-frame-getting-keyerror
# paid_success_with_data
# Use regular expressions to identify and remove numeric IATA codes
# Merge CSV
# run query, convert results to a local pandas dataframe, and display as an HTML table.
#check the loaded file #Remove unnecessary columns
#'New York': '27485069891a7938'
#Great. Now let's use the groupby function to count the number of  permits completed by year and month.
#Finding the duplicated user id
# checking for NaNs in `location` # going to have an issue because there are NaN's -- cannot split NaN's -- going to get an error
# group by order_num, then sum up number of tubes per order # lets peek at the result
#run a sample query
# train and evaluate the decision tree chunker
# Read OGLE data table
# Create data frame #Force column order
# create a DataFrame of author data
# Creating a rate column with the extracted numerical rating  # WHAT AM I DOING?
# Test & extract results 
# models trained using gensim implementation of word2vec
# remove '$' in values and convert to numeric
# 3.2.B OUTPUT/ANSWER
# data=stock_data["CLOSE"].iloc[-100:]
# Run the testing model 
#select by day
# Get data
# determine the probability of an individual of the control group converting
# Remove the duplicate  # Verify if the duplicate has been removed
# reflect an existing database into a new model # reflect the tables
# remove the retweets from the dataset by selecting only records with retweeted_status_timestamp is null: # Test - we expect the number of records to be 2097 as there are 181 retweets # Remove the irrelevant columns: in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp 
# Number of populate for new Page
# Assign the stations class to a variable called `stations`
# quadratic features to use in our estimation
#== Boolean Indexing 
#extract rows where misgligned==False
# identify duplicated user_id using duplicated()
# calculate standard deviation of p_diffs
# Divide each number of each countries columns by it's annual maximum 
# making dummy variables.
#use numpy to simulate 10000 trials
# Given the large dataset, I chose to drop all rows that have NullValues because they are small percent of the data
# import pickle # # Getting back the objects:
# We want to expand these fields into our dataframe. First expand into its own dataframe.
##### modify to use variable Year as index
## Probability of False Positive
# instantiate the complete, unprocessed training dataset as given on kaggle
# Instantiate TextBlob on tweet text & record sentiment scores
