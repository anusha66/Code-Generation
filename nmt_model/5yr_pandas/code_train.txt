df_new.head()
ltdate = pd.to_datetime(voters.LTDate.map(lambda x: x.replace(' 0:00', '')))$ print(ltdate.describe())$ ltdate.value_counts(dropna=False).head(5)
t_len.plot(figsize=(16,4), label="Length", color='r', legend=True)$ t_fav.plot(figsize=(16,4), label="Likes", legend=True)$ t_ret.plot(figsize=(16,4), label="Retweets", color="g", legend=True)$
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost'], axis=1)$ dfY = data['ooCost']
import sqlalchemy as sa$ engine = sa.create_engine('redshift+psycopg2://admissionsapi:admissionsapi@admissions-api.cv90snkxh2gd.us-west-2.rds.amazonaws.com:5432/admissionsapi')$ pd.read_sql("select * from applicants limit 10",engine)
exiftool -csv -createdate -modifydate ciscij10/CISCIJ10_cycle2.mp4 ciscij10/CISCIJ10_cycle3.mp4 ciscij10/CISCIJ10_cycle4.mp4 ciscij10/CISCIJ10_cycle5.mp4 ciscij10/CISCIJ10_cycle6.mp4 > ciscij10.csv
pd.DataFrame(records.loc[:, records.columns != 'ID']).hist();$ plt.savefig('images/histograms.png')
globalCityRequest = requests.get(globalCity_pdf, stream=True)$ print(globalCityRequest.text[:1000])
df = pd.read_csv('C:\\Users\\nreitz\\GAdata\\Airbnb\\train_users_2.csv')$ df.head(5)
df.index.values$
1/np.exp(-0.0150)
df = pd.read_csv('cars.csv')$ len(df)
ward_df = pd.DataFrame(wards)
from sklearn.feature_extraction.text import TfidfVectorizer$ from sklearn.cluster import KMeans$ from sklearn.metrics import silhouette_samples, silhouette_score
contractor_clean['zip_part1'] = contractor_clean.zipcode.str.split('-').str[0]$ contractor_clean['zip_part2'] = contractor_clean.zipcode.str.split('-').str[1]
df2[df2.duplicated(['user_id'], keep=False)]
vocab = {v: k for k, v in vectorizer.vocabulary_.items()}$ vocab
conn_str = "mongodb://127.0.0.1/clintrials"$ client = pm.MongoClient(conn_str)$ client["admin"].command("listDatabases")
df_l[['ship_name','date']].sort_values('date').head()
df = pd.DataFrame(X)$ df['labels'] = pd.Series(y)$
len(kochdf.loc[kochdf['user'] == "Rezeptsammlerin"]['name'])
response.headers
df.irlco.sum()
df.diff()
pure_final_df=pure_final_df._get_numeric_data()$
!rm world_bank.json.gz -f$ !wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz
df_vow.columns.values$
human_genome_length = gdf.length.sum()$ print("The length of the human genome is {} bases long.".format(human_genome_length))
pre_analyzeable = pre[(pre['id'].isin(ids_match_pre))&(pre['Internal ID']!=86221654)]$ len(pre_analyzeable)
xmlData = pd.DataFrame(recordSeries)
march_2016 = pd.Period('2016-03', freq='M')$ print(march_2016.start_time)$ print(march_2016.end_time)
no_dog_stage = df_complete.loc[:,['text','dog_stage']].query("dog_stage=='None'")$ no_dog_stage.sample(10)
indexes=df.query('group=="treatment" and landing_page!="new_page" or group=="control" and landing_page!="old_page"').index.values$ indexes
df_l2.loc[df_l2["CustID"].isin([customer])]
data_ps['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_ps['Tweets'] ])$ display(data_ps.head)
stat, p, med, tbl = scipy.stats.median_test(df2["tripduration"], df4["tripduration"])$ print p$ print tbl
not_alined = df.query('group=="treatment" and landing_page != "new_page" or group=="control" and landing_page=="new_page"').count()$ not_alined
mv_lens.head()
results_ball_rootDistExp, output_ball_rootDistExp = S.execute(run_suffix="ball_rootDistExp", run_option = 'local')
prcp_12monthsDf = pd.DataFrame(prcp_12months)
p_mean = np.mean([p_new, p_old])$ p_mean$ print('Probability of conversion under the null hypothesis ',p_mean )
ymc=yearmonthcountcsv.coalesce(1)$ path1=input("Enter the path where you wanna save your csv")$ ymc.write.format('').save(path1,header = 'true')
file_name = url.split('/')[-1]$ with open(file_name,'wb') as f:$     f.write(r.content)
get_nps(combined_df, 'country').sort(columns='score', ascending=False).head(10)
duplicated_user_df = df2[df2.duplicated(['user_id'], keep=False)]$ duplicated_user_df
with open('../data/faa.txt') as f:$     data = f.read()
rankings_USA = rankings[rankings['country_full'] == "USA"].reset_index(drop=True)$
model.save('ubs_model.h5')  
paired_df_grouped['best_co_occurence'] = paired_df_grouped.apply(lambda x: x['all_co_occurence'][:rule_of_thumb(x['top10'])], axis=1)$ del paired_df_grouped['top10']$
tweets['hour']= dts.dt.hour$ tweets['date']= dts.dt.date$ tweets
df['screen_name'].value_counts()
df_2001['bank_name'] = df_2001.bank_name.str.split(",").str[0]$
predict.predict_score('Stuart_Bithell')
df_western.groupby(['release_decade'], as_index = False)['popularity'].mean()
print(data.shape)$ print(data.columns)$ data.head()
rural_driver_total = rural_type_df.groupby(["city"]).mean()["driver_count"]$ rural_driver_total.head()
n_new = df_treatment.nunique()['user_id']
import pandas as pd$ import numpy as np$ autos = pd.read_csv("autos.csv", encoding="Latin-1")
building_pa_prc_fix_issued=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date','issued_date'])
g = sns.factorplot(x = 'brand', col = 'gender', data =df_ts, size = 10, kind = 'count')
p_new = df2['converted'].mean()$ p_new
cityID = 'bced47a0c99c71d0'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Durham.append(tweet) 
b.loc[1:7]
users_conv = df.query('converted == 1').user_id.nunique()$ print('Proportion of users converted : {:.3f} %'.format(100.0*users_conv/unique_users_num))
train['first_affiliate_tracked'].fillna(train['first_affiliate_tracked'].mode()[0] , inplace = True)
import gmplot$ gmap = gmplot.GoogleMapPlotter.from_geocode("New York",10)$
oz_mask = oz_stops['stop_id'].isin(shared_ids)$ not_in_stops = oz_stops.loc[~oz_mask]$ not_in_stops
df.iloc[[11,24,37]]
import numpy as np$ X_nonnum = X_copy.select_dtypes(exclude=np.number)
pgh_311_data['REQUEST_TYPE'].value_counts(ascending=True).plot.barh(figsize=(10,50))
train.AUTEUR_INCIDENT.value_counts()$
p_new = df2.query('landing_page == "new_page"').converted.mean()$ p_new
s2 = s.reindex(['a', 'c', 'e', 'g']) $ s2['a'] = 0 $ s2
af.sort_values(by=['length'], ascending=False).head()
for row in session.query(Measurements).limit(5).all():$     print(row)$
df_new.groupby('country').nunique()$ df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])$ df_new['intercept'] = 1
modern_train.to_csv('modern_train.csv',sep=',') $ modern_test.to_csv('modern_test.csv',sep=',')
coarse_groups = mgxs.EnergyGroups([0., 0.625, 20.0e6])$ fine_groups = mgxs.EnergyGroups([0., 0.058, 0.14, 0.28,$                                  0.625, 4.0, 5.53e3, 821.0e3, 20.0e6])
df = pd.DataFrame({'Char':chars,'Target':y_true})
df2 = df[((df.landing_page=='new_page')&(df.group=='treatment'))$         |((df.landing_page=='old_page')&(df.group=='control'))]$ df2.head()
sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df.loc[sprintsWithStoriesAndEpics_df.groupby("key_story")["startDate"].idxmax()]$ sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[pd.notnull(sprintsWithStoriesAndEpics_df.index)]$
loss = 10 / np.linspace(1, 100, a.size)$ loss.shape
assembly = openmc.RectLattice(name='1.6% Fuel Assembly')$ assembly.pitch = (1.26, 1.26)$ assembly.lower_left = [-1.26 * 17. / 2.0] * 2
print match.text
for model_name in nbsvm_models.keys():$     test_probs[model_name] = nbsvm_models[model_name].predict_proba(test_cont_doc)
with open("valence_df_dict.json", "r") as fp:$     valence_df_dict = json.load(fp)$ valence_df = pd.DataFrame(valence_df_dict)
df4 = df3.merge(df_countries, on = 'user_id')
train['is_weekend'] = train['start_timestamp'].map(lambda x: 1 if x.weekday() in [5,6] else 0)$ test['is_weekend'] = test['start_timestamp'].map(lambda x: 1 if x.weekday() in [5,6] else 0)
user_dropped = df2.user_id.duplicated().sum()$ print('There are {} duplicate rows in df2.'.format(user_dropped))
to_be_changed = df2[df2['group']=='treatment'].index$ df2.loc[to_be_changed, 'ab_page'] = 1$ df2[['intercept', 'ab_page']] = df2[['intercept', 'ab_page']].astype(int)
sum(df2['user_id'].duplicated())
df_period = df_period.loc[df_period.time_id < '2014-09-01']
rain_df.set_index('date').head()
weather = pd.read_csv("weather.csv", parse_dates={"date" : ['Date']})$ weather = weather.drop(['HDD', 'CDD'], axis=1)
pvt['ga:date'] = pd.to_datetime(pvt['ga:date'], format='%Y%m%d', errors='coerce')$ pvt.rename(columns={'ga:transactionId': 'transactionId', 'ga:date': 'date'}, inplace=True)$ pvt
df = pd.read_csv("../../data/msft2.csv",skiprows=[0,2,3])$ df
from sklearn.model_selection import train_test_split$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
df2 = df2.drop_duplicates(['user_id'])
qs = qs[qs.nwords > 5]
df_users = pd.read_csv('../data/march/users.csv')$ df_levels = pd.read_csv('../data/march/levels.csv')$ df_events = pd.read_csv('../data/march/events.csv', skiprows=1, names=event_header, error_bad_lines=False, warn_bad_lines=True)     
df.to_csv('ab_new.csv', index=False)
job_a_requirements = pd.DataFrame(requirements, columns = ['Job_A'])$ job_a_requirements
geo_df = pd.DataFrame(geo_events, columns=geo_columns)$ geo_df.head()
import string$ punct_re = r'[^\s\w\d]'$ trump['no_punc'] = trump['text'].str.replace(re.compile(punct_re), ' ')$
jobs_data = pd.read_pickle('D:/CAPSTONE_NEW/jobs_data_full.pkl')
df2[df2.duplicated(['user_id'], keep=False)]
table_rows = driver.find_elements_by_tag_name("tbody")[28].find_elements_by_tag_name("tr")$
lda_model = models.LdaModel(corpus=corpus, id2word=id2token, num_topics=20, update_every=0, passes=20)$ lda_model.save('/tmp/model.lda')$ lda_model = models.LdaModel.load('/tmp/model.lda ')$
df['converted'].mean()
autos["brand"].value_counts(normalize=True).head(20)*100
df = pd.read_hdf('data/games.hdf','df')$ df
materials_file = openmc.Materials([fuel, water, zircaloy])$ materials_file.export_to_xml()
master_file.to_csv(os.curdir + '/master.csv', index=False)
search_booking = search1[search1.booking == 1]$ search2 = search1.append([search_booking]*4,ignore_index=True)
iso_join.head()
print 'There are', len(df_mk) , 'posts and' , df_mk['id'].nunique() , 'users mentioning Michael Kors'
plt.pie(count_driver, labels=type_driver,explode=explode, colors=colors,$         autopct="%1.1f%%", shadow=False, startangle=140)$
print WorldBankdf.printSchema()
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
CON=CON.drop_duplicates(['Contact_ID'], keep='first')
monthly_portfolio_var = round(np.dot(stock_weights.T,np.dot(monthly_cov_matrix, stock_weights)),2)$ monthly_portfolio_var
display(data.head(4))
rng.day
import matplotlib.pyplot as plt$ %matplotlib inline$
twitter_archive.describe()
df.info()
df[['total_spend','month','day','year','cust_id']].groupby(['year','cust_id','month','day','total_spend']).agg('sum').reset_index()$
number_of_commits = git_log['timestamp'].count()$ number_of_authors = git_log["author"].nunique(dropna = True)$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
(trainingData, testData) = data.randomSplit([0.7, 0.3])$
df2[df2.duplicated('user_id', keep=False) == True]
df.isnull().sum()$
control_wrong = df.query('group == "control" and landing_page == "new_page"')$ treatment_wrong = df.query('group == "treatment" and landing_page == "old_page"')$ control_wrong.shape[0]+treatment_wrong.shape[0]
df[df.Likes == 0].tail()
new_features = tfidf_vectorizer.transform(sdf.Norm_reviewText)$ new_features_tfidf = new_features.todense()
df_l.loc[:,['ship_name','major_cruise_line']].drop_duplicates('ship_name').loc[df_l['major_cruise_line'] =='Royal Caribbean International'].sort_values('ship_name')   $
data = [{'a': i, 'b':2 * i}$        for i in range(3)]$ pd.DataFrame(data)
news_df.head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])$ z_score, p_value # but here, P_value is for two_tailed test..we need one tailed test. 
df.describe()
df.query('converted == 1').count()[0]/df.count()[0]
df_mk.index, df_ks.index = df_mk['time'], df_ks['time']$ df_mk['date'], df_mk['hour'], df_mk['brand'] = df_mk.index.date, df_mk.index.hour, 'Michael Kors'$ df_ks['date'], df_ks['hour'], df_ks['brand'] = df_ks.index.date, df_ks.index.hour, 'Kate Spade'
cityID = '9a974dfc8efb32a0'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Kansas_City.append(tweet) 
fork_table = fork_table.drop_duplicates()
unique_users = df['user_id'].unique().shape[0]$ unique_users$
df2['intercept'] = 1$ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])$ df2.head()
warnings.filterwarnings("ignore", 'This pattern has match groups')$ faulty_rating_id = archive_clean[archive_clean.text.str.contains( r"(\d+\.?\d*\/\d+\.?\d*\D+\d+\.?\d*\/\d+\.?\d*)")].tweet_id$
plt.scatter(X2[:, 0], X2[:,1], c=labels, cmap='rainbow')$ plt.colorbar()
import statsmodels.api as sm$
print(data.program_code.value_counts())
cog_simband_times.drop(labels=['CC-OTS BHC0109-1', 'DH-OTS BHC0242-1','JA-OST BHC0258-1'], inplace=True)
new_page_converted = np.random.binomial(1, p_new,n_new)$ new_page_converted
pandas_small_frame = small_frame.as_data_frame()$ print(type(pandas_small_frame))$ pandas_small_frame
model.wv.most_similar("crot")
weather['precip_total'] = weather['precip_total'].replace('NaN', None, regex=False).fillna(0)$ weather['pressure_avg'] = weather['pressure_avg'].replace('NaN', None, regex=False).fillna(0)$ weather['wind_speed_peak'] = weather['wind_speed_peak'].replace('NaN', None, regex=False).fillna(0)
lm.summary()
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))$
dailyplay = pd.read_excel('data.xlsx', sheetname = 'dailyplay')$ promotions = pd.read_excel('data.xlsx', sheetname = 'promotions')$
df.user_id.nunique()
id_of_tweet = 932626561966247936$ tweet = (api.get_status(id_of_tweet, tweet_mode='extended')._json['full_text'])$ print(tweet)
appleNegs.groupby('group_id').tweet_id.count().reset_index()$
columns = inspector.get_columns('weather')$ for c in columns:$     print(c['name'], c["type"])
df['interest_level_codes'].value_counts()
mydata.info()
train_rounds = classification_data[(classification_data.announced_on < '2015-08-01')].funding_round_uuid$ test_rounds = classification_data[(classification_data.announced_on >= '2015-08-01')].funding_round_uuid
pd.timedelta_range(0, periods=9, freq="2H30T")
sub_gene_df.sample(10)
readerASN = geoip2.database.Reader(GEOLITE_ASN_PATH)$ readerCITY = geoip2.database.Reader(GEOLITE_CITY_PATH)
cnf_matrix[1:].sum() / cnf_matrix.sum()
new_page_converted.mean()
nltk.help.upenn_tagset()
import pandas as pd$ commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ print(commits_per_year.head())
print("Feature types", tipsDF.dtypes)
df2 = df[((df.group == 'control') & (df.landing_page == 'old_page')) | ((df.group == 'treatment') & (df.landing_page == 'new_page'))]
rawInstagramPosts = coll.find({'timestamp': {'$gte': datetime.datetime(2016, 9, 5, 0,0,0)}})
auth = tweepy.OAuthHandler(consumer_key,consumer_secret)$ auth.set_access_token(access_token,access_token_secret)$ api = tweepy.API(auth) 
df_new.country.unique()
festivals.head(5)
mean = np.mean(data['len'])$ print("The length's average in tweets: {}".format(mean))
FORMAT = '%(asctime)s : %(levelname)s : %(message)s'$ logging.basicConfig(format=FORMAT)$ logging.getLogger().setLevel(level=logging.INFO)$
austin.isnull().sum()$
gDate_vProject = gDateProject_content.unstack(level=1, fill_value=0)$
average_vol = [i[5] for i in json_afx_data] $ print('Average trading volume is :', sum(average_vol) / len(average_vol)) #divide the sum by the length // count of entries.$
merged_table = merged_table.drop(columns=['host', 'last_update', $                                           'Unnamed: 0', 'queue', 'date', $                                           'weekday', 'strtime'])
summary_bystatus = df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9]).T$ summary_bystatus
m.__repr__()$
p_mean = np.mean([p_new, p_old])$ print('The mean conversion rate of p_new and p_old:',p_mean)
print("Exporting to %s"%(man_export_filename))$ extract_man_uw.to_csv(man_export_filename, index=False)
with open(output_folder+Company_Name+"_Forecast_"+datetime.datetime.today().strftime("%m_%d_%Y")+".pkl",'wb') as fp:$     pickle.dump(final_data,fp)
USvideos = pd.read_csv('data/USvideos_no_desc.csv', parse_dates=['trending_date', 'publish_time'])
url = "https://twitter.com/marswxreport?lang=en"$ response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')
plt.figure(figsize = (20,20))        # Size of the figure$ sns.heatmap(telecom3.corr())
from sodapy import Socrata$ client = Socrata("data.cityofnewyork.us", os.getenv("apptoken"))
listingRev = features.merge(cleanCal, how = 'left', left_on='id', right_on='listing_id')$ listingRev
ab_file2.drop(labels=2862, inplace=True)$ ab_file2[ab_file2['user_id']==773192]
data.iloc[:3, :2]
precip_data_df.head(3)$
node_types_DF = pd.read_csv('network/recurrent_network/node_types.csv', sep = ' ')$ node_types_DF
print(df.isnull().sum())$
df2 = pd.read_csv('ab_edited.csv')
z_score, p_value = sm.stats.proportions_ztest([old_conversions, new_conversions], [n_old, n_new], alternative = 'smaller')$ print(z_score)$ print(p_value)$
eng['time_stamp'] = pd.to_datetime(eng['time_stamp'])
bigdf = pd.concat(dfs)
features_to_use=features_to_use+[]$ train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()$ test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()$
df2[df2.duplicated(['user_id'], keep=False)]
results = rcf_inference.predict(df_numpy[:10])$ print(results['scores'])
distance_list = []$ for i in range(0, len_start_coord_list):$     distance_list.append(get_distance())
train, test = dogscats_h2o.split_frame(ratios=[0.7])
prod_sort = averages[averages.Product == 'Amarilla']$ prod_sort
column_check = inspector.get_columns('station')$ for check in column_check:$     print(check['name'],check['type'])
print('No of repeated entries: {}'.format(sum(df.duplicated())))$ n_unique_users = df['user_id'].unique().shape[0]$ print (n_unique_users)
npath = out_file3$ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
ex=savedict['2017-11-15']$ ex.head()
secclintondf.tags[0]$
t1= twitter.copy()$ t2 = pred.copy()$ t3 = tweet_df.copy()
URL = "http://www.reddit.com/hot.json"$ res = requests.get(URL, headers = {'User-agent':'Caitlin Bot 0.1'})
df_raw = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ df = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ print df_raw.head()
service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'$
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets], columns=['Tweets'])$ display(data.head(10))
session.query(measurements.date)[-1]
df['date-time'] = pd.to_datetime(df['date'] + ' ' + df['time'], format='%d-%b %I:%M%p')$ df['date-time'] = df['date-time'] + pd.to_timedelta(42369,unit='d')
data.shape
full_act_data.plot(figsize=(20,8));
sum(tw_clean.expanded_urls.isnull())
df = pd.DataFrame()$ variables = ['text','created_at','source','user']$ df = pd.DataFrame([[getattr(i,j) for j in variables] for i in tweets], columns = variables)
max_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ print("Max Date is ", max_date)
import datetime$ dNow = datetime.datetime.now()$ AcqDate = dNow.strftime("%Y-%m-%d")
results.summary()
1.40936 records are in vio.$ 2.The granularity of vio is better than that of bus and ins.$
s.asfreq('8BM')
df5 = df2.query("group == 'treatment'")$ df6 = df5['converted'].mean()
df2['user_id'].nunique()$ print ("Total Number of Unique row : {}".format(df2['user_id'].nunique()))
df_users.user_id.nunique()
tweet_data_df.to_csv("tweet_data_df.csv",sep =",")
marsfacts = df_mfacts.to_html(bold_rows=True)$ marsfacts$
from sklearn.ensemble import RandomForestClassifier$ rf_clf=RandomForestClassifier(max_depth=None)$ print(rf_clf)
linear_predictor = linear.deploy(initial_instance_count=1, #Initial number of instances. $                                  instance_type='ml.m4.xlarge') # instance type
exiftool -csv -createdate -modifydate cisuabn14/cisuabn14_cycle2.MP4 cisuabn14/cisuabn14_cycle3.MP4 cisuabn14/cisuabn14_cycle4.MP4 cisuabn14/cisuabn14_cycle5.MP4 cisuabn14/cisuabn14_cycle6_part1.MP4 cisuabn14/cisuabn14_cycle6_part2.MP4 > cisuabn14.csv
df = pd.read_csv('ab_data.csv')$ df.head()
df = pd.read_excel('accounts-annotations.xlsx', encoding='cp1252')
df = datetime.now()
pres_df.drop('time_from_creation_tmp', inplace=True, axis=1)$ pres_df.head()
product = df[df.asin=='B00L86ZKAK'].copy()$ product=product.sort_values('reviewTime')$ product.head()
list(set(df['City'][df.CustomerID=='0000118196']))   
rain_df.set_index('date').head()
mean = np.mean(data['len'])$ print("The average tweet length is: {}".format(mean))
autos["registration_year"].describe()
mars_weather = soup.find_all('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text')[0].text$ print(mars_weather)
before_date=str(fc_clean.sel(time =start_of_event, method = 'pad').time.values)[0:10]$ after_date=str(fc_clean.sel(time =start_of_event, method = 'backfill').time.values)[0:10]
gbm_model.plot()
treatment_group = df[df.group =='treatment']$ treatment_group[treatment_group.landing_page != 'new_page'].count()
df_train.head
test_df = pd.read_csv("test.csv", dtype=dtypes)$ test_df.head()
model_w.summary2() # For categorical X.
weather.loc[weather.precipitation_inches.isnull(), $             'precipitation_inches'] = weather[weather.precipitation_inches.notnull()].precipitation_inches.median()
all_patents_df.number_of_claims.describe()
from sklearn import preprocessing$ min_max_scaler = preprocessing.MinMaxScaler()
pd.crosstab(train.TYPE_BI, train.TYPE_UT)
gram_collection.find_one({"account": "deluxetattoochicago"})['date_added']$
y_pred_test = crf.predict(X_test)$ metrics.flat_f1_score(y_test, y_pred_test,$                       average='weighted', labels=labels)
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs),func.avg(Measurement.tobs)).filter(Measurement.station=="USC00519281").all()
df.to_csv('dataframe-kevin.csv')$
nodes_file = directory_name + 'nodes.h5'              # Contains information about every node$ node_models_file = directory_name + 'node_types.csv'   # Contains information about models
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
DataSet.to_csv("file_name_final.csv", sep=",")
telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format="%Y-%m-%d %H:%M:%S")
type(model.wv.syn0)$ len(model.wv.vocab)$ model.wv.syn0.shape
%matplotlib inline$ df['log_AAPL'].plot(figsize=(12,8));
filepath = os.path.join("data_output","df_sample.csv")$ df_sample = pd.read_csv(filepath)$ df_sample.head()
df2.info()
Total_Number_of_Rides_max = rides_analysis["Total Number of Rides"].max()$ Total_Number_of_Rides_max
plotdf['forecast'] = plotdf['forecast'].interpolate()$ plotdf['forecastPlus'] = plotdf['forecastPlus'].interpolate()$ plotdf['forecastMinus'] = plotdf['forecastMinus'].interpolate()
n_old = (df2.landing_page == 'old_page').sum()$ n_old
df3['Current Status'].unique()
df2 = df.drop(df[((df.landing_page == 'new_page') & (df.group != 'treatment')) | ((df.landing_page != 'new_page') & (df.group == 'treatment'))].index)
free_data.iloc[:5]
capa2017.head()
coming_next_reason.to_csv('../data/coming_next_reason.csv')
plt.scatter(y_holdout, holdout_preds)$
le = LabelEncoder()$ le.fit(data.Block)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2014-01-01&end_date=2014-01-02')$
data_df = pd.read_json(data_json_str)
df = pd.read_csv('data/goog.csv', parse_dates=['Date'], index_col='Date')$ df
M7_RMSE,M7_pred,M7_actual = ts_walk_forward(training_X_scaled,training_y.values,12,12,regr_M7)
print(len(alltrains))$ alltrains.to_csv('data/JY-HKI-dec17-mar18.csv')
bgCan = [[ 5.6 , 7.8, 6.0 ], [ 12.2, 4.4, 6.7 ]] # same ndarray as the NumPy lesson$ df = pd.DataFrame(bgCan, index=['Monday', 'Tuesday'], columns=['Breakfast', 'Lunch', 'Dinner'])$ df
df_columns[df_columns.index.month.isin([5,6,7])]['Complaint Type'].value_counts().head()$
optimal_model = grid.best_estimator_$ optimal_model.score(X_test,y_test)
df_tweet_json_clean['created_at'].head()$
products = df.groupby('asin').count()$ number_of_reviews =products['overall']$ number_of_reviews.head()
df1.index.values
import numpy as np$ raw_data = df.as_matrix()$ print(raw_data.shape)
df_new_borrowers = pd.read_sql_query('SELECT created_at FROM borrowers WHERE activation_status != 4', engine)$ df_new_borrowers = pd.DataFrame({'number':1},index=df_new_borrowers.created_at)
page[1].keys()
nyt = news_sentiment('@nytimes')$ nyt['Date'] = pd.to_datetime(nyt['Date'])$ nyt.head()
url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRlXVQ6c3fKWvtQlFRSRUs5TI3soU7EghlypcptOM8paKXcUH8HjYv90VoJBncuEKYIZGLq477xE58C/pub?gid=0&single=true&output=csv'$ df_hourly = pd.read_csv(url,parse_dates = ['time'],infer_datetime_format = True,usecols = [0,3])$ df_hourly.head()
pd.Series(0, index=days)
print csvData.dtypes
%matplotlib inline$ dedups.plot(y='yearOfRegistration', kind='hist')$
data = pd.read_csv('barbsList99.csv')$ data['order'] = data.apply(lambda x : 0,axis=1)$ data.to_csv('barbsList99.csv',index=False,index_label=False)
pres_df['metro_area'].head()
print(autos.price.describe(percentiles=[0.10, 0.90])) #Find the 10th percentile and 90th percentile $ autos = autos[(autos["price"] > 500 ) & (autos["price"] < 14000 )] #Remove outliers using the percentile values
df2.info()
reddit.info()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], $                                               nobs=[n_new, n_old],alternative='larger')$ print("z-score:", z_score,"\np-value:", p_value)
empDf.filter(empDf["age"] >30).join(deptDf, empDf.deptid == deptDf.id).\$         groupBy("deptid").\$         agg({"salary": "avg", "age": "max"}).show()
lq = lq.dropna(how = 'any')
filtered_df[filtered_df['M_pay_3d'] == 0].shape[0]
suburban = merge_table.loc[merge_table["type"] =="Suburban"]$ suburban_table_df= suburban.groupby(["city"])$ suburban_table_df.max()
instance.assumptionbreaker()
df_user = df_user.fillna(0)
ax = P.plot_1d('pptrate')$ ax.figure.savefig('/media/sf_pysumma/pptrate.png')$
df_parties.groupby(df_parties.index.weekday)['Created Date'].count().plot(kind="bar")$
obs_diff = df2[df2['converted']==1]['user_id'].count() / df2.shape[0]$ obs_diff
alphas = [0.00001,0.0001, 0.001, 0.01, 0.1,0,1, 10, 100, 1000,10000]
pulledTweets_df.sentiment_predicted_lr.value_counts().plot(kind='bar', $                                                            title = 'Classification using Logistic Regression model')$ plt.savefig('data/images/Pulled_Tweets/'+'LR_class_hist.png')
gdax_trans['Timestamp'] = gdax_trans.apply(lambda row: fix_timestamp(row["Timestamp"]), axis=1)
final_df.corr()["ground_truth_adjusted"][names]
bands.columns = ['BAND_'+str(col) for col in bands.columns]
df.info()
cityID = '013379ee5729a5e6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tucson.append(tweet) 
bucket.upload_dir('data/heat-pump/raw/', 'heat-pump/raw', clear_dest_dir=True)
driver.find_element_by_xpath('//*[@id="body"]/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[1]/b/font/a').click()
irisRDD = SpContext.textFile("iris.csv")$ print (irisRDD.count())
df_master.info()
hours[5:8].start.plot()$ sns.lmplot(x='hour', y='start', data=hours[5:8], aspect=1.5, scatter_kws={'alpha':0.5})
knn_reg.score(x_test,y_test)
newpage = df.query('landing_page == "new_page" and group != "treatment"').count()[1]$ treat = df.query('landing_page != "new_page" and group == "treatment"').count()[1]$ newpage + treat
res3a1=pysqldf(q3a1)$ res3a1.head()
issue_comments_table['action'].unique()
plt.bar(x_axis, np.sqrt(np.square(deltas_df['Delta Approval']*.8)), color="blue")$ plt.bar(x_axis, np.sqrt(np.square(deltas_df['Delta Compound']*2)), color="green")$ plt.show()
top_chater = non_na_df['sender_card'].value_counts()$ top_chater.nlargest(20).plot(kind='bar', figsize=(14,6))
timeEnd = ("2017-06-07 23:59:59")$ timeStart = ("2015-06-09 00:00:00")$ timeNow = datetime.strptime(timeNow, "%Y-%m-%d %H:%M:%S")
df_clean = df.copy()$ preds_clean = pred.copy()$ api_clean = api_df.copy()
df2.user_id.nunique()
scores_by_org = news_df.groupby('Screen Name')['Compound Score'].mean()$ scores_by_org.head()
data.keys()
auth_key = 'yZVrH8Esn8zs7vGPN2zJ'$ main_df = pd.DataFrame # Lets create a blank dataframedf = quandl.get("FMAC/HPI_KOKIN", authtoken=auth_key)
np.sum(PyReverseDims(jArr), 0)
contractor_clean.loc[contractor_clean['contractor_id'].isin([382,383,384,385,386,387]),$                      'contractor_bus_name'] ='Cahaba Government Benefit Administrators, LLC'
df2.query('landing_page == "new_page"').user_id.count()/df2.user_id.count()
df[df['bmi']< 18.5] # this is the way to select data, by using filters. $
regr.fit(X_train, y_train)
store_items = store_items.rename(columns = {'bikes': 'hats'})$ store_items
shows['plot'] = shows['plot'].dropna().apply(remove_stopwords)
print(rdc.feature_importances_[0:113])$
folder_name = '/home/workspace'$ if not os.path.exists(folder_name):$     os.makedirs(folder_name)
factors = web.DataReader("Global_Factors","famafrench")$ factors
df_ad_airings_filter_3['start_time'].max()$
df[['state_cost','state_retail','sale']] = df[['state_cost','state_retail','sale']].apply(pd.to_numeric)$ df[['store_number','vendor_number','item_number',]]=df[['store_number','vendor_number','item_number']].astype(object)$ df['date']= pd.to_datetime(df['date'], format="%m/%d/%Y")
img_url_rel = img_soup.find('figure', class_='lede').find('img')['src']$ img_url_rel
temp_df.plot.hist(bins=12)
pagecount = trackobot.history(page=1)['meta']['total_pages']+1$ pagecount
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new.head()
sns.distplot(autodf.powerPS)
cumret['monthly_cumret'].iloc[0:12].prod()
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = logit_mod.fit()
ppm_body = preProcessor_in_memory(hueristic_pct=.61, keep_n=6000, maxlen=60)$ vectorized_body = ppm_body.fit_transform(data_to_clean_body)
new_page_converted = np.random.choice(2, size = n_new, p=[0.8805, 0.1195])$
newdf = pd.merge(bc, ts, left_index=True, right_index=True)
for row in session.query(Measurements).limit(5).all():$     print(row)$
csvData[csvData['street'].str.match('.*Avenue.*')]['street']
precipitation_df.isnull().sum()
state_party_df['National_D_neg_ratio']['2016-08-01':'2016-08-07'].sum() / 7
cars.isnull().sum()
df = pd.DataFrame(results, columns=['date', 'precipitation'])$ df.set_index(df['date'], inplace=True)$ df.head()
df2.user_id.nunique()
dgdiff.order()
print(ind.size, ind.shape, ind.ndim, ind.dtype)
daily_sales.plot('date','dcoilwtico',kind='bar', color='r', figsize=(15,5))
spark.stop()
Base.classes.keys()
X_train, X_test, y_train, y_test = train_test_split(stock.drop(['target'], 1), stock['target'], test_size=0.3, random_state=42)
new_table=original_merged_table.groupby(["city","type","driver_count"]).size().reset_index().rename(columns={0:"Number of ride"})$ new_table.head()
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)$
df_all_users['Email Address'] = df_all_users['Email Address'].apply(lambda x: str(x.lower().strip()))$ df_all_users['Email Address'] = df_all_users['Email Address'].astype(str)
jobs_data['clean_titles'] = jobs_data['record.title'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", str(x)).split() if i not in s_words]).lower())
plt.hist(null_vals)$ plt.axvline(obs_diff, c='red');
top_10_authors = git_log.loc[:, 'author'].dropna().value_counts().head(10)$ top_10_authors
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
set(user_status_model._json) == set(diff.loc["status", 1])
X = incident_df[incident_df.columns.drop(['Label'])].copy()$ y = incident_df['Label'].replace(incidentTypesDict)$
temperature_df.plot.hist(bins=12)
squared_distances_i_j_k = np.power(y_i[:, np.newaxis, :] - y_i, 2)$ pairwise_squared_distances_i_j = squared_distances_i_j_k.sum(axis=2)$ pairwise_squared_distances_i_j
os.chdir(os.path.expanduser('~/PycharmProjects/chat-room-recommendation/'))$ lines = open('cornell-movie-dialogs-corpus/movie_lines.txt','r').read().split('\n')$ movie_metadata = open('cornell-movie-dialogs-corpus/movie_titles_metadata.txt','r').read().split('\n')
y_ = df2["user_id"].count()$
df_new.groupby(['country', 'group'], as_index =False).count()[['country','group', 'landing_page']]
iris = load_iris()$ X = iris.data$ y = iris.target
actual_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ actual_diff
trading_volume = [x[6] for x in data_table if x[6] is not None]$ print('Average Trading Volume: {:f}'.format(sum(trading_volume) / len(trading_volume)))
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']])$ results_1 = logit_mod.fit()$
data[[name.endswith('bacteria') for name in data.phylum] & $     (data.value > 1000)]$ data
workspace_uuid = ekos.get_unique_id_for_alias(user_id, 'lena_newdata')$
df.iloc[-1]
ArepaZone_merged_arrays_df = pd.DataFrame(data=ArepaZone_id_array, columns=['id']) 
births_by_date = births.pivot_table('births',$                                    [births.index.month, births.index.day])$ births_by_date.head()
tweet_archive_clean = tweet_archive_clean[tweet_archive_clean['retweeted_status_id'].isnull()]
len(df[~(df.data == {})])
df2.drop_duplicates(subset='user_id',inplace=True)$ df2.query('user_id == 773192')
G_df = pd.DataFrame({'Geocoded Location':appended_data})$ G_df.head(10)
dummy = pd.get_dummies(furniture, columns=['category'],drop_first=True)
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
last_year_temp = session.query(Measurement.tobs).filter(Measurement.station=="USC00519281")\$         .filter(Measurement.date>='2016-08-01').filter(Measurement.date<'2017-08-01').all()
import requests  $ r = requests.get('https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html')$ print(r.text[0:500])
sns.boxplot(data.renta.values)$ plt.show()$
precip_df.rename(columns={'prcp': 'precipitation'}, inplace=True)$ precip_df.set_index('date', inplace=True)$ precip_df.head()
one_test_pvalue = 1 - 0.19/2$ one_test_pvalue
alldata.to_pickle('test')
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status', 'OriginationPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'green', 'red', 'yellow'], grid=False)$
df.loc['20180101':'20180103', ['A','B']]
X_train[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
twitter_archive.loc[(twitter_archive['name'].str.islower())]
indeed1 = indeed[['company','location','skills','title','salary_clean','category','duration_int','summary_clean']]$ indeed1.shape
saveTweetDataToCSV(input_hash_tag.value)
df3.duplicated().sum()
df_country = pd.get_dummies(df_new['country'])$ df_new = pd.concat([df_new, df_country], axis=1)$ df_new.head()
testData = data['release_date']$ testData = testData.str[0:4].astype(float)$
stations = session.query(Station.name).count()$ print(f'There are total of {stations} stations in Hawaii'.format())
unique_words_sk = set(words_sk)$ corpus_tweets_streamed_keyword.append(('unique words', len(unique_words_sk))) # update corpus comparison$ print('Number of unique terms: ', len(unique_words_sk))
r = r[r.duration < 3000]
p_new=df2.converted.mean()$ p_new
full_data = pd.concat([jobs_data1, jobs_data2, jobs_data3, jobs_data4], ignore_index=True)
actual_new = df2.query('landing_page == "new_page"').converted.mean()$ actual_old = df2.query('landing_page == "old_page"').converted.mean()$ actual_diff = actual_new - actual_old
births['decade'] = 10 * (births['year'] // 10)$ births.pivot_table('births', index='decade', columns='gender', aggfunc='sum')
prediction_url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"$ with open(prediction_url.split("/")[-1],"wb") as file:$     file.write(requests.get(prediction_url).content)
logist = sm.Logit(df2['converted'], df2[['intercept', 'treatment']])$ res = logist.fit()
cityID = 'b046074b1030a44d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jersey_City.append(tweet) 
rows_in_table = len(ins['business_id'])$ unique_ins_ids = len(ins['business_id'].unique())$
save_model('model_knn_v1.mod', knn_grid)
data_df.iloc[535]
results3.summary()
isinstance(df.date[1],datetime)
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
births_by_date.index = [pd.datetime(2012, month, day)$                        for (month, day) in births_by_date.index]$ births_by_date.head()
customers_df = pd.read_csv('./data/User_Information.csv')$ print(len(customers_df))$ customers_df.head()
filename = 'Daily_Stock_Prediction_latest.pk'$ with open('./Models/'+filename, 'rb') as f:$     model_test = pickle.load(f)
len(calls.location.unique())
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL')$                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1))$                           &(extract_nondeduped_cmp.app_branch_state=='OH')].groupby('APP_PROMO_CD').size()
stories = stories.set_index('short_id')$ stories.head()
df.head()
np.exp(results2.params)
regr_M7 = linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)
image_predictions = pd.read_csv('image_predictions.tsv',sep='\t')
import re$ trump['source'] = trump['source'].str.replace(re.compile('<.*?>'), '')$ trump['source'].unique()
studies_a = pd.DataFrame(studies, columns=['why_stopped','verification_date','target_duration','study_type','start_date_type','start_date','source','phase','overall_status','official_title','number_of_arms','nct_id','limitations_and_caveats','last_known_status','last_changed_date','is_unapproved_device','is_fda_regulated_drug','is_fda_regulated_device','enrollment_type','enrollment','completion_date','brief_title','baseline_population'])$ studies_a.head()
for i in range(len(df.columns)):$     df.rename(columns={i:df_header[i]}, inplace=True)
ftr_imp=zip(features,xgb_model.feature_importances_)
data = data_all.dropna(subset=['has_odds', 'sigma_scaled'])$ data.tail(3)
cleanedData = allData$ cleanedData[cleanedData['text'].str.contains("\&amp;")].shape[0]
session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all()
print("Jaccard  Similarity Score: ", metrics.accuracy_score(y_test, yhat))$ print("F1-score Accuracy: ",  metrics.f1_score(y_test, yhat, average='weighted')) 
exiftool -csv -createdate -modifydate cisuabe5/cisuabe5_cycle1.MP4 cisuabe5/cisuabe5_cycle2.MP4 cisuabe5/cisuabe5_cycle3.MP4 cisuabe5/cisuabe5_cycle4.MP4 > cisuabe5.csv
so.loc[so['favoritecount'].between(30, 40), 'title'::3].head()
irisRDD = SpContext.textFile("iris.csv")$ irisData = irisRDD.filter(lambda x: "Sepal" not in x)$ print (irisData.count())
print (data.isnull().sum()/len(data)).sort_values()$ print (test_data.isnull().sum()/len(test_data)).sort_values()$
print('The probability of receiving new page:', (df2['landing_page'] == 'new_page').mean())
print('Columns:',len(free_data.columns), 'Rows:',len(free_data))
sub_gene_df['type'].value_counts()
browser.find_by_css("a.product-item")[i].click()$ hemisphere_image_urls = []$ links = browser.find_by_css("a.product-item")
result = api.search(q='%23data')
S_distributedTopmodel.forcing_list.filename
pp( autos.describe() )$ pp(autos.isnull().sum() )$
print df_nona.groupby('segment').apply(lambda v: float(sum(v.accounts_provisioned))/sum(v.district_size))$ df_nona.groupby('segment').install_rate.describe()$
print(data.petal_length.std(),$       data.petal_length.var(),$       data.petal_length.sem())
print_sessions = printer_usage.query('total_printed >= 1')$ print_sessions.describe()
permits_df = pd.read_csv('./Datasets/Building_Permits_Full.csv')$ permits_df.head()
funded_apps_2 = pd.read_csv("RISE_funded_20160808.csv")
 $ df_master = pd.merge(df_clean, image_clean, on ='tweet_id', how= 'inner' )$ df_master = pd.merge(df_master, tweet_clean, on = 'tweet_id', how = 'inner' )
most_active = stations[0][0]$ record = [func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)]$ session.query(*record).filter(Measurement.station == most_active).first()
dfjoined = dfrecent.merge(dfcounts, how = 'inner', on = ['created_date'])
contractor[contractor.contractor_id.duplicated() == True]
df = df.dropna(subset=['ACTION REQUESTED'])
import pandas as pd$ import matplotlib.pyplot as plt$ %matplotlib inline$
np.count_nonzero(np.any(nba_df.isnull(), axis = 1))
df = pd.read_csv('../Datasets/4 keywords.csv', index_col='Symbol')
merged_portfolio = pd.merge(portfolio_df, adj_close_latest, left_index=True, right_index=True)$ merged_portfolio.head()
!hdfs dfs -put Consumer_Complaints.csv {HDFS_DIR}/Consumer_Complaints.csv
results = session.query(Measurement.date, Measurement.prcp).\$     filter(and_(Measurement.date <= Curr_Date, Measurement.date >= One_year_ago_date)).\$     order_by(Measurement.date.desc()).all()$
filter_df['start_time'].min(), filter_df['start_time'].max() $
from sklearn.model_selection import train_test_split$ train, test = train_test_split(users, test_size = 0.2)
libraries_df = libraries_df.merge(libraries_metadata_df, on="asset_id")
most_active_stations = session.query(Measurement.station, func.count(Measurement.station)).\$         group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()0$ print (most_active_stations)
norm.ppf(1-(0.05/2))$
df['money_csum'] = df.groupby(['program_id'])['money_collected'].cumsum()$ df['cum_pct'] = df.groupby('program_id')['money_csum'].transform(lambda x: x * 100/ x.iloc[-1])$ df
lo = sm.Logit(df2['converted'], df2[['intercept','ab_page']])
pp.pprint(r.json())
twitter_archive.isnull().sum()
(p_diffs >actual_diff).mean()
dfz.set_index('timestamp', inplace=True)
df['date_int'] = df.created_at.astype(np.int64)$ tweetVolume(df)$ print("looks like there are some spikes in 2015.")
df_copy = df_groups.join(df_events.groupby(['group_id']).created.count(), how ='inner',on= 'group_id', lsuffix= '_left', rsuffix = '_count')$ df_copy = df_copy.sort_values(by = 'created_count', ascending = False)$ df_copy.head()
x_values = df2.reset_index()[['intercept', 'ab_page']].values.tolist()$ X = df2[['intercept', 'ab_page']] 
pt_weekly['WoW'] = pt_weekly['num_listings'].diff(periods=1)$ pt_weekly.head(10)
df['launched_year'] = df['launched'].dt.year$ df['deadline_year'] = df['deadline'].dt.year
cities_df = pd.DataFrame()$ weather_df = pd.DataFrame()
result.summary()
joined_hist.joined = pd.to_datetime(joined_hist.joined)$ joined_hist = joined_hist.sort_values(ascending=True, by='joined')$ joined_hist.head()$
colors = ["green", "red", "blue", "orange", "maroon"]$ x_axis = np.arange(len(final_df))$ x_labels = diff_df.index
tmax = tmax_day_2018.tmax[0]$ tmax.plot()
q1_url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31?api_key="+API_KEY$ r1 = requests.get(q1_url)$ r1 = r1.json()$
dict_1=request.json()$ dict_1.get('dataset_data')
response = requests.get('https://www.jconline.com/search/gun%20control/')$ soupresults2 = BeautifulSoup(response.text,'lxml')$
mapInfo = refl['Metadata']['Coordinate_System']['Map_Info'].value$ mapInfo
excelDF.drop("Calc",axis=1,inplace=True)$
%time nb.fit(train_4, y_train)
df = pd.read_csv('ab_data.csv')$ df.head()
keys = tweepy.OAuthHandler(consumer_key, consumer_secret)$ keys.set_access_token(access_token, access_token_secret)$ api = tweepy.API(keys, parser=tweepy.parsers.JSONParser())
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
logit_mod = sm.Logit(df_new["converted"], df_new[["intercept","CA", "UK", "treatment"]])$ results = logit_mod.fit()$ results.summary()
titles_list = temp_df2['titles'].tolist()
archive_clean.info()
obs_diff = np.mean(new_page_converted)-np.mean(old_page_converted) $ obs_diff
even_counter.value
d=[datetime.strptime(x, '%m/%d/%Y') for x in datestrs]$
Lags = MSUStats.groupby(level=1).shift(periods=1)  $ Lags.columns = [c+'Lag1' for c in Lags.columns]$ Lags.head()
affair_yrs_married = pd.crosstab(data.yrs_married, data.affair.astype(bool))$ affair_yrs_married
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))$ colors
precip_df = precip_df.sort_values(by = 'Date')
datascience_tweets[datascience_tweets['text'].str.contains("RT")]['text'].count() # 322
snap_data = df_snap.copy()
for col in user_df.columns[3:]:$     print col, user_df[col].unique()
gatecount_station_line_gameless = gatecount_station_line[gatecount_station_line.service_day.isin(all_game_dates) == False]
chartio.columns
train_data, test_data = train_test_split(status_data, test_size=0.50)$ train = train_data.values$ test = test_data.values
new_page_converted=np.random.choice([1,0],size=n_new,p=[p_new,1-p_new])$ new_page_converted.mean()
lm = sm.Logit(df2['ab_page'], df2[['intercept', 'CA', 'UK']])$ result = lm.fit()$ result.summary()
stations_df.head()
df_temp_by_USC00519281 = pd.read_sql_query("select tobs from measurement where station = 'USC00519281' AND date > '2016-08-22';", engine)$ df_temp_by_USC00519281
df.info()
logit = sm.Logit(df_new['converted'],df_new[['intercept','US','CA']])$ results = logit.fit()$ results.summary()
[tweet.lang for tweet in tweet_list]
topLikes.sort_values(by='Likes', ascending=[False])
from sklearn.model_selection import cross_val_score$ print(cross_val_score(model4, x_val, y_val))$ print(cross_val_score(model5, x_val, y_val))
(frame.loc['b']).loc[2]
df.info()
                                     df.dtypes))
ibm_hr_int = ibm_hr_target.select(numerical)$ ibm_hr_int.show(3)
y = df['comments']$ X = df['title']$ X = X.apply(lambda x: PorterStemmer().stem(x))
userMerged['createdtm'] = pd.to_datetime(userMerged['createdtm'], format = '%Y-%m-%d %H:%M:%S.%f')$ userMerged = userMerged[userMerged["createdtm"] < startDate]$ print("the latest account was created at:",max(userMerged.createdtm))
df_grp = df2.groupby('group')$ df_grp.describe()
male = crime.loc[crime['Sex']=='M']$ male.head(3)
df = frame_masher()$ df = df.loc[df['job_id'] == '2572']
df_combined = df2.merge(df_countries, on='user_id')$ df_combined.head()
df = PredClass.df_model$ print df.dtypes.loc[df.dtypes == 'object']
step_counts = step_counts.fillna(0.)
hm = pd.read_csv('Resources/hawaii_measurements.csv')$ hs = pd.read_csv('Resources/hawaii_stations.csv')
austin.shape$
Actual_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ Actual_diff $
os.chdir("E:/Data/Client1/Excel Files")$ path = os.getcwd()$ files = os.listdir(path)
contractions_df = pd.read_csv('data/contractions.csv', sep=' -')$ contractions = [word for word in contractions_df['from']]$ contractions[18] = "mightn't"$
[t for t in df.in_response_to_tweet_id.tolist() if not isinstance(t, float)]
gs.fit(X,y)
logit_countries_model2 = sm.Logit(df4['converted'], df4[['abs_page','country_UK', 'country_US', 'intercept']])$ results_countries_2 = logit_countries_model2.fit()
lr.fit(X_train, y_train)
print(loan_stats["revol_util"].na_omit().min())$ print(loan_stats["revol_util"].na_omit().max())$
p_new=df2.query('converted == 1').user_id.nunique()/df2['converted'].count()$ p_new
index=pd.date_range('8/14/2017', '12/31/2017')
soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
post_gc_launch_sales =  shopify_data_merge_msrp[shopify_data_merge_msrp['Created at'] >= '2018-06-08']$ post_gc_launch_sales[(post_gc_launch_sales['Email'] == 'vincent.brouwer1977@gmail.com')]$
pnew=df2.converted.mean()$ pnew
import re$ pbptweets.loc[pbptweets['text'].apply(lambda x: any(re.findall('Santos',x)))][['date','screen_name','text']]
Station = Base.classes.station$ Measurements = Base.classes.measurements
len(pd.unique(df2['user_id']))
for row in session.query(measurements, measurements.tobs, stations.station).limit(5).all():$     print(row)
get_nps(combined_df, 'device').sort(columns='score', ascending=False)
avg_att_2015_MIL = nba_df.loc[(nba_df["Season"] == 2015) & (nba_df["Team"] == "MIL"), "Home.Attendance"].mean()$ round(avg_att_2015_MIL, 0)
url_df=pd.DataFrame({'url':urls})
df_stars_count = df_stars.groupby('stars').size()$ print(df_stars_count)
engine.execute('SELECT * FROM station LIMIT 5').fetchall()
test_id = pax_raw.seqn.values[1]$ pax_sample = pax_raw[pax_raw.seqn==test_id].copy()
total = scores.sum()$ scores[:2.75].sum()/total
mean_encoding_test(val, train,'Block',"any_spot" ) $ mean_encoding_test(val, train,'DOW',"Real.Spots" ) $ mean_encoding_test(val, train,'hour',"Real.Spots" ) 
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df.dropna(inplace=True) #because we are prediction 30 days extra empty values are created in other rows.$
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.tail()
msft.dtypes
def get_filtered_tokens2(text):$     tokens = word_tokenize(text)$     filtered_tokens = [w.lower() for w in tokens if not w.lower() and not w.lower() in [':',"#", '@', 'https', ';','&', ',']]$
lReligion = list(db.osm.find({"religion":{"$exists":1}}))$ print 'length of the list = ', len(lReligion)$ lReligion[:5]
mars_facts_df.columns = ['Characteristic','Data']$ mars_df_table = mars_facts_df.set_index("Characteristic")$ mars_df_table
x_train,x_test,y_train,y_test=train_test_split(df_predictors,df_tripduration,test_size=.2, random_state=1)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=z-6V67L2Lei8_zy742pd&start_date=2017-01-01&end_date=2018-01-01', auth=('user', 'pass'))$
data_sample['value'] = [i.replace(',', '') for i in data_sample['value']]$ data_sample['value'] = pd.to_numeric(data_sample['value'])
tweet_archive_clean['stage'] = tweet_archive_clean[['doggo', 'floofer','pupper', 'puppo']].apply(lambda x:''.join(x), axis= 1)
df2.drop('drop_it', axis=1, inplace=True)$ df2.head()
sh_max_df.describe(include="all")
raw_data = pd.read_csv('./activities_201802011009.csv')
afxx_data = r.json()
df2['user_id'].nunique()
df_new2=df.query('landing_page=="old_page" & group=="treatment" ')$ df_new2.tail(10)$ df_new2.nunique()
print(afx_x_oneday.json())
initial_trend(series, 12)$ initial_seasonal_components(series, 12)$ triple_exponential_smoothing(series, 12, 0.716, 0.029, 0.993, 24)$
cnn = news_sentiment('@CNN')$ cnn['Date'] = pd.to_datetime(cnn['Date'])$ cnn.head()
ab_df.isnull().sum()$
train_df4.head()$ train_df.head(1)
df3['created_at'] = df3['created_at'].apply(lambda x: x.round('min'))
user.iloc[1, 1:5]
rPnew = df2[df2['landing_page']=='new_page']['converted'].mean()$ rPold = df2[df2['landing_page']=='old_page']['converted'].mean()$ rp_diffs = rPnew - rPold
joined = joined.astype(intDict)
temp_long_df = pd.melt(temp_wide_df, id_vars = ['grid_id', 'glon', 'glat'],$                       var_name = "date", value_name = "temp_c")$ temp_long_df.head()
validation.analysis(observation_data, BallBerry_resistance_simulation_0_25)
df2['intercept'] = 1$ df2[['control','ab_page']] = pd.get_dummies(df2['group'])
out=final.sort_values(by=['priority','RA0'])$ out=out[['name','RA0','DEC0','redshift','min_air','best_time','priority']]$ out.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/all_objs_winter2017.csv',index=False)$
sales_agg = sales[sales['Store Number'].isin(good_stores)]     #iterate through stores in the good store list and pass them to dataframe$ print sales_agg.shape   $
poparr.shape
index = similarities.MatrixSimilarity(lsi[corpus])
taxiData.Trip_distance.size$
ldamodel.print_topics()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')$ z_score, p_value
print(pd.to_numeric(countdf['number_votes']).sum())$ print(pd.to_numeric(countdf['number_votes']).sum()-pd.to_numeric(count1df['number_votes']).sum())$ print(pd.to_numeric(countdf['number_votes']).sum()-pd.to_numeric(count6df['number_votes']).sum())
X = endometrium_data.drop(['ID_REF', 'Tissue'], axis=1).values$ y = pd.get_dummies(endometrium_data['Tissue']).values[:,1]
df_enhanced[['dog_name']].groupby(['dog_name'])['dog_name'].size()
df.query('landing_page == "new_page" and group == "control"').count()[0] + df.query('landing_page == "old_page" and group == "treatment"').count()[0]
print(df.info())
utility_patents_df.number_of_claims.describe()
df3[df3['group'] == 'treatment'].head()
twitter_Archive.drop(['timestamp'], axis=1,inplace=True)$ twitter_Archive.info()
twitter_df = pd.DataFrame(results_dict)$ twitter_df.head()
twitter_archive.info()
autos = autos[autos["price"].between(100,1000000)]
from scipy.stats import norm$ norm.cdf(z_score)$
autos.dropna(subset=['price'],inplace=True)$ autos.shape
df_new[['UK','US','CA']] = pd.get_dummies(df_new['country'])$ df_new.drop('UK',axis=1, inplace=True)$ df_new.head()
x_normalized = pd.DataFrame(x_scaled)
text = data_df['clean_desc'].apply(cleaning)$ text_list = [i.split() for i in text]$ len(text_list)
start = time()$ ldamodel_3 = Lda(doc_term_matrix, num_topics=16, id2word = dictionary, passes=25, chunksize=2000)  # with 16 topics$ print('CPU time for LDA_3: {:.2f}s'.format(time()-start))$
free_data.groupby('age_cat').mean()
final_df = pd.concat([drugs_through_bp, other_drug])$ final_df = final_df.sort_values(by=['DWPC'], ascending = False)$ final_df
common_words_file = open('common_words.txt')$ common_words = common_words_file.read().split(',')$
df2 = pd.read_csv('ab_data_cleaned.csv')
n_new = df2[df2['landing_page']=="new_page"].count()[0]$ n_new
X_15_16 = np.concatenate((training_X_scaled[-15:],holdout_X_scaled),axis=0)$ y_15_16 = np.concatenate((training_y[-15:],holdout_y),axis=0)
df2['ab_page_and_UK'] = df2['ab_page']*df2['UK']$ df2['ab_page_and_CA'] = df2['ab_page']*df2['CA']$ df2.head(10)
now = Timestamp("now")$ local_now = now.tz_localize('UTC')$ now, local_now
model = app.models.get("Textures & Patterns")$ model.predict_by_url(url='https://assets.vogue.com/photos/595a6e6ca236912379c91ccf/master/pass/_ARC0003.jpg')
tweets_raw["date"] = tweets_raw["date"].apply(lambda d: parse(d, ignoretz = True))
%matplotlib inline$ commits_per_year.plot(figsize=(12,7), kind='bar', legend=None, title='Commits per year')
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period()$ print("Trend for month: ", get_trend(get_timeseries(closed_pr)))
grouped = df_cod2.groupby(["Death year", "Cause of death"])$ grouped.size()
difference = actual_p_new - actual_p_old$ p_diffs = np.array(p_diffs)$ (difference<p_diffs).mean()
cityID = '7c01d867b8e8c494'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Garland.append(tweet) 
twitter_df = spark.createDataFrame(delimited_twitter_df)
X = df_mes2.loc[0:1000000,cols]$ Y = df_mes2.loc[0:1000000,['tip_amount']]
df_goog['Closed_Higher'] = df_goog.Open > df_goog.Close$ df_goog['Closed_Higher'] = pd.get_dummies(df_goog.Open > df_goog.Close).values
post_discover_new_customers_validate = sales_data_clean[~(sales_data_clean['Email'].isin(post_discover_new_customers['Email']))]$ post_discover_new_customers_validate[post_discover_new_customers_validate['Created at'] >= "2017-09-09"]
agg_trips_data = trips_data.groupby('start_station_id').agg({'id':'count','duration':['mean','sum']}).reset_index() # aggregatig data$ agg_trips_data.columns= ['start_station_id','count','mean_duration','total_duration'] #to overwrite multi-column indexe with single index
logit = sm.Logit(df3['converted'],df3[['intercept' ,'treatment']])$ result = logit.fit()
data = pd.read_csv('./fake_company.csv')$ data
exchanges = ccw.get_exchanges_list()$ EXCHANGE_DB = pd.DataFrame.from_dict(exchanges, orient='index')$
np_diff = np.diff(close)$ max(np_diff)$
type(df_clean['tweet_id'].iloc[0])$ type(image_clean['tweet_id'].iloc[0])$ type(tweet_clean['tweet_id'].iloc[0])
import pprint$ pprint.pprint(vars(example_tweets[0]))
print("date_crawled:", autos["date_crawled"].str[:10].unique().shape[0])$ print("last_seen:", autos["last_seen"].str[:10].unique().shape[0])$ print("ad_created:", autos["ad_created"].str[:10].unique().shape[0])
treatment_df = df2.query('group == "treatment"')$ p_treatment = treatment_df.query('converted == 1').user_id.nunique()/treatment_df.user_id.nunique()$ print('Probability of conversion for the treatment group is {}.'.format(round(p_treatment,4)))
data_frame['CLASS1'].value_counts()
((revenue[revenue.cum_revenue_pct <= 0.81].fullVisitorId.nunique())/revenue.shape[0])
words = [w for w in words if not w in stopwords.words('english')]$ print(words[:100])
df[['text', 'retweet_count', 'date']][df.retweet_count == np.max(df.retweet_count)]
matches['home_team_win_or_draw'] = 0$ matches.loc[matches['home_team_goal'] >= matches['away_team_goal'], 'home_team_win_or_draw'] = 1
analyser.polarity_scores(twitter_datadf['text'][1])$
port.positions.stats
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')$ print("z-score:", z_score,$      "\np-value:", p_value)
new_page_converted = np.random.normal(0, p_new, n_new)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[np.mean([p_new, p_old]), (1-np.mean([p_new, p_old]))]).mean()$ new_page_converted
new_page_converted.mean()-old_page_converted.mean()
X = pd.get_dummies(X, drop_first = True)$ X.head()
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))
treatment_group_user_count = df2[df2['group'] == 'treatment']['user_id'].count()$ converted_treatment_user_count = df2[(df2['group'] == 'treatment') & (df2['converted'] == True)]['user_id'].count()$ p_treatment_converted = converted_treatment_user_count / treatment_group_user_count$
cityID = 'dd3b100831dd1763'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_Orleans.append(tweet) 
con = sqlite3.connect('db.sqlite')$ pd.read_sql_query("SELECT * from sqlite_master", con)
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'America/Los_Angeles').strftime('%D: %r')$ date_min = date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'America/Los_Angeles').strftime('%D: %r')
WHOregion = df['WHO Region'].values$ WHOregion$
page_size = 200$ response = client.get('/users', q='footwork', limit=page_size,$                     linked_partitioning=1)$
tweets_df.userLocation.value_counts()
df_train.columns.values$
list.sort()$ print(list)
np.shape(temp_us_full)
df1.loc[0:8,['SerNo', 'By']]
display(data.head(10))
print(response.text)
df_ad_airings_5['location'].isnull().sum()
df2 = df.dropna()$ df2.isnull().values.any()
injury_df.sort_values('Date', ascending=False).head()
df = quandl.get('WIKI/GOOGL')
df['2018-05-21'] # Try to understand the error ... Its searching in columns !!! $
tweet_clean.columns
train, valid, test = covtype_df.split_frame([0.7, 0.15], seed=1234)$ covtype_X = covtype_df.col_names[:-1]     #last column is Cover_Type, our desired response variable \n",$ covtype_y = covtype_df.col_names[-1]    
c_df = new_df.dropna(how='all') $ c_df.size
(taxiData2.Fare_amount == 0).any() # This Returns True, meaning there are values that equal to 0
date.strftime("%A")
sim_prop_new_converted = np.random.binomial(num_new, null_p_new, 10000) / num_new$ sim_prop_old_converted = np.random.binomial(num_old, null_p_old, 10000) / num_old$ p_diffs = sim_prop_new_converted - sim_prop_old_converted
df.head()
from goatools import obo_parser$ oboUrl = './data/go.obo'$ obo = obo_parser.GODag(oboUrl, optional_attrs=['def'])
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])
dd_df = df.drop_duplicates(features[i] for i in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15])$ dd_df = dd_df[['price', 'used_month', 'gearbox', 'powerPS', 'kilometer', 'notRepairedDamage', 'fuelType']]$
df_2016['bank_name'] = df_2016.bank_name.str.split(",").str[0]$
from scipy.stats import norm$ norm.cdf(z_score), norm.ppf(1 - 0.05)
f=table.find(text='Fatalities').find_next('td').text$ fatalities=re.search(r'\d+', f).group()$ fatalities
S_lumpedTopmodel.decision_obj.groundwatr.options, S_lumpedTopmodel.decision_obj.groundwatr.value
actual_diff = df[df['group'] == 'treatment']['converted'].mean() - df[df['group'] == 'control']['converted'].mean()
ny_df=pd.DataFrame.from_dict(foursquare_data_dict['response'])
df_final.loc_country.value_counts()
comments = pd.concat([comments_df, comments_df_middle])$ comments = comments.drop_duplicates()$ comments.to_csv('seekingalpha_top_comments.csv')$
import numpy as np$ ok.grade('q05')
dfAnnualMGD = dfHaw_Discharge.groupby('Year')['flow_MGD'].agg(['sum','count'])$ dfAnnualMGD = dfAnnualMGD[dfAnnualMGD['count'] > 350]$ dfAnnualMGD.columns = ['AnnualFlow_MGD','Count']
datetime.now().toordinal() - datetime(1987, 1, 4).toordinal()
df_json_tweets.to_csv('tweet_json.txt', encoding = 'utf-8', index=False)
g = open('../data/faa.txt').readlines()
segmented_rfm['RFMScore'] = segmented_rfm.r_quartile.map(str) + segmented_rfm.f_quartile.map(str) + segmented_rfm.m_quartile.map(str)$ segmented_rfm.head()
yc_new3 = yc_new2[yc_new2.tipPC < 100]
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
p_diffs = np.array(p_diffs)$ (p_diffs > actual_diff).mean()
df['sales_type'] = df['sales_type'].str.replace(u'\xa0', u' ')$
df2_new[['CA','UK']]=pd.get_dummies(df2_new['country'])[['CA','UK']]
print('Scanning for all greetings:')$ for key, row in table.scan():$     print('\t{}: {}'.format(key, row[column_name.encode('utf-8')]))$
Measurements = Base.classes.measurements$ Stations = Base.classes.stations
time2close_1 = time_to_close_1.sort_values(['launchpad_issue'])
import pandas as pd$ the_dict = pd.read_clipboard().to_dict('records')$ the_dict
conf_matrix = confusion_matrix(training_test_labels, lr.predict(preproc_training_test), labels=[1,2,3,4,5])$ conf_matrix
df2['user_id'].value_counts().head()$
itemTable["Energy"] = itemTable["Content"].map(energy_level)
StockData.reset_index(inplace=True)
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']])$ result=logit.fit()
train.nrows
The number of violations has a descending trend.$
site_visits_rebuild.head()
store1_open_data = store1_data[store1_data.Open==1]$ store1_open_data[['Sales']].plot()
tweet_archive_clean['dog_type'] = tweet_archive_clean['text'].str.extract('(doggo|floofer|pupper|puppo)', expand=True)
df3 = df.loc[lambda df: df['duplicate_user'] == 1]$
census_zip = df_station.merge(wealthy, left_on=['zipcode'], right_on=['zipcode'],  how='left')$ nan_rows = census_zip[census_zip['betw150kand200k'].isnull()]$ census_zip.dropna(inplace = True)
total_control = df2[(df2['landing_page'] == "old_page")].count()$ print(total_control)
plt.show()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
diff_act=df2[df2['group'] == 'treatment']['converted'].mean()-df2[df2['group']=='control']['converted'].mean()$ (p_diffs > diff_act).mean()
print(data_all[data_all < 101].describe())
old_page_converted = np.random.choice([0,1] ,size = n_old, p=[(1-p_old), p_old])$ old_page_converted.mean()
sales_df.groupby('Country').count()['Revenue'].sort_values(ascending=False)$
fraud.device_id.duplicated().sum()
mydf1.info()
W, H, err = fit_nmf(df_utility, 100)$ print(err)$ print(W.shape,H.shape)
import json$ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=apikey')$
posts.groupby(['from', 'hostname_clean'])['post'].aggregate(sum)
from sklearn.utils import shuffle$ qs = shuffle(qs)$ print len(qs)
pd.read_sql('SELECT * FROM experiments', conn, index_col='experiment_id')
X_new = test_tfidf.loc[:207]$ new_x = X_new.iloc[:,index_smote]$ new_y = test_tfidf['y']
CD.describe()$
lq = pd.read_csv('../Assets/Iowa_Liquor_sample.csv',parse_dates=['Date'],infer_datetime_format=True)
mostRecentTweets.head()
df2 = df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index)$ df2 = df2.drop(df.query("group == 'control' and landing_page == 'new_page'").index)$ df2.shape
Inspection_duplicates = data_FCInspevnt_latest.groupby(['brkey'])[['Inspection_number']].sum()$ Inspection_duplicates = Inspection_duplicates.loc[Inspection_duplicates['Inspection_number'] > 1]$ Inspection_duplicates
control_convert = df2.query('group =="control"').converted.mean()$ print("Probability of control group converting is :", control_convert)
data_NL = data_NL.sort_values($     'capacity', ascending=False).groupby('name', as_index=False).first()$ data_NL = data_NL.reindex(columns=columns_sorted)
wedate=data['Week Ending Date']$ wedate.head(10)
df2['dategroup'] = df2['date'].apply(date_cat)
df2.shape
from sklearn import metrics$ metrics.roc_auc_score(new.popular, new_pred_prob)
mydata.json()['dataset_data']['data'][1]
dog_ratings.source.value_counts()
techmeme['sources'] = techmeme.extra_sources.copy()$ for i, list_ in enumerate(techmeme.sources):$     list_.append(techmeme.original_source[i])
df_final_ = df_final.query('loc_country == country').reset_index().drop('index',axis=1)$ print(df_final_.head(5))$ print(df_final_.shape)
dataset.shape
conn = sqlite3.connect("/Users/nicksteil/Desktop/FPA_FOD_20170508.sqlite")$ wildfires_df = pd.read_sql_query("select FIRE_Year, FIRE_SIZE, STATE from Fires;" , conn)$
df_Tesla['tokens'] =stemmed$ df_Tesla.head(2)
df_cod2 = df_cod.copy()$ df_cod2 = df_cod2.dropna()
score = model.evaluate(test_train_vecs_w2v, y_test_train, batch_size=128, verbose=2)$ print (score[1])
for obj in bucket_obj.objects.all():$     print('Object key: {}'.format(obj.key))$     print('Object size (kb): {}'.format(obj.size/1024))
pd.to_datetime(['2009/07/31', 'asd'])
lm = sm.Logit(df4['converted'],df4[['intercept','UK','US']])$ results = lm.fit()$ results.summary()
df_western = df[df['genres'].str.contains("Western")]
n_new,n_old = df2.landing_page.value_counts()$ print(n_new,n_old)
old_page_converted = np.random.choice([0,1], size = n_old, replace=True,p=[1-p_old, p_old])
logit_countries = sm.Logit(df4['converted'], $                            df4[['US', 'UK', 'intercept']])$ result2 = logit_countries.fit()$
from carto.tables import TableManager$ help(TableManager)$ table_manager = TableManager(auth_client)$
dic = dict(req.json())
total_y = list(youTubeTitles.values[:2500,1]) + list(pornTitles.values[:2500,1])
obs_data = session.query(Measurements.tobs).filter(Measurements.station == 'USC00519397').statement$ obs_data_df = pd.read_sql_query(obs_data, session.bind)$ obs_data_df.head()
html = browser.html$ img_soup = BeautifulSoup(html, 'html.parser')
unique = df2.user_id.nunique()$ print("{} unique 'user_id' could be found in the dataset 'df2'.".format(unique))
temp_wide_df = pd.concat([grid_df, temp_df], axis = 1)$ temp_wide_df.head()
postags = sorted(set([pos for sent in train_trees for (word, pos) in sent.leaves()]))$ print(unigram_chunker.tagger.tag(postags))
import pandas as pd$ tweets = pd.read_csv('tweets.csv')
data_config = dict(path='tests/data/nhtsa_as_xml.zip',$                    databasetype="zipxml", # define that the file is a zip f$                    echo=False)
xgboostmodel = 'xgboostmodel.sav'$ pickle.dump(xgboost, open(xgboostmodel, 'wb'))
act['bin'] = pd.cut(act.activity_count, bins=[-1,0,1,2,5,100])$ gract = act.groupby(['month', 'bin']).activity_count.agg(['size'])$
A = rng.randint(10, size=(3,4))$ A
engine = create_engine("sqlite:///hawaii.sqlite")$
from sklearn.metrics import f1_score$ f1_score(y_test, y_pred_lgr, average='macro')  
features = weather_features.merge(DC_features,left_index=True,right_index=True)
r6s.num_comments.mean()
np.setdiff1d(np.arange(6567,8339),ORDERS_GEN['order_number'])
merged[['CA', 'UK', 'US']] = pd.get_dummies(merged['country'])$ merged.head()
sox.to_csv('../../../data/sports_analysis/sox.csv', index=False)$ bruins.to_csv('../../../data/sports_analysis/bruins.csv', index=False)$ celtics.to_csv('../../../data/sports_analysis/celtics.csv', index=False)
df_master.to_csv('twitter_archive_master.csv', index= False, encoding='utf-8')
vacancies['created'] = vacancies['created'].apply(lambda x: dateutil.parser.parse(x))
unique, counts = np.unique(y_hat, return_counts=True)$ print(unique, counts)
word_vecs.sample(10)
import builtins$ builtins.uclresearch_topic = 'GIVENCHY'$ from configuration import config
ttt = list(spp.term.unique())
df3['intercept'] = pd.Series(np.zeros(len(df3)), index=df3.index)$ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index=df3.index)
df_group_by = df_group_by.reset_index()$
df.groupby('key').aggregate({'data1': 'min',$                             'data2': 'max'})
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Pothole') ].groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False])$
new_model = gensim.models.Word2Vec.load(temp_path)  
data["Complaint Type"].value_counts().head(10)
iris.head().loc[:,:"Sepal.Length"]$
df = pd.read_csv(source_csv)$ df.head()
loans_df.loan_status.replace(inplace=True, to_replace='Default', value='Charged Off')
results.shape
json_data = r.json()$ for k in json_data.keys():$     print(k + ': ', json_data[k])$
pnew=df2['converted'].mean()$ print(pnew)
df_country = pd.read_csv('countries.csv')$ df_country.head()
FREEVIEW.plot_fixation_durations(raw_freeview_df)
df[0].plot()
USvideos.describe()$ USvideos.head()
df.index.tz_localize('GMT').tz_convert('US/Eastern')
df2 = df2.drop_duplicates(['user_id'])
df3[pd.isnull(df3).any(axis=1)]
for column in df_categorical:$     df_categorical[column].unique()
!wget -O cell_samples.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/cell_samples.csv
full_df['price_per_bedbath'] = full_df.apply(lambda x: x.price if x.bedrooms + x.bathrooms == 0\$                                              else x.price / (x.bedrooms + x.bathrooms), axis=1)
from pysumma.utils import utils
fix_comma = lambda x: pd.Series([i for i in reversed(x.split(','))])
df.iloc[99:104]
plt.show()$
p_new = df2[df2['landing_page']=='new_page']['converted'].mean()$ print("Probability of conversion for new page (p_new) is", p_new)
df_tte[df_tte['ReservedInstance'] == 'N']['InstanceType'].unique() 
from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report$ from sklearn.model_selection import GridSearchCV, StratifiedKFold$ from sklearn import preprocessing
daily_df = pd.concat([predictions_daily, stock_daily], axis=1, join='inner')$ daily_df.head()
p_new = df2.converted.mean()$ print("{:.4f}".format(p_new))
df['DAY'] = 1 $ df['DATE'] = pd.to_datetime(df[['YEAR','MONTH','DAY']])
df = pd.read_csv('../input/crime.csv')$ df.head()
model = gensim.models.Word2Vec(sentences, workers=4)
dog_ratings['source'] = dog_ratings.source.str.extract('<a[^>]*>([^<]*)</a>', expand=True)$ dog_ratings.source = dog_ratings.source.astype('category')$ dog_ratings.source.value_counts()
conv_prob = df2['converted'].mean() * 100$ output = round(conv_prob, 2)$ print("The probability of an individual converting regardless of the page they receive is: {}%".format(output))
inc_summ_df = incident_file_df[["WkStart","ILITOTAL" ]]$ inc_summ_df.head(10)
ibm_train = ibm_hr_final.join(ibm_hr_target.select("Attrition_numerical"))$ ibm_train.printSchema()
tweets['created_at'] = pd.to_datetime(tweets['created_at'])$ tweets.dtypes
X_copy['crfa_a'] = X_copy['crfa_a'].apply(lambda x: int(x))
df = pd.read_csv('basic_statistics_single_column_data.csv')
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
gene_df['gene_name'].unique().shape
df[(df['group'] == 'treatment') != (df['landing_page'] == 'new_page')].shape[0]$
table_rows = driver.find_elements_by_tag_name("tbody")[24].find_elements_by_tag_name("tr")$
m1.fit()$ m1.forecast()$ m1.plot(plot_type = 'autocorrelation' ,lag = 2)$
urban_ride_total = urban_type_df.groupby(["city"]).count()["ride_id"]$ urban_ride_total.head()
rankings_USA.query("rank <= 10")['rank_date'].count()$
df.to_csv('ab_cleaned.csv', index=False)
brand_pct = dict(autos["brand"].value_counts(normalize=True))
df_RSV = df_full[df_full.Field4 == "RSV"]
df.is_profile_20.value_counts()
p_new = df2['converted'].mean()$ print("{} is the convert rate for  Pnew under the null.".format(p_new))
df['converted'].mean()$
lda.print_topics()
transactions[~transactions['UserID'].isin(users['UserID'])]
poparr = fe.bs.csv2ret('boots_ndl_d4spx_1957-2018.csv.gz',$                        mean=fe.bs.SPXmean, sigma=fe.bs.SPXsigma,$                        yearly=256)
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))$
S_1dRichards = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_1dRichards.txt')
df_western.groupby(['release_decade'], as_index = False)['id'].count()
!hdfs dfs -cat {HDFS_DIR}/p32c-output/part-0000* > p32c_results.txt
print(data.petal_length.median())
df.sample(frac=0.7, replace=True).duplicated().value_counts()
weather_df = weather_df.fillna(method = "ffill")
today = pd.to_datetime("Today")$ todays_311s = pgh_311_data[str(today.date())]$ todays_311s.head()
conn.commit()
russians_df = pd.read_csv('users.csv')
files = [name for name in listdir() if 'csv' in name]$ fileName = files[0]$ fileName = '866192035974276_AXIO.csv'
fit1_test = fh_1.transform(test.device_model)$ fit2_test = fh_2.transform(test.device_id)$ fit3_test = fh_3.transform(test.device_ip)
law = tc_final$ law['YBP sub-account'].replace("195099", "590099", inplace= True)$ law
session.query(func.count(station.station)).all()
ids = topics_data.groupby('id')[['score', 'comms_num']].sum()
df['PCT_change']=(df['Adj. Close']-df['Adj. Open'])/df['Adj. Open']*100.0
true_file = pd.read_csv(filepath_or_buffer=os.path.join(edfDir, 'pt1sz2_eeg.csv'), header=None)$ test_file = pd.read_csv(filepath_or_buffer=outputData, header=None)
LSST_catalog_data.close();LSST_catalog_data.close();
with open('./data/model/age_prediction_sk.pkl', 'wb') as picklefile:$     pickle.dump(grid, picklefile)
df_potholes.groupby(df_potholes.index.weekday)['Created Date'].count().plot(kind="bar")$
print(type(data.points))$ data[['points']]$
df.to_sql('tw_posts', engine, schema=schema, if_exists='append')
stations = session.query(Measurement).group_by(Measurement.station).count()$ print(f"{stations} stations")
from sklearn.feature_selection import VarianceThreshold$ sel = VarianceThreshold(threshold=(0.8 * (1 - 0.8)))
df_ad_airings_5 = pd.read_pickle('./TV_AD_AIRINGS_FILTER_DATASET_3.pkl')
store_items.interpolate(method='linear', axis=0)
grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)$ grid.map(plt.hist, 'Age', alpha=.5, bins=20)$ grid.add_legend();
contractor.tail()
scores_pairs_by_business = ins.sort_values('new_date').loc[ins['year']==2016].groupby('business_id').filter(lambda x: len(x) ==2)[['business_id','score']].groupby('business_id').agg(lambda x:[i for i in x]).rename(columns={'score': "score_pair"})$ def group_to_list(group):$     return list(group)
os.system(string_01)
print(len(df_clean))$ df_clean = df_clean.dropna(subset=['expanded_urls'])
logit_countries2 = sm.Logit(df4['converted'], $                            df4[['ab_page', 'country_UK', 'country_US', 'intercept']])$ result3 = logit_countries2.fit()
feed_ids = feeds[feeds['publication_id'] == pubs[pubs['name'] == 'Fox News'].id.values[0]]['id']$ print(feed_ids) # these can be used as indices for feeds_items
cols = list(df.columns)$ print(len(cols))$ print(cols)
countries_df = pd.read_csv('E:/DA_nanodegree/PROJECT4/analyzeabtestresults-2/AnalyzeABTestResults 2/countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
winpct = pd.read_csv('All_Games_Win_Pct.csv')$ winpct['text'] = winpct['playtext']$ winpct['date'] = pd.to_datetime(winpct['Game Date'])
ids = new_stops['stopid']$ new_stops[ids.isin(ids[ids.duplicated()])]
manager.image_df['p_hash'].isin(tree_features_df['p_hash']).describe()
df_columns.groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False])$
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
big_change = max([day[2] - day[3] for day in afx_dict['dataset_data']['data']])$ print("The largest change in stock price was " + str(round(big_change,2)) + ".")
cbs = news_sentiment('@CBSNews')$ cbs['Date'] = pd.to_datetime(cbs['Date'])$ cbs.head()
df = pd.DataFrame(precipitation_2yearsago, columns = ["prcp","date"])$ df = df.set_index("date")$ print(df)
tips.set_index(["sex","day"]).head(5)
addOne = cylHPData.mapValues(lambda x: (x, 1))$ print (addOne.collect())$
melted_total.groupby(['Categories','Neighbourhood']).mean().unstack()['Rating'].ix[top10_categories.index].plot.bar(legend=True,figsize=(10, 5))
result_df=pd.merge(df2,countries,on='user_id')$ result_df.head()
definition_details = client.repository.store_definition(filename_keras, model_definition_metadata)
df_convs_master = df_totalConvs_day                                                $ years_unique = df_convs_master['year'].unique()$ years_unique
df.groupby('Hour').Avg_speed.mean()
ssh_host = '192.168.31.56'$ ssh_username = 'itouchtv'$ ssh_password = 'itouchtv'
print 'Read data from the web about Microsoft in 2012 - 2013'$ from pandas_datareader import data, wb$ msft = data.DataReader('MSFT', data_source='google', start='2012-01-01', end='2013-12-30')
old_page_converted = np.random.choice([1,0], size=df_oldlen, p=[pold,(1-pold)])$
df_dummies = pd.get_dummies(df_value, columns=['value'])$ df_dummies.head()
autos['date_crawled'] = autos['dateCrawled'].str[:10]
airquality_pivot = airquality_pivot.reset_index()$ print(airquality_pivot.index)
pdf = pd.read_csv('training_data/utah_positive_examples.csv')$ ndf = pd.read_csv('training_data/utah_negative_examples.csv')$ wdf = pd.read_csv('utah_weather_2010-2018_grouped.csv')
Date1yrago=dt.date.today()-dt.timedelta(days=365)$ print(Date1yrago)$
print(r.text[0:500])
len([premieScn for premieScn in SCN_BDAY_qthis.scn_age if premieScn < 0])/SCN_BDAY_qthis.scn_age.count()
p_old = df2['converted'].mean()$ p_old
df_countries.head()
tweets_master_df[tweets_master_df.isnull().any(axis=1)]
url = "https://data.wprdc.org/datastore/dump/76fda9d0-69be-4dd5-8108-0de7907fc5a4"$ pgh_311_data = pd.read_csv(url)$ pgh_311_data.head()
print("Identify Injured by Keyword")$ print(df.cdescr.str.contains('INJUR|HURT').sum())$ print(len(df))
f = open('..\\Output\\GenreString.csv','w')$ f.write(GenresString) #Give your csv text here.$ f.close()
np.var(p_diffs)
pieces = [x.strip() for x in s.split(',')]$ pieces$
X_extra = Train_extra.drop('Approved', axis=1)$ y_extra = Train_extra['Approved']$ kNN500.fit(X_extra, y_extra)
df_goog.index + timedelta(days=4380, hours=1, seconds=43)
df_date_precipitation.describe()
struct_v1_0 = sp_df_test.schema$ emptyDF = spark.createDataFrame(sc.emptyRDD(), struct_v1_0)$ emptyDF.write.parquet(os.path.join(comb_fldr, "flight_v1_0.pq"))
df_sum=pd.DataFrame(data=sum_row).T$ df_sum 
list_users = df2['user_id'].value_counts()$ list_users.head()
from sklearn.metrics import mean_squared_error, mean_absolute_error$ mean_absolute_error(y_test, y_pred_lm)
from scipy.stats import norm$ norm.ppf(1-(0.05/2))
most_recent_1yr_entry = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(most_recent_1yr_entry)
df3 = df2.copy()
ks_name_failed = ks_name_lengths.drop(ks_name_lengths.index[ks_name_lengths.state != 'failed'])$ ks_name_failed.set_index('name_length', inplace=True)
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Loud Music/Party') ].groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False])$
sns.catplot(x="Opening_year", y="HC01_VC36", hue="lic_code",$             col="isClosed", aspect=.6,$             kind="swarm", data=df);$
these NA values in country.It might me the case that user is accessing from unknown/fraudent IP address,So we will$
df1.rating_denominator.value_counts()
merged = pd.DataFrame.merge(mojog_df, youtube_df,on='movie_name', how = 'inner')$ merged.head()
newcolumns = df1.columns.str.strip()$ df1.columns = newcolumns
S_lumpedTopmodel.basin_par.filename
who_purchased.to_csv('../data/purchase.csv')
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-04-24&end_date=2018-04-24&api_key=" + API_KEY)
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False)$
dev3['type_first_engagement'] = dev3.apply(get_first_engagement, axis=1)$ dev3['type_last_engagement'] = dev3.apply(get_last_engagement, axis=1)$ dev3['days_involved'] = dev3.apply(get_time_engaged, axis=1)
regression_estimation(X,final_df['mean'])$ classification_estimation(X,final_df['mean'].map(lambda x: x*10))$
census_withdata = census.merge(all_boroughs_cleaned,how='inner',left_on='GEOID_tract',right_on='GEO.id2')
indices = ml.show_feature_importance(df, '3M')
df2['landing_page'].value_counts()[0]/len(df2)
xml_in_sample.shape
from bs4 import BeautifulSoup$ import requests$ url = 'https://www.reddit.com/r/Python/'
from scipy.stats import norm$ norm.cdf(z_score)$
cars = pd.read_csv("/Users/ankurjain/Desktop/autos.csv", encoding='latin1')$ cars.head()
lm2=sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']])$ results=lm2.fit()$ results.summary()
df_weather.dtypes
first_month = grouped_months_new.get_group(8) $ second_month = grouped_months_new.get_group(9) $ third_month = grouped_months_new.get_group(10) $
sqlCtx = pyspark.SQLContext(sc)$ sdf = sqlCtx.createDataFrame(df.astype(str))$ sdf.show(5)
predict.predict_score('Wikipedia')
ved['season'] = ved.index.str.split('.').str[0]$ ved['term'] = ved.index.str.split('.').str[1]
import sqlite3$ conn = sqlite3.connect("database.db")$ cursor = conn.cursor()$
df3 = df / df.iloc[0, :]$ df3.plot()$ plt.show()
bruins['season'] = (pd.DatetimeIndex(bruins.date) - np.timedelta64(9,'M')).year
precip_df = precip_df.sort_index(ascending = True)$ precip_df.head()
!wget http://files.fast.ai/data/dogscats.zip && unzip dogscats.zip -d data/
(act_diff < p_diffs).mean()
df["EDAD"].apply(sumar_1)
back2sent = word_vecs.apply(lambda x: ' '.join(x))$ transform = TfidfVectorizer(lowercase=False, max_df=.1, max_features=2000, ngram_range=(1,3))$ tf_idf_matrix = transform.fit_transform(back2sent.values)
stores.drop("GrandTotalSales",axis = 1,inplace=True)$
df['Vendor Number'].nunique() #115$
df.info()
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
frames = [data_BBC, data_CBS, data_CNN, data_FoxNews, data_nytimes]$ big_data = pd.concat(frames)$ big_data.to_csv('SentimentAnalysisData.csv')
IssueCommentEvents['payload'].iloc[0]
model = RandomForestClassifier(n_estimators = 10)$ model.fit(X, y)
df2 = df2.join(countries.set_index('user_id'), on='user_id')$ df2.head()
autos.drop(["seller", "offer_type","nr_of_pictures"], axis = 1, inplace = True)
logit_mod = sm.Logit(df3['converted'], df3[['intercept','ab_page', 'CA', 'UK']])$ results = logit_mod.fit()$ results.summary()
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_jp_wzh_2.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_wzh_2.w2v')
c_trt = df2.query('group == "treatment"')['converted'].mean()$ c_trt
mlab_uri = os.getenv('MLAB_URI')$ mlab_collection = os.getenv('MLAB_COLLECTION')
stat, p, med, tbl = scipy.stats.median_test(df3["tripduration"], df4["tripduration"])$ print p$ print tbl
df2.converted.mean()
nasa_url = 'https://mars.nasa.gov/news/'$ jup_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
df2.user_id.nunique()
df.info()
train_dum.drop(['Source_S130', 'Source_S135', 'Source_S140', 'Source_S154', 'Source_S160',$                'Customer_Existing_Primary_Bank_Code_B056'], axis=1, inplace=True)$ test_dum.drop(['Source_S126', 'Source_S131', 'Source_S132', 'Source_S142'], axis=1, inplace=True)
p_mean = np.mean([p_new, p_old])$ p_mean
trans_data = pd.read_csv('transaction_data/btc_transaction_data.csv')
print('Median daily volume of 2017: ' + str(int(np.median(vol_vec))))
for inst in idx_set:$     with open('../notebooks/subsample_idx_{}.json'.format(inst), 'w') as fd:$         json.dump(list(idx_set[inst]), fd, indent=2)$
m = pd.read_json('njdevils_gr_posts.json',orient='records')$ m['created_date'] = m['created_time'].dt.strftime('%Y-%m-%d')$
cats = shelter_cleaned_df.loc[shelter_cleaned_df['AnimalType'] == 'Cat']$ cats.describe()$
fire_size_file = '../data/model_data/size_mod.sav'$ pickle.dump(rf, open(fire_size_file, 'wb'))
df_columns['Day of the week'].value_counts().plot(kind='barh')$
festivals.head(5)
ip_orig = pd.read_csv('image-predictions.tsv', sep = '\t')$ ip_orig.head()
station_distance.loc[:, ['Start Station Name', 'End Station Name',$                          'Start Coordinates', 'End Coordinates']].head(2)
summary = records.describe(include='all')$ summary$
data.head()
mgxs_lib.dump_to_file(filename='mgxs', directory='mgxs')
test_case = contest_cr.where(F.col('party_id')== test[0][0])$ test_case.take(1)$
data=daily_transaction.toPandas()$ data.head(10)
print stations.installation_date.min()$ print stations.installation_date.max()
cityID =  '4b25aded08900fd8'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Reno.append(tweet) 
type(df_vow['Date'].loc[0])
df2[df2.user_id == 773192]
df['user_id'].nunique()
print(convert_old, convert_new, n_old, n_new)$ z_score, pval = sm.stats.proportions_ztest([convert_new, convert_old],[n_new, n_old], alternative='larger')$ print('Zscore: {:.3f}\np-val: {:.4f}'.format(z_score, pval))
pandas_df = pd.read_csv("./SIGHTINGS.csv")$ pandas_df.head()
unique_desc_loc=class_merged_hol['description_local_hol'].unique()$ print (unique_desc_loc)
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid$ x = df30458.groupby('end_station_name').count()$ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
folder_name = 'wrangling_project_files'$ if not os.path.exists(folder_name):$     os.makedirs(folder_name)
turnstiles_df["DATE_TIME"] = pd.to_datetime(turnstiles_df.DATE + " " + turnstiles_df.TIME, format="%m/%d/%Y %H:%M:%S")
df.head().to_csv('My_output',index=False)
max_sharpe_port = results_frame.iloc[results_frame['Sharpe'].idxmax()]$ min_vol_port = results_frame.iloc[results_frame['SD'].idxmin()]
tips.sort_values(["sex", "day"]).set_index(["sex", "day"]).head(12)$
year_prcp_df = pd.DataFrame(year_prcp, columns=['date', 'precipitation'])$ year_prcp_df.head()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
fin_r_monthly = fin_r.resample('M').asfreq()
siteNo = '02087500' #Neuse R. Near Clayton $ pcode = '00060'     #Discharge (cfs)$ scode = '00003'     #Daily mean
np.exp(0.0150)
segs = ga3.management().segments().list().execute()$ df = pd.DataFrame([x for x in segs['items']])$ df.sort_values("created", ascending=False).head(5)
sns.factorplot(x='call_type',y='length_in_sec',col='call_day',data=calls_df,kind='bar')
weather_features.iloc[:6]
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'$ browser.visit(url)
empDf.createOrReplaceTempView("employees")$ SpSession.sql("select * from employees where salary > 4000").show()
df = hpg[hpg["air_store_id"].notnull() == True]$ df.head(100000)$
ss = StandardScaler()$ X_train = ss.fit_transform(X_train)$ X_test = ss.transform(X_test)
prob_contr =df2.query("group=='control'").converted.mean()$ print('The probality of an individual converting in the control group converting is {}'.format(prob_contr))
log_mod = sm.Logit(df_combined['converted'], df_combined[['intercept', 'country_CA', 'country_UK']])$ results = log_mod.fit()$ results.summary()
articles['content_short'] = articles['tokens'].map(lambda s: ' '.join(s))
df.to_csv('twitter_archive_master.csv', index=False)
qs = qs.merge(answers, how="left", left_on="Id", right_on="ParentId", suffixes=("", "_a"))$ print qs.head()
unsorted_Yahoo_data = pd.read_csv(Yahoo_data_file) $ unsorted_Yahoo_data.head()
gmap = gmplot.GoogleMapPlotter(reviews12_lat[0], reviews12_lng[0], 14)$ gmap.scatter(reviews12_lat, reviews12_lng, '#3F3067', size=20, marker=False)$ gmap.draw(r'mymap.html')
req = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=HL_EJeRkuQ-GFYyb_sVd&start_date=2017-01-01&end_date=2017-12-31')
old_page_converted=np.random.binomial(n_old,p_old)
total_fare=pd.DataFrame(city_fare)$ totol_fare=total_fare.reset_index()$ totol_fare$
results = []$ for pr_item in pr_list:$     results.append(download_and_parse_pr(pr_item))
S.decision_obj.simulStart.value = "2006-07-01 00:00"$ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
p_value = (null_vals > obs_mean).mean()$ p_value
df_ks.groupby('id').size().nlargest(10)
for index in df_wrong_rating.index:$     df_enhanced['rating_10_scaled'][index] =  df_wrong_rating['rating_10_scaled'][index]
metadata_df_all['type'] = [t.lower() for t in metadata_df_all['type']]$ metadata_df_all['bill_id'] = metadata_df_all['type'] + metadata_df_all['number'].map(int).map(str)$ metadata_df_all = metadata_df_all[['accessids', 'bill_id']]
gDateEnergy = itemTable.groupby([pd.Grouper(freq='1W', key='Date'),'Energy'], as_index=True)$ gDateEnergy_content = gDateEnergy['Content']$
p_old = df2.converted.mean()$ p_old
cityID = '55b4f9e5c516e0b6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Orlando.append(tweet) 
df=pd.read_csv('ab_data.csv')$ df.head(10)
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period(period="quarter")$ print("Trend for quarter: ", get_trend(get_timeseries(closed_pr)))
from datetime import datetime$ xml_in['days_diff_to_publication'] = (datetime.now() - xml_in['publicationDate']).astype('timedelta64[D]')
data['affair'].value_counts()
IP = '137.110.137.158'
p_old = df2['converted'].mean()$ p_old
Stations = Base.classes.Stations$ Measurements = Base.classes.Measurements
df2_no_outliers = df2.copy()$     $ df2_no_outliers['y'] = np.log(df2_no_outliers['y'])
df.isnull().sum()[df.isnull().sum() > 0]
primitives[primitives['type'] == 'transform'].head(10)
lgreg = LogisticRegression()$ lgreg.fit(train_data, train_labels)
AFX_X_2017 = r.json()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=As4uh8SmqPWoJ1s9XXDT&start_date=2017-01-01&end_date=2017-01-01"$ r = requests.get(url)
df.ID.dtype
logistic_mod_time = sm.Logit(df3['converted'], df3[['intercept', 'wk2','wk3']])$ results_time = logistic_mod_time.fit()$ results_time.summary()
our_nb_classifier.last_probability
df3[df3['STATION'] == '103 ST'].groupby(['DATE']).sum().plot(figsize=(10,3))
store_items = store_items.append(new_store)$ store_items
df2['converted'].mean()
tt_final.drop(['id'], axis = 1, inplace= True)$
df_train.describe()
control_convert = df2[df2['group']== 'control'].converted.mean()$ print(control_convert)
todrop1 = df.loc[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')].index
day = datetime.now()$
last_12_precip_df.describe()
gDate_content_count = gDateProject_content.groupby(level=0)$ gDate_content_count = gDate_content_count.sum()$
yc_new2.dropna()$ yc_new2.isnull().sum()
df.to_json("json_data_format_values.json", orient="values")$ !cat json_data_format_values.json
print('No. of rows: ',len(df))
df_master[df_master.retweet_count == [df_master['retweet_count'].max()]]
df_raw = pd.read_csv("./datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv")$ df = df_raw.copy()$ print(df_raw.head())
cleaned = facilities.drop_duplicates(subset=['facname','address','idagency'])$ cleaned.info()
total_fare=merge_table_citytype["fare"].sum()$ total_fare.head()
p_diffs = np.array(p_diffs)$ p_diffs
tweets = pd.read_csv("tweets.csv")
contrast = df[(df['up_votes'] > df['up_votes'].quantile(q=.85)) | (df['up_votes'] == 0)].reset_index(drop=True)$ contrast['viral'] = (contrast['up_votes'] != 0).astype(int)$ contrast.shape
df = pd.read_pickle('C:/Users/Stacey/Downloads/NY_complaint_data_for_model.pkl')
new_eng = pd.merge(df_user_engagement,df_eng_mult_visits, on="user_id")$
sb.pairplot(df_new[['converted','group']])
discounts_table.groupby(['Discount Band'], sort=False)['Discount %'].min()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=xhB-Ae2VRyMYmv2CRGtV")
bikedataframe = pd.concat([dfbikes, snow_weather, df_wu, dfsunrise], axis=1)$ bikedataframe.head()
import sys$ sys.executable$
mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % ('lead-reader', 'lead-reader', 'ds025991-a0.mlab.com', '25991', 'tra-ingestion')$ con = MongoClient(mongo_uri)$ db = con['tra-ingestion']
projects.actual_hours.sum()
dates = [datetime(2014,8,1),datetime(2014,8,2)]$ ts = pd.Series(np.random.randn(2),dates)$ ts
adj_close_acq_date['Date Delta'] = adj_close_acq_date['Date'] - adj_close_acq_date['Acquisition Date']$ adj_close_acq_date['Date Delta'] = adj_close_acq_date[['Date Delta']].apply(pd.to_numeric)  $ adj_close_acq_date.head()
df_ad_airings_5['location'].unique()
last_12_precip_df = pd.DataFrame(last_12_precip, columns=['date', 'precipitation'])$ last_12_precip_df = last_12_precip_df.set_index('date')$ last_12_precip_df.head()
pd.concat([msftA01.head(3),msftA02.head(3)])
dates = pd.date_range('2016-04-01', '2016-04-06')$ dates
deaths_Sl1 =dropped_Sl.loc['death_suspected']$ deaths_Sl2 =dropped_Sl.loc['death_probable']$ deaths_Sl3 =dropped_Sl.loc['death_confirmed']$
lmscore.resid.head(4)
Final_question_df = pd.read_csv('/Users/Gian/GitHub/Giancarlo-Programming-for-Analytics/Assignments/financial_table.csv')
s = fixed.iloc[200000:205000,]$ plt.plot(s.unstack(level=0), alpha=0.2);
b = 2. * np.random.randn(*a.shape) + 1.$ b.shape
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Doctors', doc_duration, window=52)
print(trump.favorite_count.sum())$ print(" ")$ print(trump.retweet_count.sum())
old_page_converted = np.random.choice([0,1],N_old, p=(p_old,1-p_old))$ old_page_converted
tips_analysisDF.head()
for v in data.values():$     if 'Q3' not in v['answers']:$         v['answers']['Q3'] = ['Other']
exiftool -csv -createdate -modifydate cisnwf6/Cisnwf6_cycle3.MP4 > cisnwf6.csv
transactions['membership_expire_date']  = transactions.membership_expire_date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))$ transactions['transaction_date']  = transactions.transaction_date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))$
city_data_df = pd.read_csv(city_data)$ city_data_df.head()
df = pd.get_dummies(df, columns=["component", "product"], prefix=["component", "product"])$ df.head(2)
all_df.isnull().any()$
plt.rcParams['figure.figsize'] = [18.0, 10.0]
df2['intercept']=1$ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])$ df2.head()
int_ab_page = 1/np.exp(-0.0149)$ int_ab_page
create_table = create_table.drop_duplicates()$ print create_table.shape
pres_df['split_location_tmp'] = pres_df['location'].map(lambda x: x.split(','))$ pres_df.head(10)
d = pd.read_csv('twitter_archive_master.csv')
df2.head()
now_start = datetime.datetime.now()$ time_start = now_start.strftime("%Y-%m-%d (yyyy-mm-dd); %H:%M hrs.")$ print "# Starting time of computing: %s"%time_start
len(df2.user_id.unique())
lr.score(preproc_training_test, training_test_labels)
merged_df = df_ad_state_metro_2.join(df_state_victory_margins)
plot(Combineddata.Ad_Cost, Combineddata.Sales_in_CAD, Combineddata.New_or_Returning, 'plot.pdf')
ftr_imp_rf=zip(features,trained_model_RF.feature_importances_)$ for values in ftr_imp_rf:$     print(values)
autos["brand"].unique()
del merged_portfolio_sp_latest_YTD['Date']$ merged_portfolio_sp_latest_YTD.rename(columns={'Adj Close': 'Ticker Start Year Close'}, inplace=True)$ merged_portfolio_sp_latest_YTD.head()
t_len = pd.Series(data=data['len'].values, index=data['Date'])$ t_fav = pd.Series(data=data['Likes'].values, index=data['Date'])$ t_ret = pd.Series(data=data['RTs'].values, index=data['Date'])
goals_df[['PKG', 'PKA']] = goals_df['PKG/A'].str.split('/', expand=True)$ goals_df.drop('PKG/A', axis=1, inplace=True)
%%bash$ cut -f 1,4-6 PPMI/logASYN/PHENO0_0.txt | sed 's/_/\t/g' | sed 's/(\//\t/g' | sed 's/)//g' | head
df.var()
df_pol_t = pd.concat([df_pol_matrix_df, df_pol_t.drop('title', axis=1)], axis=1)$
from kipoi_veff.parsers import KipoiVCFParser$ vcf_reader = KipoiVCFParser("example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf")$ print(list(vcf_reader.kipoi_parsed_colnames.values()))
cars = cars[(cars.yearOfRegistration > 1950) & (cars.yearOfRegistration <= 2017)]$ cars2 = cars2[(cars2.yearOfRegistration > 1950) & (cars2.yearOfRegistration <= 2017)]
average_chart_upper_control_limit = average_of_averages + 3 * d_three * average_range / \$                                     (d_two * math.sqrt(subgroup_size))
df2_countries[['UK','US']] = pd.get_dummies(df2_countries['country'])[['UK', 'US']]$ df2_countries.head()$
df_mas['p1'] = map(lambda x: x.upper(), df_mas['p1'])$ df_mas['p2'] = map(lambda x: x.upper(), df_mas['p2'])$ df_mas['p3'] = map(lambda x: x.upper(), df_mas['p3'])
df_enhanced['rating_10_scaled'].dtypes
for tweet in results:$    print (tweet.text)
dictionary = corpora.Dictionary(text_list)$ dictionary.save('dictionary.dict') $ print dictionary
graf_train, graf_test=train_test_split(graf, test_size=.33, random_state=42)
small_movies_file = os.path.join(dataset, 'ml-latest-small', 'movies.csv')$ small_movies_raw_data, small_movies_raw_data_header = read_file(small_movies_file)$
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)$ training_loss = model_with_loss.get_latest_training_loss()$ print(training_loss)
df = pd.merge(train,user_logs, on = 'msno', how = 'left')   $ del user_logs$ gc.collect()
df2['intercept'] = 1$ df2[['ab_page','ab_page2']] = pd.get_dummies(df2['landing_page'])$
import pandas as pd$ date = pd.to_datetime("4th of July, 2015")$ date
p_old = df2['converted'].mean()$ p_old
%matplotlib inline$ import matplotlib.pyplot as plt
print('N recalls:', len(recalls))$ print('Percentage: {0:.2f}%'.format((len(recalls) / len(df)) * 100))$ recalls.groupby('ACTION REQUESTED').size()
df2.country.unique()
df_ad_airings_4 = pd.read_pickle('./TV_AD_AIRINGS_FILTER_DATASET_2.pkl')
S_distributedTopmodel.decision_obj.hc_profile.options, S_distributedTopmodel.decision_obj.hc_profile.value
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep=',', encoding='utf-8', header=True)
train['Age'] = train.apply(lambda row: int(row[6].split("-")[0]) - int(row[1].split("-")[0]), axis = 1)$ test['Age'] = test.apply(lambda row: int(row[6].split("-")[0]) - int(row[1].split("-")[0]), axis = 1)
dfEtiquetas["latitude"] = dfEtiquetas["place"].apply(lambda p: p["location"]["latitude"])$ dfEtiquetas["longitude"] = dfEtiquetas["place"].apply(lambda p: p["location"]["longitude"])
dates = pd.date_range('8/1/2017', periods=100, freq='W-MON')$ dates
c.loc[c>0.7]
df['tax_type'] = df['tax_type'].str.replace(u'\xa0', u' ')$
p_new = df2['converted'].mean()$ print(p_new)
df_countries.info()
autos['odometer'] = autos['odometer'].str.replace('km','').str.replace(',','').astype(int)
df_precep_dates_12mo.set_index('date')
df_rs = master_df_total.sample(frac = 0.085, random_state = 1)
new_messages = new_messages[new_messages.user_id == 42966]
rf=RandomForestRegressor()$ rf.fit(X_train,Z_train)$ rf.score(X_test,Z_test)
! unzip -p {path_to_zips}On_Time_On_Time_Performance_2015_1.zip | head -n 2
Gun = G.to_undirected()$ politiciansPartyDict = nx.get_node_attributes(Gun, "party")$ partitions = community.best_partition(Gun, weight='weight')
lq.columns = lq.columns.str.replace('(','')$ lq.columns = lq.columns.str.replace(')','')$ lq.columns.values
seqsize = subs.user_id.value_counts(normalize=False)[0]$ print ("the longuest sequence is %d accepted submissions long" % seqsize )
df_enhanced['rating_10_scaled'] = df_enhanced['rating_10_scaled'].astype('float')
df = pd.read_csv('basic_graphics_single_column_data.csv')
%%time $ nlp = spacy.load('en_core_web_lg') 
import json$ list_of_issues_dict_data = [json.loads(line) for line in open('SPM587SP18issues.json')]
mismatch_grp1 = df.query("group == 'control' and landing_page == 'new_page'")$ mismatch_grp2 = df.query("group == 'treatment' and landing_page == 'old_page'")$ print("no. of times the new_page and treatment don't line up = {}".format(len(mismatch_grp1) + len(mismatch_grp2)))
td = td.fillna(0)
mean = np.mean(data['len'])$ print("The average length in tweets: {}".format(mean))
data_donald_replies = pd.read_csv("trump_replies.csv", dtype={'reply_id':str})$ data_donald_replies.head()
store_items.dropna(axis = 0)
shows['release_weekday'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%w'))$ shows['release_weekday'] = shows['release_weekday'].dropna().apply(lambda x: str(x))$ shows['release_weekday'] = shows['release_weekday'].dropna().apply(lambda x: int(x))
control_conv = df2[df2['group'] == 'control'].converted.mean()$ df_grp = df.groupby('group')$ df_grp.describe()$
data_l2_begin = tmpdf.index[tmpdf[tmpdf.isin(DATA_L2_HDR_KEYS)].notnull().any(axis=1)].tolist()$ data_l2_begin
print(rmse_scores.mean())
print(df.tail()) 
df_c_merge[['CA', 'UK', 'US']] = pd.get_dummies(df_c_merge['country'])$ df_c_merge.head()
X_copy['score_edi_instability_avg'] = X_copy['score_edi_instability_avg'].apply(lambda x: float(x))
n_new=df2.query('landing_page=="new_page"').count()[0]$ n_new$
data.head()
mars_df = pd.read_html(mars_facts_url)$ mars_facts_df = pd.DataFrame(mars_df[0])
data_df.groupby('topic')['ticket_id'].nunique()
with open(content_analysis_save, mode='w', encoding='utf-8') as f:$     f.write(wikiContentRequest.text)
for c in ccc:$     vwg[c] = vwg[vwg.columns[vwg.columns.str.contains(c)==True]].sum(axis=1)
alpha+ beta_race.tag.test_value.reshape((-1,1)).T.dot(X[:,1:6].T)+ beta_income.tag.test_value.reshape((-1,1)).T.dot(X[:,6:7].T) +beta_complaint.tag.test_value.reshape((-1,1)).T.dot(X[:,7:8].T) +beta_pop.tag.test_value.reshape((-1,1)).T.dot(X[:,8:9].T)$
df = df.drop_duplicates()$ df.loc[df['Name'] == 'Mary']$ df = df.sample(frac=1).reset_index(drop=True)
Imagenes_data.info()
df['intercept']=1$ df['ab_page'] = pd.get_dummies(df['group']) ['treatment']$
sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[(sprintsWithStoriesAndEpics_df['Sprint Age In Days'] > 3) | (sprintsWithStoriesAndEpics_df['Age In Days'] > 3)]$ sprintsWithStoriesAndEpics_df[['key_story', 'Team_story', 'fixVersions_story', 'summary_story', 'status_story', 'Age In Days', 'Sprint Age In Days', 'Open Set To Date']].sort_values(by=['Age In Days', 'Sprint Age In Days'], ascending = False)
df2= df2.join(pd.get_dummies(df2['landing_page']))
df_first_days['active_user'].value_counts()
number = 100$ factor = 1.1$ text =  "hello world"
print(df,'\n')$ print(df.apply(lambda x: x.max()-x.min()))
grouped=dup_ts.groupby(level=0)$ grouped.mean()
temps_df.Missoula
tweets_predictions_all = tweets_predictions_all[tweets_predictions_all['jpg_url'].notnull()]
df2['converted'].mean()
z_score, p_val = z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new], nobs=[n_old, n_new], alternative='smaller')$ print(z_score, p_val)
df3 = df.copy()$ df3 = df3.drop('ENTRIES',1)$ df3 = df3.drop('EXITS',1)$
row_idx = (slice(None), slice(None), slice('Bob', 'Guido'))  # all years, all visits, Bob + Guido$ health_data_row.loc[row_idx, 'HR']
msftAC = msft['Adj. Close']$ msftAC['2012-01-03']
%time tsvd = tsvd.fit(train_4)
dataframe.groupby('dday_of_week').daily_worker_count.mean()
data.values
tweets = pd.read_csv('df_tweets.csv', parse_dates=['created_at'], infer_datetime_format= True, low_memory=False,\$                     usecols=['id', 'created_at', 'hashtags'])
df.user_id.nunique()
ind = np.arange(len(feature_importances))$ plt.bar(ind, feature_importances, .35)
high_polarity = df[df['polarity']>0.3]$ mid_polarity = df[(df['polarity']<0.3) & (df['polarity']>-0.3)]$ low_polarity = df[df['polarity']<-0.3]
'Similarly, militant groups in the Niger Delta attacked Nigeria oil facilities to the point that production fell to roughly half of its former levels at times.'$ 'But Nigeria still expects that its exemption will continue for at least six months, according to oil minister Ibe Kachikwu.'$ 'Libya also has plans to raise output to 1.32 million bpd by the end of the year, up from an earlier target of 1.1 million bpd.')
date.strftime('%A')
df['Size'].sum()/(1024*1024*1024)
df = pd.DataFrame(recentd, columns=['prcp'])$ df.head(20)
month.head(5)
workclass = pd.read_sql(q, connection)$ workclass.head(10)
data=data.loc[~data[nan_cols(data)].isnull().any(axis=1),:]$ data.shape #(1268728, 23) records left
df2['intercept'] = 1$ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])
Not_same = df_3['user_id'].count() + df_43['user_id'].count() $ Not_same$
active_df.sort_values(by="station_count", ascending=False, inplace=True)
alice_sel_shopping_cart = pd.DataFrame(items, index=['glasses', 'bike'], columns=['Alice'])$ alice_sel_shopping_cart
import warnings$ warnings.simplefilter("ignore", UserWarning)$ warnings.simplefilter("ignore", FutureWarning)
control_group = df2.query('group == "control"')$ print(control_group.shape[0])
P_new = ab_df2['converted'].mean()$ print(P_new)
site_visits_rebuild.MarketingChannel.unique()
news_title = soup.find_all("div", class_="content_title")[0].text$ print(news_title)
df2 = pd.read_csv('ab_edited.csv', sep=',')
(null_vals > obs_diff).mean()
search['days_plan_ahead'] = (search['trip_start_date'] - search['timestamp']).dt.days+1
for table in table_list: display(DataFrameSummary(table).summary())
store_items.fillna(method = 'backfill', axis = 0)
contractor = pd.read_csv('contractor.csv')$ state_lookup = pd.read_csv('state_lookup.csv')
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])
ripple_market_info.drop(['Date'],inplace=True,axis=1)$ scaler_rip = MinMaxScaler(feature_range=(0, 1))$ scaled_rip = scaler_rip.fit_transform(ripple_market_info)$
df_pos_emo_end = df_pos_emo_end[df_pos_emo_end.last_word_category.str.len() == 1]$ df_pos_emo_end.last_word_category.value_counts().head(20)
url = "https://mars.nasa.gov/news/"$ response = requests.get(url)
y = x.loc[:,"A"]$ y
dfEtiquetas.dropna(subset=["created_time"], inplace=True)
new_model = gensim.models.Word2Vec(min_count=1)  $ new_model.build_vocab(sentences)                     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)
if not np.any(cust['churn'] == 1):$     labels['churn'] = 0$     labels['days_to_next_churn'] = np.nan
print('The proportion of users converted:',df['converted'].mean())
data = requests.get ("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key="+API_KEY ).json()
print(s['2015-01-01':'2015-04-30'].idxmax()) #Printing the max value from January, 2015 to April, 2015$ print(s['2015-05-01':'2015-08-31'].idxmax()) #Printing the max value from May, 2015 to August, 2015$ print(s['2015-09-01':'2015-12-31'].idxmax()) #Printing the max value from September, 2015 to December, 2015
df_con_treat = df_con1.query('group =="treatment"')$ x_treat = df_con_treat["user_id"].count()$ x_treat$
error = train_ALS(training_RDD, test_RDD, best_rank, seed, iterations, lambda_=0.01)$ print('For testing data the RMSE is %s' + str(error))
session.query(func.count(station.id)).scalar()
print(train_df[train_df.author.isnull()].shape[0])$ print(train_df.shape[0])
twitter_Archive['rating']=twitter_Archive.rating_numerator/twitter_Archive.rating_denominator$ twitter_Archive=twitter_Archive.drop(['rating_numerator','rating_denominator'],axis=1)
pd.read_csv("../../data/msft.csv",nrows=3)
x = x.assign(A2 = x["A"]**2)$ x
df['date'] = df['raw'].str.extract('(....-..-..)', expand=True)$ df['date']
json_data = r.json()$ print(type(json_data))
store_items.dropna(axis=1) # or store_items.dropna(axis=1, inplace=True)
df1.sort_values(by=['date'], ascending=True, inplace=True)
auth = tweepy.OAuthHandler(consumer_key=cKey, consumer_secret=cSecret)$ api = tweepy.API(auth)
response = requests.get('http://www.reddit.com/r/goodnews')$ page_source = response.text$ print(page_source[:1000])
bbc_df = constructDF("@BBC")$ display(constructDF("@BBC").head())
series1['1975-12-31':'1980-12-31'].plot()$ plt.show()
total_students_with_passing_math_score = len(df_students.loc[df_students['math_score'] > 69])$ total_students_with_passing_math_score
fed_reg_data = r'data/fed_reg_data.pickle'$ final_df.to_pickle(fed_reg_data)
nitrodata['Month'].value_counts().sort_index()
pipe.fit(train, labelsTrain)
if create_sql_indexes:$     conn.execute('create index left_ix on left(key, key2)')$     conn.execute('create index right_ix on right(key, key2)')$
columns = inspector.get_columns('measurements')$ for c in columns:$     print(c['name'], c["type"])$
voters = pd.read_csv('data_raw_NOGIT/voters_district3.txt', sep='\t')$ households = pd.read_csv('data_raw_NOGIT/households_district3.txt', sep='\t')$ households_with_count = pd.read_csv('data_raw_NOGIT/AK_hhld_withVoterCounts.txt', sep='\t')
class_merged.isnull().sum()
probability = df2["converted"].mean()$ print("The probability is {}.".format(probability))
cohorts_size = cohorts['TotalUsers'].groupby(level = 'CohortGroup').first()$ cohorts_size.head()
url_c = "https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv"$ c = pd.read_csv(url_c)$ c
df.isnull().any().any(), df.shape
columns = inspector.get_columns('clean_hawaii_measurements.csv')$ for c in columns:$     print(c['name'], c['type'])$
filtered_store_stuff = joined_store_stuff.filter("store_level > 2").sort( joined_store_stuff['count'].desc() )$ filtered_store_stuff.show()
len_1 = len(df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')])$ len_2 = len(df[(df['group'] != 'treatment') & (df['landing_page'] == 'new_page')])$ print('The number of times "new_page" and "treatment" do not line up is {}.'.format(len_1+len_2))
d.weekday()
products.head(2) 
digits.target[5]$ clf.predict(digits.data[5:6])
print(type(df_final['created_time'][0]))$ pd.__version__
baseball.corr()
with open('key_phrases.pickle', 'rb') as f:$     key_phrases = pickle.load(f)
test[['clean_text','user_id','predict']][test['user_id']==5563089830][test['predict']==12].shape[0]
train_Features, test_Features, train_species, test_species = train_test_split(Features, species, train_size=0.5, random_state=0)
row_num = df.shape[0]$ print("Number of rows is: {}".format(row_num))
plot_price(f,'Close',start='Jan 01, 2017',end='Dec 31, 2017')$ plt.legend("last year")
regr2.score(X2, y)  # when we fit all of the data points
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd[:10]
join_d = join_c.groupby("party_id_orig").agg(F.max("aggregated_prediction"))$ join_d = join_d.withColumn("highest_match", F.lit(1))$
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))$
twitter_archive_clean = twitter_archive.copy()$ image_predictions_clean = image_predictions.copy()$ tweet_json_clean = tweet_json[['id','retweet_count','favorite_count']].copy()
country_dummies = pd.get_dummies(df_new['country'])$ df_new = df_new.join(country_dummies)$ df_new.head(5)
df.info()
data.show()$ data.printSchema()$
ds_info = ingest.upload_dataset(database=db,$                                 dataset=test,$                                 validation_map={"bools": lambda x: isinstance(x, str)})
df_tweet.duplicated().sum()
con = cx_Oracle.connect(connection_string)
coll.find_one({'timestamp': {'$gte': datetime.datetime(2016, 9, 5, 0,0,0)}})
approved.loan_amnt.describe(), rejected.loan_amnt.describe()
df.boxplot('MeanFlow_cms');
median_trading_volume = statistics.median([day[6] for day in data])$ print ('Median trading volume for 2017:', median_trading_volume)
df_writer_combined_text['combined_text_tokenized'] = df_writer_combined_text['combined_text'].apply(lambda x: nlp(x))$
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative="smaller")$ z_score, p_value
total_trt = (df2['group']=='treatment').sum()$ prob_trt = total_trt/unique_users$ df2[(df2['group']=='treatment') & (df2['converted']== 1)].sum() 
df1 = pd.read_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext_all.csv", encoding='ANSI', $                  index_col='tweet_id', $                  sep=',')$
tweet_df_raw.head(5)
sentiments_df = pd.DataFrame(sentiment_array)
np.datetime64('2015-07-04')
pd.DataFrame(dummy_var["_Source"][Company_Name]['Open']['Forecast'])[:12]$
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys() # Retrieve the names of the tables in the database
df.index
dict2 = {k:g['DATETIMEENTRIESVAL'].tolist() for k, g in df.groupby('TUPLEKEY')}$ dict2[('A002','R051','02-00-00','LEXINGTON AVE')]
cityID = '73d1c1c11b675932'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chesapeake.append(tweet) 
pivoted.T[labels == 0].T.plot(legend=False, alpha=0.1)
duplicate_row = df2.loc[df2.duplicated('user_id') == True]$ duplicate_row
hit_filter_df.to_csv("Desktop/Project-2/numberOneUnique.csv", index=False, header=True)
autos = autos.drop(["nr_of_pictures", "seller", "offer_type"], axis=1)
query = 'CREATE TABLE new_york_new_york_points_int_ct10 AS (SELECT geoid, osm_id, latitude, longitude FROM beh_nyc_walkability, new_york_new_york_points WHERE ST_INTERSECTS(beh_nyc_walkability.the_geom, new_york_new_york_points.the_geom))'$
!hdfs dfs -cat {HDFS_DIR}/hw3.1-output/* > complaintCounts.tsv$ !cat complaintCounts.tsv
cityID = 'b71fac2ee9792cbe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Sacramento.append(tweet) 
sum_ratio = absorption_to_total + scattering_to_total$ sum_ratio.get_pandas_dataframe()
tweets_df = tweets_df.sort_values("Date", ascending=False)$ tweets_df
df = df[(df['Age_Years'] <= 30) & (df['Age_Years'] >= 20)]$ df.head()
perf_train.info()
print(np.exp(results.params))
lr2 = LogisticRegression(solver='saga', multi_class='multinomial', random_state=20, max_iter=1000)$ lr2.fit(X, y)
weather_df["weather_main"] = weather_df.weather_main.str.lower()$ weather_df["weather_description"] = weather_df.weather_description.str.lower()
stemmer = nltk.stem.porter.PorterStemmer()$ %timeit articles['tokens'] = articles['tokens'].map(lambda s: [stemmer.stem(w) for w in s])
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, wait_on_rate_limit = True)
tweet_data.info()
from sklearn.linear_model import LogisticRegression$ from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score$ from sklearn.model_selection import train_test_split
tmp = 'is ;) :) seven.<br /><br />Title (Brazil): Not Available'$ print(preprocessor(tmp))$
sum(df2['user_id'].duplicated())$ df2[df2.duplicated(['user_id'], keep=False)]['user_id']$ df2[df2.duplicated('user_id')]
qt_convrates_toClosedWon.applymap(lambda x: "{0:.2f}%".format(x * 100.00))$
gene_df.sort_values('length').head()
train_df["num_description"]= train_df["description"].apply(num_description)$
conv_cont_prob = df2[df2['group']=='control']['converted'].mean() * 100$ output = round(conv_cont_prob, 2)$ print("The probability of the control group individual converting regardless of the page they receive is: {}%".format(output))
test_scores = run(q_agent_new, env, num_episodes=100, mode='test')$ print("[TEST] Completed {} episodes with avg. score = {}".format(len(test_scores), np.mean(test_scores)))$ _ = plot_scores(test_scores)
def quadratic(x, **kwargs):$     return np.hstack([np.ones((x.shape[0], 1)), x, x**2])
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old,(1-p_old)])$ old_page_converted.mean()
df2.plot()$ plt.show()
Latest_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ start_date = pd.to_datetime(Latest_date[0]).date()- timedelta(days=365)$ print(start_date)
bucket.upload_dir('data/city-util/proc', 'city-util/proc', clear_dest_dir=True)
store_items.interpolate(method = 'linear', axis = 0)
%matplotlib notebook$ plt.style.use('seaborn-paper')$ mpl.rcParams['figure.facecolor'] = (0.8, 0.8, 0.8, 1)
df3['tweet'] = df3['full_text'].where(pd.notna(df3['full_text']), other = df3['text'])
vocab = vect.get_feature_names()
liquor['Sale (Dollars)'] = [s.replace("$","") for s in liquor['Sale (Dollars)']]$ liquor['Sale (Dollars)'] = [float(x) for x in liquor['Sale (Dollars)']]$ liquor_state_dollars = liquor['Sale (Dollars)']
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])
df2[['CA', 'UK','US']]=pd.get_dummies(df2['country'])$ df2.head(1)
c = b[b == 7].index.get_level_values('user_id').tolist()$
cityID = '161d2f18e3a0445a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Raleigh.append(tweet) 
df_final = df_grouped.merge(df_schoo11, on='school_name', how='left')$ df_final.drop(['budget_x', 'size_x', 'School ID','size_y', 'budget_y'], axis = 1, inplace=True)$ df_final
honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['Time stamp'].str.split("mhn").tolist(), columns = ['time_stamp1','time_stamp2','time_stamp3'])],axis = 1)$ honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['time_stamp3'].str.split("T").tolist(), columns = ['date','time'])], axis = 1)
data2['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data2['Tweets'] ])$ display(data2.head(10))
test = tmax_day_2018.sel(lat=16.75, lon=81.25).to_dataframe()
lassoreg = Lasso(alpha=0.01, normalize=True)$ lassoreg.fit(X_train, y_train)$ print(lassoreg.coef_)
len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
state_DataFrames_list = []$ for item in state_keys_list:$     state_DataFrames_list.append(state_DataFrames[item])
df = pd.read_csv('ab_data.csv')$ df.head()
df.loc[df.userTimezone == 'Mountain Time (US & Canada)', 'tweetText']$
closing_prices = (x[4] for x in data_table if x[4] is not None)$ inter_day_changes = (abs(i - j) for i, j in itertools.combinations(closing_prices, 2))$ print('Largest Change between days: {:.2f}'.format(max(inter_day_changes)))
nba_df[nba_df.isnull().any(axis=1)].loc[:, ["Season", "Team", "G", "Opp", "Home.Attendance", "Referee3"]]
df = pd.read_csv('cleaned_and_merged.csv').drop('Unnamed: 0', axis = 1)$ df.head()
autos['price'].describe()
! ./data_tools/download.sh$ ! ./data_tools/split.sh
    return "https://github.com/{0}/{1}.git".format(org, project)$ 
import nltk; nltk.download('stopwords')
bucket.upload_dir('data/city-util/raw', 'city-util/raw', clear_dest_dir=True)
tweets ['apple'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'apple' in x else False)$ tweets ['samsung'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'samsung' in x else False)
Base = automap_base()$ Base.prepare(engine, reflect=True)
city_pd.replace('',np.nan,inplace=True)$ city_pd.dropna(axis=0,how='any')
all_sets.loc[:, ["name", "releaseDate", "setSize"]].sort_values(["releaseDate"]).tail()
%matplotlib inline$ plt.hist(threeoneone_geo['fix_time_sec']/60,bins=100)$ plt.show()$
soup.get_text()
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt"$ df = pd.read_table(path, sep ='\s+', na_values=['.'])$ df.head(5)
options_frame['BidAskSpread'] = options_frame['Ask'] - options_frame['Bid']$ errors_20_largest_by_spread = options_frame.ix[sorted_errors_idx.index]$ errors_20_largest_by_spread[['BidAskSpread', 'ModelError']].sort_values(by='BidAskSpread').plot(kind='bar', x='BidAskSpread')
model_NB = MultinomialNB()$ model_NB.fit(count_vectorized, df_train.author)
df3 =df2.merge(countries, on='user_id')$ df3.head(3)$
sample.dtypes
compound_df = pd.DataFrame([avg_compound]).round(3)$ compound_df
dummy_df.multiply(logit_params).sum(axis=1).apply(lambda x : np.divide(np.exp(x),1+np.exp(x)))$
df2['intercept']=1$ df2['ab_page']=pd.get_dummies(df2['group'])['treatment']
pres_df['state'].tail()
df_f2.loc[df_f2["CustID"].isin([customer])]
df2_unique_users = len(df2['user_id'].unique().tolist())$ df2_unique_users
print("Probability of treatment group converting:", $       df2[df2['group']=='treatment']['converted'].mean())
df_c = pd.read_csv('countries.csv')
pd.MultiIndex.from_product([['a','b'], [1,2]])
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-01-02&api_key=" + API_KEY)
np.std(null_vals),np.std(p_diffs)
stations_df.count()
plt.hist(p_diffs)$ plt.title("sample distribution for the conversion rate")$ 
df = pd.merge(df,rsvp_df,how="left",on="urlname")$ df.head()
expectancy_for_least_country = le_data.min(axis=0)$ expectancy_for_least_country
airlines_sim = [d for d in airlines if len(d.tweets) == 2]$ len(airlines_sim)
reviews.rename_axis('wines',axis='rows')$
prcp_year_df.describe()
cursor1=db.bookings.find({"consignee.city":"Indore"},{'booking_id':1,'_id':0,'booking_items.is_cod':1})
readRenviron("~/.Renviron")
df2.groupby(df2['landing_page']=='new_page').size().reset_index()[0].iloc[1]/df2['landing_page'].count()
avgAge_df1['Avg_Career'] = avgAge_df1['Career_length'] / avgAge_df1['Count_career']$ avgAge_df1.head()
df.dropna()
twelve_months_prcp.head()
validation.analysis(observation_data, simple_resistance_simulation_1)
temp_df.head()
kick_data_state = kick_data[kick_data.state != 'suspended'][kick_data['launched_at'].dt.year >= 2014]
table_rows = driver.find_elements_by_tag_name("tbody")[14].find_elements_by_tag_name("tr")$
norm.ppf(1-(0.05/2))
shows['release_monthday'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%d'))$ shows['release_monthday'] = shows['release_monthday'].dropna().apply(lambda x: str(x))$ shows['release_monthday'] = shows['release_monthday'].dropna().apply(lambda x: int(x))
cityID = '1c69a67ad480e1b1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Houston.append(tweet) 
def class_size(Y):$     unique, counts = np.unique(Y, return_counts=True)$     return dict(zip(unique, counts))$
cgm = cgm.rename(columns={"value": "mmol_L"})$ cgm["mg_dL"] = (cgm["mmol_L"] * 18.01559).astype(int)$ cgm.mg_dL.head()
logit_mod_joined_result = logit_mod_joined.fit()$ logit_mod_joined_result.summary()
df3.groupby('created_at').count()['tweet'].plot()
df = pd.read_csv("ab_data.csv")$ df.head()
t1.info()
countries = pd.read_csv('countries.csv')$ countries.head()
pd.date_range('2015-07-03', '2015-07-10')
data = pd.read_json('UCSD_records.json', orient = 'columns')
int_and_tel.loc[int_and_tel['sentiment_magnitude'] >= 10]
df_tobs = pd.DataFrame(data)$ display(df_tobs.head())$ display(df_tobs.info())
df2 = df2.drop_duplicates(subset='user_id')
if not os.path.isdir('output'):$     os.makedirs('output')
import matplotlib.pyplot as plt$ plt.plot(y,model.predict(X),'.')$ plt.show()
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new['country'].astype(str).value_counts()
g_live_df = live_df.groupby(['blurb'])$ live_df_filtered = g_live_df.filter(lambda x: len(x) <= 1).sort_values(['blurb','launched_at'])
new_price = np.array([price[1:] for price in b_cal_q1['price']])$ print(new_price)$ print(len(new_price))
utils.serialize_data(data)
n_old = df2.query('group == "control"').shape[0]$ n_old
plt.tick_params(axis='x',width=2,colors='k')$ plt.tick_params(axis='y',width=2,colors='k')$
max_ch_ol2 = max(abs(u.close-v.close) for u,v in zip(list(o_data.values()),list(o_data.values())[1:]))$ print('Another one liner using islice: {:.2f}'.format(max_ch_ol2))
(train.shape, test.shape, y_train.shape)
temp_df['date'] = [dt.datetime.strptime(x, "%Y-%m-%d") for x in temp_df['date']]
print cust_data.groupby('SeriousDlqin2yrs').apply(analyze) $ print $ print cust_data.groupby('SeriousDlqin2yrs').apply(sum)
tweets_df = pd.DataFrame(tweets_all)$ tweets_df['retweet_ratio'] = (tweets_df['retweet_count']/tweets_df['followers_count'])*10000
d = feedparser.parse('http://rss.nytimes.com/services/xml/rss/nyt/InternationalHome.xml')
df2['intercept']=1$ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])$ df2.head()
np.exp(results.params)
brandValues.mapValues(lambda x: int(x[0])/int(x[1])). \$     collect()$
result.summary()
logit_new = sm.Logit(df_mod['converted'], df_mod[['intercept','US', 'UK','ab_page','UK_ab_page']])$ model_new = logit_new.fit()$ model_new.summary()
df2['timestamp']=pd.to_datetime(df['timestamp'],format='%Y-%m-%d %H:%M:%S')$ df2.dtypes
tickerdf = tickerdf.dropna(axis=0,how='any')
dfz=df_clean.dropna(axis=0, how='any')$
locationing = pd.read_csv('location.csv',names=['Date', 'Text'])$ locationing.Text.replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
df_tweet_json.head()
network_simulation[network_simulation.generations.isin([8])]$
df.shape[0]
df_u= df_vu.groupby(["landing_page","group"]).count()$ df_u$
merged_portfolio_sp_latest_YTD_sp = pd.merge(merged_portfolio_sp_latest_YTD, sp_500_adj_close_start$                                              , left_on='Start of Year', right_on='Date')$ merged_portfolio_sp_latest_YTD_sp.head()
concat_3 = pd.concat([df1, df2, df5], axis=1)$ concat_3
for name in invalid_names: $     twitter_archive_clean.loc[twitter_archive_clean.name == name ] = np.NaN
num_users = df.nunique()['user_id']$ print("The number of unique users in the dataset is : {}".format(num_users))
test_results = lrModel.evaluate(test_data)$ test_results.rootMeanSquaredError
print pd.merge(df0, df2, how='left')$
session.query(Station.name).count()
df_measurement.describe()
df3 = pd.read_csv("countries.csv")$ df3.head()
df_d.describe()
import seaborn as sns$ sns.set(rc={"figure.figsize": (12, 12)})$ sales_agg.groupby(['City'])['Sale (Dollars)'].sum().sort_values(ascending = False)[0:10].plot(kind='barh')
tweetsIn22Mar = tweetsIn22Mar.loc['2018-03-22']$ tweetsIn1Apr = tweetsIn1Apr.loc['2018-04-1']$ tweetsIn2Apr = tweetsIn2Apr.loc['2018-04-2']
telecom2 = telecom2.dropna(axis=0, subset=['onnet_mou_6', 'onnet_mou_7', 'onnet_mou_8'])$ print(telecom2.shape)$
tlen.plot(figsize=(16,4), color='b');
data.info()
manager.image_df[manager.image_df['filename'] == 'image_picea_sitchensis_in_winter_11.png']$
import pandas as pd$ with pd.option_context("max.rows", 10):$     print(dta.results.value_counts())
retail_data = raw_data.loc[pd.isnull(raw_data.accountid) == False]
r2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=%s' % API_KEY)
pst = ps.to_timestamp()$ pst.index
g_goodbad_not_index = sl_data.groupby(['goodbad','AGE_groups'], as_index=False).sum()$ g_goodbad_not_index
colmns=['category', 'launched_year', 'launched_quarter', 'goal_cat_perc', 'participants']$ ks_particpants.columns=colmns
df = pd.merge(ffr, vc, left_index=True, right_index=True, how="left")$ df_info(df)
funded_apps_1 = pd.read_csv("RISE_funded_20160722.csv")
n_new, n_old = df2['landing_page'].value_counts()
scores = [datum['score'] for datum in results['scores']]$ df['score'] = pd.Series(scores, index=df.index)$ df.head(10)
sns.jointplot(x = "positive_ratio", y = "negative_ratio", data = news_df)
CON = CON.sort_values(by = ['Contact_ID','OppName'])
np.all(x == 6)
df_clean['doggo'].value_counts(),df_clean['floofer'].value_counts(),df_clean['pupper'].value_counts(),df_clean['puppo'].value_counts()
slicer.apply(np.mean, axis=1)
columns = inspector.get_columns('measurements')$ for c in columns:$     print(c['name'], c["type"])
df.rename(columns={'85235_00060_00003':'MeanFlow_cfs','85235_00060_00003_cd':'Confidence'},inplace=True)$ df.head()
 (merged_visits.pipe(lambda df: df['Fail'].div(df['Pass']))$   .dropna()$   .sort_values(ascending=False))$
data = TwitterData_Initialize()$ data.initialize("data/train.txt", is_spain = False)$ data.processed_data.head(10)
errors.mae_vals.idxmin(errors.mae_vals.min())
df_countries.isnull().sum()
resource_id = hs.createHydroShareResource(title=title, content_files=files, keywords=keywords, abstract=abstract, resource_type='genericresource', public=False)
df2.drop([2893], inplace=True)
import gspread$ from oauth2client.service_account import ServiceAccountCredentials$ from operator import itemgetter
x_train, x_test, y_train, y_test = train_test_split(data_woe, df['y'], test_size=0.2, stratify=df['y'], random_state=17)$ print(str(datetime.datetime.now()) + ' train_test_shapes:', x_train.shape, x_test.shape)$
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'$ 
response=r.json()$ print(response)
groupby_time = df1['time'].groupby(df1['time']).count()$ groupby_time$
USvideos['like_ratio'] = USvideos['likes'] / (USvideos['likes'] + USvideos['dislikes']) $ USvideos['like_ratio'].describe()$
autos = autos[autos["year_of_registration"].between(1910,2016)]$ autos["year_of_registration"].value_counts(normalize=True).head(10)
df_tsv.head()
plt.scatter(X2[:, 0], X2[:, 1], c=labels, cmap='rainbow')$ plt.colorbar();
users.head(2) 
json_data = r.json()
free_data.head(5)
pop_result = pop_rec.recommend()
for screen_name in napturalistamo_followers_timelines_grouped.index:$     with open("../output/napturalistamo_followers/text/{0}_timeline.txt".format(screen_name), "w") as text_file:$         text_file.write(napturalistamo_followers_timelines_grouped[screen_name])
tweets = pd.read_csv('data/US-tweets.csv',encoding='utf-8',parse_dates=['dt'])$ tweets.twhandle = tweets.twhandle.str[1:]$
pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
Measurement_data = session.query(Measurement).first()$ Measurement_data.__dict__
import matplotlib.cm as cm$ dots_c, vhlines_c, *_ = cm.Paired.colors
df2.query($     'group=="treatment" & converted==1').user_id.count() / df2.query($     'group=="treatment"').user_id.count()
for row in selfharmm_topic_names_df.iloc[4]:$     print(row)
df2.groupby(['TUPLEKEY', 'DATE']).sum()
old_page_converted = np.random.choice([1, 0], size=len(df2_control.index), p=[df2.converted.mean(), (1-(df2.converted.mean()))])
hist.transpose()
guido_title = soup.title$ print(guido_title)
state_party_df = state_party_df.rolling(7).mean()
path = os.path.join("D:/Call_for_service/")$ path
ins['type'].value_counts()
(p_diffs > 0.000913).mean()
stops.to_csv('stops_all_ireland.csv', sep=';')
df_countries['country'].unique()
clf.predict(users_to_predict)$
a['SA'] = np.array([ analyze_sentiment(tweet) for tweet in a['Tweets'] ])$ b['SA'] = np.array([ analyze_sentiment(tweet) for tweet in b['Tweets'] ])
pca.fit(scaled_data)
d = datetime(2014,8,29)$ do = pd.DateOffset(days = 1)$ d + do
tweets_df.tail()
history_df = pd.read_sql('measurement',engine)$ history_df['station'].value_counts()
engine = create_engine("sqlite:///hawaii.sqlite")
plt.savefig(str(output_folder)+'NB01_6_NDVI_change_'+str(cyclone_name)+'_'+str(location_name))
session.query(Measurements.station, func.strftime("%m-%d", Measurements.date), func.avg(Measurements.prcp)) \$        .group_by(Measurements.station, func.strftime("%m-%d", Measurements.date)).limit(5).all()
c.loc["c":"e"]
norm.ppf(1-(0.05))
data[(data.value>1000) & (data.phylum.str.endswith('bacteria'))]
yhat = loansvm.predict(X_test)$ yhat [0:5]
file = "../Data/output/sampleDataWithSentiment.csv" $ allData = pd.read_csv(file,index_col=None, header=0, engine="c", encoding='utf-8', low_memory=False)
df1=data.rename(columns={'SA':'Polarity'})$ df1.head()
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']#['link_weight'].loc[97528]
np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)
base = wards.plot(column = 'geometry', figsize = (10,10), color = 'grey', edgecolor = 'black')$ delays_geo.plot(ax = base, marker='o', color='blue', markersize = 5)
destination=table.find(text='Destination').find_next('td').text$ destination
text_classifier.get_step_params_by_name("text1_char_ngrams")
coins_mcap_today = mcap_mat.iloc[-2]$ coins_mcap_today = coins_mcap_today.sort_values(ascending=False)
p_new = df2.converted.mean()$ p_new
guinea = concatenated.loc[concatenated['country'] == 'guinea']$
folds = 5$ n = training_data.shape[0]$ kf = KFold(n,folds,random_state=123)
print df.set_index(['date', 'item']).unstack()$
X = sales_2015[['store_number','sales_jan_mar', 'volume_sold_lt', 'net_profit', $                 'state_bottle_cost', 'state_bottle_retail', 'bottles_sold']]$ X.head()
auth = tweepy.OAuthHandler(consumer_key,consumer_secret)$ auth.set_access_token(access_token,access_token_secret)$ api = tweepy.API(auth,parser=tweepy.parsers.JSONParser())
np.mean(p_diffs > obs_diff)
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new$ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old$ p_diffs = new_converted_simulation - old_converted_simulation
sum(contractor.state_id.isnull()) #the count of missing state_id value is 0$ contractor.state_id.value_counts() #The state_id columns do not have missing data
gdax_trans_btc['Balance'] = gdax_trans_btc['Trade_amount'].cumsum();
df2[df2.duplicated(['user_id'], keep=False)]
start = pd.datetime(2015,4,16,10,0,0)$ stop = pd.datetime(2015,4,16,20,0,0)$ t_diff = calc_passed_time(df, start, stop)
G_df.to_csv("location.csv", index=False,encoding ='utf8')
rng = pd.date_range('3/6/2012 00:00:00', periods=10,freq="D",tz="US/Mountain")$ rng.tz, rng[0].tz
df2['user_id'].duplicated().sum()
df = pd.read_csv('ab_data.csv')$ df.head()
DataSet = DataSet[DataSet.userTimezone.notnull()]$ len(DataSet)
data["type"].unique()
model.most_similar("man")$
n_old = len(df2.query("group == 'control'"))$ print('The n_old is: {}.'.format(n_old))
sherpa = current.loc[df["By Name"] ==  "Sherpa "] $ sherpa
autos.sort_values("price_dollars",ascending=False).head(10)
from IPython.core.display import display, HTML$ display(HTML("<style>.container { width:100% !important; }</style>"))
print("The proportion of users converted is: {}".format(df['converted'].mean()))
vol = vol.replace(0, 1)$ vol.describe()
CRnew = df2.query('landing_page == "new_page"')['converted'].mean()$ CRold = df2.query('landing_page == "old_page"')['converted'].mean()
from scipy.stats import norm$ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05/2)))$
tweetsIn22Mar.index = pd.to_datetime(tweetsIn22Mar['created_at'], utc=True)$ tweetsIn1Apr.index = pd.to_datetime(tweetsIn1Apr['created_at'], utc=True)$ tweetsIn2Apr.index = pd.to_datetime(tweetsIn2Apr['created_at'], utc=True)
df_new.groupby('country')['converted'].mean()
%matplotlib inline$ commits_per_year.plot(kind='line', title='History of Linux', legend=False)
snowshoe_df = pd.read_pickle('..\data\interim\df_for_snowshoe_classification')$ snowshoe_df.drop(columns = ['datetime'],inplace=True)$ snowshoe_df.head()
df.subtract(df['R'], axis=0)
temps_df.iloc[1]
bottom_views = doctype_by_day.loc[:,doctype_by_day.max() < 10]$ ax = bottom_views.plot()$ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
result = clustering.mean_shift(reduced, bandwidth=3.8)$ labels = np.unique(result['labels'])$ print(len(labels))
trimmedDataFrame = noTempNullDF.drop(['DATE_1','ID_y','X','Y','Z'], axis = 1)$ trimmedDataFrame.head(10)
df.loc[(df["date"].dt.hour > 6) & (df["date"].dt.hour < 19), 'day'] = 1 #this is day$ df.loc[(df["date"].dt.hour <= 6) | (df["date"].dt.hour >= 19), 'day'] = 0 #this is night
twitter_df_wa=twitter_df[twitter_df.location=='Washington, DC']$ plt.plot(twitter_df_wa['created_at_time'], twitter_df_wa['retweet_count'], 'ro')$ plt.show()
train_set.columns
df2.groupby('converted').count()$
m = Prophet()$ m.fit(df1);
inoroffseason = ALLbyseasons.groupby("InorOff") # This groups our sample by whether transactions took place in-season or during$
logit_mod = sm.Logit(df3['converted'], df3[['intercept','ab_page']])$ results=logit_mod.fit()
image_predictions_clean=image_predictions_clean[~image_predictions_clean['tweet_id'].isin(retweets)]$
y_pred = model.predict(X_test)$ y_test_rescaled = scaler.inverse_transform(y_test)$ y_pred_rescaled = scaler.inverse_transform(y_pred)
from pyspark.sql.functions import col$
result2 = result.groupby(["Artist","Song"])["Rank"].min().reset_index().set_index('Artist')
df3 = df2.set_index('user_id').join(countries.set_index('user_id'))$ df3.head()
prop_grthan_p_diffs = len(p_diffs[np.where(p_diffs > calc_diff)]) / len(p_diffs)$ prop_grthan_p_diffs
dc['created_at'].hist(color="blue") # blue, David Cameron$ tm['created_at'].hist(color="orange") # orange, Theresa May
my_gempro.prep_itasser_modeling('~/software/I-TASSER4.4', '~/software/ITLIB/', runtype='local', all_genes=False)
lm = smf.ols(formula='y ~ x', data=df).fit()$ lm.params
dforders = ml4t.build_orders(dfprediction, abs_threshold=0.01, startin=False, symbol='USD-BTC')$ dforders
loan_stats["loan_status"].table()
auth = tweepy.OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)$ api = tweepy.API(auth)
sqlContext.registerFunction("TimesTen", TimesTen)
val_df.reset_index(drop=True, inplace=True)$ val_y.reset_index(drop=True, inplace=True)$ test_df.reset_index(drop=True, inplace=True)
cur.execute('SELECT EmpGradeRank, count(EmpGradeRank) FROM demotabl WHERE EmpGradeRank="SENIOR";')$ cur.fetchone()
incident_file_df = pd.read_csv(flu_incident_file, encoding = "ISO-8859-1")$ incident_file_df.head()
trump.describe()
bnb[bnb['age']>80].head()$
display(data.head(10))
archive_clean.info()$
to_plot = customer_emails[['Email', 'Days Between Int']]$ plt.title("Actual number of days between purchases")$ sb.distplot(to_plot['Days Between Int'].dropna())$
dj_df = dj_df[dj_df['company_ticker'] != 'V'] $ dj_df['quarter_start'] = pd.to_datetime(dj_df['quarter_start'])$ print(dj_df.info())
from templates.invoicegen import create_invoice
my_gempro.set_representative_structure()$ my_gempro.df_representative_structures.head()
shots_df[['PKG', 'PKA']] = shots_df['PKG/A'].str.split('/', expand=True)$ shots_df.drop('PKG/A', axis=1, inplace=True)
rtc = rtc.set_index('Export Region')$ rtc = rtc.drop(['Year'], axis=1)
df.head(20)
with open('united_list_lower.pickle', 'rb') as f:$     united_list_lower = pickle.load(f)
ab_df2 = ab_df2.drop(labels=2893)
plt.hist(null_vals);$ plt.axvline(x=act_diff,color ='red');
conf = SparkConf().setAll([('spark.executor.memory', '6g'), ('spark.executor.cores', '6'), ('spark.cores.max', '6'), ('spark.sql.session.timeZone', 'UTC')])$ sc = SparkContext("local", "llite", conf=conf)
df_tick_sent = df_tick.join(df_amznnews_2tick)
stocks.to_sql('stocks', engine, index = False, if_exists = 'append')
df.loc[:,"message"] = df['message'].str.findall('\w{3,}').str.join(' ') $ df.head()
print(f'dataframe shape: {playerAttr.shape}')$ print(playerAttr.columns)$ playerAttr.head(3)
cand_date_df['party'] = cand_date_df['sponsor_class'].map(party_dict)$ cand_date_df.head()
cvec.fit(X_train)$ X_train_matrix = cvec.transform(X_train)$ print(X_train_matrix[:5])
data_df.groupby('type')['ticket_id'].nunique()
TrainData.describe(include=['object'])
data.plot.line(x = 'Unnamed: 0', y = 'Age', figsize = (15, 10))
a = np.arange(25).reshape(5,5)$
S_distributedTopmodel.decision_obj.simulStart.value, S_distributedTopmodel.decision_obj.simulFinsh.value
tlen.plot(figsize=(16,4), color='r');$
print(autos.info())$ autos.head()
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week))$ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week))
df_data_desc = pd.read_csv(filepaths['data_desc'], encoding='latin1')$ df_data_desc = df_data_desc[df_data_desc.columns[1:]].fillna('')$ df_data_desc.head(10)
station_df = pd.read_sql("SELECT * FROM station", conn)
bool(login_page.find_all(string=re.compile('redirect')))
sub_data = data[data["time_zone"].notnull()]$ sub_data.head()
test = pd.read_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep = ',', $                    encoding='utf-8', low_memory=False)
subdf = data_df[:25]$ df = subdf.copy()$ df.head()
session.query(func.count(Stations.station)).all()$
train2014 = data[np.logical_and(data['date'] > '2013-12-31',data['date'] < '2015-01-01')]$ print(train2014.shape)
autos.describe(include='all')
log_mod1 = sm.Logit(df3['converted'], df3[['intercept','ab_page' ,'country_UK','country_US']])$ results1 = log_mod1.fit()
import cv2$ cv2.imread("smallgray.png",1)$
archive_df_clean=archive_df[archive_df.retweeted_status_id.isnull() == True]$
p_new = df2['converted'].mean()
zone_train.shape$
df_birth[df_birth.population > 1000000000]$
df2[(df2.group == 'control')].converted.mean()
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'pure_scorers'
reg_country.summary()
df.eval('D = (A + B) / C', inplace= True)$ df.head()
for  feature  in  ['country']:$     print("{}: {}".format(feature, countries_df[feature].unique()))
y=dataframe1['Close']$ plt.plot(y)$ plt.show()
df2[df2.duplicated(subset=['user_id'], keep='first')]$ print(df2.query('user_id == 773192'))
df.age.describe()$
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
station_activity = df.groupby('station').count().sort_values(['id'], ascending=False)$ station_activity
soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())$
for tweet in negativeTweets['Tweets'].iteritems():$     print (tweet)
new_page_converted=np.random.choice([1,0],size=n_new,p=[pnew,(1-pnew)])$ new_page_converted.mean()
df = pd.read_excel("Data/Moments Report.xls")$
session.query(Measurement.date).order_by(Measurement.date.desc()).first()
from pymongo import MongoClient$ client = MongoClient()$ db = client.yelp
allVars = read.getVariables()$ variables_df = pd.DataFrame.from_records([vars(variable) for variable in allVars], index='VariableID')$ variables_df.head(10)
df.set_index(rng, inplace=True)
h3 = qb.History(spy.Symbol, datetime(2014,1,1), datetime.now(), Resolution.Daily)
import pandas as pd$ data_df = pd.read_csv("metric_data.csv")$
precipitation_df.describe()$
response = chatbot.get_response("Where are you")$ print(response)
df_in = pd.merge(transactions,users,on='UserID',how='inner')$ df_in$
print(r.json()['dataset']['data'][0])
result1 = (df1 < df2) & (df2 <= df3) * (df3 != df4)$ result2 = pd.eval('df1 <df2 <= df3 != df4')$ np.allclose(result1, result2)
network_simulation[network_simulation.generations.isin([2])]$
kick_projects = pd.merge(kick_projects, ks_ppb, on = ['category', 'launched_year','goal_cat_perc'], how = 'left')
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
data.iloc[[1,10,3], [2, 3]]
data_set.to_csv("C://SHERYL MATHIAS/US Aug 2016/Fall 2017/INFM750/HillaryClintonTweets.csv", index=False, encoding='utf-8')
station_data = session.query(Stations).first()$ station_data.__dict__
excel_data[['Postal Code']] = excel_data[['Postal Code']].astype(object)$ print("Data Types: \n%s" % (excel_data.dtypes))
np.unique(bdata.index.time)
area.index | population.index
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 1), 'type'] = 'defenders'
session = pd.read_csv('Airbnb/sessions.csv')$
df_reg=injuries_hour[['date_time','Rain','injuries','wet','low_vis']]$ df_reg['hour']=pd.to_datetime(df_reg.date_time).dt.hour$ df_reg.head()$
df_totalConvs_byCountry = df_conv.groupby(['country_code'], as_index=False).sum()$ print 'DataFrame df_totalConvs_byCountry', df_totalConvs_byCountry.shape$ df_totalConvs_byCountry.sort('conversions', ascending=[0])
df.info()
df2 = pd.read_csv('new_edited.csv')
master_df.name.value_counts().hist();
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
quartiles = np.percentile(births['births'], [25, 50, 75])$ mu = quartiles[1]$ sig = 0.74 * (quartiles[2] - quartiles[0])
data.index
codes = pd.read_csv('data/icd-main.csv')$ codes = codes[(codes['code'] != codes['code'].shift())].set_index('code')
raw_data.head()
df_twitter_copy = df_twitter_copy.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis = 1)
store=join_df(store,store_states,"Store")$ weather=join_df(weather,state_names,'file','StateName')$ sum(store['State'].isnull()),sum(weather['State'].isnull())
series.index$
mlp_df = pd.read_csv(mlp_fp, index_col='Date', parse_dates=True)$ mlp_df.head()
lm=smf.ols('tripduration ~ gender',data=df1).fit()$ lm.summary()
def clean_companies(companies_df):$     companies_df.first_funding_at = companies_df.first_funding_at.apply(lambda x: np.nan if type(x) == unicode else x)$     return companies_df
df['converted'].mean()$
import builtins$ builtins.uclresearch_topic = 'GIVENCHY' # 226984 entires$ from configuration import config
df['Position'].plot(figsize=(20,10))$
noHandReliableData = reliableData[reliableData['ID'] != 'Hand']$ noHandReliableData['ID'].unique()
user_total = df.nunique()['user_id']$ print("Number of unique users is : {}".format(user_total))
dataset.to_csv("tweet_energia_ok.csv",index=False, sep=';',encoding='utf-8')
df_tte_all = pd.concat([df_tte_ri,df_tte_ondemand])
result_final.summary2()
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
n_new = df2[df2['landing_page'] == 'new_page'].shape[0]$ n_new
new_page_converted.mean() - old_page_converted.mean()
train_data_df = pd.read_csv('./data/n26/train.csv')
backers = databackers.pivot_table(index = 'backers', columns='successful',aggfunc=np.size)$ backers = backers.rename(columns= {1: 'Successful', 0:'Failed'})$ backers[backers['Successful'] == backers['Successful'].max()]
from sklearn import metrics$ print metrics.accuracy_score(y_test, predicted)$
Base = automap_base()$ Base.prepare(engine, reflect=True)$
df2c = df.query('group == "control" and landing_page == "old_page"')
os.environ.get('FOO')
vocab = vectorizer.get_feature_names()$ print (vocab)
lmdict = pd.read_excel('https://www3.nd.edu/~mcdonald/Word_Lists_files/LoughranMcDonald_MasterDictionary_2014.xlsx')
appointments = pd.read_csv('./data/AppointmentsSince2015.csv')
sl[sl.status_binary==0][(sl.today_preds<sl.one_year_preds)].shape
pd.melt(df, id_vars=['A'], value_vars=['B'])
df.info()
result = customer_visitors.groupby('Yearcol').mean().astype(int)$ result$
df[df.client_event_time >= datetime.datetime(2018,4,2,0,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
dframe_team.drop(dframe_team.columns[[2,3,4,5]],inplace=True,axis=1)
df2.head()
hdf.reset_index().tail()
year_prcp_df = year_prcp_df.sort_values("date")$ year_prcp_df.head(10)
data.columns = ['Tweets','len', 'ID','Date', 'Source', 'Likes', 'RTs', 'SA']$ data.to_csv('experiment1.csv', index_label='Index_name')
url = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2017-01-01&end_date=2017-12-31&' + API_KEY$ r = requests.get(url)$ json_data = r.json()
p_new = df2['converted'].mean()$ p_new
import numpy as np$ step_counts = step_counts.astype(np.float)$ print step_counts.dtypes
df_tte['InstanceType'] = df_tte['UsageType'].apply(lambda usage: usage.split(':')[1]) #create column with instance type
clf.fit(X_train, y_train)
df2.drop_duplicates('user_id', inplace=True)$ df2[df2['user_id']==773192]
date = pd.to_datetime("4th of July, 2017")$ date
df["grade"] = df["raw_grade"].astype("category")$ df["grade"]
weather.info()
results = session.query(Measurement.tobs).all()$ tobs_values = list(np.ravel(results))$ tobs_values
data.iloc[0, 2] = 90$ data
match_results = pd.read_csv("data/afl_match_results.csv")$ odds = pd.read_csv("data/afl_odds.csv")$ player_stats = pd.read_csv("data/afl_player_stats.csv")
precip_data = session.query(measurment).first()$ precip_data.__dict__
df[pd.isnull(df.Address1)]
df.text[df.text.str.len() == df.text.str.len().min()]
archive_clean.loc[archive_clean['rating_denominator'] != 10, 'rating_denominator'] = 10
df_new['intercept'] = 1$ df_new[['US', 'UK']] = pd.get_dummies(df_new['country'])[['US','UK']]
dataset.loc[dataset['created_at'].isnull()]
def listening_longevity(x):$     x['listening_longevity'] = (x.iloc[-1].date - x.iloc[0].date).days$     return x$
filtered_brewery = df.groupby('brewery_name').filter(lambda x: x.brewery_name.value_counts() >= 3)$ brewery_bw = filtered_brewery.groupby('brewery_name').rating_score.mean().sort_values(ascending=False)
Station_data = session.query(Measurement).first()$ Station_data.__dict__
col_names = list(zip(df_test.columns, df_train.columns))$ for cn in col_names:$     assert cn[0] == cn[1]
Z = np.dot(np.ones((5,3)), np.ones((3,2)))$ print(Z)$
df_tweet = pd.read_json('./WeRateDogs_data/tweet_json.txt', lines=True)
total_fit_time = (end_fit - start_fit)$ print(total_fit_time/60.0)
x = store_items.isnull().sum().sum()$ print(x)
observed_mean_control = df2.query('group=="control"')['converted'].mean()$ print(observed_mean_control)
a = np.arange(1, 11)$ a
flight_delays = flight_delays.reindex(taxi_hourly_df.index)$ flight_delays.fillna(0, inplace=True)
for edge in H.edges():$     if H[edge[0]][edge[1]]['weight'] > 3000:$         print(edge,partition[edge[0]],partition[edge[1]])
import os$ import gmaps$ gmaps.configure(api_key='AIzaSyC-TiBcF4pJGqxRyg0G3q7tCajyHYnf98E') # Your Google API key$
twitter_ar.text[1]
train_frame = train_frame.reset_index()$ df_series = pd.Series(train_frame["values"])$
data.name.isnull().sum()
total.iloc[-3:]
train_set.drop(["socialEngagementType"], axis=1, inplace=True)
df_dummies = pd.get_dummies(df_onc_no_metac['METAC_SITE_NM1'], prefix = 'METAC_SITE')$ df_dummies.head()
vlc[pd.notnull(vlc.bio.str.strip())].id.size
df['time_detained'] = df['time_detained'] / np.timedelta64(1,'D')
df = df.sort_values('DATE')
df_freq_users = df[df["user_id"].duplicated()]
p_diffs = np.array(p_diffs)$ (p_diffs > diff1).mean()$
df2[df2['group'] == "treatment"].converted.mean()
pf.cost.sum()/100
lin_mod = sm.OLS(df_new['converted'], df_new[['intercept', 'ab_page', 'US', 'new_US', 'UK', 'new_UK']])$ result = lin_mod.fit()$ result.summary()
stations_df.count()
reviews.groupby('taster_twitter_handle').taster_twitter_handle.count()
plt.hist(p_diffs);
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)$ core_samples_mask[db.core_sample_indices_] = True$ core_samples_mask
weights['0.encoder.weight'] = T(new_w)$ weights['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))$ weights['1.decoder.weight'] = T(np.copy(new_w))
df2 = df.drop(df[((df.group =='treatment') & (df.landing_page == 'old_page') | (df.group == 'control') & (df.landing_page == 'new_page'))].index)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
import engarde.decorators as ed
print(groceries.index)$ print(groceries.values)
df2=df2.drop(treat_old.index)$ df2=df2.drop(control_new.index)$ df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
new_messages = messages.copy() $ new_messages.head()
print("Probability an individual recieved new page:", $       df2['landing_page'].value_counts()[0]/len(df2))
uniqueusers=df.user_id.value_counts()$ uniqueusers.size$
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new$ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old$ p_diffs = new_converted_simulation - old_converted_simulation
np.count_nonzero(nba_df.isnull())
d = {'id': ids, 'sentiment': solution}$ output = pd.DataFrame(data=d)$ output.to_csv( "Word2Vec_Keras_AverageVectors.csv", index=False, quoting=3 )
top_places = df["user_location"].value_counts()[:15]$ print top_places
fav_plot = t_fav.plot(figsize=(16,4), label="Favorites", legend=True, title='Number of favorites for tweets over time')$ fav_vs_time_fig = fav_plot.get_figure()$ fav_vs_time_fig.savefig('num_favs_over_time.png')
tweet_df_clean.reset_index(inplace=True)$ tweet_df_clean.drop('index', axis=1, inplace=True)$
table.colnames
df['_merge'].value_counts()
building_pa_prc.describe(include='all')
df_inventory_santaclara =df_santaclara.transpose()                     ##Transpose $ df_inventory_santaclara.reset_index(level=0,inplace=True)$ df_inventory_santaclara.columns=['Date','Units']
xmlData['county'].value_counts()
containers[0].find("li", {"class":"paid-amount"}).span.contents[-1].split()[2]
diffs = new_page_converted.mean() - old_page_converted.mean()$ print("Difference between mean of each scenarios probability:",diffs)
engine = create_engine("sqlite:///hawaii.sqlite")
hdf['Age'].mean(level=[0, 1]).head()
df.head(10)$ df[['Indicator_ID','Country','Year','WHO Region','Publication STATUS']].sort_index().head(3)
statezips = csvData['statezip'].value_counts().reset_index()$ statezips.columns = ['statezip', 'count']$ statezips[statezips['count'] < 5]
previous_month_date = end_date - timedelta(days=30)$ pr = PullRequests(github_index).get_cardinality("id").since(start=previous_month_date).until(end=end_date)$ get_aggs(pr)
column_list2 = ['Temperature','DewPoint']$ df[column_list2].plot()$ plt.show()
bruins = pd.read_csv('../../../data/bruins/home.csv')
import sys$ src_dir = os.path.join('..', 'src')$ sys.path.append(src_dir)
y = tweets['handle'].map(lambda x: 1 if x == 'realDonaldTrump' else 0).values$ print max(pd.Series(y).value_counts(normalize=True))
file_cap = pyshark.FileCapture('captures/botnet-sample.pcap')
ws = Workspace.from_config()$ print(ws.name, ws.resource_group, ws.location, sep = '\n')
modelCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])$
df_weekly = df_mean.merge(df_count[['date', 'id']], suffixes=('_average','_count'), on='date').merge($     df_max[['date', 'week', 'text', 'polarity', 'negative', 'retweets']], on='date', suffixes=('_average','_max'))
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
res = sts.query(qry)
Tip_percentage_of_total_fare = taxiData.Tip_amount / taxiData2.Fare_amount
data.loc[:, ['TMIN', 'TMED']].head()
df.info()
eval_data = pd.concat([predictions, actuals])$ eval_data['day'] = eval_data.index
plt.scatter(tfav,tret, marker='.', color='red')
df.isnull().any().any()
parameter = '00060'$ Shoal_Ck_15min = pull_nwis_data(parameter=parameter, site_number='08156800', start_date='2018-02-01', end_date='2018-02-18', site_name='08156800')$ Shoal_Ck_15min.head(10)$
data.head()
print('Outliers are points below {} or above {}.'.format((scores_firstq - (1.5 * IQR)), (scores_thirdq + (1.5 * IQR))))
df1 = df_arch_clean.copy()$ df1['timestamp'] = pd.to_datetime(df1['timestamp'])$ df1['day_of_week'] = df1['timestamp'].dt.weekday_name$
df2[df2.duplicated(['user_id'], keep=False)]
prev_year = date.today() - timedelta(days=365)$ results = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= prev_year).all()$ df = pd.DataFrame(results, columns=['date', 'precipitation'])
net.save_nodes(nodes_file_name='nodes.h5', node_types_file_name='node_types.csv', output_dir=directory_name)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
countries = pd.read_csv('countries.csv')$ countries.head()
data =[]      $ for row in result_proxy:$     data.append({'station':row.stations.name,'RainFall':row.rainfall})
df['2017-06-01':'2017-06-10'].Close.mean()$
questions = pd.concat([questions.drop('bands', axis=1), bands], axis=1)
endog = data_df.Count$ exog = data_df[['TempC','WindSpeed','Precip']]$
dfd = pd.get_dummies(dfm, columns=['country'], drop_first=True)$ dfd = pd.get_dummies(dfd, columns=['cat_type'], drop_first=True)
errors = pd.DataFrame.from_dict(d, orient="index").rename(columns={0:"mae_vals"})$ errors.head()
y[y**2 - np.sin(y) >= np. cos(y)] # math function based mask!$
driver = webdriver.Chrome('/Users/daesikkim/Downloads/chromedriver', chrome_options=options) # chrome_options=options$ driver.implicitly_wait(3)
image_predictions.info()
stations = session.query(func.count(Station.id)).scalar()$ print(stations)
vec1 = modelD2V.infer_vector(currentData[0].words)$ vec2 = modelD2V.infer_vector(currentData[1].words)$
prec_long_df.describe()
session.query(tobs.station, func.count(tobs.tobs)).group_by(tobs.station).\$                order_by(func.count(tobs.tobs).desc()).all() 
((pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days)*365
df = pd.concat([df1,df2])$ df
df_new.country.value_counts()$ df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])
df1.columns = df2.columns$ df=pd.concat([df1,df2])$ df.head()
df2 = df2.drop_duplicates(subset=['user_id'])$
jobs_data.nunique()
df = pd.concat([chgis, new_data])
reddit = reddit.drop_duplicates(subset=['Titles', 'Subreddits'], keep='last', inplace=False) #dropping those $ reddit.head()$ reddit.shape #now I see those specific rows have been dropped$
theta = np.minimum(1,np.maximum(0,np.random.normal(loc=0.75, scale=0.1, size=(10000,5))))
df_traffic.to_csv('Cleaned_Traffic_Count_Coordinates.csv')$ df_traffic$
print('There are {} news articles'.format(len(news)))$ print('Timewise, we have news from {} to {}'.format(min(news.date), max(news.date)))
sing_fam = sing_fam[sing_fam.rp1lndval != 0]$ sing_fam = sing_fam[sing_fam.larea > 0]$ sing_fam.shape
breast_cancer_df = pd.read_csv('data/breast_cancer.csv')$ breast_cancer_df.head()
pgh_311_data.sample(10)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print("z-score:", z_score,$      "\np-value:", p_value)
(B[B.values>0.01])
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
df_C = df_A.copy()$ columns = df_C.columns.tolist()$ res = pd.concat([pd.DataFrame(df_A.values, columns=columns), pd.DataFrame(df_B.values, columns=columns)], keys=["A", "B"])$
print("_____________________________________________")$ media_user_overall_df = pd.DataFrame.from_dict(media_user_overal_results)$ media_user_overall_df.head() 
model_ft = FastText.load_fasttext_format('C:/Users/edwin/Desktop/FastText/wiki.zh.bin')$
RDDTestScorees.groupByKey().collect()
import pickle$ filename = 'finalized_automl.sav'$ pickle.dump(automl, open(filename, 'wb'))
print(gdp_df.head())$ print(gdp_df.GDPC1.head())$ gdp_df.GDPC1.plot()
df.shape$
learner.save_encoder('adam3_10_enc')$
import IPython$ print (IPython.sys_info())$ !pip freeze
sns.heatmap(data_numeric.corr().abs().round(2),annot=True)
df_adopted = df_adopted.groupby('user_id').sum()$ df_adopted.head()$
df2[df2.duplicated(['user_id'], keep=False)]
expanded_data.head()
metrics = pd.read_json('../metadata/metrics.json')$ metrics
plt.clf()
parse_dict['creator'].head(5)$
writer = pandas.ExcelWriter(os.path.join(output_dir,$                                          '{}_example_stats.xlsx'.format(cur_experiment)))
classifier = svm.SVC(kernel='linear')$ classifier.fit(X_train, y_train)$ y_prediction_SCM = classifier.predict(X_test)$
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=JyyZ2KQSdpCZHjbk1E9x"$ req  = requests.get(url)$
survey = resdf.iloc[:,:113]$ survey.insert(2,'LangCd',resdf.iloc[:,120])$ survey.to_sql('surveytabl',conn)
df_tweet_clean.rename(columns={'id': 'tweet_id'}, inplace=True)
con = sqlite3.connect('db.sqlite') # open connection$ con.close() # close connection, important to prevent data losses
dict(list(r_close.items())[0:10])
trades = ta.Signal(signal).close_to_close(msft.pxs.close)$ trades
tz_dateutil = dateutil.tz.gettz('Europe/London')
transit_df['DELEXITS']= transit_df['EXITS'].diff()$
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
name = "binding_2018-04-03_no1"$ data.to_csv("models/comparison/"+name+".csv")
analysis_historical =final_coin_data[['coin_name','date_hist','open_price','close_price']]$ a['Volitility_30_day'] = a['close_price'].rolling(window=30).std().reset_index(drop=True)$
weather.zip_code.unique()
mRF.predict(test)
(p_diffs > obs_diffs).mean()
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)$ df.info()
type2017.isnull().sum() 
cur = conn.cursor()$ query = "SELECT name FROM sqlite_master WHERE type='table';"$ cur.execute(query)
csvData[csvData['street'].str.match('.*North.*')]['street']
filter_commands = ['module load python/python-3.3.0']$ filter_commands.append("python %s/filter_fasta.py %s/swissProt_emb_proteins.fasta -out_fasta %s/swissProt_emb_proteins_filtered.fasta -v %s" %(PY_PATH, proteins_evidence_dir, proteins_evidence_dir, "'Arabidopsis thaliana' 'Solanum lycopersicum' 'Oryza sativa' 'Glycine max' 'Vitis vinifera'"))$ send_commands_to_queue('filter_swissprot', filter_commands, queue_conf)
S_1dRichards.basin_par.filename
df_treat= df2[df2['group'] == 'treatment']['converted'].mean()$ print("{} is the probability they converted.Thus, given that an individual was in the treatment group.".format(df_treat))
seng = 'mysql+pymysql://root:'+spassword+'@localhost/sakila'$ engine=create_engine(seng)$ data = pd.read_sql_query('select * from actor', engine)$
df_vow.plot()$ df_vow[['Open','Close','High','Low']].plot()
print("dataframe df row names as follows : ")$ df.index.values
df_user = pd.read_csv(f_user)$ df_user.head(3)
new_page_converted = np.random.binomial(1, p_new, n_new)$ new_page_converted[:5]
df.drop(['adwordsClickInfo'],axis=1, inplace=True)
a = a.reshape(4, 5)$ a
lm = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','US']])$ results_lm = lm.fit()$ results_lm.summary()
for i, words in enumerate(kochdf['name']):$         words = words.replace(char,' ')$     words = words.replace('  ',' ')
drop_list = ['FEDFUNDS','DGS1MO','DGS3MO','DGS6MO','DGS1','DGS2','DGS3']$ for drop_x in drop_list:$     df.drop(drop_x, axis=1, inplace=True)
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK', 'US']]$ df_new.head()
tallies_file = openmc.Tallies()
hdf['Age'].mean(level=0) # simple as that
print(df2['landing_page'].value_counts(normalize=True))$ print(df2['landing_page'].value_counts())
df1 = df1[df1['Title'].str.contains(blacklist) == False]$ df1.shape
AFX_X_dict = json.loads(AFX_X_data.text)
new_df = df.dropna(subset=['driver_id','pickup_lat','pickup_lon','dropoff_lat','dropoff_lon'],inplace=False)
not_in_oz = not_in_oz[['stopid', 'address', 'lat', 'lng', 'routes']]$ new_stops = new_stops.append(not_in_oz)$ new_stops.head(5)
ride_df_suburban = suburban.groupby('city')$ city_df_suburban = suburban.set_index('city')
au.save_df(df_city, 'data/city-util/proc/city')$ au.save_df(df_util, 'data/city-util/proc/utility')$ au.save_df(misc_info, 'data/city-util/proc/misc_info')  # this routine works with Pandas Series as well
df2[df2['user_id'].duplicated()]$
print("Repeated users = ", len(df2) - len(df2.user_id.unique()))$ df2[df2.duplicated('user_id')]
s=random_numbers[random_numbers.index.weekday==2].sum()
df_favored = df_stars[df_stars.stars > 4]$ user_favored = df_favored[df_favored.user_id == df_favored.user_id.iloc[0]]$ df_similar_items[df_similar_items['business_id'].isin(user_favored.business_id)].sort_values('score',ascending=False).similar[:5]
d =  pd.get_dummies(dataset,columns=['age','salary','pet','lvl']);$ d.head()$
import seaborn as sns$ sns.set()$ sns.set_context("talk")
df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))$ df['body_tokens'] = df['body'].str.lower()
recortados = [recortar_tweet(t) for t in tweets_data_all]$ tweets = pd.DataFrame(recortados)
type2017.head()
df['date'] = df.start_date.dt.date
df_master_select = df_master_select.dropna()$ df_master_select.head()
rain_df.describe()
mbti_text_collection_filler.to_csv('Reddit_mbti_data_filler.csv',encoding='utf-8')
u235_scatter_xs = fuel_xs.get_values(nuclides=['(U235 / total)'], $                                 scores=['(scatter / flux)'])$ print(u235_scatter_xs)
divs = body.find_all('div',class_='slide')$ divs
df_new['active_days'] = df_new['last_session_creation_time'] - df_new['creation_time']$ df_new['active_days'] = df_new['active_days'].astype(pd.Timedelta).apply(lambda l: l.days)$
preview = pd.read_csv("Data/311_Service_Requests_from_2015.csv",          # Check the filepath!$                       nrows=100)$ print("Done loading the data frame!")
final_df.corr()["ground_truth_crude"][names]
autos = autos[autos['registration_year'].between(1900,2016)]$ autos['registration_year'].value_counts(normalize=True)$
locations = session.query(Station).group_by(Station.station).count()$ print(locations)
tweetDF = sqlContext.createDataFrame(data, filtered_check_list)
import pprint as pp$ pp.pprint(lds.describe())
df2['ab_page*CA'] = df2.ab_page*df2.CA$ df2['ab_page*US'] = df2.ab_page*df2.US
subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})]
questions = pd.concat([questions.drop('vip_reason', axis=1), vip_reason], axis=1)
import pandas_datareader as pdr$ q=pdr.get_data_yahoo('AAPL')$ q
df2['tweet_id'][(df2['predict_1_breed'] == True)].count()/2075$
trip_data_q5["hour"] = trip_data_q5.lpep_pickup_datetime.apply(lambda x: pd.to_datetime(x).hour)$ trip_data_q5["hour"] = trip_data_q5["hour"].astype('category')$ mean_hour = trip_data_q5.groupby("hour").agg(["mean"])
model = sm.Logit(df_new['converted'], df_new[['UK', 'US', 'ab_page', 'intercept']])$ result = model.fit()
Base = automap_base()$ Base.prepare(engine, reflect=True)
tweets_clean = tweets.copy()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-02-02&end_date=2018-02-03&api_key=' + API_KEY$ r = requests.get(url)$ json_data = r.json()
company_df.to_csv('companies.csv')
d311 = pd.read_csv(dic_inp["detroit-311.csv"], quotechar='"',converters={"lng":float,"lat":float})$ d311.drop_duplicates(subset="ticket_id")$ d311.rename(columns={"lng":"long"},inplace=True)
convert_new = df2.query("landing_page == 'new_page'")['converted'].sum()$ convert_new
x = store_items.isnull()$ print(x)$
tokens['one_star'] = tokens.one_star / nb.class_count_[0]$ tokens['five_star'] = tokens.five_star / nb.class_count_[1]
urls_to_shorten = [link for link in urls if ux.is_short(link)]$ urls_to_shorten
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct="%1.1f%%", shadow=True, startangle=180)
ts = pd.Timestamp(datetime.datetime(2016,5, 12, 3, 42, 56))$ ts
df['Request Status'].value_counts()
model_x.summary2() # For categorical X.
df2.iloc[146678]$
train[train.url.isnull()].head()
df2 = df2t.merge(df2c, how='outer')
import gensim, logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
df_cs.head(2)
tweets['timestamp'] = pd.to_datetime(tweets['timestamp'])$
exiftool -csv -createdate -modifydate MVI_0011.mp4 MVI_0012.mp4 > out.csv
afx_17 = json.loads(afx_x_2017.text)$ type(afx_17)
model = gensim.models.Word2Vec(sentences, min_count=1)
df = pd.read_csv("ab_data.csv")$ df.head()
    return sorted(d.items(), key=lambda x: x[1], reverse=reverse)
autos["odometer"]= autos["odometer"].replace("[,km]",'',regex=True) $ autos["odometer"]
stories = pd.concat([stories.drop('tags', axis=1), tag_df], axis=1)$ stories.head()
df.iloc[]
all_df = train_df.append(test_df)$
z_score, p_value = sm.stats.proportions_ztest((convert_new, convert_old), (n_new, n_old), alternative='larger')$ z_score, p_value
from sklearn.model_selection import train_test_split$ tips_train, tips_test = train_test_split(tips, test_size=0.2, random_state=123)$ tips_train.shape, tips_test.shape, tips.shape
x['age'] = x['age'].astype('timedelta64[D]')
us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())$ business_days = pd.DatetimeIndex(start=train.date.min(), end=train.date.max(), freq=us_bd)
my_stocks = ["WIKI/AAPL.1", "EIA/PET_RWTC_D.1", "NSE/OIL.1"]$ import quandl as q$ stock_open = q.get(my_stocks, start_date="2017-01-01", end_date="2018-02-01")
print("Err rate Model")$ for rate, name in sorted((rate, name) for name, rate in best_error.items()):$     print("%f %s" % (rate, name))$
d1=Series(pd.to_datetime(datestrs));  $ d1
df_new[['new_page', 'old_page']] = pd.get_dummies(df_new.landing_page)
np.exp(result2.params)
port.pl.txn_frame.tail(5)
df2.query('landing_page == "new_page"').count()[0]/df2.count()[0]
print('Median trading volume during 2017 - {}'.format(list_median(traded_volumes)))
lm = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']])$ res = lm.fit()
len(cvec.get_feature_names())$
col['date_time'] = col.index.map(str) + " " + col["TIME"]$ col['date_time'] = pd.to_datetime(col['date_time'])$ col['date_time']=pd.to_datetime(col.date_time.dt.date) + pd.to_timedelta(col.date_time.dt.hour, unit='H')
useful_indeed = indeed[~indeed['salary_clean'].isnull()]$ useful_indeed.shape$
df3 = df3.drop('wk1', axis=1)
df['price_log'] =  np.log10(df['price'])$ df['powerPS_log'] =  np.log10(df['powerPS'])                           
table = soup.find('table')$ print (table)
earlier_timestamps = len(timestamp_left_df[timestamp_left_df['time_delta_seconds'] > 0])$ ratio = earlier_timestamps / len(timestamp_left_df) *100$ print("There are %.2f%% objects in which the 'timestamp' are even earlier than events were created." % ratio)
df_lm.filter(regex='q_lvl_0|last_month|q_lvl_0_c').boxplot(by='last_month', figsize=(10,10),showfliers=False)
p_old = round(df2['converted'].mean(),4)$ print(p_old)
inspector = inspect(engine)$ inspector.get_table_names()
therm_fiss_rate = sp.get_tally(name='therm. fiss. rate')$ fast_fiss = fiss_rate / therm_fiss_rate$ fast_fiss.get_pandas_dataframe()
data_2018 = pd.read_csv(filename_2018)
url_template.format(start,)
days = pd.date_range('2014-08-29','2014-09-05',freq='B')$ for d in days: print(d)
dataframe.columns = column_names
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(RN_PA_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours First Difference Partial Autocorrelation')
pickle_in = open('linearregression.pickle', 'rb')$ clf = pickle.load(pickle_in)
matched2014_rents = pd.read_pickle('INFO_PUMA')$ logit_params = pd.read_pickle('LOGIT_PARAM')
data = data.sort_index()$ data
pd.value_counts(Stockholm_data.tweet_created_at).reset_index()$ countData = pd.read_csv(path + "countData.csv")$ countData
mlp_pc_fp = 'data/richmond_median_list_prices_percent_change.csv'$ mlp_pc.to_csv(mlp_pc_fp, index=True) # Pass index=True to ensure our DatetimeIndex remains in the output
open('test_data//open_close_test.txt')
tweets_data_path = '../data/tweets_si.json'$ tweets_file = open(tweets_data_path, "r")$ tweets_data = json.load(tweets_file)
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()
fmt = '%Y-%m-%d %H:%M:%S'$ dateparse = lambda dates: pd.datetime.strptime(dates, fmt)$ rawautodf = pd.read_csv('used-cars-database/autos.csv', sep=',',encoding='Latin1',parse_dates = ['dateCrawled','dateCreated','lastSeen'], date_parser = dateparse)
df_ratings.head()
df.loc[df['field1']>27,['created_at','field1']]
WHOregion = df['WHO Region'].values$ WHOregion
shows['cancelled'].value_counts()$
costs = 0.1$ df_customers['profits'] = (df_customers['price']-costs)*df_customers['number of customers']
theft = crimes[theft_bool]$ theft.head()
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
old_page_converted = np.random.normal(0, p_old, n_old)
day_change = [(daily[2]-daily[3]) for daily in afx_17['dataset']['data']]$ print('Largest change in 1 day based on High and Low price is $%.2f.' % max(day_change))
tweet_json.info()
appleInitialNegs = neg_tweets[neg_tweets.author_id_y == 'AppleSupport']
AFX_X_data = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=ZfFwbzzp8_Rsbi_mGznR&start_date=2017-01-01&end_date=2017-12-31')
assert len(target_docs) == 200000, 'target_docs should be truncated to the first 200k rows to use the cached model.'$ fname = get_file(fname='kdd_lm_v2.h5', origin='https://storage.googleapis.com/kdd-seq2seq-2018/kdd_lm_v2.h5', )$ model = load_model(fname)
mta_avg = dfs_morning.groupby(by = 'STATION').ENTRIES_MORNING.mean().reset_index()$ mta_census = mta_avg.merge(census_zip, left_on=['STATION'], right_on=['station'], how='left')$ mta_census.drop('station', axis = 1, inplace = True)
Temperature_DF=pd.DataFrame(results)$ Temperature_DF.head()
predictions = model.predict(Train_X)
dummy_df['price_change_1day'] = (dummy_df['price'].shift(-1440) - dummy_df['price']).fillna(method = 'ffill')$ dummy_df['price_change_2days'] = (dummy_df['price'].shift(-2880) - dummy_df['price']).fillna(method = 'ffill')$ dummy_df['price_change_3days'] = (dummy_df['price'].shift(-4320) - dummy_df['price']).fillna(method = 'ffill')
yc_new2['Tip_Amt'] = yc_new['Tip_Amt'] / yc_new['Fare_Amt'] * 100$ yc_new2.head()
plt.rcParams['figure.figsize'] = 8, 6 $ plt.rcParams['font.size'] = 12$ viz_importance(rf_reg, wine.columns[:-1])
by_area['AQI Category'].value_counts()
print df.shape$ ab_counts = df.groupby('ab_test_group').count().reset_index()$ ab_counts$
coin_data.columns
df.dtypes
response = requests.get(url)$ response
np.exp(reg_country.params)
average_of_averages = df['average'].mean()
rows=df.shape[0]$ print("The total number of rows are : {}".format(rows))
for urlTuple in otherPAgeURLS[:3]:$     meritParagraphsDF = meritParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ meritParagraphsDF
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url)
weather.precipitation_inches = pd.to_numeric(weather.precipitation_inches, errors = 'coerce')
n_new = df2.query('group == "treatment"')['user_id'].count()$ n_new = int(n_new)$ n_new
cercanasA1_11_14Entre25Y50mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 25) & (cercanasA1_11_14['surface_total_in_m2'] < 50)]$ cercanasA1_11_14Entre25Y50mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre25Y50mts.apply(descripcionDistancia, axis = 1)$ cercanasA1_11_14Entre25Y50mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
unsorted_AMD_data = pd.read_csv(AMD_data_file) $ unsorted_AMD_data.head()
len([col_n for col_n in df.columns if 'warning' in col_n])$
weather_df["weather_description"] = weather_df.weather_description.str.replace("light rain and snow", "light snow")$ weather_df["weather_description"] = weather_df.weather_description.str.replace("rain and snow", "snow")$ weather_df["weather_description"] = weather_df.weather_description.str.replace("sleet", "snow")
df1['PCT_Change']=(df1['Adj. Close']-df1['Adj. Open'])/df1['Adj. Open'] $ df1['PCT_Change'].head()
df = pd.DataFrame(x_9d)$ df = df[[0,1,2]] # only want to visualise relationships between first 3 projections$ df['X_cluster'] = X_clustered
table_rows = driver.find_elements_by_tag_name("tbody")[4].find_elements_by_tag_name("tr")$
df.converted.mean()
%run '../forecasting/helpers.py'$ %run '../forecasting/main_functions.py'$ %run '../forecasting/ForecastModel.py'
from arcgis.gis import GIS$ from IPython.display import display$ gis = GIS() # anonymous connection to www.arcgis.com
taxi_hourly_df.loc[index_missing, :]
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
dr_new = doctors[doctors['ReasonForVisitDescription'].str.contains('New')]$ dr_existing = doctors[~doctors['ReasonForVisitDescription'].str.contains('New')]
containNaN=final_df[final_df.isnull().any(axis=1)]$
tweetsPorFecha=tweet_frame[['tweetCreated','userID']]$ tweetsPorFecha.loc[:,('tweetCreated')]=tweetsPorFecha['tweetCreated'].map(lambda x: x.strftime('%Y/%m/%d') if x else '')$ tweetsPorFecha
old_page_converted = np.random.choice([0, 1], n_old, p = [p_old, 1-p_old])
ex4.drop([1,2])
new_page_converted = np.random.binomial(1, p_new,n_new)$ new_page_converted.mean()
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
%matplotlib inline$ %pylab inline$ pylab.rcParams['figure.figsize'] = (20, 9)   # Change the size of plots
for iter_x in np.arange(lookforward_window)+1:$     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
df4.describe() # basic stats all from one method$
data = pd.concat([data, df_cat], axis=1)
df.isnull().sum()
Net_Capital_Gain = pd.read_sql(q, connection)$ Net_Capital_Gain.head()
iris.loc[iris["Species"].isin(["setosa","versicolor"]),:].sample(10)
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
tweet_length.plot(figsize=(20,5),color='r')
df2[df2.duplicated('user_id')==True].user_id
from gensim import corpora, models$ dictionary = corpora.Dictionary(sws_removed_all_tweets)
datetime.now().date()
tick_locations = [value+0.4 for value in x_axis]$ plt.xticks(tick_locations, bar_outlets)
rt_re = r''$ hash_re = r''$ hash_or_link = ...
sh_max_df = pd.DataFrame(np.array(sh_results), columns=(["date","tobs"]))$ sh_max_df
df_final_edited_10 = df_final_edited.loc[df_final['rating_denominator']==10]$
masked['user_age'] = masked['created_at'] - pd.to_datetime(masked['user_created_at'])
uber1.to_csv('uber1.csv', index=False)$ uber2.to_csv('uber2.csv', index=False)$ uber3.to_csv('uber3.csv', index=False)
df.shape 
tweet_df = pd.read_csv('tweet_json.txt')$ tweet_df.info()
march_2016 = pd.Period('2016-03', freq='M')$ print march_2016.start_time$ print march_2016.end_time
today_ = datetime.date.today().strftime("%Y_%m_%d")$ today_ = '2018_06_07'$ print("today is " + today_)
faa_data_phil_pandas = faa_data_pandas[faa_data_pandas['AIRPORT_ID'] == "KPHL"]$ print(faa_data_phil_pandas.shape)$ faa_data_phil_pandas.head()
train_df = pd.read_json('train.json')$ test_df = pd.read_json('test.json')$ train_df.shape, test_df.shape
t1.stage.value_counts()
calc_temp_df = pd.DataFrame(calc_temp, columns=['Min', 'Max', 'Avg'])$ calc_temp_df
temps_df.iloc[[1,3,5]]['Difference']
archive_clean.text.str.extractall( r"(\d+\.?\d*\/\d+\.?\d*\D+\d+\.?\d*\/\d+\.?\d*)")$
state_grid_new = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(21, 21))$ q_agent_new = QLearningAgent(env, state_grid_new)$ q_agent_new.scores = []  # initialize a list to store scores for this agent
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])$ df_new = df_new.drop('CA', axis = 1)$ df_new.head()
plt.hist(y_res)$ plt.hist(y)$ plt.show()
fakes.comment[0]
count_cols = [col_str for col_str in list(merged_feature_df.columns) if col_str.endswith('_count')]$ merged_feature_df[count_cols] = merged_feature_df[count_cols].fillna(0)
dataframe.loc[rows,columns] # .loc can take row ID$ dataframe.iloc[row_ix,columns_ix]
series_d = merged_df[merged_df['Funding Type'] == "Series D"]$ print(len(series_d.index))
airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')$ print(airquality_pivot.head())
data.userScreen.max() # calls the user that appears most times under the userScreen column$
table_list = pd.read_html("http://www.psmsl.org/data/obtaining/")
df2 = df.drop(df_nomatch.index)
predict.predict_score('Water_fluoridation')
rows - df.shape[0]
new_page_ind = df2.query('landing_page == "new_page"').shape[0]$ print('Probability and individual got the NEW PAGE: {:.4f}'.format(new_page_ind/df2.shape[0]))
forecast_range = forecast1[::-1]$ forecast_range.info()$ forecast_range.head()
!head -n 2 ProductPurchaseData.txt
data = session.query(hi_measurement).first()$ data.__dict__
%matplotlib inline$ word_freqs.plot(30)
node_types_DF = pd.read_csv(node_models_file, sep = ' ')$ node_types_DF
old_page_converted=np.random.choice([0,1],size=nold[0],p=[pold,1-pold])$ print(len(old_page_converted))
groupby_breed = df_master.groupby('dog_breed').sum().reset_index()$ groupby_breed.sort_values('favorite_count',inplace=True,ascending=False)
T_stn=pd.DataFrame(dfA_stn)$ T_stn.head(1)
autos['brand'].value_counts(normalize=True)$ brand_unique = autos['brand'].unique()[:6]$
df2[df2.user_id.duplicated(keep=False)]
airline_training_set = nltk.classify.util.apply_features(extract_features, airline_tweets)$ NBClassifier = nltk.NaiveBayesClassifier.train(airline_training_set)
data.head()$ data.to_csv('FBprices.csv')
df_new[['UK','US']] = pd.get_dummies(df_new['country'])[['UK','US']]$ df_new.head()
cabs_df_rsmpld = cabs_df_byday.resample('1M')['passenger_count'].count()$ cabs_df_rsmpld.head()
df_rows = df.shape[0]$ print("There are {} rows in the dataset.".format(df_rows))
df['age'] = 2017 - df['birth year']$ df['age'].dropna(inplace= True)
for df in Train_kNN, Test_kNN:$     df.Var1 = str(df.Var1)
df2['user_id'].nunique()
print '::'.join(pieces)$ print '--'.join(pieces)$ print ' '.join(pieces)
pd.DataFrame(dummy_var["_Source"][Company_Name]['High']['Forecast'])[-6:]
with open('key_phrases.pickle', 'wb') as f:$     pickle.dump(key_phrases, f, pickle.HIGHEST_PROTOCOL)
stmt = text("SELECT * FROM states where last_changed>=:date_filter")$ stmt = stmt.bindparams(date_filter=datetime.now()-timedelta(days=20))
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
X_train.isnull().sum()
df2.head()
scores_mode = scores.sort_values(ascending=False).index[0]$ print('The mode is {}.'.format(scores_mode))
userProfile = userGenreTable.transpose().dot(inputMovies['rating'])$ userProfile
store_items.fillna(0)
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ]$ pd.pivot_table(games_2017, values = "Opp.Pts", index = "Team", aggfunc = np.mean).sort_values(by = "Opp.Pts", ascending = False).head(5)
soup = BeautifulSoup(response.text, 'html.parser')
celtics = pd.read_csv('../../../data/celtics/home.csv')
data[['Sales']].resample('D').mean().expanding().mean().head()$
scipy.stats.kruskal(df2["tripduration"], df4["tripduration"])
trips.duration.describe()
old_prob = gb.count().values / sum(gb.count().values)
Jarvis_ET_Combine = pd.concat([Jarvis_rootDistExp_1, Jarvis_rootDistExp_0_5, Jarvis_rootDistExp_0_25], axis=1)$ Jarvis_ET_Combine.columns = ['Jarvis(Root Exp = 1.0)', 'Jarvis(Root Exp = 0.5)', 'Jarvis(Root Exp = 0.25)']
df.drop('Ehail_fee', axis=1, inplace=True)$ df.columns
bufferdf.Fare_amount.mean().ix[[2,3,4]]
support.amount.sum()
print pd.merge(df0, df2, how='right')$
outlet_mean_score=sentiments_df.groupby("Media_Source")["Compound"].mean()$ outlet_mean_score = round (outlet_mean_score,2) $ outlet_mean_score #check
new_stops.to_sql('main_stops', engine, if_exists='append', index=False)
df2[df2['group']=='control']['converted'].mean()
date.month
tags = tags.sum(level=0)$ tags.head()
print(soup.prettify())
df3.to_csv('lyincomey_subset_v5_wseconds.csv')
from nltk.corpus import stopwords$ portuguese_stop_words = stopwords.words('portuguese')
df_fbase.l_total_asset = df_fbase.total_asset.shift(1)$ df_fbase.l_total_asset = df_fbase.l_total_asset.where(df_fbase.year != "2012")$ df_fbase.avg_total_asset = df_fbase.total_asset.where(df_fbase.year != "2012").rolling(2).mean()
ts.shift(-2)
df_predictions_clean=df_predictions.copy()
df_bill_data= df_bill_data.drop('bill_id', axis=1)$ df_bill_data = df_bill_data.groupby(['patient_id','date_of_admission']).sum().reset_index()$ df_bill_data.to_csv('df_bill_data.csv')
df_cs['Sentiment_class'] = df_cs.apply(conditions, axis=1)$ df_cs.to_csv("costco_senti_score.csv", encoding='utf-8', index=False)
keep_RNPA_cols = ['follow up Telepsyche', 'Follow up', 'follow', 'Returning Patient']$ RNPA_existing = RNPA_existing[RNPA_existing['ReasonForVisitName'].isin(keep_RNPA_cols)]$ pd.value_counts(RNPA_existing['ReasonForVisitName'])
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],$                    'data': range(6)}, columns=['key', 'data'])$ df
df_p = pd.DataFrame({'Date': list_date, 'Clique Words': list_clique, 'Tweets':list_whole})$ df_p = df_p.sort_values('Date')$ df_p$
portfolio_df.reset_index(inplace=True)$ adj_close_acq_date = pd.merge(adj_close, portfolio_df, on='Ticker')$ adj_close_acq_date.head()
columnsToDropDuplicates = ['body']$ dfTickets = dfTickets.drop_duplicates(columnsToDropDuplicates)$ print(dfTickets.shape)
test_features = bind_features(test, train_test="test").cache()$ test_features.count()
full_data.to_pickle('D:/CAPSTONE_NEW/jobs_data_full.pkl')
df_z= df_cb.groupby(["landing_page","group"]).count()$ df_z$
bool_list = crime_data["OFNS_DESC"].isin(['VEHICLE AND TRAFFIC LAWS', 'ALCOHOLIC BEVERAGE CONTROL LAW']) == False$ crime_data = crime_data[bool_list]$
grouper = dta.groupby(dta.results)
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
cityID = '300bcc6e23a88361'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Seattle.append(tweet) 
two_day_sample.reset_index(inplace=True)
df2[df2['group'] == "treatment"]['converted'].mean()
sns.pairplot(iris, hue='Species')
lda_model = LatentDirichletAllocation(n_components=15, random_state=42, learning_method = 'batch')$ train_lda_features = lda_model.fit_transform(tfidf_vectorized)$ np.mean(np.max(train_lda_features, axis=1)) #15
test[['clean_text','user_id','predict']][test['user_id']==1497996114].shape[0]
df3['WEEK'] = df3['DATE'].dt.week
lims_query = 'SELECT cell.name, cell.external_specimen_name, donors.full_genotype, donors.name \$ FROM specimens cell JOIN donors ON cell.donor_id = donors.id \$ WHERE cell.ephys_roi_result_id IS NOT NULL'$
cust_data['MonthlySavings1'] = cust_data['MonthlyIncome'] * 0.15$ cust_data.head(2)
gpCreditCard.Passenger_count.describe()
sp500.Price
df['converted'].mean()
fig, ax = plt.subplots(figsize=(16, 9))$ df_t1.boxplot(column='time_hrs', by='Task', ax=ax)$ ax.set_xticklabels([])$
mean_pr = prec_long_df.groupby(['date'])['prec_kgm2'].mean()$ mean_pr.plot(x='date', y='prec_kgm2')
print(p.shape)$ p.head()
columns = ['doggo', 'floofer', 'pupper', 'puppo']$ twitter_archive_clean = twitter_archive_clean.drop(columns, axis=1)
calls.head()
lr = LinearRegression()$ lr.fit(train_data, train_labels)
subset1 = df[5500:6500]$ subset1.plot(x='timestamp', y='value')
nasa_url = 'https://mars.nasa.gov/news/'
P = type.__new__(LetsGetMeta,'S',(),{})$ P.__class__.__class__
symbol='^NBI'$ benchmark1 = web.DataReader(symbol, 'yahoo' , start_date ,end_date)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata  = pd.read_csv(path, sep =' ')$ mydata.head(5)
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')$ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
print("prediction: {0}, actual: {1}, 2018-03-31".format(y_pred_list[0], y_test_aapl[0,0]))$
np.exp(result4.params)
s.describe()
df_new[['CA','US','UK']]=pd.get_dummies(df_new['country'])$ df_new.head()$
twitter_Archive.info()
%%time$ grid_svc.fit(X_tfidf, y_tfidf)
n_old = df2[df2.group == "control"].count()[0]$ print("The population of user under treatment group: %d" %n_old)
en_es = pd.read_csv(FPATH_ENES, sep=" ", header=None)$ en_es.columns = ["en", "es"]$ en_es.describe()
title_alt = imgs.find_all("img", class_="thumb")$ title_alt
conn_laurel = psycopg2.connect("dbname='analytics' user='analytics' host='analytics.cv90snkxh2gd.us-west-2.rds.amazonaws.com' password='!TgP$Ol9Z&6QhKW0tmn9mOW5rYT2J8'")$
with open('train.csv') as f:$     size=len([0 for _ in f])$     print("Records in train.csv => {}".format(size))
df_twitter_copy[df_twitter_copy.retweet_count.isnull()]
birth_dates.head(3)
baseball1_df = baseball_df[['playerID','birthYear','birthMonth','birthDay','debut','finalGame']].dropna()$ baseball1_df.reset_index(inplace = True)$ baseball1_df.tail()$
temp_df = active_psc_records[active_psc_records.company_number.isin(secret_corporate_pscs.company_number)]$ len(temp_df[temp_df.groupby('company_number').secret_base.transform(all)].company_number.unique())
p_new = df2['converted'].mean()$ print ("convert rate for p_new under the null :{} ".format(round(p_new, 4)))
model = tree.DecisionTreeClassifier()$ print ('Decision tree')$ reg_analysis(model,X_train, X_test, y_train, y_test)
%%time$ reader = pa.RecordBatchFileReader(data_dir + file_name + '.arrow')$ read_table = reader.read_all()
F_hc = a_hc[score_variable].var() / m_hc[score_variable].var()$ degrees1hc = len(m_hc[score_variable]) -1$ degrees2hc = len(a_hc[score_variable]) -1
df2 = df.query('group == "control" & landing_page == "old_page" | group == "treatment" & landing_page == "new_page"')
S_1dRichards.meta_basinvar.filename
test_copy1.bot.value_counts()
p_mean = np.mean([p_new,p_old])$ print("Probability of conversion under the null hypothesis: " + str(p_mean))
path = 'C:\\Users\\lefte\\Desktop\\Data Science Challenge\\all journeys'$ path = path.replace('\\', os.sep)$
stations = "Resources/data/hawaii_stations.csv"
cp311.columns = [ x.lower().replace(' ','_') for x in cp311.columns]
results_para = soup.find_all("div", { "class" : "rollover_description_inner" })$ news_p = results_para[0].text$ news_p$
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2016-06-07&end_date=2016-06-09&api_key=yUSy9ms6sx2Ubk55dXdN")$
train_df, y, nas = proc_df(train_data, 'totals.transactionRevenue')
matches = re.findall('\d+', $     'the recipe calls for 10 strawberries and 1 banana')$ print(matches)$
spp['DK'] = spp[spp.columns[spp.columns.str.contains('DK')==True]].sum(axis=1)$ spp['DE'] = spp[spp.columns[spp.columns.str.contains('DE')==True]].sum(axis=1)
d4 = df.iloc[[0, 3], [1, 2]]
npath = save_filepath + '/uva_hpc/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
s = df_sms.groupby(['group','ShopperID'])['ID'].count().reset_index()$ s.groupby(['ID','group']).size();
df.zip.replace(56201,52601,inplace=True)
df_piotroski = pd.DataFrame({'id' : stock_id_ls, 'name': stock_name_ls}, columns = ['id', 'name', 'market_type', 'quant', 'market_sum', 'property_total', 'debt_total', 'listed_stock_cnt', 'pbr', 'face_value',])
df_old = df.query('landing_page == "old_page"')$ p_old = df_old[df_old['converted'] == 1]['converted'].count() / df_old[['converted']].count()$ print (p_old)
with open('youtube_urls.json', 'r') as fp:$     youtube_urls = json.load(fp)
gdax_trans['Timestamp'] = pd.to_datetime(gdax_trans['Timestamp'], format="%d/%m/%Y %H:%M:%S")
!grep -A 50 "build_estimator" taxifare/trainer/model.py
ndf.max()
top_10_authors = git_log.author.value_counts(dropna=True).head(10)$ top_10_authors
train_features = bind_features(train_reduced, train_test="train").cache()$ train_features.count()
service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'
xml_in['publicationDate'] = pd.to_datetime(xml_in['publicationDate'], format='%Y-%m-%d', errors='coerce')
df1 = make_df('AB', [1, 2])$ df2 = make_df('AB', [3, 4])$ display('df1', 'df2', 'pd.concat([df1, df2])')
rf=RandomForestClassifier(labelCol="label", featuresCol="features")$ labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels)$ pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,labelIndexer, OH1, OH2, OH3, OH4, OH5, OH6, assembler, rf, labelConverter])
pt_all=pd.DataFrame.pivot_table(df_users_6,index=['cohort'],values=['uid'],aggfunc='count',fill_value=0)
display(data[data['age'] == data['age'].min()])$ display(data[data['age'] == data['age'].max()])
afx_x_2017 = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY)$ type(afx_x_2017)
import pandas as pd$ loctweetdf = tweetdf.loc[~pd.isnull(tweetdf['lat'])]
df.mean() # same as df.mean(0)
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf],$       labels=["bad", "ok", "yeah!"]).sample(10)
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new = df_new.drop(['CA'], axis=1)$ df_new.head()
df.shape
df2['user_id'].nunique()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative="larger")$ print(f"z-score: {z_score} \np-value: {p_value}")
!pwd #location of the current working directory. $
zip_counts=bus["business_id"].groupby(bus["postal_code"]).size().sort_values(ascending=False)$
categories_feat = df.groupby(['business_id']).categories.apply(np.unique)$ categories_feat.head()
images_copy = images.copy()$ images_copy['tweet_id'] = images_copy['tweet_id'] .astype('str')$ print (type(images_copy['tweet_id'].iloc[0]))
from scipy import stats$                $ stats.pearsonr(ng['pc_change'], lll['pc_change'])$
df_tweet_clean.tweet_id = df_tweet_clean.tweet_id.astype(str)
df_new.groupby('country').count()
violations_trimmed.head(1)
twitter_data = "TrumpTweets.csv"$ twitter_df = pd.read_csv(twitter_data, encoding = "ISO-8859-1")$ twitter_df.head()
for dim in d.dimensions:$     print('%s:\t%s' % (dim, d.dimensions[dim]))
hi_iday_delta_index = close_deltas.index(max(close_delta for close_delta in close_deltas if close_delta is not None))$ ans_5 = ('%s had greatest between-day difference: %s' % (dates[hi_iday_delta_index], close_deltas[hi_iday_delta_index]))
x_scaled = 0$ if len(x_normalized) > 1:$     x_scaled = min_max_scaler.fit_transform(x_normalized)
df2[df2.duplicated(['user_id'], keep=False)]
soups = [soup(requests.get(url).text, 'html.parser') for url in article_urls]
users = list(db.osm.aggregate([{"$group":{"_id": "$created.user", "count":{"$sum": 1}}}, {"$sort": {"count": -1}}]))$ print 'Total users = ',len(users)$ users[:80]$
engine.execute("select * from measurements").fetchall()
list(map(range, a.shape))
groupby_x = df['y'].groupby(df['x'])$ round(groupby_x.describe(), 3)
fse2017 = dict(r.json())
scoring_url = client.deployments.get_scoring_url(deployment_details)$ print(scoring_url)
print type(RandomOneRDD)$ print RandomOneRDD.collect()
github_data['project_year'] = github_data.project_creation_date.apply(lambda x: x[:4])$ github_data['user_year'] = github_data.user_creation_date.apply(lambda x: x[:4])
points.iloc[6]$
df_insta=pd.read_csv('Instagram_Mobility_20_fixed.csv')$ df_insta=df_insta.dropna(subset=['coords'])$ df_insta.head()
station_tobs = session.query(Measurement.tobs).filter(Measurement.station == 'USC00519281', Measurement.date >= '2016-08-23').all()$
new_page_converted.mean()
df2[df2.duplicated(['user_id'], keep=False)]
recipes.name[np.argmax(recipes.ingredients.str.len())]
contractor_final.to_sql('contractor', con=engine,if_exists='append',index = False)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Therapist Hours Autocorrelation')
df_new['intercept']=1$ df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
new_group.get_group('N')
df.hist(bins=50, figsize=(15,15));$
df2[['old_page', 'ab_page']] = pd.get_dummies(df['group'])
df2.country.value_counts()
url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'$ browser.visit(url)
twitter_archive[twitter_archive.tweet_id.duplicated()]
datAll['year'] = datAll['Date'].map(lambda x: x.year)$ datAll['month'] = datAll['Date'].map(lambda x: x.month)$
assert (ebola >= 0).all().all()
df_countries = pd.read_csv('countries.csv')$ df_countries.head()
data_df.clean_desc[22]
svc.score(X_tfidf_test, y_tfidf_test)
df_co = pd.read_csv('countries.csv')$ df_co.head()
noise = [np.random.normal(0,noise_level*p,1) for p in weather.power_output]
automl = autosklearn.regression.AutoSklearnRegressor($ )
df2[df2.duplicated(['user_id'])]
tempDf=df.drop([5,12,23,56])$ tempDf.head(60)
import statistics$ statistics.median(v)
may_acj_data.head()
p4_result = p1_table.sort_values('Profit', ascending=True)$ p4_result.head()
my_gempro.get_scratch_predictions(path_to_scratch='scratch', $                                   results_dir=my_gempro.data_dir,$                                   num_cores=4)
newfile = newfile[~newfile['Reason for rejection'].isin({'X1', 'X8'})]
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']])$ results = mod.fit()$ results.summary()
seed = 2210$ (train1, dev1, modeling2) = (modeling1$                              .randomSplit([0.4, 0.1, 0.5], seed=seed))
df2.drop(labels = 2893, axis = 0, inplace = True)
tweet_json = pd.read_json('tweet_json.txt',lines=True)$ tweet_json.to_csv('tweet_json.csv')
lr = LogisticRegression(random_state=20, max_iter=10000)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr','multinomial'], 'solver':['saga','newton-cg', 'lbfgs']}$ grid = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
df = df.set_index('email')
os.chdir("C:\\#Study\\2. Ryerson_all\\THE MRP\\Dataset\\test_pad")
np.array(df[['Visitors','Bounce_Rate']]).tolist()
groupby_location = df1['location'].groupby($     df1['location']).count().sort_values(ascending=False)$ print(groupby_location.head())$
df.sample(10)
(actual_diff>p_diffs).mean()
twitter_archive.sample(5)
image_predictions.sample(5)
page_interaction_result.summary()
df.sort_values (by='J')
Shoal_Ck_hr = Shoal_Ck_15min.resample('h', on=str('DateTime')).mean() $ Shoal_Ck_hr.head(10)$
df_session_dummies = pd.get_dummies(df, columns=['action'])$ df_session_dummies.head()
knn = KNeighborsClassifier(n_neighbors=5)$ knn.fit(X_train_total, y_train)$ knn.score(X_test_total_checked, y_test)
model3 = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK','ab_page']])$ model3.fit().summary()
df2[df2['landing_page'] == 'new_page']['landing_page'].count() / df2['landing_page'].count()
df.game_type.value_counts()
ch.setLevel(logging.WARNING)
filenames = os.listdir(path)$ filenames
str(containers[0].find("li", {"class":"transaction"}).a["title"])
match = len(df[(df['group']=='control') & (df['landing_page'] == 'old_page')]) + len(df[(df['group']=='treatment') & (df['landing_page'] == 'new_page')])$ print(df.shape[0] - match)
len(df.user_id.unique())
requests.delete(BASE + 'networks/' + str(sample_network_suids[0]) + '/nodes')
unique_users = len(df['user_id'].unique().tolist())$ print('There are ' + str(unique_users) + ' unique users in the dataset.')
paired_df[['dataset_1', 'dataset_2']] = paired_df.paired_datasets.apply(pd.Series)$ paired_df = paired_df.loc[:, ['dataset_1', 'dataset_2','co_occurence']]$ paired_df = paired_df.loc[paired_df.co_occurence > 1]
s=table.find(text='Survivors').find_next('td').text$ survivors=re.search(r'\d+', s).group()$ survivors
df = pd.read_sql_query("select * from track limit 3 offset 5 ;", conn) $ df
dup_user = df2['user_id'][df2['user_id'].duplicated()]$ dup_user
df1.isnull().sum()
df = pd.read_csv(chat_file, error_bad_lines=False)
lm = sm.Logit(df2['converted'],df2[['intercept','treatment']])$ result = lm.fit()$ result.summary()
import pandas as pd$ df = pd.read_csv('autos.csv',encoding='cp1252')$
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative='larger')$ print('Value of z-score is:',z_score)$ print('p-value is the following:',p_value)
INT = INT.sort_values(by = ['Contact_ID','Int_Name'])
df.index = df.datetime
latest_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()[0]$ format_latest_date = dt.strptime(latest_date,"%Y-%m-%d")$ format_latest_date # Display the date
top_10_authors = git_log['author'].value_counts()[:10]$ top_10_authors
unsorted_Google_data = pd.read_csv(Google_data_file) $ unsorted_Google_data.head()
df['complaint_'] = df['complaint'].apply(lambda x: (x[0]))
url='https://dzone.com/articles/top-5-data-science-and-machine-learning-course-for'
top_songs['Year'] = top_songs['Date'].dt.year
result_new.summary2()
df = pd.read_csv('ab_data.csv')$ df.head()
mean_sea_level.plot(kind="kde", figsize=(12, 8))
cityID = '8e9665cec9370f0f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Minneapolis.append(tweet) 
df2 = df.drop(control_wrong.index)$ df2.drop(treatment_wrong.index, inplace = True)
df3 = df8.add_suffix(' Created')$ df7 = pd.merge(df,df3,how='left',left_on='Date Created',right_on='MEETING_DATE Created')$
import numpy as np$ X_nonnum = X.select_dtypes(exclude=np.number)
df_ratings.describe()
df3.head()
ymc=yearmonthcsv.coalesce(1)$ path=input("Enter the path where you wanna save your csv")$ ymc.write.format('com.databricks.spark.csv').save(path,header = 'true')
plt.savefig("SentimentScatterPlot.png")$ plt.show()
df_repeat=df2[df2['user_id'].duplicated()]['user_id'].values$ print("Repeated user_id is "+str(df_repeat[0]))
author_commits = git_log.groupby('author').count().sort_values(by='timestamp', ascending=False)$ top_10_authors = author_commits.head(10)$ top_10_authors
precip_df = pd.DataFrame(year_precip)$ index_date_df = precip_df.set_index('date')$ index_date_df.head()
JoinedDF = RandomOneDF.join(RandomTwoDF, RandomOneDF["ID"] == RandomTwoDF["id"] )$ for row in JoinedDF.take(5):$     print row
april_acj_data.head()
transactions.merge(users, how='inner', on='UserID')
extract_deduped_with_elms_v2.shape, extract_deduped_with_elms.shape
test_tweet = api.user_timeline(newsOutlets[0])$
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = log_mod.fit()
df = pd.DataFrame(data=fruits_and_veggies)$ df
df2.drop(labels = 2862, axis = 0, inplace = True)
home_dest = df_titanic['home.dest']$ print(home_dest.describe())$
fix_space = lambda x: pd.Series([i for i in reversed(x.split(' '))])
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, 1-p_new]).mean()$ new_page_converted
reduced_trips_data = trips_data.loc[abs(trips_data.duration - trips_data.duration.mean()) <= (3*trips_data.duration.std())]$
response = requests.get(nasa_url)$ print(response.text)
import matplotlib.cm as cm$ proudlove_c, perry_c, all_c, regression_c, *_ = cm.Paired.colors
jobs_data3 = json_normalize(json_data3['page'])$ jobs_data3.head(5)
users_nan = (users.isnull().sum() / users.shape[0]) *100$ users_nan
df_concat_2.message_likes_rel = np.where(df_concat_2.message_likes_rel > 10000, 10000, df_concat_2.message_likes_rel)
faa_data_destroyed_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "D"]$ print(faa_data_destroyed_damage_pandas.shape)$ faa_data_destroyed_damage_pandas
print 'There are %d unique names in the data set' % len(np.unique(df['Name'].values))$ df['has_name'] = df['Name'].apply(lambda x: 0 if pd.isnull(x) else 1)$ del df['Name']$
output_fn = "News_Sentiment_Analysis.csv"$ sentiment_df.to_csv(output_fn)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])$ old_page_converted.mean()
df2['ab_page']=df2.group$ df2.ab_page.replace(('treatment','control'), (1,0), inplace=True)
reddit_master.shape
light_curve_fit_df.to_hdf('/Users/jmason86/Dropbox/Research/Postdoc_NASA/Analysis/Coronal Dimming Analysis/Example Fit Dimming Light Curve.hdf', 'light_curve_df')
extracted_rating=twitter_archive_clean.text.str.extract('([0-9]+\.?[0-9]*/[0-9]+)', expand=False)$ extracted_numerator=extracted_rating.apply(lambda x: x.split('/')[0])$ twitter_archive_clean['rating_numerator']=extracted_numerator
df.index.weekday_name
purchase_history = pd.merge(orders_subset, $                             order_details_prior, $                             on=['order_id'])[['user_id', 'product_id']]
ps.sort_index().tail(600)
client.deployments.list()
unique_users = set(orders.user_id.unique())$ selected_users = random.sample(unique_users, 20000)
by_lp_issue = time2close_1.groupby(['launchpad_issue'])
bus.groupby(["name","business_id"]).count()
dti2 = pd.to_datetime(['Aug 1, 2014', 'foo']) $ type(dti2)
df['date'] = pd.to_datetime(df['date'])
df.Visitors.tolist()
logger.info('Define Source')$ data = pd.DataFrame.from_csv('~\\neo.csv')$ logger.debug('df: %s', data)
df.iloc[]
from IPython.display import display, Math, Latex
n_old = df2[df2['group'] == 'control'].shape[0]$ n_old
regressor = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']])$ result = regressor.fit()
unassembled_human_genome_length = gdf[gdf['type'] == 'supercontig'].length.sum()$ percentage_incomplete = (unassembled_human_genome_length / human_genome_length)*100$ print("{}% of the human genome is incomplete.".format(round(percentage_incomplete, 4)))
dataset = dataset.drop('Unnamed: 0', axis=1)$
train, test = data.randomSplit([0.8,0.2], seed=6)$ train.cache()$ test.cache()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-12-29&end_date=2017-12-29&api_key=' + API_KEY)$ r.json()
to_plot_df = df.groupby(['original_language']).mean()$ to_plot_df
df = pd.read_csv('ab_data.csv')$ df.head()
df = pd.DataFrame(results, columns=['date', 'precipitation'])$ df.set_index(df['date'], inplace=True)$ df.tail()
users.groupby('CreationDate')['LastAccessDate'].count().plot()$
df.loc[df.index.isin([11,24,37])]
from sqlalchemy_utils.functions import create_database$
df.reorder_levels(order=['Date', 'Store', 'Category', 'Subcategory', 'UPC EAN', 'Description']).head(3)
ts.tshift(5,freq="H")
total_new_page = (df2['landing_page'] == 'new_page').sum()$ total_new_page/unique_users
print(festivals.dtypes)$ print(festivals.info())$
new_converted_simulation = np.random.binomial(n_new, p_new, 10000)/n_new$ new_converted_simulation.mean()
df_tweet.head()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger')$ z_score, p_value$
cust_data1.drop('MonthlySavings1', axis=1).head(2) # it creates new data$ cust_data.head(2)
(null_vals > obs_diff).mean()
plt.hist(old_page_converted);
df4 = df3.drop_duplicates()$ df4.describe()
search.groupby(['search_type','booking']).size()
autos.describe(include='all')
hits_df = number_one_df.drop_duplicates(subset=['URL','Region'], keep ='first')
len(station_tobs)$ print(" There are {} tobs within the data set".format(len(station_tobs)))
news_title_docs_high_freq_words_df = pd.read_pickle(news_title_docs_high_freq_words_df_pkl)$ with pd.option_context('display.max_colwidth', 100):$     display(news_title_docs_high_freq_words_df)
dat[dat.ward.isnull()]
tweetering.polarity.plot(kind='hist', normed=True)$ range = np.arange(-1.5, 1.5, 0.001)$ plt.plot(range, norm.pdf(range,0,1))
noise_data[noise_data["Created Date"] > 20151201].head()
df_new['day'] = np.where(pd.to_datetime(df_new['timestamp']).dt.dayofweek >= 5, 'weekend', 'weekday')
import datetime$ print (datetime.datetime.now())
first_row = session.query(Station).first()$ first_row.__dict__
df.rename(columns={'Indicator':'Indicator_id'}).head(2)
print 'There are', len(df_ks) , 'posts and' , df_ks['id'].nunique() , 'users mentioning Kate Spade'
dup_ent = "2017-01-09 05:37:58.781806"$ df2 = df2[df2.timestamp != dup_ent]
assert not tcga_target_gtex_expression_hugo_tpm.isnull().values.any()$ assert not treehouse_expression.isnull().values.any()$ assert np.array_equal(tcga_target_gtex_expression_hugo_tpm.index, treehouse_expression.index)
df_clean = df_clean.drop('variable', axis = 1)$ df_clean = df_clean.rename(columns = {'value':'dog_stage'})$
df_ab_cntry.drop(['UK'], axis=1, inplace=True)
session.query(Measurement.date).\$ filter(Measurement.station == 'USC00519281').\$ order_by(Measurement.date.desc()).first()
df_new['country'].unique()$
logit_mod_joined = sm.Logit(df_joined_dummy.converted, \$                            df_joined_dummy[['intercept', 'ab_page_new_page',\$                                             'country_UK', 'country_US']])
daily_constituent_count = QTU_pipeline.groupby(level=0).sum()$ QTU_pipeline.groupby(level=0).median().describe()
google.resample('M').median()
autos.describe(include='all')
iris_csv = pathlib.Path('../datasets/iris.csv').absolute()$ iris = pd.read_csv(str(iris_csv))
cityID = 'c0b8e8dc81930292'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baltimore.append(tweet) 
model = smf.ols(formula='revenue ~ genre', data=tmdb_movies_genre_revenue)$ results = model.fit()$ print (results.summary())
df.head(10)
cl_ca.columns = ['XML_' + x if x!='APPLICATION_ID' else 'APP_APPLICATION_ID' for x in cl_ca.columns.values]
ml.confusion(dep_test.reshape(dep_test.shape[0]), $              predicted, ['No', 'Yes'], 2, 'Smoker Classification [Numeric & Categoric]')
df_sample_day=df_sample[(df_sample["day"] == 1)]$ df_sample_night=df_sample[(df_sample["day"] == 0)]$ scipy.stats.ks_2samp(df_sample_day["tripduration"],df_sample_night["tripduration"])
df2 = pd.DataFrame({'attributes': cols, 'correlation': correlations}) 
dire = os.getcwd()$ tr = pd.read_csv(dire+'/train.csv',header = 0,encoding="utf-8")$ re = pd.read_csv(dire+'/resources.csv',header = 0,encoding="utf-8")
c.description
norm.ppf(1-(0.05/2))
df_clean['timestamp']=pd.to_datetime(df_clean['timestamp'], format='%Y-%m-%d %H:%M')$ df_clean.info()
new_page_converted = np.random.binomial(n=n_new, p=p_null) / n_new$ new_page_converted
import ssbio.core.io$ my_saved_gempro = ssbio.core.io.load_json('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.json', decompression=False)
((null_values>obs_diff).mean())
df2 = df.query("(group == 'control' & landing_page == 'old_page') or (group == 'treatment' & landing_page == 'new_page')")
userByCountry_df  = youtube_df[youtube_df["channel_title"].isin(channel_namesC)]
twitter_Archive.info()$
df.info()
from bs4 import BeautifulSoup as soup ##BeautifulSoup$$ from urllib.request import urlopen as uReq$ import requests
import pandas as pd$ git_log = pd.read_csv('datasets/git_log.gz', sep='#', header=None, encoding='latin-1', names=['timestamp','author'])$ print(git_log.head(5))
autos = autos.drop(index = price_outliers)
df.to_csv('Tableau-CitiBike/TripData_2018_Winter.csv', index=False)
display(data.head(10))
df2_treatment = df[df.group == 'treatment']$ (df2_treatment.converted == 1).sum()/len(df2_treatment)
df2 =df2[df2.index !=1899]
sqladb=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',header=None,skipinitialspace=True)
metadata['spatial_extent'] = refldata.attrs['Spatial_Extent_meters']$ metadata
url = 'https://mars.nasa.gov/news/'$ response = requests.get(url)$ soup = bs(response.text, 'html.parser')
data['sepal_area'] = data.sepal_length * data.sepal_width$ print(data.iloc[:5, -3:])
users = pd.read_json("C:/users/jchung/desktop/D-Matrix/users.json") #Work
df_business = df_average.join(df_categories_svd)$ print df_business.shape$ df_business.head()
submit.head(1)
image_predictions = pd.read_csv(filename, sep='\t')$ image_predictions.head()
df2 = pd.read_csv('ab_cleaned.csv')
old_page_converted=np.random.binomial(n_old,p_old)$ old_page_converted
experimentp = df2[df2['group'] == 'treatment']['converted'].mean()$ experimentp
con = cx_Oracle.connect('FSCPHKL', '<put password here>', '172.18.20.70:/FSCLIVE')$ print con.version$
cutoff_date = datetime.strptime("Fri Jan 13 00:00:00 +0000 2017",'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo = pytz.UTC)$ data = data[data["user_created_at"] < cutoff_date]
combined_df['Returned'] = combined_df['Returned'].fillna('No')$ print(combined_df.head(5))
StockData.set_index(['Date'], inplace=True)
pd.DataFrame(hillary)
cityID = '4fd63188b772fc62'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Laredo.append(tweet) 
churned_bool = pd.Series([USER_PLANS_df.loc[uid]['status'][np.argmax(USER_PLANS_df.loc[uid]['scns_created'])] =='canceled' for uid in USER_PLANS_df.index],index=USER_PLANS_df.index)
raw_data_df = pd.read_csv('Daisy Debt.csv', parse_dates=[0]) $ raw_data_df.head()
kick_projects.loc[:,'goal_reached'] = kick_projects['pledged'] / kick_projects['goal'] # Pledged amount as a percentage of goal.$ kick_projects.loc[kick_projects['backers'] == 0, 'backers'] = 1 $ kick_projects.loc[:,'pledge_per_backer'] = kick_projects['pledged'] / kick_projects['backers'] # Pledged amount per backer.
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
ax = pdf.plot()$
small_data = pd.concat([data.iloc[:2],$                       data.iloc[-2:]])
print 'RF: %s' % rf.score(X_test, y_test)$ print 'KNN: %s' % knn.score(X_test, y_test)
sub_df2=df2.query('dow=="Friday" | dow=="Monday"' )
%matplotlib notebook$ df1 = var_per_portfolio.toPandas()$ df2 = df1.set_index(['date', 'portfolio'])$
pd.Timedelta('2D 3H')
print(X.shape,y.shape)$ print(X_train.shape,y_train.shape)$ print(X_test.shape,y_test.shape)
model.fit([sources], targets, epochs=10, batch_size=512, validation_split=0.1)$
df2[((df2['group'] == 'control') == (df2['landing_page'] == 'old_page')) == False].shape[0]
df = pd.read_csv('data/df_new.csv').drop('Unnamed: 0', 1)$ print df.info()$
df_reviews.stars.value_counts()
head = pd.Timestamp('20150101')$ tail = pd.Timestamp('20160101')$ df = hp.get_data(sensortype='water', head=head, tail=tail, diff=True, resample='min', unit='l/min')$
start_coord_list = station_distance['Start Coordinates'].tolist()$ end_coord_list = station_distance['End Coordinates'].tolist()
num_early_users = len(people_person[people_person['date_joined'] <= '2017-01-12']['id'].unique())$ print("The number of users signed up prior to 2017-01-12 is: ", num_early_users)
sm_model = sm.Logit(df_new['converted'] , df_new[['intercept','ab_page_UK','ab_page_CA','CA','UK','ab_page']])$ results_model=sm_model.fit()$ results_model.summary()
rule_one_below = df[df['X'] < x_chart_lcl]$ for i in range(0, rule_one_below.shape[0], 10):$     display_html(rule_one_below.iloc[i:i+10].T)
df_columns['Complaint Type'].value_counts() # Homeless$ df_columns[df_columns['Complaint Type'].str.contains('Homeless')].index.month.value_counts().sort_index().plot()$
emails_dataframe['address'].str.split("@").str.get(1)
results2 = []$ for tweet in tweepy.Cursor(api.search, q='%23puppy').items(2000):$     results2.append(tweet.text)
data.info()
final_rf_predictions = rf_v2.predict(test[:-1])
df = pd.read_csv('data/test1.csv', parse_dates=['date'])$ df
def get_num(booth_id):$     return int(mapping.ix[mapping["booth_id"] == booth_id, "num"].values[0])
pivoted.plot(legend=False, alpha=0.1)
df['launched'] = pd.to_datetime(df.launched)$ df['deadline'] = pd.to_datetime(df.deadline)$
c = df.groupby(['education', 'purpose']).agg({'applicant_id': lambda x: len(set(x))}).reset_index()$ c['education+purpose'] = c['education']+c['purpose']$ sns.barplot(data=c, x='education+purpose', y='applicant_id')
final_test_pred_nbsvm1 = test_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
plt.xlim(-0.25, len(x_axis))$ plt.ylim(-.3, .1)
products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv')$ products.head()
rf = RandomForestClassifier(n_estimators=50, max_depth=50, n_jobs=4)$ rf.fit(X = clim_train, y = size_train)
classifier = tf.estimator.Estimator($     model_fn=mobilenet_model_fn, model_dir=paths["Checkpoints"])
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country'])$ df_new = df_new.drop('US', axis = 1)$ df_new.head()$
df['HIGH_LOW']=(df['num_comments']>df['num_comments'].median()).astype(int)$ print(df.shape)$ df.head()
voters.LastVoted.value_counts(dropna=False)$
r=requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=7up7a3-xNp8abz4VJGsq&start_date=2018-07-22')
n_old = len(df2.query('landing_page != "new_page"'))$ n_old
df_temp = df_measurement[['station','tobs']].groupby(['tobs']).min()$ df_temp.head(1)$ df_temp.tail(1)
sum(df2['user_id'].duplicated())
df.to_csv('Crimes_-_2001_to_present_v2.csv', index=False)
min(df2.timestamp), max(df2.timestamp)
precipitation_df.describe()
p_diffs = np.array(p_diffs)$ (p_diffs > actual_diff).mean()
print(df_1.head())
pd.merge(transactions,transactions,how='outer',on='UserID')
colnames = ['log***','gfhdfxh']$ df.columns = colnames
secondary_temp_dat=dat[secondary_temp_columns].copy()$ secondary_temps_exist= not secondary_temp_dat.dropna(how='all').empty$
twitter_archive.loc[twitter_archive['rating_numerator'] == 1776,['tweet_id','text']]
Measurements = Base.classes.measurements$ Stations = Base.classes.stations
df.shape
pd.to_datetime(df.time.unique(), unit='s')
kick_data = k_var_state[['name','category_name','blurb','blurb_count','goal_USD','backers_count','launched_at','state_changed_at','days_to_change','state']]$ kick_data.head()
chinese_vessels.isnull().sum()
sample_text = "Hey there! This is a sample review, which happens to contain punctuations."$ print(text_process(sample_text))
points.name="WorldCup"$ points.index.name="Previous Points"$ points$
df = pd.read_csv('ab_data.csv')
tweets_clean[tweets_clean['p1_dog'] == True]['p1'].value_counts()
from scipy.stats import norm$ norm.cdf(z_score)$ norm.ppf(1-(0.05/2))
clean_sentiments = pd.DataFrame(clean_sentiments, columns = ['Target', 'Date', 'Tweet Ago', 'Compound',$                                                              'Positive', 'Negative', 'Neutral', 'Text', 'Source'])$ select_data = clean_sentiments
data = pd.concat([nyt_data, fox_data, wp_data], axis = 0)
aTL[['system.record_id','system.created_at','APP_LOGIN_ID','APP_PRODUCT_TYPE','BRANCH','AREA_MANAGER','REGIONAL_MANAGER']].to_excel(cwd + '\\CA TL 0521-0526.xlsx', index=False)$ aSL[['system.record_id','system.created_at','APP_LOGIN_ID','APP_PRODUCT_TYPE','BRANCH','AREA_MANAGER','REGIONAL_MANAGER']].to_excel(cwd + '\\CA SL 0521-0526.xlsx', index=False)
frames = [df_tw, df_tw1]$ df = pd.concat(frames)
df = pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv')$ df1 = pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv')
total_users = df2['user_id'].count()$ converted_users = df2[df2['converted'] == True]['user_id'].count()$ converted_users / total_users$
print("Unique users:", len(ab_file2.user_id.unique()))$ print("Non-unique users:", len(ab_file2)-len(ab_file2.user_id.unique()))
active_stations = session.query(func.distinct(Measurement.station), func.count(Measurement.station)).\$                 group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()$ active_stations
(taxiData2.Fare_amount == 0).any() # This Returns True
n_old = df2.query('landing_page == "old_page"').shape[0]$ n_old
donors[donors['Donor Zip'] == 606 ].head()
year11 = driver.find_elements_by_class_name('yr-button')[10]$ year11.click()
(p_diffs > (experimentp - controlp)).mean()
tweeter_url = 'https://twitter.com/marswxreport?lang=en'$ browser.visit(tweeter_url)
bar_plot = sns.barplot(x="News Organization", y="Mean Compound Score", hue="News Organization", $                        data=aggrigate_results_df, palette=news_colors)$ bar_plot.axes.set_title(f'Overall Media Sentiment, date: {today}', fontsize=14)$
print(pd.unique(stops['operator'].ravel().tolist()))
mod2 = sm.Logit(final_df['converted'], final_df[['ab_page','country_CA','country_UK','intercept']])$ fit2 = mod2.fit()$ fit2.summary()
dev = auth_time2.merge(comm_time2, left_index=True, right_index=True, how='outer')
len([earlyScn for earlyScn in SCN_BDAY_qthis.scn_age if earlyScn < 3])
for i in range(len(recent_tweets)):$     print(recent_tweets[i].created_at)
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'$ response = requests.get(url)$ response
idx = pd.IntervalIndex.from_arrays(df2.Start, df2.End, closed='both')$ idx
df_pop = df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ df_pop$
print("Mean squared error: %.2f"$       % mean_squared_error(y_test, y_pred))
df_hubs = df_avg_use.query('city != "non hub"').copy()$ df_lg_hubs = df_hubs.query('annual_avg > 500')$ df_lg_hubs
df['DAY_OF_WEEK'] = df['DATETIME'].dt.dayofweek
dd.to_csv("processed_users_verified.csv", sep=',', encoding='utf-8')$
df4[['Canada','UK','US']] = pd.get_dummies(df4['country'])
print(__doc__)$ logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
print(Airbnb_df.head(5))$         $ df_new = Airbnb_df$
df_tweet_json_clean.created_at = pd.to_datetime(df_tweet_json_clean.created_at)
stop_words = nltk.corpus.stopwords.words('portuguese')
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, ndvi_us)
Features = iris.values[:, :4]$ species = iris.values[:, 4]
print(df2['timestamp'].max())$ print(df2['timestamp'].min())
url = "https://data.wprdc.org/dataset/a8f7a1c2-7d4d-4daa-bc30-b866855f0419/resource/7794b313-33be-4a8b-bf80-41751a59b84a/download/311-codebook-request-types.xlsx"$ pgh_311_codes = pd.read_excel(url) # parse the excel sheet$ pgh_311_codes.sample(10) # pull ten random rows
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
from scipy.stats import norm$ print("cdf: {}".format(norm.cdf(z_score)))$ print("ppf: {}".format(norm.ppf(1-(0.05/2))))$
set(stories.columns) - set(stories.dropna(thresh=9, axis=1).columns)$
output_path = os.path.join(output_dir, 'prediction.csv')$ write_output(ids, ids_col, y_pred, label_col, output_path)
results.coordinates
tvec = TfidfVectorizer(stop_words='english')$ X_train_counts = tvec.fit_transform(X_train)$ X_test_counts = tvec.transform(X_test)
from collections import Counter$ c = Counter([int(stats.coleman_liau(x['cdescr'])) for x in df.to_dict(orient='records')])$
df2.to_csv("../../data/msft_modified.csv",index_label='date')
countries.country.unique()
cp311["created_date"] = pd.to_datetime(cp311['created_date'],infer_datetime_format=True)
df_ad_airings_5[df_ad_airings_5['location'].isnull()]
df_joined.groupby(['country','ab_page_new_page']).converted.mean()
(mydata / mydata.iloc[0] * 100).plot(figsize = (15, 8)); # 15 deals with the width and 8 deals with the price.$ plt.show() # this function always plots the price using matplotlib functions.$
p_sort=p_sort.rename(columns = {'Product_x':'Product'})$ p_sort.drop(['Product_y'], axis = 1, inplace = True)$ p_sort
zz = z["uri"].groupby(pandas.Grouper(freq='M')).count()$ zz.plot()
volt_prof_before=pd.read_csv('../inputs/opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder))$ volt_prof_after=pd.read_csv('../outputs/from_opendss/to_opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder))
tweet_group = tweet_df.groupby(['source']).mean().reset_index()$ tweet_group
betas_mask = np.zeros(shape=(mcmc_iters, n_bandits))$ betas_mask[np.arange(mcmc_iters), betas_argmax] = 1
train_pos = train_sample.filter(col('is_attributed')==1)$ n_pos = train_pos.count()$ print("number of positive examples:", n_pos)
df1 = pd.DataFrame(np.random.randn(6,3),columns=['col1','col2','col3'])$ df2 = pd.DataFrame(np.random.randn(2,3),columns=['col1','col2','col3'])$ print(df2.reindex_like(df1))
jcomplete_profile = json.loads(profile)[0]$ jcustomer_info = jcomplete_profile['customer']$ dfbasic = json_normalize(jcustomer_info)
norm.ppf(1-(0.05/2))
fe.bs.bootshow(256, poparr2, repeat=3)$
train['business_day'] = train.date.isin(business_days)$ train['holiday'] = train.date.isin(holidays)
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK', 'ab_page']])$ results = lm.fit()$ results.summary()
df5[['CA', 'US']] = pd.get_dummies(df5['country'])[['CA', 'US']]$ df5.head()
dataset.head()
tweet_df.count()
print(data["Ganhadores_Sena"].sum())$ print(data["Ganhadores_Quina"].sum())$ print(data["Ganhadores_Quadra"].sum())
n_new = df2.query("group == 'treatment' ").shape[0]$ n_new
building_pa_prc_zip_loc.to_csv("buildding_03.csv",index=False)$ building_pa_prc_zip_loc=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date'])
prcip_df.describe()$
df_bud = pd.read_csv(budFile, usecols = budCols, $                  dtype = bud_dtypes)
users_unique = df.user_id.nunique()$ users_unique
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)$
data['TMAX'].head()
df_ks[df_ks.location!=''].groupby('location').size().nlargest(10)
frames = [train_users, test_users]$ users = pd.concat((frames), axis=0, ignore_index=True)
ctc_alpha = ctc ** (1/3)$ ctc_alpha = ctc_alpha / ctc_alpha.max().max()
unknown_users_log = df_log[~df_log['user_id'].isin(df_users['user_id'])]
data_final['authorId'].nunique()
measure_df.to_csv("Hawaii_measurements_clean.csv", index=False, sep='\t', encoding='utf-8')$ station_df.to_csv("Hawaii_stations_clean.csv", index=False, sep='\t', encoding='utf-8')$ hawaii_df.to_csv("Hawaii_merged_clean.csv", index=False, sep='\t', encoding='utf-8')
recommendations = (model.recommendProducts(1059637, 5))$ recommendations[: 5]$ recArtist = set(list(elt[1] for elt in recommendations))$
userproduct.merge(transactions,how='outer')[['UserID','ProductID','Quantity']].fillna(0).astype('int64')
n_old = df2.query('group == "control"').count()[0]$ n_old
autos["registration_month"].value_counts().sort_index(ascending=True)
ab_df = pd.read_csv('ab_data.csv')$ ab_df.head()
X = df[['word_count','sentiment','subjectivity','domain_d','post_duration']]$ y_cm = LabelEncoder().fit_transform(df['com_label'])#Comments$ y_sc = LabelEncoder().fit_transform(df['score_label'])# Score$
most_recent_investment = pd.DataFrame(investment_dates.groupby('investor_uuid')['announced_on'].max()).reset_index()
session.query(Adultdb).filter_by(occupation="?").delete(synchronize_session='fetch')$ session.commit()
baby_scn_created['BABY_CREATED'] = pd.to_datetime(baby_scn_created['BABY_CREATED'])$ baby_scn_created['SCN_CREATED'] = pd.to_datetime(baby_scn_created['SCN_CREATED'])$
twitter_page = requests.get("https://twitter.com/marswxreport?lang=en")$ twitter_soup = BeautifulSoup(twitter_page.content, 'html.parser')$ mars_weather = twitter_soup.find('p', class_="tweet-text").string
station_count = session.query(Station).count()$ station_count
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
df.time_diff.describe()
df2.converted.mean()$ df2.describe().loc['mean'].converted
dta.query(("facility_type == 'Restaurant' and "$            "inspection_type == 'Complaint Re-Inspection'"))$
co_occurence_on_top50 = {ds: [(el[0], str(el[1])) for el in datasets_co_occurence[ds]] for ds in top50.dataset_id.tolist()}
mojog_df["unemp_rate"] = 4.1$ mojog_df.head()
df_students['passing_reading'] = df_students.apply(passing_reading, axis = 1) $ df_students['passing_math'] = df_students.apply(passing_math, axis=1)$ df_students.head()
dfg.describe()
max_change_1day = max((v.High - v.Low) for v in data.values()) $ print('=>The maximum change in a day was {:.2f} ' .format(max_change_1day))
df2['user_id'].nunique()$
fh_2 = FeatureHasher(num_features=uniques.iloc[1, 1], input_type='string', non_negative=True)$ %time fit2 = fh_2.fit_transform(train.device_id)
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_hat), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
list(r_ord.values())[2]$
submit = perf_test[['ID_CPTE']]$ submit['Default'] = log_reg_pred$ submit.head()
df.head()
temp_df2.shape
!head ../../data/msft2.csv  # Linux
!cat ProductPurchaseData.txt | head
print(commmon_intervention_train.values.shape)$ print(commmon_intervention_test.values.shape)
TempObs = pd.DataFrame(Active_TempObs, columns=["Date", "Temperature Observations"])$ TempObs.head()
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['NewWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']$ df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['LastBitWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(-1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI']$
PushEvent_payload['distinct_size'].unique()
engine.execute('SELECT * FROM measurement LIMIT 5').fetchall()
df_sched.fillna(value= '1900-01-01',inplace=True)
df_json_tweets.head()
df = pd.read_csv('ab_data.csv')$ df.head(5)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1&api_key='API_KEY$ r = requests.get(url)
df['dateCrawled'] = df['dateCrawled'].astype('datetime64')$ df['dateCreated'] = df['dateCreated'].astype('datetime64')$ df['lastSeen'] = df['lastSeen'].astype('datetime64[ns]')
date_first_booking = pd.to_datetime(df['date_first_booking'])$ print date_first_booking
weights = pd.read_csv('sd_weights.csv', low_memory=False,  nrows=1000000)$ weights.head(3)
releaseDateDataValues = releaseDateData.values$ print(releaseDateDataValues)
print 'See correlation with actual: ',test_case.select('address1').take(1)$ actual_acct_id.select('address1').distinct().show(10,False)
s.startswith('ready')$
df = DF.copy()$ df = df[24:]$ print("... created a copy and cropped odd convos from start")
image_predictions_copy['tweet_id'] = image_predictions_copy['tweet_id'].astype(str)
n_neg = n_pos * 10$ train_neg = train_sample.filter(col('is_attributed')==0).orderBy(func.rand(seed=seed)).limit(n_neg).cache()$ print("number of negative examples:", n_neg)
df2.drop(2893, inplace=True)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[np.mean([p_new, p_old]), (1-np.mean([p_new, p_old]))]).mean()$ old_page_converted
for g in my_gempro.genes:$     for s in g.protein.structures:$         g.protein.align_seqprop_to_structprop(seqprop=g.protein.representative_sequence, structprop=s)
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'game_winners'
df_example3 = rdd_example3.toDF()$ df_example3.registerTempTable("df_example3")$ print type(df_example3)
autos.head()
parts = legos['parts'].copy()$ parts.rename(columns = {'name': 'part_name'}, inplace = True)
df_clean = df.copy()$ pred_clean = pred.copy()$ tweet_df_clean = tweet_df.copy()
pop_cat = timecat_df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ pop_cat.head()
n_inc_series = 5$ Data['train'] = pd.concat([Data['train']]*n_inc_series, ignore_index=True)$ Data['train'].loc[:,'relevant_last_step'] = Data['train']['res_store_date_partial_sum-0'].apply(lambda n: randint(-39, -1))
df2.converted.mean()
df2[df2.duplicated('user_id',keep=False)]$
image_predictions.describe()
pd.merge(df1, df3, left_on="employee", right_on="name" ).drop('name', axis=1) #axis=1 is column (=0 is row)$
mp2013 = pd.period_range('1/1/2013','12/31/2013',freq='M')$ mp2013
df2.set_index('user_id').index.get_duplicates()
cityID = '629f4a26fed69cd3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Hialeah.append(tweet) 
df_countries.user_id.nunique()
page_html= uClient.read()$ uClient.close()
df['MeanFlow_cms'] = df['MeanFlow_cfs'] * 0.028316847
train_set.columns = ['tweet_id', 'tweetText', 'polarity_value', 'polarity_type', 'topic','set']
cercanasAfuerteApacheEntre25Y50mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 25) & (cercanasAfuerteApache['surface_total_in_m2'] < 50)]$ cercanasAfuerteApacheEntre25Y50mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre25Y50mts.apply(descripcionDistancia2, axis = 1)$ cercanasAfuerteApacheEntre25Y50mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
news_df = (news_df.groupby(['topic'])$               .filter(lambda x: len(x) >= 25))
def words_at_start(index, df):$     tweet_text = df["Words"][index]$     return tweet_text.split()[:3]$
Xfull = Xtrain.append(Xtest)$ yfull = ytrain.append(ytest)$ print(Xfull.shape, yfull.shape)
data_hpg_reserve['hpg_store_id'].drop_duplicates().count()
df.head(10)
new_page = df2[df2.landing_page =='new_page'].count()['user_id']$ prob_new = new_page/df2.count()['user_id']$ prob_new
print df_mk.groupby('date').size().nlargest(1), df_ks.groupby('date').size().nlargest(1)
df['CIK']=df['CIK'].map(lambda x:str(x).zfill(10))
logit_pageCountry = sm.Logit(merged['converted'],merged[['ab_page', 'intercept', 'UK', 'US']])$ result_pageCountry = logit_pageCountry.fit()
from scipy.stats import norm$ norm.cdf(z_score), norm.ppf(1-(0.05/2))$
df = pd.read_csv('data/mlb-4.1.2016-4.30.2016-2.csv', encoding='utf-8', header=None)
final_data.to_pickle('D:/CAPSTONE_NEW/jobs_data_final.pkl')$ gc.collect()
ward_df['Delay Time'].corr(ward_df['Crime Count'])
from tensorforce.agents import PPOAgent$ from tensorforce.core.networks import LayeredNetwork, layers, Network, network
londonGeoDF = gpd.read_file('OA_2011_BGC_london.json')$ londonDFSubset = londonGeoDF.ix[:,['OA11CD', 'POPDEN', 'WD11NM_BF', 'geometry']]
df_json_tweets.drop(['date_timestamp'], axis=1,inplace=True)$ df_json_tweets.info()
results = session.query(Station.station, Station.name).count()$ print(f"There are {results} stations.")
df.groupby([df.Date.dt.month, df.Date.dt.day]).count().Tweets
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/mydata.json"$ df = pd.read_json(path)$ df.head(5)
avgComp = groupedNews["Compound"].mean()$ avgComp.head()
train = pd.read_csv("../data/wikipedia_train3.csv")$ test = pd.read_csv("../data/wikipedia_test3.csv")
client.repository.delete('50017ab0-237c-451b-befe-ef435c1b5d86')
n_old = len(df2.query("group == 'control'"))$ print(n_old)
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets if tweet.lang == 'en'], columns=['Tweets'])$ display(data.head(10))
bands.sum(axis=0)$
p_diff = (new_page_converted/N_new) - (old_page_converted/N_old)$ print("The p-difference for the simulated values is: {}".format(p_diff))
train_downsampled = training.sampleBy('label', fractions={0.0: 0.135, 1.0: 1.0}, seed=123).cache()$ train_downsampled.groupby('label').count().show()$ testing.groupby('label').count().show()$
plot_LC_solar_Flare('FERMI/SolarFlares/LAT_Flares/lat_LC_20170910.fits','Flare20170910')
def fix_github_links(url):$     return url.replace('//github.com', '//raw.githubusercontent.com').replace('/master', '')
lmdict[lmdict.Positive != 0].head()
conn = pymysql.connect(host='localhost', port=3306, user='root', password='pythonetl', db='pythonetl')
df = pd.read_sql('SELECT * from payment', con=conn_b)$ df
prediction = pd.read_csv('image-predictions.tsv', sep='\t')$ prediction.head()
for i in range(10,len(myIP)):$     print table2[table2['lower_bound_ip_address']== low[low < myIP[i]].max()]['country']$
%matplotlib inline $ import matplotlib.pyplot as plt, numpy as np
from scipy.stats import norm$ norm.ppf(1-0.05)$
pipe_lr_3 = make_pipeline(tvec, lr)$ pipe_lr_3.fit(X_train, y_train)$ pipe_lr_3.score(X_test, y_test)
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
s_pacific = s_eastern.tz_convert("US/Pacific")$ s_pacific
pd.merge(df1, df3, left_on="employee", right_on="name" ).drop("name", axis=1)
cercanasAfuerteApacheEntre50Y75mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 50) & (cercanasAfuerteApache['surface_total_in_m2'] < 75)]$ cercanasAfuerteApacheEntre50Y75mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre50Y75mts.apply(descripcionDistancia2, axis = 1)$ cercanasAfuerteApacheEntre50Y75mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
df2['DATE'] = df2['DATE'].apply(lambda x: dateutil.parser.parse(x))
archive_df.head(5)
kick_projects = kick_projects.replace({'country': 'N,0"'}, {'country': 'NZERO'}, regex=True)
table_rows = driver.find_elements_by_tag_name("tbody")[20].find_elements_by_tag_name("tr")$
import pandas as pd$ data_df = pd.read_excel('data/Financial Sample.xlsx', error_bad_lines=False)$ data_df
DataAPI.write.update_secs_industry_gics(industry='A_GICSL1', trading_days=trading_days, override=False)
df3[df3['group']=='treatment'].head()
xmlData.drop('county', axis = 1, inplace = True)
watch_table = watch_table.drop_duplicates()$ print watch_table.shape
elms_all_0611.loc[range(1048575)].to_excel(cwd+'\\ELMS-DE backup\\elms_all_0611_part1.xlsx', index=False)
mask = df2.user_id.duplicated(keep=False)$ df2[mask] #found this technique on Stack Overflow - nice!?$
df1 = df1.drop_duplicates(subset = 'Title', keep = False)$
temps_df[temps_df.Missouri > 82]
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff 
df = pd.read_csv("https://raw.githubusercontent.com/YingZhang1028/practicum/master/Data_DataMining/test_score.csv")$ test_df["description_score"] = df["description_score"]$ test_df["description_score"].ix[np.isnan(test_df["description_score"]) == True] = test_df["description_score"].mean()
DummyDataframe = DummyDataframe.set_index("Date")$ DummyDataframe = DummyDataframe.apply(lambda x: update_values_category(x, "Tokens"), axis=1)$ DummyDataframe
state = environment.reset()$ state, reward, done=environment.execute(env.action_space.sample())$ state.shape
p_new = round(df2['converted'].mean(),4)$ print(p_new)
%pycat bikescore.py$
faa_data_minor_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "M"]$ print(faa_data_minor_damage_pandas.shape)$ faa_data_minor_damage_pandas.head()
OldConverted = df2.query('converted == 1')['user_id'].count() $ HNull = (OldConverted/NewTotalUser)$ print("Null Convert Rate: ",HNull)
treatment_conv_prob = df2.loc[(df2["group"] == "treatment"), "converted"].mean()$ treatment_conv_prob
for row in schemaExample.take(2):$     print row.ID, row.VAL1, row.VAL2
n = 4$ km = KMeans(n_clusters=n, random_state=123)$ km_res = km.fit(finalDf)
shows = pd.read_pickle("ismyshowcancelled_raw_pull.pkl")
url = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-06-25&end_date=2018-06-25&' + API_KEY$ r = requests.get(url)$ json_data = r.json()
train.isnull().sum()
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'$ response = requests.get(url)
df_new['country'].value_counts()
breakdown[breakdown != 0].sort_values().plot($     kind='bar', title='Russian Trolls Number of Links per Topic'$ );
df2_control = df[df.group == 'control']$ (df2_control.converted == 1).sum()/len(df2_control)
ob_diffs = df2.query('group == "treatment"').converted.mean() - df2.query('group == "control"').converted.mean()
new_page_converted = np.random.binomial(n_new,p_new)$ print('The new_page_converted is: {}.'.format(new_page_converted))
tweet_data.sort_values(['favorite_count','retweet_count'], ascending=[False, False]).head(5)$
mngr = dsdb.ConnectionManager(dsdb.LOCAL, user="jacksonb")$ local = mngr.connect(dsdb.LOCAL)$ local._deep_print()
maxData = flowerKV.reduceByKey(lambda x, y: max(float(x),float(y)))$ print (maxData.collect())$
soup.find('div', class_="poster-section left").find('img')['src']
from selenium import webdriver$ driver = webdriver.Chrome()
spark_df = sqlContext.createDataFrame(pandas_df)
df.iloc[0:4]
def zscore(series):$     return (series-series.mean())/series.std()
measurements = Base.classes.measurements$ measurements
pd.Series(master_file.axes[1])
df = pd.concat([df, sentiments_df], axis=1)$ df.to_csv('file_output\\news_mood.csv')$ df.head()
joined.dtypes.filter(items=['Frequency_score'])
len(master_df[['name', 'tweet_id']].groupby('name').filter(lambda x: len(x)==1))
plt.scatter(compound_final.index, compound_final.Compound)$ plt.show()
dow_columns=pd.get_dummies(df2['dow'])$ dow_rate=pd.DataFrame([(lambda x:(df2[x] * df2.converted).sum()/df2[x].sum()) (x) for x in dow_columns], index=list(pd.get_dummies(df2['dow']).columns), columns=['conversion_rate'])$ dow_rate
os.chdir(out_path)$ os.getcwd()
titles = soup.find_all('div', class_='content_title')   $ print(titles)
df_new = df_new.join(pd.get_dummies(df_new['country']))
funding_sum = merged_df.groupby('Funding Type')["Money Raised"].sum()$
df.user_id.nunique()
title = soup.title.text$ print(title)
b = conn.get_bucket('ev.chicagoviolencepredictor') $ b.get_all_keys()
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local')$
dataset = final_member_pivot['Percent Purchase'].values*100$ print dataset
drone_utc = drone2.set_index('drone_date_utc')
crime_count = crime_count[(crime_count.index >= '2014-01-01') & (crime_count.index <= '2016-12-31')]$ crime_count.head(10)
%matplotlib inline$ dedups.plot(y='powerPS', kind='hist')$
session.query(func.count(Station.station)).all()$
np.all(x < 10)
rf.score(X_train_total, y_train)
f = v.reset_index().rename(columns={'level_0':'ID'})$ f['ballot_type'] = f['vote'].str.extract('\((.*?)\)', expand=True).fillna('')$ f['vote'] = f['vote'].replace('\(.*\)', '', regex=True)
for col in user_df.columns:$     print col, len(user_df[user_df[col].isnull()])
all_p = all_p.rename('all_p')$ tt_final = pd.concat([tt_final,all_p], axis = 1)$ tt_final.info()
twitter_archive = pd.read_csv('twitter-archive-enhanced.csv', encoding = 'utf-8')$ twitter_archive.info()
print('number of contributions with missing candidate name: ',len(dat[dat.CAND_NAME.isnull()==True]))$ print('number of candidate ids for contributions with missing candidate name: ',len(pd.unique(dat[dat.CAND_NAME.isnull()==True].CAND_ID)))
tlen = pd.Series(data=data['len'].values, index=data['Date'])
len(df_proj.ProjectId.unique())
print bthDF.loc(axis=0)[2,:][2].values$
autos["date_crawled"].sort_index() #sort_index default is ascending, to do descending do this: sort_index(ascending=False)$
df = pd.read_csv("FuelConsumption.csv")$ df.head()$
x.mean(axis=0)
auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret)$ api = tweepy.API(auth)
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
X_test_dtm = stfvect.transform(X_test)$ X_test_dtm.shape
"From {0} to {1}".format(dataset["PubDate"].min(), dataset["PubDate"].max())
final_df_copy=final_df.copy()$ final_df=final_df._get_numeric_data()$ final_df.head()
import os$ place = os.chdir("..")$
csvData['date'] = pd.to_datetime(csvData['date'], format = "%Y%m%dT%H%M%S", errors = 'raise')
plt.axvline(obs_diff)$ plt.axvline(np.mean(p_diffs)-obs_diff);$ plt.hist(p_diffs, alpha=0.3);$
df['Mo'] = df['datetime'].map(lambda x: x.month)$ df.head()
DataSet.tail(2)
s = pd.Series([1,2,3,4,5,6,7,8,9,10])$ s
df_reader = pd.read_csv(file_wb, chunksize = 10)$ print(next(df_reader))$ print(next(df_reader))$
print(sp500['Price'].head(3))$ print(sp500[['Price','Sector']].head(3))
print(parquet_file)$ df = sqlContext.read.load(parquet_file)$ df.show()
total_stations = session.query(Stations).distinct().count()
num_closures = re.search(r'[0-9]{3}', data)$ num_closures = int(num_closures.group())$ num_closures == faa_length
feature_cols = ['TV', 'radio', 'newspaper']$ X = data[feature_cols]$ y = data.sales
autos = autos.rename({'odometer':'odometer_km'}, axis="columns")
svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf')$ svc.fit(X_tfidf, y_tfidf)$ svc.score(X_tfidf_test, y_tfidf_test)
yc_new2.describe()
a = uc.set_in_units(4.05, 'angstrom')$ box = am.Box(a=a, b=a, c=a)
results.summary()
df_full["Field4"] = df_full.Field4.fillna("None")
final_data = pd.read_pickle('D:/CAPSTONE_NEW/jobs_data_clean.pkl')
with_countries['CA_ab_page'] = with_countries['ab_page']* with_countries['CA']$ ca_new_page = sm.Logit(with_countries['converted'], with_countries[['intercept', 'ab_page', 'CA_ab_page', 'CA']]).fit()$ print(ca_new_page.summary())
plt.pie(total_ride, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()
from scipy.stats import norm$ norm.cdf(z_score)$ norm.ppf(1-(0.05))$
password = getpass(prompt='Please enter your password to log in to the database:')$ conn = psycopg2.connect('host=localhost dbname=re_ds user=tiffanydwilson password='+password+' port=5432')
syn = synapseclient.login()$ syn.login()
pgh_311_data_merged['Category'].value_counts(ascending=True).plot.barh(figsize=(10,10))
tobs_date = session.query(measurement.date, measurement.tobs).all()$ tobs_date                   
datetime.now()
n_new = len(df2.query('landing_page=="new_page"'))$ n_new
AFX = r.json()$ print(type(AFX))
data[data['processing_time']<datetime.timedelta(0,0,0)]
%%time$ model = AlternatingLeastSquares(use_gpu=True)$ model.fit(matrix_data)
label_and_pred = lrmodel.transform(testData).select('label', 'prediction')$ label_and_pred.rdd.zipWithIndex().countByKey()
df = df.loc[(df['Direction'] == 1)]
atdist_4x_positive = atdist_4x[atdist_4x['emaResponse'].isin(["Yes! This info is useful, I'm going now.", "Yes. This info is useful but I'm already going there."])]$ atdist_4x_positive.merge(atloc_4x, how='left', on=['vendorId', 'taskLocationId'])
prob_convert_t = df2_treatment.converted.mean()$ prob_convert_t
temporal_group = 'weekly'$ df = pd.read_csv('../data/historical_data_{0}.csv'.format(temporal_group))
segmentData = pd.read_excel('2017 08 take home data.xlsx')
tw_clean[tw_clean.text.str.startswith('RT @')].shape[0]
pd.read_csv('twitter_archive_master.csv').head()
yxe_tweets = pop_tweets('stoonTweets.json')
df_dateorder = df.sort_values('timestamp', ascending=True)$ df_dateorder.head(), df_dateorder.tail()
fit1.resid.hist();
df2 = df[((df.group == 'treatment') & (df.landing_page == 'new_page'))|((df.group == 'control')&(df.landing_page == 'old_page'))]
df_merge.head()
print('reduce memory')$ utils.reduce_memory(transactions)$ utils.reduce_memory(tran_time_diff)$
xmlData.set_value(296, 'zipcode', ' 98011')$ print xmlData.loc[296]
part_of_site_name = input('What are some of the letters in site name?')$ part_of_site_name = part_of_site_name.upper()$ matching = [s for s in Site_names if (part_of_site_name in s )]$
post_discover_sales[post_discover_sales['Email'] == 'Jennyann57@yahoo.com.sg']
proj_df['Project Subject Category Tree'].value_counts()
df =pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv')$ df.head(2)
btc_wallet.plot(kind='line',x='Timestamp',y='BTC_balance_GBP', grid=True);
df2['timestamp'] = pd.to_datetime(df2['timestamp'], format='%d/%b/%Y:%H:%M:%S +0000', utc=True)
df2 = df2.add_suffix(' Created')$ df4 = pd.merge(df,df2,how='left',left_on='Date Created',right_on='date Created')$
! mrec_tune -d 'splits1/u.data.train.0' --input_format tsv --l1_min 0.001 --l1_max 1.0 --l2_min 0.0001 --l2_max 1 --max_sims 200 --min_sims 1 --max_sparse 0.3
ad_source = ad_source.drop(['[', ']'], axis=1)$ ad_source = ad_source.drop(ad_source.columns[0], axis=1)
save_model('model_svm_v1.mod', svc_grid)
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
df_first_days['active_user'] = df_first_days['user_id'].apply(lambda user: 1 if user in active_users else 0)$ df_first_days = df_first_days.merge(df_user_info[['user_id', 'marketing_source']], on='user_id', how='left')
twitter_archive_clean_without_stage['stage'] = 'None'$ twitter_archive_clean_without_stage = twitter_archive_clean_without_stage.drop($     columns = ['doggo','floofer','pupper','puppo'], axis = 1)$
df_users_products.groupby(["UserID","ProductID"])["Quantity"].sum().reset_index()
ts.shift(1,freq="B")
training = dfM[dfM.YEAR <= 2015].dropna() #first row will be dropped$ holdout = dfM[dfM.YEAR == 2016].dropna()
remove_list = df_errors.tweet_id$ archive_clean = archive_clean.loc[~archive_clean['tweet_id'].isin(remove_list),:]
cercanasA1_11_14Entre100Y125mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 100) & (cercanasA1_11_14['surface_total_in_m2'] < 125)]$ cercanasA1_11_14Entre100Y125mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre100Y125mts.apply(descripcionDistancia, axis = 1)$ cercanasA1_11_14Entre100Y125mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
start_date=dt.date(2017,8,1)$ end_date=dt.date(2017,8,10)
dataframe.groupby('year').daily_worker_count.agg(['count','min','max','sum','mean'])
nnew=df2.query('landing_page=="new_page"')['converted'].shape$ print(nnew[0])
words_mention_sp = [term for term in words_sp if term.startswith('@')]$ corpus_tweets_streamed_profile.append(('mentions', len(words_mention_sp))) # update corpus comparison$ print('List and total number of mentions: ', len(set(words_mention_sp))) #, set(terms_mention_stream))
rfe = RFE(lg, n_features_to_select=None,step = 1, verbose=2 )$ rfmodel = rfe.fit(X_train,y_train)
S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
output.coalesce(2).write.parquet("/home/ubuntu/parquet/output.parquet/output.parquet")
pokemon_train = pokemon[~pokemon['Name'].isin(pokemon_test['Name'])]
df.head()
df.min()
model.get_params()
from scrapy.selector import Selector
df1.num_words.describe()$
df.describe()
df_new['country'].value_counts()$ df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA', 'US']]
X_trainset.shape$ y_trainset.shape$
plt.hist(taxiData.Trip_distance, bins = 50, range = [3, 7])
pivoted_data.resample("M").sum().plot(figsize=(10,10))
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
time6MDict = averagebytime(typesDict, '6M')$ time6MDict.keys()
xml_in_sample = xml_in[xml_in['authorName'].isin(random_authors_final)]
df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopMcMrNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TMcM_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))$ df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopPalNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TPal_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0))$
import os$ KAGGLE_USERNAME = os.environ.get("KAGGLE_USERNAME")$ print(KAGGLE_USERNAME)
plt.plot(y_validation.as_matrix()[0:50], '+', color ='blue', alpha=0.7)$ plt.plot(predictions[0:50], 'ro', color ='red', alpha=0.5)$ plt.show()
pre_name=list(twitter[twitter.name==twitter.name.str.lower()].name)$ for item in pre_name:$     t1['name'].replace(item, 'None', inplace=True)
df_new.groupby(['country','ab_page'], as_index=False).mean()
temp = temp.fillna(method='ffill')
dataframe.columns
df_final['recency'] = (now - df_final['last_trx']).dt.days
df.shape[0]
reviews.groupby('variety').price.agg([min,max]).sort_values(by=['min','max'], ascending=False)
from subprocess import call$ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
logistic = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']])$ res = logistic.fit()$ res.summary()
data.isnull().sum()
sale_average = sales.reset_index(drop=False)$ sale_average
df = pd.DataFrame()$ df
season_groups.aggregate(np.sum).sort_values(by = "Tm.3PA", ascending = False)
summary.loc['missing'] = len(records) - summary.loc['count']$ summary
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
api_copy = api_copy[pd.isnull(api_copy['retweeted_status']) == True]$ api_copy.drop('retweeted_status',axis =1 , inplace= True)
df['opening-time'] = df['date-time'].groupby(df['filename']).transform('min')$ df['closing-time'] = df['date-time'].groupby(df['filename']).transform('max')$ df['open-close-dif'] = df['closing-time'] - df['opening-time']
reviews.price.astype(str)
headers = {'X-Api-Key': '[Enter your API Key Here]'}$ r = requests.get(url1, headers= headers)   $ r.status_code
restaurants.rename(columns={"VIOLATION CODE": "VIOLATION"}, inplace = True)
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[["CA","US"]]
OR = np.exp(-1.9888-0.0150) / np.exp(-1.9888)$ print (OR)
DataSet['tweetSource'].value_counts()[:5]$
data.iloc[:,10:14] = data.iloc[:,10:14].fillna("0")  # Overall_Credit_Status, Delivery_Block, Billing_Block, Block_flag$ data.iloc[:,26] = data.iloc[:,26].fillna("0")   # state$ data.dropna(how='any',axis='rows',inplace=True) # district
ks_goal_success = ks_goal_success.sort_values(by = ['counts'], ascending = False)$ ks_goal_success.head(10)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
tweets_master.to_csv('tweets_master.csv', index = False)$ tweets_i_master.to_csv('tweets_i_master.csv', index = False)
beta_dist[np.arange(mcmc_iters), betas_argmax] *= 10000$ sum(beta_dist)/1000
from sklearn.ensemble import RandomForestClassifier
aliases = [b for b in BID_PLANS_df.index if '\n' in b]
csvData[csvData['street'].str.match('.*East.*')]['street']
df3 = df2.merge(df_countries)$ df3.head()
np.std(p_diffs)
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).head(5)
soup.find('p', attrs={'class' : 'article-location-publishdate'})
from sklearn.preprocessing import Imputer$ trainDataVecs = Imputer().fit_transform(trainDataVecs)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=zDPbq2QVaB7jFAEq5Tn6')$ print(r.json())
Desc_active_stations = session.query(Measurement.station, func.count(Measurement.prcp)).\$                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).all()$ Desc_active_stations
FREEVIEW.plot_heatmap(raw_freeview_df,raw_fix_count_df)
act_diff = prop_treat - prop_cntrl$ (p_diffs > act_diff).mean()
df_new[['ca','uk','us']] = pd.get_dummies(df_new['country'])
sortedprecip_12mo_df.describe()
live_kd915_df = pd.concat([live_df, kd915], axis = 0)
train_age = train[(train.age.notnull()) & (train.age<150)]$ sn.distplot(train_age['age'])
final_data = final_data[jobs_data['clean_titles'].isin(freq_titles['clean_titles'])]$ final_data.nunique()
mask = y_test.index$ t_flag = y_test == 0$ p_flag = pred == 1
ab_file.isnull().sum()
cityID = '01fbe706f872cb32'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Washington.append(tweet) 
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)
scolumns = inspector.get_columns('station')$ for c in scolumns:$     print(c['name'], c["type"])
df2.drop(labels=1876, axis=0, inplace=True)
df2['ab_page'] = pd.get_dummies(df['group']) ['treatment']
for i in categorical:$     train_binary[i] = train_binary[i].astype('category')
new_conv_rate = df2.query('group == "treatment"')['converted'].mean()$ new_conv_rate
new_texas_city["Measurement_date"] = pd.to_datetime(new_texas_city["Measurement_date"])
import matplotlib.pyplot as plt$ import seaborn as sns$ %matplotlib inline
print len(stations.name.unique())$ print stations.shape
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger')
qs[dtanswer.dt.total_seconds() < 60][["Id", "CreationDate", "CreationDate_a"]].head()
top_songs.to_csv('top_songs_clean.csv')
events_per_day = events_df[['event_day','event_id']].groupby('event_day').count()$ events_per_day.rename(columns={'event_id':'count_event_day'},inplace=True)$ events_per_day.reset_index(inplace=True)
pres_df['subject_count_tmp'] = pres_df['subjects'].apply(subject_count) # can use apply instead of map$ pres_df.head()
df_max = df.groupby('date').head(1)$ df_count = df.groupby(['date'] ,as_index=False).count()$ df_mean = df.groupby(['date'], as_index=False).mean()
df_crea = parse_dict['creator'].drop(['avatar','id','slug','urls'],axis=1).rename(columns={'name':'creator_name',$                                                                                  'is_registered':'creator_registered'})
Base = automap_base()$ Base.prepare(engine, reflect=True)
filepath = 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'$
stat, p, med, tbl = scipy.stats.median_test(df2["tripduration"], df3["tripduration"])$ print p$ print tbl
df[df.client_event_time < datetime.datetime(2018,4,1,23,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
pop_df.stack()
year2 = driver.find_elements_by_class_name('yr-button')[1]$ year2.click()
day_markers = np.arange(df1['DATETIME'].min(), df1['DATETIME'].max(), timedelta(days=1))$ day_markers = np.append(day_markers, df1['DATETIME'].max())$ day_markers
model_df = topics_data.drop(['body', 'comms_num', 'id', 'title'], axis=1)
new_df = df.dropna(how = 'all')$ new_df
rain_df.set_index('date').head()
df.drop(df.query("group =='treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group =='control' and landing_page == 'new_page'").index, inplace=True)$
index_missing = taxi_hourly_df.loc[(taxi_hourly_df.missing_dt == True) & $                                                     (taxi_hourly_df.num_passengers.isnull()) , :$                                                    ].index
yc_new2[yc_new2.tipPC > 100]
rounds_df = rounds[rounds.company_uuid.isin(df.uuid)].copy()$ rounds_df = rounds_df[(rounds_df.announced_year >= 1990) & (rounds_df.announced_year <= 2016)].copy()
coinbase_btc_eur_min['Timestamp'] = pd.to_datetime(coinbase_btc_eur_min['Timestamp'], format="%Y/%m/%d %H:%M")
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(z_score, p_value)
results = []$ for line in file_handle:$     results.append(line.replace('foo', 'bar'))
model.load_weights('best.hdf5')
gbm_predictions = pd.concat([pd.Series(gbm_pred, index=y_test.index, name='Predictions'), y_test], axis=1)$ gbm_predictions.head()
sns.boxplot(x='rating', y='text length', data=dataset)$
df2[df2.duplicated('user_id')]
dfs.sort_values(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True, ascending=False)$ dfs.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True)
cnx = sqlite3.connect('database.sqlite')$ df = pd.read_sql_query("SELECT * FROM Player_Attributes", cnx)$ type(df)
!wget -O ChurnData.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/ChurnData.csv
df_temp_by_station = engine.execute("select min(tobs), max(tobs), avg(tobs) from measurement where station = 'USC00519281';").fetchall()$ df_temp_by_station
firebase.post("Exhibitions/",new_data)$
df_new[['CA', 'US', 'UK']] = pd.get_dummies(df_new['country'])[['CA', 'US', 'UK']]$ df_new['country'].value_counts()
pd.crosstab(df_result.launched_year, df_result.State).plot()
sentiment_df = ave_sentiment_by_company_df.reset_index(drop = False)$ sentiment_df
csvData[csvData['street'].str.match('.*South.*')]['street']
jail_census['Race'].value_counts()['x']
file_name = str(time.strftime("%m-%d-%y")) + "-tweets.csv"$ tweet_df.to_csv("analysis/" + file_name, mode = 'w',encoding="utf-8",index = False)$ 
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
df.Date = pd.to_datetime(df.Date)
dul = pd.concat([dule2, dulp], ignore_index=True)$ dul
idx = pd.IndexSlice$ hdf.loc[idx[['adult', 'child'], ['Alcoholic Beverage', 'Choc/Cocoa Prod']], :].head()
df.groupby('key')
single_time['month']=map(lambda x:x[5:7],single_time.created)
from bs4 import BeautifulSoup$ import requests$ url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
random.shuffle(porn_ids)$ porn_bots = porn_ids[:6000]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(z_score, p_value)
sns.heatmap(ndvi_us, vmin = -.1, vmax=1)
df.data.head()
dfD.to_sql('distances', conn)
for row in mw.itertuples():$     rtp = RichTextPage(content=row[4])$     rtp.save()$
col = ['msno','plan_list_price','payment_plan_days','actual_amount_paid','payment_method_id','transaction_date',]$ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions') # 20,000,000$
df_tweets = pd.DataFrame(tweets)$ df_tweets
pd.isnull(r_forest).any(1).nonzero()[0]
back_to_h2o_frame = h2o.H2OFrame(pandas_small_frame)$ print(type(back_to_h2o_frame))$ back_to_h2o_frame
z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]),np.array([n_new,n_old]), alternative = 'larger')$ print ("z_score: "+ str(z_score) + "   p_value: "+str(p_value))
dummies = pd.get_dummies(plate_appearances['bb_type']).rename(columns=lambda x: 'bb_type_' + str(x))$ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)$ plate_appearances.drop(['bb_type'], inplace=True, axis=1)
dat['orig_T1']=dat[primary_temp_column].copy()$ primary_temp_aspirated='aspirated' in primary_temp_column.lower()
df = pd.read_csv('jamesblanchard_tweets.csv')$ print(df)
s2.mean(), s2.std(), s2.unique() # numpy functions ignore nan's
print(pandas_list_2d.head(2))
print 'Python Version: %s' % (sys.version.split('|')[0])$ hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER$ print 'HDFS filesystem running at: \n\t %s' % (hdfs_conf[0])
import csv$ data_with_header = list(csv.reader(open("test_data//askreddit_2015.csv", encoding="utf8")))$ data = data_with_header[1:len(data_with_header)]
unique_desc_reg=class_merged_hol['description_regional_hol'].unique()$ print (unique_desc_reg)
from IPython.display import Image$ Image(url='http://talhaoz.com/wp-content/uploads/2015/03/Modularity_Labeled.png')
celtics.reset_index(drop=False, inplace=True)$ celtics.rename(columns={'index':'game_id'}, inplace=True)
bob_shopping_cart = pd.DataFrame(items, columns=['Bob'])$ bob_shopping_cart
new_df = cs.apply(lambda x : np.min(x[cols_select]), axis = 1)$           $ new_df.head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_table(path, sep ='\s+', na_values=['.'], names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'class'])$ mydata.head(5)$
df.isnull().sum()
json_data['dataset'].keys()
describe_precipitation = df['precipitation'].describe()$ describe_precipitation
X_future = sandag_df.values
my_columns = list(Users_first_tran.columns)$ print(my_columns)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]$
Measurement = Base.classes.measurement$ Station = Base.classes.station
df['duration'].mean()
rain_df.describe()$
twitter_data.info()$
le_data_all = wb.download(indicator="SP.DYN.LE00.IN",country=countries['iso2c'],start='1980',end='2012')$ le_data_all
model.predict_proba(np.array([0,0,1,0,0,0,0,0,30,50,1100]))
page.exists()
tweetsOverall.head()
state_lookup.info()
today = pd.to_datetime("Today")$ today
np.exp(results.params)
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
obs_diff = (df2.query('landing_page == "new_page"').converted.mean() -$             df2.query('landing_page == "old_page"').converted.mean())$ obs_diff
cityID = '00ae272d6d0d28fe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lexington_Fayette.append(tweet) 
index = pd.date_range('2018-3-1', periods=1000)$ index
! rm -rf models1$ ! mrec_train -n4 --input_format tsv --train "splits1/u.data.train.*" --outdir models1 --model=popularity
s.index[[2,3]]
pd.date_range('2015-07-03', periods=8, freq='H')
merged.isnull().sum()
pred = model.predict(X_test)
daily_returns=portfolio_func.calc_daily_returns(closes)$ huber = sm.robust.scale.Huber()$ returns_av, scale = huber(daily_returns)
df_final = df_final.replace('nan', '')
df.index[0]
df.info()
funding_type = merged_df["Funding Type"].value_counts()$ funding_type
df_l = pd.merge(df_l,df_s.rename(columns={'callsign': 'callsign_id', 'major_cruise_line': 'major_cruise_line_id'}), left_on='ship_name', right_on='name')$
df = pd.read_csv('data/goog.csv', parse_dates=['Date'])$ df
dc = datacube.Datacube(app='dc-nbart')
crypto_data.head(5)
foursquare_data_dict['response'].items()[2][1][0]
news_df = pd.DataFrame(average_sentiment_list).set_index("User").round(3)$ news_df.head()
df2.head(5)
engine = create_engine('mysql+mysqldb://spmb:appoloodatavag@cw-spmb.ct4csmrmjrt7.us-west-2.rds.amazonaws.com/spmb_101')$
sortHighThenVol = AAPL.sort_values(by=["high", "volume"], ascending=False)$ sortHighThenVol.head()
new_page_converted=np.random.binomial(n_new,p_new)
np.exp(-0.0150)
log_mod = sm.Logit(df_new['converted'], df_new[['intercept','treatment','CA', 'UK']])$ results = log_mod.fit()$ results.summary()
from helper_code import mlplots as ml$ ml.confusion(y_test.reshape(y_test.shape[0]), $              predicted, ['No Failure', 'Failure'], 2, 'O-Ring Thermal Distress')
sl[sl.status_binary==0][(sl.today_preds==1)].shape[0]*.57
temp = we_rate_dogs.name.str.contains('^[a-z]')$ we_rate_dogs[temp].name.head()
for x in prop.columns:$     print (x,prop[x].isnull().sum())
delay_delta['delay'].isnull().value_counts()
newdf = pd.DataFrame(taxiData.Trip_distance)
future = m.make_future_dataframe(periods=90)$ future.tail()
import seaborn as sns$ import matplotlib.pyplot as plt$ %matplotlib inline
cleaned_df = split_df.na.fill({'content_size': 0})$ exprs = [count_null(col_name) for col_name in cleaned_df.columns]$ cleaned_df.agg(*exprs).show()
plt.title('Burberry ngram', fontsize=18)$ burberry_ng.plot(kind='barh', figsize=(20,16));$ plt.savefig('../visuals/Burberry_ngram.jpg')
cwd = os.getcwd()$ cwd
root_universe = openmc.Universe(name='root universe', cells=[cell])
df_user[df_user['user.name'].str.contains('marco rubio', case=False)]
print(datetime.now() - timedelta(hours=1))$ print(datetime.now() - timedelta(days=3))$ print(datetime.now() + timedelta(days=368, seconds=2))
result_control_2.summary()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()$
reorder_customers = np.fromiter(result.keys(), dtype=int)$ reorder_customers.size
scores['test_accuracy'].mean()$
df_user_product_ids=pd.merge(left=df_userIds,right=df_productIds,how="outer",on="Key")[["UserID","ProductID"]]
for v in d.variables:$     print(v)
tweets_master_df.iloc[tweets_master_df['retweet_count'].nlargest(10).index, :]
(df2['landing_page'] == 'new_page').sum()/unique_users_2$
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
test_data = pd.read_csv("test.csv")$ print test_data.head()$ print test_data.count()
compound_df.plot(kind='scatter', x='index', y='@BBCWorld', subplots=False)$ plt.show()
from sqlalchemy import create_engine$ engine = create_engine('mysql+pymysql://root:kobi5555@0.0.0.0/proddb')
data['inday_icu_wkd'] = np.where(data.intime_icu.dt.weekday <= 4, $                                  'weekday','weekend')$ data['inday_icu_wkd'].value_counts()
!curl -L -O  https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt
week_ny['count'].groupby(week_ny['week']).apply(shannon)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
bird_data['timestamp'] = pd.Series(timestamps, index = bird_data.index)$ print(bird_data.timestamp.head())
df3[df3['STATION']=='1 AVE'].pivot_table(index = 'DAY', columns = 'WEEK', values = 'INCR_ENTRIES').plot(figsize=(10,3))
news_sentiments.to_csv('New_Source_sentiments.csv', encoding='utf-8', index=False)
new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs$ new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))$ new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)$
plt.hist(p_diffs);$ plt.axvline(actual_diff, color = 'red');
conv_users=df.query('converted==1').user_id.nunique()$ conv_users_prop=conv_users/df['user_id'].nunique()$ conv_users_prop
df2[df2.user_id.duplicated()]
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))$
model_x = sm.formula.ols('y ~ C(x)', data = df).fit()$ anova_x_table = sm.stats.anova_lm(model_x, typ = 1)$ anova_x_table.round(3)
masked['user_age_days'] = [ele.days for ele in masked['user_age']]$ masked.head()
df4 = df3.dropna(axis=0, inplace=False)$ df4.head()
search['one_way'] = search.apply(lambda x: 0 if x['trip_end_loc'] == x['trip_start_loc'] else 1, axis=1)
jobs_data2 = json_normalize(json_data2['page'])$ jobs_data2.head(5)
reviews.groupby('price').points.max().sort_index()
transactions.merge(users, how='outer', on='UserID')
split_pct=0.75$ X_train, y_train, X_test, y_test, scaler = train_test_split(df, split_pct=split_pct, scale_data=True)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ request = requests.get(url)
sns.boxplot(x=df['liked'],y=df['Age_Years'],data=df,whis=np.inf)$
rain_df = pd.DataFrame(rain_result)$ rain_df.head()$ rain_df.set_index('date').head()
df_countries.user_id.nunique()
delimited_hourly.reset_index(inplace=True)$ delimited_hourly.head()
btc['2017-09-12':'2017-09-22'].plot(y='price')$ plt.show()
df = pd.read_excel(workbook_name, sheetname=1)
clean_users = users.drop(['user_id','name','email'], axis = 1)$ clean_users['account_life'] =  clean_users['last_session_creation_time'] -clean_users['creation_time']
print("Probability of treatment group converting:", $       ab_file2[ab_file2['group']=='treatment']['converted'].mean())
r= requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json")$ json_data = r.json()$ json_data["dataset_data"]["data"][0]
X_new.shape$
df2['converted'].mean()
price_outliers = autos.loc[expensive, :].index
sorted.select('count').toPandas().hist(bins=15)
sentiment.to_csv("sentiment_by_vader.csv", index=False, header=True)$ watson_df.to_csv("sentiment_by_watson.csv",index=False,header=True)$  $
counts = Counter(l_hashtags)$ df = pd.DataFrame(counts.most_common(20), columns=['Hashtag', 'Count'])$ df.to_csv('hashtag_counts.csv')
df_valid = pd.read_csv('/home/bmcfee/data/cc_tracks.csv.gz', usecols=[0], nrows=1000000)['track_id']$ df_valid = df_valid.apply(lambda x: '{:06d}'.format(x))
custom = pd.get_dummies(auto_new.Custom)$ custom.head()
import name_entity_recognition as ner$ ner.load_data()
dateCounts = pd.DataFrame(all_dates.value_counts().reset_index())$ dateCounts.columns = ['dates','countsOnDate']$ dateCounts.head()
feat = ['categoryname', 'eventname', 'location']$ for f in feat:$     PYR[f] = PYR[f].apply(clean_dataset)
shelter_df_only_idx = shelter_df_idx$ shelter_df_only_idx = shelter_df_only_idx.drop('OutcomeType', 'AnimalType', 'SexuponOutcome', 'Breed', 'Color')$
plt.hist(new_page_converted);
print(list(cos.buckets.all()))
cityID = '00ab941b685334e3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Nashville.append(tweet) 
model.create_timeseries(scenario)$ model.close_db()
S_1dRichards.decision_obj.thCondSoil.options, S_1dRichards.decision_obj.thCondSoil.value
c.index = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j"]$ c
print("Times new_page and treatment don't line up is {}".format(len(gr_e1) + len(gr_e2)))
Base.classes.keys()
from sklearn.naive_bayes import GaussianNB
sum(df2['user_id'].duplicated())
len(df['user_id'].unique())
from statsmodels.tsa.stattools import adfuller as ADF$
dict_wells_df_and_Nofeatures_20180707 = dict_of_well_df$ pickle.dump(dict_wells_df_and_Nofeatures_20180707, open( "dict_of__wells_df_No_features_class3_20180707.p", "wb" ) )
jsonUrl = "s3a://DH-DEV-PROMETHEUS-BACKUP/prometheus-openshift-devops-monitor.1b7d.free-stg.openshiftapps.com/"+metric_name+"/"$ jsonFile = sqlContext.read.option("multiline", True).option("mode", "PERMISSIVE").json(jsonUrl)
ngrams_summaries = cvec_3.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
df['duration'].std()
iowa['state_bottle_cost'] = iowa['state_bottle_cost'].str.replace('$', '').astype('float64')$ iowa['state_bottle_retail'] = iowa['state_bottle_retail'].str.replace('$', '').astype('float64')$ iowa['sale_dollars'] = iowa['sale_dollars'].str.replace('$', '').astype('float64')
print(players.shape)$ players.head()
df = pd.read_csv('ab_data.csv')$ df.head()
df.columns=['created_at','id_str','in_reply_to_user_id_str','raw_text']$ df = df.drop(columns='created_at')
np.sqrt(metrics.mean_squared_error(y_val, preds))$
ratings=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\ratings.csv')
leadPerMonth = segmentData[['opportunity_month_year', 'lead_source']].pivot_table($                                 index='opportunity_month_year',$                                 columns='lead_source', aggfunc=len)
df_with_predictions = pd.DataFrame(np.c_[X_test, y_test, y_pred])$ df_with_predictions.columns = list(acs_df.columns) + ['homeval_pred']$ df_with_predictions.head()
df_ad_airings_5.to_pickle('./TV_AD_AIRINGS_NO_NAN_LOC_DATASET_4.pkl')
grid.cv_results_['mean_test_score']
logit_countries = sm.Logit(newset['converted'], $                            newset[['country_UK', 'country_US', 'intercept']])$ result_new = logit_countries.fit()
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep$ lee_train_file = test_data_dir + 'lee_background.cor'
df_license_opening.head(2)
recipes.iloc[135598]['ingredients']
df_twitter_copy['retweet_count'] = df_twitter_copy['tweet_id'].map(df_twitter_extract_copy.set_index('tweet_id')['retweet_count'])$ df_twitter_copy['favorite_count'] = df_twitter_copy['tweet_id'].map(df_twitter_extract_copy.set_index('tweet_id')['favorite_count'])
df.head()
y_pred = logreg.predict(X_train)$ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_train, Y_train)))
cp = nltk.RegexpParser(grammar)$ result = cp.parse(sentence)$ print(result)
new_page_converted = np.random.binomial(1, p_new,n_new)$ p_new_sample = new_page_converted.mean()$ p_new_sample$
new_stops = merged_stops[['stopid', 'stop_name', 'lat', 'lng', 'routes']]$ new_stops = new_stops.rename(columns={'stop_name': 'address'})$ new_stops.head(5)
def create_soup(x):$     return ''.join(x['categoryname']) + ', ' + ''.join(x['eventname']) + ', ' + ''.join(x['location'])
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
miner = TweetMiner(twitter_keys, api, result_limit=400)$ trump_tweets = miner.mine_user_tweets("realDonaldTrump")
three = df.pop('three')
csv_07_2015$ csv_07_2015_aux = csv_07_2015.rename(columns = {'place_with_parent_names':'state_name','surface_in_m2':'surface_total_in_m2'})$ csv_07_2015_aux.keys()$
image_clean.drop(['p1','p1_conf','p1_dog','p2','p2_conf','p2_dog','p3','p3_conf','p3_dog'],axis=1,inplace=True)
top_10_authors = git_log.author.value_counts().head(10)$ top_10_authors
year_with_most_commits = ... 
dataCapGba = data.loc[(data.state_name.str.contains('Capital Federal') | data.state_name.str.contains('G.B.A') ), ['created_on','operation', 'property_type', 'state_name', 'place_name', 'lat', 'lon', 'price', 'surface_total_in_m2', 'surface_covered_in_m2', 'price_usd_per_m2', 'price_per_m2', 'floor', 'rooms', 'expenses', 'description']]
last_year = dt.date(2018, 6, 2) - dt.timedelta(days=365)$ print(last_year)
statistics_table = win_rates_table.merge(pick_rates_table, left_index=True, right_index=True)$ statistics_table.head()
df_ad_airings_5.dropna(subset= ['location'], inplace=True)
df3 = df2.merge(c, on ='user_id', how='left')$ df3.head()
df2['converted'].mean()
predict = 0.5
df_clean.head()
base_df.head()
x = api.GetUserTimeline(screen_name="HillaryClinton", count=20, include_rts=False)$ x = [_.AsDict() for _ in x]
df = pd.DataFrame.from_dict(tweet_info)$ df.head()
le.classes_
corr = newMergedDF.corr().fillna(0)$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
p_old = df2[df2['landing_page']=='old_page']['converted'].mean()$ print("The probability of conversion for the old page is: " + str(p_old))
%matplotlib inline$ import matplotlib.pyplot as plt
print(y_test.mean(), y_train.mean())
top_10_authors = git_log['author'].value_counts().head(10)$ top_10_authors
output_SVM = model_SVM.predict(test[:, 1:5])$ rowID_SVM = [TEST.rowID for TEST in test_data.itertuples()]$ result_df_SVM = pandas.DataFrame({"rowID": rowID,"cOPN": list(output_SVM)})
my_gempro.uniprot_mapping_and_metadata(model_gene_source='ENSEMBLGENOME_ID')$ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping)$ my_gempro.df_uniprot_metadata.head()
time_dup = "2017-01-09 05:37:58.781806"$ df2 = df2[df2.timestamp != time_dup]
crypto_combined = pd.concat([crypto_data, crypto_ggtrends], axis=1).dropna(how='any')   ### Remove NaN $ crypto_combined_s = crypto_combined.copy(deep=True)$ print(crypto_combined_s.head(5))
from sqlalchemy import create_engine$ engine = create_engine('postgresql+psycopg2://postgres:admin@localhost:5432/DTML')$ X_copy.to_sql('dtml_featr_num', engine, if_exists='append')
media_classes = [c for c in df_os.columns if c not in ['domain', 'notes']]$ media_classes
df_expand = pd.DataFrame()
dates = pd.date_range('19740101', periods=6)$ df = pd.DataFrame(np.random.randn(6, 4), index = dates, columns = list('ABCD'))$ df
data = pd.read_csv('./fake_company.csv')$ data
df2[df2.duplicated('user_id')]['user_id']$
USvideos = pd.read_csv('data/USvideos.csv', parse_dates=['trending_date', 'publish_time'])
movies_df.loc[movies_df['movieId'].isin(recommendationTable_df.head(20).keys())]
scipy.stats.kruskal(df3["tripduration"], df4["tripduration"])
activity = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\$ filter(Measurement.station == Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()$ activity
!grep -A 15 "add_engineered(" taxifare/trainer/model.py
p_value = scipy.stats.chi2_contingency(full_contingency)[1]$ print(p_value)
df2['intercept'] = 1$ df2[['control','treatment']] = pd.get_dummies(df2['group'])
multi_var = sm.OLS(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'UK']])$ multi_var_results = multi_var.fit()$ multi_var_results.summary()
measurements_df = measurements_df.dropna(how='any')
data = pd.read_csv('dog_rates_tweets.csv', parse_dates=[1])$
precipitation_stations = session.query(Station.station).distinct().all() $ print(str(len(precipitation_stations)))
df2['intercept'] = 1$ df2.head()
item_lookup = shopify_full_size_only[['child_sku', 'child_name']].drop_duplicates() # Only get unique item/description pairs$ item_lookup['child_sku'] = item_lookup['child_sku'].astype(str) # Encode as strings for future lookup ease
import time$ ctime = obj['openTime']/1000$ time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ctime))
fin_r = fin_r.reindex(df.resample('D').index, method='ffill') #df.resample('D').index$ assert (fin_r.index == r_top10_mat.index).all()
year_to_date = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(year_to_date)
dataframe.drop(['Date'],inplace=True,axis=1)$ scaler = MinMaxScaler(feature_range=(0, 1))$ scaled = scaler.fit_transform(dataframe)
augur.GitHubAPI.code_reviews = code_reviews$ ld = github.code_reviews('rails', 'rails')$
@app.route("/api/v1.0/stations")$ def stations():$     return jsonify(session.query(hi_stations.STATION.distinct()).all())
df.info()
cross_val_score(pipe, X, y, cv=5, scoring='roc_auc').mean()
importances=model_rf_14_b.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
store_items.dropna(axis = 0)
df2[df2.duplicated(['user_id'], keep=False)]['user_id']$
events_enriched_df = pd.merge(events_df[['event_id','group_id','yes_rsvp_count','waitlist_count','event_time']]$                               ,groups_topics_unique_df[['group_id','topic_name','topic_id']]$                               ,on ='group_id' , how='left' )
openmc_geometry = openmc.Geometry(root_universe)$ openmc_geometry.export_to_xml()
!hdfs dfs -put CC_records.csv {HDFS_DIR}/Consumer_Complaints.csv
NYT = news_df.loc[(news_df["Source Account"] == "nytimes")]$ NYT.head(2)
zipcodes_listings = listings['mainzip'].value_counts()$ zipcodes_listings.plot()
news_sentiments = pd.DataFrame(sentiments_df, columns= ["News Source", "Date/Time", "Compound","Positive",$                                                         "Neutral", "Negative", "Tweet", "Tweets Ago"])$ news_sentiments.head()
results_lumpedTopmodel, output_LT = S_lumpedTopmodel.execute(run_suffix="lumpedTopmodel_hs", run_option = 'local')
df.dtypes
df.replace('\n', ' ', inplace=True, regex=True)$ df.lic_date = pd.to_datetime(df.lic_date, errors='coerce')$ df = df.sort_values('lic_date', ascending=False)
session.query(Measurement.station,func.count(Measurement.station))\$       .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
np.any(x > 8)
sns.heatmap(shows.corr())
df_img_algo.head()
rng=pd.period_range('1/1/2000', '6/30/2000', freq='M')$ rng
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'$ r = requests.get(url)
All_tweet_data_v2.retweeted_status_timestamp=All_tweet_data_v2.retweeted_status_timestamp.str.replace(' \+0000','')$ All_tweet_data_v2.retweeted_status_timestamp$ All_tweet_data_v2.retweeted_status_timestamp.dropna().head()
df_arch.info()$
df_ad_airings = pd.read_csv('./data/TV_AD_AIRINGS_ENTIRE_DATASET.csv', parse_dates=['start_time','end_time','date_created'])$ df_ad_airings.head(5)
raw = pd.read_json('data/raw_data.json') #not needed$ object_check = raw['object_id'][0]
logodds.drop_duplicates().sort_values(by=['count']).head(10)$ logodds.drop_duplicates().sort_values(by=['count']).tail(10)$
station_count = df['station'].count()$ station_count
mask = stops['stopid'].isin(shared_ids)
X_know.shape
if not os.path.exists('new_data_files'):$     os.mkdir('new_data_files')$ records3.to_csv('new_data_files/Q3B.csv')
df.converted.mean()
page_interaction_log = sm.Logit(df4['converted'], df4[['ab_page', 'country_UK', 'country_US', 'intercept']])$ page_interaction_result = page_interaction_log.fit()
users_converted = df['converted'].mean()*100$ print("The proportion of users converted - {}%".format(round(users_converted,2)))
demand.loc[:,'value'] = demand.loc[:,'value'].copy() *1.5$ demand.head()
data = res.json()   
results_list = []$ for y in results:$     results_list.append(y)
boros = ["".join(boro.lower().split()) for boro in boros]
from scipy.stats import norm$ norm.cdf(z_score)$
df2.columns
url = 'https://twitter.com/marswxreport?lang=en'$ response = requests.get(url)$ soup = bs(response.text, 'lxml')
logit = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','UK','US']])$ result=logit.fit()
data.dropna(axis=1)
crimes.drop('LOCATION', axis=1, inplace=True)
json_data = r.json()$ print(type(json_data))$
joined[['Frequency_score']] = joined[['Frequency_score']].astype(int)$ joined.dtypes.filter(items=['Frequency_score'])
scratch['created_at'] = pd.to_datetime(scratch['created_at'], coerce=True)
countries_df = pd.read_csv('./countries.csv')$ df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
p=np.mean([p_old,p_new])$ p
ual = df1['cleantime'].quantile(q=0.75) + (iqr* 1.5)$ ual
fashion[fashion.index == 'gucci'].sort_values("PRADA-proba", ascending=False).head(10)
Genres=",".join(Genres).join(("",""))
month = dd.read_csv('training/yellow_tripdata_2015-01.csv')$ print(month.columns)
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'$ response = requests.get(url)$ response
df['DATETIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'])
year_dict = year_request.json()$ print (type(year_dict))
reddit_master['Class_comments'].value_counts()/reddit_master.shape[0]
review_df.date = pd.to_datetime(review_df.date)$ review_df=review_df[review_df['date']>='2017-01-01']
query_date_2yearsago = dt.date.today() - dt.timedelta(days = 730)$ print(query_date_2yearsago)
df["Diff"] = df["Close"] - df["Open"]$ df["Percent_Diff"] = (df["Diff"]/df["Open"])*100$ df.head()
df = df[df.launched_year != 1970]
merged_portfolio_sp_latest_YTD = pd.merge(merged_portfolio_sp_latest, adj_close_start, on='Ticker')$ merged_portfolio_sp_latest_YTD.head()
engine = create_engine('sqlite:///hawaii.sqlite')
df_day=df[(df["day"] == 1)]$ df_night=df[(df["day"] == 0)]$ scipy.stats.ks_2samp(df_day["tripduration"],df_night["tripduration"])
merged1.isnull().sum()
df = df.withColumn('city', lit(city)) $ hz.addTransformDescr('city','"city" assigned by harmonization code')$ print("Add 'city' variable")
prob_converted = df2.converted.mean()$ print (prob_converted)
pres_df.drop('split_location_tmp', inplace=True, axis=1)$ pres_df.head(2)
train_test['price'].ix[train_test['price']>13000] = 13000
df['y'].plot.hist()
MFThermalCorrDF=pd.read_pickle('MFThermalCorrDF.p')
stations = session.query(Station.station).count()$ print(f"There are {stations} stations.")
list(pd.read_csv(projFile, nrows=1).columns), list(pd.read_csv(schedFile, nrows=1).columns), list(pd.read_csv(budFile, nrows=1).columns)
word_centroid_map = dict(zip(model.wv.index2word, idx))
shows = pd.DataFrame(data={'title':titles,'status':statuses,'years':years,'network':networks,'genre':genres,\$                           'tagline':taglines,'link':links})
df.resample('H').mean()
os.getcwd()$ model_path = 'task1_pm_model.h5'
my_tweet_df.count()
(diffs_evening>obser_eve).mean()
total_num_stations = session.query(func.count(Station.station)).first()$ print(f"Total number of stations: {str(total_num_stations[0])}")
df['time_online'] = (df.lastSeen - df.dateCreated).apply(lambda x: int(x.days))
miner = TweetMiner(api, result_limit=200)$ trump_tweets = miner.mine_user_tweets("realDonaldTrump", max_pages=14)
def feature_derivative(errors, feature):$     derivative = 2 * np.dot(errors, feature)$     return(derivative)
df2['tripDay'] = df2['tripduration'][(df2['DN'] == "D")]$ df2['tripNight'] = df2['tripduration'][(df2['DN'] == "N")]$ df2.head()
s.index('go')
rng = pd.date_range('1/1/2018', periods=120, freq='S')$ len(rng)
ab_df2.user_id.nunique()
print("Data Types: \n%s" % (excel_data.dtypes))
cnct = pd.Series(df.Title.values,index=df.index).to_dict()$ EEdgeDF['From'] = EEdgeDF['From'].map(cnct)$ EEdgeDF.head(7)
df2.drop_duplicates('user_id',inplace=True)
noHandReliableData.shape
kick_projects_ip_scaled_ftrs = pd.DataFrame(preprocessing.normalize(kick_projects_ip[features]))$ kick_projects_ip_scaled_ftrs.columns=list(kick_projects_ip[features])
print pd.concat([s1, s2, s3], axis=1)
average_for_each_calendar_month = s.resample('M').mean() #Calculating the avarage of each calender month in series 's'$ print(average_for_each_calendar_month) #Printing the average of values in each calender month in series 's'
countries = pd.read_csv('countries.csv')$ countries.head()
dataset = member_pivot['Percent Purchase']*100$ print dataset.values
output= "Update user SET following=50 where user_id='@Pratik'"$ cursor.execute(output)$
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
for row in session.query(Measurements).limit(5).all():$     print(row)
training_data,holdout = train_test_split(lq2015_combined,test_size=0.10,random_state=123)
pd.read_sql(q, connection) # Avergae working hours of Private Sector Men
df.groupby('category')['position','hourly_rate','num_completed_tasks'].agg({'median','mean','min','max'}).reset_index() \$     .rename(columns={'category': 'category', 'position': 'position_stats_overall','hourly_rate':'hourly_rate_stats_overall', \$                  'num_completed_tasks':'num_completed_tasks_stats_overall'})
greaterpropdiff = p_diffs > act_diff$ greaterpropdiff.mean()
df_uv = df.query('landing_page != "new_page"') $ df_vu = df_uv.query('group == "treatment"')$ df_vu.count()$
df2['intercept'] = 1$ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])$ df2.head()
plt.hist(p_diffs)$ plt.axvline(obs_diff, color='r')
df_raw[df_raw.list_date.isnull()]
plt.savefig(str(output_folder)+'NB01_5_NDVI02_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice02_str)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', header=None)$ mydata.head(5)
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA', 'US']]$ df_new.head()
df2['Geocoded Location'] = df2['Geocoded Location'].astype(str)$ df2['Geocoded Location'] = df2['Geocoded Location'].str.extract('([a-zA-Z ]+)', expand=False).str.strip()
assert(tw[tw.text.str.startswith('RT @')].shape[0] == sum(tw.retweeted_status_id.notnull()))
logr = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK', 'ab_page']])$ result = logr.fit()$ result.summary()
plot_BIC_AR_model(data=doc_duration.diff()[1:], max_order_plus_one=10)
tweet_info = pd.DataFrame()
station_activity = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\$ filter(Measurement.station == Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()
qt_ind_ARR[industries] = qt_ind_ARR[industries].applymap(lambda x: np.float(x))$ qt_ind_ARR
locations = session.query(Measurement).group_by(Measurement.station).count()$ print(f"There are",locations,"stations.")
!head -n 5 ProductPurchaseData.txt
df.isnull().values.any()
def yearly_data_csv_writer(start_year, end_year, all_data, path='./data/', name='surveys'):$     for year in range(start_year, end_year+1):$         one_year_csv_writer(year, all_data, path, name)
sent_groups_pivot_df = sent_groups_df.pivot(index="Sentiment group", columns="Twitter account", values="Group total")$ sent_groups_pivot_df
twitter_archive[twitter_archive.text.str.contains(r"(\d+\.\d*\/\d+)")]$
cont_probability = df2[df2['group'] == 'control']['converted'].mean()$ cont_probability
autos["price"]= autos["price"].astype(int)  $
pantab.frame_to_hyper(active_with_return, 'Avtive User Analysis.hyper')
sl[sl.status_binary==0][(sl.today_preds==1)].shape
transactions.merge(users, how='outer', on=['UserID'])
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']])$ results=lm.fit()$ results.summary()
rng = pd.date_range(start='6/1/2017', end = '6/30/2017', freq='B')$ rng$
df.groupby('label').count().show()$
lm3 = sm.Logit(ab_df_new['converted'], ab_df_new[['intercept','CA','UK','treatment_US','treatment_CA']])$ result3 = lm3.fit()$ result3.summary2()
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
data= pd.concat([btypedums, data], axis=1)
print(df.shape)
pd.merge(cust_data, cust_demo, how='left', left_on='ID', right_on='ID').head(3)
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
t_cont_prob = df2[df2['group']=='treatment']['converted'].mean() * 100$ output = round(t_cont_prob, 2)$ print("The probability of the treatment group individual converting regardless of the page they receive is: {}%".format(output))
    df.to_csv(path,sep=',',index=False)
Base.classes.keys()
!git clone https://github.com/u110/WallClassification
trimmedDataFrame.ID.unique()
test[['clean_text','user_id','predict']][test['user_id']==5563089830].shape[0]
lm=sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results=lm.fit()$
name_df.sum(axis=0).sort_values(ascending=False).head(20)
countries_df = pd.read_csv('./countries.csv')$ countries_df.head()
xml_in.dropna(subset = ['venueName', 'publicationKey', 'publicationDate'], inplace = True) $
re.sub(rpt_regex, rpt_repl, "Reppppeated characters in wordsssssssss" )
excelDF[["Sales","Region"]].head()$
df2[df2.duplicated('user_id')]$
RNPA_existing_8_to_16wk_arima = RNPA_existing_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ RNPA_existing_8_to_16wk_arima.index = RNPA_existing_8_to_16wk_arima.index.date
type(ts.index)
(p_diffs > obs_diff).mean()
df['range'] = (df.max(axis='columns') - df.min(axis='columns'))
df3['ab_page'] = pd.get_dummies(df3.group)['treatment']
cols = ['GP', 'GS', 'MINS', 'G', 'A', 'SHTS', 'SOG', 'GWG', 'HmG', 'RdG', \$         'G/90min', 'SC%', 'Year', 'PKG', 'PKA']$ goals_df[cols] = goals_df[cols].apply(pd.to_numeric)
beirut.dtypes
import statsmodels.api as sm$ model=sm.Logit(df2['converted'],df2[['intercept','treatment']])$ result=model.fit()
Muliple_columns = data_set_1[["indicator_id","country","year","who_region","publish_states"]]$ print (Muliple_columns.head()) # Extracted Columns and printed over First 5 Records
df.isnull().sum()
countries_df = pd.read_csv('countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
print data.describe()
last_12_precip_df = pd.DataFrame(last_12_precip, columns=['date', 'precipitation'])$ last_12_precip_df.head()
fraq_volume_m_coins = volume_m.div(volume_m.sum(axis=1), axis=0)
n_old = df2[df2['landing_page']=="old_page"].count()[0]$ n_old
cleanedData[cleanedData['text'].str.contains("&amp;")].text
url = 'https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2015-01-01&end_date=2015-01-01&api_key='$
df['score'] = df['raw'].str.extract('(\d\d\d\d\.\d)', expand=True)$ df['score']
train_df = pd.read_csv(slowdata + 'train.csv', index_col='id', parse_dates=['date'], dtype=dtypes)$ save_n_load_df(train_df, "training_dev.pkl")$ train_df.name = "TRAINING"
topLikes = data[(data.Likes >= data.Likes.max()-1200)]$ topLikes$
df2_copy['intercept'] = 1$ df2_copy[['landing_page_new', 'landing_page_old']] = pd.get_dummies(df2_copy['landing_page'])$ df2_copy[['ab_page_control','ab_page_treatment']] = pd.get_dummies(df2_copy['group'])
pgh_311_data_merged['CREATED_ON'] = pd.to_datetime(pgh_311_data_merged['CREATED_ON'])$ pgh_311_data_merged.info()
Base = automap_base()$ Base.prepare(engine, reflect=True)
miss_grp1 = df.query("group =='treatment' and landing_page == 'old_page'")$ print('The number of times a user from the treatment group lands on the old page is {}'.format(len(miss_grp1)))
pokemon.drop(["id"],inplace=True,axis=1)$ pokemon.drop(["Type 2"],inplace=True,axis=1)$ pokemon.head()
delay_delta['delay'].astype('timedelta64[D]').hist(bins=20)$ plt.xlabel('Response in Minutes')$ plt.ylabel('Frequency of Resonse')
                                               df['AgeNormed'].min(),$                                                df['AgeNormed'].max()))
giss_temp.boxplot();
impact_effort = pd.read_csv('impact_effort.csv')
df_countries = pd.read_csv('countries.csv')$ df_countries.head()
print('reduce memory')$ utils.reduce_memory(user_logs)
df_vow['Volume'].unique()
df_countries = pd.read_csv("countries.csv")$ df_countries.head()
import urllib3, requests, json, base64, time, os$ warnings.filterwarnings('ignore')
cig_data_SeriesCO.dtype
lr = LogisticRegressionCV()$ lr.fit(train_Features, train_species)
time_open_days_issues = Issues(github_index)$ time_open_days_issues.is_open()$ time_open_days_issues.fetch_results_from_source('time_open_days', 'id_in_repo', dataframe=True)
df = pd.read_sql('SELECT * from hotel', con=conn_a)$ df
max_mean_km = price_vs_km["mean_odometer_km"].max()$ price_vs_km.loc[price_vs_km["mean_odometer_km"] == max_mean_km, :]
print (r.json())
invalid_name_list = twitter_archive_full[twitter_archive_full['name'].str.islower()].name.unique()$ twitter_archive_full.loc[twitter_archive_full.name.isin(invalid_name_list), 'name'] = 'None'
result1 = df2.T[0] + df3.iloc[1]$ result2 = pd.eval('df2.T[0] + df3.iloc[1]')$ np.allclose(result1, result2)
dicttagger_beverages = DictionaryTagger(['beverages.yml'])
print(plate_appearances.shape)$ plate_appearances = plate_appearances.dropna()$ print(plate_appearances.shape)
df.info()$ df.isnull().values.any()
len(df2['user_id'].unique())
df1['K_lbs-Site1'] = df1["AnnualFlow_MGD"] * df1['TotalN'] * 8.34 / 100000$ df2['K_lbs-Site2'] = df2["AnnualFlow_MGD"] * df2['TotalN'] * 8.34 / 100000$ df3['K_lbs-Site3'] = df3["AnnualFlow_MGD"] * df3['TotalN'] * 8.34 / 100000
df_joined[['CA','UK','US']] = pd.get_dummies(df_joined['country'])
type_df.columns = ['column_name','column_type']$ type_df.groupby('column_type').aggregate('count').reset_index()$
tweets = pd.read_json('tweet_json.txt')
tmax_day_2018.attrs
df_countries = pd.read_csv('countries.csv')$ df_countries.head()
df.loc[[11,24,37]]
df2.head(3)
nitrodata['MonitoringLocationIdentifier'].nunique()
Log_model2 = sm.Logit(df_new['converted'],df_new[['intercept','CA','UK','treatment']])$ Log_model2.fit().summary()
np.mean(df['converted'] == 1)
final_df = final_df[pd.isnull(final_df.retweeted_status_id)]$ final_df = final_df.dropna(subset = ['jpg_url'])$ fin_df=final_df.copy()
df_weather = df_weather[['STATION_NAME','DATE','HOURLYVISIBILITY','HOURLYDRYBULBTEMPC','HOURLYWindSpeed','HOURLYPrecip']].copy()
req_dict = dict(req.json())$
df2= df2.drop_duplicates(['user_id'],keep='first');
set(hourly_dat.columns) - set(out_temp_columns+incremental_precip_columns+general_data_columns+ wind_dir_columns)
spencer_bday = dt.date(1989, 4, 25)$ thirty_years = dt.timedelta(days=365*30 + 7)
google.sort_values(by='Date', ascending=True, inplace=True)
nba_df.sort_values(by = ["Tm.STL"], ascending = False)
unitech_df.dtypes
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df =sentiments_df[['Date','Compound','Count']]$ sentiments_df.head()
baseball1_df.loc[baseball1_df['ageAtFinal'].idxmax()]
from h2o.estimators.gbm import H2OGradientBoostingEstimator
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.csv"$ df = pd.read_csv(path, sep =',', na_values=['.'])$ df.head(5)
tips["sex"].index
train_df["price_t"] =train_df["price"]/train_df["bedrooms"]$ train_df["num_description_words"] = train_df["description"].apply(lambda x: len(x.split(" ")))
dates = pd.date_range('2018-05-01', '2018-05-06')$ temps1 = Series([80, 82, 85, 90, 83, 87], index = dates)$ temps1
ts_count_mk = df_mk_type.groupby(['date','type'])['text'].count()$ ts_count_mk.to_csv('daily_count_mk.csv')$ ts_count_mk.head(10)
city_group_df_merge_urban = city_group_df_merge.loc[city_group_df_merge['City Type']=='Urban',:]$ city_group_df_merge_suburban = city_group_df_merge.loc[city_group_df_merge['City Type']=='Suburban',:]$ city_group_df_merge_rural = city_group_df_merge.loc[city_group_df_merge['City Type']=='Rural',:]$
autoDf = SpSession.createDataFrame(autoMap)$ print (autoDf.show())$
data.tasker_id.value_counts().head()
f_lr_hash_modeling2 = f_lr_hash_modeling2.withColumn('id', col('id').cast('long'))$ f_lr_hash_test = f_lr_hash_test.withColumn('id', col('id').cast('long'))
autos[~autos["registration_year"].between(1900,2016)]
twitter_archive.loc[(twitter_archive['name'].str.islower()) & (twitter_archive['text'].str.contains('named'))]
df_final['join_days'] = (now - df_final['first_trx']).dt.days
status_data = status_data.drop(['STATUS', '#AUTHID', 'sEXT', 'sNEU', 'sAGR',$                                     'sCON', 'sOPN', 'DATE'], axis=1)
system = system.supersize(2, 2, 2)$ print(system)
fe.bs.csv2ret??
df.groupby('Week').Avg_speed.mean()
datetime.now().toordinal()/365$
xml_in[xml_in['publicationDate'].isnull()].count()
marsweath_url = 'https://twitter.com/marswxreport?lang=en'
df.reset_index().to_csv('final.csv',header=True, index=False)
dups = df2.user_id.value_counts()$ dups[dups > 1]$
park = load_data('../../static/parkinson_1960tonow.csv')
results.summary()
for df in (joined, joined_test):$   df.loc[df['CompetitionDaysOpen']<0,'CompetitionDaysOpen']=0$   df.loc[df['CompetitionOpenSinceYear']<1900,'CompetitionDaysOpen']=0
MetaMetaclass.__call__(MetaClass,'Example', (), {})$
transactions.merge(users, how='left', on='UserID')
small_train.to_csv('small_train.csv', index=False) 
np.exp(result.params)
df2=pd.concat([df[(df['group']=='treatment') & (df['landing_page']=='new_page')],df[(df['group']=='control') & (df['landing_page']=='old_page')]])
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']])$ results = logit_mod.fit()$
log_mod_2 = sm.Logit(df_new['converted'], df_new[['CA', 'UK', 'intercept']])$ result_2 = log_mod_2.fit()$ result_2.summary()
rng = pd.date_range(end='2018-01-19', periods=200, freq='BM')$ type(rng)
df_clean = pd.merge(left=df_enhanced,right=df_json, left_on='tweet_id', right_on='id', how = 'left')$ df_clean = df_clean.drop(['id', 'retweeted'], axis = 1)$ df_master = pd.merge(left=df_clean,right=df_breed, left_on='tweet_id', right_on='tweet_id', how = 'left')
len(pres_df['subjects'][0].split())
df = pd.read_csv('ab_data.csv')$ df.head()
temp_df2 = temp_df.drop_duplicates()
ward_df['Delays'].corr(ward_df['Crime Count'])
my_gempro.uniprot_mapping_and_metadata(model_gene_source='TUBERCULIST_ID')$ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping)$ my_gempro.df_uniprot_metadata.head()
def known(words): $     "The subset of `words` that appear in the dictionary of WORD_COUNTS."$     return set(w for w in words if w in WORD_COUNTS)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY$ data = requests.get(url)
irisDF1 = SpSession.read.csv("iris.csv",header=True)$ print (irisDF1.show())
array_expected =[np.array(data_set_1.who_region)]$ print (array_expected) # region wise 1-darray as request in Question$
number_of_commits = len(git_log)$ number_of_authors = git_log.loc[:, 'author'].dropna().nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
lm.fit(x_train, y_train)
df.head()
dictionary=json.loads(json_string) #Convert string to dictionary
records['Gender'] = records['Gender'].apply(str.capitalize)
compound_df = compound_df.reset_index()
df.drop(df.query('group == "treatment" and landing_page == "old_page"').index, inplace=True)$ df.drop(df.query('group == "control" and landing_page == "new_page"').index, inplace=True)$ df2=df
from sqlalchemy import distinct$ number_of_stations = session.query(func.count(distinct(Measurement.station))).scalar()$ print(f'The total number of stations : {number_of_stations}')
flu_vacc_file4= "Data/NYVacc2015-17.csv"$ vacc15_17 = pd.read_csv(flu_vacc_file4)$ vacc15_17.head()
print(test_df[test_df.author.isnull()].shape[0])$ print(test_df.shape[0])
pvt.to_csv('./output/ganalytics_transactions_and_associated_products.txt', sep='|', index=False, quoting=csv.QUOTE_NONE)
from IPython.display import FileLink$ FileLink(str(FLASK_PATH/'final_df.csv'))
with open('key_phrases_rake.pickle', 'wb') as f:$     pickle.dump(key_phrases_rake, f, pickle.HIGHEST_PROTOCOL)
(~autos["registration_year"].between(1900,2016)).sum()/autos.shape[0]
news_sentiments.to_csv("News_Sentiments.csv")
predictions = regressor.predict(test_features)$ test['prediction'] = predictions$ test
for a in cells: $     ct.save_reconstruction(a['id'], file_name='allen_morpho/ai'+str(a['id'])+'.swc')$     ct.save_reconstruction_markers(a['id'],file_name='allen_morpho/ai'+str(a['id'])+'_marker.swc')
print(store_info.CompetitionDistance.describe())$ store_info.CompetitionDistance.plot.hist();
df=data$ print(df.isnull().sum())
pd.Series({'a': 42, 'b': 13, 'c': 2})
dataset.columns.str.extract(r'^(\w+)$')
close_series = stock_data['close']$ display(close_series.head(5)) 
events.head()
pd.isnull(train_df).sum()$ pd.isnull(test_df).sum() 
transactions[~transactions['UserID'].isin(users['UserID'])]
model.clusterCenters()
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
from scipy.stats import norm$ norm.cdf(z_score)$ norm.ppf(1-(0.05/2))$
total_students_with_passing_reading_score = len(df_students.loc[df_students['reading_score'] > 69])$ total_students_with_passing_reading_score
joined=join_df(train,store,"Store")$ joined_test=join_df(test,store,"Store")$ sum(joined['State'].isnull()),sum(joined_test['State'].isnull())
import pandas as pd$ date = pd.to_datetime("4th of July, 2015")$ date
df.user_id.nunique()
varcode = 'TN'$ siteds_dct = siteds[siteds['DataSetCode'].str.contains(varcode)].reset_index().iloc[0].to_dict()$ siteds_dct
df_schools.head()
temp = pd.merge_asof(df_expand, ari_games2, on='Date', direction='forward')
dupID = '2017-01-09 05:37:58.781806'$ df2 = df2[df2.timestamp != dupID]
result.summary()
y_range = (0, 200)$ x_range = bokeh.models.ranges.Range1d(start = datetime.datetime(2017,1,1),$                                       end = datetime.datetime(2017,1,31))
q_agent_new.scores += run(q_agent_new, env, num_episodes=50000)  # accumulate scores$ rolling_mean_new = plot_scores(q_agent_new.scores)
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,300], ylim=[0,300], title="Favorites and Retweets:300x300")$
dfrecent = dfdaycounts[dfdaycounts['created_date']> pd.to_datetime('2012-12-31')]
print(psy_hx.shape)$ psy_hx = psy_hx[psy_hx["subjectkey"].isin(incl_Ss)]$ psy_hx.shape$
df = df[pd.notnull(df['jpg_url'])]
import numpy as np$ ok.grade('q02')
DC_features = pd.read_csv('Historical_Work_Sunlight_Data.csv',index_col=0,parse_dates=True)
experiment_df.drop(['email_opened'], axis=1, inplace=True)$ experiment_df.drop(['date'], axis=1, inplace=True)$ experiment_df.drop(['UUID_hash2'], axis=1, inplace=True)
html=HTML(html=doc)$ html.links
with pd.option_context('display.max_colwidth', 100):$     display(news_period_df[['news_collected_time', 'news_title']])
df2[((df2.group == 'treatment') == (df2.landing_page == 'new_page')) == False].shape[0]
df3 = df2.query("group == 'control'")$ df4 = df3['converted'].mean()$
ts=df15.groupby('day_of_week').agg({'sale':['sum']})$ ts
subcols = dropcol(subcols, dnas.keys())$ print(len(subcols))$ print(subcols)
set_option("display.max_colwidth",280)$
n_new = (df2.landing_page == 'new_page').sum()$ n_new
data_l1 = tmpdf.index[tmpdf[tmpdf.isin(DATA_L1_HDR_KEYS)].notnull().any(axis=1)].tolist()$ data_l1
graph_url = 'https://graph.facebook.com/v2.11/'$ req_url = '74133697733?fields=posts{message,created_time,comments.limit(0).summary(true), likes.limit(0).summary(true)}'$ final_url = graph_url + req_url
df['out_content_match_strength'] = df['out_x_coord_match'].astype(int) + df['out_y_coord_match'].astype(int) + df['out_beg_match'].astype(int) + df['out_end_match'].astype(int)
print(df.iloc[0])
train.target.sum() == data.shape[0]
dfx = df.copy()$ dfx[dfx < 0] = -dfx  # abs$ print(dfx)
print(type(df.groupby("grade").size()))  # as series (series values is count of each category)$ df.groupby("grade").size()
seq2seq_Model.save('seq2seq_model_tutorial.h5')
joined.describe()$
pro_result = pro_table.sort_values('Profit', ascending=False)$ pro_result.head()
df['converted'].mean()
store_items.fillna(method='ffill', axis=0)
sql_extract = pd.read_sql('select [id],[turn], [name], [player], [mana], [import_date],[Date_Time],RANK() over(partition by [id],[turn], [name], [player], [mana] order by Date_Time )Rank From naturex.[dbo].[HS_CardTable] order by id,Date_Time,turn, player', engine)$                           $ sql_extract.head()
df_missing = df[df.isnull().any(1)]$ print(df_missing.shape)$ df_missing.head()
contractor_final.to_csv('contractor_final.csv')
df.head()
df.hastags = df.hastags.apply(getHasTags)
urb_pop_reader = pd.read_csv(file_wb, chunksize = 1000)$ df_urb_pop = next(urb_pop_reader)$ print(df_urb_pop.head())
df_teacher_behavior = df_teacher_behavior[df_teacher_behavior.awj_teacher_id.isin(pick_list)]$
print(ttarc_clean.loc[ttarc_clean.rating_denominator != 10, 'rating_denominator'])
filename = "../datasets/catalogs/fermi/gll_psc_v16.fit.gz" $ print([_.name for _ in fits.open(filename)])$ extended_source_table = Table.read(filename, hdu='ExtendedSources')
set(bookings.columns).intersection(set(releases.columns))
ReleaseEvent = events[events['type'] == 'ReleaseEvent']$ print ReleaseEvent.shape$ ReleaseEvent.head()
df[df['group']=='treatment'].groupby('landing_page').count()
df2.drop(df2[df2['user_id'].duplicated(keep = False)].index[0], inplace = True)
df = pd.read_sql('SELECT * from booked_room', con=conn_b)$ df.head(10) # show 10 rows only
planets.head()
autos[["price","odometer"]].head()
events_enriched_df['topic_name'].drop_duplicates().count()
tweets.head()
data = ['peter', 'Paul', 'MARY', 'gUDIO']$ [s.capitalize() for s in data]
dfall.info()
pd.core.common.is_list_like = pd.api.types.is_list_like$ import pandas_datareader.data as web
reviewsDFslice = reviewsDF[:25924]$ reviewsDFslice.tail(50)
dr_new.columns$
autos["price"] = autos["price"].str.replace("$","")$ autos["price"] = autos["price"].str.replace(",","")$ autos.price = autos.price.astype(float)
data = pd.read_csv('fatal-police-shootings-data.csv')$ print(data.shape)$ data.info()
if 1 == 1:$     news_titles_sr = pd.read_pickle(news_period_title_docs_pkl)
df_new['country'].unique()
trainpath = "./data/train.csv"$ testpath = "./data/test.csv"$
df_twitter_copy = df_twitter_copy[df_twitter_copy['in_reply_to_status_id'].isnull()]
for active_add_date in daterange:$     active_add_rows = active_df[active_df['start_date']==active_add_date]$     cohort_active_activated_df.loc[active_add_date,active_add_date:] = len(active_add_rows) 
df = pickle.load(open( "lyincomey.p", "rb" ))
age_gender.shape$ age_gender.head(5)$
for f in read_in["files"]:$     fp = getattr(processing_test.files, f)$     print(json.load(open(fp.load())))
new_converted_simulation = np.random.binomial(n_new, p_new, 10000)/float(n_new)$ old_page_simulation = np.random.binomial(n_old, p_old, 10000)/float(n_old)$ p_diffs = new_converted_simulation - old_page_simulation$
test_collection.insert_one(temp_dict)
pd.DataFrame(sorted([[n, len(g)] for n,g in pd.groupby(topics, by=topics.name)], key=lambda x: x[1])[-9:], columns=["name", "count"])
result = requests.get(url)$ c = result.content$ soup = BeautifulSoup(c, "lxml")
df.drop(['p1', 'p1_conf', 'p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog',], axis=1, inplace=True)
df1 = df1.dropna()
sqladb.columns=headers
from dateutil import parser$ date = parser.parse("4th of July, 2017")$ date
session.query(Measurement.station, func.count(Measurement.tobs)).\$ group_by(Measurement.station).\$ order_by(func.count(Measurement.tobs).desc()).all()
prcp_12monthsDf.set_index('date').describe()
df_ad_airings_filter_3.to_pickle('./TV_AD_AIRINGS_FILTER_DATASET_2.pkl')
nba_df['Date'] = pd.to_datetime(nba_df['Date'])
%matplotlib inline$ commits_per_year['author'].plot( kind='line', title='Commits per Year', grid=True)
url_votes = grouped['net_votes'].agg({'total_votes': 'sum', 'avg_votes': 'mean'})    $
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
regGridSearch.best_estimator_.coef_
df2_converting = df2.converted.mean()$ print('The probability of converting regardless of the page is: {}'.format(round(df2_converting, 4)))
des2doc = dict()$ for index,row in temp.iterrows():$     des2doc[index] = row['description']
data.plot.area()
monte.str.startswith('T')
ratio = result["Fail"].div(result["Pass"])$ ratio.sort_values(ascending=False, inplace=True)$ ratio$
tf.reset_default_graph()$ P("Reseting TF graph")$ the_graph = tf.Graph()
game_data_all.shape
exl=pd.read_excel('Book1.xlsx', converters={'item#':str,'part#':str, 'deliverydate':pd.to_datetime})$
prcp_12monthsDf = prcp_12monthsDf.set_index('date')$ prcp_12monthsDf
crimes.head()
X_nonnum = X_copy.select_dtypes(exclude=np.number)$ X_nnumcols = list(X_nonnum)$ print(X_nnumcols)
columns = ['dates','cashAmount','stockValue','totValue','tradeProfit','tradeLoss']$ dfTgt = pd.DataFrame({'symbol':syms,'dist':compos,'ticker':tickers}).set_index('ticker')
intervention_train.index.duplicated()
rides_fare_average_min = rides_analysis["Average Fare"].min()$ rides_fare_average_min
mean = np.mean(data['len'])$ print("The average length of all tweets: {}".format(mean))
results_distributedTopmodel, output_DT = S_distributedTopmodel.execute(run_suffix="distributedTopmodel_hs", run_option = 'local')
from pandas.tools.plotting import scatter_matrix$ scatter_matrix(mean_sea_level, figsize=LARGE_FIGSIZE);
station = pd.read_sql("SELECT * FROM station", conn)$ station.head()
pd.set_option('max_colwidth', 100)
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]
data_year_df = pd.DataFrame.from_records(data_year, columns=('Date','Station','Prcp'))$ data_year_df.head()
churned_ordered = ordered_df.loc[churned_ord]
close_price=[row[4] for row in daily_price if row[4] is not None]$ close_price.sort()$ print(close_price[-1]-close_price[0])$
prcp_df.describe()
the_means = np.mean(the_sample, axis=1)$ print(f'Mean ({num_samples} samples) = {np.mean(the_means):5.3f}')$ print(f'Standard deviation ({num_samples} samples) = {np.std(the_means):5.3f}')
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df =sentiments_df[['Date','Compound','Count']]$ sentiments_df.tail()
count_pages=df2.groupby(['landing_page']).size()# this gives the count of both new and old pafes together$ n_new=count_pages[0]# this gives the count for the new pages$ n_new
vwg = pd.read_excel('input/Data.xlsm', sheet_name='53', usecols='A:Y', header=13, skipfooter=10)
lin2 = sm.OLS(df_new['converted'], df_new[['intercept','US','UK']])$ result2 = lin2.fit()
covar=np.cov(x,y)$ covar
df1.describe()
Temperature_DF.plot.hist(by="Tobs",bins=12,title="Observed Temperature from %s to %s"%(start_date,end_date))$ plt.ylabel("Frequency of Temperature")$ plt.show()
temps_df.Difference[1:4]
data.plot(legend=True,figsize=(15,8))
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).order_by(Measurements.date).all()
col = pymongo.MongoClient()['Tweets']['Streaming']$ col.count()
plt.xlim(100, 0)$ plt.ylim(-1, 1)
df.shape
plt.hist(null_vals);$ plt.axvline(x=obs_diff, color = 'red');
my_stream_listener = PrintingStreamListener()$ my_stream = tweepy.Stream(auth = api.auth, listener=my_stream_listener)
dataframe.head(20)
sat_5am = pendulum.datetime(2017, 11, 25, 5, 30, 0, tzinfo='US/Eastern')$ sat_6am = pendulum.datetime(2017, 11, 25, 6, 0, 0, tzinfo='US/Eastern')$ sat_spike = tweets[(tweets['time_eastern'] >= sat_5am) & (tweets['time_eastern'] <= sat_6am)]
p = fp7.append(h2020)$ fp7 = 0$ h2020 = 0
Ser1.iloc[]
twitter_Archive.info()
np.random.seed(123456)$ ps = pd.Series(np.random.randn(12),mp2013)$ ps
twitter_archive_clean['source'].astype('category')$ twitter_archive_clean.head()
exiftool -csv -createdate -modifydate cisrol12/cycle1_MVI_0032.mp4 cisrol12/cycle1_MVI_0033.mp4 cisrol12/cycle1_MVI_0034.mp4 cisrol12/cycle2_MVI_0035.mp4 cisrol12/cycle2_MVI_0036.mp4 cisrol12/cycle2_MVI_0037.mp4 cisrol12/cycle2_MVI_0038.mp4 cisrol12/cycle2_MVI_0039.mp4 cisrol12/cycle2_MVI_0042.mp4 cisrol12/cycle2_MVI_0043.mp4 cisrol12/cycle2_MVI_0044.mp4 cisrol12/cycle2_MVI_0045.mp4 cisrol12/cycle2_MVI_0046.mp4 cisrol12/cycle2_MVI_0047.mp4 cisrol12/cycle2_MVI_0048.mp4 cisrol12/cycle2_MVI_0049.mp4 cisrol12/cycle2_MVI_0050.mp4 cisrol12/cycle2_MVI_0051.mp4 cisrol12/cycle2_MVI_0052.mp4 cisrol12/cycle2_MVI_0053.mp4 cisrol12/cycle2_MVI_0054.mp4 cisrol12/cycle2_MVI_0055.mp4 cisrol12/cycle4_MVI_0075.mp4 cisrol12/cycle4_MVI_0076.mp4 cisrol12/cycle4_MVI_0078.mp4 cisrol12/cycle4_MVI_0079.mp4 cisrol12/cycle4_MVI_0080.mp4 cisrol12/cycle4_MVI_0081.mp4 cisrol12/cycle4_MVI_0082.mp4 cisrol12/cycle4_MVI_0083.mp4 cisrol12/cycle4_MVI_0084.mp4 cisrol12/cycle4_MVI_0085.mp4 cisrol12/cycle4_MVI_0086.mp4 cisrol12/cycle4_MVI_0089.mp4 cisrol12/cycle5_MVI_0095.mp4 cisrol12/cycle5_MVI_0096.mp4 cisrol12/cycle5_MVI_0097.mp4 cisrol12/cycle5_MVI_0098.mp4 cisrol12/cycle5_MVI_0100.mp4 cisrol12/cycle5_MVI_0101.mp4 cisrol12/cycle5_MVI_0102.mp4 cisrol12/cycle5_MVI_0106.mp4 > cisrol12.csv
mydata.iloc[0] 
print(ndvi_nc)$ for v in ndvi_nc.variables:$     print(ndvi_nc.variables[v])
autos.columns
treatment_conv = conv_ind.query('group == "treatment"').shape[0]$ treatment_group = df2.query('group == "treatment"').shape[0]$ print('Probability of TREATMENT page converted individual: {:.4f}'.format(treatment_conv/treatment_group))
exec(open("./secrets.literal").read())$ gh_user = "holdenk"$ fs_prefix = "gs://boo-stuff/"
table=pd.DataFrame(pd.date_range('01-01-2008','31-12-2008'),columns=['dates'])$ table['dates']=list(map(lambda x:x.strftime('%m-%d'), table['dates']))
rhum_nc = Dataset("../data/nc/rhum.mon.mean.nc")
    return stemmer.stem(word)
r =requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2017-01-01&end_date=2017-12-31')$ print(r.json())
ValidDates = df_1.groupby(df_1.index).size()[5:112].copy()
!mv 9 blast.txt
dfBRML3.head()
df = df.drop("a")$ df
cand.CAND_ST.value_counts()
ride_percity=original_merged_table["type"].value_counts()$
addicks.head()
nba_df.loc[nba_df.loc[:, "Tm.Pts"] > 145, "Team"].tolist()
cityID = 'a6c257c61f294ec1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Greensboro.append(tweet) 
df2[((df2['group'] == 'treatment') ==(df2['landing_page'] == 'new_page')) == False].shape[0]
combined_df = test.join(prediction_df)$ combined_df.head(15)
response = requests.get('http://www.reddit.com/r/aww')$ page_source = response.text$ print(page_source[:1000])
json_str=json.dumps([status._json for status in ArepaZone_timeline])$ type(json_str)
tweet_clean.drop('Unnamed: 0',axis=1,inplace=True)
print(actual_value_second_measure[actual_value_second_measure==2])$ holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00063550') ]
old_page_converted = np.random.choice([1, 0], size=n_old,p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
measurements_df.to_csv('clean_hawaii_measurements.csv', index=False)
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page'))== False].describe()
flu_vacc_file1= "Data/NYVacc2015-16CalcData.csv"$ flu_vacc_file2= "Data/NYVacc2016-17CalcData.csv"
print(metrics.classification_report(y_test, y_pred_class))
df.drop(df.query('group == "treatment" and landing_page == "old_page"').index,inplace = True)$ df.drop(df.query('group == "control" and landing_page == "new_page"').index,inplace = True)$ df2 = df
term_tops = lda.get_term_topics(rand_word, minimum_probability=.0001)$ term_tops, get_topic_desig(term_tops)
logs['key'].value_counts().plot()$ plt.show()
hour = noise_data["Created Date"].dt.floor("60min").to_frame("Hour of Day")$ hour.head()
grid_pr_size.plot.scatter(x='glon',y='glat', c='size_val', $                            colormap = 'RdYlGn_r')
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-06-01&end_date=2018-06-01&api_key=")$
outfile = os.path.join("Resource_CSVs","Main_data_revised.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
for stat in session.query(Measurement.station).distinct():$      print(stat)
groups = trees.groupby('village')$ groups.get_group('Upper West Side').describe()$
pd.value_counts(RNPA_existing['ReasonForVisitName'])
from statsmodels.stats.weightstats import ztest$ ztest(hm_data.weight.values, value=85., alternative="two-sided")
df2['converted'].mean()*100
df_input_clean = df_input_clean.withColumn("isLate", (df_input_clean.Resp_time > 1).cast('integer'))
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)$ results.summary()
X = lq2015_combined[feature_cols]$ y = lq2015_combined.SaleDollars_fy
print grid.best_score_$ print grid.best_params_$ print grid.best_estimator_
A = nx.to_numpy_matrix(G, weight=None)$ np.savetxt("data/congress_actor.txt", A, fmt='%d')
dir(friends_n_followers)$ help(friends_n_followers.sort_values)
df = pd.DataFrame(np.arange(12).reshape(4, 3), columns = ['a', 'b', 'c'])$ df
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']])$ results = mod.fit()$ results.summary()
ks_goal_failed = ks_goal_failed.sort_values(by = ['counts'], ascending = False)$ ks_goal_failed.head(10)
(df_cprc.isnull()).sum()
df_new_conv = df_newpage.query('converted == "1"')$ x_new_conv = df_new_conv["user_id"].count()$
eve_contr =df_eve.query("ab_page==0").converted.mean()$ eve_contr
full_p_new = df2.converted[df2.group == 'treatment'].mean()$ full_p_new
autos["price"].sort_index(ascending=False) #sorts according to index aka row positioning, so if its descending it will show from bottom up. $
df.to_sql('places', conn)
df['created_at'] = pd.to_datetime(df['created_at'])
autos[autos.price > 500000].loc[:,["name", "odometer_km","price" ]]$
store_items = store_items.drop(['bikes'], axis=1)  # bikes column$ store_items
output = pipeline.fit(flight7).transform(flight7)$ output = output.withColumnRenamed('price_will_drop_num', 'label')$ output.cache()
sns.barplot(y='postsCount',x='date_quarter',data=postsQ,hue='year')$ plt.xticks(rotation = 90)$
selection = spice_df.query('parsley & tarragon')$ len(selection)
sns.distplot(dfz.favorite_count.apply(np.log), color = 'red', label = 'Favorites')$ sns.distplot(dfz.retweet_count.apply(np.log), color = 'blue', label = 'Retweets')
df=df[df.Trip_duration >0]
r = requests.get("https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2017-01-01&end_date=2017-01-03&api_key="", auth=(''))$
train.reset_index(inplace=True)$ test.reset_index(inplace=True) ##Note reset_index add another column called 'index with the old index cols$ train.head(5)
temp_df2['titles'] = temp_df2['titles'].str.lower()
station_count.sort_values(['Count'],ascending=False, inplace=False, kind='quicksort', na_position='last')
my_gempro.pdb_downloader_and_metadata()$ my_gempro.df_pdb_metadata.head(2)
rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]$
df_new[['CA','UK','US']]= pd.get_dummies(df_new['country'])$ df_new.head()
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new['country'].value_counts()
csvData[csvData['street'].str.match('.*Drive.*')]['street']
con = sqlite3.connect('db.sqlite')$ print(pd.read_sql_query("SELECT * FROM temp_table ORDER BY total DESC", con))$ con.close()
df = pd.merge(train,user_logs, on = 'msno', how = 'left')$ df.sort_values(by = ['msno','date'], inplace = True)
potential = lmp.Potential('2015--Pascuet-M-I--Al--LAMMPS--ipr1.json')
data.learner_id.value_counts()
bots = df[df['author'].str.contains('Bot')]['author'].unique()
df2 = df2.drop(1899, axis=0)
z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old], alternative = 'larger')$ print('z score is '+ str(z_score))$ print('p_value is '+str(p_value))
engine = create_engine("sqlite:///hawaii.sqlite")
from sklearn.cluster import KMeans$ kmeans_model = KMeans(n_clusters=7) $ kmeans_model.fit(df)
cursor.execute(" select distinct issue_id from ticket_issue ")
from nltk.corpus import stopwords$ from sklearn.feature_extraction.text import CountVectorizer
df.dateCrawled = pd.to_datetime(df.dateCrawled)$ df.dateCreated = pd.to_datetime(df.dateCreated)$ df.lastSeen = pd.to_datetime(df.lastSeen)
autos[((autos.price > 500000) & ~(autos.name.str.contains("Ferrari")) )]$ autos=autos[~((autos.price > 500000) & ~(autos.name.str.contains("Ferrari")) )] #antimasking$ autos.shape #stripped off 12 rows
jan_2015_frame = add_pickup_bins(frame_with_durations_outliers_removed,1,2015)$ jan_2015_groupby = jan_2015_frame[['pickup_cluster','pickup_bins','trip_distance']].groupby(['pickup_cluster','pickup_bins']).count()
r.text
au.clear_dir('data/city-util/proc')
pairofTransdf = pd.merge(transactions, transactions, how='left', on=['UserID'])$ print(pairofTransdf.sort_values(by=['UserID'],ascending=[False]))
(df2.landing_page == 'new_page').mean()
go_no_go = 1 #NO_GO - ie do NOT run all the long run-time items.  Note that some of these long$
print(list(cos.buckets.all()))
client.repository.list_models()
df[df['yearOfRegistration'] > 2016].head(1)$
borough_group = data.groupby('Borough')$ borough_group.size().plot(kind='bar')$
ttarc_clean = ttarc_clean[ttarc_clean.rating_denominator == 10]$ ttarc_clean.shape
autos.brand.value_counts(normalize=True)$
groups_topics_df.shape, groups_topics_unique_df.shape
twitter_data = pd.DataFrame(list(twitter_coll_reference.find()))$ twitter_data.head()
df.dropna(how='any', inplace=True)$
IBMspark_df.registerTempTable("IBM")$ print sqlContext.sql("select * from IBM limit 10").collect()
np.shape(rhum_us_full)
results = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= prev_year).all()$ results[0:10]
df_active_user_metrics['group_code_activations'].value_counts()
z_score,p_value=sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old],alternative='larger')
labelencoder_X  = LabelEncoder()$ train['gender'] = labelencoder_X.fit_transform(train['gender'])
top_10_authors = pd.value_counts(git_log['author'])$ top_10_authors = top_10_authors.head(10)$ print(top_10_authors)
bus['postal_code_5'] = bus['postal_code_5'].str.replace('94602','94102')
dfCountry = pd.read_csv('countries.csv')$ dfCountry.head()
shows['stemmed_keywords'] = shows['keywords'].dropna().apply(split_and_stem2)
df.quantile(.75) - df.quantile(.25)
df3 = df2.join(df_countries.set_index('user_id'), on='user_id')$ df3.head()
svg2 = displacy.render(parsed4, style='ent', jupyter=True)
for c in ccc:$     for i in vhd[vhd.columns[vhd.columns.str.contains(c)==True]].columns:$         vhd[i] /= vhd[i].max()
x = content_performance_bytime.groupby(['document_type', pd.Grouper(freq='M')])['pageviews'].sum()
newsMood_df['Timestamp'] = pd.to_datetime(newsMood_df['Timestamp'])$ newsMood_df.to_csv('newsMood_dataframe.csv')$ newsMood_df.head()
stationweekday = data.groupby(['STATION','SaturdayWeekEndingDate','DayOfWeek'])['EntriesDifference'].agg(pd.np.sum)$
archive_clean = archive_clean.loc[~archive_clean['tweet_id'].isin(remove_list),:]
import numpy as np$ ok.grade('q04')
uni = free_data[free_data['educ']<=5]$ uni.groupby('country')['country'].count()
print(teamAttr.shape)$ teamAttr.head(3)
np.exp(cig_data_SeriesCO)
ng_stor_json = ng_stor.response.json()
plt.imshow(np.log(sentimentHist+0.001),interpolation='none')$
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','OriginationPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['orange','green', 'blue', 'red', 'black'], grid=False)$
print(pd.isnull(dfx))
cityID = '0c2e6999105f8070'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anaheim.append(tweet)  
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key="$ r = requests.get(url+API_KEY)$
clean = cleanup(df)$ print clean['user_location'].value_counts()[:15]
print ("There are " + str(rawautodf['nrOfPictures'].sum()) + " total pictures.")$ print ("Hence to point in having that column")$
data.area is data['area']
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])
station_sum = session.query(Measurement.station).distinct().count()$ print (station_sum)
station_most_active = session.query(Measurement.station, Station.name).group_by(Measurement.station).\$                         order_by(func.count(Measurement.tobs).desc()).first()$ station_most_active
a=[0,1]$ old_page_converted = np.random.choice(a,145274,p=[0.8804,0.1196])$ print(old_page_converted.mean())
print("dataframe df row names as follows : ")$ df.index.values
regressor3 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'country_US', 'country_UK']])$ results3 = regressor3.fit()
for column in list1:$     election_data.loc[election_data['st'] == 'DC',column] = election_data[column].mean()$
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY$ r = requests.get(url)
df2[df2.duplicated(['user_id'], keep=False)]
from statsmodels.tsa.arima_model import ARIMA$ model_6203 = ARIMA(dta_6203, (6, 1, 0)).fit() $ model_6203.forecast(5)[:1] 
df_2012['bank_name'] = df_2012.bank_name.str.split(",").str[0]$
pd.Series(np.random.randn(1000)).hist(bins=20, alpha=0.4);
data_test = data_s[data_s['date'] < datetime(2017,1,1)]$ data_test['date'].max()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])$ norm.cdf(z_score) $ norm.ppf(1-(0.05/2)), norm.cdf(z_score), z_score, p_value$
precip = session.query(Precip.date, Precip.prcp, Precip.station).filter(Precip.date >= date_ly)
df[['polarity', 'subjectivity']] = df['text'].apply(lambda text: pd.Series(TextBlob(text).sentiment))$ df['SA'] = np.array([ analize_sentiment(tweet) for tweet in df['text'] ])
Measurement_df = pd.read_sql("SELECT * FROM measurement", conn)$ Measurement_df.head(10)
p_new_null = df.converted.mean()$ p_new_null
soup.find_all('p')
df2.user_id.nunique()
p_old = df2.query('landing_page == "old_page"')['converted'].mean()$ print('The convert rate for the new page:', p_old)
frame = pd.DataFrame(data)$ frame
sns.set(style="white", color_codes=True)$ sns_plot = sns.jointplot(x = source_pos_df['Position_x'], y = source_pos_df['Position_y'], kind='kde', color="skyblue")$ sns_plot.savefig('kde_source.png')
plt.hist(np.nan_to_num(_delta.values()), bins=10)$ plt.tight_layout()
df_new.country.nunique()
dataframe.dtypes
merge.shape
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLA.csv"$ tesla = pd.read_csv(url, index_col=0, parse_dates=True) # index_col = 0, means that we want date to be our index$ tesla
df2['ab_page'] = df2.apply (lambda row: label_abpage (row),axis=1)$ df2['intercept'] = 1$
data = pandas.read_csv("MSFT.csv", index_col='Date')$ data.index = pandas.to_datetime(data.index)
reverseAAPL = AAPL.sort_index(ascending=False)$ reverseAAPL.head()
import sklearn.model_selection as model_select$ results = model_select.cross_val_score(classifier, X, y, cv=10, scoring='accuracy')$
df_merge.info()
archive_df_clean['rating_numerator'] = archive_df_clean.rating_numerator.astype(float)
df= pd.read_csv('ab_data.csv')$ df.head()
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])$
log_mod = sm.Logit(df_new['converted'], df_new[['CA', 'US']])$ results = log_mod.fit()$ results.summary()
df_goog.describe()$ df_goog.dtypes$ df_goog[['Open','Close','High','Low','Adj Close']].plot()
plt.pie(totalDrivers, labels = labels, explode = explode, $         colors = colors, autopct="%1.1f%%", shadow=True, startangle=180)$ plt.show()
retail_data = retail_data[['key', 'quantity', 'accountid']]$ retail_grouped = retail_data.groupby(['accountid', 'key']).sum().reset_index()
bad_r = requests.get('http://httpbin.org/status/404')$ bad_r.status_code
ideas.drop(['Authors', 'Link', 'Tickers', 'Title', 'Strategy'],axis=1,inplace=True)$ ideas = ideas.applymap(to_datetime)
overall_df = pd.DataFrame(data= twitter_df.groupby('User')['Compound'].mean())$ overall_df
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read())
sox.to_csv('../../../../../CS 171/cs171-gameday/data/sox.csv', index=False)$ bruins.to_csv('../../../../../CS 171/cs171-gameday/data/bruins.csv', index=False)$ celtics.to_csv('../../../../../CS 171/cs171-gameday/data/celtics.csv', index=False)
from statsmodels.tsa.arima_model import ARIMA$ arima10 = ARIMA(dta_713,(1,0,0),freq='Q').fit()$ arima10.summary()$
df_Tesla = pd.DataFrame.from_dict(dataset)$ df_Tesla[['created_at','text','hashtags','username','user_followers_count','topic']].head()
twitter_merged_data.describe()
crimes.columns = crimes.columns.str.strip()$ crimes.columns
for item in bottom_three:$     print('{} has a std of {}.'.format(item[0], item[1]))
print('Groups: {}'.format(df['group'].unique()))$ print('Landing pages: {}'.format(df['landing_page'].unique()))$ print('Converted: {}'.format(df['converted'].unique()))
with open(output_folder+Company_Name+"_Forecast_"+datetime.datetime.today().strftime("%m_%d_%Y")+".pkl",'rb') as fp:$     dummy_var = pickle.load(fp)
df_image_predictions = pd.read_csv(file_name,sep='\t')$ df_image_predictions.head()
malebydate = male.groupby(['Date','Sex']).count().reset_index()$ malebydate.head(3)
df_concensus = pd.read_csv( pwd+'/consensus_shift_history.052317.csv') #consensus?
datetime.now().strftime('%A')
transactions.merge(users,how="inner",on="UserID")
grad_GPA_mean = records3[records3['Graduated'] == 'Yes']['GPA'].mean()$ grad_GPA_mean
requests.get(wikipedia_marvel_comics)
cabs_df_byday = cabs_df.loc[cabs_df.index.weekday == weekday]$ cabs_df_byday.info()$ cabs_df_byday.head()
df = df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]
df_new['intercept'] = 1$ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','canada','uk']])$ results = logit_mod.fit()
print(abs(-2))$ print(round(3.234543,3))$
jp.sprints(board_id=18)$ jp.sprint_info(board_id=18,sprint_id=74)
plt.savefig(str(output_folder)+'NB01_2_landscape_image01_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice_str)
i = np.random.randint(x.shape[0]-1)$ m_star = m_star + 2*learn_rate*(x[i]*(y[i]-m_star*x[i] - c_star))$ c_star = c_star + 2*learn_rate*(y[i]-m_star*x[i] - c_star)
train['year'] = pd.to_datetime(train['date']).dt.year$ train['month'] = pd.to_datetime(train['date']).dt.month$ train['weekday'] = pd.to_datetime(train['date']).dt.weekday
x.loc[:,["B","A"]]
result = api.search(q='%23HarryPotter')$ len(result)
import pandas as pd$ dataset = pd.ExcelFile("basedados.xlsx")$ data = dataset.parse(0)
import sys$ sys.version
sales_2015 = df_2015.groupby(by=["store_number"], as_index=False)
paired_df_grouped.sort_values('co_occurence', ascending=False, inplace=True)$ paired_df_grouped.loc[:, ['dataset_1', 'all_co_occurence']].head(20)$ paired_df_grouped['top10'] = paired_df_grouped.co_occurence.apply(lambda x: x[:10])$
ind = pd.date_range(start= '01-07-2017',periods= 9,freq= '2W-MON')$ ind
df = df.dropna().copy()
plt.plot(total['field4'])$ plt.show()
full_data = pd.concat([train_data,test_data], ignore_index=True)$
S_distributedTopmodel.decision_obj.thCondSoil.options, S_distributedTopmodel.decision_obj.thCondSoil.value
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])
const_cols = [c for c in train_data.columns if train_data[c].nunique(dropna=False)==1 ]$ const_cols
counts_df_clean['tweet_id'] = counts_df_clean.tweet_id.apply(str)$
zf_train = zipfile.ZipFile('train.csv.zip', mode='r')$ zf_test = zipfile.ZipFile('test.csv.zip', mode='r')
autos["odometer_km"].value_counts()$
primary_temp_null_indx=dat[primary_temp_column].isnull()
game_vail=df.loc[:,r]$
df = pd.merge(train,user_logs, on = 'msno', how = 'left')   $
hi_day_delta_index = day_deltas.index(max(delta_val for delta_val in day_deltas if delta_val is not None))$ ans_4 = ('%s had greatest within day difference: %s' % (dates[hi_day_delta_index], day_deltas[hi_day_delta_index]))
sess = tf.Session()$ K.set_session(sess)
number_rows = len(df)$ print('The number of rows in the dataset is {}'.format(number_rows))
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
logistic_mod_country = sm.Logit(df3['converted'], df3[['intercept', 'UK','US']])$ results_country = logistic_mod_country.fit()$ results_country.summary()
df["grade"].cat.categories = ["excellent", "good", "okay", "bad", "miserable", "fail"]$ df
X = df_final[features2]$ X = X.values$ X
graph = facebook.GraphAPI(access_token=access_token, version='2.7')
df.to_csv("../../data/msft_piped.txt",sep='|')$ !head -n 5 ../../data/msft_piped.txt
tsd = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with tsd.open('inpt-forecasting/Transplant Stemsoft data -040117 to 061518.xlsx') as tsd_f:$   tsd_df = pd.read_excel(tsd_f)
coinbase_btc_eur_min.plot(kind='line',x='Timestamp',y='Coin_price_EUR', grid=True);
donor_groups = df3_obs[['Donor ID', 'Donation Received Date']].groupby('Donor ID')$ time_diff = donor_groups.apply(lambda df: df['Donation Received Date'].diff().mean().fillna(0))$ print(time_diff.head())
engine.execute("SELECT  * FROM contractor").fetchall()
oil_nulls_after_interpolation=pd.DataFrame(oil_interpolation[pd.isnull(oil_interpolation.dcoilwtico)])$ pd.DataFrame.head(oil_nulls_after_interpolation)
df_ad_airings_5.shape
df2.query("group=='control'").count()
temp = pd.merge_asof(temp, df_ari, on='Date', by='MatchTimeROT', direction='forward')
print(sample_data.json())
new_page_converted = np.random.binomial(n_new,p_new,10000)/n_new$ print (new_page_converted)
colors = ('blue', 'red', 'gold', 'green', 'c')$ lgd = zip(tweet_df["Tweet Source"].unique(),colors)$
model = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US', 'ab_page']])$ results_2 = model.fit()$ results_2.summary()
daily_df = twitter_daily_df.join(stock_df, ["Day","Company"]).orderBy('Day','Company')
full_act_data = steps.join(heart, how='left')
all_data_vectorized = body_pp.transform_parallel(all_data_bodies)
ab_file2 = pd.read_csv('ab_edited.csv')
df_l['meters_traveled'].groupby(df_l['ship_name']).sum().sort_values(ascending=False).head(10)$
new_page_converted = np.random.choice(a=[1,0], size=n_new, replace=True, p=[p_new, 1-p_new])$
pd.date_range('2015-07-03', '2015-07-10')
autos_p = autos[autos['price'].between(10, 350001)]
t = 0.25$ c_y = 0.6607/(1-t)$ print(c_y)
df_goog['Closed_Higher'] = df_goog.Open > df_goog.Close$
happiness_df=happiness_df.groupby('dates').mean()$
grid_pr_size.describe()
import nltk$ from nltk.corpus import stopwords$ print(stopwords.words('english'))
print(df.group.unique())$ print(df.landing_page.unique())$
inspector = inspect(engine)$ inspector.get_table_names()
import numpy as np$ import matplotlib.pyplot as plt$ import pandas as pd
df.num_comments= df.num_comments.apply(lambda x:0 if x <= 74 else 1)
df2_convert = sum(df2.converted == 1)/len(df2)$ pnew = df2_convert$ pnew
greater_than_p_diff = [i for i in p_diffs if i > p_diff]$ less_than_zero = [i for i in p_diffs if i < 0]$ print("number of elements less than zero is {} of the total {}".format(len(less_than_zero), len(p_diffs)))
openmc.run()
pd.to_datetime('11/12/2010', format='%d/%m/%Y')
timezones = DataSet['userTimezone'].value_counts()[:10]$ print(timezones)
Measurement = Base.classes.measurement$ Station = Base.classes.station
df.shape
grouped_newsorgs = news_sentiment_df.groupby(["News Organization"], as_index = False)
def cosine_similarity(u, v):$     return(np.dot(u, v)/np.sqrt((np.dot(u, u) * np.dot(v, v))))
output = pd.DataFrame({id_label:ids, target_label:predictions})$ print_and_log(output)
p_diffs = np.array(p_diffs)$ null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)$ plt.hist(null_vals);
hyperparam =[0.01, 0.1, 1, 10, 100]$
df['LinkedAccountName'] = df['LinkedAccountId'].apply(num_to_name)$ df['PayerAccountName'] = df['PayerAccountId'].apply(num_to_name)
news = pd.DataFrame([(div.h3.text.rsplit(' - ')[0],str(datetime.strptime(div.h3.text.rsplit(' - ')[1],'%B %d, %Y')),str(div.p).rsplit('<br/>')[0].replace('<p>','')) $         for div in soup.find_all('div',{'class':'bitcoin_history'})])$ news.columns=['Headline','Date','News']
df_upsampled = pd.concat([df_majority, df_minority_upsampled])$ df_upsampled.irlco.value_counts()
p_new_sample = new_page_converted.mean()$ p_old_sample = old_page_converted.mean()$ p_new_sample - p_old_sample
df.info()
iris.head().iloc[:,0]
recipes.description.str.contains('[Bb]reakfast').sum()
pyber_df.head()$
print list_1[:5]$ print REGION['y_m_d'].head()
n_new=df2.query('landing_page == "new_page"').user_id.nunique()$ n_new
df_final['out_content_match_strength'] = df_final['out_x_coord_match'].astype(int) + df_final['out_y_coord_match'].astype(int) + df_final['out_beg_match'].astype(int) + df_final['out_end_match'].astype(int)$
files4= files4.rename(columns={'jobid':'jobId'})$ files4['jobcandidate']=files4['jobId'].astype(str)+'-'+files4['candidateid'].astype(str)
lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics = topics)
df3['timestamp'] = pd.to_datetime(df3['timestamp'])$ df3.info()
vocab = vectorizer.get_feature_names()$ print(len(vocab))$ print(vocab[:10])
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(therapist_duration, params=params, freq=31, title='Therapist Decomposition')
df2=pd.read_csv("location.csv")
import matplotlib as mpl$ mpl.get_backend()
data_vi[['VIOLATIONS']].plot(figsize=(8,4),$                                style=['-']);
engine=create_engine(seng)$ data3 = pd.read_sql_query('describe actor;', engine)$ data3
print(r.json())
year_ago = dt.date.today() - dt.timedelta(days=730)$
f_regex = re.compile('([^a-z]|^|$)f([^a-z]|^|$)')$ m_regex = re.compile('([^a-z]|^|$)m([^a-z]|^|$)')$ num_regex = re.compile('[0-9]+')
my_cols = list(new_df.columns)$ print(my_cols)
small_frame = loan_stats[1:3,['loan_amnt','installment']]$ single_col_frame = loan_stats[1:3,'grade']$ small_frame.cbind(single_col_frame) 
n_old = df2[df2.group == 'control'].count()[0]$ n_old
with open('components/pop_models/excitatory_pop.json') as exc_data:$     exc_prs = json.load(exc_data)$ pprint.pprint(exc_prs)
df_gnis_test = df_gnis.dropna(axis=0, subset=['PRIM_LAT_DEC','PRIM_LONG_DEC'],thresh=1)$ df_gnis_test.shape
with open(marvel_comics_save, mode='w', encoding='utf-8') as f:$     f.write(wikiMarvelRequest.text)
table = Table.read("../datasets/catalogs/fermi/gll_psc_v16.fit.gz")$
func_vals = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).filter(Measurement.station == "USC00519281").all()$ func_vals$
s3 = pd.Series(np.arange(12, 14), index=[1, 2])$ pd.DataFrame({'c1': s1, 'c2': s2, 'c3': s3})
plt.hist(np.log(threeoneone_census_complaints[threeoneone_census_complaints['avg_fix_time_sec']>0]['avg_fix_time_sec']),bins=100)$ plt.show()
engine = create_engine("sqlite:///hawaii.sqlite", echo = False)
file_attrs_string = str(list(hdf5_file.items()))$ file_attrs_string
time_chart = vincent.Line(school_per_minute)$ time_chart.axis_titles(x='Time', y='Hashtag frequencies')$ time_chart.display()
Tip_percentage_of_total_fare = taxiData.Tip_amount / taxiData2.Fare_amount
preds = gbm.predict(T_test_xgb)
engine = create_engine("sqlite:///Resources/hawaii.sqlite")
df2 = df2.drop_duplicates(['user_id'], keep='first')$ df2.info()
json_data = r.json()$ print(json_data)
sns.violinplot(data=pd.DataFrame(pred)[pd.DataFrame(pred) > 0].dropna(), jitter=True)$ plt.ylabel('stopped_minutes')$ plt.xlabel("distribution with zero (min) removed")$
from sklearn.cross_validation import train_test_split$ features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.1)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closerToBotOrTop'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], 'FromTopWell', 'FromBotWell')
apple.asfreq('D',method = 'pad')$ apple.head()
thistweet=pd.read_csv("../output/tweet_with_location.csv")$
import time$ print (time.strftime("%Y%m%d"))
p_val = (p_diffs > observation_mean).mean()$ p_val
s4g.groupby('Symbol')
station_count = session.query(Measurement.station).distinct().count()$ print('There are',station_count,'stations')
dfs = [breed_predict_df_clean, counts_df_clean, archive_df_clean]$ df_final = reduce(lambda left,right: pd.merge(left,right,on='tweet_id'), dfs )$
annual_returns.head() $
words = [w for w in words if not w in stopwords.words("english")]$ print (words)
df.isnull().any()
data[data.density > 10]
c_df['Date']= c_df.index$
df = pd.read_csv('ab_data.csv')$ df.head()
segDataGrouped = segmentData.groupby(['lead_mql_status','opportunity_month_year', 'opportunity_stage']).opportunity_stage.count()$ oppstagepct = segDataGrouped.groupby(level=[0,1]).apply(lambda x: x / float(x.sum())); oppstagepct.head()
siteFeatures = read.getSamplingFeatures(type='Site')$ df = pd.DataFrame.from_records([vars(sf) for sf in siteFeatures if sf.Latitude])
total_users1 = df2.nunique()['user_id']$ print("There are {} unique Users IDs in df2".format(total_users1))
stations=session.query(Station.station, Station.name).all()
 $ session.query(Measurement.id, func.min(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
print list(label_encoder.inverse_transform([0,1]))$ model.predict_proba(np.array([0,0,1,0,0,0,0,0,0,5,0])) 
myTxns.isnull().sum()
df.iloc[:,1:3] = df.iloc[:,1:3].apply(pd.to_datetime, errors='coerce')$ df['Trip_duration']=df['Lpep_dropoff_datetime']-df['lpep_pickup_datetime']
models.save('models')
tia['date'] = tia['date'].apply(lambda x: x[14:])$ tia['date'][0]
rhum_us_full = rhum_nc.variables['rhum'][:, lat_li:lat_ui, lon_li:lon_ui]
logit = sm.Logit(df2.converted, df2[['intercept', 'new_page', 'CA', 'UK']])$ result = logit.fit()$ result.summary()
data.userScreen.nunique()
scaler = MinMaxScaler(feature_range=(0, 1))$ dataset = scaler.fit_transform(dataset)
from cfg import mv_buy_hold$ (target_pf.iloc[-1]['date'] - target_pf.iloc[0]['date']).days$ period = 'month'$
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean[twitter_archive_enhanced_clean.retweeted_status_id.isna()]$ twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'], axis = 1)
jobs_data.to_pickle('D:/CAPSTONE_NEW/jobs_data_clean.pkl')$ import gc$ gc.collect()
data_df['tone'] = data_df.apply(tone, axis=1)
data_AFX_X['diff_rows'] = data_AFX_X['Close'].diff(periods=-1)$ data_AFX_X.describe()$ min(data_AFX_X['diff_rows'])
df = pd.DataFrame(pd.read_csv(filepath, header = None, index_col = False))
df_cust_data = pd.read_excel('mazda_dataset.xlsx','Customer data')$ df_prospects_data = pd.read_excel('mazda_dataset.xlsx','Prospects data')
results = [i.lower() for i in names]$ df = pd.DataFrame(results,columns={'Sample'})$ df['Number'] = df.index
new_treatment = df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page == "new_page")')['user_id'].count()$ print("number of times the new_page and treatment don't line up : {}".format(new_treatment))
amznnews_matrix = count_vectorizer.transform(df_amznnews.headline_text)$ amznnews_pred = nb_classifier.predict(amznnews_matrix)
print(autos['odometer_km'].unique().shape)$ print(autos['odometer_km'].describe())$ print(autos['odometer_km'].value_counts().sort_index(ascending=False))
df_arch_clean['source'].value_counts()$
temp_data = pd.DataFrame(clf.predict(X_all), index = data.index)$ data['renta'].fillna(temp_data[0], inplace=True)$ del temp_data
tweet_df.retweeted.value_counts()$
train_df = effectiveness(train_df)$ print(train_df.head(5))
time_chart = vincent.Line(per_minute)$ time_chart.axis_titles(x='Time', y='Hashtag frequencies')$ time_chart.display()
sp500.ix[['MSFT', 'ZTS']]
df2.converted.mean()
store_items = store_items.rename(index = {'store 3': 'last store'})$ store_items
miner = TweetMiner(twitter_keys, api, result_limit=20)$ sanders = miner.mine_user_tweets(user='bernisanders', max_pages=10)
londonDFSubsetWithCounts = londonDFSubset.merge(grouped, on='OA11CD')
csv2 = pd.read_csv(csv_files[1])$ print(csv2.head())
series.index = list('abcdefghij')
predict_day = weather_x.index[-60]
dicttagger_food = DictionaryTagger(['food.yml'])
df2.drop(labels = 2893, axis=0, inplace=True)
df.info()
output_fn = "NewsMedia.csv"$ result.to_csv(output_fn)
r1 = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=")$
mfp_boss.build_prior()
df['MeanFlow_cfs'].plot();
!rm microbiome.sqlite3
la_inspections = pd.read_csv('/Users/salilketkar/thinkful/LA_health_score_inspections.csv')
user_ratings = df_ratings[(df_ratings.user_id==9)]$ user_ratings
df.isnull().sum()
liquor=pd.read_csv('../Assets/Iowa_Liquor_sample.csv',parse_dates=['Date'],infer_datetime_format=True)
s.index
print("This dataset contain {} rows, and {} columns.".format(df.shape[0],df.shape[1]))
reddit.Comments.value_counts(ascending=False).head(25) #just seeing the distribution of the number of comments$
todays_datetimes = [datetime.datetime.today() for i in range(100)]
train['frequent_author'] = train.author.isin(frequent_authors.index).astype(int)$ train.groupby('frequent_author').popular.mean()
density = 1.32
players = player_p1.append(player_p2)
twitter_archive.sample(4)
mask_group = df.group == 'control'$ mask_page = df.landing_page == 'old_page'$ df2 = df[mask_group == mask_page]
df2.jpg_url.duplicated().value_counts()
new_stops = new_stops.drop(new_stops.index[[4473,4474]])
page = requests.get(url, timeout=5)
from app.crawler import Crawler$ crawler = Crawler()$ crawler.pull_save_data()
t3.head(5)
df2 = pd.read_csv('../output/data/expr_2/expr_2_pcs_nth_1_div_51_04-18.csv', comment='#')
df_en.to_csv('bitcoin_eng.csv') # this has all 4 bitcoins$
city_avg_fare = pd.merge(citydata_df, city_avg_fare, on='city', how='left')$
lr1 = LogisticRegression(random_state=20, max_iter=10000, C=0.5, multi_class= 'ovr', solver= 'saga')$ lr1.fit(X, y)$ lr1.score(X_test, y_test)
p_old = df2['converted'].mean()
print login_response.status_code$ login_page = BeautifulSoup(login_response.content, 'lxml')
n_new = df2.query(('landing_page == "new_page"')).count()[0]$ n_new
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
display(data.describe())
ice = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with ice.open('inpt-forecasting/Inpatient Census extract - PHSEDW 71118.csv') as ice_f:$   ice_df = pd.read_csv(ice_f)
warnings.filterwarnings('ignore')$
columns = ['id', 'datetime', 'created_at', 'text', 'file', 'twitter_handle']$ df = pd.DataFrame(columns=columns)
airline_df['sentiment'] = airline_df['tweet'].apply(lambda tweet: NBClassifier.classify(extract_features(getFeatureVector(processTweet2(tweet)))))$
dr_new_patient_8_to_16wk_arima = dr_new_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']]$ dr_new_patient_8_to_16wk_arima.index = dr_new_patient_8_to_16wk_arima.index.date
reddit = praw.Reddit(client_id='yRkwuIWiThfXeQ',$                      client_secret='Kond5JUokaOkKKXu6LipwN_CtcM',$                      user_agent='weekdayjason')
url = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=xQWdR8uvz6QJeQ6gqN5g&start_date=2017-01-01&end_date=2017-12-31')$ json_data = url.json()$ data = json.dumps(json_data['dataset'])$
stop_words_update = list(pipe_cv.get_stop_words())$ stop_words_update.append('pron')$ stop_words_update.append('aa')
Base.metadata.create_all(engine)
cr_control = df2[df2['group'] == 'control']['converted'].mean()$ cr_control$
df2.shape[0]-df2['user_id'].nunique()$ df2[df2['user_id'].duplicated() == True]
for tweet in public_tweets:$     print (tweet.text + " : " + str(tweet.created_at) + " : " + tweet.user.screen_name + " : " + tweet.user.location) $     print (" ")
print(np.exp(results.params))$ print('\n')$ print(1/np.exp(results.params))
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new['country'].astype(str).value_counts()
tcat = pd.read_csv(filepath + 'cat_tweets.csv')$ tdog = pd.read_csv(filepath + 'dog_tweets.csv')$ tcat.head()
df.set_index('datetime',inplace=True)
state_party_df = pd.concat(state_DataFrames_list, axis=1, join='outer')$ state_party_df.columns = state_keys_list$ state_party_df.head(20)
dfQ1['flow_MGD'] = dfQ1['meanflow_cfs'] * 0.64631688969744$ dfQ2['flow_MGD'] = dfQ2['meanflow_cfs'] * 0.64631688969744$ dfQ3['flow_MGD'] = dfQ3['meanflow_cfs'] * 0.64631688969744
top_subreddit = subreddit.new(limit=None)
print()$ print(df.groupby('Class').size())
X = reddit_master['title']$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
issue_comments_table = issue_comments_table.drop_duplicates()
np.exp(-0.0150)$ 1/np.exp(-0.0150)
crimes.groupby(['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']).size()
merge[merge.columns[11]].value_counts()$
df.values
df.loc[df['seller'] != 'privat'].shape
np.exp(results.params)
import sys$ import os$ from urllib.request import urlretrieve
p_diffs = np.asarray(p_diffs)$ (p_diffs > actual_diff).mean() #In how many cases the calculated differences (from the simulations) were higher than the actual differences?
preview.to_csv("example.csv")
CON=pd.read_csv('C:/Users/mjc341/Desktop/UMAN 1507 Monthly INQ summary Report/Interactions.Opportunity.Term.csv',skipfooter=5,encoding='latin-1',engine ='python')$
ARStores = $ ARStores.head()
for post in posts.find():$     print(post)
p_value = (null_vals>p_obs_diff).mean()$ p_value
min(close, key=close.get)
prcp_df.plot()$ plt.show()
datasets_ref = pd.read_csv('../list_of_all_datasets_dgfr/datasets-2017-12-13-18-23.csv', sep=';')$ datasets_slug_id = datasets_ref.set_index('slug')['id'].to_dict()$ datasets_id_slug = datasets_ref.set_index('id')['slug'].to_dict()
response = requests.get("https://data.bitcoinity.org/markets/bidask_sum/24h/USD/bitfinex?bp=1&bu=c&r=minute&t=m")
from sklearn.model_selection import train_test_split$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
import scipy.stats$
print('Get subset data of rows no. 11, 24, 37')$ df.loc[[11,24,37]] 
GIT_LOCATION = \$ 'C:\\Users\\donrc\\AppData\\Local\\GitHub\\PortableGit_f02737a78695063deace08e96d5042710d3e32db\\cmd\\git.exe'$
print('Very short time travels number %d.'%len(train[train['duration'] <= 60]))
fund_avgs = np.array(merged_df.groupby('Funding Type')["Money Raised"].median()/1000000)$ fund_avgs
df_enhanced = df_enhanced.query('retweeted_status_id == "NaN"')
all_sets.cards["XLN"].shape
df_teacher_info = df_teacher_info[df_teacher_info.awj_teacher_id.isin(pick_list)]$ df_teacher_info.drop(["country", "timezone", "degree_and_university", "child_exp"], axis=1, inplace=True)
sns.factorplot('SA','len',data = data, kind = 'point', size = 6)
data_links = ['https://raw.githubusercontent.com/sinny777/hukam/master/machinelearning/tf-nlc-model/data/data.csv']
gmap.heatmap(data['Latitude'], data['Longitude'])
cwd1 = os.getcwd()$ print cwd1$
measurement_df = pd.read_sql("SELECT * FROM measurement", conn)$ measurement_df['date'].max()$ measurement_df.head()
QLESQ = QLESQ[(QLESQ["level"]=="Level 1") & (QLESQ["days_baseline"] <= 7)]$ QLESQ.shape
def add_to_list(word_list, dictionary):$     for each in word_list:$         dictionary.append(each)
master_df.name.value_counts().nlargest(10)
query = pgh_311_data_merged['Category'] == "Road/Street Issues"$ pgh_311_data_merged[query]['Issue'].value_counts()
fcc_nn.head()
cityID = '0570f015c264cbd9'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Louis.append(tweet) 
p_diff=(new_page_converted.mean()) - (old_page_converted.mean())$ p_diff
rows=session.query(Adultdb).filter_by(native_country='Outlying-US(Guam-USVI-etc)')
rdf.loc[pd.isna(rdf.accident_counts),'accident_counts'] = 0.0
metadata['reflectance_scale_factor'] = float(refldata.attrs['Scale_Factor'])$ metadata
df_new.drop('user_id', axis=1, inplace=True)$ df_new = df_new.fillna(0)
df_detail = df_detail.fillna(0)
exiftool -csv -createdate -modifydate cisuabf6/cisuabf6_cycle1.MP4 cisuabf6/cisuabf6_cycle2.MP4 cisuabf6/cisuabf6_cycle3.MP4 cisuabf6/cisuabf6_cycle4.MP4  > cisuabf6.csv
soup.p
print(dictionary.token2id['like'])
month_zero = autos.loc[autos["registration_month"] == 0, :].index$ autos = autos.drop(index = month_zero)
train['title_author'] = train.title.str.cat(train.author, sep=' ')$ new['title_author'] = new.title.str.cat(new.author, sep=' ')
very_pop_df = au.filter_for_support(popular_trg_df, min_times=6)$ au.plot_user_dominance(very_pop_df)
from dateutil import parser$ date = parser.parse("4th of July, 2017")$ date.strftime('%A')
import datetime$ date = datetime.datetime(year=2017, month=6, day=13)$ date
p_old = df2.converted.mean()$ print(p_old)
set(dat.columns) - set(temp_columns+ incremental_precip_columns+ general_data_columns+ wind_dir_columns)
import pandas as pd$ pd_cat = pd.get_dummies(ibm_hr_cat.select("*").toPandas())$ pd_cat.head(3)
(p_diffs > obs_diff).mean()
Measurements = Base.classes.hawaii_measurement$ Stations = Base.classes.hawaii_station$
df = pd.read_csv("twitter-archive-enhanced.csv")
shows.dtypes
query.get_dataset(db, id=ds_info["DatasetId"][0], columns="ndarrays", get_info_items=True)
df2[df2['landing_page'] == 'new_page'].user_id.count()/df2.user_id.count()
merge_table = pd.merge(extreme_dates1, main_df, on= "Date", how = "left")$ merge_table
import pandas as pd$ import matplotlib.pyplot as plt$ import seaborn as sns
plt.style.use('default')$ %pprint
import matplotlib.cm as cm$ lines_c, limits_c, average_c, *_ = cm.Paired.colors
article_divs = [item.find('div',{'class':'article--container'}) for item in soups]
img = mpimg.imread('input/c(DE-DK-NO-SE).png')
stations = pd.read_csv("hawaii_stations.csv")$ stations.head()
import numpy as np$ import matplotlib.pyplot as plt
CREATE OR REPLACE VIEW all_rooms AS$     SELECT h.hotel_id, h.hotel_name, r.room_num, r.room_type_code, r.occupancy, r.rate $
df['Injury_Type'].value_counts()
cityID = '7d62cffe6f98f349'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Jose.append(tweet) 
pd.DataFrame(data['data']['children'])    # will give us the kind of data as well as the subreddit id
df['y'].plot()
for model_name in nbsvm_models.keys():$     valid_probs[model_name] = nbsvm_models[model_name].predict_proba(X_valid_cont_doc)
number_rows = df.shape[0]$ print("Number of rows: {}".format(number_rows))$
convert_me = "2016bbb12---15"$ datetime.datetime.strptime(convert_me, "%Ybbb%m---%d")
mni =raw.iloc[50610:50619]
response_cpi_all = requests.get(url_cpi_all)
abunai_merged_arrays_df=abunai_merged_arrays_df.merge(abunai_tweet_array_df,left_index=True, right_index=True)$ abunai_merged_arrays_df=abunai_merged_arrays_df.merge(abunai_date_array_df,left_index=True, right_index=True)
mean = mc_estimates.expanding().mean()
cityID = 'cb74aaf709812e0f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tulsa.append(tweet) 
support.sort_values("amount", ascending=False).head()
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
df2 = df2.drop_duplicates(subset='user_id', keep='last')
df2[df2.user_id == 773192]
my_df_free1.iloc[100:110]
grouped_months_Sl = Total_deaths_Sl.groupby(Total_deaths_Sl['date'].apply(lambda x: x.month))$
init = tf.global_variables_initializer()$ sess.run(init)
%matplotlib inline$ commits_per_year.plot(kind='line', title='Commits per year', legend=False)
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()
tsprior = pd.Timestamp('1/1/2016')$ tsprior2 = pd.Timestamp ('1/1/2017')$ tsprior3 = pd.Timestamp('1/1/2018')
actual_converted_diff = p_treatment_converted - p_control_converted$ plt.hist(p_diffs)$ plt.axvline(x=(actual_converted_diff), color='red')$
P.plot_1d_hru(0,'airtemp')
merged['onpromotion'].value_counts(normalize=True,dropna=False)
display(data.tail(10))
df1['label'] = df[forcast_col].shift(-forcast_out)$ df1['label'].head()
ab_df2.query('group == "control"')['converted'].mean()
normal_dist = np.random.normal(0, np.std(p_diffs), p_diffs.size)$ plt.hist(normal_dist);$ plt.axvline(diff, color='red')
[x for x in tweets_df.userLocation if 'Costa Rica' in x]$
pd.DataFrame({'population': population, 'area': area})
result = DBRunQuery(q)$ print("Result = ")$ print(json.dumps(result, indent=2))
clf = LogisticRegression(fit_intercept=True).fit(X, y)
p_new = df2['converted'].mean()$ p_new
1/np.exp(-0.0123)$
lr_y_score = model.predict_proba(X_test)[:, 1] #[:,1] is formatting the output$ lr_y_score
df = df.sort_values(by=['date'])$ df = df.rename(columns={'prcp':'precipitation'})$ df.head(10)
time_sentiment = data_sentiment.groupby(["created_at"])["sentiment_score"].mean()$ time_sentiment = time_sentiment.to_frame().reset_index()$ sentiment_plot_data = time_sentiment.groupby(pd.Grouper(key='created_at', freq='300s'))["sentiment_score"].mean()
filtered.to_csv('update/filtered-users.csv', index=False)
df['reviw'] = df['review'].apply(preprocessor)
df3.country.unique()
df_new['country'].value_counts()
new_cases_liberia_concat.columns = ["Date", "New Case/s (Suspected)","country","Date1","New Case/s (Probable)","country","Date2","New case/s (confirmed)","country","Total_new_cases_liberia"]$ Total_new_cases_liberia =new_cases_liberia_concat[['Date','Total_new_cases_liberia']]$ Total_new_cases_liberia.head()$
df_countries.info()
temp_df2['timestamp'].max() - temp_df2['timestamp'].min()
df_clean['tweet_id'] = df_clean.tweet_id.astype(str)$ df_clean.info()
sim_p_old = old_page_converted.mean()$ sim_p_new = new_page_converted.mean()
for i, row in breakfastlunchdinner.iterrows():$     breakfastlunchdinner.loc[i, 'day_sales'] = sum(row[1:]) * .002 $ breakfastlunchdinner
tranny.values
data.groupby(['Year'])['Salary'].sum()$ data.groupby(['Year'])['Salary'].mean()
iris.head().iloc[:,:1]
plt.style.use('default')
data.dropna(axis=1)
image_predictions_copy = image_predictions_copy[image_predictions_copy.p1_dog == True]
print 'DataFrame conversion_data (FEATURE MATRIX)', conversion_data.shape, type(conversion_data)$ conversion_data.head()
df_twitter_copy.ix[df_twitter_copy.name.isin(no_name_list), 'name'] = 'None'
tmdb_movies_genre_revenue.shape
pd.crosstab(df_result.launched_year, df_result.State).plot.bar()
df = pd.read_csv('ab_data.csv')$ df.head()
ts.head()
s12.fillna(0)$ s12.fillna(s12.median())$ s12.dropna()
INSERT INTO payment (booking_id, payment_date, payment_method, payment_amount)$ VALUES (8, TO_DATE('2017-09-10', 'YYYY-MM-DD'), 'BPay', -741.96)$
df2['converted'].mean()$
df['cowbell'] = df['body'].apply(lambda x: len([x for x in x.split() if '(((' in x]))
metadata_df[1]['Variable'].fillna(method='ffill', inplace=True)$ metadata_df[1].head()
results.summary()
gender_count = df_ts_alltype.groupby(['gender','type','brand'])['text'].count()$ gender_count
table = pd.crosstab(df["grade"], df["loan_status"], normalize=True)$
data["time_up_sec"] = pd.to_datetime(data["time_up_clean"], format= "%H:%M:%S")
df_from_json = pd.read_json("../../data/stocks.json")$ df_from_json.head(5)
saved = savedict.copy()$ ex = saved['2017-11-15']$ print("Choose Campaign Name: ",ex['Campaign Name'].unique())$
pop.loc['California': 'New York']
plt.hist(name_array)$ plt.title('The distribution of the names\' length')$ plt.show()$
data.loc[[pd.to_datetime("2016-12-01"), pd.to_datetime("2016-12-03")]]
pd.unique(tag_df.values.ravel())
from pandas_datareader import wb$ all_indicators = wb.get_indicators()$ all_indicators.ix[:,0:1]
autos.head()$
metatable.to_sql('HS_MetaTable', engine, if_exists='replace', index=False, schema='dbo')$ print("finished")$
scores[scores.RottenTomatoes == scores.RottenTomatoes.max()]
emb, summ = seq2seq_inf.generate_issue_title(txt)$ summ
DummyDataframe2 = DummyDataframe2.apply(lambda x: calPercentage(x, "Token_Count", ["Positiv", "Negativ"]), axis=1)$ DummyDataframe2
actual_old = df2.query('converted == 1 and landing_page=="old_page"').count()[0]/n_old
session.query(func.min(Measurement.tobs),$               func.max(Measurement.tobs),$               func.avg(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
directory = 'C://Users//Joan//OneDrive//capstone//'$ dta = pd.read_csv(directory + 't_asv.csv')
actual = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ actual
df2.drop_duplicates (inplace=True)
engine.execute('SELECT * FROM measurements LIMIT 15').fetchall()$
from dateutil import parser$ date = parser.parse("4th of July, 2015")$ date
songLink = 'https://www.youtube.com/embed/'+soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href'][9:]$ HTML('<iframe width="560" height="315" src='+songLink+' frameborder="0" allowfullscreen></iframe>')
print(s1.head(3))$ print()$ print(s1.tail())
temp_df = pd.DataFrame(temps)
pax_raw.paxstep.max() <= step_threshold
pred_labels = rdg.predict(test_data)$ print("Training set score: {:.2f}".format(rdg.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(rdg.score(test_data, test_labels)))$
results = api.search(q="%23blues")
lb = aml.leaderboard$ lb
avg_neighborhood_rt[avg_neighborhood_rt>14].sort_values()
df["timePeriodStart"] = pd.to_datetime(df.timePeriodStart)
assembler = VectorAssembler(inputCols = feature_col, outputCol = "features")$ assembled = assembler.transform(ibm_train)
uber = pd.read_csv('https://assets.datacamp.com/production/course_2023/datasets/nyc_uber_2014.csv')$ print(uber.shape)$ print(uber.head())
date + pd.to_timedelta(np.arange(12), 'D')
print("Mean absolute percentage error: %.3f" % mean_absolute_percentage_error(y_test, y_pred) + '%')
merge[merge.columns[45]].value_counts()$
from pandas.plotting import autocorrelation_plot$ autocorrelation_plot(CH_electric['Total_Demand_KW'])
master_df['day of week']=master_df['day of week'].astype(str)
df_new2['intercept'] = 1$ df_new2.head()
plt.pie(totalRides, labels = labels, explode = explode, $         colors = colors, autopct="%1.1f%%", shadow=True, startangle=180)$ plt.show()
weather = weather[weather.zip_code == 94107]
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=10, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
data = pd.read_csv('data/analisis_invierno_3.csv')
adj_close_start = adj_close[adj_close['Date']==end_of_last_year]$ adj_close_start.head()
pca = PCA(2)$ X = pca.fit_transform(new_df.ix[:,1:20])$
f = open("json_example.json","w")$ json.dump(obj, f)$ f.close()
df2 = df2.drop_duplicates(subset=['user_id'])$ print(df2.shape)
attend_with.to_csv('../data/attend.csv')
df_pr.sort_values('sentiment').iloc[:10]
import functools$ crit_numstarters_8_11= df_factors_PILOT['x8_num_starters_pp'].isin([8,9,10,11])$ crit_racetype_M = df_factors_PILOT['race_type'].isin(['C','M'])
engine.score_all_on_classifier_by_key("nhtsa_classifier")
df1 = pd.DataFrame(last_year, columns=['date', 'precipitation'])
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
q_all_pathdep = c.retrieve_query('https://v3.pto.mami-project.eu/query/8da2b65bd4f7cd8d56d90ddfcd85297e8aac54fcd0e04f0a0fa51b2937b3dc62')$ q_all_pathdep.metadata()
print(df2['landing_page'].value_counts())
sns.violinplot(x="borough", y="complaint_type",data = samp311)
params =  {'reg_alpha': 0.1, 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'min_child_weight': 1$           , 'n_estimators': 300, 'reg_lambda': 1.0, 'random_state': 1, 'max_depth': 4} $ reg_final = xgb.XGBRegressor(**params).fit(X, y)#**params$
autos.info()$ autos.head()
!date$ sns.distplot(pd.DataFrame(pred)[pd.DataFrame(pred) > 0].dropna(), kde=False, rug=True, bins=30, axlabel='predicted stopped_minutes')  # I want the Hue to be in_port$ !date$
np.exp(-0.0150)
d.groupby('label')['tweet_id'].count()
np.mean(shows['first_year'].dropna())
results.summary()
page_size = 200$ response = client.get('/tracks', q='footwork', limit=page_size,$                     linked_partitioning=1)$
nonzero = grades.drop(grades.index[(grades.Mark == 0) | (grades.ATAR == 0)])$ nonzero.plot(x='ATAR', y='Mark', color='red', kind='scatter')
df_sb = pd.read_csv("sobeys_all.csv", encoding="latin-1")
d=soup.find('li').get_text()$ des
df_merged = chgis.merge(new_data_and_output, how='outer', left_on=chgis_name_match_field, right_on=input_name_match_field, indicator=True)$ df_merged = df_merged[df_merged['_merge'] != 'left_only']
controlp = df2[df2['group'] == 'control']['converted'].mean()$ controlp
pd.set_option('max_colwidth', 500)
tipsDF = pd.read_csv('yelp_tips.csv', index_col=False, encoding='UTF-8')$
df_release = pd.read_csv( pwd+'/releases.052317.csv', encoding='utf-8') #earning release?
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, wait_on_rate_limit=True)
btc.describe()
df.query('landing_page == "new_page" & group == "control"').shape[0] + df.query('landing_page == "old_page" & group == "treatment"').shape[0]
df_fireworks = df[df['Complaint Type'] == 'Illegal Fireworks']$ df_fireworks.groupby(df_fireworks.index.month)['Created Date'].count().plot(kind="bar")$
n_old_page = len(df2.query("group == 'control'"))$ print(n_old_page)
1/np.exp(results.params[1])
autos.registration_year.describe()$
pulledTweets_df['emoji_enc_text'] = pulledTweets_df['Processed_tweet'].apply(declump_emojis_in_text)$ pulledTweets_df['emoji_enc_text'] = pulledTweets_df['emoji_enc_text'].apply(encode_emojis)$ pulledTweets_df['emojis'] = pulledTweets_df['Processed_tweet'].apply(extract_emojis)
records4 = records.copy()
baseball.corr().replace(1, np.NaN)
print("Probability of individual user converting is", df2.converted.mean())
df['2015-06-03']['battle_deaths'].mean()
match[match.iloc[:,55 :66].notnull().any(axis=1)].iloc[:5,55 :66] # get rows with non-nulls in columns 55 :66
df3[['ab_page', 'not_ab_page']] = pd.get_dummies(df3['landing_page'])$ df3 = df3.drop('not_ab_page', axis=1)$ df3['intercept'] = 1
model.save('/tmp/model.atmodel')$
mRF = H2ORandomForestEstimator()$ mRF.train(['sepal_len','sepal_wid','petal_len','petal_wid'],'class',train)
img_url_valles = soup.find('div', 'downloads').a['href']
gbm = GradientBoostingClassifier(max_depth = 8, n_estimators= 200, max_features = 0.3)$ gbm.fit(X_reshaped, y)$ getAUC(gbm, X_reshaped, y, X_test_reshaped, y_test)
print("Number of rows in slice", reviewsDFslice.shape)
df.iloc[0:3, [0, 2]]
x_train, x_test, y_train, y_test = train_test_split(x, y , test_size=0.4, random_state=2)
dedups.head()
df_2010['bank_name'] = df_2010.bank_name.str.split(",").str[0]$
print(r.json()['dataset_data']['column_names'])#['start_date'])#
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)$
n_new = df2[df2.landing_page == 'new_page'].count()['user_id']$ n_new
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ca','uk']])$ results = logit_mod.fit()
df.query('director == "Unknown"').head(20)$ df.query('production_companies == "Unknown"').head(20)$ df.query('genres == "Unknown"').head(20)
result.summary()
df = train[columns].append(test[columns])
train_data.to_feather('data/train_data')$ test_data.to_feather('data/test_data')
(act_diff < p_diffs).mean()
autos.describe(include="all")$
BASE_URL = 'https://orgdemo.blastmotion.com'
hit_tracker_df = clean_merge_df.loc[clean_merge_df["Reached Number One"] == "Number One Hit",:]
len(df_new.UWI.unique())
with open("Valid_events_eps0.7_5days_500topics","rb") as fp:$     Valid_events = pickle.load(fp)
commit_stats2['diff_dev_commit'] = commit_stats2.apply($     lambda x: 0 if x['author_email']==x['commiter_email'] else 1, axis=1)
plt.hist(review_df.fake_review)$ plt.show()
df_new[['UK','US']] = pd.get_dummies(df_new['country'])[['UK','US']]
df2_dummy = pd.get_dummies(data = df2, columns = ['landing_page'], prefix = ['ab_page'])$ df2_dummy.head()
df_tweet_clean=df_tweet.copy()$
X = dfX[fNames].values$ y = dfY.values
soup = bs(response.text, "html.parser")
nf = 1e-1$ loss = tf.reduce_mean(tf.squared_difference(Ypred*nf,Y*nf))$
graph.run(tweet_query,param=list_to_merge[23000::])$
col = ['msno','plan_list_price','payment_plan_days','actual_amount_paid','payment_method_id','transaction_date','membership_expire_date']$ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions', col) # 20,000,000$
tweet_summary = tweet_df.groupby(["Tweet Source"], as_index = False)["Compound Score"].mean().round(3)$ tweet_summary
df = df.drop(['cast', 'homepage', 'tagline', 'keywords', 'overview', 'imdb_id'], axis=1)
scores[scores.IMDB == max_IMDB]
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s')$ print(git_log.timestamp.describe())
twitter_archive_full['retweet_count'] = twitter_archive_full.retweet_count.fillna(0.0).astype(int)$ twitter_archive_full['favorite_count'] = twitter_archive_full.favorite_count.fillna(0.0).astype(int)$
logit_mod1 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'CA','US']])$ result1 = logit_mod1.fit()$ result1.summary()
logit_mod = sm.Logit(df3['converted'], df3[['intercept', 'CA', 'UK']])$ results = logit_mod.fit()$ results.summary()
errors['datetime'] = pd.to_datetime(errors['datetime'], format="%Y-%m-%d %H:%M:%S")$ errors['errorID'] = errors['errorID'].astype('category')
df.index.values
df=json_normalize(data["dataset"], "data")$ df.columns = col_names$ df.head(3)
new_page_converted.sum()/len(new_page_converted) - old_page_converted.sum()/len(old_page_converted)$
most_freq = data[(data['longitude'] == 103.93700000000001) & (data['latitude'] == 1.34721)]
df_old = df2.query('group == "control"')$ n_old = df_old.shape[0]$ n_old
model.wv.most_similar(positive=['pasta','chinese'], negative=['italian'])$
summed.fillna(method='pad')  # The NaN column remained the same, but values were propagated forward$
[random_date(pd.datetime.now() - pd.offsets.Day(10), pd.datetime.now()) for _ in range(10)]
tweet_archive_clean['text'] = tweet_archive_clean['text'].apply(lambda x: x.split('https')[0])
dummies = pd.get_dummies(plate_appearances['events']).rename(columns=lambda x: 'event_' + str(x))$ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)
df.isnull().values.ravel().sum()
a = tweets.apply(lambda row: countOcc(row['tokens']), axis=1)$ sorted_x = sorted(occ.items(), key=operator.itemgetter(1), reverse=True)$
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new.head()
new_page_converted = np.random.binomial(1, 0.1196, 145310)
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S_1dRichards.executable = excutable +'/summa.exe'
show_data.plot(kind="bar")
total_ride=merge_table_citytype["ride_id"].count()$ total_ride.head()
df['MeanFlow_cms'].describe()
dftotal=pd.concat([df1,df1_dummies],axis=1)$ dftotal.head()
suburban_summary_table = pd.DataFrame({"Average Fare": suburban_avg_fare,$                                     "Total Rides": suburban_ride_total})$ suburban_summary_table.head()
auth_endpoint = 'https://iam.bluemix.net/oidc/token'
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = log_mod.fit()
data.learner_id.value_counts()[:1]
pivoted_data.cumsum().plot(figsize=(10,10))
if not database_exists(engine.url):$     create_database(engine.url)$ print(database_exists(engine.url))
fb = cb.organization('facebook')
keep_vars = set(no_hyph.value_counts().head(12).index)
pnew = df2[df2['landing_page']=='new_page']['converted'].mean()$ pnew
ldf['Traded Volume'].median()
plt.barh(xrange(0,2000), df['polarity'], color='k')$ plt.xlabel('Sentiment Polarity')$ plt.ylabel('index of tweet')
geometry.export_to_xml()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ req = requests.get(url)
vio2016 = vio2016.rename(index=str, columns={"description_y": "num_vio"})$ ins2016 = pd.merge(ins2016, vi, on=["business_id", "date"]).rename(index=str, columns={"description": "num_vio"})$ ins2016
print('#events :', len(all_data.ID.unique()), '#sources :', len(all_data.Source.unique()), '#articles', len(all_data.Article.unique()), '#mention_date', len(all_data.MentionDate.unique()))
 1 - (election_data['pop_2012'].isnull().sum() / len(election_data['pop_2012']))$
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)$ results.summary()
ts1 = pd.Series(np.arange(len(df)), df_Created)$ df_Month = ts1.resample('M').count()$ df_Month.head(9)
Geocoder.geocode(festivals["Location"][9]).valid_address
soup.nav
not_lined_up = df_ab[(df_ab['group']=='treatment') & (df_ab['landing_page']!= 'new_page')|(df_ab['group']=='control') & (df_ab['landing_page']!= 'old_page')]$ not_lined_up.count()[0]$
r = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
grpd = builder.groupby('endcustomerlinefixed_data','party_name_savm','sales_acct_id_savm').count().orderBy(["endcustomerlinefixed_data", "count"], ascending=[0, 0]).dropDuplicates(['endcustomerlinefixed_data']).drop('count')
date.strftime('%A')
cruise_data = pd.read_table(data_file, delim_whitespace=True, header=None, skiprows=1)$ cruise_data = cruise_data.rename( columns={ 0:'Pressure', 1:'Temperature', 13:'Salinity' } )$ cruise_data[0:5]
data['intercept'] = 1.0
sum(image_clean['jpg_url'].duplicated())
df = pd.read_csv('ab_data.csv')
prcp_1_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation'])$ prcp_1_df.set_index('Precipitation Date', inplace=True) # Set the index by date$ prcp_1_df.count()
df_new = df2.query('group == "treatment"')$ n_new = df_new.shape[0]$ n_new
a_df.to_csv('a_tweets.csv')$ b_df.to_csv('b_tweets.csv')
s4 = pd.Series([10, 0, 1, 1, 2, 3, 4, 5, 6, np.nan])$ len(s4)
reports = reports[(reports[u'Service Location'].isin(booths)) & \$                   (reports[u'Request Status'] == 'Closed')]$ reports.info()
df2['seller'].value_counts().plot(kind='bar')$ print(df2['seller'].value_counts())
a_tags = soup.find_all('a')$ for link in a_tags:$     print(link.get('href'))$
country_dummies=pd.get_dummies(df_new['country'])$ df_new=df_new.join(country_dummies)$ df_new.head()
monte.str.findall(r'^[^AEIOU].*[^aeiou]$')
G = nx.Graph() #creates empty graph, initiliasize a graph object$ G.add_nodes_from(node_names)$ G.add_edges_from(edges)
df2[['ab_page', 'drop_it']] = pd.get_dummies(df2['landing_page'])$ df2.head()
result = api.search(q='%23flu') #%23 is used to specify '#'$ len(result)
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, prec_us)
adf_check(dfs['Seasonal Difference'].dropna())
tot = pd.merge(dailyplay, promotions, how = 'left', left_on = ['Date'], right_on = ['Date'])
urban = merge_table.loc[merge_table["type"] =="Urban"]$ urban_table_df=urban.groupby(["city"])$ urban_table_df.max()
df.columns = df.columns.str.replace('[', '')$ df.columns = df.columns.str.replace(']', '')$ df.columns = df.columns.str.replace(' ', '_')
data.isnull().sum()
import pickle$ filename = 'models/sentiment_model.sav'$ pickle.dump(sentiment_model, open(filename, 'wb'))
NEGATIVE_THRESHOLD=0.855154197601$ VERY_NEGATIVE_THRESHOLD=0.853012564249  
urban_avg_fare = urban_type_df.groupby(["city"]).mean()["fare"]$ urban_avg_fare.head()
active_df = pd.read_sql("SELECT s.station, count(m.station) as station_count FROM measurements m, stations s WHERE m.station=s.station GROUP BY m.station", con=engine, columns=[["station"],["station_count"]])
last12_df.describe()
df2['intercept'] = 1$ df2['ab_page'] = pd.get_dummies(df2['landing_page'])['new_page']$ df2.head()
(null_vals>actual_diff).mean()
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ]$ pd.pivot_table(games_2017, values = "Tm.Pts", index = "Team", aggfunc = np.mean).sort_values(by = "Tm.Pts", ascending = False).head(5)
df2['user_id'].duplicated().sum()
archive_clean['dog_stages'] = archive_clean['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True)$ archive_clean.drop(['doggo','floofer','pupper','puppo'],axis=1,inplace=True)
tweetsIn22Mar.head()$ tweetsIn1Apr.head()$ tweetsIn2Apr.head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])$ z_score, p_value
new_page_converted = np.random.choice([1, 0], size = n_new, p = [p_mean, (1-p_mean)], replace = True)
tcat_df.to_csv('cat_tweets.csv')$ tdog_df.to_csv('dog_tweets.csv')
grouped_publications_by_author['countCollaborators'] = grouped_publications_by_author['authorCollaboratorIds'].apply(len)
plt.hist(p_diffs);
!wget http://files.grouplens.org/datasets/movielens/ml-latest.zip
cohort_activated_df = pd.DataFrame(index=daterange,columns=daterange)
df.start_date = pd.to_datetime(df.start_date, format='%m/%d/%Y %H:%M')
df.info()
index = pd.bdate_range('2018-3-1', periods=1000)$ index
price = autos['price']$ ax = sns.boxplot(y= price)$
apps_by_dist = df_nona.groupby(['segment', 'district_id']).app_id.nunique()$ print apps_by_dist.groupby(level='segment').mean()$ apps_by_dist.groupby(level='segment').describe()$
print(rows)$ df.isnull().any().any(), df.shape
clean_stations.columns = ['station', 'name', 'latitude', 'longitude', 'elevation', 'city', 'country', 'state']
shows['fixed_runtime'].value_counts()
query = pgh_311_data_merged['Category'] == "Road/Street Issues"$ pgh_311_data_merged[query]['Issue'].value_counts(ascending=True).plot.barh(figsize=(10,10))
pres_df.drop('time_from_creation_tmp', inplace=True, axis=1)$ pres_df.head()
movies_df = pd.read_csv('movies.csv')$ ratings_df = pd.read_csv('ratings.csv')$ movies_df.head()
df['index1'] = df.index
sale_prod_sort = sale_average[sale_average.Product == 'Amarilla']$ sale_prod_sort
plt.bar(x_axis, bar_Compound, color=["gold","blue","green","red","lightblue"], align="edge")
bus.describe()$ ins.describe()$ vio.describe()$
dft = df2.T$ dft
accidents["DATE"] = pd.to_datetime(accidents[["DAY","MONTH","YEAR","HOUR","MINUTE"]])$ accidents["TIME"] = accidents["DATE"].dt.time
for row in selfharmm_topic_names_df.iloc[3]:$     print(row)
scope_df['Invalid AC'] = scope_df['textinfo'].str.contains('Acceptance|AC', case = False, regex = True) == False$ scope_df[scope_df['Invalid AC']].to_excel(writer, index=False, sheet_name='Invalid AC', freeze_panes=(1,0), columns=['Team_story', 'key_story', 'reporter_story'])$
np.sum(x < 6, axis=1)
mean = np.mean(data['len'])$ print("The average length of all tweets is: {}".format(mean))
plt.hist(p_diffs, bins=21)$ plt.axvline(p_convert_obs_diff, color='green')$ plt.axvline(np.percentile(p_diffs, 95), color='red');
import matplotlib.pyplot as plt$ hurricane = loan.loc[loan['Disaster_Type'] == '8']
studies = pd.read_csv('C:/Users/akapoor/Music/01 Docs/HealthCare App/ctdb/studies.txt', sep="|")
frames1 = [first_values, last_values, count]$ result1 = pd.concat(frames1, axis=1)$
pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
pgh_311_data['REQUEST_TYPE'].value_counts(ascending=True).plot.barh()
workspace_data = w.data_handler.get_all_column_data_df()$ lst = workspace_data.WATER_BODY_NAME.unique()$ print('WATER_BODY_NAME in workspace:\n{}'.format('\n'.join(lst)))
from sklearn.model_selection import GridSearchCV$ param_grid = {'learning_rate': [0.05,0.1],'num_leaves': [40,60,80]}
pred_labels = lr2.predict(test_data)$ print("Training set score: {:.2f}".format(lr2.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(lr2.score(test_data, test_labels)))
payments_all_yrs = \$ df_providers.groupby(['id_num','name','year'])[['disc_times_pay']].agg(['sum', 'count'])$ payments_all_yrs.head()
r = requests.post(url=url, json={'query': query}, headers=headers)
import xgboost as xgb$ from xgboost.sklearn import XGBClassifier$ XGB_model = xgb.XGBClassifier(objective='multi:softprob')
wordsearch_dict = {}$ for word in order.keys():$     wordsearch_dict.update({word: df.text.str.extractall(r'({})'.format(word), re.IGNORECASE).size})
df2.user_id.duplicated().sum()
data.name.duplicated(keep=False).sum()
randomdata1 = randomdata.describe()$ print randomdata1
print(f"lowest_temp = {min(tobs_data)} \$         highest_temp = {max(tobs_data)}\$         avg_temp_of_most_active_station = {np.mean(tobs_data)}")
S_1dRichards.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
pres_df['ad_length_tmp'] = pres_df['ad_length_tmp'] / np.timedelta64(1, 's')$ pres_df['ad_length_tmp'].head(10)
new_columns = status_data.columns.values$ new_columns[0] = "rowID"$ status_data.columns = new_columns
X = df_modeling_categorized.drop('final_status', axis=1)$ y = df_modeling_categorized['final_status']
df = df.reset_index(drop=True)$ df.head()
new_page_converted = np.random.choice([1,0], size=NewPage, p=[0.12, 0.88])$ new_page_converted
for row in df.itertuples():$     print(row)
df_regression.head()
df['user_id'].nunique()
plt.hist(review_length, bins =100)$ plt.show() # most reviews are short, only few reviews are very long.
df_day_pear=df_day.sample(n=774000).sort_values('tripduration')$ df_night_pear=df_night.sort_values('tripduration')
train = pd.read_csv("data/wikipedia_train3.csv")$ test = pd.read_csv("data/wikipedia_test3.csv")
dfData.shape
fig = plt.figure(figsize=(12, 8))$ print type(fig)
temp_df2['titles'] = temp_df2['titles'].astype(str)
last_year = today.year + 1$ years = list(range(join_date.year, last_year))$ years
index_name = df.iloc[0].name$ print(index_name)
df.columns
fig,ax=plt.subplots(1,2,figsize=(15,3))$ ax[0].boxplot(joined['Promo2SinceYear'],vert=False)$ ax[1].boxplot(joined['Promo2Days'],vert=False)
number_one_charts_df.to_csv("Desktop/Project-2/number_one_chart.csv", index=False, header=True)
automl_feat = pickle.load(open(filename, 'rb'))
df=pd.read_csv('created_train.csv')$ df=df.drop(['TID','vendor_id'],axis=1)$ df.dropna()
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.tail(10)
vocab = vectorizer.get_feature_names()$ print(vocab)
treatment_converted=df2.query('group=="treatment"').converted.mean()$ treatment_converted
input_col = ['msno','transaction_date','is_cancel']$ transactions_price_plan_days = utils.read_multiple_csv('../../input/preprocessed_data/transaction_price_and_play_days_base') # 20,000,000$
gdf = gdf.copy()$ gdf['length'] = gdf['end'] - gdf['start'] + 1$ gdf.head()
df[df['converted']==1]['user_id'].count() / df.shape[0]
df_new = countries_df.set_index('user_id').join(df.set_index('user_id'), how='inner')$ df_new.head()
dat_dow.vgplot.line(value_name='Hospital mortality rate')
X_train = X_train.replace({"interest_level": {"low": 0, "medium": 1, "high": 2}})$ X_train = X_train.join(pd.get_dummies(X_train["interest_level"], prefix="pred").astype(int))$ prior_0, prior_1, prior_2 = X_train[["pred_0", "pred_1", "pred_2"]].mean()
df2.drop(2893, inplace=True)
xmlData.set_value(89, 'address', u'290th Avenue Southeast, Hobart, King County, Washington, 98025, United States of America')$ xmlData.loc[89, 'address']
contractor_clean = contractor.copy()
lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]$ print(lemmed)
len(df.user_id.unique())
file_obj = open('BCH_person.csv', 'r')$ data = list(file_obj)
df2['abtest'].value_counts().plot(kind='bar')$ print(df2['abtest'].value_counts())
df.shape[0]
pres_df.rename(columns={'subject_count': 'subject_count_test'}, inplace=True)$ pres_df.head(2)
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')
(Actual_diff < P_diffs).mean()
print("Unique users:", len(df2.user_id.unique()))$ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
check_null = df.isnull().sum(axis=0).sort_values(ascending=False)/float(len(df))$ print(np.sum(check_null.values) == 0.0)
iso_join.plot();
csvData[csvData['street'].str.match('.*Court.*')]['street']
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])$ print(old_page_converted)
rfc = RandomForestClassifier(n_estimators=1000, max_depth=100, max_features=2, n_jobs=-1)$ scores = cross_val_score(rfc, X, np.ravel(y,order='C'), cv=5)$ print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
df2.converted.mean()
pattern = re.compile('AA')$ print(pattern.match('AAbc'))$ print(pattern.match('bcAA'))
num_users2 = df2['user_id'].nunique()$ print('Number of unique users in dataset: ',num_users2)
merged_data = pd.merge(left=surveys_df,right=species_df, how='inner', left_on='species_id', right_on='species_id')$ merged_data.head()
df_emoji_rows = new_df[new_df.extr_emojis != '']$
p_old = df2.converted.mean()$ p_old
dataset[dataset.duplicated()]$
df.shape$
cur.execute('SELECT count(Comments_Ratings) FROM surveytabl WHERE Comments_Ratings is not null;')$ cur.fetchall()
coming_next_reason = questions['coming_next_reason'].str.get_dummies(sep="'")
cust_clust = crosstab.copy()$ cust_clust['cluster'] = c_pred$
gpCreditCard.Hour_of_day.describe()
df_twitter_archive_copy = df_twitter_archive.copy()$ df_img_predictions_copy = df_img_predictions.copy()$ df_tweet_data_copy = df_tweet_data.copy()
%matplotlib inline  $ import matplotlib.pyplot as plt
print('Endpoint name: {}'.format(rcf_inference.endpoint))
station_count = session.query(Stations.name).count()$ print(station_count)
review_df.head()
db = client['instagram-london']$ coll = db.posts
transactions[~transactions["UserID"].isin(users["UserID"])]$
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country'])$ df2.drop(['country', 'US'], axis = 1, inplace = True)$ df2.head()
freq_station = {'id':"",'name':""}$ freq_station['id'] = active_station_df.iloc[:1]['station'][0]$ freq_station['name'] = active_station_df.iloc[:1]['name'][0]
df_clean['body_length'].hist(range = (0,100))
image_predictions_clean[image_predictions_clean.jpg_url.duplicated()].jpg_url$
engine = create_engine("sqlite:///dropna_hawaii.sqlite")
from fastai.fastai.structured import *$ from fastai.fastai.column_data import *$ from IPython.display import HTML
df2 = pd.read_csv('ab_edited.csv')$ df2.head(5)
avgPurchP = train.groupby(by='Product_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseP'})$ train = train.merge(avgPurchP, on='Product_ID', how='left')$ test = test.merge(avgPurchP, on= 'Product_ID', how='left')
stations = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.date).desc()).all()$ stations
song_tracker_grouped_df.to_csv("Desktop/Project-2/song_tracker.csv", index=False, header=True)
sentiment=sentiment.sort_values(by='Tweet_date', ascending = False)$ sentiment.head()
df['user_id'].nunique()
df_clean.dog_stage.value_counts()
p_new = df2['converted'].mean();$ p_new
ten = pd.merge(left=free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].mean().idxmax(1).to_frame(), right = free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].median().idxmax(1).to_frame(), left_index=True, right_index=True)$ ten.columns = ['mean', 'median']$ ten
table_rows = driver.find_elements_by_tag_name("tbody")[8].find_elements_by_tag_name("tr")$
exploration_titanic.findupcol()$
data = fat.add_sma_columns(data, 'Close', [6,12,20,200])$ data.tail()
globalCityContent = readPDF(globalCityBytes)$ globalCitySentences = globalCityContent.replace('\n','').split('.')$ type(globalCitySentences)
df2[df2['user_id'].duplicated()]
df_2014['bank_name'] = df_2014.bank_name.str.split(",").str[0]$
tweet_archive_clean.drop(['new'], axis= 1, inplace= True)
accuracy = accuracy_score(y_test, predictions)$ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
rets.hist()$ plt.show()
ddp = dfd.dropna(axis=0, subset=['in_reply_to_screen_name'], how='any')
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=featured#submit'$ browser.visit(url)$ time.sleep(3)  #allow time for page to load
df['DETAILS']=df['DETAILS'].fillna("")$
merged2['AppointmentDuration'] = merged2['AppointmentDuration'] / 60.0
df[df.location!=''].groupby('location').size().nlargest(10)
openmoc_geometry = get_openmoc_geometry(sp.summary.geometry)
images.head()
expanded_data = pd.read_json( (data['_source']).to_json(), orient='index')
df['converted'].mean()
df.loc[df['user_id'] == 63]
df.isnull().values.any()
diffs_evening =np.array(diffs_evening)
twelve_months = session.query(Measurements.date, Measurements.prcp).filter(Measurements.date > year_before)$ twelve_months_prcp = pd.read_sql_query(twelve_months.statement, engine, index_col = 'date')
def get_data():$     return pd.DataFrame.from_csv("../../../data/mbta.csv").reset_index()$ gatecount_raw = date.init(get_data())
df_2002['bank_name'] = df_2002.bank_name.str.split(",").str[0]$
small_frame.rbind(small_frame)
reg_mod = sm.OLS(df2['converted'], df2[['intercept', 'ab_page']])$ model = reg_mod.fit()
old_page_converted=np.random.binomial(n_old,p_old)
logit_countries = sm.Logit(df_countries['converted'], df_countries[['intercept','new_page','CA','UK']])$ result = logit_countries.fit()$ result.summary()
df['label'] = df[forecast_column].shift(-forecast_out)
df2_dummy.tail(1)
train_test["num_description_words"] = train_test["description"].apply(lambda x: len(x.split(" ")))$ train_test = train_test.drop(['description'], axis=1)
f, ax = plt.subplots(figsize=(12, 8))$ ax.plot(magento2_data_start_dates)$ ax.set(title='Number of magento2 repos created per month', ylabel='Number of repos')
fh_1 = FeatureHasher(input_type='string', non_negative=True) # so we can use NaiveBayes$ %time fit = fh_1.fit_transform(train.device_model)
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))$
dfGoles.columns
dates =  pd.DatetimeIndex(pivoted.columns)$ dates[(labels == 1) & (dayofweek < 5)]
norm.ppf(1-(0.05/2))$
tokens = pd.DataFrame({'token':X_train_tokens, 'one_star':one_star_token_count, 'five_star':five_star_token_count}).set_index('token')
dog_ratings = dog_ratings[dog_ratings.tweet_id!='810984652412424192']$ dog_ratings = dog_ratings[dog_ratings.tweet_id!='670842764863651840']$ dog_ratings = dog_ratings[dog_ratings.tweet_id!='749981277374128128']
median = reviews.price.median()$ reviews.price.map(lambda v: v-median)
df2.drop_duplicates(subset='user_id', keep="last", inplace=True)$
import time$ import datetime as dt$ import matplotlib.dates as mdates
X_train_tokens = stfvect.get_feature_names()$ len(X_train_tokens)
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]$
questions.to_csv('../data/clean.csv')
stn_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).filter(Measurement.station == busiest_stn).filter(Measurement.date > data_oneyear).order_by(Measurement.date).all()$ stn_temp
df2.loc[df2.duplicated('user_id') == True]['user_id']
aug2014 = pd.Period('2014-08',freq='M')$ aug2014
BAL_analysis = team_analysis.get_group("BAL").groupby("Category") # Pulls only team transactions from sample, then groups$
most_confident_predictions = pd.merge(left=highest_confidence, right=stacked_image_predictions, how='left',on=['tweet_id','confidence'])
engine = create_engine("sqlite:///hawaii.sqlite")
print(rhum_nc)$ for v in rhum_nc.variables:$     print(rhum_nc.variables[v])
df2.user_id.drop_duplicates(inplace = True)
df = pd.read_json("data/open_tender/29b5a17b771e70eaa61ea7ce9ee1fb1c.tenders.json")$
season_type_groups.aggregate(np.mean)
if 1 == 1:$     news_period_df = pd.read_pickle(config.NEWS_PERIOD_DF_PKL)
pres_df['subjects'].isnull().sum()
year_prcp_df.plot()$ plt.show()
df2[df2.duplicated(['user_id'], keep=False)]
hn['popular'] = (hn.num_points > 5).astype(int)
[x.text for x in html.find_all('a', {'class':'next '})]
np.exp(results.params)
df.user_id.nunique()
precipitation_year = session.query(Measurements.date,func.avg(Measurements.prcp)) \$              .filter(Measurements.date >= '2016-08-23').filter(Measurements.date <= '2017-08-23') \$              .group_by(Measurements.date).all()
tlen = pd.Series(data['len'].values, index=data['Date'])$ tfav = pd.Series(data['Likes'].values, index=data['Date'])$ tret = pd.Series(data['RTs'].values, index=data['Date'])
pd.DataFrame(np.array([[10, 11], [20, 21]]))
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_new, (1 - p_new)])$ p_old_sim = old_page_converted.mean()$ p_old_sim
type_df = new_df_left.dtypes.reset_index()$
tweet_df.sort_values(by="date", ascending=True)$
df_newpage = df.query('landing_page == "new_page"')$ df_2 = df_newpage.query('group == "treatment"')$ df_2.nunique()$
df_concat['date'] = pd.DatetimeIndex(df_concat.date_series).normalize()$ df_concat.date.head()
dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',$                        '2015-Jul-6', '07-07-2015', '20150708'])$ dates
columns = [m.key for m in Measurement.__table__.columns]$ print(columns)
df7['avg_health_index Closed'].value_counts(dropna=False)
bruins = bruins[pd.to_datetime(bruins.date).isin(bad_dates) == False]$ bruins = bruins[pd.to_datetime(bruins.date) <= dt.datetime(2015,2,11)]
df2 = df.iloc[10:11,0:7].T$ print(df2)
btypedums = pd.get_dummies(data.bill_type)$ data.drop(['bill_id', 'bill_type'], axis=1, inplace=True)$ data.billtext = map(str.lower, data.billtext)$
dfX_hist = lbh.dfX.copy()$
t1.head(5)
print(list(festivals.columns.values))$ festivals = festivals[['Index', 'Date', 'Location', 'latitude', 'longitude', 'Website']]$ festivals.head(3)
releases.head()
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])$ df_new.head()
apple.resample('M').mean()
station_count = session.query(Station.id).count()$ station_count
import os$ data = pd.read_csv('https://raw.githubusercontent.com/ogrisel/parallel_ml_tutorial/master/notebooks/titanic_train.csv', $                     sep = ',')
df_ad_airings_2 = pd.read_pickle('./TV_AD_AIRINGS_ENTIRE_DATASET_DATES_FIXED.pkl')
train_small_data = pd.read_feather("../../../data/talking/train_small_data.feather")$ val_small_data = pd.read_feather("../../../data/talking/val_small_data.feather")$ test = pd.read_feather("../../../data/talking/test_small_data.feather")
collection = store.collection('NASDAQ.EOD')$ collection
precipitation_df.describe()
loan['Approval_Dt2'] = pd.to_datetime(loan['Approval_Dt'], errors='coerce')$ loan['Deadline_Dt2'] = pd.to_datetime(loan['Deadline_Dt'], errors='coerce')$ loan['Declaration_Dt2'] = pd.to_datetime(loan['Declaration_Dt'], errors='coerce')
labeled_features['failure'].value_counts()
pd.Series(feature_names).sample(20)
preci_data = session.query(Measurement.date, Measurement.prcp).\$     filter(Measurement.date > last_year).\$     order_by(Measurement.date).all()
df.injured.describe()
autos.head()
df2.user_id.nunique()
from google.cloud import bigtable$ client     = bigtable.Client.from_service_account_json(JSON_SERVICE_KEY,project=project_id, admin=True)$ instance   = client.instance(instance_id)$
reddit['Class_comments'] = reddit.apply(lambda x: 'High' if x['num_comments'] > median_comments else 'Low', axis = 1)$ reddit.head()
Pold = df2.converted.mean()$ Pold
res3 = rs.post('http://bsr.twse.com.tw/bshtm/bsMenu.aspx', headers = headers, data = payload)$
df['car_age'] =  (last_seen.year - df.yearOfRegistration).apply(lambda x: int(x))
def getStartDate(collectionDate):$   startDate = max(df_epi.index[df_epi.index<collectionDate])$   return startDate.strftime('%Y-%m-%d')
edge_types_DF = pd.read_csv('network/recurrent_network/edge_types.csv', sep = ' ')$ edge_types_DF
data = pd.read_csv('Eplusfanpage.csv')$ data$
df_track.to_sql('track_db', cnx)$ df_artist.loc[:,:].to_sql('artist_db', cnx)
df.info()
import statsmodels.api as sm$ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'treatment']])$ results = log_mod.fit()
df.shape
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
df.dropna().shape == df.shape, df.shape
fig = sns.FacetGrid(trip_data, row = "subscription_type", aspect = 4)$ fig.map(sns.countplot, 'start_hour')
pt_after=pd.DataFrame.pivot_table(df_users_6_after,index=['cohort'],values=['uid'],aggfunc='count',fill_value=0)
mean = np.mean(data['len'])$ print("The length's average in tweets: {}".format(mean))
os.chdir(root_dir + "data/")$ filtered_df_fda_drugs.to_csv("filtered_fda_drug_reports.csv", index=False)
reqs.count()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep ='\s+', header=None)$ df.head(5)
num_missing = df.isnull().values.ravel().sum()$ print("Number of row with missing values - {}".format(num_missing))
def tokenizer(text):$     return text.split()
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1 - convert_mean), convert_mean])
twitter_api = tweepy.API(authorization_instance)$ twitter_api
df.info()
contractor[contractor.contractor_number.duplicated() == True]
(df2.landing_page == "new_page").mean()
df.head()
top_5_beats = cfs_df['Beat'].groupby(cfs_df['Zip']).value_counts()$ top_5_beats.groupby(level=0).nlargest(5)
TrainingSamples = int(MaxPoints * 0.7)$ ValidationSamples = int((MaxPoints-TrainingSamples)/2)$ TestSamples = MaxPoints - TrainingSamples - ValidationSamples
edftocsv.edftocsv(inputFile, outputFileHeaders, outputChanHeaders, outputData, True)
tweets_predictions_all['source'].value_counts()
from time import mktime$ current_time.timetuple()$ mktime(current_time.timetuple())
for route in routes:$     print('mode: {} | id: {} | route name: {}'.format(route['mode'], route['id'], route['longName']))
df_userIds = pd.DataFrame({"UserID":user_ids,"Key":1})$ df_productIds = pd.DataFrame({"ProductID":product_ids,"Key":1})$
gender = records[records['Gender'].isnull()]['First_name'].apply(get_gender)
df_vndr_id = pd.DataFrame(X_copy['vndr_id'])
df1_normal = df1.copy()$     $ df1_normal['y'] = np.log(df1_normal['y'])
df2['Geocoded Location'].apply(str)
df.head()
store_items.fillna(method='backfill', axis=0)$
tallies_file = openmc.Tallies()$ mgxs_lib.add_to_tallies_file(tallies_file, merge=True)
X_train_valid, X_test, y_train_valid, y_test = train_test_split(pm_final.drop('status', axis = 1)$                                                     ,pm_final[['status']],$                                                     test_size=0.20)
df2_control = df2.query("group == 'control'")
dat.status[dat.completed.isnull()].unique()
type2017 = type2017.dropna() 
building_pa_prc_shrink[building_pa_prc_shrink.columns[0:5]].head(10)
max(twitter_archive['tweet_id'].value_counts())
baseball[baseball['team'].isin(['LAN', 'SFN'])]
df_tweets.reset_index().to_pickle("../tweets_extended.pkl")
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
forecast_column = 'Adj. Close'$ df.fillna(value=-99999, inplace=True)$ forecast_out = int(math.ceil(len(df)*0.01))
pivoted_data = popular_centers.pivot(columns="center_name", values="attendance_count")$ pivoted_data.head()
classification_df = encoded_df.query('(id == 5 or id == 14) and best != 0')$ classification_df.best.describe()
df_enhanced = df_enhanced.drop('name', axis = 1)
commit_nova = pandas_ds[(pandas_ds["current_status"] == 'MERGED') & (pandas_ds["gerrit_tracker"].str.contains("/nova"))];$ commit_nova
df[df.bottles_sold==2508]
df3 = df2$ df3.head()
twitter_archive_full.loc[(twitter_archive_full.stage == 'None') & (twitter_archive_full.text.str.contains('doggos')), 'stage'] = 'doggo'
date_ly = dt.date.today() - dt.timedelta(days = 2*365)$ print(date_ly)
sns.regplot(x="totqlesq", y="y", data=psy_native).set_title("Quality of Life Enjoyment and Satisfaction")$
g = sns.FacetGrid(data=dataset, col='rating')$ g.map(plt.hist, 'text length', bins=50)$
bd.reset_index(drop=True)
sim_ET_Combine = pd.concat([simResist_rootDistExp_1, simResist_rootDistExp_0_5, simResist_rootDistExp_0_25], axis=1)$ sim_ET_Combine.columns = ['simResist(Root Exp = 1.0)', 'simResist(Root Exp = 0.5)', 'simResist(Root Exp = 0.25)']
df3 = df_tweet_json_clean.copy()$ df3.sort_values(by = ['created_at'])$
RIDs_DXSUM = list(diagnosis_DXSUM_PDXCONV_ADNIALL['RID'].unique())$ RIDs_ADSXLIST = list(diagnosis_ADSXLIST['RID'].unique())$ RIDs_BLCHANGE = list(diagnosis_BLCHANGE['RID'].unique())
last_year = dt.date(2017, 4, 15) - dt.timedelta(days=365)$ print(last_year)
pd.merge(test, session, how=left)
df_genres = df.groupby("genre")$ print(df_genres['score'].mean().sort_values(ascending=False))
log_mod = sm.Logit(df3['converted'], df3[['intercept', 'ab_page' , 'CA' , 'UK']])$ results = log_mod.fit()$ results.summary()$
engine = create_engine("sqlite:///hawaii.sqlite")$ engine
ET_Combine = pd.concat([hour_1dRichards, hour_lumpedTopmodel, hour_distributedTopmodel_average], axis=1)$ ET_Combine.columns = ["Baseflow = 1D Richards'", 'Baseflow = Topmodel(lumped)', 'Baseflow = Topmodel(distributed)']
vocab = vectorizer.get_feature_names()$ dist = np.sum(train_data_features, axis=0)
edu_gen_edad = pd.merge(educacion, genero_edad, on = 'idpostulante', how = 'inner')$ edu_gen_edad.head(1)
df=pd.read_json('tweet_json.txt', lines=True)$
np.std(diffs)
image_predictions_clean[image_predictions_clean.jpg_url=='https://pbs.twimg.com/media/CkjMx99UoAM2B1a.jpg']$
merge.to_csv('Data/Incidents.csv')$
X_df = train_df12.ix[:, train_df12.columns != 'interest_level']$ X_df.head()
joined['Retire'].dtypes
print('Number of rows with invalid values = {}'.format(len(joined[joined.isnull().any(axis=1)])))$ joined[joined.isnull().any(axis=1)]
df.drop('four', axis = 'columns')$ df
exiftool -csv -createdate -modifydate cisuabd4/cisuabd4_cycle1.MP4 cisuabd4/cisuabd4_cycle2.MP4 cisuabd4/cisuabd4_cycle3.MP4 cisuabd4/cisuabd4_cycle4.MP4 cisuabd4/cisuabd4_cycle5.MP4 cisuabd4/cisuabd4_cycle6.MP4 > cisuabd4.csv
logit_country = sm.Logit(merged['converted'],merged[['intercept','UK', 'US']])$ result_merged = logit_country.fit()
df2 = df_ab.query('(group == "treatment" & landing_page == "new_page")|(group == "control" & landing_page == "old_page")')$
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
grouped_by_letter = df1.groupby(['tactic_letter'])$ list_of_letter = df1['tactic_letter'].unique()$
cur = conn.cursor()
df.plot(x="newDT", y=["ED",'DD','CVX','FL','CAT'], kind="line")$
crimes.tail()
vader_df = twitter_df.groupby(["user"]).mean()["compound"]
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
popularity_clean.info()
giss_temp.fillna(method="ffill").tail()
filter_df['start_time'].min(), filter_df['start_time'].max()
tweet_json_clean.rename(index = str, columns={"id": "tweet_id"}, inplace = True)
auth = tweepy.OAuthHandler(consumerKey, consumerSecret)$ api = tweepy.API(auth)
grid_df.tail()
b2b_df = pd.read_csv(data_fp, header=None, names=['brain_weight', 'body_weight'])$ b2b_df.head()
mean_values = sample_df3.mean(axis=0)$ test_df_new = sample_df3.fillna(mean_values, inplace=True)
df = pd.read_csv("https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv")$ df[:5]
converted_users = df[df["converted"] == 1].shape[0]$ prop_converted = converted_users/df.shape[0]$ prop_converted
print(dfx.dropna(how='any'))
ari_games.set_index('MatchTimeROT', inplace=True)
lmdict[lmdict.Negative != 0].head()
borough_group = data.groupby('Borough')$ borough_group.size().plot(kind='bar')$
actual_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean()$ (actual_diff < p_diffs).mean()
train_binary_dummy = pd.get_dummies(train_binary, columns = categorical)$ train_binary_dummy.head()
s_filled = s.fillna(0)$ s_filled
cgmStats = get_stats(cgm)$ cgmStats.T
price_data_df = quandl.get('BCHARTS/KRAKENUSD', start_date="2018-04-18", end_date="2018-04-20")
new_page_converted = np.random.binomial(1, p_new,n_new)
df2[df2['group']=='treatment'].head()
af.length.sum() / 1e9
spark.sql('create database if not exists my_analysis_work')$ spark.sql('drop table if exists my_analysis_work.example_timeseries')$ joined_patient.write.saveAsTable('my_analysis_work.example_timeseries')
print data_df.clean_desc[26]
json.dumps(obj)
g = kick_data[['backers_count','state']].groupby(['state', kick_data['launched_at'].dt.year,]).agg(['size','sum'])$ g.unstack('state').fillna(0)
journeys.fillna(method = 'ffill', inplace = True)$ journeys.fillna(method = 'bfill', inplace = True)$
for names in asdf:$     name = names.div['data-name']$ name
df1['Hour'] = pd.to_datetime(df1['Date'], format='%H:%M').dt.hour # to create a new column with the hour information$ df1.head()
df['day_of_week'] = df['date'].dt.dayofweek$ days = {0:'Mon',1:'Tues',2:'Wed',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}
df2[df2['user_id'].duplicated(keep = False)]
validation.analysis(observation_data, lumped_simulation)
pd.to_datetime(['2009/07/31', 'asd'], errors='ignore')
keep_day_threshold = 4$ n_user_days = n_user_days.loc[n_user_days >= keep_day_threshold].reset_index()$ n_user_days = n_user_days[['seqn']]
csvpath2 = os.path.join('Desktop', 'Project-2', 'numberOneUnique.csv')$ import csv$ numberOneUnique_df = pd.read_csv(csvpath2, encoding="ISO-8859-1")
results.summary()
dat[dat.x.isnull()]
new_group=df.groupby(by=result)$ new_group
os.remove(fileName)
df_clean.loc[df_clean['name'].str.islower(),'name'] = None$
p_diffs = np.array(p_diffs)$ (p_diffs > p_diff).mean()$ print('As a percentage: {}%'.format((p_diffs > p_diff).mean()*100))
model.score(combined_transformed_array, y_test)
trip_data.ix[trip_data.Trip_distance == 0].shape[0]
with open('key_phrases_rake.pickle', 'rb') as f:$     key_phrases_rake = pickle.load(f)
def convert_to_datetime(dt_str):$     fmt = "%Y-%m-%d %H:%M:%S +0000"$     return dt.strptime(dt_str,fmt)
RegO = pd.to_datetime(voters.RegDateOriginal.map(lambda x: x.replace(' 0:00', '')))$ Reg = pd.to_datetime(voters.RegDate.map(lambda x: x.replace(' 0:00', '')))$ datecomp = pd.DataFrame({'OrigRegDate':RegO, 'RegDate':Reg})
crime = crime[crime.Sex != '*']
pd.io.json.json_normalize(playlist['tracks']['items'][2])
titanic.duplicated().sum()
model = smf.ols(formula='vote ~ genre', data=tmdb_movies_vote_revenue)$ results = model.fit()$ print (results.summary())
X = X.drop(0, axis='index')
writer = pd.ExcelWriter('output.xlsx')$ frame.to_excel(writer,'Sheet1')$ writer.save()
movies.describe()
engine = create_engine("sqlite:///hawaii.sqlite")$
test_ind["Pred_state_XGB"] = xgb_model.predict(test_ind[features])$ train_ind["Pred_state_XGB"] = xgb_model.predict(train_ind[features])$ kick_projects_ip["Pred_state_XGB"] = xgb_model.predict(kick_projects_ip_scaled_ftrs)
flight = spark.read.parquet("/home/ubuntu/parquet/flight.parquet")
Z = np.arange(10)$ np.add.reduce(Z)
data['SA'] = np.array([analize_sentiment(tweet) for tweet in data['Tweets']])
for script in soup(["script", "style"]):$     print script$     script.extract()
random.sample(labels.items(), 25)
datetimes = pd.date_range(DATA_STARTTIME, DATA_ENDTIME, freq='min')$ datetimes[0:10]
null_values = np.random.normal(0, p_diffs.std(), p_diffs.size)$ plt.hist( null_values)$ plt.axvline(obs_diff, color = 'red');
round((timelog.seconds.sum() / 60 / 60), 1)
trump.index=pd.DatetimeIndex(trump['created_at'])$ trump.drop('created_at', axis = 1, inplace =True)$ trump.shape
tweet_data.sort_values('rating', ascending= False).head()$
endometrium_data = pd.read_csv('data/small_Endometrium_Uterus.csv', sep=",")  # load data$ endometrium_data.head(n=5)  # adjust n to view more data
df_new[(df_new['group'] == 'control')]['converted'].mean()$
lq2015_q1.Date.sort_values(ascending=False)
df_urls = pd.read_csv('ny_times_url_dataframe.csv', encoding = 'utf-8')$ df_urls.shape$
import nltk.data$
cur.execute('SELECT results FROM trials WHERE trial_id = 1')$ trial1_results = cur.fetchone()$ pd.read_csv(io.StringIO(trial1_results[0]), index_col=0)$
metrics.confusion_matrix(y_test, y_pred_class)
df1 = pd.DataFrame(bmm_series, columns=['mean_mileage'])$ df1
json_data = r.json()$ json_data
gpByDate = data_air_visit_data.groupby('visit_date').agg(np.sum)$
df_new[['CA','UK','US']]= pd.get_dummies(df_new['country'])$ df_new.head()
dreamophone_url = 'https://dreamophone.com/dream/2186'
set(user_df.columns).intersection(stories.columns)
compared_resuts.to_csv("data/output/logitregres.csv")
hn.shape[0]
etsamples_100hz.iloc[0:1]
site_visits.MarketingChannel.unique()
tweets_df.head()
df_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis=1, inplace= True)
p_diffs=np.array(p_diffs)
aussie_search = api.search(q='%23Aussie')$ len(aussie_search)
store_items.fillna(method = 'ffill', axis = 1)
df_th = df.loc[df['original_language'] == 'th']$ df_th
precip = session.query(Measure.date, Measure.prcp).\$     filter(Measure.date > '2016-08-23').\$     order_by(Measure.date).all() 
symbol='IBB'$ benchmark2 = web.DataReader(symbol, 'yahoo' , start_date ,end_date)
plt.hist(p_diffs)
twitter_archive_clean.reset_index(drop=True, inplace=True)
doc_id_list = np.array(reuters.fileids(category_filter))$ doc_id_list = doc_id_list[doc_id_list != 'training/3267']
path = os.path.join("../Datasets/Beat_Data/")$ path
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df = pd.read_pickle('C:/Users/Stacey/Downloads/NY_complaint_data_for_model_prec62.pkl')
df2 = df.copy()$ df2 = df2[((df.group == 'control') & (df.landing_page == 'old_page')) | $           ((df.group == 'treatment') & (df.landing_page == 'new_page'))]
test_tweet = api.user_timeline(newsOutlets[0])$ print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
contractor_clean['updated_date'].head()$
print("State space samples:")$ print(np.array([env.observation_space.sample() for i in range(10)]))
tail = df.copy()$
twitter_df.rating_numerator.nunique()
df_country.nunique()
D2 = [(str(v), str(t).replace("|","")) for v,t in D2]$ D2[0:5]
df.to_csv("NewsMood.csv")
from sklearn.preprocessing import StandardScaler$
all_tables_df.iloc[:, 1]
tmax_day_2018.tmax[100].plot();
df_nuevo = df.sample(3005)
%matplotlib inline$ import seaborn; seaborn.set()
fig, ax = plt.subplots(figsize=(12,12))$ xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)$ plt.show()
list_to_merge = list(db.tweets.find({},{"id": 1, "user": 1,"text": 1,"hashtags":1, "_id": 0}))
df.query('converted == 1').user_id.nunique() / df.user_id.nunique()
df3[df3['STATION'] == '103 ST'].groupby(['DATE']).sum()
grouped_months_liberia = deaths_liberia.groupby(deaths_liberia['Date'].apply(lambda x: x.month))$ deaths_liberia['National'] = deaths_liberia['National'].astype(int)$
prcp_df = pd.DataFrame(prcp_in_last_year, columns=['date', 'prcp'])$ prcp_df.head(11)
hp = houseprint.Houseprint()$
dem = dem[dem["subjectkey"].isin(incl_Ss)]
nulls=(c_date.notnull()==False)$ nulls.value_counts()
df.to_csv('ab_data_mod.csv')
for f in potentialFeatures:$     related = df['overall_rating'].corr(df[f])$     print("%s: %f" % (f,related))
fm = pd.merge(crime_df, weather_df, how='left', on=None, left_on='Date_Time', right_on='Date_ TimeCST',$       left_index=False, right_index=False, sort=True,$       suffixes=('_x', '_y'), copy=True, indicator=False)
df2[df2['user_id'].duplicated()]
all_slices.sort(key = lambda slice: slice['expires'])$     $ print(f"Found {len(all_slices)} slices")    
tmp = df[selected_features].join(outcome_scaled).reset_index().set_index('date')$ tmp.dropna().resample('Q').apply(lambda x: x.corr()).iloc[:,-1].unstack()\$ .iloc[:,:-1].plot(title='Correlation of Features to Outcome\n (by quarter)')$
data.loc[data.L2 == ' ', ['L2']] = np.nan$ data.L2.unique()
data.ix[:3, :'pop']
import gmplot$ gmap = gmplot.GoogleMapPlotter.from_geocode("New York",10)
sox['season'] = pd.DatetimeIndex(sox.date).year
engine = create_engine("sqlite:///Resources/hawaii.sqlite")
p_old=df2.query('converted == 1').user_id.nunique()/df2['converted'].count()$ p_old
testR.iloc[:,:101]
df['tl_svl']=(df.tl/df.svl)$ df['mass_svl']=(df.mass/df.svl)
df = data_archie.copy()
inspector = inspect(engine)$ inspector.get_table_names()$
df_eng.info()$
pd.Series([1,2,9])
df.to_csv('df_ancestry.csv')$ df_model = h2o.import_file(path='df_ancestry.csv')
df3 = pd.DataFrame(df, index = ['b', 'c', 'd', 'a'])$ df3
load_dotenv('.env')
df.drop(df[df.zipcode_initial.isin(['GU214ND','94000'])].index, axis=0, inplace=True)
ExtractData = twitter_api()$ tweets = ExtractData.user_timeline(screen_name="chintkap", count=200)$
season_team_groups = nba_df.groupby(["Season", "Team"], as_index = False)
test = talks_train.ix[23,'speaker_ids']
misaligned_row_count = ab_df[((ab_df['group'] == 'treatment') & (ab_df['landing_page'] == 'old_page')) | ((ab_df['group'] == 'control') & (ab_df['landing_page'] == 'new_page'))]['user_id'].count()$
empDf.select("name").show()
data.Likes.value_counts(normalize=True).head()
rootDistExp = Plotting(S.setting_path.filepath+S.para_trial.value)
query = session.query(Measurement)$ rain = query.filter(Measurement.date >= year_ago_str).all()$ print(len(rain))
df.head()
from sklearn.ensemble import RandomForestClassifier$ random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)
y = list(train_1m_ag.is_attributed)$ X = train_1m_ag.drop(['is_attributed'],axis=1)
float(df.converted.sum())/df_length
HAMD = HAMD[HAMD["level"]=="Enrollment"]$ HAMD = HAMD.dropna(axis=1, how='all')$ HAMD.columns
t2.tail(10)
kyt_lat = 34.955205 #35.005205$ kyt_long = 135.675300 #135.7353$ diff_wid = (135.795300 - 135.675300)/grid_size$
df2[df2.user_id.duplicated()]
calls_nocontact_simp['ticket_closed_date_time'] = pd.to_datetime(calls_nocontact_simp['ticket_closed_date_time'])
df_users_test = df_users.iloc[:2, :]$ df_users_test.iloc[1, -1] = '2017-09-20'$ df_users_test
td_alpha = td ** (1/3)$ td_alpha = td_alpha / td_alpha.max().max()
from dateutil import parser$ date = parser.parse("4th of July, 2015")$ date
print(prop_zn.describe())
pd.options.display.max_colwidth = 100$ data_df[data_df.nwords == 1]['clean_desc'].head(15)
frames_liberia = [first_values_liberia, last_values_liberia, count_liberia]$ result_liberia = pd.concat(frames_liberia, axis=1)$
allVars = read.getVariables()
archive = pd.read_csv('twitter-archive-enhanced.csv')$ archive
active_psc_records.month_year_birth.hist(figsize=(20,5),bins=50)
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig8_left_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
print np.mean(low_polarity['polarity'])*len(low_polarity)$ print np.mean(high_polarity['polarity'])*len(high_polarity)
null_vals=np.random.normal(0,p_diffs.std(),p_diffs.size)
results1.summary()
df2['intercept'] = 1$ df2[['alt', 'ab_page']] = pd.get_dummies(df2['group'])$ df2 = df2.drop('alt', axis=1)
data = data.sort_values(by=['time'])
stations = session.query(Measurement).group_by(Measurement.station).count()$ print (f"There are {stations} stations")
df['user_id'][df['converted'] == 1].count() / df['user_id'].count()
bigram_chunker = BigramChunker(train_trees)$ print(bigram_chunker.evaluate(valid_trees))
column_check = inspector.get_columns('measurement')$ for check in column_check:$     print(check['name'],check['type'])
file = open("datasets/git_log_excerpt.csv", "r")$ print(file.read())
tweet_df["tweet_source"].unique()
!wget https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv$ !head -n 2 Consumer_Complaints.csv
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY+'&limit=1')
go_no_go_date = go_no_go.groupby('subject_id').Date.unique()$ simp_rxn_time_date = simp_rxn_time.groupby('subject_id').Date.unique()$ proc_rxn_times_date = proc_rxn_time.groupby('subject_id').Date.unique()
df_goog.Open.resample('a').plot()$
df1[40:].head(5)
archive_clean.timestamp = pd.to_datetime(archive_clean.timestamp)
df2017 = dfSubSites[dfSubSites.Year == 2017]$ df2017sites = df2017.groupby('MonitoringLocationIdentifier').mean()$ df2017sites.TotalN
rng = np.random.RandomState(23)$ sample_size = 50$ rng.choice(sizes, sample_size, replace=True)
from pyspark.sql.functions import isnan, when, count, col$ ibm_hr.select([count(when(isnan(c), c)).alias(c) for c in ibm_hr.columns]).show()
northern_sea_level = pd.read_table("http://sealevel.colorado.edu/files/current/sl_nh.txt", $                                    sep="\s+")$ northern_sea_level
dfUK = final_df.query('country == "UK"')$ dfUS = final_df.query('country == "US"')$ dfCA = final_df.query('country == "CA"')
ari_temp = pd.DataFrame(ari_games.MatchTimeROT.str.split('$',1).tolist(),columns = ['first','rot'])$ ari_games.ROT = ari_temp.rot
issue_table = issue_table.drop_duplicates()
twitter_df_us=twitter_df[twitter_df.location=='United States']$ plt.plot(twitter_df_us['created_at_time'], twitter_df_us['retweet_count'], 'ro')$ plt.show()$
freq_titles1 = jobs_data.groupby(['record.title']).size().reset_index(name='counts').sort_values('counts', ascending=False).head(50)$ freq_titles1
ab_file.drop(ab_file.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ ab_file.drop(ab_file.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
autos.info()
body = soup.find('div', attrs={'class' : 'article-body'}).get_text()$ print(body)
plt.figure(figsize=(15,8))$ ax = sns.boxplot(x = 'Type 1', y = 'Total', data = pokemon )$ plt.show()
df = pd.read_csv(pump_data_path, index_col = 0)$ df.head(1)
stamp_name = '4. timestamp'
res = requests.get('http://togows.org/search/kegg-pathway/cancer/1,50.json')$ pp(res.json())
data.loc[data.PRECIP.isnull()]
import matplotlib.pyplot as plt$ plt.hist(p_diffs)$ plt.axvline(x = prob_convert_given_treatment-prob_convert_given_control,color='red')$
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'ab_page','US', 'UK']])$ result = logm2.fit()$ result.summary2()
print data.mean()
print(preprocessor(df.loc[0, 'review'][:-500]), '\n')$ print(preprocessor("</a>This :) is :( a test :-)!"))
df_enhanced = df_enhanced.rename(index=str, columns={"rating_numerator": "rating_10_scaled"})
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'CA']])$ results = mod.fit()$ results.summary()
r = requests.get ('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=4zX7CA93VkKSEygsbSLv')$ print (r.json()['dataset']['data'][0])
coef_uk = np.exp(0.0099)$ coef_ca = np.exp(-0.0408)$ print("UK: {0}, CA: {1}".format(coef_uk, coef_ca))
df.head()
freq = nltk.FreqDist(tokens)$ pd.Series(freq).sort_values(ascending=False).head(20)
np.std(old_means)
input_col = ['msno','w','days_since_the_last_expiration','order_number_rev']$ membership_loyalty = utils.read_multiple_csv('../../input/preprocessed_data/days_since_the_last_transactions',input_col) # 20,000,000$
top_brands = autos["brand"].value_counts(normalize=True).head(20).index$ top_brands
S = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure08/summa_fileManager_riparianAspenPerturbRoots.txt')
import statsmodels.api as sm
checking.iloc[z,0] = np.nan$ checking['age'] = checking['age'].fillna(checking['age'].mean())
trans.groupby('msno').count().sort_values('is_cancel').tail()
obj = json.loads(rec)$ type(obj)
ET_Combine = pd.concat([simResis_hour, BallBerry_hour, Jarvis_hour], axis=1)$ ET_Combine.columns = ['Simple resistance', 'Ball-Berry', 'Jarvis']
ari_games2['Date'] = ari_games2.MatchTimeC
p_diffs = np.array(p_diffs)$ plt.hist(p_diffs)
Incid = pd.read_csv('Data/Incidents.csv')$ Incid.head()$
oil_pandas = pd.read_csv("oil.csv")
health_data_row.loc[2013:2017]  # 2017 doesn't exist, but Python's slicing rules prevent an exception here$
ride_df_urban = urban.groupby('city')$ city_df_urban = urban.set_index('city')
temp_us_full = temp_nc.variables['air'][:, lat_li:lat_ui, lon_li:lon_ui]
desc_stats.columns = ['Count', 'Mean', 'Std. Dev.', 'Min.', '25th Pct.', 'Median', '75th Pct.', 'Max']$ desc_stats
psy_df5 = HAMD.merge(psy_df4, on='subjectkey', how='right') # I want to keep all Ss from psy_df$ psy_df5.shape
tweets_clean['p1_dog'].value_counts()
all_text.head()
df[df['text'].str.contains('appointment')]
d = 30$ df['date_after_30d'] = df['datetime'].apply(lambda x: x + pd.Timedelta(d, unit='d'))$ print(df.to_string())
auth = tweepy.OAuthHandler(consumer_key=consumerKey, $     consumer_secret=consumerSecret)$ api = tweepy.API(auth)$
i=random.randrange(len(train_trees))$ print(train_trees[i])
len(topUserItemDocs['user_id'].unique())$
cur.execute("select * from sqlite_master where type == 'table';").fetchall()
df = pd.DataFrame(daily_norm, columns = ["date","min_temp","avg_temp","max_temp"])$ df["date"] = pd.to_datetime(df["date"]).dt.date$ df.set_index(["date"], inplace = True)
user_df = pd.read_csv('verified_user.csv')$ print(len(user_df))$ print(user_df.created_at.min())$
train = train[0:int(0.8*len(train))]$ train_sub_test = train[int(0.8*len(train)):]
merged2.shape$
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ data.head(10)
crime_data["OFNS_DESC"].sort_values().unique()
template_df = pd.concat([X_valid, y_valid], axis=1)$ template_df['is_test'] = np.repeat(True, template_df.shape[0])
cityID = 'd98e7ce217ade2c5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Stockton.append(tweet) 
counts = clinton_df.source.value_counts().sort_index()
df2.head()
preprocessor.print_infos('consistency')
np.random.seed(123)$ my_model_q1 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label')$ my_model_q1.fit(X_train, y_train)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-05-15&api_key=' + API_KEY)$ r.json()
df_kosdaq = pd.DataFrame({'id' : kosdaq_id_ls, 'name' : kosdaq_name_ls, 'market_type' : 2}, columns = ['id', 'name','market_type', 'quant', 'market_sum', 'property_total', 'debt_total', 'listed_stock_cnt', 'pbr', 'face_value',])
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_15min'], axis=1)$ dfY = data['corrCost']
p_new = df2[df2['landing_page']=='new_page']['converted'].mean()$ print("Probability of conversion for new page (p_new):", p_new)
df['ageM'].dropna(inplace= True)$ df['ageF'].dropna(inplace= True)
print('Best score for data:',np.mean(forest_clf.feature_importances_))
print 'See correlation with actual: ',test_case.select('street_name').take(1)$ actual_acct_id.select('street_name').distinct().show(10,False)
df.loc[df['waiting_days']>=0]['waiting_days'].describe()
p1.age= 45$ print(p1.age)
df.source.value_counts()
jpl_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars"$ browser.visit(jpl_url)
frames = [nuc, part_nuc, out_out, noSeal, entirecell]$ ps = pd.concat(frames)$ ps.head(10)
df3 = df2.set_index('user_id').join(df_countries.set_index('user_id'))$ df3.head()
print("Probability an individual recieved new page is", $       df2['landing_page'].value_counts()[0]/len(df2))
data.index[0] = 15
number_of_commits = git_log.timestamp.count()$ number_of_authors = git_log.author.value_counts(dropna=True).count()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
from Lightcurve import plot_LC_solar_Flare$ % matplotlib inline $ plot_LC_solar_Flare('FERMI/SolarFlares/LAT_Flares/lat_LC_20120307.fits', 'Flare20120307')
results[results['type']=='Other']['tone'].value_counts()
sentiments_df.to_csv("NewsMood.csv")
print('Average daily volume of 2017: ' + str(np.mean(vol_vec).round(1)))
df2.shape[0]
df2['converted'].mean()
df_new = df_new.join(pd.get_dummies(df_new.country))$ df_new.head()
cand_date_df = pd.read_pickle('data/pres_sorted_with_sponsors_and_party.pickle')$ cand_date_df.head()
archive_clean.drop(['rating_numerator',$                     'rating_denominator'],$                   axis=1, inplace=True)
club_data = data[data['competitionid'] <> 4]$ data[data['competitionid'] == 4].iloc[0]$
subreddit = reddit.subreddit('Bitcoin')$ for submission in reddit.subreddit('all').hot(limit=25):$     print(submission.title)
x1 = pd.DataFrame(df.groupby(['donor_id','zipcode']).zipcode.nunique())$ x1[x1.zipcode != 1]
df2=df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page'))==True]$ df2.head()
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)$ os.chdir(str(today))$ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)$
tmax_day_2018.coords
indexed_df = movie_df.set_index("title")
!wget https://raw.githubusercontent.com/jacubero/ColabEnv/master/awk_netstat.sh -O awk_netstat.sh$ !chmod +x awk_netstat.sh$ !./awk_netstat.sh
import datetime as dt$ Todays_date = dt.date.today() - dt.timedelta(days=365)$ print("Today's Date:", Todays_date)
tm_2030 = pd.read_csv('input/data/trans_2030_m.csv', encoding='utf8', index_col=0)
dataset.groupby(by=['place_country_code'])['id'].count()
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old, n_new],alternative='smaller') $ print(z_score,p_value)
train=mydf.sample(frac=0.9,random_state=200)$ test=mydf.drop(train.index)
iv = options_frame[options_frame['Expiration'] == '2016-03-18']$ iv_call = iv[iv['OptionType'] == 'call']$ iv_call[['Strike', 'ImpliedVolatilityMid']].set_index('Strike').plot(title='Implied volatility skew')
countries_df = pd.read_csv('countries.csv')$ countries_df.head() # we firstly see that we need to match the user_id $
dataframe.head()
sale2_table = sale_prod_table.groupby(['Product', 'Country']).SalePrice.mean()$ sale2_table
Google_stock = pd.read_csv('~/workspace/udacity-jupyter/GOOG.csv')$ print('Google_stock is of type:', type(Google_stock))$ print('Google_stock has shape:', Google_stock.shape)$
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger')$ print ('z-score:',z_score)$ print ('p-value:',p_value)
store_items = store_items.rename(index = {'store 2': 'last store'})$ store_items
response = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?column_index=4&start_date=2018-02-01&end_date=2018-02-01&collapse=daily&transform=rdiff&api_key={}'.format(API_KEY))$ print(response.json())
page.status
[i for i in train.columns if i not in test.columns],[i for i in test.columns if i not in train.columns]
df.groupby( 'Hour' ).count()['passenger_count'].plot()$
s = pd.Series([1,3,5,np.nan,6,8])$ print(s)
df_2018.dropna(subset=['Specialty'], how='all', inplace=True)
merge[merge.columns[17]].value_counts().sort$
p_new = .1196
autos["gear_box"].unique()$
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
p_sort = prod_sort.merge(sale_prod_sort,how='left', left_on='Country', right_on='Country')$ p_sort['SalePrice'] = p_sort['SalePrice'].fillna(0)$ p_sort
afx_x_oneday = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2014-12-01&end_date=2014-12-01&api_key=F57SPh3gyaXMHjLAp-pY')$
df_genres = df.groupby("genre")$ print(df_genres)$ print(df_genres['score'].mean().sort_values(ascending=False))
experience = pd.get_dummies(questions['rate_experience'])
exploration_titanic.nacolcount()
tobs_df=pd.DataFrame(date_tobs, columns=['date','tobs'])$ tobs_df.head()
df2['ab_page'] = pd.get_dummies(df2['landing_page'])['new_page']
text = df.section_text[2461]$ text
pd_qnt = pd.merge(dst,transactions,how='left',on=['UserID','ProductID'])$ usr_qnt = pd_qnt.groupby(['UserID','ProductID'])['Quantity'].sum()$ usr_qnt.reset_index().fillna(0)
countries_df = pd.read_csv('countries.csv')$ df_joined = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_joined.head()
rob = df.loc['lossyrob'].resample('D').size() # we can drop the users level with access by .loc and a username$
p_converted = df['converted'].mean()$ print('Proportion of converted users: ',p_converted)
df_birth.population.value_counts(dropna=False).head()
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)$ conn = engine.connect()
df2_control = df2[df2['group'] == 'control']['converted'].mean()$ print('The probability that an individual in the control group converted is: {}'.format(round(df2_control, 4)))
e.__class__
results.summary()
!sed -i -e 's/if self.max_leaf_nodes == "None":/if self.max_leaf_nodes == "None" or not self.max_leaf_nodes:/' \$   /usr/local/lib/python3.5/dist-packages/autosklearn/pipeline/components/regression/gradient_boosting.py
movies=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\movies.csv')
stn_rainfall = session.query(Measurement.station, func.sum(Measurement.prcp)).filter(Measurement.date >= data_oneyear).group_by(Measurement.station).order_by(func.sum(Measurement.prcp).desc()).all()$ stn_rainfall$
print total.shape[0], total.dropna().shape[0]$ total.head()
(null_vals > p_observed).mean()
xml_in[xml_in['publicationKey'].isnull()].count()
%%writefile trainer/__init__.py$
df.shape
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))$
print(discharge_list[0].date)$ print(discharge_list[len(discharge_list)-1].date)
%matplotlib inline $ import matplotlib.pyplot as plt # this imports the plotting library in python}
print('ensemble model performace is: {}'.format(np.around(f1_score(actual1, pred1, average='weighted'), decimals=5)))$
country.name.unique()
word_counts = bow_features(sentences, words_in_articles)$ word_counts.shape$
import pickle$ df = pickle.load(open("kicks.pkl",'rb'))
player_totals = pd.merge(table_1, table_2, left_index = True, right_index = True)$ player_totals
databreach_2017 = databreach_2017.dropna(axis=0,how='any')
df = pd.read_csv(r"C:\Users\Adi\Desktop\Data_Science\Capstone1\DataSet.csv")$ df.info()
datos['AdS'] = np.array([ analiza_sentimiento(tweet) for tweet in datos['Tweets'] ])$ display(datos.head(10))
ref_vars[12].strip()
taxi_sample = conn.select_ipc_gpu(query, device_id=2)
fitfile = FitFile('fit_files/2871238195.fit')#longer swim two way$ fitfile = FitFile('fit_files/2913114523.fit')#krumme lanke swim$
RE_EMOJI = re.compile('[\U00010000-\U0010ffff]', flags=re.UNICODE)$ def strip_emoji(text):$     return RE_EMOJI.sub(r'', text)$
dataset.User.value_counts()
store_items.fillna(method='ffill', axis=1) # filled with previous value from that row
subreddit = 'AskReddit'$ sr = reddit.subreddit(display_name = subreddit)
%timeit articles['tokens'] = articles['content'].map(nltk.tokenize.RegexpTokenizer('[A-Za-z]+').tokenize)
year1 = driver.find_element_by_name('2001')$ year1.click()
df.info()
with open("../data/reputation.txt") as f:$     data = f.read()
sns.set_color_codes("pastel")$ sns.despine(offset=10, trim=True)
df = df.swaplevel('Description', 'UPC EAN')$ df.head(3)
hn = pd.read_csv('../data/HN_posts_year_to_Sep_26_2016.csv', index_col='id', parse_dates=['created_at'])$ hn.dtypes
train3.shape$
%time df = pd.read_csv("/data/311_Service_Requests_from_2010_to_Present.csv", error_bad_lines=False)$
time_columns = ["created_at", "user_created_at"]$ for column in time_columns:$     data[column] = data[column].apply(lambda x: datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo = pytz.UTC))
criteria = so['ans_name'].isin(['Scott Boston', 'Ted Petrou', 'MaxU', 'unutbu'])$ so.loc[criteria].head()
print("Probability an individual recieved new page:", $       ab_file2['landing_page'].value_counts()[0]/len(ab_file2))
a = dat['police district'].unique()$ a.sort()$ a
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'})$ station_count = station[['Count']]$ station_count $
austin['driver_rating'] = austin['driver_rating'].apply(lambda x: round(x))$ driver_rat = austin['driver_rating'].value_counts().sort_index()$ print(driver_rat)
%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)), dtype=x.dtype, count=len(x))
es_dictindex = "{0}_dictionary".format(city.lower())$ es_dicttype = 'dictionary'$ es.createOrReplaceIndex(es_dictindex)
dfHaw_Discharge['flow_MGD'] = dfHaw_Discharge['meanflow_cfs'] * 0.64631688969744
logit_new_countries = sm.Logit(df_countries['converted'], df_countries[['intercept','new_page','CA_new','UK_new','CA','UK']])$ result_countries_new = logit_new_countries.fit()$ result_countries_new.summary()
autos.columns = columns
todays_date = datetime.datetime.now()
sandag_df_w_pred.to_csv('sandag_df_w_pred.csv')
words_only_scrape_freq = FreqDist(words_only_scrape)$ print('The 20 most frequent terms (terms only): ', words_only_scrape_freq.most_common(20))
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))$
list(data.dropna(thresh=int(data.shape[0] *.9), axis=1).columns)
autos["date_crawled"].str[:10].\$ value_counts(normalize=True,dropna=False).\$ sort_index(ascending=True)
display(results.summary())$
flight.crosstab('start_date','from_city_name').show()$
babies.head()
Test.dtypes
df_arch_clean[df_arch_clean.text.str.contains(r"([0-9]+\.[0-9]*\/\d+)")]
df = pd.read_csv('ab_data.csv', sep=",")$ df.head()
contractor_merge = pd.merge(contractor_clean, state_lookup,$                             on=['state_id'], how='left')
all_noms[(all_noms["agency"] != "Foreign Service") & (all_noms["confirmed"] == "yes")]["nom_count"].sum()
featured_image_url = soup.select_one("figure.lede img").get("src") $ print(featured_image_url)
pd.to_datetime(['2009/07/31', 'asd'], errors='coerce')
new_page_converted.mean() - old_page_converted.mean()
s_median = s.resample('5BM').median()$ s_median
mgxs_lib.build_hdf5_store(filename='mgxs.h5', directory='mgxs')
[each for each in df2.columns if 'ORIGIN' in each.upper()]
big_data_station = big_data.groupby('STATION')$
a = tweets.apply(lambda row: countOcc(row['tokens']), axis=1)$ sorted_x = sorted(occ.items(), key=operator.itemgetter(1), reverse=True)$ sorted_x
temps_df.iloc[1]
eve_new= len(df_aft.query('ab_page==1'))$ eve_old= len(df_aft.query('ab_page==0'))$ eve_prob =df_aft.converted.mean()
punkt = nltk.data.load('tokenizers/punkt/english.pickle')
result = cur.fetchall()$
properties.head(1)
data = data.dropna(subset=['name'], how='any')$ data.info()
rides_fare_average_max = rides_analysis["Average Fare"].max()$ rides_fare_average_max
noTempNullDF.head(10)
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True).hist()
df_data=pd.DataFrame({'time':(times+utcoffset).value[:-sa],'chips':final.chips.values})$
bm1 = list(map(int, benchmark11))$ print('benchmark1 score is: {}'.format(np.around(f1_score(actual1, bm1, average='weighted'), decimals=5)))$
df_new.groupby('country').mean()['converted']
X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75)$ y_train = np.log(y_train + 1)$ y_test = np.log(y_test + 1)
os.chdir(root_dir + "data/")$ df_fda_drugs_reported = pd.read_csv("filtered_fda_drug_reports.csv", header=0)
tobs_date_df = pd.DataFrame.from_records(tobs_date)$ tobs_date_df.head()
print([X.shape[0],y.shape[0]])
ctc = pd.DataFrame(columns=ccc, index=ccc)
dj_df =  getTextFromThread(urls_df.iloc[0,0], urls_df.iloc[0,1])$ for i in range(1,5):$         dj_df.append(getTextFromThread(urls_df.iloc[i,0], urls_df.iloc[i,1]), ignore_index = True)
df_ad_airings_5['state'] = df_ad_airings_5['metro_area_state'].map(lambda x: x[1])
data.index = pd.to_datetime(data.index)$ data.interpolate(method='time',inplace=True)$
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep =' ')$ df.head(5)
print('Before removing reactivations:',df.shape)$ df = df[df.Injury != 0]$ print('With only placements onto the Disabled List:',df.shape)
crimes[['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']].head()
stn_cnt_df=pd.DataFrame(stations_des,columns=['Station','Counts'])$ stn_cnt_df.head()
twitter_archive[twitter_archive['tweet_id'].duplicated()]
dr_new_data_plus_forecast.to_csv('./data/dr_new_patients_arimax_forecast.csv')$ dr_existing_data_plus_forecast.to_csv('./data/dr_existing_patients_arimax_forecast.csv')
df_atends['Prefeitura Regional'].value_counts()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/EON_X.json?api_key='+API_KEY+'&start_date=2018-06-28&end_date=2018-06-28')
for col in list(df.columns) :$     k = sum(pd.isnull(df[col]))$     print(col, '{} nulls'.format(k))
tweets = megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False]$ megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False]['text'].count() # 895
embarked.value_counts().sort_index().plot(kind='bar')$ embarked.value_counts()
inter = est.get_prediction(X_tab.astype(float))$ inter = inter.summary_frame(alpha=0.05)[['mean_ci_lower','mean_ci_upper']]$ inter.rename(columns={ 'mean_ci_lower':'CI lower', 'mean_ci_upper':'CI upper'})
df = pd.read_sql("SELECT * FROM Sections ", con=engine)$ df.head(3)
driver.get("http://www.reddit.com")$ time.sleep(2)$ html = driver.page_source
stoplist = stopwords.words('english')$ stoplist.append('free')$ print(stoplist)
plt.hist(np.log(threeoneone_census_complaints['median_income_new']+1),bins=100)$ plt.show()
new_page_converted = new_page_converted[:145274]$ new_page_converted.mean() - old_page_converted.mean()
train.shape[0] + new.shape[0]
serieschallenge5 = data.groupby(['KEY2','DATE'])['EntriesDifference'].agg(pd.np.sum)$
obs_diff=df2[(df2.group == 'treatment')].converted.mean() - df2[(df2.group == 'control')].converted.mean()$ (p_diffs > obs_diff).mean()
   $ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=amQDZBVZxsNFXgn8Dmpo')$ r.json()['dataset']['data'][1]
twitter_archive.describe()
cities = xmlData['city'].value_counts().reset_index()$ cities.columns = ['cities', 'count']$ cities[cities['count'] < 5]
typesub2017['MTU2'] = typesub2017.MTU.str[:16]$ typesub2017['DateTime'] = pd.to_datetime(typesub2017['MTU2'])
acs_df = pd.read_csv('./Datasets/ACS_Cleaned.csv').drop('Unnamed: 0', axis=1)$ acs_df.head()
recipes['ingredients'].str.len().idxmax()
price_series = pd.Series(price_dict)$ odom_series = pd.Series(odom_dict)
df_links = df_links[df_links['link.domain'] != 'twitter.com']
norm.ppf(1-(0.05/2))$
X_trainfinal = X_trainfinal.rename(columns={'fit': 'fit_feat'})
import pandas as pd$ dataset = pd.ExcelFile("basedados.xlsx")$ data = dataset.parse(0)
dfSummary[4:9].plot(kind='bar',$                     figsize=(20,6),$                     title="Data Summaries: Quantiles");
url = 'https://mars.nasa.gov/news/'
my_gempro.blast_seqs_to_pdb(all_genes=True, seq_ident_cutoff=.7, evalue=0.00001)$ my_gempro.df_pdb_blast.head(2)
dfCleaningData = dfData.dropna()$ dfCleaningData.count()
pax_raw = pax_raw.merge(n_user_days, on='seqn', how='inner')
cityID = '946ccd22e1c9cda1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Pittsburgh.append(tweet) 
lr = LogisticRegressionCV(n_jobs=3)$ lr.fit(X_train[['avg_shifted_against', 'def_shift_pct']], y_train)
data.groupby(['Year', 'Department'])['Salary'].sum()
pd.to_datetime(['2009/07/31', 'asd'], errors='raise')
print('the one user_id repeated is {}'.format(df2[df2.duplicated(['user_id'], keep=False) ]['user_id'].iat[0]))$
dataset = pd.read_csv("NYTimesBlogTrain.csv")
bucket.upload_dir('data/wx/tmy3/proc/', 'wx/tmy3/proc', clear_dest_dir=True)
df2['tweet_id'][(df2['predict_2_breed'] == True)].count()/2075$
winpct.loc[winpct['text'].apply(lambda x: any(re.findall('Santos',x)))][['playId','homeWinPercentage','playtext','date']]
DBPATH = 'results.db'$ conn = sqlite3.connect(DBPATH)$ cur = conn.cursor()
r6s.groupby('created')['num_comments'].sum().plot()
df = pd.concat([df.title, df.type], axis=1, keys=["title", "type"])$
for key, value in machine_labeled.iteritems(): # for each media$     machine_labeled[key].to_csv("../Machine_labeled_Haru/machine_labeled_"+key+".csv")
past_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ past_year
df_eng['time_stamp'] = pd.to_datetime(df_eng['time_stamp'])$
mean = np.mean(data['len'])$ print ("The length's average in tweets: {}".format(mean))$
n_old=df2.query("landing_page=='old_page'").user_id.count()$ n_old
print()$ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
plt.scatter(dataframe['lat'], dataframe['lon'],c = dataframe['label'],cmap=plt.cm.Paired)$ plt.scatter(center['lat'],center['lon'],marker='s')$ plt.show()
results = sm.OLS(gdp_cons_df.Delta_C1[141:280], gdp_cons_df.Delta_Y1[141:280]).fit()$ print(results.summary())
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','US','UK']])$ results=lm.fit()$ results.summary()
predictions = np.array([item['classes'] for item in classifier.predict(input_fn=eval_input_fn)])$ ids = np.array([i + 1 for i in range(len(predictions))])$ output = pd.DataFrame({id_label:ids, target_label:predictions}, dtype=np.int32)
titanic3['home.dest'].str.upper().head()
unitech_df.tail(5)
df_=data[['QG_mult','QG_ptD','QG_axis2']].copy()$ df_['target']=data['isPhysG']$ df_.head()
df['age_fillna'] = df.age.fillna(value=34)$ df.info()
df.shape
QUIDS_wide.dropna(subset =["qstot_0"], axis =0, inplace=True)
df_new[['CA','UK']] = pd.get_dummies(df_new['country'])[['CA','UK']] #choosing to show CA and UK in terms of US
print("Converted users proportion is {}%".format((df['converted'].mean())*100))
companies = ["WIKI/ATVI.11","WIKI/ADBE.11","WIKI/AKAM.11","WIKI/ALXN.11","WIKI/GOOGL.11","WIKI/AMZN.11","WIKI/AAL.11","WIKI/AMGN.11","WIKI/ADI.11","WIKI/AAPL.11","WIKI/AMAT.11","WIKI/ADSK.11","WIKI/ADP.11","WIKI/BIDU.11","WIKI/BIIB.11","WIKI/BMRN.11","WIKI/CA.11","WIKI/CELG.11","WIKI/CERN.11","WIKI/CHKP.11","WIKI/CTAS.11","WIKI/CSCO.11","WIKI/CTXS.11","WIKI/CTSH.11","WIKI/CMCSA.11","WIKI/COST.11","WIKI/CSX.11","WIKI/XRAY.11","WIKI/DISCA.11","WIKI/DISH.11","WIKI/DLTR.11","WIKI/EBAY.11","WIKI/EA.11","WIKI/EXPE.11","WIKI/ESRX.11","WIKI/FAST.11","WIKI/FISV.11","WIKI/GILD.11","WIKI/HAS.11","WIKI/HSIC.11","WIKI/HOLX.11","WIKI/IDXX.11","WIKI/ILMN.11","WIKI/INCY.11","WIKI/INTC.11","WIKI/INTU.11","WIKI/ISRG.11","WIKI/JBHT.11","WIKI/KLAC.11","WIKI/LRCX.11","WIKI/LBTYA.11","WIKI/MAR.11","WIKI/MAT.11","WIKI/MXIM.11","WIKI/MCHP.11","WIKI/MU.11","WIKI/MDLZ.11","WIKI/MSFT.11","WIKI/MNST.11","WIKI/MYL.11","WIKI/NFLX.11","WIKI/NVDA.11","WIKI/ORLY.11","WIKI/PCAR.11","WIKI/PAYX.11","WIKI/PCLN.11","WIKI/QCOM.11","WIKI/REGN.11","WIKI/ROST.11","WIKI/STX.11","WIKI/SIRI.11","WIKI/SWKS.11","WIKI/SBUX.11","WIKI/SYMC.11","WIKI/TSLA.11","WIKI/TXN.11","WIKI/TSCO.11","WIKI/TMUS.11","WIKI/FOX.11","WIKI/ULTA.11","WIKI/VRSK.11","WIKI/VRTX.11","WIKI/VIAB.11","WIKI/VOD.11","WIKI/WBA.11","WIKI/WDC.11","WIKI/WYNN.11","WIKI/XLNX.11"]#"WIKI/PCLN.11",
df2.to_csv('ab_data_new.csv', index=False)
xmlData.drop('address', axis = 1, inplace = True)
new.describe()
df_twitter_copy['score_rating'] = df_twitter_copy.rating_numerator / df_twitter_copy.rating_denominator
data = pd.read_sql("SELECT salary FROM empvw_20",xedb)$ print(data)
   $ save_model('model_random_forest_v1.mod', random_forests_grid)    
archive_df_clean['tweet_id'] = archive_df_clean['tweet_id'].astype(str)$ status_df['tweet_id'] = status_df['tweet_id'].astype(str)$
boxed_closes = pd.DataFrame(closes[BOX_START:BOX_MID], columns=['msft','amzn','googl','fb'])$ avg_closes = pd.DataFrame(closes[BOX_START:BOX_MID], columns=['avg'])
rows_in_table = len(ins)$ unique_ins_ids = len(ins['business_id'].unique())
all_sets.shape
jail_census.loc["2017-02-01"]
total=extraD$ total.loc[:,'exposure_time']=total.apply(exposure_redshift, args=('normal',), axis=1)
df[(abs(df['Open']-df['High'])<0.2 ) | (abs(df['Close']-df['Low'])<0.2)]
master_df_rand = master_df.sample(frac = 0.067, random_state = 1)$ master_df_rand = master_df_rand.reset_index(drop=True)$ master_df_rand
from statsmodels.api import Logit$ from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
df = pd.read_csv('./dataset/master_06-02-2018(hotposts).csv')$ df.drop(columns='Unnamed: 0', axis=1, inplace=True)
types = ['doggo', 'floofer', 'pupper', 'puppo']$ tweet_archive_clean = tweet_archive_clean.drop(types, axis=1)
options_frame['ModelError'].hist()$
data.info()
list(db.osm.aggregate([{"$group":{"_id": "$type", "count":{"$sum": 1}}}]))
mft = most_followed_tweeters.followers$ normalized_followers = np.round((np.abs(np.array(mft)-np.array(mft).mean())/mft.std())*10, 0)
df = df.drop(['index', 'starttime', 'stoptime', 'start station name', 'end station name', 'usertype'], axis=1)
df['DAY_OF_WEEK'] = df['DATE'].dt.dayofweek
tweet_clean.info()
Largest_change_between_any_two_days = mydata['Close'].diff().max() $ Largest_change_between_any_two_days
firstWeekUserMerged.isnull().sum()
df.tail()$
files = os.listdir(os.getenv('PUIDATA') + '/archives')
pickle.dump((fraud_data_updated),open('preprocess2.p', 'wb'))$
sn.distplot(a.A.flatten()[:],kde=False,norm_hist=True,bins=900)$ plt.xlim((0.,0.5))
city_dict = {}$ for city in cities_list:$     city_dict[city] = api.geo_search(query = city, wait_on_rate_limit=True, granularity = 'city')[0].id
csvHead = pd.read_csv('Data/TripData/JC-201706-citibike-tripdata.csv')
tweets['created_at'] = pd.to_datetime(tweets['created_at'])$ tweets.dtypes
(act_diff < p_diffs).mean()
ts.tshift(-1,freq="H")
a = master_file.iloc[:, 28:-1].count(axis=0)$ a = len(master_file.index) - a$ print(a)
plt.subplot(2,2,1)$ plt.subplot(2,2,2)$ plt.show()
df.shape$
caption_text_clean = [review_to_words(doc, True) for doc in captions.caption]$ caption_text_clean[:10]
events_by_waterway = merged_cso_data.groupby(['Waterway Reach'])['Gate Open Period'].count().sort_values(ascending=False)$ events_by_waterway.plot(kind='bar')
(df2.converted == 1).sum()/len(df2)
df1 = pd.DataFrame(X_selected_features)$ df1['labels'] = pd.Series(y)$ sns.pairplot(df1, hue='labels')
data = pd.read_csv("ml-100k/u.data", sep="\t", header=None, index_col=0)$ data.columns = ["item id" , "rating" , "timestamp"]
df2[df2.user_id == 773192]
word_count = np.array([len(d.split(' ')) for d in docs])$ print('Done.')
linear_model_p3 = sm.OLS(df_c_merge['converted'], df_c_merge[['intercept', 'ab_page', 'US', 'UK']])$ lin_results = linear_model_p3.fit()$ lin_results.summary()
gr_e2 = df.query("group == 'control' and landing_page == 'new_page'")
df5 = make_df('ABC', [1, 2])$ df6 = make_df('BCD', [3, 4])$ display('df5', 'df6', 'pd.concat([df5, df6])')
df.describe()
pd.options.display.max_colwidth = 200$ data_df[['ticket_id','type','clean_desc','nwords']].head(30)
gdf.plot();
logit_new = sm.Logit(df_new['converted'],df_new[['intercept','CA','US','treatment']])$ results_new = logit_new.fit()$ results_new.summary()
salidaFinal2.count()
QUIDS_wide["y"] = QUIDS_wide[['qstot_12','qstot_14']].apply(lambda x: x['qstot_14'] if np.isnan(x['qstot_12'])$                                                             else x['qstot_12'], axis=1)
df_raw_fb = pd.read_csv('./Datasets/Facebook_Training_Data.csv', encoding='latin1')$ print (df_raw_fb.head())
total_ctrl = df2.query('group == "control"').user_id.nunique()$ prob_ctrl = total_ctrl/unique_users$ conv_and_ctrl = df2[(df2['group']=='control') & (df2['converted']== 1)].user_id.nunique() $
news_t = soup.find("div", class_="content_title").text
df = pd.read_csv("/data/311_Service_Requests_from_2010_to_Present.csv", error_bad_lines=False)$
archive_clean.head(3)
store_items = store_items.set_index('pants')$ store_items
df.groupby('key').sum()
tweets = tweets_orig.copy()$ ip = ip_orig.copy()$ extended_tweets_orig = extended_tweets.copy()
df_newhouse.reset_index(level=0,inplace=True)$ df_newhouse.columns = ["Date","Average_Housing_permits"]$ df_newhouse.head(5)$
percipitation_year = session.query(Measurements.date, func.avg(Measurements.prcp)) \$     .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \$     .group_by(Measurements.date).all()
no_hyph = df_nona[df_nona['variety']\$                     .apply(lambda x: len(x.split('-')) < 2)]['variety'].str.lower()$ no_hyph = no_hyph[no_hyph.apply(lambda x: x.split()[-1] != 'blend')].replace(repl_dir)
import pandas as pd$ import matplotlib.pyplot as plt$ dataframe = pd.read_csv("assignment.csv")
df_totalVisits_byCountry = df_visits.groupby(['country_code'], as_index=False).sum()$ print 'DataFrame df_totalVisits_byCountry', df_totalVisits_byCountry.shape$ df_totalVisits_byCountry.sort('user_visits', ascending=[0])
df=pd.read_csv('ab_data.csv')$ df.head()
active_station_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).filter(Measurement.date > last_year).filter(Measurement.station == most_active_station).all()$
df_plot=pd.DataFrame(df_recommended_menues['dates'].value_counts())$ df_plot.head()
temps_df.Missoula - temps_df.Philadelphia
import alpha_vantage$ from alpha_vantage.timeseries import TimeSeries$ ts = TimeSeries(key='1250F9WWA3Z77BIK')$
date_cal2 = pd.to_datetime(cars2['dateCrawled'])$ date_cal2 = date_cal2.dt.year$ cars2['Age'] = date_cal2 - cars2.yearOfRegistration$
from nltk.stem.wordnet import WordNetLemmatizer$ lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]$ print(lemmed)
df['hashtags'] = df.text.str.extract('(#\w*)')
client.training.get_details('training-WWmHAB5ig')
temps_df.Missoula > 82
obs_diff = np.mean(df.query('group == "treatment"')['converted']) - np.mean(df.query('group == "control"')['converted'])$ obs_diff
print pd.concat([s1, s2, s3], axis=1, keys=['S1', 'S2', 'S3'])
df_members = df_members[df_members.registered_via != -1]
df.query('group != "treatment" and landing_page == "new_page"')$ df.query('group == "treatment" and landing_page != "new_page"')$
(act_diff < p_diffs).mean()
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href'][9:]
geocode_endpoint = 'https://maps.googleapis.com/maps/api/geocode/json?'
learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)
twitter_archive_clean['favorite_count'].corr(twitter_archive_clean['retweet_count'])
pd.DatetimeIndex(pivoted.columns)
appointments['AppointmentCreated'] = pd.to_datetime(appointments['AppointmentCreated'], errors='coerce')$ appointments['AppointmentDate'] = pd.to_datetime(appointments['AppointmentDate'], errors='coerce')
print(df['State'].value_counts(dropna=False))
ab_df_new[['CA','UK','US']] = pd.get_dummies(ab_df_new.country)
ms_columns = inspector.get_columns('measurements')$ for c in ms_columns:$     print(c['name'], c["type"])$
data = pd.Series([0.25,0.5,0.75,1.0],$                 index=[2,5,3,7])$ data
df2['user_id'].duplicated().sum()
session.query(measurement.date).order_by(measurement.date).first()
free_data.describe()
test_data.columns
for title, artist in unique_title_artist[current_len:min(current_len+batch_size, len_unique_title_artist)]:$     youtubeurl = urllib.parse.quote_plus(YOUTUBE_URL_TEMPLATE.format(gc.search(title +' '+artist)), safe='/:?=')$     youtube_urls[str((title, artist))] = youtubeurl
df = raw_df.copy()$
df_predictions_clean['tweet_id'] = df_predictions_clean.tweet_id.astype(str)
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
table_grouped_type=original_merged_table.groupby("type")$
num_id = df.nunique()['user_id']$ print("{} unique users in the dataset.".format(num_id))
autos.dtypes
new_page_converted = np.random.choice([0,1] ,size = n_new, p=[(1-p_new),p_new])
precipitation_df.dropna(inplace=True)$ precipitation_df.head()
pystore.list_stores()
indexed_activity = active_df.set_index("station")
mlp = df[['Median Listing Price']].values$ print('shape:\n', mlp.shape, '\n',$      '1st 5 rows:\n', mlp[0:5, :])
with open(os.path.join(url.split('/')[-1]), mode='wb') as file:$     file.write(response.content)
building_pa = pd.read_csv('Building_Permits.csv')$ building_pa.head(5)
archive_df = pd.read_csv('twitter-archive-enhanced.csv', encoding = 'utf-8')$ archive_df.info()
tweet_group_df = tweet_df.groupby(["Target"])['Compound'].agg(['mean']).sort_index().reset_index()$ tweet_group_df = tweet_group_df.rename(columns={"mean":"Tweet Polarity"})$ tweet_group_df.head()
affair_children = pd.crosstab(data.children, data.affair.astype(bool))$ affair_children
rain_2017_df = rain_df.set_index("date")$ rain_2017_df.head()$
p_diffs = np.array(p_diffs)$ plt.hist(p_diffs)$ null_vals = np.random.normal(0,np.std(p_diffs),10000)#Converting to be a distribution under tha null
nold=df2.query("group == 'control'")$ n_old=nold.shape[0]$ n_old
df['Date Created'] = pd.to_datetime(df['Date Created'])$ df['Date Closed'] = pd.to_datetime(df['Date Closed'])
print ('Python Version: %s' % (sys.version.split('|')[0]))$ hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER$ print ('HDFS filesystem running at: \n\t %s' % (hdfs_conf[0]))
from datetime import datetime $ from dateutil import parser$ ind_date= pd.bdate_range('2015-01-01','2015-12-31') 
p_old_null = df.converted.mean()$ p_old_null
df2[(df2.group == 'treatment')].converted.mean()
print('Variance score: %.2f' % regressor.score(test_features, test_occupancy))
archive_df_clean['tweet_id'] = archive_df_clean.tweet_id.apply(str)
points=pd.Series([630,25,26,255],$     index=['India','Bangladesh','Pakistan','China'])$ print(points)
pop_flat.set_index(['states', 'year'])
club_data = data[data['competitionid'] <> 4]$ data[data['competitionid'] == 4].iloc[0]
data.loc[[pd.to_datetime("2016-09-02")]]
vacc15_17['Month'] = pd.to_datetime(vacc15_17['Month'], format="%m/%d/%Y")$ vacc15_17.head()
roundedDF=np.around(overallDF, decimals=2)$ roundedDF["Compounded Score"]
plt.figure(figsize = (10, 10))$ df_master[['retweet_count', 'favorite_count']].plot(style = '.')$ plt.title('Retweet and Favorite Counts');
segments.st_time.dtype$ datetime.strptime(segments.st_time.loc[0],'%m/%d/%y %H:%M')
abunai_tweet_array_df = pd.DataFrame(data=abunai_tweet_array, columns=['tweet']) $ abunai_date_array_df = pd.DataFrame(data=abunai_date_array, columns=['date']) 
model_data = pd.read_pickle('D:/CAPSTONE_NEW/jobs_data_final.pkl')
logit_countries = sm.Logit(df4['converted'], $                            df4[['country_UK', 'country_US', 'intercept']])$ result2 = logit_countries.fit()
nitrogen = results[mediaMask & hydroMask & charMask & sampFracMask] $ nitrogen.shape
directory = './Models'$ if not os.path.exists(directory):$     os.makedirs(directory)
engine.return_as_panda_dataframe = True
tree_features_df['filename'].isin(manager.image_df['filename']).describe()$
Measurements = Base.classes.hawaii_measurement$ Stations = Base.classes.hawaii_station$
autos["ad_created_year"] = autos["ad_created"].dt.year$ autos["ad_created_month"] = autos["ad_created"].dt.month$
status_data = status_data.dropna()
for name, val in zip(x.columns, adult_model.feature_importances_):$     print(f'{name} importance = {100.0*val:5.2f}%')
trip_month_day = trip_dates.strftime('%m-%d')$ trip_month_day
df2.head()
df2[df2.user_id.duplicated()]
aapl = getStockPrice("AAPL",1982, 1, 1, 2018, 1, 23)$ aapl.head()$
sn.distplot(train['age'])
new_df = df.fillna(0)$ new_df
apple['2017-07']['Close'].mean()
pulledTweets_df['Processed_tweet'] = pulledTweets_df['text'].apply(cleaner)$ pulledTweets_df['Processed_tweet'] = pulledTweets_df['Processed_tweet'].apply(remove_stopwords)$ pulledTweets_df['Processed_tweet'] = pulledTweets_df['Processed_tweet'].apply(lemmatize)
len(email_age_unique[pd.isnull(email_age_unique['request.ageRange']) == True])
loan.loc[loan['ddl_dcl'] < 0].shape$ loan.loc[loan['appv_dcl'] < 0].shape$ loan.loc[loan['appv_ddl'] < 0].shape
%matplotlib inline$ df['Close'].plot()
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
total.first_valid_index()
old_page_converted = np.random.binomial(1, p_old,n_old)
prec_wide_df = pd.concat([grid_df, prec_df], axis = 1)$ prec_wide_df.head()
v = tmdb_movies.production_countries.apply(json.loads)
URL = "http://www.reddit.com"
plt.figure(figsize=(8, 5))$ plt.scatter(train_df.favs_lognorm, train_df.views);$ plt.title('The distribution of the favs_lognorm and number of the views');$
shows['plot'] = shows['plot'].dropna().apply(lambda x: x.lower())$ shows['plot'] = shows['plot'].dropna().apply(remove_punctuation)
vhd = pd.read_excel('input/Data.xlsm', sheet_name='52', usecols='A:AO', header=6, skipfooter=16)
pd.concat([df1,df2,df3])$
df.iloc[:4]
filepath = os.path.join('input', 'input_plant-list_SI.csv')$ data_SI = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_SI.head()
df_25year=df[df['date']>'1901-02-24'] #last 25 years$ df_25year.groupby('origin')['description'].agg('count').sort_values(ascending=False).head(1)
id_of_tweet = 932626561966247936$ tweet = api.get_status(id_of_tweet)$ print(tweet.text)
dt_1 = pd.datetime(2016, 1, 1)
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
scores_df.to_csv('../Results/News_Tweets.csv', encoding="utf-8", index=False)
df_sb['Sentiment_class'] = df_sb.apply(conditions, axis=1)$ df_sb.to_csv("sobeys_senti_score.csv", encoding='utf-8', index=False)$
noise_data["Created Date"].dtype
outliersDict = {key: df[abs(df['value'] - np.mean(df['value'])) > 3 * np.std(df['value'])] for key, df in typesDict.items()}$ outliersDict.keys()
ser = pd.Series(['Tues','Wed'], index=days)$ ser
! rm -rf ~/s3/comb/flight_v1_0.pq
df2 = df.copy()$ df2['one'] = 0$ df2
df_2008['bank_name'] = df_2008.bank_name.str.split(",").str[0]$
print(data.json()['dataset']['column_names'])
autos['price'].value_counts().sort_index(ascending=True).head(20)
model_sklearn=linear_model.LinearRegression()$ model_sklearn.fit(X_train,Z_train)$ model_sklearn.score(X_test,Z_test)
A = nx.to_numpy_matrix(G, weight='weight')$ np.savetxt("data/congress_actors_weighted.txt", A, fmt='%d')
liquor['State Bottle Retail'] = [s.replace("$","") for s in liquor['State Bottle Retail']]$ liquor['State Bottle Retail'] = [float(x) for x in liquor['State Bottle Retail']]$ liquor_state_retail = liquor['State Bottle Retail']
github_data.drop_duplicates('user_id').user_type.value_counts()
df['female'] = df['raw'].str.extract('(\d)', expand=True)$ df['female']
plt.hist(null_vals);$ plt.axvline(x=obs_diff, color='red')
control_converted=df2.query('group=="control"').converted.mean()$ control_converted
csvData['street'] = csvData['street'].str.replace(' Southeast', ' SE')$ csvData['street'] = csvData['street'].str.replace(' South', ' S')
new_df = df.replace(['poor','average','good','exceptional'], [1,2,3,4])$ new_df
px = pd.read_csv(dwld_key + '-hold-pricing.csv', index_col='Date', parse_dates=True)
ss_sparse = (~df_uro_dd_dummies.isnull()).sum() < 3$ ls_sparse_cols = ss_sparse[ss_sparse].index.tolist()
urban_driver_total = urban_type_df.groupby(["city"]).mean()["driver_count"]$ urban_driver_total.head()$
obs_diff_newpage = df2.query('landing_page == "new_page"')['user_id'].count() / df2.shape[0]$ obs_diff_newpage
df_from_csv.equals(df)
sox = pd.read_csv('../../../data/sox_master.csv')
act_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean()$ act_diff
int_and_tel.plot(y='sentiment_score', use_index= True, figsize=(18, 6))
merged_stops = pd.merge(stops, oz_stops, on='stopid', how='inner')
nan_rows(data).shape #(9402, 23) rows unmatched from meter location file$ nan_cols(data)
production_df = pd.merge(future_predictions, features, on=['Date', 'HomeTeam', 'AwayTeam', 'season'])
import pandas as pd$ bild = pd.io.json.json_normalize(data=bild)$ spon = pd.io.json.json_normalize(data=spon)
url_1day='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=TaaJzC_if3-1gX5Wty4D&start_date=2017-04-10&end_date=2017-04-10'$ response=requests.get(url_1day)
(y_hat.shape, y_train.shape)
data = json.loads(r.text)
for v in contin_vars:$     joined[v] = joined[v].fillna(0).astype('float32')$
df_ari = df_ari.sort_values('LastUpdatedC')$ df_ari = df_ari.reset_index(drop=True)
new_page_converted = np.random.binomial(1,p_new,n_new) $ new_page_converted.mean()
prev_year = dt.date.today() - dt.timedelta(days=365)$ prev_year
df.to_json("json_data_format_columns.json", orient="columns")$ !cat json_data_format_columns.json
import pandas as pd$ import matplotlib.pyplot as plt$ import pandasql as sqldf
sns.boxplot(autodf.price)
people = odm2rest_request('affiliations', {'organizationCode': ','.join(orgs)})
df_countries['country'].unique()
daily_transaction=file4.groupby('date').agg({'uuid':'count','trans_amt':'avg'}).sort(desc("date"))$ daily_transaction.show()
a=[0,1]$ new_page_converted = np.random.choice(a,145310,p=[0.8804,0.1196])$ print(new_page_converted.mean())
station_df['station'].count()
df_day['Forecast'] = bound_prediction(sarima_mod.predict())$ df_day$ df_day.plot(figsize=(14, 6));
s519397_df["prcp"].sum()
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))
df2[df2.duplicated(['user_id'], keep=False)]
merged2.dropna(subset=['Specialty'], how='all', inplace=True)
print(tweets[0].id)$ print(tweets[0].created_at)
pd.read_csv("test.csv", encoding="utf-8").head()
dfDay = dfDay[(dfDay['Date'].dt.year == 2018) | (dfDay['Date'].dt.year == 2019)]
from scipy.stats import norm$ norm.cdf(z_score)
crime = pd.read_csv('clean_data/KCPD_Crime_Data_2017_clean.csv')$ moon = pd.read_csv('clean_data/Moon_Data_2017_cleaned.csv')
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]$
Average_Compounds.to_csv("Average_Compound_Sentiments.csv")
readme = processing_test.README()$ readme = open(readme, "r")$ print(readme.read())
np.isnan(StockData).sum()
cursor.fetchall()
files_gps = []$ for x in range(771,797): #can change this range to pull any gps files; might be limited by API calls depending on #$     files_gps.append(syn.downloadTableFile(table_results, column='UnknownFile_1.json.items', rowId=x, versionNumber=1, ifcollision=u'keep.both'))
y_class = demo.get_class(y_pred)$ cm(y_test,y_class,['0','1'])
duration_by_waterway = merged_cso_data.groupby(['Waterway Reach'])['Gate Open Period'].sum().sort_values(ascending=False)$ duration_by_waterway.astype('int64').plot(kind='bar')
df.describe()
(new_page_converted.mean()) - (old_page_converted.mean())
df2.sort_values('total_comments',inplace=True,ascending=False)$ top_comments=df2.head(10)$ top_comments
df2.user_id.count() == df2.user_id.nunique()
import pandas as pd$ from pyspark.sql import SparkSession$ import numpy as np
cols = data_df.columns.tolist()$ cols = cols[-1:] + cols[:-1]$ cols
for row in cursor.columns(table='TBL_FCBridge'):$     print(row.column_name)
sns.distplot(data['Age'], kde = False, bins = 60)
new_df = df.dropna()$ new_df
weather_features = pd.DataFrame(index=weather_data.index)
df.iloc[99,3]
sess_df = pd.merge(left=users,right=sessions,how="inner",left_on=['UserID','Registered'],right_on=['UserID','SessionDate'])$ sess_df
! rm -rf recs1$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models1 --outdir recs1
pd.DatetimeIndex(pivoted.T[labels==1].index)[Tue_index]
sns.set()$ sns.distplot(df_input_clean.toPandas().Resp_time, kde=True, color='b')
plt.hist(taxiData.Trip_distance, bins = 20, range = [50,150])
df['sin_day_of_year'] = np.sin(2*np.pi*df.day_of_year/365)$ df['cos_day_of_year'] = np.cos(2*np.pi*df.day_of_year/365)
logging.info('Dropping unnecessary variables')$ data.drop(['Date', 'Open', 'High', 'Low', 'Market Cap', 'Dependent Variable'], axis=1, inplace=True)
df_ct = pd.read_csv("can_tire_all_final.csv", encoding="latin-1")
df = pd.read_csv('weather_data_austin_2010 (1).csv')$ cols = ['Temperature','DewPoint','Pressure']$ df = df[cols]
session.query(func.count(Measurement.tobs), Measurement.station).\$     filter(Measurement.date > year_ago).\$     order_by(func.count(Measurement.tobs).desc()).first()
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')$ logging.debug('Start of program')
compound_df = pd.DataFrame(save_compound_list)$ compound_df = compound_df.transpose()$ compound_df.head()
df['device_class'] = df.device_type.apply(lambda d: classify_device_type(d))$ df.drop(['device_family', 'device_id', 'device_model', 'device_type'], axis=1, inplace=True)
df2 = df2.drop_duplicates(subset='user_id', keep='first')$ df2[df2['user_id'] == 773192]$
data['intra_day_diff'] = data.High - data.Low$ data.head()
sale_prod_table = avg_sale_table.rename(columns = {'Sale Price':'SalePrice'})$ sale_prod_table.head()
treatment_convert = df2.query('group =="treatment"').converted.mean()$ print("Probability of treatment group converting is :", treatment_convert)
dataset = dataset[dataset['lang'].isnull() == False]$ dataset.shape
!head data/goog.csv$
stations_count = session.query(Measurement).group_by(Measurement.station).count()$ stations_count
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])
pd.Series({2:'a', 1:'b', 3:'c'})
results=pd.read_csv("results.csv")$ results.tail()$
df2['landing_page'].replace({'old_page':0,'new_page':1},inplace=True)$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=["Tweets"])$ display(data.tail(10))
(train.shape, test.shape)
btc.plot(y=['price'])$ plt.show()
for column in all_df.columns:$     print ("{:<20s} {:>6.0f}".format(column, all_df[column].nunique()))$     $
def errorEvaulation(predict, actual):$     return np.sqrt((np.log10(predict+1) - np.log10(actual+1) ** 2).mean())$
logit2 = sm.Logit(df_new['converted'],df_new[['intercept','old_page','CA', 'UK']])$ result1 = logit2.fit()$ result1.summary()
tallies_file.export_to_xml()
tweets = pd.concat([tweets1,tweets2,tweets3])$ tweets.shape
cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)$ cvModel = cv.fit(trainingData)
df3 = df3.dropna(how='all')$ print(df3.sample(5))
session.query(func.count(Stations.station)).all()$
sns_plot = sns.lmplot(x='score',y='favorite_count',data=rating_and_retweet,fit_reg=False,scatter_kws={'alpha':0.05})$ sns_plot.savefig("score_vs_favorite.jpg")
nba_df.loc[(nba_df['Season'] == 2015) & (nba_df['Team'] == "BOS") & (nba_df["Opp"] == "MIL") & (nba_df["G"] == 82), "Home.Attendance"] = avg_att_2015_BOS$ nba_df.loc[(nba_df['Season'] == 2015) & (nba_df['Team'] == "MIL") & (nba_df["Opp"] == "BOS") & (nba_df["G"] == 82), "Home.Attendance"] = avg_att_2015_MIL
print('reduce memory')$ utils.reduce_memory(tran_time_diff)$ tran_time_diff.info()
df = pd.read_csv('ab_data.csv')
avgcomp = groupedNews['Compound'].mean()$ avgcomp.head()
DataSet = DataSet[DataSet.tweetSource.notnull()]$ len(DataSet)
df3['timestamp'].max(),df3['timestamp'].min()
pax_raw = pax_raw.merge(keep_days, on=['seqn', 'paxday'], how='inner')
df_customers['MA_3'] = pd.rolling_mean(df_customers['number of customers'],window=3)$ df_customers['MA_6'] = pd.rolling_mean(df_customers['number of customers'],window=6)
actual_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ actual_diff
BPAIRED_SHOPIFY['shopify_order_id'] = [a.split('-')[0] for a in BPAIRED_SHOPIFY['channel_order_id']]
average_trading = statistics.mean([day[6] for day in data])$ print ('Average daily trading volume for 2017:', round(average_trading,2))
i = issues$ len(i[(i.activity > '2015-01-01')])
X = df.drop(ls_head.index('sr_flag'), axis='columns')$ X = X.drop(0, axis='columns')$ y = df[ls_head.index('sr_flag')]
import datetime$ data['Created Date'] = data['Created Date'].apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%y %H:%M'))
data = pd.read_csv('../comment.csv')$
data = data.dropna()
df['datetime'] = df.index.values
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
df_state_victory_margins = pd.read_csv('./data/state_victory_margins_data.csv')$ df_state_victory_margins
weather['dateShort'] = pd.to_datetime(weather['EST'])$ print weather.ix[:, 'dateShort'].head()
print(fitB_Cl12l21.summary2())
miss_grp2 = df.query("group =='control' and landing_page =='new_page'")$ print('The number of times a user from the control group lands on the new page is {}'.format(len(miss_grp2)))
df = pd.read_csv('ab_data.csv')$ df.head()
iris_new = pd.DataFrame(iris_mat, columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','Species'])$ iris_new.head(20)
df_users.invited_by_user_id.isnull().values.ravel().sum()
data.mean()
headers=[x.replace('-','_') for x in headers]
lal=df1['cleantime'].quantile(q=0.25) - (iqr*1.5) $ lal
round(np.sqrt(model_x.scale), 3)
cityID = '3df0e3eb1e91170b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Columbus.append(tweet) 
autos["vehicle_type"] = autos["vehicle_type"].map(vehicle_type_eng)$ autos["gearbox"] = autos["gearbox"].map(gearbox_eng)$ autos["fuel_type"] = autos["fuel_type"].map(fuel_type_eng)
automl.refit(X_test.copy(), y_test.copy())$ print(automl.show_models())
weather_df.loc[weather_df["weather_main"].isin(["Dust", "Sand", "Smoke", "Squall"]), ["weather_main", "weather_description"]] = np.NaN$ weather_df = weather_df.fillna(method="ffill")
configure['run']['duration'] = 0.5
df.isnull().any()
num_words=["roussia","jayin","page","chance"]$ print (words_df[ words_df['Word'].isin(num_words) ])
outname = 'user_dataset_results.csv' # You can rename this file to whatever you'd like. Using the same output filename multiple times could cause overwriting, be careful!$ fullpath = os.path.join('./output', outname)  $ combined_df.to_csv(fullpath, index=False)
%%time$ grid_svc.fit(X, y)
expected_index = pd.DatetimeIndex(start=raw.index[0], end=raw.index[-1], freq='1H')$ ideal_index_df = pd.DataFrame(index=expected_index)$ clean = pd.concat([ideal_index_df, raw], axis=1)$
ins.groupby("type").count()$
artistByID.filter(lambda line: '[unknown]' in line).take(5)
pd.Series(np.random.randn(1000)).plot(kind='hist', bins=20, color='Y');
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt"$ mydata = pd.read_csv(path, sep= '\t')$ mydata.head(5)
ADNI_diagnosis_data_description = pd.read_csv('ADNIMERGE_DICT.csv')$ print(ADNI_diagnosis_data_description['FLDNAME'].unique())$ ADNI_diagnosis_data_description[['FLDNAME','TEXT']]
chefdf.to_csv('exports/trend_data_chefkoch.csv')
bg2 = pd.read_csv('Libre2018-01-03.txt', sep='\t', nrows=100) # local$ print(bg2)$ print(type(bg2))
s = pd.Series(np.random.randint(0,7,size=10))  #low,high,size $ print(s,'\n')$ s.value_counts()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
estimator.predict_proba(X1)
data.describe()
max_sharpe_port_b = results_frame_b.iloc[results_frame_b['Sharpe'].idxmax()]$ min_vol_port_b = results_frame_b.iloc[results_frame_b['SD'].idxmin()]
len(df['user_id'].unique())
cust_group_date = df.groupby(['cust_id','order_date'])['cust_id'].count().reset_index(name="count")$ cust_group_date$
mgxs_lib = openmc.mgxs.Library(geometry)$ mgxs_lib.energy_groups = groups
countries_df = pd.read_csv('countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
log_mod = sm.Logit(df_new.converted,df_new[['intercept','CA','UK','new_page','CA_new_page','UK_new_page']])$ results = log_mod.fit()$ results.summary()
zstats,pvalue=sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old],alternative="larger")$ print("Zstats",zstats)$ print("pvalue",pvalue)$
image_predictions = pd.read_csv('image-predictions.tsv', sep = '\t', encoding = 'utf-8')$ image_predictions.info()
url = "https://bittrex.com/api/v1.1/public/getmarketsummaries"$ r = requests.post(url)  $ data = json.loads(r.content.decode())$
contest_data.where(F.col('end_customer_party_ssot_party_id_int_sav_party_id')== 7689590).take(1)#.select('endcustomerlinefixed','end_customer_party_ssot_party_id_int_sav_party_id','prior_party_ssot_party_id_int_sav_party_id','sales_acct_id','decision_date_time').take(10)
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
autos["last_seen"].str[:10].value_counts(normalize=True,dropna=False).sort_index(ascending=True)
new_page_converted=np.random.binomial(1,Conversion_Rate,Conversion_No)$
data_kb['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_kb['Tweets'] ])$ display(data_kb.head)
r= requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json")$ json_data = r.json()$ json_data["dataset_data"]["data"][0]
model.save_weights(filepath) $
df_clean3.loc[s3.index, 'name'] = 'Not a dog'
df = pd.read_excel('weather_nan.xlsx')$ df
duplicate_check_df = pd.DataFrame(duplicate_check_events, columns=duplicate_check_columns)$ duplicate_check_df.head()
twitter_archive_clean.to_csv('twitter_archive_master.csv')$ image_predictions_clean.to_csv('image_predictions_master.csv')
start_date = "2016-03-24"$ end_date = "2016-04-09"$ calc_temps(start_date,  end_date).mean(axis = 1)
import calendar $ month2int = {v.lower():k for k,v in enumerate(calendar.month_name)}$ month2int    
df['date'] = pd.to_datetime(df.date)$ df.set_index('date', inplace=True)
ks_name_lengths = ks_projects.groupby(['name_length', "state"]).size().reset_index(name='counts')$ ks_name_success = ks_name_lengths.drop(ks_name_lengths.index[ks_name_lengths.state != 'successful'])$ ks_name_success.set_index('name_length', inplace=True)$
df2.shape[0]
df_c_merge.groupby('country')['converted'].mean()
r = requests.get(url)$ json_data = r.json()
rmse_scores = np.sqrt(mse_scores)$ print(rmse_scores)
from h2o.estimators.glm import H2OGeneralizedLinearEstimator$ glm_model = H2OGeneralizedLinearEstimator(model_id = "GLM", family = "binomial")$ glm_model.train(x = predictor_columns, y = target, training_frame = train, validation_frame = valid)
df2.nunique()['user_id']
exiftool -csv -createdate -modifydate MVI_0011.mp4 > out.csv
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
df_json_tweets.info()
pd.DataFrame(dummy_var["_Source"][Company_Name]['Close']['Forecast'])[-6:]$
columns_name = ["content","creationTime","isTop","referenceId","userClientShow","isMobile"]$ content = re.findall(r'"showOrderComment".*?"content":"(.*?)","creationTime":"(.*?)","isTop":(\w*).*?"referenceId":"(\d*)".*?"userClientShow":"(.*?)".*?"isMobile":(\w*).*?',c)$ content$
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
cryptos['market_cap_usd_billions'] = cryptos.market_cap_usd / 1e9  $ cryptos.head()
vect = Seq2WordVecTransformer()$ X_vect = vect.fit_transform(X, verbose='debug')$ print ('len(X_vect):', len(X_vect))$
salida_all = getFacebookPageFeedData(page_id, access_token, 1000)$ columns = ['post_from', 'post_id', 'post_name', 'post_type', 'post_message', 'post_link', 'post_shares', 'created_time']$ df_posts = pd.DataFrame(columns=columns)
output= "Delete from user where user_id='@Pratik'"$ cursor.execute(output)$
dupl_row = df2_duplicated_row.index.values # retrieving index of duplicated row$ df2.drop(dupl_row,inplace=True) # dropping duplicated row from df2
Regex to get the Facebook Page ID from a given URL$ https://gist.github.com/marcgg/733592$ http://bhan0507.logdown.com/posts/1291544-python-facebook-api
logit_mod3=sm.Logit(df_new['converted'],df_new[['ab_page','intercept', 'CA', 'UK', 'CanadaNew','UKNew']])$ fit3=logit_mod3.fit()$ fit3.summary()
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)$ plt.hist(null_vals)$ plt.axvline(x=-0.001576, color='r')
df_loans_borrowers = pd.merge(df_borrowers, df_loans, left_on='id', right_on='borrower_id')$ df_loans_borrowers_countries = pd.merge(df_loans_borrowers, df_countries, left_on='country_id',right_on='id')
colArr=df['WHO Region'].get_values()$ colArr
step_counts.fillna(0., inplace=True)
print ("Data Frame with Forward Fill:")$ df2.reindex_like(df1,method='ffill')
for field_name, dtype in df.select_dtypes(include=categories).items():$     print(field_name)$     df[field_name] = pd.Series(pd.Categorical(df[field_name]).codes)
exiftool -csv -createdate -modifydate ciscih8/CISCIH8_cycle1.mp4 ciscih8/CISCIH8_cycle2.mp4 ciscih8/CISCIH8_cycle3.mp4 ciscih8/CISCIH8_cycle4.mp4 ciscih8/CISCIH8_cycle5.mp4 ciscih8/CISCIH8_cycle6.mp4 > ciscih8.csv
ratings_df = ratings_df.drop('timestamp', 1)$ ratings_df.head()
np.exp(results3.params)
merge.tail()$
from google.colab import auth$ auth.authenticate_user()
df1 = ml.select_features(indices.shape[0], indices, df)
SNL.groupby('Gender').Host.count()$
stationCounts = engine.execute('SELECT station, count(*) AS station_cnt FROM measurement group by station order by count(*) desc').fetchall()$ print (stationCounts)$
json_data= response.json()['dataset_data']$ print(json_data)$
dfincident = pd.read_csv("../../../Data/vw_Incident.csv",encoding='latin-1',low_memory=False)$ dfincident["Created_On"] = pd.to_datetime(dfincident["Created_On"])$
inspector = inspect(engine)$ inspector.get_table_names()
fed_reg_dataframe = pd.DataFrame.from_records(tuple_lst, columns=['date','str_text'], index = 'date')
TEXT.vocab.stoi['the']
df.head()
plot_autocorrelation(series=RNPA_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of RNPA hours new patients'))$ plot_autocorrelation(series=RNPA_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of RNPA hours new patients'))
transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv')$ transactions.head()
pd.Series(PDSQ.min(axis=1)<0).value_counts()  # no
happiness_df=pd.DataFrame(columns=['dates','happiness'])$ for j in range(0,len(happiness)):$             happiness_df.loc[j]=[str(time[j])+"-12-31"+"T00:00:00Z",happiness[j]*33.33]$
index = dframe_team['Draft_year']$ dframe_team.set_index(index, inplace = True) # sets the column 'Draft_year' as the index'$ dframe_team.head()
df2.columns[df2.columns.str.upper().str.contains('ORIGIN')]
pd.read_csv("../../data/msft.csv", skiprows=100, nrows=5, header=0,names=['open','high','low','close','vol','adjclose'])
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'$ r= requests.get(url)$
stock_weights = np.asarray([0.5,0.5,0.5,0.5])
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
Precipitation_DF.describe()
autos[autos.registration_year > 2016]
resDir  = "results/basic weekly move predict quaterly train results.csv"$
Bot_tweets.groupby('sentiment').count()
len(cats_in['Animal ID'].unique())
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
tickers = portfolio_df['Ticker'].unique()$ tickers
suburban_avg_fare = suburban_type_df.groupby(["city"]).mean()["fare"]$ suburban_avg_fare.head()$
df = df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]
df_mas['name'] = df_mas['name'].replace('None', np.NaN)
recipes.ingredients.str.contains('[Cc]inamon').sum()
len([earlyPr for earlyPr in BDAY_PAIR_df.pair_age if earlyPr < 3])/BDAY_PAIR_df.pair_age.count()
file_name = 'campaign_finance_clean_data.csv'$ dat.to_csv(path_or_buf=file_name,sep=',')
for item in rows:$     item.native_country='United-States'$ session.commit()
print(today.strftime('%Y-%m-%d'))
Geocoder.geocode("4207 N Washington Ave, Douglas, AZ 85607").valid_address
old_page_converted = np.random.binomial(1, p_old, n_old)$ old_page_converted
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL')$                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1))$                           &(extract_nondeduped_cmp.app_branch_state=='CA')].groupby('APP_PROMO_CD').size()
autos = autos.rename(columns = {'odometer':'odometer_km'})
StockData.count().sum()
np.isnan(StockData).sum().sum()
station_count = session.query(func.count(distinct(Measurement.station))).all()$ print ("There are" + " " + str(station_count[0][0]) + " " + "unique stations")
df = pd.concat(frames, axis=1)$
os.environ['PROJECT'] = PROJECT$ os.environ['BUCKET'] = BUCKET$ os.environ['REGION'] = REGION
df2['user_id'].nunique()
sentiments_pd = sentiments_pd[["Outlet", "Date", "Tweet", "Compound", "Positive", "Neutral", "Negative", "Tweets Ago"]]$ sentiments_pd.tail()
d.groupby(d['pasttweets'].str.len())['tweet_id'].count()
states.index
df = df.sort_values('label', ascending = True)$ df.head()
store_items.fillna(method='backfill', axis=0)
destination_dir = '../charts/follower_factory/'$ if not os.path.exists(destination_dir):$     os.makedirs(destination_dir)
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read(), end='')
twitter_archive_df_clean = twitter_archive_df.copy()$ tweets_df_clean = tweets_df.copy()
_factors = ['SMB', 'HML', 'RMW', 'CMA', 'MOM','Mkt-RF',]$ index_ts = pd.read_sql("select * from daily_price where ticker in %s and instrument_type='factor'" % str(tuple(_factors)), engine)$
recipes.description.str.contains('[Bb]reakfast').sum()
df_nullcount = df_closed.isnull().apply(np.count_nonzero)$ print df_nullcount
weather1=pd.read_csv('data/Crime/weather1.csv')$ weather2=pd.read_csv('data/Crime/weather2.csv')$ print(weather1.shape, weather2.shape)
df2 = df.query("group == 'treatment' and landing_page == 'new_page' or group == 'control' and landing_page == 'old_page'")$ df2.head()
active_users.shape # there are only 7479 active users out of 22884 signed users$
dicttagger_location = DictionaryTagger(['location.yml'])
Final_question_df.describe()
top_views = doctype_by_day.loc[:,doctype_by_day.min().sort_values(ascending=False)[:10]]$ ax = top_views.plot()$ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
twitter_archive_enhanced_clean['timestamp'] = pd.to_datetime(twitter_archive_enhanced_clean['timestamp'])
x_min_max = pd.DataFrame({'x': [df['x'].min(), df['x'].max()]})$ x_min_max
mars_html_table = mars_df_table.to_html(classes='marsdata')$ mars_table = mars_html_table.replace('\n', ' ')$ mars_table
tlen.plot(figsize=(16,4), color='r')
print(train.isnull().sum())$ train_att = train[train['is_attributed']==1]$ print(train_att.isnull().sum())
%%time$ data_demo["num_child"] = data_demo["comment_id"].apply(lambda x: data_demo[data_demo["parent_id"]==x].shape[0])
dfSummary = pd.concat([sumAll,sumPre,sumPost],axis=1)$ dfSummary.columns = ("all","before","after")
logging.info('Finishing dataset')$ df_final['Close: t'] = df_final['Close: t'].shift(1)$ df_final = df_final[2:-15]
cat_sz = [(c, len(full_data[c].unique())) for c in cats]$ emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]$ n_conts = len(full_data.columns) - len(cats)
pd.Series([2, 4, 6])
df_bill_data[df_bill_data['patient_id'] == sample_repeat]['amount'].sum()
print()$ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
all_tables_df.loc[[2, 3, 4, 10, 2000], ['OBJECT_NAME', 'OBJECT_TYPE']]
df_nona.groupby('district_id').district_size.mean().hist(bins=12)$ df_nona['segment'] = pd.cut(df_nona.district_size, bins=[0,1000,4000,120000])$ df_nona.groupby('segment').district_id.nunique()
precip_data = session.query(Measurements).first()$ precip_data.__dict__
print(df.shape)$ print("The number of rows in the dataset is: {}".format(df.shape[0]))
df1.tail(5)
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])
pmean = np.mean([p_new,p_old])$ round (pmean, 4)
print ("Propotion of users converted:",df.converted.mean())
shutil.rmtree(images_folder_name) #remove folder$ os.remove(markdown_name) #remove file
tripduration_minutes = pd.Series(minutes_list)
url_weather = "https://twitter.com/marswxreport?lang=en"$ browser.visit(url_weather)
df['status'].unique()
birth_dates.set_index("BirthDate_dt").loc["2014-01-01":,:]
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-07-13&end_date=2018-07-13&api_key=%s' % API_KEY)
df_new = df_new.join(pd.get_dummies(df_new['group']))
url = "https://soundcloud.com/edm/blackmail-by-dallask-henry"$ html = requests.get("http://localhost:8950/render.html?url=").text
dfCr = pd.read_csv('data/CC2017.csv')$ dfCr = dfCr[dfCr['Transaction']=='CREDIT'] $ dfCR= dfCr[dfCr['Memo'].notnull()] # remove null entries
regr = linear_model.LinearRegression()
from sklearn.metrics import mean_squared_error, mean_absolute_error$ mean_absolute_error(y_test, y_pred_dt)
merge_table_df = pd.merge(spotify_df, numberOneUnique_df, on=['URL','Region'], how='left')
S_1dRichards.decision_obj.hc_profile.options, S_1dRichards.decision_obj.hc_profile.value
volume.sort()$ volume[len(volume)/2]
print xmlData.dtypes$ print ''$ print csvData.dtypes
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
(~autos["price"].between(200,155000)).sum() / autos.shape[0] #In this code ~ invert operator selects $
loan = loan.loc[loan['Declaration_FY'] >= 2006]
xmlData['bedrooms'] = pd.to_numeric(xmlData['bedrooms'], errors = 'raise')$ xmlData['bathrooms'] = pd.to_numeric(xmlData['bathrooms'], errors = 'raise')$ xmlData['floors'] = pd.to_numeric(xmlData['floors'], errors = 'raise')
word_centroid_map = dict(zip( model.wv.index2word, idx ))
df_users = df_users.dropna(axis=0)$ print(df_users.shape)$ df_users.head(10)
tsla_neg = mapped.filter(lambda row: row[3] < 0)$
session.query(func.min(measurement.tobs),func.max(measurement.tobs),func.avg(measurement.tobs)).\$     filter(measurement.station == 'USC00519281').all()
epochs = 50$ batch_size = 128$ keep_probability = 0.5
from scipy.stats import norm$ print(norm.ppf(1-(0.05)))
affair_age = pd.crosstab(data.age, data.affair.astype(bool))$ affair_age
df_new['country'].value_counts()
dfM = df.copy()
df.converted.mean()
y = tweets['handle'].map(lambda x: 1 if x == 'Donald J. Trump' else 0).values$ print(np.mean(y))
terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=2)$ feature_columns = [ terms_embedding_column ]
sp = openmc.StatePoint('statepoint.082.h5')
model.save('model/my_model_current.h5')$ model.summary()
x = df_1.groupby('Lang').Review.count().sort_values(ascending = False)$
df2.groupby(df2['group']=='control')['converted'].mean()[1]
fin_coins_r.isnull().sum()
table_names = ['train']$ str('{PATH}{fname}.csv')
result.street_number
df_tweets_sort.to_csv('News_Tweets_Data.csv',index=False)
street_median_length =data.groupby("Street")["Street.Length"].median()$ data = data.join(street_median_length, on="Street", rsuffix='_correct')
df_tweet = pd.read_json('tweet_json.txt',orient='index')$
df = pd.read_csv('anova_one_factor.csv')
predictions_gbt.groupby('indexedLabel', 'prediction').count().show()$
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old])$ old_page_converted.mean()$ print(old_page_converted)
percipitation_2017_df.plot()$ plt.show()
sox.ix[(pd.to_datetime(sox.date) >= dt.datetime(2013,10,4)) & (pd.to_datetime(sox.date) < dt.datetime(2014,1,1)),'playoff'] = 1
damage = crimes[crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE']$ damage.head()
ax=fbdata[['Close']].plot()$ aapl[['Close']].plot(ax=ax)$
url = 'https://api.twitter.com/1.1/account/verify_credentials.json'
df2.user_id.nunique()
stations = session.query(func.count(Station.station))$ station_count = stations[0]$ station_count
pred = predict_class(np.array(theta), X_test_1)$ print ('Test Accuracy: %f' % ((y_test[(pred == y_test)].size / float(y_test.size)) * 100.0))
df_merged = df_merged.replace('nan', '')$
df.shape
churn_df.shape$
stero_grid_file = E.obs['NSIDC_0051']['grid']$ obs_grid = import_data.load_grid_info(stero_grid_file, model='NSIDC')$ obs_grid['lat_b'] = obs_grid.lat_b.where(obs_grid.lat_b < 90, other = 90)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt"$ mydata = pd.read_table(path, sep= '\t')$ mydata.head(5)
df.dropna(axis = 0, inplace = True)$ df.reset_index(inplace=True, drop=True)$ df.shape
pd.crosstab(df.group, df.landing_page, margins=True)
query ="SELECT * FROM tdata_db_2.Weather_Log ORDER BY Log_Id DESC"$ df = pd.read_sql(query,session)$ df.head(10)
plt.bar(indices, sorted(total_ridership_list))
x["sum"]=x[list(x.columns)].sum()$ x
def filter_func(x):$     return x['data2'].std() > 4$ display('df', "df.groupby('key').std()", "df.groupby('key').filter(filter_func)")
df_new[['CA', 'UK','US']] = pd.get_dummies(df_new['country'])$ df_new = df_new.drop('CA', axis=1)$ df_new.head()
df2.converted.mean()
df2.user_id.nunique()
df.shape[0]
class_merged_hol.to_csv('class_merged_hol.csv',sep=',')
c = R18df.rename({'Create_Date': 'Count-2018'}, axis = 'columns')
year_info_df.describe()
df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country'])$ df3.head()$
datetime.date(datetime(2014,12,14))
holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00102032') ]
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', na_values=['.'], names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'class'])$ mydata.head(5)
new_page_converted = np.random.binomial(new_n, new_p)$ new_page_converted
model = pd.get_dummies(auto_new.CarModel)$ model = model.ix[:, ["ToyotaCamry", "ToyotaCorolla","ToyotaRav4", "ToyotaLandCruiserPrado", "ToyotaIpsum", "ToyotaSienna", "Toyota4-Runner"]]$ model.head()
table = soup.table$ print (table)
jobs_data.drop_duplicates(subset='clean_description', keep='first', inplace=True)
df.dropna(axis='columns', how='all')
targettraffic = dfs_morning.merge(census_zip, left_on=['STATION'], right_on=['station'], how='left')$ targettraffic['targettraffic'] = targettraffic['ENTRIES_MORNING'] * targettraffic['betw150kand200k']/100
twitter_Archive.describe()
words_hash_scrape = [term for term in words_scrape if term.startswith('#')]$ corpus_tweets_scraped.append(('hashtags', len(words_hash_scrape))) # update corpus comparison$ print('Total number of hashtags: ', len(words_hash_scrape)) #, set(terms_hash_stream))
size_pred = rf.predict(climate_vars)
autos.head()
x_normalized = intersections[for_normalized_columns].values.astype(float)
period_df.iloc[0]['url']$ period_df.iloc[0]['time']$ test_image, test_image_url, test_datetime = download_single_snapshot(period_df.iloc[0]['url'], period_df.iloc[0]['time'], workspace_dir)
df.set_index(['Date', 'Store', 'Category', 'Subcategory', 'Description'], inplace=True)$ df.head(3)
new_df = df.drop(df.columns[[1,2,3,4,6,7,8,9]], axis=1)$ new_df.head()
from src.image_manager import ImageManager$
cycling_data = [10.7, 0, None, 2.4, 15.3, 10.9, 0, None]$ joined_data = list(zip(step_counts, cycling_data))$ print joined_data$
total_users = df.nunique()['user_id']$ print("Number of unique users are : {}".format(total_users))
tmp = tweets.groupby(['snsuserid','text']).size().reset_index()$ tmp.rename(columns={0:'counts'},inplace=True)$ tmp.sort_values(by=['counts'],ascending=False).query("counts > 2").counts.sum()
train_topics_df=train_topics_df.fillna(0)$ test_topics_df=test_topics_df.fillna(0)
lines=content_joined.split('@@@')
climate_vars.head()
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).head(10)
bymin.resample("S").bfill()
df_totalConvs_day = df_conv.groupby(['datestamp'], as_index=False).sum()$ print '\n DataFrame df_totalConvs_day', df_totalConvs_day.shape$ df_totalConvs_day.head()
print([x for x in df.columns])
data.fillna(0).describe()
for tweet in tw.Cursor(api.home_timeline).items(10):$     print(tweet.text) 
df.rename(columns={"Indicator":"Indicator_Id"},inplace=True)
df_combined['country'].unique()
rtc = pd.read_excel('input/data/ExogenousTransmissionCapacity.xlsx',$                     pars_cols='B:R',$                     header=3)
km = joblib.load('./data/KMeans.pkl')
pres_df = pres_df.rename(columns={'subject_count_tmp': 'subject_count'})$ pres_df.head(2)
df.isnull().values.any()
df2.info()
iso_json = json.loads(iso_response.text)$ iso_gdf = gpd.GeoDataFrame.from_features(iso_json['features'])$ iso_gdf[:]
Google_stock.tail()
print(df_subset.info())
df = pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv') $ df.head(2)
actual_diff = (df2[df2['group'] == "treatment"]['converted'].mean()) - (df2[df2['group'] == "control"]['converted'].mean())$ actual_diff
with open("mojo_df2.pkl", 'r') as picklefile: $     df = pickle.load(picklefile)$
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 0), 'type'] = 'inside_gamers'
joined['dcoilwtico']=joined['dcoilwtico'].astype(np.int8)
converted_users = sum(df.converted == 1)/len(unique_usrs)$ converted_users #proportion of users converted
n_new = df2[df2['group'] == 'treatment'].shape[0]$ n_new
d1.sum() # reduces over columns$
data = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True)$ data.head()
df2['quarter_opened'] = df2['account_created'].map(lambda x: int((x.month+2)//3))
df.user_id.nunique()
ffrM_resample = ffr.resample("MS")$ type(ffrM_resample)
data.TMED.head()
import pandas as pd$ tweets = pd.DataFrame(tweet_list)$ tweets.info()
df_clean['dog_nick'].value_counts()
layout_row = bokeh.layouts.row(fig_out)$ bokeh.plotting.show(layout_row)
actual_new = df2.query('converted == 1 and landing_page=="new_page"').count()[0]/n_new
columns = inspector.get_columns('measurement')$ for column in columns:$     print(column["name"], column["type"])
top10_topics_2 = top10_topics_1.sort_values(by='count_event_per_topic', ascending=False)$ top10_topics_2.head(10)
df2_treat = df2.query('group == "treatment"').converted.mean()$ df2_treat
df.user_id.nunique()
df_imputed_median_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.median())
from IPython.display import IFrame$ IFrame('data/lda.html', width=1220, height=860)
data=users.merge(transactions, how='left',on='UserID')$ data
header_names = {'1. symbol':'sym',$                 '2. price':'price_str', $                 '3. volume':'vol'}
cluster = model.transform(feature_sel).cache()$ cluster_pd = cluster.toPandas()
com_eng_df['issues_open'].plot()
if not os.path.isdir('output/electricity_demand'):$     os.makedirs('output/electricity_demand')
df_new['intercept']=1$ log_mod= sm.Logit(df_new['converted'], df_new[['intercept','CA', 'UK']])$ results= log_mod.fit()
old_page_converted.mean()
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
now = time.strftime("%c")$ todays_date = time.strftime("Current date & time" + time.strftime("%c"))$ print(now.split(':')[0].replace('  ',' ') + ':' + now.split(':')[1])$
temps_df.loc['2014-07-03']
client = MongoClient()$ db = client.kojak$ orgs_api = db.orgs_api
prob_group = df2.query("group == 'control'")["converted"].mean()$ print("In the 'control' group the probability they converted is {}.".format(prob_group))
for col in temp_columns:$     print(col)$     dat.loc[:,col]=dat[col].interpolate(method='linear', limit=3)
df_master.drop('Unnamed: 0',axis=1,inplace=True)
sorted_budget_biggest = df.sort_values(by=['budget_adj'], ascending = False).head(200)
temp=pd.read_json("./jsondata/condensed_2017.json")$ temp.head()
t1.tweet_id =t1.tweet_id.astype(str)$
twosample_sub = scipy.stats.ttest_ind(locationing.subjectivity, tweetering.subjectivity)$ twosample_sub
new_converted = np.random.choice([1, 0], size=nnew, p=[pmean, (1-pmean)])$ new_converted.mean()
sql = "SELECT * FROM main_stops;"$ stops = pd.read_sql(sql, engine)
columns = ['DepartmentId', "Date", "Volume"]$ df_ = pd.DataFrame(data = new_df, columns = columns)$ df_.to_csv("Task1.csv", sep='\t')
len(df2.user_id.unique())
list(df_users_first_transaction.dropna(thresh=int(df_users_first_transaction.shape[0] * .9), axis=1).columns)
reviews = np.array(tf.review)$ reviews_vector = vectorizer.transform(reviews)$ predictions = clf.predict(reviews_vector)$
train = pd.read_csv("../input/web-traffic-time-series-forecasting/train_1.csv")$ keys = pd.read_csv("../input/web-traffic-time-series-forecasting/key_1.csv")$ ss = pd.read_csv("../input/web-traffic-time-series-forecasting/sample_submission_1.csv")
a = np.arange(25).reshape(5,5) ; a$
styles = df.groupby('simple_style').size().sort_values(ascending=False)$ styles.head(5)
df_new.reset_index(inplace=True)$ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
Base.classes.stations
most_temp_info = session.query(Measurement.station, Measurement.tobs).filter(Measurement.station == most_busy).all()$ most_temp_info
autos.head()
schedId_in_proj = [x for x in df_sched.ProjectId if x in list(df_proj.ProjectId)]$ schedId_in_bud = [x for x in df_proj.FMSID if x in list(df_bud.project_id)]
hours = df4['Date'].dt.hour$ hours$
df.index = df['Date']
file_names = []$ file_names = glob.glob('*.csv')
Top_tweets = data.drop(['text','text_lower'],axis=1)$ Top_tweets.head()
print 'Total cycling distance of the whole trip: \t%.2f km \nTotal time cycled: \t\t\t\t%s h|m|s' % (sum($     cycling_df['ttl_cyc_km']),secToHours(sum(cycling_df['ttl_cyc_seconds'])))
sortedprecip_12mo_df=precip_12mo_df.sort_values('date',ascending=True)$ sortedprecip_12mo_df.head()
remove_cols = [ 'img_num','p1', 'p1_conf', 'p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog' ]$ images_copy.drop (remove_cols, axis =1 , inplace= True)$ images_copy.tail()
tlen.plot(figsize=(16,4), color='r');
data_json_str = "[" + ','.join(lines) + "]"$ data_df = pd.read_json(data_json_str)$ data_df
unique_users = df.user_id.nunique()$ unique_users
validation.analysis(observation_data, simple_resistance_simulation_0_5)
df.corr()$
df2 = pd.DataFrame({'attributes': cols, 'correlation': correlations})
bigquery_dataset = "{}:{}.hygiene".format(PROJECT,PROJECT)$ job = preprocess('Dataflow', BUCKET, bigquery_dataset)
countries_df = pd.read_csv('countries.csv')$ df_ab_cntry = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
combined_df4['split_llpg1']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(x.split(',')[1:2]))$ combined_df4['split_llpg2']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(x.split(',')[1:3]))$ combined_df4.head()
autos.columns = new_cols
df.get_dtype_counts()
np.array([1,2,3,4,5])$
df_2011['bank_name'] = df_2011.bank_name.str.split(",").str[0]$
index_strong_outliers = (strong_outliers_fare.is_outlier == 1)
np.array(p_diffs).mean()
from scipy.stats import norm$ norm.cdf(z_score)
avg_monthly_search_volumes = {"AU": 301000, "CA": 246000, "DE": 246000, "ES": 165000, "FR": 201000, "GB": 150000, "IT": 201000, "NL": 110000, "PT": 110000, "US": 500000}$ data_countries["avg_monthly_search_volume"] = data_countries.apply(lambda x: avg_monthly_search_volumes[x.country_destination], axis=1)$
o_page_converted = np.random.binomial(1, p_old, n_old)$ print('The old_page convert rate: {}.'.format(o_page_converted.mean()))$ print('The old_page convert rate: {}.'.format(round(o_page_converted.mean(), 4)))
s = '2016-01-01 00:00:00'$ s = '2018-07-01 00:00:00'$ int(time.mktime(datetime.datetime.strptime(s, "%Y-%m-%d %H:%M:%S").timetuple()))
ds_issm = xr.open_dataset(data_url1)$ ds_issm = ds_issm.swap_dims({'obs': 'time'})$ ds_issm
Base = automap_base()$ Base.prepare(engine, reflect=True)$
cursor.execute("SHOW TABLES")$ cursor.fetchall()
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path)$ df.head(5)
df_providers.head()$ idx = df_providers[ (df_providers['id_num']==50030)].index.tolist()$ df_providers.loc[idx[:3],:]$
cityID = '42e46bc3663a4b5f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Worth.append(tweet) 
words_only_sk_freq = FreqDist(words_only_sk)$ print('The 100 most frequent terms (terms only): ', words_only_sk_freq.most_common(30))
ADNI_merge = pd.read_csv('ADNIMERGE.csv')$ ADNI_merge.head()
output = pd.DataFrame(data={"id":test.id, "rating":predictions})$ output.to_csv( "new_naive.csv", index=False, quoting=3 )$
df_twitter_copy['timestamp'] = pd.to_datetime(df_twitter_copy['timestamp'])
df['date_of_birth'] = pd.to_datetime(df['date_of_birth'])$ df['age'] = df['date_of_admission'].apply(lambda x: x.year) - df['date_of_birth'].apply(lambda x: x.year)$ df['duration'] = (pd.to_datetime(df['date_of_discharge']) - pd.to_datetime(df['date_of_admission'])).dt.days
arr_size = reflClean.shape$ arr_size
tmp = tmp.sort_values(by = 'meantempm', axis = 0, ascending = False)$
df.to_csv('ab_edited.csv', index=False)
np.save('p_diffs', p_diffs)
active_stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()$ active_stations
if (kaggle|sim): test = test.reset_index(drop=True)
sub_mean = df.groupby('subreddit').agg({'num_comments': 'mean'})$ top_com = sub_mean.sort_values('num_comments', ascending = False).head()$ top_com
df.drop_duplicates(['title'],keep= 'first',inplace =True)
trip_data_sub["lpep_pickup_datetime"] = trip_data_sub.lpep_pickup_datetime.apply(lambda x: (pd.to_datetime(x) -  pd.datetime(2015, 9, 1))/ np.timedelta64(1, 's'))$ trip_data_sub["Lpep_dropoff_datetime"] = trip_data_sub.Lpep_dropoff_datetime.apply(lambda x: (pd.to_datetime(x) -  pd.datetime(2015, 9, 1))/ np.timedelta64(1, 's'))
B2.paths['directory_paths']['indicator_settings']$
grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)$ grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)$ grid.add_legend()
import os$ os.environ["THEANO_FLAGS"] = "mode=FAST_RUN,device=cuda0,floatX=float32"
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall()
plt.hist(p_diffs);$ plt.axvline(x = full_diff, color = 'red');
lm = sm.Logit(df_new['converted'],df_new[['intercept','treatment','UK','US']])$ results = lm.fit()$ results.summary()
adjClose_pyo = pd.DataFrame(data=df_jArr, columns=etfSymbols)$ corr = adjClose_pyo[:corr]()
print('\n{0} Movie Recommendations for User = {1}' \$       .format(mov_vec[mov_vec == 0].shape[0], $               tmp_df[tmp_df.tmp_idx == usr_idx].index[0]))
fromId = dfUsers['userFromId'].unique()$ dfChat = dfUsers[dfUsers['userToId'].apply(lambda x: x in fromId)]
df2.shape$ df2.query('landing_page == "new_page"')$
y_axis = np.arange(math.floor( rides_fare_average_min ) - 5, math.floor(rides_fare_average_max) + 6, 5)$ y_axis
df = pd.DataFrame(A, columns=list('QRST'))$ df - df.iloc[0]
old_page_converted = np.random.choice([1,0], size=OldPage, p=[0.12, 0.88])$ old_page_converted
logistic_mod_full = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','US', 'wk2', 'wk3']])$ results_full = logistic_mod_full.fit()$ results_full.summary()
print(new_df.isnull().sum())$
all_simband_data['group_type'] = all_simband_data['subject_id'].apply(lambda x: subject_id_to_group[x])
df.info()$
qs = qs.join(features, how="inner", rsuffix="_r")$ qs.head()
hurricane_percentage = disaster_type_by_value['Approval_Amt'][0]/disaster_type_by_value['Approval_Amt'].sum()$ hurricane_percentage$
(df2.landing_page == "new_page").mean()
df3 =pd.get_dummies(df3,prefix=['country'], columns=['country'])$ df3.drop('country_CA', inplace=True, axis=1)$ df3.head()
import json$ with open('data_comparision/heremap.json', 'w') as outfile:$     json.dump(j_data, outfile)
feeds = json.loads(response)$ print(type(feeds))$ print(feeds)
x_min=np.around(np.amin(windfield_matched_array),decimals=-1)-10$ x_max=np.around(np.amax(windfield_matched_array),decimals=-1)+10$ x_num= np.around(np.amax(windfield_matched_array)-np.amin(windfield_matched_array))
score.head()
spark_df.registerTempTable("ufo_sightings")
df_countries = pd.read_csv('countries.csv')$ df_countries.head()
df2.loc['2016-09-18', ['GrossIn', 'NetIn']]$
df.index
cassession.close()
data['Age'].max()
news_df = (news_df$            .loc[news_df['num_reactions'] - $                 news_df['num_likes'] >= 10,:])
np.any(x < 0)
df4[['ab_page', 'country_UK', 'country_US','intercept', 'converted']].astype(int)$ df4.head()
table_rows = driver.find_elements_by_tag_name("tbody")[22].find_elements_by_tag_name("tr")$
count_liberia = grouped_months_liberia.count()$ count_liberia=count_liberia.rename(columns = {'National':'count_v_T'})$
dist = np.sum(X, axis=0)$ for tag, count in zip(vocab, dist):$     print (count,tag)
X_model = df_model.drop('liked',axis=1)$ Y_model = df_model.liked
ab_dataframe.shape
trt_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][1]$ print("P(trt_con) = %.4f" %trt_con)
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin'$ S_distributedTopmodel.executable = excutable +'/summa.exe'
tips.index
df_eng.groupby(['user_id','time_stamp']).count()$
df_R['Month'] = df_R['Date'].apply(lambda s:s.split('-')[1])
len(df[df['text'].str.contains('appointment')])
BMonthEnd().rollforward(datetime(2014,9,15))
z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new,n_old],alternative='larger')$ print(z_score, p_value)$
max(close, key=close.get)
(df_final[df_final['Scorepoints'] == 200]).head()
randomdata2 = randomdata1[(randomdata1 >=3) | (randomdata1 <=-3)]$ randomdata2.describe()$
min(TestData.DOB_clean), max(TestData.DOB_clean)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json')
retweet_columns = ['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp']$ twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()]$ twitter_archive_clean = twitter_archive_clean.drop(columns=retweet_columns)
data['indrel_1mes'].replace(to_replace = [1.0, '1.0', '1', 3.0, '3.0', '3', 'P', '2', '2.0', 2.0, 4.0, '4.0', '4'], value = [1, 1, 1, 3, 3, 3, 'P', 2, 2, 2, 4, 4, 4], inplace = True)$ data['antiguedad'].replace(to_replace = -999999, value = data['antiguedad'].median(), inplace = True)$ test_data['antiguedad'].replace(to_replace = -999999, value = test_data['antiguedad'].median(), inplace = True)
grpConfidence = df.groupby(['Confidence'])
auth = tweepy.OAuthHandler(api_key, api_secret)$ auth.set_access_token(access_token, access_secret)$ api = tweepy.API(auth)
archive_clean = archive.copy()$ images_clean = images.copy()$ popularity_clean = popularity.copy()
data.head(10)
df = pd.read_csv('data_clean.csv', parse_dates=['First_Payment_Date', 'Maturity_Date'])$ df.head()
min_div_stock=df.iloc[df["Dividend Yield"].idxmin()]$ min_div_stock$ print("The stock with the minimum dividend yield is %s with yield %s" % (min_div_stock['Company Name'],min_div_stock['Dividend Yield']))
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
live_kd915_filt_df = pd.concat([live_df, kd915_filtered], axis = 0)
x = pd.Series(range(2), dtype=int)$ x
pres_df['time_from_creation_tmp'] = pres_df['start_time'] - pres_df['date_created']$ pres_df['time_from_creation_tmp'].head(10)
def check_http_status(url):$     response = s.head(url, allow_redirects=True)$     return response.status_code
brand_top_10 = autos['brand'].value_counts().sort_values(ascending=False)$ brand_top_10 = brand_top_10.index[0:10]$ brand_top_10
ScoreLabel = 'HDWPSRRating'$ A = ScoreToProbViaIntegral(Score, ScoreLabel)$ dfX_hist['prob_'+ScoreLabel] = dfX_hist.groupby('race_id')[ScoreLabel].transform(lambda x:A(x))
tweet_archive_clean.shape$
tweetsIn22Mar.head()$ tweetsIn1Apr.head()$ tweetsIn2Apr.head()
df_CLEAN1A.info()$
cv.get_feature_names()
df_SP.to_pickle('DDC_data_cleaned.pkl')
cityID = '52445186970bafb3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chandler.append(tweet) 
data.groupby(['Year'])['Salary'].mean()
Combineddata = merging_data(Cleaneddata, Adsdata)$ Combineddata.head()
df_cat = pd.get_dummies(data[catFeatures])
msftA = msft[['Adj Close']] $ closes = pd.concat([msftA, aaplA], axis=1)$ closes[:3]
feature_mapper.fit_transform(X_test[0:5])
universe = ["SAP.DE", "UN01.DE", "BAS.DE"]$ price_data = yahoo_finance.download_quotes(universe)
pprint.pprint(test_collection.find_one())
at_messages = non_na_df[non_na_df.message_text.str.startswith('@')]
lr.score(test_array, y_test)$
industry_group=df.groupby(by="GICS Sector")$ industry_group
test_data = load_df('data/test.csv')
df.landing_page.unique()
from scipy.stats import norm$ print('CDF of Z-Score: ', norm.cdf(z_score))$ print('Z-Score: ', z_score, ' < ',norm.ppf(1-0.05))
!head ml-100k/u.data
from scipy.stats import norm$ norm.ppf(1-(0.05/2))$
weight_is_relevant = 2*1/(np.sum(article_isRelevant)/ len(article_isRelevant))$ weight_is_not_relevant = 1$ weights = {0:weight_is_not_relevant, 1:weight_is_relevant}
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
r_test = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-04-30&end_date=2018-04-30&api_key=YOURAPIKEY')$
WorldBankLimited.toPandas()
p_treatment = df2.query("group == 'treatment'")['converted'].mean()$ p_treatment
price_data = heading.append(price_data)$ price_data.columns = price_data.iloc[0]
p_new = df2.converted.mean()$ p_new
results = pd.read_csv('datasets/results.csv')
donald_trump_tweets['screen_name'].value_counts()
p_diff = new_page_converted.mean() - old_page_converted.mean()$ print("{} is simulated value of pnew - pold".format(p_diff))
conn.get_tables()
validation.analysis(observation_data, Jarvis_resistance_simulation_0_5)
num_rows = df.shape[0]$ print('Number of rows in dataset: ',num_rows)
idx = pd.period_range('2011',periods=10,freq='Q')$ idx
p_diffs = np.array(p_diffs)
sample_record="0,5,1,4,0,0,0,0,0,1,0,0,0,0,0,6,1,0,0,0.9,1.8,2.332648709,10,0,-1,0,0,14,1,1,0,1,104,2,0.445982062,0.879049073,0.40620192,3,0.7,0.8,0.4,3,1,8,2,11,3,8,4,2,0,9,0,1,0,1,1,1"$ label,payload = sample_record.split(',',maxsplit=1)
X_15_16 = np.concatenate((training_X_scaled[-12:],holdout_X_scaled),axis=0)$ y_15_16 = np.concatenate((training_y[-12:],holdout_y),axis=0)
model = sm.OLS(df_new['converted'], df_new[['intercept','ab_page','CA','UK']])$ results = model.fit()$ results.summary()
df2.drop(1899, inplace=True)
c=table.find(text='Crew').find_next('td').text$ crews=re.search(r'\d+', c).group()$ crews
closePrice = aapl['Close']$ closePrice.head(5)
car18= car.groupby('dayofweek')['incidntnum'].agg({'numofinstance_car':'nunique'}).reset_index()$
penalties.head(2)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2.drop(['Neighbors_Obj'], axis=1)$
grads = K.gradients(loss, [w,b])$ updates = [(w, w-lr*grads[0]), (b, b-lr*grads[1])]
plt.show()
cars_scale = preprocessing.scale(cars)$ cars2_scale = preprocessing.scale(cars2)
documents = [x.split(' ') for x in documents]
c_date = scratch['created_at']$ c_date.shape
archive = pd.read_csv('twitter-archive-enhanced.csv')
df_predictions_clean['p1'] = df_predictions_clean['p1'].str.replace('_', ' ')$ df_predictions_clean['p2'] = df_predictions_clean['p2'].str.replace('_', ' ')$ df_predictions_clean['p3'] = df_predictions_clean['p3'].str.replace('_', ' ')$
information_ratio = pd.Series([40,50], ['Manager_A', 'Manager_B'])$ print(information_ratio)
link="https://s3.amazonaws.com/adsteam8finalairbnb/RawData/listings.csv"$ Airbnb_df =  pd.read_csv(link,low_memory=False)$ print(Airbnb_df.shape)$
with open('./data/train.json') as json_data:$     d = json.load(json_data)$
gdax_trans_btc = pd.merge(gdax_trans_btc, coinbase_btc_eur_min.iloc[:,:2], on="Timestamp", how="left")
tweet_full_df.to_csv('twitter_archive_master.csv')
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])
hdf['Age'].groupby(level=0).apply(lambda x: x.max() - x.min()).reset_index(name='diff')
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
df_inner_users_sess = pd.merge(users,sessions,how='left',on='UserID')$ df_inner_users_sess[df_inner_users_sess['Registered']== df_inner_users_sess['SessionDate'] ].reset_index().drop('index',axis=1)
suburban_driver_total = suburban_type_df.groupby(["city"]).mean()["driver_count"]$ suburban_driver_total.head()
cpi_sdmx.lookup_code('','Time',component='TimeDimension')$
months['date'] = pd.to_datetime(months['starttime'])$ months.head()
old_page_converted = np.random.choice([0, 1], size=n_old, p=[1-p_old, p_old])$ p_old_sim = old_page_converted.sum()/len(old_page_converted)$ p_old_sim
lok = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA','US']])$ result2 = lok.fit()$
news_organizations_df['tweets'] = news_organizations_df.handle.map(get_df_for_user_tweets)$
gs.score(X_test_all, y_test)
data_mean.mean(axis=1, level='type')
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours Autocorrelation')
precip_data_df.head(3)
meets_credit_policy = doesnt_meet_credit_policy.logical_negation()$ meets_credit_policy.head(rows=2)
combined_data = pd.concat([data, expanded_data], axis=1)$ combined_data = combined_data.drop('_source', axis=1)
unique_user_conversions = df.user_id[(df.converted == 1)].nunique()$ prop_conversions = unique_user_conversions / unique_users$ prop_conversions
Google_stock['Adj Close'].describe()
ADBC = AdaBoostClassifier(n_estimators=50)$ ADBC.fit(X_resampled, y_resampled) 
s.loc[s>4]
calls_nocontact.issue_type.value_counts()
adopted_cats = cats_merge.loc[cats_merge['Outcome Type']=='Adoption']
num_rows = df2.shape[0]$ new_rows = df2.query("landing_page == 'new_page'").shape[0]$ new_rows/num_rows
weather.columns = weather.columns.str.strip().str.upper().str.replace(' ','_')$ weather.dtypes
pook_url = "http://www.djbible.classicalgasemissions.com/book_of_pook.pdf"$ pook_dl = requests.get(pook_url, stream = True)
train[['ld','md','mc','jd','vd','sd','dc']] = pd.get_dummies(train['DayOfWeek'])$ test[['ld','md','mc','jd','vd','sd','dc']] = pd.get_dummies(test['DayOfWeek'])
dul_isbns = dul['ISBN RegEx']$ dul_isbns.size
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df['Category']=df['Memo'].apply(returnCategory)$ df['Single Name']=df['Name'].apply(returnName)$ df.head()
iris.drop(['Id'], inplace=True, axis=1)$ print(iris.shape)
News_title = Mars_soup.find('div',class_="content_title").a.text$  $ print(News_title)
melted_total.groupby(['Categories','Neighbourhood']).mean().unstack()['Review_count'].ix[top10_categories.index].plot.bar(legend=True,figsize=(10, 5))
shows3 = pd.read_csv('scraped_data5.csv')
date.day
result = api.search(q='%23H2P')
logit4 = sm.Logit(df_new['converted'], df_new[['intercept','new_page','CA_new_page','US_new_page','CA','US']])$ results4 = logit4.fit()$ results4.summary()$
y_pred = regr.predict(X_test)
local_tz = pytz.timezone('America/New_York')
df_new.head(5)
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
df_new = df_new.join(pd.get_dummies(df_new['country']))$ df_new.head()
ebola_melt['country'] = ebola_melt.str_split.str.get(1)$ ebola_melt.head()
df_new.country.unique()
vocab = list(model.wv.vocab.keys())$ vocab[:25]
data.to_csv('TwitterData.csv')
cityID = '0a0de7bd49ef942d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Scottsdale.append(tweet) 
plt.hist(p_diffs);$ plt.title("Histogram of P_diffs");
jobPostDFSample = jobPostDF.sample(100)
turnstiles_df['keys'] = turnstiles_df[['C/A', 'UNIT', 'SCP', 'STATION']].values.tolist()$ turnstiles_df['values'] = turnstiles_df[['LINENAME','DIVISION','DATE','TIME','DESC','ENTRIES','EXITS']].values.tolist()$ turnstiles_df['keys'] = [tuple(x) for x in turnstiles_df['keys'].values]
print(data.head())$ print(data.tail(3))$ print(data.shape)$
stop = stopwords.words('english')$ locationing['Text'] = locationing['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))$ pd.Series(' '.join(locationing['Text']).lower().split()).value_counts()[:50]
CM_s = pendulum.datetime(2017, 11, 27, 0, 0, 0, tzinfo='US/Eastern') $ CM_e = pendulum.datetime(2017, 11, 27, 23, 59, 59, tzinfo='US/Eastern') $ CM = tweets[(tweets['time_eastern'] >= CM_s) & (tweets['time_eastern'] <= CM_e)]
train_data.groupby(['device.browser']).agg({'totals.transactionRevenue': 'mean'}).reset_index().set_index("device.browser",drop=True).plot.bar()
df.sort_values( ['Year'], inplace=True )$ df.head()
with open('./data/Responses.json') as file_in:$     for line in file_in:$         is_json(line)
Val_eddyFlux = Plotting('/glade/u/home/ydchoi/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')$
cols_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'splitseed']$ X_train.drop(cols_to_drop, axis=1, inplace=True)$ X_age_notnull.drop(cols_to_drop, axis=1, inplace=True)
treat_prob = df2[df2['group'] == 'treatment']['converted'].mean()$ treat_prob
df[df.install_rate.isnull()]$
df.info()
print ("The number Unique userid in dataset is {}".format(df.user_id.drop_duplicates().count()))
contractor_merge.rename(index=str, columns={"state_abbrev" :"state_code"}, inplace =True)
pd.options.display.max_colwidth = 100$ it_df[it_df["price"] < 100]
findNum = re.compile(r'\d+')$ for i in range(0, len(postsDF)):$ 	print(findNum.findall(postsDF.iloc[i,0]))
from sklearn.neighbors import KNeighborsClassifier
pickle.dump(final_xgb, open(base_filename + '_model.sav', 'wb'))
final=final[(final.redshift<0.3) & (final.redshift>0.)]$ print final.shape[0]
coarse_fuel_mgxs = coarse_mgxs_lib.get_mgxs(fuel_cell, 'nu-fission')$ coarse_fuel_mgxs.get_pandas_dataframe()
data = pd.read_json('train.json')
old_page_converted =  np.random.binomial(1, p = p_old,size = n_old)$ old_page_converted
prob_converted = df2.converted.mean()$ prob_converted
fpr_a = (grid_pr_fires.sort_values(['glat', 'glon'], ascending=[False,True])['pr_fire']$          .values.reshape(26,59))
vals1.sum()
df2['user_id'].nunique()
interaction = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','US', 'UK']])$ result_interact = interaction.fit()$ result_interact.summary()
test_data.text
session.query(Measurements.station,func.count(Measurements.date)) \$              .group_by(Measurements.station).order_by(func.count(Measurements.date).desc()).all()$ 
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])$
from sklearn.feature_extraction.text import TfidfVectorizer # Import the library to vectorize the text$ tfidf_vect = TfidfVectorizer(ngram_range=(1,3), stop_words='english')$ tfidf_vectorized = tfidf_vect.fit_transform(df_train.text)
data.plot(figsize=(16,6))$ plt.show()
station_df = pd.read_sql("SELECT * FROM station", conn)$ station_df.head(10)
news_period_df.loc[3, 'news_title']
n_new = df2[df2['group']=='treatment'].shape[0]$ n_new
all_cards = all_cards[~all_cards.index.duplicated(keep = "first")]
topics_data = pd.DataFrame(topics_dict)
df=pd.read_csv('2017-2018_NBA_Player_stats.csv')$ df.info()
df_test = pd.read_hdf('bittrex.h5', 'bittrex_tb', where = ["mn_ix= 'BTC-VTC'"])$ df_test$
brewery_bw.head(5)
subtract = y-x$ plt.hist(subtract,np.arange(-28,36,2))
trace.analysis.cpus.plotCPU()
flu_incid= "Data/ILINet_2015_2017.csv"$ fluout = pd.read_csv(flu_incid)$ fluout.head()
df.rename(columns={'Indicator':'Indicator_id'}, inplace= True )$ df.head()
data.printSchema()
x = np.linspace(0, 10, 100)$ df = pd.DataFrame({"y":np.sin(x), "z":np.cos(x)}, index=x)$ df.head()
celtics.rename(columns={'Playoff':'playoff','Opponent':'opponent'}, inplace=True)
airbnb_df = airbnb_df[airbnb_df['city']=='Seattle']$ airbnb_df['city'].value_counts(dropna=False)
X2.shape
with open('united_list_fresh_lower.pickle', 'rb') as ff:$     united_list_fresh_lower = pickle.load(ff)
w = np.zeros(shape=(2, 1))$ w[0] = m$ w[1] = c
s519397_df = pd.DataFrame(s519397) $ print(len(s519397_df.index))$ s519397_df.info()
df1=pd.read_csv("approval data clean values only.csv")$ df1.head()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK','ab_page']])$ results=logit_mod.fit()$ results.summary()
measurements_df = pd.read_csv("clean_measurements.csv")$ measurements_df
clients['join_month'] = clients['joined'].dt.month$ clients['log_income'] = np.log(clients['income'])$ clients.head()
df_m['in_port'] = df_m.in_port.map({True: 1, False:0})   ## dont run twice
df['Days to Resolution'] = df['Date Closed'] - df['Date Created']$ df['Days to Resolution'] = df['Days to Resolution'].dt.days$ df['Days to Resolution'].value_counts()
df.mean(1)
df2 = df2.join(df_country.set_index('user_id'), on = 'user_id')$ df2.head()
df_new = df.query('landing_page == "new_page"')$ p_new = df_new[df_new['converted'] == 1]['converted'].count() / df_new[['converted']].count()$ print (p_new)
df_mes['travel_time'] = df_mes['travel_time'].astype('timedelta64[s]').dt.total_seconds()
graf=df.copy()
df2[df2.duplicated(['user_id'], keep=False)]
ArepaZone_merged_arrays_df=ArepaZone_merged_arrays_df.merge(ArepaZone_tweet_array_df,left_index=True, right_index=True)$ ArepaZone_merged_arrays_df=ArepaZone_merged_arrays_df.merge(ArepaZone_date_array_df,left_index=True, right_index=True)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]$ df2.shape
v = tmdb_movies.genres.apply(json.loads)
from sklearn.naive_bayes import MultinomialNB$ nb = MultinomialNB()
tokens = reddit.title.apply(process)
df_members = pd.read_csv('members.csv', encoding = 'latin-1')$ df_members['joined'] = pd.to_datetime(df_members['joined'], yearfirst = True)$ df_members.head()
r.groupby('cat').count()
tweets_df_lang = tweets_df.groupby('language')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ tweets_df_lang$
df['age'].fillna(df.groupby(['gender'])['age'].transform(mean))
year15 = driver.find_elements_by_class_name('yr-button')[14]$ year15.click()
dfnew1=df.loc[(df['landing_page']=='new_page') & (df['group']=="treatment"),] $ dfnew2=df.loc[(df['landing_page']=='old_page') & (df['group']=="control"),]$ df2=pd.concat([dfnew1,dfnew2],axis=0)   #Concanating the Rows
df1.info()
flight_cancels = flight_cancels.reindex(taxi_hourly_df.index)$ flight_cancels.fillna(0, inplace=True)
log_mod4 = sm.Logit(df3['converted'], df3[['intercept','ab_page','evening','eve_new']])$ results4 = log_mod4.fit()$ results4.summary()
%timeit pd.eval('df1 + df2 + df3 + df4')
example1_df = sqlContext.read.json("./world_bank.json.gz")
todaydate = datetime.now()$ oneyearback = todaydate.replace(year=todaydate.year-1).strftime("%Y-%m-%d")$ oneyearback
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1?api_key="+API_KEY
search.timestamp = pd.to_datetime(search.timestamp)$ search.trip_start_date = pd.to_datetime(search.trip_start_date)$ search.trip_end_date = pd.to_datetime(search.trip_end_date)
mydata.tail()
df_tsv.drop_duplicates()$ df_archive_csv.drop_duplicates()$ df_json_tweets.drop_duplicates()
vacancies['weekday'] = vacancies['created'].apply(lambda x: x.weekday() + 1)$ vacancies['hour'] = vacancies['created'].apply(lambda x: x.hour)
with open('100UsersResults.data', 'rb') as filehandle:  $     result = pickle.load(filehandle)
hawaii_measurement_df = hawaii_measurement_df.replace(np.nan, 0)
movies['scoreRank'] = movies.score.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)$ movies['grossRank'] = movies.gross.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)
for numbData in collData.distinct().collect():$     print(numbData)
if not os.path.isdir('output'):$     os.makedirs('output')
scipy.stats.pearsonr(df_day_pear['tripduration'],df_night_pear['tripduration'])$
df = pd.read_csv('ab_data.csv')$ df.head()
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])$ ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])$ pd.concat([ser1, ser2])
rm_indices = events['type'][events['type']== 'type'].index$ events = events.drop(events.index[[rm_indices]])
tdf = pd.read_csv('ksdata.csv', index_col=0)
df['Shipping Method name'] = df['Shipping Method name'].fillna(df['Shipping Method Id'])
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:10]
rnd_reg_2.oob_score_
taxiData.columns
logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']])$ results = logit_mod.fit()
import nltk.data$ tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))#considering 95 percent confidance interval
predictions = lrmodel.transform(testData)
df_customers[['number of customers','MA_3','MA_6']].plot()
OldPage = np.random.choice([1, 0], size=old, p=[p, (1-p)])$ old_avg = OldPage.mean()$ print(old_avg)
df2['converted'].mean()
dataset.drop_duplicates(['id'], keep='first', inplace=True)
dev4['avg_rank'] = dev4[[c for c in dev4.columns if c.startswith('rank_')]].apply(np.mean, axis=1)$ dev4['rank'] = dev4.avg_rank.rank()
iowa_2015 = iowa.loc[iowa['Year'] == 2015,:]$ total_sales_2015_by_month = iowa_2015.groupby('Month')['Sale (Dollars)'].sum()$ total_sales_2015_by_month.plot(kind = 'bar')
df_wm = pd.read_csv("walmart_all.csv", encoding="latin-1")
tweet_df["tweet_source"].unique()
df['converted'].mean()$
Lowest_opening_price = mydata['Open'].min()$ Lowest_opening_price
for node in nodes:$     if node is not None:$         print node.text_content()
store_items = store_items.drop(['watches', 'shoes'], axis=1)$ store_items
merged_data = youTubeTitles.append(pornTitles, ignore_index=True)$ title_matrix= merged_data.loc[:,["title"]]$ target_array=merged_data.loc[:, ["isPorn"]]
cig_data['tar'].value_counts()
for column in ideas:$     ideas[column] = ideas[column].dt.week
daily_df.reset_index(inplace=True)
y_pred = rnd_reg.predict(X_test)
new_df = df.replace(-99999, np.NAN)$ new_df
cur.execute("SELECT * FROM pgsdwh.pgsparts.geps_distinct_shipping_lines_dw_t")
df = pd.read_csv('ab_data.csv')$ df.head(5)
red_4 = red[['title', 'subreddit', 'num_comments', 'created_utc', 'id', 'time fetched']].copy(deep = True)$ red_4.head()
mapInfo_split = mapInfo_string.split(",") $ print(mapInfo_split)
from sqlalchemy import func$ num_stations = session.query(Stations.station).group_by(Stations.station).count()
bm2 = list(map(int, benchmark21))$ print('benchmark2 score is: {}'.format(np.around(f1_score(actual1, bm2, average='weighted'), decimals=5)))$
import numpy as np$ data['float_time'] =data['processing_time'].apply(lambda x:x/np.timedelta64(1, 'D'))
station_stat = session.query(Measurement.station, Station.name, func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$ filter(Measurement.station == 'USC00519281').all()$ station_stat
df_unique = df.user_id.nunique()$ print("There are {} unique users in the dataset.".format(df_unique))
fuel_therm_abs_rate = sp.get_tally(name='fuel therm. abs. rate')$ therm_util = fuel_therm_abs_rate / therm_abs_rate$ therm_util.get_pandas_dataframe()
sdf = sdf.assign(Norm_summary = normalize_corpus(sdf.summary))
maxprcp = max(prcpDateDF.prcp)$ prcpDateDF.plot(kind="bar", grid=True, ylim=(0, maxprcp))
tweets = tweets[tweets['in_reply_to_status_id'].isnull()]$ tweets.info()
df.groupby('user_id').nunique().shape[0] == df.nunique()['user_id']
total_trt = df2.query('group == "treatment"').user_id.nunique()$ prob_trt = total_trt/unique_users$ conv_and_trt = df2[(df2['group']=='treatment') & (df2['converted']== 1)].user_id.nunique() 
dfDir = '/home/parkkeo1/reddit_unlocked/Isaac/DataFrames/RedditData_Nov-06-2017'$ print_df = pd.read_pickle(dfDir)$ print_df$
df_grp = df.groupby('group')$ df_grp.describe()
df.describe()
all_turnstiles.head()
from pyspark.sql.functions import log$ df_input_clean = df_input_clean.withColumn("log_Resp_time", log(df_input_clean.Resp_time))
p_old = len(df2.query("converted=='1'"))/ len(df2)$ p_old
y.mean()
results.summary()
tobs_date_df = tobs_date_df.rename(columns={0: "date", 1: "tobs" })$ tobs_date_df.head()
IBMspark_df = sqlContext.createDataFrame(IBMpandas_df)$ for row in IBMspark_df.take(2):$     print row
lq2015_q1_features = lq2015_q1_drop.groupby(lq2015_date.StoreNumber).agg({'SaleDollars':'mean', 'BottlesSold':'mean', 'VolumeSoldLiters':'mean', 'VolumeSoldGallons':'mean', 'Sold_div_Sales':'mean'}) [['BottlesSold','VolumeSoldGallons','VolumeSoldLiters','SaleDollars','Sold_div_Sales']]
bg_df2 = pd.DataFrame(bg3) # create new variable explicitly for it being a DataFrame in pandas$ bg_df2$
prob_new_page = df2['landing_page'].value_counts()[0]/len(df2)$ print (prob_new_page)
plt.semilogy([k for k,v in sorted(sentimentCounter.iteritems(),key=lambda x:x[0])],[v for k,v in sorted(sentimentCounter.iteritems(),key=lambda x:x[0])])$ plt.savefig('dist.png',dpi=400)
users = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv')$ users.head()
df_A_not_B = df.query('group == "treatment" & landing_page != "new_page"')$ df_B_not_A = df.query('group != "treatment" & landing_page == "new_page"')$ len(df_A_not_B) + len(df_B_not_A)
df.info()
df_ad_state_metro_1 = pd.read_pickle('./data/TV_AD_AIRINGS_STATE_METRO_AREA_5.pkl')$ df_ad_state_metro_1.head(5)
import pandas as pd$ cs = pd.Series(cleaned)$ by_tweeter['cleaned'] = cs
print("The probability of individual in the treatment group converting is: {}".format(df2[df2['group'] == 'treatment']['converted'].mean()))$
def closeConn():$     cur.close()$     conn.close()$
db.query("select min(created) from bike_locations where provider='limebike' and raw->'attributes'->>'vehicle_type'='scooter'").export('df')
print(today.strftime("???"))
siteMask = nitrodata['MonitoringLocationIdentifier'].isin(siteInfo.index)$ dfSubSites = nitrodata[siteMask]$ dfSubSites.shape
qs.columns
hashtags = df.text.str.extractall(r"#(\w+)")$ mask = hashtags.loc[:, 0].value_counts() > 4$ hashtags.loc[:,0].value_counts()[mask]
r = dict(r.json())
session.query(stations).count()
precipitation_df['date'] = pd.to_datetime(precipitation_df["date"]).dt.date$ precipitation_df.set_index(["date"], inplace = True)
df.to_csv('new_edited.csv', index=False)
a = tips.loc[:,"tip"]$ a.head()
test = pd.read_csv('properties/properati_dataset_testing_noprice.csv')$ test.shape
ibm_hr_target_small.stat.corr("Age", "DailyRate")
p_new = df2.converted.mean()$ print("Convert rate for p_new:", p_new)
temp_df.plot(kind="hist", bins=12)
b_launch_df = launch_df[launch_df['source']=='product-b']$ b_first_launch_count = len(launch_df['device_id'].unique())$ print("There are %d first-time launches of product-b in this period." % b_first_launch_count)
df_ratings.shape
greater_first = git_log[git_log['timestamp'] >= str(first_commit_timestamp.iloc[0]['timestamp'])]$ corrected_log = greater_first[greater_first['timestamp'] <= str(last_commit_timestamp.iloc[0]['timestamp'])]$ corrected_log.sort_values('timestamp')                 
integratedData.to_csv('merged.csv', index = False)$ integratedData.drop('True', axis = 1, inplace = True)$ integratedData.head()
train['first_booking_made'] = pd.notnull(train.loc[:, 'date_first_booking'])
(act_diff < p_diffs).mean()
average_range = df['range'].mean()
Tweets = AllTweets.loc[:,AllTweets.loc['username'] == "b'SFBARTalert '"]
train = hn[hn.created_at < july_1].copy()$ new = hn[hn.created_at >= july_1].copy()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$ print('Size of new_page_converted: ', len(new_page_converted))
lengths = sent.Post.apply(len)$ print('Average character length of the posts are:')$ print (np.mean(lengths))
who_purchased.columns = ['BOUGHT_'+str(col) for col in who_purchased.columns]
ttarc.info() #The others are fine in terms of completeness, so I must address the missing data first, which occur$
if not database_exists(engine.url):$     create_database(engine.url)$ print(database_exists(engine.url))
twitter_archive_clean = twitter_archive.copy()
temp = open('datasets/git_log_excerpt.csv')$ print(temp)
rows = df.shape[0]$ print("Number of rows - {}".format(rows))
unordered_df = USER_PLANS_df.iloc[unordered_timelines]
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
file_name = "./data/train_preprocessed2.csv"$ train_df2 = pd.read_csv(file_name, low_memory = False)$ train_df2.head()
rng2 = pd.date_range(start='4/7/2018', end='4/8/2018', freq='3H')$ rng2$
from bs4 import BeautifulSoup$ import pandas as pd$ from urllib.request import urlopen
temps_mosact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == most_activity[0], Measurements.date > year_ago).all()
buckets_to_df(contributors.fetch_aggregation_results()['aggregations']['0']['buckets'])
test.nrows
pres_df['state'] = pres_df['state'].map(lambda x: x.strip())
result = dta.groupby((dta.results, dta.dba_name)).size()
results.summary()
plt.title(f"Overall Media Sentiment Based on Twitter as of {curDate}")$ plt.xlabel("Outlets")$ plt.ylabel("Tweet Polarity")
thisDir = os.getcwd()$ csvDir = thisDir + '/../dbases/'
df2['user_id'].nunique()$
traffic_df_rsmpld = traffic_df_rsmpld.sort_index().loc[idx['2016-01-01':'2017-12-31',:],:].sort_index()$ traffic_df_rsmpld.info()$ traffic_df_rsmpld.head()
print('Shape : ', pd_train.shape, '\n')$ print('Type : ', '\n', pd_train.dtypes)$ pd_train.describe()
post_gen_paired_cameras_missing_from_shopify_orders = np.setdiff1d(BPAIRED_GEN['shopify_order_id'],ORDERS_GEN['order_number'].astype(str))$ np.array_equal(post_gen_paired_cameras_missing_from_shopify_orders,post_gen_paired_cameras_missing_from_join)
df2[df2.duplicated('user_id')]['user_id'].iloc[0]
df3.fillna(0, inplace=True)$ df3.head()$ df3.isnull().sum()
emojis_db=pd.read_csv('emojis_db_csv.csv')$ emojis_db.head()
import exampleModule # This has one function called ex$ exampleModule.ex([4,5,6])
df_result['diff_abs_logprob_final_tote_morning_line'] = abs(df_result['diff_logprob_final_tote_morning_line']/df_result['num_starters'])$ df_result['diff_sum_logprob_final_tote_morning_line'] = df_result.groupby('race_id')['diff_abs_logprob_final_tote_morning_line'].transform(lambda x:sum(x))$ df_result.head()
grouped_authors_by_publication.rename(columns = {'authorName':'authorNames_in_given_publication'}, inplace = True)$ grouped_authors_by_publication.rename(columns = {'authorId':'authorIds_in_given_publication'}, inplace = True)
df_clean.name.value_counts()
sample = rng.choice(sizes, sample_size, replace=True)$ print(f'Mean (one sample) = {np.mean(sample):5.3f}')$ print(f'Standard deviation (one sample) = {np.std(sample):5.3f}')
paired_cameras_missing_from_shopify_orders = np.setdiff1d(BPAIRED_SHOPIFY['channel_order_id'],ORDER_TO_PAIR_SHOPIFY['channel_order_id'])
sales_df[sales_df['Product_Id'] == 5].groupby(['Country']).sum()['Quantity'].sort_values(ascending=False)$
staff_df = staff_df.reset_index()$ student_df = student_df.reset_index()$ pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name')
new_ticket = 'It seems like I have followed the set up directions but I see the set up WEPAY \$                 and am not sure if I have completed everything..thank you'$
sentiments_pd.count()
charge = reader.select_column('charge')$ charge = charge.values # Convert from Pandas Series to numpy array$ charge
df_CLEAN1A['AGE'].min()
rf_average = rf.rainfall.mean(dim =('longitude','latitude'))$ rf_mnthy_mean = rf_average.resample('1M', dim='time',how='mean')$ rf_mnthy_tot = rf_average.resample('1M', dim='time',how='sum')$
twitter_archive.loc[531,'text']
df.sort_values(['operator', 'part'], inplace=True)
rnd_reg.fit(X_train, y_train)
df_twitter_copy[['jpg_url', 'p1']] = df_twitter_copy[['jpg_url', 'p1']].fillna('None')$ df_twitter_copy['p1_conf'] = df_twitter_copy['p1_conf'].fillna(0)
measurement = Base.classes.measurements
df = pd.read_csv('ab_data.csv')$ df.head()
predictions = forest.predict(test_data_features)$ output = pd.DataFrame( data={"id":test["id"], "rating":predictions} )
nb.class_count_
df_variables.loc[df_variables["CustID"].isin([customer])]
df_ll = pd.read_csv("loblaws_all.csv", encoding="latin-1")
questions = pd.concat([questions.drop(['month_bought'], axis=1), month], axis=1)
df_new['ab*CA'], df_new['ab*UK'] = df_new['ab_page']*df_new['country_CA'], df_new['ab_page']*df_new['country_UK']$ df_new.head()
twitter_archive_clean.head(1)
df[['Indicator_id','Country','Year','Who Region','Publication Status']].head(3)
rf = RandomForestClassifier()$ rf.fit(X_train, y_train)$ rf.score(X_test, y_test)
from sklearn.cluster import AgglomerativeClustering$ agg = AgglomerativeClustering(n_clusters=3, affinity='precomputed',linkage='average')$
Base.classes.keys()
row_num = df.shape[0]$ row_num
print("Min " + str(dc['created_at'].min()) + " Max " + str(dc['created_at'].max()))$ print("Min " + str(tm['created_at'].min()) + " Max " + str(tm['created_at'].max()))
financial_crisis.loc['Tulip Mania']
ts.get_k_data(code="sh",start="2016-08-02",end="2018-08-21")
rf = pickle.load(open('../data/model_data/size_mod.sav', 'rb'))
today = datetime(2014,11,30)$ tomorrow = today + pd.Timedelta(days=1)$ tomorrow
tweets = pd.read_csv('twitter-archive-enhanced.csv')
def getResults(JsonReponse):$     return JsonReponse.get('ResultSet').get('Result')
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df = pd.read_csv(reviews_file_name, encoding = "ISO-8859-1")$ df.head(n = 10)
Output_two = New_query.ss_get_results(sport='football',league='nfl', ep='team_game_logs', season_id='nfl-2017-2018',team_id='nyg')$ Output_two
avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_reviewText, model=model, num_features=5000)$ print (np.round(avg_word_vec_features, 3))
precip_data_df1=precip_data_df.copy()$ precip_data_df1.reset_index(inplace=True,drop=False)$ precip_data_df1.head(5)
y = list(train_10m_ag.is_attributed)$ X = train_10m_ag.drop(['is_attributed'],axis=1)$ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)
df_clean.loc[mask, 'name'] = 'None'
len(mod.coef_)
df[~df.amount_initial.str.startswith('-$') & ~df.amount_initial.str.startswith('$')]
sentiments_pd= sentiments_pd.sort_values("Date", ascending=False)
S_lumpedTopmodel.decision_obj.simulStart.value, S_lumpedTopmodel.decision_obj.simulFinsh.value
df6 = df4.where( (hours > 10) & (hours < 13)) # show lunch data rows only$ df6 = df6[df6['BG'].notnull()]$ df6 # got same data as previous technique
df.info()
p_new = df2['converted'].mean()$ print("The convert rate for p_new under the null: ",p_new)
print(type(data.values))$ data.values
merged_311_data[merged_311_data['NEIGHBORHOOD'] == 'Greenfield'].groupby('REQUEST_ORIGIN').size()
users = users.join(df_active,on = 'user_id')$ users['active'] = users['max_login_per_7'] >=3
with open(url.split('/')[-1], mode='wb') as file:$     file.write(response.content)
s_filled.plot()$ plt.show()
Osha_AccidentCases['Title_Summary_Case'] = Osha_AccidentCases['Title_Summary_Case']$ Osha_AccidentCases.head()
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
Base.classes.keys()
data.fillna(method='bfill')
urbanData_df = data_df[data_df.type == "Urban"]$ suburbanData_df = data_df[data_df.type == "Suburban"]$ ruralData_df = data_df[data_df.type == "Rural"]
from scipy.stats import norm$ print("Singificance of z-score: ",norm.cdf(z_score))$ print("Critical Value: ",norm.ppf(1-(0.05/2)))$
sel=[Measurement.date,$      func.sum(Measurement.prcp)]$ day_prcp=session.query(*sel).filter((Measurement.date>=year_ago)).filter((Measurement.date<'2017-08-04')).group_by(Measurement.date).all()$
import os$ files = os.listdir("/data/measurements")$ print('\n'.join(files))
display('df5', 'df6',$        "pd.concat([df5, df6], join='inner')")
from collections import Counter$ freq = Counter([g[1] for g in genres]).most_common()$
data.drop(catFeatures, axis=1,inplace=True)
interests_groupby_user = df_interests['interest_tag'].groupby(df_interests['user_handle'])$ lst_user_interests = [[name, group.tolist()] for name, group in interests_groupby_user]$ lst_user_interests[1]
abc = df_providers.groupby(['year','drg3']) orange$ abc.head()
df = pd.read_csv(data_location)$ df.head(5)
print("Unique users:", len(df2.user_id.unique()))$ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
from subprocess import call$ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df2.iloc[2862]
festivals['Index'] = range(1, len(festivals) + 1)$ list(festivals.columns.values)$ festivals.head(3)$
pd.groupby(vlc, by=vlc.visited.dt.year).size()
df_tweet_api = pd.DataFrame(new_list,columns=['ranking','tweet_id','retweet_count','favourite_count','timestamp'])$ df_tweet_api = df_tweet_api.sort_values('ranking').reset_index(drop=True)$ df_tweet_api.tail()
precip_data_df2=precip_data_df1[["date","Precipitation"]]$ precip_data_df2.describe()
comments = df.loc[:,'comment_body']$ print(comments[0])$ type(comments)
df_new1['review_scores_rating']=df_new1['review_scores_rating'].astype(int)$ print(df_new1.groupby(['review_scores_rating']).id.count())$
p_old = df2[df2['converted']==1]['user_id'].count() / df2.shape[0]$ p_old
set(list(building_pa_specs['Column name'].values))==set(list(building_pa.columns))
stock_subset = stock_data[ [ 'open', 'close' ] ] $ print(display(stock_subset.head(5)))
knn = KNeighborsClassifier()$ knn.fit(X_train_pca, y_train)$ knn.score(X_test_pca, y_test)
all_noms[all_noms["agency"] == "Foreign Service"]["nom_count"].sum()
df['log_price']=np.log(df['price_doc'].values)
session.query(Measurements.date).order_by(Measurements.date).first()
txt_tweets = txt_tweets[~txt_tweets.str.startswith('RT')]$ txt_tweets_position = tweets[~tweets['text'].str.startswith('RT')]['position']$ txt_tweets_position.value_counts()
!apt install libnvrtc8.0$ !pip install mxnet-cu80$ import mxnet as mx
autos['odometer'] = autos['odometer'].str.replace("km","").str.replace(",","").astype(int)$ autos.rename({"odometer": "odometer_km"}, axis=1, inplace=True)$ autos["odometer_km"].head()
engine.execute('SELECT * FROM measurement LIMIT 5').fetchall()
cityID = 'c3f37afa9efcf94b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Austin.append(tweet) 
pd.set_option('display.expand_frame_repr', False)$ df = pd.read_csv("twitter-archive-enhanced.csv")$ df
merged2.dropna(subset=['Specialty'], how='all', inplace=True)
best_model = h2o.get_model(gbm_grid_cart.model_ids[0])$ best_model
ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')$ ebola_melt.head()
words_only_scrape = [term for term in words_scrape if not term.startswith('#') and not term.startswith('@')]$ print('The number of words only (no hashtags, no mentions): ', len(words_only_scrape))
pixiedust.enableJobMonitor()$ sqlContext = pixiedust.SQLContext(sc)
df_prec = df_prec.set_index('date',drop=True)$ display(df_prec.head())$ display(df_prec.shape)
authors['mean'].head()
np.log(points)
today = pd.to_datetime("today")$ today
rural_summary_table = pd.DataFrame({"Average Fare": rural_avg_fare, $                                    "Total Rides": rural_ride_total})$ rural_summary_table.head()
import pymysql$ pymysql.install_as_MySQLdb()$ import MySQLdb
print(df.Col_1)
df_users_6.shape
df_daily5=df_daily.groupby(["STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index()$ df_daily5.head(5)$
_index = 'SPX'$ index_ts = pd.read_sql("select * from daily_price where ticker='SPX' and instrument_type='index'", engine)$
news_dict_df.to_json("Newschannel_tweets_df.json")
type(df_vow['Date'].loc[0])
fl_100 = fl.loc[fl['appv_dcl'] >= 100]$ fl_100.shape[0]*100/fl.shape[0]$
unigram_chunker = UnigramChunker(train_trees)$ print(unigram_chunker.evaluate(valid_trees))
ab_df2.converted.mean()
import quandl$ quandl.ApiConfig.api_key = 'RGYoyz3FAs5xbhtGVAcc'
!wget ftp://ftp.solgenomics.net/tomato_genome/annotation/ITAG3.2_release/ITAG3.2_RepeatModeler_repeats_light.gff -P $DATA_PATH
dframe_team['Draft_year'] = (dframe_team['start_year']+1)[(dframe_team['Start'] >= dframe_team['start_cut'])] # This gives the GM's first year of the draft as the year after he started if he started after July 1 of the previous year$ dframe_team['Draft_year'] = dframe_team['Draft_year'].fillna(dframe_team['start_year'])$ dframe_team.head()
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ result = lm.fit()$ result.summary()
colors = ["lightblue", "green", "red", "blue", "orange", "maroon"]$ x_axis = np.arange(len(final_df))$ x_labels = diff_df.index
youtube_df= pd.read_csv("../Data/youtubeVid_main.csv",sep = ",")$ youtube_df["trending_date"] = pd.to_datetime(youtube_df["trending_date"] \$                                            , format = "%Y/%m/%d")
np.datetime64('2015-07-04 12:59:59.50', 'ns')
vac_start_date = '2015-09-01' # start date for Sep 1st 2015$ vac_end_date = '2015-09-16'$ data_start_date = dt.date(2015,9,1) - dt.timedelta(days=365)$
from textblob import TextBlob$ data_df['sent_pola'] = data_df.apply(lambda x: TextBlob(x['clean_desc']).sentiment.polarity, axis=1)$ data_df['sent_subj'] = data_df.apply(lambda x: TextBlob(x['clean_desc']).sentiment.subjectivity, axis=1)
cityID = 'b49b3053b5c25bf5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Denver.append(tweet) 
cmask = hdf.index=='child'$ hdf.loc[cmask, 'Age'].head()
df2 = df2.drop_duplicates(['user_id'], keep='last')$ df2[df2.duplicated(['user_id'], keep=False)]
p_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean()$ p_diff
df_2015 = df[df['Date'].dt.year==2015]$ df_2016 = df[df['Date'].dt.year==2016]
Measurements = Base.classes.measurement$ Stations = Base.classes.station
df_low_temps.describe()$
cats_out = outcome.loc[outcome['Animal Type']=='Cat']$ cats_out.shape
stemmer = SnowballStemmer('english')$ s_words = stopwords.words('english')
dataset['text length'] = dataset['text'].apply(len)
jobs = jobs.loc[jobs.JobID.str.contains('_') == False]
df.columns
health_data_row.loc[2013, 1, 'Guido']  # index triplet
start_time = pd.tslib.Timestamp('2017-04-07 00:00:00')$ end_time = pd.tslib.Timestamp(datetime.datetime.utcnow())$ repeatable = False
temp = pd.read_csv('tweets_i_master.csv')$ tweets_clean = temp.copy()$ tweets_clean.info()$
executable_path = {'executable_path': 'chromedriver.exe'}$ browser = Browser('chrome', **executable_path, headless=False)
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))$
avg_monthly = np.mean(df.month.value_counts())$ std_monthly = np.std(df.month.value_counts())$ print('The average beers drank per month is {:.2f} beers and the standard deviation is {:.2f} beers.'.format(avg_monthly, std_monthly))
playlist = sp.user_playlist(spotify_url.split('/')[4],spotify_url.split('/')[6])$ pd.io.json.json_normalize(playlist)
df = df[df.tax_class2 == '1']$
print activity_df.iloc[-3]
cr_treatment = df2[df2['group'] == 'treatment']['converted'].mean()$ cr_treatment$
file = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load('treas_parking_payments_2017_datasd_parsed.csv')$ file.show(3)
df.to_csv(save_dir)
sex.value_counts().sort_index().plot(kind='bar')$ sex.value_counts()
measurements_df.count()
validation.analysis(observation_data, BallBerry_resistance_simulation_1)
max_open = ldf['Open'].max()$ max_open
df_clean = df.copy()
print 'Total walking distance covered during the trip: %.2f km \nTotal steps covered during the trip: \t\t%d steps' % ($     sum(walking_df['ttl_wal_distance']),sum(walking_df['ttl_steps']))
archive_clean.sample(5)
items = {'Bob': pd.Series([245, 25, 55], index=['bike', 'pants', 'watch']),$          'Alice': pd.Series([40, 110, 500, 45], index=['book', 'glasses', 'bike', 'pants'])}
date_mask = (liquor['Date'] >= "2015-01-01") & (liquor['Date'] <= "2015-12-31")$ liquor_2015 = liquor[date_mask].sort_values(by=['Date'])
with open('datasets/git_log_excerpt.csv', 'r') as file:$     file = file.read()$     print(file)$
births['day'] = births['day'].astype(int)
lossprob2 = fe.bs.smallsample_loss(2560, poparr2, yearly=256, repeat=500, level=0.90, inprice=1.0)$
df3 = tier1_df.reset_index()$ df3 = df3.rename(columns={'Date':'ds', 'Incidents':'y'})
df.first_affiliate_tracked.fillna(value = 'no_info', inplace=True)$ df.info()
plt.pie(total_fare, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()$
df4[['CA','UK','US']] = pd.get_dummies(df4['country'])$ df4.head()
df['MatchTimeAwayROT'] = df.MatchTime + '$' + df.AwayROT.map(str)$ df['MatchTimeHomeROT'] = df.MatchTime + '$' + df.HomeROT.map(str)
sub = pd.DataFrame(np.column_stack((ids, countryList)), columns=['id', 'country'])$ sub.to_csv('sub.csv',index=False)
results2.summary()
svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf')$ svc.fit(x_res, y_res)$ scores = cross_validate(svc, x_res, y_res, cv=10, n_jobs=-1, return_train_score=True)
stats = loans.groupby('client_id')['loan_amount'].agg(['mean', 'max', 'min'])$ stats.columns = ['mean_loan_amount', 'max_loan_amount', 'min_loan_amount']$ stats.head()
co2_concentration = pd.read_table("data/greenhouse_gaz/co2_mm_global.txt", sep="\s+", $                                   parse_dates = [[0, 1]])$ co2_concentration
df2_old_page = len(df2.query("landing_page == 'old_page'")) / df2.shape[0]$ print('The probability that an individual received the old page is: {}.'.format(round(df2_old_page, 4)))
mylist = session.query(measurement.date, measurement.prcp).filter(measurement.date.between('2016-08-23', '2017-08-23')).all() $ 
old_page_converted=np.random.choice([1,0],size=n_old,p=[p_old,1-p_old])$ old_page_converted.mean()
from bmtk.builder.networks import NetworkBuilder$ net = NetworkBuilder('V1')
sanders_df = pd.DataFrame(sanders)$ sanders_df.head()
session.query(Measurement.station,func.count(Measurement.station)).\$         group_by(Measurement.station).\$         order_by(func.count(Measurement.station).desc()).all()
data.to_csv('prepared_data_with_cuts.csv', index = True)$
tweet_archive_clean.tweet_id = tweet_archive_clean.tweet_id.astype('str')$ tweet_archive_clean.timestamp = pd.to_datetime(tweet_archive_clean.timestamp)
cityID = '960993b9cfdffda9'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Bakersfield.append(tweet) 
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"$ date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read())$     f.close()
fine_xs = xs_library[fuel_cell.id]['transport']$ condensed_xs = fine_xs.get_condensed_xs(coarse_groups)
pd.Series(42)
joined = joined.dropna(axis=0)$ print('Number of rows in joined = {}'.format(joined.CustomerID.count()))
stations_df.to_csv('clean_hawaii_stations.csv', index=False)
df['converted'].mean()
df = pd.read_csv('end-part2_df.csv').set_index('date')$ df.describe().T
treatment_ctr=treatment_df.query('converted==1').user_id.nunique()/treatment_df.query('converted==0').user_id.nunique()$
p_diff_act=new_page_converted.mean()-old_page_converted.mean()$ p_diff_act
print("Predicted:", "\n",' '.join(y_pred[0]))$ print('\n')$ print("Correct:", "\n" ,' '.join(sent2labels(sample_sent)))
df3['country'].value_counts()
dfcsv = df.loc[df['fileType'] == 'csv']$ dfcsv['fileCount'].describe()
twitter_Archive=twitter_Archive.merge(df_json_tweets, left_on='tweet_id', right_on='tweet_id',how='left', left_index=True)$ twitter_Archive.info()
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])$
random_series = pd.Series(np.random.randn(1000), $                           index=pd.date_range('1/1/2000', periods=1000))$ random_series.plot();
df.shape[0]
recipes.ingredients.str.contains('[Cc]innamon').sum()
logmod = sm.Logit(ab_df2['converted'], ab_df2[['intercept','ab_page']])$ results = logmod.fit()
df_t1 = df.query('landing_page == "new_page"').query('group == "treatment"')$ df_t2 = df.query('landing_page == "old_page"').query('group == "control"')$ df2 = df_t1.append(df_t2, ignore_index=True)
rain = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23").\$        group_by(Measurement.date).all()$ rain
df_onc_no_metac = df_onc.drop(columns=ls_metac_colnames)$ df_onc_no_metac = df_onc_no_metac.rename(columns = {'METAC_SITE_NM1': 'METAC_SITE'})
y_train = train.click$ train = train.drop(columns=['click', 'id'])$
artists_info = [sp.artist(artist_id[i]) for i in range(0,num_songs)]
p_mean = np.mean([p_new, p_old])$ print("Probability of conversion udner null hypothesis (p_mean):", p_mean)
import time$ print("The time (in seconds) is:", time.time())    # Method
!hdfs dfs -cat {HDFS_DIR}/p32cf-output/part-0000* > p32cf_results.txt
precipitation_df = pd.DataFrame(sq.prec_last_12_months())
soup.find_all('div', class_='schedule-container')[0].select('.active')
twitter_archive_clean.loc[(twitter_archive_clean.rating_denominator> 10),"rating_denominator"]=10
images = pd.read_csv('image-predictions.tsv', sep ='\t')
df.count(), len(df.columns)
def remove_punctuation(text):$     exclude = set(string.punctuation)$     return "".join(ch for ch in text if ch not in exclude)
experience.to_csv('../data/experience.csv')
user = {"password":'e479deb2e1', 'username':'still-gnoll-6604'}$ trackobot = trackopy.Trackobot(user['username'], user['password'])
df = pd.read_csv('ab_data.csv')$ df.head()
logit_mod2 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US', 'week2', 'week3']]);$ results3 = logit_mod2.fit()$ results3.summary()
P.plot_1d('scalarCanopyTranspiration')
new_df = pd.read_sql_query('select * from actor where first_name = "Groucho"', engine)$ new_df.head()$
np.exp(ser)
df = pd.read_sql('SELECT * from membership', con=conn_b)$ df
data.head()$
results = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ for row in results:$     print(f"The most recent date recorded is {row}.")
weather_mean.iloc[1, 4]
threeoneone_census = gpd.sjoin(threeoneone_geo,census_withdata, how="inner", op='within')
df2_dup['group'].iloc[0],df2_dup['landing_page'].iloc[0],df2_dup['converted'].iloc[0]
trading_volume.sort()
slist = [s1, s2, s3]$ for item in slist:$     item.name == 'NAME':$
df_clean3['dog_name'] = df_clean3['name']$ df_clean3 = df_clean3.drop('name', axis=1)$ df_clean3.head()
test_df = allData[allData.lang=='es']$ print(test_df)
quandl.ApiConfig.api_key = API_KEY$ quandl.get('FSE/AFX_X', start_date='2018-08-08', end_date='2018-08-08')
import pandas as pd$ import numpy as np$ import matplotlib.pyplot as plt
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'),how='inner')
weather_file= "Data/CentralPark15-17.csv"$ flu_incident_file= "Data/ILINet_2015_2017.csv"$
weather = pd.concat([w10, w11, w12, w13, w14, w15, w16])$ weather.tail()$
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(z_score, p_value)
image_clean = image_clean.drop_duplicates(subset=['jpg_url'], keep='last')
chinese_vessels.to_csv('chinese_vessels_CLAV.csv', index=False, encoding='utf-8')
B_INCR2 = 0.293$ B_DECR2 = -0.293
df.xs(key=('a', 'ii', 'z'))
player['event'] = np.where((player['events'] == 'home_run'), #|$                            1, 0)$ player['event'].sum()
np.savetxt(r'text_preparation/tips.txt',tipsDFslice['text'], fmt='%s')
fa_index = pd.read_excel(path + 'fa_indexs.xlsx')$ fa_index
Addition = df.loc['Total']['AveragePrice'] + df.iloc[18248]['AveragePrice']$ print(Addition)
doctype = billstargs[billstargs.billtext.str.contains('DOCTYPE')]$ doctype.shape
old_page_converted= np.random.binomial(1, p=p_old, size=n_old)$ old_page_converted
news_p = soup.find('div', class_='rollover_description').text.strip()$ news_p
taxiData.head(5)
df = pd.read_csv('ab_data.csv')$ df.head()
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB.json?start_date=2014-01-01&end_date=2014-01-03&api_key=HL_EJeRkuQ-GFYyb_sVd')
dd = df.duplicated(['user_id'])$ df['duplicate_user'] = dd$
longest_date_interval = longest_date_interval.reset_index()$ longest_date_interval.columns = longest_date_interval.columns.remove_unused_levels().droplevel(level=1)$ print(longest_date_interval.head(3))$
loans_df = loans_df.query('home_ownership != "ANY"')
df = pd.read_csv('ab_data.csv')$ df.head()
df_day = endog.to_frame()$ df_day.rename(columns={"BikeID" : "Count"}, inplace=True)
df.to_csv('ab_data_cleaned.csv', index = False)
latest_version = str(d.id[0])$ print latest_version
red.columns
cityID = 'b004be67b9fd6d8f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Norfolk.append(tweet) 
data = data[~data["Improvements"].isnull()]$ print("Number of surveys: ", len(data))
result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4)$ result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)')$ np.allclose(result1, result2)
y_train=bow["label"]$ X_train=bow.drop({"ID","label"},axis=1)$
df2[df2.duplicated('user_id', keep=False) == True]
grouped_by_year_DRG.groupby(level=0)$
new_df_left = train_df.merge(prop,how='left',on = 'parcelid')
lstCols = list(df.columns.values)$ lstCols
df_joined[['CA', 'UK', 'US']] = pd.get_dummies(df_joined.country)$ df_joined = df_joined.drop(['CA'], axis=1)$ df_joined.head()
yhat_summore = modell.predict(X_dunno)$
import pprint$ summary = learn.summary()$ print(str(pprint.pformat(summary))[:1000])
print ab_counts.first_name.values$ print type(ab_counts.first_name.values)$
print "Total number of rows of table 'photoz_bcnz': ", len(df3) $ print df3.columns.values$ df3
df.describe()
Base.classes.keys()
df2['converted'].mean()$
sentiments_pd.to_csv("Twitter_News_Mood.csv", index=False)
corrplot(corr=r[coins_infund].loc[start_date:end_date].corr())$ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date)$ plt.show()
engine.execute('SELECT * FROM station LIMIT 10').fetchall()
def ceil_dt(dt):$     delta = timedelta(minutes=15)$     return (dt + (datetime.min - dt) % delta) + timedelta(hours=2)
prcp_df = pd.DataFrame(prcp_scores, columns = ['Date', 'prcp'])$ prcp_df.set_index('Date' , inplace=True)$ prcp_df.head()
states = pd.DataFrame({'population': population,$                        'area': area}   )$ states
tranny = df.T$ tranny
df_ec2[df_ec2['AvailabilityZone'].isnull()]['UsageType'].unique()
ss_scaler.fit(active_list_pending_ratio_test)$ active_list_pending_ratio_test_transform = ss_scaler.transform(active_list_pending_ratio_test)$ active_list_pending_ratio_transform[0:5,:]
df2 = df2.drop_duplicates(subset=['user_id'])
tweets_by_user_mentions = pd.read_sql_query(query, conn, parse_dates=['created_at'])$ tweets_by_user_mentions.head()
merged1.drop('id_y', axis=1, inplace=True)
data=pd.read_sql_query("select date,prcp from measurement where date >'2016-08-22';",engine)$ data
open('data/wx/tmy3/proc/700197.csv').readlines()[:6]
DynamicColumnFormatter(port.pl.report_by_year())
df2 = df.query('(group == "treatment" and landing_page == "new_page") or (group == "control" and landing_page == "old_page")')$ df2.head()
for entry in data_list:$     daily_traded.append(entry[6])
%%time$ [upload_blob(bucket_name, source_file, source_file.replace('\\', '/')) for source_file in filepathlist]
list(tweets_total[0].keys())
data[(data['author_flair'] == 'Saints') & (data['game_state'] == 'close')].comment_body.head(15)$
active_with_return.dropna(inplace = True)
vlc = df[(df.country == "es") & (df.city == "Valencia") & (df.status == "active")]
ns1min=1*60*1000000000   # 1 minutes in nanoseconds $ pd.DatetimeIndex(((all_site_visits.index.astype(np.int64) // ns1min + 1 ) * ns1min))$
errors_df = pd.read_csv('tweet_json_errors.csv')$ errors_df
df = pd.read_sql(SQL, db)$ df
data = pd.read_csv("BreastCancer.csv")$ data.head()
q = pd.Period('2017Q1',freq='Q-JAN')$ q=q.asfreq('M',how="start")$ q
feedbacks_stress.loc[feedbacks_stress['versao'] == 1, ['incomodo', 'interesse1', 'interesse2'] ] //= 2$
df1=df[df.Trip_distance>=100]$ df1.loc[:,['Trip_distance','Trip_duration']]
df[(df.landing_page == 'new_page') & (df.group != 'treatment')].user_id.count() + df[(df.landing_page != 'new_page') & (df.group == 'treatment')].user_id.count()
autos = autos.drop(["num_photos", "seller", "offer_type"], axis=1)
df3[df3['group'] == 'treatment'].head(3)
mean = np.mean(df.len)$ print('Tweet length average is: {}\n'.format(mean))
session.query(Adultdb).filter_by(education="9th").delete(synchronize_session='fetch')
with open("happiness.json", "r") as fp:$     happiness = json.load(fp)
predict_acct_id_udf = F.udf(predict_acct_id,types.DoubleType())$
df_clean.info()
total3=total.ix[(total['RA0']<335) & (total['RA0']>225)]$ total3$
customers_arr = np.array(cust_list)$ items_arr = np.array(item_list)
(p_diffs > ab_data_diff).mean()
sen_dat = pd.read_msgpack('full_DataFrame/combined_data/master/sentiment_data_2min.msg')$ sen_data2 = pd.read_msgpack('full_DataFrame/combined_data/master/sentiment_data_5min.msg')
filter_df = filter_df[filter_df['start_time'] >= datetime(2016, 8, 1, 0, 0)]$ filter_df.head(2)
portfolio_df = pd.read_excel('Sample stocks acquisition dates_costs.xlsx', sheetname='Sample')$ portfolio_df.head(10)
import warnings; warnings.simplefilter('ignore')
import re$ df2['rx_requested'] = re.sub(r'[^\w]', ' ', df2['rx_requested'])
grouped_data = dj_df.groupby(by='company_ticker')$ cmp_to_scaler = {}$ norm_dj_df = pd.DataFrame(columns=dj_df.columns) # Dataframe with quarter_start, company_ticker, normalized-revenue information.
results.summary()
yc_new3.describe()
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(new_df2.set_index('user_id'), how='inner')
df2['Change'].abs()  
df['job'] = df['job'].fillna(value=0)
autos['price'].unique()
df2 = pd.DataFrame(bmp_series, columns=['mean_price'])$ df = pd.concat([df1,df2], axis=1)$ df
countries.user_id.nunique()
print("Number of columns: %s" % (len(excel_data.columns)))
print(scratch.shape)$ scratch = scratch.dropna(subset=['age'])$ print(scratch.shape)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-05-22&end_date=2018-05-22&api_key=zDPbq2QVaB7jFAEq5Tn6')
df3 = pd.read_csv('countries.csv')$ df4 = df3.set_index('user_id').join(df2.set_index('user_id'))$ df4.head()
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)$ tf.summary.scalar("loss", loss)$ merged_summary_op = tf.summary.merge_all()
old_page_converted = np.random.binomial(1, p_old, n_old)$ old_page_converted
 $ sorted_budget_cheapest = df.sort_values(by=['budget_adj'], ascending = True).head(200)
station_count.loc[(station_count['Count'] >= 2000)]
example_sent = "April 2018 JAX Magazine is Out: Machine Learning. #BigData #DeepLearning #MachineLeaning #DataScience #AI #Python #RStats @filtration \\ppo."$ word_tokens = tokenize_tweet_text(example_sent, Qye_words = ['BigData'])$ print(word_tokens)$
log_file_name = 'shopee_add_and_cancel_fans_log\\shopee_add_and_cancel_fans_log_' + today_date_string + '.txt'$ logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
dataframe.groupby('day_of_week').daily_worker_count.agg(['count','min','max','sum','mean'])
hpd["2015-06":"2015-08"]['Complaint Type'].value_counts().head(5)
fill = A.stack().mean()$ A.add(B, fill_value=fill)
print(spike_tweets.iloc[5000, 2])
lm_2 = sm.Logit(new_df2['converted'], new_df2[['intercept', 'Middle', 'End', 'ab_page']])$ reg_lm2 = lm_2.fit()$ reg_lm2.summary()
lm_withsubID_export_path = cwd+'\\LeadGen\\Ad hoc\\SubID\\LM Sig Loans with SubID.xlsx'$ lm_withsubID.to_excel(lm_withsubID_export_path, index=False)
new_page_converted = np.random.choice([1,0], size = nNew, p=[pMean,oneMinusP])$ new_page_converted.mean()
cityID ='5c62ffb0f0f3479d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Phoenix.append(tweet) 
preview.shape
def tempF2C(x): return (x-32.0)*5.0/9.0$ def tempC2F(x): return (x*9.0/5.0)+32.0
intervention_train['MILLESIME'] = intervention_train['MILLESIME'].astype(int)$ intervention_train['RESOURCE_ID'] = intervention_train['RESOURCE_ID'].astype(int)
fixey = tidy_format.merge(sent, how='left', right_index=True, left_on='word').groupby(['id']).sum()$ trump['polarity'] = fixey['polarity']$
from scipy.stats import norm$ critical_value = norm.ppf(1-0.05)$ critical_value
df1.isnull().sum()
train = train[train.price < 100000]
dateList = LBF.start_date_local$ print(len(dateList), dateList.min(), dateList.max())$
ndf = ndf.assign(Norm_reviewText = normalize_corpus(ndf.reviewText))
x_train.head()$
tweet_df["tweet_date"] = pd.to_datetime(tweet_df["tweet_date"])
print("The number of unique users in df2 is: {}".format(df2.nunique()['user_id']))$
sampleSize=ratings.groupby("rating").count().reset_index()['userId'].min()$ sampleSize
df_twitter = pd.read_csv('data/twitter-archive-enhanced.csv')$ df_twitter_copy = df_twitter.copy()$ df_twitter_copy.head()
tdf = sns.load_dataset('tips')$ tdf['data']= 1$ tdf.sample(5)
%%time$ max_key = max( r_dict.keys(), key = get_nextday_chg )$ print('largest change in price between any two days: '+ str( get_nextday_chg(max_key) ) )
s = pd.Series([7, 'AmarAkbar', 3.14, -1789710578, 'Sholay'])$ s
oil_interpolation['date']=oil_interpolation.index$ pd.DataFrame.head(oil_interpolation)$ oil_interpolation.plot('date','dcoilwtico',kind='bar', color='r', figsize=(15,5))
df_clean=df.copy()$ df_clean.head()
train_body_raw = traindf.body.tolist()$ train_title_raw = traindf.issue_title.tolist()$ train_body_raw[0]
precip_df = pd.DataFrame(precip, columns=['date', 'prcp'])$ precip_df.set_index('date', inplace=True, )$ precip_df.head()
iowa_file = '../../DSI-SF-3/datasets/iowa_liquor/Iowa_Liquor_sales_sample_10pct.csv'$ iowa = pd.read_csv(iowa_file)$ print "Dataframe is of size: " + str(iowa.values.nbytes / 10**6) + "MB"
api_copy.info()
df_new = df_new.drop('CA', axis=1)
df_corr = result.groupby(['type', 'scope'])['site'].sum().reset_index()$ display(df_corr.sort_values('site',ascending=False).head(10))$ plot2D(df_corr, 'type', 'scope','site')
newfile = newfile[~newfile['Ov.transport status'].isin({5, 6, 7})]
fan_zip = [zipcode.isequal(str(zc)) for zc in questions['zipcode']]$ questions['zipcode'] = fan_zip
Rural = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Rural")]$
xml_in_sample1['authorId'].nunique()
recent_prcp_data = session.query(Measurement.date, Measurement.prcp).\$     filter(Measurement.date >= YrFrombd).\$     order_by(Measurement.date).all()$
p_old = df2['converted'].mean()$ print(p_old)
tUnderweight = len(df[df['bmi']< 18.5])$ tUnderweight
soup = BeautifulSoup(page, 'html.parser')
exec(open ('../../config.py').read ()) # Load file where API keys are stored$ API_KEY = QUANDL_API_KEY
print(d + 3.33, '\n') #See that 3.33 is added to each element in the array$ print(e * 2, '\n') # '*' operator works elementwise
words = [w for w in words if w not in stopwords.words('english')]$ print(words)
weather['events'] = weather['events'].apply(lambda x: 'Rain' if x == 'rain' else x)
days = ['day1', 'day2']$ nocol=pd.DataFrame(index=days)$ nocol
cities =   './brandon-telle-cruise-ship-locations/data/output_cities.csv'                        ## 55k$ location = './brandon-telle-cruise-ship-locations/data/output_daily_ship_location.csv'           ## 72Meg$ ships =    './brandon-telle-cruise-ship-locations/data/output_ships.csv' ## wikipedia references ## 51k$
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']])$ res = mod.fit()$ res.summary()
result = forest.predict(testDataVecs)$ output = pd.DataFrame(data={"id":test["id"], "sentiment":result})$ output.to_csv( "output.csv", index=False, quoting=3 )
np.mean([len(h.tweets) for h in heap])
dataset.groupby(by=['user_location'])['id'].count()
from pysumma.utils import utils$ import os
no_of_stations = engine.execute('SELECT count(*) FROM station').fetchall()$ print("No_of_stations", no_of_stations)
cur.execute(sql_all_tables)$ all_tables_df = pd.DataFrame($     cur.fetchall(), columns=[rec[0] for rec in cur.description])
fashion['created'] = pd.to_datetime(fashion.created)$ fashion.set_index('handle', inplace=True)$ fashion.dtypes
buyer=patch.groupby(['buyer_name'])$ repeat=buyer.count()
df = pd.read_csv(datafile)$ print(df.as_matrix().shape)
df['user_id'].nunique()
n_new = len(df2.query('landing_page == "new_page"'))$ n_new
df2 = df2.drop_duplicates(['user_id'])
census_tracts_df['shapes'] = census_tracts_df['geometry'].map(loads)
df.loc['20180103','A']
precipitation_totals.describe()
df2.query('group == "control"')$ df2.query('group == "control" and converted == 1')$
dfp= dfp.reset_index()$ dfp.columns
AAPL.to_csv('/home/hoona/Python/mpug/directFromPandaDataFrame.csv')
prob_group3 = df2.query("landing_page == 'new_page'")["user_id"].count()$ prob = prob_group3 / df2.shape[0]$ print("The probability that an individual received the new page is {}.".format(prob))
Magic.__repr__
taxi_hourly_df.loc[index_missin_hr0to6_before2016, "num_pickups"] = 0$ taxi_hourly_df.loc[index_missin_hr0to6_before2016, "num_passengers"] = 0$ taxi_hourly_df.loc[index_missin_hr0to6_before2016, "missing_dt"] = False
from pprint import pprint # ...to get a more easily-readable view.$ pprint(example_tweets[0])
from src.environments.portfolio import PortfolioEnv$
pivoted.shape
store_items.interpolate(method = 'linear', axis = 1)
df2[['ab_page', 'ab_page1']] = pd.get_dummies(df2['landing_page'])$ df2 = df2.drop('ab_page1', axis = 1)
unique_cols=pd.unique(["Country"]+df.columns.values.tolist())$ unique_cols
df_ad_state_metro_1['state'].value_counts().plot(kind='bar')
LabelsReviewedByDate = wrangled_issues_df.groupby(['Priority','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
metadata = df_small[['order_num', 'date', 'country']].drop_duplicates()$ metadata.head()
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&collapse=null&api_key=my_API_KEY'
df2.query('landing_page == "new_page"').shape[0]/df2['landing_page'].shape[0]
from nltk.corpus import stopwords$ english = stopwords.words('english')
scores_firstq = np.percentile(raw_scores, 25)$ scores_thirdq = np.percentile(raw_scores, 75)$ print('The first quartile is {} and the third quartile is {}.'.format(scores_firstq, scores_thirdq))
null = df.isnull().values.sum()$ print('The number of missing values in the dataset is {}'.format(null))
dc.head(5)
events_popularity_summary = events_top10_df[events_top10_df['yes_rsvp_count']>5][['event_id','popularity','topic_name']].pivot_table($                                 index='topic_name', columns='popularity', aggfunc='count')
p_old = df2.converted.mean()$ p_old$
total_treatment = df2[(df2['landing_page'] == "new_page")].count()$ print(total_treatment)
twelve_months_prcp.head()
sns.distplot(virginica)$ sns.distplot(versicolor)
date + pd.to_timedelta(np.arange(12), 'D')
precp_df = pd.DataFrame(results, columns=['date', 'prcp'])$ precp_df.set_index('date', inplace=True, )$ precp_df.head(10)
model_filename = 'models/finalized_traffic_flow_prediction_model.sav'$ loaded_traffic_flow_prediction_model = pickle.load(open(model_filename, 'rb'))$
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(doc_duration, params=params, freq=12, title='Doctors Decomposition')
del sales['County Number']$ true_sales = sales.dropna()$ true_sales.shape
df_students.shape
model_arma = sm.tsa.ARMA(AAPL_array, (2,2)).fit()$ print(model_arma.params)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path)$ df.head(5)
print(hn.created_at.min())$ print(hn.created_at.max())
df["Source"].unique()
session.query(func.count(station.id)).scalar()
(act_diff < p_diffs).mean()
crimes['month'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.month)
fire_size = pd.read_csv('../data/model_data/1979-2016_fire_size.csv', index_col=0)$ fire_size.dropna(inplace=True)$
ab_diff = df[df['landing_page'] == 'new_page']['converted'].mean() -  df[df['landing_page'] == 'old_page']['converted'].mean()$ p_diffs = np.array(p_diffs)$ (ab_diff < p_diffs).mean()
sample.groupby('tasker_id').hired.count().sort_values(ascending=False).head(1)
fname = '/Users/kylefrankovich/Desktop/insight/list_test_data/deluxetattoochicago/deluxetattoochicago.json'$ data = json.load(open(fname))$ len(data)
train_df[train_df['parcelid'].duplicated(keep=False)]$
seventeen = r.json()['dataset_data']$ seventeen
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')$ git_log['timestamp'].describe()
a =R16df.rename({'Create_Date': 'Count-2016'}, axis = 'columns')$
df_final_edited = pd.read_csv('twitter_archive_master_edited.csv', sep=',')
scratch['created_at'] = scratch['created_at'].map(lambda x: x.lstrip('"').rstrip('"'))
Base.prepare(engine, reflect=True)
df.loc[0, 'review'][:-500]
trip_data = pd.read_csv("green_tripdata_2015-09.csv", header=0, delimiter=",")
reddit = pd.read_csv(filename)$ reddit.drop('Unnamed: 0', axis = 1, inplace = True)$ reddit.head()
autos["price"]= autos["price"].replace('[$,]','',regex=True).astype(int) 
plt.hist(data.sepal_length, bins=25)
df.head()$
from sklearn.neighbors import KNeighborsClassifier $ from sklearn.model_selection import train_test_split$ V=pd.merge(cards,game_vail,right_index=True, left_index=True)$
precip_data_df.set_index("date",drop=True,inplace=True)$ precip_data_df.columns$ precip_data_df.tail()
first_values = grouped_months.first()$ first_values=first_values.rename(columns = {'Totals':'First_v_T'})$
df.head(2)
print data_df.clean_desc[20]$ print 'becomes this ---'$ print text_list[20]
df2 = df2.join(country_dummies);
newdf.describe()
tips.head(3) # default 5$
driver.get("http://www.reddit.com")$ time.sleep(1)
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="Dreamers").items(max_tweets):$     print(tweet)
full_monte['info'].str.get_dummies('|')
for c in ccc:$     vwg[c] /= vwg[c].max()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].count()
calendar = USFederalHolidayCalendar()$ holidays = calendar.holidays(start=train.date.min(), end=train.date.max())
1/np.exp(-0.0149), 1/np.exp(-0.0408), np.exp(0.0099)
df2=pd.merge(df2,country, on=['user_id'])$ df2.head()
import statsmodels.api as sm$ convert_old = df2.converted[df.landing_page == 'old_page'].sum()$ convert_new = df2.converted[df.landing_page == 'new_page'].sum()$
ctrl_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][0]$ print("P(ctrl_con) = %.4f" %ctrl_con)
outfile = path + '../output/allData_NoRetweets_May27_Cleaned.csv'$ cleanedDataNoRetweets.to_csv(outfile, index=False, encoding='utf-8')
df_new.country.unique()
path = 'C:\\Users\\U502175\\Documents\\Projects\\Active\\{}\\info\\'.format(project_name)$ fig.savefig(path+'metrics.svg', transparent=False, dpi=80, bbox_inches='tight')$
ids = df2['user_id']$ df2[ids.isin(ids[ids.duplicated()])]
data_df.groupby('tone')['ticket_id'].nunique()
if 0 == go_no_go:$     all_top_vecs = [lda.get_document_topics(serial_corp[n], minimum_probability=0) \$                     for n in range(len(serial_corp))]
techDetails.to_pickle('PC_World_Latops_Scrape.pickle')$ techDetails.to_csv('PC_World_Latops_Scrape.csv')
mRF.model_performance(test)
df_clean['tweet_id'] = df_clean['tweet_id'].astype('str')$ image_clean['tweet_id'] = image_clean['tweet_id'].astype('str')$ tweet_clean['tweet_id'] = tweet_clean['tweet_id'].astype('str')
messages = rip.messages()$ while messages.iolder():$     pass
zipcode = "nyc_zip.geojson"$ zc = gpd.read_file(zipcode)$ zc.shape
normals=[]$ for item in date_list:$     normals.append(daily_normals(item)[0])
twitter_archive_master[twitter_archive_master.duplicated()]
abunai_timeline=api.user_timeline("abunaifood")$ abunai_timeline
wrd = pd.read_csv('twitter-archive-enhanced.csv')$ wrd.head()
df.head()
deck = pd.DataFrame(titanic3['cabin'].dropna().str[0])$ deck.columns = ['deck']  # Get just the deck column$ sns.factorplot('deck', data=deck, kind='count')
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
import  statsmodels.api  as sm$ log_mod = sm.Logit(df2['converted'],df2[['intercept','treatment']])$ results = log_mod.fit()
y_pred = lassoreg.predict(X_test)$ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
exiftool -csv -createdate -modifydate ciscid4/CISCID4_cycle2.mp4 ciscid4/CISCID4_cycle3.mp4 ciscid4/CISCID4_cycle4.mp4 ciscid4/CISCID4_cycle5.mp4 ciscid4/CISCID4_cycle6.mp4 > ciscid4.csv
df_joined_dummy = pd.get_dummies(data=df_joined, columns=['country'])$ df_joined_dummy.head()
df2.info()
df2=df[df.Trip_distance < 100]$ df2['Avg_speed'].describe()
import gensim$ LabeledSentence = gensim.models.doc2vec.LabeledSentence
transform = TfidfVectorizer(lowercase=False, min_df=.01)$ tf_idf_matrix = transform.fit_transform(back2sent.values)$ tf_idf_matrix.shape
df2.groupby(df2['group']=='treatment')['converted'].mean()[1]
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','US','CA']])$ results = logit_mod.fit()$ results.summary()
if weights_flag:$     regridder.clean_weight_file()  # clean-up
sentiment_df = pd.DataFrame(save_sentiment)$ sentiment_df.to_csv('sentiment.csv', sep=',', header=True, index=True, index_label=None)$ sentiment_df
df=pd.read_csv('twitter_archive_master.csv')
log_sgm_bgp_100yr_run(L0 = 1000, E0 = 1, Delta_n = 0.02)
archive_clean[archive_clean.name == 'None']
%matplotlib inline$ TpFDroped.plot(kind='bar', legend=True)$
df1 = df.groupby("Symbol")["Percent_Diff"].sum()$ df1
dataframe['ddate'] = pd.to_datetime(dataframe.ddate)
tc = pd.concat([tce2, tcp], ignore_index=True)$ tc
df2 = df2.drop_duplicates(subset='user_id');
merge[merge.columns[47]].value_counts()$
print(team.shape)$ team.head()
df2 = pd.DataFrame(df.groupby("media source").mean()['compound score'])$
y_estimates = lm.predict(x_min_max)$ y_estimates
bufferdf.Fare_amount.mean().ix[[2,3,4]]
cp311['day'] = cp311.created_date.dt.to_period('D')$ cp311.set_index('day', inplace=True)$ cp311.head(2)
model2=sm.Logit(df_new['converted'],df_new[['intercept','treatment','CA','UK']])$ result_2=model2.fit()$ result_2.summary()
df_pol['text']=df_pol['title'].str.replace('\d+', '')$
display(Markdown(q5d_answer))$ ins.groupby(["year"]).count()
df4 = bg_df2[['Date', 'BG']].copy()$ df4 # whew, this way works fine$
vals2.sum(), vals2.min(), vals2.max()
df['user_id'].nunique()
qrtSurge = ((qrt.shift(-3)- qrt) / qrt )$ surge = qrtSurge[qrtSurge>1]$ surge.sort_values(ascending=False)$
ds = df[(df['created_at']>'2017-07-01')].reset_index()$ ds = ds.drop('index',1)$ ds = ds.drop('created_at',1)
old_page_converted = np.random.choice([1,0], size=n_old, p=(p_old, 1-p_old)).mean()$ old_page_converted
files_txt = glob.glob("*.txt")
jobs_data['clean_description'] = jobs_data['record.description'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", str(x)).split() if i not in s_words]).lower())
con = sqlite3.connect('db.sqlite')$ print(pd.read_sql_query("SELECT * FROM temp_table ORDER BY year ASC, total DESC", con))$ con.close()
y_prob = gnb.predict_proba(X_clf)$ print(y_prob.shape)
tweets_df = tweets_df[tweets_df['location'] != ''].reset_index(drop=True) # reset index from 0$ tweets_df = tweets_df.sort_values('timestamp')$ print('got locations for {} retweets'.format(len(tweets_df)))
df = pd.read_csv('data/goog.csv')$ df
w.index.freq, b.index.freq, r.index.freq
feature_cols = ['TV', 'radio']$ X = data[feature_cols]$ print(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')).mean())
sample=train.head(100).copy()$
deployment_details = client.deployments.create(model_uid, 'Retail_Churn_with_XGBoost')
means = df_lg_hubs.mean()$ default_use = [means[str(i)] for i in range(1, 13)]$ default_use
fs = gridfs.GridFS(db)
len([col_n for col_n in df.columns if '_rolling' in col_n])$
year_tobs_df = pd.DataFrame(year_tobs, columns=['date', 'tobs'])
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))$
df.loc[:,"Date"] = 
nnew =  df2[((df2['group'] == 'treatment') )].count()[0]$ print(nnew)
train['discussion_other'] = ((train.url.isnull()) & (~train.title.str.contains(' HN: '))).astype(int)$ train.groupby('discussion_other').popular.mean()
fg.savefig("Sentiment Analysis of Tweets.png")
cleansed_search_df['Date'] = pd.to_datetime(cleansed_search_df['Date'], format="%Y%m%d", errors="coerce")$ cleansed_search_df
df_birth['Country '].value_counts(dropna=False).head()
n_new = df2[df2['group'] == 'treatment']['user_id'].count()$ print('Elements of treatment group n_new: ',n_new)
y = df['comments']$ X = df[['subreddit', 'title']].copy(deep=True) 
df_joined.groupby(['country','ab_page_new_page']).converted.count()
rs = session.query(func.max(Measurement.date))$ Last_date = rs[0][0]$ print(Last_date)$
groupedNews = sentiments_df.groupby(["Target"], as_index=False)$
precipitation_df.sort_values("date", inplace = True)$ precipitation_df.head(15)
measurement_results = session.query(Measurements.station,Measurements.date,Measurements.prcp, Measurements.tobs).all()$
monte.str.split()
ctd = gcsfs.GCSFileSystem(project='inpt-forecasting')$ with ctd.open('inpt-forecasting/Census Tool Data Pull CY2017- May 2018CONFIDENTIAL.xls.xlsx') as ctd_f:$   ctd_df = pd.read_excel(ctd_f)
tweetsIRMA = pd.read_sql("SELECT tc.tweet_id, i.DateTime, tc.text, tc.longitude as 'tweet_long', tc.latitude as 'tweet_lat', i.lon as 'irma_long', i.lat as 'irma_lat' FROM tweetIrmaTimes ti JOIN irmaFeatures i on ti.irmaTime = i.DateTime JOIN tweetCoords tc on tc.tweet_id = ti.tweet_id",Database().myDB)
free_data.head(10)
cityID = '813a485b26b8dae2'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Albuquerque.append(tweet)  
cust_demo.sort_values(by=['Location', 'Gender'], ascending=[False, True]).head(50)$
norm.ppf(1-0.05)
users.head()
print ('Difference of means:', Ralston["TMAX"].mean() - Ralston["TMIN"].mean())
multi_app = picker[picker>1]$ print len(multi_app)$ df_nona[df_nona.district_id.isin(multi_app.index)].install_rate.hist()$
n_old = df2[df2['group'] == 'control']['converted'].count()$ n_old
dframe_team.set_value(3, 'Executive', 'Dave Twardzik, Otis Smith')$ dframe_team.set_value(4, 'Start', '2006-05-03')$ dframe_team
measurements_year = session.query(Measurements.date,Measurements.prcp).limit(10).all()$ for mammal in measurements_year:$     print(mammal)
df_ctr = df2[df2['group'] == 'control']['converted'].mean()$ print("{} is the probability they converted.Thus, given that an individual was in the control group.".format(df_ctr))
plt.hist(null_vals)$ plt.axvline(x=obs_diff,color ='red');
grpConfidence['MeanFlow_cfs'].count()
transactions['items_total'].describe()
df2['tweet_id'][(df2['predict_3_breed'] == True)].count()/2075$
validation.analysis(observation_data, Jarvis_simulation)
image_predictions.to_csv(data_folder+'image-predictions.tsv', sep = '\t')
from sklearn.model_selection import cross_val_score$ from sklearn.ensemble import GradientBoostingClassifier$ forest_clf.score(X_test, Y_test) # test socre$
locations = DataSet['userLocation'].value_counts()[:10]$ print(locations)
categories = ['event','eventClassification']#,'companyInvolved','operationOrDevelopment','jobTypeObserved','stopJob','immediateActionsTaken','rigInvolved']$ BUMatrix = pd.get_dummies(df_trimmed,columns = categories)
df['group'].value_counts()
country_with_least_expectancy = le_data.idxmin(axis=0)$ country_with_least_expectancy
np.random.seed(123456)$ ps = pd.Series(np.random.randn(24),pd.period_range('1/1/2013','12/31/2014',freq='M'))$ ps
df.head()
df['age'] = df['fetched time'] - df['created_utc']$ df['age'] = df['age'].astype('timedelta64[m]')$ df.head()
df.index
data.info()
num_users2 = df2.nunique()['user_id']$ print("The number of unique users in the dataset is : {}".format(num_users2))
df=pd.read_csv("../Raw Data/Trump tweets 3 day groups jan-dec 2017.csv")$ df.head()
dfd['brand'] = [b.strip().title() for b in dfd.brand]$ dfd.brand.unique()
df2[df2.duplicated('user_id')]
df = pd.read_csv("nyhto_twitter_edited.csv")$ df2018 = pd.read_csv("nyhto_twitter_edited2018.csv")
country = pd.get_dummies(auto_new.Country)$ country.head()
daily_df['Price_Change'].value_counts()
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','US','UK']])$ result = logit_mod.fit()$ result.summary()
df.info()
df4.dtypes
pca = PCA(n_components=6858)$ pca.fit(crosstab)
! hdfs dfs -rm -R -f -skipTrash lastfm_model.spark$ model.save(sc,"lastfm_model.spark")$
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2016-07-01&end_date=2016-07-01&api_key={}'.format(API_KEY)$ r=requests.get(url)$ print(r.json())
brand_info.sort_values("mean_price", ascending=False)
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df_amznnews_clsfd_2tick = df_amznnews_clsfd_2tick.set_index('publish_time')$ df_amznnews_clsfd_2tick.info()
tobs_df.hist(bins=12)$ plt.show
TrainData.describe(include='all')$
df2.drop(2893, inplace=True)$ df2[df2['user_id'].duplicated()]$ df2.shape$
session.query(func.count(Station.station)).all()
prec_us_full = prec_nc.variables['pr_wtr'][:, lat_li:lat_ui, lon_li:lon_ui]
engine.execute("SELECT count(station), station FROM measurement GROUP BY station ORDER BY count(station) DESC").fetchall()
new_years_eve = dt.date(2016, 12, 31)
df = pd.read_csv('ab_data.csv')$ df.head()
sns.distplot(dfz.retweet_count, color = 'blue', label = 'Retweets')
n_old = df2.query('group == "control"').shape[0]$ print(n_old)
dfprediction = pd.DataFrame(data={'y_hat': rf.predict(X_btc_test)}, index=X_btc_test.index)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-20&end_date=2018-08-20&api_key=' + API_KEY$ r = requests.get(url)
val='2017-08-14-23' # string$ datetime.strptime(val, '%Y-%m-%d-%H') # date
model = pipeline.fit(train)
from scipy.stats import norm$ norm.cdf(z_score)
model.wv.syn0.shape
props.prop_name.value_counts()$ propnames.value_counts()
pd.merge(df1, df3, left_on="employee", right_on="name" )
for p in mp2013:$     print("{0} {1}".format(p.start_time,p.end_time))
dtanswer = qs.CreationDate_a - qs.CreationDate
measurements = Base.classes.measurements$ Station = Base.classes.stations
df_customers['number of customers'].mean()
df['year'] = pd.DatetimeIndex(df['datetime']).year$ df['month'] = pd.DatetimeIndex(df['datetime']).month$ df.head()
cog_simband_times = cog_simband_times.drop(labels=['ST-OTS BHC0066-1'])
containers[0].find("li", {"class":"name"}).a['title']
GamelogParser.GameLog().passing(urls[:1][0], season)
loanTree.fit(X,y)
query ="SELECT * FROM tddb_00.Weather_Log ORDER BY Log_Id DESC"$ df = pd.read_sql(query,session)$ df.head(10)
df.rename(columns={"PUBLISH STATES":"Publication Status","WHO region":"WHO Region"},inplace=True)
P = Plotting(output_path)$ Plot = P.open_netcdf()
df_arch_clean[df_arch_clean['tweet_id']== '883482846933004288'].rating_numerator$
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ print(commits_per_year.apply(lambda x: x.head())[0:5])
year_prcp = session.query(Measurements.date, Measurements.prcp).order_by(Measurements.date).filter(Measurements.date > year_ago)
sumAll = df['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])$ sumAll
a= bus[bus["postal_code_5"].isin(all_sf_zip_codes)==False]$ weird_zip_code_businesses=a[a.notnull()["postal_code_5"]]$
df2.head()
from statsmodels.tsa.stattools import acf, pacf
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-05-23&end_date=2018-05-23&api_key='+API_KEY$ r1 = requests.get(url1)
pattern = re.compile(' +')$ print(pattern.split('AA   bc'))$ print(pattern.split('bcA A'))
Session=sessionmaker(bind=engine)$ session=Session()$ result_set=session.query(Adultdb).first()$
df_MC_least_Convs = pd.concat([year_month.transpose(), df_MC_leastConvs], axis=1)$ print 'DataFrame df_MC_least_Convs: ', df_MC_least_Convs.shape$ df_MC_least_Convs
df2 = pd.read_csv('ab_new.csv')
!arvados-cwl-runner --name "Encode Blood Types" encode.cwl --arr ./npy_data/train_data.npy --script just_encode.py
inspector = inspect(engine)$ inspector.get_table_names()
df.nunique()['user_id']
class First_scrapyItem(scrapy.Item):$     name = scrapy.Field() #attribute "name" to be extracted $     address = scrapy.Field() #attribute "address" to be extracted$
title = soup.find_all('div', class_="content_title")[1].text.strip()$ description=soup.find_all('div', class_="rollover_description_inner")[1].text.strip()
df.head()
state_party_df.fillna(0, inplace=True)$ state_party_df.head(20)
path = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=fr89z8vqESGWrVzvNFxC&start_date=2017-01-01&end_date=2018-01-01?api_key=API_KEY'$ r = requests.get(path).json()
cityID =  '5a110d312052166f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Francisco.append(tweet) 
df_group = df.groupby('group')$ df_group.describe()
hawaii_measurement_df = pd.read_csv(r"\Users\Josue\Desktop\\hawaii_measurements.csv")
stocks.loc['Apple']
nb = MultinomialNB()$ nb.fit(X_train, y_train)$ nb.score(X_test, y_test)
df_a = df.query('landing_page == "old_page" and group == "control"') $ df_b = df.query('landing_page == "new_page" and group == "treatment"')$ df2 = df_a.append(df_b, ignore_index=True)
df['TYPE'].value_counts().sort_index()
df.sum().plot()$
print(df.columns)
sns.regplot(x=dfz.retweet_count, y=dfz.favorite_count)
xres3['hits']['hits']$ pd.DataFrame(xres3['hits']['hits']). ._source$
twitter_df[twitter_df.in_reply_to_status_id.notnull()]['in_reply_to_status_id'][0:5]
body = soup.body$ for paragraph in body.find_all('p'):$     print (paragraph.text)
!ptdump -av 'data/my_pytables_file.h5'
founddocs = fs.find({"$text": { "$search": "Cundall" }})$ print founddocs.count()
plantlist.capacity_net_bnetza = plantlist.capacity_net_bnetza.round(decimals=5)$ plantlist.capacity_gross_uba = plantlist.capacity_gross_uba.round(decimals=5)  $ plantlist.efficiency_estimate = plantlist.efficiency_estimate.round(decimals=5)
from jupyterworkflow.data import get_fremont_data$ Fremont = get_fremont_data()
import pandas_datareader.data as web  #Not 'import pandas.io.data as web' as in the book.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
team.isnull().sum()
load2017.isnull().sum() 
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=ibBvKxUvyJJC1ziPSUPJ'$ r = requests.get(url)
metatable.hist()$ pl.show()
df1.info()$
X = reddit_master[['age', 'subreddit']].copy(deep=True)$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
datos = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(datos.head(10))
print(autos['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).sort_index())$ print(autos['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).shape)
n_old = df2[df2['landing_page']=="old_page"].count()[0]$ n_old
criteria = so['ans_name'] == 'Scott Boston'$ so[criteria].head()
df2.drop(labels=2862, inplace=True)$ sum(df2['user_id'].duplicated())
for row in session.query(stations, stations.station).all():$     print(row)
correlations = [ df['overall_rating'].corr(df[f]) for f in cols ]
reports_sub = reports[["Service Location" , "Created Date", "Close Date", \$                        "Major Issue", "Description", "Resolution Description"]].reset_index()$ reports_sub.head()
volume_weather=general_volume.merge(df, left_on='ds' , right_on='Date')$ volume_weather.head()$ volume_weather=volume_weather[~volume_weather.T_avg.isnull()]
year_prcp_df = pd.read_sql_query(year_prcp.statement, engine,index_col = 'date')$ year_prcp_df.head()
newdf.groupby([newdf.Hour_of_day]).Trip_distance.median()
QLESQ.drop_duplicates(subset=["subjectkey"], keep='first', inplace=True)$ QLESQ.shape$ QLESQ.describe()
measure_val_2014_to_2017.count()
posts.groupby('from').aggregate(sum)
new_pred_prob = grid.predict_proba(X_new)[:, 1]$ new_pred_prob
empDf.join(deptDf, empDf.deptid == deptDf.id).show()
google_stock.isnull().any()
df_new_page = df.query("landing_page == 'new_page'")$ df_new_page.count()['user_id'] / df['landing_page'].count()
sample_diff = new_page_converted.mean() - old_page_converted.mean()$ sample_diff
ADBC = AdaBoostClassifier(n_estimators=50)$ ADBC.fit(X, y) 
btc = pd.read_csv('/home/rkopeinig/workspace/Time-Series-Analysis/data/btc.csv')$ btc['date'] = pd.to_datetime(btc['date'])$ btc = btc.set_index('date')
import statsmodels.api as sm$ df2['intercept'] = 1$ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']$
masked.user_created_at = pd.to_datetime(masked.user_created_at)
s3.reindex(np.arange(0,7), method='ffill')
hmeq.head()
display(data.head(20))
prcp_df.count()
cFrame['Cumulative'] = cFrame.groupby('Client')['Change'].apply(lambda x: x.cumsum())$ cFrame.head(20)
df_arch.describe()$
df.nunique().user_id
adj_close.head()
logit_mod_new = sm.Logit(df_new['converted'], df_new[['intercept','US','UK']])$ results_new = logit_mod_new.fit()$ results_new.summary()
mars_images = requests.get('https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars')$ mars_soup = BeautifulSoup(mars_images.content, 'html.parser')$
df.to_csv('ab_updated.csv', index=False)
locationing.polarity.plot(kind='hist', normed=True)$ range = np.arange(-1.5, 1.5, 0.001)$ plt.plot(range, norm.pdf(range,0,1))
df.isna().sum()
foxnews_df = constructDF("@FoxNews")$ display(constructDF("@FoxNews").head())
results = logit.fit()$ results.summary()
df = df.replace('nan', '')$
from scipy.stats import norm$ norm.ppf(1-(0.05/2))
df.reviewTime=pd.to_datetime(df.reviewTime)
station_count = session.query(Station.id).count()$ print(" There are {} stations within the data set".format(station_count))
crimes.iloc[10:20,4:6]
kick_projects = pd.merge(kick_projects, ks_particpants, on = ['category', 'launched_year', 'launched_quarter','goal_cat_perc'], how = 'left')
df.count()
Delta_r = +0.04$ Delta_Y_PS12taskB = -1
results2.summary()
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']])$ result = logit.fit()
Suburban = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Suburban")]$
for node in nodes:$     print node.text
test_classifier('c0', WATSON_CLASSIFIER_ID_1)$ plt.plot(classifier_stats['c0'], 'ro')$ plt.show()
val = val.split(b'\r\n\r\n',1)[1]$ print(val.decode('utf-8'))
pax_path = 'paxraw_d.csv'$ %time pax_raw = pd.read_csv(os.path.join(data_dir, pax_path), dtype=type_map)
sentimentDf.to_csv("twitterSentiment.csv")
df.head(5)
cdf.plot(kind='barh', x='category', y='occurrence_count', figsize=(12, 10), title= 'Categories', label= "Occurrence Count")$ plt.gca().invert_yaxis()$ plt.legend(loc= 4, borderpad= True)
sns.distplot(df['num_comments'])$ plt.title("Distribution - Number of comments");
false_churn_bool = [np.any([USER_PLANS_df.loc[unchurn]['status'][i] !='canceled' for i,j in enumerate(USER_PLANS_df.loc[unchurn]['scns_created']) if j==max(USER_PLANS_df.loc[unchurn]['scns_created'])]) for unchurn in churned_ix] 
df['word_count'] = df['body'].apply(lambda x: len(str(x).split(" ")))$ df[['body','word_count']].head()
data_path = 'Crimes_-_2001_to_present.csv'$ df = spark.read.csv(data_path, sep = ',', header = True)$ df.take(1)
ds1.plot('x', 'y', kind='scatter')
h2o.init()
more_info_elem = browser.find_link_by_partial_text('more info')$ more_info_elem.click()
job_requirements = pd.DataFrame(requirements)$ job_requirements$
plt.scatter(fulldf["daysaway"], fulldf["lowest_price"])$ print np.mean(fulldf["lowest_price"][fulldf["daysaway"] < 90])$ print np.mean(fulldf["lowest_price"][fulldf["daysaway"] > 90])$
(df.groupby([df['group']=='treatment', df['landing_page']=='old_page']).size().reset_index()[0].iloc[0])+(df.groupby([df['group']=='control',df['landing_page']=='new_page']).size().reset_index()[0].iloc[0])
example1_df = spark.read.json("./world_bank.json.gz")
airlines_day_unstacked = airlines_day_unstacked[(airlines_day_unstacked != 0).all(1)]
import sqlite3$ conn = sqlite3.connect('../data/database.sqlite')$ pd.read_sql_query('SELECT * FROM artists',conn)
df3.describe()
hawaii_station_df = pd.read_csv(r"\Users\Josue\Desktop\\hawaii_stations.csv")$
df=df.set_index('Date')$ bwd = df[['Store']+columns].sort_index().groupby("Store")[columns].rolling(7, min_periods=1).sum()$ fwd = df[['Store']+columns].sort_index(ascending=False).groupby('Store')[columns].rolling(7,min_periods=1).sum()
y=dataframe1['CCI_30']$ plt.plot(y)$ plt.show()
for tweet in tweepy.Cursor(api.search, q='#GES2017',$                           lang='en',since='2017-11-28',max='2017-11-30').items(200):$     csv_writter.writerow([tweet.text.encode("utf-8")])
X_copy['item_no'] = X_copy['item_no'].apply(lambda x: hash(x))
X = pivoted.fillna(0).T.values$ X.shape
scaler_M7 = StandardScaler().fit(training_X)
df_arch_clean = df_arch.copy()
weekly_videosGB = weekly_dataframe(nodesGB)$ weekly_videosGB[0].head()
xml_in_merged = pd.merge(xml_in_sample, grouped_authors_by_publication, on=['publicationKey'], how='left')
adj_close_pivot = adj_close_acq_date_modified.pivot_table(index=['Ticker', 'Acquisition Date'], values='Adj Close', aggfunc=np.max)$ adj_close_pivot.reset_index(inplace=True)$ adj_close_pivot
master_df=pd.read_csv("CitiBike_Data_for_Machine1.csv")$ master_df.head()
words_android = str(tweets_android.text)$ words_android = TextBlob(" ".join(re.findall("\w+", words_android)))$ Counter(words_android.words).most_common(10)
precipitation_data = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23")
n_estimator = 100$ rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator, n_jobs=3, verbose=2)$ rf.fit(X_train, y_train)$
gDate_vEnergy = gDateEnergy_content.count().unstack()$ gDate_vEnergy = gDate_vEnergy.fillna(0)
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$     filter(Measurement.station == most_active_id[0]).all()
np.exp(0.0408),np.exp(0.0506)
auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)$ auth.set_access_token(acc_token, acc_secret)$ api = tweepy.API(auth)
df_merge = df_clean.merge(df_twitter_clean, how='inner', left_on='tweet_id', right_on='id')$ df_merge.drop('id', axis=1, inplace=True)
files8= files8.drop('EndDate',axis=1)$ files8= files8.drop('StartDate',axis=1)$ files8.head()$
finals[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 1)].PLAYER_NAME.value_counts()
gs.score(X_test_total, y_test)
col = list(X_trainfinal.columns)$ col[2]= '1hr'$ X_trainfinal.columns = col$
import statsmodels.api as sm$ log_model = sm.Logit(df3['converted'], df3[['intercept','abs_page']])$ results = log_model.fit()
reg_traffic_with_flags = search_algo(regular_traffic,honeypot_df,['id.orig_h','src'],['ts','ts_unix'])
old_p = df2['converted'].mean()$ old_p
df2.sort_values('fatalities',ascending=False).head(5) #top 5 fatalities
bob_shopping_cart = pd.DataFrame(items, columns=['Bob'])$ bob_shopping_cart
usecols=[0,6,21,30,31,32,33,34,58,59,60]$ nitrogen = nitrogen.iloc[:,usecols]$ nitrogen.columns
df.nunique()['user_id']
norm.cdf(z_score)
df.groupby(['line']).agg([sum])$
iso_gdf.plot();
stops.loc[~mask].head(10)
!hdfs dfs -cat 32ordered_results-output/part-0000* > 32ordered_results-output.txt$ !tail 32ordered_results-output.txt
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative = 'smaller')$ z_score, p_value, norm.cdf(z_score)
string_2018 = 'extracted_data/tmin.2018.csv'
np.zeros(5)
df2.tail(2)
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page','UK','US']])$ results = logit_mod.fit()$ results.summary()
y.head()
p_tags = sample_soup.find_all("p")$ for p in p_tags:$     print(p.text)
df = pd.DataFrame({'data1' : np.random.randn(5),$                    'data2' : np.random.randn(5)})$ df
old_page_converted = np.random.binomial(1,p_old,n_old)$ old_page_converted.mean()
df_users_test = df_users.iloc[:2, :]$ df_users_test.created_on[1] = '2017-09-20'$ df_users_test
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])$ from scipy.stats import norm$ z_score,1-p_value/2$
print("Probability of individual converting:", df2.converted.mean())
df_traffic.head(n=10)
df = pd.read_csv('ab_data.csv')$ df.head(3)
yc_new4.head()
df.rename(columns={'Indicator':'Indicator_Id'}).head(2)
noise_data_2 = noise_data.join(hour)$ noise_data_2.head()
X_copy['crfa_c'] = X_copy['crfa_c'].apply(lambda x: ord(x))
p_old = df2.converted.mean()$ p_old
json_dict=req.json()$ json_dict
print("Probability of control group converting is", $       df2[df2['group']=='control']['converted'].mean())
delta=datetime(2011,1,7)-datetime.now();$ delta
data_kb = pd.DataFrame(data=[tweet.text for tweet in tweets_kb], columns=['Tweets'])$ display(data_kb.head(10))
d = df_tot.duplicated(['id'])$ df_tot['duplicate_id'] = d$ df = df_tot.loc[lambda df_tot: df_tot['duplicate_id'] == 0]$
ts = df.groupby(pd.Grouper(key='created_at', freq='D')).mean()
U,V,svd = fit_uvd(ratings_mat,100)$ print(U.shape, V.shape)
%%writefile test.py$ print("Hello World!")
status = merged_df.groupby('Funding Type')["Status"].value_counts()$ status.head(10)
df['Address1'].fillna("895 SABINE ST", inplace = True)
ISR_df = ISR_df.merge(ISR_df_agg, how='left', on='student_program')$ ISR_df.drop_duplicates('Student_Section__c', inplace=True)$ df = df.merge(ISR_df[['Section__c', 'Student__c'] + list(ISR_df_agg.columns)], on='Section__c')
data.drop(['ceil_10min'], axis=1,inplace=True)
tipsDF = tipsDF.drop(tipsDF.columns[0], axis = 1)$ tipsDF.head()
df.index.get_level_values('Subcategory').unique()$
images_clean.info()
    def __init__(self, point, x):$         super().__init__(point, x, x)
output_gnb = model_gnb.predict(test[:, 1:5])$ rowID_gnb = [TEST.rowID for TEST in test_data.itertuples()]$ result_df_gnb = pandas.DataFrame({"rowID": rowID,"cOPN": list(output_gnb)})
cust_data = pd.read_csv("DataSets/Cust_data.csv")$ cust_demo = pd.read_csv("DataSets/Cust_demo.csv")$ cust_new  = pd.read_csv("DataSets/cust_new.csv")
df_ad_airings.to_pickle('./TV_AD_AIRINGS_ENTIRE_DATASET_DATES_FIXED.pkl')
df.head(3)
fcc_nn = indexed.loc[indexed['parent_id'] == 't3_7ej943'].reset_index()$
df_release = df_release.dropna(axis=1, how='all') $ df_release.shape
sts_model.pred_table()
compared_resuts = ka.predict(test_data, results, 'Logit')$ compared_resuts = Series(compared_resuts)  # convert our model to a series for easy output
np.exp(-0.0150)
1/np.exp(results.params)
import requests$ base_url = 'http://www.mlssoccer.com/stats/season'$ response = requests.get(base_url)$
chance = cnf_matrix[1:].sum() / cnf_matrix.sum()
for tweet in df_tweets:$     print(TextBlob(tweet).sentiment)$     print(TextBlob(tweet).sentiment.subjectivity)$
sentiment_df.to_csv("sentiment.csv", encoding = "utf-8-sig", index = False)
np.all(southern_sea_level.year == northern_sea_level.year)
norm.cdf(z_score)$ norm.ppf(1-(0.05))$
daily_returns = calc_daily_returns(closes)$ daily_returns.plot(figsize=(8,6));
pd.Series([2,4,6])
df.sort_values(by=['Year'],axis=0)
df_json = pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')
d_housing=detroit_census2.drop(detroit_census2.index[:24])$ d_housing=d_housing.drop(d_housing.index[5:])$ d_housing
df_countries['country'].value_counts()
users['creation_time'] = pd.to_datetime(users['creation_time'])$ users['last_session_creation_time'] = pd.to_datetime(users['last_session_creation_time'], unit = 's')
df_features_checkcorr = df_features$ df_features_checkcorr.to_csv("data new/features.csv", encoding="utf-8", sep=",", index=False)
cityID = 'e67427d9b4126602'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Madison.append(tweet) 
autos = pd.read_csv('autos.csv',encoding='Latin-1')$
gmaps = googlemaps.Client(key='xxxxxxxxx')
db.collection_names(include_system_collections=False)
totalfare_drivers_by_city= cityfare_driver.groupby(cityfare_driver['type']).sum().reset_index()$ citylabels = totalfare_drivers_by_city['type']$ totalarea_fares = totalfare_drivers_by_city['fare']
r = requests.get('https://www.quandl.com/api/v3/datasets/OPEC/ORB.json?start_date=2013-01-01&end_date=2013-01-01&api_key='+API_KEY)
df2=pd.read_csv('ab_data.csv')
twitter_Archive.to_csv('twitter_archive_master.csv', index=False)
bmp_series = pd.Series(brand_mean_price)$ bmp_series
filtered_df = data_df.drop(labels=['id', 'fst_subject', 'fst_tutor_type'], axis=1)
trump = trump.drop(["id_str"], axis = 1)
countries = pd.get_dummies(df2['country'])$ df2 = df2.join(countries)
df_new.head()
m = Base.classes.measurement$ s = Base.classes.station$
p_diffs = np.random.binomial(n_new, CRnew, 10000)/n_new - np.random.binomial(n_old, CRold, 10000)/n_old$
! rm -rf recs2$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models2 --outdir recs2
StockData.reset_index(inplace=True)$ StockData.index.values
from nltk.tokenize import sent_tokenize$ sentence = sent_tokenize(text)$ print(sentence)
df_users.last_session_creation_time.isnull().values.ravel().sum()
significance_level = 0.05$ confidence_level = 1 - significance_level$
git_log['timestamp'].describe()
top100ratings=ratings.groupby('movieId').count().nlargest(100,'rating').reset_index()
p_new=df2.query('converted==1').count()[0]/df2.count()[0]$ p_new
df=pd.read_csv('ab_data.csv')$ df.head()
clean_appt_df[['Age','No-show']].boxplot(by='No-show')
print("{} is unique user_id are in dataset df2.".format(df2['user_id'].nunique()))
z = indx[indx == False].index.tolist()$ scratch['created_at'].iloc[z]
file1 = '/Users/gta/dev/hw-4/schools_complete.csv'$ file2 = '/Users/gta/dev/hw-4/students_complete.csv'
preprocessor = PreProcessor(titanic, copy=True)$ print("We made a copy so id titanic :  {} different from id preprocessor.data {}".format($         id(titanic),id(preprocessor.data)))
df7.to_csv('Completed_Permits_by_Month.csv')
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
logreg_countries = sm.Logit(df_new['converted'], df_new[['UK', 'US', 'intercept']])$ results = logreg_countries.fit()$ results.summary()
interest = todf( homepx * (mortgage / 100.00) )
delete_indexes = [416,562,689,748,848,897]$ twitter_archive_clean.drop(delete_indexes, inplace=True)
station_data = session.query(Stations).first()$ station_data.__dict__
df.to_csv('ab_edited.csv', index=False)
url = 'https://raw.githubusercontent.com/udacity/new-dand-advanced-china/master/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/WeRateDogs%E9%A1%B9%E7%9B%AE/image-predictions.tsv'
gender.plot.bar()$ plt.title("Students per Gender", fontdict={'fontsize': 14});$ plt.savefig('images/barplot_gender.png')
df2['user_id'].duplicated().sum()
pd.concat([msftA[:5], aaplA[:3]], axis=1, join='inner', keys=['MSFT', 'AAPL'])
session.query(measurement.station,func.count(measurement.station)).group_by(measurement.station).order_by(func.count(measurement.station).desc()).all()
df2 = df$ mismatch_index = mismatch_df.index$ df2 = df2.drop(mismatch_index)
df_visits_master = df_totalVisits_day                                                $ years_unique = df_visits_master['year'].unique()$ years_unique
combined_df = pd.merge(sales_data, returns_data, how = 'left', on = 'Order ID')$ print(combined_df)
model = sm.OLS(df_new['converted'],df_new[['intercept','UK','US']])$ result = model.fit()$ result.summary()
bigram = gensim.models.Phrases(text_list)$ bigram_text = [bigram[line] for line in text_list]
news9= ('Iraq backed a proposal from Saudi Arabia and Russia to extend output cuts for nine months, removing one of the last remaining obstacles to an agreement at the OPEC meeting in Vienna this week.'$ 'Iraq has the worst record of compliance with its pledged cuts, pumping about 80,000 more barrels of oil a day than permitted during the first quarter. If that deal gets extended to 2018, the nation will have even less incentive to comply because capacity at key southern fields is expanding and three years of fighting Islamic State has left it drowning in debt.'$
print(df.head())
sqlContext.sql("select id, borrower from world_bank limit 2").toPandas()
image_pred_df = pd.read_csv('image-predictions.tsv', sep='\t')$ image_pred_df.head(2)
re_json.get('statuses', [{}])[0] #advanced
result1.summary2()$
twitter_archive_clean[['tweet_id','in_reply_to_status_id','in_reply_to_user_id']] = twitter_archive_clean[['tweet_id','in_reply_to_status_id','in_reply_to_user_id']].astype('object')$ twitter_archive_clean['timestamp'] = pd.to_datetime(twitter_archive_clean['timestamp'])$ image_predictions_clean['tweet_id'] = image_predictions['tweet_id'].astype('object')
elon['nlp_text'] = elon.text.apply(lambda x: tokenizer.tokenize(x.lower()))$ elon.nlp_text = elon.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ elon.nlp_text = elon.nlp_text.apply(lambda x: ' '.join(x))
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')$ git_log.describe()
sh_max_df.tobs = sh_max_df.tobs.astype(float)
number_of_commits =git_log['timestamp'].size$ number_of_authors = git_log.dropna(how='any')['author'].unique().size$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df.to_csv('ab_cleaned.csv', index=False)$ df2 = pd.read_csv('ab_cleaned.csv')
! find /home/ubuntu/s3/flight_1_5/extracted/final_results -name '*.txt' -exec mv {} /home/ubuntu/s3/flight_1_5/extracted \;$ ! rmdir /home/ubuntu/s3/flight_1_5/extracted/final_results
df.date[0]
df2.groupby([df2['group']=='treatment',df2['converted']==1]).size().reset_index()[0].iloc[3]/df2.query('landing_page == "new_page"').user_id.nunique()
pca_full = PCA()$ pca_full.fit(crosstab) ## note: This takes 1:20 minutes to complete 20,000 records
brands = autos["brand"].value_counts(normalize=True)[autos["brand"].value_counts(normalize=True)>0.02].index.tolist()$ brands
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())$
conn.commit();$ conn.close();
crimes[crimes['CATEGORY'] == 'Theft']['TYPE'].value_counts()
post_id = posts.insert_one(post) #insere dados$ post_id$
data = r.json()$ print(data)$
oper_df = merged_df.loc[merged_df["Status"] == "Operating"]$ oper_df.head(5)$
df = pd.read_csv('ab_data.csv')$ df.head()
df2['user_id'].index.name == countries_df['user_id'].index.name
cityID = 'ab2f2fac83aa388d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oakland.append(tweet) 
dataFile = S3_HOME + "/sampledata.csv"$ dataFileSep = ','$ df = sqlContext.read.csv(dataFile, header=True, sep=dataFileSep, inferSchema=True, nanValue="", mode='PERMISSIVE')
df3.sort_values('timestamp').head(2), df3.sort_values('timestamp').tail(2)
feature_names = vectorizer.get_feature_names()$ class_labels = clf.classes_$ print(class_labels)
df2.groupby(["C/A", "UNIT", "SCP", "STATION", "DATE"])['DAILY_ENTRY'].apply(lambda x:x[0:1]).reset_index().head()
iris.loc[:,"Species"].cat.categories
file = open('chinese_stopwords_2.txt','rb').read().decode('utf8').split('\n')$ stop_words = list(set(file))
y_pred = model.predict(X_test)$ predictions = y_pred.tolist()
data = r.json()
json_tweets = pd.DataFrame(API_tweet_data, columns = ['tweet_id','retweet_count', 'favorite_count',$                                                'text','retweeted', 'date_time'])$ json_tweets.to_csv('tweet_json.txt', encoding = 'utf-8', index=False)
random.sample(words.items(), 10)
df3.join(highly_delayed.head(5), on='Origin', how='inner')['Carrier'].unique()
f = open(util_folder + 'investor_centrality_degree.pickle', 'rb')$ investors_high_central = pickle.load(f)$ f.close()
pd.read_sql('SELECT * FROM experiments WHERE irradiance = 700 ORDER BY temperature', conn, index_col='experiment_id')
total_user_count = ab_df['user_id'].nunique()$ converted_user_count = ab_df.query('converted == 1')['user_id'].count()$ converted_proportion = converted_user_count / total_user_count$
cells_file = 'network/recurrent_network/node_types.csv'$ rates_file = configure['output']['rates_file']$ plot_rates_popnet(cells_file,rates_file,model_keys='pop_name')
df=pd.read_csv('E:/DA_nanodegree/PROJECT4/analyzeabtestresults-2/AnalyzeABTestResults 2/ab_data.csv')$ df.head(3)
unique_users = df.nunique()['user_id']$ print(unique_users)
tobs_df = pd.DataFrame(Waihee_in_last_year, columns=['Temperature'])$ tobs_df.head(11)
df_ad_state_metro_1[df_ad_state_metro_1['candidates'] =='Donald Trump, Hillary Clinton'].head(5)
db = client.test_database$ collection = db.test_collection$ collection
opening_prices = [x for x in opening_prices if x is not None]$ print('Highest opening price - {} \nLowest opening price - {}'.format(max(opening_prices), min(opening_prices)))
from pyspark.sql import functions as F$ access_logs_df.select(min('contentSize'), avg('contentSize')).show()$ access_logs_df.describe(["contentSize"]).show()
df2.shape
ET_Combine = pd.concat([BallBerry_hour, Jarvis_hour, simResis_hour], axis=1)$ ET_Combine.columns = ['Ball-Berry', 'Jarvis', 'Simple resistance']
dfClientes.iloc[10: 20, [0, 2, 3]]
movies_df['genres'] = movies_df.genres.str.split('|')$ movies_df.head()
new_df = new_df.loc[df['ooCancelReason'] == 0]
df.fillna(method='ffill', axis=1)$
df_user['user.name'].value_counts()[:10]
tmi = indices(tmaggr, 'text', 'YearWeek')$ tmi.head()
df2['converted'].mean()
ans = df.groupby(['A','B']).sum()$ ans
print(tweet_archive[tweet_archive.tweet_id.isnull()])$ print(tweet_archive[tweet_archive.tweet_id.duplicated()])
y = df['comments']$ X = df[['title', 'age', 'subreddit']].copy(deep=True)
p_old = df2.query('converted == 1').user_id.count()/df2.user_id.count()$ print('p old convert rate: ', p_old)
df.user_id.nunique()
from quantopian.pipeline.data import Fundamentals$ exchange = Fundamentals.exchange_id.latest
ab_df2.isna().sum()
df2 = pd.read_csv('ab_updated.csv')
model.doesnt_match("man tiger woman child ".split())$
hdf5_file = h5py.File(refl_filename,'r')$ hdf5_file
mlp_df.plot(figsize=(10, 10))$ plt.show()
year7 = driver.find_elements_by_class_name('yr-button')[6]$ year7.click()
g[['blurb','launched_at','state_changed_at','days_to_change','state']].filter(lambda x: len(x) > 1).sort_values('blurb')
pres_df.dropna(subset=['location'], inplace=True)$ pres_df.shape
df_TempIrregular['timeStamp'] = pd.to_datetime(df_TempIrregular.pubTimeStamp)$
with open(os.path.join(folder_name, url.split('/')[-1]), mode = 'wb')as file:$       file.write(r.content)
df_new = df2[df2['landing_page'] == 'new_page']$ n_new = df_new.user_id.count()$ n_new
idx = pd.IndexSlice$ health_data_row.loc[idx[:, :, 'Bob'], :]  # very close to the naive implementation
pd.scatter_matrix(cust_demo.select_dtypes(include = ['number']), color='k', alpha=0.5, figsize=(12, 6))$ tight_layout()
data_compare['SA_mix'] = np.array([ analize_sentiment_multilingual(tweet) for tweet in data_compare['tweets_original'] ])
pivoted.T.shape
joined_logit_mod = sm.Logit(joined['converted'], joined[['intercept', 'US', 'CA']])$ country_result = joined_logit_mod.fit()
v_item_hub.columns[~v_item_hub.columns.isin(item_hub.columns)]
theft = crimes[crimes['PRIMARY_DESCRIPTION']=='THEFT']$ theft.head()
Base.prepare(engine, reflect=True)
idx = pd.IndexSlice$ health_data.loc[idx[:, 1], idx[:, 'HR']]
df.T
plt.hist(p_diffs);$ plt.axvline(obs_diff, c='red')
log_mod3 = sm.Logit(df3['converted'], df3[['intercept','ab_page',$                                             'evening']])$ results3 = log_mod3.fit()
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = lm.fit()
now = pd.Timestamp('now')$ now, now.tz is None
plt = r6s.num_comments.hist(bins = 1000)$ plt.set_xlim(0,50)
del p1.age$ print(p1.age)   # It will give you error
plt.scatter(X2[:, 0], X2[:,1])
station_analysis_df = tobs_df.rename(columns={0: "station", 1: "name", 2: "date", 3: "tobs"})$ station_analysis_df.head()
p6_result = p3_table.sort_values('Profit', ascending=True)$ p6_result.head()
df.info()
print('Number of rows in dataset :: ',df.shape[0])$ print('dataset information :: ')$ df.info()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK', 'ab_page', 'CA_ab_page', 'UK_ab_page']])$ results = logit_mod.fit()$ results.summary2()
directory_name = os.path.join('network', 'recurrent_network')
df.select('CRIME_CATEGORY_DESCRIPTION').distinct().show(truncate=False)
from scipy.stats import norm
df['end date'] = df.index.map(lambda x:x.end_time)$ df
(null_vals > obs_diff).mean()
analysis_df.head()$
df_times.head()$
ab_file2[((ab_file2['group'] == 'treatment') == (ab_file2['landing_page'] == 'new_page')) == False].shape[0]
telecom1 = telecom[total_rech_amt_6_7 >= amont_70_pc]$ print('Dataframe Shape: ', telecom1.shape); print_ln();
import numpy as np$ ok.grade('q01')
df_mk[df_mk.location!=''].groupby('location').size().nlargest(10)
cityID = '60e2c37980197297'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Paul.append(tweet) 
Base = automap_base()$ Base.prepare(engine, reflect= True)$ Base.classes.keys()$
[column.name for column in get_child_column_data(observations_node)]
p_new = df2.converted.sum() / df2.shape[0]$ p_new
df2['intercept'] =1$ mapper = {'treatment':1, 'control':0}$ df2['ab_page'] = df2.group.map(mapper)
trump.dtypes
corpus = [dictionary.doc2bow(text) for text in texts]$ corpora.MmCorpus.serialize('bible.mm', corpus)$ print(corpus)
df.num_comments = df.num_comments.astype(int)
z_stat, p_val = stats.ranksums(virginica, versicolor)$ print('MWW RankSum p-value %0.15f' % p_val)
watch_table = watch_table.rename(columns={'created_at':'date_starred', 'login':'user_login'})$ watch_table['date_starred'] = watch_table['date_starred'].apply(pd.to_datetime)
last_unix=last_date.timestamp() #it is a function of the time module that can convert the time to$
options_frame.head()
exploration_titanic.findcorr() # no highly numerical correlated columns 
scaler=StandardScaler()$ scaler.fit(x_train)
df.shape[0]
df2.plot.pie(subplots=True,figsize=(10,10), autopct='%.1f', legend='best')$ plt.title('IOOS Percent of total messages to the GTS')$
bds['Employment'].plot.bar()
P_old=df2['converted'].mean()$ print("convert rate for P_old under the null is {}" .format(P_old))
df_release = df_release.dropna(axis=0, subset=['actual'])$ df_release.shape
df.drop_duplicates(subset=['first_name', 'last_name'], keep='last')$
k = 7$ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train_knn,y_train_knn)$ neigh
import ta # technical analysis library: https://technical-analysis-library-in-python.readthedocs.io/en/latest/$ features['f13'] = ta.momentum.money_flow_index(prices.high, prices.low, prices.close, prices.volume, n=14, fillna=False)$ features['f14'] = features['f13'] - features['f13'].rolling(200,min_periods=20).mean()$
cityID = '28ace6b8d6dbc3af'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chula_Vista.append(tweet) 
honolulu = session.query(Station).filter(Station.name == 'HONOLULU OBSERVATORY 702.2, HI US').all()$ for city_id in honolulu:$     print("Station: {0}, Name: {1}".format(city_id.station, city_id.name))
df2 = df.copy()$ df2 = df.drop(df[(df.group == "control") & (df.landing_page == "new_page") | (df.group == "treatment") & (df.landing_page == "old_page")].index)
df2['landing_page'].value_counts()[0] / len(df2)
df1.join(df2)
sns.distplot(dfz.favorite_count, color = 'red', label = 'Favorites')$
mgxs_lib.build_library()
for i in TrainData_ForLogistic.columns:$     if i not in TestData_ForLogistic.columns:$         print i
plt.hist(taxiData.Trip_distance, bins = 100, range = [7, 10])
len(df[~(df.groups == {})])
label_and_pred = rfModel.transform(testData).select('label', 'prediction')$ label_and_pred.rdd.zipWithIndex().countByKey()$
overall_df[overall_df['News Outlet'] == 'BBC']['Compound Score'].values[0]$
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(z_score, p_value)
TestData_ForLogistic = dum.drop(['City_Code', 'Employer_Code', 'Customer_Existing_Primary_Bank_Code', 'Source',$                                  'DOB', 'Lead_Creation_Date'], axis=1)
active_devices_df = device_activity_df.loc[device_activity_df['device_id'].isin(active_devices_list)]$ a_active_devices_df = active_devices_df[active_devices_df['source']=='product-a']$ a_active_devices_df.head()
ax2.plot(anomalies.index, anomalies.score, 'ko')$ fig
json = r1.json()$ print(type(json)) #result is dictionary. yay!$ print(json)
sns.regplot(x=df['score'], y=df["comms_num"], $             line_kws={"color":"r","alpha":0.7,"lw":5});
org = "a_unique_organizations.csv"$ unique_org = pd.read_csv(org, encoding="iso-8859-1")$
df_event.occurred_at = pd.to_datetime(df_event.occurred_at)$ df_event["nweek"] = df_event.occurred_at.dt.week #strftime('%Y-%U')
reduced = merged_data.loc[merged_data['time_sec'].notnull()]$ compacted = reduced[['new_time','drone_rtk_lat','drone_rtk_lon','drone_rtk_alt','drone_lat','drone_lon','rel_alt','evnt']]$
api_copy = df_filtered.copy()$ api_copy.info()
stations_df=pd.read_csv(stations, dtype=object)
corpus = [dictionary.doc2bow(interests_token) for interests_token in interests_tokens]$ print(interests_tokens[1])$ print(corpus[1])
df2.plot.
tweets['id'].groupby(pandas.to_datetime(tweets['created_at']).dt.date).count().mean()
dpth = os.getcwd()$ dbname_sqlite = "ODM2_Example2.sqlite"$ sqlite_pth = os.path.join(dpth, os.path.pardir, "data", dbname_sqlite)
df = pd.read_csv("AAPL.csv",na_values = ["null"])$ df.dtypes
tropical = reviews.description.map(lambda r: 'tropical' in r).value_counts()$ fruity = reviews.description.map(lambda r : 'fruity' in r).value_counts()$ pd.Series([tropical[True],fruity[True]],index=['tropical','fruity'])
X = trip_data_sub.ix[:,0:trip_data_sub.shape[1]].values$ y = trip_data_sub.ix[:,trip_data_sub.shape[1]-1].values$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
xmlData['country'].value_counts()
df_counts = df.loc[(df["C/A"] == 'A002')&(df.UNIT == 'R051')&(df.SCP=='02-00-00')&(df.STATION=='59 ST'),["DAILY_ENTRIES"]]
train_ratio = 0.75$ train_size = int(samp_size * train_ratio); print(train_size)$ val_idx = list(range(train_size, len(df)))
predictions = model_rf.transform(test_data)$ evaluatorRF = MulticlassClassificationEvaluator(labelCol="trips", predictionCol="prediction", metricName="accuracy")$ accuracy = evaluatorRF.evaluate(predictions)$
df3[['c1', 'c2','c3']] = pd.get_dummies(df3['country_code'])
logreg.fit(X, y)$ new_pred_prob = logreg.predict_proba(X_new)[:, 1]$ metrics.roc_auc_score(new.popular, new_pred_prob)
reddit = praw.Reddit(client_id=keyVars[0], client_secret=keyVars[1], password=keyVars[2],$                      user_agent=keyVars[3], username=keyVars[4])
df.to_csv('GageData.csv',index=False)
news_dict_df.to_csv("Newschannel_tweets_df.csv")
for field in list(df.columns):$     df[field] = df[field].astype(str)
news_title = soup.find('div', class_='content_title').text.strip()$ news_title
from keras.preprocessing.text import Tokenizer$ import numpy as np
min_lat = weather_df["Lat"].min()//10*10$ max_lat = weather_df["Lat"].max()//10*10+15$ tick_locations = np.arange(min_lat -10, max_lat +10, 10)
def day_of_week(times):$     datetime_df = pd.DataFrame(times)
tobs_df = pd.DataFrame(tobs, columns=['Date','Tobs'])$ tobs_df
geovol = fe.std(ss)$ geovol
github_data = github_data[github_data.user_year != '2018']$ github_data = github_data[github_data.project_year != '2018']$ github_data.reset_index(inplace=True, drop=True)
average_math_score = df_students['math_score'].mean()$ average_math_score
geometry = openmc.Geometry(root_universe)
words = lc_review.split(" ")$ words_no_stop = [w for w in words if w not in stopwords.words("english")]
class_merged.dtypes
req = requests.get(url)$ html = req.text
counts = review_df.groupby('author').size()$ df2 = pd.DataFrame(counts, columns = ['size'])$ df2[df2['size']>1]
new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))$ new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)$ new_user_recommendations_rating_title_and_count_RDD.take(3)
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score
dfSummary = pd.concat([summary_all, summary_bystatus],axis=1)$ dfSummary.columns = ("1930-2017","1930-1980","1984-2017")$ dfSummary
model = gensim.models.Word2Vec(sentences, size=200)
fund_avgs = np.array(merged_df.groupby('Funding Type')["Money Raised"].median())$
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep$ lee_train_file = test_data_dir + 'lee_background.cor'$ print lee_train_file
!wget -nv -O /resources/data/PierceCricketData.csv https://ibm.box.com/shared/static/reyjo1hk43m2x79nreywwfwcdd5yi8zu.csv$ df = pd.read_csv("/resources/data/PierceCricketData.csv")$ df.head()
[list(np.ravel(item)) for item in normals] 
from bmtk.analyzer import nodes_table$ nodes_table('network/recurrent_network/nodes.h5', 'Cortical')
act_diff = df2.query("group == 'treatment'")['converted'].mean() - df2.query("group == 'control'")['converted'].mean()
daily_df.reset_index(inplace=True)$
Users_first_tran=pd.merge(users,transactions,how="left",on="UserID").groupby("UserID").min().reset_index()
(trainingData, testData) = output2.randomSplit([0.7, 0.3])
days_alive = (datetime.datetime.today() - datetime.datetime(1981, 6, 11))$ days_alive.days$ days_alive
from sklearn.preprocessing import Imputer$ trainDataVecs = Imputer().fit_transform(trainDataVecs)$ testDataVecs = Imputer().fit_transform(testDataVecs)
metrics.accuracy_score(y_test, predicted)
data_current = current.loc[(df['Date'] == '2018-05') | (df['Date'] == '2018-06')] $
station_count = session.query(Stations.id).count()$ print (f"Station Count = {station_count}")
taxdata_blocks = pd.merge(taxdata,parcel_blocks, how='left', left_on=['pin'], right_on=['PIN'])$ taxdata_blocks = taxdata_blocks.drop(['pin','PIN'],axis=1)$ taxdata_blocks = taxdata_blocks.dropna(subset=['TRACTCE10','BLOCKCE10'])
with pd.option_context('display.max_rows', 150):$     print(news_period_df.groupby(['news_collected_time']).size())
exiftool -csv -createdate -modifydate cisuabg7/cisuabg7_cycle1.MP4 cisuabg7/cisuabg7_cycle4.MP4 cisuabg7/cisuabg7_cycle6.MP4 > cisuabg7.csv
df2.drop_duplicates(subset='user_id',inplace=True)$ len(df2)
print('F : {}\nM : {}'.format(len(df.ageF), len(df.ageM)))
print('Total null values in name column: ', data.name.isnull().sum())$ print('\nInconsistent data for the missing name rows\n')$
import numpy as np$ np.random.randint(1,10,len(rng))$
df_pr = pd.DataFrame([r for r in results if r is not None])
active_devices = duplicate_check_df['device_id'].value_counts()$ active_devices_list = list(active_devices[active_devices > 1].index)$ len(active_devices_list)
sortedILI =sorted_ili.reset_index()$ sortedILI = sortedILI.rename(columns={"index": "Month"})$ sortedILI.head()
np.datetime64('2015-07-04 12:59:59.50', 'ns')
tl_2020 = pd.read_csv('input/data/trans_2020_ls.csv', encoding='utf8', index_col=0)
os.chdir('/Users/AlexandraDing/Desktop')$ pickle.dump( top_movies_list, open( "top_movies_100_year2016_list.p", "wb" ) )$
%run twitter_creds.py
station_count = session.query(Station.station).count()$ station_count
def get_client():$     client = soundcloud.Client(client_id=CLIENT_ID)$     return client
train_df = replace(combats)$ print(train_df.head(5))
! rm -rf ../../results/mrec-4-2 && mkdir -p ../../results/mrec-4-2$ ! cp -R tmp/* ../../results/mrec-4-2
df = pd.read_csv('../Data/bidsmerged_update__2_.csv') $ df.shape$
DATA_PATH = "../data/query.csv"$ df = pd.read_csv(DATA_PATH)
data.dropna().describe()
print(tweet_df.groupby(["Tweet Source"])["Tweet Source"].count())$ tweet_df.head()
fraud_country[fraud_country['class']==1][['purchase_value']].hist()
df = DataObserver.build_simply(file_path= '/Users/admin/Documents/Work/AAIHC/AAIHC-Python/Program/DataMine/Reddit/json_data/Processed_DataFrames/r-news/DF-version_2/DF_v2.json')$ cw_df = df[['ups', 'downs', 'category', 'sentiment_score', 'sentiment_magnitude', 'date_created', 'time_created']]
it_df[it_df["price"].isnull()].shape
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))
import pyspark.sql.functions as F$ df_business.createOrReplaceTempView("business")$ df_restaurants = sqlc.sql("select * from business where array_contains(categories, 'Restaurants')")
inspector =inspect(engine)$ inspector.get_table_names()
seventh.to_csv('data/word_rates.csv')
N_old = df2.query('landing_page == "old_page"')['user_id'].nunique() $ N_old 
columns_chosen = ['Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close','Adj. Volume']$ df = df[columns_chosen]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'larger')$ z_score, p_value
len([premiePair for premiePair in BDAY_PAIR_qthis.pair_age if premiePair < 0])/BDAY_PAIR_qthis.pair_age.count()
Image("../../raw_data/images/visual_studio_community.png", width=1000)
predicted_values = model_NB.predict_proba(X_test)
conn_string = "mssql+pymssql://" + '192.168.1.52' + "/" + 'naturex'$ engine = create_engine(conn_string)$ conn = engine.connect()
file = 'data/pickled/Emoticon_NB4/full_emoji_dict.obj'$ gu.pickle_obj(file, emoji_dict)
year16 = driver.find_elements_by_class_name('yr-button')[15]$ year16.click()
df2.user_id.duplicated().sum()
df = pd.get_dummies(df, columns = ['new_home', 'hail_resistant_roof'],$                     drop_first=True)
nonunique = df2.user_id.value_counts()$ nonunique[nonunique > 1].index[0]
df_tte_ondemand = df_tte[df_tte['ReservedInstance'] == 'N']$ df_tte_ondemand['UsageType'].unique()
train.FORMULE.value_counts()
my_zip =zipfile.ZipFile(dest_path,'r')$ list_names = [f.filename for f in my_zip.filelist]$ list_names$
start_of_event=datetime.datetime.strptime(start_of_event,'%Y-%m-%d') #Convert to datetime$ end_of_event=datetime.datetime.strptime(end_of_event,'%Y-%m-%d') #Convert to datetime$ location_name=location_name.replace(" ","_") #replace spaces with underscore
from sklearn.linear_model import LogisticRegression$ log_reg = LogisticRegression(C = 0.0001)$ log_reg.fit(train, train_labels)
transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv')$ transactions.head()
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(therapist_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Therapists Hours First Difference Partial Autocorrelation')
contractor_merge['contractor_bus_name'] = contractor_merge['contractor_bus_name']+" - "+contractor_merge['contractor_number'].astype(str)
calls_nocontact = calls[calls.location != '(530187.70942, 3678691.8167)']
mentors = data_final[data_final['countCollaborators'] > 4]['authorId'].unique()$ mentees = data_final[data_final['countCollaborators'] <= 4]['authorId'].unique()
sd.to_csv("media_sentiment.csv",header=True)
df = pd.read_csv(SRC_FILE)$ df['CreatedDate'] = pd.to_datetime(df['CreatedDate'], format='%m/%d/%Y %I:%M:%S %p +0000')$ df.set_index('CreatedDate', inplace=True)
n_trees = 10$ y_pred = model.named_steps['XGBClassifier'].predict(mapper.fit_transform(X_test), ntree_limit= n_trees)
live_df = k_var[k_var.state == 'live']$ live_df.to_csv('/Users/auroraleport/Documents/LePort_git/07_15_live.csv')
crypto_data.tail(5)
start_date = dt.date(2018, 3, 7)$ end_date = dt.date(2018, 3, 17)
rating_and_retweet = twitter_archive_clean.copy()$ rating_and_retweet = rating_and_retweet.sort_values(by='score')$ rating_and_retweet = rating_and_retweet[:int(len(rating_and_retweet)*0.9)]
price_dict = price_data.to_dict()
np.shape(prec_us_full)
pred_labels = lr.predict(test_data)$ print("Training set score: {:.2f}".format(lr.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(lr.score(test_data, test_labels)))
df_new['country'].unique()
tmp1 = (x > 0.5)$ tmp2 = (y < 0.5)$ mask = tmp1 & tmp2
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep ='\s+', na_values=['.'])$ df.head(5)
df.describe()
psy_df4 = PDSQ.merge(psy_df3, on='subjectkey', how='right') # I want to keep all Ss from psy_df$ psy_df4.shape
n_new, n_old = df2['landing_page'].value_counts()$ print("new:", n_new, "\nold:", n_old)
metadata_documents = [x.name() for x in query.element("asset/meta").elements() if x.name() != "mf-revision-history"]$ metadata_documents
df_ad_state_metro_1['sponsors'].value_counts()
style.use('ggplot')
users_converted = df.query('converted == 1').count()[0] / (df.query('converted == 1').count()[0] + df.query('converted == 0').count()[0])$ users_converted_percentage = users_converted * 100$ print('The proporation of converted users are ' + str(users_converted_percentage) + '%')
for col in merge_df.columns: $     print(merge_df[col].describe())
df_all_columns.head()
X=final_df.drop(['max', 'mean','std','sum'], axis=1)$ print("number of samples are= "+str(X.shape[0]) +" with number of features= "+str(X.shape[1]))
weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')$ by_time = data.groupby([weekend, data.index.time]).mean()
autos["fuel_type"].unique()
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))$
ks_name_success = ks_name_success.sort_values(by = ['counts'], ascending = False)$ ks_name_success.head(10)
texts = df[df['section_text'].str.contains('fees')]['section_text'].values[0:5]
twitter_daily_df = twitter_daily_df.join(combined_text, ["Day","Company"]).orderBy('Day','Company')
mlp_fp = 'data/richmond_median_list_prices.csv'$ mlp_df = pd.read_csv(mlp_fp)$ mlp_df.head()
(p_diffs > obs_diff).mean()$
if not os.path.exists('new_data_files'):$     os.mkdir('new_data_files')$ records2.to_csv('new_data_files/Q3A.csv')
tlen = pd.Series(data = data['len'].values, index = data['Date'])$ tfav = pd.Series(data = data['Likes'].values, index = data['Date'])$ tret = pd.Series(data = data['RTs'].values, index = data['Date'])$
df.info()
grades.plot('ATAR', 'Mark', kind='scatter')
prcp_analysis_df.set_index(['Date'], inplace=True)$ prcp_analysis_df.head()
uk, us, abs_pg = 1/np.exp(0.0506), 1/np.exp(0.0408), 1/np.exp(-0.0150)$ uk,us, abs_pg
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
tweets.dtypes
sentence = 'the sun rises in the east'$ sentence.find('east')
with open('total_review_apps_eng_lower.pickle', 'rb') as d:$     total_review_apps_eng_lower = pickle.load(d)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger')$ z_score,p_value
data_file='data/airports.csv'
print(train["comment_text"][0])$ print(example1.get_text())$
S_1dRichards.decision_obj.groundwatr.options, S_1dRichards.decision_obj.groundwatr.value
abc = abc.reset_index(level=[0,1])$ abc.columns
print(df['Site Fill'].value_counts(dropna=False))
def deleteTable(projectId, datasetId, tableId):$     service.tables().delete(projectId=projectId, datasetId=datasetId, tableId=tableId).execute()
train_data_df ['created_year'] = train_data_df ['created'].apply(lambda ts:ts_interval_to_years(ts))
from pyspark.sql import SQLContext$ sqlContext = SQLContext(sc)
df_CLEAN1A = pd.read_csv(url_CLEAN1A,sep=',')$ df_CLEAN1B = pd.read_csv(url_CLEAN1B,sep=',')$ df_CLEAN1C = pd.read_csv(url_CLEAN1C,sep=',')
index.save('trump.index')$ index = similarities.MatrixSimilarity.load('trump.index')
for c in ['date_listed', 'last_review', 'first_review']:$     airbnb_df[c] = pd.to_datetime(airbnb_df[c])$
best_worst = data_df.loc[(stars==5) | (stars==1), :]$ best_worst.head()
news_df=pd.DataFrame({'Source Account':[],'Text':[],'Date':[],'Compound Score':[],'Positive Score':[],'Neutral Score':[],'Negative Score':[]})$ news_df.head()
df2_control = df2.query("group == 'control'")$ convereted_rate_old = round(df2_control.converted.mean(),4)$ print(convereted_rate_old)
series + pandas.Series({'a': 2})
test = old_test.append(new_test).reset_index()$
new_page_converted = np.random.binomial(1, p_new, n_new)$ new_page_converted
data = allocate_equities(allocs=[  0.25 , 0.25,  0.25 , 0.25],dates=dates)$ data.plot(),$ plt.show()$
df['y'].plot.box(notch=True)
model = word2vec.Word2Vec.load("200features_30minwords_10context")
p_new = (df2['converted']).mean()$ print(p_new)
sns.heatmap(df.corr())$ plt.show()
newsorgs_bar = mean_newsorg_sentiment["News Organization"]$ compound_bar = mean_newsorg_sentiment["Compound"]$ x_axis = np.arange(0, len(compound_bar), 1)
hit_tracker_grouped_df.to_csv("Desktop/Project-2/hit_tracker.csv", index=False, header=True)
daily_change = [daily[2]-daily[3] for daily in json_data['dataset_data']['data'] if daily[2:3] != None]$ largest_change = str(round(max(daily_change), 2))$ print('The largest change in any one day was $' + largest_change + ' in 2017.')$
commmon_intervention_train = intervention_train.index.intersection(intervention_history.index)
!pip install -q xgboost==0.4a30$ import xgboost
sql="SELECT * FROM %s.%s LIMIT 3" % (schema, data_table)$ HTML(hc.sql(sql).toPandas().to_html())
dataframe.head(20)
print "https://twitter.com/AirbnbHelp/status/{}".format(tweets_df['09/20/2017'].iloc[0]['twitter_id'])
hours.shape$
new_page_converted= np.random.binomial(1, p=p_new, size=n_new) $ new_page_converted
for i1 in range(2):$     writer = pd.ExcelWriter('Bitfinex/Bitfinex_2017.xlsx')$     df_table_Bitfinex.to_excel(writer, 'Bitfinex_2017', index = False, header = True)
fixed.head()
df['AQI Category'].cat.categories = ['Good', 'Moderate', 'Unhealthy', 'Somewhat Unhealthy']$ df['AQI Category'] = df['AQI Category'].cat.add_categories(['Very Unhealthy'])$ df['AQI Category'] = df['AQI Category'].cat.reorder_categories(['Good', 'Moderate', 'Somewhat Unhealthy', 'Unhealthy', 'Very Unhealthy'], ordered=True)
sl['two_measures'] = np.where((sl.mindate!=sl.maxdate),1,0)
print 'Total negative amount is: ', df[df.amount < 0].amount.sum()
rep = re.findall(r'[-]{2} ([\d]{4}-[\d]{2}-[\d]{2}) [a-z]{3} (.[\d]).+ = ([\d]+)', data)$ rep
df2 = df2.join(df_countries.set_index('user_id'), on='user_id')$ df2.head()
from statsmodels.tsa.arima_model import ARIMA$ model_713 = ARIMA(dta_713, (2, 2, 0)).fit() $ model_713.forecast(5)[:1] 
relevant_data['Invitee Name'].value_counts()
recipes.name[selection.index]
clients = pd.read_csv('data/clients.csv', parse_dates = ['joined'])$ loans = pd.read_csv('data/loans.csv', parse_dates = ['loan_start', 'loan_end'])$ payments = pd.read_csv('data/payments.csv', parse_dates = ['payment_date'])
all_sets.shape
tfav.plot(figsize = (16,4), label = "Likes", legend = True)$ tret.plot(figsize = (16,4), label = "Retweets", legend = True);  $
ab_dataframe['converted'].mean()
t3.tweet_id=t3.tweet_id.astype('str')
new_logistics = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']])$ new_model = new_logistics.fit()$ new_model.summary()
df_new.head()
tweet_df_polarity = tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
cercanasAfuerteApacheEntre75Y100mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 75) & (cercanasAfuerteApache['surface_total_in_m2'] < 100)]$ cercanasAfuerteApacheEntre75Y100mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre75Y100mts.apply(descripcionDistancia2, axis = 1)$ cercanasAfuerteApacheEntre75Y100mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
print('Total minutes over time series: {}'.format(df_btc.shape[0]))$ print('% minutes in time series with {} trades: {}'.format('USD-BTC', df_btc['USD-BTC_low'].notna().sum()/df_btc.shape[0] ))$
bloomfield_pothole_data['REQUEST_ID'].resample("M").count().plot(figsize=(10,6))
data.head()
periods = 31 * 24$ hourly = Series(np.arange(0,periods),pd.date_range('08-01-2014',freq="2H",periods=periods))$ hourly
df_new.country.unique()
year_info = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date.between('2016-08-23', '2017-08-23')).all()$ year_info_df = pd.DataFrame(year_info)$ year_info_df.set_index("date").head()$
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY)
output= "CREATE TEMPORARY TABLE ABC AS select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc"$ cursor.execute(output)$
df['is_lasvegas'] = df['city'].map(lambda x: 1 if x == "Las Vegas" else 0)
data_issues=pd.read_json('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-issues.json',lines=True)
print(r_test.json())
fire_pred = log_mod.predict_proba(climate_vars)
cond_1 = '(group == "treatment" and landing_page != "new_page")'$ cond_2 = '(group != "treatment" and landing_page == "new_page")'$ df.query(cond_1 + ' or ' + cond_2).shape[0]
df1 = add_percentiles(df)$ df1.head()
df['datetime'] = pd.to_datetime(df['datetime'],format=('%Y-%m-%d'))$ df.dtypes
data4.to_file('Twitters_FSGutierres.shp', driver='ESRI Shapefile')
p_old = df2.converted.mean()$ p_old
from scipy.stats import norm$ print(norm.cdf(z_score)) # 0.905058312759$ print(norm.ppf(1-(0.05))) # 1.64485362695$
new_page_converted = np.random.choice([1, 0], size=len(df2_treatment.index), p=[df2.converted.mean(), (1-(df2.converted.mean()))])
tweetnet = nx.read_gpickle('twitter_graph_data/bigraph_full_pickle')
e.instance_method
raw_data = raw_data.loc[raw_data['date'] >= pd.to_datetime(start_date)]$ raw_data = raw_data.loc[raw_data['date'] <= pd.to_datetime(end_date)]
len(df)$ len(df[df['processed2'].str.count(r'\w+') >= 3])$ df = df[df['processed2'].str.count(r'\w+') >= 3]
Measurement = Base.classes.measurement$ Station = Base.classes.station
telemetry.dtypes
new_page_converted.mean() - old_page_converted.mean()
data_df.drop(['review_id', 'user_id', 'date', 'cool', 'funny', 'useful', 'type'], axis=1, inplace=True)
p_treatment = df2.query('group=="treatment"').converted.mean()$ p_treatment
punctuations = list(string.punctuation)$ df['body_tokens'] = df['body_tokens'].apply(lambda x: [word for word in x if word not in punctuations])$ print(df['body_tokens'])
print(data.first_name + " " +data.last_name)
cityID = '3f3f6803f117606d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lubbock.append(tweet) 
len(cats_out['Animal ID'].unique())
data = grouped_publications_by_author.copy()
A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])$ A
autos.head()
data['SA'] = np.array([analyse_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
post_gen_paired_cameras_missing_from_join = np.setdiff1d(BPAIRED_GEN['shopify_order_id'],ORDER_BPAIR_POSTGEN['shopify_order_id'].astype(str))
df_ad_airings_2.dtypes$
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \$       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY$ request = requests.get(url)
date.
cat_sz = [(c, len(full_data[c].unique())) for c in cats]
cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name='dot_311'")$
engine = create_engine('mysql+pymysql://root:@localhost/sawi_tweets?charset=utf8mb4', encoding='utf8', echo = False)$ df = pd.read_sql_table("sawi_tweets_historical", con = engine)
treatment_df = df2.query('group == "treatment"')$ treatment_pro = treatment_df.query('converted == 1').user_id.nunique() / treatment_df.user_id.nunique()$ treatment_pro
date_list = []$ for day in days:$     date_list.append(day.strftime("%m-%d"))
df_vow.head()
df=df.drop('SEC filings',axis=1)
(dl_creations["non_autoconfirmed_creations"].sum() / dl_creations["creations"].sum()) * 100
jail_census.groupby('Gender')['Age at Booking'].describe()
P_new = df2.converted.mean()$ print("The convert rate for p-new under the null is {}.".format(P_new))
df1=files3.pivot_table(index='candidateid', columns='dimensiontype', values='finalscore')$ df1new = pd.DataFrame(df1.to_records())$ df1new.head()
unique_users = df['user_id'].nunique()$ unique_users
prcp_results_df = pd.DataFrame(prcp_results, columns=['date', 'precipitation'])$ prcp_results_df.set_index('date', inplace=True)$ prcp_results_df.head()
s = pd.Series(todays_datetimes)$ s.diff().mean()
twitter_archive_clean['dog_type'] = twitter_archive_clean['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True)
pipe_dat = df_pipe.copy()
cancelled = flights[flights.cancelled == 1].copy()$ not_cancelled = flights[flights.cancelled == 0].copy()
sns.violinplot(x=df['liked'],y=df['profile_popularity'],data=df,whis=np.inf)$
rhum_long_df = pd.melt(rhum_wide_df, id_vars = ['grid_id', 'glon', 'glat'],$                       var_name = "date", value_name = "rhum_perc")$ rhum_long_df.head()
np.intersect1d(top_10_KBest, top_10_elnet)
requests.get(wikipedia_content_analysis)
sum(df2['user_id'].duplicated())
%time train = pd.read_csv("../assets/trainaa")$ train.head(1)
p_new = df2['converted'].mean()$ p_new
body = pd.get_dummies(auto_new.Body_Type)$ body.head()
df = pd.read_csv('ab_data.csv')$ df.head()
df = pd.DataFrame([x._json for x in tweets])[['text', 'created_at', 'user']]$ df['label'] = df.user.map(lambda x: x.get('name'))$ df.head()
df2.dtypes
df_sched = df_sched[~(df_sched.Initiation < 0)].copy()
print('The probability of conversion:', df2['converted'].mean())
plt.hist(p_diffs);
(obs_diff < p_diffs).mean()
df.dealowner.unique()
import os$ os._exit(0)
ab_groups = pickle.load(open("ab_groups.p", "rb"))
new_comments_df = pd.read_csv('input/test.csv') # Replace 'test.csv' with your dataset$ X_test = test["comment_text"].str.lower() # Replace "comment_text" with the label of the column containing your comments
df_new[["CA", "UK", "US"]] = pd.get_dummies(df_new["country"])$ df_new.head()
n_new = df2.query('landing_page == "new_page"').user_id.count()$ n_new
data.head()
trump_tweets=pd.read_csv('Resource_CSVs/Twitter_RawData.csv')$ type(trump_tweets)$ trump_tweets
dataset_rows = df.shape[0]$ print(dataset_rows)
print("Action space:", env.action_space)$ print("Action space samples:")$ print(np.array([env.action_space.sample() for i in range(10)]))
lr_df = df2.copy()$ lr_df.head()
start_date = datetime.datetime(year=2017,month=1,day=1)$ stop_date = datetime.datetime(year=2017,month=8,day=23)
last_commit_timestamp = git_log[git_log['timestamp'] $                                 < pd.to_datetime('today')].sort_values('timestamp', ascending=False).head(1)$ last_commit_timestamp
red_4['num_comments'].max()
avg_att_2015_BOS = nba_df.loc[(nba_df["Season"] == 2015) & (nba_df["Team"] == "BOS"), "Home.Attendance"].mean()$ round(avg_att_2015_BOS, 0)
archive_clean['timestamp'] =pd.to_datetime(archive_clean.timestamp)
plt.rcParams['figure.figsize'] = [16,4]$ plt.plot(pd.to_datetime(mydf1.datetime),mydf1.fuelVoltage,'g.', markersize = 2);$
model = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']])$ results = model.fit()$ results.summary()
df = pd.read_csv('ab_data.csv')$ df.head()
df3 = df3.drop('CA', axis=1)$ df3.head()
walkmin = walk.resample("1Min")
df.to_excel("../../data/stocks_msft.xlsx", sheet_name='MSFT')
df_3_test = pd.read_csv('Medicare_Hospital_Spending_by_Claim.csv')$ df_4_test = df_3_test.drop(df_3_test.columns[[11, 12]], 1)$
datesStr=dates.strftime('%Y-%m-%d')
fcc_nn.tail()
pm_data.dropna(inplace = True)$ pm_data.shape
city_ride_data = clean_combined_city_df.groupby(["type"]).sum()["fare"]$ city_ride_data.head()$
accuracy = metrics.accuracy_score(predictions,y_test)$ print ("Accuracy : %s" % "{0:.3%}".format(accuracy))$
df_csv = pd.read_csv(datafile)$ df_csv.head()$
df2 = ab_dataframe.query(' (group=="treatment" & landing_page == "new_page") | ( group =="control" & landing_page == "old_page") ')$ print(df2.shape)$
day_change = (x[2] - x[3] for x in data_table if x[2] is not None and x[3] is not None)$ print('Largest Day Change: {:.2f}'.format(max(day_change)))
fig = sns.countplot(x = 'weekday', hue = 'subscription_type', data = trip_data)$ plt.xticks(np.arange(0,7),['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
test_count_vect = CountVectorizer(vocabulary=significant_features, tokenizer=lambda x: x.split(','))$ X_test_feature_counts = count_vect.fit_transform(test['feature_list'])$ X_test_feature_counts
import qgrid$ qgrid.set_grid_option('forceFitColumns', False)$ qgrid.set_grid_option('editable', False)
print("Mean squared error: %.2f"% mean_squared_error(y_test, y_pred))
results['HydrologicEvent'].value_counts()
S_1dRichards.forcing_list.filename
df.iloc[:4]
diff_df = df_df.reset_index(drop = False)$ diff_df
listings_unique_ids = list(listings['id'].unique())$ len(listings_unique_ids)$
from statsmodels.tsa.arima_model import ARIMA$ model_701 = ARIMA(dta_701, (4, 0, 0)).fit() $ model_701.forecast(5)[:1] 
git_log.head(10)
df3.groupby('created_at').count()['tweet']
news_titles_sr = news_period_df.resample('D', on='news_collected_time')['news_title'].apply(lambda x: '\n'.join(x))
df['MatchTimeC'] = pd.to_datetime(df.MatchTime)$ df['LastUpdatedC'] = pd.to_datetime(df.LastUpdated)$ df = df.drop_duplicates(df.columns.difference(['_id']))
df_master=pd.read_csv('cleaned data/final_master.csv')
series1.cov(series2)
logistic = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = logistic.fit()
tm_2040 = pd.read_csv('input/data/trans_2040_m.csv', encoding='utf8', index_col=0)
df2.groupby(['group'],as_index=False).mean()
xtrain,xtest,ytrain,ytest=cross_validation.train_test_split(X,Y,test_size=0.2)
import seaborn as sns$ sns.set(style="darkgrid")$ ax = sns.countplot(x="AGE_groups", data=df_CLEAN1A)$
archive_clean.to_csv('archive_clean.csv')$ images_clean.to_csv('images_clean.csv')$ popularity_clean.to_csv('popularity_clean.csv')
print  [v for v in pre_analyzeable['prior_lab'].values if str(v) != 'nan']
woba_to_date = plate_appearances.sort_values(['batter','game_date','at_bat_number'], ascending=True).groupby('batter')['woba_value'].expanding(min_periods=1).mean()
xml_in_sample1.shape
dfM['COUNT_s1'] = dfM['COUNT'].shift(1)$ dfM['diff_1'] = df.COUNT - dfM.COUNT_s1$ dfM.head(5)
sgm_bgp_100yr_run(L0 = 1000, E0 = 1, Delta_n = 0.02)
companies = data['Company'].value_counts()$ print('Number of Companies compained about: {}'.format(len(companies)))
from dotce.visual_roc import roc_curve, precision_recall_curve
odometer_IQR = autos["odometer_km"].quantile(.75) - autos["odometer_km"].quantile(.25)$ odometer_LF = autos["odometer_km"].quantile(.25) - (1.5 * odometer_IQR)$ print(odometer_LF)
print('The number of unique users:', df['user_id'].nunique())
df2 = df.drop(df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page == "new_page") or (group == "control" and landing_page != "old_page") or (group != "control" and landing_page == "old_page")').index)$ df2.shape[0]
print("Probability of control group converting:", $       ab_file2[ab_file2['group']=='control']['converted'].mean())
twitter_Archive.head()
session.query(Measurement.station).distinct().count()
type(twitter_archive_df_clean.timestamp[0])
pres_df['location'][0].split(',')
CRnew = df2.converted.sum()/df2.count()[0]$ CRnew
chunker = ConsecutiveNPChunker(train_trees)$ print(chunker.evaluate(valid_trees))
df['Approximate Age at Designation'] = df['membership_date'].apply(lambda x: x.year)-df['yob']
news_sentiment_df.to_csv('news_sentiments.csv')
archive_clean.info()
df = pd.read_parquet('training_data.parquet')
df3['intercept'] = pd.Series(np.zeros(len(df3)), index = df3.index)$ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index = df3.index)
ratings.sample(5)
plt.hist(np.log(threeoneone_census_complaints[threeoneone_census_complaints['complaint_density']>0]['complaint_density']+1),bins=100)$ plt.show()
combined_factor_df['intercept'] = 1$ logistic_model = sm.Logit(combined_factor_df['converted'], combined_df[['intercept','US_ab', 'CA_ab']])$ result = logistic_model.fit()
reviews_w_sentiment['sentiment_score_scaled'] = reviews_w_sentiment['sentiment_score']*5 + 5 $ reviews_w_sentiment[['sentiment_score_scaled', 'sentiment_score']].describe() 
r.json()['dataset']['data'][0]$
style_bw.head(5)
join_a.count()
grouped.get_group('MSFT')
a=df2.nunique()['user_id']$ print("There are "+str(a)+" unique users in the new dataset")
t = input("Input a template: ")$ t = json.loads(t)$ DBfindByTemplate(t)$
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)$ results.summary()
forecast_set = clf.predict(X_lately) #selecting only the last few rows to predict$ print(forecast_set)$ df['Forecast'] = np.nan #Adding new column to store all the values
theft.iloc[4]
daily_transaction = daily_transaction.select(col("date").alias("date"), col("avg(trans_amt)").alias("daily_rev_avg"), $                                      col("count(uuid)").alias("count_trans"))$ daily_transaction.show()
import numpy as np$ ep_data = np.loadtxt('data/XO1_wl_transit_FLUX.txt')
ndf.gender = ndf.gender.replace(2, "Female")$ ndf.gender = ndf.gender.replace(1, "Male")$ ndf.gender = ndf.gender.replace(0, "Unknown")
new_page_converted.mean() - old_page_converted.mean()
df = pd.DataFrame({"date": date_list, "tmin":tmin, "tavg": tavg, "tmax": tmax})
dat_hcad['blk_lower'] = dat_hcad['0'] - dat_hcad['0'] % 100$ dat_hcad['blk_upper'] = dat_hcad['blk_lower'] + 99$ dat_hcad['blk_range'] = dat_hcad['blk_lower'].map(str)+'-'+dat_hcad['blk_upper'].map(str)+' '+dat_hcad['COMMERCE'].map(str)
df2 = df$ drop_rows = df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].index.values
tips.groupby(["sex","size"]).mean().loc[:,"total_bill"].loc["Female",3:5]
myTimeZone = pytz.timezone('US/Eastern')$ itemTable["Date"] = itemTable["Date"].apply(localize_time, args=(myTimeZone,))
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])$
nold = df2.query('landing_page == "old_page"').count()[0]$ print ("The population of Oldpage is : {}".format(nold))
url = "https://mars.nasa.gov/news/"
taxi_hourly_df.loc[taxi_hourly_df["missing_dt"] == True, :].shape
y_pred = fit5.predict(X_test)
trips_data['bike_id'].hist(bins=20)$ plt.show()
df.groupby('brand').count().reset_index().sort('price',ascending=False)
instance.updateParameters(cannull,ranges,tests)$ instance.testassumptions()
df_merge['budget_binned'] = pd.cut(df_merge['per_student_budget'], bins, labels = bin_names)$ df_merge.head()
def remove_punctuation(text):$     exclude = set(string.punctuation)$     return "".join(ch for ch in text if ch not in exclude)
tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))$ tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0])$ print(tips.head())
high_sta_obs = session.query(measurements.station, func.count(measurements.stations)).\$                 group_by(measurements.station).order_by(measurements.tobs).all()$ high_sta_obs
sub_dataset[ sub_dataset['WordCount'] == 0 ]['WordCount'].count()
df['full_text'] = df['full_text'].apply(lambda x: x.replace('\r', ' '))$ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\n', ' '))$ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\t', ' '))
r_2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY+'&start_date=2017-01-01&end_date=2017-12-31')$ data_2017 = r_2017.json()
Xs = pd.get_dummies(df.subreddit, drop_first = True)
xml_in.head(5)
n_old = df2[df2['group'] == 'control']['user_id'].count()$ print('Elements of treatment group n_old: ',n_old)
mod_model.scen2xls(version=None)
def listening_longevity(x):$     x['listening_longevity'] = (x.iloc[-1].date - x.iloc[0].date).days$     return x
pokemon['Total']= pokemon['HP']+pokemon['Attack']+pokemon['Defense']+pokemon['Sp. Atk']+pokemon['Sp. Def']+pokemon['Speed']$ pokemon.head()
export_path=cwd+'\\ca_simu_from_python.csv'$ ca_de.to_csv(export_path, index=False)
genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId'])$ genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ genreTable.head()
from patsy import dmatrices$ from statsmodels.stats.outliers_influence import variance_inflation_factor$ y, X = dmatrices('converted ~ intercept + ab_page + CA + UK', df2, return_type='dataframe')
data1 = mergeairflu['PASSENGERS_NET']/1000$ data2 = mergeairflu['ILITOTAL']$ mergeairflu.dtypes
hawaii_measurement_df.head(10)
np.exp(results.params)
import numpy as np$ x = np.array([0,1,2,3,4,5,6,7])$ x.dtype
n_user_days = pax_raw[['seqn', 'paxday']].drop_duplicates().groupby('seqn').size()
df2.shape[0]
poll_data = "approval data clean values only.csv"$ poll_df = pd.read_csv(poll_data, encoding = "ISO-8859-1")$ poll_df.head()
df_twitter_enhanced_clean = df_twitter_enhanced.copy()$ df_image_predictions_clean = df_image_predictions.copy()$ df_tweet_api_clean = df_tweet_api.copy()
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']])$ res = lm.fit()$ res.summary2()
segments = pd.read_csv("transit_segments.csv", parse_dates=['st_time', 'end_time'])$
engine = create_engine("sqlite:////Users/daryarudych/Desktop/repos/SQL-Python/hawaii.sqlite")
n_old = df2['group'].value_counts()[1]$ n_old$
datacounts = pd.read_sql_query(query,engine)
nsw_bb = Polygon([[-35.52052802079999,140.999279200001],[-28.1570199879999,140.999279200001],$                   [-28.1570199879999,159.105444163417],[-37.5052802079999,159.105444163417]])
lines = [line.rstrip() for line in tqdm(open('yelp_academic_dataset_review.json')) if (random.random() >= 0.85 and (json.loads(line)['business_id'] in USids))]
stock['target'] = stock.daily_gain.shift(-1)
df[((df.landing_page=='new_page')==(df.group=='treatment'))== False].shape[0]
transfer_duplicates = BTC.loc[BTC['Smoother'].isnull()==False,['Year','Month','Day','Smoother']];
keys_0611 = keys.copy()$ keys_0611.to_excel(cwd + '\\ELMS-DE backup\\keys_0611.xlsx', index=False)
df['consolidated_bottle_ml'].fillna('Other_ml', inplace=True)
def trip_start_date(x):$     return re.search(r'(\d{4})-(\d{2})-(\d{2})', x).group(0)
n_new = df2[df2.group == "treatment"].count()[0]$ print("The population of user under treatment group: %d" %n_new)
launch_df = pd.DataFrame(launch_events, columns=launch_columns)$ launch_df.head()
old_page_sim = (np.random.choice([1, 0], size=n_old, p=[p_mean/100, (1-p_mean/100)])).mean()$ output2 = round(old_page_sim, 4) * 100$ print(output2,'%')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY)
twitter_archive_clean = twitter_archive_clean_with_stage.append(twitter_archive_clean_without_stage)$
con,cur = helper.connect_to_db()$
df['MeanFlow_mps'] = df['MeanFlow_cfs'] * 0.028316847
subwaydf['DESC'].value_counts()
print example1_df.printSchema()
n_old = df2.query('group == "control"')['user_id'].count()$ n_old = int(n_old)$ n_old
local.get_dataset(post_process_info["DatasetId"])
itemTable["Time"] = itemTable["Content"].map(time_effort)
connection = sqlite3.connect(':memory:')$ cursor = connection.cursor()$
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])$
prcp_df = pd.DataFrame(prcp_data, columns=['Date', 'Precipitation'])$ prcp_df.set_index('Date', inplace=True) # Set the index by date
pd.merge(left=users,right=sessions,how="inner",left_on=['UserID','Registered'],right_on=['UserID','SessionDate'])
hru_rootDistExp = rootDistExp.open_netcdf()
y = sales_2015[['store_number']].rename(columns={'sum':'store_number'})$ y['sale_dollars'] = sales_2015['sale_dollars']$ y.head()
df.dropna(axis="columns")
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.groupby('country')['converted'].mean()
df2['landing_page'].value_counts()$
X = vectorizer.fit_transform(clean_train_reviews)
manager.image_df[manager.image_df['filename'] == 'image_sitka_spruce_71.png']$
ts_count_ks = df_ks_type.groupby(['date','type'])['text'].count()$ ts_count_ks.to_csv('daily_count_ks.csv')$ ts_count_ks.head(10)
n_new = df2_treatment.shape[0]$ n_new
df_time = df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean()$ df_time
grid_id = np.arange(1, 1535,1)$ grid_id_array = np.reshape(grid_id, (26,59))$ grid_id_flat = grid_id_array.flatten()
min(ORDER_BPAIR_POSTGEN['order_number'].astype(int))
total_stations = session.query(Station).group_by(Station.station).count()$ print('Number of stations: ' + str(total_stations))
df_ad_airings_5['subjects'].isnull().sum()
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count()$ commits_per_year.rename(columns={'author': 'num_commits'}, inplace=True)$ commits_per_year.head(5)
req_test.text
X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75)$ y_train = np.log(y_train + 1)$ y_test = np.log(y_test + 1)
giss_temp = giss_temp.set_index("Year")$ giss_temp.head()
dfPre = df.loc['1930-01-01':'1979-12-31']$ dfPost = df.loc['1984-01-01':'2017-12-31']
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print("The value for the z_score is - {}".format(z_score))$ print("The value for the p_value is - {}".format(p_value))
dfClientes.iloc[20, :]
plt.title("homework")$ plt.xlabel("Date")$ plt.ylabel("Prcp")
rf.score(X_train, y_train)
plate_appearances.loc[plate_appearances.batter_name=='ryan howard',].head()
kick_data_state = kick_data_state[kick_data_state.state != 'canceled'][kick_data_state['launched_at'].dt.year >= 2014]
df_user_count = df_stars.groupby('user_id').size()$ print(df_user_count )
(df['converted'].value_counts()[1].sum()) / ((df['converted'].value_counts()[0]) + (df['converted'].value_counts()[1]))
file = 'https://assets.datacamp.com/production/course_2023/datasets/dob_job_application_filings_subset.csv'$ df = pd.read_csv(file)$ print(df.head())
df_new.hist(figsize=(10,5))$ plt.show()$
new_page_converted=np.random.binomial(n_new,p_new)
url  = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv"$ iris = h2o.import_file(url)
Station_id = session.query(Station.station).filter(Station.name == 'WAIHEE 837.5, HI US').all()$ Station_id
nlp = spacy.load('en')$ op_ed_articles['full_text_tokenized'] = op_ed_articles['full_text'].apply(lambda x: nlp(x))$
train_prices = train.loc[:, 'price']
rng_dateutil = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='dateutil/Europe/London')
t1.tweet_id =t1.tweet_id.astype(str)
df_email.occurred_at = pd.to_datetime(df_email.occurred_at)$ df_email["nweek"] = df_email.occurred_at.dt.week #strftime('%Y-%U')
print('Day : {}\nNight : {}'.format(len(df2.tripDay), len(df2.tripNight)))
new_df_left['parcelid'].sort_values()$ new_df_left[new_df_left['parcelid'].duplicated(keep=False)].reset_index()
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
norm.ppf(1-(0.05))
y = w * x + b$ loss = K.mean(K.square(y-target))
irl.drop(['bill', 'congress'], axis=1, inplace=True)$ irl.head()
numUsers = firstWeekUserMerged[['userid']].drop_duplicates().count()$ print("The number of users is :",int(numUsers))
dataset.columns = dataset.columns.str.replace('.', '_')
cfs_df.head()
sns.barplot(data=df.groupby('education').agg({'applicant_id':lambda x:len(set(x))}).reset_index(),$             x='education',y='applicant_id')
logit_stats = sm.Logit(df['converted'],df[['intercept','treatment']])
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)
df.describe()
logit = sm.Logit(df3['converted'],df3[['intercept','ab_page']]) $ r = logit.fit()
%matplotlib inline$ import seaborn as sns  $ sns.set_style('darkgrid')
logreg = LogisticRegression()$ logreg.fit(X_train_all, y_train)$ logreg.score(X_test_all, y_test)
lm=sm.Logit(df_new['converted'],df_new[['intercept','new','CA','UK']])$ results=lm.fit()$ results.summary()
!hdfs dfs -cat /user/koza/hw3/3.2/issues/frequencies_part3/* | sort -k1,1nr -k3,3 | head -n 20
new_model = gensim.models.Word2Vec(min_count=1)  # an empty model, no training$ new_model.build_vocab(sentences)                 # can be a non-repeatable, 1-pass generator     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)                       $
we_rate_dogs.shape[0] - tweet_fav_counts.shape[0]
pd.get_dummies(train.raceeth).head(5)
original = pd.read_csv("data/train.csv")$ print "Data read successfully!"
blight_inc = pd.read_csv(processed_path+"blight_incident_count.csv")$ blight_inc.head()
device = train_data.groupby(["device.browser","device.operatingSystem","device.isMobile"]).agg({'totals.transactionRevenue': 'sum'})$ device.sort_values(by = ["device.browser","totals.transactionRevenue"],ascending=False)
sortedILI$ sortedILI['YEAR'], sortedILI['MONTH'] = sortedILI['Month'].dt.year, sortedILI['Month'].dt.month$ sortedILI.head()
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new])$ p_new_sim = new_page_converted.sum()/len(new_page_converted)$ p_new_sim
cityID = 'fa3435044b52ecc7'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Newark.append(tweet) 
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')$ print(z_score, p_value)
from sklearn import metrics$ print("Accuracy: %.3f" % # TODO$          )
stacked=football.stack()$ stacked.head()$ stacked.unstack().head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\mydata.json"$ df = pd.read_json(path)$ df.head(5)
password_str = ""$ with open("dbpass.txt", mode="r") as pass_f:$     password_str = pass_f.readline()
unidentified = list(gender[gender == -1].index)
y_pred_class = lr.predict(X_test_dtm)$ metrics.accuracy_score(y_test, y_pred_class)
surveys_df = pd.read_csv("surveys.csv", keep_default_na=False, na_values=[""])$ species_df = pd.read_csv("species.csv", keep_default_na=False, na_values=[""])
studies_c = pd.merge(studies_b,countries[['nct_id','name']],on='nct_id', suffixes=('_sponsor', '_country'),how='left')$ studies_c.head()
market_cap_df = pd.read_csv('../data/total_market_cap.csv', index_col='Date', parse_dates=True)$ market_cap_df.head()
s4 = df_clean3[df_clean3['name'] == 'the'].sample()$ s4.text.tolist()
from matplotlib import style$ style.use('fivethirtyeight')$ import matplotlib.pyplot as plt
start_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ start_date
df.query('group == "treatment" and landing_page != "new_page"').group.count() +df.query('group != "treatment" and landing_page == "new_page"').group.count()
df = df.drop_duplicates(subset='id')
df2.converted.value_counts(normalize=True)[1]
log_mod_int = sm.Logit(df_new['converted'], df_new[['intercept', 'treatment', 'UK', 'US']])$ results_int = log_mod_int.fit()$ results_int.summary()
recipes.ingredients.str.contains('[Cc]innamon').sum()
sel = [Measurements.station, func.count(Measurements.tobs)]$ active_stations_data = session.query(*sel).group_by(Measurements.station).order_by(desc(func.count(Measurements.tobs))).all()$ active_stations_data
S_1dRichards.decision_obj.simulStart.value, S_1dRichards.decision_obj.simulFinsh.value
Measurement = Base.classes.measurements
autoDf.createOrReplaceTempView("autos")$ SpSession.sql("select * from autos where hp > 200").show()
list(cur.execute('SELECT * FROM experiments'))
m.fit([X], Y, epochs=10, batch_size=16, validation_split=0.1)$
df1 = pd.DataFrame([pd.Series(np.arange(10, 15)), pd.Series(np.arange(15, 20))])$ df1
session.query(func.count(Station.station)).all()
liquor2015 = liquor[liquor.Date.dt.year == 2015]
s = pd.Series([99,5,60], index = ['HPI','Int_rate','Low_tier_HPI'])$ df1.append(s,ignore_index=True) # append the series to the data frame. The line 4 is appended$
exp_budget_vote = sorted_budget_biggest.groupby(['original_title'])['vote_average'].mean()
projFile = "Projects.csv"$ schedFile = "Schedules.csv"$ budFile = "Budgets.csv"
counts.plot.barh()
df2.query('user_id==773192')
pd.isnull(df)
transactions.merge(users, how='left')
data.info()
autos.info()
body = body.replace('\n', ' ')$ body
df = pd.read_csv('SHARE_cleaned_lists.csv')
dayofweek = pd.DatetimeIndex(pivoted.columns).dayofweek
df2_treatment = df2.query('group == "treatment"')
future_dates = prophet_model.make_future_dataframe(periods=forecast_steps, freq='W')$ future_dates.tail(3)
sum(insertid_freq.values())
df.drop(['MINUTE'], axis = 1, inplace=True)
experiment_df = pd.read_csv('multi_arm_bandit_example_distrib.csv')$ experiment_df.drop(['Unnamed: 0'], axis=1, inplace=True)$ experiment_df.head(2)
price_data = json_normalize(data, [['dataset', 'data']])$ heading = json_normalize(data_test, [['dataset', 'column_names']]).transpose()
red_4.to_csv(filename)
index = similarities.MatrixSimilarity(lsi[corpus])$ index.save('bible.index')
train['first_booking_made'] = train['first_booking_made'].astype(int)
geoCodeList = dfCleaningData['geo_code'].unique()$ geoCodeList
lm = sm.Logit(df_new["converted"],df_new[["intercept","control","CA","UK"]])$ results = lm.fit()$ results.summary()$
vendors = df_receipts.vendor_search.unique()$ vendors
g8_groups.mean()
logistic_countries = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']])$ results2 = logistic_countries.fit()
tweets.head()
recent_date = session.query(Measures.date).order_by('Measures.date desc').first()$ recent_date = recent_date[0]$ prior_date = get_prior_years_date(recent_date,1)
cityID = 'a3d770a00f15bcb1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Corpus_Christi.append(tweet) 
autos['odometer'].unique()
extract_deduped_cmp = extract_deduped[f_remove_extract_fields(extract_deduped.sample()).columns.values].copy()
df = stories_df[((stories_df['fixVersions'].isin([reln, 'Backlog']) | pd.isnull(stories_df['fixVersions'])) & $                         (stories_df['status'].isin(['Code Review', 'In Progress', 'Approval', 'Closed'])) )]$ df[['key', 'status', 'fixVersions', 'summary']]
data.groupby(['Name'])['Salary'].sum()
crimes['2011-06-15']['TYPE'].value_counts().head(5)
df_expand['Game'] = temp.Game_x
df[['Open','Close']]  # we need to pass the list of columns that we want access. Not just the individual column names $
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
user_df.columns = ['response_id', 'id', 'customer_id', 'crm_tier', $                    'sales_tier', 'mktg_tier', 'addons',$                    'screen_size', 'role', 'language']
prob_convert = df2.converted.mean()$ print("Probability of individual converting is :", prob_convert)
autos['year_of_registration'].value_counts(normalize = True).sort_index()
X_copy['crfa_r'] = X_copy['crfa_r'].apply(lambda x: int(x))
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key='YOUR_API_KEY'&start_date=2018-07-04&end_date=2018-07-05")
df_birth.Continent.value_counts(dropna=False)
poparr2 = fe.bs.hybrid2ret(poparr,$                            mean=-fe.bs.SPXmean, sigma=fe.bs.SPXsigma,$                            yearly=256)
low_odometer = (autos["odometer_km"] < odometer_LF) == True$ odometer_outliers = autos.loc[low_odometer, :].index
df.tail(50)
df.to_json('twitter_data_5aug.json')
df[['Footnote']].isnull().sum()$
data.registerTempTable("my_data")
tweets_raw = pd.read_table(filepath_or_buffer='tweets_terror2.txt', names=["lan","id","date", "user_name", "content"])
data.pivot(columns="center_name", values="attendance_count").head()
dataframe.groupby('dyear').daily_worker_count.mean()
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))$
print("Only %d objects in the dataset have 'timestamp'. " % len(timestamp_left_df))
df_ari['TeamOdds'] = np.where(df_ari.Location == 'Away', df_ari.MoneyLineAway, df_ari.MoneyLineHome)$ df_ari['TeamOddsStd'] = np.where(df_ari.TeamOdds < 0, df_ari.TeamOdds + 200, df_ari.TeamOdds)
h5 = qb.History[QuoteBar](eur.Symbol, timedelta(30), Resolution.Daily)$
from lxml.cssselect import CSSSelector$ sel = CSSSelector('table[summary] > tbody > ._tracklist_move > .name > a.title')$ nodes = sel(_html)
multi_col_lvl_df.applymap(lambda x: np.nan if np.isnan(x) else str(round(x/1000, 2)) + "k").head(10)
plt.plot(model_output.history['loss'],c='k',linestyle='--')$ plt.plot(model_output.history['val_loss'],c='purple',linestyle='-')$ plt.show;
p_range = max(v.high-v.low for v in trade_data_dict.values())$ p_range_date = [v for v in trade_data_dict.values() if v.high-v.low == p_range][0][0]$ print('The largest trading range in 2017 was: {:.2f} on {}'.format(p_range, p_range_date))
tlen = pd.Series(data=df['len'].values, index=df['Date'])$ tfav = pd.Series(data=df['Likes'].values, index=df['Date'])$ tret = pd.Series(data=df['RTs'].values, index=df['Date'])
df.replace('712-2',51529,inplace=True)$ df[df['zip']==51529].head()
data.head()
print(scores.mean())
mv_lens = pd.merge(movies, ratings)
price2017['DateTime'] = pd.to_datetime(price2017['Date'] + ' ' + price2017['Time'])
rowsToSkip = list(range(28))$ rowsToSkip.append(29)
df2.loc[2893]
counts = df2['landing_page'].value_counts()$ counts
dummies = pd.get_dummies(df_new.country)$ df_new = df_new.join(dummies)$ df_new.head()
df.describe()
email_sku_count = pd.DataFrame(transaction_dates.groupby(['Email', 'Lineitem sku'])['Lineitem quantity'].sum()).reset_index()$ email_sku_count
d['pasttweets_text']=d['pasttweets'].apply(lambda x: ', '.join(x))
iris.iloc[iris.iloc[:,1].between(3.5, 3.6).values,1]
df.info()$ df.isnull().sum()$
breaches = pd.read_csv('breaches.csv', parse_dates=['BreachDate', 'AddedDate', 'ModifiedDate'])$ breaches.head()
ct = CellTypesApi()$ cells = ct.list_cells(require_reconstruction=True)$
x=[0,1,2,3,4,5]$ network_simulation[network_simulation.generations.isin(x)]$
precip_data = session.query(Measurements).first()$ precip_data.__dict__
ss = fe.bs.smallsample_gmr(256, poparr, yearly=256, repeat=300)$
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-07-11&end_date=2018-07-11&api_key={}".format(API_KEY), $                  auth=('user', 'pass'))
df.converted.mean()
pd.period_range('2015-07', periods=8, freq='M')
merge[merge.columns[7:22]].head(3)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X'$ r = requests.get(url)$ data = r.json()
number_of_commits = git_log['timestamp'].count()$ number_of_authors = len(git_log['author'].dropna().unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df2.duplicated().sum()
tips_train = tips.sample(frac=0.8, random_state=123)$ tips_test = tips.loc[tips.index.difference(tips_train.index),:]$ tips_train.shape, tips_test.shape, tips.shape
df2.user_id.nunique()
df_new.head()$
df['intercept']=1$ df['ab_page'] = pd.get_dummies(df['group'])['treatment']
dfTickets.to_csv('all_tickets.csv', index=False, index_label=False)
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
grid_pr_fires.plot.scatter(x='glon',y='glat', c='pr_fire', $                            colormap = 'RdYlGn_r')
google.resample('Q').mean()
tweet_df = df_api[['id', 'retweet_count', 'favorite_count']]$  $ tweet_df.head()
df1['volatility']=(df1['Adj. High']-df1['Adj. Close'])/df1['Adj. Close']$ df1['volatility'].head()
r.json()['dataset_data']
plt.style.use('ggplot')$
import pandas as pd$ df = pd.DataFrame(dataset)
df_parties = df[df['Descriptor'] == 'Loud Music/Party']$ df_parties.groupby(df_parties.index.hour)['Created Date'].count().plot(kind="bar")$
sub_gene_logical_vector = df.source.isin(['ensembl', 'havana', 'ensembl_havana'])$ sub_gene_df = df[sub_gene_logical_vector]$ sub_gene_df.shape
session.query(Measurement.date).order_by(Measurement.date.desc()).first()$
df_search_cate = df_session_dummies[df_session_dummies.action_search_cate == 1 ]$ df_search_cate = df_search_cate.dropna()$ df_search_cate_dummies = pd.get_dummies(df_search_cate, columns=['value'])$
df2.head()
twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')
response = requests.get(url)
IBMpandas_df_subset = IBMpandas_df[list(['Date',"Close"])]$ IBMpandas_df_subset.plot.line()
def load_data(url):$     return pd.read_json(url, orient='columns') $
tokens = nltk.regexp_tokenize(text.lower(), '[a-z]+')$ tokens[:10]
n_old = df2.query('group == "control"').shape[0]$ n_old
html_table = df.to_html()$ html_table
df = pd.DataFrame(data = pd.Series(range(12)).reshape(3, 4), columns = list('abcd'))$ df
dr_existing_8_to_16wk_arimax = dr_existing_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ dr_existing_8_to_16wk_arimax.index = dr_existing_8_to_16wk_arimax.index.date
df3 = df2.join(df_country.set_index('user_id'), on = 'user_id')
topics = pd.DataFrame(interests)
df2.head()
train.OPTION.value_counts()
df1_clean.drop('test', axis=1, inplace=True)
user_total = df.nunique()['user_id']$ print("Number of unique users is : {}".format(user_total))
a.iloc[:3]
diff_real = 0.118808 - 0.120386  $ p_diffs = np.array(p_diffs)$ (p_diffs > diff_real).mean()
grid.grid_scores_
grp = dta.groupby(dta.results.str.contains("Pass"))$ grp.groups.keys()
(a + b).describe()$
cityID = 'd5dbaf62e7106dc4'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jacksonville.append(tweet) 
url = 'https://www.reddit.com/r/Python/'
sample_weight = np.array([1 if not p else 1.25 for p in joined_train.perishable])
weather_dates = []$ for pair in xrange(len(weather_dt)):$     weather_dates.append(datetime.datetime.combine(weather_dt[pair][0], weather_dt[pair][1]))$
combined_df[combined_df['classifier_summary'].isnull() == True]
trump_df.to_csv("test.csv", index=False, encoding="utf-8")
yc_new2.rename(columns={'Tip_Amt':'tipPC'}, inplace=True)$ yc_new2.head()
goog.plot(y='Close')
news_paragraph = soup.find_all("div", class_="rollover_description_inner")[0].text$ print(news_paragraph)
countries.shape[0] == df2.shape[0]
active_station = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\$                order_by(func.count(Measurement.tobs).desc()).all()$ active_station
new_page_converted.mean()- old_page_converted.mean()
import numexpr$ mask_numexpr = numexpr.evaluate('(x > 0.5) & (y < 0.5)')$ np.allclose(mask, mask_numexpr)
options = webdriver.ChromeOptions()$ options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome')
combined_df = user_df.merge(response_df, how='inner', left_on='response_id', right_on='id')$ print len(combined_df), len(user_df), len(response_df)
y.mean()
dr_existing_patient_8_to_16wk_arima = dr_existing_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']]$ dr_existing_patient_8_to_16wk_arima.index = dr_existing_patient_8_to_16wk_arima.index.date
stock_data.describe()
df['install_rate'] = df.accounts_provisioned.div(df.district_size)$ df.describe().T$
df.corr().round(2)$
table = pd.read_html("https://en.wikipedia.org/wiki/List_of_sandwiches", header=0)[0]$
ctd_df.head(10)
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
p_old = df2['converted'].mean()$ p_old
trump_df.text = trump_df.text.str.lstrip("b'")
np.exp(reg_lm2.params)
vip_reason = vip_reason.drop(['[', ']'], axis=1)$ vip_reason = vip_reason.drop(vip_reason.columns[0], axis=1)
pd.crosstab(df.group, df.landing_page, margins=True)
display(data.head(10))$
lm_multi = sm.OLS(df_new['converted'], df_new[['intercept', 'US', 'UK', 'ab_page']])$ multi_results = lm_multi.fit()$ multi_results.summary()
fname = "tests/test_data/acoust.ic/wa.ve0.wav"$ re.findall(r"^.*\.(.+)",fname)[0]$
[k for val in train_x[0] for k,v in words.items() if v==val]
temp_df2['timestamp'] = pd.to_datetime(temp_df2['timestamp'],infer_datetime_format=True)
df.plot();
author_comm_pairs = df3[['author_id', 'author_email', 'commiter_id', 'commiter_email', 'timestamp']].values
sample.head(1)
df = pd.read_csv('ab_data.csv')$ df.head(5)
cityID = 'ac88a4f17a51c7fc'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Portland.append(tweet) 
df_rounds['funding_round_type'].value_counts()
INT = INT.drop(columns= ['Contact: First Name', 'Contact: Middle Name',$        'Contact: Last Name'])
shows[shows['first_year'] >= 1980].shape
df_notnew = df.query('landing_page != "new_page"')$ df_3 = df_notnew.query('group == "treatment"')$ df_3.nunique()$
$hadoop fs -put /data/tg_cg18_bigdata/rc_2018_02.csv /user/sohom/$
req = requests.request('GET', 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country'])$ df2.head(10)
df_copy=df.copy()$ image_copy=image.copy()$ tweet_data_copy=tweet_data.copy()
kmf.plot()$ plt.title('Kaplan Meier Fitter estimates')
x_train = scaler.transform(x_train)$ x_test = scaler.transform(x_test)
plt.hist(data['Age'])
1 - 0.5000619442226688
obs_diff = prob_treat - prob_contr$ (p_diffs > obs_diff).mean()$
import statsmodels.api as sm$ logit = sm.Logit(df2['converted'],df2[['intercept','treatment']])$ results = logit.fit()
pd.date_range('1/1/2000', '1/1/2000 23:59', freq='4h')$
p_diff = p_new - p_old$ print("Difference in probability of conversion(not under H_0) is {}".format(p_diff))
df.query('landing_page == "new_page"').query('group == "control"').count() + df.query('landing_page == "old_page"').query('group == "treatment"').count()
df2.head()
df2.converted.mean()
pd.options.display.max_colwidth = -1$ df[['Text', 'PP Text']]$
df2.rename(columns = {'treatment': 'ab_page'}, inplace=True)$ df2.drop('control', axis=1, inplace=True)$ df2.head()
df_twitter_copy = df_twitter_copy.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis = 1)
df=pd.read_csv("Monika__link_syn11_sel.csv")#,encoding="cp1252")#"utf-16")#$ df.head()$
df.drop(df[df.donation_date == '1899-12-31'].index, axis=0, inplace=True)
conv_ind = df2.query('converted == 1')$ num_conv = conv_ind.shape[0]$ print('Probability of converted individual regardless of the page: {:.4f}'.format(num_conv/df2.shape[0]))
a=unique_users[unique_users.created_at.isin(['0000-00-00 00:00:00'])].index.values$ unique_users.loc[a,'created_at']='2010-11-11 03:59:09'$
twelve_months_prcp.head()
y.end_time
idx = np.random.permutation(train_data.index)$ train_data = train_data.reindex(idx)$ train_labels = train_labels.reindex(idx)
x.dropna()
data["engagement"] = np.where(data["comms_num"]>500, 1, 0)
t = thisyear$ len(t[(t.creation <= '2013-01-01') & (t.creation > '2012-01-01')])
all_colnames = [clean_string(colname) for colname in df_total.columns]$ df_total.columns = all_colnames$ df_total.head()
p_old = df2.query('landing_page == "old_page"').converted.mean()$ p_old
mammals = session.query(NA).filter(NA.genus == 'Antilocapra').all()$ for mammal in mammals:$     print("Family: {0}, Genus: {1}".format(mammal.family, mammal.genus))
model = model_list[1]$ pprint(model.show_topics(formatted=False))
data['processing_time'].describe()$
dict1 = {k: g["VALUE"].tolist() for k,g in df.groupby("TUPLEKEY")}$ dict1[('A002','R051','02-00-00','LEXINGTON AVE')]
hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),$                  callbacks=[RocAuc], verbose=1)
n_old = df2.query('landing_page == "old_page"').count()[0]$ print("Number of users with old page :",n_old)
titanic.pivot_table('survived', index='sex', columns='class', margins=True)
np.r_[np.random.random(5), np.random.random(5)]
df['user_id'].nunique()$
tweets = pd.read_csv('https://raw.githubusercontent.com/acefoxy/Project/master/realDonaldTrump_tweets.csv',encoding='latin1')$ tweets.source.value_counts()$
top_words = pd.DataFrame(X.toarray()).sum().sort_values(ascending=False).head(10)$ for word in top_words.index:$     print 'Feature: {}, Token: {}'.format(word, tfidf.get_feature_names()[word])
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_hat), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
row_num = df.shape[0]$ print("Number of rows is: {}".format(row_num))
results = soup.find_all('li', class_="content_title")
df_joined=df_countries.join(df2_dummy)$ df_joined.tail(1)
df2.drop(labels=1899, axis=0, inplace=True)$ df2[df2['user_id']==773192]
shopping_carts = pd.DataFrame(items)$ shopping_carts
print(df2,'\n')$ print(df2[df2['E'].isin(['test'])])  # select E column=='test' only 
df.shape
usernodes = list( db.osm.find({"created.user": "Bot45715"}))$ print len(usernodes)$ usernodes[:10]
not_missing_values_pd = shelter_pd.loc[(shelter_pd['SexuponOutcome'] != 'Unknown') & (shelter_pd['SexuponOutcome'] != 'NaN')]$ not_missing_values_pd.describe()
last.split('-')
p_old = df2['converted'].mean()$ (p_old)
y = K.dot(x, W) + b$ loss = K.categorical_crossentropy(y, target)
autos['odometer_km'].value_counts()$
hexbin = sns.jointplot(x="item", y="sentiment", data=dta, kind="scatter")$
SCR_PLANS_df = USER_PLANS_df.drop(free_mo_churns)
from IPython.display import Image$ from IPython.core.display import HTML $ Image(url= "https://pbs.twimg.com/media/CmgBZ7kWcAAlzFD.jpg")
result = cur.fetchall()$
soup.find('div', class_='movie-add-info left').find_all('li')
df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9])
twosample_pol = scipy.stats.ttest_ind(locationing.polarity, tweetering.polarity)$ twosample_pol
injury_df.iloc[:40,:]
filter_df = filter_df[filter_df['start_time'] >= datetime(2016, 8, 1, 0, 0)]$ filter_df.head(2)
tweet_df["date"] = pd.to_datetime(tweet_df["date"])$
reflClean = reflRaw.astype(float)$ reflClean
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="vegan").items(max_tweets):$     print(tweet)
train_df[train_df.author.isnull()].head(1)
from pyspark.ml.evaluation import BinaryClassificationEvaluator$ evaluator = BinaryClassificationEvaluator(rawPredictionCol="prediction", labelCol="label", metricName="areaUnderROC")$ print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))
transfered_holidays=holidays_events[(holidays_events.type=='Holiday') & (holidays_events.transferred==True)]$ print("Rows and columns:",transfered_holidays.shape)$ pd.DataFrame.head(transfered_holidays)
Precipitation,=plt.plot(df_d['date'], df_d['Prcp'],label='precipitation')$ plt.tight_layout()$ plt.show()
precip_df = query2pandas(precip)$ precip_df = precip_df.set_index('Date')
null_vals = np.random.normal(0, p_diffs.std(), 10000)
reviews.loc[(reviews.points/reviews.price).idxmax()].title
df.groupby(['filename'])[['ml-fav-change','ml-dog-change','rl-fav-change','rl-dog-change',$                           'total-over-change','total-under-change']].sum().mean()
our_nb_classifier.predict("The car lights turned off and THROTTLE did not work when driving for a long time")
print('RF: {}'.format(rf.score(X_test, y_test)))$ print('KNN: {}'.format(knn.score(X_test, y_test)))
data.quantile(0.25)
df2['offerType'].value_counts().plot(kind='bar')$ print(df2['offerType'].value_counts())
dataset[dataset[dataset['user_location'].str.contains("argentina",na=False)]['place_country_code'].isnull() == True]['place_country_code'].count
time_length.plot(figsize = (16, 4), color = 'r')$ time_fav.plot(figsize = (16,4), color = 'g')$
df1 = df[df['Title'].str.contains(search_terms)]$ df1.shape
df_m.loc[df_m["CustID"].isin([customer])]
columns = inspector.get_columns('stations')$ for c in columns:$     print(c['name'], c["type"])$
trans = pd.read_csv("raw/translationsbackup.csv", encoding="utf-8", index_col=0)$ trans.head()$
for i in range(len(billstargs.billtext)):$     value = billstargs.get_value(i, 'billtext')$     billstargs.set_value(i, 'billtext', replace_bill(value))
prediction_proba = grid.predict_proba(X_test)$ prediction_proba = [p[1] for p in prediction_proba]$ print(roc_auc_score(y_test, prediction_proba))
start = "2005-01-01"$ end = "2018-06-31"$ trading_days = fk.get_monthly_last_trading_days(start=start, end=end)$
outfile = os.path.join("Resource_CSVs","Main_data_Likes.csv")$ merge_table1.to_csv(outfile, encoding = "utf-8", index=False, header = True)
pd.DataFrame(dummy_var["_Source"][Company_Name]['Low']['Forecast'])[-4:]
ab_dataframe.query(' (group=="treatment" & landing_page != "new_page") | (group !="treatment" & landing_page == "new_page" ) ').shape[0]
sorteios_2017 = data[data["Data Sorteio"] > "2017-01-01"]$ sorteios_2017.Ganhadores_Sena.sum()$
df2['user_id'].nunique()
s = pd.Series(np.random.randn(4))$ s
counts,x,y = np.histogram2d(theta[:,0], theta[:,1], bins=[50,50], range=[[0,1],[0,1]])$ plt.imshow(counts, extent=(x[0],x[-1],y[0],y[-1]), origin='lower');
year_with_most_commits = commits_per_year["author"].idxmax().year$ print(year_with_most_commits)
p_old=df2.converted.mean()$ p_old
data_matrix = np.zeros((n_user, n_item))$ for line in dfToProcess.itertuples():$     data_matrix[customerList.index(line[1]), productList.index(line[2])] = line[3]
df2.columns = ['c','d']$ df2
print( "\nSize of the dataset - " + str(len(autodf)))$ autodf = autodf[(autodf.yearOfRegistration >= 1990) & (autodf.yearOfRegistration < 2017)]$ print( "\nSize of the dataset - " + str(len(autodf)))
df = pd.DataFrame.from_records(mylist)$ df.head()
ndvi_grid = np.array(np.meshgrid(lon_us, lat_us)).reshape(2, -1).T$ np.shape(ndvi_grid)
import pandas as pd$ data=pd.read_table("https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv",sep=',')
validation.analysis(observation_data, richard_simulation)
data_issues_transitions=pd.read_csv('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-transitions.csv')
from scipy.stats import norm$ norm.cdf(z_score)$
number_of_commits = len(git_log)$ number_of_authors = git_log['author'].nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
autos=autos[autos["price"].between(500,350000)]
x_axis = np.arange(0,len(target_users))$ x_axis
race_vars.columns = race_vars.columns.str.replace(' ', '_')$ race_vars.columns = race_vars.columns.str.replace('/', '_')$ race_vars.columns = race_vars.columns.str.lower()
from keras.models import Sequential$ from keras.layers import Dense$
F = new_dynamic_formatter(method='col', pcts=1, trunc_dot_zeros=1)$ F(port.performance.report_by_year(prior_n_yrs=2, ranges=[(2011, 2012)]))
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])$ df_new.head()
gain_df.corr()
twitter_archive_copy = twitter_archive.copy()$ image_predictions_copy = image_predictions.copy()$ tweet_json_copy = tweet_json.copy()
last_entry_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ last_entry_date
print('reduce memory')$ utils.reduce_memory(demographics)$
cassession.caslibinfo()
np.count_nonzero(np.any(nba_df.isnull(), axis = 0))
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)$
means1_table = avg1_table.rename(columns = {'Units Sold':'UnitsSold'})$ means2_table = means1_table.groupby(['Product', 'Country']).UnitsSold.mean()$ means2_table
citydata_nbr_rides_renamed = citydata_with_nbr_rides.rename(columns={"ride_id": "nbr_rides"})$ citydata_work = citydata_nbr_rides_renamed[['city', 'driver_count', 'type', 'average_fare', 'nbr_rides']]$
pd.concat([s1, s2, s3], axis=0)
pd.merge(d1, d3, left_on='city', right_on='place').drop('place', axis=1)
df_new['intercept'] = 1$ df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]
total_stations = session.query(Station.station).distinct(Station.station).count()$ print('Total number of stations is ' + str(total_stations))
df2['ab_page'] = pd.get_dummies(df['group']) ['treatment']
df_test_user = df_users.iloc[0, :]$ df_test_user
pr_item = (2017, 1)$ url = 'https://www.sec.gov/news/press-release/' + str(pr_item[0]) + '-' + str(pr_item[1])$ page = urlopen(url)
plt.hist(p_diffs);$
pivoted_data.plot(figsize=(10,10))
df.head()$
dataframe.groupby('dday_of_week').daily_worker_count.sum()
precipitation_2yearsago = session.query(Measurement.prcp, Measurement.date).filter(Measurement.date > query_date_2yearsago).order_by(Measurement.date.desc()).all()$ print(precipitation_2yearsago)
S_lumpedTopmodel.forcing_list.filename
len(dataset.dropna(subset=['place_country']))/len(dataset)$
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])$ ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])$ pd.concat([ser1, ser2])
df_twitter_archive_master=pd.read_csv('twitter_archive_master.csv')
1 + np.nan
df['state_cost'].replace('\$','',regex=True,inplace=True)$ df['state_retail'].replace('\$','',regex=True,inplace=True)$ df['sale'].replace('\$','',regex=True,inplace=True)
lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit()$ lm.params
picker = df_nona.groupby('district_id').app_id.nunique()$ single_app = picker[picker==1]$ df_nona[df_nona.district_id.isin(single_app.index)].install_rate.hist()
for table in cur.fetchall():$     print(table)
np.shape(prec_fine)
year17 = driver.find_elements_by_class_name('yr-button')[16]$ year17.click()
ohlc = walk.resample("H").ohlc()$ ohlc
plt.savefig("Scatter.jpg")$ plt.show()
df_new[['US', 'UK']] = pd.get_dummies(df_new['country'])[['US','UK']]
c_new = df2['converted'].mean()$ print(c_new)
zf = zipfile.ZipFile(path)$ df = pd.read_excel(zf.open('Sample_Superstore_Sales.xlsx'))$ df.head(5)
conn.execute("SELECT * FROM posts WHERE id=?", (158743,)).fetchall()
%config InlineBackend.figure_format='svg'$ plt.plot(x, np.sin(x)/x)
merged_data['drone_rtk_lat'] = merged_data['rtk_lat'].interpolate(method='linear')$ merged_data['drone_rtk_lon'] = merged_data['rtk_lon'].interpolate(method='linear')$ merged_data['drone_rtk_alt'] = merged_data['rtk_alt'].interpolate(method='linear')$
timelog = timelog.drop(['Email', 'User', 'Amount ()', 'Client', 'Billable'], axis=1)
lst = data_after_first_filter.WATER_BODY_NAME.unique()$ print('Waterbodies in workspace dataset:\n{}'.format('\n'.join(lst)))
twitter_ar.expanded_urls[100]
country = pd.get_dummies(df2['country'])$ df2 = df2.join(country)$ df2.head()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']])$ result = logit_mod.fit()$ result.summary()
df15stats=df15.groupby(['county','store_number']).agg({'state_retail':'mean','profit':'mean','ppb':'mean','volume_sold_l':'mean','bottles_sold':'mean','bottle_ml':'mean','sale':'sum','sale':'mean',})$ df15stats  
recortados = [recortar_tweet(t) for t in tweets_data]$ tweets = pd.DataFrame(recortados)
festivals.head(3)
df.select('station','year','measurement').show(5)
image_df.head(5)
pcpData_df.prcp.describe()
status = client.training.get_status(training_run_guid_async)$ print(json.dumps(status, indent=2))
df.shape
cityID =  '4ec71fc3f2579572'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $          Shreveport.append(tweet) 
B2.print_all_paths()$
groceries.drop('apples', inplace=True)
questions = questions.reset_index(drop=True)
sample = pd.read_csv('data/sample_submission.csv')
plt.boxplot(raw_scores, vert=False)$ plt.xticks(xlocs, xlocs)$ plt.xlabel('Beer Ratings');
display(data.head())
data[(data['author_flair'] == 'Broncos') & (data['win_differential'] >= 0.9)].comment_body.head(15)$
pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)$ tweets = api.search(q='Deloitte', rpp=1)$ pp.pprint([att for att in dir(tweets) if '__' not in att])
tdf[tdf['smoker'] == 'No'].describe()
df.isnull().any().any()$
pdf.loc['2016-1-1':'2016-3-31'].plot()$
print(df.tail())
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ print(act_diff)
s = pd.Series(np.random.randn(len(rng)).cumsum(), index=rng)$ s.head()
reviews.groupby('variety').price.agg([min,max])
pd.set_option('max_colwidth', 100) #Its nice to see all columns
pulledTweets_df.sentiment_predicted_nb.value_counts().plot(kind='bar', $                                                            title = 'Classification using Naive Bayes model')$ plt.savefig('data/images/Pulled_Tweets/'+'NB_class_hist.png')
round((timelog.seconds.sum() / 60 / 60 / 24), 1)
df_groups = pd.read_csv('groups.csv')$ df_groups.head()
kwargs = {'num_workers': 4, 'pin_memory': True} if args.cuda else {}$ kwargs
df.info()$
repeat_customer_purchase_timing['first last gap'].hist()
X = acs_df.drop('homeval', axis=1).values$ y = acs_df['homeval'].values
autos = autos.drop(index = odometer_outliers)
autos = pd.read_csv("autos.csv", encoding = "Latin-1") # Default encoding UTF-8 threw an error$ autos.info()  # printing information about the'autos' dataframe$ autos.head()  # printing first few rows of "autos"
housing['Created Datetime'] = pd.to_datetime(housing['Created Date'])
df=pd.read_excel("data/DataSet_GasPrice_ Outlier_Removed.xlsx")$ df.head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path, sheetname = 'Superstore2')$ df.head(5)
ari_games2['MatchTimeROT'] = ari_games2.index
fi = ed_level.append(free_sub)
indx_weather=pd.read_csv(working_folder+indx_stn)
sns.pairplot(df, x_vars=['Bottles Sold','Volume Sold (Liters)'], y_vars='Sale (Dollars)', size=7, aspect=0.7)
fh_2 = FeatureHasher(input_type='string', non_negative=True)$ %time fit2 = fh_2.fit_transform(train.device_id)
df2.drop(df2.index[2893], inplace=True)$ df2.head()
df[df.state_cost==425]$ df[df.item_number==995381].item_descript
df_con_control = df_con1.query('group =="control"')$ x_control = df_con_control["user_id"].count()$ x_control$
cursor = db.tweets.find({}, {'text':1, 'id':1,'user':1, 'hashtags':1,'_id': 0})$ df =  pd.DataFrame(list(cursor))$ df.head(3)
neg = df_feedparse[df_feedparse.flag == -2].sample(20)$ neg.shortDesc = neg.shortDesc.map(lambda x: strip_tags(x))$ neg.longDesc  = neg.longDesc.map(lambda x: strip_tags(x))$
log_mod = pickle.load(open('../data/model_data/log_pred_mod.sav', 'rb'))
pres_df['state'].unique()
filterdf = result.query("0 <= best <= 1 and fileType == 'csv' and teamCount < 1000")$ filterdf.corr()['best'].sort_values()
twitter_archive.rating_denominator.value_counts()
delays_geo.to_crs(wards.crs, inplace=True)$ delays_geo.geometry.total_bounds
df.iloc[0]
conn = psycopg2.connect("dbname='pgsdwh' user='502689880' host='alpgpdbgp2prd.idc.ge.com' password='pass66824w'")
twitter_archive_clean.name.value_counts()
indeed.dropna(subset=['summary'], inplace=True)$ indeed.isnull().sum()
df = data1.append(data2).reset_index()$ df['date'] = pd.to_datetime(df['starttime'])$ df.head(3)
data.loc[data['hired']==1].groupby('category').hourly_rate.mean()
random_integers.max()
columns = inspector.get_columns('station')$ for c in columns:$     print(c['name'], c["type"])
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean[twitter_archive_enhanced_clean.in_reply_to_user_id.isna()]$ twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(['in_reply_to_status_id','in_reply_to_user_id'], axis = 1)
df = cleanup(result)$ print df['user_location'].value_counts()[:15]
month_counts = df.month.value_counts().sort_index()
tokenizer = RegexpTokenizer(r'\w+')$ bill_tokens= [tokenizer.tokenize(i) for i in ogxprep.billtext]$ new_list = [" ".join([lemmatizer.lemmatize(word) for word in x]) for x in bill_tokens]
fh_1 = FeatureHasher(num_features=uniques.iloc[2, 1], input_type='string', non_negative=True) # so we can use NaiveBayes$ %time fit = fh_1.fit_transform(train.device_model)
data.dtypes
df.sort_values(by=['prs'], ascending=False)
apple_data = google_stocks('AAPL')$ apple_data.columns$ apple_data.index
dfn['goal_log'] = np.log10(dfn['goal'].values)
shelter_df_only_idx.registerTempTable("shelter")$ sqlContext.sql("SELECT OutcomeType_idx, AnimalType_idx, SexuponOutcome_idx, Breed_idx, Color_idx, AgeuponOutcome_binned FROM shelter").show()$
df = pd.read_csv('twitter-archive-enhanced.csv')
df2 = df2.add_suffix(' Closed')$ df4 = pd.merge(df,df2,how='left',left_on='Date Closed',right_on='date Closed')$
rfreg = RandomForestRegressor(random_state=42)$ rfreg.fit(X_train, y_train)$ rfreg.score(X_test, y_test)
blink= condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="BLINK")
move_1_herald = sale_lost(breakfastlunchdinner.iloc[1, 1], 10)$ print('Adjusted total for route: ' + str(move_34p34h34h - move_1_herald))
questions.set_index('createdAt', inplace=True)
df.loc[:, ['B', 'D']] # notice lack of parentheses here!
pivoted.T[labels==1].T.plot(legend=False, alpha = 0.1);
for row_index, row in df.iterrows():$     print(row_index, row)
bow_corpus  = [dictionary.doc2bow(text) for text in list(repos)]$ index = SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12418)
if not os.path.isdir('output/pv_production'):$     os.makedirs('output/pv_production')
grouped_authors_by_publication.tail()
ws.delete_cols(1)
session.query(Measurement.id, func.avg(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
len(df2['user_id'].unique())
df_20180113 = pd.read_csv('data/discovertext/hawaii_missile_crisis-export-20180221-113313.csv',$                           infer_datetime_format=True)$ df_20180113.head()
from scipy.stats import norm$ print(norm.cdf(z_score)) #Z-score significance$ print(norm.ppf(1-(0.05))) # It tells us what our critical value at 95% confidence is $
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
dataBloodType.human_id = dataBloodType.human_id.str.lower()$ df2 = df.merge(dataBloodType,left_on = 'Sample', right_on='human_id', how='inner')$ del dataBloodType$
free_data.sort_values(by="country")$ free_data.sort_values(by="educ")$ free_data.sort_values(by="age", inplace=True, ascending = False)
total_rech_amt_6_7 = (telecom["total_rech_amt_6"] + telecom["total_rech_amt_7"] + telecom["total_rech_data_amt_6"] + telecom["total_rech_data_amt_7"]) /2.0$ amont_70_pc = np.percentile(total_rech_amt_6_7, 70.0)$ print('70 percentile of first two months avg recharge amount: ', amont_70_pc); print_ln();
df.shape$ df.tail()
df_merge.groupby(['school_name','grade']).reading_score.mean().unstack()
%matplotlib inline$ from IPython.display import Image$ from IPython.core.display import HTML
twitter.name.value_counts()$
Mars_Weather_URL = 'https://twitter.com/MarsWxReport/status/1017925917065302016'$ Weather_response = requests.get(Mars_Weather_URL)$ Weather_soup = BeautifulSoup(Weather_response.text, 'html.parser')
pd.options.display.max_rows$ pd.set_option('display.max_colwidth', -1)$ new_df = df_filtered_by_RT.assign(clean_text = df_clean_text, extr_emojis = df_extr_emojis)$
obs_diff = df_treatment['converted'].mean()- df_control['converted'].mean()$ obs_diff
learn.predict()
oz_stops.loc[oz_stops['stopid'] == '7270']
ebola_melt['type'] = ebola_melt.str_split.str.get(0)$ ebola_melt.head()
pd.Series(np.random.randn(5))
bixi_hourly=bixi_hourly.resample('1H', how={'duration_sec': np.mean,$                                             'distance_traveled': np.mean, 'is_member':np.sum,$                                             'number_of_trips':np.sum})
expx=np.mean(x)$ expy=np.mean(y)$ expx, expy
import pickle$ filename = 'automl_feat.sav'$ pickle.dump(automl_feat, open(filename, 'wb'))
last_date = dt.date(2017, 8, 23)$ last_date
oz_stops = pd.read_csv('../../Data/all_bus_stops.csv', sep=',')
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/SCN_BDAY_qthis.scn_age.count()
pathdataOH = np.repeat(newPaths[idxKeep], invals)$ oldpath = np.repeat(idxOP[idxKeep], invals)
train_df = pd.read_csv("train.csv", skiprows=range(1,159903891), nrows=25000000, dtype=dtypes)$ train_df.head()
df_grp= df.groupby('group')$ df_grp.describe()
df2['converted'].mean()
session.query(Measurements.station,Measurements.date,Measurements.prcp, Measurements.tobs).first()
sqlContext.sql("select * from example2").toPandas()
fetch_measurements('http://archive.luftdaten.info/2016-05-09/')
pm_data = pd.concat([pm_data,$                      pd.DataFrame(pm_data.groupby('unit_number').cumcount(), columns = ['obs_count'])]$                     , axis = 1)
n_new = df2[df2.group == 'treatment'].count()[0]$ n_new
print(pandas_list_2d_rename.loc[:, 'Name'])
mergedNewSet = sortedSetNewIndex.merge(laggedSortedSet,how = 'outer',left_index = True, right_index = True)$ print mergedNewSet;
coin_data = quandl.get("BCHARTS/ITBITSGD")$
temps_df.Missoula
trial_parameter_nc = Plotting(S_distributedTopmodel.setting_path.filepath+S_distributedTopmodel.local_attr.value)$ trial_parameter = trial_parameter_nc.open_netcdf()$ trial_parameter['HRUarea']
for df in (joined, joined_test):$   df['Promo2Since']=pd.to_datetime(df.apply(lambda x: Week(x['Promo2SinceYear'],x['Promo2SinceWeek']).monday(),axis=1).astype(pd.datetime))$   df['Promo2Days']=df['Date'].subtract(df['Promo2Since']).dt.days
all_data_df = pd.read_csv('github_issues.csv')$ all_data_bodies = all_data_df['body'].tolist()
lrs = [0.0001, 0.0001, 0.0001, 0.0001, .001]
dollars_per_unit.columns = pd.MultiIndex.from_product([['Dollars per Unit'], dollars_per_unit.columns])$ pd.concat([multi_col_lvl_df, dollars_per_unit], axis='columns').head(3)
cand.CAND_ID.value_counts()$ cand.CAND_NAME.value_counts()
from pandas.tseries.offsets import BDay$ pd.date_range('2015-07-01', periods=5, freq=BDay())
for name, regex in regex_col.items():$     create_regex_col(df_train,regex,name)$
forcast_set=clf.predict(X_lately)
X = pd.merge(X, latestTimeByUser, on="userid")$ X = pd.merge(X, uniqueCreatedTimeByUser , on ="userid")$ X.head(5)
np.shape(temp_fine)
data=requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=%s" %API_KEY)
df2.drop(df2.index[2893])$ df2.user_id.duplicated().sum()
%%time$ responded_all_jan23[fields].to_excel(export_filename_jan23, index=False)$ responded_all_feb5[fields].to_excel(export_filename_feb5, index=False)
transactions.merge(transactions,how='inner',on='UserID').head(5)
df.user_id.nunique()
def tweet_extend (tweet_id):$     $     return api.get_status(tweet_id, tweet_mode='extended')._json['full_text']
pernan = 0.80$ nbart_allsensors = nbart_allsensors.dropna('time',  thresh = int(pernan*len(nbart_allsensors.x)*len(nbart_allsensors.y)))
S.decision_obj.simulStart.value = "2007-07-01 00:00"$ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
grouped2.size().unstack().plot(kind="bar", stacked=True, figsize=(8,6))$ plt.show()
n_new = df2.query('landing_page == "new_page"').shape[0]$ n_new
data[['TMAX', 'TMED']].head()
surveys_df = pd.concat([surveys2001_df, surveys2002_df], axis=0, sort=False)$ surveys_df = surveys_df.reset_index(drop=True)
print len(nodes)$ for node in nodes:$     print node.get('href'), node.text
output_test = output_list$ output_test.count()
dataset['Created_at'] = pd.to_datetime(dataset.Created_at)$ dataset.head()
value_end_09 = float(final_df.loc[(final_df["Date"] == "2014-09-30") & (final_df["Country"]=="Guinea")]["Total number of deaths"])$ value_start_10=float(final_df.loc[(final_df["Date"] == "2014-10-01") & (final_df["Country"]=="Guinea")]["Total number of deaths"])$ final_results.iloc[2, 1] = value_start_10 - value_end_09
re.match('^\w+@[a-zA-Z_]+?\.[a-zA-Z]{2,3}$', 'abc@gmail.com').group()
overallDf = pd.DataFrame({$     "News Outlet": overallOutlet,$     "Compound Score" : overallCompound})
np.exp(result.params)
old_page_converted = np.random.binomial(1, P_old, n_old)$ old_page_converted
shelter_pd[['DateTime', 'OutcomeType', 'AnimalType', 'SexuponOutcome', 'AgeuponOutcome', 'Breed', 'Color']] = shelter_pd[['DateTime', 'OutcomeType', 'AnimalType', 'SexuponOutcome', 'AgeuponOutcome', 'Breed', 'Color']].astype(str)$ shelter_df = sqlContext.createDataFrame(shelter_pd)$
print(type(plan["plan"]))$ print(plan['plan'].keys())
all_cards = pd.DataFrame(data = None, columns = all_cards_columns)$ all_cards.rename_axis("name", inplace = True)$ all_cards.head()
autos["gearbox"].value_counts(normalize=True,dropna=False)
block_geoids_2010 = json.load(open('block_geoids_2010.json'))$ print 'There are', len(block_geoids_2010), 'blocks'$ assert(len(block_geoids_2010) + 1 == len(block_populations))
from features.build_features import remove_invalid_data$ df = remove_invalid_data(pump_data_path)$ df.shape
df_new.tail()
cgm = data[data["type"] == "cbg"].copy()$ cgm.head()
%matplotlib inline$ import matplotlib.pyplot as plt
stream.filter(track=['clinton','trump','sanders','cruz'])
df.user_id.nunique()
population.apply(lambda val: val > 1000000)
serc_pixel_df = pd.DataFrame()
results_df = pd.DataFrame(dict_results).set_index("Username").round(2)$ results_df
userProfile = userGenreTable.transpose().dot(inputMovies['rating'])$ userProfile
financial_crisis.loc['Wall Street Crash / Great Depression'] = '1929 - 1932'$ print(financial_crisis)
df_new1=df.query('landing_page=="new_page" & group=="control" ')$ df_new1.tail(10)$ df_new1.nunique()
plate_appearances['pitcher_throws_left'] = np.where(plate_appearances['p_throws'] == 'L', 1, 0)$ plate_appearances['left_handed_batter'] = np.where(plate_appearances['stand'] == 'L', 1, 0)
df.dropna(axis='rows', thresh=3)
cityID = '2409d5aabed47f79'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Rochester.append(tweet) 
np_x = np.random.rand(1000)$ np_target = 0.96*np_x + 0.24
df2[df2.duplicated('user_id')]
plt.style.available
df2.country.unique()
df3['day'] = df3['timestamp'].dt.day$ df3.head()
itemTable["Project"] = itemTable["Project_Id"].map(project_link)
df[(df.state == 'YY') & (df.amount >= 45000)]
mileage = pd.Series(mileage)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative='smaller')$ z_score, p_value$
twitter_ar.text[1]
probarr = fe.toar(lossprob)$ fe.plotn(fe.np.sort(probarr), title="tmp-SORTED-prob")
max_key = max(open_dict.keys(), key=(lambda k: open_dict[k]))$ min_key = min(open_dict.keys(), key=(lambda k: open_dict[k]))
neuron_no = 10$ source_indices_L23exc_L23fs = np.where(np.array(conn_L23exc_L23fs.i)==neuron_no)$ target_indices_L23exc_L23fs = np.array(conn_L23exc_L23fs.j)[source_indices_L23exc_L23fs]
plt.scatter(X2[:, 0], X2[:, 1], c=dayofweek, cmap='rainbow')$ plt.colorbar();
df['water_year2'] = df[['year','month']].apply($     lambda row: row['year'] if row['month'] < 10 else row['year'] + 1, $     axis = "columns")
LT906474 = pd.read_table("GCA_900186905.1_49923_G01_feature_table.txt.gz", compression="infer")$ CP020543 = pd.read_table("GCA_002079225.1_ASM207922v1_feature_table.txt.gz", compression="infer")
df_customers['number of customers'] = np.exp(df_customers['log number of customers'])
temps_df.Difference[1:4]
from scipy.cluster.hierarchy import dendrogram, linkage, set_link_color_palette
cityID = 'adc95f2911133646'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Colorado_Springs.append(tweet) 
data = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY)
df2.head()
month.columns = ['MONTH_'+str(col) for col in month.columns]
df2.drop('group',inplace=True, axis=1)$ df2.head(2)
df1['forcast']=np.nan
s519397_df["prcp"].min()
df = pd.read_csv('ab_data.csv')$ df.head()
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df_full = pd.DataFrame()$ df_list = []
dfU.shape[0] - (1257+420+2629)
from shapely.geometry import Polygon$
type(df.date[0])
pd.MultiIndex(levels=[['a','b'], [1,2]],$              labels=[[0,0,1,1], [0,1,0,1]])
warnings.simplefilter('ignore')
p_diffs.mean()
executable_path = {'executable_path': 'C:/Program Files (x86)/Google/Chrome/Application/chromedriver.exe'}$ browser = Browser('chrome', **executable_path, headless=False)$ url = 'https://mars.nasa.gov/news/'
h2o.init()$
fb = cb.organization('Facebook')
clean_stations.columns = ['station', 'latitude', 'longitude', 'elevation', 'name', 'country', 'state']
appmag_lim = 21.0$
tfidf = models.TfidfModel(corpus)
lr.fit(features_class_norm, overdue_transf)$     $ print_feature_importance(vectorizer.feature_names_, lr.coef_)
df = pd.DataFrame(np.random.randn(3, 4))$
transactions['TransactionDate']=pd.to_datetime(transactions['TransactionDate'])$ transactions
country_size=pd.value_counts(ac['Country'].values, sort=True, ascending=False)$ country_size.head()
obs_diff = p_treatment - p_control$ print('The observed difference in the conversion rates is {}.'.format(round(obs_diff,4)))
non_usa_states = ['ON', 'AP', 'VI', 'PR', '56', 'HY', 'BC', 'AB', 'UK', 'KA']$ print 'Total amount for locations outside USA: ', sum(df[df.state.isin(non_usa_states)].amount)$
sub = pd.DataFrame()$ sub['click_id'] = test_df['click_id']$ print("Sub dimension "    + str(sub.shape))
df_leiden = df_city[(df_city.Lon>=4.438) & (df_city.Lon<=4.525) & (df_city.Lat>=52.118) & (df_city.Lat<=52.187)]$ sns.pairplot(df_leiden, x_vars='Lon', y_vars='Lat', size=6)$ plt.show()
BTC['Smoother'] = BTC.apply(lambda row: smoother_function(row["Transfer_method"], row["Type"], row["BTC_amount"]), axis=1)
stock_df = spark.createDataFrame(stock_delimited_daily)
df.loc['1975-01-01']
our_nb_classifier = engine.get_classifier("nhtsa_classifier")
df = pd.read_csv('autos.csv', encoding='cp1252')$ print(df.describe())
df.loc[dates[0]]
soup.title.contents
prophet_model.fit(prophet_df)
labels = investments.copy()$ labels['invested'] = 1
df2['intercept']=1$ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']$ df2.head()
car=pd.read_csv('http://data.sfgov.org/resource/cuks-n6tp.csv')$ car.dtypes
y = x.astype(float)$ y.base is x
df.loc['a', 'ii', 'z']
joined.dtypes
df2.drop(['UK', 'country'], axis = 1, inplace = True)
summary = records.describe(include='all')$ summary
m=sm.Logit(f_new['converted'],f_new[['intercept','US','CA']])$ results=m.fit()$ results.summary()
df = pd.DataFrame(results)$ df = df.rename(columns={0: 'text'})
df_concensus['esimates_count'].describe()$
df_expand['Odds'] = temp.TeamOddsStd + df_expand.Offset
S_distributedTopmodel.basin_par.filename
pgh_311_data.info()
X[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
p3_table = profits_table.groupby(['Segment']).Profit.sum().reset_index()$ p3_result = p3_table.sort_values('Profit', ascending=False)$ p3_result.head()
a_active_devices_df['longest_activity_time'] = a_active_devices_df['last_activity_time'] - a_active_devices_df['first_activity_time']$ a_active_devices_df.head()
greater = (p_diffs > actual_diff).mean()$ print('Proportion of simulated differences that are greater than actual difference:', greater)
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner')$ df_new.head(10)
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
daily_trading_volume = [daily[6] for daily in json_data['dataset_data']['data'] if daily[6] != None]$ average = str(round(sum(daily_trading_volume)/len(daily_trading_volume),1))$ print('The average daily trading volume during 2017 was ' + average + '.')$
final_csv.sentiment_score.idxmax()$
obs_diff= new_page_converted.mean() - old_page_converted.mean()# differences computed in from p_new and p_old$ obs_diff
ser7 = pd.Series([9,8,7,6,5,34,2,98])$ ser7.head(7) $ ser7.tail(2)
EXIFtool_command = 'exiftool'+' -csv="'+basedirectory+projectname+'_exiftool.csv" '+imagepath$ EXIFtool_command$
df_clean3.loc[992, 'name'] = 'Quizno'
media_user_results_df = pd.DataFrame.from_dict(results_list)$ media_user_results_df.head(10)
station_cnt = session.query(Measurement.station).distinct().count()$ station_cnt
df_zillow.to_csv('cleaned_Zillow.csv', index = False)$ df_zillow = pd.read_csv('cleaned_Zillow.csv')
df_new[['US','UK']] = pd.get_dummies(df_new['country'])[['US','UK']]$ df_new.tail()
ts[pd.datetime(1951, 6, 1):pd.datetime(1952, 1, 1)]
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ print(last_date)$
AVG_daily_trading_volume = mydata['Traded Volume'].mean()$ AVG_daily_trading_volume
pd.set_option('display.mpl_style', 'default')$ plt.rcParams['figure.figsize'] = (15, 5)
print(temp_nc)$ for v in temp_nc.variables:$     print(temp_nc.variables[v])
Base = automap_base()$ Base.prepare(engine, reflect=True)
h4 = qb.History(spy.Symbol, 360, Resolution.Daily)$
tokenized_reviewText = [nltk.word_tokenize(text) for text in sdf.Norm_reviewText]$ tokenized_newReview = [nltk.word_tokenize(text) for text in ndf.Norm_reviewText]                        
X = joined.drop('CHURN', axis = 1)$ y = joined['CHURN']$ X.sample(n=5, random_state=2)
jevent_scores = jcomplete_profile['event_scores']$ esdf = pd.DataFrame.from_dict(jevent_scores)$ print(esdf[['event_type_id','model_scope_forecast_horizon','effective_date', 'score_value']])
twitter_archive.info()
!wget -O ChurnData.csv https://ibm.box.com/shared/static/8s8dn9gam7ipqb42cm4aehmbb26zkekl.csv
stations = df.station.unique()$ no_of_stations = len(stations)$ no_of_stations
number_of_commits = len(git_log.index)$ number_of_authors = git_log.author.nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df2['Agency'].value_counts()
kick_projects[['goal', 'pledged', 'backers','usd_pledged','usd_pledged_real','usd_goal_real','state']].corr()
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])
salida_all = getFacebookPageFeedData(page_id, access_token, 100)$ columns = ['post_from', 'post_id', 'post_name', 'post_type', 'post_message', 'post_link', 'post_shares', 'created_time']$ df_posts = pd.DataFrame(columns=columns)
df.time.unique().shape
df['created_at'] = pd.to_datetime(df['created_at'])
not_in_oz = stops.loc[~mask & (stops['operator'] == 'bac')].head(5)$ not_in_oz
StockData.loc[StockData['Date-Fri'] == 1].head()
pd.read_sql('SELECT * FROM experiments WHERE temperature = 375 ORDER BY irradiance DESC', conn, index_col='experiment_id')
exportparams = urllib.parse.urlencode({$     'action': 'tweet-export',$     'format': 'csv'})
results_df.to_csv("2018-04-09-Dow-NewsStats.csv", index=False)$
!hdfs dfs -put Consumer_Complaints.csv Consumer_Complaints.csv
df2 = df.sum(axis=0)$ display(df2)$
url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars"$ base_url = "https://astrogeology.usgs.gov"$ hemisphere_image_urls = []
g = sns.factorplot(x = 'gender', col = 'brand', data =df_ts, size = 10, kind = 'count')
agg_trips_data.plot.scatter(x='start_station_id',y='count', s= 20, c= 'r')$ agg_trips_data.plot.scatter(x='start_station_id',y='mean_duration')$ agg_trips_data.plot.scatter(x='start_station_id',y='total_duration')$
ab_df2.query('group == "treatment"')['converted'].mean()
sel_stats = [func.min(Measurement.tobs),func.max(Measurement.tobs), func.avg(Measurement.tobs)]$ session.query(*sel_stats).filter(Measurement.station == station_with_highest_observations).all()
print('The half percent of the distribution, median: {}'.format(df_measures['hour'].median()))
red.shape
(df.xs(symbol,level='symbol')['2011':].flow*100.).rename(symbol).plot.hist(bins=61,$ title='{}: Daily Change in Shares (%)'.format(symbol),figsize=(10,4),xlim=(-10,10),alpha=0.75)
mgxs_lib.by_nuclide = True
message_filename = data_file_path + "raw_data_messages.json"$ DF = pd.read_json(message_filename)$ print("... read in dataframe")
df2.user_id.nunique(), df2.shape
ab_dataframe['user_id'].nunique()
Base.prepare(engine, reflect=True)$
is_service_focused = []$ for i in range(0,2500):$     is_service_focused.append(0)
data3.to_csv('data3.csv')
df_sqr = df_final[features2]$ sqrt_val = df_sqr.apply(np.sqrt)$ sqrt_val.tail()
avgPurchU = train.groupby(by='User_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseU'})$ train = train.merge(avgPurchU, on='User_ID', how='left')$ test = test.merge(avgPurchU, on= 'User_ID', how='left')
engine = sqlalchemy.create_engine('mysql://asharma:PatuKhola1@orchard.cvt6t7xaljhq.us-east-1.rds.amazonaws.com:3306')$
new_page_converted = np.random.choice([0, 1], size = n_new, p = [1 - p_new, p_new])$ p_new_sim = new_page_converted.mean()
ORDER_BPAIR_POSTGEN = pd.merge(left=BPAIRED_GEN,right=ORDERS_GEN[['order_number','created_at']].astype(dtype),left_on='shopify_order_id',right_on='order_number')
cityID = '249bc600a1b6bb6a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Aurora.append(tweet) 
df_countries = pd.read_csv('countries.csv')$ df_countries.head()
abc = Grouping_Year_DRG_discharges_payments.groupby(['year','drg3']).get_group((2015,871))$ abc.head()
lr_best = gs.best_estimator_
joined=join_df(joined,trend_de,["Year","Week"],suffix='_DE')$ joined_test=join_df(joined_test,trend_de,["Year","Week"],suffix='_DE')$ sum(joined['trend_DE'].isnull()),sum(joined_test['trend_DE'].isnull())
active_with_return.iloc[:,1] = pd.to_datetime(active_with_return.iloc[:,1])
df['Unique Key', 'Created Date', 'Closed Date'].show(truncate = False)
c = y.ravel() # returns a reference to the same array if possible $ c.base is x
scores[scores.IMDB == min_IMDB]
df_twitter_copy['jpg_url'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['jpg_url'])$ df_twitter_copy['p1'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['p1'])$ df_twitter_copy['p1_conf'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['p1_conf'])
topTweeters = data.groupby(by=['userScreen', 'userName'])['Tweets'].count()$ topTweeters.sort_values(ascending=[False]).head(10)
df2['intercept'] = 1$ df2[['control','ab_page']]= pd.get_dummies(df2['group'])$ df2 = df2.drop('control',axis = 1)
Base.classes.keys()
sum(df2['user_id'].duplicated())
df_c.head()
df['x'].unique()
p_new = df2['converted'].mean()$ p_new
USvideos.info()
i = issues$ len(i[((i.activity <= '2012-01-01') & (i.activity > '2011-01-01'))])
desc_stats = desc_stats.transpose().unstack(level=0).transpose()$ desc_stats
pd.DataFrame(np.random.rand(3, 2),$             columns=['foo', 'bar'],$             index=['a', 'b', 'c'])
gs_lr_tfidf.fit(X_train, y_train) 
Series(np.random.randn(6), index=rng)
ts.shift(1,DateOffset(minutes=0.5))
class_merged.to_csv('class_merged_excl_hol.csv',sep=',')
df['intercept'] = 1$ df[['control', 'treatment']] = pd.get_dummies(df['group'])
(session.query(Measurement.station, Station.name, func.count(Measurement.station))$  .filter(Measurement.station==Station.station)$  .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).limit(1).all())
df.to_json('data\8oct_pre_processed_stemmed_polarity.json', orient='records', lines=True)
data_FCInspevnt["Inspection_number"] = data_FCInspevnt.groupby("brkey")["in_modtime"].rank(ascending=False)$ data_FCInspevnt['Inspection_number'] = data_FCInspevnt['Inspection_number'].astype('int64')$ data_FCInspevnt.head(15)
x.iloc[:3,:]
pd.set_option('display.max_colwidth', -1)$ df[['text', 'favorite_count', 'date']][df.favorite_count == np.max(df.favorite_count)]
pd.date_range(pd.datetime(2016, 1, 1), pd.datetime(2016, 7, 1), freq="W")
matchinglist = getmatchinglist()$ matchinglist.head()
malebyphase = malemoon.groupby(['Moon Phase']).sum().reset_index()$ malebyphase
building_pa_prc_shrink.to_csv("buildding_01.csv",index=False)$
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','Status']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)$
df_log.user_id.nunique()
df2t = df.query('group == "treatment" and landing_page == "new_page"')
store_items.loc[['store 1']]
unique_users = df.nunique()['user_id']$ print("The number of unique users in the dataset is: {}".format(unique_users))$
print('{0:.2f}%'.format((scores[4.0:5.0].sum()/total) * 100))
df.info()
KKK = df.groupby(["Source"]).mean()["Compounded Score"]$ overallDF = pd.DataFrame.from_dict(KKK)$ overallDF["Compounded Score"]
df.user_id.nunique()
mergde_data.loc[mergde_data['max']==0]
df1.append(df2).append(df3)$
final_df = diff_df.merge(final_sentiment_df, how = "outer")$ final_df$ round(final_df, 3)
print('reduce memory')$ utils.reduce_memory(membership_loyalty)$
df.set_index('UPC EAN', append=True, inplace=True)$ df.head(3)
Tue_index  = pd.DatetimeIndex(pivoted.T[labels==1].index).strftime('%a')=='Tue'
blurb_SVD = tSVD.fit_transform(blurbs_to_vect)
if pattern.search('AAbc') is not None:$     print('asdfdsf')
df['converted'].mean()
from scipy.stats import norm$ norm.cdf(1.3109241984234394), norm.ppf(1-(0.05))
df.isnull().any() 
np.exp(-0.0099), np.exp(-0.0507)
actual_diff = (df2[df2['group'] == "treatment"]['converted'].mean()) - (df2[df2['group'] == "control"]['converted'].mean())$ actual_diff
ride_level_data = ridedata_df.groupby(ridedata_df['city'])$ city_avg_fare = ride_level_data.mean().reset_index()$ city_nbr_rides = ride_level_data.count().reset_index()$
time_to_close["time2close"].plot(kind="hist")
joined=join_df(joined,googletrend,["State","Year","Week"])$ joined_test=join_df(joined_test,googletrend,["State","Year","Week"])$ sum(joined['trend'].isnull()),sum(joined_test['trend'].isnull())
lm = sm.OLS(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']])$ result3 = lm.fit()$ result3.summary()
Mars_soup = BeautifulSoup(html, 'html.parser')
data = aapl.get_call_data(expiry=aapl.expiry_dates[4])$ data.iloc[0:5:, 0:5]
sample = pd.read_csv('../assets/sampleSubmission')$ test = pd.read_csv('../assets/test')
df.to_excel("../../data/msft2.xlsx")
rural = merge_table.loc[merge_table["type"] =="Rural"]$ rural_table_df= rural.groupby(["city"])$ rural_table_df.max()
import pandas_datareader $ faamg = pandas_datareader.get_data_google(['FB','AAPL', 'AMZN','MSFT', 'GOOGL' ])['Close']$
plt.scatter(USvideos['dislikes'], USvideos['views'])$
df_n_rows = df.shape[0]$ df_n_rows
print(prec_nc)$ for v in prec_nc.variables:$     print(prec_nc.variables[v])
weather1.head()
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
containers = page_soup.find_all("section", {"class":"ref-module-paid-bribe"})
store_items = store_items.drop(['store 3'], axis = 0)$ store_items
soup = bs(response.text, 'html.parser')
item_similarity = skl.metrics.pairwise.cosine_similarity(data_matrix.T)$ user_similarity = skl.metrics.pairwise.cosine_similarity(data_matrix)
recipes.iloc[135598]
df_all_wells_wKNN_DEPTHtoDEPT.tail()
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])$ df_new.head()
device = train_data.groupby(['device.browser',"device.operatingSystem","device.isMobile"]).agg({'totals.transactionRevenue': 'sum'})$ device = device.groupby(level=[0]).apply(lambda x:100 * x / float(x.sum()))$ device #.reset_index()
newdf.info()
df_clean.head()
forest_clf = RandomForestClassifier(random_state=0)$ forest_clf.fit(X_train, Y_train)
from scipy.stats import norm$ norm.ppf(0.95)$
df['pb_prod'] = df['pledged'] * df['backers']$ df['pb_avg'] = df[['pledged', 'backers']].mean(axis=1)
a.iloc[3]
base_url = 'https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_'
menes_postulados.groupby()
old_n = df2[df2['group'] == 'control'].shape[0]$ old_n
busy_stations = session.query(Measurement.station, func.count(Measurement.tobs)).filter(Station.station == Measurement.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()$ busy_stations
df[df['group'] == 'control'].query("landing_page == 'new_page'").index.values$ df2 = df.drop(df[df['group'] == 'control'].query("landing_page == 'new_page'").index.values)
%%time$ table = pq.read_table(data_dir + file_name + '.parq')
billtargs['billtext'] = np.nan
print(len(sessions_p_caption), len(sessions_p_cuepoint), sessions.id.count())$
tmax_day_2018 = xr.open_dataset('/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/maximum temperature/tmax.2018.nc', decode_times = False)
df_select_cats = df_select.copy()$ df_select_cats = df_select_cats.groupby(['Categories'], as_index=False).mean()$ df_select_cats
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country'])[['UK', 'US', 'CA']]$ df_new.head()
train.StateHoliday = ( train.StateHoliday != '0' )$ test.StateHoliday = ( test.StateHoliday != '0')
from statsmodels.api import Logit$ model = Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = model.fit()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative = 'smaller')$ print(' z-score = {}.'. format(round(z_score,4)))$ print(' p-value = {}.'. format(round(p_value,4)))$
df.to_csv('citation-data-clean.csv', index=False)$
events['payload'][events['type'] == 'PushEvent'].iloc[0]
c = MongoClient().sb1.music
x_train, x_test, y_train, y_test = train_test_split(bow_df, amazon_review['Sentiment'], test_size=0.2)$ x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1.0/3.0)
query = session.query(Measurement)$ rain = query.order_by(Measurement.date.desc()).all()$ print(rain[0].date)
vehicle_type_unique = list(autos.loc[autos["vehicle_type"].notna() == True, "vehicle_type"].unique())$ gearbox_unique = list(autos.loc[autos["gearbox"].notna() == True, "gearbox"].unique())$ fuel_type_unique = list(autos.loc[autos["fuel_type"].notna() == True, "fuel_type"].unique())
info_final.head(1)
pivoted = Fremont.pivot_table('Total', index=Fremont.index.time, columns=Fremont.index.date)$ pivoted.plot(legend=False, alpha=0.01);
print(autos['price'].max())$ print(autos['price'].min())
t3[t3['retweeted_status'].notnull()== True]
to_datetime = lambda x: pd.to_datetime(x)
df2 = df2.drop_duplicates(['user_id'])$ df2[df2.duplicated(['user_id'])]
R_sq=1.0-(SSE/SST)$ R_sq
y.name = "huhu"$ y = y.rename("jsdfjkdsfhsdfhdsfs")$ y
movies['year']=movies['title'].str.extract('(\(\d\d\d\d\)$)',expand=True)$ movies['year'] = movies['year'].str.replace('(', '')$ movies['year'] = movies['year'].str.replace(')', '')
from tempfile import mkstemp$ fs, temp_path = mkstemp("gensim_temp")  $ model.save(temp_path)  
x=[0,1,2,3,4,5]$ network_simulation[network_simulation.generations.isin([5])]$
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)
index = [('California', 2000), ('California', 2010), ('New York', 2000), ('New York', 2010), ('Texas', 2000), ('Texas', 2010)]$ index = pd.MultiIndex.from_tuples(index)$ index$
year_ago = datetime.now().date()$ query.filter(cast())
df.gender.replace('-unknown-', 'UNKNOWN', inplace=True)$ df.gender.replace('OTHER', 'UNKNOWN', inplace=True)$ df.gender.unique()
len(df2.user_id.unique())
y = train.rating
auth = tweepy.OAuthHandler(api_key, api_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, wait_on_rate_limit_notify=True, wait_on_rate_limit=True)
! rm -rf recs3$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models3 --outdir recs3
sales_update = sales.ix[:,'2015 Sales':]$ sales_update.head()
df_2009['bank_name'] = df_2009.bank_name.str.split(",").str[0]$
df_tick_clsfd_sent = df_tick.join(df_amznnews_clsfd_2tick)$ df_tick_clsfd_sent.info()
df2['timestamp'] = pd.to_datetime(df2['timestamp'])
api_json = json.loads(api_call.text)$ for x in api_json['messages']:$     print(x['body'])$
imgp_clean = imgp_clean.drop(imgp_clean[(imgp_clean.p1_dog == False) & (imgp_clean.p2_dog == False) & (imgp_clean.p3_dog == False)].index)$
prcp_12months = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > '2016-08-23').order_by(Measurement.date).all()$ prcp_12months$
df_control = df2.query('group=="control"')$ y_control = df_control["user_id"].count()$
user_unique= df.nunique()['user_id']$ len(df.user_id.unique())$ print("Number of unique users is : {}".format(user_unique))
festivals_clean.info()
list(np.log(df2.cv))$
int_tel_sentiment[:100].plot(figsize=(20, 20))
%%html$ <img src = "https://data.globalchange.gov/assets/e5/ee/9329d76bf3f5298dad58d85887bd/cs_ten_indicators_of_a_warming_world_v6.png", width=700, height=700>
SAMPLES = 100; gamma_vals = np.logspace(-2, 3, num=SAMPLES)$ gamma_val, gamma_sr = quick_gamma(gamma_vals, consol_px, hist_window, lb, frequency, min_gross, max_gross, min_w, max_w)$ gamma_val, gamma_sr
ind_result_list=ind_result.tolist()$ ids=independent_dataset['id'].tolist()
sess = data_t[data_t['Session'] == data_t.ix[0, 'Session']]$ fg, ax = plt.subplots()$ sess.plot(y='X', ax=ax)$
logit3 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA','US']])$ r3 = logit3.fit()$ r3.summary()
gene_count_df = chromosome_gene_count.to_frame(name='gene_count').reset_index()$ merged_df = gdf.merge(gene_count_df, on='seqid')$ merged_df
np.linspace(0, 100, 5)
release_table = release_table.drop_duplicates()$ release_table.shape
dict(list(result.items())[:20])
BUMatrix.plot(x='eventOccurredDate', y=['event_Observation','event_Incident'],style=".",figsize=(15,15))$ plt.show()$
movies_df = pd.read_csv('movies.csv')$ ratings_df = pd.read_csv('ratings.csv')$ movies_df.head()
bymin = walk.resample("1Min")$ bymin.resample('S').mean()
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new['country'].astype(str).value_counts()
print(grid.best_score_)$ print(grid.best_params_)
liquor2015_fy = liquor2015.SaleDollars.groupby(liquor.StoreNumber).agg(['sum'])$ liquor2015_fy.columns = ['Total_Sales']$ liquor2015_fy.tail()
min_IMDB = scores.IMDB.min()
train_labels = one_hot_matrix(dflong[dflong['Date'].isin(train_dates)]['Y'].as_matrix(), 3) $ validation_labels = one_hot_matrix(dflong[dflong['Date'].isin(validation_dates)]['Y'].as_matrix(), 3)$ test_labels = one_hot_matrix(dflong[dflong['Date'].isin(test_dates)]['Y'].as_matrix(), 3)
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)$ results.summary()$
Combined_data = Combined_data.fillna('unknown')$ Combined_data = Combined_data.set_index('Name')$ Combined_data
payments_NOT_common_all_yrs = (df_sites_NOT_common_DRGs.groupby(['id_num','year'])[['disc_times_pay']].sum())$ payments_NOT_common_all_yrs = payments_NOT_common_all_yrs.reset_index()$ payments_NOT_common_all_yrs.head()
1/np.exp(.0506), 1/np.exp(.0408)
print(plan.keys())
delays = pd.read_csv('output.csv', header=None)$ delays.columns = ['Date','ID','Delay','Latitude','Longitude']$ delays_geo = GeoDataFrame(delays)
dfm = dfn.drop(['usd_pledged','goal','state','slug','currency','deadline','state_changed_at','created_at','backers_count','spotlight','period'], axis=1).copy()
RNPA_new_8_to_16wk_arima = RNPA_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ RNPA_new_8_to_16wk_arima.index = RNPA_new_8_to_16wk_arima.index.date
alpha = 0.05$ np.random.seed(999)$ dist_n = (np.random.randn(10000) + 5) * 4 # +5 fixes mean, *4 fixes stdev
new_sample = new.copy()
precipitation_df = pd.DataFrame(precipitation, columns=['date', 'prcp'])$ precipitation_df.set_index('date', inplace=True)$ precipitation_df.head()
cityID = '7f061ded71fdc974'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Montgomery.append(tweet) 
ab_df.user_id.nunique()
p_old = df2['converted'].mean()$ print(p_old)
print '\n DataFrame df_totalConvs_day, sorted in descending order', df_totalConvs_day.shape$ df_totalConvs_day.sort('conversions', ascending=[0]).head(10)
grouped_df = news_sentiment_df.groupby('News_Source')$ grouped_compound = grouped_df['compound'].mean()$ grouped_compound
df2.drop(df2.index[2862], inplace=True)
api_key = '25bc90a1196e6f153eece0bc0b0fc9eb'$ units = 'Imperial'$ url = 'http://api.openweathermap.org/data/2.5/weather'
import tweepy$ import pandas as pd$ import matplotlib.pyplot as plt
pd.read_json("json_data_format_columns.json", orient="columns")
df.head(10)
screen_name="SKinnock"$ data.to_csv('%s_likes_rts.csv' % screen_name, sep=',', encoding='utf-8')$
station_count = session.query(func.distinct(Measurement.station)).count()$ station_count$
for title in soup.find_all('a'):$     print(title.text)$
df2.drop_duplicates(['user_id'], inplace=True)
contractor_clean[contractor_clean.city.isnull()]
df.converted.mean()
df2 = df.drop(df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page == "new_page") or (group == "control" and landing_page != "old_page") or (group != "control" and landing_page == "old_page")').index)
xmlData[xmlData['street'].str.match('^(?:[A-Z]).*$')][['street','city','address']]
tzs = tweets_df['userTimezone'].value_counts()[:10]$ print(tzs)$
html = requests.get(marsweath_url)$ soup = bs(html.text, 'html.parser')
df_concat_2["time"].str.split(':')$ df_concat_2["time"] = df_concat_2["time"].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
np.exp(-1.7227), np.exp(-1.3968), np.exp(-1.4422)$
sessions.head(2) 
New_York = New_York.sort_values('date', ascending=True)$ plt.plot(New_York['date'], New_York['m_time'])$ plt.xticks(rotation='vertical')
!ls -l ./cnn/questions/training/ | grep -v ^l | wc -l
data.drop(['ceil_15min'], axis=1,inplace=True)
year4 = driver.find_elements_by_class_name('yr-button')[3]$ year4.click()
recipes['ingredients'].str.len().describe()
trip_data = pd.read_csv("green_tripdata_2015-09.csv", header=0, delimiter=",")$ trip_data.head(5)
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))$
tdf = sns.load_dataset('tips')$ tdf.sample(5)
df.sort_values(['Year'], ascending=True).head(5)
m = Prophet(interval_width=0.95)$ m.fit(df);
df_students.columns
lm(Combineddata.Ad_Cost, Combineddata.Sales_in_CAD, Combineddata)
train.dropna(inplace=True)$ test.dropna(inplace=True)
df_archive_csv=pd.read_csv('twitter-archive-enhanced.csv')$ df_archive_csv.tweet_id=df_archive_csv.expanded_urls.str.extract('(\d{18})')
pd.date_range('2015-07-03', periods=9)
Twitter_map2.save('Twitter_FSGutierres_map.html') #save HTML
from scipy.stats import norm$ print(norm.ppf(1-(0.05)))
print('No missing values \nIsnull: {}'.format(df.isnull().values.any()))$
amazon_gp = amazon_gp.applymap(lambda x : round(x, 1))$ amazon_gp[['amazon_gmv12','total_amount_cents','cnt']]
index = similarities.MatrixSimilarity(doc_vecs, $                                       num_features=topics)$ sims = sorted(enumerate(index[doc_vecs[6]]), key=lambda item: -item[1])
Twitter_map = folium.Map([45.955263, 8.935129], tiles='cartodbdark_matter', zoom_start = 5)$ Twitter_map
df["DATE_TIME"] = pd.to_datetime(df.DATE + " " + df.TIME, format = "%m/%d/%Y %H:%M:%S")$ df
df_vow['Year'] = df_vow.index.year$ df_vow['Month'] = df_vow.index.month$ df_vow['Day'] = df_vow.index.day
df1.join([df2, df3, df4]).fillna('')
tuna_neg = mapped.filter(lambda row: row[4] < 0)$
hawaii_df.describe()
highest_temp_station_no = active_stations.iloc[0,0]$ highest_temp_obs = active_stations.iloc[0,1]$ print(f"The station with the highest number of temperature observations is {highest_temp_station_no} with total observations of {highest_temp_obs}." )$
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=3w2gwzsRAMrLctsYLAzN')$ json_data = r.json()$ print(type(json_data))
twitter_archive_clean.loc[(twitter_archive_clean.rating_denominator< 10),"rating_denominator"]=10
id_list = df_tweet_clean[df_tweet_clean.tweet_id.isin(df_image_clean.tweet_id)].tweet_id
timestamp_left_df['time_delta_seconds'] = (timestamp_left_df['create_time'] - timestamp_left_df['timestamp']) / np.timedelta64(1, 's')$ timestamp_left_df.head()
df = pd.read_csv(dp('tmdb_5000_movies.csv'), encoding='utf-8')$ df
df_converted = df.converted.mean()$ print("The proportion of users converted is {}%.".format(round(df_converted * 100)))
tables=db_engine.table_names()$
conversion_rates = total_Visits_Convs_month_byMC[['marketing_channel', 'conversion_rate']]$ print'conversion_rates', conversion_rates.shape$ conversion_rates.head()
df_pop['pop_t-1'] = df_pop['population'].shift(1) $ df_pop['pop_change'] = df_pop['population']- df_pop['pop_t-1']$
crime_df.tail()
tOverweight = len(df[(df['bmi'] >= 25.0) & (df['bmi'] < 30.0)])$ tOverweight
merge_table_tc= merge_table.groupby(["type", "city"])$ merge_table_tc.max()
searched_tweets = [status for status in tweepy.Cursor(api.search, wait_on_rate_limit=True,                                        wait_on_rate_limit_notify=True, $ since='2018-08-05', until='2018-08-06', q=query).items(max_tweets)]$
df2.loc[df2.user_id.duplicated(), :]['user_id']
for i, row in problems.iterrows():$     srx, srr = row.srx, row.srr$
customers_arr = np.array(cust_list)$ items_arr = np.array(item_list)
stories = pd.read_json('https://lobste.rs/hottest.json')$ stories.head()
TestData_ForLogistic.columns
INT['Create_Date']= pd.to_datetime(INT.Create_Date)
fps.pyldavis_fp
workspace_uuid = ekos.get_unique_id_for_alias(user_id, 'lena_indicator')$
df.asfreq('W',method= 'ffill')
start = datetime.datetime(2010, 1, 1)$ df = web.DataReader("GOOG", 'yahoo', start)$ df.head()
session.query(Station).count()$
df_ratings.drop('video_watched_type', axis=1, inplace=True)$ df_ratings.drop('rating_date_creation', axis=1, inplace=True)$ df_ratings
top_10_authors = git_log.author.value_counts().nlargest(10)$ top_10_authors
payload_scoring = {"fields":X_test.columns.tolist(), "values": values}$ print(payload_scoring)
p_diffs = np.array(p_diffs)$
columns = inspector.get_columns('stations')$ for c in columns:$     print(c['name'], c["type"])$
port.positions.frame
adj_close = all_data[['Adj Close']].reset_index()$ adj_close.head()
df3 = df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')]$ df4 = df[(df['group'] != 'treatment') & (df['landing_page'] == 'new_page')]$ df3.shape[0] + df4.shape[0]
df[df.duplicated(['user_id'], keep=False)]
client = boto3.client('s3')$ with open(datafile, 'wb') as f:$     client.download_fileobj(bucket, prefix, f)
payload = "elec,id=500 value=24 "#+str(pd.to_datetime('2018-03-05T19:29:00.000Z\n').value // 10 ** 9)$ r = requests.post(url, params=params, data=payload)
df=df[["tripduration","date"]]$ df.head()
dfA_stn.to_csv('dfA_stn.csv')
M_data = session.query(Measurement).first()$ M_data.__dict__
org = "a_unique_organizations.csv"$ unique_org = pd.read_csv(org, encoding="iso-8859-1")$ print(len(unique_org.index)) 
%matplotlib inline$ import matplotlib.pyplot as plt$ import seaborn as sns
df1 = pd.read_csv('twitter-archive-enhanced.csv')
df2.count()
station_count = session.query(func.count(Station.id)).all()$ print(station_count)
print(dfd.hspf.describe())$ dfd.hspf.hist()
plt.rcParams['figure.figsize'] = [16,4]$ plt.plot(pd.to_datetime(mydf2.datetime),mydf2.fuelVoltage, 'g.');$ plt.xlim(datetime.datetime(2018,2,8),datetime.datetime(2018,3,25))
conv_users = df2.query('converted == 1').shape[0]$ p1 = float(conv_users/df2.shape[0])$ print("The probability of an individual converting regardless of the page is {:.4f}".format(p1))$
n_new = df2['group'].value_counts()[0]$ n_new$
tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')$ tips['tip'] = pd.to_numeric(tips['tip'], errors = 'coerce')$ print(tips.info())$
fig, ax = plt.subplots(figsize = (14.5,8.2))$ ax.hist(events_df['words_count'], 50, facecolor='green', alpha=0.75)$ plt.show()
names=my_zip.namelist()$ size= [f.file_size for f in my_zip.filelist]$
df.isnull().sum()
pivoted.T.shape$
tweet_json = pd.read_json('tweet_json.txt',lines=True)$ tweet_json.to_csv('tweet_json.csv')
df = pd.read_csv('ab_data.csv')$ df.head()
date = session.query(prcp.date).order_by(prcp.date.desc()).first()$ print(date)
missing_values = missing_values_table(perf_train)$ missing_values.head(20)
df2[df2.duplicated(['user_id'], keep=False)]
coming_next_reason.columns = ['NEXT_'+str(col) for col in coming_next_reason.columns]
animals = pd.Series([1,5,2,5], index=["cats", "dogs", "chickens", "spiders"])$ animals.plot(kind="bar", rot=0);
df2_dup = df2[df2.duplicated(['user_id'], keep = False)]$ df2_dup
test_array = np.concatenate([test_active_listing_dummy, test_pending_ratio_dummy], axis=1)$ print(test_array.shape)
obs_p_new = df2.query('group == "treatment"')['converted'].mean()$ obs_p_old = df2.query('group == "control"')['converted'].mean()$ obs_p_diff = obs_p_new - obs_p_old
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);$
noise_data_midnight = noise_data[noise_data["Created Date"].dt.hour==0]$ noise_data_noon = noise_data[noise_data["Created Date"].dt.hour==12]$ print(noise_data_midnight.shape, noise_data_noon.shape)
count_by_insertid = Counter(df.insert_id)$ insertid_freq = {x : count_by_insertid[x] for x in count_by_insertid if count_by_insertid[x] > 1 }$ print('Duplicated insert_ids: {}'.format(len(insertid_freq.keys())))
results.to_csv("mb.csv")$ results.to_pickle("football_pickle")$ pd.read_pickle("football_pickle")
tweets_df.loc[tweets_df.language == 'und', :]$
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])
df['is_rank_1'] = False$ df.loc[df['Rank'] == 1, 'is_rank_1'] = True
print len(hpdpro)$ data = pd.merge(data, hpdpro, how='outer', on='ComplaintID')$ print len(data)
df2['intercept']=1$ df2[['other_page','ab_page']]= pd.get_dummies(df2['group'])
new_page_rows = len(df2.query("landing_page == 'new_page'"))$ total_rows = df2.shape[0]$ print('New page probability :: ',new_page_rows/total_rows)
dbData.head(7)  # NaN's show up when the field has no data.  Need both masses, eccentricity, semimajor axis, $
dfg = dfg.set_index(['drg3', 'discharges'])$
precipitation_measurement_df.describe()
all_res = pd.read_csv("/Users/sdas/GoogleDrive/Papers/DeepLearning/DLinFinance/SP_Data_shared/DLIndex_Random_results_30_10000_30.csv")$ all_res.head()$ all_res.describe()
subset_data = pd.merge(interest_dates, product_time, left_index=True,#left_on= 'date', $                        right_index=True, how='inner') #Match dates and merge
ttTimeEntry.drop_duplicates(subset = mainkey + ['DT'], inplace = True)
df.isnull().sum()
total_ridership_list = pd.Series(ridership['INCR_ENTRIES']).tolist()$ indices = range(len(total_ridership_list))
df.info()
StockData = StockData.set_index('Date')$ StockData.head()
driver = webdriver.Chrome()
(p_diffs>p_diff).mean()
df_activity_prediction = \$     df_active_user_prediction[df_active_user_prediction['user_id'].isin(active_users)]\$     .merge(df_active_user_metrics,on="user_id", how="left")
df.sort_index(axis=1,ascending=False)
plot_BIC_AR_model(data=therapist_duration.diff()[1:], max_order_plus_one=10)
master_df.name.isnull().sum()
logit_control = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ result_control = logit_control.fit()
rain_df = pd.DataFrame(rain)$ rain_df.head()$
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Pacific').strftime('%D: %r') + " (PST)"$ date_min = date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Pacific').strftime('%D: %r') + " (PST)"
twitter_daily_df = twitter_daily_df.join(distinct_users, ["Day","Company"]).orderBy('Day','Company')
final_data["clean_titles_cat"] = final_data["clean_titles"].astype('category').cat.codes$ final_data.head()
import numpy as np$ np.random.randint(1,10,len(rng))$
print(100*dedups['name'].count()/df['name'].count())$
X = dataset[dataset.columns[1:]]$ Y = np.array(dataset["label"])
model = os.path.join('files', '2008--Mendelev-M-I--Al--111-GSF.xml')$ gamma = am.defect.GammaSurface(model=model, box=box)
list(new_df.dropna(thresh=int(new_df.shape[0] * .9), axis=1).columns)
sns.factorplot('lgID',hue='divID',kind='count',data=wcPerf1_df)
cityID = '8173485c72e78ca5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Atlanta.append(tweet) 
p_converted2 = df2['converted'].mean()$ print('The probability of an individual converting: ', p_converted2)
qrt = closePrice.resample('Q').mean()
acs_df.head()
Ralston["TMAX"].mean()$
archive_clean = archive_clean[archive_clean['in_reply_to_status_id'].isnull()]$ archive_clean = archive_clean[archive_clean['retweeted_status_id'].isnull()]
@functions.udf$ def lowercase(text):$
data_read = pd.read_csv("data_music_replay_train")$ data = data_read.copy()
lgb_mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', $           objective = 'binary')$
keep_cols = ['Follow up Telepsychiatry', 'Follow up', 'Therapy Telepsychiatry', 'Returning Patient', 'Returning Patient MD Adult']$ dr_existing = dr_existing[dr_existing['ReasonForVisitName'].isin(keep_cols)]
df.median()
players = pd.read_sql_query('select * from Player', conn)  # don't forget to specify the connection$ print(players.shape)$ players.head()
tmp = df[selected_features].join(outcome_scaled).reset_index().set_index('date')$ tmp.dropna().resample('Q').apply(lambda x: x.corr()).iloc[:,-1].unstack().iloc[:,:-1].plot()$
tweet = api.get_status(892420643555336193)$ print(tweet.text)
siteInfo = siteInfo.query('Range >= 10 & EndYear >= 2017')$ siteInfo
testurl = r"https://www.cobbgis.org/openimage/bravescam/Cam128/Cam_12818-06-27_21-00-09-03.jpg"$ get_datetime_from_cobburl(testurl)
df2 = merged_pd.loc[:,['visitors', 'weekday', 'holiday_flg']]$ df2.groupby(['weekday','holiday_flg']).agg(['count',np.sum])
kochdf = kochdf.dropna()
doc_df = pd.read_sql("SELECT * FROM Documents", con=engine)$ doc_df.head(3)
df_arch_clean['dog_stage'] = df_arch_clean['text'].str.extract('(doggo|floofer|pupper|puppo)', expand=True)$
tweets_clean.to_csv('tweets_clean_final.csv', index = False)
missing_incorrect_names = df_complete_a.query("name=='None' or name=='an' or name=='a' or name=='the' or name=='just'")
df.index
New_York.dtypes
flights2.passengers.plot()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$ print(new_page_converted)
Station = Base.classes.station$ stations = session.query(Station).count()$ print(f'There are {stations} stations in the database')
measure_df.head()$
df3['DATE'] = df3['DATE'].apply(lambda x: dateutil.parser.parse(x))
df.columns = ['C/A', 'UNIT', 'SCP', 'STATION', 'LINENAME','DIVISION', 'DATE', 'TIME', 'DESC', 'ENTRIES', 'EXITS']
model = ols("happy ~ age + income", training).fit()$ model.summary()
pred_labels = rdg2.predict(test_data)$ print("Training set score: {:.2f}".format(rdg2.score(train_data, train_labels)))$ print("Test set score: {:.2f}".format(rdg2.score(test_data, test_labels)))
twitter_data = pd.read_csv("twitter-archive-enhanced.csv")
engine = create_engine("sqlite:///../Resources/hawaii.sqlite", echo=False)
df_new = df2.query('landing_page == "new_page"')$ p_new = df2['converted'].mean()$ print(p_new)
df2['converted'].mean()
predictions_count = model_count_NB.predict(X_test_count)$ predictions_tfidf = model_tfidf_NB.predict(X_test_tfidf)
df2_count_by_liker = df2.groupby("liker").sum()$ df2_count_by_liker[:6]
kushy_prod_data_path = "products-kushy_api.2017-11-14.csv"$ kushy_prod_df = pd.read_csv(kushy_prod_data_path, low_memory=False)$ kushy_prod_df.tail(10)
crimes['2011-06-15']['HOUR'].value_counts().head(5)
auth = tweepy.OAuthHandler(config.consumer_key, config.consumer_secret)$ auth.set_access_token(config.access_token, config.access_token_secret)$ api = tweepy.API(auth)
station_obs_df = pd.DataFrame(sq.station_obs(), columns = ["Station name", "Observation counts"])$ station_obs_df
ndf = ndf.assign(Norm_summary = normalize_corpus(ndf.summary))
genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId'])$ genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ genreTable.head()
gs.fit(x_train,y_train)
data_for_model = pd.read_pickle('data_for_model')$ df_modeling = data_for_model.copy(deep=True)
plt.show()
fig, ax = plt.subplots(figsize=(12, 4))$ births_by_date.plot(ax=ax)
conn = 'mongodb://localhost:27017'$ client = pymongo.MongoClient(conn)
df2 = df.query('group == "treatment" and landing_page == "new_page" or group == "control" and landing_page == "old_page"  ')$
def fix_runtime(runtime):$     return runtime.split(' ')[0]
difference = (actual_diff < p_diffs).mean()$ perc = difference * 100$ print("{}% are greater than the actual difference.".format(perc))
for item in result_set:$     print(item.index,item.education)
apple.index.is_unique
final_df = diff_df.merge(final_sentiment_df, how = "outer")$ final_df
tdf = sns.load_dataset('tips')$ tdf['size'].sample(5)
sdf = sdf.assign(Norm_reviewText = normalize_corpus(sdf.reviewText))
df.drop(["city","lat","lon"], inplace = True, axis = 1)
df = pd.read_csv('ab_data.csv')$ df.head()
import pandas as pd$ df = pd.DataFrame(query_op)$
df.sort_values(by='Year',ascending=True).head(5)
station_count = station_df['Station ID'].nunique()$ print(station_count)
print('reduce memory')$ utils.reduce_memory(transactions_price_plan_days)
df2.user_id[df2.user_id.duplicated() == True]
twitter_Archive.drop(['retweeted_status_user_id',$                'retweeted_status_id',$                'retweeted_status_timestamp'], axis=1,inplace=True)
cnn_df = constructDF("@CNN")$ display(constructDF("@CNN").head())
df['VALUE'] = zip(df['DIVISION'], df['DATE'], df['TIME'], df['DESC'], df['ENTRIES'], df['EXITS'])$ df['VALUE'] = df['VALUE'].apply(lambda x: list(x))
tweet = tweets[1]$ pp.pprint([att for att in dir(tweet) if '__' not in att])
results1.summary()
new_page_converted.mean() - old_page_converted.mean()
session.query(Measurements.station,func.strftime("%m-%d", Measurements.date),func.avg(Measurements.prcp)) \$        .group_by(Measurements.station,func.strftime("%m-%d", Measurements.date)).limit(5).all()
n_old = df2.query("group == 'control'").shape[0]$ n_old
texts = df[df['section_text'].str.contains('fees')][['filename','section_text']].values$ len(texts)
user = user[user.is_enabled >= 0]
df2[df2.duplicated('user_id')]
final_valid_pred_nbsvm1 = valid_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
num_uniqueid = df.user_id.nunique()$ print('No. of unique user ids: ' + str(num_uniqueid))$
s1 = pd.Series(np.arange(1, 6, 1)) $ s2 = pd.Series(np.arange(6, 11, 1)) $ pd.DataFrame({'c1': s1, 'c2': s2})
records3.loc[(records3['Graduated'] == 'Yes') & (records3['Days_missed'].isnull()), 'Days_missed'] = grad_days_mean$ records3.loc[(records3['Graduated'] == 'No') & (records3['Days_missed'].isnull()), 'Days_missed'] = non_grad_days_mean
pres_df.drop('split_location_tmp', inplace=True, axis=1)$ pres_df.head(2)
results = soup.find_all('div', class_='slide')$ print(results)
df2['monthOfRegistration'].value_counts().plot(kind='bar')
min_mean_km = price_vs_km["mean_odometer_km"].min()$ price_vs_km.loc[price_vs_km["mean_odometer_km"] == min_mean_km, :]
results.summary()
df2['converted'].mean()
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ last_date
x_mean, up, low = bollinger_band(DataFrame(values), 20, 1.5)
n_new = df2[df2['landing_page']=="new_page"].count()[0]$ n_new
p_new = df2[df2['converted']==1]['user_id'].count() / df2.shape[0]$ p_new
old_page_converted = np.random.choice([0,1], size = (145274), p = [0.88, 0.12]) $ p_old = (old_page_converted == 1).mean()$
%%time$ ddf = dd.read_parquet(data_dir + file_name + '.parq', index='Date')
sum(contractor.city.isnull())
from gensim.models import Doc2Vec$ model = Doc2Vec.load('/tmp/movie_model.doc2vec')
for i in range(len(tweets['Tweet'])):$     tweets['Tweet'][i] = " ".join([word for word in tweets['Tweet'][i].split()$                                 if 'http' not in word and '@' not in word and '<' not in word and '#' not in word and '\\xf' not in word])
print (collData.getNumPartitions()) # this is the number of CPU cores$
pd.merge( users,sessions, right_on=['UserID','SessionDate'],left_on=['UserID','Registered'],how='inner')
from pandas.tseries.offsets import *$ d + BusinessDay()
list_of_features.append(("Age", series_of_converted_ages))
y_pred_rf = rf.predict(X_test)$ y_train_pred_rf=rf.predict(X_train)$ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(rf.score(X_test, y_test)))
cohorts = cohorts.reset_index()$ cohorts = cohorts.set_index(['CohortGroup', 'CohortPeriod'])$ cohorts.head()
df_date = df_date['unique_date'].apply(lambda x: x.strftime("%Y-%m-%d %H:%M:%S"))$
df = pd.read_csv(location+'paula_ratings.csv.bz2', parse_dates=['date_of_birth', 'date_of_registration'])
df = pd.read_csv('ab_data.csv')$ df.head()
league.name.unique()
df2[df2.duplicated(['user_id'], keep=False)]
print(type(df.groupby("grade").count())) # as data frame ('id' column and 'raw_grade' column both contained)$ df.groupby("grade").count()
df.count()
gender = records['Gender'].value_counts()$ gender['Missing'] = len(records) - gender['Male'] - gender['Female']$ gender
ndf.head()
import tensorflow as tf$ sess=tf.Session()    $ saver = tf.train.import_meta_graph('/tmp/testing/stock_prediction_12_21/model.ckpt-1000.meta')$
crimes.PRIMARY_DESCRIPTION.value_counts()
dfa=new_table.groupby("type")
df_G.val = np.random.rand(7).round(1)$ df_G$ pd.get_dummies(pd.cut(df_G['val'], 3, labels=list('XYZ')), prefix='dummy')
url_domains = grouped['domain'].agg({'domain': lambda x: np.unique(x)})$ unique_urls = pd.merge(unique_urls, url_domains)$ unique_urls.head()
p_control = df2.query('group == "control"').converted.mean()$ p_control
ward_df['Crime Count'] = ser.values
precipitation_df.set_index("date", inplace=True)$ precipitation_df.head(15)
ctc = ctc.fillna(0)
data_matrix = np.zeros((n_user, n_item))$ for line in dfToProcess.itertuples():$     data_matrix[customerList.index(line[1]), productList.index(line[2])] = line[3]$
QLESQ = QLESQ.dropna(axis=1, how='all')$ QLESQ.columns
cityID = '7a863bb88e5bb33c'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anchorage.append(tweet) 
df_geo_segments.crs = {'init': 'epsg:4326'}
X_copy['outproc_flag'] = X_copy['outproc_flag'].apply(lambda x: np.where(x=='N',0,1))
new_discover_sale_transaction = post_discover_sales[post_discover_sales['Email'].isin(new_customers_test['Post Launch Emails'].unique())]$ new_discover_sale_transaction['Total'].mean()$
(null_value > obs_diffs).mean()
col_list = list(df.columns.values)$ print(col_list)
unique_users=df['user_id'].nunique()$ print("The total number of unique users are : {}".format(unique_users))
p_diffs = np.array(p_diffs)$ plt.hist(p_diffs);
    no_punc = [char for char in string.lower() if char not in punctuation]$     no_punc = ''.join(no_punc)$     return [word for word in no_punc.split() if word not in stopwords.words('english')]
kickstarters_2017 = pd.read_csv("ks-projects-201801.csv")$ kickstarters_2017.head()
(~autos['registration_year'].between(1900,2016)).sum()/autos.shape[0]
with client_hdfs.read('/mydata/helloworld.csv', encoding = 'utf-8') as reader:$     readdf=pd.read_csv(reader,index_col=0)
commit_nova_2015 = pandas_ds[(pandas_ds["gerrit_closing_date"] >= '2011-01-01') & (pandas_ds["gerrit_closing_date"] <= '2012-01-01') & (pandas_ds["current_status"] == 'MERGED') & (pandas_ds["gerrit_tracker"].str.contains("/tempest"))];$ commit_nova_2015
td_wdth = td_norm * 1.5$ td_alph = td_alpha$ td[:] = td[:].astype(int)
df2.query('user_id == 773192')
df.resample('M').mean()
S_lumpedTopmodel.decision_obj.hc_profile.options, S_lumpedTopmodel.decision_obj.hc_profile.value
autos = autos.rename(columns={"odometer": "odometer_km"})
df = pd.read_csv('ab_data.csv')
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
data_issues.columns
tlen.plot(figsize=(16, 4), color='r')
bc['newdate'] = pd.DatetimeIndex(bc.date).normalize()
df = pd.read_csv('./Data/AAPL.csv', index_col=0)$ df.head()
df.max()
output.to_csv( "randomforest.csv", index=False, quoting=3 )
old_page_converted = np.random.binomial(n_old,p_old)$ print('The old_page_converted is: {}.'.format(old_page_converted))
import pandas as pd$ props = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
temp_df['date'].describe()
np.exp(0.0408) #US$
result = calc_temps('2012-02-28', '2012-03-05')$ result.min, result.avg, result.max
ign = ign.rename(columns={'title':'Name', 'platform':'Platform'})$
bokeh_output_file('iris.html')
files1= files1.loc[files1['Shortlisted']==1]$ files1.shape
import builtins$ builtins.uclresearch_topic = 'HAWKING'$ from configuration import config
dj_df = pd.read_table(data_file_path)$ print(dj_df.head())$ print(dj_df.info())
sqlContext.registerFunction("simple_function", simple_function)
closed_df = merged_df.loc[merged_df["Status"] == "Closed"]$ closed_df.head(5)$
dataframe.groupby('dmonth').daily_worker_count.mean()
horizAAPL = AAPL.sort_index(axis=1)$ horizAAPL.head()
df=pd.read_csv("dataset_quora/quora_train_test.csv")
df_user_count.apply(np.log).hist()$ plt.show()
df.rename(columns={'Price':'House_Price'},inplace=True) # renaming inplace i.e. in that same data frame$ df.head()
with pd.option_context('display.max_colwidth', 130):$     print(news_titles_sr)
num_users = df.user_id.nunique()$ num_users
results = pp.get_results()
scores[:1.625].sum()
treaties = db.treaties$ treaty_id = treaties.insert_one(treaty).inserted_id$ treaty_id
x.loc[:,"B":"C"]
receipts = pd.DataFrame.from_dict(Receipts)
df_arch_clean['rating_value'].head()$
PRE_PATH = PATH/'models'/'wt103'$ PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'
def predict (x):$     return predict_house_price.slope * x + predict_house_price.intercept
data.loc[:'Illinois', :'pop']
new_stock_data = stock_data.drop('volume', axis = 1)$ print(display(new_stock_data.head()))
df_grouped.drop(['passing_reading', 'passing_math'], axis = 1, inplace = True)$ df_grouped.columns
purchases = sql_query('select * from purchases')$ purchases.head(3)
engine = create_engine("sqlite:///hawaii.sqlite")$ Base = automap_base()$ Base.prepare(engine, reflect=True)$
contractor_clean=contractor_clean[contractor_clean['contractor_id'].isin([139,140,228,236,238]) & $     contractor_clean['contractor_version']!=1 ]$ contractor_clean=contractor_clean.loc[~contractor_clean.contractor_id.isin([373,374,378])]
nold=df2.query('landing_page=="old_page"')['converted'].shape$ print(nold[0])
import statsmodels.api as sm$ log_mod = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']])$ results1 = log_mod.fit()$
import pandas as pd$ import matplotlib.pyplot as plt
search_rating = np.vectorize(search_rating, otypes=[np.float])$ data['rate'] = search_rating(data['text'])$ data = data[pd.notnull(data['rate'])]
plt.figure(1)$ actor_counts = df['Actor1Name'].value_counts().head(10)$ actor_counts.plot.bar()
df_ari = df[(df.AwayTeam == 'Arizona Diamondbacks') | (df.HomeTeam == 'Arizona Diamondbacks')]
trip_df = pd.DataFrame(normals, columns=['tmin', 'tavg', 'tmax'])$ trip_df.head()
p_old = df2[df2['landing_page']=='old_page']['converted'].mean()$ print("Probability of conversion for old page (p_old) is", p_old)
print (user_s.columns)$ print (product_s.columns)$ print  (len(user_s),len(product_s))
Y = np.ones((num_names,1))$ Y[df['Gender'] == 'F',0] = 0
df.describe()
print('Number of renames', df3.loc[df3.is_rename==True].shape)$ print('Total shape ', df3.shape)$
index_missin_hr0to6_before2016.shape
image_predictions = pd.read_csv('image-predictions.tsv', sep ='\t')$
test_features = users[["timestamp_first_active", "signup_flow", "age"]].values
data[['TMAX']].head()
df_uro = df_uro.rename(columns = {'METAC_SITE_NM1': 'METAC_SITE'})
print('AUC using XGBoost = {:.2f}'.format(roc_auc_score(y_test, y_pred)))
from sklearn.externals import joblib$ joblib.dump(pca, '../models/pca_20kinput_6858comp.pkl')
num_benfords = 10$ benfords = [np.log10(1+1/i) for i in range(1, num_benfords + 1)]$ x_ben = [x for x in range(0, num_benfords)]
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
y.start_time
pd.date_range('1/1/2016', '12/1/2016', freq='BM')
def scoring(clf):$     scores = cross_val_score(clf, X_train, y_train, cv=15, n_jobs=1, scoring = 'neg_median_absolute_error')$     print np.median(scores) * -1
most = dfrecent.head(50)
plt.scatter(df['B'], df['C'])$ plt.title('Scatterplot of X and Y')
df2 = df.copy()$ df2 = df2.drop('ENTRIES',1)$ df2 = df2.drop('EXITS',1)$
final.to_csv('final.csv')
station_count["Count"].hist(bins=12, color="darkblue")$ plt.title("Histogram: Observation count by station")$ plt.savefig("Histogram Observation count by station")$
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Type"])
md_vals = list(tmpdf_md.loc[:,METADATA_VAL_COLS].unstack().values)$ md_vals = [s.lower() if isinstance(s, str) else s for s in md_vals]$ md_vals
df['len_convo'] = df.conversations.apply(len)$ print("... got conversation lengths")
df['conv_flag'] = df.conversations.apply(lambda x: 1 if isinstance(x, dict) else 0)$ df = df[df.conv_flag==1].drop(['users','conv_flag'],axis=1)$ print("... got just conversations")
cats_df = cats_df.set_index(['Unnamed: 0'])$ cats_df.index.name = 'seq'
n_new = df2.query('group == "treatment"').shape[0]$ n_new
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=5&column_names=[api_key=***')$ print(r.json())$
fundret.idxmax(), fundret[fundret.idxmax()]
tweet_info_pd = pd.DataFrame(tweet_info)$ tweet_info_pd = tweet_info_pd[['Account', 'Text', 'Date', 'Compound Score', 'Positive Score', 'Neutral Score', 'Negative Score','Tweets Ago']]$ tweet_info_pd.head()$
element = driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a')$ element.click()
r_forest['has_extended_profile'] = r_forest['has_extended_profile'].fillna(0)
optimizer = tf.train.AdagradOptimizer(0.01 )$ train = optimizer.minimize(loss)$
assert(df.duplicated().sum()==0)
print('The API status code is {}'.format(ng_stor.response))$ print('The URL sent to the API is {}'.format(ng_stor.response.url))$ print('The API header is {}'.format(ng_stor.response.headers))
stories.submitter_user[0]
TripData_merged3.isnull().sum()
(df['converted'] == 1).mean()
reddit_data = pd.DataFrame$ reddit_data = pd.DataFrame.from_dict(topics_dict)$
df_clean.drop(df_clean[(df_clean.rating_denominator != 10)].index,inplace=True);
obs_diff = df.query("group == 'treatment'").converted.mean() - df.query("group == 'control'").converted.mean()$ obs_diff
train_data = pd.read_feather('data/train_data')$ test_data = pd.read_feather('data/test_data')
features.tail(3)
pattern = re.compile('AA')$ print([x for x in pattern.finditer('AAbcAA')])$ print([x for x in pattern.finditer('bcAA')])
skf = StratifiedKFold(n_splits=5)$ skf.get_n_splits(X, y)$ folds = [(tr,te) for (tr,te) in skf.split(X, y)]
df.shape[0]
group1 = df[((df['group'] == 'treatment') & (df['landing_page'] == 'old_page'))]$ group2 = df[((df['group'] == 'control') & (df['landing_page'] == 'new_page'))]$ print('"new_page" and "treatment" do not line up {} times.'.format(len(group1 + group2)))
xml_in_merged['authorId'].nunique()
new_page_converted = np.random.binomial(1,p_new,n_new)$ new_page_converted.mean()
siteIDs = dfSubSites['MonitoringLocationIdentifier'].unique().tolist()$ siteIDs
print pd.concat([s1, s4], axis=1)
parse_dict['profile'].head(5)$
df2 = pd.DataFrame(np.array([[10, 11], [20, 21]]), columns=['a', 'b'])$ df2
fig_1 = plt.figure()$ plot_1 = amzn.plot()$ plot_1
rf1000 = RandomForestClassifier(n_estimators=1000)$ rf1000.fit(X, y)
user_logs = pd.read_csv('../input/user_logs.csv') # 392,106,544 rows --> nealy 400 millions rows
idx = index.Index()$ for i, poly in enumerate(census_tracts_df['shapes']):$     idx.insert(i, poly.bounds)
violations_dash = violations_noMCL[violations_noMCL['id'].str.match("-", na=False)]$ violations_dash = violations_noMCL.drop(violations_dash.index)
southern_sea_level.year == northern_sea_level.year
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()$
print(df2['user_id'].nunique())
reviews.groupby('taster_name').points.mean()
DataSet.head(10)$ DataSet.tail(5)
bonus = pd.DataFrame(d)$ bonus.columns = ['Date', 'Increment', 'Total_Score']
rankings_USA['year'] = pd.DatetimeIndex(rankings_USA['rank_date']).year$
df2 = pd.read_csv('image-predictions.tsv', sep='\t')
p = apple['Adj Close'].plot(title="Apple Stock")$ fig = p.get_figure()$
stock['daily_gain'] = stock.close - stock.open$ stock['daily_change'] = stock.daily_gain / stock.open
players=df.groupby('Player').agg({'Pts':np.average,'REB':np.average,'AST':np.average})$ top_five=players.nlargest(5,'Pts').index.tolist()$ players.nlargest(5,'Pts')
INQ2016.head(1)
n_new = df2.query('group == "treatment"').shape[0]$ n_new
cars.query('mpg > 15')             # Simple query$
df_pivot = df_feat.pivot(index='id_ndaj1',columns='id_measure_type',values='counter')$ df_pivot.head(10)
train_b, valid_b, test_b = df_b.split_frame([0.7, 0.15], seed=1234)$ valid_b.summary()
cdata.describe()
temp_data=stock_data[stock_data.SYMBOL == 'ABB'] $ temp_data=temp_data.rename(columns={"OPEN": "Open", "HIGH": "High", "LOW": "Low", "CLOSE": "Close", "TIMESTAMP": "Date"})$
mentions = api.GetMentions()$ print([m.text for m in mentions])
df['Year'] = df['Date'].apply(lambda x: x[:4])$ df.head()
df2['datetime'].min(), df2['datetime'].max()
df.describe()
g_kick_data = kick_data_state.groupby(['blurb'])$ g_filtered = g_kick_data.filter(lambda x: len(x) > 1).sort_values(['blurb','launched_at'])$ len(g_filtered)
data_sets[0] = ("users", users.withColumn("active", expr('cast(active as int)')))
df = df[df.year >2005]
csvWriter = csv.writer(csvFile)
march_2016 = pd.Period('2016-03', freq='M')$ print march_2016.start_time$ print march_2016.end_time
df.converted.mean()$
store_items.pop('glasses')$ store_items
engine.execute('SELECT * FROM Station').fetchall()
dataset.columns.str.extract(r'^(\w+)$')
data_path = os.path.join(os.getcwd(), 'datasets', 'cryptocurrencyhistoricaldata','ethereum.csv')$ train_A = pd.read_csv(data_path, delimiter = ';')
results = logit.fit()$ results.summary2()
averagerank = rankings_USA.groupby('year')['rank'].mean()$ averagerank.plot(y='rank', x='year')$
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
df_2016['sales_jan_mar'] = [y if ((x.month >=1) & (x.month <=3)) else 0.0 for x, y in zip(df['Date'],df['sale_dollars'])]
hours = bikes.groupby('hour_of_day').agg('count')$ hours['hour'] = hours.index$ hours.start.plot()$
user_df.fillna(value='', inplace=True)
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
t = thisyear$ len(t[(t.creation <= '2012-01-01') & (t.creation > '2011-01-01')])
cityID = 'dc62519fda13b4ec'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tampa.append(tweet) 
ArepaZone_timeline=api.user_timeline("ArepaZone")$ json_str=json.dumps([status._json for status in ArepaZone_timeline])$ parsed_json=json.loads(json_str)
new_page_converted.mean() - old_page_converted.mean()
p_new = df2['converted'].mean()$ (p_new)
X = x['age'].copy()$ X['C_sign'] = np.sign(X)$ X['C_sign'].value_counts()
df1 = pd.DataFrame({'full_text': full_text})$ combined_df = pd.concat([df_urls, df1], axis=1, join_axes=[df_urls.index])$ data = combined_df.drop(['source','type_material','headline','url'],axis=1)
np.exp(-0.0150)
DataSet[['userName','tweetRetweetCt']].sort_values('tweetRetweetCt',ascending=False).head(10)
table = pd.pivot_table(df, columns=['currency'], values=['fraud'], aggfunc=np.mean)$ print table
ratings = ['review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value']$ filtered_df.dropna(axis=0, subset=[ratings], inplace=True)
number_pos = data_df[data_df['is_high_val'] == 1].shape[0]$ number_all = data_df.shape[0]$ print(f'Target labels of class \'1\': {number_pos} or {(number_pos/number_all)*100:.2f}% over all.')
mid = (len(trading_volume) - 1) // 2 + 1$ median_volume = sorted(trading_volume)[mid]$ print('Median Trading Volume: {:.0f}'.format(median_volume))
mean_absolute_error(Y_btc_val, [Y_btc_train.mean() for i in range(Y_btc_val.shape[0])])
status_data = pandas.read_csv("./Dataset Processed/mypersonality_final_classifiedByClass_onlyColumn.csv",encoding='cp1252')$
rf=RandomForestRegressor()$ rf.fit(X_train,Z_train)$ rf.score(X_test,Z_test)
S_1dRichards.decision_obj.bcLowrSoiH.options, S_1dRichards.decision_obj.bcLowrSoiH.value
obs_diff_treatment = df2.query('group == "treatment"').query('converted == 1')['user_id'].count() / df2.query('group == "treatment"')['user_id'].count()$ obs_diff_treatment
response = requests.post("https://data.bitcoinity.org/chart_data", data=chartParams)$ response.content
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Doctors', doc_duration, window=52)
from matplotlib.dates import MonthLocator, WeekdayLocator, DateFormatter$ %pylab inline$ pylab.rcParams['figure.figsize'] = (15, 9)$
count_vect = CountVectorizer(token_pattern="[A-Za-z-]{3,}")$ textFeatures = convert_data(apDF['features'])$ features_counts = count_vect.fit_transform(textFeatures)$
pass_file_name = "C:\KUBootCamp\\passw.json"$ data = json.load(open(pass_file_name))$ spassword = data['mysql_root']
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig8_right_Using_TestCase_from_Hydroshare.ipynb'$ hs.addContentToExistingResource(resource_id, [npath])
visits = sql_query('select * from visits')$ visits.head(3)
df2 = pd.read_csv('data/2013-2017_Council_And_Committee_Meetings_-_Meeting_Details.csv', parse_dates=['MEETING_DATE'], low_memory = False)$ df2['MEETING_TYPE'].value_counts()$
lr2 = LinearRegression()$ lr2.fit(train_data, train_labels)
filename = processed_dir+'pulledTweetsProcessedAndClassified_df'$ gu.pickle_obj(filename,pulledTweets_df)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])$
dates.to_period('D')
dfFull['OverallCondNorm'] = dfFull.OverallCond/dfFull.OverallCond.max()
contractor_clean.head() #these columns have been dropped
df2[df2.user_id.duplicated(keep = False)]
df = pd.read_csv('ab_data.csv')$ df.head()
cities.notnull()
filt_zip_loc=~np.logical_and(building_pa_prc_shrink[['zipcode']].isna().values ,building_pa_prc_shrink[['location']].isna().values)
df.info()
max_retweet = DataSet['tweetRetweetCt'].max()$ max_retweet
clean_appt_df = train_set.copy()$ clean_appt_df.shape
from dotce.report import generate_chart
df2_dummy = df2_dummy.drop(['ab_page_old_page'],axis=1)$ df2_dummy.tail(1)
data.RTs.describe()
precip_data = session.query(Measurements).first()$ precip_data.__dict__
data.to_csv('/home/sb0709/github_repos/bootcamp_ksu/Data/data.csv', sep = ',')
f.loc[:1] # this slice no longer works!
X.columns
cols = status_data.columns.tolist()$ cols = cols[:5] + cols[5:10]$ status_data = status_data[cols]
df.loc[monthMask, 'water_year'] = df['year'] + 1
df['x'] = df.index$ df.plot.scatter(x='x', y='y')
r_json = r.json()$
really_large_dataset = sc.broadcast(100)
payments_total_yrs.head(2)
amzn_csv = pathlib.Path('../datasets/amzn.csv').absolute()$ amzn = pd.read_csv(str(amzn_csv), index_col=0, parse_dates=True)
p_diffs = np.array(p_diffs)$ plt.hist(p_diffs);
sum_of_values_for_every_wednesday = s[date_time_index.weekday == 2].sum() #Calculating the sum of values in s for every Wednesday$ print(sum_of_values_for_every_wednesday) #Printing the sum
twitter_Archive.dropna(subset = ['rating'])$ twitter_Archive = twitter_Archive.drop(twitter_Archive[twitter_Archive.rating > 3].index)
df = pd.read_csv('first_17500_csv', index_col=0)
tweet_json_df = pd.DataFrame(columns= ['id','retweet_count', 'favorite_count'])
pattern = re.compile('AA')$ print(pattern.sub('BB', 'AAbcAA'))$ print(pattern.sub('BB', 'bcAA'))
test = df["converted"].mean()$ test
charge = reader.select_column('charge', start=0, stop=100)$ charge = charge.values # Convert from Pandas Series to numpy array$ charge
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')$ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(RN_PA_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours First Difference Autocorrelation')
df2_treatment = df2[df2['group'] == 'treatment']['converted'].mean()$ print('The probability that an individual in the treatment group converted is: {}'.format(round(df2_treatment, 4)))
import quandl$ quandl.ApiConfig.api_key  = 'r4mgK1hxD11kLoFtRsso'
df_new['week1'] = np.where(df_new['timestamp'] <= '2017-01-09', 1, 0)$ df_new['week2'] = np.where((df_new['timestamp'] > '2017-01-09') & (df_new['timestamp'] <= '2017-01-16'), 1, 0)$ df_new['week3'] = np.where(df_new['timestamp'] > '2017-01-16', 1, 0)
print 'Total amount for non-USA location: ', df[((df.state == '') & (df.city != ''))].amount.sum()
sns.regplot(x="qstot_0", y="y", data=psy_native).set_title("Depressive Symptomatology")$
trump_o.drop("text", axis = 1)$ trump_con = trump_o.drop(["favorite_count", "text", "id_str", "is_retweet", "retweet_count", "source", "created_at"], axis = 1)$ trump_con.shape
results = pd.concat([pd.Series(preds).reset_index(drop=True), Y_test.reset_index(drop=True)], axis = 1)$ results.columns = ["predicted", "actual"]$ results["diff"] = (results["predicted"] - results["actual"])/results["actual"]
df.info()
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
stringlike_instance.content = 'changed content'
p = len(train_att)/len(train)$ print(len(train_att))$ print('The percentage of converted clicks is {num:.10%}'.format(num=p))
mgxs_lib.domain_type = 'cell'$ mgxs_lib.domains = geometry.get_all_material_cells().values()
err = (actual - expected[np.newaxis,:,:]).reshape(-1)$ err.shape
train_data_np = x_val.as_matrix()$ train_vehicles_np = y_val.as_matrix()
from tensorflow.python.client import device_lib$ device_lib.list_local_devices()
sp500[(sp500.Price < 10) & (sp500.Price > 0)][['Price']]
rng_utc = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz=dateutil.tz.tzutc())
pd.set_option('display.max_colwidth',-1)
df['profit']=(df.state_retail-df.state_cost)*df.bottles_sold$ df['ppb']=(df.state_retail-df.state_cost)$ df['profit_margin']=(df.ppb/df.state_retail)*100
(own_star[~own_star['starred'].isnull() & ~own_star['owned'].isnull()]).shape
roi = pd.DataFrame({'roi_0': roi_on_day(0, users_costs_df, orders_df)})$
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
df_arch_clean.loc[df_arch_clean.tweet_id== '681340665377193984', 'text']$
google_stock.describe()
sum(clintondf.text.apply(lambda s: s.endswith(' -H')))
tweets_master_df.iloc[tweets_master_df['favorite_count'].nlargest(10).index, :]
df6.mean() # mean of lunch values$
events_df['utc_offset'].head(5)
lag_list = list(df.columns.values)
dfd.zones.value_counts()
nbar_clean.time
tweets_RDD = sc.textFile('./PSI_tweets.txt')$ tweets = tweets_RDD.collect() #or tweets = tweets_RDD.take(500) for some testing $
X = reddit_master['title']$ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
country_dummies = pd.get_dummies(df2['country'])
chk = joined.loc[joined['nat_event']==1].head()$ holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
s=r.json()$ type(s)
import matplotlib.pyplot as plt$ %matplotlib inline$ plt.bar(range(len(langdata1)),langdata1)$
S_distributedTopmodel.initial_cond.filename
n = np.count_nonzero(e==0)$ k = np.count_nonzero(y[e==0])
pd.set_option('display.max_columns', 100)$ tmpdf
china_violations = df[df['COUNTRY'] == 'CHINA'].shape[0]$ print('{0:.2f}% Chinese products'.format((china_violations / len(df)) * 100))
store_items.fillna(method = 'backfill', axis = 1)
yhat_prob = loanlr.predict_proba(X_test)$ yhat_prob
(p_diffs > p_diff_real).mean()$
months = pd.concat([nov, dec])$ print nov.shape, dec.shape, months.shape
combats.Winner[combats.Winner == combats.First_pokemon] = 0$ combats.Winner[combats.Winner == combats.Second_pokemon] = 1$ print(combats.head(5))
n_new, n_old = df2.landing_page.value_counts()$ print('n_new is:',n_new)
df2.dtypes
articles = db.articles.find()$ for article in articles:$     print(article)
dataframe.head()
locations = session.query(Measurement).group_by(Measurement.station).count()$ print("There are {} stations.".format(locations))
df_concat.drop(df_concat.columns[[0,1,2]], axis=1, inplace= True)
autos = autos[autos["registration_year"].between(1900,2016)]$ autos["registration_year"].value_counts(normalize=True).head(10)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=uEEsBcH3CPhxJazrzGFz&start_date=2017-01-01&end_date=2017-12-31")$
df2['intercept'] = 1$ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']$ df2.head()
df2.drop(2893,inplace=True)
products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv')$ products.head()
df_proj = pd.read_csv(projFile, usecols = projCols, $                  dtype = proj_dtypes)
print("Converted users probability is {}%".format((df2['converted'].mean())*100))
trump.info()
df7['avg_dew_point Closed'].value_counts(dropna=False)
weather_df_rsmpld = weather_df_byday.resample('1M').mean()$ weather_df_rsmpld.head()
ranks = dev3[to_rank_cols].apply(lambda x: x.rank(ascending=False), axis=0)
text_df.head()
df_new['country'].unique()
filenames = glob.glob(os.path.join(input_folder, glob_string))
df.sort_values(by="pickup", inplace=True)
y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)$ y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)
df2['landing_page'].value_counts()
df_user['opted_in_to_mailing_list'].value_counts()/ len(df_user['object_id'].unique())$
df.iloc[::20].plot.bar(title="Percipitation")$ plt.tight_layout()$ plt.show()$
ks_particpants=kick_projects.groupby(['category','launched_year','launched_quarter','goal_cat_perc']).count()$ ks_particpants=ks_particpants[['name']]$ ks_particpants.reset_index(inplace=True)
records3.loc[(records3['Graduated'] == 'Yes') & (records3['GPA'].isnull()), 'GPA'] = grad_GPA_mean$ records3.loc[(records3['Graduated'] == 'No') & (records3['GPA'].isnull()), 'GPA'] = non_grad_GPA_mean
target.disconnect()
my_gempro.get_msms_annotations()
plt.hist(p_diffs);
df['converted'].mean()
pdiff = (new_page_converted.mean()) - (old_page_converted.mean())$
len(calls_nocontact.location.unique())
print ('Before Dummies',df.shape)$ df = df.merge(pd.get_dummies(df['Date'].dt.strftime('%B')),left_index=True,right_index=True,how='left')$ print ('After Month',df.shape)$
df = pd.DataFrame.from_dict(dst_cap, orient="index")$ df.columns = ['Fequency']
tripAux = trip.groupby('InicioFin')$ tripAux = tripAux.get_group('66/66')$ tripAux
fin_p.index
plt.scatter(X2[:, 0], X2[:,1], c=dayofweek, cmap='rainbow')$ plt.colorbar()
LR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train)$ yhat_prob2 = LR2.predict_proba(X_test)$ print ("LogLoss: : %.2f" % log_loss(y_test, yhat_prob2))$
autos.head()$
df.shape
print(df.head())$
sp_500_adj_close = sp500[['Adj Close']].reset_index()
def make_soup(url):$     html = urlopen(url).read()$     return BeautifulSoup(html, "lxml")
filename_tups = df3[['timestamp', 'filename_old', 'filename_new']].sort_values('timestamp').values
len(df2.query('landing_page=="new_page"'))/len(df2.index)
taxiData[Tip_percentage_of_total_fare != 0].Tip_percentage_of_total_fare.mean() # tip percentage mean is around 0.25$ print("MEAN of Tip as a percentage of the total fare = {}"$       .format(taxiData[Tip_percentage_of_total_fare != 0].Tip_percentage_of_total_fare.mean()))
df.filter(like="FLG_", axis=1)
artistAliasDF[ artistAliasDF.mispelledID==1000010 ].show()$ artistAliasDF[ artistAliasDF.mispelledID==2082323].show()$
sub_data = data[data["place"].notnull()]$ sub_data.head()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY$ r = requests.get(url)$ json_data = r.json()
temp = pd.read_table('vader_lexicon.txt', names=('word', 'polarity', 'idk', 'idk1'))$ sent = pd.DataFrame({'polarity':temp['polarity']})$ sent.index = temp['word']$
movies['profit'] = (movies.revenue - movies.budget)$ movies
tipsDF.dtypes
autos['odometer'] = autos['odometer'].astype(int)$ autos.rename(columns={'odometer':'odometer_km'}, inplace=True)$ autos['odometer_km'].head()
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
PYR['soup'] = PYR.apply(create_soup, axis = 1)
search['prefetch'] = search['message'].apply(prefetch)
step_counts.dtypes
sp500.iloc[[0, 2]]
red_4.info()
tot_station= session.query(station).count()$ for station in session.query(station.station):$     print(station)
df['order_date'].max()
i=random.randrange(len(train_x))$ print_query(i)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$ print(new_page_converted)
display(data.head(10))
dfAll = pd.read_csv('All.csv',index_col=0)$ df_ohc = pd.get_dummies(dfAll,columns=['events','day_of_week'])$ df_ohc.head()
top_active = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()$ most_active_id = session.query(Measurement.station).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).first()$ top_active$
group_sizes = (data.$               groupby('species')$               .size())
groups = mgxs.EnergyGroups()$ groups.group_edges = np.array([0., 0.625, 20.0e6])
stationActive_df = pd.read_sql("SELECT station, count(*) as `Station Count` FROM measurement group by station order by `Station Count` DESC", conn)$ stationActive_df
data_FR[data_FR.duplicated(subset='name', keep=False)]
print('{0:.2f}%'.format((scores[0:2.5].sum() / total) * 100))
age_up70.shape
idx = data['dataset_data']['column_names'].index('Open')$ open_price = [day[idx] for day in data['dataset_data']['data'] if day[idx]]$ print('Highest and lowest opening prices in 2017 were {} and {}'.format(max(open_price), min(open_price)))
print('reduce memory')$ utils.reduce_memory(transactions)$ transactions.info()
df = pd.read_sql(SQL, db)$ df
iqr=df1['cleantime'].quantile(q=0.75)-df1['cleantime'].quantile(q=0.25)$ iqr
import statsmodels.api as sm$ convert_old = df2.query('converted == 1 & landing_page == "new_page"')['user_id'].nunique()$ convert_new = df2.query('converted == 1 & landing_page == "old_page"')['user_id'].nunique()
ab_df.nunique()$
data.to_csv('TwitterSentimentData.csv')
perf_test = pd.read_csv('performance_test.csv')$ print('Testing data shape: ', perf_test.shape)$ perf_test.head(50)
def keras_rnn_predict(samples, empty=human_vocab["<pad>"], rnn_model=m, maxlen=30):$     data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty)$     return rnn_model.predict(data, verbose=0)
df_ad_airings_5['location'][0].split(",")[1]
df_features2.loc[df_features2["CustID"].isin([customer])]
s2 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])$ s2
df_ct.head(2)
autos['brand'].value_counts(normalize=True)
column_name = autos.columns
data.loc[data['hired']==1].groupby('category').num_completed_tasks.mean()
theft.loc[12]
plt = df2.plot(legend=False)$ plt.set_ylabel("complaint count")$ plt.set_xlabel("DOTCE predicted grade school level (high school 9-12)");
df[unique_cols].head(2)
df.at[dates[2],'A']
sentimentDf.sort_values("date", inplace=True, ascending=True)$ sentimentDf.head()
df_ad_airings_filter_3['start_time'].max()$
df2.shape[0]$ print ("Total Number of row : {}".format(df2.shape[0]))
adjusted_width = 25$ ws.column_dimensions['L'].width = adjusted_width$ ws.column_dimensions['Q'].width = adjusted_width
print('Groceries has shape:', groceries.shape)$ print('Groceries has dimension:', groceries.ndim)$ print('Groceries has a total of', groceries.size, 'elements')
training_X_scaled = scaler_M7.transform(training_X)$ holdout_X_scaled = scaler_M7.transform(holdout_X)$ len(training_X_scaled),len(holdout_X_scaled)
df_first = transactions.groupby(["UserID"]).first()$ df_first['UserID'] = df_first.index # Assign a column that is equal to the index created from the Groupby$ df_first
print df_mk.groupby('hour').size().nlargest(1), df_ks.groupby('hour').size().nlargest(1)
blahblah = 'This string is stored in the variable on the left.'$ blahblah
plt.hist(p_diffs)$ plt.axvline(x=actual_diff, color='r', linewidth=2);
rain_df = pd.DataFrame(rain_score)$ rain_df.set_index('date').head()$ rain_df.head()
graf_train=pd.concat([graf_train, train_topics_df], axis=1)$ graf_test=pd.concat([graf_test, test_topics_df], axis=1)
Val_eddyFlux = Plotting(hs_path+'/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
adj_glm = smf.glm('hospital_expire_flag ~ C(inday_icu_wkd) + C(admission_type)', $                      data=data, family=sm.families.Binomial()).fit()$ adj_glm.summary2()$
BallBerry_ET_Combine = pd.concat([BallBerry_rootDistExp_1, BallBerry_rootDistExp_0_5, BallBerry_rootDistExp_0_25], axis=1)$ BallBerry_ET_Combine.columns = ['BallBerry(Root Exp = 1.0)', 'BallBerry(Root Exp = 0.5)', 'BallBerry(Root Exp = 0.25)']
crimes.PRIMARY_DESCRIPTION[3:10]
t2['p1'] = t2['p1'].str.title()$ t2['p2'] = t2['p2'].str.title()$ t2['p3'] = t2['p3'].str.title()
print(wcPerf1_df.loc[wcPerf1_df['Score'] == 4])
opener = r.build_opener()$ opener.addheaders = [('User-agent', 'Mozilla/5.0')]$
rain_df.set_index('date').head()
scipy.stats.spearmanr(df["tripduration"],df["day"])
df2['user_id'][df2.duplicated(['user_id'], keep=False)]
link_list = set(list_of_links)$ link_list.remove('/news/')$ link_list = list(link_list)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42) $ X_train.shape , y_train.shape,  X_test.shape, y_test.shape
conf = SparkConf().setAll([('spark.executor.memory', '6g'), ('spark.executor.cores', '6'), ('spark.cores.max', '6'), ('spark.sql.session.timeZone', 'UTC')])$ sc = SparkContext("local", "ib", conf=conf)
data_tickers = data_tickers.resample(sampling, how='last') $ data_tickers.head()
page_size = 200$ response = client.get('/tracks', tags='footwork', limit=page_size,$                     linked_partitioning=1)$
tobs = session.query(Measure.station, Measure.date, Measure.tobs).order_by(Measure.date).\$ filter(Measure.date > '2016-08-23').all()$ tobs
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&API_KEY'$ r = requests.get(url)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(doc_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours First Difference Partial Autocorrelation')
result = pd.merge(df, ser, on = 'key').drop('key', axis = 1)$ result
df.sample(False, 0.001).count()
X_centered = X - X.mean()
precip = session.query(Precip.date, Precip.prcp, Precip.station).filter(Precip.date >= date_ly)#.filter(Precip.station == 'USC00511918')
qty_left_1c_store$ table_store.iloc[0].at[u'Quantity Txn Part']$
import datetime as dt$ from dateutil.relativedelta import relativedelta
bad_df = df.index.isin([5,12,23,56])$ df[~bad_df].head()
raw.columns
taxiData[Tip_percentage_of_total_fare != 0].Tip_percentage_of_total_fare.mean() # tip percentage mean is around 0.25
psy_prepro = pd.read_csv("psy_prepro.csv")$ psy_prepro = psy_prepro.set_index('subjectkey', verify_integrity=True)$
tlen = pd.Series(data = data.len.values, index = data.Date)$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
table_rows = driver.find_elements_by_tag_name("tbody")[26].find_elements_by_tag_name("tr")$
res = requests.get('http://elasticsearch:9200')$ r=json.loads(res.content)$ r
dataframe = pd.read_csv("assignment.csv", parse_dates=['date'], index_col="date")
nt_price = nt.groupby(pd.Grouper(freq='W'))["sq_price_value"].mean().to_frame().rename(index=str, columns={"sq_price_value":"nt_sq_price_value"})
data_df.desc[15]
import ibm_boto3$ from ibm_botocore.client import Config
df_img_algo = pd.read_csv("file_2", sep= "\t")$ df_img_algo.head()$
pres_df.rename(columns={'subject_count': 'subject_count_test'}, inplace=True)
matt1 = dta.t[(dta.b==40) & (dta.c==1)]$
df['year'] = df.index.year
data.loc[pd.to_datetime("2016-09-02")]
type(df.date[0])
df2 = df.dropna(subset = ['one'])$ df2
y_pred_class = nb.predict(X_test_dtm)$ from sklearn import metrics$ metrics.accuracy_score(y_test, y_pred_class)
autos["odometer_km"].unique().shape[0]
store_items.pop('new watches')$ store_items
df_artist.artist_genres = df_artist.artist_genres.apply(lambda x:",".join(map(str,x)))$ df_artist = pd.concat([df_artist.iloc[:,:-1],df_artist.artist_genres.str.get_dummies(sep = ',')], axis = 1)$ df_artist = df_artist.drop_duplicates(subset = 'artist_id').reset_index(drop = True)
import statsmodels.api as sm$ convert_old = df2.query("landing_page == 'old_page'")['converted'].sum()$ convert_old$
june_acj_data.head()
len(df['user_id'].unique())
df_user['invited_by_user_id'].value_counts().head(10)$
unique_desc=class_merged_hol['description'].unique()$ print (unique_desc)
df.apply(func8, axis=1)
df_wm.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
train_data.groupby(['device.browser']).agg({'visitNumber': 'count'}).reset_index().set_index("device.browser",drop=True).plot.bar()
qt_ind_ARR_closed.applymap(lambda x: "{0:,.2f}".format(x))
station_count.loc[(station_count['Count'] >= 2000)]
df_obj2.index[0] = 2$
df2[df2.user_id == 773192]
daily = hourly.asfreq('D')$ daily
print(autos.price.value_counts().head(10).sort_index(ascending=True))
p_new = new_page_converted.mean()$ p_old = old_page_converted.mean()$ p_new - p_old
df2["Temperature Groups"] = pd.cut(df2["tobs"], bins, labels=bin_names)$ df2_grp1=df2.groupby("Temperature Groups").count()$ df2_grp1["tobs"]
df3.index
breakdown[breakdown != 0].sort_values().plot($     kind='bar', title='Russian Trolls Number of Links per Topic'$ );
import pickle$ with open('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.pckl', 'rb') as f:$     my_saved_gempro = pickle.load(f)
print(len(labels.keys()))
urls = pd.read_pickle('PC_World_Urls.pickle')$ headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}$ shop_web_addr = 'https://www.pcworld.co.uk'
tNormal = len(df[ (df['bmi'] > 18.5) & (df['bmi'] < 25.0) ])$ tNormal
print(df.shape)$
df.info()
joined.country.value_counts()
southern_sea_level = pd.read_table("http://sealevel.colorado.edu/files/current/sl_sh.txt", $                                    sep="\s+")$ southern_sea_level
import builtins$ builtins.uclresearch_topic = 'NYC'$ from configuration import config
from pandas.plotting import scatter_matrix$ axes = scatter_matrix(data.loc[:, "TMAX":"TMIN"])
events = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).last().reset_index()$ events = events[['atbat_pk', 'events', 'woba_value']]
woba_leaderboard = df.groupby(['batter_name', 'batter'])['woba_value'].agg(['mean', 'count'])$ woba_leaderboard.loc[woba_leaderboard['count']>100,].sort_values('mean', ascending=False).head()
cars2 = cars.drop_duplicates(['name' , 'price' , 'vehicleType' ,'abtest', 'yearOfRegistration' , 'gearbox' , 'powerPS' , 'model' , 'kilometer' , 'monthOfRegistration' , 'fuelType' , 'notRepairedDamage'])
df_roll = df.set_index("posted_date")$ df_roll = df_roll.resample("1h").sum().fillna(0).rolling(window=3, min_periods=1).mean()$ df_roll.reset_index(inplace=True)
df.iloc[:,1:3] = df.iloc[:,1:3].apply(pd.to_datetime, errors='coerce') $ df['Trip_duration']=df['Lpep_dropoff_datetime']-df['lpep_pickup_datetime']
df['rating'] = df.rating_numerator/df.rating_denominator$ df['rating_category'] = pd.cut(df.rating, bins = [0.0, np.percentile(df.rating,25), np.percentile(df.rating,50), np.percentile(df.rating,75), np.max(df.rating)],labels=['Low','Below_average','Above_average','High'])$ df.drop(['rating_numerator','rating_denominator'], axis=1, inplace=True)
labels = list(crf.classes_)$
public_tweets = api.user_timeline(target_user,count=1)
total_tokens_sk = len(all_tokens_sk)$ corpus_tweets_streamed_keyword.append(('total tokens', total_tokens_sk)) # update corpus comparison$ print('Total number of words (including mentions, hashtags and links) in the collection: ', total_tokens_sk)
trunc_df = trunc_df.drop(trunc_df.columns[[0]],1)
df_merged.shape
min_open = ldf['Open'].min()$ min_open
c>0.7
merge[merge.columns[40:]].head(3)$
Aussie_df = toDataFrame(aussie_results)$ Aussie_df.head(5)
for tweet in public_tweets:$     print(json.dumps(tweet, sort_keys=True, indent=4))
rf.score(X_train, y_train)
display_all(train_data[['date','fullVisitorId']].isnull().sum()/len(train_data))
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep ='\s+') # Index in the first column "col=0" and header on the first "row"$ df.head(5)$
df = demo.make_df(35175,35180)$ df.head()
env = Environment.from_studio("ticketing", "http://10.29.28.147:8090",$                               studio_api_token="dXNlci1kZXY6UGFzc3dvcmQ0REVW")$ exp = env.create_experiment('imdb-ratings')
archive_df_clean.loc[archive_df_clean['name'] == 'Bella']$
prcp_analysis_df = pd.read_sql("SELECT date, prcp FROM measurements", con=engine, columns=[["date"],["prcp"]])
X_test=tok.texts_to_sequences(X_test)$ x_test=sequence.pad_sequences(X_test,maxlen=maxlen)
df_new = df2.merge(df3, on='user_id', how='left')$ df_new.head()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
df = df.drop(['Ticket','Cabin'], axis=1)$ df = df.dropna() 
cut_data = pd.cut(df["latitude"], 50000)$ counts = pd.value_counts(cut_data)$ print(counts)
cityID = '018929347840059e'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Indianapolis.append(tweet) 
stations_all=session.query(Measurement.station).all()$ q_stations=session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).all()$ q_stations
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}]$ new_store = pd.DataFrame(new_items, index = ['store 3'])$ new_store
df.loc[:,'A']
df.groupby('key').sum()
gene_df[gene_df['length'] > 2e6].sort_values('length').iloc[::-1]
print train.shape$ print weather.shape
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);
print pd.concat([s1, s4], axis=1, join='inner')
c = (y - m*x).mean()$ print(c)
df_countries = pd.read_csv('./countries.csv')$ df_countries = df_countries.set_index('user_id')$ df_countries.head()$
shows['stemmed_plot'] = shows['plot_cleaned'].dropna().apply(split_and_stem)
lm = sm.OLS(df_new['converted'], df_new[['intercept', 'US', 'UK']]) #using only 2 dummy variables from country$ results = lm.fit()$ results.summary()
df.info(null_counts=True, memory_usage='deep')
df.groupby(['month']).agg([sum])$
groupby_breed = df_master.groupby('dog_breed').sum().reset_index()$ groupby_breed.sort_values('rating_numerator',inplace=True,ascending=False)$
af.length.describe()
now = datetime.now() + pd.Timedelta('010:30:00')$ stories_df['Age In Days'] = stories_df.apply(lambda x: (now - x[x['status'] + ' Set To Date']).days, axis = 1)
df.to_csv('ab_edited.csv', index=False)
df.sort_values(by="grade")
phone = "2004-959-559 # This is Phone Number"$ num = re.sub(r'#.*$', '', phone)$ print('Phone Num : {}'.format(num))
sns.countplot(x='country', data=data_for_model, hue='final_status')
1/np.exp(-0.0149), 1/np.exp(-0.0408), np.exp(0.0099)
!hdfs dfs -cat {HDFS_DIR}/p31-output/part-0000* > p31_results.txt
get_assignment('ariel/ariel3/vlf-receiver_fixed-frequency/signal_strength/DATA2_DR002106_DR002106_20080804_071839/dr002106_f00001.phys.1')
model = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA','UK']])$ results = model.fit()$ results.summary()
p5_result = p2_table.sort_values('Profit', ascending=True)$ p5_result.head()
data = data.drop_duplicates(subset=['name'], keep=False)
temperature_2016_df = pd.DataFrame(Temperature_year)#.set_index('date')$ temperature_2016_df.head()
S_lumpedTopmodel.decision_obj.bcLowrSoiH.options, S_lumpedTopmodel.decision_obj.bcLowrSoiH.value
df2.info()
df2[df2['user_id'].duplicated()]['user_id']
DF1 = pd.DataFrame(datePrecip, columns=["Date","Precipitations"])$ DF1.head()
spotify_df['Number One']=np.nan
cityID = 'e444ecd51bd16ff3'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cincinnati.append(tweet) 
import matplotlib.pyplot as plt$ %matplotlib inline
df2.drop(1899, inplace=True)
print(type(some_rdd),type(some_df))$ print('some_df =',some_df.collect())$ print('some_rdd=',some_rdd.collect())
import dotce$ stats = dotce.LanguageStats()
df = spark.read.csv('../NYC311/nyc311_sample.csv', inferSchema=False, header=True)$ df.printSchema()
print("Percentage of positive tweets: {}%".format(data_spd.query('SA>1').shape[0]*100/len(data_spd['tweets'])))$ print("Percentage of neutral tweets: {}%".format(data_spd.query('SA==1').shape[0]*100/len(data_spd['tweets'])))$ print("Percentage of negative tweets: {}%".format(data_spd.query('SA<1').shape[0]*100/len(data_spd['tweets'])))
print(r.json()['dataset_data']['column_names'])$ print(r.json()['dataset_data']['data'][0])$ print(r.json()['dataset_data']['data'][-1])
df2['timestamp'] = pd.date_range('8/8/2018', periods=len(df2['MATCHKEY']), freq='D')
unitech_df.head(5)
print(lgb_cv.best_params_)$ print(lgb_cv.best_score_)
offseason18 = ALL[(ALL.index > '2018-02-07')] # Finally, this says that everything after this past Superbowl is part of$
pd.Timestamp('2014-12-14 17:30')
final.index=range(final.shape[0])
turnstiles_df.sort_values(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True, ascending=False)$ turnstiles_df.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True)
plt.figure()$ precip_df.plot(kind = 'line', x = 'Date', y = 'Precip')$ plt.legend(loc='best')$
groups_topics_unique_df = groups_topics_df.drop_duplicates(subset=['group_id'])
forecast_col = 'Adj. Close'$ df.fillna(value=-99999, inplace=True)$ forecast_out = int(math.ceil(0.01 * len(df))) #round off to the nearest value $
kick_projects['goal_cat_perc'] =  kick_projects.groupby(['category'])['goal'].transform($                      lambda x: pd.qcut(x, [0, .35, .70, 1.0], labels =[1,2,3]))
train['is_holiday'] = train['start_timestamp'].map(lambda x: 1 if str(x.date()) in us_holidays else 0)$ test['is_holiday'] = test['start_timestamp'].map(lambda x: 1 if str(x.date()) in us_holidays else 0)
rf = RandomForestClassifier(n_estimators = 30)$ rf.fit(X_train, y_train)$ rf.score(X_test, y_test)
df_new = pd.merge(df2, df_countries, on='user_id')$ df_new.head()
df_TempIrregular.to_csv('data/All_Irregularities_20180601_to20180607.csv', sep=',')
dataFile = "wasb://sampledata@pysparksampledata.blob.core.windows.net/sampledata.csv"$ dataFileSep = ','$ df = sqlContext.read.csv(dataFile, header=True, sep=dataFileSep, inferSchema=True, nanValue="", mode='PERMISSIVE')$
apple["Regime"] = np.where(apple['20d-50d'] > 0, 1, 0)$ apple["Regime"] = np.where(apple['20d-50d'] < 0, -1, apple["Regime"])$ apple.loc[dt.date(2016,8,7):dt.date(2016,1,1),:].plot(ylim = (-2,2)).axhline(y = 0, color = "black", lw = 2)
tweet_df.to_csv('recent_news_tweets.csv')
import pandas as pd$ df = pd.read_csv('aapl_no_dates.csv')$ df.head()$
print(json.dumps(geocode_result, indent=4))$
autos["date_crawled"].str[:10].value_counts(normalize=True,dropna=False).sort_index()$
df.info()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyze = SentimentIntensityAnalyzer()
n_old = df2[df2['landing_page'] == 'old_page'].shape[0]$ n_old
df[df['userID'] == 3532804813.00]
X_test_dtm = stfvect.transform(X_test)$ X_test_dtm
news_p = news_items[0].find(class_='rollover_description_inner').text$ print (news_p)
df2.converted[df2.group == 'control'].mean()
reg_df['page_US'] = reg_df['is_US'] * reg_df['ab_page']$ reg_df['page_UK'] = reg_df['is_UK'] * reg_df['ab_page']$ reg_df.head()$
stacked_image_predictions.drop('is_dog', axis=1, inplace=True)$ stacked_image_predictions.head()
act_diff = df[df["group"]=='treatment']["converted"].mean() - df[df["group"]=='control']["converted"].mean()$ act_diff
avgvolume.sort()$ mediankey = int((len(avgvolume)/2))$ print('Median trading volume for the year was: ',avgvolume[mediankey])
for factor in factors:$     df_factors_PILOT['rank_' + factor] = df_factors_PILOT.groupby("race_id")[factor].transform(lambda x:x.rank(ascending=False))$
df['receive_time'] = pd.to_datetime(df['receive_time'])$ df['created_at'] = pd.to_datetime(df['created_at'])
for row in RandomOneDF.take(2):$     print row.ID, row.VAL1, row.VAL2
combined_df.to_csv('manual_comment_results.csv', index=False)
control_group_user_count = df2[df2['group'] == 'control']['user_id'].count()$ converted_control_user_count = df2[(df2['group'] == 'control') & (df2['converted'] == True)]['user_id'].count()$ p_control_converted = converted_control_user_count / control_group_user_count$
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href']
search_key_words = "\"" + search_key_words.replace(" ","+") + "\""$ df['newspaper_search'] = df['newspaper'].apply(lambda x: x.replace("/","%2F"))
calls_nocontact_2017.to_csv("311_calls_new.csv")
r.status_code
info_df.plot(kind='area', stacked=False, alpha=0.5, colormap='Spectral')$ plt.show()
print df.shape$ df.index
coords = frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']].values$ kmeans = MiniBatchKMeans(n_clusters=40, batch_size=10000,random_state=0).fit(coords)$ frame_with_durations_outliers_removed['pickup_cluster'] = kmeans.predict(frame_with_durations_outliers_removed[['pickup_latitude', 'pickup_longitude']])
rain_df.set_index('date').head()$
print( df['ghost_crabs'].where((df['survey'] == '201704') & ( df['group'] == 'control')).sum())$ print(df['ghost_crabs'].where((df['survey'] == '201704') & ( df['group'] == 'treatment')).sum())
print(soup.prettify())
data = pd.DataFrame(data = [x.text for x in tweets], columns = ['Tweets'])$ print(data.head(10))
print(norm.ppf(1-(0.05)))$
input_node_types_DF = pd.read_csv('network/source_input/node_types.csv', sep = ' ')$ input_node_types_DF
df_mas=df_mas[df_mas.rating_numerator>9]$ df_mas=df_mas[df_mas.rating_numerator<15]
lv_workspace.get_subset_object('B').get_step_object('step_1').show_settings()$ lv_workspace.get_subset_object('B').get_data_filter_object(step=1).exclude_list_filter
scipy.stats.kruskal(df2["tripduration"], df3["tripduration"])
df_all.to_csv('./data/df_all_no_sess.csv', index=False)$
df1 = df[df.isnull().any(axis=1)] # axis=1 specifies rows instead of columns$ df1.shape
tree_features_df['p_hash'].isin(manager.image_df['p_hash']).describe()$
data = pd.read_csv("orders_data.csv")$ print(data.shape)$ data.head(2)
output = pd.DataFrame(data={"Id":id, "bot":sub1})$ print(metrics.accuracy_score(output.bot, test_copy1.bot))$
concat4 = pd.concat([free_sub,ed_level], ignore_index=True)$ merge4 = pd.merge(left=free_sub, right=ed_level, how='outer' ,left_index=True, right_index=True)
for user in targets:$     clean_sentiments = clean_sentiments.append(news(user))
pgh_311_data.resample("M").count()
events = events.withColumn('event_date', events['event_date'].cast('date'))
dataPath = os.path.join("..", "example-data")$ data = pd.read_csv(os.path.join(dataPath, "example-from-j-jellyfish.csv"))$ data.head()
violations_list.info()
df['Community League'].value_counts()$
out_temp_columns = [s for s in daily_dat.columns if primary_temp_column in s] #only save select temperature columns$ save_name=Glacier.lower()+ Station + "_daily_"+"LVL2.csv" #filename$ save_pth=os.path.join(save_dir, save_name) #location to save file
print("P(converted) = %.4f" %df2.converted.mean())
utils.fix_coordinates(data)
extractor = twitter_setup()$ tweets = extractor.user_timeline(screen_name="realDonaldTrump", count=200)$ print("Number of tweets extracted: {}.\n".format(len(tweets)))
doc_term_matrix = [dictionary.doc2bow(doc) for doc in tweets_list]
logregmodel = 'logregmodel.sav'$ pickle.dump(pipeline, open(logregmodel, 'wb'))
for column in all_df.columns:$     print ("{:<20s} {:>6.0f}".format(column, all_df[column].nunique()))$     $
tweet_df = pd.DataFrame.from_dict(tweet_ls)$ tweet_df.sort_values(by='Date', ascending=False)$ tweet_df.head()
print("Before drop these columns, data table has shape: ", data.shape)$ data = data.drop(to_drop, axis=1)$ print("After dropping these columns, data table has shape: ", data.shape)
df.isnull().values.any()$
probs = model2.predict_proba(x_test)$ print probs
test_classifier('c2', WATSON_CLASSIFIER_ID_3)$ plt.plot(classifier_stats['c2'], 'ro')$ plt.show()
grouped_publications_by_author.tail(10)
retweet_plot = t_ret.plot(figsize=(16,4), label="Retweets", color="g", legend=True, title='Number of retweets for tweets over time')$ retweet_vs_time_fig = retweet_plot.get_figure()$ retweet_vs_time_fig.savefig('num_ret_over_time.png')
%matplotlib inline$ seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)
Highest_opening_price = mydata['Open'].max()$ Highest_opening_price
df_new.groupby(['country','group']).converted.mean()
crime_and_permits_df = pd.DataFrame({'permit_count': permits_agg['permit_count'], 'crime_count': crime_agg['crime_count']})$ crime_and_permits_df.head()
tweet.text
cens_key = open(os.getenv('PUIDATA')+'/census_key.txt', 'r+')$ myAPI = cens_key.readlines()[0]$
cig_data_SeriesCO.shape
supreme_court_df.head()
grouped_dpt_city = department_df.groupby(["Department", "City"])
jobs_data['clean_description'].replace(to_replace=r"\s([a-z])\1+", value="", regex=True, inplace=True)
df.to_csv('Tableau-CitiBike/TripData_2017_Summer.csv', index=False)
!cd .. && python -m scripts.retrain -h
df_page1 = len(df2.query("landing_page == 'new_page'")) / df2.shape[0]$ print("{} is the probability that an individual received the new page.".format(df_page1))
df.drop(df.query("group == 'treatment' & landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' & landing_page == 'new_page'").index, inplace=True)$ df2 = df
coefficients = pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))$ coefficients
dates.to_period('D')
imgp_clean.info()
df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country'])$ df3 = df3.drop('US', axis = 1)
liquor['Category'] = liquor['Category'].map(lambda x: int(x))$ liquor['County Number'] = liquor['County Number'].map(lambda x: int(x))
df_template = pd.DataFrame(index=datetimes)$ df_template.index.name = 'dt'
abunai_merged_arrays_df = pd.DataFrame(data=abunai_id_array, columns=['id']) 
hired = data.loc[data['hired']==1].tasker_id.value_counts()$ hired[:5]
new_fan.to_csv('../data/new_fan.csv')$ return_fan.to_csv('../data/return_fan.csv')
scaler = preprocessing.StandardScaler().fit(X_train)$ X_train_scaled = scaler.transform(X_train)$ X_test_scaled = scaler.transform(X_test)
rng = pd.date_range(start = '1/1/2017', periods = 72, freq = 'B')$ rng
twitter_archive_clean[twitter_archive_clean['in_reply_to_status_id'].isnull()==False]
import pandas as pd$ import matplotlib.pyplot as plt
print(raw_data.head())$ print(raw_data.shape)
top_songs[top_songs['Track Name'].isnull()]['Region'].unique()$
b_cal_q1.loc[:,'date'] = pd.to_datetime(b_cal_q1['date'])
list(bow_test.columns).index("word2vec_90")$ print((bow_test.columns))$
active_station = session.query(Measurements.station,func.count(Measurements.date)) \$              .group_by(Measurements.station)
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLAday.csv"$ tesla_days = pd.read_csv(url, index_col=0, parse_dates=True)$ tesla_days
Measurement = Base.classes.measurement$ Station = Base.classes.station$
users=pd.concat([train_users,test_users],ignore_index=True)$ users.drop('id',axis=1,inplace=True)$ users
tokens.sort_values('five_star_ratio', ascending=False).head(10)
len(df[~(df.event_properties == {})])
p_diffs = np.array(p_diffs)$ plt.hist(p_diffs);
celtics['season'] = (pd.DatetimeIndex(celtics.date) - np.timedelta64(9,'M')).year
df_new.sample(5)
rng2017 = pd.date_range('2017 Jan 1 00:00', periods = 12, freq = 'MS')$ rng2018 = pd.date_range('2018 Jan 1 00:00', periods = 12, freq = 'MS')$ rng2019 = pd.date_range('2019 Jan 1 00:00', periods = 12, freq = 'MS')
print ("categorical accuracy from mean      :", float(categorical_accuracy(val_labels, val_avg_preds2).eval()))$ print ("best individual categorical accuracy:", np.max(cat_acc))$
coin_data.describe()
session.query(Measure.date).order_by(Measure.date.desc()).first()
df_tsv.info()
store_items = store_items.rename(columns={'bikes': 'hats'})$ store_items
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], 0, 'smaller')$ z_score, p_value
grouped_dpt["Revenue"].filter(lambda x: len(x) < 5)
%matplotlib inline$ import seaborn as sns$ sns.heatmap(pd.DataFrame(years).T.fillna(0))
sns.pairplot(data, hue='species', size=3)
y = api.GetUserTimeline(screen_name="HillaryClinton", count=20, max_id=935706980643147777, include_rts=False)$ y = [_.AsDict() for _ in y]
pd.Series(pd.DatetimeIndex(pivoted.T[labels==0].index).strftime('%a')).value_counts().plot(kind='bar');
dataframe = dataframe.sort_index()
df.describe()
df2['intercept'] = 1$ df2[['control','treatment']] = pd.get_dummies(df2['group'])
bd.reset_index()
df['month_account_created'] = date_account_created.dt.month$ print df.month_account_created$
df[df.isnull().any(axis=1)]
df_mfacts = mfacts[0]$ df_mfacts.columns = ['Name', 'Values']$ df_mfacts
dic={'longitude':x_filtered,'latitude':y_filtered,'density':z}$ searching_set=pd.DataFrame(dic)
train.sort_values('num_points', ascending=False).head()
(pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days
pd.concat([msftAV[:5], aaplAV[:3]], axis=1, keys=['MSFT', 'AAPL'])
pres_df['location'].isnull().sum(), pres_df.shape
df.group.unique()
print(data.head(10))
(~autos["year_of_registration"].between(1910,2016)).sum() / autos.shape[0]
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
df_ad_airings_5['location'][0].split(",")[0]
df_final = pd.concat([df_us_, df_cat, df_crea, df_loc], axis=1, join='inner')
client = foursquare.Foursquare(client_id='1KNECHOW4ALXKWS4OWU2TEUZMPW0WUN1NORS2OUMWWBBCV4C', client_secret='O4QOOLKGDZK44DTBRPQIUDGO2Z4XQYYJQOJ0LN5E5FAQASMM', redirect_uri='http://fondu.com/oauth/authorize')$ auth_uri = client.oauth.auth_url()
import pandas as pd$ pd.set_option('display.max_columns', None)
from sklearn.model_selection import train_test_split as tts$ X_train, X_test, Y_train, Y_test = tts(X,Y, test_size=0.2, random_state = 56)
attend_with = attend_with.drop(['[', ']'], axis=1)$ attend_with = attend_with.drop(attend_with.columns[0], axis=1)
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
data_set.to_csv("hiv_antibody_found.csv", index=False, encoding='utf-8')$
data = []$ for row in result_proxy:$     data.append({'date': row.measurements.date, 'tobs': row.measurements.tobs})
s.loc['c'] $
df.info()
squared_errors_quadratic = [(y_hat[i] - y[i]) ** 2 for i in range(len(y))]$ print("Total sq error is {0}".format(sum(squared_errors_quadratic)))$ print("Average sq error is {0}".format(sum(squared_errors_quadratic)/len(squared_errors_quadratic)))
df2  = df$ df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
from sklearn.linear_model import LinearRegression$ from sklearn.model_selection import train_test_split
json.dumps(letters)[:1000]
(taxiData2.Tip_amount < 0).any() # This Returns False
print(df.apply(np.cumsum))
pd_train.sample(10)
DATA_PATH = "~chandler.mccann/Downloads/"$ INPUT_FILE = os.path.join(DATA_PATH, "cleaned_water_data2.csv") #after running prep_water_data.py$
merge_df['Satisfaction'] = pd.qcut(x=merge_df['Score'],labels=[False, True],q=2)
diff_between_group = mean_per_group.treatment - mean_per_group.control$ diff_between_group
p_old = df2['converted'].mean()$ print("Convert rate for p_old :", p_old)
duplicates = df_cust_data[df_cust_data['Email Address'].isin(df_cust_data['Email Address'].\$                     value_counts()[df_cust_data['Email Address'].value_counts()>1].index)]$ len(duplicates[duplicates['Email Address'] != "no email"])/2
iso_join.fillna(99999, inplace=True)
df2 = df.copy()$ df2.columns
new_page_converted.mean()-old_page_converted.mean()$
bands.to_csv('../data/bands.csv')
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
thai_province_list = set(df.loc[:,'province_in_thai'])$ len(thai_province_list) ## it have 77 province
import requests, json$ data = requests.get(url).json()
temperature_2016_df = pd.DataFrame(Temperature_year)  # .set_index('date')$ temperature_2016_df.head()
measure_avg_prcp_year_df.set_index('Date',  inplace=True)$ measure_avg_prcp_year_df.head()$
crimes[(crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE')&(crimes['SECONDARY_DESCRIPTION']=='TO PROPERTY')].head()$
DummyDataframe = DummyDataframe.set_index("Date").sort_index()$ DummyDataframe = DummyDataframe.groupby("Date").sum()
!open table.html
seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)
df2['user_id'][df2.duplicated(['user_id'], keep=False)]
df.groupby(['weekday']).agg([sum])$
data.sort_values(by = 'Age', ascending = False) #Really? What is a 95 yr old doing on NairaLand?
aa=ins.groupby(["business_id",'year']).size().to_frame().reset_index()$ inspections_by_id_and_year = aa.rename({0:"count"}, axis='columns').set_index(["business_id","year"])$
device_browser = train_data.groupby(['device.browser']).agg({'totals.transactionRevenue': 'sum'}).reset_index()$ device_browser = device_browser.assign(pct = (device_browser["totals.transactionRevenue"]/device_browser["totals.transactionRevenue"].sum()))$ device_browser.set_index("device.browser",drop=True)["pct"].plot.bar()
df = pd.read_csv('../data/hash_rate_raw.csv', names=['Date', 'Hashrate'])
FIGURE_PREFIX = '../figures/'
unique_users = df.user_id.nunique()$ print('The number of unique users in the dataset is {}'.format(unique_users))
AAPL_array=df["log_AAPL"].dropna().as_matrix() 
x = api.GetUserTimeline(screen_name="berniesanders", count=20, include_rts=False)$ x = [_.AsDict() for _ in x]
datasets_co_occurence = paired_df_grouped[['dataset_1', 'best_co_occurence']].set_index('dataset_1').to_dict()['best_co_occurence']
plt.figure(figsize=(8, 5))$ plt.scatter(train_df.favs_lognorm, train_df.comments);$ plt.title('The distribution of the favs_lognorm and number of the comments');$
tempXxgb = tempX.copy()$ tempXxgb = tempXxgb.rename(columns={'fit': 'fit_feat'})
if 0 == 1:$     news_titles_sr.to_pickle(news_period_title_docs_pkl)
print("# of files in unsubscribed :", filecount(os.fspath(master_folder + lists + "unsubscribed/")))$ print("# of files in members :", filecount(os.fspath(master_folder + lists + "members/")))$ print("# of files in cleaned :", filecount(os.fspath(master_folder + lists + "cleaned/")))
print('{0:.2f}%'.format((scores[:1.625].sum() / total) * 100))
control_cnv = df2.converted.mean()$ control_cnv
p_new = df2.query('landing_page == "new_page"')['converted'].mean()$ print('The convert rate for the new page:', p_new)
metadata['wavelength'] = refl['Metadata']['Spectral_Data']['Wavelength'].value$ metadata
tweet_sentiment = []$ for i in range(len(df['text'])):$     tweet_sentiment.append(classify_tweet(df['text'].values[i], classifier))$
df_col_merged =pd.concat([df1, df2], axis=1)$ df_col_merged.head(2)
len(free_data.country.unique())
random_crashes_upsample_df = pd.DataFrame(random.choices(random_crashes_df.values, k=sample_size), columns=list(random_crashes_df.columns))$ random_crashes_upsample_df
df2['date'] = pd.to_datetime(df2['timestamp'])$ df2['date'] = pd.DatetimeIndex(df2.date).normalize()
lm = sm.Logit(df_new['converted'],df_new[['intercept','CA','US','ab_page']])$ result = lm.fit()$ result.summary()
print(groceries.loc[['eggs', 'apples']])$ print(groceries.iloc[[2, 3]])
n_old = df2.query('landing_page == "old_page"')['user_id'].count()$ print(n_old)
gpCreditCard.Tip_amount.describe()
metadata['data_ignore_value'] = float(refldata.attrs['Data_Ignore_Value'])$ metadata
news_organizations_df['tweets'] = news_organizations_df['tweets'].map(df_to_list)
days_range = pd.date_range(start=min_date, end=max_date, freq='D')$ idx_days = [str(s)[:10] for s in days_range]
sns.factorplot('name',kind='count',size = 10,aspect = 2.8, data=wcPerf1_df)$
data.head()
hand = pd.get_dummies(auto_new.Hand_Drive)$ hand.head()
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd.head()
pandas_list = pd.DataFrame(list)$ print("New data type: %s" % (type(pandas_list)))
d = x.T.flatten() # is this data preserved?$ print(d)$ d.base is x # numpy wants to be efficient, but the memory isn't contiguous
afx_17 = json.loads(afx_x_2017.text)$ type(afx_17)
df2.query("group=='treatment'").count()
twitter_archive.source.value_counts()
retweets = megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")]$ megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")]['text'].count() # 1,633
n_old = df2_control.shape[0]$ n_old
from pyspark.sql.functions import regexp_extract, col$
dict_r = r.json()$ type(dict_r)
DataSet.head()
date_max = news_df['Date'].max().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"$ date_min = date_min = news_df['Date'].min().replace(tzinfo=timezone.utc).astimezone(tz = 'US/Eastern').strftime('%D: %r') + " (ET)"
data[(data['author_flair'] == 'Bears') & (data['win_differential'] >= 0.9)].comment_body.head(15)$
df_twitter_archive_master.describe()
free_data.index.name = 'id'$ free_data.head(2)
treatment_cnv = df2.converted.mean()$ treatment_cnv
people_with_one_or_zero_collab['authorId'].nunique()
df3['us_new'] = df3['country_US']*df3['ab_page']$ df3['uk_new'] = df3['country_UK']*df3['ab_page']$ df3.head(3)
df_concat_2 = pd.concat([df_bild, df_spon]) #concats a list of dfs to one df.$
df_pd.sort_values(by='timestamp')$ train_frame = df_pd[0 : int(0.7*len(df_pd))]$ test_frame = df_pd[int(0.7*len(df_pd)) : ]$
archive_copy['tweet_id']= archive_copy['tweet_id']. astype('str')$ type(archive_copy['tweet_id'].iloc[0])
import statsmodels.api as sm$ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller')$ print(z_score, p_value)
    l = s.split(' ')$     return date(to_int(year), month_to_int[l[0]], to_int(l[1]))
x = np.array([0,1,2,3,4,5,6,min])$ x.dtype # it's a C array of python objects!$
df_grouped.columns.tolist()
load2017 = load2017.dropna() 
!bzip2 -d 2001.csv.bz2
mydata.head(3)
import pandas as pd$ data_for_model = pd.read_pickle('data_for_model')
%%bash$ sed 's/^chr//g' post/allV.tab | (head -n 1 - && tail -n +2 - | LANG=C sort) |\$     LANG=C join -t$'\t' --header - ../tools/rs_37_sort.txt > post/allV_RS.tab
df_arch_clean[df_arch_clean['retweeted_status_timestamp'].notnull()]$
df.drop(['seller', 'offerType', 'abtest', 'nrOfPictures', 'dateCreated'], axis='columns', inplace=True)
data = data[data.state != 'undefined']$ data.info()
import pandas as pd       $ train = pd.read_csv("movie-data/labeledTrainData.tsv", header=0, \$                     delimiter="\t", quoting=3)
y_clf = np.where(y_tr >= 1800, 1, 0)$ print(np.where(y_clf==0)[0].shape)$ print(np.where(y_clf==1)[0].shape)
small_ratings_file = os.path.join(dataset, 'ml-latest-small', 'ratings.csv')$ print('small_ratings_file: '+ small_ratings_file)$ small_ratings_raw_data , small_ratings_raw_data_header = read_file(small_ratings_file)
air_store_t = join_df(air_store, air_rsrv_by_date, ["air_store_id","visit_date"])$ air_store_t.head()
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
i = issues$ len(i[(i.activity <= '2011-01-01')])
df_nona['create_date'] = df_nona.created.map(pd.to_datetime)$ app_usage = df_nona.groupby('create_date').count().app_id$ app_usage.resample('w', sum).plot(title='Adoption through time')$
inspector = inspect(engine)$ inspector.get_table_names()
df_ad_airings.dtypes$
df2['DepTimeStr'].count() == df2['DepTime'].count()
ok.auth()
num_users_w_pets = len(pets_pet['owner_id'].unique())$ pct_users_w_pets = round(num_users_w_pets / num_unique_users * 100., 2)$ print("The percentage of users who have added pets is: " + str(pct_users_w_pets) + "%." )
import pandas as pd$ %matplotlib inline$ %config InlineBackend.figure_format = 'svg'
Labels_majority = (((CurrentA1.iloc[:,3:14] + CurrentA2.iloc[:,3:14] + CurrentA3.iloc[:,3:14])/3)>0.3).astype("int32")$ Class_frame_majority = pd.concat([CurrentA1.iloc[:,0:2],Labels_majority],axis=1)$ Class_frame_majority.to_csv("union_voting_elisa.csv")
df_mas['rating_numerator'] = df_mas['rating_numerator'].astype(str)$ df_mas['rating_denominator'] = df_mas['rating_denominator'].astype(str)
bp = USvideos.boxplot(column='like_ratio', by='category_name', vert = False)$
g8_aggregates.columns = ['_'.join(col) for col in g8_aggregates.columns]$ g8_aggregates
rain_df = pd.DataFrame(rain)$ rain_df.head()
dfa.head()
kmeans_model = KMeans(n_clusters=2, init='k-means++', random_state=42).fit(crosstab_transformed)$ c_pred = kmeans_model.predict(crosstab_transformed)
print((p_diffs > obs_diff ).mean())$ print(((p_diffs < obs_diff ).mean()) * 2)
year_prcp_df.set_index('date', inplace=True)$ year_prcp_df.head()
df.isnull().values.any()
some_df = sqlContext.createDataFrame(some_rdd)$ some_df.printSchema()
req = requests.request('GET', 'https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2017-02-01&end_date=2017-02-01&api_key='+API_KEY)$ req.json()
a = temps.read_array()$ plt.imshow(a)
jeff = Customer('Jeff Knupp', 1000.0)    #jeff is the object, which is an iinstance of the *Customer* class
rain = session.query(Measurement.date, Measurement.prcp).\$     filter(Measurement.date > last_year).\$     order_by(Measurement.date).all()$
n_old = df2[df2['group'] == 'control'].shape[0]$ n_old
priors_product_reordered_spec= priors_reordered.groupby(["user_id","product_id"]).size().reset_index(name ='reordered_count_spec')$ priors_product_reordered_spec['userprod_id']=priors_product_reordered_spec['product_id'] + priors_product_reordered_spec['user_id'] *100000$ priors_product_reordered_spec.head(10)
pd.merge(users, transactions.groupby('UserID').first().reset_index(), how='left', on='UserID')
ad_source.to_csv('../data/ad_source.csv')
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print('z-score:', z_score, '\np-value:', p_value)
yc_new = yc_depart.merge(destinationZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')$ yc_new.shape
data_2018 = data_2018.reset_index()
driver = selenium.webdriver.Safari() # This command opens a window in Safari$ driver.get("https://xkcd.com/")
engine = create_engine("sqlite:///hawaii.sqlite")$
import tensorflow as tf$ tf.test.gpu_device_name()
df2['intercept'] = 1$ df2[['ab_page', 'drop_me']] = pd.get_dummies(df2['landing_page'])$ df2.head()
import matplotlib$ import matplotlib.pyplot as plt$ pd.plotting.scatter_matrix(newMergedDF, figsize=(88, 88))$
df2_prop_new = df2.query('landing_page == "new_page"').user_id.nunique() / df2.user_id.nunique()$ df2_prop_new$
dfClientes.iloc[10:20, :]
df[(df.full_sq>10)&(df.full_sq<1500)]$ df.query('full_sq>10 and full_sq<1500')$
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$
frames = [bbc, cbs, cnn, fox, nyt]$ result = pd.concat(frames)$ result.head()
annual_returns = returns.mean() * 250 $
sns.set_palette("deep",desat=.6)$ sns.set_context(rc={"figure:figsize":(8,4)})
clinton_tweets[0].source
from gensim.models import Word2Vec$ model = Word2Vec.load("300features_40minwords_10context")
classes = ['actif', 'churn', 'lost']$ conf_matrix = confusion_matrix(y_true, y_pred)$ plot_confusion_matrix(conf_matrix, classes, 'True label', 'Predicted label', title='Confusion matrix', cmap=plt.cm.Blues)
df2['intercept'] = 1$ df2['ab_page'] = np.where(df2['group'] == 'control', 0, 1)$ df2.head()
df2.user_id.nunique()
raw_data = pd.read_csv('AMZN.csv',sep=',')
pattern = r'^[a-z]'$ mask = df.name.str.contains(pattern)$ mask.value_counts()
assert mcap_mat.shape[0] < 100
Base.classes.keys()$
print('Day : {}\nNight : {}'.format(len(tripDay), len(tripNight)))
p_new = df2[df2['landing_page']=='new_page']['converted'].mean()$ print("The probability of conversion for the new page is: " + str(p_new))
plt.hist(null_values)$ plt.axvline(obs_diff, c='red')
column_list1 = ['DewPoint']$ df[column_list1].plot()$ plt.show()
type(ts.index[0])
NS_active_2015_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2015-06-01']$ NS_active_2016_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2016-06-01']$ NS_active_2017_06 = data_non_seoul.loc[data_non_seoul["Month"]=='2017-06-01']
station_count.sort_values(['Count'],ascending=False, inplace=False, kind='quicksort', na_position='last')
tips.groupby(["sex", "day"]).mean().reset_index()
compound_final.head()
full_act_data.to_csv(os.path.join(data_dir, 'bbradshaw_fbml_data.csv'))
train.business_day = train.business_day.map(lambda x: 1 if x == True else 0)$ train.holiday = train.holiday.map(lambda x: 1 if x == True else 0)
X = pd.get_dummies(X, drop_first=True)
DummyDataframe2 = DummyDataframe[["Tokens","Token_Count"]].copy()$ DummyDataframe2 = DummyDataframe2[["Tokens","Token_Count"]].groupby('Date').agg({'Tokens': 'sum', 'Token_Count': 'sum'})$ DummyDataframe2
training, test = train_test_split(review_df, test_size=0.2, random_state=233)$ print(len(training), "train +", len(test), "test")
prop_conv = df['converted'].mean()$ output = round(prop_conv, 4)$ print("The proportion of Converted users is {}%".format(output*100))
df_videos.head()
weekly_tagsVideosGB = weekly_dataframe(TagsVideosGB)$ weekly_tagsVideosGB[0].head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(z_score)$ print(p_value)
print(r.json()['dataset_data']['column_names'])
raw_readings = {}    $ for row in rows:    $     raw_readings.setdefault(tuple(row[:4]), []).append(tuple(row[4:])) 
mod3 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'US', 'CA']])$ mod3.fit().summary()
tret.plot(figsize=(16,4), color='r');
df2.query('group == "treatment"')$ df2.query('group == "treatment" and converted == 1')$
df2.info()$ df2['user_id'].unique()$ print('There are {} unique user ids'.format(len(df2['user_id'].unique())))
p_diffs = np.array(p_diffs)$ plt.hist(p_diffs)$ plt.axvline(x=obs_diff, color='green');$
confidence  = clf.score(X_test, y_test)$ print("Confidence our SVR classifier is: ", confidence)
df_train = pd.concat((df_train, pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/train_v2.csv',dtype={'is_churn' : np.int8} )), axis=0, ignore_index=True).reset_index(drop=True)
sentiments_df = pd.DataFrame.from_dict(sentiments)$ unique_sentiments_df = sentiments_df.drop_duplicates("Tweet Text", keep = "first")$ unique_sentiments_df
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf]).sample(10)
sites.dtypes
autos["date_crawled"].str[:10].value_counts(normalize=True,dropna=False).sort_index(ascending=True)
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative = 'larger')$ print(' z-score = {}.'. format(round(z_score,4)))$ print(' p-value = {}.'. format(round(p_value,4)))$
uniqueArtists = newUserArtistDF.select("artistID").distinct().count()$ print("Total n. of artists: ", uniqueArtists)$
plt.hist(threeoneone_census_complaints[threeoneone_census_complaints['perc_white']>0]['perc_white'],bins=100)$ plt.show()
results_df.plot(label='prcp', xticks=[], figsize = (20,7)) $
csvFile = open('ua.csv', 'a')
print cust_data.drop_duplicates().head(3)$
if 0 == go_no_go:$     lda_vis_serialized = pyLDAvis.gensim.prepare(lda, serial_corp, d)$     pyLDAvis.save_html(lda_vis_serialized, fps.pyldavis_fp)
top_10_authors = git_log['author'].value_counts().head(10).to_frame()$ top_10_authors
recess = pd.read_csv("payems.csv", header = 5, parse_dates = ['date'])$ recess['date'] = pd.to_datetime(recess['date'])$ recess.index = recess['date']
df2.query('user_id == 773192')
df2.converted.mean()
ab_df_new['treatment_US'] = ab_df_new.ab_page * ab_df_new.US$ ab_df_new['treatment_CA'] = ab_df_new.ab_page * ab_df_new.CA$ ab_df_new.head()
crosstab_transformed = pca.transform(crosstab)
grouped_date = merged_table.groupby(['Date']).agg({"Likes": "sum","Retweets": "sum","Compound":"mean",$                                                       "Negative":"mean","Neutral":"mean","Positive":"mean"}).apply(list).reset_index()$ grouped_date
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ result = logit_mod.fit()$
r_train, r_test, rl_train, rl_test = train_test_split(r_forest.ix[:,col], r_forest['bot'], test_size=0.2, random_state = 2)$ r1_train, r1_test, rl1_train, rl1_test = train_test_split(r_forest.ix[:,col1], r_forest['bot'], test_size=0.2, random_state = 2)$
xgb_learner.fit_best_model(dtrain)
fcc_nn.plot(y='score', use_index=True)
tlen = pd.Series(data=datos['len'].values, index=datos['Creado'])$ tfav = pd.Series(data=datos['Likes'].values, index=datos['Creado'])$ tret = pd.Series(data=datos['RTs'].values, index=datos['Creado'])
autos.describe(include = 'all')
tips.sort_values(["tip","size"],ascending=False).head(10)
apple.resample('M').mean().plot(grid=True)
retweet_and_count = sns.factorplot(data=tweets_df, x="name", y="retweet_count", kind="box")$ retweet_and_count.set(yscale="log")$ plt.xticks(rotation=60)     # alwatan have above average number of retweets and alseyassah have below average number of retweets$
display('df5', 'df6',$        "pd.concat([df5, df6], join_axes=[df5.columns])")
p_null = np.mean(df2['converted'])$ p_null
sentences = [['first', 'sentence'], ['second', 'sentence']]$ model = gensim.models.Word2Vec(sentences, min_count=1)
s519397_df["prcp"].mean()
df.sum().plot(kind='bar')
df2 = df.drop(df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')].index)$ df2 = df2.drop(df2[(df2['group'] != 'treatment') & (df2['landing_page'] == 'new_page')].index)$ df2.head()
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)$
plt.savefig('Sentiment_Analysis_News_Organizations_Tweets.png')
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')$ print(soup.prettify())
df = pd.read_csv('data/cornwall_phones.csv')$ df.head()
btc.corr()
events_top10_df = events_enriched_df[events_enriched_df['topic_id'].isin(top10_topics_list)].copy()$ events_enriched_df.shape[0], events_top10_df.shape[0]
lm.pvalues
df_precep_dates_12mo = pd.read_sql_query("select date, prcp from measurement where date > '2016-08-22';", engine)$ df_precep_dates_12mo
!gsutil cp gs://solutions-public-assets/smartenup-helpdesk/ml/issues.csv $CSV_FILE
df_vow.plot()$ df_vow[['Open','Close','High','Low']].plot()
f_new[['CA','US']]=pd.get_dummies(f_new['country'])[['CA','US']]
((null_values<obs_diff).mean())
log_mod2 = sm.Logit(df2['converted'], df2[['intercept', 'CA', 'UK']])$ results2 = log_mod2.fit()$ results2.summary()
most_yards[['Date','PlayType','Yards.Gained','qtr','desc','Rusher','Receiver', 'Jimmy']][:10]
prcp_query = session.query(func.strftime(measurements.date), (measurements.prcp)).\$ filter(measurements.date <= last_date, measurements.date>= last_date-relativedelta(months=12)).all()$ prcp_query
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
df = pd.read_csv('msa.csv')
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])$ df_new.head()$
path_ids = list(map(lambda x: x.split('\t')[0], res.json()))$ path_url_human = list(map(lambda x: 'http://rest.kegg.jp/get/' + x.replace('path:map', 'hsa') + '/kgml', path_ids))$ pp(path_url_human)
print('The number of rows in the dataset:', df.shape[0], 'rows')
tsla_30 = mapped.filter(lambda row: row[3] > T0)$ tsla_30_DF = tsla_30.toDF(["cid","ssd","num_ssd","tsla","tuna"])$ tsla_30_pd = tsla_30_DF.toPandas()
prob_treat =df2.query("group=='treatment'").converted.mean()$ print('The probality of an individual converting in the treatment group converting is {}'.format(prob_treat))
df_master[df_master.rating_numerator==1776].jpg_url
df_new.country.unique()
compacted.to_csv(basedirectory+projectname+'/'+projectname+'_images.csv')
precipitation_df.index=pd.to_datetime(precipitation_df.index)
lgb.plot_importance(lgb1, max_num_features=30)$ plt.show()$
((df-df_from_csv)**2 < 1e-25).all()
X = reddit['title'].values$ y = reddit['engagement']$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
added_series = pd.Series(index=daterange)$
qw = qgrid.show_grid(all_tables_df, show_toolbar=True)
countries = wb.get_countries()$ countries.iloc[0:10].ix[:,['name','capitalcity','iso2c']]
df.loc[6:10]
df2_with_country = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df2_with_country.head()
train.popular.value_counts(normalize=True)
df['y'].plot.box(notch=True, showmeans=True)
def my_scaler(col):$   return (col - np.min(col))/(np.max(col)-np.min(col))$ data_scaled = data_numeric.apply(my_scaler)
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
StockData.describe()
df['text']=df['title'].str.replace('\d+', '')$
df.sort_values('date', inplace=True)$ df.head()
print(norm.cdf(z_score))
s1.index
auth = tweepy.OAuthHandler(consumer_key=consumer_key, $     consumer_secret=consumer_secret)$ api = tweepy.API(auth)
df.plot();
twitter_coll_reference.count()
df2_dummy.set_index('user_id', inplace=True)
bad_iv_post = options_frame[np.isnan(options_frame['ImpliedVolatilityMid'])]
data_df.groupby('nwords')['ticket_id'].nunique()
df_chapters_read['referer'].value_counts()
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'$ response = requests.get(url)$ response
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]
unique_users = len(df['user_id'].unique())$ print('Number of unique users in the dataset:{}'.format(unique_users))
df_goog['Closed_Higher'] = df_goog.Open > df_goog.Close$ df_goog['Closed_Higher'] = pd.get_dummies(df_goog.Open > df_goog.Close).values
station_count = session.query(Station.station).count()$ print(f"There are a total of {station_count} stations in this dataset")$
a = [0,1]$ ps_new = [p_new, 1-p_new]$ new_page_converted = np.random.choice(a, size=n_new, p=ps_new)
data = spark.read.csv('sensor_data.csv',header=True)
sample_item = [df_stars.iloc[0].business_id]$ content_rec.recommend_from_interactions(sample_item)
coll.distinct("overall_status")
calories_df.to_csv('python_data/calories_from_moves.csv', index=False)
shows['genres'].fillna('missing', inplace=True)
data.show()
pd.date_range('8/14/2017 14:41:31', periods=5)
x = df.day_of_year.value_counts(sort=False)$ vc = pd.DataFrame(x)
for row in cursor.columns(table='TBL_FCInspevnt'):$     print(row.column_name)
plt.hist(p_diffs)$ plt.axvline(x=0.000913, color='r')
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_decomposition(RN_PA_duration, params=params, freq=31, title='RN/PA Decomposition')
model_artifact = MLRepositoryArtifact(model, training_data=train.select('ATM_POSI','POST_COD_Region','DAY_OF_WEEK','TIME_OF_DAY_BAND','FRAUD'),\$                                       name="Predict ATM Fraud")$ model_artifact.meta.add("authorName", "Data Scientist");
twitter[twitter.tweet_id.duplicated()]$
exclude_year = [1985, 1986, 1987] $ lv_workspace.get_subset_object('A').set_data_filter(step=1, filter_type='exclude_list', filter_name='YEAR', data=exclude_year) 
p_old = df2.query('converted == 1').user_id.count()/df2.user_id.count()$ print('Conversion of old Page is: ', p_old)
minimum_temp = df_2005_2014[df_2005_2014['Element'] == 'TMIN'].groupby('Month-Date').aggregate({'Data_Value':np.min})$ maximum_temp = df_2005_2014[df_2005_2014['Element'] == 'TMAX'].groupby('Month-Date').aggregate({'Data_Value':np.max})$ maximum_temp.head()
df = df.T$ df
1-0.190/2
transformed_test_pending_ratio = \$ polynomial_features.transform(x_test['Pending Ratio'].values.reshape(-1,1))
labels.describe()
mean_temp = temp_long_df.groupby(['date'])['temp_c'].mean()$ mean_temp.plot(x='date', y='temp_c')
bd.index.name
userID = np.array(users["id"])$ my_solution = pd.DataFrame(my_prediction, userID, columns = ["country_destination"])$ print(my_solution)
Line_Treatement=df.loc[(df['landing_page']=='new_page') & (df['group']=="treatment"),].shape[0] #Find no of entries where they do line up$ Line_Control=df.loc[(df['landing_page']=='old_page') & (df['group']=="control"),].shape[0] $ print("The total no of times new page and treatment dont line up are",Total_entried-Line_Upentries-Line_Control)
print(oil.isnull().sum(), '\n')$ print('Type : ', '\n', oil.dtypes)$ oil.head(5)
cohort_activated_df = pd.DataFrame(index=daterange,columns=daterange)
s = BeautifulSoup(doc,'html.parser')$ print(s.prettify())
obs_diff_control = df2.query('group == "control"').query('converted == 1')['user_id'].count() / df2.query('group == "control"')['user_id'].count()$ obs_diff_control
DataSet[['userName','userFollowerCt','tweetRetweetCt']].sort_values('tweetRetweetCt',ascending=False).head(10)
funding_pct = merged_df.groupby('Funding Type')["Money Raised"].count()$ funding_pct$
df_subset['diff'] = df_subset.apply(diff_money, axis = 1, pattern = pattern)$ print(df_subset.head())$ 
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG RN/PAs', RN_PA_duration, window=52)
groups = openmc.mgxs.EnergyGroups()$ groups.group_edges = np.array([0., 0.625, 20.0e6])
pd.DataFrame(features['LAST(loans.MEAN(payments.payment_amount))'].head(10))
train.ORIGINE_INCIDENT.value_counts()
writer = pd.ExcelWriter('my_dataframe.xlsx')$ merged_df.to_excel(writer, 'Sheet1')$ writer.save()
google[['Open', 'High','Low','Close','Adj Close']].plot()
extended_tweets.info()$ get_real_types(extended_tweets)
%cd drive/CloudAI/nmt-chatbot$ !python utils/prepare_for_deployment.py
preview.loc[0:10,["Complaint Type", "Agency"]]
plt.hist(null_vals)$ plt.axvline(x=obs_diff,color ='red')
filename1 = 'expr_3_nmax_32_nth_0.1_ns_0.01_04-19.csv'$ df = pd.read_csv('../output/data/expr_3/' + filename1, comment='#')$
train = train.loc[lambda x: x['duration'] < 1800,:]$
rf_v2.hit_ratio_table(valid=True)
df2 = df[((df.group == 'treatment') & (df.landing_page == 'new_page')) | ((df.group == 'control') & (df.landing_page == 'old_page'))]
import os$ sc.addPyFile(os.path.expanduser('~/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar'))$ from graphframes import *
acs_df.dropna(inplace=True)$ acs_df = acs_df[['pop', 'age', 'pct_male', 'pct_white', 'income', 'unemployment', 'pct_bachelors',$                   'crime_count', 'permit_count', 'homeval']]
df.age.hist(bins = 40, figsize = (10,5))
X_prepro = psy_prepro.drop(labels=["y"], axis =1)$ y_prepro = psy_prepro["y"]
str(df.columns)
pd.concat([msftAV, aaplA], join='inner')
df.head()
plt.figure(figsize=(12,12))$ sns.heatmap(df_m[features].corr(), annot=True)   ## these are features only$ plt.title("features=12 Correlation Heatmap") 
!hdfs dfs -cat {HDFS_DIR}/p32b-output/part-0000* > p32b_results.txt$
turnstiles_df = turnstiles_df.rename(columns=lambda x: x.strip())$
sess = tf.Session()$ print(sess.run(result))
releases = pd.read_csv('../input/jail_releases.csv')$ bookings = pd.read_csv('../input/jail_bookings.csv')
new_page_group = df2.query('landing_page == "new_page"')$ print(treatment_group.user_id.nunique() / df2.user_id.nunique())$ print(new_page_group.user_id.nunique() / df2.user_id.nunique())
x = plt.gca().xaxis$ for item in x.get_ticklabels():$     item.set_rotation(45)
svc = SVC(random_state=20)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'decision_function_shape':['ovo', 'ovr'], 'kernel':['linear', 'rbf']}$ grid_svc = GridSearchCV(svc, param_grid=param_grid, cv=10, n_jobs=-1)
archive_clean.info()
Base.prepare(engine, reflect=True)
train = hn[hn.created_at < july_1].copy()$ new = hn[hn.created_at >= july_1].copy()
number_of_commits = len(git_log)$ number_of_authors = len(git_log.query("author != ''").groupby('author'))$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
Counter(orgs['primary_role']).most_common(10)
header = flowerData.first()$ flowerKV= flowerData.filter(lambda line: line != header)$ print (flowerKV.collect())
sum([1 for row in U_B_df.cameras if len(row) > 2])
billstargs.drop(([408], [416], [456], [718], [757], [811], [928], [968], [1075], [1204], [1463], [1464], [1466], [1467], [1468], [1469], [1470], [1497], [1552], [1555], [1576], [1577], [1655], [1801], [2125], [2126], [2127], [2128], [2129], [2130], [2131], [2260], [2428], [2599]), inplace=True)$
cumFrame = pd.DataFrame(changes, columns=('Date', 'Client', 'Change'))$ cumFrame = cumFrame.sort_values('Date')$ print(cumFrame.loc[cumFrame['Client'] == 'AT&T'])$
print('\n Row x Columns of imported data:')$ print(dfa.shape)$ print(type(dfa))
df2 = df2.drop(df2.index[2893])
sample = pd.read_csv(os.getcwd() + "/../Turk/6source_results_filtered.csv")$ print sample.head()
df_new[["CA","UK","US"]]=pd.get_dummies(df_new["country"])$ df_new.drop("US", inplace=True, axis =1)$ df_new.head()
lm = sm.Logit(df['converted'], df[['intercept', 'ab_page']])$ results = lm.fit()
tweets['time_eastern'] = tweets['created_at'].apply(lambda x: x.tz_localize('UTC').tz_convert('US/Eastern'))
nb_pipe_2.fit(X_train, y_train)$ nb_pipe_2.score(X_test, y_test)
engine = create_engine("sqlite:///hawaii.sqlite")
n = 1$ selection = 'confidence'$ topn = summary.sort_values(by=['week_id', selection], ascending=[True, False]).groupby('week_id').head(n)
df_sum_by_stage = twitter_archive_full[twitter_archive_full.stage !='None'].groupby('stage')['retweet_count','favorite_count'].sum()$ df_sum_by_stage.plot(y = ['retweet_count','favorite_count'], kind='bar').set_title('Distribution of retweet and favorite by stage');$
df_cod2["Cause of death"].unique()
from nltk.corpus import stopwords$ stop_words = stopwords.words(['english','danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish'])$ stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
def unicode_normalize(text):$     return text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22,$                             0xa0:0x20 }).encode('utf-8')
from sklearn.mixture import GaussianMixture$ gmm = GaussianMixture(2)
output= "Create view ViewDemo as select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc;"$ cursor.execute(output)$
all_zeros = 1 - val_y.mean()$ all_zeros
autos['odometer'] = autos.odometer.str.replace(',','').str.replace('km','').astype(float)
df_links['link.domain'].value_counts().head(25)
mid = sample_submission['ID']$ result.insert(0, 'ID', mid)$ result.head()
Genres=movie_rating['genres'].values.tolist()
df_geo_insta['hour']=hours$ df_geo_insta['hour'].unique()$
twitter_archive_full = pd.merge(twitter_archive_with_json, image_prediction_clean, how='left', on=['tweet_id'])
df2['user_id'][(df2['user_id'].duplicated() == True)]
positive = '/Users/EddieArenas/desktop/Capstone/positive-words.txt'$ positive = pd.read_table('/Users/EddieArenas/desktop/Capstone/positive-words.txt')
print('F : {}\nM : {}'.format(len(ageF), len(ageM)))
port.txns.frame.tail()
baseball_df.info()
sh_max_df.dtypes
merged_df.fillna('unknown', inplace=True)$ merged_df.rename(columns={'GROUP':'Group'}, inplace=True)$ merged_df.head()
search['search_weekday'] =  search.timestamp.dt.dayofweek+1$ search['trip_start_date_weekday'] =  search.trip_start_date.dt.dayofweek+1$ search['trip_end_date_weekday'] =  search.trip_end_date.dt.dayofweek+1
print(df.columns)$ df.rename(columns = {'usd pledged':'usd_pledged'}, inplace=True)$
ride_df_rural = rural.groupby('city')$ city_df_rural = rural.set_index('city')
cust_data1=cust_data.assign(No_of_30_Plus_DPD=cust_data.No_of_30_59_DPD+cust_data.No_of_60_89_DPD+cust_data.No_of_90_DPD,$                            MonthlySavings=cust_data.MonthlyIncome*0.15)
old_page_converted = np.random.binomial(1, p_old,n_old)$ old_page_converted
from gensim.models import Word2Vec$ model = Word2Vec.load("300features_40minwords_10context")
fb_data = graph.get_object(id='DonutologyKC/', fields=req_fields)$ type(fb_data)
daily_mcap_mat.resample('M').asfreq().index
aug2014.start_time, aug2014.end_time
nitrogen['ActivityMediaSubdivisionName'].unique()
com_eng_df = ghtorrent.community_engagement('rails', 'rails')$ com_eng_df = com_eng_df.set_index(['date'])
plt.hist(null_vals)$ plt.axvline(x=act_diff, color='red');
plt.hist(p_diffs);$ plt.axvline(obs_diff, color='red')
marsfacts_url = 'https://space-facts.com/mars/'$
n_old=df2.query('landing_page == "old_page"').user_id.nunique()$ n_old
first_result.find('strong')$ first_result.strong
yuniq = df['hotel_cluster'].unique()$ if list(range(len(yuniq)))==sorted(list(yuniq)):$     print('Hotel cluster classes span range of {0} to {1}'.format(min(yuniq), max(yuniq)))
print(df.shape)$ df = df[df['Month-Date'] != '02-29']$ print(df.shape)
df2017=df1.iloc[0:165,]  #line 165 is 2016 so we need 1 more than line 164 ie 165$ df2017.tail()
df.to_csv('ab_cleandata.csv', index=False)
cur.fetchall()
dc['created_at'] = pd.to_datetime(dc['created_at'], format='%Y-%m-%d %H:%M:%S')$ tm['created_at'] = pd.to_datetime(tm['created_at'], format='%Y-%m-%d %H:%M:%S')$ dc.created_at.dtype #tm.created_at.dtype
words = latest_tweet['full_text'].split(' ')
from sightengine.client import SightengineClient$ client = SightengineClient("737618018", "bstrJ5VzARavYy5FsELN")$ output = client.check('face-attributes').set_url('https://instagram.fprg2-1.fna.fbcdn.net/vp/aa7a6811e2fbc814c91bb94e92aa8467/5B197538/t51.2885-19/s150x150/11875444_527830867370128_1019973931_a.jpg')
leadsdf.to_csv('Leads App Report From 30-08-2017 to 02-09-2017.csv')$
import statsmodels.api as sm$ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ (z_score,p_value)
from scipy.stats import norm$ norm.cdf(z_score), norm.ppf(1-(0.05))# Tells us what our critical value at 95% confidence is
df2 = df.drop(df.query('group == "control" and landing_page == "new_page"').index)$ df2 = df2.drop(df.query('group == "treatment" and landing_page == "old_page"').index)$ df2.head()
np.count_nonzero(df.isnull().values)
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True)
typesub2017['Solar'] = typesub2017['Solar'].astype(int)$ typesub2017['Wind Offshore'] = typesub2017['Wind Offshore'].astype(int)$ typesub2017['Wind Onshore'] = typesub2017['Wind Onshore'].astype(int)
mhemi_image = soup.find_all('div',class_="item")
len(df_events), len(df_events.group_id.unique())
plot_mnist_sample(mnist_train.train_data, $                   sample_idx=[i for i in range(10)])$ plot_mnist_sample(mnist_test.test_data, size=10)$
highest_temp_station_no = active_stations[0][0]$ highest_temp_obs = active_stations[0][1]$ print(f"The station with the highest number of temperature observations is {highest_temp_station_no} with total observations of {highest_temp_obs}.")
url_hemispheres = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars"$ browser.visit(url_hemispheres)
actual_diff = df.query("group == 'treatment'").converted.mean() - df.query("group == 'control'").converted.mean()$ p_diffs = np.array(p_diffs)$ (actual_diff < p_diffs).mean()
writer=pd.ExcelWriter('output.xlsx')$ kk.to_excel(writer,'Sheet1')$ writer.save()
np.exp(-0.0149), np.exp(0.0506), np.exp(0.0408)
dfm = (filtered_df['l_req_3d'] - filtered_df['l_req_3d_num_tutors'])$ filtered_df.loc[dfm > 0, :]
df.shape[0]
import datetime as dt$ mydata = dc2015.copy()$ mydata['new'] = np.where((mydata['created_at'].dt.time >= '0:00') & (mydata['created_at'].dt.time < '12:00'), 'morning', 'evening')
messages=df.status_message.tolist()$ messages[:5]
autos.price.unique().shape
df.drop(remove_cols, axis=1, inplace=True)
extract_deduped_with_elms = extract_deduped_with_elms_v2.copy()
tallies_file.export_to_xml()
grouped.describe()
df3 = df2.drop(['FlightDate', 'DepTime', 'DepTimeStr'], axis=1)
n_new = df2[df2['group'] == 'treatment'].shape[0]$ n_new
techmeme['nlp_text'] = techmeme.titles.apply(lambda x: tokenizer.tokenize(x.lower()))$ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: ' '.join(x))
twitter_json = r'data/twitter_01_20_17_to_3-2-18.json'$ tweet_data = pd.read_json(twitter_json)
test_preds_df = pd.DataFrame(test_preds,index=test_target.index,columns=['kwh_pred'])$
p_mean = np.mean([p_new, p_old])$ print("Probability of conversion according to null hypothesis (p_mean) is",$       p_mean)
autos['odometer'] = autos['odometer'].str.replace(',', '').str.replace('km', '')$
df_schools.columns.tolist()
numbers = {'integers': [1,2,3], 'floats': [1.1, 2.1, 3.1]}$ numbers_df = pd.DataFrame(numbers)$ numbers_df
es.get(index="test-index", doc_type='tweet', id=1)
sns.violinplot(x=df['liked'],y=df['Time_In_Badoo'],data=df,whis=np.inf)$ plt.title('By Time In Badoo')$
maxCol=lambda x: max(x.min(), x.max(), key=abs)$ journeys['acceleration'] = journeys[['x', 'y', 'z']].apply(maxCol,axis=1)$
msft = pd.read_csv("msft.csv", $                     dtype = { 'Volume' : np.float64})$ msft.dtypes
from statsmodels.tsa.arima_model import ARIMA$ model_6203 = ARIMA(dta_6208, (2, 1, 2)).fit() $ model_6203.forecast(5)[:1] 
CSV_FILE_PATH = os.path.join('pims_cloudpbx_subset_201806051550_1million.csv')$ GEOLITE_ASN_PATH = os.path.join('GeoLite2-ASN.mmdb')$ GEOLITE_CITY_PATH = os.path.join('GeoLite2-City.mmdb')
user_repeated = df2[df2.duplicated(['user_id'])]['user_id'].unique()$ print('The repeated user_id is {}'.format(user_repeated))
df_cod3 = df_cod2.copy()$ df_cod3["Cause of death"] = df_cod3["Cause of death"].apply(classify_natural)$ df_cod3
pd.date_range('1/1/2017', '12/1/2017', freq='M') $
drop_cols = ["seller", "offer_type", "nr_of_pictures"]$ autos = autos.drop(columns = drop_cols)
month.to_csv('../data/month.csv')
df_geo_count = df_geo.groupby("geo_code").count()$ dict_geo_count = df_geo_count.to_dict()["id_str"]
predictor_df = kick_data_state[['backers_count', 'blurb_count', 'goal_USD']]$ predictors = list(predictor_df.columns.values)$ predictors
df.loc[df['last_name']=='Copening', 'age'] = df.age.median()$ df.loc[df['last_name'] == 'Copening']
ans = pd.pivot_table(df, values='D', index=['A','B'], columns=['C'])$ ans
import pandas as pd$ PATH='light_sensor-20180705-1304.csv'$ df=pd.read_csv(PATH)
print(soup.prettify())
dfg = dfg.sort_values(['discharges'], ascending=[False])$ dfg = dfg.reset_index(['drg3']) $ dfg.head()
plt.rc('figure', figsize=(5, 5))$ mosaic(crosstabkw.stack(),gap=0.03 )$ plt.title('Mosaic graph: Successful / Contain Keyword')$
y_hat = model.predict(X_test)
crypto_combined = pd.concat([crypto_data, crypto_ggtrends], axis=1).dropna(how='any')$ crypto_combined_s = crypto_combined.copy(deep=True)$ print(crypto_combined_s.head(10))
events_df['event_day'] = events_df['event_time'].apply(lambda d: d.replace(hour=0,minute=0,second=0))$ events_df['event_week'] = events_df['event_day'].apply(lambda d: d - datetime.timedelta(d.weekday()))$ events_df['event_weekday'] = events_df['event_day'].apply(lambda d: d.weekday())
reviewsDF.head()
cpdi = pd.DataFrame(coreval); cpdi.rename(columns={0:'i', 1:'core'}, inplace=True)$ xpdi = pd.DataFrame(xtraval); xpdi.rename(columns={0:'i', 1:'xtra'}, inplace=True)
xml_in.shape
df_countries = pd.read_csv("countries.csv")$ df3 = df_countries.set_index('user_id').join(df2.set_index('user_id'), how = 'inner')$ df3.head()
y_train.value_counts(normalize=True)
lookforward_window = 1$ for iter_x in np.arange(lookforward_window)+1:$     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
aldf = indeed1.append(tia1, ignore_index=True)
weather_x = weather_norm.drop('tavg', axis=1)$ weather_y = weather_norm['tavg'].shift(-1)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_secret)$ api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)
print("The tweet with more retweets is: \n{}".format(data['Tweets'][rt]))$ print("Number of retweets: {}".format(rt_max))$ print("{} characters.\n".format(data['len'][rt]))
dfd = dfh[dfh['Centrally Ducted or Ductless'].str.startswith('Ductless')]$ print(len(dfd))$ dfd['Centrally Ducted or Ductless'].unique()
import statsmodels.api as sm$ logit = sm.Logit(df2_new['converted'], df2_new[['ab_page', 'intercept']])$ results=logit.fit()
msft = pd.read_csv("../../data/msft.csv")$ msft.head()
df['timestamp'] = pd.to_datetime(df['timestamp'])$ df['day'] = df['timestamp'].dt.day$ df_converted = df.query('converted == 1 & timestamp > "2017-01-03 00:00:00.00" & timestamp < "2017-01-24 00:00:00.00"')$
S_lumpedTopmodel.initial_cond.filename
df2 = df.loc[lambda df: df['duplicate_user'] == 0]$
DataSet.head(2)
df.info() 
data.sort_values('TMED', inplace=True, ascending=False)$ data.head()
data.count()
rhum_us = rhum_nc.variables['rhum'][1, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(rhum_us)
df2['converted'].mean()
cursor = collection_reference.find()
Magic.__dict__['__repr__'].__get__(None, Magic)$
df2017.groupby(['EventID'])['MoneyLineAway-change'].sum().mean()
cityID = '319ee7b36c9149da'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Arlington.append(tweet) 
df = pd.read_pickle('all-RSS.pkl')
df["mail ID"] = df["mail ID"].apply(lambda x: x.split('.')[0])$ df = df.set_index("mail ID")
mask = ((combined.state=='Colombia') & combined.city.str.startswith('Pe')).pipe(np.invert)$ combined = combined.loc[mask]
walk.resample("1Min", closed="right")
df = pd.concat(map(pd.read_csv, glob.glob("*.csv")))
def first_event_check(id):$     return min(a_active_devices_df[a_active_devices_df['device_id']==id]['create_time'])$ a_active_devices_df['first_activity_time'] = a_active_devices_df['device_id'].apply(first_event_check)
repeat_user = df2.user_id.value_counts()[0:1]$ print('The repeat user is {}'.format(repeat_user))
unique_users2 = df2.user_id.nunique()$ print('The unique number of users_id in the df2 is {}'.format(unique_users2))
print(mbti_text_collection.info())
gs.score(X_test_total_checked, y_test)
author_df = pd.DataFrame(list(map(lambda x: x.__dict__, session.query(Developer).all())))
y.mean()
titanic.groupby('sex')[['survived']].mean()
twitter_Archive.info()
taxiData.Trip_distance.describe()
diffs = np.array(p_diffs)$ p_val=(diffs > obs_diff).mean()$ print('The p-value for the differences of conversion rates is {}.'.format(round(p_val,4)))
dict_tokens = corpora.Dictionary(tokens)
df2 = df.dropna(how = 'any')$ df2
def get_duration_career(input_):$     return max(input_) - min(input_)$ grouped_publications_by_author['duration_career'] = grouped_publications_by_author['publicationDates'].apply(get_duration_career)$
np.mean(df['converted'])
df_ad_airings_5.info()$
cityID = '9531d4e3bbafc09d'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oklahoma_City.append(tweet) 
city_avg_fare_renamed = city_avg_fare.rename(columns={"fare": "average_fare"})$ citydata_avg_fare_work = city_avg_fare_renamed[['city', 'driver_count', 'type', 'average_fare']]$
windfield_proj = windfield.GetProjection()$ windfield_proj
archive_df.shape,image_file.shape,tweet_df.shape
rd_xls = pd.read_excel('../data/WICAgencies2014ytd.xls',sheet_name='Pregnant Women Participating')$ rd_xls.head()
last_day = (dt.datetime.strptime('2017-08-01', '%Y-%m-%d') - dt.timedelta(days=365)).strftime('%Y-%m-%d')$ last_day
temp_df['reorder_interval_group'] = temp_df['reorder_interval_group'].astype(float)
tfidf = models.TfidfModel(corpus) #just to train the tfidf model$ corpus_tfidf = tfidf[corpus] #now the corpus is represented as a tfidf matrix$ lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)#nitialize an LSI transformation$
country_sort = averages[averages.Country == 'Canada']$ country_sort
!hdfs dfs -cat /user/koza/hw3/3.2.1/productFrequencies/* | wc -l
html_table_marsfacts = html_table_marsfacts.replace('\n', ' ')$ html_table_marsfacts
who_purchased = pd.get_dummies(questions['purchased'])
payload = {'start_date':'2017-01-01','end_date':'2017-12-31','api_key':API_KEY}$ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X', params=payload)$
y_train_hat = model.predict(X_train)$ y_train_hat = squeeze(y_train_hat)$ CheckAccuracy(y_train, y_train_hat)
! rm -rf splits1$ ! mrec_prepare --dataset ml-100k/u.data --outdir splits1 --rating_thresh 4 --test_size 0.5 --binarize
jail_census.loc['2017-02-01'].groupby('Gender')['Age at Booking'].mean()
pd.DataFrame([[feat, coef] for feat, coef $               in zip(test_X.columns, fit.coef_[0])], $              columns = ['Feature', 'Coefficient']).sort_values(by='Coefficient', ascending=False)
columns = inspector.get_columns('measures')$ for c in columns:$     print(c['name'], c["type"])$
df1.tail(36)
pystore.set_path('./pystore_demo')$ pystore.get_path()
wrsids[np.where(ids=='105001')[0][0]]$ 
height = soup.find_all(class_='quiver-surf-height')$ height.text
baseball1_df.loc[baseball1_df['ageAtDebut'].idxmin()]
import pandas as pd$ df = pd.DataFrame(tweets_data, columns=['text', 'id'])$ print(df.head())
sns.boxplot(data=sample)
len(api_result_df.loc[api_result_df['project_id'].isin(project_df['project_id'])]['project_id'].unique())
df.shape$
from pyspark.sql.functions import col $ file2=file.where((col('trans_start_month')==7) | (col('trans_start_month')==8))$ file2.show(3)
df.converted.mean()
treatment_group = df2.query('group == "treatment"')$ print(treatment_group.shape[0])
x_axis = np.arange(0, Total_Number_of_Rides_max+6, 5)$ x_axis
autos.loc[:,"ad_created"] = pd.to_datetime(autos.loc[:,"ad_created"])$ autos.loc[:,"last_seen"] = pd.to_datetime(autos.loc[:,"last_seen"])
df2.groupby(['group'],as_index=False).mean()
station_count= session.query(Measurement.station ).group_by(Measurement.station ).count()$ print(f"There are {station_count} available in dataset")
user_df = user_df.rename(columns={'created_at': 'user_created_at'})
(final_rf_predictions['predict']==test['Cover_Type']).as_data_frame(use_pandas=True).mean()
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))
S_distributedTopmodel.meta_basinvar.filename
df2_treatment = df2.query("group == 'treatment'")$ convereted_rate_new = round(df2_treatment.converted.mean(),4)$ print(convereted_rate_new)
cr_under_null = df2['converted'].mean()    # Under the null, use both groups.$ cr_under_null$
Base.classes.keys()
cleansed_search_df['SearchTerm'] = np.where(cleansed_search_df['SearchCategory'] == "Plant", cleansed_search_df['SearchTerm'].str.rpartition('-')[2].str.strip() , cleansed_search_df['SearchTerm'])$ cleansed_search_df.loc[cleansed_search_df['SearchCategory'] == "Plant"]
vectorizer = TfidfVectorizer(max_df=0.1)  $ train_tweets_vector = vectorizer.fit_transform(train_tweets['pasttweets_text'])$ dev_tweets_vector = vectorizer.transform(dev_tweets['pasttweets_text'])
expiry = datetime.date(2015,1,17)$ aapl_calls = aapl.get_call_data(expiry=expiry)$ aapl_calls.iloc[0:5,0:4]
df2 = df2.drop(dup_user.index)$ df2[df2['user_id'].duplicated()]
df.describe()
conn_a.commit()
telecom3.to_csv("E:\IIIT Bangalore AIML\Group Assignment 2\\telecom_churn_data_clean2.csv", sep=',', index=False)
df1.fillna(-999999,inplace=True)
cityID = 'e4a0d228eb6be76b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Philadelphia.append(tweet) 
sale_country_sort = sale_average[sale_average.Country == 'Canada']$ sale_country_sort
Tweet_DF=Tweet_DF3.append(Tweet_DF2,ignore_index=True)
new_page_converted.mean() - old_page_converted.mean()
num_stations = session.query(func.count(hi_stations.STATION.distinct())).scalar()$ print("Number Of Stations: " + str(num_stations))
aaplA = aapl[['Adj Close']] $ pd.concat([msftAV, aaplA])
df_subset['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=False, logy=False)$ plt.show()
df_user = u.copy()
df.cdescr.head()
with open('image-predictions.tsv',mode='wb')as file:$     file.write(response.content)
print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
(null_vals > actual_diff).mean()
monte.str.lower()
words_hash_sk = [term for term in words_sk if term.startswith('#')]$ corpus_tweets_streamed_keyword.append(('hashtags', len(words_hash_sk))) # update corpus comparison$ print('List and total number of hashtags: ', len(words_hash_sk)) #, set(terms_hash_stream))
dates = pd.date_range('20130101', periods=6)$ df = pd.DataFrame(np.arange(24).reshape((6,4)),index=dates, columns=['A','B','C','D'])$ df.A.max
log_mod = sm.Logit(df_new['converted'], df_new[['UK', 'US']])$ results = log_mod.fit()$ results.summary()
old_page_converted = np.random.choice([0,1],size = n_new, p = [1-p_old,p_old])$ old_page_converted
df1.shape
pd.merge(transactions, transactions, on='UserID')
resreviewsyelp=[]$ for i in range(len(dataid)):$     resreviewsyelp.append(requests.get('https://api.yelp.com/v3/businesses/'+dataid[i]+'/reviews',headers=yelp_headers).json())
df_out = pd.merge(transactions, users, on='UserID',how='inner')$ df_out
kochdf.loc[kochdf['date'] == max_date]
aggregate_by_name = pd.concat(g for _, g in df_Providers.groupby("name") if len(g) > 1)$ aggregate_by_name.head()
pm_data.dropna(inplace=True)$ pm_data.status.value_counts()
data.loc[:, ['TMAX']].head()
df['Hour']=pd.DatetimeIndex(df['lpep_pickup_datetime']).hour
pd.merge(df1,df2, on='HPI')$
print(reg1.score(X_train, y_train))$ print(reg1.score(X_test, y_test))
p_mean = df2['converted'].mean() * 100$ output3 = round(p_mean, 2)$ print("The convert rate under the null hypothesis is: {}%".format(output3))
        select *$         from public.bookings b$         where b.CREATED_AT BETWEEN '2018-08-01' AND '2018-08-10' $
df_vow.plot()
np.shape(rhum_fine)
df_twitter_copy = df_twitter_copy[df_twitter_copy.retweet_count.notnull()]
plt.title('Number of Posts, Averaged Daily', fontsize = 18)$ sns.boxplot(data = daily_posts)$ sns.stripplot(data = daily_posts,jitter=True)
!wget https://pjreddie.com/media/files/yolov3.weights$
df_sb.isDuplicated.value_counts()$ df_sb.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
df.loc[df.category ==1022200, 'category_name'] = 'TEQUILA'
print ('Cantida de Columnas: ', len(dataset.columns))$ print ('Cantida de Filas: ', max(dataset.count()))
measure_df = pd.read_csv(measure, encoding="iso-8859-1", low_memory=False)$ measure_df.head()
vec1.get_feature_names() #feature names in test sample.$
df=pd.read_csv('ab_data.csv')$ df.head()
orig_ticker = pd.DataFrame(ticker) $ orig_ticker['Ticker_Symbol'] = pd.Series(ticker, index=data_ticker.index)
tips.sex = tips.sex.astype('category')$ tips.smoker = tips.smoker.astype('category')$ print(tips.info())$
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)$ Base = automap_base()$ Base.prepare(engine, reflect=True)
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])$ df_new.reset_index(inplace=True)$ df_new.head()
df.isnull().sum(), df.isnull().sum()/float(len(df))$
display('df', "df.groupby(df['key']).sum()")
df_dates = df.loc[(df.STATION=='59 ST'),["DATE"]]$ df_dates = pd.to_datetime(df_dates.DATE, infer_datetime_format=True)
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$ model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)
data.plot.scatter(x = 'Unnamed: 0', y = 'Age', figsize = (15, 10))
obs_old = df2.query('group == "control" and converted == 1').count()/df2[df2['group'] == "control"].count()$ obs_old[0]
numPurchU = train.groupby(by='User_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesU'})$ train = train.merge(numPurchU, on='User_ID', how='left')$ test = test.merge(numPurchU, on= 'User_ID', how='left')
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']['authorCollaborators']#.loc[97545]
p_new = df2[df2['landing_page'] == 'new_page']['converted'].mean()$ print("Probability of conversion for new page is {}".format(p_new))
donnees.index.month$ mask = df[i].index.month < 6$ df[i][mask].describe()
iris = pd.read_csv('data/iris.csv')$ print(iris.shape)
dfHaw_Discharge = getNWISData('02096960')$ dfHaw_Discharge.head()
festivals.rename_axis('')$ festivals.columns.names = ['Index']
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'US', 'UK']])$ results = logit_mod.fit()$ results.summary()
new_page = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$ print(len(new_page))
groupedNews = sentiments_df.groupby(["User"], as_index=False)$ groupedNews.head()
fit_result.summary2()
words_mention_sk = [term for term in words_sk if term.startswith('@')]$ corpus_tweets_streamed_keyword.append(('mentions', len(words_mention_sk))) # update corpus comparison$ print('List and total number of mentions: ', len(set(words_mention_sk))) #, set(terms_mention_stream))
for cell in openmc_cells:$     for rxn_type in xs_library[cell.id]:$         xs_library[cell.id][rxn_type].load_from_statepoint(sp)
months = months.dropna(subset = ['birth year'])$ print(months.shape)$ print(months.head(5))
engine=create_engine(seng)$ data['Actor Name'] = pd.read_sql_query('select UPPER(concat(first_name," ", last_name))  from actor', engine)$ data
autos['date_crawled'].str[:10].head()
engine.table_names()
print("\nThe types of weather events in the 'events' column are:")$ evnts = [str(x) for x in df_weather.events.unique()]$ print("".join([str(i+1) + ". " + evnts[i] + "\n" for i in range(len(evnts))]))
combined_df4['split_llpg1']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(str(x).split(',')[1:2]))$ combined_df4['split_llpg2']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(str(x).split(',')[1:3]))$ combined_df4.head()
df[y_col] = df['variety'].str.lower().replace(repl_dir)$ df_noblends = df[df[y_col].replace(repl_dir).str.lower().isin(keep_vars)]$ df_noblends[y_col].unique().size
print("Rows: {}".format(train.shape[0]))$ print("Columns: {}".format(train.shape[1]))
sentiment.to_csv("sentiment.csv", encoding="utf-8", index=False, header=True)
int_and_tel = cw_df.loc[cw_df['category'] == 'Internet & Telecom']$ int_and_tel.sort_index(axis= 0, inplace= True)$ int_and_tel.reset_index(inplace= True, drop= True)
fig, axes = plt.subplots(2, 2, figsize = (12, 6), sharex=True)$
model = GradientBoostingRegressor(verbose=True)$ model.fit(X_train, y_train)$ model.score(X_test, y_test) # This is the R^2 value of the prediction
df = df.replace('nan', '')
new_files = (df3['filename_old'] == '/dev/null')$ df3['groupby_filename'] = df3['filename_old']$ df3.loc[new_files, 'groupby_filename'] = df3[new_files]['filename_new']$
S_1dRichards.initial_cond.filename
goodTargetUserItemInt['weeknum_rank']=[(i+(52-44))%52 for i in goodTargetUserItemInt['weeknum']]$ print sorted(goodTargetUserItemInt['weeknum_rank'].unique())
points_dic={"India":345,"Bangladesh":456,"Pakistan":789,"China":90}$ points=pd.Series(points_dic)$ points$
df2.drop_duplicates(subset = "user_id",inplace = True)
1-0.190/2
test[['clean_text','user_id','predict']][test['user_id']==1895520105][test['predict']==10].shape[0]
df['converted'].mean()
autos["date_crawled"].value_counts(normalize=True, dropna=False)$
df_intermediate  = df_[selected_feature]$ df_norm = (df_intermediate - df_intermediate.mean()) / (df_intermediate.max() - df_intermediate.min())$ df_std = (df_intermediate - df_intermediate.mean()) / df_intermediate.std()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots$  $ stock_data["CLOSE"].iloc[:].plot() # Plot the adjusted closing price of AAPL
print(str(inspector.get_pk_constraint("stations")))$ print(str(inspector.get_pk_constraint("measurements")))
print("The tweet with more likes is: \n{}".format(data['Tweets'][fav]))$ print("Number of likes: {}".format(fav_max))$ print("{} characters.\n".format(data['len'][fav]))
df.info()
gene_df['gene_id'].unique().shape
billstargs.billtext = billstargs.billtext.replace({r'\n': '', r'<html><body><pre>': '', r'</pre></body></html>': '', r'_':'', r'&lt;all&gt;': '', r'&lt;DOC&gt;':'', r'  ': ' ', r'   ': ' ', r'    ':'', r'(\d)': '', r'[()]':'', r';':'', r'--':'', r',':'', r':':'', r'\.(?!\d)': ''}, regex=True)
ls_data = pd.read_excel(cwd+'\\LS_mail_0604_from_python.xlsx')$ ls_columns=ls_data.columns.values
url_template = "http://www.basketball-reference.com/teams/{team}/executives.html"
df_twitter_archive_master.tweet_id = df_twitter_archive_master.tweet_id.astype(str)
tweet_df_polarity = tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
df2.drop(2893, axis=0,inplace=True)
excel=pd.rea_excel("File Name")
df2['converted'].mean()
df_dont_line_up = df.query("(landing_page == 'new_page' and group == 'control') or (landing_page == 'old_page' and group == 'treatment')")$ print('The number of times the new_page and treatment don\'t line up is {}.'.format(df_dont_line_up.shape[0]))
sel1=[Measurement.date,$      func.sum(Measurement.prcp)]$ all_prcp=session.query(*sel1).group_by(Measurement.date).all()$
stock_data = db.stock_data$ stock_data_id = stock_data.insert_one(realtime_stock_data).inserted_id$ stock_data_id
data_df.info()
p_new = df2.converted.mean()$ p_new
sns.countplot(y="action",data=firstWeekUserMerged)$ plt.show()
tweet_df_clean = tweet_df.rename(columns={'id':'tweet_id'})
events[events['type'] == 'WatchEvent'].head()
IBMpandas_df = pd.read_csv("IBM.csv")$ IBMpandas_df.head()
knn = KNeighborsClassifier(n_neighbors=5)$ knn.fit(X_train, y_train)$ knn.score(X_test, y_test)
val_pred_svm = lin_svc_clf.predict(X_valid_cont_doc)
S_lumpedTopmodel.meta_basinvar.filename
stocks.loc[('Apple', '2017-12-29')]
results.summary()
tweet_df = pd.DataFrame(tweet_data)$ tweet_df.shape
from matplotlib import style$ style.use('fivethirtyeight')$ import matplotlib.pyplot as plt
no_name_list = df_twitter_copy[df_twitter_copy.name.str.contains('(^[a-z])')].name.tolist()
tweet_full_df.info()
df2.drop_duplicates('user_id',inplace=True);
table_rows = driver.find_elements_by_tag_name("tbody")[10].find_elements_by_tag_name("tr")$
sum(df2['user_id'].duplicated())
matched2014_rents[dummy_df.multiply(logit_params).sum(axis=1).apply(lambda x : np.divide(np.exp(x),1+np.exp(x)))>.9]
access_logs_parsed = access_logs_raw.map(parse_apache_log_line).filter(lambda x: x is not None)$
df2 = df.drop(df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].index)
df_l[['ship_name','date']].sort_values('date').tail()
inc_weath_off_df = pd.merge(inc_summ_df, weath_summ_off_df, on ="WkStart")$ inc_weath_off_df.head()
target_column = 'DGS30'$ col_list.remove(target_column)
mask = df2.user_id.duplicated()$ df2.user_id[mask]
save_n_load_df(ph, 'mergeable_holidays.pkl')$
random_series.cumsum().plot()
p_new = df2.converted.mean()$ p_new
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA','UK']])$ results = logit_mod.fit()$ results.summary()
cv_results = pd.DataFrame(rs.cv_results_)$ cv_results
hit_tracker_df.to_csv("Desktop/Project-2/hit_songs_only.csv", index=False, header=True)
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=JVTZ9kPgnsnq9oFbym2s&start_date=2018-05-17'$ r = requests.get(url)$ print(r.text)
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comments', ''))
response = requests.get(url)
df3 = pd.read_csv('countries.csv')$ df3.head()
port.pl.dly_frame.tail(5)
for col in missing_info:$     num_missing = data[data[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing))
bad_df = df.index.isin([5,12,23,56])$ df[~bad_df].head()
join_c.orderBy(F.desc("party_id_orig"),F.desc("aggregated_prediction")).select('party_id_orig','aggregated_prediction','predicted').show(500)
transactions.merge(users, how='inner', on=['UserID'])
ldamodel_Tesla= models.ldamodel.LdaModel(corpus_Tesla, num_topics=3, id2word = dictionary, passes=20)
xml_in_merged.tail(2)
df.info()$
workspace_ids = []$ for i in workspaces_list:$     workspace_ids.append(i['id'])$
last_12_precip_df = pd.DataFrame(data=last_12_precip)$ last_12_precip_by_date_df  = last_12_precip_df.set_index("date")$ last_12_precip_by_date_df.head(2500)
for post in posts.find({"reinsurer": "AIG"}):$     pprint.pprint(post)
dataM = pd.read_sql("SELECT Measurement.station, COUNT(Measurement.station) FROM Measurement GROUP BY Measurement.station ORDER BY COUNT(Measurement.station) DESC", conn)$ dataM
inspector = inspect(engine)$ inspector.get_table_names()
contract_history['NUM_CAMPAGNE'] = contract_history['NUM_CAMPAGNE'].map(lambda x: float(x) if x not in ['N', ''] else np.nan)
def calc_tmps(start_date, end_date):$     return session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$     filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all()
train_ind, test_ind, train_dep, test_dep = train_test_split(kick_projects_ip_scaled_ftrs, kick_projects_ip[response], test_size=0.3, random_state=0)
records3.loc[(records3['Graduated'] == 'Yes') & (records3['Age'].isnull()), 'Age'] = grad_age_mean$ records3.loc[(records3['Graduated'] == 'No') & (records3['Age'].isnull()), 'Age'] = non_grad_age_mean
df2['user_id'].duplicated().sum()
archive_clean.loc[archive_clean['tweet_id'].isin(remove_list),:]
plt.hist(p_diffs)$ plt.axvline(x=obs_diff_conversion, color = 'red'); 
new.to_csv('dates.csv')$ test.to_csv('count.csv')
df = pd.read_csv(r'C:\Users\lenovo\Downloads\mohitudacityproject\ab_data.csv')$ df.head()
r6s['score'].corr(r6s['num_comments'])
s_cal = pd.read_csv('seattle/calendar.csv')$ s_list = pd.read_csv('seattle/listings.csv')$ s_rev = pd.read_csv('seattle/reviews.csv')
model=LogisticRegression(penalty='l2', C=1)$ model.fit(X_train_mean, y_train_mean)$ preds = model.predict_proba(X_test_mean)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=YxSY4z-2vyxvJ15WjtFa&start_date=2017-01-01&end_date=2017-12-31"$ req = requests.get(url)
indices = df.query('group != "treatment" and landing_page == "new_page" or group == "treatment" and landing_page != "new_page"').index$ df2 = df.drop(indices)
data2 = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$
$sudo wget http://files.pushshift.io/reddit/comments/RC_2018-02.xz$
df_control = df.query("group == 'control'")$ df_control.converted.mean()
two_day_sample.head()
df_plot = df_recommended_menues.sort_values(by=['dates'])
S_lumpedTopmodel.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
pclass.value_counts().sort_index().plot(kind='bar')$ pclass.value_counts()
train.MARQUE_LIB.value_counts()
FREEVIEW.plot_histogram(raw_fix_count_df)
df2.groupby('group').mean()
url_test = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \$       "&start_date=2017-01-01&end_date=2017-01-02&api_key=" + API_KEY$ req_test = requests.get(url_test)
test_ind["Pred_state_LR"] = best_model_lr.predict(test_ind[features])$ train_ind["Pred_state_LR"] = best_model_lr.predict(train_ind[features])$ kick_projects_ip["Pred_state_LR"] = best_model_lr.predict(kick_projects_ip_scaled_ftrs)
df = pd.read_csv('/Users/Stav/Desktop/Iowa_Liquor_sales_sample_10pct.csv')$ print df.columns$ df.head()
g8_groups.agg(['mean', 'std'])
df = df.set_index('date')$ month_plot = df.resample('d', how=sum).plot()$ plt.show()
merged_data = pd.merge(companies_pf, funding_pf,how="inner",on="Company")$ print(len(merged_data.index))$
df.isnull().sum()
ex4.drop(ex4.index[-1])
metadata['epsg'] = int(refl['Metadata']['Coordinate_System']['EPSG Code'].value)$ metadata['epsg']
input_col = ['msno','plan_list_price','actual_amount_paid','payment_plan_days']$ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions',input_col)$
dupes_to_delete = dfd.duplicated(subset=['brand', 'outdoor_model', 'indoor_model'])$ dupes_to_delete.value_counts()
df.iloc[(len(df)-lookforward_window)-2:(len(df)-lookforward_window),:]
jail_census.loc['2017-02-01'].groupby('Gender')['Age at Booking'].describe()
transfer_duplicates.apply(lambda row: smoother_function_part2(row["Year"], row["Month"], row["Day"], row["Smoother"]), axis=1);
results = nfl.interest_over_time()$
iso_gdf_2.plot();
inspector = inspect(engine)$ inspector.get_table_names()
station_distance.insert(loc=11, column='Distance(Miles)', value=distance)
df_parsed = pd.read_json(df.to_json(orient="records"))
df_train.head()
noise_data = data[data["Complaint Type"].str.contains("noise")]$ noise_data.shape
data_by_date_df["date"] = data_by_date_df["date"].dt.strftime("%Y-%m")
np.abs(df2['Change'])
weather_df.to_csv("Weather data.csv", encoding = "utf-8-sig", index = False)
df[df['AgeNormed'] < 0.01] # as for the other end of hte spectrum, it does look like when $
tweets_clean.drop(columns = ['pupper', 'doggo', 'puppo', 'source'], inplace=True)$ tweets_clean.info()
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])  $ results = lm.fit()
plt.hist(p_diffs);
treatment_notAligned = ((df['group'] == 'treatment') & (df['landing_page'] != 'new_page'))$ treatment_notAligned.sum()
df[df['converted'] == 1].groupby('user_id').nunique().shape[0] / df.nunique()['user_id']
b[b.T.sum()==c].index.min()
data.describe()
import os, os.path$ def filecount(dir_name):$     return len([f for f in os.listdir(dir_name) if os.path.isfile(dir_name +f)])
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$ filter(Measurement.station == 'USC00519281').all()$
authorization_instance = tweepy.OAuthHandler(consumer_key,consumer_secret)$ authorization_instance.set_access_token(access_token, access_token_secret)$ authorization_instance.secure = True
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Therapists Hours Partial Autocorrelation')
df['intercept']=1$ df[['control', 'treatment']] = pd.get_dummies(df['group'])$
df2.drop_duplicates('user_id',inplace=True)$ df2.info()
chefdf = chefdf.dropna()
(p_diffs > actual_converted_diff).mean()$
turnstiles = df.groupby(['STATION','C/A','UNIT','SCP'])$ print('There are {} unique turnstiles.'.format(len(turnstiles)))$
state_party_df['National_D']['2016-08-01':'2016-08-07'].sum() / 7
plt.figure(figsize=(15,5))$ sns.barplot(data=aa,x='weekofyear',y='message')
tlen.plot(figsize=(16,4), color='r')
sorted(twitter_df.rating_denominator.unique())
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv"$ df  = pd.read_csv(path, sep =',') # df  = pd.read_csv(path) may be too. sep =',' is by deffect.$ df.head(5)
prcp_analysis_df["date"] = pd.to_datetime(prcp_analysis_df["date"],format="%Y-%m-%d", errors="coerce")
url = "https://www.reddit.com/hot.json"
pd.merge(msftAR0_5, msftVR2_4, how='outer')
url2 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2017-01-02&end_date=2017-12-31&api_key=mVyTTi52QUPLHnmV-tx_'$ r2 = requests.get(url2)$ jd = r2.json()$
dr_test_data = dr_test_data.resample('W-MON').sum()$ RN_PA_test_data = RN_PA_test_data.resample('W-MON').sum()$ therapist_test_data = therapist_test_data.resample('W-MON').sum()
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(["doggo", "floofer", "pupper", "puppo"], axis = 1)$ twitter_archive_enhanced_clean.head()
now = datetime.now()$ print(now)
red_4['created_utc'] = red_4['created_utc'].astype('datetime64[s]')$ red_4['time fetched'] = red_4['time fetched'].astype('datetime64[s]')$ red_4.head()
data_archie = data_archie[data_archie['user_id'].notnull()]$
df_t['day'] = df_t['timestamp'].dt.day$ df_t['day'] = df_t['day']-1
cityID = '0e2242eb8691df96'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Henderson.append(tweet) 
RandomTwoDF = RowedRDD.toDF()$ RandomTwoDF.registerTempTable("RandomTwo")$ print type(RandomTwoDF)
df.info()
! head -n 5 ../../data/msft.csv # OS/Linux$
df = pd.read_csv(CSV_FILE_PATH,delim_whitespace=True)
X.drop('title', axis=1, inplace=True)$ X = pd.get_dummies(X, drop_first=True)
bus['zip_code'] = bus['postal_code'].str[:5]$ bus['zip_code'].value_counts(dropna=False)
df2[df2.duplicated(['user_id'], keep=False)]
liquor["Margin"] = (liquor["State Bottle Retail"] - liquor["State Bottle Cost"]) * liquor["Bottles Sold"]$ liquor["Price per Liter"] = liquor["Sale (Dollars)"] / liquor["Volume Sold (Liters)"]$ liquor.head()
list(set(df['City'][df.CustomerID=='0000119007']))   
plot = events_top10_df.boxplot(column=['yes_rsvp_count']\$                            , by='topic_name', showfliers = False, showmeans = False, figsize =(17,8.2)\$                            ,whis=[20, 80])
print('The Jupyter notebook stores information in the "Kernel".\$       \nRestart the Kernel to clear noteook memory.')
tcat_df = tcat_df.append(tcat)$ tdog_df = tdog_df.append(tdog)
df['signup_time']=pd.to_datetime(df['signup_time'])$ df['purchase_time']=pd.to_datetime(df['purchase_time'])$ df['dtime'] = (df['purchase_time']-df['signup_time']).astype('timedelta64[h]')$
oldest_date = pd.to_datetime(pd.Series([t["created_at"] for t in trump_tweets])).min()$ oldest_date
jsondata = df3.to_json(orient='records')$ with open("_df_subset_v5_wseconds.json", "w") as outfile:$     json.dump(jsondata, outfile)
betas_argmax = np.zeros(shape=mcmc_iters)$ betas_argmax = beta_dist.argmax(1)
import tensorflow as tf$ from tensorflow.contrib.tensorboard.plugins import projector
transactions1 = transaction_s.drop(['UserID'], axis=1) # drop on columns$ pair_Of_Transdf = pd.merge(transaction_s, transaction_s, how='left',   on=['UserID']) # left join orginal with new$ print(pair_Of_Transdf.sort_values(by=['UserID'],ascending=[False]).head(10))
unique_users_num = df.user_id.nunique()$ print('Unique users: {}'.format(unique_users_num))
data_df.describe()
all_sets.cards = all_sets.cards.apply(lambda x: pd.read_json(json.dumps(x), orient = "records"))
np.exp(result4.params)
cnn_g , cnn_op= cnn_graph()$ runtime(name = "2", op_list = cnn_op, datalist = [ctrain_x ,ctrain_y, cvalid_x, cvalid_y], g = cnn_g  )$ cnn_g.get_operations()
df.cumsum().plot(figsize=(14, 6))
len(df_enhanced.query('retweeted_status_id != "NaN"'))
df = pd.read_sql('SELECT * from booking', con=conn_b)$ df
[(k, aff1y['variables'][k]['label'])  for k in affkeys if k.startswith ("B28002") and $  'Broadband' in aff1y['variables'][k]['label']]
type(t1.tweet_id.iloc[3])
from sklearn.linear_model import LogisticRegressionCV$ lr = LogisticRegressionCV(Cs=5, n_jobs=-1, random_state=42)
frame.loc[('b', 2), 'Colorado']
ad_data=appended.union(ad_data3)
from datetime import datetime$ df3['DAY'] = df3['DATE'].apply(lambda x: x.weekday())$ df3['DATE'] = df3['DATE'].apply(pd.datetools.normalize_date)
df2.user_id[df2.user_id.duplicated(keep=False)]
from pandas.tools.plotting import lag_plot$ dataSeries = pd.Series(Q3['Average Temperature'])
mask = (x > 0.5) & (y < 0.5)
kick_projects.groupby(['main_category','state']).size()$
print cust_data.drop_duplicates(keep='last').head(3)$ print cust_data.drop_duplicates(keep=False).head(3)
autos = autos[autos.registration_year.between(1920,2016)]$ autos.shape
price=data.json()
year_with_most_commits = commits_per_year['commits'].idxmax().year
twitter_archive_full.to_csv(data_folder+'twitter_archive_master.csv', index=False)
 print(r.text)
cityID = 'fef01a8cb0eacb64'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Akron.append(tweet)   
df2['intercept'] = 1$ df2['ab_page'] = 0$ df2.loc[(df2["group"] == "treatment"), "ab_page" ] = 1
breed_predict_df_clean = breed_predict_df_clean[breed_predict_df.p1_dog == True]
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))$
cityID = '1661ada9b2b18024'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Wichita.append(tweet) 
df = pd.read_csv('ab_data.csv') $ df.head()
pres_df['subjects'].isnull().sum()
image_predictions_df.tail(3)
params = {}$ params['penalty'] = ['l1','l2']$ params['C'] = [.001,.01,.1,.5,1,5,10]#[.03,.05,.1]
df['duration'] = np.round((df['deadline'] - df['launched']).dt.days / 7)
df2['converted'].mean()
df_new[(df_new['group'] == 'treatment')]['converted'].mean()
!hdfs dfs -mkdir hw3$ os.getcwd() 
tdf['label'] = (tdf['smoker'] == 'Yes').astype(float)$ tdf.sample(5)
mean_price = pd.Series(top_brand)$ top20 = pd.DataFrame(mean_price, columns = ['mean_price'])
df_25year=df[df['date']>'1991-02-24'] #last 25 years$ df_25year.groupby('origin')['description'].agg('count').sort_values(ascending=False).head(1)
help(h2o.estimators.glm.H2OGeneralizedLinearEstimator)$ help(h2o.estimators.gbm.H2OGradientBoostingEstimator)
fh_3 = FeatureHasher(num_features=uniques.iloc[0, 1], input_type='string', non_negative=True)$ %time fit3 = fh_3.fit_transform(train.device_ip)
b = news_df[news_df['Source Acc.'] == 'nytimes']$ b.head()$ print(b['Compound Score'].sum())
print example1_df.printSchema()
calls_df.groupby(["dial_type"])["length_in_sec"].mean()
stops_heatmap = folium.Map(location=[39.0836, -77.1483], zoom_start=11)$ stops_heatmap.add_child(heatmap_full)$
feats_dict = dict()$ for i, name in enumerate(dummies_df.keys()):$     feats_dict[i] = name $
list(filter(lambda num:num%2==0,seq))    # seq = [1,2,3,4,5]  you can see above just below section 8.2
df1.index.values
climate_df.describe()$
combined = pd.merge(hpg, air, on = "air_store_id", how='outer', suffixes = ["_hpg","_air"])$ hpg[hpg["air_store_id"].notnull() == True]$
ohlc_BRML3['Date']=ohlc_BRML3['Date'].map(mdates.date2num)
yhat = kNN_model.predict(X_test)$ yhat[0:5]
display(data.head(10))
pd.MultiIndex.from_arrays([['a','a','b','b'], [1,2,1,2]])
(p_diffs > obs_diff).mean()
age_new = pd.cut(age, [0,10,20,30,40,50,60,70,80]) #discrete decade intervals$ age_new.value_counts().sort_index().plot(kind='bar')$ age_new.value_counts()
hs.setAccessRules(C_resource_id, public=True)
data_df.clean_desc[15]
number_one_df = spotify_df.loc[spotify_df["Position"] == 1,:]
df2[df2.duplicated('user_id', keep=False)]
quandl.ApiConfig.api_key = 'gTmSNjbQR-8Q5U9pukHX'$ quandl.get('BITFINEX/BTCEUR',collapse="monthly")$
ayush = relevant_data[relevant_data['User Name'] == 'AYUSH JAIN']$ ayush['Event Type Name'].value_counts().plot(kind='barh')
rain_df = pd.DataFrame(rain)$ rain_df.head()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)
end_date = (datetime.strptime(most_recent_tobs , '%Y-%m-%d') - timedelta(weeks=48)).\$     strftime('%Y-%m-%d') #converts datetime to string$ end_date
post_number = len( niners[niners['Jimmy'] == 'yes']['GameID'].unique() )$ print post_number
df = titanic3[['cabin', 'pclass']].dropna()$ df['deck'] = df.apply(lambda row: ord(row.cabin[0]) -64, axis=1)$ sns.regplot(x=df["pclass"], y=df["deck"])
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ print(last_date)
sorted_posts = gram_collection.find({"contains_tattoo": 1}).sort([("likes", pymongo.DESCENDING)])
df = pd.read_csv('ab_data.csv')$ df.head()
columns = inspector.get_columns('station')$ for column in columns:$     print(column["name"], column["type"])
pyLDAvis.enable_notebook()$ vis = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)$
news_df = pd.DataFrame(news_dict) 
campaigns_under_50 = marketing_sources_active_users[marketing_sources_active_users['user_id']<50].index.tolist()$ df_first_days = df_first_days[~df_first_days['marketing_source'].isin(campaigns_under_50 )]
places = extractor.geo_search(query="Philippines", granularity="country")$ place_id = places[0].id$ print("Philippines id is:", place_id)
df2_copy = df2.copy()
data.loc[:, 'TMAX'].head()
final_train_users = train_nonUS_purchase.copy()$ final_train_users = final_train_users[final_train_users['id'].isin(sessions['user_id'])]$ final_train_users.head()$
from sklearn.linear_model import RidgeCV$ rr = RidgeCV(alphas=alphas, fit_intercept=True, normalize=False)$ score_model(rr, alpha=True)
a.
forcast_out=int(math.ceil(.01*len(df1)))$ forcast_out$
tweets_clean.to_csv('tweets_clean.csv', index=False)
time_length = pd.Series(df.len.values, index=df.Date)$ time_favorite = pd.Series(df.Likes.values, index=df.Date)$ time_retweet = pd.Series(df.Retweets.values, index=df.Date)
appendedFrames = noHandReliableData.append(extraRecordsForLag);$ appendedFrames.shape
y = x.loc[:,"A"]$ y.mean()$ np.mean(y)
events.groupBy("event_type").count().toPandas().head(50).plot(kind="bar")
pivoted.T[labels == 1].T.plot(legend=False, alpha=0.1)
df.converted.mean()
samp = pd.DataFrame(samp, columns=['tweet_id','favorite_count','retweet_count'])$ samp.to_csv('tweet_json.txt')
Aussie_result=pd.concat([sydney_aussie,brisbane_aussie,melbourne_aussie])$
shape_file = ('/g/data/r78/vmn547/GWandDEA_bex_ness/Little_GW_AOI_for_demo/kEEP_ord/KEEP_AOI.shp')
ab_df2.loc[ab_df2.user_id.duplicated(),:]
perf_train.dtypes.value_counts()
most_active_df = most_active_df.loc[most_active_df['date'] > year_ago]$ most_active_df.plot.bar
df.groupby(['COUNTRY','PRODUCT','PRIMARY VIOLATION']).size().nlargest(5)
df_json_tweets['tweet_id'] = df_json_tweets['tweet_id'].astype(str)$ df_json_tweets.info()
newdf.loc[newdf['score'] == 0.187218571]
p_diffs= np.random.binomial(n_new, p_new, 10000)/n_new\$         -np.random.binomial(n_old, p_old, 10000)/n_old
spacy_url = 'https://spacy.io/assets/img/pipeline.svg'$ iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url)$ HTML(iframe)
df2.groupby(df2['landing_page']=='old_page').size().reset_index()[0].iloc[1]/df2['landing_page'].count()
yc_depart = yc_merged_drop.merge(departureZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')$ yc_depart.shape
save_model('model_logistic_reg_v1.mod', lgr_grid)
test[['clean_text','user_id','predict']][test['user_id']==1895520105].shape[0]
df.to_csv('/Users/digitalmarketer1977/Desktop/620trumptweets.csv',index = False,encoding='utf-8')
ppt = session.query(Measurement.date,Measurement.prcp).filter(Measurement.date.between ('2016-08-23','2017-08-23')).order_by(Measurement.date).all()$ ppt$
with open(example_text_file, mode = 'w', encoding='utf-8') as f:$     f.write(stringToWrite)
max_price = autos["price"] == autos["price"].max()
Top_tweets.groupby('sentiment').count()
logit_multi_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_test','UK','US']])$ results = logit_multi_mod.fit()$ results.summary()
for v in d.variables:$     print(v)
testing = pd.read_csv('SHARE-UCSD-export_reformatted.csv')
df_tte['ItemDescription'].unique()
df_sched.iloc[:,1:] = df_sched.iloc[:,1:].apply(lambda x: x.str[:10])
train, test = pd.read_csv('train.csv'), pd.read_csv('test.csv')
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[1]').send_keys('Avengers: Infinity War')$ driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[2]').click()
print("Mean squared error: %.2f"$       % np.mean((regressor.predict(test_features) - test_occupancy) ** 2))
n_old = df2.query('landing_page == "old_page"').user_id.count()$ n_old
hm_sub = grid_heatmap[grid_heatmap['pr_fire'] >= 0.25]$ fire_hm = hm_sub[['glat', 'glon', 'pr_fire']].values
monthly_portfolio_average = round(np.sum(MonthlyReturns * stock_weights),2)$ monthly_portfolio_average
t = thisyear$ len(t[(t.creation <= '2015-01-01') & (t.creation > '2014-01-01')])
sox.loc[sox.hour >= 18,'late'] = 1
iowa = iowa.dropna(axis=0, how='any')$ iowa["Year"], iowa["Month"] = iowa["Date"].dt.year, iowa["Date"].dt.month
df2 = df.copy()$ df2.iloc[3] = 0$ df2
listings = pd.read_csv('data/listings.csv')$ calendar = pd.read_csv('data/calendar.csv') #NOTE I changed the British for the 'Merican pronunciation$ reviews = pd.read_csv('data/reviews.csv')
pres_date_df = pres_df.copy()$ pres_date_df.head(2)
people.dtypes
precipitation = measure_avg_prcp_year_df['Avg Prcp'].describe()$ pd.DataFrame(precipitation, columns=['Avg Prcp'])
import pandas as pd$ pd.core.common.is_list_like = pd.api.types.is_list_like$ import pandas_datareader.data as web
model.most_similar('hand',  topn=100)$
my_url = "http://www.ipaidabribe.com/reports/all#gsc.tab=0"
df = web.get_data_yahoo("SPY",start ="2000-01-01",end = "20-03-2018")
pickle_in = open("neuralNet.pickle","rb")$ ANNModel = pickle.load(pickle_in)
mmx = MinMaxScaler()$ %time train_4_reduced = mmx.fit_transform(train_4_reduced)
df.rename(columns={'Created Date': 'created_at'})
containers[0].find("span",{"class":"date"}).contents[0]
en_translation_counts = en_es.groupby(by='en').size()$ en_translation_counts[en_translation_counts > 1].hist(bins=10)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-06-27&end_date=2018-06-27&apikey='+API_KEY)
good_stores.shape
df = pd.read_csv('ab_data.csv')$ df.head()
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']])$ results = logit_mod.fit()
myDataFrame = df[['Country','Indicator_ID','PUBLISH STATES', 'Year', 'WHO region', 'World Bank income group', 'Sex', 'Display Value', 'Numeric',$  'Low', 'High', 'Comments']]$ myDataFrame.head(2)
df.info()
mismatch_all = pd.concat([no_mismatch_newpage, no_mismatch_oldpage])
df2['landing_page'].value_counts()[0] / len(df2)
bbc = news_sentiment('@BBCNews')$ bbc['Date'] = pd.to_datetime(bbc['Date'])$ bbc.head()$
multi.handle.value_counts()/multi.shape[0]
red.shape
client = MongoClient()$ db = client.test_database #acessa ou cria o banco$
results['HydrologicEvent'].unique()
y_pred = pipe_lr.predict(pulledTweets_df.emoji_enc_text)$ y_proba = pipe_lr.predict_proba(pulledTweets_df.emoji_enc_text)$ pulledTweets_df['sentiment_predicted_lr']=[classes[y_pred[i]] for i in range(len(y_pred))]
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY\$ +'&start_date=2018-05-18&end_date=2018-05-18'$ r = requests.get(url)
i = issues$ len(i[(i.activity <= '2014-01-01') & (i.activity > '2013-01-01')])
from pyspark.sql import SQLContext$ sqlContext = SQLContext(sc)
df_2015['bank_name'] = df_2015.bank_name.str.split(",").str[0]$
df['AQI Category'] = df['AQI Category'].astype('category')$ df['AQI Category'].cat.categories
firstWeekUserMerged[firstWeekUserMerged.objecttype.isnull()].head(5)
my_gempro.blast_seqs_to_pdb(all_genes=True, seq_ident_cutoff=.9, evalue=0.00001)$ my_gempro.df_pdb_blast.head(2)
df2_rows_removed = df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]$ print('Now, the number of rows where the landing_page and group columns don\'t align is {}.'.format(df2_rows_removed))
sdf.createOrReplaceTempView("tempTable")$ res.show()
country_result.summary()
taxiData.columns
data=pd.merge(file2, loc, left_on='pole_id', right_on='pole', how='left')$ data.columns
len(archive_clean[archive_clean.name.isnull()]) == len(faulty_names)
top_10_authors = git_log.groupby("author").count().sort_values(by="timestamp", ascending=False).head(10)$ top_10_authors
df_test['due_date']=pd.to_datetime(df_test['due_date'])$ df_test['effective_date']=pd.to_datetime(df_test['effective_date'])$ df_test.head()
df['user_id'].nunique()
country_result.summary()
SandP = ['MMM', 'ABT', 'ABBV', 'ACN', 'ATVI', 'AYI', 'ADBE', 'AAP', 'AES', 'AET', 'AMG', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ALXN', 'ALLE', 'AGN', 'ADS', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'APC', 'ADI', 'ANTM', 'AON', 'APA', 'AIV', 'AAPL', 'AMAT', 'ADM', 'ARNC', 'AJG', 'AIZ', 'T', 'ADSK', 'ADP', 'AN', 'AZO', 'AVB', 'AVY', 'BHI', 'BLL', 'BAC', 'BCR', 'BAX', 'BBT', 'BDX', 'BBBY', 'BRK.B', 'BBY', 'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'AVGO', 'BF.B', 'CHRW', 'CA', 'COG', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CAT', 'CBOE', 'CBG', 'CBS', 'CELG', 'CNC', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHTR', 'CHK', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CME', 'CMS', 'COH', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'CXO', 'COP', 'ED', 'STZ', 'GLW', 'COST', 'COTY', 'CCI', 'CSRA', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DLPH', 'DAL', 'XRAY', 'DVN', 'DLR', 'DFS', 'DISCA', 'DISCK', 'DG', 'DLTR', 'D', 'DOV', 'DOW', 'DPS', 'DTE', 'DD', 'DUK', 'DNB', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EVHC', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'EXC', 'EXPE', 'EXPD', 'ESRX', 'EXR', 'XOM', 'FFIV', 'FB', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC', 'FTI', 'FL', 'F', 'FTV', 'FBHS', 'BEN', 'FCX', 'FTR', 'GPS', 'GRMN', 'GD', 'GE', 'GGP', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GS', 'GT', 'GWW', 'HAL', 'HBI', 'HOG', 'HAR', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HP', 'HSIC', 'HES', 'HPE', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HPQ', 'HUM', 'HBAN', 'IDXX', 'ITW', 'ILMN', 'INCY', 'IR', 'INTC', 'ICE', 'IBM', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IRM', 'JBHT', 'JEC', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KHC', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LEG', 'LEN', 'LUK', 'LVLT', 'LLY', 'LNC', 'LLTC', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MNK', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MJN', 'MDT', 'MRK', 'MET', 'MTD', 'KORS', 'MCHP', 'MU', 'MSFT', 'MAA', 'MHK', 'TAP', 'MDLZ', 'MON', 'MNST', 'MCO', 'MS', 'MSI', 'MUR', 'MYL', 'NDAQ', 'NOV', 'NAVI', 'NTAP', 'NFLX', 'NWL', 'NFX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'PCAR', 'PH', 'PDCO', 'PAYX', 'PYPL', 'PNR', 'PBCT', 'PEP', 'PKI', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'RL', 'PPG', 'PPL', 'PX', 'PCLN', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'QCOM', 'PWR', 'DGX', 'RRC', 'RTN', 'O', 'RHT', 'REG', 'REGN', 'RF', 'RSG', 'RAI', 'RHI', 'ROK', 'COL', 'ROP', 'ROST', 'RCL', 'R', 'SPGI', 'CRM', 'SCG', 'SLB', 'SNI', 'STX', 'SEE', 'SRE', 'SHW', 'SIG', 'SPG', 'SWKS', 'SLG', 'SNA', 'SO', 'LUV', 'SWN', 'SWK', 'SPLS', 'SBUX', 'STT', 'SRCL', 'SYK', 'STI', 'SYMC', 'SYF', 'SYY', 'TROW', 'TGT', 'TEL', 'TGNA', 'TDC', 'TSO', 'TXN', 'TXT', 'BK', 'CLX', 'COO', 'HSY', 'MOS', 'TRV', 'DIS', 'TMO', 'TIF', 'TWX', 'TJX', 'TMK', 'TSS', 'TSCO', 'TDG', 'RIG', 'TRIP', 'FOXA', 'FOX', 'TSN', 'USB', 'UDR', 'ULTA', 'UA', 'UAA', 'UNP', 'UAL', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'URBN', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'WM', 'WAT', 'WEC', 'WFC', 'HCN', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WFM', 'WMB', 'WLTW', 'WYN', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XL', 'XYL', 'YHOO', 'YUM', 'ZBH', 'ZION', 'ZTS'] $ for company in SandP:$     qb.AddEquity(company)
print 'df_average : ', df_average.shape$ print 'df_categories : ', df_categories.shape$ print 'df_categories_svd : ', df_categories_svd.shape
df_example4 = df_example3.join(schemaExample, schemaExample["ID"] == df_example3["id"] )$ for row in df_example4.take(5):$     print row
drop_num.append(35)$
file_names = []$ file_names = glob.glob('../craigslist-data/final-data-multiple-cities/*.csv')
df = $ df.head()
autos["price"] = autos["price"].str.replace("$", "").str.replace(",", "").astype(int)$ autos.rename({"price" : "price_euro"}, axis = 1, inplace = True)
twelve_months_prcp.head()
year_prcp = session.query(Measurements.date, Measurements.prcp).order_by(Measurements.date).filter(Measurements.date > year_ago)$
rfc.fit(features_class_norm, overdue_transf)
number_of_commits = len(git_log)$ number_of_authors = len(git_log.dropna().author.unique())$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df.rename(columns={'PUBLISH STATES':'Publication status','WHO region':'WHO Region'}, inplace= True )$ df.head()
new_discover_sale_transaction = post_discover_sales[post_discover_sales['Email'].isin(new_customers_test['Post Launch Emails'].unique())]$ new_discover_sale_transaction['Total'].mean()
Today = pd.to_datetime('now')$ YrFromToday = Today - dt.timedelta(days=365)$ print(YrFromToday)
donors.loc[donors['Donor Zip'] == 606 , 'Donor City'].value_counts()
df.donation_date = df.donation_date.apply(pd.to_datetime)$ df.charitable = df.charitable.apply(bool)$ df.amount = df.amount.apply(int)
o_data = OrderedDict(sorted(data.items(), key=lambda t:t[0]))
artistDF[artistDF.artistID==1000010].show()$ artistDF[artistDF.artistID==2082323].show()
flight.describe("from_city_name").show() 
html = requests.get(nasa_url)$ soup = bs(html.text, 'html.parser')
test_ind["Pred_state_RF"] = trained_model_RF.predict(test_ind[features])$ train_ind["Pred_state_RF"] = trained_model_RF.predict(train_ind[features])$ kick_projects_ip["Pred_state_RF"] = trained_model_RF.predict(kick_projects_ip_scaled_ftrs)
xml_in_merged = pd.merge(xml_in, grouped_df, on=['authorId', 'authorName'], how='left')
df3 = pd.DataFrame(df3_list, columns = ['tweet_id', 'retweet_count', 'favorite_count'])$ df3 = df3.sort_values('retweet_count').reset_index(drop=True)
test.iloc[40:100]
import os # To use command line like instructions$ if not os.path.exists(DirSaveOutput): os.makedirs(DirSaveOutput)
ground_alt = merged_data['rtk_alt'].mode()[0]$ ground_alt
X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(df_ml_features_scaled, \$                                                     df_ml_target, test_size = 0.2, random_state = 10,\$                                                     stratify = df_ml_target)
zone_train = df_run[df_run['zone'].notnull()]$ zone_test = df_run[df_run['zone'].isnull()]$ zone_train.head()
df['new_column'] = df['column_1'] + df['column_2']$ df['new_column'] = df.apply(my_function, axis = 1)$ df.to_csv['my_data.csv']$
relevant_data['User Name'].value_counts().plot(kind='barh')
prcp['date']=pd.to_datetime(prcp['date'])$ type(prcp.loc[0,'date'])
df_twitter_enhanced = pd.read_csv('twitter-archive-enhanced.csv')$ df_twitter_enhanced.head(3)
n_new = df2.query('group == "treatment"').shape[0]$ print(n_new) 
def getHour(x):$     return x.split()[1].split(':')[0]$
from scipy.stats import norm$ print(norm.ppf(1-0.05))
print("{0} comments with links".format(len(commenters)))$ print("{0} unique commenters".format(len(set(commenters))))$ print("{0} of these comments were made by the post author".format(author_links))$
lb = articles['tokens'].map(len).quantile(.025)$ ub = articles['tokens'].map(len).quantile(.975)$ articles = articles.loc[(articles['tokens'].map(len)>lb) & (articles['tokens'].map(len)<ub),:]
df.rename(columns={'Indicator':'Indicator_ID'},inplace=True)$ df.head(2)
all_tables_df.loc[:, 'OBJECT_NAME']
active_ordered = ordered_df.loc[~churned_ord]$
df_new.rename(columns={"treatment":"ab_page"}, inplace=True)$ df_new.head(1)
lg = pd.read_csv('awesome_go_web.txt.csv')$ lg.head()
df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']]$ print(df.head())
print(airquality_melt.head())
!open resources/html_table_marsfacts.html
with open('united_total_list_rake.pickle', 'wb') as f:$     pickle.dump(united_total_list_rake, f, pickle.HIGHEST_PROTOCOL)
import pandas as pd$ dff = pd.DataFrame(g.featureMatrix)$ dff.to_csv("foo.csv")
df_20180113_filtered.to_csv('data/hawaii_missile_crisis-20180113.csv')
df.query('group != "treatment" & landing_page == "new_page"').count()$
fire=load_data('https://data.sfgov.org/resource/wbb6-uh78.json')$ fire.dtypes$
filename_output = f'en-wikipedia_traffic_{PAGECOUNTS_START[:6]}-{PAGEVIEWS_END[:6]}.csv'$ filename_output
df_dates_final.shape[0] - df_dates_new.shape[0]$
result2.summary()
obs_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ obs_diff
t = thisyear$ len(t[(t.creation <= '2014-01-01') & (t.creation > '2013-01-01')])
lbh.get_trainer_wins(attr_result='x8is_win')$ lbh.get_jockey_wins(attr_result = 'x8is_win')$ lbh.get_trainerjockey_wins(attr_result='x8is_win')
artistDF[locate("Aerosmith", "name") > 0].show(20)
min_train, max_train = pd_train_filtered['unit_sales'].min(), pd_train_filtered['unit_sales'].max()
collection.delete_snapshot('snapshot_name')$
print('min wavelength:', np.amin(wavelengths),'nm')$ print('max wavelength:', np.amax(wavelengths),'nm')
sp = openmc.StatePoint('statepoint.50.h5')
cm5 = ConfusionMatrix(y5_test, predicted5)$ cm5.print_stats()
url = "https://mars.nasa.gov/api/v1/news_items/?order=publish_date+desc"$ response = requests.get(url).json()$ body = response.get('items')[0]$
df_date_precipitation=pd.DataFrame(date_precipitation, columns=['date','precipitation'])$ df_date_precipitation.set_index('date', inplace=True)$ df_date_precipitation.head()
df_new.country.unique()
fires.columns = map(str.lower, fires.columns)$ fires.columns = map(lambda x: x.replace(' ', r'_'), fires.columns)$ fires.head(2).T
df2.query('group=="treatment"')['converted'].mean()
images_copy = images_copy[pd.notnull(images_copy['jpg_url'])] #without pics$ images_copy.info()$
df.iloc[0]
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
staging_bucket = 'gs://' + google.datalab.Context.default().project_id + '-dtlb-staging-resolution'$ !gsutil mb -c regional -l {storage_region} {staging_bucket}
df_concat["date_series"] = pd.to_datetime(df_concat["created_time"])$ df_concat["date_series"].head()
session.query(Measurement.station, func.count(Measurement.station).label('count')).\$                                   group_by(Measurement.station).\$                                   order_by('count DESC').all()
index = pd.date_range('2018-3-1', periods=1000, freq='B')$ index
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new,(1-p_new)])$ new_page_converted.mean()
df['director'] = df['director'].fillna('Unknown')$ df['production_companies'] = df['production_companies'].fillna('Unknown')$ df['genres'] = df['genres'].fillna('Unknown')
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$
df.status.value_counts()
killfile = [i[0] for i in [i for i in list(pgn2value.items()) if i[1] == 1]]$ np.random.shuffle(killfile)$ killfile = killfile[:20572 - 15286 ]$
result.route
df = pd.concat([sanders_df, dtrump_df, jimcramer_df], ignore_index=True) # ignore index = True it will reset all the$
support.amount.sum() / merged.amount.sum()
session.query(Measurement.station, func.count(Measurement.station)).\$     group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
df_2007['bank_name'] = df_2007.bank_name.str.split(",").str[0]$
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
results_df.plot(label='prcp', xticks=[]) $
df.shape[0]
from sklearn import preprocessing$ le = preprocessing.LabelEncoder()$ le.fit(list(set(df[:1000]['brand'])))$
lm = sm.OLS(df_new["converted"],df_new[["intercept","control","US","UK"]])$ results = lm.fit()$ results.summary()
dump["country"].value_counts() / dump["country"].value_counts().sum() $
cityID = '84229b03659050aa'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Virginia_Beach.append(tweet) 
import pandas as pd$ df = pd.read_csv(datafile)$ df.head(5)
tmp = combined[['city','state','lat','long']].drop_duplicates()
df_input_clean.filter("`Resp_time` <= 0").groupBy("Resp_time").count().sort(desc("count")).show(50)
free_data.mean(),free_data.count()
df['Month-Date'] = df['Date'].apply(lambda x: x[5:])$ df.head()
import matplotlib.pyplot as plt$ %matplotlib inline$ aapl[['Close', 'Adj Close']].plot(figsize=(8,6));
resp_dict = r.json()$ type(resp_dict)
S_distributedTopmodel.decision_obj.bcLowrSoiH.options, S_distributedTopmodel.decision_obj.bcLowrSoiH.value
print('{}index/agencies/{}'.format(base_url, '1')) $ print(json.loads(requests.get('{}index/agencies/{}'.format(base_url, '1')).text))
obj.rank(ascending=False, method='max')
auth = tweepy.OAuthHandler(consumer_key, consumer_secret, callback_url)
df_vow.describe()
modern_train.to_csv('modern_train.csv',sep=',')$ modern_test.to_csv('modern_test.csv',sep=',')
fulldf.to_csv('FullResults.csv', encoding='utf-8', index=False)
data.loc[[pd.to_datetime("2016-12-01"), pd.to_datetime("2016-12-03")], ["TMAX", "TMIN"]]
pres_df['location'][0].split(',')
session.query(func.count(Station.station)).all()
user_similarity = skl.metrics.pairwise.cosine_similarity(train_data_matrix)$ item_similarity = skl.metrics.pairwise.cosine_similarity(train_data_matrix.T)
grad_days_mean = records3[records3['Graduated'] == 'Yes']['Days_missed'].mean()$ grad_days_mean
df_clean.columns
1/np.exp(result_2.params)
tmdb_movies_vote_revenue['genre'] = tmdb_movies_vote_revenue['genre'].map(lambda x: x.get('name'))$
my_df["company_create"] = df_user.groupby('nweek_create')['company_id'].nunique()$ my_df["company_active"]   = df_user.groupby('nweek_active')['company_id'].nunique()
daily_change={row[0]:row[2]-row[3] for row in price_list if row[1]!=None}$ max_daily_change=max(daily_change.items(), key=lambda k: k[1])[1]$ print("The largest change in any one day is "+str(round(max_daily_change,2)))
r = requests.get("https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2014-06-01&end_date=2014-06-02&api_key="+API_KEY)$ print(r.json())
df[df['group']=='control'].groupby('landing_page').count()
print (df.dtypes[df.dtypes == 'object'])$
tickers = companies['tweet_ticker'].tolist()$
pres_df['ad_length_tmp'] = pres_df['end_time'] - pres_df['start_time']$ pres_df['ad_length_tmp'].head(10)
favorite_count_3q = master_df.favorite_count.quantile(0.75)$ favorite_count_iqr = master_df.favorite_count.quantile(0.75) - master_df.favorite_count.quantile(0.25)$ outlier_df = master_df.loc[master_df.favorite_count >= (favorite_count_3q+(1.5*favorite_count_iqr))]
df_schools = pd.read_csv(file1)$ df_students = pd.read_csv(file2)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
users.shape
data["hours"] = data["time_up_clean"].astype(str).str[0:2]$ data["hours"] = data["hours"].astype(int)$ data.loc[data["hours"] == 0, "hours"] = 1
train=pd.read_csv('pisa2009train.csv')$ test=pd.read_csv('pisa2009test.csv')$ train.shape
observed_mean_treatment = df2.query('group=="treatment"')['converted'].mean()$ print(observed_mean_treatment)
x_test = np.array(data)
average_chart_lower_control_limit = average_of_averages - 3 * d_three * average_range / \$                                     (d_two * math.sqrt(subgroup_size))
image_clean.dog_breed.value_counts()
df_final.drop(['doggo', 'floofer', 'puppo', 'pupper'], axis=1, inplace=True)
temperature_df = pd.DataFrame(temperature, columns=['date', 'tobs'])$ temperature_df.set_index('date', inplace=True)$ temperature_df.head()
invalid_sets = ["UGL", "UNH", "UST", "pCEL", "pHHO", "VAN"]$ all_sets = all_sets.loc[~all_sets.code.map(lambda x: x in invalid_sets)]
tfidf_vectorizer, tdidf_features = tfidf_extractor(sdf.Norm_reviewText)$ features = tdidf_features.todense()$ features_tfidf = np.round(features, 2)
bad_iv.groupby(['Strike']).count()['Expiration']
n_old = len(df2.query("group == 'control'"))$ n_old
film_category_df = pd.read_sql(SQL, db)
display('x', 'y', 'pd.concat([x, y], ignore_index=True)')
import findspark$ findspark.init("/usr/lib/spark")
df.Date = pd.to_datetime(df.Date_Hour)$ df = df.set_index('Date_Hour').sort_index(ascending=True)
import seaborn as sns$ dict_temp={'long':x_filtered,'lat':y_filtered,'interest':interest[x_filtered.index]}$ sns.swarmplot(x="long", y="lat", data=pd.DataFrame(dict_temp), hue="interest", size=1)
data.info()
cityID = '67b98f17fdcf20be'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Boston.append(tweet) 
tlen.plot(figsize=(16,4), color='r')
df.iloc[3,] # is this a row or column?
df2=pd.read_csv('C:\\Users\\Christopher\\Google Drive\\TailDemography\\csvFiles by year\\xCC2017x.csv')$ df2.loc[df2.Toes=='43085',]
msft = pd.read_csv("../../data/msft.csv", index_col=0)$ msft.head()
bruins.rename(columns={'Playoff':'playoff','Opponent':'opponent'}, inplace=True)
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1 - convert_mean), convert_mean])
volt_prof_before['Bus']=volt_prof_before['Bus'].apply(lambda x:x.lower())$ volt_prof_after['Bus'] =volt_prof_after['Bus'].apply(lambda x:x.lower())
df_average = df.groupby(['business_id']).mean()$ df_average.head()
financial_crisis.drop('Spain defaults 7x', inplace = True)$ print(financial_crisis)
orders_subset = orders[orders.user_id.isin(selected_users)]
df_twitter_extract_copy['tweet_id'] = df_twitter_extract['tweet_id'].astype(str)
cig_data_SeriesCO[cig_data_SeriesCO > cig_data_SeriesCO.median()]
logit = sm.Logit(df2.converted, df2[['intercept', 'new_page', 'CA_new', 'UK_new', 'CA', 'UK']])$ result = logit.fit()$ result.summary()
converted_usersU = sum(dfU.converted == 1)/len(unique_usrs)$ converted_usersU
station_count = session.query(Station.id).count()  $ print(f'There are {station_count} weather stations in Hawaii.')
df.sum()
df_us = df_parsed.query('country == "US"').reset_index()$ df_us.shape
table.info('stats')
title.apply(updateG,G,set(poltweeps.index))$ print (G.number_of_nodes(),G.number_of_edges())
df3['timestamp'] = pd.to_datetime(df3['timestamp'])$
md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128)$
knn.fit(X_train_scaled, y_train)$ y_pred_class = knn.predict(X_test_scaled)$ print metrics.accuracy_score(y_test, y_pred_class) # This is the accuracy
results3 = logistic_countries2.fit()$ results3.summary()
import pandas as pd$ reviews=pd.read_csv("ign.csv")$ reviews.head()
xmlData['statezip'] = xmlData[['state', 'zipcode']].apply(lambda x: ''.join(x), axis = 1)$ xmlData.drop('state', axis = 1, inplace = True)$ xmlData.drop('zipcode', axis = 1, inplace = True)
autos = autos.drop(['seller','offer_type','no_of_pictures'],axis=1)
list.remove(3)$ print(list)
station = session.query(Station).count()$ station
QUIDS = QUIDS.loc[(QUIDS['week'] == 0) |(QUIDS['week'] == 12)  |(QUIDS['week'] == 14)]$ QUIDS = QUIDS[["subjectkey", "qstot", "week"]].sort_values(['subjectkey', 'week'])
evaluator.evaluate(cvPredictions)
df_master.info()
prcp.describe()
1/np.exp(-0.0149)
np.exp(-0.0150)
words_mention_scrape = [term for term in words_scrape if term.startswith('@')]$ corpus_tweets_scraped.append(('mentions', len(words_mention_scrape))) # update corpus comparison$ print('Total number of mentions: ', len(words_mention_scrape)) #, set(terms_mention_stream))
median_comments = reddit['num_comments'].median()$ median_comments
model.fit(X_tr, y_tr)$ preds = model.predict(X_val)
len_plot = t_len.plot(figsize=(16,4), label="Length", color='r', legend=True, title='Length of tweets over time')$ len_vs_time_fig = len_plot.get_figure()$ len_vs_time_fig.savefig('len_vs_time.png')
test_data["Age no Nans"] = test_data["AgeuponOutcome"].fillna(value="2 years")$
nba_df.loc["2017-01-01", "Referee1":]
grid.fit(X_trainfinaltemp, y_train)
top_10_authors = git_log['author'].value_counts().iloc[:10]$ print(top_10_authors)
pd.set_option('display.max_rows', 500)
df['Trip_duration'].dt.components
df_st.head()
before_sherpa = df.loc[df["index"] <= 1685.0]$ after_sherpa = df.loc[df["index"] > 1685.0]$
festivals.at[2,'latitude'] = 41.9028805$ festivals.head(3)
n_new = df2.query("landing_page == 'new_page'")['landing_page'].count()$ n_new
n_new = df2[df2['group'] == 'treatment']['converted'].count()$ n_new
from pandas_datareader import data$ goog = data.DataReader('GOOG', start='2004', end='2016', data_source='morningstar')$ goog.head()
inspector = inspect(engine)$ inspector.get_table_names()
fare = df_titanic['fare']$ print(fare.describe())$
print (train["review"][0])$ print ("\n", example1.get_text())
temp_data_query = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
joined.tail(100)
print(open('datasets/git_log_excerpt.csv'))
stat_info_st = stat_info[0].apply(fix_space_0)$ print(stat_info_st)
X_train_predict = model.predict(X_train)$ X_test_predict = model.predict(X_test)
data_folder = '../input/export_Feb_8_2018'$ ods = SlackLoader(data_folder)
plt.scatter(USvideos['likes'], USvideos['views'])
result_df = pd.DataFrame({'profile_id':profile_ids})$ result_df['num_events'] = result_list
df2 = df2.drop(1899)$ df2.info()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ z_score, p_value
part = total[datetime(2017, 6, 30, 11, 0, 0):datetime(2017, 6, 30, 13, 59, 59)]$ plt.plot(part['field4'])$ plt.show()
tweet_archive_clean = pd.merge(tweet_archive_clean, info, on = 'tweet_id', how = 'inner')
total_docks = []$ for day in train.date:$     total_docks.append(sum(stations[stations.installation_date <= day].dock_count))
(p_diffs > act_diff).mean()
from bs4 import BeautifulSoup$ example1 = BeautifulSoup(train["comment_text"][0], "html.parser")
print("Min " + str(tm['created_at'].min()) + " Max " + str(tm['created_at'].max()))
df.index
left = pd.DataFrame({'key':['foo','boo'], 'qval': [1,2]})$ right = pd.DataFrame({'key': ['foo','boo','zoo'], 'rval': [4,5,100]})$ print( pd.merge(left,right,on='key'))
Measurement = Base.classes.measurement$ Station = Base.classes.station
ws = Workspace.from_config()$ print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\n')
print (df.shape)$ df2 = df.drop(df.index[[5, 12, 23, 56]])$ print (df2.shape)$
stats = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$ filter(Measurement.station=="USC00519281").group_by(Measurement.station).all()$ stats
n_new = df2.query('group == "treatment"').count()[0]$ n_new
ttarc_clean = ttarc.copy()$ imgp_clean = imgp.copy()$ tt_json_clean = tt_json.copy()
autos.head()
df_new['country'].unique()
logit_mod = sm.Logit(df_joined['converted'], df_joined[['intercept','ab_page', 'US', 'UK']])$ results = logit_mod.fit()
for col in Y_train_df.columns:$     nbsvm_models[col] = NbSvmClassifier(C=10, dual=True).fit(X_train_cont_doc, Y_train_df[col])
log_user2 = df_log[df_log['user_id'].isin([df_test_user_2['user_id']])]$ print(log_user2)
Inspection_duplicates = data_FCInspevnt_latest.loc[data_FCInspevnt_latest['brkey'].isin(vals)]$ Inspection_duplicates
np.exp(-0.0099), np.exp(-0.0506) $
col = list(X_testfinal.columns)$ col[2]= '1hr'$ X_testfinal.columns = col$
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)$ regr = LinearRegression()$ regr.fit(X_train, y_train)
s = pd.Series([1, 2, 3, 4, 5, 6])$ s
mcg = s4g.groupby(['Symbol', 'Year', 'Month'])
tweets_df.to_csv("CityData.csv", encoding='utf-8')
base_df.tail()
train = pd.DataFrame(dates.items(), columns=['date', 'trips'])
xml_in_sample['authorId'].nunique()
fires = pd.read_csv('../../datasets/san_francisco/san_francisco/fire_data/fire_incidents.csv')$ crimes = pd.read_csv('../../datasets/san_francisco/san_francisco/sf_crime/sfpd_incidents_from2003.csv')$
my_list = [0.25, 0.5, 0.75, 1.0]$ data = pd.Series(my_list) # "the constructor"$ data
input_nodes_DF = nodes_table('network/source_input/nodes.h5', 'inputNetwork')$ input_nodes_DF[1:5]
pd.isnull(r1_test).values.any()$
flight_hex = h2o.H2OFrame(flight_pd)
mars_df.rename(columns={0: 'description', 1: 'value'}, inplace=True)$ mars_df.set_index('description', inplace=True)$ print(mars_df)
print k_var.category_name.unique()$ print k_var.groupby('category_name').size()
print("Saving data to elasticsearch - please be patient")$ df = df.withColumn("datetime",df["datetime"].cast("string")) # elasticsearch needs datetimes in a string type$ es.saveToEs(df,index=es_dataindex,doctype=es_datatype)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller' , prop_var=False )$ print(z_score, p_value)
seed = 2210$ (train_sample, validation) = modeling2.randomSplit([0.9,0.1], seed=seed)
early = df['created_at'].min()$ recent = df['created_at'].max()$ print("the earliest tweet of this set is {} and the most recent is {}.".format(early,recent))
Largest_change_in_one_day = (mydata['High'] - mydata['Low']).max()$ Largest_change_in_one_day
returned_orders_data.to_csv("returned_orders_data.csv")
freq_dict = grouped_words['Frequency'].to_dict()
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
df.A
df.info()
df2['intercept'] = 1$ df2[['control','treatment']] = pd.get_dummies(df2['group'])$ df2.head()
import IPython  # for displaying parse trees inline$ for tree in parser.parse(sentence):$     IPython.display.display(tree)  # instead of tree.draw()
pd.pivot_table(df,index= ['class'],columns= ['year'],values= ['hwy', 'cty'],aggfunc= [np.mean, np.max],margins= True)
p_old = df2.converted.mean()$ p_old
group_vT = df2.groupby('vehicleType').agg({"powerPS": [min, max, mean]}) $ group_vT.columns = ["_".join(x) for x in group_vT.columns.ravel()]$ group_vT
fig = plot2.get_figure()$ fig.savefig("output_figure.svg")
reddit.Upvotes.value_counts(ascending=False).head(25) #just seeing the number of upvotes for each Reddit post$
control_new_page = df[(df.group == 'control') & (df.landing_page == 'new_page')]$ treatment_old_page = df[(df.group == 'treatment') & (df.landing_page == 'old_page')]$ control_new_page.shape[0] + treatment_old_page.shape[0]
my_tag={'name':'img','title':'Sunset boulevard',$        'src':'sunset.jpg','cls':'framed'}$ tag(**my_tag)
pd.DataFrame(pred)[pd.DataFrame(pred) > 0].dropna().head(10)
for i, row in companies.iterrows():$     print(companies['twitter'][i])
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall()$
df["created"] = pd.to_datetime(df["created"])$ df["last_event"] = pd.to_datetime(df["last_event"])$ df.head()
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$
stc.checkpoint("checkpoint10")
punct_re = '[^\w\s]'$ trump['no_punc'] = trump['text'].str.replace(punct_re, " ")$
taxiData2.loc[taxiData2.Fare_amount <=0, "Fare_amount"] = 1
own_star[own_star.duplicated('uniqueID', keep=False)].shape
print("Unique users:", len(df2.user_id.unique()))$ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&start_date=2017-01-02&end_date=2017-01-02&api_key=nK_oDNyRo17zSF7LsAUb')$ print(r.json())$
RF_feature_impt = pd.DataFrame({'features':features.columns, 'importance':model_rf.feature_importances_}).sort_values('importance', ascending=False)$ RF_feature_impt.head(20)
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))$
print 'All IDs are unique' if len(response_df) == len(response_df.id.unique()) else 'IDs not all unique'
X_testset.shape$ y_testset.shape
pd.set_option('max_colwidth',150)$ df_en.head()
prob_new_page = df2.query('landing_page== "new_page"')['user_id'].count()/df2.shape[0]$ print("Probability of individual receiving new page is :", prob_new_page)
plot_data = df['amount_tsh']$ sns.kdeplot(plot_data, bw = 1000)$ plt.show()
sentiments_df = sentiments_df.sort_values(["Target","TweetsAgo"], ascending=[True, False])$ sentiments_df.head()
throwaway_df = sqlContext.createDataFrame([('Anthony', 10), ('Julia', 20), ('Fred', 5)], ('name', 'count'))$
There is an ascending trend in the data.It means the higher the score is,the more likely shops can get this score.$ The unusual features are that the shops are much less likely to get odd scores than even scores,$ especially when the score is above 85.$
old_page_converted = np.random.binomial(1,p_old,n_old)$ old_page_converted.mean()
df.info()
app_install = df_nona.groupby('create_date').apply(lambda v: sum(v.accounts_provisioned)/float(sum(v.district_size)))$ app_install.resample('w', np.mean).plot(title='Install Rate by Adopted Time')$
year9 = driver.find_elements_by_class_name('yr-button')[8]$ year9.click()
len([b for b in BDAY_PAIR_df.pair_age if b<0 ])
primitives = ft.list_primitives()$ print(primitives.shape)$ primitives[primitives['type'] == 'aggregation'].head(10)
df[df.duplicated(subset='user_id', keep=False)].groupby(['group', 'landing_page']).count()
non_cancel_df = data_df[~(data_df['delay_time'] == "Cancelled")].copy()$ non_cancel_df['delay_time'] = non_cancel_df['delay_time'].apply(lambda x: float(x))$ non_cancel_df = non_cancel_df.sort_values(['Departure', 'Arrival', 'Airline', 'flight_year', 'flight_month', 'flight_day', 'std_hour'],ascending=False)
pd.read_csv("data.csv", index_col=[0, 1, 2, 3, 4, 5], skipinitialspace=True, parse_dates=['Date']).head(3)
sel=[Measurement.date,$      Measurement.tobs]$ day_prcp=session.query(*sel).filter((Measurement.date>year_ago)).all()$
unique_Taskers_shown_most = sample['tasker_id'].value_counts().head(1)$ unique_Taskers_shown_most
grouped_df = news_sentiment_df.groupby('News_Source')$ grouped_compound = grouped_df['compound'].mean()$ grouped_compound
results.summary()
i = issues$ len(i[(i.activity <= '2013-01-01') & (i.activity > '2012-01-01')])
fraud_data.loc[0,"ip_address"]
df['up_votes'].quantile(q=.50)$ df.loc[df['up_votes'] == 192674]['title'].values
ab_df2[((ab_df2['group'] == 'treatment') == (ab_df2['landing_page'] == 'new_page')) == False].shape[0]
dfp.groupby("congress").bill_type.count().plot(kind='bar')$ plt.xlabel('Bills per Session')
for columns in DummyDataframe[["Positiv", "Negativ"]]:$     basic_plot_generator(columns, "Graphing Dummy Data" ,DummyDataframe.index, DummyDataframe)
image_predictions.info()
model_unigram = models.Word2Vec(sentences = flatSentenceList,iter=5)
from sightengine.client import SightengineClient$ client = SightengineClient("737618018", "bstrJ5VzARavYy5FsELN")$ output = client.check('face-attributes').set_url('http://cache.magazine-avantages.fr/data/photo/w1000_c18/4j/hommebrunyeuxverts.jpg')
import statsmodels.formula.api as sm$ result = sm.OLS( wait_time_df, regression_df ).fit()$ result.summary()
df.set_value(306, 'yob',1998)
clean_sentiments['Date'] = pd.to_datetime(clean_sentiments['Date'])
plt.plot('date', "posts", label='FB posts')$ plt.legend(loc='best')$ plt.show()
station_counts = station_df.groupby('Station ID').count().sort_values(by = 'Precip', ascending = False)$ station_counts
numofstations=session.query(Station.station).count()$ numofstations
df1_dummies.shape
df.shape
!hdfs dfs -cat 31results-output/part-0000* > 31results-output.txt$ !head 31results-output.txt
print ('tweets with no image:' + str(df_archive['expanded_urls'].isnull().sum()))$ df_archive = df_archive[df_archive['expanded_urls'].notnull()]$ len(df_archive)
df2.drop([1899], inplace=True)
Precipitation_DF.plot(rot=45,title="Precipitation from %s to %s"%(start_date,end_date),figsize=(8,5),grid=None,colormap="PuOr_r")$ plt.show()
df['is_application'] = df.application_date.apply(lambda x: 'No Application' if x == None else 'Application')$ df.head(5)
pd.to_datetime(pgh_311_data['CREATED_ON']).head()
nitrodata['ActivityStartDate'] = pd.to_datetime(nitrodata['ActivityStartDate'],format="%Y-%m-%d")
df_variables.to_csv("../01_data preprocessing/data new/variables.csv", encoding="utf-8", sep=",", index=False)
json_data_2017 = request_data_2017.json()
df3 = pd.merge(df1, df2, on='employee', how='outer') # default is how='inner'$ df3
recipes.ingredients.str.len().describe()
all_noms[(all_noms["agency"] == "Foreign Service") & (all_noms["confirmed"] == "yes")]["nom_count"].sum()
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
from horse.betsim.wrap.daily import DailyRaces$ from horse.betsim.data.db.engines import engine_ac, engine_factors;
timestamps_df = pd.DataFrame(times_events, columns=time_columns)$ timestamp_left_df = timestamps_df[timestamps_df['timestamp'] != ""]$ timestamp_left_df.head()
df.isnull().sum()
def create_df_grouped_by_date( tweets_df ):$     return tweets_df.groupby( Grouper( 'date' )).mean()
print (test.shape)
testdata = np.load(outputFile)$ data = pd.read_csv(inputFile, delimiter=',', header=None)
rtitle = [x.text for x in soup.find_all('a', {'data-event-action':'title'})]$ rtitle.pop(0)
transactions.merge(users,how="outer",on="UserID")
f_u18 = pop_df['under18'] / pop_df['total']$ f_u18.unstack()
clf.fit(digits.data[:-2], digits.target[:-2])
mask = y_test.index$ t_flag = y_test == 1$ p_flag = pred == 0
pd.merge(df3,df4)
norm.ppf(1-(0.05))
wordfreq = FreqDist(words_sk)$ print('The 100 most frequent terms, including special terms: ', wordfreq.most_common(100))
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings_v1.csv', sep=',', encoding='utf-8', header=True)
np.exp(logit_mod_joined2_result.params)
dfClientes.loc[dfClientes["MES"] == 201701].head()
n_old = df2.query('landing_page == "old_page"')['user_id'].count()$ n_old
df.drop(df.query("group =='treatment' and landing_page =='old_page'").index, inplace = True)$ df.drop(df.query("group =='control' and landing_page =='new_page'").index, inplace =True)
print("State space:", env.observation_space)$ print("- low:", env.observation_space.low)$ print("- high:", env.observation_space.high)
measurements_df.head()
print('Total number of different vote types recorded: {}'.format(len(v['vote'].value_counts(dropna=False))))$ v['vote'].value_counts(dropna=False).nlargest(10)
Base = automap_base()$ Base.prepare(engine, reflect=True)
df_amznnews['publish_time'] = df_amznnews['publish_time'].astype(str)$ df_amznnews['publish_time'] = pd.to_datetime(df_amznnews['publish_time'], format='%Y%m%d%H%M')$ df_amznnews.info()$
model = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']])$ results = model.fit()$ results.summary()
my_df[my_df.isnull().any(axis=1)].head()
station_df =pd.read_sql('station',engine)$ station_unique_df = station_df['station']$ station_unique_df.nunique()$
news_p = soup.find("div", class_="rollover_description_inner").text
from h2o.estimators.random_forest import H2ORandomForestEstimator
pd.to_datetime(segments.st_time[:])
col_eliminar = ['bbox_coords','coords_coords','country_code','ext_media_t_co','ext_media_url','symbols']$ tweetsDf = tweetsDf.drop(columns=col_eliminar)
plate_appearances = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).first().reset_index()
print "About: Incidents table date field 1 {}".format(INC.ix[:, 'date_incid'].head())$ print "About: Incidents table date field Original {}".format(INC.ix[:, 'incident_d'].head())$ print "About: Weather table date field {}".format(weather.ix[:, 'EST'].head())$
df_concat.rename(columns={"likes.summary.total_count" : "likes_total",$                           "comments.summary.total_count" : "comments_total" }, inplace = True)
table.meta['TSMIN']
old_page_converted = np.random.binomial(N_old, P_old)$ old_page_converted
stop_words = nltk.corpus.stopwords.words('english')$ print(stop_words)$ print('Count: {}'.format(len(stop_words)))
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)$ df.info()
os.remove(os.path.join(os.path.expanduser('~'), '.config', 'ok', 'auth_refresh'))$ ok.auth()
pd.merge(msftAR0_5, msftVR2_4)
df_predictions.head()
cabin = df_titanic['cabin']$ print(cabin.describe())$
date_ny['date'] = pd.to_datetime(date_ny.year*10000+date_ny.month*100+date_ny.day,format='%Y%m%d')$ date_ny['shannon'] = date_ny['count']
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113447896253141.json', encoding="utf-8-sig") as data_file:$     json_data1 = j.load(data_file)
lm = smf.ols(formula='sales ~ TV + radio + newspaper + TV*radio', data=data).fit()$ lm.params
df_new['country'].value_counts()
os.listdir()
tweets['SA'] = tweets['full_text'].apply(lambda x: analize_sentiment(x))
results.summary()
t = thisyear$ len(t[t.creation > '2015-01-01'])
pres_df['subject_count_tmp'].value_counts(dropna=False)$
ddf=df.drop(df.columns[[0,1,2,3,5]], axis=1)
master_df['Month of Year']=pd.to_datetime(master_df['startday']).dt.month.astype(str)
(keys.shape, keys_0611.shape)
old_page = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])$ print(len(old_page))
kushy_prod_df.describe(include="all")$
idx_close = r.json()['dataset']['column_names'].index('Close')
actual_diff = df2.converted[df2.group == 'treatment'].mean() - df2.converted[df2.group == 'control'].mean()$ (actual_diff < p_diffs_alt).mean()
data_libraries_df = pd.merge(left=libraries_df, right=tmp, on="asset_name", how="outer")
np.ones(5)
cityID = 'f995a9bd45d4a867'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Memphis.append(tweet) 
with open(json_filename) as json_file:  $     data = json.load(json_file)$ tweet_json = pd.DataFrame.from_records(data)
df_tweet_clean.drop(df_tweet_clean[df_tweet_clean['retweeted_status'].isnull()==False].index,inplace=True)
df1=pd.read_csv("https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv",skipinitialspace=True)
for column in trip_data_sub.columns:$     print column, ":", trip_data_sub[column].dtype
df.drop('zipcode_initial', axis=1, inplace=True)
from pyspark.sql.functions import desc$ df.groupby('Park Facility Name').count().sort(desc("count")).show()
data.head(5)
df4 = df3.copy()$ df4 = df4.drop('DAY',1)$ df4 = df4.drop('WEEK',1)
training.index = range(608)$ test.index = range(153)$ training.head()
train_data["totals.transactionRevenue"] = train_data["totals.transactionRevenue"].fillna(0.0)$ train_data["totals.transactionRevenue"] = np.log1p(train_data["totals.transactionRevenue"])
sells_primer_2015 = pd.read_csv('sells_primer_2015.csv',low_memory=False)$ sells_segundo_2015 = pd.read_csv('sells_segundo_2015.csv',low_memory=False)$ sells_primer_2015$
data = utils.load_data()
n_new = len(df2.query("group == 'treatment'"))$ n_new$
p_old = df2['converted'].mean()$ print("{} is the convert rate for Pold under the null.".format(p_old))
print topUserItemDocs.shape$ print topUserItemDocs.columns$ topUserItemDocs.head()
contractor_clean['last_updated'].head()$
full_image_elem = browser.find_by_id('full_image')
import statsmodels.api as sm$ logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']])$ result=logit.fit()
checking['age'].iloc[z]
index = pd.to_datetime(non_na_df['created_at'])$ non_na_df.index = index
k = 4$ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)$ neigh
avg_price_series = pd.Series(avg_price_by_brand)
<img src="/images/MongoDB1.png" alt="[img: MongoDB view]" title="MongoDB View" />
info_final.drop('idAviso', axis = 1, inplace = True)$
df = pd.read_csv('ab_data.csv')$ df.head()
dict_data = r.json()$ print(type(dict_data))$ print(dict_data)
sort_df = df.sort_values(by='date',ascending=True)$ sort_df.head(10)
temp_long_df.describe()
building_pa.to_csv("buildding_00.csv",index=False)
df_ad_state_metro_2.set_index('state', inplace=True)$ df_state_victory_margins.set_index('state', inplace=True)
crimes.resample('D').size().idxmax()
results = soup.find_all('span', attrs={'class':'short-desc'})$ results = soup('span', attrs={'class':'short-desc'})$ results = soup('span', class_='short-desc')
new_page_sim = (np.random.choice([1, 0], size=n_new, p=[p_mean/100, (1-p_mean/100)])).mean()$ output1 = round(new_page_sim, 4) * 100$ print(output1,'%')
print('Number of rows', reviewsDF.shape)
df2['converted'].mean()
plt.pie(drivers_totals, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()
df2.info()
!wget https://storage.googleapis.com/aibootcamp/data/ortec_templates.zip
print("Number of unique visitors in train set : ",train_data.fullVisitorId.nunique(), " out of rows : ",train_data.shape[0])$ print("Number of unique visitors in test set : ",test_data.fullVisitorId.nunique(), " out of rows : ",test_data.shape[0])$ print("Number of common visitors in train and test set : ",len(set(train_data.fullVisitorId.unique()).intersection(set(test_data.fullVisitorId.unique())) ))
air.groupby(["air_store_id"],axis=1).plot()$
plidata_blocks = pd.merge(plidata, parcel_blocks, how='left', left_on=['PARCEL'], right_on=['PIN'])$ plidata_blocks = plidata_blocks.drop(['PARCEL','PIN'], axis=1)$ plidata_blocks=plidata_blocks.dropna(subset=['TRACTCE10','BLOCKCE10'])
new_page_filtered = df[(df["landing_page"] == 'new_page') == (df["group"] != 'treatment')].shape[0]$ new_page_filtered
top_score = df.rating_score.max()$ print('The highest rating is a {} out of 5.'.format(top_score))
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new['country'].astype(str).value_counts()
sum(df.rating_denominator.value_counts()!=10)
z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]),np.array([n_new,n_old]), alternative = 'larger')$ print(z_score, p_value)
dataset[dataset['user_location'].str.contains("argentina",na=False)].groupby(by=['user_location'])['id'].count()
md_keys = ['   ' + s + ':' for s in METADATA_KEYS]$ md_idx = tmpdf_md.index[tmpdf_md[tmpdf_md.isin(md_keys)].notnull().any(axis=1)].tolist()$ md_idx
df_licenses.head(2)
store_items.fillna(0)
df_json_tweets['date_timestamp'] = pd.to_datetime(df_json_tweets['date_timestamp'])$ type(df_json_tweets['date_timestamp'].iloc[0])
t = pd.read_hdf('../t.hdf','table')$ dd = pd.read_hdf('../dd.hdf','table')
empty_sample.iloc[1000:1010]
p_old = df2.query("converted==1").user_id.nunique()/df2.user_id.nunique()$ p_old$
df = df[df["category"]=="Tech"].reset_index(drop=True)$ df.drop(["category"], axis = 1, inplace = True)
sales = sales.join(shops.set_index('shop_id'))  # train + shops join by shop_id $ items_categories = item_categories.join(items.set_index('item_category_id'))    # item_categories + items join by item_category_id$ sales = sales.join(items_categories.set_index('item_id'))    # train + items_categories join by item_id
aux = aux.join(pd.get_dummies(aux['country']))$ aux.drop(['user_id', 'country', 'CA'], axis=1, inplace=True)$ aux.head()
len(pres_df['subjects'][0].split())
PTrueNegative = (PNegativeTrue * PTrue) / PNegative$ "%.2f" % (PTrueNegative * 100) + '%'
col_arr=df["WHO Region"].values$ col_arr
nytimes_df = constructDF("@nytimes")$ display(constructDF("@nytimes").head())
temps_df.columns
mov_ids = tmp_df[tmp_df.tmp_idx == usr_idx].columns
labeled_docs = LabeledLineSentence(candidate_data['messages'], candidate_data.index)
round((model_x.rsquared), 3)
df_columns[df_columns.index.month.isin([11,12,1])]['Complaint Type'].value_counts().head()$
airports_df[airports_df['city'].str.lower() =='new york']
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))$
df['State'][df.CustomerID=='0000114883']='MA'
np.exp(-0.0150)
print("{:.2f} GB".format(df.fileSizeMB.sum() / 1024))
!ls ../input/ #MacOS command$
df_data = pd.read_csv(CSV_FILE, dtype=str)$ print '%d rows' % len(df_data)$ df_data.head()
sc.getConf().get("spark.yarn.historyServer.address")
(data_2017_12_14[data_2017_12_14['text'].str.contains("felicidades", case = False)])["text"].head()
free_data[free_data['educ']>5].groupby('age_cat').describe()
tt_final.groupby(['all_p'])['favorite_count'].mean().sort_values(ascending=False).head(10)$
df_countries = pd.read_csv('./countries.csv')$ df_countries.head()
per_var = np.round(pca.explained_variance_ratio_*100, decimals=1)$ labels = ['PC' + str(num) for num in range(1, len(per_var) + 1)]
conn = 'mongodb://localhost:27017'$ client = pymongo.MongoClient(conn)$
tree = RandomForestClassifier(max_depth = 4)$ tree.fit(X_train, y_train)
df_full.rename(columns=DATA_L2_HDR_DICT,inplace=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)
weather_file_df = pd.read_csv(weather_file, encoding = "ISO-8859-1")$ weather_file_df.head()
control_new_page.set_index('timestamp')$ treatment_old_page.set_index('timestamp');
sentimentDf["date"] = pd.to_datetime(sentimentDf["date"])$
indeed[indeed['summary'].isnull()]$
ax = time_length.plot(figsize=(16,4), color='r', title='Tweet length vs. DateTime')$ ax.set_xlabel("DateTime")$ ax.set_ylabel("Tweet length")
new_df = pd.read_sql_query('select * from film where title = "Hunchback Impossible"', engine)$ new_df.head()$
df_new[['US','UK','CA']] = pd.get_dummies(df_new['country'])$ df_new.drop('CA', axis = 1, inplace = True)$ df_new.head()
for df in dfs:$     if len(df.text[0].split('\n')) != df.npar[0]:$         print(df.shape[0])
df2 = df2[df2.timestamp != '2017-01-09 05:37:58.781806']
cdum=pd.get_dummies(df_new['country'])$ df_new=df_new.join(cdum)
days_alive / pd.Timedelta(nanoseconds=1)
cols='average_stars 	compliment_cool 	compliment_cute 	compliment_funny 	compliment_hot 	compliment_list 	compliment_more 	compliment_note 	compliment_photos compliment_plain  compliment_writer compliment_profile review_count  fans  funny'.split()$ X=df[cols]$ X.head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ mydata = pd.read_csv(path, sep ='\s+', na_values=['.'])$ mydata.head(5)
df2.drop(labels=1876, axis=0, inplace=True)$
data.dtypes
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False]['user_id'].count()
grades = grades.drop(grades.index[(grades.Mark == 0) & (grades.ATAR == 0)])$ noatar = grades[(grades.ATAR == 0)]$ noatar.Mark.hist(bins=25)
pd.options.mode.chained_assignment = None$ sub_dataset['WordCountBins'] = pd.cut(sub_dataset['WordCount'], bins = [0, 1, 128, 256, 512, 1024, 2048, 1048576], include_lowest=True, right=True)
yhat = loanlr.predict(X_test)$ yhat
with open(os.path.join('/Users/LucasRamadan/Play-Bigger-Research/descriptions/', 'facebook.txt'), 'w', encoding='utf-8') as f:$     f.write(fb_desc)
locations = session.query(Measurements).group_by(Measurements.station).count()$ print("There are {} stations.".format(locations))
MEDIAN_daily_trading_volume = mydata['Traded Volume'].median()$ MEDIAN_daily_trading_volume
filter_df = filter_df[filter_df['race'] == 'PRES']$ filter_df.head(2)
df['CI_Age Unit'].unique().tolist()
converted_users_count = df[df['converted'] == 1].user_id.count()$ print(converted_users_count)$ (converted_users_count / unique_users_count) * 100
df.groupby('group').mean()['converted']
df_mk.groupby('id').size().nlargest(10)
honeypot_df.drop(['time_stamp1','time_stamp2','time_stamp3'], axis = 1, inplace = True)
print(giss_temp.shape)$ print(giss_temp.dtypes)
print(rf.feature_importances_)$ print(col)
p_mean = np.mean([p_new, p_old])$ print("Probability of conversion udner null hypothesis (p_mean):", p_mean)$
client.repository.ExperimentMetaNames.show()
df2[df2.duplicated(['user_id'], keep=False)]
giss_temp = giss_temp.drop("Year")$ giss_temp
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']])$ results = lm.fit()$ results.summary()
df = pd.read_csv('ab_data.csv')$ df.head()
engineroute = "H:/Google Drive/WORK/Groundwater Chemistry"$ sys.path.append(engineroute)$ import enginegetter
df_new = df.query('group == "treatment" and landing_page == "old_page" or group == "control" and landing_page == "new_page"  ')$ df_new.shape[0]                 
df.AGREEMENT_REQUIRED_NEW_POSITIONS.str.contains("^Yes", na=False) 
print("The probability of an individual converting is: {}".format(df['converted'].mean()))
old_page_converted = np.random.binomial(n_old,p_old,10000)/n_old$ print (old_page_converted)
spreadsheet = '1LTXIPNb7MX0qEOU_DbBKC-OwE080kyRvt-i_ejFM-Yg'$ wks_name = 'CleanedData'$ d2g.upload(df_dn,spreadsheet,wks_name,col_names=True,clean=True)
lon_us = lon[lon_li:lon_ui]-360$ lat_us = lat[lat_li:lat_ui]$ print(np.min(lon_us), np.max(lon_us), np.min(lat_us), np.max(lat_us))
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Amount"])
gnb.fit(X_clf, y_clf)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key={}".format(API_KEY))$ print(r.json()['dataset']['data'][0])
df = pd.read_csv('../data/rawdata/HPD_Heat_HotWater.csv'$                   , dtype = {'Master SR #': str, 'SR #': str})
df.loc[df.userLocation == 'Manchester, PA', 'tweetText']$
clusters_compare=pd.concat([final_df_copy,pd.DataFrame(data=kmeans3,columns=['kmeans3']),pd.DataFrame(data=kmeans5,columns=['kmeans5']),pd.DataFrame(data=kmeans6,columns=['kmeans6'])],axis=1)$ clusters_compare.head()$ clusters_compare.to_csv(encoding='utf-8',path_or_buf='clusters.csv')
userMovies = userMovies.reset_index(drop=True)$ userGenreTable = userMovies.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)$ userGenreTable
year14 = driver.find_elements_by_class_name('yr-button')[13]$ year14.click()
stop_words = set(stopwords.words('english'))$
df_amznnews_clsfd_2tick = df_amznnews_clsfd[['publish_time','textblob_sent', 'vs_compound']]$ df_amznnews_clsfd_2tick.head()
df_modeling_categorized = pd.get_dummies(df_modeling, columns=['country', 'currency'])
coins = ccw.get_coin_list()$ COIN_DB = pd.DataFrame.from_dict(coins, orient='index')$
    $     text = clean_text(text)$     return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]$
%matplotlib inline$ autos = autodf.groupby('abtest').size() #ok
tweet = result[0] #Get the first tweet in the result$ for param in dir(tweet):$ (param, eval('tweet.'+param))
bldg_data_0 = bldg_data[bldg_data['255_elec_use']==0]$ bldg_data_0.groupby([bldg_data_0.index.year,bldg_data_0.index.month]).agg('count').head(100)$
categorical = free_data.dtypes[free_data.dtypes == "object"].index$ free_data[categorical].describe()
df_2013['bank_name'] = df_2013.bank_name.str.split(",").str[0]$
pax_raw['minute_of_day'] = pax_raw.paxhour*60 + pax_raw.paxminut$ pax_raw.head()
(obs_diff < p_diffs).mean()
ctc = ctc.round(1)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK']])$ results=logit_mod.fit()$ results.summary()
r.summary()
from scipy.stats import norm$ norm.cdf(z_score)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)])$ old_page_converted.mean()
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-06-25&end_date=2018-06-25&api_key={}'.format(API_KEY))
fraud_df['signup_time'] = fraud_df['signup_time']+pd.to_timedelta(fraud_df['gmt_hour_avg'],unit='h')$ fraud_df['purchase_time'] = fraud_df['purchase_time']+pd.to_timedelta(fraud_df['gmt_hour_avg'],unit='h')
df[12].plot()
p_new = df2['converted'].mean()$ print("Convert rate for p_new :", p_new)
columns = inspector.get_columns('measurement')$ for c in columns:$     print(c['name'], c["type"])
!head -n 2 Consumer_Complaints.csv
tesla['nlp_text'] = tesla.text.apply(lambda x: tokenizer.tokenize(x.lower()))$ tesla.nlp_text = tesla.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ tesla.nlp_text = tesla.nlp_text.apply(lambda x: ' '.join(x))
pd.Timestamp('17:55')
df['messages'].value_counts()
df2['ab_page']=pd.get_dummies(df['group'])['treatment']$ df2['intercept']=1$ df2.head()
df.resample('A').mean()
dflong = pd.read_csv("SP500_Long_V4.CSV")$ print dflong.shape$
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)
autos = autos[autos['registration_year'].between(1900,2016)]$ autos['registration_year'].describe()
recipes.head()
df = pd.read_csv('ab_data.csv')$ df.head()
gpCreditCard.Trip_distance.describe()
df.groupby('COUNTRY').size().nlargest(5)
s = pd.Series([1,2,3], index =['one','two','three'])$ s
Difference_mean = (new_page_converted.mean() - old_page_converted.mean() )$ Difference_mean
reviews.region_1.fillna('Unknown').value_counts()
commonwords=set(list(zip(*wordfrequencylist))[0]).intersection(list(zip(*hashtaglistw))[0])$ frequentcommonwords=set(list(zip(*wordfrequencylist[:200]))[0]).intersection(list(zip(*hashtaglistw[:200]))[0])
tweet_data = pd.read_json('tweet_json.txt', lines=True, encoding='utf-8')
ward = joblib.load('./data/Ward.pkl')
conn = sqlite3.connect("geo.db")$
totalVisits_month_byMC = pd.DataFrame(df_visitsbyCountry.groupby(['marketing_channel', 'country_code', 'year', 'month', 'day'], as_index=False).sum())$ print 'DataFrame totalVisits_month_byMC: ', totalVisits_month_byMC.shape$ totalVisits_month_byMC.head()
df_os[df_os['domain'] == 'infowars.com']
df2.query('user_id == 773192')
converted_group = df2.query('converted == 1')$ print(converted_group.user_id.nunique() / df2.user_id.nunique())
combined_data.to_excel('SHARE-UCSD-export_reformatted.xlsx',encoding="utf-8", index=False)$ combined_data.to_csv('SHARE-UCSD-export_reformatted.csv',encoding="utf-8", index=False)
with open(json_file,'w+') as f:$         json.dump(tlist,f,indent=4)
model_uid = client.repository.get_model_uid(saved_model_details)$ print("Saved model uid: " + model_uid)
shopping_carts = pd.DataFrame(items)$ shopping_carts
with open('data.json', 'w') as f:$     json.dump(r.json(), f)
s4.count()
prob_group2 = df2.query("group == 'treatment'")["converted"].mean()$ print("In the 'treatment' group the probability they converted is {}.".format(prob_group2))
conn.execute(q)
df_new['country'].astype(str).value_counts()
gnb.__dict__
count_authors_with_given_numer_publications = data_final.groupby('countPublications', as_index=False)['authorId'].count()$ count_authors_with_given_numer_publications.columns = ['number_publications', 'how_many_authors_with_given_publications']$ count_authors_with_given_numer_publications.head(20)
df_t.sort_values(by='timestamp').head(2)
print(fraud.user_id.duplicated().sum())
import pickle$ posts_transformed = pickle.load(open('posts.pkl', 'rb'))$ posts_transformed[0].head()
monte.str.len()
model_knn_vector = KNeighborsClassifier(n_neighbors=250)$ model_knn_vector = model_knn_vector.fit(train_arrays, train_labels)$ model_knn_vector.score(test_arrays, test_labels)
chained2 = itertools.chain.from_iterable([[1, 2, 3, 4], [5, 6, 7, 8]])$ for letter in chained2:$     print(letter)
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
plt.scatter(X2[:, 0], X2[:, 1])
mask = (df['message'].str.len() > 3) #Remove cases with message length < 3.$ df = df.loc[mask]
mapping = pd.read_csv("../../../data/essentials/md_paths_v3.csv", index_col=[0])$ reports = pd.read_excel('../../../data/essentials/Report By Category.xls', index_col=[0])
file_names = []$ file_names = glob.glob('../craigslist-data/final-data-multiple-cities/*.csv')
hdf = df.set_index(['AgeBins', 'Industry'])$ hdf.loc[('adult', 'Cosmetics'), :].head()
int_ab_page = 1/np.exp(-0.0150)$ int_ab_page
result_set =session.query(Adultdb).filter(Adultdb.education.in_(['Masters', '11th'])).all()
data_AFX_X.median()['Traded Volume']
df3_obs.loc['Churn Date'] = obs_end_date - dt.timedelta(df3_obs.loc['Frequency of donations'])
print data.shape$ sum(data.irlco)
df.head(2)
twitter_archive_master.to_csv('twitter_archive_master.csv')$ 
df_temp = df_combined['DateTime'] - df_combined['Intake DateTime']$ print df_temp.mean()
female = crime.loc[crime['Sex']=='F']$ female.head(3)
l=len(col)$ for i in range(1,l):$     X=np.column_stack((X,col[i]))
country_dummies = pd.get_dummies(df['country'])$ currency_dummies = pd.get_dummies(df['currency'])$ delivery_dummy = pd.get_dummies(df['delivery_method'])
pn['words'] = pn[0].apply(cw)$ d2v_train = pd.concat([pn['words'], comment['words']], ignore_index = True) 
import numpy as np$ result=results.home_score$ result.apply(np.median)
hist(df.pre_clean_len,100)$ grid()
autos.describe(include='all')
df_vow['Low'].unique()
result = pd.concat(six)$ free_data = pd.merge(left=free_data, right = result.to_frame(), left_index=True, right_index=True)
ans = reviews.loc[reviews.country.notnull() & reviews.variety.notnull()]$ ans = ans.apply(lambda srs: srs.country+'-'+srs.variety, axis='columns')$ ans.value_counts()
df_1 = df.drop(['AppID','AppName', '1', '2', '3', 'Device','DeviceType','Tags','Updated'], axis =1)$
ans_1 = len(bus.groupby(['business_id']))$ ans_2 = 'Each record represents a store.'$ ans_3 = 'The primary key is business_id'
import tensorflow as tf$ x1 = tf.constant(5)$ x2 = tf.constant(6)
contrast['title'] = contrast['title'].apply(cleaner)
p_new = df2['converted'].mean()$ print(p_new)
ny_df=pd.DataFrame.from_dict(foursquare_data_dict)
df.loc['1998-09-10':'1998-09-15']
train.num_points.describe()
match[match.iloc[:,66 :77].notnull().any(axis=1)].iloc[:5,66 :77] # get rows with non-nulls in columns 66 :77
df_3.loc[3535: 3541]
y_pred = crf.predict(X_valid)$ metrics.flat_f1_score(y_valid, y_pred,$                       average='weighted', labels=labels)
r.json()
df_nona = df.fillna('NA')
precipitation_count = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23").count()$ print(" There are {} results being analyzed over the past 12 months".format(precipitation_count))
df.to_csv(LM_PATH/'df.csv', index=False)
crime = crime[crime.Sex != 'U']
log_model_us = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','US']])$ result = log_model_us.fit()$ result.summary()
df3 = df3.reset_index(drop=True)$ df_list = df_list.reset_index(drop=True)$ combine = (df_list.join(df3))
print (str(len(df_eng.groupby(['user_id']).sum()>=3 )*100/ len(df_user['object_id'].unique()))+'%'$        + ' of users used the system more than 3 times')
x.dtype = 'int32' # (need to revert)$ x.astype(float) # creates a new array
import statsmodels.api as sm$ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = logit_mod.fit()
bruins.reset_index(drop=False, inplace=True)$ bruins.rename(columns={'index':'game_id'}, inplace=True)
with pd.option_context('display.max_rows', 150):$     print(news_period_df[news_period_df['news_entities'] == ''].groupby(['news_collected_time']).size())
df.head(3) # only 3 data rows
mydict = r.json()['dataset']$ dict(list(mydict.items())[0:10])
result_set=session.query(Adultdb).filter_by(relationship='Not-in-family').all()
tweetdf.loc[tweetdf['lat'].notnull(),'lga'] = np.asarray(output)$
df['created_date'] = pd.to_datetime(df['created_date'])$ df.head()
prcp = Base.classes.prcp$ tobs = Base.classes.tobs$ stations = Base.classes.stations
from IPython.core.display import HTML$ css_file = '../style/style.css'$ HTML(open(css_file, "r").read())
p_percprofit = 1 - stats.norm.cdf(0.005,loc = monthly_portfolio_average,scale = np.sqrt(monthly_portfolio_var))$ p_percprofit
for item in result_set:$     print(item.index,item.relationship)
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)$ sns.boxplot(x="frcar", y="y", data = psy_prepro, ax=ax1).set_title("Anxiety driving/riding in a car")$ sns.boxplot(x="jmp2w", y="y", data = psy_prepro, ax=ax2).set_title("Feel jumpy and restless in past 2 weeks")$
tfidf_vect = TfidfVectorizer(stop_words="english")$ tfidf_mat = tfidf_vect.fit_transform(PYR['soup'].values)
len([premieSn for premieSn in SCN_BDAY.scn_age if premieSn < 0])/SCN_BDAY.scn_age.count()
keep_cols = ['id', 'favorite_count', 'retweet_count']$ df_tweet_clean = df_tweet_clean[keep_cols]$ df_tweet_clean.head()
cs.set_index('STNAME').groupby(level =0)['POPESTIMATE2010', 'POPESTIMATE2011', 'POPESTIMATE2012'].agg({'add': np.sum,$                                                                                   'avg' :np.average}).head()
for v in data.values():$     if v['answers']['Q4'] == 'No':$         v['answers']['Q4A'] = 'n/a'
pd.DataFrame(d, index=['d', 'b', 'a']) # uses d, not df, as data input
country_dummy = pd.get_dummies(df_new['country']) ## creating dummy$ df_new = df_new.join(country_dummy) # joining dummy column to dataframe
crime_df.info()
organize_data.to_csv("NewsAccountData.csv")
np.exp(result_countries_new.params)
df['gathering'].fillna(0, inplace = True)
grpConfidence.count()
clean_merge_df = merge_table_df.drop(["Number One_x"], axis=1)$ clean_merge_df.rename(columns={'Number One_y': 'Reached Number One'}, inplace=True)
twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')
end_date = pd.Series(pd.to_datetime(end_date).strftime('%Y-%m'),index=churned_ix)
def avg_forest_area(forest_areas):$     a = [25, 75, 125, 150]$     return np.dot(a, forest_areas)
psy_df3 = QLESQ.merge(psy_df2, on='subjectkey', how='right') # I want to keep all Ss from psy_df
df2 = df.groupby('filename_new')
g = data_s[data_s['isvalid']>0].groupby('customer').apply(lambda x: x['isvalid'].sum()).reset_index()
sql_query = "SELECT * FROM events WHERE actor_id= %s"$ df = pd.read_sql_query(sql_query, session.bind, params=[userid])$ df
df_ad_airings_4.dtypes
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))
from scrapy.settings import Settings$ from scrapy.crawler import CrawlerProcess, Crawler$ from scrapy import signals
df = df[(df['state'] == 'successful') | (df['state'] == 'failed')]$ df['state'] = df['state'].replace({"failed":0,'successful':1})$ df['spotlight'] = df['spotlight'].replace({False:0,True:1})
import sys$ sys.path.insert(0, '..')
import numpy as np$ ok.grade('q03')
url = 'https://space-facts.com/mars/'
weather.loc[weather.max_gust_speed_mph.isnull(), 'max_gust_speed_mph'] = weather.groupby('max_wind_Speed_mph').max_gust_speed_mph.apply(lambda x: x.fillna(x.median()))
train_set.date = pd.to_datetime(train_set.date,format='%Y%m%d')
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])$ df_clean['date'] = df_clean['timestamp'].apply(lambda time: time.strftime('%m-%d-%Y'))$ df_clean['time'] = df_clean['timestamp'].apply(lambda time: time.strftime('%H:%M'))
save_model('model_gbdt_v1.mod', gbdt_grid)    
import datetime $ now = datetime.datetime.now()$ print now
gr_e1 = df.query("group == 'treatment' and landing_page == 'old_page'")
data_df['clean_desc'] = data_df.apply(text_clean, axis=1)$ data_df['clean_desc'] = data_df['clean_desc'].astype(unicode)$ data_df['nwords'] = data_df['clean_desc'].str.count(' ').add(1)
df2 = df2.drop_duplicates(['user_id'])$ df2.query('user_id == 773192')
xml_in[xml_in['venueName'].isnull()].count()
turnstiles_df = turnstiles_df.drop(["EXITS", "DESC"], axis=1, errors="ignore")
max_fwing = df.loc[df.following.idxmax()] $ name = max_fwing['name'] if max_fwing['name'] is not None else max_fwing['login']$
df.info()
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','country_UK','country_US']])$ r=lm.fit()
mlab_uri = os.environ['MLAB_URI']$ mlab_collection = os.environ['MLAB_COLLECTION']
new_fan = questions.loc[questions['years_attend'] == 0]$ return_fan = questions.loc[questions['years_attend'] != 0]
old_page_converted = np.random.binomial(1, cr_under_null, size=n_old)$ print(len(old_page_converted))$ old_page_converted
xgb_grid.fit(X_train, y_train)$
ix = ((iris.Species == "setosa") | (iris.Species == "versicolor")) & (iris["Sepal.Length"]>6.5)$ iris.loc[ix.values,iris.columns[:2]]
y = tweets['handle'].map(lambda x: 1 if x == 'Donald J. Trump' else 0).values$ print(np.mean(y))
folium.GeoJson(watershed).add_to(m);
google_stock['Adj Close'].describe()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv"$ df = pd.read_table(path, sep =',', na_values=[' ?'])$ df.head(5)
print(np.shape(b))$ print(np.size(b))$ print(np.ndim(b))
calls_nocontact.zip_code.value_counts()
df.sort_values('Year',ascending=True).head()
for col in X_numcols:    $     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
BuyingData = pd.read_excel('/Users/itsjustme/Desktop/BuyingBehaviourTwoYrs.xlsx')
materials_file = openmc.Materials([fuel, water, zircaloy])$ materials_file.export_to_xml()
tweets_df = pd.read_csv('C:\\Users\\ericr\\Google Drive\\schoolwork\\current classes\\CS 230\\Project\\230_crypto\\data\\twitter\\hourly\\labeled_tweets_hourly.csv', encoding='ISO-8859-1')$ print("There are {} tweets".format(tweets_df.shape[0]))$ tweets_df.head()
dci = indices(dcaggr, 'text', 'YearWeek')$ dci.head()
df['lead_mgr_counts'] = df.groupby(['lead_mgr'])['lead_mgr'].transform('count')$
price2017 = price2017[['Date', 'Time', 'Germany/Austria/Luxembourg[Euro/MWh]']]$ price2017.columns = ['Date', 'Time', 'DE-AT-LUX']$ price2017.head()
page.text
prcp_year_df['date'] = pd.to_datetime(prcp_year_df['date'])
np.exp(results.params)
!pip install --upgrade tensorflow==1.4
sns_plot = sns.lmplot(x='score',y='retweet_count',data=rating_and_retweet, fit_reg=False,scatter_kws={'alpha':0.05})$ sns_plot.savefig("score_vs_retweet.jpg")
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(20))
train_set.to_csv(PROCESSED_DATA_DIR + '/train_set.csv', index=False)$ test_set.to_csv(PROCESSED_DATA_DIR + '/test_set.csv', index=False)
transactions_price_plan_days['transaction_date']  = transactions_price_plan_days.transaction_date.apply(lambda x: datetime.strptime(str(x), '%Y-%m-%d'))$
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).filter(Measurement.station == "USC00519281").all()
crime_data = crime_data[crime_data["DATE_TIME"].dt.year >= 2016]$ print(crime_data["DATE_TIME"].dt.year.value_counts())$ print(crime_data["BORO_NM"].value_counts())
countries.info()
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
day_later_timestamps = len(timestamp_left_df[timestamp_left_df['time_delta_seconds'] < -86400])$ print("There are %d objects in which the 'timestamp' are 1 day later than events were created." % day_later_timestamps)
pd.set_option("display.max_rows", 20)$ np.random.seed(12)
import os$ print(os.listdir())
sentiments_pd.to_excel("NewsMood.xlsx", encoding="latin-1") 
date_df = pd.DataFrame(normals, columns=('tmin','tavg','tmax'))$ date_df['date'] = trip_dates$ date_df.set_index('date')
results.summary()
def split_data(x,y):$     x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, shuffle=False)$     return x_train, x_test, y_train, y_test
year_prcp_df.describe()
df2 = df.drop(df.columns[non_null_counts < 1000], axis=1) #by default df.drop will take out rows, axis = 0
data['Traded Volume'].mean()
mgxs_lib.load_from_statepoint(sp)
y = api.GetUserTimeline(screen_name="berniesanders", count=20, max_id=935706980643147777, include_rts=False)$ y = [_.AsDict() for _ in y]
df.brewery_name.value_counts().head(5)
autos["price"].sort_values(ascending=False).head(50)
qends = pd.date_range('2014-01-01','2014-12-31',freq='BQS-JUN')$ qends.values
grad_age_mean = records3[records3['Graduated'] == 'Yes']['Age'].mean()$ grad_age_mean
deployment_details = client.deployments.create(model_uid, "My NLC Car model deployment")
feedbacks_stress.describe()$
pd.merge(df1, df3, left_on='employee', right_on='name' )
tlen.plot(figsize = (16,4), color = 'r')
autos["vehicle_type"].unique()$
results.summary()
print(pandas_list_2d.iloc[[0]])
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
print(data_tokenized.shape)$ print(label.shape)$ searchParameters(data_tokenized, np.ravel(label), MLPClassifier(max_iter=500, tol=0.00001), container, text='mlp')
df_numpy = df.value.as_matrix().reshape(-1,1)$ print(df_numpy[:10])
df.isnull().values.any()
file = open('tweets/tweets_Trump.csv', 'w')$ for tweet in results:$     file.write("%s\n" % tweet)
validation.analysis(observation_data, distributed_simulation)
df['text_extended'] = df.index1[898:1061].apply(tweet_extend)$
filepath = os.path.join('input', 'input_plant-list_SE.csv')$ data_SE = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_SE.head()
spyder_etf = get_pricing(dwld_key, start_date.strftime(date_fmt))$ spyder_etf.name = dwld_key + ' ETF Index'$ s_etf = (spyder_etf.pct_change() + 1).cumprod()
sns.boxplot(calls_df["length_in_sec"],orient='v')
topics = lda.get_term_topics('network')$ for t in topics:$     print(t)
json=request.json()
df = df[df.user_location_country.notnull() & df.user_location_region.notnull()& df.srch_ci.notnull() & df.srch_co.notnull()]
def get_api_key():$     return os.environ['PL_API_KEY']$ assert get_api_key(), "PL_API_KEY not defined."
print("The probability that an individual received the new page is: {}".format((df2['landing_page'] == 'new_page').mean()))$
df2.query('converted == 1').count()[0]/df2.count()[0]
mapInfo_string = str(serc_mapInfo.value) #convert to string$ mapInfo_string.split?
s.plot(figsize=(12,10), label="Actual")$ s_median.plot(label="Median")$ plt.legend();
X = pd.get_dummies(X, columns=['subreddit'], drop_first = True)
trains_fe2_y= trains_fe1[['reordered']]$ trains_fe2_y.head()
LARGE_GRID.display_fixations(raw_large_grid_df, option='fixations')
dr_new_8_to_16wk_arimax = dr_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']]$ dr_new_8_to_16wk_arimax.index = dr_new_8_to_16wk_arimax.index.date
pd.date_range('3/7/2012 12:56:31', periods=6, normalize=True)
df_joined.country.value_counts()
model.wv.most_similar('cost_function', topn=10)
df['production_companies'].head(15)
prcp_df = pd.DataFrame(prcp_query).set_index('date')$ prcp_df.head()$ prcp_df_flat = pd.DataFrame(prcp_query)
df_all_wells_wKNN_DEPTHtoDEPT.info()
closes.plot(figsize=(8,6));
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ data.head(10)
session.query(Measurements.station, func.count(Measurements.date)) \$     .group_by(Measurements.station).order_by(func.count(Measurements.date).desc()).all()
cm = our_nb_classifier.conf_matrix(our_nb_classifier.data_results['out_y'],$                             our_nb_classifier.data_results['out_n'])$ print(ascii_confusion_matrix(cm))
for temp_col in temp_columns:$     dat[temp_col]=LVL1.hampel(dat[temp_col], k=7) #remove outliers with hampel filter$     dat[temp_col]=LVL1.basic_median_outlier_strip(dat[temp_col], k=8, threshold=4, min_n_for_val=3) #remove remaining outliers; spikes where val exceeds 2-hr rolling median by 4C    
temp_df = temp_df.join(metadata.set_index('order_num'), how='left', on='order_num')$ temp_df['year'] = temp_df.date.dt.year
archive_clean.name.value_counts()
df_twitter_extract_copy = df_twitter_extract_copy.drop('Unnamed: 0', axis = 1)
current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')$ path = 'Tweet_Frame_'+current_time$ print(path)
np.random.seed(42)$ new_page_converted=np.random.binomial(n=1,p=P_new,size=n_new)$ (new_page_converted)
def predict_output(feature_matrix, weights):$     predictions = np.dot(feature_matrix, weights)$     return(predictions)
discounts_table.groupby(['Discount Band'], sort=False)['Discount %'].max()
df.loc[:, topics[0]:topics[-1]] = df.apply(lambda x: \$                                            pd.Series([t in x['tags'] for t in topics], index=topics), axis=1)
tzs = DataSet['userTimezone'].value_counts()[:10]$ print(tzs)
merged_df = pd.merge(left=surveys_df, right=species_df, how='left', on='species_id')
target = users["country_destination"].values$ features_one = users[["timestamp_first_active", "signup_flow", "age"]].values
autos["odometer_km"].unique().shape
n_user_days.value_counts().sort_index()
df.groupby('group').mean()['converted']
hdf = df.set_index('AgeBins')$ hdf.loc['child', :].head()
cur.execute(query)$ cur.fetchall()
data.sample(4)
cursor_sampl = collection_reference.aggregate($     [{'$sample': {'size': 20}}]$ )
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'],unit='s')$ git_log.describe()
year_with_most_commits = commits_per_year.sort_values(by='author', ascending=False).head(1) $ year_with_most_commits = 2017
min_time_spent = fraud_data_updated.time_spent.min()$ max_time_spent = fraud_data_updated.time_spent.max()
df2[df2['user_id'].duplicated()]['user_id']
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Pothole') ]['Day of the week'].value_counts().head()$
df2.apply(lambda x: len(x.unique()))
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
data = res.json()
print('input dim = {}'.format(input_image.ndim))$ print('input shape = {}'.format(input_image.shape))
len(pd.unique(df2['user_id']))$
load2017.columns = ['time', 'day-ahead', 'actual'] $ load2017.head()
n_new = df2.query('landing_page == "new_page"').shape[0]$ n_new
len(df['user_id'].unique())
print(model.summary())
l=[(x[6]) for x in json_dict]$ avg_vol = sum(l)/len(l)$ print('Average volume for all 2017 is {:.2f}'.format(avg_vol))$
lst = data_after_subset_filter.SEA_AREA_NAME.unique()$ print('Waterbodies in subset:\n{}'.format('\n'.join(lst)))
from sklearn.linear_model import LogisticRegression$ logreg = LogisticRegression()$ print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())
lda_tfidf = LatentDirichletAllocation(n_topics=10, random_state=0)$ lda_tfidf.fit(X)
df2.set_index('user_id', inplace = True)$ df_c.set_index('user_id', inplace = True)$ df2 = df2.join(df_c)
!rm SIGHTINGS.csv -f$ !wget https://www.quandl.com/api/v3/datasets/WIKI/IBM.csv
from bmtk.analyzer import nodes_table$ input_nodes_DF = nodes_table(nodes_file, 'FridayHarborBiophysics')$ input_nodes_DF[:5]
psy = pd.get_dummies(psy, columns = cat_vars, drop_first=False)
df2[['ab_page', 'intercept']] = pd.get_dummies(df2['group'])$ df2['intercept'] = 1$ df2.head()
eve_treat =df_eve.query("ab_page==1").converted.mean()$ eve_treat
cityID = 'a592bd6ceb1319f7'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Diego.append(tweet) 
from sqlalchemy.orm import Session$ session = Session(engine)
X_train, X_test, y_train, y_test = train_test_split(features,classification_open,test_size=0.2)
testing.to_csv('SHARE_cleaned_lists.csv', index=False)
movies[movies['homepage'].isnull()]
autos["price"].sort_values(ascending=True)[2000] # 150$ autos.loc[autos["price"] < 150,:]
save_model('model_xgb_v1.mod', xgb_grid) 
tdelta = pd.to_datetime(loctweetdf['created_at'].max()) - pd.to_datetime(loctweetdf['created_at'].min())$ tdelta = tdelta.days$ tdelta
afx_x_2017 = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY)
from pyspark.sql import Row$ RowedRDD = RandomOneRDD.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2]))$ print RowedRDD.collect()                                               
tokens['five_star_ratio'] = tokens.five_star / tokens.one_star
autos['price'].unique().shape[0]
pro_result['MY'] = pro_result['Month Name'] + pro_result['Year'].astype(str)$ pro_result
kochdf.to_csv('exports/trend_data.csv')
canonical_name_to_other_names_dict[$     'test/core-tests/test-init.js'] = ['test/core-tests/test-init.js', 'test/core/both/test-init.js']
len(pd.unique(ratings['movieId'].ravel()))
display(flightv1_1.select('timeline_leg1.getItem(0)').show(100, truncate=False))$
t1.head(5)$
feature_set = layer.query(out_sr={'wkid': 4326})$
tweet_list = api.search(q='#%23mallorca')
prcpDF.describe()
prcp_df.describe()
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['tweets'] ])$ display(data.head(5))
S_data = session.query(Station).first()$ S_data.__dict__
rfc_features = sorted(list(zip(test_features, rfc.feature_importances_)), key=lambda x: x[1], reverse=True)$ rfc_features
countries = pd.read_csv('countries.csv')$ countries.head()
import numpy as np$ ok.grade('q06')
import psycopg2 as pg2$ conn = pg2.connect(dbname='ChicagoViolencePredictor', user='postgres', host='localhost')
req = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY)$ json_data = req.json()
Base.classes.keys()
tweet_ids_twitter_archive = twitter_archive['tweet_id'].unique()$ tweet_ids_image_predictions = image_predictions['tweet_id'].unique()
df4 = df.drop('Cabin', axis=1) \$     .loc[lambda x: pd.notnull(x['Embarked'])] \$     .fillna(30)
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.tail()
print(np.shape(a))$ print(np.size(a))
df2_unique = df2.user_id.nunique()$ df2_unique
closes_csv = pathlib.Path('../datasets/daily_closes.csv').absolute()$ closes = pd.read_csv(str(closes_csv), index_col=0, parse_dates=True)
sumTable = tips.groupby(["sex","day"]).mean()$ sumTable
Total_Number_of_Rides_min = rides_analysis["Total Number of Rides"].min()$ Total_Number_of_Rides_min
ma_mov_idx = ma.array(mov_ids, mask = mov_vec)$ mov_idx = ma_mov_idx[~ma_mov_idx.mask]        
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2014-01-01&end_date=2014-01-02&api_key=S9bQCgmEaGHM8HSDLBNo')$ r.status_code
perc = lambda x: x/x.max() * 100$ perc_df = cryptos.set_index('name')[['24h_volume_usd', 'market_cap_usd', 'price_btc']].apply(perc)$ perc_df.head()
session.query(Stations.station,Stations.name,Stations.latitude, Stations.longitude,Stations.elevation).first()$
merge[merge.columns[16]].value_counts().sort$
consumer_info = [line.strip() for line in open("df_twitter_consumer_auth.txt")]$ access_info = [line.strip() for line in open("df_twitter_access_auth.txt")]$
rural_avg_fare = rural_type_df.groupby(["city"]).mean()["fare"]$ rural_avg_fare.head()$
df_loc = df_location[['country','name','state']]$ df_loc = df_loc.rename(columns={'country':'loc_country','name':'loc_name','state':'loc_state'})$
df_defects = pd.read_excel('ncr_data.xlsx', index='Notification')$ df_defects
twitter_archive.loc[(twitter_archive['name'].str.islower()) & (twitter_archive['text'].str.contains('name is'))]
tweet_scores = tweet_scores[['id', 'retweet_count','favorite_count']]$ tweet_scores.head()
weath_summ_df = weather_file_df[["PRCP", "SNOW", "TAVG", "WkStart"]]$ weath_summ_df.head()
print("The number of rows in the dataset is " + str(df.shape[0]))
print("Converted users proportion is {}%".format((df['converted'].mean())*100))
cityID = '095534ad3107e0e6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Louisville.append(tweet) 
print(f'dataframe shape: {match.shape}')$ match.head(3)
from scipy.stats import norm$ norm.cdf(z_score)
deltadf.to_csv('exports/trend_deltas_chefkoch.csv')
output_path = "/Users/kai.bernardini/Documents/MA575/stock_data/"
active = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\$                order_by(func.count(Measurement.tobs).desc()).all()$ print(active)
for tweet in query2:$     if replies_blm.get(tweet.in_reply_to_status_id_str) != None:$
pd.DataFrame(pairing_dict_names)
with open('hashtags/hashtags.csv', 'w') as f:$     [f.write('{0},{1}\n'.format(tag, val)) for tag, val in tag_cloud.items()]
predictions = dtModel.transform(testData)
forecast_set = clf.predict(X_lately)$ len(forecast_set)
for col in response_df.columns:$     if response_df[col].dtype in (object, str):$         print col, pd.np.where(response_df[col] == '', 1, 0).sum()
filtered_styles = df.groupby('simple_style').filter(lambda x: x.simple_style.value_counts() >= 5)$ style_bw = filtered_styles.groupby('simple_style').rating_score.mean().sort_values(ascending=False)
ca_all_path = cwd + '\\LeadGen\\Ad hoc\\SubID\\CA_SubID_from_python.csv'$ ca_all.to_csv(ca_all_path, index=False)
df.shape[0]
cityID = '18810aa5b43e76c7'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Dallas.append(tweet) 
Stations = Base.classes.stations$ Measurements = Base.classes.measurements
my_df_free1.iloc[100:110,0:3]
bufferdf.Fare_amount[(bufferdf.Fare_amount==2) | (bufferdf.Fare_amount==3) | (bufferdf.Fare_amount==4)].apply(int).size
df = pd.read_csv('/home/bmcfee/data/vggish-likelihoods-a226b3-maxagg10.csv.gz', nrows=1000000, index_col=0)
means = pd.DataFrame(means2_table)$ means
train.drop(['White','raceeth'],axis=1,inplace=True)$ test.drop(['White','raceeth'],axis=1,inplace=True)
df.plot();
!rm world_bank.json.gz -f$ !wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz
df2.loc['2018-01-18', 'high_tempf']
cercanasA1_11_14Entre125Y150mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 125) & (cercanasA1_11_14['surface_total_in_m2'] < 150)]$ cercanasA1_11_14Entre125Y150mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre125Y150mts.apply(descripcionDistancia, axis = 1)$ cercanasA1_11_14Entre125Y150mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
dts = list(map(pd.to_datetime, observation_dates))$ dts
ppp=  pd.Series(list(x))-pd.Series(list(z))$ plt.hist(ppp, bins=30, histtype="stepfilled")$
train['default'] = np.where(train.loan_status == 'Charged Off', 1, 0)
sample = df.sample(n=1000, replace=False)
df['StatusDate'] = pd.to_datetime(df['StatusDate'], format='%m/%d/%Y %I:%M:%S %p +0000') $ df['ResolutionTime'] = (df['StatusDate']-df.index).astype('timedelta64[h]') / 24
df = pd.read_sql('SELECT * from room', con=conn_b)$ df.head(10) # show 10 rows only
x = tags['Count'][0:20]$ y = tags['TagName'][0:20]$ sns.barplot(x, y, color = 'g')
df = pd.read_json("data\8oct_pre_processed_stemmed.json", orient='records', lines=True)
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
words = [w for w in words if not w in stopwords.words("english")]$ print(words)
print(type(coin_data.index))$ coin_data.columns
old_page_converted = np.random.binomial(1, p_old, n_old)$ old_page_converted[:5]
precip_df.describe()
hourly_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner')$ hourly_df.reset_index(inplace=True)$ hourly_df.head()
prcp = session.query(prcp.date, prcp.prcp).\$     filter(prcp.date > first_date).\$     order_by(prcp.date).all()
my_list = [0.25, 0.5, 0.75, 1.0]$ data = pd.Series(my_list, index=[1,2,3,4])$ data
import statsmodels.api as sm$ convert_old = df2[df2['group'] == 'control']['converted'].sum()$ convert_new = df2[df2['group'] == 'treatment']['converted'].sum()
contextFreeList = [i/10 for i in range(1,3,step = 0.5)]$ for i in contextFreeList:$     print(i,tuning(w_contextFree = i))
df = pd.read_csv("https://s3.amazonaws.com/tripdata/JC-201707-citibike-tripdata.csv.zip")
df_eng7days = df_eng.groupby(['user_id', pd.TimeGrouper(freq='7D')]).sum()$ df_eng7days
correlations = [ df['overall_rating'].corr(df[f]) for f in cols ]$ type(correlations)
fit_time = (end_fit - start_fit)$ print(fit_time/60.0)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY$ r = requests.get(url)
dfs_resample.reset_index(inplace = True)$ dfs_resample['TIME'] = pd.to_datetime(dfs_resample['DATE_TIME']).dt.hour$ dfs_morning = dfs_resample[(dfs_resample['TIME'] == 6) | (dfs_resample['TIME'] == 12)]
file = 'boulder_weather.csv'$ weather = pd.read_csv(file)
taxiData2.loc[taxiData2.Tip_amount < 0, "Tip_amount"] = 0
store_items = store_items.drop(['store 2', 'store 1'], axis = 0)$ store_items
df['timestamp'] = pd.to_datetime(df['timestamp'])$ df['EST'] = df['timestamp'] - pd.Timedelta(hours=5) #Convert to EST
sns.set_style('whitegrid')$ sns.distplot(data_final['countCollaborators'], kde=False,color="red")#, bins=20)$
previousYearTemps = calc_temps('2011-02-28','2011-03-05')$ previousYearTemps.minimum, previousYearTemps.average, previousYearTemps.maximum
table = pd.read_html(url_mars_facts)$ table[0]
random.seed(1234)$ new_page_converted = np.random.choice([1, 0], size = n_new, p = [p_mean, (1-p_mean)])
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
df['raw'].str.contains('....-..-..', regex=True)
import numpy as np$ x = np.array([2, 3, 5, 7, 11, 13])$ x * 2
active_station = session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all()$ active_station
df.iloc[[11,24,37]]
last_year = dt.date(2017, 6, 2) - dt.timedelta(days=365)$ print(last_year)
commits_per_year = corrected_log.resample('AS',on='timestamp')['author'].agg('count')$ print(commits_per_year.head())
df_station = pd.DataFrame(list(station_zipcode.items()), columns = ['station', 'zipcode'])$ df_station['zipcheck']=df_station.zipcode.apply(lambda x:len(x))$ df_station[df_station['zipcheck']!=5]$
minimum = df['time_open'].min()$ print(minimum)$
no_lineup = df[((df.group =='treatment') & (df.landing_page == 'old_page') | (df.group == 'control') & (df.landing_page == 'new_page'))].shape[0]$ print(str(no_lineup) + ' times the new_page and treatment didnt line up')
(gamma_chart - gamma_chart.shift(1))['Risk'].plot()
messages = pd.read_csv('message_read.csv')
fig, ax = plt.subplots(1, figsize=(12,4))$ plot_with_moving_average(ax, 'Seasonal AVG Therapists', therapist_duration, window=52)
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)$
df_new[['CA','US','UK']] = pd.get_dummies(df_new['country'])[['CA','US','UK']]
logit2 = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']])$ result = logit2.fit()$ result.summary2()
sum(df['converted'].values)/row_num
coarse_groups = openmc.mgxs.EnergyGroups(group_edges=[0., 20.0e6])$ coarse_mgxs_lib = mgxs_lib.get_condensed_library(coarse_groups)
mismatch_treatment_to_old_pg = df.query("group=='treatment' & landing_page =='old_page'")$ mismatch_control_to_new_page = df.query("group=='control' & landing_page =='new_page'")
dataset.describe(include=['object'])
result = api.search(q='%23Australia')  # "%23" == "#"$ len(result)
age_difference = jail_census.loc['2017-02-01']['Current Age'] - jail_census.loc['2017-02-01']['Age at Booking']$ age_difference.value_counts()
tipsDF.shape
df[df.Predicted == 5]
retweets=twitter_archive[~twitter_archive.retweeted_status_id.isnull()].tweet_id.values$
Base.classes.keys()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ station = Base.classes.station
taxiData.shape
all = pd.Series(list(df_twitter_enhanced)+list(df_image_predictions)+list(df_tweet_api))$ all[all.duplicated()]
session.query(Measurement.station).group_by(Measurement.station).count()$
transactions.merge(transactions,on="UserID")
xml_in.dtypes
df.loc[mask,:]
data.describe()
null_valls = np.random.normal(0,p_diffs.std(),p_diffs.size)$ plt.hist(null_valls);$ plt.axvline(x=actual_obs_diff, color = 'g', linewidth = 1);
n_new = df2.query('landing_page == "new_page"')['user_id'].count()$ print(n_new)
sample_size_old_page = df2.query('landing_page == "old_page"').shape[0]$ print('Sample size old_page: {}'.format(sample_size_old_page)) 
normals_df = pd.DataFrame(normals, columns=["Date","Min Temp","Avg Temp","Max Temp"])$ normals_df = normals_df.set_index('Date')$ normals_df
kick_projects['launched_date'] = pd.to_datetime(kick_projects['launched'], format='%Y-%m-%d %H:%M:%S')$ kick_projects['deadline_date'] = pd.to_datetime(kick_projects['deadline'], format='%Y-%m-%d %H:%M:%S')
last_12_precip = session.query(Measurement.date, Measurement.prcp).\$ filter(Measurement.date >= '2016-08-24').filter(Measurement.date <= '2017-08-23').order_by(Measurement.date).all()
df_input.toPandas().info()
X_train = sparse.hstack([train[simple_features], X_train_feature_counts]).tocsr()$ X_test = sparse.hstack([test[simple_features], X_test_feature_counts]).tocsr()$
convert_rate_p_new = df2.converted.mean()$ print('Convert rate of p_new under the null is:{}'.format(convert_rate_p_new))
url = 'https://www.dropbox.com/s/yz5biypudzjpc12/PSI_tweets.txt?dl=1'$ location = './' #relative location for Linux, saves it in the same folder as this script$ download_file(url, location)
master_file['TECTONIC SETTING'].value_counts()
logit_countries = sm.Logit(df3['converted'], $                            df3[['country_UK', 'country_US', 'intercept']])$ result1 = logit_countries.fit()
xyz = json.dumps(youtube_urls, separators=(',', ':'))$ with open('youtube_urls.json', 'w') as fp:$     fp.write(xyz)$
countries_df['country'].unique()
events['payload'][events['type'] == 'ForkEvent'].iloc[0]
tip_sample.to_csv('text_prepartion/yelp_tips_prepared.csv')
df["qty"].sum()
pold=df2['converted'].mean()$ print(pold)
!jupyter nbconvert index.ipynb --to html$ bucket.upload_file('index.html', 'index.html')$ bucket.upload_file('index.ipynb', 'index.ipynb')
df_user_info = df_user_info[df_user_info['activated']==1]$ df_user_info['marketing_source'] = \$     df_user_info['marketing_source'].loc[:].fillna("unknown")
df_converted = df[df['converted'] == 1]$ df_converted.shape[0] / df['user_id'].nunique()$
df_members['bd_c'] = pd.cut( df_members['bd'] , age_bins, labels=age_groups)
pokemon['Legendary'] = np.where(pokemon['Legendary'] == True, 1, 0)$
data = pd.merge(irl, billstargs, left_on='bill_id', right_on='bill_id')$ data.drop_duplicates(['bill_id'], keep='last')$
df.shape[0]
summaries = "".join(df.title)$ ngrams_summaries = cvec_1.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
model = ARIMA(AAPL_array, (2,2,1)).fit()$
labeled_news[0].describe()
merged_data['tax'].fillna(0, inplace=True)
print("Converted users proportion is {}%".format((df['converted'].mean())*100))
from config import config$ CONFIGS = config.Config.get(env='prod', caller_info=False)
df2['user_id'].nunique()
df_5['weeks_between'] = df_5.submitted_at - df_5.application_created_at$ df_5['weeks_between'] = np.ceil(df_5.weeks_between.dt.days/7)
print('if we spend 50k on TV ads, we predict to sell:', round(lm.predict(X_new)[0],2), ' thousand units')
old_page_converted = np.random.choice([0, 1], p=[(1-p_old), p_old], size=n_old)
companies = pd.read_csv("companies.csv")$ companies[companies['trc_industry_group'] == "Investment Banking & Investment Services"]
cityID = '5d231ed8656fcf5a'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Petersburg.append(tweet) 
plt.hist(p_diffs)$ plt.axvline(x=diffs, color= 'red');
train_data[train_data['totals.transactionRevenue'].notnull()].groupby(['device.browser']).agg({'visitNumber': 'count'}).reset_index().set_index("device.browser",drop=True).plot.bar()
cell = openmc.Cell(cell_id=1, name='cell')$ cell.region = +min_x & -max_x & +min_y & -max_y$ cell.fill = inf_medium
data.values[0]
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,200], ylim=[0,200], title="Favorites and Retweets:200x200")
%matplotlib inline$ cat_group_counts = df.groupby("category").size().sort_values(ascending=False)[:10]$ cat_group_counts.plot(kind="bar", title="Top 10 Meetup Group Categories")
stationCount = func.count(Measurement.station)$ session.query(Measurement.station, stationCount).group_by(Measurement.station).order_by(stationCount.desc()).all()$
df.set_index('Parameter', inplace=True)$ df.head()
yc200902_short = yc[::1000]$ yc200902_short.shape
heading_row = ['date', 'time','ml-fav', 'ml-dog','rl-fav', 'rl-dog','total-over', 'total-under',$                '1st-half-fav', '1st-half-dog','2nd-half-fav', '2nd-half-dog','filename']$ df.columns = heading_row
df.printSchema()
data.Likes.value_counts(normalize=True).head().plot(kind='bar')
result_merged.summary2()
gmm = GaussianMixture(2).fit(X)$ labels = gmm.predict(X)
pd.Series(pd.DatetimeIndex(pivoted.T[labels==1].index).strftime('%a')).value_counts().plot(kind='bar');
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=", auth=('', ''))$
numPurchP = train.groupby(by='Product_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesP'})$ train = train.merge(numPurchP, on='Product_ID', how='left')$ test = test.merge(numPurchP, on= 'Product_ID', how='left')
x.loc[x.loc[:,"A"]>0.6,"A"]
test['visitors'] = 0.2*preds1+0.2*preds2+0.3*preds3+0.1*preds4+0.2*preds5$ test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)$ sub1 = test[['id','visitors']].copy()
drop_num.append(19)
preds = aml.leader.predict(test)
filename = processed_dir+'pulledTweetsCleanedLemmaEmEnc_df'$ gu.pickle_obj(filename,pulledTweets_df)
pd.Series({2:'a', 1:'b', 3:'c'})
created_table=pd.merge(new_table,y_axis_value)$ created_table.head()
first_row=session.query(meas).first()$ first_row.__dict__
cust_data.sort_values(by='MonthlyIncome', ascending=False).head(10)$
candidate_data['messages'] = data.groupby('from_name')['message'].apply(' '.join)
df2.shape
perf_train = pd.read_csv('performance_train.csv')$ print(' data shape: ', perf_train.shape)$ perf_train.head()
profits_table = data_df[['ID','Segment','Country','Product','Profit']].copy()$ profits_table.head()
[tweet for tweet in df_clusters[df_clusters.cluster_cat==15].text[:10]]
my_stream.filter(track=['data'])
print(chunker.evaluate(valid_trees))
input_shape_param = (X_train.shape[1],X_train.shape[2])$ input_shape_param
user_table = events[['user_id', 'login']].drop_duplicates()$ user_table.to_sql(name='users', con=con, if_exists='append', index=False)
grouped = df2.groupby('group')$ grouped.describe()
(training,testing) = clean_data.randomSplit([0.7,0.3])
print(nc_file)$ nc_file.close(); print('Dataset is closed!')
plt.hist(p_diffs);$ plt.axvline(diff, c='red');
p_new = (df2.query('converted == 1')['user_id'].nunique())/(df2.user_id.nunique())$ p_new
%%time$ nodeInDegreeDict = network_friends.in_degree()$ nodeOutDegreeDict = network_friends.out_degree()
log_mod_int = sm.Logit(df_new['converted'],df_new[['intercept','US','UK','ab_page','US_ab_page','UK_ab_page']])$ results_int = log_mod_int.fit()$ results_int.summary()
pgh_311_data = pd.read_csv("https://data.wprdc.org/datastore/dump/40776043-ad00-40f5-9dc8-1fde865ff571")$ pgh_311_data.head()
import pandas as pd$ git_log = pd.read_csv('datasets//git_log.gz', sep='#', encoding='latin1', header=None,  names=["timestamp", "author"], compression='infer')$ git_log.head(5)
df_new['intercept'] = 1$ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
count = grouped_months.count()$ count=count.rename(columns = {'Totals':'count_v_T'})$
df_session_dummies = pd.get_dummies(df, columns=['action'])$
sn.boxplot(data = monthly_gain_summary, palette="Set2")
duplicated_user = df2[df2.duplicated(['user_id'], keep=False)]['user_id']$ duplicated_user
autos["price"] = autos["price"].astype(int)$ autos["odometer"] = autos["odometer"].astype(int)
print(re.match('AA', 'AAbc'))$ print(re.match('AA', 'bcAA'))
tfav.plot(figsize=(16,4), label="Likes", legend=True)$ tret.plot(figsize=(16,4), label="Retweets", legend=True);
url_yr2017='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=MY_API_KEY&start_date=2017-01-01&end_date=2017-12-31'$ response=requests.get(url_yr2017)
Pnew = df2.converted.mean()$ Pnew
titanic_clean = preprocessor.basic_cleaning()
cats_df = cats_df.set_index(['Unnamed: 0'])$ cats_df.index.name = 'seq'$ cats_df['remove'] = False #Used to identify samples to be removed
test_tweet = api.user_timeline(newsOutlets[0])$ print(json.dumps(test_tweet[0], sort_keys=True, indent=4))
from zipline.pipeline import Pipeline$ def make_pipeline():$     return Pipeline()
flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')). \$     agg(F.mean('price_will_drop_num')).orderBy(['trip', 'stay_days', 'lead_time'], ascending=[1, 1, 0]).show(1000)$
df['intercept']=1$ df[['control', 'treatment']] = pd.get_dummies(df['group'])$
cercanasAfuerteApacheEntre125Y150mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 125) & (cercanasAfuerteApache['surface_total_in_m2'] < 150)]$ cercanasAfuerteApacheEntre125Y150mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre125Y150mts.apply(descripcionDistancia2, axis = 1)$ cercanasAfuerteApacheEntre125Y150mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
html = browser.html$ soup = BeautifulSoup(html, 'html.parser')
autos.rename({'odometer':'odometer_km'},axis=1,inplace=True)
plotdf.loc[:thisWeekHourly['hourNumber'].max(), :] = plotdf.loc[:thisWeekHourly['hourNumber'].max(), :].fillna(method='ffill')$ plotdf.loc[:thisWeekHourly['hourNumber'].max(), :] = plotdf.loc[:thisWeekHourly['hourNumber'].max(), :].fillna(0)
sumPre = dfPre['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])$ sumPost = dfPost['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])
df2_filtered = df2[df2["liker"].isin(top_likers)]
print(df['Confidence'].nunique())
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
(p_diffs>real_diff).mean()
scores_mean = np.mean(raw_scores)$ scores_std = np.std(raw_scores)$ print('The mean is {:.5} and the standard deviation is {:.5}.'.format(scores_mean, scores_std))
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,100], ylim=[0,100], title="Favorites and Retweets:100x100")
common_brands = brand_counts[brand_counts > .05].index$ common_brands
twitter_archive_enhanced_clean = twitter_archive_enhanced.copy()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-05-08&end_date=2018-05-08&api_key=' + API_KEY)$
index = pd.date_range('2018-3-1', periods=1000, freq='M')$ index 
raw_scores = df.rating_score[df.rating_score.notnull()]
results=session.query(Measurements.Station,Measurements.Date,Measurements.Tobs).\$         filter(Measurements.Date>=start_date, Measurements.Date<=end_date, Measurements.Station == station_mostobs).\$         order_by(Measurements.Date.desc()).all()
df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],$                     'supervisor': ['Carly', 'Guido', 'Steve']})$ df4
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'green', 'red', 'yellow'], grid=False)$
print(plan['plan']['itineraries'][0]['legs'][0].keys())
df3[df3['group'] == 'treatment'].head()
df_treatment = df.query("group == 'treatment'")$ df_treatment.converted.mean()
INQ2016.Create_Date.dt.month.value_counts().sort_index()
betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality$ nx.set_node_attributes(G, betweenness_dict, 'betweenness')$
site_visits_rebuild['VisitTime'] = pd.to_datetime(site_visits_rebuild['DateCreated'], format='%d%b%Y:%H:%M:%S.%f')$ site_visits_rebuild.set_index('VisitTime', inplace=True)
facilities.head()
df_test.groupby(['Gender'])['loan_status'].value_counts(normalize=True)
df_reg = df2.copy()
pbls = subs.problem_id.unique()$ print "%d resolved problems" % len(pbls)
client = MongoClient()$ db = client.sample_1m$ db.sample_1m.find_one()
spark = SparkSession.builder.appName('nyc311').getOrCreate()
df['state'] = df['raw'].str.extract('([A-Z]\w{0,})', expand=True)$ df['state']
test_pred_svm = lin_svc_clf.predict(test_cont_doc)
sig_sq = np.var(d_3)$ mu = np.mean(d_3)$ scipy.stats.norm.interval(0.95, loc=mu, scale=sig_sq)
archive_clean['name'].replace('None',np.nan,inplace= True)
one_star_token_count = nb.feature_count_[0, :]$ five_star_token_count = nb.feature_count_[1, :]
print("Age:-",result_set.age,"workclass:-",result_set.workclass)
from statsmodels.tsa.arima_model import ARIMA$ model_6203 = ARIMA(dta_6204, (8, 1, 1)).fit() $ model_6203.forecast(5)[:1] 
list = ['a','b','c','d','e']$ list.insert(1,'x')$ print(list)
my_data.shape$
new_converted_simulation.mean() - old_converted_simulation.mean()
from nltk.corpus import conll2000$ from nltk import conlltags2tree, tree2conlltags$ train_labels[3]
import json$ list_of_issues_dict_data = [json.loads(line) for line in open('SPM587FA17issues.json')]
sum(df2['user_id'].duplicated())
df2.drop_duplicates(subset=['user_id'], inplace=True)$ print(df2[df2.duplicated(['user_id'])])
sr_top = sr.top(limit = 1000) # limit is 1000 -- won't return any more than that
master_df.name.nunique()
df.as_matrix() # not a matrix, however, just numpy array
words_hash_sp = [term for term in words_sp if term.startswith('#')]$ corpus_tweets_streamed_profile.append(('hashtags', len(words_hash_sp))) # update corpus comparison$ print('List and total number of hashtags: ', len(words_hash_sp)) #, set(terms_hash_stream))
data[data.name == 'New EP/Music Development'].head()
autos["registration_year"].value_counts(normalize=True).sort_index(ascending=True).head(10)
treehouse_expression_hugo_tpm = treehouse_expression.apply(np.exp2).subtract(1.0).add(0.001).apply(np.log2)
archive_df_clean['timestamp'] = archive_df_clean.timestamp.str[:19]$ archive_df_clean['timestamp'] = pd.to_datetime(archive_df_clean['timestamp'], format = "%Y-%m-%d %H:%M:%S")
print("Probability of control group converting:", $       df2[df2['group']=='control']['converted'].mean())
FREEVIEW.plot_number_of_fixations(raw_fix_count_df, option=None)
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid$ x = df30458.groupby('start_station_name').count()$ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
inspector = inspect(engine)$ inspector.get_table_names()$
image = soup.find('a',class_="button fancybox")['data-fancybox-href']
tmax_day_2018.dims
data.loc[data["Breed"]== "Mix", "Breed"] = 1$ data.loc[data["Breed"]!= 1, "Breed"] = 0$ print(data["Breed"].unique())
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative='larger')$ print('z-score: ', z_score)$ print('p-value: ', p_value)
cbg = cbg.loc[cbg['COUNTYFP'].isin(['061', '081', '085', '047', '005'])]
train['timestamp_first_active_to_date'] = pd.to_datetime(train['timestamp_first_active_to_date'])
plot_autocorrelation(series=dr_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr hours new patients'))$ plot_autocorrelation(series=dr_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr hours existing patients'))
print(Probit(y, X).fit().summary())
display(teamAttr.iloc[:3,:13])$ display(teamAttr.iloc[:3,13:25])
bg3 = bg2.drop(bg2.index[0]) # drop first row$ bg3
data_archie.loc[data_archie['user_id'].isnull()].head(5)$
utility_patents_subset_df['figure_density'] = utility_patents_subset_df['number-of-figures'] / utility_patents_subset_df['number-of-drawing-sheets']$ utility_patents_subset_df['figure_density'].describe()
from sklearn.ensemble import RandomForestClassifier$ rf = RandomForestClassifier(n_estimators = 10, random_state = 42)$ rf.fit(X_train,y_train)
import pandas as pd$ git_log = pd.read_csv("datasets/git_log.gz", sep='#', encoding='latin-1', header=None, names=['timestamp', 'author'], compression='gzip')$ git_log.head(5)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])$
x_normalized = intersections_irr[for_normalized_columns].values.astype(float)
yourstartdate=datetime.strptime(input('Enter End date in the format %Y-%m-%d'), '%Y-%m-%d')$ yourenddate=datetime.strptime(input('Enter End date in the format %Y-%m-%d'), '%Y-%m-%d')$ calc_temps(yourstartdate,yourenddate)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt"$ df = pd.read_csv(path, sep ='\s+', na_values=['.'])$ df.head(5)
df_twitter_copy['tweet_id'] = df_twitter_copy['tweet_id'].astype('str')
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY)$ r=requests.get(url)
Quandl_DF['Month'] = Quandl_DF['Date'].dt.month$ Quandl_DF['Year'] = Quandl_DF['Date'].dt.year$ Quandl_DF['WeekNo'] = Quandl_DF['Date'].dt.week
outdir = './output'$ if not os.path.exists(outdir):$     os.mkdir(outdir)
row_id_exclude = [2,5,8,12,15,23,56] # these row should be excluded from data frame$ excluded_rows_df = pd.DataFrame(data_set_1.loc[x] for x in data_set_1.index if x not in row_id_exclude ) #$ print (excluded_rows_df.head(10))
artistDF[locate("Aerosmith", "name") > 0].show(20)$ artistDF[artistDF.artistID==1000010].show()$ artistDF[artistDF.artistID==2082323].show()$
db = client.Mars_db$ collection = db.mars_news
date + np.arange(12)
df2 = df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')")$ df2.shape[0]
df = hpg[hpg["air_store_id"].notnull() == True]$ df.head(100000)$
v_today = datetime.datetime.now()$ today_str  = '('+v_today.strftime("%m/%d/%y")+')'$ today_str_plot  = v_today.strftime("%m-%d-%y")$
news_df = pd.DataFrame(news_dict)$ news_df.head()
data.values
station_count = session.query(Stations.id).count()$ print ("Total Number of Stations are: "+ str(station_count))
validation.analysis(observation_data, BallBerry_resistance_simulation_0_5)
df['Avg_speed']= (df['Trip_distance'] / df['Trip_duration']) #miles/second$ df['Avg_speed']=df['Avg_speed']*3600  #miles/hour$ df['Avg_speed'].describe()
monte.str.split().str.get(-1)
Measurement = Base.classes.measurements$ Station = Base.classes.stations
filter_df['race'].unique()
cassession.columninfo('HMEQ_SCORED_GB')
print(norm.cdf(z_score))
TEXT.vocab.itos[:12]
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US', 'ab_page']])$ results = mod.fit()$ results.summary()
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
start_mask =  df_goog.index >= pd.to_datetime('2014-07-01')$ end_mask   =  df_goog.index <= pd.to_datetime('2014-12-01')$ df_goog[(start_mask) & (end_mask)]$
users.dtypes
iris.groupby('Species')['Species'].count()
stations_df = pd.read_csv("clean_stations.csv")$ stations_df
experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)
selected_users = pd.merge(df_mk, df_ks, how = 'inner', on = 'id')$ (selected_users.groupby('id')['time_x'].nunique()/selected_users.groupby('id')['time_y'].nunique()).nlargest(10)
from datetime import date$ pd.options.mode.chained_assignment = None  # default='warn'$ df = df[df['Date Created'] < date(2017,11, 1)]
conn.execute(sql)
list(t2.p1)
indexed = base_df$ supreme_court_df = base_df.loc[indexed['parent_id'] == 't3_3b6zln'].reset_index()
df.dtypes
df4.sort_values(by='BG')
difference = total.xs_tally - absorption.xs_tally - scattering.xs_tally$ difference.get_pandas_dataframe()
os.listdir(os.getcwd() + "/2018-05-26/")[0]
donors_c.iloc[2097169, :]$
df.loc[index_name]
mydata = mydata.json()$ type(mydata)
np.count_nonzero(x < 6)
df.sort_values('dealid', ascending = True, inplace = True)
df.injured.value_counts()
a = dat['community area'].unique()$ a.sort()$ a
taxiData.Trip_distance.size
df.head(5)
nb_topics = 5$ lda = LatentDirichletAllocation(n_components=nb_topics, learning_method='batch', random_state=42)$ lda.fit(vects)
train_df['clicked'] = train_df.clicked.map( {False: 0, True: 1} ).astype(int)$ train_df['hacker_confirmation'] = train_df.hacker_confirmation.map( {False: 0, True: 1} ).astype(int)$
df_new[['CA','UK', 'US']] = pd.get_dummies(df_new['country'])
result = pd.concat([df1, df3], axis = 0) # concatenate one dataframe on another along rows$ result
plt.hist(p_diffs)$ plt.axvline(x=p_diffs_obs, color = 'r', linewidth=2)$ plt.title('Fig 2: Distribution of p_diffs depicting the observed difference');
p_old = df2.converted.mean()$ print('The convert rate for old_page under the null is: {}.'.format(round(p_old, 4)))
def infer_one_doc(doc):$     rep = doc2vec_model.infer_vector(doc[1])$     return (doc[0], rep)
import json$ import pandas as pd$ import matplotlib.pyplot as plt
dataset.count()
data_set.to_csv("disbarred.csv", index=False, encoding='utf-8')$
train=pd.concat([race_train,train],axis=1)$ test=pd.concat([race_test,test],axis=1)$ train.head(4)
city_data ="Pyber/raw_data/city_data.csv"$ ride_data ="Pyber/raw_data/ride_data.csv"
merge_df['Primary Language'] = merge_df['Language'].str.split('-').str[0]$ merge_df['Language Location'] = merge_df['Language'].str.split('-').str[1]
df2_with_country[['CA', 'US','UK']] = pd.get_dummies(df2_with_country['country'])[['CA','US','UK']]$ df2_with_country[['old_page','new_page']] = pd.get_dummies(df2_with_country['landing_page'])[['old_page','new_page']]$ df2_with_country.head()$
plt.show()$
X_copy['crfa_f'] = X_copy['crfa_f'].apply(lambda x: int(x))
inter_by_date = niners.groupby('Date')['InterceptionThrown'].sum()$ inter_by_date;
data[data.userScreen=='KTHopkins']
excutable = utils.download_executable_lubuntu_hs(save_filepath)
pd.DataFrame(sanders)
coinbase_btc_eur_min=coinbase_btc_eur.groupby('Timestamp', as_index=False).agg({'Coin_price_EUR':'mean', 'Coin_volume':'sum'})
from sklearn.ensemble import RandomForestClassifier$ crf = RandomForestClassifier(n_estimators=1000, max_features=numfeat, max_depth=5) $ print("Parameters used in chosen RF model:\n " , crf.get_params())
df_mas.timestamp = pd.to_datetime(df_mas.timestamp)$ df_mas.retweeted_status_timestamp = pd.to_datetime(df_mas.retweeted_status_timestamp)
store_items.dropna(axis=0) # or store_items.dropna(axis=0, inplace=True) 
log_mod_3 = sm.Logit(df_new['converted'], df_new[['ab_page','CA', 'UK', 'intercept']])$ result_3 = log_mod_3.fit()$ result_3.summary()
result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4)$ result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)')$ np.allclose(result1, result2)
df_a = pd.DataFrame(df_graph.groupby('dates')['click_rec_menues'].value_counts())
pivoted_df = news_sentiment_df.pivot(index='Tweet_Number', columns='News_Source', values='compound')$ pivoted_df.head()
sublist = [BAL, CHI, HOU, PIT] # These are the four teams with an original Per Seat Price column uploaded.$ for team in sublist:$     team.drop(["Per Seat Price"], axis = 1, inplace = True) # Drops the column
print('RF:', rf.score(X_test, y_test))$ print('KNN:', knn.score(X_test, y_test))
_delta=_delta.to_dict()
fed_reg_dataframe['token_text'] = fed_reg_dataframe['str_text'].apply(lambda x: word_tokenize(x.lower()))
date = datetime.datetime.strptime(   )$ mask = 
df = pd.read_csv('ab_data.csv')$ df.head()
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
results = logit.fit()$ results.summary()
print sentence.find('west')$
jobs_data4 = json_normalize(json_data4['page'])$ jobs_data4.head(5)
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
print cust_data.assign(Dups = cust_data.duplicated()).head(5)$
n_new=df2.landing_page.value_counts()[0]$ n_new
df2.set_index('user_id').index.get_duplicates()
url_CLEAN1A = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1A.csv"$ url_CLEAN1B = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1B.csv"$ url_CLEAN1C = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1C.csv"$
for k, v in each_dict.items():$     print(k, v)
sales_corr = sales_update.corr()$ sales_corr
dataframe.groupby('month').daily_worker_count.agg(['count','min','max','sum','mean'])
measurement_df.describe()
ave_sentiment_by_company = unique_sentiments_df.groupby("Symbol")["Compound"].mean()$ ave_sentiment_by_company
plt.plot(model_output.history['loss'],c='k',linestyle='--')$ plt.show;
df2['intercept'] = 1$ df2['ab_page'] = pd.get_dummies(df2['group']).drop(columns=['control'])$ df2.head()
90# distribution of tuna *given* tuna > 0 (they come back)$ tuna_90_pd.tuna.hist(bins=range(0, T1, 1), normed=1)
df.iloc[1] # select row by integer location 1$
autos["price"].unique().shape[0]
con = sqlite3.connect('db.sqlite')$ print(pd.read_sql_query("SELECT * FROM temp_table", con))$ con.close()
sc = spark.sparkContext$ access_logs_raw = sc.textFile('data/apache.log')
print(df['Borough'].value_counts(dropna=False))
pd.date_range('1/1/2017', '12/1/2017', freq='BM')$
combined = dfs[0].append(dfs[1], ignore_index=True)$ combined = combined.append(dfs[2], ignore_index=True)$ combined.head()
np.sum(jArr, 0)
output = model.predict(test[:, 1:5])$ rowID = [TEST.rowID for TEST in test_data.itertuples()]$ result_df = pandas.DataFrame({"rowID": rowID,"cOPN": list(output)})
prediction_df = pd.DataFrame(predictions, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"])$ combined_df = comment_df.join(prediction_df) # join the comment dataframe with the results dataframe$ combined_df.head(15)
df2['user_id'].duplicated().sum()
def line_eq(x, mm, bb):$     return  x*mm + bb$ print "# test: %s"%(line_eq(10, 0.4471, -4.2076))$
for c in ccc:$     ved[c] /= ved[c].max()
df = pd.read_csv('ab_data.csv')$ df.head(5)
for col in df:$     print col,':', df[col].unique()
new_page_converted.mean() - old_page_converted.mean()
grouper = df.groupby(['source account'])$ overall_score = grouper['compound score'].mean()$ overall_score
df_borrowers_time = df_borrowers.set_index('created_at')$ df_borrowers_time = df_borrowers_time[df_borrowers_time.activation_status != 4]$ s_new_rates = df_borrowers_time.groupby([lambda x: x.year, lambda x: x.month, ])['id'].count()$
df2 = df[(df.group == 'treatment') & (df.landing_page == 'new_page') | (df.group == 'control') & (df.landing_page == 'old_page')]$ df2.head()$ df2.info()
df_hdf = dd.read_hdf(target, '/data')$ df_hdf.head()
airbnb_od.structure()
file_name = str(time.strftime("%m-%d-%y")) + "-NewsMoodTweets.csv"$ sentiments_pd.to_csv(file_name, encoding="utf-8")
discounts_table.Segment.unique()
df_merge = pd.merge(df_schoo11, df_students1, on='school_name')$ df_merge.drop(['School ID', 'Student ID'], axis = 1, inplace=True)
df_concat = pd.concat([bild, spon]) $
tweets_master = step_2.copy()$ tweets_i_master = step_3.copy()
investors_df = investors_df[(investors_df.most_recent_investment >= '2015-10-15') & $                             (investors_df.investment_count > 5)].copy()
data.describe().transpose()
cbd = CustomBusinessDay(holidays=cal.holidays())$ datetime(2014,8,29) + cbd
branch_dist = [analyze_swcfile.get_dist_to_root(a,example_df) for a in branches]
session.query(func.count(weather.id)).scalar()
users = users.sort_values(by=['avg_score'], ascending=False)$ top_25_users = users.index[1:50].tolist()
fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 7,8], feature_names=X_test.columns, grid_resolution=70, n_cols=7)$ fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 10], feature_names=X_test.columns, grid_resolution=70, n_cols=7)$
odds = get_cleaned_odds(odds)$ odds.tail()
tobs_df = pd.DataFrame.from_records(station_tobs)$ tobs_df.head()
ny_100 = ny.loc[ny['appv_dcl'] >= 100]$ ny_100.shape[0]*100/ny.shape[0]$
print("{} is the proportion of users converted.".format(df['converted'].mean()))
classifier = linear_model.SGDClassifier(random_state = 0)$ cv = cross_validation.StratifiedShuffleSplit(train_labels, n_iter = 10, test_size = 0.2, random_state = 0)
merged_df = df_genre.join(dtm_tfidf_df, how = 'right', lsuffix='_x')$ merged_df
df.to_csv('ab_edited.csv', index=False)$ df2 = pd.read_csv('ab_edited.csv')
X_test[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
pd.read_pickle('data/wx/tmy3/proc/tmy3_meta.pkl', compression='bz2').head()
lr2 = LogisticRegression(random_state=20, max_iter=10000, C= 1, multi_class='ovr', solver='saga')$ lr2.fit(X_tfidf, y_tfidf)
inspector = inspect(engine)$ inspector.get_table_names()$
reviews.loc[reviews.price.isnull()]
df = pd.read_csv('tmdb-movies.csv')$ df.head(5)
new.Purchased.value_counts()/len(new)*100$
def last_event_check(id):$     return max(a_active_devices_df[a_active_devices_df['device_id']==id]['create_time'])$ a_active_devices_df['last_activity_time'] = a_active_devices_df['device_id'].apply(last_event_check)
round(max(multi.handle.value_counts(normalize=True)),4)
df_Diff=df_Modified-df_Created$ df_Diff.head(9)
final_rm=final.drop(target0.index)$ X=create_time_list(final_rm)
r6s = r6s[['created_utc', 'num_comments', 'score', 'title', 'selftext']]$ r6s['created'] = pd.to_datetime(r6s['created_utc'],unit='s')$ r6s = r6s[r6s['created']>datetime.date(2017,12,1)]
rng_pytz = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='Europe/London')
test_clean["Month"] = test_clean["DateTime"].apply(month_slicer)$ list_of_test_features.append(("Month", test_clean["Month"]))
print(prec_long_df['date'].min(), prec_long_df['date'].max())
Z = np.random.randint(0,3,(3,10))$ print((~Z.any(axis=0)).any())
automl_feat.fit(X_train, y_train,$            dataset_name='psy_native',$            feat_type=feat_type)
from scipy.stats import norm$ norm.cdf(z_score)  # significance of our z_score
jscores = jcomplete_profile['scores']$ sdf = pd.DataFrame.from_dict(jscores)$ print(sdf[['score_code','model_scope_forecast_horizon','effective_date', 'score_value']])
modelFCDL.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])$
df_course_tags = pd.read_csv('course_tags.csv')  $ df_user_assess_scores = pd.read_csv('user_assessment_scores.csv')$ df_user_course_views = pd.read_csv('user_course_views.csv')
train_df.head()
tbl_detail = conn.get_table_details("nyctaxi")$ pd.DataFrame(tbl_detail)
lr_pipe.fit(X_train, y_train)$ lr_pipe.score(X_test, y_test)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print("Z_score:",z_score)$ print("P_value:",p_value)
wb.save('most_excellent.xlsx')
df2[df2['TUPLEKEY']==("A002", "R051", "02-00-00", "LEXINGTON AVE")].groupby(['DATE']).sum().plot(figsize=(10,3))
df.drop(1899, inplace=True)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
X_new = pd.DataFrame({'TV': [40]})$ X_new.head()
df_concensus_uaa = df_concensus_uaa.sort_index()
delimited_hourly = delimited_twitter_df.groupby([pd.Grouper(freq="H"), 'company']).count()['text'].to_frame()$ delimited_hourly.columns = ['Number_of_Tweets']$
df_country=pd.get_dummies(data=countries_df, columns=['country'])$ df_country.head()
df_new['intercept'] = 1$ lm = sm.OLS(df_new['converted'], df_new[['intercept','CA', 'US']])$ lm.fit().summary()
tf_idf = gensim.models.TfidfModel(corpus)$ print(tf_idf)
df_events = pd.read_csv("data_output/df_events.csv",low_memory=False)
for i in check_cols:$     print(train[i].value_counts())$     print('')
csvDF=csvDF.rename(columns = {'CustID':'CustId'})
obs = df_gp_hr.groupby('level_0').mean()$ observation_data = obs['Observation (aspen)']
import matplotlib.cm as cm$ dots_c, line_c, *_ = cm.Paired.colors
permits_df.dropna(inplace=True)
df_tick = df_tick.loc['2017-08-24':'2017-08-30']
crimes['year'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.year)
Multiplication = df.loc['Total']['AveragePrice'] * df.iloc[18248]['AveragePrice']$ print(Multiplication)
WorldBankdf = sqlContext.read.json("world_bank.json.gz")
ad_source.columns = ['AD_'+str(col) for col in ad_source.columns]
df2.drop_duplicates('user_id', keep='first', inplace=True)$ df2[df2['user_id'] == repeated_id]
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)$
display(data_rf.head(10))
df_countries = pd.read_csv('countries.csv')$ df_full = pd.merge(df_countries, df2, how='outer')$ df_full.head()
print(z_score)$ print(p_value)$ print( norm.ppf(1-(0.05/2)))
fe.bs.bootshow(256, poparr, repeat=3)$
temps_df.loc['2016-04-05']
reports = reports[(reports["Equipment Type"] == "MILK CHIILLING UNIT") | \$                   (reports["Equipment Type"] == "MILK CHILLING UNIT")]$ reports.info()
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
df.shape
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])$ df_new.head(1)
for urlTuple in otherPAgeURLS[:3]:$     contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ contentParagraphsDF.head()
bc = pd.read_csv("bitcoinity_all.csv")
election_data.groupby('st')['labor_force_13'].mean()$
p_mean = np.mean([p_new, p_old])$ p_mean
new_log_mod = sm.Logit(df3['converted'], df3[['intercept', 'new_page', 'UK', 'US']])$ new_results = new_log_mod.fit()$ new_results.summary()
import nltk.data$ tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')$
data_dir = '/srv/zooniverse/raw_data/panoptes/all-panoptes-classifications-2017-01-30.csv'$ classification_df = pd.read_csv(data_dir)$ project_df = classification_df.groupby('project_id').size().to_frame('total_classifications')
dictionary = Dictionary(messages_clean)$ corpus = [dictionary.doc2bow(text) for text in messages_clean]
bds.shape
image_predictions.describe()
df = pd.read_csv("../Files/371SpectraDeMeoColor.csv", index_col=0)$ df = shrink_classes(df)
train = pd.read_csv("wikipedia_train3.csv")$ test = pd.read_csv("wikipedia_test3.csv")
with open('test.csv') as f:$     size=len([0 for _ in f])$     print("Records in test.csv => {}".format(size))
WorldBankdf.registerTempTable("worldbank")
bands.at['2018-04-29 13:24:49', '68'] = 1$ bands = bands.drop(['drop', 'drop2', '68_2', 'drop3', 'drop4'], axis=1)
df_tweet_json_clean = df_tweet_json.copy()
control_group = df2.query('group == "control"')$ converted_control_group = converted_group.query('group == "control"')$ print(converted_control_group.user_id.nunique() / control_group.user_id.nunique())
X = [string.replace('\n', ' ') for string in X]
df.shape
total.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_3/target_winter2017_night2.csv',index=False)
p_diffs=np.array(p_diffs)$ plt.hist(p_diffs)
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
df.isnull().sum()$
results_df.describe()
print(data.index.value_counts())$ data=data.drop(data.index[-1])
new = df.sort_values("dates")$ new.reset_index()$
p_old = df2.converted.mean()$ p_old
f = open("datasets/git_log_excerpt.csv")$ print(f.read())
pd.period_range('2015-07', periods=8, freq='M')
conn.close()
df_ad_airings_filter_3['race'].value_counts()
df2 = df2.drop(df.index[2893])$ df2.reset_index(drop=True, inplace=True)$
df_subset.boxplot(column='Initial Cost', by='Borough', rot=90)$ plt.show()
wqYear = dfWQ.groupby('Year')['TotalN'].mean()$ dfWQ_annual = pd.DataFrame(wqYear)
obj.groups.keys()$
tweet_archive.name.value_counts().head(5)$
eta = therm_fiss_rate / fuel_therm_abs_rate$ eta.get_pandas_dataframe()
df_categories = pd.read_csv('categories.csv')$ df_categories.head()
import logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)
test_id = np.array(test_clean["ID"])[:,None]$ print test_id[:10]$
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
import gensim, logging$ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
df_new['intercept'] =1$ df_new = df_new.join(pd.get_dummies(df_new.country))$ df_new = df_new.join(pd.get_dummies(df_new.landing_page))
TrainData_ForLogistic = dum.drop(['City_Code', 'Employer_Code', 'Customer_Existing_Primary_Bank_Code', 'Source',$                                  'DOB', 'Lead_Creation_Date'], axis=1)
df_weather = pd.read_csv("../data/weather.csv", parse_dates=["date"])$ display(df_weather.head())$ print("This table has {npnts} data points and {col} columns.".format(npnts=df_weather.shape[0], col=df_weather.shape[1]))
tweet_data = pd.read_json(twitter_json)$ tweet_data.set_index('created_at', drop=True, inplace= True)$ pd.to_datetime(tweet_data.index)
old_page_converted=np.random.binomial(n=1,p=P_old,size=n_old)$ old_page_converted$
df = pd.DataFrame.from_dict(last_12_months)$ df = df.set_index('date')$ df.head()
session.query(func.count(measurement.date)).all()
lda_tfidf.print_topics(num_topics=10, num_words=7)$
groupby_w = df['y'].groupby(df['w'])$ round(groupby_w.describe(), 3)
ave_sentiment_by_company_df = pd.DataFrame(ave_sentiment_by_company)$ ave_sentiment_by_company_df
df.converted.sum()/df_length$
df.plot()
print(dfx.fillna(value=-999.25),'\n')$ print(dfx) # original data remains unchanged. 
prcp_summ_stats = prcp_df['prcp'].describe()$ prcp_summ_stats_df = round(pd.DataFrame(prcp_summ_stats), 2)$ prcp_summ_stats_df
merged1.columns
X_train, X_test, y_train, y_test = train_test_split(features,classification_price,test_size=0.2)
keywords = ['earthquake', 'quake', 'magnitude', 'epicenter', 'magnitude', 'aftershock']$ search_results = api.search(q=' OR '.join(keywords), count=100)
df_green.head(n=10)
df['processed'] = df['processed'].str.lower()
ps = pst.to_period()$ ps
max(image_predictions['tweet_id'].value_counts())
df['INCR_ENTRIES'] = df['ENTRIES'] - df.groupby('TUPLEKEY')['ENTRIES'].shift(1)
page = requests.get('https://www.r-bloggers.com')$ soup = BeautifulSoup(page.text, "html5lib")$
from bs4 import BeautifulSoup             $ example1 = BeautifulSoup(train["review"][0], 'html.parser')  
ptgeom = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]$ sites = gpd.GeoDataFrame(df, geometry=ptgeom, crs={'init': 'epsg:4326'})$ sites.head(5)
(obs_diff-np.mean(p_diffs))/np.std(p_diffs)
y_test.value_counts().head(1)/y_test.shape
salesdec['Standard_Plat'] = salesdec['Platform']$ salesdec['Standard_Plat'].isnull().any()$
vlc[pd.isnull(vlc.bio.str.strip())].id.size
google_fill = google.asfreq('D', method='bfill')
aapl = pd.read_csv(file_name, index_col='Date', usecols=['Date', 'Adj Close'])$ aapl.columns = ['AAPL']$ aapl
g = sns.factorplot(x = 'date', hue = 'gender', col = 'brand', data =df_ts, size = 10, kind = 'count')$ g.set_xticklabels(df_ts.date.unique(),rotation = 90)
rfbestgini = RandomForestClassifier(max_depth=14, n_estimators=10, criterion = 'gini')$ rfbestgini.fit(X_trainfinaltemp, y_train)
items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']),$          'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])}$ print(type(items))
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]
ccc = td.columns
x = [[1, 2], $     [3, 4]]$ np.concatenate([x, x], axis=1)
results.summary()
df2['intercept'] = 1$ df2['ab_page'] = pd.get_dummies(df2['landing_page'])['new_page']$ df2.head()
ratings.describe()
stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\$ order_by(desc(func.count(Measurement.station))).all()$ stations
conv_mean = df2.converted.mean()$ conv_mean
sentiment_overall = news_sentiments.groupby('News Source').agg({'Compound': np.mean}).reset_index() $ sentiment_overall
data['data'].keys()
df.LinkedAccountId.fillna(value=1,inplace=True)
sns.countplot(x='badge_#', data=df)
df_homeless = df[df['Complaint Type'] == 'Homeless Encampment']$ df_homeless.groupby(df_homeless.index.month)['Created Date'].count().plot(kind="bar")$
df.head()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ hawaii = Base.classes.measurement
from scipy.stats import f_oneway
scn_genesis = pd.to_datetime(min(USER_PLANS_df['scns_created']))
df.isnull().values.any()
twitter_archive_clean.describe()
np.exp(results.params)
train['smart_author'] = train.author.isin(smart_authors.index).astype(int)$ train.groupby('smart_author').popular.mean()
leadsdf['simpleDate'] = pd.to_datetime(leadsdf["lastEnteredOn"],format="%Y-%m-%d")$ leadsdf['simpleDate'] = leadsdf['simpleDate'].dt.strftime('%Y-%m-%d')
df = pd.read_csv('ab_data.csv')$ df.head()
sp500.index
dfLikes.dropna(subset=["created_time"], inplace=True)
extract_deduped_with_elms_v2.loc[(~extract_deduped_with_elms_v2.ACCOUNT_ID.isnull())$                              &(extract_deduped_with_elms_v2.LOAN_AMOUNT.isnull())].shape
ab_dataframe.isnull().any().any()
joined.dtypes.filter(items=['Frequency_score'])
twitter_Archive.duplicated()
planets.groupby('method')['orbital_period'].median()
left.head()$
import tweepy$ import pandas as pd$ import matplotlib.pyplot as plt
df2_new_page = len(df2.query("landing_page == 'new_page'")) / df2.shape[0]$ print('The probability that an individual received the new page is: {}.'.format(round(df2_new_page, 4)))
merged = pd.merge(train,properties,how='left',on='parcelid')
plt.plot(y_validation_RF.as_matrix()[0:50], '+', color ='blue', alpha=0.7)$ plt.plot(predictions[0:50], 'ro', color ='red', alpha=0.5)$ plt.show()
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
retweets = twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()==False]$ twitter_archive_clean.drop(retweets.index, inplace=True)$ twitter_archive_clean.reset_index(drop=True,inplace=True)
print(sum(receipts.duplicated(['parseUser','reportingStatus','iapWebOrderLineItemId','iapPurchaseDate'])))$ print(sum(receipts.duplicated(['iapWebOrderLineItemId'])))
dftemp = df1[(df1['Area'] == "Iceland")]$ dftemp.head(10)$
start_t = '2013-01-01 00:00:00'$ end_t=pd.to_datetime('today')- timedelta(days=1)$ end_t1=str(end_t)$
df.iloc[1]
top20_mostfav = top20_mostfav.sort_values(by='favorite_count', ascending=False).copy()$ top20_mosttweeted = top20_mosttweeted.sort_values(by='tweet_count', ascending=False).copy()
from nltk.stem.porter import PorterStemmer$ stemmed = [PorterStemmer().stem(w) for w in words]$ print(stemmed)
dfs_morning.sort_values(by='ENTRIES_MORNING', ascending = False).head(50)$ threshold = 100000$ dfs_morning.loc[dfs_morning['ENTRIES_MORNING']>threshold, 'ENTRIES_MORNING'] = dfs_morning.ENTRIES_MORNING.quantile(.5)
print(autos['odometer_km'].max())$ print(autos['odometer_km'].min())
low[low < myIP[2]].max()$
for c, f in zip(tips_model.coef_[0], features.columns.values):$     print(f'{c:5.2f} * {f}')
cercanasANuevasEstaciones = dataCapGbamayoresA1000.loc[dataCapGbamayoresA1000.lat.notnull() & dataCapGbamayoresA1000.lon.notnull(), ['created_on', 'place_name', 'lat', 'lon', 'price_usd_per_m2']]$ cercanasANuevasEstaciones.loc[:, 'distanciaAEstacion'] = cercanasANuevasEstaciones.apply(distanciaANuevasEstaciones, axis = 1)$ cercanasANuevasEstaciones = cercanasANuevasEstaciones.loc[cercanasANuevasEstaciones["distanciaAEstacion"] < 600, :]
station = Base.classes.stations
rule_one_above = df[(df['X'] > x_chart_ucl)]$ for i in range(0, rule_one_above.shape[0], 10):$     display_html(rule_one_above.iloc[i:i+10].T)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')$ z_score, p_value
g = sns.factorplot(x = 'date', hue = 'type', col = 'brand', data =df_ts, size = 8, kind = 'count')$ g.set_xticklabels(df_ts.date.unique(),rotation = 90)
s519397_df["date"] = pd.to_datetime(s519397_df["date"], format='%Y-%m-%d')$ s519397_df.info()
p_new = df2.converted.mean()$ print('The convert rate for new_page under the null is: {}.'.format(round(p_new, 4)))
noTempNullDF.dtypes
aci_service.get_logs()
pd.merge(df1, df4, left_on='key', right_on='rkey')$
grouped_publications_by_author['authorId'].nunique()
print("Probability of user converting:", df2.converted.mean())
df.head()
df.rename(columns={'Indicator':'Indicator_id'}).head(2)
df['duration'].quantile(0.995)$ df = df[df.duration <= 360]
df['positive_tokens'] = df['body_tokens'].apply(lambda x: len([word for word in x if word in positive_words]))$ df['negative_tokens'] = df['body_tokens'].apply(lambda x: len([word for word in x if word in negative_words]))$ print(df[['token_count', 'positive_tokens', 'negative_tokens']])
display(datos.head(10))
csvfile ='results.csv'$ resdf = pd.read_csv(csvfile)$ resdf.info()
t = datetime.time(1,30,5,2)$ print(t)
df2[df2['group']=='treatment']['converted'].mean()
Measurements = Base.classes.measurements$ Stations = Base.classes.stations
data.iloc[1]
events_df['event_time'].max(),events_df['event_time'].min()
data = []$ for row in result_proxy:$     data.append({'date': row[0], 'prcp': row[1]})
bins = [-1,-0.5,0,0.5,1]$ group_names = ["Between -1 and -0.5", "Between -0.5 and 0", "Between 0 and 0.5", "Between 0.5 and 1"]$ sentiment_df["Sentiment group"] = pd.cut(sentiment_df["Compound score"], bins, right = False, labels=group_names)
jail_census.groupby('Gender')['Age at Booking'].mean()
bus_new=bus.iloc[:,0:3]$ ins_named = ins.merge(bus_new, how="left", left_on='business_id', right_on='business_id')$ ins_named$
nbart_allsensors =nbart_allsensors.sortby('time')
conversion_prob = df2["converted"].mean()$ conversion_prob
precip_data_df = pd.DataFrame(precip_data_dict)$ precip_data_df = precip_data_df.rename(columns={"prcp":"Precipitation"})$ precip_data_df.head()$
noTempNullDF = mergedNewSet[mergedNewSet.TEMP.notnull()]$ noTempNullDF.shape
df3[['CA', 'US']] = pd.get_dummies(df3['country'])[['CA', 'US']]
data.plot(kind='scatter', x='TV', y='sales')$ plt.plot(X_new, preds, c='red', linewidth=2)
network_simulation[network_simulation.generations.isin([])]$
print("Proportion of converted users is {}%".format((ab_file['converted'].mean())*100))
df_not_treat_new = df[((df['group'] == 'treatment') & (df['landing_page'] == 'new_page')) == True]$ df_not_cntrl_old = df[((df['group'] == 'control') & (df['landing_page'] == 'old_page')) == True]$ df2 = pd.concat([df_not_treat_new, df_not_cntrl_old])$
print('With KNN (K=) accuracy is: ',knn.score(x_test,y_test)) $
k ['MONTH'] = k.index.map(dict)
xmlData['country'].replace({' United States of America': 'USA'}, inplace = True)
categories_feat = categories_feat.str.join('').apply(lambda x: x[1:-1])
pd.set_option('display.max_columns', 23)$ movies.head(5)
y_2_pred = rnd_reg_2.predict(X_future)
cityID = 'b463d3bd6064861b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Detroit.append(tweet) 
countries_df = pd.read_csv('./countries.csv')$ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner')
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::1].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.'])$ ax.set_ylabel('Relative error')$
for iter_x in np.arange(lookback_window)+1:$     df['{0}-{1}'.format(base_col,str(int(iter_x)))] = df[base_col].shift(iter_x)
options_frame = pandas.read_pickle('options_frame.pickle')
corr_matrix = dftotal.corr()$ corr_matrix["tripduration"].sort_values(ascending=False)
londonDFSubset.head()
words_hash_scrape = [term for term in words_scrape if term.startswith('#')]$ corpus_tweets_scraped.append(('hashtags', len(words_hash_scrape))) # update corpus comparison$ print('Total number of hashtags: ', len(words_hash_scrape)) #, set(terms_hash_stream))
fraud['device_count'] = fraud.groupby('device_id').device_id.transform('count')$ fraud['ip_count'] = fraud.groupby('ip_address').ip_address.transform('count')
import pandas as pd$ groceries = pd.Series(data = [30, 6, 'Yes', 'No'], index = ['eggs', 'apples', 'milk', 'bread'])$ groceries
sns.distplot(movies.vote_average);$
for c in ccc:$     if not os.path.isdir('output/' + c):$         os.makedirs('output/' + c)
words_only_sp_freq = FreqDist(words_only_sp)$ print('The 100 most frequent terms (terms only): ', words_only_sp_freq.most_common(20))
data.area
cityID = '6a0a3474d8c5113c'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         El_Paso.append(tweet) 
df_uro_no_metac = df_uro.drop(columns = ['MET_DATE1'])$ df_uro_no_metac.head()
KKK = str(time.strftime("%m-%d-%y")) + "-Output.csv"$ df.to_csv("output/" + KKK, encoding="utf-8")
df2.drop(labels = 1899, axis=0, inplace=True)
df.to_csv('Tableau-CitiBike/TripData_2017_Winter.csv', index=False)
columns = inspector.get_columns('measurements')$ for c in columns:$     print(c['name'], c["type"])$
p_old = .1196
import matplotlib.pyplot as plt $ %matplotlib inline$ ax = delays_by_origin.plot.scatter('mean', 'count', alpha=.2)$
df_expand['Date'] = pd.date_range(start='2017-04-01 17:42', end='2017-07-02 19:49', freq='min')
[scores.apply(sum), scores.apply(np.mean)]
payload = {'api_key': 'apikey'}$ r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json',params=payload)$
store_items.dropna(axis = 1)
merged2 = pd.DataFrame.merge(merged,omdb_df,on="movie",how="inner")$ merged2.head()
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)
test_df["price_t"] =test_df["price"]/test_df["bedrooms"]$ test_df["num_description_words"] = test_df["description"].apply(lambda x: len(x.split(" ")))
%matplotlib inline$ commits_per_year.plot(kind="line", title="Commits Per Year", legend=False)
festivals.at[2,'Location'] = "Humboldt Park and Division Street"$ festivals.head(3)
data=requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-07-01&end_date=2018-07-31&api_key=%s" %API_KEY)
rain_df.describe()
input_nodes_DF=nodes_table('network/source_input/nodes.h5', 'inputNetwork')$ input_nodes_DF[:5]
df2['user_id'].duplicated().sum()
csvData['street'] = csvData['street'].str.replace(' Avenue', ' Ave')$ csvData['street'] = csvData['street'].str.replace(' Court', ' Ct')$ csvData['street'] = csvData['street'].str.replace(' Drive', ' Dr')
p_old = df2['converted'].mean()$ p_old
df_l.loc[:,['ship_name','major_cruise_line']].drop_duplicates('ship_name').loc[df_l['major_cruise_line'] =='Carnival Cruise Line'].sort_values('ship_name')$
my_prediction = my_tree_one.predict(test_features)
df3 = df2[df2['Current Status']=='complete']$ df3.head()
feedbacks_stress.describe()
data=users.merge(transactions,how='left',on='UserID').groupby(['UserID']).min()$ data
preci_df = pd.DataFrame(preci_data)$ preci_df.head()
Base = automap_base()$ Base.prepare(engine, reflect=True)$ weather = Base.classes.weather
londonDFSubset['POPDEN'].describe()
grafBoedo = boedoEnMesesGrouped['price_usd_per_m2']['mean'].plot(kind='line', color='c', title='Precio promedio mensual del m2 en Boedo')$ grafBoedo.set_xlabel("Periodo")$ grafBoedo.set_ylabel("Precio promedio m2")
df_json_tweets['date'] = df_json_tweets['date_timestamp'].apply(lambda time: time.strftime('%m-%d-%Y'))$ df_json_tweets['time'] = df_json_tweets['date_timestamp'].apply(lambda time: time.strftime('%H:%M'))
bus.head(10)$ ins.head(10)$ vio.head(10)$
allItemIDs = np.array(allData.map(lambda x: x[1]).distinct().collect())$ bAllItemIDs = sc.broadcast(allItemIDs)
!tar -xzvf cudnn-8.0-linux-x64-v7.1.tgz
df2['converted'].mean()
scores[scores.IMDB == scores.IMDB.min()]
exiftool -csv -createdate -modifydate cisnwe5/Cisnwe5_cycle4.MP4 > cisnwe5.csv
nb = MultinomialNB()$ nb.fit(X_train_dtm, y_train)$ y_pred_class = nb.predict(X_test_dtm)
for key,value in deaths_sorted.items():$     print( "Deaths in " + str(key) + " = " + str(value))
first_row = session.query(Measurement).first()$ first_row.__dict__
conn_SF.close()
%%time$ pd.to_datetime(df['Created Date'], format='%m/%d/%Y %X %p')$
weather_cols = ['Weather', 'TempF', 'TempC', 'Humidity', 'WindSpeed', 'WindDirection', 'Pressure', 'Precip', ]$ weather_df = weather_df[weather_cols]$ weather_df.head()
events.schema
lr = LogisticRegression(random_state=20)$ lr.fit(X, y)
df.head()
from sklearn.neighbors import KNeighborsClassifier$ from sklearn.model_selection import train_test_split
year3 = driver.find_elements_by_class_name('yr-button')[2]$ year3.click()
df.quantile([.01, .05, .1, .25, .5, .75, .9, .95, .99])
merge_df = pd.merge(customers_df, responses_df,left_on='RESPONSE_ID', right_on='ID')$ print(len(merge_df))$ merge_df.head()
df_clean[df_clean['tweet_id'] == int(df_tweet_clean['id'].sample().values)]
df_user['org_id'].value_counts().head(10)$
from subprocess import call$ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
c.find_one({'$or': [{'name.first': 'Michael'},$                      {'name.last': 'Bowie'}]})
precipitation_df.plot()$
non_na_df = df.dropna()
festivals['lat_long'] = festivals[['latitude', 'longitude']].apply(tuple, axis=1)
preci_df.describe()$
model_uid = client.repository.get_model_uid(model_details)$ print(model_uid)
pd.Series(pd.Categorical(iris["Species"])).sample(5)
df_new.head()
for zc in weather.zip_code.unique():$     print weather[weather.zip_code == zc].isnull().sum()$     print
ts = pd.to_datetime('2015-01-15 08:30')$ ts
plot_autocorrelation(series=dr_num_new_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of new patients'))$ plot_autocorrelation(series=dr_num_existing_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of existing patients'))
new_converted_simulation = np.random.binomial(n_new, p_new, 10000)/n_new$ old_converted_simulation = np.random.binomial(n_old, p_old, 10000)/n_old$ p_diffs = new_converted_simulation - old_converted_simulation
sc=SparkContext.getOrCreate()$ sqlContext = SQLContext(sc)
temp_open = temp.groupby('Game_x').first()$ temp_close = temp.groupby('Game_x').last()$ temp_8 = temp.groupby('Game_x').nth(-481)
viz_1=sb.countplot(x=studies_b.enrollment, data=studies_b)$
print(abs(-12.0), '\n')$ print(len([1, 2, 3, 4, 5]))$ print(set([1, 2, 3, 4, 5, 5, 4]))
treatment_group = df2.query('group == "treatment"')$ converted_treatment_group = converted_group.query('group == "treatment"')$ print(converted_treatment_group.user_id.nunique() / treatment_group.user_id.nunique())
sp = openmc.StatePoint('statepoint.20.h5')
(p_diffs>p_diff_abdata).mean()
twitter_ar.expanded_urls[100]
measure.info()
twitter_Archive['timestamp'] = pd.to_datetime(twitter_Archive['timestamp'])$ type(twitter_Archive['timestamp'].iloc[0])
price_vs_km = pd.DataFrame(avg_price_series, columns = ["mean_price"])
tweets_l_scrape = d_scrape['text'].tolist() # create a list from 'text' column in d dataframe$ print(tweets_l_scrape[-1:])
dog_stages = Counter(df_clean.dog_stage)$ dog_stages
finals['type'] = "normal"$ finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'facilitator'
sel_shopping_cart = pd.DataFrame(items, index = ['pants', 'book'])$ sel_shopping_cart
joblib.dump(nmf, './data/NMF.pkl')$
np.random.binomial(n_new,p_new,10000)$
st_columns = inspector.get_columns('stations')$ for c in st_columns:$     print(c['name'], c["type"])$
for word in STOP_WORDS:$     nlp.vocab[word].is_stop = True
sns.set_style('whitegrid')$ sns.distplot(data_final['countCollaborators'], kde=False,color="red", bins=25) $
R_features = vectorizer.transform(reviewsR2_clean)$ R_features = R_features.toarray()$ result = forest.predict(R_features)$
sample.head()
df = pd.read_csv('~/mids/w266/w266_final_project/Combined_Comments.csv', delimiter=',')
df['Duration'].plot.hist()
results_df.plot()$ plt.show()
print (quand_request.json())
df_users.sum()
nitrodata['Year'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).year$ nitrodata['Month'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).month
go_no_go_times = go_no_go.groupby('subject_id').Time.unique()$ simp_rxn_time_times = simp_rxn_time.groupby('subject_id').Time.unique()$ proc_rxn_time_times = proc_rxn_time.groupby('subject_id').Time.unique()
suburban_merged_1 = pd.merge(suburban_average_fare, suburban_total_rides, on="city")$ suburban_merged_df=pd.merge(suburban_merged_1, suburban_drivers, on="city")$ suburban_merged_df.head()
discounts_table = data_df[['ID','Segment','Country','Product','Gross Sales','Discount Band','Discounts']].copy()$ discounts_table.head()
df['HOUR'].fillna(99, inplace = True)$ df['NEIGHBOURHOOD'].fillna('N/A', inplace = True)$ df['HUNDRED_BLOCK'].fillna('N/A', inplace = True)
df_vets = df_vets.loc[df_vets['d_birth_date'] > '1980-01-01']$ drop_cols = ['d_suffix', 'section_id', 'row_num', 'site_num', 'cem_url', 'cem_phone', 'cem_addr_two', 'cem_addr_one', 'city', 'state', 'zip', 'v_first_name', 'v_mid_name', 'v_last_name', 'v_suffix']$ df_vets = df_vets.drop(drop_cols, axis=1)
df2.sort_values('total_likes',inplace=True,ascending=False)$ top_likes = df2.head(10)['id']
aaplhv.head()
sns.set_style('whitegrid')$ sns.distplot(data_final['countPublications'], kde=False,color="red") # bins=30, $
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=uEEsBcH3CPhxJazrzGFz&start_date=2017-01-01&end_date=2017-12-31")$ r.json()
liquor = liquor.dropna()
calls_nocontact.council_district.value_counts()
file = open(resumePath, "r")$ resume = file.read()$ corpus.insert(0, resume)
twitter_df.tweet_id.sample(1)
df2.to_csv("newstweets.csv",index=False)
scores[3.5:].sum()/total
ndvi_of_interest02= ndvi.sel(time = time_slice02, method='nearest')$ ndvi_of_interest02
clf = svm.SVR()$
data['Age'].value_counts()
local.export_to_quilt(post_process_info["DatasetId"])
ex_index=df.index.isin([5,12,23,56])$ df[~ex_index].head(10)
df2 = df.drop(index = control_new_page.index)$ df2 = df2.drop(index = treatment_old_page.index)
145311.000000/df2.shape[0] $
measure_nan.count()
fin_r_monthly = fin_r_monthly.iloc[:-1]
predictions = np.array([item['classes'] for item in classifier.predict(input_fn=test_input_fn)])$ predictions = [reverse_lookup[x] for x in predictions]
file4=file3.join(loc, file3.pole_id == loc.pole, "left_outer").drop(loc.pole)$ file4.show(3)
pax_raw.info()
old_page_converted = np.random.binomial(1, p_old,n_old)
lq2015_q1_drop = lq2015_q1.drop(['City', 'ZipCode', 'CountyNumber', 'County','CategoryName', 'VendorNumber','ItemDescription', 'BottleVolumeml'], axis=1)
print(df['Confidence'].unique())
DataSet[['userName','tweetFavoriteCt']].sort_values('tweetFavoriteCt',ascending=False).head(10)
regr.score(X, y)  # when we fit all of the data points
df_master = pd.read_csv('twitter_archive_master.csv')
os.chdir(root_dir + "data/")$ df_fda_drugs_reported.to_csv("filtered_fda_drug_reports.csv", index=False)
df.index$
df.rename(columns={'Indicator':'Indicator_id'},inplace=True)$ df.head(2)
plt.scatter(branch_dist[1:],branch_r[1:])
df['user_id'].nunique()
n_old = df2.query('landing_page == "old_page"')['user_id'].shape[0]$ n_old
autos = pd.read_csv("autos.csv", encoding = "Latin-1")
unordered_timelines = list(chain(*np.where([min(USER_PLANS_df.iloc[i]['scns_created']) != USER_PLANS_df.iloc[i]['scns_created'][0] for i in range(len(USER_PLANS_df))])))
ppm_title = preProcessor_in_memory(hueristic_pct=.99, append_indicators=True, padding='post', keep_n=4000, maxlen=12)$ vectorized_title = ppm_title.fit_transform(data_to_clean_title)
featured_image_url = 'https://www.jpl.nasa.gov/images/cassini/20170809/cassini20170809-16.jpg'
sort_df.tail(10)
df_df = pd.DataFrame(df1)$ df_df
import warnings$ warnings.simplefilter('ignore')
inspector = inspect(engine)$ inspector.get_table_names()
mars_table = table[0]$ mars_table.columns = ["Parameter", "Values"]$ mars_table.set_index(["Parameter"])$
model.fit(X_tr, y_tr)$
df["DISPOSITION_TYPE"].value_counts().sort_values(ascending=True).plot(kind='barh')$
data['SA'] = np.array([analize_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
df_users = pd.read_csv('../../data/march/users.csv')$ df_levels = pd.read_csv('../../data/march/levels.csv')$ df_events = pd.read_csv('../../data/march/events.csv', skiprows=1, names=event_header, error_bad_lines=False, warn_bad_lines=True)     
df2['user_id'].nunique()
chinese_vessels_wcpfc = pd.read_csv('chinese_vessels_wcpfc.csv')
brands_np = np.asarray(brands)$ models_np = np.asarray(models)
not_numbers = data_read.genre_ids.astype(str).apply(lambda x: x.isnumeric()) == False$ data_read["genre_ids"][not_numbers.values].sample(10)
df.drop(["join_mode"], axis = 1, inplace = True)
df2.head()
cityID = '389e765d4de59bd2'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Glendale.append(tweet) 
import gcsfs$ import google.datalab.bigquery as bq$ import pandas as pd
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index$ all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))$ all_features = all_features.fillna(all_features.mean())
df.status_message.fillna('NA', inplace=True)$
data['intra_day_diff'].max()
df.info()
df_final.columns
y_pred = rf.predict(X_test)
print(len(free_data.country.unique()))
datAll['Offense Type'] = datAll['Offense Type'].str.strip()$ datAll['Offense Type'] = np.where(datAll['Offense Type']=="AutoTheft",'Auto Theft',datAll['Offense Type'])
df_new[['US', 'UK', 'CA']] = pd.get_dummies(df_new['country'])$ df_new.drop('CA', axis=1, inplace=True)
tl_2050 = pd.read_csv('input/data/trans_2050_ls.csv', encoding='utf8', index_col=0)
counts = combined_df5['vo_propdescrip'].value_counts()$ repl = counts[counts <= threshold].index$ combined_df5['vo_propdescrip']=combined_df5['vo_propdescrip'].replace(repl, 'Other')$
display((data['Tweets'][fav]))$ print("Number of likes: {}".format(fav_max))$ print("{} characters.\n".format(data['len'][fav]))
ea_pr=pd.merge(transactions,transactions,how='outer',on='UserID')$ ea_pr
origin=table.find(text='Flight origin').find_next('td').text$ origin
data['Complaint Type'] = data['Complaint Type'].str.lower()$ data["Complaint Type"].value_counts().head(10)
from sklearn.svm import SVR$ model = SVR(kernel='rbf', C=1e3, gamma=0.1)$ print ('SVR')$
print(tipsDF.describe())
df.isnull().sum()
logit4 = sm.Logit(df3['converted'], df3[['intercept','new_page','UK_new_page','US_new_page','UK','US']])$ result4 = logit4.fit()$ result4.summary()
data = data.set_index('time')
msft = pd.read_csv("../../data/msft.csv", dtype={'Volume': np.float64})$ msft.dtypes
ndvi_us = ndvi_nc.variables['NDVI'][0, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(ndvi_us)
train_data, test_data = df_input_final.randomSplit([0.7, 0.3])
red_4.isna().sum()
freqs = [(word, X_train_feature_counts.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()]$ print sorted(freqs, key = lambda x: -x[1])[:20]
dr_2018 = dr_2018.resample('W-MON').sum()$ RNPA_2018 = RNPA_2018.resample('W-MON').sum()$ ther_2018 = ther_2018.resample('W-MON').sum()
X = train.title$ y = train.popular
vacation_data_df=pd.DataFrame(vacation_data)$ rain_per_station = pd.pivot_table(vacation_data_df,index=['station'],values=['prcp'], aggfunc=sum)$ rain_per_station
dfbreakfast = df[(df['TIME'] == '11:00:00') | (df['TIME'] == '12:00:00')]$ dfbreakfast.head(2)
plt.plot(glons, glats, marker='.', color='k', linestyle='none')$ plt.show()
data = users.merge(df_first, left_on=['UserID'], right_on=['UserID'], how='left' )$ data.drop(['key'], axis = 1, inplace = True)  # Column 'key' was added in a previous step that is now dropped$ data
df3['country'].value_counts()$ dummy_country = pd.get_dummies(df3['country'], prefix='country')$ dummy_country.head()
today = datetime.datetime.today()$ start_date = str(datetime.datetime(today.year, today.month-1, 1).date())$ print('predictions for', start_date)
from scipy.stats import norm$ norm.ppf(1-(0.05))
cursor = db.TweetDetils.aggregate([ {"$group" : {"_id":"$user_name", "score":{"$sum":"$retweet_cnt"}}},  {"$sort":{"score" : -1}},{"$limit":5}])$ for rec in cursor:$     print(rec["_id"], rec["score"])
iowa["Date"] = pd.to_datetime(iowa["Date"], errors='raise', format="%m/%d/%Y")$ iowa
input_folder = '/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/minimum temperature'$ glob_string = '*.nc'
df.rename(columns={"PUBLISH STATES":"Publication Status","WHO region":"WHO Region"},inplace=True)$ df.head(2)
print('Get subset data of rows excluding rows no. 5, 12, 23, and 56')$ lst_exclude = [5, 12, 23, 56]$ df[~df.index.isin(lst_exclude)].head(15)
from sklearn.model_selection import KFold
df_countries.nunique()
print ("Number of unique users:",df.nunique()['user_id'])
least = pd.DataFrame(data.groupby('tasker_id').hired.sum())$ least.loc[least['hired']==0]
(null_vals< ab_dif).mean()
net.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=directory_name)
LabelsReviewedByDate = wrangled_issues_df.groupby(['created_at','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
print(bag.toarray())
df_result[df_result.loglikelihood > 0].iloc[0]
jan_2015_frame.head()
pred = pd.read_table('image-predictions.tsv', sep = '\t')$ pred.info()
pd.DataFrame({'features': X.columns, 'LasoCoefficients': lasso.coef_})
smooth = condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="SMOOTHPURSUIT")
prec_df = pd.DataFrame(data = us_prec)$ prec_df.columns = ts.dt.date
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))$
engine = create_engine('sqlite:///hawaii.sqlite')
df_birth.population = pd.to_numeric(df_birth.population.str.replace(',',''))
df.head(5)
noTempNullDF = noTempNullDF.reset_index(drop=True)
df2.drop_duplicates('user_id', keep='first', inplace=True)$ df2[df2.duplicated('user_id')]
df1.info()
countries = pd.get_dummies(df_new['country'])$ df_new = df_new.join(countries)$ df_new.head()
y_pred = pipeline.predict(X_test)$ print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))$
X_lowVar = df.copy()
grades = pd.read_csv("files/grades.csv")$ grades.head()
joined=join_df(joined,weather,["State","Date"])$ joined_test=join_df(joined_test,weather,["State","Date"])$ sum(joined['Max_TemperatureC'].isnull()),sum(joined_test['Max_TemperatureC'].isnull())
df3.country.unique()
df2.groupby('group').describe()
high_low_diff = TenDayMeanDifference(inputs=[USEquityPricing.high, USEquityPricing.low])
df.reset_index(inplace=True)$ df.head()
open('test_data//write_test.txt', mode='w')
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')$ z_score, p_value
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv',$                  parse_dates=['deadline', 'launched'],$                  encoding = "ISO-8859-1")
image_clean['p1'] = image_clean['p1'].str.replace('_', ' ')$ image_clean['p2'] = image_clean['p2'].str.replace('_', ' ')$ image_clean['p3'] = image_clean['p3'].str.replace('_', ' ')
ab_new['country'].value_counts()$
json_data2017=response.json()['dataset_data']$
with open('./data/processed/X_train_age_imputed.pkl', 'wb') as picklefile:$     pickle.dump(X_train,picklefile)$
df = df.reindex(np.random.permutation(df.index))$ df.head(2)
p_new = df2.converted.mean()$ print(p_new)
building_pa.sample(5)
df = pd.read_csv('ab_data.csv')$ df.head()
df = df.loc[df['game_type'] == 'R',]
df_merge.plot(title='retweet_count over favorite_count',x='favorite_count',y='retweet_count',kind='scatter',alpha=0.1);
jeff.withdraw(100.0)   # Instruction to withdraw$ jeff.balance           # Shows 900.0
reg = linear_model.LinearRegression(fit_intercept=False, normalize=False)
plots_df = pd.read_csv("plots.csv", keep_default_na=False, na_values=[""])$ plots_df.groupby(["plot_type"]).count().unstack()
df.info()
alice_sel_shopping_cart = pd.DataFrame(items, index = ['glasses', 'bike'], columns = ['Alice'])$ alice_sel_shopping_cart
norm.ppf(1-(0.05/2))
df_sp_500 = df_sp_500.withColumn('IDX', func.lit('SP_500'))
plot_BIC_AR_model(data=RN_PA_duration.diff()[1:], max_order_plus_one=8)
yc200902_short['Trip_Pickup_DateTime'] = pd.to_datetime(yc200902_short['Trip_Pickup_DateTime'])
dpth = os.getcwd()$ dbname_sqlite = "ODM2_Example2.sqlite"$ sqlite_pth = os.path.join(dpth, os.path.pardir, "data/expectedoutput", dbname_sqlite)
import graphlab$ sf_stars = graphlab.SFrame(df_stars)$ sf_stars
pgh_311_data.resample("Q").mean()
rf.score(X_train_all, y_train)
import pandas as pd$ import os$ os.chdir('C://Users/dane.arnesen/Documents/Projects/pytutorial/')
comp = reduce(lambda x, y: pd.merge(df_test, forecast_range, on = 'ds'), df_test.append(forecast_range))$ comp.describe()$
countries_df = pd.read_csv('./countries.csv')$ df4 = countries_df.set_index('user_id').join(df3.set_index('user_id'),how = 'inner')$ df4.head()
df2.query('landing_page=="new_page"').user_id.nunique()/df2.user_id.count()
scores_df.to_csv('News_Tweets.csv', encoding="utf-8", index=False)
lm=sm.Logit(df2['converted'],df2[['intercept','ab_page']])$ r=lm.fit()
df7 = df6.rename('Completed_Permits').reset_index()
df = df[df["status"] == "active"]$ df.drop(["status"], axis = 1, inplace = True)$ df.shape
conn_hardy = psycopg2.connect("dbname='analytics' user='udacian' host='udacity-segment.c2zpsqalam7o.us-west-2.redshift.amazonaws.com' port='5439' password='AYEe&mtihMqtXQbWR2xgWrhmKzd6]F'")
X.shape
svm_classifier.score(X_test, Y_test)
fb_all = farebox.copy()$ fb_day_time = fb_all.groupby(['service_day','day_of_week','service_time','service_datetime']).agg({'entries':np.sum}).reset_index()$ fb_day_time.rename(columns={'entries':'entries_green'}, inplace=True)$
logreg = LogisticRegression(penalty='l1', solver='liblinear')$ y_pred = cross_validation.cross_val_predict(logreg, X, y, cv=5)$ print(metrics.accuracy_score(y, y_pred))
query = session.query(Station).all()$ print(len(query))
df_users.index
google.loc[(google['year'] == 2015)]
RMSE_list[W.index(12)]
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
with open("TestUser1.json","r") as fh:$     data = json.load(fh)$
successful_campaigns = results[results['tone']=='Positive']['campaign_id'].unique()$ print('first five positive campaigns (unordered): \n{0}'.format(successful_campaigns[:5]))
res = es.search()$ print(res["_shards"])$
b_cal = pd.read_csv('boston/calendar.csv')$ b_list = pd.read_csv('boston/listings.csv')$ b_rev = pd.read_csv('boston/reviews.csv')
df_ab_cntry['intercept'] = 1$ logit_mod = sm.Logit(df_ab_cntry['converted'], df_ab_cntry[['intercept','US','CA']])$ results = logit_mod.fit()
engine.execute('SELECT * FROM stations LIMIT 10').fetchall()
new_page_converted.mean() - old_page_converted.mean()
logit_mod1 = sm.Logit(df_new['converted'], df_new[['intercept', 'CA','US']])$ result1 = logit_mod1.fit()$ result1.summary()
top_20 = data['Company'].value_counts()[0:20]$ top_20[::-1].plot(kind='barh');
!rm -fv .cache/runtimes.csv
g_goodbad_index = sl_data.groupby(['goodbad','AGE_groups']).sum()$ g_goodbad_index
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-24&end_date=2018-08-24&api_key=" + API_KEY)
output.shape
crimes['2011-06-15']['NEIGHBOURHOOD'].value_counts().head(5)
prices=pickle.load(open('Q://LB2//dump//prices.p', 'rb'))$ prices=prices.reindex_axis(sorted(prices.columns), axis=1)$ returns=Factor.prices_to_returns(prices, replace_missing=True)
dfn = df.loc[df.period].copy()$ dfo = df.loc[~df.period].copy()$ dfn.shape, dfo.shape
print('Best Score: {}'.format(XGBClassifier.best_ntree_limit))
unique_users2 = df2['user_id'].nunique()$ unique_users2
y = df_series#pd.Series(y, index=dates)$ arma_mod = sm.tsa.ARMA(y, order=(2,2))$ arma_res = arma_mod.fit(trend='nc', disp=-1)
xmlData['price'] = pd.to_numeric(xmlData['price'], errors = 'raise')
ok.auth()
pres_df['day_of_week'] = pres_df['start_time'].map(lambda x: x.strftime("%A"))$ pres_df.head(10)
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
yearago_date = dt.date(2016, 8 , 22)$ print(yearago_date)
c.find_one({'name.first': 'Michael',$              'name.last': 'Bowie'})
measure_df.info()$
df['2015-06'].resample('D').sum().plot()
df2.loc[df2.user_id.duplicated()] 
Measurement = Base.classes.hi_measurements$ Station = Base.classes.stations
td_norm = td ** (10/11)$ td_norm = td_norm.round(1)
df3 = df3.add_suffix(' Created')$ df7 = pd.merge(df4,df3,how='left',left_on='Date Created',right_on='MEETING_DATE Created')$
jail_census.resample("D").mean()
df2[['no_ab_page','ab_page']] = pd.get_dummies(df2.group)$ df2 = df2.drop('no_ab_page', axis=1)$ df2.head()
new_texas_city.columns = ["Measurement_date", "Compaction"] $ new_texas_city.loc[546, "Measurement_date"] = "2015-07-23 00:00:00"$ new_texas_city.tail(10)
df2.loc[df2['user_id'].duplicated(keep = False)]
station_query = session.query(Measurement.station).group_by(Measurement.station)$ station_result = pd.read_sql(station_query.statement, station_query.session.bind).count()$ print("There are ", station_result.station , 'station')
df_new.head(1)
prob_conrol_converted = df2[df2['group'] == 'control']['converted'].mean()$ print (prob_conrol_converted)
control_df = df2.query('group == "control"')$ p_control = control_df.query('converted == 1').user_id.nunique()/control_df.user_id.nunique()$ print('Probability of conversion for the control group is {}.'.format(round(p_control,4)))
import pandas as pd$ tweets = pd.read_csv ("./twitter.csv", header=None)$ tweets.head()
n_new = df2.query('landing_page == "new_page"')['user_id'].count()$ n_new
1.If a restaurant's score improves from the first to the second inspection,the point on the scatter plot should be above the red line.$ Over half of the points are above the red line.$ 2.If a restaurant's score improves from the first to the second inspection,the difference should be above zero.The mean of the differences is positive.$
es = es.entity_from_dataframe(entity_id = 'clients', dataframe = clients, $                               index = 'client_id', time_index = 'joined')
prcp_df.describe()
StockData.describe()
df_names = pd.read_csv('names.csv')$ df_names
total_newpage = df2[df2.landing_page == 'new_page'].count()['user_id']$ total_pages = df2.shape[0]$ total_newpage / total_pages #0.50006194422266881$
USERNAME = ''$ TOKEN = ''$ API_BASE_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'
n_old = df_control.nunique()['user_id']
hdf.sort_index(inplace=True,$               level=[0, 1])$ hdf.index.lexsort_depth # both levels are sorted against
df_pr.sort_values('sentiment', ascending=False).iloc[:10]
suburban_ride_total = suburban_type_df.groupby(["city"]).count()["ride_id"]$ suburban_ride_total.head()$
df[df.country == "es"].id.size
twitter_archive_full.drop(twitter_archive_full[(twitter_archive_full.tweet_id.isin(duplicated_list)) & $                                               (twitter_archive_full.stage != 'doggo')].index, inplace=True)
data = r.json()
print(df)$ df[pd.isnull(df)] = 0.777$ print(df)$
df = pd.concat([chgis, new_data])
df = pd.read_csv("ab_data.csv")$ df.head()
autos.odometer.unique()
new_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_newReview, model=model, num_features=5000)$ print (np.round(new_avg_word_vec_features, 3))
df['month_first_booking'] = date_first_booking.dt.month$ df.month_first_booking.replace('NaN', 'None', inplace=True)$ print df.month_first_booking
df2[df2['group'] == "control"]['converted'].mean()
combo = first + ' ' + last[0] + '.'$ combo
fields = ['customer_id', 'product_id','created_at']$ df = pd.read_csv('query_result.csv', usecols=fields)
df['Date'] = pd.to_datetime(df['Date'])  #convert date to datetime dtype$ df['Date'].head()$
FREEVIEW.plot_main_sequence(raw_freeview_df)
fig = ax.get_figure()$ fig.savefig("Overall Sentiment.png")
twitter_archive_full[(twitter_archive_full.in_reply_to_status_id_x.isna()==False) | $                      (twitter_archive_full.retweeted_status.isna()==False)]
import statsmodels.api as sm$ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = "smaller")$ print("The computed z-score is: {} and the p-value is: {}".format(z_score, p_value))
model_filename = 'models/finalized_mpg_estimation_model.sav'$ loaded_mpg_estimation_model = pickle.load(open(model_filename, 'rb'))$
data = pd.concat([approved, rejected], ignore_index=True)
cats_in = intake.loc[intake['Animal Type']=='Cat']$ cats_in.shape
missing_values = missing_values_table(perf_test)$ missing_values.head(20)
!cat ../../data/msft_with_footer.csv # osx / Linux
print("Number of mislabeled points out of a total %d points : %d" % \$       (X_clf.shape[0], (y_clf != y_pred).sum()))
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == True].shape[0]
from jira import JIRA$ jira = JIRA(cred['host_jira'], basic_auth=(cred['email'], cred['password_jira']))$ issues = jira.search_issues('issuetype = "Bug de Tracking"')
typesub2017['Solar'].sum() 
jail_census.resample("D").size().plot()
for column in ls_other_columns:$     df_uro[column].unique()
stations = session.query(Measurement).group_by(Measurement.station).count()$ print(stations)
usage_400hz = hc.table('asm_wspace.usage_400hz_2017_Q4') $
df['SA'] = np.array([ analize_sentiment(tweet) for tweet in df['Tweets'] ])$ display(df.head(10))
(cs.loc[cs['SUMLEV'] == 50,:]$ .groupby('STNAME').agg({'CENSUS2010POP': np.average})$ .head())
(act_diff < p_diffs).mean()
df.dtypes
mars_weather = current_weather_info[0].text$ mars_weather
data = pd.read_csv('Dumping.csv', delimiter = ',', skiprows=0, squeeze=False, skip_blank_lines=True, index_col=None)$
df = df.drop_duplicates()$
Featured_image = image_soup.find('img',class_="fancybox-image")$ print (Featured_image)$
df2[df2.duplicated(['user_id'], keep=False)]
psy_prepro = psy$ psy_prepro.to_csv("psy_prepro.csv")
for i in range(0,10):$     topics = model.show_topic(i, 10)$     print "%s: %s" %(i, (', '.join([str(word[0]) for word in topics])))
print('reduce memory')$ utils.reduce_memory(transactions)
float(result2[result2['dt_deces'].notnull()].shape[0]) / float(result2.shape[0]) * 100.
tweets_data_path = '../data/tweets_no.json'$ tweets_file = open(tweets_data_path, "r")$ tweets_data = json.load(tweets_file)
negs= [sentiments.index(x) for x in sentiments if (x and float(x)<=0)]$ print(len(negs))$ negs[5:15]
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']])$ results = logit_mod.fit()$ results.summary2()
result_3 = pd.concat([df1, df3], ignore_index = True) # same as option 1 but with reset index $ result_3
p_diffs = np.array(p_diffs)$ plt.hist(p_diffs)$ plt.title('The normal distribution of mean differents of new_page and old_page converted');
df_ari['Game'] = df_ari.MatchTimeROT.map(ari_games.Game)
all_sets = pd.read_json("AllSets.json", orient = "index")$ all_sets.head()
free_data.groupby('age_cat')['educ'].mean()
stat_info_merge = pd.concat([stat_info_city[1], stat_info_st[[0,1]]], axis=1)
giss_temp = giss_temp.where(giss_temp != "****", np.nan)
support = merged[merged.committee_position == 'SUPPORT']$ oppose = merged[merged.committee_position == 'OPPOSE']$ oppose.amount.sum()
np.exp(result_interact.params)
dat=pd.read_csv(pth)$ print(pth)
cityID = '0eb9676d24b211f1'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cleveland.append(tweet) 
iso_join = iso_join.dissolve(by=['time', 'time_2']).reset_index()$
data_df = df.sort_values(by = "date", ascending=True)$ data_df.fillna(0)$ data_df.head()
sub_dataset.groupby(["NewsDesk", "Popular"]).size()
df.describe()
dataset['user_location'] = dataset['user_location'].str.lower()
bucket.upload_dir('data/wx/tmy3/raw/', 'wx/tmy3/raw', clear_dest_dir=True)
crime['Sex'].value_counts()
df1.info()
np.datetime64('2015-07-04 12:00')
v_invoice_hub.columns[~v_invoice_hub.columns.isin(invoice_hub.columns)]
reddit_info.to_csv('reddit_data_2.csv', encoding = 'utf-8', index = False)
data4.crs= "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
tmp = [i.lower() for i in df_repub_2016['text']]
calls_nocontact.ticket_status.value_counts()
dummy_colnames = [clean_string(colname) for colname in df_dummies.columns]$ df_dummies.columns = dummy_colnames$ df_dummies.head()
df_ad_airings_1.dtypes
response_raw = requests.get(url,params)$ response_clean = response_raw.content.decode('utf-8')
temps2  = pd.Series([70,75,83,79,77, 69], index = dates)$ temps_diffs = temps1 - temps2$ temps_diffs
df2.head()
dframe_team['start_year'] = dframe_team['Start'].dt.year$ dframe_team.head()
tmdb_movies_genre_revenue['genre'] = tmdb_movies_genre_revenue['genre'].map(lambda x: x.get('name'))$
df2.shape
drivers = data_merge.groupby("type")$ drivers_total = drivers['driver_count'].sum()$ drivers_totals = [drivers_total['Urban'], drivers_total['Suburban'], drivers_total['Rural']]
control_notAligned = ((df['group'] == 'control') & (df['landing_page'] != 'old_page'))$ control_notAligned.sum()
studies_b=studies_a.merge(sponsors,on='nct_id', how='left')$ studies_b.head()
bds.dtypes
fh_3 = FeatureHasher(input_type='string', non_negative=True)$ %time fit3 = fh_3.fit_transform(train.device_ip)
result2.summary2()
encoded_df = pd.get_dummies(df, columns=['category', 'fileType'])$ encoded_df.shape
session.query(func.count(distinct(Measurement.station))).all()
n_new = len(df2.query("group == 'treatment'"))$ print('The n_new is: {}.'.format(n_new))
df1=pd.read_csv("TreyGowdyCustody51.csv")$ loc_df=df1['user_location']$ len(loc_df)$
bixi=pd.read_csv('OD_2018-07.csv')$ stations=pd.read_csv('Stations_2018.csv')
numero = json.loads(ejemplo_json).get('phoneNumbers')[1].get('number')$ print(numero)
np.array(df[['Visitors','Bounce_Rate']])
table_rows = driver.find_elements_by_tag_name("tbody")[12].find_elements_by_tag_name("tr")$
new_page_converted = np.random.binomial(1, p_new, n_new)
sentiments_pd["Outlet"].unique()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
title_sum = preproc_titles.sum(axis=0)$ title_counts_per_word = list(zip(pipe_cv.get_feature_names(), title_sum.A1))$ sorted(title_counts_per_word, key=lambda t: t[1], reverse=True)[:20]$
probs = model2.predict_proba(X_test)$ print probs
df.word_count.sum()
sorted_errors_idx = options_frame['ModelError'].map(abs).sort_values(ascending=False).head(50)$ errors_20_largest_by_strike = options_frame.ix[sorted_errors_idx.index]$ errors_20_largest_by_strike[['Strike', 'ModelError']].sort_values(by='Strike').plot(kind='bar', x='Strike')$
df.shape
sqlContext.sql("select * from RandomTwo").toPandas()
Image(url= "https://pbs.twimg.com/media/C2tugXLXgAArJO4.jpg")
for col_cells in ws.iter_cols(min_col=28, max_col=28):$     for cell in col_cells:$         cell.number_format = '0.00'
image_predictions[image_predictions.tweet_id.duplicated()]
twitter_archive_enhanced[twitter_archive_enhanced.rating_denominator != 10][['rating_numerator','rating_denominator','text']]
missing_millesime = intervention_train.MILLESIME.isnull()$ intervention_train.loc[missing_millesime, 'MILLESIME'] = intervention_train.loc[missing_millesime, 'CRE_DATE_GZL'].apply(lambda x: x.year)
trains_fe2_x= trains_fe1.drop(['days_since_prior_order','add_to_cart_order','reordered','eval_set'], axis=1)$ trains_fe2_x.head() #12 features as the predictor variables
session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()$
wget.download('https://cernbox.cern.ch/index.php/s/ibtnI2ESaFjIgSi/download')
hours.dropna(subset=['Specialty'], how='all', inplace=True)
twitter_archive_clean['quick_url'] = twitter_archive_clean.text.str.extract('(https.+)', expand=True)$
idx = pd.IndexSlice$ df.loc[idx[:, :, 'x'], :]
df_merge.groupby(['school_name', 'grade']).math_score.mean().unstack()
old_page_converted = np.random.choice([1,0], size=sample_size_old_page, p=[p_old_null, 1-p_old_null])
fit = log_model.fit()
len(purchase_history.product_id.unique())
marketSeries = pd.Series(sp500["Dif"].values, index=sp500["Date"])$ marketSeries.plot();
calls_nocontact_simp = calls_nocontact.drop(columns=['ticket_id', 'issue_description', 'city', 'state', 'location', 'geom'])$ calls_nocontact_simp.head()
customer_emails = sales_data_clean[['Email', 'Paid at']].drop_duplicates()$ customer_emails.dropna(inplace=True)
categories = pd.cut(df_titanic_temp.age, [0,age_median,80], labels=['category1','category2'])$ print(categories.value_counts())
df_joined[['CA','UK','US']] = pd.get_dummies(df_joined['country'])$ df_joined = df_joined.drop(['US'], axis = 1)$ df_joined.head()
small_lst = df.sample(100)$ location_small_lst, message_small_lst, date_small_lst = small_lst.locations,small_lst.messages, small_lst.posted_date
submit = perf_test[['ID_CPTE']]$ submit['Default'] = predictions$ submit.to_csv('random_forest_baseline.csv', index = False)
exploration_titanic.numeric_summary() # you can access to numeric
first_commit_timestamp = git_log[git_log['author'] == 'Linus Torvalds'].sort_values('timestamp').head(1)$ first_commit_timestamp
t1.
samp311.columns
new_array = np.concatenate((training_active_listing_dummy,training_pending_ratio),axis=1)
df.shape
mike=pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/mike_previous_observing.txt',delim_whitespace=True,header=None,names=['index','name','ra','dec'])$ mike['RA0']=[float(Angle(i, u.hr).to_string(unit=u.degree, precision=5, decimal=True)) for i in mike.ra]$ mike['DEC0']=[float(Angle(i, u.deg).to_string(unit=u.degree, precision=5, decimal=True)) for i in mike.dec]$
df2.landing_page.value_counts('new_page') # Only the value for new_page is relevant here as mentioned in the question above
print(pd.to_numeric(countdf['number_ratings']).sum())$ print(pd.to_numeric(countdf['number_ratings']).sum()-pd.to_numeric(count1df['number_ratings']).sum())$ print(pd.to_numeric(countdf['number_ratings']).sum()-pd.to_numeric(count6df['number_ratings']).sum())
df.drop(['Unnamed:_13', 'addressee', 'delivery_line_2', 'ews_match', 'suitelink_match', 'urbanization', 'extra_secondary_number', 'extra_secondary_designator', 'pmb_designator', 'pmb_number'], axis = 1, inplace=True)$ df.drop([326625], axis=0, inplace=True)
stopword_list.extend(["dass", "wer", "wieso", "weshalb", "warum", "gerade"]) #Add the words ["dass"] to the list.$ print(len(stopword_list))
lq2015_date.Date.sort_values(ascending=False)
print(open(os.path.join(PROJ_ROOT, 'requirements.txt')).read())
f = open("json_example.json","r")$ print(json.load(f))$ f.close()
data_store_id_relation.count()
nasa_articles = soup.find_all("div", class_="slide")
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
dfLookup = dfChat[['userFromName','userFromId']]$ dfLookup = dfLookup.drop_duplicates()$ dfLookup.columns = ['userToName','userToId']$
results3.summary()
a=contractor_clean.groupby(['contractor_id','contractor_bus_name'])['contractor_bus_name'].nunique()$ print a[a >1]$ contractor_clean[contractor_clean.contractor_number.duplicated() == True]
source_list = df1_clean.source.unique()$ source_list
for columns in DummyDataframe:$     basic_plot_generator(columns, "Graphing Dummy Data" ,DummyDataframe.index, DummyDataframe)
assert stacked_image_predictions.shape[0] == 3*image_predictions.shape[0]
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
archive = pd.read_csv('twitter-archive-enhanced.csv')
df_estimates_false.groupby('metric').count()$
df["DATETIME"] = pd.to_datetime(df.DATE + " " + df.TIME, $                                            format = "%m/%d/%Y %H:%M:%S")$ df
print ('\nThere are {} rows in this dataframe.'.format(len(df)))$ uniqueRows = len(pd.unique(df['user_id']))$ print ('\nThere are {} unique rows in this dataframe.'.format(uniqueRows))
autos["ad_created"].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True)
print("Probability of user converting is :", ab_file2.converted.mean())
df[df.amount_initial == '$0.00'].groupby(['fund', 'appeal'])['donor_id'].count()
engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))$ print(engine.url)
df['genre'].value_counts()
df2.user_id.nunique()
df1=df1[['Adj. Close','volatility','PCT_Change','Adj. Open','Adj. Volume']]$ df1.head()
html = browser.html$ soup = bs(html, 'html.parser')$ browser.click_link_by_partial_text('FULL IMAGE')
df_business.index = range(len(df_business))
tz_pytz = pytz.timezone('Europe/London')
merged1.drop('Id', axis=1, inplace=True)
results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'local')$
data[['clean_text','issue']].head(5)
words_only_sk = [term for term in words_sk if not term.startswith('#') and not term.startswith('@')]$ corpus_tweets_streamed_keyword.append(('words', len(words_only_sk))) # update corpus comparison$ print('The number of words only (no hashtags, no mentions): ', len(words_only_sk))
prcp_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation'])$ prcp_df.set_index('Precipitation Date', inplace=True) # Set the index by date
features, feature_names = ft.dfs(entityset = es, target_entity = 'clients', $                                  agg_primitives = ['mean', 'max', 'percent_true', 'last'],$                                  trans_primitives = ['years', 'month', 'subtract', 'divide'])
holdout_preds = lr.predict(x_holdout)
kick_data_state.columns
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff
images_path=images_folder_name+'\*.png'$ image_links = [f for f in glob.glob(images_path)]$ image_links
pd.Series(np.r_[1,2,9])
print(model.aic,model.bic,model.hqic)
test_df = df[df["dataset"]=="test"]; test_df.shape
len(df_measurement['station'].unique())$
cvPredictions = cvModel.transform(testData)
for c in ccc:$     for i in ved[ved.columns[ved.columns.str.contains(c)==True]].columns:$         ved[i] /= ved[i].max()
p_old = df2.converted.mean()$ p_old
popCon.sort_values(by='counts', ascending=False).head(9).plot(kind='bar')
(a_diff<np.array(p_diffs)).mean()
cris = CrisAI()$ cris.collect_cris_datasets(["tests/test_data/acoustic.zip", "tests/test_data/acoustic2.zip"])$ cris.SpeechCollector.audio_collection$
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
import pandas as pd$ y= pd.Period('2016')$ y
airbnb_df['weekly_price'].fillna(airbnb_df['price']*7, inplace=True)$ airbnb_df['monthly_price'].fillna(airbnb_df['price']*30, inplace=True)
investments = pd.read_excel(io="/Users/DanShin/Downloads/Data/crunchbase_export.xlsx", sheetname="Investments")
old_page_converted = np.random.choice([0, 1], size = n_old, p = [1 - p_old, p_old])$ p_old_sim = old_page_converted.mean()
sentiments_pd.to_excel("NewsMood.xlsx", encoding="UTF-8")
transactions[~transactions['TransactionID'].isin(dfTemp['TransactionID'].values)]
df.mean()
df.query('Confidence == "A:e"')
us_grid_id = (pd.read_csv('../data/model_data/us_grid_id.csv', index_col = 0)$               .groupby('grid_id').count().reset_index())$
url = "https://www.fdic.gov/bank/historical/bank/"$ driver.get(url)
model.wv.syn0.shape
df2[df2['user_id'].duplicated(keep = False)]
n_new = df2.query('landing_page == "new_page"').count()[0]$ print("Number of users with new page :",n_new)
df2.query('user_id==773192')
iowa_csv = 'file:///Users/Andrew/Downloads/iowa_liquor_sales_sample_10pct.csv'
tweet = api.get_status('892420643555336193')._json$ tweet['retweet_count']
y_train = np_utils.to_categorical(y_train, 90)$ y_test = np_utils.to_categorical(y_test, 90)
year_with_most_commits=commits_per_year[commits_per_year == commits_per_year.max()].sort_values(by='num_commits').head(1).reset_index()['timestamp'].dt.year$ year_with_most_commits='2016'$ print(year_with_most_commits)
Data['Pred'] = logit.fit().predict(X)$ Data['PredWin'] = np.round(Data['Pred'])$ print('accuracy =', np.mean(Data['PredWin'] == Data['Win']))
print activity_df.iloc[:,0]
temps_df.columns
result1 = -df1 * df2 / (df3 + df4) - df5$ result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5')$ np.allclose(result1, result2)
test_id_ctable = df.groupby(['test_id', 'pass_test']).size().unstack('test_id').fillna(0)$ _, p, _, _ = chi2_contingency(test_id_ctable)$ p
article_urls = ['http://www.nhm.ac.uk/discover/the-cannibals-of-goughs-cave.html','http://www.nhm.ac.uk/discover/how-we-became-human.html','http://www.nhm.ac.uk/discover/the-origin-of-our-species.html']$
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
mergedNewSet.shape
model.load_weights('model')$ loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)$ print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))
combined_df['intercept'] = 1$ logistic_model = sm.Logit(combined_df['converted'], combined_df[['intercept', 'CA', 'US']])$ result = logistic_model.fit()
Y = 'label'$ dogscats_h2o[Y] = dogscats_h2o[Y].asfactor()$
openmc.run()
t = thisyear$ len(t[(t.creation <= '2011-01-01') & (t.creation > '2010-01-01')])
INT=pd.read_csv('C:/Users/mjc341/Desktop/UMAN 1507 Monthly INQ summary Report/Interactions.Contacts.csv',skipfooter=5,encoding='latin-1',engine ='python')$
ebola_tidy = pd.concat([ebola_melt_1, status_country], axis = 1)$ print(ebola_tidy.shape)$ print(ebola_tidy.head())$
!hdfs dfs -cat 32ordered_results-output/part-0000* > 32ordered_results-output.txt$ !head 32ordered_results-output.txt
df = pd.DataFrame([tweet.text for tweet in tweets], columns=['Tweets'])$ df.head(n=10)
p_diffs = np.array(p_diffs)$ ab_data_diff = df[df['group'] =='treatment']['converted'].mean() - df[df['group'] =='control']['converted'].mean()$ (ab_data_diff <p_diffs).mean()
t3=t3.rename(columns={"id": "tweet_id"})
festivals.index
users = subs.user_id.unique()$ print "%d Users" % len(users)
sns.set()$ sns.distplot(np.log(df_input_clean.toPandas().Resp_time), kde=True, color='b')
print pd.merge(df0, df2, how='outer')$
crimes.columns = crimes.columns.str.replace('__', '_')$
Customer.withdraw(jeff, 200.0)$ jeff.balance           # Shows 700.0
funding_types = merged_df["Funding Type"].unique()$ funding_types
count_by_Confidence = grpConfidence['MeanFlow_cfs'].count()$ count_by_Confidence.plot(kind='pie');
all_tables_df.OBJECT_NAME
temps_df.Missouri - temps_df.Philadelphia
df = orgs[(orgs.founded_year >= 1990) & (orgs.founded_year <= 2016)].copy()
df = pd.read_hdf('data/games.hdf','df')$ df
dataset.columns
by_area['AQI'].hist(bins=20,histtype='step',stacked=True,linewidth=3)$ plt.legend(['Cottage Grove','Eugene/Springfield','Oakridge']);
df['label'] = df[forecast_col].shift(-forecast_out)
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1-p_new), p_new])
df_ad_state_metro_2 = df_ad_state_metro_1.copy().groupby('state')['ad_duration_secs'].sum().reset_index()$ df_ad_state_metro_2.sort_values('ad_duration_secs', ascending=False, inplace=True)
pd.value_counts(ac['Compliance Report Issued'])
printer_usage.rename(columns={'SUM(pages)' : 'total_printed'}, inplace=True)$ printer_usage.drop(columns=['pages'])$ print('')
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")$ contribs.info()
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1 - p_new)])$
df['message_vec'] = vec2.fit_transform(df['message'])$
ts_filter = ts_mean[ticker][ts_mean[euphoria].shift(1)<0.05]
all_noms[all_noms["agency"] != "Foreign Service"]["nom_count"].sum() + additional_june_noms
youtube_df["isPorn"] = pd.Series()$ youtube_df=youtube_df.fillna(0)$ youTubeTitles = youtube_df.loc[:, ["title", "isPorn"]]$
df_merge.head()
df.converted.mean()
def sigmoid(z):$     s =  1.0 / (1.0 + np.exp(- z))$     return s
session.query(Measurement.tobs).order_by(Measurement.tobs.desc()).first()
df.drop(df[(df.group == 'treatment') & (df.landing_page == 'old_page')].index, inplace=True)$ df.drop(df[(df.group == 'control') & (df.landing_page == 'new_page')].index, inplace=True)$ df2 = df
s.count(',')
df_combined.to_csv('twitter_archive_master.csv')$
datetime.now().time()
funding_types = merged_df["Funding Type"].unique()$
compound_df.columns = target_terms$ compound_df.head()
tobs_df.hist(column='Temperature', bins=12)$ plt.ylabel("Frequency",fontsize=12)$
df = pd.read_csv('Water_Quality_complaints.csv')$
df_sample = df.sample(frac=0.1, replace=True).reset_index()$ df_sample$ X_train_, X_test_, y_train_, y_test_ = sample_split(df_sample[selected_feature])
df_cities_clean = df_cities.loc[df_cities['lon']!='']$ df_cities_clean.to_csv('Cities_Data.csv',index=False)$ df_cities_clean.head()
grid.best_params_
spp['season'] = spp.index.str.split('.').str[0]$ spp['term'] = spp.index.str.split('.').str[1]
resumePath = "resumes/jacky.txt"
mis_1 = df.query('landing_page == "new_page" & group != "treatment"').count()['user_id']$ mis_2 = df.query('landing_page == "old_page" & group != "control"').count()['user_id']$ mis_1 + mis_2
df.groupby(['filename'])[['ml-fav-change','ml-dog-change','rl-fav-change','rl-dog-change',$                           'total-over-change','total-under-change']].sum()
featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA22456_hires.jpg'$ featured_image_url
df_ad_airings_5['state'].unique()
df = pd.DataFrame(data)$ df= df[['station','RainFall']]$ df.style.bar(subset=['RainFall'], align='mid', color=['#5fba7d'])
n_old = df2.query("landing_page == 'old_page'")['landing_page'].count()$ n_old
data_AFX_X.describe()['Traded Volume']
print df2['# Of Positions'].unique()$
data.loc[(80, slice(None),'put'),:].iloc[0:5,0:4]
if not os.path.isdir('output/heat_demand'):$     os.makedirs('output/heat_demand')
shows['stemmed_plot'] = shows['stemmed_plot'].dropna().apply(reassemble_plots)
ds_info = ingest.upload_dataset(database=db, dataset=test)$ ds_info
print(station_availability_df.info())$
con = sqlite3.connect('db.sqlite')$ print(pd.read_sql_query("SELECT * FROM temp_table LIMIT 2", con))$ con.close()
if 1 == 1:$     token_counter = collections.Counter(tokens)$     print(str(token_counter.most_common(50)))
inspector = inspect(engine)$ inspector.get_table_names()
tzs = DataSet['userTimezone'].value_counts()[:10]$ print tzs
multi.handle.value_counts() / multi.shape[0]
elms_all = elms_sl.append(elms_pl, ignore_index=True)$ elms_all = elms_all.append(elms_tl, ignore_index=True)$ elms_all['SSN'] = [float(x) for x in elms_all.SSN.values]
A = [np.int(station['id']) for station in r.json()['stationBeanList']]$ B = [station for station in r.json()['stationBeanList']]
mars_weather_tweet = weather_soup.find('div', attrs={"class": "tweet", "data-name": "Mars Weather"})
data_AFX_X.describe()['High']
pd.options.display.max_colwidth = 300$ data_df[['ticket_id','type','desc']].head(10)
csvpath = os.path.join('Desktop', 'Project-2', 'spotify_data.csv')$ import csv$ spotify_df = pd.read_csv(csvpath, encoding="ISO-8859-1")
model.score(X_test,y_test)
type(archive_df_clean['timestamp'])$ archive_df_clean.head(10)
old_page_converted = np.random.binomial(n_old, p_old)$ old_page_converted
alldata = pd.merge(data_FCBridge, data_FCInspevnt_latest, on='brkey')$ alldata.loc[alldata['bridge_id'] == 'LMY-S-FOSL']$
constants = pd.read_csv('control_chart_constants.csv')$ d2 = constants['d2'][constants['n'] == subgroup_size].values[0]$ d3 = constants['d3'][constants['n'] == subgroup_size].values[0]
with open('/Users/ianbury/Springboard/springboard/quandl_api_key','r') as file:$     API_KEY = file.read()$
logit_mod2 = sm.Logit(df_new.converted, df_new[['intercept', 'CA', 'US', 'new_page']])$ result2 = logit_mod2.fit()$ result2.summary()
cityID = '3b77caf94bfc81fe'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Los_Angeles.append(tweet) 
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))
ctd_df.dtypes
dates = pd.date_range('2010-01-01', '2010-12-31')$ df1=pd.DataFrame(index=dates)$ print(df1.head())
df_western.genres.str.contains(r'Western').sum() == df_western['id'].count()
df.sort_values(['Likes', 'len'], ascending=False).head()
df.columns
df_ad_state_metro_2.plot(x='state', y='ad_duration_secs', kind='bar')$ df_state_victory_margins.plot(x='state', y='Percent margin', kind='bar')$
user = reddit.redditor('Shadow_Of_')$ for comment in user.comments.new():$     print(comment.body)
import statsmodels.api as sm$ logit = sm.Logit(df['converted'],df[['intercept','treatment']])$
from IPython.display import HTML$
!head ../../data/msft_modified.csv
move_3_union = sale_lost(breakfastlunchdinner.iloc[3, 1], 20)$ adjustment_2 = move_1_union + move_2_union$ print('Adjusted total for route: ' + str(move_34p14u14u - move_3_union))
df.iloc[[11, 24,37], :]
df.set_index('Day') # this returns the new data frame with index set as 'Day' but does not modify the existing df dataframe$ df.head()
import pandas as pd$ git_log['timestamp']=pd.to_datetime(git_log['timestamp'],unit='s')$ git_log.timestamp.describe()$
udacity_image_prediction_url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"$ with open(udacity_image_prediction_url.split("/")[-1],"wb") as file:$     file.write(requests.get(udacity_image_prediction_url).content)
engine = sqlalchemy.create_engine('postgresql://carl:@localhost:5432/mailcharts_development')
df.cumsum().plot(figsize=(12, 16), subplots=True)
top_songs['Date'] = pd.to_datetime(top_songs['Date'])
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-0.05))$
plt.hist(p_diffs);
with open('100UsersResults.data', 'wb') as filehandle:  $     pickle.dump(results, filehandle)
train.columns
df.user_mentions = df.user_mentions.apply(usersMentions)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]$
strip=udf(lambda x:x.replace(' ',''))
df_parsed.country.value_counts()
g2_filtered = g_kick_data.filter(lambda x: len(x) <= 1).sort_values(['blurb','launched_at'])$ len(g2_filtered)$
columns = inspector.get_columns(db_tables[0])$ columns
billstargs.drop(['congress', 'number', 'chamber', 'committee_ids', 'introduced_on', 'last_action_at', 'last_vote_at', 'official_title', 'urls'], axis=1, inplace=True)
df2.shape[0]
stop = np.floor(len(X)/4).astype(int)$ model.fit(X[:stop], y[:stop], epochs=200, batch_size=128*2)$
!head -24 fremont.csv
Lda = gensim.models.ldamodel.LdaModel$ ldamodel = Lda(corpus=doc_term_matrix, id2word=dictionary, num_topics=topics)
from sqlalchemy import create_engine$ engine = create_engine('postgresql+psycopg2://postgres:admin@localhost:5432/DTML')$ X.to_sql('dtml_mstr_scld', engine, if_exists='append')
INC = pd.read_csv('s3://datapd/SS_DUN_Incidents_Dem_Zone_join.csv')$
sqlContext.sql("select * from RandomOne").toPandas()
oz_stops = oz_stops.rename(columns={'stop_id': 'stopid'})$ oz_stops.head(1)
dataset.id.describe()
set_themes.rename(columns = {'name_x': 'set_name', 'name_y': 'theme_name'}, inplace = True)
image_df_tomerge = image_df_clean.loc[:, ['tweet_id', 'prediction_1', 'prediction_1_confidence','prediction_1_result']]$ image_df_tomerge['tweet_id'] = image_df_tomerge['tweet_id'].astype(str)$ twitter_df_merged = pd.merge(pd.merge(archive_df_clean, status_df, on='tweet_id'), image_df_tomerge, on='tweet_id')#pd.merge(status_df, image_df_clean, on = 'tweet_id', how='inner')
rsvp_df = pd.read_csv("last_five_rsvp_means.csv")$ rsvp_df.head()
psy = psy_native.copy()$ psy = psy.dropna(thresh=(psy.shape[1]/2), axis=0)$ psy.shape  # 18 people removed
%%time$ pd.to_datetime(df['Created Date'], format="%m/%d/%Y %I:%M:%S %p").head(10000)$ pd.to_datetime
df2.tail()$
Google_stock.describe()
Urban = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Urban")]$
conn.execute(sql)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
temp = we_rate_dogs.text.str.contains('[Dd]ogg[a-z]*')$ we_rate_dogs.loc[temp, ['text', 'doggo']].head()
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False)$
my_gempro.map_uniprot_to_pdb(seq_ident_cutoff=.3)$ my_gempro.df_pdb_ranking.head()
recommendationTable_df = recommendationTable_df.sort_values(ascending=False)$ recommendationTable_df.head()
df_full = df_full[list(DATA_L2_HDR_DICT.values()) + ['school_type']]
autos = pd.read_csv('autos.csv', delimiter=',', encoding='Latin-1')
df_ad_airings_5.to_pickle('./TV_AD_AIRINGS_STATE_METRO_AREA_5.pkl')
data.fillna(0)
df2.converted.mean()$
dict_urls['web']['project'].split('?')[0]
df=pd.read_csv('twitter-archive-enhanced.csv')
grouped.size()
s.asfreq('3B', method='bfill').head(15)
df2['intercept']=1$ df2[['control','treatment',]]=pd.get_dummies(df2['group'])$
print(type(texas_city.index))$ print(type(addicks.index))$
tipsDF = tipsDF.drop(tipsDF.columns[0], axis = 1)
base_col = 't'$ df.rename(columns={target_column: base_col}, inplace=True)
df_license_activities.head(2)
all_tables_df.OWNER.nunique()
new_page_converted =  np.random.binomial(1,0.12,n_new)
x_scaled = min_max_scaler.fit_transform(x_normalized)
df.ix[:4]
grouped_df.to_csv('results.csv', index=False)
missing_info=list(data.columns[data.isnull().any()])$ missing_info
all_df = pd.read_csv('data/political_ads.csv', parse_dates=['start_time', 'end_time', 'date_created'])
df.loc[df.userLocation == 'Youngstown, Ohio', :]
deut5 = dta.t[(dta.b==5) & (dta.c==5)]
df_train.head(10)$
old_page_converted.mean()
test_classifier('c1', WATSON_CLASSIFIER_ID_2)$ plt.plot(classifier_stats['c1'], 'ro')$ plt.show()
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date'], axis=1)$ dfY = data['corrCost']
featured_image_url = browser.find_by_xpath('//*[@id="page"]/section[1]/div/div/article')
dayofweek = pd.DatetimeIndex(pivoted.columns).dayofweek
import nltk$ nltk.download('punkt')$ from nltk.tokenize import word_tokenize
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_10min'], axis=1)$ dfY = data['corrCost']
num_status = subset["content"].size$ print num_status
joined[['Frequency_score']] = joined[['Frequency_score']].apply(pd.to_numeric, errors='coerce')
train_df.describe()$ train_df['transaction_month'] = train_df['transactiondate'].dt.month$ train_df
df2 = pd.read_csv("../../data/msft.csv",usecols=['Date','Close'],index_col=['Date'])$ df2.head()
master_file = pd.read_csv(os.curdir + '/master.csv', encoding='iso-8859-1', dtype=object)
print(data.learner_id.nunique())
data.isna().sum()
df['converted'].sum()/df['user_id'].count()
mars_table = mars_df.to_html()$ print(mars_table)
aml_deployment.deploy()
if  not os.path.exists("movies.csv"):$     print("Missing dataset file")
import numpy as np$ X_nonnum = X.select_dtypes(exclude=np.number)$ X_num = X.select_dtypes(include=np.number)
liquor['Year'] = liquor['Date'].dt.year$ liquor['Month']=liquor['Date'].dt.month
data['Age'].min()
X_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)$ X_test[0:5]
max(tweet_json['id_str'].value_counts())
print( df_l.shape )        ## 506377, 28$ df_l.head()
'_'.join([x.strip() for x in s.split(',')])
bus['zip_code'] = bus['zip_code'].str.replace("94602", "94102")$
print(r.text)
import psycopg2$ import pandas as pd$ from skbio.diversity.alpha import shannon
payments_total_yrs.tail()$ payments_total_yrs.to_csv('Top Total Payees More than 1 Million Total.csv')$
price_data = price_data.iloc[1:]
p_diffs_sim10000 = p_diffs.mean()$ print('Mean: {:.4f}%\nStd: {:.4f}'.format(p_diffs_sim10000,p_diffs.std()))
result3 = df.eval('(A + B) / (C - 1)')$ np.allclose(result1, result3)
from datetime import datetime$ df['ts_visitStartTime']=df.apply(lambda x:datetime.utcfromtimestamp(x['visitStartTime']), axis=1 )$ df.iloc[1,]
num_unique_users = len(df.user_id.unique())$ print("Number of unique users: {}".format(num_unique_users))
df_enhanced[['dog_name']].groupby(['dog_name'])['dog_name'].size()
df_vow['Closed_Higher'] = df_vow.Open > df_vow.Close$ df_vow['Closed_Higher'] = pd.get_dummies(df_vow.Open > df_vow.Close).values
locationsDF.crs = londonDFSubset.crs$ mergedLocationDF = gpd.sjoin(locationsDF, londonDFSubset, how="left", op='within')$ mergedLocationDF.head()
for t in [36, 58, 65, 72, 88]:$     str = f'P(Fail) of O-Ring at {t} deg. = {lgt.predict([t, 1])[0]:4.2f}%'$     print(str)
records2.loc[records2['Age'].isnull(), 'Age'] = records2['Age'].mean()$ records2.loc[records2['GPA'].isnull(), 'GPA'] = records2['GPA'].mean()$ records2.loc[records2['Days_missed'].isnull(), 'Days_missed'] = records2['Days_missed'].mean()
top_songs['Month'] = top_songs['Date'].dt.month
.5 / 1.6
X_copy['loc'] = X_copy['loc'].apply(lambda x: int(x))
s.name = 'Justice League ages'$ s.index = ['bruce', 'selina', 'kara', 'clark']$ s
trumpdf.dtypes
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country'])$ df_new=df_new.drop('UK',axis=1)$
S.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
locations = soup.find_all(class_='sl-spot-details__name')$ locations
df2.drop('building_use', axis = 1, inplace=True)
cityID = '30344aecffe6a491'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fremont.append(tweet) 
Let's get started. First, let's import the Pandas package to use for the analysis. The Pandas package also allows the$
merged_predictions_archive = pd.merge(left=twitter_archive_clean, right=most_confident_predictions, how='inner', on=['tweet_id']).copy()
merged = merged.set_index('DateTime')
file_names = []$ file_names = glob.glob('*.csv')
df2.info()
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new])$ new_page_converted.mean()$ print(new_page_converted)
result1 = (df1 < df2) & (df2 <= df3) & (df3 != df4)$ result2 = pd.eval('df1 < df2 <= df3 != df4')$ np.allclose(result1, result2)
df_new['us_page'] = df_new['us'] * df_new['ab_page']$ df_new['uk_page'] = df_new['uk'] * df_new['ab_page']$ df_new.head()
data.groupby(['Year'])['Salary'].sum()
tweet_df_raw = pd.read_json('tweet_json.txt', lines = 'True')$ tweet_df = tweet_df_raw[['id', 'retweet_count', 'favorite_count']]
from pyspark.ml.feature import StringIndexer, VectorIndexer
print(dfd.in_pwr_47F_min.describe())$ dfd.in_pwr_47F_min.hist()
df = pd.read_csv('ab_data.csv')$ df.head()
results_1dRichards, output_R = S_1dRichards.execute(run_suffix="1dRichards_hs", run_option = 'local')
sites = pd.read_csv('../data/station.csv',$                    dtype={'HUCEightDigitCode':'str'})
a = news_df[news_df['Source Acc.'] == 'BBC']$ a.head()$ print(a['Compound Score'].sum())
new_page_converted.mean() - old_page_converted.mean()
!wget http://stat-computing.org/dataexpo/2009/2001.csv.bz2
x["A"]$ x.get("A")$ x.A # works only for column names that are valid Python variable names
df.query('MeanFlow_cfs < 50')
z_score, p_value = sm.stats.proportions_ztest(count = [convert_new,convert_old], nobs = [n_new,n_old], alternative ='larger' )$ print ("z_score:",z_score)$ print("p_value:",p_value)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\metrics.sas7bdat"$ df = pd.read_sas(path)$ df.head(5)
from scipy.stats import norm$ print(1- norm.cdf(z_score))$ print(norm.ppf(1-0.05))
for row in session.query(Measurement):$ 
last_values = grouped_months.last()$ last_values=last_values.rename(columns = {'Totals':'last_v_T'})$
(taxiData2.Fare_amount <= 0).any() # This Returns False, proving we have successfully changed the values with no negative$
df_ari = df_ari.reset_index(drop=True)
compressed_data = compress_data("loans_2007.csv", 5, dropcols=[], encodeval="ISO-8859-1", $                                 parsedatecols=["issue_d","earliest_cr_line","last_pymnt_d","last_credit_pull_d"])
zstat_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(zstat_score, p_value)$ print("The Pvalue for null Hypothesis is {}".format(p_value))
s_mountain = Series(np.arange(0,5),index=pd.date_range('2014-08-01', periods=5, freq="H",tz="US/Mountain"))$ s_eastern = Series(np.arange(0,5),index=pd.date_range('2014-08-01', periods=5, freq="H",tz="US/Eastern"))$ s_mountain
percipitation_year = session.query(Measurements.date,func.avg(Measurements.prcp)) \$              .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \$              .group_by(Measurements.date).all()
df2['user_id'].value_counts().sum()
print df.groupby('state')['amount',].sum().sort_values(by='amount', ascending=False)[0:10]$ print df.groupby('state')['donor_id',].count().sort_values(by='donor_id', ascending=False)[0:10]
df.sort_index(axis=0, ascending=True)
df2 = df$ mismatch_index = mismatch_df.index$ df2 = df2.drop(mismatch_index)
y_pred = lassoreg.predict(X_test)$ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
recalls = df[df['ACTION REQUESTED'].str.contains('Recall') == True]$ recalls.head(2)
df2[df2['group']=="control"]['converted'].mean()
ab_df.converted.mean()
pca_data = pca.transform(scaled_data)
df_arch_clean["source"].value_counts()$
ds = tf.data.TFRecordDataset(train_path)$ ds = ds.map(_parse_function)$ ds
le_indicators = wb.search("life expectancy")$ le_indicators.iloc[:3,:2]
cust_data[["MonthlyIncome","ID"]].head(5)
print pearsonr(weather.max_wind_Speed_mph[weather.max_gust_speed_mph >= 0], $                weather.max_gust_speed_mph[weather.max_gust_speed_mph >= 0])
pvt = pvt.drop(['ga:dimension2', 'customerId'], axis=1)$ pvt = pvt[['ga:transactionId', 'ga:date', 'customerName', 'productAndQuantity']]$ pvt
for urlTuple in otherPAgeURLS[:3]:$     contentParagraphsDF = contentParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True)$ contentParagraphsDF
print(regression_model.coef_)$ print(regression_model.intercept_)
df = df.fillna(0, subset=features_numeric)$ df = df.fillna("Unknown", subset=features_categorical)$ print(df['fault_code_type_3',].head(3))$
all_simband_data.to_csv('all_simband_data.csv', sep=',')
(p_diffs > actual_diff).mean()
temp_df=temp_df.set_index('date')$ temp_df.head()
df_ad_state_metro_1['sponsor_types'].value_counts()
def rescue_code(function):$     import inspect$     get_ipython().set_next_input("".join(inspect.getsourcelines(function)[0]))
df['created_at_time'] = pd.to_datetime(df.created_at)
cercanasAfuerteApacheEntre150Y200mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 150) & (cercanasAfuerteApache['surface_total_in_m2'] < 200)]$ cercanasAfuerteApacheEntre150Y200mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre150Y200mts.apply(descripcionDistancia2, axis = 1)$ cercanasAfuerteApacheEntre150Y200mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
! rm -rf /home/ubuntu/s3/flight_1_5/extracted/flight_1_5_price_2017-05-10_.pq$
flight6 = spark.read.parquet("/home/ubuntu/parquet/flight6.parquet")$
Measurement = Base.classes.measurement$ Station = Base.classes.station
very_pop_df = au.filter_for_support(popular_trg_df, max_times=5, min_times=3)$ au.plot_user_dominance(very_pop_df)
tobs_stn_281_df = pd.DataFrame.from_records(tobs_stn_281, columns=('Date','Station','Tobs'))$ tobs_stn_281_df.head()
df = pd.DataFrame({'A': rng.rand(5),$                   'B': rng.rand(5)})$ df
t3['timestamp'] = t3.index$ t3.reset_index(drop=True,inplace=True)
pres_df['state'].head()
df_new = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') #joining countries_df with df2$ df_new.head()#displays first 5 rows of df_new
gmap = gmplot.GoogleMapPlotter(28.68, 77.13, 13)
ax=temp_df.plot.area(stacked=False, alpha=0.2)$ ax.xaxis.set_tick_params(rotation=30)$ plt.tight_layout()
FOX = news_df.loc[(news_df["Source Account"] == "FoxNews")]$ FOX.head(2)
print(train.age.describe())$ print(len(train[train["age"]>90])) #jc: 1.2% of age above 90, drop these? team decision$ train[train["age"]>90]$
Measurements = Base.classes.hawaii_measurement$ Stations = Base.classes.hawaii_station$
y_size = fire_size['SIZECLASS']$ x_climate = fire_size.loc[:, 'glon':'rhum_perc_lag12']
df = pd.read_csv('ab_data.csv')$ df.head()
dicttagger_sentiment = DictionaryTagger(['positive.yml','negative.yml'])
Asending_order_df = data_set_1.sort_values(['year'],ascending=True)$ print (Asending_order_df.head()) # Showing First 5 Record on Shorting the data
p_new = df2.converted.mean()$ p_new
df.shape[0]
df2.converted[df2.group == 'treatment'].mean()
random_integers = rng.randint(1, 10, size = 16).reshape(4, 4)
weather.info()
logregmodeltf = 'logregmodeltf.sav'$ pickle.dump(pipeline, open(logregmodeltf, 'wb'))
tlen.plot(figsize=(16,4), color='r');
newfile = newfile.loc[newfile['Delivery block'].notnull()]
df3 = df2.copy()
train['From-To'] = train.apply(lambda row: row[1]+'-'+row[2], axis = 1)$ test['From-To'] = test.apply(lambda row: row[1]+'-'+row[2], axis = 1)
fig1, ax1 = plt.subplots()$ ax1.pie(stage_summary['count'], labels = stage_summary['stage']) $ fig1.savefig('Stage Pie Plot')$
train_dum_clean = train_dum[['ID'] + [col for col in train_dum.columns if 'Source_' in col or$                       'Customer_Existing_' in col]]
tweet_df = pd.DataFrame(twitter_list)$ tweet_df.head()
import pandas as pd$ users_df = pd.read_csv('data/{country}_users.csv'.format(country=country).replace(' ', '+'))$ users_df.sample(5)
p_new = df2.converted.mean()$ p_new
raw_final["Tracking Tag Name"].unique()
archive_copy['timestamp'] = pd.to_datetime(archive_copy['timestamp'])$ archive_copy.info()
dfs[25].describe()
pres_df.to_pickle('data/pres_sorted_data.pickle')
df.isnull().values.any()$ df.info()
londonDFSubsetWithCounts.ix[:, ['OA11CD', 'count', 'lng', 'lat', 'POPDEN', 'WD11NM_BF']].to_csv('instagram-london-counts-by-OA.csv')
movies.sample(5)
sdsw = sd[(sd['JOB_TITLE'].str.contains('SOFTWARE')) | (sd['JOB_TITLE'].str.contains('PROGRAMMER')) | (sd['WAGE_UNIT_OF_PAY'] == 'Year')]$ sdsw.sample(50)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()$
IQR = scores_thirdq - scores_firstq$ print('The IQR of the ratings is {}.'.format(IQR))
df_small = raw_data[raw_data.num_tubes < sets_threshold]$ df_sets = raw_data[raw_data.num_tubes >= sets_threshold]
print("Number of rows and features", tipsDFslice.shape)
tipsDFslice = tipsDF[:50000]$ tipsDFslice.tail()
df2.groupby('group').describe()
!ls ../input/ #MacOS command$
df = pd.read_excel("mails taggen.xlsx")
target_city = {"lat": 35.227724, "lng": -80.839699} $ target_coords = f"{target_city['lat']},{target_city['lng']}"$
df.info()
walk['2014-08-01 00:00'].mean()
googClose = goog.reset_index(level=0, drop=True)$ googClose = googClose['Close']$ googClose
tweet_json.describe()
dataset[dataset['place_country'].isnull() == True]
df['intercept'] = 1$ df[['control', 'treatment']] = pd.get_dummies(df['group'])
zipcodes = xmlData['zipcode'].value_counts().reset_index()$ zipcodes.columns = ['zipcodes', 'count']$ zipcodes[zipcodes['count'] < 5]
df=dbobject.postgre_to_dataframe('select * from tripadvisor')$ df.head(200)
from sklearn.metrics import accuracy_score$ accuracy_score(y_train, y_predict)$
scidat = pandas.read_csv('./scientists_1.csv')$ print(scidat.columns)$
measurement = Base.classes.measurement$ station = Base.classes.station
df.dropna(axis=1, thresh=1000).shape
df_merge.per_student_budget.describe()
bins = [70, 72, 74, 76, 78, 80]$ temp_freq_dict={"Temperature":temp_list, "Frequency":frequency }$ temp_freq_df=pd.DataFrame(temp_freq_dict)
vect = CountVectorizer(stop_words=stop_words, ngram_range=(2,4), min_df=0.0001)$ X = vect.fit_transform(fashion.text)
df_new = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner')$ df_new.head()
lg = sm.Logit(df_new["converted"], df_new[[ "intercept", "ab_page", "CA", "UK",]])$ res = lg.fit()$ res.summary()
matt2 = dta.t[(dta.b==40) & (dta.c==2)]
print(airquality_pivot.index)
df.xs(key='x', level=2)
tweets_clean['user'] = user_info['id']$ tweets_clean.head()
df2.drop(labels=1899, axis=0, inplace=True)
Measurement = Base.classes.measurement$ Station = Base.classes.station$ session = Session(engine)
for col_x in np.arange(len(col_list)):$     df.rename(columns={col_list[col_x]: rename_list[col_x]}, inplace=True)
df = engine.get_data(**opts)
df_treat = df2.query('group=="treatment"')$ y_treat = df_treat["user_id"].count()$
filter_df = all_df.copy()$ filter_df.head(2)
from pyspark.streaming import StreamingContext$  $ streamContext = StreamingContext(SpContext,3) # micro batch size is 3 seconds here . It will receive an RDD every 3 seconds
df = df.merge(pd.get_dummies(df['Device']),left_index=True,right_index=True,how='left')$ print ('Device',df.shape)
(r_clean * hist_alloc).sum(axis=1).hist(bins=50)
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']])$ results = logit_mod.fit()$ results.summary()
@pyimport numpy as np$ a = PyObject(np.array(1:10))$ jpyArr = a[:reshape](5,2)  # uses the Python reshape method for an ndarray object (not Julia's reshape function)
miss = df.isnull().values.any()$ print ("Missing Values : {}".format(miss))
vip_reason = questions['vip_reason'].str.get_dummies(sep="'")
import pandas as pd$ git_log = pd.read_csv('datasets/git_log.gz',sep='#', header= None,encoding='latin-1',  names=['timestamp', 'author'])$ git_log.head()
t2['p1'] = t2['p1'].str.replace('_', ' ')$ t2['p2'] = t2['p2'].str.replace('_', ' ')$ t2['p3'] = t2['p3'].str.replace('_', ' ')
NewConverted = df2.query('converted == 1')['user_id'].count() $ HAlte = (NewConverted/NewTotalUser)$ print("Alternate Convert Rate: ",HAlte)
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new,convert_old],nobs=[n_new,n_old],alternative='larger')$ print(z_score,p_value)
print ("The original dtype of dateCrawled is " + str(rawautodf.dateCrawled.dtype)) # checked datatype$
cityID = '7068dd9474ab6973'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Toledo.append(tweet) 
train_data.dtypes
Train.fillna(0, inplace=True)$ Test.fillna(0, inplace=True)
p_diffs = np.array(p_diffs)$ plt.hist(p_diffs);
adjmats, timepoints = pt1.runltvmodel(rawdata, numwins=10)
df.tail(5)
df_3.to_csv('LBDLduplicitepermanente_2016_day_media.csv')$
temps1 = pd.Series([80,82,85,90,83,87], index = dates)$ temps1
averages = means.reset_index(drop=False)$ averages
df_ad_airings['race'].value_counts()$
df2.loc[df['user_id'] == 773192]
attend_with = questions['attend_with'].str.get_dummies(sep="'")
df_genre = df['genre'].to_frame()$ print(df_genre)
! mkdir census_data$ ! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data --output census_data/adult.data$ ! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test --output census_data/adult.test
ks_projects['name_length'] = ks_projects['name'].apply(lambda x: len(str(x))) $ ks_projects.head(5)
df_con1 = df2.query("converted=='1'") $ x_ = df_con1["user_id"].count()$ x_$
new_page_converted = np.random.choice([1,0], size=sample_size_new_page, p=[p_new_null,1-p_new_null])
list(festivals.columns.values)
com_eng_df['issues_closed_total'].plot()
df.query("group == 'treatment' and landing_page == 'old_page'").count()[0] + df.query("group == 'control' and landing_page == 'new_page'").count()[0]$
twitter_archive_df_clean['timestamp'] = twitter_archive_df_clean['timestamp'].map(lambda x: pd.to_datetime(x).date())
obs_diff = (df2_treat - df2_control)$ (p_diffs > obs_diff).mean()$
type(df_clean['date'].iloc[0])$ type(df_clean['time'].iloc[0])
(session.query(Measurement.station, Station.name, func.count(Measurement.station))$  .filter(Measurement.station==Station.station)$  .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all())
sales_update_nan.plot.scatter(x='Volume Sold (Liters) sum',y ='2015 Sales')
df_new1.monthly_price.fillna(0, inplace=True)$ print(df_new1.head(5))$
data["log_acct_age_weeks"] = (data["user_created_at"] - data["created_at"]).apply(lambda x: math.log((x.days / -7) + 1))
api = twitter.Api(consumer_key = consumer_key, consumer_secret=consumer_secret,$                  access_token_key=access_token,access_token_secret = access_token_secret,$                   sleep_on_rate_limit=False)
df_predictions=pd.read_table(r.url,sep='\t')
df_new[['ca', 'uk', 'us']] = pd.get_dummies(df_new['country'])$ df_new.head()
ra_min_ogle_fix = 250.0; ra_max_ogle_fix = 283.0;$ dec_min_ogle_fix = -40.0; dec_max_ogle_fix = -15.0;
lm = sm.Logit(new_df2['converted'], new_df2[['intercept','ab_page']])$ reg_lm = lm.fit()
fld_student = np.array(np.unique(d['If I could be any type of scientist when I grow up, I would want to study:']))$ fld_student_all = d['If I could be any type of scientist when I grow up, I would want to study:']
pattern = re.compile('AA')$ print(pattern.search('AAbcAA'))$ print(pattern.search('bcAA'))
product_color_grouped = paid_success_with_data.groupby(['MVGR6']).size().sort_values(ascending=False)$ product_color_grouped = product_color_grouped[product_color_grouped > 1]$ product_color_grouped.head()
tweet_archive_clean = tweet_archive.copy()$ tweet_image_clean = tweet_image.copy()$ tweet_df_clean = tweet_df.copy()
s = pendulum.datetime(2017, 11, 23, 0, 0, 0, tzinfo='US/Eastern') # Thanksgiving day$ e = pendulum.datetime(2017, 11, 25, 23, 59, 59, tzinfo='US/Eastern') # Small Businees Saturday$ Th_BF_Sa = tweets[(tweets['time_eastern'] >= s) & (tweets['time_eastern'] <= e)]
train_df.columns[train_df.isnull().any()].tolist()
p_diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_new, 10000)/n_old$ 
transactions.merge(users,how="left",on="UserID")
master_file['FILENAME'].value_counts()
rejected.shape, approved.shape
S = "create table available_bikes(execution_time INT, "+", ".join(station_ids)+")"
predictSVR = svr_rbf.predict(np.concatenate((X_train,X_validation,X_test),axis=0))$ predictANN = ANNModel.predict(np.concatenate((X_train,X_validation,X_test),axis=0))#one can ignore this line if ANN is $
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3.to_hdf('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.h5', key='df', mode='w')
X= df_loantoVal.drop(['First_Payment_Date' ,'Maturity_Date','LoanToValue'], axis=1)$ y = df_loantoVal['LoanToValue']
tmp = data_df.copy()$ tmp.columns = [x if re.search("size", x) else "data_{}".format(x) for x in tmp.columns]
yc_new4 = yc_new3[yc_new3.tipPC > 1]
spp = pd.read_excel('input/Data.xlsm', sheet_name='43', header=11, skipfooter=8793)
total_rows = len(df)$ print("Total number of rows in the dataset:{}".format(total_rows))
myTxns = myTxns.drop(columns=['User','Gender','Registered','Cancelled'], axis=1)$ myTxns
most_active_stations = session.query(Measurement.station).\$                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).limit(1).scalar()$ print ("Station: " + str(most_active_stations), "has the highest number of observations")
cdate=[x for x in building_pa.columns if 'date' in x]$ cdate$
df2['intercept'] = 1$ df2['ab_page'] = df2['group'].replace(('treatment', 'control'), (1, 0))
sub_dataset[['WordCount']].describe()
zipincome['ZIPCODE'] = zipincome['ZIPCODE'].astype(float)
from dateutil.parser import parse $
pgh_311_data['NEIGHBORHOOD'].value_counts()
churned_unordered = unordered_df.loc[churned_unord]
park_attendance_mean = games_df.groupby('park_id').mean()['attendance'].to_frame()$ park_attendance_mean.reset_index(inplace=True)
df2['US_ab'] = np.multiply(df2['ab_page'],df2['US'])$ df2['UK_ab'] = np.multiply(df2['ab_page'],df2['UK'])$ df2.head()
count_hash = Counter()$ count_hash.update(clean_hashlist3)$ print(count_hash.most_common(30))$
grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)$ grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')$ grid.add_legend()
ddf = pd.read_csv('new_ddf.csv')
df['NDATE']=pd.to_datetime(df['DATE'],unit='s')
diff = [abs(list(r_close.values())[i] - list(r_close.values())[i-1]) for i in range (1,len(r_close))]$     $
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['Retweets'].values, index=data['Date'])
Base.classes.keys()
negatives = month['PERIOD ENTRIES'] < 0$ print negatives.sum()$
from statsmodels.tsa.arima_model import ARIMA$ model_6201 = ARIMA(dta_6201, (5, 1, 2)).fit() $ model_6201.forecast(5)[:1] 
df_ari['Date'] = df_ari.LastUpdatedC
sel = [Measurements.date, func.avg(Measurements.prcp)]$ initial_date = format_latest_date - timedelta(days=365) # This will be start date from 12 months before final date of 8/23/17$ prcp_data = session.query(*sel).filter((Measurements.date >= initial_date)).group_by(Measurements.date).order_by(Measurements.date).all()
flights2.loc[[1950, 1951]]
%matplotlib inline$ commits_per_year.plot(kind='line', legend=False, title='commits per year' )
TEST_DATA = SHARE_ROOT + 'test_dataframe.pkl'$ df.to_pickle(TEST_DATA)
last_values_liberia = grouped_months_liberia.last()$ last_values_liberia=last_values_liberia.rename(columns = {'National':'last_v_T'})$
df_enhanced = df_enhanced.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis=1)
clean_merge_df.to_csv("Desktop/Project-2/spotify_merged_data.csv", index=False, header=True)
building_pa_prc_shrink.dtypes
null_mean = 0$ p_value = (null_valls > actual_obs_diff).mean() $ p_value
tfidf_matrix = tfidf.fit_transform(df_videos['video_desc'])$ tfidf_matrix.toarray()
lims_df.columns
training_df = features[~features.gameId.isin(production_df.gameId)]
dataset.dtypes
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'UK', 'US']])$ results = logit_mod.fit()$ results.summary()$
df2.drop_duplicates(subset='user_id', keep='last', inplace=True)$ counts = df2.user_id.value_counts()$ len(counts[counts > 1])
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall()
from sklearn.externals import joblib$ joblib.dump(pca, '../models/pca_20kinput_6858comp.pkl')$ np.save('../models/crosstab_40937.pkl', crosstab_transformed)
res = df_imgs.tweet_id.value_counts() > 1$ res.value_counts()
results = requests.get(final_url,{'access_token':token})
INT['Create_Date'].min()
prec_us = prec_nc.variables['pr_wtr'][1, lat_li:lat_ui, lon_li:lon_ui]$ np.shape(prec_us)
total4=total.ix[(total['RA0']<50) & (total['RA0']>15)]$ total4.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/objs_miss_fall2016_ra_gt_1.csv',index=False)
countries_df = pd.read_csv('countries.csv')$ countries_df.head(5)
all_df = test_set.append(properties)$ len(all_df)
sqcities = np.square(cities)$ print(sqcities)
giss_temp = giss_temp.drop("Year.1", axis=1)$ giss_temp
cbs_df = constructDF("@CBS")$ display(constructDF("@CBS").head())
le_data_all.index.levels[0]
cercanasAfuerteApache = dataLatLon.loc[(dataLatLon['distanciaAfuerteApache'] > 1000) & (dataLatLon['distanciaAfuerteApache'] < 2000), :]$ cercanasAfuerteApache.loc[:, 'price'] = cercanasAfuerteApache['price_usd_per_m2'] * cercanasAfuerteApache['surface_total_in_m2']$ cercanasAfuerteApache
reviews_w_sentiment=reviews_w_sentiment[reviews_w_sentiment.comments != ' ']$ reviews_w_sentiment[reviews_w_sentiment['date']>='2018-01-01'].head()$ len(reviews_w_sentiment)
p = data_s[data_s['isvalid']>0].groupby('customer')['date'].agg({"first": lambda x: x.min(),"last": lambda x: x.max()}).reset_index()
OldPage = np.random.choice([1, 0], size=old, p=[pMean, (1-pMean)])$ old_avg = OldPage.mean()$ print()
df_merge.drop(df_merge[df_merge.jpg_url.isnull()].index,axis=0,inplace=True)$
from bmtk.analyzer import nodes_table$ nodes_table('network/recurrent_network/nodes.h5', 'V1')
sel = [Measurement.station, func.count(Measurement.station)]$ stations_and_tobs = session.query(*sel).group_by(Measurement.station).order_by(-func.count(Measurement.station)).all()$ stations_and_tobs
(p_diffs > obs_diff).mean()
total = df.shape[0]$ print('The dataset consists of {} rows.'.format(total))
df_agg= df_merge.groupby('start_date').agg({'id_x':'count','mean_temperature_f':'mean','cloud_cover':'mean'}).reset_index()$ df_filter = df_agg[(df_agg['start_date']<='2013-09-30')&(df_agg['start_date']>='2013-09-01')]$ df_filter['start_date'] = pd.to_datetime(df_filter['start_date'])
for addr, geo in zip(addresses_df.index.levels, df.index.levels):$     addr.name = geo.name
tag_df = tag_df.sum(level=0)$ tag_df.head()
separated_dog_stage = twitter_archive_clean[['doggo','floofer','pupper','puppo']].copy()
(act_diff < p_diffs).mean()
session.query(Measurement.tobs).order_by(Measurement.tobs).first()
Base.classes.keys()
i = issues$ thisyear = i[(i.activity > '2015-01-01')]$ len(thisyear)
with open("datasets/git_log_excerpt.csv", "r") as file:$     print(file.read())
duplicate_bool = df2.duplicated(subset=['user_id'], keep='first')$ df2_duplicated_row = df2.loc[duplicate_bool == True]$ df2_duplicated_row
pd.Period('1/1/21', 'D') - pd.Period(pd.datetime.today(), 'D')
df_new['timestamp'].min(), df_new['timestamp'].max()
!wc -l $ml_data_dir/ratings.csv
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new = df_new.drop(['CA'], axis=1)$ df_new.head()
df_schools.describe()
prcp_df =pd.read_sql_query(prcp_data, session.bind)$ prcp_df.set_index('date',inplace=True)$ prcp_df.head()
mergde_data=pd.merge(game_meta,labels_group , right_index=True, left_index=True)$ merge_val=pd.merge(game_meta,game_vail,right_index=True, left_index=True)$
    $ eng = create_engine("sqlite:///hawaii.sqlite")
nba_df.drop([30,31], axis=0, inplace= True)$ nba_df.loc[28:34]
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])$ df_new.head()
reviews_w_sentiment = pd.merge(reviews, pd.DataFrame(sentiments, columns=['sentiment_score']),left_index=True, right_index=True)$ reviews_w_sentiment['date']  = pd.to_datetime(reviews_w_sentiment['date']) $ len(reviews_w_sentiment)
df.sort_values(by=['Year', 'Display Value'])
start_t = '2018-05-01 00:00:00'$ end_t=pd.to_datetime('today')- timedelta(days=1)$ end_t1=str(end_t)$
digitalgc_tweets = api.search(q="#digitalgc", $                               count=100, lang="en", $                               since="2018-01-01")$
df3.groupby(['TUPLEKEY2', 'DATE']).sum()
M7_RMSE,M7_pred,M7_actual = ts_walk_forward(training_X_scaled,training_y.values,15,15,regr_M7)
ng_stor_df = CreateDataFrame(ng_stor_json).df$ print(ng_stor_df.head(n=20))
pdiff = new_page_converted.mean() - old_page_converted.mean()$ pdiff
engine = create_engine("sqlite:///hawaii.sqlite")
sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')$
for c in ccc:$     vhd[c] = vhd[vhd.columns[vhd.columns.str.contains(c)==True]].sum(axis=1)
mbti_text_collection.to_csv('Reddit_mbti_data_2.csv',encoding='utf-8')
final_df = df3.join(dummy_country)$ final_df.head()$
precip_df = history_df['precipitation']$ precip_df.describe()
properati= properati.loc[properati['place_name'] == le_barrio.transform(['Flores'])[0]] #porque devuelve un array
accidents_df['HOUR_LABEL'] = accidents_df.apply (lambda row: label_hour (row),axis=1)
INSERT INTO payment (booking_id, payment_date, payment_method, payment_amount)$ VALUES (8, TO_DATE('2017-09-10', 'YYYY-MM-DD'), 'BPay', 741.96)$
AFX_X_2017_dict = AFX_X_2017_r.json()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ z_score, p_value
cont_prob = df2[df2['group'] == 'control']['converted'].mean()$ cont_prob
plotdf.loc[plotdf.index[-1], 'forecast'] = hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'meanRemainingTweets'].iloc[0] + plotdf['currentCount'].max()$ plotdf.loc[plotdf.index[-1], 'forecastPlus'] = plotdf.loc[plotdf.index[-1], 'forecast'] + hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0]$ plotdf.loc[plotdf.index[-1], 'forecastMinus'] = plotdf.loc[plotdf.index[-1], 'forecast'] - hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0]
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
df2.set_index('user_id').index.get_duplicates()
base_df = DataObserver.major_df$ base_df_body = DataObserver.major_df['body']
df_select = df[select5features].copy(deep=True)
num_data=pd.DataFrame(df[['sale','volume_sold_l','bottles_sold','state_cost','state_retail','bottle_ml']],index=None)$ num_data.head()
df2.drop(labels=2893, inplace=True)
yearmonthcsv=yearmonth_sumsizecsv.select(yearmonth_sumsizecsv['yearmonth'],(yearmonth_sumsizecsv['sum(size_in_KB)']/1000).alias("total_size_in_MB"))
new_doc = repos[4]$ new_vec = dictionary.doc2bow(new_doc)$ repos_langs.iloc[4, 0] # Para testar nossa hipotese vamos encontrar usuarios similares ao usuario 4
newdf.groupby([newdf.Hour_of_day]).Trip_distance.mean()
tweet_df.count()
h = plt.hist(tag_degrees.values(), 100) #Display histogram of node degrees in 100 bins$ plt.loglog(h[1][1:],h[0]) #Plot same histogram in log-log space
MemberEvent_payload = pd.DataFrame(MemberEvent['payload'].tolist())$ MemberEvent_payload['action'].unique()
to_pickle('Data files/clean_dataframe.p',adopted_cats)$ adopted_cats=from_pickle('Data files/clean_dataframe.p')
with open('yelp_academic_dataset_business.json') as f:$     data = [x.rstrip() for x in f.readlines()]
df_usage.info()
df.groupby(['year','month']).agg([sum])$
small_train = pd.merge(small_train, dests_small, on='srch_destination_id', how='left')$ small_train.isnull().sum(axis=0) #how many nulls were introduced?
pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])$
num_clusters = 3$ km = KMeans(n_clusters=num_clusters, random_state=0).fit(tfidf_matrix)#TODO$ clusters = km.labels_.tolist()
var_per_portfolio = res.groupBy('date', 'portfolio').sum()$ var_per_portfolio = var_per_portfolio.map(lambda r: (r[0], r[1], r[2], float(var(np.array(r[3:]) - r[2]))))$ var_per_portfolio = sql.createDataFrame(var_per_portfolio, schema=['date', 'portfolio', 'neutral', 'var'])
temp_df = pd.DataFrame(data = us_temp)$ temp_df.columns = ts.dt.date[:843]
logit_countries_model = sm.Logit(df4['converted'], df4[['country_UK', 'country_US', 'intercept']])$ results_countries = logit_countries_model.fit()
titanic3 = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls')$ titanic3.head()
%cd "C:\Users\Safaa\Desktop\PythonStuff\10-16-2017-Gw-Arlington-Class-Repository-DATA\Homework\11-Adv-Data-Storage-Retrieval\Instructions\Resources"$ measure = "hawaii_measurements.csv"$ station = "hawaii_stations.csv"
ttarc_clean[(ttarc_clean.doggo == 3) & (ttarc_clean.pupper == 2)].head()$
df_complete_agg =df_complete_set.loc[:,[1,'Tweets Ago','compound']]$ df_complete_agg.rename(columns={'compound' : 'Tweet Polarity'  ,1 : 'News Agency' },inplace=True)$ df_complete_agg.head()$
sf = read.getSamplingFeatures(codes=['RB_1300E'])[0]$ type(sf)
df.converted.mean()
from sklearn import linear_model$ lin_reg = linear_model.LinearRegression()$ lin_reg.fit(x_train,y_train)
rain_df = pd.DataFrame(rain)$ rain_df.head()
import statsmodels.api as sm$ convert_old = df2.query('group == "control" & converted == 1')['user_id'].nunique()$ convert_new = df2.query('group == "treatment" & converted == 1')['user_id'].nunique()$
tweet_df = pd.read_json('tweet_json.txt',orient='index')
df_sched = pd.read_csv(schedFile, usecols = schedCols, $                  dtype = sched_dtypes)
violations_list = pd.read_excel("../data/Codes' List of Violations 15 aug 2017_edit.xlsx")
df_nott = df.query('landing_page == "old_page"')$ df_4 = df_nott.query('group != "control"')$ df_4.nunique()$
df = pd.read_csv("https://raw.githubusercontent.com/YingZhang1028/practicum/master/Data_DataMining/train_score.csv")$ train_df["description_score"] = df["description_score"]$ train_df["description_score"].ix[np.isnan(train_df["description_score"]) == True] = train_df["description_score"].mean()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print('z-score:', z_score)$ print('p-value:', p_value)
airbnb_df['host_acceptance_rate'] = np.random.randint(70,100, size=(airbnb_df.shape[0],1))$ airbnb_df['host_response_rate'] = np.random.randint(70,100, size=(airbnb_df.shape[0],1))
new_cont = df.query("landing_page == 'new_page' and group == 'control'")$ old_trt = df.query("landing_page == 'old_page' and group == 'treatment'")$ len(new_cont)+len(old_trt)
oldPage_df = df2.query('landing_page == "old_page"')$ n_old = oldPage_df.shape[0]$ n_old
bloomfield_pothole_data.info()
from functools import reduce$ dfs = [df_CLEAN1A, df_CLEAN1B, df_CLEAN1C] # lift of the dataframes$ data = reduce(lambda left,right: pd.merge(left,right,on='MATCHKEY', how='inner'), dfs)$
meanMonthlyReturns = dict( { 'Infy': monthly_gain_summary.Infy.mean(), 'Glaxo': monthly_gain_summary.Glaxo.mean(),$                           'BEML': monthly_gain_summary.BEML.mean(),'Unitech': monthly_gain_summary.Unitech.mean()  } )
!ln -s -f ~/.local/lib/python2.7/site-packages/systemml/systemml-java/systemml-1.0.0-SNAPSHOT-extra.jar ~/data/libs/systemml-1.0.0-SNAPSHOT-extra.jar$ !ln -s -f ~/.local/lib/python2.7/site-packages/systemml/systemml-java/systemml-1.0.0-SNAPSHOT.jar ~/data/libs/systemml-1.0.0-SNAPSHOT.jar
eth = pd.read_csv("Ether-chart.csv", sep=',')$ eth['date'] = ' '$ eth.info()$
TripData_merged = pd.concat([TripData1, TripData2, TripData3])
merged_data = pd.merge(companies_pf, funding_pf,how="inner",on="Company")$
all_data_merge = pd.merge(all_data, mapping, on = ['rpc'])$ all_data_merge.shape
precip = session.query(Measurement.date, Measurement.prcp).group_by(Measurement.date).\$                     having(Measurement.date.like('2016%')).all()$ precip$
df=pd.read_csv('ab_data.csv')$ df.head()
assert len([col for col in cols_to_drop if col in twitter_archive_clean.columns])==0
wqYear = dfHawWQ.groupby('Year')['TotalN'].mean()$ dfAnnualN = pd.DataFrame(wqYear)
df = df[pd.isnull(df['retweeted_status_id'])]$ df.shape[0]
s = gp.GeoSeries([Point(x,y) for x, y in zip(delays_geo['Longitude'], delays_geo['Latitude'])])$ delays_geo['geometry'] = s$ delays_geo.crs = {'init': 'epsg:4326', 'no_defs': True}
random_integers.max(axis=1)
accumulated_num, frequency = split_by_time(dates, periode= 60 * 60 * 20)$ print 'There are totally '+str(len(frequency)) + ' periods.'$
r = requests.get(url1)$ json_data = r.json()
lr = LogisticRegression(random_state=20, max_iter=10000)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr', 'multinomial'], 'solver':['saga','newton-cg', 'lbfgs']}$ grid_tfidf = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
cnn_compound=df.loc[df.company=='CNN']['compound']$ cnn_when=df.loc[df.company=='CNN']['tweets_ago']$ print(cnn_when.values)
total.load_from_statepoint(sp)$ absorption.load_from_statepoint(sp)$ scattering.load_from_statepoint(sp)
p_old = df2['converted'].mean()$ p_old
df2[df2['group']=='control']['converted'].mean()$
url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"$ r = requests.get(url)$
all_df.to_pickle('data/all_political_ads.pickle')$
session.query(func.count(Station.id)).scalar()
df_out = pd.merge(df_userid,df_Tran,how='outer',on="Key")[['UserID','ProductID']]
session.query(func.count(Measurement.date)).all()
prcp_df.describe()
rat_data.head()
json_data = r.json()$ json_limit = r_limit.json()$ pp.pprint(json_limit)
result=results.set_index(['date','home_team','away_team'])$ result.head()
check_rhum = rhum_fine[1]$ sns.heatmap(check_rhum)
df2=df2.drop_duplicates(['user_id'])$ df2.shape[0]$
tweetid_series = ttarc.tweet_id
gdax_trans_btc = gdax_trans.loc[gdax_trans['Account_name'] == 'BTC',:]
data_2018 = data_2018.rename(columns={'Unnamed: 0':'time'})
Base.classes.keys()
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline',$                         xls_dir='scen2xls', file_name='data.xlsx', verbose=False)
max_tweets=1$ for tweet in tweepy.Cursor(api.search,q="ivanka").items(max_tweets):$     print(tweet)
print(imgp_clean[(imgp_clean.p1_dog == True) | (imgp_clean.p2_dog == True) | (imgp_clean.p3_dog == True)].shape)$ print(imgp_clean.shape)
active_with_return.iloc[:,0] = active_with_return.iloc[:,0].astype("int")
tweets_predictions_all.info()
pgh_311_data.index = pd.to_datetime(pgh_311_data['CREATED_ON'])$ pgh_311_data.head()
filepath = os.path.join('input', 'input_plant-list_NO.csv')$ data_NO = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_NO.head()
print contractor_clean['zip_part1'].head(); contractor_clean['zip_part2'].head()$
air = pd.merge(air_store_info, air_reserve, on='air_store_id', how='outer') # merging air data store_id wise$ hpg_1 = pd.merge(hpg_store_info, hpg_reserve, on='hpg_store_id', how='outer') # merging hpg data store_id wise$ hpg = pd.merge(hpg_1, store_id_relation, left_on = "hpg_store_id", right_on = "hpg_store_id", how='outer') # mapping hpg store_id to air store_id$
parsed_locations[parsed_locations.country.isnull()]
df_null_acct_name = df[df['LinkedAccountName'].isnull()]
stat_info_city = stat_info[1].apply(fix_space)$ print(stat_info_city)
pp.pprint(tweet._json)
import matplotlib.pyplot as plt$ import seaborn as sns
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113515380318786.json', encoding="utf-8-sig") as data_file:$     json_data3 = j.load(data_file)
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).tail(5)
!head -1000 2001.csv > temp.csv
data['Open'].min()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=YxSY4z-2vyxvJ15WjtFa&start_date=2017-01-01&end_date=2017-12-31"$ req = requests.get(url)$ req.text
movies.to_csv('..\\Output\\CleanedMovies.csv')
df.index = pd.to_datetime(df.index)
engine.execute("select * from station limit 5").fetchall()
df.iloc[[11,24,37]]$
df_daily.dropna(subset=["PREV_DATE"], axis=0, inplace=True)$ df_daily.head(5)
mean_of_tweets = data.mean()
pax_raw.paxcal.value_counts() / len(pax_raw)
stand_err = std_w/np.sqrt(len(hm_data))$ z_score = (mean_w - 85.)/stand_err$ print stand_err, z_score
df = pd.read_csv(project_path + '/data/raw/data.csv', index_col=0)$ df.head()
training_set, test_set = newdf[newdf['date']<split_date], newdf[newdf['date']>=split_date]$ training_set = training_set.drop('date', 1)$ test_set = test_set.drop('date', 1)
RunSQL(sql_query)$ actor = pd.read_sql_query(sql_query, engine)$ actor.head()
inflex_words = en_translation_counts[en_translation_counts < 2]$ print("Total: {} unique words".format(len(inflex_words)))$ inflex_words.sample(10)
mv_lens.title.value_counts().head()
my_columns = list(df_.columns)$ print(my_columns)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]$
df = pandas.DataFrame(data['11']['data']['month']['data'])
engine = create_engine("sqlite:///hawaii.sqlite")$
!pip install --upgrade git+https://github.com/GeneralMills/pytrends$ !pip install lxml
db = client["clintrials"]$ coll = db["clinical_studies"]$ client["clintrials"].command("listCollections")
ibm_hr_cat = ibm_hr_no_numerical.select(categorical_no_target)$ ibm_hr_cat.show(3)
pd.set_option('display.max_colwidth', -1)$ df.loc[df.Sentiment==1, ['description','Sentiment']].head(10)
probs_test[:, 1].mean()
dataFiltrada["colectivos_cercanos"] = paradasColectivos.loc[:, ["lat", "lon"]].apply(colectivosCercanos, axis = 1)
df_final_.state.value_counts()$
from scipy.optimize import curve_fit$ popt, pcov = curve_fit(sigmoid, xdata, ydata)$ print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
autos = autos.drop(index = impossible_year)
ideas = categorical.join(ideas)
print('dataframe shape: {}'.format(country.shape))$ country.head()
df2.drop(labels=1899, axis=0, inplace=True)$ df2[df2['user_id']==773192]
cities = set(us_cities['City'].str.lower())$ states = set(us_cities['State full'].str.lower())$ counties = set(us_cities['County'].str.lower())
df_dates = df.loc[(df["C/A"] == 'A002')&(df.UNIT == 'R051')&(df.SCP=='02-00-00')&(df.STATION=='59 ST'),["DATE"]]$ df_dates = pd.to_datetime(df_dates.DATE, infer_datetime_format=True)
cityID = '44d207663001f00b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Mesa.append(tweet) 
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])$ print(old_page_converted)
tweet_df.iloc[:,1:15].sample(5)
bus.groupby("business_id").count()$
p_val = (p_diffs > (p_new-p_old)).mean()$ print('The proportion of p_diffs greater than the actual difference:',p_val)
df['w'].unique()
df.rename(columns={'PUBLISH STATES':'Publication STATUS','WHO region':'WHO Region'},inplace=True)$ df.info()
twitter_df.to_csv("twitter_data.csv")
full_df['photos_count'] = full_df.photos.apply(len)
random_forests_grid.fit(X_train,y_train)$
if search_results is not []:$     print(json.dumps(search_results[0]._json, indent=2))
temp_df2['titles'] = temp_df2['titles'].str.lower()
ngrams_summaries = cvec_2.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
normal_dist = np.random.normal(0, np.std(p_diffs), p_diffs.size)$ plt.hist(normal_dist);$ plt.axvline(diff, color='red');
len(kochdf.loc[kochdf['user'] == "Rezeptsammlerin"]['name'])
df2.groupby('landing_page').user_id.count()[0]/df2.user_id.count()
scaler = MinMaxScaler()$ data[intFeatures] = scaler.fit_transform(data[intFeatures])
users.merge(sessions, how='inner', left_on=['UserID','Registered'],right_on=['UserID','SessionDate'])
player_frame = baseball[baseball['team'].isin(['LAN', 'SFN'])]$ print(player_frame['player'])$ print('The number of records is: ',len(baseball[baseball['team'].isin(['LAN', 'SFN'])]))
for i in range(3):$     dfs[i]['SA'] = np.array([analyze_sentiment(tweet) for tweet in dfs[i]['Tweets']])
p_control = df2.query("group == 'control'")['converted'].mean()$ p_control
def tokenizer(text):$     return text.split()$ tokenizer('runners like running and thus they run')
df_daily[df_daily["ENTRIES"] < df_daily["PREV_ENTRIES"]].head()
varianceMonthlyReturns = dict( { 'Infy': monthly_gain_summary.Infy.var(), 'Glaxo': monthly_gain_summary.Glaxo.var(),$                           'BEML': monthly_gain_summary.BEML.var(),'Unitech': monthly_gain_summary.Unitech.var()  } )
wa_sales_decompose = sm.tsa.seasonal_decompose(wa_sales, freq=12, model='additive')$ decmpose_plot = wa_sales_decompose.plot()
df2017['MoneyLineAway-change'] = (df2017.duplicated(['EventID']) & (df2017['MoneyLineAway'] != df2017['MoneyLineAway'].shift(1)))$ df2017['MoneyLineHome-change'] = (df2017.duplicated(['EventID']) & (df2017['MoneyLineHome'] != df2017['MoneyLineHome'].shift(1)))
yc_new2 = yc_new1[['Fare_Amt', 'tripDurationHours', 'Trip_Distance',$        'Tip_Amt', 'income_departure', 'income_dest']]$ yc_new2.head()
files = glob.glob(os.path.join(input_folder, year_string))
p_val = (p_diffs > act_diff).mean()$ print("Proportion greater than actual difference: %.4f" %p_val)
iotc = pd.read_excel('IOTC_positive_list_2017-03-09.xls')
df[0:1000]['AQI'].mean()
df['upper'] = df['body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))$ df[['body','upper']].head()
test = pd.DataFrame(test_d)
p_diffs = np.array(p_diffs)
tree = DecisionTreeClassifier(criterion='gini')$ model = tree.fit(X_train_total, y_train)$ model.score(X_test_total_checked, y_test)
1/np.exp(-0.0211)$
df.tail(8)
countries['country'].value_counts()
orig_ct = len(dfd)$ dfd = dfd.query('hspf >= 10.0 and hspf <= 18')$ print(len(dfd) - orig_ct, 'eliminated')
test_input_fn = create_predict_input_fn(test_data, DEFAULT_BS)$ stats = classifier.evaluate(input_fn=test_input_fn)
df = pd.read_excel('data/analysis2/Dow.xlsx')$ df.head(3)$ df.shape
p_new = df2.converted.mean()$ p_new$
npath = '/glade/u/home/ydchoi/sopron_2018/notebooks/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb'$ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
stats.chisquare(gender_top_school_tab)
marvelPTags = wikiMarvelSoup.body.findAll('p')$ for pTag in marvelPTags[:8]:$     print(pTag.text)
sdf.createOrReplaceTempView("tempTable")$ res.show()
predictions = model.predict(Test_X)$ predicted_prob_true = predictions[:,1]$ print(predicted_prob_true)
df2 = df2.drop(146678)$ df2 = df2.reset_index(drop=True)$ df2.iloc[146678]
data_df = data_df[data_df["from.id"]==("368759307733")]
y.name
x.drop(["sum"], axis=1)
avg_test_pred2 = np.stack(test_pred2).mean(axis=0)$ print ("type(avg_test_pred2):", type(avg_test_pred2), avg_test_pred2.shape)$ print(avg_test_pred2[0:10, :])
plt.hist(p_diffs);$ plt.axvline(-0.00127,color='red')$
sp500 = pd.read_csv('sp500.csv', index_col='Symbol', usecols=[0,2,3,7])$ sp500.head()
tw['tweet_id'].map(lambda x: len(str(x).strip())).value_counts()
data_df.desc[17]
bufferdf = bufferdf.groupby([bufferdf.RateCodeID])
session.query(func.count(station.id)).scalar()
np.where(y > 10)
station_df = pd.read_sql(session.query(Measurement.tobs).filter(Measurement.station == 'USC00519281', Measurement.date >= '2016-08-23').statement,session.bind)$ station_df.head()
data_df.head()
pvt['customerId'] = pvt['ga:dimension2'].str.rpartition('-')[0].str.strip()$ pvt['customerName'] = pvt['ga:dimension2'].str.rpartition('-')[2].str.strip()$ pvt
no_of_obs_by_station = engine.execute('SELECT station, count(*)  FROM measurement group by station order by count(*) desc').fetchall()$ no_of_obs_by_station
lastyear_day = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ lastyear_day$
df_test_index = pd.merge(df_test_index[event_list['event_start_at'] > df_test_user['created_on']],$                             log_user1[event_list['event_start_at'] > df_test_user['created_on']], on='event_id', how='left')$ df_test_index
my_tree_one = tree.DecisionTreeClassifier()$ my_tree_one = my_tree_one.fit(features_one, target)
Base = automap_base()$ Base.prepare(engine, reflect=True)
year10 = driver.find_elements_by_class_name('yr-button')[9]$ year10.click()
df_countries.country.nunique()
from sklearn.naive_bayes import GaussianNB$ gnb = GaussianNB()$ gnb.fit(X_train, Y_train)
!curl -L -O  https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv$ !head -n 1 Consumer_Complaints.csv > CC_header.csv$ !tail -n +2 Consumer_Complaints.csv > CC_records.csv
A.add(B, fill_value=0)
df = pd.read_csv('GageData.csv', dtype={'site_no':'str'}) 
sdks = attribute_df(df, 'sdk')$ sdks.head()
s.values
local_sea_level_stations.columns
amazon_review = pd.read_csv('amazon_cells_labelled.tsv',sep='\t') $ amazon_review
print(trump.favorite_count.describe())$ print("   ")$ print(trump.retweet_count.describe())
df.info()
link = soup.find('li').find('a')['href']$ link
data[data.index.duplicated()]
price_series = pd.Series(unique_brand_dict)$ mileage_series = pd.Series(unique_brand_dict_mileage)
store_items.insert(5, 'shoes', [8, 5, 0])$ store_items
lons, lats = np.meshgrid(lon_us, lat_us)$ plt.plot(lons, lats, marker='.', color='k', linestyle='none')$ plt.show()
df_joined.groupby('country').converted.mean()
plot_q_table(q_agent_new.q_table)
contractor_clean[contractor_clean.city.isnull()] # The result is empty. $ contractor_clean.loc[contractor_clean['contractor_id'] == 139]$
print("Url: " + client.repository.get_model_url(saved_model_details))
import test_package.print_hello_function_container$ import test_package.print_hello_class_container$ import test_package.print_hello_direct # note that  the paths should include root (i.e., package name)$
for col in X_nnumcols:$     X[col] = X[col].apply(lambda l: hash(l))$     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
import matplotlib.pyplot as plt$ %matplotlib inline$ plt.style.use('fivethirtyeight')$
import statsmodels.api as sm
df2.date.value_counts().sort_index(axis = 0)
x_axis =  np.arange(len(inc_weath_df))$ x_axis
df_movies = pd.merge(df, xml_in_sample, left_on=['movieId'], right_on=['authorId'],  how='left')[['movieId', 'authorName']]$ df_movies.drop_duplicates(subset=None, keep='first', inplace=True)
print(gs.best_estimator_)
(p_diffs>act_diff).mean()
def load_train_pp(data_path=DATA_PATH,ssize):$     train_pp_path = os.path.join(data_path, "train_pp.csv")$     return pd.read_csv(train_pp_path,nrows=ssize)
tweets_df.iloc[3, 10]
with open("TestUser3.json","r") as fh:$     data = json.load(fh)$
datetime.time(datetime(2015,12,14,15,17,30))
texts = ''.join(str(e) for e in itemlist)$
high = max(v.Open for v in data.values() )$ low = min(v.Open for v in data.values())$ print('=>The high and low opening prices for this stock in 2017 were {:.2f} and {:.2f} '.format(high, low) + 'respectively.')
url = "http://sealevel.colorado.edu/files/2015_rel2/sl_ns_global.txt"$ global_sea_level = pd.read_table(url, sep="\s+")$ global_sea_level
X = [html.unescape(string) for string in X]
pgh_311_data['REQUEST_ID'].resample("M").count().plot()
set(tcga_target_gtex_labels.disease).intersection(treehouse_labels_pruned.disease)
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05)))$
p_new = df2.converted.mean()$ p_new
for_plt = tweets.set_index('created_at', drop=False, inplace=False)$ for_plt.created_at.groupby(pd.Grouper(freq='H')).count().plot(kind='bar', figsize=(20, 6))$
df_twitter_copy = df_twitter_copy.drop(['rating_numerator', 'rating_denominator'], axis = 1)
news_sentiment_df.to_csv("twitter_news_org_sentiment.csv", encoding="utf-8", index=False)
op_ed_articles = pd.read_json('nytimes_oped_articles.json')$ op_ed_articles = op_ed_articles.reset_index(drop=True)$ op_ed_articles.head()
Z = np.arange(11)$ Z[(3 < Z) & (Z <= 8)] *= -1$ print(Z)
np.sort(df2.date.unique())
ari_games2 = ari_games.sort_values('MatchTimeC', ascending=False).drop_duplicates('Game').sort_index()
fin_p.dropna(inplace=True)
converted = df.groupby(['converted']).nunique().user_id[1] # Number of converted users$ converted_proportion = (converted / number_of_users) * 100  # Get converted users proportion$ converted_proportion
df.columns = ['ID', 'name', 'category', 'Main category', 'currency', 'deadline',$               'goal', 'launched', 'pledged', 'State', 'Backers', 'country',$               'usd pledged', 'Pledged (USD)', 'Goal (USD)']
df_imputed = pd.DataFrame(df_imput)$
df['converted'].mean()
item = collection.item('AAPL')$ item
logit = sm.Logit(df3['converted'],df3[['intercept' ,'treatment']])
print(with_countries['country'].unique())$ with_countries[['CA','UK','US']] = pd.get_dummies(with_countries['country'])$ with_countries.head()
for col in X_nnumcols:$     X[col] = X[col].apply(lambda l: hash(l))$     X[col] = X[col].apply(lambda l: ((1+l)/(1+abs(l)))*(np.log(1 + abs(l))))
df_filtered_by_RT = df[~df.raw_text.str.startswith('RT')]$ df_filtered_by_RT.head(1)
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))
net.build()
weekday_listings = drace_df['weekday_created'].value_counts()$ sns.barplot(weekday_listings.index, weekday_listings.values, alpha=0.8)$
df['body_tokens'] = df['body_tokens'].apply(nltk.word_tokenize)$ print(df['body_tokens'])
learn.data.test_dl = test_dl$ log_preds_test = learn.predict(is_test=True);
X = pd.merge(X, expAndCoinByUser, on="userid")$ X.head(5)
old_page_converted = np.random.binomial(old_n, old_p)$ old_page_converted
fullDf.level.value_counts()$
shows['release_month'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%m'))$ shows['release_month'] = shows['release_month'].dropna().apply(lambda x: str(x))$ shows['release_month'] = shows['release_month'].dropna().apply(lambda x: int(x))
submit = pd.read_csv('mloutput20160428_0512.csv')$ print(submit.shape)$ submit.tail()
train,test = iris.split_frame([0.8])
all_tweets = df_Tesla['text'].values$ print(all_tweets[2])
with open('/Users/annalisasheehan/Dropbox/Climate_India/Data/data_child/lat_long_younglives_AS_cut.csv') as f:$     latlon = f.read().splitlines()
file_path = "2017 CAM data from iPads.xlsx"
print(voters.PermCategory.unique())$ voters.PermCategory.value_counts(dropna=False)
for v in data.values():$     if v['answers']['Q1'] == 'yes':$         v['answers']['Q1A'] = 0
cityID = 'a84b808ce3f11719'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Omaha.append(tweet) 
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK','ab_page']])$ results = mod.fit()$ results.summary()
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly RN/PA Hours Partial Autocorrelation')
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
type(data.Likes.value_counts())
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ act_diff 
QUIDS = pd.read_table("qids01.txt", skiprows = [1])
!wget https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt$ !head -n 5 ProductPurchaseData.txt
access_logs_df.createOrReplaceTempView('AccessLog')$
df2['intercept'] = 1$ df2.head()$
df_l['date'] = pd.to_datetime(df_l['date'], infer_datetime_format=True, format="%y-%m-%d", utc=True) $
stfvect.get_stop_words()
chgis.rename(columns={'src':'data_source'}, inplace=True)$ chgis.columns = ['tgaz_%s' % x for x in chgis.columns]$ chgis.columns
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator$ paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build()$ cvEvaluator = MulticlassClassificationEvaluator()$
sorted_measurements_df.to_excel(writer, '{}'.format(settings['ExposureTime']))$ writer.save()
bruins_postgame.to_csv('../../../../../CS 171/cs171-gameday/data/bruins_postgame.csv',   index=False)$ celtics_postgame.to_csv('../../../../../CS 171/cs171-gameday/data/celtics_postgame.csv', index=False)$ sox_postgame.to_csv('../../../../../CS 171/cs171-gameday/data/sox_postgame.csv',         index=False)
df['incident_zip'] = df.incident_zip.astype(int)$ df = df[df['incident_zip'] < 12000 ]
data.loc[data.density > 10, ['pop', 'density']]
newfile = pd.read_excel('export new.xlsx')$ oldfile = pd.read_excel('export old.xlsx')
newfile.insert(0, 'CSCA - Add notes here of customer requirement', '')$ newfile.insert(1, 'Direction from Diana', '')
train.MODELE_CODE.value_counts()
plt.xlabel('dog stage')$ plt.ylabel('count')$ plt.hist(df_merge.dog_stage);
pd.date_range(end='6/1/2016', periods=20)
predictions = model.transform(data)$ predictions.toPandas()$
crimes_by_yr_month = pd.DataFrame(datAll.groupby([datAll['year'],datAll['month']])$                                .agg({'Offense_count':'sum'}))
df.user_id.nunique()
df_pix4D = pd.read_json(basedirectory+projectname+'/'+Pix4D_filename, typ = 'series')$ images_json = df_pix4D['actual']['photos']
agg_function = {'budget':np.mean, 'reading_score':np.mean, 'math_score': np.mean, 'size': np.mean, 'passing_reading': np.sum, 'passing_math': np.sum}
bgf = BetaGeoFitter(penalizer_coef=0.0)$ bgf.fit(m['frequency'], m['recency'], m['T'])$ print(bgf)$
twitter_archive_with_json = pd.merge(twitter_archive_clean, tweet_json_clean, how='left', on=['tweet_id'])$
tag_df = tag_df.stack()$ tag_df
n_new = df2.query('group == "treatment"').shape[0]$ n_new
engine = create_engine("sqlite:///hawaii.db", echo = True)$
random.shuffle(porn_ids)$ porn_bots = porn_ids[:5000]
i = random.randrange(len(train_pos))$ train_pos[i]
my_gempro.kegg_mapping_and_metadata(kegg_organism_code='mtu')$ print('Missing KEGG mapping: ', my_gempro.missing_kegg_mapping)$ my_gempro.df_kegg_metadata.head()
r.json()['dataset']
print(archive_copy[archive_copy.new_rating_numerator == 'Error']['text'][1202])$ print(archive_copy[archive_copy.new_rating_numerator == 'Error']['text'][1165])$ print(archive_copy[archive_copy.new_rating_numerator == 'Error']['text'][313])
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, temp_us)
uniq_sorted_churned_plans_counts = sorted(uniq_churned_plans_counts,key=lambda x:x[0].tolist())
df['language'] = [np.nan if l == 'C' else l for l in df.language]
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall()
version = str(int(time.time()))$ database_log('Daily_Stock_Prediction', version, float(auc), float(build_time))
census_tracts_df = pd.read_csv('./Datasets/Census Tracts.csv')$ census_tracts_df.head()
np.exp(logit_mod_joined_result.params)
def jprint(j):$     print(json.dumps(j,indent=4,sort_keys=True))
%%time$ body_pp = processor(keep_n=8000, padding_maxlen=70)$ train_body_vecs = body_pp.fit_transform(train_body_raw)
np.savez('my_filename2', a=a, b=b) # The second variable name is the $
tip_sample.describe()
query.get_dataset(db, id=ds_info["DatasetId"][0])
measurement_results = session.query($     Measurements.station, Measurements.date, Measurements.prcp, Measurements.tobs).all()
exploration_titanic.nearzerovar()
table_rows = driver.find_elements_by_tag_name("tbody")[16].find_elements_by_tag_name("tr")$
p_new = df2['converted'].mean()$ p_new
giss_temp.columns
print (new_df_left['rawcensustractandblock'].dtype)$
max(dif_dict, key=dif_dict.get) 
results.summary()$
dat.status[dat.completed.notnull()].unique()
all_games = bruins.append(celtics).append(sox)$ all_game_dates = all_games.date.unique()
plt.hist(holdout_residuals)
import scipy.stats as stats$ t_stat, p_val = stats.ttest_ind(dftop['temp'],weather['temp'], equal_var=False)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=6QBbyTowfjjBzvYS8nXF')
import builtins$ builtins.uclresearch_topic = 'GIVENCHY'$ from configuration import config
print(norm.ppf(1-(0.05)))
X = train.title_author$ y = train.popular$ cross_val_score(pipe, X, y, cv=5, scoring='roc_auc').mean()
two_day_sample['date'] = two_day_sample.timestamp.dt.date
turnstiles_df.DATE.value_counts().sort_index()
dat=dat_before_fill.copy() #reset 'dat' to be un-interpolated data$ for temp_col in temp_columns:$     dat.loc[:,temp_col]=dat[temp_col].interpolate(method='linear', limit=12) #<= 3hrs;  at a 15 min interval, 3 hrs is 12 measurements
df = pd.read_sql('SELECT * from customer', con=conn_b)$ df
autos.columns = new_col_names
def yearMarkers(axis_obj, x_pos, **kwargs):$     axis_obj.axvline(x_pos, linestyle='--', color='w', alpha=.4, **kwargs)$ years = np.arange(0,len(grouped), 51)
lr = LogisticRegression(C=3.0, random_state=42)
df['year_month']=df['timestamp'].values.astype('datetime64[M]')$
df = df.drop(['Unnamed: 0', 'id', 'created_at', 'crawl_at'], axis=1)
Measurement = Base.classes.measurement$ Station = Base.classes.station
old_page_converted = np.random.choice(a=[1,0], size=n_old, replace=True, p=[p_old, 1-p_old])
print(autos.columns)  $ autos.columns = autos.columns.str.replace('([a-z0-9])([A-Z])', r'\1_\2').str.lower()
sat_spike.info()
df1.describe()
tipsDFslice.tail()
partition = community.best_partition(G)$ print('Modularity: ', community.modularity(partition, G))
fed_reg_dataframe[fed_reg_dataframe.index > '2017-01-20']
sns.boxplot(data.antiguedad.values)$ plt.show()
w_train, w_test=train_test_split(df3, test_size=.33, random_state=42)
print(airquality_pivot.head())
df = pd.read_excel("F:/web mining/webmining project/crawler/annotation_data/samsung/Merged_final.xlsx")$
brand_names = autos["brand"].value_counts().index
sum(contractor.address1.isnull())
df = pd.read_csv('ab_data.csv')$ df.head()
df.loc[:50,'name'].value_counts()
s1.values
year_string = 'extracted_data/*.csv'
FTE = pd.DataFrame(requirements, index = ['FTE'])$ FTE
open('test_data//open_close_test.txt', encoding='utf8')
tweets_orig = pd.read_csv('twitter-archive-enhanced.csv')$ tweets_orig.head()
content_joined="@@@".join(lines)$
df['y'].plot.box()
geometry = [Point(xy) for xy in zip(g['X Coordinate (State Plane)'], g['Y Coordinate (State Plane)'])]$ crs = {'init': 'epsg:2263'}$ g_geo = gpd.GeoDataFrame(g, crs=crs, geometry=geometry)
unique_users = df2.user_id.nunique()$ unique_users
irisDF = SpSession.createDataFrame(irisMap)$ irisDF.show()$
station_num = session.query(func.count(Station.station)).all()$ station_num
data_other = tmpdf.index[tmpdf[tmpdf.isin(DATA_SUM2_KEYS)].notnull().any(axis=1)].tolist()$ data_other
payments_all_yrs_ZERO_discharge_rank = (df_providers.loc[idx_ZERO_discharge_rank,:].groupby(['id_num','year'])[['discharge_rank']].sum())$ payments_all_yrs_ZERO_discharge_rank = payments_all_yrs_ZERO_discharge_rank.sort_values(['discharge_rank'], ascending=[False])$ print('payments_all_yrs_ZERO_discharge_rank.shape',payments_all_yrs_ZERO_discharge_rank.shape)
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()
from scipy import stats$ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)$ results.summary()
reddit = praw.Reddit(client_id=os.environ.get('REDDIT_CLIENT_ID'),$                      client_secret=os.environ.get('REDDIT_CLIENT_SECRET'),$                      user_agent=os.environ.get('REDDIT_USER'))
os.getcwd()
to_plot.plot.hist(by='Days Between Int', bins=100);$ plt.title("Actual number of days between purchases")$
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')$ print(z_score, p_value)$
pattern = re.compile('AA')$ print(pattern.findall('AAbcAA'))$ print(pattern.findall('bcAA'))
station_distance.insert(loc=1, column='Trip Duration(Minutes)', value=tripduration_minutes)
StockData.loc[(StockData['Date-Fri'] == 1) & (StockData['Date-Q2'] == 1), 'Twitter']$
results['CharacteristicName'].value_counts()
fit.summary()
df2.query('landing_page=="new_page"')['user_id'].count()/df2.shape[0]
df2[df2.user_id == 773192]
predictions=model.predict(X)$ rounded =[round(x[0]) for x in predictions]$ print(rounded)
df = pd.read_csv('ab_data.csv')$ df.head()
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ print(last_date)$
TripData_merged = pd.concat([TripData1, TripData2])
df_estimates_false = df_estimates_false.dropna(axis=0, subset=['points'])$ print(df_estimates_false)$
baseline_acc = 1. - df.is_lasvegas.mean()$ baseline_acc
new_page_converted = np.random.binomial(1,P_new,n_new)$ new_page_converted
logit_modB = sm.Logit(df_new_log['converted'], df_new_log[['intercept', 'group_treatment', 'country_US', 'country_UK']])$ resultsB = logit_modB.fit()$ resultsB.summary()
import os$ os.environ["instagram_client_secret"] = "91664a8e599e42d2a6a824de6ea456ec"$
path_to_token <- normalizePath("data_sci_8001_token.rds")$ envvar <- paste0("TWITTER_PAT=", path_to_token)$ cat(envvar, file = "~/.Renviron", fill = TRUE, append = TRUE)
filepath = 'C:\\Users\\jennifer.bosch\\Documents\\Metrics\\NDBC_GTS_metrics_2016.xlsx'$ df = pd.read_excel(filepath,0, index_col='Month')$ df.dtypes$
session.query(func.count(Measurements.date)).all()
calls_nocontact.neighborhood_district.value_counts()
with open('tweet_json.txt') as j_file:$     data = json.load(j_file)
discounts_table.Country.unique()
df_users_first_transaction=users.merge((transactions.groupby('UserID').first()), how ="left", on="UserID")$ df_users_first_transaction
top_10_authors = git_log.loc[:, 'author'].dropna().value_counts().head(10)$ top_10_authors
prob_convert_c = df2_control.converted.mean()$ prob_convert_c
df2017 = pd.read_json('data/mlbcontests6.26.2017-7.2.2017', lines=True)
(p_diffs > p_treatment - p_control).mean()
relatedSite = read.getRelatedSamplingFeatures(sfid=26, relationshiptype='Was Collected at')[0]
df_test_user_2 = df_test_user.copy()$ df_test_user_2['created_on'] = '2017-09-20 00:00:00'
import matplotlib.pyplot as plt$ import seaborn as sns$ sns.set()
data.keys()
train_4_reduced.shape, y_train.shape
tsla_tuna_neg = mapped.filter(lambda row: (row[3] < 0 and row[4] < 0))$
input_edge_types_file  = input_directory_name + 'input_edge_types.csv'$ Ext_input.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=input_directory_name)
len(ibm_train.columns), len(feature_col)
dgtotl.order(ascending=False).head(60)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page' ").index, inplace = True)$ df.drop(df.query("group == 'control' and landing_page == 'new_page' ").index, inplace = True)
coefs.loc['age', :]
inspector = inspect(engine)$ inspector.get_table_names()
for index, row in susp.iterrows():$     print(row.link)
engine = create_engine('sqlite:///results.db')$ pd.read_sql_query('SELECT * FROM demotabl LIMIT 5;',engine)
store = pd.HDFStore("../../data/store.h5")$ df = store['df']$ df
rain_df.set_index('date').head()
tweet_json.sample(5)
plt.legend(loc = 1).get_frame().set_edgecolor('white') $ my_label_max = "2015 Highs"$ my_label_min = "2015 Lows"
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url)$
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(doc_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours First Difference Autocorrelation')
monte.str.extract('([A-Za-z]+)', expand=False)
time_series.describe()
store = Arctic('localhost')$ store.initialize_library('ETF')$ library = store['ETF']
for c in ccc:$     vhd[c] /= vhd[c].max()
df = df[['Country', 'Indicator_id', 'Publication Status', 'Year', 'WHO Region', 'World Bank income group', 'Sex', 'Display Value', 'Numeric','Low', 'High', 'Comments']]$ df.head()
events_df['event_time'] = events_df['event_time'].apply(lambda s: datetime.datetime.strptime(str(s),'%Y-%m-%d %H:%M:%S'))
compound(px_etfs).plot(fontsize='small') # exclude from long strat the negative sectors?
df.to_csv(dataurl+'train200th.csv', sep=',', encoding='utf-8', index=False)
df.median() + 1.57 * (df.quantile(.75) - df.quantile(.25))/np.sqrt(df.count())
df = pd.read_csv("../../data/msft_with_footer.csv",skipfooter=2,engine='python')$ df
config = {"features analyzed": ["Sex", "Pclass", "FamilySize", "IsAlone", "Embarked", "Fare", "Age", "Title"]}$ datmo.snapshot.create(message="EDA", config=config)
all_sets.cards["XLN"].head()
print("Probability of treatment group converting:", df2[df2['group']=='treatment']['converted'].mean())$ print("Probability an individual recieved new page:", $       df2['landing_page'].value_counts()[0]/len(df2))
df3[['CA','UK', 'US']] = pd.get_dummies(df3['country'])$ df3 = df3.drop('CA', axis = 1)$ df3.head()
extractor = connectToTwitterAPI()$ tweets = extractor.search(q="#BlackPanther", count=50)$ print("Number of tweets extracted: {}.\n".format(len(tweets)))
df.isnull().sum()
data2.SA = data2.SA.astype(str)
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'})$ station_count = station[['Count']]$ station_count $
tweet_df.to_csv("output/Sentiment_Analysis_Media_Data.csv",$                      encoding="utf-8", index=False)
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']]$ df_new['country'].astype(str).value_counts()
df = pd.read_csv("../../data/msft.csv",header=0,names=['open','high','low','close','volume','adjclose'])$ df.head()
(df.doggo.value_counts(), df.floofer.value_counts(), df.pupper.value_counts(), df.puppo.value_counts())
yc_merged = yc_trimmed.merge(yc_sd, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')
print("Converted users proportion is "+str(df['converted'].mean()*100))
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])$ df_new.head(10)
grid_lat = np.arange(24, 50.0, 1)$ grid_lon = np.arange(-125.0, -66, 1)$ glons, glats = np.meshgrid(grid_lon, grid_lat)
url_img = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'$ browser.visit(url_img)
print(pandas_list_2d.iloc[:, 0])
cityID = 'af2a75dbeb10500'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lincoln.append(tweet) 
local_sea_level_stations.columns = [name.strip().replace(".", "") $                                     for name in local_sea_level_stations.columns]$ local_sea_level_stations.columns
station_id_col = ["_"+str(x)+' INT' for x in df['id'].tolist()]$
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comment', ''))
mgxs_lib = openmc.mgxs.Library.load_from_file(filename='mgxs', directory='mgxs')
from sklearn.model_selection import train_test_split$ trip_data_sub = trip_data.ix[trip_data.Total_amount >0,:]$ trip_data_sub["tip_percentage"] = trip_data_sub.apply(lambda x: float(x["Tip_amount"])/float(x["Total_amount"]), axis = 1)
df_joined = df2.join(df3.set_index('user_id'),on ='user_id')$
movies[movies['homepage'].isnull()].shape[0]
merged_data['cs_creation_day'] = merged_data['customer_creation_date'].dt.day$ merged_data['cs_creation_month'] = merged_data['customer_creation_date'].dt.month$ merged_data['cs_creation_year'] = merged_data['customer_creation_date'].dt.year
df_twitter_copy = df_twitter_copy[df_twitter_copy['retweeted_status_id'].isnull()]
df_tweet_json.info()
merged = price2017.merge(typesub2017,how='inner',left_on='DateTime',right_on='DateTime')
grouped_mean = tweet_df.groupby(["source"]).mean()["compound"]
%pylab inline$ pandas_ds["time2close"].plot(kind="hist")
transactions = sql_context.read.json('data/transactions.ndjson')
daily_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner')$ daily_df.head()
df[df['landing_page'] == 'new_page'].landing_page.count()
trip_data_q5["weeks"] = trip_data_q5["weeks"].astype('category')$ trip_data_q5.groupby("weeks").agg(["mean"])
p_old=df2.query('converted==1').count()[0]/df2.count()[0]$ p_old
relevance_scores_df = pd.DataFrame(relevance_scores[0]).mean(axis=1)$ relevance_scores_df.describe()
df.nunique()['user_id']
jd = r1.json()$ print(jd)
users = pd.read_csv('LaManada_new/tbluserinfo.csv',sep=SEP)$ users.shape
activity_df = pd.DataFrame(joined_data)
elms_all_0611 = elms_all_0604.append(elms_all, ignore_index=True)$ elms_all_0611.drop_duplicates('ACCOUNT_ID',inplace=True)
archive_clean.drop(archive_clean[archive_clean.retweeted_status_id.notnull()].index, axis=0,inplace=True)
trump = api.user_timeline(id='realDonaldTrump') # last 20 tweets
df['Year'] = df['created_date'].dt.strftime('%Y')$ df['YearMonth'] = df['created_date'].dt.strftime('%Y/%m')$ df['Month'] = df['created_date'].dt.strftime('%b')
fraud_data = pd.read_csv("Fraud_Data.csv")$ ipAddress_to_country =pd.read_csv("IpAddress_to_Country.csv")$
median_trading_volume = statistics.median([day[6] for day in data])$ print ('Median trading volume for 2017:', median_trading_volume)
def validation_score(score_series):$     return score_series.mean()
url='https://api.twitter.com/1.1/trends/place.json?id=2459115'$ parameters={'q':'Trump'}$ topics=requests.get(url,auth=auth,params=parameters)
df['Trip_duration']=df['Trip_duration'].astype('timedelta64[s]')
contractor[contractor.duplicated() == True]
fraud_df['purchase_time']= pd.to_datetime(fraud_df['purchase_time']) $ fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time'])$ fraud_df['delay_time'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).astype('timedelta64[h]')
twitter_archive_clean = twitter_archive.copy()$ image_predictions_clean = image_predictions.copy()$ tweet_json_clean = tweet_json.copy()
df['id'] = df['id'].astype('category')$ df['sentiment'] = df['sentiment'].astype('category')$ df['created_at'] = pd.to_datetime(df['created_at'])
pres_df['metro_area'] = pres_df['split_location_tmp'].map(lambda x: x[0])
plot_data(df[~df.index.isin(df.query('state == "YY" and amount > 5000').index)])
scores = raw_scores.value_counts().sort_index()$ scores
hmeq.summarize(cardinality=dict(name='hmeq_card', replace=True))
autos.info()
df_p3.loc[df_p3["CustID"].isin([customer])]
X = df2.ix[:, 1:]$ y = df2.country_destination$ X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)$
dtc_features = sorted(list(zip(test_features, dtc.feature_importances_)), key=lambda x: x[1], reverse=True)$ dtc_features
import requests$ import quandl$ quandl.ApiConfig.api_key = 'AnxQsp4CdfgzKqwfNbg8'
TestData_ForLogistic.to_csv('test_logistic.csv')
tweets_clean[tweets_clean['p1_dog'] == False]['name'].value_counts()$ tweets_clean[tweets_clean['p1_dog'] == False][tweets_clean['p2_dog'] == False]['name'].value_counts()$
df2.converted.mean()
p_new = df2.converted.mean()$ p_new
df2[((df2.group == 'control') == (df2.landing_page == 'old_page')) == False].shape[0]
import statsmodels.api as sm
pnew = df2['converted'].mean()$ print(round(pnew,4))$
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=' + API_KEY + '&start_date=2017-1-1&end_date=2017-12-31')$ 
df['Month Created'] = df['Date Created'].dt.month$ df['Month Closed'] = df['Date Closed'].dt.month$
p_value = (null_vals>p_obs_diff).mean()$ p_value
time_dup = "2017-01-09 05:37:58.781806"$ df2 = df2[df2.timestamp != time_dup]
df2.plot.
all_tables_df.iloc[0]
df2['low_tempf'].mean()
open_list = [d[dt]['Open'] for dt in d.keys() if d[dt]['Open'] is not None]$
train_ratio = 0.75$ train_size = int(samp_size * train_ratio); train_size$ val_idx = list(range(train_size, len(df)))
sales_update_nan.plot.scatter(x='2015 Sales Q1',y ='2015 Sales')
miss_grp_sum = len(miss_grp1) + len(miss_grp2)$ print('The total number of misalinged cases is {}'.format(miss_grp_sum))
%%time$ df = table.to_pandas()
print(df.shape)$ df = df[pd.notnull(df['is_shift'])]$ print(df.shape)
displacy.serve(doc, style='ent')
eppd = eppd.merge(cpdi, on='i')$ eppd = eppd.merge(xpdi, on='i')$ eppd.head(40)
rejected['approved'] = False$ rejected.head()
with open('../data/rockville_unclean.json') as f:$     data = json.load(f)$     $
printer_data = pd.read_csv('http://ocf.io/shichenh/ocf_datathon_ds/printing.csv')$ session_data = pd.read_csv('https://www.ocf.berkeley.edu/~shichenh/ocf_datathon_ds/sessions.csv')$ staff_data = pd.read_csv('https://www.ocf.berkeley.edu/~shichenh/ocf_datathon_ds/s_sessions.csv')
filename = '../data/DadosBO_2017_1(FURTO DE CELULAR).csv'$ df_BOs = pd.read_csv(filename, sep=';', encoding='utf-8')$ df_BOs.info()
df = pd.read_csv("WaterUsageSample.csv")$ df.shape
df = pd.merge(t1, t2, on='tweet_id', how='inner')$ df = pd.merge(df, t3, on='tweet_id', how='inner')
from statsmodels.tsa.arima_model import ARIMA$ model_713 = ARIMA(dta_713, (2, 0, 1)).fit() $ model_713.forecast(5)[:1] 
weather_features = weather_features.resample('D').sum()
dem["date"] = dem.apply(lambda line: datetime.strptime(str(line['date']),"%m/%d/%y").date(),axis=1)
df_old = df2[df2['landing_page'] == 'old_page']$ n_old = df_old.user_id.count()$ n_old
plt.savefig(str(output_folder)+'NB05_2_FC_before_and_after'+str(cyclone_name)+'_'+str(location_name))
reviewsDF = reviewsDF.drop(reviewsDF.columns[0], axis = 1)
list(data.dropna(thresh=int(data.shape[0] * .9), axis=1).columns)
import plotly.plotly as py$ import plotly.graph_objs as go$
sentiments_df = df.apply(get_polarity_scores, axis=1)$ sentiments_df.head()
new_page_converted = np.random.choice([0,1], size=n_new, replace=True, p=[1-p_new,p_new])$ np.bincount(new_page_converted)
gdax_trans['Balance']= 0.00
households_with_count.iloc[30:40, 15:]
plt.savefig(str(output_folder)+'NB05_1_FC_'+str(cyclone_name)+'_'+str(location_name))
df7.loc[df7['avg_dew_point Created'] == 'dry', 'avg_dew_point Created'] = np.nan$ df7['avg_dew_point Created'].value_counts(dropna=False)
bloomfield_pothole_data.index = bloomfield_pothole_data['CREATED_ON']$ bloomfield_pothole_data.info()
plt.hist(p_diffs);
res.summary2()
max_ch_ol1 = max(abs(v.close-next(islice(o_data.values(), i+1, i+2)).close) for i, v in enumerate(o_data.values()) if i < len(o_data)-1)$ print('A one liner using islice: {:.2f}'.format(max_ch_ol1))
firstWeekUserMerged["day_time_stamp2"] = firstWeekUserMerged.time_stamp2.dt.day
display('x', 'y', "pd.concat([x, y], keys=['x', 'y'])")
news_df = guardian_data.copy()
df.to_html('resources/html_table_marsfacts.html')
dfz['retweet_count'].plot(color = 'red', label='Retweets')$ dfz['favorite_count'].plot(color = 'blue', label='Favorites')
planets.groupby('method')['orbital_period']
screen_name = 'cdvel'$ with open(os.path.join(os.getcwd(),"data/credentials.json")) as data_file:    $     key = json.load(data_file)$
austin.loc[austin['miles'].argmax()]$
bild = bild[bild.message != "NaN"]$ spon = spon[spon.message != "NaN"]
df = pd.read_csv("contact.csv", index_col=None) 
dat = pd.read_csv('./311_clean.csv',parse_dates=[0,2])$ dat.head(2)
sim_diff = new_page_converted.mean() - old_page_converted.mean()$ print('The difference in the simulated conversion rates is {}.'.format(round(sim_diff,4)))$
reg_target_encoding(train,'Block',"any_spot") $ reg_target_encoding(train,'DOW',"Real.Spots") $ reg_target_encoding(train,'hour',"Real.Spots") $
close_idx = afx['dataset']['column_names'].index('Close')
from pyspark.sql.types import DoubleType, IntegerType$ for col_name in data.columns:$     data = data.withColumn(col_name, data[col_name].cast(DoubleType()))
df2.user_id.nunique()
df['date'] = df['date'].apply(lambda time: time.strftime('%m-%d-%Y'))$ df['time'] = df['time'].apply(lambda time: time.strftime('%H:%M'))
grid_lat = np.arange(np.min(lat_us), np.max(lat_us), 1)$ grid_lon = np.arange(np.min(lon_us), np.max(lon_us), 1)$ glons, glats = np.meshgrid(grid_lon, grid_lat)
sentiment_df = pd.DataFrame(results_list)$ sentiment_df
sub_dataset.groupby(["NewsDesk", "SectionName", "Popular"]).size()
image_file=pd.read_csv('image-predictions.tsv',sep='\t')
df.drop(todrop2, inplace=True)
df = quandl.get('NASDAQOMX/COMP', api_key='yEFb5f6a7oQL91qzEsvg',start_date = '1960-01-01',end_date = '2016-12-26')$ a=df$ a.rename(columns={'Index Value' : 'index_value'}, inplace=True)
pca_df = pd.DataFrame(pca_data, index = pivoted.T.index.values,columns=labels)
conn = engine.connect()$ measure_df = pd.read_sql("SELECT * FROM Measurement", conn)
cust_demo.sample(n=700, replace=False).duplicated().value_counts()
new_page_converted=np.random.binomial(n_new,p_new)$ new_page_converted
haw.to_csv('./haw_python.csv',index=True)
lr = LogisticRegression()$ lr.fit(X, y)
Irregularities_data = []$ time_hour_for_file_name = 0 #datetime.datetime.now().time().hour$
df_campaigns['Open Rate'] = df_campaigns['Open Rate'].apply(lambda x: x[:-1]).astype('float')$ df_campaigns['Click Rate'] = df_campaigns['Click Rate'].apply(lambda x: x[:-1]).astype('float')
Base.prepare(engine, reflect=True)$ Base.classes.keys()
year8 = driver.find_elements_by_class_name('yr-button')[7]$ year8.click()
df1 = df.loc[:, ('Close')].reset_index().rename(index=str, columns={"Date": "ds", 'Close': 'y'})
resdf.iloc[:,114:129].to_sql('demotabl',conn)
series_of_converted_ages = age_converter(test_clean["Age no Nans"])$ list_of_test_features.append(("Age", series_of_converted_ages))
df = df[(df.age >= 18) & (df.age <= 105)]$ df.head(2)
df2.converted.mean()
import sys$ sys.version
All_tweet_data_v2=pd.melt(All_tweet_data_v2, id_vars=Remaining_columns, value_vars=stages, var_name="Remove", value_name='Stage')
records.loc[records['Gender'].isnull(), 'Gender'] = gender
!cat "json_example.json"
df.rename(columns={'Indicator':'Indicator_ID'})
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))$
merged_portfolio_sp['Equiv SP Shares'] = merged_portfolio_sp['Cost Basis'] / merged_portfolio_sp['SP 500 Initial Close']$ merged_portfolio_sp.head()
data_dict = json.loads(data.text)
jobs_data['clean_description'].head(5)
df2['user_id'].value_counts().head()
df.loc[1:4,"Date"]
pd.set_option('display.max_columns', 25)$ pd.set_option('display.max_rows', 25)$ pd.set_option('display.precision', 3)
buckets_to_df(contributors.fetch_aggregation_results()['aggregations']['0']['buckets']).head()
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_jp_200_wzh.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_200_wzh.w2v')
ab_file2[ab_file2.duplicated('user_id')]
sites.shape
with open('all_data_vectorized.dpkl', 'wb') as f:$     dpickle.dump(all_data_vectorized, f)
selected_line = contest_data.where(F.col('sales_order_line_key')== 240314891)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2017-11-01&end_date=2017-11-02')
date_account_created = pd.to_datetime(df['date_account_created'])$ print date_account_created
rf.score(clim_test, size_test)
plt.title("Pyber Rides Sharing (2016)")$ plt.xlabel("Total Number of Rides Per City")$ plt.ylabel("Average Fare ($)")
df_events.to_csv("data_output/df_events.csv", encoding="utf-8", index=False)
X = pivoted.fillna(0).T.values$ X.shape
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()$ p_value = (p_diffs > act_diff).mean()$ print('The proportion of the p_diffs greater than the actual difference observed is: {}.'.format(p_value))
df_raw = df_raw.loc[df_raw.artist.notnull(),:]
url=("/Users/maggiewest/Projects/detroit_census.csv")$ detroit_census = pd.read_csv(url)
cr_under_null = df2['converted'].mean()    # Same as above: under the null, use both groups.$ cr_under_null$
df.resample('M').mean()
jobs_data1 = json_normalize(json_data1['page'])$ jobs_data1.head(5)
df_ad_airings_5['state'] = df_ad_airings_5['state'].map(lambda x: x.strip()) 
idx = pd.period_range('2011','2017',freq='Q')$ idx
countries_log_reg = sm.Logit(df4['converted'], df4[['country_UK', 'country_US', 'intercept']])$ country_result = countries_log_reg.fit()
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
import matplotlib.pyplot as plt$ plt.plot(twitter_df['created_at_time'], twitter_df['retweet_count'], 'ro')$ plt.show()$
results_measurement = session.query(Measurements.station,Measurements.date,Measurements.prcp, Measurements.tobs).all()$ results_measurement
trump_originals = trump[trump.is_retweet == False]$ trump_originals.shape$
path = "http://www.principlesofeconometrics.com/stata/consumption.dta"$ df = pd.read_stata(path)$ df.head(5)
no3Mask = nitrogen['DetectionQuantitationLimitMeasure/MeasureUnitCode'] == 'mg/l NO3'$ nitrogen.loc[no3Mask,'TotalN'] = nitrogen['TotalN'] * 0.2259
df_clean.drop(['doggo','floofer', 'pupper','puppo'], axis=1, inplace= True)
current_img = current_img_link[0]['style']$ print(current_img.find("('"))$ print(current_img.find("')"))
Base = automap_base()$ Base.prepare(engine, reflect=True)
rain_df.describe()
from scipy.stats import norm$ z_crit = norm.ppf(0.95)$ print(z_crit)
df_ad_airings_5['location'].isnull().sum()
lagou_df = pd.read_sql(sql_lagou, conn)
df_similar_items = content_rec.get_similar_items().to_dataframe()$ df_similar_items.head(20)
df.loc[df['Sold_to_Party'] == '0000101663'].sample(10)['SalesOffice']
np.std(new_means)
from scipy import stats$ results = logit.fit()$ results.summary()
news_df['Timestamp'] = pd.to_datetime(news_df['Date'], infer_datetime_format=True)$ news_df.head()
capture-tweets.py(Apple)$
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df.head()
print(df.dtypes)
git_log = git_log.sort_index()$ git_log[git_log['author'] == 'Linus Torvalds'].head(10)
print("Min " + str(dc['created_at'].min()) + " Max " + str(dc['created_at'].max()))$
body = re.sub(r'[^\x00-\x7F]+',' ', body)$ body
df_us_.drop(['category','creator','location','profile','deadline'], axis=1, inplace = True)
from h2o.estimators.deeplearning import H2ODeepLearningEstimator$ help(h2o.model.model_base.ModelBase)
df_new['country'].unique() # what countries are in the dataset now?$ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.drop(['US'], axis =1)
close_px_all = pd.read_csv('stock_px.csv', parse_dates=True, $                            index_col=0)$ close_px_all.head() # data 2003-2011, >2000 rows
weather_df.is_copy = False$ weather_df["Time of retrieval"] = [datetime.fromtimestamp(d) for d in weather_df["Time of retrieval"]]$ weather_df.head()
status_data = status_data.drop(['BROKERAGE', 'BETWEENNESS', 'NBROKERAGE',$                                     'NBETWEENNESS', 'DENSITY', 'TRANSITIVITY', 'NETWORKSIZE'], axis=1)
test.fillna(0, axis=1, inplace=True)$ train.fillna(0, axis=1, inplace=True)
by_weekday = data.groupby(data.index.dayofweek).mean()$ by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']$ by_weekday.plot(style=[':', '--', '-'])
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
df['Non-Crime Criteria Met'] = df_criteria.apply(lambda x: y_count(x),axis=1)
df2['intercept'] = 1$ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']$ df2['ab_page'].mean()
UserInsight = pd.DataFrame(columns=['_id','owner','source','category','type','message','data','upvote','downvote'])$ UserInsight
f1 = df2['sizec'].values$ f2 = df2['breakfastc'].values$ X = np.array(list(zip(f1, f2)))$
fox = news_sentiment('@FoxNews')$ fox['Date'] = pd.to_datetime(fox['Date'])$ fox.head()
nav = soup.nav$ for url in nav.find_all('a'):$     print (url.get('href'))
tweetdf = pd.read_csv('../../data/clean/tweets_w_lga.csv') # shortcut
HOU_analysis = team_analysis.get_group("HOU").groupby("Category") # Pulls only team transactions from sample, then groups$
active_station = str(session.query(Measurement.station).group_by(Measurement.station).\$                        order_by(func.count(Measurement.id).desc()).first())
df_geo_count = df_geo.groupby("geo_code").count()$ dict_geo_count = df_geo_count.to_dict()["id_str"]
negative = '/Users/EddieArenas/desktop/Capstone/negative-words.txt'$ negative = pd.read_table(negative, encoding = "ISO-8859-1")
p_old = df2['converted'].mean();$ p_old
deaths=dropped.loc['Total deaths (confirmed + probables + suspects)']$
cur.execute('INSERT INTO experiments (material_id) VALUES ("EVA")')  # use defaults, $ conn.commit()
grid_id = np.arange(1, 1535,1)$ grid_id_array = np.reshape(grid_id, (26,59))
temp = df2.converted.value_counts()$ prop = temp/temp.sum()$ print "Probability of converting regardless of the page they receive:",prop[1]
p_treatment = df2.query('group == "treatment"').converted.mean()$ p_treatment
df.to_json("json_data_format_records.json", orient="records")$ !cat json_data_format_records.json
dtm_df = pandas.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index)$ dtm_df
model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'afternoon', 'evening']])$ results = model.fit()
temp_df.groupby('reorder_interval_group')['Order_Qty'].mean()
tweet_df_polarity = my_tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"]$ pd.DataFrame(tweet_df_polarity)
df['gender']=df['gender'].replace({'Male': 0, 'Female': 1})$ for col in ['Partner','Dependents','PhoneService','PaperlessBilling','Churn']:$      df[col].replace({'Yes': 1, 'No': 0}, inplace=True)
num_dr_new = dr_new['Provider'].resample('W-MON', lambda x: x.nunique())$ num_dr_existing = dr_existing['Provider'].resample('W-MON', lambda x: x.nunique())
model.save_weights('best.hdf5')
results_df = pd.DataFrame(results) $ results_df.head(5)
idx = df2['Number'].values$ Xtrain = Xtrain[idx,:] $ Xtrain.shape
p_undernull = df2.converted.mean()
dat.plot.scatter('longitude','latitude')$ print ' '
df.groupby('domain').count().sort_values('id',ascending=False).head(25)
transit_df_rsmpld = transit_df_byday.reset_index().groupby('FUZZY_STATION').apply(lambda x: x.set_index('DATETIME').resample('1M').sum()).swaplevel(1,0)$ transit_df_rsmpld.info()$ transit_df_rsmpld.head()
df.drop_duplicates(subset=['last_name'], keep='last')$
df = pd.read_csv('/Users/yennanliu/Desktop/used-cars-database/autos.csv' ,encoding = "ISO-8859-1" )
df = pd.read_csv('../datasets/CAERS_ASCII_2004_2017Q2.csv')
df.to_excel("../../data/stocks2.xlsx")
experiment_details = client.repository.store_experiment(meta_props=experiment_metadata)$ experiment_uid = client.repository.get_experiment_uid(experiment_details)$ print(experiment_uid)
N_new = df2.query('landing_page == "new_page"')['user_id'].nunique()$ N_new 
wednesdays = pd.date_range('2014-06-01','2014-08-31', freq='W-WED')$ wednesdays.values
taxiData2.loc[taxiData2.Tip_amount < 0, "Tip_amount"] = 0
dframe_team = pd.io.html.read_html(url)$ dframe_team = dframe_team[0]$ dframe_team.head()
pystore.delete_store('mydatastore')$
columnsTitles=["UserID","User","Gender","Registered","Cancelled","TransactionDate","TransactionID","ProductID","Quantity"]$ hist=hist.reindex(columns=columnsTitles)
now = datetime.now()
ms_df = mt_df[mt_df['mortality_salience'] == 1]$ print("HI tweets between 18:07-18:21 AM 2019-01-13 labelled as mortality_salience: {}".format(len(ms_df)))$ print("percent mortality salience: {}".format(len(ms_df)/len(mt_df)))
cvecogXfinaltemp= cvecogXfinal.copy()$ cvecogXfinaltemp = cvecogXfinaltemp.rename(columns={'fit': 'fit_feat'})
old_conv_rate = df2.query('group == "control"')['converted'].mean()$ old_conv_rate
results = model_selection.cross_val_score(gnb, X_test, Y_test, cv=kfold)$ results.mean()
tweets_df.language.value_counts()$
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.hour).count().plot(y='Agency')$
results.head()
n_old = df2.query('landing_page == "old_page"').shape[0]$ n_old
1/np.exp(results.params)
df2 = df.drop(df[((df.group == 'treatment') & (df.landing_page =='old_page') )$                  |((df.group ==  'control') & (df.landing_page =='new_page') ) ]$                   .index)
def dist(a, b, ax=1):$     return np.linalg.norm(a - b, axis=ax)
tlen = pd.Series(data=data['len'].values, index=data['Date'])$ tfav = pd.Series(data=data['Likes'].values, index=data['Date'])$ tret = pd.Series(data=data['Retweets'].values, index=data['Date'])
lda_corpus = model[corpus]$ doc_topic_matrix = matutils.corpus2dense(lda_corpus, num_terms=n_topics).transpose()$ df = df.reset_index(drop=True).join(pd.DataFrame(doc_topic_matrix))
g = sns.catplot(x="AGE_groups", col="goodbad",$                  data=data, kind="count",$                  height=5, aspect=.9);
missing_latlongs = len(bus[bus.isnull()['longitude']==True])$ missing_latlongs$
TrainData.describe(include=['float64', 'int64'])
url ='https://graph.facebook.com/v2.6/'+str(x[1])+'/posts/?fields=id,comments.limit(0).summary(true),shares.limit(0).summary(true),likes.limit(0).summary(true),reactions.limit(0).summary(true)'
countries_df.info()
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = log_mod.fit()
withdraws = 4$ all_noms["nom_count"].sum() + additional_june_noms - all_noms[all_noms["confirmed"] == "yes"]["nom_count"].sum() - withdraws
x_minor_ticks = 10 # Note that this doesn't work for datetime axes.$ y_minor_ticks = 10
GenresString=Genres.replace('|',',')$ GenresString=GenresString.replace(',',',\n')
svm_classifier = GridSearchCV(estimator=estimator, cv=kfold, param_grid=svm_parameters)$ svm_classifier.fit(X_train, Y_train)
total.index = DatetimeIndex(total['created_at'])
acs_df.to_csv('final_df.csv')
[[scores.loc[:,[row]].mean(),scores.loc[:,[row]].sum()]for row in scores]
p_converted = convert/df.shape[0]$ p_converted
sauce = urllib.request.urlopen('https://pythonprogramming.net/sitemap.xml').read()$ soup = bs.BeautifulSoup(sauce, 'xml')$ soup
station_count = session.query(func.count(distinct(Measurement.station))).all()$ station_count$
columns = inspector.get_columns('clean_hawaii_stations.csv')$ for c in columns:$     print(c['name'], c['type'])$
station_count = session.query(func.count(Station.id)).all()$ station_count
len(df2['user_id'].unique())
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
r_lol = r.json()['dataset']['data']
digdiv[(digdiv > -1) & (digdiv < 1)].plot();
free_data.dtypes
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/EON_X.json?api_key=&start_date=2017-01-01&end_date=2017-12-31')$ print(r.json())
json_filename = data_folder+'tweet_json.txt'
actual_value_second_measure=pd.DataFrame(actual_value_second_measure)$ actual_value_second_measure.replace(2,1, inplace=True)
reviewsDF = pd.read_csv('ABT.csv', index_col=False, encoding='UTF-8')
us_temp = temp_fine.reshape(843,1534).T #.T is for transpose$ np.shape(us_temp)
print(f"Number of stations is {sq.station_count()}")
%time train_4_reduced = tsvd.transform(train_4)
prcp_query = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > last_year).all()
plt.savefig(str(output_folder)+'NB01_3_landscape_image02_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice02_str) 
twitter_archive_full = twitter_archive_full[twitter_archive_full.in_reply_to_status_id_x.isna()].copy()$ twitter_archive_full = twitter_archive_full[twitter_archive_full.retweeted_status.isna()].copy()$
data_file_path = "/Users/gandalf/Documents/coding/do_not_commit/capstone/"$ website_file_path = '/Users/gandalf/Documents/coding/rczyrnik.github.io/capstone/'
df.groupby(df.index.tz_localize('GMT').tz_convert('US/Eastern').hour).count().Tweets.plot(kind='barh')
xmlData[xmlData['street'].str.match('.*[^St|Ave|S|N|E|W|NE|SE|NW|SW|Pl|Ct|Dr|Ln|Blvd] $')]['street']
df.nunique()['user_id']
new_train_raw.to_csv('train_w_sessions.csv',sep=',')$ test_w_sessions.to_csv('test_w_sessions.csv',sep=',')
sortedDF = dateCounts.sort_values('dates')$ print(max(sortedDF['countsOnDate']),min(sortedDF['countsOnDate']))$ sortedDF[0:5]
df_l[['ship_name','year_built']].sort_values('year_built').dropna(subset=['year_built']).drop_duplicates('ship_name').tail(10)
crime_geo_table = pa.Table.from_pandas(crime_geo_df)$ crime_geo_table
AAPL.info()
data.L2.unique()
data['SA'] = np.array([analyze_sentiment(tweet) for tweet in data['Tweets']])$ display(data.head(10))
subject_df = latest_df[['classification_id', 'subject_ids', 'subject_data']].copy()$ subject_df['subject_data'] = subject_df.subject_data.apply(lambda x: list(json.loads(x).values())[0]['Filename'])
Train_extra = Train.merge(train_dum_clean, right_on='ID', left_index=True)$ Train_extra.head()
cercanasAfuerteApacheEntre100Y125mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 100) & (cercanasAfuerteApache['surface_total_in_m2'] < 125)]$ cercanasAfuerteApacheEntre100Y125mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre100Y125mts.apply(descripcionDistancia2, axis = 1)$ cercanasAfuerteApacheEntre100Y125mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
iris.head()
import pandas as pd$ names = pd.Series(data)$ names
sentiment_df["date"] = pd.to_datetime(sentiment_df["date"])$
reviewsDF.dtypes
df2 = df.query('landing_page == "new_page" & group == "treatment"')$ df2 = df2.append(df.query('landing_page == "old_page" & group == "control"'))
param_grid = {}$ param_grid['penalty'] = ['l1', 'l2']$ param_grid['C'] = [0.1, 1.0, 10]
toyotaData=autoData.filter(lambda x: "toyota" in x)$ print (toyotaData.count())$
urban_merged_1 = pd.merge(urban_average_fare, urban_total_rides, on="city")$ urban_merged_df=pd.merge(urban_merged_1, urban_drivers, on="city")$ urban_merged_df.head()
v_invoice_link.columns[~v_invoice_link.columns.isin(invoice_link.columns)]
from sklearn.feature_extraction.text import TfidfVectorizer$ tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=2, max_df=0.5, stop_words=portuguese_stop_words)
df_new[['canada','uk','us']] = pd.get_dummies(df_new['country'])
i1 = pd.Index([1, 3, 5, 7, 9])$ i2 = pd.Index([2, 3, 5, 7, 11])
for key, value in df.iteritems():$     x = {key : value}$     print(x)
telecom3 = telecom2.drop(['mobile_number', 'churn'], axis=1)$ plt.figure(figsize = (20,20))        # Size of the figure$ sns.heatmap(telecom3.corr())
tipsDF.head()
plt.plot(df[base_col].rolling(window=12).std(),color='green',lw=2.0,alpha=0.4)$ plt.plot(df[base_col].rolling(window=3).std(),color='purple',lw=0.75)$ plt.show;
df2[df2['group'] == "control"].converted.mean()
sentiments_pd = pd.DataFrame.from_dict(sentiments)$ sentiments_pd.head()
header = cylData.first()$ cylHPData= cylData.filter(lambda line: line != header)$ print (cylHPData.collect())
femalebydatenew  = femalebydate[['Sex','Offense']].copy()$ femalebydatenew.head(3)$
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
key = 'o_r_mapper:91ala:ala103077070913625'$ rdb = c.srdb_new(0)$
pipeline.fit(X_train, y_train)
len(df2.query("landing_page == 'new_page'")) / df2.shape[0]
not_lineup = len(df[((df['group'] == 'treatment') == (df['landing_page'] != 'new_page')) ])$ print('The new_page and treatment dont line up {} times'.format(not_lineup))
hourly_df['Open_Price_Change'].value_counts()
plt.plot(kind='bar')$ plt.show()
df_2017['bank_name'] = df_2017.bank_name.str.split(",").str[0]$
tempXtf = pd.concat([X_trainfinaltf, X_testfinaltf])$ print tempXtf.shape
df2= df_img_algo_clean.copy()$ df2.info()$ df2[(df2['predict_1_breed'] == False) & (df2['predict_2_breed'] == False) & (df2['predict_3_breed'] == False)]
driver = webdriver.Chrome(executable_path="/Users/dale/Downloads/chromedriver")
sm.stats.proportions_ztest([17489, 17264], [145274, 145310], alternative ="smaller")$
data['Open'].max()
with open('image_predictions.tsv', 'wb') as file:$     file.write(response.content)$ image_predictions = pd.read_csv('image_predictions.tsv', sep='\t')
print(type(dicts))$ print(dicts.keys())
X2 = PCA(2, svd_solver='full').fit_transform(X)$ X2.shape
firstWeekUserMerged = firstWeekUserMerged.drop(["gender","birth_year"], axis=1)$ firstWeekUserMerged.isnull().sum()
sox = sox[pd.to_datetime(sox.date).isin(bad_dates) == False]$ sox = sox[pd.to_datetime(sox.date) >= dt.datetime(2013,1,1)]$ sox.reset_index(drop=True, inplace=True)
df_user[df_user['user.name'] == 'Marco Rubio']
df = pd.read_csv('artist-pitch.csv')$ df.head()
df.label.value_counts()
poparr2.shape
def arrayToDataframe(inData,customerList,productList):$     resultDf = pd.DataFrame(inData,index=customerList,columns=productList)$     return resultDf
df_master.drop('created_at',axis =1, inplace = True)$ df_master.info()
norm.ppf(1-(0.05/2))
df2.shape[0]
archive_clean['rating_denominator'].value_counts()
mean_newsorg_sentiment = grouped_newsorgs['Compound'].mean()$ mean_newsorg_sentiment.head()
zipincome = pd.DataFrame(jsonData)$ zipincome.head()
demographics = my_df_free1.iloc[:,0:3]$ scores = my_df_free1.iloc[:,5:]
obs_diff = df.query("group == 'treatment'").converted.mean() - df.query("group == 'control'").converted.mean()$ obs_diff
Linear_Reg=LinearRegression()$ Linear_Reg.fit(X_train,Z_train)$ Linear_Reg.score(X_test,Z_test)
car = car[['dayofweek','date','incidntnum']].reset_index(drop=True)$ car.head()
df1 = pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
X_copy['vndr_id'] = X_copy['vndr_id'].apply(lambda x: hash(x))
logit = sm.Logit(df2['converted'],df2[['intercept' ,'treatment']])$ results = logit.fit()
arrows = pd.read_csv('input/data/arrow_positions.csv', encoding='utf8', index_col=[0,1])
fraud.ip_address.duplicated().sum()
bucket_name = buckets[0]$ bucket_obj = cos.Bucket(bucket_name)
obs_diff=df2.query("landing_page=='new_page'").converted.mean()-df2.query("landing_page=='old_page'").converted.mean()$ (obs_diff<p_diffs).mean()$
df.to_sql('fb_posts', engine, schema=schema, if_exists='append')
image_predictions = re.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
S_lumpedTopmodel.decision_obj.thCondSoil.options, S_lumpedTopmodel.decision_obj.thCondSoil.value
routes = json.loads(requests.get('{0}index/agencies/{1}/{2}/routes'.format(base_url, '1', '1')).text)$ print(routes)
newdf['score'].fillna(0.187218571, inplace=True)
plt.hist(p_diffs) #same histogram as above$ plt.axvline(x=obs_diff, color='red'); #adding in the line for the actual/observed difference since we are about to calculate p-value
stn_tempobs_df=pd.DataFrame(stn_tempobs,columns=['Station','Temperature (Deg. Fah.)'])$ stn_tempobs_df
for screen_name in napturalistamo_youtubers_timelines_grouped.index:$     with open("../output/black_hair_culture/text/{0}_timeline.txt".format(screen_name), "w") as text_file:$         text_file.write(napturalistamo_youtubers_timelines_grouped[screen_name])
data_dict = json.loads(r.text)['dataset']
archive_clean.to_csv('twitter_archive_master.csv')
from carto.datasets import DatasetManager$ dataset_manager = DatasetManager(auth_client)$ datasets = dataset_manager.all()
words_sum = preproc_reviews.sum(axis=0)$ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1))$ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]
print(r.json())$
site_visits = pd.read_csv("RISE_Visit_To_App_20160809.csv")
f = {'total_cost':['sum','count'], 'date_of_birth':['first']}$ total_spending = df.groupby(['uid'])['total_cost','date_of_birth'].agg(f)$
s.index
spotify_df["Streams per 1 million"] = (spotify_df["Streams"]/spotify_df["Population"]*1000000).round(2)
recommendationTable_df = recommendationTable_df.sort_values(ascending=False)$ recommendationTable_df.head()
ins.groupby('type').size()
print("1 second:", pd.to_datetime(1000000000, unit='ns'))$ print("Example 2:", pd.to_datetime(1490195805433502912, unit='ns'))
liveonly_live_woc = live_woutcome[live_woutcome.state == 'live']
tweet_df_polarity = tweet_df.groupby(["Source"]).mean()["Vader_score"]$ pd.DataFrame(tweet_df_polarity)
tweets.sort_values(by="retweets", ascending=False).head()
print(1/np.exp(-0.0408),1/np.exp(0.0099))
df.dtypes
np.exp(-0.0674), np.exp(0.0175), np.exp(0.0118), np.exp(0.0469), np.exp(0.0783)
for country in list(df_joined['country'].unique()):$     df_joined['it_'+country] = df_joined.apply(lambda row: row['ab_page']*row[country], axis=1)$ df_joined.head()
gmm.fit(X)$ labels = gmm.predict(X)$ labels
n_new = df2[df2['group']=='treatment'].count()[0]$ n_new
Base = automap_base()$ Base.prepare(engine, reflect=True)$ Base.classes.keys()$
reviews.info()$ reviews=pd.read_csv("ign.csv",index_col=['Unnamed: 0','score_phrase'])$ reviews.head()
kickstarter_req = requests.get("https://webrobots.io/kickstarter-datasets/")$ zip_to_down_ls = re.findall("href=\"(.*s3\.amazonaws.*\.json\.gz)\"", kickstarter_req.text)$ zip_to_down_ls
adj_close_acq_date_modified = adj_close_acq_date[adj_close_acq_date['Date Delta']>=0]$ adj_close_acq_date_modified.head()
date.strftime("%A")
df_new[['CA','UK','US']] = pd.get_dummies(df_new.country)$ df_new.head()
a = dat.ward.unique()$ a.sort()$ a
data_path = os.path.join(os.path.dirname('__file__'), "Data/obfuscated_data_array.json")
df[['control', 'treatment']] = pd.get_dummies(df['group'])
Base.prepare(engine, reflect=True)$
top10_df_pd=top10_df.toPandas()$ top10_df_pd.head(10)
n_new = len(df2.query("group == 'treatment'"))$ print(n_new)
n_new = df2.query('landing_page == "new_page"')['landing_page'].count()$ n_new
df2['new_page_uk'] = df2['new_page'] * df2['country_uk']$ df2['new_page_us'] = df2['new_page'] * df2['country_us']$ df2.head()
new_model =  gensim.models.KeyedVectors.load_word2vec_format(path_database+'lesk2vec.bin', binary=True)
closed_value_list = []$ for day in data_list:$     closed_value_list.append(day[4])
data.columns = ['West', 'East']$ data['Total'] = data.eval('West + East')
session.query(measurement.date).\$     filter(measurement.station == 'USC00519281').\$     order_by(measurement.date.desc()).first()
import random$ sample = data.iloc[list(random.sample(range(data.shape[0]), 200)), :]$ sample.plot.scatter(x='WRank', y='WPts')
stock_data.index
df2 = df2.join(df3.set_index('user_id'), on='user_id')$ df.isnull().sum()
sl['mindate'] = sl.groupby('wpdx_id')["new_report_date"].transform('min')$ sl['maxdate'] = sl.groupby('wpdx_id')["new_report_date"].transform('max')
t1.drop(t1[t1['retweeted_status_id'].notnull()==True].index, inplace=True)
inflation = pd.read_csv("inf.csv")$ print inflation
get_nps(combined_df, 'role_en').sort(columns='score', ascending=False)
hits = baseball_newind.h$ hits
p_old = df2['converted'].mean()$ print "Convert rate of an individual received the old page:",p_old
df.info()
text_results = []$ for tweet_text in no_dog_stage['text']:$     text_results.append(find_dog_stage_names(tweet_text))
adj_glm_int = smf.glm('hospital_expire_flag ~ C(inday_icu_wkd) * C(admission_type)', $                      data=data, family=sm.families.Binomial()).fit()$ adj_glm_int.summary2()$
G = nx.DiGraph()$ G.add_nodes_from(dfUsers['userFromId'])$
new_page_converted =  np.random.binomial(1, p = p_new,size = n_new)$ new_page_converted
eegRaw_df.to_csv(outputData, index=False, header=False) 
df2.head()
time_stamp = "2017-01-09 05:37:58.781806"$ df2 = df2[df2.timestamp != time_stamp]$ df2.info()
df_2015['sales_jan_mar']=[y if ((x.month >=1) & (x.month <=3)) else 0.0 for x, y in zip(df['Date'],df['sale_dollars'])]
print('Total items received: {:,.0f}'.format(items_received(df_receipts, rpt_dt_start, rpt_dt_end, vendor_num)))
url = 'https://twitter.com/marswxreport?lang=en'$ browser.visit(url)
polynomial_features = PolynomialFeatures(2, include_bias=False)$ polynomial_features.fit(x_train['Pending Ratio'].values.reshape(-1,1))$ training_pending_ratio = polynomial_features.transform(x_train['Pending Ratio'].values.reshape(-1,1))
top_100_2016['release_date'] = pd.to_datetime(top_100_2016['release_date'], format = '%Y-%m-%d' )
sessions.head()
df.date
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative = 'smaller')$ z_score,p_value
resultDS.to_csv(fileName, sep=delimiter, header=True, index = False)
new_df = df.dropna(subset=['driver_id'],inplace=False)
y = np.ravel(y)$ y
intervention_train.reset_index(inplace=True)$ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL'], inplace=True)
day_order = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']$ days = sns.countplot(x='Day of Week', data=merge_df, palette='viridis',order=day_order)$ days.set_title('Surveys by Day of Week')
print("customer_transaction.shape",customer_transaction.shape)$ print("data.shape",data.shape)
precipitation_df.describe()
r=pd.date_range(start='8/14/2017', periods=5)$ r
le_data = le_data_all.reset_index().pivot(index='country',columns='year')$ le_data.iloc[:,0:3]
df_countries = pd.read_csv('countries.csv')$ df_countries.head()
soup = bs(html.text, 'html.parser')
sns.distplot(questions_scores[:int(len(questions_scores)*0.99)])
new_p = df2['converted'].mean()$ new_p
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'US', 'UK']])$ result = logm2.fit()$ result.summary2()
dftops.plot(kind='bar')
station_df.head(10)
df_group = df.groupby(['breed'])['retweet_count', 'favorite_count'].mean()$ df_group = df_group.sort_values(['retweet_count', 'favorite_count'], ascending=False)$ df_group.iloc[0:14,].plot(kind='bar');
HYB, STD = set(HYB_customer_order_intervals), set(STD_customer_order_intervals)$ for customerID in HYB.intersection(STD):$     print(customerID)
mi=single_time.loc[:,['month','interest_NUM']]$ mi.groupby(by='month').mean()$
df = pd.read_csv('data/test1.csv', parse_dates=['date'], index_col='date')$ df
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')
records2 = records.copy()
plt.hist(p_diffs);$
df2.info()
def esol(time, a, datetime):$     pesol = a * 0.000015 * (1 + 0.5 * np.cos(((datetime[int(time)].month - 1) * 3.14) / 6.0))$     return pesol
ccp_df = cc_df[coins].div(market_cap_df['Total Market Cap'], axis=0)$ ccp_df = ccp_df[::-1]    # Reverse order of df$ ccp_df.head()
data_df = pd.read_csv('../datasets/flight_delays_data.csv')$ data_df.shape
complaints['Date Index'] = pd.to_datetime(complaints['Date Index'], format = "%m/%d/%Y %H:%M:%S")$
train.head()
def only_upper(s):$     return "".join(c for c in s if c.isupper())
merged_data['due_day'] = merged_data['due'].dt.day$ merged_data['due_month'] = merged_data['due'].dt.month$ merged_data['due_year'] = merged_data['due'].dt.year
auth = tweepy.OAuthHandler(ckey, csecret)$ auth.set_access_token(atoken, asecret)
clean_stations = pd.concat([stations, stat_info_merge], axis=1)
df.to_csv('../output/releases_with_demographics.csv')
df_new['US_page'] = df_new['US'] * df_new['ab_page']$ df_new['UK_page'] = df_new['UK'] * df_new['ab_page']$ df_new.head()
df.index$
loc = session.query(Stations.station_id).count()$ print("Total {} stations were used to gather data.".format(loc))
rain_df = pd.DataFrame(rain)$ rain_df.head()
old_page_converted = np.random.binomial(1, p_old,n_old)$ old_page_converted.mean()
df2['intercept'] = 1$ df2['ab_page'] = np.where(df2['group'] == 'treatment', 1, 0)$ df2.head()
df_schools.describe()
model_df['score_str'] = "x"$ model_df.score_str[model_df.score <= model_df.score.quantile(.5)] = "below_avg"$ model_df.score_str[model_df.score > model_df.score.quantile(.5)] = "above_avg"
df_inventory_santaclara['Year']=(df_inventory_santaclara['Date'].str.split('-').str[0])
data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/0ddd2680-e35d-46bc-ac1a-d350da4f409d/ar24011.asc'
precipitation_df = pd.read_sql(session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23").statement,session.bind)$ precipitation_df.head(15)
list_of_features.append(("Month", animals_clean["Month"]))
my_model_q9 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label')$ my_model_q9.fit(X_train, y_train)$ base_model_relation, base_accuracy_comparison = my_model_q9.base_model_eval()
s3 = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5})$ s3
shows = pd.read_csv("ismyshowcancelled_tmp_1.csv",index_col=0)
ls = pd.to_datetime([obs['ResultDateTime'] for obs in results])$ print('Start Date: {0} | End Date: {1}'.format(ls.min().isoformat(), ls.max().isoformat()))
b_df = df.index.isin([5,12,23,56])$ df[~b_df].head()
dataframe.head(20)
final_df = pd.merge(result,price_by_cluster,on='cluster',how = 'left')$ final_df.head(10)
test_df["num_description"]= test_df["description"].apply(num_description)$
print(r.json()['dataset_data']['data'][:4])
data['SA'] = np.array([ analyseSentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df_new['US_ab_page'] = df_new['US'] * df_new['ab_page']$ df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page']$ df_new.head()
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
from scipy.stats import norm$ print("The significance of our z-score is: {}".format(norm.cdf(z_score)))$ print("Our critical value at 95% confidence is: {}".format(norm.ppf(1-(0.05))))
df_precep_dates_sorted_12mo = df_precep_dates_12mo.sort_values(by=['date'])$ df_precep_dates_sorted_12mo
data['timestamp'] = pd.to_datetime(data['Date'], unit='s')
df.sheet_names # see all sheet names$
result = api.search(q='%23Australia')  # "%23" == "#"$ len(result)
single_time=train_data.loc[:,['created','interest_level']]
df.describe()
np.shape(prec_fine)
print(sells_primer_2015.keys())$ print(sells_segundo_2015.keys())$
plt.plot(scores); plt.title("Scores");
N_new = df2.query("landing_page == 'new_page'")["user_id"].count()$ print("The dataset consists of {} new pages.".format(N_new))
users_chanel = pd.merge(Users, Relations, how = 'outer', on=['id_partner', 'name'])$ users_chanel.head()
df = pd.read_csv('weather.csv')
import RPi.GPIO as GPIO$ import time
plt.hist(p_diffs)$ ab_data_diff = prob_treatment_converted - prob_control_converted$ plt.axvline(ab_data_diff , color = 'red');
for col in data_df.columns:$     if np.unique(data_df[col].dropna().astype(str)).shape[0] <= 1:$         print(col)
pd.timedelta_range(0, periods=10, freq='H')
df2 = df.fillna(0)$ df2
df_img_algo_clean = df_img_algo.copy()$ df_img_algo_clean.head()
! wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.9" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/10038/99000/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1538843579&Signature=CMMknLvJRFdZOeLNVz20xx84XMeuMgwdpJ9sZmDpZPTIy2%2BUsg0KfFz8fZx15CyMQt1FJNNt35Asqz8w96H%2BgDb2apGOiXkTCIMQAebwSmcT82A%2Bn8mNIBA7UVb4vsLnokTIy39%2FtzLAAaIgDfOBk0224DGt0Qzve1Yridk33aOCBYZD1C7NGmG%2FMOzdLjJp3%2FkqS08PVcDlSg6ZIKrziIdg%2BMAhILrQtenrAOGXuKWON2mE7S5%2BY379ubviVYXltB3vDgwMPIA%2BQmGQLvokne4k7uPI9OQf4MZBStN74Ndmldcifn6Pi72wDXIYbEqnbsFq0geTRZBDzZQm9VsE8A%3D%3D" -O "data/train.csv.zip" -c$ ! wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.9" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/10038/99000/test.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1538843531&Signature=ScKXJBDHKlCtQw1iWjzD7AF%2FRPlr3zC3szdm%2B6MTX5WUyYfVZDelYXGfpGFHSsO1io2rmo5wzseYxZHQWg5zlaAY8TWtRMnTRUTjEJQHwrrF8amHsKwE%2FabUh%2B%2BIxfmrQygH36MMPy4UUBs1UGKRaby9R9LsqxkSf5WhBZrIrBpYfPoOFishYdIqfMd6QStEPyjZg5ciEyd1nQYuUP7jY%2BMfSsLjw7XS8jNUq8qXh1cb86D07jjyEwCtumIxTMPClh6Wwp1ljFXe1pOlxKHRQwWecPe6yfJXRqrOcptnUSI71NPMPBcXw9%2Fbx8dFK0%2BZsGXgNb7GZh1wH5eNaVrMWQ%3D%3D" -O "data/test.csv.zip" -c$ ! wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-US,en;q=0.9" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/10038/99000/sample_submission.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1538843600&Signature=WQ1mnLPbKV0sob7l2PIFWW3gXXNkWUcsVks0njs82Pvq4BeV3t3K5sOXiYyIsP68TieXXfDIQRAn4bFfCIAJZr11tt5T6Va7tsD0lvjXBKzpTzMaDK%2BBQagUul2nul25r1gPg4ajwbIzts%2BRoAK7y%2FWiRXNwWNwBAGJo1RbHTVTFMvFLqJpcj28o5zFPM6h%2BgT%2FPAtN6zPpuCFw6epDne8v1qhEtJYBiYUoDJVyJg1Fdsq9QCun%2F2MmH0T9dOu2OWWcDM73RewFs1QVbIGPlAFF8V0yiH4GEX2srF%2Bk96Gb6jeLAav4TnZ2MPOLwXDfTUQO0uucKbhFiKqUc8C3DeA%3D%3D" -O "data/sample_submission.csv.zip" -c$
state_DataFrames['OH_R_con'].head(30)
pd_exec_time=pd.to_datetime(r.json()['executionTime'])$ pd_exec_time.strftime('%s')
gdax_trans_btc['Timestamp'] = gdax_trans_btc['Timestamp'].map(lambda x: x.replace(second=0))
importances=model_tree_6_b.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
july_1 = pd.to_datetime('7/1/2016')$ july_1
actualDiff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean()$ (actualDiff < pDiffs).mean()
pd.set_option('display.max_colwidth', 100)$ clinton_df.head()
ved = pd.read_excel('input/Data.xlsm', sheet_name='51', usecols='A:W', header=12, skipfooter=4)
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", start='1980',end='2014')$ le_data_all
!cp "empty_db.sqlite" "db.sqlite" 
dog_ratings.to_csv('twitter_archive_master.csv', index=False, encoding = 'utf-8')
df['y'].plot.hist(bins=15)
df2 = df$ df2.drop(mismatch_all.index,inplace=True)
filename = "HubProcessDurations.xlsx"$ xl = pd.ExcelFile(filename)$ tabs = xl.sheet_names
guido_text = soup.text$ print(guido_text[:500])
cityID = '1d9a5370a355ab0c'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chicago.append(tweet) 
print(filecounts.to_string())
df_weather.precipitation_inches.unique()
autos["price"] = autos["price"].str.replace("$", "").str.replace(",", "").astype(int)$ autos["price"].head()
(category_count_df2["category"].sum() / category_count_df1["category"].sum()).round(2)$
stocks['Date'] = stocks['Date'].dt.week
y = df['hospitalmortality'].values$ X = df.drop(['hospitalmortality'], axis=1).values$ X_header = df.drop(['hospitalmortality'],axis=1).columns
cur.execute('SELECT LangCd ,count(*) FROM surveytabl WHERE LangCd="QB";')$ cur.fetchall()
results = logit.fit()$ results.summary()
df = pd.read_csv("FuelConsumption.csv")$ df.head()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)])$ new_page_converted.mean()
print(AFX_X_06082017_r.json())
import matplotlib.pyplot as plt
cercanasA1_11_14Entre150Y200mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 150) & (cercanasA1_11_14['surface_total_in_m2'] < 200)]$ cercanasA1_11_14Entre150Y200mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre150Y200mts.apply(descripcionDistancia, axis = 1)$ cercanasA1_11_14Entre150Y200mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
df = pd.read_excel("msft.xls")$ df.head()
containers[0].find("div",{"class":"key"}).a['title'].split()[0].replace(',',"")
recommendationTable_df = ((genreTable*userProfile).sum(axis=1))/(userProfile.sum())$ recommendationTable_df.head()
year_with_most_commits = commits_per_year.idxmax()[0].year$ year_with_most_commits
trip_data_q5["weeks"] = trip_data_q5.lpep_pickup_datetime.apply(lambda x: np.floor((pd.Timestamp(x) - pd.datetime(2015, 9, 1))/np.timedelta64(1, 'W'))+1)
pd.DataFrame.to_csv(receipt_data, 'receipt_data.csv')
log_m = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])$ results = log_m.fit()
df_new['country'].unique()
tz_cat['tweetRetweetCt'].max()$ tz_cat.index[tz_cat['tweetRetweetCt'] == tz_cat['tweetRetweetCt'].max()].tolist()
matplotlib.pyplot.hist(y_pred)
content = [item.find_all(['p','h2']) for item in article_divs]$
country=pd.read_csv('countries.csv')
log_mod_new = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']])$ results_new = log_mod_new.fit()$ results_new.summary()
df.user_id.nunique()
top20 = autos['brand'].value_counts().head(20).index
train.head(3)
people['Id'] = pd.to_numeric(people['Id'], errors='coerce')
p_new = df2['converted'].mean()$ p_new
df = df.loc[df['job_id'] == '2572']
scraped_batch6_top.to_csv('batch6_top.csv', encoding='utf-8')$ scraped_batch6_sec.to_csv('batch6_sec.csv', encoding='utf-8')
df[df.converted == 1].user_id.count()/df.shape[0]$ df.converted.mean()
1/np.exp(-0.0150)
Precipitation_DF.head(10)
import pandas as pd$ weather = pd.DataFrame(weather_json['data'])$ weather['date']  = pd.to_datetime(weather['date'])
from dotce.report import (most_informative_features,$                           ascii_confusion_matrix)
pos_sent = open("positive_words.txt").read()$ neg_sent = open("negative_words.txt").read()$ print(pos_sent[:101])
iex_coll_reference.count()
autos.head()
y_pred = logreg.predict(X_test)$ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))
lsi = models.LsiModel(corpus_tfidf, id2word=bag, num_topics=10)$ corpus_lsi = lsi[corpus_tfidf]
engine = create_engine("sqlite:///hawaii.sqlite")
theft.iloc[0:10]
faulty_names = archive_clean[archive_clean.name.str.islower()].name$ print(faulty_names)
accuracy = accuracy_score(y_test, y_pred)$ print('Accuracy: {:.1f}%.'.format(accuracy * 100.0))
shown = pd.DataFrame(data.tasker_id.value_counts())$ shown.loc[shown['tasker_id']==1]
df_ct.to_csv("can_tire_senti_score.csv", encoding='utf-8', index=False)
df3.values
with open(saem_women_save, mode='w', encoding='utf-8') as f:$     f.write(SAEMRequest.text)
df_twitter_copy['rating_numerator'] = df_twitter_copy['rating_numerator'].astype(float)$ df_twitter_copy['rating_denominator'] = df_twitter_copy['rating_denominator'].astype(float)
df8_lunch.count()
df2.drop_duplicates('user_id',inplace=True)
fuel_mgxs = mgxs_lib.get_mgxs(fuel_cell, 'nu-fission')
S_distributedTopmodel.decision_obj.groundwatr.options, S_distributedTopmodel.decision_obj.groundwatr.value
graph.run("CREATE CONSTRAINT ON (u:User) ASSERT u.user_name IS UNIQUE;")$ graph.run("CREATE CONSTRAINT ON (t:Tweet) ASSERT t.tweet_id IS UNIQUE;")$ graph.run("CREATE CONSTRAINT ON (h:Hashtag) ASSERT h.tag IS UNIQUE;")
df_draft.isnull().sum().sum()
r1.keys()
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
df.drop(["urlname"], axis = 1, inplace=True)$ top_rsvp.drop(["urlname"], axis = 1, inplace=True)$ top_rsvp.head(5)
df.isnull().sum().sum()
trump_month_distri.plot(kind='bar', figsize=(10,5), rot= 45,title="# of Twitters of Donald Trump")$ plt.savefig('fig/trump_month.png');
n_old = df_old.shape[0]$ print(n_old)
tweets_clean.dog_type.value_counts()
driver = selenium.webdriver.Safari() # This command opens a window in Safari$ driver.get('https://www.boxofficemojo.com')
data = pd.DataFrame(data=[tweet.text for tweet in searched_tweets], columns=['Tweets'])$ display(data.tail(10))
df_new[['ca','uk','us']] = pd.get_dummies(df_new['country'])$ df_new.drop(['ca'], axis=1, inplace=True)$ df_new.head()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='API_KEY$ r = requests.get(url)
len(df.DATE.unique()), len(df)
df_students.describe()
print cust_data.head(5)$ print cust_demo.head(5)$ print cust_new.head(5)$
breed_predict_df_clean['tweet_id'] = breed_predict_df_clean.tweet_id.apply(str)$
prob_treat_converted = df2[df2['group'] == 'treatment']['converted'].mean()$ print (prob_treat_converted)
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]
top_apps2 = df_nona.groupby('app_id').district_id.nunique()$ top_apps2.sort(inplace=True, ascending=False)$ top_apps2.head(10)
my_stream.disconnect()
tweets['location'] = tweets['location'].str.strip()$ tweets.groupby(tweets.location).count()['id'].sort_values(ascending=False)$
pd.Timestamp("now")
ax = trump_histogram(trump_df)$ plt.show()
pd.Series(np.random.randint(0,10,10)).plot();
X_train = X_train.join(y_train) 
import datetime as dt$ start_time = dt.datetime.now()
pp = rf.predict_proba(X_train)$ pp = pd.DataFrame(pp, columns=['The_Onion_Prob', 'VICE_Prob', 'GoldenStateWarriors_Prob'])
plantlist[plantlist['commissioned'].isnull()]
sentiment_hour = {k: g['sentiment_score'].tolist() for k,g in sentiment_plot_data.iloc[6:].groupby('created_hour')}$ sentiment_hour
trips_data['duration'].hist() # displays histogram for duration of trips $
myTxns[~myTxns['UserID'].isin(users['UserID'])]
start = datetime.now()$ print((datetime.now() - start).seconds)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)$
avg_order_intervals = np.fromiter(result.values(), dtype=float)$ avg_order_intervals.size
trump['text'] = trump['text'].str.lower()$
sum(df.duplicated())
pd.to_datetime(['04-01-2012 10:00'])
treatment_conv = df2.query('group == "treatment"')['converted'].mean()$ treatment_conv
mean = np.mean(data['len'])$ print("The length's average in tweets: {}", format(mean))$
y=dataframe1['RSI_30']$ plt.plot(y)$ plt.show()
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv', nrows=10)
archive_df_clean = archive_df.copy()$ archive_df_clean = archive_df_clean[archive_df_clean['retweeted_status_id'].isnull()]
df_group=df2.groupby('group')$ df_group.describe()$
predictions = rfModel.transform(testData)
check_null = df.isnull().sum(axis=0).sort_values(ascending=False)/float(len(df))$ np.sum(check_null.values) == 0
S_lumpedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_lumpedTopmodel.txt')
c = watershed.unary_union.centroid # GeoPandas heavy lifting$ m = folium.Map(location=[c.y, c.x], tiles='CartoDB positron', zoom_start=12)
names.str.capitalize()
df[(df.country == "es") & (df.city == "Valencia")].id.size
df.head()
health_data.loc[:, ('Bob', 'HR')]
dfn.to_csv('News.csv')
startups_USA = df_companies[df_companies['country_code'] == 'USA']$ startups_USA.head()
df_user = pd.read_csv(user_source)$ df_user.head()
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()$ print(last_date)
questions = questions.drop(questions.index[[9,22]])
turnstiles_df['DateTime'] = turnstiles_df['DATE'] + turnstiles_df['TIME']$ turnstiles_df['DateTime'] = pd.to_datetime(turnstiles_df['DateTime'], format='%m/%d/%Y%H:%M:%S')
plt.hist(p_diffs);$ plt.axvline(x=calc_diff, color = 'red');
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline',$                         xls_dir='scen2xls',$                         file_name='data.xlsx', verbose=False)
paragraphs = soup.find_all('div', class_='rollover_description_inner')   $ print(paragraphs)
views = containers[0].find("li", {"class":"views"}).contents[1][0:3]$ views
df_new['country'].value_counts()
learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr=(32,10)) $
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
df.language.value_counts()$
pandas_ds["time2close"].plot(kind="hist")
old_page_converted = np.random.binomial(1, p_old, n_old)$ old_page_converted
df2['user_id'].duplicated()$ sum(df2['user_id'].duplicated())
df2 = df[['MeanFlow_cfs','Confidence']]$ df2.head()
appended = ad_data1.union(ad_data2)
user = DataSet['userName'].value_counts()[:10]$ print(user)
temps_df.Missouri
intervention_train.reset_index(inplace=True)$ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
match = results[2]$ print lxml.html.tostring(match)
punct_re = r''$ trump['no_punc'] = ...
pos_sent = open("../data/positive_words.txt", encoding='utf-8').read()$ neg_sent = open("../data/negative_words.txt", encoding='utf-8').read()$ print(pos_sent[:101])
games_df = pd.DataFrame(game_data_all).copy()
elements.head(5)
planets.groupby('method')['year'].describe().unstack()
tfidf.fit(text)
conn.execute(sql)
engine = create_engine("sqlite:///Resources/hawaii.sqlite")
url = "https://api.census.gov/data/2016/acs/acs1/variables.json"$ resp = requests.request('GET', url)$ aff1y = json.loads(resp.text)
station_availability_df.loc[station_availability_df['status_key']==1,'status_value'] = "In Service"$ station_availability_df.loc[station_availability_df['status_key']==3,'status_value'] = "Not In Service"$ station_availability_df.status_value.value_counts()
sgd = SGDClassifier(loss="log", alpha=0.01,$                                          learning_rate='optimal')
df2.user_id.nunique(), countries_df.user_id.nunique()
mit.date = pd.to_datetime(mit.date)
metrics.r2_score(y_minutes_test,pred)   # we got 88.6% r2_score with test/prediction$
logistic_mod_pagecountry = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','US']])$ results_pagecountry = logistic_mod_pagecountry.fit()$ results_pagecountry.summary()
TestData[['DOB_clean', 'Lead_Creation_Date_clean']].describe()
df2_copy.shape == df2.shape
c_ctrl = df2.query('group == "control"')['converted'].mean()$ c_ctrl
bar_prep = sentiments_pd.groupby(['Outlet'],as_index=False).mean()[['Outlet','Compound']]$ bar_prep.head()
sentiment_df['Timestamp'] = pd.to_datetime(sentiment_df.Timestamp)$ sentiment_df.sort_values(by='Timestamp')
funding_pf = funding_df.rename(columns={$     "Company Name" : "Company"$ })$
cust_demo.duplicated().value_counts()$
store_items = store_items.append(new_store, sort=False)$ store_items
Z = np.arange(1,15,dtype=np.uint32)$ R = stride_tricks.as_strided(Z,(11,4),(4,4))$ print(R)
df3[['CA','UK','US']] = pd.get_dummies(df3['country'])$ df3.head()
json.loads(rec)
df2 = df.drop(df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].index)
count_vect = CountVectorizer(vocabulary=unique_feature_list, tokenizer=lambda x: x.split(','))
x=[i for i in range(11)]$ x$ network_simulation[network_simulation.generations.isin(x)]$
data_df = pd.read_csv('bouldercreek_09_2013.txt', sep='\t', keep_default_na=False, na_values=[""])$ data_df['datetime'] = pd.to_datetime(data_df['datetime'])
df_corr = result.groupby(['type', 'scope'])['user'].sum().reset_index()$ display(df_corr.sort_values('user',ascending=False).head(10))$ plot2D(df_corr, 'type', 'scope','user')
results_jar_rootDistExp, output_jar_rootDistExp = S.execute(run_suffix="jar_rootDistExp", run_option = 'local')
new_page_converted=np.random.choice([0,1],size=nnew[0],p=[pnew,1-pnew])$ new_page_converted = new_page_converted[:145274]$ print(len(new_page_converted))
data = pd.Series([0.25, 0.5, 0.75, 1.0],$                 index=['a','b','c','d'])$ data
jail_census.loc['2017-02-01'].groupby('Race')['Age at Booking'].mean()
from nltk.corpus import stopwords$ import string$ punc = list(string.punctuation)$
tweets_df.tail(3)
tweets.dtypes
rows.describe()
requests.get(wikipedia_content_analysis)
income_raw = data['income']$ features_raw = data.drop('income', axis = 1)$ vs.distribution(data)
import matplotlib.pyplot as plt$ import seaborn as sns$ %matplotlib inline
df.head(20)
cm10 = ConfusionMatrix(y10_test, predicted10)$ cm10.print_stats()
df_vow['Close'].unique()
stations = session.query(Measurement).group_by(Measurement.station).count()$ print("There are {} stations.".format(stations))
live_capture = pyshark.LiveCapture(interface='wlan1')
train.drop('second',1,inplace=True)$ train.drop('minute',1,inplace=True)$ train.drop('action',1,inplace=True)$
prcp_analysis_df = df.rename(columns={0: "Date", 1: "precipitation"})$ prcp_analysis_df.head()
(fe.bs.SPXmean, fe.bs.SPXsigma)
pd.read_pickle('data/wx/tmy3/proc/703950.pkl', compression='bz2').head()
tm_2020 = pd.read_csv('input/data/trans_2020_m.csv', encoding='utf8', index_col=0)
df.dropna(inplace=True)$ df.shape
p_old = round(float(df2.query('converted == 1')['user_id'].nunique())/float(df2['user_id'].nunique()),4)$ p_old
coming_next_reason = coming_next_reason.drop(['[', ']'], axis=1)$ coming_next_reason = coming_next_reason.drop(coming_next_reason.columns[0], axis=1)
sentiments_df = pd.DataFrame.from_dict(sentiments)$ sentiments_df.head()
TEST_DATA_PATH = SHARE_ROOT + 'test_dataframe.pkl'$ test_df.to_pickle(TEST_DATA_PATH)
open_price = [x[1] for x in data2 if x[1]!=None]  # needs to exclude None. $ print('The highest opening price during 2017 is : ',max(open_price))$ print('The lowest opening price during 2017 is : ',min(open_price))
perf_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)
plt.hist(null_value)$ plt.axvline(obs_diffs, c="r")
mfx = sts_model.get_margeff()$ print(mfx.summary())
df.to_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext.csv", sep=',')
cities[cities.isnull()] = 0$ print(cities)
plt.title("Aggregate Media Sentiment baed on Twitter")$ plt.ylabel("Tweet Polarity")
results = session.query(measurement.date, measurement.prcp).filter(measurement.date >= prev_year).all()
def join_df(left, right, left_on, right_on=None, suffix="_y"):$   if right_on is None: right_on=left_on$   return left.merge(right, left_on=left_on, right_on=right_on, suffixes=("",suffix))
df_countries.set_index('user_id', inplace=True)
df1['cleantime'].max()-df1['cleantime'].min()
session.query(measurement.date).order_by(measurement.date.desc()).first()
log_mod_countries = sm.Logit(df_new['converted'],df_new[['intercept','US','UK']])$ results_countries = log_mod_countries.fit()$ results_countries.summary()
dog_ratings = pd.merge(twitter_archive_copy, image_predictions_copy, how = 'left', on = ['tweet_id'] )$ dog_ratings = pd.merge(dog_ratings, tweet_json_copy, how = 'left', on = ['tweet_id'])$ dog_ratings.info()
utility_patents_subset_df['prosecution_period'] = utility_patents_subset_df.grant_date - utility_patents_subset_df.filing_date$ utility_patents_subset_df.prosecution_period = utility_patents_subset_df.prosecution_period.apply(lambda x: x.days)$ utility_patents_subset_df.prosecution_period.describe()
print(dfd.in_pwr_5F_max.describe())$ dfd.in_pwr_5F_max.hist()
df_repub = df[df.party == 'Republican']
most_active_station_tobs = session.query(Measurement.tobs).\$ filter(Measurement.station == most_active_station, Measurement.station == Station.station,\$        Measurement.date >="2017-08-01", Measurement.date <="2018-07-31").all()
year_dict = year_request.json()$ print (type(year_dict))
crimes.dtypes
df.dropna(subset=['insert_id'], how='all')$ df = remove_duplicate_index(df)
X1 = pd.concat([X, df_yelp, df_frnd, df_elite], axis=1)$ X1.head()
results.summary()
for div in soup.find_all('div'):$     print (div.text)
df = df.drop_duplicates(subset='id', keep='last')$ df.drop(columns='Unnamed: 0', axis=1, inplace=True)$ print(df.shape)
eth = pd.read_csv("Ether-chart.csv", sep=',')$ eth['date'] = ' '$ eth.info()$
df[['beer_name', 'rating_score', 'simple_style', 'brewery_name', 'brewery_country']][df.rating_score < 2].sort_values('rating_score')
print("Today's Date is:", dt.date.today())$ query_date = dt.date.today() - dt.timedelta(days=365)$ print("One year ago today is:", query_date)$
validation.analysis(observation_data, BallBerry_simulation)
fe.plots.plotqq(ss)
query = session.query(Measurement.date, func.sum(Measurement.prcp)). \$ group_by(Measurement.date).order_by(Measurement.id.desc()).limit(365).all()$ query
print('Coefficients: \n', list(zip(feature_names, regressor.coef_)))
end_time = dt.datetime.now()$ (end_time - start_time).total_seconds()
ppt_date.describe()$
norm.ppf(1-(0.05/2))
rain_df = pd.DataFrame(rain)$ rain_df.head()
g_kd915 = kd915[(kd915.state != 'live') & (kd915.state != 'suspended') & (kd915.state != 'canceled')].groupby(['blurb'])$ kd915_filtered = g_kd915.filter(lambda x: len(x) <= 1).sort_values(['blurb','launched_at'])
(data_2017_12_14[data_2017_12_14['text'].str.contains("instantaneamente", case = False)])["text"].head()
X = data.values$ from sklearn.preprocessing import StandardScaler$ X_std = StandardScaler().fit_transform(X)
m_vals = np.linspace(m_true-3, m_true+3, 100) $ c_vals = np.linspace(c_true-3, c_true+3, 100)
df1['forcast'].head() #allthe data will be nan $
res = sts.query(qry)
RDDTestScorees.map(lambda entry: (entry[0], entry[1] * 0.9)).collect()
pt = pd.pivot_table(airbnb_df, index='date_listed', values='id', aggfunc=[len])$ pt.rename(columns={'len':'num_listings'}, inplace=True)$ pt.head(10)
logit_result = logit_mod.fit()$ logit_result.summary()
df_new[['US','UK']] = pd.get_dummies(df_new['country'])[['US', 'UK']]                        
new_page_converted = np.random.binomial(n_new , p_new)$ new_page_converted
df.index = pd.DatetimeIndex(df['DATE'])
!pip install pandas-datareader$ !pip install --upgrade html5lib==1.0b8$
with model:$     idx = np.arange(n_count_data)$     lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)
groceries.drop('apples')
def prepare_data_directory():$     if not os.path.exists(data_directory):$         os.makedirs(data_directory)
r2017_dict = r2017.json()
!h5ls -r 'data/my_pytables_file.h5'
SP_data = "..\Raw Data\S&P 500 Raw data_1-20-2017 ~ 3-21-2018.csv"$ SP_df = pd.read_csv(SP_data, encoding = "ISO-8859-1")$ SP_df.head()
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2018-03-27&end_date=2018-03-27&api_key='+API_KEY)
records['State'].value_counts().plot.bar()$ plt.title("Students per State", fontdict={'fontsize': 14});$ plt.savefig('images/barplot_state.png')
df.drop(df[['prospectid', 'ordernumber', 'ordercreatedate', 'dnatestactivationdayid', 'xsell_gsa', 'xsell_day_exact' ]], axis=1, inplace=True)
max_change_key = max(change_dict.keys(), key=(lambda k: change_dict[k]))
act_diff = df[df['landing_page'] == 'new_page']['converted'].mean() -  df[df['landing_page'] == 'old_page']['converted'].mean()$ (act_diff < p_diffs).mean()
df.rename(columns={'nm':'city','countryCode':'countrycode'},inplace=True)$ df.set_index("id", inplace=True)$ df.head()
num_rows = df.shape[0]$ num_rows
shows['fixed_runtime'] = shows['runtime'].dropna().apply(fix_runtime)
trump['source'] = trump["source"].str.replace("<.*?>", "").str.replace("</a>", "")$ trump['source'].value_counts().plot(kind="bar")$ plt.ylabel("Number of Tweets");
from IPython.display import Image$ Image('/Users/jdchipox/Desktop/SV40table.png')
df2.info()
clean_appt_df.groupby('Gender')['No-show'].value_counts(normalize=True).plot.bar()
df.iloc[99:110,3:]
def dropyears(df_):$     return(df_.truncate(before = "1980-7"))  
speeches_metadata = speeches_cleaned.merge(metadata_df_all, left_on = 'id', right_on = 'accessids', how = 'inner')
importances=model_rf_19_S.feature_importances_$ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns)$ features
df.hist(bins=100, figsize=(11,8),color='black')$ plt.show()
new_page_conversions = np.random.binomial(n_new,p_new,10000)$ old_page_conversions = np.random.binomial(n_old,p_old,10000)$ p_diffs = [(x/n_new) - (y/n_old) for x, y in zip(new_page_conversions, old_page_conversions)]
all_sessions = cuepoints['session_id'].unique()$ all_sessions
plt.rcParams['figure.figsize'] = 8, 6 $ plt.rcParams['font.size'] = 12$ viz_importance(rf_reg, wine.columns[:-1])
df2 = df.copy()$ df2[df2.isnull()] = 0$ df2
calls = pd.read_csv("311_Calls__2012-Present_.csv")
Val_eddyFlux = Plotting(hs_path + '/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
%matplotlib inline$ env_test.unwrapped.render('notebook', close=True)$ env_test.unwrapped.render('notebook')
df.head()
data.dtypes
date + np.arange(12)
capitalizer = lambda x: x.upper()$
census['GEOID_tract']=census.apply(lambda x: x['GEOID'][:11],axis=1)
max_tweets=1$ for tweet in tweepy.Cursor(api.search, q="$AXP").items(max_tweets):$     print(tweet.text)
group_period = '7D'$ cc_df = cc_df.resample(group_period).mean()$ cc_df.head()
df_users_products=pd.merge(left=df_user_product_ids,right=transactions,how="left",left_on=["UserID","ProductID"],right_on=["UserID","ProductID"])[["UserID","ProductID","Quantity"]]
df_CLEAN1A['AGE_groups'] = df_CLEAN1A['AGE_groups'].astype('category')
from scipy.stats import norm$ norm.cdf(z_score)
load_weigths_into_target_network(agent, target_network) $ sess.run([tf.assert_equal(w, w_target) for w, w_target in zip(agent.weights, target_network.weights)]);$ print("It works!")
df['Forecast'] = np.nan
r = requests.get(data_request_url, params=params, auth=(USERNAME, TOKEN))$ data = r.json()
df_new.info()
news_p = soup.find_all('p')$ for paragraph in news_p:$     print(paragraph.text)
df.groupby("PredictedIntent").agg({'Notes':'count'})
scaler_fit.inverse_transform(predict_actual_df).shape
from sklearn.decomposition import PCA$ pca = PCA(random_state=100)$ pca.fit(X_train)
np.random.seed(1) $ model = models.LdaMulticore(corpus, id2word=dictionary, num_topics=10, workers=5,passes=200, eval_every = 1)
df_enhanced = df_enhanced.drop('rating_denominator', axis=1)
flu_vacc_file4= "Data/NYVacc2015-17.csv"$ vacc15_17 = pd.read_csv(flu_vacc_file4)$ vacc15_17.head()
fraud = pd.read_csv('./data/problem2/fraud_with_countries.csv')$ fraud.head()
df[df.isnull().any(axis=1)]
tweet_df.info()$
pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')
from pandas.io.json import json_normalize$ user_info = json_normalize(tweets_clean['user'])$ user_info.columns
pd.concat(g for _, g in df2.groupby("user_id") if len(g) > 1)$
pd.date_range('12/31/2017', '01/01/2018', freq='H') $
tweetsIn22Mar = tweets22Mar[tweets22Mar.lang == 'in']$ tweetsIn1Apr = tweets1Apr[tweets1Apr.lang == 'in'] $ tweetsIn2Apr = tweets2Apr[tweets2Apr.lang == 'in']
session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\$     order_by(desc(func.count(Measurement.station))).all()
type(parsed_json)$ print(dir(parsed_json))$ parsed_json.items()
grp_num_starters_pp = df_factors_PILOT.groupby('x8_num_starters_pp')$
df1 = df.groupby('id').median()$
print("Percentage of positive Federer's tweets: {}%".format(len(pos_tweets_rf)*100/len(data_rf['Tweets'])))$ print("Percentage of neutral Federer's tweets: {}%".format(len(neu_tweets_rf)*100/len(data_rf['Tweets'])))$ print("Percentage de negative Federer's tweets: {}%".format(len(neg_tweets_rf)*100/len(data_rf['Tweets'])))
tweet_pickle_path = r'data/twitter_01_20_17_to_3-2-18.pickle'$ tweet_data.to_pickle(tweet_pickle_path)
headers= ({'User-agent': 'kiros Bot 0.1'})
df3 = pd.concat([non_align_1, non_align_2])$ df2 = df.drop(df3.index) 
df['intercept']=1$ df[['treatment','control']]=pd.get_dummies(df['group'])
credentials = json.load(open('./apikey.json', 'r'), encoding='utf-8')
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new = df_new.drop('US', axis=1)$ df_new['intercept'] = 1
df_users =  pd.read_sql(SQL, db)$
new_page_converted = np.random.choice([0,1], size = n_new, p=[1-p_new,p_new])
e_gb_month = e_df.groupby(e_df.unix_time)
pd.read_pickle('data/city-util/proc/city.pkl', compression='bz2').head()
archive_clean[archive_clean['in_reply_to_status_id'].notnull()]
active_unordered = unordered_df.loc[~churned_unord]
repeated = df2[df2.user_id.duplicated()]$ print("The repeated 'user_id' in df2 is:\n {}".format(repeated))
p_new = len(df2.query("converted=='1'"))/ len(df2)$ p_new
newdf["Hour_of_day"] = taxiData.lpep_pickup_datetime.apply(getHour)
pulledTweets_df = gu.read_pickle_obj(processed_dir+'pulledTweetsProcessedAndClassified_df')
plt.hist(p_diffs);$ plt.ylabel('Frequency')$ plt.xlabel('p_diffs')
tweet_archive_clean = pd.merge(tweet_archive_clean, images, on = 'tweet_id', how = 'inner')
kayla['SA'] = np.array([ analyze_sentiment(tweet) for tweet in kayla['Tweets'] ])$ kelsey['SA'] = np.array([ analyze_sentiment(tweet) for tweet in kelsey['Tweets'] ])
seriesPost_df = pd.read_csv('SeriesPost.csv')$ seriesPost_df.tail()
iris.groupby('Species')['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'].describe()
tweet = api.get_status(id = '892420643555336193')$ print(type(tweet))$ print(tweet.keys())
!rm lazy_helpers.py*$ !wget https://raw.githubusercontent.com/holdenk/diversity-analytics/master/lazy_helpers.py
evaluation_df = predictions_df.reset_index().sort_values(by=['unit', 'item', 'index'])$ evaluation_df['actual_value'] = test_df.sort_values(by=['unit', 'item', 'date'])['value'].values
tweets['rating_denominator'].nunique()$
df1_clean.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis = 1, inplace=True)
import re$ regex_1 = re.compile('\w[A-Z]+')
f.groupby('Price').mean()
articles['tokens'] = articles['tokens'].map(lambda s: [w for w in s if len(w)>1])
all_funded_apps = pd.concat([funded_apps_1, funded_apps_2])
df.ix['2010-01-07']
joined=pd.merge(df2, countries, on=['user_id','user_id'])$ joined.head()
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ]$ GSW_2017 = games_2017.loc[games_2017.loc[:, "Team"] == "GSW", ]$ rest_2017 = games_2017.loc[games_2017.loc[:, "Team"] != "GSW", ]
data_year_df['Date'] = pd.to_datetime(data_year_df['Date'])$ data_year_df.head()$
predictions = lrModel.transform(testData)$ predictions.select("prediction", "label", "features").show(5)$ predictions.select('label', 'prediction').coalesce(1).write.csv('D://Data Science//pySpark//check_pred7.csv')
df_ad_airings_4.to_pickle('./TV_AD_AIRINGS_FILTER_DATASET_3.pkl')
from nltk import pos_tag$ sentence = word_tokenize('I always lie down to tell a lie.')$ pos_tag(sentence)
MNB = MultinomialNB()$ model3 = MNB.fit(x_train, y_train)
popular_centers.plot()
input_node_types_DF = pd.read_csv(input_models_file, sep = ' ')$ input_node_types_DF
twitter_archive_full[twitter_archive_full.retweet_count == 77458][$     ['tweet_id', 'stage','retweet_count']]
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_autocorrelation(therapist_duration.diff()[1:], params=params, lags=30, alpha=0.05, \$     title='Weekly Therapist Hours First Difference Autocorrelation')
df3[['wk1', 'wk2', 'wk3']] = pd.get_dummies(df3['period'])$ df3.head()$
results2 = model_selection.cross_val_score(gnb, X_test, Y_test, cv=loocv)$ results2.mean()
from pandas import DataFrame$ df = DataFrame.from_csv("DonaldTrump_statusesfacebook.csv")$ df.head()
lm.predict(x_test)
autos["odometer"].shape #autos[column name] will always give 1-d array which is a.k.a as Series in pandas, just like 1-d array in Numpy is known as vector$
read_in = pd.read_pickle(processing_test.data())$ read_in
iris.head().iloc[:,[0]]
tweet_clean.rename({'id': 'tweet_id'}, axis=1, inplace=True)
fp7_proj.shape, fp7_part.shape, fp7.shape
date.month
test = vec1.fit_transform(df.message[1]) #takes 2. row in df for testing$ for i in test:$     print(i)$
measure_df.to_sql(name='Measurements', con=engine, index=False)
import pandas as pd$ companies = pd.read_csv('Fortune-1000-Company-Twitter-Accounts.csv')
df_man = reports_sub.copy()$ df_man.head()
html_page = browser.html$ JPL_soup = bs(html_page, "lxml")
tweetdf['lga'].unique()
from sklearn.decomposition import LatentDirichletAllocation$ from sklearn.feature_extraction.text import CountVectorizer
df.head()
megmfurr_tweets = pandas.read_csv('@megmfurr_tweets.csv')$ megmfurr_tweets
ds_train = FileDataStream.read_csv(train_file, numeric_dtype=np.float32, sep='\t')$ print(repr(ds_train.Schema))$ print(ds_train.Schema)
pivoted_data.resample("Y").sum().plot(figsize=(10,10))
average_trading = statistics.mean([day[6] for day in data])$ print ('Average daily trading volume for 2017:', average_trading)
start = time.time()$ print(list(map(sum_prime, [300000, 600000, 900000])))$ print("Time taken = {0:.5f}".format(time.time() - start))
pd.merge(transactions,transactions,how='outer',on='UserID')
s5 = df_clean3[df_clean3['name'] == 'the'].sample()$ s5.text.tolist()
master_df_total=pd.concat([master_df,master_dummies],axis=1)
from IPython.core.display import display, HTML$ display(HTML("<style>#notebook-container { margin-left:-14px; width:calc(100% + 27px) !important; }</style>"))
my_df["user_create"] = df_user.groupby('nweek_create')['user_id'].nunique()$ my_df["user_active"] = df_user.groupby('nweek_active')['user_id'].nunique()
data2=data.set_index("date")$ data2.head()
data = pd.read_csv('dog_rates_tweets.csv', parse_dates=[1])
product_mat_grouped = paid_success_with_data.groupby(['MVGR2']).size().sort_values(ascending=False)$ product_mat_grouped = product_mat_grouped[product_mat_grouped > 1]$ product_mat_grouped.head()
flights = flights[-((flights['origin_airport'].str.contains('[0-9]')) &$                     (flights['destination_airport'].str.contains('[0-9]')))]$ flights.head()
combined_city_df = pd.merge(city_data_df, ride_data_df,$                                  how='outer', on='city')$ combined_city_df.head(5)
sql="SELECT * FROM %s.%s ORDER BY dict_field ASC" % (schema, dict_table)$ HTML(hc.sql(sql).toPandas().to_html())
df_ll.head(2)$ df_ll.isDuplicated.value_counts()$ df_ll.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
cityID = '27485069891a7938'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_York.append(tweet) 
df6 = df5.Completed_Date.groupby([df5.Completed_Date.dt.year.rename('Completed_Year'), df5.Completed_Date.dt.month.rename('Completed_Month')]).agg('count')$ print(df6)
df2[df2['user_id'].duplicated()]$
pres_df['location'].unique()
temp_df=df_small.groupby('order_num').sum().reset_index()$ temp_df.head()
cursor.execute('SELECT * FROM fib')$ print(cursor.fetchall())
tree_chunker = ConsecutiveNPChunker(train_trees)$ print(tree_chunker.evaluate(valid_trees))
OGLE_file = 'tl.txt'$ Dir_OGLE_file = '/Users/arturo/Documents/Research/LSST/OGLE/'$ OGLE_ra_dec_data = np.genfromtxt(Dir_OGLE_file+OGLE_file, usecols=[9,10])
sentiments_df = pd.DataFrame(sentiment_array)$ sentiments_df = sentiments_df[["TweetsAgo","Target","User","Date","Positive","Neutral","Negative","Compound","Text"]]$ sentiments_df.head()
authors = train.groupby('author').popular.agg(['count', 'mean'])$ authors.shape[0]
search_rating = np.vectorize(search_rating, otypes=[np.float])$ data['rating'] = search_rating(data['text'])$ data = data[pd.notnull(data['rating'])]
result = modelk.predict(X_test )$
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_zh_wzh_2.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_wzh_2.w2v')
adjust_cols = ['StateBottleCost','StateBottleRetail','SaleDollars']$ for col in adjust_cols:$     liquor[col] = pd.to_numeric(liquor[col].str.replace('$',''),errors='coerce')
!hdfs dfs -cat 32B_results-output/part-0000* > 32B_results-output.txt$ !head 32B_results-output.txt
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots$  $ temp_data["CLOSE"].iloc[:].plot() # Plot the adjusted closing price of AAPL
gbm.predict(test)
weather_df_byday = weather_df.loc[weather_df.index.weekday == weekday]$ weather_df_byday.info()$ weather_df_byday.head()
request_data_2017 = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31')
p_control_converted = df2[df2['group'] == 'control']['converted'].mean()$ print('The probability of an individual in the control group converting: ', p_control_converted)
df2 = df2.drop(2893)$ df2[df2.user_id.duplicated(keep = False)]
Base = automap_base()$ Base.prepare(engine, reflect=True)$ session = Session(engine)$
tweets = tweets[tweets['retweeted_status_timestamp'].isnull()]$ tweets.info()$
nnew = df2.query('landing_page == "new_page"').count()[0]$ print ("The population of Newpage is : {}".format(nnew))
stations = Base.classes.stations$ stations
quadratic = [[x ** 2, x, 1] for x in np.arange(0, len(y))] 
print(df[df.B>0],'\n')$ print(df[df>0],'\n')
df2=df.query('misaligned==False')
id_duplicated = df2[df2.duplicated(['user_id'])]['user_id'].values$ print('The user IDs of duplicated users are: ',id_duplicated)
np.std(p_diffs)
for c in ccc:$     for i in vwg[vwg.columns[vwg.columns.str.contains(c)==True]].columns:$         vwg[i] /= vwg[i].max()
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]
new_converted_rate = np.random.binomial(n_new, p_new, 10000)/n_new$ old_converted_rate = np.random.binomial(n_old, p_old, 10000)/n_old$ p_diffs = new_converted_rate - old_converted_rate$
cp311 = cp311.dropna(axis = 0).copy()
with open('180219_10slsqpDM.pkl', 'rb') as f:  # Python 3: open(..., 'rb')$     rez_2 = pickle.load(f)
user_df = stories.submitter_user.apply(pd.Series)$ user_df.head()
modCrimeData = crimeData$ modCrimeData.set_index("Year", inplace=True)$ modCrimeData.head()
PFalsePositive = (PPositiveFalse * PFalse) / PPositive$ "%.2f" % (PFalsePositive * 100) + '%'
train = pd.read_csv('./data/train.csv')
df['polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)$ df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
