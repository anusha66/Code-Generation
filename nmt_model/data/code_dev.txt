start_point_2 = [41.884260, -87.630344] # Traffic Court in Richard J. Daley center $ url_2 = ("{}isochrone?fromPlace={},{}&mode={}&date=2016-06-01&cutoffSec={}").format( $     base_url,start_point_2[0],start_point[1],mode,travel_time) $ iso_json_2 = json.loads(requests.get(url_2).text) $ iso_gdf_2 = gpd.GeoDataFrame.from_features(iso_json_2['features'])
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}] $ new_store = pd.DataFrame(new_items, index = ['store 3']) $ new_store $ store_items = store_items.append(new_store, sort = False) $ store_items
!rm microbiome.sqlite3
def cosine_similarity(u, v): $     return(np.dot(u, v)/np.sqrt((np.dot(u, u) * np.dot(v, v))))
MonthlyReturns = np.asarray([meanMonthlyReturns['BEML'],meanMonthlyReturns['Glaxo'], $                              meanMonthlyReturns['Infy'],meanMonthlyReturns['Unitech']]) $ MonthlyReturns
logit = sm.Logit(df_new['converted'],df_new[['intercept','US','CA']]) $ results = logit.fit() $ results.summary()
to_remove = ["Current", "In Grace Period", "Late (16-30 days)", "Late (31-120 days)"] $ loan_stats = loan_stats[loan_stats["loan_status"].isin(to_remove).logical_negation(), :]
autos["brand"].describe()
wed11 = wed11.drop('id', axis=1)
questions = questions.rename({ $     'Maybe - I want to see the lineup first':'NEXT_maybe_lineup', $     'Maybe - Other factors':'NEXT_maybe_other', $     'No - definitely not coming':'NEXT_no', $     'Yes - Coming no matter what':'NEXT_yes'}, axis=1) $
obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])
datatest.loc[datatest.place_name == "Sourigues",'lat'] = -34.800140 $ datatest.loc[datatest.place_name == "Sourigues",'lon'] = -58.220011
final_data.sort_values('created_at')
topicmax = list() $ for topic in doctopic: $     topicmax.append(argmax(topic))
df['y'].plot()
summary_all = df['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9]) $ summary_all
df.to_pickle("dfAllGPSTweetsFilterChile.p")
tweets['user_id'].value_counts()[:10]
zipscore_dict = {'ZIPCODE' : zip_codes, $                  'mean_scores' : mean_scores} $ zipscore_df = pd.DataFrame(zipscore_dict) $ zipscore_df.to_csv('zipcode_scores_map.csv', index=False)
archive_copy = pd.merge(left=archive_copy, right=tweet_data_copy, left_on='tweet_id', right_on='id', how='inner')
def combine_names(row): $     if row.contributor_fullname.startswith('SEAN PARKER'): $         return 'SEAN PARKER' $     return row.contributor_fullname
archive_df.name.value_counts()
rng = pd.date_range('3/6/2012 00:00', periods=15, freq='D') $ rng.tz is None, rng[0].tz is None
temp_df2 = Ralston.TMAX $ for i in range(10): $     temp_df2= temp_df2 * 2 $     print ("Iteration",i+1,":", temp_df2.mean(), temp_df2.std())
e_pos = df_elect[df_elect['Polarity'] >= 0] $ e_pos.count()
def user_tweet_time_dist(df, username): $     user_tweets = df[df['username'] == username] $     user_tweets_count = user_tweets.groupby('time')['time'].count() $     user_tweets_count.plot.bar(figsize=(15,4))
bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')
etsamples_100hz.iloc[0:1]
geo=Geolocator.Geolocator() $ geo.init(worldPickleFileName='../geo-world-spark-nepal.pkl',dataFileName='../geo-data/ungp-geo.txt.gz')
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK']]) $ results=logit_mod.fit() $ results.summary()
result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4) $ result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)') $ np.allclose(result1, result2)
complete_ratings_file = os.path.join(dataset, 'ml-20m', 'ratings.csv') $ complete_ratings_raw_data, complete_ratings_raw_data_header = read_file(complete_ratings_file) $ complete_ratings_data = remove_header(complete_ratings_raw_data, complete_ratings_raw_data_header, 3) $ print ("There are %s recommendations in the complete dataset" + str(complete_ratings_data.count()))
y_proba_argmax = tf.argmax(y_proba, axis=2, name="y_proba")
points_dic={"India":345,"Bangladesh":456,"Pakistan":789,"China":90} $ points=pd.Series(points_dic) $ points $
type(s.str.extract('([ab])(\d)', expand=False))
from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.cluster import KMeans $ from sklearn.metrics import silhouette_samples, silhouette_score
t = pd.read_csv('count.csv')
def GroupColFunc(df, ind, col): $     if df[col].loc[ind] > 0: $         return 'Group1' $     else: $         return 'Group2'
df2  = df $ df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
model.predict(validation_data[:5])
flood_reports = pd.read_csv(INPUT_PATH + '311_2015_flood_reports.csv', parse_dates=[1])
kickstarter["country"].unique()
print(datetime.datetime.now())
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 1)].groupby(['Group']).JobID.count().sort_values(ascending = False)
titanic.describe()
avg_temp_recorded = session.query(func.avg(Measurement.tobs)).filter(Measurement.station=='USC00519281').group_by(Measurement.station).all() $ avg_temp_recorded
treatmentdf=df2[df2.group=="treatment"] $ treatmentdf[treatmentdf.converted==1].count()[0]/treatmentdf.count()[0]
df.num_comments= df.num_comments.apply(lambda x:0 if x <= 74 else 1)
cols = ['id_partner', 'Order_14', 'Order_30', 'Order_60', 'Cost_14', 'Cost_30', 'Cost_60'] $ users_orders = users_orders[cols] $ users_orders.head()
df_prep0 = df_prep(df0) $ df_prep0_ = pd.DataFrame({'date':df_prep0.index, 'values':df_prep0.values}, index=pd.to_datetime(df_prep0.index))
a_day.days, a_day.resolution
actor = pd.read_sql_query('select * from actor where last_name like "%%GEN%%"', engine) $ actor.head()
active_count = len(clean_users[clean_users['active']==1]) $ active_mean = clean_users[clean_users['active']==1]['account_life'].dt.days.mean() $ active_sd = clean_users[clean_users['active']==1]['account_life'].dt.days.std()
df_uv = df.query('landing_page != "new_page"') $ df_vu = df_uv.query('group == "treatment"') $ df_vu.count() $
pd.crosstab(index=data["Survived"],columns=data["Sex"])
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head(10)
df = df.drop('vegetables', axis=1) $ df
autos["price"].value_counts().sort_index(ascending=False).head(20)
filt_zip_loc=~np.logical_and(building_pa_prc_shrink[['zipcode']].isna().values ,building_pa_prc_shrink[['location']].isna().values)
data_config = dict(path='tests/data/nhtsa_as_xml.zip', $                    databasetype="zipxml", # define that the file is a zip f $                    echo=False)
df.head()
list(tweet_archive_clean.columns.values)
fulldf.to_csv('FullResults.csv', encoding='utf-8', index=False)
(temp_df.emailHash != '44d0dc437936b13f7cea2f77053806bd').sum()
html_table = df.to_html() $ html_table
pd.isnull(train_df).sum() $ pd.isnull(test_df).sum() 
import statsmodels.api as sm $ convert_old = df2[(df2["landing_page"] == "old_page") & (df2["converted"] == 1)]["user_id"].count() $ convert_new = df2[(df2["landing_page"] == "new_page") & (df2["converted"] == 1)]["user_id"].count() $ n_old = df2[df2['landing_page']== 'old_page'].shape[0] $ n_new = df2[df2['landing_page']== 'new_page'].shape[0]
df_combined[['CA','UK','US']] = pd.get_dummies(df_combined['country'])
reqs.index.set_names('num', level=0, inplace=True) $ reqs.head()
bacteria_data.columns
resultvalue_df = pd.DataFrame.from_records(resultvalue_records)
people.sort_index(ascending=False)
if committee_df.iloc[0]['party_full'] is not None: $     print(committee_df.iloc[0]['party_full'] ) $ else: $     print('yes')
train_df[['comments', 'favs', 'views', 'views_lognorm', 'comments_lognorm', 'favs_lognorm']].corr()
old_page = df2.query('landing_page == "old_page"') $ old_page.landing_page.count()
data = pd.DataFrame.from_dict(raw_data) $ a = pd.DataFrame(raw_data['data']) $ with open('data/raw_data.json', 'w') as fp: $     json.dump(raw_data['data'], fp)
print 'Use loc like any other Dataframe' $ print 'A single index value:' $ msft.loc['2012-01-03']
print str(len(df.district_id[(df.district_size == 0) & (df.accounts_provisioned <> 0)].unique())) + ' districts w/ no size but w accounts' $ print str(len(df.district_id[(df.district_size == 0) & (df.accounts_provisioned == 0)].unique())) + ' districts w/ neither'
mainstream_facts_metrics.to_csv('DATA/mainstream_content.csv.gz', compression='gzip')
results = soup.find_all('li', class_="content_title")
a.zfill(8)
to_be_predicted_Day5 = 21.38582566 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
rf.predict(clim_test)[:10]
def get_twitter_api(consumer_key,consumer_secret, access_token, access_token_secret): $     auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $     auth.set_access_token(access_token, access_token_secret) $     return tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)
coming_next_reason.columns = ['NEXT_'+str(col) for col in coming_next_reason.columns]
retention_10.columns = [f'Week {i}' for i in retention_10.columns]
df_new['intercept'] = 1 $ df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country'])
logging.debug('End of program')
df1['forcast'].plot() $ plt.xlabel('Date') $ plt.ylabel('Price')
area.index | population.index
vol = volume.between_time('10:00', '16:00') $ vol.head(20)
from scipy.stats import f_oneway
ridge2 = linear_model.Ridge(alpha=317) $ ridge2.fit(X_17, y2) $ (ridge2.coef_, ridge2.intercept_)
from sklearn.ensemble import RandomForestClassifier $ algo = make_pipeline(preprocessing.MinMaxScaler(), RandomForestClassifier(max_features=3, n_estimators=1000)) $ scores = cross_val_score(algo, X_train, y_train, cv=5, scoring='f1') $ print(scores.mean()) $ algo.fit(X_train,y_train)
e_date = dataset['User_Created_At'].min() $ l_date = dataset['User_Created_At'].max() $ print('Earliest account creation date: ', e_date) $ print('Latest account creation date: ', l_date)
df['budget']=df['budget'].replace(0,df['budget'].mean()) $ df['revenue']=df['revenue'].replace(0,df['revenue'].mean()) $ df['budget_adj']=df['budget_adj'].replace(0,df['budget_adj'].mean()) $ df['revenue_adj']=df['revenue_adj'].replace(0,df['revenue_adj'].mean())
stfvect.get_feature_names()
quarterly_revenue.plot(kind="line") $ plt.show()
data[['Close', 'Volatility', 'ATR']].plot(subplots=True, color='blue',figsize=(8, 6))
old_page_converted = np.random.choice([0, 1], size = n_old, p = [1 - p_old, p_old]) $ p_old_sim = old_page_converted.mean()
pnew = df2['converted'].mean() $ print(round(pnew,4)) $
n_new =df2[df2['group'] == "treatment"].shape[0] $ print(n_new)
mallet_path = 'Data/mallet-2.0.8/bin/mallet' $ ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=7, id2word=dictionary)
flight2.dtypes
df_protest.columns[df_protest.dtypes=='object'].tolist()
%%timeit $ for i in range(10000): $     if pattern.search(the_str): $         b = the_str.replace('AA', 'BB')
import pandas as pd $ import numpy as np $ from sklearn.neighbors import KNeighborsRegressor
df['Symbol'] = finalSymbolsList.values
df_regression = df2.copy()
submission_full[['proba']].median()
df_transactions['membership_duration'] = df_transactions['membership_duration'].astype(int)
df_total.to_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_Urology\01_Urology_EMR_cleaned_with_dummies.csv', index=False)
ds_temp_casts_CTD_1988['Project'] = ds_temp_casts_CTD_1988.Project.astype(str) $ ds_temp_casts_CTD_1988['Platform'] = ds_temp_casts_CTD_1988.Platform.astype(str) $ ds_temp_casts_CTD_1988['Institute'] = ds_temp_casts_CTD_1988.Institute.astype(str)
from sklearn.ensemble import RandomForestClassifier $ rf = RandomForestClassifier(n_estimators = 10, random_state = 42) $ rf.fit(X_train,y_train)
common_brands = unique_brands[unique_brands >= 0.05].index $ common_brands $
all_df = pd.read_pickle('../dataset/twitter_data_11949.pkl')
Addressing Class Imbalance $ We have only 16% minority class representation in this reduced dataset. Let's see whether changing this balance results in better model performance.
station_availability_df['tot_docks'].plot(kind='hist', rot=70, logy=True) $ plt.show() $ station_availability_df = station_availability_df[station_availability_df.tot_docks != 2727] $ station_availability_df['tot_docks'].plot(kind='hist', rot=70, logy=True) $ plt.show()
errors['datetime'] = pd.to_datetime(errors['datetime'], format="%Y-%m-%d %H:%M:%S") #string to date time format.V:Wasn't datetime already in the ymd-hms format?? $ errors.count()
df.show(1)
results.summary()
nitrogen['CharacteristicName'].unique()
df.groupby('episode_id')['id'].nunique().max()
day_ahead_price_df = pd.read_excel(day_ahead_folder + '/new-DA-price.xlsx')
X2.shape $ X2 = X2.values.reshape([785024,3]) $ print(X2.shape)
train.loc['p081434']
autos.brand.value_counts().head(20)
fit1_test = fh_1.transform(test.device_model) $ fit2_test = fh_2.transform(test.device_id) $ fit3_test = fh_3.transform(test.device_ip)
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_713.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_713, lags=40, ax=ax2)
result['timestampUTC'] = pd.to_datetime(result['timestampCorrected'], unit='ms')
explotions['Deaths'] = explotions['Deaths'].map(lambda x:get_num(x))
dups = df2.user_id.value_counts() $ dups[dups > 1] $
converted = df[df['converted']==1].count() $ proportion_converted = (((converted/number_of_rows)*100) ) $ print('The proportion of users converted is {}%'.format(round(proportion_converted['converted'],3)))
archive_df_clean = archive_df.copy() $ archive_df_clean = archive_df_clean[archive_df_clean['retweeted_status_id'].isnull()]
day.apply(pd.Timestamp('2014-01-01 09:00'))
df2 = df.drop(['wait_estimate', 'ta_id', 'id', 'semester'], axis=1) $ df2 = df2.dropna(axis=0, how='any') $
new_page_converted = np.random.choice([1,0], size=n_new_page, p=[p_mean, (1-p_mean)]) $ print(new_page_converted.mean())
df['Created_at'] = pd.to_datetime(df['Created_at'], errors='coerce') $ df.to_sql(name='Tweet_sports', con=conn, if_exists='replace')
question_3_dataframe = question_3_dataframe.merge(population_by_zip, how='left', on=['incident_zip']) $ question_3_dataframe.head(5)
stadium_arr.groupby('away_team')['arrests'].sum().sort_values(ascending=False)[:10].plot(kind='bar', title='Overall Number of Arrests when Away (top 10)') $ plt.savefig('overallArrestsAtAway.png', bbox_inches='tight')
sfs1.k_score_
exploration_titanic.count_unique()
df_final = df_complete_b.drop(bad_replies.index) $ df_final.reset_index(drop=True, inplace=True) $ df_final.head()
target = 'project_is_approved' $ sum(data.project_is_approved==1)/data.shape[0]
nbar_clean = xr.concat(sensor_clean.values(), 'time') $ nbar_clean = nbar_clean.sortby('time') $ nbar_clean.attrs['crs'] = crs $ nbar_clean.attrs['affin|e'] = affine
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
vocab_size = max(proc.id2token.keys()) + 1 $ max_length = proc.padding_maxlen $ print('vocab size: ', vocab_size) $ print('max length allowed for documents: ', max_length)
prcp_1_df.head() # Display the top 5 records of the dataframe
Grouping_Year_DRG_discharges_payments.head()
seq=[] $ for i in range(0,l2): $     if t_open_resol[i]['lifetime']==0: $         seq.append(i)
new_page_converted.mean() - old_page_converted.mean()
page.status
import datetime $ import matplotlib.pyplot as plt $ import matplotlib.dates as dates $ %matplotlib inline
df.product_type.value_counts(normalize=True)
df_new['country'].unique()
print(train.head()) $ print(test.head())
dfb_train.describe()
bands.sum(axis=0) $
count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english') $ vects = count_vectorizer.fit_transform(data[' Tweet Text']) $ vocabulary = count_vectorizer.get_feature_names() $ print('Vocabulary size: '+str(len(vocabulary)))
dataurl = '/Users/dbricare/Documents/Python/datasets/expedia/' $ dfXy = pd.read_csv(dataurl+'train200th.csv', sep=',', encoding='utf-8') $ print(dfXy.shape) $ print(list(dfXy.columns))
TotalNameEvents = TotalNameEvents.rename(index=str, columns={"dmax": "created_time", "events_Name": "events_name","politician_Name":"politician_name"}) $ TotalNameEvents["diff"] =  TotalNameEvents["diff"].astype('int', copy=False)+1
from sklearn.cross_validation import cross_val_score $ scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10) $ print scores $ print scores.mean()
pd.options.display.max_colwidth = 200 $ data_df[['ticket_id','type','clean_desc','nwords']].head(30)
from sklearn.cross_validation import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) $ model2 = LogisticRegression() $ model2.fit(X_train, y_train) $ model.score(X_train, y_train) $
dta.query("(risk == 'Risk 1 (High)') | (risk == 'Risk 1 (Medium)')").head()
df.set_index(['operator', 'part', 'range'], inplace=True)
from nltk.sentiment.vader import SentimentIntensityAnalyzer
df_campaigns['Open Rate'] = df_campaigns['Open Rate'].apply(lambda x: x[:-1]).astype('float') $ df_campaigns['Click Rate'] = df_campaigns['Click Rate'].apply(lambda x: x[:-1]).astype('float')
series1= df[(df.Series == 'N   1')]['value'] $ series1.index=date $ series1= series1.dropna(how='all') $ df_1= series1 $
df_goog.Close.resample('A').std().plot()
dataset.describe()
documents = jobPostDFSample.JobDescription
prcp_df.dtypes
def stockchart(symbol): $     key = 'PD3O8KG4280HX51V' $     ts = TimeSeries(key=key, output_format='pandas') $     data, meta_data = ts.get_intraday(symbol=symbol,interval='15min', outputsize='full') $     return data
tset.head()
r.content
test = datatest[datatest['covered/total'].isnull()] $ train = datatest[datatest['covered/total'].notnull()]
s_filled.plot() $ plt.show()
from scipy.sparse import hstack $ x_train = hstack((train_words, fb_train[features])) $ x_test = hstack((test_words, fb_test[features]))
dac = np.vstack(df_all.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) $ df_all['dac_year'] = dac[:,0] $ df_all['dac_month'] = dac[:,1] $ df_all['dac_day'] = dac[:,2] $ df_all = df_all.drop(['date_account_created'], axis=1)
read_in = pd.read_pickle(processing_test.data()) $ read_in
tiguan_merge = pd.concat([tiguan, tiguan2]) $ tiguan_merge = tiguan_merge.reset_index(drop=True) $ grby = tiguan_merge.groupby(list(tiguan_merge.columns)) $ unqidx = [x[0] for x in grby.groups.values() if len(x) == 1] $ tiguan_merge.reindex(unqidx)
data.to_csv("/users/danielcorcoran/desktop/github_repos/python_nb_data_pulling/quotes_data.csv", index_label= "row_index")
from datetime import timedelta $ start_time = datetime.now() $ scrape_time = timedelta(seconds=5) $ print 'START:', start_time, 'SCRAPE:', scrape_time, 'START + SCRAPE:', start_time + scrape_time
rejected = train[train['project_is_approved'] == 0] $ for i in check_cols: $     print(rejected[i].value_counts()) $     print('')
archive_clean = archive_clean.loc[~archive_clean['tweet_id'].isin(remove_list),:]
new_page_converted = np.random.choice([1,0], size=sample_size_new_page, p=[p_new_null,1-p_new_null])
manager = ImageManager('../tree_photos/', '../data/image_log_20180205.csv', '../data/image_syncs_20180205.csv')
dfTemp=transactions.merge(users, how='inner',left_on='UserID',right_on='UserID') $ dfTemp
df_breed.head(3)
countries = pd.read_csv('./airbnb firstdestinations/countries.csv')
pd.value_counts(no_specialty['ReasonForVisitName'])
end_time = (time.perf_counter() - start_time)/60 $ print("This notebook took {:0.2f} minutes to run".format(end_time))
intersections_irr['isWeekend'] = [ 0 if  datetime.datetime.strptime( dateStr,'%Y-%m-%d %H:%M:%S').weekday()<5 else 1 for dateStr in intersections_irr['updateTimeStamp'] ]
df = pd.concat(pd.read_csv(fname, index_col=0) for fname in csv_paths)
new_page_converted.mean()/n_new  - old_page_converted.mean()/n_old
ALL = BAL.append([CHI, CIN, HOU, NYG, PHI, PIT, SEA]) # Merges the eight data sets into one big sample $ ALL.head() # Just checking the column headers
a.sum(0)
mean_mileage = {} $ for brand in top_5_percent.index: $     mileage_mean = autos_pr[autos_pr['brand'] == brand]['odometer_km'].mean() $     mean_mileage[brand] = mileage_mean $ print(mean_mileage)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
daily_transaction = daily_transaction.select(col("date").alias("date"), col("avg(trans_amt)").alias("daily_rev_avg"), $                                      col("count(uuid)").alias("count_trans")) $ daily_transaction.show()
y.mean()
tweet_archive_clean = tweet_archive_clean.sort_values(by='dog_stage').drop_duplicates(subset='tweet_id', keep='last')
intake.info()
p_old = df2[df2['converted']==1]['user_id'].count() / df2.shape[0] $ p_old
twitter_archive_clean[twitter_archive_clean['retweeted_status_timestamp'].isnull()==False].shape[0]
pd.isnull(df)
data_file = pandas.ExcelFile(file_path)
sns.countplot(x="y",data=X,palette='hls') $ plt.show()
twitter_df_clean['retweet_count'] = pd.to_numeric(twitter_df_clean.retweet_count) $ twitter_df_clean['favorite_count'] = pd.to_numeric(twitter_df_clean.favorite_count)
temps_mosact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == most_activity[0], Measurements.date > year_ago).all()
gis = arcgis.gis.GIS(os.environ['ESRIFEDERAL_URL'], username="Anieto_esrifederal")
index_strong_outliers = (strong_outliers_fare.is_outlier == 1)
d8 = d7.unstack() $ d8
dump["country"].value_counts().sum() # Calculates sum of users in every country listed in the Series
df_all_backup = df_all.copy()
m.load('val0')
df_concensus_uaa['latest_consensus_created_date'] = pd.to_datetime(df_concensus_uaa['latest_consensus_created_date'], dayfirst=True)
daily_cases.unstack().T.head()
res = sts.query(qry)
dfs_morning.sort_values(by='ENTRIES_MORNING', ascending = False).head(50) $ threshold = 100000 $ dfs_morning.loc[dfs_morning['ENTRIES_MORNING']>threshold, 'ENTRIES_MORNING'] = dfs_morning.ENTRIES_MORNING.quantile(.5)
svc = SVC() $ svc.fit(X_train, Y_train) $ Y_pred = svc.predict(X_test) $ acc_svc = round(svc.score(X_test, Y_test) * 100, 2) $ acc_svc
dtm = vectorizer.fit_transform(df['lemma'])
keys_0604.shape
X_train.head()
columns = orgs.columns
dfs_morning = (dfs_morning.sort_values(by=['STATION', 'DATE_TIME']) $                .groupby(['STATION', 'DATE_TIME'])['ENTRIES'] $                .sum() $                .reset_index()) $ dfs_morning['DATE'] = pd.to_datetime(dfs_morning['DATE_TIME']).apply(lambda x:x.date())
sites_on_net.head()
df.info()
autos.loc[autos["seller"] == 'gewerblich']
hi_conf_no_gain = x.loc[(x['pred'] + x['pred_std']) < 0.5] $ hi_conf_no_gain
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
yxe_tweets['source'].value_counts()
QUIDS_wide.dropna(subset =["y"], axis =0, inplace=True)
test_df.head(1000)
np.sqrt(mean_squared_error(y, predictions)) #216.21142249869081 $ model.score(X,y) # R^2 0.68136076061510931 $ lm.intercept_ #-1.0834100912850033 $ lm.coef_ # array([ 13.15693634]) $
mlp_df = pd.read_csv(mlp_fp, index_col='Date', parse_dates=True) $ mlp_df.head()
flights2.passengers.plot()
from alpha_vantage.timeseries import TimeSeries $ ts = TimeSeries(key='824R4DDDH2LAKPEL')
feat = ['categoryname', 'eventname', 'location'] $ for f in feat: $     PYR[f] = PYR[f].apply(clean_dataset)
df.columns
x = [] $ with open("data1.csv") as f: $     for line in f: $         a = line.split(',') $         x.append(a)
tweet_archive_clean = pd.melt(tweet_archive_clean, id_vars=columns_to_keep, $                               var_name='stages', value_name='dog_stage')
plt.show()
Base.classes.keys() $
conn_b.commit()
dfs['Seasonal Difference'] = dfs['Close'] - dfs['Close'].shift(9) $ dfs['Seasonal Difference'].plot()
ssc.start()
for team in sorted(elo_dict, key=elo_dict.get)[::-1]: $     print(team, elo_dict[team])
FREEVIEW.plot_main_sequence(raw_freeview_df)
hp2 = pd.Series(np.random.uniform(-0.05, 0.15, 36), index=pd.date_range('1/1/2011', periods=36, freq='MS')) $ hp2.tail()
BTC = pd.concat([btc_wallet,gdax_trans_btc])
matches = pd.read_sql_query('select * from Match', conn)  # specify the connection $ print(matches.shape) $ matches.head()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=JVTZ9kPgnsnq9oFbym2s&start_date=2018-05-17' $ r = requests.get(url) $ print(r.text)
latest_timelog.to_csv('data/toggl-timelog-latest.csv')
nba_df.size
X_train, X_test, y_train, y_test = train_test_split(X, y, $                                                     train_size=0.75, test_size=0.25, random_state=1) $
df2['intercept'] = 1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop(['control',],axis=1) $
_ = notus['country'].value_counts(dropna=False).index.tolist()[1:45] $ _
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=32000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
from repository.mlrepositoryclient import MLRepositoryClient $ from repository.mlrepositoryartifact import MLRepositoryArtifact
df2[df2.user_id.duplicated(keep = "first")]
plt.hist(p_diffs); $ plt.xlabel('Value of Size Difference (p_diffs)'); $ plt.ylabel('Frequency'); $ plt.title('Histogram Plot of p_diffs');
print(kmeans.labels_)  $
pd.set_option('display.max_columns',53) $ df
t = splits[0].examples[0]
autos['last_seen'].str[:10].describe()
df_clean3.sample(5)
df = pd.read_csv(reviews_file_name, encoding = "ISO-8859-1") $ df.head(n = 10)
a_col = df['A'] $ df.sub(a_col, axis=0)
len(active_psc_statements[active_psc_statements.statement.str.contains('no-individual-or-entity-with-signficant-control')].company_number.unique())
print(r.json()) $
tn, fp, fn, tp = mx.ravel() $ sensitivity = 100*tp/(tp+fn) $ specificity=100*tn/(tn+fp) $ print('sensitivity:', sensitivity) $ print('specificity:', specificity)
store=join_df(store,store_states,"Store") $ weather=join_df(weather,state_names,'file','StateName') $ sum(store['State'].isnull()),sum(weather['State'].isnull())
messages=df.status_message.tolist() $ messages[:5]
comms_data = pd.DataFrame(comms_dict) $ comms_data $ timestamps = comms_data["created"].apply(get_date) $ comms_data = comms_data.assign(timestamp = timestamps)
daily_price_mat = price_mat $ daily_mcap_mat = mcap_mat $ month_price_mat = daily_price_mat.resample('M', convention='start').asfreq() $ month_mcap_mat = daily_mcap_mat.resample('M', convention='start').asfreq()
consumption_df.plot(figsize=(16,8)) $ plt.title('Time serie of the consumption') $ plt.show()
db.aio_input( $  "'/tmp/samples'", $  "'num_attributes={}'".format(num_attributes), $  "'split_on_dimension=1'" $ )[:].head(15)
rfc_bet_over  = [x[1] > .54 for x in pred_probas_rfc_over]
reader.monitor.columns
csvData['street'] = csvData['street'].str.replace(' Southeast', ' SE') $ csvData['street'] = csvData['street'].str.replace(' South', ' S')
url = 'http://mobilizationjournal.org/toc/maiq/22/2' $ html= requests.get(url).text $ save_file(html, 'moby_22_2.html') $
autos = autos[autos["registration_year"].between(1900,2017)] $ autos["registration_year"].value_counts(normalize=True).head(10)
to_be_predicted_Day3 = 22.39050512 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
print noloc_df.state.unique() $ print noloc_df.city.unique() $ print noloc_df.zipcode.unique()
print(grid.best_score_) $ print(grid.best_params_)
print(pd.isnull(df).any(axis = 1).sum()) $ df.info() #there are no missing values
df2[df2.duplicated('user_id')]
pred_labels = rdg.predict(test_data) $ print("Training set score: {:.2f}".format(rdg.score(train_data, train_labels))) $ print("Test set score: {:.2f}".format(rdg.score(test_data, test_labels))) $
import json $ server_response = urlopen(search_request_url) $ data = json.loads(server_response.read().decode('utf-8')) $ server_response.close()
df_predictions_clean.p1_conf = df_predictions_clean.p1_conf*100 $ df_predictions_clean.p2_conf = df_predictions_clean.p2_conf*100 $ df_predictions_clean.p3_conf = df_predictions_clean.p3_conf*100
from collections import Counter $ def Most_Common(lst): $     data = Counter(lst) $     return data.most_common(1)[0][0]
time_window_tops = time_window[time_window["screen_name"].isin(["MyFancyOne","Jeanne_vanced","browngravy_93","AmericanMom2","toksikshok","ResistOpression","UTHornsRawk","RandieK"])] $ myplot_parts = [go.Scatter(x=time_window_tops["stamp"],y=time_window_tops["screen_name"],mode="markers")] $ mylayout = go.Layout(autosize=False, width=1000,height=500) $ myfigure = go.Figure(data = myplot_parts, layout = mylayout) $ iplot(myfigure,filename="crisis")
import statsmodels.api as sm $ df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error, $            name="L")
df.drop(['ARR_DELAY'],axis=1,inplace=True)
print '%d distinct users before and %d distinct users after (%.2f%%)' % \ $ (pd.unique(beforeUsers).shape[0],\ $  pd.unique(fullDf[fullDf.index>pd.datetime(2015,4,25)].user).shape[0],\ $  float(pd.unique(beforeUsers).shape[0])/pd.unique(fullDf[fullDf.index>pd.datetime(2015,4,25)].user).shape[0])
loans_df = loans_df.query('loan_status == "Fully Paid" | loan_status == "Charged Off" | loan_status == "Default"')
df_users_4.head()
print(dfUD_unique.shape) $ print(df_all.shape) $ dfOld = pd.merge(df_all,dfUD_unique,how='right',left_index=True,right_index=True,sort=False) $ print(dfOld.shape) $ dfOld.head()
prop.info()
localFile = open("loan_test.csv", "wb") $ localFile.write(rawdata) $ localFile.close()
to_be_predicted_Day3 = 52.49437626 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
dfleft=pd.DataFrame({'key1':['K0','K1','K2','K3'],'key2':['K0','K1','K2','K3'],'A':['A0','A1','A2','A3'],'B':['B0','B1','B2','B3']}) $ dfright=pd.DataFrame({'key1':['K0','K1','K4','K3'],'key2':['K0','K1','K2','K3'],'C':['C0','C1','C2','C3'],'D':['D0','D1','D2','D3']}) $ print(dfleft) $ print(dfright)
ea.head()
airlines_sim = [d for d in airlines if len(d.tweets) == 2] $ len(airlines_sim)
test_df[["id", "labels"]].to_csv("submission_8.25.csv", index=False)
filename = 'people.csv' $ DF.to_csv(filename, index=False, encoding='utf-8')
datetime.datetime.fromtimestamp(1433213314.0)
sns.barplot(data=df.groupby('purpose').agg({'applicant_id':lambda x:len(set(x))}).reset_index(), $             x='purpose',y='applicant_id')
df.drop(df[(df.state == 'YY') & (df.amount >= 45000)].index, inplace=True)
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_total_in_m2']])
print(df['date'].min().month) $ print(df['date'].min().year) $ print(df['date'].min().day)
f_ip_app_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_app_hour_clicks"), header=True) $ print('Found %d observations.' %f_ip_app_hour_clicks.count())
 sum(edad.isnull())
sensors_num_df.plot('sensor.yweather_wind_speed')
test_df = pd.read_json('../data/test.json')
dup_labels = pd.Index(['foo', 'foo', 'bar', 'bar'])
freq_station = {'id':"",'name':""} $ freq_station['id'] = active_station_df.iloc[:1]['station'][0] $ freq_station['name'] = active_station_df.iloc[:1]['name'][0]
btc_price_df.index = btc_price_df.index.map(lambda x: x.replace(second=0)) $ btc_forum_df.index = btc_forum_df.index.map(lambda x: x.replace(second=0))
sns.set_style("whitegrid") $ fig, ax = pyplot.subplots(figsize=(10,10)) $ sns.boxplot(x='Species', y='PetalWidthCm', data=iris, ax=ax) $ sns.despine(offset=10, trim=True)
f_ip_app_hour_clicks.show(1)
sensor.description
df_nott = df.query('landing_page == "old_page"') $ df_4 = df_nott.query('group != "control"') $ df_4.nunique() $
"unique user-sessions in click data: ", len(df_click.user_session.unique())
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=False)
data[['cust_id','year','total_spend']].groupby(['cust_id','year']).agg('mean').reset_index()
print(type(some_rdd),type(some_df)) $ print('some_df =',some_df.collect()) $ print('some_rdd=',some_rdd.collect())
y_pred = pipe_lr.predict(pulledTweets_df.emoji_enc_text) $ y_proba = pipe_lr.predict_proba(pulledTweets_df.emoji_enc_text) $ pulledTweets_df['sentiment_predicted_lr']=[classes[y_pred[i]] for i in range(len(y_pred))]
logit_mod_2 = sm.Logit(merged['converted'], merged[['intercept', 'UK', 'CA']]) $ results_2 = logit_mod_2.fit() $ results_2.summary()
df.describe()
def expand_counts(source_counts): $     return np.repeat(np.arange(len(source_counts)), source_counts)
pca_test = sklearn_pca.fit_transform(dftouse[continuous_test_features][~mask]) $ testf1 = sklearn_transf[:,0] $ testf2 = sklearn_transf[:,1] $ testf3 = sklearn_transf[:,2]
tokens.sort_values('five_star_ratio', ascending=True).head(10)
raw_data = pd.read_csv('data/aligned_cmd.csv', index_col='DATES', usecols=['DATES', asset1, asset2]) $ df = raw_data.loc[bt_start_date:bt_end_date] $ bt_data = raw_data.loc[bt_end_date:] $ df.plot()
df_new.tail()
Trump_week_total = Trump_week.groupby(by=["year","week"]).sum() $ Trump_week_total = Trump_week_total.reset_index() $ Trump_week_total.shape
iplot(monthly_mean.iplot(asFigure = True, vline=['2017/09', '2017/03', '2016/09', '2016/03', '2015/09', '2014/12', '2014/04', '2013/10'], dimensions=(750, 500)))
df2['intercept'] = 1 $ df2['ab_page'] = df2.group.map({'treatment':1,'control':0})
overallGarageCars = pd.get_dummies(dfFull.GarageCars)
plt.plot(X,data, "r") $ plt.legend(loc='upper left') $ plt.show()
all_sets["releaseDate"] = pd.to_datetime(all_sets["releaseDate"])
portal_json_1 = "bigcz-cuahsi-water-slcretbuttecrk-2017-11-15.json"
twitter_archive_master['Stage'] = 'Unknown' $ twitter_archive_master.loc[(twitter_archive_master.doggo == 'doggo'),'Stage']='doggo' $ twitter_archive_master.loc[(twitter_archive_master.floofer == 'floofer'),'Stage']='floofer' $ twitter_archive_master.loc[(twitter_archive_master.pupper == 'pupper'),'Stage']='pupper' $ twitter_archive_master.loc[(twitter_archive_master.puppo == 'puppo'),'Stage']='puppo'
from sqlalchemy import inspect $ inspector = inspect(engine) $ print(inspector.get_columns('Measurement')) $ print(inspector.get_columns('Station'))
highly_retweeted=tizibika[tizibika['retweets']==tizibika['retweets'].max()]
print('acc =',hist.history['acc'][-1]) $ print('val_acc =',hist.history['val_acc'][-1])
g18 = g_all.loc[0:259,'1800':'1899'] $ g1800s = pd.concat([country, g18], axis = 1) $ print(g1800s.shape) $ print(g1800s.head())
trump = api.user_timeline(id='realDonaldTrump') # last 20 tweets
df['sent_label'] = 'neutral' $ df.loc[df['sentiment'] > 0.1, 'sent_label'] = 'positive' $ df.loc[df['sentiment'] < -0.1, 'sent_label'] = 'negative' $ df.head(3) $
users_ben = ben_final.dropna(axis=0) $ users_ben.apply(lambda x: sum(x.isna()))
df_grp = df.groupby('group') $ df_grp.describe()
ekos.workspaces
pd.Series(data=predicted5).hist()
p2_table = profits_table.groupby(['Country']).Profit.sum().reset_index() $ p2_result = p2_table.sort_values('Profit', ascending=False) $ p2_result.head()
dft.head()
shifted_forward = msftAC.shift(1) $ shifted_forward
tweet_archive_enhanced_clean['rating_denominator'] = tweet_archive_enhanced_clean['text'].str.extract('([0-9]+[.]*[0-9]*)\/(10)',expand = True)[1].astype('float') $
autos["odometer_km"].describe()
def EMA(x, alpha=0.8): $     em = x.rolling() $     return em $ df1.apply(EMA) $
X.isnull().sum()
conn.upload('/u/username/data/iris.csv')
data.columns
data = ['peter', 'Paul', 'MARY', 'gUIDO'] $ for s in data: $     print(s.capitalize())
result = json_normalize(engagement) $ result.head()
df1= df_inventory_santaclara[df_inventory_santaclara['Month']=='01'] $ df1 $ df2= df1.shift(-1) $ df2
raw_data.describe()
random.seed(1234) $ na_index = random.sample(range(X_mice.shape[0]), int(X_mice.shape[0]*0.3))
df_vow['Open'].unique()
new_page_conversions = np.random.binomial(n_new,p_new,10000) $ old_page_conversions = np.random.binomial(n_old,p_old,10000) $ p_diffs = [(x/n_new) - (y/n_old) for x, y in zip(new_page_conversions, old_page_conversions)]
old_page_converted = np.random.binomial(n = 1,p = pold, size =n_old ) $ old_page_converted
Image("/Users/jamespearce/repos/dl/data/dogscats/train/cat.3822.jpg")
plt.figure(figsize=(15,5)) $ sns.countplot(auto_new.CarYear)
week16 = week15.rename(columns={112:'112'}) $ stocks = stocks.rename(columns={'Week 15':'Week 16','105':'112'}) $ week16 = pd.merge(stocks,week16,on=['112','Tickers']) $ week16.drop_duplicates(subset='Link',inplace=True)
df=df.sort_values(['store_nbr', 'date'])
ebola_melt = pd.melt(ebola, $     id_vars=['Date', 'Day'], $     var_name='type_country', $     value_name='counts') $ ebola_melt.head()
multiIndex=pd.MultiIndex.from_arrays(myarray,names=['Number','Colour']) $ print(multiIndex)
autos['price'] = autos['price'].str.replace('$', '').str.replace(',', '') $ autos['price'] = autos['price'].astype(int) $ autos['price'].head()
now.year, now.month, now.day
country = pd.get_dummies(auto_new.Country) $ country.head()
linkpp = gpd.sjoin(pumashp,linkNYC).groupby('puma').count()[['date_link_']] $ linkpp.head()
lr = 5e-4 $ learn.fit(lr, 20, cycle_len=1, use_clr=(10,10))
from scipy import stats $ stats.chisqprob = lambda chisq, df2: stats.chi2.sf(chisq, df2) $ results.summary()
chefdf = pd.merge(chefdf, chef12df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_12'))
ts.head()
apple.groupby(pd.TimeGrouper(freq = 'M')).agg(np.mean).shape[0]
add_quarter(epa, year='YEAR', month='MONTH') $ epa.head()
discounts_table.Product.unique()
knn = KNeighborsClassifier(n_neighbors = 100) $ knn.fit(x_train,y_train > 0)
pickle.dump((fraud_data_updated),open('preprocess2.p', 'wb')) $
regr.alpha_
from sklearn.preprocessing import MinMaxScaler $ from sklearn.model_selection import train_test_split
tickerstart = time.time() $ mydata = quandl.get(companies, start_date="2012-01-01", end_date="2018-06-17") $ tickerend = time.time() $ print(tickerend-tickerstart)
log_m = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_m.fit()
print (series_of_converted_ages.mean())/365
df_CV_test = pd.DataFrame(X_test) $ df_CV_test['y'] = y_test $ df_CV_test.columns = variable_name $ df_CV_test.head(10) # Test set without tf-idf
df2[df2.group=='treatment']['converted'].mean()
twitter_archive.loc[(twitter_archive['name'].str.islower())]
autos = autos.drop(index = odometer_outliers)
df1.head()
np.__version__
df.dot(df.T)
cpq['On Zayo Network Status'].value_counts()
content_wed11.shape
df_ab_page = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_ab_page['intercept'] = 1 $ df_ab_page[['UK','US','CA']] = pd.get_dummies(df_ab_cntry['country']) $ df_ab_page[['new', 'old']] = pd.get_dummies(df_ab_page['landing_page']) $ df_ab_page[['ab_page', 'control']] = pd.get_dummies(df_ab_page['group'])
venues_df[venues_df['rating_count']>0][['venue_id','venue_type','normalised_rating']].groupby('venue_type')\ $             .agg({'venue_id':'count','normalised_rating':['mean','median',percentile(10),percentile(90)]})
prices.head()
output= "SELECT count(*) from tweet where urls='NO_URL'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['Number of NULL Columns'])
group_by_catgory = review['rating'].groupby(review['Category']) $ for name, group in group_by_catgory: $     print name $     print group.mean() $     print group.median()
autos['registration_year'].describe()
gs.score(X_test_total_checked, y_test)
df2['converted'].sum()/len(df2)
df_predictions_clean = df_predictions.copy()
cnxn = pyodbc.connect(conn_str) $ cursor = cnxn.cursor() $ for table_info in cursor.tables(tableType='TABLE'): $     print(table_info.table_name)
projects_csv = non_blocking_df_save_or_load_csv(projects, "{0}/projects".format(fs_prefix))
token_sendReceiveAvg_month = pd.merge(token_sendReceiveAvg_month,empInfo[["ID","level"]],how ="left",on="ID")
pold=df2['converted'].mean() $ print(pold)
pd.to_datetime(a[0]).date() == start_idx
factor_ts = index_ts.pivot(index='price_date', columns='ticker', values=['adj_close_price'])['adj_close_price'] $ factor_ts.head()
train_set.polarity_value.value_counts()
test = datatest[datatest['expenses'].isnull()] $ train = datatest[datatest['expenses'].notnull()]
list(refl.attrs)
df2[['control','ab_page']]=pd.get_dummies(df2['group']) $ df2.head()
fin_p.index
pin2 = pin.copy() $ for j in pin: $     pin2.append(j)
f0 = w.get_data_filter_object(step=0) $ f0.include_list_filter
from sklearn.metrics import confusion_matrix,accuracy_score
df_subset.dtypes
events_enriched_df['topic_name'].drop_duplicates().count()
df = pd.DataFrame({'one' : [1., 2., 3.], 'two' : [3., 2., 1.]}); df
bst = xgboost.train(param, dtrain, 10)
X = df[features] $ y = np.ravel(df[target]) $ rfe = RFE(model, 13) $ rfe.fit(X, y)
corr_q5 = df_return.copy(deep=True) $ corr_q5.drop('Dummy Variables',axis=1,inplace=True) $ corr_q5.head()
autos['vehicle_type'].unique()
avg_annual_days_traded = years.median() $ avg_annual_days_traded
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new.head(10)
google_stock.isnull().any()
df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique()
df = git_log['2007':'2018'] $ df['timestamp'].hist(bins=200, figsize=(10,8)) $ df[df['author'] == 'Linus Torvalds']['timestamp'].hist(bins=200, figsize=(10,8))
_delta=_delta.to_dict()
a.size
df2 = df.drop(['year'], axis=1) $ df2.corr()
local.export_to_quilt(3)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head(10)
subreddits = pd.read_sql("SELECT DISTINCT subreddit FROM comments", con=engine) $ print(subreddits['subreddit'].values)
raw = fat.get_price_data(ticker) $ ohlcv = raw
df2 = df $ df2.drop(df2.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df2.drop(df2.query("group == 'control' and landing_page == 'new_page'").index, inplace=True) $ df2.info()
clf2 = svm.SVC(kernel='linear') $ clf2.fit(X_train, y_train) $ yhat2 = clf2.predict(X_test) $ print("Avg F1-score: %.4f" % f1_score(y_test, yhat2, average='weighted')) $ print("Jaccard score: %.4f" % jaccard_similarity_score(y_test, yhat2)) $
df=df[df.Trip_duration >0]
mw = pd.read_csv('mw.csv', sep='~')
df2.loc[ab_page_index, "ab_page"] = 1
sarima_mod = sm.tsa.statespace.SARIMAX(endog, exog=exog, order=(1,1,1), seasonal_order=(1,1,0,52), trend='c').fit() $ print(sarima_mod.summary()) $ plt.plot(sarima_mod.resid, "bo") $ print(plot_acf(sarima_mod.resid, lags=104)) $ print(plot_pacf(sarima_mod.resid, lags=104)) $
pivot = pd.pivot_table(noise_data_2, $                         index = "Hour of Day", $                         aggfunc={"Hour of Day":"count", $                                  "Borough": lambda x:x.value_counts().index[0]}) # Anonymous function $ pivot.head()
df2.query('landing_page=="new_page"').count()/df2.shape[0] #probability that an individual received the new page
infreq_users = tweet_freq[tweet_freq['count'] < 300].index.tolist() $ logic = (associations['associationType'] != 'retweet') & (associations['screenName'].isin(infreq_users) == True) $ selected_users = associations[logic]['tweetId'].unique() $ df = df[df['tweetId'].isin(selected_users)]
d = datetime.date(2016, 7, 8) $ d.strftime("On %A %B the %-dth, %Y it was very hot.")
Ralston.head()
perf_train['Default'].value_counts()
for df in (test,train): $     df["Promo2Since"] = pd.to_datetime(df.apply(lambda x: Week( $         x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1).astype(pd.datetime)) $     df["Promo2Days"] = df.Date.subtract(df["Promo2Since"]).dt.days
Sandbox = Historical_Raw_Data.copy()
df.info()
user_info_df.head()
manager.image_df.head()
df = pd.DataFrame(np.random.rand(5,2)) $ df.index = [ 'row_' + str(i) for i in range(1, 6) ] $ df
grid.cv_results_['mean_test_score']
eur_usd = df.loc['EUR-USD']['Change'] #This is chained indexing $ df.loc['EUR-USD']['Change'] = 1.0 #Here we are changing a value in a copy of the dataframe $ print(eur_usd) $ print(df.loc['EUR-USD']['Change']) #Neither eur_usd, nor the dataframe are changed
rsskb_gbm_best = gbm.set_params(**rsskb_gbm.best_params_).fit(X_train, y_train) $ y_pred_rsskb = rsskb_gbm_best.predict(X_test)
params = {'kneighborsclassifier__n_neighbors': [i for i in range(2, 14)]} $ grid = GridSearchCV(pipe, param_grid=params, scoring = 'neg_mean_squared_error') $ grid.fit(X_train, y_train) $ grid.best_estimator_.predict(X_test) $ print('\n The Negative Mean Square error score is {:.2f}'.format(grid.best_estimator_.score(X_test, y_test)))
images_clean.head()
control_mean = (df2.query('group == "control"')['converted']==1).mean() $ control_mean
words = ['stately', 'controlled', 'pulsing', 'channeling', 'sprectral', 'seamlessly', 'cracked', 'characteristic', 'sustained', 'passable', 'emphasizes', 'rollicking', 'fluttering', 'suck', 'insistent'] $ importance = [9.9, 8.5, 7.8, 6.6, 6.5, 6.2, 6.2, 6.1, 5.8, -5.7, 5.7, 5.7, 5.6, -5.5, 5.4]
autos.unrepaired_damage.value_counts(normalize=True, $                                      dropna=False)
metrics, predictions = pipeline.test(X_valid, y_valid, output_scores=True) $ print("Performance metrics on validation set: ") $ display(metrics) $ print("Individual scores: ")
!pttree -L 2 --use-si-units --sort-by 'size' 'data/my_pytables_file.h5'
df_clean3 = df_clean2.drop('timestamp', axis=1)
plt.figure(figsize=(16,8)) $ fb['2012':].resample('W')['share_count'].sum().plot()
print xmlData.dtypes $ print '' $ print csvData.dtypes
events["speed"] = events.distance / events.time $ events.sort_values('speed', ascending = False).head()
(df2.landing_page=='new_page').mean()
sampled_authors_grouped_by_author_id_flattened.cache() $ sampled_authors_grouped_by_author_id_flattened.count()
targetUsersRank=targetUserItemInt.groupby(['user_id'])['label'].sum().sort_values(ascending=False).to_frame() $ print targetUsersRank.shape $ targetUsersRank.head()
m = RandomForestClassifier(n_estimators=100, min_samples_leaf=30, max_features='log2', n_jobs=-1, oob_score=True, random_state=42) $ m.fit(X_train, y_train) $ print_score(m)
from sklearn.model_selection import train_test_split $ trip_data_sub = trip_data.ix[trip_data.Total_amount >0,:] $ trip_data_sub["tip_percentage"] = trip_data_sub.apply(lambda x: float(x["Tip_amount"])/float(x["Total_amount"]), axis = 1)
df = pd.read_csv("msft.csv", skiprows=[0, 2, 3]) $ df
hdf['Age'].groupby(level=0).apply(lambda x: x.max() - x.min()).reset_index(name='diff')
datetime_df.dtypes
run txt2pdf.py -o "MONTEFIORE MEDICAL CENTER  Sepsis.pdf"   "MONTEFIORE MEDICAL CENTER  Sepsis.txt"
grouped1 = df.groupby(['product_type', 'state'])['price_doc'].mean() $
pred = cross_val_predict(etr, X, y) $ plt.figure(figsize=(12, 8)) $ plt.plot(temp['Close'], label='Close') $ plt.plot(temp['Pred'], label='Pred') $ plt.legend()
tweets.head()
stocks.index.names
t2['p1'] = t2['p1'].str.title() $ t2['p2'] = t2['p2'].str.title() $ t2['p3'] = t2['p3'].str.title()
secclintondf = secclintondf.reset_index(drop=True)
df3 = df3.where(df3['HT'] > 30).dropna() $ df3 = df3.where(df3['HT'] < 2700).dropna() $ print(len(df3))
length_of_old = (df2[df2['landing_page'] == "old_page"].user_id.count()) $ length_of_old
plt.hist(p_diffs); $ plt.axvline(org_diff,c='r',linewidth = 2);
epoch3_df.head(5)
df[df.location_id>0].raw_location_text.value_counts().tail()
url_root = 'https://api-eu.hosted.exlibrisgroup.com/almaws/v1/analytics/reports' $ key = 'REDACTED' $ limit = '1000'
my_string = "Hi world" $ re.sub("Hi", "Hello", my_string) $ print("[1] my_string is unchanged: " + str(my_string)) $ my_string = re.sub("Hi", "Hello", my_string) $ print("[2] my_string is changed after variable assignment: " + str(my_string))
pres_df.head()
twitter_archive_full.rating_denominator.unique() $ len(twitter_archive_full[twitter_archive_full.rating_denominator == 0])
major_cities_l1 = major_cities_layers[0] $ major_cities_l1_fset = major_cities_l1.query(where= 'FID < 11') $ type(major_cities_l1_fset)
df.info() $
from pyspark.sql.functions import lit, concat $ bad_content_size_df.select(concat(bad_content_size_df['value'], lit('*'))).show(truncate=False)
p = apple['Adj Close'].plot(title="Apple Stock") $ fig = p.get_figure() $
result = grouper.dba_name.value_counts()
future_pd_30360_origpd_all[(future_pd_30360_origpd_all.fk_loan==330) & (future_pd_30360_origpd_all.fk_user_investor==63)].to_clipboard()
bnb.columns $ type(bnb.gender)
cityID = '0c2e6999105f8070' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anaheim.append(tweet)  
f_ip_device_minute_clicks.show(1)
print("Percentages of bad clients with crash pings:\t{}".format(pct(len(clients_with_crashes), len(misbehaving_clients))))
df = pd.DataFrame(res)
data.head()
df = bq.Query(query).execute().result().to_dataframe() $ df.head() $ df.describe()
autos.rename(index=str, columns={"odometer":"odometer_km"})
full['Gender'].value_counts().plot(kind='bar',figsize=(12,4)) $ plt.title('Gender Distribution',size=15) $ plt.xlabel('gender') $ plt.ylabel('count')
first_result.find('a') 
out_df = pd.DataFrame(preds) $ out_df.columns = ["high", "medium", "low"] $ out_df["listing_id"] = test_df.listing_id.values $ out_df.to_csv("xgb_starter2.csv", index=False)
test_centroids = np.zeros((test["review"].size, num_clusters), dtype="float32") $ counter = 0 $ for review in clean_test_reviews: $     test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map) $     counter += 1
jobs = pd.read_sql_table('jobs', con=engine) $ occurrences = pd.read_sql_table('occurrences', con=engine) $ zones = pd.read_sql_table('zones', con=engine) $ cities = pd.read_sql_table('cities', con=engine) $ availabilities = pd.read_sql_table('availabilities', con=engine)
df_archive["doggo"].value_counts()
joined_hist = pd.crosstab(index=goodreads_users_df['joined'], columns="count") $ joined_hist['joined_freq'] = joined_hist['count'] * 100 / joined_hist.sum()['count'] $ joined_hist = joined_hist.sort_values(ascending=False, by='joined_freq') $ joined_hist.head(10)
'my string'.index('s')
auto = pd.read_excel("Auto.am_Final.xlsx")
shopping_carts = pd.DataFrame(items) $ shopping_carts
df1 = pd.DataFrame([pd.Series(np.arange(10, 15)), pd.Series(np.arange(15, 20))]) $ df1
g_sorted1.head(10)
candidates.head()
grouped = pd.DataFrame(more_100_df.groupby('username')['full_text']\ $                                   .apply(lambda tweets: '|||'.join(tweets[1:101])))
str_gpd = gpd.read_file('Street_Segments/Street_Segments.geojson') $ str_bfr_gpd = str_gpd[['OBJECTID','STREETSEGID','geometry','SHAPE_Length']].copy() $ str_bfr_gpd.geometry = str_bfr_gpd.buffer(0.0001) $ str_gpd.head().T
dframe_team.drop(dframe_team.columns[[2,3,4,5]],inplace=True,axis=1)
set(geocoded_df['Judgment.Text'].str.lower())
tweets_df.tweet_place.unique()
print("SVM Classification 2_gram Results:") $ print("Training time: %.3fs; Prediction time: %.3fs" % (time_train, time_predict)) $ print("-"*55) $ print(classification_report(y_test, y_prediction_SCM)) $ print("="*55)
import TwitterGEXF as TG #custom gexf saver $ today = date.today() $ filename = 'twitter_graph_data/%s_tweet_bigraphFULL.gexf' % date.today() $
print(type(df_final['created_time'][0])) $ pd.__version__
engagement = json.loads(resultJson)
conditions_clean.value_counts(dropna=False)
processed_working_exp.working_exp.plot.box(showmeans=True, figsize=(4,8))
lower_vars = sample.variable.str.lower()
conditions_lower = conditions_m.str.lower() $ conditions_lower
load_rf = pickle.load(open('random_forest.sav', 'rb')) $ load_rf.score(x_test,y_test)
import datetime $ start = datetime.date(2014, 1, 1) $ weekly_data = yahoo_finance.download_quotes("GILD", start_date=start, interval=yahoo_finance.WEEKLY) $ print weekly_data
bb.plot(y=['high','low'])
d = {'prediction' : pd.Series(predictions), 'SalePrice' : pd.Series(y_test)} $ pd.DataFrame(d)[:500].plot(y=['prediction', 'SalePrice'], figsize=(16,4), alpha=0.5)
deletes.to_csv(folder + "\\" + law_deletes_file, sep="\t", index = False)
merged_NNN.info()
pt_weekly = pt_weekly.reindex(weeks, method='bfill') $ pt_weekly.head(10)
param_test3 = {'min_samples_split':range(1,400, 20), 'min_samples_leaf':range(30,71,10)} $ gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80,max_depth=11,max_features='sqrt',subsample=0.8),\ $                         param_grid = param_test3, scoring='log_loss',n_jobs=4,iid=False, cv=5) $ gsearch3.fit(drace_df[feats_used],y)
data.plot(x='VMM', y='mean', kind='scatter')
sites_on_net.rename(columns={'Building ID': '# Buildings on Net'}, inplace=True) $ sites_no_net.rename(columns={'Building ID': '# Buildings not on Net'}, inplace=True)
prob_new_page = df2['landing_page'].value_counts()[0]/len(df2) $ print (prob_new_page)
for v in data.values(): $     if v['answers']['Q5'] == 'Yes - Coming no matter what': $         v['answers']['Q5A'] = 'n/a' $     if v['answers']['Q5'] == 'Maybe - I want to see the lineup first': $         v['answers']['Q5A'] = 'n/a'
specs_eval_api.append(specsJson["nebl.io"])
subway4.to_csv('station_weekly.csv')
contractor_clean.loc[contractor_clean['contractor_id'] == 139,'city'] = 'Fargo' $ contractor_clean.loc[contractor_clean['contractor_id'] == 139,'address1'] = '900 42nd Street S' $ contractor_clean.loc[contractor_clean['contractor_id'] == 139,'address2'] = 'P.O. Box 6740'
tweet_df.info()
df['auto_express_with_rank_2'] = (df['Tags'].str.contains("auto express")) & (df['Rank'] < 4)
plt.plot(df['yearOfRegistration'],df['price'],'.') $ plt.show()
[tweet for tweet in df_clusters[df_clusters.cluster_cat==15].text[:10]]
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
p_stats = p_stats.drop_duplicates()
MATTHEWKW.head()
VMMmeans = {} $ for station, Id in stations.items(): $     temp = df.loc[setup.loc[Id, 'StartTimeStamp']:setup.loc[Id, 'EndTimeStamp'], station] $     VMMmeans[station] = temp.mean()
cercanasA1_11_14Entre150Y200mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 150) & (cercanasA1_11_14['surface_total_in_m2'] < 200)] $ cercanasA1_11_14Entre150Y200mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre150Y200mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre150Y200mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
merged_df.index
students.sort_values(by='weight', ascending = False)
df_copy.sample(3)
row_count = df.shape[0] $ print(row_count) $
print('Scores') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.score),np.median(df.score))) $ print('Number of coments') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.comms_num),np.median(df.comms_num))) $
dataframe = pd.read_csv("stateratios.csv") $ dataframe = dataframe.sort_values('Ratio') $ dataframe.head(n=10)
types_of_complaints = all_complaints['Descriptor'].value_counts() $ types_of_complaints.head()
df3['intercept'] = 1 $ logit3 = sm.Logit(df3['converted'], df3[['intercept','new_page','UK','US']]) $ result = logit3.fit() $ result.summary()
importances = final_xgb.get_fscore() $ importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())}) $ importance_frame.sort_values(by = 'Importance', inplace = True) $ importance_frame.plot(kind = 'barh', x = 'Feature', figsize = (8,12), color = 'orange')
df_new = pd.merge(df2, countries_df, how='inner', left_index=True, right_index=True) $ df_new[['UK','US']] = pd.get_dummies(df_new['country'])[['US','UK']] $ df_new.sample(5)
df_genres = df.groupby("genre") $ print(df_genres['score'].mean().sort_values(ascending=False))
weather_all.index
new_bandits = sum(betas_mask) $ sample_sizes = gb.count().values.ravel() $ posteriors_nume = np.sqrt(sample_sizes) * new_bandits $ posteriors_denom = sum(posteriors_nume) $ posteriors = posteriors_nume / posteriors_denom
happiness_df=happiness_df.groupby('dates').mean() $
engine = create_engine("sqlite:////Users/daryarudych/Desktop/repos/SQL-Python/hawaii.sqlite")
df['date'] = pd.to_datetime(df['date'])
print ("Training the random forest...") $ from sklearn.ensemble import RandomForestClassifier $ forest = RandomForestClassifier(n_estimators = 100) $ forest = forest.fit( train_data_features, train["sentiment"] ) $
for col in x.columns: $     if sum(x[col].isnull()) > 1: $         print('Column ', col, ' contains nans') $         x[col].fillna((x[col].mean()), inplace=True)
(np.array(p_diffs) >  obs_diffs).mean()  
filt_M=building_pa_prc_shrink.permit_number.str.contains('M')
y_pred = lr.predict(X_test)
model = sm.OLS.from_formula('arrests ~ gal_eth', summary) $ results = model.fit() $ print(results.summary())
unique_top_tracks.plot.bar() $ plt.xticks(np.array(range(0,5)),unique_top_tracks.track_name) $ plt.title('Top Tracks by Unique Artist Followers')
billstargs.billtext = billstargs.billtext.replace({r'\n': '', r'<html><body><pre>': '', r'</pre></body></html>': '', r'_':'', r'&lt;all&gt;': '', r'&lt;DOC&gt;':'', r'  ': ' ', r'   ': ' ', r'    ':'', r'(\d)': '', r'[()]':'', r';':'', r'--':'', r',':'', r':':'', r'\.(?!\d)': ''}, regex=True)
%bash $ gsutil cat "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" | head
data2=data.set_index("date") $ data2.head()
print("1 second:", pd.to_datetime(1000000000, unit='ns')) $ print("Example 2:", pd.to_datetime(1490195805433502912, unit='ns'))
df_sched = df_sched2 $ print(df_sched.dtypes) $ df_sched.head()
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'],c['type'])
full_p_new = df2.converted[df2.group == 'treatment'].mean() $ full_p_new
typesub2017 = typesub2017.drop(['MTU','MTU2'],axis=1)
len(finnal_data["u_id"].unique())
df['state'].value_counts()
props.prop_name
(df2['landing_page']=='new_page').mean()    #probability that an individual received the new page
new_dems.newDate.isnull().sum()
data = [{'a': 1, 'b': 2}, {'a': 5, 'b': 10, 'c': 20}] $ df = pd.DataFrame(data, columns=['a', 'b']) $ print(df.shape) $ print(df)
df2['tripDay'] = df2['tripduration'][(df2['DN'] == "D")] $ df2['tripNight'] = df2['tripduration'][(df2['DN'] == "N")] $ df2.head()
autos["registration_year"].value_counts()
df2['intercept']=1 $ df2[['control','treatment']]=pd.get_dummies(df2['group'])
print(contribs.info())
daterange = pd.date_range(scn_genesis[0],datetime.today(),freq='1M')
df.to_csv('ab_edited.csv', index=False) $ df2 = pd.read_csv('ab_edited.csv')
QUIDS["week"].value_counts()
df_twitter_archive.loc[np.random.randint(0,df_twitter_archive.shape[0],40), ['text','name']]
speakers[0].keys()
from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.neighbors import LSHForest
df.to_csv('new_edited.csv', index=False)
kick_projects_ip[features].shape
hpd["2015-06":"2015-08"]['Complaint Type'].value_counts().head(5)
xmlData['county'].value_counts()
amps = measures / (2 * ureg.ohm)  # I = V/R $ amps.dimensionality
! /usr/local/cuda/bin/nvcc test_cpp_cuda_codes/hello-world.cu $ !./a.out
portvals = ml4t.compute_portvals(dforders=dforders, dfprices=trade_data_btc.df_h, trend=X_btc_test.index $                                 , start_val=10000, commission=0.0029, impact=0.001) $ portvals
new_page_converted = np.random.choice([1, 0], size=nnew, p=[pnew, (1-pnew)])
print(df_users['bio1'].value_counts()) $
precipitation = pd.read_csv('../clean_data/hourly_precipitation.csv')
tmp_cov_1 = tmp_cov.reset_index(level=0) $ tmp_cov_1
cohort = sts.query(qry)
date_now = datetime.today() $ date_now
df.head(10)
predict_day = weather_x.index[-60]
pd.Period('3/5/2016')
avg_tweek_stops_window_crimes =  int(np.average(stops_two_week_window['sum_window_crimes'])) $ avg_tweek_tweek_crimes = int(np.average(crime_tweek_tweek_window['sum_crimes'])) $ avg_week_tweek_crimes = int(np.average(crime_week_tweek_window['sum_crimes'])) $ avg_datily_tweek_crimes = int(np.average(crime_day_tweek_window['sum_crimes'])) $
a = df2['group']=='control' $ actual_p_old =df2[a].converted.mean() $ print('Probability of individual converted if he is in control group:{}'.format(actual_p_old))
df = df.reset_index(drop=True)
clf_vot_tfidf = VotingClassifier(estimators=[('rf', clf_RF_tfidf), $                                              ('lr', clf_LR_tfidf), $                                              ('rgf', clf_RGF_tfidf)], voting='soft').fit(X_traincv_tfidf, y_traincv_tfidf)
reddit_df.drop_duplicates( $     ['author', 'body', 'controversiality', 'created_utc', 'distinguished', $      'gilded', 'id', 'name', 'parent_id', 'subreddit', 'month', 'year'], $     inplace=True $ )
dec07=pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_3/all_file.txt',names=['chips']) $ matched_id07=[np.where((test1['chips']==i.split('_')[0])==True)[0][0] for i in dec07['chips']] $ test3=test1.reset_index().drop(matched_id07) $ print test1.shape, test3.shape
endpoint_instance = wml_credentials['url'] + "/v3/wml_instances/" + wml_credentials['instance_id'] $ header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} $ response_get_instance = requests.get(endpoint_instance, headers=header) $ print response_get_instance $ print response_get_instance.text
type(df.date[0])
tfidf_vectorizer = TfidfVectorizer(min_df=0) $ tfidf_vectorizer.fit(dataset.data)#TODO $ tfidf_matrix = tfidf_vectorizer.transform(dataset.data)#TODO $ tfidf_matrix $
pvt = pvt.drop(['ga:dimension2', 'customerId'], axis=1) $ pvt = pvt[['ga:transactionId', 'ga:date', 'customerName', 'productAndQuantity']] $ pvt
X_new = test_tfidf.loc[:207] $ new_x = X_new.iloc[:,index_smote] $ new_y = test_tfidf['y']
shows['release_month'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%m')) $ shows['release_month'] = shows['release_month'].dropna().apply(lambda x: str(x)) $ shows['release_month'] = shows['release_month'].dropna().apply(lambda x: int(x))
first_result.find('strong').text[0:-1]
groupmean = tokendata.drop("ID",axis=1).groupby("level").agg(mean_except_outlier).reset_index()
columns = ['No-show', 'Scholarship', 'Hypertension', 'Diabetes', $            'Alcoholism', 'Handicap', 'SMS_received'] $ clean_appt_df[columns].groupby('No-show').mean()
result_pageCountry.summary2()
pd.np.nan
ks_ppb=pd.DataFrame(kick_projects.groupby(['category','launched_year','goal_cat_perc'])['pledge_per_backer','goal_reached'].mean()) $ ks_ppb.reset_index(inplace=True) $ ks_ppb.columns= ['category','launched_year','goal_cat_perc','avg_ppb','avg_success_rate'] $ ks_ppb[:2]
c = np.concatenate([a, b]); c  #Concatenate along an existing axis.
overweight_threshold = 30 $ people.eval("overweight = body_mass_index > @overweight_threshold", inplace=True) $ people
df['gts'].max() #Gross ticket sales??
experiment_df[['UUID_hash2', 'date']].groupby('date').count()
y = df_series#pd.Series(y, index=dates) $ arma_mod = sm.tsa.ARMA(y, order=(2,2)) $ arma_res = arma_mod.fit(trend='nc', disp=-1)
test.first_affiliate_tracked.isnull().sum()
autos = autos.drop(['seller','offer_type','no_of_pictures'],axis=1)
to_be_predicted_Day5 = 38.62511148 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
from IPython.display import FileLink
top_brand_prices = dict() $ for brand in top_brands: $     top_brand_prices[brand] = autos.loc[autos['brand']==brand, 'price'].mean() $ top_brand_prices
result.isnull().sum(axis=0)
df_small = df_small.drop(columns='variable')
print("Individual scores: ") $ scores = pd.concat([y_eval, predictions, X_eval], axis=1) $ display(scores[:6])
drop1 = ['story','alternative name','NO OF RECORDS STOLEN','interesting story','UNUSED' , 'UNUSED.1', 'Exclude', 'Unnamed: 13', '1st source link', '2nd source link', '3rd source', 'source name'] $ Lab7 = Lab7[[col for col in Lab7.columns if col not in drop1]]
to_be_predicted_Day5 = 22.24313565 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
tweet1.lang
pop_flat = pop.reset_index(name='population') $ pop_flat
sorted_requests.foreachRDD((rdd, time) => { $     println("Top users @ " + time) $     rdd.take(5).foreach( $     pair => printf("User: %s (%s)\n", pair._2, pair._1)) $ }
twitter_archive_df_clean = twitter_archive_df_clean[pd.isnull(twitter_archive_df_clean['retweeted_status_id'])]
result.head(20)
new_reps.tail(10)
ra = urllib.request.urlopen("https://worlddatabaseofhappiness.eur.nl/hap_nat/desc_na_genpublic.php?cntry=35") $ r=ra.read() $ soup = BeautifulSoup(r) $ print(type(soup)) $
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
test.sample(5)
y_test_under[fm_bet_under].sum()
series = pandas.Series(np.random.standard_normal(10))
pd.Series({"one": 1, "three": 3}, index=["one", "two"])
rng_utc.tz
words_mention_sk = [term for term in words_sk if term.startswith('@')] $ corpus_tweets_streamed_keyword.append(('mentions', len(words_mention_sk))) # update corpus comparison $ print('List and total number of mentions: ', len(set(words_mention_sk))) #, set(terms_mention_stream))
ax2 = sns.scatterplot(subs, comments, hue=senti_vader, ); $ ax2.set_ylabel('Comment Count'); $ ax2.set_xlabel('Subreddit Sub Count');
filename = "RC_2018-01-01to06_crypto" $ nlp = spacy.load('en') $ occurances = {}
patterns = {'rent':rent_patterns, $             "cash": cash_patterns, $             'deposit': deposit_patterns} $ PATTERNS = {k: "|".join(["({})".format(el)for el in v]) for k, v in patterns.items()}
plt.scatter(USvideos['dislikes'], USvideos['views']) $
for col_x in np.arange(len(col_list)): $     df.rename(columns={col_list[col_x]: rename_list[col_x]}, inplace=True)
a[a.find(':')]
pred=classifier.predict(X_test)
mars_tweets = [] $ for status in tweepy.Cursor(api.user_timeline, id="@MarsWxReport").items(10): $     mars_tweets.append(status) $ mars_weather = mars_tweets[0]._json $ mars_weather
pos_tfidf = models.TfidfModel(pos_bow, id2word=pos_dic) $ neg_tfidf = models.TfidfModel(neg_bow, id2word=neg_dic)
datAll['year'] = datAll['Date'].map(lambda x: x.year) $ datAll['month'] = datAll['Date'].map(lambda x: x.month) $
expr = "(landing_page == 'new_page' and group == 'control') or (landing_page == 'old_page' and group == 'treatment')" $ df.query(expr)['user_id'].count()
y_pred_rf = rf.predict(X_test) $ y_train_pred_rf=rf.predict(X_train) $ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(rf.score(X_test, y_test)))
scores = [] $ for i in range(1,25): $     rfe = RFE(lm,i) $     rfe.fit(x_train,y_train) $     scores.append(rfe.score(x_test,y_test))
top_supporters = merged.groupby(["contributor_firstname", "contributor_lastname"]).amount.sum().reset_index().sort_values("amount", ascending=False).head(11)
valid['load'] = scaler.transform(valid) $ valid.head()
odometer.describe()
cont_total = df2.query('group == "control"').nunique() $ pct_conv_cont = df2.query('group == "control" and converted == 1').user_id.nunique() $ print(pct_conv_cont/cont_total['user_id'])
autos['year_of_registration'].describe()
from IPython.core.display import HTML $ def css_styling(): $     styles = open("styles/custom.css", "r").read() $     return HTML(styles) $ css_styling()
data_month = data_nonan_temp.groupby(['Month'], sort=True) $ data_month.describe()
twitter_archive_df_clean['stage'] = twitter_archive_df_clean[['doggo','floofer','pupper', 'puppo']].replace('None','').sum(1)
df_main.p3.head(3)
new_page_converted = np.random.choice([1,0], size = n_new, p = [0.1196,1-0.1196]) $ new_page_converted
df1.index[-1] -  df1.index[0]
cfModel = Sequential() $ merge = Merge([userModel, poiModel], mode='dot') $ cfModel.add(merge)
fb_all = farebox.copy() $ fb_day_time = fb_all.groupby(['service_day','day_of_week','service_time','service_datetime']).agg({'entries':np.sum}).reset_index() $ fb_day_time.rename(columns={'entries':'entries_green'}, inplace=True) $
customer_id = '/7/KMLZlMBnmWtb9NNkm3bYMQHWrt0C1BChb62EiQLM=' $ cust = trans.loc[trans['msno'] == customer_id].copy() $ cust.iloc[:, 1:].tail(6)
new_page_converted = np.random.binomial(1, cr_under_null, size=n_new) $ print(len(new_page_converted)) $ new_page_converted
p_conv = df2['converted'].mean() $ p_conv
df_daily[["PREV_DATE", "PREV_ENTRIES"]] = \ $     (df_daily.groupby(["C/A", "UNIT", "SCP", "STATION"])["DATE", "ENTRIES"] $             .transform(lambda grp: grp.shift(1))) $ df_daily.tail(5) $ df_daily.head(5)
%sql \ $ SELECT DISTINCT twitter.user_id, twitter.tweet_text FROM twitter \ $ WHERE twitter.tweet_text REGEXP 'Roger Federer|Tennis';
wrQualified = nvidia.filter(lambda p: p["gfx"]["features"]["wrQualified"]["status"] == "available" ) $ wrQualified.count()
print(df1.append([df2]))
kayla['SA'] = np.array([ analyze_sentiment(tweet) for tweet in kayla['Tweets'] ]) $ kelsey['SA'] = np.array([ analyze_sentiment(tweet) for tweet in kelsey['Tweets'] ])
retweet_df['retweet_user_id'].unique().size
cleanedData[cleanedData['text'].str.contains("&amp;")].text
grouped_modern = modern_train.groupby(['country_destination'],as_index=False)
jail_census = pd.concat([january_jail_census, $                          feburary_jail_census, $                          march_jail_census]) $ jail_census
doc_id_list = np.array(reuters.fileids(category_filter)) $ doc_id_list = doc_id_list[doc_id_list != 'training/3267']
df_data.PERIDOOCORRENCIA.value_counts()
vocab = cvec.get_feature_names() $ print(vocab)
rng.asi8.dtype
y.mean()
df['text_no_urls_names'] = remove_by_regex(df_1, re.compile(r"@[^\s]+[\s]?"))['text']
deserialized = json.loads(serialized) $ if "data science" in deserialized["topics"]: $     print (deserialized)
plt.hist(p_diffs); $ plt.axvline(actual_diff, c='red');
tickets = pd.read_csv('./feastly/tickets.csv', delimiter='|')
mask=f>=0.5 $ print(f[mask])
df_protest.loc[2]
e = Example() $ print(e.__dict__) $ print(e.__dict__.__class__)
ser = pd.Series([1, np.nan, 2, None]) $ ser
QUIDS2 = QUIDS.copy() $ QUIDS2["week"] = QUIDS["week"].apply(lambda x: x if np.isnan(x) else int(x)) $ import seaborn as sns $ ax = sns.boxplot(x="week", y="qstot", data=QUIDS2).set_title("Descrease in depressive symptoms over time")
namb_instance = td_amb / np.timedelta64(30, 's') $ ndoor_instance = td_door / np.timedelta64(30, 's') $ nmcu_instance = td_mcu / np.timedelta64(30, 's') $ nrelay_instance = td_relay / np.timedelta64(30, 's') $ ntemp_instance = td_temp / np.timedelta64(30, 's')
y_train_pred = model.predict(X_train.as_matrix()) $ utils.metrics(y_train, y_train_pred)
df2.info()
df.groupby(df.index.hour).count().plot(y='unique_key',kind='bar')
testheadlines = test["text"].values $ basictest = basicvectorizer.transform(testheadlines) $ predictions = basicmodel.predict(basictest)
n_old = df2[df2['landing_page'] == 'old_page'].shape[0] $ n_old
classification_data = classification_data[classification_data.primary_role == 'company'].copy()
df2_control = df2.query("group == 'control'") $ convereted_rate_old = round(df2_control.converted.mean(),4) $ print(convereted_rate_old)
new_page_converted = np.random.binomial(1,P_new,n_new) $ new_page_converted
with pd.HDFStore("all_data.h5") as writer: $     giss_temp.to_hdf(writer, "/temperatures/giss") $     full_globe_temp.to_hdf(writer, "/temperatures/full_globe") $     mean_sea_level.to_hdf(writer, "/sea_level/mean_sea_level") $     local_sea_level_stations.to_hdf(writer, "/sea_level/stations")
import sklearn.model_selection as model_select $ results = model_select.cross_val_score(classifier, X, y, cv=10, scoring='accuracy') $
df_input['created_at'] = pd.to_datetime(df_input['created_at']) $ df_train = df_input.loc[df_input['created_at'] < datetime(2015, 9, 1)].copy() $ display(df_train.head()) $ display(df_train.tail()) $ display(df_train.describe())
df_sched = df_sched[~(df_sched.Initiation < 0)].copy()
df2_control = df[df.group == 'control'] $ (df2_control.converted == 1).sum()/len(df2_control)
young.registerTempTable("young") $ context.sql("SELECT count(*) FROM young")
sns.set(color_codes=True) $ sns.distplot(utility_patents_subset_df.number_of_claims, bins=40, kde=False) $ plt.show()
cityID = 'd5dbaf62e7106dc4' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jacksonville.append(tweet) 
df1.columns = df2.columns $ df=pd.concat([df1,df2]) $ df.head()
def print_time_range(col_name): $     print('--- {} ---'.format(col_name)) $     print('min: {}'.format(min(df[col_name]))) $     print('max: {}'.format(max(df[col_name]))) $     print()
X_mice.iloc[na_index, 1] = np.NAN
ts[pd.datetime(1951, 6, 1):pd.datetime(1952, 1, 1)]
df['Mo'] = df['datetime'].map(lambda x: x.month) $ df.head()
cross_val_score(lr,X_train,y_train,cv=10).mean()
pd.get_option("display.max_rows")
df2['user_id'].nunique() $ print ("Total Number of Unique row : {}".format(df2['user_id'].nunique()))
Obama["sentiment"] = [sentiment_of_tweet(text) for text in Obama["text"]] $ Obama["negative"] = Obama["sentiment"]<0 $ Obama.groupby(by="negative").count()["id"]
extract_deduped_with_elms.drop_duplicates(inplace=True)
fig, ax = plt.subplots(figsize = (20,15)) $ plt.hist(df.num_comments, bins = 100, range = (0,500));
list(c.find({}, {'name.first': 1, $                  'born': 1}))
agent.epsilon=0 # Don't forget to reset epsilon back to previous value if you want to go on training
print(d) $ d.sort() $ print(d)
Project.head()
shows['fixed_runtime'] = shows['fixed_runtime'].dropna().apply(int)
journalist_gender_summary_df = pd.DataFrame({'count':user_summary_df.gender.value_counts(), 'percentage':user_summary_df.gender.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'}) $ journalist_gender_summary_df
df1.io_state[3856]
sys.path
df3[df3['group'] == 'treatment'].head(3)
joined_store_stuff.show()
data.columns
plt.scatter(df_hate['Polarity'], df_hate['Subjectivity'], alpha=0.5, color='purple') $ plt.title('Tweet #hate, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
week41 = week40.rename(columns={287:'287'}) $ stocks = stocks.rename(columns={'Week 40':'Week 41','280':'287'}) $ week41 = pd.merge(stocks,week41,on=['287','Tickers']) $ week41.drop_duplicates(subset='Link',inplace=True)
df_clean.head()
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2','floor','rooms']])
california_house_dataframe = pd.read_csv("https://storage.googleapis.com/mledu-datasets/california_housing_train.csv", sep=',') $ california_house_dataframe.describe()
"{0} what is up {1}? I am just {2}".format('Yo','breh', $                                           'chilling')
reddit.Comments.value_counts(ascending=False).head(25) #just seeing the distribution of the number of comments $
pickle.dump(posts, open('data/posts_with_age_gender.dat', 'wb'))
pd.period_range('2017-01', '2017-12', freq='M')  # This gives us 12 month long periods
closingPrices.nlargest(1)
linkNYC.head()
n_new = df2.query('group == "treatment"').shape[0] $ print(n_new) 
cityID = '7a863bb88e5bb33c' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anchorage.append(tweet) 
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p0, (1-p0)])
for idx, col_name in enumerate(X_train.columns): $     print("The coefficient for {} is {}".format(col_name, classifier.coef_[0][idx]))
from sklearn.tree import DecisionTreeClassifier
for column in ls_other_columns: $     df_onc_no_metac[column].unique()
fin_df['reportDate'] = pd.to_datetime(fin_df['reportDate'])
%config InlineBackend.figure_format = 'svg' $ %config InlineBackend.figure_format = 'retina'
plt.violinplot(resampled1_groups)
s.str.get_dummies()
df_providers.groupby(['drg3']).get_group(39).head(3)
trump.tail()
data.info() 
random.choice( ['red', 'black', 'green'] )
np.exp(0.0149)
nnew=df2.query('landing_page=="new_page"').count()[0] $ nnew
twitter_archive_master['rating_numerator'].value_counts()
df_new.head()
import sklearn $ from sklearn import decomposition $ from sklearn.decomposition import PCA $ from sklearn import datasets
pd.options.display.max_rows = 200 $ pd.options.display.max_colwidth = 300 $ cotradicted_pairs.style.set_properties(**{'text-align': 'left'})
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'],format='%Y-%m-%d %H:%M:%S')
twitter_master2.shape 
c.set_index('cutoff_time')['days_to_next_churn'].plot(); $ plt.vlines(x = (c.loc[c['churn'] == 1, 'cutoff_time']).values, ymin = 0, ymax = 200, color = 'r'); $ plt.ylabel('Days to Next Churn');
df.head(2)
idx = a[ (a['drg3_str']=='303')].index.tolist() $ print('length of IDX',len(idx)) $ assert len(idx) > 0, 'Length of IDX is NULL' $ print( a.loc[idx[0]] ) $
df.info()
df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))
hist.head()
display_code('models.py',[262,264])
df_clean.info()
t_old = len(df.query("group == 'treatment' and landing_page == 'old_page'")) $ print("Number of treatment group users landing on old_page is {}".format(t_old)) $ c_new = len(df.query("group == 'control' and landing_page == 'new_page'")) $ print("Number of control group users landing on new_page is {}".format(c_new)) $ print("Number of users landing on the wrong page is {}".format(t_old + c_new))
data.groupby(['Name'])['Salary'].sum()
scale = preprocessing.StandardScaler() $ scale.fit(X) $ X = scale.transform(X) $ X[0:5]
people.sort_index(axis=1, inplace=True) $ people
df_new['weekday'] = pd.get_dummies(df_new['day'])['weekday'] $ df_new.head()
autos["price"].sort_values(ascending=True)[2000] # 150 $ autos.loc[autos["price"] < 150,:]
a = 5 $ b = 0.5 $ df['X'] = a + b * norm.rvs(size=N, loc=df['W']) $ df['Y'] = a + b * norm.rvs(size=N, loc=df['W'])
extract_all.loc[(extract_all.APP_FIRST_NAME.isin(['GARY','Gary','gary'])) $                &(extract_all.app_branch_state=='VA'), $                 ['APP_APPLICATION_ID','APPLICATION_DATE_short','APP_PRODUCT_TYPE','APP_LOGIN_ID', $                 'APP_FIRST_NAME','APP_MIDDLE_NAME','APP_LAST_NAME','APP_SSN', $                 'APP_DOB','APP_CELL_PHONE_NUMBER','DEC_LOAN_AMOUNT1']]
hp = houseprint.load_houseprint_from_file('new_houseprint.pkl')
df = pd.read_csv('dataForLab2.csv')
loan_stats = loan_stats[no_missing_values_response] $ print('how many missing values do we have now:' , loan_stats['loan_status'].isna().sum())
tmp_df.to_file('geocoded_evictions.shp')
temp = weather_mean['Temp (deg C)'] $ temp.head()
df_clean.rating_denominator = df_clean.rating_denominator.astype(str)
tweets_original['retweeted'].value_counts()
noaa_data['AIR_TEMPERATURE'].replace(-9999,np.nan,inplace=True)
clf = LinearRegression(n_jobs=-1) $ clf.fit(X_train, y_train) $ confidence  = clf.score(X_test, y_test) $ print("Confidence our Linear Regression classifier is: ", confidence)
learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr=(32,10)) $
sql = "SELECT * FROM paudm.cosmos order by paudm_id limit 5 " $ df3 = pd.read_sql(sql,engine)
import math $ def roundup_15k(x): $     return float(math.ceil(x / 15000.0)) * 15000 $ df['sale_price'] = df['sale_price'].apply(lambda x: roundup_15k(x)) $ df['sale_price']
raw , y = clean_test_data('data/raw_data.json')
import pandas $ df2 = pandas.DataFrame(list(dict(c).values())[1:20]) $ df2.head()
BLINK.plot_count(blink)
news_title = soup.find('div', class_='content_title').text.strip() $ news_title
testing[(testing.Open!= 0.0)&(testing.Open!= 1.0)]
dfData.get_dtype_counts()
df.donation_date = pd.to_datetime(df.donation_date) $ df.charitable = df.charitable.astype('bool') $ df['zipcode'] = df.zipcode_initial.str[0:5]
sel_df = df[df['Beat'].isin(selected_beats)]
unique, counts = np.unique(y_hat_lr2, return_counts=True) $ print(unique, counts)
fuelType_list = list(set(train_data.fuelType))
from nltk.corpus import stopwords # Import the stop word list $ print (stopwords.words("english")) 
volt_prof_before.set_index('Bus', inplace=True) $ volt_prof_after.set_index('Bus', inplace=True)
no_null_events = events.filter("store_id is not null")
crimes.info(null_counts=True)
conn = 'mongodb://localhost:27017' $ client = pymongo.MongoClient(conn) $
load_dotenv('.env')
plot_confusion_matrix(cm_knn, classes=['COLLECTION', 'PAIDOFF'], $                      normalize=False, title="Confusion matrix for knn", $                      cmap=plt.cm.Blues)
run txt2pdf.py -o"2018-06-19 2015 UNIVERSITY OF MIAMI HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 UNIVERSITY OF MIAMI HOSPITAL Sorted by discharges.txt"
!ls ../data
if  not os.path.exists("movies.csv"): $     print("Missing dataset file")
ax = df['SPY'].plot(title="SPY Rolling Mean",label="SPY")
df_goog.index = df_goog.index - pd.DateOffset(years = 12)
df2[(df2['landing_page']=='new_page') & (df2['converted']==1)].count()[0]
pd.scatter_matrix(df, color='k', alpha=0.5, figsize=(12, 6)) $ tight_layout()
mode_fn = (lambda x: stats.mode(x)[0][0])
print(temp1.shape, temp1.ndim)
df_u2=df2.user_id.nunique() $ print("Number of unique user_ids in df2 : {}".format(df_u2))
authors = EQCC(git_index) $ authors.get_cardinality("author_uuid").by_period() $ print(pd.DataFrame(authors.get_ts()))
targetUserItemInt['label']=[((1.0 if i1>0 else 0.0)+(5.0 if (i2>0 or i3>0) else 0.0)+(20.0 if i5>0 else 0.0)+\ $                              (-10.0 if i1==0 and i2==0 and i3==0 and i4>0 and i5==0 else 0.0) $                             )*(2.0 if pr==1 else 1.0) $                             for i1,i2,i3,i4,i5,pr in zip(targetUserItemInt['1'],targetUserItemInt['2'],targetUserItemInt['3'],\ $                                                       targetUserItemInt['4'],targetUserItemInt['5'],targetUserItemInt['premium'])] $
pd.concat([s1, s2], ignore_index=True)
df_trips.head(3)
pandas_list = pd.DataFrame(list) $ print("New data type: %s" % (type(pandas_list)))
test_sentence = test_sentence.replace(re.compile(r"@[^\s]+[\s]?")) $ test_sentence[0]
active_referred = referred_users['active'].sum()/len(referred_users) $ inactive_referred = unreferred_users['active'].sum()/len(unreferred_users) $ total_users_referred = len(clean_users[clean_users['invited_by_user_id'].notnull()]) $ SD_referred = np.sqrt(2*total_users_referred/total_users * (1-total_users_referred/total_users) / total_users)
pd.options.display.max_rows = 60 $ pd.options.display.max_columns = 20 $ pd.options.display.max_colwidth = 50
df2['ab_page'] = df2['old_page'] $ df2 = df2.drop(['old_page', 'new_page'], axis=1) $ df2.head()
get_items_purchased('alpa.poddar@gmail.com', product_train, customers_arr, products_arr, item_lookup)['child_sku'].unique() $
grouped = retweets.groupby(['parentPostAuthor','parentPost']).size().reset_index() $ grouped.rename(columns={0:'counts'},inplace=True) $ grouped.sort_values('counts', ascending=False, inplace=True)
pd.date_range('2015-07-03', periods=8, freq='H')
df_new['country_UK'] = df_new['country'].replace(('US', 'UK', 'CA'), (0, 1, 0)) $ lm_uk = sm.OLS(df_new['converted'], df_new[['intercept', 'country_UK']]) $ results_uk = lm_uk.fit() $ results_uk.summary()
logit_mod = sm.Logit(df_new.converted, df_new[['intercept', 'UK', 'US']]) $ results = logit_mod.fit() $ results.summary()
bus['text'].iloc[1]
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','UK_ab_page','US_ab_page']]) $ results=logit_mod.fit() $ results.summary()
api_df.info()
crimes['2011-06-15']['NEIGHBOURHOOD'].value_counts().head(5)
raw_df = pd.DataFrame.from_csv(path=raw_data, header=None)
site_valsdf = pd.DataFrame.from_records(site_vals)
P.plot_1d_layer('mLayerLiqFluxSoil')
proj_df.head()
df_bug.head()
ddp.head()
appleinbounds.shape
twitter_dataset.to_csv('Twitter_archive_master.csv') $ image_copy.to_csv('Image_master.csv') $ tweet_data_copy.to_csv('Twitter_api.csv') $ df_copy.to_csv('Twitter_archive.csv')
measure.columns = ['station', 'date', 'precip', 'tobs']
df.shape
print('{} false positives and {} false negatives'.format(confusion_matrix(y_test,knn_pred)[0,1],confusion_matrix(y_test,knn_pred)[1,0])) $
unique_animals.add("Tiger")
old_p = df2['converted'].mean() $ old_p
prev_year = dt.date.today() - dt.timedelta(days=365) $
soup = BeautifulSoup(response.text,'lxml')
o_page_converted = np.random.binomial(1, p_old, n_old) $ print('The old_page convert rate: {}.'.format(o_page_converted.mean())) $ print('The old_page convert rate: {}.'.format(round(o_page_converted.mean(), 4)))
data.head()
df["DISPOSITION_TYPE"].value_counts()
image_array = to_array_variabel(image, (608,1)) $ format_array = to_array_variabel(review_format, (608,1)) $ fake_array = to_array_variabel(fake, (608,1)) $ X = np.hstack((image_array, format_array, body.toarray(), title.toarray(), fake_array))
measurements_df.head()
gdf_gnis.shape
df.to_csv('ab_new.csv', index=False)
data.cm_type.value_counts().sort_index()
%timeit StockData = pd.read_csv(StockDataFile + '.gz', index_col=['Date'], parse_dates=['Date'])
InfinityWars_Predictions.to_csv('InfinityWars_Predictions.csv', encoding='utf-8')
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=.20, random_state=42) $ X1_train.shape , y1_train.shape,  X1_test.shape, y1_test.shape
def auth(): $     oauth = tweepy.OAuthHandler( CONSUMER_KEY, CONSUMER_SECRET ) $     oauth.set_access_token( ACCESS_TOKEN, ACCESS_TOKEN_SECRET ) $     return oauth
during.head()
udacity_image_prediction_url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ with open(udacity_image_prediction_url.split("/")[-1],"wb") as file: $     file.write(requests.get(udacity_image_prediction_url).content)
df.loc[:, 'name']
for key,value in trends_per_year_avg_ord.items(): $     print( "Deaths and Trend average in " + str(key) + " = " + str(deaths_per_year[key]) + ", " + str(trends_per_year_avg[key]))
engine = create_engine('sqlite:///results.db') $ pd.read_sql_query('SELECT * FROM demotabl LIMIT 5;',engine)
vocab = set(tokens) $ vocab = pd.Series(range(len(vocab)), index=vocab) $ vocab
!ls ../input/ #MacOS command $
rate.dtypes # Making sure I'm not crazy
report_score(y_test, y_fit_proba, 0.2)
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)']) $ confusion_mat
df_users = df_users.dropna(axis=0) $ print(df_users.shape) $ df_users.head(10)
menu_dishes['about'].fillna('', inplace=True) $ menu_dishes_to_analyze = menu_dishes[menu_dishes['menu_id'].isin(X['master_menu_id'])]
train_sample.info()
for x in other: $     df.set_value(x, 'company', False) 
requests.get(FAFRPOS_pdf)
backup = aux.copy()
Test.AutoRun(1, True)
metric_group_type_id = 30 $ url = form_url(f'metricGroupTypes/{metric_group_type_id}/metricTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
data.columns
data.drop('month',axis=1) $
qty_left_1c_store $ table_store.iloc[0].at[u'Quantity Txn Part'] $
lst_all_date = [pd.to_datetime(x).date() for x in pd.date_range(start, end, freq=pd.tseries.offsets.BDay())] $ lst_all_date $
n_old = df2.query('group == "control"')['user_id'].count() $ n_old
bitcoin_price = pd.read_csv('/Users/Madhu/Documents/Courses/data_bds/price_BTC_oct29.csv')
df.nsmallest(1,'rating')
unique_tweets['normalized_user_statuses_count'] = np.divide(unique_tweets.user_statuses_count, unique_tweets.user_created_days) $ unique_tweets['normalized_user_followers_count'] = np.divide(unique_tweets.user_followers_count, unique_tweets.user_created_days) $ unique_tweets['normalized_user_favourites_count'] = np.divide(unique_tweets.user_favourites_count, unique_tweets.user_created_days) $ unique_tweets['normalized_user_listed_count'] = np.divide(unique_tweets.user_listed_count, unique_tweets.user_created_days) $ unique_tweets['normalized_user_friends_count'] = np.divide(unique_tweets.user_friends_count, unique_tweets.user_created_days)
stepwise_model.fit(train2)
model_w_int = Logit(df2['converted'], $                            df2[['intercept', 'ab_page', 'CA', 'UK', $                                'ab_page_and_CA', 'ab_page_and_UK']]) $ results_w_int = model_w_int.fit() $ results_w_int.summary()
df2 = pd.read_csv('clean_data_df2.csv') $ df2.head()
train_small_sample.is_attributed.mean(), val_small_sample.is_attributed.mean()
{x:len(re[x].unique()) for x in re.columns}
shows[shows['genre'].isnull()]
full = full.set_index('AdmitDate')
s[s > 10].head()
located_data['country'][:7000].value_counts()
bnbAx['target'] = np.where(bnbAx['country_destination']=='US', 1, 0)
df = pd.DataFrame(data, index = ['label 1', 'label 2', 'label 3']) $ df
Base.classes.keys() $
def get_integer2(s): $     return vehicleType_list.index(s)
lm = sm.OLS(df3['converted'], df3[['intercept', 'ab_page', 'UK', 'US']]) $ results = lm.fit() $ results.summary()
from dateutil import parser $ columns = ['timestamp', 'name', 'location'] $ formatted_retweets = [] $ for rt in retweets: $     formatted_retweets.append([parser.parse(rt['created_at']), rt['user']['name'], rt['user']['location']]) $
newdf= newdf.join(mom, how='inner')
yhat = kNN_model.predict(X_test) $ yhat[0:5]
yhat_prob = lr.predict_proba(X_test) $ yhat_prob
df_combined = df_combined.join(pd.get_dummies(df_combined['country'], prefix='country')) $ df_combined.head()
df[abs(df.dollar_change_open).round(2) != abs((df.open_price-df.offer_price)).round(2)]
print("Number of unique visitors in train set : ",train_data.fullVisitorId.nunique(), " out of rows : ",train_data.shape[0]) $ print("Number of unique visitors in test set : ",test_data.fullVisitorId.nunique(), " out of rows : ",test_data.shape[0]) $ print("Number of common visitors in train and test set : ",len(set(train_data.fullVisitorId.unique()).intersection(set(test_data.fullVisitorId.unique())) ))
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value 
(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011'] $     .agg({'POPESTIMATE2010': np.average, 'POPESTIMATE2011': np.sum}))
bytes_val = b'this is bytes' $ bytes_val $ decoded = bytes_val.decode('utf8') $ decoded  # this is strkk (Unicode) now
new_page_converted.mean()- old_page_converted.mean()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].count()
df.loc['1998-09-10':'1998-09-15','MeanFlow_cfs':'Confidence']
BID_PLANS_df.loc['5ec976dd'].to_frame().transpose()
plt.figure(figsize=(30, 10)) $ shortened_df = df_stations.head(20) $ sns.barplot(shortened_df.STATION,shortened_df.ENTRIES) $
yc_new1 = yc_new.merge(zipincome, left_on='zip_depart', right_on='ZIPCODE', how='inner') $ yc_new1.head()
scoresdf[1].head(20)
len(image_predictions_df[(image_predictions_df.p1_dog == False) & $                          (image_predictions_df.p2_dog == False) & $                          (image_predictions_df.p3_dog == False)])
definition_2_details = client.repository.store_definition(filename_mnist, model_definition_2_metadata) $ definition_2_url = client.repository.get_definition_url(definition_2_details) $ definition_2_uid = client.repository.get_definition_uid(definition_2_details) $ print(definition_2_url)
csvtonumpy.csvtonumpy(inputFile, outputFile, True)
twitter_df_clean = twitter_df_clean.drop(['doggo', 'floofer','pupper','puppo'], axis=1)
parking_planes_df=parking_planes.toPandas() $ parking_planes_df
properati['state_name'].value_counts(dropna=False)
temp_df = pd.DataFrame(tobs_info, columns=['date','tobs']) $ temp_df.set_index('date', inplace=True) $ temp_df.head()
tweets_clean.drop(columns = ['in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'], inplace = True)
Presentation: $ https://docs.google.com/presentation/d/1XHIZjRBZu-u2tOOpH9cECmVPLyFc7PT59l7CZbfzahQ/edit?usp=sharing
vc.head()
df_joined[['CA','UK','US']] = pd.get_dummies(df_joined['country']) $ df_joined = df_joined.drop(['US'], axis = 1) $ df_joined.head()
len(fda_drugs.ActiveIngredient.unique())
carrierDF = trainDF[['UNIQUE_CARRIER', 'CARRIER_CODE']].drop_duplicates() # Only get unique examples
y_age = (pd.DataFrame(X_age_notnull['age'], columns=['age'])) $ X_age = deepcopy(X_age_notnull) $ X_age.drop('age', axis=1, inplace=True)
browser.quit()
df_new.groupby('country')['user_id'].count() #see the distribution of three countries
alice_sel_shopping_cart = pd.DataFrame(items, index=['glasses', 'bike'], columns=['Alice']) $ alice_sel_shopping_cart
data.loc[(80, slice(None),'put'),:].iloc[0:5,0:4]
(p_diffs > obs_diff).mean()
pitches_unique_labels = list(pitches.columns.difference(responses.columns)) $ pitches_unique_labels
Counter(df['tweet_ats']).most_common(11)
rf = ensemble.RandomForestClassifier(n_estimators=300) $ rf.fit(cX, cy) $ cm = confusion_matrix(cy, rf.predict(cX)) $ sns.heatmap(cm, annot=True)
plt.scatter(compound_final.index, compound_final.Compound) $ plt.show()
df.isnull().values.any() $ df.info()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').user_id.nunique() $ convert_new = df2.query('landing_page == "new_page" and converted == 1').user_id.nunique() $ n_old = df2.query('landing_page == "old_page"').user_id.nunique() $ n_new = df2.query('landing_page == "new_page"').user_id.nunique()
spark.stop()
results.summary2()
data_for_model.groupby('country').describe().T
rng.to_pydatetime()[0]
dta.loc[dta.risk == "Risk 1 (High)"]
for col in missing_info: $     num_missing = data[data[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing)) #count of missing data
test.to_pickle('../data/merged_data/test.pkl')
train = pd.read_csv('train.csv') $ test = pd.read_csv('test.csv')
dupli = df2[df2['user_id'].duplicated()==True] $
BID_PLANS_df.drop(aliases,inplace=True)
df.head()
def replaceLastOccurrence(mystring, mysubstring, replacement): $     k = mystring.rfind(mysubstring) $     new_string = mystring[:k] + replacement + mystring[k+1:] $     return new_string
guineaFullDf = pd.concat(frameList,axis=0) $ guineaFullDf.head()
autos = autos[autos['registration_year'].between(1900, 2016)] $ autos['registration_year'].value_counts(normalize = True).head(10)
countries_df['country'].unique()
gps__interestlevel_df.to_csv('gps_coords_w_interestlevel_df.csv',index=False)
ratings = ['review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value'] $ filtered_df.dropna(axis=0, subset=[ratings], inplace=True)
equipment.columns
df.dollar_change_open = (df.open_price-df.offer_price) $ df.dollar_change_close = (df.first_day_close-df.offer_price)
@udf(returnType=IntegerType()) $ def weekday(date): $     return datetime.strptime(date, DATETIME_PARSE_PATTERN).weekday()
reg_mod_uk = sm.OLS(df_all['converted'], df_all[['UK_int', 'ab_page']]) $ analysis_uk = reg_mod_uk.fit() $ analysis_uk.summary()
condos.drop(['OBJECTID', 'STATUS', 'UNITTYPE', 'METADATA_ID'], axis=1, inplace=True)
df = pd.DataFrame({'Count 1': 100 + np.random.randint(-5, 10, 9).cumsum(), $                   'Count 2': 120 + np.random.randint(-5, 10, 9)}, index=dates) $ df
JoinedDF = RandomOneDF.join(RandomTwoDF, RandomOneDF["ID"] == RandomTwoDF["id"] ) $ for row in JoinedDF.take(5): $     print row
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
retweet_relation=tweets.loc[notnaindex,["user_name","user_followers","retweet_status_user_name","retweet_status_follower_count"]]
y = df3['converted'] $ X = df3[['intercept', 'new_page', 'UK_new_page', 'US_new_page', 'UK', 'US']] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0)
SANDAG_ethnicity_df[SANDAG_ethnicity_df['ETHNICITY'] == 'White'].head()
lm=sm.OLS(df2['converted'],df2[['intercept','ab_page']]) $ results=lm.fit() $ results.summary()
tweet_df.text[4]
np.array([1, 2, 3, 4]) + 2
factor_df = index_df.set_index([index_df.index, 'ticker']).unstack(level=-1)['adj_close_price'] $ factor_df.head()
df = pd.read_sql('SELECT a.address_id, a.city_id, c.city FROM address a JOIN city c ON a.city_id = c.city_id ORDER BY a.address_id DESC;', con=conn) $ df
df.shape
os.getcwd()
train=pd.read_csv('pisa2009train.csv') $ test=pd.read_csv('pisa2009test.csv') $ train.shape
browser = webdriver.Chrome('chromedriver.exe') $ browser.get(url_cerberus) $ time.sleep(5) $ html = browser.page_source $ soup = BeautifulSoup(html, "html.parser") $
events.organiser.to_frame()
dfRegMet2016.to_pickle(data+"dfRegMet2016Sentences.p")
p_diffs = [] $ for i in range(10000): $     converted_new = np.random.choice([1,0], size=n_new, p=[conversion_rate_null, (1 - conversion_rate_null)]) $     converted_old = np.random.choice([1,0], size=n_old, p=[conversion_rate_null, (1 - conversion_rate_null)]) $     p_diffs.append(converted_new.mean() - converted_old.mean()) $
df.describe()
compound_final['Date'] = pd.to_datetime(compound_final['Date'])
train_holiday_oil_store_transaction = train_holiday_oil_store.join(transactions, ['date', 'store_nbr'], 'left_outer') $ train_holiday_oil_store_transaction.show()
weather = pd.read_csv("weather.csv", parse_dates={"date" : ['Date']}) $ weather = weather.drop(['HDD', 'CDD'], axis=1)
_ = ok.grade('q03e') $ _ = ok.backup()
jobs.loc[(jobs.FAIRSHARE == 10) & (jobs.ReqCPUS == 1) & (jobs.GPU == 1)].groupby(['Group']).JobID.count().sort_values(ascending = False)
(pd.DataFrame.from_dict(data=importances, orient='index') $    .to_csv(base_filename + '_featimp.csv', header=False))
def filter_columns(row, all_cards_cols): $     set_cols = list(row.columns) $     intersection = list(set(set_cols) & set(all_cards_cols)) $     return row.filter(intersection) $ only_cards = only_cards.apply(lambda x: filter_columns(x, all_cards_columns))
df.Date = pd.to_datetime(df["Date"]) $ df.head()
url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ response = requests.get(url) $ with open('image-predictions.tsv', mode ='wb') as file: $     file.write(response.content) $ Imagenes_data = pd.read_csv('image-predictions.tsv', sep='\t' )
data = pd.DataFrame(data_dict) $ data.head()
hashed_modeling2.unpersist()
data = pd.read_hdf('rel_data.h5', 'behav') $ data.head()
dfEtiquetas["city"] = dfEtiquetas["place"].apply(lambda p: p["location"]["city"] if "city" in p["location"].keys() else None)
df.reindex(['b', 'c', 'd', 'a', 'e']) # compare to df2 above
results=lm.fit() $ results.summary()
verify_response.json()['name']
split_index = df.index[df['Date'] == split_date].tolist()[0]
sfrom = vacancies[vacancies.salary_from > 10000].salary_from.min() $ sto = vacancies.salary_to.mean() $ sfrom, sto
fb.set_index('created_time', inplace=True)
svm_model = SVC(C=0.01, class_weight='balanced', kernel='linear')
cutoff_times.groupby('msno')['churn'].sum().sort_values().tail()
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Station = Base.classes.stations
testheadlines = test["text"] $ advancedtest = advancedvectorizer.transform(testheadlines) $ advpredictions = advancedmodel.predict(advancedtest)
empInfo["senderlevel"] = empInfo.level $ empInfo["receiverlevel"] = empInfo.level
tweet1.user == tweet1.author #Maybe if this were a retweet, this would be false?
df.query('group != "treatment" & landing_page == "new_page"').count() $
reddit['Subreddits'].nunique() #how many unique subreddits do we have? 
import seaborn as sns $ sns.heatmap(fpr_a)
y_pred = classifier.predict( ... ) $ report = sklearn.metrics.classification_report( ... , ... ) $ print(report)
%%HTML $ <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> keeps disappearing as a trend and it&#39;s not because trump has gotten better as negotiating. <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/GOPShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#GOPShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/TrumpShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#TrumpShutdown</a> <a href="https://twitter.com/hashtag/GOPShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#GOPShutdown</a> <a href="https://twitter.com/hashtag/GOPShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#GOPShutdown</a></p>&mdash; mach229 (@mach229) <a href="https://twitter.com/mach229/status/954503720934535168?ref_src=twsrc%5Etfw">January 19, 2018</a></blockquote> $ <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
geopy.distance.vincenty(start, end).miles
startDay = pd.to_datetime("2017-01-25 16:00:00", utc=1).tz_convert('US/Eastern') $ now = pd.to_datetime(pd.datetime.now() + pd.Timedelta('4h'), utc = 1).tz_convert('US/Eastern') $ endDay = startDay + pd.Timedelta(np.floor(((now - startDay) / np.timedelta64(1, 'D'))/7)*7, unit = 'd')
nums = pd.Series(['first', 'second', 'third', 'fourth'], index=[1, 2, 3, 4]) $ print(f'Item at explicit index 1 is {nums.loc[1]}') $ print(f'Item at implicit index 1 is {nums.iloc[1]}') $ print(nums.loc[1:3]) $ print(nums.iloc[1:3])
%matplotlib inline $ import matplotlib $ import matplotlib.pyplot as plt $ matplotlib.rcParams['figure.dpi'] = 100 $ df.plot(x='timestamp', y='value')
submit = pd.read_csv('mloutput20160428_0512.csv') $ print(submit.shape) $ submit.tail()
df['country'].replace('', None,inplace = True) $ df['delivery_method'].fillna(value=4,inplace=True) $ df['currency_match']=(df['country'].map(Country_dictionary) == df['currency']).replace([True,False], $ df['venue_name_exits'] = df['venue_name'].isnull().replace([False, True], [0,1])                                                                                      [1,0])
metrobus = pd.read_csv('../Datos Capital/estaciones-de-metrobus.csv',low_memory=False)
sentiments_pd.to_csv("quickbook_competitors_tweet_data.csv")
import numpy as np $ A = np.ones((5,5,3)) $ B = 2*np.ones((5,5)) $ print(A * B[:,:,None])
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs);
dul_adds = "DUL-adds.txt" $ dul_deletes = "DUL-deletes.txt"
topC.groupby(by='date_simple').size().plot(kind='bar')
dt_features_test['state_changed_at'] = pd.to_datetime(dt_features_test['state_changed_at'],unit='s')
top_songs.shape
print gs_1.best_params_ $ print gs_1.best_score_
crimes.columns = crimes.columns.str.strip() $ crimes.columns
r_np= df[df['landing_page']=='new_page'] $ p_np=r_np['landing_page'].count()/df['landing_page'].count() $ print('The probability that an individual received a new page is:  ' + str(p_np))
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
tweet_text.head(10)
odm.head()
pd.read_sql('SELECT * FROM experiments WHERE temperature = 375 ORDER BY irradiance DESC', conn, index_col='experiment_id')
new_df.head()
tweetering.polarity.plot(kind='hist', normed=True) $ range = np.arange(-1.5, 1.5, 0.001) $ plt.plot(range, norm.pdf(range,0,1))
apple_tweets2.shape
df['Updated Created diff'] = (df['Updated At'] - df['Created At']).astype('timedelta64[h]') $ df['Updated Shipped diff'] = (df['Updated At'] - df['Shipped At']).astype('timedelta64[h]') $ df['Shipped Created diff'] = (df['Shipped At'] - df['Created At']).astype('timedelta64[h]')
y2 = api.GetUserTimeline(screen_name="mattjpfmcdonald", count=20, max_id=935706980643147777, include_rts=False) $ y2 = [_.AsDict() for _ in y2]
data = [{'a': i, 'b': 2 * i} $         for i in range(3)] $ pd.DataFrame(data)
df.describe(include=[np.object])
IBMspark_df = sqlContext.createDataFrame(IBMpandas_df) $ for row in IBMspark_df.take(2): $     print row
trading = pt.PairsTrading(asset1 = 'COPUSD', $                           asset2 = 'AUDUSD', $                          df=df)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(new_df2.set_index('user_id'), how='inner')
tokens['five_star_ratio'] = tokens.five_star / tokens.one_star
for action in list_top_actions: $     action_feat = "action_" + action $     sessions_summary[action_feat] = sessions_summary.apply(lambda r: action in r["action"], 1)
df_ad_airings_filter_3.shape
intactness = lambda x: 'Fixed' if x.split()[0] in ['Spayed','Neutered'] else 'Intact' $ df['is_fixed'] = df['SexuponOutcome'].apply(intactness) $ df['gender'] = df['SexuponOutcome'].apply(lambda x: x.split()[1]) $ df['SexuponOutcome'] $
flag = df.pop('flag')
prediction_clean.p1.value_counts()[:20].rename_axis('prediction').reset_index(name='counts')
merged.committee_position.value_counts()
sql("show tables").show()
print('Best Score: {:.3f}'.format(XGBClassifier.best_score)) $ print('Best Iteration: {}'.format(XGBClassifier.best_iteration))
trainDF.drop(trainDF.columns[0], axis = 1, inplace = True)
with open("cache_{}.pkl".format(date), "wb") as f: $     logging.info("Saving query cache.") $     pickle.dump(query_cache, f, pickle.HIGHEST_PROTOCOL)
xml_in_sample['authorId'].nunique()
df2.head()
df['Language'] = None $ df.head(1)
DataSet.head()
pd.concat([test,future_forecast],axis=1).plot(figsize= (12,8)) $ plt.legend(('Market data','Prediction' ))
details = client.repository.get_experiment_details(experiment_uid)
non_na_df['hour'] = (non_na_df.index.hour + 10) % 24 $ non_na_df['dayofweek'] = non_na_df.index.dayofweek
mw.content.replace(to_replace='http://www.youtube',value='https://www.youtube',inplace=True, regex=True) $
countries = pd.read_csv('countries.csv') $ countries.head()
logm=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results=logm.fit()
len(df2.query('landing_page == "new_page"')) / len(df2)
with open('ogh_meta.json','r') as r: $     meta_file = json.load(r) $     r.close() $ sorted(meta_file.keys())
tokenizer = Tokenizer(char_level=True, filters=None) $ tokenizer.fit_on_texts(invoices)
train[~train.date_first_booking.isnull()]['country_destination'].value_counts()
classifier = svm.SVC(kernel='linear') $ classifier.fit(X_train, y_train) $ y_prediction_SCM = classifier.predict(X_test) $
priors_product_reordered= priors_reordered.groupby(["product_id"]).size().reset_index(name ='reordered_count') $ priors_product_reordered.head()
df = read_xml_file("data/safety/toronto_200k.xml") $ df.head()
city_eco["economy"].cat.categories = ["Finance", "Energy", "Tourism"] $ city_eco
allqueryDF.columns = ['state_id', 'domain', 'entity_id', 'state', 'attributes', $                       'origin', 'last_changed', 'last_updated', $                       'created'] # 'event_id' no longer exists?
train['answer_dt']=pd.to_datetime(train['answer_utc'], unit='s') $ test['answer_dt']=pd.to_datetime(test['answer_utc'], unit='s')
open('test_data//open_close_test.txt').read()
df.index
for lyr in freeways.layers: $     print(lyr.properties.name)
joined = join_df(joined, googletrend, ["State","Year", "Week"]) $ joined_test = join_df(joined_test, googletrend, ["State","Year", "Week"]) $ len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])
df_joined = df3.set_index('user_id').join(df2.set_index('user_id')) $ df_joined.sample(5)
print('ADM total: %6d'%adm.ec550.count()) $ print('in_latlon: %6d'%ec550.count())
def mean_grouping(df): $     return df.set_index('datetime').groupby(pd.TimeGrouper('D')).mean().dropna() $ ltccomment = mean_grouping(ltc) $ xrpcomment = mean_grouping(xrp) $ ethcomment = mean_grouping(eth) $
train_small_data.info("deep")
x_train, y_train = x[0:int(0.6*len(x))], y[0:int(0.6*y.size)] $ x_train = np.float32(x_train) $ y_train = np.float32(y_train)
doi_pid = pd.read_csv("crossref-pid-from-doi.csv", $                       dtype=str, $                       keep_default_na=False) $ pd.concat([doi_pid.head(), doi_pid.tail()])
score['pred_rf'].sum()
yc_new4 = yc_new3[yc_new3.tipPC > 1]
df.head()
huffman = df5['HT'].where(df5['new_id'] == 'huffman').dropna() $ plt.hist(huffman, bins=50)
nypd_df.head()
df_new['new_page'] = pd.get_dummies(df_new['landing_page'])['new_page'] $ log_m3 = sm.Logit(df_new['converted'], df_new[['intercept','new_page', 'CA', 'UK']]) $ results3 = log_m3.fit() $ results3.summary()
plot_chernoff_data(df1, 1e-6, 1, "Nth = 1.0") $ plt.savefig('../output/g_perr_vs_M_1.pdf', bbox_inches='tight')
model_lm3 = LogisticRegression() $ model_lm3.fit(X3_train, y3_train) $ y3_preds = model_lm3.predict(X3_test) $ confusion_matrix(y3_test, y3_preds)
autos["date_crawled"].str[:10].value_counts(normalize=True, $                                    dropna = False).sort_index()
tOverweight = len(df[(df['bmi'] >= 25.0) & (df['bmi'] < 30.0)]) $ tOverweight
pred = predict_class(np.array(theta), X_train_1) $ print ('Train Accuracy: %f' % ((y_train[(pred == y_train)].size / float(y_train.size)) * 100.0))
X.shape
pickle.dump(lda_cv, open('iteration1_files/epoch3/lda_cv.pkl', 'wb'))
import test_package.package_within_package
df['source'].value_counts() $ df
sentimentResults = sentim_analyzer.evaluate(test_set).items()
acc.get_monthly_balance(range(4))
stars.to_pickle('data/pickled/new_subset_starred.pkl')
df.to_csv('injuries.csv',index=False)
print('Difference in Variance for HC: {}%  p-value: {}'.format( $     round(F_hc*100,2), stats.f.cdf(F_hc, degrees1hc, degrees2hc)))
import matplotlib.pyplot as plt $ popular_programs.plot.bar()
vect = CountVectorizer(stop_words = 'english',ngram_range = (1,3),min_df = 0.2, binary = True,max_features = 100)
tobs_df = pd.DataFrame.from_records(station_tobs) $ tobs_df.head()
df_tsv_clean = df_tsv.copy() $ df_archive_csv_clean = df_archive_csv.copy() $ df_json_tweets_clean = df_json_tweets.copy()
sentiment_df = sentiment_df.sort_values(["Date"]) $ sentiment_df
suspects_with_1T = suspects_data[suspects_data['imsi'].isin(imsi_with_1T)]
all_gen1_verse=[] $ for verse in gen1: $     gen1verse = sent_tokenize(verse) $     all_gen1_verse.extend(gen1verse) $ print(all_gen1_verse)
trace0 = go.Scatter3d(x=lat_25_1.values, y=lng_25_1.values, z=day_25_1.values,mode='markers',marker=dict(size=12,line=dict(color='rgba(217, 217, 217, 0.14)', width=0.5),opacity=0.8)) $ data = [trace0] $ layout=go.Lay
input =  df_MC_least_Convs.MC_leastConvs.tolist() $ c = Counter(input) $ df_MC_least_Convs = pd.DataFrame(c.items()) $ df_MC_least_Convs.rename(columns={1:'number_months'}, inplace=True) $ df_MC_least_Convs
print(rhum_nc) $ for v in rhum_nc.variables: $     print(rhum_nc.variables[v])
twitter.info() $
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
print(s1.head(3)) $ print() $ print(s1.tail())
workspace_uuid = ekos.get_unique_id_for_alias(user_id, workspace_alias) $ print(workspace_uuid)
ltv_cat = pd.pivot_table(df2, index=['category'], values=['ltv'], aggfunc=[sum, np.mean, len, np.max, np.min]) $ ltv_cat
right = pd.DataFrame({'key': ['yes', 'no'], 'rval': [3, 0]}) $ right
network = popnet.PopNetwork.from_config(configure) $ sim = popnet.PopSimulator.from_config(configure, network) $ sim.run()
recipes.name[selection.index]
students[students.gender == 'F']
content_wed11.to_csv('DATA/mainstream_content_weds_11APR18.csv.gz', compression='gzip')
to_be_predicted_Day3 = 48.70640398 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
data['Company'].groupby([data.index]).agg([ 'count']).resample('W').sum().plot();
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'UK', 'CA']]) $ results = logit_mod.fit() $ results.summary()
model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'afternoon', 'evening']]) $ results = model.fit()
Test.SetFlowVals
df_merged.boxplot(column='retweet_count');
import sys, os $ insertIntoPath(os.path.join(os.getcwd(), '..', '..', 'playground')) $ insertIntoPath(os.path.join(os.getcwd(), '..', 'price'))
score_l50.shape[0] / score.shape[0]
print('Shape : ', pd_train_filtered.shape) $ pd_train_filtered.sample(10)
maximum = df['time_open'].max() $ print(maximum)
confederations = pd.read_csv("confederations.csv")
recommendation_df.drop(['contest_id', 'rank'], axis = 1, inplace = True)
r_t_r = get_raw_txt(word_to_id, x_train[20]) $ print(r_t_r + "\n") $ print("Length: {}".format(len(r_t_r.split(' '))))
rows_in_table = ins.count()[0] $ unique_ins_ids = len(ins["business_id"].unique()) $
bitcoin = df[df['tweet'].str.contains("bitcoin") | df['tweet'].str.contains("btc") | df['tweet'].str.contains("BTC")] $ bitcoin = bitcoin.reset_index(drop=True) $ bitcoin.info()
for idx in myPD.index: $     val = myPD.ix[idx] $     updateQuery = "UPDATE irma SET {0} = '{1}', {2} = '{3}', {4} = '{5}', {6} = '{7}', {8} = '{9}', {10} = '{11}', {12} = '{13}' WHERE DateTime = '{14}';".format('p_mean', val[0], 'p_min', val[1],'p_max', val[2],'p_var', val[3],'p_std', val[4],'p_sum', val[5],'p_count', val[6],idx)
r = requests.get(url) $ r.json()
dfX.head()
frame2.columns
grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)
temp_cat.cat.categories
def plot_data(date): $     plt.figure() $     plt.plot(data.loc[pd.to_datetime(date), ["TMAX", "TMIN", "TMED"]], marker='o') $     plt.ylim([0, 40]) $     plt.show()
print "The percentage the total NetOut of the total NetIn in 2016 was " + str((df2['NetOut'].sum() / df2['NetIn'].sum()) * 100) + "%"
col=['ab_page', 'intercept', 'converted'] $ data=df3[col].join(country.ix[:, 'CA':]) $ data.head()
import statsmodels.api as sm $ convert_old = df2.query(" landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query(" landing_page == 'new_page' and converted == 1").shape[0] $ n_old = len(df[df['group']=='control']) $ n_new = len(df[df['group']=='treatment'])
mis_match = [] $ for i in range(len(image_predictions)): $     if (image_predictions['tweet_id'][i]) not in list(tweet_json['id']): $         mis_match.append(image_predictions['tweet_id'][i]) $ print(len(mis_match))
df.shape[0]
twitter_data_v2=twitter_data.copy() $ Imagenes_data_v2=Imagenes_data.copy() $ tweet_data_v2=tweet_data.copy()
billtargs['billtext'] = np.nan
vwg['season'] = vwg.index.str.split('.').str[0] $ vwg['term'] = vwg.index.str.split('.').str[1]
df = df.dropna() $ df.tail(5)
my_df_free1.iloc[100:110,0:3]
AAPL.shape  # returns 858 rows and 6 columns
example_list = ['The', 'Zebra', 'has', 5, 'stripes'] $ comma_sep_string = ','.join(map(str, example_list)) $ quote_comma_sep_string = ','.join(f"'{w}'" for w in example_list) $ print(comma_sep_string) $ print(quote_comma_sep_string)
jobPostDF['month'] = jobPostDF['date'].apply(lambda x: x.month)
df.query('group == "treatment" and landing_page != "new_page"').count()[0]
daily_df.reset_index(inplace=True) $
search['flight_type'] = search['message'].apply(flight_type)
cleaned_df.drop('Unnamed: 0', axis=1, inplace=True)
LSST_sample_filename = 'LSST_ra_250_283_dec_-40_-15.dat' $ LSST_data = np.genfromtxt(DirSaveOutput+LSST_sample_filename, usecols=[5])
(len(df[df['converted']==1]))/df.shape[0]
r_col = ['835152434251116546', '746906459439529985', '670842764863651840', '855862651834028034', '810984652412424192'] $ archive_df.drop(index=r_col, inplace=True, errors='ignore')
businessyear(Exchange_df.Date_of_Order, Exchange_df.Sales_in_CAD)
A = data['Gender'].value_counts() $ B = data['Gender'].value_counts() $ B.plot(kind='bar') $ plt.show()
has_text[has_text.url=='http://www.nytimes.com/2012/07/30/opinion/keller-the-entitled-generation.html']
pMean = np.mean([pNew,pOld]) $ pMean
stool_metadata_df = metadata_df[metadata_df['SAMPLE'] == 'stool'] $ tissue_metadata_df = metadata_df[metadata_df['SAMPLE'] == 'tissue'] $ na_metadata_df = metadata_df[metadata_df['SAMPLE'].isnull()]
pickle_full = "sorted_stays.p"
from sklearn.model_selection import train_test_split $ X_prepro = psy_prepro.drop(labels=["y"], axis = 1) $ y_prepro = psy_prepro["y"] $ X_train, X_test, y_train, y_test = train_test_split(X_prepro, y_prepro, random_state=1)
ben_dummy.head(20)
model = SGDClassifier(loss='log', random_state=0, n_iter=100) $ model.fit(train_vectors, train_targets)
df_DRGs.tail()
autos.columns
qW.transform = 'r'
api_request = requests.get("http://jordan.emro.info/api/locations") $ api_request.json()[1]
type2017 = type2017.dropna() 
df.fillna(0)
tonecounts = results['tone'].value_counts() $ positive_precentage = tonecounts['Positive'] / sum(tonecounts) $ print ('positive result count: {0} \ntotal result count: {1}\n% of positive result:\ $  {2}%'.format(tonecounts['Positive'],sum(tonecounts),np.round(positive_precentage, 3)*100))
wd = 1e-7 $ bptt = 70 $ bs = 52 $ opt_fn = partial(optim.Adam, betas=(0.8, 0.99))
p_diffs = np.array(p_diffs) $ (p_diffs > act_diff).mean()
responses.columns
dataset[~dataset["article_doi"].str.startswith("1")]["article_doi"].unique().tolist()
for e in _td: $     print e.string
df2.head()
excelDF.head()
list(df_tsv.columns.values)
atloc_opp_dist_tabledata = atloc_opp_dist_count_prop_byloc.reset_index() $ create_study_table(atloc_opp_dist_tabledata, 'locationType', 'remappedResponses', $                    location_remapping, atloc_response_list)
tweets[(tweets['created_at'].dt.year > 2016) & (tweets['created_at'].dt.month > 9)].plot(x='created_at', y='chars')
twitter_archive.name.sort_values()
dti1 = pd.to_datetime(['8/1/2014']) $ dti2 = pd.to_datetime(['1/8/2014'], dayfirst=True) $ dti1[0], dti2[0]
try: $     b'mat\xc3\xa9'.decode('ascii') $ except Exception: $     print(b'mat\xc3\xa9'.decode('ascii', errors="ignore")) # don't print the trouble character.        
users.info()
lm = sm.OLS(df_new['converted'],df_new[['a/b_page','ca_intercept']]) $ result = lm.fit() $ result.summary()
trips.duration.describe()
import matplotlib.pyplot as plt $ import teaching_plots as plot
reddit.head()
df.injured.describe()
n_old = df2[df.group == 'control'].shape[0] $ n_old
plt.style.use('ggplot') $ bb.plot(y='close')
df = pd.read_pickle(train_data_dir+'/LSTM_train_data.pkl') $ df.shape
churned_ordered.head()
assert ft.__version__ == '0.1.19', 'Make sure you run the command above with the correct version.'
! tar -xzvf wikipedia_devtst.tgz
avg = session.query(func.avg(Measurement.tobs)).\ $     filter(Measurement.station == "USC00519281").all() $ print(f"Average Temp: {avg}")
import json $ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=apikey') $ print(r.json())
from sklearn.metrics import mean_squared_error $ y_pred = lin_svr.predict(X_train_scaled) $ mse = mean_squared_error(y_train, y_pred) $ mse
missing_sample.fillna(-999)
tipsDF.tail()
result = json.loads(get_req.content)['result']
dfD.to_sql('distances', conn_aws)
autos["abtest"].value_counts()
preg.shape
twelve_months_prcp.plot(figsize = (10,7), rot = 45, use_index = True, legend=False) $ plt.ylabel('Precipation') $ plt.xlabel('Date') $ plt.title("Precipition in Hawaii from %s to %s" % (twelve_months_prcp.index.min(),twelve_months_prcp.index.max())) $ plt.show()
pd.value_counts(ac['Description']).head()
dataset_ja = dataset.join(longest_date_each_costumer, ['customer_id'],rsuffix="_day") $ dataset_ja.head(3)
df.groupby('lizardNumber').species.count()
temp_us = temp_nc.variables['air'][1, lat_li:lat_ui, lon_li:lon_ui] $ np.shape(temp_us)
index_remove = list(ab_df[mismatch1].index) + list(ab_df[mismatch2].index) $ ab_df2 = ab_df.drop(labels=index_remove,axis=0)
yhat = LR.predict(X_test) $ yhat
fprice_df.to_csv('fprice.csv',index=False)
freq, values = np.histogram(ret_aapl-mu+r, bins=250) $ freq = freq.astype('float32') $ prob = freq/np.sum(freq)
ValidNameEvents.head(1)
data['data'].keys()
digits.target
X = acs_df.drop('homeval', axis=1).values $ y = acs_df['homeval'].values
image_predictions_clean.p1.unique()
abc.get_levels(0)
data.dtypes
tweet_archive_enhanced_clean.head()
noaa_data.loc[:,'AIR_TEMPERATURE'].groupby(pd.Grouper(freq='w')).mean()
df_TempIrregular.set_index('timeStamp',inplace=True)
idx + pd.tseries.offsets.Hour(2)
new_reps.newDate[new_reps.Kasich.isnull()]
tmp = tmp.sort_values(by = 'meantempm', axis = 0, ascending = False) $
df2.groupby(['group'])['converted'].mean()[1] - df2.groupby(['group'])['converted'].mean()[0]
df = df.drop(['APT2'], axis=1)
conn.commit()
baseball_newind.sort_index(ascending=False).head()
pd.isnull(twitter_archive_master).sum()
model = sm.OLS(y_train, sm.add_constant(X_train)) $ model.fit().summary()
ab_file.isnull().sum()
auto_new.Body_Type.unique()
from pandas_datareader import data $ goog = data.DataReader('GOOG', start='2004', end='2016', data_source='google') $ goog.head()
resultsList[:5] # Citigroup in no.3
plt.figure(figsize=(8,6)) $ plt.hist(hm_data.weight, bins=25) $ plt.title('Weight distribution');
sl['mindate'] = sl.groupby('wpdx_id')["new_report_date"].transform('min') $ sl['maxdate'] = sl.groupby('wpdx_id')["new_report_date"].transform('max')
liberiaDf.index.is_unique
df2_new[df2_new['group']=='treatment'].head()
autos = autos[autos['registration_year'].between(1900,2016)] $ autos['registration_year'].describe()
l = df.select_dtypes(exclude=[object]).shape[1] $ n = int(np.floor(np.sqrt(l))) $ m = int(np.ceil(l / n))
df = pd.DataFrame({'foo': [1,2,3], 'bar':[0.4, -1.0, 4.5]}) $ df.values
x_new = df2[ (df2['group'] == 'treatment') & (df2['landing_page'] != 'new_page')].shape[0] $ y_new= df2[ (df2['group'] != 'treatment') & (df2['landing_page'] == 'new_page')].shape[0] $ x_new+y_new
live_capture = pyshark.LiveCapture(interface='wlan1')
trunc_df.loc[168].description
! unzip -o {path_to_zips}On_Time_On_Time_Performance_2015_1.zip readme.html -d $(pwd)
noNulls.orderBy(sort_a_desc).show(5)
new.to_file('heat_complaints.geojson', driver="GeoJSON")
from pandas.tseries.holiday import USFederalHolidayCalendar
data = pd.read_csv('datafinal.csv') $ xprep = data.copy() $ xprep.drop('irlco', axis=1, inplace=True) $ xprep.columns.values[2] = '1hr'
fcst_weekly =fcst_trips.resample('W').sum() #resample weekly $ fcst_trips['date']=fcst_trips.index #add date column for saving $ fcst_weekly['date']=fcst_weekly.index #add date column for saving
df2.query('group == "treatment"').converted.mean()
model_json = model.to_json() $ with open("model/model.json", "w") as json_file: $     json_file.write(model_json)
bus["postal_code"] = bus["postal_code"].fillna("MISSING") $ bus
%%R $ holidays <- c('2014-01-01', '2014-01-20', '2014-02-17', '2014-05-26', $               '2014-07-04', '2014-09-01', '2014-10-13', '2013-11-11', $               '2013-11-28', '2013-12-25') # Ten major holidays, including Memorial Day, Columbus Day, Labor Day, MLK Day $ holidayDates <- as.Date(holidays)
model.save_word2vec_format('/tmp/doc_tensor.w2v', doctag_vec=True, word_vec=False)
sns.heatmap(prec_fine)
!spark-submit --master spark//localhost:7077 --name 'App Name' script.py data/*
total.size
print 'Total amount for non-USA location: ', df[((df.state == '') & (df.city != ''))].amount.sum()
def color_formatting(result): $     if result: $         return("\x1b[32m {}  \x1b[0m".format(result)) $     return("\x1b[31m {} \x1b[0m".format(result))
df['DATE'] = pd.to_datetime({'year':df['YEAR'], 'month':df['MONTH'], 'day':df['DAY']})
scoreCts =  ins.groupby(["score"]).size() $ a=ins.groupby(["score"]).size().reset_index().rename({0:"count"}, axis="columns") $ plt.bar(a["score"], a["count"]) $ plt.show()
y_pred = ADBR.predict(VX) $ print('MSE test: %.3f' % ( $         mean_squared_error(Vy, y_pred))) $ print('R^2 tset: %.3f' % ( $         r2_score(Vy, y_pred)))
aldf = indeed1.append(tia1, ignore_index=True)
new_page_converted = np.random.choice([0,1], size = n_new, p=[1-p_new,p_new])
df.values
to_be_predicted_Day1 = 31.30 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
data= pd.concat([btypedums, data], axis=1)
X, y = merged_data.drop('overdue', axis=1), merged_data['overdue']
ldamodel_Tesla.print_topics(num_topics=3, num_words=7)
pd.concat([test, train]).plot(y=['PJME', 'Random_Forest_Prediction'], figsize=(15,5))
predict_actual = pd.DataFrame(data= description_predict,columns=['description_predict'], index=None)
session.query(Measurement.station,func.count(Measurement.station)).\ $         group_by(Measurement.station).\ $         order_by(func.count(Measurement.station).desc()).all()
df = df.dropna() $ df = df[list(df_symbols.columns) + targets]
TestData = pd.read_csv('test/test.csv')
e_p_b_one.rename(columns = {0:"Count"},inplace=True)
row_vals = list(df_wb.iloc[0,:]) $ row_vals
from mlxtend.frequent_patterns import apriori $ from mlxtend.frequent_patterns import association_rules
weather.head(10)
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 1) & (finals["reb_l"] == 1), 'type'] = 'inside_gamers'
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_opb_brace_saddle[0][0])) $ perr = np.sqrt(np.diag(pcov_opb_brace_saddle[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df = pd.DataFrame({'retweeter_id':retweet_lst_ids_flat,'retweeter_follower_count':retweet_fol}) $
svr= SVR(kernel='rbf') $ svr.fit(X_train_tfidf, y_train)
tweet = api.get_status(id = '892420643555336193') $ print(type(tweet)) $ print(tweet.keys())
h5.close()
m = RandomForestRegressor(n_estimators=40, max_features=0.99, min_samples_leaf=2, $                           n_jobs=-1, oob_score=True) $ m.fit(trn, y_trn);
extract_all.loc[(extract_all.APP_APPLICATION_ID.isin(ids))].groupby('APP_PRODUCT_TYPE').size()
def remove_punctuation(text): $     exclude = set(string.punctuation) $     return "".join(ch for ch in text if ch not in exclude)
np.mean(shows['first_year'].dropna())
msft = pdr.get_data_yahoo('MSFT', $                           start=datetime.datetime(2014, 1, 1), $                           end=datetime.datetime(2017, 3, 1))
print("{} is the proportion of users converted.".format(df['converted'].mean()))
lr_mod = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ lr_fit = lr_mod.fit()
cutoff_times.loc[cutoff_times['days_to_next_churn'] == 0]
s3 = pd.Series(0,pd.date_range('2013-01-01','2014-12-31')) $ s3['2013']
model_yaml = model.to_yaml() $ with open("model_cnn_imdb_{}.yaml".format(epochs), "w") as yaml_file: $     yaml_file.write(model_yaml) $ model.save_weights("model_cnn_imdb_{}.h5".format(epochs)) $ print("Save model to disk")
rng.tz_convert('US/Eastern')
df[df['D'] == 10.72]
del merged_portfolio_sp_latest['Date'] $ merged_portfolio_sp_latest.rename(columns={'Adj Close': 'SP 500 Latest Close'}, inplace=True) $ merged_portfolio_sp_latest.head()
bruins.rename(columns={'Playoff':'playoff','Opponent':'opponent'}, inplace=True)
df_a= df[(df.landing_page =="new_page") & (df.group !="treatment")]; $ print("The number of times the new_page and treatment don't line up: " ,len(df_a))
youthUser2 = pd.merge(left=youthUser1, right=city1, how='left', left_on='cityId', right_on='_id' ) $ youthUser2.head()
df = pd.DataFrame({'Sentence' : sen, 'Crypto Negative' : neg, 'Crypto Neutral' : neu, 'Crypto Positive' : pos, 'Crypto Compound' : com}) $ df = df.set_index('Sentence') $ txt = txt.set_index('Sentence')
log_with_day.select('dateTime','dayOfWeek').show(10) $
train_pos = train_sample.filter(col('is_attributed')==1) $ n_pos = train_pos.count() $ print("number of positive examples:", n_pos)
store_items.fillna(method='ffill', axis=0)
data_set.tail(5)
df_members = pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/members_v3.csv', parse_dates=['registration_init_time'], dtype={'city': np.int8, 'bd': np.int16, 'registered_via': np.int8})
acc.find(agent='agent 4')
new_reps.head()
df_new['CA_ind_ab_page'] = df_new['CA']*df_new['ab_page'] $ df_new['US_ind_ab_page'] = df_new['US']*df_new['ab_page'] $ df_new.head()
grouped = df.groupby('hash_id')
def find_string_in_dict(dict_to_search,string_to_find): $     return dict([(i,n) for i,n in dict_to_search.iteritems() if string_to_find.lower() in n.lower()])
brand_cars = autos.brand.value_counts(normalize=True) $ branding = brand_cars.loc[brand_cars >0.01].index
noNulls.orderBy(sort_a_desc).show(5)
msftAV = msft[['Adj Close', 'Volume']] $ aaplAV = msft[['Adj Close', 'Volume']] $ pd.concat([msftAV, aaplAV])
boto.s3.connection.Location.USWest2
country_dummies = pd.get_dummies(df_new.country) $ df_new = df_new.join(country_dummies) $ df_new['intercept'] = 1 $ df_new.head()
plt.hist(km.labels_)
("1 2 abc Two words.").split(" ")
df_prod_wide = df_prod.pivot(columns='Cantons').resample('1 d').sum() $ df_prod_wide.head()
idx = pd.IndexSlice $ transit_df_rsmpld = transit_df_rsmpld.sort_index().loc[idx['2016-01-01':'2017-12-31',:],:].sort_index() $ transit_df_rsmpld.info() $ transit_df_rsmpld.head()
proj_df['Project Subject Category Tree'].fillna('').apply(has_literacy_and_language)
!h5ls -r 'data/NYC-yellow-taxis-100k.h5'
uber_14["hour_of_day"] = uber_14["Date/Time"].apply(lambda x: getHour(x)) $ uber_14.head()
%matplotlib inline $ sns.violinplot(data=december, inner="box", orient = "h", bw=.03)
df2_treat = df2.query('group == "treatment"').converted.mean() $ df2_treat
pd.set_option('display.max_columns',100) $ df.tail()
data = pd.read_csv(url)
df[['beer_name', 'simple_style', 'brewery_name', 'brewery_country']][df.rating_score == top_score]
train_small_data.head(1)
post_discover_first_purchase_numerator = pd.DataFrame(df).reset_index().drop_duplicates('Email', keep='first') $ post_discover_first_purchase_numerator_count = len(post_discover_first_purchase_numerator[post_discover_first_purchase_numerator['Bought Recommended'] > 0]) $ post_discover_first_purchase_denom_count = len(post_discover_first_purchase_numerator) $ print('Percentage of customers who bought from recommended list: {:.2f}%'.format(100*post_discover_first_purchase_numerator_count/post_discover_first_purchase_denom_count )) $ print('Distribution of time to buy: \n{}'.format(post_discover_first_purchase_numerator['discover_sales_lead_time'].describe()))
p = np.array([[1,2,3],[4,5,6]])
pd.Series(data=y5_train).hist()
%%time $ vectorizer = TfidfVectorizer(min_df=5, analyzer=stemmed_words).fit(all_texts)
test['price_usd'] = pd.Series(predictions)
lr = 1e-3 $ m.lr_find()
import matplotlib.pyplot as plt $ %matplotlib inline $ import matplotlib.ticker as ticker $ import seaborn as sns $ sns.set(color_codes=True) $
signals['short_mavg'] = aapl['Close'].rolling(window=short_window, min_periods=1, center=False).mean() $ signals['long_mavg'] = aapl['Close'].rolling(window=long_window, min_periods=1, center=False).mean() $ signals['signal'][short_window:] = np.where(signals['short_mavg'][short_window:] > signals['long_mavg'][short_window:], 1.0, 0.0)   $ signals['positions'] = signals['signal'].diff()
tx, ty = tsne[:,0], tsne[:,1] $ tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx)) $ ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty)) $ pd.DataFrame(list(zip(tx,ty))).plot.scatter( $     x=0, y=1, alpha = .4)
df['text']= df['text'].apply(lambda x: re.sub('RT |#|@', '', x)) $ df.head(3)
df[df['converted']==1]['user_id'].count()/df.shape[0]
count = merge['zip'].value_counts $ count
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
kickstarter_failed_successful = kickstarter_failed_successful.drop(['name', $                                                                     'category', $                                                                     'deadline', $                                                                     'launched'], axis=1)
retrain_model(load_embeddings('data/numberbatch-en-17.04b.txt'))
%run -i 'label_image/label_image.py' --graph='/tmp/output_graph.pb' --labels='/tmp/output_labels.txt' --input_layer='Mul' --output_layer='final_result' --input_mean=128 --input_std=128 --image='test/George-W-Bush.jpg'
np.info(np.arange)
tweet_df_clean[tweet_df_clean['retweeted_status'].isnull() == False]
traintrain_pd= traintrain.pivot_table(values = 'Visits', index = 'Page', columns = 'date') $ trainvalidation_pd= trainvalidation.pivot_table(values = 'Visits', index = 'Page', columns = 'date')
support.amount.sum() / merged.amount.sum() $
qrtSurge = ((qrt.shift(-3)- qrt) / qrt ) $ surge = qrtSurge[qrtSurge>1] $ surge.sort_values(ascending=False) $
merged.sort_values("amount", ascending=False)
test_df_01.loc[test_df_01['predict'] != test_df_01['label'], 'imagePath']
crimes.dtypes
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"]) $
def nLargest(array,nValues): $     answer = np.mean(array[np.argsort(array)[-nValues:]])  $     return answer
labels.shape
us['country'] = 'USA' $ us['country'].value_counts(dropna=False)
x["A"] $ x.get("A") $ x.A # works only for column names that are valid Python variable names
datelist = ['14-Sep-2017', '9-Sep-2017'] $ search_dates=pd.to_datetime(datelist) $ print(search_dates)
df.drop('date_first_booking',axis=1,inplace=True)
df_celebrities = pd.read_json("celebrities.json", orient="records")
test['load'] = scaler.transform(test) $ test.head()
fulldata_copy[['AUDUSD','MACD']][:500].plot(figsize=(10,6),secondary_y = 'MACD');
reddit_comments_data.groupby('controversiality').count().orderBy('count', ascending = False).show(100, truncate = False)
prcp_analysis_df["date"] = pd.to_datetime(prcp_analysis_df["date"],format="%Y-%m-%d", errors="coerce")
m.fit(lr, 3, metrics=[exp_rmspe])
run txt2pdf.py -o '2018-06-22  2014 872 discharges.pdf'  '2018-06-22  2014 872 discharges.txt'
from IPython.display import IFrame $ IFrame('http://www.aflcio.org/Legislation-and-Politics/Legislative-Alerts', 800, 600)
import pandas as pd $ import numpy as np $ from google.colab import files as filess
red_4.info()
plt.plot(x, y, label='C47C8D65CB0F');
1/np.exp(results.params[1])
reddit.head() #Yes! It worked.
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=2500) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
print(np.shape(b)) $ print(np.size(b)) $ print(np.ndim(b))
data['affair'] = (data.affairs > 0).astype(int)
stats.chisquare(gender_top_school_tab)
archive_df.info()
regions = df.groupby('location') $ print(regions.mean())
ac['Monitoring End'].describe()
train["date_time"] = pd.to_datetime(train["date_time"], errors="coerce") $ train["srch_ci"] = pd.to_datetime(train["srch_ci"], errors="coerce") $ train["srch_co"] = pd.to_datetime(train["srch_co"], errors="coerce") $
np.random.seed(123456) $ ps = pd.Series(np.random.randn(24),pd.period_range('1/1/2013','12/31/2014',freq='M')) $ ps
print(trump.favorite_count.describe()) $ print("   ") $ print(trump.retweet_count.describe())
ffr.index.day
btc = ha.accounts.manual_current('bitcoin', path=os.path.join('data', 'manual_accounts'), $                                  currency='BTC') $ btc.add_transaction('20.04.2016', 'me', 'buy', 6.5382, t_type='buying rate: 391.3129 EUR') $ dep.add_account(btc)
sorted(skills.items(), key=operator.itemgetter(1), reverse=True)[:20]
for f in feature_layer.properties.fields: $     print(f['name'])
events['categories'] = events.category.apply(lambda l: json.loads(l)) $ events.categories.iloc[0]
non_feature_cols = ['team', 'date', 'home_game', 'game', 'round', 'venue', 'opponent', 'season'] $ afl_data = afl_data.rename(columns={col: 'f_' + col for col in afl_data if col not in non_feature_cols})
for x in links: $     print(base_hems_url + x.attrs.get('href')) $
full_url = "https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced" #change this to loop through all $ high_res_soup = BeautifulSoup(requests.get(full_url).text, "html.parser") $ hem_hr = [x.attrs.get('href') for x in high_res_soup.find_all('a') if ".tif" in x.attrs.get('href')] $ hem_hr[1] $
!find {TRN} -name '*.txt' | xargs cat | wc -w
tweets1.text[0]
df = df_data.groupby('DATA')['COM_LUZ_SOLAR','SEM_LUZ_SOLAR'].sum() $ df.head() $ df.reset_index()
autos['fuel_type'].value_counts()
mom = pd.read_csv('FF_Momentum_Factor_Monthly.CSV')
mean = np.mean(data['len']) $ print("The lenght's average in tweets: {}".format(mean))
short_urls = df_congress[ $     df_congress['link_url_long'].apply(ux.is_short) $ ]['link_url_long'].unique() $ len(short_urls)
pd.DataFrame({'population': population, $               'area': area})
import matplotlib.pyplot as plt $ plt.style.use('ggplot')
tweet_data.sort_values('rating', ascending= False).head() $
min_date = max(tweets.date.min(), terror.date.min()) $ max_date = min(tweets.date.max(), terror.date.max()) $ terror = terror[(terror["date"]>min_date) & (terror["date"]<max_date )] $ tweets = tweets[(tweets["date"]>min_date) & (tweets["date"]<max_date )]
features.to_csv('cbg_features_4_19.csv')
vow['Date'] = pd.to_datetime(vow['Date'])
lr_model_saga.fit(X_train, y_train)
lite = df[df['tweet'].str.contains("Litecoin") | df['tweet'].str.contains("ltc") | df['tweet'].str.contains("LTC")] $ lite = lite.reset_index(drop=True) $ lite.info()
to_be_predicted_Day1 = 50.15 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
jail_census = pd.concat([january17_jail_census, $                          feburary17_jail_census, $                          march17_jail_census]) $ jail_census
N_old = df2.query('landing_page == "old_page"')['user_id'].nunique() $ N_old 
dfWords = pd.read_pickle("dfWords.p")
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
df = pd.read_csv('estimating_quartiles.csv')
Raw_Forecast["ID"] = Raw_Forecast.Date_Monday.astype( $     str)[:] + "/" + Raw_Forecast.Product_Motor + "/" + Raw_Forecast.Part_Number.str[:] $ Raw_Forecast.head(20)
print(contribs.contributor_state.unique())
modCrimeData = crimeData $ modCrimeData.set_index("Year", inplace=True) $ modCrimeData.head()
WkData = Data.resample('1W').median() $ WkData
s519397 = session.query(weather.date, weather.prcp).\ $  filter(and_(weather.date.between('2015-01-01','2015-12-31'), weather.station == 'USC00519397')).\ $  order_by(weather.station, weather.date.asc()).all() $ s519397
avg_day_of_month14 = day_of_year14.groupby("avg_day_of_month").mean() $ avg_day_of_month14.head()
sns.factorplot(y='Sale_Price',x='Date',data=df_sale_price,kind='bar',aspect=20)
pd.Timestamp('2018-01-01') + timedelta(hours=3)
weather_yvr.plot(subplots=True, figsize=(10, 10))
twitter_archive.timestamp
num_distinct_payments_money_per_user=df_payments['amountpaid'].value_counts() $ freq_distinct_payments_money_per_user=num_distinct_payments_money_per_user.value_counts() $ cum_dist_user_payments_money = np.linspace(0.,1.,len(num_distinct_payments_money_per_user)) $ cdf_user_payments_money = pd.Series(cum_dist_user_payments_money, index=num_distinct_payments_money_per_user.sort_values())
def readData(startDate,endDate,compTicker): $     dataF = pandas_datareader.DataReader(compTicker,"yahoo",startDate,endDate) $     return dataF
fb_vec.todense()
meal_is_interactive_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meal_is_interactive.csv')['Body'].read().decode('utf-8') $ meal_is_interactive = pd.read_csv(StringIO(meal_is_interactive_csv_string), header=0, delimiter='|')
old_page_converted = np.random.choice([0, 1], size = n_old, p = [(1 - p_old), p_old])
model.save("/home/kmisiunas/Documents/Quipu/models/binding_metric971_2018-04-03_no1.h5") $
df1['label'] = df[forcast_col].shift(-forcast_out) $ df1['label'].head()
to_be_predicted_Day1 = 80.95 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
print('ET score: {:.4f}'.format(scores))
import seaborn as sns $ goog[['Open','High','Low','Close','Adj Close']].plot()
df_goog.describe()
sns.regplot(x=final["ETH Volume"], y=final['Crypto Compound'], fit_reg=False) $ sns.regplot(x=final["ETH Volume"], y=final['Crypto Negative'], fit_reg=False, color = 'r') $ sns.regplot(x=final["ETH Volume"], y=final['Crypto Positive'], fit_reg=False, color = 'g').invert_yaxis()
X_train.head()
actual_payments_combined["payout_date"]=np.array(actual_payments_combined["payout_date_x"],'datetime64[D]')
added_series = pd.Series(index=daterange) $
import statsmodels.api as sm
feature_df = data_df.copy()
tq = tq1 + tq2 $ tokenizer = Tokenizer(num_words=MAX_NB_WORDS) $ tokenizer.fit_on_texts(tq) $ question1_word_sequences = tokenizer.texts_to_sequences(tq1) $ question2_word_sequences = tokenizer.texts_to_sequences(tq2)
sum(df2['user_id'].duplicated())
twitter_archive_clean.info()
f,ax = plt.subplots(1,1,figsize=(12,4)) $ fte_voters.plot.line(x='modeldate',y='approve_estimate',ax=ax)
y_label_train_OneHot.shape
data[data['Processing Time']<datetime.timedelta(0,0,0)]
jobPostDF['date'] = pd.to_datetime(jobPostDF['date'], errors='coerce')
r = kimanalysis.queryresults(test='TE_320860761056_001', model='MO_800509458712_001')
autos = autos[autos['price'].between(1,351000)] $ print(autos['price'].describe()) $
shifted_backwards = msftAC.shift(-2) $ shifted_backwards[:5], shifted_backwards.tail(5)
df2_new = df2.query('landing_page == "new_page"')['landing_page'].count() $ total = df2['landing_page'].count() $ prob = (df2_new / total) $ print(prob)   
merged = pd.merge(zipShp, dfList, left_index=True, right_index=True, on='ZIPCODE', how = 'inner')
tweets.info()
ttTimeEntry['DT'] = pd.to_datetime(ttTimeEntry['DT'])
df_twitter_copy.info()
temps_df.Missouri - temps_df.Philadelphia
style_bw.tail(5)
collection.append('AAPL', aapl[-1:]) $ df = collection.item('AAPL').to_pandas() $ df.tail()
cars = pd.read_csv("/Users/ankurjain/Desktop/autos.csv", encoding='latin1') $ cars.head()
def drop_sequence(cur, sequence_name): $     cur.execute(sql)
from dateutil.parser import parse $ parse(segments.st_time.iloc[0])
punct_re = r'' $ trump['no_punc'] = ...
autos["price_dollars"].describe()
df.tail(10)
from skmultilearn.problem_transform import ClassifierChain $ from sklearn.naive_bayes import GaussianNB $ classifier = ClassifierChain(GaussianNB()) $ classifier.fit(X_tr[0:len(X_train)-40-1], y_ls) $ classifier.score(X_tr[0:len(X_train)-40-1], y_ls)
y = df['loan_status'].values $ y[0:5] $ y[y=="PAIDOFF"] = 1 $ y[y=="COLLECTION"] = 0 $ y = y.astype("int")
with open("data1.csv") as f: $     a = f.readline()
ttarc_clean['dog_stage'] = np.where(ttarc_clean['puppo']==1, 1, 0) $ ttarc_clean['dog_stage'] = np.where(ttarc_clean['pupper']==2, 2, ttarc_clean['dog_stage']) $ ttarc_clean['dog_stage'] = np.where(ttarc_clean['doggo']==3, 3, ttarc_clean['dog_stage']) $ ttarc_clean['dog_stage'] = np.where(ttarc_clean['floofer']==4, 4, ttarc_clean['dog_stage'])
print(samples_query.private_attributes)
subway3_df['Hourly_Entries'].max()
crime_1k=pd.read_csv('data/crime/crimes_chi.csv', nrows=1000) $ crime_1k.head()
from pyspark.ml.classification import LogisticRegression $ lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8) $
df.shape
df_never_moved = df_never_moved.drop_duplicates(subset='id')
joined = pd.merge(tweets1, Probas, left_index=True, right_index=True)
c1 = df.count() $ df = df.where(length('latitude') > 0).where(length('longitude') > 0) $ df = df.where(df.latitude > 0) $ c2 = df.count() $ print("Deleted {} rows with corrupted coordinates in latitude and longitude".format(c1-c2))
weather_mean.columns
plt.pie(total_fare, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140) $ plt.show() $
clustered_hashtags.filter(clustered_hashtags.prediction == 12).show(100)
top_5_percent.index
model = SelectFromModel(lr2, prefit=True) $ index_smote = model.get_support()
tweet_archive_enhanced_clean.info()
print(f.variables.keys()) # get all variable keys
h2o.shutdown(prompt=False)
test_df["labels"] = probs_test.detach().data.cpu().numpy()[:, 1];
cust.iloc[:, 6:]
control_converted = df2[(df2['group'] == 'control')]['converted'].mean() $ treatment_converted = df2[(df2['group'] == 'treatment')]['converted'].mean() $ diff = treatment_converted - control_converted $ p_diffs = np.asarray(p_diffs) $ (p_diffs > diff).mean() $
autos['registration_year'].value_counts().sort_index()
pd.DataFrame?
model_df.news_text = model_df.news_text.fillna('') $ model_df.tesla_tweet = model_df.tesla_tweet.fillna('') $ model_df.elon_tweet = model_df.elon_tweet.fillna('')
x = np.float32(x) $ x = x.reshape((len(x),1)) $ Y = neigh.predict(x)
dfEtiquetas["date"] = dfEtiquetas["created_time"].apply(lambda d: datetime.strptime(d, '%Y-%m-%dT%H:%M:%S+%f').strftime('%Y%m%d'))
df2_converting = df2.converted.mean() $ print('The probability of converting regardless of the page is: {}'.format(round(df2_converting, 4)))
train = train.replace(['M', 'F'], [1, 0]) $ test = test.replace(['M', 'F'], [1, 0])
injuries_hour['injuries']=injuries_hour['injuries'].fillna('0')
b_cal_q1.loc[:,'price'] = pd.to_numeric(b_cal_q1['price'], errors='coerce')
cov_df.head(7)
rfc = RandomForestClassifier(n_estimators=1000, max_depth=100, max_features=2, n_jobs=-1) $ scores = cross_val_score(rfc, X, np.ravel(y,order='C'), cv=5) $ print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
r = requests.get('http://api.open-notify.org/iss-now.json') $ r.status_code
to_be_predicted_Day3 = 50.95463678 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
import statsmodels.api as sm $ convert_old = ((df2.query('landing_page == "old_page"')).query('converted == 1')).count()[0] $ convert_new = ((df2.query('landing_page == "new_page"')).query('converted == 1')).count()[0] $ n_old = df2.query('landing_page == "old_page"').count()[0] $ n_new = df2.query('landing_page == "new_page"').count()[0]
import statsmodels.api as sm $ convert_old = df2.query('landing_page=="old_page" & converted==True').shape[0] $ convert_new = df2.query('landing_page=="new_page" & converted==True').shape[0] $ n_old = df2.query('landing_page=="old_page"').shape[0] $ n_new = df2.query('landing_page=="new_page"').shape[0]
avg_day_of_month14 = avg_day_of_month14.rename(columns={"day_of_year":"rides"}) $ avg_day_of_month14.head()
out_df[["listing_id","high","medium","low"]].head(10)
twitter_final['year'] = twitter_final['date'].apply(lambda x : x.year) $ twitter_final['month'] = twitter_final['date'].apply(lambda x : x.month) $ twitter_final['day'] = twitter_final['date'].apply(lambda x : x.day) $ twitter_final['dayofweek'] = twitter_final['date'].apply(lambda x : x.dayofweek)
dfM.head(3)
advancedmodel = LogisticRegression() $ advancedmodel = advancedmodel.fit(advancedtrain, train["rise_nextday"])
df.loc[0, 'fruits']
owns.info() $ owns.head(10)
temp_ser = pd.Series(temp) $ temp_cat = pd.Series(temp).astype('category') $ print (type(temp_cat))       # Series object $ print (type(temp_cat.cat))   # Categorical Accessor
average_trading=df['Traded Volume'].sum()/len(data)
df["created"] = pd.to_datetime(df["created"]) $ df["last_event"] = pd.to_datetime(df["last_event"]) $ df.head()
df_users_6_after.shape
set_themes.drop(['set_num', 'num_parts', 'theme_id', 'id_x', 'id_y', 'is_spare'], axis = 1, inplace = True)
print("{0} eligible first-time posts per day in r/Feminism".format(len([x for x in eligible_posts if x['previous.posts']>0]) / days_in_dataset)) $ print(eligible_posts[0]['created'])
api_copy = api_copy[pd.isnull(api_copy['retweeted_status']) == True] $ api_copy.drop('retweeted_status',axis =1 , inplace= True)
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1')['user_id'].count() $ convert_new = df.query('group == "treatment" and converted == 1')['user_id'].count() $ n_old = df2.query('group == "control"')['user_id'].count() $ n_new = df.query('group == "treatment"')['user_id'].count()
s = pd.Series([1,2,3,4,5,4]) $ s
twitter_archive_master.to_csv('twitter_archive_master.csv')
run txt2pdf.py -o"2018-06-19 2013 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2013 FLORIDA HOSPITAL Sorted by discharges.txt"
df3 = df3.drop('wk1', axis=1)
ratings.printSchema()
borough_group = data.groupby('Borough') $ borough_group.size().plot(kind='bar')
df.loc[df.Sentiment==0, ['description','Sentiment']].head(10)
clean_combined_city_df = combined_city_df.dropna(how="any") $ clean_combined_city_df.head()
old_page_converted = np.random.choice([0, 1], size=n_old, p=[1 - p_old, p_old])
def Check_Data(dataframe, number): $     return dataframe.head(number)
df["ARRIVIAL_DELAYED"].value_counts()
a = np.array([1, 2, 3, 4], dtype='float64')  #or np.array([1., 2., 3., 4.]) $ a.dtype
users_visits = pd.merge(users_visits, Relations, how='outer', on=['name', 'id_partner']) $ users_visits = users_visits[['visits', 'chanel']] $ users_visits.head()
index_weighted_cumulative_returns = calculate_cumulative_returns(index_weighted_returns) $ etf_weighted_cumulative_returns = calculate_cumulative_returns(etf_weighted_returns) $ project_helper.plot_benchmark_returns(index_weighted_cumulative_returns, etf_weighted_cumulative_returns, 'Smart Beta ETF vs Index')
dates = pd.date_range('20180101', periods=6) $ print(dates, '\n') $ data = np.random.randn(6,4) $ df = pd.DataFrame(data, index=dates, columns=['A','B','C','D']) $ print(df)
save_model('model_knn_v1.mod', knn_grid)
adjust_cols = ['StateBottleCost','StateBottleRetail','SaleDollars'] $ for col in adjust_cols: $     lq[col] = pd.to_numeric(lq[col].str.replace('$',''),errors='coerce')
treatment_converting = df2[df2['group'] == 'treatment']['converted'].mean() $ print('Probability of an individual in the treatment group converting:') $ print(treatment_converting)
from orderbookrl.evaluate.collect_data import load_env_agent, run_through_all_data $ import pandas as pd $ %matplotlib inline
pax_raw.paxcal.value_counts() / len(pax_raw)
print('The user_id that is repeated is 773192.')
new_page_converted = np.random.choice([0,1],N_new, p=(p_new,1-p_new)) $ new_page_converted 
graph = nx.DiGraph()
user_data = sc.textFile("file:/path/*") $ user_profile = user_data \ $     .map(lambda line: line.split(',')) \ $     .map(lambda words: (words[0], words[1:]))
den.plot(x='date', y='temp_c')
breakdown = df_questionable[media_classes].sum(axis=0) $ breakdown
X = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values  #.astype(float) $ X[0:5] $
df4.dropna(how = 'any')
tf_vec = CountVectorizer(max_df=0.95, min_df=2, $                          max_features=max_features, stop_words="english") $ tf = tf_vec.fit_transform(comment_sentences) $ tf_feats = tf_vec.get_feature_names()
posts_by_sampled_authors_saved.schema
df2.head()
movie2000rating=pd.merge(ratings,new_df, on='movieId', how='inner')
df_a.join(df_b, how = "outer") # outer join (see above for definition)
df_csv.visualize()
autos.columns = new_cols
USvideos.head(10)
CO_profit.sort_values(by='Profit Including Build Cost', ascending=False, inplace=True) $ TX_profit.sort_values(by='Profit Including Build Cost', ascending=False, inplace=True) $ GA_profit.sort_values(by='Profit Including Build Cost', ascending=False, inplace=True)
columns = train.columns
import statsmodels.api as sm $ lm = sm.Logit(df2['converted'], df2[['intercept','treatment']]) $ result = lm.fit()
Xs = pd.get_dummies(df.subreddit, drop_first = True)
dates=pd.date_range(start='1/Oct/2020', periods=5, freq='B') $ print(dates)
df_archive_clean.info()
clientes_path="data/clientes_sociodemografica.csv" $ transacciones_path="data/transacciones.csv"
cwd = os.getcwd() $ file_path = os.path.join(cwd, 'Data', 'feed 20180419-1133.csv') $ phox_df = pd.read_csv(file_path, sep = ',', parse_dates = [0]) $ print("Top few rows of", file_path, "look like...") $ phox_df.head()
df.head()
TrainData[['DOB_clean', 'Lead_Creation_Date_clean']].describe()
new_page_converted = np.random.choice([0,1], n_new, p=[conversion_rate_all_pages, 1-conversion_rate_all_pages])
Which_DRGs_in_each_year = The_DRGs_in_each_year(True) $ Which_Years_for_each_DRG = df_providers.groupby('drg3')['year'].unique() $
tweet = 'I know a very good Sino - Portuguese restaurant that makes food from Macau' $ print("{} :  {}".format(color_formatting(ner.is_tweet_about_country(tweet, 'PT')), tweet)) $ print("\t\t| \n\t\t|-> {}".format(ner.get_countries_from_content(tweet)) )
p_new = df2['converted'].mean() $ print("Convert rate for p_new :", p_new)
train=train.sort_values(ascending=[True],by=['air_store_id']) $ train.head()
df2.drop(repeated_user.index, inplace=True) $ len(df2)
import statsmodels.api as sm $ convert_old = df.query('group == "control" and converted == 1').shape[0] $ convert_new = df.query('group == "treatment" and converted == 1').shape[0] $ n_old = df.query('group == "control"').shape[0] $ n_new = df.query('group == "treatment"').shape[0]
closed_pr = PullRequests(github_index).is_closed()\ $                                       .since(start=start_date).until(end=end_date)\ $                                       .get_percentiles("time_to_close_days").by_period() $ print(get_timeseries(closed_pr, dataframe=True).tail())
sql("show tables").show()
ids = df.id.tolist()
md.trn_ds[0].text[:12]
month_bins = [0,6,12] $ datAll['month_rng'] = pd.cut(datAll['month'],month_bins) $ datAll['half_year'] = np.where(datAll['month_rng']=="(0, 6]","first","second") $ datAll['half_year'] = datAll['half_year'].astype(str)
test_result = [] $ for t in testing: $     test_result.append(tweet_cleaner(t))
building_pa_prc_fix_issued.loc[filt_dt,'issued_date']=building_pa_prc_fix_issued.permit_creation_date[filt_dt] $ building_pa_prc_fix_issued.head(10)
ip.info()
pd.get_dummies(tokens)
df[df.amount_initial == '$0.00'].groupby(['fund', 'appeal'])['donor_id'].count()
for df in (joined,joined_test): $     df["CompetitionOpenSince"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, $                                                      month=df.CompetitionOpenSinceMonth, day=15)) $     df["CompetitionDaysOpen"] = df.Date.subtract(df.CompetitionOpenSince).dt.days
print(train['date_created'].min()) $ print(train['date_created'].max())
temp = master_file['ROCK NAME'].value_counts() $ filter_ = r'\[(\d)*\]' $ temp.index = temp.index.astype(str).str.replace(filter_,'').str.strip().str.replace('  ',' ') $ temp = temp.groupby(temp.index).sum().sort_values(ascending=False) $ print(temp)
total_ridepercity=pd.DataFrame(ride_percity) $ total_ridepercity=total_ridepercity.reset_index() $ total_ridepercity
start_date = "2016-03-24" $ end_date = "2016-04-09" $ calc_temps(start_date,  end_date).mean(axis = 1)
from pyspark.sql.functions import col
cand_date_df.shape
condensed_xs.print_xs()
labels_np.shape
a1.head()
pumaPop = pumaPop[['B00001_001E','public use microdata area']].rename(columns={'B00001_001E':'Pop'})
df.tail()
df2[df2['landing_page']=='new_page'].shape[0]/df2.shape[0]
(p_diffs > act_diff ).mean()
zipcodesdetail = zipcodesdetail[zipcodesdetail.country == 'US'][['zip', 'primary_city', 'county', 'state', 'timezone', 'latitude', 'longitude']] $ zipcodesdetail = zipcodesdetail.rename(columns = {'zip':'zipcode', 'primary_city': 'city'})
df['time_detained'] = df['time_detained'] / np.timedelta64(1,'D')
control_df.query('converted==1').user_id.nunique()/control_df.converted.count()
tweet_archive_clean.info()
train_data.isnull().sum()
api.authorise() $ api.get_data(a=1)
count = wo_rt_df.groupby('username')['full_text'].count() $ more_100 = count[count > 100].index
trump.head()
crimes.head()
LARGE_GRID.plot_accuracy(raw_large_grid_df)
bnb[bnb['age']<100].plot(kind='hist', y='age') $
toy.shape $ toy.head()
to_be_predicted_Day5 = 26.72164129 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
age_gender_bkts = age_gender_bkts.join(cntry)
commits_per_day = git_timed.resample("D").count() $ commits_per_day.head()
backup = clean_rates.copy() $ sources = clean_rates.source.str.extract(r'>(?P<src>.*)<', expand=True) $ clean_rates.loc[:, 'source'] = sources.src.copy() $ clean_rates.source = clean_rates.source.astype('category')
joined_with_project_info = sampled_contirbutors_human_agg_by_gender_and_proj_df.join( $     project_human_cleaned_df, $     on="project").join( $     agg_post_sentiment, $     on="project")
import spacy $ nlp = spacy.load('en')
df.head()
join_c.select('party_id_orig','aggregated_prediction','predicted').show(50)
engine.build_classifier_by_key("nhtsa_classifier", # self: named in previous cell. $                                "cdescr", # subject: the text variable to be classified. $                                "injured", # classes: the target variable. $                                df.to_dict(orient='records'), # data dictionary. $                                "cmplid") # primary key: unique identifier for each complaint.
plot(fig, filename="example_plot.html")
sns.boxplot(autodf.price)
tvec = TfidfVectorizer(stop_words=stopwords.words('english'), $                                  lowercase=True,max_features=500) $ X_train_matrix = tvec.fit_transform(X_train['text']) $ X_test_matrix = tvec.transform(X_test['text'])
csv_df['uploader'].value_counts()[:12]
with open('house_db.pickle','wb') as f: $     pickle.dump(house_db,f)
dtmodel.fit(X_train, y_train) $ dtmodel_predictions = dtmodel.predict(X_test) $ num_of_dtmodel_pred = collections.Counter(dtmodel_predictions) $ num_of_dtmodel_pred
alg6 = DecisionTreeClassifier() $ alg6.fit(X_train, y_train) $ probs = alg6.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
sm.qqplot(price_z, loc=price_z.mean(), scale=price_z.std())
(p_diffs > p_diff_ab).mean()
file = 'https://assets.datacamp.com/production/course_2023/datasets/dob_job_application_filings_subset.csv' $ df = pd.read_csv(file) $ print(df.head())
data = data[(data['Latitude'].notnull()) & $             (data['Longitude'].notnull())  & $             (data['Closed Date'].notnull())]
df.head()
from sklearn.neighbors import KNeighborsClassifier $ k = 3 $ kNN_model = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train) $ kNN_model
test_df.head()
post_sentiment_df = mailing_list_posts_mbox_df_saved.select("project_name", lookup_sentiment_udf("body").alias("sentiment"))
survey.columns 
accuracy_result = model.accuracy('../../gensim_word2vec/data/questions-words.txt')
dfz=df_clean.dropna(axis=0, how='any') $
n_new,n_old = df2['landing_page'].value_counts() $ n_new
result.head()
np.histogram(s2)
merged1 = merged1.set_index('AppointmentDate')
match_results = afl_data_cleaning_v2.get_cleaned_match_results()
1 + np.nan
hp.sites[:5]
df.loc[:, 'prev_location_id'] = df.groupby( $     'episode_id').location_id.shift().fillna(0).astype('int64')
high_rev_acc_opps_net.sort_values(by='Total Buildings', ascending=False, inplace=True)
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred))
%time df = pd.read_csv("/data/311_Service_Requests_from_2010_to_Present.csv", error_bad_lines=False) $
html = open("example.html").read()
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
from spacy.matcher import Matcher
sns.barplot(x="class", y="survived", hue="sex", data=titanic)
project_human_df.show()
f_u18 = pop_df['under18'] / pop_df['total'] $ f_u18.unstack()
weather_mean.shape
pair.DATETIME[65000]
pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
to_be_predicted_Day3 = 22.24302433 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
val_utf8.decode('utf-8')
np.exp(results.params)
old_page_converted = np.random.choice([1, 0], size=nold, p=[np.mean([pnew, pold]), (1-np.mean([pnew, pold]))]).mean() $ print(old_page_converted)
tweets_df.in_reply_to_user_id.describe()
stock['predict_grow'] = stock[['forecast', 'next_day_open']].apply(lambda x: 1 if x[0] - x[1] >= 0 else 0, axis=1) $ stock['true_grow'] = stock[['target', 'next_day_open']].apply(lambda x: 1 if x[0] - x[1] >= 0 else 0, axis=1)
df['date_of_admission'] = pd.to_datetime(df['date_of_admission']) $ df_bill_data['date_of_admission'] = pd.to_datetime(df_bill_data['date_of_admission']) $ df = pd.merge(df, df_bill_data, on=['patient_id','date_of_admission']) $ print(df.shape)
noloc_df = noloc_df.append(df[(df.city.str.lower() == 'yyy') | (df.city.str.lower() == 'yyyy')]) $ df = df[~((df.city.str.lower() == 'yyy') | (df.city.str.lower() == 'yyyy'))]
siteMask = nitrodata['MonitoringLocationIdentifier'].isin(siteInfo.index) $ dfSubSites = nitrodata[siteMask] $ dfSubSites.shape
In the previous case the the difference between the convertion rate for new website and old website is very less. $ By statistics we proved that old method has better conversion rates than the new method.
sl[sl.status_binary==0][(sl.today_preds<sl.one_year_preds)].shape
data['Gender'] = data['Gender'].replace(['M'], ['Male']) $ data['Gender'] = data['Gender'].replace(['Male/Female'], ['Unknown']) $ data['Gender'] = data['Gender'].replace(['M/F'], ['Unknown']) $ data['Gender'] = data['Gender'].replace(['Unkown'], ['Unknown']) $ data['Gender'].value_counts()
df_ad_airings_5['location'][0].split(",")[1]
df2 = pd.read_csv('so2_summary.csv', names=names2, skiprows=89, parse_dates=True, index_col='Date')
df = df.dropna() $ print(len(df))
p_diffs = [] $ for _ in range(10000): $     old_page_converted = np.random.binomial(1, control_cnv, control_num) $     new_page_converted = np.random.binomial(1, treatment_cnv, treatment_num) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean()) $
from scipy.stats import ttest_rel, ttest_ind_from_stats
%run ipython_script_test.py $ c
FTE = pd.DataFrame(requirements, index = ['FTE']) $ FTE
size_c = df2.query("group=='control'").count()[0] $ size_c
series=np.reshape(data['valuea'].values,(np.shape(data['valuea'].values)[0],1))
fig, ax = plt.subplots(1, figsize=(10,3)) $ plot_linear_trend(ax, title='Doctors', series= doctors_detrended) $ plt.title('Doctors data, linearly detrended') $ plt.tight_layout()
[row["longitude"], row["latitude"]] for row in morning_rush.iloc[:5].iterrows()
df['comb'] = df['DayOfService'].map(str) + df['TripID'].map(str) $ u = df['comb'].unique()
g.isnull().sum()
df.tail(2)
for column in list1: $     election_data.loc[election_data['st'] == 'DC',column] = election_data[column].mean() $
df['hireable'] = df['hireable'].fillna('0').astype('int64')
ridgereg = Ridge(alpha=0.1, normalize=True) $ ridgereg.fit(X_train, y_train) $ y_pred = ridgereg.predict(X_test) $ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
overdue.shape
df_twitter_copy.loc[1445].text
sns.lmplot(x="labor_force_16", y="total_2016", data=election_data)
da.head()
southern_sea_level = pd.read_table("http://sealevel.colorado.edu/files/current/sl_sh.txt", $                                    sep="\s+") $ southern_sea_level
community_index=np.repeat(np.array(communities), 7)
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23trump').items(100): $     results.append(tweet)
pd.bdate_range(end=end, periods=20)
logreg_countries = sm.Logit(df_new['converted'], df_new[['UK', 'US', 'intercept']]) $ results = logreg_countries.fit() $ results.summary()
predict_actual_df=pd.concat([prediction_dataframe.iloc[:,0:1], actuals_dataframe], axis=1) $ predict_actual_df.shape
tol_num_stations = session.query(measurement).group_by(measurement.station).count() $ tol_num_stations $
suspects_with_25_2['day'] = suspects_with_25_2['timestamp'].dt.day
df_tweet.describe()
y, X = dmatrices('converted ~ ab_page + UK + US', df3, return_type='dataframe') $ vif = pd.DataFrame() $ vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] $ vif['features'] = X.columns
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt
r.index = r.group $ r.loc['ecn.negotiation.succeeded']['count'] / r['count'].sum()
Test.DOB_clean.value_counts()[1]
tweet_counts_by_month.plot(subplots=True)
df=pd.read_csv('C:\\Users\\Christopher\\Google Drive\\TailDemography\\outputFiles\\mapped-data-all_18-01-08_post_openrefine.csv')
pd.value_counts(appointments['Specialty'])
allWordExceptStopDist.most_common(10)
logit = sms.Logit(df2['converted'],df2[['intercept','treatment']]) $ results = logit.fit() $ results.summary2()
soup.select('a[href*="/item/main.nhn"], td[class^="number"]')
import pandas as pd $ names = pd.Series(data) $ names
tweet_archive_clean.dtypes
test_kyo1.describe()
old_page_converted = np.random.binomial(1, p_old,size = n_old)
for num, entity in enumerate(parsed_review.ents): $     print 'Entity {}:'.format(num + 1), entity, '-', entity.label_ $     print ''
new_bino = np.random.binomial(n_new, p_new, int(1e5)) / n_new $ old_bino = np.random.binomial(n_old, p_old, int(1e5)) / n_old $ p_diffs_3 = new_bino - old_bino
cityID =  '4ec71fc3f2579572' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $          Shreveport.append(tweet) 
df2['tripDay'].dropna(inplace= True) $ df2['tripNight'].dropna(inplace= True)
print(df['Confidence'].nunique())
df2.drop_duplicates(['user_id'],inplace=True) $ df2.shape[0]
giss_temp.plot(figsize=LARGE_FIGSIZE)
pizza_poor_train_model.print_topics(num_topics = 10 ,num_words = 10)
people_person['date'] = people_person.date_joined.dt.date $ people_person.head()
df.dropna(axis=1,inplace=True) $ df.head(3)
req = requests.get('https://api.coinmarketcap.com/v1/ticker/?limit=50') $ res = req.text $ cryptos = pd.read_json(res) $ cryptos.head() $
lm.pvalues
cooks['joined_date'] = pd.to_datetime(cooks['joined_date']) $ X['cook_joined_date'] = cooks['joined_date'] $ X['cook_days_on_platform'] = X.apply(compute_cook_days_on_platform, axis=1) $ X['couldnt_compute_cook_days_on_platform'] = X['cook_days_on_platform'].apply(lambda x: 0 if x > 0 else 1)
loans_df.home_ownership.value_counts()
station_cluster.head()
orgs['founded_on'] = pd.to_datetime(orgs['founded_on'], errors = 'coerce') $ orgs['first_funding_on'] = pd.to_datetime(orgs['first_funding_on'], errors = 'coerce') $ orgs['last_funding_on'] = pd.to_datetime(orgs['last_funding_on'], errors = 'coerce') $ orgs['closed_on'] = pd.to_datetime(orgs['closed_on'], errors = 'coerce') $ orgs['founded_year'] = orgs.founded_on.dt.year
twitter_df.head()
reddit['title'] = reddit['title'].map(lambda x: x.lower())
precipitation = session.query(measurement.date, measurement.prcp).\ $     filter(measurement.date >= '2016-08-23').\ $     order_by(measurement.date).all() $ precipitation
holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
toyotaData=autoData.filter(lambda x: "toyota" in x) $ print (toyotaData.count()) $
print(res.index.is_unique)
df2 = df.drop(df.query('group == "treatment" & landing_page != "new_page"').index) $ df2.drop(df2.query('group != "treatment" & landing_page == "new_page"').index, inplace=True)
df_new[['US','UK']] = pd.get_dummies(df_new['country'])[['US', 'UK']]                        
shows.head(10)
p = pd.Period('2014-07', freq='M')
dflunchbreak['totals_and_exits'] = dflunchbreak['lunch_totals'] + dflunchbreak['breakfast_exits'] $ dflunchbreak[['STATION','totals_and_exits']].nlargest(10,['totals_and_exits'])
vectorizer = TfidfVectorizer(sublinear_tf=True, $                                  stop_words='english',analyzer='word') $ tf = vectorizer.fit_transform(posts_df["processed_text"]) $ posts_df["tf_idf"] = [x for x in tf.toarray()]
historicFollowers.to_csv('Data/todaysFollowers_all.csv',sep=';',index=False) #save
by_time = data.groupby(data.index.time).mean() $ hourly_ticks = 4 * 60 * 60 * np.arange(6) $ by_time.plot(xticks=hourly_ticks, style=[':', '--', '-'])
from sqlalchemy import create_engine, func, inspect $ inspector = inspect(engine) $ inspector.get_table_names() $
data.groupby(['Year', 'Department'])['Salary'].sum()
first_commit_timestamp = '2005-04-16 22:20:36' $ last_commit_timestamp = '2017-10-03 12:57:00' $ corrected_log = git_log[(git_log['timestamp'] >= first_commit_timestamp) & (git_log['timestamp'] <= last_commit_timestamp)] $ corrected_log.describe() $
forest = RandomForestClassifier(max_depth=10, n_estimators=5) $ forest.fit(X_train_matrix, y_train)
churned_unordered['end_date'] = pd.to_datetime(churned_unordered_end_date).strftime('%Y-%m')
session.query(func.count(Sta.name)).all()
pd.Series(bnbA.age).isnull().any()
port_val.head()
control_conv = df2.query('group == "control"')['converted'].mean() $ control_conv
merged_data['tax'].fillna(0, inplace=True)
df2=df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == True] $ df2.head()
import pickle $ pkl_file = open('votes_by_party.pkl', 'rb') $ votes_by_party = pickle.load(pkl_file) $ pkl_file = open('speeches_features.pkl', 'rb') $ speeches_cleaned = pickle.load(pkl_file)
properati['zone'].value_counts(dropna=False)
print(abs(-12.0), '\n') $ print(len([1, 2, 3, 4, 5])) $ print(set([1, 2, 3, 4, 5, 5, 4]))
pd.PeriodIndex(['2011-1', '2011-2', '2011-3'], freq='M')
dfWordsEn['Line'] = dfWordsEn['Line'].str.lower() $ dfFirstNames['Line'] = dfFirstNames['Line'].str.lower() $ dfBlackListWords['Line'] = dfBlackListWords['Line'].str.lower()
np.mean(score)
frame = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), $                     index=['Utah', 'Ohio', 'Texas', 'Oregon'])
print('renters only use style lend one time: {}'.format(sum(total_cost['number']==1))) $ total_cost = total_cost[total_cost['number']<40] $ total_cost['number'].plot(kind='hist', logy=True, $                          title='Histogram on renting counts',yticks=[]) $ plt.xlabel('Counts')
trace1 = go.Scatter(x=df['Date'], y=df['Likes'], name='Likes') $ trace2 = go.Scatter(x=df['Date'], y=df['Re_Tweets'], name='Re-Tweets') $ data = [trace1, trace2] $ py.iplot(data, filename='2line')
((~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0])*100
df_fireworks = df[df['Complaint Type'] == 'Illegal Fireworks'] $ df_fireworks.groupby(df_fireworks.index.month)['Created Date'].count().plot(kind="bar") $
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
lmdict.tail()
ax1 = sns.scatterplot(subs, comments, hue=senti, ); $ ax1.set_ylabel('Comment Count'); $ ax1.set_xlabel('Subreddit Sub Count');
prediction_df = pd.DataFrame(predictions, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]) $ combined_df = comment_df.join(prediction_df) # join the comment dataframe with the results dataframe $ combined_df.head(15)
tweet_df.describe()
idx = pd.period_range('2014-07', periods=5, freq='M')
check_int('2017-01-17').number_repeat.max()
t2 = pd.Series(list('def'), [pd.Period('2016-09'), pd.Period('2016-10'), pd.Period('2016-11')]) $ t2
a = list(topic_list.values()) $ a.extend(usrfeat + docfeat) $ zipped = zip(a, np.ravel(coefs)) $ coef = sorted(list(zipped), key=lambda tup: abs(tup[1]), reverse=True) $ coef
bad_indices = [] $ for i in range(len(temp['c'])): $     if not isinstance(temp['c'][i], list): $         bad_indices.append(i)
lr = 4e-4 $ learn.fit(lr, 10, cycle_len=1, use_clr=(10,10))
df['Market'].head(1)
tablename='users' $ pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False).head(10)
inv.shape
raw.gender.head()
df.dtypes
print("Number of posts: {0}".format(len(all_posts))) $ earliest_date $ time_before_algorithm_change = (parser.parse("2016-12-07 00:00:00") - earliest_date) $ minutes_before_algorithm_change = time_before_algorithm_change.seconds/60 + time_before_algorithm_change.days * 1440 $ print("{0} minutes elapsed between the beginning of experiment and roughly when the algorithm changed.".format(minutes_before_algorithm_change))
flight_cancels = pd.read_csv("../clean_data/cancelled_flights_per_hour.csv")
cityID = '9531d4e3bbafc09d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oklahoma_City.append(tweet) 
items = [{'bikes': 20, 'pants': 30, 'watches': 35}, {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants': 5}] $ store_items = pd.DataFrame(items) $ store_items = pd.DataFrame(items, index=['store 1', 'store 2']) $ store_items
drawPoliGraph(G, 0.1)
df_wrong_rating = df_enhanced[df_enhanced.text.str.contains(r"(\d+\.\d*\/\d+)")] $ l = [] $ for item in  df_wrong_rating['text']: $     l.append(item.split('/')[0].split()[-1]) $ print(l)
dul1 = dul[~dul['ISBN RegEx'].isin(bad_dul_isbns)] $ dul1['ISBN RegEx'].size
closingPrices.max()
meta = parse('metadata.json.gz')
autos_df = pd.DataFrame({"price_comp":price_dict, "odom_comp":odom_dict}) $ autos_df
df.head() $ df.info()
autos.describe(include="all")
y_pred = etr.predict(X_test)
twitter_df.in_reply_to_status_id.isnull().sum()
_ = ok.grade('q01') $ _ = ok.backup()
plt.figure(figsize=(8, 5)) $ plt.hist(train_df.comments_lognorm); $ plt.title('The distribution of the property comments_lognorm');
np.histogram(noaa_data.loc["2018-05-29":"2018-05-29","AIR_TEMPERATURE"])
LSVC = SklearnClassifier(LinearSVC()) $ LSVC.train(train_set)
n_old = df2.query('landing_page == "old_page"').count()[0] $ print("Number of users with old page :",n_old)
df5[['CA', 'US']] = pd.get_dummies(df5['country'])[['CA', 'US']] $ df5.head()
customer_purchases_1to3 = df_proc1["Counter"].isin(["1","2","3"]) $ df_r = df_proc1.loc[customer_purchases_1to3,:].copy()
ssc.stop()
top=df.groupby('breed').filter(lambda x: len(x) >= 20) $ top['breed'].value_counts().plot(kind = 'bar') $ plt.title('The Most Rated Breeds');
autos["odometer"].unique()
old_page_converted = np.random.binomial(1, p_old,n_old) $ old_page_converted
rate_change.date=pd.to_datetime(rate_change.date)
df_agg_bounces_rand.head()
df_pilots.head(3)
pipe.set_params(**gs.best_params_).fit(X_train, y_train) $ print('Best features:', pipe.steps[0][1].best_idx_)
diffs = [] $ for _ in range(10000): $     new_page = df2.converted.sample(n_new, replace = True) $     old_page = df2.converted.sample(n_old, replace = True) $     diffs.append(new_page.mean() - old_page.mean()) $
loan_stats.describe()
jobPostDFSample.Duration.isnull().sum()
a=net_loans_exclude_US_outstanding[net_loans_exclude_US_outstanding.Ratenart=='J'] $ a.to_clipboard()
ng_m1 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=100, sg=1) $ ng_m2 = gensim.models.Word2Vec(train_clean_token, min_count=5, workers=2, window = 5, size=100, sg=1) $ ng_m3 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 10, size=100, sg=1) $ ng_m4 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=300, sg=1) $ ng_m5 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=300, sg=1)
df["sentences"].head()
y_pred = logreg.predict(X_train) $ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_train, Y_train)))
daily_hashtag.show(5)
df.head(20)
multi_bulk.ix['ibm us equity','best analyst recs bulk'].dropna()
df3 = df2[df2['group'] == 'control'] $ num_group = df3.user_id.count() $ num_converted = df3[df3['converted'] == 1].user_id.count() $ p_old_actual = num_converted / num_group $ p_old_actual
df_release = df_release.dropna(axis=1, how='all') $ df_release.shape
test['srch_ci'].head()
learn.fit(lr, 1, cycle_len=1)
sp['close_chg'] = ((sp.day_ago_close - sp.week_ago_close)/sp.week_ago_close) $ sp['open_chg'] = ((sp.day_ago_open - sp.week_ago_open)/sp.week_ago_open) $ sp['high_chg'] = ((sp.day_ago_high - sp.week_ago_high)/sp.week_ago_high) $ sp['low_chg'] = ((sp.day_ago_low - sp.week_ago_low)/sp.week_ago_low) $ sp['vol_chg'] = ((sp.day_ago_vol - sp.week_ago_vol)/sp.week_ago_vol)
poverty_2011_2015.reset_index(inplace=True)
pred_CSCO=model_CSCO.predict()
temp_df.head()
transactions.div(customers, fill_value=0)
plt.hist(p_diffs) $ plt.xlabel('p_diff') $ plt.ylabel('Frequency') $ plt.title('10,000 simulated p_diffs');
df.instagrams = ins $ df
print(X_train.shape) $ print(X_valid.shape) $ print(X_test.shape)
endDate = datetime.strptime("2017-07-08 00:00:00.000000", '%Y-%m-%d %H:%M:%S.%f') $ firstWeekUserMerged = userMerged[userMerged['time_stamp2'] < endDate] $ print("the most recent activity is :",max(firstWeekUserMerged.time_stamp2)) $ firstWeekUserMerged.shape $
top_10_authors = git_log.groupby("author").count().sort_values(by="timestamp", ascending=False).head(10) $ top_10_authors
holidays_events.dtypes
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
mypath = '/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data/unsupervised' $ clustering_df.to_csv(mypath + '/clustering_data.csv') $ comps_df.to_csv(mypath + '/competitors.csv')
mean_w = np.mean(hm_data.weight) $ std_w = np.std(hm_data.weight) $ print 'Mean weight: {}'.format(mean_w) $ print 'StDev weight: {}'.format(std_w)
dates['date'] = pd.to_datetime(dates['date'])
total_users_treat = df2.query('group == "treatment"')['user_id'].nunique() $ user_treat_conv = df2.query('group == "treatment" & converted == 1')['user_id'].nunique() / total_users_treat $ user_treat_conv
datetime.now().toordinal() - datetime(1987, 1, 7).toordinal()
df['imm_hold'] = 0 $ df.loc[hold_mask, 'imm_hold'] = 1
df_new['US_ab_page'] = df_new['ab_page'] * df_new['US'] $ df_new['UK_ab_page'] = df_new['ab_page'] * df_new['UK'] $ model3 = sm.Logit(df_new['converted'], df_new[['intercept', 'US_ab_page', 'UK_ab_page','ab_page', 'UK', 'US']]) $ results3 = model3.fit()
temps_diffs.mean()
%time preds = np.stack([t.predict(df) for t in m.estimators_]) $ np.mean(preds[:,0]), np.std(preds[:,0])
d.strftime("%Y-%m-%d %H:%M:%S.%f")
merged.corr()
rainbow_groups = plot_df.groupby('WEEK_OF_YEAR', as_index=True)
X = sales_2015[['store_number','sales_jan_mar', 'volume_sold_lt', 'net_profit', $                 'state_bottle_cost', 'state_bottle_retail', 'bottles_sold']] $ X.head()
df_con_treat = df_con1.query('group =="treatment"') $ x_treat = df_con_treat["user_id"].count() $ x_treat $
m3 = np.matmul(m2.T,m) $ print("m3: ", m3)
print("Total proportion of Converted users :: {}%".format((df['converted'].mean())*100))
df.info()
b = np.load('myfile.npy')  #Load the data into variable b. $ os.remove('myfile.npy')  #Clean up.
outcomes = [0,1] ### 0 is not converted and 1 is converted $ probs = [1-c_rate_null,c_rate_null] ### probability of 0 is one minus conversion rate at null, and probability of 1 is c_rate_null $ new_page_converted = np.random.choice(outcomes, size= n_new,replace = True, p = probs) $ new_page_converted
p=df_usa['Population'].values $ a = df_usa['Area'].values $ df_usa['PD2']=(p*1000)/a $ df_usa.round(decimals=2)
d={'c1':pd.Series(['A','B','C']),'c2':pd.Series(np.random.randint(0,4,4))} $ pd.DataFrame(d)
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Base.classes.keys()
hour_of_day14.to_excel(writer, index=True, sheet_name="2014")
df[df.index.month.isin([12,1,2])].head()
print 6000/5000.
furniture = data.loc[(data['hired']==1)]
result['portalId'].nunique()
test['visitors']=model_avg $ test['visitors'] =(test['visitors']).clip(lower=0.)
ab_df2.user_id.nunique()
corr=sess.get_historical_data('index:indu','px last', start_date='2014-01-01', end_date='2015-01-01', $                               format=bp.data_frame('date', 'Security')).pct_change().corr() $ corr.ix[0:5,0:5]
jobs.loc[(jobs.FAIRSHARE == 64) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0) & (jobs.Group == 'tplab')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
data.isnull().sum()
parser.HHParser.keys
print sentiDf['2015-12-03' == sentiDf.index].mean()
import seaborn as sns $ %matplotlib inline $ sns.countplot(data.OutcomeType, palette='Set3')
df['MeanFlow_cfs'].plot();
df['three'] = df['one'] * df['two'] $ del df['two'] $ df.insert(1, 'bar', df['one'][:2]) $ df
news_df.to_csv('../data/news.csv', index=False)
helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)
prediction_proba = grid.predict_proba(X_test) $ prediction_proba = [p[1] for p in prediction_proba] $ print(roc_auc_score(y_test, prediction_proba))
tlen.plot(figsize=(16,4), color='r');
zc.head()
from pandas_datareader.famafrench import get_available_datasets $ import pandas_datareader.data as web $ len(get_available_datasets()) $ ds = web.DataReader("5_Industry_Portfolios", "famafrench") $ print(ds['DESCR'])
data = data.withColumnRenamed('Day of Week','DAY_OF_WEEK') $ data = data.withColumnRenamed('Time of Day','TIME_OF_DAY') $ data = data.withColumnRenamed('Time of Day Band','TIME_OF_DAY_BAND')
bars.close.at_time('16:00')
per_var = np.round(pca.explained_variance_ratio_*100, decimals=1) $ labels = ['PC' + str(num) for num in range(1, len(per_var) + 1)]
workspace_ids = [] $ for i in workspaces_list: $     workspace_ids.append(i['id']) $
print(data.dtypes)
sub_gene_df['type'].value_counts()
import pickle $ pkl_file = open('speeches_metadata_evidence.pkl', 'rb') $ speeches_metadata = pickle.load(pkl_file)
MergeWeek = MergeWeek.groupby(['Date'])[['Contract Value', 'Pipeline Contract Value', 'Pipeline Weighted Value']].sum().reset_index()
for i in dir(pandas): $     if i.startswith("read"): $         print i
npath = '/glade/u/home/ydchoi/sopron_2018/notebooks/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb' $ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
new_df.plot(x='index', y='OT Temp')
stringlike_instance.content = 'changed content'
import seaborn as sns $ corr = df_cont.corr().abs() $ sns.heatmap(corr, xticklabels=False, yticklabels=False)
Conversion_No=df2.loc[df2['landing_page']=="new_page",].shape[0] $ print("The no of observations regarding the new page equals",Conversion_No)
top_tracks.to_csv("top_200_table.csv",sep="\t")
df_all.to_csv('./data/df_all_no_sess.csv', index=False) $
mar_file = os.path.join(DATA_DIR, "addresses.xlsx")
pd.Timestamp('2015.11.30')
tweet_lang_hist = pd.crosstab(index=tweets_df["tweet_lang"], columns="count") $ tweet_lang_hist['lang_freq'] = tweet_lang_hist['count'] * 100 / tweet_lang_hist.sum()['count'] $ tweet_lang_hist = tweet_lang_hist.sort_values('lang_freq', ascending=False) $ tweet_lang_hist.head(10)
data['created'] = pd.to_datetime(data['created'], infer_datetime_format=True)
census_pd_complete.loc[:'Population' ,:'Poverty' ,:'Rate for Housing Units']
data.recommendation_id.nunique()
old_page_converted = np.random.choice([1,0], size=OldPage, p=[0.12, 0.88]) $ old_page_converted
len(free_data.country.unique())
prob_newpage = df2[df2.landing_page=='new_page'].shape[0]/df2.shape[0] $ print('Probability that an individual recieved new page: ' + str(prob_newpage))
wsj = sorted(set(nltk.corpus.treebank.words())) $ fd = nltk.FreqDist([vs for word in wsj $            for vs in re.findall(r'[aeiou]{2,}', word.lower())]) $ fd.most_common(12)
df_freq_users.user_location_latitude.isnull().sum()
df[df['landing_page'] == 'new_page'].landing_page.count()
liquor['State Bottle Retail'] = [s.replace("$","") for s in liquor['State Bottle Retail']] $ liquor['State Bottle Retail'] = [float(x) for x in liquor['State Bottle Retail']] $ liquor_state_retail = liquor['State Bottle Retail']
notus.loc[notus['country'].isin(canada), 'country'] = 'Canada' $ notus.loc[notus['country'].str.contains('Canada', case=False) == True, 'country'] = 'Canada' $ notus.loc[(notus['cityOrState'] == 'Canada') & (notus['country'] == 'Canada'), 'cityOrState'] = np.nan
gb = GradientBoostingClassifier(n_estimators=100, min_samples_leaf = 25, max_features = 0.7) $ gb.fit(train_data_features, y_train) $ y_fit_proba = gb.predict_proba(test_data_features) $ report_score(y_test, y_fit_proba, 0.2)
new_page_converted = np.random.binomial(1, conv_mean, 145310)
data['SA'] = np.array(x)
tweets_df.place.describe()
mi_index=pd.MultiIndex.from_arrays(arraylist) $ print(mi_index)
databreach_2017.to_csv('LAB3_ticker.csv')
investors_df.groupby('investor_type').size().plot(kind = 'bar',figsize = (20,8))
g = sns.FacetGrid(data=motion_at_home_df, row="is_weekday",  hue="time_category", aspect=3, size=3) $ g = (g.map(plt.scatter, "time", "state", edgecolor="w").add_legend())
df=pd.read_csv('/Users/haninjaber/Documents/SBData/dogdata.csv')
trip_data["hours"] = trip_data.lpep_pickup_datetime.apply(lambda x: pd.to_datetime(x, infer_datetime_format=False).hour)
data2['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data2['Tweets'] ]) $ display(data2.head(10))
daily.asfreq('H',method='bfill')
stock['daily_gain'] = stock.close - stock.open $ stock['daily_change'] = stock.daily_gain / stock.open
type(pd.to_timedelta(np.arange(12), 'D'))
promo_df['onpromotion']=promo_df['onpromotion'].astype(np.int8)
data_df.cycles.unique()
ratings['rating_numerator'] = ratings['rating_numerator'] / ratings['rating_denominator'] * 10 $ ratings['rating_denominator'] = 10 $ ratings = ratings.round({'rating_numerator': 2})
connections.to_sql(con=engine, name='systemconnections', if_exists='replace', flavor='mysql')
pd.set_option('display.max_colwidth', 150) $ clinton_df.head()
pos_bow = [pos_dic.doc2bow(review) for review in pos_review_tokens] $ neg_bow = [neg_dic.doc2bow(review) for review in neg_review_tokens] $ pos_tokens = pd.DataFrame(list(pos_dic.token2id.items()), columns=['token','id']) $ pos_dt = pd.DataFrame(pos_bow[0], columns=['id','freq']) $ pos_dt.merge(pos_tokens)
from sklearn.neighbors import KNeighborsClassifier $ knn = KNeighborsClassifier(n_neighbors=1)
df = pd.DataFrame(results, columns=['observ_time','observ_prcp']) $ df['observ_time']=pd.DataFrame(df['observ_time']) $
autos["num_pictures"].value_counts()
df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2['intercept'] = 1 $ df2.head()
print("Mean absolute percentage error: %.3f" % mean_absolute_percentage_error(y_test, y_pred) + '%')
df2 = pd.read_csv('../output/data/expr_2/expr_2_pcs_nth_1_div_51_04-18.csv', comment='#')
def find_dot(x): $     if '.' in str(x): $         return True $     else: $         return False
!mkdir ../data/imsa-cbf
n_top_words = 10 $ for i, topic_dist in enumerate(topic_word): $     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words):-1] $     print('Topic {}: {}'.format(i, ' '.join(topic_words)))
DatePrcp = session.query(Measurement.hw_date, Measurement.prcp).filter(Measurement.hw_date >= lastYear).all()
tt1_ok = tt1.loc[tt1_ok_indices]
building_pa.sample(5)
if 0 == go_no_go: $     all_top_vecs = [lda.get_document_topics(serial_corp[n], minimum_probability=0) \ $                     for n in range(len(serial_corp))]
def printer(date_time): $     return date_time.weekday()
print('Best features:', gs.best_estimator_.steps[0][1].k_feature_idx_)
All_tweet_data_v2.rating_denominator.value_counts()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv" $ df  = pd.read_csv(path, sep =',') # df  = pd.read_csv(path) may be too. sep =',' is by deffect. $ df.head(5)
reddit['Time'] = reddit['Time'].dt.tz_localize('UTC').dt.tz_convert('US/Eastern') 
abstract = df.at[3,'abstract'] $ latex_regex = latex() $ whitespace_regex = re.compile('[:;.,?\s]+') $ process_abstract(abstract, latex_regex, latex_repl, whitespace_regex)
def get_rental_concession_vec(dSeries): $     return pd.DataFrame({k:dSeries.str.contains(p) for k, p in PATTERNS2.items()}) $
unique_domains.sort_values('total_votes', ascending=False).head()
w.get_step_object(step = 2, subset = subset_uuid).load_indicator_settings_filters()
plt.style.use('fivethirtyeight') $ logregRocPlot,= plt.plot(logregFpr,logregTpr, label='3 Feat. Log. Reg. Model') $ plt.xlabel('False Positive Rate', fontsize= 13) $ plt.ylabel('True Positive Rate', fontsize= 13) $ plt.title('ROC Curve for Logistic Regression', size= 15, fontweight= 'bold') $
print("{} games in January of 2016".format(len(nba_df.loc[(nba_df.index.year == 2016) & (nba_df.index.month == 1), ]))) $ print("{} games in January of 2017".format(len(nba_df.loc[(nba_df.index.year == 2017) & (nba_df.index.month == 1), ])))
def false_negative(predicted_pivot, test): $     predicted_table = pd.DataFrame(predicted_pivot.unstack()).groupby(['CUSTOMER_ID','MATNR']).sum().reset_index() $     predicted_item = predicted_table[predicted_table[0]==1] $     upper = pd.merge(test, predicted_item, how='left', on=['CUSTOMER_ID','MATNR']) $     return sum(np.isnan(upper[0]))
fig = plt.figure() $ ax = fig.add_subplot(111) $ ax.scatter(user1.TowerLon,user1.TowerLat, c='g', marker='o', alpha=0.2) $ ax.set_title('Weekend Calls (<6am or >10p)') $ plt.show()
vectorizer2 = TfidfVectorizer(use_idf=True, ngram_range=(3,4))  $ transformed2 = vectorizer2.fit_transform(cleaned_texts.values) $ features2 = vectorizer2.get_feature_names()
BDAY_PAIR_df.head()
visited.head()
twitter_df_clean.groupby('rating_numerator')['rating_numerator'].count().plot()
data.isna().sum()
for i, image in enumerate([img1, img2]): $     plt.subplot(2, 2, i + 1) $     plt.axis('off') $     plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
msft.loc['2012-01':'2012-01-05'] #slicing till particular date
df_course_association.head()
p_undernull = df2.converted.mean()
b = a[:, 0]; b #Entire first column. Note that this yields a 1-dimensional array (vector), not a matrix with one column. 
duplicate_events_df = pd.DataFrame(duplicate_check_df['event_id'].value_counts()) # count values for events $ duplicate_events_df.reset_index(inplace=True)  # abstratc the event_id to be a column $ duplicate_events_df.columns = ['event_id', 'frequency']  # rename columns with more clear names $ duplicate_events_df[duplicate_events_df['frequency'] > 1] # filter the duplicate events and show the results
fruits = pd.Series([10, 6, 3], ['apples', 'oranges','bananas']) $ fruits
day_of_year15["avg_day_of_month"] = day_of_year15.index.map(lambda x: int(x.split("-")[2])) $ day_of_year15.head()
autos_p['registration_year'].describe()
s4 = pd.Series(weights, index = ["colin", "alice"]) $ s4
get_freq(series_obj=raw.sex_age)
locations = session.query(Measurement).group_by(Measurement.station).count() $ locations
df2 = gdf_gnis[(gdf_gnis['FEATURE_ID'] >= 2300000)] $ df2.rename(columns={'_id': 'identifier'}, inplace=True)
counts = pd.Series([632, 1638, 569, 115]) $ counts
dummy_Contract = pd.get_dummies(df['Contract'], prefix='Contract') $ print(dummy_Contract.head())
n_new = len(df2.query("landing_page == 'new_page'")) $ print('N_new is {}'.format(n_new))
c['landmark'] = c['landmark'].fillna('') $ c['landmark'] = c['landmark'].str.replace("\s+\d+\s+Z\d", '') $ c['landmark'].value_counts().head(10)
tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce') $ tips['tip'] = pd.to_numeric(tips['tip'], errors = 'coerce') $ print(tips.info()) $
pd.Series({'a' : 0., 'c' : 1., 'b' : 2.})  # from Python dict, autosort by default key
df_hour = df.copy()
for col in ['Partner','Dependents','PhoneService','PaperlessBilling','Churn']: $      df_raw[col].replace({'Yes': 1, 'No': 0}, inplace=True)
stock = data2.Close
errors['errorID'].value_counts().plot(kind='bar') $ plt.ylabel('Number of Errors') $ plt.show()
job_requirements = pd.DataFrame(requirements) $ job_requirements $
sets_node['date'][DATA].head()
df2 = df2.drop_duplicates("user_id") $ df2.query('user_id == @repeated_user')
datetime.strptime('09/Aug/1995:09:22:01 -0400', DATETIME_PARSE_PATTERN).weekday()
df2.drop_duplicates(inplace = True)
mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = mod.fit()
scr_churned_ix = SCR_PLANS_df[scr_churned_bool].index
cs.describe()
df2.query('group == "control"').converted.sum() / df2.query('group == "control"').shape[0]
pd.crosstab(df['k'], df['author'])
print("Number of Groups in Mobile ATT&CK") $ groups = lift.get_all_mobile_groups() $ print(len(groups)) $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
def strip_singlequote(text): $     try: $         return text.strip('\'') $     except AttributeError: $         return text
df.head(1)
from netCDF4 import num2date, date2num, date2index $ timedim = sfctmp.dimensions[0] # time dim name $ print('name of time dimension = %s' % timedim) $ times = gfs.variables[timedim] # time coord var $ print(times)
complete_techniques = lift.get_all_techniques_with_mitigations()
plot_df = pd.DataFrame.from_dict({'train_loss':history.history['loss'], 'val_loss':history.history['val_loss']}) $ plot_df.plot(logy=True, figsize=(10,10), fontsize=12) $ plt.xlabel('epoch', fontsize=12) $ plt.ylabel('loss', fontsize=12) $ plt.show()
users[users.age < 18].age = np.nan
print('Proportion of users converted :: ',df['converted'].mean())
import pandas as pd $ df = pd.read_csv('./data/collapsed_go.no_IGI.propagated.term_sizes', sep='\t', names=['id', 'genes']) $ df.head(10)
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US', 'UK']]) $ results = logit_mod.fit() $ results.summary()
r.xs('AMD', level=1)[['weight']].plot()
merged_df = pd.merge(left=surveys_df, right=species_df, how='left', on='species_id')
train.bathrooms.describe()
df.sort_values(by="grade")
temperature_df = pd.DataFrame(temperature, columns=['date', 'tobs']) $ temperature_df.set_index('date', inplace=True) $ temperature_df.head()
interactionsData.groupby(['interaction_type']).size()
print(autos['price'].unique().shape) $ print(autos['price'].describe())
DataAPI.write.update_indicators(["HIGH", "LOW", "IPO_LISTDAYS"], trading_days, override=False, log=True)
apple.groupby(pd.TimeGrouper(freq = 'M')).agg(np.mean).index
time_window["screen_name"].value_counts().head(25)
t = 0.25 $ c_y = 0.6607/(1-t) $ print(c_y)
result = logit.fit() $ result.summary() 
appleInitialNegs.shape
srp_url = 'https://pythonprogramming.net/parsememcparseface/' $ src = urllib.request.urlopen(srp_url).read() $ soup = bs.BeautifulSoup(src,'lxml') $ for para in soup.find_all('p'): $     print(para.text) $
X_test.iloc[0:5,:12]
my_zip = zipfile.ZipFile(dest_path, 'r') $ with my_zip.open("old_trump_tweets.json", "r") as f: $     old_trump_tweets = json.load(f)
k1 = data[['cust_id','lane_number','total_spend']].groupby(['cust_id','lane_number']).agg('mean').reset_index() $ k1 = k1.pivot(index='cust_id', columns='lane_number', values='total_spend').reset_index().replace(np.nan,0) $ train = train.merge(k1,on=['cust_id'],how='left') $ test = test.merge(k1,on=['cust_id'],how='left')
rodelar_pages = pagegenerators.CategorizedPageGenerator(category)
first_row=session.query(meas).first() $ first_row.__dict__
taxiData.columns
archive_df_clean.head(10)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ (z_score, p_value)
optimizer = tf.train.AdagradOptimizer(0.01 ) $ train = optimizer.minimize(loss) $
sst.shape
mean_weekday = daily_averages.groupby('Weekday').mean()
df=pd.get_dummies(df,columns=['signup_app','affiliate_channel','affiliate_provider','first_affiliate_tracked','first_browser','first_device_type','signup_method','language'],drop_first=True)
pp_new = new_page_converted.mean() $ pp_old = old_page_converted.mean() $ p_new_old = pp_new - pp_old $ p_new_old
articles['created_at'] = articles.apply((lambda row: dateparser.parse(row.date[0])), axis=1)
%%time $ del extract_deduped_with_elms_v2['LOAN_AMOUNT_'] $ extract_deduped_with_elms_v2.to_csv(cwd + '\\ELMS-DE backup\\extract_deduped_with_elms_0611.csv', index=False)
results = model.transform(test) $ results=results.select(results["ID"],results["CHURN"],results["label"],results["predictedLabel"],results["prediction"],results["probability"]) $ results.toPandas().head(6)
sns.distplot(titanic.fare)
loans_plan_origpd_xirr=cashflows_plan_origpd_all.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
h.load_session(36)
viz = cdf[['CYLINDERS','ENGINESIZE','CO2EMISSIONS','FUELCONSUMPTION_COMB']] $ viz.hist() $ plt.show()
cars.info() $
subway5 = subway3_df.groupby(['STATION','LINENAME','datetime'])[['Hourly_Entries','Hourly_Exits']].agg('mean')
new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))
is_duplicate = [] $ qbfile1 = open("/home/midhu/DataScience/nlp/keras-quora-question-pairs/label_test_infost.txt","r") $ for aline in qbfile1.readlines(): $     line_ = aline.split("\n") $     is_duplicate.append(int(line_[0]))
fish.stack()
Google_stock.describe()
INT.loc[:,'YEAR'] =INT['Create_Date'].apply(lambda x: "%d" % (x.year))
df_image.sample(5)
actual_payments_combined[actual_payments_combined.residual_principal_amount_borrower.isnull()].head().T
dummies = pd.get_dummies(df2['group']) $ dummies.head()
classification_ex = (train['Percent Change'] > 0).astype(int)
times = pd.date_range('2017-09-10 13:00:00', periods=25, freq='60min') $ days = pd.date_range('2017-09-01', periods=40, freq='1D') $ plt.figure(1, figsize = (20,7)) $ plt.plot(times, filteredTweets.groupby(['DateTime']).mean()['Has_IRMA'])
import numpy as np $ sources = data_set["source"].value_counts()[:5][::-1] $ plt.barh(range(len(sources)), sources.values) $ plt.yticks(np.arange(len(sources)) + 0.4, sources.index) $ plt.show()
for i in loading.print_topics(num_words=8): $     for j in i: print(j)
data.drop_duplicates().shape
excelDF = pd.read_excel("C://SuperStore.xls", sheet_name = "Orders")
df_trump_device_non_retweets.created_at.max()
cur = conn.cursor() $ cursor=conn.cursor()
ax = sns.barplot(x=df['state'].value_counts(), y=df['state'].value_counts().index)
dcrime.head()
import statsmodels.api as sm $ log_model = sm.Logit(df3['converted'], df3[['intercept','abs_page']]) $ results = log_model.fit()
from sklearn.linear_model import LogisticRegression $ model = LogisticRegression() $ model = model.fit(X, y) $ model.score(X, y) 
df_sched = pd.read_csv(schedFile, usecols = schedCols, $                  dtype = sched_dtypes)
old = df2['landing_page'] $ old_converted_null = df2.query('converted == 1') $ p_old = old_converted_null.count()[0]/old.count() $ p_old
print('R_in_fs',1/fs_df.iloc[0,:][1]) $ print('tau-fs',fs_df.iloc[0,:][0]/fs_df.iloc[0,:][1])
tweets_prediction.info()
df_ab_raw['line_up'].sum()
predictions_clean.sample(10)
eia_gen_annual.head()
sum_of_values_for_every_wednesday = s[date_time_index.weekday == 2].sum() #Calculating the sum of values in s for every Wednesday $ print(sum_of_values_for_every_wednesday) #Printing the sum
df_wm['blobw'] = df_wm['cleaned_text'].apply(TextBlob)
df2=df.drop(index=x,axis=0)
ab_data.head()
df1 = df.drop(columns = ['Area Id', 'Variable Id', 'Symbol'])   #droping the columns
df.index.values   # underlying values are numpy.ndarray
sdf.head()
descrip_ct =  pd.crosstab(combined_df5['vo_propdescrip'],combined_df5['bin_label']) $ descrip_ct['neg_pctg']=descrip_ct[1]/(descrip_ct[0]+descrip_ct[1])*100 $ descrip_ct.sort_values('neg_pctg',ascending=False)
s4g = combined[['Symbol', 'Adj Close']].reset_index() $ s4g.insert(1,'Year',pd.DatetimeIndex(s4g['Date']).year) $ s4g.insert(2, 'Month',pd.DatetimeIndex(s4g['Date']).month) $ s4g[:5]
autos['price'].describe()
users = pd.read_csv('data/new_subset_data/new_subset_users.csv', sep='\t') $ users.info() $ users.head()
df_goog.describe() $ df_goog.dtypes $ df_goog.plot() $ df_goog[['Open','Close','High','Low','Adj Close']].plot()
s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])
df_joined['intercept'] = 1 $ df_joined[['US','UK']] = pd.get_dummies(df_joined['country'])[['US','UK']]
sn.distplot(train_binary_dummy['unique_action'])
m3 =np.around(m3,2) $ print("m3: ", m3)
pres_df['metro_area'].unique()
authors = Query(git_index).get_cardinality("author_name").by_period() $ print(get_timeseries(authors, dataframe=True).tail())
from pixiedust.display import * $ display(df)
convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'")) $ print(convert_old, convert_new, n_old, n_new)
Bool_vars = ['trafficSource.adwordsClickInfo.isVideoAd','trafficSource.isTrueDirect' ] $ df_train['trafficSource.adwordsClickInfo.isVideoAd'] = df_train['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True) $ df_test['trafficSource.adwordsClickInfo.isVideoAd'] = df_test['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True) $ df_train['trafficSource.isTrueDirect'] = df_train['trafficSource.isTrueDirect'].fillna(False) $ df_test['trafficSource.isTrueDirect'] = df_test['trafficSource.isTrueDirect'].fillna(False)
print(serc_pixel_df.head(5)) $ print(serc_pixel_df.tail(5))
s2 = s.copy() $ s2.reindex(['a', 'f'], fill_value=0)
a_set.difference(another_set)
bloomfield_pothole_data.index = bloomfield_pothole_data['CREATED_ON'] $ bloomfield_pothole_data.info()
tfidf.idf_[75]
X = pd.get_dummies(df[['month', 'day', 'gap_open_pct', $                        'dollar_change_open', 'offer_price', 'open_price', $                        'dollar_chg_opencls']], drop_first = True)
contractor_clean[contractor_clean.city.isnull()]
cnf_matrix = confusion_matrix(y_test, yhat_lr, labels=['PAIDOFF','COLLECTION']) $ np.set_printoptions(precision=2) $ print (classification_report(y_test, yhat_lr)) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')
twitter_ar = twitter_ar[pd.isnull(twitter_ar['retweeted_status_id'])] $ twitter_ar = twitter_ar[pd.isnull(twitter_ar['in_reply_to_status_id'])] $ twitter_ar = twitter_ar[pd.isnull(twitter_ar['in_reply_to_user_id'])] $ twitter_ar = twitter_ar[pd.isnull(twitter_ar['retweeted_status_user_id'])] 
df['ViewCount'][df['ViewCount']>100000].count()
from IPython.display import Image $ Image("pvalue.png")
for v in squares:  # calls .__iter__() $     print(v)
gram_collection.find_one({"account": "deluxetattoochicago"})['date_added'] $
multi.handle.value_counts() / multi.shape[0]
from tensorflow.python.client import device_lib $ device_lib.list_local_devices()
fd_dist = [vs for word in wsj $            for vs in re.findall(r'[aeiou][aeiou]', word.lower())] $ fd = nltk.ConditionalFreqDist(fd_dist) $ fd = pd.DataFrame(fd).fillna(value=0).astype(dtype=int) $ fd
pa_max_date = prcp_analysis_df["date"].max().date() $ pa_today = dt.date.today() $ pa_min_date = (pa_max_date - dt.timedelta(days=365)) $ print("Date Range: "+str(pa_min_date)+" to "+str(pa_max_date))
url=("/Users/maggiewest/Projects/Portland_census.csv") $ portland_census = pd.read_csv(url) $ portland_census2=portland_census.drop("Fact Note", axis=1) $ portland_census2=portland_census2.drop("Value Note for Portland city, Oregon", axis=1)
columns = ['day_period', 'weekday', 'category', 'is_self', 'is_video'] $ le = LabelEncoder() $ model_df[columns] = model_df[columns].apply(lambda x: le.fit_transform(x))
lines = sc.textFile("data.txt") $ lineLengths = lines.map(lambda s: len(s)) $ totalLength = lineLengths.reduce(lambda a, b: a + b) $ print(totalLength) $ lineLengths.persist()
y.to_frame().sample(n=5, random_state=10)
res = requests.get("https://steamcommunity.com/market/search?appid=570&key=36C87A2958CA864AF19E1CBA635CA32D&format=json&q=%D0%B0%D0%B2%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D1%84")
gen1 = dta.t[(dta.b==1) & (dta.c==1)]
gmap.heatmap(data['Latitude'], data['Longitude'])
df2.query('user_id == 773192').index.get_values()
df_pivot.head(5)
top_songs['Country'].unique()
ticks.shift(1).head()
table1.dtypes
df['loan_status']= df['loan_status'].apply(lambda x: 0 if (x == "PAIDOFF")  else 1) $ y = df['loan_status'].values $ y[0:5]
df.nunique() #identifying the count of unique values in each column
stocks_pca_m3=stocks_pca_m3[~np.isnan(stocks_pca_m3).any(axis=1)]
plt.title('Burberry ngram', fontsize=18) $ burberry_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/Burberry_ngram.jpg')
a_b page is not statistically dependent. So it is difficult to predict the values. and R squared is 0 . It is not legitimate to predict using these two values.
newdf.describe()
tweets.head()
df['Datetime'].min()
reg_mod_us = sm.OLS(df_all['converted'], df_all[['US_int', 'ab_page']]) $ analysis_us = reg_mod_us.fit() $ analysis_us.summary()
all_test_times_dates = pd.concat([go_no_go_times, simp_rxn_time_times, proc_rxn_time_times, $                    go_no_go_date, simp_rxn_time_date, proc_rxn_times_date], axis=1) $ all_test_times_dates.columns = ['go_no_go_time', 'simp_time', 'proc_time', $                                'go_no_go_date', 'simp_date', 'proc_date']
sns.pairplot(df, x_vars=['Bottles Sold','Volume Sold (Liters)'], y_vars='Sale (Dollars)', size=7, aspect=0.7)
tweet_th[tweet_th['text'].apply(lambda x: "I'm at" in x)]['text']
plt.plot(y_age_test, y_pred_test, 'bo')
df_main.p2.head(3)
f.query("group =='treatment' and landing_page == 'old_page' or group == 'control' and landing_page=='new_page'").shape[0]
dfOld= dfOther[dfOther['Name'].apply(lambda x: x.split(' ')[0])=="HOMESENSE"] $ dfOld.head()
autos = autos[autos["registration_year"].between(1900,2018)]
df['site_admin'] = df['site_admin'].astype('int64')
dinw_filter_set = w.get_step_object(step = 2, subset = subset_uuid).get_indicator_ref_settings('din_winter') $ dinw_filter_set.allowed_variables
plt.figure(figsize = (7,7)) $ plt.xlabel('Size') $ plt.ylabel('Eat Breakfast') $ plt.scatter(df["size"],df["age"],c="g", cmap='rainbow')  $
stock['volatility'] = stock.high - stock.low
full_data.dtypes
before.head()
concerns = ['sexual','abuse','kill','murder','suicide', 'rape'] $ medical = wk_output[wk_output.explain.str.contains('|'.join(concerns))] $ medical.shape $ medical.to_csv('medical_feedback.csv')
os.chdir("E:/Data/Client1/Excel Files") $ path = os.getcwd() $ files = os.listdir(path)
SampleIndex = StockData.loc[StockData.Set == 'train'].index.values.copy()  # Make a copy to avoid modifying StockData.index $ NumRows = len(SampleIndex) $ NumTest = int(TestRatio * NumRows) $ NumTrain = NumRows - NumTest $ print("We have {:,} rows, we will use {:,} of them for training, {:,} for testing.".format(NumRows, NumTrain, NumTest))
train = df[df.index < '2017-01-01'] $ test = df[df.index > '2016-12-31']
train_df1.head(1)
df2 = df.dropna(subset = ['one']) $ df2
model_guid = client.repository.get_model_uid(saved_model_details) $ print("Saved model guid: " + model_guid)
test = df_main.index == 24 $ df_main[test]
spencer_bday_time.strftime("Spencer was born on %A, %B %dth at %I:%M %p")
inspector = inspect(engine) $ columns = inspector.get_columns('stations') $ for c in columns: $     print(c['name'], c["type"]) $
rng = pd.date_range(start, end)
predictions = rfr.predict(test[['expenses', 'floor', 'lat', 'lon', 'property_type',\ $                                 'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']])
final_topbikes['Timestamp index'] = final_topbikes['Timestamp +2'].apply(pmam) $ final_topbikes['Timestamp index'] = final_topbikes['Timestamp index'].apply(lambda x: $                                     dt.datetime.strptime(x,'%d/%m/%Y %H:%M %p')) $ final_topbikes.index = final_topbikes['Timestamp index']
new_page_converted = np.random.binomial(n=n_new, p=p_null) / n_new $ new_page_converted
autos['unrepaired_damage'].value_counts()
test_path = '../../Data/test_users.csv' $ test_users = pd.read_csv(test_path) $ print(len(test_users)) $ test_users.head()
def pretty_print(data, indent=4): $     if type(data) == dict: $         print(json.dumps(data, indent=indent, sort_keys=True)) $     else: $         print(data)
gram_collection = db.gram_posts $ print(gram_collection.count())
x_train_data = training_DF.drop(['IncidentInfo','ATRW','LRW','DRW','NLT','NTT','NDT','cut','text'],axis=1) # $ y_train_data = training_DF['IncidentInfo']
plt.plot(pipe.tracking_error)
dfs_morning.loc[dfs_morning['ENTRIES_MORNING']<=0, 'ENTRIES_MORNING'] = dfs_morning.ENTRIES_MORNING.quantile(.5)
df5 = df[(df['group'] == 'treatment') & (df['landing_page'] == 'new_page')] # there are simpler ways to do this, $ df6 = df[(df['group'] != 'treatment') & (df['landing_page'] != 'new_page')] # but I wanted to preserve the syntax from above. $ df2 = df5.append(df6).reset_index() $ df2.sample(5)
g = sns.distplot(dftop['temp'], label = "days most complaints") $ g = sns.distplot(weather['temp'], label = "all weather data") $ g.legend(loc='upper left') $ plt.savefig('Top_complaints_days_temp_dist_versus_all_data.png')
np.exp(0.0408),np.exp(0.0506)
df.isnull().any().any(), df.shape
max_dif_Q5 = max(close.values()) $ min_dif_Q5 = min(close.values())
MostHourlyExits.
df2['low_tempf'].mean()
tweets.head()
df['bikeid']= df['bikeid'].astype(str) $ df['bikeid'].describe()
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets if tweet.lang == 'en'], columns=['Tweets']) $ display(data.head(10))
training_active_listing.transform(X_test)
hot_df.to_csv('data_redditv2.csv')
pd.to_datetime('12-11-2010 00:00', format='%d-%m-%Y %H:%M')
test.columns = ['streamTweets', 'accountDuration', 'numDays', 'allTweets', 'followers', 'following'] $ candSplit.columns = ['streamTweets', 'accountDuration', 'numDays', 'allTweets', 'followers', 'following'] $ test.reset_index(inplace=True, drop=False) $ candSplit.reset_index(inplace=True, drop=False) $ candSplit.head()
engine.return_as_panda_dataframe = True
merged = pd.merge(props, contribs, on="calaccess_committee_id") $ merged.info()
csvData[csvData['street'].str.match('.*South.*')]['street']
cols_to_export = ["epoch","src","trg","src_str","src_screen_str","trg_str","trg_screen_str"] $ mentions_df.to_csv("/mnt/idms/fberes/network/ausopen18/data/ao18_mentions_with_names.csv",columns=cols_to_export,sep="|",index=False)
df.info()
p_all = df2['converted'].mean() $ print('The probability of an individual converting is {}.'.format(round(p_all,4)))
full.tail()
iris_df.describe()
archive_clean[archive_clean.name == 'None']
df2.converted[df2.group == "treatment"].mean()
df['effective_date']=pd.to_datetime(df['effective_date']) $ df['dayofweek'] = df['effective_date'].dt.dayofweek $ df['weekend']= df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0) $ df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)
run txt2pdf.py -o"2018-06-19 2015 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 FLORIDA HOSPITAL Sorted by discharges.txt"
dataset.head()
helper.print_dataframe(close)
df3 = df2.rename(columns={'treatment': 'ab_page'}) $ df3.head()
y_test_under[fm_bet_under].mean()
ADP_array=df["NASDAQ.ADP"].dropna().as_matrix() $
data_t = data.copy() $ data_t['Time'] = pd.to_datetime(data['Time'], unit='s') $ data_t = data_t.set_index('Time') $ data_t.head()
new =  len(df2.query("landing_page == 'new_page'")) $ old = len(df2.query("landing_page == 'old_page'")) $
from sklearn.preprocessing import StandardScaler $ scaler = StandardScaler() # create scaler object $ scaler.fit(x_train_numerical) # fit with the training data ONLY $ x_train_numerical = sparse.csr_matrix(scaler.transform(x_train_numerical)) # Transform the data and convert to sparse $ x_test_numerical = sparse.csr_matrix(scaler.transform(x_test_numerical))
h2o.init()             #specify max number of bytes. uses all cores by default. $ h2o.remove_all()                          #clean slate, in case cluster was already running
df= df[['created_at','display_text_range','favorite_count','full_text','retweet_count']] $ df['tweet_lenght']= df['display_text_range'].apply(lambda arr: arr[1]) $ x = df.drop(['display_text_range','retweet_count','favorite_count','full_text'], axis=1)   # Input Feature Matrix
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x, np.dot(x3, ridge1.coef_) + ridge1.intercept_, color ='darkorchid', linewidth = '5') $ plt.plot(x, np.dot(x3, model3.coef_) + model3.intercept_, color = 'dimgrey', linewidth = '3.5')
df_samples.to_pickle(train_data_dir+'/LSTM_train_data.pkl')
to_be_predicted_Day5 = 21.39121267 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
pd.bdate_range(s,e)
df = df.sort_values(by="text") $ vader_df = vader_df.sort_values(by="text") $ df = df.reset_index(drop=True) $ vader_df = vader_df.reset_index(drop=True) $ df.head()
df['closed_at'] = pd.to_datetime(df['Closed Date'], format="%m/%d/%Y %I:%M:%S %p")
with open('data/chefkoch_01.json') as data_file:    $     chef01 = json.load(data_file) $ clean_new(chef01) $ chef01df = convert(chef01) $ chef01df.info()
tpot = TPOTRegressor(verbosity=2) $ tpot.fit(X_train, y_train) $ print(tpot.score(X_test, y_test)) $ tpot.export('bikeometer_pipeline_custis4boost.py')
result.summary2()
df[df.sentiment == 4].index
df_h1b_nyc_ft.groupby(['lca_case_employer_name']).lca_case_employer_name
StockData.loc[StockData['Date-Fri'] == 1].head()
df[(df.state == 'YY') & (df.amount >= 45000)]\ $     .sort_values(by='amount', ascending=False)\ $     .head(6)[source_columns]\ $     .to_csv('out/0/outlier_data.csv')
title = soup.find(class_='content_title').text $ first_par = soup.find(class_='rollover_description_inner').text $ print(f'Title:{title}') $ print(f'First Paragraph:{first_par}')
tw.name.value_counts()
git_log.timestamp = pd.to_datetime(git_log.timestamp) $ git_log.head()
count_categories = kickstarter.groupby('main_category').size() $ count_categories
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']
float(df.converted.sum())/df_length
new_page_converted=np.random.choice([1,0],size=nnew,p=[pnew,1-pnew])
df3['intercept'] = pd.Series(np.zeros(len(df3)), index=df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index=df3.index)
print(fdist.most_common(50))
sns_plot = sns.lmplot(x='favorite_count',y='retweet_count',data=twitter_archive_clean,fit_reg=False,scatter_kws={'alpha':0.1}) $ sns_plot.savefig('favorite_vs_retweet.jpg')
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ logres.summary()
sentiment_scores = [] $ for text in data_sentiment["text"]: $     sentiment_score = sid.polarity_scores(text) $     sentiment_scores.append(sentiment_score['compound']) $ data_sentiment["sentiment_score"] = sentiment_scores
assert sorted(troll_users.columns) == sorted(expected_user_cols)
access_logs_df.select('ipAddress').distinct().count()
med = out_df.copy() $ med['low']=0 $ med['medium']=1 $ med['high']=0 $ med.to_csv('all_med.csv',index=False)
x_data = data1[features].as_matrix() $ y_data = data1['y_flag'].as_matrix()
weather_data_used = weather_data['20130101':'20171231'] $ weather_data_used = pd.DataFrame(weather_data_used.temp, index=weather_data_used.index) $ weather_data_null = weather_data_used[weather_data_used['temp'].isnull()] $ weather_data_null = weather_data_null.fillna(0) $ weather_data_null.groupby([weather_data_null.index.year,weather_data_null.index.month]).agg('count').head(60) $
df.dtypes
store_items.fillna(0)
by_hour.interpolate()
train.info()
precipitation_df.set_index("date", inplace=True) $ precipitation_df.head(15)
len(df2[df2.converted==1])/len(df2)
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0)].groupby('Memory').Wait.agg(['mean','median', 'count'])
df.set_index('Date',inplace=True) $ df.head()
tweets_df.tail(3)
MergeMonth.set_index('Date').to_csv('Sales_MonthlySummary.csv')
dft.loc['2013-1-15 12:30:00']
tweets_RDD = sc.textFile('./PSI_tweets.txt') $ tweets = tweets_RDD.collect() #or tweets = tweets_RDD.take(500) for some testing $
control['converted'].sum() / control.shape[0]
model_df = stock.iloc[915:].copy()
tl_2040 = pd.read_csv('input/data/trans_2040_ls.csv', encoding='utf8', index_col=0)
doctopic = clf.fit_transform(dtm)
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.hist(results["units"]["uqs"]) $ plt.xlabel("Sentence Quality Score") $ plt.ylabel("Sentences")
predict_acct_id_udf = F.udf(predict_acct_id,types.DoubleType()) $
movies_df['genres'] = movies_df.genres.str.split('|') $ movies_df.head()
model_gb_20_14 = GradientBoostingClassifier(min_samples_leaf=20, max_depth=14, random_state=42) $ model_gb_20_14.fit(x_train,y_train) $ print("Train: ", model_gb_20_14.score(x_train,y_train)*100) $ print("Test: ", model_gb_20_14.score(x_test,y_test)*100) $ print("Difference between train and test: ", model_gb_20_14.score(x_train,y_train)*100-model_gb_20_14.score(x_test,y_test)*100)
twitter_df.tweet_id.sample(1)
url = "https://api.nytimes.com/svc/books/v3/lists/overview.json?api-key=e096672c9fd940b0a45e031e17e5f002" $ request = urllib.request.Request(url) $ response = urllib.request.urlopen(request) $ rs = response.read().decode('utf-8')
os.chdir('code')
csv_df['timestamp'].min()
weather_mean.iloc[1, 4]
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ results  = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]).fit()
priors_product_purchase_spec= priors_product.groupby(["user_id","product_id"]).size().reset_index(name ='purchase_count_spec') $ priors_product_purchase_spec['userprod_id']=priors_product_purchase_spec['product_id'] + priors_product_purchase_spec['user_id'] *100000 $ priors_product_purchase_spec.head(10)
print(pd.value_counts(train_df['os'])[:20])
stat_info_st = stat_info[0].apply(fix_space_0) $ print(stat_info_st)
n_old = df2['landing_page'].value_counts()[1] $ n_old
from sklearn.metrics import accuracy_score $ accuracy_count = accuracy_score(y_test_count, predictions_count) $ accuracy_tfidf = accuracy_score(y_test_tfidf, predictions_tfidf) $ print('Count Vectorized Words Accuracy:', accuracy_count) $ print('TfIdf Vectorized Words Accuracy:', accuracy_tfidf)
stopword_list.extend(["dass", "wer", "wieso", "weshalb", "warum", "gerade"]) #Add the words ["dass"] to the list. $ print(len(stopword_list))
previous_month_date = end_date - timedelta(days=30) $ pr = PullRequests(github_index).is_closed().get_cardinality("id")\ $                                .since(field="closed_at", start=previous_month_date)\ $                                .until(field="closed_at", end=end_date) $ get_aggs(pr)
element = driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a') $ element.click()
twitter_archive_clean.loc[(twitter_archive_clean.rating_denominator< 10),"rating_denominator"]=10
average_for_each_calendar_month = s.resample('M').mean() #Calculating the avarage of each calender month in series 's' $ print(average_for_each_calendar_month) #Printing the average of values in each calender month in series 's'
df_R.info()  #274 rows
ddp = dfd.dropna(axis=0, subset=['in_reply_to_screen_name'], how='any')
mco=np.vstack([x,np.ones(len(x))]).T $ solm,solb=np.linalg.lstsq(mco,y)[0] $ print(solm,solb) $ y_predicted =np.array((solm*x+solb)) $
count1df = pd.DataFrame(chef02) $ count1df = count1df.drop_duplicates(subset=['name', 'user']) $ count1df.info()
df_twitter_archive_copy.drop(['doggo','pupper','floofer','puppo'], axis=1, inplace=True)
duration_test_data.to_csv('./data/hours_test_data.csv')
cg_counts = aqi.groupby(['AQI Category_cg']).size() $ (eug_cg_counts / cg_counts).unstack(fill_value=0)
df = pd.read_csv("../Data/user_summary_id_infos.csv") $ df.info()
p=(null_vals>actual_diff).mean() $ p
X_train, X_test, y_train, y_test = train_test_split(tweet_embedding, train['label'], test_size = 0.15, random_state=42)
twitter_archive_df.describe()
menus = pd.read_csv('./raw_data/menus.csv') $ menus['About'].fillna('', inplace=True) $ menus['Cuisine Type'].fillna('', inplace=True)
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US', 'ab_page']])
print(a) $ print(['dog'] + a)  # + can be used to concanetenate lists; implemented by list.__add__ $ a.append('dog')  # append() can be used for concatenating elements $ print(a)
train.head(5)
corr = dd_df.corr()
my_own_data = r.json() $ print(type(my_own_data))  
fare = df_titanic['fare'] $ print(fare.describe()) $
loan_fundings.columns
df.loc[df['edition']=='NBC']
df_mes = df_mes[df_mes['tip_amount']>=0] $ df_mes.shape[0]
so = stackexchange.Site(stackexchange.StackOverflow) $
df_predictions.Q0.value_counts()
df['year_built'] = df['year_built'].iloc[:, 0].fillna(-1) + df['year_built'].iloc[:, 1].fillna(-1)
by_party = candidates.groupby(["party_name", "election_year"]).size().reset_index()
df.cumsum().plot();
df_new['US_ind_ab_page'] = df_new['US']*df_new['treatment'] $ df_new['CA_ind_ab_page'] = df_new['CA']*df_new['treatment'] $ logit_h = sm.Logit(df_new['converted'], df_new[['intercept', 'treatment', 'US', 'CA', 'US_ind_ab_page', 'CA_ind_ab_page']]) $ result = logit_h.fit() $ result.summary()
BID_PLANS_df.loc['506b9ecc'].to_frame().transpose()
input_edge_types_file  = input_directory_name + 'input_edge_types.csv' $ Ext_input.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=input_directory_name)
Z = np.random.random((10, 3)) $ print(Z) $ zmin, zmax = Z.min(axis=0), Z.max() $ print(zmin) $ print(zmax)
np.random.shuffle(SampleIndex)
data = pd.read_csv('data/Building_Permits.csv') $
n_old=df2.query("landing_page=='old_page'").user_id.count() $ n_old
JAR_FILE = "/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.6.0-mr1-cdh5.7.0.jar" # eg. /usr/lib/hadoop-mapreduce/hadoop-streaming.jar $ HDFS_DIR = "/user/root/hw3" # eg. /user/root/hw3 $ HOME_DIR = os.getcwd() 
grades = grades.drop(grades.index[(grades.Mark == 0) & (grades.ATAR == 0)]) $ noatar = grades[(grades.ATAR == 0)] $ noatar.Mark.hist(bins=25)
cities.reindex([0, 4, 5, 2])
test_preds_mnb = mnb_best.predict(X_test) $ display_f1(y_test, test_preds_mnb)
exploration_titanic.nacolcount()
pop_flat.set_index(['states', 'year'])
autos.head()
HARVEY_gb_user = df.groupby('user')
verify_response.json().keys()
sn.barplot(x ='funding_rounds', y="count", data=round_count)
initial_commit = blame[blame.author == "Linus Torvalds"].timestamp.min() $ initial_commit
logit_mod_3 = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','new_page_UK','US','new_page_US']]) $ results_3 = logit_mod_3.fit() $ results_3.summary()
dates = pd.date_range('2010-01-01', '2010-12-31') $ df = get_data(symbols=['XOM','GLD'],dates=dates) $ df = normalize_data(df) $ df.plot() $ plt.show()
k = 6 $ neigh6 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train) $ yhat6 = neigh6.predict(X_test) $ print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh6.predict(X_train))) $ print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat6))
run txt2pdf.py -o"2018-06-19 2012 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2012 FLORIDA HOSPITAL Sorted by discharges.txt"
positive_list=list(support_or_not.loc[support_or_not["compound"]>=pos_threshold].text)
Data.head()
tweets_p1_month.groupby('M_Y', sort=False).mean()[::-1].plot(kind='bar')
autos["last_seen"].str[:10].value_counts(normalize=True, $                                    dropna = False $                                   ).sort_index(ascending = True)
p_old = df2[df2['landing_page'] == 'old_page']['converted'].mean() $ print("Probability of conversion for old page is {}".format(p_old))
plt.hist(results["workers"]["wqs"]) $ plt.xlabel("Worker Quality Score") $ plt.ylabel("Workers")
df.plot(column='cluster_labels', cmap='Reds', legend=True) $ plt.title("Clusters: K Means")
df_B4=pd.read_csv("classB.csv",header=None,skiprows=1) $ df_B4.shape
print("The average weekly tweet count is {} with a standard deviation of {}".format(weeklyTotals.mean(), $                                                                                     weeklyTotals.std()))
if os.path.isfile(pickle_full): $     print("loading pickle") $     df = pd.read_pickle(pickle_full) $ else: $     print("Did you run 1- Raw Data Visualisation?")
%load "solutions/sol_2_25.py"
tweetnet = nx.read_gpickle('twitter_graph_data/bigraph_full_pickle')
post_url = 'https://staging.app.wikiwatershed.org/api/watershed/'
import os $ data = pd.read_csv('https://raw.githubusercontent.com/ogrisel/parallel_ml_tutorial/master/notebooks/titanic_train.csv', $                     sep = ',')
plt.scatter(active_list_pending_ratio_transform[:,0], active_list_pending_ratio_transform[:,1])
learn.load('clas_2')
merge[merge.columns[39]].value_counts()
zone_train.shape $
test_dum.shape
news_period_df.dtypes
topCandidateInt['score_weight'].describe()
for post in posts.find({"reinsurer": "AIG"}): $     pprint.pprint(post)
s1[:3].dot(s2)
construction.to_sql(con=engine, name='construction', if_exists='replace', flavor='mysql',index=False)
orig_ct = len(dfd) $ dfd = dfd.query('hspf >= 10.0 and hspf <= 18') $ print(len(dfd) - orig_ct, 'eliminated')
predicted_talks_vector = classifier.predict( vectorized_text_predict )
[column.name for column in get_descendant_column_data(observations_node)]
data['duration [days]'] = data['duration'].dt.days
cross_val_score(LogisticRegression(), X, y, cv=cv5_idx).mean()
MostHourlyExits = subway3_df.nlargest(100,'Hourly_Exits')
support.amount.sum() / merged.amount.sum()
training_data = query_training_data(model) $ model.train_model(training_data) $ df_backcast = swag.control.training.run.create_backcast(model, start_date, end_date) $ backcast = df_backcast[0].pivot_table(index="Gas_Date", columns='Line_Item', values="Daily", aggfunc=np.sum).reset_index()
date_df.plot.area(stacked=False)
coming_next_reason.to_csv('../data/coming_next_reason.csv')
df = df.drop('Unnamed: 0', axis=1)
fb['2012':].resample('W')['share_count'].sum().sort_values(ascending=False)[:5]
df = df[['text', 'sentiment']] $ display.display(df['text'].describe()) $ display.display(df['sentiment'].describe())
recent = df.ix[df['date'] > cutoff] $ is_core = recent.groupby(['org'])['is_core'].mean() $ is_core
archive.info()
pd.crosstab( $     pd.Series(y_test,name='Actual'), $     pd.Series(y_pred_lgr,name='Predicted'), $     margins=True $ )
(p_diffs>a_diff).mean()
fig, ax = plt.subplots(figsize=(14,8)) $ g = sns.boxplot(y='PRICE', x=condo_6['SALEDATE'].dt.year, data=condo_6, ax=ax) $ t = g.set_title("Distribution of Sale Price by Year")
age_plot = df2017.groupby(['Week Ending Date','age'])['Sales'].sum()
p_mean = np.mean([p_new, p_old]) $ p_mean $ print('Probability of conversion under the null hypothesis ',p_mean )
all_lum = LUM.process_lum(etsamples,etmsgs) $ all_lum_binned = LUM.bin_lum(all_lum)
liquor['Category'] = liquor['Category'].map(lambda x: int(x)) $ liquor['County Number'] = liquor['County Number'].map(lambda x: int(x))
df['ab_page'] = pd.Series(np.zeros(len(df)), index=df.index)
import re $ import itertools $ from urllib.parse import urlparse
from nltk.tokenize import RegexpTokenizer $ from nltk.corpus import stopwords $ from nltk.stem.snowball import SnowballStemmer $ from gensim import models, corpora $ from nltk.sentiment.vader import SentimentIntensityAnalyzer
rf200 = RandomForestClassifier(n_estimators=200) $ rf200.fit(X, y)
jobs.loc[(jobs.MAXCPUS == 600) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Memory % 4000 == 0)].groupby('Memory').Wait.median().plot(kind = 'bar', logy = False)
plt.plot(dataframe.groupby('quarter').daily_worker_count.mean()) $ plt.show()
voc = vect.vocabulary_
pd.DatetimeIndex(pivoted.columns)
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','DetectionPhase']).created_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False) $
new_df_left = train_df.merge(prop,how='left',on = 'parcelid')
to_be_predicted_Day4 = 17.67907235 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
MostActiveStationId = "USC00519281" $ Active_TempObs = session.query(Measurement.date, Measurement.tobs).order_by(Measurement.id.desc()). \ $     filter(Measurement.station == MostActiveStationId).limit(365).all() $ Active_TempObs
count_pages=df2.groupby(['landing_page']).size()# this gives the count of both new and old pafes together $ n_new=count_pages[0]# this gives the count for the new pages $ n_new
import pandas as pd $ import numpy as np $ pd.options.mode.chained_assignment = None  # default='warn' $ baseball = pd.read_csv("Data/baseball.csv", index_col='id')
reviewsDF.dtypes
urls["domain"] = urls["url"].str.replace("http.*://([^/]+).*",r"\1") $ urls.head()
park[park.named_drug.notnull()][:5]
data = pd.read_csv('data.csv') $
archive_df_clean['name'].replace(['very','the','a','an','None','not','one'],['NA','NA','NA','NA','NA','NA','NA'], inplace=True) $
dfMeta.head(5)
image_predictions_clean.info()
!cp ./stockdemo-model/score.py ./
ratings.head()
autos = autos[autos['price'].between(1,350000)] $ autos['price'].describe()
logreg_words.fit(X_words, y).coef_[0].round(3)[-4:]
os.getcwd()
df2[df2['user_id'].duplicated()] $
(p_diffs > obs_diff).mean()
test.head()
ratedCars = pd.merge(cars, $                      coolCars, $                      on=['Model']) $ ratedCars[5:10]
alg = LogisticRegression(random_state=1) $ alg.fit(X_train, y_train)
fb_tokens = word_tokenize(ftfy.ftfy(fb_cleaned))
s5 = pd.Series([1000,1000,1000,1000]) $ print("s2 =", s2.values) $ print("s5 =", s5.values) $ s2 + s5
print(df_users['bio3'].value_counts())
d = json.loads(r.text[len('var tumblr_api_read = '):-2]) $ print(d.keys()) $ print(len(d['posts']))
total_events_number = len(duplicate_check_df) $ unique_events_number = len(duplicate_check_df['event_id'].unique()) $ duplicate_events_number = total_events_number - unique_events_number $ print("The total events number is %d, and the unique events number is %d, so in dataset %d events appeard more than once." $       % (total_events_number, unique_events_number, duplicate_events_number))
df['closed_at'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %I:%M:%S %p')
autos["price"].unique()
LinkNYCpLag = ps.lag_spatial(qW, pumashplc['linkNYCppcBB'])
customer_visitors_new.index
merged2['Specialty'].isnull().sum(), merged2['Specialty'].notnull().sum()
active_mailing = clean_users[clean_users['active']==1]['opted_in_to_mailing_list'].sum()/active_count
rng = pd.date_range('3/6/2012 00:00:00', periods=10,freq="D",tz="US/Mountain") $ rng.tz, rng[0].tz
org_member_id = '791f91db-ae04-46ba-ab6d-42a71058f5f6' $ url = form_url(f'organizationMembers/{org_member_id}/actions') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
import matplotlib.pyplot as plt $ import seaborn as sns $ sns.set()
print(df2.info())
import pickle $ features,target= pickle.load(open('preprocess_data.p', mode='rb'))
saved_model_details = client.repository.store_model(best_model_uid, {'name': 'k_flw-test2'})
review_body.head()
val_small_data.click_timeHour.unique()
converted = ts.asfreq('45Min', method='pad')
df = pd.DataFrame({'foo': [1,2,3], 'bar':[0.4, -1.0, 4.5]}) $ df.values, df.values.dtype
fail_df = input_df[df['Match']=='No_Match'].copy() $ fail_df['street'] = fail_df['street'].str.replace('\b(?:apt *)?(?:no|#) (?:- *)? *[a-z0-9-]','')
df2['intercept']=1 $ logit_mod=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results=logit_mod.fit() $
vwap.ix['2011-11-01 09:27':'2011-11-01 09:32']
%matplotlib nbagg $ df4.plot(x='Date', y='BG') $ plt.title('Blood Glucose over 100 Values on Dec. 22, 2017', $           color='Blue') $ plt.show() $
cust, c = generate_labels('jdYSK7I9TjjADXtMsv8vhN362hfIfC0Qyrn4fSZNHKY=', trans, $                           label_type = 'SMS', churn_period = 30, return_cust = True) $ c[c['days_to_next_churn'] < 15] $ cust.iloc[:, 6:]
from gensim import corpora, models $ dictionary = corpora.Dictionary(sws_removed_all_tweets)
session.query(Measurement.station, func.count(Measurement.tobs)).\ $ group_by(Measurement.station).\ $ order_by(func.count(Measurement.tobs).desc()).all()
print mike.ra[0] $ print mike.RA0[0]
tia.head()
df_tweets.shape[0]
print(x_tr.shape) $ print(x_tr[1][1]) $ print() $ print(y_tr.shape) $ print(y_tr[1])
import json $ test_sample = json.dumps({"data": test_df.to_json(orient='records')}) $ prediction = aks_service.run(input_data = test_sample) $ print(prediction)
df_ad_airings_5['location'][0].split(",")[0]
df=pd.read_excel("../../data/stocks.xlsx") $ df.head(2).to_html("../../data/stocks.html") $ !head -n 28 ../../data/stocks.html
station_df['station'].count()
cig_data.info()
times.dt?
SEA = pd.read_excel(url_SEA, $                     skiprows = 8)
%matplotlib notebook $ import matplotlib.pyplot as plt $ plt.plot_date(df_meme['date'],df_meme['occur'],linestyle='solid') $ plt.xticks(rotation=-40) $ plt.show()
tweet_counts_by_month['tweet_count'] = tweet_archive_master[['timestamp', 'tweet_id']].groupby(pd.Grouper(key='timestamp', freq='M')).count()
SCN_BDAY = pd.merge(BID_PLANS_df,pd.to_datetime(BDAY_PAIR_df['birthdate']).dt.strftime('%Y-%m-%d').to_frame(),how='left',left_index=True,right_index=True)
postsTypes = posts[(posts['type']=='link')|(posts['type']=='status')].groupby(['type']).mean() $ postsTypes[['likesCount','commentsCount','sharesCount']].round(2) $
plt.figure(figsize = (10, 10)) $ df_master[['retweet_count', 'favorite_count']].plot(style = '.') $ plt.title('Retweet and Favorite Counts');
idx_inst = df[inst_order[0]].sort_values(ascending=False).index
df.drop('Date', inplace=True, axis=1)
bigdf.head()
reddit.shape
obs_diff = np.array(treatment_cr - control_cr) $ (p_diffs > obs_diff).mean()
y = df['comments'] $ X = df[['title', 'age', 'subreddit']].copy(deep=True)
model.infer_vector(["system", "response"])
import pandas as pd $ df = pd.DataFrame({'a': 1, 'b': range(4)}) $ df
print ('training_pending_ratio: ', training_pending_ratio.shape)
data.keys()
cust_demo.sort_values(by=['Location', 'Gender'], ascending=[False, True]).head(50) $
van_final['diffs'] = van_final.groupby(['userid'])['revtime'].transform(lambda x: x.diff()) / np.timedelta64(1, 'm')
walking_df.head()
suspects_with_27_1['timestamp'] = pd.to_datetime(suspects_with_27_1['timestamp'])
sns.barplot(y = X.columns, x = list(logr.coef_.reshape(-1)))
JournalStories = pd.DataFrame(JournalStories)
df.head(n=2)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(RN_PA_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly RN/PA Hours Partial Autocorrelation')
df_train['date'] = pd.to_datetime(df_train['date'], format='%Y-%m-%d') $ df_test['date'] = pd.to_datetime(df_test['date'], format='%Y-%m-%d')
biz_df.head()
import test_package.print_hello_function_container
G = nx.Graph() #creates empty graph, initiliasize a graph object $ G.add_nodes_from(node_names) $ G.add_edges_from(edges)
act_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ act_diff
import matplotlib.pylab as plt $ fig, ax = plt.subplots(figsize=(12,8)) $ weather.hist( ax = ax); $ plt.show()
geo_TempJams.head()
sub1.head(10)
ls -n data.*
!scrapy runspider src/ds_web_data_hello_scrapy.py --output='src/ds_web_data_hello.csv' -t csv --logfile='src/ds_web_data_hello.log'
from gensim.models.doc2vec import Doc2Vec
top_supporters.head(5).to_csv("top_supporters.csv")
train['air_genre_name']=train['air_genre_name'].fillna('Other')
count_non_null(geocoded_df, 'Judgment.Against')
accuracy_valid = pipeline.predict(X_valid) $ accuracy = np.mean(np.array(accuracy_valid['PredictedLabel'].astype(int)) == y_valid) $ print('validation accuracy: ' + str(accuracy))
df_ratings.describe()
df_prep99 = df_prep(df99) $ df_prep99_ = pd.DataFrame({'date':df_prep99.index, 'values':df_prep99.values}, index=pd.to_datetime(df_prep99.index))
left = pd.DataFrame({'key' : ['foo', 'bar'], 'lval': [1,2]}) $ left
df["booking_user_agent"].value_counts()
post_urls = [] $ for i in range(len(df[0:5])): $     post_urls.append(df.iloc[i]['link_to_post'][i][-11:]) $ post_urls $
plot = song_tracker["Bad and Boujee (feat. Lil Uzi Vert)"][1:].astype(float).plot(figsize=(20,10),title="Bad and Boujee by Migos") $ plot.set_xlabel("Date") $ plot.set_ylabel("Chart Position")
print('Most LaziestCanine:', tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values( $     'laziestcanine_pp', ascending=False).text.values[0]) $ print('Least LaziestCanine:', tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values( $     'laziestcanine_pp', ascending=True).text.values[0])
grouped.ngroups
ts = df.groupby(pd.Grouper(key='created_at', freq='D')).mean()
%timeit our_function(df_protest.rangecode)
prcp_1_df.plot() $ plt.xlabel("Date range of 365 records from 8/24/2016 through 8/23/2017") $ plt.ylabel("Precipitation") $ plt.show()
df2 = pd.read_csv('comma_delim_clean.csv', index_col='id') $
tags_counts = {} $ words_counts = {} $ from collections import Counter $ tags_counts = Counter([item for taglist in y_train for item in taglist]) $ words_counts = Counter([word for line in X_train for word in line.split(' ')])
train_df = pd.read_json("train.json") $ test_df = pd.read_json("test.json")
mention_pairs.sort_values("Weight",ascending=False).head()
liberiaCasesSuspected = liberiaCasesSuspected.fillna(0) $ liberiaCasesProbable = liberiaCasesProbable.fillna(0) $ liberiaCasesConfirmed = liberiaCasesConfirmed.fillna(0)
df_state_dummy = pd.get_dummies(df_more['State Initials']) $ df_state_dummy.head()
sgd = SGDClassifier() $ sgd.fit(X_train, Y_train) $ Y_pred = sgd.predict(X_test) $ acc_sgd = round(sgd.score(X_test, Y_test) * 100, 2) $ acc_sgd
print '\n DataFrame df_totalConvs_day, sorted in descending order', df_totalConvs_day.shape $ df_totalConvs_day.sort('conversions', ascending=[0]).head(10)
learn.load('clas_0')
S.executable = "/code/bin/summa.exe"
staging_bucket = 'gs://' + google.datalab.Context.default().project_id + '-dtlb-staging-resolution' $ !gsutil mb -c regional -l {storage_region} {staging_bucket}
driver = webdriver.Chrome(executable_path = "C:/libs/web_driver_for_crawl/chromedriver") $ driver.wait = WebDriverWait(driver, 10)
data.info() $
mb.Stool / mb.Tissue
topics = lda.get_term_topics('network') $ for t in topics: $     print(t)
body = df_titanic['body'] $ print(body.describe()) $
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs);
df=df[df["ORIGIN"].apply(lambda x: x in ["BNA","DAL","HOU","STL"])] $ df=df[df["DEST"].apply(lambda x: x in ["BNA","DAL","HOU","STL"])]
tweet_json.info()
df_events.iloc[:,7].value_counts()
autos["odometer_km"].value_counts()
from urbanmetabolism.population.model import transition_rate $ from urbanmetabolism.population.model import plot_transition_rate $ from urbanmetabolism.population.model import reduce_consumption
finals.head(2)
df_new.head()
from src.pipeline import pipeline_json $ pj = pipeline_json(response.content) $ X = pj.convert_to_df(scaling=True, filtered=True)
print('RMSE LGBMRegressor: ', RMSLE(np.log1p(train['visitors'].values), lgbmrscv.predict(train[col])))
pop_dog.describe()
temp_df = proportion_and_count(active_psc_records,'kind',len(active_psc_records)) $ temp_df.to_csv('data/viz/types_of_psc.csv') $ temp_df
def explicit(): $     from google.cloud import storage $     storage_client = storage.Client.from_service_account_json(JSON_SERVICE_KEY) $     buckets = list(storage_client.list_buckets()) $     print(buckets)
xml_in['publicationDate'] = pd.to_datetime(xml_in['publicationDate'], format='%Y-%m-%d', errors='coerce')
reddit_comments_data.groupBy('author').agg({'sentiment':'mean'}).orderBy('avg(sentiment)', ascending = False).show()
testy.describe()
accuracies = cross_val_score(LogisticRegression(), X2, y2, cv=10, verbose=1) $ print accuracies.mean() $ print y2.mean()
from sklearn.preprocessing import StandardScaler $ scaler = StandardScaler() $ x_train1 = scaler.fit_transform(x_train1) $ x_test1 = scaler.transform(x_test1)
autos["price"] = (autos["price"] $                       .str.replace("$", "") $                       .str.replace(",", "") $                       .astype(float) $                  )
telecom2 = telecom2.dropna(axis=0, subset=['onnet_mou_6', 'onnet_mou_7', 'onnet_mou_8']) $ print(telecom2.shape) $
result = clf_RF_WV.predict(testDataVecs) $ result_prob = clf_RF_WV.predict_proba(testDataVecs) $ print(result[0:10]) $ print(result_prob[0:10])
test['price_usd'] = pd.Series(pr)
cc['date'] =  pd.to_datetime(cc['date'], format='%Y-%m-%d') $ cc.date.describe()
genreTable.shape
before['count'] = before.groupby('hashtags')['hashtags'].transform(pd.Series.value_counts) $ before.sort('count', ascending=False) $ before.hashtags.dropna().head()
df = df.replace('tomato', 'pea') $ df
ts.resample('T').mean().dropna()
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and group == 'control' and converted == '1'").count()[0] $ convert_new = df2.query("landing_page == 'new_page' and group == 'treatment' and converted == '1'").count()[0] $ n_old = n_old $ n_new = n_new
data.sort_values('commits', ascending=False)
unique_users = len(df['user_id'].unique().tolist()) $ print('There are ' + str(unique_users) + ' unique users in the dataset.')
df = pd.read_sql("select a.item_id , a.order_item_size_id , sum(quantity) as quantity , b.title from order_details a inner join store_items_sizes b on a.order_item_size_id = b.order_item_size_id where a.item_id = 11 or a.item_id = 23 GROUP BY order_item_size_id, item_id",conn)
cols_to_show = ['Indicator_id', 'Country', 'Year', 'WHO Region', 'Publication Status']  # picked the columns based on the output format shown in assignment sheet $ df[cols_to_show].sort_values(['Year','Indicator_id','Country','WHO Region' ], ascending=[True, True, True, False]).head() $
train.StateHoliday = train.StateHoliday!='0'; test.StateHoliday = test.StateHoliday!='0'
p_new_real = df2.query('landing_page == "new_page"')['converted'].mean() $ p_new = df2.query('converted == 1').count()[0]/df2.count()[0] $ p_new
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ results = logit_mod.fit() $ results.summary()
np.exp(0.0112), 1/np.exp(-0.0144)
exploration_titanic.findcorr() # no highly numerical correlated columns 
dc_week = dc.groupby(['YearWeek']) $ x = dc_week.aggregate(np.count_nonzero) $ x = x.reset_index(level=['YearWeek']) $ x.head(5) $
df_json_tweets.drop(['date_timestamp'], axis=1,inplace=True) $ df_json_tweets.info()
all_df['onpromotion']=all_df['onpromotion'].astype(np.int8)
reorder_customers = np.fromiter(result.keys(), dtype=int) $ reorder_customers.size
p_new = df2.converted.mean() $ print("Convert rate for p_new:", p_new)
weights = np.array([.25, .25, .25, .25]) $
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=50000, usecols=['Closed Date', 'Created Date', 'Agency', 'Complaint Type', 'Borough']) $
from io import StringIO $ Z = np.genfromtxt(s, delimiter=",", dtype=np.int) $ print(Z)
df3[['CA', 'US']] = pd.get_dummies(df3['country'])[['US', 'CA']]
df.info()
logreg = LogisticRegression() $ logreg.fit(X_train_all, y_train) $ logreg.score(X_test_all, y_test)
grouped_dpt.first() # first row of each group 
joined = joined.dropna(axis=0) $ print('Number of rows in joined = {}'.format(joined.CustomerID.count()))
data_archie.loc[data_archie['user_id'].isnull()].head(5) $
b.d
import seaborn.apionly as sns $ sns.set_style('whitegrid') $ sns.jointplot("nb_words_content", "pp_uniq_words", data = train_data, $               kind='reg', size=6, space=0, color='b')
from lifetimes.plotting import plot_frequency_recency_matrix $ plot_frequency_recency_matrix(bgf)
bigrams_fd = nltk.FreqDist() $ bigram_words = [ ','.join(map(str,bg)) for bg in nltk.bigrams(wordsX) ] $ bigrams_fd.update(bigram_words) $
f0.include_list_filter
dcrime_gb.head()
np.mean(df['converted'])
print('The proportion of users converted is {}.'.format(round(df['converted'].mean(),4)))
df = pd.read_excel("out/ref.xlsx")
xml_in_merged = pd.merge(xml_in, grouped_df, on=['authorId', 'authorName'], how='left')
per_tweet_archive_by_month.plot()
paragraphs = soup.find_all('p') $ paragraphs
z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new,n_old],alternative='larger') $ (z_score,p_value)
wine_reviews = pd.read_csv("D:\kagglelearn\kaggledatasets\winemag-data-130k-v2.csv")
containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest', $               'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest', $               'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest', $               'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest'}
from autosklearn.regression import AutoSklearnRegressor
merged1 = pd.merge(left=merged1, right=meeting_status, how='left', left_on='MeetingStatusId', right_on='Id')
twitter_archive_enhanced_clean['timestamp'] = pd.to_datetime(twitter_archive_enhanced_clean['timestamp'])
df_draft.isnull().sum().sum()
df["grade"] = df["raw_grade"].astype("category") $ df["grade"]
All_tweet_data_v2.rating_denominator.value_counts()
iplot((monthly_mean - closed_monthly_mean).iplot(asFigure = True, vline=['2017/09', '2017/03', '2016/09', '2016/03', '2015/09', '2014/12', '2014/04', '2013/10'], dimensions=(750, 500)))
df.loc[2218, 'dollar_change_open'] = 0.09
hamburg.info()
ttarc.shape
result_df['UK_page']=result_df.ab_page*result_df.UK $ result_df['US_page']=result_df.ab_page*result_df.US $ result_df.head()
sum(new_page_converted).astype('float32')/145310 - sum(old_page_converted).astype('float32')/145274
pd.to_datetime('2010/11/12', format='%Y/%m/%d')
x_values = df2.reset_index()[['intercept', 'ab_page']].values.tolist() $ X = df2[['intercept', 'ab_page']] 
df1.head()
model.most_similar("awful")
reviews.country.unique()
df_weather['DATE'] = pd.to_datetime(df_weather['DATE']) $ df_weather['YEAR']= df_weather['DATE'].apply(lambda time: time.year) $ df_weather['MONTH']= df_weather['DATE'].apply(lambda time: time.month) $ df_weather['DAY_OF_MONTH']= df_weather['DATE'].apply(lambda time: time.day) $ df_weather['HOUR']= df_weather['DATE'].apply(lambda time: time.hour)
duplicated_list = twitter_archive_full[twitter_archive_full.tweet_id.duplicated()].tweet_id $ twitter_archive_full[twitter_archive_full.tweet_id.isin(duplicated_list)][['tweet_id','stage']].sort_values('tweet_id')
data.area is data['area']
all_df.head(2)
len(df_new.UWI.unique())
p_mean = np.mean([p_new,p_old]) $ print("Probability of conversion under the null hypothesis: " + str(p_mean))
raw_data.head()
chinadata["NFCR_change"] = chinadata.Non_Financial_Credit_Ratio.pct_change()
%%R $ flightsDB <- subset(flightsDB, select = -c(X, YEAR, X.1))
access_logs_df.cache()
vectorizer = CountVectorizer(analyzer = "word") $ freq_tweets = vectorizer.fit_transform(tweets) $ modelo = MultinomialNB() $ modelo.fit(freq_tweets, classes)
df['Media URL'].sample(10, random_state=42)
save_it = input('Do you want to save df_everything_about_DRGs? (Must say "yes")') $ if (save_it)=='yes': $     df_everything_about_DRGs.to_csv('df_everything_about_DRGs.csv')
plt.axvline(obs_diff) $ plt.axvline(np.mean(p_diffs)-obs_diff); $ plt.hist(p_diffs, alpha=0.3); $
(df2.converted).mean()
logit_mod = sm.Logit(df2['converted'], df2[['ab_page', 'intercept']]) $ results = logit_mod.fit()
group_time = sales_agg[['Store Number','month','Sale (Dollars)','Date']] $ group_time_2015 = group_time[group_time['Date'].dt.year == 2015] $ over_time_2015 = group_time_2015.groupby(['Store Number','month']).sum() $ over_time_2015.head(1)
y_hat = linreg.predict(quadratic) $ plt.plot(y_hat,'-b') $ plt.show()
df.drop_duplicates('title', inplace = True)
df['day_of_year'] = df.time_created.dt.date
!cat barclay_banks.txt
organisation.dtypes
autos['registration_month'].value_counts()
df_usa = df_usa.rename(index = str, columns = {"Gross Domestic Product (GDP)":"GDP", "National Rainfall Index (NRI)":"NRI", "Population density":"PD", "Total area of the country":"Area", "Total Population":"Population"})
df2.groupby('group').group.value_counts()
for word in words: $     print(conv.do(word))
df2 = df2.drop(dup_user.index) $ df2[df2['user_id'].duplicated()]
print(df_new['adNetworkType'].value_counts()) $ df_new.loc[:,'adNetworkType_google']=np.nan $ df_new.loc[df_new['adNetworkType']=='Google Search','adNetworkType_google']=0 $ df_new.loc[df_new['adNetworkType']=='Search partners','adNetworkType_google']=1 $ print(df_new['adNetworkType_google'].value_counts())
df_rows = df.shape[0] $ print("There are {} rows in the dataset.".format(df_rows))
train_data, test_data, train_labels, test_labels = train_test_split(spmat, y_data, test_size=0.10, random_state=42)  
api_df.sample(10)
ol.funding_rounds
df.loc[:, 'vegetables']
first_movie.h3.a
station_distance['Start Coordinates'] = station_distance['Start Station Latitude'].astype(str) \ $     + " , " + station_distance['Start Station Longitude'].astype(str) $ station_distance['End Coordinates'] = station_distance['End Station Latitude'].astype(str) \ $     + " , " + station_distance['End Station Longitude'].astype(str)
my_311 = pd.read_csv("MyLA311_All_Requests.csv",sep=',', low_memory = False) $ my_311['Longitude'].replace('', np.nan, inplace=True) $ my_311.dropna(subset=['Longitude'], inplace=True) $ my_311.to_csv('311_parsed_coordinates.csv', encoding = 'utf-8', index = False)
ffr.resample("2w")
df_mes2 = df_mes2.head(shape1)
no_dog_stage = df_complete.loc[:,['text','dog_stage']].query("dog_stage=='None'") $ no_dog_stage.sample(10)
brand_info.sort_values("mean_price", ascending=False)
test_ind.shape
au.save_df(dfd, f'data/heat-pump/proc/hp_specs')
np.random.seed(123456) $ ps = pd.Series(np.random.randn(12), mp2013) $ ps
autos['registration_month'].hist()
from nltk.tokenize import WordPunctTokenizer $ tok = WordPunctTokenizer()
def Plot_Boxplot(dataframe): $     return dataframe.plot(kind = "box")
education_data.reset_index(inplace=True)
df = df[df.BEDRM >= 2.0]
old_page_converted = np.random.choice([0, 1], n_old, p = [p_old, 1-p_old])
frame.loc['a', 'Ohio']
results.summary()
df_test.to_csv("test_transformV2.csv") 
class_merged_city=class_merged_hol['city'].unique() $ print(class_merged_city)
ex3data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'], $            'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3], $            'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1], $            'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}
df2['landing_page'].value_counts()[0]/len(df2)
plt.hist(p_diffs); $ plt.title("Histogram of P_diffs");
df_clean2.info()
print(autos['price'].unique().shape) $ print(autos['price'].describe()) $ print(autos['price'].value_counts().sort_index(ascending=False).head(20)) $ print(autos['price'].value_counts().sort_index(ascending=False).tail(20))
df_top10 = df[df['name'].notnull()]
def tweet_extend (tweet_id): $     try: $         return api.get_status(tweet_id, tweet_mode='extended')._json['full_text'] $     except: $         return "ERROR" $
submission_full['proba'].mean()
client = MongoClient() $ db = client.test_database #acessa ou cria o banco $
df2[["control","treatment"]] = pd.get_dummies(df2["group"])
result = pd.concat([df1, df3], axis = 0) # concatenate one dataframe on another along rows $ result
StationCount = session.query(Station.id).count() $ print(f'There are {StationCount} stations.')
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05))) $
column_names=list(All_tweet_data_v2.columns.values) $ stages=list(list(All_tweet_data_v2.columns.values)[i] for i in [13,14,15,16,22]) $ Remaining_columns=[names for names in column_names if names not in stages]
collection.list_items(source='Quandl')
print("# of unique STD customer ID in df:", len(df.loc[df['SalesOffice']=='STD']['Sold_to_Party'].unique())) $ print('# of unique HYB customer ID in df:', len(df.loc[df['SalesOffice']=='HYB']['Sold_to_Party'].unique()))
w.get_step_object(step = 2, subset = subset_uuid).allowed_data_filter_steps
url_weather = "https://twitter.com/marswxreport?lang=en" $ browser.visit(url_weather)
import os $ os.environ['BUCKET'] = BUCKET $ os.environ['PROJECT'] = PROJECT $ os.environ['REGION'] = REGION
pop={'Nevada':{2001:2.4,2002:2.9},'Ohio':{2000:1.5,2001:1.7,2002:3.6}} $ frame3=DataFrame(pop) $ frame3
for col in giss_temp.columns: $     giss_temp.loc[:, col] = giss_temp[col].astype(np.float32)
pred2 = nba_pred_modelv1.predict(g2) $ prob2 = nba_pred_modelv1.predict_proba(g2) $ print(pred2) $ print(prob2)
calls_nocontact.council_district.value_counts()
a.split(',')
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']]) $ results = lm.fit() $ results.summary()
print df.set_index(['date', 'item']) $
np.exp(-1.9967), np.exp(-0.0408), np.exp(0.0099)
pair.tail(10)
fix_space_0 = lambda x: pd.Series([i for i in reversed(x.split(' '))])
df.pupper.value_counts()
pumashp = pumashp.to_crs(epsg=2263) $ pumashp.crs
CONFIG_PATTERN = 'https://api.themoviedb.org/3/movie/550?api_key={key}' $ url = CONFIG_PATTERN.format(key=tmdb.API_KEY) $ r = requests.get(url) $ config = r.json()
store_items.loc[['store 1']]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(groceries.loc[['eggs', 'apples']]) $ print(groceries.iloc[[2, 3]])
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df.shape
test[['id','visitors']].to_csv('submission_avg.csv',index=False)
selected_df = clean_df[clean_df["summary_count"]>3] $ selected_df.head()
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-(0.05))# Tells us what our critical value at 95% confidence is
small_train['user_id'].nunique()
service_path = 'https://internal-nginx-svc.ibm-private-cloud.svc.cluster.local:12443' $ ml_repository_client = MLRepositoryClient()
df.to_csv('GageData.csv',index=False)
np.std(p_diffs)
import pyspark.sql.functions as func $ hashed_test.groupBy().agg(func.min(col('id'))).show()
df2.head()
train_visitor_map.head()
data = {'empID':  [100,      101,    102,      103,     104], $         'name':   ['Alice', 'Bob',  'Charles', 'David', 'Eric'], $         'year':   [2017,     2017,   2017,      2018,    2018], $         'salary': [40000,    24000,  31000,     20000,   30000] } $ pd.DataFrame (data, index=['r1','r2','r3','r4','r5'])
top_songs['Artist'].isnull().sum()
popCon[popCon.content == 'photo'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
cols = ['chanel', 'retention'] $ users_visits = users_visits[cols] $ users_retention_per_chanel_path =  output_path + '\\users_retention_per_chanel.csv' $ users_visits.to_csv(users_retention_per_chanel_path)
snow.select("select count(patient_id) from st_rvo_index where left(patient_id, 5) = 'XXX -'")
ti_suning.rename(columns={'review_image':'image_url','harvest_product_description':'product_description','retailer_product_code':'rpc','user_id':'username'}, inplace=True) $ ti_suning['store'] = 'Suning' $ ti_suning = ti_suning[ti_clm] $ ti_suning.shape
print(pandas_list_2d.iloc[[0]])
titanic.head()
import os $ file_list = os.listdir("C:\\Users\\janney.zhang\\Desktop\\work\\projects\\Nestle\\2018\\reviews\\April") $ file_list
term_freq_df.columns = ['negative','neutral','positive'] $ term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['neutral'] + term_freq_df['positive'] $ term_freq_df.sort_values(by='total', ascending=False).iloc[:10]
with open ('Hospital_List', 'rb') as fp: $     abc = pickle.load(fp) $
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC - SAINT MARYS HOSPITAL - 2011 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC - SAINT MARYS HOSPITAL - 2011 Percentiles.txt"
[md.trn_ds[0].text[:12]]
my_columns = list(data.columns) $ my_columns
result.summary()
P.plot_1d('scalarCanopyTranspiration')
small_f = urllib.request.urlretrieve (ml_1m_url, ml_1m_path)
df_img_algo_clean.info() 
solar_wind_df.loc[3080:3085]
model = MixedInputModel(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)
users= pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv' ) $ sessions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv' ) $ products =pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv' ) $ transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv') $
weather_df_byday = weather_df.loc[weather_df.index.weekday == weekday] $ weather_df_byday.info() $ weather_df_byday.head()
news_df = pd.DataFrame(news_list)
merged1['DaysFromAppointmentCreatedToVisit'] = (merged1['AppointmentDate'] - merged1['AppointmentCreated']).dt.days
stream_measures.columns = ['stream', 'min_stream_occurances', 'max_stream_occurances', 'mean_occurances']
tweets_clean.info()
outfile = path + '../output/allData_NoRetweets_May27_Cleaned.csv' $ cleanedDataNoRetweets.to_csv(outfile, index=False, encoding='utf-8')
df.columns
(autos['date_crawled'] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
data.info()
autos = autos.drop(['seller', 'offer_type', 'num_photos'], axis=1) $ autos.columns
result = pd.merge(df, leaderboard, on='ref') $ result.head()
w = 'soros' $ model.wv.most_similar (positive = w)
with open('incidents092815.pkl') as f: $     incidents = pickle.load(f)
ad_group_performance['DayOfWeek'] = ( $     ad_group_performance['Date'].dt.dayofweek $ ) $ ad_group_performance
google = data.DataReader(name='GOOG', data_source='iex' $                         , start='2016-01-01', end='2018-05-01') $ google.head()
for column in df.columns: $     print column, df[column].isnull().sum()
jobs.loc[(jobs.FAIRSHARE == 132) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
tweet_full_df['rating_numerator']=tweet_full_df['rating_numerator'].astype(float)
bacteria2.index = bacteria.index $ bacteria2
df_new['country'].unique()
result = Geocoder.geocode("7250 South Tucson Boulevard, Tucson, AZ 85756")
bnbAx.first_browser.value_counts()
actual = y_test $ pd.value_counts(actual).plot.bar()
df.corr()
abr = AdaBoostRegressor(n_estimators = 100, $                         learning_rate = 0.1, $                         loss = 'linear', $                         random_state = 2) $ scoring(abr)
churned_ordered['end_date'] = pd.to_datetime(churned_ordered_end_date).strftime('%Y-%m')
gcv.best_score_
print(np.shape(lats), np.shape(lons))
text_classifier.set_step_params_by_name("text1_char_ngrams", ngram_range =(3,4), use_idf = False) $ text_classifier.get_step_params_by_name("text1_char_ngrams")
converted_rate = df2['converted'].mean() $ print('converted_rate: ', converted_rate)
df_worst_chart.pivot_table(values='Updated Shipped diff_normalized',columns='Place Name',index='Shipped At').plot()
rate["answer"] = pd.to_numeric(rate["answer"]) # Converting the answer Series to numeric
archive_clean.sample(5)
df['Complaint Type'].value_counts()  #Categories
df.shape[0]
aTL[['system.record_id','system.created_at','APP_LOGIN_ID','APP_PRODUCT_TYPE','BRANCH','AREA_MANAGER','REGIONAL_MANAGER']].to_excel(cwd + '\\CA TL 0521-0526.xlsx', index=False) $ aSL[['system.record_id','system.created_at','APP_LOGIN_ID','APP_PRODUCT_TYPE','BRANCH','AREA_MANAGER','REGIONAL_MANAGER']].to_excel(cwd + '\\CA SL 0521-0526.xlsx', index=False)
filecounts = pd.Series(collections.Counter([t[1] for t in allfiles()]), name='datafiles')
datetime.date.today()
df_new['country'].value_counts()
reddit['Late Night Hours'] = reddit['Hours'].apply(lambda x: 1 if x<3 and x>22 else 0)
pd.options.display.float_format = '{:.6f}'.format
dates = pd.date_range('1950-01', '2013-03', freq='M') $ ts =pd.DataFrame(np.random.randn(758, 4), columns=list('ABCD'), index=dates) $ ts['year'] = ts.index.year $ ts.drop('year', axis=1).cumsum().plot(figsize=(10, 6))
bm2 = list(map(int, benchmark21)) $ print('benchmark2 score is: {}'.format(np.around(f1_score(actual1, bm2, average='weighted'), decimals=5))) $
train.loc[train['price_per_bedroom']==np.inf, 'price_per_bedroom'] = train.loc[train['price_per_bedroom']==np.inf, 'price'] $ train.loc[train['price_per_bathroom']==np.inf, 'price_per_bathroom'] = train.loc[train['price_per_bathroom']==np.inf, 'price'] $ test.loc[test['price_per_bedroom']==np.inf, 'price_per_bedroom'] = test.loc[test['price_per_bedroom']==np.inf, 'price'] $ test.loc[test['price_per_bathroom']==np.inf, 'price_per_bathroom'] = test.loc[test['price_per_bathroom']==np.inf, 'price']
for id_never_moved in Del_list: $     df_new = df[df['id'] == id_never_moved] $     frames = [df_never_moved, df_new] $     df_never_moved = pd.concat(frames)
baseball[['player','sb','cs']].sort_values(ascending=[False,True], $                                            by=['sb', 'cs']).head(10)
restaurants.describe()
rounds['announced_on'] = pd.to_datetime(rounds['announced_on'], errors = 'coerce')
pred = pd.read_table('image-predictions.tsv', sep = '\t') $ pred.info()
for i in range(-5, 0, 1) : $     data[f'High {i}d'] = data['High'].shift(-i) $ data = data.dropna() $ data.head()
s.resample('Q').head()
raw_data.info()
intake.shape
df.groupby(by = ['Item Description']).sum().sort_values(by = ['Sale (Dollars)'], ascending = False).head()
cat Data/microbiome_missing.csv
for key,value in df.iteritems(): $     print(key,value)
s = sm.tsa.seasonal_decompose(working_data.Weighted_Price.values, freq=60)
data.phone.value_counts()
strategy.trades(start_date = '2018-03-23', end_date = '2018-03-24')
dataset=pd.read_csv(train_path) $
!curl -X POST -F image=@$(pwd)/images/dog.jpg 'http://localhost:8080/predict'
display(data.head())
print("The probability that an individual received the new page is: {}".format((df2['landing_page'] == 'new_page').mean())) $
df.drop_duplicates(subset=['first_name', 'last_name'], keep='last') $
samp_size = n $ joined_samp = joined.set_index("Date")
predictions.select("predictedLabel").groupBy("predictedLabel").count().show(truncate=False)
def load_tweets(path): $     with open(path, "r") as f: $         example_tweets = json.load(f) $     return example_tweets $
i_unique_user = df.user_id.nunique() $ i_unique_user
to_be_predicted_Day2 = 14.52028076 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df_train.head()
df_image_clean.duplicated('tweet_id').value_counts()
cityID = 'c0b8e8dc81930292' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baltimore.append(tweet) 
df2 = pd.read_csv('ab_cleandata.csv')
wrd.sample(3)
htmldmh.args.addtable
cursor.execute(sq83) $ cursor.execute(sq84) $ results = cursor.fetchall() $ results
df2['intercept'] =1 $ df2['ab_page'] =pd.get_dummies(df2['landing_page'])['new_page'] $ df2.head()
df1.io_state.apply(lambda x: x.zfill(8))[70:800]
Google_stock['Adj Close'].describe()
today = datetime(2014,11,30) $ tomorrow = today + pd.Timedelta(days=1) $ tomorrow
proj_df['Project Subject Category Tree'].unique()
ts3 = pd.Series(np.random.randn(100), index = pd.date_range('1/1/2003', periods=100)) $ ts3 = ts3.cumsum() $ ts3.plot() $ plt.show()
df2['intercept'] = 1 $ df2[['new_page','old_page']] = pd.get_dummies(df2['landing_page']) $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()    
df2_treatment = df2.query('group == "treatment"') $ df2_treatment.converted.mean()
from sklearn.cross_validation import train_test_split $ SEED = 2000 $ x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.10, random_state=SEED) $ x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)
treatment_df = df2.query('group == "treatment"') $ p_treatment = treatment_df.query('converted == 1').user_id.nunique()/treatment_df.user_id.nunique() $ print('Probability of conversion for the treatment group is {}.'.format(round(p_treatment,4)))
authors.head()
with_countries['UK_ab_page'] = with_countries['ab_page']* with_countries['UK'] $ uk_new_page = sm.Logit(with_countries['converted'], with_countries[['intercept', 'ab_page', 'UK_ab_page', 'UK']]).fit() $ print(uk_new_page.summary())
print(nba_df.columns.values.tolist())
f = open("json_example.json","w") $ json.dump(obj, f) $ f.close()
departure_datetimes = pd.to_datetime(df2['FlightDate'] + ' ' + df2['DepTimeStr']) $ df2['DepDateTime'] = departure_datetimes
m.fit([inputs], targets, epochs=1, batch_size=64, validation_split=0.1)
from datetime import datetime, date, time $ dt = datetime(2011, 10, 29, 20, 30, 21) $ dt.day $ dt.minute
dfBill = df[df['Memo'].apply(returnCategory)=="Bill Payment"]
X_train.shape
%time ADM.read(vars_to_read=['ec550aer'])
google['high'].apply(custome_roune).plot(kind='hist', bins=6)
fdist.plot(100, cumulative=True)
pf_values = res.map(lambda r: Row(date=r.date, $                                   neutral=r.neutral*r.qty, $                                   scenarios=DenseVector(r.scenarios.array * r.qty))) $ aaa = pf_values.map(lambda x: (x[0], (x[1],x[2]))).aggregateByKey(0, lambda v, d: d, lambda x,y: (x[0]+y[0], x[1]+y[1])).map(lambda r: Row(date=r[0], neutral=r[1][0], scenarios=r[1][1]))
del scaled['Close'] $ scaled.head()
grouped_publications_by_author['authorId'].nunique()
val_df.head(1)
df_n = df2.query('landing_page == "new_page"') $ df_n.shape[0]
b_cal_q1.dtypes
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s') $ git_log.describe() $
who_purchased.to_csv('../data/purchase.csv')
Brooklyn_gdf = newYork_gdf[newYork_gdf.boro_name == 'Brooklyn'] $ Brooklyn_gdf.crs = {'init': 'epsg:4326'}
test_data_features = vectorizer.transform(test.review) $ test_data_features = test_data_features.toarray()
df_protest.rename(columns={'issid': 'id'}, inplace=True)
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis=1) $ df2.head()
cityID = '6a0a3474d8c5113c' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         El_Paso.append(tweet) 
datacamp.groupby([datacamp['publishdate'].dt.year, datacamp['publishdate'].dt.month]).size().plot(kind='bar', figsize=(15,7), color='b') $
sub_df = pd.concat([sub_df, doc_top_mat], axis=1)
model_lm = LogisticRegression() $ model_lm.fit(X_train, y_train) $ y_preds = model_lm.predict(X_test) $ confusion_matrix(y_test, y_preds)
conn.close()
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=10)
house_data.describe()
days = [] $ for day in range(0, num_days_online): $     days.append(day) $ days[0:10]
keys.shape
pd.to_datetime('2018-4-14')
plt.plot(df_ror_1) $ plt.hold $ plt.plot(df_ror_2) $ plt.show;
i = starting_index +1 $ state_now = pi_mat[i , :] $ for t in range(0,predict_period): $     vol[t,0] = np.inner(np.matmul(state_now, np.linalg.matrix_power(A, t+1)),s[1,:])
new_page_converted = np.random.binomial(n_new , p_new)
S_1dRichards.basin_par.filename
(act_diff < p_diffs).mean()
import sklearn $ sklearn.__version__
df_vals.groupBy('date').agg({'neutral': 'sum'}).collect()
events_pd['just_date'] = events_pd['event_date'].dt.date $ events_pd['just_date'].value_counts().head(10).plot.bar()
train_ratio = 0.75 $ train_size = int(samp_size * train_ratio); print(train_size) $ val_idx = np.flatnonzero( $     (df.index<=datetime.datetime(2014,9,17)) & (df.index>=datetime.datetime(2014,8,1)))
def print_tweet(tweet): $     print ("@%s - %s (%s)" % (tweet.user.screen_name, tweet.user.name, tweet.created_at)) $     print (tweet.text) $ tweet=results[0] $ print_tweet(tweet)
IndianaNews = ExponentStories.append(JournalStories)
tweet_data.sample(20)
df.describe()
train.columns.values
vec_code = modal_model.predict(encoder_model.predict(source_proc.transform(source_docs))) $ vec_code.shape
thecmd = 'ogr2ogr -f "CSV" ' + dataDir + 'input/new-york_new-york_points.csv ' + dataDir + 'input/new-york_new-york.db -lco GEOMETRY=AS_XY -progress -explodecollections -sql "select * from points"' $ print thecmd
with tf.variable_scope('Optimizer', reuse=tf.AUTO_REUSE): $     train_op_model = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=model_vars) $     train_op_elmo = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars) $     train_op_elmo_coef = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_coef) $     train_op_elmo_cell_weights = get_train_op(loss, learning_rate_ph, tf.train.AdamOptimizer, clip_norm=clip_grad_norm, trainable_vars=elmo_vars_cell_weights)
train_df1 = train_df.copy() $ train_df1["created"] = pd.to_datetime(train_df1["created"]) $ train_df1['month'] = train_df1['created'].dt.strftime('%b')
data_dict = r.json() $ print(type(data_dict))
learn.sched.plot()
old_page_converted = df2.sample(n_old, replace=True).converted $ p_old = sum(old_page_converted)/old_page_converted.shape[0] $ p_old
tweet_df = pd.read_csv('../data/twitter_personality/tweets_full_parsed.csv', lineterminator='\n')
!wget http://ekpwww.ekp.kit.edu/~tkeck/realdonaldtrump.csv
Today = pd.to_datetime('now') $ YrFromToday = Today - dt.timedelta(days=365) $ print(YrFromToday)
giss_temp.fillna(method="ffill").tail()
technique_name = lift.get_technique_by_name('Rundll32')
def is_palindrome(s): $     return(str(s) == str(s)[::-1]) $ p=girls.Name.copy().map(is_palindrome).values $ girls.loc[p,:].groupby(['Name']).agg('median').sort_values('Fraction',ascending=False).sample(10)
stock_data.index=pd.to_datetime(stock_data['latestUpdate']) $ stock_data['latestUpdate'] = pd.to_datetime(stock_data['latestUpdate'])
%matplotlib inline $ pandas_df.plot.bar()
df['time open'].describe()
blame.head()
merged = merged.set_index('DateTime')
df_tweet.info()
train.reset_index(inplace=True) $ test.reset_index(inplace=True) ##Note reset_index add another column called 'index with the old index cols $ train.head(5)
np.exp(result_countries_new.params)
retweet_columns = ['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'] $ twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()] $ twitter_archive_clean = twitter_archive_clean.drop(columns=retweet_columns)
twitter_archive_enhanced[twitter_archive_enhanced.rating_denominator != 10][['rating_numerator','rating_denominator','text']]
from sklearn.metrics import f1_score $ from sklearn.metrics import confusion_matrix
act_diff = prop_treat - prop_cntrl $ (p_diffs > act_diff).mean()
print bnb.first_affiliate_tracked.shape $ print pd.Series(bnb.first_affiliate_tracked).value_counts()
dfdaycounts = dfdaycounts.sort_values(['created_date'])
def combine_names(row): $     if row.contributor_fullname.startswith('SEAN PARKER'): $         return 'SEAN PARKER' $     return row.contributor_fullname
df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page == "new_page")')['user_id'].count()
df_piotroski = pd.DataFrame({'id' : stock_id_ls, 'name': stock_name_ls}, columns = ['id', 'name', 'market_type', 'quant', 'market_sum', 'property_total', 'debt_total', 'listed_stock_cnt', 'pbr', 'face_value',])
dt_1 = pd.datetime(2016, 1, 1)
df2[df2['converted'] == 1].user_id.count() / df2.user_id.count()
import datetime $ Trump["created_time"] = [datetime.datetime.strptime(Trump["created_time"].iloc[i],"%Y-%m-%d %H:%M:%S") for i in xrange(Trump.shape[0])] $ Trump["year"] = [Trump["created_time"].iloc[i].year for i in xrange(Trump.shape[0])] $ Trump["week"] = [Trump["created_time"].iloc[i].week for i in xrange(Trump.shape[0])]
wrd_clean = wrd.copy() $ wrd_clean['name'] = wrd_clean['name'].apply(lambda x: 'NaN' if x == "a" else x) $ wrd_clean['name'] = wrd_clean['name'].apply(lambda x: 'NaN' if x == "an" else x)
total_differences = df.sales - df.new_sales # create a series of all of the differences $ random_differences = generate_random(total_differences) $ random_differences
n_old = gb_page.loc['old_page'][0] $ n_old
data.head(50)
fda_drugs.head()
count_non_null(geocoded_df, 'Judgment.In.Favor.Of')
df2['landing_page'].mean()
sorted(Counter(reducedwordlist).items(),key=lambda wordtuple: wordtuple[1],reverse=True)[:30]
movie1=ratings[ratings['movieId'] == 1] $ movie1['ceiled_ratings'] = movie1['rating'].apply(np.ceil)
to_be_predicted_Day4 = 17.79222525 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
data.to_csv('TwitterData.csv')
staff.values
df.shape # output is in format of rows, columns
text1.similar("monstrous")
import matplotlib.pyplot as plt $ df[['Date','GasPrice']].set_index('Date').plot()
results.summary2()
from gensim.models import KeyedVectors $ fr_embeddings = KeyedVectors.load_word2vec_format("/tmp/wiki.multi.fr.vec") $ en_embeddings = KeyedVectors.load_word2vec_format("/tmp/wiki.multi.en.vec")
X_test[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True, $                                              layout = (1, 3), sharex = False)
print(autos.price.describe().apply(lambda x: format(x, 'f'))) #lambda to supress scientific notation convertion $ print(autos.price.value_counts().head(10).sort_index(ascending=True)) $ print(autos.odometer_km.describe()) $ print(autos.odometer_km.value_counts().head(10).sort_index(ascending=True))
autos["registration_year"].describe()
index = similarities.MatrixSimilarity(doc_vecs, $                                       num_features=topics) $ sims = sorted(enumerate(index[doc_vecs[6]]), key=lambda item: -item[1])
new_page_converted = np.random.binomial(1,Pnew,Nnew) $ new_page_converted
import numpy as np $ Z = np.random.random(30) $ print(Z) $ print(Z.mean())
processed_tweets= pd.concat([pd.DataFrame({'tweetID':tweets.tweetID, 'tweetText':tweets.tweetText,'polarity_value':tweets.polarity_value, 'set':tweets.set}), $                              pd.DataFrame({'tweetID':train_set.tweet_id, 'tweetText':train_set.tweetText, 'polarity_value':train_set.polarity_value,'set':train_set.set})], ignore_index=True) $ processed_tweets['processed_tweet'] = processed_tweets.tweetText $ processed_tweets.sample(4)
iris.head()
timelog.info()
import matplotlib.pyplot as plt $ plt.scatter(rets.FSLR,rets.TAN)
print("MSE:", metrics.mean_squared_error(stock.target, xgb.predict(stock.drop(['target', 'close', 'open'], 1))))
invalid_name_list = twitter_archive_full[twitter_archive_full['name'].str.islower()].name.unique() $ twitter_archive_full.loc[twitter_archive_full.name.isin(invalid_name_list), 'name'] = 'None'
to_be_predicted_Day2 = 31.22544606 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df2.query('converted == 1').user_id.nunique() / len(df2)
df_old = df2[(df2['landing_page'] == 'old_page')] $ converted = df_old['converted'] $ old_page_converted = np.random.choice(converted, n_old).mean() $ old_page_converted
BID_PLANS_df.loc['f6dd6544']
df_draft.to_csv('1976_to_2015_Draftees.csv')
d = 30 $ df['date_after_30d'] = df['datetime'].apply(lambda x: x + pd.Timedelta(d, unit='d')) $ print(df.to_string())
%%bash $ cd 1st_flask_app_1/ $ python3 app.py
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir(str(today)) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END)
np.mean(df2.ltv)
tf.value_counts()
candidates = pd.read_csv("https://calaccess.download/latest/Candidates.csv")
df.head()
df_series = pd.Series(train_frame["values"], name='values',index=train_frame["timestamp"])
all_years_by_DRG =Grouping_Year_DRG_discharges_payments['discharges'].groupby(level=['year','drg3']).sum() $ all_years_by_DRG.tail() 
autos.odometer_km.unique().shape
logit2 = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ result = logit2.fit() $ result.summary2()
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35, 'shirts': 15, 'shoes':8, 'suits':45}, $ {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5, 'shirts': 2, 'shoes':5, 'suits':7}, $ {'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4, 'shoes':10}] $ store_items = pd.DataFrame(items2, index = ['store 1', 'store 2', 'store 3']) $ store_items
train.StateHoliday.head()
%%time $ df['created_at']= pd.to_datetime(df['Created Date'], format="%m/%d/%Y %X %p")
userActivity = userArtistDF.groupBy("userID").sum("playCount").collect() $ pd.DataFrame(userActivity[0:5], columns=['userID', 'playCount'])
featured_img_url = "https://www.jpl.nasa.gov" + current_img_url $ featured_img_url
df = pd.DataFrame([[1, np.nan, 2], $                   [2, 3, 5], $                   [np.nan, 4, 6]]) $ df
import numpy as np $ dist = np.sum(train_data_features, axis=0) $ for tag, count in zip(vocab, dist): $     print(count, tag)
url_PHI = "https://manage.strmarketplace.com/Images/Teams/PhiladelphiaEagles/SalesData/Philadelphia-Eagles-Sales-Data-PSLs.xls"
lowest_temp_recorded = session.query(func.min(Measurement.tobs)).filter(Measurement.station=='USC00519281').group_by(Measurement.station).all() $ lowest_temp_recorded $
grpConfidence['MeanFlow_cfs'].count()
orgs.loc[0]
n_old = len(df2.query('group == "control"')) $ print(n_old)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='two-sided') $ print(str(z_score) + ", "+ str(p_value)) $ print("The significance of our z-score is " + str(norm.cdf(z_score))) $ print("Critical Z-score value at 95% confidence is " + str(norm.ppf(1-(0.05/2))))
print 'A Datetime index range selection:' $ msft.loc['2012-01-03':'2012-01-5']
n_old = df2.loc[(df2.landing_page == "old_page")].user_id.nunique() $ n_old
df3=df2 $ df3['intercept'] = 1 $ df3[['control','ab_page']] = pd.get_dummies(df2['group']) $ df3.head()
sandwich_ratings.head(3)
dfFull['OverallCondNorm'] = dfFull.OverallCond/dfFull.OverallCond.max()
np.r_[np.random.random(5), np.random.random(5)]
elms_all_0611.loc[range(1048575)].to_excel(cwd+'\\ELMS-DE backup\\elms_all_0611_part1.xlsx', index=False)
df_countries = pd.read_csv('countries.csv') $ df_countries.head()
for column in has_stage_archive.iloc[:,8:12].columns: $     avg_favorite_count = has_stage_archive[has_stage_archive[column] == 1]['favorite_count'].mean() $     print('The average favorite count for {}s is {}'.format(column, avg_favorite_count))
trans_counts.keys()
df2[['CA', 'UK']] = pd.get_dummies(df2.country)[['CA', 'UK']]
df_vow.plot()
sum((df2.group == 'treatment')&(df2.converted == 1)) / sum(df2.group == 'treatment')
AFX = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=7T7et9e5SeTEaGxhEFsz') $ frame = AFX.json() $ frame $ frame[u'dataset'][u'data'][0]
autos = autos.loc[(autos["registration_year"] > 1900) & (autos["registration_year"] < 2006)] $ autos["registration_year"].value_counts(normalize=True)
r.json()
tablename='gateways' $ pd.read_csv(read_inserted_table(dumpfile, tablename), $             delimiter=",", $             error_bad_lines=False).head(10)
scr_retention_sum = pd.Series([sum(scr_retention_df[col]) for col in scr_retention_df.columns],index=scr_retention_df.columns,name='Total')
train = train.drop(['Unnamed: 0'], axis=1)
(df2.shape[0])
sp.head()
data['subreddit'].value_counts()
model.score(X, y)
cust_demo.columns
atloc_opp_loc_valid_responses = ['Valid Response'] $ atloc_opp_loc_valid_count_prop_byuser = compute_valid_count_prop_byuser(atloc_opp_loc, users_opp_loc, 'vendorId', $                                                                          'remappedResponses', atloc_opp_loc_valid_responses) $ atloc_opp_loc_valid_count_prop_byuser.head()
closePrice.plot()
sets_node['date'][DATA].dropna().head(12)
scores = cross_val_score(model, X_train, y_train, cv=5)  # cross Validation $ np.mean(scores), np.std(scores) #  scoring the performance of training data
df_onc_no_metac = df_onc.drop(columns=ls_metac_colnames) $ df_onc_no_metac = df_onc_no_metac.rename(columns = {'METAC_SITE_NM1': 'METAC_SITE'})
model.fit(X_train, y_train)
dummy_var_df = pd.DataFrame(dummy_var_df)
url_customers = 'https://ibm.box.com/shared/static/gwak77ibs1zy7i5foza03f9u7zcdgn6j.csv' $ url_transactions = 'https://ibm.box.com/shared/static/zjx66wjdtl02gi9rr2nmal2po4eqaxtc.csv'
train_set = train_set.loc[(train_set.polarity_value == 'P') | (train_set.polarity_value == 'N') ]
if 'TRAVIS' in os.environ: $     df.loc[:500].to_csv('movie_data.csv') $     df = pd.read_csv('movie_data.csv', nrows=500) $     print('SMALL DATA SUBSET CREATED FOR TESTING')
SVPOL(data/'realdata'/'MM.dat').to_dataframe().head()
donors_c.iloc[2097169, :] $
tempXxgb = tempX.copy() $ tempXxgb = tempXxgb.rename(columns={'fit': 'fit_feat'})
all_transactions = all.groupby('user_id')['course'].apply(lambda x: list(x)).tolist() $ all_rules = apriori(all_transactions, min_support = 0.01, min_confidence = 0.5, min_lift = 1.1, min_length = 2) $ all_results = list(all_rules) $ all_results
results.summary()
sns.factorplot(data=tweets_df, x="created_at", y="retweet_count", kind="box")
lr.fit(X, y)
train_session.isNDF.value_counts()/train_session.shape[0]
df_sb['Sentiment_class'] = df_sb.apply(conditions, axis=1) $ df_sb.to_csv("sobeys_senti_score.csv", encoding='utf-8', index=False) $
details.dtypes
from sklearn.preprocessing import Imputer
rng_pytz = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='Europe/London')
to_be_predicted_Day4 = 52.53461933 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
LSTM_training_inputs = [np.array(LSTM_training_input) for LSTM_training_input in LSTM_training_inputs] $ LSTM_training_inputs = np.array(LSTM_training_inputs) $ LSTM_test_inputs = [np.array(LSTM_test_inputs) for LSTM_test_inputs in LSTM_test_inputs] $ LSTM_test_inputs = np.array(LSTM_test_inputs)
twitter_archive_master.sort_values(by=['retweet_count'], ascending = False).head()[['created_at','favorite_count','retweet_count','name','url','rating_num','dog_stage','breed' ]]
problems = pd.DataFrame(problems, columns=['srx', 'srr'])
aldf.head()
vis = sercRefl[:,:,57] $ nir = sercRefl[:,:,89] $ ndvi = np.divide((nir-vis),(nir+vis))
building_pa_prc_zip_loc.to_csv("buildding_03.csv",index=False) $ building_pa_prc_zip_loc=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date'])
df['log_EBAY']=np.log(df['NASDAQ.EBAY']) $ Ebay_array=df["log_EBAY"].dropna().as_matrix() $ df['diff_log_EBAY']= df["log_EBAY"]-df["log_EBAY"].shift(periods=-1) $ model_Ebay = ARIMA(Ebay_array, (2,2,1)).fit() $ predEbay=model_Ebay.predict() $
confusion = pd.DataFrame({"Predicted": test_labels, "Actual":test_labels })
d = {'Flavor': ['Strawberry', 'Vanilla', 'Chocolate'], 'Price' : [3.50, 3.00, 4.25]} $ icecream = pd.DataFrame(data=d) $ icecream
df_concat_2.hist(column="message_likes_rel",by = "page", bins=20)
rows_exp = parts.map(parse_explicit) $ df_exp = sql.createDataFrame(rows_exp)
df.iloc[:,1:3] = df.iloc[:,1:3].apply(pd.to_datetime, errors='coerce') $ df['Trip_duration']=df['Lpep_dropoff_datetime']-df['lpep_pickup_datetime']
df_final = pd.concat([df_us_, df_cat, df_crea, df_loc], axis=1, join='inner')
df.info()
df.columns
print('There are {} datasets'.format(len(dsdf)))
for col in list(df.columns) : $     k = sum(pd.isnull(df[col])) $     print(col, '{} nulls'.format(k))
df_combined = pd.merge(df_clean, df_predictions_clean,on='tweet_id', how='inner') $ df_combined = pd.merge(df_combined, df_tweet_clean,on='tweet_id', how='inner') $
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ results = mod.fit() $ results.summary()
import spacy $ import en_core_web_sm $ nlp = en_core_web_sm.load()
df_vow.head()
for ix, file in enumerate(s3_files): $     os.environ["gs_key"] = "gs://resource-watch-public/" + s3_key_edits[ix] $     os.environ["asset_id"] = "users/resourcewatch/" + file[:-4] + "_edit" $     !earthengine upload image --asset_id=$asset_id $gs_key
vals1.sum()
final_topbikes['Distance'].count() * 1.5
df_joined['intercept'] = 1 $ logit_mod = sm.Logit(df_joined['converted'],df_joined[['intercept','UK','CA']]) $ results_2 = logit_mod.fit() $ results_2.summary()
data[['Sales']].resample('D').mean().rolling(window=200, center=True).mean().plot()
print(dfd.in_pwr_5F_max.describe()) $ dfd.in_pwr_5F_max.hist()
joblib.dump(full_data, 'pickles/full_data.pkl') $ joblib.dump(cleaned_synops, 'pickles/cleaned_synops.pkl') $ joblib.dump(cluster_names, 'pickles/cluster_names.pkl')
ab_df2.isna().sum()
gbc = ensemble.GradientBoostingClassifier(n_estimators=300) $ gbc.fit(X, y) $ cm = confusion_matrix(y, gbc.predict(X)) $ sns.heatmap(cm, annot=True)
finalDf['kmLabels'] = km_res.labels_ $ finalDf.head()
plt.plot(dataframe.groupby('day_of_week').daily_worker_count.mean()) $ plt.show()
df_questionable_3[df_questionable_3['state_IL'] == 1]['link.domain_resolved'].value_counts()
observed_difference = len(df2.query('group == "control" and converted == 1')) / len(df2.query('group == "control"')) - len(df2.query('group == "treatment" and converted == 1')) / len(df2.query('group == "treatment"')) $ observed_difference
type(r.json())
ngrams_summaries = cvec_2.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
df.iloc[1,2:3]   # What it returned?
df_dirty['body_length'].hist(range = (0,100))
twitter_ar.text[1]
train.head()
top_chater = non_na_df['sender_card'].value_counts() $ top_chater.nlargest(20).plot(kind='bar', figsize=(14,6))
pd.read_sql('SELECT * FROM experiments WHERE irradiance = 700 ORDER BY temperature', conn, index_col='experiment_id')
users = df2.nunique()['user_id'] $ sessions = df2.count()['user_id'] $ print('Now, in df2 we have {} unique users and a total of {} sessions.'.format(users,sessions))
ts.index[0]
Z = np.random.uniform(-10,+10,10) $ print (Z) $ print (np.abs(Z)) $ print (np.ceil(np.abs(Z))) $ print (np.copysign(np.ceil(np.abs(Z)), Z))
df['release_date'] = pd.to_datetime(df['release_date']) $ df['budget_adj'] = df['budget_adj'].astype(int) $ df['revenue_adj'] = df['revenue_adj'].astype(int) $ df['budget'] = df['budget'].astype(int) $ df['revenue'] = df['revenue'].astype(int)
X_valid.shape
rspm = aqmdata['rspm'].value_counts() $ rspm.iloc[0:10]
model_w.summary2() # For categorical X.
df2[df2.duplicated(['user_id'])]
f.shape
both_dfs = pd.concat(list_of_df)
autos["odometer_km"].unique().shape $ autos["odometer_km"].head(200).value_counts() $ autos["odometer_km"].sort_values(ascending=True) $ autos["odometer_km"].describe() $ autos["odometer_km"].value_counts()
weekly_cases = daily_cases.unstack().T.resample('W').sum() $ weekly_cases
from sklearn import datasets $ iris = datasets.load_iris() $ iris_df = pd.DataFrame(iris['data'], columns=iris['feature_names']) $ iris_df['species'] = iris['target'] $ iris_df.head()
df.drop(["last_event"], axis = 1, inplace=True) $ query_date = dt.date(2018,1,23) #this is when this particular dataset was generated, so we will use it to calculate group age $ df["created"] = [d.date() for d in df["created"]] #we only need the date from the original column (ignore time) $ age = query_date - df["created"] #find the age in days by subtracting creation date from the date the dataset was generated $ df["event_freq"] = age.dt.days / df["past_event_count"].astype(float) #needs to be float for division
oil_interpolation.columns=['dcoilwtico'] $ pd.DataFrame.head(oil_interpolation)
sns.distplot(df['num_comments']) $ plt.title("Distribution - Number of comments");
from statsmodels.tsa.stattools import adfuller as ADF $ print() $ print(ADF(resid_713.values)[0:4]) $ print() $ print(ADF(resid_713.values)[4:7])
from sklearn.ensemble import RandomForestClassifier $ from sklearn.model_selection import cross_val_score, StratifiedKFold
(np.multiply(validation_labels[:1] , -100) + val_prediction[:1]).min(axis=1) 
ffr.rolling(window=7).max().head(10)
df.head()
df2.converted.mean()
f.close() $ gfs.close()
newYorker = [user.iloc[i]["name"] for i in range(len(user)) $                                   if user.iloc[i]["location"] == "NYC"] $ newYorker[:10]
df2.set_index('user_id').index.get_duplicates()
data=[tweet.text for tweet in results]
print("Number of Relationships in PRE-ATT&CK") $ print(len(all_pre['relationships'])) $ df = all_pre['relationships'] $ df = json_normalize(df) $ df.reindex(['id','relationship', 'source_object', 'target_object'], axis=1)[0:5]
session.run(tf.global_variables_initializer())
dump = dump.dropna() # Drop all null values
breed_conf = df_twitter.groupby('p1')['p1_conf'].mean()
month.columns = ['MONTH_'+str(col) for col in month.columns]
sns.set_style('whitegrid') $ sns.distplot(data_final['countCollaborators'], kde=False,color="red", bins=25) $
disposition_df=pd.read_excel("Call Records.xlsx",sheetname="Disposition Definition",na_values=["NULL",""," "],keep_default_na=False).iloc[:,1:]
ls ../outputs/CSFV2_outlook_weekly_90th_per_summary_table_from*
df2.query('converted == 1').user_id.count()/df2.user_id.count()
v_item_hub.shape
dataset.head()
df2.sort_values('total_likes',inplace=True,ascending=False) $ top_likes = df2.head(10)['id']
joined.head().T.head(40)
df_new.to_csv(path_or_buf='data/df_fe_comma.csv', sep=',', $               header=True, index=False, $               encoding='utf-8')
treatment_cnv = df2.converted.mean() $ treatment_cnv
X_train_predict = model.predict(X_train) $ X_test_predict = model.predict(X_test)
interactions1Data = pd.read_pickle(folderData + 'interactions1Data.pkl')
p_diffs = [] $ for _ in range(10000): $     boot_new_page_converted=np.random.binomial(n=1,p=pnew,size = n_new) $     boot_old_page_converted = np.random.binomial(n = 1,p = pold, size =n_old ) $     p_diffs.append(boot_new_page_converted.mean() - boot_old_page_converted.mean())
data_df.head()
df_2008['bank_name'] = df_2008.bank_name.str.split(",").str[0] $
df2.orderBy('count', ascending=False).show()
df = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),columns=['a', 'b', 'c', 'd', 'e']) $ df.loc[0, 'a'] = "This is some text" $ df
docs = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names()) $ docs.sum()
d1 = dt.datetime(yr, mo, dd, hr, mm, ss, ms) $ d2 = dt.datetime(yr + 1, mo, dd, hr, mm, ss, ms) $ print(d2-d1)
tst_lat_lon_df.tail()
data_2017_12_14_iberia_negative = (data_2017_12_14_iberia.\ $                                    loc[data_2017_12_14_iberia.airline_sentiment == "negative"])["text_2"]
from oauth2client.service_account import ServiceAccountCredentials $ scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive'] $ credentials = ServiceAccountCredentials.from_json_keyfile_name('API Project-f22fe0b03992.json', scope)
print(any(df_new_test.UWI == '00/11-04-067-03W4/0'))
gbm_v1.score_history() $
oecd['year'] = pd.to_datetime(oecd.Date).apply(lambda x: x.year) $ oecd_year = oecd.set_index(oecd.Country.str.title())['year'].dropna() $ oecd_year
users.merge(sessions, left_on=['Registered','UserID'], right_on=['SessionDate','UserID'], how='inner')
uni_users = df2['user_id'].nunique() $ uni_users $ converts = df2['converted'].sum() $ proportion = converts / uni_users $ proportion
X_train[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True, $                                              layout = (1, 3), sharex = False)
data['comments'].describe()
temp_df = pd.concat([most_common_registered_addresses_for_no_psc_companies,temp_s],axis=1) $ temp_df.columns = ['number_no_psc','total_companies'] $ temp_df['proportion_no_psc'] = temp_df['number_no_psc'] / temp_df['total_companies'] $ temp_df[temp_df.total_companies > 100].sort_values(by='proportion_no_psc',ascending=False).head(10)
people = df.In.unique() $ people.tolist() $
print('Largest change bewteen any two days (consecutive) is:',np.diff(closep).max())
sp.head()
df_times.head() $
p_new=df2.converted.mean() $ p_new
df_master = pd.read_csv('twitter_archive_master.csv') $ df_master.head()
ttt = list(spp.term.unique())
merged_data['paid_status'].value_counts()
len(train_data[train_data.abtest == 'control'])
(np.asarray(p_diffs) > obs_diff).mean()
k1 = data[['cust_id','is_month_start','total_spend']].groupby(['cust_id','is_month_start']).agg('mean').reset_index() $ k1 = k1.pivot(index='cust_id', columns='is_month_start', values='total_spend').reset_index().replace(np.nan,0) $ k1.columns = ['cust_id', 'is_month_start0', 'is_month_start1'] $ train = train.merge(k1,on=['cust_id'],how='left') $ test = test.merge(k1,on=['cust_id'],how='left')
df_van_asn.head()
print("The loglikelihood estimations are below. \n ") $ print("Loglikelihood:\n",  loglikA) $
model.compile(loss='categorical_crossentropy', $               optimizer='adam', metrics=['accuracy'])
old_page_converted=np.random.choice([1,0],size=n_old,p=[pold,(1-pold)]) $ old_page_converted.mean()
breed_predict_df[breed_predict_df.tweet_id.duplicated()]
X_train, X_val, \ $ y_train, y_val = train_test_split(training_features, training_target, test_size=0.33, random_state=12)
all_features = train['feature_list'].str.cat(sep=',')
autos['brand'].unique().tolist()
print ('{} users unfiltered'.format(tidy.shape[0])) $ print ('{} users seen more than one day'.format(tidy[tidy['numberDays'] > 1].shape[0])) $ print ('{} users with less than 15 followers'.format(tidy[tidy['followers'] < 16].shape[0])) $ print ('{} users following less than 15'.format(tidy[tidy['following'] < 16].shape[0])) $ print ('{} users whose accounts are new (31 days)'.format(tidy[tidy['accountDurationDays'] < 32].shape[0]))
distinct_uname=[] $ for i in uname.drop_duplicates().values: $     distinct_uname.append((str('@'+i).replace("[u'","")).replace("']",''))
August_deaths_Sl = result_Sl.iloc[0]['Mean deaths'] $ September_deaths_Sl = result_Sl.iloc[1]['Mean deaths'] $ October_deaths_Sl = result_Sl.iloc[2]['Mean deaths'] $ November_deaths_Sl = result_Sl.iloc[3]['Mean deaths'] $ Desember_deaths_Sl = result_Sl.iloc[4]['Mean deaths']
data[data.close==0].sort_values(['order','lastAttacked'],ascending=[0,1])
df1.io_state[7], len(df1.io_state[7])
metadata['map_info'] = refl['Metadata']['Coordinate_System']['Map_Info'].value $ metadata
recommend.dtypes # Making sure I'm not crazy
autos.columns
df4["intercept"] = 1 $ logit_mod = sm.Logit(df4["converted"], df4[["intercept","CA","UK"]]) $ results = logit_mod.fit() $ results.summary()
autos["price_euro"].value_counts(normalize = True).sort_index(ascending = True).head(20)
twitter_df_clean = twitter_df.copy() $ image_pred_df_clean = image_pred_df.copy() $ tweet_json_df_clean = tweet_json_df.copy()
client.repository.list_experiments()
grid_search.best_estimator_
g.first()
df.head()
games_to_bet = pd.concat([y_test_over, y_test_under], axis=1)
brand_counts = autos["brand"].value_counts(normalize=True) $ common_brands = brand_counts[brand_counts > .05].index $ print(common_brands)
cserie(exploration_titanic.narows_full) # no rows of only missing values
BID_PLANS_df.head()
len(df2[df2['landing_page'] == 'new_page']) / df2.shape[0]
n_new = df2.loc[(df2.landing_page == "new_page")].user_id.nunique() $ n_new
plot_series_save_fig(series=doc_duration, figsize=(12,6), xlabel='Date', ylabel='Appointment Time (hours)',\ $                      plot_name='Doctors', figname='./images/doctors_weekly_time_series.png')
suspects_with_25_2['is_trip'] = suspects_with_25_2['timestamp'] - suspects_with_25_2['timestamp'].shift(1) > "12:00:00"
df.loc['20180103','A']
vectorizer.stop_words_
hpg_reserve= pd.merge(hpg_reserve, store_id_relation, how='inner', on=['hpg_store_id'])
collection.write('AAPL', snap_df, $                  metadata={'source': 'Quandl'}, $                  overwrite=True) $ df = collection.item('AAPL').to_pandas() $ df.tail()
y_preds = lgr_grid.best_estimator_.predict(X_test) $ lgr_scores = show_model_metrics('Logistic Regression', lgr_grid, y_test, y_preds)
pd.isnull(pd.read_csv("../data/microbiome/microbiome_missing.csv")).head(20)
Raw_Forecast["ID"] =  Raw_Forecast.Date_Monday.astype(str)[:] + "/" + Raw_Forecast.Product_Motor +"/" + Raw_Forecast.Part_Number.str[:] $ Raw_Forecast.head(10)
local.export_to_quilt(post_process_info["DatasetId"])
diff_p= new_page_converted.mean()-old_page_converted.mean() $ diff_p
toy = df.sample(frac=0.00005)
(v_item_hub.loc[:, item_hub.columns] == item_hub).sum()
train['Date'] = pd.to_datetime(train['Date']) $ test['Date'] = pd.to_datetime(test['Date'])
.groupby(level=0)\ $ ['discharges'].agg({'avg' : np.average, 'sum': np.sum}) $ df.apply(lambda x: x.head())
df3=pd.DataFrame({'key':['K0','K1','K4','K3'],'C':['C0','C1','C2','C3'],'D':['D0','D1','D2','D3']})
import re $ df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.replace('x', '') $ df.head()[['Product', 'Consumer complaint narrative']]
t0, t1 = lin_reg.intercept_, lin_reg.coef_ $ t0, t1
tweets_df_clean.columns
git_blame['age'] = pd.Timestamp('now') - git_blame.timestamp $ git_blame.head()
tweets_df_clean[tweets_df_clean['id'].duplicated()]
autos['date_crawled'].str[:10].head()
greaterpropdiff = p_diffs > act_diff $ greaterpropdiff.mean()
brand_info = pd.DataFrame(mean_mileage,columns=['mean_mileage']) $ brand_info
soup=bs(site.text,"html")
google_stock.describe()
from sklearn.linear_model import LogisticRegression $ from sklearn import metrics $ logreg = LogisticRegression() $ y = train.target.values $ logreg.fit(train1, y)
df_mysql = psql.read_sql('select * from test.test_table;', con=conn)
df = pd.read_csv('ZILLOW-Z49445_ZRISFRR.csv') $ df.head()
df1.append(df2).append(df3) $
history_df = pd.read_sql('measurement',engine) $ history_df['station'].value_counts()
import numpy as np $ data = {i : np.random.randn() for i in range(7)} $ data
transactions.merge(users,how="outer",on="UserID")
df_actor_name.head()
from sklearn.naive_bayes import MultinomialNB $ nb = MultinomialNB() $ nb.fit(X_train, y_train)
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop(['CA'], axis=1) $ df_new.head()
sub_df.groupby('Rating').size().plot(kind='bar') $ plt.title("Frequency of Review Rating") $ plt.xlabel("Rating") $ plt.ylabel("Number of reviews") $ plt.xticks(rotation=0)
tweet_df_clean.columns
df = pd.read_csv('msa.csv')
twitter_archive = pd.read_csv('twitter-archive-enhanced.csv') $ twitter_archive.head()
plot = song_tracker[["rockstar","Bad and Boujee (feat. Lil Uzi Vert)"]].astype(float).plot(figsize=(20,15)) $ plot.set_xlabel("Date") $ plot.set_ylabel("Chart Position")
data.info()
np.exp(-0.0099), np.exp(-0.0507)
df.name.value_counts()[0:19].plot(kind='bar');
fruits = pd.Series([10, 6, 3], ['apples','oranges','bananas']) $ fruits
dfTemp = getDataFiltering(df_train) $ dfTemp.shape[0]
autos['odometer_km'].describe(percentiles=[.10, .90])
topic_geo_infomation = pd.DataFrame(l, columns=['date', 'placeId', 'place_value'])
for columns in DummyDataframe2[["Positiv", "Negativ"]]: $     basic_plot_generator(columns, "Graphing Dummy Data using Percent" ,DummyDataframe2.index, DummyDataframe2)
rounds.loc[0]
table = soup.find_all('table', class_='list hours responsive') $ print(type(table), type(table[1]))
by_hour = sample.resample("H") $ by_hour.head(2),by_hour.tail(2) 
sqft_intercept, sqft_slope = simple_linear_regression(train_data['sqft_living'], train_data['price']) $ print('Intercept: {:.2f}'.format(sqft_intercept)) $ print('Slope: {:.2f}'.format(sqft_slope))
learn.unfreeze()
dfg.sort_values(['clusterlabel']).style.set_table_styles([dict(selector="th",props=[('max-width', '150px')])])
data.columns
n_new = len(df2.query('landing_page == "new_page"')) $ n_new
g['created_year'].dtypes
trigram_dictionary_filepath = paths.trigram_dictionary_filepath
labeled_features = final_feat.merge(failures, on=['datetime', 'machineID'], how='left') $ labeled_features.head()
result = tf.multiply(x1,x2) $ print(result)
autos = autos[autos["registration_year"].between(1900, 2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
hashed_test.printSchema()
df.rename(columns={'Indicator':'Indicator_id'}).head(2)
df_master.to_csv('twitter_archive_master.csv', index= False, encoding='utf-8')
rdd.map(lambda x: x**2 + really_large_dataset).collect()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
WHOregion = df['WHO Region'].values $ WHOregion
df.group.unique()
nobel[nobel['Category'] == 'Literature'].groupby('Birth Country').size().sort_values(ascending=False).head()
cityID =  '4b25aded08900fd8' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Reno.append(tweet) 
top_20_breed_stats['rating_numerator'].sort_values(ascending=False).plot(kind='bar', ylim=[10,12])
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative = 'smaller') $ z_score, p_value, norm.cdf(z_score)
crimes['PRIMARY_DESCRIPTION'].head()
train_corpus[0].tags
condition = (notus['cityOrState'].str.contains('Canada', case=False)) & (notus['country'] == 'Canada') $ notus.loc[condition]
covar=np.cov(x,y) $ covar
len(df2.query('landing_page == "new_page"'))/len(df2)
data.drop(['month', axis=1])
result.summary2()
tcat_df = tcat_df.append(tcat) $ tdog_df = tdog_df.append(tdog)
df_raw_fb = pd.read_csv('./Datasets/Facebook_Training_Data.csv', encoding='latin1') $ print (df_raw_fb.head())
df3.isnull().sum()
profit_table_simple = data_df[['ID','Profit','Month Name','Year']].copy() $ profit_table_simple.head()
r.text
z=recoveries #repaid_loans_cash $ z[z.fk_loan==loan_test].groupby(['dcf']).payment.sum()
mentors = data_final[data_final['countCollaborators'] > 4]['authorId'].unique() $ mentees = data_final[data_final['countCollaborators'] <= 4]['authorId'].unique()
df_goog.plot() $ df_goog[['Open','Close','High','Low','Adj Close']].plot()
df_columns[df_columns['Agency']=='DOT']['Complaint Type'].value_counts().head() $
import nltk $ nltk.download('stopwords')
jobs.loc[(jobs.FAIRSHARE == 64) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
df2.head(1) $ df2.converted.mean()
len(set(b_cal_q1['listing_id'].unique())-set(b_list['id'].unique()))
stations.head()
from sklearn import tree $ clf = tree.DecisionTreeClassifier() $ clf = clf.fit(train_matrix, train['sentiment'])
len(bow_transformer.vocabulary_)
df.iloc[0]
orgs = EQCC(git_index) $ previous_month_date = end_date - timedelta(days=31) $ orgs.since(start=previous_month_date).until(end=end_date) $ orgs.get_terms(field="author_org_name") $ print(buckets_to_df(orgs.fetch_aggregation_results()['aggregations'][str(authors.parent_id-1)]['buckets']))
sc.stop()
senateAll = pd.merge(senate, txSenate, how='left', on = 'district') $ houseAll = pd.merge(house, txHouse, how='left', on = 'district')
tipsDF = pd.read_csv('yelp_tips.csv', index_col=False, encoding='UTF-8') $
donors_c.iloc[2097169, 4]
support.amount.sum()/merged.amount.sum()
df_master.loc[rate_change[rate_change['rating']==42].index]
r.xs('UA', level=1)[['capital']].plot()
! head -3 ./data/raw-news_tweets-original/dataset1/tweets/2014-11-18/2685_Missouris_Nixon_Declares_State_of_Emergency_Awaiting_Grand_Jury-Businessweek
    def file_to_module(path): $         with loader: $             name = path.relative_to(Path(deathbeds.__file__).parent).stem.split('.', 1)[0] $             return getattr(__import__('.'.join(('deathbeds', name))), name)
labeled.corr().loc[:,'price'].abs().sort_values(ascending=False)[1:]
target.disconnect()
shoppingCart = pd.Series(["apple", "cherry", "banana", "cucumber", "apple"]) $ priceCategory = {"apple": "$3", "cherry": "$20", "banana": "$1.5", "cucumber": "$3"}
total_sales.corr()
def connect(): $     return pg2.connect(dbname='steam_capstone', host='localhost'), pg2.connect(dbname='steam_capstone', host='localhost').cursor()
archive_df.info()
model_tree.fit(x_train,y_train) $ model_linear.fit(x_train,y_train) $ model_rf.fit(x_train,y_train) $ model_gb.fit(x_train,y_train)
adam = optimizers.Adam(lr=0.01) $ l2 = regularizers.l2(0.5)
df_members['gender'] = df_members['gender'].map(gender) # making category column to int
plt.style.use('seaborn') $ location_ax = user_extract.location.value_counts()[1:49].plot(kind = "bar", figsize = (25,8)) $ location_ax.set_title("Most Popular Locations from WeRateDogs Tagged Users", fontsize = 24) $ location_ax.set_xlabel("") $ plt.savefig('plots/popular_locations.png', bbox_inches='tight')
s4.shape
print(np.info(np.linspace))
cars.columns
helpthispersonpls297 = df5['HT'].where(df5['andrew_id_hash'] == '21cb1f9adbacb7cf4bfb15085966652648310c05').dropna() $ plt.hist(helpthispersonpls297, bins=20)
print(int(2.5)) # Convert to int with truncation $ print(round(2.5))  # Convert to int with rounding (oddly, round() with 0 $ print(round(3.5))  #   decimal places rounds to even number, not up). $ print(round(2.5001))  # Convert to int with rounding
knn_yhat = kNN_model.predict(test_X) $ print("KNN Jaccard index: %.2f" % jaccard_similarity_score(test_y, knn_yhat)) $ print("KNN F1-score: %.2f" % f1_score(test_y, knn_yhat, average='weighted') )
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new],[n_old, n_new]) $ (z_score, p_value)
len(list(set(orgs.uuid)))
rootDistExp = Plotting(S.setting_path.filepath+S.para_trial.value)
csgo_profiles = csgo_profiles.dropna(subset=['total_kills','total_deaths','total_time_played','total_rounds_played', $                                              'total_shots_fired']) $ csgo_profiles.head()
df_new[['c1','c2','c3']] = pd.get_dummies(df_new['country'])
pumashplc.head(1)
knn_reg.score(x_test,y_test)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
method_label = train['signup_method'].unique() $ print(method_label)
xs = xs_df.as_matrix() $ ys = ys_df.as_matrix()
for v in d.variables: $     print(v)
sub_set['DEVICE_MODEL'].value_counts()[:30].plot(kind='barh')
iris.iloc[(iris.iloc[:,0]>7.5).values,4] $
a = np.array([True, False, False, False, False]) $ so_head[a]
df_usage.info()
df2.plot.bar(stacked=True, rot=0)
data = pd.merge(data,users,left_on='user',right_on='id',how='inner') $ data = data[['user','ts','channel','text','clean_text','name']]
df_columns[df_columns['Agency']=='NYPD']['Complaint Type'].value_counts().head() $
X_age_notnull = X_train[X_train['age'].notnull() & ((X_train['age'] > 10) & (X_train['age'] < 100))] $ print (X_age_notnull.shape[0]) $ X_age_null = X_train[X_train['age'].isna() | (X_train['age'] <= 10) | (X_train['age'] >= 100)] $ print (X_age_null.shape[0]) $
number = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0] $ number
unique_domains = group_on_field_with_metrics('domain') $ unique_domains.head()
au.show_frequent_items(mentions_df,user_names,"src",k=10)
print(giss_temp.shape) $ print(giss_temp.dtypes)
eve_treat =df_eve.query("ab_page==1").converted.mean() $ eve_treat
hn.shape[0]
mars_df = pd.read_html(mars_facts_url) $ mars_facts_df = pd.DataFrame(mars_df[0])
run txt2pdf.py -o"2018-06-18  2015 871 discharges.pdf"  "2018-06-18  2015 871 discharges.txt"
movies.createOrReplaceTempView("movies") $ ratings.createOrReplaceTempView("ratings") $ sql("show tables").show()
dummy_colnames = [clean_string(colname) for colname in df_dummies.columns] $ df_dummies.columns = dummy_colnames $ df_dummies.head()
ridge = linear_model.Ridge(alpha=0.5) $ ridge.fit(X_17, y2) $ (ridge.coef_, ridge.intercept_)
df.iloc[-1]
log_mod = sm.Logit(df_new['converted'],df_new[['ab_page','US','UK','intercept']]) $ result = log_mod.fit() $ result.summary()
feeds.head()
top_rsvp = df.sort_values("mean_rsvp", ascending=False).reset_index(drop=True) $ top_rsvp.head(5)
df.head()
from ipywidgets import interact $ interact(sdof_resp, x0 = (0,2,0.1), v0 = (0,2,.1), m = (0,2,0.1), k = (0,100,1), c = (-1,5,.1));
df = pd.DataFrame(d) $ df
tmax = tmax_day_2018.tmax[0] $ tmax.plot()
fig = plt.figure() $ ay = fig.add_subplot(111, projection='3d') $ ay.scatter(lat_25_2.values, lng_25_2.values, minute_25_2.values, c=in_cp_25_2.values) $ plt.show()
df['Cat'] = pd.cut(df.Size, bins=[0, 1024, 1024*1024, 40*1024*1024, 1.6e+09], $               labels=['zeros', 'small', 'moderate', 'oversized'])
Xs=pd.DataFrame(Xs) $
sub_df = sub_df.merge(op_add_comms, on='id', how='outer')
filename = "../datasets/catalogs/fermi/gll_psc_v16.fit.gz" $ print([_.name for _ in fits.open(filename)]) $ extended_source_table = Table.read(filename, hdu='ExtendedSources')
df.head(3)
machines.describe()
r_dict = r.json() $ print(r_dict['dataset_data']['column_names'])
print "We investigate the two ids missing in posts." $ print 561164, 561164 in ids_posts $ print list(posts[posts['id']==561164]['Created At']) $ print 192168, 192168 in ids_posts $ print list(posts[posts['id']==192168]['Created At'])
result1 = (df1 < df2) & (df2 <= df3) & (df3 != df4) $ result2 = pd.eval('df1 < df2 <= df3 != df4') $ np.allclose(result1, result2)
commits_per_day_cumulative.plot()
c = crime_data.reset_index() $ c = c.merge(gt_enrollment_data, how='left', on=['year', 'semester']) $ c = c.merge(atl_data, how='left', on='year') $ c = c.set_index('time') $ c.head()
ride_demand.head()
pd.read_csv("ign.csv",nrows=4)
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df_complete_a['name'].value_counts()
stations = session.query(Measurement).group_by(Measurement.station).count() $ stations
with open(catboost_pred_pickle_file_name, 'rb') as f: $     y_pred = pickle.load(f) $ print(y_pred[:10])
dfX = df2 $ dfX['intercept'] = 1 $ dfX[['X','ab_page']] =pd.get_dummies(dfX['group']) $ dfX = dfX.drop(['X'],axis=1) $ dfX.head()
tweets = pd.concat([tweets1,tweets2,tweets3]) $ tweets.shape
ca = ['California', 'Los Angeles', 'San Francisco', 'LA', 'SF', 'Southern California', 'SF Peninsula'] $ us.loc[us['country'].isin(ca)|us['cityOrState'].isin(ca), 'cityOrState'] = 'CA' $ us.loc[us['country'].isin(ca)|us['cityOrState'].isin(ca), 'country'] = 'USA' $ us['country'].value_counts(dropna=False)
utc_dt_str = '2017-05-03T14:15:00Z' $ import pytz $ dt = datetime.datetime.strptime(utc_dt_str, '%Y-%m-%dT%H:%M:%SZ') $ utc_tz = pytz.timezone('UTC') $ utc_dt = utc_tz.localize(dt)
from sklearn.pipeline import make_pipeline $ from imblearn.pipeline import Pipeline $ from sklearn.preprocessing import StandardScaler $ from sklearn.model_selection import GridSearchCV $ from imblearn.over_sampling import SMOTE
df_nona = df.fillna('NA')
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['text'].str.extract('(([0-9]*[.\\]*[0-9]+)\/(10))',expand = True)[1]=='...10']
df = pd.read_sql_query('SELECT Agency, COUNT(*) as `num_complaints`' $                        'FROM data ' $                        'GROUP BY Agency ', disk_engine) $ df.head()
df.info()
top_20['retweet_count'].sort_values().plot.barh(figsize=(10, 8));
display('df1a', 'df2a', "pd.merge(df1a, df2a, left_index=True, right_index=True)")
df['weekday'] = df['DATE'].apply(lambda x: datetime.strptime(x, '%m/%d/%Y').date().weekday())
print('Number of results for {0}: {1}'.format(varcode, len(results)))
df_predictions_clean.p1 = df_predictions_clean.p1.str.title()
knn.score(x_test,y_test > 0)
df = pd.read_sql('SELECT * from booking', con=conn_b) $ df
df2.head()
def twitter_setup(): $     auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $     auth.set_access_token(access_token, access_secret) $     api = tweepy.API(auth) $     return api
h5 = qb.History[QuoteBar](eur.Symbol, timedelta(30), Resolution.Daily) $
df.to_csv('DC Council Calendar - 2017-07-25')
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
m=vio[vio['year']==2016].groupby(['business_id','date']).count() $ num_vio=m.iloc[:,1:2].reset_index() $ num_vio=num_vio.rename(columns={'new_date' : 'num_vio'}) $ ins2016=pd.merge(ins2016,num_vio, on=['business_id','date'], how='left')
loans_df.head()
dt = datetime.datetime(year=2018,month=4,day=14,hour=10,minute=30) $ dt
df2.head(5)
train_features = bag_of_words_vectorizer.transform(train_corpus)
facilities = pd.read_csv('facilities.csv') $ facilities.info()
test_orders_prodfill_finale=test_orders_prodfill_final2[['order_id','products']] $ test_orders_prodfill_finale.head()
df['sp_close_chg'] = sp.close_chg $ df['sp_open_chg'] = sp.open_chg $ df['sp_high_chg'] = sp.high_chg $ df['sp_low_chg'] = sp.low_chg $ df['sp_vol_chg'] = sp.vol_chg
cityID = 'dd3b100831dd1763' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_Orleans.append(tweet) 
pd.read_csv?
df = pd.read_sql('SELECT * FROM customer_male', con=conn_a) $ df
final_annotations_df.young_present.value_counts()
myDict = {'Guinea':guineaDf,'Liberia':liberiaDf,'SL':slDf} $ finalDf = pd.concat(myDict) $ finalDf
df_questionable_3[df_questionable_3['state_NY'] == 1]['link.domain_resolved'].value_counts()
mb = pd.read_csv("../data/microbiome/microbiome.csv") $ mb.head()
SCN_BDAY.head()
import fasttext $ import gensim $ from gensim.models.fasttext import FastText $ from gensim.models import KeyedVectors
rs = pd.Series(cs[0]).sort_values(ascending=0) $ top5 = rs.iloc[0:5] $ top5
raw_df.replace({'3wordweather': ''}, inplace=True, regex=True) $ raw_df.replace({'threewordweather': ''}, inplace=True, regex=True) $ raw_df.replace({'and': ''}, inplace=True, regex=True) $ raw_df.shape
r = requests.get( $     'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
posts.columns
oppose.sort_values("amount" , ascending=False).head(2)
grouped_dpt_city.aggregate(np.sum) # aggregate sum based on two groups
col_names = data.columns.values[6:] $ col_names
stores_pd = stores.toPandas() $ pd.value_counts(stores_pd['store_level'].values, sort=True).plot(kind="bar")
injuries_hour.head()
convert_old = df2[(df2['landing_page']=='old_page') & (df2['converted']==1)].count()[0] $ convert_old
cfs_df.head()
pd.Period('2012-05', freq='D')
squares.items()  # Iterable
pd.merge(left,right, on='key')
education_data.head()
X = data.drop(['class','ip_address','signup_time','purchase_time','device_id','user_id','age'],axis=1) $ y = data['class'] $ X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) $ X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=0.2) $
html=urlopen(base_url + '/wiki/1942_Qantas_Short_Empire_shootdown') $ soup1= BeautifulSoup(html, "lxml") $ table=soup1.find('table',attrs={'class':'infobox'}) $ table
for df in (joined, joined_test): $     df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32) $     df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32) $     df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32) $     df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)
rodelar.registration().strftime("%Y-%m-%d")
predictions = pd.DataFrame(y_pred.view(len(y_pred), -1).data.numpy(), columns=['tavg_norm']) $ predictions['series'] = 'predicted'
pd.concat([d1, d3], axis=1)
cust_data.MonthlyIncome
a,b = data.shape $ print('Number of merchants '+str(a)) $ print('Number of features '+str(b))
print(action.shape) $ action.head()
ts.resample('D', how='mean')
[i.contents[0] for i in ts]
print example1_df.printSchema()
store.head()
df_selected.select('user_id','date','seq', 'numOfReviews').orderBy('user_id', 'seq').limit(40).toPandas()
df_train.to_csv("train_transformV2.csv")
p_values = (p_diffs > obs_diff).mean() $ p_values
stations = session.query(Precip.date, Precip.prcp, Precip.station) $ station_df = pd.DataFrame(stations[:], columns = ['Date', 'Precip', 'Station ID'])
cust_demo.head(3)
data = data.dropna(axis = 0)
df_sp_500 = df_sp_500.withColumn('IDX', func.lit('SP_500'))
sel_df.Beat.unique()
mom['Date'] = mom['Date'].apply(lambda x: str(pd.to_datetime(str(x), format= '%Y%m'))[0:7])
reddit['Titles'].nunique() #how many unique titles do we have? 
learner.unfreeze()
X_test = pd.get_dummies(columns=['term', 'home_ownership', 'verification_status', 'purpose'], data=X_test)
df_pop = df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ df_pop $
adv_stats.head()
logit_pageCountry = sm.Logit(merged['converted'],merged[['ab_page', 'intercept', 'UK', 'US']]) $ result_pageCountry = logit_pageCountry.fit()
sales = pd.read_csv('sales_train.csv') $ item_categories = pd.read_csv('item_categories.csv') $ items = pd.read_csv('items.csv') $ shops = pd.read_csv('shops.csv') $ test = pd.read_csv('test.csv')
n_new = df2.query('group == "treatment"')['user_id'].nunique() $ n_new
val ssc = new StreamingContext(new SparkConf(), Seconds(1)) $ val my_stream = ssc.socketTextStream(hostname, port)
merged.loc[merged['state/region'] == 'PR', 'state'] = 'Puerto Rico' $ merged.loc[merged['state/region'] == 'USA', 'state'] = 'United States' $ merged.isnull().any()
df_joined.groupby('country').converted.mean()
active_users = users[users[['Asked', 'Answered', 'Comments']].notnull().any(axis = 1)] $ active_users.head()
userItems.registerTempTable("predictions") $ query = "select * from predictions order by prediction desc limit 5" $ sqlContext.sql(query).toPandas()
plt.figure(figsize = (6,6)) $ kmeans.fit_predict(X) $ plt.scatter(X[:,0],X[:,1], c=kmeans.labels_, cmap='rainbow')  $
dfs_loan[0].columns
from sklearn.metrics import log_loss $ log_loss(y_test, yhat_prob)
metrics.roc_auc_score(actual_value_second_measure.values, predicted_probs_first_measure)
sns.pairplot(iris, hue='Species')
dfd = dfh[dfh['Centrally Ducted or Ductless'].str.startswith('Ductless')] $ print(len(dfd)) $ dfd['Centrally Ducted or Ductless'].unique()
path = ValidFiles['H'].path $ path
spacy_tok = spacy.load('en')
tw_clean['timestamp'] =  pd.to_datetime(tw_clean['timestamp'], format='%Y-%m-%d %H:%M:%S +%f')
from sklearn.model_selection import GridSearchCV
for i in tqdm(range(0,23)): $     mol = molecules.loc[i, 'molecule'] $     m2 = Chem.AddHs(mol) $     if AllChem.EmbedMolecule(m2) < 0: $         print 'oh no {}'.format(i)
sqcities = np.square(cities) $ print(sqcities)
print(autos.columns)
normalized_df.head()
us.loc[us['country'] != 'USA', 'cityOrState'] = us.loc[us['country'] != 'USA', 'country']
result['timestampCorrected'].isnull().sum()
ny_df=pd.DataFrame.from_dict(foursquare_data_dict)
clf = RandomForestClassifier(n_estimators = 100, max_depth = 30) $ clf.fit(X_train, y_train)
pgh_311_data.info()
plt.barh(ds['title'], ds['quantity'], align='center') $ plt.show()
Q3 = pd.DataFrame(avg_values, columns=['Average Temperature'])
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head(2)
agency_borough = data.groupby(['Agency','Borough']) $ agency_borough.size().unstack().plot(kind='bar', $                                      title='Incidents in each Agency by Borough', $                                     figsize=(15,15))
df_max = df_max.iloc[:-1,:]
p_old=df2[df2['landing_page']=='old_page']['converted'].mean() $ p_old $ p_diff = p_new-p_old $ p_diff
pd.Period('2016-01-07')
exiftool -csv -createdate -modifydate cisuabf6/cisuabf6_cycle1.MP4 cisuabf6/cisuabf6_cycle2.MP4 cisuabf6/cisuabf6_cycle3.MP4 cisuabf6/cisuabf6_cycle4.MP4  > cisuabf6.csv
rng = pd.date_range('1/1/2011', periods=72, freq='H')
df_uro_dd_dummies.to_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_Urology\02_EMR_urology_with_dd_with_dummies.csv') $ df_uro_dd_dummies_no_sparse.to_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_Urology\02_EMR_urology_with_dd_with_dummies_no_sparse.csv')
price_series = pd.Series(branding_price)
order_pay_price_sum = pd_data.loc[lambda pd_data: pd_data.pay > 0,:].groupby(['appCode','orderType','year','month'])['orderPrice'].sum()
comps_count = pd.DataFrame(comps.groupby('entity_uuid').size()).reset_index() $ comps_count.columns = ['entity_uuid','competitor_count']
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL') $                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1)) $                           &(extract_nondeduped_cmp.app_branch_state=='OH')].groupby('APP_PROMO_CD').size()
df = df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False) $ df.shape
score, acc = model.evaluate(np.asarray(test_x), np.asarray(new_test_y)) $ print 'Test score:', score $ print 'Test accuracy:', acc
users = df.index.get_level_values('user').unique()
print "Number of rows with 'production_id'==701   : ", len(np.where(df1['production_id']==701)[0]) $ print "Total number of rows of table 'photoz_bcnz': ", len(df1) $ df1
(p_diffs > p_diff).mean()
random.seed(1234) $ new_page_converted = np.random.choice([1, 0], size = n_new, p = [p_mean, (1-p_mean)])
question_1_dataframe = question_1_dataframe.groupby(['borough', 'complaint_type']) $ question_1_dataframe
head = pd.Timestamp('20150617') $ tail = pd.Timestamp('20150618') $ df=sensor.get_data(head,tail,diff=True, unit='W') $ charts.plot(df, stock=True, show='inline')
c.set_index('cutoff_time')['days_to_next_churn'].plot(); $ plt.vlines(x = (c.loc[c['churn'] == 1, 'cutoff_time']).values, ymin = 0, ymax = 200, color = 'r'); $ plt.ylabel('Days to Next Churn');
subrcvec1.head()
group=merged.groupby(['date','store_nbr','class','family'],as_index=False) $ class_sales=pd.DataFrame(group['unit_sales'].agg('sum')) $ pd.DataFrame.head(class_sales)
r = requests.get(url)
f = h5py.File('../../data/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5','r') 
df.set_index('Country').reset_index().head(5)
terror.head()
yarr, ynames = pd.factorize(cat_outcomes['outcome_subtype'])
latest_version = str(d.id[0]) $ print latest_version
df_selection.describe()
idx = pd.date_range('2015/01/01', '2015/12/31 23:59', freq='T') $ dn = np.random.randint(2, size=len(idx))*2-1 $ rnd_walk = np.cumprod(np.exp(dn*0.0002))*100 $ df = pd.Series(rnd_walk, index=idx).resample('B').ohlc()
most_active_stations = session.query(Measurement.station, func.count(Measurement.station)).\ $         group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()0 $ print (most_active_stations)
v_lng = data.LONGITUDE $ v_lat = data.LATITUDE
plt.plot(kind='bar') $ plt.show()
rng_dateutil = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz=tz_dateutil)
df_never_moved = df.head(0)
trn_lm, val_lm = sklearn.model_selection.train_test_split(np.array(df["numerized_tokens"]), test_size=0.1) $ len(trn_lm), len(val_lm)
df2[['control_group', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control_group', axis=1)
df_users.head()
data_repo = "../data/" $ IMPORT_PARAMS = {'sep': '|', 'encoding': 'latin-1'}
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs); $ plt.axvline(diff, color = 'red');
train.groupby('popular').num_points.agg(['min', 'max'])
df["updated_at"].dt.date.value_counts().head(10)
ax = df.boxplot(column='time_open_hours', by='complaint_', figsize=(20,20), rot=45, fontsize=14)
X_train, X_test, y_train, y_test = train_test_split(train_data_features, y, test_size=0.15, random_state=42)
const_cols = [c for c in train_data.columns if train_data[c].nunique(dropna=False)==1 ] $ const_cols
avg1_table = data_df[['ID','Segment','Country','Product','Units Sold']].copy() $ avg1_table.head()
test_data = pd.read_csv("test.csv", parse_dates=['DateTime']) $ test_data.head()
merged_df_cut["category"] = merged_df_cut["category"].apply(lambda x: x.lower()) $ merged_df_cut["category"].replace(to_replace="sky_state", value="sky state", inplace=True)
iris_dataset['data'].shape
df.T
df['Col5_minus_Col2'] = df.Col_5 - df.Col_2 $ df.head()
lasso.score(X_train,Y_train)
mask = f>=0.5 $ print(f[mask])
yc200902_short = yc[::1000] $ yc200902_short.shape
train_df[train_df['parcelid'].duplicated(keep=False)] $
sns.countplot(calls_df["dial_type"])
store.list_collections()
file = 'literary_birth_rate.csv' $ df_birth = pd.read_csv(file, sep = ';')
results = soup.find_all('div', class_="slide", limit=1)
tm_2030 = pd.read_csv('input/data/trans_2030_m.csv', encoding='utf8', index_col=0)
test.head()
converted = (df2.query('converted == 1').count()[0]) $ total = df2.shape[0] $ print ("The number of user converted is {}".format(float(converted) /  float(total)))
df_countries.user_id.nunique()
gas.head()
f.info()
data.to_csv("Data.csv",index=False)
tweets['id_str'] = tweets['id_str'].str.decode('utf-8') $ tweets['full_text'] = tweets['full_text'].str.decode('utf-8') $ tweets['created_at'] = tweets['created_at'].str.decode('utf-8')
tweet_archive_clean['dog_type'] = tweet_archive_clean['text'].str.extract('(doggo|floofer|pupper|puppo)', expand=True)
suspects_with_1T.groupby('imsi')['event_type'].unique().apply(lambda x: x[0]).unique()
tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy') $ tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_axial_brace_crown[1][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_crown[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
grouped_dpt.aggregate(np.mean) # means based on one group
new_page_converted = np.random.binomial(new_n, new_p) $ new_page_converted
y_resampled.shape
sentiment_pipeline = joblib.load("../data/other_data/subtweets_classifier.pkl")
con = sqlite3.connect('microbiome.sqlite3') $ con.execute(query) $ con.commit()
df2.rename(columns={'activity_date_time_c': 'average_activities_per_gallery'}, inplace=True) $ df2.groupby(df2.index).mean()
df_snapshots = pd.read_csv(path) $ print(df_snapshots.shape) $ df_snapshots.head()
consumerKey = 'XXXXXXXXXXXXXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
all_p.value_counts().head(10)
wikiMarvelRequest = requests.get(wikipedia_marvel_comics) $ print(wikiMarvelRequest.text[:1000])
bacteria2[bacteria2.notnull()]
import pandas_datareader.data as web
df2[df2['user_id'].duplicated(keep=False)]
print("Train set has total {0} entries with {1:.2f}% negative,{2:.2f}% neutral, {3:.2f}% positive".format(len(x_train),(len(x_train[y_train == -1]) / (len(x_train)*1.))*100,(len(x_train[y_train == 0]) / (len(x_train)*1.))*100,(len(x_train[y_train == 1]) / (len(x_train)*1.))*100)) $ print("Validation set has total {0} entries with {1:.2f}% negative,{2:.2f}% neutral, {3:.2f}% positive".format(len(x_validation),(len(x_validation[y_validation == -1]) / (len(x_validation)*1.))*100,(len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,(len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100)) $ print("Test set has total {0} entries with {1:.2f}% negative,{2:.2f}% neutral, {3:.2f}% positive".format(len(x_test),(len(x_test[y_test == -1]) / (len(x_test)*1.))*100,(len(x_test[y_test == 0]) / (len(x_test)*1.))*100,(len(x_test[y_test == 1]) / (len(x_test)*1.))*100))
print(newdf['Area'].unique()) $ print(newdf['Year'].unique()) $ print(newdf['Variable Name'].unique())
s = pd.Series(np.random.randn(len(rng)).cumsum(), index=rng) $ s.head()
groups_by_metro_count = groups_by_metro.count() $ top_metros = groups_by_metro_count.nlargest(30, 'id') $ top_metros['MSA_NAME'] = top_metros["MSA_NAME"].apply(lambda x: x.split(',')[0])
mysom = tfsom.SOM.load_trained_som("./minisom.save")
hdf.loc[slice('adult', 'child'), :].head()
rf.score(clim_test, size_test)
df.index.month.value_counts().sort_index().plot()
git_log["timestamp"] = pd.to_datetime(git_log["timestamp"], unit="s") $ git_log["timestamp"].describe()
train[(train.age>90) & (train.age<1000)].age.value_counts()
data = pd.Series(['a', 'b', 'c'], index=[1,3,5]) $ data
londonGeoDF = gpd.read_file('OA_2011_BGC_london.json') $ londonDFSubset = londonGeoDF.ix[:,['OA11CD', 'POPDEN', 'WD11NM_BF', 'geometry']]
random_integers.idxmax(axis=1)
users.groupby('CreationDate')['LastAccessDate'].count().plot() $
p_old = df2.query('converted == "1"').count()[0]/df2.shape[0] $ old_page_convert = np.random.choice([0,1], n_old, p=(1-p_old, p_old))
train_small_data.click_timeHour.unique()
dfHashtags.head()
data2Scaled = data1Scaled.dropna(inplace=False)
df1_clean.rating_denominator = 10
training_active_listing_dummy.shape
engine = create_engine('postgresql://maasr:UWCSE414@dssg18.cofuftxjqsbc.us-east-1.rds.amazonaws.com/dssg18')
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid $ x = df30458.groupby('start_station_name').count() $ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
df_train = df_total[df_total['is_test'] == False].drop('is_test', axis=1) $ df_test = df_total[df_total['is_test'] == True].drop('is_test', axis=1)
class_test = sub_df[(sub_df['text'].str.find('?') != -1)].reset_index()
df3 = pd.concat([non_align_1, non_align_2]) $ df2 = df.drop(df3.index) 
y = df['comments'] $ X = df[['subreddit', 'title', 'age']].copy(deep=True)
df_wm['cleaned_text'] = df_wm['text'].apply(lambda x : text_cleaners(x))
util.cov_to_melted(tmp_cov)
monte.str.extract('([A-Za-z]+)', expand=False)
n_new, n_old = df2['landing_page'].value_counts() $ print("new:", n_new, ",old:", n_old) $ new_p_c = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)]) $ new_p_c.mean()
a.append(4) $ print b
msft_cum_ret = (msftAC / msftAC.shift()).cumprod() $ msft_cum_ret
CRnew = df2.query('landing_page == "new_page"')['converted'].mean() $ CRold = df2.query('landing_page == "old_page"')['converted'].mean()
features_scores ={} $ for c, v in zip(logreg_sentiment.fit(X, y).coef_[0].round(3), ['Negative sentiment','Positive Sentiment','Neutral','Compound','Length']): $     features_scores[v] = c $ sent_coef = pd.DataFrame(features_scores, index=['Coefficient']).T.sort_values(by='Coefficient', ascending=False) $ sent_coef
goog['Month'] = goog.index.month $ goog['Year'] = goog.index.year $ goog
cur = conn.cursor() $ cur.execute('ALTER TABLE actor ALTER COLUMN middle_name TYPE text;') $ df = pd.read_sql('SELECT * FROM actor', con=conn) $ df.head()
df_protest.loc[:, 'duration'] =  df_protest.end_date - df_protest.start_date $ df_protest.duration
df = pd.DataFrame({"number": [2,5,4,7,3,0], "raw_grade": ['a', 'b', 'c', 'd', 'e', 'f']}) $ df
df.groupby("cancelled")["local_rental"].value_counts()
news_df = (news_df $            .loc[news_df['num_reactions'] - $                 news_df['num_likes'] >= 10,:])
plt.hist(p_diffs) $ plt.axvline(obs_diff, c='r');
tlen_k1.plot(figsize=(16,4), color='r'); $ tlen_k2.plot(figsize=(16,4), color='b');
weather_x = weather_norm.drop('tavg', axis=1) $ weather_y = weather_norm['tavg'].shift(-1)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ result = logit_mod.fit() $ result.summary()
soup.select('table.type_2 a[href]')
pd.date_range('2017-12-30', '2017-12-31', freq='h')  # Hourly frequency
players_df.drop(columns={'player_id', 'Unnamed: 0', 'Name'}, inplace=True) $ players_df.to_csv('~/dotaMediaTermPaper/data/players_df.csv')
svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf') $ svc.fit(x_res, y_res) $ scores = cross_validate(svc, x_res, y_res, cv=10, n_jobs=-1, return_train_score=True)
import os $ cmdoutput = os.popen('python sumxy.py 1 2').read() $ print cmdoutput
images_url = r"https://www.cobbgis.org/openimage/bravescam" $ cams = ["128", "129", "130"] $ cam_urls = ["{0}/Cam{1}".format(images_url, cam) for cam in cams]; cam_urls
old_page_converted = np.random.binomial(N_old, P_old) $ old_page_converted
stories[['created_hour','created_dow']].head()
trump[trump_fb].head()
v_invoice_hub.merge(v_item_hub, left_on='fk_s_change_context_id_cr', right_on='fk_s_change_context_id_cr')
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1').count()[0] $ convert_new = df2.query('group == "treatment" and converted == 1').count()[0] $ convert_old, convert_new, n_old, n_new  
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)]) $ new_page_converted.mean()
topics_by_time(period=topics_data.timestamp.dt.weekofyear, period_str='Week Number')
locations = session.query(Measurements).group_by(Measurements.station).count() $ print("There are {} stations.".format(locations))
itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]
house_data['renovated'] = house_data['yr_renovated'].apply(lambda x: 1 if x > 0 else 0)
y_hat = lr.predict(df_pol_d) $ df_pol['y_hats'] = y_hat
processed_tweets.sample(4)
autos["price"].value_counts().head()
dupes_to_delete = dfd.duplicated(subset=['brand', 'outdoor_model', 'indoor_model']) $ dupes_to_delete.value_counts()
condo_6 = condos[condos.MAR_WARD == 'Ward 6']
knn5.fit(X_train, y_train)
df2[['cntry_US','cntry_UK','cntry_CA']]=pd.get_dummies(df2['country']) $ df2.head()
movies_df['year'] = movies_df.title.str.extract('(\(\d\d\d\d\))',expand=False) $ movies_df['year'] = movies_df.year.str.extract('(\d\d\d\d)',expand=False) $ movies_df['title'] = movies_df.title.str.replace('(\(\d\d\d\d\))', '') $ movies_df['title'] = movies_df['title'].apply(lambda x: x.strip()) $ movies_df.head()
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0)
grouped_ct = events.groupBy("geography_id").agg( count("geography_id") ) $ grouped_ct = grouped_ct.withColumnRenamed( "count(geography_id)", "count" ) $ sorted_ct = grouped_ct.sort( grouped_ct['count'].desc() )
parking_planes_df.count()
converted_first_week.shift(2) # shift values by 2 periods 
history = model.fit(X_dense_train, y_train, epochs=30, validation_split=0.2, batch_size=64)
sqlContext.sql(query).toPandas()
tm_2030 /= 1000 $ tm_2030_norm = tm_2030 ** (10/11) $ tm_2030_norm = tm_2030_norm.round(1) $ tm_2030_alpha = tm_2030 ** (1/3) $ tm_2030_alpha = tm_2030_alpha / tm_2030_alpha.max().max()
net_loans_exclude_US_outstanding.Ratenart.unique()
tsne_filepath = os.path.join(intermediate_directory, u'tsne_model') $ tsne_vectors_filepath = os.path.join(intermediate_directory, u'tsne_vectors.npy')
m1.fit() $ m1.forecast() $ m1.plot(plot_type = 'autocorrelation' ,lag = 2) $
All_tweet_data.shape
test_sentence = test_sentence.replace(re.compile(r"\s?[0-9]+\.?[0-9]*")) $ test_sentence[0]
p_old = round(float(df2.query('converted == 1')['user_id'].nunique())/float(df2['user_id'].nunique()),4) $ p_old
df_int.plot() $ plt.show()
print(df.sex.str.len().unique())# returns unique lengths of sex $ df.sex=df.sex.str.strip() $ print(df.sex.str.len().unique())
Quandl_DF = ql.get("BOE/XUDLHDS", authtoken="AUpKE1-xMYxathtHrkNK", start_date="1989-01-01", end_date="2017-12-31").reset_index() $ Quandl_DF.columns = ['Date', 'GBP_to_HKD']
plt.plot(df_y_actual, df_y_pred, 'go') $ plt.xlabel('Actual age') $ plt.ylabel('Predicted age')
temporal_group = 'monthly' $ df = pd.read_csv('../data/historical_data_{0}.csv'.format(temporal_group)) $ df['date'] = pd.to_datetime(df['date']) $ df = df.set_index('date')
y_pred = reg.predict(X_)
plt.hist(p_diffs, color='g') $ plt.title('\n Simulated differences for conversion rates \n', fontsize=16) $ plt.axvline(x = obs_diff, color='red');
nold = len(df2.query("group == 'control'")) $ print(nold)
bnb = pd.concat([bnb,pd.get_dummies(bnb['first_affiliate_tracked'],prefix='channel_')],axis=1) $ bnb.head()
company = ["AAPL","FB","LNKD","AMZN","GOOG","MSFT","BABA"] $ numdays = 20 $ base = datetime.datetime(2016,4,20) $ preDate = [base - datetime.timedelta(days=x) for x in range(0, numdays)]
integratedData.sort_values(by = 'date', ascending = 0, inplace = True) $ integratedData.reset_index(inplace = True) $ integratedData.drop('index', axis = 1, inplace = True) $ integratedData.head()
inc = np.exp(0.0140) $ inc
df_c = pd.read_csv('countries.csv') $ df_c.head()
df_byzone = df_byzone.loc[:,['time_stamp_local', 'price']]
transformed2.shape
tweet_hour['tweet_text'].map(tweet_tokenizer.tokenize)
df.text[175]
ts.index=ts.index.to_period() $ ts.tail()
control_df = df2.query('group == "control"') $ control_conv = control_df.query('converted == 1').user_id.nunique() / control_df.user_id.nunique() $ control_conv
len(M7_pred),len(M7_actual),len(dfM.DATE[13:-12])
a = 5.6 $ s = str(a) $ s
tweet_data.to_csv('twitter_archive_master.csv', index= False)
import statistics $ statistics.median(volume)
index.save('trump.index') $ index = similarities.MatrixSimilarity.load('trump.index')
vals = data.value.copy() $ vals[5] = 1000 $ vals
k1.head(10)
no_specialty.shape
temp_df = pd.DataFrame(highest_tobs_station_yearly) $ temp_df.head()
temperature_year = session.query(Measurements.date,Measurements.tobs) \ $              .filter(Measurements.date >= '2016-08-23').filter(Measurements.date <= '2017-08-23') \ $              .filter(Measurements.station == 'USC00519281').all() $ temperature_measurement_df = pd.DataFrame(temperature_year[:], columns=['Date','tobs',]) $ temperature_measurement_df.head()
words_only_sp = [term for term in words_sp if not term.startswith('#') and not term.startswith('@')] $ corpus_tweets_streamed_profile.append(('words', len(words_only_sp))) # update corpus comparison $ print('The number of words only (no hashtags, no mentions): ', len(words_only_sp))
clean_appt_df.groupby('Neighbourhood')['No-show']\ $              .value_counts(normalize=True)\ $              .loc[:,'No']\ $              .plot.hist() $
db = client.test_database
def get_covariance(returns, weighted_index_returns): $     assert returns.index.equals(weighted_index_returns.index) $     assert returns.columns.equals(weighted_index_returns.columns) $     return None, None $ project_tests.test_get_covariance(get_covariance)
appointments['Provider'].unique(), len(appointments['Provider'].unique())
test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True) $ test_df.head()
stocks['Date'].head(1)  # Inspecting progress
df_prep1 = df_prep(df1) $ df_prep1_ = pd.DataFrame({'date':df_prep1.index, 'values':df_prep1.values}, index=pd.to_datetime(df_prep1.index))
df_en['polarity_blob'].plot.hist(figsize=(10,5), bins=70, title="TextBlob Histogram")
df_bthlst["booth_id"] = list(df_bthlst['client_display_name'\ $                                       ].apply(lambda x: x.split("|")[0].split(".")[-1]).astype(int)) $ df_bthlst["md_id"] = list(df_bthlst['MD_id'].apply(lambda x: abs(int(x.split("MotherDairy")[1]))))
df1.Protocol.unique()
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'CA']]) $ results = logit_mod.fit() $ results.summary()
act_diff = df2[df2['group']=='treatment']['converted'].mean()- df2[df2['group']=='control']['converted'].mean() $ act_diff
cp311 = pd.concat([cp311,dfagency],axis=1,join='inner').copy()
Project = Project[['Date', 'Contract Value (Daily)']] $ Project = Project.groupby(['Date'])[['Contract Value (Daily)']].sum().reset_index()
df=data $ print(df.isnull().sum())
comment_rate = ncptable[["author", "num_comments"]] $ comment_rate = pd.merge(comment_rate, posts, how="left", on="author") $ comment_rate["rate"] = comment_rate["num_comments"] / comment_rate["links"] $ comment_rate.sort_values(inplace=True, by="links", ascending=False) $ comment_rate[0:9].sort_values(by="rate", ascending=False)
education_2011_2015.columns=new_columns
pd.DataFrame(user_predict_df.loc[25451]).sort_values(25451,ascending=False)
iso_gdf_2.plot();
a = df2['group']=='treatment' $ actual_p_new =df2[a].converted.mean() $ print('Probability of individual converted if he is in treatment group:{}'.format(actual_p_new))
tweet_archive_clean[tweet_archive_clean.name.isin(list_of_exclusion_words)]
df_twitter_archive_master.sort_values(by=['timestamp'],ascending=False).head(5)
bigdf_read.reset_index().loc[1399301]
cars= cars.drop(['dateCrawled','name','seller','offerType','abtest','nrOfPictures','postalCode'], axis=1) $ cars.head()
print ("Mean:%f STD:%f Min:%f Max:%f Median:%f" % (df.petal_length.mean(),df.petal_length.std(),df.petal_length.min(),df.petal_length.max(),df.petal_length.median()))
B = pd.DataFrame({'team':['cle','cle','bos'],'ast':[10,9,8]}) $ B
index_to_change = df3[df3['group']=='treatment'].index $ df3.set_value(index=index_to_change, col='ab_page', value=1) $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
pd.merge(msftAR0_5, msftVR2_4)
train.cust_id.value_counts().unique(),test.cust_id.value_counts().unique()
p_diffs = np.array(p_diffs) #converting to numpy array as taught in nano degree $ plt.hist(p_diffs);
shelter_df_only_idx.registerTempTable("shelter") $ sqlContext.sql("SELECT OutcomeType_idx, AnimalType_idx, SexuponOutcome_idx, Breed_idx, Color_idx, AgeuponOutcome_binned FROM shelter").show() $
twitter_archive_master[twitter_archive_master.img_num != 1].head()
"cost is: {}".format(cost)
train = train.sort_values(by='date_created', ascending=True) $ train = train.loc[train['date_created'] >= pd.to_datetime('2016-01-01')]
dup_labels.intersection(dup_labels2)
output= "select tag_id, count(tag_id) as pm from (select tag_id,date as dt from tweet_tags inner join tweet_details where tweet_tags.tweet_id=tweet_details.tweet_id) where dt='2018-02-26' group by tag_id order by pm desc limit 10 " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['tag_id','Count'])
new_dems.Clinton.isnull().sum()
plt.plot(range(-5, 5), [math.exp(i) for i in range(-5, 5)])
df_3.to_csv('LBDLduplicitepermanente_2016_day_media.csv') $
soup.find_all(attrs={'id': 'gbar'})
activity = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\ $ filter(Measurement.station == Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ activity
user_converted = df['converted'].mean() $ print('Proportion of user converted:{}%'.format(user_converted*100))
import pandas as pd $ import numpy as np $ df=pd.read_csv('talks.csv') $ df.head()
autos["last_seen_10"].value_counts(normalize = True, dropna = False).sort_index()
daily_deltas = (hits_df.hits - hits_df.hits.shift()).fillna(0)
trump[trump['is_retweet']==True].count()
df_selparams.loc[df_namb.index, 'n_amb'] = df_namb.values $ df_selparams.loc[df_ndoor.index, 'n_door'] = df_ndoor.values $ df_selparams.loc[df_nmcu.index, 'n_mcu'] = df_nmcu.values $ df_selparams.loc[df_nrelay.index, 'n_relay'] = df_nrelay.values $ df_selparams.loc[df_ntemp.index, 'n_temp'] = df_ntemp.values
grouped = writers.groupby(['Country','Gender']) $ grouped.groups
pd.crosstab(calls_df['user'], calls_df['call_type'], margins=True)
!!from time import sleep $ for i in range(0, 100): $     print(i) $     sleep(0.1) $ do_something()
new_discover_sale_transaction = post_discover_sales[post_discover_sales['Email'].isin(new_customers_test['Post Launch Emails'].unique())] $ new_discover_sale_transaction['Total'].mean()
from sklearn.feature_extraction.text import TfidfVectorizer $ vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words="english")
print("Dimensions used by time:\t\t%s" % d.variables['time'].dimensions) $ print("Dimensions used by SATCTD7229_PRES:\t%s" % d.variables['SATCTD7229_PRES'].dimensions)
df_transactions.head()
df['consolidated_bottle_ml'].fillna('Other_ml', inplace=True)
autos["price"].value_counts().sort_index(ascending = True)
plt.plot(W,RMSE_list) $ plt.xticks(np.arange(min(W),max(W)+1,6.0)) $ plt.ylabel('RMSE') $ plt.xlabel('Window (months)') $ plt.title('RMSE by window')
click_condition_meta.to_csv('./data/action_condition_meta.csv', index = False) # for usage in other notebooks and scripts
n_old = df[df.landing_page == 'old_page'].count()[0] $ n_old
print(data.iloc[[265631]])
import imp $ imp.reload(imp)
contribs=pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
knowledge_per_component.sort_values(ascending=False).head(10).plot.bar();
gs_rfc_over.score(X_train, y_train_over)
remove_dog = np.intersect1d(remove_dog, dog3)
df2.drop(index=2893, inplace=True) $ df2.query("user_id == 773192")
master = master() $ master = master[['bbrefID', 'bats']] $ names = names.merge(master, left_on='key_bbref', right_on = 'bbrefID', suffixes=('_chadwick', '_lahman')) $ names = pd.DataFrame(names[['key_mlbam', 'batter_name', 'bats']])
df2_curr.head()
by_periods = msft_cum_ret .resample("M", kind='period').mean()
np.exp(results.params)
from sklearn.feature_extraction.text import CountVectorizer $ import nltk $ import re
IFrame('http://www.navan.name/roc/', width=700, height=800)
tlen_d.plot(figsize=(16,4), color='r');
%matplotlib inline $ taxa_count.plot(kind='bar') $
diff=new_page_converted.mean()-old_page_converted.mean() $ diff
loaddata = the_frame $ loaddata["time"] = loaddata.set_index("local_15min").index.tz_localize(pytz.timezone('America/Chicago'), ambiguous = True) $ loaddata["date"] = [ dt.datetime(d.year,d.month,d.day,0,0,0,0) for d in loaddata['time'] ] $ import pickle as pk $ pk.dump( loaddata, open( "data/demand.pkl", "wb" ) )
df2[df2.duplicated(['user_id'], keep=False)]
caps2_output_round_1_tiled = tf.tile( $     caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1], $     name="caps2_output_round_1_tiled")
pumashp = pumashp.merge(pumaBB[['public use microdata area','pcBB']],left_on='puma',right_on='public use microdata area') $ pumashp.head(1)
old_page_converted = np.random.binomial(n_old , p_old) $ old_page_converted
reddit.Upvotes.value_counts(ascending=False).head(25) #just seeing the number of upvotes for each Reddit post $
df.columns
def sharpe(returns, freq=30, rfr=0): $     peak = returns.max() $     i = returns.argmax() $     trough = returns[returns.argmax():].min() $     return (trough-peak)/trough
df.drop(["city","lat","lon"], inplace = True, axis = 1)
Q1 = numpy.array(h.logdict['Q1'][startcut:]) $ T1 = numpy.array(h.logdict['T1'][startcut:])
X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]) $
kimanalysis.getfile(model, 'kimspec.edn', contentformat='edn')
test_scores=test_join.filter('p_repo_id is not null')
data6_1 = pd.DataFrame(data=[tweet.text for tweet in tweets_neta], columns=['Tweets']) $ data6_2 = pd.DataFrame(data=[tweet.text for tweet in tweets_erdog], columns=['Tweets'])
print(r"'\xba\xba'.decode('mbcs'):",repr('\xba\xba'.decode('mbcs')))
from sklearn.metrics import f1_score $ f1_score(y_test, yhat, average='weighted') 
corn.filter(lambda x: x["price"].min() > 10, dropna = False)
df = pd.DataFrame({"counts1" : counts1, "counts2" : counts2}, index = ind) $ df
games_to_bet.drop(labels=[x for x in games_to_bet.index if x%2], inplace=True)
xirrs_all['payout_quarter'].to_clipboard()
new_seen_and_click =  seen_and_click[~(seen_and_click['seen_at'] > seen_and_click['clicked_at']) ] $ print(new_seen_and_click.shape) $ new_seen_and_click.head()
lims_query = "SELECT specimens.cell_depth, donors.weight, specimens.id, donors.id \ $               FROM donors INNER JOIN specimens ON donors.id = specimens.id" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
df_old[df_old['public']=='offline'].groupby('abuse_type').count()
full_globe_temp = pd.read_table(filename, sep="\s+", names=["year", "mean temp"]) $ full_globe_temp
@app.route("/api/v1.0/tobs") $ def tobs(): $     tob_results = session.query(Measurement.tobs).filter(Measurement.date >= 2017-8-23).all() $     all_tobs = list(np.ravel(tob_results)) $     return jsonify(all_tobs)
dc.head(2)
test_data_features = vectorizer.transform(test.review) $ test_data_features = test_data_features.toarray() $ predictions = clf.predict(test_data_features)
ts = pd.Series(np.random.randint(1,10,len(rng)), index = rng) $ ts.head(15)
print('First Example :\n=====\n', [ppm_body.id2token[x] for x in vectorized_body[0] if x > 1]) $ print('\nSecond Example :\n=====\n', [ppm_body.id2token[x] for x in vectorized_body[1] if x > 1])
financial_crisis.loc['Wall Street Crash / Great Depression'] = '1929 - 1932' $ print(financial_crisis)
result['timestampDate'] = result['timestampUTC'].dt.date
jobs = pd.read_csv("descriptions.csv.gz", compression='gzip')
save_n_load_df(df, 'rolled_filled_elapsed_events_df.pkl')
matrix1 = np.array([(2,2,2),(2,2,2),(2,2,2)],dtype='float32') $ matrix2 = np.array([(1,1,1),(1,1,1),(1,1,1)],dtype='float32')
a2 = 1 $ b2 = 0.2 $ df2['W'] = a2 + b2 * norm.rvs(size=N, loc=df['X']) $ df2['Y'] = a2 + b2 * norm.rvs(size=N, loc=df['W'])
df_members['bd_c'] = le.transform(df_members['bd_c'])
print ("evals_mean:", evals_mean.shape)
df_twitter_archive_master[(df_twitter_archive_master['tweet_id']=='666287406224695296') & (df_twitter_archive_master['favorite_count']==149)].jpg_url
hemisphere_img_pages = [] $ for link in url_hemispheres_link: $     himg = "https://astrogeology.usgs.gov" + link['href'] $     if himg not in hemisphere_img_pages: $         hemisphere_img_pages.append(himg)
%matplotlib inline $ import seaborn; seaborn.set()
%matplotlib inline $ import pandas_profiling $ import seaborn as sns; $ titanic = sns.load_dataset('titanic') $ pandas_profiling.ProfileReport(titanic)  # You may need to run cell twice
df.loc[df.toes.str.match(pattern1)==True]=df.loc[df.toes.str.match(pattern1)==True].replace(" ","",regex=True) $ df.loc[df.toes.str.match(pattern2)==True]=df.loc[df.toes.str.match(pattern2)==True].replace(" ","-",regex=True) $ df.loc[df.toes.str.match(pattern3)==True]=df.loc[df.toes.str.match(pattern3)==True].replace("'","",regex=True)
df_treat = df2.query('group == "treatment"') $ treat_convert = df_treat.query('converted == 1').shape[0] / df_treat.shape[0] $ treat_convert
history = model.fit(train_X, train_y, epochs=130, batch_size=72, validation_data=(test_X, test_y), verbose=1, shuffle=False) $ pyplot.plot(history.history['loss'], label='train') $ pyplot.plot(history.history['val_loss'], label='test') $ pyplot.legend() $ pyplot.show()
autos['last_seen'] = autos['last_seen'].str[:10].str.replace('-', '').astype(int) $ autos['last_seen'].head()
people.plot(kind = "scatter", x = "height", y = "weight", s=[40, 120, 200]) $ plt.show()
df_h1b_ft_US_Y = df_h1b_ft_US[df_h1b_ft_US.pw_unit_1=='Year']
obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])
def trainModel(m, xtr, ytr, ep, bsz,l=0): $     m.fit(xtr, ytr, batch_size=bsz, epochs=ep, validation_split=0.079, verbose=l) $     return m
y_pred = crf.predict(X_valid) $ metrics.flat_f1_score(y_valid, y_pred, $                       average='weighted', labels=labels)
series = df2['DepTime'].dropna().apply(deptime_to_string) $ series.count()
dule_files = [f for f in os.listdir(folder) if re.match('Dul.*.elec.*.csv', f)] $ dule_files
icecream.plot.bar(x ='Flavor', y='Price', color='green')
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df['group']) $ df2 = df2.drop(['control'], axis = 1) $ df2.sample(5)
import statsmodels.formula.api as smf
def f(x,a,mu,sigma): $     r=a*np.exp(-(x-mu)**2/(2*sigma**2)) $     return (r)
pd.merge(left,right, on='key')
data.head(5)
result2 = sm.ols(formula="port_ret ~ mkt_ret", data=tbl).fit() $ result2.summary()
plt.figure(figsize=(15, 10)) $ la_inspections_new.boxplot('score') $ plt.title("Health Inspection Scores") $ plt.show()
match = len(df[(df['group']=='control') & (df['landing_page'] == 'old_page')]) + len(df[(df['group']=='treatment') & (df['landing_page'] == 'new_page')]) $ print(df.shape[0] - match)
im_clean.to_csv("image_predictions_master.tsv", sep='\t')
plt.hist(p_diffs); $ plt.axvline(x = full_diff, color = 'red');
search_response.json()
print(data["Ganhadores_Sena"].sum()) $ print(data["Ganhadores_Quina"].sum()) $ print(data["Ganhadores_Quadra"].sum())
grinter_day0.number_int.max()
doi_pid["DOI"].unique().shape[0], doi_pid["PID"].unique().shape[0]
twitter_archive.rating_numerator.value_counts()
old_page_converted = np.random.choice([0,1], n_old, [1-p_old, p_old]) $
for col in missing_info: $     num_missing = df_[df_[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing))
list(set(nameOrder).intersection(nameTask)) $ list(set(nameTask).intersection(nameTask)) $ list(set(nameOrder).intersection(nameOrder))
!cat ../../data/msft_with_footer.csv # osx / Linux
df2 = df.copy() $ df2[df2 > 0] = -df2 $ df2
def get_sparse_matrix(trainIndices, valIndices): $     train_word_features = char_vectorizer.transform(train.loc[trainIndices, 'comment']) $     val_word_features = char_vectorizer.transform(train.loc[valIndices, 'comment']) $     return (train_word_features, val_word_features)
nonpitch_unique_labels = list(results.columns.difference(pitches.columns)) $ nonpitch_unique_labels
tmp_cov.index.names[0]
df_namb = (st_streams['/streams/amb'].count() * 100.0) / namb_instance $ df_ndoor = (st_streams['/streams/door'].count() * 100.0) / ndoor_instance $ df_nmcu = (st_streams['/streams/mcu'].count() * 100.0) / nmcu_instance $ df_nrelay = (st_streams['/streams/relay'].count() * 100.0) / nrelay_instance $ df_ntemp = (st_streams['/streams/temp'].count() * 100.0) / ntemp_instance
df.iloc[1:3, :]
df2.shape
p_old = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ print('Conversion of old Page is: ', p_old)
df_master[df_master.favorite_count==144118].jpg_url
samp311.columns
author_data.reset_index(inplace=True)
Z = np.zeros((8,8),dtype=int) $ Z[1::2,::2] = 1 $ Z[::2,1::2] = 1 $ print(Z)
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_axial_chord_crown[0][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_crown[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
s2 = df_image_clean[df_image_clean['img_num'] == 4].sample() $ s2.image_url
print('Groceries has shape:', groceries.shape) $ print('Groceries has dimension:', groceries.ndim) $ print('Groceries has a total of', groceries.size, 'elements')
tweet = "Wed Aug 27 13:08:45 +0000 2008" $ time_info = datetime.strptime(tweet,'%a %b %d %H:%M:%S +%f %Y')
plt.show()
train_df[["SibSp", "Survived"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)
plt.hist(p_diffs);
tcp_files = [f for f in os.listdir(folder) if re.match('.*TC.*.phys.*.csv', f)] $ tcp_files
plt.plot(MaxPercentage) $ plt.plot(RandomPercentage) $ plt.ylabel("percentual change used for profit") $ plt.xlabel("time") $ plt.show()
df.groupby('key').transform(lambda x: x - x.mean())
mask = (youthUser3["creationDate"] > '2016-01-01') & (youthUser3["creationDate"]<= '2016-12-31') $ youthUser2016 = (youthUser3.loc[mask]) $ youthUser2016.head()
df.Genre.value_counts()
old_page_converted = np.random.binomial(1, size = n_old, p= p_old) $ old_page_converted
df_stemmed_train = df['text_stemming'].tolist()
df.query("landing_page=='old_page' and group=='control'").shape[0]
hr, mm, ss, ms= 12, 21, 12, 21 $ dt.time(hr, mm, ss, ms)
fullDf[pd.isnull(fullDf.level)].location.value_counts()
df2_treatment = df2.query('group == "treatment"')
co2_concentration = pd.read_table("data/greenhouse_gaz/co2_mm_global.txt", sep="\s+", $                                   parse_dates = [[0, 1]]) $ co2_concentration
access_logs_parsed = access_logs_raw.map(lambda x: parse_apache_log_line(x)).filter(lambda x: x is not None)
ideas.drop(['Unnamed: 0', 'Week','Month'],axis=1,inplace=True)  # Dropping columns - new week column to be engineered
!wget 'https://datahack-prod.s3.amazonaws.com/test_file/test_nvPHrOx.csv'
reddit['Subreddits'].value_counts().sum() #how many total subreddits do we have? 
liquor2016_q1_volume = liquor2016_q1.VolumeSoldLiters.groupby(liquor2016_q1.StoreNumber).agg(['sum']) $ liquor2016_q1_volume.columns = ['Volume'] $ liquor2016_q1_volume.tail()
tweets_kyoto.describe()
df_with_metac_with_onc = df_with_metac_with_onc.drop(columns = ['ONC_LATEST_STAGE']) $ df_with_metac_with_onc.head()
query = center_attendance_pandas["center_name"] == "Brookline Community Center" $ brookline_center_attendance = center_attendance_pandas[query] $ brookline_center_attendance.head(10)
data_tokenized = [] $ for item in data_splitted: $     item = item.split(",") $     data_tokenized.append(item) $ print(data_tokenized[0][0])
tokenizer = RegexpTokenizer(r'\w+') $ bill_tokens= [tokenizer.tokenize(i) for i in ogxprep.billtext] $ new_list = [" ".join([lemmatizer.lemmatize(word) for word in x]) for x in bill_tokens]
new_df = pd.DataFrame() #creates a new dataframe that's empty $ new_df["index"] = range(num_embedding) $ import csv $ new_df.to_csv(os.path.join(LOG_DIR, 'output2.tsv'), sep='\t', quoting=csv.QUOTE_NONE) $
subject_df = latest_df[['classification_id', 'subject_ids', 'subject_data']].copy() $ subject_df['subject_data'] = subject_df.subject_data.apply(lambda x: list(json.loads(x).values())[0]['Filename'])
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep ='\s+', nrows=80, skiprows=(1,2,5,10,60), usecols=(1,3,5)) $ print("df.head(5)\n",df.head(5)) $ print("Length of df = ",len(df))
scolumns = inspector.get_columns('station') $ for c in scolumns: $     print(c['name'], c["type"])
type(t2.index)
data = pd.read_csv("datosSinNan.csv") $ test = pd.read_csv("datatestSinNan.csv")
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])
catalog_df.rename(columns={'title' : 'Course Name'})
features = [col for col in df.columns if not col == 'Close'] $ X = normalized_df[features] $ model.predict(X) $
secclintondf.tags[0] $
crs = {'init': 'epsg:4326'} $ geometry = df_TempIrrs['lineString'] $ geo_TempIrrs = gpd.GeoDataFrame(df_TempIrrs, crs=crs,geometry = geometry)
im.sample(5)
pd.options.display.max_colwidth=100
df2.query('converted == 1').shape[0] / df2.shape[0]
cust = cust.sort_values(['transaction_date', 'membership_expire_date']).reset_index(drop = True)
reviews.rename_axis('wines',axis='rows') $
trips.dtypes
cluster = model.transform(feature_sel).cache() $ cluster_pd = cluster.toPandas()
cur = conn.cursor() $ cur.execute('UPDATE actor SET first_name = CASE WHEN first_name = \'HARPO\' THEN \'GROUCHO\' ELSE \'MUCHO GROUCHO\' END WHERE actor_id = 172;') $
from sklearn.cross_validation import KFold $ kf = KFold(25, n_folds=5, shuffle=False) $ print('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations')) $ for iteration, data in enumerate(kf, start=1): $     print('{:^9} {} {:^25}'.format(iteration, data[0], data[1]))
image_predictions_df.info()
import scipy.optimize
multi_index=pd.MultiIndex.from_tuples([(i, j, k) for i , j, k in zip(label_index, $                                                                      community_index, poverty_data['Category'].tolist())])
from sklearn.datasets import california_housing $ data = california_housing.fetch_california_housing()
df2['landing_page'].replace({'old_page':0,'new_page':1},inplace=True) $
combined_df = pd.merge(business_df, address_df,  how = 'left', left_on = 'prop', right_on = 'nndr_prop_ref')
df_enhanced.info()
d1 = stms.dCohen(m_ms[score_variable], a_ms[score_variable]).effect_size() $ d2 = stms.dCohen(m_hc[score_variable], a_hc[score_variable]).effect_size() $
cur.execute("select * from sqlite_master where type == 'table';").fetchall()
plt.scatter(X2[:, 0], X2[:,1])
y_age = pd.DataFrame(X_age_notnull['age'], columns=['age']) $ X_age = deepcopy(X_age_notnull) $ X_age.drop('age', axis=1, inplace=True)
for index in All_tweet_data_v2.index: $     if (All_tweet_data_v2.loc[index,'pupper']=='None' and All_tweet_data_v2.loc[index,'puppo']=='None' and All_tweet_data_v2.loc[index,'floofer']=='None' and All_tweet_data_v2.loc[index,'doggo']=='None'): $         All_tweet_data_v2.loc[index, 'unknown']='Unknown' $     else: $         All_tweet_data_v2.loc[index, 'unknown']='None'
train_data.groupby(['device.browser']).agg({'visitNumber': 'count'}).reset_index().set_index("device.browser",drop=True).plot.bar()
raw_large_grid_df.groupby(["subject","eyetracker"],as_index=False).duration.agg("median").groupby("eyetracker").agg("mean")
test = kimanalysis.queryitems('test', limit=1).iloc[0]['short-id'] $ kimanalysis.queryresults(test=test)
data = [12,44,23,2,1,66,7,85,3,34,23,6,34,23,7,21,14,34,23,88] $ data.sort() $ print "The mean is " + str(np.mean(data)) $ print "The median is " + str(np.median(data)) $ print "The mode is " + str(stats.mode(data))
probs = model2.predict_proba(X_test) $ print probs
season16 = ALL[(ALL.index >= '2016-09-08') & (ALL.index <= '2017-02-05')]
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=850) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
check_df = pd.read_csv(file_name) $ check_df.head()
logreg = LogisticRegression(penalty='l1', solver='liblinear') $ y_pred = cross_validation.cross_val_predict(logreg, X, y, cv=5) $ print(metrics.accuracy_score(y, y_pred))
!rm train.zip
for t in tweets: $     t["date"] = datetime.strptime(t["date"], "%Y-%m-%d").date() $ tweets[:2]    
%%time $ max_key = max( r_dict.keys(), key = get_open_price) $ min_key = min( r_dict.keys(), key = get_open_price) $ print('highest opening price: '+ str( get_open_price(max_key) ) ) $ print('lowest opening price: '+ str( get_open_price(min_key) ) )
users =pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv' ) $ sessions =pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv' ) $ products =pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv' ) $ transactions =pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv')
dt.datetime(yr, mo, dd, hr, mm, ss, ms)
df.subreddit.value_counts()
iterator.get_batch(2)
fps.model_dir
ts = pd.Series(np.random.randint(1,10, len(rng)), index=rng) $ ts.head(10) $
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
tablename='iplogs' $ pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False).head(10)
dfb_train.isnull().sum()
au.find_some_docs(ao18_coll,sort_params=[("id",1)],limit=3)
print(depthdf['time_taken'].sum())
import numpy as np $ ok.grade('q03')
una_tweets = pd.DataFrame(una) $ una_tweets
df2['landing_page'].value_counts()[0] / len(df2)
lel_dummies.head()
num_early_users = len(people_person[people_person['date_joined'] <= '2017-01-12']['id'].unique()) $ print("The number of users signed up prior to 2017-01-12 is: ", num_early_users)
autos = autos.drop(["nr_of_pictures"], axis=1)
lr_y_score = model.predict_proba(X_test)[:, 1] #[:,1] is formatting the output $ lr_y_score
autos['price'].value_counts().sort_index(ascending=True).head(20)
train_ = processed_tweets.loc[processed_tweets.set == 'train'] $ test_ = processed_tweets.loc[processed_tweets.set == 'test']
q1= pd.Period('2017Q1') $ q1
series = frame.iloc[0]
df2.head()
corr = all_coins_df.iloc[:,1:].pct_change().corr(method='pearson') $ sns.heatmap(corr, xticklabels=[col.replace("_price", "") for col in corr.columns.values], $             yticklabels=[col.replace("_price", "") for col in corr.columns.values],vmin=0, vmax=1, cmap="Greens") $ plt.show()
frame.head(3)
df_download_node=pd.merge(df_download_node,df_users_2[['uid','no_of_downloads','downloaded_or_not']],left_on='uid',right_on='uid',how='inner')
myplot = plot_ensemble_score(means) $ myplot.savefig("AUC.png", dpi=300)
vhd['season'] = vhd.index.str.split('.').str[0] $ vhd['term'] = vhd.index.str.split('.').str[1]
df.set_index('userid', inplace=True)
df['body'] = df['body'].str.replace('[^\w\s]','')
p_old = df2.query('landing_page == "old_page"').converted.mean() $ p_old
pd.melt(df, id_vars=['A'], value_vars=['B', 'C']) $
train_view.sort_values(by=3, ascending=False)[0:10]
%%time $ lr1 = LogisticRegression(random_state=20, max_iter=10000, C=0.5, multi_class= 'ovr', solver= 'saga') $ lr1.fit(X, y) $ lr1.score(X_test, y_test)
with open("allMovies_new.json", "r") as fd: $     movie_df = json.load(fd) $     movie_df = pd.DataFrame(movie_df)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()
tweets['text'].str.lower().str.contains('donald trump|president trump').value_counts()
Meter1.MakeMeasurment()
metadata['spatial_extent'] = refldata.attrs['Spatial_Extent_meters'] $ metadata
fixed_bonus_points = bonus_points.fillna(0) $ fixed_bonus_points
est = trading.regression()
d[d['name']=='Bo'].jpg_url
universe = ["ADS.DE", "SAP.DE", "BAS.DE", "VOW3.DE", "CON.DE"] $ price_data = yahoo_finance.download_quotes(universe) $ return_vector = price_data.pct_change().dropna() $ print return_vector
learner.metrics = [accuracy] $ learner.unfreeze()
values = [] $ for df in dfs: $     values.append(df['Outside Temperature'].max()) $ print (values)
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
np.log(0.025/0.28)
gs_k150.score(X_test, y_test_over)
X_new = model.transform(X_tfidf)
ControlTrtmtConverted = df2.query('group == "treatment" and converted ==1')['user_id'].count() $ print("Probability of Treatment Group Converted: ",(ControlTrtmtConverted/ControlGroup)) $ trtmt_conv_rt = (ControlTrtmtConverted/ControlGroup)
print(hn.created_at.min()) $ print(hn.created_at.max())
df.loc[[1,2],:]
cumulative_returns.plot()
news_df.to_csv('Output_CSVs/news_analysis.csv') $
list(zip(features,(np.std(train_features_arr)*logisticRegr.coef_[0])))
df_final.shape[0]
dr.index
! 7za x ./data/train-jpg.tar.7z $
!conda install -c conda-forge --prefix {sys.prefix} --yes ipywidgets
from bmtk.analyzer.visualization.spikes import plot_rates_popnet $ cells_file = 'network/recurrent_network/node_types.csv' $ rates_file = configure['output']['rates_file'] $ plot_rates_popnet(cells_file,rates_file,model_keys='pop_name')
ranking = pd.read_csv('datasets/fifa_rankings.csv') # Obtained from https://us.soccerway.com/teams/rankings/fifa/?ICID=TN_03_05_01 $ fixtures = pd.read_csv('datasets/fixtures.csv') # Obtained from https://fixturedownload.com/results/fifa-world-cup-2018 $ pred_set = [] $
df2 = pd.read_csv("../nba-enhanced-stats/2017-18_teamBoxScore.csv") $ df2.head()
input_node_types_DF = pd.read_csv(input_models_file, sep = ' ') $ input_node_types_DF
new_colorder = [ 'empID', 'name', 'salary', 'year1', 'year2'] $ df.reindex(columns = new_colorder).head(2)
df_movies[df_movies.movieId == "907474"]
d_ny = d_utc.tz_convert('US/Eastern') $ d_ny
results3.summary()
pd.merge(df1, df3, left_on="employee", right_on="name" ).drop('name', axis=1) #axis=1 is column (=0 is row) $
now = datetime.now() $ christmas = datetime(2016, 12, 25, 0, 0, 0) $ christmas - now
os.getcwd()
ratio_df=twitter_df_clean.groupby('ratio')['ratio'].count() $ plt.scatter(x=ratio_df.index.get_level_values(0), y=[n for n in ratio_df.values])
with_countries['US_ab_page'] = with_countries['ab_page']* with_countries['US'] $ us_new_page = sm.Logit(with_countries['converted'], with_countries[['intercept', 'ab_page', 'US_ab_page', 'US']]).fit() $ print(us_new_page.summary())
lda_model_filepath = paths.lda_model_filepath
data.loc[data['hired']==1].groupby('category').num_completed_tasks.mean()
df_countries.set_index('user_id', inplace=True)
text = " ".join(tweetering['Text'].values.astype(str)) $ match = re.findall(r"#(\w+)", text) $ vict_count = collections.Counter(match) $ vict_det=vict_count.most_common() $
pd.Series([1, 2, 3, 6., 'foo']).dtypes
skills = pickle.load(open('./data/skills.pickle.dat', 'rb')) $ sim = TagSimilarity(skills, df.iloc[:-100]) $ sim.train()
tweets['chars'] = tweets['full_text'].str.len()
for_plaintiff = like_plaintiff > like_defendant $ for_plaintiff.sum(), for_plaintiff.count()
wd=1e-7 $ bptt= 70 $ bs= 52 $ opt_fn = partial(optim.Adam, betas=(0.8, 0.99))
init_sigma = 0.01 $ W_init = tf.random_normal( $     shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims), $     stddev=init_sigma, dtype=tf.float32, name="W_init") $ W = tf.Variable(W_init, name="W")
ypred = lr.predict(X_test)
ps.to_timestamp()
df.index.name=None $ df.reset_index(inplace=True)
Y_valid_df = pd.DataFrame(Y_valid, columns=['Tag']) $ Y_valid_df = pd.get_dummies(Y_valid_df)
df_test.shape
df[df.state.isin(invalid_states)].state.value_counts().sort_index()
df_2012['bank_name'] = df_2012.bank_name.str.split(",").str[0] $
n_old = control.shape[0] $ n_old
localized.asi8[0]
print("R^2 = ", compute_perf(finalmodel, tsX, tsY)) $ finalmodel.save("finalmodel.sav")
df[df.country == "es"].id.size
np.exp(-0.0150), 1/np.exp(-0.0150)
df.main_category.unique()
dbdata = pd.read_sql_query(dbquery, con=dbcon) $ dbdata.head()
df = pd.DataFrame(A, columns=list('QRST')) $ df - df.iloc[0]
popCon[popCon.content == 'link'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
pd.value_counts(train_data["geoNetwork.subContinent"],normalize=True)
dates = ['07-01', '07-02', '07-03', '07-04', '07-05'] $ trip_df['date'] = dates $ trip_df.head()
eg = train.school_state.value_counts()
merged_data.head()
to_be_predicted_Day2 = 17.7492913 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
criteria = (df.salary > 30000) & (df.year1==2017) $ print (df[criteria])
df_new.to_csv('bank-financial-info.csv', index=False)
a.isalnum()
cp311.head(2)
order_data['CohortGroup'] = order_data.groupby(level =0)['created'].min().apply(next_weekday)
train_size = 0.7 $ unique_days = np.unique( df['Date'].values ) $ split_date = unique_days[ math.floor(len(unique_days)*train_size)] $ split_date
conn.summary(table=dict(name='data.iris', caslib='casuser'))
RK=match_id(merkmale, merkmale.Merkmalcode.isin(['RK'])) $ RK.Merkmalcode.unique()
plt.plot(range(df_concensus_uaa_rev.shape[0]), df_concensus_uaa_rev['esimates_count'])
counts_df.tail()
df.head()
X_train.shape, y_train.shape, X_test.shape, y_test.shape
sets = sets[date_column.notnull()] $ sets.head(12)
twitter_archive.retweeted_status_id.count()
print(autos['ad_created'].str[:10].value_counts(normalize = True, dropna = False).sort_index()) $ print(autos['ad_created'].str[:10].value_counts(normalize = True, dropna = False).shape)
themes = themes.lower()
from datetime import date, datetime $ max_date = datetime.strptime(str(max(df['timestamp'])[0:10]), '%Y-%m-%d') $ min_date = datetime.strptime(str(min(df['timestamp'])[0:10]), '%Y-%m-%d') $ max_date - min_date
from scipy.stats import norm $ norm.cdf(z_score) # Tells us how significant our z-score is
hp.listprice = hp.listprice.astype('float', inplace=True)
msft['2012-02'].head(5)
months = users.pivot_table(index = 'Month', $                       columns = 'Active', $                       values = 'CreationDate', $                       aggfunc = 'count', margins = True) $ months['% of Total'] = (months[True] / months['All'] * 100)
bruins = pd.read_csv('../../../data/bruins/home.csv')
df.mean()
df_predictions=pd.read_table(r.url,sep='\t')
in_forecast = lambda x: np.logical_and(in_domain(x), in_range(x.time, 'time')) $ print('in_forecast: %6d'%in_forecast(adm).sum())
stack_with_kfold_cv.to_csv('stacking_input/stack_kfold_cv_pub_mine.csv',index=False)
sess.get_historical_data('ibm us equity','g:ohlc', start_date='2014-01-01', end_date='2015-01-01').head()
ndf = new_df.copy() $ ndf.index = range(new_total) $ ndf.head()
sep2014.start_time, sep2014.end_time
impute_solar_wind_df = pd.read_excel(weather_folder + '/averaged_solar_wind.xlsx') $ impute_solar_wind_df.head()
results.summary()
models = { $           'rf' :RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1, verbose = 2 ,max_depth=17,min_samples_leaf=17 ) , $           'et' :ExtraTreesClassifier(n_estimators=50, random_state=42, n_jobs=-1, verbose = 2 ,max_depth=17,min_samples_leaf=17 , max_features = 0.8) , $          }
joined_sample_saved.show()
pd.DataFrame(population, columns=['population'])
jobPostDFSample = jobPostDF.sample(100)
archive_copy[['tweet_id', 'id_str']].info()
train, valid, test = flight_hex.split_frame([0.6, 0.2], seed=1234) $ flight_X = flight_hex.col_names[2:] $ flight_y = flight_hex.col_names[1]    $
a.shape[1]
%%time $ pool = ThreadPool(16) $ doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED) $ threaded_reps = pool.map(infer_one_doc, doc_contents) $
DD_p= df_pol['domain'].value_counts().head(29).index.tolist() $ df_pol['domain_d'] = [type_ if type_ in DD_p $                       else "OTHER" for type_ in df_pol['domain']] $ print(df_pol['domain_d'].nunique()) $ print(df['domain_d'].nunique())
results.predict(exog=[1, mean_expr])
data.head(10)
from matplotlib_venn import venn3 $ plt.figure(figsize=(8,8)) $ plt.rcParams.update({'font.size': 9}) $ venn3(subsets={'100':n_just_jobroles,'010':n_just_titles,'001':n_just_tags,'110':n_just_jobroles_titles,'111':n_all,\ $                '011':n_just_titles_tags,'101':n_just_jobroles_tags},set_labels=('Job Roles','Titles','Tags')) $
lr2.fit(X_new, y_tfidf)
ts.shift(1)
df3.head()
data.info()
df['const2'] = 1 $ print("... added constant")
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-01-02&api_key=XXXXX') $ r.status_code $ r.headers['content-type'] $ r.encoding $ r.text
print('Retweet tweets: ' + str(len(tweets_clean.query('retweeted_status_id == retweeted_status_id')))) $ print('Total tweets: ' + str(len(tweets_clean))) $ tweets_clean = tweets_clean[tweets_clean.retweeted_status_id != tweets_clean.retweeted_status_id]
g_live_df = live_df.groupby(['blurb']) $ live_df_filtered = g_live_df.filter(lambda x: len(x) <= 1).sort_values(['blurb','launched_at'])
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='two-sided') $ z_score, p_value
np.vstack((a, a))[1::,:] # through the end. 
titanic_df['cabin'] = titanic_df['cabin'].apply( $     lambda x: None if not x else str(x).split()[0])
brand_mean_price={} $ for b in popular_brands: $     brand = autos[autos["brand"] == b] $     mean_price = brand["price"].mean().astype(int) $     brand_mean_price[b] = mean_price
df[(df.group == 'treatment')&(df.landing_page != 'new_page')].shape
newset = pickle.load(open('preprocess.p', mode='rb'))
mismatch_g2 = df.query("group == 'control' and landing_page == 'new_page'") $ len(mismatch_g2)
df2.head()
df.head()
measurement = Base.classes.measurement $ Stations = Base.classes.station
print(pd.merge(df1,df2))
PIL.Image.from
autos[["ad_created", $        "date_crawled", $        "registration_year", $        "registration_month", "last_seen"]].info()
figure, axes = plt.subplots() $ num_fli_by_month.plot(ax=axes, kind='bar', color='blue') $ axes.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sept','Oct','Nov','Dec']) $ axes.set_xlabel("Month")
n_old = df2['landing_page'].value_counts()[1] $ print('Number of tests with the old page:', n_old)
df_ta_caps_user.to_csv('BaseTravailPSE')
geocoded_df[list(filter(lambda x: x.endswith('Date'), geocoded_df.columns))].sample(5)
for colour in ['red','green','blue']: $     print(nbar_clean.variables['red'][0,:,:])
df_goog.Close.asfreq('D', method='backfill')
df1 = df.drop(['Time', 'Amount', 'Class'], axis=1) $ df1.boxplot(figsize=(20, 10)) $ plt.show() $
num2=df.query('landing_page!="new_page" and group=="treatment"').count()[0]
round(len(df_twitter[df_twitter.dog_label == 'floofer']) / len(df_twitter.dog_label), 3)
run txt2pdf.py -o "VIA CHRISTI HOSPITALS WICHITA, INC  Sepsis.pdf"   "VIA CHRISTI HOSPITALS WICHITA, INC  Sepsis.txt"
total= pd.concat([df1,df2]) $ total.index=range(total.shape[0]) $ total
!tar -xvf ssd_mobilenet_v2_coco_2018_03_29.tar.gz
df2=df2.drop('control',axis=1)
df.describe()
session_v2.head()
df.index
plt.scatter(df['gdp_per_capita'], df['big_mac_price']) $ plt.show()
plt.scatter(df['ticket_price'], df['percentage_seats_sold']) $ plt.xlabel('price') $ plt.ylabel('percentage of seats sold') $ plt.title('2016') $ plt.show()
conn_a.commit()
type2017.head()
predictions = ab.predict(test[['expenses', 'floor', 'lat', 'lon', 'property_type',\ $                                 'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']])
X_testfinaltemp = X_testfinal.copy() $ X_testfinaltemp = X_testfinaltemp.rename(columns={'fit': 'fit_feat'})
tweets_clean.head()
support.sort_values("amount", ascending=False).head()
classify_df = pd.get_dummies(adopted_cats, columns=['Intake Type', 'Intake Condition'])
ax = mains.plot() $ h.steady_states['active average'].plot(style='o', ax = ax); $ plt.ylabel("Power (W)") $ plt.xlabel("Time"); $
df_goog[['Open','Close','High','Low','Adj Close']].plot()
sizes = [0.0,] $ for i in range(0,len(newdf)-1): $     size = 0.4 / newdf['Ret_stdev'][i] $     sizes.append(size) $
price_to_num = lambda x: float(x.replace("$","")) $ df['Price'] = df['Price'].map(price_to_num)
polarity_avg.to_csv('polarity_results_LexiconBased/daily2012/polarity_avg_daily_2012.csv', index=None) $ polarity_count.to_csv('polarity_results_LexiconBased/daily2012/polarity_count_daily_2012.csv', index=None)
convert_rate = (df2['converted'] == 1).sum() / len(df2.index) $ pnew = convert_rate $ print(pnew)
uk = ['London', 'United Kingdom', 'England'] $ notus.loc[notus['country'].isin(uk), 'country'] = 'UK' $ notus.loc[notus['cityOrState'].isin(uk), 'country'] = 'UK' $ notus.loc[notus['country'] == 'UK', 'cityOrState'].value_counts(dropna=False)
with open('../data/categories.json', 'r') as f: $     video_categories = json.load(f)
math.exp(max(df['genes'])/1000)
greater_than_difference = [] $ for i in range(len(p_diffs)): $     if p_diffs[i] > obs_mean: $         greater_than_difference.append(p_diffs[i]) $
ripple = df[df['tweet'].str.contains("ripple") | df['tweet'].str.contains("Ripple") | df['tweet'].str.contains("XRP")] $ ripple = ripple.reset_index(drop=True) $ ripple.info()
plt.figure(figsize=(8, 5)) $ plt.hist(train_df.favs_lognorm); $ plt.title('The distribution of the property favs_lognorm');
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 2) & (jobs.GPU == 1)][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
newgeo = pd.concat([us, notus]) $ len(geo) == len(newgeo)
for df in Train, Test: $     df['DOB_NAind'] = (df.DOB_clean == 1) * 1 $     df.describe()
ndvi = ((nbar_clean.nir-nbar_clean.red)/(nbar_clean.nir+nbar_clean.red)) $ ndvi = ndvi.where(ndvi>=0.0) $ ndvi_of_interest= ndvi.sel(time = time_slice, method='nearest') #select scene $ ndvi.attrs['crs'] = crs $ ndvi.attrs['affine'] = affine
barcelona.head()
df3.iat[2, 3] = 0 $ df3
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key='YOUR_API_KEY'&start_date=2018-07-04&end_date=2018-07-05")
df2['ab_page'] = pd.get_dummies(df['group']) ['treatment'] $ df2.head()
false_scr_churn_bool = [np.any([SCR_PLANS_df.loc[unchurn]['status'][i] !='canceled' for i,j in enumerate(SCR_PLANS_df.loc[unchurn]['scns_created']) if j==max(SCR_PLANS_df.loc[unchurn]['scns_created'])]) for unchurn in scr_churned_ix] 
submit.to_csv('lgbmabs.csv', index=False)
results['gender_sentiment'].plot.bar() $ import matplotlib.pyplot as plt $ plt.show()
logs['key'].value_counts().plot() $ plt.show()
print('Bill id repeat id', df_bill_id[df_bill_id.duplicated(['patient_id'])].shape) $ print('Bill repeat id', df_bill[df_bill.duplicated(['bill_id'])].shape)
double_app = picker[picker==2] $ print len(double_app) $ df_nona['create_date'] = df_nona.created.map(pd.to_datetime) $ dbl = df_nona[df_nona.district_id.isin(double_app.index)].groupby(['district_id', 'app_id', 'create_date']).install_rate.mean() $ dbl
dates = ["2016-10-11", "2016-11-09", "2016-09-09", "2016-10-19"] $ df = pd.DataFrame({"name": ['john', 'alice', 'bob', 'jane'], $                    "age": [34, 56, 31, 24], $                    "subs": [True, True, False, True], $                    "logged": [pd.Timestamp(x) for x in dates]})
old_page_converted = np.random.choice([0,1],N_old, p=(p_old,1-p_old)) $ old_page_converted
data.columns = ['West', 'East'] $ data['Total'] = data.eval('West + East')
checkDF = pd.read_csv(path_join(DATA_DIR, 'best_submition/submission_RIt.csv'))
mydf1 = pd.DataFrame() $ mydf1[['datetime','speed','distance','fuelVoltage', 'dev_state']] = df1[['received_at','speed','distance','FuelVoltage', 'dev_state']] $
frequent_authors.head()
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(df['text']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(df['text']))) $ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(df['text'])))
df2 = df.copy()
precipitation_df = pd.DataFrame(one_yr_prcp, columns=['Date', 'Prcp']) $ precipitation_df['Date'] = pd.to_datetime(precipitation_df['Date'], format='%Y/%m/%d') $ precipitation_df.set_index(precipitation_df['Date'], inplace=True) $ precipitation_df.head()
import requests $ import json $ url = "https://www.dcard.tw/_api/posts?popular=false" $ resp = requests.get(url, headers=headers)
old_converted = np.random.choice([1, 0], size=len(df_old), p=[P_mean, (1-P_mean)]) $ old_converted.mean()
fig, ax = plt.subplots() $ ax.scatter(df.track_popularity, df.artist_popularity) $ ax.set_title('Artist Popularity vs. Track Popularity') $ ax.set_xlabel('Track Popularity') $ ax.set_ylabel('Artist Popularity')
pgh_311_data['REQUEST_TYPE'].value_counts(ascending=True).plot.barh()
users_orders = users_orders.assign(Cost_14 = lambda x: x.Costs * (x.Reg_date + pd.DateOffset(14) >= x['Order Date'])) $ users_orders = users_orders.assign(Cost_30 = lambda x: x.Costs * (x.Reg_date + pd.DateOffset(30) >= x['Order Date'])) $ users_orders = users_orders.assign(Cost_60 = lambda x: x.Costs * (x.Reg_date + pd.DateOffset(60) >= x['Order Date'])) $ users_orders.head()
data.drop(["link_flair", "ops_flair"], axis=1, inplace=True)
mean = np.mean(twitter_data['length']) $ likes_max = np.max(twitter_data['Likes']) $ print('mean = ' + str(mean)) $ print('max likes = ' + str(likes_max))
autos[["price", "odometer_km"]].describe()
cursor.execute(sq81) $ cursor.execute(sq82) $ results = cursor.fetchall() $ results
os.chdir("..")
to_be_predicted_Day5 = 48.310516 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
xgb.fit(X_train, y_train) $ test_predictions = xgb.predict(X_test) $ eval_sklearn_model(y_test, test_predictions, model=xgb, X=X_test)
stocksDf = spark.read.option("header", True).option("inferSchema", True).csv("stocks") $ stocksDf.cache() $ stocksDf.count()
path = os.path.join('Data for plots', 'Monthly index.csv') $ monthly_index.to_csv(path, index=False)
logit = sm.Logit(df_new['converted'],df_new[['intercept','treatment', 'CA', 'UK']]) $ results = logit.fit() $ results.summary()
urls_pattern = re.compile(r'(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?\xab\xbb\u201c\u201d\u2018\u2019]))')
S.decision_obj.stomResist.value = 'BallBerry' $ S.decision_obj.stomResist.value
week47 = week46.rename(columns={329:'329'}) $ stocks = stocks.rename(columns={'Week 46':'Week 47','322':'329'}) $ week47 = pd.merge(stocks,week47,on=['329','Tickers']) $ week47.drop_duplicates(subset='Link',inplace=True)
df_loantoVal = df[(df.Loan_Amount != 0) & (df.Property_Value != 0)] $ df_loantoVal['LoanToValue'] = (df_loantoVal.Loan_Amount/df_loantoVal.Property_Value)
df2.drop_duplicates('user_id',keep='first',inplace=True) $ df2[df2.duplicated('user_id',keep=False)] $
toRG.sum().tail()
windfield_proj = windfield.GetProjection() $ windfield_proj
df['TMAX']= df['TMAX'] * 0.10 $ df['TMIN']= df['TMIN'] * 0.10 $ df['TOBS']= df['TOBS'] * 0.10 $ df['PRCP'] = df['PRCP'] * 0.10
top_supporters = top_supporters.groupby("contributor_cleanname").amount.sum().reset_index().sort_values("amount", ascending=False)
group1 = len(df.query('group!="treatment" and landing_page=="new_page"'))# number of times when group is not treatment but langing page is new page $ group2 = len(df.query('group!="control" and landing_page=="old_page"'))# number of times when group is not control but langing page is old page $ group=group1+group2 $ group
cvecdata =cvec.fit_transform(X_train)
sample = sample[sample['polarity'] != 2] $ sample['sentiment'] = (sample['polarity'] ==4).astype(int)
from pandas_datareader.data import Options $ goog = Options('goog', 'google') $ data = goog.get_options_data(expiry=goog.expiry_dates[0]) $ data.iloc[0:5, 0:5]
pivoted_data.resample("Y").sum().plot(figsize=(10,10))
twitter_archive_clean['stage']=twitter_archive_clean['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True) $ twitter_archive_clean.drop(columns=['doggo','floofer','pupper','puppo'],inplace=True)
xgb_learner.random_search(100, verbose=True)
"My name is %f and i'm %f" % (13.1, 12.0)
hist = sns.distplot(senti_vader, bins=20)
null = df.isnull().values.sum() $ print('The number of missing values in the dataset is {}'.format(null))
sns.heatmap(rhum)
df2.drop_duplicates(subset='user_id', keep="last", inplace=True) $
pred.img_num.value_counts() $
price_mat.columns = names $ mcap_mat.columns = names $ volumes.columns = names
p_converted_control_user2 = df2.query('converted==1 and group=="control"').user_id.nunique()/df2.query('group=="control"').user_id.nunique() $ p_converted_control_user2
asf = after_sherpa["Date"].value_counts() $ asf.to_csv("GG_a.csv") $ cols = ['Day', 'Count'] $ asfr = pd.read_csv("GG_a.csv", header=None, names=cols) $
df_t = df_new $ df_t['timestamp'] = pd.to_datetime(df_t['timestamp']) $ df_t.head(3)
from sodapy import Socrata $ client = Socrata("data.cityofnewyork.us", os.getenv("apptoken"))
url = "https://data.cityofnewyork.us/download/i8iw-xf4u/application%2Fzip" $ urllib.request.urlretrieve(url, 'nyc.zip') $ os.system("unzip -d %s nyc.zip"%(os.getenv("PUIDATA"))) $ nycshp = gpd.GeoDataFrame.from_file((os.getenv("PUIDATA") + "/ZIP_CODE_040114.shp"))
plt.title('GC_MARK_MS') $ (aggregated_content + aggregated_parent).plot(kind='bar', figsize=(15, 7))
hist(df.pre_clean_len,100) $ grid()
filter_df = filter_df[filter_df['start_time'] <= datetime(2016, 11, 8, 23, 59, 59)] $ filter_df.head(2)
model.add(Conv1D(filters=10, kernel_size=10, $                  activation='relu', padding='same')) $ model.add(MaxPooling1D(pool_size=10)) $ model.add(Dropout(rate=0.25))
autos.price.unique()[-10:]
pnew = df2.converted.mean() $ pnew
newdf.describe()
df['genre'].value_counts()
y_hat = nb.predict(train_4)
info={'1-id_product':product.asin.unique()[0]} $ info['3-rating_average'] = product.overall.mean() $ info['2-number_reviewers'] =len(product.reviewerID.unique()) $ info
subrcvec = pd.DataFrame(cvecdata.todense(),      $              columns=cvec.get_feature_names())      # subreddit cvec $ subrcvec.shape
squares.index
print(df['one'][1]); df.iloc[1,0]
weather['precip_total'] = weather['precip_total'].replace('NaN', None, regex=False).fillna(0) $ weather['pressure_avg'] = weather['pressure_avg'].replace('NaN', None, regex=False).fillna(0) $ weather['wind_speed_peak'] = weather['wind_speed_peak'].replace('NaN', None, regex=False).fillna(0)
%%writefile /tmp/test.json $ {"dayofweek": "Sun", "hourofday": 17, "pickuplon": -73.885262, "pickuplat": 40.773008, "dropofflon": -73.987232, "dropofflat": 40.732403, "passengers": 2}
import statsmodels.api as sm $ logm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
'acct_type' in query_df.columns
df = pd.read_json('realDonaldTrump.jsonl', lines=True)
tar_counts[:12].plot(kind='bar')
plt.hist(null_vals); $ plt.axvline(x=diff, color='r')
df['product_type'].value_counts(normalize=True)
writer.save()
rhum_us = rhum_nc.variables['rhum'][1, lat_li:lat_ui, lon_li:lon_ui] $ np.shape(rhum_us)
X.dropna().shape
df_new.country.nunique()
! gcloud ml-engine predict --model $MODEL_NAME --version $VERSION_NAME --json-instances less_than_50K.json
df.groupby('episode_id')['character_id'].nunique().hist()
from IPython.display import SVG $ from keras.utils.vis_utils import model_to_dot $ SVG(model_to_dot(model).create(prog='dot', format='svg'))
base_dir = "M:/Performance_Analytics/Training/" $ data_dir = base_dir + "Data/" $ irradiance_clear_file_path = data_dir + "White_Cross/WhiteCross_Irradiance_Clearsky_20180708.csv" $ irradiance_cloudy_file_path = data_dir + "White_Cross/WhiteCross_Irradiance_Cloudy_20180712.csv"
s = issues_df.to_json(orient='records')
1/np.exp(-0.0149), np.exp(0.0506), np.exp(0.0408)
df2.query("landing_page=='new_page'").shape[0]/df2.shape[0]
c.values
pd.Series([1, 2, 3, 'a', 'b', 'c']).dtypes
idx = all_sites_with_unique_id_nums_and_names[ (all_sites_with_unique_id_nums_and_names['id_num']==10029)].index.tolist() $
lm = sm.OLS(df_new["converted"],df_new[["intercept","control","US","UK"]]) $ results = lm.fit() $ results.summary()
train_session_v2.head()
my_data = np.array([['','Column1','Column2','Column2'], $                 ['Row1',1,2,3], $                 ['Row2',4,5,6]]) $ my_data
width, height = 18, 6 $ plt.figure(figsize=(width, height)) $ jobs.loc[(jobs.FAIRSHARE == 132) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'h_vuiis')][['Memory','Wait']].groupby('Memory').Wait.median().plot(kind = 'bar', logy = True) $
dictionary = corpora.Dictionary(texts) $ dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))  # store the dictionary, for future reference $ print(dictionary)
d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])} $ df = pd.DataFrame(d) # create a DataFrame from dictionary d $ df $
print('Total minutes over time series: {}'.format(df_btc.shape[0])) $ print('% minutes in time series with {} trades: {}'.format('USD-BTC', df_btc['USD-BTC_low'].notna().sum()/df_btc.shape[0] )) $
df2.groupby('group')['converted'].describe()
df_tweets[df_tweets.oembed.isnull()]
df_tweet_clean.to_csv('./WeRateDogs_data/twitter_archive_clean.csv') $ df_image_clean.to_csv('./WeRateDogs_data/image_predictions_clean.csv') $ df_clean3.to_csv('./WeRateDogs_data/tweet_json_clean.csv')
obs = np.array([list(clean_users[clean_users['active']==1]['creation_source'].value_counts()),list(clean_users[clean_users['active']==0]['creation_source'].value_counts())]) $ obs
input_data.describe(include='all')
tokendata = pd.merge(empInfo[["ID"]],tokendata,how="left",on="ID")
'-'.join(np.array('asdf-tre-asf-paoa-pamf'.split('-'))[[0,1,2,4]])
sns.barplot(x='Average_Num_Comments', y='Subreddit', orient='h', data=subred_num_avg[:10], color='g')
waihee_tobs = session.query(Measurement.tobs).\ $ filter(Measurement.station == "USC00519281", Measurement.station == Station.station, Measurement.date >="2017-07-29", Measurement.date <="2018-07-29").\ $ all()
autos.describe(include='all')
activeByAge = activeWikis.groupby(by=['age']).url.count() $ inactiveByAge = inactiveWikis.groupby(by=['age']).url.count()
batting_df2 = batting_df.set_index(['playerid', 'yearid']) $ salary_df2 = salary_df2.set_index(['playerid', 'yearid'])
retention.reset_index(inplace=True)
sqlContext.sql(query).toPandas() $
for table in table_list: display(DataFrameSummary(table).summary())
gs.fit(x_train,y_train)
from sklearn.model_selection import StratifiedKFold, cross_val_score $ from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier $ from sklearn.linear_model import LogisticRegression $ from sklearn.pipeline import Pipeline
ghana = ghana.rename(columns={'WindDirDegrees<br />' : 'WindDirDegrees'})
all_data_merge.groupby('substore').size()
X_train, X_test, y_train, y_test = train_test_split(stock.iloc[915:-1].drop(['target'], 1), stock.iloc[915:-1]['target'], test_size=0.3, random_state=42)
df4 = df.assign(four = df.three/df.one).assign(five = lambda q: q.three * q.bar) $ df4
gm_df.tail()
transactions[transactions.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro=']
df_final.shape[0]
autos['odometer_km'].value_counts()
data = json.loads(page.text)
gas_df = gas_df.melt(id_vars=['MONTH'],var_name='YEAR',value_name='PRICE') $ gas_df.head()
path = os.path.join('Data for plots', 'Annual generation.csv') $ eia_gen_annual.to_csv(path, index=False)
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(therapist_duration, params=params, freq=31, title='Therapist Decomposition')
mask = (df['message'].str.len() > 3) #Remove cases with message length < 3. $ df = df.loc[mask]
sqladb.to_sql('SqlaDB',con=engine)
dft['stamp_round'] = dft.stamp.dt.round('1Min')
df.query("converted == 1").user_id.unique().size/df.user_id.unique().size
tw1.latitude = tw1.latitude.apply(float) $ tw1.longitude = tw1.longitude.apply(float)
s2 = pd.Series(np.random.randint(0, 7, size=10)) $ s2
p_conv_treatment = df2.query('group == "treatment"')['converted'].mean() $ p_conv_treatment
transfered_holidays=holidays_events[(holidays_events.type=='Holiday') & (holidays_events.transferred==True)] $ print("Rows and columns:",transfered_holidays.shape) $ pd.DataFrame.head(transfered_holidays)
df_melt['result_type'] = df_melt.apply(lambda x: get_correct_result_type(x), axis=1)
df = pd.read_csv('../Datasets/4 keywords.csv', index_col='Symbol')
graf_counts = pd.DataFrame(graffiti['graffiti_count'].groupby(graffiti['AFFGEOID']).sum())
correct = tf.equal(y, y_pred, name="correct") $ accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")
os.remove(os.path.join(outputs,'example.db'))
with open('data/chefkoch_12.json') as data_file:    $     chef12 = json.load(data_file) $ clean_new(chef12) $ chef12df = convert(chef12) $ chef12df.info()
n_new = treatment_df.shape[0] $ n_new
plt.style.use('seaborn-notebook') $ bb.plot(y='close')
df = df[(df.yearOfRegistration > 1900) & (df.yearOfRegistration <= 2016) ]
a = "cost is: {}" .format(cost)
target0.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/trial_730.csv',index=False)
options_data.sample(5)  # spot check TTM
results = Geocoder.reverse_geocode(41.9028805,-87.7035663)
n_user = len(df.query("group == 'treatment'")) $ users=df.shape[0] $ p_user = n_user/users $ p_user
df_ll.head(2)
from sklearn.ensemble import GradientBoostingRegressor $ from sklearn.model_selection import train_test_split $ from sklearn.utils import shuffle $ from sklearn.metrics import mean_squared_error $
regr = linear_model.LinearRegression()
df.head(10)
order = 'DESC' $ sort = 'posts' $ pages_per = '100' $ page = 1 $ members_url = 'http://prisontalk.com/forums/memberlist.php?order='+order+'&sort='+sort+'&pp='+pages_per+'&page='+str(page) $
incident_dict = {'timestamp' : incidents_time_index, 'activeincidents' : incidents_yes_no} $ incident_DF = pd.DataFrame(incident_dict) $ incident_DF.index = incident_DF.timestamp
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) #workaround $ log_mod = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ results = log_mod.fit()
sst.dimensions
prop57=props[props.prop_name=="PROPOSITION 057 - CRIMINAL SENTENCES. JUVENILE CRIMINAL PROCEEDINGS AND SENTENCING. INITIATIVE CONSTITUTIONAL AMENDMENT AND STATUTE."]
archive_clean.head(3)
gdax_trans_btc['Year'] = gdax_trans_btc['Timestamp'].dt.year $ gdax_trans_btc['Month'] = gdax_trans_btc['Timestamp'].dt.month $ gdax_trans_btc['Day'] = gdax_trans_btc['Timestamp'].dt.day $ gdax_trans_btc = pd.merge(gdax_trans_btc, usd_gbp_rate.iloc[:,1:], on=['Year','Month','Day'], how="left")
df_new.head(1)
word = 'convolution' $ word = 'machine' $ w2v.create_3d_tsne_gif('test3.gif', word, number_closest_words=25, num_positions = 10, delete_images=True, figsize = [15, 15]) $ print('Done.') $
meanDailyReturns = dict( { 'Infy': infy_df.gain_perc.mean(), 'Glaxo': glaxo_df.gain_perc.mean(), $                           'BEML': beml_df.gain_perc.mean(), 'Unitech': unitech_df.gain_perc.mean()  } )
pk.dump( weather, open( "data/weather.pkl", "wb" ) )
prison_talk = BeautifulSoup(response.content, 'lxml')
SCP_ENTRY_day1totals = (df_day1 $                   .sort_values(by=['STATION','UNIT', 'C/A', 'SCP'])[['STATION','UNIT', 'C/A', 'SCP', 'ENTRY_DIFFS']] $                   .groupby(['STATION', 'UNIT', 'C/A', 'SCP'])['ENTRY_DIFFS'] $                   .sum()) $ SCP_ENTRY_day1totals.sort_values(ascending=False).head(5)
posts_renamed = posts.rename(columns={col: 'post_' + col for col in posts.columns}) $ comments_renamed = comments.rename(columns={col: 'comment_' + col for col in comments.columns})
from sklearn.preprocessing import LabelEncoder
autos['brand'].value_counts(normalize=True)
weather_df.shape
from pandas_datareader.data import Options $ aapl = Options('aapl', 'yahoo') $ data = aapl.get_all_data() $ data.iloc[0:5, 0:5]
pmol.df[pmol.df['atom_type'] != 'H'].tail(10)
autos = autos[autos['price'].between(500, 14000)] $ autos['price'].hist()
current_time = datetime.now() $ print(current_time) $ past_year = current_time - timedelta(days=365) $ print(past_year)
sql = "SELECT * FROM paudm.forced_aperture_coadd as coadd where coadd.production_id=779 limit 100" $ df4 = pd.read_sql(sql,engine)
del df_train['visitId'] $ del df_test['visitId'] $ del df_train['totals.newVisits'] $ del df_test['totals.newVisits']
indeed1 = indeed[['company','location','skills','title','salary_clean','category','duration_int','summary_clean']] $ indeed1.shape
active_station = session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all() $ active_station
def find_name(slice): $     return re.findall('title="(.*?)"', slice)[0]
yc_new2.rename(columns={'Tip_Amt':'tipPC'}, inplace=True) $ yc_new2.head()
stock.head()
df_ad_airings_5.head(2)
sid = SentimentIntensityAnalyzer() $ def analyze_sentiment( input_tweet ): $        RT metadata etc. $
new_logistics = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ new_model = new_logistics.fit() $ new_model.summary()
total_users = df2['user_id'].nunique() $ converted_users = df2[df2['converted'] == 1].count() $ conversion_prob = converted_users/total_users $ print(conversion_prob) $
plt.rcParams['figure.figsize'] = [18.0, 10.0]
ph_pair_count = error_ph.groupby('provider')['smooth_error'].agg('count') $ valid_ph_providers = ph_pair_count.index.values[ph_pair_count > 20]
def strip_parenthesisandsemicolon(text): $     try: $         return text.replace(');',"") $     except AttributeError: $         return text 
tt_json.shape
print(df.columns)
product_df = pd.DataFrame(launch_df['source'].value_counts().sort_index()) # count values for products and sort by product names $ product_df.reset_index(inplace=True) # abstratc the product name to be a column $ product_df.columns = ['product', 'values'] # rename columns with more clear names $ product_df
active_station = session.query(Measurement.station, func.count(Measurement.station)).distinct().group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $ active_station
transactions.merge(transactions,on="UserID")
df_columns[df_columns.index.month.isin([5,6,7])]['Complaint Type'].value_counts().head() $
comm_mean = reddit['num_comments'].mean() $ comm_median = reddit['num_comments'].median() $ print 'Comments median is: ', comm_median $ print 'Comments mean is: ', round(comm_mean,2)
data.head()
top20brands = autos["brand"].value_counts().index[:20]
df2.set_index('user_id').join(ct_df.set_index('user_id')).head()
ggplot(etsamples_100hz.loc[1:35000].query("eyetracker=='el'"),aes(x="pa",y="pa_diff"))+geom_point()
y = api.GetUserTimeline(screen_name="itsamandaross", count=20, max_id=935706980643147777, include_rts=False) $ y = [_.AsDict() for _ in y]
state_party_df['National_D_neg_ratio']['2016-08-01':'2016-08-07'].sum() / 7
y_pred = model.predict(X_test) $ predictions = y_pred.tolist()
tweet_df = pd.read_json('tweet_json.txt',orient='index') $
exceldf = pd.read_excel('http://www.fsa.usda.gov/Internet/' + $                         'FSA_File/disaster_cty_list_ytd_14.xls')
from sklearn.preprocessing import StandardScaler $ ss = StandardScaler() $ Xs = ss.fit_transform(X) $ type(Xs) $ Xs
wines = np.unique(dataset_filtered['variety'].tolist())
df.index # pandas Index object
X_val = X_val.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id') $ new_manager_ixes = X_val['high_frac'].isnull() $ X_val.loc[new_manager_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values $ X_val.head()
(train.shape, test.shape)
den.tail()
df.head(2)
tweets_df = pd.DataFrame(formatted_retweets, columns=columns)
constructor.sort_values(by='price',ascending=False)
n_unique_users = df['user_id'].nunique() $ print ("Number of unique users in the dataset: {}".format(n_unique_users))
active_with_return.iloc[:,0] = active_with_return.iloc[:,0].astype("int")
scores = raw_scores.value_counts().sort_index() $ scores
train.head()
word_count = Counter() $ for sent in df_links[df_links['link.domain'] == 'hannity.com']['tweet.text'].values: $     word_count.update([w for w in sent.split() if w not in stopwords.words('English')]) $ word_count.most_common(10)
learner.save_encoder('adam3_10_enc') $
full_history = pd.DataFrame( $     modifications_per_authors_over_time.reindex(time_range).fillna(0).unstack().reset_index() $ ) $ full_history.head()
plt.scatter(X2[:, 0], X2[:, 1])
df2_copy.drop(['landing_page_old','ab_page_control'], axis=1, inplace=True)
types = ['doggo', 'floofer', 'pupper', 'puppo'] $ tweet_archive_clean = tweet_archive_clean.drop(types, axis=1)
1/np.exp(-1.7227), 1/np.exp(-1.3968), 1/np.exp(-1.4422) $
df.head()
sns.heatmap(data=stars.corr(), annot=True)
prcp_df.describe()
support.amount.sum() / oppose.amount.sum()
raw_df.shape
tweet_archive_clean.dog_stage.value_counts()
s = (p_diffs > actual_diff) $ null_prop = np.where(s == True)[0].size / p_diffs.size $ print('Proportion of p_diffs greater than the actual difference: {}'.format(null_prop))
autos['registration_year'].max()
data = fat.add_bollinger_bands(data, 'Close') $ data.tail()
result = pd.merge(result, data_client, how='left', left_on='new_month', right_on='month') $ del result['month'] $ result = result.rename(columns={'value':'Total ACtive Paid Client'})
temp_cat_more = temp_cat.add_categories(['susah','senang']) $ temp_cat_more
fixtures.insert(1, 'first_position', fixtures['Home Team'].map(ranking.set_index('Team')['Position'])) $ fixtures.insert(2, 'second_position', fixtures['Away Team'].map(ranking.set_index('Team')['Position'])) $ fixtures = fixtures.iloc[:48, :] $ fixtures.tail()
print("We have {0} tweets.".format(len(tweets))) $ print("We have {0} Bible passages.".format(len(bible_data)))
mismatch['match'] = np.where((df['group'] == 'treatment') & (df['landing_page'] == 'new_page'), 'match','mismatch')
DataSet.loc[(DataSet['userLocation']=="Melbourne")&(DataSet['userTimezone']=="Melbourne"),:]
df_students.columns.tolist()
threeoneone_census = gpd.sjoin(threeoneone_geo,census_withdata, how="inner", op='within')
PYR['soup'] = PYR.apply(create_soup, axis = 1)
len(df.groupby(['week','year','date'],as_index=False).max())
df = pd.DataFrame(np.random.randn(8, 4), $ index = ['a','b','c','d','e','f','g','h'], columns = ['A', 'B', 'C', 'D']) $ df
import numpy as np $ slicer.apply(np.mean)
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit_mod.fit() $
crime_data["OFNS_DESC"].sort_values().unique()
typesub2017 = typesub2017.rename(index=str, columns={"Solar  - Actual Aggregated [MW]": "Solar", "Wind Offshore  - Actual Aggregated [MW]": "Wind Offshore", "Wind Onshore  - Actual Aggregated [MW]" : "Wind Onshore" }) $ typesub2017.head()
train.teacher_prefix[train.teacher_prefix.isnull()] = 'Teacher' $ try: $     test.teacher_prefix[test.teacher_prefix.isnull()] = 'Teacher' $ except: $     pass
df_old = df2[df2['landing_page'] == 'old_page'] $ n_old = df_old.user_id.count() $ n_old
test_float['transporte'] = 0 $ test_float.loc[test_float.description.str.contains('transporte|transporte publico', na=False), 'transporte'] = 1 $ test_float.transporte.value_counts()
print('Mean:',np.mean(timedifference)) $ print('Median:',np.median(timedifference)) $ print('Max:',np.max(timedifference))
pumashp.crs
appt_mat[:5,:].toarray()
df2.drop(df2.index[1947], inplace=True)
hometeams = newdf.home_team.unique() $ awayteams = newdf.away_team.unique() $ allteams = set(hometeams) | set(awayteams) $ print (len(allteams))
raw_data = pd.read_csv('train.csv')#, encoding='1252',low_memory=False, parse_dates=True) $ col_names = ['deadline', 'state_changed_at', 'created_at', 'launched_at'] $ for i in col_names: $     raw_data[i] = pd.to_datetime(raw_data[i], unit='s') $ raw_data.head(3).T
raw_test.keys()
tweets_df = tweets_df[tweets_df['location'] != ''].reset_index(drop=True) # reset index from 0 $ tweets_df = tweets_df.sort_values('timestamp') $ print('got locations for {} retweets'.format(len(tweets_df)))
print 'Examine the contents of the data structure' $ for p in mp2013: $     print "%s %s %s %s" % (p, p.freq, p.start_time, p.end_time)
df2.loc['2016-09-18', ['GrossIn', 'NetIn']] $
min_df = 0.0025 # minimum frequency of words needed to be a part of the model $ max_df = 1.0 # max frequency of words to take into account as part of the model $ num_k = 10 # number of clusters $ data=train $ model_iteration(min_df, max_df, num_k, train)
n_new = df2[df2['group'] == 'treatment']['group'].count() $ n_old = df2[df2['group'] == 'control']['group'].count() $ print('n_new:', n_new, 'n_old:', n_old)
!curl -X POST -F image=@$(pwd)/images/hotdog.jpg 'http://localhost:8080/predict'
pd.read_sql(sql_sub, engine).head()
df_clean.name.value_counts()
import pandas as pd $ url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json' $ r = requests.get(url) $ json_data = r.json() $
df_new[df_new['country'].unique()] = pd.get_dummies(df_new['country'])
sdMonthlyReturns = dict( { 'Infy': monthly_gain_summary.Infy.std(), 'Glaxo': monthly_gain_summary.Glaxo.std(), $                           'BEML': monthly_gain_summary.BEML.std(),'Unitech': monthly_gain_summary.Unitech.std()  } ) $ sdMonthlyReturns
df.text.str.extractall(r'(MAKE AMERICA GREAT AGAIN)|(MAGA)').index.size
descr = df.select('CRIME_CATEGORY_DESCRIPTION').toPandas() $ descrGrp = descr.groupby('CRIME_CATEGORY_DESCRIPTION').size().rename('counts') $ descrPlot = descrGrp.plot(kind='bar')
len(train_data[train_data.notRepairedDamage == 'ja'])
titanic_clean.shape # We removed the dupliated columns 
test_collection.count()
repl = r"us['cityOrState'] = us['cityOrState'].str.replace(" $ repl
if not database_exists(engine.url): $     create_database(engine.url) $ print(database_exists(engine.url))
import time $ time.clock()
df = pd.read_csv('twitter-archive-enhanced.csv')
data_donald_replies['classification'] = classified['classification'] $ data_donald_replies.head()
money_filter = (1 <= y_test) & (y_test < 10.)
contract.info()
additional_limit_outstanding_user.head()
cars.query('mpg > 15')             # Simple query $
df_dl = df[((df['group'] != 'treatment') & (df['landing_page'] != 'old_page')) | ((df.group == 'treatment') & (df.landing_page == 'old_page'))] $ df_dl.shape
def conv(x): $     return time.mktime(datetime.datetime.strptime(str(x), "%Y-%m-%d %H:%M:%S").timetuple())
for (name, sex) in b_list: $     get_all_tweets(name, sex) $     df = df.append(pd.read_csv('%s_tweets.csv' % name))
def print_body(response, max_array_components=None, depth=0, skip_audit_info=False): $     print_json(response.json(), max_array_components, depth, skip_audit_info)
df[df['lead_mgr'].str.contains('Stanl')]['lead_mgr'].unique()
df.idxmax()
model_dt = DT_clf.fit(X_train, y_train) $ model_rf = RF_clf.fit(X_train, y_train)
predict_final = reg_final.predict(X) $ print mean_squared_error(y, predict_final) $ print r2_score(y, predict_final)
autos['registration_year'].value_counts(normalize=True).sort_index()
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount" , ascending=False)
df_vow.set_index('Date', inplace=True) $ df_vow.index = pd.to_datetime(df_vow.index)
coins_thisplot = ['Monero'] + coins_top10 # incl monero so BTC is orange in fig $ w[coins_thisplot].plot(kind='area', legend=True) $ plt.title('Fund weights for different coins \n mostly BTC before 2017') $ plt.ylabel('Weight') $ plt.show()
all_years_by_DRG =Grouping_Year_DRG_discharges_payments.groupby(level=['year','drg3']).sum() $ all_years_by_DRG.head() 
df[['product_type','price_doc']].groupby('product_type').median()
mlp_df.index
us[us['country'].isna()]
print(X_resampled.shape, y_resampled.shape, X_val.shape, y_val.shape, test_features.shape, test_target.shape)
from sklearn.linear_model import LogisticRegression $ from sklearn.metrics import confusion_matrix $ LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train) $ LR
rolling_mean = data['Close'].rolling(signal_lookback).mean() $ rolling_std = data['Close'].rolling(signal_lookback).std() $ data['Rolling Mean'] = rolling_mean $ data['Bollinger High'] = rolling_mean + (rolling_std * no_of_std) $ data['Bollinger Low'] = rolling_mean - (rolling_std * no_of_std)
def to_pickle(filename,objname): $     with open(filename, 'wb') as f: $         pickle.dump(objname, f)
def get_question_number(question): $     l = question.strip(']').split('[') $     n = len(l) $     return l[-1]
logit = sm.Logit(ab_data['converted'], ab_data[['intercept','treatment']]) $ logit_results = logit.fit()
import pandas as pd $ file_name = "data/survey_Brussels_Electricity_Water_projected_dynamic_resampled_bias_1000_{}.csv" $ sample_survey = pd.read_csv(file_name.format(2016), index_col=0) $ sample_survey.head()
wic = pd.read_excel("D:\\kagglelearn\\kaggledatasets\\WICAgencies2013ytd.xls", sheet_name='Total Women') $ wic.head()
pn_and_qty = pd.read_csv(r'data/PN_and_QTY.csv')
ts.shift(3, freq='D')
measure_df.head() $
maint['comp'].value_counts().plot(kind='bar') $ plt.ylabel('Number of replacements') $ plt.show()
df = df[df.year >2005]
pprint(Counter( [d['PutRequest']['Item']['pmcid']['S']  for d in output_dict['demographics']]).most_common(40)) $ pprint(Counter( [d['PutRequest']['Item']['pmid']['S']  for d in output_dict['demographics']]).most_common(40)) $ pprint(Counter( [d['PutRequest']['Item']['date_processed']['S']  for d in output_dict['demographics']]).most_common(40)) $ pprint(Counter( [d['PutRequest']['Item']['title']['S']  for d in output_dict['demographics']]).most_common(40))
graphs_filename = graphs_title + ".html" $ output_file(graphs_filename, title=graphs_title) $ show(tabs)
xml_in_merged.tail(2)
df2.head()
newpage = df.query('landing_page == "new_page" and group != "treatment"').count()[1] $ treat = df.query('landing_page != "new_page" and group == "treatment"').count()[1] $ newpage + treat
cohorts['TotalUsers'].unstack(0).head(10)
SCOPES = ['https://www.googleapis.com/auth/analytics.readonly'] $ KEY_FILE_LOCATION = './Gerdau-83372fd33b23.json' $ ACCOUNT_ID = '35160809' $ WEB_PROPERTY_ID = 'UA-35160809-5' $ VIEW_ID = '112966131'
st.version
StockData.head()
df_dates_new.to_csv('medium_urls_dates_unique_NEW.csv')
appleNeg = companyNeg2[companyNeg2['author_id'].str.contains('AppleSupport') | $                        companyNeg2['text'].str.contains('AppleSupport')]
Maindf.head(5)
df.to_csv("airquality-data.csv")
evaluator.get_metrics('precision')
dau = activeDF.select("cid").distinct().count()*100 $ print "DAU for {}: {:,}".format(D0.isoformat(), dau) # ~110M users
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Categorical(temp, categories=['low','medium','high'], ordered=True) $ temp_cat
(df[df["converted"] == 1]['user_id'].count())/df.shape[0]
df.info()
shelter_cleaned_df.loc[shelter_cleaned_df['AnimalType'] == 'Cat']
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $
b_cal.shape
print(len(pca.components_))
grid_id = np.arange(1, 1535,1) $ grid_id_array = np.reshape(grid_id, (26,59))
rf_v2.hit_ratio_table(valid=True)
week31 = week30.rename(columns={217:'217'}) $ stocks = stocks.rename(columns={'Week 30':'Week 31','210':'217'}) $ week31 = pd.merge(stocks,week31,on=['217','Tickers']) $ week31.drop_duplicates(subset='Link',inplace=True)
propnames = props.prop_name
new_dems.drop(new_dems.index[0], inplace = True) $ new_dems.reset_index() $ new_dems.head()
submission_full[['proba']].mean()
df2.any(axis=1)
df.country_destination.value_counts()
%%bash $ cd /data/LNG/Hirotaka/ASYN $ cut -f1 sign.txt > sign.list $ grep -f sign.list PPMI0_all
stack_X_test = preds_test.drop('<=30Days',axis=1) $ stack_y_test = preds_test['<=30Days']
df.head(500).to_csv('comma_delim_trunc.csv', sep=',')
doglist['weight'].fillna(50, inplace = True) $ doglist
pd.to_datetime('4.7.12', dayfirst=True)
df_twitter_copy[['jpg_url', 'p1']] = df_twitter_copy[['jpg_url', 'p1']].fillna('None') $ df_twitter_copy['p1_conf'] = df_twitter_copy['p1_conf'].fillna(0)
pt = pd.pivot_table(airbnb_df, index='date_listed', values='id', aggfunc=[len]) $ pt.rename(columns={'len':'num_listings'}, inplace=True) $ pt.head(10)
%timeit MyList4 = MyList.copy(); MyList4.sort() $ MyList4 = MyList.copy(); MyList4.sort() $ MyList4[:5]
from sklearn.metrics import accuracy_score $ y_pred = rnd_clf.predict(X_test) $ accuracy_score(y_test, y_pred)
model = gensim.models.Word2Vec(sentences, min_count=1)
bnbAx['country_code'] = bnbAx.country_destination.map({'other':0,'US':1,'FR':2,'IT':3,'GB':4,'ES':5,'CA':6,'DE':7,'NL':8,'AU':9,'PT':10})
tweet1.favorite_count
plt.figure(figsize=(8,8)); $ plt.plot(tt_final['favorite_count'], tt_final['retweet_count'], 'ro', alpha = 0.08) $ plt.xlabel('Favorite Count (LIKES)') $ plt.ylabel('Retweet Count') $ plt.title('Retweets v. Likes'); $
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True) $ toma.iloc[::1].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.']) $ ax.set_ylabel('Relative error') $
x,y=m.predict_with_targs()
datatest = pd.read_csv('csvs/properati_dataset_testing_noprice.csv', low_memory=False)
if tres.code == 200: $     tw = pd.read_csv(tres) # A HTTPResponse is an IO object $     print(tw.shape)
graph_stats_df.head(3)
q = [ [scores.iloc[score].idxmax(),scores.iloc[score].max()] for score in range(len(scores)) ]
autos['price'].value_counts().sort_index().head(20)
plt.title('McQueen ngram', fontsize=18) $ mcqueen_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/mcqueen_ngram.jpg')
archive_copy['source'].sample(10)
submit_button = driver.find_element_by_name('submit') $ submit_button.click()
order_date = 'Order Date' $ df = pd.merge(Users, Orders, left_on = ['id'], right_on = ['id_user']) $ df['Order Date'] = pd.to_datetime(df['Order Date']).dt.date $ df['Reg_date'] = pd.to_datetime(df['Reg_date']).dt.date
appleinbounds.to_csv('../data/appleNeg2.csv')
pd.DataFrame ([[101,'Alice',40000,2017], $                [102,'Bob',  24000, 2017], $                [103,'Charles',31000,2017]], index   = ['r1','r2','r3'] )
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
exceldmh = dmh.Excel('http://www.fsa.usda.gov/Internet/' +  $                      'FSA_File/disaster_cty_list_ytd_14.xls')
fires = pd.read_sql_query("SELECT * from fires", conn) # I get a no such table error here
_ = ok.grade('q05f') $ _ = ok.backup()
df_members.isnull().sum()  
df.Close.plot();
import os $ params_file_path = os.path.join(resources_dir, "params.tsv") $ text_classifier.export_params(params_file_path) $ print("Saving the configuration parameters to {}".format(params_file_path))
autodf.info()
print('Slope: Efthymiou vs experiment: {:0.2f}'.format(popt_axial_chord_saddle[2][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_saddle[2]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
result.to_csv("gaurav4.csv", index=False)
aux = aux.merge(countries_df, how='inner', on='user_id') $ aux.country.value_counts()
n1 = len(df.query('landing_page=="new_page"').query('group=="control"')) $ n1
van = pd.merge(comvandal,users,how='left',on='username') $ van.head()
all_df = train_df.append(test_df) $
df2['intercept'] = 1 $ df2[['ba_page','ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('ba_page', axis=1) $ df2.head()
obj.index
df[df['Complaint Type'] == 'Street Condition']['Descriptor'].value_counts().head()
query = session.query(Measurement.date,Measurement.prcp) $ rain = query.order_by(Measurement.date).filter(and_(Measurement.date >= year_ago_str, $                     Measurement.date < fake_today)) $ df = pd.read_sql(rain.statement,session.bind) $ df.head(10)
filename = 'Best_model.sav' $ joblib.dump(reg_logit, filename) $
url = "https://www.fdic.gov/bank/individual/failed/banklist.html" $ banks = pd.read_html(url) $ banks[0][0:5].ix[:,0:4]
rdf.to_csv('training_data/road_features.csv') $ df.to_csv('training_data/collisions_new.csv')
yt.get_featured_channels(channel_id)
import datetime $ datetime.datetime(year = 2010, month = 3, day=3)
ip.isnull().sum()
autos['name'].str.rsplit('_', expand = True, n = 2).head()
n_new = df2[df2['group']=='treatment']['user_id'].nunique() $ n_new
df_cal = pd.read_csv('calendar.csv')
clf.predict(digits.data[-1:]) # Hoping to get 8 here...
aapl = getStockPrice("AAPL",1982, 1, 1, 2018, 1, 23) $ aapl.head() $
plt.hist((score_pair["first"].values - score_pair["last"].values))
train_data, test_data, train_labels, test_labels = train_test_split(spmat, labels, test_size=0.10, random_state=42)  
uvws = [[ 1, 1, 0], $         [-1, 1, 0], $         [ 0, 0, 1]] $ system = system.rotate(uvws) $ print(system)
with gzip.GzipFile('data/cleaned_df.pkl.gz', 'wb') as file:  $     joblib.dump(df, file)
param_grid = {} $ param_grid['penalty'] = ['l1', 'l2'] $ param_grid['C'] = [0.1, 1.0, 10]
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'game_winners'
%timeit df_protest.rangecode.map(lambda x: our_function(x))
f = open("json_example.json","r") $ print(json.load(f)) $ f.close()
from scipy.stats import chi2_contingency
password = getpass.getpass() $ conn = pymysql.connect(host='mysql.guaminsects.net',user='aubreymoore',passwd=password,db='oryctes') $ df_new.to_sql(name='YigoBarrelObs',con=conn,flavor='mysql',if_exists='append',index=False) $ conn.close()
users['key'] = 0 $ products['key'] = 0 $ df_user_product = users.merge(products, how='outer') $ cols = ['UserID','ProductID'] $ df_user_product[cols]
df = pd.DataFrame(airline_tw_collec) $ df.head(2)
lr.fit(X_train, y_train.iloc[:,2])
cats_merge.head(5)
df_country.nunique()
for name in invalid_names: $     twitter_archive_clean.loc[twitter_archive_clean.name == name ] = np.NaN
model.most_similar('queen')
df_clean.info()
with open(os.path.join('/Users/LucasRamadan/Play-Bigger-Research/descriptions/', 'facebook.txt'), 'w', encoding='utf-8') as f: $     f.write(fb_desc)
df2[df2['landing_page'] == 'new_page']['user_id'].count() / df2['user_id'].count()
AFX_X_2017_dict = AFX_X_2017_r.json()
yp = clf.predict(X) $ print yp $ training_metrics = get_metrics(np.array(y), yp, yp)
df.head(5)
df.tail(2)
rob.plot()
miss_vals = df.isnull().any(axis=1).sum() $ print('The number of rows with missing values is {}'.format(miss_vals))
auto_new.CarYear.unique()
plate_appearances['pitcher_throws_left'] = np.where(plate_appearances['p_throws'] == 'L', 1, 0) $ plate_appearances['left_handed_batter'] = np.where(plate_appearances['stand'] == 'L', 1, 0)
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
print('Autocorrelation 1: ', BPL_electric['Total_Demand_KW'].autocorr(1)) $ print('Autocorrelation 7: ', BPL_electric['Total_Demand_KW'].autocorr(24)) $ print('Autocorrelation 168: ', BPL_electric['Total_Demand_KW'].autocorr(168))
ip.info()
def get_date(dates): $     new_dates = [] $     for date in dates: $         new_dates.append(dt.datetime.strptime(date, '%d-%b-%y')) $     return np.array(new_dates)
students.sort_values(by='weight', ascending = False, inplace = True) $ students
new_page_converted = np.random.choice([0, 1], size=n_new, p=[converted_rate, 1 - converted_rate])
treino[treino.sentiment == 0].count() #Tweets com sentimento negativo
hot_df.nunique()
def train_cats(df): $     for n,c in df.items(): $         if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()
joined=join_df(joined,trend_de,["Year","Week"],suffix='_DE') $ joined_test=join_df(joined_test,trend_de,["Year","Week"],suffix='_DE') $ sum(joined['trend_DE'].isnull()),sum(joined_test['trend_DE'].isnull())
results1 = pd.DataFrame(results, columns=['itapudid', 'max1stdetectwssc', 'max1stdetectwssd', 'max1stdetectwsse', 'max1stdetectwssf', 'eventtime', 'correct'])
dashes = {'PA':[6,2], 'PAS':[6,2,1,2], 'PS':[3,2], 'PSA':[3,2,1,2], 'PCS':[1,1]} $ names = ('PA', 'PAS', 'PCS') $ traces = {'TMSS':0.999914200051, 'PS':0.999677113293, 'PSA':0.998900522177, $           'PCS':0.994016471141, 'PA':0.999263243254, 'PAS': 0.99778322983}
path_to_token <- normalizePath("data_sci_8001_token.rds") $ envvar <- paste0("TWITTER_PAT=", path_to_token) $ cat(envvar, file = "~/.Renviron", fill = TRUE, append = TRUE)
sm = SMOTE(random_state=12, ratio = 'auto') $ x_res, y_res = sm.fit_sample(X_tfidf, y_tfidf)
df.query('group == "control" and landing_page == "new_page"').count()
transit_df_rsmpld = transit_df_byday.reset_index().groupby('FUZZY_STATION').apply(lambda x: x.set_index('DATETIME').resample('1M').sum()).swaplevel(1,0) $ transit_df_rsmpld.info() $ transit_df_rsmpld.head()
tipsDF = pd.read_csv("yelp_tips.csv", encoding = 'latin-1') $ tipsDF.head()
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.choice([0,1], size=n_new, p=[p_new, (1-p_new)]) $     old_page_converted = np.random.choice([0,1], size=n_old, p=[p_old, (1-p_old)]) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())    
tweets["retweet"].sum()/len(tweets.retweet)
df.shape
tlen_e.plot(figsize=(16,4), color='r');
big_df_count.reset_index(inplace=True) $ big_df_avg.reset_index(inplace=True)
twitter_ar.expanded_urls[100]
df2['intercept']=1 $ df2[['control','treatment']]=pd.get_dummies(df2['group'])
from sklearn.feature_extraction.text import CountVectorizer
full['LOS'].unique().shape[0]
df_link_meta['link.domain_resolved'] = df_link_meta['link.domain_resolved'].astype(str)
X_train, X_test, y_train, y_test = train_test_split(features,classification_open,test_size=0.2)
pd.crosstab(calls_df['call_day'], calls_df['call_type'], margins=True)
df2.drop(df2.index[2893], inplace = True)
baseball_newind.index.is_unique
rounds['announced_year'] = rounds['announced_on'].dt.year
from DeepText.preprocess import preProcessor $ pp = preProcessor() $ clean_func = pp.default_cleaner $ tokenizer = pp.default_tokenizer
print(heights_A.shape,end='\n') $ print(heights_A)
tweet_archive_clean.info() $ print() $ image_predictions_clean.info() $ print() $ tweet_json_clean.info()
percent_unique_convert1 = df[df['converted']==1]['user_id'].nunique()/df['user_id'].nunique()*100 $ percent_unique_convert1
YS1517 = pd.read_csv('yahoostock1517.csv', parse_dates = True, engine ='c', $                      index_col = 0, sep = ',') $ YS1517
class_data['Census Tract'].value_counts()
autos["price"] = autos["price"].str.replace("$", "") $ autos["price"] = autos["price"].str.replace(",", "") $ autos["price"] = autos["price"].astype("int")
train_data.loc[(train_data.gearbox == 'automatik') & (train_data.vehicleType.isnull()), 'vehicleType'] = 'limousine'
from google.colab import auth $ auth.authenticate_user()
data.dtypes
df_comment.sort_values(by='likes_count',ascending=False) $ df_comment = df_comment[df_comment.likes_count>10]
subs = subs.sort_values(by='time_out')
plt.figure(figsize = (20,20))        # Size of the figure $ sns.heatmap(telecom3.corr())
countries = [] $ for i in places2.index: $     countries.append(places2[i]['country'])
X_clf = training_data.values
temperature_2016_df = pd.DataFrame(Temperature_year)  # .set_index('date') $ temperature_2016_df.head()
results = soup.find("article")["style"]
url_CHI = "https://bears.seasonticketrights.com/Images/Teams/ChicagoBears/SalesData/Chicago-Bears-Sales-Data.xls"
df2.loc[df2.index == 2893]
from pandas_datareader.nasdaq_trader import get_nasdaq_symbols $ symbols = get_nasdaq_symbols() $ print(symbols.ix['IBM'])
header    = root.find('af:header',namespaces) # zoekt in de xml naar de tag header $ company   = root.find('af:company', namespaces) # zoekt in de xml naar de tag company $ transactions = root.find('af:company/af:transactions', namespaces) # zoekt in de xml naar de tag company/transactions (rekening houdend met de prefix van de namespaces)
df_bthlst = pd.read_csv('../../data/essentials/md_paths_v3.csv', index_col=[0]) $ df_bthlst = df_bthlst[~df_bthlst["client_display_name"].isnull()]
pandas.Series([1,2,3], index=['foo', 'bar', 'baz'])
embarked = df_titanic['embarked'] $ print(embarked.describe()) $ df_titanic['embarked'] = df_titanic.embarked.astype('category')
df2.duplicated('user_id').sum()
test $ test = get_regression(test) $ test = get_weights(test) $ test.max() - test.min() $ test.max()
lsi.save('/tmp/model.lsi') # same for tfidf, lda, ... $ lsi = models.LsiModel.load('/tmp/model.lsi')
p_diffs = np.array(p_diffs) $ null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist(null_vals) $ plt.axvline(x=p_diff_original,color ='red')
model.create_timeseries(scenario) $ model.close_db()
lr_best.coef_
df.printSchema()
ab_df.isnull().sum() $
frame = pd.DataFrame({'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1], 'c': [-2, 5, 8, -2.5]})
tipsDFslice = tipsDF[:50000] $ tipsDFslice.tail()
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("Probability of conversion for old page (p_old) is", p_old)
oil_interpolation['date']=oil_interpolation.index $ pd.DataFrame.head(oil_interpolation) $ oil_interpolation.plot('date','dcoilwtico',kind='bar', color='r', figsize=(15,5))
master_df.name.nunique()
for key,value in trends_per_year_avg_ord.items(): $     print( "Trend avg in " + str(key) + " = " + str(value))
kick_data_state.columns
print(DataSet_sorted['tweetText'].iloc[0])
df_clean[df_clean['tweet_id'] == int(df_tweet_clean['id'].sample().values)]
df_twitter_archive.sample(25)
df.loc['2017-01-12':'2017-02-05', ['Open', 'Close']].plot(figsize=(15,5));
logit = sm.Logit(df_new['converted'],df_new[['intercept','treatment', 'CA', 'UK']]) $ results = logit.fit() $ results.summary()
dicttagger_food = DictionaryTagger(['food.yml'])
df2=df2.drop(df2.index[2893])
data2.head()
aru_df.shape
df2[df2['group'] == 'treatment']['converted'].mean()
run txt2pdf.py -o '2018-06-22 2014 FLORIDA HOSPITAL Sorted by discharges.pdf'  '2018-06-22 2014 FLORIDA HOSPITAL Sorted by discharges.txt'
joined.reset_index(inplace=True)
print("Annualized mean:", fundmean*100, "%", $       "Annualized vol:", fundvol*100, "%", $      "(Based on monthly data.)")
faa_data_phil_pandas = faa_data_pandas[faa_data_pandas['AIRPORT_ID'] == "KPHL"] $ print(faa_data_phil_pandas.shape) $ faa_data_phil_pandas.head()
from sklearn.feature_extraction.text import CountVectorizer $ countvec = CountVectorizer() $ dtm_df = pandas.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index) $ dtm_df
merged.shape
tweets_df['Countries'] = tweets_df['Text'].apply(lambda x: ner.get_countries_from_content(x).keys())
agg_team=teams.groupby('Team').agg({'win_numb':np.sum,'Pts':np.sum,'TOV':np.sum, $                                     'FTM':np.sum,'FTA':np.sum, $                                     'REB':np.sum,'AST':np.sum,'PF':np.sum,'FTPct':np.average $                                    }) $ sns.regplot(x='Pts',y='win_numb',data=agg_team);
%%time $ df_from_csv = pd.read_csv('losses.csv', index_col=['epoch', 'batch', 'datapoint'], float_precision='high')
twitter_final.groupby('time_cat')['length'].mean().reset_index(name="mean").sort_values(by='mean', ascending=False)
w.get_step_object(step = 3, subset = subset_uuid).indicator_objects['din_winter'].get_water_body_indicator_df(water_body = wb)
plt.plot(ds_cnsm['time'],ds_cnsm['met_salsurf_qc_executed'],'b.') $ plt.title('CP01CNSM, OOI QC Executed SSS') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
tweet_archive_enhanced_clean = tweet_archive_enhanced_clean.drop(['in_reply_to_status_id','in_reply_to_user_id', $                                                                   'retweeted_status_id','retweeted_status_user_id', $                                                                   'retweeted_status_timestamp'],axis =1)
users.email = users.email.apply(email_clean)
Boxscores = get_boxscores() $ Boxscores.head()
dframe_team.drop(dframe_team.columns[[0,4]],inplace=True,axis=1) $ column_names = ['Executive', 'Start', 'End'] $ dframe_team.columns = column_names $ dframe_team.insert(0, 'Team', 'ATL') $ dframe_team.head()
print('alpha=0.1:\n%s\n\nalpha=0.25\n%s' % (dd, dd2))
stockdf['Stock'].unique()
archive_df_clean=archive_df[archive_df.retweeted_status_id.isnull() == True] $
reddit = pd.read_csv(filename) $ reddit.drop('Unnamed: 0', axis = 1, inplace = True) $ reddit.head()
autos['date_crawled'] = autos['dateCrawled'].str[:10]
plt.figure(figsize=(12, 6)) $ plt.plot(x, x ** 2) $ plt.plot(x, -1 * (x ** 2)) $ plt.title('My Nice Plot')
grid.best_score_
options_data.sample(20)
coefficients = pd.DataFrame(zip(X.columns, np.transpose(model.coef_))) $ coefficients
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
df2['intercept'] =1 $ df2[['page','ab_page']] = pd.get_dummies(df2['group'])
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep ='\s+', header=None) $ df.head(5)
goog.set_index('Date', inplace=True)
df_birth['Country '].value_counts(dropna=False).head()
print(f'The value of variable b plus variable a is {b + a}.')
sns.regplot(x=dfz.rating_numeric, y=dfz.retweet_count)
mdl1 = XGBRegressor(learning_rate=0.1, n_estimators=1000, booster='gblinear' )
df[df.pre_clean_len > 140].head(10)
import copy $ ndvi_gtpt6 = copy.copy(ndvi) $ ndvi_gtpt6[ndvi<0.6] = np.nan  $ print('Mean NDVI > 0.6:',round(np.nanmean(ndvi_gtpt6),2))
pd.read_json("json_data_format_columns.json", orient="columns")
documents = [] $ for i in messages_clean: $     documents.append(str(i).replace(",", "").replace("u'","").replace("'", "")) $ documents[:5]
df_ad_state_metro_1['sponsor_types'].value_counts()
fav = twitter_final.filter(['favorite_count','dog_species']).sort_values(by='favorite_count',ascending=False).head(10) $ fav.plot.bar('dog_species')
conv_prob_treatment = df2[df2.group == 'treatment'].converted.mean() $ print('Given that an individual was in the treatment group, the probability they converted is {}.'.format(conv_prob_treatment))
print airbnb_df[airbnb_df['room_type']>='Private room'].shape $ airbnb_df[airbnb_df['room_type']>='Private room']['room_type'].value_counts()
autos = autos[autos['price'].between(0, 1e5)]    # Filter out the outliers
crimes_by_yr_month.head()
autos.info()
d={'c1':('A','B','C','D'),'c2':np.random.randint(0,4,4)} $ pd.DataFrame(d)
cust_demo[cust_demo['ID'].duplicated()]
def latex_repl(latex_string): $     return " " + latex_string.group(0) + " "
users_pd = users.toPandas() $ pd.value_counts(users_pd['active'].values, sort=True).plot(kind="bar")
with open('tfidf_vect_5_29.pkl', 'wb') as piccle3: $     pickle.dump(t_vect, piccle3)
df2.drop(df2.index[2862], inplace = True);
resultsList = responsesJsonList.apply(getResults)
df_new.query('group == "treatment" and country =="UK"')['converted'].mean() $
timelog['seconds'] = timelog.apply(dur2sec, axis=1)
records2 = records.copy()
from sklearn.tree import DecisionTreeRegressor $ regressor=DecisionTreeRegressor(random_state=0) $ regressor.fit(y,z) $ z_pred=regressor.predict(6.5)
by_area['AQI Category'].value_counts().unstack()
g.head()
t = 4 $ m['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(t, m['frequency'], m['recency'], m['T']) $ m.sort_values(by='predicted_purchases').tail(15)
print(users['Registered'].dtype) $ print(users['Cancelled'].dtype) $ print(sessions['SessionDate'].dtype) $ print(transactions['TransactionDate'].dtype)
tobsDF = pd.DataFrame(popStationData[:-1], columns=["date", "tobs"]) $ tobsDateDF = tobsDF.set_index("date") $ tobsDateDF = tobsDateDF.sort_values(by=["date"], ascending=False) $ tobsDateDFclean = tobsDateDF.dropna(axis=0, how="any") $ tobsDateDFclean.head()
staff.describe()
np.count_nonzero(np.any(na_df.isnull(), axis=1)) # total number of rows with missing values 
result['timestampCorrected'] = result['timestamp'] $ result.loc[result['timestamp']> 6e+17, 'timestampCorrected' ] = np.NaN
ideas = pd.read_csv('final_ideas_df.csv') # Compiled in Notebook 1 $ stocks = pd.read_csv('final_stocks.csv')  # Compiled in Notebook 3
df2.reset_index(inplace=True)
print 'Percentage of total amount for data with valid US state, but no city, zipcode: {:.3f}'.format(100*sum(df[(df.city == '') & (df.zipcode_initial == '')].amount)/sum(df.amount))
sns.kdeplot(utility_patents_subset_df.number_of_claims, shade=True, color="purple") $ plt.show()
np.exp(0.0469), np.exp(0.0783), 1/np.exp(-0.0674), np.exp(0.0118), np.exp(0.0175)
engine.execute('SELECT * FROM station LIMIT 5').fetchall()
statistics = powerConsumptionsPerDay.groupby(['timestamp']).agg(['sum', 'mean', 'min', 'max', 'median']) $ statistics $
df_dummies.head()
df2['user_id'].duplicated().sum()
data.loc[3]
relevant_data['Invitee Name'].value_counts()
(autos['ad_created'] $         .str[:10] $         .value_counts(normalize = True, dropna = False) $         .sort_index() $         )
autos.columns
doctype_by_day.columns
index_group3 = user_group['Group'].apply(checkGroup, number=3) $ user_group3 = user_group[index_group3] $ print(len(user_group3)) $ user_group3.head()
df['ch_json'] = df.name.apply(search_org) $ df['ch_postcodes'] = df.ch_json.apply(ch_postcodes)
autos["price"].value_counts().sort_index(ascending=False).head(20) $ autos = autos[autos["price"].between(1,351000)] $ autos["price"].describe()
df1['ad']=df1['Additional Limit'].astype(np.double) $
centroids = kmeans_model.cluster_centers_ $ centroids
b = news_df[news_df['Source Acc.'] == 'nytimes'] $ b.head() $ print(b['Compound Score'].sum())
ny = ['New York', 'NY', 'NYC', 'New York City', 'Brooklyn'] $ us.loc[us['country'].isin(ny)|us['cityOrState'].isin(ny), 'cityOrState'] = 'NY' $ us.loc[us['country'].isin(ny)|us['cityOrState'].isin(ny), 'country'] = 'USA' $ us['country'].value_counts(dropna=False)
new_contributors = EQCC(git_index) $ new_contributors.get_min("author_date").by_authors("author_name") $ response = new_contributors.fetch_aggregation_results() $ buckets = response['aggregations']['0']['buckets'] $ print(buckets)
sl[(sl.age_well_years!= 274) & (sl.second_measurement==0)].age_well_years.describe()
bikes['hour_of_day'] = (bikes.start.dt.hour + (bikes.start.dt.minute/60).round(2)) $ hours = bikes.groupby('hour_of_day').agg('count') $ hours['hour'] = hours.index $ hours.start.plot() $ sns.lmplot(x='hour', y='start', data=hours, aspect=1.5, scatter_kws={'alpha':0.2})
valence_df.positive = 1.0*valence_df.positive
print (df.dtypes[df.dtypes == 'object']) $
pipeline.steps[1][1].coef_
pattern = [{'LOWER':'fees'}] # LOWER coverts words to lowercase before matching $ matcher = Matcher(nlp.vocab) $ matcher.add('fee', collect_sents, pattern) $ matcher(doc)
plt.style.use('seaborn-darkgrid') $ bb.plot(y='close')
user_slice = USER_PLANS_df.loc[np.unique([a.strip() for a in U_B_df.loc[0,'cameras']])]
result.summary(title='Summary of countries and pages')
priceChange = closePrice.pct_change(periods=5) $ priceChange.nlargest(5)
df2[df2['group'] == "treatment"].converted.mean()
exploration_titanic.structure()
@app.route("/api/v1.0/tobs") $ def tobs(): $     results = session.query(measurement.date, measurement.tobs).filter(measurement.date >= prev_year).all() $     temp_results = list(np.ravel(results)) $     return jsonify(temp_results) $
words = [w for w in words if w not in stopwords.words('english')] $ print(words)
result1 = (df['A'] + df['B'] / df['C'] - 1) $ result2 = pd.eval("(df.A + df.B) / (df.C - 1)") $ np.allclose(result1, result2)
nold=df2.query('landing_page=="old_page"')['converted'].shape $ print(nold[0])
%%bash $ grep -A 50 "build_estimator" taxifare/trainer/model.py
obs = monthly.resample('M')['Size'].sum() $ cts = monthly.resample('M')['Size'].count()
df = pd.merge(applications,questions,on='applicant_id') $ df['response_'] = np.where(df['response'].isnull(),'No Response',df['response']) $ df.head()
state_names.head()
_ = ok.grade('q06c') $ _ = ok.backup()
from sklearn.naive_bayes import GaussianNB
learn.save("dnn10")
df = pd.DataFrame(np.random.rand(4,2), $                  index=[['a','a','b','b'], [1,2,1,2]], $                  columns=['data1','data2']) $ df
SANDAG_population_df = pd.read_csv(open(SANDAG_population_filepath), dtype={'TRACT': str}) $ SANDAG_population_df.set_index(['YEAR', 'TRACT'], inplace=True) $ SANDAG_population_df.sort_index(inplace=True) $ SANDAG_population_df.head()
df.fillna(method='backfill')
ax = P.plot_1d('pptrate') $ ax.figure.savefig('/media/sf_pysumma/pptrate.png') $
hm_clean.columns
xmlData['price'] = pd.to_numeric(xmlData['price'], errors = 'raise')
training_df = features[~features.gameId.isin(production_df.gameId)]
details['Year'], details['Month'] = details['Released'].dt.year, details['Released'].dt.month
bb = data.DataReader(name='F', data_source='iex' $                         , start='2017-07-01', end='2018-05-01') $ bb.head()
print(actual_value_second_measure[actual_value_second_measure==2]) $ holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00063550') ]
ptgeom = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])] $ gdf = gpd.GeoDataFrame(df, geometry=ptgeom, crs={'init': 'epsg:4326'}) $ gdf.head(5)
df2.user_id.nunique()
f_close_clicks_os_train.show(3)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = "larger") $ z_score, p_value
print(df_raw.loc[(df_raw.year==1993) & (df_raw.data.str.match(r'\s*\d+\s*\d+\s*NEVER KEEPING SECRETS')),:]) $
test_size = 0.2 $ training_X, testing_X, training_y, testing_y = train_test_split(experiment_X,  experiment_y,test_size=test_size, random_state = state) $
tweet = tweets[1] $ pp.pprint([att for att in dir(tweet) if '__' not in att])
def get_date(created): $     return datetime.fromtimestamp(created) $ topics_data = topics_data.assign(timestamp = topics_data["created"].apply(get_date)) $ topics_data = topics_data.drop('created', axis=1) # remove the 'created' column
driver.get('https://en100.tribalwars.net/game.php?village=2502&screen=map#542;512')
merge.amount.sum()
%%time $ dfRegMet["sentences"] = dfRegMet["sentences"].str.split()
df.head()
twitter_ar.info()
liberiaCases = liberiaCases.rename(columns = {'National':'New cases'}) $ liberiaDeaths = liberiaDeaths.rename(columns = {'National':'New deaths'})
donald_breed = cats_df[cats_df['breed'] == 'Donald'] $ cats_df['remove'].iloc[donald_breed.index] = True $ del donald_breed
joined_store_stuff = counted_store_events.join(stores, counted_store_events.store_id == stores.id)
X, Y = np.meshgrid(np.linspace(-1,1,10), np.linspace(-1,1,10)) $ D = np.sqrt(X*X+Y*Y) $ sigma, mu = 1.0, 0.0 $ G = np.exp(-( (D-mu)**2 / ( 2.0 * sigma**2 ) ) ) $ print(G)
brand_counts = autos["brand"].value_counts(normalize=True) $ common_brands = brand_counts[brand_counts > .05].index $ print(common_brands)
preds_train=preds_train.as_data_frame() $ h2o_train['visitors']=preds_train
logit_mod = sm.Logit(df3['converted'], df3[['intercept', 'treatment']]) $ results = logit_mod.fit()
from sklearn.metrics import mean_squared_error
results_Jarvis, output_path = S.execute(run_suffix="Jarvis_plot", run_option = 'local')
pageviews_tags.head(2)
conn.execute("update related_content set dest='https://s3.amazonaws.com/dataskeptic.com/guests/2018/robert-sheaffer.jpg' where content_id=702")
data_scrapped.drop_duplicates().shape
systemuseData.groupby('Year')['Total'].sum().plot() $ plt.xlim(1980,2020) $ plt.ylabel('Use (ac-ft)')
model.score(X_test, y_test)
print("date_crawled:", autos["date_crawled"].str[:10].unique().shape[0]) $ print("last_seen:", autos["last_seen"].str[:10].unique().shape[0]) $ print("ad_created:", autos["ad_created"].str[:10].unique().shape[0])
engine = create_engine("sqlite:///hawaii.sqlite") $ conn = engine.connect()
extract_all.loc[extract_all.app_id_short=='5b155a8df1f17915'].shape
contractor_clean[contractor_clean.city.isnull()] # The result is empty. $ contractor_clean.loc[contractor_clean['contractor_id'] == 139] $
print(df.columns) $ print(df.head())
brand_top_10 = autos['brand'].value_counts().sort_values(ascending=False) $ brand_top_10 = brand_top_10.index[0:10] $ brand_top_10
pca=decomposition.PCA() $ stocks_pca_t2= pca.fit_transform(stocks_pca_m2)
top_supporters["contributor_fullname"] = top_supporters.contributor_firstname + " " + top_supporters.contributor_lastname
url = 'http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0026752.s001' $ labmt = pd.read_csv(url, skiprows=2, sep='\t', index_col=0)
nb_pipe.fit(X_train, y_train) $ nb_pipe.score(X_test, y_test)
df['closed_at'].head()
data.loc[7, 'year'] = np.nan $ data
display(data.describe())
json_df.tail()
party_type_crosstab = party_type_crosstab.reset_index() $ party_type_crosstab.columns = ['INDEX', 'YEAR', 'DEMOCRATIC', 'REPUBLICAN', 'THIRD'] $ party_type_crosstab = party_type_crosstab.rename_axis(None, axis=1) $ party_type_crosstab.drop("INDEX", axis=1, inplace=True)
all_tables_df.OWNER.unique()
len(global_sea_level.year) == len(northern_sea_level.year)
bb_df.describe()
X_train.dtypes
unique_urls[unique_urls.domain == domain].head()
ts_mean[ticker].shape, ts_filter.shape
killfile = [i[0] for i in [i for i in list(pgn2value.items()) if i[1] == 1]] $ np.random.shuffle(killfile) $ killfile = killfile[:20572 - 15286 ] $
df = pd.read_csv('./scraping_results.csv')
explotions.head()
df_age = pd.concat([X_age, y_age], axis=1)
p_old = df2.converted.mean() $ p_old #  displaying the convert rate for the control
data = data.json() $
stock_df.head()
group_key = list(grouper.groups.keys())[0]
print(soup.prettify())
test= test.reset_index(drop = True) $ test.surface_covered_in_m2 = pd.Series(predictions)
brand_mean_price = {} $ for item in brand_top_10: $     brand_mean_price[item] = autos.loc[autos['brand']==item, 'price'].mean().round(2) $ print(brand_mean_price)
def city_to_airport_code(city): $     df = airports_df[(airports_df['city'].str.lower() == city) & \ $                 ((airports_df['classification'] == 1) | \ $                  (airports_df['classification'] == 2))]['fs'] $     return list(df) $
modern_combos.cache()
cats_df = cats_df.set_index(['Unnamed: 0']) $ cats_df.index.name = 'seq' $ cats_df['remove'] = False #Used to identify samples to be removed
merged_df = (odds[odds.team.isin(agg_stats.team.unique())] $                 .pipe(pd.merge, match_results, on=['date', 'team', 'home_game']) $                 .pipe(pd.merge, agg_stats, on=['date', 'team', 'opponent']) $                 .sort_values(by=['game']))
url = "http://www.fdic.gov/bank/individual/failed/banklist.html" $ banks = pd.read_html(url) $ banks[0][0:5].ix[:,0:4]
training = not_missing_values_pd.sample(frac=0.7, replace=False) $ testing = not_missing_values_pd.drop(training.index)
y_col = ["state"] $ X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y_encode, $                                                     test_size=0.2, $                                                     random_state=42) $
pd.DataFrame(d, index=['apple', 'clock'])
type(t1.index)
precision = float(precision_score(y, gbc.predict(X))) $ recall = float(recall_score(y, gbc.predict(X))) $ print("The precision is {:.1f}% and the recall is {:.1f}%.".format(precision * 100, recall * 100))
clean_train_df = pd.concat([clean_train_df, selected_train_df], axis=1)
arr = np.arange(12.).reshape((3, 4))
df.head(3)
wages = web.DataReader("A576RC1A027NBEA", $               "fred", $               datetime.date(1929, 1, 1), $               datetime.date(2013, 1, 1)) $ wages
dd=cfs.diff_abundance('Subject','Control','Patient')
for category in page.categories() : $     if not category.isHiddenCategory() : $         print (category.title())
df_resolved_links.resolved_domain.value_counts().head(25)
facts_metrics.shape
os.chdir(today)
text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])
for key in sorted(counter_trump, key=counter_trump.get, reverse=True): $     print(key,counter_trump[key])
tweets_df = tweets_df[tweets_df['Text'].apply(lambda x: lr.is_tweet_english(x))] $ tweets_df
ac['Filing Date'].describe()
df.loc[:, 'id'].is_unique
columns = inspector.get_columns('station') $ for column in columns: $     print(column["name"], column["type"])
example['hour (sin)'] = np.sin((2. * example['hour'] *  np.pi / 24)) $ example['hour (cos)'] = np.cos((2. * example['hour'] *  np.pi / 24))
np.exp(results.params)
pilot_ratings = np.concatenate(( $     stats.norm.rvs(loc=4.5, scale=0.3, size=70), $     stats.skewnorm.rvs(a=-1, loc=4, scale=1, size=30) $ )).clip(0.01, 4.99) $ df_pilots = pd.DataFrame({"rating" : pilot_ratings, "id": range(len(pilot_ratings))})
oil_interpolation=oil_interpolation.interpolate() $ pd.DataFrame.head(oil_interpolation)
most_active_df = most_active_df.loc[most_active_df['date'] > year_ago] $ most_active_df.plot.bar
np.random.binomial(n_new,p_new,10000) $
plt.rcParams['axes.unicode_minus'] = False $ dta_54.plot(figsize=(15,5)) $ plt.show()
merged2 = merged1.copy()
for i in default.index: $     percentdonationoptional(i)
plt.show()
results["units"]["unit_annotation_score"].head()
df_release = pd.read_csv( pwd+'/releases.052317.csv', encoding='utf-8') #earning release?
age_gender_bkts.info()
len(air_store[air_store.air_genre_name.isnull()])
top_10_above_20 = tweet_archive_df[tweet_archive_df.rating_numerator > 20].sort_values(by=['rating_numerator'],  ascending=False)[0:10] $ bottom_10_above_20 = tweet_archive_df[tweet_archive_df.rating_numerator > 20].sort_values(by=['rating_numerator'],  ascending=True)[0:10]
print("All Tweets: {0} | Users: {1}".format(len(matthew), matthew.user.nunique())) $ print("Tweets in the Matthew 92 Collection: ", len(matthew.query('matthew92'))) $ print("Users in the Matthew 92 Collection: ", matthew.query('matthew92').user.nunique())
autos['fuel_type'].unique()
def get_list_User_ID(the_posts): $     list_User_ID = [] $     for i in list_Media_ID: $         list_User_ID.append(the_posts[i]['shortcode_media']['owner']['id']) $     return list_User_ID
for i in genre_vectors.columns: $     print i, genre_vectors[i].sum()
brands = autos["brand"].value_counts().head(20).index.tolist()
series1.corr(series2, method='pearson')
query1 = ("DROP VIEW IF EXISTS genre_revenue;") $ cur = conn.cursor() $ cur.execute(query1)
for word in first_words.take(10): $     print word
state_party_df = state_party_df.rolling(7).mean()
fsrq = np.where( np.logical_or(table['CLASS1']=='fsrq ',table['CLASS1']=='FSQR '))
local.get_dataset(1)
result_df.head()
mainkey = ['C/A','UNIT','SCP','STATION'] $ ttTimeEntry.sort_values(mainkey + ['DT'], inplace = True, ascending = False)
bacteria_data.index
datafiles = sorted(glob.glob('drive/NBA_Data_Hackathon/gts_*.csv'))
index_weighted_cumulative_returns = calculate_cumulative_returns(index_weighted_returns) $ etf_weighted_cumulative_returns = calculate_cumulative_returns(etf_weighted_returns) $ helper.plot_benchmark_returns(index_weighted_cumulative_returns, etf_weighted_cumulative_returns, 'Smart Beta ETF vs Index')
df = pd.read_pickle("dfWords.p")
final_cols = list(set(df17).intersection(list(df16))) $ f16 = df16[final_cols].copy() $ f17 = df17[final_cols].copy() $ final_result = pd.concat([f16, f17]) $ len(final_result)
contribs.info()
small_train = small_train.dropna() $ small_train.shape
w.cfg['indicators'] $ [item.strip() for item in w.cfg['indicators'].loc['din_winter'][0].split(', ')]
df_train.info()
passenger_ratings = np.concatenate(( $     stats.norm.rvs(loc=4.5, scale=0.3, size=500), $     stats.skewnorm.rvs(a=-1, loc=4, scale=1, size=500) $ )).clip(0.01, 4.99) $ df_passengers = pd.DataFrame({"rating": passenger_ratings, "id": range(len(passenger_ratings))})
All_tweet_data_v2.name[All_tweet_data_v2.name.str.contains('^[(0-9)]')]
outcomes = [0,1] $ probs = [1-c_rate_null,c_rate_null] ### same as above $ old_page_converted = np.random.choice(outcomes, size= n_old,replace = True, p = probs) $ old_page_converted
cnn_g.get_collection('variables')
url_domains = grouped['domain'].agg({'domain': lambda x: np.unique(x)}) $ unique_urls = pd.merge(unique_urls, url_domains) $ unique_urls.head()
conn.tableinfo(name='data.iris', caslib='casuser')
model = gensim.models.Word2Vec(sentences, workers=4)
data = {'Integers' : [1,2,3], $         'Floats' : [4.5, 8.2, 9.6]} $ df = pd.DataFrame(data, index = ['label 1', 'label 2', 'label 3']) $ df
str_2017 = datetime.datetime(2017, 1, 1, 0, 0, 0) $ end_2017 = datetime.datetime(2017, 12, 31, 23, 59, 59)
1/np.exp(.0783), 1/np.exp(.0469), 1/np.exp(.0118), 1/np.exp(.0175), np.exp(results.params)
df = df_with_metac_with_onc.drop(columns = ls_other_columns) $ df_total = pd.concat([df, df_other_dummies], axis = 1)
print(pd.read_csv("loans_2007.csv", parse_dates=["issue_d","earliest_cr_line","last_credit_pull_d"], nrows=5))
noaa_data.set_index('LST_DATE_LST_TIME',inplace=True)
news_sentiment_analysis.head()
def getHourDay(d): $     tempDate=d+datetime.timedelta(hours=5,minutes=45) $     return tempDate.hour,pd.datetime(tempDate.year,tempDate.month,tempDate.day)
logit_country = sm.Logit(df_new['converted'], $                            df_new[['country_UK', 'country_US', 'intercept']]) $ result_country = logit_country.fit()
data.groupby('Agency').size().sort_values(ascending=False)
df_sum=pd.DataFrame(data=sum_row).T $ df_sum 
investors.shape
knn.fit(X_train, y_train)
null_mean = 0 $ p_value = (null_valls > actual_obs_diff).mean() $ p_value
print("Predicted:", "\n",' '.join(y_pred[0])) $ print('\n') $ print("Correct:", "\n" ,' '.join(sent2labels(sample_sent)))
sns.lmplot(x="totalAssets", y="totalRevenue", data=fin_df) $ ax = plt.gca() $ ax.set_title("Relationship between Assets and Revenue") $ plt.show()
lrs=loan_requests_indebtedness.id_loan_request[loan_requests_indebtedness.postcheck_data!='null'].unique()
df.groupby(('C/A', 'UNIT', 'STATION', 'SCP', 'DATE')).sum()
my_gempro.uniprot_mapping_and_metadata(model_gene_source='ENSEMBLGENOME_ID') $ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping) $ my_gempro.df_uniprot_metadata.head()
df_video_meta = pd.DataFrame(video_meta) $ df_video_meta.head(2)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0] $
df[    df.overworked == False       ]
cross_val_score(rf,X_train,y_train,cv=10).mean()
tweets['created_at'] = pd.to_datetime(tweets['created_at']) $ tweets.dtypes
nulls=pd.DataFrame(merged[pd.isnull(merged.onpromotion)]) $ stores_with_nulls=len(nulls['store_nbr'].unique()) $ all_stores=len(stores['store_nbr'].unique()) $ stores_with_nulls/all_stores
states.set_index(['location', 'day'], inplace=True) $ states
for i, row in problems.iterrows(): $     srx, srr = row.srx, row.srr $
local_2 = pd.read_csv('mar_quarterly_raw_data/local_Nestle.reviews.adhoc.F180319T180331.2018_0411_1610.xlsx.csv') $ local = pd.concat([local_1, local_2]) $ local.rename(columns={'title':'product_description', 'creation_date':'review_date'}, inplace= True) $ local_mar = local[local_clm] $ local_mar.shape
ten = pd.merge(left=free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].mean().idxmax(1).to_frame(), right = free_data.groupby('educ')['v1','v2','v3','v4','v5','v6'].median().idxmax(1).to_frame(), left_index=True, right_index=True) $ ten.columns = ['mean', 'median'] $ ten
ls_columns_reordered = df_total.columns.tolist() $ ls_columns_reordered.remove('pn_flag') $ ls_columns_reordered = [ls_columns_reordered[0]] + ['pn_flag'] + ls_columns_reordered[1:]
variables = ["CRW_BAA_Week_{:02d}".format(x) for x in range(2, 14)]
def cohort_period(df): $     df['CohortPeriod'] = np.arange(len(df)) $     return df
pd.period_range('2020Q1','2020Q4', freq='Q')
df_ad_airings_4.to_pickle('./TV_AD_AIRINGS_FILTER_DATASET_3.pkl')
with open('apiconfig.json') as json_data_file: $     config = json.load(json_data_file)
p.end_time
data['moving_avg'] = pd.rolling_mean(data['sentiment'], 100)
age.sort_values()
df_prep17_.index
df_npc = df2[(df2['landing_page'] == 'new_page')] $ conv = df_npc['converted'] $ new_page_converted = np.random.choice(conv, nnew)
TBL_FCBridge_query = "SELECT bridge_id, brkey, struct_num, yearbuilt, latitude, longitude FROM TBL_FCBridge" $ data_FCBridge = pd.read_sql(TBL_FCBridge_query, cnxn) $ data_FCBridge
np_tr = ab_data[(ab_data['group']=='treatment') & (ab_data['landing_page']=='new_page') ].count()
topic_words = [] $ for topic in clf.components_: $     word_idx = np.argsort(topic)[::-1][0:n_top_words] $     topic_words.append([vocab[i] for i in word_idx])
northern_sea_level = pd.read_table("http://sealevel.colorado.edu/files/current/sl_nh.txt", $                                    sep="\s+") $ northern_sea_level
sm.qqplot(price_mm, loc=price_mm.mean(), scale=price_mm.std())
cols = ['md5_R1', 'libsize_R1',  'avgLen_R1',  'md5_R2', 'libsize_R2',  'avgLen_R2'] $ fastq = df.loc[~df.isnull().all(axis=1), cols].copy()
df2.drop(labels = 2893, axis=0, inplace=True)
violation_counts = restaurants["VIOLATION CODE"].value_counts(); $ violation_counts[0:10]
df = df.selectExpr("_c0 as text", "_c1 as senti_val") $ df.show()
top_10_authors = pd.value_counts(git_log['author']) $ top_10_authors = top_10_authors.head(10) $ print(top_10_authors)
ebay[pd.date_range('2017-01', periods=52, freq='W-FRI')].plot()  
n_new = df2[df2['group']=='treatment']['user_id'].count() $ n_new
building_pa_prc_shrink=pd.read_csv('buildding_01.csv',parse_dates=['permit_creation_date'])
np.ma.masked_where(f>=0.5, f)  #printing either true or false for the condition given above $
token_sendavg["ID"] = token_sendavg.sender $ token_receiveavg["ID"] = token_receiveavg.receiver
vio.head(3)
df1 = data_df.Count.resample('W').sum() $ df = df1.copy() $ df = data_df.Count
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=850) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df['timestamp'] = pd.to_datetime(df['timestamp'])
caps2_output_masked = tf.multiply( $     caps2_output, reconstruction_mask_reshaped, $     name="caps2_output_masked")
user = api.get_user('sheltielove') $ print (user.screen_name) $ print (user.followers_count) $ for friend in user.friends(): $    print (friend.screen_name)
df.groupby("pickup_week")["cancelled"].mean()
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
dfRegMet2013.shape
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head() $ df_new.tail()
dropoff_demand.head()
dfRegMet = dfRegMet[dfRegMet["longitude"] > -71.934259]
import matplotlib as mpl $ births.pivot_table('births', index='dayofweek', $                   columns='decade', aggfunc='mean').plot() $ plt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']) $ plt.ylabel('mean births by day')
git_log = pd.read_csv( 'datasets/git_log.gz', sep='#', encoding='latin-1', header=None, names=['timestamp', 'author'] ) $ git_log.head()
df3 = pd.DataFrame((df2['Ret_stdev']).resample('M').last())
c.update_one({'name.last': 'Bowie'}, $              {'$push': {'albums': {'name': "Let's Dance", $                                    'released': 1983}}})
p_old = df2.converted.mean() $ print("Convert rate for p_old:", p_old)
zipcodesdetail = pd.read_csv('in/zip_code_database.csv')
_,ax=plt.subplots(2,1,figsize=(20,20)) $ sns.countplot(calls_df["user"],ax=ax[0]) $ sns.countplot(calls_df["status"],ax=ax[1]) $
movies = pd.read_csv('WATCHLIST.csv', sep=',')
n_new = df2.query('landing_page=="new_page"').count()[0] $ n_new
import dllib.lr_sched as lr_sched
print(dictionary.token2id)
plt.plot(CumulativeProfit, label="Algorithm") $ plt.xlabel("Time") $ plt.ylabel("Money") $ plt.legend() $ plt.show()
mars_facts_df=pd.DataFrame(mars_facts[0]) $ mars_facts_df.columns=["description", "value"] $ mars_facts_df
commits_per_weekday = git_log.timestamp.dt.weekday.value_counts(sort=False) $ commits_per_weekday
grouped_authors_by_publication.tail()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, retry_delay=1, timeout=120, # 2 minutes $                  compression=True, $                  wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
r = requests.get(post_url)
unsorted_df.sort_index(ascending=False)
parch = df_titanic['parch'] $ print(parch.describe()) $ parch.value_counts()
rng_dateutil.tz == tz_dateutil
labels = common.board.create_uci_labels() $ label2ind = dict(zip(labels,list(range(len(labels)))))
samp_size = n $ joined_samp = joined.set_index("timePeriodStart")
df.index = range(len(df)) $ timedifference = [] $ for i in range(len(df) - 1): # range is df - 1 because the last tweet can't add 1 $     timedifference.append(df.created_at[i] - df.created_at[i+1])
df_sb = pd.read_csv("sobeys_all.csv", encoding="latin-1")
return_series=pd.Series(list(return_list.values()),index=return_list.keys()) $ np.argmax(return_series)
labels_ = np.array(is_duplicate, dtype=int) $ print('Shape of label tensor:', labels_.shape)
def getHour(x): $     return x.split()[1].split(':')[0] $
Celsius.__dict__['temperature'].__delete__(Celsius)
ctc_alpha = ctc ** (1/3) $ ctc_alpha = ctc_alpha / ctc_alpha.max().max()
from sklearn.model_selection import train_test_split
guinea_data.Description.value_counts()
sns.set() $ sns.distplot(df_input_pd.Resp_time.dropna(), kde=True, color='b')
jointplot = sns.jointplot(user_answered_counts, user_corrs, height=10, color='darkblue') $ jointplot = jointplot.set_axis_labels('number of times user answered a question', $                                       'mean correlation of prediction and ground truth for user', $                                       fontsize=16)
cust_demo.dtypes
twitter_data_v2.tweet_id.unique().shape
df2 = df2.set_index('user_id') $ df2.head()
my_df_small = pd.DataFrame(np.arange(10)) $ my_df_large = pd.DataFrame(np.arange(100000))
df_country_join[['UK','US']] = pd.get_dummies(df_country_join['country'])[['UK','US']] $ df_country_join.head()
df = pd.DataFrame() $ for u in users: $     df[u] = pd.Series(subs[subs.user_id == u].problem_id.values)
import statsmodels.api as sm $ convert_old = 17489 $ convert_new = 17263 $ n_old = 145274 $ n_new = 145310
df_vow.plot() $ df_vow[['Open','Close','High','Low']].plot()
valid_fn = r"../input/wiktraffictimeseriesforecast/validation_score.csv" $ valid_score_data = pd.read_csv(valid_fn, index_col=0) $ print(valid_score_data[0:10])
twitter_archive_clean['favorite_count'].corr(twitter_archive_clean['retweet_count'])
df_breed.head()
model = sm.OLS.from_formula('arrests ~ score_diff', stadium_arr) $ results = model.fit() $ print(results.summary())
with open('stations_info_dict.json', 'w') as fp: $     json.dump(stations_info_dict, fp)
cond_1 = df.query("group == 'treatment' and landing_page == 'old_page' ") $ cond_2 = df.query("group == 'control' and landing_page == 'new_page' ") $ event_count = len(cond_1)+ len(cond_2) $ print ("Total mismatch events:",event_count)
r.json()['dataset_data']['column_names']
machin.shape
df_census.Ward.head()
reddit.info()
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
(p_diffs > ab_data_diff).mean()
theta = np.minimum(1,np.maximum(0,np.random.normal(loc=0.75, scale=0.1, size=(10000,5))))
full['AdmitDate'].hist(xrot=90,figsize=(12,4)) $ plt.title('Patient distribution over dates',size=15) $ plt.ylabel('count')
df_treatment_group = df2.query('group == "treatment"') $ conversion_rate_treatment = df_treatment_group.query('converted == 1').shape[0] / df_treatment_group.shape[0] $ print('The probability of an individual from the treatment group converting is {}'.format(conversion_rate_treatment))
df.loc[df.public_gists.idxmax()]
sql = "SELECT * FROM paudm.forced_aperture_coadd_deprecated as coadd limit 5 " $ df3 = pd.read_sql(sql,engine)
yc_merged.head()
output = spark.read.parquet("/home/ubuntu/parquet/output.parquet/output.parquet")
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred_mdl))
df.message.head()
pdf = pd.read_csv('training_data/utah_positive_examples.csv') $ ndf = pd.read_csv('training_data/utah_negative_examples.csv') $ wdf = pd.read_csv('utah_weather_2010-2018_grouped.csv')
sns.boxplot(california.FIRE_SIZE)
top_songs['Position'].unique()
db = client.test_database $ collection = db.test_collection $ collection
unique_relpath = nocachedf.relpath.unique() $ unique_relpath
try: $     cur_b.execute('UPDATE room set hotel_id=99 WHERE hotel_id=1') $ except Exception as e: $     print('Exception: ', e)
results = logit_stats.fit() $ results.summary()
useful_indeed.isnull().sum() $
autos.columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', 'vehicle_type', 'registration_year', 'gearbox', 'power_ps', 'model', 'odometer', 'registration_month', 'fuel_type', 'brand', 'unrepaired_damage', 'ad_created', 'num_photos', 'postal_code', 'last_seen'] $ autos.head()
class_codes = class_codes.dropna() $ class_codes = class_codes[class_codes.use != 'USE'] $ class_dic = {key:value for key, value in zip(class_codes['class'].values, class_codes['use'].values)} $ ass15['use'] = ass15.rp1clacde.apply(lambda x: class_dic[x] if x in class_dic else np.nan) $ ass15.head()
mylist = session.query(measurement.date, measurement.prcp).filter(measurement.date.between('2016-08-23', '2017-08-23')).all() $
df.head()
autos['ad_created'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
geo_db.dtypes
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']])
df_new['last_active'] = df_new['last_active'].astype(str).str[0] $ df_new['last_active']= df_new['last_active'].apply(pd.to_numeric) $ df_new['last_active']
df_from_csv.equals(df)
data_final.head(2)
com_grp.size()  # return panda Series object
for user1, count in Counter(hashtags).most_common(10): $       print(user1 + "\t" + str(count))
print ("Probability that individual was in the treatment group,and they converted: %0.4f" % (df2.query('converted == 1 and group == "treatment"').shape[0]/df2.shape[0]))
db.remove_versions(GENOTYPE_ARRAY, v['version_id'].max())
print(airquality_pivot.index)
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42) $ training_loss = model_with_loss.get_latest_training_loss() $ print(training_loss)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=vq_k_-sidSPNHLeBVV8a')
with open('tweet_json.txt', 'w') as outfile: $     for tweet_json in tweet_list: $         json.dump(tweet_json, outfile) $         outfile.write('\n') #add a newline character at the end of each json
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_692.values.squeeze(), lags=40, ax=ax1)
table_rows = driver.find_elements_by_tag_name("tbody")[26].find_elements_by_tag_name("tr") $
year_prcp = session.query(Measurements.date, Measurements.prcp).order_by(Measurements.date).filter(Measurements.date > year_ago)
sns.heatmap(users.corr())
keras.Model.1
sum(clintondf.text.apply(lambda s: s.endswith(' -H')))
atdist_info_opp_dist_tabledata = atdist_opp_dist_info_count_prop_byloc.reset_index() $ create_study_table(atdist_info_opp_dist_tabledata, 'locationType', 'emaResponse', $                    location_remapping, atdist_info_response_list)
WorldBankdf.toPandas()
dtypes={'date':np.str,'type':np.str,'locale':np.str,'locale_name':np.str,'description':np.str,'transferred':np.bool} $ parse_dates=['date'] $ holidays_events = pd.read_csv('holidays_events.csv', dtype=dtypes, parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",holidays_events.shape) $ pd.DataFrame.head(holidays_events)
plate_appearances.shape
sandag_df_w_pred.to_csv('sandag_df_w_pred.csv')
df_breed = df_breed.query('p1_dog == True or p2_dog == True or p3_dog == True') $ df_breed.head()
p_new = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ print('Conversion of new Page is: ', p_new)
print('We have {} comments in total'.format(len(comments))) $ print('Average comment lenght is {:.2f} characters'.format(np.mean(comments.comment_content.apply(len)))) $ print('Average number of likes is {:.2f}'.format(np.mean(comments.likes))) $ print('There are {} comments without parent (those are direct responces to related article)'.format(np.sum(comments.parent_id.isna())))
crimes_by_type.sort_values("Offense_count",ascending=False)
float(pca.explained_variance_ratio_[0])*100
m.__repr__() $
df2.query('group =="treatment"').converted.mean()
df['text2'] = vader_df.text
matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) $ small_big_coeffs.plot.barh(colormap='Pastel1')
df_2015['bank_name'] = df_2015.bank_name.str.split(",").str[0] $
df_y_pred.values
tweepy.__version__
pos_dic.id2token.items()
df.pct_chg_opencls.hist(bins= 100)
index_name = df.iloc[0].name $ print(index_name)
full_history.columns = ["key", "date", "value"] $ full_history = full_history.reindex(columns=["key", "value", "date"]) $ full_history.to_csv(FILENAME_PREFIX + "modifications" + FILENAME_SUFFIX + ".csv", index=False) $ full_history.head()
regional_hol=regional_holidays[['date','description','locale','locale_name']] $ regional_hol.columns=['date','description_regional_hol','locale_regional','state'] $ pd.DataFrame.head(regional_hol)
nconvert = len(df2[(df2.converted==1) & (df2.group=='control')]) $ ntot = len(df2[(df2.group=='control')]) $ prob_1 = nconvert/ntot $ print(prob_1)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret)
data_demo["num_child"].describe()
print(df.columns) $ df.head(1)
pipe = pc.PipelineControl(data_path='examples/simple/data/varying_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=False) $ pipe.runPipeline()
rhum_df = pd.DataFrame(data = us_rhum) $ rhum_df.columns = ts.dt.date $ rhum_wide_df = pd.concat([grid_df, rhum_df], axis = 1) $ rhum_wide_df.head()
merkmale.Merkmal_typ.unique()
for file in os.listdir(PATH): $     if not file.endswith('zip'): $         continue $     !unzip -q -d {PATH} {PATH}{file}
df['lead_mgr'] = df['managers'].astype('str').map(lambda x:x.split('/')[0])
lower, upper = np.percentile(p_diffs, 2.5), np.percentile(p_diffs, 97.5)
df2[df2.user_id == duplicate_userid]
def timestamps_between(start, end, size): $     ts_start = int((start - datetime.datetime(1970, 1, 1)).total_seconds()) $     duration = int((end - start).total_seconds()) $     return np.array(random.sample(range(int(duration)), size)) + ts_start
df_cryptdex.head(5)
melted_total.groupby(['Categories','Neighbourhood']).mean().unstack()['Review_count'].ix[top10_categories.index].plot.bar(legend=True,figsize=(10, 5))
movies.printSchema()
data.rename(columns = {'Long':'long', 'Lat':'lat'}, inplace = True)
target_pf = test_portfolio[final_idx].copy()
for script in soup(["script", "style"]): $     print script $     script.extract()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
result = pd.concat(six) $ free_data = pd.merge(left=free_data, right = result.to_frame(), left_index=True, right_index=True)
auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret) $ auth.set_access_token(acc_token, acc_secret) $ api = tweepy.API(auth)
calc_temp_df = pd.DataFrame(calc_temp, columns=['Min', 'Max', 'Avg']) $ calc_temp_df
pandas.read_csv('Movies_countries.csv')
eia_extra.loc[idx['DPV',:,:]]
print "Baseline probability of a show being cancelling in season 1 is: ", 901.0/2590
train_binary = train_session.copy() $ drop_feature = ['user_id', 'total_secs_elapsed', 'date_account_created','timestamp_first_active','date_first_booking','country_destination'] $ train_binary.drop(drop_feature, axis=1, inplace=True)
cvec.fit(reddit['title']) $ title_tokens = pd.DataFrame(cvec.transform(reddit['title']).todense(), $              columns=cvec.get_feature_names()) $ title_tokens.shape
df.drop(['created_at', 'updated_at'], axis=1).describe(exclude = np.number)
plot = dfPriceCalculations.plot(title='Peak and Base Price per Day') $ plot.set(ylabel='Price') $ plt.xticks(rotation=45)
df.loc[dates[0]]
ser6.sum()
df_2007['bank_name'] = df_2007.bank_name.str.split(",").str[0] $
autos.drop(['seller', 'offer_type'], axis = 1, inplace = True) $ autos.head()
ab_dataframe['user_id'].nunique()
df_new=df2.query("landing_page=='new_page'") $ P=len(df_new)/len(df2) $ P
print('Repeated user_id in df2 dataframe is:{}'.format(df2[df2.user_id.duplicated(keep=False)].iloc[0,0]))
print(re.match('AA', 'AAbc')) $ print(re.match('AA', 'bcAA'))
n_old=df2.query('landing_page == "old_page"').shape[0] $ n_old
plt.hist(injury_df.DL_length, normed=True) $ plt.title('Length of Time on Disabled List') $ plt.xlabel('Number of Days') $ plt.ylabel('Number of Injuries')
df.iloc[::20].plot.bar(title="Percipitation") $ plt.tight_layout() $ plt.show() $
trigram_sentences_filepath = paths.trigram_sentences_filepath
bnbAx['gender_female'] = np.where(bnbAx['gender']=='FEMALE', 1, 0)
from sklearn.model_selection import train_test_split $ from sklearn.neighbors import KNeighborsClassifier $ from sklearn.preprocessing import StandardScaler $ from sklearn.model_selection import cross_val_score $ from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
df_clusters.columns=['cluster']+list(df.columns)+['text'] $ df_clusters['cluster_cat']=pd.Categorical(df_clusters['cluster']) $ df_clusters.head()
df_repub['text'].dtypes
convo2 = companyNeg[companyNeg.group_id == 'group_98019' ]
import pandas as pd $ stocks_info_df = pd.read_csv('../data/stocks_info.csv')
from svpol.tests import ValidFiles $ ValidFiles
plot_price(f,'Close',start='Jan 01, 2017',end='Dec 31, 2017') $ plt.legend("last year")
usersDf.hist(column=['friends_count'],bins=50) $
guinea_data4 = guinea_data3.set_index(['Description', 'Date']).sort_index() $ guinea_data4.columns = ['Guinea'] $ guinea_data4
print "Columns: ", df3.columns.values $ df3
df_ab_page.drop(['group','landing_page','old','control','UK','US','CA','ab_page','UK_ind_ab_page'], axis=1, inplace=True)
print(applications.shape) $ print(applications.dtypes) $ print(applications.head(5))
%%time $ df["sentences"] = df["sentences"].str.split()
tuned_parameters = [{'alpha': [0.001,0.01,0.1,1,10] , 'eta0': [10 ** -4,10 ** -5,10 ** -6,10 ** -7,10 ** -8,10 ** -9,10 ** -10]}] $ gridSearchModel = GridSearchCV(SGDRegressor(random_state=42,loss='squared_loss',penalty='l2'),tuned_parameters,scoring = my_scorer, cv=5) $ gridSearchModel.fit(df_train, tsne_train_output) $ results = gridSearchModel.cv_results_
from sklearn.model_selection import train_test_split $ from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_auc_score
Stemmed = [stemmer.stem(word) for word in word_freq_df.Word] $ word_freq_df['Word_stem'] = Stemmed
githubid.get_repo_version(git_location = GIT_LOCATION) $
from pandas import Series $ df.instagrams = 50 $ ins = Series([10, 20, 30], index=[1, 3, 5]) $ ins
df_twitter.info()
node_types_DF = pd.read_csv(node_models_file, sep = ' ') $ node_types_DF
drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7
with open('data/chefkoch_02.json') as data_file:    $     chef02 = json.load(data_file) $ clean_new(chef02) $ chef02df = convert(chef02) $ chef02df.info()
import matplotlib.pyplot as plt $ % matplotlib inline
forcast_set=clf.predict(X_lately)
etf_weights = calculate_dividend_weights(dividends) $ project_helper.plot_weights(etf_weights, 'ETF Weights')
ab_data.isnull().values.any()
stringlike_instance.content
df_page1 = len(df2.query("landing_page == 'new_page'")) / df2.shape[0] $ print("{} is the probability that an individual received the new page.".format(df_page1))
store = pystore.store('mydatastore') $ store
newdf.head()
target_date = tmp_cov.index.get_level_values(0).unique() $ target_date = target_date[0] $ target_date
df_all_loans.to_clipboard()
speaker_collection = [] $ for i in xrange(17): $     offset = i*100 $     response = requests.get(url_speaker,params = dict(limit = 100, offset = offset)) $     speaker_collection.append(response.json())
df2 = df2t.merge(df2c, how='outer')
df.dtypes
data["Date"] = np.array([tweet.created_at for tweet in tweets]) $ data["Date"] = pd.to_datetime(data["Date"]) $ data["Likes"] = np.array([tweet.favorite_count for tweet in tweets]) $ data["RTs"] = np.array([tweet.retweet_count for tweet in tweets]) #RTs stands for "retweets" for those less twitter savvy $ display(data.head(10))
!spark-submit --properties-file spark.conf script.py data/*
prices.info() $ prices[0:100] $ prices.loc[prices['symbol'] == 'XRP']
rf.feature_importances_
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
X = lq2015_combined[feature_cols] $ y = lq2015_combined.SaleDollars_fy
df_protest.loc[0:10, 'start_date'].dt.weekday
df.query('group == "treatment" and landing_page == "new_page"').user_id.count()
generate_chart(precision_recall_curve, $                "", $                "injured", $                [('a', our_nb_classifier), ]) $ plt.show() $
df2[df2['landing_page']=='new_page'].count()[0]/df2.count()[0]
shows = pd.DataFrame(data={'title':titles,'status':statuses,'years':years,'network':networks,'genre':genres,\ $                           'tagline':taglines,'link':links})
n_old = df2_control.shape[0] $ n_old
all_a=soup.find_all('a') $ print(all_a)
zero_rev_acc_opps.drop(labels=[' Total BRR ', ' AnnualRevenue ', $        'NumberOfEmployees', ' DandB Revenue ', 'DandB Total Employees'], axis=1, inplace=True)
import matplotlib.pyplot as plt $ k1.plot(x='cust_id', y='cust_id_total_send_mean', kind='bar') $ plt.show()
new_df = df[["C/A", "UNIT", "SCP", "STATION", "DATE","DATE_TIME","ENTRIES"]] $ new_df.head(3) $ scp_dict = new_df.set_index(["C/A","STATION", "UNIT", "SCP", "DATE"]).T.to_dict(orient='list') $ print dict(itertools.islice(scp_dict.iteritems(), 5)) #print 5 entries from dict
stats_url = 'https://api.blockchain.info/stats'
dpickle.dump(ppm_body, open( "ppm_body.dpkl", "wb" ) )
posts.groupby('PostTypeId').count()
logit_countries2 = sm.Logit(new_sf['converted'],new_sf[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ country_effect = logit_countries2.fit() $ country_effect.summary()
mrtrumpdf['tags'] = mrtrumpdf.text.apply(lambda s: TextBlob(s).tags)
clf.fit(X_train, y_train)
new_page_converted = np.random.choice([1,0], size=NewPage, p=[0.12, 0.88]) $ new_page_converted
from sklearn.neighbors import KNeighborsClassifier $ from sklearn.model_selection import train_test_split
treatment_df = df2.query('group == "treatment"') $ (treatment_df['converted'] == 1).mean()
merged1 = merged1.rename(columns={'Name':'MeetingStatusName', 'Description':'MeetingStatusDescription'})
zeros = (grades.Mark == 0) $ grades_no_zeros = grades.drop(grades.index[zeros]) $ grades_no_zeros.describe() $
df.head() $ df.user_id.nunique()
print 'If we create a Series from the DataFrame, then passing a single value does work.  CONFUSING!' $ msftC = msft['Close'] $ msftC['2012-01-03']
next(file.generateRecords())
data4.to_file('Twitters_FSGutierres.shp', driver='ESRI Shapefile')
ridgeregcv = RidgeCV(alphas=alpha_range, normalize=True, scoring='neg_mean_squared_error') $ ridgeregcv.fit(X_train, y_train) $ ridgeregcv.alpha_
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean[twitter_archive_enhanced_clean.retweeted_status_id.isna()] $ twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'], axis = 1)
access_logs_df.select(min('contentSize'), avg('contentSize'),max('contentSize')).show()
from gensim.models import Word2Vec $ model = Word2Vec.load("../src/300features_40minwords_10context")
maxEndDate = df_obs['endDate'].max() $ df_obs = df_obs[df_obs['endDate'] == maxEndDate] $ df_obs = df_obs.sort('barrelID') $ df_obs
np.zeros(10)
combined_df = test.join(prediction_df) $ combined_df.head(15)
properati.loc[((properati['state_name'] == "Bs.As. G.B.A. Zona Norte") | (properati['state_name'] == "Bs.As. G.B.A. Zona Oeste") | \ $               (properati['state_name'] == "Bs.As. G.B.A. Zona Sur")),\ $               'state_name'] = "G.B.A"
sub_score = df.groupby(['subreddit'])['s_score'].mean()
recommendation_df['hacker_count'] = recommendation_df.groupby(['hacker_id'])['challenge_id'].transform('count')
support.amount.sum()
df2['datetime']=pd.to_datetime(df2['timestamp'], errors='coerce')
df2.query('group == "treatment"').converted.sum()/df2.query('group == "treatment"').converted.count()
from sklearn.linear_model import Lasso
components3= pd.DataFrame(pca.components_, columns=['SP500','DJIA','happiness'])
backup = clean_rates.copy()
df2 = df2.drop([1899])
aq = [w for w in cfd_index if re.search('^[aq]+$', w)] $ print({k:cfd_index[k] for k in aq if k in cfd_index})
df_students.shape
print(model.predict_output_word(['emergency', 'beacon', 'received']))
overallYearRemodAdd = pd.get_dummies(dfFull.YearRemodAdd)
Trump_raw.head(3)
df['Score_Binary'] = df.Score.apply(lambda x : 1 if x >= 5 else 0) $ df.head()
k = k.sort_values('count',ascending=False)
f, ax = plt.subplots() $ ax.set_ylim(ymax=4); $ ax.set_xlabel('Shift duration [h]'); $ ax.set_ylabel('Count'); $ df['duration'].astype('timedelta64[h]').hist(bins=75, ax=ax);
df.to_csv('trump_lies.csv', index=False, encoding='utf-8')  
(t2-t1).total_seconds()
%matplotlib inline $ commits_per_year.plot(kind="line", title="Commits Per Year", legend=False)
noaa_data['PRECIPITATION'].replace(range(-9999,0),np.nan,inplace=True)
rPnew = df2[df2['landing_page']=='new_page']['converted'].mean() $ rPold = df2[df2['landing_page']=='old_page']['converted'].mean() $ rp_diffs = rPnew - rPold
data['Borough'].unique()
print(deep_learning_tweet1.text)
df1 = make_df('AB', [1, 2]) $ df2 = make_df('AB', [3, 4]) $ display('df1', 'df2', 'pd.concat([df1, df2])')
class Rectangle: $         self.corner = point $     def set_width(self, dx): $         self.y = dy        
monitor_index = reader.select_column("monitor_index") $ temp = reader.get_monitor_column(monitor_index, "TM_T_SIPM")
X.to_csv('just_title', index=False)
data = pd.read_csv('csvs/datosSinDuplicados.csv', low_memory=False)
backup = clean_rates.copy() $ clean_rates.text = clean_rates.text.str.replace('\n', '')
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ new_temp_cat = temp_cat.rename_categories(['sejuk','sederhana','panas']) $ new_temp_cat 
s1 = pd.Series([4,3,5,2,5], index=['Mon', 'Tues', 'Wed', 'Thur', 'Fri'], name='Test1')
nonunique = df2.user_id.value_counts() $ nonunique[nonunique > 1].index[0]
df_archive_clean["source"] = df_archive_clean["source"].replace('<a href="http://vine.co" rel="nofollow">Vine - Make a Scene</a>', $                                                                "Vine - Make a Scene")
plt.hist(df['log_price'], bins=100) $ plt.show()
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-(0.05))
filtered_active_sample_sizes = active_sample_sizes.filter( $     active_sample_sizes.sample_size_1 > 10).persist()
state_grid_new = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(21, 21)) $ q_agent_new = QLearningAgent(env, state_grid_new) $ q_agent_new.scores = []  # initialize a list to store scores for this agent
