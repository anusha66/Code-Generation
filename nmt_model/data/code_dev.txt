from tempfile import mkstemp$ fs, temp_path = mkstemp("gensim_temp")  $ model.save(temp_path)  
df.to_csv("../../data/msft_piped.txt",sep='|')$ !head -n 5 ../../data/msft_piped.txt
data.describe().transpose()
df1['label'] = df[forcast_col].shift(-forcast_out)$ df1['label'].head()
help(pd.read_csv)
high_idx = afx['dataset']['column_names'].index('High')$ low_idx = afx['dataset']['column_names'].index('Low')$ change_values = [entry[high_idx] - entry[low_idx] for entry in afx['dataset']['data'] if entry[high_idx] and entry[low_idx]]
df.Visitors.tolist()
merged2.shape$
from matplotlib.dates import MonthLocator, WeekdayLocator, DateFormatter$ %pylab inline$ pylab.rcParams['figure.figsize'] = (15, 9)$
df['name'].value_counts()
fig,ax=plt.subplots(1,2,figsize=(15,3))$ ax[0].boxplot(joined['Promo2SinceYear'],vert=False)$ ax[1].boxplot(joined['Promo2Days'],vert=False)
from features.build_features import remove_invalid_data$ df = remove_invalid_data(pump_data_path)$ df.shape
yourstartdate=datetime.strptime(input('Enter End date in the format %Y-%m-%d'), '%Y-%m-%d')$ yourenddate=datetime.strptime(input('Enter End date in the format %Y-%m-%d'), '%Y-%m-%d')$ calc_temps(yourstartdate,yourenddate)
back_to_h2o_frame = h2o.H2OFrame(pandas_small_frame)$ print(type(back_to_h2o_frame))$ back_to_h2o_frame
train_topics_df=pd.DataFrame([dict(ldamodel[item]) for item in train_corpus], index=graf_train.index)$ test_topics_df=pd.DataFrame([dict(ldamodel[item]) for item in test_corpus], index=graf_test.index)$
gram_collection.find_one({"account": "deluxetattoochicago"})['date_added']$
tlen.plot(figsize=(16,4), color='r');
import pickle$ bild = pickle.load(file=open("facebookposts_bild.pickle", "rb"))$ spon = pickle.load(file=open("facebookposts_spon.pickle", "rb"))
new_df = df.replace(['poor','average','good','exceptional'], [1,2,3,4])$ new_df
customer_emails = sales_data_clean[['Email', 'Paid at']].drop_duplicates()$ customer_emails.dropna(inplace=True)
df.to_csv('GageData.csv',index=False)
print(airquality_pivot.head())
raw_data = pd.read_csv("kickstarter.csv") #Example$ raw_data.head()$ print ("The provided data set consists of",raw_data.shape[0],"rows and",raw_data.shape[1],"columns (features.")
df2.shape
ebola_tidy = pd.concat([ebola_melt_1, status_country], axis = 1)$ print(ebola_tidy.shape)$ print(ebola_tidy.head())$
cityID = '488da0de4c92ac8e'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Plano.append(tweet) 
display(data.head())
np.mean(cross_val_score(lgb1, train_X, train_Y, scoring='r2', cv=5, verbose=5))
data =[]      $ for row in result_proxy:$     data.append({'station':row.stations.name,'RainFall':row.rainfall})
df['text'] = df.apply(clean, args=(emojistring,), axis=1)
print('Number of rows with invalid values = {}'.format(len(joined[joined.isnull().any(axis=1)])))$ joined[joined.isnull().any(axis=1)]
S_distributedTopmodel.decision_obj.hc_profile.options, S_distributedTopmodel.decision_obj.hc_profile.value
data['2018-5-1']['res_time_avg'].plot()$ plt.show()
network_simulation[network_simulation.generations.isin([8])]$
df = df.loc[df['game_type'] == 'R',]
kyt_lat = 34.955205 #35.005205$ kyt_long = 135.675300 #135.7353$ diff_wid = (135.795300 - 135.675300)/grid_size$
r = requests.get(url_api)$ print(r.status_code)
average_chart_upper_control_limit = average_of_averages + 3 * d_three * average_range / \$                                     (d_two * math.sqrt(subgroup_size))
aug2014 = pd.Period('2014-08',freq='M')$ aug2014
hs_path = utils.install_test_cases_hs(save_filepath)
B2.print_all_paths()$
news_sentiments.to_csv("News_Sentiments.csv")
db = client.Mars_db$ collection = db.mars_news
tfidf_vect = TfidfVectorizer(stop_words="english")$ tfidf_mat = tfidf_vect.fit_transform(PYR['soup'].values)
kushy_prod_data_path = "products-kushy_api.2017-11-14.csv"$ kushy_prod_df = pd.read_csv(kushy_prod_data_path, low_memory=False)$ kushy_prod_df.tail(10)
X_train_valid, X_test, y_train_valid, y_test = train_test_split(pm_final.drop('status', axis = 1)$                                                     ,pm_final[['status']],$                                                     test_size=0.20)
display(data.head(10))
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key="+API_KEY)
citydata_with_nbr_rides = pd.merge(citydata_avg_fare_work, city_nbr_rides, on='city', how='left')$ citydata_with_nbr_rides.head()
xml_in_sample1['authorId'].nunique()
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL')$                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1))$                           &(extract_nondeduped_cmp.app_branch_state=='CA')].groupby('APP_PROMO_CD').size()
merged = pd.merge(train,properties,how='left',on='parcelid')
print("Min " + str(dc['created_at'].min()) + " Max " + str(dc['created_at'].max()))$
converted = ts.asfreq('45Min', method='pad')
rate_change['rating'].sort_values(ascending=False)[0:2]
urban_ride_total = urban_type_df.groupby(["city"]).count()["ride_id"]$ urban_ride_total.head()
result_new.summary2()
columns = inspector.get_columns('clean_hawaii_stations.csv')$ for c in columns:$     print(c['name'], c['type'])$
pd.DataFrame(data['data']['children'])    # will give us the kind of data as well as the subreddit id
df.head()
model.wv.doesnt_match("aku ngentot anda sex".split())
data_frame['CLASS1'].value_counts()
pivoted.T.shape
inflex_words = en_translation_counts[en_translation_counts < 2]$ print("Total: {} unique words".format(len(inflex_words)))$ inflex_words.sample(10)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt"$ df = pd.read_table(path, sep ='\s+', na_values=['.'])$ df.head(5)
a['SA'] = np.array([ analyze_sentiment(tweet) for tweet in a['Tweets'] ])$ b['SA'] = np.array([ analyze_sentiment(tweet) for tweet in b['Tweets'] ])
siteMask = nitrodata['MonitoringLocationIdentifier'].isin(siteInfo.index)$ dfSubSites = nitrodata[siteMask]$ dfSubSites.shape
import matplotlib.pyplot as plt$ import seaborn as sns$ %matplotlib inline
recommendationTable_df = recommendationTable_df.sort_values(ascending=False)$ recommendationTable_df.head()
gbm_model.plot()
sss = list(spp.season.unique())
train_small_data = pd.read_feather("../../../data/talking/train_small_data.feather")$ val_small_data = pd.read_feather("../../../data/talking/val_small_data.feather")$ test = pd.read_feather("../../../data/talking/test_small_data.feather")
ecxels=pd.merge(ecxels, ecxels, on=['אורחת','מארחת','מועד המשחק']).drop_duplicates(subset=['אורחת','מארחת','מועד המשחק'],keep="first")
data.head(10)
net.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=directory_name)
payments_all_yrs_ZERO_discharge_rank = (df_providers.loc[idx_ZERO_discharge_rank,:].groupby(['id_num','year'])[['discharge_rank']].sum())$ payments_all_yrs_ZERO_discharge_rank = payments_all_yrs_ZERO_discharge_rank.sort_values(['discharge_rank'], ascending=[False])$ print('payments_all_yrs_ZERO_discharge_rank.shape',payments_all_yrs_ZERO_discharge_rank.shape)
for url in soup.find_all('a'):$     print (url.get('href'))
y=dataframe1['Chaikin']$ plt.plot(y)$ plt.show()
B_INCR2 = 0.293$ B_DECR2 = -0.293
response = requests.get(url)
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365)$ print(last_year)$
sub = pd.DataFrame()$ sub['click_id'] = test_df['click_id']$ print("Sub dimension "    + str(sub.shape))
number_pos = data_df[data_df['is_high_val'] == 1].shape[0]$ number_all = data_df.shape[0]$ print(f'Target labels of class \'1\': {number_pos} or {(number_pos/number_all)*100:.2f}% over all.')
d = nc.Dataset('data/otn200_20170802T1937Z_a755_2845_bd51_e1ca_6d36_6133.nc', 'r')
url = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2017-01-01&end_date=2017-12-31&' + API_KEY$ r = requests.get(url)$ json_data = r.json()
y = list(train_50m_ag.is_attributed)$ X = train_50m_ag.drop(['is_attributed'],axis=1)$ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)
cbs = news_sentiment('@CBSNews')$ cbs['Date'] = pd.to_datetime(cbs['Date'])$ cbs.head()
validation.analysis(observation_data, Jarvis_resistance_simulation_0_5)
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
tables=db_engine.table_names()$
userProfile = userGenreTable.transpose().dot(inputMovies['rating'])$ userProfile
all_text.head()
pp = rf.predict_proba(X_train)$ pp = pd.DataFrame(pp, columns=['The_Onion_Prob', 'VICE_Prob', 'GoldenStateWarriors_Prob'])
sentiments_pd.to_excel("NewsMood.xlsx", encoding="UTF-8")
elms_all_0604 = pd.read_excel(cwd+'\\ELMS-DE backup\\elms_all_0604.xlsx')$ elms_all_0604['ORIG_DATE'] = [datetime.date(int(str(x)[0:4]),int(str(x)[5:7]),int(str(x)[8:10]))$                              for x in elms_all_0604.ORIG_DATE.values]
dfa=new_table.groupby("type")
df3.duplicated().sum()
lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics = topics)
kick_projects[['goal', 'pledged', 'backers','usd_pledged','usd_pledged_real','usd_goal_real','state']].corr()
salida_all = getFacebookPageFeedData(page_id, access_token, 1000)$ columns = ['post_from', 'post_id', 'post_name', 'post_type', 'post_message', 'post_link', 'post_shares', 'created_time']$ df_posts = pd.DataFrame(columns=columns)
tag_df = pd.get_dummies(tag_df)$ tag_df.head()
data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)
yc200902_short = yc[::1000]$ yc200902_short.shape
grouped_by_letter = df1.groupby(['tactic_letter'])$ list_of_letter = df1['tactic_letter'].unique()$
import json$ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=apikey')$
results.summary2()
cur.execute('SELECT results FROM trials WHERE trial_id = 1')$ trial1_results = cur.fetchone()$ pd.read_csv(io.StringIO(trial1_results[0]), index_col=0)$
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])
data = pd.read_sql("SELECT * FROM session_privs",xedb)$ print(data)$
final.loc[(((final.loc[:,'RA']-261.8475)**2+(final.loc[:,'DEC']-55.1813888889)**2)**0.5).idxmin(),:]
crimes['month'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.month)
new_messages['sub'] = (new_messages.timestamp - new_messages.watermark)$ new_messages['day'] = pd.to_datetime(new_messages.watermark, unit='ms').dt.date$
m.fit([X], Y, epochs=10, batch_size=16, validation_split=0.1)$
df_treat= df2[df2['group'] == 'treatment']['converted'].mean()$ print("{} is the probability they converted.Thus, given that an individual was in the treatment group.".format(df_treat))
! mkdir census_data$ ! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data --output census_data/adult.data$ ! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test --output census_data/adult.test
len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
experiment_df = pd.read_csv('multi_arm_bandit_example_distrib.csv')$ experiment_df.drop(['Unnamed: 0'], axis=1, inplace=True)$ experiment_df.head(2)
from pyspark.ml.feature import StringIndexer, VectorIndexer
resp_dict = r.json()$ type(resp_dict)
sns.distplot(temp_df[temp_df.total_companies > 100].proportion_no_psc)
the_data = tmp_df.applymap(lambda x: 1 if x > 3 else 0).as_matrix()$ print(the_data.shape)
search2 = search2.sample(frac=1)
sentences = [['first', 'sentence'], ['second', 'sentence']]$ model = gensim.models.Word2Vec(sentences, min_count=1)
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'})$ station_count = station[['Count']]$ station_count 
!wget -nv https://data.wprdc.org/datastore/dump/40776043-ad00-40f5-9dc8-1fde865ff571 -O 311.csv
df.set_value(306, 'yob',1998)
temps_df.Missoula > 82
USvideos = pd.read_csv('data/USvideos.csv', parse_dates=['trending_date', 'publish_time'])
pd.to_datetime(['2009/07/31', 'asd'], errors='raise')
fix_comma = lambda x: pd.Series([i for i in reversed(x.split(','))])
unordered_df = USER_PLANS_df.iloc[unordered_timelines]
labeled_news = pd.read_csv('./labeled_news.csv', encoding='cp1252', header=None, names = ["class", "discription", "source", "title"])$ labeled_news = resample(labeled_news)$ labeled_news.head()
graph = facebook.GraphAPI(access_token=access_token, version='2.7')
top_genre= pd.read_sql_query('select * from top_genre', engine)$ top_genre.head()
payments_total_yrs.head(2)
BASE_URL = 'https://orgdemo.blastmotion.com'
session.query(func.count(Station.station)).all()
future = m.make_future_dataframe(periods=90)$ future.tail()
fcc_nn.plot(y='score', use_index=True)
fh_1 = FeatureHasher(input_type='string', non_negative=True) # so we can use NaiveBayes$ %time fit = fh_1.fit_transform(train.device_model)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=JyyZ2KQSdpCZHjbk1E9x"$ req  = requests.get(url)$
pd.Series([2, 4, 6], index=['a','b','c'])
print ("The number Unique userid in dataset is {}".format(df.user_id.drop_duplicates().count()))
p(locale.getdefaultlocale)
df3.values
taxi_sample = conn.select_ipc_gpu(query, device_id=2)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2}$ plot_partial_autocorrelation(doc_duration, params=params, lags=30, alpha=0.05, \$     title='Weekly Doctor Hours Partial Autocorrelation')
ward_df['Crime Count'] = ser.values
top_allocs = hist_alloc.loc[pd.to_datetime(intervals)].sum(axis=0).sort_values(ascending=False)$ top_allocs[:10], top_allocs[-10:]
tweets['hashtags'] = tweets['hashtags'].apply(lambda x: x.strip('[]').lower().replace("'",'').split(', '))$ tweets['user_mentions'] = tweets['user_mentions'].apply(lambda x: x.strip('[]').lower().replace("'",'').split(', '))
pgh_311_data['REQUEST_ID'].resample("M").count().plot()
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/Sample_Superstore_Sales.xlsx"$ df = pd.read_excel(path)$ df.head(5)
from pyspark.ml.evaluation import BinaryClassificationEvaluator$ evaluator = BinaryClassificationEvaluator(rawPredictionCol="prediction", labelCol="label", metricName="areaUnderROC")$ print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))
csv2 = pd.read_csv(csv_files[1])$ print(csv2.head())
html=HTML(html=doc)$ html.links
keys_0611 = keys.copy()$ keys_0611.to_excel(cwd + '\\ELMS-DE backup\\keys_0611.xlsx', index=False)
import pandas as pd$ pd_cat = pd.get_dummies(ibm_hr_cat.select("*").toPandas())$ pd_cat.head(3)
data_df.desc[17]
calls_nocontact.ticket_status.value_counts()
df['created_at_time'] = pd.to_datetime(df.created_at)
to_plot.plot.hist(by='Days Between Int', bins=100);$ plt.title("Actual number of days between purchases")$
datetime.now().strftime('%A')
%matplotlib inline$ commits_per_year.plot(kind='bar',title='Linux Commits by Year',legend=False)
local = mngr.connect(dsdb.LOCAL)$ local
bands.columns = ['BAND_'+str(col) for col in bands.columns]
retweet_plot = t_ret.plot(figsize=(16,4), label="Retweets", color="g", legend=True, title='Number of retweets for tweets over time')$ retweet_vs_time_fig = retweet_plot.get_figure()$ retweet_vs_time_fig.savefig('num_ret_over_time.png')
mgxs_lib = openmc.mgxs.Library.load_from_file(filename='mgxs', directory='mgxs')
hits_df = number_one_df.drop_duplicates(subset=['URL','Region'], keep ='first')
mRF.predict(test)
data.loc[9323,'Tweets']$
cityID = '095534ad3107e0e6'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Louisville.append(tweet) 
x.loc[:,"B":"C"]
result = DBRunQuery(q)$ print("Result = ")$ print(json.dumps(result, indent=2))
authors = EQCC(git_index)$ authors.get_cardinality("author_uuid").by_period()$ print(pd.DataFrame(authors.get_ts()))
df = df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]
plantlist[plantlist['commissioned'].isnull()]
print('Loading models...')$ model_source = gensim.models.Word2Vec.load('model_CBOW_jp_wzh_2.w2v')$ model_target = gensim.models.Word2Vec.load('model_CBOW_en_wzh_2.w2v')
bar_outlets = avgComp["Target"]$ bar_Compound = avgComp["Compound"]$ x_axis = np.arange(0, len(bar_Compound), 1)
n_user_days.hist()$ plt.axvline(x=4, c='r', linestyle='--')
final_rm=final.drop(target0.index)$ X=create_time_list(final_rm)
ccp_df = cc_df[coins].div(market_cap_df['Total Market Cap'], axis=0)$ ccp_df = ccp_df[::-1]    # Reverse order of df$ ccp_df.head()
iris_new = pd.DataFrame(iris_mat, columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','Species'])$ iris_new.head(20)
year2 = driver.find_elements_by_class_name('yr-button')[1]$ year2.click()
from templates.invoicegen import create_invoice
X = reddit['title'].values$ y = reddit['engagement']$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
cur.execute("SELECT * FROM pgsdwh.pgsparts.geps_distinct_order_lines_dw_t")
intervention_train.reset_index(inplace=True)$ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL'], inplace=True)
recortados = [recortar_tweet(t) for t in tweets_data_all]$ tweets = pd.DataFrame(recortados)
mars_table = mars_df.to_html()$ print(mars_table)
json_data = r.json()
from spacy.lang.en.stop_words import STOP_WORDS$ print('Example stop words: {}'.format(list(STOP_WORDS)[0:10]))
print(checking['age'].iloc[z])$ print(test_checking['age'].iloc[zt])
cityID = 'e4a0d228eb6be76b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Philadelphia.append(tweet) 
X_copy['crfa_c'] = X_copy['crfa_c'].apply(lambda x: ord(x))
logging.info('Saving')$ df_final.to_csv('~\\df_final.csv')
cwd = os.getcwd()$ cwd
print example1_df.printSchema()
from sklearn.metrics import r2_score$ r2_score(y_test, pred)$
reviews = np.array(tf.review)$ reviews_vector = vectorizer.transform(reviews)$ predictions = clf.predict(reviews_vector)$
y.head()
new_model = gensim.models.Word2Vec.load('lesk2vec')
data_set.to_csv("hiv_antibody_found.csv", index=False, encoding='utf-8')$
listings.loc[0]$
with open('./data/model/age_prediction_sk.pkl', 'wb') as picklefile:$     pickle.dump(grid, picklefile)
days = ['day1', 'day2']$ nocol=pd.DataFrame(index=days)$ nocol
grid.best_params_
Top_tweets = data.drop(['text','text_lower'],axis=1)$ Top_tweets.head()
groupedNews = sentiments_df.groupby(["Target"], as_index=False)$
bp = USvideos.boxplot(column='like_ratio', by='category_name', vert = False)$
cm = our_nb_classifier.conf_matrix(our_nb_classifier.data_results['out_y'],$                             our_nb_classifier.data_results['out_n'])$ print(ascii_confusion_matrix(cm))
url = "https://space-facts.com/mars/"$ table = pd.read_html(url)$ print(table)
testdata = np.load(outputFile)$ data = pd.read_csv(inputFile, delimiter=',', header=None)
column_list2 = ['Temperature','DewPoint']$ df[column_list2].plot()$ plt.show()
dts = list(map(pd.to_datetime, observation_dates))$ dts
ax = trump_histogram(trump_df)$ plt.show()
year_with_most_commits=commits_per_year[commits_per_year == commits_per_year.max()].sort_values(by='num_commits').head(1).reset_index()['timestamp'].dt.year$ year_with_most_commits='2016'$ print(year_with_most_commits)
p_val = (p_diffs > act_diff).mean()$ print("Proportion greater than actual difference: %.4f" %p_val)
screen_name = 'cdvel'$ with open(os.path.join(os.getcwd(),"data/credentials.json")) as data_file:    $     key = json.load(data_file)$
cleanedData[cleanedData['text'].str.contains("&amp;")].text
datetime.strptime('2016-03-28','%Y-%m-%d') < datetime.strptime('2016-03-29','%Y-%m-%d')$ print(datetime.strptime('2016-03-28','%Y-%m-%d'))
dfEtiquetas.dropna(subset=["created_time"], inplace=True)
j = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&start_date=2014-01-01&end_date=2014-01-02&api_key=' + API_KEY)$
bob_shopping_cart = pd.DataFrame(items, columns=['Bob'])$ bob_shopping_cart
pernan = 0.80$ nbart_allsensors = nbart_allsensors.dropna('time',  thresh = int(pernan*len(nbart_allsensors.x)*len(nbart_allsensors.y)))
from src.image_manager import ImageManager$
CON = CON.sort_values(by = ['Contact_ID','OppName'])
BUMatrix.plot(x='eventOccurredDate', y=['event_Observation','event_Incident'],style=".",figsize=(15,15))$ plt.show()$
pm_data.dropna(inplace = True)$ pm_data.shape
cells_file = 'network/recurrent_network/node_types.csv'$ rates_file = configure['output']['rates_file']$ plot_rates_popnet(cells_file,rates_file,model_keys='pop_name')
env = gym.make('MountainCar-v0')$ env.seed(505);
df_p3.loc[df_p3["CustID"].isin([customer])]
cityID = '0c2e6999105f8070'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anaheim.append(tweet)  
coarse_fuel_mgxs = coarse_mgxs_lib.get_mgxs(fuel_cell, 'nu-fission')$ coarse_fuel_mgxs.get_pandas_dataframe()
%config InlineBackend.figure_format='svg'$ plt.plot(x, np.sin(x)/x)
print(re.match('AA', 'AAbc'))$ print(re.match('AA', 'bcAA'))
portfolio_df = pd.read_excel('Sample stocks acquisition dates_costs.xlsx', sheetname='Sample')$ portfolio_df.head(10)
df2.drop(df2.index[2893])$ df2.user_id.duplicated().sum()
bd.reset_index()
brand_names = np.unique(np.asarray(data.brand[:]))$ model_names = np.unique(np.asarray(data.model[:]))
pre_name=list(twitter[twitter.name==twitter.name.str.lower()].name)$ for item in pre_name:$     t1['name'].replace(item, 'None', inplace=True)
df.loc[0:2, ['A', 'C']]
SCR_PLANS_df = USER_PLANS_df.drop(free_mo_churns)
data.fillna(0).describe()
engine=create_engine(seng)$ data['Actor Name'] = pd.read_sql_query('select UPPER(concat(first_name," ", last_name))  from actor', engine)$ data
resdf=resdf.drop(['Unnamed: 0'], axis=1)$ resdf.head(3)$
raw_data.head()
df.mean() # same as df.mean(0)
df['word_count'] = df['body'].apply(lambda x: len(str(x).split(" ")))$ df[['body','word_count']].head()
auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret)$ api = tweepy.API(auth)
foxnews_df = constructDF("@FoxNews")$ display(constructDF("@FoxNews").head())
df = pd.read_csv('Pro3ExpFinal0602.csv',index_col ='Unnamed: 0' , engine='python')
df_cod2 = df_cod.copy()$ df_cod2 = df_cod2.dropna()
msft.dtypes
for url in soup.find_all('loc'):$     print (url.text)
logit2_countries = sm.Logit(newset['converted'], $                            newset[['ab_page', 'country_UK', 'country_US', 'intercept']])$ result_final = logit2_countries.fit()
df_new[['US','UK']] = pd.get_dummies(df_new['country'])[['US','UK']]$ df_new.tail()
url = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=xQWdR8uvz6QJeQ6gqN5g&start_date=2017-01-01&end_date=2017-12-31')$ json_data = url.json()$ data = json.dumps(json_data['dataset'])$
dailyplay = pd.read_excel('data.xlsx', sheetname = 'dailyplay')$ promotions = pd.read_excel('data.xlsx', sheetname = 'promotions')$
cityID = '3df0e3eb1e91170b'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Columbus.append(tweet) 
engine.execute('SELECT * FROM station LIMIT 10').fetchall()
sum(contractor.address1.isnull())
df2.info()
poparr2.shape
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY\$ +'&start_date=2018-05-18&end_date=2018-05-18'$ r = requests.get(url)
filepath = os.path.join('input', 'input_plant-list_SE.csv')$ data_SE = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_SE.head()
df3.index
psy = pd.get_dummies(psy, columns = cat_vars, drop_first=False)
df['pb_prod'] = df['pledged'] * df['backers']$ df['pb_avg'] = df[['pledged', 'backers']].mean(axis=1)
from bs4 import BeautifulSoup$ import requests$ url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
print data.describe()
top_10_authors = git_log['author'].value_counts().iloc[:10]$ print(top_10_authors)
df[df.client_event_time >= datetime.datetime(2018,4,2,0,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
new_df = df.dropna()$ new_df
daily_df.reset_index(inplace=True)
url=("/Users/maggiewest/Projects/detroit_census.csv")$ detroit_census = pd.read_csv(url)
df_writer_combined_text['combined_text_tokenized'] = df_writer_combined_text['combined_text'].apply(lambda x: nlp(x))$
gene_df.sort_values('length').head()
data.loc[:, ['TMAX']].head()
df_date_precipitation.describe()
pd.concat([df1,df2,df3])$
forcast_out=int(math.ceil(.01*len(df1)))$ forcast_out$
(trainingData, testData) = data.randomSplit([0.7, 0.3])$
high = max(v.Open for v in data.values() )$ low = min(v.Open for v in data.values())$ print('=>The high and low opening prices for this stock in 2017 were {:.2f} and {:.2f} '.format(high, low) + 'respectively.')
tmax_day_2018 = xr.open_dataset('/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/maximum temperature/tmax.2018.nc', decode_times = False)
players=df.groupby('Player').agg({'Pts':np.average,'REB':np.average,'AST':np.average})$ top_five=players.nlargest(5,'Pts').index.tolist()$ players.nlargest(5,'Pts')
json = r1.json()$ print(type(json)) #result is dictionary. yay!$ print(json)
val_pred_svm = lin_svc_clf.predict(X_valid_cont_doc)
results=pd.read_csv("results.csv")$ results.tail()$
fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)$ toma.iloc[::20].plot(ax=ax, logy=True, ms=5, style=['.', '.', '.', '.', '.', '.'])$ ax.set_ylabel('Relative error')$
calls = pd.read_csv("311_Calls__2012-Present_.csv")
precip_df = pd.DataFrame(precip)$ date_precip_df = precip_df.set_index("date")$ date_precip_df.head()$
for names in asdf:$     name = names.div['data-name']$ name
elon['nlp_text'] = elon.text.apply(lambda x: tokenizer.tokenize(x.lower()))$ elon.nlp_text = elon.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])$ elon.nlp_text = elon.nlp_text.apply(lambda x: ' '.join(x))
train = pd.read_csv('../../input/preprocessed_data/trainW-0.csv') #沒有重複的user$ train.info()
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])$ df_new.head()
for c in ccc[:2]:$     spp[c] /= spp[c].max()
station_count = session.query(Station).count()$ station_count
post_gen_paired_cameras_missing_from_shopify_orders = np.setdiff1d(BPAIRED_GEN['shopify_order_id'],ORDERS_GEN['order_number'].astype(str))$ np.array_equal(post_gen_paired_cameras_missing_from_shopify_orders,post_gen_paired_cameras_missing_from_join)
auth = tweepy.OAuthHandler(consumerKey, consumerSecret)$ api = tweepy.API(auth)
from pyspark.sql.functions import *$ display(flight2.select(max("start_date")).show())$ display(flight2.select(min("start_date")).show())
all_tables_df.loc[[2, 3, 4, 10, 2000], ['OBJECT_NAME', 'OBJECT_TYPE']]
top_10_authors = git_log.author.value_counts().nlargest(10)$ top_10_authors
tweet_df = tweet_df.sort_values(['source', 'created_at'], ascending=False).reset_index()$ del tweet_df['index']$ tweet_df.head()
rdf.loc[pd.isna(rdf.accident_counts),'accident_counts'] = 0.0
response_raw = requests.get(url,params)$ response_clean = response_raw.content.decode('utf-8')
news_sentiments = pd.DataFrame.from_dict(sentiments)$ news_sentiments.head()
now = pd.Timestamp('now')$ now, now.tz is None
with open('../data/rockville_unclean.json') as f:$     data = json.load(f)$     $
dictionary=json.loads(json_string) #Convert string to dictionary
max_retweet = DataSet['tweetRetweetCt'].max()$ max_retweet
df['sin_day_of_year'] = np.sin(2*np.pi*df.day_of_year/365)$ df['cos_day_of_year'] = np.cos(2*np.pi*df.day_of_year/365)
stations_df.to_csv('clean_hawaii_stations.csv', index=False)
sample_weight = np.array([1 if not p else 1.25 for p in joined_train.perishable])
df['y'].plot.hist()
pd.set_option('max_colwidth', 100) #Its nice to see all columns
parameter = '00060'$ Shoal_Ck_15min = pull_nwis_data(parameter=parameter, site_number='08156800', start_date='2018-02-01', end_date='2018-02-18', site_name='08156800')$ Shoal_Ck_15min.head(10)$
data.groupby(['Year', 'Department'])['Salary'].sum()
inter = est.get_prediction(X_tab.astype(float))$ inter = inter.summary_frame(alpha=0.05)[['mean_ci_lower','mean_ci_upper']]$ inter.rename(columns={ 'mean_ci_lower':'CI lower', 'mean_ci_upper':'CI upper'})
data.drop(catFeatures, axis=1,inplace=True)
dfLikes.groupby("año").agg({"fan_count": "mean"}).astype(int)
df = pd.read_csv('basic_graphics_single_column_data.csv')
session = tf.Session()$ session.run(tf.global_variables_initializer())
plt.scatter(dataframe['lat'], dataframe['lon'],c = dataframe['label'],cmap=plt.cm.Paired)$ plt.scatter(center['lat'],center['lon'],marker='s')$ plt.show()
df_geo_segments.crs = {'init': 'epsg:4326'}
popular_programs = challange_1["program_code"].value_counts()$ popular_programs
all_tables_df.OBJECT_NAME
df["DATE_TIME"] = pd.to_datetime(df.DATE + " " + df.TIME, format="%m/%d/%Y %H:%M:%S")$ df.head(5)
no3Mask = nitrogen['DetectionQuantitationLimitMeasure/MeasureUnitCode'] == 'mg/l NO3'$ nitrogen.loc[no3Mask,'TotalN'] = nitrogen['TotalN'] * 0.2259
all_turnstiles.head()
Largest_change_in_one_day = (mydata['High'] - mydata['Low']).max()$ Largest_change_in_one_day
train_data, test_data, train_labels, test_labels = train_test_split(spmat, y_data, test_size=0.10, random_state=42)  
!curl -L -O  https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt
X = dfX[fNames].values$ y = dfY.values
dfLikes.dropna(subset=["created_time"], inplace=True)
print(data.learner_id.nunique())
containers[0].find("span",{"class":"date"}).contents[0]
df['intercept']=1$ df[['control', 'treatment']] = pd.get_dummies(df['group'])$
news_df = (news_df.groupby(['topic'])$               .filter(lambda x: len(x) >= 25))
bc['newdate'] = pd.DatetimeIndex(bc.date).normalize()
print(temp_nc)$ for v in temp_nc.variables:$     print(temp_nc.variables[v])
cohort_retention_df.fillna(0,inplace=True)
expected_index = pd.DatetimeIndex(start=raw.index[0], end=raw.index[-1], freq='1H')$ ideal_index_df = pd.DataFrame(index=expected_index)$ clean = pd.concat([ideal_index_df, raw], axis=1)$
views = containers[0].find("li", {"class":"views"}).contents[1][0:3]$ views
df_concat.drop(df_concat.columns[[0,1,2]], axis=1, inplace= True)
type(df_master.tweet_id[1])
logging.info('Finishing dataset')$ df_final['Close: t'] = df_final['Close: t'].shift(1)$ df_final = df_final[2:-15]
merged1.drop('Id', axis=1, inplace=True)
calls.head()
if 1 == 1:$     ind_shed_word_dict = pd.read_pickle(config.IND_SHED_WORD_DICT_PKL)$     print(ind_shed_word_dict.values())
authors = Query(git_index).get_cardinality("author_name").by_period()$ print(get_timeseries(authors, dataframe=True).tail())
feature_cols = ['TV', 'radio', 'newspaper']$ X = data[feature_cols]$ y = data.sales
with open(output_folder+Company_Name+"_Forecast_"+datetime.datetime.today().strftime("%m_%d_%Y")+".pkl",'wb') as fp:$     pickle.dump(final_data,fp)
to_week = lambda x: x.dt.week
tree_features_df[~(tree_features_df['p_hash'].isin(manager.image_df['p_hash']) | tree_features_df['filename'].isin(manager.image_df['filename']))]
file = open('chinese_stopwords_2.txt','rb').read().decode('utf8').split('\n')$ stop_words = list(set(file))
birth_dates.set_index("BirthDate_dt").loc["2014-01-01":,:]
gdf = gdf.copy()$ gdf['length'] = gdf['end'] - gdf['start'] + 1$ gdf.head()
engine = create_engine('sqlite:///hawaii.sqlite')
compound_df.columns = target_terms$ compound_df.head()
f = {'total_cost':['sum','count'], 'date_of_birth':['first']}$ total_spending = df.groupby(['uid'])['total_cost','date_of_birth'].agg(f)$
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1&api_key='API_KEY$ r = requests.get(url)
hourly_df = pd.concat([twitter_delimited_hourly, stock_delimited_hourly], axis=1, join='inner')$ hourly_df.reset_index(inplace=True)$ hourly_df.head()
grouped_df = xml_in.groupby(['authorId', 'authorName'], as_index=False)['publicationKey'].agg({'countPublications': 'count'})
grouped.get_group('MSFT')
df2.user_id.duplicated().sum()
time_length.plot(figsize = (16, 4), color = 'r')$ time_fav.plot(figsize = (16,4), color = 'g')$
DataSet.head(2)
cur.execute("SELECT * FROM pgsdwh.pgsparts.geps_distinct_shipping_lines_dw_t")
df.head()
tlen.plot(figsize=(16,4), color='r');$
%matplotlib inline$ from IPython.display import Image$ from IPython.core.display import HTML
bg2 = pd.read_csv('Libre2018-01-03.txt') # when saved locally$ print(bg2)$ type(bg2) # at bottom we see it's a DataFrame$
avg_order_intervals = np.fromiter(result.values(), dtype=float)$ avg_order_intervals.size
df.iloc[1] # select row by integer location 1$
new_stops = merged_stops[['stopid', 'stop_name', 'lat', 'lng', 'routes']]$ new_stops = new_stops.rename(columns={'stop_name': 'address'})$ new_stops.head(5)
import pandas as pd$ pd.set_option('display.max_columns', None)
temps_df.Missoula
aTL[['system.record_id','system.created_at','APP_LOGIN_ID','APP_PRODUCT_TYPE','BRANCH','AREA_MANAGER','REGIONAL_MANAGER']].to_excel(cwd + '\\CA TL 0521-0526.xlsx', index=False)$ aSL[['system.record_id','system.created_at','APP_LOGIN_ID','APP_PRODUCT_TYPE','BRANCH','AREA_MANAGER','REGIONAL_MANAGER']].to_excel(cwd + '\\CA SL 0521-0526.xlsx', index=False)
dr = dr.resample('W-MON').sum()$ RNPA = RNPA.resample('W-MON').sum()$ ther = ther.resample('W-MON').sum()
data[(data.phylum.str.endswith('bacteria')) & (data.value>1000)]
git_log['timestamp']=pd.to_datetime(git_log['timestamp'], unit='s')$ print(git_log['timestamp'].describe())
yxe_tweets = pop_tweets('stoonTweets.json')
station_availability_df['avail_docks'].plot(kind='hist', rot=70, logy=True)
df_providers.head()$ idx = df_providers[ (df_providers['id_num']==50030)].index.tolist()$ df_providers.loc[idx[:3],:]$
prcp = Base.classes.prcp$ tobs = Base.classes.tobs$ stations = Base.classes.stations
alldata = pd.merge(data_FCBridge, data_FCInspevnt_latest, on='brkey')$ alldata.loc[alldata['bridge_id'] == 'LMY-S-FOSL']$
df = pd.read_csv('../Data/bidsmerged_update__2_.csv') $ df.shape$
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth)
filtered_df = data_df.drop(labels=['id', 'fst_subject', 'fst_tutor_type'], axis=1)
data.iloc[:,10:14] = data.iloc[:,10:14].fillna("0")  # Overall_Credit_Status, Delivery_Block, Billing_Block, Block_flag$ data.iloc[:,26] = data.iloc[:,26].fillna("0")   # state$ data.dropna(how='any',axis='rows',inplace=True) # district
grouper = dta.groupby(dta.results)
[k for val in train_x[0] for k,v in words.items() if v==val]
template_df = pd.concat([X_valid, y_valid], axis=1)$ template_df['is_test'] = np.repeat(True, template_df.shape[0])
! rm -rf recs2$ ! mrec_predict --input_format tsv --test_input_format tsv --train "splits1/u.data.train.*" --modeldir models2 --outdir recs2
theta = np.minimum(1,np.maximum(0,np.random.normal(loc=0.75, scale=0.1, size=(10000,5))))
movies['year']=movies['title'].str.extract('(\(\d\d\d\d\)$)',expand=True)$ movies['year'] = movies['year'].str.replace('(', '')$ movies['year'] = movies['year'].str.replace(')', '')
%matplotlib inline$ sns.violinplot(data=october, inner="box", orient = "h", bw=.03)$
from sklearn.model_selection import train_test_split $ X_train,X_test,y_train,y_test = train_test_split(X,y, random_state=42)
df.index = df['Date']
dfCountry = pd.read_csv('countries.csv')$ dfCountry.head()
from IPython.core.interactiveshell import InteractiveShell$ InteractiveShell.ast_node_interactivity = "all"
logodds.drop_duplicates().sort_values(by=['count']).head(10)$ logodds.drop_duplicates().sort_values(by=['count']).tail(10)$
import pandas as pd$ y= pd.Period('2016')$ y
print(data.petal_length.std(),$       data.petal_length.var(),$       data.petal_length.sem())
print("Age:-",result_set.age,"workclass:-",result_set.workclass)
typesub2017 = typesub2017.rename(index=str, columns={"Solar  - Actual Aggregated [MW]": "Solar", "Wind Offshore  - Actual Aggregated [MW]": "Wind Offshore", "Wind Onshore  - Actual Aggregated [MW]" : "Wind Onshore" })$ typesub2017.head()
intervention_history.sort_values(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
pd.set_option('display.mpl_style', 'default')
dates_D = load_dates('../data/worldnews_2016_10-2017_9_submissiondates.txt')$
df = pickle.load(open( "lyincomey.p", "rb" ))
df.describe()
from gensim import corpora, models$ dictionary = corpora.Dictionary(sws_removed_all_tweets)
la_inspections = pd.read_csv('/Users/salilketkar/thinkful/LA_health_score_inspections.csv')
pd.concat([msftAV[:5], aaplAV[:3]], axis=1, keys=['MSFT', 'AAPL'])
combined_df[combined_df['classifier_summary'].isnull() == True]
btc.describe()
df.drop(bad_indices, inplace=True)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
val = val.split(b'\r\n\r\n',1)[1]$ print(val.decode('utf-8'))
fan_zip = [zipcode.isequal(str(zc)) for zc in questions['zipcode']]$ questions['zipcode'] = fan_zip
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}]$ new_store = pd.DataFrame(new_items, index=['store 3'])$ new_store
print(festivals.dtypes)$ print(festivals.info())$
ab_df2 = ab_df2.drop(labels=2893)
model = gensim.models.Word2Vec(sentences, workers=4)
%matplotlib notebook$ plt.style.use('seaborn-paper')$ mpl.rcParams['figure.facecolor'] = (0.8, 0.8, 0.8, 1)
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-07-11&end_date=2018-07-11&api_key={}".format(API_KEY), $                  auth=('user', 'pass'))
ldf['Traded Volume'].median()
cruise_data = pd.read_table(data_file, delim_whitespace=True, header=None, skiprows=1)$ cruise_data = cruise_data.rename( columns={ 0:'Pressure', 1:'Temperature', 13:'Salinity' } )$ cruise_data[0:5]
df = pd.read_csv('artist-pitch.csv')$ df.head()
rain = session.query(Measurements.date, Measurements.prcp).\$     filter(Measurements.date > last_year).\$     order_by(Measurements.date).all()
pp.pprint(r.json())
name =contractor.groupby('contractor_bus_name')['contractor_number'].nunique() $ print(name[name>1])
df = pd.DataFrame(results, columns=['date', 'precipitation'])$ df.set_index(df['date'], inplace=True)$ df.head()
os.environ['PROJECT'] = PROJECT$ os.environ['BUCKET'] = BUCKET$ os.environ['REGION'] = REGION
data4.crs= "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
df = pd.read_csv('msa.csv')
S.decision_obj.simulStart.value = "2007-07-01 00:00"$ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
pd.DataFrame(np.array([[10, 11], [20, 21]]))
p_tags = sample_soup.find_all("p")$ for p in p_tags:$     print(p.text)
prcp_df =pd.read_sql_query(prcp_data, session.bind)$ prcp_df.set_index('date',inplace=True)$ prcp_df.head()
temp_us_full = temp_nc.variables['air'][:, lat_li:lat_ui, lon_li:lon_ui]
from sklearn.model_selection import train_test_split$ x_train, x_test, y_train, y_test = train_test_split($     data, targets, test_size=0.25, random_state=23)
datetime.now().toordinal()/365$
data.name.duplicated(keep=False).sum()
last = session.query(Measurement.date).order_by(Measurement.date.desc()).first()[0]$ last_date = datetime.strptime(last, '%Y-%m-%d')$ year_ago = last_date - dt.timedelta(days=365)
df.isnull().sum()
df["Diff"] = df["Close"] - df["Open"]$ df["Percent_Diff"] = (df["Diff"]/df["Open"])*100$ df.head()
betas_mask = np.zeros(shape=(mcmc_iters, n_bandits))$ betas_mask[np.arange(mcmc_iters), betas_argmax] = 1
impact_effort = pd.read_csv('impact_effort.csv')
stars = dataset.groupby('rating').mean()$ stars.corr()
sns.set_style('whitegrid')$ sns.distplot(data_final['countCollaborators'], kde=False,color="red", bins=25) $
df_notnew = df.query('landing_page != "new_page"')$ df_3 = df_notnew.query('group == "treatment"')$ df_3.nunique()$
print("State space:", env.observation_space)$ print("- low:", env.observation_space.low)$ print("- high:", env.observation_space.high)
Base.prepare(engine, reflect=True)
containers[0].find("div",{"class":"key"}).a['title'].split()[1].replace(',',"")
df.Date = pd.to_datetime(df.Date_Hour)$ df = df.set_index('Date_Hour').sort_index(ascending=True)
rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]$
graf['DETAILS2']=graf['DETAILS'].progress_apply(text_process)
joined['Retire'].dtypes
! rm -rf ~/s3/comb/flight_v1_0.pq
df.to_csv('dataframe-kevin.csv')$
combined_df4['split_llpg1']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(str(x).split(',')[1:2]))$ combined_df4['split_llpg2']=combined_df4['llpg_usage'].apply(lambda x: '-'.join(str(x).split(',')[1:3]))$ combined_df4.head()
plt = r6s.num_comments.hist(bins = 1000)$ plt.set_xlim(0,50)
data.to_json('nytimes_oped_articles.json')
df = pd.DataFrame(x_9d)$ df = df[[0,1,2]] # only want to visualise relationships between first 3 projections$ df['X_cluster'] = X_clustered
model.most_similar("man")$
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv"$ df  = pd.read_csv(path, sep =',') # df  = pd.read_csv(path) may be too. sep =',' is by deffect.$ df.head(5)
cityID = 'a307591cd0413588'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Buffalo.append(tweet) 
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])$ display(data.head(10))
df.loc[:50,'name'].value_counts()
df_estimates_false = df_estimates_false.dropna(axis=0, subset=['points'])$ print(df_estimates_false)$
s2 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])$ s2
citydata_nbr_rides_renamed = citydata_with_nbr_rides.rename(columns={"ride_id": "nbr_rides"})$ citydata_work = citydata_nbr_rides_renamed[['city', 'driver_count', 'type', 'average_fare', 'nbr_rides']]$
country_with_least_expectancy = le_data.idxmin(axis=0)$ country_with_least_expectancy
foursquare_data_dict['response'].items()[2][1][0]
u235_scatter_xs = fuel_xs.get_values(nuclides=['(U235 / total)'], $                                 scores=['(scatter / flux)'])$ print(u235_scatter_xs)
s_filled.plot()$ plt.show()
tweet_df = tweet_df[['tweet', 'source', 'created_at', 'compound', 'positive', 'neutral', 'negative']]$ tweet_df.head()
new_page_converted.mean()-old_page_converted.mean()
x = store_items.isnull().sum()$ print(x)
calls_nocontact.zip_code.value_counts()
my_gempro.get_dssp_annotations()
set(hourly_dat.columns) - set(out_temp_columns+incremental_precip_columns+general_data_columns+ wind_dir_columns)
number_of_commits = git_log['timestamp'].count()$ number_of_authors = git_log['author'].nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
assert (ebola >= 0).all().all()
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
result_df, total_count = search_scopus(key, 'TITLE-ABS-KEY(gender in science)')$ from IPython.display import display, HTML$ display(result_df)$
print(df['Confidence'].nunique())
elms_all_0611.iloc[1048575:].to_excel(cwd+'\\ELMS-DE backup\\elms_all_0611_part2.xlsx', index=False)
with open(content_analysis_save, mode='w', encoding='utf-8') as f:$     f.write(wikiContentRequest.text)
y_train=bow["label"]$ X_train=bow.drop({"ID","label"},axis=1)$
from textblob import TextBlob$ from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
df.drop(["urlname"], axis = 1, inplace=True)$ top_rsvp.drop(["urlname"], axis = 1, inplace=True)$ top_rsvp.head(5)
$hadoop fs -put /data/tg_cg18_bigdata/rc_2018_02.csv /user/sohom/$
precipitation_totals.describe()
INT['Create_Date']= pd.to_datetime(INT.Create_Date)
prcp_df.describe()
soup = BeautifulSoup(response.text, 'html.parser')
print("Action space:", env.action_space)$ print("Action space samples:")$ print(np.array([env.action_space.sample() for i in range(10)]))
!wget https://download.pytorch.org/tutorial/faces.zip$ !unzip faces.zip
m.__repr__()$
df.count()
from pyspark.sql.functions import regexp_extract, col$
result = dta.groupby((dta.results, dta.dba_name)).size()
df.plot(kind='scatter', x='RT', y='fav', xlim=[0,300], ylim=[0,300], title="Favorites and Retweets:300x300")$
df_merge.head()
sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[(sprintsWithStoriesAndEpics_df['Sprint Age In Days'] > 3) | (sprintsWithStoriesAndEpics_df['Age In Days'] > 3)]$ sprintsWithStoriesAndEpics_df[['key_story', 'Team_story', 'fixVersions_story', 'summary_story', 'status_story', 'Age In Days', 'Sprint Age In Days', 'Open Set To Date']].sort_values(by=['Age In Days', 'Sprint Age In Days'], ascending = False)
stn_rainfall = session.query(Measurement.station, func.sum(Measurement.prcp)).filter(Measurement.date >= data_oneyear).group_by(Measurement.station).order_by(func.sum(Measurement.prcp).desc()).all()$ stn_rainfall$
from sklearn.feature_extraction.text import TfidfVectorizer$ from sklearn.cluster import KMeans$ from sklearn.metrics import silhouette_samples, silhouette_score
! mrec_tune -d 'splits1/u.data.train.0' --input_format tsv --l1_min 0.001 --l1_max 1.0 --l2_min 0.0001 --l2_max 1 --max_sims 200 --min_sims 1 --max_sparse 0.3
crimes.tail()
df.shape$ df.tail()
def load_train_pp(data_path=DATA_PATH,ssize):$     train_pp_path = os.path.join(data_path, "train_pp.csv")$     return pd.read_csv(train_pp_path,nrows=ssize)
engine = create_engine("sqlite:///dropna_hawaii.sqlite")
tickers = portfolio_df['Ticker'].unique()$ tickers
labeled_news[0].describe()
for index, row in susp.iterrows():$     print(row.link)
dict_1=request.json()$ dict_1.get('dataset_data')
plt.axis('equal')$ plt.plot(  np.nancumsum(tab.delta_long), np.nancumsum(tab.delta_lat))$ plt.show()$
df['2018-05-21'] # Try to understand the error ... Its searching in columns !!! $
sns.distplot(questions_scores[:int(len(questions_scores)*0.99)])
tm_2020 = pd.read_csv('input/data/trans_2020_m.csv', encoding='utf8', index_col=0)
save_filepath = os.path.expanduser("~")
cityID = '8e9665cec9370f0f'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Minneapolis.append(tweet) 
df.head()
rural_summary_table = pd.DataFrame({"Average Fare": rural_avg_fare, $                                    "Total Rides": rural_ride_total})$ rural_summary_table.head()
state = environment.reset()$ state, reward, done=environment.execute(env.action_space.sample())$ state.shape
data = pd.read_excel("D:\ML\AnomalyDetection\datasetFun_cleaned.xls")$
punct_re = '[^\w\s]'$ trump['no_punc'] = trump['text'].str.replace(punct_re, " ")$
df_survival_by_donor = df_survival[['Donor ID', 'Donation Received Date']].groupby('Donor ID')$ mean_time_diff = df_survival_by_donor.apply(lambda df: df['Donation Received Date'].diff().mean())$
df[y_col] = df['variety'].str.lower().replace(repl_dir)$ df_noblends = df[df[y_col].replace(repl_dir).str.lower().isin(keep_vars)]$ df_noblends[y_col].unique().size
appleNegs.groupby('group_id').tweet_id.count().reset_index()$
people_with_one_or_zero_collab['authorId'].nunique()
df.head(10)
data_store_id_relation.count()
apple.resample('M').mean().plot(grid=True)
result_final.summary2()
nasa_url = 'https://mars.nasa.gov/news/'$ jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'
readRenviron("~/.Renviron")
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])$ from scipy.stats import norm$ z_score,1-p_value/2$
elements.head(5)
df_estimates_false.groupby('metric').count()$
grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)$ grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')$ grid.add_legend()
sns.pairplot(data, hue='species', size=3)
df_cities = pd.read_csv('cities.csv')$ df_cities
siteIDs = dfSubSites['MonitoringLocationIdentifier'].unique().tolist()$ siteIDs
mod_model.scen2xls(version=None)
mta_avg = dfs_morning.groupby(by = 'STATION').ENTRIES_MORNING.mean().reset_index()$ mta_census = mta_avg.merge(census_zip, left_on=['STATION'], right_on=['station'], how='left')$ mta_census.drop('station', axis = 1, inplace = True)
active_station = session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all()$ active_station
dfQ1['flow_MGD'] = dfQ1['meanflow_cfs'] * 0.64631688969744$ dfQ2['flow_MGD'] = dfQ2['meanflow_cfs'] * 0.64631688969744$ dfQ3['flow_MGD'] = dfQ3['meanflow_cfs'] * 0.64631688969744
stc.checkpoint("checkpoint10")
result = api.search(q='%23arena') $ len(result)
%%time$ tcga_target_gtex_expression_hugo_tpm = tcga_target_gtex_expression_hugo \$     .apply(np.exp2).subtract(0.001).groupby(level=0).aggregate(np.sum).add(0.001).apply(np.log2)
pp.pprint(tweet._json)
url = "https://mars.nasa.gov/news/"$ response = requests.get(url)
a =R16df.rename({'Create_Date': 'Count-2016'}, axis = 'columns')$
conf = SparkConf().setAll([('spark.executor.memory', '6g'), ('spark.executor.cores', '6'), ('spark.cores.max', '6'), ('spark.sql.session.timeZone', 'UTC')])$ sc = SparkContext("local", "llite", conf=conf)
f.index = [1,3,2,1,5] ; f
df['is_rank_1'] = False$ df.loc[df['Rank'] == 1, 'is_rank_1'] = True
! rm -rf ../../results/mrec-4-2 && mkdir -p ../../results/mrec-4-2$ ! cp -R tmp/* ../../results/mrec-4-2
output= "Delete from user where user_id='@Pratik'"$ cursor.execute(output)$
header = cylData.first()$ cylHPData= cylData.filter(lambda line: line != header)$ print (cylHPData.collect())
station_df = pd.read_csv(station, encoding="iso-8859-1", low_memory=False)$ station_df.head()
dat_before_fill=dat.copy()$ for temp_col in temp_columns:$     dat.loc[:,temp_col]=dat[temp_col].interpolate(method='linear', limit=3)
ground_alt = merged_data['rtk_alt'].mode()[0]$ ground_alt
c_date = scratch['created_at']$ c_date.shape
Y = np.ones((num_names,1))$ Y[df['Gender'] == 'F',0] = 0
df.iloc[0]
import matplotlib.cm as cm$ dots_c, vhlines_c, *_ = cm.Paired.colors
URL = "http://www.reddit.com/hot.json"$ res = requests.get(URL, headers = {'User-agent':'Caitlin Bot 0.1'})
prcp_df = pd.DataFrame(prcp_in_last_year, columns=['date', 'prcp'])$ prcp_df.head(11)
n_new = df2.query(('landing_page == "new_page"')).count()[0]$ n_new
yc_new3.describe()
result = pd.concat([df1, df3], axis = 0) # concatenate one dataframe on another along rows$ result
LabelsReviewedByDate = wrangled_issues_df.groupby(['created_at','DetectionPhase']).created_at.count()$ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
set(bookings.columns).intersection(set(releases.columns))
! rm -rf models2$ ! mrec_train -n4 --input_format tsv --train "splits1/u.data.train.*" --outdir models2 --model=knn
len(df.user_id.unique())$
avg_monthly = np.mean(df.month.value_counts())$ std_monthly = np.std(df.month.value_counts())$ print('The average beers drank per month is {:.2f} beers and the standard deviation is {:.2f} beers.'.format(avg_monthly, std_monthly))
so.loc[(so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')]
weekly_videosGB = weekly_dataframe(nodesGB)$ weekly_videosGB[0].head()
df.to_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext.csv", sep=',')
result = cur.fetchall()$
df.game_type.value_counts()
t1.info()
comments = pd.concat([comments_df, comments_df_middle])$ comments = comments.drop_duplicates()$ comments.to_csv('seekingalpha_top_comments.csv')$
bldg_data_0 = bldg_data[bldg_data['255_elec_use']==0]$ bldg_data_0.groupby([bldg_data_0.index.year,bldg_data_0.index.month]).agg('count').head(100)$
response = requests.get(nasa_url)$ print(response.text)
ccc = td.columns
input_shape_param = (X_train.shape[1],X_train.shape[2])$ input_shape_param
print(prec_nc)$ for v in prec_nc.variables:$     print(prec_nc.variables[v])
mylist = session.query(measurement.date, measurement.prcp).filter(measurement.date.between('2016-08-23', '2017-08-23')).all() $ 
files = [name for name in listdir() if 'csv' in name]$ fileName = files[0]$ fileName = '866192035974276_AXIO.csv'
data.count(axis=0)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyzer = SentimentIntensityAnalyzer()
validation.analysis(observation_data, Simple_resistance_simulation)
print (test.shape)
from gensim.models import Doc2Vec$ model = Doc2Vec.load('/tmp/movie_model.doc2vec')
result = api.search(q='%23puravida') #%23 is used to specify '#'$ len(result)
urls_to_shorten = [link for link in urls if ux.is_short(link)]$ urls_to_shorten
average_of_averages = df['average'].mean()
response = requests.get('https://www.purdueexponent.org/search/?f=html&q=gun+control&d1=2018-02-14&d2=2018-06-25&sd=desc&l=100&t=article%2Ccollection&nsa=eedition')$ soupresults1 = BeautifulSoup(response.text,'lxml')$
eth_market_info.drop(['Date'],inplace=True,axis=1)$ scaler_eth = MinMaxScaler(feature_range=(0, 1))$ scaled_eth = scaler_eth.fit_transform(eth_market_info)$
df.describe()
health_data_row.loc[2013, 1, 'Guido']  # index triplet
orig_ct = len(dfd)$ dfd = dfd.query('hspf >= 10.0 and hspf <= 18')$ print(len(dfd) - orig_ct, 'eliminated')
filtered_df[filtered_df['M_pay_3d'] == 0].shape[0]
closed_issue_age = Issues(github_index).is_closed()\$                                        .fetch_results_from_source('time_to_close_days', 'id_in_repo', dataframe=True)$ print(closed_issue_age.head())
USvideos.describe()$ USvideos.head()
twitter_df = pd.DataFrame(results_dict)$ twitter_df.head()
columns_chosen = ['Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close','Adj. Volume']$ df = df[columns_chosen]
gdf = gdf[gdf['seqid'].isin(chromosomes_list)]$ gdf.drop(['start', 'end', 'score', 'strand', 'phase', 'attributes'], axis=1, inplace=True)$ gdf.sort_values('length').iloc[::-1]
y = df['comments']$ X = df[['title', 'age', 'subreddit']].copy(deep=True)
pivoted.T[labels == 0].T.plot(legend=False, alpha=0.1)
d['pasttweets_text']=d['pasttweets'].apply(lambda x: ', '.join(x))
results.to_csv("mb.csv")$ results.to_pickle("football_pickle")$ pd.read_pickle("football_pickle")
max_div_stock=df.iloc[df["Dividend Yield"].idxmax()]$ max_div_stock$ print("The stock with the max dividend yield is %s with yield %s" % (max_div_stock['Company Name'],max_div_stock['Dividend Yield']))
all_features = pd.get_dummies(all_features, dummy_na=True) $ all_features.shape
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer$ analyze = SentimentIntensityAnalyzer()
free_sub = free_data.loc[:,['country','y']]
ab_df2.query('group == "control"')['converted'].mean()
response = requests.get(url)$ soup = BeautifulSoup(response.text, 'html.parser')
nodes = nodereader.values.tolist() 
datascience_tweets[datascience_tweets["text"].str.contains("RT")==False]['text'].count() # 895
cbs_df = constructDF("@CBS")$ display(constructDF("@CBS").head())
df1 = pd.read_feather('new_sensor_data2') $ df1.head(10)
d = {'id': ids, 'sentiment': solution}$ output = pd.DataFrame(data=d)$ output.to_csv( "Word2Vec_Keras_AverageVectors.csv", index=False, quoting=3 )
df_sched2 = df_sched.iloc[:,1:].apply(pd.to_datetime,format='%Y-%m-%d')
df['cowbell'] = df['body'].apply(lambda x: len([x for x in x.split() if '(((' in x]))
trump = trump.drop(["id_str"], axis = 1)
data_compare['SA_textblob_de'] = np.array([ analize_sentiment_german(tweet) for tweet in data_compare['tweets_original'] ])$ data_compare['SA_google_translate'] = np.array([ analize_sentiment(tweet) for tweet in data_compare['tweets_translated'] ])
Base.prepare(engine, reflect=True)$ Base.classes.keys()
print()$ print(df.groupby('Class').size())
page_html= uClient.read()$ uClient.close()
df.loc[df['waiting_days']>=0]['waiting_days'].describe()
appmag_lim = 21.0$
btc = pd.read_csv('/home/rkopeinig/workspace/Time-Series-Analysis/data/btc.csv')$ btc['date'] = pd.to_datetime(btc['date'])$ btc = btc.set_index('date')
turnstiles_df = turnstiles_df.rename(columns=lambda x: x.strip())$
df = pd.DataFrame(results, columns=['date', 'precipitation'])$ df.set_index(df['date'], inplace=True)$ df.tail()
news_items = soup.find_all(class_='slide')$ news_title = news_items[0].find(class_='content_title').text$ print (news_title)
nsw_bb = Polygon([[-35.52052802079999,140.999279200001],[-28.1570199879999,140.999279200001],$                   [-28.1570199879999,159.105444163417],[-37.5052802079999,159.105444163417]])
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=a2rusGHVqV67tgCdY38x')$ r.headers['content-type']  #Type of data format queried for.  In this case, json. $ json_string=r.text  #Convert object to text so that it can be converted to a dictionary.
nu_fiss_xs = fuel_xs.get_values(scores=['(nu-fission / flux)'])$ print(nu_fiss_xs)
element = driver.find_element_by_xpath('//*[@id="comic"]/img')$ element.get_attribute("title")
ddp = dfd.dropna(axis=0, subset=['in_reply_to_screen_name'], how='any')
for h in heap:$     h.company = [t.author_id for t in h.tweets if t.author_id in names][0]
index_remove = list(ab_df[mismatch1].index) + list(ab_df[mismatch2].index)$ ab_df2 = ab_df.drop(labels=index_remove,axis=0)
deltadf.to_csv('exports/trend_deltas_chefkoch.csv')
print(df.info())
master_df['day of week']=master_df['day of week'].astype(str)
clean_stations.columns = ['station', 'name', 'latitude', 'longitude', 'elevation', 'city', 'country', 'state']
df1 = pd.DataFrame(X_selected_features)$ df1['labels'] = pd.Series(y)$ sns.pairplot(df1, hue='labels')
p_old = df2[df2['landing_page']=='old_page']['converted'].mean()$ print("Probability of conversion for old page (p_old):", p_old)
store_items = store_items.append(new_store)$ store_items
df.loc[:,['A','B']]
k = 4$ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)$ neigh
tlen = pd.Series(data = data['len'].values, index = data['Date'])$ tfav = pd.Series(data = data['Likes'].values, index = data['Date'])$ tret = pd.Series(data = data['RTs'].values, index = data['Date'])$
stat_info_st = stat_info[0].apply(fix_space_0)$ print(stat_info_st)
df['MeanFlow_cms'] = df['MeanFlow_cfs'] * 0.028316847
my_gempro.map_uniprot_to_pdb(seq_ident_cutoff=.3)$ my_gempro.df_pdb_ranking.head()
ftr_imp=zip(features,xgb_model.feature_importances_)
centroids1 = clusters_kmeans1.cluster_centers_
vals = Inspection_duplicates.index.values$ vals = list (vals)$ vals
mean = np.mean(data['len'])$ print("The lenght's average in tweets: {}".format(mean))
model_with_loss = gensim.models.Word2Vec(sentences, min_count=1, compute_loss=True, hs=0, sg=1, seed=42)$ training_loss = model_with_loss.get_latest_training_loss()$ print(training_loss)
text = df.section_text[2461]$ text
readme = processing_test.README()$ readme = open(readme, "r")$ print(readme.read())
engine.execute('SELECT * FROM Station').fetchall()
DF1.describe()
geometry = openmc.Geometry(root_universe)
status = client.training.get_status(training_run_guid_async)$ print(json.dumps(status, indent=2))
URL = "http://www.reddit.com"
store_items = store_items.rename(columns = {'bikes': 'hats'})$ store_items
y=dataframe1['Close']$ plt.plot(y)$ plt.show()
fig, ax = plt.subplots(figsize=(12,12))$ xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)$ plt.show()
my_model_q2 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_rf, training='label')$ cross_validation.cross_val_score(my_model_q2, X_test, y_test, cv=10, scoring='accuracy')
db = client.insight_database$ collection = db.posts
p_old = df2['converted'].mean()$ p_old
df_birth.population = pd.to_numeric(df_birth.population.str.replace(',',''))
init = tf.global_variables_initializer()$ sess.run(init)
crimes.PRIMARY_DESCRIPTION[3:10]
utils.add_coordinates(data)
payload = "elec,id=500 value=24 "#+str(pd.to_datetime('2018-03-05T19:29:00.000Z\n').value // 10 ** 9)$ r = requests.post(url, params=params, data=payload)
dummies = pd.get_dummies(plate_appearances['events']).rename(columns=lambda x: 'event_' + str(x))$ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)
Measurement = Base.classes.measurements
df2=df.copy()$ print(id(df),sep='\n')$ print(id(df2),sep='\n')
max_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first()$ print("Max Date is ", max_date)
data_tickers = data_tickers.resample(sampling, how='last') $ data_tickers.head()
number_of_commits = len(git_log.index)$ number_of_authors = git_log.author.nunique()$ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
conn.execute(sql)
import os # To use command line like instructions$ if not os.path.exists(DirSaveOutput): os.makedirs(DirSaveOutput)
financial_crisis.drop('Spain defaults 7x', inplace = True)$ print(financial_crisis)
mask = y_test.index$ t_flag = y_test == 1$ p_flag = pred == 0
output_path = os.path.join(output_dir, 'prediction.csv')$ write_output(ids, ids_col, y_pred, label_col, output_path)
my_model_q9 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label')$ my_model_q9.fit(X_train, y_train)$ base_model_relation, base_accuracy_comparison = my_model_q9.base_model_eval()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)$ auth.set_access_token(access_token, access_token_secret)$ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
data['team']$ data.team
df_sched.iloc[:,1:] = df_sched.iloc[:,1:].apply(lambda x: x.str[:10])
df2[df2.duplicated('user_id')]$
dat_hcad['blk_lower'] = dat_hcad['0'] - dat_hcad['0'] % 100$ dat_hcad['blk_upper'] = dat_hcad['blk_lower'] + 99$ dat_hcad['blk_range'] = dat_hcad['blk_lower'].map(str)+'-'+dat_hcad['blk_upper'].map(str)+' '+dat_hcad['COMMERCE'].map(str)
reddit.info()
tree_chunker = ConsecutiveNPChunker(train_trees)$ print(tree_chunker.evaluate(valid_trees))
fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 7,8], feature_names=X_test.columns, grid_resolution=70, n_cols=7)$ fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 10], feature_names=X_test.columns, grid_resolution=70, n_cols=7)$
df.loc['20180103','A']
df.resample('M').mean()
theft = crimes[theft_bool]$ theft.head()
stats = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\$ filter(Measurement.station=="USC00519281").group_by(Measurement.station).all()$ stats
cityID = '60e2c37980197297'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Paul.append(tweet) 
pd.date_range('1/1/2000', '1/1/2000 23:59', freq='4h')$
sentiment_pd.to_csv('Senitment_On_Tweets.csv')
df_new.head()
<img src="/images/MongoDB1.png" alt="[img: MongoDB view]" title="MongoDB View" />
df_all_payments  = ( df_providers.groupby(['id_num','year'])[['disc_times_pay']].sum())$ df_all_payments.head()$
sp500.at['MMM', 'Price']
df_data = pd.read_csv(CSV_FILE, dtype=str)$ print '%d rows' % len(df_data)$ df_data.head()
daily = hourly.asfreq('D')$ daily
theft = crimes[crimes['PRIMARY_DESCRIPTION']=='THEFT']$ theft.head()
data.tail(6)
attend_with = attend_with.drop(['[', ']'], axis=1)$ attend_with = attend_with.drop(attend_with.columns[0], axis=1)
%%html$ <img src = "https://data.globalchange.gov/assets/e5/ee/9329d76bf3f5298dad58d85887bd/cs_ten_indicators_of_a_warming_world_v6.png", width=700, height=700>
with open('datasets/git_log_excerpt.csv') as f:$     print(f.read())$     f.close()
prcp_analysis_df = df.rename(columns={0: "Date", 1: "precipitation"})$ prcp_analysis_df.head()
news_df = pd.DataFrame(news_dict)$ news_df.head()
base_col = 't'$ df.rename(columns={target_column: base_col}, inplace=True)
avgcomp = groupedNews['Compound'].mean()$ avgcomp.head()
compacted.to_csv(basedirectory+projectname+'/'+projectname+'_images.csv')
AFX_X_2017 = r.json()
data = res.json()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=", auth=('', ''))$
test = all_sets.cards["XLN"]$ test.loc["Search for Azcanta", ["manaCost", "types", "printings"]]
DataSet.tail()$
iris.iloc[iris.iloc[:,1].between(3.5, 3.6).values,1]
Bot_tweets.groupby('sentiment').count()
train2014 = data[np.logical_and(data['date'] > '2013-12-31',data['date'] < '2015-01-01')]$ print(train2014.shape)
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&collapse=null&api_key=my_API_KEY'
df_ct = pd.read_csv("can_tire_all_final.csv", encoding="latin-1")
import numpy as np$ x = np.array([0,1,2,3,4,5,6,7])$ x.dtype
merged1 = pd.merge(left=merged1, right=offices, how='left', left_on='OfficeId', right_on='id')
plt.rcParams['figure.figsize'] = 8, 6 $ plt.rcParams['font.size'] = 12$ viz_importance(rf_reg, wine.columns[:-1])
yc_new4 = yc_new3[yc_new3.tipPC > 1]
df_all_wells_wKNN_DEPTHtoDEPT.tail()
tag_df = tag_df.stack()$ tag_df
fin_r_monthly = fin_r.resample('M').asfreq()
assert pd.notnull(ebola).all().all()
customers_df = pd.read_csv('./data/User_Information.csv')$ print(len(customers_df))$ customers_df.head()
yc_merged = yc_trimmed.merge(yc_sd, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')
orders = pd.read_csv('../data/raw/orders.csv')$ products = pd.read_csv('../data/raw/products.csv')$ order_details_prior = pd.read_csv('../data/raw/order_products__prior.csv')$
seng = 'mysql+pymysql://root:'+spassword+'@localhost/sakila'$ engine=create_engine(seng)$ data = pd.read_sql_query('select * from actor', engine)$
round((model_x.rsquared_adj), 3)
url  = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris_wheader.csv"$ iris = h2o.import_file(url)
df.head()
DataSet.head(100)
filename = "HubProcessDurations.xlsx"$ xl = pd.ExcelFile(filename)$ tabs = xl.sheet_names
X = preprocessing.scale(X)$
df.index = pd.to_datetime(df.index)
windfield_matched_array=windfield_matched.ReadAsArray()$ print('windfield shape = '+ str(shape(windfield_matched_array)))$ print('ndvi_change shape = '+ str(shape(ndvi_change.values)))
data.describe()
client.repository.delete('50017ab0-237c-451b-befe-ef435c1b5d86')
auth = tweepy.OAuthHandler(api_key, api_secret)$ auth.set_access_token(access_token, access_secret)$ api = tweepy.API(auth)
from textblob import TextBlob$ data_df['sent_pola'] = data_df.apply(lambda x: TextBlob(x['clean_desc']).sentiment.polarity, axis=1)$ data_df['sent_subj'] = data_df.apply(lambda x: TextBlob(x['clean_desc']).sentiment.subjectivity, axis=1)
from IPython.display import HTML$
print activity_df.iloc[-3]
df_indices = df_sp_500.unionAll(df_nasdaq).unionAll(df_russell_2k)$ df_indices.sample(False, 0.01).show()
index = similarities.MatrixSimilarity(lsi[corpus])
from scipy.stats import norm$ print(norm.cdf(z_score))$ print(norm.ppf(1-(0.05/2)))$
dd = pd.read_csv("processed_users_verified.csv")$ dd.head()$
overallDf = pd.DataFrame({$     "News Outlet": overallOutlet,$     "Compound Score" : overallCompound})
engine = create_engine("sqlite:///hawaii.sqlite", echo = False)
from sklearn.linear_model import LinearRegression$ from sklearn.model_selection import train_test_split
atdist_4x_positive = atdist_4x[atdist_4x['emaResponse'].isin(["Yes! This info is useful, I'm going now.", "Yes. This info is useful but I'm already going there."])]$ atdist_4x_positive.merge(atloc_4x, how='left', on=['vendorId', 'taskLocationId'])
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
ngrams_summaries = cvec_2.build_analyzer()(summaries)$ Counter(ngrams_summaries).most_common(10)
words_only_scrape_freq = FreqDist(words_only_scrape)$ print('The 20 most frequent terms (terms only): ', words_only_scrape_freq.most_common(20))
temp_df2['titles'] = temp_df2['titles'].str.lower()
df1 = pd.DataFrame([pd.Series(np.arange(10, 15)), pd.Series(np.arange(15, 20))])$ df1
mgxs_lib.mgxs_types = ['nu-transport', 'nu-fission', 'fission', 'nu-scatter matrix', 'chi']
import json$ import pandas as pd$ import matplotlib.pyplot as plt
open_price = [x[1] for x in data2 if x[1]!=None]  # needs to exclude None. $ print('The highest opening price during 2017 is : ',max(open_price))$ print('The lowest opening price during 2017 is : ',min(open_price))
files8= files8.drop('EndDate',axis=1)$ files8= files8.drop('StartDate',axis=1)$ files8.head()$
print('There are {} news articles'.format(len(news)))$ print('Timewise, we have news from {} to {}'.format(min(news.date), max(news.date)))
s4.shape
s2 = pd.Series([10,100,1000,10000],subset.index)$ s2
goog.plot(y='Close')
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
writer = pd.ExcelWriter('my_dataframe.xlsx')$ merged_df.to_excel(writer, 'Sheet1')$ writer.save()
df3_obs.loc['Churn Date'] = obs_end_date - dt.timedelta(df3_obs.loc['Frequency of donations'])
ds = tf.data.TFRecordDataset(train_path)$ ds = ds.map(_parse_function)$ ds
m_vals = np.linspace(m_true-3, m_true+3, 100) $ c_vals = np.linspace(c_true-3, c_true+3, 100)
pre_number = len( niners[niners['Jimmy'] == 'no']['GameID'].unique() )$ print pre_number
data.iloc[[1]]
df_geo = pd.DataFrame(sub_data["id_str"]).reset_index(drop=True)$ df_geo["geo_code"] = geo_code$ df_geo.head()
kick_projects = df_kick[(df_kick['state'] == 'failed') | (df_kick['state'] == 'successful')]$ kick_projects['state'] = (kick_projects['state'] =='successful').astype(int)
city_group_df_merge_urban = city_group_df_merge.loc[city_group_df_merge['City Type']=='Urban',:]$ city_group_df_merge_suburban = city_group_df_merge.loc[city_group_df_merge['City Type']=='Suburban',:]$ city_group_df_merge_rural = city_group_df_merge.loc[city_group_df_merge['City Type']=='Rural',:]$
lm = smf.ols(formula='sales ~ TV + radio + newspaper + TV*radio', data=data).fit()$ lm.params
cityID = '7a863bb88e5bb33c'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anchorage.append(tweet) 
df_test_user_2 = df_test_user.copy()$ df_test_user_2['created_on'] = '2017-09-20 00:00:00'
sortedprecip_12mo_df=precip_12mo_df.sort_values('date',ascending=True)$ sortedprecip_12mo_df.head()
cityID = '5c2b5e46ab891f07'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Las_Vegas.append(tweet) 
xml_in['authorId'].nunique()
teams=pd.unique(results[['home_team','away_team']].values.ravel())$ teams
actual_diff = (df2[df2['group'] == "treatment"]['converted'].mean()) - (df2[df2['group'] == "control"]['converted'].mean())$ actual_diff
data.describe()  # 看所有统计值，然后根据中位值的标准差三倍原则来找
nb_pipe.fit(X_train, y_train)$ nb_pipe.score(X_test, y_test)
lr = LogisticRegression(random_state=20, max_iter=10000)$ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr','multinomial'], 'solver':['saga','newton-cg', 'lbfgs']}$ grid = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).head(5)
plt.pie(total_fare, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140)$ plt.show()$
reddit['Class_comments'] = reddit.apply(lambda x: 'High' if x['num_comments'] > median_comments else 'Low', axis = 1)$ reddit.head()
DBPATH = 'results.db'$ conn = sqlite3.connect(DBPATH)$ cur = conn.cursor()
conn = 'mongodb://localhost:27017'$ client = pymongo.MongoClient(conn)$
%matplotlib inline$ import matplotlib.pyplot as plt
plot_BIC_AR_model(data=RN_PA_duration.diff()[1:], max_order_plus_one=8)
attend_with = questions['attend_with'].str.get_dummies(sep="'")
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closerToBotOrTop'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], 'FromTopWell', 'FromBotWell')
tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')$ tips['tip'] = pd.to_numeric(tips['tip'], errors = 'coerce')$ print(tips.info())$
keep_cols = ['Follow up Telepsychiatry', 'Follow up', 'Therapy Telepsychiatry', 'Returning Patient', 'Returning Patient MD Adult']$ dr_existing = dr_existing[dr_existing['ReasonForVisitName'].isin(keep_cols)]
Customer.withdraw(jeff, 200.0)$ jeff.balance           # Shows 700.0
modCrimeData = crimeData$ modCrimeData.set_index("Year", inplace=True)$ modCrimeData.head()
s1.values
data = pd.read_csv('Eplusfanpage.csv')$ data$
from sklearn.utils import shuffle$ qs = shuffle(qs)$ print len(qs)
df = df[df.launched_year != 1970]
data.quantile(0.25)
tmax_day_2018.attrs
df_control = df2.query('group=="control"')$ y_control = df_control["user_id"].count()$
slope, intercept, r_value, p_value, std_err = stats.linregress(data['timestamp'],data['rate'])
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
flight = spark.read.parquet("/home/ubuntu/parquet/flight.parquet")
df2 = df.drop(remove_index)$ print(df2.shape)  # This should be 294478 - 3893$ df2.head()
np.shape(temp_fine)
pass_file_name = "C:\KUBootCamp\\passw.json"$ data = json.load(open(pass_file_name))$ spassword = data['mysql_root']
history = model.fit(train_X, train_Y, epochs=num_epochs, batch_size=1, verbose=2, shuffle=False) $
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])$ display(data.head(10))
df_new[['ca','uk','us']] = pd.get_dummies(df_new['country'])
sample=train.head(100).copy()$
spreadsheet = '1LTXIPNb7MX0qEOU_DbBKC-OwE080kyRvt-i_ejFM-Yg'$ wks_name = 'CleanedData'$ d2g.upload(df_dn,spreadsheet,wks_name,col_names=True,clean=True)
!hdfs dfs -cat 32B_results-output/part-0000* > 32B_results-output.txt$ !head 32B_results-output.txt
bc = pd.read_csv("bitcoinity_all.csv")
eegRaw_df.to_csv(outputData, index=False, header=False) 
print(stock_data.shape)$ print("The number of rows in the dataframe is: ", stock_data.shape[0])$ print("The number of columns in the dataframe is: ", stock_data.shape[1])$
df["grade"].cat.categories = ["very good", "good", "very bad"]$ df["grade"]
df = PredClass.df_model$ print df.dtypes.loc[df.dtypes == 'object']
url = "http://www.fdic.gov/bank/individual/failed/banklist.html"$ banks = pd.read_html(url)$ banks[0][0:5].ix[:,0:4]
a = pd_series.sort_values()$ a.to_csv("stackoverflow.csv")$ a
print("Predicted:", "\n",' '.join(y_pred[0]))$ print('\n')$ print("Correct:", "\n" ,' '.join(sent2labels(sample_sent)))
(a + b).describe()$
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
affair_children = pd.crosstab(data.children, data.affair.astype(bool))$ affair_children
r.groupby('cat').count()
(a_diff<np.array(p_diffs)).mean()
print(actual_value_second_measure[actual_value_second_measure==2])$ holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00063550') ]
dates = pd.date_range('20130101', periods=6)$ df = pd.DataFrame(np.arange(24).reshape((6,4)),index=dates, columns=['A','B','C','D'])$ df.A.max
full_act_data = steps.join(heart, how='left')
df2[df2.duplicated(['user_id'], keep=False)]
c.index = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j"]$ c
btc.plot(y=['price'])$ plt.show()
gbm.predict(test)
accuracy=100*metrics.accuracy_score(y_test, y_hat)$ print(accuracy)
Twitter_map2.save('Twitter_FSGutierres_map.html') #save HTML
tweet_df.head()$
act_diff = df[df["group"]=='treatment']["converted"].mean() - df[df["group"]=='control']["converted"].mean()$ act_diff
data = pd.read_csv('data/analisis_invierno_3.csv')
us_rhum = rhum_fine.reshape(844,1534).T #.T is for transpose$ np.shape(us_rhum)
df = pd.read_excel("mails taggen.xlsx")
get_nps(combined_df, 'country').sort(columns='score', ascending=False).head(10)
fcc_nn = indexed.loc[indexed['parent_id'] == 't3_7ej943'].reset_index()$
df_users = df_users.dropna(axis=0)$ print(df_users.shape)$ df_users.head(10)
data = data.dropna()
with open(output_folder+Company_Name+"_Forecast_"+datetime.datetime.today().strftime("%m_%d_%Y")+".pkl",'rb') as fp:$     dummy_var = pickle.load(fp)
data_df.clean_desc[22]
data = pd.read_sql("SELECT * FROM session_privs",xedb)$ print(data)$
x = content_performance_bytime.groupby(['document_type', pd.Grouper(freq='M')])['pageviews'].sum()
if 0 == 1:$     news_titles_sr.to_pickle(news_period_title_docs_pkl)
ndExample = df.values$ ndExample
small_ratings_file = os.path.join(dataset, 'ml-latest-small', 'ratings.csv')$ print('small_ratings_file: '+ small_ratings_file)$ small_ratings_raw_data , small_ratings_raw_data_header = read_file(small_ratings_file)
df_students.head()
train_ratio = 0.75$ train_size = int(samp_size * train_ratio); train_size$ val_idx = list(range(train_size, len(df)))
exportparams = urllib.parse.urlencode({$     'action': 'tweet-export',$     'format': 'csv'})
r.json()['dataset']['data'][0]$
print 'Python Version: %s' % (sys.version.split('|')[0])$ hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER$ print 'HDFS filesystem running at: \n\t %s' % (hdfs_conf[0])
utility_patents_subset_df['figure_density'] = utility_patents_subset_df['number-of-figures'] / utility_patents_subset_df['number-of-drawing-sheets']$ utility_patents_subset_df['figure_density'].describe()
df['Injury_Type'].value_counts()
def trip_start_date(x):$     return re.search(r'(\d{4})-(\d{2})-(\d{2})', x).group(0)
X_train[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True,$                                              layout = (1, 3), sharex = False)
cityID = '7d62cffe6f98f349'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Jose.append(tweet) 
session.query(measurements.date)[-1]
df.tail()
import IPython  # for displaying parse trees inline$ for tree in parser.parse(sentence):$     IPython.display.display(tree)  # instead of tree.draw()
des2doc = dict()$ for index,row in temp.iterrows():$     des2doc[index] = row['description']
festivals.at[2,'latitude'] = 41.9028805$ festivals.head(3)
p(sys.getfilesystemencoding)
fed_reg_dataframe['token_text'] = fed_reg_dataframe['str_text'].apply(lambda x: word_tokenize(x.lower()))
cat_sz = [(c, len(full_data[c].unique())) for c in cats]$ emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]$ n_conts = len(full_data.columns) - len(cats)
precip_data = session.query(Measurements).first()$ precip_data.__dict__
print ('Python Version: %s' % (sys.version.split('|')[0]))$ hdfs_conf = !hdfs getconf -confKey fs.defaultFS ### UNCOMMENT ON DOCKER$ print ('HDFS filesystem running at: \n\t %s' % (hdfs_conf[0]))
params =  {'reg_alpha': 0.1, 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'min_child_weight': 1$           , 'n_estimators': 300, 'reg_lambda': 1.0, 'random_state': 1, 'max_depth': 4} $ reg_final = xgb.XGBRegressor(**params).fit(X, y)#**params$
p = len(train_att)/len(train)$ print(len(train_att))$ print('The percentage of converted clicks is {num:.10%}'.format(num=p))
df_a = pd.DataFrame(df_graph.groupby('dates')['click_rec_menues'].value_counts())
cityID = 'd98e7ce217ade2c5'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Stockton.append(tweet) 
pattern = re.compile('AA')$ print(pattern.sub('BB', 'AAbcAA'))$ print(pattern.sub('BB', 'bcAA'))
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets'])))$ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets'])))$ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
r = requests.get('https://www.quandl.com/api/v3/datasets/OPEC/ORB.json?start_date=2013-01-01&end_date=2013-01-01&api_key='+API_KEY)
url2 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2017-01-02&end_date=2017-12-31&api_key=mVyTTi52QUPLHnmV-tx_'$ r2 = requests.get(url2)$ jd = r2.json()$
Base.classes.keys()
df1 = df[df['Title'].str.contains(search_terms)]$ df1.shape
if create_sql_indexes:$     conn.execute('create index left_ix on left(key, key2)')$     conn.execute('create index right_ix on right(key, key2)')$
birth_dates.head(3)
month_counts = df.month.value_counts().sort_index()
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))$ new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)$ new_user_recommendations_rating_title_and_count_RDD.take(3)
pd.set_option("display.max_rows", 20)$ np.random.seed(12)
grouped_newsorgs = news_sentiment_df.groupby(["News Organization"], as_index = False)
merged2 = pd.DataFrame.merge(merged,omdb_df,on="movie",how="inner")$ merged2.head()
filepath = os.path.join('input', 'input_plant-list_NO.csv')$ data_NO = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None)$ data_NO.head()
plate_appearances['pitcher_throws_left'] = np.where(plate_appearances['p_throws'] == 'L', 1, 0)$ plate_appearances['left_handed_batter'] = np.where(plate_appearances['stand'] == 'L', 1, 0)
r_forest['has_extended_profile'] = r_forest['has_extended_profile'].fillna(0)
print(soup.find_all('div', 'sammy')[0])$
sumPre = dfPre['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])$ sumPost = dfPost['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])
df.head()
cityID = '28ace6b8d6dbc3af'$ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chula_Vista.append(tweet) 
my_gempro.uniprot_mapping_and_metadata(model_gene_source='TUBERCULIST_ID')$ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping)$ my_gempro.df_uniprot_metadata.head()
all_tables_df.iloc[2:4, 1:]
brands_np = np.asarray(brands)$ models_np = np.asarray(models)
scores[scores.IMDB == max_IMDB]
std_df = choose_local_df('STD')$ std_df.loc[std_df['Sold_to_Party']=='0000101348'][['Sold_to_Party','Sales_document','Material_Description','Unit_Price','Document_Date']]
df.iloc[99:110,3:]
park = load_data('../../static/parkinson_1960tonow.csv')
ratings = ['review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value']$ filtered_df.dropna(axis=0, subset=[ratings], inplace=True)
words_mention_sk = [term for term in words_sk if term.startswith('@')]$ corpus_tweets_streamed_keyword.append(('mentions', len(words_mention_sk))) # update corpus comparison$ print('List and total number of mentions: ', len(set(words_mention_sk))) #, set(terms_mention_stream))
from app.crawler import Crawler$ crawler = Crawler()$ crawler.pull_save_data()
df_obj2['G'] = df_obj2['D'] + 4$ print(df_obj2.head())
mvrs = ratings.groupby('movieId').size().sort_values(ascending=False)$ tmp_ratings = ratings.ix[mvrs[mvrs > rating_count].index].dropna()
y_train.value_counts(normalize=True)
df2 = df2.add_suffix(' Closed')$ df4 = pd.merge(df,df2,how='left',left_on='Date Closed',right_on='date Closed')$
dci = indices(dcaggr, 'text', 'YearWeek')$ dci.head()
colnames = ['log***','gfhdfxh']$ df.columns = colnames
print 'Date range: %s - %s' % (response_df.created.min(), response_df.created.max())
results = soup.find_all('div', class_='slide')$ print(results)
fin_coins_r.isnull().sum()
df1.dropna(inplace=True)
twitter_data = pd.DataFrame(list(twitter_coll_reference.find()))$ twitter_data.head()
client = MongoClient()$ db = client.test_database #acessa ou cria o banco$
precipitation_df = pd.DataFrame(sq.prec_last_12_months())
df2['Change'].abs()  
import numpy as np$ X_nonnum = X.select_dtypes(exclude=np.number)
for iter_x in np.arange(lookforward_window)+1:$     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
dfs['DE'].groupby(['comment', 'data_source'])['electrical_capacity'].sum().to_frame()$ dfs['DE'].groupby(['comment', 'energy_source_level_2'])['electrical_capacity'].sum().to_frame()
news_sentiment_df.to_csv("twitter_news_org_sentiment.csv", encoding="utf-8", index=False)
releases = pd.read_csv('../input/jail_releases.csv')$ bookings = pd.read_csv('../input/jail_bookings.csv')
plt.title("Aggregate Media Sentiment baed on Twitter")$ plt.ylabel("Tweet Polarity")
dfClientes.loc[(dfClientes["MES"] == 201701) & (dfClientes["EDAD"] < 30)].head()
google['high'].apply(custome_roune).value_counts().sort_index()
sample = rng.choice(sizes, sample_size, replace=True)$ print(f'Mean (one sample) = {np.mean(sample):5.3f}')$ print(f'Standard deviation (one sample) = {np.std(sample):5.3f}')
print(discharge_list[0].date)$ print(discharge_list[len(discharge_list)-1].date)
sl['second_measurement'] = np.where((sl.new_report_date==sl.maxdate) & (sl.mindate!=sl.maxdate),1,0)
data = pd.DataFrame(data=[tweet.text for tweet in results], columns=['Tweets'])$ display(data.head(10))$ display(data.tail(10))
for a in cells: $     ct.save_reconstruction(a['id'], file_name='allen_morpho/ai'+str(a['id'])+'.swc')$     ct.save_reconstruction_markers(a['id'],file_name='allen_morpho/ai'+str(a['id'])+'_marker.swc')
dates = pd.date_range(date.today(), periods=2)$ dates
dj_df = pd.read_table(data_file_path)$ print(dj_df.head())$ print(dj_df.info())
td_alpha = td ** (1/3)$ td_alpha = td_alpha / td_alpha.max().max()
coming_next_reason.to_csv('../data/coming_next_reason.csv')
i = 0$ sample_sent = valid_labels[i]$ print(' '.join(sent2tokens(sample_sent)), end='\n')
for word in STOP_WORDS:$     nlp.vocab[word].is_stop = True
df = pd.DataFrame([x._json for x in tweets])[['text', 'created_at', 'user']]$ df['label'] = df.user.map(lambda x: x.get('name'))$ df.head()
malebydatenew  = malebydate[['Sex','Offense']].copy()$ malebydatenew.head(3)
TripData_merged2.isnull().sum()
url_domains = grouped['domain'].agg({'domain': lambda x: np.unique(x)})$ unique_urls = pd.merge(unique_urls, url_domains)$ unique_urls.head()
invalid_sets = ["UGL", "UNH", "UST", "pCEL", "pHHO", "VAN"]$ all_sets = all_sets.loc[~all_sets.code.map(lambda x: x in invalid_sets)]
tweet_archive.name.value_counts().head(5)$
print(len(countdf['user'].unique()))$ print(len(countdf['user'].unique())-len(count1df['user'].unique()))$ print(len(countdf['user'].unique())-len(count6df['user'].unique()))
df['is_application'] = df.application_date.apply(lambda x: 'No Application' if x == None else 'Application')$ df.head(5)
import requests$ base_url = 'http://www.mlssoccer.com/stats/season'$ response = requests.get(base_url)$
new_df = df.dropna(subset=['driver_id'],inplace=False)
Rural = rides_analysis[rides_analysis["City Type"].notnull() & (rides_analysis["City Type"] == "Rural")]$
tlen.plot(figsize=(16,4), color='r');
femalebydate = female.groupby(['Date','Sex']).count().reset_index()$ femalebydate.head(3)
instance.assumptionbreaker()
masked['user_age'] = masked['created_at'] - pd.to_datetime(masked['user_created_at'])
df = pd.concat(map(pd.read_csv, glob.glob("*.csv")))
most_yards[['Date','PlayType','Yards.Gained','qtr','desc','Rusher','Receiver', 'Jimmy']][:10]
festivals.head(5)
df.to_sql('tw_posts', engine, schema=schema, if_exists='append')
experience.to_csv('../data/experience.csv')
print(dfx.dropna(how='any'))
ADNI_diagnosis_data_description = pd.read_csv('ADNIMERGE_DICT.csv')$ print(ADNI_diagnosis_data_description['FLDNAME'].unique())$ ADNI_diagnosis_data_description[['FLDNAME','TEXT']]
print(list(festivals.columns.values))$ festivals = festivals[['Index', 'Date', 'Location', 'latitude', 'longitude', 'Website']]$ festivals.head(3)
coins_mcap_today[50:].index
start = "2005-01-01"$ end = "2018-06-31"$ trading_days = fk.get_monthly_last_trading_days(start=start, end=end)$
news_sentiment_df.to_csv('news_sentiment.csv', sep=',')
from sklearn.model_selection import train_test_split$ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
