# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Encontrar los registros que se encuentran entre las posiciones [10mo - 20vo] de mi DataFrame # pero sólo con las columnas de MES DISTRITO y EDAD
# of unique messages = len
# Print the first 5 external nodes # input_nodes_DF = pd.DataFrame.from_csv(input_nodes_file, sep = ' ')
# How many stations are available in this dataset? # Print results of above count query# Print  
# Retrieving a total amount of dates
# find historical data for 2015
#setting unique ID as index of the table #this is because the ID column will not be used in the algorithm. yet it is needed to identify the project
#load a Parquet file
# 8. 分析连续的⼏几天数据
# Use `engine.execute` to select and display the first 10 rows from the table
#review expanded data- note could further do the same thing for contributors, or affiliations
# Test score is better than the train score - model generalizes well 
#find outliers  #X = x['age'].astype('timedelta64[D]') ###Value counts to see any outliers for the data
#Dropping the duplicates to then merge later
#5a. You cannot locate the schema of the address table. Which query would you use to re-create it? 
#Compute the sigmoid function
# Split PKG/A column into to and drop
### Step 12: Review the two-dimensional array created from the PCA
# 转换为dataframe
#pivot_table의 결과로 붙은 컬럼 제목 정리 
# Converting column to date format
# Set up access credentials
# FileLink(str(FLASK_PATH/'df_parent.csv'))
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# This graph is the right data but the x and y axis are not on the right scale. Notice the date span on the x axis # Notice the y axis labels
#drop guidance_high and guidance_low, always NaN
#tweepy API Authentication
#Display head of df_schools
#add it to the csv
# subset us lon # subset us lat # print dimensions
# Get a response to the input text 'How are you?'
# also a series
#Stores file in the same directory as the iPython notebook
# Check to make sure everything worked out
# df.groupby([df.created_at.dt.month,'product_version']).count()['Id'].reset_index(1)
query_term = 'Disaster risk assessment'$ query = """SELECT distinct e.pullquery, e.pullby, a.optionalid01 as doi, c.title, d.abstracttext,c.journalname,c.pubyear, c.publicationdate from public.datapull_detail as a inner join public.datapull_uniqueid as b on a.pullsource = b.pullsource AND a.associatedid = b.associatedid inner join public.datapull_title as c on b.uniqueid = c.uniqueid inner join public.datapull_text as d on b.uniqueid = d.uniqueid inner join public.datapull_id as e on e.pullid = a.pullid where e.pullquery = '{}' limit 25""".format(query_term)$
#get the sum of a column
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# asfreq :: you can convert one period to another eg. quater period to month period # following will change freq from quater to month  # how :: this will decide which month freqency needs to be created from quater weather it is start of month or end of month
#reading csv file created by webscraping
#Export Canadian Tire file to local machine
#plot histogram of the 'Amazon Customer' # these customers never update images of the products.
# dropping the "hour" column now that we no longer need it
# Replies of Trump
# prefix an extra column of ones to the feature matrix (for intercept term)
# TODO: overall validation score in one number.
#Creating new columns 
# resample to weekly data
# TASK D ANSWER CHECK
# the 'ensemble', 'havana', and 'ensemble_havana' annotation sources describe sub-gene elements. # use logical indexing to filter the dataframe for sub-gene element annotations
#Let's see if we can scrape the urls from 'Top 5 Data Science and Machine Learning Course for Programmers' by DZone
# get the top model to use
#return the last 12 months
# show a sample of the data from the new dataframe 'gdf'
#ensure you have a full dataset, 100 per account  #len(organize_data) #save data to csv file 
# Counting the no. commits per year # Listing the first rows
# Loading in the pandas module # Reading in the log file # Printing out the first 5 rows
# Make sure no variable is the index
## create a new column 'score_str' which will hold a value about the score - is it below average or above average?
#sort by nytimes in order to create different color scatter plot
#help
# Display the entries where close is greater then 80
#Looks like a lot more people from Costa Rica now. Let's try to aggregate all the locations with 'Costa Rica' in the name
# Check how many tweets creatd every minute during the data collection period # At this point I just want to check the time trend of the tweets which can be done without time-zone conversion.
# Getting polarith and subjectivity as well as assigning an Sentiment to each of the tweet. 
#Check that the concatenation worked
# Car KOL pred
# Count hashtags only
# fill values from forward
# points.iloc[0, 1]  # ERRORS
# Process data using Natural Language Processing techniques: clean, remove stop words and lemmatize.
# Common citation violations from parking during certain hours and expired meters
#Lets check out the languages used #I wonder what 'und' is!?
#extract individual genres from list of genre, turn into dummy variables, drop duplicates
# plot the top 5 buzzing subreddit by # of posts
# Step 5: Time to review data.py file and review where function went wrong... ## Review the first 24 entries of the fremont.csv file ## This displays the time is 12 hours and not 24, which means our strftime; We need to use I instead of H
# The 'Balance' column actually stores the same value for every record - so delete it, and fix later
#Problem 1
#Clean the data
#load objects
# No duplicated tweet id
# describe dataframe
# This is to force jupyter to display all rows when asked.
# take the publications whose publication date is yonger than x days
# print min and max date
# create a length column on the gene dataframe # calculate basic statistics about length values
#Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).
# Create timeseries from scenario results in preperation to post processing
# Step 3: Plot pivot table data ## Here we see the data doesn't have the two peaks, but instead just one...This is what we need to fix
#Variables to populate the metadata in the SB Item #Retrieve Acquisition Date
#Grabs the last date entry in the data table
# output2.take(2)
#Return a list of all of the columns in the dataframe as a list
#process things
# Predict the on the train_data # Predict the on the train_data # Predict the on the train_data
# indexer untuk tanggal pembuatan status - pas di tanggal tersebut
# choose a random point for the update  # update m # update c
# This code was tested with TensorFlow v1.4 # The import statements will not work with earlier versions, because Keras is in tf.contrib in those versions
# check option and selected method of (27) choice of thermal conductivity representation for soil in Decision file
# fire violation  # firevio.dtypes  
# converting the timestamp column # summarizing the converted timestamp column
#Let's go grab those articles out of lists, then we roll
#m1.diagnostics()
#select only the rows in files1 who had completed application
#WARNING: This cell will take a while to execute - up to a couple of hours
#Read the first 5 records
# scikit learnのライブラリを使って認識率の計算
# Create an OpenMOC Geometry from the OpenMC Geometry
# NLTK Stop words
# メインスクリプトの格納ディレクトリに移動
# Note: Count can't be greater than 20
# Renaming the columns to station, name, tobs
#Convert the returned JSON object into a Python dictionary.
## a method to convert the response into JSON!
# filtering out wrong timestamps
# We create a pandas dataframe as follows: # We display the first 10 elements of the dataframe:
#empty becuse the duplicated row is dropped
#set input/output paths #can eventually set this to the SOAPY API https://dev.socrata.com/foundry/data.waterpointdata.org/gihr-buz6 #DATA_PATH = "/Users/chandlermccann/Google Drive/Google Drive/Berkeley MIDS 2016/W210-Capstone_WaterProject"
# Откроем сохраненные таблицы
#Analyzing distributions
# Education KOL pred
####TEST #a['Compound Score'].mean()
# Check if there are any NaN values in the Test DF
# Are there any outliers that would make this task difficult??
# a[y,x] = avg
#ax = plt.bar(temp_list,frequency,color='b',alpha=.05,align="center") #plt.bar(x_axis, users, color='r', alpha=0.5, align="center")
### Let's drop that silly row with huge snack expenses
### Step 20: Use the day of week attribure of the datetime64 object 
# Create engine using the `hawaii.sqlite` database file created in database_engineering steps
# Create engine using the `hawaii.sqlite` database file
# combine lat and lon column to a shapely Point() object
#Plot the min, avg, and max temperature from your previous query as a bar chart. #Use the average temperature as the bar height. #Use the peak-to-peak (tmax-tmin) value as the y error bar (yerr).
# columns in table x
# создадим отдельный столбец с названием города
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#print(highlight(json.dumps(jsummaries, indent=4, sort_keys=True), JsonLexer(), TerminalTrueColorFormatter()))  # remove the comment from the beginning of the line above to see the full results
#full_image =  #featured_image_url = soup.find('https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16715_hires.jpg') #for image in images:
# 今回使用するレポジトリをGoogle Colabの環境にクローン # !git clone https://github.com/s0yamazaki/WallClassification.git
# read parquet file with arrow
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# print the text of the first result.
## we will gather all topics from subreddit (limit=None)
# Variables contained in this file:
# %reload_ext autoreload # %autoreload 2 # from DeepText.preprocess import preProcessor_in_memory
# 23 ST -> 14 ST-UNION SQ @ 11:30 am = 20 min # Print total crepe sales lost due to transit
# get the days_plan_adead column using trip_start_date - timestamp
# Find duplicated user
# pickle final data
#白噪聲檢查 #返回統計量和p值
# Headless Chrome #options.add_argument('headless') #options.add_argument('window-size=1920x1080')
#load data into dataframe
# Neat way to rename columns and order appropriately
# Printing the content of git_log_excerpt.csv
# another example, list of countries
# List comprehension for reddit titles
# most common tags
#Importing and instantiating Vader Sentiment
#Display the last 5 rows in the contractor
# the cursor - holds a result
#Determine the top five schools that have the lowest percent of students passing math and reading (overall passing rate)
# Also only pulling the ticker, date and adj. close columns for our tickers.
# To validate charmander #post_discover_sales[~(post_discover_sales['Email'].isin(pre_discover_sales['Email']))]
# scrs < 0m / all scrs . 
# To insert a document into a collection we can use the insert_one() method:
# Normalization for non-object features - Numeric featue normalizatoin
# this is what infowars shows up as...
# Set index to Date # Apply the count function # Seeing what DummyDataframe look like
# check to see if there are any np.nans
# How good was our prediction? Calculate the total squared error!
# To flatten after combined everything. 
#df_concat_2
# take the first value of each bucket
# Ensuring data is ordered by date
# Making a test user dataframe and change [user_id = 2]'s created-on date
#Convert unix date format
# controls the session # closes session
# First Column
#combining the new dummie variables table to the original table using user_id
# Now we can set the new index. This is a destructive # operation that discards the old index, which is # why we saved it as a new column first.
# Observation counts per station in descending order
# show topics
# Exogenous transmission capacities in 2025 per region [MW]
# proportion of users converted
# save df2 to a new csv file # also specify naming the index as date
## creat a new DF with 2 columns - sum of score and sum of comments with post id as index
#Or we can just show the count by a single column
# -> Having more life enjoyment and satisfaction leads to better outcomes
########################## Check Models Folder #################################
# For this, I will use the preprocessed df again.
# called the stats_diff function that gives us the difference
#checking distribution of projects across various main categories #kick_projects.groupby(['category','state']).size()
# figure out what was the spike at 5 am (related to 5-5:59 am tweets) on Small Business Saturday 
# Now we use Pandas to print the summary statistics for the precipitation data-- IE, the describe function.
# march has most delays
# Lets create a copy of the dataframe so that we can run some tests on it
#getting the summamry of the above fitted model
# merge 'yc_merged_drop' with the departure zip codes
#will login to synapse
#%matplotlib notebook
# stack the various columns
#Task 7: Find the median trading volume
#Find Mean Sentiment by news org
# Check the data
# Retrieve the parent divs for all articles
# loading the data # new_pw(data)
#Save latest weather info in var mars_weather
# To produce pie_chart by city type #we group the original csv read file by "type"
# pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
# integer index and .values field: the values proper (numpy array)
# Desktop with Development C++
# now we need to rebuild the prior parameter covariance matrix
# Let's group purchase quantities by Stock Code and CustomerID
# Object oriented approach
# Collect data from FSE for AFX_X for 2017 with API call
# Verify that an address is valid (i.e. in Google's system)
# use read_table with sep=',' to read a csv
#Exporting transactions with their associated orders
# split and dummy bands column
# load data description and see samples
##   Creating data frames directly from CSV
# The protocol version used is detected automatically, so we do not # have to specify it.
# Find the max and min of the 'Average Fare' to plot on y axis
# 計算結果をDataFrameに変換
# 调整边界距离
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#creating a backup copy of the input dataset
## YOUR CODE HERE
# group by multiple columns
#Get data for the Haw River (siteNo = '02096960') using the function above
#List the unique values inthe CharacteristicName column
# Similarly for an instance, although this really is a dict, not a mappingproxy.
#let's use the function with our list #Pass the tweets list to the above function to create a DataFrame # sort by retweet count
# show available waterbodies
# URL and get request from Reddit # Set `limit=100` for fetch 100 posts per attempt
#write to a CSV file
# Export the dataframe to a csv
# 3.
## collect oppening prices in a list
# as it can be seen, the star should be in the stop word list
# Calculate the date 1 year ago from today # to get the last 12 months of data, last date - 365
# drop rows with missing specialty
#Your code goes here # normalization factor # seting up the loss function
# MAC
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#show behaviour during sepcific time window #pdf.loc['2016-1-1':'2016-3-31',top_allocs[:10].columns].plot()
# Look for Retweets
#### Define #### # Convert index into a column and reset the index # #### Code ####
# Now remove the sample rows from the main dataset
#[i for i in my_list if '91' not in i and '18' not in i]
#concatanate the predictor variables with the dummies dataframe
# import modules & set up logging
# Just Use DataFrame to have a clear view of the dataset
# Generate entries in column "technology" according to energy source "hydro"
# check inserted records
#export and load the df_sample csv
# Add a blank column to flag number one hits
# удалим html теги
# load CSV files
# If python2 is not loaded kill the hypervisor # ! kill -9 -1
# Convert age into hours and inspect head of scrape 
# get indices
# Once the column has been created, I want to sample parts of the database to ensure integrity in the matching.
# id and sentiment are categoricals
# Requirement #1: Add your code here # updated by Huang, Silin ID: A20383068
# convert series to categorical, then back to series  # because categoricals lack many of the useful methods of series
# prada styled info tweeted by gucci 
# you can also' take a dictionary of lists and then pass it into the data frame
# For a quick check, let's also look at this where it is ascending # code here:
# Distribution of daily changes 
#see the values at those index # outliers values replace it with mean should be fine for now
#see if wards are in same bounds as delay points
# easily count the data rows now # note this data is continuous, every few minutes, so a count isn't really helpful but nice to know it can be done
# tweetsIRMA = pd.read_sql("SELECT tc.tweet_id,tc.longitude, tc.latitude, i.lon, i.lat, sqrt(pow(tc.longitude - i.lon,2)+pow(tc.latitude - i.lat,2)) as 'distance', atan(tc.latitude - i.lat,tc.longitude - i.lon)*57.2958 as 'angle' FROM tweetIRMA ti JOIN irma i on ti.irmaTime = i.DateTime JOIN tweetCoords tc on tc.tweet_id = ti.tweet_id",Database().myDB)
# merge with QUIDS
# Let's get the more detailed Titanic data set
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# parse_dates=True, means we let Panda to "understand" the date format for us
# We change the row index to be the data in the pants column # we display the modified DataFrame
# %matplotlib notebook
sql_query = """$ SELECT * FROM top_5_by_genre;$ """$
#add birthdate for subscribed-babies
# finds station activity and sorts from most active to least active.
# Keep relevant columns only (23 columns of 49) #studies_a=pd.DataFrame(studies,columns=['nct_id','overall_status'])
# What are the most active stations? # List the stations and the counts in descending order.
#clean up the raw data 
#### Work with dates
#Determine number of students with a passing reading score (assume that a passing reading score is >69)
################################################## # Load transaction  ##################################################
#os.listdir(A1.paths['directory_paths']['indicator_settings'])
#one popular person in und... let's see this famous tweet #perhaps 'und' is just 'undefined'. The two heavily retweeted tweets are actually RTs themselves!
# df.head()
# Using the station id from the previous query, calculate the lowest temperature recorded,  # highest temperature recorded, and average temperature most active station?
## Summarize 
# Create tf-idf model from corpus # num_nnz is the number of tokens # num_docs here is the number of users which is 10000
# Check data type and missing value for each column to make sure they meet constraint of destination table 
# Lenghts along time:
# Design a query to calculate the total number of stations.
#percent_quarter = percent_quarter.apply(lambda x: x > 1)
# Train the Naive Bayes model
# load the model from disk
# the logistic regression accuracy score
# m.add_regressor('TempC')
# Number of non-merge commits that modified the manuscript markdown source
# Keep only text in the text column
# Grouping by "grade" category and count the number of occurences 
"""$ Test recover df pickle$ """$
# Start the experiment run.
#df.dropna(how = 'all')
#wav_version = pydub.AudioSegment.from_file("tests/test_data/acoustic/wave0.wav", "wav") #wav_version.frame_rate #wav_version.channels
# Group data by death year and cause of death
#plot_user_popularity(very_pop_df,day_list)
# Creating folds
# creating new vs return fan csv
#Plot using Pandas
# Extract spatial extent from attributes
# Get json object with the intraday data and another with  the call's metadata
# Series what's the name for in Series # operation
# TASK F ANSWER CHECK
#drop columns 'text' & 'full_text' #Extract hashtag from 'tweet' and create a new cloumn 'hashtag'
##### key, value pairs
# Revenue has null values for some company. 'V' has been such identified company. # In this experiment, we remove the company from the dataset instead of interpolating. # Convert quarter_start field to datetime.
# Obtain the source names for reference
# Create a Spark DataFrame from Pandas
# Stats model logit() and fit() countries
# A cleaner one line solution
#Diplay the top model parameters
# Qn 1. # Collect data from FSE for AFX_X for 2017 with API call
#Saving the output to CleanedMovies.csv in an Output folder
#print randomdata2 #randomdata3 = randomdata1[(randomdata1 <=-3)] #print randomdata3
# import and instantiate TF-IDF Vectorizer
#idx = pd.IndexSlice
#df= df.sort_values(['label'], ascending = True) #temp= df.sort_values(['label'])
# Define default tick locations for our plots
#Search some tweets! Let's look at hashtag #H2P (Hail to Pitt)
#check
#pnew conversion rate:
#appending 1st,2nd and 3rd computer into one system
# selecting two columns # enter code to select two columns. It will look something like: dataframe[[column 1, column 2]]
# Setup Tweepy API Authentication
#looking for all AZs
# focusing on the Black Friday Weekend
#Find the highest number of returning stations
# Now let's clean up and change the index column to just be the actual data column, instead:
# checking whether the data has any null values
# Veamos una descripcion que incluya todo los casos (numericos y no numericos)
#Austin
# remove multiindex # users with only one prediction got NaN and it's OK
#R18df.rename({'Create_Date': 'Count-2018'}, axis = 'columns')
# Largest change in any one day
# a = 4.05 angstroms (Al fcc lattice constant) # Create cubic box (alpha, beta, gamma angles default to 90)
#We parse the date to have a uniform 
# year 2019 and up is clearly wrong
#Quandl_DF['Date_series'] = pd.to_datetime(Quandl_DF['Date'])
# We should make sure there are no collisions in column names.
# Okay, let's try a second example.
# histogram in matplotlib
#### Define #### # Remove all tweet record that have a non-null retweeted_status # #### Code ####
# get odds ratio for the age
# Removing the records with number of reactions less than 10 # Total number of reactions includes the likes so they have to be removed
# final dataframe
#-------IMport Countries-------#
#Describe the set of all sessions where a printer was used #Important features are mean, 50%, and 75%
#import plotly.tools as tls #tls.set_credentials_file(username='drozen', api_key='GTP8SX2KBqr3loYdTVb6')
#== By label: only one row -> dimension reduction 
#Show the data types of each column
# a 10-row mini test df
# Read the item's data
# Read data into a DataFrame
"""$ Group news by day of news_collected_time and concatenate news_titles$ """$
#Generate html table from df
# default value of size=100
# Remove the seconds in the timestamp column
# Split the label column out from the features dataframe # Sample the indexed DataFrame
#It's another video.
#reorder+rename cols before saving
#Grabbing first occurrence of scns_created, argmin should provide startdate
#trainDataVecs.shape #testDataVecs.shape #np.any(np.isnan(trainDataVecs))
#df.to_csv('df.csv')
#'Memphis': 'f995a9bd45d4a867'
# not only is Pandas much nicer, it also executes queries!
# un peu plus lisible : les CNI avant le 10 janvier, # en erreur ( status : False)
# read from Kenneth French fama global factors data set
#Remove duplicated rows by contractor_id #Remove duplicated rows by contractor_number
# Convert date in 'Timestamp' column from strftime to datetime
#Save Test Dataset as pickle for later use # the test data in pkl format
#weather_snow = weather_df['SNOW'].resample('1M').mean() #weather_snow = weather_df['SNOW'].resample('1M').mean()
# YOUR CODE HERE # raise NotImplementedError()
#total4
# try again with a tab separator # some errors occurred with missing values after the 600th row so I keep it small here, 100 rows #bg2 = pd.read_csv('/home/hbada/BGdata/Libre2018-01-03.txt', sep='\t', nrows=100) # pythonanywhere
# Describe is the basic summary stats function for numerical values.
# Every row in 'beta_dist' is a simulation of all the 'arms' in the experiment. # This step identifies the winning arm in each iteration, by calculating the MAX per row
#Create predictions and evaluate
#finds date of most recent and least recent tweet
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
# Locating row information for duplicate user_id
# Drop the rows with NAN values 
# which neighborhoods had an average response time of at least two weeks?
# TASK 2
# Find meaningless columns (<=1 categories)
# Fit pipeline 
# create a Series with a PeriodIndex and which # represents all calendar month period in 2013 and 2014
# Parse the resulting html with soup
# check shape
# now it would be nice if they were assigned to a few different days and the usual three meals # aka a DataFrame with 3 columns, and 7 rows # NumPy's ndarray can be reshaped - here's how it's done with a pandas DataFrame
#Which station has the highest number of observations?
#print the rows with index label 12
# number of patients over time, new and existings
# convert 'publish_time' to datetime format # the big 'Y' was key to making the 'to_datetime()' work
#Total retweets, favorites
#explore pandas dataframe work functions. 
# for each reprinted card, the first reprint has the completed printing/rarity dictionary, so we can get rid of every other duplicate
# .grep() allows you to search for string patterns within a column
# Find user with most tweets:
#convert to df so can change more times and also download as csv if wanted
# 索引对象不可变 # -> report error: Index does not support mutable operations
#read in the projects file
# write your code here
# Inspect number of rows and columns 
# What was the median trading volume during this year?
# Dump data to pickle # top_100_2016 = pickle.load( open( "top_movies_100_year2016_list.p", "rb" ))
# loop through all period objects in the index # printing start and end time of each
#firebase = firebase.FirebaseApplication('', None)
# Query for 1 week before `2017-08-23` using the datetime library
# get the first index
# keep only columns with more than # non-null rows
# create new DataFrame to select a time range # remove NaN values that result in the new DataFrame
#  ENCOURAGE RE-RUNS of this cell #  showing one-year of simulated price histories... #  ... like playing with a kaleidoscope.
# Create a Prophet object and fit the data
# concat grid id and temp
# merging #print(facebok_and_label.shape)
#Problem 2
#!pip install -U jupyter #from jupyter_core.paths import jupyter_data_dir jupyter_data_dir()
# Perform a second groupby command on the 'data_FCInspevnt_latest' table # Filter Inspection_duplicates for those with an a sum of the Inspection Number greater than 1, i.e. a duplicate entry
# shows time stability
# Construct and display pivot table
# logistic regression # fit model
# We will join on the email so we need to make sure the email address column is formatted in the same way
# Watsonライブラリの導入
#Loading Data
# Write the lambda function using replace # Write the lambda function using regular expressions # Print the head of tips
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# migrating table info # for row in table_rows: #     print(row.text)
## Fill NA values to avoid Null errors
# Checking out statistics for transaction order
#Compute accuracy on our training set
# Alternative solution, in Python 3.5 and above # Z = np.ones((5,3)) @ np.ones((3,2))
# we can see ~our~ most followed users,
# Step 2: Conduct Pivot Table on Data
# create time series for data
# adding back to df
# Inspect maximum number of comments in the recent scrape 
#2a. You need to find the ID number, first name, and last name of an actor, of whom you know only the first name, "Joe." What is one query would you use to obtain this information?
#cascading operations
# dateutil - utc special case
# Histograms can be crated from Pandas
#The number of times the new_page and treatment don't line up
# Average Order Value of _New_ Customers who made their first purchase after taking Discover # new_discover_sale_transaction.groupby(by=['Email', 'Created at'])['Total'].sum()
# set up interp spline, coordinates need to be 1D arrays, and match the shape of the 3d array.
# set list of columns/points on yield curve used
# The *self* parameter in *Customer* methods performs the given instructions. # For e.g. to withdraw
#Perform a basic search query where we search for the '#flu' in the tweets # Print the number of items returned by the search query to verify our query ran. Its 15 by default
# Ben helped me with this lovely for loop...
# check json errors
# Find the div that will allow you to retrieve the latest mars images
# 3.2.C OUTPUT/ANSWER #top 10
#Visualize the frequency of #hermosabeach was tweeted during the data collection period
#tidy_format = tidy_format.groupby(tidy_format.index.get_level_values(0)) #print(tidy_format.head)
#print(preprocessor('</a>This :) is :( a test :-)!' )) #print(re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', '</a>This :) is :( a test :-)!'))
# 按照原始数据显示 显示总数
#The date format is correct
# most common hashtags
# Create x, where x the 'scores' column's values as floats
#percentage
# Reading the dataset in a dataframe using Pandas
## Top 10 Bloggers
#col.assign(rounded_dt=weather.datetime.dt.round('H'))
# read excel file
# open the model
# execute query again # fetch the results and use them to create the dataframe
# thats games without vectors, but with labels
# load the data to a .csv
# Define endpoint information.# Define endpoint information.
# 1000010 is a standard id, so it haven't been considered as mispelled id in the dataset
#disable warnings
#Find current featured image
# Numpy broadcasting to perform cartesian product of nxn euclidean distances # https://stackoverflow.com/a/37903795/230286 # (n x 1 x d) - (n x d) -> (n x n x d)
# TODO plot main sequence
# Let's have a look
# Plotting one of the users...but I have 5 total users
# Country map to plot the transmission arrows on
# pivot dataframe for plotting
# Let's use the Support Vector Regression from Scikit Learn  svm package # using the defaults only
# Transforming dataset values # vetor de frequencia de palavras = repos freq. # Calcula matriz esparsa de similaridades User_LangsxUser_Langs
# We'll should also remove the double occurence of "_" in DATE__OF_OCCURENCE. Do so below: # New Code here # crimes.columns
#month pandas.DataFrame(data['11']['data'])
# Applying the lamba function on each row
# Add stop name from oz_stops as column in stops.
## The model was created above: ## It should be noted that they do not accurately specify the time constraints for these models!!!!!
# using the rest of the training data
#the WOEID of 2459115 is NEW YORK #url = 'https://api.twitter.com/1.1/trends/place.json?id=2459115'
#with open('../notebooks/subsample_idx_greedy_balanced.json', 'w') as fd: #    json.dump(list(idx_set), fd, indent=2)
#### dumping dict of data frame to pickle file
# get a list of the unique data types
# datetime conversion and created a new column NDATE
# Function to only select the upper case characters from a string
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Convert this cell to markdown and write your answer here.
# Salva o df com todos os tweets a partir da data de análise para um arquivo # Esse arquivo será usado para mostrar os tweets na visualização
def similar(a, b):$     """Get a similarity metric for strings a and b"""$
# doctors
#Got the run ID from Watson Environment.
#drop all rows that have any NaN values # general observation: NaNs in name_of_conference columns
# storing cities, states and counties as sets
# create a fitted model with three features # print the coefficients
# Create: vader df
# Getting phenotypes for huIDs that have associated genotypes
# Write to CSV
# note: although the strs and the converter # have format m-d-y the output has format y-m-d-h-min
# Now we are justing picking up the randome code without the '/watch?v=' in the string so we are getting  # from the 9th index to the rest of it. 
# myclient = InfluxDBClient(host, port, user, password, dbname) # myclient.create_database(dbname) # myclient.create_retention_policy('inf_policy', 'INF', 1, default=True)
# Use tally arithmetic to compute the absorption-to-total MGXS ratio # The absorption-to-total ratio is a derived tally which can generate Pandas DataFrames for inspection
# We import Pandas as pd into Python # We create a Pandas Series that stores a grocery list # We display the Groceries Pandas Series
#changed the name of columns
# Read in reflectance hdf5 file
# 3.2.C OUTPUT/ANSWER 
### Step 13: Create a scatter plot to compare the two data sets ## The clusters suggests 2 unique type of days
##categories can be considered counts over time. Which could then become ratios.
#preserve # Esta celda da el estilo al notebook
# builtins.uclresearch_topic = 'GIVENCHY' # builtins.uclresearch_topic = 'HAWKING' # builtins.uclresearch_topic = 'FLORIDA'
# Run in python console
#import pattern
# Logistic Regression.
# When both left and right has unique key values # The 'inner' is taken. key='zoo' in the right is not adapted into merge result.  
# Read OGLE data table
#Example 3:
# Requirement #2: Add your code here # updated by Huang, Silin ID: A20383068
#3 # the R^2 was lower on the test data compared to the training data
#Example 6: Specify question mark (' ?') value as missing values
# creating a copy of dataframe df # dropping rows where pages and group don't line up
#stop_words
# Read dataset
# export lm_withsubID
#file path
#Twitter accounts in the decreasing order of frquency tweeting about Donald Trump
# convert the boolean outproc_flag to numeric value Y/N == 1/0
# Create model # Compile model
# Add markers that are circles and of a certain size.
# Check all of the correct rows were removed - this should be 0
# Check for unique values in each non-id series
# Find the index of this maximum 'yob' (year of birth) row
# network_simulation[network_simulation.generations.isin([0])] # network_simulation.info()
#df_csv.info()
# graphe des sources à partir du fichier graphml # lit le fichier graphml comportant le graphe souhaité # détection de communauté
# validation set score
#find number of rows that has appointment - could be interesting number to get as well
## Additional: look at the distribution of daily trading volume
# query for the available stomatal resistance parameterizations
# We drop any columns with NaN values
# Extract the uid of the saved model
# State of the git repo (deep-review submodule)
# Instantiate a 2-group EnergyGroups object
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# what are our unique tags?
# Group data by death year and cause of death
# Creating a rate column with the extracted numerical rating  # WHAT AM I DOING?
# Imprimimos porcentajes:
# make new column for day. 
###YOUR CODE HERE###
# result = customer_visitors.groupby('Yearcol').mean() # result.customers_visited = np.round(result.customers_visited) # result
# Find and list the most active stations
# Write data to excel file in sheet with labeled with exposure time
#calculate the average across each time slice #calcuate the Mean Daily Rainfall for each Month #calcuate the Total Rainfall for each Month
# Graficar precios de cierre y precios de cierre ajustados
#Stores file in the same directory as the iPython notebook
#save the results in excel or show it #df_p.to_excel(str(keyword)+'20.xlsx', sheet_name='sheet1', index=False)
#Create a df from table extracted from webpage
# HACKER EXTRA - experiment with additional plots that help us understand this dataset
# Regular Expression 에서 split 기능 하나를 가져다 쓰자
# purpose
# determining the first real commit timestamp
#text_noun = myutilObj.tag_noun_func_words(Osha_AccidentCases['Title_Summary_Case'])
# analyze validtation between BallBerry simulation and observation data.
#request 2017 stock  #convert to dict 
# nan = 0
# Save corpus for later
# When we dereference an instance method, we get a *bound method*; the instance method bound to the instance:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
# Logging functionality
# how many rows, columns do we have now?
# DEFAULT, MERGES ON ALL OVERLAPPING COLUMNS
# using .div maxes sure you divide the corresponding indices, in this case business names #ratio fillna??
# Double check if our cleaner functions works well by looking at how many tickets by # of words
# Sill have duplicate data, also it created extra columns
#Find the index of opening price #Some opening prices were None
# We count the number of NaN values in store_items # We print x
# Sanity check: sort with commandline
#do the non RI credit charges contain all the instance types, can we get the prices from here? compare above^ #yes, so until we have a good way of pulling the billing data, we'll just pull if from this TTE billing
# Read csv
#print(pred) #print(clf.predict_proba(x_test))
# Visualización de longitudes:
#Show 100th row
#Load the saved csv file, reading the site_no column as a string, not a number.
# select fileid with the category filter
# could call out to OS and run this but sometimes I work across systems and it is easier to just cut and paste command line #  exiftool -csv="FILE GENERATED ABOVE" /path/from/root/to/your/images
#Multiply the genres by the weights and then take the weighted average
#r_train['has_extended_profile'].value_counts()
# Extract mean of length of tweets
# ciscih8
# Read dataset csv file downloaded from Kaggle website # Check the basic structure for all features
# quadratic features to use in our estimation
#Tells us the probablity of a Reddit will be correctly identified in the class its assigned
# Infer the schema, and register the DataFrame as a table.
# Update emoji dict. Eventually unicode-escape this.
#2 Convert the returned JSON object into a Python dictionary.
#checking np.arange array for use on next cell
# Get the shape and # of elements. shape() gives the number of rows (axis 0) # and the number of columns (axis 1). These axis indices are important later.
# count by date in datetimes
# target is the handle. # make trump 1 and sanders 0
#retrieve pages with request module
# Instantiate a Cell # Register bounding Surfaces with the Cell # Fill the Cell with the Material
# tushare包
# Using the API object to get tweets from my timeline, and storing it in a variable called public_tweets
# select all the non-numeric columns # get the list of all the non-numeric columns
# Choose a start date and end date for your trip. Make sure that your vacation range is approximately 3-15 days total.
#plot = df['6/27/2018':'6/28/2018'].plot(y='lux')
# to get the last 12 months of data, last date - 365
# generate train/test data
# abs is actually a numpy function so it can also be implemented as follows
# treatment group converting mean
# Columnas
# Lets add a new column to the dataframe
# Cut postTestScore and place the scores into bins # Creating a group based off of the bins
# convert strings to dates using datetime.strptime:
# Cleaning up DataFrame for calculation  # Lagged total asset # Average total asset - "rolling" 
# model from previous logistic regression
# Divide each number by each countries annual maximum
#Show the slice of rows spanning september 10th thru 15th, 1998
# query to pull the last 12 months of precipitation data
#date_splits
#check if there is duplicated name; #The name is unique per record
# Create a pandas dataframe from the model predictions (prev numpy array)
# Repeated part from morning track: impute with the mean 
#print(agg.fit_predict(locationDistanceMatrix_norm)
# Create the Dataframe for storing the SQL query results for last 12 months of preceipitation data # Get the count of total number of records in the dataframe
# Using the groupby method calculates aggregated DataFrame statistics
# in our case we have more than 2 dataframes so the following code will merge more than 
#random forrest classifier
# Remove the bullet point # Save the first name as its own column # Save the last name as its own column
# Convert all of the pushed_at dates to time # Normalizing to the earliest date.
# Set legendary False = 0, True = 1
# plot autocorrelation function for first difference RN/PA data
# then, we update the stop-words list to see the result
#data = pd.read_csv('BCH_credid.csv', sep=',', header=None)
#del df.index.name
# Preview the ratings dataframe
#'Seattle': '300bcc6e23a88361'
#Get the top 10 locations of the tweet authors (include no location listing)
#concatenate the sentiments_df, then export it to csv, and show the completed dataframe
# Retrieving statistical information
# Initialize empty dataframes
# Combine ET for each rootDistExp # add label 
# get the dataset we just uploaded
# Create the inspector and connect it to the engine # Collect the names of tables within the database
# Supposedly this AppAuthHandler is faster since it doesn't authenticate
# Summary statistics for the precipitation data
#stopword_list.remove("ich")    #Remove "ich" from the list.
# Before we merge the two datasets, let's ensure there are no duplicate rows in the user information dataset
# flight7 = flight7.na.drop()
#== By Label: Same as above, but faster 
# days from today until a given day
# results are returned as an iterable list
# Force the Frequency_score column to a numeric data type as it should be
# Show model training parameters
# check inserted records
#calling only 2017 from api
#write your dataframes to a csv file, if you want to be able to use it later. # Write out the DF as a new CSV file
# How many stations are available in this dataset?$ stations_df = pd.read_sql("""SELECT * FROM station""", engine.connect())$
#change date fields to date types in schedule tab #Convert all nan observations to 1/1/1900 so they can be easily  #identified and excluded
# We create a Pandas Series that stores a grocery list of just fruits # We display the fruits Pandas Series
# うんこツイートなしでもう一度グラフで可視化してみる
## daily normal 
# read csv file
# Change the name of first row from "Unknown" to "rowID"
# ...and plot it against time
# affair =0 means there is no affair and rating are good (>=4) #and womens having affair have rated thier marriage <=3.5 on an average 
# Defenders
# For demo purposes we can look what our invoice looks like
#stats.normaltest(model_arima121)
# Visit URL
#y_test_array = y_test.values.reshape(-1, 1)
# groupBy function usage with the columns we group to become as the index.
#query tables to get count of daily report, all temp data is complete for each record, so the count #reflects a count of a station giving temp data, prcp data may or may not have been reported on that date
# Converting to Timestamps
# Optional: Create an output folder to store your predictions
# Create ARIMA model
# Suburban cities total drivers
#To find value of n_new
# save csv file as megmfurr_tweets
# Remove NaN values
# drop last row
#9.7 Delete Object Properties # You can delete properties on objects by using the del keyword:
#one more filtering criteria:  no missing values.  The last_event column is a datetime object, #and if there were any groups with no past events, this column will be a NaN (missing value).
## Paste for 'construction_year' and plot ## Paste for 'gps_height' and plot
# cannot validate a boolean as a string
# We can resample the 15 minute data to make it hourly # this resamples a datetime index to hourly and averages each value within the hour 
# Grab the data
# alternative: assign method (input arg must be a valid statement # that could be dumped on the prompt)
# Read gzipped file in a dataframe # compression="infer" tells pandas to look at file extension, which is "gz"
# extend the stop-words list
# count the number of times each GPE appears
# Extracting CNN news sentiment from the dataframe
# Convert into log2(tpm+0.001)
# are all values in each row less than 8?
# notice the change in encoding in the output
# Now we'll try "device_id" - about 700k values
# again, default is to generate an error
# view the head of our df
#Akron
# https://stackoverflow.com/questions/40894739/dataproc-jupyter-pyspark-notebook-unable-to-import-graphframes-package
#get projection information from windfiled
# limit the query using WHERE and LIMIT
# running in one Core
# Creates a dictionary based on the sentence tokens
# Hmm...in this case, we want the Date column to be our index, so let's try again
# What are the most active stations? # List the stations and the counts in descending order.
# proportion of p_diffs greater than the actual difference observed in ab_data.csv is computed as:
# my fire size model doesn't predict any fires over 100; or size class C.  # this is a limitation
# check the frequency is BusinessDay
# monthly data
# get indices for the rows where the misc headers are present # these will indicate the end of data
# if you want more fields brought through to the final final just add more lines here
# All tweets's retweeted value is 0
#So we do have some duplicated permit numbers. How many duplicates are there?
# Write to CSV -- to insure code runs if there is no internet connectivity and API can't be called
# Drop the irrelevant columns
# each key of 'words' is a word, each value its index # printing some random key:value pairs of 'words'
# Where would trades be if only bought when predicted return > 0.01% and sold when < -0.01%
#plot histogram
# you can label the rows in the data frame
# Create invalid data
# filter out Twitter links
# Set up logistic regression # Calculate results
# create an isntance of the ModifyModel class 
# Convert datatype of created_at column for better manipulation
# Export to csv
# Standard deviation of the residuals.
#feature selection
# Load a single column for rows 0 to 100
# plot the sum by day for June 2015:
# return a frame eliminating rows with NaN values
# inspecting df2
#importação das bibliotecas necessárias # arquivo com os dados da planilha excel
# Creating a df of just the nullse
# Random Forest Model 
#Wrapped with pd.DataFrame() for pretty printing
# Get a list of column names and types # columns
# Start to extract the number out of the Notes column. # Replace the hyphen in '15-day' with a space to help splitting and extracting digits.
# number of events, articles and mention_date of the dataset
# dependent variable #y = df.rating
#checked values using (len(new_page_converted))
# Creating new df with only a few wanted cloumns
#run ML flow
#Converting timestamp to a true datetime64 format
# This is where we break the json file to a dataframe with its individual cols
# ahora leamos nuestra copia. Todavia no incorporamos los dates en la importacion.
# if we'd rather have the grouping variables as columns # (not as index):
# Tokenize tweet data in the dataframe
#Rename columns.
# plot fixation counts
# Upload files to the buckets.
# For Displaying Data
# df_2001
#Dallas': '18810aa5b43e76c7'
# can sort order using index
#probabilities are skewed left, 
# extracting environment variable using os.environ.get
#To save excel file 
# Identify possible issues with some columns
#import bible text
# Read All 5M data points # Extract the bodies from this dataframe
#Sharpe Ratio #Menor SD
# end month frequency from given quater
# check for unbalanced classes - the others should be balanced 
#Set index to date
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse # into the JSON structure that will be returned
#print the first two parts: overall introduction and timely publication
# export latest elms_all
# Estimate 'y' for these 'x' values and store them in 'estimates'.
# Verify DataFrame:
# rf.fit(features_class_norm, paid_status_transf)
# Averaged predictions
# to find unique user_id
#Arlington
# printing first five rows of the data frame #df.head()  # 5 is the default value. we can ask for any number as shown below
#Store-store_states (checking if there was any unmatched value in right_t) #Weather-state_names
# converting date to datetime
#stories.drop('tags', axis=1)
#perform delete
# create a safe copy #Category: Music
# get just the cgm data # look at the first 5 rows of data
# Create a dataframe that just consists of the songs that make it to number one in at least one region
# Check for any missing data
#map titles to edge From column
# Reading from pickle # read the data as binary data stream
# Transform all words to lower case
# recuperar los tipos de datos de cada columna
# Mean calculated on a DataFrame
#pop_df_3.index
#Set index to date
# introduce some condition, e.g. only users with 14 or more consecutive days
#eliminate duplicate measurments
# Lemmatize verbs by specifying pos
# 'supercontig' types columns represent unassembled sequences # (incomplete/total) * 100
# appointments = pd.read_csv('Appointments.csv')
# Shuffle the data set
# Count tweets per time zone for the top 10 time zones
# 返回用户设定的文本数据编码 # 文档提到this function only returns a guess
# check option and selected method of (12) choice of hydraulic conductivity profile in Decision file
#Next we will read the data in into an array that we call tweets.
# Concatenating the two pd.series
# Match project id with project name
# Use Inspector to print the column names and types for measurement
#Converting JSON object to a python dict
# demo interpolating the NaN values
#Plot the distribution
#Understanding the total amount of time for the postings
# download the text
#Percentage missed for each tip level #Negative means that actual > predicted: 80% of time, predicted tip was too low
# split features and target
#now the information is empty because i ran the command after dropping the duolicated row
# We print percentages:
# inspect duplicate userid
# add porn column and remove extra headings
# How many complaints actually contain the words "injured" or "hurt"?
#1 #data= quandl.get("FSE/AFX_X",start_date="2017-01-01",end_date="2017-12-31") #data
# Our "Date" looks like strings.
# Convert the returned JSON object into a Python dictionary
# load the string into a list, and remove duplcates by casting the list as a set.
# Examine GDP Series
#  如果/100 然後剩乘100，就可以看股價在100、200、300、的次數
# And now load the data. Not needed if it is all run in one piece, but useful  # if the notebook needs to be re-run/debugged in multiple iterations.
# create timestamp index
# get new and existing patient dataframes
# Alternative way to get the date only 
##### sort by label on column lables
#Design a query to calculate the total number of stations.
# Use Pandas to calcualte the summary statistics for the precipitation data
# Urban cities average fare
# Print the columns of df
# load json twitter data # Convert to pandas dataframe
#Export Canadian Tire file to local machine
# clean up whitespace # clean up spelling
#Save node information. The columns to be saved in this file are set by the user. Note that node_type_id is needed here
# Examine purchases here
# determine the order of the AR(p) model w/ partial autocorrelation function of first difference, alpha=width of CI
# series with all NaN
# Identifying the top 10 authors # Listing contents of 'top_10_authors'
# Check for missing data - none
# analyze validtation between Jarvis simulation and observation data.
# Take a look at the words in the vocabulary
#find average by dividing HP total by count total
# To avoid ambiguity between index label and index, we can use the loc or iloc attributes. # loc means location and points to an index label # iloc mean integer location and points to a numeric index
# take only elements that belong to groups that have less than 5 members
# finally save to preprocessed folder
#merging the particpants column into the base table
#Concatenate (or join) the pre and post summary objects
# Write to disk to prevent us to have to hit the API again
# import ab_data.csv and display first 5 rows
# Now, train the model
#reading in csv files
#Chesapeake
# No need to add ab_page, as new_page will always have treatment, we'll end up having singular matrix
#Execute zhunt with the arguments windowsize, minsize, maxsize and datafile...
# Make Predictions
# experiment実行状況の取得
# Create BeautifulSoup object; parse with 'html.parser'
# Fit the model
# Create the plot
# as it can be seen, the star should be in the stop word list
# Calculate Driver Percents
# Train model
# default DatetimeIndex and its Timestamps do not have # time zone information
# example of a cross-listed dealer
## Note the type, its dict ## Dictionary have key dataset and nested key data. Lets see "column_names" details
# Now we load the data from the query into a pandas DF: # call it "rain_df":
# melihat hasil perubahan zona waktu di kolom index
# posts[posts['PostTypeId'] == 2].groupby('CreationDate')['Body'].count().plot(alpha = 0.3, color = 'r') # posts[posts['PostTypeId'] == 1].groupby('CreationDate')['Body'].count().plot(alpha = 0.3, color = 'g')
# by repeated tweet
## ------ Avergae Polarities of each news source ------ ##
# Ordeno de índice más antiguo a más moderno
# instantiating weather feature dataframe
# mode calculated on a Series
#Remove unnecessary columns
#Before cleaning data process begin, I will copy the contractor table to contractor_clean table  #to check original data or roll back the operation. 
# Here's where I apply the train test split
### train + items + item_categories + shops Join
# Stacking the lists column wise, so that they can fit in shape in the dataframe.
#Saving the model to HDFS
# this cell will evaluate silently 🙊, and not print anything.   # This is desired, because a person with your keys can act as you on Twitter in literally every way 😟
# check the simulation start and finish times
# selecting two columns
#dates = sm.tsa.datetools.dates_from_range('1980m1', length=nobs)
# correct joining
# Break the text content apart for better formatting.
#adopted_cats.reset_index()
# percentage of ratings above 4.0
#add it to the csv
# Store which indexes need to be removed #check total row count
# From a list with an implicit index
#agg([np.sum,np.count]) #sum() #agg({'col5':np.sum,'col6':np.sum})
#Clear to see that the Monetary_score column contains data outliers
# export/ create the processed dataset
# 8. Print the mean education for each age_cat using groupby.
# Delete items (returns a copy with deleted item -out of place!)
# shift forward one business day
# convert pandas data frame to arrow table
# Export collected weather data to csv file
#Convert dataframe to list
#Apply functions to create new columns in dataframe 
# Load the model that we created in Part 2
# Modified the dataframe being evaluated to look at highest close which occurred after Acquisition Date (aka, not prior to purchase).
# apply the function to the dataframe rowwise
# Never forget to map
# accediendo a files y columnas por etiquetas
# extract the price data only # extract the column headings and convert to a row:
# Compute the probability of failure for different temperatures
# How many stations are available in this dataset?
# Find the more info button and click that
# go to FDIC site
#cur.execute('UPDATE actor SET first_name = \'HARPO\' WHERE actor_id = 172;')
# Get all titles that have "SOFTWARE" or "PROGRAMMER" in them
## Solar Flare 2017-09-10 
# Put the columns to be predicted, at the end
#2 drop duplicate records
#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 6)
# Start Chrome with Selenium
# Remove duplicates
#'Portland': 'ac88a4f17a51c7fc'
# read in the iris data # create X (features) and y (response)
# stacking predicted probabilities by mean
#check to ensure all the dummies were created successfully
# the number of reports from the most active station
# Retrieve the parent divs for all articles
# AUROC Score
# Requirement #1: Add your code here
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# Confirm that you have 8 values for each column.
# Reading dataset after ignoring initial space by skipinitialspace=True
# We display the updated dataframe with the new column:
# Helper function to add to spelling dictionary
# 5. What was the largest change between any two days (based on Closing Price)?
# Many of these are just MAGA related # Let's see how many
# preprocessing and cleaning  # remove NaN and NA from dataframe
# Labels - row: 'CHARLOTTETOWN', column: 'Wind Spd (km/h)'
# Loading data
#this is the plot for the last year
# Índices
#Chicago
#new_messages.timestamp = pd.to_datetime(new_messages.timestamp, unit='ms') #new_messages.watermark = pd.to_datetime(new_messages.watermark, unit='ms') #new_messages.datetime_created = pd.to_datetime(new_messages.datetime_created)
# Alternative way to get the date only 
#Visualize results
# Inspect the JSON structure of the object you created, and take note of how nested it is, # as well as the overall structure
# remove 'O' from the labels # labels.remove('O') # labels[:5]
# We can rename the column to avoid the clash
#Check that the filters worked
#Select only one topic per group
# Create Excel File
# Store the map_info into a key of the same name
# they do intersect, so create an overlay with a 'union'
# Calculate the interquartile range (IQR).
# Let's get back to the data frames we had before
# Officer citations are wayyy skewed
hotel_query = """ $         """$ booking_df = snowflake_query(hotel_query)
# Determine a monthly profile to be used by cities that are not covered. # Because the above data came from PCE, non-covered cities are primarily Urban. # Take the average of the Hub cities that have annual usages > 500 kWh/month
# Latest Date
# Which GPU am I using?
# analyze validtation between BallBerry simulation and observation data.
# Make sure to also output the intermediary steps
# Plot the temperatures for the week
#Remove Duplicates (don't keep any posts which appear more than once)
# fraction of volume for every coin
#INT is the contact information for INTERACTIONS #INT=pd.read_csv('C:/Users/sxh706/Desktop/Interactions.by.Month/2018/June/Interactions.Contacts.csv',skipfooter=5,encoding='latin-1',engine ='python')
#Testing on a sample text
# Create a new year column
# The Directory to save the csv file.
# check the structure of the dataframe
#proportion of p_diffs greater than act_diffs
#First let's load meta graph and restore weights
# Read the filtered tweets from the .txt files
