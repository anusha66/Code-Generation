print(pd.DataFrame(train_matrix).head())
pp.pprint(metadata_dict)
cars.head() $
X_train, X_test, Y_train, Y_test=tts(X,Y, random_state=0)
apple.set_index(rng, inplace = True) # This sets our index to the range that we just created $ apple.head() # And we have our datetime index 
print (z_score) $ print (p_value_ztest)
joinresult = data.join(meta, on="BARCODE") $ joinresult.reset_index(inplace=True)
greater_than_diff = [i for i in p_diffs if i > p_diff]
file_list = drive.ListFile({'q': "'1vrFHzEYTZkF_vJvn7KYh6OhYLphLvzsX' in parents and trashed=false"}).GetList() $ for file1 in file_list: $   print('title: %s, id: %s' % (file1['title'], file1['id']))
openmc.run()
people_person['date'] = people_person['date'].astype('str')
df_tte_all[df_tte_all['UsageType'].str.contains('UGW')]['ItemDescription'].unique()
matrix = tf.fit_transform(corpus)
import re $ for j in range(len(tweets)): $     tweets[j] = tweet_cleaner(tweets[j]) $ print(tweets)
yc_new2[yc_new2.tipPC > 100]
tree_features_df['p_hash'].isin(manager.image_df['p_hash']).describe() $
dfNiwot.head()
xgb_md = XGBModelData(trn_df, val_df, trn_y, val_y)
!pip install xlrd
df_tweets[df_tweets.no_conversation == True].head()
taxiData.tail(5)
X = pd.merge(X, meals[['id','menu_id']], left_on='meal_id', right_on='id', how='inner') $ del X['id'] $ X = pd.merge(X, menus[['id', 'is_menu_style_not-defined', 'is_menu_style_finedining-elegant', 'is_menu_style_semicasual-upscale', 'is_menu_style_casual-homestyle']], left_on='menu_id', right_on='id', how='inner') $ del X['id'] $ del X['menu_id']
baseball_swing_action_type_id = 1 $ url = form_url(f'actionTypes/{baseball_swing_action_type_id}/eventMarkerTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_status(response)
disposition_df=pd.read_excel("Call Records.xlsx",sheetname="Disposition Definition",na_values=["NULL",""," "],keep_default_na=False).iloc[:,1:]
aug2014 = pd.Period('2014-8',freq='M') $ aug2014
master_list = pd.DataFrame( $     {'Word': vocab, $      'Count': dist, $     })
p_old = df2.converted.mean() $ print('The convert rate for old_page under the null is: {}.'.format(round(p_old, 4)))
p_diffs = np.array(p_diffs) $ (p_diffs > actual_diff).mean()
import tclab
subtes.head(10)
rdf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1) $ rdf_model = rdf.fit(X_train, y_train)
df["pickup_end_of_month"] = df["pickup_week"].isin([4, 5])
from statsmodels.tsa.stattools import adfuller as ADF $ print() $ print(ADF(resid_6201.values)[0:4]) $ print() $ print(ADF(resid_6201.values)[4:7])
df.describe()
df_all_backup = df_all.copy()
ser5.iloc[[1,2,3]]
scaler=StandardScaler() $ scaler.fit(x_train)
import csv $ data = list(csv.reader(open("test_data//dummy.csv"))) $ print(data)
S_lumpedTopmodel.meta_basinvar.filename
p_new_sim - p_old_sim
run txt2pdf.py -o "FLORIDA HOSPITAL  Sepsis.pdf"   "FLORIDA HOSPITAL  Sepsis.txt"
df.isnull().sum()
keywords_dataframe = pandas.DataFrame(Counter(keywords).most_common(40), columns=['keyword', 'count'])
autos['odometer_km'].unique().shape
so = (np.sort(-m3, axis=None)) $ rsf = so.reshape(2, int((m3.size)/2)) $ sorted_m3 = -rsf $ print("sorted m3: ", sorted_m3)
df_archive_clean.source.value_counts()
con = sqlite3.connect('db.sqlite') $ con.execute("CREATE TABLE tbl(wikipedia TEXT, topic TEXT, year INTEGER, month INTEGER, pageviews INTEGER);") $ con.commit() $ con.close()
print mike.ra[0] $ a=Angle(mike.ra[0], u.hr).to_string(unit=u.degree, precision=5, decimal=True) $ print float(a) $ b=Angle(a, u.deg).to_string(unit=u.hr, sep=':', precision=1, pad=True) $ print b
display('df5', 'df6', $        "pd.concat([df5, df6], join='inner')")
londonDFSubset[londonDFSubset['POPDEN'] > 22000]
violations08Acounts / boro_counts
bmp = pd.Series(brand_top_mean_price).sort_values(ascending=False) $ bmm = pd.Series(brand_top_mean_mile).sort_values(ascending=False) $ bmp_bmm_cor = pd.DataFrame(bmp, columns = ["mean_price"]) $ bmp_bmm_cor["mean_milage"] = bmm $
drg_string_value = '001' $ year = 2015 $ x = df_everything_about_DRGs[(df_everything_about_DRGs['drg3_str'] == drg_string_value) &\ $                         (df_everything_about_DRGs['year'] == year)].index.tolist() $ print('There are',len(x),'values for DRG',drg_string_value,'in',year)
round(df1['converted'].mean(), 4)
nold = 145274
ordered
pos_tfidf = models.TfidfModel(pos_bow, id2word=pos_dic) $ neg_tfidf = models.TfidfModel(neg_bow, id2word=neg_dic)
df[df.msno == '++5wYjoMgQHoRuD3GbbvmphZbBBwymzv5Q4l8sywtuU=']
courses[1]
-33.015542, -69.726007
def load_rainfall(query): $     dc_rf =datacube.Datacube(config='/g/data/r78/bom_grids/rainfall.conf') $     rf_data = dc_rf.load(product = 'rainfall_grids_1901_2017',**query) $     return rf_data
model = H2OAutoEncoderEstimator(activation="Tanh", hidden=[50, 50, 50], $                                 ignore_const_cols=False, epochs=100)
t=mini_df.groupby(['wait', 'month']).mean()
reddit['Hour of Day'] = np.abs(reddit.Time.dt.hour - 5) 
matches = pd.read_sql_query('select * from Match', conn)  # specify the connection $ print(matches.shape) $ matches.head()
len(pd.unique(ratings['movieId'].ravel()))
display('df1a', 'df2a', 'df1a.join(df2a)')
findM = re.compile(r'wom[ae]n', re.IGNORECASE) $ for i in range(0, len(postsDF)): $ 	print(findM.findall(postsDF.iloc[i,0]))
df3 = df3.drop('wk1', axis=1)
people.sort_index(axis=1, inplace=True) $ people
hemispheres_url= "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(hemispheres_url) $ html = browser.html $ soup = bs(html, "html.parser") $ mars_hemisphere = []
weather_mean[['Rel Hum (%)', 'Pressure (kPa)', 'Temp (deg C)']]
payment = pd.get_dummies(auto_new.Payment_Option) $ payment.head()
pickle.dump(lda_tfidf_df, open('iteration1_files/epoch3/lda_tfidf_df.pkl', 'wb'))
ds_info = ingest.upload_dataset(database=db, dataset=test) $ ds_info
twitter.info() $
id_range=range(results.index.values.min(),results.index.values.max()) $ results.reindex(id_range).head() $
timedog_df = tdog_df[tdog_df.userTimezone.notnull()] $ timedog_df = timedog_df[['Hashtag', 'tweetCreated', 'tweetFavoriteCt', 'tweetID', 'tweetRetweetCt', 'tweetSource', $                          'userID', 'userLocation', 'userName', 'userScreen', 'userTimezone']] $ timedog_df.head() $ timedog_df.size
feature_df = cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']] $ X = np.asarray(feature_df) $ X[0:5]
data[ $     get_child_column_names(observations_node) + $     get_child_column_names(observations_ext_node) $ ].T
bo[1]
new_page_converted = [] $ for _ in range(n_new): $     a = np.random.binomial(1, p_new) $     new_page_converted.append(a) $ new_page_converted = np.asarray(new_page_converted)
site_dct
twitter_merged_data.hist(column='rating'); $ plt.title('Rating Histogram') $ plt.xlabel('Rating') $ plt.ylabel('Count');
r = requests.get(url)
url = "http://oglobo.globo.com" $ html_txt = urllib.request.urlopen(url).read() $ soup = bs(html_txt, "html.parser") $ [line.get('href') for line in soup.find_all('a')][0:10]    
joined = pd.read_sql_query(sql_query, engine) $ joined.head()
df.groupby('hash_id')
df_master = pd.read_csv('twitter_archive_master.csv') $ df_master.head()
result_df.shape
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     my_group = f.get_node(where='/my group') $     print(my_group) $     my_group = f.get_node(where='/', name='my group') $     print(my_group)
!pip install scikit-image $ !pip install pandas $ !pip install torch $ !pip install torchvision
import folium $ from folium import plugins
len(news_df)
df_enhanced['source'].nunique()
df_bug = df_gt[(df_gt[u'Service Location'].isin(bthlst)) & \ $                (df_gt[u'Equipment Sap Code'].str.match('^PPG-R')) & \ $                (df_gt[u'Request Status'] == 'Closed')][my_cols]
if len(active_psc_records[(active_psc_records.company_number.isin(active_psc_statements[active_psc_statements.statement.str.contains('no-individual-or-entity-with-signficant-control')]\ $                                             .company_number))]) > 0: $     print('It\'s possible!') $ else: $     print('Not possible!')
print(avg_preds[0]) $ print(type(avg_preds[0])) $ print(type(list(avg_preds[0]))) $
df_m.loc[df_m["CustID"].isin([customer])]
df.groupby('raw_character_text')['episode_id'].nunique().reset_index().head()
zipShp = gpd.read_file(os.environ.get('PUIDATA') + '/zipcodes/ZIP_CODE_040114.shp')
image_coords = ~affine * (x, y) $ imagex = int(image_coords[0]) $ imagey = int(image_coords[1])
autos['date_crawled'] = autos['date_crawled'].str[:10] $ date_crawled_count_norm = autos['date_crawled'].value_counts(normalize=True, dropna=False) $ date_crawled_count_norm.sort_index()
translated_expression[2]
scores, metrics = pipeline.test(ds_valid, 'Label') $ print("Performance metrics on validation set: ") $ display(metrics)
get_response('Why must I prove anything to you?')
print (autos['price'].value_counts().head(10)) $ print(autos['price'].value_counts().tail(10) )
f = open('my_filename.txt','w') $ f.write(str(a)) $ f.close()
conn.dropcaslib('research')
df2[df2['group'] == "treatment"].converted.mean()
df.groupby("loan_status").aggregate([np.mean, np.std])
print("Number of Software in ATT&CK") $ software = lift.get_all_software() $ print(len(software)) $ df = json_normalize(software) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
import os $ data = pd.read_csv('https://raw.githubusercontent.com/ogrisel/parallel_ml_tutorial/master/notebooks/titanic_train.csv', $                     sep = ',')
print("RF Accuracy: ") $ print(accuracy_score(score['is_shift'], np.where(score['pred_prob_rf']>=.25, 1, 0))) $ print('Logit Accuracy:') $ print(accuracy_score(score['is_shift'], np.where(score['pred_prob_lr']>=.4, 1, 0)))
def datetimeinvertedstring2datetime(text): $     try: $         return  pd.to_datetime(text,format='%Y-%m-%d %H:%M:%S') $     except AttributeError: $         return text 
iris_new = pd.DataFrame(iris_mat, columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','Species']) $ iris_new.head(20)
pd_aux2['diff']=pd_aux2.issued_date-pd_aux2.permit_creation_date
aldf.head()
df1_clean.source.unique()
autos["offer_type"].value_counts()
df2['intercept']=1 $ df2[['ab_extra','ab_page']]=pd.get_dummies(df['group'])#creating dummy varibles $ df2.drop(['ab_extra'],axis=1, inplace=True)#droping one of them for maintaining the rank $ df2.head()
final_topbikes.groupby(by=final_topbikes.index.hour)['Distance'].median().plot(kind='bar', figsize=(12,6))
df.query("group == 'control' and landing_page != 'old_page'").shape[0] + df.query("group != 'control' and landing_page == 'old_page'").shape[0]
e_p_b_one = cfs_df[cfs_df.Beat=='5K02'] $ e_p_b_one = e_p_b_one.groupby(e_p_b_one.TimeCreate).size().reset_index()
my_house_price = 800000 $ estimated_squarefeet = inverse_regression_predictions(my_house_price, sqft_intercept, sqft_slope) $ print('The estimated squarefeet for a house worth {} is {:.2f}'.format(my_house_price, estimated_squarefeet))
example_document = index.document(index.document_base()) $ print(example_document)
train['smart_author'] = train.author.isin(smart_authors.index).astype(int) $ train.groupby('smart_author').popular.mean()
df_parties = df[df['Descriptor'] == 'Loud Music/Party'] $ df_parties.groupby(df_parties.index.hour)['Created Date'].count().plot(kind="bar") $
from w2v_cal import get_doc_matrix $ test_matrix = get_doc_matrix(model, test_clean_token) $ train_matrix = get_doc_matrix(model, train_clean_token)
KoverY_50_alphaquarter = __ # In the simulation run boosting the savings rate $ KoverY_50_alphathird = __ # In the simulation run boosting the savings rate $ KoverY_50_alphahalf = __ # In the simulation run boosting the savings rate $ KoverY_50_alphatwothirds = __ # In the simulation run boosting the savings rate $ KoverY_50_alphathreequarters = __ # In the simulation run boosting the savings rate $
allFiles = glob.glob('./data/geocoded_loc/geocode_*.csv') $ dataframes = (pd.read_csv(f, index_col=None, dtype=object) for f in allFiles) $ df_geo = pd.concat(dataframes, ignore_index=True).drop('Unnamed: 0', axis=1)
df2['landing_page'].value_counts(normalize=True)
autos.describe(include='all')
data.loc[(80, slice(None),'put'),:].iloc[0:5,0:4]
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','control']]) $ results = logit.fit() $ results.summary()
arr = Array{Int}([1,2,3,2]) $ str = "hello, world!" $ ios = open("chess_moves.txt", "r") $ d = DateTime(2016,11,14,16,45) $ funcArr = Array{Function}([max, min, sum, sort!])
props = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
X_tr, X_val, y_tr, y_val = train_test_split(X_num, y, random_state = 3)
plt.plot(runningMean(track.speed_fit, 100))
metrics.name.value_counts()
m.plot(forecast);
def get_list_tot_comments(the_posts): $     list_tot_comments = [] $     for i in list_Media_ID: $         list_tot_comments.append(the_posts[i]['activity'][-1]['comments']) $     return list_tot_comments
dataset = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=45)
year14 = driver.find_elements_by_class_name('yr-button')[13] $ year14.click()
retweets = twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()==False] $ twitter_archive_clean.drop(retweets.index, inplace=True) $ twitter_archive_clean.reset_index(drop=True,inplace=True)
df2=df2.drop_duplicates('user_id')
df2[df2['E'].isin(['two','four'])]
dataset.head()
c = pd.read_csv('countries.csv') $ c.head()
new_page_converted=np.random.binomial(nnew,nullrate) $ new_page_converted
sum(contractor.city.isnull())
trend_de = googletrend[googletrend.file == 'Rossmann_DE']
reference_frame = proc_rxn_time[['Subject First Name', 'Subject Last Name', 'subject_id']] $ reference_frame.columns = ['Subject First Name', 'bhc_id', 'subject_id']
first_row = session.query(Measurement).first() $ first_row.__dict__.get('prcp') $ one_year_prcp = session.query(Measurement.station, Measurement.date, Measurement.prcp, Measurement.tobs)\ $ .filter(Measurement.date >= '2016-08-23').all() $ one_year_prcp[0]
all_lum = LUM.process_lum(etsamples,etmsgs) $ all_lum_binned = LUM.bin_lum(all_lum)
df_new_npage = df_new.query('ab_page == 1') $ df_new_npage.tail(10)
plt.plot(pipe.tracking_error)
users['DaysActive'] = (users['LastAccessDate'] - users['CreationDate']) / np.timedelta64(1, 'D')
%matplotlib inline
from IPython.display import SVG $ from keras.utils.vis_utils import model_to_dot $ SVG(model_to_dot(encoder_model_inference).create(prog='dot', format='svg'))
total_sales.ratio.max()
plot_reorder_kde(HYB_reorder_stats['mean'], 'kernel density estimate for HYB re-order intervals')
zip_check(df.zip)
periods = 31 * 24 $ hourly = Series(np.arange(0,periods),pd.date_range('08-01-2014',freq="2H",periods=periods)) $ hourly
%sql \ $ SELECT avg(retweet_count + twitter.like_count) AS heat \ $ FROM twitter \ $ WHERE user_id = 42869268;
df = df.sort(columns=['Year', 'Month ID'])
n_neg = n_pos * 10 $ train_neg = train_sample.filter(col('is_attributed')==0).orderBy(func.rand(seed=seed)).limit(n_neg).cache() $ print("number of negative examples:", n_neg)
gender_freq_hist = pd.crosstab(index=goodreads_users_df["gender"], columns="count") $ gender_freq_hist['gender_freq'] = gender_freq_hist['count'] * 100 / gender_freq_hist.sum()['count'] $ gender_freq_hist = gender_freq_hist.sort_values('gender_freq', ascending=False) $ gender_freq_hist.head(10)
vocab = np.array(vectorizer.get_feature_names())
merged_data['paid_status'].value_counts()
X = incident_df[incident_df.columns.drop(['Label'])].copy() $ y = incident_df['Label'].replace(incidentTypesDict) $
tia['date'] = pd.to_datetime(tia['date'])
pd.value_counts(df['completed_by']).plot.bar()
model.add(Dense(4, activation='softmax'))
def datetimestring2datetime(text): $     try: $         return  pd.to_datetime(text,format='%d-%m-%Y %H:%M') $     except AttributeError: $         return text 
save_obj('translated_ham_tweet', df_ham)
df2[df2['user_id'] == repeated_id]
term_freq_df.columns = ['negative','neutral','positive'] $ term_freq_df['total'] = term_freq_df['negative'] + term_freq_df['neutral'] + term_freq_df['positive'] $ term_freq_df.sort_values(by='total', ascending=False).iloc[:10]
accuracy = (4936+7695) / (4936+1074+598+7695) $ accuracy
StockData.reset_index(inplace=True)
train.isnull().sum()
bnbx = bnb $ bnbx.shape
df_c = pd.read_csv('countries.csv')
t + d
tags = df.loc[:, 'tags --> ...':].apply(lambda x: x.dropna().tolist(), axis=1) $ df = df.drop(df.columns.to_series()['tags --> ...':], axis=1) $ tags = tags.apply(lambda x: list(map(lambda y: '_'.join(y.lower().strip().split()), x))) $ df['tags'] = tags
ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'] $ )
lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation $ corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi
df[df['converted']==1]['converted'].count()/df['converted'].count()
df1.append(df2).append(df3) $
from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report $ from sklearn.model_selection import GridSearchCV, StratifiedKFold $ from sklearn import preprocessing
df['G'] = (df['Rating'] == 'G').astype(int) $ df['PG'] = (df['Rating'] == 'PG').astype(int) $ df['PG13'] = (df['Rating'] == 'PG13').astype(int) $ df['R'] = (df['Rating'] == 'R').astype(int) $ df
latest_df = raw_df[raw_df.workflow_version == 47.58].copy()
df_test.info()
CONSUMER_KEY    = 'EV6p8KSFnZCFm6MQemqhAtlzl' $ CONSUMER_SECRET = 'jr6Bcy6ciThkwGlfuw2nfBOTiS5dGZtexcqbsZCHskXv7QZJqk' $ ACCESS_TOKEN  = '2268722353-80wej1GZNdECvjMAzHGuHY1Rer1YRGZhqNhYFdG' $ ACCESS_SECRET = '5x5x1ZnarCKljy4cVoeA0BOy8L5dXOXoo3ulVcIamonaB'
bigram_sentences_filepath = paths.bigram_sentences_filepath
cwd = os.getcwd() $ cwd
client = pymongo.MongoClient() $ tweets = client['twitter']['tweets'].find() $ latest_tweet = tweets[200]
"We have {0} Bible passages.".format(len(D2))
merged_portfolio_sp_latest_YTD = pd.merge(merged_portfolio_sp_latest, adj_close_start, on='Ticker') $ merged_portfolio_sp_latest_YTD.head()
BallBerry_rootDistExp_1 = BallBerry_rootDistExp[0] $ BallBerry_rootDistExp_0_5 = BallBerry_rootDistExp[1] $ BallBerry_rootDistExp_0_25 = BallBerry_rootDistExp[2]
user.loc["Trump"]
clean_trips = trips $ clean_trips = clean_trips[clean_trips['duration'] <= 86400 ] $ clean_trips = clean_trips[clean_trips['duration'] > 120 ] $ clean_trips.duration.describe()
prods=pd.merge(priors_product_purchase,priors_product_reordered, on='product_id') $ prods['prod_reorder_rate']=prods['reordered_count']/prods['purchase_count'] $ prods.head()
ts = pd.Timestamp(datetime.datetime(2016,5, 12, 3, 42, 56)) $ ts
met = get_metrics(STAMP, y_valid, y_pred, chance)
import matplotlib.pyplot as plt $ % matplotlib inline $ plot=twitter_data.rating_numerator.hist(bins=50) $ plot=plot.set_xlim([0,200])
df["Source"].unique()
train.FORMULE.value_counts()
smpl_new.take(2)
data.loc['AT'] = [386.4, 8.7, 'German', 'EUR']  #Add a row with index 'AT'. $ s = pd.DataFrame([[511.0, 9.9, 'Swedish', 'SEK']], index=['SE'], columns=data.columns) $ data = data.append(s)  #Add a row by appending another dataframe. May create duplicates. $ data
df.groupby('user_gender')['user_gender'].apply(len)
submit.to_csv("properati_dataset_sample_submision.csv", index = False)
pres_df['split_location_tmp'].head()
model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)
prod_ch.columns.values
pd.read_csv?
def compute_average_sentence_length(text_list): $     return np.mean([len(x) for x in text_list]) $ compute_average_sentence_length(['huhuh.', 'sbbbasdsads'])
print "Probability of treatment group converting:",df2.groupby(['group'], as_index = False)['converted'].mean().iloc[1,1]
append_paths = { $     'physical_activity_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/PAQ_D.XPT' $ }
v_invoice_hub = v_invoice_hub.reindex_axis(sorted(v_invoice_hub.columns), axis =1) $ invoice_hub = invoice_hub.reindex_axis(sorted(invoice_hub.columns), axis=1)
!h5ls -r 'data/NYC-yellow-taxis-100k.h5'
cur.execute(sql_all_tables)
import pandas as pd $ airport_lookupDF = pd.read_csv('https://s3.amazonaws.com/jsw.dsprojects/AirlinePredictions/Airport_Lookup.csv', $                                header = 0) # Airport codes $ trainDF = pd.read_csv('FinalFlightsNumeric.csv', header = 0) # Data from R
autos.isnull().sum()
beirut['Mean Humidity'].mean(), summer['Mean Humidity'].mean()
problems = [x for x in dbBad if x not in bad]
exportOI['avg_distance'] = exportOI['event.longSum_sum_distance']/exportOI['event.longSum_total_records']
response
from IPython.display import Image $ Image(filename='images/mgxs.png', width=350)
df.irlco.sum()
tweet_data = pd.read_csv(r'./tweetCoords.csv',header=None,names=columns,parse_dates=[1],infer_datetime_format=True)
np.array(p_diffs) $ plt.hist(p_diffs);
df = pd.read_csv(url)
(mismatch['match'] == 'mismatch').sum()
pulledTweets_df.sentiment_predicted_nb.value_counts().plot(kind='bar', $                                                            title = 'Classification using Naive Bayes model') $ plt.savefig('data/images/Pulled_Tweets/'+'NB_class_hist.png')
df['Name'].value_counts().head(35)
((revenue[revenue.cum_revenue_pct <= 0.81].fullVisitorId.nunique())/revenue.shape[0])
df_webshells = df_webshells.append(df_webshells_temp)
n_page_converted = np.random.binomial(1, p_new, n_new) $ print('The new_page convert rate: {}.'.format(n_page_converted.mean())) $ print('The new_page convert rate: {}.'.format(round(n_page_converted.mean(), 4)))
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist(null_vals) $ plt.axvline(x=0.000913, color='r') $ (null_vals > 0.000913).mean()
csvfile ='results.csv' $ resdf = pd.read_csv(csvfile) $ resdf.info()
bnbAx[bnbAx['language']=='fr'].first_browser.value_counts().plot.pie()
visits = visits.sort_values(["address", "dba_name", "inspection_date"])
from swat import * $ cassession = CAS(server, 5570, authinfo='~/.authinfo', caslib="casuser") $ cassession.loadactionset(actionset="table")
cashflows_act_arrears_investor[(cashflows_act_arrears_investor.id_loan==675) & (cashflows_act_arrears_investor.fk_user_investor==38)].to_clipboard()
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
nbar_clean.time
X_s_n=X_d
df.loc[[1,2],:]
f_close_clicks_os_train.show(3)
evaluator.plot_confusion_matrix(normalize=False, $                                 title='Confusion matrix, without normalization', $                                 print_confusion_matrix=False, $                                 figsize=(8,8), $                                 colors=None)
data.dropna()
some_articles = newsapi.get_everything(domains='wsj.com', language='en') $ some_articles['articles'][0]
raw_data[["campaign_id", "campaign_budget", "objective", "bid"]].drop_duplicates()
times.head(30)
def bsm_call_imp_vol(S0, K, T, r, C0, sigma_est, it=100): $     for i in range(it): $         sigma_est -= ((bsm_call_value(S0, K, T, r, sigma_est) - C0) / bsm_vega(S0, K, T, r, sigma_est)) $     return sigma_est
print(twitter_archive_df_clean['timestamp'][0]) $ type(twitter_archive_df_clean['timestamp'][0])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
aqmdata.info()
df[df['Duration'] < 60*45]['Duration'].plot.hist(bins=30)
order_data = pd.read_excel('order data.xlsx')
df_new = df2.set_index('user_id').join(countries_df.set_index('user_id')) $ df_new[['CA','US', 'UK']] = pd.get_dummies(df_new['country'])[['CA','US', 'UK']] $ df_new.head()
itm_add_elapsed('onpromotion', 'after_')
contribs.info()
new_page_converted = np.random.choice(np.arange(2), size=n_new, p=[(1-p_new), p_new]) $
df_link_match = df_links[df_links['link.domain'].isin(all_short_domains)] $ df_link_match_unique = df_link_match.drop_duplicates(subset=['link.url_long']) $ len(df_link_match_unique)
summer['Mean TemperatureC'].plot(grid=True, figsize=(10,5))
twitter_ar = twitter_ar[twitter_ar.rating_num<=17.0] 
df_2007 = pd.DataFrame(rows)
jobs_data2 = json_normalize(json_data2['page']) $ jobs_data2.head(5)
run txt2pdf.py -o "METHODIST HEALTHCARE MEMPHIS HOSPITALS  Sepsis.pdf"   "METHODIST HEALTHCARE MEMPHIS HOSPITALS  Sepsis.txt"
print(filenames)
df_new['US_ab_page'] = df_new['ab_page']*df_new['US'] $ df_new['UK_ab_page'] = df_new['ab_page']*df_new['UK'] $ log_mod_con = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','US_ab_page', 'UK_ab_page', 'US', 'UK']]) $ results = log_mod_con.fit() $ results.summary()
_ = ok.grade('q08b') $ _ = ok.backup()
df2.to_csv("../../data/msft_modified.csv",index_label='date')
model_dt = DT_clf.fit(X_train, y_train) $ model_rf = RF_clf.fit(X_train, y_train)
test_data = merged1['2018-05-01':'2018-02-28']
df.info()
df2.query('group == "treatment"').user_id.size
injuries_hour.columns
len(data_issues_transitions)
"my string".find('ng')
my_df_small.head()
type(rawData)
jpncodes = ["CRDQJPAPABIS","DDDM01JPA156NWDB","QJPHAM770A","DDOI02JPA156NWDB","MKTGDPJPA646NWDB", "DEXJPUS"] $ startj = dt.datetime(1975,1,1) $ japcolumns = ["Credit_2NonFinSec","Mkt_Cap2GDP", "Household_Credit", "Deposits_2GDP", "GDP", "Yen2USD" ] $ japandata= data.DataReader(jpncodes, "fred", startj) $ japandata.columns = japcolumns
mode_fn = (lambda x: stats.mode(x)[0][0])
!jupyter nbconvert index.ipynb --to html $ bucket.upload_file('index.html', 'index.html') $ bucket.upload_file('index.ipynb', 'index.ipynb')
bacteria_dict = {'Firmicutes': 632, 'Proteobacteria': 1638, 'Actinobacteria': 569, $                  'Bacteroidetes': 115} $ pd.Series(bacteria_dict)
cur.execute("SELECT * from tweet_dump order by id DESC limit 10;") $ result = cur.fetchall() $
df_nona = df.fillna('NA')
n_new = df2[df2['landing_page']=="new_page"].count()[0] $ n_new
train[train['is_holiday'] == 1].head(3)
latestTimeByUser = firstWeekUserMerged.groupby('userid')['time_stamp2'].max().reset_index(name="recentTime") $ latestTimeByUser.head(5)
os.chdir("..")
import statsmodels.api as sm $ convert_old = df2.query('converted == 1 and landing_page == "old_page"')['user_id'].nunique() $ convert_new = df2.query('converted == 1 and landing_page == "new_page"')['user_id'].nunique() $ n_old $ n_new
apple_tweets.to_pickle('../data/apple.pkl')
compound_sub2 = compound_wdate_df2.append(compound_wdate_df3)
df.eval('D = (A + B) / C', inplace=True) $ df.head()
pd.Series([1, 2, 3]).dtypes
greater_than_diff = [i for i in p_diffs if i > Actual_diff]
with open(TEST_DATA_PATH, 'rb') as handle: $     test_df = pickle.load(handle) $     print("Test Dataframe loaded")
X_test_all.shape
with pdfplumber.open('../pdfs/collections.pdf') as pdf: $     test = pdf.pages[1] $     table = test.extract_table() $     print(table)
kaggle = False $ sim = False $
conditions.unique()
df['num_comments'].describe()
df_2007.dropna(inplace=True) $ df_2007
jArr = convert(Array{Int}, reshape(1:10, 5, 2))
monthMask = df['month'] >= 10
df3 = pd.read_csv('countries.csv') $ df3 = df3.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df3.head()
df_passengers.head(5)
sns.heatmap(viscorr, $  cmap = 'Blues')
old_page_converted = np.random.binomial(1, p_null, n_old) $ old_page_converted.mean()
conversion_prob = df2["converted"].mean() $ conversion_prob
[md.trn_ds[0].text[:12]]
sns.countplot(auto_new.Payment_Option)
linreg = LinearRegression(normalize=True) $ linreg.fit(X_train, y_train)
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall() $
a.keys() & b.keys()
new_df = pd.DataFrame() #creates a new dataframe that's empty $ new_df["index"] = range(num_embedding) $ import csv $ new_df.to_csv(os.path.join(LOG_DIR, 'output2.tsv'), sep='\t', quoting=csv.QUOTE_NONE) $
df2_len,num_uniqueid = df2.shape[0],df2.user_id.nunique() $ print('Number of rows in the new dataframe: '+ str(df2_len)) $ print('Number of unique ids in the new dataframe:' + str(num_uniqueid))
test = datatest[datatest['expenses'].isnull()] $ train = datatest[datatest['expenses'].notnull()]
df.head()
grad_age_mean = records3[records3['Graduated'] == 'Yes']['Age'].mean() $ grad_age_mean
URL = "https://raw.githubusercontent.com/feststelltaste/software-analytics/master/demos/dataset/git_demo_timestamp_linux.gz" $ git_log = pd.read_csv(URL, compression="gzip") $ git_log.head()
all_df.info()
df['DATETIME'] = df[['DATE','TIME']].apply(lambda x: ' '.join(x),axis=1) $ df['DATETIME'] = df['DATETIME'].map(lambda x: parse(x)) 
p_old=df2.converted.mean() $ p_old
df_new[["CA","UK","US"]]=pd.get_dummies(df_new["country"]) $ df_new.drop("US", inplace=True, axis =1) $ df_new.head()
df.to_csv('trump_lies.csv', index=False, encoding='utf-8')
np.exp(ser)
alg = LogisticRegression(random_state=1) $ alg.fit(X_train, y_train)
del scaled['Close'] $ scaled.head()
df2.info()
from sklearn.metrics import accuracy_score $ y_pred = dnn_clf.predict(X_test) $ accuracy_score(y_test, y_pred['classes'])
data['pickup_cluster'] = id_pickup_label.astype(str) $ data['dropoff_cluster'] = id_dropoff_label.astype(str) $ data['ride_cluster'] = id_ride_label.astype(str)
autos.head()
df2.groupby('user_id')['user_id'].count().sort_values(ascending=False).head(1)
pickle.dump(cv_data, open('iteration1_files/epoch3/cv_data.pkl', 'wb'))
autos["price"] = autos["price"].str.replace("$", "").str.replace(",", "").astype(int) $ autos.rename({"price" : "price_euro"}, axis = 1, inplace = True)
z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]),np.array([n_new,n_old]), alternative = 'larger') $ print(z_score, p_value)
model.wv.doesnt_match("input is lunch he sentence cat".split())
df.location_id.head()
result1 = df[(df.A < 0.5) & (df.B < 0.5)] $ result2 = pd.eval('df[(df.A < 0.5) & (df.B < 0.5)]') $ np.allclose(result1, result2)
data.drop('two', axis=1)
new_page_converted = np.random.choice([0,1], p=[1 - p_new, p_new], size=n_new)
df.head()
coasts_dict = {'Florida' : 8436,'Texas' : 3359,'Louisiana' : 7721,'Mississippi' : 359,'Alabama' : 607} $ with open('C:/Users/sethh/OneDrive/Desktop/Springboard/Capstone Project 1/coast_lengths.json', 'w') as fp: $     json.dump(coasts_dict, fp)
imagego = gen_filter(master_df,'go_no_go_63x', '63x go') $ imagego
converted_users = df2.converted.mean() $ print("The probability of an individual converting regardless of the page they receive is {0:.4}".format(converted_users))
ldamodel.save("lda.model")
vi_ok['']
data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} $ df = pd.DataFrame(data) $ print(df)
df_li_ordered
y_ = df['loan_status'].replace(to_replace=['PAIDOFF', 'COLLECTION'], value=[1, 0]) $ y_[0:5]
building_pa.to_csv("buildding_00.csv",index=False)
Log_model.fit().summary()
data_list = list(data)
def sentiment_scores(doc): $     snt = analyser.polarity_scores(doc) $     return snt
output= "Select * from ViewDemo where retweets=3439" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
eval_RGF_tf_tts = clf_RGF_tf.score(X_testcv_tf, y_testcv_tf) $ print(eval_RGF_tf_tts)
ls_not_numeric = [not pd.api.types.is_numeric_dtype(dtype) for dtype in df_uro.dtypes] $ prog = re.compile('DATE[0-9]*$') $ ls_not_date = [not bool(prog.search(colname)) for colname in df_uro.columns] $ ls_both = [num and date for num, date in zip(ls_not_numeric, ls_not_date)] $ df_uro.loc[:,ls_both].nunique()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx" $ df = pd.read_excel(path) $ df.head(5)
df_members = df_members[df_members.registered_via != -1]
vect = TfidfVectorizer(ngram_range=(2,4), stop_words='english') $ summaries = "".join(matt_tweets['text']) $ ngrams_summaries = vect.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
new_df.head()
numbers_df = pd.DataFrame(numbers, index = ['number_1', 'number_2','number_3']) $ numbers_df
dta = sentiments.head(100) # testdata $ dta['item'] = dta.index $ dta.head()
ids = df2['user_id'] $ print('User Id repeated is {}'.format(np.asarray(ids[ids.duplicated()])))
np.arange(0, 10, 2)  #Like range, but creates an array instead of a list.
executable_path = {'executable_path': "C:\Users\Brittney_Joyce\AppData\Local\Temp\Temp1_chromedriver_win32.zip)"} $ browser = Browser('chrome', **executable_path, headless=False)
one_station['DATE'] = pd.to_datetime(one_station['DATE']) 
output.printSchema() $ output2 = output.select('label', 'features') $
df1 = pd.read_csv('../Data/sector_joined_closes_train.csv', index_col='Date_Time') $ display(DataFrameSummary(df1).summary())
for name,group in new_group: $     print(name,len(group))
import numpy as np
pmol.df[pmol.df['atom_type'] != 'H'].tail(10)
plt.figure(figsize = (10,5)) $ sns.heatmap(df, cmap="PuBuGn").set_title("Month & Day of Week")
import plotly.graph_objs as go $ import plotly.plotly as py
y_pred = RF.predict(X) $ result = Events.copy() $ result.insert(loc=1, column='valid_result', value=y_pred) $ result
precipitation_df.isnull().sum()
filePath = '/green-projects/project-waze_transportation_network/workspace/share/Data/20180601103403.json'
pd.crosstab(aqi['AQI Category_eug'], aqi['AQI Category_cg'], normalize='index', margins=True)
X = stock.iloc[925:-1].drop(['volatility', 'volume', 'high', 'low', 'close', 'open', 'target', 'true_grow'], 1) $ y = stock.iloc[925:-1].true_grow
tweets_df.text.describe()
df_h1b_nyc_ft = df_h1b_nyc[df_h1b_nyc.full_time_pos=='Y'] $ print('There are {:.0f} visa applications for full-time jobs located in NYC in this dataset.'.format(df_h1b_nyc_ft.shape[0])) $
data_train = pd.read_json('train.json') $ data_test = pd.read_json('test.json')
corr_map(X)
pd.DataFrame(lostintranslation.credits()['crew'])[["job", "name", "id"]].head()
len(segmentData.opportunity_owner_id.unique())
bigdf.loc[bigdf['comment_fullname'] == 't1_drdvqmo']
intervention_test.reset_index(inplace=True) $ intervention_test.set_index(['INSTANCE_ID', 'CRE_DATE_GZL'], inplace=True)
k1 = data[['cust_id','total_spend']].groupby('cust_id').agg('mean').reset_index() $ k1.columns = ['cust_id','cust_id_total_send_mean'] $ train = train.merge(k1,on='cust_id',how='left') $ test = test.merge(k1,on='cust_id',how='left')
Grouping_Year_DRG_discharges_payments.groupby('drg3').get_group(871)[:4]
education_2011_2015=education_data.unstack(level=2, fill_value=0)
pca=decomposition.PCA() $ stocks_pca_t4= pca.fit_transform(stocks_pca_m4)
df_newpage = df.query('landing_page == "new_page"') $ df_2 = df_newpage.query('group == "treatment"') $ df_2.nunique() $
df = pd.read_csv("../Data/user_summary_id_infos.csv") $ df.info()
plt.rcParams['axes.unicode_minus'] = False $ dta_59.plot(figsize=(15,5)) $ plt.show()
plt.plot(ds_issm['time'],ds_issm['met_salsurf'],'r.') $ plt.title('CP03ISSM, Sea Surface Salinity') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
df2.converted.sum()/df2.converted.count()
types = ['doggo', 'floofer', 'pupper', 'puppo'] $ tweet_archive_clean = tweet_archive_clean.drop(types, axis=1)
train['Age_bin'] = train.apply(lambda row: str(int(row[9]/10)*10) +'-'+str(int(row[9]/10)*10 + 9)\ $                                , axis = 1) $ test['Age_bin'] = test.apply(lambda row: str(int(row[8]/10)*10) +'-'+str(int(row[8]/10)*10 + 9)\ $                                , axis = 1) $ train.head(3)
festivals.at[2,'longitude'] = -87.7035662
grouped = positive_results.groupby('user_id') $ successful_campaigns_per_user = dict(grouped.campaign_id.count()) $ successful_users = OrderedDict(Counter(successful_campaigns_per_user).most_common(20)) $ successful_users
dfcounts = read_gbq(query=query, project_id='opendataproject-180502', dialect='standard')
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=30000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df = pd.read_csv('../../datasets/crypto-index-fund/crypto_data/CryptoDataWide.csv', parse_dates=True)
trip_data['zip'] = trip_data['start_city'].apply(zip_match)
plot_correlation_map(df_pivot)
df_all_users = join_lists()
free_data.groupby('age_cat').mean()
m3 =np.around(m3,2) $ print("m3: ", m3)
likes['year'] = pd.DataFrame(likes.date.dt.year) $ likes['month'] = pd.DataFrame(likes.date.dt.month)
countries = pd.read_csv("country.csv", encoding = "utf-16") $ print (countries.head())
print(df2[df2['group']=='treatment']['converted'].mean())
filtered_df[['accommodates','bedrooms']].corr()
run txt2pdf.py -o"2018-06-19 2015 MAYO CLINIC Sorted by discharges.pdf"  "2018-06-19 2015 MAYO CLINIC Sorted by discharges.txt"
googletrend.head()
lons, lats = np.meshgrid(lon_us, lat_us) $ plt.plot(lons, lats, marker='.', color='k', linestyle='none') $ plt.show()
series1 = pd.Series(np.random.randn(1000)) $ series2 = pd.Series(np.random.randn(1000))
import numpy as np $ import matplotlib.pyplot as plt $ import pandas as pd 
bday = datetime(1986, 3, 6).toordinal() $ now = datetime.now().toordinal() $ now - bday
results2.summary()
meeting_status = pd.read_csv('./data/MeetingStatus.csv')
def trainModel(m, xtr, ytr, ep, bsz,l=0): $     m.fit(xtr, ytr, batch_size=bsz, epochs=ep, validation_split=0.079, verbose=l) $     return m
from bs4 import BeautifulSoup $ import pandas as pd $ import numpy as np $ import math $ import matplotlib.pyplot as plt
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2015 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2015 Percentiles.txt"
building_pa_prc_zip_loc['permit_type'].unique()
autos.describe(include='all') # include both numeric and categorical variables
df_Q123 = data[['Date', 'Time', 'Outside Temperature', 'Hi Temperature', 'Low Temperature']]
food["created_datetime"].head()
print(train_data.gearbox.isnull().sum()) $ print(test_data.gearbox.isnull().sum())
y = df2.converted $ X = df2[['intercept', 'ab_page']]
siteID = 1  # Red Butte Creek at 1300 E (obtained from the getRelatedSamplingFeatures query)
dcrime_gb.head()
for i in range(3): $     dfs[i]['SA'] = np.array([analyze_sentiment(tweet) for tweet in dfs[i]['Tweets']])
feats = dims $ training_steps = 25
df_goog.Open.asfreq('D', method='backfill').plot()
df_country = pd.read_csv('countries.csv') $ df_country.head()
df2[df2.landing_page=="new_page"].count()[0]/df2.count()[0]
for row in selfharmm_topic_names_df.iloc[3]: $     print(row)
person = "UTHornsRawk" $ person = df[df["screen_name"]==person] $ person.head(100)
a = np.arange(10) $ b = np.sin(a) $ array_dict = {'a': a, 'b': b} $ df = pd.DataFrame(array_dict) $ df
compulsory_fields = w.data_handler.physical_chemical.filter_parameters['compulsory_fields'] $ display_columns = compulsory_fields + use_parameters $ print(display_columns)
state_grid = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(10, 10)) $ state_grid
Results_kNN1000d = Results_kNN1000d[['ID', 'Approved']] $ Results_kNN1000d.head()
merged = merged.set_index('DateTime')
old_page_converted = np.random.choice([1,0], size=sample_size_old_page, p=[p_old_null, 1-p_old_null])
cvec = CountVectorizer(ngram_range = (1,2), lowercase = True, stop_words = 'english', max_features = 5000)
p_old = df2['converted'].mean() $ print "Convert rate of an individual received the old page:",p_old
df3.head(5)
model.save_word2vec_format('/tmp/doc_tensor.w2v', doctag_vec=True, word_vec=False)
data.head()
len(df2.query('converted == 1')) / len(df2)
to_be_predicted_Day4 = 31.23097052 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
np.random.seed(123456) $ dates = ['2014-08-01', '2014-08-02'] $ ts = pd.Series(np.random.randn(2), dates) $ ts
scipy.stats.kruskal(df2["tripduration"], df4["tripduration"])
for i in train_dum.columns: $     if i not in test_dum.columns: $         print i
results = logm.fit() $ results.summary()
import quandl $ import pandas as pd $ import numpy as np $ import time
target = users["country_destination"].values $ features_one = users[["timestamp_first_active", "signup_flow", "age"]].values
two_day_sample.head()
high = np.array(data_dict['High'],dtype='float64') $ low = np.array(data_dict['Low'],dtype='float64') $ change = high - low
from splinter import Browser $ browser = Browser('chrome', headless=True) $ url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars' $ browser.visit(url)
for dataset in full_data: $     dataset['Embarked'] = dataset['Embarked'].fillna('S') $ print (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())
cities = [] $ for i in places2.index: $     cities.append(places2[i]['name'])
train_session.isNDF.value_counts()/train_session.shape[0]
precip_df.describe()
sum(df.duplicated())
data2.info()
activation = K.softmax(y) # Softmax
autos["price"].value_counts()
df = pd.DataFrame() $ for u in users: $     df[u] = pd.Series(subs[subs.user_id == u].problem_id.values)
session.query(Station.name).count()
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True)
games.head()
df.head()
df_comb[['CA', 'UK', 'US']] = pd.get_dummies(df_comb['country']) $ df_comb.head()
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
abc.tail()
autos['price'].value_counts()
grouped_dpt = department_df.groupby('Department') $ grouped_dpt # groupby object 
embeddings = tf.Variable(tf.random_uniform((len(vocab), embed_size), -1, 1)) $ embed = tf.nn.embedding_lookup(embeddings, inputs)
data['claps'].describe()
p_conv = df2.converted.mean() $ p_conv
twitter_df_clean.loc[200]
from sklearn.ensemble import RandomForestRegressor $ model = RandomForestRegressor() $ print ('Random forest') $ RandomForestRegressor_model = reg_analysis(model,X_train, X_test, y_train, y_test)
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'], $        'year': [2000, 2001, 2002, 2001, 2002, 2003], $        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
import seaborn as sns $ %matplotlib inline
smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name)['HyperParameterTuningJobStatus']
records.loc[records['Gender'].isnull(), 'Gender'] = gender
ol.funding_rounds
combined_factor_df = combined_df $ combined_factor_df['US_ab'] = combined_df['US']*combined_df['ab_page'] $ combined_factor_df['CA_ab'] = combined_df['CA']*combined_df['ab_page'] $ combined_factor_df = combined_factor_df.drop(['US', 'CA'], axis=1) $ combined_factor_df.head()
test.head()
new_page_converted = \ $     np.random.choice([1,0], size=n_new, p=[(1-p_new), p_new])
from sklearn.feature_extraction.text import CountVectorizer $ vect = CountVectorizer(ngram_range=(1,1)) $ vect.fit_transform(['no i have cows', 'i have no cows']).toarray(), vect.vocabulary_
top20_mostfav = fav_summary[['prediction_1', 'favorite_count']]
from keras.utils import to_categorical $ target = to_categorical(titanic.survived) $ target
event_sdf = events_grouped_no_nulls.sort( events_grouped_no_nulls['count'].desc() )
from pyspark.sql import Row $ rddRow = rdd.map(lambda f: Row(f)) $ spark.createDataFrame(rddRow).toDF("col1").show()
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func, inspect
txns.tail()
input_data = np.array([3, 5]) $ weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}
yeardf = pd.DataFrame((1+newdf['TSRet']/100).resample('Y').prod())
print(train.shape) $ unique_user_count = len(set(train["id"])) $ print(unique_user_count) $ train.head(5) $ train.describe(include = "all") #many cols have missing information
preparser(False) # Turn off Sage preparser for use with pandas
y.tail()
data.to_csv('data_pubg.csv', encoding = "utf8")
print(automl_feat.show_models())
USER_PLANS_df
print(filtered_df['name'][0].split(' '))
import numpy as np    $ import pandas as pd $
tw_clean.sort_values('retweet_count').tail(1)
lammps_exe = 'lmp_serial'
for row in reader.iterate_over_rows(): $     break $ row
print ("Data Frame with Forward Fill:") $ df2.reindex_like(df1,method='ffill')
data_df
test.head()
df2 = nyc_jobs.copy() $ df2 = df2.dropna(axis=1, how='any') $ df2 = df2.drop_duplicates('Job ID') $ df2.head()
api = Api() $ print(api.auth_time)
plt.plot(dataframe.groupby('month').daily_worker_count.mean()) $ plt.show()
for row in cursor.columns(table='TBL_FCBridge'): $     print(row.column_name)
import pandas as pd $ url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat' $ df = pd.read_csv(url,header=None,sep=' ') $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ df.head()
import datetime
print('The number of unique genera at Sakhalin Island:', len(sakhalin_filtered.species_id.unique()))
unique_urls.sort_values('mentions', ascending=False).head()
display(df)
parameters = {'classification__mlpclassifier__solver': ['sgd'], 'classification__mlpclassifier__max_iter': [1500], $               'classification__mlpclassifier__alpha': 10.0 ** -np.arange(1, 7), $               'classification__mlpclassifier__hidden_layer_sizes':[[30, 30, 30, 30]], $              'classification__mlpclassifier__momentum':[0, 0.3, 0.6, 1]} $ parameters
price.describe()
to_be_predicted_Day2 = 36.50976151 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
raw_data.drop(null_name.index, axis=0, inplace=True)
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results = mod.fit() $ results.summary()
DBPATH = 'results.db' $ conn = sqlite3.connect(DBPATH) $ cur = conn.cursor()
from pandas_datareader.data import Options $ aapl = Options('aapl', 'yahoo') $ data = aapl.get_all_data() $ data.iloc[0:5, 0:5]
crimes_date_cnt = crimes.groupby('Date')[['IncidntNum']].count() $ crimes_date_cnt.reset_index(inplace=True) $ crimes_date_cnt.rename(columns={"Date": "incident_date"}, inplace=True)
posts = pd.read_csv('data/posts.csv', index_col = 0, parse_dates= [5, 6, 7]) $ posts.head()
noise = np.random.normal(scale=0.5, size=4) # standard deviation of the noise is 0.5 $ y = m_true*x + c_true + noise $ plt.plot(x, y, 'r.', markersize=10) $ plt.xlim([-3, 3]) $ mlai.write_figure(filename="../slides/diagrams/ml/regression_noise.svg", transparent=True)
mario_game.dropna(subset = ['Year_of_Release'], inplace = True) $ mario_game.Year_of_Release = mario_game.Year_of_Release.astype('int')
tweet_data.head()
driver = webdriver.Firefox(executable_path = '/usr/local/bin/geckodriver')
d5.columns = d5.columns.droplevel(level = 0) $ d5
df['date'] = df.date.astype(str)
plt.plot(tfidf_svd_v2.explained_variance_ratio_);
texts = [text.lower() for text in texts]
autos = autos[autos['registration_year'].between(1900,2016)] $ autos['registration_year'].value_counts(normalize=True).sort_index()
from sklearn.naive_bayes import MultinomialNB $ model_count_NB = MultinomialNB() $ model_tfidf_NB = MultinomialNB() $ model_count_NB.fit(X_train_count, y_train_count) $ model_tfidf_NB.fit(X_train_tfidf, y_train_tfidf)
pts_df.stack()  # back to a series
from sklearn.neighbors import KNeighborsRegressor $ knn_reg = KNeighborsRegressor(n_neighbors = 101) $ knn_reg.fit(x_train,y_train)
tweets_unique = tweets_unique.drop_duplicates(subset='id') $ tweets_unique = tweets_unique.sort('id')
counts.values
train = pd.read_csv("../data/wikipedia_train3.csv") $ test = pd.read_csv("../data/wikipedia_test3.csv")
lookback_window = 21 $ epoch_num_param = 25 $ batch_size_param = 25 $ layer1_num_neurons_param = 50 $ dropout_pct_1 = 0.20
mapInfo = refl['Metadata']['Coordinate_System']['Map_Info'].value $ mapInfo
tuna_90 = mapped.filter(lambda row: (row[4] > 0 and row[4] <= T1)) $ tuna_90_DF = tuna_90.toDF(["cid","ssd","num_ssd","tsla","tuna"]) $ tuna_90_pd = tuna_90_DF.toPandas()
portfolio_df.info()
from pyspark.ml.evaluation import MulticlassClassificationEvaluator $ evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction") $ accuracy = evaluator.evaluate(predictions) $ print "Model Accuracy: ", accuracy
ds_eval = FileDataStream.read_csv(eval_file, collapse=False, header=False, names=columns, numeric_dtype=np.float32, sep='\t', na_values=[''], keep_default_na=False)
search_criteria = {'strategy': 'RandomDiscrete', 'max_runtime_secs': 30}
kick_projects['goal_cat_perc'] =  kick_projects.groupby(['category'])['goal'].transform( $                      lambda x: pd.qcut(x, [0, .35, .70, 1.0], labels =[1,2,3]))
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
year2 = driver.find_elements_by_class_name('yr-button')[1] $ year2.click()
conn.columninfo(table=dict(name='iris_db', caslib='casuser'))
at_menu_id_granularity = menu_dishes_about_latent_features.groupby('menu_id').sum().reset_index()
reddit_comments_data.select(mean("score")).show(truncate=False)
data[data.notnull()]
feature_importances.iloc[:15,:].plot.bar() $ plt.ylabel('Importance') $ plt.title('Feature Importance Plot') $ plt.show()
first_cluster.fit(docs)
from hs_restclient import HydroShare, HydroShareAuthBasic $
df_clean.info()
sf = dataframe.groupby(['year','month']).daily_worker_count.sum()
index_name = df.iloc[0].name $ print(index_name)
dd=cfs.diff_abundance('Subject','Control','Patient', random_seed=2018)
def sql_to_df(sql_query): $     df=pd.read_sql(sql_query,db) $     return df
s4 =pd.Series(np.random.randn(5), index=list('abcde'), name='S4') $ print s4
knn = KNeighborsClassifier() $ knn_grid = GridSearchCV(knn,params,scoring='f1')#'f1')
rt = pd.merge(ranking_table, games_played, on=['season', 'PLAYER_ID', 'PLAYER_NAME']) $ rt.rename(columns={"GAME_ID":"games_played"}, inplace=True)
r.summary2()
permits_df = pd.read_csv('./Datasets/Building_Permits_Full.csv') $ permits_df.head()
print(f.variables.keys()) # get all variable keys
df_y_pred = np.exp(y_age_predict.as_data_frame()) $ df_y_actual = np.exp(y_age_valid.as_data_frame())
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer()
misk_df = get_all_repos(baseurl, 'DSI-ME-1')
cnxn = pyodbc.connect(conn_str)
convers_con.time_difference.value_counts(dropna=False)
for index, row in im_clean.iterrows(): $     assert(any(letter.isupper() for letter in row["p1"]) == False) $     assert(any(letter.isupper() for letter in row["p2"]) == False) $     assert(any(letter.isupper() for letter in row["p3"]) == False)
apple["20d"] = np.round(apple["close"].rolling(window = 20, center = False).mean(), 2) $ apple_rstd = np.round(apple['close'].rolling(window = 20, center = False).std(), 2) $ apple['upperband'] = apple['20d'] + 2 * apple_rstd $ apple['lowerband'] = apple['20d'] - 2 * apple_rstd $ pandas_candlestick_ohlc(apple.loc['2017-08-25':'2018-03-13',:], otherseries = ["20d", "upperband", "lowerband"])
def pratio(x, y): $     if isinstance(x, float) or isinstance(y, float): $         return None $     return fuzz.ratio(x, y)
ekos.load_workspace(user_id, alias = 'lena_newdata')
data.fillna(0)
df2.landing_page.value_counts()
display(DataFrameSummary(ph).summary(), ph.info(), ph.head())
tokendata = pd.merge(keydf,tokendata,on="ID",how="left") $ len(tokendata)
bc = pd.read_csv("bitcoinity_all.csv")
%%time $ valence_df = getAllInfo(IMDB_df, happiness, stopwords)
country_dummies = pd.get_dummies(df3['country']) $ df3_new = df3.join(country_dummies) $ df3_new['intercept'] = 1 $ df3_new.head()
v_to_c['time'] = v_to_c.checkout_time - v_to_c.visit_time $ print(v_to_c)
print('Given that an individual was in the \'control\' group, the probability they converted is 0.120386')
bad_comments[3].replace('\r\r', ' ')
df['tstamp'] = df['tstamp'].apply(lambda x: pd.to_datetime(x))
%run '../forecasting/helpers.py' $ %run '../forecasting/main_functions.py' $ %run '../forecasting/ForecastModel.py'
dfSF["Year"] = dfSF["Opened"].apply(YearColumn)
df.info()
findNumbers = r'\d+' $ regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number') $ print(regexResults.group(0))
df_final_edited['rating_denominator'].value_counts() $
print(f'There are {len(cutoff_times)} first and fifteenth of the month labels for partition {PARTITION}.')
Image.open('sign_up.png')
%%timeit $ scipy.optimize.broyden2(globals()[function_name], 2, f_tol=1e-14)
pn_and_qty_with_duplicates = pd.read_csv('data/PN_and_QTY_with_duplicates.csv')
squared_distances_i_j_k = np.power(y_i[:, np.newaxis, :] - y_i, 2) $ pairwise_squared_distances_i_j = squared_distances_i_j_k.sum(axis=2) $ pairwise_squared_distances_i_j
df[df.category != 'top'].head()
AAPL.iloc[25:30,0:4].plot.bar()
classweights = pd.read_csv('classweights.csv', sep=',', header=None) $ classweights.set_index(0, inplace=True) $ classweights[1].to_dict() $
gas_df = gas_df.melt(id_vars=['MONTH'],var_name='YEAR',value_name='PRICE') $ gas_df.head()
cars.columns
mean_database["mean_price"] = pmbb_series $ mean_database
df[df.province<99].groupby("province").sum().sort_values(by="postcount",ascending=False)["postcount"][0:10]
df.artist_popularity.hist()
normal_forecast_exp = np.exp(normal_forecast[['yhat','yhat_lower','yhat_upper']]) $ normal_forecast_exp.index = normal_forecast['ds'] $ normal_model_error = normal_forecast_exp['yhat'] - df_orig['y'] $ MAPE_for_normal_model = (normal_model_error/df_orig['y']).abs().sum()/n *100 $ print ("Normal model error: ",round(MAPE_for_normal_model,2))
df['Department'] = pd.Series({0: 'HR', 2: 'Marketing'}) $ df
gDate_vEnergy = gDateEnergy_content.count().unstack() $ gDate_vEnergy = gDate_vEnergy.fillna(0)
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)
df2[((df2['user_id'] == 773192) == True)]
nonunique = df2.user_id.value_counts() $ nonunique[nonunique > 1].index[0]
twitter_archive_clean['source'].value_counts()
dup_id = df2[df2.duplicated(['user_id'], keep=False)]['user_id'].reset_index()
plt.bar(np.arange(1), [temp_ranges[2]], yerr=(temp_ranges[1]-temp_ranges[0]),xerr=None,color=['orange']) $ plt.xticks([]) $ plt.ylabel("Temp (F)") $ plt.title("Trip Avg Temp") $ plt.show()
df_control = df2[df2.group == 'control'] $ sum(df_control.converted==1) / df_control.shape[0]
assert tidy_format.loc[894661651760377856].shape == (27, 2) $ assert ' '.join(list(tidy_format.loc[894661651760377856]['word'])) == 'i think senator blumenthal should take a nice long vacation in vietnam where he lied about his service so he can at least say he was there'
introexplore(donors)
googletrend.loc[googletrend.State=='NI', "State"].head()
to_be_predicted_Day4 = 21.20401743 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df_a.join(df_b, how = "inner") # inner join (see above for definition)
csvData.drop('condition', axis = 1, inplace = True)
users["age"] = users["age"].fillna(0) $ users["country_destination"] = users["country_destination"].fillna("NDF") $ users["date_first_booking"] = users["date_first_booking"].fillna("-unknown-") $ users["first_affiliate_tracked"] = users["first_affiliate_tracked"].fillna("untracked")
y_test_advanced, x_test_advanced = for_analysis(main[main.season == main.season.max()], model_9_ad_players) $ y_train_advanced, x_train_advanced = for_analysis(main[(main.season < main.season.max())], model_9_ad_players)
autos = autos[autos["price"].between(100, 500000)]
import datetime $ day = lambda x: x.split(' ')[0].replace('-',',') $ emotion_big_df['date']=emotion_big_df['createdAt'].apply(day)
forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 1 $ forecast_df['male_pct'] = forecast_df['male_pop'] / forecast_df['pop'] * 100 $ forecast_df.drop('male_pop', axis=1, inplace=True) $ forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 0 $ forecast_df.head()
response = requests.get("http://api.open-notify.org/iss-now.json") $ print('response status: {}'.format(response.status_code)) $ data = response.json() $ print(data['iss_position']) $ print(data['timestamp'])
bd.index
df_cs.isDuplicated.value_counts() $ df_cs.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
cat_map_train = cat_preproc(joined_train) $ cat_map_valid = cat_preproc(joined_valid) $ cat_map_test = cat_preproc(joined_test_df)
%timeit our_function(df_protest.rangecode)
df6 = pd.read_csv('2006.csv')
ben_fin.head()
data[data['ComplaintID'].notnull()][['ComplaintID']].head()
results = session.query(Station.name).limit(5) $ for result in results: $     print(result)
df = pd.read_csv('./WeRateDogs_data/twitter-archive-enhanced.csv')
y.head()
multi_index=pd.MultiIndex.from_tuples([(i, j, k) for i , j, k in zip(label_index, community_index, $                                                                      education_data['Category'].tolist())])
importances=model_rf_19_S.feature_importances_ $ features=pd.DataFrame(data=importances, columns=["importance"], index=x.columns) $ features
query = session.query(User).filter(User.name.like('%ed')).order_by(User.id)
sl['two_measures'] = np.where((sl.mindate!=sl.maxdate),1,0)
df.show()
coefs_sorted.head(best_number_of_variables)[coefs_sorted.head(best_number_of_variables).index.str.contains("\(real")]
X = yelp_class['text'] $ y = yelp_class['rating']
density.sort_values(ascending=False, inplace=True) $ density.head()
sdof_resp(1,0,1,.1,10,10)
df_tweet_json.head()
rmse_CSCO=np.sqrt(mean_squared_error(df2['diff_CSCO_log'][1:-1],pred_CSCO))
df_new = country_df.set_index('user_id').join(df2.set_index('user_id'),how='inner') $ df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new.country) $ df_new.head()
train.printSchema()
len(df2.query('landing_page == "new_page"'))/len(df2)
z_values, _ = vae_latent(nn_vae, mnist_train_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ recon_x = vae_recon(nn_vae, z_values) $ plot_mnist_sample(recon_x) $
byName = df.groupby('Name') $ byName.sum()
all_data.reindex(columns=['date', 'country', 'variable', 'totals']).head()
df_mes[(df_mes['improvement_surcharge']!=0.3)].shape[0]
fwd.head(20)
cohort_retention_df = cohort_activated_df - cohort_churned_df
df['2017-03-20':]
label_csv=pd.read_csv(PATH+'train_v2.csv'); label_csv.head()
ls ../../outputs-git_ignored/gpu_mod_run_1
xplot = np.linspace(0.5,2.5,21) $ yplot = f(xplot) $ plt.plot(xplot, yplot) $ plt.plot(min_result.x,min_result.fun,'ro') $ plt.show()
data
df['date'] = pd.to_datetime(df['date'])
tweet_ids_twitter_archive = twitter_archive['tweet_id'].unique() $ tweet_ids_image_predictions = image_predictions['tweet_id'].unique()
print("(num. rows, num. cols):", allData.shape, "\n") $ print("Head:\n", allData.head(), "\n") $ print("Describe:\n", allData.describe())
pipeline_rf = Pipeline(stages=[stringIndexer_label, stringIndexer_prof, stringIndexer_gend, stringIndexer_mar, vectorAssembler_features, rf, labelConverter])
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1-p_new)]) $ print(len(new_page_converted))
cell_df['Class'] = cell_df['Class'].astype('int') $ y = np.asarray(cell_df['Class']) $ y [0:5]
mitigations[0]
model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)
autos.describe(include='all')
cohort['percentage'] = cohort['users'] / cohort['users'].sum() $ cohort['cdf'] = cohort['percentage'].cumsum() $ cohort.head(25)
datacamp = pd.read_csv("c:\\users\\ssalahuddin\\documents\\datacamp130818.csv", parse_dates=["publishdate"], infer_datetime_format=True)
maxtobs = max(tobsDateDFclean.tobs) $ tobsDateDFclean.hist(bins=12)
ac_tr_prepared = full_pipeline.fit_transform(ac_train)
len(df) #len finds out the number of rows in this dataset
data_2017_12_14_iberia_negative.head()
p - 1
all_count = df_aggregate.shape[0] $ broken_count = df_aggregate[df_aggregate.iloc[:, 1] >= 1].shape[0] $ print ('number of disks : ',all_count) $ print ('number of failed disks: ',broken_count) $ print ('percentage of broken disks: ',broken_count/all_count*100,'%' )
df.query("converted==1").user_id.nunique()/len(df['user_id'].unique())
treatment = df2.query("group == 'treatment'") $ prop_conv_treat = len(treatment.query("converted == '1'")) / treatment.shape[0] $ print('Given that an individual was in the treatment group, the probability they converted is {}'.format(prop_conv_treat))
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
ctc = pd.DataFrame(columns=ccc, index=ccc)
jobs_data.nunique() $
z_score,p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative = "smaller") $ print(z_score, p_value)
plt.hist(p_diffs)#plot of 10k simulated p-diffs $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('10K simulated p_diffs'); $ print ("histogram of the p_diffs")
f_ip_device_minute_clicks.show(1)
users.head(20)
tweet_clean.info()
df_mas['rating']=df_mas.rating_numerator +'/' +df_mas.rating_denominator
df_1.shape
del merged_portfolio_sp_latest_YTD_sp['Date'] $ merged_portfolio_sp_latest_YTD_sp.rename(columns={'Adj Close': 'SP Start Year Close'}, inplace=True) $ merged_portfolio_sp_latest_YTD_sp['Share YTD'] = merged_portfolio_sp_latest_YTD_sp['Ticker Adj Close'] / merged_portfolio_sp_latest_YTD_sp['Ticker Start Year Close'] - 1 $ merged_portfolio_sp_latest_YTD_sp['SP 500 YTD'] = merged_portfolio_sp_latest_YTD_sp['SP 500 Latest Close'] / merged_portfolio_sp_latest_YTD_sp['SP Start Year Close'] - 1 $ merged_portfolio_sp_latest_YTD_sp.head()
X_test = test_df.drop(['target', 'true_grow'], 1) $ y_test = test_df.true_grow
top_10_3 = scores.loc[7].argsort()[::-1][:11] $ trunc_df.loc[list(top_10_3)]
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['favorite_count']==tweet_archive_enhanced_clean['favorite_count'].max()]
daily_returns.plot(kind='scatter',x='SPY',y='GLD') $ plt.show()
utils.read_sas_write_hdf(source_paths, data_dir, 'nhanes.h5')
result
import numpy as np
committees_NNN.head()
df = pd.read_excel(workbook_name, sheetname=1)
",".join(list(grouped_df.get_group('2011-07-12')['Grouped Corrected Common Name']))
df.plot(figsize=(16, 9), title='Bitcoin Price 2017-2018')
autos[["date_crawled","date_created","last_seen"]].describe()
df1['label'] = df[forcast_col].shift(-forcast_out) $ df1['label'].head()
scalingDF = trainDF[['DISTANCE', 'HDAYS']].astype('float') # Numerical features $ categDF = trainDF[['MONTH', 'DAY_OF_MONTH', 'ORIGIN_AIRPORT_ID', $                    'DEST_AIRPORT_ID', 'ARR_HOUR', 'DEP_HOUR', $                    'CARRIER_CODE', 'DAY_OF_WEEK']] # Categorical features $
clf=clf.fit(X_train, y_train) # perform training
knn = KNeighborsRegressor(n_neighbors=3, weights='distance', p=1)
tags_counts = {} $ words_counts = {} $ from collections import Counter $ tags_counts = Counter([item for taglist in y_train for item in taglist]) $ words_counts = Counter([word for line in X_train for word in line.split(' ')])
net_loans_exclude_US_outstanding.xs(99550,level='id').to_clipboard()
properati.created_on.value_counts()
autos["price"].describe()
cast_data.tail()
counts_df.info()
f = open('..\\Output\\GenreString.csv','w') $ f.write(GenresString) #Give your csv text here. $ f.close()
df=pd.DataFrame({"Year":[2017,2018],"Month":[1,12],"Day":[26,25]}) $ print(df)
data['signup_time'] = pd.to_datetime(data['signup_time']) $ data['purchase_time'] = pd.to_datetime(data['purchase_time']) $ data['difference'] = data['purchase_time'] - data['signup_time'] $ data['difference'] = data['difference'].astype('timedelta64[m]') $ data.head()
bow_2 = bow_transformer.transform([review_2]) $ bow_2
test_.head()
df['simple_context'].value_counts()
ssc.stop()
daily_averages['Month'] = daily_averages.index.month
total_rows_in_control = (df2['group'] == 'control').sum() $ rows_converted = len((df2[(df2['group'] == 'control') & (df2['converted'] == 1)])) $ rows_converted/total_rows_in_control
print fs.get_last_version("scansmpl.pdf").uploadDate
Nnew = df2.query('landing_page == "new_page"').user_id.count() $ Nnew
1 + np.nan
serhc.describe()
weather_yvr_dt = weather_yvr.copy()
crimes_all=crimes[~crimes['Latitude'].isnull()]
df_all['datetime'] = pd.to_datetime(df_all['created'], unit='s') $ df_all['datetime'] = pd.DatetimeIndex(df_all['datetime']) $ print(df_all['datetime'][0:5])
uber_15["month"].value_counts()
data[data.name == 'New EP/Music Development'].head()
pd.concat([bnb, a],axis=1)
properati.loc[:,'state_name'] = states
df2[df2['landing_page']=='old_page']['converted'].mean()
dbdata = pd.read_sql_query(dbquery, con=dbcon) $ dbdata.head()
def create_gan(generator, discriminator): $   GAN = keras.models.Sequential([generator, discriminator], name='GAN') $   GAN.compile(optimizer=Adam(lr=1e4), loss='binary_crossentropy') $   GAN.summary() $   return GAN
frame.sort_index(axis=1, ascending=False)
repos.loc[repos.forked_from == '\N', 'forked_from'] = '0'
dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5
tbl[tbl.msno == '++mnT+fA+ilPMzHgEhsyS91vWjM7iRy7//aq/eVTm6s=']
df_protest.loc[df_protest.loc[df_protest.Rural==-1].index, 'Rural'] = np.nan
df.query('salary>30000 & year==2017')
autos['brand_model'] = autos['brand'].map(str) + '_' + autos['model'].map(str) $ autos['brand_model'].head()
with open('Al-fcc.poscar', 'w') as f: $
move_34p34h34h = (breakfastlunchdinner.iloc[1, 1] $                + breakfastlunchdinner.iloc[2, 2] $                + breakfastlunchdinner.iloc[2, 3]) * 0.002 $ move_34p34h34h
pd.timedelta_range(0, periods=10, freq='H')
X.flags
knn_reg.predict([x_test.loc[278100]])[0] > 0
test_data_features = vectorizer.transform(clean_test_reviews)
df2 = df.query('landing_page == "new_page" & group == "treatment"') $ df2 = df2.append(df.query('landing_page == "old_page" & group == "control"'))
vol.describe()
s.iloc[[1,3]] 
crime['Sex'].value_counts()
vow.head()
articles_by_pub = db.get_sql(sql) 
txt_tweets = txt_tweets.apply(preprocess)
import datetime as dt $ import numpy as np $ import pandas as pd
player_search_count_df = pd.DataFrame.from_dict(player_search_count, orient='index').reset_index() $ player_search_count_df.columns = ['name', 'search_count'] $ players_df = pd.read_csv('~/dotaMediaTermPaper/data/players_df.csv') $ players_df = pd.merge(player_search_count_df, players_df, left_on='name', right_on='Name', how='right')
crimes.info(null_counts=True)
def project_link(project_id): $     try: $         return (str(projectTable.index[projectTable['id'] == project_id][0])) $     except: $         return ('Personal')
df_clean4.head()
dfa.head()
def plot_data(date): $     plt.figure() $     plt.plot(data.loc[pd.to_datetime(date), ["TMAX", "TMIN", "TMED"]], marker='o') $     plt.ylim([0, 40]) $     plt.show()
shopList = ["apple", "cherry", "banana", "cucumber","bluberry"] $ ser = pd.Series(pd.Categorical(["apple", "cherry", "banana", "apple","apple"], $                                categories = shopList)) $ ser.value_counts()
full_act_data = steps.join(heart, how='left')
total_rows = len(df) $ print("Total number of rows in the dataset:{}".format(total_rows))
summed
df2.drop('drop_it', axis=1, inplace=True) $ df2.head()
df_new2=df.query('landing_page=="old_page" & group=="treatment" ') $ df_new2.tail(10) $ df_new2.nunique()
with open('test_data//open_close_test.txt') as input_file: $     with open('test_data//write_test.txt', mode='w') as output_file: $         output_file.write(input_file.read()[0:25]) $ with open('test_data//write_test.txt') as output_file: $     print(output_file.read())
contractor_clean.head() #these columns have been dropped
vip_reason = vip_reason.drop(['[', ']'], axis=1) $ vip_reason = vip_reason.drop(vip_reason.columns[0], axis=1)
date_frequency = my_df["date"].value_counts() $ print(date_frequency.head())
import statsmodels.api as sm $ convert_old = df2[(df2['landing_page'] == 'old_page') & (df2['converted'] == 1)].user_id.count() $ convert_new = df2[(df2['landing_page'] == 'new_page') & (df2['converted'] == 1)].user_id.count() $ n_old = df2[df2['landing_page'] == 'old_page'].user_id.count() $ n_new = df2[df2['landing_page'] == 'new_page'].user_id.count()
df.head(2)
post_sentiment_df = mailing_list_posts_mbox_df_saved.select("project_name", lookup_sentiment_udf("body").alias("sentiment"))
crimes.columns
df2.converted.mean() $ df2.describe().loc['mean'].converted
df['lemma'] = np.array([ lemmatiz(comment) for comment in df['body'] ])
df_data.DESCRICAOLOCAL.value_counts()
sub1 = sub1[sub1['contest_id']=='c8ff662c97d345d2']
ab_new['US_ab_page'] = ab_new['US']*ab_new['ab_page'] $ ab_new['CA_ab_page'] = ab_new['CA']*ab_new['ab_page'] $ ab_new
flight6 = spark.read.parquet("/home/ubuntu/parquet/flight6.parquet") $
df.isna().sum()
(x_train, y_train), (x_test, y_test) = fnLoadData(0.2)
start_end_points_df.to_csv('start_end_gps_from_garmin.csv')
df6[df6['hired_not_hired'] ==0].count()
sub1.drop('contest_id', axis = 1, inplace = True)
nums = Series(range(10, 16), $               index=['t', 'u', 'v', 'x', 'y', 'z']) $ nums
rng = pd.date_range('2012-01-01', '2012-01-03')
autos['price'].describe()
df.describe()
transactions[(transactions.transaction_date < datetime.strptime('2017-01-01', '%Y-%m-%d'))]
train['target']=0 $ train.loc[pd.notnull(train.units_purchased ),'target']=1 $ train = train.drop(['units_purchased','lane_number','total_spend'],axis=1)
from scipy.stats import norm $ print("Singificance of z-score: ",norm.cdf(z_score)) $ print("Critical Value: ",norm.ppf(1-(0.05/2))) $
pd.merge(transactions, transactions, on='UserID')
df = ayush['Event Type Name'].value_counts() $ df_ayush = pd.Series.to_frame(df) $ df_ayush.columns = ['Count_Ayush'] $ df_ayush
df_train['date'] = pd.to_datetime(df_train['date'], format='%Y-%m-%d') $ df_test['date'] = pd.to_datetime(df_test['date'], format='%Y-%m-%d')
StockData.columns
from sklearn import metrics $ print "MAE: ", metrics.mean_absolute_error(y_test,predictions) $ print "RMSE: ", np.sqrt(metrics.mean_squared_error(y_test,predictions)) $ print "R2: ", metrics.r2_score(y_test,predictions)
dogscats_h2o = h2o.H2OFrame(dogscats_df) 
station_activity = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                         order_by(func.count(Measurement.tobs).desc()).all() $ station_activity
g_hash = spike_tweets.groupby('hashtags')['id'].count().sort_values(ascending=False)[:5] $ print(g_hash)
df.ix[df.injured > 0, 'injured'] = 1 
df.replace({0: 3}, {0: 99})
random.sample(words.items(), 10)
k_var_state = k_var[k_var['state'] != 'live'] $ print len(k_var) $ print len(k_var_state) $ k_var_state.groupby(['state']).size()
stmt1=employee.delete().where(employee.c.id==1) $ stmt2=employee.delete().where(employee.c.id==2 and employee.c.sex=='Male') $ conn.execute(stmt1) $ conn.execute(stmt2)
department_df_sub.apply(np.sum, axis = 0) # apply function to each column 
flight_cancels = pd.read_csv("../clean_data/cancelled_flights_per_hour.csv")
dollars_per_unit.columns = pd.MultiIndex.from_product([['Dollars per Unit'], dollars_per_unit.columns]) $ pd.concat([multi_col_lvl_df, dollars_per_unit], axis='columns').head(3)
my_date_only_rows= autos["date_crawled"].str[:10] #strip first 9 characters $ my_date_only_rows $
svm_clf = SVC(kernel="linear") $ X = X_train $ y = y_train $ svm_clf.fit(X, y)
x = len(B4JAN16) $ x
data['temp'] = pd.to_datetime(data['temp']) $ data['month'],data['day'] = data['temp'].dt.month, data['temp'].dt.day $ data.head(10)
first_votes = int(first_votes['data-value']) $ first_votes
! head wmt/newstest2015.tok.bpe.32000.de
air_reserve['visit_datetime'] = pd.to_datetime(air_reserve['visit_datetime']) $ air_reserve['visit_datetime'] = air_reserve['visit_datetime'].dt.date $ air_reserve['reserve_datetime'] = pd.to_datetime(air_reserve['reserve_datetime']) $ air_reserve['reserve_datetime'] = air_reserve['reserve_datetime'].dt.date $ air_reserve['reserve_datetime_diff'] = air_reserve.apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)
aviation.sort_values('Deaths', ascending = False).head(1)['Date']
%pylab inline $ import pandas as pd
chambers = chambers[['state', 'swing_state', 'red_state', 'blue_state', 'swing_state2']]
bounds = track.get_bounds()
5760 - 4810
airbnb_df.filter(regex='fee|price|deposit', axis=1).head()
log_reg_over.score(X_test, y_test_over)
df1.dropna(how='any')
new_page_converted = np.random.choice(2,n_new,p=[1-p_new,p_new]) $ print(new_page_converted)
h=sess.get_historical_data(['ibm us equity','aa us equity'],['px last','px open'])
station_query = session.query(Measurement.station).group_by(Measurement.station) $ station_result = pd.read_sql(station_query.statement, station_query.session.bind).count() $ print("There are ", station_result.station , 'station')
df2['intercept'] = 1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2.head()
final_feat.head()
tobs_values_df=pd.DataFrame([tobs_values]).T $ tobs_values_df.head()
for col in b_list.columns: $     if len(b_list[col].unique()) < 2: $         b_list.drop(col,axis=1,inplace=True) $         print(f"Dropped {col}")
print(cv_score) $ print(np.mean(cv_score)) $ print(accuracy_score(y_train, cv_predict_score))
walmart.start_time # The start of Q1 is 1st Feb rather then the normal 1 st Jan
a
stats = CheckAccuracy(y_test, y_test_hat)
df=pd.DataFrame(data, columns=col_names)
cv_results = cross_val_score(regr, X, y, cv = 5)  # 5 fold cross validation $ print(cv_results)  # lots of variation $
gs = gs.fit(X_train, y_train) $ gs.best_estimator_.steps
import pandas as pd $ df = pd.read_csv("autos.csv") $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ print "Dimensiones en el dataset: ",df.shape $ df.head()
autos['odometer'] = autos['odometer'].astype(int) $ autos.rename(columns={'odometer':'odometer_km'}, inplace=True) $ autos['odometer_km'].head()
S_1dRichards.initial_cond.filename
lsi_tf.show_topic(1)
import pandas as pd  # This is an implicit agreement: everytime you need pandas, it is better to import it as `pd`.
scores['point_diff'] = scores['home pts']- scores['away pts'] $ scores.head()
mnb.fit(Bow_X_train, Bow_y_train) $ print('Training set score:', mnb.score(Bow_X_train, Bow_y_train)) $ print('\nTest set score:', mnb.score(Bow_X_test, Bow_y_test)) $ print('\nCross Val score:',cross_val_score(mnb, Bow_X_test, Bow_y_test, cv=5))
df['date'] = pd.to_datetime(df['date'],infer_datetime_format=True)
treatment_df2 = df2.query('group == "treatment"') $ treatment_conversion = treatment_df2['converted'].mean() $ treatment_conversion
autos["registration_year"].describe()
df_comb['country'].value_counts()
from datetime import datetime, timedelta $ df_gene = df[df['action']=="click_genetic_rec"] $ df_gene['created_at'] = pd.to_datetime(df_gene["created_at"]) $ df_gene['created_atc']=df_gene['created_at'].dt.tz_localize('UTC').dt.tz_convert('Asia/Bangkok') $
trunc_df[trunc_df.company_name == 'Airbnb']
temp_cat.set_categories(['low','medium','sederhana','susah','senang'])
def get_table_names(database): $
df2.head()
pnew_null=(df2['converted']==1).mean() $ pnew_null
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3]) $ ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6]) $ pd.concat([ser1, ser2])
week2_df.rename(columns = {' 31 July - 4 August 2017' : 'title'}, inplace=True) $ week2_df['date'] = '31 July - 4 August 2017' $ week2_df
max_trip = int((end_2017 - datetime.datetime(1970, 1, 1)).total_seconds())
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=12000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
! hdfs dfs -ls
autos['registration_year'].describe()
text_classifier = text_classifier.fit(df_train)  
df['target']=df.num_comments.map(lambda x: 1 if x>=med else 0)
countries = pd.read_csv('countries.csv') $ countries.head()
train['discussion'] = train.url.isnull().astype(int) $ train.groupby('discussion').popular.mean()
merged = pd.merge(prop, contribs, on="calaccess_committee_id")
target_pf.iloc[-1]['date']
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] == 'control', 0, 1) $ df2.head()
df2_daily.head()
print(Counter(counter_entities))
df = pd.read_csv('in/gifts_Feb2016_2.csv') $ source_columns = ['donor_id', 'amount_initial', 'donation_date', 'appeal', 'fund', 'city', 'state', 'zipcode_initial', 'charitable', 'sales'] $ df.columns = source_columns
df_archive["timestamp"].sample(5)
gucci_ng           = all_text[all_text.index == "gucci"].sum().sort_values(ascending=False).head(25) $ mcqueen_ng         = all_text[all_text.index == "Alexander McQueen"].sum().sort_values(ascending=False).head(25) $ stellamccartney_ng = all_text[all_text.index == "Stella McCartney"].sum().sort_values(ascending=False).head(25) $ robertocavalli_ng  = all_text[all_text.index == "Roberto Cavalli"].sum().sort_values(ascending=False).head(25)
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
from sklearn.linear_model import LogisticRegressionCV $ lr = LogisticRegressionCV(Cs=5, n_jobs=-1, random_state=42)
predicted = model2.predict(x_test) $ print predicted
n_old = df2.query('landing_page == "old_page"').count()[0] $ print("Number of users with old page :",n_old)
control_conv = conv_ind.query('group == "control"').shape[0] $ control_group = df2.query('group=="control"').shape[0] $ print('Probability of CONTROL page converted individual: {:.4f}'.format(control_conv/control_group))
scenario_dates = df_exp.groupBy('date').sum() $ var_rdd = scenario_dates.map(lambda r: (r[0], r[1], float(var(np.array(r[2:]) - r[1])))) $ df_var = sql.createDataFrame(var_rdd, schema=['date', 'neutral', 'var'])
df_tweet_clean['tweet_id'] = df_tweet_clean['id'].astype(str) $ df_tweet_clean             = df_tweet_clean.drop('id', axis=1) $ df_tweet_clean.head()
ab_df.query('(group == "control" and landing_page == "new_page") or (group == "treatment" and landing_page == "old_page")').count() 
sns.countplot(auto_new.Hand_Drive)
elms_all.ORIG_DATE.max()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],value=None, alternative='smaller', prop_var=False) $ z_score, p_value
df.track_popularity.nunique()
my_df_small = pd.DataFrame(np.arange(10)) $ my_df_large = pd.DataFrame(np.arange(100000))
index_to_datetime(doc_duration) $ index_to_datetime(RN_PA_duration) $ index_to_datetime(therapist_duration)
store_items.dropna(axis=1) # or store_items.dropna(axis=1, inplace=True)
args = mfclient.XmlStringWriter("args") $ args.add("where", "namespace=/projects/proj-hoffmann_data-1128.4.49/individuals") $ args.add("action", "get-path") $ args.add("size", "infinity") $ individuals_query = con.execute("asset.query", args.doc_text())
df2['converted'].mean()
tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]
df['user_id'].nunique()  #Identify the number of unique elements
df2['converted'][(df2['group'] == 'treatment')].sum()/df2['converted'][(df2['group'] == 'treatment')].count()
df_final_ = df_final.query('loc_country == country').reset_index().drop('index',axis=1) $ print(df_final_.head(5)) $ print(df_final_.shape)
survey.head()
df2.set_index('user_id', inplace = True) $ df_c.set_index('user_id', inplace = True) $ df2 = df2.join(df_c)
shirt_1.color
data_CH = importdata('CH', 'BFE')
import requests $ import numpy as np $ import json
pred_probas_under_k150 = gs_k150_under.predict_proba(X_test)
def get_rental_concession_2(description): $     tags = {k for k, p in PATTERNS2.items() if re.search(p, description)} $     if tags: $         return tags
type(df_clean['tweet_id'].iloc[0]) $ type(image_clean['tweet_id'].iloc[0]) $ type(tweet_clean['tweet_id'].iloc[0])
imagehash.hex_to_hash('e6e6c9791516c2c3') - imagehash.hex_to_hash('e6e6c9791516c2e2')
temp = mid.to_frame().join(result.possession) $ temp.plot(secondary_y='possession', figsize=(15, 5));
plot_by_topic(df, nstates=nstates)
!pip3 install fastai
tweet_full_df.to_csv('twitter_archive_master.csv')
links_df=url_df_full[url_df_full['url'].str.contains('http')] $ print(links_df.head())
autos['registration_year'].describe()
ls
func_vals = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).filter(Measurement.station == "USC00519281").all() $ func_vals $
sns.countplot(x='country', data=data_for_model, hue='final_status')
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ result = logit_mod.fit() $ result.summary()
len(df_events), len(df_events.group_id.unique())
trading.df.head()
df2.drop(df2[df2['user_id'].duplicated()].index, inplace=True) $ df2.shape[0]
species_count = faa_data_pandas['SPECIES'].value_counts() $ print(species_count)
unitech_df.head(5)
jcp = JCapper(verbose=True) $ jcp.load(date_range(date(2017, 7, 3), date(2017, 7, 9))) $ jcp.add_computed_columns() $ print('load_jcapper: loaded %d rows' % (len(jcp.df)))
df_ml_55 = df.copy() $ df_ml_55.index.rename('date', inplace=True) $ df_ml_55_01=df_ml_55.copy()
index_missin_hrafter6_before2016
df.head().to_json("../../data/stocks.json") $ !cat ../../data/stocks.json
lostintranslation = tmdb.Movies(153) $ lostintranslation.info()
draw_histogram("Parent Per Hang Delta Distribution (bhr - input)", parent_delta_hist)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
num_unique_users = df.nunique()['user_id'] $ print("There are {} many unique users ".format(num_unique_users))
(np.array(p_diffs) >  obs_diffs).mean()  
feature_importance = exported_pipeline.named_steps['gradientboostingregressor'].feature_importances_ $ feature_importance = 100.0 * (feature_importance / feature_importance.max()) $ print(names) $ print(feature_importance)
Obama_raw = pd.read_csv('data/BarackObama.csv', encoding='latin-1') $ Trump_raw = pd.read_csv('data/realDonaldTrump.csv', encoding='latin-1')
Raw_Forecast["ID"] =  Raw_Forecast.Date_Monday.astype(str)[:] + "/" + Raw_Forecast.Product_Motor + "/" + Raw_Forecast.Part_Number.str[:] $ Raw_Forecast.head(20)
df.sort_index(axis=0, ascending=True)
lds = dw.load_dataset('data-society/the-simpsons-by-the-data')  # , force_update=True)
vacancies['created'] = vacancies['created'].apply(lambda x: dateutil.parser.parse(x))
df_protest.columns = df_protest.columns.str.lower()
X_reduced =X_train[X_train.columns[6:]] $ X_reduced.info() $ X_reduced.head()
au.show_frequent_items(mentions_df,user_names,"src",k=10)
data.groupby("Gender").mean()["Age (Years)"]
s_n_s_epb_one.rename(columns = {0:"Count"},inplace=True)
learn.data.test_dl = test_dl $ log_preds_test = learn.predict(is_test=True);
for _item in sample(vectorizer.stop_words_,100): $     print(_item,end='\t')
import pandas as pd $ import matplotlib.pyplot as plt $ % matplotlib inline $ import seaborn as sns
df3['UK_new_page'] = df3['new_page']* df3['UK'] $ df3['US_new_page'] = df3['new_page']* df3['US']
final = sample.merge(c_df,on="Date",how="outer")
df_twitter_copy[df_twitter_copy.rating_numerator == '3 1']
df3['US_con'] = df3['ab_page']*df3['US'] $ df3['UK_con'] = df3['ab_page']*df3['UK'] $ df3['CA_con'] = df3['ab_page']*df3['CA'] $ df3.head()
QUIDS_wide["y"] = QUIDS_wide[['qstot_12','qstot_14']].apply(lambda x: x['qstot_14'] if np.isnan(x['qstot_12']) $                                                             else x['qstot_12'], axis=1)
import numpy as np $ AAPL.iloc[::3,-1] = np.nan  # assinging scalar value to column, slice broadcast value to each row $ AAPL.head(7)
df = df[df.Category.isin(df.Category.value_counts()[:8].index)]
age.iloc[1]
paired_percentage_payment_cumsum = [(i,j) for (i,j) in zip(percent_of_total_list,cum_sum_percentage_payments)] $ print('paired_percentage_payment_cumsum is of type:',type(paired_percentage_payment_cumsum)) $ paired_percentage_payment_cumsum[:4]
importances = randfor.feature_importances_ $ importanceDF= pd.DataFrame() $ importanceDF['features']= test.ix[:, test.columns != 'class'].columns $ importanceDF['importance']= importances $ importanceDF
cities.reindex([2, 0, 1])
datatest.loc[datatest.place_name == "Terralagos",'lat'] = -34.907001 $ datatest.loc[datatest.place_name == "Terralagos",'lon'] = -58.514885
retailDf = sqlContext.createDataFrame(loadRetailData) $ print retailDf.printSchema()
dcAutos=cleanedAutos.dropna() $ dcAutos.isnull().sum()
filtered_df['years_being_host'] = 2018 - pd.DatetimeIndex(filtered_df['host_since']).year $ filtered_df['years_being_host'] = filtered_df['years_being_host'].fillna(0) $ filtered_df['years_being_host'] = filtered_df['years_being_host'].astype('int64', copy=False) $ filtered_df.drop(['host_since'], axis = 1, inplace=True) $ filtered_df['years_being_host'].head()
result.summary()
house_data.groupby(['bedrooms'])['bedrooms'].count()
list(df_users_first_transaction.dropna(thresh=int(df_users_first_transaction.shape[0] * .9), axis=1).columns)
group_by_month = sorted_by_date.groupby(['contb_receipt_dt']).sum().reset_index() $ group_by_month.contb_receipt_dt = group_by_month.contb_receipt_dt.apply(lambda date: dt.datetime.strptime(date, '%d-%b-%y').month) $ group_by_month = group_by_month.groupby(['contb_receipt_dt']).sum().reset_index() $ group_by_month.contb_receipt_amt
paragraphs = [] $ conversations = [] $ sentences = [] $ for para in hellotrans_ptags[1:]: $     cleaned_para = re.sub(r'(\(\?\))', '', para.text) $
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']] $ df_new.head()
round(x)
shannon_petropavlovsk_relative = shannon_petropavlovsk / np.log2(len(petropavlovsk_freq)) $ shannon_petropavlovsk_relative
df2.query('group == "control" & converted == 1').shape[0] / df2.query('group == "control"').shape[0]
home_dir = "/user/cloudera/movielens/"
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2014 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2014 Percentiles.txt"
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=50000, usecols=['Closed Date', 'Agency Name', 'Complaint Type', 'Borough'])
df.isnull().sum()
def fully_conn(x_tensor, num_outputs): $     return tf.layers.dense(x_tensor, num_outputs, activation=tf.nn.relu) $ tests.test_fully_conn(fully_conn)
to_be_predicted_Day1 = 48.45 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
trips_data['duration'].describe().round() # statistical measures of central tendency and variance are displayed
expenses_df.pivot(index = "Type", columns = "Buyer", values = "Amount")
cassession.CASTable('hmeq_clusters').head()
df['2015-06']
dfWeek['Date'] = dfWeek['Date'].dt.to_period("W").dt.start_time
df.head()
corr_matrix = my_df[["user", "day_from", "time", "week_day", "distance"]].corr() $ print(corr_matrix)
click_condition_meta['refr_source'] = np.where((pd.isnull(click_condition_meta['refr_source'])), $                                                'refr_source not available', click_condition_meta.refr_source)
bact
all_tables_df.OWNER.nunique()
autos = autos[autos["registration_year"].between(1900,2017)] $ autos["registration_year"].value_counts(normalize=True).head(10)
plt.legend(handles=[Precipitation], loc="best") $ plt.savefig("date_Fre.png")
autos['num_doors'].unique()
bikedataframe.dtypes
r['BTC Price'].plot()
from datetime import datetime, date, time $ dt = datetime(2011, 10, 29, 20, 30, 21) $ dt.day $ dt.minute
results1.summary()
treino[treino.sentiment == 0].count() #Tweets com sentimento negativo
df_chdesc = pd.read_csv('http://chilp.it/ad0b6e4') $ print 'Marketing Channels: ', df_chdesc['marketing_channel'].unique(), '\n' $ print 'DataFrame df_chdesc: ', df_chdesc.shape $ df_chdesc
(di - di[0]) + di[-1]
reddit_comments_data.groupBy('author').agg({'score': 'mean'}).orderBy('avg(score)', ascending = False).show()
df2['intercept'] = 1 $ df2[['alt', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('alt', axis=1)
X_test[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True, $                                              layout = (1, 3), sharex = False)
pd.Series(pd.DatetimeIndex(pivoted.T[labels==0].index).strftime('%a')).value_counts().plot(kind='bar');
con = sqlite3.connect('db.sqlite') $ con.execute("CREATE TABLE tbl(wikipedia TEXT, topic TEXT, year INTEGER, month INTEGER, pageviews INTEGER);") $ con.commit() # apply transaction $ con.close()
ins['new_date'] = pd.to_datetime(ins['date'], format='%Y%m%d') $ ins.head(5)
tweets['daysFromStart'] = (tweets['fromStart'] / np.timedelta64(1, 'D')).apply(np.floor) $ tweets['hoursFromStart'] = (tweets['fromStart'] / np.timedelta64(1, 'h')).apply(np.floor)
eth_df = general_info.groupby('ethnicity').count()['candidate_id'].sort_values(ascending=False) $ ethnicity_plot = eth_df.plot.bar(figsize=(13,8), color=['green']) $ for p in ethnicity_plot.patches: $     ethnicity_plot.annotate(str(p.get_height()), (p.get_x() * 1.025, p.get_height() * 1.005))
df_users_6['atleast_one_course_completed_or_not'].unique()
(df_new.style $     .set_table_styles(styles)) $
lines = access_logs_df.count() $ lines
sns.violinplot(california.FIRE_SIZE)
finals['type'] = "normal" $ finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'facilitator'
qty_left_1c_store $ table_store.iloc[0].at[u'Quantity Txn Part'] $
%%time $ %matplotlib inline $ import pandas as pd $ df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv")
tfidfnmf_topics.head()
serious_end_count = 0 $ for row in data: $     if re.search("[\[\(][Ss]erious[\]\)]$", row[0]): $         serious_end_count = serious_end_count + 1 $ print("[2] serious_end_count: " + str(serious_end_count))
catefeatures_index = [4, 5, 9, 10, 11, 12, 13, 14, 16] $ columns[features_index]
kmeans = KMeans(k=3, seed=1) $ model = kmeans.fit(feature_sel)
metadata = pd.read_csv('Data/DataDictionary.csv') $ metadata $
Which_DRGs_in_each_year[2011][:5]
columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"]) $
d = {'x' : pd.Series(random_x),  'y' : pd.Series(random_y)} $ dat = pd.DataFrame(d) $ plt.scatter(dat["x"], dat["y"]) $ dat
data[["Class","Amount"]].groupby(["Class"]).mean()
df = pandas.read_csv('data/201508_trip_data.csv.gz')
india = ['India', 'india', 'INDIA', 'India.', 'New Delhi', 'Mumbai', $          'Bengaluru', 'Pune', 'Hyderabad', 'Kolkata', 'Chennai', 'Bangalore', 'Delhi'] $ notus.loc[notus['country'].isin(india), 'country'] = 'India' $ notus.loc[notus['cityOrState'].isin(india), 'country'] = 'India' $ notus.loc[notus['country'] == 'India', 'cityOrState'].value_counts(dropna=False)
df2['new_d']=df2['DATE'].apply(lambda x: datetime.fromtimestamp(x).strftime("%Y-%m-%d"))
uniqID = df['user_id'].nunique() $ print('The data set contains ' + str(uniqID) + ' unique users')
station_df = pd.read_sql("SELECT * FROM station", conn)
logodds.drop_duplicates().sort_values(by=['count']).to_csv('logsodd.csv')
df['text_cleaned'][40]
ab_df = pd.read_csv('ab_data.csv') $ ab_df.head()
raw_df = raw_df.apply(lambda x: x.astype(str).str.lower()) $ raw_df.head(5)
score.head()
train_df[train_df.author.isnull()].head(1)
data[1:3]
%matplotlib notebook $ df_var.toPandas().plot()
svm_tunned = GridSearchCV(SVC(), svm_parameters, cv=5, scoring='roc_auc')
df = pd.merge(releases, demographics, on='hash_id', how='left')
plt.plot(ages, weights,'.') $ plt.xlabel("mother's age") $ plt.ylabel("birth weight") $
def updateDict(category): $     categoryAmtDict[category] = categoryAmt(category)
df
df2.drop_duplicates(subset = ['user_id'], inplace = True)
p_new = df2['converted'].mean() $ print "Convert rate of an individual received the new page:",p_new
calls_df.groupby(["call_time"])["length_in_sec"].mean()
from sklearn.metrics import classification_report $ target_names = ["Class {}".format(i) for i in range(num_classes)] $ print(classification_report(y_true, predicted_classes, target_names=target_names))
merged_df
C = pd.merge(A,B, on = 'team', how = 'left',left_index=True, right_index=True) $ C
loans_act_arrears_latest_paid_repaid_xirr=cashflows_act_arrears_latest_paid_investor_repaid.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf)) $ loans_act_origpd_latest_paid__repaid_xirr=cashflows_act_origpd_latest_paid_investor_repaid.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
len(timelog)
convo_frame.iloc[top5.index]['q']
le_data = le_data_all.reset_index().pivot(index='country',columns='year') $ le_data.iloc[:,0:3]
pd.to_datetime(df['Created Date'].head(10000), format="%m/%d/%Y %H:%M:%S %p")
most_recent_investment = pd.DataFrame(investment_dates.groupby('investor_uuid')['announced_on'].max()).reset_index()
titanic.fare.describe()
final_log_mod = sm.Logit(df_comb['converted'], df_comb[['intercept', 'ab_page', 'CA', 'CA_new_page', 'UK', 'UK_new_page']]) $ results_final_log_mod = final_log_mod.fit() $ results_final_log_mod.summary()
print 'number of datasets:',len(dataset_dict)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old,n_new], alternative='smaller') $ z_score,p_value
result = customer_visitors.groupby('Yearcol').mean() $ result
df[45:89]
target_list = ['BBC', 'CBS', 'CNN', 'Fox', 'New York Times'] $ target_user_list = ['@BBC', '@CBS', '@CNN', '@FoxNews', '@nytimes']
amenities = list(db.osm.aggregate([{"$match":{"amenity":{"$exists":1}}},{"$group":{"_id": "$amenity",\ $             "count":{"$sum": 1}}}, {"$sort": {"count": -1}}])) $ print len(amenities) $ amenities
df_twitter_copy = df_twitter_copy[df_twitter_copy['retweeted_status_id'].isnull()]
train['From-To'] = train.apply(lambda row: row[1]+'-'+row[2], axis = 1) $ test['From-To'] = test.apply(lambda row: row[1]+'-'+row[2], axis = 1)
df_estimates_false = df_estimates_false[df_estimates_false['release_id']==122711] $ print(df_estimates_false)
len(temperature)
iowa = pd.read_csv(iowa_csv) $ iowa.head()
df_mas['rating_numerator'] = df_mas['rating_numerator'].astype(str) $ df_mas['rating_denominator'] = df_mas['rating_denominator'].astype(str)
df_master.drop('created_at',axis =1, inplace = True) $ df_master.info()
df['department'] = 'Sociology' $ df
data2.head()
np.eye(3, dtype='float64')  #Identity matrix. float64 is the default dtype and can be omitted
k = pd.read_sql_query(QUERY, conn) $ k
df.shape
df.isnull().sum()[0]
data = df.values
df3.country.unique()
print("Shape of data frame: " + str(df.shape))
requests.get(FAFRPOS_pdf)
temp_us = temp_nc.variables['air'][1, lat_li:lat_ui, lon_li:lon_ui] $ np.shape(temp_us)
df5_lunch = df5.between_time('11:00:00', '13:00:00') # one of my favourite methods so far in pandas $ df5_lunch 
Features = iris.values[:, :4] $ species = iris.values[:, 4]
merged=pd.merge(train,items,on='item_nbr', how='left') $ print("Rows and columns:",merged.shape) $ pd.DataFrame.head(merged)
find_string_in_dict(dataset_dict,'population')
data[:10]
Temperature_year = session.query(Measurements.date,Measurements.tobs) \ $              .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \ $              .filter(Measurements.station == 'USC00519281').all() $ Temperature_year
walk.resample("1Min", closed="right")
print("Number of Relationships in Enterprise ATT&CK") $ relationships = lift.get_all_enterprise_relationships() $ print(len(relationships)) $ df = json_normalize(relationships) $ df.reindex(['id','relationship', 'relationship_description', 'source_object', 'target_object'], axis=1)[0:5]
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
import sentiment_impact as si $ np.random.seed(50) $ a = np.random.normal(size=2000) $ a = a/max(abs(a)) $ print('The sentiment is {}'.format(si.overall_sentiment(a)))
musk.head()
y_pred = log_reg.predict(X_test)
convert_rate_p_old = df2.converted.mean() $ print('Convert rate of p_old under the null is:{}'.format(convert_rate_p_old))
df.info()
train_df.describe()
print_body(response, skip_audit_info=True)
df_subset['Initial Cost'] = pd.to_numeric(df_subset['Initial Cost'].str.replace('$',''), $                                           errors = 'coerce') $ df_subset['Total Est. Fee'] = pd.to_numeric(df_subset['Total Est. Fee'].str.replace('$',''), $                                           errors = 'coerce') $ df_subset.info()
autos["price"].describe()
df['units_purchased'] = pd.to_numeric(df.units_purchased) $ df['total_spend'] = pd.to_numeric(df.total_spend, errors='coerce')
f_counts_week_os = spark.read.csv(os.path.join(mungepath, "f_counts_week_os"), header=True) $ print('Found %d observations.' %f_counts_week_os.count())
import test_package.print_hello_direct
print("The latest Mars news is:",title) $ print("The summary of this latest news is:",news_p)
del crimes_all
type(all_attack)
df_result.sort('loglikelihood', inplace=True) $ df_result.to_csv('loglikelihoods.csv')
Show_Row(election_data, election_data['st'], "DE", list1) $
data2Scaled = data1Scaled.dropna(inplace=False)
building_pa_prc_shrink.sample(5)
df.year_built.isnull().sum()
from pyspark.sql.types import * $ from pyspark.sql import SQLContext $ sc = pyspark.SparkContext(appName="graph") $ sqlContext = SQLContext(sc) $
print(autos['price'].unique().shape) $ autos['price'].describe()
char_index = dict((c, i) for i, c in enumerate(vocab)) $ print char_index
from nltk.stem.wordnet import WordNetLemmatizer $ lemmed = [WordNetLemmatizer().lemmatize(w) for w in words] $ print(lemmed)
df['hireable'] = df['hireable'].fillna('0').astype('int64')
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False)
c1 = covtype_df[covtype_df['Cover_Type'] == 'class_1'] $ c2 = covtype_df[covtype_df['Cover_Type'] == 'class_2'] $ df_b = c1.rbind(c2)
del df_train['visitId'] $ del df_test['visitId'] $ del df_train['totals.newVisits'] $ del df_test['totals.newVisits']
df_ad_airings_4.drop('ad_duration',inplace=True, axis=1)
tweets_clean.rating_numerator.unique()
import requests $ import pandas as pd $ import datetime $ import json
treatment = df2.query("group == 'treatment'")
TEXT = pickle.load(open(f'{PATH}models/TEXT.pkl','rb'))
from sqlalchemy.sql import select $ s = select([employee]) $ result = conn.execute(s) $ row = result.fetchall() $ print(row)
search_response.json()
from sklearn.preprocessing import LabelEncoder $ from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer $ from sklearn.decomposition import TruncatedSVD $ from nltk.corpus import stopwords
import pandas as pd $ import geopandas as gpd $ from shapely.geometry import Point
df_columns[df_columns.index.month.isin([5,6,7])]['Complaint Type'].value_counts().head() $
df['body_tokens'] = df['body'].str.lower() $ print(df[['body','body_tokens']])
df_potholes = df[df['Descriptor'] == 'Pothole'] $ df_potholes.groupby(df_potholes.index.hour)['Created Date'].count().plot(kind="bar") $
df2.drop_duplicates(subset = "user_id",inplace = True)
data.loc[data.surface_covered_in_m2.isnull(),'surface_covered_in_m2'] = data['surface/price']*data['surface_total_in_m2']/data['price_aprox_usd'] $ del data['surface/price']
pd.date_range('2018-01-01', '2018-01-03', freq='T')
linkNYC.head()
fig, ax = plt.subplots(figsize = (14.5,8.2)) $ ax.hist(events_df['words_count'], 50, facecolor='green', alpha=0.75) $ plt.show()
print(len(df.index))
age
tia['date'] = tia['date'].apply(lambda x: x[14:]) $ tia['date'][0]
4.832214765100671 * 298
actual_diff = df[df['group'] == 'treatment']['converted'].mean() - df[df['group'] == 'control']['converted'].mean()
train['Embarked'].mode()
sales = sales.join(shops.set_index('shop_id'))  # train + shops join by shop_id $ items_categories = item_categories.join(items.set_index('item_category_id'))    # item_categories + items join by item_category_id $ sales = sales.join(items_categories.set_index('item_id'))    # train + items_categories join by item_id
log_2 = pd.read_csv('log_2.csv') $ log_2.head()
y_train = train['Sales'] $ y_validation = validation['Sales'] $ X_train = train[featuresToTakeForTraining] $ X_validation = validation[featuresToTakeForTraining] $ X_test = df_test[featuresToTakeForTraining]
average_mileage = {} $ for brand in brand_list: $     average_mileage[brand] = autos[autos['brand'] == brand]['odometer_km'].mean() $ average_mileage
for household_name, household_dict in households.items(): $     data_households[household_name].to_pickle('raw_'+household_dict['dir']+'.pickle')
p_convert
print(kmeans.cluster_centers_) $ print(kmeans.labels_)  $
groups = groups.loc[groups.UPD_DATE <=  groups.CRE_DATE_GZL, :].groupby(['INSTANCE_ID', 'CRE_DATE_GZL'])
MAX_TWEETS = 200 $ tweet_spd = [] $ for tweet in tweepy.Cursor(extractor.search, q='#bvb', rpp=100).items(MAX_TWEETS): $     tweet_spd.append(tweet.text) $     pass $
Vycnn = np.array(Vy)
categorised_words = pd.read_csv('/s3/three-word-weather/hack/categorised_words.csv', header=0, names=['word', 'weight', 'category'], na_values=' ') $ top_words = list(categorised_words.loc[categorised_words['category'] == 'temperature'].sort_values(by=['weight'], ascending=False)[:10]['word'])
df_new.tail()
df['day_of_week'] = df['date'].dt.dayofweek $ days = {0:'Mon',1:'Tues',2:'Wed',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}
f = lambda x: x.max() - x.min()
df['currency'].unique()
fig_size = plt.rcParams["figure.figsize"] $ fig_size[0] = 12 $ fig_size[1] = 9 $ plt.rcParams["figure.figsize"] = fig_size
logs.name.tail(13)
pd.Series([1, np.nan, 2, None])
autos["odometer_km"].value_counts()
s[[1, 2]]
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
price_odometer["odometer_km"] = odometer $ price_odometer
total_sales.corr()
baseball.hr.rank()
building_pa_prc.sample(5)
df['atbat_pk'] = df['game_pk'].astype(str) + df['at_bat_number'].astype(str) $ df['is_shift'] = np.where(df['if_fielding_alignment'] == 'Infield shift', 1, 0)
X_train
store_items['suits'] = store_items['pants'] + store_items['shirts'] $ store_items
import pandas as pd $ import mrec
ip_clean.tweet_id = ip_clean.tweet_id.astype(str)
prop_users_converted
! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py $ import setup_google_colab $ setup_google_colab.setup_week1()  # change to the week you're working on
import time $ print(time.localtime())
speeches_df4.drop_duplicates(inplace = True)
q_derived_count.results()["count"].sum()
print('Ridge Coefficient: \n', ridgemodel.coef_) $ print('Ridge Intercept: \n', ridgemodel.intercept_)
df4[['CA', 'UK', 'US']] = pd.get_dummies(df4['country']) $ df4.head()
train = make_features(train) $ new = make_features(new)
(pf_data / pf_data.iloc[0] * 100).plot(figsize=(10, 5))
results = calc_temps('2010-01-01', '2012-01-01')[0] $ error = results[0:2] $ data = results[2] $ error
analysis = None
scores_mode = scores.sort_values(ascending=False).index[0] $ print('The mode is {}.'.format(scores_mode))
cbd = CustomBusinessDay(holidays=cal.holidays()) $ datetime(2014,8,29) + cbd
cityID = 'a3d770a00f15bcb1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Corpus_Christi.append(tweet) 
old_page_converted = np.random.choice([0, 1], size=n_old, p=[1 - p_old, p_old])
trigram_dictionary_filepath = paths.trigram_dictionary_filepath
S.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
nullCity = youthUser4.loc[youthUser4["cityName"]=="UNKNOWN"] $ nullCity.head()
active_distinct_authors_latest_commit.show() $ print(active_distinct_authors_latest_commit.count()) $ active_distinct_authors_latest_commit.take(5)
df
def stem_func(word): $
dict_profile = json.loads(df.loc[row,'profile']) $ pprint(dict_profile)
cohort_active_activated_df
df.to_pickle("dfAllGPSTweetsFilterChile.p")
plt.hist(np.log(threeoneone_census_complaints[threeoneone_census_complaints['complaint_density']>0]['complaint_density']+1),bins=100) $ plt.show()
search['days_plan_ahead'] = (search['trip_start_date'] - search['timestamp']).dt.days+1
all_df.shape
test.shape
df['previous_payouts'][1] #Interesting- can we see if someone might be potentially fraud $
uids = uids.reshape((len(uids), 1)) $ mids = mids.reshape((len(mids),1)) $ rates = rates.reshape((len(rates),1))
n_old = df2[df2['group']=='control'].shape[0] $ n_old
features = ["Log_price","num_photos","created_month","created_day","bathrooms", "bedrooms", $             "latitude","longitude","description_score","num_features","num_description", $             "created_dayofyear","manager_level_low","manager_level_medium","manager_level_high", "display_address","street_address"] $ X = train_df[features].iloc[:,0:].values
dict_wells_df_and_Nofeatures_20180707 = dict_of_well_df $ pickle.dump(dict_wells_df_and_Nofeatures_20180707, open( "dict_of__wells_df_No_features_class3_20180707.p", "wb" ) )
import pandas as pd $ %matplotlib inline
cnx.commit() $ c.fetchall()
dfm = (filtered_df['l_req_3d'] - filtered_df['l_req_3d_num_tutors']) $ filtered_df.loc[dfm > 0, :]
archive.puppo.value_counts()
n_samples = 40000 $ n_features = 2000 $ n_topics = 5 $ n_top_words = 10
model.save("/home/kmisiunas/Documents/Quipu/models/binding_metric971_2018-04-03_no1.h5") $
manager.image_df[manager.image_df['filename'] == 'image_sitka_spruce_71.png'] $
df_ml_57 = df.copy() $ df_ml_57.index.rename('date', inplace=True) $ df_ml_57_01=df_ml_57.copy()
users = pd.read_csv("data/2017/user-geocodes-dump.csv", quotechar='"', skipinitialspace=True) $ print('Shape before dropping duplicates', users.shape) $ users = users.drop_duplicates(subset='id', keep='last') $ print('Shape after  dropping duplicates', users.shape) $ users.head()
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05)))
heart = pull_act_range('2018-04-15', '2018-05-14', auth2_client, 'activities/heart')
folder_name = 'image-predictions' $ if not os.path.exists(folder_name): $     os.makedirs(folder_name)
feature_importances = rnd_reg.feature_importances_ $ feature_importances
agg_trips_data.plot.scatter(x='start_station_id',y='count', s= 20, c= 'r') $ agg_trips_data.plot.scatter(x='start_station_id',y='mean_duration') $ agg_trips_data.plot.scatter(x='start_station_id',y='total_duration') $
print(parse('1/10/2020')) $ print(parse('1/10/2020', dayfirst=True))
from scipy import stats $ stats.chisqprob = lambda chisq, df2: stats.chi2.sf(chisq, df2) $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ log_mod_results = log_mod.fit()
y = df['hospitalmortality'].values $ X = df.drop(['hospitalmortality'], axis=1).values $ X_header = df.drop(['hospitalmortality'],axis=1).columns
countries_df = pd.read_csv('./countries.csv') $ countries_df.head()
datetime.date.today()
control_conv = df2.query('group == "control"')['converted'].mean() $ control_conv
df_CLEAN1A['AGE'].max()
df["tokens"] = df["text"].apply(tokenise)
popCon[popCon.content == 'album'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
r.json()['dataset_data']['column_names']
CIN = pd.read_excel(url_CIN, $                     skiprows = 8)
conn.upload('https://github.com/sassoftware/' $             'sas-viya-programming/blob/master/data/class.csv')
!wget --directory-prefix=downloads ftp://ftp.ensembl.org/pub/release-85/gff3/homo_sapiens/Homo_sapiens.GRCh38.85.gff3.gz
data['b']
avg_per_seat_price_seasonsandteams["2013 Offseason", "BAL"] - avg_per_seat_price_seasonsandteams["2012 Season", "BAL"] $
movie2000rating=pd.merge(ratings,new_df, on='movieId', how='inner')
df1 = pd.DataFrame(list('ABC'), columns=['c1']) $ df2 = pd.DataFrame(list('DEF'), columns=['c2']) $ pd.merge(df1, df2, left_index=True, right_index=True)
austin.shape
df_master.info()
from pyhive import presto
headers = {'X-Api-Key': '[Enter your API Key Here]'} $ r = requests.get(url1, headers= headers)   $ r.status_code
autos[((autos.price > 500000) & ~(autos.name.str.contains("Ferrari")) )] $ autos=autos[~((autos.price > 500000) & ~(autos.name.str.contains("Ferrari")) )] #antimasking $ autos.shape #stripped off 12 rows
census_pd_complete.loc[:'Population' ,:'Poverty' ,:'Rate for Housing Units']
df=pd.read_csv('ab_data.csv') #. importing the csv file to the dataframe $ df.head(10) #. printing the first 10 rows, to note the dataset 
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=4000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
summary[summary['Topic Num']==6].head(10)
text=[tweet.text for tweet in all_tweets] $ text[:5]
import pandas as pd $ import numpy as np $ import re
df_master.head()
to_be_predicted_Day4 = 14.80705454 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
!h5dump 'data/my_pytables_file.h5'
print(dir(doc))
content_rec = graphlab.recommender.item_content_recommender.create(sf_business, 'business_id')
parent_input_count_hist = cached.map(lambda x: x[1]['parent_input']).histogram(range(20)) $ draw_histogram("Parent Input Hangs Distribution", parent_input_count_hist)
transit_df.info()
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns
def basicPredict(inpData): $     m, c = np.polyfit(inpData["Flat dates"], inpData["Adj_mean"], 1) $     prediction = c + m*(inpData["Flat dates"][-1]+7) $     return prediction
comps_df.rename(columns={'company_name_x': 'company_name','company_name_y': 'competitor_name'}, inplace=True)
lasso = Lasso(alpha=0.0002) $ lasso.fit(train_data, train_labels)
from sklearn import linear_model $ model = linear_model.LinearRegression() $ print ('Linear Regression') $ reg_analysis(model,X_train, X_test, y_train, y_test)
df_node = pd.read_sql(query_node,engine) $ df_node=df_node[['nid','title']] $ df_download = pd.read_sql(query_download,engine) $ df_download['date']=df_download['date'].apply(lambda x: datetime.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d')) $ df_download['date']=pd.to_datetime(df_download['date'])
texts = ['It always seems impossible until its done.', $          'In order to succeed, we must first believe that we can.', $          'Life is 10% what happens to you and 90% how you react to it.', $          'Start where you are. Use what you have. Do what you can.',] $ cur.executemany('INSERT INTO test.test_table (text) VALUES (%s)', texts) $
df_new[['Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', 'Wednesday']] = pd.get_dummies(df_new['day_of_week'])[['Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', 'Wednesday']] $ df_new.head()
data['VolRatio'] = (data['ATR'] / data['ATR5']) $ data.tail()
ds_valid = FileDataStream.read_csv(valid_file, collapse=False, header=False, names=columns, numeric_dtype=np.float32, sep='\t', na_values=[''], keep_default_na=False)
def downsample_data(dataframe1): $
session = Session(engine) $ conn = engine.connect()
child.head()
sum(df2[df2['group'] == 'control']['converted'])
f.info()
print("tweet archive contains {} entries.".format(len(tweet_archive_df.tweet_id))) $ print("tweepy api retrieved {} entries.".format(len(tweet_json_df.id)))
print('The number of unique species in 200-km circle around the Petropavlovsk-Kamchatsky city:', len(petropavlovsk_filtered.species_id.unique()))
df = df.set_index(df.Date)
response = requests.get(mars_news_url)
to_be_predicted_Day3 = 14.84307103 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df_mas = fin_df.sort_values('dog_stage').drop_duplicates('tweet_id', keep = 'last')
litters = pd.read_csv('data/litters.csv') $ litters.head()
df.drop(df[df.amount < 0].index, axis=0, inplace=True)
print("Row information of repeated user_id:") $ df2[df2.user_id.duplicated(keep=False)]
X_test_term.shape
import pandas as pd $ from pandas_datareader import data $ import matplotlib.pyplot as plt $ %matplotlib inline
dft[pd.datetime(2013, 1, 1):pd.datetime(2013, 2, 28)]
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head(8)
df['mood']= df['SA'].rolling(window=5).apply(lambda x : np.sum(x))
data.iloc[[1,100,20], [6,7,8]].plot(kind='bar', stacked=True)
df_twitter_extract_copy['tweet_id'] = df_twitter_extract['tweet_id'].astype(str)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ print('Size of new_page_converted: ', len(new_page_converted))
prob_new = df2[df2.group == 'treatment'].converted.mean() $ prob_new
from sklearn import feature_extraction $ sklearn.__version__
pd.concat([city_loc, city_pop], join="inner")
bacteria2.mean()
topUserItemDocs.columns=['item_index_corpus','score','user_id','source_item_id','score_weight','item_id'] $ topUserItemDocs.head()
l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2)) $ l2_tfidf
sns.distplot(autodf.powerPS)
predictions_table.count()
url_PHI = "https://manage.strmarketplace.com/Images/Teams/PhiladelphiaEagles/SalesData/Philadelphia-Eagles-Sales-Data-PSLs.xls"
df2.head()
print weather_data1.shape $ print weather_data1.columns.values
null_values = np.random.normal(0, p_diffs.std(), 10000) $ plt.hist(null_values) $ plt.axvline(pop_diff, c='red') $ plt.show()
cityID = '7a863bb88e5bb33c' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Anchorage.append(tweet) 
for line in autoData.collect(): $     print (len(line))
import datetime as dt $
f_counts_week_device = spark.read.csv(os.path.join(mungepath, "f_counts_week_device"), header=True) $ print('Found %d observations.' %f_counts_week_device.count())
dci['weekEnergy'].max()
print(data_new)
cohort_retention_df.fillna(0,inplace=True)
finalData=dat.append(Stockholm_data) $ X.f = vectorizer.fit_transform(finalData['tweet_text']) $ y.f = finalData['class'] $
poverty.drop(385, inplace=True)
p_diffs = [] $ for i in range(10000): $     p_diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_new, 10000)/n_old $
df2.drop(2893);
import statsmodels.api as sm $ convert_old = df.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new =  df.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = df[df['group'] == 'control'].shape[0] $ n_new = df[df['group'] == 'treatment'].shape[0]
df['created_at'] = pd.to_datetime(df.created_at)
forecast = m.predict(future) $ forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()
active_with_return.iloc[:,1] = pd.to_datetime(active_with_return.iloc[:,1])
import time $ import datetime as dt $ import matplotlib.dates as mdates
len(train_data[train_data.fuelType == 'cng'])
qt_convrates_toClosedWon.applymap(lambda x: "{0:.2f}%".format(x * 100.00)) $
num_early_users = len(people_person[people_person['date_joined'] <= '2017-01-12']['id'].unique()) $ print("The number of users signed up prior to 2017-01-12 is: ", num_early_users)
df_index_demo = df_protest.iloc[0:3, 0:3]
p_conv_treat = df2.query('converted == "1" and group == "treatment"').user_id.nunique() / df2.query('group == "treatment"').user_id.nunique() $ p_conv_treat 
!mkdir imsa-cbf
len(cleansing)
bufferdf = taxiData[['RateCodeID','Fare_amount']]
df_os['domain'] = df_os['domain'].apply(remove_www) $ df_os.head()
df['time'] = df['closed_date']-df['created_date'] $ df.head(3)
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ print(last_year)
start = time.time() $ lrmodel = lr.fit(trainData) $ end = time.time() $ print(end - start)
monday = df[df["day"] == 18] $ tuesday = df[df["day"] == 19] $ friday = df[df["day"] == 15]
wrd_clean['name'].value_counts()[:10]
p_diffs=[] $ new_convert=np.random.binomial(n_new, p_new, 10000)/n_new $ old_convert=np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs=new_convert-old_convert
autos['odometer'] = (autos['odometer'] $                  .str.replace('km','') $                  .str.replace(',','') $                   .astype(float) $                  )
(df.converted == True).mean()
conv_learner.ConvLearner.pretrained(arch, data, ps=0, precompute=True)
import datetime $ outdir = "/Users/jeriwieringa/Dissertation/drafts/data/word-lists/" $ with open("{}{}-Base-Word-List-SCOWL&KJV.txt".format(outdir, str(datetime.date.today())), 'w') as outfile: $     for each in spelling_dictionary: $         outfile.write("{}\n".format(each))
df.user_id.nunique()
df2 = df[~mismatch].copy()
data.shape
df.index
df = pd.read_csv('msft.csv', $                    names = ['open', 'high', 'low', $                            'close', 'volume', 'adjclose']) $ df.head()
t = pd.read_csv('count.csv')
autos['registration_year'].describe()
autos['price'] = autos['price'].str.replace("$","").str.replace(",","").astype(int) $ autos['price'].head()
last_date = session.query(measurements.date).order_by(measurements.date.desc()).first() $ year_ago = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ year_ago
t.second
to_be_predicted_Day1 = 35.82 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
for v in cat_vars: joined[v] = joined[v].astype('category').cat.as_ordered() $ for v in contin_vars: joined[v] = joined[v].astype('float32') $ dep = 'Sales' $ joined = joined[cat_vars + contin_vars + [dep, 'Date']]
print(r_clean.shape, hist_alloc.shape) $
discConvpct.head()
k = k.sort_values('count',ascending=False)
df = pd.read_csv("ab_data.csv") $ df["intercept"] = 1 $ df[["control","treatment"]] = pd.get_dummies(df["group"]) $ df.head()
print("Read {} labeled train reviews, \ $ {} labeled test reviews, and \ $ {} unlabeled reviews\n".format(train["review"].size, $                                test["review"].size, $                                unlabeled_train["review"].size))
from arcgis.gis import GIS $ from IPython.display import display $ gis = GIS() # anonymous connection to www.arcgis.com
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-12-29&end_date=2017-12-29&api_key=' + API_KEY) $ r.json()
plt.scatter(my_df["xcoordinate"], my_df["ycoordinate"]) $ plt.show()
fig, ax = plt.subplots() $ typesub2017['Wind Offshore'].plot(ax=ax, title="Offshore Wind Energy Generation 2017 (DE-AT-LUX)", lw=0.7) $ ax.set_ylabel("Offshore Wind Energy") $ ax.set_xlabel("Time")
df.head()
foil_538 = pd.read_csv("data/uber/uber-foil.csv", parse_dates=["date"]) $ foil_538.head(3)
x.iloc[[0,2,1],:]
data_full = data_full.drop(['customer_id', 'invoice_id', 'created_at', 'due', 'last_payment_date', 'paid_status_PARTIAL', 'paid_status_UNPAID', 'customer_created_at'], axis = 1)
df=pd.read_csv('ab_data.csv') $ df.head(10)
df_new.groupby("country").size()
df=pd.read_json('dataframe.json') #check the last row $ df.loc[1057,:]
testObjDocs.outDF.drop(testObjDocs.outDF.index[985:990], inplace=True)
glm_binom_feat_1.accuracy(valid=True)
if  not os.path.exists("movies.csv"): $     print("Missing dataset file")
from gensim import models, similarities $ lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=100)
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_axial_brace_saddle[0][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_saddle[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
treatment=df2.query('group=="treatment"') $ exp=df2.query('group=="treatment" and converted==1').count()[0]/treatment.count()[0] $ exp
client_credentials_manager = SpotifyClientCredentials(client_id, client_secret) $ sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)
tweet_image_predictions_clean.info()
%store -r lin_reg
rounds['announced_year'] = rounds['announced_on'].dt.year
rides_urban = urban['fare'].count() $ rides_suburban = suburban['fare'].count() $ rides_rural = rural['fare'].count() $ total_ride = [rides_urban, rides_suburban, rides_rural] $
title_mapping = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5} $ for dataset in combine: $     dataset['Title'] = dataset['Title'].map(title_mapping) $     dataset['Title'] = dataset['Title'].fillna(0) $ train_df.head()
client.repository.list_models()
b = df2['landing_page'] == 'old_page' $ n_old = df2[b] $ n_old = n_old.converted.count() $ print('Number of rows in which landing page is aligned with new page is:{}'.format(n_old))
merged.committee_position.value_counts()
df1 = pd.read_csv('loan_test.csv') $ df1['due_date']=pd.to_datetime(df1['due_date']) $ df1['effective_date']=pd.to_datetime(df1['effective_date']) $ df1.head()
plt.hist(np.log(threeoneone_census_complaints['median_income_new']+1),bins=100) $ plt.show()
etsamples_100hz = etsamples_100hz.query("smpl_time>%.2f"%(max(etsamples_100hz.groupby(["eyetracker"]).smpl_time.agg(min))))
now = Time.now() $ print(now) $ print(now.mjd)
plt.scatter(pairs['score_x'].loc[0:2261:2],pairs['score_x'].loc[1:2261:2]) $ plt.plot(np.arange(50,105),np.arange(50,105),color='red') $ plt.show()
type(df)
user_tweet_count_df = tweet_df[['user_id', 'tweet_type']].groupby(['user_id', 'tweet_type']).size().unstack() $ user_tweet_count_df.fillna(0, inplace=True) $ user_tweet_count_df['tweets_in_dataset'] = user_tweet_count_df.original + user_tweet_count_df.quote + user_tweet_count_df.reply + user_tweet_count_df.retweet $ user_tweet_count_df.count()
archive_clean.drop(['rating_numerator', $                     'rating_denominator'], $                   axis=1, inplace=True)
returns = generate_returns(close) $ helper.plot_returns(returns, 'Close Returns')
a=[] $ a[0] = 5.5
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country']) $ df2.drop(['country', 'US'], axis = 1, inplace = True) $ df2.head()
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['personalize'])
baseball_h.index.is_unique
pp.ProfileReport(test)
df2.drop(2893, inplace=True) $ df2[df2.user_id == 773192]
import pandas as pd $ import requests
import numpy as np
newdf.set_index('Date', inplace=True)
nulls=pd.DataFrame(merged[pd.isnull(merged.onpromotion)]) $ stores_with_nulls=len(nulls['store_nbr'].unique()) $ all_stores=len(stores['store_nbr'].unique()) $ stores_with_nulls/all_stores
TEXT.vocab.itos[1003]
weights
ll=[['ff','rtert','fgdfgf'],['ff','trt','hgj'],['trt','rtert','tyt']] $ model2 = gensim.models.KeyedVectors.load_word2vec_format(ll, binary=False)
url = form_url(f'teams/{team_id}/squads', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
todrop2 = df.loc[(df['group'] == 'control') & (df['landing_page'] == 'new_page')].index
new_page_converted = np.random.choice([0,1], size=n_new, p=[1-p_new, p_new]) $ print(new_page_converted.mean())
df_A[(df_A.Student_height>160) & (df_A.Student_weight<80)].head(1)
text = "This is a text with one emoticon :) and another :(" $ for (repl, regx) in emoticons_regex : $     text = re.sub(regx, ' '+repl+' ', text) $ print(text)
pca.explained_variance_ratio_
save_n_load_df(joined, 'joined_holidays.pkl')
import config
matthew.head()
PYR['soup'] = PYR.apply(create_soup, axis = 1)
writer = pd.ExcelWriter("../visualizations/uber_hour_of_day.xlsx")
print(filou.buddy.name) $ print(filou.buddy.age)
df.describe()
port_val.head()
ndvi_coarse = interp_spline(grid_lat, grid_lon)
breaches.groupby('IsVerified').size()
df_q = pd.read_sql(query, conn, index_col='work_order_id') $ df_q
grouped_by_day_df = full_df.groupby('day')['listing_id'].count().reset_index().copy() $ grouped_by_day_df.columns = ['day_of_month','count_of_listings'] $ grouped_by_day_df.head(3)
Bow_train_X = train_Bow.drop([0, 1, 2, 4, 5, 6], 1) $ Bow_train_X = scalar.fit_transform(Bow_train_X) $ Bow_train_y = train_Bow[3].astype(int) $ Bow_X_train, Bow_X_test, Bow_y_train, Bow_y_test = train_test_split(Bow_train_X, Bow_train_y, random_state=24)
df2_portfolio_value =  data['Strategy Return'].cumsum() $ perf2 = df2_portfolio_value.calc_stats() $ print perf2.display()
import pandas as pd $ from datetime import datetime
orgs.loc[0]
merge.amount.sum()
prediction_test = lr_best_model.transform(hashed_test) $ prediction_test.show(1, truncate=False)
print(dfpolicies[:10])
process(text[0])
patient_times
autos['date_crawled'].str[:10].value_counts(dropna=False).sort_index()
filecounts = pd.Series(collections.Counter([t[1] for t in allfiles()]), name='datafiles')
RDDTestScorees.map(lambda entry: (entry[0], entry[1] * 0.9)).collect()
print(my_df["xcoordinate"].describe()) $ print() $ print(my_df["ycoordinate"].describe())
df_ml_713_01.tail(5)
df.iloc[1:3,:]
autos.shape
df2['landing_page'][df2['landing_page'] == 'new_page'].count()/290584
df1=data $ del df1['day'] $ df1.head(5) $
le = LabelEncoder()
np.isnan(StockData).sum()
temperature_sensors_list = [sensor for sensor in sensors_list if 'temperature' in sensor.split('.')[1]] $ temperature_sensors_list
md = pd.read_csv(os.path.join(srcdir, 'speaker_metadata.csv')) $ md
data = pd.read_csv("data.csv", sep=";", index_col=0) $ print(data)
graf_counts = pd.DataFrame(graffiti['graffiti_count'].groupby(graffiti['AFFGEOID']).sum())
for layer in model.layers: $     weights = layer.get_weights() # list of n $ weights
df = df.reset_index(drop=True)
df.columns
from pyspark.sql.types import DoubleType $ lookup_user_means = ... $ def zero_mean(user_id, rating): $     pass $ df_centered = df_selected.withColumn('ratings_centered', df_selected.stars - F.avg('stars').over(w2))
techmeme = pd.read_csv('web_scraping/techmeme.csv', index_col=0)
Image("/Users/jamespearce/repos/dl/data/dogscats/train/cat.3822.jpg")
my_df["week_day"] = my_df["timestamp"].dt.dayofweek $ print(my_df["week_day"].head())
some_rdd = sc.parallelize([Row(name=u"John", age=19), $                            Row(name=u"Smith", age=23), $                            Row(name=u"Sarah", age=18)]) $ some_rdd.collect()
len(df.columns)
1928 + 1965
!head data/countries.csv
Ralston.describe()
route = 'C:/PROJECTS/WR_DATA/' $ wellpath = 'C:/PROJECTS/WR_DATA/RawWellogs/' $ syspath = 'C:/PROJECTS/WR_DATA/RawSystems/'
session = Session(engine)
df2
tweet_json_clean.rename(index = str, columns={"id": "tweet_id"}, inplace = True)
url = "https://www.reddit.com/hot.json"
for i in places.index: $     results = Geocoder.reverse_geocode(places[i]['coordinates'][0], places[i]['coordinates'][1]) $     print(results.city)
X_train = add_features(X_train) $ X_test = add_features(X_test)
df1.unstack()
class_merged_city=class_merged_hol['city'].unique() $ print(class_merged_city)
df_merged[['US', 'UK']] = pd.get_dummies(df_merged['country'])[['US','UK']] $ df_merged.head()
index_ts.head()
df.head()
imagelist = [i for i in os.listdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS')\ $              if i.endswith('Percentiles.txt') ] $ imagelist.sort() $ for k in range(5): $     print(imagelist[k])
weather['dateShort'] = pd.to_datetime(weather['EST']) $ print weather.ix[:, 'dateShort'].head()
len(df_merged[0].unique())
df2.drop(2893, inplace=True)
print(est.summary())
workspace_dir = os.getcwd()+"\\Temp"
client.get_list_database()
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df['group']) $ df2 = df2.drop(['control'], axis = 1) $ df2.sample(5)
pd.read_csv('data/cleaned_data.csv').shape
log_reg_pred = log_reg.predict(test)
df = pd.read_csv("data/csvfiles/test1.csv", $                  encoding='utf-8', $                  header=None, $                  names=['airline', 'date', 'tweet'])
print("Mean squared error: %.2f" $       % mean_squared_error(y_test, y_pred))
df['time_open_hours'] = df['time_open'].apply(lambda x: x.total_seconds()) / 60 / 60
props.prop_name.value_counts() $ propnames.value_counts()
df2.user_id.drop_duplicates(inplace = True)
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']] $
obs_diff = (sum((df2.group == 'treatment')&(df2.converted == 1)) / sum(df2.group == 'treatment')) - (sum((df2.group == 'control')&(df2.converted == 1)) / sum(df2.group == 'control')) $ obs_diff
number_of_commits = git_log['timestamp'].count() $ number_of_authors = len(git_log['author'].dropna().unique()) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
def getFinalSymbol(strippedSymbols): $     if len(strippedSymbols) == 0: $         return strippedSymbols $     else: $         return strippedSymbols[0]
dti = pd.date_range('2014-08-29','2014-09-05',freq='B') $ dti.values
mcap_mat.loc['2014':'2018-02', coins_comp].plot(logy=True, color=['darkorange', 'g', 'grey', 'r']) $ plt.ylabel('Market cap in $') $ plt.show()
print(classification_report(y_true=y_train, y_pred=y_t))
temps_df.loc['2018-05-02'].index
cols = ['doggo', 'floofer', 'pupper', 'puppo'] $ archive_df[cols] = archive_df[cols].replace('None', '') $ archive_df['doggolingo'] = archive_df[cols].apply(lambda x: ';'.join(filter(None, x)), axis=1) $ archive_df.drop(cols, axis='columns', inplace=True, errors='ignore')
concat.groupby('store').size()
top_songs[top_songs['Track Name'].isnull()]
pax_raw.paxstep.hist(bins=30)
from sklearn.manifold import LocallyLinearEmbedding $ t0 = time.time() $ X_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X_train) $ t1 = time.time() $ print("LLE took {:.1f}s.".format(t1 - t0)) $
df_oldlen = len(df2.query("group =='control'")) $ df_oldlen
conv_treat_users = df2.query('converted == 1 and group == "treatment"').shape[0] $ treatment_users = df2.query('group == "treatment"').shape[0] $ p3 = conv_treat_users /treatment_users $ print(" Given that an individual was in the treatment group, the probability they converted is {:.4f}".format(p3))
irradiance_clear_df = irradiance_clear_df.loc['2018-07-08 07:00' : '2018-07-08 22:00'] $ irradiance_cloudy_df = irradiance_cloudy_df.loc['2018-07-12 07:00' : '2018-07-12 22:00']
today = dt.date.today() $ print("the type of today is ", type(today))
store.info()
%matplotlib notebook
type(git_log.timestamp[0])
s.value_counts()
import locale $ locale.setlocale(locale.LC_ALL, 'en_US.UTF-8') $ new_users = users[:] $ new_users['CUSTOMER_ID'] = users['CUSTOMER_ID'].apply(locale.atoi) $ new_users.head()
dfs.drop(['Stock First Difference', 'Seasonal Difference'], axis = 1, inplace=True)
prob_old_conv = x_old_conv/x_oldpage $ prob_old_conv $
dates = pd.date_range(date.today(), periods=2) $ dates
df['hr_range']=df.apply(lambda x:hour_range2(x['hr']), axis=1 ) $ df.iloc[1,]
sample_sizes = non_blocking_df_save_or_load( $     raw_sample_sizes, $     "{0}/sample_sizes_10".format(fs_prefix))
noaa_data['AIR_TEMPERATURE'].groupby(noaa_data.index.dayofweek).mean()
tips.sample(5)
) $ df_country_in3
snow.select("select * from st_rvo_me_ref")
print [x for x in X['priority']]
p_diffs = np.asarray(p_diffs) $ (p_diffs > actual_diff).mean() #In how many cases the calculated differences (from the simulations) were higher than the actual differences?
y_test.shape
sentimentos = [1, 0] $ print(metrics.classification_report(classes, classificacoes_treino, sentimentos))
relevant_info_agg_by_gender_and_proj_df.alias("infered").show()
ax = mains.plot() $ co.steady_states['active average'].plot(style='o', ax = ax); $ plt.ylabel("Power (W)") $ plt.xlabel("Time"); $
df1.shape, df2.shape
users = df.nunique()["user_id"] $ print("Number of unique users - {}".format(users))
(taxiData2.Fare_amount <= 0).any() # This Returns False, proving we have successfully changed the values with no negative $
data[['Close', 'Volatility', 'ATR']].plot(subplots=True, color='blue',figsize=(8, 6))
coins_mcap_today = mcap_mat.iloc[-2] $ coins_mcap_today = coins_mcap_today.sort_values(ascending=False)
df_categorical = df_EMR.select_dtypes(exclude=['float64', 'datetime64', 'int64'])
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
users[users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))].shape
conn.execute(addresses.insert(),[ $     {"user-id":1, "email_address":"anonymous.kim@test.com"} $ ])
raw_annotations_df = pd.io.json.json_normalize(latest_df.annotations, record_path='value', meta='classification_id') $ raw_annotations_df.tail(5)
df.groupby(level=0).count()
import pandas as pd $ git_log = pd.read_csv('datasets/git_log.gz',sep='#', header= None,encoding='latin-1',  names=['timestamp', 'author']) $ git_log.head()
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2, columns=['group'])['group_treatment']
from pylab import * $ plot(contour_sakhalin[:,0], contour_sakhalin[:,1]) $ gca().set_aspect('equal') $ title('Sakhalin Island') $ show()
(details.Genres.isnull()).value_counts()
display(Markdown(q4d_answer))
from sklearn.metrics import confusion_matrix
%%bash $ gsutil cat "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" | head
Nold = df2.query('landing_page == "old_page"').user_id.count() $ Nold
dummy_df = helpers.create_rolling_sums(sentiment,[1440,2880,4320])
len(orig_iphone_tweets[orig_iphone_tweets['date'].dt.year == 2016])
sqlc.commit()
df_arch_clean.info()
tweet_clean['retweeted_status'] = tweet_clean.retweeted_status.str.replace('This is a retweet' , '')
plt.figure() $ plt.title("Trip Avg Temp") $ plt.bar(.5, tavg, yerr=(tmax-tmin))
assortativity = nx.attribute_assortativity_coefficient(multiG, 'Block') $ print assortativity
print(tipsDF.groupby('likes').count())
result.summary(title='Model Summary')
df_usa['Area'] = df_usa['Area']*10 $ df_usa
calls_df.loc[(calls_df["call_type"]=="Important"),"phone number"].value_counts()[0:9]
plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue') $ plt.xlabel("Engine size") $ plt.ylabel("Emission") $ plt.show()
index_remove = list(ab_df[mismatch1].index) + list(ab_df[mismatch2].index) $ ab_df2 = ab_df.drop(labels=index_remove,axis=0)
pd.to_datetime('11/12/2010')
pred = gbm_model.predict(test)
SCN_BDAY.head()
results2.summary()
for i in range(-5, 0, 1) : $     data[f'Low {i}d'] = data['Low'].shift(-i) $ data = data.dropna() $ data.head()
tweets['sentiment'] = tweets['text'].apply(lambda t : computeSent(t))
pd.read_csv("Data/microbiome.csv", skiprows=[3,4,6]).head()
11785764 - 11467560
df.shape
final.to_csv('tweets.csv', index=False) $ tweetsPerDay.to_csv('tweetsPerDay.csv', index=False)
dfRegMet2015.shape
import seaborn as sns $ sns.set() $ sns.set_context("talk")
df["2015-06":"2015-08"]['Complaint Type'].value_counts().head(5)
cols_to_export = ["epoch","src","trg","src_str","src_screen_str","trg_str","trg_screen_str"] $ mentions_df.to_csv("/mnt/idms/fberes/network/ausopen18/data/ao18_mentions_with_names.csv",columns=cols_to_export,sep="|",index=False)
ab_data.query('converted == 1').shape[0] / ab_data.shape[0]
fill = A.stack().mean() $ A.add(B, fill_value=fill)
high_rev_acc_opps_net.sort_values(by='Total Buildings', ascending=False, inplace=True)
q_all_count = c.retrieve_query('https://v3.pto.mami-project.eu/query/80a720a924566b9d77e0cab57aac276eefff645fa03af386515ca9814664aa84')
tweet_tokenizer = TweetTokenizer() $ wordnet_lemmatizer = WordNetLemmatizer() $ vectorizer = TfidfVectorizer() $ lsh_forest = LSHForest()
store_items = store_items.append(new_store, sort=True) $ store_items
df_countries=df_new.copy() $ df_countries['UK_ind_ab_page']=df_countries['UK']*df_countries['ab_page'] $ df_countries['CA_ind_ab_page']=df_countries['CA']*df_countries['ab_page'] $ df_countries['US_ind_ab_page']=df_countries['US']*df_countries['ab_page'] $ df_countries.head()
act_diff =  (prob_treat-prob_control).mean() $ print(act_diff)
df2_cont = df2.query('group == "control"'); $ df2_cont['converted'].mean()
data_predict = data_test.iloc[:,:5] $ data_predict[freq_avg] = data[freq_avg] * std + mu $ data_predict['predict'] = predict $ data_predict['variance'] = predict - data_predict['sales'] $ data_predict.head()
submit['proba'][15560:] = 0
df.isnull().sum()
vol = df['Close'] $ vol.head()
os.chdir(Base_Directory) $ os.chdir(today) $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $
df_test = pd.read_csv('../Data/sector_joined_closes_test.csv', index_col='Date_Time') $ display(DataFrameSummary(df_test).summary())
telemetry_feat = pd.concat([telemetry_mean_3h, $                             telemetry_sd_3h.ix[:, 2:6], $                             telemetry_mean_24h.ix[:, 2:6], $                             telemetry_sd_24h.ix[:, 2:6]], axis=1).dropna() $ telemetry_feat.describe()
heights = [59.0, 65.2, 62.9, 65.4, 63.7, 65.7, 64.1] $ data = {'height':heights,'sex': 'M'} $ results = pd.DataFrame(data) $ print(results)
uber1 = uber[0:99] $ uber2 = uber[100:199] $ uber3 = uber[200:] $ print(uber1.head()) $ print(uber3.tail())
from sklearn.model_selection import train_test_split $ X = df.drop('bound_at', axis=1) $ y = df['bound_at'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) $ df.isnull().any()[df.isnull().any() == True].index # should not show any values
hourly_df['Price_Change'].value_counts()
df['encrypted_customer_id'].nunique()
new_page_converted = np.random.choice([0,1], size=n_new, p=[1-p_new, p_new])
den.tail()
print("The minimum donation amount is: $", donations['Donation Amount'].min()) $ print("The maximum donation amount is: $", donations['Donation Amount'].max()) $ print("The average donation amount is: $", donations['Donation Amount'].mean()) $ df_mode = donations['Donation Amount'].mode() #note the mode outputs a dataframe where each row would identify the most common element in the case of ties. $ print("The most common donation amount is: $", df_mode.iloc[0]) $
import pandas as pd $ import numpy as np $ import datetime
data.columns
pd.DataFrame(features['MEAN(loans.loan_amount)'].head(10))
df_world_map = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) $
step_counts = pd.Series(step_data, name='steps') $ print(step_counts)
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Base.classes.keys() # Retrieve the names of the tables in the database
ftrs = vectorizer.get_feature_names()
aaplA01 = aapl['2012-01'][['Adj Close']] $ withDups = pd.concat([msftA01[:3], aaplA01[:3]]) $ withDups
df_t_best['Shipped At'] = df_t_best['Shipped At'].apply(lambda x: x.date()) $ df_best_chart = pd.DataFrame(df_t_best[df_t_best['Place Name'].isin(['Leeuwarden','Valkenswaard','Rokkeveen-West'])] $                     .groupby(['Place Name','State','Latitude','Longitude','Shipped At'])['Updated Shipped diff_normalized'] $              .mean()).sort_values(by='Place Name',ascending=True)
df.groupby('raw_character_text')['episode_id'].nunique().reset_index().head()
print(gMapAddrDat)
tweets_raw.head(5)
analyzer = SentimentIntensityAnalyzer()
knn.fit(train[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2']], train['floor'])
start = datetime(2011,1,1) $ end = datetime(2012,1,1)
n_new=df2.query('landing_page=="new_page"').shape[0] $ n_new
df = table[0] $ df.columns = ["Parameters", "Values"] $ df.head()
fuel_therm_abs_rate = openmc.Tally(name='fuel therm. abs. rate') $ fuel_therm_abs_rate.scores = ['absorption'] $ fuel_therm_abs_rate.filters = [openmc.EnergyFilter([0., 0.625]), $                                openmc.CellFilter([fuel_cell])] $ tallies_file.append(fuel_therm_abs_rate)
QTU_pipeline
del train['month_created'], train['year_created']
df = pd.merge(train.to_frame(), ccl["rise_in_next_week"].to_frame(), left_index=True, right_index=True)
properati[properati['zone'] == ""]['place_name'].value_counts(dropna=False)
All_tweet_data=pd.merge(twitter_data_v2, tweet_data_v2, on='tweet_id') $ All_tweet_data.shape
SCN_BDAY.head()
flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')). \ $     agg(F.mean('price_will_drop_num')).orderBy(['trip', 'stay_days', 'lead_time'], ascending=[1, 1, 0]).show(1000) $
autos[autos.registration_year < 1920]
customer_with_purchases_3ormore = df_r1["Counter"].isin(["3"]) $ df_r2 = df_r1.loc[customer_with_purchases_3ormore,:]
arr1d = np.linspace(start=0, stop=100, num=10, dtype=np.int8) $ arr1d
for c in ['date_listed', 'last_review', 'first_review']: $     airbnb_df[c] = pd.to_datetime(airbnb_df[c]) $
runner_test = Runner(agent=agent, environment=environment_test)
table1.head(3)
df.quantile([.01, .05, .1, .25, .5, .75, .9, .95, .99])
a.all
df
final_tweets = tweet_cleaner(tweet_text)
gscv.best_estimator_.fit(x_train, y_train)
autos['brand'].value_counts(normalize=True) $ brand_unique = autos['brand'].unique()[:6] $
hs_path = save_filepath+'/a0105d479c334764ba84633c5b9c1c01/a0105d479c334764ba84633c5b9c1c01/data/contents'
include_stations = [] $ include_years = [] $ w.set_data_filter(step=0, filter_type='include_list', filter_name='STATN', data=include_stations) $ w.set_data_filter(step=0, filter_type='include_list', filter_name='MYEAR', data=include_years) 
grades.mean()
print("Number of tweets extracted from BBC: ",len(bbc))
p_housing=portland_census2.drop(portland_census2.index[:26]) $ p_housing.head(6)
rankImportance= importanceDF.sort_values(by='importance', ascending= False)
iplot(data.groupby(data.created_at.dt.year).size().iplot(asFigure=True, dimensions=(750, 500), kind="bar"))
weather_df.groupby(["weather_main", "weather_description"]).sum()
hashed_modeling2.unpersist()
corn_vege = ex5.groupby(["corn", "vegetable"])
regr2.fit(X2,y)
my_gempro.get_scratch_predictions(path_to_scratch='scratch', $                                   results_dir=my_gempro.data_dir, $                                   num_cores=4)
vars2 = [x for x in dfa.ix[:,6:54]] $ vars2
Pop_df.columns = ['Date', 'Population']
query = 'SELECT count(*) FROM nodes' $ c.execute(query) $ results = c.fetchall() $ print results[0][0]
pred_probas_over_fm = gs_from_model.predict_proba(X_test) $ fm_bet_over = [x[1] > .6 for x in pred_probas_over_fm]
submission.head()
print('goodreads_users_df: ', len(goodreads_users_df))
a = pd.read_sql_query(q, conn) $ a $
pd.Series(topic_preds).value_counts(normalize=True).plot.barh() $ plt.title('Topic distribution in Queensland floods', fontsize=20) $ plt.xlabel('Proportion') $ plt.ylabel('Topic') $ plt.show()
print("\nThe types of weather events in the 'events' column are:") $ evnts = [str(x) for x in df_weather.events.unique()] $ print("".join([str(i+1) + ". " + evnts[i] + "\n" for i in range(len(evnts))]))
from sklearn.naive_bayes import GaussianNB $ nbc = GaussianNB() $ nbc = nbc.fit(d_train_sc, l_train) $ score = 100.0 * nbc.score(d_test_sc, l_test) $ print(f"Gaussian Naive Bayes accuracy = {score:5.1f}%")
merged2.index
old_page_converted = np.random.binomial(1,Pold,Nold) $ old_page_converted
del df['From Country']
top.groupby('breed')['rating'].describe()
temp_df.tail(-100)
print('The median trading volume during 2017 is : ',func_median(daily_trade_volume))
m.plot_components(forecast); $ plt.title(type_of_compaint) $ plt.savefig(type_of_compaint+'_season'+'.png')
barcelona.columns
sb.heatmap(components1)
all_311_requests = pd.concat([fifteen, sixteen, seventeen, eighteen], axis=0).sort_index()
sales_data = excel_data $ returns_data = csv_data
treatment_df = df2.query('group == "treatment"') $ p_treatment = treatment_df.query('converted == 1').user_id.nunique()/treatment_df.user_id.nunique() $ print('Probability of conversion for the treatment group is {}.'.format(round(p_treatment,4)))
pca.explained_variance_ratio_
seats_per_hour.fillna(0, inplace=True)
for item in bottom_three: $     print('{} has a std of {}.'.format(item[0], item[1]))
with tf.Session() as session: $     result = session.run(slice,feed_dict={my_image: input_image}) $     print(result.shape)
TrainData_ForLogistic.shape
eve_treat =df_eve.query("ab_page==1").converted.mean() $ eve_treat
median_15 = ntrain.loc[ntrain['date'] >= '2016-06-01'].groupby(['Page']).median().reset_index() $ median_15.rename(columns={"Visits": "pred_Visits"}, inplace=True)
round((timelog.seconds.sum() / 60 / 60), 1)
for i, word in enumerate(results): $     print([int_to_char[n] for n in word])
df['Datetime'].max()
gs.score(X_test_total, y_test)
df0901['speed_kmh'] = df0901['dist_km']/(df0901['bike_time']/60)
dataset_ja.set_index(['customer_id']) $ dataset_ja.loc[dataset_ja['customer_id'] == "000011265b8a3727c4cc77b494134aca"]
df_new.head() $
data_countries = pd.read_csv("./data/countries.csv") $ data_age_gender_bkts = pd.read_csv("./data/age_gender_bkts.csv")
df.query("landing_page == 'new_page'").count()['landing_page']/df['landing_page'].count()
msft['2012-01-01':'2012-01-05']
for i in image.list(workspace = ws,tag = TICKER): $     print('{} {}(v.{} [{}]) stored at {} with build log {}'.format(i.id, i.name, i.version, i.creation_state, i.image_location, i.image_build_log_uri))
tsla.head()
tweet1.user == tweet1.author #Maybe if this were a retweet, this would be false?
%%bash $ DIR=data $ if [ ! -d "$DIR" ] ; then $     mkdir "$DIR" $ fi
keras_entity_recognizer.get_step_params_by_name("learner")
for c in text: $     if ord(c) > 127: # Skips single characters $         print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))
tlen = pd.Series(data=data['len'].values, index=data['Date'])
scores.shape
testObjDocs.buildOutDF(tst_lat_lon_df)
model = RandomForestClassifier(max_depth = 5, n_estimators = 10, min_samples_leaf = 5, random_state=8)
train_df.describe().T
groceries['apples'] = 6
train_set['set'] = 'train'
spacy_url = 'https://spacy.io/assets/img/pipeline.svg' $ iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url) $ HTML(iframe)
vote_sort = to_plot_df.sort_values(by=['vote_count'], ascending=False) $ pt = vote_sort['vote_count'].plot(kind='bar', figsize=(20,10), rot=0, fontsize=20) $ plt.xlabel(pt.get_xlabel(), fontsize=22) $ plt.ylabel('Average of vote_count', fontsize=22) $ plt.title('Average of vote_count by original_language', fontsize=30)
reddit.head()
stamp=datetime(2020,10,1) $ stamp.strftime('%Y-%m-%d')
by_periods = msft_cum_ret .resample("M", kind='period').mean()
autos['brand'].value_counts(normalize=True)
not_linedup = df[(((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False)] $ len(not_linedup)
df_new.shape $ df_new.sample(3)
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func
print(real_diff) $ plt.hist(p_diffs) $ plt.axvline(x = real_diff,color = 'red');
ss = StandardScaler() $ X = ss.fit_transform(X) $ type(X) $ X
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_axial_chord_crown[1][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_crown[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
sqlContext.sql("select * from df_example3").toPandas()
sheets.keys()
no_of_samples_control
Lyr=session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ Lstyr=str(Lyr)[2:-3] $ Lstyr $ engine.execute("SELECT * FROM measurement WHERE DATE >'2016-08-22' ").fetchall()
y.mean()
df2['intercept']=1 $ df2[['control','treatment']]  = pd.get_dummies(df2['group']) $ df2.rename(columns={'treatment':'ab_page'},inplace=True) $ df2.head()
data2['weeks'] = list(range(len(data2.index)))
p_range = max(v.high-v.low for v in trade_data_dict.values()) $ p_range_date = [v for v in trade_data_dict.values() if v.high-v.low == p_range][0][0] $ print('The largest trading range in 2017 was: {:.2f} on {}'.format(p_range, p_range_date))
HC = [] $ for tweet in tw: $     HC.append(parse_tweets(tweet)) $
r =requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=&start_date=2010-01-01&end_date=2010-01-01')
days = ['day1', 'day2'] $ nocol=pd.DataFrame(index=days) $ nocol
temp_series_freq_15min = temp_series.resample("15Min").interpolate(method="cubic") $ temp_series_freq_15min.head(n=10)
train.info()
regr.fit(X,y)
df_archive["name"].value_counts()
with open('C:/Users/willk/OneDrive/Desktop/slack_api_python.txt', 'r') as f: $     slack_api_token = f.read()
df2[(df2['group'] == 'control')].shape
reddit_comments_data.groupBy('author').agg({'sentiment':'mean'}).orderBy('avg(sentiment)', ascending = False).show()
to_be_predicted_Day1 = 26.67 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
m3 = np.round(m3, decimals = 2)  #rounding the decimal upto two points $ print("m3: ", m3)
engine = create_engine("sqlite:///hawaii.sqlite", echo = False)
mars_facts=df.to_dict('records') $ mars_facts
melted = pd.melt(s4p, id_vars=['Date','Symbol']) $ melted[:5]
df_new['timestamp'] = pd.to_datetime(df_new['timestamp']) $ df_new['day_of_week'] = df_new['timestamp'].dt.weekday_name $ df_new.head(10)
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1 - p_new), p_new]) $ new_page_converted.mean()
importlib.reload(util)
non_na_df.groupby('hour').count()['sender_qq'].plot(kind='bar', figsize=(18,6)) $ plt.tight_layout() $ plt.title('Activity by aHour') $ plt.ylabel('Total Chat Count') $ plt.xlabel('Hour (24h)')
learn.freeze_to(-1)
df_regression = df2.copy()
popCon = pd.DataFrame(likes.groupby(by=['contact','content']).size()) $ popCon.columns = ['counts'] $ popCon = popCon.reset_index() $ popCon.sort_values(by='counts', ascending=False).head(10)
autos[['date_crawled', 'ad_created', 'last_seen']].head()
all_data = pd.concat(df)
pwd
!wget -nv https://www.py4e.com/code3/mbox.txt -O mbox.txt
X = reddit_master[['age', 'subreddit']].copy(deep=True) $ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
yc_new4 = yc_new3[yc_new3.tipPC > 1]
y=train.readingScore $ X=train[features_to_use] $ X=sm.add_constant(X) $ lmscore=sm.OLS(y,X) $ lmscore=lmscore.fit()
cust_group_date = df.groupby(['cust_id','order_date'])['cust_id'].count().reset_index(name="count") $ cust_group_date $
for res_key, df in data_sets.items(): $     df.to_pickle('patched_'+res_key+'.pickle')
pca = PCA(n_components=9) $ x_9d = pca.fit_transform(X_std)
df.iloc[[11,24,37]]
to_be_predicted_Day3 = 55.25522979 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
plt.plot(dataframe.groupby('year').daily_worker_count.mean()) $ plt.show()
(~autos["registration_year"].between(1900,2016)).sum()/autos.shape[0]
handler.to_dataframe().head()
dfNYC["Year"] = dfNYC["Created Date"].apply(YearColumn)
datetime.now().toordinal() - datetime(1987, 1, 7).toordinal()
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf]).sample(10)
typesub2017 = typesub2017.rename(index=str, columns={"Solar  - Actual Aggregated [MW]": "Solar", "Wind Offshore  - Actual Aggregated [MW]": "Wind Offshore", "Wind Onshore  - Actual Aggregated [MW]" : "Wind Onshore" }) $ typesub2017.head()
surprise_slice.iloc[0]
df['intercept'] = 1 $ logit2 = sm.Logit(df_new['converted'], df_new[['UK', 'US', 'intercept']]) $ result2 = logit2.fit() $ result2.summary2()
for key, value in sorted(mean_price_by_brand.items(), $                          key=lambda kv: (kv[1],kv[0]), reverse=True): $     print("{}: {}".format(key,value))
prob_treatment = prob.mean()['converted']['treatment'] $ print("The probability of an individual converted, being in the 'treatment' group is - {}".format(prob_treatment))
summer = df[df.index.month.isin([6, 7, 8])]
train['date'] = pd.to_datetime(train['date']) $ test['date'] = pd.to_datetime(test['date'])
df[ $     (df.u_state != "USA") $   & (~df.u_state.isnull()) $   & (df.t_n_urls > 0) $ ].assign(state_color=lambda x: x.u_state.map(STATE_COLORS)).groupby("state_color").t_n_urls.count()
df2.converted.value_counts(normalize=True)[1]
es_dataindex = "{0}_harmonized".format(city.lower()) $ es_datatype = 'incidents' $ es.createOrReplaceIndex(es_dataindex) $ es.addTypeMapping(es_dataindex, es_datatype, mapping)
arma_mod40 = sm.tsa.ARMA(dta_713, (4,0)).fit(disp=False) $ print(arma_mod40.params)
cityID = '6a0a3474d8c5113c' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         El_Paso.append(tweet) 
utility_patents_subset_df['prosecution_period'] = utility_patents_subset_df.grant_date - utility_patents_subset_df.filing_date $ utility_patents_subset_df.prosecution_period = utility_patents_subset_df.prosecution_period.apply(lambda x: x.days) $ utility_patents_subset_df.prosecution_period.describe()
retweets
from h2o.estimators.glm import H2OGeneralizedLinearEstimator $ glm_model = H2OGeneralizedLinearEstimator(model_id = "GLM", family = "binomial") $ glm_model.train(x = predictor_columns, y = target, training_frame = train, validation_frame = valid)
medals = pd.read_table('../data/olympics.1996.txt', sep='\t', $                        index_col=0, $                        header=None, names=['country', 'medals', 'population']) $ medals.head()
CryptoComm['TextLen'] = CryptoComm['CommentText'].apply(len) $ CryptoComm['TimeDiff'] = (CryptoComm.CommentTime - CryptoComm.PostTime).total_seconds()
list_congress = ['113', '114', '115'] $ members_df_recent = members_df[(members_df['members_congress'].isin(list_congress))] $ members_df_current['members_yrs_until_nxt_election'] = members_df_current['next_election'].map(int) - 2018 $ members_df_all = members_df_recent.merge(members_df_current, left_on = 'members_member_id', right_on = 'id', how = 'left')
leadConvpct = leadConvpct.filter(like='convertedLead').rename('leadConversionPercent').to_frame().reset_index()
df_sched.head()
tickerstart = time.time() $ mydata = quandl.get(companies, start_date="2012-01-01", end_date="2018-06-17") $ tickerend = time.time() $ print(tickerend-tickerstart)
time2close_2013 = merged_tickets_2013["time2close"]
df2.query('group == "treatment"').converted.sum()/df2.query('group == "treatment"').user_id.nunique()
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s') $ git_log['timestamp'].describe()
autos.describe(include="all")
twitter_archive_master.to_csv('twitter_archive_master.csv', index=False)
par.loc[wpars,"parubnd"] = 1.1 $ par.loc[wpars,"parlbnd"] = 0.9 $ pst.parameter_data
results.summary()
y_class = demo.get_class(y_pred) $ cm(y_test,y_class,['0','1'])
mostRT = output['text'].iloc[0] $ pd.set_option('display.max_colwidth',-1) $ mostRT
df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('MA/Manchester', 'NH') $ df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('DC/Hagerstown', 'VA') $ df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('Ohio', 'OH') $ df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('Iowa', 'IA') $ df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('CA', 'National')
unique_buying_browsers = transactions.browser_id.unique()
tmp = [i.lower() for i in df_repub_2016['text']]
offset3 = timedelta(days=368, seconds=2) $ df + offset3
df = pd.DataFrame() $ df['date'] = day_list $ df['score'] = score_day_list $ df.to_csv('score_day.csv')
history_data = prepare_data(intervention_history_df).drop(drop_columns, axis=1)
log_mod=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results=log_mod.fit() $ results.summary2()
to_be_predicted_Day4 = 21.38790489 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
students.loc['Ryan']['weight']
reviews.loc[reviews.price.isnull()]
sns.swarmplot(x="embark_town", y="age", hue="sex", data=titanic)
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
discovery = qualification[qualification['qual_conversion'] == 'convertedQual']
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $ )
maxi = stock.cummax()
word_count = df.groupby(['subreddit'])['word_count'].sum()
p_null=pnew_null
run txt2pdf.py -o"2018-06-14 2148 CLEVELAND CLINIC Sorted by Payments.pdf"  "2018-06-14 2148 CLEVELAND CLINIC Sorted by Payments.txt"
autos = autos.drop(["nr_of_pictures", "seller", "offer_type"], axis=1)
exceldf = pd.read_excel('http://www.fsa.usda.gov/Internet/' + $                         'FSA_File/disaster_cty_list_ytd_14.xls')
fraud_data_updated["class"].value_counts()
from gensim.models import Word2Vec $ model = Word2Vec.load("300features_40count_10context") $ type(model.wv.syn0)
df1 = make_df('AB', [1, 2]) $ df2 = make_df('AB', [3, 4]) $ display('df1', 'df2', 'pd.concat([df1, df2])')
gdp.tail()
Qbar = 50 $ deltaQ = A = 10
conn.execute(sql)
df_protest = df_protest.dropna(subset=["Rural"])
lr_cv = GridSearchCV(LogisticRegression(), lr_parameters, cv=5, scoring='neg_log_loss')
events['payload'] = events['payload'].str.encode('ascii', 'ignore') $ events['payload'] = events['payload'].apply(json.loads) $ events[['repo_id', 'user_id', 'archive_id']] = events[['repo_id', 'user_id', 'archive_id']].applymap(pd.to_numeric) $ events['created_at'] = events['created_at'].apply(pd.to_datetime) $ events.shape
pres_df['subject_count_tmp'].mean(), pres_df['subject_count_tmp'].median()
sns.set() $ sns.distplot(df_input_clean.toPandas().Resp_time, kde=True, color='b')
df.drop(["join_mode"], axis = 1, inplace = True)
a.shape[1]
n_old=df2.query("landing_page=='old_page'").user_id.count() $ n_old
news9= ('Iraq backed a proposal from Saudi Arabia and Russia to extend output cuts for nine months, removing one of the last remaining obstacles to an agreement at the OPEC meeting in Vienna this week.' $ 'Iraq has the worst record of compliance with its pledged cuts, pumping about 80,000 more barrels of oil a day than permitted during the first quarter. If that deal gets extended to 2018, the nation will have even less incentive to comply because capacity at key southern fields is expanding and three years of fighting Islamic State has left it drowning in debt.' $
oil_interpolation['date']=oil_interpolation.index $ pd.DataFrame.head(oil_interpolation) $ oil_interpolation.plot('date','dcoilwtico',kind='bar', color='r', figsize=(15,5))
print fb_desc
daily_ret_mean = daily_ret.mean() $ daily_ret_mean
test_corpus = [" ".join([t for t in reuters.words(test_doc[t])]) $                for t in range(len(test_doc))] $ print("test_corpus is created, the first line is: {} ...".format(test_corpus[0][:100]))
top_songs.shape
np.median(df2.days_active)
((~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0])*100
pd.set_option('display.max_colwidth', -1) $ df[['text', 'favorite_count', 'date']][df.favorite_count == np.max(df.favorite_count)]
grouped = word_freq_df.groupby('Word_stem', as_index=False).sum()
ek.get_news_headlines('R:TD.TO', date_from='2017-10-19T09:00:00', date_to='2017-10-20T18:00:00')
lr = LogisticRegression()
data.groupby(['Date received', 'State'])['Company'].agg([ 'count']).pivot_table('count', index = 'Date received', columns='State', fill_value=0).sum().sort_values(ascending=False)
import numpy as np $ data = np.random.normal(0.0, 1.0, 1000000) $ np.testing.assert_almost_equal(np.mean(data), 0.0, decimal = 2)
pd.DataFrame(d, index=['apple', 'clock'])
Base.classes.keys() $
frtcor = pearsonr(df[df['RT']==False]['fcount'],df[df['RT']==False]['rtcount']) $ print('Correlation between favourite and retweet counts is',round(frtcor[0],2),get_sigstar(frtcor[1])) $ df[df['RT']==False][['op','text','time','fcount']].sort_values(by='fcount',ascending=False).head()
autos['price'] = autos['price'].str.replace("$","").str.replace(",","").astype(int) $ autos['price'].head(10)
df.head()
print 'Create a DatetimeIndex even from gratuitously mixing date formats.' $ print 'An invalid type is convereted to NaT, which means Not A Time' $ dti = pd.to_datetime(['Aug 1, 2014', '2014-08-02', '2014.8.3', None]) $ dti
df[df['Agency Name'] == 'New York City Police Department']['Complaint Type'].value_counts().head().plot(kind='bar') $ df[df['Agency Name'] == 'Department of Transportation']['Complaint Type'].value_counts().head().plot(kind='bar')
new_page_converted = np.random.binomial(1, conv_mean, 145310)
directory = 'C://Users//Joan//OneDrive//capstone//' $ dta = pd.read_csv(directory + 't_asv.csv')
results.reindex(results.index[::-1]).head()
np.sum(df["same_location"] == 0)
df['PCT_change'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.0 $
seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
s.resample('Q', closed='left').head()
learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=4) #JH did 15 here. Trying 10. RESULT: 4 is ideal. 8.26 doing 4 
from helpers import *
sqlContext.sql("select count(person) from pcs").show()
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.title('Histogram of 10000 simulations') $ plt.xlabel('p_diffs') $ plt.ylabel('frequency');
campaign_weekday_performance[('Impressions', 0)]
cust_data['No_of_30_Plus_DPD'] = cust_data['No_of_30_59_DPD']+cust_data['No_of_60_89_DPD']+cust_data['No_of_90_DPD'] $ print cust_data1.head(2)
tmp_df.sample(10)
autos.price.describe()
manual_uniprot_dict = {'Rv1755c': 'P9WIA9', 'Rv2321c': 'P71891', 'Rv0619': 'Q79FY3', 'Rv0618': 'Q79FY4', 'Rv2322c': 'P71890'} $ my_gempro.manual_uniprot_mapping(manual_uniprot_dict) $ my_gempro.df_uniprot_metadata.tail(4)
df_subset['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=False, logy=False) $ plt.show()
dt_features_test['launched_at'] = pd.to_datetime(dt_features_test['launched_at'],unit='s')
now = datetime.datetime.now() $ time_stamp =  str(now.strftime("%Y%m%d%H%M%d")) $ csv_name = 'bridge_data_' + time_stamp + '.csv' $ print('csv name: ', csv_name) $
f = open('BKK_th-en_tweets_gamcom.txt','r',encoding='utf-8')
df2.drop(1899, inplace = True)
treatment = df2[df2['group'] == 'treatment'] $ size_treatment = treatment.shape[0] $ prop_conv_treatment = treatment[treatment['converted'] == 1].shape[0] / treatment.shape[0] $ prop_conv_treatment
grads = K.gradients(loss, [w,b]) $ updates = [(w, w-lr*grads[0]), (b, b-lr*grads[1])]
merged_portfolio_sp = pd.merge(merged_portfolio, sp_500_adj_close, left_on='Acquisition Date', right_on='Date') $ merged_portfolio_sp.head()
segmentData.opportunity_size.value_counts()
p_new=df2[df2['converted']==1].shape[0]/df2.shape[0] $ p_new
initial_commit = blame[blame.author == "Linus Torvalds"].timestamp.min() $ initial_commit
word_counts = get_topic_freqs(model_spec, 50)
tmp = tweets.query("snsuserid.isin(@useractivitytopk_tweets.snsuserid.values)") $ text = _get_text(tmp.text.values) $ makeImage(getFrequencyDictForText(text), width=400, height=400, shape=(6,6))
x = pd.read_sql_query(q, conn) $ x $
td
twitter_archive_clean.head(1)
url = "http://oglobo.globo.com" $ html_txt = urllib.request.urlopen(url).read() $ soup = bs(html_txt, "html.parser") $ for line in soup.find_all('a'): $     print(line.get('href'))
support_NNN.amount.sum() / merged_NNN.amount.sum()
df_input_clean = df_input_clean.filter("`Resp_time` > 0") $ df_input_clean.toPandas().info()
network.rebuild() $ sim.run()
teams_sorted_by_wins = df.sort_values(by=['wins', 'teamid'], ascending=False) $ teams_sorted_by_wins.head(10)
X.head()
data['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ]) $ SA1 = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ]) #store data in its own vector $ display(data.head(10))
full = full.replace('@NA',np.NaN) # replace @NA $ full = full.replace('',np.NaN) # didn't find any empty strings $ full = full.replace(np.NaN,'')
p - 3
def mini_concat_udf(array_strs): $
%load_ext watermark $ %watermark $ %watermark -p plotly,tqdm,joblib,pandas,numpy,requests
engine.execute('SELECT * FROM measurement LIMIT 5').fetchall()
predictions.printSchema()
print("Number of Tools in Enterprise ATT&CK") $ print(len(all_enterprise['tools'])) $ df = all_enterprise['tools'] $ df = json_normalize(df) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
pf_values = res.map(lambda r: Row(date=r.date, $                                   neutral=r.neutral*r.qty, $                                   scenarios=DenseVector(r.scenarios.array * r.qty))) $ aaa = pf_values.map(lambda x: (x[0], (x[1],x[2]))).aggregateByKey(0, lambda v, d: d, lambda x,y: (x[0]+y[0], x[1]+y[1])).map(lambda r: Row(date=r[0], neutral=r[1][0], scenarios=r[1][1]))
Base.classes.keys() $
motion_at_home_df = binary_sensors_df[binary_sensors_df['entity_id']=='binary_sensor.motion_at_home'] $ motion_at_home_df = motion_at_home_df[motion_at_home_df['state']==True] # Since on/off are always paired for motion, drop false $ motion_at_home_df.head()
data.groupby('rate_marriage').mean() $
kick_projects['duration']=(kick_projects['deadline_date']-kick_projects['launched_date']).dt.days $ kick_projects['launched_quarter']= kick_projects['launched_date'].dt.quarter $ kick_projects['launched_month']= kick_projects['launched_date'].dt.month $ kick_projects['launched_year']= kick_projects['launched_date'].dt.year
unique_member_ids = votes_by_party['member_id'].unique() $ print(len(unique_member_ids))
df = pd.DataFrame(data, columns=['postdate','author','text','systemmessage']  ) $
pbptweets = pbptweets.drop_duplicates(subset='text', keep='first')
import pandas as pd $ import numpy as np $ from io import StringIO $ import re $ from IPython.display import display,clear_output
models = [train(hits_df.loc[:date]) for date in model_dates]
df.groupby('episode_id')['id'].nunique().mean()
test.shape,train.shape,full_data.shape
grid_id = np.arange(1, 1535,1) $ grid_id_array = np.reshape(grid_id, (26,59))
sum((df2.group == 'treatment')&(df2.converted == 1)) / sum(df2.group == 'treatment')
df_weather.head(5)
ek.set_app_id('D163218EE154B9D1851F8C9')
plat
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.len() < 3) & (All_tweet_data_v2.name.str.contains('^[(a-z)]'))]='None'
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ print(last_year)
df_clean3.nlargest(10, 'rating_numerator')[['text', 'rating_numerator', 'rating_denominator']]
all_test_times_dates = pd.concat([go_no_go_times, simp_rxn_time_times, proc_rxn_time_times, $                    go_no_go_date, simp_rxn_time_date, proc_rxn_times_date], axis=1) $ all_test_times_dates.columns = ['go_no_go_time', 'simp_time', 'proc_time', $                                'go_no_go_date', 'simp_date', 'proc_date']
daily_cases.unstack().T.fillna(0).head()
anomaly_condition2 = (journeys_scored['anomaly_ma'] + journeys_scored['anomaly_lof'] + $                       journeys_scored['anomaly_svm'] + journeys_scored['anomaly_if'] + $                       journeys_scored['anomaly_ma2'] + journeys_scored['anomaly_lof2']) > 2 $ journeys_scored.loc[anomaly_condition2, ['Journey_ID']].Journey_ID.unique()
precip_df.head(10)
X = np.random.rand(5, 10) $ Y = X - X.mean(axis=1, keepdims=True) $ Y = X - X.mean(axis=1).reshape(-1, 1) $ print(Y)
a = set(X_train.columns) $ b = set(X_test.columns) $ print(a.difference(b))
diff1 = treatment_convert - control_convert $ diff1 $ plt.hist(p_diffs) $ plt.axvline(x=diff1,color ='red') $ plt.show
auto_new.CarModel.value_counts()
bb = data.DataReader(name='F', data_source='iex' $                         , start='2017-07-01', end='2018-05-01')
autos[autos.price > 500000].loc[:,["name", "odometer_km","price" ]] $
df_concat = pd.concat([bild, spon]) $
joined.dtypes
stack_with_kfold_cv=train_orig[['air_store_id','visit_date']]
df_piotroski
b_rev.head(2)
archive_clean[archive_clean['retweeted_status_id'].notnull()]
average_reading_score = df_students['reading_score'].mean() $ average_reading_score
week43 = week42.rename(columns={301:'301'}) $ stocks = stocks.rename(columns={'Week 42':'Week 43','294':'301'}) $ week43 = pd.merge(stocks,week43,on=['301','Tickers']) $ week43.drop_duplicates(subset='Link',inplace=True)
r = requests.get('https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html')  
from sklearn.linear_model import LinearRegression $ regression_model = LinearRegression() $ regression_model.fit(X_train, y_train)
pd.Series(zip(labels, y)).value_counts()
cursor = conn.cursor()
xmlData['street']
train_users_pd['language'].unique()
LogisticModel_ZeroFill.fit(X, y)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=31000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113447896253141.json', encoding="utf-8-sig") as data_file: $     json_data1 = j.load(data_file)
data = pd.read_csv('data.csv', $                     parse_dates={'Timestamp': ['Date', 'Time']}, $                     index_col='Timestamp') $ data
import pandas as pd $ import numpy as np $ %matplotlib inline
data.drop('id', axis=1, inplace=True)
data["Improvements_lemma"] = data["Improvements_nltk_token"].apply(lemmatise)
tweets_clean.info()
melted_total.groupby('Categories')['Neighbourhood'].value_counts().ix[top10_categories.index].unstack().plot.bar(legend=True,figsize=(10, 5))
for fld in tqdm(holidays): $     add_elapsed(fld, 'after_')
likes.groupby(['month']).size().plot(kind='bar')
%time lr = lr.fit(train_4, y_train)
TotalNameEvents.head(1)
git_log.timestamp = pd.to_datetime(git_log.timestamp) $ git_log.head()
import pandas as pd $ import numpy as np $ import seaborn as sns $ from cfatools import *
pr1=len(df2.query("group=='treatment' & converted == 1")) $ prob1 = pr1/length $ prob1
df2.query('group=="treatment"').converted.sum()/df2.query('group=="treatment"').count()[0]
from sklearn.linear_model import LogisticRegression $ log_reg = LogisticRegression(random_state=42) $ log_reg.fit(X_train, y_train)
research_df = dic['research'] $ oilgas_df = dic['oil & gas'] $ alternative_df = dic['alternative energy'] $
search = api.GetSearch('#Redsox', count = 1) $ for item in search: $     print(item) $
other_list = list(org_counts[org_counts >= 5].index) $ other_list.remove('unlisted') $ other_list.remove('unknown')
print ('The mean of test_accuracy of the model with the default hyperparameter:') $ scores['test_accuracy'].mean()
df_temp_1 = df.query('landing_page =="new_page" and  group=="treatment"') $ df_temp_2 = df.query('landing_page =="old_page" and  group=="control"') $ df2= df_temp_1.append(df_temp_2)
y_true = dataset_test["highest_reaction"] $ y_pred = clf.predict(preprocess(dataset_test["name"]))
session.query(measurement.date).\ $     filter(measurement.station == 'USC00519281').\ $     order_by(measurement.date.desc()).first()
df.info() $ df.isnull().sum() $
df = df_data.groupby('DATA')['COM_LUZ_SOLAR','SEM_LUZ_SOLAR'].sum() $ df.head() $ df.reset_index()
s519397_df = s519397_df.set_index('date') $ print(len(s519397_df.index)) $ s519397_df.info() $ s519397_df.head(5)
df.plot(kind='scatter', x='B', y='C', title = 'Scatterplot')
ABT_tip.to_csv('text_preparation/abt_text_analysis_with_tips.csv')
from sklearn.svm import SVC
p_diffs = [] $ for _ in range(10000): $     new_page_converted_samples = np.random.choice([1,0], size=n_size, p=[p_new, (1 - p_new)]) $     old_page_converted_samples = np.random.choice([1,0], size=o_size, p=[p_old, (1 - p_old)]) $     p_diffs.append(new_page_converted_samples.mean() - old_page_converted_samples.mean())
timezone_df['gmt_hour'] = timezone_df['gmt_offset'].apply(lambda x:x/3600)
from pyspark.ml.clustering import KMeans $ kmeans = KMeans(featuresCol='features', predictionCol='prediction',k=4) $ pipeline = Pipeline(stages=[assembler, kmeans]) $ PipelineModel = pipeline.fit(data) $ predictions = PipelineModel.transform(data)
N_new = df2.query("landing_page == 'new_page'")["user_id"].count() $ print("The dataset consists of {} new pages.".format(N_new))
autos["registration_year"].value_counts(normalize=True)
df_new.head()
year_ago = dt.date.today() - dt.timedelta(days=730) $
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=870) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
blocks= pd.read_csv("../Data/blocks_clean.csv", usecols=range(1,23)) $ blocks.head()
print(stock.head()) $ print(mini.head())
ttt = np.argmax(out_df[['low','medium','high']].as_matrix(),axis=1)
twitter_filename = "2017-09-23:11:05:01.json" $ twitter_file = open(twitter_filename, 'r')
data.columns = data.columns.str.replace(' ', '_') $ data.columns[data.columns != data.columns.str.extract(r'^(\w+)$')] 
list(festivals.columns.values)
df2[df2['user_id'].duplicated(keep=False)]['user_id'].values[0]
dt_features_test['created_at'] = pd.to_datetime(dt_features_test['created_at'],unit='s')
prop.info()
words_only_sk_freq = FreqDist(words_only_sk) $ print('The 100 most frequent terms (terms only): ', words_only_sk_freq.most_common(30))
data = data.loc[data['tmin'] <= 15]
autos['odometer_km'].describe(percentiles=[.10, .90])
df_ids
all_turnstiles.head()
dummy_var_df.shape
n_old = df2_control.shape[0] $ n_old
mask_with_labels = tf.placeholder_with_default(False, shape=(), $                                                name="mask_with_labels")
unique_users = df.user_id.nunique() $ print('The number of unique users in the dataset is {}'.format(unique_users))
df = df.join(cluster.cluster_labels)
import tweepy $ import boto3 $ import yaml $ import os
count_authors_with_given_numer_publications = data_final.groupby('countPublications', as_index=False)['authorId'].count() $ count_authors_with_given_numer_publications.columns = ['number_publications', 'how_many_authors_with_given_publications'] $ count_authors_with_given_numer_publications.head(20)
df2[df2['user_id'] == 773192]
stations_count = session.query(Measurement).group_by(Measurement.station).count() $ stations_count
r.json()['dataset_data']
filenames = os.listdir(path) $ filenames
countries_df = pd.read_csv('./data/countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ print(df_new['country'].unique()) $ df_new.head()
articles['tokens'] = articles['tokens'].map(lambda s: [w for w in s if len(w)>1])
spks = np.loadtxt('output/spikes.csv') $ print(spks[1:10, :])
to_remove = ["Current", "In Grace Period", "Late (16-30 days)", "Late (31-120 days)"] $ loan_stats = loan_stats[loan_stats["loan_status"].isin(to_remove).logical_negation(), :]
x_scaled.shape
df2.drop(df2[df2.duplicated(['user_id'],keep=False)].index[0],inplace=True)
smpl_join_r = smpl_join.select(*(F.col(x).alias(x + '_orig') for x in smpl_join.columns)) $ join_a = smpl_join_r.join(contest_savm,F.col('postal_code_orig')==F.col('postal_code')).where(F.col('decision_date_time_orig')<=F.col('start_date'))
from sklearn.feature_selection import SelectFromModel $ model = SelectFromModel(lm,prefit = True) $ lm_new = model.transform(x_test) $ lm.score(x_test,y_test)
df.to_pickle('all-RSS.pkl')
NYG_analysis = team_analysis.get_group("NYG").groupby("Category")
(pd.DataFrame(click_condition_meta.groupby('geo_country').count())).sort_values('action', ascending = False).head(4)
df2.query("group == 'control'")['converted'].mean()
val_df.head(1)
to_be_predicted_Day2 = 22.32749815 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
tweet_archive_enhanced_clean['in_reply_to_status_id'].value_counts()
df_vec.columns
merged = merged.drop(['comment_parent_post_id', 'comment_edited'], axis='columns')
iris.head()
import statsmodels.api as sm $ Log_model = sm.Logit(df2['converted'], df2[['intercept','ab_page']])
relevant_data['User Name'].value_counts().plot(kind='barh')
df3 = pd.read_csv('2003.csv')
def get_coin_list(): $
Y_df1 = Y_df.copy() $ Y_df1['interest'] = [0 if x=='low' else(1 if x=='medium' else(2)) for x in Y_df.interest_level] $ Y_df1.head()
session.query(Measurement.date).\ $ filter(Measurement.station == 'USC00519281').\ $ order_by(Measurement.date.desc()).first()
svm_tunned.fit(X_train, y_train)
ps.to_timestamp('D', how='start')
na_df = df = pd.DataFrame(np.arange(15).reshape(5, 3), index=['a', 'c', 'e', 'f', 'h'], $                           columns=['one', 'two', 'three']) $ na_df = na_df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']) $ na_df
pd.read_csv(r'C:\Users\Patrik\Downloads\webrobots.iokickstarter-datasets\Kickstarter_2017-10-15T10_20_38_271Z\Kickstarter016.csv').info()
events.loc['2017-01-10':'2017-01-11'].head(4)
df_R['Month'] = df_R['Date'].apply(lambda s:s.split('-')[1])
s1.index
X = X[feature_names] $ X = X.apply(pd.to_numeric)
berlin.info()
df2['intercept'] =1
df2.query('landing_page == "new_page"').count().user_id
ftfy.ftfy(fb_cleaned)
X_know.shape
print("P-I-New_Page:", $       df2['landing_page'].value_counts()[0]/len(df2))
fashion = pd.read_pickle('../data/fashion.pkl')
df2['intercept'] = 1 $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit() $ results.summary()
df2 = pd.read_csv('countries.csv') $ df2.head()
output= "SELECT * from user where user_id='@Pratik'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Followers','Following','TweetCount'])
omdb_df
set(y_valid)-set(bm1), set(y_valid)-set(bm2)
not_in_dsi.head(3)
df.loc['b']
sample_single_treat = treatment_group.sample(size_treatment, replace = True) $ new_page_converted = (sample_single_treat['converted'] == 1).sum() / sample_single_treat.user_id.count() $ new_page_converted
writer.save()
dataset=pd.read_csv(train_path) $
dtmodel.fit(X_train, y_train) $ dtmodel_predictions = dtmodel.predict(X_test) $ num_of_dtmodel_pred = collections.Counter(dtmodel_predictions) $ num_of_dtmodel_pred
test_embedding=test_embedding.rename({'DETAILS3':"WordVec"}, axis=1)
small_frame = loan_stats[1:3,['loan_amnt','installment']] $ single_col_frame = loan_stats[1:3,'grade'] $ small_frame.cbind(single_col_frame) 
es = Elasticsearch('elasticsearch:9200') $ if es.indices.exists('stream-test'): $     es.indices.delete('stream-test') $     es.indices.create('stream-test')
def cleaning_function(row_data): $     return ... $ df.apply(cleaning_function, axis = 1) $ assert (df.column_data > 0).all()
print('n_old is:', n_old)
df.loc['a':'d']
stock.news_sources = stock.news_sources.fillna('') $ stock.news_text = stock.news_text.fillna('') $ stock.tesla_tweet = stock.tesla_tweet.fillna('') $ stock.elon_tweet = stock.elon_tweet.fillna('')
%load "solutions/sol_2_43.py"
news_dict_df.to_csv("Newschannel_tweets_df.csv")
tweet = result[0] $ for param in dir(tweet): $     if not param.startswith("_"): $         print ("%s : %s\n" % (param, eval('tweet.'+param)))
df['marks']= zero_labels $ df.tail()
bruins = bruins[['game_id','season','date','day_of_week','time','hour','minute','late','opponent','playoff']]
token_lemma = [token.lemma_ for token in parsed_review] $ token_shape = [token.shape_ for token in parsed_review] $ pd.DataFrame(zip(token_text, token_lemma, token_shape), $              columns=['token_text', 'token_lemma', 'token_shape'])
a.shape #Number of rows and columns.
backup = clean_rates.copy() $ clean_rates.text = clean_rates.text.str.replace('\n', '')
num_comps = 2 $
bow_converter = CountVectorizer(token_pattern='(?u)\\b\\w+\\b') $ x = bow_converter.fit_transform(review_df['text']) $ x
obs_diff = treatment_mean - control_mean $ obs_diff
print ("There {} rows in page views and {} rows in the visitors, using unique visitors by day".format(len(pageviews),len(visitors))) $ print ("Therefore, we've got {} pageviews by visitor".format(round(len(pageviews)/len(visitors),2))) $ print ("The number of visitors above should be {} but there is something wrong in the data ".format(len(pageviews.session.unique())))
df.info()
errors = y_pred - y_train
df1 = pd.read_csv('df_providers.csv' )
autos["last_seen_10"].value_counts(normalize = True, dropna = False).sort_index()
print(df_release.shape) $ df_release.columns
average_polarity=pd.DataFrame() $ count_polarity=pd.DataFrame()
sum(rfc_bet_over)
html = browser.html $ soup = bs(html, 'html.parser') $ current_weather_info = soup.find_all('p', class_="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text") $ current_weather_info
yc200902_short_with_duration = pd.read_csv('http://cosmo.nyu.edu/~fb55/data/yc200902_short_with_duration.csv') $ yc_sd = yc200902_short_with_duration $ yc_sd.head()
print("Number of Tools in Mobile ATT&CK") $ print(len(all_mobile['tools'])) $ df = all_mobile['tools'] $ df = json_normalize(df) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
customer_visitors_new.index.levels
gs_from_model.score(X_test, y_test_over)
yelp_class = dataset[(dataset['rating'] == 1) | (dataset['rating'] == 5)]
atloc_opp_dist_count_prop_overall = compute_count_prop_overall(atloc_opp_dist, 'remappedResponses') $ atloc_opp_dist_count_prop_overall
data_activ = pd.read_csv(r'X:\data_report\01.Result\jobsid\applications\result.csv', chunksize=None)
combined_df = pd.merge(business_df, address_df,  how = 'left', left_on = 'prop', right_on = 'nndr_prop_ref')
n_new = df2.query('landing_page == "new_page"')['landing_page'].count() $ n_new
autos['odometer_km'].describe()
reg_lm.summary()
merged_index_month = merged2.index.month
doglist = dogs.dropna(how='all') $ doglist
conn = sqlite3.connect('twitter_testing.sqlite') $ cur = conn.cursor() $
url_SEA = "https://seahawks.strmarketplace.com/Images/Teams/SeattleSeahawks/SalesData/Seattle-Seahawks-Sales-Data.xls"
potholes = data_311['REQUEST_TYPE'] == "Potholes" $ highland_park= data_311['NEIGHBORHOOD'] == "Highland Park" $
rng = pd.date_range('1/1/2012', periods=100, freq='Min') $ ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) $ ts
df["three_day_reminder_profile_incomplete_sent"].value_counts()
random.choice( ['red', 'black', 'green'] )
hashtag_counts.plot(kind='bar')
joined['PubDate'] = pd.to_datetime(joined['PubDate']) $ joined['Weekday'] = joined['PubDate'].dt.weekday $ joined['Hour'] = joined['PubDate'].dt.hour
job_requirements.shape $
events_pd['just_date'] = events_pd['event_date'].dt.date $ events_pd['just_date'].value_counts().head(10).plot.bar()
print df1.columns $
logit_countries = sm.Logit(df_final['converted'], $                            df_final[['country_UK', 'country_US', 'intercept']]) $ results_fit = logit_countries.fit() $ results_fit.summary()
import pandas as pd $ pd.crosstab(test_set[1], predictions, rownames=['actuals'], colnames=['predictions'])
print('There are {} news articles'.format(len(news))) $ print('Timewise, we have news from {} to {}'.format(min(news.date), max(news.date)))
df.set_index('datetime',inplace=True)
day = lambda x: x.split(',') $ emotion_big_df['date']=emotion_big_df['date'].apply(day)
nba_df.dtypes
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
avg_per_seat_price_seasonsandteams["2013 Offseason", "BAL"] # This is the average price of PSLs after BAL won the SuperBowl.
actual_payments['iso_date_next']=pd.to_datetime(actual_payments.iso_date)+pd.
cols = {i:[] for i in range(300)} $ index = [] $ for line in lines: $     row_split(line, cols, index) $ df = pd.DataFrame(data=cols, index=index).astype(float) $
twitter_df_clean['source'].unique()
season15["InorOff"] = "In-Season"
pk_planes = pk_planes.distinct()
bb_df.describe()
first_time - time_delta
damd['created'] = pd.to_datetime(damd['created']) $ damd['created'].head(3)
df_test = pd.read_csv('loan_test.csv') $ df_test.head() $
product_ids=products["ProductID"]
dule2.columns
results_df
converted_users = sum(df.converted == 1)/len(unique_usrs) $ converted_users #proportion of users converted
df.to_csv('ab_new.csv', index=False)
day_of_week15 = uber_15["day_of_week"].value_counts().to_frame()
print("dfWeek = ",dfWeek['Contract Value (Weekly)'].sum(), "dfWeek Project Count = ", dfWeek['Project Name'].nunique()) $ print("dfDay = ",dfDay['Contract Value (Daily)'].sum(), "dfDay Project Count = ", dfDay['Project Name'].nunique())
rng.to_pydatetime()[0]
df_new['country_UK'] = df_new['country'].replace(('US', 'UK', 'CA'), (0, 1, 0)) $ lm_uk = sm.OLS(df_new['converted'], df_new[['intercept', 'country_UK']]) $ results_uk = lm_uk.fit() $ results_uk.summary()
n_new = len(df2.query('landing_page=="new_page"')) $ n_new
archive_df_clean.info()
countries = pd.read_csv("../data/airbnb/countries.csv")
url = 'http://mobilizationjournal.org/toc/maiq/22/2' $ html= requests.get(url).text $ save_file(html, 'moby_22_2.html') $
%%timeit $ scipy.optimize.fsolve(globals()[function_name], 2)
filtered_df.shape
tickerdata = tickerdata.ix['Close']
trial
ts
ux.html_utils.get_webpage_description(urls[0])
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) $ X_train.shape
df_mod['UK_ab_page'] = df_mod['UK']*df_mod['ab_page'] $ df_mod['US_ab_page'] = df_mod['US']*df_mod['ab_page']
from sklearn import mixture
list(purchases.columns)
for col in missing_info: $     num_missing = data[data[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing)) #count of missing data
a_mismatch = df.query('group == "control" and landing_page =="new_page"' ) $ b_mismatch = df.query('group == "treatment" and landing_page =="old_page"' ) $ a_mismatch.shape[0] + b_mismatch.shape[0]
SCR_PLANS_df.head()
%run twitter_creds.py
aggreg1
df_json_tweets['date_timestamp'] = pd.to_datetime(df_json_tweets['date_timestamp']) $ type(df_json_tweets['date_timestamp'].iloc[0])
boto.s3.connection.Location.USWest2
token_receivecnt = token.groupby(["receiver","month"]).size().reset_index(name= "receivecount")
data['orientation'].value_counts()
goog.Open.plot() $ goog.Close.plot()
rsp.json()
print 'Set date strings to be month or day first.  Month first (AMERICA) is default.' $ dti1 = pd.to_datetime(['8/1/2014']) $ dti2 = pd.to_datetime(['2/8/2014'], dayfirst=True) $ dti1[0], dti2[0]
pres_df['day_of_week'] = pres_df['start_time'].map(lambda x: x.strftime("%A")) $ pres_df.head()
df3.to_csv('output/last_100_tweets.csv', index=False)
df_sms['group'] = df_sms['BulkSmsBatchName'].str.extract('^(.*)/', expand=True) $ df_sms['messageCount'] = np.ceil(df_sms['PayloadSummary'].str.len()/160.0) $
tweet_image_predictions_clean = tweet_image_predictions
df.neu = df[["COMPANY", "DISPOSITION_TYPE", "TOTAL_PAYMENT", "DATE"]] $ df.neu
atdist_4x = get_merged_at_distance(deepcopy(tasklocations_4x), $                                    deepcopy(atdistnotif_4x), $                                    deepcopy(atdistresp_4x)) $ atdist_4x = atdist_4x[atdist_4x['infoIncluded']] # 4X Only: remove cases without info
vidsByCountry_df = youtube_df[youtube_df["title"].isin(title_namesC)] $ vidsByCountry_dfMax = vidsByCountry_df.loc[vidsByCountry_df.groupby(["title"])["views"].idxmax()] $ vidsByCountry_dfMax
data.columns
eia_total_monthly.head()
tweet_counts_by_month.plot()
df_master.info()
twitter_archive_master.groupby('has_stage')['rating'].mean()
sample_index = honeypot_df['src'].sample(10,random_state = 0).reset_index(drop = True) $ sample_regular = regular_traffic['id.orig_h'].sample(10, random_state = 0).reset_index(drop = True) $ print(sample_index) $ print(sample_regular)
sales_change_growth = (sales_diff['Sale (Dollars)_y'].values - sales_diff['Sale (Dollars)_x'].values)/(sales_diff['Sale (Dollars)_x'].values) $ len(sales_change_growth) $ total_sales['sales_change_growth'] = sales_change_growth $
gr_e1 = df.query("group == 'treatment' and landing_page == 'old_page'")
afreq = np.logspace( -5, 0, 30 ) $ Sconf = gwt.lisa_Sconf(3) $ plt.loglog( afreq, np.sqrt( Sconf(afreq) ), 'b-' ) $ plt.ylim([1e-21, 1e-14]) $ plt.grid(True) $
calls_df.groupby(["dial_type"])["length_in_sec"].mean()
catList = [] $ for index, row in db.iterrows(): $     itemCategoryString = "{} {}".format(row['Category'],row['Sub-Category'])   $     for word in itemCategoryString.split(" "): $         catList.append(word)   
dfp['2013-01-01 10H':'2013-01-01 11H']
print(f'Got {len(rentals):,.0f} listings')
pop_dog = twitter_archive_master.groupby('p1')['Rating_number'].mean().reset_index() $ pop_dog.sort_values('Rating_number', ascending=False).head()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ print(len(new_page_converted))
bg_df2.dtypes # list columns with data types
!kaggle datasets download -d cfpb/us-consumer-finance-complaints $ !ls
df.text.str.len().describe()
!ls -l StockData*
print(contribs.info())
firstday = (vader_df['created_at'] > '2017-10-28') & (vader_df['created_at'] < '2017-10-31') $ firstday_df = df[firstday] $ firstday_df = firstday_df.sort_values(by="sentiment", ascending = False) $ firstday_df.to_csv("firstday.csv") $ firstday_df
pred_probas_over
c=contractor[contractor['contractor_number'].isin([11004,11201,11202])] $ c[['contractor_id','contractor_type_id','contractor_version','contractor_bus_name', $    'contractor_number','last_updated']].sort_values(by='last_updated', ascending=False)
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     print(repr(f.root.multidimensional_data.array_3d[2:7, 1, :]))
n_old = df2.query('group == "control"')['user_id'].count() $ n_old
row_count = df.shape[0] $ print(row_count) $
import numpy as np $ x = np.array([2, 3, 5, 7, 11, 13]) $ x * 2
twitter_archive_enhanced_clean.head(50)
precip_df.plot(title="Hawaii Precipitation 2016") $ plt.tight_layout() $ plt.ylabel("prcp") $ plt.show()
B4JAN16['Contact_ID'].value_counts().sum() $
testmx = transformed[4]['text']
crimes=pd.read_csv('data/crime/crimes_chi.csv', parse_dates=True)
obj3
len(df[df.elapsed < datetime.timedelta(minutes=0)])
segmentData = segmentData[segmentData.opportunity_stage != 'Closed - Duplicate Record']
df_usnpl_one_hot = pd.get_dummies(df_usnpl[['domain', 'state_level_media']], columns=['state_level_media'])
cassession.caslibinfo()
df_tweet_json_clean = df_tweet_json_clean.drop('hashtags', axis = 1)
print(X_train.shape) $ print(X_valid.shape) $ print(X_test.shape)
list_of_genre_1990s_clean = list() $ for x in list_of_genre_1990s_1: $     if(x != ''): $         list_of_genre_1990s_clean.append(x)
gpd.read_file('moscow/moscow0.shp')
print ('Mean difference:', Ralston["T_DELTA"].mean()) $
for Column in StockData: $     print(Column) $
dreamRequest = requests.get(dreamophone_url) $ dreamSoup = bs4.BeautifulSoup(dreamRequest.text, 'html.parser') $ with open('dreamFormat.html', mode='w', encoding='utf-8') as f: $     f.write(dreamRequest.text)
attend_with = attend_with.drop(['[', ']'], axis=1) $ attend_with = attend_with.drop(attend_with.columns[0], axis=1)
df.dropna(axis=1, thresh=1000).shape == df2.shape
seaborn.countplot(vacancies.hour)
temp = X_train['building_id'] $ print temp.groupby(temp).size().sort_values(ascending=False).head(10) $ print temp.value_counts().head(10) #above is doing the same as this line
df = pd.read_sql('SELECT hotel_name, contact_email FROM hotel WHERE hotel_name=\'Hyatt Regency Sydney\'', con=conn_a) $ df
remove_cols = find_missing_data_cols(df)
np.unique(bdata.index.time)
hc.sql('drop table if exists asm_wspace.usage_400hz_2017_Q4')
np.exp(0.0507), np.exp(0.0408)
display(data.head(10))
session_df.groupby(by='experiment').agg({'delta_recall': ['mean', 'min', 'max']})
iplot(monthly_mean.iplot(asFigure = True, vline=['2017/09', '2017/03', '2016/09', '2016/03', '2015/09', '2014/12', '2014/04', '2013/10'], dimensions=(750, 500)))
assert stacked_image_predictions.shape[0] == 3*image_predictions.shape[0]
for row in selfharmm_topic_names_df.iloc[4]: $     print(row)
df
search_df = pd.DataFrame(data['data'])
min(TestData.DOB_clean), max(TestData.DOB_clean)
%matplotlib inline
target = "loan_result" $ predictor_columns = ["loan_amnt", "term", "home_ownership", "annual_inc", "verification_status", "purpose", $           "addr_state", "dti", "delinq_2yrs", "open_acc", "pub_rec", "revol_bal", "total_acc", $           "emp_length", "credit_length_in_years", "inq_last_6mths", "revol_util"]
import pickle $ features,target= pickle.load(open('preprocess_data.p', mode='rb'))
kw_list = ["Bitcoin"] $
print dfNYC.columns $ print len(dfNYC.columns)
import numpy as np $ import pandas as pd $ import datetime $ import matplotlib.pyplot as plt $ import seaborn as sns $
train.head()
s = BeautifulSoup(doc,'html.parser') $ print(s.prettify())
target = np.array(fraud_data_updated['class']) $ features = np.array(fraud_data_updated.drop(['class'],axis=1))
posts.find({"reinsurer": "AIG"}).count()
def sunday(s): $     return s.split("-")[1] $ ccl["Date"] = ccl["Date"].apply(sunday)
dtypes={'id': np.int64,'date':np.str,'store_nbr':np.int64,'item_nbr':np.int64,'unit_sales': np.float64,'onpromotion':np.float64} $ parse_dates=['date'] $ train = pd.read_csv('train.csv', dtype=dtypes,parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",train.shape) $ pd.DataFrame.head(train)
print 'train mse: ', mean_squared_error(y_train, predictions) $ print 'train r2: ', r2_score(y_train, predictions) $ print 'dev mse: ', mean_squared_error(y_dev, dev_predictions) $ print 'dev r2: ', r2_score(y_dev, dev_predictions)
sandag_df = pd.read_csv('./Datasets/SANDAG_Cleaned.csv') $ sandag_df = sandag_df.set_index(['year', 'tract']).sort_index() $ sandag_df = sandag_df[['pop', 'age', 'male_pct', 'white_pct']] $ sandag_df.head()
df.iloc[1:4,0:3]
df_usa['GDP/capita'] = df_usa['GDP']/(df_usa['Population']*1000) $ df_usa
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()
reddit_comments_data.groupby('author_flair_css_class').count().orderBy('count', ascending = False).show(100, truncate = False)
print("The tweet with most likes is: \n{}".format(data['Tweets'][likes])) $ print("Number of likes: {}".format(likes_max)) $ print("{} characters.".format(data['len'][likes])) $ print("Tweeted at {s}".format(s=data['Date'][likes]))
%matplotlib inline $ git_blame['knowing'].value_counts().plot.pie(label="") $ print(git_blame['knowing'].mean())
pulledTweets_df['Processed_tweet'] = pulledTweets_df['text'].apply(cleaner) $ pulledTweets_df['Processed_tweet'] = pulledTweets_df['Processed_tweet'].apply(remove_stopwords) $ pulledTweets_df['Processed_tweet'] = pulledTweets_df['Processed_tweet'].apply(lemmatize)
dfTransacciones.tail()
df['loan_status'].value_counts()
len(dfs)
week22 = week21.rename(columns={154:'154'}) $ stocks = stocks.rename(columns={'Week 21':'Week 22','147':'154'}) $ week22 = pd.merge(stocks,week22,on=['154','Tickers']) $ week22.drop_duplicates(subset='Link',inplace=True)
for i, row in breakfastlunchdinner.iterrows(): $     breakfastlunchdinner.loc[i, 'day_sales'] = sum(row[1:]) * .002 $ breakfastlunchdinner
from sklearn.linear_model import LogisticRegression $ from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import os $ os.makedirs(PATH) $ os.chdir(PATH) $ !wget http://files.fast.ai/part2/lesson14/rossmann.tgz $
students[(students.gender == 'F') | (students.weight >= 140)]
data = open("test_data//dummy.txt").read()
df_selected = df_selected.filter("numOfReviews > 4")
df2['converted'].mean()
E = np.logspace(1, 4, 10) * u.GeV $ print(E.to('TeV'))
obs_mean = df2.converted.mean() $ obs_mean
old_page_converted = np.random.binomial(1, p_old, n_old) $ old_page_converted
tw_clean.name = tw_clean.name.replace("a", "None")
results = logit.fit() $ results.summary()
rounds_df = rounds[rounds.company_uuid.isin(df.uuid)].copy() $ rounds_df = rounds_df[(rounds_df.announced_year >= 1990) & (rounds_df.announced_year <= 2016)].copy()
import imp $ imp.reload(imp)
testObjDocs.buildOutDF(tst_lat_lon_df[985:990])
df_measures_users = pd.read_csv('../data/interim/df_measures_users.csv', encoding="utf-8")
len(df[(df['Complaint Type'] == 'Homeless Encampment')&(df.index.month.isin([6,7,8]))])
plt.figure(figsize = (6,6)) $ kmeans.fit_predict(X) $ plt.scatter(X[:,0],X[:,1], c = kmeans.labels_, cmap='rainbow')  $ plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,0], color='black')  
train_set = train_set.loc[(train_set.polarity_value == 'P') | (train_set.polarity_value == 'N') ]
ripple_github_issues_url = blockchain_projects_github_issues_urls[4] $ ripple_github_issues_df  = pd.read_json(get_http_json_response_contents(ripple_github_issues_url))
df_interests = pd.read_csv('user_interests.csv') $ df_interests.head(3)
twitter_archive_clean=twitter_archive_clean[twitter_archive_clean['in_reply_to_status_id'].isnull()]
american_corpus, american_train_model, american_dictionary = create_LDA_model(american_train, $                                                                               'reviews_without_rare_words', 10, 40)
save_n_load_df(promo_df, 'promo_df6.pkl')
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2 =  pd.read_pickle("df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p")
import pandas as pd $ import matplotlib $ import numpy as np $ import calendar $ %matplotlib inline
df = df[['Adj. Open',  'Adj. High',  'Adj. Low',  'Adj. Close', 'Adj. Volume']]
target = wages['wage_per_hour'].values $ print(target.shape) $ target[0:6] $
print ts.groupby('year').sum().tail(5)
before = np.array(tweets_df[tweets_df['Date'] == "2017-11-12"]['Sentiment']) $ during = np.array(tweets_df[tweets_df['Date'] == "2017-11-13"]['Sentiment']) $ after = np.array(tweets_df[tweets_df['Date'] == "2017-11-14"]['Sentiment'])
Meter1.EndAutoRun()
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv', nrows=10)
archive_clean.info()
df2.query('landing_page == "new_page"').shape[0]/df2['landing_page'].shape[0]
df2.shape[0]
df2_dummy = pd.get_dummies(data = df2, columns = ['landing_page'], prefix = ['ab_page']) $ df2_dummy.head()
print("Probability of user converting is :", ab_file2.converted.mean())
import requests $ import numpy as np
!ls
pickle.dump(nmf_cv_df, open('iteration1_files/epoch3/nmf_cv_df.pkl', 'wb'))
word_freq.most_common(25)
wrd_clean['rating_numerator'] = wrd_clean['rating_numerator'].astype('str').apply(lambda x: x.split('/10')[0]).astype('float') $ wrd_clean['rating_denominator'] = wrd_clean['rating_denominator'].astype('str').apply(lambda x: x if x == 'nan' else x.split('/')[1]).astype('float')
def linear_func(xdata, m): $     return m*xdata
cost(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate) $
yc_new1.rename(columns={'incomePC':'income_departure'}, inplace=True) $ yc_new1.columns
hsi = pd.read_csv("HSI.csv") $ hsi.head()
df_comb['CA_new_page'] = df_comb['ab_page']* df_comb['CA'] $ df_comb['UK_new_page'] = df_comb['ab_page']* df_comb['UK'] $ df_comb.head()
old_page_converted = np.random.choice([0,1],size=n_old, p=(cr_old_null,1-cr_old_null))
df2.query('user_id == 773192')
X.head(2)
fp7 = fp7_proj.merge( $     fp7_part, $     on=["projectRCN","projectID","projectAcronym"] $ )
linear = linear_model.LinearRegression() $ linear.fit(x_15, y) $ (linear.coef_, linear.intercept_) $
loans_df.issue_d.value_counts()
femalebydate = female.groupby(['Date','Sex']).count().reset_index() $ femalebydate.head(3)
df['A']  # Selecting a single column
price2017 = price2017.drop(['Date','Time'], axis=1)
!wc -l data/autos.csv
a1.head()
df['in_reply_to_user_id'].value_counts()
train_data['abtest_int'] = train_data['abtest'].apply(get_integer1) $ test_data['abtest_int'] = test_data['abtest'].apply(get_integer1) $ del train_data['abtest'] $ del test_data['abtest']
autos["registration_year"].describe()
squares[2:4]
all_cities = pd.merge(left=city_loc, right=city_pop, on="city", how="outer") $ all_cities
us.loc[us['country'].str.len() == 2, 'country'] = us.loc[us['country'].str.len() == 2, 'country'].str.upper() $ us.loc[us['country'].str.len() == 2, 'country'].value_counts(dropna=False)
guinea_data['Description'].replace('New deaths registered today', 'New deaths registered', inplace=True)
list(venues.columns)
Lab7_RevenueEPS0 = pd.read_excel(r"C:\Users\Tanushree\Desktop\UNIVERSITY\Quarter 4\Dashboard\Lab7_Session\Nasdaq data.xlsx",sheetname = 0)
train['day'] = train.created_at.dt.weekday $ train.groupby('day').popular.mean()
GBR=GradientBoostingRegressor(alpha=0.001,n_estimators=200, random_state=34)
params = {'figure.figsize': [8,8],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(RN_PA_duration, params=params, freq=31, title='RN/PA Decomposition')
data.text.str.contains("sandwich").resample("1T").sum().plot()
print(len(BroncosBillsTweets['text30'])) $ print(len(BroncosBillsTweets['text30'].unique()))
r.content
package.infer('./solutions/data/**/*.csv')
df_breed.head()
itemTable["Project"] = itemTable["Project_Id"].map(project_link)
condition = (users['cityOrState'].isna() == False) & (users['country'].isna() == False) $ users[condition].head()
session.query(Stations.station, Stations.name).all()
image_clean.head(5)
print(gs_rfc_under.best_score_) $ print(gs_rfc_under.best_params_)
indeed.dropna(subset=['summary'], inplace=True) $ indeed.isnull().sum()
df_place.head()
ser.iloc[:1]
df[df['group'] == 'control'].query("landing_page == 'new_page'").index.values $ df2 = df.drop(df[df['group'] == 'control'].query("landing_page == 'new_page'").index.values)
norm.ppf(1-0.05)
my_df['target'] = my_df['target'].astype(int)
data2
i = [t for t in range(5)] 
if 1 == 1: $     news_period_df = pd.read_pickle(config.NEWS_PERIOD_DF_PKL)
job_requirements = pd.DataFrame(requirements) $ job_requirements $
evaluator = MulticlassClassificationEvaluator( $     labelCol="label", predictionCol="prediction", metricName="weightedPrecision") $ precision = evaluator.evaluate(predictions) $ print("Precision = %g " % (precision))
reactions_list = ["fb_like"] + actual_reactions_list $ def fb_total_reactions(dataframe_row): $     return sum(dataframe_row[r] for r in reactions_list) $ function = fb_total_reactions $ dataset_test.loc[:,function.__name__] = dataset_test.apply(lambda x: function(x), axis=1)
raw_data.isna().sum()
n_new=(df2['landing_page']=='new_page').sum() $ n_new
df[df['Agency Name'].isin(['Department of Transportation', 'DOT'])].count()
dftrips.tail(5)
for row in selfharmm_topic_names_df.iloc[1]: $     print(row)
post_urls = [] $ for i in range(len(df[0:5])): $     post_urls.append(df.iloc[i]['link_to_post'][i][-11:]) $ post_urls $
countries_df.head()
properati.loc[(properati['zone'] == "") & (properati['place_name'] == "Capital Federal"),'zone'] = "Capital Federal"
class LetsGetMeta(type): $     def __init__(cls, name, bases, namespace): $         print(cls, name, bases, namespace)
print gs.best_params_ $ print gs.best_score_
start_lat = [] $ start_lon = []
weekly=one_station.groupby(['week_num', 'weekday'])['DAILY_ENTRIES'].sum().reset_index() $ weekly.head(10)
df = target_pf[target_pf['date'] == inception_date].copy()
A = pd.DataFrame(rng.randint(0, 20, (2,2)), columns=list('AB')) $ A
usernodes = list( db.osm.find({"created.user": "Bot45715"})) $ print len(usernodes) $ usernodes[:10]
rejected['approved'] = False $ rejected.head()
df.resample('D', how='mean')
lsa_tfidf_topic6_sample_fp_list = [2900, 1509, 2106, 1087, 556, 1429, 4108, 2925, 3630, 1092, 2758, 192, 4768, 1116, 4789, 2365, 2015, 2983, 4609, 3292, 1296, 4717, 1269, 1370, 2696, 5468, 4005, 4484, 1824, 1447, 1445, 5304, 2669, 3633, 2110, 2746, 3551, 2288, 1463]
salesdec = pd.read_csv ("Video_Games_Sales_as_at_22_Dec_2016.csv") $ salesdec.head() $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') $ print ('z-score:',z_score) $ print ('p-value:',p_value)
ser8 = pd.Series(np.random.randint(0,1000,10000)) $ for label, value in ser8.iteritems(): $     ser8.loc[label]= value+2  
df2.query('landing_page == "old_page"').shape[0]
df_all.to_clipboard()
df.head()
df_all_loans.head()
df2.query('user_id == "773192"')
dci = indices(dcaggr, 'text', 'YearWeek') $ dci.head()
success_rate = df_select['status'].mean() $ success_rate
plt.rcParams['axes.unicode_minus'] = False $ dta_692.plot(figsize=(15,5)) $ plt.show()
plt.rcParams["figure.figsize"] = [20,30] $ country_size.plot.barh().invert_yaxis()
plt.hist(p_diffs); $
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner')
contribs.head()
DT_yhat = DT.predict(test_X) $ print("DT Jaccard index: %.2f" % jaccard_similarity_score(test_y, DT_yhat)) $ print("DT F1-score: %.2f" % f1_score(test_y, DT_yhat, average='weighted') )
poverty_2011_2015.head()
df_kick= kickstarters_2017.set_index('ID')
traindata = pd.read_csv('data/new_subset_data/final_train_data.csv') $ testdata = pd.read_csv('data/new_subset_data/final_test_data.csv')
lv_workspace.get_subset_object('B').get_step_object('step_1').show_settings() $ lv_workspace.get_subset_object('B').get_data_filter_object(step=1).exclude_list_filter
es_dictindex = "{0}_dictionary".format(city.lower()) $ es_dicttype = 'dictionary' $ es.createOrReplaceIndex(es_dictindex)
df_image_clean.info()
%run '../additional_functions.ipynb'
file_name = str(time.strftime("%m-%d-%y")) + "-tweets.csv" $ tweet_df.to_csv("analysis/" + file_name, mode = 'w',encoding="utf-8",index = False) $
df_potholes.groupby(df_potholes.index.weekday)['Created Date'].count().plot(kind="bar") $
pivoted.plot(legend=False, alpha = 0.1);
len(df_license_activities)
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK','US']] $ df_new.head()
stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)}) $ len(itos)
cnn_fox_data = cnn_fox_data.reindex(np.random.permutation(cnn_fox_data.index))
from sklearn.feature_extraction.text import CountVectorizer $ count = CountVectorizer(stop_words='english', $                         max_df=.1, $                         max_features=5000) $ X = count.fit_transform(df['review'].values)
list(filter(lambda x: x.endswith('Date'), geocoded_df.columns))
!convert materials-xy.ppm materials-xy.png $ Image(filename='materials-xy.png')
weather_mean.iloc[:, 2:5]
archive_copy.info()
avg_att_2015_MIL = nba_df.loc[(nba_df["Season"] == 2015) & (nba_df["Team"] == "MIL"), "Home.Attendance"].mean() $ round(avg_att_2015_MIL, 0)
df = pd.DataFrame.from_records(results) $ df.shape
df_events = pd.read_csv('events.csv') $ df_events.head()
df_new['intercept'] = 1 $ df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country'])
importances = zip(X.columns, rsskb_gbm_best.feature_importances_) $ importances.sort(key=lambda x: x[1], reverse=True) $ importances
df.ix[0].A = 1 $ store['df'] = df $ pd.HDFStore('store.h5')['df'].head(2)
start_date=dt.date(2017,8,1) $ end_date=dt.date(2017,8,10)
got_data['total_comments'] > 5000 $ got_data[got_data['total_comments'] > 10000]
files8['Tenure'] = files8['Tenure'].fillna(0)
print(display(stock_data.head(3))) $ stock_data = stock_data.iloc[::-1] $ print(display(stock_data.head(3))) $ print(display(stock_data.tail(3)))
unique_users = df.nunique()['user_id'] $ print(unique_users)
df.info()
df2 = DataFrame(data) $ df2
! free -h
df_train.info()
ab_df.converted.mean()
for i in inv: $     a1=repaid_loans_cash[(repaid_loans_cash.fk_loan==36) & (repaid_loans_cash.fk_user_investor==i)] $     a2=loan_principals[(loan_principals.fk_loan==36) & (loan_principals.fk_user_investor==i)] $     print i,xirr([a1,a2]) $
df = pd.DataFrame([['Daniel','-',42], $                    ['John', 12, 31], $                    ['Jane', 24, 27]]) $ df.columns = ['name','treatment a','treatment b'] $ print(df)
ndvi_grid = np.array(np.meshgrid(lon_us, lat_us)).reshape(2, -1).T $ np.shape(ndvi_grid)
countries=pd.read_csv('countries.csv') $ countries.head()
learning_rate = 0.01 $ with tf.name_scope("train"): $     optimizer = tf.train.GradientDescentOptimizer(learning_rate) $     training_op = optimizer.minimize(loss)
df.head()
import re $ df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.replace('x', '') $ df.head()[['Product', 'Consumer complaint narrative']]
journalist_gender_summary_df = pd.DataFrame({'count':user_summary_df.gender.value_counts(), 'percentage':user_summary_df.gender.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'}) $ journalist_gender_summary_df
df_twitter_clean.head()
len(submission_full)
van_final['FirstMeta'] = van_final.groupby('userid')['pagetitle'].first().str.contains('/')
archive_df_clean['name'].replace(['very','the','a','an','None','not','one'],['NA','NA','NA','NA','NA','NA','NA'], inplace=True) $
df.converted.sum()/df_length
x = range(len(df.index)) $ x
df2.drop_duplicates(subset="user_id", inplace = True)
def get_list_tot_likes(the_posts): $     list_tot_likes = [] $     for i in list_Media_ID: $         list_tot_likes.append(the_posts[i]['activity'][-1]['likes']) $     return list_tot_likes
ux.html_utils.get_webpage_title(urls[0])
'{:,.2f}'.format(datetime.datetime(2099,12,31,0,0).timestamp()*1000)
ids = [] $ for page in tweepy.Cursor(api.followers_ids, screen_name="DibeBot").pages(): $     ids.extend(page) $     time.sleep(60) $ print(len(ids))
new_cases=dropped.loc['Total new cases registered so far'] $ new_cases.head()
initialdate_per_coin = df0.groupby('symbol').date.min() $ initialdate_per_coin.to_csv('output/initialdate_per_coin.csv')
print(dtm_tfidf_df.max().sort_values(ascending=False)[0:20])
vehicleType_list = list(set(train_data.vehicleType))
by_area['AQI'].plot(); plt.legend();
def upper_bound(QCB, M): $     return 0.5 * QCB ** M $ def lower_bound(tr, M): $     return (1 - np.sqrt(1 - tr ** (2 * M))) / 2
autos["brand"].value_counts()
df = pd.DataFrame(bmp_series, columns=["mean_price"]) $ df["mean_mileage"] = bmm_series $ df.head()
train.date_first_booking.dtypes
df_DST.to_csv('DST(Time-Series Format).csv')
fd_dist = [vs for word in wsj $            for vs in re.findall(r'[aeiou][aeiou]', word.lower())] $ fd = nltk.ConditionalFreqDist(fd_dist) $ fd = pd.DataFrame(fd).fillna(value=0).astype(dtype=int) $ fd
data2.head()
fit3.resid.hist();
new_albums = pd.read_csv("new_albums.csv") $ top_200 = pd.read_csv("top_200_table.csv",sep="\t",index_col = 0) $ del new_albums["Unnamed: 0"]
alltweets = [] $ new_tweets = api.user_timeline(screen_name = 'realDonaldTrump', count=200) $ alltweets.extend(new_tweets) $ oldest = alltweets[-1].id - 1
sum(df2.duplicated(subset='user_id')) $ list(df2[df2.duplicated(subset='user_id')]['user_id']) $
backup = clean_rates.copy() $ rating_numbers = clean_rates.text.str.extract(r'(?P<numerator>\d+(\.\d+)?)/(?P<denominator>\d+(\.\d+)?)', expand=True) $ clean_rates['rating_numerator'] = rating_numbers['numerator'] $ clean_rates['rating_denominator'] = rating_numbers['denominator']
print(f'We eliminated {b_cal.shape[0]-b_cal_q1.shape[0]} listings')
(LM_PATH / 'tmp').mkdir(exist_ok=True)
pd.Series(gbm_pred).isnull().sum(), pd.Series(y_test).isnull().sum()
titanic_clean.shape # We removed the dupliated columns 
%%time $ model = GradientBoostingClassifier() $ model.fit(train_x, train_y)
emails = re.findall('mailto:(.*?) so', text) $
train = monthly_sales[['shop_id', 'item_category_id', 'date_block_num', $                        'year','month', 'item_cnt_mean', 'prev_item_cnt_month', 'item_cnt_month']]
reddit_comments_data.groupby('edited').count().orderBy('count', ascending = False).show(100, truncate = False)
import re
df.plot() $ plt.show()
cp311["created_date"] = pd.to_datetime(cp311['created_date'],infer_datetime_format=True)
df_temperature.head()
print mike.ra[0] $ print mike.RA0[0]
import pandas as pd
df = df_1.reset_index().set_index('user_id').join(df_churn) $ df = df[df['event_leg']<=df['churn_leg']] $ df = df.drop('churn_leg',axis=1) $ df = df.groupby(['user_id','event_leg']).sum() $ df.head() $
converted_controlusers2 = float(df2.query('converted == 1 and group == "control"')['user_id'].nunique()) $ control_users2 =float(df2.query('group == "control"')['user_id'].nunique()) $ cp2 = converted_controlusers2 /control_users2 $ print(" Given that an individual was in the control group, the probability they converted is {0:.2%}".format(cp2))
country_result.summary()
trump['source'].unique()
tipsDF.tail()
sheets_with_bad_column_names = {sheet_name: sheet for sheet_name, sheet in sheets.items() $                                 if any(len(column.split()) > 1 for column in sheet.columns)}
X_train, X_val, \ $ y_train, y_val = train_test_split(training_features, training_target, test_size=0.33, random_state=12)
df_image.info()
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df['group']) $ df2.head()
sns.barplot(x="MILLESIME", y="target", hue="PROBLEM_CODE", data=df)
print 'Percentage of total amount for data with valid US state, but no city, zipcode: {:.3f}'.format(100*sum(df[(df.city == '') & (df.zipcode_initial == '')].amount)/sum(df.amount))
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new]) $ len(new_page_converted)
from dateutil.relativedelta import relativedelta $ import math
temps_df.columns
DummyCapRepricing
sweden = ['Sverige', 'Sweden', 'Stockholm', 'Gothenburg', 'Schweden', 'Sweden, Earth', 'Sweden, Universe'] $ notus.loc[notus['country'].isin(sweden), 'country'] = 'Sweden' $ notus.loc[notus['cityOrState'].isin(sweden), 'country'] = 'Sweden' $ notus.loc[notus['country'] == 'Sweden', 'cityOrState'].value_counts(dropna=False)
df.groupby(df.index.hour)['Created Date'].count().plot()
kmeans.labels_
df = pd.read_csv("/data/311_Service_Requests_from_2010_to_Present.csv", error_bad_lines=False) $
pd.DataFrame({'population': population, $               'area': area})
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
results = six_number_summary(df['y']) $ results
from IPython.display import Image $ Image(url= url_poster)
to_be_predicted_Day1 = 14.46 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
my_columns= list(df_users_first_transaction) $ my_columns
data.dtypes
col_names = json_normalize(j['datatable'], 'columns')['name'] $ print(col_names)
import kipoi_veff $ from kipoi_veff import VcfWriter $ vcf_path = "example_data/clinvar_donor_acceptor_chr22.vcf" $ out_vcf_fpath = vcf_path[:-4] + "%s.vcf"%model_name.replace("/", "_") $ writer = VcfWriter(model, vcf_path, out_vcf_fpath)
print('Collecting Todays Followers') $ todaysFollowers[date] = todaysFollowers['Account'].apply(get_NumberofFollowers)
if download: $     cmd = "curl --silent {} -o {}/{}".format(url, opath, filename) $     r = call(cmd, shell=True)
to_be_predicted_Day3 = 34.13953228 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
top_songs['Region'].unique()
dta.b.unique()
ibm_train = ibm_hr_final.join(ibm_hr_target.select("Attrition_numerical")) $ ibm_train.printSchema()
contractor[contractor.contractor_id.duplicated() == True]
type(sort_a_asc)
df.shape
trace0 = go.Scatter3d(x=lat_25_1.values, y=lng_25_1.values, z=day_25_1.values,mode='markers',marker=dict(size=12,line=dict(color='rgba(217, 217, 217, 0.14)', width=0.5),opacity=0.8)) $ data = [trace0] $ layout=go.Lay
doc_duration.index
import numpy as np
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata = pd.read_csv(path, sep ='\s+', na_values=['.'], names=['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'class']) $ mydata.head(5)
%%timeit $ scipy.optimize.root(globals()[function_name], 2)
groups_topics_df.shape, groups_topics_unique_df.shape
reddit = pd.read_csv('./datasets/reddit_scrape.csv')
df2.apply(lambda x: len(x.unique()))
ad_source = ad_source.drop(['[', ']'], axis=1) $ ad_source = ad_source.drop(ad_source.columns[0], axis=1)
date_ly = dt.date.today() - dt.timedelta(days = 2*365) $ print(date_ly)
print('The total number of tweets is: ', all_tweets.shape[0], 'tweets')
url = "https://mars.nasa.gov/news" $ response = requests.get(url) $ soup = bs(response.text, 'html.parser') $ print(soup.prettify())
data.groupby(['longitude', 'latitude']).count().idxmax()
hemisphere__image_urls
trips_data['duration'].hist() # displays histogram for duration of trips $
print X_train['building_id'].factorize()[0] $ print X_train['building_id'].factorize()[1] $ print X_train['building_id'].factorize()[0].shape, X_train['building_id'].factorize()[1].shape
tt_final.dropna(inplace=True) $ tt_final.info()
df2_new=countries_df.join(df2.set_index('user_id'),how='inner')
import pandas as pd $ import numpy as np $ from sklearn import linear_model $ import matplotlib.pyplot as plt $ %matplotlib inline
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Plot of simulated p_diffs');
df2[df2.landing_page == 'new_page'].shape[0]/df2.shape[0]
df[0:31]['link_to_post'][1][-11:]
tlen
latest_date = session.query(Measurement.date).\ $ order_by(Measurement.date.desc()).first()[0] $ latest_date
times.head()
df = pd.get_dummies(df)
df1 = transactions.groupby(['UserID','ProductID']).sum() $ df1
X_train.head(10)
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=20000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df1 = pd.DataFrame(pv1.to_records()) $ df1 = df1.set_index("Death year") $ df1.plot() $ plt.show()
Pnew = df2.converted.mean() $ Pnew
autos['price'].value_counts().sort_index(ascending = True).head(15)
universe = ["ADS.DE", "SAP.DE", "BAS.DE", "VOW3.DE", "CON.DE"] $ price_data = yahoo_finance.download_quotes(universe) $ return_vector = price_data.pct_change().dropna() $ print return_vector
h = pd.read_sql_query(QUERY, conn) $ h $
count = [convert_new, convert_old] $ nobs = [n_new, n_old] $ z_score, p_value = sm.stats.proportions_ztest(count, nobs, alternative = 'larger') $ z_score, p_value
import pandas as pd $ test_data = pd.read_csv("relex_example.csv") $ test_data.head()
vect = CountVectorizer(ngram_range=(2,4), max_df=0.4)
conversion = df['converted'].mean() $ conversion
trunc_df.iloc[list(indices)]
intFeatures = ['ride_distance','ooSuburbanDistance','ooCarSearchTime','quarter_int','month_int','day_int','dayofweek_int','ooSurgeMultiplier','Temp,C','Wind,km/h','Humidity','Barometer,mbar','pickup_cluster_demand','dropoff_cluster_demand','ride_cluster_demand']
df_lm['last_month'] = label $ df_lm.head()
!python csv_to_tfrecords.py --csv_input=images/test/test_labels.csv --image_dir=images/test --output_path=test.record
close_month.loc[month].nlargest(2)
try: $     c.iloc[c>0.7] $ except: $     print("nah") $     print(c.iloc[c.values>0.7])
pd.to_datetime(eth['Date(UTC)']).head()
sum(df2['converted'] == 1)/len(df2['user_id'])
tier3 = getcrimesby_tier(store1,Violent) $ tier3_df = tier3.copy()
party_type_crosstab['DEM_VS_GOP'] = party_type_crosstab['DEMOCRATIC'] - party_type_crosstab['REPUBLICAN']
S.decision_obj.stomResist.value = 'simpleResistance' $ S.decision_obj.stomResist.value
zipincome = pd.DataFrame(jsonData) $ zipincome.head()
import sys $ sys.path.append('/Users/argall/Documents/Python/') $ sys.path
nums = [1, 2, 5]  # a list of items, in this case, numbers
columns = inspector.get_columns('Measurement') $ for c in columns: $     print(c['name'], c["type"])
more_grades.columns = ["name", "month", "grade"] $ more_grades["bonus"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0] $ more_grades
pd.merge(df1, df2, how='outer')
m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1, $           layers=[em_sz*3, 50, c], drops=[dps[4], 0.1], $           dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])
age_gender_bkts = pd.read_csv("../data/airbnb/age_gender_bkts.csv")
qualification['qual_conversion'] = qualification.apply(qualConversion, axis=1)
X.head()
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG_max =(grouped.aggregate(np.max)['disc_times_pay']) $ grouped_by_year_DRG_max.head()
df['Complaint Type'][df['Agency'] == 'DOT'].value_counts().head()
free_data.head() $ free_data.groupby("age_cat").mean() $ free_data.groupby("age_cat").educ.mean()
plt.scatter(df_hate['Polarity'], df_hate['Subjectivity'], alpha=0.5, color='purple') $ plt.title('Tweet #hate, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
results['CharacteristicName'].value_counts()
freq = session.query(Measurement.date, Measurement.tobs).group_by(Measurement.date).\ $                     having(Measurement.date.like('2016%')).filter(Measurement.station == 'USC00519281').all() $ freq $
mongoDBname = "airline_database" $ collec_name = "new_airline_tweets"
print(len(fin_coins_r), " months")
loaded_text_classifier = TextClassifier.load(model_file) $ from tatk.feature_extraction import NGramsVectorizer $ word_ngram_vocab = NGramsVectorizer.load_vocabulary(word_vocab_file_path) $ char_ngram_vocab = NGramsVectorizer.load_vocabulary(char_vocab_file_path)
log_reg_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'country_CA', 'country_UK']]) $ results = log_reg_mod.fit() $ results.summary()
import pandas,numpy
df_clean.source.head(100)
with open('data/stopwords.txt') as fh: $     stopwords = set(fh.read().splitlines())
country_df = pd.read_csv('countries.csv') $ country_df.head() $
df.groupby('season')['episode_id'].nunique().agg(['min', 'mean', 'max'])
df_input_clean.filter("`Resp_time` <= 0").count()
reg_target_encoding(train,'Block',"any_spot") $ reg_target_encoding(train,'DOW',"Real.Spots") $ reg_target_encoding(train,'hour',"Real.Spots") $
print(dfd.in_pwr_5F_max.describe()) $ dfd.in_pwr_5F_max.hist()
import pandas as pd
df_prep4 = df_prep(df4) $ df_prep4_ = pd.DataFrame({'date':df_prep4.index, 'values':df_prep4.values}, index=pd.to_datetime(df_prep4.index))
gpCreditCard.Tip_amount.describe()
with sqlite3.connect('my83.sqlite') as db: $     df2 = pandas.read_sql_query('select * from answers, questions where answers.link = questions.link', con = db) $ df2.head()
hour_of_day15 = uber_15["hour_of_day"].value_counts().sort_index().to_frame()
from mlxtend.frequent_patterns import apriori $ from mlxtend.frequent_patterns import association_rules $ frequent_itemsets = apriori(basket_sets, min_support=0.001, use_colnames=True) $ rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
slices[4]
df2 = df2.drop_duplicates("user_id", keep="first")
male_journalists_retweet_summary_df = journalists_retweet_summary_df[journalists_retweet_summary_df.gender == 'M'] $ male_journalists_retweet_summary_df.to_csv('output/male_journalists_retweeted_by_journalists.csv') $ male_journalists_retweet_summary_df[journalist_retweet_summary_fields].head(25)
metrics.adjusted_rand_score(labels, km.labels_)
knn_10 = KNeighborsClassifier(n_neighbors=10)
pd.merge(df1,df3,how='right')
df_ca = df_new[df_new['country'] == 'CA'] $ df_ca['converted'].mean()
df3[df3['group'] == 'treatment'].head()
print('number of breeds: {}'.format(len(tweet_archive_master['dog_breed'].value_counts())))
df.shape 
countries_df = pd.read_csv('countries.csv') $ countries_df.head() # we firstly see that we need to match the user_id $
my_columns = list(Users_first_tran.columns) $ print(my_columns)
testing = df.text[226].decode("utf-8-sig") $ testing
conv_cont_prob = df2[df2['group']=='control']['converted'].mean() * 100 $ output = round(conv_cont_prob, 2) $ print("The probability of the control group individual converting regardless of the page they receive is: {}%".format(output))
df_twitter_copy.loc[375].text
ways_tags.head()
data.head() $
df['intercept']= 1 $ df[['control','treatment']] = pd.get_dummies(df['group'])
session.query(Measurements.station, func.count(Measurements.date)) \ $     .group_by(Measurements.station).order_by(func.count(Measurements.date).desc()).all()
appendMe = "\nThis is a new line" $ appendFile = open("exampleFile.txt","a") $ appendFile.write(appendMe) $ appendFile.close() # Don't forget to close the file
props.prop_name
df_countries.head()
df[(df['abuse_type']=='A') & (df['public']=='offline')].count()[0]
y_test.shape
estimator.predict_proba(X2)
print(a[2:])   # skip first two characters $ print(a[-7:])  # the last 7 characters $ print(a[2:6])  # 4 characters starting after 2nd character $ print(a[::2])  # Every second character
tidy = pd.concat([tweetsOverall, mostRecentTweets], axis='userID', join='inner') $ tidy.head()
df_goog.sort_values(by='Date', ascending=True, inplace=True) $ df_goog = df_goog.loc[(df_goog['Date'] != '2014-03-27')]
(null_vals< ab_dif).mean()
import numpy as np $ import pandas as pd $ import datetime as dt
rng_pytz = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='Europe/London')
nonpitch_unique_labels = list(results.columns.difference(pitches.columns)) $ nonpitch_unique_labels
model_sm.summary()
X_test_df = pd.DataFrame(X_test_matrix.todense(), $                         columns=tvec.get_feature_names(), $                         index=X_test.index)
mentions_raw, user_names_raw, user_screen_names_raw, num_tweets_raw, num_retweets_raw = au.get_mentions(uso17_coll)
facts_metrics.shape
import numpy as np $ tensor_1d = np.array([1, 2.5, 4.6, 5.75, 9.7]) $ tf_tensor=tf.convert_to_tensor(tensor_1d,dtype=tf.float64)
cutoff_times = generate_labels('/7/KMLZlMBnmWtb9NNkm3bYMQHWrt0C1BChb62EiQLM=', trans, $                                label_type = 'MS', churn_period = 14) $ cutoff_times[cutoff_times['churn'] == 1].head()
df_prep98 = df_prep(df98) $ df_prep98_ = pd.DataFrame({'date':df_prep98.index, 'values':df_prep98.values}, index=pd.to_datetime(df_prep98.index))
props.prop_name.value_counts().reset_index()
r = kimanalysis.queryresults(test='TE_320860761056_001', model='MO_800509458712_001')
print("\nClassification Report:\n",classification_report(y_test, y_hat)) $
%bash $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" sample/train.csv $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-*" sample/valid.csv
diff_pnew_pold = (new_page_converted.mean()) - (old_page_converted.mean()) $ print('P_new - P_old for simulated values from e and f is {}'.format(diff_pnew_pold))
lat = ndvi_nc.variables['latitude'][:] $ lon = ndvi_nc.variables['longitude'][:] $ time = ndvi_nc.variables['time'][:] $ ndvi = ndvi_nc.variables['NDVI'][0,:,:] $ np.shape(ndvi)
techmeme.columns = ['date', 'original_source', 'original_title', 'extra_sources', $        'extra_titles', 'date_time', 'news_sources', 'titles', 'news_text']
!ls
list(df_clean)
sess.get_historical_data('ibm us equity','px last',periodicity='MONTHLY')
plt.figure(figsize = (7,7)) $ plt.xlabel('Size') $ plt.ylabel('Eat Breakfast') $ plt.scatter(df["size"],df["age"],c="g", cmap='rainbow')  $
tweet_archive_clean.dog_stage.value_counts()
r.headers['content-type']
regr = linear_model.LinearRegression() $ regr.fit(X_features, y_features) $ print (regr.coef_)
sampling_rules = { $     "i_Education == 'Education_Elementary_School'": 100, $ }
lr = LinearRegression() $ lr.fit(train_features,train_target)
df.head(10)
calls_df["dial_type"].unique()
display(data.head(10))
sh_max_df = pd.DataFrame(np.array(sh_results), columns=(["date","tobs"])) $ sh_max_df
print('Merchants with more than one e-shop') $ print(data['Merchant_ID'].duplicated().sum())
autos["price"].value_counts().sort_index(ascending=False).head(20)
ab_data_diff = control_diff - treatment_diff $ ab_data_diff
def get_residual_sum_of_squares(model, data, outcome): $     predictions = model.predict(data) $     residuals = outcome - predictions $     RSS = (residuals ** 2).sum() $     return(RSS)
malebyphase = malemoon.groupby(['Moon Phase']).sum().reset_index() $ malebyphase
final_grades_clean = final_grades.dropna(how="all") $ final_grades_clean
autos['brand'].unique() $ print(len(autos['brand'].unique()))
s.index
kickstarter.isnull().sum()    
data_df.info()
twitter_archive[twitter_archive.text.str.contains(r"(\d+\.\d*\/\d+)")] $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') #larger for > sign in alternate hypothesis $ z_score, p_value
sample.head()
pro_table = profit_table_simple.groupby(['Year', "Month Name"]).Profit.sum().reset_index() $ pro_table
importlib.reload(util)
ADP_array=df["NASDAQ.ADP"].dropna().as_matrix() $
price_mat.columns = names $ mcap_mat.columns = names $ volumes.columns = names
path = '/Volumes/Development/Stock/Twits_res.csv'
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') $ print(z_score, p_value)
bad_comments[2]
print(nodes[1])
visits_parsed_as_dict = dict(zip(range(0,len(visits_parsed)), visits_parsed)) $ visits_parsed_as_dict[2]
count=0 $ for i in p_d: $     if i>p_diff: $         count+=1 $ greater=count
got_data.sort_values('total_comments', ascending=False, inplace=True)
logit = sm.Logit(df2['converted'], df2[['ab_page', 'intercept']]) $ result=logit.fit()
fix_wages(df_h1b_mv_ft, 6000, 35000, 500)
df_imput = MICE(n_imputations=200, impute_type='col', verbose=False).complete(X_mice_dummify.as_matrix())
most_active_stations = session.query(Measurement.station).\ $                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).limit(1).scalar() $ print ("Station: " + str(most_active_stations), "has the highest number of observations")
datatest.loc[datatest.rooms>16,'rooms'] = np.NaN
run txt2pdf.py -o"2018-06-18  2015 291 discharges.pdf"  "2018-06-18  2015 291 discharges.txt"
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
(turnstiles_df $  .groupby(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"]) $  .ENTRIES.count() $  .reset_index() $  .sort_values("ENTRIES", ascending=False)).head(5)
reason_for_visit.head()
df
sdf.select('_c0', date_format('_c0', 'HH:mm').alias('time'), date_format('_c0', 'E').alias('weekday'), date_format('_c0', 'u HH:mm').alias('weekdaytime')).show()
control_new.index
titanic = pd.read_excel('Data/titanic.xls') $ print(titanic.head)
for row in spark_df.take(2): $     print row
missing_sample.dropna()
prior_orders=orders[orders['eval_set']=='prior'] $ priors = pd.merge(order_products_prior, prior_orders, on='order_id',how ='right') $ priors.head()
double_app = picker[picker==2] $ print len(double_app) $ df_nona['create_date'] = df_nona.created.map(pd.to_datetime) $ dbl = df_nona[df_nona.district_id.isin(double_app.index)].groupby(['district_id', 'app_id', 'create_date']).install_rate.mean() $ dbl
sns.violinplot(autos["price"]);
planets.groupby('method')['orbital_period'].median()
df.drop(["last_event"], axis = 1, inplace=True) $ query_date = dt.date(2018,1,23) #this is when this particular dataset was generated, so we will use it to calculate group age $ df["created"] = [d.date() for d in df["created"]] #we only need the date from the original column (ignore time) $ age = query_date - df["created"] #find the age in days by subtracting creation date from the date the dataset was generated $ df["event_freq"] = age.dt.days / df["past_event_count"].astype(float) #needs to be float for division
v_to_c = pd.merge(visits, checkouts)
gdax_trans_btc
lm2 = sm.Logit(df_new['converted'], df_new[['control', 'US', 'UK','intercept']]) $ res = lm2.fit() $ res.summary()
X_test.head()
dfEPEXpeak = dfEPEXbase[(dfEPEXbase.index.hour >= 8) & (dfEPEXbase.index.hour < 20)] $ dfEPEXpeak.head(15) # verify extracted data only contains 09:00 to 20:00
pd.read_csv?
df_tot.head()
fifteen = pd.read_csv("MyLA311_Service_Request_Data_2015.csv", low_memory = False) $ sixteen = pd.read_csv("MyLA311_Service_Request_Data_2016.csv", low_memory = False) $ seventeen = pd.read_csv("MyLA311_Service_Request_Data_2017.csv", low_memory = False) $ eighteen = pd.read_csv("MyLA311_Service_Request_Data_2018.csv", low_memory = False)
url= 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json' \ $      '?&start_date=2018-08-21&end_date=2018-08-21&api_key={}' \ $         .format(API_KEY) $ r = requests.get(url)
df_amznnews_clsfd_2tick = df_amznnews_clsfd[['publish_time','textblob_sent', 'vs_compound']] $ df_amznnews_clsfd_2tick.head()
countries['country'].value_counts()
len([premieScn for premieScn in SCN_BDAY_qthis.scn_age if premieScn < 0])/SCN_BDAY_qthis.scn_age.count()
print(when)
browsers_different_users = (transactions.groupby('browser_id').user_id.count() > 1).sum() $ print('Browsers with same user_id is {0:.2f}% of the database'.format((browsers_different_users/len(transactions))*100))
(apple.index[-1] - apple.index[0]).days
Lab7_RevenueEPS = Lab7_RevenueEPS0.drop('Dividends', axis=1)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print('z-score:', z_score, '\np-value:', p_value)
df_null_acct_name = df[df['LinkedAccountName'].isnull()]
data.loc[(80, slice('20150117','20150417'), $              'put'), :].iloc[:, 0:4]
class Spider(Bug): $     def __init__(self): $         super().__init__(legs=8, name='spider') $ Spider()
sorted_o = df.sort_values(['customer_id', 'created_at_date']).groupby(['customer_id', 'order_id'])['created_at_date'].max() $ interval_c = sorted_o.groupby('customer_id').diff().groupby('customer_id').max().dt.days.rename(('created_at_date', 'diff', 'interval'))
autos.odometer_km.value_counts()
prop_conv = df2[df2['converted'] == 1].shape[0] / df2.shape[0] $ prop_conv
!type data\test1.csv 
(df.loc[df['margin_val'] < 0]).head()
weather['PV_noise'] = np.absolute(np.asarray(noise))
num_of_new_page = len(df2[df2['landing_page'] == 'new_page']) $ num_of_new_page/len(df2)
data_merge = pd.merge(data_volumn, stock_data, how = 'left', left_on = 'stock_name', right_on = 'Symbol') $
df_predictions=pd.read_table(r.url,sep='\t')
exiftool -csv -createdate -modifydate ciscid4/CISCID4_cycle2.mp4 ciscid4/CISCID4_cycle3.mp4 ciscid4/CISCID4_cycle4.mp4 ciscid4/CISCID4_cycle5.mp4 ciscid4/CISCID4_cycle6.mp4 > ciscid4.csv
def generate_returns(prices): $     returns = (prices - prices.shift(1))/prices.shift(1) $     return returns $ project_tests.test_generate_returns(generate_returns)
df2=df.query('misaligned==False')
df = df.dropna() $ df = df[df!=0] $ df = df.dropna() $ df.describe()
df = df[df.PRICE.notnull()]
files = mysdc.FileNames() $ print(*files, sep='\n')
hr["new charttime"]=hr["charttime"]-time_delta $ hr["new realtime"]=hr["realtime"]-time_delta $ bp["new charttime"]=bp["charttime"]-time_delta $ bp["new realtime"]=bp["realtime"]-time_delta
for column in ls_other_columns: $     df_onc_no_metac[column].unique()
notes = pd.read_csv("Z:/notes v2/cohort1_deid_df.csv")
X = df.loc[:,"Timestamp":"Volume_(Currency)"] $ y = df["Weighted_Price"]
cur.execute('DELETE FROM materials WHERE material_id = "SiO2"') $ cur.execute('UPDATE materials SET alpha=1.23E-4, beta=8.910E+11 WHERE material_id = "CaF2"') $ conn.commit()
donald_combined_df = combine_instrument_and_sentiment_dfs( spy_df, $                                                            donald_sentiment_score_df ) $ donald_combined_df.head()
print(autos['odometer_km'].unique().shape) $ print(autos['odometer_km'].describe()) $ print(autos['odometer_km'].value_counts())
df3.index
orders = pd.read_csv('../data/raw/orders.csv') $ products = pd.read_csv('../data/raw/products.csv') $ order_details_prior = pd.read_csv('../data/raw/order_products__prior.csv') $
from datetime import datetime $ pd.Timestamp(datetime(2015,11,11))
plt.hist(ort_avg16, bins=20, align='mid'); $ plt.xlabel('Offensive Rating') $ plt.ylabel('Count') $ plt.title("Histogram of Teams' Offensive Rating, 2016-2017 Season\n");
from sklearn.model_selection import train_test_split $ tips_train, tips_test = train_test_split(tips, test_size=0.2, random_state=123) $ tips_train.shape, tips_test.shape, tips.shape
print (json_data)
temp_long_df = pd.melt(temp_wide_df, id_vars = ['grid_id', 'glon', 'glat'], $                       var_name = "date", value_name = "temp_c") $ temp_long_df.head()
df2.plot(x='station_name',y='observ_ct',kind = 'bar',title = 'Observation Frequency by Station') $ plt.show()
dates =  pd.date_range(dt.datetime(2014,01,01),dt.datetime(2017,01,01))
df.loc[df.public_gists.idxmax()]
average_polarity_2012, count_polarity_2012=polarity_year1('2012') $ average_polarity_2013, count_polarity_2013=polarity_year1('2013') $ average_polarity_2014, count_polarity_2014=polarity_year1('2014') $ average_polarity_2015, count_polarity_2015=polarity_year1('2015') $ average_polarity_2016, count_polarity_2016=polarity_year1('2016') $
np.argmax(sp_rec)
print(ozzy)
pattern = re.compile('AA') $ print(pattern.sub('BB', 'AAbcAA')) $ print(pattern.sub('BB', 'bcAA'))
print d.variables['time']
year16 = driver.find_elements_by_class_name('yr-button')[15] $ year16.click()
df[df['converted'] == 1].user_id.count()/df['user_id'].count()
df_users = pd.DataFrame(user_ids)
neg = df[df['SA'] < 0]
condos.shape
ekos.import_default_data(user_id, workspace_alias = workspace_uuid)
a_list.extend(another_list) $ a_list
stocks.tail()
tweet_archive_enhanced_clean.head()
df.info()
!hdfs dfs -cat 32ordered_results-output/part-0000* > 32ordered_results-output.txt $ !head 32ordered_results-output.txt
np.exp(results.params)
normal_dist = np.random.normal(0, np.std(p_diffs), p_diffs.size) $ plt.hist(normal_dist); $ plt.axvline(diff, color='red')
q2 = pd.Period('2018Q2', freq='Q-JAN') $ q2
!pip install tqdm
df_mas.rating_numerator.value_counts()
data_vi.index
df_clean['tweet_id'] = df_clean['tweet_id'].astype(str)
using_sample = True $
lat = rhum_nc.variables['lat'][:] $ lon = rhum_nc.variables['lon'][:] $ time = rhum_nc.variables['time'][:] $ rhum = rhum_nc.variables['rhum'][1,:,:] $ np.shape(rhum)
pd.crosstab(aqi['AQI Category_eug'], aqi['AQI Category_cg'], normalize='columns', margins=True)
weather.info()
pieces = [df[:2], df[2:8], df[8:]] $ pieces
fig = gamma.E_gsf_surface_plot() $ plt.show()
station_with_highest_observations = stations_and_tobs[0][0] $ station_with_highest_observations
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_axial_brace_saddle[1][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_saddle[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
plot_corr(input_data)
lat = prec_nc.variables['lat'][:] $ lon = prec_nc.variables['lon'][:] $ time = prec_nc.variables['time'][:] $ prec = prec_nc.variables['pr_wtr'][1,:,:] $ np.shape(prec)
store.initialize_library('POSITION') $ position=store['POSITION']
cars.pivot_table(index=['cyl'], columns=['am'], aggfunc=np.mean)
new_df = pd.merge(tweets_df, stock_data, on='Time')
from sklearn.mixture import GaussianMixture $ gmm = GaussianMixture(2) $ gmm.fit(scaled_data) $ labels = gmm.predict(scaled_data) $ labels
dat.isnull().sum() $
print(len(autos.groupby('name').size()))
for i in top_tracks: $     if not top_tracks[i][0].split(",")[0].isdigit(): $         del top_tracks[i] $
dfData.count()
a = (df['landing_page'] == 'new_page').sum() + (df['group'] == 'treatment').sum() $ b = len(df.query('landing_page == "new_page" & group == "treatment"')) $ a -2*b
print(data.iloc[[265631]])
temp_df = pd.DataFrame(highest_tobs_station_yearly) $ temp_df.head()
data = pd.read_csv('Datamart-Export_DY_WK100-500 Pound Barrel Cheddar Cheese Prices, Sales, and Moisture Content_20170829_122601.csv')
df=iris_fromUrl[iris_fromUrl.target=='setosa'] $ df
from sklearn.pipeline import Pipeline $ pipeline = Pipeline([ $         ('features', cv), $         ('model', model)   $     ])
stream = ssc.textFileStream('sample/')
data_df.cycles.unique()
weather = weather[['date', 'max_temp', 'min_temp', 'rain', 'snow_depth', 'ice_pellets']]
rf_pred = m.predict(X_test)
positive_review
loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)
collection.delete_item('AAPL')
merged_test["T"] = (pd.datetime(2017,1,1) - merged_test['first']).astype('timedelta64[W]') $ merged_test['recency'] = (merged_test['last'] - merged_test['first']).astype('timedelta64[W]') $ merged_test['frequency'] = merged_test['validOrders'] - 1 $ merged_test.shape
combined_df['Returned'] = combined_df['Returned'].fillna('No') $ print(combined_df.head(5))
to_be_predicted_Day2 = 25.11647691 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
X.shape
df.isnull().sum()
ind.head(2)
duplicate_rows = df2[df2.user_id == repeated_user_id] $ duplicate_rows
local_sea_level_stations.columns
r = requests.get(data_request_url, params=params, auth=(username, token)) $ r
assert trn_df.shape[0] == train_nrows $ assert val_df.shape[0] == val_nrows $ assert test_df.shape[0] == test_nrows
legHouse = legHouse.fillna(np.nan)
bacteria2.fillna(0)
lobbyFrame.sort_values(by='Start') $ lobbyFrame.head(10)
import time $ current_time = time.time() # returns numbr of seconds passed since 1970 (current Unix time stamp) $ print(current_time)
print(Meter1.Mode) $ Meter1.ModeSet('DC_V') $ print(Meter1.Mode)
for dataset in full_data: $     dataset['IsAlone'] = 0 $     dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1 $ print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())
df_twitter_archive_copy.stage.value_counts()
main = pd.read_csv("1986_2016_seasons_shifted_v1.csv") $ main.shape
cFrame.reset_index(inplace=True)  $ cFrame.head(20)
wine_reviews = pd.read_csv("D:\kagglelearn\kaggledatasets\winemag-data-130k-v2.csv")
weather_all.groupby('Station Name').agg(['mean', 'std'])
precipitation_df = pd.DataFrame(precipitation, columns=['date', 'prcp']) $ precipitation_df.set_index('date', inplace=True) $ precipitation_df.head()
print(tweet_list[0])
unstacked = retention['userId'].unstack(1)
employer_dict, unique_employers = create_employer_dict(df_h1b_nyc_ft.lca_case_employer_name.unique(),0.75)
train.shape[0] + new.shape[0]
Lab7_Equifax.head() $
data = pd.merge(data,users,left_on='user',right_on='id',how='inner') $ data = data[['user','ts','channel','text','clean_text','name']]
def remove_duplicate_index(df): $     df = df.set_index('insert_id') $     return df[~df.index.duplicated(keep='first')] $
multipoint_nego_pivot = pivot_condition_time(multipoint_df, "ecn.multipoint", "negotiation", $                                              ('succeeded','failed','reflected','path_dependent','unstable')) $ multipoint_nego_pivot
np.clip(bag, 0, 1, out=bag)
new_orgs = pd.merge(orgs, org_descs, on = 'uuid')
from timeseries_functions import index_to_datetime, plot_all_df_columns, weekly_resample, plot_series,\ $ plot_series_save_fig, plot_series_and_differences, run_augmented_Dickey_Fuller_test, \ $ plot_autocorrelation, plot_partial_autocorrelation, plot_decomposition
df_combined['point_diff'] = df_combined['home_weighted_points'] - df_combined['away_weighted_points'] $ df_combined.head()
USvideos[1499:1510]
csv_file2 = "countries.csv" $ df3 = pd.read_csv(csv_file2) $ df3.head()
thresh = .98 $ score_thresh = int(ids.score.quantile(thresh)) $ comm_thresh = int(ids.comms_num.quantile(thresh)) $ best_ids = list(ids[(ids.score >= score_thresh) & (ids.comms_num >= comm_thresh)].index)
df1.head()
y_train = y_train.iloc[:nb_samples]
!pip install --upgrade git+https://github.com/GeneralMills/pytrends $ !pip install lxml
month2 = oanda.get_history(instrument_1, $                           start = '2018-2-1', $                           end = '2018-3-1', $                           granularity ='M10', $                           price = 'A')
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
Results_ZeroFill.to_csv('soln_logistic_zerofill.csv', index=False)
data.LRank[list(map(lambda x: isnotfloat(x), data.LRank))]
bsfr.describe()
print("All Tweets: {0} | Users: {1}".format(len(matthew), matthew.user.nunique())) $ print("Tweets in the Matthew 92 Collection: ", len(matthew.query('matthew92'))) $ print("Users in the Matthew 92 Collection: ", matthew.query('matthew92').user.nunique())
from pymongo import MongoClient
columns = [m.key for m in Measurement.__table__.columns] $ print(columns)
gender_dummies=pd.get_dummies(df.gender,prefix='gender').iloc[:,0:2] $ gender_dummies
df_usa = df1[(df1['Area'] == "United States of America")].set_index('Year') $ df_usa.head(5)
broadband.info()
with open('image-predictions.tsv',mode='wb')as file: $     file.write(response.content)
labels=list(D.keys()) $ values=list(D.values()) $ trace=go.Pie(labels=labels,values=values) $ py.iplot([trace])
f_ip_os_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_os_clicks"), header=True) $ print('Found %d observations.' %f_ip_os_clicks.count())
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import datetime as dt
def add_percentiles(df, quantiles = [25,75], look_back = 365): $     data_frame = df.copy() $     return data_frame
to_be_predicted_Day3 = 26.69278823 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
{ f'Square of {x}': x*x for x in range(1, 6)}
n_old = df2['landing_page'].value_counts()[1] $ n_old
pd.Series(sales, index=pd.date_range(start='2018-01-01', periods=len(sales), freq='T'))
load2017['actual'] = load2017['actual'].astype(int)
vulnerability_type_histogram = cved_df.groupby(by=['vulnerability_type','cwe_id'])['cve_id','n_exploits'].count() $ print(vulnerability_type_histogram) $ vulnerability_list = np.unique(cved_df['vulnerability_type']) $ vulnerability_by_month = cved_df.groupby(by=['vulnerability_type','month'])['cve_id'].count() $
df3 = df2[df2['group'] == 'treatment'] $ num_group = df3.user_id.count() $ num_converted = df3[df3['converted'] == 1].user_id.count() $ p_new_actual = num_converted / num_group $ p_new_actual
pprint(lda_model.print_topics()) $ doc_lda = lda_model[corpus]
norm.ppf(1-0.05)
plt.title('GC_MARK_MS_content') $ aggregated_content.plot(kind='bar', figsize=(15, 7))
brand_mean_km = {} $ for brand in brands: $     mean_km = autos.loc[autos["brand"]==brand,"odometer_km"].mean() $     brand_mean_km[brand] = mean_km $ brand_mean_km
start_mask =  df_goog.index >= pd.to_datetime('2014-07-01') $ end_mask   =  df_goog.index <= pd.to_datetime('2014-12-01') $ df_goog[(start_mask) & (end_mask)] $
print(df,'\n') $ s = pd.Series([1,3,5,np.nan,6,8],index=dates).shift(2) # shift 2 rows down (2 NaN at top) $ print(s,'\n') $ print(df.sub(s,axis='index'))
(taxiData2.Tip_amount < 0).any() # This Returns False, proving we have successfully changed the values with no negative
Top15=answer_one() $ Top15['PopEst'] = (Top15['Energy Supply'] / Top15['Energy Supply per Capita']).astype('float64') $ Top15['PopEst']=Top15['PopEst'].map('{:,}'.format) $ Top15['PopEst']
print("2 sample T-test for sbp: ", ttest_ind(sbp_dbp_means.sbp_no_pill, sbp_dbp_means.sbp_taken_pill)) $ print("2 sample T-test for dbp: ", ttest_ind(sbp_dbp_means.dbp_no_pill, sbp_dbp_means.dbp_taken_pill))
bmp_series = pd.Series(mean_prices) $ pd.DataFrame(bmp_series, columns=["mean_mileage"])
browser.click_link_by_id('full_image')
cityID = 'dd3b100831dd1763' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_Orleans.append(tweet) 
r= requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31") $
geo_db.dtypes
classifier_tfidf = train_classifier(X_train_tfidf, y_train)
df_usa = df_usa.rename(columns={'Gross Domestic Product (GDP)':'GDP', 'National Rainfall Index (NRI)':'NRI', 'Population density':'PD', 'Total area of the country':'Area', 'Total population':'Population'})
df.to_excel("../../data/stocks_msft.xlsx", sheet_name='MSFT')
import matplotlib.pyplot as plt $ plt.imshow(input_image) $ plt.show()
forecast_data.describe()
ts.head()
tweet_archive.info()
master_list[master_list['Count'] < 5]['Count'].describe()
data.plot(kind='scatter', x='TV', y='sales') $ plt.plot(X_new, preds, c='red', linewidth=2)
bad_dtype_df = pd.read_csv('multi_col_lvl_output.csv', header=[0, 1], index_col=[0, 1, 2, 3, 4, 5], $                             skipinitialspace=True).head(3) $ display(bad_dtype_df) $ display(index_level_dtypes(bad_dtype_df))
uniqueusersdf2[uniqueusersdf2==2]
pip install mxnet $ pip install numpy $ pip install matplotlib $ pip install pandas $
df.filter(a_sevenPointSeven).count()
cmp(logs)
pd.DataFrame(l_norm,columns=['score']).plot.hist() #1 user has 14.000 stars $ plt.xlabel('scores', fontsize=14) $ plt.ylabel('number of tests', fontsize=14) $ plt.title('UR scores test', fontsize=17) $ plt.savefig('ur_score_test.png')
from scipy.stats import norm $ norm.cdf(z_score),norm.ppf(1-(0.05/2)) $
df1=df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']] $ df1.head() $
sentiment_data = pd.DataFrame() $ sentiment_data = pd.read_csv("sentiment_output.csv") $ sentiment_data = sentiment_data[["class","p_neg","p_pos"]] $ sentiment_data[0:2]
caps2_output_masked
cid = areas_dataframe.company_id[0]
bottom10 = tt_final.sort_values(['rating_numerator'], ascending=True,).head(10) $ bottom10[['expanded_urls', 'rating_numerator', 'jpg_url', 'text', 'all_p']]
url_hemispheres = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(url_hemispheres)
vectorizers = cvec.fit_transform(xprep.billtext) $ df_vec = pd.DataFrame(vectorizers.todense(), columns = cvec.get_feature_names()) $
data = pd.read_hdf('rel_data.h5', 'behav') $ data.head()
start_from_user = None  # i.e. date(2016, 1, 1) $ end_from_user = None  # i.e. date(2016, 1, 31)
sentiment_df = pd.DataFrame(sentimentList) $ sentiment_df.head()
from textblob import TextBlob $ df['Tweet'] = df['Tweet'].str.decode("utf-8")
check_measurements = session.query(Measurements).first()
against.groupby(['contributor_firstname','contributor_lastname'])['amount'].sum().reset_index().sort_values('amount', ascending = False)
df2['user_id'].count()
positive.head()
dates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H') $ dates
(p_diffs>p_diff).mean()
consumerKey = 'XXXXXXXXXXXXXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
df.head(1)
df_all_wells_basic.head()
df_uro.dtypes.unique()
url = 'https://mars.nasa.gov/news/'
df[(df['county_number'].isnull())&(df['county'].isnull())].head()
data.loc[data.expenses < 150, 'expenses'] = np.NaN
train.dropna(inplace=True) $ test.dropna(inplace=True)
import pandas as pd $ from sqlalchemy import create_engine $ engine = create_engine('postgresql://postgres:@localhost/tidyme')
api = twitter.Api(consumer_key=consumer_key, consumer_secret=consumer_secret, $                   access_token_key=access_token, access_token_secret=access_token_secret)
import pandas as pd $ import numpy as np $ from sklearn.model_selection import train_test_split $ import datetime
data.to_csv("raw/1_cleaning_translated.csv", encoding="utf-8")
df1.plot( kind='line', x='timestamp', y='Polarity',title='Polarity by date') $ axes = plt.gca() $ plt.xticks(rotation='vertical', fontsize=11) $ plt.show()
df.tail(2)
trips_data['start_date'] = pd.to_datetime(trips_data['start_date']) $ trips_data['weekday'] = trips_data['start_date'].dt.weekday
nr_rev ='the tacos were great the carne asada was very yummy and the cheese was\ $ very stringy. yum. would go back great tacos and cold beers' $ vectorizer.transform([nr_rev]) $ M_NB_model.predict_proba(vectorizer.transform([nr_rev]))
import pandas $ import requests $ import datetime $ import bs4
df_countries = pd.read_csv('countries.csv') # reading 'countries.csv' $ df_countries.shape[0] == df2.shape[0] # checking if they have the same number of rows
df_vow['Open'].unique()
model.show_topics(10)
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred_clf))
train.head()
print(a) $ print(['dog'] + a)  # + can be used to concanetenate lists; implemented by list.__add__ $ a.append('dog')  # append() can be used for concatenating elements $ print(a)
frame.loc[('b', 2), 'Colorado']
print fs.list() $ print fs.find({"filename" : "scansmpl.pdf"}).count()
temp = list(zip(*df['Ranking Full URL on Sep  1, 2017'].map(get_url_parts))) $ for i, c in enumerate(columns): $     df[c] = temp[i]
set(df["NAME"])
model.most_similar([trial])
print("\nClassification Report:\n",classification_report(y_test, y_pred)) $
len(tweets)
contractor_clean.loc[contractor_clean['contractor_id'].isin([382,383,384,385,386,387]), $                      'contractor_bus_name'] ='Cahaba Government Benefit Administrators, LLC'
df_bthlst["booth_id"] = list(df_bthlst['client_display_name'\ $                                       ].apply(lambda x: x.split("|")[0].split(".")[-1]).astype(int)) $ df_bthlst["md_id"] = list(df_bthlst['MD_id'].apply(lambda x: abs(int(x.split("MotherDairy")[1]))))
click_condition_meta.info()
print(pd.merge(All_tweet_data, Imagenes_data_v2, on='tweet_id', how='inner').shape) $ print(pd.merge(twitter_data_v2, tweet_data_v2, on='tweet_id', how='inner').shape)
yt.get_featured_channels(channel_ids, key)
import matplotlib.pyplot as plt $ import numpy as np #library for math operations $ import pandas as pd #this is how I usually import pandas $ import sys #only needed to determine Python version number $ import matplotlib #only needed to determine Matplotlib version number
prob_temp = df2.query("group == 'control'")["converted"].mean() * 100 $ print("Probability if user in control group: {}%".format(round(prob_temp, 2)))
pnew = df2.converted.mean() $ pnew
twitter_ar['name1'] = twitter_ar.text.apply(lambda row: findname(row))
date.strftime('%A')
def sentiment_finder_partial(comment): $     analysis = TextBlob(comment) $     return analysis.sentiment.polarity
df.isnull().sum()
variables_list = ["country_destination", "action_reviews", "action_social_connections"] $ subset_sessions_summary = sessions_summary[variables_list]
data = brazil.join(biology, lsuffix='_brazil', rsuffix='_biology') $ data.plot()
temperature = 6 $ humidity = 6 $ proc_string = "CALL tddb_00.Weather_Log_Create({0}, {1})".format(temperature, humidity) $ cursor =  session.cursor() $ cursor.execute(proc_string);
joined=join_df(train,store,"Store") $ joined_test=join_df(test,store,"Store") $ sum(joined['State'].isnull()),sum(joined_test['State'].isnull())
obj.sort_values()
obs_diff = conv_giv_ctrl - conv_giv_trt; $ obs_diff
%matplotlib inline $ df.resample(rule='M').count().plot(y='Complaint Type')
g_kd915 = kd915[(kd915.state != 'live') & (kd915.state != 'suspended') & (kd915.state != 'canceled')].groupby(['blurb']) $ kd915_filtered = g_kd915.filter(lambda x: len(x) <= 1).sort_values(['blurb','launched_at'])
df["screen_name"].value_counts().head()
rollcorr_daily.plot() $ plt.show()
dfTemp
full_dataset.shape
df1 = df.drop(['Area Id', 'Variable Id', 'Symbol'], axis=1) $ df1
print(loan_stats["issue_d"].head(rows=2)) $ print(loan_stats["issue_d"].year().head(rows=2))
frauds = ['fraudster_event', 'fraudster', 'fraudster_att'] $ df['frauds'] = [True if i in frauds else False for i in df['acct_type']] $
temp_df.shape
flight6.count()
steemit_urls = unique_urls[(unique_urls.domain == domain)] $ steemit_urls.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
p_old = p_new # this is the same based on the requirements above
coin_mean = coins.mean() $ coin_mean
by_area['AQI Category'].value_counts().unstack()
boat = df_titanic['boat'] $ print(boat.describe())
pax_raw.columns = [x.lower() for x in pax_raw.columns]
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, inspect $ from sqlalchemy.sql import func
autos['date_crawled'].str[:10].value_counts(normalize=True, $                                            dropna=False).sort_index()
stations = session.query(func.count(distinct(Measurement.station))).all() $ print(f"Number of stations: {stations}")
zip_1_df.rename(columns = {0:"Count"},inplace=True) $ zip_2_df.rename(columns = {0:"Count"},inplace=True) $ zip_1_sns.rename(columns = {0:"Count"},inplace=True) $ zip_2_sns.rename(columns = {0:"Count"},inplace=True)
r_top10_mat_na = returns_calc(price_mat[coins_top10], fill_na_0=False) $ fin_coins_r = pd.concat([fin_r_monthly.loc[start_date:end_date], #financial assets $            r_top10_mat_na.loc[start_date:end_date], # top 10 crypto assets $                          fundret[start_date:end_date] # fund return $                         ], axis=1, join='inner')
twitter_archive.sample(4)
weather.boxplot()
df4 = df2[df2['group']=='treatment']
autos['price'].head()
df.head(8)
len(df2[(df2['landing_page']=='new_page')])/df2.shape[0]
fast_scatter_xs = fuel_xs.get_values(filters=[openmc.EnergyFilter], $                                      filter_bins=[((0.625, 20.0e6),)], $                                      scores=['(scatter / flux)']) $ print(fast_scatter_xs)
print trans_counts
tokendata = tokendata.fillna(0)
and_list1 = df.query("group=='control'& landing_page=='old_page'").index
crime_df['Date'] = pd.to_datetime(crime_df['Date'], format='%m/%d/%y %H:%M') $ crime_df['Weather_Datetime'] = crime_df['Date'] $ crime_df['Updated On'] = pd.to_datetime(crime_df['Updated On'], format='%m/%d/%y %H:%M') $ print "Crime data dates converted to datetime." $ crime_df.head()
print('Before deleting out of bounds game rows:',injury_df.shape) $ injury_df = injury_df[(injury_df['Date'] > '2000-03-28') & (injury_df['Date'] < '2016-10-03')] $ print('After deleting out of bounds game rows:',injury_df.shape)
train['question_dt'].describe()
sentences = sent_tokenize(bios['Grover Cleveland']) $ sentences
tsla_tuna_neg_cnt = tsla_tuna_neg.count()*100 $ print "{:,} users were only active on {} ({:.2%} of DAU)"\ $       .format(tsla_tuna_neg_cnt, D0.isoformat(), tsla_tuna_neg_cnt*1./dau)
evaluation_1 = api.create_evaluation(ensemble_1, test_dataset) $ api.ok(evaluation_1)
n_final_participants = pax_raw.seqn.nunique() $ print(f'Total participants remaining in analysis set: {n_final_participants}')
tree_c_features_test = c_features_test.apply(class_le.fit_transform)
tree_features_df = pd.read_csv('../data/tree_images.csv', sep='|', index_col=0) $ tree_features_df.head()
rain = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date > last_year).\ $     order_by(Measurement.date).all() $ print(rain)
tweet_df.total.value_counts()
d={'c1':('A','B','C','D'),'c2':np.random.randint(0,4,4)} $ pd.DataFrame(d)
df2[df2['user_id']==773192]
!wget https://cdn.pixabay.com/photo/2017/03/21/17/27/daffodils-2162825_1280.jpg
d + pd.tseries.offsets.YearEnd(month=6)
grouped = df.groupby(['product_type', 'state'])['price_doc'].size() $ grouped
public_table = events[['repo_id', 'user_id', 'login', 'created_at', 'archive_id']][events['type'] == 'PublicEvent']
vi = vio2016.groupby(["business_id", "date"]).count().reset_index() $ pd.merge(vio2016, vi, on=["business_id", "date"]) $ vi = vi.iloc[:, :3] $ vi $ vio2016 = pd.merge(vio2016, vi, on=["business_id", "date"])
df2.country.unique()
df.rename(columns={"PUBLISH STATES":"Publication Status","WHO region":"WHO Region"},inplace=True) $ df.head(2)
df_user_extract_copy.head()
df2.query('converted == 1').user_id.count()/df2.user_id.count()
sns.violinplot(calls_df["length_in_sec"],orient='v')
print(np.info(np.linspace))
autos["registration_year"].describe()
tweets_clean.info()
start_date = '2017-12-01' $ end_date = '2017-12-20' $ config_root = 'C:\\Users\\A130893\\Git Hub\\Swag\\static\\model_configuration' $ model_name = 'wa_mm.yaml'
X=users.drop(['object_id', 'creation_time', 'name', 'invited_by_user_id', 'last_session_creation_time'], axis = 1) $ X.head()
crimes.PRIMARY_DESCRIPTION.value_counts()
plt.savefig(str(output_folder)+'NB01_5_NDVI02_'+str(cyclone_name)+'_'+str(location_name)+'_'+time_slice02_str)
columns = ['SchoolHoliday', 'StateHoliday', 'Promo']
gc.collect()
df_twitter_copy = df_twitter_copy[df_twitter_copy.retweet_count.notnull()]
lr = 1e-2 $ learn.fit(lr, 20, cycle_len=1, use_clr=(10,10))
train_set = [(find_features(tokens, word_features), cat) for tokens, cat in zip(X_train.Text_Tokenized, y_train)] $ test_set = [(find_features(tokens, word_features), cat) for tokens, cat in zip(X_test.Text_Tokenized, y_test)]
test_float['transporte'] = 0 $ test_float.loc[test_float.description.str.contains('transporte|transporte publico', na=False), 'transporte'] = 1 $ test_float.transporte.value_counts()
print("n_bytes = ", reader.n_bytes * 1E-9, "GB") $ print("n_rows = ", reader.n_rows) $ print("columns = ", reader.columns)
print("KS : {}\nR Pearson : {}\nSpearman : {}".format(ks,r,spearman))
df['converted'].sum()/df.shape[0]
banks[0]
autos.columns
recommendationTable_df = ((genreTable*userProfile).sum(axis=1))/(userProfile.sum()) $ recommendationTable_df.head()
print(type(hmeq)) $ print(type(df_hmeq_card)) $
df["DISPOSITION_TYPE"].value_counts()
for file in s3_key_edits: $     os.environ["s3_key"] = "s3://wri-public-data/" + file $     os.environ["gs_key"] = "gs://resource-watch-public/" + file $     !gsutil cp $s3_key $gs_key
hrefs = soup.find_all('a', href='http://www.iana.org/domains/example') $ hrefs
df_lm.filter(regex='tch_view_assig|last_month').boxplot(by='last_month', figsize=(10,10),showfliers=False)
df2.drop(1899, inplace=True)
x_normalized.columns = normalized_columns
gs_rfc_over.score(X_test, y_test_over)
number_of_commits = len(git_log) $ number_of_authors = len(git_log.dropna().author.unique()) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
n_new = df2[df2['group'] == 'treatment'].shape[0] $ print(n_new)
myTweets.to_sql(con=engine,name='tweetFeatures',index=False)
intersection = set(coins_mcap_today[50:].index) & set(coins_infund) $ len(intersection)
airbnb_df['listed_year_month'] = airbnb_df['date_listed'].dt.to_period('M')
df_btc['created_at'] = pd.to_datetime(df_btc['Date'])
print("Last modified on: ", time_Now())
print(len(df[df.Address == 'Not associated with a specific address'])) $ df.Address.replace('Not associated with a specific address', np.nan, inplace=True)
all_feature = pdf("com.mycompany.airlineapp.feature.cmallfeat.CustomerAllFeatureTreat")
abc
logit_mod = sm.Logit(df3['converted'], df3[['intercept', 'CA', 'UK']]) $ results = logit_mod.fit() $ results.summary()
local_edit_keys
%run ./'Nashville Accident Analysis -- Cleanup'.ipynb
len(cfs_df)
IMDB_dftouse_dict = {feature: IMDB_dftouse[feature].values.tolist() for feature in IMDB_dftouse.columns.values} $ fp = open("IMDB_dftouse_dict.json","w") $ json.dump(IMDB_dftouse_dict, fp) $ fp.close()
import graphlab
data['Created Date'].head()
from sklearn.model_selection import train_test_split $ train, test = train_test_split(loans_df, test_size=0.30, random_state=1)
wrd_api
new_page_converted = np.random.choice([1,0],size=nnew,p=[pmean, (1-pmean)]) $ new_page_converted.mean()
import foursquare
run txt2pdf.py -o"2018-06-19 2015 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 FLORIDA HOSPITAL Sorted by discharges.txt"
pumpkin = data["author"] == "PumpkinDevourer" $ pd.options.display.max_colwidth = 100 $ data[pumpkin].sort_values(by="num_comments", ascending=False)[0:9][["author", "num_comments", "score", "title"]] $
plt.hist(p_diffs); $ plt.title("Histogram of P_diffs");
github_data.shape
lin_clf = LinearSVC(random_state=42) $ lin_clf.fit(X_train_scaled, y_train)
n_new = (df2['landing_page'] == 'new_page').sum() $ n_new
df2.converted.sum()/df2.count()[0]
job_requirements.index
Dxs_info['A41']
autos['price'] = (autos['price'].str.replace('$','').str.replace(',','').astype(int)) $ autos['odometer'] = (autos['odometer'].str.replace('km','').str.replace(',','').astype(int)) $ autos.rename({'price':'price_dollars'}, axis=1, inplace=True) $ autos.rename({'odometer':'odometer_km'}, axis=1, inplace=True)
new_page_converted = np.random.choice([0,1],size = nnew,p=[1-rate_pnew_null,rate_pnew_null]) $ print(len(new_page_converted))  
document = [i for i in tweets['text']]
features.head(2)
df_new['US*treatment']=df_new['US']*df_new['treatment']
df.isnull().values.any() $
rddScaledScores = RDDTestScorees.map(lambda entry: (entry[1] * 0.9)) $
pd.DataFrame({'Yes': [50, 21], 'No': [131, 2]})
df1.dtypes
stages_with_naive_bayes = stages + [nb]
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1 - convert_mean), convert_mean])
new_page_converted = np.random.choice([1, 0], size = nnew, p = [p_mean, (1-p_mean)]) $ print(len(new_page_converted))
df.head
plt.scatter(rets.SCTY,rets.TAN)
 result = db.profiles.create_index([('user_id', pymongo.ASCENDING)], $                                   unique=True) $  sorted(list(db.profiles.index_information()))
import pandas as pd
assert X_train.index.all() == X_train_df.index.all()
print(q6c_answer)
new_page_converted = np.random.binomial(n_new,p_new,10000)/n_new $ print (new_page_converted)
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "CNN"]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "CNN"]["tweet_vader_score"],fit_reg=False,marker = "^",scatter_kws={"color":"purple","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (CNN News)",fontsize = 12) $ plt.savefig('Sentiment_CNN.png')
display(prepared)
data_2018 = data_2018.rename(columns={'Unnamed: 0':'time'})
data['SA'] = np.array([ analyze_sentiment(tweet) for tweet in data['Tweets'] ]) $ display(data.head(10))
data=data.dropna(subset=['lat-lon']).reset_index(drop = True)
data.T
fwd.head()
autos = autos[autos["price"].between(1,351000)] $ autos["price"].describe()
new_page_converted - old_page_converted
filt_M=building_pa_prc_shrink.permit_number.str.contains('M')
print('raw time value: {}'.format(plan['plan']['date'])) $ print('datetime formatted: {}'.format(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(plan['plan']['date']/1000))))
time_cols = ['approx_payout_date', 'event_created', 'event_published','event_start', 'user_created']
prcp_df.plot() $ plt.xlabel("Date") $ plt.ylabel("Precipitation") $ plt.savefig("precipitation_analysis.png") $ plt.show()
np.exp(-2.0300), np.exp(0.0506), np.exp(0.0408)
c_denom = [0, 2, 7, 11, 15, 16] $ archive_df[archive_df.rating_numerator.isin(c_denom)][['text', 'rating_numerator', 'rating_denominator']]
df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique()
df_combined = pd.merge(df_clean, df_predictions_clean,on='tweet_id', how='inner') $ df_combined = pd.merge(df_combined, df_tweet_clean,on='tweet_id', how='inner') $
match_results = pd.read_csv("data/afl_match_results.csv") $ odds = pd.read_csv("data/afl_odds.csv") $ player_stats = pd.read_csv("data/afl_player_stats.csv")
importances = final_xgb.get_fscore() $ importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())}) $ importance_frame.sort_values(by = 'Importance', inplace = True) $ importance_frame.plot(kind = 'barh', x = 'Feature', figsize = (8,12), color = 'orange')
y_test_advanced, x_test_advanced = for_analysis(main, model_9_ratio_vars) $ y_train_advanced, x_train_advanced = for_analysis(main, model_9_ratio_vars)
uber_15["hour_of_day"] = uber_15["Pickup_date"].apply(lambda x: getHour(x)) $ uber_15.head()
lower_vars = sample.variable.str.lower()
pgh_311_data.groupby("REQUEST_TYPE")['REQUEST_TYPE'].count()
hobbies_learned.findall(r"<\w*> <\w*> <or> <other> <\w*> <\w*>")
table_store.ix[5:]
train_pos = train_sample.filter(col('is_attributed')==1) $ n_pos = train_pos.count() $ print("number of positive examples:", n_pos)
!less 'data/taxi+_zone_lookup.csv'
df_input_clean.filter("`Resp_time` <= 0").groupBy("Resp_time").count().sort(desc("count")).show(50)
metrics.v_measure_score(labels, km.labels_)
image_predictions = re.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
df.groupby('userid').max() $
from sklearn import naive_bayes
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new, n_old],alternative='larger') #larger one-sided $ z_score, p_value
df_state_votes.sort_values("hill_trump_diff", ascending=False).head(10)
sourcetake = pd.concat(sourcet) $ sourcetake.reset_index(inplace=True) $ sourcetake.rename(columns={'level_0':'systemid'},inplace=True) $ sourcetake.set_index(['systemid','Year'],inplace=True) $ sourcetake.drop(['level_1','Measuring','MeasuringMethod','Unnamed: 15','Mea','Meth','Ann'],axis=1,inplace=True)
intersections_final_for_update_no_dupe = intersections_final_for_update[~intersections_final_for_update.index.duplicated(keep='first')]
(autos["date_crawled"].str[:10] $  .value_counts(normalize=True) $  .sort_index(ascending=True) $ )
titanic[filter.ticket & filter.cabin].head()
menu_dishes_about_vectorizer = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,1))
import csv
keep_day_threshold = 4 $ n_user_days = n_user_days.loc[n_user_days >= keep_day_threshold].reset_index() $ n_user_days = n_user_days[['seqn']]
authors_count = db.get_sql(sql)
sentim_analyzer = SentimentAnalyzer() $ allNeg = sentim_analyzer.all_words([mark_negation(doc) for doc in train['trainTuple']])
prediction_modeling2.unpersist()
twitter_ar.drop(['retweeted_status_id','source','rating_numerator','rating_denominator','timestamp','in_reply_to_status_id','in_reply_to_user_id','retweeted_status_user_id','retweeted_status_timestamp'],axis=1,inplace=True)
dtypes={'date':np.str,'store_nbr':np.int64,'class':np.str,'family':np.str,'sum_unit_sales': np.float64,'no_items':np.float64,'no_perishable_items':np.float64,'items_onpromotion':np.float64} $ parse_dates=['date'] $ class_merged = pd.read_csv('class_merged.csv', dtype=dtypes,parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
print 'Collection osm size = ', db.command("collstats","osm")['storageSize']/1024, 'KB' $ print 
live_df = k_var[k_var.state == 'live'] $ live_df.to_csv('/Users/auroraleport/Documents/LePort_git/07_15_live.csv')
autos['price'].unique().shape
url = form_url(f'organizations/{org_id}') $ response = requests.get(url, headers=headers) $ print_body(response, skip_audit_info=True)
measure.columns = ['station', 'date', 'precip', 'tobs']
df["past_rides"] = df["num_rides"].apply(lambda lst: len(lst)) $ df["past_cancellations"] = df["num_rides"].apply(lambda lst: sum(lst)) $ df["past_percent_cancelled"] = df["past_cancellations"] / df["past_rides"]
data1[0]=pd.to_datetime(data1[0],unit='s') $ data1=data1.set_index(0,drop=True) $ data1_new= data1[1].resample('60Min', how='ohlc') $ data1_new['volume'] = data1[2].resample('60Min').sum() $
DataDescription = {}
X_train = train_set[:,lookforward_window:].copy() $ y_train = train_set[:,:lookforward_window].copy() $ X_test = test_set[:,lookforward_window:].copy() $ y_test = test_set[:,:lookforward_window].copy()
queensland_data = pd.read_csv('../data/2-data-collection/2013_Queensland_floods-tweets_labeled.csv') $ topic_preds = [topics[p] for p in np.argmax(lda.transform(count_vectorizer.transform(queensland_data[' Tweet Text'])), axis=1)]
y3, X3 = patsy.dmatrices('DomesticTotalGross ~ Constant + Budget + G + PG + PG13 + R', data=df, return_type="dataframe") $ model = sm.OLS(y3, X3, missing='drop') $ fit3 = model.fit() $ fit3.summary()
df.printSchema()
df5 = pd.concat([df3, Senate], axis=1, join='inner') $ df5.columns = ['tweet_sentiment', 'Senate_sentiment']
df_convs_master = df_totalConvs_day                                                $ years_unique = df_convs_master['year'].unique() $ years_unique
df_train = pd.read_csv('../data/train.csv') $ df_test = pd.read_csv('../data/test.csv')
df[df['Complaint Type'] == 'Street Condition']['Descriptor']
sns.barplot(x='frequency',y='word',data=words_df.head(15))
data = data[data.price_aprox_usd!=0]
twitter_ar.head(2)
a_df.to_csv('a_tweets.csv') $ b_df.to_csv('b_tweets.csv')
affair_children = pd.crosstab(data.children, data.affair.astype(bool)) $ affair_children
to_be_changed = df2[df2['group']=='treatment'].index $ df2.loc[to_be_changed, 'ab_page'] = 1 $ df2[['intercept', 'ab_page']] = df2[['intercept', 'ab_page']].astype(int)
df.shape
cleaned2['tc_sentiment'] = cleaned2['text'].apply(lambda t : tc.get_tweet_sentiment(t))
grid_props = [["upw.sy",0],["upw.vka",1]] $ for k in range(m.nlay): $     grid_props.append(["upw.hk",k]) $
s = pd.Series([1, 2, 3]) $ s.loc['Animal'] = 'Bears' $ s
len(have_seen_two_versions)
f_counts_week_os.show(1)
year_prcp#check
csvDF.head(10) $ csvDF.tail() $ csvDF.tail(10)
y_test_pred = model.predict(X_test.as_matrix()) $ utils.metrics(y_test, y_test_pred)
df_new = df2.query('group == "treatment"') $ n_new = df_new.shape[0] $ n_new
imin = amount.idxmin()
df_merge.head()
len(mrtrumpdf)
amaz_df.head()
len(set(ioDF.group_id_x))
sum(contractor.address1.isnull())
weekly = data.resample('W').sum() $ weekly.plot(style=[':', '--', '-']) $ plt.ylabel('Weekly bicycle count')
doc_input = Input(shape=(DOC2VEC_SIZE,), name='doc_input') $ hidden = Dense(NN_HIDDEN_NEURONS, activation='relu', name='hidden_layer')(doc_input) $ softmax_output = Dense(NN_OUTPUT_NEURONS, activation='sigmoid', name='softmax_output')(hidden) $ model = Model(input=doc_input, output=softmax_output) $ model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', 'fbeta_score', theano_coverage_error])
jobs.loc[(jobs.MAXCPUS == '600') & (jobs.GPU == 0)].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
dfTemp=transactions.merge(users, how='inner',left_on='UserID',right_on='UserID') $ dfTemp
%matplotlib $
data.drop(columns=['affairs'],axis=1,inplace=True) # after binary target variable is created we delete affairs variable
print(trump.shape,schumer.shape)
import seaborn as sns $ import matplotlib as plt $ %matplotlib inline
commits = pd.read_pickle('data/pickled/commits.pkl') $ commits.head()
active.head()
i = np.random.randint(x.shape[0]-1) $ m_star = m_star + 2*learn_rate*(x[i]*(y[i]-m_star*x[i] - c_star)) $ c_star = c_star + 2*learn_rate*(y[i]-m_star*x[i] - c_star)
data_dir = 'data/preview/'
prices
print(df.size) $ df.size == df.shape[0] * df.shape[1]
n_old = ab_file2[ab_file2['landing_page']=='old_page'].shape[0] $ print(n_old)
states = pd.DataFrame({'population': population, $                        'area': area}) $ states
g.reset_index(inplace=True) $ g.head(2)
tt_json_clean.id = tt_json_clean.id.astype(str)
predictions = [model.predict(start=start_date, end=end_date, dynamic=True) for model in models]
sns.lmplot(x="apparentTemperatureHigh", y="polarity", data=twitter_moves,lowess=True,size=8,aspect=1.5)
def Twitter_setup(): $     auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET) $     return auth
logit_mod = sm.Logit(df_total.converted, df_total[['intercept','UK','US']]); $ results = logit_mod.fit() $ results.summary()
all_mentioned_df.sort_values(['mentioning_count', 'mention_count'], ascending=False).head(25)
tweet_archive_clean.info()
np.sort(df2.date.unique())
df1.head()
print('Change the names of multiple columns permanently') $ df.rename(columns = {'PUBLISH STATES':'Publication Status','WHO region':'WHO Region'},inplace=True) $ df.head(2)
pros.plot()
speakers.whotheyare = speakers.whotheyare.apply(lambda x: re.sub("<.*?>", "", x))
unsorted_df.sort_index(axis=1)
node_names[0:5]
list = [1,3,4,30] $ list.append(21) $ print(list)
gifs = [".gifv", ".gif", "gfycat"] $ images = ["i.redd", "https://imgur.com"]
click_condition_meta.loc[click_condition_meta.dvce_type == "Unknown"].head(2)
twitter_archive_df.describe()
df_unique_users_control = df2[df2['group'] == 'control'] $ n_unique_users_control = len(df_unique_users_control) $ n_conversion_control = len(df_unique_users_control[df_unique_users_control['converted'] == 1]) $ probability_control = n_conversion_control/n_unique_users_control $ print ("The probability of an individual converting from the control group: {:.4f}".format(probability_control))
month_bins = [0,6,12] $ datAll['month_rng'] = pd.cut(datAll['month'],month_bins) $ datAll['half_year'] = np.where(datAll['month_rng']=="(0, 6]","first","second") $ datAll['half_year'] = datAll['half_year'].astype(str)
coinbase_btc_eur_cols = ['Time', $                    'Coin_price_EUR', $                    'Coin_volume', $                     ] $ coinbase_btc_eur = pd.read_csv('coinbase_BTC_EUR.csv', header=0, names=coinbase_btc_eur_cols, skiprows=2000000)
print(a) $ a.remove('dog')  # Remove first matching instance of element $ print(a) $ del a[-1]  # Remove element at index; implementedby list.__del__
print(min_max_dict_model.name, min_max_dict_model.description, min_max_dict_model.version, sep = '\t')
df_archive_clean.info()
print(fruits['bananas'] + 2) $ print(fruits[['apples', 'oranges']] * 2)
texasurl = 'https://pubs.usgs.gov/sim/3365/tables/sim3365_table4G.xlsx' $ new_texas_city = pd.read_excel(texasurl,skiprows = [0, 1, 2], skip_footer = 2, parse_cols=[0,2])
final_topbikes['Distance'].count() * 1.5
n=13 $ df['Close'].pct_change(n) #n timeperiods percent change
import pprint $ pprint.pprint(pipeline.get_fit_info(X_train)[0])
dfAbes
print(json.dumps(res_json, indent=4))
spencer_bday_time = dt.datetime(1989, 4, 25, 16, 33, 5) $ seconds_to_30 = (spencer_bday_time + thirty_years - now).seconds $ print("Spencer will be 30 in {} seconds".format(seconds_to_30))
cp311 = pd.read_csv('cp311.csv')
results_1.summary()
horror_readings=m1[m1['category_one'] =='horror']
import re
df[df.client_event_time >= datetime.datetime(2018,4,2,0,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
gender = talks_train['speaker_ids'].apply(speakerinfo)
tt_json.info()
accuracy = accuracy_score(y_test, predictions) $ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
xt = pd.to_datetime(df.created_at) $ df.index = xt $ del df['created_at'] $ df.index.names = ['Date']
df1_clean
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt
SCC_MSP_year.reset_index(level=0,inplace=True) $ SCC_MSP_year.columns = ['Year','Median_Sales_Price'] $ SCC_MSP_year
clean_en_test_df = pd.concat([time_df, clean_en_test_df], axis=1)
driver.title
sum(df_h1b_nyc_ft.pw_1*df_h1b_nyc_ft.total_workers)/sum(df_h1b_nyc_ft.total_workers)
df.registerTempTable("dataframe_name")
model.doesnt_match("paris berlin london austria".split()) $
plt.plot(data.moving_avg)
os.environ.get('FOO')
get_high_price = lambda key: get_float_value( r_dict, key, 'High' ) $ get_low_price = lambda key: get_float_value( r_dict, key, 'Low' ) $ get_daily_chg = lambda key: abs( get_high_price(key) - get_low_price(key) )
rng + pd.tseries.offsets.BQuarterEnd()
df.drop( columns='year1') # drop single column
def stemming(tokenized_text): $     text = [ps.stem(word) for word in tokenized_text] $     return text $ infinity['text_stemmed'] = infinity['text_nostop'].apply(lambda x: stemming(x))
temp_df.hist("tobs",bins=12, color="blue", grid=True) $ plt.ylabel("Frequency", fontsize = 15) $ plt.xlabel("Temperature", fontsize = 15) $ plt.title("Last 12 Months Temperature Histogram", fontsize = 15) $
type(s), s.dtype, len(s), s.shape
threeoneone_geo = threeoneone_geo[threeoneone_geo['Latitude'].notnull()] $ threeoneone_geo = threeoneone_geo[threeoneone_geo['fix_time_sec'].notnull()] $ threeoneone_geo = threeoneone_geo[threeoneone_geo['fix_time_sec']>0]
last_year.shape
n_user_days = pax_raw[['seqn', 'paxday']].drop_duplicates().groupby('seqn').size()
Test.ALLOFF()
def lookup_sentiment(document): $
june1518['dataset']['data'] $
df1_stdev = pd.DataFrame(lst, columns = cols, index= df1.index)
df.groupby('Year').get_group(2014)
x = np.linspace(0, 10, 100) $ df = pd.DataFrame({"y":np.sin(x), "z":np.cos(x)}, index=x) $ df.head()
metadata['map_info'] = refl['Metadata']['Coordinate_System']['Map_Info'].value $ metadata
brands = autos["brand"].value_counts(normalize=True).head(9) $ brands = brands.index $ brands
temp = weather_mean['Temp (deg C)'] $ temp.head()
print("State space samples:") $ print(np.array([env.observation_space.sample() for i in range(10)]))
df[null_columns].isnull().sum()
import lightgbm as lgb
thecmd = 'ogr2ogr -f "CSV" ' + dataDir + 'input/new-york_new-york_points.csv ' + dataDir + 'input/new-york_new-york.db -lco GEOMETRY=AS_XY -progress -explodecollections -sql "select * from points"' $ print thecmd
from sklearn.tree import export_graphviz $ lt = ["low","medium","high"] $ predict = out_train[lt].idxmax(axis=1) $ target = {'high':0, 'medium':1, 'low':2} $ predict= np.array(predict.apply(lambda x: target[x]))
precip_df = pd.DataFrame(year_precip) $ index_date_df = precip_df.set_index('date') $ index_date_df.head()
d.sum()
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x, np.dot(x3, ridge2.coef_) + ridge2.intercept_, color ='fuchsia', linewidth = '5') $ plt.plot(x, np.dot(x3, model3.coef_) + model3.intercept_, color = 'dimgrey', linewidth = '3.5')
transfer_duplicates = BTC.loc[BTC['Smoother'].isnull()==False,['Year','Month','Day','Smoother']];
df2.drop([1899], inplace=True) $ df2[df2['user_id'] == 773192]
vect1 = CountVectorizer(vocabulary = final_text_words, stop_words = 'english', ngram_range = (1,2))
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount" , ascending=False)
all_data_merge.to_csv('Nestle_normal_march.csv', index=False, sep=',', encoding='utf-8')
rdd = sc.parallelize([ $     Row(c1 = 1.0, c2 = None, c3 = None), $     Row(c1 = None, c2= "Apple", c3 = None)]) $ df = spark.createDataFrame(rdd, samplingRatio=1.0) # samplingRatio = 1 forces to see all records $ df.printSchema()
weather = bq.Query(weather_query, dialect='standard').to_dataframe()
run txt2pdf.py -o "OROVILLE HOSPITAL ALL DRGs.pdf"  "OROVILLE HOSPITAL ALL DRGs.txt"
test1 = get_neg_convo('group_426696')
plots.top_n_IPs(5)
B2_NTOT_WINTER_SETTINGS.settings.mapping_water_body['N m Bottenvikens kustvatten']
geocoded_df[list(filter(lambda x: x.endswith('Date'), geocoded_df.columns))].sample(5)
my_df["user_create"] = df_user.groupby('nweek_create')['user_id'].nunique() $ my_df["user_active"] = df_user.groupby('nweek_active')['user_id'].nunique()
data
import pandas as pd $ import numpy as np $ from sklearn.linear_model import LinearRegression
all_cards = pd.concat(list(only_cards)) $ all_cards.sample(10)
old_page_converted = np.random.choice([1,0], size = nOld, p=[pMean,oneMinusP]) $ old_page_converted.mean()
df2[df2['group'] == "control"]['converted'].mean()
dfX = df2.query('group == "control"') $ control_convert = dfX.converted.sum()/dfX.count()[0] $ control_convert
dfg = Grouping_Year_DRG_discharges_payments.groupby(['drg3']).\ $ agg({'discharges':[np.size, np.mean,np.max]}) $ dfg.head()
grouped.ngroups
tw.describe()
tweet_clean.describe()
Imagenes_data_v2.head()
df.query('converted == 1').count()[0]/df.count()[0]
df_range = select_range(df, 10, 6000)
r_chart_title = 'Range control chart' $ r_chart_subtitle = 'Thickness range (mm)' $ r_chart_ylabel = 'Thickness range (mm)' $ r_chart_xlabel = 'Operator--Part'
linear = linear_model.LinearRegression() $ linear.fit(x, y) $ linear.coef_, linear.intercept_
fig, ax = plt.subplots(1, 1) $ station_sample_group = station_by_week[station_by_week.STATION == '1 AV'].groupby(['Week_Number']) $ for key, grp in station_sample_group: $     grp.plot("Week_day", "entries_diff", ax=ax, label=key) $
df['isMobile'] = df['isMobile'].astype('int')
df.Date.dtype
QLESQ.drop_duplicates(subset=["subjectkey"], keep='first', inplace=True) $ QLESQ.shape $ QLESQ.describe()
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
bst.get_fscore()
df.to_excel("../../data/msft2.xlsx")
newfile[['Sales Document', 'Reason for rejection', $          'Delivery block', 'Ov.transport status']].head()
rfr = RandomForestRegressor() $ rfr.fit(x_train1, y_train1) $ rfr.score(x_test1, y_test1)
out_df['medium']=out_df['medium']/sump $ out_df['low'] = out_df['low']/sump
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
df2_control=df2.query('group=="control"') $ p_control=df2_control['converted'].sum()/len(df2_control) $ p_control
get_freq(series_obj=raw.income_flag)
cur.execute("show databases;") $ for r in cur.fetchall(): $    print(r)
bobby_ols = ols('opening_gross ~ star_avg',dftouse_seven).fit() $ bobby_ols.summary()
austin.isnull().sum()
bacteria.name = 'counts' $ bacteria.index.name = 'phylum' $ bacteria
api_request = requests.get("http://jordan.emro.info/api/locations") $ api_request.json()[1]
with TablesBigMatrixReader(output_path) as reader: $     row_indices = reader.get_row_indices('id == {}'.format(some_id)) $     col_indices = reader.get_col_indices_by_name(['my_col5', 'my_col6']) $     data_tmp = reader[row_indices, col_indices] $ data_tmp
july_1 = pd.to_datetime('7/1/2016') $ july_1
Geocoder.geocode("4207 N Washington Ave, Douglas, AZ 85607").valid_address
X_test.columns #age_well is 10
%load_ext watermark $ %watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,pandas,matplotlib,nltk,sklearn,tensorflow,theano,mxnet,chainer,seaborn,keras,tflearn,bokeh,gensim
from bs4 import BeautifulSoup $ import pandas as pd $ from urllib.request import urlopen
a_list.remove(another_list) $ a_list
sf_small_grouped.reset_index(inplace=True)
import matplotlib.pyplot as plt $ sb.heatmap(components)
df = pd.concat([df, race_vars], axis=1) $ df = pd.concat([df, gender_vars], axis=1)
lr = LogisticRegression(random_state = 42) $ param_grid = {'penalty': ['l1', 'l2'], $               'C':np.logspace(0, 2, 10)} $ lr_gd = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, scoring='f1', n_jobs = -1) $ lr_gd.fit(X_train, y_train)
print(autos.columns) $ autos['unrepaired_damage'].value_counts()
from dask import dataframe as dd
df2[df2['user_id'].duplicated()]
scores
pysqldf("select * from genes where genomic_accession = 'LT906474.1' and start > 3252780 and end < 3302485")
gender.plot.bar() $ plt.title("Students per Gender", fontdict={'fontsize': 14}); $ plt.savefig('images/barplot_gender.png')
dates_list=train["visit_date"].unique()
df.head()
my_gempro.pdb_downloader_and_metadata() $ my_gempro.df_pdb_metadata.head(2)
dataAnio.to_csv('data/dataPorUbicacion_Anios.csv')
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
gpCreditCard.Passenger_count.describe()
kochdf.loc[kochdf['date'] == max_date]
df_new['ab_CA'] = df_new['ab_page']*df_new['CA'] $ df_new['ab_UK'] = df_new['ab_page']*df_new['UK'] $ lm = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA', 'UK','ab_CA','ab_UK']]) $ results3 = lm.fit() $ results3.summary()
movie_df.shape
actual_diff=actual_pnew-actual_pold $ actual_diff, actual_pnew, actual_pold
print (df_country.head(), df2.head())
full.groupby(['Age'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False)
new_bandits = sum(betas_mask) $ sample_sizes = gb.count().values.ravel() $ posteriors_nume = np.sqrt(sample_sizes) * new_bandits $ posteriors_denom = sum(posteriors_nume) $ posteriors = posteriors_nume / posteriors_denom
from sklearn.svm import LinearSVR $ model = LinearSVR() $ print ('SVR') $ reg_analysis(model,X_train, X_test, y_train, y_test)
data.info()
result.summary()
print "{:,} total users".format(filteredPingsDF.groupBy("cid").count().count()*100) # ~830M users
most_informative_features_top_and_bottom(vectorizer=vectorizer, classifier=lr2, binary=False, n=15)
import pandas_datareader.wb as wb $ all_indicators = wb.get_indicators() $
import numpy as np $ import pandas as pd $ from numpy import genfromtxt $ filename = 'data/allnames.txt' $ df=pd.read_csv(filename, sep=',', names = ["Name", "Gender", "Count"])
df.drop(['YR_RMDL', 'ROOMS'], axis=1, inplace=True)
frames = [dummy_var_df, data['cat'], data['funny'], data['love'], dfcv] $ features_df = pd.concat(frames, axis=1, ignore_index=True )
def trip_start_weekday_m(x): $     if x['search_type'] == 'CAR': $         return trip_start_weekday_car(x['trip_start_date_weekday']) $     else: $         return trip_start_weekday_fh(x['trip_start_date_weekday'])
tw_clean[tw_clean.name == "a"]
df_u= df_vu.groupby(["landing_page","group"]).count() $ df_u $
with open('files/approved users.txt') as fin: $     approved_users = [] $     for line in fin: $         approved_users.append(line.rstrip('\n')) $ print(approved_users)
chambers['swing_state'] = (chambers.index.isin( $     ['CO', 'FL', 'IA', 'MI', 'MN', 'OH', 'NV', 'NH', 'NC', 'PA', 'VI', 'WI']))
paired_payment_cumsum = [(i,j) for (i,j) in zip(list_payments_by_DRG,cum_sum_payments_list)] $ paired_payment_cumsum[:5]
print('Proportion of users converted :: ',df['converted'].mean())
control_convert = df2.query('group == "control"')['converted'].mean() $ control_convert
n_new = df2[df2['group'] == 'treatment']['converted'].count() $ n_new
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index().plot(kind="bar", title="Ad_created (All)", colormap="Blues_r")
mean_sea_level.plot(subplots=True, figsize=(16, 12));
attend_with.to_csv('../data/attend.csv')
y_cat.value_counts() $
df_protest.loc[:, 'duration'] =  df_protest.end_date - df_protest.start_date $ df_protest.duration
data3.head()
facts_metrics = pd.read_sql_query('select * from "facts_metrics"',con=engine)
bacteria2.fillna(method='bfill')
df[['age']].describe()
df_mes = df_mes[df_mes['fare_amount']>0] $ df_mes.shape[0]
slices = dataset.Sentiment.value_counts() $ activities = ['Neutral', 'Positive', 'Negative']
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
segDataGrouped = segmentData.groupby(['lead_mql_status','opportunity_month_year', 'opportunity_stage']).opportunity_stage.count() $ oppstagepct = segDataGrouped.groupby(level=[0,1]).apply(lambda x: x / float(x.sum())); oppstagepct.head()
X_train, X_test, y_train, y_test = train_test_split(ad_nlp['Text'], ad_nlp['Score_Binary'], random_state=42, stratify=ad_nlp['Score_Binary'])
new_user_project = user_project.drop_duplicates(subset=['user_id', 'project_id'], keep='first') $ new_user_project.groupby(['user_id','project_id']).count().head()
findNumbers = r'\d+' $ regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number') $ print(regexResults.group(0))
logreg = LogisticRegression() $ logreg.fit(X_train, y_train) $ logreg.score(X_test, y_test)
df_protest.info()
df.columns
df_transactions['membership_duration'] = df_transactions['membership_duration'].astype(int)
from bs4 import BeautifulSoup $ soup = BeautifulSoup(resp.text, 'lxml')
train = train.fillna(-999) $ test = test.fillna(-999)
from sklearn.neural_network import MLPClassifier $ from sklearn.preprocessing import StandardScaler $ from sklearn.neural_network import MLPRegressor
au.find_some_docs(ao18_qual_coll,sort_params=[("id",1)],limit=3)
result.to_csv("gaurav3.csv", index=False)
properati.loc[(properati['zone'] == "") &\ $               ((properati['place_name'] == "Bs.As. G.B.A. Zona Norte") | (properati['place_name'] == "Bs.As. G.B.A. Zona Oeste") | \ $               (properati['place_name'] == "Bs.As. G.B.A. Zona Sur")),\ $               'zone'] = "G.B.A"
df.loc[d[1]]
data.to_csv('prepared_data_with_cuts.csv', index = True) $
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index()
(~autos["registration_year"].between(1970,2016)).sum() / autos.shape[0]
df1 = df1.rename(columns={"0": 'created_at', "1": "tweet"}) $ df2 = df2.rename(columns={"0": 'created_at', "1": "tweet"})
other_text_feature = vect1.fit_transform(talks['text'].as_matrix())
p.end_time
from dask.diagnostics import ProgressBar $ with ProgressBar(): $     d.compute()
InfinityWars_PRED_df = pd.DataFrame(Y_TFIDF_PRED)
p=df2['converted'].mean() $ p
plt.scatter(aqi['AQI_eug'], aqi['AQI'], alpha=0.2) $ plt.xlabel('Eugene/Springfield'); plt.ylabel('Oakridge');
access_logs_df = access_logs_parsed.toDF() $ access_logs_df.printSchema()
print iowa.info() $ iowa.describe() $
df_countries.nunique()
df2 = df.query('landing_page == "new_page" & group == "treatment" | landing_page == "old_page" & group == "control"')
autos['price_dollars'].describe()
support.head()
df_new['stack_reputation'] = u.reputation.format()
sub1 = sub1.drop_duplicates()
pf_rdd = sc.parallelize([('RF1', 1.), ('RF2', 2.)]) $ dfpf = sqlContext.createDataFrame(pf_rdd, ['rf', 'qty'])
ax = statistics['power_consumption'].plot(y='sum') $ ax.set_xlabel('Date') $ ax.set_ylabel('Total Power Consumption')
sorted_budget_biggest.groupby('original_title')['vote_average'].mean()
pivoted.head()
%%time $ c = collection.find({}, {'_id':0, 'id':0, 'building_id':0, 'manager_id':0, 'listing_id':0, 'photos':0}) $ raw_df = pd.DataFrame(list(c)) $ raw_df.interest_level = raw_df.interest_level.astype('category', categories=['low', 'medium', 'high'])
with urllib.request.urlopen("https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20yahoo.finance.xchange%20where%20pair%20in%20(%22GBPHKD%22%2C%22HKDGBP%22)&format=json&diagnostics=true&env=store%3A%2F%2Fdatatables.org%2Falltableswithkeys&callback=") as url: $     data = json.loads(url.read().decode()) $     print(data)
autos.shape
spp = pd.read_excel('input/Data.xlsm', sheet_name='43', header=11, skipfooter=8793)
a = np.arange(1, 11) $ a
import statsmodels.api as sm $ df_reg['intercept']=1 $ logit_mod= sm.Logit(df_reg['converted'], df_reg[['intercept','ab_page']]) $ results= logit_mod.fit() $ results.summary()
segments.seg_length.hist(bins=100)
es.get(index="test-index", doc_type='tweet', id=1)
df_links.head()
%%R $ AirPlot('Boston', 'Atlanta', 'ARR_HOUR')
avg_per_seat_price_seasonsandteams["2013 Offseason", "BAL"] - avg_per_seat_price["BAL"] $
from scipy.stats import norm $ print(norm.cdf(z_score)) # significance of our z-score $ print(norm.ppf(1-(0.05))) # critical value at 95% confidence for one-tail test
p=df2.query('landing_page=="new_page"').describe() $ p
autos["price"].head() #head() displays first few rows, probably first 5
voc=vectorizer.vocabulary_
tweet_archive_clean[['doggo', 'floofer','pupper', 'puppo']].head()
flight.count()
print(stringlike_instance_3)
df[df['zip']>53000]
actual_difference = df[df.group == 'treatment'].converted.mean() -  df[df.group == 'control'].converted.mean() $ (actual_difference <  np.array(p_diffs)).mean() $
!wc -l /home/ubuntu/kmer-hashing/sourmash/lung_cancer/samples.csv
multi_bulk.ix['ibm us equity','best analyst recs bulk'].dropna()
output_variables = S.modeloutput_obj.read_variables_from_file() $ output_variables
engine.execute('SELECT * FROM Measurement LIMIT 10').fetchall()
SANDAG_age_df.loc[(2012, '1'), 'POPULATION'].sum()
ben_scores = ben_final.loc[:,['userid','stiki_score','cluebotRevert']] $ van_scores = van_final.loc[:,['userid','stiki_score','cluebotRevert']]
!pwd
d311_in = d311.apply(within_area,axis=1) $ d311["inc_311"] = 1 $ d311_gb = d311.loc[d311_in,["long","lat","inc_311"]].groupby(["long","lat"],as_index=False).sum()
((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idx, df, yl.astype(np.float32))
print('The highest opening price is ' + str(max(data['Open']))) $ print('The lowest opening price is ' + str(min(data['Open'])))
desc_stats.columns = ['Count', 'Mean', 'Std. Dev.', 'Min.', '25th Pct.', 'Median', '75th Pct.', 'Max'] $ desc_stats
fh_3 = FeatureHasher(input_type='string', non_negative=True) $ %time fit3 = fh_3.fit_transform(train.device_ip)
df.query('group == "treatment"')['converted'].mean()
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05))) $
words_df.head()
n_old = control_df.shape[0] $ print('The number of individuals in the treatment group is n_old = {}.'.format(n_old))
df3 = pd.DataFrame(q3_results,columns=['station_name','observed_tobs']) $ df3['observed_tobs']=df3['observed_tobs'].astype(float) $ df3.dtypes
trigram_dictionary = prep.get_corpus_dict(from_scratch=False):
liberia_data3=liberia_data3.fillna(0)
print(convert_new, convert_old, n_new, n_old)
ff3.set_index('Date', inplace=True)
with open('data/kochbar_11.json') as data_file:    $     kochbar11 = json.load(data_file) $ koch11df = preprocess(kochbar11) $ koch11df.info()
pnew=df2['converted'].mean() $ pnew
google_stock = pd.read_csv('./GOOG.csv')
response = requests.get(url)
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.contains('^[(a-z)]'))]='None'
fraud_data_updated = fraud_data_updated.drop(["user_id",'device_id','ip_address'],axis=1) $
tweet_en[tweet_en['text'].apply(lambda x: "I'm at" in x)]
measure_val_2010_to_2013 = measure_nan[(measure_nan['date'] > '2010-1-1') & (measure_nan['date'] <= '2013-12-31')]
tweet_archive_clean = pd.merge(tweet_archive_clean, images, on = 'tweet_id', how = 'inner')
print(list_of_lines)
df2.index.get_loc(2893)
df4[['new','old']] = pd.get_dummies(df4['landing_page']) $ df4.head()
df_l.groupby(["landing_page","group"]).count() #verifying the values in the new dataset
own_star.drop('uniqueID', axis=1, inplace=True)
mc_results_x = ssm.MultiComparison(df['y'], df['x']) $ mc_results_x_tukey_hsd = mc_results_x.tukeyhsd() $ print(mc_results_x_tukey_hsd)
x=df2[['intercept','new','ab_page']] $ y=df2['converted'] $ x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)
%load_ext autoreload $ %autoreload 2 $
max(TrainData['Lead_Creation_Date_clean']), min(TrainData['Lead_Creation_Date_clean'])
SEA_analysis = team_analysis.get_group("SEA").groupby("Category")
nnew=df2.query('landing_page=="new_page"').count()[0] $ nnew
incidents_time_index = pd.date_range(start,finish,freq='1t')
pprint.pprint(posts.find_one({"reinsurer": "AIG"}))
google_stock.describe()
df_group_by2.head()
newdf['TSRet'] = newdf['TSRet'] *100
prediction_clean.p1.value_counts()[:20].rename_axis('prediction').reset_index(name='counts')
for info in zf_test.infolist(): $     print("File Name         -> {}".format(info.filename)) $     print("Compressed Size   -> {:.2f} {}".format(info.compress_size/(1024*1024), "MB")) $     print("UnCompressed Size -> {:.2f} {}".format(info.file_size/(1024*1024), "MB"))
df2
people_person['date'] = people_person.date_joined.dt.date $ people_person.head()
df_protest.columns[df_protest.dtypes=='object'].tolist()
result_df.head()
e_p_b_two.index = e_p_b_two.TimeCreate $ del e_p_b_two['TimeCreate'] $
test= test.reset_index(drop = True) $ test.surface_covered_in_m2 = pd.Series(predictions)
print(repos_users.shape) $ repos_users.head()
autos[["date_crawled", "ad_created", "last_seen", "registration_month", "registration_year"]].head()
faa_data_pandas = pd.read_csv("Pennsylvania_Condensed.csv") # Bringing in the Facebook message data $ faa_data_pandas.head()
new_page_converted.mean()-old_page_converted.mean()
df.query('converted == True').user_id.nunique() / df.user_id.nunique()
df.groupby('user_id').nunique().shape[0] == df.nunique()['user_id']
word_freq_df = pd.read_csv('word_freq.txt', delim_whitespace=True) $ word_freq_df.columns = ['Frequency','Word','Parts_of_Speach','Num_doc_occurences'] $ word_freq_df = (word_freq_df[~word_freq_df.Word.isnull()])
flow_data.loc[:, field_headers].to_excel('/root/notebook/stg/flow_report.xlsx', index=False)
df = df.drop(['cast', 'homepage', 'tagline', 'keywords', 'overview', 'imdb_id'], axis=1)
data['funny'].value_counts()
df_data.printSchema()
for data, label in zip(x_test, predicted): $     print(data, label)
for index, row in df.iterrows(): $     df.at[index, 'text'] = clean_text(df.at[index, 'text'])
dfx = df.copy() $ dfx[dfx < 0] = -dfx  # abs $ print(dfx)
df.groupBy("column_name").count()
pred3 = nba_pred_modelv1.predict(g3) $ prob3 = nba_pred_modelv1.predict_proba(g3) $ print(pred3) $ print(prob3)
cached = subset.cache()
to_be_predicted_Day4 = 43.44784915 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
(autos["last_online"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
df.isnull $ df.drop(['Md'], axis=1, inplace=True) $ df = df[:-8] $ df
data[(data['author_flair'] == 'Bears') & (data['win_differential'] >= 0.9)].comment_body.head(15) $
smooth = condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="SMOOTHPURSUIT")
lgb_model = lgb.LGBMRegressor(objective='mape',n_estimators=400, learning_rate=0.2, num_leaves=75, random_state=1)
big_df_count.reset_index(inplace=True) $ big_df_avg.reset_index(inplace=True)
df_archive.iloc[19,:]
rr = pd.concat([fundret, r[coins_top10]], axis=1)
users_not_odu = users.loc[users['OneDayUser'] == False] # odu - one day user $ Asked = set(users_not_odu.loc[users['Asked'] > 0].index) $ Answered = set(users_not_odu.loc[users['Answered'] > 0].index) $ Commented = set(users_not_odu.loc[users['Comments'] > 0].index) $ venn3([Asked, Answered, Commented], ('Asked', 'Answered', 'Commented')) $
freeways = gis.content.get('91c6a5f6410b4991ab0db1d7c26daacb') $ freeways
os.getcwd()
goog.describe()
volkswagen_cars = df2[df2.brand =='volkswagen'] $ print('Number of Volkswagen cars on sale is:',len(volkswagen_cars),'and the mean price is', $        int(np.mean(volkswagen_cars.price)), 'Euros.')
by_year = compare_annual(df).timing_impact.unstack().round(3) $ by_year.index = by_year.index.year $ title = '"Behavior Gap" By Fund (%)' $ sns.heatmap(by_year,center =0.00, cmap = sns.diverging_palette(10, 220, sep=1, n=21),annot=True) $ plt.title(title)
eclf2 = VotingClassifier(estimators=[('lr', alg), ('calC', alg7), ("ranFor", alg2)], voting='soft') $ eclf2.fit(X_train, y_train) $ probs = eclf2.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
pickleobj('specjson210618', specsJson)
new_page_converted = np.random.binomial(1, prop_users_converted, n_new)
sqlCtx = pyspark.SQLContext(sc) $ sdf = sqlCtx.createDataFrame(df.astype(str)) $ sdf.show(5)
df[df.sentiment == 0].count()
ax = users.created_at.hist(bins=12) $ ax.set_xlabel('Date') $ ax.set_ylabel('# Users') $ ax.set_title("Users' account creation per year")
test_df = dogscats_df.loc[dogscats_df['pred_00'].notnull()] $ test_df.loc[test_df['pred_00'] != test_df['label'], 'imagePath']
errors = pd.DataFrame.from_dict(d, orient="index").rename(columns={0:"mae_vals"}) $ errors.head()
n_old = df2.query('landing_page=="old_page"').shape[0] $ n_old
datetime64("2013-01-01T00","h")
clf = svm.SVR()
train_view.sort_values(by=4, ascending=False)[0:10]
get_response('I think it might be a bit aggressive for a kitten')
f_device_hour_clicks.show(1)
prob_new_page = len(df2.query("landing_page == 'new_page'")) / df2.shape[0] $ print('The probability that an individual received the new page is {}'.format(prob_new_page))
wrd_clean.info()
support_NNN = merged_NNN[merged_NNN.committee_position == "SUPPORT"]
room_temp.__repr__ = lambda : 'lol2'
soup = BeautifulSoup(response.text, "html5lib")
feature_data = pd.read_csv('data/kaggle_data/features.txt', header=None, sep="  ", names=['feature_names', 'feature_description']) $ feature_data.head(5)
log_mod=sm.Logit(result_df['converted'],result_df[['intercept','ab_page','UK','US','UK_page','US_page']]) $ results=log_mod.fit()
us_holidays = ['2015-01-01', '2015-01-19', '2015-02-16', '2015-05-25', '2015-07-03', '2015-09-07', $                '2015-10-12', '2015-11-11', '2015-11-26', '2015-12-25', '2016-01-01'] $ str(train.start_timestamp.max().date()) in us_holidays
michael_df = df[michaelkorsseries]
import string $ strings = string.ascii_lowercase[1:26] $ print(strings)
poverty_data_rows=[] $ for i in range(int(len(poverty)/4)): $     for j in range(3): $         poverty_data_rows.append(i*4+j+1)
energy_cpi.dtypes
properati[properati['zone'] == ""]['place_name'].value_counts(dropna=False)
mydata.iloc[0] # specificies the actual row in terms of index type
pubs = db.get_publications(limit=None)  # limit sets a limitation for sql commands $ pubs.head()
n = len(joined);n
Results_rf200.to_csv('soln_rf200.csv', index=False)
random_walk.plot();
test[test['Open'].isnull()]
trans = trans.loc[trans['membership_expire_date'] >= trans['transaction_date']]
df_userid = pd.DataFrame({"UserID":users["UserID"]}) $ df_Tran = pd.DataFrame({"ProductID":products["ProductID"]}) $ df_userid['Key'] = 1 $ df_Tran['Key'] = 1 $ df_out = pd.merge(df_userid,df_Tran,how='outer',on="Key")[['UserID','ProductID']]
grouped.size()
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'),how='inner')
baseball_newind.sort_index(axis=1).head()
store_items.dropna(axis = 0)
class_test = sub_df[(sub_df['text'].str.find('?') != -1)].reset_index()
print(repos_users[repos_users['language'] == 'Python'].shape) $ repos_users[repos_users['language'] == 'Python'].head()
df_ml_6201 = df.copy() $ df_ml_6201.index.rename('date', inplace=True) $ df_ml_6201_01=df_ml_6201.copy()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&apikey='+API_KEY)
import pandas as pd $ df = pd.read_csv("software1.csv") $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ df.head()
CVIterator = [] $ for i in dates: $     trainIndices, valIndices = create_validation(train, i) $     CVIterator.append( (trainIndices, valIndices) )
full['WillBe<=30Days'] = full['WillBe<=30Days'].fillna(0)
donors_c[donors_c['Donor Zip'] == '606' ]['Donor State'].value_counts().head()
api_clean = from_api.copy()
ggplot(mres.data.assign(idx=list(range(mres.data.shape[0]))),aes(x="idx",y="residuals",color='subject',shape="et"))+geom_point()
output.count()
object_by_id
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
import platform $ print(platform.python_version()) $ %load_ext watermark $ %watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,pandas,scipy,scikit-learn,matplotlib,seaborn,jupyter,notebook,line_profiler,memory_profiler,numexpr
p_percprofit = 1 - stats.norm.cdf(0.005,loc = monthly_portfolio_average,scale = np.sqrt(monthly_portfolio_var)) $ p_percprofit
rmse
c.index
y_train = y[:2800] $ y_test = y[2800:]
!convert materials-xy.ppm materials-xy.png $ Image(filename='materials-xy.png')
breed_conf = df_twitter.groupby('p1')['p1_conf'].mean()
lr = K.constant(0.01) $ grads = K.gradients(loss, [W,b]) $ updates = [(W, W-lr*grads[0]), (b, b-lr*grads[1])]
sum(df2.landing_page == 'new_page').astype('float32') / 290584
"In the last {0} tweets, I've been postive {1} percent, negative {2} percent and neutral {3} percent".format(polarity.sum().counts, percentConvert(polarity.loc["positive"].percent), percentConvert(polarity.loc["negative"].percent), percentConvert(polarity.loc["neutral"].percent))
df.loc[102]  # by single row label
wikipedia_base_url = 'https://en.wikipedia.org' $ wikipedia_marvel_comics = 'https://en.wikipedia.org/wiki/Marvel_Comics' $ marvel_comics_save = 'wikipedia_marvel_comics.html' $ globalCity_pdf = 'http://journals.sagepub.com/doi/pdf/10.1177/0042098007085099'
len(unprocessed_list)
keep_vars = set(no_hyph.value_counts().head(12).index)
from matplotlib.pyplot import figure $ figure(num=None, figsize=(17, 4), dpi=80, facecolor='w', edgecolor='k') $ names = list(tweet_lang_hist.index) $ values = list(tweet_lang_hist.lang_freq) $ plt.bar(names, values)
import pandas as pd $ pd.options.mode.chained_assignment = None
tweet_archive.describe()
idx = pd.isnull(geocoded_df[['Case.File.Date','Judgment.Date']] $                ).apply(lambda x: not (x['Case.File.Date'] or x['Judgment.Date']), axis=1)
parks_don['fq'] = parks_don.close_date.apply(lambda x: 'FY'+ str(x.strftime('%y')) + 'Q' + str((x.month-1)//3)) $ parks_don['year'] = parks_don.close_date.apply(lambda x: x.year) $ parks_don.head(2)
%%bq query -n wine_query $ select * $ from `looker-action-hub.wine_data.new_wines_no_quality` $ where created_at < '2018-05-01'
plt.scatter(y=dftouse.opening_gross, x=dftouse.star_avg)
print len(df_goog.Open.asfreq('D', method='backfill')) $ print len(df_goog.Open) $ print len(df_goog.Open.asfreq('D')) $
s.resample('Q').head()
dataframe.groupby('year').daily_worker_count.agg(['count','min','max','sum','mean'])
print(DataSet_sorted['tweetText'].iloc[1])
a_list[len(a_list) - 1]
class_merged.isnull().sum()
if(not(os.path.exists("model/{}".format(version)))): $     os.makedirs("model/{}".format(version))
X_test.shape
users_converted = df.query('converted == 1').count()[0] / (df.query('converted == 1').count()[0] + df.query('converted == 0').count()[0]) $ users_converted_percentage = users_converted * 100 $ print('The proporation of converted users are ' + str(users_converted_percentage) + '%')
df2 = df2.query('duplicated_users == False')
prop = props[props.prop_name == 'PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE.']
project_id  = "bigdatasystems-spring2018" $ instance_id = "lab1-section3"
df.loc[((df['group']=='treatment')& (df['landing_page']!='new_page')) | ((df['group']!='treatment')& (df['landing_page']=='new_page'))].count()
df1.join(df2)
salesfull['full_2016']=salesfull['quarter23416']
merkmale = extract_merkmale(scoring_ind)
np.count_nonzero(na_df.isnull()) # total number of missing values
with tf.name_scope("loss"): $     xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) $     loss = tf.reduce_mean(xentropy, name="loss")
def Show_Row(dataframe, column, name, list_columns): $     return dataframe.loc[column == name, list_columns]
autos["price"].value_counts().sort_index(ascending=False).head(15)
df3['timestamp'] = pd.to_datetime(df3['timestamp']) $ df3.info()
mi = s4g.set_index(['Symbol', 'Year', 'Month']) $ mi
analyze_set.info()
_feeder='epri_j1'
tf_vec = CountVectorizer(max_df=0.95, min_df=2, $                          max_features=max_features, stop_words="english") $ tf = tf_vec.fit_transform(comment_sentences) $ tf_feats = tf_vec.get_feature_names()
SAMPLES = 100; gamma_vals = np.logspace(-2, 3, num=SAMPLES) $ gamma_val, gamma_sr = quick_gamma(gamma_vals, consol_px, hist_window, lb, frequency, min_gross, max_gross, min_w, max_w) $ gamma_val, gamma_sr
df_new['uk_intercept'] = df_new['country'].replace(('US','UK','CA'),(0,1,0)) $ lm = sm.OLS(df_new['converted'],df_new[['intercept','uk_intercept']]) $ lm.fit().summary()
model = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']]).fit()
abc.head()
users[users['screenName'] == 'ahmediaTV']
organisation.columns
X_train_dummify, _ = custom_dummify(X_train, 0.01) $ df_imput = MICE(n_imputations=200, impute_type='col', verbose=False).complete(X_train_dummify.as_matrix())
wrong_date_format = cats_df[cats_df['date of last vet visit'].apply(lambda c: pd.to_datetime(c, errors='coerce')).isnull()] $ print(wrong_date_format) $ cats_df['date of last vet visit'] = cats_df['date of last vet visit'].apply(lambda c: pd.to_datetime(c, errors='coerce')) $ cats_df['remove'].iloc[wrong_date_format.index] = True $ del wrong_date_format
S_lumpedTopmodel.decision_obj.groundwatr.options, S_lumpedTopmodel.decision_obj.groundwatr.value
from sqlalchemy import func
df = pd.DataFrame(d) $ df
db = client['instagram-london'] $ coll = db.posts
tmp_df = tmp_ratings.pivot(index='userId', columns='movieId', values='rating')
start = datetime.now() $ modelrf100 = RandomForestClassifier(n_estimators=50, n_jobs=-1).fit(Xtr.toarray(), ytr) $ print(RandomForestClassifier(n_estimators=50, n_jobs=-1).fit(Xtr.toarray(), ytr)\ $     .score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
mom['Date'] = mom['Date'].apply(lambda x: str(pd.to_datetime(str(x), format= '%Y%m'))[0:7])
results = pd.read_csv('player_stats/{}_results.csv'.format(team_accronym), parse_dates=['Date']) $ results_postseason = pd.read_csv('player_stats/{}_results_postseason.csv'.format(team_accronym), parse_dates=['Date'])
from datetime import datetime $ datetime.now().strftime('%Y-%m-%d %H:%M:%S')
df_questionable_3[df_questionable_3['state_IL'] == 1]['link.domain_resolved'].value_counts()
tweet_df.count()
%timeit numba(s)
treat_con = df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)].shape $ treat_con
lda_tf = models.LdaModel.load(os.path.join(outputs, 'model_tf.lda')) $ corpus_lda_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf.mm')) $ lda_tfidf = models.LdaModel.load(os.path.join(outputs, 'model_tfidf.lda')) $ corpus_lda_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tfidf.mm'))
new_model = gensim.models.Word2Vec(min_count=1)  $ new_model.build_vocab(sentences)                     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'weekday']]) $ results = log_mod.fit() $ results.summary()
cycling_df.head(10)
FREEVIEW.plot_main_sequence(raw_freeview_df)
approved.isnull().sum()
result= logit.fit() $ result.summary()
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG_max =(grouped.aggregate(np.max)['disc_times_pay']) $ grouped_by_year_DRG_max.head()
p_old = prob_convert $ p_old
coefs.loc['age', :]
tokens['five_star_ratio'] = tokens.five_star / tokens.one_star
weather.head()
notus['cityOrState'].value_counts(dropna=False)
diz = {'pippo':3, 'pluto':4, 'topolino':"ciao"} $ s2 = pd.Series(diz) $ s2
total_Visits_Convs_month_byMC = pd.concat([totalVisits_month_byMC[['year', 'month', 'day', 'marketing_channel', 'user_visits']], totalConvs_month_byMC.conversions], axis=1) $ total_Visits_Convs_month_byMC['conversion_rate'] = total_Visits_Convs_month_byMC['conversions'] / total_Visits_Convs_month_byMC['user_visits'] $ total_Visits_Convs_month_byMC.head()
time_local = (np.array(time_utc) - time_utc[0]) $ key_press = np.ones((l,1))
df \ $     .groupby('name') \ $     .count() \ $     .collect()
adj_close_pivot = adj_close_acq_date_modified.pivot_table(index=['Ticker', 'Acquisition Date'], values='Adj Close', aggfunc=np.max) $ adj_close_pivot.reset_index(inplace=True) $ adj_close_pivot
def get_cycle(dt): $     if dt.year % 2 == 0: $         return dt.year $     else: $         return dt.year +1
head = "https://wwww.codechef.com/users/" $ var = user $ URL = head + user $ page  = requests.get(URL) $ soup = BeautifulSoup(page.content,'html.parser')
df2.head(2)
event_sdf.toPandas().head()
pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))
id = 'realDonaldtrump' $ new_tweets = api.user_timeline(screen_name = id,count=20) $ print(len(new_tweets)) $ print(new_tweets)
get_all_tweets('Tesla')
crawler( VidGenerator(1,10), 'data.csv', 'failed.csv')
csvFile = open('ua.csv', 'a') $ csvWriter = csv.writer(csvFile)
predictions = forest.predict(test_data_features) $ output = pd.DataFrame( data={"id":test["id"], "rating":predictions} )
all_a=soup.find_all('a') $ print(all_a)
train['month_of_first_booking'] =  train['date_first_booking'].map(lambda x: x.strftime('%m') if pd.notnull(x) else '')
df_term = pd.read_csv(f_term) $ df_term.head(10)
X = users_usage_summaries.iloc[:, users_usage_summaries.columns!='churned'].values $ y = users_usage_summaries.loc[:,'churned'].values $ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05)
pd.set_option('display.max_columns', 65) $ df.head()
a_set.difference(another_set)
print(metrics.classification_report(y_test, y_pred_class))
from xgboost import XGBClassifier
!pwd
a.items() & b.items()
ts.tshift(5,freq="H")
full.groupby(['Dx3'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False) $
page.text
df_characters = pydata_simpsons.create_simpsons_characters_dataframe(df)
column_names = ['daily_worker_count']
series = [30,21,29,31,40,48,53,47,37,39,31,29,17,9,20,24,27,35,41,38, $           27,31,27,26,21,13,21,18,33,35,40,36,22,24,21,20,17,14,17,19, $           26,29,40,31,20,24,18,26,17,9,17,21,28,32,46,33,23,28,22,27, $           18,8,17,21,31,34,44,38,31,30,26,32]
age_groups = ['children', 'adults', 'medium','old']
f_ip_app_clicks.show(1)
autos= autos.drop('seller',1)
df_joined[['CA','UK','US']] = pd.get_dummies(df_joined['country']) $ df_joined = df_joined.drop(['US'], axis = 1) $ df_joined.head()
t_likes = pd.Series(data=data['Likes'].values, index=data['Date']) $ t_retweets = pd.Series(data=data['RTs'].values, index=data['Date'])
CryptoComm['CommentTime'] = pd.to_datetime(CryptoComm['CommentTime'], format='%Y-%m-%d %H:%M:%S') $ CryptoComm['PostTime'] = pd.to_datetime(CryptoComm['PostTime'], format='%Y-%m-%d %H:%M:%S')
plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101_d_b[df_nd101_d_b['ud120']>0].ud120)
a = sqlContext.createDataFrame(pnls)
apple.set_index('Date', drop=True, inplace=True) $ apple.head()
df_rand.head()
contractor_merge['contractor_bus_name'].head() $
print pd.concat([s1, s4], axis=1)
df_gnis.head()
SANDAG_jobs_df.loc[(2012, '1'), 'JOBS'].sum()
minTemp = func.min(Measurement.tobs).label("Lowest Temperature") $ maxTemp = func.max(Measurement.tobs).label("Highest Temperature") $ avgTemp = func.avg(Measurement.tobs).label("Average Temperature") $ session.query(minTemp, maxTemp, avgTemp).filter_by(station="USC00519281").all() $
ed = dt.datetime.today()
df2.head(3)
import pandas as pd $ import numpy as np
print("Train set:") $ print(train_set['No-show'].value_counts() / len(train_set)) $ print("Test set:") $ print(test_set['No-show'].value_counts() / len(test_set))
youtube_df= pd.read_csv("../Data/youtubeVid_main.csv",sep = ",") $ youtube_df["trending_date"] = pd.to_datetime(youtube_df["trending_date"] \ $                                            , format = "%Y/%m/%d")
dsi_me_1_df = get_all_repos(baseurl, 'DSI-ME-1')
themes = themes.lower()
autos['price'].value_counts()
autos['gear_box'].value_counts()
for h in heap: $     h.company = [t.author_id for t in h.tweets if t.author_id in names][0]
z=loan_fundings[['fk_loan','amount','investment_yield']].copy() $ z['amount_yield']=z['amount']*z['investment_yield'] $ avg_yield=z.groupby('fk_loan').sum() $ avg_yield['avg_investment_yield']=avg_yield['amount_yield']/avg_yield['amount'] $ avg_yield['avg_investment_apr']=np.power(1+avg_yield['avg_investment_yield']/1200,12)-1
n_net2 = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(512, 256, 128), $                       random_state=1, verbose = True) $ n_net2.fit(x_train,y_train)
json_data_2017 = request_data_2017.json()
new_page_converted = np.random.binomial(1, p_new, n_new) $ sum(new_page_converted)
print(autodf.groupby('seller').size()) $ autodf = autodf[autodf['seller'] != 'gewerblich'] $ autodf = autodf.drop('seller',axis = 1) $ print( "\nSize of the dataset - " + str(len(autodf)))
y_pred = model.predict(x_test)
df1_clean.source.value_counts()
last_day = (dt.datetime.strptime('2017-08-01', '%Y-%m-%d') - dt.timedelta(days=365)).strftime('%Y-%m-%d') $ last_day
df_onc_no_metac.head()
duration_test_data.to_csv('./data/hours_test_data.csv')
big_df_avg.head()
ps['2014-03':'2014-06']
imgp_clean.shape
vectorizer = CountVectorizer(analyzer='word', stop_words='english', tokenizer=lambda text: text, $                              lowercase=False, binary=True)#, min_df=10) $ spmat = vectorizer.fit_transform(x_tokens)
token.sender = token.sender.astype(int) $ token.receiver = token.receiver.astype(int)
max(results.argsort())
fouls_df
data[(0<data['TMIN']) & (data['TMIN']< 2)]
type(dsg)
prob_conv_treatment = df2[df2['group']=='treatment'].sum()['converted'] / len(df2[df2['group']=='treatment']) $ prob_conv_treatment
obama_speech_save = 'obama_speech_8.html' $ with open(obama_speech_save, mode='w', encoding='utf-8') as f: $     f.write(obamaSpeechRequest.text)
props.prop_name.value_counts()
places2 = dfwithplace['place']
points_node = sets_node['oPoints'] $ observations_node = points_node['observations']
autos["odometer_km"].value_counts()
df.group.unique(), df.landing_page.unique()
tweet_df = pd.read_json('tweet_json.txt',orient='index')
df2[df2.converted == 1].shape[0]/df2.shape[0]
p=(null_vals>actual_diff).mean() $ p
discounts_table.Segment.unique()
sample_new_page_converted = np.random.binomial(n=df_n.shape[0], p=p_new, size=10000) / df_n.shape[0] $ sample_old_page_converted = np.random.binomial(n=df_o.shape[0], p=p_old, size=10000) / df_o.shape[0] $ p_diffs = sample_new_page_converted - sample_old_page_converted
X = c_pd[['name_similairity','address_similairity','st_name_similairity']] $ y = c_pd['predicted']
shifted_forward = msftAC.shift(1) $ shifted_forward
!wget -O Cust_Segmentation.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/Cust_Segmentation.csv
prediction = naive_model.predict(df_test) $ indic = test['Id'] $ res = pd.DataFrame(indic) $ res['Sales'] = prediction $ res.to_csv('data/submission_naive.csv', index = False)
fig = plt.figure(figsize=(18, 6)) $ ax1 = fig.add_subplot(111) $ bins =[0,1,5,10,60,120,180,240,300,600,1200,1800,2400,3000,3600,7200,10800,21600,43200] $ counts, bins, fig = ax1.hist(train.duration, bins=bins)
print(stats.describe(project_0_cycle_times)) $ print(stats.describe(project_1_cycle_times))
df = ek.get_news_headlines('R:IBM.N AND Language:LEN', date_to = "2017-12-04", count=100) $ df.head()
df_countries = pd.read_csv('countries.csv') $ df_countries.head()
plt.figure() $ hourly_dat.WindSpeed.plot()
userModel.summary()
melted_total.groupby(['Categories','Neighbourhood']).mean().unstack()['Review_count'].ix[top10_categories.index].plot.bar(legend=True,figsize=(10, 5))
df[['favorites', 'rating', 'retweets']].corr(method='pearson')
pd.DataFrame({'Social Networks' : [x[1] for x in HARVEY_92_USERS_SN]}).hist(bins=[1,2,3,4,5,6,7,8,10,100,200,1000])
print('Number of unique users in dataset: {}'.format(df['user_id'].nunique()))
dfs[index_max].head()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
bb = data.DataReader(name='F', data_source='iex' $                         , start='2017-07-01', end='2018-05-01') $ bb.head()
n_old = df2[df2['group'] == 'control'].shape[0] $ print(n_old)
m3.unfreeze() $ m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)
df = pd.read_csv('../../data/processed/complaints-before-state-updates.csv')
g.isnull().sum()
jobPostDF.month.value_counts()
table_1c.drop(pn_qty[pn]['1cstoreinfo'])
log_reg_over = LogisticRegressionCV(Cs=[0.1, 0.6], scoring='neg_log_loss') $ log_reg_over.fit(X_train, y_train_over)
tmdb_movies_production_countries_revenue.head()
df = pd.DataFrame(holder)
feature_set = {} $ for features in df_train['features']: $     for feature in features: $         feature_set[feature.lower()] = feature_set.get(feature.lower(), 0) + 1 $
df = df.dropna() $ print(len(df))
features_classif, features_regress, paid_status, overdue, overdue_duration = extract_targets(data_features)
neg_tweets.shape
baseball.hr - baseball.hr.max()
df_low_temps.describe() $
archive_df.info()
df_event.occurred_at = pd.to_datetime(df_event.occurred_at) $ df_event["nweek"] = df_event.occurred_at.dt.week #strftime('%Y-%U')
import datetime $ today = datetime.datetime.now() $ diff = datetime.timedelta(weeks = 5, days = 2) $ result = today + diff $ print(result)
store_items.isnull()
weather['precipitation_inches'].unique()
print("There have been {} tweets, with an average of {} and std {} remaining".format(thisWeek['text'].count() - 1, $                                                                                      hourlyRates.loc[hourlyRates['hourNumber'] == thisWeek['hourNumber'][0],'meanRemainingTweets'].iloc[0], $                                                                                      hourlyRates.loc[hourlyRates['hourNumber'] == thisWeek['hourNumber'][0],'stdRemainingTweets'].iloc[0]))
df.set_index("stamp",inplace=True) $ df.head()
ign.head()
tweet_json_df_clean['tweet_id'] = tweet_json_df['tweet_id'].astype(str) $ twitter_df_clean['tweet_id'] =  twitter_df['tweet_id'].astype(str)
(temp_df.email == '\\N').sum()
f0.include_list_filter
dc['created_at'].dt.hour.hist(density=True, color="blue")
tweet_archive_master.columns
pipe_lr = make_pipeline(cvec, lr) $ pipe_lr.fit(X_train, y_train) $ pipe_lr.score(X_test, y_test)
df_2017['bank_name'] = df_2017.bank_name.str.split(",").str[0] $
train = train.merge(data, on=['cust_id','order_date'],how='left')
del(train_df); del(test_df)
df2['new_page'] = pd.get_dummies(df2.landing_page)['new_page']
props.head(1)
df_image_clean.nlargest(5, 'img_num')[['Frist_pred_conf', 'Second_pred_conf', 'Third_pred_conf']]
plt.hist(freq_df['tobs'], bins=12) $ plt.tight_layout() $ plt.ylabel("Frequency") $ plt.show() $
move_34p14u34p = (breakfastlunchdinner.iloc[1, 1] $                + breakfastlunchdinner.iloc[5, 2] $                + breakfastlunchdinner.iloc[1, 3]) * 0.002 $ move_34p14u34p
bar_plot(merged_df)
generate_5_year_data('medicare_payment' ,'max') #medicare_payment
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True) $ df.info()
data2.head()
from sklearn.linear_model import LogisticRegression
plot_distplot(df["trip_duration"], xlim=(0, 18))
data_df.clean_desc[15]
tweets_clean[tweets_clean.retweet_count == 0].index
ax = hits_df.plot(title="GitHub search hits for {} days sans outliers".format(len(hits_df)), $                   figsize=figsize) $ ax.set_xlabel('Date') $ _ = ax.set_ylabel('# of ipynb files')
jsondata = json.dumps(output_dict) $ f = open("output_dict.json","w") $ f.write(jsondata) $ f.close()
print('Records in train %d.'%len(train)) $ print('Records in test %d.'%len(test))
df_trump_device_non_retweets.created_at.max()
%reload_ext autoreload $ %autoreload 2 $ from ktext.preprocess import processor
df.select('label').describe().show() $
df_total.to_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_Oncology\01_Oncology_EMR_cleaned_with_dummies.csv', index=False)
df[df['price'] == df['price'].max()]
sql_createdb = 'create database HelloDB3;' $ conn_helloDB.execute(sql_createdb)
fraud_data_updated = fraud_data
df_vec_sums = df_vec.sum() $ df_vec_sums = df_vec_sums.sort_values(ascending=False) $ dftops = df_vec_sums[df_vec_sums>=8000] $ dftops
df0 = pd.read_csv('../../datasets/crypto-index-fund/crypto_data/CryptoData.csv', parse_dates=True)
backup = clean_rates.copy() $ clean_rates.cuteness = clean_rates.cuteness.astype(str) $ clean_rates.cuteness = clean_rates.cuteness.str.replace('nan', 'none') $ clean_rates.cuteness = clean_rates.cuteness.astype('category')
autos.info()
pn_qty
pickle.dump(mapWordsSimilar, open( data + "mapWordsSimilarHashtag"+str(augment_size)+'.p', "wb")) 
"unique user-sessions in click data: ", len(df_click.user_session.unique())
measurement_df = pd.read_sql("SELECT * FROM measurement", conn) $ measurement_df.head()
disag_filename = join(data_dir, 'disag_gjw_CO.hdf5') $ output = HDFDataStore(disag_filename, 'w') $ co.disaggregate(mains,output) $ output.close()
df[df.index.month.isin([5, 6])]['Complaint Type'].value_counts().head()
df2[df2['landing_page'] == 'new_page'].count()[0]/df2.count()[0]
locationing = pd.read_csv('location.csv',names=['Date', 'Text']) $ locationing.Text.replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
sox = sox[pd.to_datetime(sox.date).isin(bad_dates) == False] $ sox = sox[pd.to_datetime(sox.date) >= dt.datetime(2013,1,1)] $ sox.reset_index(drop=True, inplace=True)
plt.figure(figsize=(10,3)) $ plt.plot(one_station['DATE'],one_station['DAILY_ENTRIES'])
df[df.converted == 1]['user_id'].nunique()/df['user_id'].nunique()
np.sum(rfc_feat_sel.feature_importances_ > 0)
df['19740102':'19740104']
print(a) $ a.pop()  # Removes last element $ print(a) $ a.pop(0)  # removes element at index 0 $ print(a)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer() $
c.find_one() # any object
df.iloc[  0:3  ]    # by row number range
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0] $
new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))
def cohort_period(df): $     df['CohortPeriod'] = np.arange(len(df)) $     return df
inspector_station = inspect(engine) $ columns_station = inspector.get_columns('station') $ for column in columns_station: $     print(column["name"], column["type"])
random_sample = dataset_test.sample(8, random_state=123) $ headlines = random_sample["name"]
collection.list_items()
df2[df2['group']=='treatment'].head()
data['Population']  #data.Population works too
val requests = my_stream $     .map(line => (line.split(" ")(2), 1)) $     .reduceByKey((x, y) => x + y)
df0.date = pd.to_datetime(df0.date)
events_df.head(5)
!python get_model.py
horror = X_topics[:, 5].argsort()[::-1] $ for iter_idx, movie_idx in enumerate(horror[:3]): $     print('\nHorror movie #%d:' % (iter_idx + 1)) $     print(df['review'][movie_idx][:300], '...')
df = df[(np.log(df.price)>12) & (np.log(df.price)<14)]
train_norm.head(3)
properati.info()
treatment_but_not_new = df[(df['group'] == 'treatment') & (df['landing_page'] != "new_page")] $ new_but_not_treatment = df[(df['group'] != 'treatment') & (df['landing_page'] == "new_page")] $ dont_line_up_1 = pd.concat([treatment_but_not_new,new_but_not_treatment]) $ print('The number of times the new_page and treatment don\'t line up is:' + str(dont_line_up_1.shape[0])) $ dont_line_up_1.head()
shopping_carts.ndim
data_for_model.head()
old_page_converted = np.random.choice([0,1], size=n_old, p=[1-p_old, p_old]) $ old_page_converted.mean()
popC15[popC15.content == 'photo'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
df.drop(remove_cols, axis=1, inplace=True)
print("total number of records:", len(df)) $ print("\rnumber of records with 0-day waiting interval:", len(df.loc[df['waiting_days']==0]), "\t{0:.3%}".format(len(df.loc[df['waiting_days']==0])/len(df)) ) $ print("\rnumber of records with at least 1 day waiting interval:", len(df[df['waiting_days'] > 0]), "\t{0:.3%}".format(len(df.loc[df['waiting_days']>0])/len(df)) )
league.name.unique()
d = {'col1': red_test[:,0], 'col2': red_test[:,1]} $ red_test_data = pd.DataFrame(data=d) $ test_float['PCA1'] = red_test_data['col1'] $ test_float['PCA2'] = red_test_data['col2'] $ test_float.head()
fraq_volume_m_sel = b_mat.as_matrix() * fraq_volume_m_coins $ fraq_fund_volume_m = fraq_volume_m_sel.sum(axis=1) $
from pandas_datareader import DataReader $ from datetime import date $ from dateutil.relativedelta import relativedelta $ goog = DataReader("GOOG", "morningstar", date.today() + relativedelta(months=-3)) $ goog.tail()
print 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))
nodes.info()
from sklearn.linear_model import LinearRegression $ lm = LinearRegression() $ lm.fit(X,y) $ predictions = lm.predict(X) $ plt.scatter(predictions,y)
sns.kdeplot(pm_final['obs_count'], shade=True).set_title("Distribution of reading counts for turbines")
file1 = '/Users/gta/dev/hw-4/schools_complete.csv' $ file2 = '/Users/gta/dev/hw-4/students_complete.csv'
order_data.head()
df_columns['Day in the year'] = df_columns['Created Date'].str.extract(r"(\d\d/\d\d)/\d\d\d\d ", expand=False) $ df_columns.head() $
y_pred = pipeline.predict(X_test)
maint = pd.read_csv('maint.csv', encoding='utf-8') $ maint.head()
plt.hist(p_diffs) $ plt.axvline(x=obs_diff_conversion, color = 'red'); 
data.isnull().sum()
cell_df = cell_df[pd.to_numeric(cell_df['BareNuc'], errors='coerce').notnull()] $ cell_df['BareNuc'] = cell_df['BareNuc'].astype('int') $ cell_df.dtypes
dpickle.dump(ppm_title, open( "ppm_title.dpkl", "wb" ) )
len(stfvect.get_stop_words())
print('Number of rows in joined = {}'.format(joined.CustomerID.count())) $ joined.head()
df_comment[df_comment.authorName=='The Zeus'].groupby('date').count()
s4 = pd.Series(weights, index = ["colin", "alice"]) $ s4
data.columns
df5 = df[(df['group'] == 'treatment') & (df['landing_page'] == 'new_page')] # there are simpler ways to do this, $ df6 = df[(df['group'] != 'treatment') & (df['landing_page'] != 'new_page')] # but I wanted to preserve the syntax from above. $ df2 = df5.append(df6).reset_index() $ df2.sample(5)
lag_plot(dataSeries) $ pyplot.show()
ab_dataframe['converted'].mean()
BroncosBillsPct['text30'] = BroncosBillsPct['text'].apply(lambda x: x[:30])
Inspection_duplicates = data_FCInspevnt_latest.loc[data_FCInspevnt_latest['brkey'].isin(vals)] $ Inspection_duplicates
dfss = transDF(dfs, stamp_name, header_names, stock_names)
df_new.head(1)
twitter_archive_df_clean.rating.head()
result = pd.DataFrame(list(results), columns=['tags', 'values']) $ result
'5' + 5
bymin.resample("S").bfill()
df_users.to_csv('data/twitter_users.csv', index=False)
pred9 = nba_pred_modelv1.predict(g9) $ prob9 = nba_pred_modelv1.predict_proba(g9) $ print(pred9) $ print(prob9)
from sklearn.linear_model import LinearRegression $ LR = LinearRegression() $ LR.fit(X_train, y_train) $ pred = LR.predict(X_test)
df2 = df.query("group == 'treatment' and landing_page == 'new_page' or group == 'control' and landing_page == 'old_page'") $ df2.head()
y = new["Price"] $ x = new.drop("Price",axis=1)
import os $ import twitter $ from textblob import TextBlob
def clean(num): $
image_predictions_copy = image_predictions_copy[image_predictions_copy.p1_dog == True]
autos.head() #print the dataset after the column labels were updated
nan_samples = cats_df[cats_df.isnull().any(axis=1)] $ cats_df['remove'].iloc[nan_samples.index] = True $ del nan_samples
no_conv, yes_conv = df2.converted.value_counts() $ yes_conv/(no_conv + yes_conv)
large_image_url = browser.find_by_xpath('//*[@id="page"]/section[1]/div/article/figure/a/img')["src"] $ print(large_image_url)
!rm hawaii.sb $ !rm hawaii.sqlite
pd.bdate_range(end=end, periods=20)
keep_types = [u'WWW', u'WND', u'WAS', u'TSN', u'NUC', u'NG', $        u'PEL', u'PC', u'OTH', u'COW', u'OOG', u'HPS', u'HYC', u'GEO'] $ eia_total_annual = eia_total_monthly.reset_index().groupby('year').sum() $ eia_total_annual['index (g/kWh)'] = eia_total_annual['elec fuel CO2 (kg)'] / eia_total_annual['generation (MWh)']
ts.asfreq('B')
stockdftest.head(3)
import os
stock['next_day_open'] = stock.open.shift(-1) $ stock['true_grow'] = stock.target.apply(lambda x: 1 if x >= 0 else 0)
min_vol_port_b
confusion_matrix(y_test,nb_pred[:,0])
df.replace('712-2',51529,inplace=True) $ df[df['zip']==51529].head()
rankings_USA.plot(y ='rank', x = 'rank_date')
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.listdir()
(np.array(null_vals) > obs_diff).mean()
data, config = crowdtruth.load( $     file = "data/person-video-sparse-multiple-choice.csv", $     config = TestConfig() $ ) $ data['judgments'].head()
!$TARGETCALIBPATH/apply_calibration -i refdata/Run17473_r0.tio -p refdata/Run17438_ped.tcal -x
control_convert_rate = df2.query('group == "control"').converted.mean() $ print(' Given that an individual was in the control group, what is the probability they converted is {}'. $       format(control_convert_rate))
session.get("https://demo.emro.info/api/aggregate/tot_1/1" $             ).json()
journalists_retweeted_by_male_summary_df = journalist_retweet_summary(journalists_retweet_df[journalists_retweet_df.gender == 'M']) $ journalists_retweeted_by_male_summary_df.to_csv('output/journalists_retweeted_by_male_journalists.csv') $ journalists_retweeted_by_male_summary_df[journalist_retweet_summary_fields].head(25)
minMaxDate['minDateTS'] = minMaxDate['minDate'].map(lambda x: np.datetime64(x)) $ minMaxDate['diffTS'] = minMaxDate['diff'].map(lambda x: np.timedelta64(x))
len(trading_exemption_records.company_number.unique())
date_range_t = pd.date_range(start_date, end_date, freq = 'SMS') $ date_range_t[:5] $ date_range_t[-5:]
senti_udf = udf(sentiment_extraction, FloatType()) $ subj_udf = udf(subjectivity_extraction, FloatType())
X = np.array(df.drop(['label'], 1)) $ y = np.array(df['label'])
fig_si.savefig('../figs/l23oneneuron10.jpg') $
cog_simband_times[cog_simband_times.index == 'LW-OTS BHC0048-1']
len(player_search_count)
df['Date'] = pd.to_datetime(df['Date'])
with open(marvel_comics_save, mode='w', encoding='utf-8') as f: $     f.write(wikiMarvelRequest.text)
fruits_and_veggies['fruits'][2]
df['date_created'] = pd.to_datetime(df['created_at']) $ df['date_updated'] = pd.to_datetime(df['updated_at'])
weather['ice_pellets'] = weather['ice_pellets'].astype('int') $ weather['hail'] = weather['hail'].astype('int')
plt.hist(null_vals); $ plt.axvline(x=diff, color='r')
df.loc[:4,['id','price_doc']]
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
pres_df['location'].unique()
df = sqlContext.jsonFile("file:/path/file.json")
pageviews_tags.head(2)
from textblob import TextBlob
df_new[['US','UK','CA']] = pd.get_dummies(df_new['country']) $ df_new.head()
df_ad_airings_5['location'].value_counts()
tweet_json.info()
dtm_pos['pos_count'] = dtm_pos.sum(axis=1) $ dtm_pos['pos_count']
%%timeit -n 10 $ for state in df['STNAME'].unique(): $     avg = np.average(df.where(df['STNAME']==state).dropna()['CENSUS2010POP']) $     print('Counties in state ' + state + ' have an average population of ' + str(avg))
print lesson_date + timedelta(hours=1) $ lesson_date - timedelta(days=3) $ print lesson_date + timedelta(days=-3) $ print lesson_date + timedelta(days=368, seconds=2)
taxiData.shape
tweet=results[2] $ for param in dir(tweet): $     if not param.startswith("_"): $         print ("%s : %s" % (param, eval("tweet." + param)))
pair[np.isnan(pair)] = 0 $ pair['ann_return'] = np.cumprod(pair['return_c'] + 1) ** (252/len(pair)) - 1 $ pair['ann_' + stock1] = np.cumprod(pair[stock1 + '_return'] + 1) ** (252/len(pair)) - 1 $ pair['ann_' + stock2] = np.cumprod(pair[stock2 + '_return'] + 1) ** (252/len(pair)) - 1
s.str.strip()
weather.events.unique()
Counter(df2.Language)
open('test_data//open_close_test.txt')
collection.write('AAPL', snap_df, $                  metadata={'source': 'Quandl'}, $                  overwrite=True) $ df = collection.item('AAPL').to_pandas() $ df.tail()
tweet_archive_clean.head(30)
data.groupby(['Year'])['Salary'].sum() $ data.groupby(['Year'])['Salary'].mean()
News_outlets_df['Date'] = pd.to_datetime(News_outlets_df['Date'])
X.columns
tweets.sort_values(by="frequency", ascending=False).head()
image_predictions_clean.info()
tce2.columns
print cust_data1.columns
train_view.sort_values(by=3, ascending=False)[0:10]
result = pd.concat(six) $ free_data = pd.merge(left=free_data, right = result.to_frame(), left_index=True, right_index=True)
cars.loc[cars.yearOfRegistration < 1960].count()['name'] $
[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]
df.dtypes
tw_clean = tw_clean.drop(tw_clean[tw_clean.text.str.contains('RT @')].index)
journalists_retweeted_by_female_summary_df = journalist_retweet_summary(journalists_retweet_df[journalists_retweet_df.gender == 'F']) $ journalists_retweeted_by_female_summary_df.to_csv('output/journalists_retweeted_by_female_journalists.csv') $ journalists_retweeted_by_female_summary_df[journalist_retweet_summary_fields].head(25)
food["created_time"]=food["created_datetime"].apply(time)
uber_14["day_of_year"].value_counts().head()
df['zip'].replace(regex=True,inplace=True,to_replace=r'\D',value=r'')
hello()
row_0
autos['brand'].value_counts(normalize=True)
p_hat = 5. / 8. $ freq_prob = (1 - p_hat) ** 3 $ print("Naive Frequentist Probability of Bob Winning: %.2f" %freq_prob)
compared_resuts = ka.predict(test_data, results, 'Logit') $ compared_resuts = Series(compared_resuts)  # convert our model to a series for easy output
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(new_df, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
df = pd.merge(train, $ transactions[(transactions.transaction_date < datetime.strptime('2017-03-01', '%Y-%m-%d'))], $ on='msno', $
zscore_fun_improved = lambda x: (x - x.rolling(window=200, min_periods=20).mean())/ x.rolling(window=200, min_periods=20).std() $ features['f10'] =prices.groupby(level='symbol').close.apply(zscore_fun_improved) $ features.f10.unstack().plot.kde(title='Z-Scores (Correct)')
df.dropna(axis='rows', thresh=3)
def aggregate_series(s1, s2): $
to_be_predicted_Day1 = 80.95 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
archive_copy['in_reply_to_status_id'] = archive_copy['in_reply_to_status_id'].astype('str') $ archive_copy['in_reply_to_user_id'] = archive_copy['in_reply_to_user_id'].astype('str') $ archive_copy['timestamp'] = pd.to_datetime(archive_copy['timestamp']) $ archive_copy['dog_description'] = archive_copy['dog_description'].astype('str')
ad_source.columns = ['AD_'+str(col) for col in ad_source.columns]
Imagenes_data[(Imagenes_data['p1_dog'] == False) & (Imagenes_data['p2_dog'] ==False ) & (Imagenes_data['p3_dog'] == False )].shape
df_combined.to_csv('twitter_archive_master.csv') $
precipitation_df = pd.DataFrame(precipitation_data, columns=['Date', 'Precipitation']) $ precipitation_df.set_index('Date', inplace=True, ) $ precipitation_df
import pickle $ DATA_PREDICTION_FILE = "../../data/Data_visit-20180107-1" $ tmp = Data['test'][['store_id','visit_date','reservations_prediction']] $ tmp.to_pickle(DATA_PREDICTION_FILE)
df = pd.read_pickle(pretrain_data_dir+'/pretrain_data_02.pkl')
dayofweek.size()
    !rm SIGHTINGS.csv -f $     !wget https://www.dropbox.com/s/iqf7w9xon14du2e/SIGHTINGS.csv
print(df.Opened.min()) $ print(df.Opened.max())
print('Data cleaned at : ', datetime.datetime.now())
regular_posters
res.head(1)
autos.columns
cvec.fit(X_train) $ X_train_matrix = cvec.transform(X_train) $ print(X_train_matrix[:5])
cats_df[cats_df['date of last vet visit'].isnull() == True]
fin_r.index
print additional_limit.shape $ print additional_limit_paid_off.shape $ print additional_limit_outstanding.shape
bnb['timestamp_first_active'][1]
print('EXP(X) = \n', np.exp(fruits))
z_values, labels = create_latent(nn_aae.nn_enc, train_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ plt.scatter(z_values[:,0], z_values[:,1], s=1)
plt.plot(model_output.history['loss'],c='k',linestyle='--') $ plt.plot(model_output.history['val_loss'],c='purple',linestyle='-') $ plt.ylim(0,0.02) $ plt.show;
store_items.count()
state_keys_list = sorted(list(state_DataFrames.keys())) $ state_keys_list
bacteria2[bacteria2.notnull()]
gene_df['gene_id'].unique().shape
%load_ext autoreload $ %autoreload 2
people.eval("weight / (height/100) ** 2 > 25")
net.save_nodes(nodes_file_name='nodes.h5', node_types_file_name='node_types.csv', output_dir=directory_name)
0 * np.nan
cdata.describe()
sess = bp.Session()
toggl = TogglPy.Toggl() $ toggl.setAPIKey(APIKEY) 
%pprint
df_with_metac_with_onc = df_with_metac_with_onc.drop(columns = ['ONC_LATEST_STAGE']) $ df_with_metac_with_onc.head()
total_user_count = ab_df['user_id'].nunique() $ converted_user_count = ab_df.query('converted == 1')['user_id'].count() $ converted_proportion = converted_user_count / total_user_count $
s = oauth.get_session(tokens)
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(doc_duration, params=params, freq=12, title='Doctors Decomposition')
df.head(10)
new_df[['US', 'CA']] = pd.get_dummies(new_df['country'])[['US', 'CA']]
df.to_pickle('Data/NLP.pkl')
for c in ccc[:2]: $     for i in spp[spp.columns[spp.columns.str.contains(c)==True]].columns: $         spp[i] /= spp[i].max()
plt.hist(p_diffs) $ ab_data_diff = prob_treatment_converted - prob_control_converted $ plt.axvline(ab_data_diff , color = 'red');
images_clean.head()
twitter_archive_master = twitter_archive_master.drop(['tweet_id'],axis = 1)
parallel_plot(P)
train_session['isNDF'] = [True if x == 'NDF' else False for x in train_session['country_destination']]
assert 2000 <= len(trump_tweets) <= 4000
twitter_archive_df_clean['rating'] = twitter_archive_df_clean['rating_numerator'] / twitter_archive_df_clean['rating_denominator'] 
dd_df['notRepairedDamage'].fillna('nRD_unknown', inplace=True)
df.info()
cv_score = cross_val_score(prediction_model, x_train_scaled, y_train, cv=cv, scoring='accuracy') $ cv_predict_score = cross_val_predict(prediction_model, x_train_scaled, y_train, cv=cv)
plt.scatter(ort_avg16, drt_avg16);
plt.plot(sample[:, 0], sample[:, 1], ',k', alpha=0.1) $ plt.xlabel('intercept') $ plt.ylabel('slope');
ts.head()
mit.sort_values('num_commits', ascending=False).head(20)
rank_meters['night_0_7'].most_common(11)[1:]
test_df = pd.read_csv("test.csv", dtype=dtypes) $ test_df.head()
twitter_df_clean['timestamp'] = twitter_df_clean['timestamp'].str[:-5]
tweet_image_clean = tweet_image_clean[tweet_image_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']) == True]
train = hn[hn.created_at < july_1].copy() $ new = hn[hn.created_at >= july_1].copy()
("Tue, 11 Feb 2014 15:18:55").split()
microsaccades = MICROSACC.detect_microsaccades(etsamples=etsamples,etmsgs=etmsgs,etevents=etevents)
!curl -L -s "https://dl.dropboxusercontent.com/u/16006464/DwD_Fall2014/Restaurants.xlsx" -o Restaurants.xlsx
s['2013-03-01':'2015-12-12':4]
s.index
root_cell = openmc.Cell(name='root cell') $ root_cell.fill = pin_cell_universe $ root_cell.region = +min_x & -max_x & +min_y & -max_y & +min_z & -max_z $ root_universe = openmc.Universe(universe_id=0, name='root universe') $ root_universe.add_cell(root_cell)
df_session[0:5]
np.sort(top_songs['Country'].unique())
rollcorr_daily[['Ethereum', 'Ripple', 'Bitcoin Cash', 'Litecoin']].plot() $ plt.show()
tmp_df.columns.sort_values()
archive[archive.duplicated]
treatment_df = df2[df2['group']=='treatment'] $ n_treatment = treatment_df.shape[0] $ 1.0*sum(treatment_df['converted']==1)/ n_treatment
draft_df.shape
import pandas as pd $ from mputil import lazy_imap $ from biopandas.mol2 import PandasMol2 $ from biopandas.mol2 import split_multimol2
from dotce.visual_roc import roc_curve, precision_recall_curve
staff.loc[2]
data.head()
print len(com311)
query = select([users]) $ result = conn.execute(query) $ for row in result : $     print(row)
inter = mr * dev $ inter = inter.between_time('10:00', '15:59') $ pd.ols(y=mr, x=inter.tshift(1, 'min'))
CO_accounts.head()
df2 = df[['MeanFlow_cfs','Confidence']] $ df2.head()
plt.hist(commits.log10_commits, bins=50) $ plt.xlim((0.5,3)) $ plt.title('Distribution of Commits per Repo') $ plt.xlabel('Log10(commits per repo)') $ plt.ylabel('Count');
df_new_log['UK_ind_ab_page'] = df_new_log['country_UK']*df_new_log['group_treatment'] #add new column UK_ind_ab_page $ df_new_log['US_ind_ab_page'] = df_new_log['country_US']*df_new_log['group_treatment'] #add new column US_ind_ab_page $ logit_modC = sm.Logit(df_new_log['converted'], df_new_log[['intercept', 'group_treatment', 'country_US', 'country_UK', 'UK_ind_ab_page', 'US_ind_ab_page']]) $ resultsC = logit_modC.fit() $ resultsC.summary()
cities = ['Queens, NY', 'Manhattan, NY' ,'Brooklyn, NY', 'Bronx, NY', 'Staten Island, NY']
(gamma_chart - gamma_chart.shift(1))['Risk'].plot()
n_new=df2.query('landing_page=="new_page"').count()[0] $ n_new
clicks = pd.read_sql_query("SELECT * from clicks"+RetSqlLimit("clicks",sqlLimit), conn) $ clicks['group'] = clicks['ab'].apply(lambda x: x[:1]) $ clicks['session'] = clicks['ab'].apply(lambda x: x[2:])
kfpd.plugin = KDEPlugin(verbose = False) $ fdf = time_method(kfpd, verbose = False, rerun_query = False, repetitions = 10) $ fdf.head()
hm_clean = hm.dropna()
df.user_id.nunique() #return number of unique rows
traded_volumes = [] $ for ele in r.json()['dataset_data']['data']: $     traded_volumes.append(ele[6]) $ print('Average daily trading volume - {}'.format(sum(traded_volumes) / float(len(traded_volumes))))
twitter_final.groupby('time_cat')['length'].mean().reset_index(name="mean").sort_values(by='mean', ascending=False)
print("auc is:", roc)
meal_cuisine_types_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meal_cuisine_types.csv')['Body'].read().decode('utf-8') $ meal_cuisine_types = pd.read_csv(StringIO(meal_cuisine_types_csv_string), header=0, delimiter='|')
df.describe()
df_vow.describe()
features = hourly_df[['Number_of_Tweets', 'Number_of_Users','Mean_Volume','Clean_text']] $ classification_price = hourly_df['Price_Change'] $ classification_open = hourly_df['Open_Price_Change'] $ regression_price = hourly_df['Price_Percent_Change'] $ regression_open = hourly_df['Price_Percent_Open']
print("{0} comments with links".format(len(commenters))) $ print("{0} unique commenters".format(len(set(commenters)))) $ print("{0} of these comments were made by the post author".format(author_links)) $
plt.hist(x)
students.weight.value_counts().sort_index()
model.fit(X, y, epochs=20, batch_size=164)#, callbacks=callbacks_list)
summer[summer['Complaint Type'] == 'Homeless Encampment']['Unique Key'].count()
merkmale.to_clipboard()
QLESQ = pd.read_table("qlesq01.txt", skiprows = [1])
df_loan2=pd.DataFrame(df_loan)
df_characters = df.groupby(['character_id', 'raw_character_text'])['id'].nunique().reset_index().rename( $     columns={'id': 'num_lines'})
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,shuffle=False)
c_df.describe()
age.loc[age == 29]
tbl[tbl.msno == '++5wYjoMgQHoRuD3GbbvmphZbBBwymzv5Q4l8sywtuU=']
df.ID # Shows us the elements inside the column named, "ID"
sel_df.Beat.value_counts(dropna=False)
merge[merge.columns[40:]].head()
learn.unfreeze()
with pd.option_context('display.max_rows', 150, 'display.max_colwidth', 130): $     print(high_freq_words_sr)
wlcomments = pd.read_csv(route+'wlcomments.txt',delimiter='\t',error_bad_lines=False)
html = browser.html $ image_soup = BeautifulSoup(html, 'html.parser')
out_df.to_csv('stacking_gbm_quant.csv', index=False)
import gym.wrappers $ env_monitor = gym.wrappers.Monitor(make_env(),directory="videos",force=True) $ sessions = [evaluate(env_monitor, agent, n_games=1) for _ in range(100)] $ env_monitor.close()
import pandas as pd $ datos=pd.read_csv('data_thinkspeak.csv') $ df = pd.DataFrame(datos) $ df.head() $
weekly = data.resample('W').sum() $ weekly.plot(style=[':', '--', '-']) $ plt.ylabel('Weekly bicycle count');
pystore.get_path()
df.index $
model = Sequential() $ model.add(Dense(12, input_dim=26, activation='relu')) $ model.add(Dense(8, activation='relu')) $ model.add(Dropout(0.2)) $ model.add(Dense(5, activation='softmax'))
test2 = vec2.fit_transform(df.message[1]) $ print(vec2.get_feature_names()) $ test2.toarray()
svr= SVR(kernel='rbf') $ svr.fit(X_train_tfidf, y_train)
len(b_cal_q1)
autos.describe(include='all') #include all to get both categorical(non-numeric) and numeric analyses for any columns
type(t2.tweet_id.iloc[2])
len(df2.query('landing_page=="new_page"'))/len(df2.index)
twitter_archive_master['rating_denominator'][884] = 10 $ twitter_archive_master['rating_numerator'][884] = 13
df_onc_no_metac.head()
x = daily_normals(["05-25", "05-26", "05-27"]) $ x
print("The middle donation amount is: $", donations['Donation Amount'].median()) $
avg_data.head()
data['y'] = y $ data['y_hat'].plot() $ data['y'].plot() $ plt.show() # will be mirrored if loaded
df.rolling(window=3,min_periods=2).aggregate(np.sum) $
auto.loc[auto.Price.isnull(), 'Price'] = auto.groupby(['CarModel', 'CarYear']).Price.transform('mean') #excelov el stugeci, chishta hashvel
plt.hist(p_diffs) $ plt.title('Simulating From the Null Hypothesis') $ plt.xlabel('Difference between p_new and p_old') $ plt.ylabel("Count");
iowa_2015 = iowa.loc[iowa['Year'] == 2015,:] $ total_sales_2015_by_month = iowa_2015.groupby('Month')['Sale (Dollars)'].sum() $ total_sales_2015_by_month.plot(kind = 'bar')
df_new['country_CA'] = df_new['country'].replace(('US', 'UK', 'CA'), (0, 0, 1)) $ lm_ca = sm.OLS(df_new['converted'], df_new[['intercept', 'country_CA']]) $ results_ca = lm_ca.fit() $ results_ca.summary()
support.groupby(["contributor_lastname","contributor_firstname"]).amount.sum().reset_index().sort_values("amount",ascending=False).head(10)
tweet_df.head()
pops = [] $ for country in data_countries.country_destination.values: $     print "country: %s, popul: %d" % (country, sum(data_age_gender_bkts[data_age_gender_bkts.country_destination == country].population_in_thousands.values)) $     pops.append(sum(data_age_gender_bkts[data_age_gender_bkts.country_destination == country].population_in_thousands.values)) $ data_countries["population_in_thousands"] = pops $
set(dat.columns) - set(temp_columns+ incremental_precip_columns+ general_data_columns+ wind_dir_columns)
to_be_predicted_Day1 = 34.69 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df.info() $
csvWriter = csv.writer(csvFile)
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets']))) $ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df2[df2['landing_page']=='new_page']['landing_page'].count()/df2['landing_page'].count()
p = pd.Period('2005', 'A') $ p
sen_dat=twt_data.join(sentiment_data[['class','p_neg','p_pos']])
money_filter = (1 <= y_test) & (y_test < 10.)
plt.style.use('ggplot') $ bb['close'].apply(rank_performance).value_counts().plot(kind='bar')
engine.execute(create_new_table) $
print(condos.UNITTYPE.value_counts()) $ print('\n') $ print(condos.STATUS.value_counts())
aapl['10/5/2017':'10/10/2017']
df_predictions['p1_dog'].value_counts()
ps0 = ps.drop(labels = ["Post patch?", "PCR cycles", "SM_QC_PF", "Bad dates"], axis = 1) $ ps0.head() $
df_enhanced['dog_name'] = df_enhanced['dog_name'].replace(to_replace = r'^([a-z])', value = np.nan, regex = True)
'my string my'.index('ng')
users = api.GetFriends() $ print([u.name for u in users])
labeled_features = final_feat.merge(failures, on=['datetime', 'machineID'], how='left') $ labeled_features = labeled_features.fillna(method='bfill', limit=7) # fill backward up to 24h $ labeled_features = labeled_features.fillna('none') $ labeled_features.head()
print(system_info)
 import datetime $ expiry = datetime.date(2016, 1, 1) $ data = aapl.get_call_data(expiry=expiry) $ data.iloc[0:5:, 0:5]
sing_fam = sing_fam[sing_fam.rp1lndval != 0] $ sing_fam = sing_fam[sing_fam.larea > 0] $ sing_fam.shape
(a + b).describe() $
os.chdir('AV')
store_items.dropna(axis=1)
_train, _test = train[train['date']<separizer], train[train['date']>=separizer]
vs=len(itos) $ vs,len(trn_lm)
pd.read_csv("Data/microbiome.csv", header=None).head()
results.summary()
assert isinstance(my_zip, zipfile.ZipFile) $ assert isinstance(list_names, list) $ assert all([isinstance(file, str) for file in list_names]) $
figure = df_parsed['Open'].plot();
result.summary2() 
df_new['UK'].sum()
bird_data.speed_2d.plot(kind='hist', range=[0,30]) $ plt.xlabel('2D speed (m/s)') $ plt.ylabel('Frequency')
HARVEY_gb_user.get_group('2015gardener').text.values
clean_archive.columns
labels2idx = {label: i for i, label in enumerate(clf.classes_)} $ labels2idx
pd.Series(np.random.randn(1000)).hist(bins=20, alpha=0.4);
to_be_predicted_Day4 = 82.26274673 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
params = {'figure.figsize': [8,8],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(therapist_duration, params=params, freq=31, title='Therapist Decomposition')
def day_of_week(date): $     days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'} $     return days_of_week[date.weekday()] $ day_of_week(lesson_date)
plt.violinplot(resampled1_groups)
df['text'] = df['text'].apply(clean_tweet)
jeff.withdraw(100.0)   # Instruction to withdraw $ jeff.balance           # Shows 900.0
from sklearn.ensemble import RandomForestClassifier $ crf = RandomForestClassifier(n_estimators=1000, max_features=numfeat, max_depth=5) $ print("Parameters used in chosen RF model:\n " , crf.get_params())
for k in range(len(imagelist)): $     send2trash.send2trash(imagelist[k]) $ imagelist = [i for i in os.listdir() if i.endswith(".pdf")  ] $ len(imagelist)
nps.head(8)
sub_df = pd.concat([sub_df, doc_top_mat], axis=1)
tweets.drop(['retweeted','favorited'],1,inplace=True)
df.plot(kind='bar')
small_ratings_data.take(3)
std_for_each_weekday = delineate_in_weekdays.std().reset_index() $ std_for_each_weekday = std_for_each_weekday.rename(columns = {'duration(min)':'std(duration)'}) $ std_for_each_weekday
df_gene['dates'] = df_gene['created_atc'].dt.date $ df_gene.head()
ab_data.group.value_counts()
previous_month_date = end_date - timedelta(days=30) $ pr = PullRequests(github_index).is_closed().get_cardinality("id")\ $                                .since(field="closed_at", start=previous_month_date)\ $                                .until(field="closed_at", end=end_date) $ get_aggs(pr)
departures.head()
round(df2['converted'].mean(), 4)
print("Number of docs in 'uso17': %i" % auso18_size) $ print("Number of docs in 'auso18_qual': %i" % auso18_qual_size)
stars.info() $ owns.info()
with open('data/data_complex.txt', 'r') as f: $     data = f.readlines() $ for line in data: $     print(line)
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2']])
unique_cols=pd.unique(["Country"]+df.columns.values.tolist()) $ unique_cols
import json $ import pandas as pd $ with open('./data/clixo-mapping.json', 'r') as f: $     clixo_map = json.load(f) $ print(len(clixo_map.keys()))
errors['errorID'].value_counts()
cityID = 'b71fac2ee9792cbe' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Sacramento.append(tweet) 
result.sum() $ result.mean() $ result.mean(skipna=False) $ result.describe()
baseball.hr.rank(method='first')
train_data['gearbox_int'] = train_data['gearbox'].apply(get_integer3) $ test_data['gearbox_int'] = test_data['gearbox'].apply(get_integer3) $ del train_data['gearbox'] $ del test_data['gearbox']
x1 = df2[df2['group']=='treatment']['converted'].mean() $ x2 = df2[df2['group']=='control']['converted'].mean() $ difference = x1 - x2 $ proportion = (p_diffs > (difference)).mean() $ printmd("**Proportion is**: {:0.2%}".format(proportion))
historicFollowers = pd.concat([historicFollowers,todaysFollowers[date]],axis=1) #add todays column
print interactionsData.shape $ print itemData.shape $ itemDataWithIntListIDs=pd.DataFrame(sorted(interactionsData['item_id'].unique()),columns=['item_id']).set_index('item_id')
X.shape
def injury(url): $
RegO = pd.to_datetime(voters.RegDateOriginal.map(lambda x: x.replace(' 0:00', ''))) $ Reg = pd.to_datetime(voters.RegDate.map(lambda x: x.replace(' 0:00', ''))) $ datecomp = pd.DataFrame({'OrigRegDate':RegO, 'RegDate':Reg})
r[['optimized_weight', 'weight', 'capital']].head(100)
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import json
pres_df['hour_aired'] = pres_df['start_time'].map(lambda x: int(x.strftime("%H"))) $ pres_df.head(2)
df.drop(df[df.donation_date == '1899-12-31'].index, axis=0, inplace=True)
roll_test_df.columns
regression_line = [(m*x)+b for x in xs]
import pandas as pd
airbnb_df['host_is_superhost'].value_counts(dropna=False)
df2.info()
for feat in ohe_feats: $     feature_barplot(feature=feat)
df_train = pd.read_csv('data/df_1.csv') $ df_test = pd.read_csv('data/df_1_test.csv')
df.info()
classification_model(rfc, X_train_scaled, 1-y_train, X_test_scaled, 1-y_test)
tmin = [normal[0] for normal in normals] $ tavg = [normal[1] for normal in normals] $ tmax = [normal[2] for normal in normals]
SAEMSoup = bs4.BeautifulSoup(SAEMRequest.text, 'html.parser') $ print(SAEMSoup.text[100:1000])
all_data.shape
people.plot(kind = "scatter", x = "height", y = "weight", s=[40, 120, 200]) $ plt.show()
%%R $
data.text.str.contains("snack").resample("1T").sum().plot()
df = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)),columns=['a', 'b', 'c', 'd', 'e']) $ df.loc[0, 'a'] = "This is some text" $ df
df.text[279]
price_data = price_data.iloc[1:]
help(pd)
user_df = user_df.rename(columns={'created_at': 'user_created_at'})
_factors = ['SMB', 'HML', 'RMW', 'CMA', 'MOM','Mkt-RF',] $ index_ts = pd.read_sql("select * from daily_price where ticker in %s and instrument_type='factor'" % str(tuple(_factors)), engine) $
result["Pass"]
df.index = pd.to_datetime(df.index) $ df
import pandas as pd $ import numpy as np $ from sklearn import cluster $ import matplotlib.pyplot as plt $ import matplotlib
pages=pd.read_csv('pages.tsv') $ pages=pd.DataFrame(pages) $ pages.head(10)
True_positive(predicted_table,test)
df_time['tweetRetweetCt'].plot(kind = 'bar') $ plt.xlabel('Time Zone') $ plt.ylabel('Number of Retweets') $ plt.show()
print(df.isnull().sum().all()) $ print('None of the rows have missing values.')
train.target.sum() == data.shape[0]
mu = ret_aapl.mean().AAPL $ sigma = ret_aapl.std().AAPL $ ndays = (datetime(2018,6,15).date()-datetime(2018,3,21).date()).days $ nscen = 10000 $ K = 180
df_stars = df[['business_id','user_id','stars']]
subwaydf.iloc[122449:122454] #this low number seems to be because entries and exits resets
!wget -O FuelConsumption.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv
df["user_name"] = df["user"].apply(lambda x: x["screen_name"])
df_image.sample(5)
encoder_model_inference = extract_encoder_model() $ encoder_model_inference.summary()
plt.figure(figsize=(20,6)); $ plt.plot(df['datetime'],df['MeanFlow_cms'],linewidth=0.5,color='orange') $ plt.ylim((0,700)) $ plt.show;
df.iloc[[1,3,5]]
questions['createdAt'] = pd.to_datetime(questions['createdAt'])
df_test_user = df_users.iloc[0, :] $ df_test_user
ControlTrtmtConverted = df2.query('group == "treatment" and converted ==1')['user_id'].count() $ print("Probability of Treatment Group Converted: ",(ControlTrtmtConverted/ControlGroup)) $ trtmt_conv_rt = (ControlTrtmtConverted/ControlGroup)
channel_namesC = list(topUsers_df["channel_title"]) $ title_namesC = list(topVideo_df["title"]) $ category_namesC = list(topCat_df["category_name"])
recipes.head()
codes = access_logs_df.groupBy('responseCode').count()
pd.date_range('3/7/2012 12:56:31', periods=6, normalize=True)
len(tweets_master_df)
reviews.points.dtype
validation_data.groupby(['last_pymnt_d', 'loan_status']).agg({'member_id':lambda x:x.nunique(0)})
all_tables_df.describe()
dfz.set_index('timestamp', inplace=True)
dates=joined['date'].drop_duplicates(); dates[0]
writer.save()
trend_de = googletrend[googletrend.file == 'Rossmann_DE'] $ trend_de.head()
import re
df_users_6.loc[df_users_6['unit_flag']=='No unit taken so far','cohort']='Browser' $ df_users_6.loc[df_users_6['unit_flag']=='Users started units but did not complete any','cohort']='DriverThru User' $ df_users_6.loc[df_users_6['unit_flag']=='Completed atleast one but less than 10 units','cohort']='Occasional User'
df.shape
sorted_features = reversed(sorted([(index, x) for index, x in enumerate(clf.feature_importances_)], key=lambda x: x[1]))
compare = np.where(data.suma != data.view_count) $ print(compare[-1])
df1['dayofweek'] = df1['effective_date'].dt.dayofweek $ df1['weekend']= df1['dayofweek'].apply(lambda x: 1 if (x>3)  else 0) $ df1['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True) $ Feature1=df1[['Principal','terms','age','Gender','weekend']] $ Feature1=pd.concat([Feature1,pd.get_dummies(df1['education'])], axis=1) $
len(M7_pred),len(M7_actual),len(dfM.DATE[13:-12])
df.dtypes
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns
autos['odometer'] = autos['odometer'].str.replace(',', '').str.replace('km', '') $
id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())
loud_parties = df[df['Descriptor'] == 'Loud Music/Party'] $ loud_parties.groupby(loud_parties.index.hour)['Created Date'].count().plot()
crimes_by_yr_month_type = pd.DataFrame(datAll.groupby([ $     datAll['year'],datAll['month'],datAll['Offense Type']]).agg({'Offense_count':'sum'}))
del staff["Department"] $ staff
len(metadata_list)
df[name]['tweet_id'] $
autos[["date_crawled", $        "last_seen", $        "ad_created", $        "registration_month", $        "registration_year"]].info()
mobile.filter(lambda p: len(p['adapters'])> 1).take(1)
D = 2 $ q_mu = np.float32([ 1., 4.]) $ p_mu = np.float32([-3., 2.]) $ q_sigma = np.ones(D).astype('float32') $ p_sigma = 2.5*np.ones(D).astype('float32')
total_ridepercity=pd.DataFrame(ride_percity) $ total_ridepercity=total_ridepercity.reset_index() $ total_ridepercity
print(diagnosis_DXSUM_PDXCONV_ADNIALL.columns.values) $ print(diagnosis_DXSUM_PDXCONV_ADNIALL['DXCURREN'].unique()) $ dfv = diagnosis_DXSUM_PDXCONV_ADNIALL['DXCURREN'].value_counts(dropna=False) $ print(dfv) $ print(diagnosis_DXSUM_PDXCONV_ADNIALL['DXCURREN'].shape[0])
import examples.simple.backtest_pkg.PipelineControl as pc $ pipe = pc.PipelineControl(data_path='examples/simple/data/fixed_process_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=True) $ pipe.runPipeline()
autos.rename(columns={'odometer':'odometer_km'}, inplace=True) $ autos.rename(columns={'price':'price_$'}, inplace=True)
from sklearn.metrics import f1_score $ from sklearn.metrics import confusion_matrix
autos["date_crawled"].values
print ("Probability that individual was in the treatment group,and they converted: %0.5f" % (df2.query('converted == 1 and group == "treatment"').shape[0]/df2.query('group == "treatment"').shape[0]))
s.iloc[:3]
from profootballref.Parsers import TeamStats $ year = 2015 $ df = TeamStats.TeamStats().defense(year)
cityID = '512a8a4a4c4b4be0' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Charlotte.append(tweet) 
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist(null_vals);
df.to_pickle('/Users/emilytew/Documents/rwanda/Avocado_changed.pkl')
df.rename(columns={'PUBLISH STATES':'Publication Status', 'WHO region':'WHO Region'}, inplace=True) $ df.head(2)
try: $     my_df.to_excel("my_df.xlsx", sheet_name='People') $ except ImportError as e: $     print(e)
autos['date_crawled'] = autos['date_crawled'].str[:10] $ autos['date_crawled'].value_counts(normalize=True, dropna=False).sort_index()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print (z_score, p_value)
df_tweet_clean.rename(columns={'id': 'tweet_id'}, inplace=True)
dic_a = {"a":1,"b":2,"c":3,"d":4} $ ser3 = pd.Series(dic_a) $ ser3
sel = [Measurements.date, func.avg(Measurements.prcp)] $ initial_date = format_latest_date - timedelta(days=365) # This will be start date from 12 months before final date of 8/23/17 $ prcp_data = session.query(*sel).filter((Measurements.date >= initial_date)).group_by(Measurements.date).order_by(Measurements.date).all()
df_goog['good_month'].describe()
!python csv_to_tfrecords.py --csv_input=images/train/train_labels.csv --image_dir=images/train --output_path=train.record
df_ml_58_01.tail(5)
df_tweets.shape[0]
consumer_key = config['twitter']['consumer_key'] $ consumer_secret = config['twitter']['consumer_secret'] $ access_token = config['twitter']['access_token'] $ access_token_secret = config['twitter']['access_token_secret']
my_zip =zipfile.ZipFile(dest_path,'r') $ list_names = [f.filename for f in my_zip.filelist] $ list_names $
sub_df.head()
re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])
Image('https://secure.gravatar.com/avatar/f85b7564fd35d5c86054b95090052d94.jpg?s=192&d=https%3A%2F%2Fa.slack-edge.com%2F7fa9%2Fimg%2Favatars%2Fava_0025-192.png')
import graphlab $ sf_stars = graphlab.SFrame(df_stars) $ sf_stars
re_tokenize_raw = nltk.regexp_tokenize(raw, r"\w+(?:[-']\w+)*|'|[-.(]+|\S\w*") $ print(re_tokenize_raw[100:150])
alg7 = CalibratedClassifierCV() $ alg7.fit(X_train, y_train) $ probs = alg7.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
print(df1.append([df2]))
my_gempro.find_disulfide_bridges(representatives_only=False)
X = dd_df.drop(['price'], axis=1, inplace=False) $ y = dd_df['price'] $ plt.figure(figsize=(10, 10)) $ y.hist(grid=True, bins=50)
prediction_df = pd.DataFrame(predictions, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]) $ combined_df = comment_df.join(prediction_df) # join the comment dataframe with the results dataframe $ combined_df.head(15)
August_July = df["2015-07":"2015-08"] $ August_July['Complaint Type'].value_counts().head(5)
for i in range(len(data_type)): $     print (data_type.index[i],data_type[i])
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s') $ git_log.timestamp.describe()
most_recent_investment.head()
apple["Signal"].plot(ylim =(-2, 2))
typesub2017['Solar'] = typesub2017['Solar'][typesub2017['Solar']!='-'] $ typesub2017['Wind Onshore'] = typesub2017['Wind Onshore'][typesub2017['Wind Onshore']!='-'] $ typesub2017['Wind Offshore'] = typesub2017['Wind Offshore'][typesub2017['Wind Offshore']!='-'] $ typesub2017['Solar'] = typesub2017['Solar'].dropna()
population['California']
perf_train = pd.read_csv('performance_train.csv') $ print(' data shape: ', perf_train.shape) $ perf_train.head()
News_title = Mars_soup.find('div',class_="content_title").a.text $ print(News_title)
git_log = pd.read_csv( 'datasets/git_log.gz', sep='#', encoding='latin-1', header=None, names=['timestamp', 'author'] ) $ git_log.head()
zscore_fxn = lambda x: (x - x.mean()) / x.std() $ features['f09'] =prices.groupby(level='symbol').close.apply(zscore_fxn) $ features.f09.unstack().plot.kde(title='Z-Scores (not quite accurate)')
rodelar = pb.User(commons_site, "Rodelar")
inputs = feables.drop('match_api_id', axis = 1) $ inputs['label'] = inputs.label.apply(lambda x: 1 if x =='Win' else 0) $ labels = inputs['label'] $ features = inputs.drop(['label'], axis = 1)
xmlData.drop('date', axis = 1, inplace = True)
data.groupby('affair').mean() $
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid $ x = df30458.groupby('start_station_name').count() $ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
columns_header = ['id', 'userId', 'createdAt', 'text', 'longitude', 'latitude', 'placeId', $                   'inReplyTo', 'source', 'truncated', 'placeLatitude', 'placeLongitude', 'sourceName', 'sourceUrl', $                  'userName', 'screenName', 'followersCount', 'friendsCount', 'statusesCount', $                  'userLocation'] $ filename = os.path.join('data', 'twex.tsv')
highest_change = r.json()['dataset_data']['data'][0][2] - r.json()['dataset_data']['data'][0][3] $ for ele in r.json()['dataset_data']['data']: $     if ele[2] - ele[3] > highest_change: $         highest_change = ele[2] - ele[3] $ print('Highest change based on High and Low prices - {}'.format(highest_change))
conn.execute("UPDATE bonus_episodes set blog_id=398 WHERE episode_id=24")
samples_query.execute_call('Person', 'Extent')
pd.melt(df, id_vars=['A'], value_vars=['B', 'C']) $
df['intercept'] = 1 $ df[['control', 'treatment']] = pd.get_dummies(df['group'])
corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tf.mm'), corpus_tf) $ corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tfidf.mm'), corpus_tfidf)
engine.execute('SELECT * FROM measurement WHERE DATE > "2016-08-22"').fetchall()
import numpy as np $ import os # To use command line like instructions $ import glob # To read the files in my directory $ from matplotlib import pyplot as plt $ 5+4
df_click = pd.read_csv('data/clicking_data.csv') $ df_click.head()
df_input['created_at'] = pd.to_datetime(df_input['created_at']) $ df_train = df_input.loc[df_input['created_at'] < datetime(2015, 9, 1)].copy() $ display(df_train.head()) $ display(df_train.tail()) $ display(df_train.describe())
idx1 = (df['group'] == 'treatment') & (df['landing_page'] == 'new_page') $ idx2 = (df['group'] == 'control') & (df['landing_page'] == 'old_page') $ df2 = df[idx1 | idx2] $ print('df2 rows', len(df2)) $ print('{} rows removed'.format(df.shape[0] - df2.shape[0]))
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
!bigmatrix_repack --help
xs = np.array([1,2,3,4,5], dtype=np.float64) $ ys = np.array([5,4,6,5,6], dtype=np.float64)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv" $ df  = pd.read_csv(path, sep =',') # df  = pd.read_csv(path) may be too. sep =',' is by deffect. $ df.head(5)
temp_df =  sqlContext.sql("select * from world_bank limit 2") $ print type(temp_df) $ print "*" * 20 $ print temp_df
browser = Browser('chrome', headless=False) $ weather_url = 'https://twitter.com/marswxreport?lang=en' $ browser.visit(weather_url) $ time.sleep(1)
df1=pd.concat([df['author'], df['edited'].sub(df['dated'])], axis=1) $ df1.columns=['author','Duration'] $ df1 = df1[df1.Duration.notnull()] $ df1.nsmallest(5, 'Duration')
print(pd.value_counts(train_df['app'])[:20])
autos["odometer_km"].value_counts().sort_index(ascending=False).head(15)
plot_chernoff_data(df1, 1e-6, 1, "Nth = 1.0") $ plt.savefig('../output/g_perr_vs_M_1.pdf', bbox_inches='tight')
tweets.head()
holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00102032') ]
ac['Registration Date'].groupby([ac['Registration Date'].dt.year]).agg('count')
fig, ax = plt.subplots() $ typesub2017['Solar'].plot(ax=ax, title="Solar Energy Generation 2017 (DE-AT-LUX)", lw=0.7) $ ax.set_ylabel("Solar Energy") $ ax.set_xlabel("Time")
df_data_1.describe(include = 'all')
lm.score(x_test, y_test > 0),np.mean(y_test > 0)
df[(df["compound"]==0)].groupby("newsOutlet").count()
for inst in idx_set: $     with open('../notebooks/subsample_idx_{}.json'.format(inst), 'w') as fd: $         json.dump(list(idx_set[inst]), fd, indent=2) $
print(len(auto_new.CarModel.unique())) $ print(auto_new.CarModel.unique())
reviews.groupby('variety').price.agg([min,max]).sort_values(by=['min','max'], ascending=False)
df.groupby(by=['landing_page', 'group']).count() $
forecast_df['Under 10', '10 to 19', '20 to 29', '30 to 39', '40 to 49', '50 to 59', '60 to 69', '70 to 79', '80+'] = 0 $ for ind, row in SANDAG_age_df[SANDAG_age_df['AGE_RANGE'] == '10 to 19'].iterrows(): $     forecast_df.loc[ind, 'under_20'] = row['POPULATION'] $ forecast_df.head()
smpl = contest_data_sample.select('end_customer_party_ssot_party_id_int_sav_party_id','sales_acct_id','decision_date_time') #Randomly picking few enteries
run txt2pdf.py -o '2018-06-22 2013 FLORIDA HOSPITAL Sorted by payments.pdf'  '2018-06-22 2013 FLORIDA HOSPITAL Sorted by payments.txt'
resource_id = hs.createHydroShareResource(title=title, content_files=files, keywords=keywords, abstract=abstract, resource_type='genericresource', public=False)
filtered_events = events.where( col("user_id").isNotNull() ) $ events_grouped_no_nulls = filtered_events.groupBy(["user_id", "event_date"]).count()
lsi_out.head()
train['diff_lng'] = train['diff_lng'].map(lambda x: abs(x)) $ train['diff_lat'] = train['diff_lat'].map(lambda x: abs(x)) $ test['diff_lng'] = test['diff_lng'].map(lambda x: abs(x)) $ test['diff_lat'] = test['diff_lat'].map(lambda x: abs(x))
youthUser1['demographicInfodob'] = youthUser1['demographicInfodob'].fillna(0) $ youthUser1['demographicInfodob'] = youthUser1['demographicInfodob'].apply(date) $ youthUser1.describe()
sql.registerDataFrameAsTable(df, 'scen')
weather_data1 = pd.read_csv('201402_weather_data.csv'); weather_data1.head()
n_new = len(df2.query('landing_page == "new_page"')) $ n_new
np.median(shows['first_year'].dropna())
withDups.ix['2012-01-03']
my_followers = [str(user_id) for ids_list in $                 tweepy.Cursor(api.followers_ids, $                               screen_name="NoahSegalGould").pages() $                 for user_id in ids_list]
user.query("followers_count > 100 and friends_count > 200").head(3)
rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)
tweets_df = tweets_df[tweets_df.userTimezone.notnull()] $ len(tweets_df) $
skip_idx.sort()
from seq2seq_utils import Seq2Seq_Inference $ seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp, $                                  decoder_preprocessor=title_pp, $                                  seq2seq_model=seq2seq_Model)
for row in my_df_large.itertuples(): $     pass $
plt.bar([1,2,3,4,5,6,7,8,9,10,11,12],monthly_residuals) $ plt.xlabel('Month of 2017') $ plt.ylabel('Monthly Residuals')
for p in paragraphs: $     print(etree.tostring(p, pretty_print=True, method='html'))  
dates
def text_normalize(otherfunction, *args, **kwargs): $
df['out']=pd.Series(diff,dtype=int) # creating dataframe using input and output $ df
classifier = MultinomialNB() $ classifier.fit(train_features_tokenized, authors_train) $ test_features_tokenized = vectorizer2.transform(articles_test) $ classifier.score(test_features_tokenized, authors_test)
tweets_clean.head()
null_vals=np.random.normal(0,p_diffs.std(),p_diffs.size)
be = CustomBusinessDay(weekmask = 'Sun Mon Tue Wed Thu', holidays = ['2018-08-01']) $ be
trump.text.describe()
noNulls.show(5)
ethc = ethc[(ethc.w_sentiment_score != 0.000)] $ ethc = ethc.reset_index(drop=True) $ print(ethc)
pd.Series({'a': 42, 'b': 13, 'c': 2})
pd.Series(dates)
locations = session.query(Measurement).group_by(Measurement.station).count() $ print(locations)
display('df6', 'df7', "pd.merge(df6, df7, how='outer')")
df1['State'].replace('PCS', 'PCS-opt', inplace=True) $ df2['State'].replace('PCS', 'PCS-etgl', inplace=True)
senateAll = senateAll.set_index('TLO_id') $ houseAll = houseAll.set_index('TLO_id')
df2[df2.duplicated(["user_id"])]
archive_clean['rating_denominator'].value_counts()
titanic.shape
m.fit(lr, 3, metrics=[exp_rmspe], cycle_len=1)
learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42)
authors_count.sort_values('count', ascending=False).head()
final = cryptoprice.join(txt, how = 'outer')
df_data = pd.read_csv(CSV_FILE, dtype=str) $ print '%d rows' % len(df_data) $ df_data.head()
clean_rates = we_rate_dogs.merge(tweet_fav_counts, how='inner', on='tweet_id').copy()
top_songs['Position'].dtype
df = tables[0] $ df.columns = ['', 'Mars'] $ df.set_index('', inplace=True) $ df
control_cnv = df2.converted.mean() $ control_cnv
logit_mod3=sm.Logit(df_new['converted'],df_new[['ab_page','intercept', 'CA', 'UK', 'CanadaNew','UKNew']]) $ fit3=logit_mod3.fit() $ fit3.summary()
unassembled_human_genome_length = gdf[gdf['type'] == 'supercontig'].length.sum() $ percentage_incomplete = (unassembled_human_genome_length / human_genome_length)*100 $ print("{}% of the human genome is incomplete.".format(round(percentage_incomplete, 4)))
df["loc"] = df["start station latitude"].astype(str)+","+" "+df["start station longitude"].astype(str)
ab_new = countries_df.set_index('user_id').join(ab_file2.set_index('user_id'),how='inner') $ ab_new.head()
trunc_df = trunc_df.drop(trunc_df.columns[[0]],1)
import os $ import glob $ from ramutils.reports.summary import FRStimSessionSummary $ fr5_session_summary_locations = glob.glob(os.path.join(report_db_location, '*FR5*session_summary*')) $ print("Found {} FR5 or catFR5 session summaries".format(len(fr5_session_summary_locations)))
to_be_predicted_Day1 = 54.45 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
for subreddit in reddit.subreddits.default(limit=None): $     print(subreddit)
autos = autos[autos["price_euro"].between(0,351000)] $ autos["price_euro"].describe()
X_extra.dtypes
retweets_of_me = api.retweets_of_me()
df_ml_53_01.tail(5)
lq = lq.dropna(how = 'any')
import pickle $ def pickle_data(data, outputfilename): $     with open(outputfilename, 'wb') as handle: $         pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL) $     print("m-lab data saved to ",outputfilename)    
gapminder = pd.concat([country, g18, g19, g20], axis = 1) $ print(gapminder.shape) $ print(gapminder.head()) $
df_2018 = df['2018-07-31':'2018-01-01']
x_train, x_test, y_train, y_test = train_test_split(x, y , test_size=0.4, random_state=2)
dti.freq
proportion_and_count(active_psc_controls,'nature_of_control',len(active_psc_controls)).head(10)
df2['intercept'] = 1 # adding intercept column $ dummy = pd.get_dummies(df2['group'],drop_first=True) ## creating dummy $ dummy['ab_page'] = dummy['treatment'] $ df2 = df2.join(dummy['ab_page']) # joining dummy column to dataframe
tips.sort_values(["tip","size"],ascending=False).head(10)
df.head()
sel = [Measurements.date, Measurements.prcp] $ initial_date = format_latest_date - timedelta(days=365) # This will be start date from 12 months before final date of 8/23/17 $ prcp_data = session.query(*sel).\ $     filter((Measurements.date >= initial_date)).all() $ prcp_data[:10]
dtc.predict(X)
city_loc.append(city_pop)
import pandas as pd $ data = pd.read_csv('../../assets/dataset/rossmann.csv', skipinitialspace=True, $                    low_memory=False)
poverty_data.head()
search['trip_end_loc'] = search['message'].apply(trip_end_loc)
df = df.reset_index() $ df.irlco.sum()
lr_all_user[lr_all_user['indebt:actual'].notnull()].to_clipboard()
gjw.store.window = TimeFrame(start='2015-09-03 00:00:00+01:00', end='2015-09-05 00:00:00+01:00') $ gjw.set_window = TimeFrame(start='2015-09-03 00:00:00+01:00', end='2015-09-05 00:00:00+01:00') $ elec = gjw.buildings[building_number].elec $ mains = elec.mains() $ mains.plot() $
len(features)
df2.drop_duplicates(subset='user_id',keep='first',inplace=True)
df_final = pd.concat([df_big, df_small])
data=part.resample('5T').median() $ plt.plot(data['field4']) $ plt.show()
arrPriceList.head(1)
details.sort_values(by='Released')
cm_lr = confusion_matrix(y_final, lr_predicted)
pred = rfbestgini.predict(cvecogXfinaltemp) $ print classification_report(pred, ogy) $ print confusion_matrix(ogy, pred) $ print "cross val score accuracy :"+str(sum(cross_val_score(rfbestgini, cvecogXfinaltemp, ogy))/3) $ print 'roc_auc score :'+str(sum(cross_val_score(rfbestgini, cvecogXfinaltemp, ogy, scoring='roc_auc'))/3) $
orgs
xml_in_sample1.shape
tweet_full_df.info()
X_testfinaltemp = X_testfinal.copy() $ X_testfinaltemp = X_testfinaltemp.rename(columns={'fit': 'fit_feat'})
total_stations = session.query(Station.station).distinct(Station.station).count() $ print('Total number of stations is ' + str(total_stations))
new=new[new['created_year']>=2016]
learn.load_encoder('lm1_enc')
datay = data.loc[:,['winner_of_match', 'player_team_name', 'match_id']]
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05/2))) $
store_items = store_items.drop(['watches', 'shoes'], axis = 1) $ store_items
tweets_master.to_csv('tweets_master.csv', index = False) $ tweets_i_master.to_csv('tweets_i_master.csv', index = False)
yt.get_subscriptions(channel_id, key)
from tempfile import mkstemp $ fs, temp_path = mkstemp("gensim_temp")  # creates a temp file $ model.save(temp_path)  # save the model
!echo "foo foo quux labs foo bar quux" > text.3.2.txt
df.index.month
srcdf = srcdf.merge(md, on='subject', how='left') $ srcdf
df = pd.read_sql('SELECT cust_id, first_name, last_name, member_credit FROM customer WHERE cust_id=8', con=conn_b) $ df
score_0.shape[0] / score.shape[0]
msft['2012-02'].head(5)
5/9*(86.53-32)
member_table.to_sql(name='member_events', con=con, if_exists='append', index=False)
%%bash $ cd /data/LNG/Hirotaka/ASYN $ awk '!/NA|SNP/{print $0 | "sort -gk6"}' PPMI1_all|  head
NoOfRows = df.shape[0] $ NoOfRows
df_unit.head()
flight.select("arr_time").show(2, truncate=False)
m.fit(lr, 5, metrics=[exp_rmspe], cycle_len=1)
df_archive["floofer"].value_counts()
r_aux=[] $ for i in range(0,l2): $     if i not in seq: $         r_aux.append(resilience[i]) $ col.append(np.array(r_aux))
my_gempro.get_sequence_properties()
nt["Date"] = pd.to_datetime(nt.date, format="%Y-%m-%d", errors='ignore')
import statsmodels.formula.api as smf
df_2017 = pd.DataFrame(rows)
from scipy.stats import norm $ print('CDF of Z-Score: ', norm.cdf(z_score)) $ print('Z-Score: ', z_score, ' < ',norm.ppf(1-0.05))
g8_aggregates.columns
train_small_data = dtype_transform(train_small_data) $ val_small_data = dtype_transform(val_small_data) $ test = dtype_transform(test)
dfdummy = pd.get_dummies(data = df_countries, columns=['country'], drop_first = True) $ df4 = dfdummy.merge(df3, on='user_id') $ df4 = df4[['user_id', 'timestamp', 'group', 'landing_page', $            'ab_page', 'country_UK', 'country_US', $            'intercept', 'converted']]
url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars' $ browser.visit(url)
train_df = pd.read_csv('../input/train_cont.csv') $ test_df = pd.read_csv('../input/test_cont.csv')
new_dems.drop(new_dems.index[0], inplace = True) $ new_dems.reset_index() $ new_dems.head()
print("Number of rows: %s" % (len(excel_data)))
df['name']
validate_smape(CVIterator, last_15_days_pred)
retention.set_index(['CohortGroup', 'CohortPeriod'], inplace=True)
import scipy
plt.plot(df['yearOfRegistration'],df['price'],'.') $ plt.show()
sess.get_data('ibm us equity',['px last','px open','px high','px low'])
statetotals = pd.DataFrame(locations['State'].value_counts()) $ top20 = statetotals[0:20]
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
df_con.shape
tag_df.head()
tokens = getTokens() $ corpus = list(tokens.get(df['desc']))
rf = RandomForestClassifier(min_samples_leaf=10, max_features='sqrt', n_estimators=500, random_state=10) $ rf.fit(X_train, y_train) $ val_pred_probs = rf.predict_proba(X_val) $ val_pred = rf.predict(X_val)
import pandas
len(graffiti2[graffiti2['precinct']==1.0])
cig_data.columns
sorted = grouped.sort( grouped['count'].desc() ) $ sorted.show()
data_after_subset_filter = lv_workspace.get_filtered_data(level=1, subset='A') # level=0 means first filter $ print('{} rows mathing the filter criteria'.format(len(data_after_subset_filter))) $ data_after_subset_filter.head()
autos["postal_code"].value_counts()
oModel.summary()
test_words = cv.transform(fb_test.message)
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
firstWeekUserMerged = firstWeekUserMerged.drop(["gender","birth_year"], axis=1) $ firstWeekUserMerged.isnull().sum()
to_be_predicted_Day3 = 83.26256668 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
reflClean = reflRaw.astype(float) $ reflClean
non_align_1 = df.query('group == "treatment" and landing_page != "new_page"') $ non_align_2 = df.query('group != "treatment" and landing_page == "new_page"') $ non_align = non_align_1.count()[0] + non_align_2.count()[0] $ print('{} number of times new_page and treatment do not line up'.format(non_align))
filter_df = filter_df[filter_df['race'] == 'PRES'] $ filter_df.head(2)
dfRegMet =pd.read_pickle(data + "dfRegMet.p")
import requests $ import json
data[1:3]
type(pd.crosstab(index=mydf.comp, columns='counter'))
df.columns[non_null_counts < 1000]
twitter_archive_master.groupby('has_name')['rating'].mean()
df_only_headline[df_only_headline["tags"].apply(lambda x: "nlp" in set(x))].groupby(["headline", "date"]).size()
pnewnull = df2.query('converted == 1').shape[0] / df2.shape[0] $ pnewnull
print('WITH resetting the random seed:') $ print('%d different bacteria between the two function calls' % len(set(dd.feature_metadata.index)^set(dd2.feature_metadata.index)))
u235_scatter_xs = fuel_xs.get_values(nuclides=['(U235 / total)'], $                                 scores=['(scatter / flux)']) $ print(u235_scatter_xs)
data_url2 = datasets2[0],datasets2[2] $ data_url2
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23Trump').items(2000): $     results.append(tweet.text) $ print len(results)
names(field_importance_ensemble_2)
import pandas as pd $ import datetime $ import numpy as np
gs_from_model.score(X_train, y_train_over)
df.isnull().sum()/len(df)
print(df2['landing_page'].value_counts())
'xx'.split('x')
fig, ax = plt.subplots() $ price2017['DE-AT-LUX'].plot(ax=ax, title="Market Price 2017 (DE-AT-LUX)", lw=0.7) $ ax.set_ylabel("Market Price") $ ax.set_xlabel("Time")
print("Dimensions used by time:\t\t%s" % d.variables['time'].dimensions) $ print("Dimensions used by SATCTD7229_PRES:\t%s" % d.variables['SATCTD7229_PRES'].dimensions)
print("There are {} in this collection of Tea Party tweets".format(len(df)))
autos["fuel_type"].unique()
print(len(pbptweets['text'])) $ print(len(pbptweets['text'].drop_duplicates()))
from multiprocessing import Pool $ import time $
most_active_stations = session.query(Measurement.station, func.count(Measurement.station)).\ $         group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()0 $ print (most_active_stations)
temp_df[temp_df.total_companies > 100].proportion_no_psc.describe()
df1 = data_df.Count.resample('W').sum() $ df = df1.copy() $ df = data_df.Count
horror_readings=m1[m1['category_one'] =='horror'] $
%%sql $ UPDATE facts $ SET Created_Date_key = hour.hour_key $ FROM hour $ WHERE hour.hour  = TO_CHAR(facts.Created_Date, 'YYYY-MM-DD HH24:00:00')
control_df=df.query('group=="control"') $ uuc_old=control_df.query('landing_page=="old_page"').user_id.nunique() $ uuc_new=control_df.query('landing_page=="new_page"').user_id.nunique() $ print("Number of unique users in the control group landed in old page is :{}".format(uuc_old)) $ print("Number of unique users in the control group landed in new page is :{}".format(uuc_new))
ol = cb.organization('origami-logic')
def get_tuple(series): $     return tuple(series) $ grouped_authors_by_publication = xml_in_sample.groupby(['publicationKey'], as_index=False)[['authorName', 'authorId']].agg({'authorName': get_tuple, $                                                                                                                             'authorId': get_tuple})
futures_data.info()
fashion['created'] = pd.to_datetime(fashion.created) $ fashion.set_index('handle', inplace=True) $ fashion.dtypes
archive_copy.to_csv('twitter_archive_master.csv')
log_mod_countries = sm.Logit(df_new['converted'],df_new[['intercept','US','UK']]) $ results_countries = log_mod_countries.fit() $ results_countries.summary()
answer1[0]
print(len(tweets_total))
if sim: $     test = make_sim('p039565') $     train = train.drop('p039565',axis=0) $     del make_sim
twitter_archive[twitter_archive['rating_denominator'].astype(int)>10] $
events_df['event_time'].max(),events_df['event_time'].min()
df.head()
lagged.at_time('9:30')
frame2.year
titanic.describe()
propnames = props.prop_name
by_time = data.groupby(data.index.time).mean() $ hourly_ticks = 4 * 60 * 60 * np.arange(6) $ by_time.plot(xticks=hourly_ticks, style=[':', '--', '-']);
oppstage.columns = ['lead_mql_status', 'opportunity_month_year', 'opportunity_stage', 'counts']
airbnb_df['host_acceptance_rate'] = np.random.randint(70,100, size=(airbnb_df.shape[0],1)) $ airbnb_df['host_response_rate'] = np.random.randint(70,100, size=(airbnb_df.shape[0],1))
shifted_leaderboard = df.groupby(['batter_name', 'batter', 'bats'])['is_shift'].agg(['mean', 'count']) $ shifted_leaderboard.loc[shifted_leaderboard['count']>1500,].sort_values('mean', ascending=False).head(15)
print_return_stats()
df_vow['Year'] = df_vow.index.year $ df_vow['Month'] = df_vow.index.month $ df_vow['Day'] = df_vow.index.day
indeed = pd.read_pickle('indeed.pickle') $ tia = pd.read_pickle('tia.pickle')
type(logmodel_predictions)
table
transformed_df = data.transform_data(raw_df) $ transformed_sample_df = transformed_df.sample(4, random_state=42) $ transformed_sample_df[['Name', 'Rating'] + data.simple_features].T
len(df)
(Hashtags_list[3]).head(10)
df.isnull().any(axis=1).sum()
vals = data.value.copy() $ vals[5] = 1000 $ vals
print(autos['last_seen'].str[:10].value_counts(normalize = True, dropna = False).sort_index()) $ print(autos['last_seen'].str[:10].value_counts(normalize = True, dropna = False).shape)
len(ben_final['userid'].unique())
df = pd.read_json("C://employee.json")
df_test_index_2 = event_list $ df_test_index_2 = pd.merge(df_test_index_2[event_list['event_start_at'] > df_test_user_2['created_on']], $                             log_user2[event_list['event_start_at'] > df_test_user_2['created_on']], on='event_id', how='left') $ df_test_index_2
pres_df['ad_length'] = pres_df['ad_length_tmp']
neg_tweets = ioDF[ioDF.all_sent_x < -.5]
important_tweets
ax = users.created_at.hist(bins=4380) $ ax.set_xlabel('Date') $ ax.set_ylabel('# Users') $ ax.set_title("Users' account creation per day")
p.crs
df_kws = df.set_index('datetime').resample('1m').aggregate('sum')
token_sendReceiveAvg_month.level =  token_sendReceiveAvg_month.level.fillna("null_level")
NYPD_df = df[df['Agency'] == 'NYPD'] $ NYPD_df.head(10) $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ print(z_score, p_value)
hs.columns
autos.info()
display(observations_ext_node[DATA].dropna().head(9))
len(df2.loc[(df2['group'] == 'control') & (df2['converted'] == 1)]['user_id'])/len(df2.loc[df2['group'] == 'control']['user_id'])
df_users_6_after.shape
data.tasker_id.value_counts().head()
! ls  | grep --fixed-strings --file samples_with_signatures.txt -v | grep log | xargs tail -n 20 | head -n 50
predictions_final=predictions_final.select(col('user_id').alias('p_user_id'),col('repo_id').alias('p_repo_id'),'score') $ test_forks=test_set.filter(test_set.event=='fork') $ test_join=test_forks.join(predictions_final,(test_forks.user_id==predictions_final.p_user_id)\ $                    & (test_forks.repo_id==predictions_final.p_repo_id),'left')
df = pd.read_csv(fileIn, encoding = 'utf-8') $ df.head()
pop_cat.describe()
n_old = total_pages - n_new $ n_old
archive_copy.head(1)
plt.hist(p_diffs) $ plt.title("sample distribution for the conversion rate") $
well_data.drop(columns=['api number', 'water_bbl'], inplace=True) $ print(well_data.head())
learner.unfreeze()
X_train_1 = np.append( np.ones((X_train.shape[0], 1)), X_train, axis=1) $ X_test_1 = np.append( np.ones((X_test.shape[0], 1)), X_test, axis=1) $ X_train_1[0:5]
new_pages = df2.query('landing_page == "new_page"').user_id.count() $ new_pages/tc
unique_users_2
twitter_archive_master.head()
pprint("Company with the most number of job ads for the past 2 years: Mentor Graphics Development Services CJSC - 64")
df_joy = pandas_from_hashtag('joy', 10) $ df_joy.head()
vol = vol.replace(0, 1) $ vol.describe()
print(df['Borough'].value_counts(dropna=False))
brand_price = {} $ for brand in brands: $     mean_price = autos['price'][autos['brand']==brand].mean() $     brand_price[brand] = int(mean_price) $ brand_price
df = pd.DataFrame(results) $ df = df.rename(columns={0: 'text'})
plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments) $ plot.set_ylim([-10, 10])
for i, x in data.iterrows(): $     if (x[8] == 1) and (compute_score(x[3]) > 0.05): $         print x[0]
def sharpe(returns, freq=30, rfr=0): $     peak = returns.max() $     i = returns.argmax() $     trough = returns[returns.argmax():].min() $     return (trough-peak)/trough
tweetsDf.lang.hist()
daily_unit_df = all_turnstiles.groupby(['STATION','C/A','UNIT','DATE'], as_index=False).sum().drop(['ENTRIES','EXITS'], axis=1) $ daily_unit_df.sample(5)
tt_json_df.info()
pd.to_datetime("03/28/2018") + -3*bday_us
sales.head(1)
p_diff = new_page_converted.mean() - old_page_converted.mean() $ p_diff
bonus_points
df3 = df2.join(country_df.set_index('user_id'), on = 'user_id') $ df3.head()
properati[properati['place_name'] == "Bs.As. G.B.A. Zona Norte"]
flightv1_1.where(flightv1_1.trip == 1).show(2)
plt.style.use('default')
type(df.date[0])
import pyLDAvis.gensim
(p_diffs - diff).mean()
ip = pd.read_csv('image-predictions.tsv', sep = '\t')
df['polarity'] = polarity $ df['negative'] = negative
from biopandas.mol2 import PandasMol2 $ pmol = PandasMol2().read_mol2('./data/1b5e_1.mol2')
for i in dir(pandas): $     if i.startswith("read"): $         print i
TrainData.drop(['DOB', 'Lead_Creation_Date'], axis=1).to_csv('TrainData_clean.csv', index=False) $ TestData.drop(['DOB', 'Lead_Creation_Date'], axis=1).to_csv('TestData_clean.csv', index=False)
tw['tweet_id'].map(lambda x: len(str(x).strip())).value_counts()
dat2.sort_values(by='tweet.created_at', ascending=False).head(10)
basic_pred = unscale(y_pred_basic, scaler, template_df, toint=True) $ complex_pred = unscale(y_pred_complex, scaler, template_df, toint=True) $ real = unscale(y_real, scaler, template_df, toint=True)
df.set_index('UPC EAN', append=True, inplace=True) $ df.head(3)
df['A']
r = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv') $ code = r.status_code $ print(code) $ open('image-predictions.tsv', 'wb').write(r.content) $
a = pd.DataFrame(np.array([[34,24,45,42,53],[12,45,13,67,23]]).T,columns=['a','b'])
df_arch_clean.info()
df_drug_counts.to_csv('../../static/dash_data/named_drugs.csv')
baseball.corr()
! ls ./data/raw-news_tweets-original/dataset1/tweets/2014-11-18/
confusion_matrix(y_test,lr_balanced_pred)
x_test = test_data.values
new_page_converted  = np.random.choice([1, 0], size=n_new, p=[convert_p_new, (1-convert_p_new)])
d = datetime.datetime(2018, 2, 18)
df_group1=df.query("landing_page=='old_page'and group=='control'")
pred = stacking(train_X,train_y,test_X,nClass=3,nFold=9)
import pandas as pd $ items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']), $          'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])} $ print(type(items))
svm_clf = SVC(decision_function_shape="ovr") $ svm_clf.fit(X_train_scaled[:10000], y_train[:10000])
print(autos.groupby('offerType').size())
fruits[['apples', 'oranges']] * 2
print(model.predict_output_word(['emergency', 'beacon', 'received']))
census_tracts_df.head()
inspector = inspect(engine) $ inspector.get_table_names()
df.source[:5]
df.info()
df = df.merge(nycshp, on="ZIPCODE")
loaded_keras_entity_recognizer = KerasEntityExtractor.load(pipeline_zip_file) $ predictions = loaded_keras_entity_recognizer.predict(df_test) $ evaluator2 = loaded_keras_entity_recognizer.evaluate(predictions) $ evaluator2.get_metrics('phrase_level_results')[['Recall', 'Prec.', 'F-score']]
import pandas as pd $ import pulp $ import matplotlib.pyplot as plt $ from IPython.display import Image
df_2018 = df[(df.year == 2018)]
mask = (nullCity["creationDate"] > '2017-01-01') & (nullCity["creationDate"]<= '2017-12-31') $ nullCity2017 = (nullCity.loc[mask]) $ nullCity2017.head()
s4.count()
session = Session(engine)
michaelkorsseries.sum()
df = pd.read_csv('ab_data.csv', sep=",") $ df.head()
p(sys.getfilesystemencoding)
res = requests.get(url, headers=headers)    # Sending an HTTP request with URL and the Requesting agent as header
p_treatment_converted = df2.query('group == "treatment"')['converted'].mean() $ p_treatment_converted
print(rf_gd.best_params_, rf_gd.best_score_)
type(s1), s1.dtype, len(s1), s1.shape
arr_size = reflClean.shape $ arr_size
plt.hist(taxiData.Trip_distance, bins = 50, range = [0,10]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
print(y.shape) $ print(X.shape)
df_goog.Date $ type(df_goog.Date.loc[0])
mars_weather = public_tweets[0]["text"] $ print(f"Most Recent Tweet: {mars_weather}")
pd.Series(0, index=days)
sensors_num_df.entities
if range_chart_lower_control_limit < 0: $     range_chart_lower_control_limit = 0
full_df['ymd'] = full_df['created'].map(lambda x: x.strftime('%Y-%m-%d'))
future = m.make_future_dataframe(periods=90) $ future.tail()
f_os_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_os_hour_clicks"), header=True) $ print('Found %d observations.' %f_os_hour_clicks.count())
autos['price'].describe()
df_a.join(df_b, how = "left") # same as above (default)
data = Dict['data'] $ print(type(data))
twitter_df[['rating_numerator','rating_denominator']].describe()
df2
features['f16'] = features['f05'].apply(np.sign) $
grouped_publications_by_author.columns = grouped_publications_by_author.columns.droplevel(0) $ grouped_publications_by_author.columns = ['authorId', 'authorName','publicationTitles','authorCollaboratorIds','authorCollaborators','countPublications','publicationKeys','publicationDates']
s_t = np.sqrt(((n1-1)*n1*sd1+(n2-1)*n2*sd2)/(n1+n2-2)) $ t = (m2-m1)/(s_t*np.sqrt(1/n1+1/n2)) $ tscore = stats.t.ppf(.95,n1+n2-2) $ print("t stats is {0}; 95% t score is {1}".format(t,tscore))
date1 = datetime(2014,12,2) $ date2 = datetime(2014,11,28) $ date1 - date2
secclintondf = clintondf[mask_H] $ secclintondf = secclintondf.reset_index(drop=True)
for x in tweets_clean.dog_class.unique(): $     print('Mean retweets for ' + str(x) + ' class:' + str(tweets_clean[tweets_clean.dog_class == x].retweet_count.mean()))
first_qtr = df['time_open'].quantile(q=0.25) $ print(first_qtr)
df.query("group == 'treatment' and landing_page == 'old_page'").count()[0] + df.query("group == 'control' and landing_page == 'new_page'").count()[0] $
df = pd.read_csv('data/btc-market-price.csv', header=None)
classify_df.head()
daily_hashtag.show(5)
tuna_neg = mapped.filter(lambda row: row[4] < 0) $
countries_df.shape
v=sns.factorplot(data=tweets_df, x="name", y="favorite_count", kind="box") $ plt.xticks(rotation=60) $ v.set(yscale="log")
tables = pd.read_html(facts_url) $ df = tables[0] $ df.head()
close_planets = time_light[time_light < 40] $ close_planets
save_model(model_outer, "Sierra Leone Holdout")
bnbAx.country_destination.value_counts().plot.bar()
tweetsDf.hist(column=['favorite_count'])
col_short=pd.DataFrame(col_totals[col_totals.index>'2016-12-01 00:00:00']) $ col_short=pd.DataFrame(col_short[col_short.index<'2017-01-01 00:00:00']) $ col_short.reindex? $ col_short['date_time']=col_short.index $ col_short.head(1)
idx = data['dataset_data']['column_names'].index('Close') $ closing = [day[idx] for day in data['dataset_data']['data']] $ change_two_days = [abs(closing[index]-closing[index-1]) for index, value in enumerate(closing) if index > 0] $ print('The largest chagne between any two days in 2017 was {:.2f}'.format(max(change_two_days)))
results = log_mod.fit() $ results.summary()
df.shape
run txt2pdf.py -o"2018-06-19 2011 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2011 FLORIDA HOSPITAL Sorted by discharges.txt"
mentions_df.head()
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new.head(1)
lm.fit(X_train,y_train) $ predictions = lm.predict(X_test) $ plt.scatter(predictions,y_test)
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
print(pd.to_datetime(['20180101'],format='%Y%m%d'))
pd.Series(master_file.axes[1])
no_outliers_model = Prophet(yearly_seasonality =True,weekly_seasonality= True,daily_seasonality = True) $ no_outliers_model.fit(df1_no_outliers); $ no_outliers_future = no_outliers_model.make_future_dataframe(periods= 6, freq = "m") $ no_outliers_forecast = no_outliers_model.predict(no_outliers_future)
rf_v1.hit_ratio_table(valid=True)
aa2 = aa.replace('\n ',',').replace(' ',',') $ aa2
deck = pd.DataFrame(titanic3['cabin'].dropna().str[0]) $ deck.columns = ['deck']  # Get just the deck column $ sns.factorplot('deck', data=deck, kind='count')
dfRegMet2014 = dfRegMet[dfRegMet.index.year == 2014]
df_gnis.shape
from IPython.display import FileLink $ FileLink('../develop/a.ipynb')
titanic.fare.hist()
countries = pd.get_dummies(df_new['country']) $ df_new = df_new.join(countries) $ df_new.head()
1/np.exp(-0.0211) $
p_diff_orig = df[df['landing_page'] == 'new_page']['converted'].mean() -  df[df['landing_page'] == 'old_page']['converted'].mean() $ print('p_diff from ab_data.csv :: ',p_diff_orig)
pd.crosstab(df['k'], df['author'])
autos["registration_year"].value_counts().sort_index(ascending=True)
idx = 0 $ festivals.insert(loc=idx, column='latitude', value=[41.9028805]) $ idx = 0 $ festivals.insert(loc=idx, column='longitude' value=[-87.7035663]) $ head.festivals(5)
print(norm.ppf(1-(0.05))) # Tells us what our critical value at 95% confidence is
data = data_train.append(data_test)
details.isnull().sum()
tweets2=pd.read_json(r'C:\Users\shampy\Desktop\project\RedCarpetUp\socialmediadata-tweets-of-congress-november\2017-11-02.json') $ tweets2.shape
news.shape
sm.get_values()
print(data)
joined.head()
students.sort_values(by='weight', ascending = False, inplace = True) $ students
df.head(7)
df_archive_clean["timestamp"].unique()
dftop2.sort_values(['days_top_complaint'], ascending = 0, inplace = True)
attend_with.columns = ['ATTEND_'+str(col) for col in attend_with.columns]
(p_diffs > (experimentp - controlp)).mean()
num_words = data_df.groupby('stars').text.apply(lambda x: mean(x.apply(lambda y: len(y.split())))) $ plt.xlabel('Stars') $ plt.ylabel('Average number of words') $ plt.bar(num_words.index, num_words.values) $ plt.savefig('words_hist.png',bbox_inches='tight', dpi=150)
for p in mp2013: $     print("{0} {1} {2} {3}".format(p,p.freq,p.start_time,p.end_time))
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
releases['jurisdiction_cd'].unique()
med=np.median(df.num_comments) $ med
p_val
for (method, group) in planets.groupby('method'): $     print("{0:30s} shape={1}".format(method, group.shape))
df.batter = df.batter.astype(int) $ df = df.merge(names, left_on='batter', right_on = 'key_mlbam', suffixes=('_old', ''))
from sklearn import metrics $ print metrics.accuracy_score(y_test, predicted) $
len(table3['device_id'].unique())
!which chromedriver $ executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ image_browser = Browser('chrome', **executable_path, headless=False)
ip_clean.tweet_id.dtype
empDf.createOrReplaceTempView("employees") $ SpSession.sql("select * from employees where salary > 4000").show()
import dask $ import distributed $ client = distributed.Client('tcp://oa-32-cdc.nexus.csiro.au:8786') $ client
allVars = read.getVariables() $ variables_df = pd.DataFrame.from_records([vars(variable) for variable in allVars], index='VariableID') $ variables_df.head(10)
add_datepart(scaled, 'Date') $ del scaled['Elapsed'] $ del scaled['Year'] $ scaled.head()
with pd.option_context('float_format', '{:.2f}'.format): print(df_mes.describe()) #10.294.628 cases
users = subs.user_id.unique() $ print "%d Users" % len(users)
first_commit_timestamp = git_log.timestamp.iloc[-1] $ last_commit_timestamp = pd.to_datetime('today') $ corrected_log = git_log[(git_log.timestamp>=first_commit_timestamp) & (git_log.timestamp<=last_commit_timestamp)] $ corrected_log.timestamp.describe()
user_table = df[['user_id','user_screen_name','user_created_at','user_location','user_description','user_verified','user_followers_count','user_statuses_count']] $ user_table.sort_values(by=['user_statuses_count'],ascending=False) $ user_table_1 = user_table.drop_duplicates(['user_screen_name'],keep='first') $ user_table_1.count()
top20_mosttweeted = top20_df['prediction_1'].value_counts() $ top20_mosttweeted = pd.DataFrame(top20_mosttweeted).reset_index() $ top20_mosttweeted.columns = ['prediction_1', 'tweet_count']
df2[(df2['user_id'] == 773192)]
df.head(5)
from sklearn import linear_model $ from sklearn.linear_model import LogisticRegression $ logit = linear_model.LogisticRegression() $ logit.set_params(C = 1)
np.power(b, 2)  # Raise every element to second power
df_joined_dummy = pd.get_dummies(data=df_joined, columns=['country']) $ df_joined_dummy.head()
df_users = df_users.drop(columns='Unnamed: 0') $ df_users.head()
measurement_df['station'].value_counts().count()
new_page_converted = np.random.binomial(1, p_new, n_new) $ print('binomial', new_page_converted.mean()) $ new_page_converted = np.random.choice([1, 0], n_new, p=(p_new,1-p_new)) $ print('random choice', new_page_converted.mean())
winpct.head()
y_pred=knn5.predict(X_test)
twitter_archive.name.sort_values()
sql = "SELECT * FROM paudm.forced_aperture_coadd_deprecated as coadd limit 5 " $ df3 = pd.read_sql(sql,engine)
tmax_day_2018.attrs
test = vec1.fit_transform(df.message[1]) #takes 2. row in df for testing $ for i in test: $     print(i) $
errors
df_onc_no_metac = df_onc_no_metac.drop(columns = ['METAC_SITE_NM1', 'MET_DATE1'])
dframe_team['start_cut'] = dframe_team.start_year.map(draftDates) $ dframe_team
ids = [1,3,56,83,34634,536352,45745] $ user = 34 $ df = pd.DataFrame({'repo_id': ids, 'user_id': [user for i in range(len(ids))]}) $ df.head()
data.head()
df = df.dropna()
cm = metrics.confusion_matrix(y_val, res_val) $ print(cm) $ print(classification_report(y_pred=res_val,y_true=y_val)) $ print(np.round(f1_score(y_pred=res_val,y_true=y_val),3))
df.head()
k1.head(10)
raw_full_df[['building_id_iszero','is_train']].groupby('is_train').mean()
mapping_dict = {} $ for row in sponsors_df.index: $     mapping_dict[row] = sponsors_df['Sponsor_Classification'][row] $ mapping_dict
f_counts_hour_ip = spark.read.csv(os.path.join(mungepath, "f_counts_hour_ip"), header=True) $ print('Found %d observations.' %f_counts_hour_ip.count())
stock = pd.DataFrame(np.log(data_tickers['adj_close'][ticker]).diff().fillna(0)) $ ts_mean = ts_mean.join(stock) $ ts_mean.head()
import statsmodels.api as sm; $ convert_old = df2[df2['group']=='control'].converted.sum() $ convert_new = df2[df2['group']=='treatment'].converted.sum() $ n_old = df2[df2['group']=='control'].shape[0] $ n_new = df2[df2['group']=='treatment'].shape[0] $
lens = train_df['Content'].str.len() $ lens.mean(), lens.std(), lens.max(), lens.min()
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df['group'])
train_groupped = pd.DataFrame(train.groupby(by = ['Page', 'date']).agg({'Visits' : np.mean})) $ test_groupped = pd.DataFrame(test.groupby(by = ['Page', 'date']).agg({'Visits' : np.mean})) 
friends_n_followers=DataSet.groupby('userTimezone')[['userFriendsCt','userFollowerCt']].mean() $ friends_n_followers=friends_n_followers.sort_values(['userFriendsCt'],ascending=False) $ friends_n_followers.head(10) $ len(friends_n_followers)
print 'DataFrame conversion_data (FEATURE MATRIX)', conversion_data.shape, type(conversion_data) $ conversion_data.head()
yc_new1 = yc_new1.merge(zipincome, left_on='zip_dest', right_on='ZIPCODE', how='inner') $ yc_new1.head()
import requests $ from collections import namedtuple
!curl -s https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py | python -
closing_prices = [] $ for ele in r.json()['dataset_data']['data']: $     closing_prices.append(ele[4]) $ print('Highest change between any two days based on Closing Price - {}'.format(max(closing_prices) - min(closing_prices)))
c.execute('SELECT * FROM cities') $ print(c.fetchall())
labels = pd.DataFrame({'cutoff_time': date_range}) $ labels['next_cutoff_time'] = labels['cutoff_time'].shift(-1) $ labels['msno'] = customer_id $ labels.head()
festivals['Date'] = pd.to_datetime(festivals['Date'], dayfirst=True, errors='corerce')
intersections.head() $
y_age = pd.DataFrame(X_age_notnull['age'], columns=['age']) $ X_age = deepcopy(X_age_notnull) $ X_age.drop('age', axis=1, inplace=True)
PIT_analysis = team_analysis.get_group("PIT").groupby("Category")
Z = np.random.random((10, 3)) $ print(Z) $ zmin, zmax = Z.min(axis=0), Z.max() $ print(zmin) $ print(zmax)
df = pd.read_csv(cama_file)
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
df_pol_matrix_df = pd.DataFrame(df_pol_matrix.todense(), $                          columns=tvec.get_feature_names(), $                          index=df_pol_t.index)
df_newpage = df2.query('landing_page =="new_page"') $ x_newpage = df_newpage["user_id"].count() $
y_label_train_OneHot.shape
datetime(2014,12,15,17,30)
test_embedding=pd.DataFrame(test_embedding)
MAX_DEPTH = 4   # choose a MAX_DEPTH based on cross-validation... $ print("\nChoosing MAX_DEPTH =", MAX_DEPTH, "\n")
day_of_year14 = uber_14["day_of_year"].value_counts().sort_index().to_frame() $ day_of_year14.head()
colNames
import pandas as pd $ rpd = pd.read_csv(fl) $ df = pd.DataFrame(rpd) $ df.head()
engine = create_engine('postgresql://mengeling:mengeling@localhost:5432/silvercar') $ df = pd.read_sql_table("reservations", engine)
topsales=df15[[store in top500 for store in df15.store_number]].groupby('store_number').agg(lambda x:x.value_counts().index[0]) $ topsales
twitter_archive_master.columns
consumer_key = "Ed4RNulN1lp7AbOooHa9STCoU"                                   # Twitter API keys (class) $ consumer_secret = "P7cUJlmJZq0VaCY0Jg7COliwQqzK0qYEyUF9Y0idx4ujb3ZlW5" $ access_token = "839621358724198402-dzdOsx2WWHrSuBwyNUiqSEnTivHozAZ" $ access_token_secret = "dCZ80uNRbFDjxdU2EckmNiSckdoATach6Q8zb7YYYE5ER"
fullDf.head()
austin = austin.drop(discrp.index)
df = df.groupby(['Year', 'Month']).agg({'Temperature': np.mean, 'Precipitation': np.mean, 'Description': 'count'}).reset_index()
iso_join = gpd.overlay(iso_gdf, iso_gdf_2, how='union')
ab = pd.read_csv('ab_data.csv') $ ab.head(3)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller' ) $ z_score, p_value
df.truncate(after='5/3/2014')
data.head()  #Show the first few rows. data.tail shows the last few.
(null_vals>(prob2-prob1)).mean()
chefdf = chefdf.dropna()
base_search_url = 'https://music.youtube.com/search?q=' $ urllib.parse.quote_plus(base_search_url + ' '.join(df.loc[0:0,'title'] + df.loc[0:0,'artist']), safe='/:?=')
print(data.shape)
twitter_data.head(1)
rf = RandomForestClassifier(n_estimators = 30) $ rf.fit(X_train, y_train) $ rf.score(X_test, y_test)
len(state_keys_list), len(state_DataFrames_list)
variance(Returning_df.Sales_in_CAD, len(Returning_df.Sales_in_CAD))
q1_data1 = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH) $ q2_data2 = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH) $ print('Shape of question1 data tensor:', q1_data1.shape) $ print('Shape of question2 data tensor:', q2_data2.shape) $
intervention_train.reset_index(inplace=True) $ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL'], inplace=True)
df_tx_claims = df[df['State'] == 'TX'] $ df_tx_claims
df.query(('group=="control"' and 'landing_page=="new_page"') and ('group=="treatment"' and 'landing_page=="old_page"')).shape[0]
m = RandomForestClassifier(n_estimators=40, min_samples_leaf=20, max_features='sqrt', n_jobs=-1, oob_score=True, random_state=42) $ m.fit(X_train, y_train) $ print_score(m)
df_schools.describe()
support = merged[merged.committee_position == 'SUPPORT'] $ oppose = merged[merged.committee_position == 'OPPOSE'] $ oppose.amount.sum()
avg_temp_recorded = session.query(func.avg(Measurement.tobs)).filter(Measurement.station=='USC00519281').group_by(Measurement.station).all() $ avg_temp_recorded
scores.head()
with open('dropbox/github/Thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_continent-90m.csv', 'rU') as inputFile: $     header = next(inputFile) #iterates to the next value) $     for line in inputFile: $         print(line)
lat_dim = ncfile.createDimension('lat', 73)     # latitude axis $ lon_dim = ncfile.createDimension('lon', 144)    # longitude axis $ time_dim = ncfile.createDimension('time', None) # unlimited axis (can be appended to).
ax = df[['A']].plot(figsize = (12,2)) $ for index, row in df.iterrows(): $      if row['A'] > 1: $         ax.annotate(row['A'],xy=(index,row['A'])) $ plt.show()
df.join(d, how='outer')
r.json()
df.plot() $ plt.show()
validation.analysis(observation_data, BallBerry_resistance_simulation_0_25)
datatest = pd.read_csv('csvs/properati_dataset_testing_noprice.csv', low_memory=False)
df.drop(1899, inplace=True)
print ("Probability that individual was in the treatment group,and they converted: %0.4f" % (df2.query('converted == 1 and group == "treatment"').shape[0]/df2.shape[0]))
lda_tf.print_topics(num_topics=10, num_words=7)
old_page_converted = np.random.choice([0,1],size=n_old,p=[(1-p_old),p_old])
m.start_time
segmentData['opportunity_month_year'] = segmentData.lead_converted_date.dt.to_period('M') $ segmentData['discovery_month_year'] = segmentData.opportunity_qualified_date.dt.to_period('M') $ segmentData['lead_month_year'] = segmentData.lead_created_date.dt.to_period('M')
moves_dataframe['date'] = moves_dataframe['date'].apply(lambda x: datetime.strptime(x,"%Y%m%d")) $ moves_dataframe.index = moves_dataframe['date'] $ twitter_moves = twitter_mean.join(moves_dataframe,lsuffix='_twitter',rsuffix='_moves',how='right')
df_latest.head(1)
df.dropna(inplace=True)
for df in (joined,joined_test): $     df["CompetitionMonthsOpen"] = df["CompetitionDaysOpen"]//30 $     df.loc[df.CompetitionMonthsOpen>24, "CompetitionMonthsOpen"] = 24 $ joined.CompetitionMonthsOpen.unique()
with open("Total_Politician_and_Events","wb") as f: $     pickle.dump(TotalNameEvents,f) $
df_cont = pd.read_csv('contact.csv', index_col=None)
X_test_dtm = stfvect.transform(X_test) $ X_test_dtm
df2['intercept'] = 1 $ df2[['ba_page','ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('ba_page', axis=1) $ df2.head()
df["stamp"] = to_datetime(df["created_at"],format='%a %b %d %H:%M:%S +0000 %Y') $ df.head()
df_tweet.info()
pd.merge(df1, df2, how='left', $          right_index=True, left_index=True)
preci_df.describe() $
df['converted'].mean() #. Overall mean of the whole column
d={'Flavor':['Strawberry', 'Vanilla', 'Chocolate'],'Price':[3.50,3.00,4.25]} $ icecream = pd.DataFrame(data=d) $ icecream
y = np.log(df_nuevo['price'].values) $ df_nuevo.drop(['price','id','date','zipcode',],axis=1,inplace=True) $ X = df_nuevo.values
news_tweets_pd = news_tweets_pd[["User", "Tweet Count", "Tweet Text", "Date", "Compound", "Positive", "Neutral", "Negative"]] $ news_tweets_pd.head()
json.dumps(aoi)
another_rdd = sc.parallelize([("John", 19), ("Smith", 23), ("Sarah", 18)]) $ schema = StructType([StructField("person_name", StringType(), False), $                      StructField("person_age", IntegerType(), False)]) $ another_df = sqlContext.createDataFrame(another_rdd, schema) $ another_df.printSchema() $
rent_db.boxplot(column='price')
df_groups = pd.read_csv('groups.csv') $ df_groups.head()
!rm 'OnlineRetail.csv.gz' -f $ !wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz
q = ['what', 'when', 'where', 'while', 'who', 'why', 'which', 'whom', 'whose', 'how'] $ qfd = nltk.FreqDist([word for word in monty if word in q]) $ display(qfd)
full_globe_temp == -999.000
df2 = df1[df1['timestamp'] == '2018-01-23'] $ df2.plot( kind='line', x='Hour', y='Polarity',title='Polarity on 23/01/2018') $ axes = plt.gca() $ plt.xticks(rotation='vertical', fontsize=11) $ plt.show()
browser_dummy = pd.get_dummies(fraud_data_updated['browser']) $ fraud_data_updated = pd.concat([fraud_data_updated,browser_dummy],axis=1)
df_tweets[df_tweets["headline"] == "#UseR2018"]
new_dems.Sanders.isnull().sum()
nnew = len(df2.query('group =="treatment"')) $ nnew
sub_gene_logical_vector = df.source.isin(['ensembl', 'havana', 'ensembl_havana']) $ sub_gene_df = df[sub_gene_logical_vector] $ sub_gene_df.shape
table = soup.find_all('table', id="tablepress-mars")
autos.loc[autos["offer_type"] == 'Gesuch']
grid_pr_fires.plot.scatter(x='glon',y='glat', c='pr_fire', $                            colormap = 'RdYlGn_r')
df.info()
df2[df2['landing_page']=='new_page'].shape[0]/df2.shape[0]
f = '/scratch/olympus/womens_march_2017/data/womens_march_2017_data__01_18_2018__00_00_00__23_59_59.json.bz2' $ collect = JsonCollection(f, compression='bz2', throw_error=0)
access_logs_df.select(min('contentSize'), avg('contentSize'), max('contentSize')).show()
to_be_predicted_Day2 = 48.59906715 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
Change timestamp datatype to datetime
DummyDataframe = pd.DataFrame(columns=['Date', 'hashtag']) $ dummyDates = [ pd.Timestamp('20180101'),  pd.Timestamp('20180201'),  pd.Timestamp('20180301'),  pd.Timestamp('20180401'),  pd.Timestamp('20180501')] $ for i in range(25): $     DummyDataframe.loc[i] = [dummyDates[i % 5], np.random.randint(150), np.random.randint(150)] $ DummyDataframe
class Square(Rectangle): $
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score
df = pd.read_csv('cars.csv') $ len(df)
df[df['Complaint Type'] == 'Street Condition']['Descriptor'].value_counts().head()
predictions.select("label", "prediction", "probability").show()
tweet_data.info()
dfFull['MasVnrAreaNorm'] = dfFull['MasVnrArea']/dfFull['MasVnrArea'].max()
cols = ['bizname', 'license_loc', 'instate_loc', 'mailing_loc', $         'license_no', 'lic_date', 'status', 'cr_date', 'action'] $ df = pd.DataFrame(columns=cols)
data = pd.read_csv('csvs/datosConDuplicados.csv', low_memory=False)
X_train.iloc[0:5,:12]
twitter_archive_master.info()
df_selection = df[var_num+var_cat]
test = pd.read_csv('../input/sample_submission_v2.csv')
for param in dir(tweet): $     if not param.startswith("_"): $         print("%s: %s\n" % (param, eval('tweet.'+param)))
y_test[:5]
today = pd.to_datetime("Today") $ today
x.drop([0, 1])
X_svd = np.hstack((id_dense,blurb_SVD, goal_dense, duration_dense))
co = CombinatorialOptimisation() $ co.train(mains,cols=[('power','active')]) $
df.info()
access_token = <placeholder for API authentication variables> $ access_token_secret = <placeholder for API authentication variables> $ consumer_key = <placeholder for API authentication variables> $ consumer_secret = <placeholder for API authentication variables>
df_agg = tmdb_movies_vote_revenue.groupby('genre').agg({'vote': np.average}) #aggregate $ df_agg.sort_values(by=['vote'], ascending = False).head()
pOld
save_dat=daily_dat[out_columns][:-1] #remove last row; day is not complete upon download $ save_dat=save_dat[1:]#remove first day; likely not complete
df_wm['Sentiment_class'] = df_wm.apply(conditions, axis=1)
pd.concat([city_loc, city_pop], ignore_index=True)
train.head(3)
np.sum(all_data['price_change'].isnull().values)
print(len(df_sched)) $ df_sched.head()
df_archive_clean["tweet_id"].describe()
q3_results = session.query(Stations.name,Measurements.tobs).filter(Stations.station == Measurements.station,)\ $             .filter(Stations.name == 'WAIHEE 837.5, HI US').all() $ q3_results
y_test_pred = model.predict(X_test_poly) $ utils.metrics(y_test, y_test_pred)
full_globe_temp = full_globe_temp["mean temp"]
len(df.index) - df.count()
version = str(int(time.time())) $ name = 'Daily_Stock_Prediction_'+ version $ latest = 'Daily_Stock_Prediction_latest' $ log_model.save("./Models/"+name) $ log_model.save("./Models/"+latest)
from pandas import DataFrame $ labels = DataFrame(answer.Sentiment.map(dict(positive=1, negative=0)))
np.exp(results_1.params)
df2.shape
df['messages'].value_counts()
len(has_text)
poly_features = PolynomialFeatures(2, include_bias=False) $ poly_features.fit(X_test['Pending Ratio'].values.reshape(-1, 1)) $ test_pending_ratio = poly_features.transform( $     X_test['Pending Ratio'].values.reshape(-1, 1)) $ print(test_pending_ratio[0:5, :])
df.query('converted == 1').count()/df['user_id'].nunique()
df.head()
f1_A = lv_workspace.get_data_filter_object(step=1, subset='A') $ f1_A.include_list_filter
last_seen_count_norm.describe()
svc=svm.SVC().fit(X_train,y_train)
df2['intercept']=1 $ model=sm.Logit(df2['converted'],df2[['intercept','ab_page','US','UK']]) $ result=model.fit() $ result.summary()
import pandas as pd $ data = pd.read_csv("Fremont_Bridge_Hourly_Bicycle_Counts_by_Month_October_2012_to_present.csv", index_col='Date', parse_dates=True) $ data.head()
subjectivity = pd.DataFrame(df.groupby(['subjectivityFeeling', 'profile']).size().rename('counts')).astype(int) $ subjectivity['percent'] = subjectivity['counts'].apply(lambda x: x/subjectivity.sum()) $ subjectivity
no_images=[] $ for i in range(len(tweet_json)): $     if 'media' not in list(tweet_json['entities'][i].keys()): $         no_images.append(i) $ print("There are {} tweets that do not have an image".format(len(no_images)))
day_of_week(lesson_date)
nba_df[nba_df.isnull().any(axis=1)].loc[:, ["Season", "Team", "G", "Opp", "Home.Attendance", "Referee3"]]
print final0.shape $ print final0.drop(duplicate_id).shape $ final=final0.drop(duplicate_id)
index_to_change = df3[df3['group']=='treatment'].index $ df3.set_value(index=index_to_change, col='ab_page', value=1) $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
tf = pd.read_csv("test.csv", sep=',', encoding="latin_1")
df.sort_values('prob_off').tail(50)
data_get["HAYN"].Close.plot()
pd.Timestamp('Jan 1 2018')
df_new.country.value_counts() # confirm how many of each country are in the dataset
geo_db.head()
tweets = pd.read_csv("tweets.csv") $ tweets.head()
sns.jointplot(x='sepal_length', y='sepal_width', data=data, size=4)
celtics['season'] = (pd.DatetimeIndex(celtics.date) - np.timedelta64(9,'M')).year
df_total = df_total.reindex(columns=new_cols)
states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', $           'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', $           'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', $           'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', $           'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']
import psycopg2 as pg2 $ conn = pg2.connect(dbname='ChicagoViolencePredictor', user='postgres', host='localhost')
final_cols = list(set(df17).intersection(list(df16))) $ f16 = df16[final_cols].copy() $ f17 = df17[final_cols].copy() $ final_result = pd.concat([f16, f17]) $ len(final_result)
engine = create_engine("sqlite:///hawaii.sqlite", echo = False ) $
data_full.isnull().sum()
trips.sort_values(by='duration').head()
percent.plot()
baseball_subdomain_id = 4 $ url = form_url(f'subdomains/{baseball_subdomain_id}') $ response = requests.get(url, headers=headers) $ print_body(response, skip_audit_info=True)
sh_results = session.query(Measurements.date,Measurements.tobs).\ $             filter(Measurements.date >= pa_min_date).\ $             filter(Measurements.station==station_max).all()
counts_by_channel= USvideos.groupby('channel_title').size().sort_values(ascending = False) $ counts_by_channel[:5]
%timeit -n1 -r1 pd.read_csv(dataurl+'test.csv.gz', sep=',', compression='gzip') $ %timeit -n1 -r1 pd.read_csv(dataurl+'test.csv', sep=',')
vwap.ix['2011-11-03':'2011-11-04'].plot() $ plt.ylim(103.5, 104.5) $ vol.ix['2011-11-03':'2011-11-04'].plot(secondary_y=True, style='r')
for character in sample_str: $     print character
f = ['salt', 'water', 'fish', 'fishing'] $ fishing = wk_output[wk_output.explain.str.contains('|'.join(f))] $ fishing.shape $ fishing.to_csv('fishing_feedback.csv')
donald_trump_tweets=tweets[tweets.text.str.lower().str.contains('donald trump|president trump')==True] $ donald_trump_tweets[:5]
import json $ with open('Api_key_NO_GIT.json', 'r') as f: $     j_data = json.load(f) $ API_KEY = j_data.get('API_KEY')
a,b=validation,(validation+test) $ x,y=X[a:b],Y[a:b] $ scores_test=model.evaluate(x,y) $ print ("\n%s: %.2f%%" %(model.metrics_names[1],scores_test[1]*100))
nbsvm_models = {}
df_archive["rating_denominator"].value_counts()
results.summary()
nar3=nar2.merge(df_loan2[['fk_loan',0]].rename(columns={0:'irr'}),on='fk_loan')
autos["odometer_km"].unique()
print pd.pivot_table(data=df, $                      index='date', $                      columns='item', $                      values='status', $                      aggfunc=np.sum)
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.hour).count().plot(y="Borough")
preds.mean()
tweets = pd.read_sql_query("SELECT * FROM tweets_info;", conn, parse_dates=['created_at'] ) $ print("Number of Tweets: %s" %len(tweets))
tc_isbns = tc['ISBN RegEx'] $ print(tc_isbns.size)
X = df_train[features_to_use] $ y = df_train["interest_level"] $ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)
del final[0]
feature_importances[:10]
ls
git_log["timestamp"] = pd.to_datetime(git_log["timestamp"], unit="s") $ git_log["timestamp"].describe()
tweets1.text[0]
df_ratings[['user_id', 'video_id', 'overall_rating_value']]
len(cats_in['Animal ID'].unique())
import re $ df2['rx_requested'] = re.sub(r'[^\w]', ' ', df2['rx_requested'])
beijing['WindDirDegrees'] = beijing['WindDirDegrees'].str.rstrip('<br />')
frequency = defaultdict(int) $ for text in documents: $     for token in text.split(): $         frequency[token] += 1 $ frequency
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
nitrogen['CharacteristicName'].unique()
response_raw = requests.get(url,params) $ response_clean = response_raw.content.decode('utf-8')
del data_scrapped['index'] $ data_scrapped.shape
df.ix[0]
sum(merged_data['Precipitation_In '] == '0')/len(merged_data)
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}] $ new_store = pd.DataFrame(new_items, index = ['store 3']) $ new_store
show(p_nmf2)
try: $     cur_b.execute('UPDATE room set hotel_id=99 WHERE hotel_id=1') $ except Exception as e: $     print('Exception: ', e)
shifted_backwards = msftAC.shift(-2) $ shifted_backwards[:5]
goog['Month'] = goog.index.month $ goog['Year'] = goog.index.year
order_item_merge.dtypes
autos.odometer = (autos.odometer.str.replace('km','') $                                 .str.replace(',','') $                                 .astype(int)) $ autos.odometer.value_counts() $
_ = ok.grade('q03') $ _ = ok.backup()
passes = most_yards[most_yards["PlayType"] == 'Pass']
roc_auc_score(predictions, fb_test.popular)
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set() $ goog.plot()
file = 'literary_birth_rate.csv' $ df_birth = pd.read_csv(file, sep = ';')
scipy.stats.pearsonr(df_day_pear['tripduration'],df_night_pear['tripduration']) $
no_outliers_forecast_exp2[(no_outliers_forecast_exp2.index >= '2018-06-01') & (no_outliers_forecast_exp2.index <= '2018-12-31')].astype(int)
plt.axvline(diff_actual, c='red') $ plt.hist(p_diffs)
m.fit(lr, 2, metrics=[exp_rmspe], cycle_len=4)
lr = LinearRegression()
a.iloc[[3]]
data_url1 = datasets1[0] $ data_url1
fin_r = fin_r.reindex(df.resample('D').index, method='ffill') #df.resample('D').index $ assert (fin_r.index == r_top10_mat.index).all()
cfs=cfs.cluster_features()
a.iloc[3]
df2 = df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')") $ df2.shape[0]
data.printSchema()
RNPA = merged2[merged2['Specialty'] == 'RN/PA']
X_live.head()
len([b for b in BDAY_PAIR_df.pair_age if b<0 ])
print("Converted users proportion is {}%".format((df['converted'].mean())*100))
test_df["labels"] = np.array(probs)[:,1] $ test_df.head(500)
test_kyo2 = test_kyo1.rename(columns={"ex_lat":"end_lat", "ex_long":"end_long"}) $ test_bkk2 = test_pl1.rename(columns={"ex_lat":"start_lat", "ex_long":"start_long"})
S.decision_obj.stomResist.options
parse_name_info_udf = UserDefinedFunction( $     parse_name_info, $     StructType([StructField("title", StringType()), StructField("first", StringType())]), $     "parse_name_info")
with open('nmf_doc_top_5_29.pkl', 'wb') as piccle: $     pickle.dump(nmf_doc_top, piccle)
baseball.to_pickle("baseball_pickle")
ax = sns.barplot(x= "countPublications", y = "countPublications", data = data_final, palette='rainbow', $                  estimator = lambda countPublications: len(countPublications) / len(data_final) * 100) $ ax.set(ylabel="% of authors with a given number of publications") $ ax.set(xlabel="Number of publication by author") $ ax.set(xlim=(0, 20))
df_countries['country'].unique()
new_page_converted = np.random.choice(2, size = n_new, p=(p_new,1-p_new)) $ new_page_converted
df2 = df2.drop_duplicates(subset='user_id', keep='last')
pres_df['metro_area'] = pres_df['split_location_tmp'].map(lambda x: x[0]) $ pres_df['metro_area'].head()
to_plot_df = df.groupby(['original_language']).mean() $ to_plot_df
df_data.EXAME.value_counts()
sentences = df.text $ dates = df.created_at
sns.distplot(reddit['Hour of Day'], kde=True, bins=10, color='green')
df.info()
live_weights = weight.dropna() $ print(len(live_weights))
df.describe()
lsi.save('/tmp/model.lsi') # same for tfidf, lda, ... $ lsi = models.LsiModel.load('/tmp/model.lsi')
df.info()
subred_num_avg = reddit[['subreddit','num_comments']].groupby(by='subreddit', sort=True, as_index=False).mean().round().sort_values(by='num_comments',ascending=False)
print("cat" in a)  # 'in' is implemented by __contains__ $ print("dog" in a)
urban_ride_total = urban_type_df.groupby(["city"]).count()["ride_id"] $ urban_ride_total.head()
converted = df2.query('converted == 1').user_id.count() $ total = df2.user_id.count() $ converted / total
cust_data[["MonthlyIncome","ID"]].head(5)
p.to_timestamp()
twitter_archive_clean[twitter_archive_clean.expanded_urls.isnull()]
xmlData.head(5)
number_unpaids.hist(bins=20, figsize=(20, 5))
def testsavedmodel(): $     print("R^2 = ", compute_perf(bikescore.trainedmod, tsX, tsY))
filled = filtered.fillna(method='pad', limit=1) $ filled.ix['2011-11-03':'2011-11-04'].head(20)
nx.draw(G, pos=nx.spring_layout(G)) $ plt.show()
conn = psycopg2.connect("host=108.45.74.15 port=5432 dbname=emoji_db user=insight password=") $ cur = conn.cursor() $
max(diff)
train.head()
tweet_json_df_clean.id.dtypes
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new,convert_old],nobs=[n_new,n_old],alternative='larger') $ print(z_score,p_value)
top_songs['Date'].dtype
!mkdir ./data
print('Predicted price of the 1st house: {:.1f}'.format(prediction_simple[0])) $ print('Predicted RSS: {}'.format(np.sum((prediction_simple - test_output_simple)**2)))
df2 = df2.drop(df2.query('user_id == 773192').index[0])
pres_df['subjects'].isnull().sum()
from dllib.model import model_summary $ from dllib import train as train
autos.columns.tolist()
financial_crisis.loc['Tulip Mania']
df_planets = pd.DataFrame({"name": planets, "id": range(len(planets))}) $ df_planets
hashtags = read_csv("hashtags.csv") $ hashtags.head()
df
temp_df = pd.DataFrame(year_temp) $ index_temp_df = temp_df.set_index('date') $ index_temp_df.head()
options_frame = _interp_implied_volatility(options_frame)
from statsmodels.stats.diagnostic import acorr_ljungbox $
oppstagepct.loc[0].plot.box(by='opportunity_stage')
os.chdir(str(today))
stations_info = session.query(Station.name, Station.station ).all() $ stations_info = pd.DataFrame(stations_info) $ stations_info.head()
df = pd.read_sql('SELECT last_name, COUNT(*) FROM actor GROUP BY last_name HAVING COUNT(*) >= 2 ORDER BY last_name', con=conn) $ df
plt.hist(p_diffs);
df_json.info()
df_actor.head()
sum(twitter_archive_clean['name'].value_counts().isin(incorrect_names))
p_new
import matplotlib.pyplot as plt $ plt.style.use('ggplot')
thanks = ['thank', 'thank you', 'thanks', 'thx', 'tyvm', 'tysm', 'good to know', 'good to hear']
df_archive_clean["tweet_id"].sample(5)
p_old = df2['converted'].mean() $ print('Convert rate for p_old under the null :: ',p_old)
print len(infinity)
offseason12["InorOff"] = "Offseason"
tvec = TfidfVectorizer(stop_words=stopwords.words('english'), $                                  lowercase=True,max_features=500) $ X_train_matrix = tvec.fit_transform(X_train['text']) $ X_test_matrix = tvec.transform(X_test['text'])
idx = payments_all_yrs[ (payments_all_yrs['id_num']==110130)   ].index.tolist() $ len(idx) $
for user in missing_hacker_list: $     res = [user] + top_challenges $     reso = res[0] + ',' + res[1] + ',' + res[2] + ',' + res[3] + ',' + res[4] + ',' + res[5] + ',' + res[6] + ',' + res[7] + ',' + res[8] + ',' + res[9] + ',' + res[10] $     result.append(reso)
aapl = pd.read_excel("../../data/stocks.xlsx", sheetname='aapl') $ aapl.head()
p_y = predict(X_test, Gaussian, Bernoulli, prior)
data['temp'] = data['year'].map(str)+'-' + data['month'].map(str) +'-'+ data['day'].map(str)     $ data.head(5)
lr=126064 $ loan_requests1[loan_requests1.id_loan_request==lr]
neg_tweets = ioDF[ioDF.all_sent_x <= -.5]
tips["sex"].index
df_ad_airings_filter_3['race'].value_counts()
control_conversion = df2[(df2['group'] == "control") & (df2['converted'] == 1)].count() $ total_control = df2[(df2['group'] == "control")].count() $ print(control_conversion/total_control) $
df = pd.read_sql('SELECT * FROM customer_female', con=conn_b) $ df
from sklearn.ensemble import RandomForestClassifier $ random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)
taxi_hourly_df.head()
model.save_weights('best.hdf5')
p_old = (df2['converted'] == 1).mean() $ p_old
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False)
df['const2'] = 1 $ print("... added constant")
bigdf.to_csv('Combined_Comments-Fixed2.csv')
s.axes #print the labels $
tobs_start_date = dt.datetime.strptime(latest_date,"%Y-%m-%d") - dt.timedelta(days=365) $ tobs_start_date = tobs_start_date.strftime("%Y-%m-%d")
%matplotlib inline
elms_all.ORIG_DATE.max()
images.sample(10)
dashdata['5day_price']=dashdata['close']*dashdata['5_day_target']
data_df.tail()
df_2018.shape
def read_data(filename): $     data = pd.read_csv(filename, sep='\t') $     data['tags'] = data['tags'].apply(literal_eval) $     return data
print("Number of Techniques in Enterprise ATT&CK") $ techniques = lift.get_all_enterprise_techniques() $ print(len(techniques)) $ df = json_normalize(techniques) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'data_sources','contributors'], axis=1)[0:5]
Precipitation_DF.plot(rot=45,title="Precipitation from %s to %s"%(start_date,end_date),figsize=(8,5),grid=None,colormap="PuOr_r") $ plt.show()
before['count'] = before.groupby('hashtags')['hashtags'].transform(pd.Series.value_counts) $ before.sort('count', ascending=False) $ before.hashtags.dropna().head()
len(df.user_name.unique())  # number of authors of these tweets
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False)
yelp_dataframe.drop('index', axis=1, inplace=True)
df2[(df2.group == 'control')].converted.mean()
df2[df2['landing_page']== 'new_page'].count()[0]/df2.shape[0]
%run ../../code/version_check.py
full_clean_df = pd.merge(df1_clean, df3, on='tweet_id')
train.reset_index(inplace=True) $ train.drop('index',axis=1,inplace=True) $ train.head()
with open('topic_list_5_29.pkl', 'wb') as piccle4: $     pickle.dump(topic_list, piccle4)
sales['date'] = pd.to_datetime(sales['date'],format="%d.%m.%Y")
print_top_words()
my_df = get_lims_dataframe(my_query) $ my_df.head()
print(autos.columns)
(autos["ad_created"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
raw_df[2] = raw_df[2].apply(_parse_bytes) $ raw_df[1] = raw_df[1].apply(_parse_bytes)
df['timestamp'].astype('datetime64[ns]').hist(xrot=90); $ plt.title('Tweets Over Time');
join_date = parse(user['data']['created_at']) $ join_date
fb = pd.read_json("data/facebook.json")
appointments.to_csv('./data/appointments_full.csv')
merged_portfolio_sp_latest_YTD_sp = merged_portfolio_sp_latest_YTD_sp.sort_values(by='Ticker', ascending=True) $ merged_portfolio_sp_latest_YTD_sp
pd.date_range('2017-01', periods=4, freq='W-MON')  # First 4 Mondays in Jan 2017
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new]) $ print(new_page_converted)
ratings.loc[2436]
vip_reason = questions['vip_reason'].str.get_dummies(sep="'")
lemmatiz(t);
joined = gpd.sjoin(tazs,sw_m, how="right", op='contains') $ del joined['index_left'] $ del joined['geometry'] $ joined = joined.set_geometry('linestring') $ zones = joined['VTA_TAZ'].unique()
data_table 
model_data = model_data.drop(['creation_time','name','email','last_session_creation_time','invited_by_user_id','org_id'], axis=1)
data.ix[:3, :'pop']
met = get_metrics(STAMP, y, y_pred, 0.5)
official_data = pd.DataFrame.from_csv("data/merged-census.csv", index_col=None) $ official_data['Census Tract'] = official_data["Census Tract"].apply(fix_tract) $ official_data = official_data.set_index("Census Tract") $ official_data
tip_sample.head()
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"]) $
df.iloc[1] # select row by integer location 1 $
challange_1.shape
df2['user_id'].unique().shape[0]
df_train[0:5]
import xgboost as xgb $ gbm = xgb.XGBClassifier(max_depth=2, n_estimators=300, learning_rate=0.001).fit(X_data, y_data) $ yhat = gbm.predict(X_data) $ f_05(y_data, yhat) # 0.612 to real is 0.559 (2, 300, 0.001)
sub_df2=df2.query('dow=="Friday" | dow=="Monday"' )
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').query('converted == 1')['user_id'].count() $ convert_new = df2.query('landing_page == "new_page"').query('converted == 1')['user_id'].count() $ n_old = df2.query('landing_page == "old_page"')['user_id'].count() $ n_new = df2.query('landing_page == "new_page"')['user_id'].count()
import astropy.units as u $ from astropy.coordinates.sky_coordinate import SkyCoord
haw = pd.merge(left=dfAnnualN,right=dfAnnualMGD,how='inner',left_index=True,right_index=True) $ haw.columns
!curl -L -O  https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv $ !head -n 1 Consumer_Complaints.csv > CC_header.csv $ !tail -n +2 Consumer_Complaints.csv > CC_records.csv
 evaluator = text_classifier.evaluate(df_test)          
autos.shape
twitter_archive_df_clean.columns
df4['tobs']=df4['tobs'].astype(int) $ tobs_mean = df4['tobs'].mean() $ tobs_max = df4['tobs'].max() $ tobs_min = df4['tobs'].min() $
df.info()
sum(df.rating_denominator.value_counts()!=10)
num_elements = 1000000  # 1 million $ x = np.random.uniform(low=1, high=5, size=num_elements).astype('float32')
corrplot(corr=r[coins_infund].loc[start_date:end_date].corr()) $ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date) $ plt.show()
df = pd.read_csv(project_path + '/data/raw/data.csv', index_col=0) $ df.head()
%time preds = np.stack([t.predict(X_valid) for t in m.estimators_]) $ np.mean(preds[:,0]), np.std(preds[:,0])
images.img_num.value_counts()
spark.stop()
save_n_load_df(joined, 'joined_promo_bef_af2.pkl')
plt.hist(p_diffs) $ plt.title('the distribution of the diffrence between the two proportions') $ plt.xlabel('P_new - P_old') $ plt.ylabel('frequancy')
df_prep99 = df_prep(df99) $ df_prep99_ = pd.DataFrame({'date':df_prep99.index, 'values':df_prep99.values}, index=pd.to_datetime(df_prep99.index))
df.set_index('Parameter', inplace=True) $ df.head()
df.loc['r1':'r2']
df2.query('group == "treatment"') $ df2.query('group == "treatment" and converted == 1') $
k_var1['launched_at'] = launch_date1 $ k_var1['state_changed_at'] = change_date1 $ k_var1['days_to_change'] = (k_var1['state_changed_at'] - k_var1['launched_at'])
dummy = pd.get_dummies(data=df_country, columns=['country']) $ df4 = dummy.merge(df3, on='user_id') $ df4.head()
house_data['bedrooms'].unique()
actual_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ (np.array(p_diffs) > actual_diff).mean()
raw_df.head(2)
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
browser = Browser('chrome', headless=False) $ news_url = 'https://mars.nasa.gov/news/' $ browser.visit(news_url) $ time.sleep(1)
