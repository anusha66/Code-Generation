print(pd.DataFrame(test_matrix).head())
df = pd.read_excel('PPB_gang_records_UPDATED_100516.xlsx')
cars= cars.drop(['yearOfRegistration','dateCreated','lastSeen','dateCreatedMod','lastSeenMod'],axis=1) $
RF.fit(X_train,Y_train)
apple.asfreq('D',method = 'pad') $ apple.head()
df2['ab_page'] = pd.get_dummies(df['group'])['treatment'] $ df2.head()
res = joinresult.set_index(['Name', 'BARCODE']) $ print(res)
print("Actual difference:" , p_diff) $ p_greater_than_diff = (float(len(greater_than_diff))/len(p_diffs)) $ print('Proportion greater than actual difference:', p_greater_than_diff) $ print('As a percentage: {}%'.format(p_greater_than_diff*100))
data = drive.CreateFile({'id': '1iviqKhwsy_q3-SZ4WUlGDSOIRpzluf1I'}) $ data.GetContentFile('training_car_x_y_train.csv') $ test_data = drive.CreateFile({'id': '1Lxt5lvzbYFWSHrdtwNFoH_5n_-OEYcoK'}) $ test_data.GetContentFile('test_car_x_test.csv') $
sp = openmc.StatePoint('statepoint.50.h5')
pre_strategy = people_person.date < '2017-04-12'
df_tte_all[df_tte_all['ItemDescription'] == '$0.13 per alarm-month']
def find_similar(matrix, index, top_n = 10): $     cosine_similarities = linear_kernel(matrix[index: index + 1], matrix).flatten() $     related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index] $     return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]
df_tweets = pd.DataFrame(tweets) $ df_tweets
yc_new3 = yc_new2[yc_new2.tipPC < 100]
tree_features_df[~tree_features_df['p_hash'].isin(manager.image_df['p_hash'])] #Let's look at what's missing:
ybar_min = dfNiwot["TMIN-C"].mean() $ ybar_max = dfNiwot["TMAX-C"].mean() $ print("Mean Min Temp in Celsius = {:.3f}".format(ybar_min)) $ print("Mean Max Temp in Celsius = {:.3f}".format(ybar_max))
dtrain, dval, evals = xgb_md.get_train_eval_ds()
import google.datalab.storage as storage
df_only_headline = df_tweets[(df_tweets["sub-headline"].str.len() == 0) & (df_tweets["tags"].apply( $     lambda x: len(set(["misc", "learning", "tool", "dataviz", "research"]).intersection(set(x))) == 0 $ ))] $ headline_date_count = df_only_headline.groupby(["headline", "date"]).size() $ headline_date_count[headline_date_count > 1]
taxiData.Trip_distance.size
X = pd.merge(X, meal_is_interactive[['meal_id','is_interactive']], on='meal_id', how='inner')
url = form_url(f'actionTypes/{baseball_swing_action_type_id}/metricGroupTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
disposition_df.shape
aug2014.start_time, aug2014.end_time $
master_list.sort_values(by='Count', ascending=False).head(10)
n_new = len(df2.query("group == 'treatment'")) $ print('The n_new is: {}.'.format(n_new))
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ print(f"convert_old: {convert_old}") $ print(f"convert_new: {convert_new}")
h = tclab.Historian((('Q1', lambda: [0, 0, 0, 0]), $                      ('Q2', None), $                      ('T1', None), $                      ('T2', None)), dbfile='sinetest.db')
ferrocarriles_caba = pd.read_csv('datasets/estaciones-de-ferrocarril.csv', sep=';', error_bad_lines=False, low_memory=False) $ ferrocarriles_caba.info()
y_pred = rdf_model.predict(X_test) $ precision, recall, fscore, support = score(y_test, y_pred, pos_label = 4, average = 'binary')
df.groupby("cancelled")["pickup_end_of_month"].mean()
from statsmodels.stats.diagnostic import acorr_ljungbox $
 df.describe(include=['object'])
ohe_feats = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'] $ for f in ohe_feats: $     df_all_dummy = pd.get_dummies(df_all[f], prefix=f) $     df_all = df_all.drop([f], axis=1) $     df_all = pd.concat((df_all, df_all_dummy), axis=1)
pd.Series.loc?
x_train = scaler.transform(x_train)
data = open("test_data//dummy.txt").read()
S_lumpedTopmodel.basin_par.filename
p_diffs = [] $ diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs.append(diffs)
run txt2pdf.py -o "MEMORIAL HERMANN HOSPITAL SYSTEM  Sepsis.pdf"   "MEMORIAL HERMANN HOSPITAL SYSTEM  Sepsis.txt"
df.dropna(inplace=True) $ df.shape
a4_dims = (15, 15) $ fig, ax = plt.subplots(figsize=a4_dims) $ seaborn.barplot(data=keywords_dataframe, x='count', y='keyword', ax=ax)
autos['price'].value_counts().head()
f = np.linspace(0, np.pi, 100) $ f = np.sin(f) $ print("f: ", f)
df_archive_clean.to_csv("twitter_archive_master.csv", encoding = 'utf-8', index = False)
con = sqlite3.connect('db.sqlite') $ df=pd.read_sql_query("SELECT * from tbl", con) $ con.close() $ df
mike=pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/mike_previous_observing.txt',delim_whitespace=True,header=None,names=['index','name','ra','dec']) $ mike['RA0']=[float(Angle(i, u.hr).to_string(unit=u.degree, precision=5, decimal=True)) for i in mike.ra] $ mike['DEC0']=[float(Angle(i, u.deg).to_string(unit=u.degree, precision=5, decimal=True)) for i in mike.dec] $
display('df5', 'df6', $        "pd.concat([df5, df6], join_axes=[df5.columns])")
pd.cut(londonDFSubset['POPDEN'], bins = 5, include_lowest = True)
(violations08Acounts / boro_counts).plot(kind='bar')
autos.head() $
df_everything_about_DRGs[(df_everything_about_DRGs['drg3_str'] == '001') ].index.tolist()
df1.query("landing_page == 'new_page' and group == 'control'").shape[0] + \ $ df1.query("landing_page == 'old_page' and group == 'treatment'").shape[0]
new_page_converted = np.random.binomial(nnew, pnew)
ordered.set_index([0], inplace = True) $ odds = ordered["Odds"].plot(kind = "bar", figsize = (20, 5), title = "Percentage Odds for Winning World Cup", rot = 45, legend = True) $ odds.set_ylabel("Percentages", fontsize = 15) $ plt.show()
neg_freq = {k: neg_tfidf.dfs.get(v) for v, k in neg_dic.id2token.items()} $ sorted(neg_freq.items(), key=lambda x: x[1], reverse=True)
tbl = pd.merge(df,tbl, on = ['msno','is_cancel']) $ tbl.tail(n= 1)
week1_df = courses[0] $ week2_df = courses[1] $ week1_df.append(week2_df)
dfRegMet = dfRegMet[dfRegMet["latitude"] < -33.015542]
shape_file = ('/g/data/r78/vmn547/GWandDEA_bex_ness/Little_GW_AOI_for_demo/kEEP_ord/KEEP_AOI.shp')
model.train(x=mtcars_filtered.col_names, training_frame=mtcars_filtered)
t.reset_index().corr()
reddit.head()
league = pd.read_sql_query('select * from League', conn)  # don't forget to specify the connection $ print(league.shape) $ league.head()
mv_lens = pd.merge(movies, ratings)
display('df1a', 'df3', "pd.merge(df1a, df3, left_index=True, right_on='name')")
post_date = djb_soup.findAll('span', {"class": "DateTime"}) $ for x in post_date: $ 	print(x.text) $
logistic_mod_time = sm.Logit(df3['converted'], df3[['intercept', 'wk2','wk3']]) $ results_time = logistic_mod_time.fit() $ results_time.summary()
people.sort_values(by="age", inplace=True) $ people
products = soup.find("div", class_ = "result-list" ) $ hemispheres = products.find_all("div", class_="item")
weather_subset = weather_mean[['Rel Hum (%)', 'Pressure (kPa)', 'Temp (deg C)']] $ weather_subset.plot(kind='bar', subplots=True, figsize=(10, 6), fontsize='large');
sns.countplot(auto_new.Hand_Drive)
selfharmmm_final_df = mf.compile_combo_dfs(epoch3_df, 'cleaned_text', selfharmmm_dictionary, nmf_cv_df, nmf_tfidf_df, lsa_cv_df, lsa_tfidf_df, lda_cv_df, lda_tfidf_df)
out = query.get_dataset(db, id=ds_info["DatasetId"][0])
twitter.name.value_counts() $
teams=pd.unique(results[['home_team','away_team']].values.ravel()) $ teams
pop_dog = timedog_df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ pop_dog.head()
cell_df['Class'] = cell_df['Class'].astype('int') $ y = np.asarray(cell_df['Class']) $ y [0:5]
pandas.DataFrame( $     get_child_column_data(observations_node) + $     get_child_column_data(observations_ext_node) $ )
bo.loc[bo['BORO']==1,'BORONAME'][0]
old_page_converted = [] $ for _ in range(n_old): $     b = np.random.binomial(1, p_old) $     old_page_converted.append(b) $ old_page_converted = np.asarray(old_page_converted)
smap = folium.Map(location=[site_dct['Latitude'], site_dct['Longitude']], tiles='CartoDB positron', zoom_start=16)
twitter_merged_data.hist(column='rating', bins=30); $ plt.title('Rating Histogram') $ plt.xlabel('Rating (Bins=30)') $ plt.ylabel('Count');
data = r.json() $ data
url = "http://www.hotashtanga.com/p/letoltesek-downloads.html" $ html_txt = urllib.request.urlopen(url).read() $ dom =  lxml.html.fromstring(html_txt) $ [line for line in dom.xpath('//a/@href')][0:10]   
address = pd.read_sql_query('select * from address', engine) $ address.head()
charge_counts = df.groupby('hash_id')['cause_num'].count()
fig, ax = plt.subplots(figsize=(8,6)) $ df_master['dog_name'].value_counts().head(20).plot(kind = 'bar', color = "blue", ax = ax, edgecolor = ['Black']*len(names)) $ ax.set_facecolor('lightgrey') $ plt.show();
result_df['country'].value_counts()
arr1d = np.linspace(start=0, stop=100, num=10, dtype=np.int8) $ arr1d
!wget https://download.pytorch.org/tutorial/faces.zip $ !unzip faces.zip
grid_heatmap.head()
news_df.drop_duplicates(subset=['Date', 'Ticker', 'description', 'title'], inplace=True)
df_enhanced.head(5)
n_booths = len(bthlst) $ n_faulty = df_bug[u'Service Location'].unique().size $ n_fit = n_booths - n_faulty $ print "Out of", n_booths, "booths", n_faulty, "had fault"
no_psc_and_psc = active_psc_records[(active_psc_records.company_number.isin(active_psc_statements[active_psc_statements.statement == 'no-individual-or-entity-with-signficant-control']\ $                                             .company_number))]['company_number'] $ print("Some examples:") $ active_companies[active_companies.CompanyNumber.isin(no_psc_and_psc)].to_csv('data/for_further_investigation/no_psc_and_psc.csv') $ active_companies[active_companies.CompanyNumber.isin(no_psc_and_psc)][['CompanyNumber','CompanyName','URI']].head(5)
category = [] $ for i in range(avg_preds.shape[0]): $     category.append(np.argmax(avg_preds[i])) $ type(category), len(category), category[0:10]
df_p = df_proc1.groupby(["CustID","BookingDate","Store","Counter"],as_index=False).agg(np.sum) $ df_p1 = df_p.loc[:,["CustID","BookingDate","Store","Counter"]] $ ivtdiff = lambda x: abs(x.shift(1) - x) $ df_p1["IVT"] = (df_p1.groupby("CustID")["BookingDate"].transform(ivtdiff)).dt.days $ df_p1["Periodicity"] = df_p1["IVT"]
df.groupby('raw_character_text')['episode_id'].nunique().reset_index().rename( $     columns={'episode_id': 'num_episodes'}).head(10)
zipShp.head()
mpl.style.use('seaborn-dark')
date_crawled_count_norm.describe()
cpi_sdmx.translate_expression('1..40055+115524+97558.10')
ds_eval = FileDataStream.read_csv(eval_file, collapse=False, header=False, names=columns, numeric_dtype=np.float32, sep='\t', na_values=[''], keep_default_na=False)
get_response('My spririt animal is a menacing cat. What is yours?')
autos['price'].value_counts().sort_index( ascending = False).head(20)
f = open('my_filename.txt', 'r') $ aa = f.read() $ aa
conn.close()
(df2[df2['landing_page'] == "new_page"].user_id.count())/290585
table = pd.crosstab(df["grade"], df["loan_status"], normalize=True) $
print("Number of Relationships in Enterprise ATT&CK") $ relationships = lift.get_all_enterprise_relationships() $ print(len(relationships)) $ df = json_normalize(relationships) $ df.reindex(['id','relationship', 'relationship_description', 'source_object', 'target_object'], axis=1)[0:5]
print(data.shape) $ print(data.columns) $ data.head()
score['is_false_positive'] = np.where((score['pred_rf']==1) & (score['is_shift']==0), 1, 0) $ score['is_false_negative'] = np.where((score['pred_rf']==0) & (score['is_shift']==1), 1, 0) $ score['is_true_positive'] = np.where((score['pred_rf']==1) & (score['is_shift']==1), 1, 0) $ score['is_true_negative'] = np.where((score['pred_rf']==0) & (score['is_shift']==0), 1, 0)
def datetimeampm2datetime(text): $     try: $         return  pd.to_datetime(text,format='%d-%m-%Y %I:%M:%S %p') $     except AttributeError: $         return text 
iris_new['Sepal'] = iris_new['SepalLength'] * iris_new['SepalWidth'] $ iris_new['Sepal'].quantile([0.25, 0.5, 0.75])
pd_aux2.describe()
y_cat = aldf['category'] $ y_cat.shape
df2_clean['p1'] = df2_clean['p1'].str.title() $ df2_clean['p2'] = df2_clean['p2'].str.title() $ df2_clean['p3'] = df2_clean['p3'].str.title()
autos["nr_of_pictures"].value_counts()
lm=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ r=lm.fit()
final_topbikes.groupby(by=final_topbikes.index.weekday)['id'].count().plot(kind='bar', figsize=(6,3))
df.query("group == 'control' and landing_page != 'old_page'").index
e_p_b_two = cfs_df[cfs_df.Beat=='5G02'] $ e_p_b_two = e_p_b_two.groupby(e_p_b_two.TimeCreate).size().reset_index()
bedr_intercept, bedr_slope = simple_linear_regression(train_data['bedrooms'], train_data['price']) $ print('Intercept: {:.2f}'.format(bedr_intercept)) $ print('Slope: {:.2f}'.format(bedr_slope))
token2id, id2token, _ = index.get_dictionary() $ print(list(id2token.items())[:15])
frequent_authors = authors[(authors['count'] >= 100) & (authors['mean'] >= 0.3)] $ frequent_authors.shape[0]
df_potholes.groupby(df_potholes.index.weekday)['Created Date'].count().plot(kind="bar") $
print(pd.DataFrame(test_matrix).head())
ok.grade('q02')
df_geo.columns = ['CaseID', 'Address', 'Tract', 'Block'] $ df_geo['Tract'] = pd.to_numeric(df_geo.Tract)/100. $ df_geo[['CaseID', 'Block']] = df_geo[['CaseID', 'Block']].apply(pd.to_numeric)
p_new = df2['converted'].mean() $ p_old = p_new $ print('p_new:', p_new, 'p_old:', p_old)
autos['price'] = (autos['price'] $                  .str.replace('$','') $                  .str.replace(',','') $                   .astype(float) $                  )
data.loc[(80,slice('20150117','20150417'),'put'),:].iloc[:,0:4]
np.exp(0.0150)
@pyimport pandas_datareader.data as pdrd $ stocks = ["GLD", "TLT", "XHB", "XLB", "XLE", "XLF", "XLU", "XLV"]  $ df = pdrd.get_data_yahoo(stocks, "01/01/2010", interval="m") $
props.head()
naive_preds = np.repeat(np.mean(y_tr), X_val.shape[0]) $ np.sqrt(metrics.mean_squared_error(y_val, naive_preds))
fig=plt.subplot(1,1,1) $ fig.plot(track.speed_gps, [sqrt(track["estimated accuracy"][i]) for i in range(track.shape[0])], "b^") $ fig.figure.set_size_inches(16,16) $ fig.axis((0,7,0,7)) $
leaderboard = pd.read_json('../metadata/leaderboards.json') $ leaderboard['best'] = pd.to_numeric(leaderboard['best'], errors='coerce') $ leaderboard.head()
fig = m.plot_components(forecast);
def get_list_tot_likes(the_posts): $     list_tot_likes = [] $     for i in list_Media_ID: $         list_tot_likes.append(the_posts[i]['activity'][-1]['likes']) $     return list_tot_likes
print("%s" % dataset.description) $ print("%d documents" % len(dataset.data)) $ print("%d categories" % len(dataset.target_names))
table_rows = driver.find_elements_by_tag_name("tbody")[24].find_elements_by_tag_name("tr") $
twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()==False].shape[0]
df2.query('converted==1')['user_id'].count()/df2.shape[0]
sNew = pd.Series([1,2,3,4,5], $                  index=pd.date_range('20180102', periods=5)) $ df['F'] = sNew $ df
dataset.head()
df3 = df2.merge(c, on ='user_id', how='left') $ df3.head()
old_page_converted=np.random.binomial(nold,nullrate) $ old_page_converted
sum(contractor.state_id.isnull()) #the count of missing state_id value is 0 $ contractor.state_id.value_counts() #The state_id columns do not have missing data
trend_de.head()
ref_dict = reference_frame.set_index('bhc_id')['subject_id'].to_dict() $ patient_group_dict = patient_group.set_index('bhc_id')['reg_group'].to_dict()
df = pd.DataFrame(one_year_prcp, columns=['station', 'date', 'prcp', 'tobs']) $ df.head()
LUM.plot_time_all(all_lum_binned)
log_mod_interact = sm.Logit(df_new_npage['converted'], df_new_npage[['intercept', 'US', 'UK']]) $ log_mod_interact_results = log_mod_interact.fit() $ log_mod_interact_results.summary()
pipe = pc.PipelineControl(data_path='examples/simple/data/varying_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=True, $                           sliding_window_size=500) $ pipe.runPipeline()
users[users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))]['DaysActive'].plot(kind = 'hist') $ plt.title('Histogram of Number of Active Days \n for Users that Left Permanently') $ plt.xlim((0,2070)) $ plt.xlabel('Days Active')
import pandas as pd $ import matplotlib.pyplot as plt
decoder_model_inference.save('decoder_model_inference.h5')
np.corrcoef(total_sales.ratio.values, total_sales.sales_change_growth.values)
STD_reorder_stats.describe()
df.replace('712-2',51529,inplace=True) $ df[df['zip']==51529].head()
daily = hourly.asfreq('D') $ daily
%sql \ $ SELECT twitter.tweet_text FROM twitter \ $ WHERE twitter.tweet_text LIKE "%Roger Federer%";
df.head(12)
train_reduced = train_pos.union(train_neg).orderBy(func.rand(seed=seed)) $ train_reduced.cache() $ validation = validation.cache() $ print('reduced training set size:', train_reduced.count()) $ print('validation set size:', validation.count())
from matplotlib.pyplot import figure $ figure(num=None, figsize=(17, 4), dpi=80, facecolor='w', edgecolor='k') $ genders = list(gender_freq_hist.index) $ frequencies = list(gender_freq_hist.gender_freq) $ plt.bar(genders, frequencies)
dtm.shape
merged_data['payment_day'] = merged_data['last_payment_date'].dt.day $ merged_data['payment_month'] = merged_data['last_payment_date'].dt.month $ merged_data['payment_year'] = merged_data['last_payment_date'].dt.year
from sklearn.model_selection import train_test_split $ test_data_size = 0.25 $ seed = 8 $ X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=test_data_size, random_state=seed) $ X_train.head()
duration = tia['date'].apply(lambda x:dt.date(2018,1,30) -  x.date())
pd.value_counts(preds).plot.bar()
print(model.summary())
def datetimeinvertedstring2datetime(text): $     try: $         return  pd.to_datetime(text,format='%Y-%m-%d %H:%M:%S') $     except AttributeError: $         return text 
df_spl['place'] = df_spl['place'].apply(lambda x: translate(x,'en')) $ df_spl['text'] = df_spl['text'].apply(lambda x: translate(x,'en'))
df2.drop_duplicates('user_id', keep='first', inplace=True) $ df2[df2['user_id'] == repeated_id]
from sklearn.cross_validation import train_test_split $ x = my_df.text $ y = my_df.target $ SEED = 2000 $ x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.10, random_state=SEED)
spark.stop()
StockData.loc[StockData.Date < StartDate, 'Set'] = 'history' $ print("We now have {:,} rows marked as 'history'".format(len(StockData.loc[StockData.Set == 'history']))) $ print("We now have {:,} rows marked as 'train'".format(len(StockData.loc[StockData.Set == 'train'])))
train[train.url.isnull()].head()
b_dist =bnbx[(bnbx['age']<80) & (bnbx['age']>=18)] $ bnbx['age'] = bnbx.age.fillna(28) $
df_c.head()
dt = datetime.datetime.today() $ td = datetime.timedelta(hours=6, minutes=42, microseconds=123456) $ dt + td
topics = set() $ df['tags'].apply(lambda x: topics.update(x)) $ topics = tuple(topics) $ topics = sorted(topics) $ print('topics updated and sorted')
ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'] $ ).groups
lsi.print_topics(2)
df[df['group']=='treatment'].query('landing_page != "new_page"').shape
s = pd.Series([99,5,60], index = ['HPI','Int_rate','Low_tier_HPI']) $ df1.append(s,ignore_index=True) # append the series to the data frame. The line 4 is appended $
sns.distplot(data.song_freq)
y3, X3 = patsy.dmatrices('DomesticTotalGross ~ Constant + Budget + G + PG + PG13 + R', data=df, return_type="dataframe") $ model = sm.OLS(y3, X3, missing='drop') $ fit3 = model.fit() $ fit3.summary()
latest_df.classification_id.nunique() == len(latest_df)
col_names = list(zip(df_test.columns, df_train.columns)) $ for cn in col_names: $     assert cn[0] == cn[1]
auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $ auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $ api = tweepy.API(auth) $ public_tweets = api.home_timeline() $ data = pd.DataFrame(data=[tweet.text for tweet in public_tweets], columns=['Tweets'])
bigram_sentences = LineSentence(bigram_sentences_filepath) $ for bigram_sentence in it.islice(bigram_sentences, 230, 240): $     print u' '.join(bigram_sentence) $     print u''
today_ = datetime.date.today().strftime("%Y_%m_%d") $ today_ = '2018_06_07' $ print("today is " + today_)
def remove_from_word_list(original_list, list_subset): $     for each in list_subset: $         idx = original_list.index(each) $         del original_list[idx] $     return original_list
save_data = (D1, D2) $ import pickle $ with open("processed_data.pkl", 'wb') as f: $     pickle.dump(save_data, f)
del merged_portfolio_sp_latest_YTD['Date'] $ merged_portfolio_sp_latest_YTD.rename(columns={'Adj Close': 'Ticker Start Year Close'}, inplace=True) $ merged_portfolio_sp_latest_YTD.head()
BallBerry_ET_Combine = pd.concat([BallBerry_rootDistExp_1, BallBerry_rootDistExp_0_5, BallBerry_rootDistExp_0_25], axis=1) $ BallBerry_ET_Combine.columns = ['BallBerry(Root Exp = 1.0)', 'BallBerry(Root Exp = 0.5)', 'BallBerry(Root Exp = 0.25)']
user.loc["Trump", "location"]
clean_trips.to_csv(path_or_buf='trips_clean.csv',sep=',',index_label=False)
avg_reorder_days = prior_orders.groupby(["user_id"])['days_since_prior_order'].aggregate('count').reset_index(name='avg_days_prior_order') $ avg_reorder_days.head()
days_alive = (datetime.datetime.today() - datetime.datetime(1981, 6, 11)) $ days_alive.days
met.T.plot(kind='barh', figsize=(5,3), xlim=(0,1)) $ met
twitter_data.tweet_id.unique().size
df["Date"] = pd.to_datetime(df["Date"]) $ df.sort_values("Date", inplace=True) $ df.reset_index(drop=True, inplace=True) $ df.head()
train.OPTION.value_counts()
typ = [f.dataType for f in smpl_join.schema.fields]
data = data.dropna(); data
missing_nums = df.isnull().sum() $ print(missing_nums.iloc[missing_nums.nonzero()[0]])
import pandas as pd $ import numpy as np $ from sklearn.tree import DecisionTreeRegressor
pres_df['metro_area'] = pres_df['split_location_tmp'].map(lambda x: x[0]) $ pres_df['metro_area'].head()
model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
prod_paudm.columns.values
import pandas as pd $ pd.read_table("adj_close_stock_data_yahoo_2005_2010.txt",parse_dates={"Date":[0,1,2]},delim_whitespace=True,na_values=["-"], index_col="Date") $
def compute_sentence_length_variance(text_list): $     return np.var([len(x) for x in text_list]) $ compute_sentence_length_variance(['huhuh.', 'sbbbasdsads', 'jj djdjd', '1'])
temp = df2.landing_page.value_counts() $ prob = temp/temp.sum() $ print "Probability of an individual received the new page:", prob[0]
utils.read_sas_write_hdf(source_paths, data_dir, 'nhanes.h5', downcast=False, verbose=False)
invoice_hub_dropper = ['deleted_at','fk_s_change_context_id_cr', 'fk_s_change_context_id_dl','fk_x_billing_account_hub_id', 'fk_x_subscription_hub_id', 'sk_id'] $ for col in invoice_hub_dropper: $     print invoice_hub[col].value_counts()
%%timeit $ with tb.open_file(filename='data/NYC-yellow-taxis-100k.h5', mode='r') as f: $     table = f.get_node(where='/yellow_taxis_2017_12') $     rows = table.where('(passenger_count == 1) | (passenger_count == 3)') $     amounts = [x['total_amount'] for x in rows]
cur.fetchall()
trainDF.head()
autos.columns
beirut['Mean Humidity'].std(), summer['Mean Humidity'].std()
bob = np.array(problems)[:, 1].tolist()
exportID['join_col'] = exportID['event.origin_subzone_name'] + exportID['timestamp']
sc.stop()
%matplotlib inline $ import numpy as np $ import matplotlib.pyplot as plt $ import openmc $ import openmc.mgxs as mgxs
df = df.reset_index() $ df.irlco.sum()
time_string = "2017-09-01 00:00:00" $ delta_hours = 1 $ start_time = pd.to_datetime(time_string)
np.array(p_diffs) $ plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('size of p_diffs') $ plt.title('Sampling distribution of difference in means') $
df.head()
df.drop('match', axis = 1, inplace = True); #drops the match column above
pulledTweets_df.sentiment_predicted_lr.value_counts().plot(kind='bar', $                                                            title = 'Classification using Logistic Regression model') $ plt.savefig('data/images/Pulled_Tweets/'+'LR_class_hist.png')
df['Memo'].apply(returnCategory).value_counts()
nzi = pd.notnull(train_data["totals.transactionRevenue"]).sum() $ nzr = (revenue["totals.transactionRevenue"]>0).sum() $ print("Number of instances in train set with non-zero revenue : ", nzi, " and ratio is : ", nzi / train_data.shape[0]) $ print("Number of unique customers with non-zero revenue : ", nzr, "and the ratio is : ", nzr / revenue.shape[0])
tablename='settings' $ pd.read_csv(read_inserted_table(dumpfile, tablename), $             delimiter=",", $             error_bad_lines=False).head(10)
old_page_converted = np.random.binomial(n_old,p_old) $ print('The old_page_converted is: {}.'.format(old_page_converted))
plt.hist(p_diffs) $ plt.axvline(x=0.000913, color='r')
resdf=resdf.drop(['Unnamed: 0'], axis=1) $ resdf.head(3) $
a = bnbAx[bnbAx['language']=='en'].first_browser.value_counts()/len(bnbAx[bnbAx['language']=='en']) $ a.head()
grouper = visits.groupby((visits.address, visits.dba_name))
cassession.builtins.serverstatus()
loans_act_20150430_xirr=cashflows_act_investor_20150430.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
autos["registration_year"].describe()
product_time = nbar_clean[['time', 'product']].to_dataframe() #Add time and product to dataframe $ product_time.index = product_time.index + pd.Timedelta(hours=10) #Roughly convert to local time $ product_time.index = product_time.index.map(lambda t: t.strftime('%Y-%m-%d')) #Remove Hours/Minutes Seconds by formatting into a string
X_train, X_test, y_train, y_test = train_test_split(X_s_n, y_sc, test_size=0.5, random_state=42)
df.loc[1,"last_name"] = "Kilter" $ df
f_counts_hour_ip = spark.read.csv(os.path.join(mungepath, "f_counts_hour_ip"), header=True) $ print('Found %d observations.' %f_counts_hour_ip.count())
evaluator.plot_confusion_matrix(normalize=True, $                                 title='Confusion matrix, with normalization', $                                 print_confusion_matrix=False, $                                 figsize=(8,8), $                                 colors=None)
data.dropna(how='all')
top_headlines = newsapi.get_top_headlines(q='trump', sources='bbc-news', language='en') $ top_headlines['articles'][0]
%matplotlib notebook $ import matplotlib.pyplot as plt
df['month'] = df.activity_date_time_c.dt.to_period('M')
V0 = 17.6639 $ r = 0.01
twitter_archive_df_clean['timestamp'].sort_values(ascending=False).head()
clf = svm.SVR() $
aqmdata.describe()
df['Start Date'].head()
order_data.shape
logit = sm.Logit(df_new['converted'], df_new[['intercept', 'US','UK']]) $ results = logit.fit() $ results.summary()
save_n_load_df(promo_df, 'promo_df3.pkl')
props.value_counts()
old_page_converted = np.random.choice(np.arange(2), size=n_old, p=[(1-p_old), p_old]) $
df_link_match_unique.to_csv('data/unique_links.csv', index=False)
summer[['Mean TemperatureC', 'Precipitationmm']].plot(grid=True, figsize=(10,5))
twitter_ar.rating_num.value_counts()
df_2007.dropna(inplace=True) $ df_2007
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113515380318786.json', encoding="utf-8-sig") as data_file: $     json_data3 = j.load(data_file)
run txt2pdf.py -o "MAIMONIDES MEDICAL CENTER  Sepsis.pdf"   "MAIMONIDES MEDICAL CENTER  Sepsis.txt"
zip_file.filelist
np.exp(results.params)
vi = vio2016.groupby(["business_id", "date"]).count().reset_index() $ pd.merge(vio2016, vi, on=["business_id", "date"]) $ vi = vi.iloc[:, :3] $ vi $ vio2016 = pd.merge(vio2016, vi, on=["business_id", "date"])
!head ../../data/msft_modified.csv
predictDT_on_test = model_dt.predict(X_test) # model.predict_proba(X_test) # for predicting in probabilities $ predictDT_on_test $ predictRF_on_test = model_rf.predict(X_test) # model.predict_proba(X_test) # for predicting in probabilities $ predictRF_on_test
test_data.tail()
df[['Open', 'Close']].plot(figsize=(15,5));
df2.query('group == "control"').user_id.size
df_reg=injuries_hour[['date_time','Rain','injuries','wet','low_vis']] $ df_reg['hour']=pd.to_datetime(df_reg.date_time).dt.hour $ df_reg.head() $
data_issues_transitions.head()
'my string my'.find('x')
for row in my_df_small.iterrows(): $     print(row)
image_predictions_df.head()
japandata["Non_Financial_Credit_Ratio"] =japandata.Credit_2NonFinSec *10**9 / japandata.GDP /japandata.Yen2USD
mean_fn = (lambda x: np.mean(x))
au.clear_dir('data/city-util/proc')
bacteria2 = pd.Series(bacteria_dict, $                       index=['Cyanobacteria','Firmicutes', $                              'Proteobacteria','Actinobacteria']) $ bacteria2
pd.DataFrame(result, columns=schema)
no_hyph = df_nona[df_nona['variety']\ $                     .apply(lambda x: len(x.split('-')) < 2)]['variety'].str.lower() $ no_hyph = no_hyph[no_hyph.apply(lambda x: x.split()[-1] != 'blend')].replace(repl_dir)
n_old = df2[df2['landing_page']=="old_page"].count()[0] $ n_old
train['is_weekend'] = train['start_timestamp'].map(lambda x: 1 if x.weekday() in [5,6] else 0) $ test['is_weekend'] = test['start_timestamp'].map(lambda x: 1 if x.weekday() in [5,6] else 0)
uniqueCreatedTimeByUser = firstWeekUserMerged[['userid','createdtm']].drop_duplicates() $ uniqueCreatedTimeByUser.head(5)
big_df_count=big_df_count.reset_index() $ big_df_avg=big_df_avg.reset_index()
z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]),np.array([n_new,n_old]), alternative = 'larger')
io2 = ioDF.copy()
compound_sub2 = compound_sub2.append(compound_wdate_df4)
df.eval('D = (A - B) / C', inplace=True) $ df.head()
pd.Series(['a', 'b', 'c']).dtypes
print("Actual difference:" , Actual_diff) $ p_greater_than_diff = len(greater_than_diff)/len(p_diffs) $ print('Proportion greater than actual difference:', p_greater_than_diff) $ print('As a percentage: {}%'.format(p_greater_than_diff*100))
test_df.head(10)
print((X_train_all.select_dtypes(include=['O']).columns.values))
cols = ['bizname', 'license_loc', 'instate_loc', 'mailing_loc', $         'license_no', 'lic_date', 'status', 'cr_date', 'action'] $ df = pd.DataFrame(columns=cols)
train = pd.read_csv('./Input/train.csv', low_memory=False, index_col='id') $ if kaggle: $     if sim == False: $         test = pd.read_csv('./Input/test.csv', low_memory=False, index_col='id') $ res = pd.read_csv('./Input/resources.csv', low_memory=False, index_col='id')
len(conditions.unique())
median = np.percentile(df['num_comments'], 50) $ per_75 = np.percentile(df['num_comments'], 75) $ print("The median of the num of comments: ", median) $ print("The 75th percentile of the num of comments: ", per_75)
year8 = driver.find_elements_by_class_name('yr-button')[7] $ year8.click()
@pyimport numpy as np $ a = PyObject(np.array(1:10)) $ jpyArr = a[:reshape](5,2)  # uses the Python reshape method for an ndarray object (not Julia's reshape function)
df.loc[monthMask, 'water_year'] = df['year'] + 1
df3['country'].value_counts()
df_trips.to_csv("data/generated-marketplace-trips.csv", index=False) $ df_planets.to_csv("data/generated-marketplace-planets.csv", index=False) $ df_pilots.to_csv("data/generated-marketplace-pilots.csv", index=False) $ df_passengers.to_csv("data/generated-marketplace-passengers.csv", index=False)
sns.heatmap(viscov, $  cmap = 'Blues')
obs_diff_mean = new_page_converted.mean() - old_page_converted.mean() $ obs_diff_mean
control_conv_prob = df2.loc[(df2["group"] == "control"), "converted"].mean() $ control_conv_prob
TEXT.numericalize([md.trn_ds[0].text[:12]])
payment = pd.get_dummies(auto_new.Payment_Option) $ payment.head()
y_pred1 = linreg.predict(X_test) $ print np.sqrt(metrics.mean_squared_error(y_test, y_pred1))
Station = Base.classes.station $ Measurement = Base.classes.measurement $
a.keys()-b.keys()
import sys $ reload(sys) $ sys.setdefaultencoding('utf-8')
df2[df2.duplicated('user_id')==True].user_id
knn.fit(train[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2','floor','rooms']], train['expenses'])
df['mood']= df['SA'].rolling(window=5).apply(lambda x : np.sum(x))
non_grad_age_mean = records3[records3['Graduated'] == 'No']['Age'].mean() $ non_grad_age_mean
git_log.info()
all_df.describe()
df.head(3)
n_new=df2.query("landing_page=='new_page'").user_id.count() $ n_new
logit_mod = sm.Logit(df_new["converted"], df_new[["intercept","CA", "UK"]]) $ results = logit_mod.fit() $ results.summary()
df = pd.read_csv('trump_lies.csv', parse_dates=['date'], encoding='utf-8')
np.sin(df * np.pi / 4)
probs = alg.predict_proba(X_test) $
vol = pd.DataFrame(raw, columns=['Volume']) $ vol.head()
unique = df2.user_id.nunique $ unique()
from sklearn.metrics import log_loss $ y_pred_proba = y_pred['probabilities'] $ log_loss(y_test, y_pred_proba)
data.head()
autos.describe(include = "all")
df2.query("user_id==773192")
pickle.dump(tfidf_fitted, open('iteration1_files/epoch3/tfidf_fitted', 'wb'))
autos["odometer"] = autos["odometer"].str.replace("km", "").str.replace(",", "").astype(int) $ autos.rename({"odometer": "odometer_km"}, axis = 1, inplace = True)
df2['intercept']=1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
print(model.wv.similarity('human', 'party')) $ print(model.wv.similarity('tree', 'murder'))
df.location_id.shift().head()
result2 = df.query('A < 0.5 and B < 0.5') $ np.allclose(result1, result2)
obj.drop('c', inplace=True)
old_page_converted = np.random.choice([0,1], p=[1 - p_old, p_old], size=n_old)
fld = 'StateHoliday' $ df = df.sort_values(['Store', 'Date']) $ get_elapsed(fld, 'After') $ df = df.sort_values(['Store', 'Date'], ascending=[True, False]) $ get_elapsed(fld, 'Before')
wod_df_loc.to_csv("C:/Users/sethh/OneDrive/Desktop/Springboard/Capstone Project 1/wod_df_loc.csv") $ wod_df_date.to_csv("C:/Users/sethh/OneDrive/Desktop/Springboard/Capstone Project 1/wod_df_date.csv") $ hurricanes_df.to_csv("C:/Users/sethh/OneDrive/Desktop/Springboard/Capstone Project 1/hurricanes_df.csv")
master_df['human?'] = master_df['name'].str.match(r"H\d\d") #creates new boolean column $ human = gen_filter(master_df, 'human?', True) $ mouse = gen_filter(master_df, 'human?', False) $ mouse
converted_control = df2.query('group == "control"')['converted'].mean() $ print("Given that an individual was in the control group, the probability they converted is{0: .4} ".format(converted_control))
def KL(a, b): $     a = np.asarray(a, dtype=np.float) $     b = np.asarray(b, dtype=np.float) $     return np.sum(np.where(a != 0, a * np.log(a / b), 0))
vi_ok['VIOLATIONCODE'].values
df1 = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4']) $ print(df1)
df_actor[df_actor.last_name.str.contains('LI')].sort_values(by=['last_name', 'first_name'])
y = df['loan_status'].values $ y[0:5]
cdate=[x for x in building_pa.columns if 'date' in x] $ cdate $
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.groupby('country').count()
pandas_ds = pandas.DataFrame(data_list, columns=["gerrit_issue", "gerrit_opening_date", "gerrit_closing_date", "time2close", "gerrit_tracker", "current_status"]) $ pandas_ds.columns.values.tolist()
blah = sub_df['text'].apply(sentiment_scores)
output= "CREATE TEMPORARY TABLE ABC AS select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc" $ cursor.execute(output) $
clf_RGF_tfidf = RGFClassifier(max_leaf=240, $                               algorithm="RGF_Sib", $                               test_interval=100, $                               verbose=False,).fit(X_traincv_tfidf, y_traincv_tfidf)
ls_other_columns = df_uro.loc[:, ls_both].columns
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx" $ df = pd.read_excel(path, sheetname = 'Superstore2') $ df.head(5)
df_transactions.info()
tweet_text2 = tweets2['text'].values $ clean_text2 = [preprocess_text(x, fix_unicode=True, lowercase=True, no_urls=True, no_emails=True, no_phone_numbers=True, no_currency_symbols=True, $                               no_punct=True, no_accents=True) $               for x in tweet_text2]
new_df = new_df.set_index('index')
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2) $ store_items
hexbin = sns.jointplot(x="item", y="sentiment", data=dta, kind="scatter") $
df2[ids.isin(ids[ids.duplicated()])]
np.linspace(0, 10, 5) #5 equally spaced points between 0 and 10
url = "https://mars.nasa.gov/news" $ response = requests.get(url) $ soup = bs(response.text, 'html.parser') $ print(soup.prettify())
one_station['weekday']=one_station['DATE'].apply(dt.weekday) $ one_station.head()
output2.count()
df1_cols = df1.columns.tolist() $ print (df1_cols)
new_group.get_group('N')
with open('../../vectors/GloVe_scratch_files/vectors.txt', 'r') as file: $     lines = file.readlines() $
keto = pmol.df[pmol.df['atom_type'] == 'O.2'] $ print('number of keto groups: %d' % keto.shape[0]) $ keto
austin['hour'] = austin['started_on'].dt.round('H').dt.hour $ df1 = austin.pivot_table(index='hour', columns='weekday', values='started_on', aggfunc='count') $ df1 = df1.reindex_axis(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'], axis=1) $ plt.figure(figsize = (16,5)) $ sns.heatmap(df1, cmap="YlGnBu").set_title("Ride Requests : Day of Week & Hours")
import plotly $ plotly.tools.set_credentials_file(username='dgebert18', api_key='wWcelh2OcxQaebCFxNBF')
result = Valid_events.copy() $ result.insert(loc=1, column='valid_result', value=Vy_pred) $ result
precipitation_df.dropna(inplace=True) $ precipitation_df.head()
time_hour_for_file_name = 0 #datetime.datetime.now().time().hour
pd.crosstab(aqi['AQI Category_eug'], aqi['AQI Category_cg'], normalize='index', margins=True).plot(kind='bar', stacked=True);
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
tweets_df.name.describe()
df_h1b_nyc_ft.pw_unit_1.value_counts()
data = data_train.append(data_test)
df['domain_d'].nunique()
print "TMDB's genre id:" , lostintranslation.info()['genres'][0]["id"] $ print "TMDB's genre name:" , lostintranslation.info()['genres'][0]["name"]
segmentData.lead_source.value_counts()
bigdf_read.loc[bigdf_read['submission_title'].isnull()]
intervention_history.reset_index(inplace=True) $ intervention_history.set_index(['INSTANCE_ID', 'CRE_DATE_GZL'], inplace=True)
k1.head()
abc = Grouping_Year_DRG_discharges_payments.groupby(['year','drg3']).get_group((2015,871)) $ abc.head()
education_2011_2015.head()
components3= pd.DataFrame(pca.components_, columns=['SP500','DJIA','happiness'])
df_notnew = df.query('landing_page != "new_page"') $ df_3 = df_notnew.query('group == "treatment"') $ df_3.nunique() $
df.head()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
plt.plot(ds_issm['time'],ds_issm['met_salsurf_qc_executed'],'b.') $ plt.title('CP03ISSM, OOI QC Executed SSS') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
df2.query('group == "control"').converted.sum()/df2.query('group == "control"').converted.count()
tweet_archive_clean.head(30)
train.pivot_table(values = 'Fare', index = 'Age_bin', aggfunc=np.mean)
print(festivals.at[2,'latitude'])
z =  np.round(sum(successful_users.values())/len(positive_results), 3)*100 $ print('The top 20 users have {}% of the positive results'.format(z))
dfcounts.head()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=100, random_state=None, shuffle=True) $ estimator = Ridge(alpha=30000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df['date'] = pd.to_datetime(df.date) $ df.set_index('date', inplace=True)
merged_data = pd.merge(trip_data, weather_data, left_on = ["start_date","zip"], right_on = ["Date","zip"])
corr = df_pivot.corr()
df_all_users['Email Address'] = df_all_users['Email Address'].apply(lambda x: str(x.lower().strip())) $ df_all_users['Email Address'] = df_all_users['Email Address'].astype(str)
free_data.groupby('age_cat')['educ'].mean()
sorted_m3 =m3.ravel() $ sorted_m3[::-1].sort() $ sorted_m3=sorted_m3.reshape(2,2) $ print("sorted m3: ", sorted_m3)
likes.head(3)
valid_countries = set(countries["Countries"].tolist()) $ allteams = allteams & valid_countries $ print (len(allteams))
print(df2['landing_page'].value_counts()[0]/len(df2))
fig = plt.figure() $ ax = fig.add_subplot(1,1,1) $ ax.scatter(filtered_df['accommodates'],filtered_df['bedrooms']) $ plt.show()
run txt2pdf.py -o"2018-06-19 2015 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 FLORIDA HOSPITAL Sorted by discharges.txt"
trend_de = googletrend[googletrend.file == 'Rossmann_DE'] $ trend_de.head()
ndvi_us = ndvi_nc.variables['NDVI'][0, lat_li:lat_ui, lon_li:lon_ui] $ np.shape(ndvi_us)
series1.cov?
s = pd.Series([1,3,5,np.nan,6,8]) $ print(s)
print(datetime.now() - timedelta(hours=1)) $ print(datetime.now() - timedelta(days=3)) $ print(datetime.now() + timedelta(days=368, seconds=2))
logit_country = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'country_US', 'country_UK']]) $ results3 = logit_country.fit()
offices = pd.read_csv('./data/Offices.csv')
global batch_sz $ print(batch_sz) $ modelrest = trainModel(model, trX.reshape(-1,24,1), trY.reshape(-1,1), 1, batch_sz,2)
infile = open("data_s2_ass4.xml", "r") $ contents = infile.read() $ soup = BeautifulSoup(contents, "lxml") $ infile.close()
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC-ST MARY'S HOSPITAL - 2012 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC-ST MARY'S HOSPITAL - 2012 Percentiles.txt"
xx=list(building_pa_prc_zip_loc['permit_type'].unique()) $ aux_list=[] $ for x in xx: $     aux_list.append((x,set(building_pa_prc_zip_loc.permit_type_definition[building_pa_prc_zip_loc['permit_type']==x]))) $ aux_list
autos["seller"].value_counts()
df_Q123.head()
aa='2016-09-17T09:17:46Z' $ aa.split('T')[1][:-1]
len(train_data[train_data.fuelType == 'andere'])
lm = Logit(y, X) $ res = lm.fit()
v = variables_df[variables_df['VariableCode'] == 'TP'] $ variableID = v.index[0] $ results = read.getResults(siteid=siteID, variableid=variableID, type="Measurement") $ resultIDList = [x.ResultID for x in results] $ len(resultIDList)
dcrime_gb.to_csv(processed_path+"crime_incident_count.csv")
dfs[1].head()
x = K.placeholder(dtype="float", shape=X_train.shape) $ target = K.placeholder(dtype="float", shape=Y_train.shape) $ W = K.variable(np.random.rand(dims, nb_classes)) $ b = K.variable(np.random.rand(nb_classes))
df_goog.Open.resample('M').plot() $ df_goog.Open.asfreq('M', method='backfill').plot()
df_dummy = pd.get_dummies(data=df_country, columns=['country']) $ df3 = df2.set_index('user_id').join(df_dummy.set_index('user_id')) $ df3.head()
nullrate=df2[df2.converted==1].count()[0]/df2.count()[0] $ nullrate
for row in selfharmm_topic_names_df.iloc[4]: $     print(row)
retweets = df[~df["retweeted_screen_name"].isnull()] $ retweet_pairs = retweets[["id","screen_name","retweeted_screen_name"]].groupby(["screen_name","retweeted_screen_name"]).agg({"id":"count"}).rename(columns={"id":"Weight"}) $ retweet_pairs.reset_index(inplace=True) $ retweet_pairs.sort_values("count",ascending=False).head()
np_array = np.random.randint(1, 10, size = 16).reshape(4, 4) $ print(np_array) $ df = pd.DataFrame(np_array) $ df
df = w.data_handler.get_all_column_data_df() $ df.columns $ df[df.SEA_AREA_NAME.isnull()].loc[:,['DEPH', 'SALT_BTL', $      'SDATE', 'VISS_EU_CD', 'WATER_BODY_NAME', 'SEA_BASIN', $    'SECCHI', 'TEMP_BTL', 'VISS_EU_ID', 'WATER_DISTRICT', 'WATER_TYPE_AREA', 'MONTH']] $
state_samples = np.array([env.observation_space.sample() for i in range(10)]) $ discretized_state_samples = np.array([discretize(sample, state_grid) for sample in state_samples]) $ visualize_samples(state_samples, discretized_state_samples, state_grid, $                   env.observation_space.low, env.observation_space.high) $ plt.xlabel('position'); plt.ylabel('velocity');  # axis labels for MountainCar-v0 state space
Results_kNN1000d.to_csv('soln_kNN1000_dist.csv', index=False)
merged.head()
def calc_p(array): $     p = array.mean() $     return p
X_train_dtm = pd.DataFrame(cvec.fit_transform(X_train.title).todense(), columns=cvec.get_feature_names())
n_new, n_old = df2['landing_page'].value_counts() $ print "N_new:", n_new
df1.shape, df2.shape, df3.shape
%run word2vec2tensor.py -i /tmp/doc_tensor.w2v -o cornell-movie
sum(data.days_repayment.isnull())
len(df2.query('converted == 1')) / len(df2)
to_be_predicted_Day5 = 31.23097277 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
dti = pd.to_datetime(['Aug 1, 2014', '2014-08-02', '2014.8.3', None]) $ dti
scipy.stats.kruskal(df3["tripduration"], df4["tripduration"])
for j in test_dum.columns: $     if j not in train_dum.columns: $         print j
1/np.exp(-0.0150) $
quandl.ApiConfig.api_key = input('Please enter your quandl Key: ') $ exchange = 'WIKI'
my_tree_one = tree.DecisionTreeClassifier() $ my_tree_one = my_tree_one.fit(features_one, target)
two_day_sample.set_index('timestamp', inplace=True)
print('Largest Change in any one day:', change.max())
browser.click_link_by_id('full_image')
for dataset in full_data: $     dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median()) $ train['CategoricalFare'] = pd.qcut(train['Fare'], 4) $ print (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())
np_cities = np.array(cities)
train_session.isnull().sum()/train_session.shape[0]
station_count = session.query(Station).count() $ station_count
pred.sample(10)
pd.DataFrame(data2.groupby('name').joy.mean())
lr = K.constant(0.01) $ grads = K.gradients(loss, [W,b]) $ updates = [(W, W-lr*grads[0]), (b, b-lr*grads[1])]
autos[autos["price"].between(50,1000000)]["price"].describe()
df.T.describe().T["count"].plot()
station_count = measure_df['station'].nunique() $ station_names = measure_df['station'].unique() $ print(station_count) $ print(station_names)
autos["brand"].value_counts(normalize = True).head(10)
gameids.get_new_data(Season='2016-17',PlayerOrTeam = "T") $ gameids._set_api_parameters(Sorter = "GAME_DATE") $ gameids2017 = pd.DataFrame(gameids.game_list()) $ gameids2017.head()["GAME_DATE"]
df.isnull().sum()
new_logit_mod = sm.Logit(df_comb['converted'], df_comb[['intercept', 'ab_page', 'CA', 'UK']]) $ new_results = new_logit_mod.fit() $ new_results.summary()
import pickle $ with open("processed_data.pkl", 'rb') as f: $     tweets, bible_data = pickle.load(f)
abc.to_csv('Completed sepsis pass.csv',index=False)
autos = autos[autos['price'].between(1,150000)]
grouped_dpt.groups # groups 
logits = tf.layers.dense(embed, len(vocab), activation=None, $     kernel_initializer=tf.random_normal_initializer())
from sklearn.metrics import logloss $ loss = log_loss(y_val, val_pred, rf.classes_) $ mae = sum(abs(np.subtract(y_val, val_pred)))/len(y_val)
p_control_conv = df2.query('group == "control"')['converted'].mean() $ p_control_conv
twitter_df_clean['stage_name']="None" $ twitter_df_clean.iloc[191,13]="puppo" $ twitter_df_clean.iloc[200,13]="floofer" $ twitter_df_clean.iloc[191,7]="None" $ twitter_df_clean.iloc[200,7]="None"
linear_prediction = linear_model.predict(X_test) $ RandomForestRegressor_prediction = RandomForestRegressor_model.predict(X_test) $ group_B_predict = (linear_prediction+RandomForestRegressor_prediction)/2
frame = pd.DataFrame(data)
df['retweets'].describe()
4.832214765100671 * 298
summary = records.describe(include='all') $ summary
ol.data['funding_rounds']
combined_factor_df['intercept'] = 1 $ logistic_model = sm.Logit(combined_factor_df['converted'], combined_df[['intercept','US_ab', 'CA_ab']]) $ result = logistic_model.fit()
store.head()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[np.mean([p_new, p_old]), (1-np.mean([p_new, p_old]))]).mean() $ old_page_converted
vect = CountVectorizer(ngram_range=(1,2)) $ vect.fit_transform(['no i have cows', 'i have no cows']).toarray(), vect.vocabulary_
top20_mosttweeted = top20_df['prediction_1'].value_counts() $ top20_mosttweeted = pd.DataFrame(top20_mosttweeted).reset_index() $ top20_mosttweeted.columns = ['prediction_1', 'tweet_count']
titanic.survived.head()
event_sdf.toPandas().head()
rddRow = rdd.map(lambda f: Row(col1 = f)) $ df = spark.createDataFrame(rddRow) $ df.show()
engine = create_engine("sqlite:///..//Resources/hawaii.sqlite")
txns.describe()
node_0_value = (input_data * weights['node_0']).sum() $ node_1_value = (input_data * weights['node_1']).sum() $ hidden_layer_outputs = np.array([node_0_value, node_1_value]) $ output = (hidden_layer_outputs * weights['output']).sum() $ print(output) $
yeardf.head()
print(train.age.describe()) $ print(len(train[train["age"]>90])) #jc: 1.2% of age above 90, drop these? team decision $ train[train["age"]>90] $
import statsmodels.api as sm $ import pandas as pd $ from patsy import dmatrices
fig, ax = plt.subplots(figsize=(10,8)) $ fig = arma_res.plot_predict(start=8000, end=16000, ax=ax) $ legend = ax.legend(loc='upper left') $ plt.title("Time Series Forecasting using the ARIMA model")
display(data.head(10000))
from sklearn.metrics import r2_score $ predictions = automl_feat.predict(X_test) $ print("R2 score:", r2_score(y_test, predictions))
np.where([min(BID_PLANS_df.iloc[i]['scns_created']) != BID_PLANS_df.iloc[i]['scns_created'][0] for i in range(len(BID_PLANS_df))])
numeric_cols = ['price','security_deposit','cleaning_fee','extra_people'] $ filtered_df[numeric_cols].head(5)
df = pd.read_csv('output.csv', delimiter=',') $ df $ df.dtypes
tw_clean.sort_values('favorite_count').tail(1)
output = lmp.run(lammps_exe, 'nvt.in', return_style='object')
for df in reader.iterate_over_events(): $     break $ df
df2.reindex_like(df1,method='nearest')
revs = data_df.text $ stars = data_df.stars
test_data_features_tfidf.shape
df2['Agency'].value_counts()
api.get_data(a=1)
dataframe.groupby('quarter').daily_worker_count.agg(['count','min','max','sum','mean'])
for row in cursor.columns(table='TBL_FCInspevnt'): $     print(row.column_name)
pd.set_option('display.max_columns', None)
datetime.datetime(2018, 2, 18)
print('The number of unique species at Sakhalin Island:', len(sakhalin_filtered.genus.unique()))
unique_urls.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
df.loc['Total']['AveragePrice']
from sklearn.neural_network import MLPClassifier $ algorithm = MLPClassifier()
price_counts = price.value_counts() $ print(price_counts) $ price_counts.sort_index(ascending=False) $
to_be_predicted_Day3 = 36.48931367 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
null_desc = raw_data[(raw_data['desc'].isna())] $ null_desc.T
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US', 'ab_page']]) $ results = mod.fit() $ results.summary()
csvfile ='results.csv' $ resdf = pd.read_csv(csvfile) $ resdf.info()
data.loc[(100, slice(None), 'put'),:].iloc[0:5, 0:5]
fires.columns = map(str.lower, fires.columns) $ fires.columns = map(lambda x: x.replace(' ', r'_'), fires.columns) $ fires.head(2).T
posts.describe()
m_vals = np.linspace(m_true-3, m_true+3, 100) $ c_vals = np.linspace(c_true-3, c_true+3, 100)
ax = mario_game.groupby('Year_of_Release')['Global_Sales'].sum().plot() $ ax.set_ylabel('Games released')
tweets.info()
driver.get('https://submissions.scholasticahq.com/journals/mobilization/dashboard') $
d6 = d5.T $ d6
df.dtypes
print(tfidf_svd_v2.explained_variance_ratio_.sum())
texts = [re.sub('\W+', ' ', text) for text in texts] $ texts = [re.sub('[^a-z0-9\s]', '', text) for text in texts]
autos['registration_year'].describe()
predictions_count = model_count_NB.predict(X_test_count) $ predictions_tfidf = model_tfidf_NB.predict(X_test_tfidf)
pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])
knn_reg.score(x_test,y_test)
start = pd.Timestamp('2015-07-11 14:45:00') $ finish = pd.Timestamp('2015-09-24 19:15:00') $ time_index = pd.date_range(start,finish,freq='10t')
counts.index
train.groupby(by = 'Page').mean().head()
df = df[[target_column]].copy() $ base_col = 't' $ df.rename(columns={target_column: base_col}, inplace=True)
hdf5_file.close
tuna_pos_cnt = tuna_pos.count()*100 $ print "{:,} users have 0 < tuna < 90 ({:.2%} of DAU)"\ $       .format(tuna_pos_cnt, tuna_pos_cnt*1./dau)
start_sp = datetime.datetime(2013, 1, 1) $ end_sp = datetime.datetime(2018, 3, 9) $ end_of_last_year = datetime.datetime(2017, 12, 29) $ stocks_start = datetime.datetime(2013, 1, 1) $ stocks_end = datetime.datetime(2018, 3, 9)
from pyspark.mllib.evaluation import MulticlassMetrics $ predictionAndLabel = predictions.select("prediction", "label").rdd $ metrics = MulticlassMetrics(predictionAndLabel) $ print metrics.confusionMatrix()
scores, metrics = pipeline.test(ds_eval, 'Label') $ print("Performance metrics on evaluation set: ") $ display(metrics)
gbm_grid_rand = H2OGridSearch(model=model_to_grid, $                               grid_id=grid_id_name_rand, $                               hyper_params=gbm_hyperparams_rand, $                               search_criteria=search_criteria) $ gbm_grid_rand.train(x=predictor_columns, y=target, training_frame=train, validation_frame=valid, seed=1234)
ks_particpants=kick_projects.groupby(['category','launched_year','launched_quarter','goal_cat_perc']).count() $ ks_particpants=ks_particpants[['name']] $ ks_particpants.reset_index(inplace=True)
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new.head()
table_rows = driver.find_elements_by_tag_name("tbody")[4].find_elements_by_tag_name("tr") $
conn.fetch(table=dict(name='iris_db', caslib='casuser'), to=5)
X = pd.merge(X, at_menu_id_granularity, left_on='master_menu_id', right_on='menu_id', how='left') $ del X['menu_id']
reddit_comments_data.groupBy('author').agg({'score': 'mean'}).orderBy('avg(score)', ascending = False).show()
data.dropna()
train_X, test_X = pd.get_dummies(train_X), pd.get_dummies(test_X) $ fin_model = GradientBoostingRegressor(n_estimators=105, max_depth=8) $ fin_model.fit(train_X, train_Y) $ Predictions = fin_model.predict(test_X) $ Predictions
dfg = df[df.source=='google'] $ dfg["clusterlabel"] = first_cluster.labels
from pysumma.utils import utils
to_drop = ['in_reply_to_status_id', 'in_reply_to_user_id', $            'retweeted_status_id', 'retweeted_status_user_id', $            'retweeted_status_timestamp'] $ df_clean = df_clean.drop(to_drop, axis=1)
newdf = pd.DataFrame({'yearmonth':sf.index, 'deger':sf.values})
df.loc[index_name]
dd2=cfs.diff_abundance('Subject','Control','Patient', random_seed=2018)
cursor = db.cursor()
print pd.concat([s1, s4], axis=1)
knn_grid.fit(X_train,y_train)
rt.head()
countries_df = pd.read_csv('/Users/pra/Desktop/AnalyzeABTestResults 2/countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
def get_year(date): $     try: $         return re.match('([0-9]{4}).*', date).group(1) $     except: $         return np.nan
sst = f.variables['sst'] $ sst
df_y_pred = (y_age_predict.as_data_frame()) $ df_y_actual = (y_age_valid.as_data_frame())
query_date = datetime.now().strftime("%m/%d/%Y")
misk_df.head(3)
crsr = cnxn.cursor()
convers_con.time_difference.mean()
im_clean["p1"] = im_clean["p1"].str.replace("-", "_") $ im_clean["p2"] = im_clean["p2"].str.replace("-", "_") $ im_clean["p3"] = im_clean["p3"].str.replace("-", "_")
pandas_candlestick_ohlc(apple.loc['2017-12-04':'2018-01-15',:], otherseries = ["20d", "upperband", "lowerband"])
%%time $ like_plaintiff = geocoded_df.apply(lambda x: pratio(x['Plaintiff.Name'], x['Judgment.In.Favor.Of']), axis=1) $ like_defendant = geocoded_df.apply(lambda x: pratio(x['Defendant.Name'], x['Judgment.In.Favor.Of']), axis=1)
workspace_uuid = ekos.get_unique_id_for_alias(user_id, 'lena_newdata')
data.fillna(method='ffill')
p_new = df2.converted.mean() $ p_new
city_holidays_df = ph.loc[ph['city_hol']==1, ['date', 'city', 'city_hol']].copy().reset_index(drop=True) $ state_holidays_df = ph.loc[ph['state_hol']==1, ['date', 'state', 'state_hol']].copy().reset_index(drop=True) $ nat_holidays_df = ph.loc[ph['nat_hol']==1, ['date', 'nat_hol']].copy().reset_index(drop=True) $ nat_events_df = ph.loc[ph['nat_event']==1, ['date', 'nat_event']].copy().reset_index(drop=True)
tokendata = tokendata.fillna(0)
bc.info()
valence_df.head()
logit_mod = sm.Logit(df3_new['converted'], df3_new[['intercept','UK', 'US']]) $ results = logit_mod.fit() $ results.summary()
print(v_to_c.time.mean())
print('Given that an individual was in the \'treatment\' group, the probability they converted is 0.118808')
bigdf['comment_body'] = bigdf['comment_body'].apply(lambda x: x.replace('\r\r',' '))
df.to_csv('/tmp/coindesk.csv')
PredClass = ForecastModel('list_dol.sql', last_dt='2017-12-16')
dfSF.head()
df['rating'].mean()
saem_base_url = 'https://www.saem.org' $ saem_women = 'https://www.saem.org/docs/default-source/awaem/best-practices-awaem.pdf?sfvrsn=cb373dfd_0' $ saem_women_save = 'saem_women.html'
df_final_edited_10 = df_final_edited.loc[df_final['rating_denominator']==10] $
cutoff_times.groupby('msno')['churn'].sum().sort_values().tail()
app_label = train['signup_app'].unique() $ print(app_label)
%%timeit $ scipy.optimize.root(globals()[function_name], 2)
pn_and_qty_no_duplicates = dict([(key, 0) for key in pn_and_qty['PN']]) $ pns = pn_and_qty['PN'] $ qtys = pn_and_qty['QTY'] $ for pn, qty in itertools.izip(pns, qtys): $     pn_and_qty_no_duplicates[pn] += qty
unnormalized_q_i_j = np.exp(-pairwise_squared_distances_i_j) $ q_i_j = unnormalized_q_i_j / unnormalized_q_i_j.sum(axis=1) $ f, (left, right) = plt.subplots(1, 2, figsize=(8, 8)) $ plot_matrix(p_ij, ax=left, title='$P_{j|i}$', vmin=0, vmax=1) $ plot_matrix(q_i_j, ax=right, title='$Q_{j|i}$', vmin=0, vmax=1)
reddit.multireddit('bitcoin', 'programming')
AAPL.iloc[:,0:4].plot.hist(bins=25)
import datetime $ now = datetime.datetime.now() $ diff = now-datetime.timedelta(hours=7) $ diff.strftime('%Y%m%d_%H%M')
gas_df['MONTH'] = pd.to_datetime(gas_df['MONTH']+'-'+gas_df['YEAR'],format='%b-%Y') $ del gas_df['YEAR'] $ gas_df.head()
cars.dtypes
autos["vehicle_type"].unique()
chinapostnumber = len(df[df.province<99]) $ chinapostnumber
df.track_popularity.hist()
df2 = tier1_df.reset_index() $ df2 = df2.rename(columns={'Date':'ds', 'Incidents':'y'})
d1 = pd.DataFrame({'city': ['Seattle', 'Boston', 'New York'], 'population': [704352, 673184, 8537673]}) $ d2 = pd.DataFrame({'city': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]}) $ pd.merge(d1, d2)
gDate_vEnergy['Total'] = gDate_vEnergy.sum(axis=1) $ gDate_vEnergy
df.head()
df2 = df2.drop_duplicates(subset = ['user_id'])
df2.query('user_id == 773192')
twitter_archive_clean.drop('expanded_urls', axis=1, inplace=True)
print ("The user id which repeated is : {}".format(dup_id.user_id[0]))
session.commit() $ session.close()
df_treatment = df2[df2.group == 'treatment'] $ sum(df_treatment.converted==1) / df_treatment.shape[0]
sent.index.name = 'word' $ idk = tidy_format.join(sent, how="inner", on='word') $ trump['polarity'] = idk.groupby(['id']).sum()['polarity'] $ trump['polarity'] = trump['polarity'].fillna(0) $ idk $
donors.loc[donors['Donor Zip'] == 606 , 'Donor City'].value_counts()
googletrend.loc[googletrend.State=='HB,NI', "State"].head()
to_be_predicted_Day5 = 21.20401743 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df_a.join(df_b, lsuffix = "_l", rsuffix = "_r") # overlapping columns
csvData['yr_built'] = pd.to_datetime(csvData['yr_built'], format = '%Y', errors = 'raise') $ csvData['yr_renovated'] = pd.to_datetime(csvData['yr_renovated'], format = '%Y.0', errors = 'coerce') $ csvData['yr_built'] = [d.strftime('%Y') if not pd.isnull(d) else '' for d in csvData['yr_built']] $ csvData['yr_renovated'] = [d.strftime('%Y') if not pd.isnull(d) else '' for d in csvData['yr_renovated']]
users_nan = (users.isnull().sum() / users.shape[0]) *100 $ users_nan
reg_logit = LogisticRegression(random_state=1984, C=0.01) $ reg_logit.fit(x_train_advanced, y_train_advanced)
autos["odometer_km"].unique().shape
day = lambda x: x.split(',') $ emotion_big_df['date']=emotion_big_df['date'].apply(day)
SANDAG_age_df.loc[(2012, '1')]
df = pd.read_csv('./ab_data.csv') $ df.head(5)
bd.index.name
df_cs['cleaned_text'] = df_cs['text'].apply(lambda x : text_cleaners(x))
contin_map_train = contin_preproc(joined_train) $ contin_map_valid = contin_preproc(joined_valid) $ contin_map_test = contin_preproc(joined_test_df) $ contin_cols=contin_map_train.shape[1]
df_matric = pd.read_excel('data/2014_Matric_Results.xlsx')
df_prep6 = df_prep(df6) $ df_prep6_ = pd.DataFrame({'date':df_prep6.index, 'values':df_prep6.values}, index=pd.to_datetime(df_prep6.index))
from scipy import stats $ ben_fin['reverted_mode'] = ben_final.groupby(['userid']).agg({'isReverted': lambda x:stats.mode(x)[0]})
print len(data[data['Status'].notnull()])
results = session.query(func.count(Station.station)).all() $ for result in results: $     print(result)
df_tweet_clean = df_tweet.copy() $ df_image_clean = df_image.copy() $ df_clean       = df.copy()
import matplotlib.pyplot as plt $ %matplotlib inline
education_data.index=multi_index
selected=features[features.importance>0.03] $ selected.sort_values(by="importance", ascending=False)
query.all() #all() returns a list:
print(sl.two_measures.sum()) $ print(sl.second_measurement.sum())
@functions.udf $ def lowercase(text): $
X_final_test_3 = X_test_best_coef[[c for c in X_test_best_coef.columns if "foreign born" not in c]] $ X_training_3 = X_training_best_coef[[c for c in X_training_best_coef.columns if "foreign born" not in c]]
import string $ def text_process(text): $     nopunc = [char for char in text if char not in string.punctuation] $     nopunc = ''.join(nopunc) $     return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
density.tail()
sdof_resp() # arguments aren't necessary to use the defaults. 
df_tweet_json.info()
np.exp(rmse_CSCO)
log_mod = sm.Logit(df_new['converted'],df_new[['US','UK','intercept']]) $ result = log_mod.fit() $ result.summary()
test = spark.read.csv(os.path.join(datapath,"test.csv"), header=True) $ print('Found %d observations in test set.' %test.count())
df.head()
z_values, labels = vae_latent(nn_vae, mnist_train_loader) $ plt.plot(z_values[:,0], z_values[:,1], ".")
df.groupby('Single Name').sum()
all_data_grouped = all_data.groupby(['country', 'date']) $ daily_cases = all_data_grouped['totals'].sum() $ daily_cases.head(10)
df_mes = df_mes[df_mes['improvement_surcharge']==0.3] $ df_mes.shape[0]
bwd.head(10)
cohort_retention_df.fillna(0,inplace=True)
df.asfreq('W',method= 'ffill')
from fastai.plots import * $ list_paths = [f"{PATH}train-jpg/train_0.jpg", f"{PATH}train-jpg/train_4.jpg"] $ plots_from_files(list_paths,titles=['haze primary','agriculture clear habitation primary road'])
d = corpora.Dictionary.load(fps.dictionary_fp) $ c = CorpStreamer(d, fps.corp_lst_fp, inc_title='Y') $ bow_c = BOWCorpStreamer(d, fps.corp_lst_fp, inc_title='Y')
def error(line,data): $     err = np.sum((data[:, 1] - (line[0] * data[:,0] + line[1]))**2) $     return err
data.content
df.head()
tweet_ids_twitter_archive.shape[0]
hasGun = [' gun ' in tweet for tweet in allData.text] $ pd.options.display.max_colwidth = 280 $ pprint(allData[hasGun].text) $ pd.options.display.max_colwidth = 50
model_rf = pipeline_rf.fit(train_data)
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, (1-p_old)]) $ print(len(old_page_converted))
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
print("Number of Groups in ATT&CK") $ print(len(all_attack['groups'])) $ groups = all_attack['groups'] $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)
autos["seller"].value_counts()
cohort['users'].sum()
datacamp.shape
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2012-02-28', '2012-03-05'))
ac_tr_prepared.shape
len(df.user_id.unique()) #using unique function to find unique dataset
data_2017_12_14_iberia_negative.reset_index().text_2.str.split(expand=True).stack().value_counts().head()
p = pd.Period('2014-07-01 09:00', freq='H')
labels = ['Failed disks','Non-failed disks'] $ shares = [broken_count,all_count-broken_count] $ plt.pie(shares,explode=(0.2,0),labels=labels,autopct='%.0f%%',shadow=True,) $ plt.show()
df.query("(group=='treatment' & landing_page!='new_page')|(landing_page=='new_page' & group!='treatment')").count()[0]
prob_new_page = len(df2.query("landing_page == 'new_page'")) / df2.shape[0] $ print('The probability that an individual received the new page is {}'.format(prob_new_page))
from sklearn.linear_model import LogisticRegression
for c in ccc: $     notc = ccc[ccc!=c] $     for n in notc: $         ctc[c][n] = float(rtc.loc[:, rtc.columns.str.contains(c)==True][rtc.index.str.contains(n)].sum().sum())
jobs_data.drop_duplicates(subset='clean_description', keep='first', inplace=True)
df2['intercept']=1 $ df2[['other_page','ab_page']]= pd.get_dummies(df2['group'])
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ act_diff $ p_diffs = np.array(p_diffs) $ p_diffs $ (act_diff < p_diffs).mean()
f_ip_os_minute_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_os_minute_clicks"), header=True) $ print('Found %d observations.' %f_ip_os_minute_clicks.count())
users['date_account_created'] = pd.to_datetime(users['date_account_created'], infer_datetime_format = True) $ users['timestamp_first_active'] = pd.to_datetime(users['timestamp_first_active'], format = '%Y%m%d%H%M%S')
tweet_clean.drop('Unnamed: 0',axis=1,inplace=True)
df_mas['p1'] = map(lambda x: x.upper(), df_mas['p1']) $ df_mas['p2'] = map(lambda x: x.upper(), df_mas['p2']) $ df_mas['p3'] = map(lambda x: x.upper(), df_mas['p3'])
df_con=pd.concat([df_1, Xt], axis=1)
merged_portfolio_sp_latest_YTD_sp = merged_portfolio_sp_latest_YTD_sp.sort_values(by='Ticker', ascending=True) $ merged_portfolio_sp_latest_YTD_sp
xgb = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, $        gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=5, $        min_child_weight=3, missing=None, n_estimators=100, nthread=-1, $        objective='binary:logistic', reg_alpha=0, reg_lambda=1, $        scale_pos_weight=1, seed=0, silent=True, subsample=1) 
top_10_4 = scores.loc[20].argsort()[::-1][:11] $ trunc_df.loc[list(top_10_4)]
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['retweet_count']==tweet_archive_enhanced_clean['retweet_count'].max()]
beta_XOM , alpha_XOM = np.polyfit(daily_returns['SPY'],daily_returns['XOM'],1) $ print(beta_XOM , alpha_XOM)
with pd.HDFStore(os.path.join(data_dir, 'nhanes.h5')) as hdf: $     print(hdf.keys())
%%sql result_set << $ SELECT query_id, state, query $ FROM runtime.queries $ LIMIT 2
np.__version__
committees_NNN.info()
df.head()
for col in grouped_df.get_group('2011-07-12').columns: $     print col
eth = pd.read_csv('data/eth-price.csv') $ eth.head()
autos["date_crawled"].value_counts(normalize=True, dropna=False) $
y1=df1['label'] $ x1=range(len(df1['label'])) $ plt.bar(x1,y1) $ plt.plot(x1,y1) $ plt.grid()
from sklearn.preprocessing import OneHotEncoder $ encoder = OneHotEncoder() # Create encoder object $ categDF_encoded = encoder.fit_transform(categDF) # Can't convert this to dense array: too large!
y_predict = clf.predict(X_test) $ print(accuracy_score(y_test, y_predict)*100)
knn.fit(data[['expenses', 'floor', 'lat', 'lon', 'property_type', \ $               'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']], \ $         data[['price_aprox_usd']])
most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:3] $ most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3] $ grader.submit_tag('WordsTagsCount', '%s\n%s' % (','.join(tag for tag, _ in most_common_tags), $                                                 ','.join(word for word, _ in most_common_words)))
lr=118813 $ loan_requests[loan_requests.id_loan_request==lr]
properati.drop(inplace=True,labels=['extra','image_thumbnail','state_name','description','title','properati_url'],axis=1)
autos[['date_crawled','ad_created','last_seen']].head(5)
cast_data.columns
counts_df.describe()
movies['year'] = pd.to_numeric(movies['year'], errors='coerce')
pd.to_datetime(df)
temp = data.merge(right = data[['user_id','device_id']].groupby('device_id').count().reset_index().rename(columns={'user_id':'usage_count'}),how='left',on='device_id') $ data['usage_count'] = temp['usage_count']
X = bow_transformer.transform(X)
train_size = int(train_.shape[0]*0.8) $ sentiment_train_tweets = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values[:train_size]] $ sentiment_train_tweets_full = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values] $ sentiment_validation_tweets = [(tweet, sentiment) for tweet, sentiment in train_[['tweetText', 'polarity_value']].values[train_size:]] $ sentiment_test_tweets  = [(tweet, sentiment) for tweet, sentiment in test_[['tweetText', 'polarity_value']].values]
df['speaker_job'].value_counts()
sc = SparkContext(appName="PythonSparkReading")  $ sc.setLogLevel("WARN") 
daily_averages.groupby('Month').mean().plot.bar(ylim=(0, 4000))
total_rows_in_treatment = (df2['group'] == 'treatment').sum() $ rows_converted = len((df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)])) $ rows_converted/total_rows_in_treatment
founddocs = fs.find({"$text": { "$search": "Cundall" }}) $ print founddocs.count()
Nold = df2.query('landing_page == "old_page"').user_id.count() $ Nold
0 * np.nan
cols = list(df.columns) $ print(len(cols)) $ print(cols)
weather_yvr_dt['Datetime'] = pd.to_datetime(weather_yvr_dt['Datetime'])
crimes_all.info(null_counts=True)
temp = pd.to_datetime(df_all['created'], unit='s') $ temp = pd.DatetimeIndex(temp) $ print(temp[0:2])
uber_15["day_of_month"].value_counts().head()
data.name.duplicated(keep=False).sum()
pd.concat
properati.loc[:,'state_name']
df2.head()
os_br_colmns = list(dbdata) $ os_br_keep = ['id_os','Android', 'iOS', 'Windows', 'Chrome', 'Safari', 'IEMobile'] $ os_br_colmns = [e for e in os_br_colmns if e not in os_br_keep] $ dbdata.drop(dbdata[os_br_colmns],axis=1,inplace=True) $ dbdata
def set_trainability(model, trainable=False): $     model.trainable = trainable $     for layer in model.layers: $         layer.trainable = trainable
obj = pd.Series([4, 7, 3, -2])
repos.forked_from = pd.to_numeric(repos.forked_from)
m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1, $           layers=[em_sz*3, 50, c], drops=[dps[4], 0.1], $           dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])
tbl[tbl.msno.duplicated()]
np.nan == np.nan
df[(df.salary >= 30000) & (df.year == 2017)]
autos['brand_model'].value_counts().head()
system = am.load('poscar', 'Al-fcc.poscar') $ print(system)
move_1_herald = sale_lost(breakfastlunchdinner.iloc[1, 1], 10) $ print('Adjusted total for route: ' + str(move_34p34h34h - move_1_herald))
from pandas.tseries.offsets import BDay $ pd.date_range('2015-07-01', periods=5, freq=BDay())
gbc.get_params()['n_estimators']
import pickle $ pickle.dump(knn_reg, open('model.sav', 'wb'))
print(y.shape) $ print(X.shape)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
scaled = scaled.join(vol, how='outer')
s.iloc[:3]
crime = crime[crime.Sex != 'U']
vow['Day'] = vow.index.day $ vow['month'] = vow.index.month $ vow['year'] = vow.index.year
articles_by_pub.head()
%%time $ text_tw = txt_tweets $ with open('tweets.txt', 'w+') as f: $     for t in text_tw: $         f.write(t + '\n')
yr, mo, dd = 2012, 12, 21 $ dt.date(yr, mo, dd) $ dt.date(2012, 12, 21)
players_df.head()
Date=pd.to_datetime(crimes['Date']) $ crimes.index=Date $ crimes.sort_index(inplace=True)
itemTable["Project"] = itemTable["Project_Id"].map(project_link)
twitter_archive_master = pd.merge(df_clean4, df_image_tweet2, on='tweet_id', how='left') $ twitter_archive_master.head()
df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'}, $                     'B': {0: 1, 1: 3, 2: 5}, $                     'C': {0: 2, 1: 4, 2: 6}}) $ df
data.sort_index(inplace=True)
dic1 = {"col1":[1,2,3], "col2":["practical","data","science"],"col3":["a","b","c"]} $ ex3 = pd.DataFrame(dic1) $ ex3
full_act_data.plot(figsize=(20,8));
unique_users = len(df['user_id'].unique()) $ print('Number of unique users in the dataset:{}'.format(unique_users))
summed.interpolate()  # notice all the details in the interpolation of the three columns
from statsmodels.api import Logit $ model = Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = model.fit()
df_new1.shape[0]+df_new2.shape[0] $ print("The number of times the new_page and treatment don't line up are: {}".format(df_new1.shape[0]+df_new2.shape[0]))
data = open("test_data//legislators.csv").read() $ print(data[0])
contractor_merge = pd.merge(contractor_clean, state_lookup, $                             on=['state_id'], how='left')
vip_reason.to_csv('../data/vip_reason.csv')
ax = plt.subplot(111) $ ax.bar(date_frequency.index, date_frequency.data) $ ax.xaxis_date() $ plt.show()
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([135, 47], [1781, 1443])
print("Column names before change") $ df.head(2)
post_sentiment_df_saved = non_blocking_df_save_or_load( $     post_sentiment_df, $     "{0}/post_sentiment_df_1".format(fs_prefix))
crimes.describe()
df2.converted[df2.group == 'control'].mean()
df['lemma'] = df['lemma'].apply(lambda x: ' '.join(x))
df_data.EXAME.value_counts()
sub1.drop('contest_id', axis = 1, inplace = True)
mod1=sm.Logit(ab_new['converted'],ab_new[['intercept','CA_ab_page','US_ab_page']]) # checking if converted depends on these columns - intercept, CA, US $ result1 = mod1.fit() #fitting the new model $ result1.summary()
flight6.count()
df2 = df.copy() $ df2 = df2[(df2['group']=='treatment') & (df2['landing_page']=='new_page') | $           ((df2['group']=='control') & (df2['landing_page']=='old_page'))]
[x_train.shape, y_train.shape, x_test.shape, y_test.shape]
fig, axs = plt.subplots() $ axs.plot(points_df.index, points_df.Lat, '.') $ fig.autofmt_xdate()
df7 = df[df['hired'] ==1] $ print("Please see the table below for answers") $ display(df7.groupby('category')['position','hourly_rate','num_completed_tasks'].mean().reset_index() \ $     .rename(columns={'category': 'category', 'position': 'average_position','hourly_rate':'average_hourly_rate', \ $                  'num_completed_tasks':'average_num_completed_tasks'}))
sub1['final_solved'] = sub1.groupby(['hacker_id', 'challenge_id'])['solved'].transform('max')
i = nums.index $ print(type(i), i)
s = pd.Series(rng)
autos['price'].value_counts().sort_index(ascending=False).head(20) $
print("The highest opening prices",df.Open.max()) $ print("The highest opening prices",df.Open.min())
transactions[(transactions.transaction_date < datetime.strptime('2017-04-01', '%Y-%m-%d'))]
train.target.sum() == data.shape[0]
df2['intercept'] = 1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group'])
hist=pd.merge(users, transactions.groupby('UserID').first().reset_index(), how='left', on='UserID') $ hist.head()
ayush_sean = df_ayush.join(df_sean, how = 'outer').fillna(0) $ ayush_sean
df_train['dow_date'] = df_train['date'].dt.day_name() $ df_test['dow_date'] = df_train['date'].dt.day_name() $
for Quarter, Data in StockData.groupby('Date-Quarter'): $     print("Q{} mean prices".format(Quarter)) $     print("There are {} rows in this group of data".format(len(Data))) $     print(Data[StockNames].mean()) $     print()
pd.DataFrame(lm.coef_,X.columns,columns=['Coefficient'])
Y = 'label' $ dogscats_h2o[Y] = dogscats_h2o[Y].asfactor() $
station_most_active = session.query(Measurement.station, Station.name).group_by(Measurement.station).\ $                         order_by(func.count(Measurement.tobs).desc()).first() $ station_most_active
print(spike_tweets.iloc[5000, 2])
df.injured.value_counts()
df.replace({0: 3, 1: 1}, 99)
[k for val in train_x[0] for k,v in words.items() if v==val]
kick_data = k_var_state[['name','category_name','blurb','blurb_count','goal_USD','backers_count','launched_at','state_changed_at','days_to_change','state']] $ kick_data.head()
s = select([employee]) $ result = conn.execute(s) $ row = result.fetchall() $ print(row)
department_df_sub.sum(axis = 0) # non-apply equivalent 
flight_cancels.head()
multi_col_lvl_df.applymap(lambda x: np.nan if np.isnan(x) else str(round(x/1000, 2)) + "k").head(10)
autos.columns
x0 = np.linspace(0, 5.5, 200) $ pred_1 = 5*x0 - 20 $ pred_2 = x0 - 1.8 $ pred_3 = 0.1 * x0 + 0.5 $
R16 = INQ2016.Create_Date.dt.month.value_counts().sort_index() $ R17 = INQ2017.Create_Date.dt.month.value_counts().sort_index() $ R18 = INQ2018.Create_Date.dt.month.value_counts().sort_index()
data['temp'] = data['year'].map(str)+'-' + data['month'].map(str) +'-'+ data['day'].map(str)     $ data.head(5)
twentyth_movie_mscore = movie_containers[20].find('div', class_ = 'ratings-metascore') $ print(sixth_movie_mscore)
!python OpenSeq2Seq/run.py --config_file=OpenSeq2Seq/example_configs/nmt.json --logdir=./nmt --mode=infer --inference_out=pred.txt
air_reserve.head()
import numpy as np $ disaster_tables = pd.read_html("https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll", header = 0) $ explotions = disaster_tables[4] # reading from html
data = pd.read_csv("BreastCancer.csv") $ data.head()
import pickle $ output = open('states.pkl', 'wb') $ pickle.dump(chambers, output) $ output.close()
bounds.max_latitude
tree_features_df['p_hash'].describe()
airbnb_df.groupby('property_type').agg({'accommodates':[np.mean, np.median], 'bedrooms':max, 'bathrooms':np.median})
pred_probas_over = log_reg_over.predict_proba(X_test)
df1.fillna(value=5.)
old_page_converted = np.random.choice(2,n_old,p=[1-p_old,p_old]) $ print(old_page_converted)
h.ix['ibm us equity'].plot()
active_station_data = session.query(Measurement.station, func.count(Measurement.id)).\ $     filter(Measurement.station == Station.station).\ $     group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all() $ active_station_data
df2.drop(['control'], axis=1, inplace=True) $
failures.head()
stn_cnt_df=pd.DataFrame(stations_des,columns=['Station','Counts']) $ stn_cnt_df.head()
for col in b_list.columns: $     print(f'{col}...{len(b_list[col].unique())}')
rf_pred = prediction_model.predict(x_test_scaled)
walmart.end_time
a = a.sort_values('a')
df = pd.read_csv("/Users/srdas/GoogleDrive/Papers/DeepLearning/DLinFinance/SP_Data_shared/mer_df_percentile.csv") $ df = df.drop(df.columns[[0]],axis=1) $ print(shape(df)) $ df["Sign"] = maximum(0,sign(df["SPX"])) $
df.head()
ridgemodel = linear_model.Ridge(alpha =1) $ fit2= ridgemodel.fit(X,y)
print('Best features:', gs.best_estimator_.steps[0][1].best_idx_)
import matplotlib.pyplot as plt $ plt.plot(df['price'],'.') $ plt.show()
autos['price'].unique().shape
results_1dRichards, output_R = S_1dRichards.execute(run_suffix="1dRichards_hs", run_option = 'local')
lsi_tfidf.print_topics(10)
HERBARIUM_SEARCH_URL = 'http://botsad.ru/hitem/json/'
scores.shape
rfc.fit(Bow_X_train, Bow_y_train) $ print('Training set score:', rfc.score(Bow_X_train, Bow_y_train)) $ print('\nTest set score:', rfc.score(Bow_X_test, Bow_y_test)) $ print('\nCross Val score:',cross_val_score(rfc, Bow_X_test, Bow_y_test, cv=5))
df.dtypes
p_newpage = df2.query('landing_page == "new_page"').user_id.nunique() / df2.user_id.nunique() $ p_newpage
autos["registration_year"].value_counts(normalize=True).sort_index(ascending=True).head(10)
df_comb[['CA', 'UK', 'US']] = pd.get_dummies(df_comb['country']) $ df_comb.head()
df_gene['dates'] = df_gene['created_atc'].dt.date $ df_gene.head()
airbnb = scores.loc[94] $ airbnb_10 = airbnb.argsort()[::-1][:11]
temp_cat.describe()
def execute_statement(query, database): $     conn = lite.connect(database) $     result = pd.read_sql_query(query, conn) $     conn.close() $     return result
df2[df2['converted'] == 1].count()[1]/df2.shape[0]
pold_null=(df2['converted']==1).mean() $ pold_null
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2])
catalog_df = week1_df.append(week2_df) $ catalog_df
for idx, row in df_trips.iterrows(): $     pilot_created = df_pilots.loc[row["pilot"], "created"] $     passenger_created = df_passengers.loc[row["passenger"], "created"] $     min_trip = max(pilot_created, passenger_created) $     df_trips.loc[idx, "trip_requested"] = np.random.randint(min_trip, max_trip)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
t0 = time() $ model = MatrixFactorizationModel.load(sc,"lastfm_model.spark") $ t1 = time() $ print("finish loading model in %f secs" % (t1 - t0))
autos_real = autos.loc[(autos['registration_year'] > 1900) & (autos['registration_year'] < 2016),:]
text_classifier.get_step_params_by_name("text1_char_ngrams")
y = df['target'] $ X = df['subreddit'] $ cvec = CountVectorizer(stop_words = 'english') $ X  = pd.DataFrame(cvec.fit_transform(X).todense(), $              columns=cvec.get_feature_names())
merged = df2.set_index('user_id').join(countries.set_index('user_id')) $ merged.head()
train['discussion_hn'] = ((train.url.isnull()) & (train.title.str.contains(' HN: '))).astype(int) $ train.groupby('discussion_hn').popular.mean()
merged.head()
pd.to_datetime(next_period_date).date()
logit_control = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result_control = logit_control.fit()
df2_daily.set_index('DATE').resample('1W',how='sum')
sorted(entity_relations.items(), key=lambda x: x[1], reverse=True)
df.info()
df_archive["retweeted_status_timestamp"].unique()[0:10]
all_text[all_text.index == "Roberto Cavalli"].sum().sort_values(ascending=False).head(25)
import statsmodels.api as sm $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ results = log_mod.fit()
lr.fit(X_train_dtm, y_train)
probs = model2.predict_proba(x_test) $ print probs
new_page_converted =  np.random.binomial(1, p = p_new,size = n_new) $ new_page_converted
treatment_conv = conv_ind.query('group == "treatment"').shape[0] $ treatment_group = df2.query('group == "treatment"').shape[0] $ print('Probability of TREATMENT page converted individual: {:.4f}'.format(treatment_conv/treatment_group))
%matplotlib notebook $ df_var.toPandas().plot()
df_image_clean.info()
ab_df.isnull().sum()
auto_new.Hand_Drive.unique()
elms_all_0604 = pd.read_excel(cwd+'\\ELMS-DE backup\\elms_all_0604.xlsx') $ elms_all_0604['ORIG_DATE'] = [datetime.date(int(str(x)[0:4]),int(str(x)[5:7]),int(str(x)[8:10])) $                              for x in elms_all_0604.ORIG_DATE.values]
df2[['control','treatment']]= pd.get_dummies(df2['group']) $ df2 = df2.drop('control',axis = 1) $ df2.head()
fig, ax = plt.subplots() $ ax.scatter(df.artist_popularity, df.artist_followers) $ ax.set_title('Artist Followers vs. Artist Popularity') $ ax.set_xlabel('Artist Popularity') $ ax.set_ylabel('Artist Folowers')
my_df_small.head()
doc_duration = doc_duration.resample('W-MON').sum() $ RN_PA_duration = RN_PA_duration.resample('W-MON').sum() $ therapist_duration = therapist_duration.resample('W-MON').sum()
store_items.fillna(0)
args = mfclient.XmlStringWriter("args") $ args.add("where", "namespace=/projects/proj-hoffmann_data-1128.4.49/libraries") $ args.add("action", "get-path") $ args.add("size", "infinity") $ libraries_query = con.execute("asset.query", args.doc_text())
mean2=df2['converted'].mean() $ mean2
from IPython.display import HTML, display
(df['converted']).mean()   #conversion rate
df2['landing_page'][df2['landing_page'] == 'new_page'].count()/290584
df_final_.state.value_counts() $
dump.head()
country_dummies = pd.get_dummies(df2['country'])
shirt_1.discount(.3)
filepath = os.path.join('input', 'input_plant-list_CH_conventional.csv') $ data_nuclear_CH = pd.read_csv( $     filepath, encoding='utf-8', header=0, index_col=None)
url1 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-05-23&end_date=2018-05-23&api_key='+API_KEY $ r1 = requests.get(url1)
k150_bets_under = [x[1] > .6 for x in pred_probas_under_k150]
%%timeit -n1 -r2 $ tags = {} $ for i, el in tqdm(rentals['description'].iteritems()): $     tags[i] = get_rental_concession_2(el) $ T2 = pd.Series(tags)
df_clean['tweet_id'] = df_clean['tweet_id'].astype('str') $ image_clean['tweet_id'] = image_clean['tweet_id'].astype('str') $ tweet_clean['tweet_id'] = tweet_clean['tweet_id'].astype('str')
tree_features_df['filename'].isin(manager.image_df['filename']).describe() $
daily_returns = (port_cum_ret - 1).resample('D').last() $ daily_returns.index = pd.to_datetime(daily_returns.index.astype(str))
pd.Series(STATE_COLORS.values()).value_counts()
!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl $ !pip3 install torchvision
df_master = pd.read_csv('twitter_archive_master.csv')
links_df.to_csv('Scraped Links',index=False)
autos['registration_year'].sort_values(ascending=False).head(15)
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) $ rootLogger = logging.getLogger() $ rootLogger.setLevel(logging.INFO)
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).filter(Measurement.station == "USC00519281").all()
probability_of_winning = ( $     data_for_model[data_for_model['final_status']==1].count())/data_for_model['final_status'].count() $ probability_of_winning
np.exp(-0.0408), np.exp(0.0099)
df_copy = df_groups.join(df_events.groupby(['group_id']).created.count(), how ='inner',on= 'group_id', lsuffix= '_left', rsuffix = '_count') $ df_copy = df_copy.sort_values(by = 'created_count', ascending = False) $ df_copy.head()
trading.df.tail()
pconversion = df2['converted'].mean() $ pconversion
species_count.to_excel('species_output.xlsx') $ print("Done Writing!")
unitech_df.dtypes
df_payout = jcp.df_payout.copy() #exotic bets payouts in easier form $ df_result = jcp.df.copy() #race results dataframe
plt.rcParams['axes.unicode_minus'] = False $ dta_55.plot(figsize=(15,5)) $ plt.show()
groupby_imputation = taxi_hourly_df.groupby([taxi_hourly_df.index.month, taxi_hourly_df.index.dayofweek, taxi_hourly_df.index.hour]).mean()
df_from_json = pd.read_json("../../data/stocks.json") $ df_from_json.head(5)
ia = imdb.IMDb() $ response_imdb = ia.search_movie('Lost in translation') $ response_imdb[0] $ for s in response_imdb: $     print (s['title'], s.movieID)
content_input_count_hist = cached.map(lambda x: x[1]['content_input']).histogram(range(20)) $ draw_histogram("Content Input Hangs Distribution", content_input_count_hist)
df2["user_id"].value_counts()
prop_users_convert = (df['converted'].mean())*100 $ print("There are {}% converted users ".format(prop_users_convert ))
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = df2.shape[0] - df2.query("landing_page == 'new_page'").shape[0] $ n_new = df2.query("landing_page == 'new_page'").shape[0] $
import matplotlib.pyplot as plt $ %matplotlib inline $ import seaborn as sns; sns.set() $ bikedataframe['predicted'] = y_predit $ bikedataframe[['count', 'predicted']].plot(figsize=(24,16),alpha=0.5)
Obama_raw.head(3)
Raw_Forecast.to_csv("Master File_AU",encoding = "utf-8",index = False)
df.sort_values (by='J')
! ls -l ~/.dw/cache/data-society/the-simpsons-by-the-data/latest/data
vacancies['weekday'] = vacancies['created'].apply(lambda x: x.weekday() + 1) $ vacancies['hour'] = vacancies['created'].apply(lambda x: x.hour)
df_protest.columns
from sklearn.model_selection import GridSearchCV $ param_grid = {'learning_rate': [0.05,0.1],'num_leaves': [40,60,80]}
au.show_frequent_items(mentions_df,user_names,"trg",k=10)
data.groupby(pandas.TimeGrouper(key="Visit Date", freq="D") $             ).count()["Visit Type"]
s_n_s_epb_two.rename(columns = {0:"Count"},inplace=True)
probs_test = F.softmax(V(torch.Tensor(log_preds_test))); $
for _word, _count in sorted(vectorizer.vocabulary_.items(),key=itemgetter(1),reverse=True)[:20]: $     print(_word,end=' ') $     print(_count)
fraud_df = pd.read_csv('Fraud_Data.csv') $ ip_df = pd.read_csv('IpAddress_to_Country.csv')
logit4 = sm.Logit(df3['converted'], df3[['intercept','new_page','UK_new_page','US_new_page','UK','US']]) $ result4 = logit4.fit() $ result4.summary()
final.head()
df_twitter_copy.at[2335, 'rating_numerator'] = '9' $ df_twitter_copy.at[2335, 'rating_denominator'] = '10'
interact_lm = sm.Logit(df3.converted, df3[['intercept','ab_page','US','US_con','UK','UK_con']]) $ interact_result = interact_lm.fit() $ interact_result.summary()
QUIDS_wide.drop(labels=["qstot_12","qstot_14"], axis = 1, inplace=True)
AAPL.info()
df = df[-df.Address.str.contains('Intersection')] $ df = df[df.Address.str.count(',') == 3]
age.iloc[[1, 3, 0]]
cum_sum_percentage_payments = list(np.cumsum(percent_of_total_list)) $ print('cum_sum_percentage_payments is of type:',type(cum_sum_percentage_payments)) $ cum_sum_percentage_payments[:4] $ [i for i,v in enumerate(cum_sum_percentage_payments) if v > 50][:10]
rankImportance= importanceDF.sort_values(by='importance', ascending= False)
import numpy as np $ cities.reindex(np.random.permutation(cities.index))
datatest.loc[datatest.place_name == "Villa Celina",'lat'] = -34.706311 $ datatest.loc[datatest.place_name == "Villa Celina",'lon'] = -58.483025
retailDf.registerTempTable("retailPurchases") $ sqlContext.sql("SELECT * FROM retailPurchases limit 2").toPandas()
dcAutos['price'].count()
def num_missing(x): $   return sum(x.isnull()) $ print (filtered_df.apply(num_missing, axis=0))
c = pd.read_csv('countries.csv') $ df3 = df2.merge(c, on ='user_id', how='left') $ df3.head()
description = pd.read_csv('description.txt', sep = '\n', header=None)
missing_info = list(df_users_first_transaction.columns[df_users_first_transaction.isnull().any()]) $ missing_info
sns.barplot(x="contb_receipt_dt", y="contb_receipt_amt", data=group_by_month)
wuxia_request = requests.get('http://www.wuxiaworld.com/btth-index/btth-chapter-12/') $ wuxia_soup = bs4.BeautifulSoup(wuxia_request.text, 'html.parser') $ wuxia_ptags = wuxia_soup.find(itemprop="articleBody").findAll('p') $ for w_ptag in wuxia_ptags: $     print(w_ptag.text)
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['CA', 'US', 'intercept', 'ab_page']]) $ results = log_mod.fit() $ results.summary()
date_reach_goal = pd.DatetimeIndex(df['date'][[0]]) + pd.DateOffset(days=round(x)) $ date_reach_goal
import datetime $ print("Date of last code execution: ", datetime.datetime.now())
df2.query('group == "treatment" & converted == 1').shape[0] / df2.query('group == "treatment"').shape[0]
movies = (spark.read.format("csv") $ .options(header = True, inferSchema = True) $ .load(home_dir + "movies.csv") $ .cache()) # Keep the dataframe in memory for faster processing 
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2015 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2015 Percentiles.txt"
df.head(5)
df2 = df.drop(control_wrong.index) $ df2.drop(treatment_wrong.index, inplace = True)
def output(x_tensor, num_outputs): $     return tf.layers.dense(x_tensor, num_outputs, activation=None) $ tests.test_output(output)
to_be_predicted_Day2 = 48.33846849 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
trips_data['duration'].plot() # pandas will interact with matplotlib  - default is linechart
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Type"])
columns = ['BAD'] $ columns.extend(list(cassession.CASTable('SelectedEffects').fetch().Fetch['Variable'])) $ columns = ', '.join(['prep.{}'.format(x) for x in columns]) $ cassession.fedsql.execdirect(query)
df['2015-06-03']['battle_deaths'].mean()
dfWeek = dfWeek.groupby(['Date', 'Project Name',   $                          'Estimated Start', 'Estimated End', 'Estimated Duration (Days)',  $                           'Sales Stage', 'Likelihood of Win', 'Likelihood Percent', 'Contract Value', $                          'JTBD', 'Big Bet', 'Client Type: Direct Impact', 'Client Type: Funder', 'New/Repeat Client', $                          'Contract Band Size'])[['Weighted Value', 'Contract Value (Daily)']].sum().reset_index()
df = df.set_index("Date")
plt.imshow(corr_matrix, cmap="viridis") $ plt.show()
(pd.DataFrame(click_condition_meta.groupby('geo_country').count())).sort_values('action', ascending = False).head(4)
bacteria2 = pd.Series(bacteria_dict, $                       index=['Cyanobacteria','Firmicutes','Proteobacteria','Actinobacteria']) $ bacteria2
all_tables_df.OBJECT_TYPE.count()
autos["brand"].unique()
sel1=[Measurement.date, $      func.sum(Measurement.prcp)] $ all_prcp=session.query(*sel1).group_by(Measurement.date).all() $
autos = autos.fillna({"num_doors": "four"})
I decided to merge the snow data into one snowtotal field that merges snowfall and snow depth measurements.
r['Tweet Counts'].plot()
dt.date() $ dt.time()
np.exp(results1.params)
treino[treino.sentiment == 1].count() #Tweets com sentimento positivo
novVisits = df_visits[df_visits.datestamp == "2015-07-28"] $ print 'number of visits on 2015-07-28: ', novVisits.user_visits.values.sum(), '\n' $ df_totalVisits_day = df_visits.groupby(['datestamp'], as_index=False).sum() $ df_totalVisits_day.sort('user_visits', ascending=[0]).head()
pd.date_range('2017-12-30', '2017-12-31')
reddit_comments_data.groupBy('author').agg({'score': 'mean'}).orderBy('avg(score)').show()
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
logreg = LogisticRegression() $ logreg.fit(X_train, y_train)
Tue_index  = pd.DatetimeIndex(pivoted.T[labels==1].index).strftime('%a')=='Tue'
con = sqlite3.connect('db.sqlite') $ df=pd.read_sql_query("SELECT * from sqlite_master", con) $ con.close() $ df
ins['year'] = ins['new_date'].dt.year $ ins.head(5)
tweets['weekNumber'] = ((tweets['daysFromStart']/7) + 1).apply(np.floor) $ tweets['dayNumber'] = tweets['daysFromStart'] + 1 - (tweets['weekNumber'] -1)*7 $ tweets['hourNumber'] = tweets['hoursFromStart'] - (tweets['weekNumber'] -1)*168
pd.DataFrame(eth_df)
df_users_6.loc[df_users_6['unit_flag']=='No unit taken so far','cohort']='Browser' $ df_users_6.loc[df_users_6['unit_flag']=='Users started units but did not complete any','cohort']='DriverThru User' $ df_users_6.loc[df_users_6['unit_flag']=='Completed atleast one but less than 10 units','cohort']='Occasional User'
df_new.to_csv("final_data.csv")
access_logs_df.printSchema()
california_averages = california.groupby('FIRE_YEAR')['FIRE_SIZE'].mean() $ california_averages.plot(y= 'FIRE_SIZE', x= 'FIRE_YEAR')
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 1), 'type'] = 'defenders'
mask = [(row in pn_qty[pn]['1cstoreinfo']) for row in table_1c.iterrows()]
df.head()
docs_by_topic = tfidfnmf_topics.groupby('PrimaryTopic')
serious_count_final = 0 $ for row in data: $     if re.search("^[\[\(][Ss]erious[\]\)]|[\[\(][Ss]erious[\]\)]$", row[0]): $         serious_count_final = serious_count_final + 1 $ print("[3] serious_count_final: " + str(serious_count_final))
X_train = train.iloc[:,features_index] $ y_train = train['Label']
model.clusterCenters()
metadata2 = pd.read_csv('Data/SelectDemographicMetadata.csv') $ metadata2
Which_Years_for_each_DRG.loc[345] $
engine.execute('SELECT * FROM measurement LIMIT 5').fetchall()
import statsmodels.formula.api as sm $ sm_lm = sm.ols(data = dat, formula = "y ~ x").fit() $ print(sm_lm.summary()) $ print("----") $ print(sm_lm.params)
X_train, X_test, y_train, y_test = train_test_split(data.drop('Class',axis=1), data['Class'], test_size=0.33) $ print(X_train.shape) $ print(y_train.shape) $ print(X_test.shape) $ print(y_test.shape)
df.info()
country = ['Argentina', 'Kenya', 'Nigeria', 'South Africa', 'France', 'Germany', $            'Scotland', 'Spain', 'Ireland', 'Italy', 'New Zealand', 'Pakistan'] $ for i in country: $     notus.loc[notus['country'] == i, 'country'] = i $     notus.loc[notus['cityOrState'] == i, 'country'] = i
df2.index $
convert = df['converted'].mean() $ print('The proportion of users converted is {}'.format(round(convert, 4)))
station_df.head(10)
logodds.drop_duplicates().sort_values(by=['count']).plot(kind='barh')
df.drop(labels = ['text_no_urls', 'text_no_urls_names', 'text_no_urls_names_nums'], axis = 1, inplace = True)
ab_df.info() $
raw_df.shape
survey.head()
plt.figure(figsize=(8, 5)) $ train_df.groupby('domain').favs.median().plot.bar() $ plt.title('Median of the #favs by domain') $ plt.xticks(rotation='horizontal');
data = pd.Series([0.25, 0.5, 0.75, 1.0], $                  index=['a', 'b', 'c', 'd']) $ data
pf_rdd = sc.parallelize([('P1', 'RF1', 1.), ('P1', 'RF2', 2.), ('P2', 'RF1', 0.2), ('P2', 'RF2', -0.8)]) $ dfpf = sql.createDataFrame(pf_rdd, ['portfolio', 'rf', 'qty'])
svm_tunned.fit(X_train, y_train)
df.head()
plt.plot(ages, weights,'.', alpha = 0.1) $ plt.xlabel("mother's age") $ plt.ylabel("birth weight") $
categoryList = df['Memo'].apply(returnCategory).unique()
df.to_csv('data/dibeBot_ff.csv',index=False)
prop_conv = df2[df2['converted'] == 1].shape[0] / df2.shape[0] $ prop_conv
p_old = df2['converted'].mean() $ print "Convert rate of an individual received the old page:",p_old
calls_df.groupby(["dial_type"])["length_in_sec"].mean()
for i, correct in enumerate(correct[:9]): $     plt.subplot(3,3,i+1) $     plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none') $     plt.title("Predicted {}, Class {}".format(predicted_classes[correct], y_true[correct])) $     plt.tight_layout()
merged_df.reset_index('state', inplace=True)
pd.concat([A,B], axis = 1)
cashflows_act_arrears_investor[(cashflows_act_arrears_investor.id_loan==675) & (cashflows_act_arrears_investor.fk_user_investor==38)].to_clipboard()
timelog = timelog.drop(['Email', 'User', 'Amount ()', 'Client', 'Billable'], axis=1)
rsi = rs.index[120] $ rsi
country_with_least_expectancy = le_data.idxmin(axis=0) $ country_with_least_expectancy
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %I:%M:%S %p')
most_recent_investment.head() $ most_recent_investment.columns = ['uuid','most_recent_investment']
titanic.loc[index_strong_outliers, :].head()
1/np.exp(-0.0175), 1/np.exp(-0.0469), np.exp(-0.0057), np.exp(0.0314)
for k,d in dataset_dict.items()[:10]: $     print 'id: ',k $     print 'name: ',d $     print '---'
from scipy.stats import norm $ norm.ppf(1-(0.05/2))
result = customer_visitors.groupby('Yearcol').mean().astype(int) $ result $
import matplotlib.pyplot as plt $ df[['Date','GasPrice']].set_index('Date').plot()
twitter_data = {} $ for target_user in target_user_list: $     public_tweets = api.user_timeline(target_user, count=100, result_type="recent") $     twitter_data[target_user] = public_tweets
users = list(db.osm.aggregate([{"$group":{"_id": "$created.user", "count":{"$sum": 1}}}, {"$sort": {"count": -1}}])) $ print 'Total users = ',len(users) $ users[:80] $
df_twitter_copy = df_twitter_copy.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis = 1)
train.head(3)
df_estimates_false.groupby('metric').count() $
temperature.head()
iowa.describe()
df_mas['rating']=df_mas.rating_numerator +'/' +df_mas.rating_denominator
df_master.to_csv('twitter_archive_master.csv', encoding = 'utf-8',index= False)
df.to_csv('emails.csv')
data2['season'] = data2.sales
np.ones([2, 3])  #There's also np.zeros, and np.empty (which results in an uninitialized array).
l = pd.read_sql_query(QUERY, conn) $ l $
df.drop(['adwordsClickInfo'],axis=1, inplace=True)
df2=df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == True].copy() $ df2.shape
split_pct=0.75 $ train_test_cut_period = int(len(data)*split_pct) #split point $ train_set = data[:train_test_cut_period].copy() $ test_set = data[train_test_cut_period:].copy()
df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country']) $ df3.head() $
df['effective_date']=pd.to_datetime(df['effective_date'], errors = 'coerce') $ df.head()
FAFRPOSRequest = requests.get(FAFRPOS_pdf) $ print(FAFRPOSRequest.text[:10000])
sns.heatmap(temp_us)
df4.dtypes
train_Features, test_Features, train_species, test_species = train_test_split(Features, species, train_size=0.5, random_state=0)
merged.isnull().sum()
dsd_cpi = 'http://stat.data.abs.gov.au/restsdmx/sdmx.ashx/GetDataStructure/CPI' $ xml_cpi = requests.get(dsd_cpi).content $ tree_cpi = etree.fromstring(xml_cpi)
from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.neighbors import KNeighborsClassifier $ tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
temperature_2016_df = pd.DataFrame(Temperature_year)#.set_index('date') $ temperature_2016_df.head()
walk.resample("1Min").first()
print("Number of Relationships in PRE-ATT&CK") $ relationships = lift.get_all_pre_relationships() $ print(len(relationships)) $ df = json_normalize(relationships) $ df.reindex(['id','relationship', 'relationship_description', 'source_object', 'target_object'], axis=1)[0:5]
df.info() $ total_rows = df.shape[0] $ print("Number of rows are: {}".format(total_rows))
from sentiment_impact import measure_impact $ ex0 = np.array([.01,-.03,-.04,.5,.6,.8,.82]) $ plt.plot(range(len(ex0)), ex0) $ plt.show() $ print('The sentiment impact is {}'.format(measure_impact(ex0)))
musk.tail()
score = log_reg.score(X_test, y_test) $ print(score)
a = df2['landing_page'] == 'new_page' $ n_new = df2[a] $ n_new = n_new.converted.count() $ print('Number of rows in which landing page is aligned with new page is:{}'.format(n_new))
df.head(20)
g = train_df.groupby('interest_level') $ g.describe(include = 'all')
baseball_subdomain_id = 4 $ url = form_url(f'subdomains/{baseball_subdomain_id}') $ response = requests.get(url, headers=headers) $ print_body(response, skip_audit_info=True)
df_subset.boxplot(column='Initial Cost', by='Borough', rot=90) $ plt.show()
autos["price"].unique().shape
df.dtypes
f_counts_week_os.show(1)
import test_package.package_within_package
jpl_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ browser.visit(jpl_url)
crimes.head()
print("Number of Techniques in ATT&CK") $ print(len(all_attack['techniques'])) $ techniques = all_attack['techniques'] $ df = json_normalize(techniques) $ df.reindex(['matrix', 'created','tactic', 'technique', 'technique_id', 'data_sources'], axis=1)[0:5]
print(df_result.head(50))
for column in list1: $     election_data[column].fillna(election_data.groupby("st")[column].transform("mean"), inplace = True) $
l = data2Scaled.corr()['Label'] $ pd.DataFrame(data={'Technical Indicator':l.index.values[:-1], 'Correlation coefficients':l.values[:-1]},index=(range(1,19)))
building_pa_prc_shrink.dtypes
df.last_name.isnull().sum()
inputPath1 = "graphdata/people_images.csv" $ vertices = sqlContext.read.options(header='true', inferSchema='true').csv(inputPath1) $ vertices.show(5)
autos['price'].value_counts().sort_index(ascending = False).head(20)
youTubearray = youTubeTitles.values $ youTubearray
lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed] $ print(lemmed)
df.groupby('hireable')['login'].nunique()
url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars' $ browser.visit(url)
train_b, valid_b, test_b = df_b.split_frame([0.7, 0.15], seed=1234) $ valid_b.summary()
df_train['trafficSource.isTrueDirect'].unique()
df_ad_airings_4.columns
tweets_clean.rating_numerator.value_counts()
print('Last run:', datetime.datetime.utcnow(), 'UTC')  # timezone can't be detected from browser
treatment['converted'].sum() / treatment.shape[0]
IMDB_LABEL = data.Field(sequential=False) $ splits = torchtext.datasets.IMDB.splits(TEXT, IMDB_LABEL, 'data/')
from sqlalchemy import update $ stmt = update(employee).where(employee.c.id==1).values(age=49) $ stmt1 = update(employee).where(employee.c.id==1).values(marital_status='Married') $ conn.execute(stmt) $ conn.execute(stmt1)
re_json = search_response.json()
label = LabelEncoder() $ important_tweets['numerical_text'] = label.fit_transform(important_tweets['text'])
g = pd.read_csv('311_graffiti_2013_2016.csv') $ g.head(2)
df_columns[df_columns.index.month.isin([11,12,1])]['Complaint Type'].value_counts().head() $
df['body_tokens'] = df['body_tokens'].apply(nltk.word_tokenize) $ print(df['body_tokens'])
df_parties = df[df['Descriptor'] == 'Loud Music/Party'] $ df_parties.groupby(df_parties.index.hour)['Created Date'].count().plot(kind="bar") $
df2.query('user_id == "773192"')
data.loc[data.rooms.notnull(), 'rooms*price'] = data['rooms']*data['price_aprox_usd']
pd.Series(sales, index=index)
linkNYC = gpd.GeoDataFrame(linkNYC,geometry='geometry') $ linkNYC.head()
venues_df.tail(15)
print(df['user_id'].nunique())
age.sort_index()
tia['date'] = pd.to_datetime(tia['date'])
import pandas as pd $ tuner = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name) $ full_df = tuner.dataframe() $ full_df
difference = (actual_diff < p_diffs).mean() $ perc = difference * 100 $ print("{}% are greater than the actual difference.".format(perc))
train.corr()
sales['date'] = pd.to_datetime(sales['date'],format="%d.%m.%Y")
log_2.columns
X_train.info()
mean_price = pd.Series(average_price) $ mean_mileage = pd.Series(average_mileage) $ top_autos_stats = pd.DataFrame(mean_price,columns=['mean_price']) $ top_autos_stats['mean_mileage'] = mean_mileage $ top_autos_stats
data_households = {} $ for household_name, household_dict in households.items(): $     data_households[household_name] = pd.read_pickle('raw_'+household_dict['dir']+'.pickle')
n_treatment = df2.query('group == "treatment"').shape[0] $ n_treatment
plt.figure(figsize = (6,6)) $ kmeans.fit_predict(X_std) $ plt.scatter(X_std[:,0],X_std[:,1], c=kmeans.labels_, cmap='rainbow')  
update_date = groups.max()
data_spd = pd.DataFrame() $ data_spd['tweets'] = np.array(tweet_spd) $ data_spd.head(n=3)
from keras.utils import np_utils $ y_label_train_OneHot = np_utils.to_categorical(y_resampled) $ y_label_test_OneHot = np_utils.to_categorical(Vycnn,4) $ y_label_test_OneHot.shape
processed_tweets_with_obs = pd.read_csv('/s3/three-word-weather/hack/3ww-processed-with-obs.csv') $ processed_tweets_with_obs.head()
df_new['UK'].sum()
df['day'] = df['day_of_week'].apply(lambda x: days[x])
frame.apply(f)
df['payout_type'].unique()
df['date_int'] = df.created_at.astype(np.int64) $ tweetVolume(df) $ print("looks like there are some spikes in 2015.")
g = logs.groupby(logs.fm_ip) $ type(g) $
x = pd.Series(range(2), dtype=int) $ x
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
s.index
contribs.head()
autos["date_crawled_int"] = autos["date_crawled"].str[:10].str.replace("-", "").astype(int) $ autos["ad_created_int"] = autos["ad_created"].str[:10].str.replace("-", "").astype(int) $ autos["last_seen_int"] = autos["last_seen"].str[:10].str.replace("-", "").astype(int) $ autos.head()
total_sales = total_sales.dropna() # drop any Na values from the stores that closed
pd.Series([100,100]).rank()
building_pa_prc.describe(include='all')
df.is_shift.value_counts()
mnnb = MultinomialNB() $ mnnb.fit(X_train_dtm, y_train)
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}] $ new_store = pd.DataFrame(new_items, index = ['store 3']) $ new_store
data = pd.read_csv("ml-100k/u.data", sep="\t", header=None, index_col=0) $ data.columns = ["item id" , "rating" , "timestamp"]
ip_clean.tweet_id.dtype
n_new = treatment.shape[0] $ print(n_new)
import sys $ sys.path.append("..") $ from common.download_utils import download_week1_resources $ download_week1_resources()
lt = time.localtime() $ print('{}:{}:{}'.format(lt.tm_hour, lt.tm_min, lt.tm_sec)) $ print(time.asctime())
speeches_df4.shape
q_all_pathdep = c.retrieve_query('https://v3.pto.mami-project.eu/query/8da2b65bd4f7cd8d56d90ddfcd85297e8aac54fcd0e04f0a0fa51b2937b3dc62') $ q_all_pathdep.metadata()
ridgemodel.score(X,y)
df4['intercept'] = 1 $ logistic_reg = sm.Logit(df4['converted'], df4[['CA', 'US']]) $ results = logistic_reg.fit() $ results.summary()
feature_cols = list(train.columns[7:-1]) $ feature_cols
log_returns = np.log(pf_data / pf_data.shift(1))
plt.bar(0, data, width=1, yerr=[[data - error[0]], [error[1] - data]]) $ plt.title("Trip Average Temperature") $ plt.ylabel("Temperature (F)") $ plt.xticks([]) $ plt.show()
csvFile = open('ua.csv', 'a') $ csvWriter = csv.writer(csvFile)
scores_firstq = np.percentile(raw_scores, 25) $ scores_thirdq = np.percentile(raw_scores, 75) $ print('The first quartile is {} and the third quartile is {}.'.format(scores_firstq, scores_thirdq))
now = pd.Timestamp('now') $ now, now.tz is None
cityID = '18810aa5b43e76c7' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Dallas.append(tweet) 
sim_diff = new_page_converted.mean() - old_page_converted.mean() $ sim_diff
trigram_dictionary = prep.get_corpus_dict(from_scratch=False):
results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'local')
nullCity.groupby('creationDate')
active_num_authors_by_project = active_distinct_authors_latest_commit.groupBy("project_name").agg(F.count("Author").alias("author_count")) $ active_num_authors_by_project.cache() $ active_num_authors_by_project.show()
new_columns = ['year.1','salary','year.2','empID','name'] $ print( len(new_columns) == len(df.columns) ) $ df.columns = new_columns $ df.head(2)
corpus['Stem'] = corpus['Word'].apply(stem_func)
dict_urls = json.loads(df.loc[row,'urls']) $ pprint(dict_urls)
for active_add_date in daterange: $     active_add_rows = active_df[active_df['start_date']==active_add_date] $     cohort_active_activated_df.loc[active_add_date,active_add_date:] = len(active_add_rows) 
df = pd.read_pickle(".."+sep+"Data"+sep+"dfAllGPSTweetsFilterChile.p")
plt.hist(threeoneone_census_complaints[threeoneone_census_complaints['perc_white']>0]['perc_white'],bins=100) $ plt.show()
search['trip_duration'] = (search['trip_end_date'] - search['trip_start_date']).dt.days
all_df.info()
sns.set_style('whitegrid') $ sns.distplot(data_final['countCollaborators'], kde=False,color="red")#, bins=20) $
df['num_payouts'].nunique()
cfModel.fit([uids, mids], rates, nb_epoch=50, validation_split=.1, callbacks=callbacks, verbose=2)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean,(1-p_mean)]) $ new_page_converted.mean()
from sklearn import tree $ clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3, min_samples_split=2, max_features=4) $ clf = clf.fit(X, predict) $ tree.export_graphviz(clf, out_file='dt.dot',feature_names=features)
dictOfWellDf =  pd.read_pickle('dict_of__wells_df_No_features_class3_20180707.p')
damd = pd.read_csv("20170718 hashtag_damd uncleaned.csv") $ damd.columns
unique_top_tracks = df_track.merge(df_artist, on = ['artist_id','artist_name']).sort_values( $     by = 'artist_followers', $     ascending = False)[['track_name','artist_name','artist_followers']].drop_duplicates(subset = 'artist_name', $                                                                                        keep = 'first').head(5) $ unique_top_tracks
filtered_df[filtered_df['M_pay_3d'] == 0].shape[0]
archive.rating_numerator.value_counts()
dtm = vectorizer.fit_transform(df['body'])
t0 = time.time() $ model.evaluate(x = X_dev.reshape(shapeX),   y = Y_dev,   verbose=False) $ dt = time.time() - t0 $ print("Evaluation speed: {:.2f} traces/s ".format(len(X_dev) /dt)) $
manager.image_df[manager.image_df['filename'] == 'image_picea_sitchensis_in_winter_11.png'] $
plt.rcParams['axes.unicode_minus'] = False $ dta_57.plot(figsize=(15,5)) $ plt.show()
users.rename(columns={'id': 'user'}, inplace=True) $ users.head()
df2['intercept']=1 $ df2['ab_page']=pd.get_dummies(df2['group'])['treatment'] $ df2.head()
full_act_data = steps.join(heart, how='left')
twitter_ar = pd.read_csv('twitter-archive-enhanced.csv')
ind = np.arange(len(feature_importances)) $ plt.bar(ind, feature_importances, .35)
trips_data['start_date'] = pd.to_datetime(trips_data['start_date']) $ trips_data['weekday'] = trips_data['start_date'].dt.weekday
dts=['10/10/2020','11/10/2020'] $ dates=pd.to_datetime(dts, dayfirst=True) $ dates
log_mod_results.summary()
clf = LogisticRegression(fit_intercept=True).fit(X, y)
df_new = countries_df.set_index('user_id').join(df.set_index('user_id'), how='inner') $ df_new.head()
import time $ time.time()
treatment_conv = df2.query('group == "treatment"')['converted'].mean() $ treatment_conv
df_CLEAN1A['AGE'].min()
treatment_set, treatment_mapping, max_treatment_length = parse_treatment_definitons(open("data/treatment_definitons.txt", 'r'))
top50 = pd.DataFrame(popCon.groupby(by='contact').sum()) $ top50.columns = ['counts'] $ top50.sort_values(by='counts', ascending=False).head(50)
change_high_low_list = [] $ for m in data2: $     if m[2]!= None and m[3]!= None: $         change = m[2] - m[3] $         change_high_low_list.append(change)
HOU = pd.read_excel(url_HOU, $                     skiprows = 8)
import pandas as pd $ df = pd.read_csv('/u/username/data/iris.csv') $ df.head()
import pandas as pd $ col_names = ['seqid', 'source', 'type', 'start', 'end', 'score', 'strand', 'phase', 'attributes'] $ df = pd.read_csv('downloads/Homo_sapiens.GRCH38.85.gff3.gz', sep='\t', header=None, names=col_names, compression='gzip', comment='#', low_memory=False)
data = pd.Series([0.25,0.5,0.75,1.0], $                 index=[2,5,3,7]) $ data
avg_per_seat_price["BAL"] # This is the average price of a BAL PSL across all transactions.
movie2000rating.to_csv('..\\Output\\WordCloud2.csv')
df1.join(df2)
austin.head()
df_master.head()
con = presto.Connection(host=cred['host_presto'], port=80)
conglist = billstargs.congress.tolist() $ billlist= billstargs.bill_id.tolist() $ billlist = [s[:-4] for s in billlist]  # Origninal data had a 4 digit add-on to denote Congress number. Hence the -4. $ print len(billlist)
autos.loc[:,"ad_created"] = pd.to_datetime(autos.loc[:,"ad_created"]) $ autos.loc[:,"last_seen"] = pd.to_datetime(autos.loc[:,"last_seen"])
active_fire_zone_df=census_pd_complete[census_pd_complete.county.isin(['Ventura County','Santa Barbara County'])] $ active_fire_zone_df $
rows,columns= df.shape #. To return the number of cells $ print('There are {} total records in the dataset with {} no.of columns in it.'.format(rows-1, columns))
from sklearn.model_selection import KFold $ cv = KFold(n_splits=20, random_state=None, shuffle=True) $ estimator = Ridge(alpha=3500) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
summary.groupby(0).count().sort_values(by='documents',ascending=False).head(10)
sentiment_df["Text"]=text $ sentiment_df.head()
df = pd.read_json("data/open_tender/29b5a17b771e70eaa61ea7ce9ee1fb1c.tenders.json") $
df_master.drop('Unnamed: 0',axis=1,inplace=True)
to_be_predicted_Day5 = 14.80836429 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
!ptdump -av 'data/my_pytables_file.h5'
spacy_url = 'https://spacy.io/assets/img/pipeline.svg' $ iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url) $ HTML(iframe)
sample_item = [df_stars.iloc[0].business_id] $ content_rec.recommend_from_interactions(sample_item)
content_values = cached.map(lambda x: (x[1]['content_input'], x[1]['content_bhr'])).countByValue()
intqrange = transit_df['DELEXITS'].quantile(0.75) - transit_df['DELEXITS'].quantile(0.25) $ discard = (transit_df['DELEXITS'] < 0) | (transit_df['DELEXITS'] > 5*intqrange) $ transit_df = transit_df.loc[~discard] $ transit_df = transit_df.dropna()
ok.auth()
def basicWeeklyMovePredict(inpData): $     m, c = np.polyfit(inpData["Flat dates"][-14:-7], inpData["Adj_mean"][-14:-7], 1) $     prediction = c + m*(inpData["Flat dates"][-1]+7) $     return prediction
comps_df.drop_duplicates(['entity_uuid','competitor_uuid'],inplace = True)
pred_labels = lasso.predict(test_data) $ print("Training set score: {:.2f}".format(lasso.score(train_data, train_labels))) $ print("Test set score: {:.2f}".format(lasso.score(test_data, test_labels))) $ print("Number of features used: {}".format(np.sum(lasso.coef_ != 0)))
df_sample = df.sample(frac=0.1, replace=True).reset_index() $ df_sample $ X_train_, X_test_, y_train_, y_test_ = sample_split(df_sample[selected_feature])
df_download_node=pd.merge(df_download,df_node,left_on='nid',right_on='nid',how='left') $ df_download_node=df_download_node[df_download_node['title'].notnull()] $ df_download_node=df_download_node[['uid','nid','title','date','url']] $ df_download_node=df_download_node[df_download_node['date']>='2017-07-01'] $ df_download_node=df_download_node.rename(columns={'title':'download_title','date':'download_date'})
cur.execute("SELECT * FROM test.test_table;")# LIMIT 10;") $ for r in cur.fetchall(): $    print(r)
log_mod_dweek = sm.Logit(df_new['converted'], df_new[['intercept', 'Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', 'Wednesday']]) $ log_mod_dweek_results = log_mod_dweek.fit() $ log_mod_dweek_results.summary()
data[['Close', 'VolRatio']].plot(subplots=True, color='blue',figsize=(16, 10))
scores, metrics = pipeline.test(ds_valid, 'Label') $ print("Performance metrics on validation set: ") $ display(metrics)
Returning = downsample_data(Cleaneddata) $ New = downsample_data1(Cleaneddata) $ New.head()
Measurement_data = session.query(Measurement).first() $ Measurement_data.__dict__
b = np.zeros(60000000) $ print b.shape
import statsmodels.api as sm $ convert_old = sum(df2[df2['group'] == 'control']['converted']) $ convert_new = sum(df2[df2['group'] == 'treatment']['converted']) $ n_old = df2[(df2['group'] == 'control')].shape[0] $ n_new = df2[(df2['group'] == 'treatment')].shape[0]
df2=f.query("(group=='treatment' and landing_page == 'new_page')or(group=='control' and landing_page=='old_page')")
tweet_archive_clean = tweet_archive_df.copy()
print('The number of unique genera at Sakhalin Island:', len(sakhalin_filtered.species_id.unique()))
df.head()
soup = bs(response.text, 'html.parser')
to_be_predicted_Day4 = 14.85340501 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df_mas.dog_stage.value_counts()
plt.scatter(litters['lsize'], litters['bodywt']) $ plt.xlabel('Litter Size') $ plt.ylabel('Body Weight (g)') $ plt.title('Body Weight of Mice compared to Litter Size') $ plt.show()
df.info()
df2.drop_duplicates(subset='user_id',keep='first',inplace=True)
%%time $ pred = M_NB_model.predict(X_test_term)
bb = data.DataReader(name='F', data_source='iex' $                         , start='2017-07-01', end='2018-05-01')
dft[pd.datetime(2013, 1, 1, 10, 12, 0):pd.datetime(2013, 2, 28, 10, 12, 0)]
country_dummy_list = np.unique(df_new['country'].values) $ df_new[["control","treatment"]] = pd.get_dummies(df_new["group"]) # for group $ df_new[country_dummy_list] = pd.get_dummies(df_new["country"]) # for country $ df_new["intercept"] = 1 $ df_new.head()
df.tail()
color = dict(boxes='DarkGreen', whiskers='DarkOrange', medians='DarkBlue', caps='Gray') $ data[['MAG_MAX', 'TMAX', 'TMED', 'TMIN', 'MAG_MED']].plot.box(color=color)
df_twitter_extract_copy = df_twitter_extract_copy.drop('Unnamed: 0', axis = 1)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ print('Size of old_page_converted: ', len(old_page_converted))
df2[df2.landing_page == 'new_page'].count()[0]/df2.count()[0]
!pip install scikit-learn==0.17.1
pd.concat([city_loc, city_pop], axis=1)
bacteria2.mean(skipna=False)
topUserItemDocs.to_pickle('datasets/topUserItemDocsCB_27Jun_vFULL300.pkl')
df.loc[0, 'review'][-50:]
sns.boxplot(autodf.powerPS)
predictions_table.where(F.col('predicted')==1).count()
url_PIT = "https://steelers.strmarketplace.com/Images/Teams/PittsburghSteelers/SalesData/Pittsburgh-Steelers-Sales-Data.xls"
df2[['treatment', 'control']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('treatment', axis=1) $ df2['intercept'] = 1 $ df2.head()
weather_data2 = pd.read_csv('201408_weather_data.csv'); weather_data2.head()
(null_values > pop_diff).mean()
cityID = '319ee7b36c9149da' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Arlington.append(tweet) 
print (autoData.reduce(lambda x,y: x if len(x) < len(y) else y))
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func $ from sqlalchemy import desc
f_counts_week_device.show(1)
dci['weekFriendliness'].max()
%run process_twitter2tokens.py -i ../data/Training_promotion_people.csv -ot ../data/Training_promotion_people.txt -oc ../data/Training_promotion_people_tokenized.csv -co text
cohort_retention_df.columns
print finalData.shape $ print X.shape $ print dat.shape $ print Stockholm_data.shape
poverty.columns=poverty.iloc[0, :]
plt.hist(p_diffs); $ plt.xlabel('difference'); $ plt.ylabel('frequency'); $ plt.title('Simulated differences in conversion rate under the null');
p_converted_user2 = df2.query('converted==1').user_id.nunique()/df2.user_id.nunique() $ p_converted_user2
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ print(z_score, p_value)
df_date = df.copy()
m.plot(forecast) $ plt.title('Kotak Bank', fontsize=20)
pantab.frame_to_hyper(active_with_return, 'Avtive User Analysis.hyper')
df = pd.read_csv('Water_Quality_complaints.csv') $
len(train_data[train_data.fuelType == 'diesel'])
os.getcwd()
num_users_w_pets = len(pets_pet['owner_id'].unique()) $ pct_users_w_pets = round(num_users_w_pets / num_unique_users * 100., 2) $ print("The percentage of users who have added pets is: " + str(pct_users_w_pets) + "%." )
df_index_demo = df_index_demo.set_index('id') $ df_index_demo
p_conv_treat - p_conv_ctrl # !! Keep in mind that later, under the null, we are assuming this difference to be 0 anyways.
!ls ../data/imsa-cbf/ | tail
penalties.head(2)
bufferdf.Fare_amount[(bufferdf.Fare_amount==2) | (bufferdf.Fare_amount==3) | (bufferdf.Fare_amount==4)].apply(int).size
df_os[df_os['domain'] == 'infowars.com']
df['time'].describe()
rain = session.query(Measurements.date, Measurements.prcp).\ $     filter(Measurements.date > last_year).\ $     order_by(Measurements.date).all()
trainingSummary = lrmodel.summary
friday_means = friday.groupby(friday['hour']).mean() $ friday_means = friday_means.reset_index()
wrd_clean['doggo'].value_counts()[:10]
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs) $ plt.axvline(d, color='red') #draw a line on x-axis of value @d
autos = autos.rename(columns={'odometer':'odometer_km'})
df['user_id'].count() - df.query('group == "treatment" and landing_page == "new_page" or group == "control" and landing_page == "old_page"')['user_id'].count()
conv_learner.ConvLearner.pretrained(arch, data, ps=0, precompute=True, xtra_fc=[])
import IPython $ print (IPython.sys_info()) $ !pip freeze
df[df['converted']==1].shape[0]/df.user_id.nunique()
df2.to_csv('ab_data_new.csv', index=False)
data.iloc[:,10:14] = data.iloc[:,10:14].fillna("0")  # Overall_Credit_Status, Delivery_Block, Billing_Block, Block_flag $ data.iloc[:,26] = data.iloc[:,26].fillna("0")   # state $ data.dropna(how='any',axis='rows',inplace=True) # district
df.index + pd.DateOffset(months=2, days=5)
df2 = pd.read_csv("msft.csv", $                   usecols=['Date', 'Close'], $                   index_col=['Date']) $ df2.head()
t['Counts'] = t['Counts'].apply(lambda x: x*100/202)
autos = autos[autos['registration_year'].between(1900,2016)] $ autos['registration_year'].value_counts(normalize=True).sort_index()
autos['odometer'] = autos['odometer'].str.replace("km","").str.replace(",","").astype(int) $ autos.rename({"odometer": "odometer_km"}, axis=1, inplace=True) $ autos["odometer_km"].head()
prcp_scores = session.query(measurements.date,func.avg(measurements.prcp)).\ $                     filter(measurements.date>=year_ago).\ $                     group_by(measurements.date).all() $ prcp_scores
t.microsecond
to_be_predicted_Day2 = 81.77623296 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
idxs = dataset.get_cv_idxs(n, val_pct=150000/n) $ joined_samp = joined.iloc[idxs].set_index('Date') $ samp_size = len(joined_samp); samp_size
port_perf = calc_port_performance(r_clean.values, hist_alloc.values) $ pdf = pd.DataFrame(port_perf, index=r_clean.index, columns=[dwld_key + '_optimized'])
plt.figure(figsize=(12,6)) $ sns.barplot(x='lead_source', y= 'discConversionPercent', data=discConvpct) $ plt.xticks(rotation=90) $ plt.title('Discovery to Closed Won Percentage');
sns.barplot(k.index,k.percent_of_total)
lm = sm.Logit(df["converted"],df[["intercept","control"]]) $ results = lm.fit() $ results.summary() $
train.head()
search_results = gis.content.search('title: USA Major Cities', $                                     'Feature Layer') $ major_cities_item = search_results[0] $ major_cities_item
r.json()['dataset']['data'][0] $
plt.scatter(my_df["xcoordinate"], my_df["ycoordinate"]) $ plt.axhline(0.5) $ plt.axvline(0.5) $ plt.show()
fig, ax = plt.subplots() $ typesub2017['Wind Onshore'].plot(ax=ax, title="Onshore Wind Energy Generation 2017 (DE-AT-LUX)", lw=0.7) $ ax.set_ylabel("Onshore Wind Energy") $ ax.set_xlabel("Time")
cleaned_texts = df.cleaned_text.apply(lambda x: ' '.join(x))
dapr_538 = pd.read_csv("data/uber/uber-raw-apr14.csv", parse_dates=["Date/Time"]) $ dapr_538.head(3)
x.loc[x.loc[:,"A"]>0.6,"A"]
data_full = pd.get_dummies(data_full, columns = ["timeliness"])
df.shape
df_new[["CA", "UK", "US"]] = pd.get_dummies(df_new["country"]) $ df_new.head()
link = soup.find('li').find('a')['href'] $ link
testObjDocs.outDF[984:991]  ## as expected - bad rows dropped but we now have an indexing issue
glm_binom_feat_2 = H2OGeneralizedLinearEstimator(family='binomial', solver='L_BFGS', model_id='glm_v5', Lambda=0.001) $ glm_binom_feat_2.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)
movies = pd.read_csv('../../data/raw/movies.csv',  encoding='latin-1') # Loading in the dataset
index = similarities.MatrixSimilarity(lsi[corpus]) $ index.save('bible.index')
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_axial_brace_saddle[1][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_saddle[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df2.query('landing_page=="new_page"').count()[0]/df2.count()[0]
playlist = sp.user_playlist(spotify_url.split('/')[4],spotify_url.split('/')[6]) $ pd.io.json.json_normalize(playlist)
tweet_image_predictions_clean['tweet_id'] = tweet_image_predictions_clean['tweet_id'].astype('str')
Y_lin_reg = lin_reg.predict(X) $ from sklearn import metrics $ print('RMSE:', np.sqrt(metrics.mean_squared_error(Y, Y_lin_reg))) $ print('Variance explained: ', metrics.explained_variance_score(Y, Y_lin_reg))
rounds_df = rounds[rounds.company_uuid.isin(df.uuid)].copy() $ rounds_df = rounds_df[(rounds_df.announced_year >= 1990) & (rounds_df.announced_year <= 2016)].copy()
plt.pie(total_ride, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140) $ plt.show()
train_df = train_df.drop(['Name', 'PassengerId'], axis=1) $ test_df = test_df.drop(['Name'], axis=1) $ combine = [train_df, test_df] $ train_df.shape, test_df.shape
deployment_details = client.deployments.create(model_guid, name="Keras LSTM deployment")
new_page_converted = np.random.binomial(n_new,convert_rate_p_new)
against = merged[merged.committee_position == 'OPPOSE'] $ support = merged[merged.committee_position == 'SUPPORT']
df1['dayofweek'] = df1['effective_date'].dt.dayofweek $ df1['weekend']= df1['dayofweek'].apply(lambda x: 1 if (x>3)  else 0) $ df1['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True) $ Feature1=df1[['Principal','terms','age','Gender','weekend']] $ Feature1=pd.concat([Feature1,pd.get_dummies(df1['education'])], axis=1) $
threeoneone_census_complaints['ln_median_income_new']= np.log(threeoneone_census_complaints['median_income_new']+1) $ threeoneone_census_complaints['ln_complaint_density']= np.log(threeoneone_census_complaints['complaint_density']+1) $ threeoneone_census_complaints['population']=threeoneone_census_complaints['population'].astype(float)
etsamples_100hz.loc[etsamples_100hz.eyetracker=='el','pa_diff']=etsamples_100hz.query("eyetracker=='el'").pa_norm.values-etsamples_100hz.query("eyetracker=='pl'").pa_norm.values
from astropy.coordinates import EarthLocation, AltAz $ paris = EarthLocation(lat=48.8567 * u.deg, lon=2.3508 * u.deg ) $ crab_altaz = c2.transform_to(AltAz(obstime=now, location=paris)) $ print(crab_altaz)
s1=[pairs['score_x'].loc[1:2261:2][2*i+1]-pairs.loc[0:2261:2,'score_x'][2*i] for i in np.arange(1131)] $ plt.hist(s1,bins=np.arange(-30,40,1)) $ plt.show()
df.columns    # what are all te
user_tweet_count_df.head()
archive_clean.info()
def generate_weighted_returns(returns, weights): $     assert returns.index.equals(weights.index) $     assert returns.columns.equals(weights.columns) $     return None $ project_tests.test_generate_weighted_returns(generate_weighted_returns)
bo = pd.read_csv('Boroughs.txt')
multi_var = sm.OLS(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'UK']]) $ multi_var_results = multi_var.fit() $ multi_var_results.summary()
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['reviews'])
baseball_h.loc[(2007, 'ATL', 'francju01')]
gap = test.date.min() - train.date.max() $ gap
conv_prob = df2.converted.mean() $ print('The probability of an individual converting regardless of the page they receive is {}.'.format(conv_prob))
teams_list = pd.read_csv('~/dotaMediaTermPaper/data/teams_df.csv')['name'].tolist()
pd.pivot_table(expenses_df, values = "Amount", index = "Buyer", aggfunc = np.sum)
yeardf = pd.DataFrame((1+newdf['TSRet']/100).resample('Y').prod())
products_with_nulls=len(nulls['item_nbr'].unique()) $ all_products=len(items['item_nbr'].unique()) $ products_with_nulls/all_products
next(iter(md.trn_dl))
svm_parameters = [{'kernel':['linear', 'poly', 'rbf', 'sigmoid'], $                   'C':[1.0, 0.5, 0.25, 0.1, 0.05, 0.01], $                   'class_weight':['balanced', {0:weights[0], 1:weights[1]}]}]
new_model =  gensim.models.KeyedVectors.load_word2vec_format(path_database+'lesk2vec.bin', binary=True)
url = form_url(f'organizations/{org_id}/actions') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
df.drop(todrop2, inplace=True)
old_page_converted = np.random.choice([0,1], size=n_old, p=[1-p_old, p_old]) $ print(old_page_converted.mean())
df_A[df_A.index.str.endswith("5")]
emojis_db=pd.read_csv('emojis_db_csv.csv') $ emojis_db.head()
df_pca = pd.DataFrame(pca.transform(df_norm), columns=labels) $ df_pca.head()
dates=joined['date'].drop_duplicates(); dates[0]
import tweepy $ authentication = tweepy.OAuthHandler(config.consumer_key, config.consumer_secret) $ authentication.set_access_token(config.access_token, config.access_secret) $ api = tweepy.API(authentication)
MATTHEW_gb_user = matthew.groupby('user')
PYR.head()
hour_of_day14 = uber_14["hour_of_day"].value_counts().sort_index().to_frame()
ozzy.buddy.doginfo()
df.count()
data = allocate_equities(allocs=[0.25,0.25,0.25,0.25]) $ data.plot() $ plt.show()
np.shape(ndvi_coarse)
breaches.groupby(['IsVerified', 'IsSensitive']).sum()
df_q = pd.read_sql(query, conn, index_col=None) $ df_q.head(5)
grouped_by_day_df.to_csv('grouped_by_day_df.csv',index=False)
mnb.fit(Bow_X_train, Bow_y_train) $ print('Training set score:', mnb.score(Bow_X_train, Bow_y_train)) $ print('\nTest set score:', mnb.score(Bow_X_test, Bow_y_test)) $ print('\nCross Val score:',cross_val_score(mnb, Bow_X_test, Bow_y_test, cv=5))
ax = perf2.prices.to_drawdown_series().plot()
df_apps = pd.read_csv('apps.csv', index_col=None)
orgs = pd.merge(orgs, org_descs, on = 'uuid')
merge.committee_position.value_counts()
from pyspark.sql.functions import udf $ from pyspark.sql.types import DoubleType, StructType $ def get_pred(probability): $     return(round(probability.toArray().tolist()[1],6)) $ get_pred_udf = udf(get_pred, DoubleType())
dffreq = dfpolicies.groupby(['state'])['state'].count().to_frame()
ad_nlp = df[['Text', 'Score_Binary']] $ ad_nlp.head()
outlier = all_simband_data[all_simband_data.subject_id == 'LW-OTS BHC0048-1']
autos['ad_created'].str[:10].value_counts().sort_index()
print(filecounts.to_string())
rddScaledScores = RDDTestScorees.map(lambda entry: (entry[1] * 0.9)) $
plt.scatter(my_df["xcoordinate"], my_df["ycoordinate"]) $ plt.show()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=2500) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df.iat[1,1]
autos.loc[conditions,'registration_year'] = np.nan $ autos.dropna(subset=['registration_year'],inplace=True)
treatment_cr = df2.query('converted == 1').user_id.nunique()/df2.user_id.nunique()
years = data.set_index("year") $ years.head() $ years.tail()
Y_train_lab = le.fit_transform(Y_train) $ Y_valid_lab = le.transform(Y_valid)
np.isnan(StockData).sum().sum()
%%time $ stmt = text("SELECT * FROM states where last_changed>=:date_filter") $ stmt = stmt.bindparams(date_filter=datetime.now()-timedelta(days=100)) $ allquery = engine.execute(stmt) $ allqueryDF = pd.DataFrame(allquery.fetchall())
srcdf = srcdf.merge(md, on='subject', how='left') $ srcdf
t0 = time() $ km = KMeans(n_clusters=10, random_state=0).fit(data) $ print("Clustering sparse data with %s" % km) $ print() $ print() $
graf_counts = graf_counts.reset_index()
model.get_config()
df['Shop_Existence_Days_Corrected'] = df['Shop_Existence_Days'] $ df['Shop_Existence_Days_Corrected'].iloc[df[df['Merchant_Plan_Offer']=='Trial'].index] = 30 $ df['Shop_Gross_Sales_Ratio'] = df['Shop_Gross_Sales']/df['Shop_Existence_Days_Corrected'] $ df['Shop_Bo_Connections_Ratio'] = df['Shop_Bo_Connections']/df['Shop_Existence_Days_Corrected'] $ df['Shop_Orders_Count_Ratio'] = df['Shop_Orders_Count']/df['Shop_Existence_Days_Corrected']
count_id = df.groupby('subreddit').agg({'id': 'count'}) $ top_sub = count_id.sort_values('id', ascending = False).head() $ top_sub
df_centered.select('user_id','stars','date','seq', 'numOfReviews', 'ratings_centered').orderBy('user_id', 'seq').limit(20).toPandas()
techmeme['date_time'] = pd.to_datetime(techmeme.date) $ techmeme.date = techmeme.date_time.apply(lambda x: x.date()) $ techmeme.extra_sources = techmeme.extra_sources.apply(lambda x: ast.literal_eval(x))
aml.leader.save_mojo()
week_day_frequency = my_df["week_day"].value_counts() $ ax = plt.subplot(111) $ ax.bar(week_day_frequency.index, week_day_frequency.data) $ plt.show()
some_df = sqlContext.createDataFrame(some_rdd) $ some_df.printSchema()
df.shape[1]
df.isnull().values.any()
!head data/train_users_2.csv
temp_df = Ralston.TMAX $ for i in range(10): $     temp_df= temp_df + 5 $     print ("Iteration",i+1,":", temp_df.mean(), temp_df.std())
def make_soup(url): $     html = urlopen(url).read() $     return BeautifulSoup(html, "lxml")
Station.__table__
fil = df2['two'].isin(['t1','t3','t4'])  # Using isin for filtering $ fil
twitter_archive_with_json = pd.merge(twitter_archive_clean, tweet_json_clean, how='left', on=['tweet_id']) $
headers= ({'User-agent': 'kiros Bot 0.1'})
dfwithplace = df[~df['place'].isnull()]
pd.options.display.max_columns = 100 $ X_train.head(2)
transactions.merge(transactions, left_on=['UserID'], right_on=['UserID'], how='inner') $
class_merged_state=class_merged_hol['state'].unique() $ print(class_merged_state)
df_merged['intercept_country'] = 1 $ log_mod = sm.Logit(df_merged['converted'], df_merged[['intercept_country','US', 'UK']]) $ results = log_mod.fit() $ results.summary()
index_df = index_ts[['price_date', 'ticker', 'adj_close_price']].copy().set_index('price_date') $ index_df.head()
df.dropna(inplace=True)
run txt2pdf.py -o"FLORIDA HOSPITAL - 2014 Percentiles.pdf"  "FLORIDA HOSPITAL - 2014 Percentiles.txt" 
merge = pd.merge(left = INC, right = weather, how = 'left', left_on = 'dateShort', right_on = 'dateShort') $ merge.head() $
df_dates_final = df_merged.groupby(df_merged.columns[0]).min() $ df_dates_final.head()
df2.query("user_id == 773192")
df.corr()
visual_recognition = VisualRecognitionV3( $     '2018-03-19', $     iam_api_key=os.environ['BLUEMIX_API_KEY'])
client.get_list_users()
logit = sm.Logit(df2['converted'],df2[['intercept','treatment']])
pd.read_json('data/data.json').shape
submit = perf_test[['ID_CPTE']] $ submit['Default'] = log_reg_pred $ submit.head()
df.shape
def mean_absolute_percentage_error(y_true, y_pred): $     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
df['complaint_'] = df['complaint'].apply(lambda x: (x[0]))
propnames.value_counts().reset_index()
print('The probability of an individual converting regardless of the page they received is {}'.format(df2.converted.mean()))
df_new.head()
sum(df2.landing_page == 'new_page').astype('float32') / 290584
top_10_authors = git_log['author'].value_counts().head(10) $ top_10_authors
finalSymbolsList = strippedSymbolsList.apply(getFinalSymbol)
dti.freq
coins_mcap_today = mcap_mat.iloc[-2] $ coins_mcap_today = coins_mcap_today.sort_values(ascending=False)
y_train.value_counts()
temps_df.Missoula > 82
archive_df.loc[(archive_df.name=='None') | $                (archive_df.name.str.islower()), 'name'] = ''
all_data = pd.concat([concat, ti_mar[ti_mar['store'] == 'Suning']]) $ all_data.groupby('store').size() $
top_songs[top_songs['Track Name'].isnull()]['Region'].unique() $
step_threshold = 300 $ pax_raw[pax_raw.paxstep<step_threshold].paxstep.hist(bins=30)
from sklearn.metrics import accuracy_score $ y_pred = rnd_clf.predict(X_test) $ accuracy_score(y_test, y_pred)
new_page_converted = np.random.choice([1,0], size=df_newlen, p=[pnew,(1-pnew)]) $
new_page = df2.query('landing_page == "new_page"').shape[0] $ p4 = float(new_page / df2.shape[0]) $ print("The probability that an individual received the new page is {:.4f}".format(p4))
refcell_clear_df = irradiance_clear_df.iloc[:,:3] $ pyranometer_clear_df = irradiance_clear_df.iloc[:,-1]
print("the day of the month is: ", today.day) $ print("we are curretly in month number", today.month) $ print("The year is", today.year)
print('Here we can see that there are missing values for several features in store:', store.isnull().sum().sum())
epiweek = 201702 $ cid10 = CID10['dengue'] $ geocode = rio_id $ df_forecast_model = pd.read_sql(sql, con=engine) $ df_forecast_model
git_log.timestamp.dt.hour.head()
s = pd.Series(['A', 'asdDAr', 'dioD', 'sp', 'MelVille']) $ s.str.lower()
set(top5_best_fan.keys())
future_dates_df = pd.DataFrame(index=future_dates[1:],columns=dfs.columns)
pnew = df2['converted'].mean() $ print(pnew) #creating the mean converted values as pnew
ser = pd.DataFrame({'By': dates, 'key':[0] * len(dates)}) $ ser
df.columns
sample_sizes.show()
from calendar import day_name $ pd.DataFrame({'Day':[day_name[i] for i in range(7)], $             'Avg. Temp':noaa_data['AIR_TEMPERATURE'].groupby(noaa_data.index.dayofweek).mean()}, $              columns=['Day','Avg. Temp'] $ ).set_index('Day').T
tips.sample(5).reset_index(drop=True)
session.query( $     Country.country_id, Country.country $ ).filter( $     Country.country.in_(["Afghanistan", "Bangladesh", "China"]) $ ).frame()
snow.select("select * from st_rvo_me_ref").to_excel("out/ref.xlsx", index=False)
print total.shape[0], total.dropna().shape[0] $ total.head()
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1').user_id.nunique()  #  we had this from part I Q4. $ convert_new = df2.query('group == "treatment" and converted == 1').user_id.nunique() # this one too. $ n_old = df2.query('group == "control"').user_id.nunique() $ n_new = df2.query('group == "treatment"').user_id.nunique()
print(lr.score(testing_array, y_test))
print(pd.crosstab(classes, classificacoes_treino, rownames = ["Real"], colnames=["Predito"], margins=True))
joined_sampled_and_infered = sampled_contirbutors_human_agg_by_gender_and_proj_df.join( $     relevant_info_agg_by_gender_and_proj_df, $     on="project")
disag_filename = join(data_dir, 'disag_gjw_CO.hdf5') $ output = HDFDataStore(disag_filename, 'w') $ co.disaggregate(mains,output) $ output.close()
df1['State'].replace('PCS', 'PCS-opt', inplace=True) $ df2['State'].replace('PCS', 'PCS-etgl', inplace=True)
users_converted = df['converted'].mean()*100 $ print("The proportion of users converted - {}%".format(round(users_converted,2)))
Tip_percentage_of_total_fare = taxiData.Tip_amount / taxiData2.Fare_amount
data['ATR5'] = (data['ATR'].rolling(min_periods=1, window=5).sum())/4 $ data.tail()
coins_mcap_today[50:].index
for column in df_categorical: $     df_categorical[column].unique()
tweet.lang $ tweet.text $ tweet.retweet_count $ tweet.place $ tweet.geo
users['DaysActive'] = (users['LastAccessDate'] - users['CreationDate']) / np.timedelta64(1, 'D')
from sqlalchemy.sql import select
answers_df = pd.io.json.json_normalize(raw_annotations_df.answers.tolist()) $ answers_df.tail(5)
df.resample('D', how='count')
number_of_commits = git_log['timestamp'].count() $ number_of_authors = git_log['author'].nunique() $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
import statsmodels.api as sm $ train_cols = df2.columns[5:7] $ train_cols $ logit = sm.Logit(df2['converted'], df2[train_cols]) $ result = logit.fit() $
contour_sakhalin.shape
(details.Popularity == 0).value_counts()
_ = ok.submit()
conf_matrix = confusion_matrix(training_test_labels, lr.predict(preproc_training_test), labels=[1,2,3,4,5]) $ conf_matrix
%%bash $ mkdir sample $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" sample/train.csv $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-*" sample/valid.csv
new_page_converted = np.random.binomial(1,0.1196,145310) $ new_page_converted
dummy_df['price_change_1day'] = (dummy_df['price'].shift(-1440) - dummy_df['price']).fillna(method = 'ffill') $ dummy_df['price_change_2days'] = (dummy_df['price'].shift(-2880) - dummy_df['price']).fillna(method = 'ffill') $ dummy_df['price_change_3days'] = (dummy_df['price'].shift(-4320) - dummy_df['price']).fillna(method = 'ffill')
len(orig_android_tweets[orig_android_tweets['date'].dt.year == 2016])
cur.execute('SELECT * from iris')
df_arch_clean.loc[df_arch_clean.tweet_id== '681340665377193984', 'text'] $
tweet_clean.retweeted_status.value_counts()
session.query(Measurement.station, Station.name, Station.latitude, Station.longitude, Station.elevation, func.sum(Measurement.prcp))\ $     .group_by(Measurement.station).order_by(func.sum(Measurement.prcp).desc())\ $     .filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all()
Gun = G.to_undirected() $ politiciansPartyDict = nx.get_node_attributes(Gun, "party") $ partitions = community.best_partition(Gun, weight='weight')
print(tipsDF.describe())
countries_df = pd.read_csv('countries.csv')
g = df_usa['GDP'].values $ p= df_usa['Population'].values $ df_usa['GDP/capita']= g/(p*1000) $ df_usa.round(decimals=2)
calls_df[calls_df["phone number"]==5419196969].head()
msk = np.random.rand(len(df)) < 0.8 $ train = cdf[msk] $ test = cdf[~msk]
ab_df2[((ab_df2['group'] == 'treatment') == (ab_df2['landing_page'] == 'new_page')) == False].shape[0]
pd.Timestamp('2010-11-12')
pred.head(rows=3)
BDAY_PAIR_qthis = BDAY_PAIR_df[pd.to_datetime(BDAY_PAIR_df.created_at)>=qthis]
np.exp(results2.params)
for i in range(-5, 0, 1) : $     data[f'Open {i}d'] = data['Open'].shift(-i) $ data = data.dropna() $ data.head()
tweets['sentiment'].value_counts()
pd.read_csv("Data/microbiome.csv", nrows=4)
from datetime import datetime $ xml_in['days_diff_to_publication'] = (datetime.now() - xml_in['publicationDate']).astype('timedelta64[D]')
df.dtypes.head(20)
poptweets = dict(df[['text', 'retweetCount']].groupby('text')['retweetCount'].sum()) $ poptweets = pd.DataFrame(list(poptweets.items()), columns=['text', 'Popularity']).sort_values('Popularity', ascending=False)[0:20] $ poptweets.to_csv('poptweets.csv', index=False)
dfRegMet2014 = dfRegMet[dfRegMet.index.year == 2014]
p_hat = 5. / 8. $ freq_prob = (1 - p_hat) ** 3 $ print("Naive Frequentist Probability of Bob Winning: %.2f" %freq_prob)
df["2015-01":"2015-03"]['Complaint Type'].value_counts().head(5)
cols_to_export = ["epoch","src","trg","src_str","src_screen_str","trg_str","trg_screen_str","lang","text"] $ mentions_df.to_csv("/mnt/idms/fberes/network/ausopen18/data/ao18_mentions_with_names_and_text.csv",columns=cols_to_export,sep="|",index=False)
treat = ab_data.query('group == "treatment"') $ contr = ab_data.query('group == "control"') $ contr['landing_page'].value_counts(), treat['landing_page'].value_counts()
A = rng.randint(10, size=(3,4)) $ A
high_rev_acc_opps_net.head()
q_all_count.results()["count"].sum()
def pos_to_lemmatizer_pos(pos_initial):        $     if pos_initial != 'J': $         return pos_initial.lower() $     else: $         return 'a'
store_items.pop('glasses') $ store_items
log_mod_m = sm.Logit(df_countries['converted'], df_countries[['intercept','UK_ind_ab_page','US_ind_ab_page']]) $ results_m = log_mod_m.fit() $ print("DONE") $
(p_diffs>act_diff).mean()
df2_trt = df2.query('group == "treatment"'); $ df2_trt['converted'].mean()
py.iplot(data_predict.groupby(freq).sum()[['sales', 'predict']].iplot(asFigure=True, $                                kind='bar',xTitle='Dates',yTitle='Sales',title='Actual vs. Predicted'))
submit.tail()
df['units_purchased'] = pd.to_numeric(df.units_purchased) $ df['total_spend'] = pd.to_numeric(df.total_spend, errors='coerce')
volume_yearly = vol.groupby(vol.index.year).sum()
y = abc.loc[:10,'discharges'] $ x = abc.loc[:10,'name'] $ plot_top_sites_DRG(871,2016,x,y)
ix = df_test.index[0] $ ix
telemetry_feat.head()
results.columns = ['height (in)', 'sex'] $ results.index = ['A', 'B','C', 'D', 'E', 'F', 'G'] $ print(results)
row_concat = pd.concat([uber1, uber2, uber3]) $ print(row_concat.shape) $ print(row_concat.head()) $ print(row_concat.tail()) $
y.value_counts()
X_train, X_test, y_train, y_test = train_test_split(features,classification_price,test_size=0.2)
df['series_or_movie_name'].nunique()
old_page_converted = np.random.choice([0,1], size=n_old, p=[1-p_old, p_old]) $ old_page_converted
den.shape
print("The middle donation amount is: $", donations['Donation Amount'].median()) $
airbnb_df = pd.read_csv('data/airbnb2.csv') $ airbnb_df.head()
print(data.shape) $ data = data.drop_duplicates() $ print(data.shape)
pd.DataFrame(features['LAST(loans.MEAN(payments.payment_amount))'].head(10))
df_world_map.plot()
step_counts.index = pd.date_range('20171229', periods=step_counts.size) $ print(step_counts)
Measurements = Base.classes.measurements # Map measurements class $ Stations = Base.classes.stations # Map stations class
len(vectorizer.get_stop_words())
withDups.ix['2012-01-03']
df_best_chart.pivot_table(values='Updated Shipped diff_normalized',columns='Place Name',index='Shipped At').plot()
len(df_characters), len(df.groupby('raw_character_text')['episode_id'].nunique().reset_index())
gMapAddrDat.location = ""
tweets_raw = tweets_raw.drop(axis= 1, labels=  ["id", "user_name"]) $ tweets_raw = tweets_raw.dropna()
local_tz = pytz.timezone('America/New_York')
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2']])
pd.date_range(start,end, freq='M')
n_old=df2.query('landing_page=="old_page"').shape[0] $ n_old
html_table = df.to_html() $ html_table
therm_fiss_rate = openmc.Tally(name='therm. fiss. rate') $ therm_fiss_rate.scores = ['nu-fission'] $ therm_fiss_rate.filters = [openmc.EnergyFilter([0., 0.625])] $ tallies_file.append(therm_fiss_rate)
daily_constituent_count = QTU_pipeline.groupby(level=0).sum() $ QTU_pipeline.groupby(level=0).median().describe()
train.unique_items
df.head()
properati['zone'].value_counts(dropna=False)
twitter_data_v2[~twitter_data_v2.tweet_id.isin(tweet_data_v2.tweet_id)]
many_bdays = [ix for ix in BDAY_PAIR.index if len(np.unique(BDAY_PAIR.loc[ix,'birthdate'])) > 1]
flight6.coalesce(2).write.parquet("C:\\s3\\20170503_jsonl\\flight6.parquet")
autos[autos.registration_year > 2016]
df_r2.loc[df_r2["CustID"].isin([customer])]
arr2d = np.arange(30, dtype=np.float32).reshape(10, 3) $ arr2d
airbnb_df['room_type'].value_counts(dropna=False)
def episode_finished(r): $     print("Finished episode {ep} after {ts} timesteps (reward: {reward})".format(ep=r.episode, ts=r.episode_timestep, $                                                                                  reward=r.episode_rewards[-1])) $     return True
table1.head(10)
df.median()
a.count()
df=df.dropna(axis=1,how='all')
count_vectorizer = CountVectorizer(min_df = 10, ngram_range=(1, 2), max_df=.5, $                                    stop_words=nltk_stopwords, token_pattern="\\b[a-z][a-z]+\\b") $ count_vectorizer.fit(final_tweets)
y_pred = gscv.best_estimator_.predict(x_test)
unique_brand_dict = {} $ for b in brand_unique: $     mean_price = autos.loc[autos['brand'] == b,'price'].mean() $     unique_brand_dict[b] = mean_price $ print(unique_brand_dict)    $
from pysumma.Simulation import Simulation $ from pysumma.Plotting import Plotting
f0.exclude_list_filter
(grades > 5).all()
ls = [bbc,cnn,abc] $ print("5 recent tweets from abc: \n") $ for tweet in abc[:5]: $     print(tweet.text) $     print()
p_salary=portland_census2.drop(portland_census2.index[:49]) $ p_salary.head(3)
import seaborn as sns $ sns.barplot(y="features", x="importance", data=rankImportance, palette= 'hls')
iplot(data.groupby('user.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, dimensions=(750, 500), kind='bar'))
weather_df.loc[weather_df["weather_main"].isin(["Dust", "Sand", "Smoke", "Squall"]), ["weather_main", "weather_description"]] = np.NaN $ weather_df = weather_df.fillna(method="ffill")
from pyspark.sql.functions import udf $ from pyspark.sql.types import DoubleType, StructType $ def get_pred(probability): $     return(round(probability.toArray().tolist()[1],6)) $ get_pred_udf = udf(get_pred, DoubleType())
corn_vege.size()
print('Coefficients: \n', regr2.coef_) $ print('Intercept: \n', regr2.intercept_)
my_gempro.find_disulfide_bridges(representatives_only=False)
df = pd.melt(dfa, id_vars = 'Series', value_vars = vars2) $ print(df.head(5)) $ print(df.tail(5)) $
Pop_df['Year'] = 0.0 $ for i in range(26): $     Pop_df.Year[i] = Pop_df.Date[i] $
query = 'SELECT count(*) FROM ways' $ c.execute(query) $ results = c.fetchall() $ print results[0][0]
y_test_over[fm_bet_over].sum()
len(submission.ImageId.values)
goodreads_users_df.isnull().sum()
b = pd.read_sql_query(q, conn) $ b
classified_queensland_data = queensland_data.assign(topic=topic_preds) $ classified_queensland_data.loc[classified_queensland_data['topic'] == 'victims']
df_weather.loc[:, "events"] = df_weather.events.apply(lambda x: "Rain" if x == "rain" else x) $ df_weather.events.fillna(value="No-Event", inplace=True) $ print("\nThe types of weather events in the 'events' column are:") $ evnts = [str(x) for x in df_weather.events.unique()] $ print("".join([str(i+1) + ". " + evnts[i] + "\n" for i in range(len(evnts))]))
from sklearn.metrics import classification_report $ labels = ['Setosa', 'Versicolor', 'Virginica'] $ y_pred = nbc.predict(d_test_sc) $ print(classification_report(l_test, y_pred, \ $                             target_names = labels))
merged2['AppointmentDuration'] = merged2['AppointmentDuration'] / 60.0
obs_diff = new_page_converted.mean()-old_page_converted.mean() $ obs_diff
df.head()
df['rating'].describe()
temp_df.columns = ["id", "login", "name", "company", "location", "email", "created_at", "type", "long" , "lat"]      
import matplotlib.pyplot as plt $ %matplotlib inline
general_volume['ds']=general_volume.ds.astype('datetime64[ns]') $ joined_df = general_volume.merge(forecast, left_on='ds', right_on='ds')
barcelona.dtypes
plt.show()
all_311_requests.to_csv("MyLA311_All_Requests.csv", encoding = 'utf-8', index = False)
combined_df = pd.merge(sales_data, returns_data, how = 'left', on = 'Order ID') $ print(combined_df)
p_new = df2.query('landing_page == "new_page"').user_id.nunique()/df2.user_id.nunique() $ print('The probability that an individual received the new page is {}.'.format(round(p_new,4)))
components= pd.DataFrame(pca.components_, columns=['SP500','DJIA','happiness'])
merged_df = pd.concat([taxi_weather_df, seats_per_hour], axis=1)
filtered_brewery[['beer_name', 'brewery_name', 'rating_score']][filtered_brewery.brewery_name == top_three[0][0]]
plt.imshow(result) $ plt.show()
TrainData_ForLogistic.columns
eve_contr =df_eve.query("ab_page==0").converted.mean() $ eve_contr
y = valid['Visits'] $ y_pred = valid.merge(median_15, on="Page", how='left')['pred_Visits'] $ smape_fast(y, y_pred)
round((timelog.seconds.sum() / 60 / 60 / 24), 1)
for i in range(10): $     ypred = model.predict(np.random.randint(0, 1,(1,36,1))) $     res = [int_to_char[n] for n in word] $     print(''.join(res))
week_df = df.groupby(df['Datetime'].dt.weekday_name).count() $ print(week_df['Incident_number'])
tree = DecisionTreeClassifier(criterion='gini') $ model = tree.fit(X_train_total, y_train) $ model.score(X_test_total_checked, y_test)
df0901.head()
dataset  = dataset.join(longest_date_each_costumer, ['customer_id'],rsuffix="_day") $ dataset.rename(columns={'created_at_date_day':'day_since_last_order'}, inplace=True) $ dataset.head(3)
df_new[['UK','US','CA']]=pd.get_dummies(df_new['country'])
pops = [] $ for country in data_countries.country_destination.values: $     print "country: %s, popul: %d" % (country, sum(data_age_gender_bkts[data_age_gender_bkts.country_destination == country].population_in_thousands.values)) $     pops.append(sum(data_age_gender_bkts[data_age_gender_bkts.country_destination == country].population_in_thousands.values)) $ data_countries["population_in_thousands"] = pops $
df2.query("group == 'treatment'").count()['group']
msft.loc['2012-1-3']
from azureml.core.webservice import AciWebservice $ aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, $                                                memory_gb = 4, $                                                tags = [TICKER, "Close", "lstm"], $                                                description = "ACI Service to predict "+ TICKER +" Close price")
tsla.info()
tweet1.user.screen_name
out_file = 'data/temp.txt' $ with open(out_file, 'w') as fout: $     fout.write("Hello World!\n") $     fout.write("Goodbye World!\n")
keras_entity_recognizer.fit(df_train)
unicode = text.encode('unicode_escape') $ print(unicode) $ unicode.find(b'\U0001f60e')
tlen.plot(figsize=(16,4), color='r')
scores.head()
testObjDocs.outDF[975:1000]  ## spot check run on batches of 25 records to find the bad ones (b/2 900 and 1000) $
model.fit(train_words, fb_train.popular)
print(train_df[train_df.author.isnull()].shape[0]) $ print(train_df.shape[0])
print('How many eggs and apples do we need to buy:\n', groceries.loc[['eggs', 'apples']]) 
train_set.head(10)
tokenization_url = 'https://spacy.io/assets/img/tokenization.svg' $ iframe = '<iframe src={} width=650 height=400></iframe>'.format(tokenization_url) $ HTML(iframe)
df_th = df.loc[df['original_language'] == 'th'] $ df_th
X = pd.get_dummies(reddit['Subreddits']) $ y = reddit['Above_Below_Median'] $ from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
stamp.strftime('%d/%b/%y')
for i in by_periods.index[:5]: $     print("{0}:{1} {2}".format(i.start_time, i.end_time, by_periods[i]))
cars = autos['brand'].value_counts().head(7).index
len(df)==df.timestamp.nunique()
df_uk = df_new[df_new['country'] == 'UK'] $ df_uk['converted'].mean()
engine = create_engine("sqlite:///Resources/hawaii.sqlite") $ conn = engine.connect()
p_diffs = np.array(p_diffs) $ (p_diffs > real_diff).mean()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
print('Slope: Efthymiou vs experiment: {:0.2f}'.format(popt_axial_chord_crown[2][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_crown[2]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
print sqlContext.sql(query).toPandas()
pandas.concat(sheets).info()
new_page_converted = df_treatment_and_new_page.sample(no_of_samples_treatment) $ p_new = new_page_converted.converted.mean() $ p_new
Date1yrago=dt.date.today()-dt.timedelta(days=365) $ print(Date1yrago) $
SST=((y.mean() - test.readingScore)**2).sum() $ print SST
log_mod = sm.Logit(df2['converted'],df2[['ab_page','intercept']])
data2.index = list(range(len(data2.index)))
sorted(json_dict, key = lambda x: x[0]) $ (result,date1,date2) = max([(abs(u[4]-v[4]),u[0],v[0]) for u,v in zip(json_dict,json_dict[1:])]) $ print('The largest change between any two dates {} and {} is {:.2f}'.format(date1,date2,result)) $
clintondf = pd.DataFrame(HC,columns = ['datetime', 'id', 'text', 'source'])
print(r.json())
pd.Series(0, index=days)
temp_series.plot(label="Period: 1 hour") $ temp_series_freq_15min.plot(label="Period: 15 minutes") $ plt.legend() $ plt.show()
train.head(5)
print('Coefficients: \n', regr.coef_) $ print('Intercept: \n', regr.intercept_)
df_archive["source"].value_counts()
slack = Slacker(slack_api_token) $ channels = slack.channels.list() $ for channel in channels.body['channels']: $     print(f'Channel {channel["name"]} Purpose: {channel["purpose"]["value"]}')
n_old = df2[(df2['group'] == 'control')].shape[0] $ n_old
reddit_comments_data.groupBy('author').agg({'subjectivity':'mean'}).orderBy('avg(sentiment)', ascending = False).show()
to_be_predicted_Day2 = 26.69068092 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
m4 = m3.flatten()       #converting the array in 1-D $ m5 = np.sort(m4, axis = None)[::-1]      #arranging in descending order $ sorted_m3 = np.reshape(m5, (2, 4))      #reshaping it to 2-D $ print("sorted m3: ", sorted_m3)
inspector = inspect(engine) $ inspector.get_table_names()
tem=list(mars_facts[0].values()) $ tem
melted[(melted.Date=='2012-01-03') & (melted.Symbol=='MSFT')]
df_new.groupby(['converted', 'country'])['day_of_week'].value_counts()
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1 - p_old), p_old]) $ old_page_converted.mean()
util.get_rebalance_date(start_idx, end_idx, 'month')
non_na_df.groupby('dayofweek').count()['sender_qq'].plot(kind='bar') $ plt.tight_layout() $ plt.title('Activity by Day of Week') $ plt.ylabel('Total Chat Count') $ plt.xlabel('Day of Week')
learn.lr_find(lrs/1000) $ learn.sched.plot()
df_regression['intercept']=1 $ df_regression[['drop', 'ab_page']] = pd.get_dummies(df_regression['group']) $ df_regression.drop(['drop'], axis=1, inplace=True) $ df_regression.head()
popCon.sort_values(by='counts', ascending=False).head(9).plot(kind='bar')
print(autos['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).sort_index()) $ print(autos['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).shape)
np.sum(all_data['price_change'].isnull().values)
tweets_master_df.to_csv('twitter_archive_master.csv', header=True)
!head -n 10 mbox.txt
X = pd.get_dummies(X, drop_first = True) $ X.head()
yc_new4.describe()
lmscore.summary2()
temp = df.groupby('cust_id').cust_id.count() $ temp = pd.DataFrame(temp) $ temp.columns.values[0] = 'count' $ temp = temp1.sort_values(by=['count'], ascending=False) $ temp.head()
data_sets = {} $ data_sets['1min'] = pd.read_pickle('patched_1min.pickle') $ data_sets['3min'] = pd.read_pickle('patched_3min.pickle')
plt.figure(figsize = (7,7)) $ plt.scatter(x_9d[:,0],x_9d[:,1], c='goldenrod',alpha=0.5) $ plt.ylim(-10,30) $ plt.show()
bad_df = df.index.isin([5,12,23,56]) $ df[~bad_df].head()
to_be_predicted_Day4 = 55.27537048 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
dataframe.head()
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
parser.HHParser.regexp.pattern
dfNYC.head()
datetime.now().toordinal()/365 $
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf], $       labels=["bad", "ok", "yeah!"]).sample(10)
typesub2017.isnull().sum()
weights = {"alice": 68, "bob": 83, "colin": 86, "darwin": 68} $ s3 = pd.Series(weights) $ s3
logit3 = sm.Logit(df_new['converted'], df_new[['ab_page', 'UK', 'US', 'intercept']]) $ result3 = logit3.fit() $ result3.summary2()
top_brands = (autos["brand"].value_counts() $               .sort_values(ascending=False).head(6).index) $ print(top_brands)
num_new_user = len(df2[df2['group'] == 'treatment']) $ prob_new_user = num_new_user/(df2.shape[0]) $ print("The probability of an individual receive the new page is - {}".format(prob_new_user))
winter = df[df.index.month.isin([12, 1, 2])]
(test.groupby(by='Page').mean() - train.groupby(by='Page').mean()).plot(figsize = (15,5))
df.CATS_Counter.head()
control_converted = df2[df2['group'] == 'control']['converted'].mean() $ control_converted
es_dictindex = "{0}_dictionary".format(city.lower()) $ es_dicttype = 'dictionary' $ es.createOrReplaceIndex(es_dictindex)
arma_mod50 = sm.tsa.ARMA(dta_713, (5,0)).fit(disp=False) $ print(arma_mod50.params)
cityID = '3877d6c867447819' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Wayne.append(tweet) 
sns.distplot(utility_patents_subset_df.prosecution_period, color="orange") $ plt.show()
print("The tweet with most likes is: \n{}".format(data['Tweets'][likes])) $ print("Number of likes: {}".format(likes_max)) $ print("{} characters.".format(data['len'][likes])) $ print("Tweeted at {s}".format(s=data['Date'][likes]))
from h2o.estimators.gbm import H2OGradientBoostingEstimator $ gbm_model = H2OGradientBoostingEstimator(model_id="GBM", distribution = 'bernoulli') $ gbm_model.train(x = predictor_columns, y = target, training_frame = train, validation_frame = valid)
oecd_site = 'http://www.oecd.org/about/membersandpartners/list-oecd-member-countries.htm' $ pd.read_html(oecd_site)
CryptoComm.head()
members_df_all['members_age'] =  [(2018 - x.year) for x in pd.to_datetime(members_df_all['members_date_of_birth'])]
leadConvpct.tail()
df_sched.iloc[:,1:] = df_sched.iloc[:,1:].apply(lambda x: x.str[:10])
mydata[mydata.isnull().any(axis=1)]
print "Mean time for closing a ticket in 2013: %f hours" % (time2close_2013.mean()/3600.0) $ print "Median time for closing a ticket in 2013: %f hours" % (time2close_2013.median()/3600.0) $ print "Quantiles: " $ print time2close_2013.quantile([0.25, 0.5, 0.75])
real_diff = df2.query('group == "control"').converted.sum()/df2.query('group == "control"')['user_id'].count() - df2.query('group == "treatment"').converted.sum()/df2.query('group == "treatment"').user_id.nunique() $ real_diff
first_commit_timestamp = pd.to_datetime('2005-04-16 22:20:36') $ last_commit_timestamp = pd.to_datetime('today') $ corrected_log = git_log[(git_log['timestamp'] >= first_commit_timestamp) & $                         (git_log['timestamp'] <= last_commit_timestamp)] $ corrected_log['timestamp'].describe()
autos["num_photos"].value_counts()
twitter_archive_master = pd.read_csv('twitter_archive_master.csv')
mfp_boss.build_prior()
_, pval_2_sided = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='two-sided') $ pval_2_sided
demo.plot_weights(l,x_test)
x_axis = output['user.followers_count']
df_ad_airings_5['state'].value_counts()
views_data = pd.read_json('data/pdpviews.ndjson', lines=True, chunksize=1000000) $ views_data_clean = pd.DataFrame([],columns=['browser_id', 'product_id', 'timestamp']) $ for views_chunk in views_data: $     views_chunk = views_chunk.drop(['price','source','user_id'],axis=1) $     views_data_clean = views_data_clean.append(views_chunk.loc[views_chunk.browser_id.isin(unique_buying_browsers)])
from sklearn.feature_extraction.text import CountVectorizer $ vectorizer = CountVectorizer(stop_words='english') $ X = vectorizer.fit_transform(tmp)
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ vow = pd.read_csv('./datasets/vow.csv') $ vow.head()
sentence="Nice and friendly place with excellent food and friendly and helpful staff. You need a car though. The children wants to go back! Playground and animals entertained them and they felt like at home. I also recommend the dinner! Great value for the price!" $ sentiment_score(sentence)
intervention_train.columns
results.params
to_be_predicted_Day5 = 21.38790489 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
students.columns
reviews.region_1.fillna('Unknown').value_counts()
sns.violinplot(x="embark_town", y="age", hue="sex", data=titanic, split=True)
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
discGrouped = discovery.groupby(['lead_source', 'opportunity_stage']).opportunity_stage.count() $ discConvpct = discGrouped.groupby(level=[0]).apply(lambda x: 100* x / float(x.sum())); 
autos["registration_year"].value_counts().sort_index().head(20)
print(stock.head()) $ print(maxi.head())
style.available
n_new=(df2['landing_page']=='new_page').sum() $ n_new
run txt2pdf.py -o"2018-06-14 2148 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by Discharges.pdf"  "2018-06-14 2148 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by Discharges.txt"
print(autos['odometer_km'].unique().shape) $ print(autos['odometer_km'].describe()) $ print(autos['odometer_km'].value_counts())
exceldmh = dmh.PandasDataFrame(exceldf)
fraud_data_updated = fraud_data_updated.drop(["user_id",'device_id','ip_address'],axis=1) $
model.wv.syn0.shape
df3 = make_df('AB', [0, 1]) $ df4 = make_df('CD', [0, 1]) $ display('df3', 'df4', "pd.concat([df3, df4], axis=1)")
from pandas_datareader.famafrench import get_available_datasets $ import pandas_datareader.data as web $ len(get_available_datasets()) $
P = 2*numpy.pi/freqs
conn.execute(sql) $ conn.execute(sql)
df_protest.columns = df_protest.columns.str.lower()
lr_cv.fit(X_train, y_train)
repos_ids = pd.read_sql('SELECT DISTINCT(repo_id) AS repos_ids FROM repos;', con)
pres_df.shape
sns.set() $ sns.distplot(np.log(df_input_clean.toPandas().Resp_time), kde=True, color='b')
df.dropna(axis = 0, inplace = True) $ df.reset_index(inplace=True, drop=True) $ df.shape
from scripts.model import *
new_page_converted=np.random.binomial(n_new,p_new)
'Despite traditionally high dividends for oil sector investors, current markets require shareholders to review corporate financial records to determine if a company has the resources for payouts, according to Marco Scherer of Deutsche Asset Management.' $ 'Superbly productive oilfields are a major pre-requisite for investments in the upstream U.S. oil sector, Scheder added. The consolidation of oilfield services providers has led to the emergence of several well-diversified multinational OFS companies that will be more resilient to oil price changes in the future, he noted.' $ 'As drilling activity dried up following the oil price crash of 2014, OFS companies had no choice but to slash their prices, charging much less for rigs, equipment, and services.' $ 'Now, drilling in the U.S. is coming back quickly, shifting leverage back in favor of OFS companies, who are starting to hike their prices. According to S&P Global Platts, services costs are expected to rise by about 20 percent on average this year. That could offset some of the efficiency gains that shale drillers are accruing as they improve their drilling techniques.')
daily_sales.plot('date','dcoilwtico',kind='bar', color='r', figsize=(15,5))
link_pattern = '\[[\w ]*\]\([\w+]*[://]*[w]*[/.][\w]*[/.][/\w-]*\)'
daily_ret_b_mean = daily_ret_b.mean() $ daily_ret_b_mean
train_corpus = [" ".join([t for t in reuters.words(train_doc[t])]) $                 for t in range(len(train_doc))] $ print("train_corpus is created, the first line is: {} ...".format(train_corpus[0][:100]))
top_songs = top_songs.dropna(axis=0, how='any')
df2['c2_bin'] = df['c2_date'].map(lambda x: 1 if pd.isnull(x) == False else 0)
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True, ascending=False).head(20)
df[['text', 'favorite_count', 'date']][df.favorite_count == np.min(df.favorite_count)]
grouped.sort_index(by='Frequency').tail()
df = ek.get_timeseries(["MSFT.O"], $                        start_date="2016-01-01",  $                        end_date="2016-01-10") $ df
%time lr = lr.fit(train_4, y_train)
data.groupby(['Date received', 'State'])['Company'].agg(['count']).pivot_table( $     'count', index='Date received', columns='State', fill_value=0).resample('M').sum().plot(y=['CA', 'FL','TX','NY','IL'], legend=False);
import engarde.decorators as ed
pd.DataFrame(d, index=['apple', 'clock'], columns=['two', 'five'])
inspector = inspect(engine) $ inspector.get_table_names() $
df[df['RT']==False][['op','text','time','rtcount']].sort_values(by='rtcount',ascending=False).head()
autos['odometer'].head(10)
df.to_csv('alexa_clean.csv')
print 'Pandas will unexpectedly throw an errorif it can\'t parse a value in to_datetime()' $ print 'To avoid that, set errors = \'coerce\'' $ pd.to_datetime(['Aug 1, 2014', 'foo'], errors='coerce')
df['Complaint Type'].groupby(by=df.index.month.isin[6,7,8]).counts()
old_page_converted = np.random.binomial(1, conv_mean, 145274)
dta.head(5)
id_range=range(results.index.values.min(),results.index.values.max()) $ results.reindex(id_range).head() $
df.groupby("one_day_reminder_sent")["cancelled"].mean()
df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']] $ print(df.head())
seq2seq_model.summary()
s.ix[2:5].mean()
learner.save('lm1')
now = datetime.datetime.utcnow().isoformat() + 'Z' $ CONFERENCE_START = '2018-07-06' $ LEVEL_LIST = ['pydata_novice', 'pydata_intermediate', 'pydata_experienced']
sqlContext.sql("select person,count from pcs where count >50 order by count desc").show()
actual_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ (p_diffs > actual_diff).mean()
keywords_qs = pd.read_excel( $     '../data/data_keywords_quality_score.xlsx' $ ) $ keywords_qs
cust_data1=cust_data.assign(No_of_30_Plus_DPD=cust_data.No_of_30_59_DPD+cust_data.No_of_60_89_DPD+cust_data.No_of_90_DPD, $                            MonthlySavings=cust_data.MonthlyIncome*0.15)
col='Case.Status' $ tmp_df.loc[tmp_df[col]=='null',col] = float('nan')
autos.price.value_counts().sort_index(ascending=True).head(20)
my_gempro.kegg_mapping_and_metadata(kegg_organism_code='mtu') $ print('Missing KEGG mapping: ', my_gempro.missing_kegg_mapping) $ my_gempro.df_kegg_metadata.head()
df_subset.info()
final_df_test = dt_features_test.join(tree_c_features_test)
try: $     alldata.to_csv(csv_name, sep=',') $     print('CSV created') $ except: $     print('error creating CSV')
tweet_list = [] $ for line in f: $     tweet_list.append(line.split('\t'))    
df2[df2.user_id.duplicated(keep=False)]
prop_new_page = df2[df2['landing_page'] == 'new_page'].shape[0] / df2.shape[0] $ prop_new_page
train = K.function(inputs=[x, target], outputs=[loss], updates=updates)
del merged_portfolio_sp['Date_y'] $ merged_portfolio_sp.rename(columns={'Date_x': 'Latest Date', 'Adj Close_x': 'Ticker Adj Close' $                                     , 'Adj Close_y': 'SP 500 Initial Close'}, inplace=True) $ merged_portfolio_sp.head()
sns.pairplot(segmentData, diag_kind="kde", hue="opportunity_stage");
p_old=df2[df2['converted']==1].shape[0]/df2.shape[0] $ p_old
is_wrong_timestamp = blame.timestamp < initial_commit $ wrong_timestamps = blame[is_wrong_timestamp] $ blame.timestamp = blame.timestamp.clip(initial_commit) $ len(wrong_timestamps)
multi_counts = [(w, word_counts[w]) for w in word_counts if word_counts[w] > 0.05] $ multi_counts = sorted(multi_counts, key = lambda w: w[1], reverse = True) $ multi_counts
possible_bots = users.query("created_at > '2018-04-26'") $ possible_bots.shape
a = pd.read_sql_query(q, conn) $ a $
td + pd.tseries.offsets.Minute(15)
assert len([col for col in cols_to_drop if col in twitter_archive_clean.columns])==0
url = "http://www.hotashtanga.com/p/letoltesek-downloads.html" $ html_txt = urllib.request.urlopen(url).read() $ dom =  lxml.html.fromstring(html_txt) $ for link in dom.xpath('//a/@href'): # select the url in href for all a tags(links) $     print(link)
merged_NNN.sort_values("amount", ascending=False)
sns.set() $ sns.distplot(df_input_clean.toPandas().Resp_time, kde=True, color='b')
cells_file = 'network/recurrent_network/node_types.csv' $ rates_file = configure['output']['rates_file'] $ plot_rates_popnet(cells_file,rates_file,model_keys='pop_name')
f1 =(df['year'] >= 2000) $ f2 = (df['teamid'].isin(['BOS','NYA'])) $ interesting_teams = teams_sorted_by_wins[f1 & f2] $ interesting_teams.head(20)
y.head()
tfav2 = tfav.values $ SA2 = SA1 * 100000
for col in full.columns: $     print(full[col].value_counts())
p = pd.Period('2012-01', freq='2M')
mini_csv_data_df = non_blocking_df_save_or_load_csv( $     mini_csv_data_df, $     "{0}/apache_people.csv".format(fs_prefix))
with open("../tweets.json") as f: $     tweets = json.load(f) $ tweets[:2]
column = inspector.get_columns('station') $ for c in column: $     print(c['name'], c["type"])
predictions.select("label", "prediction", "probability").show() $
print("Number of Relationships in Enterprise ATT&CK") $ print(len(all_enterprise['relationships'])) $ df = all_enterprise['relationships'] $ df = json_normalize(df) $ df.reindex(['id','relationship', 'source_object', 'target_object'], axis=1)[0:5]
df_res = sqlContext.createDataFrame(aaa)
Measurements = Base.classes.hawaii_measurement $ Stations = Base.classes.hawaii_station $
fg = sns.FacetGrid(data=motion_at_home_df[motion_at_home_df['is_weekday']==0], hue='time_category', aspect=5)  # $ fg.map(plt.scatter, 'time', 'state').add_legend()
pd.crosstab(data.rate_marriage, data.affair.astype(bool)).plot(kind='bar') $ plt.title('Marriage Rating Distribution by Affair Status') $ plt.xlabel('Marriage Rating') $ plt.ylabel('Frequency') $ plt.show()
kick_projects.loc[:,'goal_reached'] = kick_projects['pledged'] / kick_projects['goal'] # Pledged amount as a percentage of goal. $ kick_projects.loc[kick_projects['backers'] == 0, 'backers'] = 1 $ kick_projects.loc[:,'pledge_per_backer'] = kick_projects['pledged'] / kick_projects['backers'] # Pledged amount per backer.
unique_speaker_id = speeches_cleaned['speaker_bioguide'].unique() $ print(len(unique_speaker_id))
pd.set_option('display.max_colwidth',120) $ pd.set_option('display.max_rows',300) $ df[ ( df["author"] == 'Jeff Vb') & ( df["text"].str.lower().str.contains("desi") | (df["text"].str.lower().str.contains("desy"))  ) ]
import re $ pbptweets.loc[pbptweets['text'].apply(lambda x: any(re.findall('Santos',x)))][['date','screen_name','text']]
%run '../additional_functions.ipynb'
pd.DataFrame([m.params for m in models], index=model_dates).T
df.groupby('episode_id')['id'].nunique().agg(['min', 'mean', 'max'])
import gc $ del full_data $ gc.collect()
grid_id_flat = grid_id_array.flatten()
obs_diff = (sum((df2.group == 'treatment')&(df2.converted == 1)) / sum(df2.group == 'treatment')) - (sum((df2.group == 'control')&(df2.converted == 1)) / sum(df2.group == 'control')) $ obs_diff
df_weather = df_weather[['STATION_NAME','DATE','HOURLYVISIBILITY','HOURLYDRYBULBTEMPC','HOURLYWindSpeed','HOURLYPrecip']].copy()
ek.get_news_headlines('R:TD.TO', date_from='2017-10-19T09:00:00', date_to='2017-10-20T18:00:00')
salesdec['Standard_Plat'] = salesdec['Platform'] $ salesdec['Standard_Plat'].isnull().any() $
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.contains('^[(a-z)]'))].value_counts()
rain = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date > last_year).\ $     order_by(Measurement.date).all() $ print(rain)
df_clean3.loc[979, 'text']
all_test_times_dates.head()
daily_cases.unstack().T.fillna(0).cumsum().plot()
journeys_scored.loc[anomaly_condition2, ['Journey_ID', 'timestamp', $                                          'anomaly_ma', 'anomaly_lof', 'anomaly_svm', 'anomaly_if', $                                         'anomaly_ma2', 'anomaly_lof2']]
precip_df.rename(columns={'prcp': 'precipitation'}, inplace=True) $ precip_df.set_index('date', inplace=True) $ precip_df.head()
Z = np.random.randint(0,10,(3,3)) $ print(Z) $ print(Z[Z[:,1].argsort()])
print(len(a.intersection(b)))
p_diffs = np.array(p_diffs) $ (p_diffs > diff1).mean() $
model = pd.get_dummies(auto_new.CarModel) $ model = model.ix[:, ["ToyotaCamry", "ToyotaCorolla","ToyotaRav4", "ToyotaLandCruiserPrado", "ToyotaIpsum", "ToyotaSienna", "Toyota4-Runner"]] $ model.head()
bb.head()
autos[((autos.price > 500000) & ~(autos.name.str.contains("Ferrari")) )] $ autos=autos[~((autos.price > 500000) & ~(autos.name.str.contains("Ferrari")) )] #antimasking $ autos.shape #stripped off 12 rows
print(len(df_concat)) #Prints number of rows. $ df_concat.head()
from sklearn.preprocessing import LabelEncoder
stack_with_kfold_cv['gb_preds']=gb_best.predict(train[col]) $ stack_with_kfold_cv['knn_preds']=knn_best.predict(train[col]) $ stack_with_kfold_cv['xgb_preds']=xgb_best.predict(train[col]) $ stack_with_kfold_cv['rf_preds']=rf_best.predict(train[col]) $ stack_with_kfold_cv['lgb_preds']=lgb_best.predict(train[col])
piotroski_univ_sets = soup.select('td[class^="number"]') $ piotroski_univ_sets
b_list.head(2)
archive_clean.drop(['in_reply_to_user_id', $                     'in_reply_to_status_id', $                     ], $                   axis=1, inplace=True)
total_students_with_passing_math_score = len(df_students.loc[df_students['math_score'] > 69]) $ total_students_with_passing_math_score
week44 = week43.rename(columns={308:'308'}) $ stocks = stocks.rename(columns={'Week 43':'Week 44','301':'308'}) $ week44 = pd.merge(stocks,week44,on=['308','Tickers']) $ week44.drop_duplicates(subset='Link',inplace=True)
print (r.text[0:500])
regression_model.score(X_test, y_test)
pd.Series(labels, y).value_counts() $
cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name='dot_311'") $
xmlData[xmlData['street'].str.match('^(?:[A-Z]).*$')][['street','city','address']]
train_users_pd.loc[train_users_pd['gender'] == '-unknown-','gender'] = 'UNKNOWN' $ train_users_pd.loc[train_users_pd['gender'] == 'OTHER','gender'] = 'UNKNOWN'
prob_ZeroFill = LogisticModel_ZeroFill.predict_proba(Test) $ prob_ZeroFill
to_be_predicted_Day1 = 54.45 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
jobs_data1 = json_normalize(json_data1['page']) $ jobs_data1.head(5)
ticks = data.ix[:, ['Price', 'Volume']] $ ticks.head()
df_raw_tweet = pd.read_csv('./Datasets/Twitter_Training_Data2.csv', encoding='latin1') $ print (df_raw_tweet.head())
data.date.values
data.head()
tweets_clean.to_csv('twitter_archive_master.csv') $ images_clean.to_csv('images_archive_master.csv')
melted_total.groupby(['Categories','Neighbourhood']).mean().unstack()['Rating'].ix[top10_categories.index].plot.bar(legend=True,figsize=(10, 5))
df=df.sort_values(['store_nbr', 'date'], ascending=[True, False])
yr15 = likes[likes.year == 2015] $ len(yr15)
y_hat_lr = lr.predict(train_4)
with open("Total_Politician_and_Events","wb") as f: $     pickle.dump(TotalNameEvents,f) $
type(git_log.timestamp[0])
fname_messages = 'messages.csv' $ messages = load_messages(fname_messages) # messages data $ messages.head()
new = (df2['landing_page'] == 'new_page').mean() $ new $
df2.query('landing_page=="new_page"').count()[0]/df2.shape[0]
y_pred = log_reg.predict(X_test)
def plot(x, y): $     plt.figure(figsize=(10,3)) $     plt.plot(x, y, marker='o', linewidth=0) $     plt.ylim(-1000,)
db.tweets.count()
df.loc[(df.company == False) & (df.other == False), 'other'] = df[(df.company == False) & $                                                      (df.other == False)].name.isin(other_list)
lr2 = LogisticRegression(solver='saga', multi_class='multinomial', random_state=20, max_iter=1000) $ lr2.fit(X, y)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
cm = confusion_matrix(y_true, y_pred) $ cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] $ plot_confusion_matrix(cm_normalized, labels=y_labels, title='Normalized confusion matrix')
temperature = session.query(measurement.date, measurement.tobs).\ $     filter(measurement.date >= '2016-08-18').\ $     filter(measurement.station == 'USC00519281').\ $     order_by(measurement.date).all() $ temperature
df2 = df.query("(group == 'control' and landing_page == 'old_page') or (group == 'treatment' and landing_page == 'new_page')") $ df2.head(2)
df.max()
s519397_df["prcp"].max()
df.plot.scatter(x='B', y='C', title = 'Scatterplot', color='r')
ABT_tip.shape
clf = OneVsRestClassifier(estimator=linear_model.SGDClassifier(loss='hinge', penalty='l2', $                                                          alpha=0.001, fit_intercept=True, n_iter=10, $                                                          shuffle=True, verbose=1, epsilon=0.1, n_jobs=-1, $                                                          random_state=SVM_SEED, learning_rate='optimal', eta0=0.0, $                                                          class_weight=None, warm_start=False), n_jobs=1) $
p_diff_mean = p_diffs.mean()
tz_tmp = timezone_df.groupby('zone_id').agg({'gmt_hour':{'gmt_hour_avg':'mean'}}) $ tz_tmp.columns = tz_tmp.columns.get_level_values(1) $ tz_tmp.reset_index(inplace=True)
predictions.show()
N_old = df2.query("landing_page == 'old_page'")["user_id"].count() $ print("The dataset consists of {} old pages.".format(N_old))
autos["brand"].unique().shape
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])
sel=[Measurement.date, $      Measurement.tobs] $ day_prcp=session.query(*sel).filter((Measurement.date>year_ago)).all() $
to_be_predicted_Day1 = 47.75 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
blocks.head()
max((stock - mini).abs())
np.sum(ttt[ttt>1])*1./len(ttt)
with twitter_file as f: $     first_line = f.readline() $     print (first_line) $     second_line = f.readline() $     print(second_line)
data.columns = data.columns.str.replace('_-_', '_') $ data.columns[data.columns != data.columns.str.extract(r'^(\w+)$')] 
festivals = festivals[['Date', 'Location', 'latitude', 'longitude', 'Website']] $ list(festivals.columns.values)
df2[df2['user_id'].duplicated(keep=False)]
dt_features['launched_at'] = pd.to_datetime(dt_features['launched_at'],unit='s')
merged = pd.merge(prop, contribs, on="calaccess_committee_id")
words_hash_sk = [term for term in words_sk if term.startswith('#')] $ corpus_tweets_streamed_keyword.append(('hashtags', len(words_hash_sk))) # update corpus comparison $ print('List and total number of hashtags: ', len(words_hash_sk)) #, set(terms_hash_stream))
plt.style.use('ggplot') $
autos = autos[autos['odometer_km'].between(70000, 150000)] $ autos['odometer_km'].hist()
df_ids.to_csv('follower_id.csv')
daily_df = all_turnstiles.groupby(['STATION','C/A','UNIT','SCP','DATE'], as_index=False).sum() $ daily_df.head()
data['cat'].shape
new_page_converted = np.random.choice([0,1] ,size = n_new, p=[(1-p_new),p_new])
reconstruction_targets = tf.cond(mask_with_labels, # condition $                                  lambda: y,        # if True $                                  lambda: y_pred,   # if False $                                  name="reconstruction_targets")
proportion = df[df['converted']==1].user_id.nunique()/number_rows $ print('The proportion od users converted is {}'.format(proportion))
df = gpd.GeoDataFrame(df)
with open(os.path.expanduser('~/.secrets/twitter_thebestcolor.yaml')) as f: $     creds =  yaml.load(f)
data_final.head()
df2.info()
active_stations = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                order_by(func.count(Measurement.tobs).desc()).all() $ busiest_station = active_stations[0][0]    $ print("Most active station: ",busiest_station) $ active_stations
r.json()['dataset_data']['column_names']
cfs_df = pd.read_csv(path+"crime_beat.csv")
df_new['intercept'] = 1 $ df_new[['control','ab_page']] = pd.get_dummies(df_new['group']) $ df_new = df_new.drop('control', axis=1) $ df_new.head() $ df_new[['CA','UK','US']] = pd.get_dummies(df_new['country'])
lb = articles['tokens'].map(len).quantile(.025) $ ub = articles['tokens'].map(len).quantile(.975) $ articles = articles.loc[(articles['tokens'].map(len)>lb) & (articles['tokens'].map(len)<ub),:]
plt.plot(spks[:, 0], spks[:, 1], '.k') $ plt.xlim(0, config_file['run']['tstop']) $ plt.ylim(-1, 7.5) $ plt.xlabel('Time (ms)') $ plt.ylabel('Neuron Number')
loan_stats["loan_status"].table()
x_normalized = pd.DataFrame(x_scaled)
df2.nunique()
join_a.count()
from sklearn.feature_selection import RFE $ rfe = RFE(lm,10)
plt.figure(figsize=(16,10)) $ plt.clf() $ sns.boxplot(x='Journal',y='PubDays',hue='Publisher',data=df) $ plt.xticks(rotation=90)
NYG_analysis2 = NYG_analysis["Per Seat Price"].mean()
click_condition_meta['geo_country'] = np.where((pd.isnull(click_condition_meta['geo_country'])), $                                                'NG', click_condition_meta.geo_country)
df2.query("group == 'treatment'")['converted'].mean()
trn_df.dtypes
to_be_predicted_Day3 = 22.34206034 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
tweet_archive_enhanced_clean['tweet_id'] =tweet_archive_enhanced_clean['tweet_id'].astype(str) $
log_model.coef_
merged = merged.rename(columns={'comment_name': 'comment_author'})
iris.head().iloc[:,:1]
Log_model.fit().summary()
ayush = relevant_data[relevant_data['User Name'] == 'AYUSH JAIN'] $ ayush['Event Type Name'].value_counts().plot(kind='barh')
df_prep3 = df_prep(df3) $ df_prep3_ = pd.DataFrame({'date':df_prep3.index, 'values':df_prep3.values}, index=pd.to_datetime(df_prep3.index))
coins = get_coin_list() $ COIN_DB = pd.DataFrame.from_dict(coins, orient='index') $ print(COIN_DB.head())
Y_mat = Y_df1.interest.as_matrix() $ Y_mat[:5]
tobs_info = (session.query(Measurement.date, Measurement.tobs).\ $     filter(Measurement.station == 'USC00519281').\ $     filter(Measurement.date >= '2016-08-18').all()) $ tobs_info
svm_tunned.best_params_
ps.to_timestamp('D', how='end')
na_df.isna() # check elements that are missing 
df.describe()
events[["doc_type", "status"]].head()
df_R.tail()
s1.values
y = df_cond['lead_result_max_bucket'] $ y = y.replace('max_below_5', 0).replace('max_5_plus', 1).replace('max_15_plus', 1)
bremen.info()
df2 [['control', 'ab_page']]= pd.get_dummies(df2['group']) $ df2.head()
df2.query('landing_page == "old_page"').count().user_id
fb_vec = cv.fit_transform([ftfy.ftfy(fb_cleaned)])
y_know.shape
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("P of conversion of new page(p_new):", p_new)
fashion.shape
np.exp(	-0.0140), 1/np.exp(	-0.0140)
df = df.set_index('user_id').join(df2.set_index('user_id'));
output= "Delete from user where user_id='@Pratik'" $ cursor.execute(output) $
omdb_df.info()
plt.figure(figsize=[14,6]) $ plt.plot(actual1[:200],'bo') $ plt.plot(benchmark21[:200],'r+') $ plt.plot(pred1[:200],'y*')
not_in_misk = not_in_misk.copy().sort_values('created_at')
df = pd.DataFrame(np.random.randn(4, 3), columns=['d', 'd', 'e'])
sample_single_contrl = control_group.sample(size_control, replace = True) $ old_page_converted = (sample_single_contrl['converted'] == 1).sum() / sample_single_contrl.user_id.count() $ old_page_converted
by_day_by_hour14 = uber_14.groupby(["day_of_week", "hour_of_day"]).size() $ by_day_by_hour14.head()
dataset.hist()
num_of_dtmodel_pred.keys()
graf_train=pd.concat([graf_train, train_embedding], axis=1)
small_frame.rbind(small_frame)
from pyspark import SparkContext $ from pyspark.streaming import StreamingContext $ import json $ import time $ from datetime import datetime
pd.merge(df1, def2, ...) $ pd.concat([df1, df2, df3, ...])
new_page_converted = np.random.choice([1, 0], size = n_new, p = [p_mean, (1-p_mean)], replace = True)
df.loc['a']>0
sia = SIA() $ sentiment = stock['news_text'].apply(sia.polarity_scores) $ sent = pd.DataFrame(list(sentiment)) $ sent.index = stock.index
%timeit pd.to_numeric(df_census.loc[:, "ward"])
news_dict_df.to_json("Newschannel_tweets_df.json")
tweet.author
df.to_csv('ready for classification')
bruins.head()
token_entity_type = [token.ent_type_ for token in parsed_review] $ token_entity_iob = [token.ent_iob_ for token in parsed_review] $ pd.DataFrame(zip(token_text, token_entity_type, token_entity_iob), $              columns=['token_text', 'entity_type', 'inside_outside_begin'])
np.eye(3, dtype='float64')  #Identity matrix. float64 is the default dtype and can be omitted
sum(clean_rates.text.str.contains('\n'))
gmm2 = mixture.GMM( n_components=num_comps ) $ fit2 = gmm2.fit_predict(X) $ mdf['comp2'] = fit2 $ gmm2df = mdf.copy() $ print 'BIC for 2-component mix:', gmm2.bic(X)
words = bow_converter.get_feature_names() $ len(words)
(df2['landing_page']== 'new_page').mean()
visitors.head(5)
df[df.isna()].count()
print('Scatter plot distribution of the error') $ sb.regplot(y_pred, errors, fit_reg=False) $ plt.xlabel('y_pred') $ plt.ylabel('errors')
df2 = pd.read_csv('df_providers1.csv' )
autos["registration_year"].describe()
stock_list = df_release['ticker'].unique() #plan to loop for each ticker, fiscal_year and fiscal_quarter $ print(stock_list) $ print(len(stock_list)) $ df_release_pivot = df_release.set_index(['ticker', 'fiscal_year', 'fiscal_quarter']).sort_index() $ df_release_pivot
average_polarity=pd.concat([average_polarity_2012,average_polarity_2013,average_polarity_2014 $                            ,average_polarity_2015,average_polarity_2016], axis=1)
y_test_over[rfc_bet_over].mean()
mars_weather = current_weather_info[0].text $ mars_weather
yc_sd.columns
print("Number of Relationships in Mobile ATT&CK") $ print(len(all_mobile['relationships'])) $ df = all_mobile['relationships'] $ df = json_normalize(df) $ df.reindex(['object id','relationship', 'relationship_description','source_object', 'target_object'], axis=1)[0:5]
customer_visitors_new.index.levels[1]
pred_probas_over_fm = gs_from_model.predict_proba(X_test) $ fm_bet_over = [x[1] > .62 for x in pred_probas_over_fm]
yelp_class.shape
print('Unique number of users notified: {}'.format(len(atloc_opp_dist['vendorId'].unique())))
data_activ['n_date'] = data_activ['created_date'].apply(lambda x:datetime.strptime(x,'%Y-%m-%d %H:%M:%S')if not pd.isnull(x) else '') $ data_activ['new_date']= data_activ['n_date'].apply(lambda x: x.strftime('%Y/%B')if not pd.isnull(x) else '') $ data_activ['new_date_daily'] = data_activ['n_date'].apply(lambda x:x.strftime('%Y/%B/%d')if not pd.isnull(x) else '')
combined_df.head(2)
n_old = df2.query('landing_page == "old_page"')['landing_page'].count() $ n_old
(autos['odometer_km'] $  .value_counts() $  .head() $  .sort_index(ascending=False))
lm_2 = sm.Logit(new_df2['converted'], new_df2[['intercept', 'Middle', 'End', 'ab_page']]) $ reg_lm2 = lm_2.fit() $ reg_lm2.summary()
merged_index_year = merged2.index.year
pd.isnull(doglist)
for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_L).items(maxitems): $     detected = detect(tweet.text) $     conn.commit()
BAL = pd.read_excel(url_BAL, $                     skiprows = 8) # This reads in each team's respective spreadsheet, ignoring the header.
highlandpark_potholes = data_311[potholes & highland_park] $ print(highlandpark_potholes.shape) $ highlandpark_potholes.head()
ts.resample('5Min').sum()
df.groupby("cancelled")["ignore_availability_requirements"].mean()
random.randrange(0, 101, 5)
mentions_count = mentions.value_counts().head(10) $ mentions_count
print("At the moment, we have %d entries with NewsDesk=Nan." % len(joined.loc[joined['NewsDesk'].isnull()]))
job_requirements.ndim $
events.schema
import numpy as np $ preTest = [4, 4, 31, 2, 3], $ postTest =  [25, 25, 57, 62, 70] $ print np.mean(preTest) $ print np.mean(postTest)
logit_ctries = sm.Logit(df_final['converted'], $                            df_final[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ result_ctries = logit_ctries.fit() $ result_ctries.summary()
linear.delete_endpoint()
news.date.dt.date.value_counts().plot(linewidth=0.5, figsize=(16, 5), title='Number of news per day');
dfPre = df.loc['1930-01-01':'1979-12-31'] $ dfPost = df.loc['1984-01-01':'2017-12-31']
day = lambda x: datetime.date(int(x[0]),int(x[1]),int(x[2])).weekday() $ emotion_big_df['date']=emotion_big_df['date'].apply(day)
nba_df.head(10)
print("Unique users:", len(df2.user_id.unique())) $ print("Non-unique users:", len(df2)-len(df2.user_id.unique()))
avg_per_seat_price_seasonsandteams["2012 Season", "BAL"] # This is the average price of PSLs before BAL won the SuperBowl.
actual_payments.iso_date.unique()
df.head()
twitter_df_clean.drop(twitter_df_clean[twitter_df_clean.retweeted_status_id.notnull()].index, inplace=True)
offseason16 = ALL[(ALL.index > '2016-02-07') & (ALL.index < '2016-09-08')]
a400hz = hc.table('asm_wspace.analoog_400hz_2017q4') \ $ .where("t between '{0}' and '{1}'".format(start_date, end_date)) \ $ .persist()
bb_df.isnull().any()
hr["new charttime"] = hr["charttime"] - time_delta $ hr["new realtime"] = hr["realtime"] - time_delta $ bp["new charttime"] = bp["charttime"] - time_delta $ bp["new realtime"] = bp["realtime"] - time_delta $ bp.head()
damd['created'].groupby(by=damd['created'].dt.year).count().plot.bar(figsize=(5, 6), title="Tweet activity over years").grid(True, axis="y")
df_test.dtypes
df_userIds = pd.DataFrame({"UserID":user_ids,"Key":1}) $ df_productIds = pd.DataFrame({"ProductID":product_ids,"Key":1}) $
dule2.drop(columns=['ElectronicCollection'],inplace=True) $ dule2
results_df.plot(kind='area', stacked=False, x_compat=True, alpha=.2) $ plt.show()
dfU = df.drop_duplicates('user_id') $ dfU.info()
df2 = pd.read_csv('ab_new.csv')
day_of_week15.to_excel(writer, index=True, sheet_name="2015")
dfMonth = dfDay.copy(deep=True)
type(rng.asi8)
df_new['country_CA'] = df_new['country'].replace(('US', 'UK', 'CA'), (0, 0, 1)) $ lm_ca = sm.OLS(df_new['converted'], df_new[['intercept', 'country_CA']]) $ results_ca = lm_ca.fit() $ results_ca.summary()
n_old = len(df2.query('landing_page=="old_page"')) $ n_old
archive_df_clean = archive_df_clean.drop(columns=['expanded_urls', 'in_reply_to_status_id', 'in_reply_to_user_id', 'text', 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp', 'source'])
train_users = pd.read_csv("../data/airbnb/train_users_2.csv")
def load_file(file_name): $     with codecs.open(file_name, 'rb', 'utf8') as infile: $         text = infile.read() $     return text
print(sp.pi) $ print('{:.2f}'.format(sp.pi)) $ print('{:.4f}'.format(sp.pi)) $ print('{:^12.2f}'.format(sp.pi))
filtered_df['review_scores_rating'].describe()
all_weekdays = pd.date_range(start=start, end=end, freq='B')
model.most_similar([trial])
ts.asfreq(pd.tseries.offsets.BDay())
ux.html_utils.get_webpage_meta(urls[0])
classifier = LogisticRegression(random_state=0) $ classifier.fit(X_train,y_train)
df_mod.head()
g = mixture.GMM(n_components=3) $ g.fit(cluster)
purchases['Hear About'].value_counts()
for col in missing_info: $     percent_missing = data[data[col].isnull() == True].shape[0] / data.shape[0] $     print('percent missing for column {}: {}'.format(col, percent_missing))
df[df.isnull().any(axis=1)]
end_date = [USER_PLANS_df.loc[cid,'canceled_at'][np.argmax(USER_PLANS_df.loc[cid,'scns_created'])] if USER_PLANS_df.loc[cid,'cancel_at_period_end'][np.argmax(USER_PLANS_df.loc[cid,'scns_created'])] == False else USER_PLANS_df.loc[cid,'current_period_end'][np.argmax(USER_PLANS_df.loc[cid,'scns_created'])] if USER_PLANS_df.loc[cid,'cancel_at_period_end'][np.argmax(USER_PLANS_df.loc[cid,'scns_created'])]==True else None for cid in churned_ix]
access_token = os.environ.get('access_token', 'Not Set') $ access_token_secret = os.environ.get('access_token_secret', 'Not Set') $ consumer_key = os.environ.get('consumer_key', 'Not Set') $ consumer_secret = os.environ.get('consumer_secret', 'Not Set')
aggreg1.to_excel('aggreg1.xlsx', index=False)
df_json_tweets['date'] = df_json_tweets['date_timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ df_json_tweets['time'] = df_json_tweets['date_timestamp'].apply(lambda time: time.strftime('%H:%M'))
location=boto.s3.connection.Location.USWest2
token_receiveavg = token_receivecnt.groupby("receiver").agg({"receivecount":mean_except_outlier}).reset_index()
df = (data.loc[data['online'] == False]).drop(['brand', 'arch', 'free_memory', 'model', $                                           'simulator', 'storage_size', 'charging', $                                         'memory_size', 'free_storage','family','model_id','type'], axis=1).sort_values(ascending=True, by='created') $ display(HTML(df.to_html()))
goog.sort_values('Date',inplace=True) $ goog.set_index('Date',inplace=True) $ goog
rsp.json()['code']==0
np.random.seed(123456) $ dates = pd.date_range('8/1/2014', periods = 10) $ s1 = pd.Series(np.random.randn(10), dates) $ s1[:5]
pres_df['location'][0].split(',')
print(status.created_at)
sms = df_sms.groupby('group').agg({'ID':'count', 'messageCount':'sum', 'ShopperID':'nunique'}) $ sms['R'] = sms.messageCount * sms_value_R $ sms
tweet_image_predictions_clean.info()
df.neu.shape
atdist_4x_positive = atdist_4x[atdist_4x['emaResponse'].isin(["Yes! This info is useful, I'm going now.", "Yes. This info is useful but I'm already going there."])] $ atdist_4x_positive.merge(atloc_4x, how='left', on=['vendorId', 'taskLocationId'])
categoryByCountry_df  = youtube_df[youtube_df["category_name"].isin(category_namesC)] $ categoryByCountry_dfMax = categoryByCountry_df.loc[categoryByCountry_df.groupby(["category_name"])["views"].idxmax()] $ categoryByCountry_dfMax 
train_cols = data.columns[1:]
iterables = [eia_total_monthly.index.levels[0], range(2001, 2017), range(1, 13)] $ index = pd.MultiIndex.from_product(iterables=iterables, names=['type', 'year', 'month']) $ eia_extra = pd.DataFrame(index=index, columns=['total fuel (mmbtu)', 'generation (MWh)', $                                                'elec fuel (mmbtu)'])
tweet_counts_by_month.tweet_count.plot()
df_master.to_csv('twitter_archive_master.csv', index_label=False)
has_stage_archive = twitter_archive_master[twitter_archive_master['has_stage'] == 1] $ for column in has_stage_archive.iloc[:,8:12].columns: $     avg_rating = has_stage_archive[has_stage_archive[column] == 1]['rating'].mean() $     print('The average rating for {}s is {}'.format(column, avg_rating))
for each in range(len(sample_index)): $     honeypot_df['src'].replace(sample_index[each],sample_regular[each],inplace = True)
total_sales['ratio'] = (sales_diff['Sale (Dollars)_y'].values / sales_diff['Sale (Dollars)_x'].values)
gr_e2 = df.query("group == 'control' and landing_page == 'new_page'")
dbData.head(3)
calls_df.pivot_table(["length_in_sec"],["call_day"],aggfunc="mean")
uniqueTags = np.unique(catList)
p = pd.Period('2011', freq='A-DEC')
rentals['description'] = rentals['description'].str.lower()
pop_dog = twitter_archive_master.groupby('p1')['retweet'].mean().reset_index() $ pop_dog.sort_values('retweet', ascending=False).head()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ print(len(old_page_converted))
bg_df2.index # list all the row labels
import pandas as pd $ data2 = pd.read_csv('/content/us-consumer-finance-complaints.zip', compression='zip', header=0, sep=',', quotechar='"') $
df.text[df.text.str.len() == df.text.str.len().min()]
StockData.to_csv(StockDataFile + '.gz', compression='gzip', index=False)
merged = pd.merge(prop, contribs, on= 'calaccess_committee_id')
firstday_df = firstday_df.sort_values(by="created_at", ascending = True) $ firstday_df
bet_over  = [x[1] > .64 for x in pred_probas_over]
name =contractor.groupby('contractor_bus_name')['contractor_number'].nunique() $ print(name[name>1])
!h5ls -r 'data/my_pytables_file.h5'
new_page_converted = np.random.choice([0,1], size=n_new, p=[1-p_new, p_new]) $ new_page_converted.mean()
unique_users = df['user_id'].unique().shape[0] $ unique_users $
data = ['peter', 'Paul', 'MARY', 'gUDIO'] $ [s.capitalize() for s in data]
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(["doggo", "floofer", "pupper", "puppo"], axis = 1) $ twitter_archive_enhanced_clean.head()
precip_df.describe()
rng2017 = pd.date_range('2017 Jan 1 00:00', periods = 12, freq = 'MS') $ rng2018 = pd.date_range('2018 Jan 1 00:00', periods = 12, freq = 'MS') $ rng2019 = pd.date_range('2019 Jan 1 00:00', periods = 12, freq = 'MS')
testmx2 = testmx.toarray()
crimes.info(null_counts=True)
obj4 = pd.Series(sdata, index=['California', 'Ohio', 'Oregon', 'Texas'])
pf = df $ pf = pf[(pf['event'] == 'off')] $ pf = pf[~pf['elapsed'].isnull()] $ pf = pf[pf.elapsed < datetime.timedelta(minutes=70)] $ pf['elapsed'] = pf['elapsed'].astype('timedelta64[s]')
segmentData['opportunity_month_year'] = segmentData.lead_converted_date.dt.to_period('M') $ segmentData['discovery_month_year'] = segmentData.opportunity_qualified_date.dt.to_period('M') $ segmentData['lead_month_year'] = segmentData.lead_created_date.dt.to_period('M')
df_questionable_2 = pd.merge(left= df_questionable, left_on= 'link.domain_resolved', $                              right= df_usnpl_one_hot, right_on= 'domain', how= 'left')
indata_dir = 'data' $ indata     = 'hmeq' $ result = cassession.loadTable(indata_dir + '/' + indata + '.sas7bdat', casout = indata)
df_tweet_json_clean.info()
print(y_train.status.value_counts()) $ print(y_valid.status.value_counts()) $ print(y_test.status.value_counts())
for x in range(0,len(list_of_genre_1990s_clean)): $     list_of_genre_1990s_clean[x] = re.sub('\.[0-9]+', '', list_of_genre_1990s_clean[x]).strip('')[2:]
Moscow_food.delivery_address.value_counts()
print ('Difference of means:', Ralston["TMAX"].mean() - Ralston["TMIN"].mean())
Columns = StockData.columns.values $ Columns[-4:] = ['Date-Q{}'.format(N) for N in range(1,5)] $ StockData.columns = Columns $ print(StockData.columns)
dream = str(dreamSoup.body).split('span id="dreamContent">')[1].split('</span>')[0] $ user = re.search("phone - (.*)'s", dreamSoup.title.text).group(1) $ daysPassed = re.search(r'(\d+) days ago',str(dreamSoup.body)).group(1) $
print(attend_with.sum(axis=0).sum()) $ print() $ print(attend_with.sum(axis=0))
df2.columns[df2.columns.str.upper().str.contains('ORIGIN')]
blacklist = vacancies[(vacancies.hour >= 20) | (vacancies.hour < 10) | (vacancies.weekday > 5)] $ blacklist.sort_values('company')[['company', 'weekday', 'hour']]
temp.groupby(temp).size().to_frame().rename(columns={0: "size"}).head(4)
cur_a.callproc('updateContactEmail', $                ('contact@sydney.hyatt.com', 'Hyatt Regency Sydney'))
df.drop(remove_cols, axis=1, inplace=True)
!head -24 fremont.csv
add_plane_data.registerTempTable('tmp_400hz_usage')
df_new['UK_page'] = df_new['country_UK'] * df_new['ab_page'] $ df_new['US_page'] = df_new['country_US'] * df_new['ab_page'] $ df_new.head()
mean = np.mean(data['len']) $ print("The average length of all tweets: {}".format(mean))
session_df.groupby(by='location').agg({'delta_recall': ['mean', 'min', 'max']})
merged_daily = data.set_index('closed_at').resample('D').size() $ merged_monthly_mean = merged_daily.resample('M').mean() $ merged_monthly_mean.index = merged_monthly_mean.index.strftime('%Y/%m')
tweet_not_dog = stacked_image_predictions[stacked_image_predictions['is_dog']==False].index $ stacked_image_predictions.drop(tweet_not_dog, inplace=True) $ stacked_image_predictions.reset_index(drop=True, inplace=True) $ stacked_image_predictions.head(10)
for row in selfharmm_topic_names_df.iloc[5]: $     print(row)
snow.drop_table("st_rvo_me_ref")
search_df.info()
min(TestData.Lead_Creation_Date_clean), max(TestData.Lead_Creation_Date_clean)
seaborn.countplot(vacancies.weekday)
from h2o.estimators.glm import H2OGeneralizedLinearEstimator $ glm_model = H2OGeneralizedLinearEstimator(model_id = "GLM", family = "binomial") $ glm_model.train(x = predictor_columns, y = target, training_frame = train, validation_frame = valid)
from sklearn.model_selection import StratifiedShuffleSplit $ splitObject = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42) $ for train_index, test_index in splitObject.split(features, target): $     X_train, X_test = features[train_index], features[test_index] $     y_train, y_test = target[train_index], target[test_index]
pytrends.get_historical_interest(kw_list, year_start=2018, month_start=1, day_start=1, hour_start=0, year_end=2018, month_end=2, day_end=1, hour_end=0, cat=0, geo='', gprop='', sleep=10)
dfNYC.head()
projFile = "Projects.csv" $ schedFile = "Schedules.csv" $ budFile = "Budgets.csv"
train.info()
s.find_all('a')
target.shape
d = datetime.datetime(2018, 11, 12, 12) $ for post in posts.find({"date": {"$lt": d}}).sort("reinsurer"): $     pprint.pprint(post)
ccl["Date"] = pd.to_datetime(ccl.Date, format="%Y/%m/%d", errors='ignore')
dtypes={'date':np.str,'store_nbr':np.int64,'transactions':np.int64} $ parse_dates=['date'] $ transactions = pd.read_csv('transactions.csv', dtype=dtypes,parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",transactions.shape) $ pd.DataFrame.head(transactions)
kwargs = {'alg': xgb.XGBRegressor, 'metric': mean_squared_error $           , 'X_train': X_train, 'y_train': y_train, 'X_dev': X_dev, 'y_dev': y_dev}
X_future = sandag_df.values
df.iloc[[1,2,5],[0,3]]
df_usa['PD2'] = ((df_usa['Population']*1000)/df_usa['Area']).round(2) $ df_usa
mpl.rc('figure', figsize=(6, 3.5)) $ prophet_model.plot(forecast,uncertainty=True) $ plt.show;
reddit_comments_data.groupby('author_flair_text').count().orderBy('count', ascending = False).show(100, truncate = False)
data.sort_values(by='Likes',ascending=False).head(4)
top10 = git_blame[git_blame.knowing].author.value_counts().head(10) $ top10
pulledTweets_df.head(20)
dfClientes.shape
y_ = df['loan_status'].replace(to_replace=['PAIDOFF', 'COLLECTION'], value=[1, 0]) $ y_[0:5]
dfs = pd.read_html('https://en.wikipedia.org/wiki/Timeline_of_programming_languages', header=0) $ dfs[4]
week23 = week22.rename(columns={161:'161'}) $ stocks = stocks.rename(columns={'Week 22':'Week 23','154':'161'}) $ week23 = pd.merge(stocks,week23,on=['161','Tickers']) $ week23.drop_duplicates(subset='Link',inplace=True)
def sale_lost(count, minutes): $     crepe_per_min = count // (4 * 60) * 0.002 $     crepes_lost = crepe_per_min * minutes $     return crepes_lost
X = reddit['title'].values $ y = reddit['engagement'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
!tar -xvzf rossmann.tgz
students.iloc[8]
my_file = open('test_data//open_close_test.txt') $ my_file.close()
df_selected.filter("user_id = '-2EcIDIDnA8H7N81jwYpcQ'").toPandas()
df2.groupby(['group'])['converted'].mean()
x_eff = 30 * u.mbarn $ density = 1 * u.cm ** -3 $ interaction_time = (density * x_eff * cst.c) ** -1 $ interaction_time.to('Myr')
(df2.query("group == 'control'")['converted'] == 1).mean()
new_page_converted.mean()
tw_clean[tw_clean.name == "a"]
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
print 'original rounds: %s, new count of rounds: %s' %(rounds.shape[0], rounds_df.shape[0])
import tqdm as tqdm $ import os $ import time $ for file in tqdm.tqdm(os.listdir('.')): $     time.sleep(0.5)        
testObjDocs.outDF.tail(10)  ## new records on the end ... still need to delete the bad ones
df_measures_users.head()
len(df[(df['Complaint Type'] == 'Homeless Encampment')&(df.index.month.isin([12,1,2]))])
from sklearn.preprocessing import StandardScaler $ from sklearn.decomposition import PCA $ X_std = StandardScaler().fit_transform(X) $ pca = PCA(n_components=2) $ x_9d = pca.fit_transform(X_std) $
tweets = pd.read_csv('tweets_mentioning_candidates.csv') $ tweets['set'] = 'test' $ tweets['polarity_value'] = np.NaN
ripple_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
interests_groupby_user = df_interests['interest_tag'].groupby(df_interests['user_handle']) $ lst_user_interests = [[name, group.tolist()] for name, group in interests_groupby_user] $ lst_user_interests[1]
twitter_archive_clean[~twitter_archive_clean['in_reply_to_status_id'].isnull()]
american_train_model.print_topics(num_topics = 10 ,num_words = 10)
promo_df.drop('onpromotion', 1, inplace=True)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2.drop(['Neighbors_Obj'], axis=1) $
results = pd.read_csv("analysis_csv_251560641854558.csv", index_col=None) $ number_of_posts = results["ID"].count() $ print("Total number of posts: {}".format(number_of_posts))
df['HL_PCT'] = (df['Adj. High'] - df['Adj. Low']) / df['Adj. Close'] * 100.0 $
predictors = wages.drop('wage_per_hour', axis = 1).values $ print(predictors.shape) $ predictors
cust_demo.sample(n=700, replace=False).duplicated().value_counts()
before_sentiment = si.overall_sentiment(before) $ during_sentiment = si.overall_sentiment(during) $ after_sentiment = si.overall_sentiment(after)
Meter1.ModeSet('DC_V')
df.head()
predictions_clean.tweet_id = predictions_clean.tweet_id.astype(str)
p_new = df2.converted.mean() $ p_new
sum(df2['converted'])/df2.shape[0]
df2_dummy = df2_dummy.drop(['ab_page_old_page'],axis=1) $ df2_dummy.tail(1)
print("Probability of control group converting:", $       ab_file2[ab_file2['group']=='control']['converted'].mean())
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=As4uh8SmqPWoJ1s9XXDT&start_date=2017-01-01&end_date=2017-01-01" $ r = requests.get(url)
!python csv_to_tfrecords.py --csv_input=images/train/train_labels.csv --image_dir=images/train --output_path=train.record
pickle.dump(nmf_tfidf_df, open('iteration1_files/epoch3/nmf_tfidf_df.pkl', 'wb'))
bigrams = nltk.bigrams(tweet_no_stop) $ word_freq = nltk.FreqDist(bigrams)
wrd_clean['rating_numerator'].describe()
df = pd.read_csv('result_summary_combined.csv', na_values=['NaN'])
def sigmoid_gradient(z):  $     return np.multiply(sigmoid(z), (1 - sigmoid(z)))
yc_new1 = yc_new1[['Unnamed: 0', 'Fare_Amt', 'tripDurationHours', 'Trip_Distance', 'Tip_Amt', 'income_departure', 'zip_dest']] $ yc_new1.head()
def rmprecent(s): $     return s.split("%")[0] $ hsi["rise_nextday"] = (hsi["Change %"].apply(rmprecent).astype("float") >= 1).astype("int")
final_log_mod = sm.Logit(df_comb['converted'], df_comb[['intercept', 'ab_page', 'CA', 'CA_new_page', 'UK', 'UK_new_page']]) $ results_final_log_mod = final_log_mod.fit() $ results_final_log_mod.summary()
diff_pnew_pold = (new_page_converted.mean()) - (old_page_converted.mean()) $ print('P_new - P_old for simulated values from e and f is {}'.format(diff_pnew_pold))
df2 = df2.drop(df2.index[2893])
rf = RandomForestClassifier(n_estimators=50, random_state=42) $ scores = cross_val_score(rf, X, y, cv=skf) $ print "Cross-validated RF scores based on subreddit:", scores $ print "Mean score:", scores.mean()
fp7_proj.shape, fp7_part.shape, fp7.shape
plt.scatter(x,y) $ plt.plot(x, np.dot(x_15, linear.coef_) + linear.intercept_, c='y') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
loans_df.earliest_cr_line.value_counts()
male = crime.loc[crime['Sex']=='M'] $ male.head(3)
df.loc['20180102':'20180104', ['B', 'C']]  # Selecting by label
price2017 = price2017.rename(columns={'DE-AT-LUX':'Price_Germany'})
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline $
a2.head()
df.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis=1, inplace=True)
train_data['vehicleType_int'] = train_data['vehicleType'].apply(get_integer2) $ test_data['vehicleType_int'] = test_data['vehicleType'].apply(get_integer2) $ del train_data['vehicleType'] $ del test_data['vehicleType']
autos["registration_year"].value_counts(normalize=True)
squares = pd.Series([1, 4, 9, 16, 25], $                     index=['square of 1', 'square of 2', 'square of 3', 'square of 4', 'square of 5']) $ squares
pd.merge(left=city_loc, right=city_pop, on="city", how="right")
us.loc[us['country'].str.len() == 2, 'cityOrState'] = us.loc[us['country'].str.len() == 2, 'country'] $ us.loc[us['country'].str.len() == 2, 'country'] = 'USA' $ us['country'].value_counts(dropna=False)
guinea_data1 = guinea_data[guinea_data.Description.isin(['Total new cases registered so far', $                                                          'New deaths registered'])] 
venues.to_csv('./cleaned/venues.csv', header=False, index=False)
Lab7_RevenueEPS0.head()
train['weekend'] = (train.day >= 5).astype(int) $ train.groupby('weekend').popular.mean()
GBR.fit(X_train,Y_train)
params = {'figure.figsize': [8,8],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(therapist_duration, params=params, freq=31, title='Therapist Decomposition')
data.text.str.contains("snack").resample("1T").sum().plot()
BroncosBillsPct.merge(BroncosBillsTweets, how='left', on='text30')
import json $ pos = json.loads(r.content) $ pos
len(package.resources)
df_clean = pd.merge(left=df_enhanced,right=df_json, left_on='tweet_id', right_on='id', how = 'left') $ df_clean = df_clean.drop(['id', 'retweeted'], axis = 1) $ df_master = pd.merge(left=df_clean,right=df_breed, left_on='tweet_id', right_on='tweet_id', how = 'left')
itemTable.iloc[0:30]
users.dtypes
session.query(Measurements.station, func.count(Measurements.date)) \ $     .group_by(Measurements.station).order_by(func.count(Measurements.date).desc()).all()
image_clean = image_clean.drop_duplicates(subset=['jpg_url'], keep='last')
pred_probas_rfc_under = gs_rfc_under.predict_proba(X_test)
import unicodedata $ def clean_unicode(unicode_str):      #Credit to Yingling $     return unicodedata.normalize("NFKD", unicode_str) $ indeed['summary_clean'] = indeed['summary'].apply(lambda x: x.replace('\n',' ')) $ indeed['summary_clean'] = indeed['summary_clean'].apply(clean_unicode) $
df_coor.head(20)
s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])
df[df['group'] == 'treatment'].query("landing_page == 'old_page'").index.values $ df2 = df2.drop(df[df['group'] == 'treatment'].query("landing_page == 'old_page'").index.values)
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis=1) $ df2.head()
my_df.dropna(inplace=True) $ my_df.reset_index(drop=True,inplace=True) $ my_df.info()
Barcelona =  api.search(geocode ='Paris', count =300) $ data3 = pd.DataFrame(data=[tweet.text for tweet in Barcelona], columns=['Tweets'])
X_eval.shape
news_period_df.loc[3, 'news_title']
data = {'Job_A': pd.Series(data = [2, 300.000]), $                  'Job_B': pd.Series(data = [6, 2])} $ job_requirements = pd.DataFrame(data) $ job_requirements
evaluator = MulticlassClassificationEvaluator( $     labelCol="label", predictionCol="prediction", metricName="weightedRecall") $ recall = evaluator.evaluate(predictions) $ print("Recall = %g " % (recall))
def remove_no_reaction(df): $     tmp = df[(df['highest_reaction'].notnull())] $     return tmp[tmp['fb_total_reactions'] >= 200] $ dataset_test = remove_no_reaction(dataset_test) $ dataset_test.loc[:,"highest_reaction"] = dataset_test.loc[:,"highest_reaction"].astype(int)
null_name = raw_data[(raw_data['name'].isna())] $ null_name.T
n_old=(df2['landing_page']=='old_page').sum() $ n_old
df[df['Agency Name'].isin(['New York City Police Department', 'NYPD'])].count()
file_dir = os.path.dirname(os.path.abspath("__file__")) $ parent_dir = os.path.dirname(file_dir) $ newPath = os.path.join(parent_dir, 'bus_lines/bus_66_lt.csv') $ dfleavetimes = pd.read_csv(newPath, delimiter=',', index_col=False)
for row in selfharmm_topic_names_df.iloc[2]: $     print(row)
df.iloc[0]['link_to_post']
df2_new=countries_df.join(df2.set_index('user_id'),how='inner')
properati[properati['zone'] == ""]['place_name'].value_counts(dropna=False)
s = type.__new__(type,'MetaClass',(),{"a":1})
from sklearn.model_selection import cross_val_score $ accuracies = cross_val_score(LogisticRegression(), X, y, cv=10) $ print accuracies.mean() $ print 1-y.mean()
for i in station_distance['Start Station Latitude']: $     start_lat.append(i) $ for i in station_distance['Start Station Longitude']: $     start_lon.append(i)
test=weekly[:7].sort_values(by=['weekday']) $ plt.figure(figsize=(10,3)) $ plt.plot(test['weekday'],test['DAILY_ENTRIES'])
df.columns
B = pd.DataFrame(rng.randint(0, 10, (3, 3)), columns=list('BAC')) $ B
lReligion = list(db.osm.find({"religion":{"$exists":1}})) $ print 'length of the list = ', len(lReligion) $ lReligion[:5]
approved['approved'] = True $ approved.head()
df1 = pd.DataFrame() $ df1['ACME'] = np.random.randint(low=20000, high=30000, size=100) $ df1['JUBII'] = np.random.randint(low=20000, high=40000, size=100) $ df1.index = pd.date_range('1/1/2014', periods=100, freq='H')
lsa_tfidf_topic6_sample_precision = mf.get_precision_score(lsa_tfidf_topic6_sample, lsa_tfidf_topic6_sample_fp_list) $ lsa_tfidf_topic6_sample_precision
ign = ign.rename(columns={'title':'Name', 'platform':'Platform'}) $
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df['group'])['treatment'] $ df2.head()
s = pd.Series([1, 2, 3]) $ s.loc['Animal'] = 'Bears' $ s
 new_page = np.random.binomial(1,p,df2.query('landing_page == "new_page"').shape[0])
df_all.xs(299,level='fk_loan')
from IPython.core.display import HTML $ def css_styling(): $     styles = open("styles/custom.css", "r").read() $     return HTML(styles) $ css_styling()
df_loan2[df_loan2.fk_loan==36]
df2 = df2.drop_duplicates(subset = ['user_id'], keep='first') $ df2.shape
tmi = indices(tmaggr, 'text', 'YearWeek') $ tmi.head()
df_select_cats = df_select.copy() $ df_select_cats = df_select_cats.groupby(['Categories'], as_index=False).mean() $ df_select_cats
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
pd.value_counts(ac['IAM'].values, sort=True, ascending=False)
p_diffs=np.array(p_diffs)
df_new = df_new.join(pd.get_dummies(df_new['country']))
props.prop_name
SVM_yhat = SVM.predict(test_X) $ print("SVM Jaccard index: %.2f" % jaccard_similarity_score(test_y, SVM_yhat)) $ print("SVM F1-score: %.2f" % f1_score(test_y, SVM_yhat, average='weighted') )
poverty_2011_2015.columns.name=None
kick_projects = df_kick[(df_kick['state'] == 'failed') | (df_kick['state'] == 'successful')] $ kick_projects['state'] = (kick_projects['state'] =='successful').astype(int)
plt.hist(traindata.rating) $ plt.title("Training Data Star to Own Distribution");
B2 = B.get_step_object('step_2') $ B2.load_indicator_settings_filters()
print("Saving data to elasticsearch - please be patient") $ df = df.withColumn("datetime",df["datetime"].cast("string")) # elasticsearch needs datetimes in a string type $ es.saveToEs(df,index=es_dataindex,doctype=es_datatype)
df_image_clean['tweet_id'] = df_image_clean['tweet_id'].astype('str')
lines_to_read=100 $ with open(dumpfile) as myfile: $     firstlines=myfile.readlines()[0:lines_to_read] #put here the interval you want $     for x in firstlines: $         print(x.strip())
colors = ('blue', 'red', 'gold', 'green', 'c') $ lgd = zip(tweet_df["Tweet Source"].unique(),colors) $
df_parties.groupby(df_parties.index.weekday)['Created Date'].count().plot(kind="bar") $
pivoted.T.shape $
df_license_appl.head(2)
log_mod = sm.Logit(df_new['converted'], df_new[['UK', 'US']]) $ results = log_mod.fit() $ results.summary()
trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn]) $ val_lm = np.array([[stoi[o] for o in p] for p in tok_val])
train, validate, test = np.split(cnn_fox_data.sample(frac=1), [int(.6*len(cnn_fox_data)), int(.8*len(cnn_fox_data))])
from sklearn.decomposition import LatentDirichletAllocation $ lda = LatentDirichletAllocation(n_topics=10, $                                 random_state=123, $                                 learning_method='batch') $ X_topics = lda.fit_transform(X)
geocoded_df[list(filter(lambda x: x.endswith('Date'), geocoded_df.columns))] = \ $     geocoded_df[list(filter(lambda x: x.endswith('Date'), geocoded_df.columns))].apply(pd.to_datetime)
tallies_file = openmc.Tallies()
weather_mean.iloc[5:15:3, :]
archive_copy.head()
nba_df.loc[(nba_df['Season'] == 2015) & (nba_df['Team'] == "BOS") & (nba_df["Opp"] == "MIL") & (nba_df["G"] == 82), "Home.Attendance"] = avg_att_2015_BOS $ nba_df.loc[(nba_df['Season'] == 2015) & (nba_df['Team'] == "MIL") & (nba_df["Opp"] == "BOS") & (nba_df["G"] == 82), "Home.Attendance"] = avg_att_2015_MIL
df['created_date'] = pd.to_datetime(df['created_date']) $ df.head()
len(df_events), len(df_events.group_id.unique())
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ result = lm.fit() $ result.summary()
pd.concat([pd.DataFrame(y_pred_rsskb, columns=['Predictions'], index=y_test.index), y_test], axis=1) # Test predictions
df = pd.read_csv("http://ichart.yahoo.com/table.csv?s=MSFT&" + $            "a=5&b=1&c=2014&" + $            "d=5&e=30&f=2014&" + $            "g=d&ignore=.csv") $ df[:5]
days = [start_date] $ while start_date!=end_date: $     start_date += dt.timedelta(days=1) $     days.append(start_date)
got_data.sort_values('total_comments', ascending=False, inplace=True)
rchillipivot = pd.pivot_table(files8, index='shortlistCandidateId', $                      aggfunc={'Tenure' : 'max', 'ExperienceId':'count'}) $ rchilli = pd.DataFrame(rchillipivot.to_records()) $ rchilli = rchilli.rename(columns={'shortlistCandidateId':'candidateid','ExperienceId':'cv_total_jobs','Tenure':'cv_max_tenure'})  $ rchilli.head()
print(stock_data.shape) $ print("The number of rows in the dataframe is: ", stock_data.shape[0]) $ print("The number of columns in the dataframe is: ", stock_data.shape[1]) $
converted = df['converted'].value_counts()[1] $ total =df['converted'].count() $ total_converted = (converted/ total)* 100 $ print('The total amount converted: {} \n' $      'The percentage : {}%'.format(converted, total_converted))
df.sample(5)
dft = df2.T $ dft
display_all(train_data.head(5).transpose())
df_test.info()
mismatch1 = (ab_df['landing_page'] == "new_page")&(ab_df['group'] == "control") $ mismatch2 = (ab_df['landing_page'] == "old_page")&(ab_df['group'] == "treatment") $ print(ab_df[mismatch1].shape[0]+ab_df[mismatch2].shape[0])
loan_fundings[(loan_fundings.fk_loan==36)].T.to_clipboard()
df_melt = pd.melt(frame=df, id_vars='name', $        value_vars=['treatment a', 'treatment b'], $        var_name = 'treatment', $        value_name = 'result') $ df_melt
grid_lat = np.arange(np.min(lat_us), np.max(lat_us), 1) $ grid_lon = np.arange(np.min(lon_us), np.max(lon_us), 1) $ glons, glats = np.meshgrid(grid_lon, grid_lat)
print(df2.shape) $ print(countries.shape)
with tf.name_scope("eval"): $     correct = tf.nn.in_top_k(logits, y, 1) $     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) $     accuracy_summary = tf.summary.scalar('accuracy', accuracy)
df.info()
import requests $ import json $ r = requests.get("https://quotes.rest/qod.json") $ res = r.json() $ print(json.dumps(res, indent = 4))
user_summary_df[['followers_count', 'following_count', 'tweet_count', 'original', 'quote', 'reply', 'retweet', 'tweets_in_dataset']].describe()
df_merge = df_clean.merge(df_twitter_clean, how='inner', left_on='tweet_id', right_on='id') $ df_merge.drop('id', axis=1, inplace=True)
submission_full[['proba']].max()
np.sum(van_true)
archive_df_clean['rating_numerator'] = archive_df_clean.rating_numerator.astype(float)
not_in_df = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_not_t_n = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ mismatch= len(not_in_df) + len(df_not_t_n) $ mismatch_df = pd.concat([not_in_df, df_not_t_n]) $ mismatch
y = df.values $ y.size
df2.loc[df2["user_id"] == 773192,]
def get_list_tot_vidviews(the_posts): $     list_tot_vidviews = [] $     for i in list_Media_ID: $         list_tot_vidviews.append(the_posts[i]['activity'][-1]['video_views']) $     return list_tot_vidviews
ux.html_utils.get_webpage_description(urls[0])
result['timestamp'].describe()
user = api.get_user(ids[0])
new_cases.columns = ["Date", "Total_new_cases_guinea", "country"]
coins_top10today = ['BTC', 'ETH', 'XRP', 'BCH', 'LTC', $                    'EOS', 'ADA', 'XLM', 'MIOTA', 'NEO'] $ initialdate_per_coin.T[coins_top10today]
df_genre = df['genre'].to_frame() $ print(df_genre)
gearbox_list = list(set(train_data.gearbox))
by_area['AQI'].hist(bins=20,histtype='step',stacked=True,linewidth=3) $ plt.legend(['Cottage Grove','Eugene/Springfield','Oakridge']);
df['Trace'] = df['Chernoff_Bound'].apply(lambda x: 2 * x)
brands = autos["brand"].value_counts()[autos["brand"].value_counts() > 1000].index
df.describe()
dfb_train=train.date_first_booking $ dfb_test=test.date_first_booking
print('THE DST INDEX VALUE IS PRESENT FROM YEAR 1975 TO YEAR 2018 FEBRURAY.ENTER THE DETAILS TO GET THE CORRESPONDING PLOT\n') $ print("Enter the Starting Date from where you want the plot in the form yyyy-mm-dd") $ start=input() $ print("Enter the Ending Date upto which you want the plot in the form yyyy-mm-dd") $ end=input()
cfd_nums = [vs for word in wsj $             for vs in re.findall(r'[a-z][a-z]', word.lower())] $ cfd = nltk.ConditionalFreqDist(cfd_nums) $ df_cfd = pd.DataFrame(cfd).fillna(value=0).astype(dtype=int) $ df_cfd
data2.info()
y4, X4 = patsy.dmatrices('DomesticTotalGross ~ Budget + G + PG + PG13 + R + Runtime', data=df, return_type="dataframe") $ model = sm.OLS(y4, X4, missing='drop') $ fit4 = model.fit() $ fit4.summary()
new_albums.head()
while len(new_tweets) > 0: $     new_tweets = api.user_timeline(screen_name = 'realDonaldTrump', count=200, max_id=oldest) $     alltweets.extend(new_tweets) $     oldest = alltweets[-1].id - 1
df2[df2.duplicated(subset='user_id')]
faux = clean_rates.text.str.contains('\d+\.\d+/\d+(\.\d+)?') $ clean_rates[faux]
b_cal_q1.columns
np.save(LM_PATH / 'tmp'/ 'tok_trn.npy', tok_trn) $ np.save(LM_PATH / 'tmp'/ 'tok_val.npy', tok_val)
from sklearn.metrics import roc_auc_score $ print classification_report(ada_pred, y_test) $ print 'ROC AUC Score: ', roc_auc_score(ada_pred, y_test)
titanic.shape
gbctest = pd.DataFrame(labeled_features.loc[labeled_features['datetime'] >= pd.to_datetime('2015-10-01 00:00:00')]) $ gbctest['predicted_failure'] = model.predict(test_x)
df = pd.DataFrame(emails) $ df
train.head()
reddit_comments_data.groupby('author_flair_css_class').count().orderBy('count', ascending = False).show(100, truncate = False)
h1 = re.findall(r'<h1>[\w ]+</h1>', html) $ print(h1) $ h1 = re.findall(r'<h1>([\w ]+)</h1>', html) $ print(h1)
df[0:17]=np.nan $ df.iloc[40:90]=np.nan $ df.iloc[40:90].head()
cp311['day'] = cp311.created_date.dt.to_period('D') $ cp311.set_index('day', inplace=True) $ cp311.head(2)
df_precipitation.head()
print mike.ra[0] $ a=Angle(mike.ra[0], u.hr).to_string(unit=u.degree, precision=5, decimal=True) $ print float(a) $ b=Angle(a, u.deg).to_string(unit=u.hr, sep=':', precision=1, pad=True) $ print b
facts_url = "https://space-facts.com/mars/"
df.to_csv(save_dir)
converted_controlusers2 = float(df2.query('converted == 1 and group == "treatment"')['user_id'].nunique()) $ treat_users2 =float(df2.query('group == "treatment"')['user_id'].nunique()) $ tp2 = converted_controlusers2 /treat_users2 $ print(" Given that an individual was in the treatment group, the probability they converted is {0:.2%}".format(tp2))
page_interaction_log = sm.Logit(df4['converted'], df4[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ page_interaction_result = page_interaction_log.fit()
trump['source'] = trump["source"].str.replace("<.*?>", "").str.replace("</a>", "") $ trump['source'].value_counts().plot(kind="bar") $ plt.ylabel("Number of Tweets");
tipsDF = tipsDF.drop(tipsDF.columns[0], axis = 1)
display(heading('Bad:'), $         set(sheets_with_bad_column_names.keys()))
print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)
df_archive.sample(10)
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ results = logit.fit()
pd.crosstab(train.TYPE_BI, train.TYPE_UT)
print df.shape[0] + noloc_df.shape[0]
old_page_converted = np.random.choice([0, 1], size=n_old, p=[1-p_old, p_old]) $ len(old_page_converted)
traintrain = train.loc[:11815*188-1, :] $ trainvalidation = train.loc[11815*196:, :]
temps_df.Difference[1:4]
var_df.resample("A").sum()
notus['country'] = notus['country'].str.replace('United States of America', 'USA') $ notus['country'] = notus['country'].str.replace('Maryland', 'USA')
potholes = df[df['Descriptor'] == 'Pothole'] $ potholes.groupby(potholes.index.hour)['Created Date'].count().plot(y='Created Date')
list_of_genre_1990s = list() $ respond = requests.get("https://en.wikipedia.org/wiki/1990s_in_music") $ soup = BeautifulSoup(respond.text) $ l = soup.find_all('ul') $ list_of_genre_1990s.append(l[3].text.split(','))
df.shape
pd.DataFrame(np.random.rand(3, 2), $              columns=['foo', 'bar'], $              index=['a', 'b', 'c'])
df2[((df2['group'] == 'control') == (df2['landing_page'] == 'old_page')) == False].shape[0]
end_time = dt.datetime.now() $ (end_time - start_time).total_seconds()
pd.DataFrame(lostintranslation.credits()['cast'])[["character","name", "id"]].head()
to_be_predicted_Day2 = 14.78510842 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
list(df_users_first_transaction.dropna(thresh=int(df_users_first_transaction.shape[0] * .9), axis=1).columns)
data["Zip Code"] = data["Zip Code"].astype("int")
df = json_normalize(j['datatable'], 'data') $ df.columns = col_names $ df.head()
model_info = kipoi_veff.ModelInfoExtractor(model, Dataloader) $ vcf_to_region = kipoi_veff.SnvCenteredRg(model_info)
todaysFollowers.head()
dset = xr.open_dataset(os.path.join(opath, filename))
to_be_predicted_Day4 = 34.13664722 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
top_songs['Track Name'].isnull().sum()
gen = dta[dta.b==1]
feature_col = ibm_hr_final.columns $ feature_col
contractor[contractor['contractor_id'].isin([139,140,228,236,238])]
noNulls.orderBy(sort_a_asc).show(5)
df = pd.read_csv("/Users/peterjost/Downloads/12-classwork/classwork-12-311/data/311_Service_Requests_from_2010_to_Present.csv", usecols=["Created Date", "Closed Date", "Agency", "Complaint Type", "Descriptor", "Borough"]) $ df.head()
import folium $ map_osm = folium.Map(location=[1.3521, 103.8198],zoom_start=12, tiles='Stamen Toner') $ def adding_to_map(lbs): $     return folium.CircleMarker(location=[lbs[5],lbs[6]], radius=12, color='#3186cc', fill_color='#3186cc').add_to(map_osm)
doc_duration.head()
df2.iloc[4,3]=np.nan
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt" $ df = pd.read_csv(path, sep ='\s+', na_values=['.']) $ df.head(5)
%%timeit $ scipy.optimize.fsolve(globals()[function_name], 2)
events_enriched_df = pd.merge(events_df[['event_id','group_id','yes_rsvp_count','waitlist_count','event_time']] $                               ,groups_topics_unique_df[['group_id','topic_name','topic_id']] $                               ,on ='group_id' , how='left' )
reddit.head(2)
df2['seller'].value_counts().plot(kind='bar') $ print(df2['seller'].value_counts())
ad_source.to_csv('../data/ad_source.csv')
precip = session.query(Precip.date, Precip.prcp, Precip.station).filter(Precip.date >= date_ly)#.filter(Precip.station == 'USC00511918')
pd.options.display.max_rows = 100 $ pd.options.display.max_colwidth = 300 $ all_tweets.iloc[:30,7:8].style.set_properties(**{'text-align': 'left'})
news_title = soup.find('div', 'content_title', 'a').text $ news_p = soup.find('div', 'rollover_description_inner').text $ news_title $ news_p
most_freq = data[(data['longitude'] == 103.93700000000001) & (data['latitude'] == 1.34721)]
browser.quit()
reduced_trips_data = trips_data.loc[abs(trips_data.duration - trips_data.duration.mean()) <= (3*trips_data.duration.std())] $
X_train['building_id'].unique().shape
tt_final[['rating_numerator', 'rating_denominator','favorite_count', 'retweet_count', 'img_num' ]] = tt_final[['rating_numerator', 'rating_denominator','favorite_count', 'retweet_count', 'img_num' ]].astype(int) $ tt_final.info()
df2_new.head()
df_raw = pd.read_csv('autos.csv', encoding='latin-1') $ df = df_raw.copy() #just to keep raw data in memory to later uses $ print 'list of columns available in our dataset: ' $ df.columns
actual_diff = df2[df2.group == 'treatment']['converted'].mean() -  df2[df2.group == 'control']['converted'].mean() $ actual_diff
df2[df2.landing_page == 'old_page'].shape[0]/df2.shape[0]
shortcodes = [] $ for i in range(35): $     shortcodes.append(df[0:40]['link_to_post'][i][-11:]) $ shortcodes[0] $ len(shortcodes)
tlen.plot(figsize=(16,4), color='r');
tobs_start_date = dt.datetime.strptime(latest_date,"%Y-%m-%d") - dt.timedelta(days=365) $ tobs_start_date = tobs_start_date.strftime("%Y-%m-%d")
plt.plot(times)
df.columns
df1.unstack()
with open('./data/processed/X_train_age_imputed.pkl', 'wb') as picklefile: $     pickle.dump(X_train,picklefile) $
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=20000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df_cod2 = df_cod.copy() $ df_cod2 = df_cod2.dropna()
Pold = df2.converted.mean() $ Pold
autos = autos[autos['price'].between(1, 350000)] $ autos['price'].describe()
universe = ["SAP.DE", "UN01.DE", "BAS.DE"] $ price_data = yahoo_finance.download_quotes(universe)
k = pd.read_sql_query(QUERY, conn) $ k
from scipy.stats import norm $ norm.ppf(1-(0.05))
from crowdtruth.configuration import DefaultConfig
bag = vect.fit_transform(df['lemmas']).toarray() $ bag
df.query("(group == 'treatment' and landing_page != 'new_page') or (group == 'control' and landing_page != 'old_page')").shape
trunc_df.iloc[94]
scaler = MinMaxScaler() $ data[intFeatures] = scaler.fit_transform(data[intFeatures])
df_lm.keys()
os.chdir('images')
(-1 * close_month).loc[month].nlargest(2)
today = "2018-07-25" $ past = pd.to_datetime(today) - pd.DateOffset(years=18) $ print(past) $ birth_dates["Name"].loc[birth_dates["BirthDate_dt"]<=past].head()
pd.read_csv('data/eth-price.csv', parse_dates=[0]).head()
len(df2.loc[(df2['group'] == 'control') & (df2['converted'] == 1)]['user_id'])/len(df2.loc[df2['group'] == 'control']['user_id'])
df1 = tier1_df.reset_index() $ df1 = df1.rename(columns={'Date':'ds', 'Incidents':'y'})
party_type_crosstab.plot.bar(x="YEAR", y="DEM_VS_GOP", legend=False)
results_simpleResistance, out_file1 = S.execute(run_suffix="simpleResistance_hs", run_option = 'local')
zipincome.ZIPCODE[:10]
import pymms $ mysdc = pymms.MrMMS_SDC_API(start_date='2017-07-11', end_date='2017-07-11', $                             data_root='/Users/argall/MrWebData/mms/')
nums.reverse()  # reverses the item order $ nums
columns = inspector.get_columns('Station') $ for c in columns: $     print(c['name'], c["type"])
pd.pivot_table(more_grades, index="name")
np.concatenate([np.random.random(5), np.random.random(5)])
opt_fn = partial(optim.Adam, betas=(0.7, 0.99))
countries = pd.read_csv("../data/airbnb/countries.csv")
qualification.qual_conversion.value_counts()
regr = LinearRegression() $ regr.fit(X, y) $ fl = ["${:,.2f}".format(x) for x in list(regr.coef_)] $ for feature, coef in zip(X.columns, fl): $     print(feature, coef)
the_drg_number = 66 $ idx = df_providers[ (df_providers['year']==2011) & \ $                   (df_providers['drg3']==the_drg_number)].index.tolist() $ print('There are',len(idx),'sites for DRG',the_drg_number) $ print('Max payment:',np.max( df_providers.loc[idx,'disc_times_pay'] ))
df.groupby('Agency')['Complaint Type'].value_counts().groupby(level=0, group_keys=False).nlargest(5)
free_data[free_data.educ >= 5].groupby("age_cat").describe().stack(1)
df_joy.describe()
mediaMask = results['ActivityMediaSubdivisionName'] == "Surface Water" $ hydroMask = ~results['HydrologicEvent'].isin(("Storm","Flood","Spring breakup","Drought")) $ charMask = results['CharacteristicName'] == "Nitrogen, mixed forms (NH3), (NH4), organic, (NO2) and (NO3)" $ sampFracMask = results['ResultSampleFractionText'] == 'Total'
freq_df = pd.DataFrame(freq, columns=['date', 'tobs']) $ freq_df.set_index('date', inplace=True, ) $ freq_df.head()
airline_tw_collec = mongo.mongoDB_read_collection(mongoDBname, collec_name)
fin_coins_r.isnull().sum()
predictions = loaded_text_classifier.predict(df_test) $ loaded_evaluator = loaded_text_classifier.evaluate(predictions) $ loaded_evaluator.get_metrics('macro_f1')
df_new['ab*CA'], df_new['ab*UK'] = df_new['ab_page']*df_new['country_CA'], df_new['ab_page']*df_new['country_UK'] $ df_new.head()
autos=pandas.read_csv("autos.csv", encoding="Latin-1")
pattern = re.compile(r'<a.*>([\w\s]+)</a>')
spacy_nlp = spacy.load("en_core_web_lg")
df3 = pd.merge(df2_new, country_df , on=['user_id']) $ df3.head()
df.groupby('season')['episode_id'].nunique().hist()
df_input_clean.fillna(-99999, subset=['Resp_time']).filter("`Resp_time` == -99999").count()
street_freq = train.groupby("Block")['Block'].count() $ train = train.join(street_freq, on="Block", rsuffix='_fre')
orig_ct = len(dfd) $ dfd = dfd.query('in_pwr_5F_max >= 0.75 and in_pwr_5F_max <= 10.0') $ print(len(dfd) - orig_ct, 'eliminated')
s = pd.Series() $ s
df3 = pd.read_csv('2003.csv')
gpCreditCard.Passenger_count.describe()
a = [] $ a.append(1) $ a.append(2) $ a
hour_of_day15.to_excel(writer, index=True, sheet_name="2015")
rules.sort_values(['lift'], ascending=False)
re.findall('title="(.*?)"', slices[4])
conversion_prob = df2["converted"].mean() $ conversion_prob
male_journalists_retweet_summary_df[['retweet_count']].describe()
df['k'] = km.labels_
knn_10.fit(X_train, y_train)
pd.merge(df1,df3,how='outer')
df_ca[df_ca['ab_page'] == 1]['converted'].mean()
regressor = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result = regressor.fit()
top_20_breeds = tweet_archive_master['dog_breed'].value_counts().index[:20]
df.drop('_id', axis = 1,inplace=True)
ab_new = countries_df.set_index('user_id').join(ab_file2.set_index('user_id'),how='inner') $ ab_new.head()
list(Users_first_tran.dropna(thresh=int(Users_first_tran.shape[0] * .9), axis=1).columns)
testing.replace(u"\ufffd", "?")
t_cont_prob = df2[df2['group']=='treatment']['converted'].mean() * 100 $ output = round(t_cont_prob, 2) $ print("The probability of the treatment group individual converting regardless of the page they receive is: {}%".format(output))
df_twitter_copy.loc[707].text
nodes.info()
g = sns.catplot(x="AGE_groups", col="goodbad", $                  data=data, kind="count", $                  height=5, aspect=.9);
import statsmodels.api as sm $ logit = sm.Logit(df['converted'], df[['intercept', 'treatment']])
Temperature_year = session.query(Measurements.date, Measurements.tobs) \ $     .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \ $     .filter(Measurements.station == 'USC00519281').all() $ Temperature_year
readMe  = open("exampleFile.txt",'r').read() $ readMeList = open("exampleFile.txt",'r').readlines() $ print("readMe = ", readMe, "\n") $ print("readMeList = ", readMeList)
props.prop_name.value_counts()
df_joined = df2.join(df_countries.set_index('user_id'), on='user_id') # joining the 2 data frames on the 'user_id' column $ df_joined.head()
totals = df.groupby(['omg_outcome','public']).count()['abuse_number'].unstack().reset_index()
X_test.shape
Probas2 = pd.DataFrame(estimator.predict_proba(X2), columns=["Proba_Josh", "Proba_Matt"]) $ joined2 = pd.merge(tweets2, Probas2, left_index=True, right_index=True)
print(a.find('he')) $ print(a.rfind('he')) $ print(a.find('cat')) $ print(a.find('dog'))
print (mostRecentTweets.shape) $ print (tweetsOverall.shape) $ print (tidy.shape) $ tidy.to_csv('update/unfiltered-calc-agg-users.csv', index=False)
df_goog.head()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
pd.set_option( 'display.notebook_repr_html', False)  # render Series and DataFrame as text, not HTML $ pd.set_option( 'display.max_column', 10)    # number of columns $ pd.set_option( 'display.max_rows', 10)     # number of rows $ pd.set_option( 'display.width', 80)        # number of characters per row
rng_pytz.tz
results[results['type']=='Other']['tone'].value_counts()
model_sm.params.sort_values(ascending=False)
X_train_all = pd.concat([X_train_df, X_train.drop('title', axis=1)], axis=1) $ X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)
mentions_df_raw = pd.DataFrame(mentions_raw,columns=["epoch","src","trg","lang","text"]) $ print(len(mentions_df_raw))
facts_metrics.id.nunique()
with tf.Session() as sess: $     print(sess.run(tf_tensor)) $     print(sess.run(tf_tensor[0])) $     print(sess.run(tf_tensor[2]))
cutoff_times = generate_labels('/7/KMLZlMBnmWtb9NNkm3bYMQHWrt0C1BChb62EiQLM=',  trans, $                                label_type = 'MS', churn_period = 30) $ cutoff_times[cutoff_times['churn'] == 1].head()
frames = [df_prep17_, df_prep16_, df_prep15_, df_prep14_, df_prep13_, df_prep12_, df_prep11_, df_prep10_, $          df_prep9_, df_prep8_, df_prep7_, df_prep6_, df_prep5_, df_prep4_, df_prep3_, df_prep2_, df_prep1_, $          df_prep0_, df_prep99_, df_prep98_] $ df = pd.concat(frames)
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
r.loc[0, 'test-result-id']
len(set(df.title))
!grep -A 20 "INPUT_COLUMNS =" taxifare/trainer/model.py
new_converted_simulation = np.random.binomial(n_new, new_page_converted.mean(), 10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, old_page_converted.mean(), 10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
print(np.min(ndvi), np.mean(ndvi), np.max(ndvi))
techmeme.news_text = techmeme.news_text + ' ' $ techmeme = techmeme.groupby('date')['news_sources', 'news_text'].sum()
table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test'] $ table_list = [pd.read_csv(f'{fname}.csv', low_memory=False) for fname in table_names] $ for table in table_list: display(table.head())
df_main = pd.merge(df_clean, api_clean, on='tweet_id', how='inner')
h=sess.get_historical_data(['ibm us equity','aa us equity'],['px last','px open'])
plt.scatter(df["size"],df["age"]) $ plt.show()
ratings = tweet_archive_clean.text.str.extractall(r"(?P<numerator>\d+\.?\d*)\/(?P<denominator>\d+0)") $ ratings.numerator = ratings.numerator.astype(float) $ ratings.denominator = ratings.denominator.astype(float)
j = r.json() $
firstround = pd.read_csv("firstround.csv") $ firstround.head()
import pandas as pd $ file_name = "data/survey_Sorsogon_Electricity_Water_wbias_projected_dynamic_resampled_1000_{}.csv" $ sample_survey = pd.read_csv(file_name.format(2010), index_col=0) $ sample_survey.head()
test_preds = lr.predict(test_features)
len(data_get.keys())
calls_temp=pd.DataFrame(calls_df["length_in_sec"]) $ calls_temp=pd.DataFrame(KNN(k=3).complete(calls_temp),columns=calls_temp.columns) $ calls_df=calls_df.drop(["length_in_sec"],axis=1) $ calls_df["length_in_sec"]=calls_temp["length_in_sec"] $ calls_df.head()
mean = ... $ print("The average length of the tweets: {}".format(mean))
sh_max_df.tobs = sh_max_df.tobs.astype(float)
print('Churn ratio before the end of the trial period: ') $ print(data[(data['Shop_Status']=='disabled') & (data['Shop_Existence_Days']<30)]['Merchant_ID'].count()/a)
autos = autos[(autos["price"]>0) & (autos["price"]<=350000)] $ autos["price"].describe()
(df2.landing_page == 'new_page').mean()
print('RSS for the example model: {}'.format(get_residual_sum_of_squares(example_model, test_data, test_data['price'])))
malenew = malebyphase.rename(columns={"Offense":"Male"}) $ malenew.head(3)
final_grades_clean = final_grades_clean.dropna(axis=1, how="all") $ final_grades_clean
top_brands = autos['brand'].value_counts(ascending = False, normalize = True) > 0.05 $ top_brands = top_brands[top_brands[top_brands.index] == True] $ print(top_brands) $ print(top_brands.index.shape)
pd.Series(np.random.randn(3))
kickstarter[kickstarter["usd pledged"].isnull()][0:10]
data_df.desc[17]
twitter_archive[twitter_archive['name'].apply(len) < 3]
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group']=="treatment", 1, 0) $ df2.head()
pd.crosstab(index = sample["polarity"], columns = "count").plot.barh(rot=0)
pro_result = pro_table.sort_values('Profit', ascending=False) $ pro_result.head()
stmt = "select price_date from daily_price where price_date>='%s' and price_date<='%s' " % (start, end) $ pd.read_sql(stmt, engine)['price_date'].values
model_ADP = ARIMA(ADP_array, (2,2,1)).fit() $
T = price_mat.shape[0] $ nr_coins = price_mat.shape[1] $ price_mat.shape
df.to_csv(path, encoding='utf8', index=False)
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05/2))) $
bad_comments[3].replace('\r\r', ' ')
from goatools import obo_parser $ oboUrl = './data/go.obo' $ obo = obo_parser.GODag(oboUrl, optional_attrs=['def'])
datasets_ref = pd.read_csv('../list_of_all_datasets_dgfr/datasets-2017-12-13-18-23.csv', sep=';') $ datasets_slug_id = datasets_ref.set_index('slug')['id'].to_dict() $ datasets_id_slug = datasets_ref.set_index('id')['slug'].to_dict()
print("Actual diff:" , p_diff) $ p_greater = greater/len(p_d) $ print('Prop greater than actual diff:', p_greater) $ print('percentage: {}%'.format(p_greater*100))
got_data.head(10)
result.summary(title='Model Summary')
df_h1b_mv_ft.pw_1.describe()
df_imputed = pd.DataFrame(df_imput)
tobs_results = session.query(Measurement.station, Measurement.tobs).\ $ filter(Measurement.date.between('2016-08-01', '2017-07-31')).\ $          order_by(Measurement.tobs.desc()).all() $ tobs_results
test = datatest[datatest['rooms'].isnull()] $ train = datatest[datatest['rooms'].notnull()]
run txt2pdf.py -o"2018-06-18  2015 470 discharges.pdf"  "2018-06-18  2015 470 discharges.txt"
import nltk $ from nltk.stem import PorterStemmer $ import string
turnstiles_df = turnstiles_df.drop(["EXITS", "DESC"], axis=1, errors="ignore")
reason_for_visit.info()
df.set_index("id", inplace=True) $ df
group = sdf.groupBy('Cantons', window("_c0", "1 day")).agg(sum("Production").alias('Sum Production')) $ sdf_resampled = group.select(group.window.start.alias("Start"), group.window.end.alias("End"), "Cantons", "Sum Production").orderBy('Start', ascending=True) $ sdf_resampled.printSchema() $ sdf_resampled.show()
treatment_old.index
from IPython.core.display import display, HTML $ htmlFile = 'Data/titanic.html' $ display(HTML(htmlFile)) $
spark_df.registerTempTable("ufo_sightings")
missing_sample.dropna(axis=1)
priors_product = pd.merge(priors, products, on='product_id') $ priors_product.head()
dbl2 = dbl.reset_index() $ dbl2['first_date'] = dbl2.groupby('district_id').create_date.transform(min) $ dbl2['first'] = dbl2.create_date == dbl2.first_date $ dbl2.hist('install_rate', by='first') $
sns.violinplot(autos["odometer_km"]);
for (method, group) in planets.groupby('method'): $     print("{0:30s} shape={1}".format(method, group.shape))
most_active = df.sort_values("event_freq").reset_index(drop=True) $ most_active.head(5)
v_to_c['time'] = v_to_c.checkout_time - v_to_c.visit_time
gdax_trans_btc.plot(kind='line',x='Timestamp',y='BTC_balance_GBP', grid=True);
lm3 = sm.OLS(df_new['converted'], df_new[['intercept', 'UK', 'US', 'control']]) $ res = lm3.fit() $ res.summary()
testing_active_listing=Binarizer(10000) $ testing_active_listing.fit(X_test['Active Listing Count '].values.reshape(-1,1)) $ testing_active_listing_dummy=testing_active_listing.transform( $     X_test['Active Listing Count '].values.reshape(-1,1))
peakPricePerDay = dfEPEXpeak.groupby(dfEPEXpeak.index.date).aggregate('mean')['Price'] $ peakPricePerDay.head() # verify calculation
logs.fm_ip.unique()
    i ='MMM' $     stock_ts = pd.read_sql("select * from daily_price where ticker='%s' and instrument_type='stock'" % i, engine) $     stock_df = stock_ts[['price_date',  'adj_close_price']].copy().set_index('price_date') $     stock_df.rename(columns={'adj_close_price': i}, inplace=True)
all_311_requests = pd.concat([fifteen, sixteen, seventeen, eighteen], axis=0).sort_index()
for key, value in r.json()['dataset'].items(): $     print(key, ":\n", value, "\n")
df_amznnews_clsfd_2tick = df_amznnews_clsfd_2tick.set_index('publish_time') $ df_amznnews_clsfd_2tick.info()
df3 = df2.set_index('user_id').join(countries.set_index('user_id')) $ df3.head()
len([premieSn for premieSn in SCN_BDAY.scn_age if premieSn < 0])/SCN_BDAY.scn_age.count()
list(r_ord.values())[2] $
unique_buying_browsers = transactions.browser_id.unique()
len(apple.resample('M').mean())
Lab7_RevenueEPS.to_csv('Lab7_RevenueEPS.csv',index=False) $
from scipy.stats import norm $ z_score_sig = norm.cdf(z_score) $ critical_value = norm.ppf(1-(0.05/2)) $ print('Our z-score significant value is:',  round(z_score_sig, 4)) $ print('The critical value at 95% confidence is:', round(critical_value, 4))
df_null_acct_name['LinkedAccountId'].unique()
expiry = datetime.date(2015, 1, 5) $ msft_calls = pd.io.data.Options('MSFT', 'yahoo').get_call_data(expiry=expiry) $ msft_calls.iloc[0:5, 0:5]
class Example: $         self.instance_var = 'this is an instance var' $     def class_method(): $
res_c_i = res_c.merge(interval_c.to_frame(), left_index=True, right_index=True)
autos.price.value_counts().sort_index(ascending=False).head(20)
control = df2[df2['group'] == 'control'] $ size_control = control.shape[0] $ prop_conv_control = control[control['converted'] == 1].shape[0] / control.shape[0] $ prop_conv_control
df = pd.read_csv('data/test1.csv') $ df
df2 = df.copy()
weather['power_output_noisy'] = np.maximum(weather.power_output - weather.PV_noise, 0)
num_of_converted = df2[df2.converted == 1] $ p_new = len(num_of_converted)/len(df2) $ p_new
stock_data.shape
df_predictions.head()
exiftool -csv -createdate -modifydate cisnwf6/Cisnwf6_cycle3.MP4 > cisnwf6.csv
returns = generate_returns(close) $ project_helper.plot_returns(returns, 'Close Returns')
df2.shape
df.columns
df.info()
mysdc.sc = 'mms1' $ mysdc.instr = 'fgm' $ mysdc.mode = 'srvy' $ mysdc.level = 'l2' $ file = mysdc.Download()
bp.head()
df_dummies = pd.get_dummies(df_onc_no_metac[ls_other_columns]) $ dummy_colnames = df_dummies.columns $ dummy_colnames = [clean_string(colname) for colname in dummy_colnames] $ df_dummies.columns = dummy_colnames
notes.head()
import pandas as pd $ df = pd.read_csv("autos.csv") $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ print "Dimensiones en el dataset: ",df.shape $ df.head()
cur.execute('SELECT * FROM materials') $ cur.fetchall()  # fetch all the results of the query
def save_combined_df( df_to_save, file_name ): $     df_to_save.to_csv( f'./data/{file_name}.csv' )
print(autos['price'].unique().shape) $ print(autos['price'].describe()) $ print(autos['price'].value_counts().head(15))
df3.values
len(orders.user_id.unique())
pd.Timestamp(2015,11,11)
plt.hist(drt_avg16, bins=20, align='mid'); $ plt.xlabel('Defensive Rating') $ plt.ylabel('Count') $ plt.title("Histogram of Teams' Defensive Rating, 2016-2017 Season\n");
tips_train = tips.sample(frac=0.8, random_state=123) $ tips_test = tips.loc[tips.index.difference(tips_train.index),:] $ tips_train.shape, tips_test.shape, tips.shape
op = [] $ for row in json_data['dataset_data']['data']: $     op.append(row[1]) $ print (max(op), min(op))
temp_long_df.tail()
q3_results = session.query(Stations.name,Measurements.tobs).filter(Stations.station == Measurements.station,)\ $             .filter(Stations.name == 'WAIHEE 837.5, HI US').all() $ q3_results
val = np.zeros(len(dates)) $ sample = pd.DataFrame(val, index = dates) $ sample['Date']=sample.index
sns.pairplot(df, vars=["followers", "following", "public_repos", "public_gists"], hue='hireable');
average_polarity=pd.DataFrame() $ count_polarity=pd.DataFrame()
t[np.argmax(sp_rec)]
print(ozzy.name)
'AAbcEE'.replace('AA', 'BB')
print d.variables['trajectory']
table_rows = driver.find_elements_by_tag_name("tbody")[28].find_elements_by_tag_name("tr") $
df_t = df.query('group == "treatment"') $ df_c = df.query('group == "control"') $ n_mis = df_t.query('landing_page == "old_page"').user_id.count() + df_c.query('landing_page == "new_page"').user_id.count() $ n_mis
df_users.to_csv('data/twitter_users.csv', index=False)
df.groupby('author')['body'].count().sort_values(ascending=False)[:10]
condos.head()
ekos.load_data(user_id = user_id, unique_id = workspace_uuid)
a_list.append(another_list) $ a_list
stocks.describe()
tweet_archive_enhanced_clean = tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['retweeted_status_id'].isnull()] $ tweet_archive_enhanced_clean = tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['in_reply_to_status_id'].isnull()]
df_western = df[df['genres'].str.contains("Western")]
!hdfs dfs -cat 32ordered_results-output/part-0000* > 32ordered_results-output.txt $ !tail 32ordered_results-output.txt
countries_df = pd.read_csv('./data/countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ print(df_new['country'].unique()) $ df_new.head()
p_value = (normal_dist<diff).mean() $ p_value
q3 = pd.Period('2018Q1', freq='Q-JAN')
from tqdm import tqdm $ from time import sleep $ for i in tqdm(range(100)): $     sleep(0.1)
df_mas['rating_numerator'] = df_mas['rating_numerator'].astype(float) $ df_mas['rating_denominator'] = df_mas['rating_denominator'].astype(float)
days = np.array(['Monday', 'Tuesday', 'Wednesday', $                  'Thursday', 'Friday', 'Saturday', $                  'Sunday']) $ data_vi['Weekday'] = days[data_vi.index.weekday]
df_clean['tweet_id'].dtype
%time pax_raw = pd.read_hdf(os.path.join(data_dir, hdf_path), 'pax_raw_sample')
print(np.min(rhum), np.mean(rhum), np.max(rhum))
pd.crosstab(aqi['AQI Category_eug'], aqi['AQI Category_cg'], normalize='index', margins=True)
t_ave=(weather.TMAX+weather.TMIN)/2 $ weather['TAVG']=t_ave $ weather.info()
pd.concat(pieces)
transform = am.tools.axes_check(np.array([x_axis, y_axis, z_axis])) $ b = transform.dot(burgers) $ print('Transformed Burgers vector =', b)
sel_stats = [func.min(Measurement.tobs),func.max(Measurement.tobs), func.avg(Measurement.tobs)] $ session.query(*sel_stats).filter(Measurement.station == station_with_highest_observations).all()
print('Slope: Efthymiou vs experiment: {:0.2f}'.format(popt_axial_brace_saddle[2][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_saddle[2]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
input_data.snow_depth.value_counts()
print(np.min(prec), np.mean(prec), np.max(prec))
lots.ix[-1]
cryptos.to_json() $ cryptos.to_csv()
new_df.head()
pivoted.T[labels==1].T.plot(legend=False, alpha = 0.1);
dat[dat.zip.isnull()]
autos=autos.drop('name',1)
for j in top_tracks: $     top_tracks[j] = [(i.split(",")[0] + "," + i.split(",")[1] + "," + i.split(",")[2] + "," + i.split(",")[3]) for i in top_tracks[j]] $
dfData['sold_terms'] = ['Standard Sale' if (type(x) == NoneType) else x for x in dfData['sold_terms']]  # 'Standard Sale' is the most common. $ dfData['financing'] = ['Conventional' if (type(x) == NoneType) else x for x in dfData['financing']]  # 'Conventional' is the most common. $ dfData.head(10)
df.isnull().sum()
data[(data['author_flair'] == 'Saints') & (data['game_state'] == 'close')].comment_body.head(15) $
temp_df.plot.hist(by='tobs', bins=12) $ plt.show() $ plt.savefig("Max Tobs Station Histogram.png",bbox_inches="tight")
data.shape
df.count()
pipeline.fit(fb_train.message, fb_train.popular)
def format_sample(x): $     data = json.loads(x) $     data['timestamp'] = datetime.fromtimestamp(data['timestamp']).strftime('%Y/%m/%d %H:%M:%S') $     data['doc_id'] = data.pop('count') $     return (data['doc_id'], json.dumps(data))
data_df[data_df['cycles'] == 21]['created_at'].min()
input_data = pd.merge(mit, weather, on='date')
def plot_fi(fi): return fi.plot('cols', 'imp','barh', figsize=(6,7), legend=False) $ fi = pd.DataFrame({'cols':X_data.columns, 'imp':m.feature_importances_} $                       ).sort_values('imp', ascending=False) $ plot_fi(fi) $ plt.show()
positive_review_transformed = bow_transformer.transform([positive_review]) $ nb.predict(positive_review_transformed)[0]
print(loadedModelArtifact.name) $ print(saved_model.uid)
store.delete_collection('NASDAQ.EOD')
m_test = merged_test[['customer','frequency','recency','T']].set_index('customer') $ m_test.shape
returned_orders_data = combined_df.loc[combined_df['Returned'] == 'Yes'] $ print(returned_orders_data)
to_be_predicted_Day3 = 25.10910355 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
rf_vect = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1) $ scores = cross_val_score(rf_vect, X, y, cv=skf) $ print "Cross-validated RF scores based on vectorized titles:", scores $ print "Mean score:", scores.mean()
df2 = df.drop(df.query('landing_page == "new_page" & group == "control"').index) $ df2 = df2.drop(df2.query('landing_page == "old_page" & group == "treatment"').index)
ind = ind.drop('Unnamed: 0',axis=1)
duplicate_row_indexes = list(duplicate_rows.index) $ duplicate_row_indexes
local_sea_level_stations.columns = [name.strip().replace(".", "") $                                     for name in local_sea_level_stations.columns] $ local_sea_level_stations.columns
urls = r.json() $ urls
trn_y = train_small_sample.is_attributed $ val_y = val_small_sample.is_attributed
legLastOnly = leg[leg["first_name"].isnull()]
data.fillna({'year': 2013, 'treatment':2})
lastDay = lobbyFrame['Start'].max() # This is the end date of the line chart!! $ print(lastDay)
time.gmtime()
Meter1.QurreyProgress(debug=True)
for dataset in full_data: $     dataset['Embarked'] = dataset['Embarked'].fillna('S') $ print (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())
df_twitter_archive_copy.stage.dtype
ma = ffinal.groupby(['GAME_ID', 'season', 'TEAM_ID'])['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l'].sum().reset_index() $ mb = ffinal.groupby(['GAME_ID', 'season', 'TEAM_ID'])['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l'].sum().reset_index() $ ma.columns = ['game_id', 'team_id_ta', 'season', 'pts_la', 'ast_la', 'blk_la', 'reb_la', 'stl_la'] $ mb.columns = ['game_id', 'team_id_tb', 'season', 'pts_lb', 'ast_lb', 'blk_lb', 'reb_lb', 'stl_lb'] $ ma.head()
cFrame['Cumulative'] = cFrame.groupby('Client')['Change'].apply(lambda x: x.cumsum()) $ cFrame.head(20)
wine_reviews.shape # To check the number of rows and columns in our datset
agg_dict = {'Temp (deg C)' : ['min', 'median', 'max'], $             'Rel Hum (%)' : ['mean', 'std']} $ weather_all.groupby('Station Name').agg(agg_dict)
precipitation_df.plot() $
labels = ['acc_id','acc','tweet_id','text','created_time','ex_lat','ex_long','place','lot_sw','lot_ne','lang'] $ tweet_df = pd.DataFrame(tweet_list,columns=labels)
unstacked.head(10)
len(test_dict.keys())
train.head()
Lab7_Equifax.to_csv('Lab7_Equifax.csv',index=False)
data[data.name == 'jared'].head()
df.dropna(subset=['insert_id'], how='all') $ df = remove_duplicate_index(df)
multipoint_conn_pivot = pivot_condition_time(multipoint_df, "ecn.multipoint", "connectivity", $                                              ('works','broken','transient','offline','path_dependent','unstable')) $ multipoint_conn_pivot
dist = np.sum(bag, axis=0) $ dist
new_orgs.shape
appointments = pd.read_csv('./data/AppointmentsSince2015.csv')
df_combined.dtypes $
USvideos.tail(10)
df3.info()
best_ids_df = topics_data[topics_data.id.isin(best_ids)] $ display(best_ids_df)
df1.io_state[2]
x_train = dfTrain[['GrLivAreaNorm']].iloc[:nb_samples] $ x_test = dfTest[['GrLivAreaNorm']]
teams1 = list(df1.Home.unique()) $ teams2 = list(df2.Home.unique()) $ set(teams1) - set(teams2)
month3 = oanda.get_history(instrument_1, $                         start = '2018-3-1', $                         end = '2018-3-31', $                         granularity = 'M10', $                         price = 'A')
df_new['country'].value_counts()
TrainData.drop(['DOB', 'Lead_Creation_Date'], axis=1).to_csv('TrainData_clean.csv', index=False) $ TestData.drop(['DOB', 'Lead_Creation_Date'], axis=1).to_csv('TestData_clean.csv', index=False)
data.loc[data.LRank == 'NR', ['LRank']] = np.nan
asfr.describe()
matthew['SOCIAL_NETWORKS'] = matthew.text.apply(lambda text: pd.Series([x in text for x in SOCIAL_NETWORKS]).any()) $ matthew['DECISION_MAKING']  = matthew.text.apply(lambda text: pd.Series([x in text for x in DECISION_MAKING]).any()) $ matthew['ADAPTIVE_CAPACITY']  = matthew.text.apply(lambda text: pd.Series([x in text for x in ADAPTIVE_CAPACITY]).any())
client = MongoClient('ec2-34-198-246-43.compute-1.amazonaws.com', 27017) $ db = client.renthop $ collection = db.listings $ pp_bold('{} listings'.format(collection.count()))
query = session.query(Measurement) $ rain = query.filter(Measurement.date >= year_ago_str).all() $ print(len(rain))
df=pd.concat([df,gender_dummies],axis=1) $ df.head(2)
df_usa=df_usa.pivot(columns='Variable Name',values='Value') $ df_usa.head()
print(broadband.shape) $ print(broadband.head(10))
image_file=pd.read_csv('image-predictions.tsv',sep='\t')
dfAbes = df[df['Name'].apply(lambda x: x.split(' ')[0])=="ABE'S"]
f_ip_os_clicks.show(1)
contractor = pd.read_csv('contractor.csv') $ state_lookup = pd.read_csv('state_lookup.csv')
df1 = add_percentiles(df) $ df1.head()
to_be_predicted_Day4 = 26.69300296 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
class Widget:  # same as "class Widget(object):" $         print(self.__class__)  # __class__ is the easy way to get at an object's class $     @staticmethod $     def print_class():  # Static method as it has no 'self' parameter $
new_page_converted = np.random.choice([1, 0], size=n_new, p=[convert_rate_new, (1-convert_rate_new)]) $ new_page_converted.mean()
pd.Series(sales, index=pd.date_range(start='2018-01-01', periods=len(sales), freq='5T'))
load2017['date'] = load2017['time'].str.slice(0,10) $ load2017['date'] = pd.to_datetime(load2017['date']) $ load2017['date'].head()
vulnerability_histogram = cved_df.groupby(by=['cwe_id'])['cve_id'].count() $ vulnerability_histogram
df2[df2['landing_page'] == 'new_page'].user_id.count() / df2.user_id.count()
print('\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better. $ coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v') $ coherence_lda = coherence_model_lda.get_coherence() $ print('\nCoherence Score: ', coherence_lda)
df2['intercept'] = 1 $ df2.head()
plt.title('GC_MARK_MS_parent') $ aggregated_parent.plot(kind='bar', figsize=(15, 7))
price_series = pd.Series(brand_mean_prices) $ df = pd.DataFrame(price_series,columns=['mean_price']) $ km_series = pd.Series(brand_mean_km) $ df["mean_km"]=km_series $
df_goog.index.min(), df_goog.index.max()
print(df.apply(np.cumsum))
taxiData2.loc[taxiData2.Fare_amount <=0, "Fare_amount"] = 1
'{:,}'.format(100000.154787895444)
bp_medication_df.head()
df = pd.DataFrame(bmp_series, columns=['mean_mileage']) $ df
browser.click_link_by_partial_text('more info')
cityID = '27485069891a7938' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         New_York.append(tweet) 
d = r.json() $ d
geo_db.head()
y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf) $ y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)
print("Number of NAN values before: ", df_usa.isnull().sum()) $ df_usa=df_usa.fillna(0) $ print('\n') $ print("Number of NAN values after: ", df_usa.isnull().sum())
from pandas import ExcelWriter $ with ExcelWriter("../../data/all_stocks.xls") as writer: $     aapl.to_excel(writer,sheet_name='AAPL') $     df.to_excel(writer,sheet_name='MSFT')
my_image = tf.placeholder("uint8",[None,None,3]) $ slice = tf.slice(my_image,[10,0,0],[16,-1,-1])
forecast_data = (forecast_data $                  .merge(test_data) $                 ) $ forecast_data.head()
bc = pd.read_csv("bitcoinity_all.csv")
tweet_archive.describe()
master_list[master_list['Count'] >= 5]['Count'].describe()
import seaborn as sns $ sns.lmplot(x='TV',y='sales',data=data,fit_reg=True)
bad_dtype_df.index.set_levels([pd.to_datetime(bad_dtype_df.index.levels[0]), bad_dtype_df.index.levels[1], $                                bad_dtype_df.index.levels[2], bad_dtype_df.index.levels[3], $                                bad_dtype_df.index.levels[4], bad_dtype_df.index.levels[5]], $                                inplace=True) $ index_level_dtypes(bad_dtype_df)
df2[df2.user_id==773192]
!wget https://raw.githubusercontent.com/sunilmallya/mxnet-notebooks/master/python/tutorials/data/p2-east-1b.csv
df.filter(~a_sevenPointSeven).count()
logs['key'].resample('1T').count().plot() $ plt.show()
sc.stop()
df2[['ab_page', 'ab_page_old']] = pd.get_dummies(df2['landing_page']) $ df2 = df2.drop('ab_page_old',axis = 1) $ df2.head()
df1['volatility']=(df1['Adj. High']-df1['Adj. Close'])/df1['Adj. Close'] $ df1['volatility'].head()
sen_dat=twt_data.join(sentiment_data[['class','p_neg','p_pos']])
decoder_input = tf.reshape(caps2_output_masked, $                            [-1, caps2_n_caps * caps2_n_dims], $                            name="decoder_input")
areas_dataframe[areas_dataframe.company_id == cid]
tt_final[(tt_final.p1_dog == True)]['p1'].value_counts().head(10)
html = browser.html $ soup = bs(html, 'html.parser') $ url_hemispheres_link = soup.find_all('a', class_="itemLink product-item") $ url_hemispheres_link
df_vec_sums = df_vec.sum() $ df_vec_sums = df_vec_sums.sort_values(ascending=False) $ dftops = df_vec_sums[df_vec_sums>=8000] $ dftops
data['Session'] = pd.DatetimeIndex(data['Session']) $ data.head()
archive_version = '2017-11-10' # i.e. '2017-11-10'
fullData = pd.concat([tweets, sentiment_df], axis=1) $ fullData.head()
df['Sentiment'] = df['Tweet'].apply(lambda tweet: TextBlob(tweet).sentiment[0])
check_measurements.__dict__
support.groupby(['contributor_firstname','contributor_lastname'])['amount'].sum().reset_index().sort_values('amount', ascending = False)
df2['user_id'].nunique()
negative = '/Users/EddieArenas/desktop/Capstone/negative-words.txt' $ negative = pd.read_table(negative, encoding = "ISO-8859-1")
temp_series = pd.Series(temperatures, dates) $ temp_series
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page'")['converted'].sum() $ convert_old $
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23Penguins').items(100): $     results.append(tweet)
df.columns
df_all_wells_basic.info()
df_datecols = df_uro.select_dtypes(include=['<M8[ns]']).drop(columns=['index_date', 'lookback_date']) $ df_datecols.shape
browser = webdriver.Chrome('chromedriver.exe') $ browser.get(url) $ time.sleep(3) $ html = browser.page_source $ soup = BeautifulSoup(html, "html.parser") $
df.loc[df.category ==1022200, 'category_name'] = 'TEQUILA'
data.loc[data.expenses > 150000, 'expenses'] = np.NaN
test.shape
users = pd.read_sql_table('users', con=engine) $ customers = users[users.role == 0] $ cleaners = users[users.role == 1] $ leads = users[users.role == 3] $ applicants = users[users.role == 4]
print(api.VerifyCredentials())
train = pd.read_csv("../data/wikipedia_train3.csv") $ test = pd.read_csv("../data/wikipedia_test3.csv") $ test['date'] = test['date'].astype('datetime64[ns]') $ train['date'] = train['date'].astype('datetime64[ns]')
data = data[~data["Improvements"].isnull()] $ print("Number of surveys: ", len(data))
df1['Hour'] = pd.to_datetime(df1['Date'], format='%H:%M').dt.hour # to create a new column with the hour information $ df1.head()
df.columns
trips_data.groupby('weekday').duration.mean().reset_index().plot.scatter('weekday','duration') $ plt.show() # It can be seen that average duration per trip is higher on weekends
nr_rev ='the masala was great very yummy indeed, i wish the naan was not cold but the paneer and garlic rice' $ vectorizer.transform([nr_rev]) $ M_NB_model.predict_proba(vectorizer.transform([nr_rev]))
date = datetime.datetime.now()
df_countries.head()
df_vow['High'].unique()
for i in range(0,10): $     topics = model.show_topic(i, 10) $     print "%s: %s" %(i, (', '.join([str(word[0]) for word in topics])))
from sklearn.metrics import confusion_matrix $ confusion_matrix = confusion_matrix(y_test, y_pred_clf) $ print(confusion_matrix)
train.readingScore[train.male==1].mean()
print(a) $ print(a.index('dog')) # Get index of first matching entry; throws exception if not found $ print(a.count('cat'))  # Count the number of instances of an element
mb.swaplevel('Patient', 'Taxon').head()
list(db.fs.files.find())
df.head(2)
dfNiwot = df.loc[df["NAME"] == 'NIWOT, CO US'].copy() $ dfNiwot.head() 
graf_train=graf_train.drop('WordVec', axis=1)
import regex as re $ from nltk.corpus import stopwords
tweets[0]._json.keys()
contractor_clean[contractor_clean['contractor_id'].isin([382,383,384,385,386,387])]['contractor_bus_name']
bthlst = list(df_bthlst[df_bthlst["md_id"].isin(bth_dlst)]["booth_id"])
show_max = 20 $ columns_meta = list(set(click_condition_meta.columns) - set(not_fit) - set(clicking_conditions.columns)) $ for c in columns_meta: $     print(c, '\n', "unique number of values:", len(click_condition_meta[c].unique()), '\n', $           list(click_condition_meta[c].unique())[:show_max], '\n')
All_tweet_data=pd.merge(All_tweet_data, Imagenes_data_v2, on='tweet_id', how='left') $
yt.get_subscriptions(channel_id, key)
print('Python version ' + sys.version) $ print('Pandas version ' + pd.__version__) $ print('Matplotlib version ' + matplotlib.__version__) $ print('Numpy version ' + np.__version__)
prob_temp = df2.query("group == 'treatment'")["converted"].mean() * 100 $ print("Probability if user in treatment group: {}%".format(round(prob_temp, 2)))
pold = df2.converted.mean() $ pold
twitter_ar.to_csv('twitter_archive_edited.csv',index=False)
date + pd.to_timedelta(np.arange(12), 'D')
df['SA_p'] = np.array([ sentiment_finder_partial(comment) for comment in df['body'] ])
index_of_wrong_mapped_cols = df.query('(group=="treatment" and landing_page!="new_page") or (group=="control" and landing_page!="old_page")').index
sns.pairplot(subset_sessions_summary, hue="country_destination") 
df = pd.read_json("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Pompeu_Fabra_University/daily/2018020100/2018020300",orient='records')
query ="SELECT * FROM tddb_00.Weather_Log ORDER BY Log_Id DESC" $ df = pd.read_sql(query,session) $ df.head(10)
joined=join_df(joined,googletrend,["State","Year","Week"]) $ joined_test=join_df(joined_test,googletrend,["State","Year","Week"]) $ sum(joined['trend'].isnull()),sum(joined_test['trend'].isnull())
pd.Series([4, np.nan, 7, np.nan, -3, 2]).sort_values()
plt.hist(null_value); $ plt.axvline(x=obs_diff, color= 'red');
df[df.index.month.isin([5,6,7])]['Complaint Type'].value_counts().head()
kd915_filtered.head()
person = "KianMcIan" $ person_counts = df[df["screen_name"]==person].groupby(TimeGrouper(freq='30min')).agg({"id":"count"}).rename(columns={"id":"count"}) $ person_counts.reset_index(inplace=True) $ person_counts.head()
rollcorr_daily[['Ethereum', 'Ripple', 'Bitcoin Cash', 'Litecoin']].plot() $ plt.show()
print(df_train.shape) $ df_train.head(10)
for idx, match_id in full_dataset.match_id.iteritems(): $     date = data.loc[data.match_id == match_id, 'date'].tolist()[0] $     full_dataset.loc[idx, 'date'] = date
for i in range(0, df1.columns.size - 1): $     print('The unique values in the column called ' + df1.columns[i] + ' are:') $     print(df1.iloc[:,i].unique()) $     print('\n')
loan_stats["earliest_cr_line"].head(rows=2)
df[df['frauds']].sample(10)
temp_df = temp_df.reset_index()[['titles', 'timestamp']]
flight6.printSchema()
steemit_accounts = unique_urls[(unique_urls.domain == domain) & (unique_urls.url.str.contains('.com/@'))] $ steemit_accounts.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
n_new = df2[df2['group']=='treatment'].count()[0] $ n_new
coin_mean > .5
by_area['AQI'].plot(); plt.legend();
name = df_titanic['name'] $ print(name.describe())
pax_raw.info()
engine = create_engine("sqlite:///hawaii.sqlite")
autos['registration_year'].describe()
session.query(Measurement.station, func.count(Measurement.tobs)).\ $ group_by(Measurement.station).\ $ order_by(func.count(Measurement.tobs).desc()).all() $
zip_1_sns.Date = zip_1_sns.Date.str.replace('-',"") $ zip_1_sns.Date = pd.to_datetime(zip_1_sns.Date,format="%Y%m%d") $ zip_2_sns.Date = zip_2_sns.Date.str.replace('-',"") $ zip_2_sns.Date = pd.to_datetime(zip_2_sns.Date,format="%Y%m%d")
fin_coins_r.shape, fin_r_monthly.index.min(), r_top10_mat_na.index.min(), fundret.index.min()
twitter_archive.info()
weather.loc[weather.NAME == 'RALSTON RESERVOIR, CO US'].boxplot(column="TMIN");
df4[df4['converted']==1].count()[0]/df4.count()[0]
autos['odometer'].head()
df.shape
p_new = df2['converted'].mean() $ print (p_new)
nu_fission_rates = fuel_rxn_rates.get_slice(scores=['nu-fission']) $ nu_fission_rates.get_pandas_dataframe()
z1.interval.isnull().sum()
print("null size",sum(np.sum(np.array(tokendata.isnull()),axis=1)))
delte1 = [] $ for i in or_list1: $     if(i not in and_list1): $         delte1.append(i) $
crime_df.info()
injury_df['Date'] = injury_df['Date'].map(lambda x: x.replace('-','')) $ injury_df['Date'].head()
train['question_dt'].tail(5)
print(sentences[75]) $ analyzer.polarity_scores(sentences[75])
bad_dates = mapped.filter(lambda row: (row[3] == -2 or row[4] == -2)) $
evaluation_2 = api.create_evaluation(ensemble_2, test_dataset) $ api.ok(evaluation_2)
if using_sample: $         pax_raw.to_hdf(os.path.join(data_dir, hdf_path), 'pax_clean_sample') $ else: $     pax_raw.to_hdf(os.path.join(data_dir, hdf_path), 'pax_clean')
dt_features['deadline'] = pd.to_datetime(dt_features['deadline'],unit='s')
manager.image_df['p_hash'].isin(tree_features_df['p_hash']).describe()
rain_df = pd.DataFrame(rain) $ rain_df.head()
tweet_df[tweet_df.total == 1].sum().drop('total')
item1={'c1':('A','B','C','D'),'c2':np.random.randint(0,4,4)} $ item2={'c1':np.random.randint(0,4,5),'c2':np.random.randint(0,4,5)} $ pd.Panel({'one':item1,'two':item2})
print("Probability of user converting:", df2.converted.mean())
image_path1 = "pexels-photo-635529.jpeg" $ img1 = pil_image.open(image_path1) $ img1 = img1.convert('RGB')
rng = pd.date_range('2012-01-01', '2012-01-03')
df.groupby(['product_type','state'])['full_sq'].nlargest(5) $
public_table = public_table.rename(columns={'login':'user_login', 'created_at': 'date_open_sourced'})
vio2016 = vio2016.rename(index=str, columns={"description_y": "num_vio"}) $ ins2016 = pd.merge(ins2016, vi, on=["business_id", "date"]).rename(index=str, columns={"description": "num_vio"}) $ ins2016
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country']) $ df2.head(10)
df.sort_values('Year',ascending=True).head()
df_user_extract_copy.info()
df2_c = df2.query('group == "control"') $ df2_c.query('converted == 1').user_id.count()/df2_c.user_id.count()
calls_df["dial_type"].value_counts()
Z = np.linspace(0,1,11,endpoint=False)[1:] $ print(Z)
autos = autos[autos["registration_year"].between(1900, 2100)]
tweets_clean['dog_class'] = tweets_clean['dog_class'].astype('category')
config_path = os.path.join(config_root, model_name) $ config = Config.from_config_file(config_path) $ model = seed_model_from_config(config)
users['adopted'] = [user in adopted for user in users.object_id.values]
crimes[['PRIMARY_DESCRIPTION', 'SECONDARY_DESCRIPTION']].head()
ndvi_change= ndvi_of_interest02-ndvi_of_interest $ ndvi_change.attrs['affine'] = affine
train[ ['Date','Store']+columns].head()
display_all(train_data[['date','fullVisitorId']].isnull().sum()/len(train_data))
image_predictions_copy = image_predictions_copy[image_predictions_copy.p1_dog == True]
learn.save("dnn100")
naive_bayes_classifier = nltk.NaiveBayesClassifier.train(train_set)
test_float['balcon'] = 0 $ test_float.loc[test_float.description.str.contains('balcon|terraza', na=False), 'balcon'] = 1 $ test_float.balcon.value_counts()
charge = reader.select_column('charge') $ charge = charge.values # Convert from Pandas Series to numpy array $ charge
df2 = df.copy() $ df2.columns
df.query('group == "treatment" & landing_page != "new_page"').shape[0] + \ $ df.query('landing_page == "new_page" & group != "treatment"').shape[0]
banks[0].head(2).to_html("failed_banks.html") $ ! head -30 failed_banks.html
autos['unrepaired_damage'].unique()
recommendationTable_df = recommendationTable_df.sort_values(ascending=False) $ recommendationTable_df.head()
hmeq.head()
df["DISPOSITION_TYPE"].value_counts().sort_values(ascending=True).plot(kind='barh') $
os.environ["gs_key"] = "gs://resource-watch-public/" + s3_key_merge $ os.environ["asset_id"] = "users/resourcewatch/cit_014_areas_of_urban_development_merged" $ !earthengine upload image --asset_id=$asset_id $gs_key
hrefs = soup.find_all('a', href='http://www.pther-website.org/domains/example') $ hrefs
df_lm.filter(regex='grp_add_ass|last_month').boxplot(by='last_month', figsize=(10,10),showfliers=False)
df2.query('user_id == 773192') $
x_normalized.index = intersections_irr.index
print(gs_rfc_over.best_score_) $ print(gs_rfc_over.best_params_)
top_10_authors = git_log.author.value_counts().head(10) $ top_10_authors
n_old = df2[df2['group'] == 'control'].shape[0] $ print(n_old)
irmaFlorida = pd.read_csv('Data/bufferPrecipitation.csv',index_col='TIMESTEP') $ irmaFlorida
print(len(coins_infund), "different coins has been in top 10. Out of these,", $       len(intersection),"are now outside top 50." )
airbnb_df.loc[0:5, ['date_listed', 'listed_year_month']]
df_btc.head()
%%time $ df = pd.read_pickle("newRev_VegCols_US.pkl")
df.isnull().sum()/len(df)
feature_list = ["cmcnt_seg_tvl_p1y", "cmcnt_lh_seg_tvl_p1y", "cmsum_rev_tvl_p1y", "cmint_age_now"] $ vecAssembler = VectorAssembler(inputCols=feature_list, outputCol="features") $ feature_sel = vecAssembler.transform(all_feature)
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $
np.exp(-0.0408), np.exp(0.0099)
for ix, file in enumerate(s3_key_origs): $     s3_download.meta.client.download_file(s3_bucket, file, local_orig_keys[ix])
non_empty_crash_data_df = pd.read_csv('data/Clean_Crash_Data.csv') $ non_empty_crash_data_df.drop('Unnamed: 0',axis=1,inplace=True) $ non_empty_crash_data_df
cfs_df.TimeArrive = pd.to_datetime(cfs_df.TimeArrive) $ cfs_df.TimeCreate= pd.to_datetime(cfs_df.TimeCreate) $ cfs_df.TimeClosed = pd.to_datetime(cfs_df.TimeClosed) $ cfs_df.TimeDispatch = pd.to_datetime(cfs_df.TimeDispatch)
with open("IMDB_dftouse_dict.json", "r") as fd: $     IMDB_dftouse_dict = json.load(fd)
sales = graphlab.SFrame('Data/kc_house_data.gl/') $ sales.head()
data['Closed Date'] = data['Closed Date'].apply(lambda x: datetime.datetime. $                                                strptime(x,'%m/%d/%Y %H:%M')) $ data.info()
train.shape
wrd_api_clean = wrd_api.copy() $ wrd_api_clean = wrd_api_clean.transpose().sort_index(ascending=True) $ wrd_api_clean = wrd_api_clean[['id','favorite','retweet']] $ wrd_api_clean.rename(columns={'id':'tweet_id'}, inplace=True)
old_page_converted = np.random.choice([1,0],size=nold,p=[pmean, (1-pmean)]) $ old_page_converted.mean()
client = foursquare.Foursquare(client_id='1KNECHOW4ALXKWS4OWU2TEUZMPW0WUN1NORS2OUMWWBBCV4C', client_secret='O4QOOLKGDZK44DTBRPQIUDGO2Z4XQYYJQOJ0LN5E5FAQASMM', redirect_uri='http://fondu.com/oauth/authorize') $ auth_uri = client.oauth.auth_url()
run txt2pdf.py -o"2018-06-19 2015 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by discharges.pdf"  "2018-06-19 2015 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by discharges.txt"
pd.options.display.max_colwidth = 200 $ data.sort_values(by="score", ascending=False)[["author", "title", "score", "num_comments"]][0:19]
diff = prob_convert_t - prob_convert_c $ plt.hist(p_diffs); $ plt.axvline(x= diff, color = 'red'); $ plt.title("Histogram of P_diffs");
github_data.drop_duplicates('user_id').user_type.value_counts()
y_pred = lin_clf.predict(X_train_scaled) $ accuracy_score(y_train, y_pred)
n_old = (df2['landing_page'] == 'old_page').sum() $ n_old
dfX = df2.query('group == "control"') $ control_convert = dfX.converted.sum()/dfX.count()[0] $ control_convert
job_requirements.values
Dxs_summarized = {} $ for Dx in Dxs_info.keys(): $     pprint(Dxs_info[Dx][3]) $     Dxs_summarized[Dx] = input('What is the ailment?')
autos['price_dollars'].describe()
old_page_converted = np.random.choice([0,1],size = nold,p=[1-rate_pold_null,rate_pold_null]) $ print(len(old_page_converted))  
stop_words = set(stopwords.words('english')) $ stop_words.update(['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) $ for doc in document: $     list_of_words = [i.lower() for i in wordpunct_tokenize(doc) if i.lower() not in stop_words] $ stop_words.update(list_of_words)
features = features.drop('GEOID_y',axis=1) $ features = features.rename(columns={'GEOID_x':'GEOID'}) $ features.head(2)
df_new['intercept']=1 $ lm=sm.Logit(df_new['converted'],df_new[['intercept','UK*control','US*control','CA*treatment','UK*treatment','US*treatment']]) $ res=lm.fit() $ res.summary()
df2 = df.query('(group == "treatment" and landing_page == "new_page") or \ $               (group == "control" and landing_page == "old_page")')
rddScaledScores.reduce(lambda s1,s2: s1 + s2) / rddScaledScores.count()
pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 'Sue': ['Pretty good.', 'Bland.']})
!pip install --upgrade git+https://github.com/GeneralMills/pytrends $ !pip install lxml
pipeline = Pipeline(stages=stages_with_naive_bayes) $ model = pipeline.fit(trainingData) $ predictions = model.transform(testData)
new_page_converted.mean() - old_page_converted.mean()
old_page_converted = np.random.choice([1, 0], size = nold, p = [p_mean, (1-p_mean)]) $ print(len(old_page_converted))
df.shape
solar_corr = rets.corr() $ print(solar_corr)
user_profiles = [ $     {'user_id': 218, 'reinsurer': 'Partner Re'}, $     {'user_id': 219, 'reinsurer': 'General Re'}] $ result = db.profiles.insert_many(user_profiles) $
df = pd.DataFrame(file.generateRecords()) $ df.head()
X_train_all = pd.concat([X_train_df, X_train.drop('title', axis=1)], axis=1) $ X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)
def diff(x): $     return max(x)-min(x) $ ins_named["score"].groupby(ins_named["name"]).agg(diff).to_frame().sort_values("score", ascending=False)
old_page_converted = np.random.binomial(n_old,p_old,10000)/n_old $ print (old_page_converted)
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "The New York Times"]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "fantastic ms."]["tweet_vader_score"],fit_reg=False,marker = "*",scatter_kws={"color":"orange","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (The New York Time)",fontsize = 12) $ plt.savefig('Sentiment_The_Ny_Times.png')
prepared[['goal','backers_count','days_open']].describe()
data_2018 = data_2018.reset_index()
print(data.iloc[6,0])
data = data.loc[(data.place_with_parent_names.str.contains('Capital Federal')) | \ $                 (data.place_with_parent_names.str.contains('G.B.A.'))]
data.values[0]
bwd.drop('Store',1,inplace=True) $ bwd.reset_index(inplace=True)
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
p_diffs = [] $ for _ in range(10000): $     samp = df2.sample(df2.shape[0], replace=True) $     p_diffs.append(samp.query('landing_page == "new_page"').converted.mean() -\ $                    samp.query('landing_page == "old_page"').converted.mean()) $
pd_aux=pd.DataFrame(building_pa_prc_shrink[['permit_number','permit_creation_date']][filt_M]) $ pd_aux.sort_values(by=['permit_creation_date'],inplace=True) $ pd_aux.head(20)
print(len(plan['plan']['itineraries'])) $ print(plan['plan']['itineraries'][0].keys())
for col in time_cols: $     df[col] = df[col].astype('datetime64[s]')
station_count = session.query(Stations.id).count() $ print (f"Station Count = {station_count}")
df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new['US_ab_page'] = df_new['US'] * df_new['ab_page'] $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'UK_ab_page', 'US', 'US_ab_page']]) $ results = log_mod.fit() $ results.summary()
archive_df.rating_numerator.value_counts(sort=False)
df2.query('landing_page == "new_page"').user_id.nunique()
df_combined.head(5)
odds.tail(3)
cutoff = 0.75 $ base_filename = '04-15-2018_SubsetFeatures-TestTrainVal-SMOTE-XGBoostRecallError-ParamOptCV'
reg_logit = LogisticRegression(random_state=1984, C=0.01) $ reg_logit.fit(x_train_advanced, y_train_advanced)
uber_15["hour_of_day"].value_counts()
case_mask = (lower_vars.str.contains('new') $              & (lower_vars.str.contains('case') | lower_vars.str.contains('suspect')) $              & ~lower_vars.str.contains('non') $              & ~lower_vars.str.contains('total'))
issue_category_mapping = pd.read_csv('https://docs.google.com/spreadsheets/d/' + $                    '1DTDBhwXj1xQG1GCBKPqivlzHQaLh2HLd0SjN1XBPUw0' + $                    '/export?gid=0&format=csv') $ issue_category_mapping.head(5)  # Same result as @TomAugspurger
hobbies_learned.findall(r"<\w*> <and> <not> <\w*>")
pn_qty[pn]['storeinfo'].append(table_store.ix[0])
n_neg = n_pos * 10 $ train_neg = train_sample.filter(col('is_attributed')==0).orderBy(func.rand(seed=seed)).limit(n_neg).cache() $ print("number of negative examples:", n_neg)
!less 'data/yellow_tripdata_2017-12.csv'
df_input_clean.filter("`Resp_time` <= 0").count()
metrics.adjusted_rand_score(labels, km.labels_)
image_predictions.status_code
df.groupby('userid')['price'].max()
nb = naive_bayes.GaussianNB() $ cv_score = cross_val_score(nb,X_train,y_train,cv=10)
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2['ab_page'].mean()
df_state_votes.hill_trump_diff.hist(bins=np.arange(-100, 105, 5))
srctake = sourcetake.stack().to_frame() $ srctake.rename(columns={'0':'Use (ac-ft)'},inplace=True) $ srctake.reset_index(inplace=True) $ srctake['Year'] = pd.to_numeric(srctake['Year'],errors='coerce') $ srctake = srctake[(srctake['Year']<=datetime.today().year)&(srctake['Year']>=1000)]
intersections_final_for_update_no_dupe.info()
(autos["last_seen"].str[:10] $  .value_counts(normalize=True) $  .sort_index(ascending=True) $ )
titanic.count()
menu_dishes_about_vect = menu_dishes_about_vectorizer.fit_transform(menu_dishes_to_analyze['about']) $ menu_dishes_about_feature_names = menu_dishes_about_vectorizer.get_feature_names() $ menu_dishes_about_latent_features = run_sklearn_nmf(menu_dishes_about_vect, menu_dishes_about_feature_names, 10)
with open(os.path.join("../datasets/CSVs/", csvfile), "r") as f: $     data2 = csv.DictReader(f) $     for row in data2: $         print(row)
pax_raw = pax_raw.merge(n_user_days, on='seqn', how='inner')
authors_count.sort_values('count', ascending=False).head()
unigram_feats = sentim_analyzer.unigram_word_feats(allNeg, min_freq=4) $ len(unigram_feats) $ sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)
f_lr_hash_modeling2.cache()
twitter_ar = twitter_ar[twitter_ar.rating_num<=17.0] 
group=class_merged.groupby(['date'],as_index=False) $ daily_sales=pd.DataFrame(group['sum_unit_sales'].agg('sum')) $ pd.DataFrame.head(daily_sales)
print 'Database storage size = ', db.command("dbstats")['storageSize']/1024, 'KB' $ print
kd915 = pd.read_csv('/Users/auroraleport/Documents/LePort_git/9_15_full')
autos['price'].value_counts().sort_index(ascending=False).head(10)
url = form_url(f'organizations/{org_id}/playerPositions', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response.json()['items'])
measure.dtypes
np.sum(df["past_percent_cancelled"].isnull())
data1_new=data1_new.reset_index(drop=True) $ data1_new.head()
StockData.describe()
from sklearn.ensemble import RandomForestRegressor $ start_fit = time.time() $ reg1 = RandomForestRegressor(random_state=rs, n_estimators=200, n_jobs=-1) $ reg1.fit(X_train, y_train.ravel()); $ end_fit = time.time()
pd.Series(topic_preds).value_counts(normalize=True).plot.barh() $ plt.title('Topic distribution in Queensland floods', fontsize=20) $ plt.xlabel('Proportion') $ plt.ylabel('Topic') $ plt.show()
y_predicted = fit3.predict(X3) $ plt.plot(y3, y_predicted, 'b.') $ plt.title('Actual Gross Outcome vs. Budget') $ plt.xlabel('Budget') $ plt.ylabel('Domestic Gross Total')
df = df.selectExpr("_c0 as user_screen_name", "_c1 as created_at","_c2 as tweet_type","_c3 as followers_count","_c4 as tweet_text") $ df.show()
sns.swarmplot(x='Senate_sentiment', y="tweet_sentiment", data=df5) $ plt.title('Tweet Sentiment by State Compared to Senate Sentiment by State') $ plt.ylim(-0.05, 0.2) $ plt.show()
totalConvs_month = pd.DataFrame(df_convs_master.groupby(['year', 'month'], as_index=False).sum()) $ totalConvs_month.drop('day', axis=1, inplace=True) $ print 'Total number of conversions by month: ', '\n', 'DataFrame totalConvs_month', totalConvs_month.shape $ totalConvs_month
df_train.index = pd.to_datetime(df_train['date']) $ df_train.drop('date', axis=1, inplace=True) $ df_test.index = pd.to_datetime(df_test['date']) $ df_test.drop('date', axis=1, inplace=True)
df[df['Complaint Type'] == 'Noise - Residential']['Descriptor'].value_counts()
def collect_tweet_text(tizibika): $     tweets=[] $     for tweet in tizibika['text']: $         tweets.append(tweet) $     return tweets $
data = data.reset_index(drop = True)
twitter_ar.shape
a_df.size $ b_df.size
affair_children = pd.crosstab(data.children, data.affair.astype(bool)) $ affair_children.div(affair_children.sum(1).astype(float),axis=0).plot(kind='bar', stacked=True) $ plt.title('Affair Percentage by no of children') $ plt.xlabel('no of children') $ plt.ylabel('Percentage')
df2[df2['group']=='treatment'].head()
df.drop(df.query("group == 'treatment' and landing_page != 'new_page'").index, axis = 0,inplace = True) $
cleaned2['text'].head(10)
zn_array = np.loadtxt(os.path.join("Freyberg_truth","hk.zones")) $ plt.imshow(zn_array)
original_sports = pd.Series({'Archery': 'Bhutan', $                              'Golf': 'Scotland', $                              'Sumo': 'Japan', $                              'Taekwondo': 'South Korea'}) $ original_sports
len(list(set(have_seen_two_versions) & set(df_click.user_session.unique())))
f_counts_week_channel = spark.read.csv(os.path.join(mungepath, "f_counts_week_channel"), header=True) $ print('Found %d observations.' %f_counts_week_channel.count())
year_prcp_df = pd.read_sql_query(year_prcp.statement, engine,index_col = 'date') $ year_prcp_df.head()
csvDF.columns=['CustID','Fname','Lname','Age','Job']
predict.predict_score('Stuart_Bithell')
df_old = df2.query('group == "control"') $ n_old = df_old.shape[0] $ n_old
contribs.iloc[imin] 
df_merge.columns
tweets=[] $ for page in tweepy.Cursor(api.user_timeline, screen_name='HillaryClinton').pages(): $     tweets.append(page)
df[df.tweet_id.isin([1299, 1298, 1300, 1301])].sort_values(by='created_at')[['author_id','created_at','text']]
companyNeg.columns
state_lookup.info()
daily = data.resample('D').sum() $ daily.rolling(30, center=True).sum().plot(style=[':', '--', '-']) $ plt.ylabel('mean hourly count')
model.summary()
jobs.loc[(jobs.MAXCPUS == 600) & (jobs.GPU == 0)].groupby('Group').JobID.count().sort_values(ascending= False)
transactions[~transactions['TransactionID'].isin(dfTemp['TransactionID'].values)]
import matplotlib.pyplot as plt $ import numpy as np $ plt.plot(np.random.rand(50).cumsum())
from sklearn.feature_selection import RFE $ rfe = RFE(classifier,7 ) $ rfe = rfe.fit(data,affair) $ print(rfe.support_) $ print(rfe.ranking_) $
schumer.head()
ax1 = sns.boxplot(x = tweets_clean['favorite_count']) $ ax1.set(ylabel = 'favorite_tweets')
plt.hist(commits.log_commits, bins=50) $ plt.xlim((1,8)) $ plt.title('Distribution of Commits per Repo') $ plt.xlabel('Log(commits per repo)') $ plt.ylabel('Count') $
active.info()
num_plots = plot.regression_contour_sgd(x, y, diagrams='../slides/diagrams/ml')
csv_paths = sorted([os.path.join(data_dir, fname) for fname in os.listdir(data_dir) $                     if fname.endswith('.csv')])
df1 = pd.DataFrame(list(range(20)))
df.dtypes.head(20)
new_page_conv = np.random.binomial(n_new,p_new) $
states.index
geometry = [Point(xy) for xy in zip(g['X Coordinate (State Plane)'], g['Y Coordinate (State Plane)'])] $ crs = {'init': 'epsg:2263'} $ g_geo = gpd.GeoDataFrame(g, crs=crs, geometry=geometry)
tt_final = ttarc_clean.merge(tt_json_clean, left_on = 'tweet_id', right_on='id', how='outer') $ tt_final = tt_final.merge(imgp_clean, left_on = 'tweet_id', right_on='tweet_id', how='outer') $ tt_final.timestamp = pd.to_datetime(tt_final.timestamp, infer_datetime_format=True) $ tt_final.info()
eval_df = pd.DataFrame(predictions, index=model_dates).T
sns.lmplot(x="apparentTemperatureHigh", y="subjectivity", data=twitter_moves,lowess=True,size=8,aspect=1.5)
dataset = pd.read_csv('Raw_Data.csv', encoding='latin-1').fillna(0) $ dataset.head()
[np.exp(0.0507),np.exp(0.0408)]
journalists_mention_summary_df = journalist_mention_summary(journalists_mention_df) $ journalists_mention_summary_df.to_csv('output/journalists_mentioned_by_journalists.csv') $ journalists_mention_summary_df[journalist_mention_summary_fields].head(25)
tweet_archive_clean = tweet_archive_clean[tweet_archive_clean['retweeted_status_id'].isnull()]
df2.date.value_counts().sort_index(axis = 0)
df2.head()
df.sort_values(['Year'], ascending=True).head(5)
train.groupby(['unique_items'])['project_is_approved'].mean().reset_index()
speakers.whylisten = speakers.whylisten.apply(lambda x: re.sub("<.*?>", "", x))
unsorted_df.sort_values(by='col1',ascending=False)
edges = list(zip(edgereader.source, edgereader.target)) #create tuple for every pair from the dataset
list.sort() $ print(list)
(data["comms_num"]>6).sum()
click_condition_meta['dvce_type'] = np.where(click_condition_meta.user_id == "1f336e8c-d658-4656-bd87-aae8995e2725", 'Mobile', click_condition_meta.dvce_type)
twitter_archive_df['timestamp'].value_counts(ascending=True)[:10]
df_unique_users_treatment = df2[df2['group'] == 'treatment'] $ n_unique_users_treatment = len(df_unique_users_treatment) $ n_conversion_treatment = len(df_unique_users_treatment[df_unique_users_treatment['converted'] == 1]) $ probability_treatment = n_conversion_treatment/n_unique_users_treatment $ print ("The probability of an individual converting from the treatment group: {:.4f}".format(probability_treatment))
datAll['half_year'].value_counts(dropna=False)
def unix_to_datetime(i): $     u = datetime.datetime.fromtimestamp( $         int(i) $     ).strftime('%Y/%m/%d %H:%M') $     return u
print(a) $ a.reverse()  $ print(a)
regression_models = ws.models(tag = TICKER) $ for m in regression_models: $     print("Name:", m.name,"\tVersion:", m.version, "\tDescription:", m.description, m.tags)
df_archive_clean.drop(["in_reply_to_status_id","in_reply_to_user_id"], axis =1, inplace = True)
groceries = pd.Series(data=[30, 6, 'Yes', 'No'], index=['eggs', 'apples', 'milk', 'bread']) $ print(groceries) $ print(groceries * 2) $
new_texas_city.tail(10)
round(final_topbikes['Distance'].sum())
n=13 $ df['Close'].pct_change(n).mean()
from microsoftml_scikit.utils.exports import dot_export_pipeline $ dot_vis = dot_export_pipeline(pipeline, ds_train) $ print(dot_vis)
dfSavours = df[df['Name'].apply(lambda x: x.split(' ')[0])=="SAVOURS"]
fb = pd.read_json(json.dumps(res_json['posts']['data']))
print(today.strftime("Today is %Y-%m-%d"))
cp311[['status']].groupby([cp311.borough]).count()
np.exp(results_1.params)
horror_readings.head()
a = re.match('AA', 'AAbc')
def remove_data_outside_window(df): $     window_start = datetime.datetime(2018,4,1,23,0) $     window_end = datetime.datetime(2018,4,2,0,0) $     return df[(df.client_event_time >= window_start) & (df.client_event_time < window_end)]
gender.value_counts()
tt_json.describe()
print(XGBClassifier.feature_importances_)
df.rename(columns={'value':'lux'},inplace=True)
puppos = df1_clean[df1_clean.type == 'puppo'].count()[0] $ puppers = df1_clean[df1_clean.type == 'pupper'].count()[0] $ doggos = df1_clean[df1_clean.type == 'doggo'].count()[0] $ floofers = df1_clean[df1_clean.type== 'floofer'].count()[0] $ puppos, puppers, doggos, floofers
s = pd.Series([1,2.3,np.nan,"a"]) $ s
from google.colab import files $ uploaded = files.upload()
clean_train_df = pd.DataFrame(parse_data(train_df['text'].tolist()))
pd.read_html(driver.page_source)[0]
ca_cities_list = sorted(df_h1b[df_h1b.lca_case_employer_state=='CA'].lca_case_workloc1_city.unique())
rdd_from_df = sqlContext.sql("SELECT * FROM dataframe_name")
model.doesnt_match("france england germany berlin".split()) $
data['moving_avg'] = pd.rolling_mean(data['sentiment'], 500)
import os $ %matplotlib inline $ import pandas as pd $ import seaborn as sns $ import matplotlib.pyplot as plt
%%time $ max_key = max( r_dict.keys(), key = get_daily_chg ) $ print('largest change in price in any one day: '+ str( get_daily_chg(max_key) ) )
pd.date_range(start, periods=5, freq='B')
df.drop(columns=['year2','year3'])  # drop multiple columns
infinity.head(10)
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2012-02-28', '2012-03-05'))
s1 = pd.Series([True, False, False, True]) $ s1
threeoneone_geo = threeoneone_geo.to_crs({'init': 'epsg:4326'}) $ census_withdata = census_withdata.to_crs({'init': 'epsg:4326'})
morning_rush = last_year[(last_year["date"].dt.weekday < 5) & (last_year["date"].dt.hour > 5) & (last_year["date"].dt.hour < 10)] $ print(morning_rush.shape) $ last_year.shape
n_user_days.hist() $ plt.axvline(x=4, c='r', linestyle='--')
Test.SetFlowValues(6.5, 9.5, 0.45)
lookup_sentiment("Thanks! I still think it needs a bit more work, but.")
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31& \ $ api_key=dxpXisifsxgCyjy8xgZs") $
df2 = df1.join(df1_stdev, how='inner')
df.groupby('Year')[['Points','Rank']].agg(np.sum)
df.plot();
metadata['wavelength'] = refl['Metadata']['Spectral_Data']['Wavelength'].value $ metadata
price_by_brand = {} $ for b in brands: $     price_by_brand[b] = autos.loc[autos['brand']==b, 'price'].mean() $ price_by_brand
temp_df = weather_mean[['Temp (deg C)']] $ temp_df.head()
print("Action space:", env.action_space) $ print("Action space samples:") $ print(np.array([env.action_space.sample() for i in range(10)]))
df =df.dropna(subset=['description'])
from sklearn.preprocessing import StandardScaler $ import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline $ import re
inCSV = dataDir+'input/new-york_new-york_points.csv' $ df = pd.read_csv(inCSV) $ df = df[['X','Y','osm_id']] $ df.columns = ['longitude','latitude','osm_id'] #just delcare col names here $ df.to_csv(dataDir+'processing/new-york_new-york_points.csv', index=False)
features = ["Log_price","num_photos","created_month","created_day","bathrooms", "bedrooms", $             "latitude","longitude","description_score","num_features","num_description", $             "created_dayofyear","manager_level_low","manager_level_medium","manager_level_high", "display_address","street_address"] $ X = train_df[features].iloc[:,0:].values
sorted_precip = index_date_df.sort_values(by=['date']) $ sorted_precip.head()
from datetime import datetime $ pd.Timestamp(datetime(2015,11,11))
ridge3 = linear_model.Ridge(alpha=1) $ ridge3.fit(x3, y) $ (ridge3.coef_, ridge3.intercept_)
transfer_duplicates.apply(lambda row: smoother_function_part2(row["Year"], row["Month"], row["Day"], row["Smoother"]), axis=1);
df2[df2['converted'] == 1].shape[0] / df2.shape[0]
other_text_feature = vect1.fit_transform(talks['text'].as_matrix())
top_supporters = support.groupby(["contributor_firstname", "contributor_lastname"] $ ).amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
import random $ sample = all_data_merge.sample(n = 1000, replace=True) $ sample.to_csv('Nestle_sample_March.csv', index = False, sep = ',', encoding = 'utf-8')
from pyspark.sql.types import *
weather.info()
run txt2pdf.py -o "FLORIDA HOSPITAL title_page.pdf"  "FLORIDA HOSPITAL title_page.txt"
test1.head(10)
normal,malicious = plots.top_n_ports(10) $ print(normal) $ print(malicious)
f1_A = lv_workspace.get_data_filter_object(step=1, subset='A') $ f1_A.include_list_filter
def count_non_null(df, fld): $     has_field = np.logical_not(pd.isnull(df[fld])) $     return has_field.sum(), df.shape[0]
my_df["company_create"] = df_user.groupby('nweek_create')['company_id'].nunique() $ my_df["company_active"]   = df_user.groupby('nweek_active')['company_id'].nunique()
data.drop(['Colorado', 'Ohio'])
data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)
all_cards.loc["Lightning Bolt"]
new_page_converted.mean() - old_page_converted.mean()
df2[df2['group'] == "treatment"]['converted'].mean()
dfX = df2.query('group == "treatment"') $ treatment_convert = dfX.converted.sum()/dfX.count()[0] $ treatment_convert
dfg = dfg.set_index(['drg3', 'discharges']) $
def print_groups(groupobject): $     for name, group in groupobject: $         print(name) $         print(group.head())
tw.head()
df_clean.info()
print(twitter_data_v2.shape) $ print(tweet_data_v2.shape) $ print(Imagenes_data_v2.shape) $
df.query("group == 'treatment' and landing_page == 'old_page'").count()[0] + df.query("group == 'control' and landing_page == 'new_page'").count()[0] $
data_get['SCA'].Close.plot()
average_range = df['range'].mean()
hours['pred'] = linreg.predict(X) $ plt.scatter(hours.hour, hours.start) $ plt.plot(hours.hour, hours.pred, color='red') $ plt.xlabel('hours') $ plt.ylabel('start') $
station_total = station_by_week.reset_index().groupby(['STATION'])['entries_diff'].sum().reset_index() $ station_total = station_total.sort('entries_diff', ascending = [0]) $ print "Top 10 Stations for Entry Volume: (8/29/2016 - 9/16/2016)" $ station_total.head(10).set_index('STATION')
df['country'].value_counts().to_frame().head(20).index #select top 20 country names
df.convert_objects(convert_dates='coerce',convert_numeric=True)
psy_df3 = QLESQ.merge(psy_df2, on='subjectkey', how='right') # I want to keep all Ss from psy_df
df2.rename(columns = {'treatment': 'ab_page'}, inplace=True) $ df2.drop('control', axis=1, inplace=True) $ df2.head()
from sklearn.ensemble import RandomForestClassifier $ clf = RandomForestClassifier(max_depth=2, random_state=0) $ clf.fit(X_tr[0:len(X_train)-40-1], y_ls) $ print(clf.feature_importances_)
df.head().to_json("../../data/stocks.json") $ !cat ../../data/stocks.json
newfile.insert(0, 'CSCA - Add notes here of customer requirement', '') $ newfile.insert(1, 'Direction from Diana', '')
y_pred_submit = rfr.predict(x_test) $ np.savetxt("predictions_used_car.csv", y_pred_submit, '%.5f', delimiter = ',')
ttt = np.argmax(out_df[['low','medium','high']].as_matrix(),axis=1)
rf = RandomForestClassifier() $ rf.fit(X_train, y_train) $ rf.score(X_test, y_test)
df2_treatment=df2.query('group=="treatment"') $ p_treatment=df2_treatment['converted'].sum()/len(df2_treatment) $ p_treatment
educ_freqs = [0.112, 0.291, 0.1889, 0.096, 0.202, 0.111]
cur.execute("use mysql;") $ cur.execute("show tables;") $ for r in cur.fetchall(): $    print(r)
inflation = pd.read_csv("inf.csv") $ print inflation
austin[austin['total_fare'].isnull()].index.tolist()
np.log(bacteria)
requests.get("https://demo.emro.info/api/aggregate/tot_1/1" $             ).json()
def median_of_column(path, col_name): $     with TablesBigMatrixReader(path) as reader: $         col_indices = reader.get_col_indices_by_name([col_name]) $         return np.median(reader[:, col_indices])
train = hn[hn.created_at < july_1].copy() $ new = hn[hn.created_at >= july_1].copy()
results.coordinates
fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 7,8], feature_names=X_test.columns, grid_resolution=70, n_cols=7) $ fig, axs = plot_partial_dependence(clf, X_test[X_test.age_well_years != 274], [ 10], feature_names=X_test.columns, grid_resolution=70, n_cols=7) $
from IPython.display import HTML $
urlb='http://movie.naver.com' $ urls='/movie/sdb/rank/rmovie.nhn?sel=cur&date=20170806' $ page=urlopen(urlb+urls) $ soup=BeautifulSoup(page,'html.parser')
a_list.remove(1) $ a_list
rain_period = sf_small_grouped[sf_small_grouped['Rain'] == 1].start_date
plt.show()
df.head()
print(lr_gd.best_params_) $ print(lr_gd.best_score_)
pd.set_option('display.float_format', lambda x: '%.3f' % x) $ autos['price'].describe()
mr = dd.from_pandas(rentals, npartitions=3)
df2.shape
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)
pysqldf("select * from genes where genomic_accession = 'CP020543.1' and start > 3253706 and end < 3303410")
def get_gender(name): $     urlprefix = 'https://api.genderize.io/?name='  # Define url prefix $     r = requests.get(urlprefix + name.lower()) $     return r.json()['gender']
dates_list=list(dates_list) $ dateset=pd.DataFrame({'date':dates_list,'type':'train'}) $ dateset=dateset.sort_values(ascending=True,by="date") $ dateset.head()
df.airline.unique()
my_gempro.set_representative_structure() $ my_gempro.df_representative_structures.head()
dataA = pd.read_csv('data/dataPorUbicacion_Anios.csv', header=None)
np.array(p_diffs) $ plt.hist(p_diffs);
gpCreditCard.Hour_of_day.describe()
kochdf.head(10)
ab_CA = 1/np.exp(-0.0469) $ ab_UK = 1/np.exp(.0314) $ print(ab_CA) $ print(ab_UK)
movie_df.head()
actual_diff=actual_pnew-actual_pold $ (p_diffs>actual_diff).mean()
df_country_join = df_country.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ print (df_country_join.head())
full.groupby(['PastPCPVisits'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False) $
old_prob = gb.count().values / sum(gb.count().values)
from sklearn import tree $ model = tree.DecisionTreeClassifier() $ print ('Decision tree') $ reg_analysis(model,X_train, X_test, y_train, y_test)
print('Max time:', data.created_at.max()) $ print('Min time:', data.created_at.min())
countries = pd.read_csv('countries.csv') $ countries.head()
def get_active_on_date(DF_str, date_str): $                date=date_str) $     active = spark.sql(query).cache() $     return active
lasso2 = Lasso(alpha=0.0002) $ lasso2.fit(train_data, train_labels)
all_indicators.ix[:, 0:1]
df.shape
df.info()
features_df.shape
search['trip_start_weekday_m'] = search.apply(trip_start_weekday_m, axis=1)
tw_clean = tw_clean.drop(tw_clean[tw_clean.text.str.contains('RT @')].index)
df_l = df_vu.append(df_cb) #creating a new dataset
open_users['TEST'] = open_users['USERNAME'].map(lambda name: name in approved_users)
import requests $ election_results = 'https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state' $ response = requests.get(election_results)
total_payments = np.sum(sortdf['disc_times_pay'])/1000000 $ percent_of_total = lambda x:round( x*100/total_payments,1) $ percent_of_total_list = list(map(percent_of_total, my_list)) $ percent_of_total_list[:10]
row_len=df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')").shape[0] $ print('Number of times new_page and treatment dont line up :: ',row_len)
treat_convert = df2.query('group == "treatment"')['converted'].mean() $ treat_convert
n_old = df2[df2['group'] == 'control']['converted'].count() $ n_old
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index()[:40].plot(kind="bar", title="Ad_created (1-40th)", colormap="Blues_r")
mean_sea_level.plot(kind="kde", figsize=(12, 8))
attend_with.columns = ['ATTEND_'+str(col) for col in attend_with.columns]
from sklearn.feature_extraction.text import TfidfVectorizer $ tfidf = TfidfVectorizer(ngram_range=(2,3), stop_words='english', max_features=2000) $ tfmod = tfidf.fit_transform(aldf['summary_clean']) $ tfidfdummiee = pd.DataFrame(tfmod.todense(), columns=tfidf.get_feature_names()) $ tfidfdummiee.head()
df_protest.duration.max()
print("The lenght's average in tweets: {}".format(np.mean(data3['len'])))
items = pd.read_sql_query('select * from "dimensions_items"',con=engine)
baseball.sum()
from ipywidgets import interact, interactive, fixed, interact_manual $ import ipywidgets as widgets $ from IPython.display import display $ import numpy as np $ from scipy.optimize import curve_fit as cf
df_mes = df_mes[df_mes['extra']>=0] $ df_mes.shape[0]
plt.pie(slices,labels=activities, colors=['#FF5733', '#33FF49', "#33A5FF"], shadow=True, explode=(0,0.1,0), startangle=90) $ plt.show()
logit2 = sm.Logit(df_new['converted'],df_new[['intercept','old_page','CA', 'UK']]) $ result1 = logit2.fit() $ result1.summary()
oppstagepct.loc[0].plot.box(by='opportunity_stage')
X_train = X_train.to_frame() $ X_test = X_test.to_frame()
new_user_project.to_csv('data_table/user_project.csv', index = False)
wikipedia_meritocracy = 'https://en.wikipedia.org/wiki/Meritocracy' $ meritocracy_save = 'wikipedia_meritocracy.html'
coefs = pd.DataFrame(logreg.coef_[0], index = X.columns, columns = ['coef']) $ coefs['coef'] = np.exp(coefs['coef']) $ coefs.sort_values(by='coef', ascending = False, inplace=True) $ coefs.head(10)
df_protest.columns.tolist()
df.rename(columns={'Indicator': 'Indicator_id'}, inplace=False)  # Since the change is only temporary, it is not getting reflected in the df output $ df.head(2)
df_transactions['membership_duration'] = df_transactions['membership_duration'].clip_lower(0)
for row in soup.select('h3 > a.fade'): $     print(row['href'])
def RMSLE(y, pred): $     return metrics.mean_squared_error(y, pred)**0.5
y = new["Price"] $ x = new.drop("Price",axis=1)
au.find_some_docs(ao18_qual_coll,limit=3)
alg2 = RandomForestClassifier() $ alg2.fit(X_train, y_train) $ probs = alg2.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
properati[properati['state_name'] == "Bs.As. G.B.A. Zona Norte"][['state_name','zone']]
df.loc[d[5], 'A']
data = pd.read_csv('prepared_data_with_cuts.csv', index_col=0) $ data.head()
autos['registration_year'].describe()
autos = autos[autos["registration_year"].between(1970,2016)] $ autos["registration_year"].describe()
df = df1.append(df2, ignore_index=True) $ df
other_text_feature.shape
datetime(2005, 12, 31, 10, 0, 0) < p.end_time # WAT?!
d.visualize()
InfinityWars_Predictions = infinity.join(InfinityWars_PRED_df).encode('utf-8').strip() $ InfinityWars_Predictions.columns = ['tweet','language','prediction'] $ InfinityWars_Predictions = InfinityWars_Predictions.drop(['language'],axis = 1)
df2['converted'].mean()
plt.scatter(aqi['AQI_cg'], aqi['AQI'], alpha=0.2) $ plt.xlabel('Cottage Grove'); plt.ylabel('Oakridge');
access_logs_df = access_logs_df.cache()
iowa.head()
df_countries.isnull().sum()
df2.shape[0]
price_counts = autos['price_dollars'].value_counts() $ print(price_counts.sort_index(ascending=True))
merged.sort_values("amount", ascending=False)
df_new['stack_answer_count'] = u.answers.count
sub1.shape
res = dfs.join(dfpf, dfpf.rf == dfs.rf).select(dfs.rf, dfpf.qty, dfs.date, dfs.neutral, dfs.scenarios)
fig = ax.get_figure() $ fig.savefig('n5-exercise.svg')
exp_budget_vote = sorted_budget_biggest.groupby(['original_title'])['vote_average'].mean()
daily_averages = pivoted.resample('1d').mean()
import nltk.corpus $ import nltk.stem.porter $ import re
jn = urllib.request.urlopen("https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20yahoo.finance.xchange%20where%20pair%20in%20(%22GBPHKD%22%2C%22HKDGBP%22)&format=json&diagnostics=true&env=store%3A%2F%2Fdatatables.org%2Falltableswithkeys&callback=").read() $ df = pd.read_json(jn)
autos.brand.value_counts(normalize=True)
ved = pd.read_excel('input/Data.xlsm', sheet_name='51', usecols='A:W', header=12, skipfooter=4)
a = np.arange(0.5, 10.5, 0.5) $ a
np.exp(-0.0140)
segments.seg_length.apply(np.log).hist(bins=100)
es.indices.refresh(index="test-index*")
df_links = df_links[df_links['link.domain'] != 'twitter.com']
%%R $ flightsDB$CARRIER_CODE <- as.numeric(as.factor(flightsDB$UNIQUE_CARRIER))
avg_per_seat_price_inoroffseason_teams["Offseason", "BAL"] # This is the average across BAL's off-season prices.
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) # create dummy variable columns for 'group' $ df2['ab_page'] = df2['treatment']
145311.000000/df2.shape[0] $
autos["price"].value_counts()
occurrences = np.asarray(vectorized_text_labeled.sum(axis=0)).ravel() $ terms = ( occurrences ) $ counts_df = pd.DataFrame({'terms': terms, 'occurrences': occurrences}).sort_values('occurrences', ascending=False) $ counts_df
tweet_archive_clean['stage'] = tweet_archive_clean[['doggo', 'floofer','pupper', 'puppo']].apply(lambda x:''.join(x), axis= 1)
flight.printSchema() $ flight.show(2,truncate=False) $ display(flight.count())
import test_package.print_hello_function_container $ import test_package.print_hello_class_container $ import test_package.print_hello_direct # note that  the paths should include root (i.e., package name) $
df.zip.replace(56201,52601,inplace=True)
import statsmodels.api as sm $ convert_old = sum(df2[df2.group == 'control'].converted) $ convert_new = sum(df2[df2.group == 'treatment'].converted) $ n_old = df2[df2.group == 'control'].user_id.count() $ n_new = df2[df2.group == 'treatment'].user_id.count()
! head samples.csv
sess.get_data('gbp curncy', 'fwd curve', index='Settlement Date')
S.modeloutput_obj.add_variable('scalarCanopyTranspiration')
columns = inspector.get_columns('Measurement') $ for c in columns: $     print(c['name'], c["type"])
forecast_df['Under 10', '10 to 19', '20 to 29', '30 to 39', '40 to 49', '50 to 59', '60 to 69', '70 to 79', '80+'] = 0 $ for ind, row in SANDAG_age_df[SANDAG_age_df['AGE_RANGE'] == '10 to 19'].iterrows(): $     forecast_df.loc[ind, 'under_20'] = row['POPULATION'] $ forecast_df.head()
ben_final[ben_final['pagetitle'].str.contains('/')]
!ls | grep 'sample_data.json'  # this is the file
d311_gb.head()
column_datasets = {'train': ColumnarDataset.from_data_frame(trn_df, cat_vars, trn_y), $                    'val': ColumnarDataset.from_data_frame(val_df, cat_vars, val_y)}
print('The largest change in a day is ' + str(max((data['Open'] - data['Close']).abs())))
desc_stats.to_excel('DescStats_v1.xlsx')
print(f"Fit3 shape: {fit3.shape}, Fit3 non-nulls: {fit3.nnz}") $ print(f"Non-null fraction of total: {'{:.10f}'.format(fit3.nnz/(fit3.shape[0] * fit3.shape[1]))}")
df.query('landing_page == "new_page"')['user_id'].count()/df.shape[0]
ab_file2['intercept'] = 1 #creating an intercept column with all values 1 $ ab_file2[['ab_page']] = pd.get_dummies(ab_file2['group'])[['treatment']] # a dummy variable column
sns.barplot(x='frequency',y='word',data=words_df.head(15))
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1-p_new)]) $ new_page_converted
df3['observed_tobs'].plot(kind = 'hist',bins = 12) $ plt.show()
trigram_bow_filepath = paths.trigram_bow_filepath
liberia_data3['Date'].apply(pd.to_datetime)
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old, n_new], alternative='smaller')
newdf.reset_index(inplace=True) $ newdf['Date'] = newdf['Date'].apply(lambda x: str(x)[0:7]) $ newdf.set_index('Date',inplace=True)
kochdf = pd.merge(kochdf, koch11df,  how='left', left_on=['name', 'user'], $                   right_on = ['name', 'user'], suffixes=('','_11')) $ kochdf.info()
pold=df2['converted'].mean() $ pold
print(type(google_stock)) $ print(google_stock.shape)
soup = BeautifulSoup(response.text, "html5lib")
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.len() < 3) & (All_tweet_data_v2.name.str.contains('^[(a-z)]'))]
fraud_data_updated.head()
test_pl = tweet_en[tweet_en['text'].apply(lambda x: "I'm at" in x)] $ test_pl = tweet_en[tweet_en['ex_lat']!='-1']
measure_val_2010_to_2013.count()
tweet_archive_clean.info()
data = np.linspace(0,1,201) $ print(data)
df2.iloc[2862]
lm = sm.Logit(df4['converted'],df4[['intercept','new','UK','US']]) $ results = lm.fit() $ results.summary()
df.drop(df.query("group =='treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group =='control' and landing_page == 'new_page'").index, inplace=True) $
own_star.to_pickle('data/pickled/new_ratings_data.pkl')
model_w = sm.formula.ols('y ~ C(w)',data=df).fit() $ anova_w_table = sm.stats.anova_lm(model_w, typ=1) $ anova_w_table.round(3)
log_mod=LogisticRegression() $ log_mod.fit(x_train,y_train) $ y_preds=log_mod.predict(x_test)
import numpy as np $ data = np.random.normal(0.0, 1.0, 1000000) $ np.testing.assert_almost_equal(np.mean(data), 0.0, decimal = 2)
dum = pd.get_dummies(TrainData, sparse=True, drop_first=True, dummy_na=True, $                     columns=['Gender', 'City_Category', 'Employer_Category1', 'Primary_Bank_Type', 'Contacted', $                             'Source_Category', 'Employer_Category2', 'Var1']) $ dum.head()
SEA_analysis2 = SEA_analysis["Per Seat Price"].mean()
nold=df2.query('landing_page=="old_page"').count()[0] $ nold
incidents_yes_no = [] $ for i in range(len(incidents_time_index)): $     teststamp = incidents_time_index[i] $     incidents_yes_no.append(len(incidents[(incidents['delay_start'] <= teststamp) & (incidents['delay_end'] >= teststamp)]))
pprint.pprint(posts.find_one({"reinsurer": "ACE"}))
google_stock['Adj Close'].describe()
df_train = pd.merge( df_train , df_group_by2 , on = 'msno' , how = 'inner' ) $ df_test = pd.merge( df_test , df_group_by2 , on = 'msno' , how = 'inner' )
%matplotlib inline $ plt.scatter(newdf['Mkt-RF'], newdf['TSRet'])
wrd_full['year'] = wrd_full['timestamp'].apply(lambda x: datetime.strptime(x, "%Y-%m-%d %H:%M:%S +0000").year)
with open('train.csv') as f: $     size=len([0 for _ in f]) $     print("Records in train.csv => {}".format(size))
format = lambda x: x['E']+x['F'] $ df2.apply(format,axis=1)
target_google = people_person[people_person['channel'] == "Google"] $ target_google.head()
df_protest.loc[:, df_protest.columns[df_protest.dtypes=='object'].tolist()]
def times_contacted_min_max_std(df, sample_id): $     rec = df[df['profile_id'] == sample_id] $     return rec['times_contacted'].min(), rec['times_contacted'].max(), rec['times_contacted'].std()
s_n_s_epb_one.index = s_n_s_epb_one.Date $ del s_n_s_epb_one['Date'] $
datatest = pd.concat([train, test, rest])
users.to_csv('data/2017/users.csv', index=False) $ repos_users.to_csv('data/2017/repos-users-geocodes.csv', index=False) $ repos_users.to_csv('data/2017/repos-users.csv', index=False)
autos["date_crawled"].str[:10].value_counts(dropna = False, normalize = True).sort_index()
faa_data_pandas.shape
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.binomial(1, p_new, n_new) $     old_page_converted = np.random.binomial(1, p_old, n_old) $     p_diffs.append(new_page_converted.mean()-old_page_converted.mean())
df.query('(landing_page == "new_page" and group != "treatment") or (landing_page != "new_page" and group == "treatment")').shape[0]
df[df['converted'] == 1].groupby('user_id').nunique().shape[0] / df.nunique()['user_id']
print len(word_freq_df) $ word_freq_df.head()
def get_datetime_from_microsecond(timestamp): $     return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp / 1000))
df.info()
data['love'].value_counts()
df_data.show()
from sklearn.metrics import accuracy_score, classification_report $ score = 100.0 * accuracy_score(y_test, predicted) $ print(f'Logistic Regression [Challenger Data] Score = {score:4.1f}%\n') $ print(classification_report(y_test, predicted))
df.head()
dfx = df.reindex( index=dates[0:4], columns=list(df.columns) + ['E']) $ print(dfx, '\n') $ dfx.loc[dates[0]:dates[1], 'E'] = 1 $ print(dfx, '\n')
df = sqlContext.inferSchema(my_data)
pred4 = nba_pred_modelv1.predict(g4) $ prob4 = nba_pred_modelv1.predict_proba(g4) $ print(pred4) $ print(prob4)
cached.count()
to_be_predicted_Day5 = 43.4545817 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
autos["registration_year"].describe()
df1 = df.drop(['Area Id', 'Variable Id', 'Symbol'], axis=1) $ df1
print(data.iloc[[34450]])
smooth_stanmodel = SMOOTH.compileModel() # to facilitate plotting 
lgb_model.fit(X_train, y_train)
big_df_count.head()
df_archive.info()
if True == 0: $     rr_tmp = rr.replace(0, np.nan).apply([np.mean, np.std]) $     rr_tmp.T.plot.barh() $     plt.xlabel('Monthly return (not annualized)') $     plt.show() $
users_not_odu.groupby(users['Asked'] > 0).mean()
freeways.layers 
os.chdir('C:\\Users\\admin\\pandas')
goog.Open.plot() $ goog.Close.plot()
plt.figure(figsize=(16,8)) $ plt.title('MODELS DISTRIBUTION FOR VOLKSWAGEN BRAND') $ model_count = sns.countplot(volkswagen_cars['model']) $ model = model_count.set_xticklabels(model_count.get_xticklabels(), rotation=90)
vt_daily_flow_abs = df.xs('VT',level='symbol').flow['2011':].abs().rename('VT') *100 $ spy_daily_flow_abs = df.xs('SPY',level='symbol').flow['2011':].abs().rename('SPY') *100 $ title = 'Mean Absolute Daily Fund Flows (%)' $ vt_daily_flow_abs.resample('BM').mean().plot(legend=True,title=title) $ spy_daily_flow_abs.resample('BM').mean().plot(legend=True) $
eclf3 = VotingClassifier(estimators=[('lr', alg), ('calC', alg7), ("ranFor", alg2), ("DT", alg6)], voting='soft') $ eclf3.fit(X_train, y_train) $ probs = eclf3.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
name = 'specs210618' $ full_path_to_file = 'C:\\Users\\lukas\\PycharmProjects\\monimporter\\specs210618.json'
old_page_converted = np.random.binomial(1, prop_users_converted, n_old)
sdf.rdd.first()  # ["host"]
df[df.sentiment == 1].count()
ax = users.created_at.hist(bins=144) $ ax.set_xlabel('Date') $ ax.set_ylabel('# Users') $ ax.set_title("Users' account creation per month")
from h2o.automl import H2OAutoML $
errors.mae_vals.idxmin(errors.mae_vals.min())
new_page_converted = np.random.choice(2,size=n_new,p=[1-p_new,p_new])
dates = np.array(["2013-09-01","2013-09-02"],dtype='datetime64') $ print(dates)
clf.fit(X_train, y_train)
train_view.sort_values(by=5, ascending=False)[0:10]
get_response('No need to involve the police.')
f_os_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_os_hour_clicks"), header=True) $ print('Found %d observations.' %f_os_hour_clicks.count())
cr_new_null = len(df2.query('converted == 1')) / len(df2['converted']) $ print('The convert rate for P_new under the null is {}'.format(cr_new_null))
prediction_clean.info()
oppose_NNN = merged_NNN[merged_NNN.committee_position == "OPPOSE"]
Celsius.__repr__ = lambda self : 'lol'
print(soup.prettify())
target_data = pd.read_csv('data/kaggle_data/train-targets.csv', sep=",") $ target_data.head(5)
results.summary()
train['is_holiday'] = train['start_timestamp'].map(lambda x: 1 if str(x.date()) in us_holidays else 0) $ test['is_holiday'] = test['start_timestamp'].map(lambda x: 1 if str(x.date()) in us_holidays else 0)
sentencelist = map(lambda text: list(jieba.cut(text,cut_all=True)),michael_df.text) $ reducedwordlist = reduce(lambda x,y:x+y,sentencelist)
df = pd.read_csv("all.csv",header=None) $ df.columns
poverty_data=poverty.iloc[poverty_data_rows,:]
energy_indices = energy_cpi.drop(['Measure','Region','Adjustment Type','Frequency'], $                                  axis=1).set_index(['Index','Time']).unstack('Index') $ energy_indices
properati.loc[(properati['zone'] == "") & (properati['place_name'] == "Capital Federal"),'zone'] = "Capital Federal"
returns = (mydata / mydata.shift(1)) - 1 $ returns.head()
assoc = db.get_associations(limit=150) $ assoc.head()
idxs = get_cv_idxs(n, val_pct=150000/n) $ joined_samp = joined.iloc[idxs].set_index("Date") $ samp_size = len(joined_samp); samp_size
rf1000 = RandomForestClassifier(n_estimators=1000) $ rf1000.fit(X, y)
random_steps.hist();
test.replace({"Open": {np.nan: 1}}, inplace=True)
trans = trans.sort_values(['msno', 'transaction_date', 'membership_expire_date'])
df_out = pd.merge(df_userid,df_Tran,how='outer',on="Key")[['UserID','ProductID']]
grouped.get_group('MSFT')
df_new[['ca','uk','us']] = pd.get_dummies(df_new['country'])
baseball.hr.sort_values()
store_items.dropna(axis = 1)
class_test.shape
print(repos_users.shape) $ repos_users.head()
plt.rcParams['axes.unicode_minus'] = False $ dta_6201.plot(figsize=(15,5)) $ plt.show()
afx_2017_dict = r.json()
y = df['effortProj'].values $ df.drop('effortProj',axis=1,inplace=True) $ df.drop('projID',axis=1,inplace=True) $ X = df.values
from sklearn.base import BaseEstimator, RegressorMixin $ class MedianReg(BaseEstimator, RegressorMixin):  $         Called when initializing the regression $
print('There is a {:.2f}% chance of being readmitted within 30 days'.format(100*full['WillBe<=30Days'].mean())) $
donors[donors['Donor Zip'] == 606 ].head()
api_clean['tweet_id'] = api_clean['tweet_id'].astype(str)
%load_ext rpy2.ipython $ %Rpush raw_large_grid_df
output = output.sample(False, 0.1, seed=0)
group_name = lift.get_group_by_alias('Cozy Bear')
scr_activated_df = pd.DataFrame(index=daterange,columns=daterange)
import numpy as np $ import pandas as pd $ rainfall = pd.read_csv('data/Seattle2014.csv')['PRCP'].values $ inches = rainfall / 254.0  # 1/10mm -> inches $ inches.shape
num_portfolios = 25 $ results = np.zeros((num_portfolios,6))
model_CSCO.aic
points=pd.Series([630,25,26,255], $     index=['India','Bangladesh','Pakistan','China']) $ print(points)
y_test.shape
groups = openmc.mgxs.EnergyGroups() $ groups.group_edges = np.array([0., 0.625, 20.0e6])
breed_conf = breed_conf.sort_values(ascending = False)
train = K.function(inputs=[x, target], outputs=[loss], updates=updates)
p_new = sum(df2.converted == 1) / 290584 $ p_new
"In the last {0} tweets, I've been most subjective {1} percent and neutral {2} percent".format(subjectivity.sum().counts, percentConvert(subjectivity.loc["positive"].percent), percentConvert(subjectivity.loc["neutral"].percent))
df.iloc[2]  # by single row number
requests.get(wikipedia_marvel_comics)
for i, x in enumerate(unprocessed_list): $     unprocessed_list[i] = x.split('\n')
df[y_col] = df['variety'].str.lower().replace(repl_dir) $ df_noblends = df[df[y_col].replace(repl_dir).str.lower().isin(keep_vars)] $ df_noblends[y_col].unique().size
tweet_place_hist = pd.crosstab(index=tweets_df["tweet_place"], columns="count") $ tweet_place_hist['place_freq'] = tweet_place_hist['count'] * 100 / tweet_place_hist.sum()['count'] $ tweet_place_hist = tweet_place_hist.sort_values('place_freq', ascending=False) $ tweet_place_hist.head(10)
file_path = 'files/Sample Security Tables.xlsx' $ sheet = 'DBA_USERS' $ users = pd.read_excel(file_path, sheet_name=sheet, usecols='B:ZZ')
tweet_archive.head(40)
geocoded_df.loc[idx,'Case.Duration'] = geocoded_df.loc[idx,'Judgment.Date']-geocoded_df.loc[idx,'Case.File.Date']
parks_don_quar = parks_don.groupby(['fq'])[['amount']].sum() $ parks_don_quar.plot(kind='bar');
features = wine_query.execute(output_options=bq.QueryOutput.dataframe()).result()
plt.scatter(y=dftouse_four.opening_gross, x=dftouse_four.star_avg,s=dftouse_four.review_count)
df_goog.Open
s.ix[3:6].mean()
plt.plot(dataframe.groupby('year').daily_worker_count.mean()) $ plt.show()
print(DataSet_sorted['tweetText'].iloc[2])
a_list.append(1)
class_merged=pd.merge(class_merged,transactions,on=['date','store_nbr'],how='left') $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
from preprocessing_pipeline import preprocessing $ with open("model/{}/word_embedder_500.pickle".format(version), "rb") as file: $     word_embedder = pickle.load(file)
yhat = neigh.predict(X_test) $ yhat[0:5]
wrong_treatment1 = df.query("group == 'treatment' and landing_page == 'old_page'") $ wrong_treatment2 = df.query("group == 'control' and landing_page == 'new_page'") $ print("The number of times that new_page and treatment don't line up: " + str(len(wrong_treatment1)) + str(len(wrong_treatment2)))
df2.converted.sum()/df2.converted.count()
print(prop.info())
from google.cloud import bigtable $ client     = bigtable.Client.from_service_account_json(JSON_SERVICE_KEY,project=project_id, admin=True) $ instance   = client.instance(instance_id) $
df.info()
import seaborn as sns; $ titanic = sns.load_dataset('titanic') $ titanic.head()
salesfull.head() $ print salesfull.sale.sum() $ print salesfull.full_2016.sum()
score_merkmale=scoring[['id_loan_request',u'loan_request_nr',  u'fk_user','id', $                         'is_failed', u'ignore_for_scoring',u'score', 'created_at' ]].\ $ merge(merkmale.reset_index(), on='id')
np.count_nonzero(np.any(na_df.isnull(), axis=0)) # total number of columns with missing values 
learning_rate = 0.01 $ with tf.name_scope("train"): $     optimizer = tf.train.GradientDescentOptimizer(learning_rate) $     training_op = optimizer.minimize(loss)
rankings = pd.read_csv("/Users/nicksteil/Desktop/fifa_ranking.csv") #reading in a csv file
autos["price"].value_counts().sort_index(ascending=True).head(10)
df3.sort_values('timestamp').head(2), df3.sort_values('timestamp').tail(2)
mig_l1 = mi.groupby(level=0) $ print_groups(mig_l1)
analyze_set.sample(5)
volt_prof_before=pd.read_csv('../inputs/opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder)) $ volt_prof_after=pd.read_csv('../outputs/from_opendss/to_opendss/{feeder}/voltage_profile.csv'.format(feeder=_feeder))
lda = LatentDirichletAllocation( $     n_topics=n_topics, learning_method="online", random_state=0) $ lda = lda.fit(tf)
mini_px = consol_px[consol_px.columns[:10]].tail(5) # pricing subset $ as_of = consol_px.index.to_datetime()[-1:] # date as of when we want the weights vector $ print(mini_px.shape, frame, lb, frequency, min_gross, max_gross, min_w, max_w, gamma_val) $ mini_px.shape, as_of
df_new['ca_intercept'] = df_new['country'].replace(('US','UK','CA'),(0,0,1)) $ lm = sm.OLS(df_new['converted'],df_new[['intercept','ca_intercept']]) $ lm.fit().summary()
model.summary()
dfg = Grouping_Year_DRG_discharges_payments.groupby(['drg3']).\ $ agg({'discharges':[np.size, np.mean,np.max]}) $ dfg.head()
len(users) $ users.drop_duplicates(subset='screenName', keep='last', inplace=True) $ len(users)
useless_variables = ['L1_ORGANISATION_ID', 'L2_ORGANISATION_ID', 'CIA'] $ equipment.drop(useless_variables, axis=1, inplace=True)
df_imputed = pd.DataFrame(df_imput) $
cats_df.describe()
S_lumpedTopmodel.decision_obj.hc_profile.options, S_lumpedTopmodel.decision_obj.hc_profile.value
inspector = inspect(engine) $ inspector.get_table_names()
df.index
coll.find_one({'timestamp': {'$gte': datetime.datetime(2016, 9, 5, 0,0,0)}})
the_data = tmp_df.applymap(lambda x: 1 if x > 3 else 0).as_matrix() $ print(the_data.shape)
start = datetime.now() $ modelrf250 = RandomForestClassifier(n_estimators=250, n_jobs=-1) $ modelrf250.fit(Xtr.toarray(), ytr) $ print(modelrf250.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
mom.set_index('Date', inplace = True)
fig = plotly_plotting(sentiment_track, results, results_postseason) # add 'results_postseason' as third argument only if team made playoffs $ py.iplot(fig, filename='sentiment_vs_wins')
str(datetime.now())
df_questionable_3[df_questionable_3['state_MO'] == 1]['link.domain_resolved'].value_counts()
tweet_df["tweet_source"].unique()
import dask.dataframe as dd
control_con = df2[(df2['group'] == 'control') & (df2['converted'] == 1)].shape $ control_con
lda_tf.print_topics(num_topics=10, num_words=7)
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep $ lee_train_file = test_data_dir + 'lee_background.cor' $ print lee_train_file
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'weekday', 'ab_page', 'UK', 'US']]) $ results = log_mod.fit() $ results.summary()
print 'Total cycling distance of the whole trip: \t%.2f km \nTotal time cycled: \t\t\t\t%s h|m|s' % (sum( $     cycling_df['ttl_cyc_km']),secToHours(sum(cycling_df['ttl_cyc_seconds'])))
err = pd.DataFrame() $ for subject in etsamples.subject.unique(): $     err = pd.concat([err,CALIBRATION.pl_accuracy(subject),CALIBRATION.el_accuracy(subject)],ignore_index=True) $ err.loc[:,'avg'] = err.avg.astype(float) $ err.loc[:,'msg_time'] = err.msg_time.astype(float) 
rejected.isnull().sum()
1/np.exp(result.params[1])
idx = df_providers[ (df_providers['year']==2011) & \ $                   (df_providers['drg3']==39)  ].index.tolist() $
n_new = (df2[df2['landing_page'] == 'new_page']).shape[0] $ n_new $
nb = MultinomialNB() $ nb.fit(X_train_total, y_train) $ nb.score(X_test_total_checked, y_test)
tokens.sort_values('five_star_ratio', ascending=False).head(10)
pd.merge(crimes, weather, on='date', how='left').head()
notus['country'].value_counts(dropna=False)
s3 = pd.Series(diz, ['topolino', 'paperoga', 'pippo']) $ s3
total_Visits_Convs_month_byMC.loc[total_Visits_Convs_month_byMC.conversion_rate > 1]  # outlier - this data may be incorrect $
plt.plot(time_local, key_press, 'ro') $ plt.show()
df \ $     .filter(df.name == 'Samwise')\ $     .take(1)
adj_close_pivot_merged = pd.merge(adj_close_pivot, adj_close $                                              , on=['Ticker', 'Adj Close']) $ adj_close_pivot_merged.head()
candidates['election_year'] = candidates.election_date.apply(get_cycle)
listRating = list(soup.findAll('div',class_="rating-number")) $ rating = list(listRating[0].children) $ rating = rating[0] $ print ("Rating: "+rating) $ df_new['CodeChef_rating'] = rating
df2['intercept'] = 1 $ df2[['ab_page','old_page']] = pd.get_dummies(df['landing_page']) $ df2 = df2.drop('old_page',axis=1) $ df2.head()
event_sdf_pd = event_sdf.toPandas()
TEXT.vocab.stoi['the']
import json $ tweets = [] $ for tw in new_tweets: $     tweets.append(tw.text) $ print(tweets)
tesla = pd.read_csv('tesla_tweets.csv')
my_data_test_path = "/Users/leima/OneDrive - University of New Mexico/data/mybilividdata/failed_vid_data.csv" $ my_data_test = np.genfromtxt(my_data_test_path, delimiter=',')
for tweet in tweepy.Cursor(api.search, q="#unitedAIRLINES", count=100, lang="en", since="2017-04-03").items(): $     print(tweet.created_at, tweet.text) $     analysis = tb(tweet.text) $     print(analysis.sentiment.polarity) $     csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')]) $
print (test.shape)
urls=[link.get('href') for link in all_a]
from sklearn.model_selection import train_test_split
df_meta = pd.read_csv(f_meta) $ df_meta.head(3)
algo=tree.DecisionTreeClassifier(max_features=4) $ train = algo.fit(X_train, y_train) $ res=train.predict(X_val)
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=1000000, usecols=['Created Date', 'Closed Date', 'Agency Name', 'Complaint Type', 'Descriptor', 'Borough'])
a_set.union(another_set)
best_worst = data_df.loc[(stars==5) | (stars==1), :] $ best_worst.head()
xgb = XGBClassifier(objective='binary:logistic') $ xgb.fit(X_train, y_train) $ test_predictions = xgb.predict(X_test) $ eval_sklearn_model(y_test, test_predictions, model=xgb, X=X_test)
!ls ..
c={key:a[key] for key in a.keys()-{'z','w'}} $ c
ts.shift(1,DateOffset(minutes=0.5))
full.groupby(['Age'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'})['mean'].plot(kind='bar',figsize=(12,4)) $ plt.title('Readmittance rate by Age',size=15) $ plt.ylabel('readmitted rate')
revisions = [revision for revision in page.revisions()] $ revisions[0]
df_characters.head(10)
dataframe.columns = column_names
def initial_trend(series, slen): $     sum = 0.0 $     for i in range(slen): $         sum += float(series[i+slen] - series[i]) / slen $     return sum / slen $
df_members['bd_c'] = pd.cut( df_members['bd'] , age_bins, labels=age_groups)
f_ip_device_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_device_clicks"), header=True) $ print('Found %d observations.' %f_ip_device_clicks.count())
print(autos.groupby('offerType').size())
lm = sm.Logit(df_joined['converted'],df_joined[['intercept','ab_page','CA','UK']]) $ res = lm.fit()
t_likes.plot(title='Donald Trump likes',figsize=(12,6),legend=True,label='Likes'); $ t_retweets.plot(legend=True,label='Retweets')
CryptoComm['TextLen'] = CryptoComm['CommentText'].apply(len) $ CryptoComm['TimeDiff'] = (CryptoComm.CommentTime - CryptoComm.PostTime).total_seconds()
plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101_d_b[df_nd101_d_b['ud730']>0].ud730)
%matplotlib notebook $ a.toPandas().plot();
apple.index.is_unique
for index, row in df_rand.iterrows(): $     df_rand.loc[index,'is_ch_company'] = row.postcodes in row.ch_postcodes
contractor_merge[contractor_merge.contractor_bus_name.duplicated() == True] $
print pd.concat([s1, s4], axis=1, join='inner')
df_gnis.shape
forecast_df['employed'] = 0 $ for ind, row in SANDAG_jobs_df.iterrows(): $     forecast_df.loc[ind, 'employed'] += row['JOBS'] $ forecast_df.head()
popStationData = session.query(Measurement.date, Measurement.tobs).filter_by(station="USC00519281").filter(Measurement.date > pastYear).all() $ popStationData
stocks = pdread.DataReader('AAPL', 'yahoo', st, ed)
Convert_Prob=df2.query("converted==1").shape[0]/df2.shape[0] $ print("The probability of any individual converting is ", Convert_Prob)
!ls ./anaconda3_410 $
train_set.to_csv(PROCESSED_DATA_DIR + '/train_set.csv', index=False) $ test_set.to_csv(PROCESSED_DATA_DIR + '/test_set.csv', index=False)
ytDf_ttrendDate = youtube_df["trending_date"] $ ytDf_views = youtube_df["views"] $ order = np.argsort(ytDf_ttrendDate ) $ x_trendDate = np.array(ytDf_ttrendDate)[order] $ y_views = np.array(ytDf_views )[order]
dsi_me_1_df.head(3)
themes = ' '.join([themes,'ted conference'])
autos['registration_year'].value_counts()
autos['unrepaired_damage'].value_counts()
airlines = [h for h in heap if h.company in ['Delta', 'AmericanAir', 'British_Airways']]
nar5=nar4.merge(loans[['fk_loan','rating_base','rating_switch']],on='fk_loan')
n_net2.score(x_test,y_test)
print(json_data_2017['dataset_data'].keys(), '\n') $ print(json_data_2017['dataset_data']['column_names'])
old_page_converted = np.random.binomial(1, p_old, n_old) $ sum(old_page_converted)
print(autodf.groupby('offerType').size()) $ autodf = autodf[autodf['offerType'] != 'Gesuch'] $ autodf = autodf.drop('offerType',axis = 1) $ print( "\nSize of the dataset - " + str(len(autodf)))
y_pred = y_pred.argmax(axis=1)
df1_clean.source.unique()
last_year = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date>='2016-08-01').\ $         filter(Measurement.date<'2017-08-01').order_by(Measurement.date).all()
df_with_metac1 = pd.concat([df_onc_no_metac, df_dummies], axis=1)
duration_train_df.shape, duration_test_data.shape
big_df_count.head()
from pandas.tseries.holiday import * $ cal = USFederalHolidayCalendar() $ for d in cal.holidays(start='2014-01-01', end='2014-12-31'): $     print(d)
imgp_clean = imgp_clean.drop(imgp_clean[(imgp_clean.p1_dog == False) & (imgp_clean.p2_dog == False) & (imgp_clean.p3_dog == False)].index) $
train_data, test_data, train_labels, test_labels = train_test_split(spmat, y_data, test_size=0.10, random_state=42)  
empInfo = pd.read_csv("dataset.shared.id.csv") $ empInfo.columns = ["group","ID","level","manager"]
trunc_df.iloc[list(indices)]
fouls_df.to_pickle('fouls_df.pkl')
data.loc[data.PRECIP.isnull()]
[(v.standard_name) for v in dsg.data_vars()]
prob_new_page = df2.groupby('group').count() $ prob_new_page['user_id']['treatment'] / len(df2)
contentPTags = obamaSpeechSoup.body.findAll('p') $ for pTag in contentPTags[:5]: $     print(pTag.text)
props.prop_name.value_counts().reset_index()
cities = [] $ for i in places2.index: $     cities.append(places2[i]['name'])
display(heading('children of "observations" with children:'), $         set(get_grandchild_nodes(observations_node).keys()))
print(autos["price"].unique().shape) #Find out the number of different prices in the dataset
treat_old = df.query("group == 'treatment' and landing_page == 'old_page'").shape[0] $ control_new = df.query("group == 'control' and landing_page == 'new_page'").shape[0] $ misalignment = treat_old + control_new $ misalignment
tweet_2 = pd.read_json('tweet_json.txt')
df2[(df2.group == 'control')&(df2.converted == 1)].shape[0]/df2[df2.group == 'control'].shape[0] $
import statsmodels.api as sm $ convert_old = df2.query('landing_page=="old_page" and converted==1').shape[0] $ convert_new = df2.query('landing_page=="new_page" and converted==1').shape[0] $ n_old = df2.query('landing_page=="old_page"').shape[0] $ n_new = df2.query('landing_page=="new_page"').shape[0] $
discounts_table.groupby(['Discount Band'], sort=False)['Discount %'].min()
obs_diff = (df2.query('group == "treatment"').converted).mean() - (df2.query('group == "control"').converted).mean() $ plt.hist(p_diffs); $ plt.axvline(x=obs_diff, color='red');
from sklearn.linear_model import LogisticRegression $ logmodel = LogisticRegression() $ logmodel.fit(X,y)
msftAC.tail(5), shifted_forward.tail(5)
import pandas as pd $ cust_df = pd.read_csv("Cust_Segmentation.csv") $ cust_df.head()
train, validation = train_test_split(df_train, test_size=0.3)
if False: $     plt.scatter(train.diff_lng, train.duration) $     plt.show()
print(stats.variation(project_0_cycle_times)) $ print(stats.variation(project_1_cycle_times))
df['Polarity'] = np.nan $ df['Subjectivity'] = np.nan $ df['Score'] = np.nan
df_countries.info()
plt.figure() $ dat.VecAvgWindDir.plot()
poiModel = Sequential() $ poiModel.add(Embedding(num_POI+1, output_dim=vec_dim, input_length=1)) $ poiModel.add(Reshape((vec_dim,)))
def retrieve_html(url): $     r = requests.get(url) $     return (r.status_code,r.text)
df.boxplot(column='rating', by='dog_type');
pd.DataFrame({'Decison Making' : [x[1] for x in HARVEY_92_USERS_DM]}).hist(bins=[1,2,3,4,5,6,7,8,10,100,200,1000])
print('proportion of users converted: {:.5f}'.format(df['converted'].mean()))
dfs[index_max][dfs[index_max]['Outside Temperature'] == dfs[index_max]['Outside Temperature'].max()]
unique_users_count2 = df2.user_id.nunique() $ print(unique_users_count2)
plt.style.available
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ p_new_page_converted = new_page_converted.mean() $ print(p_new_page_converted)
m3.fit(lrs, 7, metrics=[accuracy], cycle_len=2, cycle_save_name='imdb2')
df.count()[0]
g['Created Date'] = pd.to_datetime(g['Created Date']) $ g['created_year'] = g['Created Date'].dt.year $ g.head(2)
pprint("Month with the largest number of job ads: 9.0(September)-1652")
pn_qty[pn]['1cstoreinfo'].append(table_1c.ix[0])
log_reg_over.score(X_train, y_train_over)
tmdb_movies_production_countries_revenue['production_countries'] = tmdb_movies_production_countries_revenue['production_countries'].map(lambda x: x.get('name'))
df.head(5)
top_features = [(key, value) for key, value in feature_set.items() if value >= 100]
df.head()
overdue_encoder = LabelEncoder() $ overdue_transf = overdue_encoder.fit_transform(overdue) $ vectorizer = DictVectorizer() $ features_class_vect = vectorizer.fit_transform(features_classif) $ features_class_norm = normalize(features_class_vect, norm='l1', axis=1)
negGroups = list(neg_tweets.group_id_x) $ num_convos = len(set(negGroups)) $ print(f'Working with {num_convos} conversations') $ companyNeg = filtered[filtered.group_id.isin(negGroups)] $
baseball.loc[89521, "player"]
df_hi_temps.head()
archive_df[archive_df.in_reply_to_status_id.isnull() == False]
df_email.occurred_at = pd.to_datetime(df_email.occurred_at) $ df_email["nweek"] = df_email.occurred_at.dt.week #strftime('%Y-%U')
import datetime $ datetime.datetime(2015, 12, 31, 0, 0).strftime("%I:%M%p on %A %B %d, %Y")
store_items.isnull().sum()
weather[weather['precipitation_inches'] == 'T']['events'].unique()
lastPrediction = thisWeek['text'].count() - 1 + hourlyRates.loc[hourlyRates['hourNumber'] == thisWeek['hourNumber'][0],'meanRemainingTweets'].iloc[0] $ stddev = hourlyRates.loc[hourlyRates['hourNumber'] == thisWeek['hourNumber'][0],'stdRemainingTweets'].iloc[0] $ skew = hourlyRates.loc[hourlyRates['hourNumber'] == thisWeek['hourNumber'][0],'skewRemainingTweets'].iloc[0]
counts = df.groupby(TimeGrouper(freq='30min')).agg({"text":"count"}).rename(columns={"text":"count"}) $ counts.reset_index(inplace=True) $ counts.head()
x = pd.merge(ign, salesdec, on=['Name','Platform'], suffixes=['_ign','_vg'], how='outer', indicator=True) $ x.head()
tweet_json_df_clean.id.dtypes
(temp_df.email != '\\N').sum()
lv_workspace.apply_subset_filter(subset='A') # Not handled properly by the IndexHandler
tm['created_at'].dt.hour.hist(density=True, color='Orange')
tweet_counts_by_month = tweet_archive_master[['timestamp', 'favorite_count', 'retweet_count']].groupby(pd.Grouper(key='timestamp', freq='M')).sum()
pipe_lr_2 = make_pipeline(hvec, lr) $ pipe_lr_2.fit(X_train, y_train) $ pipe_lr_2.score(X_test, y_test)
df_new = pd.concat([df_2001, df_2002, df_2003, df_2004, df_2007, df_2008, df_2009, df_2010, df_2011, df_2012, df_2013, df_2014, df_2015, df_2016, df_2017], sort=True)
train['target']=0 $ train.loc[pd.notnull(train.units_purchased ),'target']=1 $ train = train.drop(['units_purchased','lane_number','total_spend'],axis=1)
all_df.isnull().any() $
logit = sm.Logit(df2.converted, df2[['intercept', 'new_page', 'CA', 'UK']]) $ result = logit.fit() $ result.summary()
props.info()
df_image_clean.loc[1024, :]
start_date = '2017-01-01' $ end_date = '2017-01-10' $ calc_temp = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $                             filter(Measurement.date >= start_date, Measurement.date <= end_date).all() $ calc_temp
move_1_union = sale_lost(breakfastlunchdinner.iloc[1, 1], 30) $ move_2_union = sale_lost(breakfastlunchdinner.iloc[5, 2], 30) $ adjustment_2 = move_1_union + move_2_union $ print('Adjusted total for route: ' + str(move_34p14u34p - adjustment_2))
json_df['post_patch'].value_counts()
os.getcwd()
df.to_csv('ab_data2.csv', index=False)
print("The lenght's average in tweets: {}".format(np.mean(data2['len'])))
classifier = train_classifier(articles_train, authors_train, articles_test, authors_test, $                               LogisticRegression()) $ print(classifier.score(test_features_tokenized, authors_test))
np.sum(df["trip_duration"] < 0.01)
data_df.groupby('type')['ticket_id'].nunique()
tweets_clean.drop(index = 754011816964026368, inplace = True) $ images_clean.drop(index = 754011816964026368, inplace = True)
total_delta_nbs = hits_df.iloc[-1] - hits_df.iloc[0] $ total_delta_nbs
PMCIDs_in_demographics = [d['PutRequest']['Item']['pmcid']['S']  for d in output_dict['demographics']] $ jsondata = json.dumps(pmcid_in_demographics) $ f = open("PMCIDs_in_demographics.json","w") $ f.write(jsondata) $ f.close()
train.head(3)
corpus = st.CorpusFromParsedDocuments(df_trump_device_non_retweets, $                                       category_col='before_or_after_election', $                                       parsed_col='parsed').build()
%%time $ body_pp = processor(keep_n=8000, padding_maxlen=70) $ train_body_vecs = body_pp.fit_transform(train_body_raw)
len([col_n for col_n in df.columns if '_rolling' in col_n]) $
df_total.head()
df.groupby('userid').max() $
conn_helloDB.execute('drop database HelloDB3;')
fraud_data_updated.info()
dftops.plot(kind='bar')
df0.date = pd.to_datetime(df0.date)
clean_rates.cuteness.value_counts()
autos.columns
store_time = table_store.get_value(0, u'Last Updated Date')
dfHashtags.to_pickle(data + "dfHashtags.p")
"unique user-id's in experiment data: ", len(df_experiment.user_id.unique())
station_df = pd.read_sql("SELECT * FROM station", conn) $ station_df.head()
disag_hart = DataSet(disag_filename) $ disag_hart_elec = disag_hart.buildings[building].elec
df[df.index.month.isin([11, 12, 1, 2])]['Complaint Type'].value_counts().head()
CRnew = df2.converted.sum()/df2.count()[0] $ CRnew
locationing['polarity'] = locationing.apply(lambda x: TextBlob(x['Text']).sentiment.polarity, axis=1) $ locationing['subjectivity'] = locationing.apply(lambda x: TextBlob(x['Text']).sentiment.subjectivity, axis=1) $ locationing = locationing.sort_values('Text', ascending=False).drop_duplicates(['Text'], keep = 'first') $ locationing.head()
sox.sort('date', ascending=True, inplace=True) $ sox.reset_index(drop=False, inplace=True) $ sox.rename(columns={'index':'game_id'}, inplace=True)
one_station['DATE'] = pd.to_datetime(one_station['DATE']) 
df.query('group == "treatment" and landing_page != "new_page"').count()[0] + df.query('group != "treatment" and landing_page == "new_page"').count()[0]
for i in range(1, 10): $     print("Features with {} importance: gives {} number of features".format(10**-i, np.sum(rfc_feat_sel.feature_importances_ >= 10**-i)))
df.loc[:, ['B', 'D']] # notice lack of parentheses here!
','.join(['cat', 'dog'])
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
list(c.find())
np.random.seed(15) $ df.sample(frac=0.6) #randomly pick 60% of rows, without replacement
print('Number of unique user_ids in df2 is {}'.format(len(df2.user_id.unique()))) $
top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1]) $ print ('TOP recommended movies (with more than 25 reviews):\n%s' % $         '\n'.join(map(str, top_movies)))
retention = grouped.groupby(level=0).apply(cohort_period)
session.query(Station.station, Station.name, Station.latitude, Station.longitude, Station.elevation, \ $      func.sum(Measurement.prcp)).filter(Measurement.station == Station.station)\ $     .filter(Measurement.date>='2016-08-01').filter(Measurement.date<'2016-08-11')\ $     .group_by(Station.station).order_by(func.sum(Measurement.prcp).desc()).all()
headlines_prep = preprocess(headlines) $ y_true = random_sample[actual_reactions_list].as_matrix() $ y_pred = clf.predict(headlines_prep) $ y_prob = clf_prob.predict_proba(headlines_prep)
item = collection.item('AAPL') $ item
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
data.loc['NL']
val sorted_requests = requests $     .map(pair => pair.swap) $     .transform(rdd => rdd.sortByKey(false))
initialdate_per_coin = df0.groupby('symbol').date.min() $ initialdate_per_coin.to_csv('output/initialdate_per_coin.csv')
events_per_day = events_df[['event_day','event_id']].groupby('event_day').count() $ events_per_day.rename(columns={'event_id':'count_event_day'},inplace=True) $ events_per_day.reset_index(inplace=True)
!ls ./container/model
! python ../.convert_notebook_to_script.py --input ch08.ipynb --output ch08.py
y = df.price.values $ plt.hist(np.log(y)) $ plt.show()
train_norm = train_norm.join(train[['duration', 'is_holiday', 'is_weekend', 'is_rushhour'] + dummy_features_test]) $ test_norm = test_norm.join(test[['row_id', 'is_holiday', 'is_weekend', 'is_rushhour'] + dummy_features_test])
properati.count().plot(kind='bar',figsize=(20,10))
print('Do any of the rows have missing values?  Answer:  ' + str(df.isnull().values.any()))
shopping_carts.values
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
p_new = new_page_converted.mean() $ p_old = old_page_converted.mean() $ p_new - p_old
popC15[popC15.content == 'post'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
df.info()
df.corr()['Order_Qty']
print(f'dataframe shape: {match.shape}') $ match.head(3)
data.info()
fraq_volume_m_coins[['Bitcoin', 'Litecoin']].plot() $ plt.ylabel('Fraction of volume') $ plt.show()
goog.plot(y='Close')
from pyspark.ml.evaluation import BinaryClassificationEvaluator $ evaluator = BinaryClassificationEvaluator(rawPredictionCol="prediction", labelCol="label", metricName="areaUnderROC") $ print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))
ways.info()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)
pm_final[(pm_final.obs_date < 40) & (pm_final.status == 1)]
df_schools = pd.read_csv(file1) $ df_students = pd.read_csv(file2)
order_data.reset_index(inplace=True)
df_columns[df_columns['Complaint Type'].str.contains('Firework')]['Day in the year'].value_counts().head() $
print("Mean squared error: %.2f"% mean_squared_error(y_test, y_pred))
maint['datetime'] = pd.to_datetime(maint['datetime'], format="%Y-%m-%d %H:%M:%S") $ maint.count()
p_value = (p_diffs > obs_diff_conversion).mean() $ print(p_value)
data.shape
feature_df = cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']] $ X = np.asarray(feature_df) $ X[0:5]
ppm_title.token_count_pandas().head()
stars.value_counts()
joined.describe() $
df_comment['likes_count']=df_comment.liked_by.apply(lambda x:len(x))
print(s2.keys()) $ print(s3.keys()) $ s2 + s3
data.describe()
df2.shape
values = pd.DataFrame(dataSeries.values) $ df = pd.concat([values.shift(1), values], axis=1) $ df.columns = ['t-1', 't+1'] $ result = df.corr() $ print(result)
ab_dataframe.query(' (group=="treatment" & landing_page != "new_page") | (group !="treatment" & landing_page == "new_page" ) ').shape[0]
BroncosBillsTweets['text30'] = BroncosBillsTweets['text'].apply(lambda x: x[:30])
data_FCInspevnt_latest = data_FCInspevnt_latest.drop_duplicates(subset = 'brkey', keep='last')
dfss.head()
df_new['intercept']=1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK']]) $ results=logit_mod.fit() $ results.summary()
twitter_archive_df_clean.columns
sq83= "CREATE TEMPORARY TABLE  newtable_22222 ( SELECT * FROM Facebook_NBA order by 0.2*likes+0.4*Comments+0.4*shares DESC limit 150)" $ sq84="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Id, SUBSTRING_INDEX(SUBSTRING_INDEX(message,' ',i+1),' ',-1) word FROM newtable_22222, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 0 ORDER BY total DESC, word;"
a = 4.5 $ b = 2 $ print('a is {0}, b is {1}'.format(type(a), type(b))) $ a / b
interpolated = bymin.resample("S").interpolate() $ interpolated
df_merge = df.merge(df_users, on='Twitter_Name', how='left')[['Name', 'Medium', 'Website', 'Facebook', 'Twitter_Name','Twitter_ID', 'Geography']] $ df_merge.to_csv('data/usnpl_newspapers_twitter_ids.csv', index=False)
pred10 = nba_pred_modelv1.predict(g10) $ prob10 = nba_pred_modelv1.predict_proba(g10) $ print(pred10) $ print(prob10)
from sklearn.metrics import r2_score $ r2_score(y_test, pred) $
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42)
twitter_key = os.environ.get('TWITTER_CONSUMER_KEY') $ twitter_secret = os.environ.get('TWITTER_CONSUMER_SECRET') $ twitter_token = os.environ.get('TWITTER_ACCESS_TOKEN') $ twitter_token_secret = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')
df['totals'] = df['entries'] + df['exits'] $ df.head(2)
image_predictions_copy[image_predictions_copy.p1_dog == True]
autos.describe(include='all') #include all to get both categorical(non-numeric) and numeric analyses for any columns
negative_visits = cats_df[cats_df['number of vet visits'] < 0]['number of vet visits'] $ cats_df['remove'].iloc[negative_visits.index] = True $ del negative_visits
no_conv, yes_conv = df2.query('group == "control"').converted.value_counts() $ yes_conv/(no_conv + yes_conv)
url = 'https://twitter.com/marswxreport?lang=en' $ browser.visit(url) $ time.sleep(3)  #allow time for page to load $ html = browser.html $ soup = bs(html, 'html.parser') $
meas=Base.classes.measurements $ station=Base.classes.stations
pd.bdate_range(start=start, periods=20)
path = os.path.join('Monthly EPA emissions.csv') $ epa = pd.read_csv(path)
ts.asfreq(pd.tseries.offsets.BDay(), method='pad')
stockdftest.tail(3)
def one_year_csv_writer(this_year, all_data, path, name): $     surveys_year = all_data[all_data.year == this_year] $     filename = path + name + str(this_year) + '.csv' $     surveys_year.to_csv(filename) $ one_year_csv_writer(1997, surveys_df, './data/', 'function_surveys')
X = stock.iloc[925:-1].drop(['volatility', 'volume', 'high', 'low', 'close', 'open', 'target', 'true_grow'], 1) $ y = stock.iloc[925:-1].true_grow
results_frame_b.iloc[results_frame_b['Rendimiento'].idxmax()]
y_pred = nb_pred[:,0] $ print('precision: {:.2f}\nrecall: {:.2f}\naccuracy: {:.2f}'.format(precision_score(y_test,y_pred), $                                                        recall_score(y_test,y_pred), $                                                        accuracy(y_test,y_pred)))
df['zip']=df['zip'].astype(int)
rankings_USA['year'] = pd.DatetimeIndex(rankings_USA['rank_date']).year $
import pickle $ with open('Hospital_List', 'wb') as fp: $     pickle.dump(list_of_hospitals, fp) $
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"')['converted'].sum() $ convert_new = df2.query('landing_page == "new_page"')['converted'].sum() $ n_old = df2.query('landing_page == "old_page"').landing_page.count() $ n_new = df2.query('landing_page == "new_page"').landing_page.count()
results = crowdtruth.run(data, config)
!python extract_dl1.py -h
treatment_convert_rate= df2.query('group == "treatment"').converted.mean() $ print(' Given that an individual was in the treatment group, what is the probability they converted is {}'. $       format(treatment_convert_rate))
import pandas $ data = pandas.read_excel("public_health.xlsx", sheetname="Sheet1", parse_dates=["Visit Date"],dayfirst=True) $ data.head(2)
journalist_retweet_gender_summary(journalists_retweet_df[journalists_retweet_df.gender == 'M'])
minMaxDate.head()
trading_exemption_records.apply(lambda x: x['exemptions.psc_exempt_as_shares_admitted_on_market.exemption_type'] if not pd.isnull(x['exemptions.psc_exempt_as_shares_admitted_on_market.exemption_type']) else x['exemptions.psc_exempt_as_trading_on_regulated_market.exemption_type'],axis=1 ).value_counts()
labels = pd.DataFrame({'cutoff_time': date_range}) $ labels['next_cutoff_time'] = labels['cutoff_time'].shift(-1) $ labels['msno'] = customer_id $ labels.head()
reddit_comments_data = reddit_comments_data.withColumn('sentiment',senti_udf(reddit_comments_data.body))
X = preprocessing.scale(X)
neuron_no = 10 $ source_indices_L23exc_L23fs = np.where(np.array(conn_L23exc_L23fs.i)==neuron_no) $ target_indices_L23exc_L23fs = np.array(conn_L23exc_L23fs.j)[source_indices_L23exc_L23fs]
cog_simband_times[cog_simband_times.index == 'AH-OTS BHC0221-1']
player_search_count_df = pd.DataFrame.from_dict(player_search_count, orient='index').reset_index() $ player_search_count_df.columns = ['name', 'search_count'] $ players_df = pd.read_csv('~/dotaMediaTermPaper/data/players_df.csv') $ players_df = pd.merge(player_search_count_df, players_df, left_on='name', right_on='Name', how='right')
changeDate = lambda x: x.weekday() $ sLength = len(df['Date']) $ Weekdays = pd.Series(np.random.randn(sLength)) $ Weekdays = df['Date'].apply(changeDate) $ df = df.assign(Weekdays=Weekdays.values)
marvelPTags = wikiMarvelSoup.body.findAll('p') $ for pTag in marvelPTags[:8]: $     print(pTag.text)
df = pd.DataFrame(data=fruits_and_veggies) $ df
plt.subplots(figsize=(15, 7)) $ df['date_created'].hist(bins=30) $ plt.xlabel('Date of account creation') $ plt.ylabel('Num of created accounts');
weather.head(10)
(null_vals>(prob2-prob1)).mean()
df[(df.full_sq>10)&(df.full_sq<1500)]
autos = autos[autos['registration_year'].between(1900,2016)] $ autos['registration_year'].value_counts(normalize=True) $
pres_df['location'].isnull().sum(), pres_df.shape
df.show()
pageviews_tags.info()
secclintondf['tags'] = secclintondf.text.apply(lambda s: TextBlob(s).tags)
df_new['intercept'] = 1 $ logit = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ results = logit.fit() $ results.summary()
df_ad_airings_5.info() $
tweet_json.head()
neg_columns = [word for word in columns if word in negative_words] $ dtm_neg = dtm_df[neg_columns] $ dtm_neg['neg_count'] = dtm_neg.sum(axis=1) $ dtm_neg['neg_count']
%%timeit -n 10 $ for group, frame in df.groupby('STNAME'): $     avg = np.average(frame['CENSUS2010POP']) $     print('Counties in state ' + group + ' have an average population of ' + str(avg))
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ df_vow = pd.read_csv('./datasets/vow.csv')
print("Loaded Data contains {} rows and {} columns".format(*taxiData.shape))
user=tweet.author $ for param in dir(user): $     if not param.startswith("_"): $         print ("%s : %s" % (param, eval("user." + param)))
pair.tail()
s.str.split(' ')
weather.loc[weather.events == 'rain', 'events'] = "Rain" $ weather.loc[weather.events.isnull(), 'events'] = "Normal"
df3 = df[df.Language == 'en'] $ df3.drop(['Language'],1)
open('test_data//open_close_test.txt', encoding='utf8')
collection.delete_snapshot('snapshot_name') $
tweet_archive_clean = pd.merge(tweet_archive_clean, info, on = 'tweet_id', how = 'inner')
data.groupby(['Name'])['Salary'].sum()
News_outlets_df.dtypes
import statsmodels.api as sm $ logit_model = sm.Logit(y, X) $ result = logit_model.fit()
tweets.sort_values(by="frequency", ascending=True).head()
image_predictions_clean['p1_dog'].value_counts()
tce2.drop(columns=['ElectronicCollection'],inplace=True) $ tce2
cust_data.sort_values(by='MonthlyIncome', ascending=False).head(10) $
train_view.sort_values(by=4, ascending=False)[0:10]
free_data.groupby('age_cat').mean()
cars= cars[(cars.yearOfRegistration <= 2016)& (cars.yearOfRegistration >= 1950) & (cars.price>=100) & (cars.price<=160000)] 
X_train = df.loc[:25000, 'review'].values $ y_train = df.loc[:25000, 'sentiment'].values $ X_test = df.loc[25000:, 'review'].values $ y_test = df.loc[25000:, 'sentiment'].values
num_data=pd.DataFrame(df[['sale','volume_sold_l','bottles_sold','state_cost','state_retail','bottle_ml']],index=None) $ num_data.head()
tw_clean[tw_clean.text.str.startswith('RT @')].shape[0]
journalist_retweet_gender_summary(journalists_retweet_df[journalists_retweet_df.gender == 'F']) $
food["created_time"].head()
uber_15.head()
df.describe()
def fahr_to_kelvin(temp): $     temp_kelvin = ((temp - 32)*5/9) + 273.15 $     return temp_kelvin
row_0.index
brand_counts = autos["brand"].value_counts(normalize=True) $ most_common = brand_counts[brand_counts > .05].index $ print(most_common)
print("Odds against Bob winning: %i to 1" %((1. - freq_prob) / freq_prob))
compared_resuts.to_csv("data/output/logitregres.csv")
from sklearn.metrics import f1_score $ from sklearn.neighbors import KNeighborsClassifier
df[df.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro=']
rollrank_fxn = lambda x: x.rolling(200,min_periods=20).apply(lambda x: pd.Series(x).rank(pct=True)[0],raw=True) $ features['f11'] = prices.groupby(level='symbol').volume.apply(rollrank_fxn)
data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde')) $ data
plt.title('GC_MARK_MS_content') $ aggregated_content.plot(kind='bar', figsize=(15, 7))
to_be_predicted_Day2 = 81.77623296 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
archive_copy.info()
questions = pd.concat([questions.drop('ad_source', axis=1), ad_source], axis=1)
Imagenes_data.describe()
df_twitter_archive_master=pd.read_csv('twitter_archive_master.csv')
precipitation_df.plot(kind='area',grid=True) $ plt.title(f"Precipitation in Honolulu from {str(data_start_date)} to {vac_start_date}") $ plt.xticks(rotation=45) $ plt.gray() $ plt.show()
n_inc_series = 5 $ Data['train'] = pd.concat([Data['train']]*n_inc_series, ignore_index=True) $ Data['train'].loc[:,'relevant_last_step'] = Data['train']['res_store_date_partial_sum-0'].apply(lambda n: randint(-39, -1))
df = df.sort_values(by=['seq_id','work_day'],axis=0,ascending=[True, False])
dayofweek.count()
    pandas_df = spark.read.format("csv").options(header="true").load("./SIGHTINGS.csv") $     pandas_df.head()
df.set_index('Opened')['CaseID'].resample('A').count()
print('Feature engineering started at : ', datetime.datetime.now())
from selenium import webdriver $ driver = webdriver.Chrome()
var_per_portfolio = res.groupBy('date', 'portfolio').sum() $ var_per_portfolio = var_per_portfolio.map(lambda r: (r[0], r[1], r[2], float(var(np.array(r[3:]) - r[2])))) $ var_per_portfolio = sql.createDataFrame(var_per_portfolio, schema=['date', 'portfolio', 'neutral', 'var'])
autos.head()
print(X_train.iloc[0,]) $ print(cvec.get_feature_names()[64], cvec.get_feature_names()[94])
cats_df.describe()
fin_r_monthly = fin_r.resample('M').asfreq()
match_id(merkmale, merkmale.Merkmalcode.isin(['KG'])).Merkmalcode.unique()
bnb.groupby('language') $
print('SQRT(X) = \n', np.sqrt(fruits))
z_values, _ = create_latent(nn_aae.nn_enc, test_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ recon_x = create_sample(nn_aae.nn_dec, z_values) $ plot_mnist_sample(recon_x)
tmp_input = pd.DataFrame(tail.iloc[-1,lookforward_window:]).T.values $ tmp_input[0] = scaler.transform(tmp_input[0].reshape(-1,1)).reshape(1,-1) $ print(tmp_input.shape) $ tmp_input = np.reshape(tmp_input, (tmp_input.shape[0], 1, tmp_input.shape[1])) $ print(tmp_input.shape)
store_items.dropna(axis=0) # or store_items.dropna(axis=0, inplace=True) 
state_DataFrames_list = [] $ for item in state_keys_list: $     state_DataFrames_list.append(state_DataFrames[item])
data.dropna()
gene_df['length'] = gene_df.end - gene_df.start + 1 $ gene_df['length'].describe()
import getpass $ account = "root@r2lab.inria.fr" $ password = getpass.getpass(f"Enter password for {account} : ")
people.eval("body_mass_index = weight / (height/100) ** 2", inplace=True) $ people
from bmtk.analyzer import nodes_table $ nodes_table('network/recurrent_network/nodes.h5', 'V1')
vals2.sum(), vals2.min(), vals2.max()
cdata[cdata.Number_TD > 1]
sess.get_data('ibm us equity','px last')
user = toggl.request("https://www.toggl.com/api/v8/me")
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
ls_not_numeric = [not pd.api.types.is_numeric_dtype(dtype) for dtype in df_with_metac_with_onc.dtypes] $ prog = re.compile('DATE[0-9]*$') $ ls_not_date = [not bool(prog.search(colname)) for colname in df_with_metac_with_onc.columns] $ ls_both = [num and date for num, date in zip(ls_not_numeric, ls_not_date)] $ df_with_metac_with_onc.loc[:,ls_both].nunique()
misaligned_row_count = ab_df[((ab_df['group'] == 'treatment') & (ab_df['landing_page'] == 'old_page')) | ((ab_df['group'] == 'control') & (ab_df['landing_page'] == 'new_page'))]['user_id'].count() $
url = 'http://fantasysports.yahooapis.com/fantasy/v2/leagues;league_keys=nfl.l.427049' $ r = s.get(url, params={'format': 'json'}) $ r.status_code
params = {'figure.figsize': [8, 8],'axes.grid.axis': 'both', 'axes.grid': True,'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(RN_PA_duration, params=params, freq=31, title='RN/PA Decomposition')
df.info()
mod = sm.Logit(new_df['converted'], new_df[['intercept', 'US', 'CA']]) $ results = mod.fit()
df = pd.read_pickle('Data/NLP.pkl')
spp['DK'] = spp[spp.columns[spp.columns.str.contains('DK')==True]].sum(axis=1) $ spp['DE'] = spp[spp.columns[spp.columns.str.contains('DE')==True]].sum(axis=1)
greater_p_diffs = (p_diffs > ab_data_diff).mean() $ greater_p_diffs
print('Reply tweets: ' + str(len(tweets_clean.query('in_reply_to_status_id == in_reply_to_status_id')))) $ print('Total tweets: ' + str(len(tweets_clean))) $ tweets_clean = tweets_clean[tweets_clean.in_reply_to_status_id != tweets_clean.in_reply_to_status_id]
import time $ twitter_archive_master.timestamp = twitter_archive_master.timestamp.apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(x,'%Y-%m-%d %H:%M:%S +0000'))) $ twitter_archive_master.timestamp = np.array(twitter_archive_master.timestamp,dtype='datetime64[ns]')
P.columns
train_session.head()
from pprint import pprint $ oldest_month = 0 $ trump_tweets $ oldest_month = trump_tweets[len(trump_tweets)-1]["created_at"].split()[1] $ oldest_month $
twitter_archive_df_clean.drop(['rating_numerator', 'rating_denominator'], axis=1, inplace=True)
dd_df['gearbox'].fillna('gear_unknown', inplace=True)
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False]["user_id"].count()
print(cv_score) $ print(np.mean(cv_score)) $ print(accuracy_score(y_train, cv_predict_score))
plt.scatter(ort_avg17, drt_avg17);
plt.plot(sample[:, 2], sample[:, 3], ',k', alpha=0.1) $ plt.xlabel('$g_1$') $ plt.ylabel('$g_2$') $ print("g1 mean: {0:.2f}".format(sample[:, 2].mean())) $ print("g2 mean: {0:.2f}".format(sample[:, 3].mean()))
plt.rcParams['figure.figsize'] = (20.0, 10.0)
mit.groupby('week_day').num_commits.sum()
rank_meters['daytime_7_12'].most_common(11)[1:]
test_df.columns[test_df.isnull().any()].tolist()
twitter_df_clean['timestamp'] = pd.to_datetime(twitter_df_clean['timestamp'])
tweet_image_clean.shape $
train.shape[0] + new.shape[0]
("15:18:55").split(":")
MICROSACC.plot_default(microsaccades,subtype="count/(6*20)")+ylab("microsaccaderate [1/s]")
restaurantsExcelFile = pd.ExcelFile("Restaurants.xlsx");
s.asfreq('8BM')
so_head.index
geometry = openmc.Geometry(root_universe)
df_sample.shape
top_songs['Country'].dtype
fin_p = pd.read_excel('../../datasets/crypto-index-fund/data-financial/price_tradfinance.xlsx', $                       index_col=0, parse_dates=True)
tmp_df.sample(10)
archive.sample(10)
1.0*sum(treatment_df['landing_page']=='new_page')/ n_valid_users
draft_df.to_pickle("draft_df.pkl")
SELECTIONS = ["(pdmol.df.atom_type == 'O.2')", $               "(pdmol.df.atom_type == 'F')"] $ MOL2_FILE = "./data/40_mol2_files.mol2" $ def data_processor(mol2): $
from dotce.report import generate_chart
staff.T
data.describe().T
print len(com311) $ com311 = pd.merge(com311, df, how='outer', on ='Unique Key') $ print len(com311)
result = conn. execute(select([users.c.name, users.c.fullname]))
rng = pd.date_range('2005', '2012', freq='M') $ rng
build_cost_CO = cpq_CO.groupby(['Building ID',' Estimated Build Cost ']).size().reset_index().rename(columns={0:'count'}) $ build_cost_TX = cpq_TX.groupby(['Building ID',' Estimated Build Cost ']).size().reset_index().rename(columns={0:'count'}) $ build_cost_GA = cpq_GA.groupby(['Building ID',' Estimated Build Cost ']).size().reset_index().rename(columns={0:'count'})
df.iloc[0]
commits_per_repo = pd.read_pickle('data/pickled/commits.pkl') $ commits_per_repo.info()
logit_modB = sm.Logit(df_new_log['converted'], df_new_log[['intercept', 'group_treatment', 'country_US', 'country_UK']]) $ resultsB = logit_modB.fit() $ resultsB.summary()
import geopandas as gpd $ geoLineData = '/green-projects/project-waze_transportation_network/workspace/share/Scripts/data/Borough Boundaries.geojson' $ newYork_gdf = gpd.read_file(geoLineData) $ print(type(newYork_gdf)) $ newYork_gdf.crs = {'init': 'epsg:4326'}
ax = gamma_chart[['gamma','Risk','SR']].plot(x=['gamma'], secondary_y=['SR'], logx=True) $ ax.set_ylabel('Risk') $ ax.right_ax.set_ylabel('Sharpe Ratio');
n_old=df2.query('landing_page=="old_page"').count()[0] $ n_old
impressions = pd.read_sql_query("SELECT * from impressions"+RetSqlLimit("impressions",sqlLimit), conn) $ impressions_products = pd.read_sql_query("SELECT * from impressions_products"+RetSqlLimit("impressions",sqlLimit), conn) $ impressions['group'] = impressions['ab'].apply(lambda x: x[:1]) $ impressions['session'] = impressions['ab'].apply(lambda x: x[2:])
kfpd.plugin = DataSynthesizerPlugin(mode='correlated_attribute_mode') $ fdf = time_method(kfpd, verbose = True, rerun_query = False, repetitions = 10) $ fdf.head()
hm_clean.count()
df.converted.mean()  #returns average of a value
traded_volumes.sort()
twitter_final.groupby('time_cat')['tweet_id'].count().reset_index(name="Count").sort_values(by='Count', ascending=False)
import matplotlib.pyplot as plt $ from sklearn.metrics import roc_curve
meals = meals[meals.id.isin(tickets['meal_id'].unique())]
num_rows = df.shape[0] $ print('Number of rows in dataset: ',num_rows)
df_vow.dtypes
hourly_df['Price_Change'].value_counts()
comment_list = sorted(Counter(commenters).items(), key=lambda x: x[1], reverse=True)
plt.hist(x);
students.weight.plot.hist()
X.shape
winter[winter['Complaint Type'] == 'Homeless Encampment']['Unique Key'].count()
scoring_ind.info()
QLESQ = QLESQ[(QLESQ["level"]=="Level 1") & (QLESQ["days_baseline"] <= 7)] $ QLESQ.shape
df_loan3=pd.DataFrame(df_loan).merge(actual_payments.loc[actual_payments.iso_date==EOM_date.date(),['fk_loan','in_arrears_since', $                                     'in_arrears_since_days',u'in_arrears_since_days_30360','bucket','bucket_pd']], $                                           left_index=True, right_on='fk_loan')
df_characters.head(10)
X_train, X_validation, y_train, y_validation = train_test_split(X_train,y_train,test_size=0.35,shuffle=False)
c_df.dtypes
age.loc['Alice']
df[df.msno == '++5wYjoMgQHoRuD3GbbvmphZbBBwymzv5Q4l8sywtuU=']
df.ID.dtype
sel_df.to_csv('crime_data_clean/beats_alternate_10_17.csv')
merge[merge.columns[45]].value_counts() $
learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))
if 0 == 1: $     news_title_docs_high_freq_words_df = pd.concat([news_titles_sr, high_freq_words_sr], axis=1) $     news_title_docs_high_freq_words_df.columns = ['news_title_doc', 'high_freq_words'] $     news_title_docs_high_freq_words_df.to_pickle(news_title_docs_high_freq_words_df_pkl)
cols = ['win','finishedDiameter','drillingMethod','totalDepth','finishedDepth','screenDepth','geologicLog'] $ wlwellfeatures = pd.read_csv(route+'wlwellfeatures.txt',header = None, $                              names = cols, delimiter='\t',error_bad_lines=False)
image = image_soup .find('div', class_='carousel_items') $ image_url = image.article['style'] $ url = image_url.split('/s')[-1].split('.')[0] $ featured_image_url= 'https://www.jpl.nasa.gov' +'/s'+ url + '.jpg' $ print(featured_image_url )
test_df['pred'] = np.argmax(pred,axis=1)
from IPython.display import HTML $ import os $ video_names = list(filter(lambda s:s.endswith(".mp4"),os.listdir("./videos/"))) $
df['field1']  #nos obtiene lacomunda llamada 'field1' $ df.loc[0,'field1']   #obtenemos el valor de la fila '0' y la  comunda 'field1' $ l=df['field1']>59  #se aplica la condicional para cada valor de la columna $ l $ df.loc[df['field1']>79,'field1'] #hacemos un filtro a la columna df.loc[boleano_o_int, 'name_columna'] $
weekly.head()
pystore.set_path('./pystore_demo') $ pystore.get_path()
df = pd.read_csv('data/test1.csv', $                  parse_dates=['date'], $                  index_col='date') $ df
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
df['message_vec'] = vec2.fit_transform(df['message']) $
X_test_tfidf = vectorizer.transform(X_test) $ y_pred_train = svr.predict(X_train_tfidf) $ y_pred_test = svr.predict(X_test_tfidf)
b_cal_q1.columns
autos.drop(["num_photos", "seller", "offer_type"], axis = 1)
t2['p1'] = t2['p1'].str.replace('_', ' ') $ t2['p2'] = t2['p2'].str.replace('_', ' ') $ t2['p3'] = t2['p3'].str.replace('_', ' ')
p_new = len(df2.query( 'converted==1'))/len(df2.index) $ p_new
twitter_archive_master['rating_denominator'].value_counts()
ls_not_numeric = [not pd.api.types.is_numeric_dtype(dtype) for dtype in df_onc_no_metac.dtypes] $ prog = re.compile('DATE[0-9]*$') $ ls_not_date = [not bool(prog.search(colname)) for colname in df_onc_no_metac.columns] $ ls_both = [num and date for num, date in zip(ls_not_numeric, ls_not_date)] $ df_onc_no_metac.loc[:,ls_both].nunique()
type(x.iloc[0]['Max_temp'])
print("The standard deviation is: $", donations['Donation Amount'].std()) $ print("The variance is: $", donations['Donation Amount'].var()) $ print("The 1st, 2nd, 3rd quantiles are: \n", donations['Donation Amount'].quantile([0.25, 0.50, 0.75]))
final_ticker_data = avg_data.T
data['close'].plot() $ plt.xlim('2014-06-15','2014-08-15') # change x-axis $ plt.ylim(60,80) # change y-axis $ plt.show() # will be mirrored if loaded $ data = data[(data.t > 519) & (data.t < 564)] $
r=df.rolling(window =3,min_periods=1) $ r['A'].aggregate(np.sum)
auto.tail(15) 
act_p_new = df2.query('group=="treatment"')['converted'].mean() $ act_p_old = df2.query('group=="control"')['converted'].mean() $ act_diff = np.array(act_p_new - act_p_old) $ (p_diffs > act_diff).mean()
import calendar $ iowa['Day of Week'] = iowa.Date.apply(lambda x: calendar.day_name[x.weekday()]) $ total_sales_2015_by_day_of_week = iowa.groupby('Day of Week')['Sale (Dollars)'].sum() $ total_sales_2015_by_day_of_week.plot(kind = 'bar')
df_new.rename(columns={'country_US':'intercept_us', 'country_UK':'intercept_uk', 'country_CA':'intercept_ca'}, inplace=True) $ df_new.head()
oppose.groupby(["contributor_lastname","contributor_firstname"]).amount.sum().reset_index().sort_values("amount",ascending=False).head(10)
tweet_df.tweet_created_at.min()
avg_monthly_search_volumes = {"AU": 301000, "CA": 246000, "DE": 246000, "ES": 165000, "FR": 201000, "GB": 150000, "IT": 201000, "NL": 110000, "PT": 110000, "US": 500000} $ data_countries["avg_monthly_search_volume"] = data_countries.apply(lambda x: avg_monthly_search_volumes[x.country_destination], axis=1) $
out_columns=[primary_temp_column] +incremental_precip_columns+general_data_columns +wind_dir_columns #columns to include in output $ save_name=Glacier.lower()+ Station + "_15min_"+"LVL2.csv" #filename $ save_pth=os.path.join(save_dir, save_name)
to_be_predicted_Day2 = 34.59184648 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df2 = df[(df.group == 'treatment') & (df.landing_page == 'new_page') | (df.group == 'control') & (df.landing_page == 'old_page')] $ df2.head()         
for tweet in tweepy.Cursor(api.search, q="unitedAIRLINES", count=100, lang="en", since="2017-04-03").items(): $     print(tweet.created_at, tweet.text) $     analysis = tb(tweet.text) $     print(analysis.sentiment.polarity)
display(data.head(10))
Pnew=df2['converted'].mean() $ Pnew
pd.Period('2006Q1', 'Q-MAR')
compiled_data=pd.merge(sen_dat,bot_data, how='left', on=['screen_name']) $ compiled_data=compiled_data.rename(columns = {'english':'botometer'}) $ old_compiled_data=compiled_data $ old_compiled_data=compiled_data
def find_k_worst(y, y_hat, k=10): $     return np.argsort(y - y_hat)[:k]
plt.style.use('ggplot') $ bb['close'].apply(rank_performance).value_counts().plot(kind='barh')
engine.execute('USE twitter')
condos = condos[condos.STATUS != 'RETIRE']
aapl=np.log(aapl)-np.log(aapl).shift(1) $ aapl.tail()
df_predictions['p2_dog'].value_counts()
ps.sort_index().tail(600)
df_enhanced[['dog_name']].groupby(['dog_name'])['dog_name'].size()
'my string my'.rfind('my')
followers = api.GetFollowers() $ print([u.screen_name for u in followers])
labeled_features.loc[labeled_features['failure'] == 'comp4'][:16]
with open('atom.dat') as f: $     print(f.read())
data = aapl.get_call_data(expiry=aapl.expiry_dates[4]) $ data.iloc[0:5:, 0:5]
plt.scatter(sing_fam.sqft.values, sing_fam.rp1lndval.values);
mask = (a + b).isnull() $ mask
os.mkdir('input')
store_items.fillna(0)
(_train.count()+_test.count()) == train.count()
! wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/
mb = pd.read_table("Data/microbiome.csv", sep=',')
countries_df = pd.read_csv('countries.csv') $ countries_df.head() # we firstly see that we need to match the user_id $
[[f.filename, f.file_size] for f in my_zip.filelist]
figure.set_ylabel('value in USD [$]');
df_countries = pd.read_csv('countries.csv') $ df_countries.head()
logit_mod_1 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'CA', 'UK']]) $ results = logit_mod_1.fit() $ results.summary()
bird_data.date_time.head()
pd.DataFrame({'Adaptive Capacity' : [x[1] for x in HARVEY_92_USERS_AC]}).hist(bins=[1,2,3,4,5,6,7,8,10,100,200,1000])
clean_predictions.columns
sub = pd.DataFrame() $ sub["listing_id"] = df["listing_id"] $ for label in ["high", "medium", "low"]: $     sub[label] = y[:, labels2idx[label]] $ sub.to_csv("submission_rf.csv", index=False)
pd.Series(np.random.randn(1000)).plot(kind='hist', bins=20, color='Y');
to_be_predicted_Day5 = 82.26274693 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(doc_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly Doctor Hours Partial Autocorrelation')
datetime.now()
plt.violinplot(resampled1_groups['Sales_in_CAD'])
df.head()
Customer.withdraw(jeff, 200.0) $ jeff.balance           # Shows 700.0
crf.fit(X_train, Y_train) $ train_score = crf.score(X_train, Y_train) $ print('crf training score is: %f' %(train_score)) $ test_score = crf.score(X_test, Y_test) $ print('crf test score is: %f' %(test_score))
imagelist $ for k in range(len(imagelist)): $     print('run txt2pdf.py -o' + '"' + imagelist[k][:-4] + '.pdf' + '"' + ' '  + '"' +  imagelist[k] + '"' )
nps.crs
sub_df.shape
tweets['isRetweet'].value_counts(dropna=False) $ tweets['truncated'].value_counts(dropna=False)
df.plot(kind='area') $ plt.ylabel("Count")
small_movies_file = os.path.join(dataset, 'ml-latest-small', 'movies.csv') $ small_movies_raw_data, small_movies_raw_data_header = read_file(small_movies_file) $
lst = [5, 1, 6, 7, 4, 2, 3] $ mean_for_each_weekday['day_order'] = pd.Series(lst, index=mean_for_each_weekday.index) $ mean_for_each_weekday
df_plot_gene=pd.DataFrame(df_gene['dates'].value_counts()) $ from pylab import rcParams $ rcParams['figure.figsize'] = 10, 10 $ ax= df_plot_gene.plot(style='.-', markevery=5,figsize = (10,7))
ab_data.landing_page.value_counts()
previous_month_date = end_date - timedelta(days=30) $ pr = PullRequests(github_index).get_cardinality("id").since(start=previous_month_date).until(end=end_date) $ get_aggs(pr)
pivoted = departures.pivot_table(index='Start Date', columns='Start Station', values='Duration')
round(df2[df2['group'] == 'control']['converted'].mean(), 4)
doc = ao18_qual_coll.find_one({"id_str":'951036001832509440'})
own_star = pd.merge(owns, stars, how='outer', on=['user_id', 'repo_id'], suffixes=('_own', '_star')) $ own_star.info() $ own_star.head()
from scipy import misc $ image = misc.imread('data/image.bmp')
test= test.reset_index(drop = True) $ test['floor'] = pd.Series(predictions) $ datatest = pd.concat([train, test]) $ datatest = datatest.reset_index(drop = True)
df[unique_cols].head(2)
with open('./data/clixo-tree-layout.cyjs', 'r') as f: $     clixo = json.load(f) $ nodes = clixo['elements']['nodes'] $ len(nodes)
maint['datetime'] = pd.to_datetime(maint['datetime'], format="%Y-%m-%d %H:%M:%S") $ maint['comp'] = maint['comp'].astype('category') $ print('Total number of maintenance records: {}'.format(len(maint.index))) $ maint.head()
cityID = '3df4f427b5a60fea' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Antonio.append(tweet) 
results.to_csv("mb.csv") $ results.to_pickle("football_pickle") $ pd.read_pickle("football_pickle")
baseball.rank(ascending=False).head()
train_data['fuelType_int'] = train_data['fuelType'].apply(get_integer4) $ test_data['fuelType_int'] = test_data['fuelType'].apply(get_integer4) $ del train_data['fuelType'] $ del test_data['fuelType']
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = df2.query("group == 'control'").shape[0] $ n_new = df2.query("group == 'treatment'").shape[0]
historicFollowers.head()
itemDataWithInt=itemData.join(itemDataWithIntListIDs,on='id',how='inner') $ print itemDataWithInt.shape $ itemDataWithInt.head()
ypred = model.predict(np.random.random((300, 95,1)))
pf = pd.read_csv('BOS.csv') $ for plyr in inj: $     pf = pf[pf.Name != plyr] $ pf.head()
(datecomp['OrigRegDate'] == datecomp['RegDate']).value_counts()
r[r['weight']==0.5]
data_issues=pd.read_json('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-issues.json',lines=True)
pres_df['hour_aired'].dtypes
df['amount_cleanup'] = df.amount_initial.str.replace(',', '') $ df['amount_cleanup'] = df.amount_cleanup.str.replace('$', '') $ df['amount'] = df.amount_cleanup.astype(float)
bwd_train = roll_train_df.rolling(7,min_periods=1).sum() $ bwd_test = roll_test_df.rolling(7,min_periods=1).sum()
regression_line = [] $ for x in xs: $     regression_line.append((m*x)+b)
van_final.apply(lambda x: sum(x.isna()))
for c in ['date_listed', 'last_review', 'first_review']: $     airbnb_df[c] = pd.to_datetime(airbnb_df[c]) $
dummies_country = pd.get_dummies(df2['country']) $ dummies_country.head()
train.first_affiliate_tracked.isnull().sum()
train, validation = train_test_split(df_train, test_size=0.1)
df2 = df.loc[~((df.group == 'control')&(df.landing_page == 'new_page')),:].copy() $ df2 = df2.loc[~((df2.group == 'treatment')&(df2.landing_page == 'old_page')),:] $ df2.shape
print('Features importances:') $ rfc_imp = rfc.feature_importances_ $ features_names = var_num + var_cat $ for i in range(0,len(rfc_imp)): $     print (features_names[i]+": %s" % "{0:.1}".format(rfc_imp[i]))
df = pd.DataFrame({"date": date_list, "tmin":tmin, "tavg": tavg, "tmax": tmax})
with open(saem_women_save, mode='w', encoding='utf-8') as f: $     f.write(SAEMRequest.text)
mapping = pd.read_excel('store_url_mapping.xlsx') $ mapping['rpc'] = mapping['rpc'].astype(str) $ all_data['rpc'] = all_data['rpc'].astype(str) $ mapping.rename(columns={'Store':'store'}, inplace=True) $
grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) $ grades = pd.DataFrame(grades_array, columns=["sep", "oct", "nov"], index=["alice","bob","charles","darwin"]) $ grades
LARGE_GRID.plot_accuracy(raw_large_grid_df, option='dodge')
data.text.str.contains("snack").resample("1T").sum().plot() $ data.text.str.contains("sandwich").resample("1T").sum().plot() $ data.text.str.contains("food").resample("1T").sum().plot() $ sns.plt.legend(["snack", "sandwich", "food"])
def sinplot(flip=1): $     x = np.linspace(0, 14, 100) $     for i in range(1, 7): $         plt.plot(x, np.sin(x + i * .5) * (7 - i) * flip)
from bs4 import BeautifulSoup $ example1 = BeautifulSoup(df.text[279], 'lxml') $ print example1.get_text()
price_data.iloc[0:5]
print([0.25,0.5,0.75,1])
stories = pd.concat([stories.drop(['submitter_user'], axis=1), $                      user_df], axis=1)
index_df = index_ts[['price_date', 'ticker', 'adj_close_price']].copy().set_index('price_date') $ index_df.head()
ratio = result["Fail"].div(result["Pass"]) $ ratio.sort_values(ascending=False, inplace=True) $ ratio $
print(df.index[0])
df = pd.read_csv("Datasets/CDR.csv") $ df['CallDate'] = pd.to_datetime(df['CallDate']) $ df['CallTime'] = pd.to_datetime(df['CallTime']) $ df.dtypes
van_final = pd.merge(van,pages,how='left',on="pagetitle") $ ben_final = pd.merge(ben,pages,how='left',on="pagetitle") $ van_final.head(10) $ ben_final.head(10)
def false_positive(predicted_pivot, test): $     predicted_table = pd.DataFrame(predicted_pivot.unstack()).groupby(['CUSTOMER_ID','MATNR']).sum().reset_index() $     predicted_item = predicted_table[predicted_table[0]==1] $     upper = pd.merge(predicted_item, test, how='left', on=['CUSTOMER_ID','MATNR']) $     return sum(np.isnan(upper['QUANTITY']))
df.loc[df.userTimezone == 'Mountain Time (US & Canada)', :]
df2 = df.query("(group == 'control' & landing_page == 'old_page') or (group == 'treatment' & landing_page == 'new_page')")
train.cust_id.value_counts().unique()
dates = pd.date_range('2018-03-22',periods=ndays).astype('str') $ sim_ret = pd.DataFrame(sigma*np.random.randn(ndays,nscen)+mu,index=dates) $ sim_closes = (closes_aapl.iloc[-1].AAPL)*np.exp(sim_ret.cumsum())
df_stars['business_id'].nunique(), df_stars['user_id'].nunique()
subwaydf.iloc[154900:154915] #this low number seems to be because entries and exits resets
df = pd.read_csv("FuelConsumption.csv") $ df.head() $
df = df.loc[:, ["id", "created_at", "text", "retweet_count", "user_name"]]
df_image.info()
from IPython.display import SVG $ from keras.utils.vis_utils import model_to_dot $ SVG(model_to_dot(encoder_model_inference).create(prog='dot', format='svg'))
df.shape
df.loc[[101,103,105]]
questions = questions.drop(questions.index[[9,22]])
event_list = pd.DataFrame() $ event_list['event_id'] = df_events.event_id $ event_list['event_start_at'] = df_events.event_start_at $ event_list
TotalNewPage = df2.query('landing_page == "new_page"')['user_id'].count() $ print("Probability of New Page: ",(TotalNewPage/NewTotalUser))
userByCountry_df  = youtube_df[youtube_df["channel_title"].isin(channel_namesC)]
recipes.shape
codes.show()
march_2016 = pd.Period('2016-03', freq='M')
tweets_master_df[tweets_master_df.isnull().any(axis=1)]
reviews.price.astype(str)
hist = plt.hist(rf.predict_proba(validation_data[Features])[:,1], bins=100) $ plt.title('Probability Histogram of Charged Off', y=1.02, fontsize=15) $ plt.xlabel('Probability', fontsize=12) $ plt.ylabel('Frequency', fontsize=12)
all_tables_df.OBJECT_NAME
dfz['retweet_count'].plot(color = 'red', label='Retweets') $ dfz['favorite_count'].plot(color = 'blue', label='Favorites')
oil_prices = joined[['date', 'dcoilwtico']].copy().drop_duplicates(['date']).reset_index(drop=True) $ oil_prices['oil_30_day_change'] = oil_prices['dcoilwtico'].pct_change(periods=30).fillna(method='bfill') $ oil_prices[['date', 'oil_30_day_change']].plot()
writer = pd.ExcelWriter("../visualizations/uber_day_of_year.xlsx")
store = join_df(store, store_states, "Store") $ len(store[store.State.isnull()])
pattern = '(.*)(ID\d)(.*)' $ matchresult = re.match(pattern, 'WIDTH_ID1')
def intermediate_cohort(row): $     if row['unit_flag']=='Completed atleast 10 units' and row['atleast_one_course_completed_or_not']=='Did not complete any course (Course started after 1st July 2018)': $         row['cohort']='Intermediate User' $     return row['cohort'] $ df_users_6['cohort']=df_users_6.apply(intermediate_cohort,axis=1)
df_cond = df.dropna(subset=['lead_result_max_bucket'])
for feature in sorted_features: $     importance = feature[1] $     print feature[0] $     print X.columns[feature[0] + 5], importance
for i in compare[-1]: $     print('element: ', i) $     print('pre: ', data.view_count[i]) $     data.view_count[i] = data.suma[i] $     print('post: ', data.view_count[i])
Feature1.drop(['Master or Above'], axis = 1,inplace=True) $ Feature1.head()
df_M7 = pd.DataFrame({'DATE':dfM.DATE[13:-12],'ACTUAL':[i[0] for i in M7_actual], $                       'M7':[i[0] for i in M7_pred], $                      'BASELINE':dfM.t[13:-12]})
dfcopy = df.copy() $ dfcopy.sort_values('date').head()
crime = pd.read_csv('clean_data/KCPD_Crime_Data_2017_clean.csv') $ moon = pd.read_csv('clean_data/Moon_Data_2017_cleaned.csv')
autos['odometer'] = autos['odometer'].astype(int) $ autos.rename(columns={'odometer':'odometer_km'}, inplace=True) $ autos['odometer_km'].head()
corpus = matutils.Sparse2Corpus(counts)
pothole = df[df['Descriptor'] == 'Pothole'] $ pothole.groupby(pothole.index.weekday)['Created Date'].count().plot()
crimes_by_yr_month_type.reset_index(inplace=True)
staff['Number'] = staff.index $ staff
def stack_unstack(df, col_target): $     return (df[col_target].apply(pd.Series) $             .stack() $             .reset_index(level=1, drop=True) $             .to_frame(col_target))
sum(df.rating_denominator.value_counts()!=10)
autos["registration_year"].describe()
desktop = nvidia.filter(lambda p: not devices[int(p['adapter']['deviceID'], 16)].endswith("M")) $ desktop.map(lambda p: len(p['adapters'])).countByValue()
q = tf.distributions.Normal(loc=q_mu, scale=q_sigma) $ p = tf.distributions.Normal(loc=p_mu, scale=p_sigma)
typeof_city=pd.Series(total_ridepercity["index"]) $ typeby_city=pd.Series(total_ridepercity["type"]) $ colors =["lightcoral","lightskyblue","gold"] $ explode = (0.1, 0, 0)
RIDs_DXSUM = list(diagnosis_DXSUM_PDXCONV_ADNIALL['RID'].unique()) $ RIDs_ADSXLIST = list(diagnosis_ADSXLIST['RID'].unique()) $ RIDs_BLCHANGE = list(diagnosis_BLCHANGE['RID'].unique())
plt.plot(pipe.tracking_error)
print(autos['odometer_km'].unique().shape) $ print(autos['odometer_km'].describe()) $ print(autos['odometer_km'].value_counts().sort_index(ascending=True))      
f1_score(Y_valid, final_valid_pred_nbsvm1, average='weighted', labels=np.unique(final_valid_pred_nbsvm1.values))
my_date_only_rows= autos["date_crawled"].str[:10] #strip first 9 characters $ my_date_only_rows $
print ("The probability that an individual received the new page is %.5f"%(df2.query('landing_page=="new_page"').shape[0]/df2.shape[0]))
s.iloc[-3:]
df.head()
cityID = '73d1c1c11b675932' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chesapeake.append(tweet) 
plt.hist(null_vals) $ plt.axvline(x=obs_diff, color='red');
df = pd.read_pickle('/Users/emilytew/Documents/rwanda/Avocado_changed.pkl')
df.sort_values('Year').head(5)
my_df_loaded = pd.read_csv("my_df.csv", index_col=0) $ my_df_loaded
autos['ad_created'] = autos['ad_created'].str[:10] $ autos['ad_created'].value_counts(normalize=True, dropna=False).sort_index()
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2[['control','treatment']] = pd.get_dummies(df2['group']) $ df2.head()
df_tweet_clean.tweet_id = df_tweet_clean.tweet_id.astype(str)
ser4 = pd.Series(dic_a,index=["p","q","r","a"]) $ ser4
prcp_1_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation']) $ prcp_1_df.set_index('Precipitation Date', inplace=True) # Set the index by date $ prcp_1_df.count()
df_goog.loc[(df_goog.index.year == 2015)]
!python csv_to_tfrecords.py --csv_input=images/test/test_labels.csv --image_dir=images/test --output_path=test.record
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=26500) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df_tweets.reset_index().to_pickle("../tweets_extended.pkl")
auth = tweepy.OAuthHandler(consumer_key,consumer_secret) $ auth.set_access_token(access_token,access_token_secret) $ api = tweepy.API(auth,parser=tweepy.parsers.JSONParser())
assert isinstance(my_zip, zipfile.ZipFile) $ assert isinstance(list_names, list) $ assert all([isinstance(file, str) for file in list_names]) $
subs_and_comments = sub_df.merge(sub_comments, on='id', how='outer')
re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])
users.body['members']
item_item_rec = graphlab.recommender.item_similarity_recommender.create(sf_stars, $                                                                         user_id = 'user_id', $                                                                         item_id = 'business_id', $                                                                         target = 'stars')
print('Avg Words Per Snt Brown Corpus: '\ $       , len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents()))
result = pd.DataFrame(alg7.predict_proba(test[features]), index=test.index, columns=alg7.classes_) $ result.insert(0, 'ID', mid) $ result.head()
s = pd.Series([80,2,50], index=['HPI','Int_rate','US_GDP_Thousands']) $ print s
my_gempro.get_dssp_annotations()
y = np.log1p(y) $ y.hist(grid=True, bins=50)
combined_df.to_csv('manual_comment_results.csv', index=False)
May = df['2015-05'] $ May['Complaint Type'].value_counts().head(5)
num_var = data_type[data_type!='object'].index.tolist() $ num_var.remove('id') $ print len(num_var)
first_commit_timestamp = git_log.iloc[-1].timestamp $ last_commit_timestamp = pd.to_datetime('now') $ corrected_log = git_log[(first_commit_timestamp <= git_log.timestamp) & (git_log.timestamp <= last_commit_timestamp)] $ corrected_log['timestamp'].describe()
investors_df = pd.merge(investors_df,most_recent_investment, on = 'uuid')
apple["Signal"].value_counts()
typesub2017 = typesub2017.dropna()
population.loc['California':'Illinois']
perf_test = pd.read_csv('performance_test.csv') $ print('Testing data shape: ', perf_test.shape) $ perf_test.head(50)
News_paragraphs = Mars_soup.find('div',class_="article_teaser_body").text $ print (News_paragraphs)
number_of_commits =git_log['timestamp'].size $ number_of_authors = git_log.dropna(how='any')['author'].unique().size $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
zscore_fun_improved = lambda x: (x - x.rolling(window=200, min_periods=20).mean())/ x.rolling(window=200, min_periods=20).std() $ features['f10'] =prices.groupby(level='symbol').close.apply(zscore_fun_improved) $ features.f10.unstack().plot.kde(title='Z-Scores (Correct)')
rodelar.editCount()
win_rows = inputs['label'].sum() $ total_rows = inputs['label'].size $ perc_wins = win_rows / total_rows * 100 $ print('there are {0}({1:.2f}%) wins out of a total of {2} rows/matches'.format(win_rows, perc_wins, total_rows))
xmlData['build_year'] = pd.to_datetime(xmlData['build_year'], format = '%Y', errors = 'raise') $ xmlData['renovate_year'].replace({'0':''}, inplace = True) $ xmlData['renovate_year'] = pd.to_datetime(xmlData['renovate_year'], format = '%Y', errors = 'coerce') $ xmlData['build_year'] = [d.strftime('%Y') if not pd.isnull(d) else '' for d in xmlData['build_year']] $ xmlData['renovate_year'] = [d.strftime('%Y') if not pd.isnull(d) else '' for d in xmlData['renovate_year']]
data.groupby('rate_marriage').mean() $
df30458 = df[df['bikeid']== '30458'] #create df with only rows that have 30458 as bikeid $ x = df30458.groupby('end_station_name').count() $ x.sort_values(by= 'tripduration', ascending = False).head() # we use tripduration as a proxy to sort the values $
pr('Starting to read file... (3 min)') $ tw = pd.read_csv(filename, sep='\t', encoding='utf-8', escapechar='\\', names=columns_header, $                       quoting=csv.QUOTE_NONE, na_values='N', header=None) $ pr('File is loaded.')
closing_prices = [] $ for ele in r.json()['dataset_data']['data']: $     closing_prices.append(ele[4]) $ print('Highest change between any two days based on Closing Price - {}'.format(max(closing_prices) - min(closing_prices)))
conn.execute("update related_content set duration=29*60 where content_id=700")
samples_query.display_records(10)
vars2 = [x for x in dfa.ix[:,6:54]] $ vars2
import statsmodels.api as sm $ log_reg = sm.Logit(df['converted'], df[['intercept', 'treatment']])
corpus_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_tf.mm')) $ corpus_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_tfidf.mm'))
import datetime as dt $ Todays_date = dt.date.today() - dt.timedelta(days=365) $ print("Today's Date:", Todays_date)
import os # To use command line like instructions $ if not os.path.exists(DirSaveOutput): os.makedirs(DirSaveOutput)
df_click.info()
df_train.loc[:, 'features'] = df_train.apply(lambda row: tweet_to_features(row['text']), axis=1) $ display(df_train.head())
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df_new['country'].value_counts()
origin = str(output_path) $ dest = Path(temp_dir.name, 'repacked.h5') $ !bigmatrix_repack --matrix-chunkshape "(5000,1)" --force $output_path $dest
plt.plot(xs, ys)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data2.csv" $ mydata = pd.read_csv(path, sep =',', header=None) $ mydata.head(5)
sqlContext.sql("select id, borrower from world_bank limit 2").toPandas()
html = browser.html $ weather_soup = BeautifulSoup(html, 'html.parser')
sort=df.sort_values('score',ascending=False,axis=0)[:25] $ sort.loc[:,['author','content','score']][:5]
print(len(train_df['device'].unique()))
autos["odometer_km"].describe()
plot_chernoff_data(df2, 1e-6, 1, "Nth = 2.0") $ plt.savefig('../output/g_perr_vs_M_2.pdf', bbox_inches='tight')
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ with open('image-predictions.tsv','wb') as file: $         file.write(response.content)
actual_value_second_measure=pd.DataFrame(actual_value_second_measure) $ actual_value_second_measure.replace(2,1, inplace=True)
ac['Registration Date'].describe()
fig, ax = plt.subplots() $ typesub2017['Wind Offshore'].plot(ax=ax, title="Offshore Wind Energy Generation 2017 (DE-AT-LUX)", lw=0.7) $ ax.set_ylabel("Offshore Wind Energy") $ ax.set_xlabel("Time")
df_data_1.dtypes
from sklearn.neighbors import KNeighborsClassifier $ knn = KNeighborsClassifier(n_neighbors = 101) $ knn.fit(x_train,y_train > 0)
df.groupby("newsOutlet")["compound"].min()
Xs = df_sample.values
auto_new.CarModel.value_counts()
reviews['n']=0 $ reviews.groupby([reviews.country,reviews.variety]).n.count().sort_values(ascending=False)
print(df.info()) $ pd.isna(df).sum() $
SANDAG_age_df[SANDAG_age_df['AGE_RANGE'] == '10 to 19'].head()
smpl.count()
run txt2pdf.py -o '2018-06-22 2014 FLORIDA HOSPITAL Sorted by payments.pdf'  '2018-06-22 2014 FLORIDA HOSPITAL Sorted by payments.txt'
npath = save_filepath + '/uva_hpc/pySUMMA_Demo_Example_Fig7_Using_TestCase_from_Hydroshare.ipynb' $ hs.addContentToExistingResource(resource_id, [npath])
event_sdf = events_grouped_no_nulls.sort( events_grouped_no_nulls['count'].desc() )
from sklearn.cluster import AffinityPropagation, MeanShift, KMeans $ from sklearn.manifold import TSNE
if True: $     train = train.round({'start_lng':2, 'end_lng':2, 'start_lat':2, 'end_lat':2}) $     test = test.round({'start_lng':2, 'end_lng':2, 'start_lat':2, 'end_lat':2})
youthUser1.head()
sql.sql('select date, fadd(scenarios) from scen group by date').collect()
print weather_data1.shape $ print weather_data1.columns.values
n_old = len(df2.query('landing_page == "old_page"')) $ n_old
plt.hist(newshows['first_year']) $ plt.title('Distribution of Release Years') $ plt.ylabel('Frequency') $ plt.xlabel('Year Released')
closes = pd.concat([msftA01[:3], aaplA01[:3]], keys=['MSFT', 'AAPL']) $ closes
print("Total number of people who follow me: {}".format(len(my_followers)))
user.query("location not in ['DC', 'NYC']").head(3)
_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled) $ accuracy_score(y_train, y_pred)
tzs = tweets_df['userTimezone'].value_counts()[:10] $ print(tzs) $
sessions_sample = pd.read_csv("../data/airbnb/sessions.csv", skiprows=skip_idx)
seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)
my_df_small_T = my_df_small.T $ my_df_large_T = my_df_large.T
plt.bar([1,2,3,4,5,6,7,8,9,10,11,12],monthly_percent_off) $ plt.xlabel('Month of 2017') $ plt.ylabel('Monthly % Error')
hrefs = tree.xpath('//a') $ for href in hrefs: $     print(href.text)  $     print(href.attrib)  
ts = pd.Series(np.random.randn(3), dates) $ ts
supertrimmed = prune_below_degree(tweetnet, 500) $ nx.info(supertrimmed) $ supertrimmed_degrees = supertrimmed.degree() $ plt.hist(supertrimmed_degrees.values(), 100) #Display histogram of node degrees in 100 bins
from datetime import datetime $ from dateutil import parser $ ind_date= pd.bdate_range('2015-01-01','2015-12-31') 
def train_classifier(X_train, y_train, X_test, y_test, classifier): $     train_features = CountVectorizer(tokenizer=tokenize_and_stem).fit_transform(X_train) $     classifier.fit(train_features_tokenized, y_train) $     return classifier
tweet_data_clean.head()
df2.head()
pd.date_range(start = '2018/08/01', end = '2018/08/20', freq = be)
print(trump.favorite_count.mean()) $ print(trump.retweet_count.mean())
sort_a_asc = noNulls['a'].asc()
ethc = ethc[np.isfinite(ethc['w_sentiment_score'])] $ ethc.info()
pd.Series(np.random.randn(5))
np.random.randn(6,4) $
locations = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\ $ filter(Measurement.station==Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()
display('df6', 'df7', "pd.merge(df6, df7, how='left')")
df = pd.concat([df1, df2[df2['State'] == 'PCS-etgl']])
senateAll.to_csv("../data/senateCrosswalk.csv") $ houseAll.to_csv("../data/houseCrosswalk.csv")
df2 = df2.drop_duplicates("user_id", keep="first")
df_errors = pd.DataFrame(new_errors, columns = ['tweet_id']) 
preprocessor.infer_subtypes() # this function tries to indentify different subtypes of data
m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range) $ lr = 1e-3
learn.sched.plot_loss()
scaler=StandardScaler() $ scaler.fit(x_train)
articles_by_pub = db.get_sql(sql) 
final.tail(3)
df_data = df_data.drop_duplicates(df_data.columns.difference(['ticketid'])) $ print '%d rows' % len(df_data)
we_rate_dogs.shape[0] - clean_rates.shape[0]
top_songs['Track Name'].isnull().sum()
df.loc['Equatorial Diameter:']
treatment_num = df2.query('group=="treatment"').shape[0] $ treatment_num
names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'] $ df_new.timestamp=pd.to_datetime(df_new.timestamp) $ df3 = df_new.join(pd.get_dummies(df_new.timestamp.dt.weekday_name) $                 .set_index(df_new.index).reindex(columns=names, fill_value=0)) $ df3.head()
sub_gene_logical_vector = df.source.isin(['ensembl', 'havana', 'ensembl_havana']) $ sub_gene_df = df[sub_gene_logical_vector] $ sub_gene_df.shape
df_l_s=df.sample(n=2000)
ab_new['country'].value_counts() $
trunc_df[trunc_df.company_name == 'Airbnb']
session_summaries = [] $ for summary_loc in fr5_session_summary_locations: $     summary = FRStimSessionSummary.from_hdf(summary_loc) $     session_summaries.append(summary)
to_be_predicted_Day2 = 55.13755314 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
subreddit = reddit.subreddit('Bitcoin') $ for submission in reddit.subreddit('all').hot(limit=25): $     print(submission.title)
autos[["ad_created", $        "date_crawled", $        "registration_year", $        "registration_month", "last_seen"]].describe(include = "all")
X_extra.describe(include=['object'])
[(s.user.name, s.text) for s in retweets_of_me[:5]]
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=24000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
lq.isnull().sum()
areacodelocs = pd.read_csv('areacode_latitude_longitude.csv', index_col=None, encoding='latin_1') $ areacodelocs.areacode = list([s[:3] for s in areacodelocs.areacode]) $ areacodelocs = areacodelocs.drop(areacodelocs[areacodelocs.city.isnull()].index) $ areacodelocs = areacodelocs.drop(areacodelocs[areacodelocs.region.isnull()].index) $
df['new_column'] = df['column_1'] + df['column_2'] $ df['new_column'] = df.apply(my_function, axis = 1) $ df.to_csv['my_data.csv'] $
df_2018.shape
print(x_train.shape) $ print(x_test.shape) $ print(y_train.shape) $ print(y_test.shape)
d = datetime(2014,8,29) $ do = pd.DateOffset(days = 1) $ d + do
len(active_psc_controls[active_psc_controls.nature_of_control.str.contains('trust')].company_number.unique())
df2.head() # checking dataframe for ab_page column
tips.index
sorted(df['lambda'].unique())
prcp_data_df  = pd.DataFrame(prcp_data,columns=["Precipitation Date", "Precipitation"])
x = search.cv_results_["param_max_depth"].data $ score = search.cv_results_["mean_test_score"] $ yerr = search.cv_results_["std_test_score"] $ fig, ax = plt.subplots(figsize=(12,8)) $ ax.errorbar(x, score, yerr = yerr);
city_eco = city_pop.copy() $ city_eco["eco_code"] = [17, 17, 34, 20] $ city_eco
data.head()
label_index=np.repeat(np.array(labels), 3) $ community_index=np.repeat(np.array(communities), 3)
def trip_start_date(x): $     return re.search(r'(\d{4})-(\d{2})-(\d{2})', x).group(0)
finaldf = df.drop(['index', 'irlsp'], axis=1) $ finaldf.head()
loan_requests1.shape
elec.mains().good_sections()
print len(ind['AFFGEOID'].unique()) $ print len(noise_graf['AFFGEOID'].unique()) $ print len(noise_graf)
probab_converted = df2['converted'].mean() $ print('Probability of an individual converted:{}'.format(probab_converted))
df_final.info()
total.first_valid_index()
%matplotlib inline $ closingPrices.plot();
(details['Runtime'] == 0).value_counts()
plot_confusion_matrix(cm_lr, classes=['COLLECTION', 'PAIDOFF'],normalize=False, title="Confusion matrix for logistic regression", cmap=plt.cm.Blues)
from sklearn.metrics import roc_curve, auc $ probs = pipeline.predict_proba(ogXfinaltemptf) $ preds = probs[:,1] $ fpr, tpr, threshold = roc_curve(ogy, preds) $ roc_auc = auc(fpr, tpr)
people = odm2rest_request('affiliations', {'organizationCode': ','.join(orgs)})
xml_in_sample1['authorId'].nunique()
tweet_full_df.to_csv('twitter_archive_master.csv')
grid.fit(X_trainfinaltemp, y_train)
desc_order_active_stations = session.query(Measurement.station ,func.count(Measurement.station)).\ $                     group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $ desc_order_active_stations
new.shape
learn.freeze_to(-1)
data_adv = pd.get_dummies(data_adv,columns = ['player_team_name', 'player_team_opponent','map']) $ data_adv = data_adv.drop(['match_id'], axis = 1)
import statsmodels.api as sm
store_items = store_items.drop(['store 2', 'store 1'], axis = 0) $ store_items
temp = pd.read_csv('tweets_i_master.csv') $ tweets_clean = temp.copy() $ tweets_clean.info() $
yt.get_subscriptions(channel_id, key, descriptive=True)[:2]
new_model = gensim.models.Word2Vec.load(temp_path)  # open the model
!hdfs dfs -mkdir {HDFS_DIR}/3.2 $ !hdfs dfs -mkdir {HDFS_DIR}/3.2/input $ !hdfs dfs -mkdir {HDFS_DIR}/3.2/output
df.index.month.value_counts().sort_index()
cachedf = dir2df(cachedir, fnpat='\.ifc$', addcols=['barename']) $ cachedf
conn_b.commit()
score_l50 = score[(score["score"] < 50) & (score["score"] > 0)]  $ score_l50.shape[0]
msft.loc['2012-02':'2012-02-09'][:5]
print (Ralston.TMAX.std(), Ralston.TMAXc.std())
events[events['type'] == 'PublicEvent'].head()
%%bash $ cd /data/LNG/Hirotaka/ASYN $ awk '$6 < 5e-8 {print}' PPMI1_all > sign.txt
NoOfUniqueUser = df['user_id'].nunique() $ NoOfUniqueUser
df_unit.shape
flight.selectExpr("regexp_extract(duration,'([0-9]+(?=h))', 1) as duration_hour").duration_hour.cast("double").show()
m.fit(lr, 2, metrics=[exp_rmspe], cycle_len=4)
df_archive["pupper"].value_counts()
d_aux=[] $ for i in range(0,l2): $     if i not in seq: $         d_aux.append(diversity[i]) $ col.append(np.array(d_aux))
my_gempro.get_scratch_predictions(path_to_scratch='scratch', $                                   results_dir=my_gempro.data_dir, $                                   num_cores=4)
nt.set_index("Date", inplace=True)
data = df.drop_duplicates(subset='hash_id')
df_2017.dropna(inplace=True) $ df_2017
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df2['group']) $ df2.head()
g8_aggregates.columns = ['_'.join(col) for col in g8_aggregates.columns] $ g8_aggregates
train_small_data.to_feather("../../../data/talking/train_small_data.feather") $ val_small_data.to_feather("../../../data/talking/val_small_data.feather") $ test.to_feather("../../../data/talking/test_small_data.feather")
df4[['ab_page', 'country_UK', 'country_US','intercept', 'converted']].astype(int) $ df4.head()
html = browser.html $ soup = bs(html, 'html.parser')
train_df.head()
new_dems.shape[0] # this is how many rows we have...
print("Number of columns: %s" % (len(excel_data.columns)))
df.name
test.head()
unstacked = retention['userId'].unstack(1)
full_contingency = np.array(pd.crosstab(index=intervention_train['target'], columns=intervention_train['ORIGINE_INCIDENT']))
df = df[(df.yearOfRegistration >= 1990) & (df.yearOfRegistration < 2017)] $ df = df[(df.price >= 100) & (df.price <= 100000)] $ df = df[(df.powerPS < 600)]   $ print "Dimensiones de dataset acotado: ",df.shape
sess.get_data(['ibm us equity','aa us equity','vod ln equity'],['px last','px open'])
top20.to_csv("ratio.csv")
texts = [] $ r = csv.reader(open(r'C:\Users\User\Desktop\670\7_Topic_Modeling\data\text8.csv')) $ for i in r: $     texts.append(i)  $ len(texts)
df_con.to_csv('titles_scores', index=False)
stories = pd.concat([stories, tag_df], axis=1)
LSI_model = LSI.train(corpus[:-100])
train_pred = rf.predict(X_train)
file_path = "2017 CAM data from iPads.xlsx"
graf_counts2['precinct'] = graf_counts2['precinct'].astype(int)
cig_data.index
sorted.select('count').toPandas().hist(bins=15)
lst = data_after_subset_filter.SEA_AREA_NAME.unique() $ print('Waterbodies in subset:\n{}'.format('\n'.join(lst)))
autos["registration_year"].value_counts()
y = yc_new4['Fare_Amt'] $ x = yc_new4['Trip_Distance'] $ print(y.mean()) $ print(x.mean()) $ sns.regplot(x, y, data=yc_new4)
test_words.shape
logit_mod_new = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','US', 'UK']]) $ results_new = logit_mod_new.fit() $ results_new.summary()
firstWeekUserMerged[firstWeekUserMerged.objecttype.isnull()].head(5)
to_be_predicted_Day4 = 83.28399652 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
arr_size = reflClean.shape $ arr_size
print('There are {} missing values.'.format(df.isnull().any().sum()))
filter_df['race'].unique()
dfRegMet.info()
page = requests.get('https://api.opencorporates.com/v0.4/companies/search?q=barclays+bank')
data.loc[1]
pd.crosstab(index=mydf.comp, columns=mydf.dept)
df2 = df.drop(df.columns[non_null_counts < 1000], axis=1) #by default df.drop will take out rows, axis = 0
twitter_archive_master.groupby('has_stage')['rating'].mean()
df_only_headline[df_only_headline["tags"].apply(lambda x: "rstats" in set(x))].groupby(["headline", "date"]).size()
poldnull = df2.query('converted == 1').shape[0] / df2.shape[0] $ poldnull
dd2=cfs.diff_abundance('Subject','Control','Patient', alpha=0.25)
fast_scatter_xs = fuel_xs.get_values(filters=[openmc.EnergyFilter], $                                      filter_bins=[((0.625, 20.0e6),)], $                                      scores=['(scatter / flux)']) $ print(fast_scatter_xs)
ds_cnsm = xr.open_mfdataset(data_url2) $ ds_cnsm = ds_cnsm.swap_dims({'obs': 'time'}) $ ds_cnsm = ds_cnsm.chunk({'time': 100}) $ ds_cnsm = ds_cnsm.sortby('time') # data from different deployments can overlap so we want to sort all data by time stamp. $ ds_cnsm
df = pd.DataFrame(results) $ df = df.rename(columns={0: 'text'})
evaluation_1 = api.create_evaluation(ensemble_1, test_dataset) $ api.ok(evaluation_1)
order_data = pd.read_excel('order data.xlsx')
gs_from_model.score(X_test, y_test_over)
df = df[df.Address.notnull()]
print(df2['landing_page'].value_counts())
re_split_raw = re.findall(r'\w+', raw) $ print(re_split_raw[100:150])
price2017['DateTime'] = pd.to_datetime(price2017['Date'] + ' ' + price2017['Time'])
print("Lenght of time:\t\t\t%s" % len(d.variables['time'])) $ print("Length of SATCTD7229_PRES:\t%s" % len(d.variables['SATCTD7229_PRES']))
df['created_at']=pd.to_datetime(df['created_at'],format='%Y-%m-%d %H:%M:%S')
autos["unrepaired_damage"].unique()
pbptweets = pbptweets.drop_duplicates(subset='text', keep='first')
np.sin(1)
engine.execute("SELECT count(station), station FROM measurement GROUP BY station ORDER BY count(station) DESC").fetchall()
circle_companies = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*1..]-(c1:Company)\ $ RETURN DISTINCT (c1.company_number)").data()) $ active_companies[active_companies.CompanyNumber.isin(circle_companies['(c1.company_number)'])].to_csv('data/for_further_investigation/circular_ownership.csv') $ len(circle_companies)
df = df.reset_index() $ df = df.rename(columns={'Time': 'ds', 'Count': 'y'}) $ df.head()
m2=pd.merge(horror_readings,visits, left_on='visit_id',right_on='visitor_id') $
%%sql $ UPDATE facts $ SET Last_Update_Date_key = hour.hour_key $ FROM hour $ WHERE hour.hour  = TO_CHAR(facts.Last_Update_Date, 'YYYY-MM-DD HH24:00:00')
treatment_df=df.query('group=="treatment"') $ uut_old=treatment_df.query('landing_page=="old_page"').user_id.nunique() $ uut_new=treatment_df.query('landing_page=="new_page"').user_id.nunique() $ print("Number of unique users in the treatment group landed in old page is :{}".format(uut_old)) $ print("Number of unique users in the treatment group landed in new page is :{}".format(uut_new))
ol.data
grouped_authors_by_publication.head()
options_data.info()
fashion.groupby(fashion.index).size()
df = pd.read_csv('twitter_archive_master.csv')
log_mod_countries_2 = sm.Logit(df_new['converted'],df_new[['intercept','US','UK','ab_page']]) $ results_countries_2 = log_mod_countries_2.fit() $ results_countries_2.summary()
answer1_quandl = quandl.get('FSE/AFX_X', start_date='2017-01-01', end_date='2017-12-31') $ answer1_quandl.head()
list(tweets_total[0].keys())
res['cost'] = res['quantity'] * res['price'] $ res_agg = res.groupby('id').agg({'description': ['nunique'], 'quantity': ['sum'], 'cost': ['mean', 'sum']}) $ res_agg.columns = ['unique_items', 'total_quantity', 'mean_cost', 'total_cost'] $ res_agg.reset_index(inplace=True) $
twitter_archive[twitter_archive['rating_denominator'].astype(int)< 10]
events_df['event_day'] = events_df['event_time'].apply(lambda d: d.replace(hour=0,minute=0,second=0)) $ events_df['event_week'] = events_df['event_day'].apply(lambda d: d - datetime.timedelta(d.weekday())) $ events_df['event_weekday'] = events_df['event_day'].apply(lambda d: d.weekday())
df.loc[:,"message"] = df['message'].str.findall('\w{3,}').str.join(' ') $ df.head()
mr.at_time('16:00')
frame2.ix['three']
print(pd.isnull(titanic).sum()) $
props.prop_name.value_counts() $ propnames.value_counts()
by_weekday = data.groupby(data.index.dayofweek).mean() $ by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun'] $ by_weekday.plot(style=[':', '--', '-']);
segmentData.opportunity_stage.value_counts()
airbnb_df['host_is_superhost'].fillna('f', inplace=True)
plate_appearances = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).first().reset_index()
def prob_loss(ann_ret): $     ann_ret = ann_ret.dropna() $     mask = (ann_ret < 0.0) $     prob = np.sum(mask) / len(mask) $     return prob
df_vow.head()
indeed.shape
dtmodel.fit(XX_train, yy_train) $ dtmodel_predictionsX = logmodel.predict(XX_test) $ num_of_dtmodel_predX = collections.Counter(dtmodel_predictionsX) $ num_of_dtmodel_predX
th = table.find('th', text='Indebtedness:') $
data.complex_features
unique = len(df) - sum(df["user_id"].duplicated()) $ print(unique)
hashtag_list = ['Model 3', 'Model S', 'Electric Cars', 'Power_wall', 'Solar_roof', 'Solar_space_panel'] $ list_length = hashtag_list.size[1] $ for i in range(list_length): $     DF_list[i].to_csv(hashtag_list[i])
df2 = df.query('landing_page == "new_page" & group == "treatment" | landing_page == "old_page" & group == "control"')
data.value[[3,4,6]] = [14, 21, 5] $ data
autos['registration_year'].describe()
van_final['FirstMeta'] = van_final.groupby('userid')['pagetitle'].first().str.contains('/')
df.head()
df_test_index = df_test_index.drop(['event_start_at','time_stamp', 'num_of_people', 'payment_method', 'total_price'], axis=1) $ df_test_index = df_test_index.iloc[:,[1, 0, 2]] $ df_test_index['user_id'] = df_test_user['user_id'] $ df_test_index = df_test_index.fillna(0) $ df_test_index                                   
pres_df.drop('ad_length_tmp', inplace=True, axis=1) $ pres_df.head()
neg_tweets.shape
stop_words = set(stopwords.words('english')) $
users.loc[:,'creation_date'] = users.created_at.dt.date $ tmp = users.groupby(['creation_date']).size().reset_index() $ tmp.rename(columns={0:'users'},inplace=True) $ ax=sns.scatterplot(data=tmp, x='creation_date', y='users')
p = p.to_crs({'init':'epsg:4269'})
df_kws.plot()
token_sendcnt["ID"] = token_sendcnt.sender $ token_receivecnt["ID"] = token_receivecnt.receiver
NYPD_df['Complaint Type'].value_counts().head()
from scipy.stats import norm $ z_sig = norm.cdf(z_score) $ crit_val = norm.ppf(1-(0.05/2)) $ print(z_sig, crit_val)
from sqlalchemy.ext.declarative import declarative_base $ from sqlalchemy import Column, Integer, String, Float 
autos.isnull().sum()/len(autos)*100
display(observations_ext_node['number'][DATA].dropna().head(9))
len(df2.loc[(df2['group'] == 'treatment') & (df2['converted'] == 1)]['user_id'])/len(df2.loc[df2['group'] == 'treatment']['user_id'])
pt_after=pd.DataFrame.pivot_table(df_users_6_after,index=['cohort'],values=['uid'],aggfunc='count',fill_value=0)
shown = pd.DataFrame(data.tasker_id.value_counts()) $ shown.loc[shown['tasker_id']==1]
! ls  | grep --fixed-strings --file samples_with_signatures.txt -v | grep log | xargs head -n 20 | head -n 50
test_join.filter('p_repo_id is not null').count()/test_forks.count()  
df.info()
timedog_df = tdog_df[tdog_df.userTimezone.notnull()] $ timedog_df = timedog_df[['Hashtag', 'tweetCreated', 'tweetFavoriteCt', 'tweetID', 'tweetRetweetCt', 'tweetSource', $                          'userID', 'userLocation', 'userName', 'userScreen', 'userTimezone']] $ timedog_df.head() $ timedog_df.size
new_page_converted = np.random.choice(np.arange(2), size=n_new, p=[(1-p_new), p_new]) $
archive_copy = archive_copy.drop(['text', 'source', 'in_reply_to_status_id', 'in_reply_to_user_id'], axis=1)
obs_diff = df2.query('group =="treatment"').converted.mean() - df2.query('group =="control"').converted.mean() $ obs_diff $ p_value =(p_diffs > obs_diff).mean() $ p_value
well_data.fillna(value=0, inplace=True)
learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)
theta_0 = 0.1* np.random.randn(X_train_1.shape[1]) $ theta = gradient_descent(X_train_1, y_train, theta_0, 0.1, 100)
p_new = len(df2.query("converted=='1'"))/ len(df2) $ p_new
prob_convert = (df2['converted'] == 1).sum()/unique_users_2 $ prob_convert
twitter_archive_master.to_csv('twitter_archive_master.csv', index=False)
jobPostDF['date'] = jobPostDF.date.astype(datetime)
df_joy.shape
fat.add_bollinger_bands(vol, 'Volume', inplace=True) $ vol = vol.dropna() $ vol.head()
print(df['State'].value_counts(dropna=False))
brand_mileage = {} $ for brand in brands: $     mean_mileage = autos['odometer_km'][autos['brand']==brand].mean() $     brand_mileage[brand] = int(mean_mileage) $ brand_mileage
df['polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity) $ df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
plot = seaborn.barplot(x='group', y='sentiment', data=name_sentiments, capsize=.1)
for i, x in data.iterrows(): $     if compute_score(x[3]) > 0.05: $         print x[0]
from rl.callbacks import TrainEpisodeLogger $ class TrainEpisodeLoggerPortfolio(TrainEpisodeLogger): $
tweetsDf = tweetsDf.fillna('nulo') $ tweetsDf.country.hist()
daily_station_df = (all_turnstiles.groupby(['STATION','DATE'], as_index=False) $                     .sum() $                     .drop(['ENTRIES','EXITS'], axis=1)) $ daily_station_df.sample(5)
tt_json = tt_json_df[['id', 'favorite_count', 'is_quote_status', 'retweet_count']] $ tt_json.head()
dates = [] $ for ann_date in announcement_dates.index: $     dates.extend(neighbor_dates(ann_date)) $ dates = pd.Series(dates)
sales_agg['Margin'] = sales_agg['State Bottle Retail']-sales_agg['State Bottle Cost'] $ sales_agg.head(1) #create margin feature
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.choice([0, 1], size = n_new, p = [p_new, 1 - p_new]).mean() $     old_page_converted = np.random.choice([0, 1], size = n_old, p = [p_old, 1 - p_old]).mean() $     p_diffs.append(new_page_converted - old_page_converted)
more_grades = final_grades_clean.stack().reset_index() $ more_grades
df3[['CA','UK','US']] = pd.get_dummies(df3['country']) $ logit_model = sm.Logit(df3['converted'], df3[['intercept','ab_page','CA','UK']]) $ result = logit_model.fit() $ result.summary()
properati[properati['zone'] == "Bs.As. G.B.A. Zona Norte"]
flight.printSchema() $ flightv1_1.printSchema()
consumerKey = 'XXXXXXXXXXXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
df.index $
pyLDAvis.enable_notebook()
p_diffs=np.array(p_diffs) $ null_vals=np.random.normal(0, p_diffs.std(), p_diffs.size)
ip.head()
df['week'] = df['datetime'].apply(lambda x:x.week) $ df['year'] = df['datetime'].apply(lambda x:x.year) $ df['date'] = df['datetime'].dt.date $
%matplotlib inline $ import matplotlib.pyplot as plt $ from matplotlib import style $ style.use('ggplot')
df = pandas.read_csv('data/201508_trip_data.csv.gz')
from sklearn.neighbors import KNeighborsClassifier
tw.apply(lambda x: x['airline'] + '-' + x['airline_sentiment'], axis=1).value_counts()
spearmanr(merge_['tweet.created_at_x'], $           merge_['tweet.created_at_y'])
basic_smapes = [vector_smape(basic_pred[col], real[col]) for col in basic_pred.columns] $ complex_smapes = [vector_smape(complex_pred[col], real[col]) for col in complex_pred.columns]
df.reorder_levels(order=['Date', 'Store', 'Category', 'Subcategory', 'UPC EAN', 'Description']).head(3)
df.loc['Mon']
df_1 = pd.read_csv('twitter-archive-enhanced.csv') $ df_1 $ IDlist = df_1['tweet_id'].tolist() $ IDlist
a['c'] = a.apply(lambda x : x['a']+x['b'], axis=1)
df_img_algo_clean = df_img_algo.copy() $ df_img_algo_clean.head()
df_drug_counts.dropna(axis = 1, thresh = 20).plot(kind = 'bar', $                                                  figsize = (10,6))
mb.head()
! head -1 ./data/raw-news_tweets-original/dataset1/news/2014-11-18.txt
y_pred = lr_pred $ print('precision: {:.2f}\nrecall: {:.2f}\naccuracy: {:.2f}'.format(precision_score(y_test,y_pred), $                                                        recall_score(y_test,y_pred), $                                                        accuracy(y_test,y_pred)))
x_train.shape
old_page_converted  = np.random.choice([1, 0], size=n_old, p=[convert_p_old, (1-convert_p_old)])
d.year
df_group2=df.query("landing_page=='new_page'and group=='treatment'")
out_df = pd.DataFrame(pred) $ out_df.columns = ["low", "medium", "high"] $ out_df["listing_id"] = test_df.listing_id.values
shopping_carts = pd.DataFrame(items) $ shopping_carts
y_pred = svm_clf.predict(X_train_scaled) $ accuracy_score(y_train, y_pred)
autos=autos.drop('offerType',1)
import pandas as pd $ distance_from_sun = [149.6, 1433.5, 227.9, 108.2, 778.6] $ planets = ['Earth','Saturn', 'Mars','Venus', 'Jupiter'] $ dist_planets = pd.Series(index=planets, data=distance_from_sun) $ print(dist_planets)
model.wv['tree']  # raw NumPy vector of a word
idx = index.Index() $ for i, poly in enumerate(census_tracts_df['shapes']): $     idx.insert(i, poly.bounds)
engine.execute('Select * from measurements LIMIT 5').fetchall()
df['source'].value_counts()
def parse_date (str_date): $     return dateutil.parser.parse(str_date) $ df['Created date']= df['Created Date'].apply(parse_date)
df.set_index("ZIPCODE", inplace=True)
df_predict = pd.DataFrame({'text' : ["I have fever. \ $                                      My doctor prescribed ibuprofen."]}) $ predictions = loaded_keras_entity_recognizer.predict(df_predict) $ predictions[['text', 'prediction', 'confidences']]
Image(filename='healthy_breakfast.jpg')
df_2018.shape
mask = (youthUser4["creationDate"] > '2017-11-01') & (youthUser4["creationDate"]<= '2017-11-30') $ youthUserNov2017 = (youthUser4.loc[mask]) $ youthUserNov2017.head()
s4.unique()
for row in session.query(Measurements).limit(5).all(): $     print(row)
len(df[michaelkorsseries].userid.unique())
print(df.shape) $ print("The number of rows in the dataset is: {}".format(df.shape[0]))
p(locale.getdefaultlocale)
res.status_code   # Checking the HTTPS Response
len(df2.query('landing_page == "new_page"')) / len(df2['landing_page'])
lr = LogisticRegression(random_state = 42) $ param_grid = {'penalty': ['l1', 'l2'], $               'C':np.logspace(0, 2, 10)} $ lr_gd = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, scoring='f1', n_jobs = -1) $ lr_gd.fit(X_train, y_train)
pd.Series([1, True, 's'])
number_of_values_where_NoData = np.count_nonzero(reflClean == metadata['data_ignore_value']) $ number_of_values_where_NoData
plt.hist(taxiData.Trip_distance, bins = 60, range = [0, 3]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
vocab = vectorizer.get_feature_names() $ print(vocab)
df_goog.sort_values('Date', inplace=True)    # This is a good idea to sort our values so the indexes ultimately line up $ df_goog.set_index('Date', inplace=True)      # also df_goog.index = df_goog['Date'] works well here $ df_goog.index = df_goog.index.to_datetime()  # Convert to datetime
fact_url = "http://space-facts.com/mars/"
ser = pd.Series(['Tues','Wed'], index=days) $ ser
sensors_num_df.data.head()
df.reset_index(inplace=True)
grouped_by_date_df = full_df.groupby('ymd')['listing_id'].count().reset_index().copy() $ grouped_by_date_df.columns = ['date','count_of_listings'] $ grouped_by_date_df.head(3)
forecast = m.predict(future) $ forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()
f_os_hour_clicks.show(1)
autos['odometer_km'].describe()
df_a.join(df_b, how = "right") # right join (see above for definition)
entry =[] $ Keys = Dict['column_names'] $ for date in data: $     entry.append(dict(zip(Keys,date))) $ print(entry[0:5])
twitter_df_clean.describe()
df3 = pd.merge(df1, df2) $ df3
plus_minus_fxn = lambda x: x.rolling(20).sum() $ features['f17'] = features['f16'].groupby(level='symbol').apply(plus_minus_fxn)
grouped_publications_by_author.tail(10)
s_t = np.sqrt(((n2-1)*n2*sd2+(n3-1)*n3*sd3)/(n2+n3-2)) $ t = (m3-m2)/(s_t*np.sqrt(1/n2+1/n3)) $ tscore = stats.t.ppf(.95,n2+n3-2) $ print("t stats is {0}; 95% t score is {1}".format(t,tscore))
dates = [datetime(2014,8,1),datetime(2014,8,2)] $ ts = pd.Series(np.random.randn(2),dates) $ ts
secclintondf = secclintondf[secclintondf.datetime>'2016-07-25 00:00:00']
for x in tweets_clean.dog_class.unique(): $     print('Mean favorites for ' + str(x) + ' class:' + str(tweets_clean[tweets_clean.dog_class == x].favorites_count.mean()))
median = df['time_open'].quantile(q=0.5) $ print(median)
df.isnull().sum()
df.head()
classify_df = classify_df.drop(['Sex upon Intake','Breed','Color','Outcome Type','OutcomeAge','DaysInShelter','Sex upon Outcome'],axis=1) $ classify_df = classify_df.drop(['tan','gray','lilac','cream'],axis=1) $ classify_df.columns
day_counts = (daily_hashtag.groupBy('day', 'hashtag', 'week') $                            .count() $                            .sort('count', ascending=False) $              ).cache()
tuna_neg_cnt = tuna_neg.count()*100 $ print "{:,} users have no activity after {} ({:.2%} of DAU)"\ $       .format(tuna_neg_cnt, D0.isoformat(), tuna_neg_cnt*1./dau)
countries_df.nunique()
v=sns.factorplot(data=tweets_df, x="name", y="favorite_count", kind="bar") $ plt.xticks(rotation=60) $
html_table = df.to_html() $ df.to_html('table.html') $ html_table
items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']), $          'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])}
xgb.plot_importance(model_outer)
bnbAx[bnbAx['language']!='en'].country_destination.value_counts().plot.bar()
tweetsDf.hist(column='retweet_count',bins=15)
col_short['str_date']=col_short['date_time'].astype(str) $ df_weather['str_date']=df_weather['date_time'].astype(str)
idx = data['dataset_data']['column_names'].index('Traded Volume') $ volume = [day[idx] for day in data['dataset_data']['data']] $ average_volume = sum(volume)/len(volume) $ print('The average daily trading volumne in 2017 was {:.2f}'.format(average_volume))
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
df["stamp"] = to_datetime(df["created_at"],format='%a %b %d %H:%M:%S +0000 %Y') $ df.head()
run txt2pdf.py -o"2018-06-19 2011 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2011 FLORIDA HOSPITAL Sorted by payments.txt"
plt.figure(figsize=(15,5)) $ plt.title("Number of mentions in time") $ mentions_df["epoch"].hist(bins=500) $ plt.show()
Log_model2 = sm.Logit(df_new['converted'],df_new[['intercept','CA','UK','treatment']]) $ Log_model2.fit().summary()
from sklearn import metrics $ print "MAE: ", metrics.mean_absolute_error(y_test,predictions) $ print "RMSE: ", np.sqrt(metrics.mean_squared_error(y_test,predictions)) $ print "R2: ", metrics.r2_score(y_test,predictions)
autos['registration_year'].describe()
df=pd.DataFrame({"Year":[2017,2018],"Month":[1,12],"Day":[26,25]}) $ print(df)
master_file['FILENAME'].value_counts()
df2 = tier1_df.reset_index() $ df2 = df2.rename(columns={'Date':'ds', 'Incidents':'y'}) $ df_orig = df2['y'].to_frame() $ df_orig.index = df2['ds'] $ n = np.int(df_orig.count())
rf_v1.varimp(use_pandas=True)
aa2 = np.array(eval(aa2)) $ aa2
df = titanic3[['cabin', 'pclass']].dropna() $ df['deck'] = df.apply(lambda row: ord(row.cabin[0]) -64, axis=1) $ sns.regplot(x=df["pclass"], y=df["deck"])
dfRegMet2014.shape
df_gnis_test = df_gnis.dropna(axis=0, subset=['PRIM_LAT_DEC','PRIM_LONG_DEC'],thresh=1) $ df_gnis_test.shape
from IPython.display import IFrame $ IFrame("http://www.net-analysis.com", width = 800, height = 200)
outlier_detection.outlier_detection_1d(cutoff_params=outlier_detection.basic_cutoff).head(20)
log_mod_2 = sm.Logit(df_new['converted'], df_new[['CA', 'UK', 'intercept']]) $ result_2 = log_mod_2.fit() $ result_2.summary()
1/np.exp(-0.0123) $
p_diffs = np.array(p_diffs) $ p_diff_proportion = (p_diff_orig < p_diffs).mean() $ print('proportion of p_diffs greater than p_diffs from ab_data.csv :: ',p_diff_proportion)
pd.crosstab(df['k'], df['topic'])
autos = autos[autos["registration_year"].between(1900, 2016)]
festivals.head(5)
df2['intercept'] = 1 $ df2_dummy = pd.get_dummies(df['landing_page']) $ df2['ab_page'] = df2_dummy['new_page'] $ df2.head()
data = data.reindex(range(data.shape[0]))
details.head(10)
tweets3=pd.read_json(r'C:\Users\shampy\Desktop\project\RedCarpetUp\socialmediadata-tweets-of-congress-november\2017-11-03.json') $ tweets3.shape
news.tail()
morning_rush.iloc[:1000][['latitude', 'longitude']].get_values()
data.columns
joined['Monetary_score'].describe()
newstudents = pd.DataFrame([(150, 62, 'M'), (170, 65, 'F')], columns = ['weight', 'height', 'gender'], index = ['Matt', 'Jen']) $ print(newstudents)
sp = pd.read_csv('SPY.csv')
df_archive_clean["retweeted_status_timestamp"].unique()[0:10]
dftop2.head()
questions = pd.concat([questions.drop('attend_with', axis=1), attend_with], axis=1)
import statsmodels.api as sm $ convert_old = df2[df2["landing_page"] == 'old_page']['converted'].sum() $ convert_new = df2[df2["landing_page"] == 'new_page']['converted'].sum() $ n_old = df2[df2["landing_page"] == 'old_page'].shape[0] $ n_new = df2[df2["landing_page"] == 'new_page'].shape[0]
%timeit ' '.join([stemmer(word) for word in regex.sub('' ,'The vintage 1930 beaded earrings are still calling to me.').split()])
np.random.seed(123456) $ ps = pd.Series(np.random.randn(12), mp2013) $ ps
consumerKey= 'XXXXXXXXXXXXXXX' $ consumerSecret='XXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, $                            consumer_secret=consumerSecret) $ api=tweepy.API(auth)
releases['jurisdiction_cd'].value_counts()
df['target']=df.num_comments.map(lambda x: 1 if x>=med else 0)
dftop['temp'].mean()
planets.groupby('method')['year'].describe().unstack()
df.if_fielding_alignment.value_counts()
print metrics.confusion_matrix(y_test, predicted) $ print metrics.classification_report(y_test, predicted)
table3= table3.drop(['device_id','source','browser','sex','signup_time','purchase_time'], axis=1)
image_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars' $ image_browser.visit(image_url) $ html = image_browser.html
ip_clean['p1'].str.replace('_', ' ').str.capitalize()
empPands = empDf.toPandas() $ print (empPands) $ print ("\n") $ for index, row in empPands.iterrows(): $     print(row["salary"]) $
IQuOD_file = '/OSM/CBR/OA_DCFP/data/observations/IQuOD/1988/iquod_ctd_1988.nc' $ ds_CTD_1988 = xr.open_dataset(IQuOD_file)
allPeople = read.getPeople() $ pd.DataFrame.from_records([vars(person) for person in allPeople]).head()
scaled.tail().T
df_mes.shape[0]
pbls = subs.problem_id.unique() $ print "%d resolved problems" % len(pbls)
corrected_log.head()
user_table_1.head(10)
top20_mostfav = top20_mostfav.sort_values(by='favorite_count', ascending=False).copy() $ top20_mosttweeted = top20_mosttweeted.sort_values(by='tweet_count', ascending=False).copy()
df2= df2.drop(index=1899, axis=0)
pd.options.display.max_columns = 55
logit.fit(X_mat, Y_mat) $ logit.score(X_mat, Y_mat)
np.array('2015-12-25', dtype=np.datetime64)  # We use an array just so Jupyter will show us the type details
logit_mod_joined = sm.Logit(df_joined_dummy.converted, \ $                            df_joined_dummy[['intercept', 'ab_page_new_page',\ $                                             'country_UK', 'country_US']])
df_users['churned'].value_counts().plot('bar')
station = pd.read_sql("SELECT * FROM station", conn) $ station.head()
old_page_converted = np.random.binomial(1, p_old, n_old) $ print('binomial', old_page_converted.mean()) $ old_page_converted = np.random.choice([1, 0], n_old, p=(p_old,1-p_old)) $ print('random choice', old_page_converted.mean())
tweets_gametitle.merge(winpct[['playId','text','Game Title Date']], $                        how='left', $                        left_on=['Game Title Date','text'], $                        right_on=['Game Title Date','text'])
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
twitter_archive.source.value_counts()
print "Total number of rows of table 'photoz_bcnz': ", len(df3) $ print df3.columns.values $ df3
tmax_day_2018.dims
a = test.toarray() #makes an array based on term frequency for the same test sample. $ a
df_resolved_links = pd.DataFrame(resolved_links) $ df_resolved_links.tail(3)
df_onc_no_metac.head()
dframe_team['cut_year'] = dframe_team['Start'].dt.year + 1 $ dframe_team['end_cut'] = dframe_team.cut_year.map(draftDates) $ dframe_team
final_data = pd.read_pickle('data/pickled/new_ratings_data.pkl') $ final_data.info() $ final_data.head()
sample = data.sample(frac=0.1, replace=False, random_state=42)
df = df.groupby('msno').apply(make_order_number)
algo = make_pipeline(preprocessing.MinMaxScaler(), svm.LinearSVC(class_weight='balanced')) $ scores = cross_val_score(algo, X, y, cv=5, scoring='f1') $ scores.mean() $ print(np.round(scores.mean(),3))
InfinityWars.to_csv('InfinityWars_Predictions_3.csv', encoding='utf-8')
k1 = data[['cust_id','total_spend']].groupby(['cust_id']).agg('count').reset_index() $ k1.columns = ['cust_id','cust_id_count'] $ train = train.merge(k1,on=['cust_id'],how='left') $ test = test.merge(k1,on=['cust_id'],how='left')
m_df = pd.merge(raw_full_df, pd.DataFrame(raw_train_y), left_index=True, right_index=True)
cand_date_df = pres_date_df.copy() $ cand_date_df.head()
f_counts_hour_ip.show(1)
ts_mean['count_norm'] = ts_mean['count'] / ts_mean['number'] * 10000 $ ts_mean['count_norm x f_norm'] = ts_mean['count_norm'] * ts_mean['f_sentiment'] $ ts_mean['count_norm x v_norm'] = ts_mean['count_norm'] * ts_mean['v_sentiment'] $ ts_mean.head()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new,convert_old],nobs=[n_new,n_old],alternative='larger') $ print(z_score,p_value)
from sklearn.model_selection import train_test_split
df2['ab_page'] = df2['treatment'] $ df2=df2.drop(['control', 'treatment'],axis=1) $ df2.head()
train_groupped.Visits['"Weird_Al"_Yankovic_en.wikipedia.org_all-access_all-agents'][0:5]
friends_n_followers['userFriendsCt'].plot(kind = 'bar') $ plt.title('Friends by Time Zone') $ plt.xlabel('Time Zone') $ plt.ylabel('Number of Friends') $ plt.show()
conversion_data_normalized = pd.DataFrame(preprocessing.scale(conversion_data.values)) $ conversion_data_normalized.columns = ['Display Ads', 'KAYAK Deals Email', 'Search Engine Ads','Search Engine Results', 'Travel Search Site'] $ print 'DataFrame conversion_data_normalized (FEATURE MATRIX - NORMALIZED)', conversion_data_normalized.shape, type(conversion_data_normalized) $ conversion_data_normalized.head()
yc_new1.rename(columns={'incomePC':'income_dest'}, inplace=True) $ yc_new1.columns
url= 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json' \ $      '?&start_date=2018-08-21&end_date=2018-08-21&api_key={}' \ $         .format(API_KEY) $ r = requests.get(url)
!rm tmp.json
traded_volumes = [] $ for ele in r.json()['dataset_data']['data']: $     traded_volumes.append(ele[6]) $ print('Average daily trading volume - {}'.format(sum(traded_volumes) / float(len(traded_volumes))))
conn.rollback()
if not np.any(cust['churn'] == 1): $     labels['churn'] = 0 $     labels['days_to_next_churn'] = np.nan
festivals = pd.read_csv("festivals_dataframe_saved.csv") $ festivals_clean = pd.read_csv("festivals_clean_dataframe_saved.csv") $ print(festivals.head(7)) $ print(festivals_clean.head(7))
intersections.info()
X_age_train, X_age_test, y_age_train, y_age_test = train_test_split(X_age, y_age, train_size=0.25)
PIT_analysis2 = PIT_analysis["Per Seat Price"].mean()
Z = np.random.random((10, 3)) $ zmin, zmax = Z.min(), Z.max() $ print((Z - zmin) / (zmax - Z)) $ print(Z.ptp)
df.head()
import statsmodels.api as sm $ log_regression =sm.Logit(df2['converted'], df2[['intercept','treatment']]) $ output = log_regression.fit()
df_pol_t = pd.concat([df_pol_matrix_df, df_pol_t.drop('title', axis=1)], axis=1) $
y_newpage = df2["user_id"].count() $ prob_newpage = x_newpage/y_newpage $ prob_newpage $
y_label_test_OneHot.shape
datetime.now()
train_embedding=train_embedding.rename({'DETAILS3':"WordVec"}, axis=1)
dtree = tree.DecisionTreeClassifier(max_depth=MAX_DEPTH) $ dtree = dtree.fit(X_train, y_train) $ print("Decision-tree predictions:\n") $ predicted_labels = dtree.predict(X_test) $ answer_labels = y_test
day_of_year14.to_excel(writer, index=True, sheet_name="2014")
colNames = colNames.str.replace("yearOfRegistration","registration_year") $ colNames = colNames.str.replace("monthOfRegistration","registration_month") $ colNames = colNames.str.replace("notRepairedDamage","unrepaired_damage") $ colNames = colNames.str.replace("dateCreated","ad_created") $ colNames = colNames.str.lower()
df.loc[df['field1']>27,['created_at','field1']]
df.head()
plt.hist(df15['bottles_sold'],bins=20,range=(0,100)) $ plt.show()
twitter_archive_master.info()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)                    # autheticate Twitter $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
fullDf.level.value_counts() $
austin.shape $
months = {datetime(2000,i,1).strftime("%b"): i for i in range(1, 13)} $ df['Month ID'] = df['Month'].map(months)
iso_join.head()
ab.shape[0]
from scipy.stats import norm $ norm.ppf(1-(0.05/2)) $
df.truncate(before='2014-MAY-2', after='2014-MAY-3')
data.to_csv('myfile.csv')  #To_excel exists as well.
import statsmodels.api as sm $ convert_old = df2[(df2['converted']==1)  & (df2['group']=='control')].shape[0] $ convert_new = df2[(df2['converted']==1) & (df2['group']=='treatment')].shape[0] $ n_old = df2[df2['group']=="control"].shape[0] $ n_new = df2[df2['group']=="treatment"].shape[0]
len(chefdf.name)
df['youtube_search_url'] = df.apply(lambda x: urllib.parse.quote_plus(base_search_url + x['title'] + x['artist'], safe='/:?='), axis=1)
print(data.describe())
print("There are", twitter_data.doggo.value_counts()[1], "doggos") $ print("There are",twitter_data.doggo.value_counts()[1], "floofers") $ print("There are", twitter_data.pupper.value_counts()[1], "puppers") $ print("There are", twitter_data.puppo.value_counts()[1], "puppos") $ twitter_data.doggo.value_counts()[1]+twitter_data.doggo.value_counts()[1]+twitter_data.pupper.value_counts()[1]+twitter_data.puppo.value_counts()[1]
rf.score(X_train, y_train)
state_party_df = pd.concat(state_DataFrames_list, axis=1, join='outer') $ state_party_df.columns = state_keys_list $ state_party_df.head(20)
rvs1 = stats.norm.rvs(loc=92576890.092929989,scale=0.1,size=500) $ rvs2 = stats.norm.rvs(loc=242523522.74525771,scale=0.2,size=500) $ stats.ttest_ind(rvs1,rvs2)
op_ = model.predict([q1_data1,q2_data2])
intervention_test.reset_index(inplace=True) $ intervention_test.set_index(['INSTANCE_ID', 'CRE_DATE_GZL'], inplace=True)
df_spend = df_tx_claims.groupby(['Hospital', 'Claim Type']).agg({'Avg Spending Hospital': [np.sum]})
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page') )==False].shape[0]
y_pred = m.predict(X_valid) $ cnf_matrix = metrics.confusion_matrix(y_valid, y_pred) $ cnf_matrix
df_students.head()
support.amount.sum()
last_date_of_ppt = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ last_date_of_ppt $
scores.describe()
with open('dropbox/github/Thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_continent-90m.csv', 'rU') as inputFile: $     header = next(inputFile) $     for line in inputFile: $         line = line.rstrip().split(',') $         print(line)
temp1 = grp1.createVariable('temp',np.float64,('time','lat','lon'),zlib=True) $ temp2 = grp2.createVariable('temp',np.float64,('time','lat','lon'),zlib=True) $ for grp in ncfile.groups.items():  # shows that each group now contains 1 variable $     print(grp)
df.index.strftime("%A")
df = df.join(d, how='outer')
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY), auth=('user', 'pass'))
df.plot(subplots=True) $ plt.show()
Jarvis_resistance_simulation_1 = Jarvis_ET_Combine['Jarvis(Root Exp = 1.0)'] $ Jarvis_resistance_simulation_0_5 = Jarvis_ET_Combine['Jarvis(Root Exp = 0.5)'] $ Jarvis_resistance_simulation_0_25 = Jarvis_ET_Combine['Jarvis(Root Exp = 0.25)']
datatest.info()
df2.converted.value_counts(normalize=True)[1]
P_treatment = 17264/145310 $ print ("Probability that individual was in the treatment group,and they converted: %0.4f" % P_treatment)
lda_tf.show_topic(5)
p_new=new_page_converted.mean() $ p_old=old_page_converted.mean() $ p_new-p_old $
m.end_time
segmentData.head()
sns.lmplot(x="apparentTemperatureHigh", y="polarity", data=twitter_moves,lowess=True,size=8,aspect=1.5)
logit_country = sm.Logit(df_latest['converted'],df_latest[['country_UK', 'country_US','intercept']]) $ results= logit_country.fit()
df.shape
for df in (joined,joined_test): $     df["Promo2Since"] = pd.to_datetime(df.apply(lambda x: Week( $         x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1).astype(pd.datetime)) $     df["Promo2Days"] = df.Date.subtract(df["Promo2Since"]).dt.days
ForPlot = TotalNameEvents.sort_values(by='created_time', ascending=False).head(200)
df_call = pd.read_csv('call.csv', index_col=None)
from sklearn.naive_bayes import MultinomialNB $ nb = MultinomialNB()
logistic_mod_page = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results_page = logistic_mod_page.fit()
df.sort_values("stamp",inplace=True) $ df.head()
df_tweet.head()
df3 = df.ix[0:3, ['X', 'Z']] $ df3.columns = ['P', 'Q'] $ df4 = df.ix[4:6, ['W']] $ df4.columns = ['R'] $ print df3, "\n\n", df4
engine.execute('SELECT * FROM Station').fetchall()
not_lining_up=df.query('group=="treatment" and landing_page != "new_page" or group=="control" and landing_page=="new_page"').count() $ not_lining_up $
plt.bar(icecream.Flavor, icecream.Price) $ plt.ylabel('Price') $ plt.xlabel('Flavor') $ plt.show()
import pandas as pd $ url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat' $ df = pd.read_csv(url,header=None,sep=' ') $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ df.head()
news_tweets_pd.to_csv("news_tweets.csv", index=False)
start_date = datetime.datetime(year=2017,month=1,day=1) $ stop_date = datetime.datetime(year=2017,month=8,day=23)
!wget 'https://mas-dse-open.s3.amazonaws.com/Moby-Dick.txt' -P ../../Data/
rent_db.boxplot(column='price', by='bedrooms')
len(df_groups['group_id'].unique())
loadRetailData = sc.textFile("OnlineRetail.csv.gz") $ for row in loadRetailData.take(5): $     print row
apostrophe_match = '[a-z]+n\'t' $ conjunctions = re.findall(apostrophe_match, text) $ tokens = [(word[:-3], word[-3:]) for word in conjunctions] $ print(tokens)
full_globe_temp[full_globe_temp == -999.000] = np.nan $ full_globe_temp.tail()
data2=data.iloc[1:4] $ data2
sex_dummy = pd.get_dummies(fraud_data_updated['sex']) $ fraud_data_updated = pd.concat([fraud_data_updated,sex_dummy],axis=1)
df_sub_headline = df_tweets[(df_tweets["sub-headline"].str.len() > 0)] $ sub_headline_date_count = df_sub_headline.groupby(["headline", "sub-headline", "date"]).size() $ sub_headline_date_count[sub_headline_date_count > 1].index.get_level_values(1)
new_dems.newDate[new_dems.Sanders.isnull()] $
nold = len(df2.query('group =="control"')) $ nold
sub_gene_df.sample(10)
facts_df = pd.read_html(str(table))
autos.drop(['seller','offer_type','num_photos'], axis=1)
fpr_a = (grid_pr_fires.sort_values(['glat', 'glon'], ascending=[False,True])['pr_fire'] $          .values.reshape(26,59))
pd.concat([df, delivery_dummy], axis=1)
p_new=df2[df2['converted']==1].shape[0]/df2.shape[0] $ p_new
collect.get_iterator()
codes = access_logs_df.groupBy('responseCode').count()
to_be_predicted_Day3 = 48.60496872 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
twitter_archive_clean['timestamp'] = pd.to_datetime(twitter_archive_clean['timestamp'])
DummyDataframe = DummyDataframe.set_index("Date").sort_index() $ DummyDataframe = DummyDataframe.groupby("Date").sum()
sq = Square((0, 0), 10) $ print(f"Square size: {sq.x, sq.y}") $ print(f"Square corner: {sq.corner}")
pd.options.display.float_format = '{:20,.4f}'.format
df.head()
df[df['Complaint Type'] == 'Noise - Residential']['Descriptor'].value_counts().head()
from pyspark.ml.evaluation import MulticlassClassificationEvaluator $ evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction") $ accuracy = evaluator.evaluate(predictions) $ print "Model Accuracy: ", accuracy
df.sample(20)
overallFireplaces = pd.get_dummies(dfFull.Fireplaces)
def page_to_df(page): $     table = page.extract_table() $     lines = table[1:] $     return pd.DataFrame(lines, columns=cols)
data.info()
Y_train.iloc[0:5]
twitter_archive_master.tweet_id=twitter_archive_master.tweet_id.astype('str')
df_selection = df_selection.dropna(how='any') 
test.info()
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23HarryPotter').items(100): $     results.append(tweet) $ len(results)
lin_pred = lin.predict(x_test) $ lin_pred[:5]
print(today.strftime('%Y-%m-%d'))
x.drop(range(2))
X_train, X_test, y_train, y_test = train_test_split(X_svd, $                                                     y_encode, $                                                     test_size=0.2, $                                                     random_state=42) $
co.steady_states
df = df[pd.notnull(df['jpg_url'])]
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
tmdb_movies['day_of_week'] = tmdb_movies['release_date'].dt.weekday_name
counts = df2['landing_page'].value_counts() $ counts
final_names=[x.replace(primary_temp_column, 'Temp') for x in out_columns] $ final_names=[x.replace("Incremental", "_Precip") for x in final_names] $ save_dat.columns=final_names $ save_dat.index.name='Date'
df_wm.to_csv("walmart_senti_score.csv", encoding='utf-8', index=False) $
pd.concat([city_loc, city_pop], join="inner")
train.pivot_table(values = 'Fare', index = 'Class', aggfunc=np.mean)
all_data[all_data['price_change'].isnull().values]
df_bud = pd.read_csv(budFile, usecols = budCols, $                  dtype = bud_dtypes)
df_archive_clean["tweet_id"].sample(5)
df3 = pd.DataFrame(q3_results,columns=['station_name','observed_tobs']) $ df3['observed_tobs']=df3['observed_tobs'].astype(float) $ df3.dtypes
plt.scatter(y_test, y_test_pred, alpha=0.05) $ plt.ylabel('Predicted rating') $ plt.xlabel('Actual rating'); $ plt.title('Predicted rating vs. actual rating\n(test set)');
print(type(full_globe_temp)) $ print(full_globe_temp.dtype) $ print(full_globe_temp.shape) $ print(full_globe_temp.nbytes)
df2 = df.drop((df[(df.group == 'treatment') & (df.landing_page == 'old_page')].index)|(df[(df.group == 'control') & (df.landing_page == 'new_page')].index))
version = str(int(time.time()))
da = DataFrame(res.A, columns=vectorizer.get_feature_names())
df3 = pd.read_csv("countries.csv") $ df3.head()
df2[df2.converted == 1].shape[0]/ df2.shape[0]
df['screen_name'].value_counts()
grade_levels = [textstat.textstat.textstat.flesch_kincaid_grade(has_text.ix[index].text) for index in range(len(has_text))]
testing_array = np.concatenate((testing_active_listing_dummy,test_pending_ratio),axis=1)
df.query('group == "treatment" and landing_page != "new_page"').group.count() +df.query('group != "treatment" and landing_page == "new_page"').group.count()
df_characters['num_lines'].agg(['min', 'mean', 'max'])
lv_workspace.get_data_filter_info(step=1, subset='A') 
autos['registration_year'].describe()
from sklearn.model_selection import cross_val_score
df2['US_ind'] = df2['US']*df2['ab_page'] $ df2['UK_ind'] = df2['UK']*df2['ab_page'] $ model=sm.Logit(df2['converted'],df2[['intercept','ab_page','US','UK','US_ind','UK_ind']]) $ result=model.fit() $ result.summary()
data.columns = ['West', 'East'] $ data['Total'] = data.eval('West + East')
def percentConvert(value): $     full = value * 100 $     return '{0:.1f}'.format(full)
tweet_json.describe()
datetime.now().toordinal() - datetime(1987, 1, 4).toordinal()
avg_att_2015_BOS = nba_df.loc[(nba_df["Season"] == 2015) & (nba_df["Team"] == "BOS"), "Home.Attendance"].mean() $ round(avg_att_2015_BOS, 0)
>ljan1=pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/observed_field.cat',names=['name']) $ matched_id=[np.where((final0['name']==i)==True)[0][0] for i in jan1['name']] $ final1=final0.drop(matched_id) $ final1.head() $ print final0.shape, final1.shape
df3[df3['group']=='treatment'].head()
reviews = np.array(tf.review) $ reviews_vector = vectorizer.transform(reviews) $ predictions = clf.predict(reviews_vector) $
from sklearn.metrics import confusion_matrix $ confusion_matrix(y_true=df['actual_class'], $                  y_pred=df['predicted_class'])
def select_range(df,  min_value, max_value, col = "price"): $     df2 = df.loc[(df != 0).all(axis=1), :] $     return df2[df[col].between(min_value, max_value, inclusive=False)]
pd.Timestamp('January 1st 2018')
df_new_log = pd.get_dummies(df_new,columns = ['landing_page', 'group','country']) #create dummies $ df_new_log = df_new_log.drop(['landing_page_old_page','landing_page_new_page','group_control','country_CA'],axis=1) #drop unnecessary columns $ df_new_log['intercept'] = 1 $ df_new_log.head()
geo_db.describe()
ser = pd.Series(["practical","data","science","tutorial"], index = [5,6,7,8]) $ ser
sns.pairplot(data, hue='species', size=3)
celtics.reset_index(drop=False, inplace=True) $ celtics.rename(columns={'index':'game_id'}, inplace=True)
assert df_total.isna().any().any() == False
df_h1b_ft_US = df_h1b[map(lambda x: x[1].lca_case_workloc1_state in states $                           and x[1].full_time_pos=='Y' $                           and (x[1].status=='CERTIFIED' or x[1].status=='DENIED'), $                           df_h1b.iterrows())] $ df_h1b_ft_US = df_h1b_ft_US.drop([33052, 44675, 45556, 48967, 51744, 53898, 135911, 188139, 225285, 332969, 366595])
conn.autocommit = True $ c = conn.cursor()
final_result = final_result[pd.notnull(final_result['Datetime'])] $ final_result = final_result.drop_duplicates(subset='Datetime', $                                             keep="first") $ len(final_result)
Base = automap_base() $ Base.prepare(engine, reflect= True) $ Base.classes.keys() $
data_full = data_full.dropna()
same_destination = trips[trips['start_station_name'] == trips['end_station_name']] $ same_destination = same_destination[same_destination['duration'] > 120] $ same_destination = same_destination[same_destination['duration'] < 3600] $ same_destination.sort_values(by='duration',ascending=True)
persort = percent.abs().sort_values(ascending=False)
response = requests.get(form_url('apiVersions')) $ print_body(response, max_array_components=3)
sh_max_df = pd.DataFrame(np.array(sh_results), columns=(["date","tobs"])) $ sh_max_df
like_stats = USvideos.groupby('channel_title').agg({'like_ratio': [np.size, np.mean]}) $ atleast_5 = like_stats['like_ratio']['size'] >= 5 $ like_stats[atleast_5].sort_values([('like_ratio', 'mean')], ascending=False)[:10] $ like_stats[atleast_5].sort_values([('like_ratio', 'mean')], ascending=True)[0:10]
rf = RandomForestClassifier() $ rfparams = {'random_state' : 42, 'n_jobs' : 8,  'class_weight' : 'balanced', $                 'max_depth': 8, 'n_estimators' : 300, 'max_features' : 0.2} $ rf.set_params(**rfparams)
ticks.head()
for key, value in sample_dic.iteritems(): $     print value
ed = ['continuing', 'education', 'school'] $ school = wk_output[wk_output.explain.str.contains('|'.join(ed))] $ school.shape $ school.to_csv('school_feedback.csv')
len(donald_trump_tweets['screen_name'].value_counts())
import requests $ import pprint $ pp = pprint.PrettyPrinter(indent=4) $ from collections import OrderedDict, defaultdict, namedtuple
y_predict=[round(ii[0]) for ii in model.predict(x)] $ deviate=[0.5 for aa,bb in zip(y,y_predict) if aa==bb] $ plt.figure() $ plt.plot(y,marker='s',linestyle='') $ plt.plot(deviate,marker='h',markersize=1,linestyle='',color='k') $
for col in Y_train_df.columns: $     nbsvm_models[col] = NbSvmClassifier(C=10, dual=True).fit(X_train_cont_doc, Y_train_df[col])
df_archive["rating_numerator"].value_counts()
one_test_pvalue = 1 - 0.19/2 $ one_test_pvalue
nar4=nar3.merge(avg_yield[['avg_investment_apr']],left_on='fk_loan',right_index=True)
autos["odometer_km"].describe()
print pd.pivot_table(data=df, $                      index='date', $                      columns='item', $                      values='status', $                      aggfunc='sum')
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.hour).count().plot(y="Borough")
test = full_orig.set_index('AdmitDate')['2016'] $ X_test = test.drop(['Patient','Readmitted','DaysSinceAdmission','<=30Days'],axis=1)
tweets.head()
bad_tc_isbns = pd.read_table("C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/TC and Law invalid ISBNs.txt") $ bad_tc_isbns = bad_tc_isbns['ISBN'] $ print(bad_tc_isbns.size)
X=preprocessing.scale(X)
final['Crime Count in 5k02'] = final['Crime Count in 5k02'].fillna(0)
feature_importances[:10].plot.bar()
os.getcwd()
first_commit_timestamp = git_log.loc[git_log["author"] == "Linus Torvalds", "timestamp"].min() $ last_commit_timestamp = pd.to_datetime("today") $ corrected_log = git_log.loc[(git_log["timestamp"]>=first_commit_timestamp) & (git_log["timestamp"]<=last_commit_timestamp)].copy() $ corrected_log["timestamp"].describe()
tweets2.text[0]
df_ratings.shape
cats_out = outcome.loc[outcome['Animal Type']=='Cat'] $ cats_out.shape
df2['rx_requested'] = df2['rx_requested'].map(lambda x: x.lower())
beijing['WindDirDegrees'] = beijing['WindDirDegrees'].astype('float64')   
texts = [[token for token in text.split() if frequency[token] > 1 and token not in nltk_stops] $           for text in documents] $ texts
df2[df2.duplicated(['user_id'], keep=False)] #find row information for the repeat user_id
from sklearn.neighbors import KNeighborsClassifier
nitrogen['ResultSampleFractionText'].unique()
rowsToSkip = list(range(28)) $ rowsToSkip.append(29)
data_scrapped = pd.read_csv("data_scraped.csv")
df.ix[1:4]
merged_data['Rain'] = merged_data['Precipitation_In '].apply(lambda x:1 if x != "0" else 0)
store_items = store_items.append(new_store) $ store_items
save(p_nmf2, 'gradadmission.html')
try: $     cur_a.execute('UPDATE hotel set hotel_id=99 WHERE hotel_id=1') $ except Exception as e: $     print('Exception: ', e)
shifted_backwards.tail(5)
goog.head()
order_item_merge.to_csv('../2_data/prepared/order_item_merge.csv', index=False)
autos.rename({'odometer':'odometer_km'},axis=1,inplace=True)
import pandas as pd $ df = pd.DataFrame()
fig, ax = plt.subplots(figsize=(8, 6), dpi = 75) $ f1 = passes['Receiver'].value_counts().plot(kind='barh') $ f1.set(title = "Players with the most Yards after receiving Passes", xlabel='Count', ylabel='Player Name') $ plt.show()
from sklearn.pipeline import Pipeline $ pipeline = Pipeline([ $         ('features', cv), $         ('model', model)   $     ])
goog.plot(alpha=0.5, styple='-') $ goog.resample('BA').mean().plot(stype=':') $ goog.asfreq('BA').plot(stype='--') $ plt.legend(['input', 'resample', 'asfreq'], loc='upper left')
df_birth.head()
df1.columns = df2.columns $ df=pd.concat([df1,df2]) $ df.head()
no_outliers_forecast_exp3[(no_outliers_forecast_exp3.index >= '2018-06-01') & (no_outliers_forecast_exp3.index <= '2018-12-31')].astype(int)
import statsmodels.api as sm $ convert_old = df2.query('group == "control"and converted == 1').count()[0] $ convert_new = df2.query('group == "treatment"and converted == 1').count()[0] $ n_old = df2[df2['landing_page']=="old_page"].count()[0] $ n_new = df2[df2['landing_page']=="new_page"].count()[0]
m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range) $ lr = 1e-3
lr.fit(X_train,y_train)
a.iloc[3]
ds_issm = xr.open_dataset(data_url1) $ ds_issm = ds_issm.swap_dims({'obs': 'time'}) $ ds_issm
fin_r.index
cfs=cfs.sort_samples('Subject')
np.random.seed(123) $ b = pd.Series(np.round(np.random.uniform(0,1,10),2)) $ c = b.copy() $ b.index = np.random.permutation(np.r_[0:10]) $ b
df.info() $ df.isnull().sum() #Counts all null values
data.show()
RNPA_new = RNPA[RNPA['ReasonForVisitDescription'].str.contains('New')] $ RNPA_existing = RNPA[~RNPA['ReasonForVisitDescription'].str.contains('New')]
predicted_live = model.predict(X_live) $ predicted_live
len([baby for baby in BDAY_PAIR_df.pair_age if baby<3])
grp1 = df.query("group == 'treatment' and landing_page == 'old_page'") $ print("Times treatment group user lands incorrectly on old_page is {}".format(len(grp1))) $ grp2 = df.query("group == 'control' and landing_page == 'new_page'") $ print("Times control group user incorrectly lands on new_page is {}".format(len(grp2))) $ print("Times new_page and treatment don't line up is {}".format(len(grp1) + len(grp2)))
test_df.labels.mean()
test_bkk2.reset_index(inplace=True) $ test_kyo2.reset_index(inplace=True)
S.decision_obj.stomResist.value = 'simpleResistance' $ S.decision_obj.stomResist.value
authors_with_name = authors_grouped_by_id_saved.select( $     "*", parse_name_info_udf("Author").alias("parsed_info")).cache()
with open('nmf_mod_5_29.pkl', 'wb') as piccle2: $     pickle.dump(nmf_new, piccle2)
pd.read_pickle("baseball_pickle")
count_authors_with_given_numer_publications = data_final.groupby('countPublications', as_index=False)['authorId'].count() $ count_authors_with_given_numer_publications.columns = ['number_publications', 'how_many_authors_with_given_publications'] $ count_authors_with_given_numer_publications.head(20)
df4 = df3.merge(df_countries, on = 'user_id')
old_page_converted = np.random.choice(2, size = n_old, p=(p_old,1-p_old)) $ old_page_converted
df2.user_id.count() == df2.user_id.nunique()
pres_df['metro_area'].tail()
pt = to_plot_df['vote_count'].plot(kind='bar', figsize=(20,10), rot=0, fontsize=20) $ plt.xlabel(pt.get_xlabel(), fontsize=22) $ plt.ylabel('Average of vote_count', fontsize=22) $ plt.title('Average of vote_count by original_language', fontsize=30)
df_data.VITIMAFATAL.value_counts()
sentences = open('metoo_tweets.txt', 'r').readlines()
plt.hist(reddit['Upvotes'], range=(20,1000)) $ plt.xlabel('Number of Upvotes',fontsize='large') $ plt.ylabel('Number of Reddit Posts',fontsize='large') $ plt.title('The distribution of upvotes has a similar shape to the comments distribution', fontsize='large') $ plt.show()
uusers = df['user_id'].nunique() $ print("Our dataset contains {} unique users.".format(uusers))
live_weights.describe()
plot_data = df['amount_tsh'] $ sns.kdeplot(plot_data, bw=100) $ plt.show()
model = models.TfidfModel(corpus, normalize=True)
df2 = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == True] $ df2.head()
subred_num_avg.head(10)
print(a[0])  # Implemented by __getitem__ $ a[0] = "t"  # No can do; strings are immutable.
suburban_ride_total = suburban_type_df.groupby(["city"]).count()["ride_id"] $ suburban_ride_total.head() $
converted = df2.query('group == "control" and converted == 1').user_id.count() $ total = df2.query('group == "control"').user_id.count() $ converted / total
cust_data.head(3)
p.to_timestamp('M', 's')
twitter_archive_clean=twitter_archive_clean[~twitter_archive_clean.expanded_urls.isnull()]
xmlData.to_csv('xmlFile3.csv', index = False)
number_unpaids.plot.scatter(x='score', y='paid_status', figsize=(20,10))
import bikescore $ bikescore.init() $ testsavedmodel() $
vol = vol.fillna(0.) $ vol.head(20)
nx.draw(H, pos=nx.spring_layout(H)) $ plt.show()
cur.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'tweet_dump';") $ result = cur.fetchall() $
import matplotlib.pylab as plt $ lists = r_close.items() # sorted by key, return a list of tuples $ x, y = zip(*lists) # unpack a list of pairs into two tuples $ plt.plot(x, y) $ plt.show()
startTime_user = train['user_date_created'].min() $ y = train['is_fake'].astype(int)
twitter_df_clean = pd.merge(twitter_df_clean, tweet_json_df, $                             on=['tweet_id'], how='inner')
df2['intercept']=1 $ df2[['ab_page','ab_page_old']]=pd.get_dummies(df2['landing_page']) $
top_songs['Date'] = pd.to_datetime(top_songs['Date'])
tweet_file = './data/tweets.json'
print('Predicted price of the 1st house: {:.1f}'.format(prediction_multiple[0])) $ print('Predicted RSS: {}'.format(np.sum((prediction_multiple - test_output_multiple)**2)))
conversion_rate_all_pages = df2.query('converted == 1').shape[0] / df2.shape[0] $ print('The probability of an individual converting regardless of the page is {}'.format(conversion_rate_all_pages))
pres_df.ix[362865] # how to get row info by index number
joined = pd.read_feather(f'{PATH}joined') $ joined_test = pd.read_feather(f'{PATH}joined_test')
autos.rename({'yearOfRegistration':'registration_year', $              'monthOfRegistration':'registration_month', $              'notRepairedDamage':'unrepaired_damage', $              'dateCreated':'ad_created'}, $             axis=1, inplace=True)
financial_crisis.iloc[-2]
%matplotlib inline $ import numpy as np $ import seaborn as sns $ import scipy.stats as stats
hashtags["hashtag"].str.lower().value_counts()
df = pd.DataFrame({'one':[10,20,30,40,50,2000], $ 'two':[1000,0,30,40,50,60]}) $ df
plt.hist(index_temp_df.tobs , bins=12) $ plt.xlabel("Temperature in Honalulu") $ plt.ylabel("Frequency") $ plt.title("2016 Temperature Observed at " + most_active) $ plt.show()
bad_iv_post = options_frame[np.isnan(options_frame['ImpliedVolatilityMid'])]
fig = plt.figure(figsize=(10,4)) $ ax = fig.add_subplot(111) $ ax = resid_713.plot(ax=ax);
oppstage = segmentData[['lead_mql_status', 'opportunity_month_year', 'opportunity_stage']].pivot_table( $         index=['lead_mql_status', 'opportunity_month_year', 'opportunity_stage'], aggfunc=len, fill_value=0).reset_index()
print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ assert os.getcwd().split('/')[len(os.getcwd().split('/'))-1] == str(today),'Hold on a second' $ imagelist = [i for i in os.listdir() if i.endswith(".txt")  ] $ imagelist
stations_info_dict = dict(zip(stations_info.name, stations_info.station)) $ stations_info_dict
df = pd.read_sql('SELECT * FROM actor WHERE first_name ilike \'Groucho\' and last_name ilike \'Williams\'', con=conn) $ df.head()
diffs = np.array(p_diffs) $ null_vals = np.random.normal(0, diffs.std(), diffs.size) $ plt.hist(null_vals); $ plt.axvline(x = obs_diff, color = 'red');
len(df_json.query('retweeted == True'))
session.query(Actor.first_name, Actor.last_name).frame().head()
twitter_archive_clean['quick_url'] = twitter_archive_clean.text.str.extract('(https.+)', expand=True) $
p_old = df2.query("converted==1 & group=='control'").shape[0]/df2.query("group=='control'").shape[0]
df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv",nrows=50000)
op_add_comms['thanks'] = op_add_comms['body'].apply(lambda x: any(substring in x.lower() for substring in thanks))
df_archive_clean[[ "timestamp", "retweeted_status_timestamp"]].sample(5)
n_new = len(df2.query("landing_page == 'new_page'"))           $ print('n_new :: ', n_new)
infinity.to_csv('InfinityWars_Predictions_2.csv', encoding='utf-8')
season12 = ALL[(ALL.index >= '2012-09-05') & (ALL.index <= '2013-02-03')]
X_train_df = pd.DataFrame(X_train_matrix.todense(), $                          columns=tvec.get_feature_names(), $                          index=X_train.index)
idx = payments_all_yrs[ payments_all_yrs['Num_DRGs']>0].index.tolist() $ len(idx) $
result = pd.DataFrame(result)
df.to_excel("../../data/stocks2.xlsx")
correct = y_test.eq(p_y>0.5) $ total_correct = sum(correct) $ print("Total correct", total_correct, " out of ", len(y_test), "which is", float(total_correct)/len(y_test), "%")
data['temp'] = pd.to_datetime(data['temp']) $ data['Week Ending Date'] = pd.to_datetime(data['Week Ending Date']) $ data.head(5) $
merkmale.xs(99550,level='id').to_clipboard()
negGroups2 = list(neg_tweets.group_id_x) $ num_convos = len(set(negGroups)) $ print(f'Working with {num_convos} conversations') $ companyNeg2 = filtered[filtered.group_id.isin(negGroups2)]
birth_dates.head(3)
df_ad_airings_filter_3.to_pickle('./TV_AD_AIRINGS_FILTER_DATASET_2.pkl')
treatment_conversion = df2[(df2['group'] == "treatment") & (df2['converted'] == 1)].count() $ total_treatment = df2[(df2['group'] == "treatment")].count() $ print(treatment_conversion/total_treatment)
df = pd.read_sql('SELECT * FROM booking', con=conn_b) $ df.head(15)
random_forest.fit(train, train_labels) $ feature_importance_values = random_forest.feature_importances_ $ feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values}) $ predictions = random_forest.predict(test)
taxi_hourly_df.shape
y_pred = model.predict(x_test, batch_size=1024, verbose=1)
df2.groupby('group').count()
url_img = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars' $ browser.visit(url_img)
df['response'] = df.len_convo.apply(lambda x: 1 if x > 1 else 0)
bigdf_read = pd.read_csv('Combined_Comments-Fixed2.csv', index_col=0)
s_empty = pd.Series() $ s_empty.empty
tobs_data = session.query(Measurement.date,Measurement.tobs).\ $     filter(sqlalchemy.and_(Measurement.date <= latest_date, Measurement.date >= tobs_start_date) ).\ $            filter(Measurement.station == most_active_station).all()
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt
(keys.shape, keys_0611.shape)
images.img_num.value_counts()
dashdata['5_day_target'].max
libraries_df.tail()
df_2018.isnull().sum()
train = read_data('data/train.tsv') $ validation = read_data('data/validation.tsv') $ test = pd.read_csv('data/test.tsv', sep='\t')
print("Number of Techniques in PRE-ATT&CK") $ techniques = lift.get_all_pre_techniques() $ print(len(techniques)) $ df = json_normalize(techniques) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'detectable_by_common_defenses', 'contributors'], axis=1)[0:5]
Precipitation_DF.describe()
during['count'] = during.groupby('hashtags')['hashtags'].transform(pd.Series.value_counts) $ during.sort('count', ascending=False) $ during.hashtags.dropna().head()
df.to_csv("tweets_with_treatments.csv")
url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ browser.visit(url)
yelp_dataframe.head(2)
df2[(df2.group == 'treatment')].converted.mean()
df2['converted'].mean()
%matplotlib inline $ import seaborn as sns  $ sns.set_style('darkgrid')
full_clean_df.sample(1)
lgbm = lgb.LGBMRegressor() $ lgbmrscv = model_selection.RandomizedSearchCV(lgbm, params, n_iter=10,cv=tscv,n_jobs=-1)  $ lgbmrscv.fit(train[col],np.log1p(train['visitors'].values)) $ print(lgbmrscv.best_params_)
from sklearn.manifold import TSNE $ tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca', metric='cosine') $ tsne_nmf = tsne_model.fit_transform(nmf_doc_top)
monthly_sales = sales.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day':'sum'}) $ monthly_sales.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True) $ monthly_sales.reset_index(level=['date_block_num', 'shop_id', 'item_id'], inplace=True) $ monthly_sales.head()
from sklearn.feature_extraction.text import CountVectorizer $ from sklearn.decomposition import LatentDirichletAllocation
my_refinedquery = "SELECT name, id, ephys_roi_result_id FROM specimens LIMIT 5" $ refined_df = get_lims_dataframe(my_refinedquery) $ refined_df.tail()
autos.columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', $        'vehicle_type', 'registration_year', 'gearbox', 'power_pS', 'model', $        'odometer', 'registration_month', 'fuel_type', 'brand', $        'unepaired_damage', 'date_created', 'nr_of_pictures', 'postal_code', $        'last_seen']
(autos["last_seen"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
raw_df.head(5)
plt.scatter(x=df['tweet_id'], y=df['favorite_count']);
today = datetime.now() $ dates = list(pd.date_range(join_date, today)) $ print("Days Since Joining: " + str(len(dates))) # days since joining
fb.head()
appointments.shape
merged_portfolio_sp_latest_YTD_sp['Cum Invst'] = merged_portfolio_sp_latest_YTD_sp['Cost Basis'].cumsum() $ merged_portfolio_sp_latest_YTD_sp['Cum Ticker Returns'] = merged_portfolio_sp_latest_YTD_sp['Ticker Share Value'].cumsum() $ merged_portfolio_sp_latest_YTD_sp['Cum SP Returns'] = merged_portfolio_sp_latest_YTD_sp['SP 500 Value'].cumsum() $ merged_portfolio_sp_latest_YTD_sp['Cum Ticker ROI Mult'] = merged_portfolio_sp_latest_YTD_sp['Cum Ticker Returns'] / merged_portfolio_sp_latest_YTD_sp['Cum Invst'] $ merged_portfolio_sp_latest_YTD_sp.head() $
import sys $ !conda install --yes --prefix {sys.prefix} pandas-datareader
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old]) $ print(old_page_converted)
tweet_archive_clean.loc[2436]
vip_reason = vip_reason.drop(['[', ']'], axis=1) $ vip_reason = vip_reason.drop(vip_reason.columns[0], axis=1)
df['lemma'] = np.array([ lemmatiz(comment) for comment in df['body'] ])
for zone in zones: $     filename = 'sidewalk_zones/sw_zone_%d' % zone + '.geojson' $     z_query = "VTA_TAZ==%d" % (zone) $     joined.query(z_query)[['linestring','surface','highway','footway']].to_file(filename, driver='GeoJSON')
data_table.to_csv("C:/R_data_new/crawling_outcome/company934+.csv",sep=",",encoding ="utf-8")
model_data['adopted_user'] = [1 if x in final_dict else 0 for x in model_data.object_id] $ print(model_data)
data.loc[data.density > 10, ['pop', 'density']]
met.T.plot(kind='barh', figsize=(5,3), xlim=(0,1)) $ met
joined_data = official_data.join(social_disorder).join(positive_amentities).join(negative_amentities).reset_index() $ joined_data
tip_sample.describe()
start_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ start_date
rand_bg = np.random.uniform(low=2.5, high=19, size=(21,)) # create a data sample of random numbers $ rand_bg_s = pd.Series(rand_bg) $ bg_s = rand_bg_s.round(1) # reduce to one decimal places to it's a little easier to read $ bg_s # bg_s name confers 'bg' and 'Series', a nice short name that reminds you it's a Series
popular_programs = challange_1["program_code"].value_counts() $ popular_programs
repeated_user=df2.groupby(['user_id']).size().idxmax() $ print (repeated_user)
df_train[0:5].T
xgb_pred = gbm.predict(X_test)
lm=sm.Logit(sub_df2['converted'], sub_df2[['intercept', 'ab_page','Monday']]) $ results=lm.fit() $ results.summary()
sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller')
def str_merge(str1, str2): $     return "{} {}".format(str1, str2)
haw['M_lbs'] = haw["AnnualFlow_MGD"] * haw['TotalN'] * 8.34 / 1000000
!head -n 2 Consumer_Complaints.csv
evaluator.plot_confusion_matrix(normalize=False, $                                 title='Confusion matrix, without normalization', $                                 print_confusion_matrix=False, $                                 figsize=(8,8), $                                 colors=None)
autos.dropna(subset=['price'],inplace=True) $ autos.shape
twitter_archive_df_clean['expanded_urls'].isnull().any()
x_axis =[''] $ plt.bar(x_axis,tobs_mean,width=0.35,yerr = (tobs_max - tobs_min)) $ plt.title("Trip Average Temperature") $ plt.ylabel("Farenheit") $ plt.show()
df.columns = ['Open', 'Closed']
df.rating_numerator.value_counts()
%%time $ with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f: $     carray = f.create_carray(where='/', name='carray_without_numexpr', atom=tb.Float32Atom(), shape=x.shape) $     carray[:] = x**3 + 0.5*x**2 - x
rr.tail(3)
df.describe()
x = raw_valid.copy() $ x['pred_std'] = np.std(preds, axis=0) $ x['pred'] = np.mean(preds, axis=0)
popularity.info()
import plotly.graph_objs as go $ import plotly.plotly as py
joined=load_df('joined_promo_bef_af2.pkl')
actual_diff = p_treatment - p_control $ (p_diffs > actual_diff).mean()
df98 = pd.read_csv('1998.csv')
html_table_marsfacts = df.to_html() $ html_table_marsfacts
df.loc['r1':'r2','B':'C']
df2.shape $ df2.query('landing_page == "new_page"') $
frames = [k_var, k_var1] $ k_var_concat = pd.concat(frames) $ g = k_var_concat.groupby(['blurb']) $ size = g.size() $ size[size > 1].head()
logit = sm.Logit(df4['converted'], df4[['country_UK', 'country_US', 'intercept']]) $ result = logit.fit()
house_data['bedrooms'].nunique()
import statsmodels.api as sm $ convert_old = sum(df2[df2['landing_page'] == 'old_page']['converted']) $ convert_new = sum(df2[df2['landing_page'] == 'new_page']['converted']) $ n_old = sum(df2['landing_page'] == 'old_page') $ n_new = sum(df2['landing_page'] == 'new_page')
raw_df = raw_df.reset_index() $ raw_df = raw_df.rename(index=str, columns={"index": "Date", 1: "Twitter Names", 2:"Words", 3:"Retweeted"}) $ raw_df.index.names = ["IndexLabel"]
from sklearn import svm $ clf = svm.SVC(kernel='rbf') $ clf.fit(X_train, y_train) 
html = browser.html $ news_soup= BeautifulSoup(html, 'html.parser') $
