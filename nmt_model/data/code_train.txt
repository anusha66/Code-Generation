merged_311_data.drop(['Definition','Department', 'Issue'], $                      axis=1, $                      inplace=True) $ merged_311_data.head()
intervention_test['CRE_DATE_GZL'].min(), intervention_test['CRE_DATE_GZL'].max()
prediction_true = twitter_df_merged[twitter_df_merged['prediction_1_result'] == True] $ top20 = prediction_true['prediction_1'].value_counts().head(20).index $ top20_df = twitter_df_merged.loc[twitter_df_merged.prediction_1.isin(top20)] $ fav_summary = top20_df.groupby('prediction_1', as_index=False).mean().sort_values('favorite_count')
model.wv.similarity('king', 'women')
merged1.to_csv('./data/appointments_full.csv')
result[result['dt_deces'].notnull()].shape
df_master.info()
dfg["cluster"] = first_cluster.predict(docs)
pickle.dump( train, open("../Data/train.csv", "wb") )
jaccard_similarity_score(y_test, yhat) $ print("Jaccard Similarity Score: ",  jaccard_similarity_score(y_test, yhat)) $ print("F1-score Accuracy: ",  metrics.f1_score(y_test, yhat, average='weighted')) $
eval_vot_tf_tts = clf_vot_tf.score(X_testcv_tf, y_testcv_tf) $ print(eval_vot_tf_tts)
mean = np.mean(data['len']) $ print("The length's average in tweets: {}".format(mean))
df1 = pd.read_csv('so2_summary.csv', names=names1, skiprows=1, nrows=88, parse_dates=True, index_col='Date') $ df1['Vanuatu - Ambae'] = np.NaN $ df1['Vanuatu - Tanna'] = np.NaN
inputNetwork.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=directory_name)
result = log_mod.fit() $ result.summary()
n_new = (df2[df2['landing_page']=='new_page']).count()[0] $ n_new
%%bash $ cd /data/LNG/Hirotaka/ASYN $ cat ASYN/*/PPMI1.*.txt > PPMI1_all $ cat ASYN/*/PPMI0.*.txt > PPMI0_all
plt.hist(null_vals) $ plt.axvline(x=act_diff, color='red');
classifier = MultinomialNB() $ classifier.fit(train_features_tokenized, authors_train) $ test_features_tokenized = vectorizer2.transform(articles_test) $ classifier.score(test_features_tokenized, authors_test)
nposts = nposts[(nposts.index >= startDate) & (nposts.index <= endDate)] $ nposts = nposts.resample(sampling, how='mean') $ nposts.head()
import lda $ model = lda.LDA(n_topics=30, n_iter=1500, random_state=1) $ model.fit(tf)
df2['intercept'] = 1 $ df2[['ab_page', 'non_ab_page']] = pd.get_dummies(df2['landing_page']) $ df2 = df2.drop('non_ab_page', axis=1) $ df2.head()
eSL['APP_STORE_BRANCH'] = [str.upper(x)[0:6] if str.upper(str(x)).find('CLACORP.COM')>-1 else np.nan $                            for x in eSL.APP_LOGIN_ID.values] $ bSL = f_ELMS_branch_info(pconn=pconn) $ bSL.drop_duplicates(inplace=True) $ aSL = eSL.merge(bSL, how='left', left_on='APP_STORE_BRANCH', right_on='BRANCH')
class_merged=pd.merge(class_merged,class_onpromotion,on=['date','store_nbr','class','family'],how='left') $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
engine.execute("SELECT count(*) FROM contractor").fetchall()
obj4.name = 'population'
plot.independent_height_weight(num_samps=8, $                                diagrams='../slides/diagrams/ml')
df['Updated Shipped ranges'] = pd.cut(df['Updated Shipped diff'], $                                      bins=[0,500,1000,2000,2500,3000,10000], $                                      right=False)
new_page_converted = np.random.binomial(n_new,p_new) $
print('max value: ' + str(df_usa['NRI'].max()) + ' occurs in ' + str(df_usa['NRI'].idxmax())) $ print('min value: ' + str(df_usa['NRI'].min()) + ' occurs in ' + str(df_usa['NRI'].idxmin()))
wrd_clean.query('rating_numerator == 9.75')
data.T
from nltk.sentiment.vader import SentimentIntensityAnalyzer
poly17= PolynomialFeatures(degree=17) $ X_17 = poly17.fit_transform(X) $ linear = linear_model.LinearRegression() $ linear.fit(X_17, y2) $ (linear.coef_, linear.intercept_)
df_clean = df_clean.sort_values('dog_stage').drop_duplicates('tweet_id', keep = 'last')
learner.model.load_state_dict(weights)
df_csv = pd.read_csv(datafile) $ df_csv.head() $
Y_tweet.max()
len(kochdf.loc[kochdf['user'] == "Rezeptsammlerin"]['name'])
data.sort_values(by='Date',ascending=False,inplace=True) $ data.head()
fb['2012-04-13':].resample('W').count()['message'].sort_values(ascending=False)[:10]
temp_df.shape
stop_words = set(nltk.corpus.stopwords.words('english')) $ for w in ['say', 'said', 'also', 'would', 'wouldve', 'would\'ve', 'mr', 'ms', 'told', 'get']: $     stop_words.add(w) $ %timeit articles['tokens'] = articles['tokens'].map(lambda s: [w.lower() for w in s if w.lower() not in stop_words])
reg_mod_us = sm.OLS(df_all['converted'], df_all[['intercept', 'US']]) $ analysis_us = reg_mod_us.fit() $ analysis_us.summary()
dat_before_fill=dat.copy() $ for temp_col in temp_columns: $     dat.loc[:,temp_col]=dat[temp_col].interpolate(method='linear', limit=3)
num_users = df['user_id'].nunique() $ print('Number of unique users in dataset: ',num_users)
returns['MSFT'].corr(returns['IBM'])
X = filtered_file.iloc[:, 28:-1] $ y = filtered_file['TECTONIC SETTING'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.55, random_state=42)
import seaborn as sns $ sns.set(style='whitegrid', color_codes=True)
df_user_extract = pd.read_csv('data/df_user_extract.csv') $ df_user_extract_copy = df_user_extract.copy() $ df_user_extract_copy.head()
X_train.shape
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
testy = pd.concat([test_kyo2, test_bkk2], axis=1)
unsorted_df.sort_index() #aort by index
pie_df=twitter_df_clean.groupby('source').source.count() $ plt.pie(pie_df,labels=pie_df.index.get_level_values(0),labeldistance=1.8);
df_cat = pd.get_dummies(data[catFeatures])
airbnb_od.numeric_summary() # you can access to numeric
c['country'].unique()
chinadata.tail(100)
iowa_fc = iowa_fc_item.layers[0]
print(condos.UNITTYPE.value_counts()) $ print('\n') $ print(condos.STATUS.value_counts())
weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend') $ by_time = data.groupby([weekend, data.index.time]).mean()
grouped_by_dow_df = full_df.groupby('weekday')['listing_id'].count().reset_index().copy() $ grouped_by_dow_df.columns = ['day_of_week','count_of_listings'] $ grouped_by_dow_df['day_of_week'] = grouped_by_dow_df['day_of_week']\ $         .map( {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Sautrday', 6: 'Sunday'}).astype(str) $ grouped_by_dow_df.head(7)
education_data.columns=edu_columns
tweet_archive_df[tweet_archive_df.rating_numerator == 0].sort_values(by=['rating_numerator'],  ascending=False)[['tweet_id','rating_numerator', 'rating_denominator', 'text']][0:10]
output= "Insert into user values('@Pratik','Demo Add User', 100000,10 , 9999)" $ cursor.execute(output)
lggs = LogisticRegression(C=10,n_jobs=-1,penalty='l1') $ model = lggs.fit_transform(X_train,y_train) $ model.
convert_p_new = convert_p_old = probability $ print ("The convert rate for  p_new  under the null is: {:.4f}".format(convert_p_new))
dfz.set_index('timestamp', inplace=True)
from sklearn.tree import DecisionTreeClassifier
import pandas as pd $ row_df = pd.DataFrame(rows)
groceries.drop('apples', inplace=True)
autos['fuel_type'].cat.categories
df = pd.read_csv("https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/AirPassengers.csv") $ df[:5]
lb = aml.leaderboard $ lb
users.shape
pd.to_datetime(data['created'], infer_datetime_format=True)
new_page_converted = np.random.choice([1,0],size=nnew,p=[pmean, (1-pmean)]) $ new_page_converted.mean()
%bash $ model_dir=$(ls ${REPO}/courses/machine_learning/feateng/taxi_trained/export/exporter) $ gcloud ml-engine local predict \ $   --model-dir=${REPO}/courses/machine_learning/feateng/taxi_trained/export/exporter/${model_dir} \ $   --json-instances=/tmp/test.json
ghana['WindDirDegrees'] = ghana['WindDirDegrees'].str.rstrip('<br />')
autos.odometer_km.value_counts()
sns.countplot(x='store_nbr', data=pd_train);
%%time $ with tb.open_file(filename='data/NYC-yellow-taxis-2017-12.h5', mode='a') as f: $     table = f.get_node(where='/yellow_taxis_2017_12') $     table.cols.trip_distance.create_index() $     table.cols.passenger_count.create_index()
prob_new = df2[df2.group == 'treatment'].converted.mean() $ prob_new
price_df.drop(['date'], axis = 1, inplace = True)
print(train.shape) $ unique_user_count = len(set(train["id"])) $ print(unique_user_count) $ train.head(5) $ train.describe(include = "all") #many cols have missing information
station_count = df['station'].count() $ station_count
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] == 'control', 0, 1) $ df2.head()
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False)
df_genres = df.groupby("genre") $ print(df_genres) $ print(df_genres['score'].mean().sort_values(ascending=False))
join_e.where(F.col('highest_match')==1).count()
monthly_gain_summary.tail(10)
sns.lmplot('yearOfRegistration','price', data= cars) $
segments.st_time.dtype $ datetime.strptime(segments.st_time.loc[0],'%m/%d/%y %H:%M')
to_be_predicted_Day4 = 81.95063916 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
paired_df[['dataset_1', 'dataset_2']] = paired_df.paired_datasets.apply(pd.Series) $ paired_df = paired_df.loc[:, ['dataset_1', 'dataset_2','co_occurence']] $ paired_df = paired_df.loc[paired_df.co_occurence > 1]
z, p = sm.stats.proportions_ztest(count = [convert_new, convert_old], nobs=[n_new, n_old], alternative='smaller') $ print('z-score:', z, $       '\np-value:', p)
mod=sm.Logit(ab_new['converted'],ab_new[['intercept','CA','US']]) # checking if converted depends on these columns - intercept, CA, US $ result = mod.fit() #fitting the new model $ result.summary()
noise = np.random.normal(scale=0.5, size=4) # standard deviation of the noise is 0.5 $ y = m_true*x + c_true + noise $ plt.plot(x, y, 'r.', markersize=10) $ plt.xlim([-3, 3]) $ mlai.write_figure(filename="../slides/diagrams/ml/regression_noise.svg", transparent=True)
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(kind='bar', y='Agency')
filename = '../data/DadosBO_2017_1(FURTO DE CELULAR).csv' $ df_BOs = pd.read_csv(filename, sep=';', encoding='utf-8') $ df_BOs.info()
words_only_sk = [term for term in words_sk if not term.startswith('#') and not term.startswith('@')] $ corpus_tweets_streamed_keyword.append(('words', len(words_only_sk))) # update corpus comparison $ print('The number of words only (no hashtags, no mentions): ', len(words_only_sk))
temperature_2017= hawaii_measurement_df[['Date','Temperature']] $ temperature_2017_df = pd.DataFrame(temperature_2017[(temperature_2017['Date'] >= '2016-08-01')\ $                                 & (temperature_2017['Date'] <= '2017-08-23')]).set_index('Date') $ temperature_2017_df.head()
len(df_tick_clsfd_sent['4. close'])
np.std(old_means)
difference = new_page_converted.mean() - old_page_converted.mean() $ difference
df2 = df $ mismatch_index = mismatch_df.index $ df2 = df2.drop(mismatch_index)
filtered_df[avails].corr()
from htsworkflow.submission.encoded import Document $ from htsworkflow.submission.aws_submission import run_aws_cp
%%time $ located_data['country'][:50] = located_data['coordinates'][:50].apply(Tag_country)  #1m38s $
df2_treatment = df2.query('group == "treatment"') $ df2_treatment.query('converted == 1').user_id.nunique() / len(df2_treatment)
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Loud Music/Party') ].groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False]) $
dtc.fit(features_class_norm, overdue_transf) $ print_feature_importance(vectorizer.feature_names_, dtc.feature_importances_)
print(sensor.site) $ print(sensor.unit)
yc200902_short_with_duration = pd.read_csv('http://cosmo.nyu.edu/~fb55/data/yc200902_short_with_duration.csv') $ yc_sd = yc200902_short_with_duration $ yc_sd.head()
df_tot.head()
n_old = df2.query("landing_page=='old_page'").shape[0] $ n_old
old_page_converted = np.random.choice([0,1] , size = n_old, p=[p_old, (1-p_old)]) $ len(old_page_converted)
s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e']) $ print(s)
import networkx as nx $ ballot = get_adjacency_matrix(1253, as_np_array=True, verbose = False) $ ballot = nx.from_numpy_matrix(ballot) $ nx.draw_networkx(ballot)
df_cs['Sentiment_class'] = df_cs.apply(conditions, axis=1) $ df_cs.to_csv("costco_senti_score.csv", encoding='utf-8', index=False)
frame.reindex(['a', 'b', 'c', 'd'], ['Texas', 'Utah', 'California'])
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\Sample_Superstore_Sales.xlsx" $ df = pd.read_excel(path) $ df.head(5)
authorize_url = oauth.get_authorize_url(request_token) $ webbrowser.open(authorize_url) $ verify = input('Enter code: ')
df.iloc[list(error_filtered.index.values)]
import nltk $ from nltk.tokenize import word_tokenize $ tweet = 'RT @akhiludathu: just an example! :D http://example.com #NLP' $ print(word_tokenize(tweet))
rounds.hist(column = 'announced_year',bins = 10, figsize = (20,8))
df_new[['US', 'UK']] = pd.get_dummies(df_new['country'])[['US', 'UK']]
df.loc[df['Name'] == 'Mary'].head(10)
p_new = df2[df2['converted'] == 1]['user_id'].count() / df2['user_id'].count() $ p_new
RFC_model_grid = RandomForestClassifier() $ parameters = dict(n_estimators=[5, 10, 50, 100, 200], criterion=['gini', 'entropy']) $ RFC_grid = GridSearchCV(RFC_model_grid, param_grid=parameters, cv=5, verbose=3)#, scoring='f1') $ RFC_grid.fit(X_train, y_train)
df.loc[:,['A','B']]
X_test_matrix = cvec.transform(X_test) $ forest.predict(X_test_matrix) $ forest.score(X_test_matrix, y_test)
from biopandas.mol2 import PandasMol2 $ pmol = PandasMol2().read_mol2('./data/1b5e_1.mol2')
hidden_states = embedding_model.predict(test_vecs[:, 1:])
df2[(((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page'))) == False].shape[0]
X_words.head(1)
print('       Sponsored Jobs: ' + str(len(dedup[dedup.job_type == 'Sponsored']))) $ print('Unique Sponsored Jobs: ' + str(dedup[dedup.job_type == 'Sponsored'].hash.nunique()) + '\n') $ print('         Organic Jobs: ' + str(len(dedup[dedup.job_type == 'Organic']))) $ print('  Unique Organic Jobs: ' + str(dedup[dedup.job_type == 'Organic'].hash.nunique()))
f_user = os.path.join(data_dir, 'following_users.csv') $ f_term = os.path.join(data_dir, 'tracking_terms.csv') $ f_meta = os.path.join(data_dir, 'collection_meta.csv')
retweet_df = base_retweet_df.join(user_summary_df['gender'], on='user_id') $ retweet_df.count()
media_classes = [c for c in df_os.columns if c not in ['domain', 'notes']] $ breakdown = df_questionable[media_classes].sum(axis=0) $ breakdown.sort_values(ascending=False)
shirt_1.color
pred.shape
train.columns, test.columns,store.columns, 
df = pd.read_csv( $     os.path.join('../resource/postproc/cache', 'subj1/trial1', 'acq1.ifc'), $     sep='\t' $ ) $ df.head()
latest_timelog.tail()
y = model_df.score_str #labels $ X = model_df.drop(['score_str'], axis=1) #variables
sorted(twitter_df.rating_denominator.unique())
np.shape(grid_id_flat)
temp = df1[df1.year==2012].groupby("month").Available_Spots.sum() $ monthly_adj =pd.DataFrame({'month': np.arange(1,13), 'parking_by_month': temp.values}).set_index('month') $ monthly_adj
infered_gender_for_authors_pq_saved.take(5)
stacked = fish.stack().reset_index()
def generate(): $     for x in range(10): $         yield x $ Z = np.fromiter(generate(),dtype=float,count=-1) $ print(Z)
Xtrain = df_train[predictors] $ ytrain = df_train[target].values.astype(np.int).flatten() $ Xtest = df_test[predictors] $ ytest = df_test[target].values.astype(np.int).flatten() 
old_p_c = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)]) $ old_p_c.mean()
vlc[pd.notnull(vlc.bio.str.strip())].id.size
for i in range(len(data_type)): $     print (data_type.index[i],data_type[i])
pca_df.head()
abc = df.groupby(['drg3','year']).describe()
from plotly import __version__ $ from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot $ init_notebook_mode(connected=True)
test_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_test_reviews_sw) $ test_data_features_tfidf = test_data_features_tfidf.toarray() # Numpy arrays are easy to work with $ print(test_data_features_tfidf.shape)
os.listdir()
np.random.seed(123456) $ dates = pd.date_range('2014-8-1',periods=10) $ s1 = pd.Series(np.random.randn(10), dates) $ s1[:5]
Dataset contains retweets.  Needs to be deleted.
monthly_residuals = test_preds_monthly - test_target_monthly $ monthly_percent_off = ((test_preds_monthly - test_target_monthly) / test_target_monthly) * 100 $ annual_percent_off = ((test_preds_monthly.sum() - test_target_monthly.sum()) / test_target_monthly.sum()) * 100 $ print(monthly_percent_off) $ print(annual_percent_off)
for v in data.values(): $     if v['answers']['Q4'] == 'No': $         v['answers']['Q4A'] = 'n/a'
p_old=df2.query('converted == 1').user_id.nunique()/df2['converted'].count() $ p_old
df[df.index.month.isin([12,1,2])]['Complaint Type'].value_counts().head()
scoring_input_data.dtypes
def evaluate_classifier(classifier, x_test, y_test): $     pred_y = predict(classifier, x_test) $     return metrics.accuracy_score(y_test, pred_y) $ accuracy = evaluate_classifier(classifier, x_test , y_test) $ print (accuracy)
jaja= dataset.loc[dataset['customer_id']=="450e1c2cbd21687780153995f1be0c23"]
stoi = collections.defaultdict(lambda: 0, {v: k for k, v in enumerate(itos)})
join_e.count()
!type data\test1.csv 
rural_driver_total = rural_type_df.groupby(["city"]).mean()["driver_count"] $ rural_driver_total.head()
df.group.unique(), df.landing_page.unique()
con = sqlite3.connect('db.sqlite') $ con.execute("CREATE TABLE tbl(wikipedia TEXT, topic TEXT, year INTEGER, month INTEGER, pageviews INTEGER);") $ con.commit() # apply transaction $ con.close()
x = datetime.strptime(inner_list[0][0], '%Y-%m-%d') $ type(x.year)
monthly_clim = data_month['Temperature(C)'].mean() $ monthly_clim.index = monthly_clim.index.map(int) $ monthly_clim = monthly_clim.sort_index() $ monthly_clim.plot()
df3['evening'] =df3['timestamp'].map(after_mapper)
df = station_distance $ df.tail(2) $ df.columns.tolist()
instance.assumptionbreaker()
data.dropna(inplace=True)
!hdfs dfs -cat {HDFS_DIR}/p32cfr-output/part-00001 {HDFS_DIR}/p32cfr-output/part-00000 > p32cfr_results.txt
sc.stop()
nb_topics = 5 $ lda = LatentDirichletAllocation(n_components=nb_topics, learning_method='batch', random_state=42) $ lda.fit(vects)
len(non_rle_pscs[non_rle_pscs.secret_base == True])
plt.scatter(aqi['AQI_eug'], aqi['AQI_cg'], alpha=0.2) $ plt.xlabel('Eugene/Springfield'); plt.ylabel('Cottage Grove');
pres_df['time_from_creation_tmp'].tail(10)
merged_df = merged_df.set_index(['Description', 'Group']) $ merged_df
1/np.exp(res.params)
df.to_csv("newsOutletTweets.csv")
new_pred_prob = grid.predict_proba(X_new)[:, 1] $ new_pred_prob
sample = pd.read_csv('../data/ebola/sl_data/2014-08-12-v77.csv')
df['post_duration'].value_counts().sort_index().plot(color='r', alpha=.5);
merge=pd.merge(contribs,prop57,on="calaccess_committee_id")
stockdftest.head(3)
df_episodes.head()
df.first_browser.value_counts()
Customers_df = Change_New[['Sales_in_CAD', 'New_or_Returning']] $ Customers_df.head()
join_b.select('party_id_orig','name_similairity','address_similairity','st_name_similairity','predicted').show(5)
print("DataFrame existing column name (before rename):") $ df.head(2)
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
def get_child_column_data(node: dict) -> list or None: $     child_data_nodes = get_child_data_nodes(node) $     if child_data_nodes: $         return [column[DATA] for column in child_data_nodes.values()] $     return []
dates = num2date(times[:], times.units) $ print([date.strftime('%Y-%m-%d %H:%M:%S') for date in dates[:10]]) # print only first ten...
first_line = blame.loc[2].copy() $ first_line.line = 1 $ first_line
automl = autosklearn.regression.AutoSklearnRegressor( $ )
from sklearn.ensemble import RandomForestRegressor $ rf_reg = RandomForestRegressor(max_depth=25, random_state=0) $ rf_reg.fit(x_train,y_train)
user_date_frequency = user_df["date"].value_counts() $ ax = plt.subplot(111) $ ax.bar(user_date_frequency.index, user_date_frequency.data) $ ax.xaxis_date() $ plt.show()
df2.to_csv('clean_data_df2.csv', index=False)
start_df['WorkDay'] = (start_df.index.weekday < 6) * 1 $
shows_good_data.to_csv('shows_good_data.csv', encoding='utf-8', index=False)
twitter_df_clean.head(2)
l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2)) $ l2_tfidf
SANDAG_jobs_df = pd.read_csv(open(SANDAG_jobs_filepath), dtype={'TRACT': str}) $ SANDAG_jobs_df.columns = ['TRACT', 'YEAR', 'TYPE', 'JOBS'] $ SANDAG_jobs_df.head()
data["Council District"] = data["Council District"].astype("int")
df_min_max = cs.apply(new_min_max_pop, axis = 1)
obs_diffs = treatment_convert_rate - control_convert_rate $ p_val = (obs_diffs < p_diffs).mean() $ print('The p value is {}'.format(p_val))
condos = pd.merge(df, aru_df, on='SSL')
df.Date = pd.to_datetime(df.Date)
dictionary.save('experiment/dictionary.dict')
df_kick= kickstarters_2017.set_index('ID')
dfs = [breed_predict_df_clean, counts_df_clean, archive_df_clean] $ df_final = reduce(lambda left,right: pd.merge(left,right,on='tweet_id'), dfs ) $
df2['intercept'] = 1 $ df2[['ab_page', 'old']] = pd.get_dummies(df2['landing_page']) $ df2.drop('old', axis=1, inplace=True) $ df2.head()
top_10_authors = git_log.loc[:, 'author'].dropna().value_counts().head(10) $ top_10_authors
df_more[df_more.Engineer != 0]
bruins_pregame.to_csv('../../../../../CS 171/cs171-gameday/data/bruins_pregame.csv',   index=False) $ celtics_pregame.to_csv('../../../../../CS 171/cs171-gameday/data/celtics_pregame.csv', index=False) $ sox_pregame.to_csv('../../../../../CS 171/cs171-gameday/data/sox_pregame.csv',         index=False)
archive_clean['rating_denominator'].value_counts()
workspaces = pd.DataFrame.from_dict(workspaces_list)
im.head()
list(df_json_tweets.columns.values)
tweet_archive_enhanced_clean.head()
mask = (youthUser3["creationDate"] > '2015-01-01') & (youthUser3["creationDate"]<= '2015-12-31') $ youthUser2015 = (youthUser3.loc[mask]) $ youthUser2015.head()
p = sns.factorplot('yearOfRegistration',data=autodf,kind='count') $ p.set_xticklabels(rotation=90)
sns.heatmap(prec)
date_of_birth.hist() $ plt.title('The birth year of users')
df.index = pd.to_datetime(df.index) $ df
twitter_df_clean.loc[191]
LARGE_GRID.display_fixations(raw_large_grid_df, option='offset',input_subject="VP1",input_block=1)
X_test.columns
ip = ip_clean.copy()
c = conn.cursor() $ c.execute("INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)") $ conn.commit() $ conn.close()
new = pd.get_dummies(data = new , columns=['country'])
r = q_pathdep_obs.results() $ r
df2['intercept']=1 $ df2['ab_page'] = df2['group'] == 'treatment' $ df2['ab_page'] = df2['ab_page'].astype(int)
for _item in sample(vectorizer.stop_words_,100): $     print(_item,end='\t')
from patsy import dmatrices $ from statsmodels.stats.outliers_influence import variance_inflation_factor $ y, X = dmatrices('converted ~ intercept + ab_page + CA + UK', df2, return_type='dataframe')
from scripts.processing import *
tweet_df.head() $
pd.set_option('display.max_colwidth', -1) $ jimcramer_df = pd.DataFrame(jimcramer) $ jimcramer_df
result = r.json() $ result
rep = df2[df2.duplicated(['user_id'], keep=False)]['user_id'] $ print(rep) $ print("\n\n user_id repetido: "+str(rep.iloc[0]))
df.query('group == "treatment" & landing_page != "new_page"').count()
df[df['public']=='online'].count()[0]
fairshare['Is_Account'] = fairshare.ACCOUNT.str.contains('account') $ for i in range(4): $     fairshare.iloc[:,5-i] = fairshare.iloc[:,4-i] $ fairshare.loc[fairshare.Is_Account == False, 'MAXCPUS'] = np.NaN $ fairshare.MAXCPUS.fillna(method='ffill', inplace=True)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
df2.query('is_duplicated==True')
scoreCts = ins2016.groupby("score").count() $ plt.bar(scoreCts.index, scoreCts["business_id"].values) $ plt.show()
bnbAx_lang = pd.get_dummies(bnbAx.language, prefix='lang_') $ bnbAx_lang.shape
y_hat_test = model.predict(X_test)
p_conv_treat = df2.query('converted == "1" and group == "treatment"').user_id.nunique() / df2.query('group == "treatment"').user_id.nunique() $ p_conv_treat 
ab_df.nunique() $
data.info() # .info method will tell theh datatype and null values of each of the column in the dataset
nconvert = len(df2[(df2.converted==1) & (df2.group=='treatment')]) $ ntot = len(df2[(df2.group=='treatment')]) $ prob_2 = nconvert/ntot $ print(prob_2)
display('df1', 'df2', 'df1.append(df2)')
from bigml.api import BigML $ api = BigML()
from sklearn.metrics import jaccard_similarity_score $ jaccard_similarity_score(y_test, yhat)
print "# %s lensing events in OGLE catalog"%len(OGLE_ra_dec_data) $ print "#  Find min, max values of (ra, dec):" $ print "# %.4f < ra < %.4f  | %.4f < dec < %.4f"%( $     min(OGLE_ra_dec_data[:,0]), max(OGLE_ra_dec_data[:,0]), $     min(OGLE_ra_dec_data[:,1]), max(OGLE_ra_dec_data[:,1])) $
speeches_cleaned['index'] = speeches_cleaned.index
(events.loc["20170110"] $         .resample('H') # H - aggregation par heure $         .doc_type $
top_20_breed_stats['favorite_count'].sort_values(ascending=False).plot(kind='bar', subplots=True)
sentiments_pd.to_csv('analysis/social_analytics_GSu.csv', sep=',' )
X=users.drop(['object_id', 'creation_time', 'name', 'invited_by_user_id', 'last_session_creation_time'], axis = 1) $ X.head()
print(np.unravel_index(100,(6,7,8)))
frame2.loc['three']
girls_by_name.loc[['PRIYANKA', 'HARJIT', 'KEIKO', 'NATALYA'], :]
chinadata.plot?
import glob $ import pandas as pd $ l = [pd.read_csv(filename) for filename in glob.glob("data/ebola/guinea_data/*.csv")] $ guinea_data = pd.concat(l, join='outer')
facts_metrics.shape
season_groups.groups
inspector.get_table_names()
X_train.isna().sum(axis=0)
def datediffnorm(date,start_date,stop_date): $     datediff = date-start_date $     date_len = stop_date-start_date $     datediff_norm = datediff.total_seconds()/date_len.total_seconds() $     return datediff_norm
contractor_clean['address1'].value_counts()
df2[((df2['group'] == 'control') & (df2['converted'] == 1))].user_id.count() / df2.user_id.count()
forked.to_pickle('data/pickled/new_subset_forks.pkl')
df2 = pd.DataFrame([[2,3],[4,5]],index=['a','c']) $ df = df2.append(df2) $ print(df)
forecast_df['male_pop'] = 0 $ for ind, row in SANDAG_age_df[SANDAG_age_df['SEX'] == 'Male'].iterrows(): $     forecast_df.loc[ind, 'male_pop'] += row['POPULATION'] $ forecast_df.head()
sub_dataset['Popular'].value_counts(sort=True, normalize=True)
hm.head()
sns.heatmap(data=pivot, cmap='coolwarm')
mb = pd.read_csv("Data/microbiome.csv", index_col=['Taxon','Patient'])
help(pd.read_excel)
contractor_clean['address2'].value_counts()
y = X.iloc[:,-1] $ X = X.iloc[:,0:-1] $ col_names = X.columns $ print(X.head(5)) $ print("\n",y.head(5))
plt.pie(drivers_totals, explode=explode, autopct="%1.1f%%", labels=labels, colors=colors, shadow=True, startangle=140) $ plt.show()
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.dayofweek).count().plot(y="Borough")
metrics.v_measure_score(labels, km.labels_)
diff = [abs(list(r_close.values())[i] - list(r_close.values())[i-1]) for i in range (1,len(r_close))] $
large_df.tail(n=2)
sump = out_df[['high','medium','low']].sum(axis=1)
df3 = df2.copy()
newfile.shape
churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']] $ churn_df['churn'] = churn_df['churn'].astype('int') $ churn_df.head()
df.text[df.text.str.isupper()].unique()
rng = pd.date_range('1/1/2012', periods=100, freq='Min') $ ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) $ ts
tfidf_vect = TfidfVectorizer(analyzer = clean_text) $ X_tfidf = tfidf_vect.fit_transform(tweets_1['text']) $ Y_tfidf = tfidf_vect.transform(infinity['text'])
df_user_info['activated'].value_counts()
df2.head()
np.random.rand(3,4)
fullDF.full_text[0]
windfield = gdal.Open(input_folder+windfield_name, gdal.GA_ReadOnly) $ windfield
opportunities_not_lost = opportunities[opportunities['StageName'] != 'Closed - Lost'].groupby('Account ID')['Building ID'].count().reset_index()
ip_df.head(2)
disposition_df.head()
rng.to_period()
iris_dataset.keys()
remove_tickers = ['CSRA', 'DPS', 'UAA', 'DISCK', 'JUNO', 'XL', 'WELL', 'BKNG', 'SNI'] # example: two M&A targets, diff share classes... $ remove = list(set(remove_tickers).intersection(consol_px.columns.tolist())) $ if len(remove) > 0: $     consol_px.drop(remove, axis=1, inplace=True)
df.loc[[11, 24,37], :]
dtm_df = pandas.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index) $ dtm_df
fig = acc.plot_history()
print("Mean squared error: %.2f" $       % mean_squared_error(y_test, y_pred))
no_of_obs_by_station = engine.execute('SELECT station, count(*)  FROM measurement group by station order by count(*) desc').fetchall() $ no_of_obs_by_station
offseason08 = ALL[(ALL.index > '2008-02-03') & (ALL.index < '2008-09-04')] # We repeat the above process until present day.
driver.find_element_by_xpath('//*[@id="body"]/table[2]/tbody/tr/td/table[2]/tbody/tr[2]/td[1]/b/font/a').click()
temp = (X.describe().T ==len(X)) $ temp.columns = ['a','b','c','d','e','f','g','h'] $ col_with_na = temp.index[~temp.a]
pageviews.info()
print(2.0.is_integer()) $ print(2.5.as_integer_ratio())  # Convert to fraction tuple; we'll cover tuples later
students.weight.value_counts().sort_index()
T = 0 $ folder = 'trainW-'+str(T) $ user_logs = utils.read_multiple_csv('../../feature/{}/compressed_user_logs'.format(folder)) $
np.sum(jArr, 0)
import json # (dong) $ movieSaved = {feature: movie_df[feature].values.tolist() for feature in movie_df.columns.values} $ fp = open("allMovies_new.json","w") $ json.dump(movieSaved, fp) $ fp.close()
features, feature_names = ft.dfs(entityset=es, target_entity='clients', $                                  max_depth = 2)
df3_month.plot(x='Donation Received Month-Year', y='Total Donations', kind='line') $ df3_month.plot(x='Donation Received Month-Year', y='Mean Donation', kind='line') $ df3_month.plot(x='Donation Received Month-Year', y='Count of Donations', kind='line') $ df3_month.plot(x='Donation Received Month-Year', y='Count of Unique Donors', kind='line') $ plt.show() $
mean_sea_level = pd.DataFrame({"northern_hem": northern_sea_level["msl_ib(mm)"].values, $                                "southern_hem": southern_sea_level["msl_ib(mm)"].values}, $                                index = northern_sea_level.year) $ mean_sea_level
endpoint_instance = wml_credentials['url'] + "/v3/wml_instances/" + wml_credentials['instance_id'] $ header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} $ response_get_instance = requests.get(endpoint_instance, headers=header) $ print(response_get_instance) $ print(response_get_instance.text)
poly17= PolynomialFeatures(degree=17) $ X_17 = poly17.fit_transform(X) $ linear = linear_model.LinearRegression() $ linear.fit(X_17, y2) $ (linear.coef_, linear.intercept_)
df.iloc[1:3,:]
data.loc[data.surface_total_in_m2.notnull(), 'surface/price'] = data['surface_total_in_m2']/data['price_aprox_usd']
tweet_hour = tweet_data[(tweet_data['timestamp'] >= start_time) & $                         (tweet_data['timestamp'] <= start_time + pd.Timedelta(hours=delta_hours))].copy()
df2 = df2.drop([2893])
bacteria.name = 'counts' $ bacteria.index.name = 'phylum' $ bacteria
print(df.shape) $ df = df[pd.notnull(df['is_shift'])] $ print(df.shape)
engine.score_all_on_classifier_by_key("nhtsa_classifier")
baseball.reindex(id_range, fill_value='charliebrown', columns=['player']).head()
bod = boedoEnMeses.loc[boedoEnMeses['yearmonth'] == '2017-04', :].sort_values('price_usd_per_m2', ascending=False) $ bod["price_usd_per_m2"].describe()
params_file_path = os.path.join(data_dir, "params.tsv") $ text_classifier.export_params(params_file_path) $ params_file_path
data.plot(x='batch_start_time', y='relative_error', kind='line')
df_oldpage = df2.query('landing_page =="old_page"') $ x_oldpage = df_oldpage["user_id"].count() $ df_old_conv = df_oldpage.query('converted == "1"') $ x_old_conv = df_old_conv["user_id"].count()
def strip_date(a_datetime): $     return datetime.strptime(a_datetime.strftime('%Y-%m-%d'), '%Y-%m-%d') $ tweet_counts_by_month.index = tweet_counts_by_month.index.map(strip_date)
data = data.join(s, how='outer'); data  #Assignment to store the modified frame.
df_2001['bank_name'] = df_2001.bank_name.str.split(",").str[0] $
Lab7_RevenueEPS0 = pd.read_excel(r"C:\Users\Tanushree\Desktop\UNIVERSITY\Quarter 4\Dashboard\Lab7_Session\Nasdaq data.xlsx",sheetname = 0)
os.chdir(out_path) $ os.getcwd()
total_cost = df.groupby(['uid'])[['total_cost']].agg(['sum','count']) $ total_cost.columns = ['total','number'] $ total_cost = total_cost.dropna() $ total_cost = total_cost[total_cost['total']>0] $ total_cost = total_cost.sort_values(by='total') $
df_copy['timestamp']=pd.to_datetime(df_copy['timestamp'])
n_old = df2.query('group == "control"').shape[0] $ print(n_old)
df_countries['country'].value_counts()
print df.shape[0] + noloc_df.shape[0]
lr.fit(X_train, y_train)
fav_max = np.max(data['Likes']) $ rt_max = np.max(data['RTs']) $ fav = data[data.Likes == fav_max].index[0] $ rt = data[data.RTs == rt_max].index[0]
c.execute('SELECT city FROM weather where cold_month = "January"') $ print(c.fetchall())
np.histogram(noaa_data.loc["2018-05-29":"2018-05-29","AIR_TEMPERATURE"])
tweet_favourite.plot(figsize=(20,8), label="Likes", legend=True) $ tweet_retweet.plot(figsize=(20,8), label="Retweets", legend=True )
students.T
data.city.value_counts()
top_50_channels = df_link_yt['channel_id'].value_counts().head(50).index $ channel_id = top_50_channels[1]
df.groupby(['education'])['loan_status'].value_counts()
fire_size_file = '../data/model_data/size_mod.sav' $ pickle.dump(rf, open(fire_size_file, 'wb'))
week2_df.rename(columns = {' 31 July - 4 August 2017' : 'title'}, inplace=True) $ week2_df['date'] = '31 July - 4 August 2017' $ week2_df
least_retweeted = tweets.loc[tweets.snspostid.isin(grouped.iloc[-100:,:].parentPost.astype(np.str).values)] $ least_retweeted.head(2)
pred_test = np.exp(pred_test)
sub1.to_csv('sub_average_5models_nocv.csv',index=False)
cleaned_asf_people_human_df_saved = non_blocking_df_save_or_load( $     cleaned_asf_people_human_df, $     "{0}/human_data_cleaned/asf_people_cleaned".format(fs_prefix)) 
df.tail(20)
print data.shape $ sum(data.irlco)
print "Before Jimmy, the amount of touchdowns per game was %.2f" % (18/pre_number) $ print "After Jimmy, the amount of touchdowns per game was %.2f" % (13/post_number)
clf = decomposition.NMF(n_components=6, random_state=1)
final=(final[final.ra>315].sort_values(by='ra')).append((final[final.ra<195].sort_values(by='ra'))) $ print final.shape
test.head() $ min(test['date_account_created'])
date + np.arange(12)
daily_returns.plot(kind='scatter',x='SPY',y='GLD') $ plt.show()
df_likes = pd.merge(df_likes,df_authors,left_on='author_id',right_on='authorId') $ df_likes.drop('authorId',inplace=True,axis=1) $ df_likes.head()
df_custype = df[['customer_type_group', 'xseller']].groupby('customer_type_group').mean()
df2[df2['group']=="control"].count()[0]
len(collection_reference.distinct('user'))
logit_mod = sm.Logit(df3['converted'], df3[['intercept','ab_page']]) $ results=logit_mod.fit()
inactive_count_with_na = len(clean_users[clean_users['active']==0]) $ inactive_mailing = clean_users[clean_users['active']==0]['opted_in_to_mailing_list'].sum()/inactive_count_with_na
data[data.index.duplicated(keep=False)]
pd.Timestamp('1/7/2018')
indexed_price(price_mat.loc['2017-03':, 'Bitcoin'])
score_fs = score[(score["score"] < 70) & (score["score"] >= 50)] $ score_fs.shape[0]
actual_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ actual_diff
lr_parameters = [{'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], $                   'max_iter':[50, 75, 100, 125, 150, 200], $                   'class_weight':['balanced', None], $                   'C':[1, 0.5, 0.10, 0.05, 0.02, 0.01]}]
number_list = [1,1,2,2,2,2,3,3,4,4,5,5,5,5,5,5,6,7,8,8,8,8,9,9,9,9] $ count_dict = collections.defaultdict(int) $ for i in number_list: $     count_dict[i] += 1
!scrapy runspider src/ds_web_data_hello_scrapy.py
df_archive_clean.rating_numerator = df_archive_clean.rating_numerator * 10 / df_archive_clean.rating_denominator
from scipy.stats import norm $ norm.ppf(1-(0.05/2))  # z score for 95% level of confidence
new_seen_and_click = new_seen_and_click.drop_duplicates(subset=['article_id', 'project_id', 'user_id', 'user_type'], keep='first') $ print(new_seen_and_click.shape) $ new_seen_and_click.head() 
distinct_user_ids = df2.nunique()['user_id'] $ print("Unique users are : {}".format(distinct_user_ids))
dblight = pd.read_csv(dic_inp["detroit-blight-violations.csv"],dtype=str) $ col_fees = ['AdminFee','LateFee','StateFee','CleanUpCost','JudgmentAmt'] $ col_sel = ["ViolationStreetNumber","ViolationStreetName","PaymentStatus"] $ dblight = dblight.drop_duplicates(subset="TicketID",keep='last')[col_sel+col_fees] $ dblight.head()
df['date_created'] = pd.to_datetime(df['created_at']) $ df['date_updated'] = pd.to_datetime(df['updated_at'])
dropoff_kmeans = KMeans(n_clusters=num_dropoff_clusters, random_state=1).fit(dropoff_coords)
contribs.info()
df.head()
data.loc[:, ['comments', 'review_comments', 'merge_duration']].describe()
predictions_final=predictions_train.filter('score>0')
festivals.at[2,'longitude'] = -87.7035662
ims = label(predicted_image>150) $ _ = plt.imshow(ims)
from ssbio.pipeline.gempro import GEMPRO
print("Probability of control group:", $       df2[df2['group']=='treatment']['converted'].mean())
us_grid = np.array(np.meshgrid(grid_lon, grid_lat)).reshape(2, -1).T $ np.shape(us_grid)
plt.plot(aapl)  # plots all col at once $ plt.show()
origin.head(10)
people = pd.read_csv("../01-data/raw/people.csv", sep=';')
Aerospace_an = aerospace_df.resample('BA', how=('sum'))  
import datetime $ import pandas_datareader as data $ start = datetime.datetime(2015,7,1) $ end = datetime.datetime(2016,6,1) $ solar_df = data.DataReader(['FSLR', 'TAN','RGSE','SCTY'],'google', start=start,end=end)['Close']
segmented_rfm[segmented_rfm['RFMScore']=='344'].sort_values('Monetary_value', ascending=False).head(10)
import os $ import sys $ module_path = os.path.abspath(os.path.join('..')) $ if module_path not in sys.path: $     sys.path.append(module_path)
localized.tz_convert('UTC')
ben_fin.head()
df.index    
df.loc['1930-01-01':'1979-12-31','status'] = "Before FL" $ df.loc['1984-01-01':'2017-12-31','status'] = "After FL" $ df.sample(10)
session_top_subset = session[session['action'].isin(top_actions)] $ session_top_subset.head()
obs_diff = df_treatment['converted'].mean()- df_control['converted'].mean() $ obs_diff
set_themes.rename(columns = {'name_x': 'set_name', 'name_y': 'theme_name'}, inplace = True)
df.columns
new.head()
committees.prop_name.value_counts()
from sklearn.metrics import classification_report $ target_names = ["Class {}".format(i) for i in range(num_classes)] $ print(classification_report(y_true, predicted_classes, target_names=target_names))
data.head(3)
df2['ab_page']=pd.get_dummies(df['group'])['treatment'] $ df2['intercept']=1 $ df2.head()
print(dt.date(2012,12,21) + dt.timedelta(30,12,0))
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(therapist_duration.diff()[1:], params=params, lags=30, alpha=0.05, \ $     title='Weekly Therapists Hours First Difference Partial Autocorrelation')
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new]) $ print(new_page_converted)
twitter_archive_df[twitter_archive_df['tweet_id'].duplicated()]
deut5 = dta.t[(dta.b==5) & (dta.c==5)]
people.insert(1, "height", [172, 181, 185]) $ people
list(filter(lambda x: x.endswith('Date'), geocoded_df.columns))
df2[df2.duplicated('user_id')]
results_jar_rootDistExp, output_jar_rootDistExp = S.execute(run_suffix="jar_rootDistExp", run_option = 'local')
NB_results = pd.DataFrame({'tweet': sentiment_test_tweets,'sentiment':test_predict})
predicted = model2.predict(X_test) $ print predicted
from sklearn.preprocessing import Binarizer, \ $     OneHotEncoder, PolynomialFeatures, StandardScaler, \ $     MinMaxScaler, RobustScaler
year_prcp_df = year_prcp_df.sort_values("date") $ year_prcp_df.head(10)
hs.to_sql("stations", conn, index=False, if_exists='replace')
df.location_id.head()
autos.price.value_counts().sort_index(ascending=False).head(20)
def parse_full_date(local_date): $     return local_date.strftime('%m-%d-%Y')
counts.values
watch_table.to_sql(name='watch_events', con=con, if_exists='append', index=False)
experience.columns = ['RATE_'+str(col) for col in experience.columns]
feature_layer.properties.extent
weather_df["weather_main"] = weather_df.weather_main.str.replace("haze", "fog") $ weather_df["weather_description"] = weather_df.weather_description.str.replace("haze", "fog") $ weather_df["weather_main"] = weather_df.weather_main.str.replace("mist", "fog") $ weather_df["weather_description"] = weather_df.weather_description.str.replace("mist", "fog")
treaties = db.treaties $ treaty_id = treaties.insert_one(treaty).inserted_id $ treaty_id
to_be_predicted_Day2 = 25.06214181 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
import pandas as pd $ file_name = "data/survey_Sorsogon_Electricity_Water_wbias_projected_dynamic_resampled_1000_{}.csv" $ sample_survey = pd.read_csv(file_name.format(2010), index_col=0) $ sample_survey.head()
data.head()
df.loc[df['lead_mgr'].str.contains('ViewTrade'), 'lead_mgr'] = 'viewtrade securities'
df2['intercept'] = 1 $ df2[['alt', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('alt', axis=1)
mileage = pd.Series(mileage)
sum(df['converted'].values)/row_num
dt_predicted = tree.predict(X_final)
logit = sm.Logit(df3['converted'],df3[['intercept' ,'treatment']]) $ result = logit.fit()
prec_fine = np.zeros((844, 26, 59)) $ for i in range(844): $     prec_mon = prec_us_full[i] $     interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, prec_mon) $     prec_fine[i] = interp_spline(grid_lat, grid_lon)
model = ARIMA(AAPL_array, (2,2,1)).fit() $
address_indexes_arr = np.asarray(address_indexes) $ address_indexes_arr[:10]
tweet_df["tweet_date"] = pd.to_datetime(tweet_df["tweet_date"]) $ tweet_df.sort_values("tweet_date", inplace=True) $ tweet_df.reset_index(drop=True, inplace=True) $ tweet_df.head()
user = np.unique([a.strip() for a in U_B_df.loc[0,'cameras']]) $
to_be_predicted_Day2 = 21.30837267 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
total_fare=pd.DataFrame(city_fare) $ totol_fare=total_fare.reset_index() $ totol_fare $
from nltk.stem.porter import PorterStemmer $ stemmed = [PorterStemmer().stem(w) for w in words] $ print(stemmed)
print model.predict(750000)
autos['num_doors'].unique()
dict_photo = json.loads(df.loc[0,'photo']) $ pprint(dict_photo)
df_cprc = df_uro_dd_dummies[ls_cprc_columns] $ df_cprc.head()
chk = joined.loc[joined['city_hol']==1]
df_input.filter("`Park Borough` == '225841'").show()
len(trading_exemption_records.company_number.unique())
gram_collection.find_one({"account": "deluxetattoochicago"})['link_to_post'] $ gram_collection.find_one({"account": "deluxetattoochicago"})
adj_close = all_data[['Adj Close']].reset_index() $ adj_close.head()
df_TempIrregular.to_csv('data/All_Irregularities_20180601_to20180607.csv', sep=',')
print len(cbg['AFFGEOID'].unique()) $ print len(noise['AFFGEOID'].unique()) $ print len(graf_counts['AFFGEOID'].unique()) $ print len(noise_graf['AFFGEOID'].unique())
database='FSE' $ dataset='AFX_X' $ start_date='2017-05-01' $ end_date = '2017-05-01' $ url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY+'&start_date='+start_date+'&end_date='+end_date $
fig, axarr = plt.subplots( 1, 1, figsize=(20,10) ) $ plt.hist(editors.control["proportion_reverted"], alpha = 0.5, range = (0, 1), bins = 20, label = "Source editor only") $ plt.hist(editors.treatment["proportion_reverted"], alpha = 0.5, range = (0, 1), bins = 20, label = "Both editors") $ plt.legend(loc="upper right") $ plt.show()
jobPostDFSample['JobDescription'].fillna("TBD", inplace=True)
url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza' $ r = requests.get(url) $ json_data = r.json() $ pizza_extract = json_data['query']['pages']['24768']['extract'] $ print(pizza_extract) $
twitter_df_clean.date = pd.to_datetime(twitter_df_clean.date) $ twitter_df_clean.time = pd.to_datetime(twitter_df_clean.time,format= '%H:%M:%S' ).dt.time
df2 = df[df['year']==2017] $ df2.head()
hits_df = hits_df.interpolate(method='time')
site=requests.get(url)
flight6.groupBy(col('trip'), col('stay_days'), col('lead_time')). \ $     agg(F.mean('price_will_drop_num')).orderBy(['trip', 'stay_days', 'lead_time'], ascending=[1, 1, 0]).show(1000) $
r.loc['20170703', ['optimized_weight', 'weight']]
corpus = [dictionary.doc2bow(text) for text in texts] $ corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)  # store to disk, for later use $ for c in corpus: $     print(c)
oppose_NNN.amount.sum()
image_clean['p1'] = image_clean['p1'].str.replace('_', ' ') $ image_clean['p2'] = image_clean['p2'].str.replace('_', ' ') $ image_clean['p3'] = image_clean['p3'].str.replace('_', ' ')
data.head(11).to_excel("data_10.xls")
df['num_payouts'].
df_merged['country'].value_counts()
svc.predict(X)
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2015 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2015 Percentiles.txt"
ax = grouped.sum().plot(kind = 'bar') $ ax.set_ylabel('total likes')
pct_play_well_cats = round(pets_pet['plays_cats'].sum() / len(pets_pet) * 100., 2) $ print("The percentage of pets that play well with cats is: " + str(pct_play_well_cats) + "%.")
q = "SELECT * FROM tir.tir WHERE source='SGCN' AND scientificname='Anodontoides ferussacianus'" $ r = requests.get("https://gc2.datadistillery.org/api/v1/sql/bcb?q="+q).json() $ for feature in r["features"]: $     display (feature["properties"]["registration"]) $     display (feature["properties"]["sgcn"])
new_df['created_time'] = pd.to_datetime(new_df['created_time']) $ new_df.head()
interact(sdof_resp, x0 = (0,2,0.1), v0 = (0,2,.1), m = (0,2,0.1), k = (0,100,1), c = (-1,5,.1));
scaler = preprocessing.StandardScaler() $ x_train_scaled = scaler.fit_transform(x_train) $ x_test_scaled = scaler.transform(x_test)
pp.barplot(df=df, filters={'technology': tecs, 'variable': ['CAP']}, $            title='CAP - light')
gc.collect()
cbg.isnull().sum()
'this is {type} number {order}'.format( type="string", order="1")
df.shape $ df.tail()
pd.DataFrame ([{'a': 1}, {'a': 5, 'b': 10}], index = ['Y', 'Z'])
df[(df.country == "es") & (df.city == "Valencia")].id.size
test_cleaned = test.replace(re.compile(r"http.?://[^\s]+[\s]?")) $ test_cleaned = test_cleaned.replace(re.compile(r"@[^\s]+[\s]?")) $ test_cleaned = test_cleaned.replace(re.compile(r"\s?[0-9]+\.?[0-9]*"))
final_data["clean_titles_cat"] = final_data["clean_titles"].astype('category').cat.codes $ final_data.head()
van15_fin['FirstMeta'] = van_final.groupby('userid')['pagetitle'].first().str.contains('/')
condition = (us['cityOrState'].isin(states) == False) & (us['cityOrState'] != 'Puerto Rico') $ us.loc[condition, 'cityOrState'] = np.nan
sinclair_stations = df[df.owner == 'Sinclair'].domain.unique() $ sinclair_stations
pp.ProfileReport(test)
max_activity = indexed_activity.station_count.max()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').converted.sum() $ convert_new = df2.query('landing_page == "new_page"').converted.sum() $ n_old = df2.query('landing_page == "old_page"').landing_page.count() $ n_new = df2.query('landing_page == "new_page"').landing_page.count() $
shelter_pd = shelter_pd[['Name', 'DateTime', 'OutcomeType', 'AnimalType', 'SexuponOutcome', 'AgeuponOutcome', 'Breed', 'Color']].astype(str) $ shelter_df = sqlContext.createDataFrame(shelter_pd) $ pipeline = Pipeline(stages=[Name_transformer, DateTime_DayExtractor, DateTime_MonthExtractor, DateTime_YearExtractor, SexuponOutcome_transformer]) $ model = pipeline.fit(shelter_df) $ shelter_cleaned_df = model.transform(shelter_df)
nnew = df2.query('landing_page == "new_page"').landing_page.count() $ nnew
token_sendReceiveAvg_month = token_sendReceiveAvg_month.apply(lambda x:round(x,2))
(autos['date_crawled'] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
age = pd.cut(titanic['age'], [0, 18, 80]) $ titanic.pivot_table('survived', ['sex', age], 'class')
ad_group_performance['Impressions'].min()
user_df = stories['submitter_user'].apply(pd.Series)
samples_query.display_records(50)
p_old=df2['converted'].sum()/len(df2) $ p_old
(diffs_evening>obser_eve).mean()
mask = y_test.index $ t_flag = y_test == 0 $ p_flag = pred == 1
df_r.head()
p_new = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ print('p new convert rate: ', p_new)
trips.trip_duration.hist() $ plt.xlabel('Trip Duration in Seconds') $ plt.ylabel('Number of Trips') $ plt.suptitle('Trip Duration Distribution') $ plt.show()
validation_features = bind_features(validation, train_test="train").cache() $ validation_features.count()
for row in session.query(User, User.name).all(): $ ...    print(row.User, row.name)
autos['brand'].value_counts()
import statsmodels.api as sm $ convert_old = 17489 $ convert_new = 17264 $ n_old = 145274 $ n_new = 145310
plt.plot(range(len(loss_history)), loss_history, 'o', label='Logistic Regression Training phase') $ plt.ylabel('cost') $ plt.xlabel('epoch') $ plt.legend() $ plt.show()
client.repository.list_definitions()
(series + pandas.Series({'a': 2, 'c': 2})).dropna()
1/np.exp(results2.params)
y_class = demo.get_class(y_pred) $ cm(y_test,y_class,['0','1'])
df2[['ab_page','t']] = pd.get_dummies(df['landing_page']) $ df2.head() $
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?' $                  + 'start_date=2017-01-01&end_date=2017-12-31' $                  + '&api_key=' + API_KEY) $
from tatk.operationalization.csi.csi_web_service import CsiWebService $ url = "<please type the service URL here>" $ key = "<please type the service Key here>" $ web_service = CsiWebService(url, key)
idx = df_providers[ (df_providers['year']==2015) & \ $                   (df_providers['drg3']==987)  ].index.tolist() $ len(idx) $
cars.info()
file_name = '2018-07-09-2015-870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS' $ print('run txt2pdf.py -o ' +"'" +file_name + '.pdf'  +"'  '" +file_name + '.txt'  +"'" )
df_to=df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_cn=df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ df_to.shape[0]+df_cn.shape[0]
b_list.columns
data.dtypes
print('Top score:\n', classifiers_comparison['best_params'].iloc[7]) $ print('Second best:\n', classifiers_comparison['best_params'].iloc[8])
clf = MultinomialNB() $ clf.fit(train_features.toarray(), authors_train)
baseball.hr - baseball.hr.max()
AAPL.mean()
tweet_image_clean.head()
non_grad_days_mean = records3[records3['Graduated'] == 'No']['Days_missed'].mean() $ non_grad_days_mean
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country']) $ lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'UK']]) $ result = lm.fit() $ result.summary()
stock_subset = stock_data[ [ 'open', 'close' ] ] $ print(display(stock_subset.head(5)))
df3.groupby('G').sum()
df[df.tstamp > '2018-01-07']
gscv.best_estimator_.fit(x_train, y_train)
df.sentiment.value_counts()
im.statistics_
taxi_hourly_df.loc[taxi_hourly_df["missing_dt"] == True, :].shape
girls_by_name.loc['FATIMA', :]
ab_page = np.exp(-0.0155) $ intercept = np.exp(-1.9879) $ (ab_page, intercept,)
fundret.plot() $ plt.title('Fund') $ plt.ylabel('Monthly return \n not annualized') $ plt.show()
table_rows = driver.find_elements_by_tag_name("tbody")[18].find_elements_by_tag_name("tr") $
price_ceiling_to_ignore = 100 $ price_floor_to_ignore = 10 $ meal_ids_to_ignore_because_price_outlier = X[(X.ticket_price < price_floor_to_ignore) | (X.ticket_price > price_ceiling_to_ignore)].meal_id.unique() $ X = X[~X['meal_id'].isin(meal_ids_to_ignore_because_price_outlier)]
glm_multi_v2.hit_ratio_table(valid=True)
logit = sm.Logit(df2['converted'], df2[['intercept','ab_page']])
pd.value_counts(appointments[appointments['Specialty'] == 'therapist']['Provider'])
reason_for_visit = pd.read_csv('./data/MeetingReasonForVisits.csv')
rows_in_table = len(ins) $ unique_ins_ids = len(ins)- sum(ins["business_id"].duplicated()) $
data.printSchema()
n_new = df2[df2.group == 'treatment'].shape[0] $ n_new
response = requests.get( $   'https://panel.sendcloud.sc/api/v2/parcels/statuses', $     auth=('key', 'secret_key'))
subwaydf['DESC'].value_counts()
test= test.reset_index(drop = True) $ test['rooms'] = pd.Series(predictions) $ datatest = pd.concat([train, test]) $ datatest = datatest.reset_index(drop = True)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') $ print(z_score, p_value)
donors_c[donors_c['Donor Zip'] == 606 ]['Donor State'].value_counts()
best_rf.cross_validation_metrics_summary
cities[cities.isnull()] = 0 $ print(cities)
consumerKey = 'XXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
len_means = len_bins.mean() $ print(len_means) $ len_std = len_bins.std() $ print(len_std)
plt.close('all')
nuscatter = xs_library[moderator_cell.id]['nu-scatter'] $ df = nuscatter.get_pandas_dataframe(xs_type='micro') $ df.head(10)
imgp = pd.read_csv('image-predictions.tsv', sep = '\t') $ imgp.head() $
day = calendar.weekday(2015, calendar.February, 28) $ day = calendar.day_name[day].encode('latin1').decode('cp1251') $
df2.sort_values(['user_id'], inplace = True) $ df2.reset_index(drop = True, inplace = True) $ df_merge = df2.join(df_countries, rsuffix = '_country') $ df_merge.drop(['user_id_country'],  axis = 1, inplace = True) $ df_merge.head()
dr_new_hours.index
data = pd.merge( $     pd.merge(setup[['SetId', 'Street', 'station']], tubes[['SetId', 'mean']], on='SetId'), $     pd.DataFrame(dict(VMM=VMMmeans)), left_on='station', right_index=True) $ data
submit.to_csv("properati_dataset_sample_submision.csv", index = False)
df2_control = df2.query('group == "control"') $ df2_control.query('converted == 1').user_id.nunique() / len(df2_control)
df_new.head(3)
df_comment.info()
df_user_engagement.head()
frame3.index.name = 'year'; frame3.columns.name = 'state'
args = mfclient.XmlStringWriter("args") $ args.add("where", "namespace=/projects/proj-hoffmann_data-1128.4.49/data") $ args.add("action", "get-path") $ args.add("size", "infinity") $ data_query = con.execute("asset.query", args.doc_text())
analyze_set.loc[analyze_set['favorites']==80]
def train_lda(dictionary,posts_words_vectors,number_of_topics): $     doc_term_matrix = [dictionary.doc2bow(doc) for doc in tqdm(posts_words_vectors)] $     return gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary, passes=10)
tw = pd.read_csv("twitter-archive-enhanced.csv") $ tw.info()
!head -20 sample_data.json
df_gamma = df[df.thermal==0] $ df_neutron = df[df.thermal==1] $ print("gamma_size: ", len(df_gamma.index)) $ print("neutron_size: ", len(df_neutron.index))
df_2011.dropna(inplace=True) $ df_2011
autos.head(1)
autos['registration_year'].hist()
outfile = "to_excel_test.xlsx" $ excel_writer = pd.ExcelWriter(outfile) $ returned_orders_data.to_excel(excel_writer, "Completeness Testing", index = False)
p_new = df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique() $ p_new
tweets = pd.DataFrame(res_dict['statuses']) $ print(tweets.shape) $ tweets.head()
pd.period_range('2015-07', periods=8, freq='M')
data.tail(-2)
Lab7_Redesign = pd.merge(Lab7, Stock_price, left_on='Ticker_Symbol',right_on='Symbol', how='right')
df_census.loc[:, 'ward'] = df_census.ward.astype(int)
access_token = response.json()['access_token'] $ print(access_token)
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG =(grouped.aggregate(np.sum)['disc_times_pay']) $ grouped_by_year_DRG.head()
mis_match = [] $ for i in range(len(tweet_json)): $     if (tweet_json['id'][i]) not in list(twitter_archive['tweet_id']): $         mis_match.append(tweet_json['id'][i]) $ print(len(mis_match))
df_clean[['rating_numerator', 'rating_denominator']].info()
train['discussion'] = train.url.isnull().astype(int) $ train.groupby('discussion').popular.mean()
counts_compare = counts_compare.drop(['source', 'Team1'], axis=1)
mmx = MinMaxScaler() $ %time train_4_reduced = mmx.fit_transform(train_4_reduced)
def time_difference_feature(df): $     diff = pd.to_datetime(df['date_created']) - pd.to_datetime(df['user_date_created']) $     result = diff.apply(lambda x: x.seconds // 60) $     return result
x.shape
autos.hist(column = 'odometer_km')
df.sort_values(by='C')
df3['country'].value_counts()
print(sample_df.shape) $ sample_df.head()
def tokenizer(text): $     return text.split() $ tokenizer('runners like running and thus they run')
f_counts_week_os = spark.read.csv(os.path.join(mungepath, "f_counts_week_os"), header=True) $ print('Found %d observations.' %f_counts_week_os.count())
leavers = users[users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))] $ leavers.groupby(['DaysActive'])['CreationDate'].count().sort_values(ascending = False).iloc[0:10]
pnew=df2['converted'].mean() $ print(pnew)
xirr_actual[('rating_switch', 'rating_base')].head()
bufferdf.Fare_amount.mean().ix[[2,3,4]]
n_net.score(x_test,y_test > 0)
df_CLEAN1A.head()
g = sns.countplot(x = "topic", data = news_df, $               order = (news_df['topic']. $                        value_counts().index)) $ g.set_xticklabels(g.get_xticklabels(),rotation=30)
r.text
standalone_series = df['instagrams'] != 10.0 $ standalone_series
austin[['driver_rating', 'rider_rating', 'charity_id', 'free_credit_used', 'round_up_amount','promocode_redemption_id','tipped_on', 'tip']]= austin[['driver_rating', 'rider_rating', 'charity_id', 'free_credit_used', 'round_up_amount', 'promocode_redemption_id','tipped_on', 'tip']].fillna(value=0)
national_holidays=actual_holidays[actual_holidays.locale=='National'] $ print("Rows and columns:",national_holidays.shape) $ pd.DataFrame.head(national_holidays)
! ls ./data/
df2.drop_duplicates(subset='user_id', keep='last', inplace=True) $ counts = df2.user_id.value_counts() $ len(counts[counts > 1])
Imagenes_data.info()
print(np.percentile(p_diffs, 2.5),np.percentile(p_diffs,98.5)) $ print(np.percentile(p_diffs, 95)) $ print(np.percentile(p_diffs, 5)) $ print(np.percentile(p_diffs, 90)) # <-- results in 0.0015 which is about the same as calculated below $
print('Reply tweets: ' + str(len(tweets_clean.query('in_reply_to_status_id == in_reply_to_status_id')))) $ print('Total tweets: ' + str(len(tweets_clean)))
pd.Period('2011-01')
df.head(2)
id_pickup_label=pickup_kmeans.labels_ $ id_dropoff_label=dropoff_kmeans.labels_ $ id_ride_label=ride_kmeans.labels_
df.groupby("cancelled")[["first_rental", "new_customer", "repeat_customer"]].mean()
doctors.index
nba_df.columns
ws = Workspace.from_config() $ print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\n')
store_items.insert(4, 'shoes', [8,5,0]) $ store_items
%matplotlib inline $ import matplotlib.pyplot as plt $ plt.figure() $ df_q.plot(x = 'UNIQUE_DATE')
submit = perf_test[['ID_CPTE']] $ submit['Default'] = predictions $ submit.to_csv('random_forest_baseline.csv', index = False)
age_new = pd.cut(age, [0,10,20,30,40,50,60,70,80]) #discrete decade intervals $ age_new.value_counts().sort_index().plot(kind='bar') $ age_new.value_counts()
confuse = pd.crosstab(nntest['failure'], nntest['predicted_failure']) $ confuse
plt.scatter(predictions15, y, s=30, c='b', marker='+', zorder=10) $ plt.xlabel("Predicted Values from Q1") $ plt.ylabel("Actual Values 2015") $ plt.show() $ print "MSE:", model.mse_model
plt.scatter(ort_avg16, drt_avg16);
Historical_Raw_Data.set_index("Date_Monday")["2017"].groupby([pd.TimeGrouper('M'),"Product_Motor"]).sum().unstack(level = 0).fillna(0)['Qty']
bike_events = pd.DatetimeIndex(['2010-05-21', '2011-05-20', '2012-05-18', '2013-05-17', '2014-05-16', '2015-05-15', '2016-05-20', '2017-05-19']) $ bike_events
df_group_by2[categorical_features] = df_group_by2[categorical_features].apply(preprocessing.LabelEncoder().fit_transform)
blame.to_csv("C:/Temp/linux_blame.gz", encoding='utf-8', compression='gzip', index=None)
df.convtd.min(), df.convtd.max()
app_install = df_nona.groupby('create_date').apply(lambda v: sum(v.accounts_provisioned)/float(sum(v.district_size))) $ app_install.resample('w', np.mean).plot(title='Install Rate by Adopted Time') $
1 - 0.5000619442226688
trace1 = go.Bar(x=months, y=plotly_df.groupby('month').sum()['total_downloads'], name='Plotly') $ trace2 = go.Bar(x=months, y=vincent_df.groupby('month').sum()['total_downloads'], name='Vincent') $ trace3 = go.Bar(x=months, y=bokeh_df.groupby('month').sum()['total_downloads'], name='Bokeh') $ trace4 = go.Bar(x=months, y=mpld3_df.groupby('month').sum()['total_downloads'], name='MPLD3')
user_summary_df[user_summary_df.gender == 'M'][['followers_count', 'following_count', 'tweet_count', 'original', 'quote', 'reply', 'retweet', 'tweets_in_dataset']].describe()
classification_data = classification_data[cols].copy()
zero_labels.extend(final_labels)
test = Plot['iLayerLiqFluxSoil'].data $ dates = Plot['time'].data $ test = np.squeeze(test) $ df = pd.DataFrame(data = test, index=dates) $ df.replace(to_replace=-9999.0, value = 0, inplace=True)
index_to_remove = not_linedup.index
weekdays_avg.fillna('No Data',inplace=True) $ weekdays_count.fillna('No Data',inplace=True) $ weekends_avg.fillna('No Data',inplace=True) $ weekends_count.fillna('No Data',inplace=True)
frame2['eastern']=frame2.state=='Ohio' $ frame2
df_archive_clean.text[1]
df1.head()
token_sendavg = token_sendcnt.groupby("sender").agg({"sendcount":mean_except_outlier}).reset_index()
items = pd.DataFrame.from_records(breadcrumbs_xmlrpc.values()) $ items.postid = items.postid.astype(int) $ items = items.set_index('postid') $ print(items.dtypes) $ items[['title', 'dateCreated']].sort_values('dateCreated').head()
from dask.diagnostics import Profiler, ResourceProfiler $ with Profiler() as prof, ResourceProfiler(dt=0.05) as rprof: $      out = d.compute()
X = aux[['intercept', 'number_of_days', 'ab_page', 'UK', 'US']] $ y = aux['converted'] $ lm = sm.Logit(y,X) $ results = lm.fit() $ results.summary()
S_lumpedTopmodel.decision_obj.thCondSoil.options, S_lumpedTopmodel.decision_obj.thCondSoil.value
baseball.player.describe()
df2.drop_duplicates(subset=['user_id'], inplace=True) $ print(df2[df2.duplicated(['user_id'])])
idx = np.tril(tmp_cov).astype(bool) $ idx
df_elect.info()
train.age.isnull().sum()
logit_multi_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_test','UK','US']]) $ results = logit_multi_mod.fit() $ results.summary()
train['date'] = pd.to_datetime(train['date']) $ test['date'] = pd.to_datetime(test['date'])
pop = pd.read_csv('data/state-population.csv') $ areas = pd.read_csv('data/state-areas.csv') $ abbrevs = pd.read_csv('data/state-abbrevs.csv') $ display('pop.head()', 'areas.head()', 'abbrevs.head()')
print(round(9876.54321, 2))  # round to 2 decimal places $ print(round(9876.54321, -2))  # round to nearest 100 (10^2)
top_10_authors = git_log.author.value_counts(dropna=True).head(10) $ top_10_authors
et_helper.plot_around_event(etsamples_grid,etmsgs_grid,etevents_grid,raw_large_grid_df.query("eyetracker=='el'&subject=='VP4'&block==1").iloc[9],plusminus=(-2,5))
df2=df2.drop('group_receive_1',axis=1) $ df2.head()
plt.hist(x)
pd.Timestamp('1st of January 2018')
liquor = liquor.dropna()
rh = ['real', 'estate'] $ real_estate = wk_output[wk_output.explain.str.contains('|'.join(rh))] $ real_estate.shape $ real_estate.to_csv('real_estate_feedback.csv')
tips.sample(5)
tweets_clean.describe()
learner.sched.plot_loss()
yc.shape
daily_averages['Weekday'] = daily_averages['Weekday'].apply(lambda x: calendar.day_abbr[x])
tweet_archive_enhanced_clean = tweet_archive_enhanced_clean.drop('id',axis =1)
df.drop(['OBJECTID', 'QUALIFIED', 'USECODE', 'LANDAREA', 'GIS_LAST_MOD_DTTM'], axis=1, inplace=True)
cohort_retention_df = cohort_retention_df.append(cohort_retention_sum)
display(teamAttr.iloc[:3,:13]) $ display(teamAttr.iloc[:3,13:25])
df_data.head()
print("Variables not in test but in train : ", set(train_data.columns).difference(set(test_data.columns)))
print(All_tweet_data_v2.rating_numerator.median(),All_tweet_data_v2.rating_numerator.mean())
S.decision_obj.simulStart.value = "2007-07-01 00:00" $ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
df2.query("converted==1").shape[0]/df2.shape[0]
sb.regplot(y_train, y_pred, fit_reg=False) $ plt.xlabel('y_train') $ plt.ylabel('y_pred')
null_vals=np.random.normal(0,np.std(p_diffs),10000) $ plt.hist(null_vals) $ plt.axvline(-0.0015, c='red') $
config.addDataEntry("nhtsa_cmpl", data_config)
result_liberia[['last_v_T']].sub(result_liberia['First_v_T'], axis=0) $ result_liberia['Mean deaths'] = result_liberia['last_v_T']/result_liberia['count_v_T'] $ result_liberia
munich.info()
%%time $ dask_df.token_body.loc[125:155].compute()
list(pgn2value.keys())[:3]
crimes.head(2)
plt.figure(figsize=(8, 5)) $ sbr.heatmap(train_df[['is_company', 'is_weekend', 'title_len', 'content_len', 'favs_lognorm']].corr()); $ print(train_df[['is_company', 'is_weekend', 'title_len', 'content_len', 'favs_lognorm']].corr())
df_f = pd.merge(df_proc1, df_master1, how="left", on="CustID", indicator=True) $ df_f["BD_copy"] = df_f["BookingDate"] $ df_f1 = df_f.loc[:,["CustID","Store","BookingDate","BD_copy"]] $ df_f1["Counter"] = df_f1.groupby("CustID").cumcount(ascending=False) + 1 $ df_f1.rename(columns={"BookingDate": "LastPurchase", "BD_copy": "FirstPurchase"}, inplace=True)
archive_copy['name'] = archive_copy['name'].replace('None', np.NaN)
num_elements = 1000000  # 1 million $ x = np.random.uniform(low=1, high=5, size=num_elements).astype('float32')
logit_mod3 = sm.Logit(df2['converted'], df2[['intercept','new_page_uk','new_page_us']]) $ results3 = logit_mod3.fit() $ results3.summary()
tweet_image.info()
r_top10_mat = returns_calc(daily_price_mat[coins_top10]) $ corr_top10daily = r_top10_mat.loc[start_date:end_date].corr() $ corrplot(corr_top10daily, annot=True) $ plt.title('Correlation matrix - daily data \n from ' + start_date + ' to ' + end_date) $ plt.show()
logit_mod = sm.Logit(df_new["converted"], df_new[["intercept","CA", "UK", "treatment"]]) $ results = logit_mod.fit() $ results.summary()
stores = pd.read_csv("DataSets/stores.csv") $ stores = stores.assign(GTSales = stores.TotalSales * stores.Total_Customers,TotalExpenses = stores.OperatingCost + stores.AcqCostPercust) $ stores
c = np.max(datacounts['count']) $ duse = np.array(datacounts.loc[datacounts['count'] >= c,'dataid'])
model = ols("satisfied ~ age + income", training).fit() $ model.summary()
acs_df.to_csv('final_df.csv')
city_loc.append(city_pop)
likes.drop(columns='actor')
result = data.apply(pd.value_counts)
url = 'http://www.w3schools.com/xml/cd_catalog.xml' $ xml_page = urllib.request.urlopen(url).read() $ e = ET.XML(xml_page)
df2 = df[['MeanFlow_cfs','Confidence']] $ df2.head()
twitter_ar.head(2)
unsorted_Yahoo_data = pd.read_csv(Yahoo_data_file) $ unsorted_Yahoo_data.head()
shows['fixed_runtime'] = shows['runtime'].dropna().apply(fix_runtime)
y = np.array(x)
control = df2[df2['group'] == 'control'] $ control_converted = control.converted.sum() $ prob_control_conversion = control_converted / control.shape[0] $ print(prob_control_conversion)
raw_authors_by_project_and_commit_df.show() $ raw_authors_by_project_and_commit_df.take(1)
from fastai.structured import * $ from fastai.column_data import * $ np.set_printoptions(threshold=50, edgeitems=20) $ PATH='data/rossmann-store-sales-additional/'
print(df[0:3],'\n') $ print(df['20180102':'20180103'], '\n') $ x=np.array([0,1,2,3,4]) $ print('Numpy slice x[0:3]=', x[0:3])
data = data[data.astype(str)["Improvements_split"] != "[]"] $ print("Number of surveys: ", len(data))
print('Difference in Variance for MS: {}%  p-value: {}'.format( $     round(F_ms*100,2), stats.f.cdf(F_ms, degrees1ms, degrees2ms)))
auto.info()
usersDf.hist(column=['listed_count'],bins=50) $
for table in cur.fetchall(): $     print(table)
modifications_per_authors_over_time = modifications_over_time.reset_index().pivot_table( $     index=modifications_over_time['timestamp'], $     columns=modifications_over_time['author'], $     values='modifications_norm') $ modifications_per_authors_over_time.head()
df_control = df2.query('group == "control"') $ df_treatment = df2.query('group == "treatment"') $ obs_diff = df_treatment.converted.mean() - df_control.converted.mean() $ obs_diff
_ = df_pilots["created"].plot(kind="density")
twitter_archive_df.tail(3)
data_sets['3min'][data_sets['3min']['interpolated_values'].notnull()].tail()
contribs.committee_name.value_counts()
pred8 = nba_pred_modelv1.predict(g8) $ prob8 = nba_pred_modelv1.predict_proba(g8) $ print(pred8) $ print(prob8)
print("Probability of treatment group converting:", df2[df2['group']=='treatment']['converted'].mean()) $ print("Probability an individual recieved new page:", $       df2['landing_page'].value_counts()[0]/len(df2))
df.head()
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) $ from gensim import corpora
print(collections.Counter(y_resampled))
df_filtered_andora = df[df['Country'] =='Andorra'] $ df_filtered_andora = df_filtered_andora[['Indicator_Id','Country','Year','WHO Region','Publication Status']] $ df_filtered_andora = df_filtered_andora.sort_values(['Indicator_Id','Country','Year','WHO Region'],ascending=[False,False,True,False]).drop_duplicates(keep="first") $ df_filtered_andora =df_filtered_andora.reset_index(drop=True) $ df_filtered_andora.head(3)
Zn_post = model.predict_proba(X)[-1]
saveTweetDataToCSV(input_hash_tag.value)
preci_df.set_index('date').head() $
metrics.recall_score(y_valid, y_pred)
try: $     cur_a.execute('DELETE FROM hotel WHERE hotel_id=1') $ except Exception as e: $     print('Exception: ', e)
dflunchbreak = pd.merge(dfstationmeanlunch[['STATION','totals']], dfbrexits, on='STATION') $ dflunchbreak.columns = ['STATION','lunch_totals', 'breakfast_exits'] $ dflunchbreak.head(2)
logit = sm.Logit(data['Churn'], X_top) $ result = logit.fit()
data1.keys()
df_concat.columns #Prints a list of all column names.
capa2017.head()
print("Don't worry MisoMunje, there are") $ print("only {} authoritarian notable items".format(len(asdf))) $ print("in {} weeks.".format(max(asdf['wk'])))
support_NNN.amount.sum()
rmse=math.sqrt((lmscore.resid**2).mean()) $ print rmse
tweets_master_df.ix[774, 'expanded_urls']
notus.loc[notus['country'].isin(mexico), 'country'] = 'Mexico' $ notus.loc[notus['cityOrState'].isin(mexico), 'country'] = 'Mexico' $ notus.loc[notus['country'] == 'Mexico', 'cityOrState'].value_counts(dropna=False)
open('test_data//open_close_test.txt').close()
mpiv.head()
deltat.days
graffiti['AFFGEOID'] = graffiti['AFFGEOID'].astype(str)
imagelist.sort() $ imagelist = [ 'BARNES JEWISH HOSPITAL - DRG 1 2011-2015.pdf', $  'BARNES JEWISH HOSPITAL - DRG 3 2011-2015.pdf', $  'BARNES JEWISH HOSPITAL - DRG 69 2011-2015.pdf']
plt.hist(length)
daily_feature.head()
df.iloc[:5,[0,-1]]
replies = twitter_archive_clean[twitter_archive_clean['in_reply_to_status_id'].isnull()==False] $ twitter_archive_clean.drop(replies.index, inplace=True) $ twitter_archive_clean.reset_index(inplace=True, drop=True)
s.plot(kind='bar')
df_test_attempt_wth_caps_name = pd.merge(left=df_capsule_name,right=df_test_attempt,on="Capsule id",how ='inner') $ df_test_attempt_wth_caps_name
with open('tweet_json.txt', 'r') as content_file: $     content = content_file.read() $ records = json.loads(content) $ df_twitter = pd.DataFrame(records) $ df_twitter.head()
from pyspark.sql import SQLContext $ sqlContext = SQLContext(sc) $ data = sqlContext.read.json("samples/sample.json")
pickle_in = open('fifty_states.pickle','rb')  # open pickle file to get the bytes. rb = read bytes $ HPI_data = pickle.load(pickle_in)
y_ls.shape
df_transactions = pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/transactions.csv', parse_dates=['transaction_date','membership_expire_date'], dtype={'payment_method_id': np.int8, 'payment_plan_days': np.int16, 'plan_list_price': np.int16, 'actual_amount_paid': np.int16, 'is_auto_renew': np.int8, 'is_cancel': np.int8}) $ df_transactions= pd.concat((df_transactions, pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/transactions_v2.csv', parse_dates=['transaction_date','membership_expire_date'], dtype={'payment_method_id': np.int8, 'payment_plan_days': np.int16, 'plan_list_price': np.int16, 'actual_amount_paid': np.int16, 'is_auto_renew': np.int8, 'is_cancel': np.int8} )), axis=0, ignore_index=True).reset_index(drop=True)
pred.head(10) $
autos['registration_year'].describe()
print("All Tweets: {0} | Users: {1}".format(len(df), df.user.nunique())) $ print("Tweets in the Harvey 92 Collection: ", len(df.query('harvey92'))) $ print("Users in the Harvey 92 Collection: ", df.query('harvey92').user.nunique())
print(pd.to_datetime(['20180101'],format='%Y%m%d'))
scores = cross_val_score(lr, X_train, y_train, cv=6) $ print("Cross-validated scores:", scores) $ print("Average: ", scores.mean())
df = pd.read_csv('census.csv') $ df = df[df['SUMLEV']==50] $ df = df.set_index('STNAME').groupby(level=0)['CENSUS2010POP'].agg({'avg': np.average}) $ pd.cut(df['avg'],10)
autos["date_crawled_10"].value_counts(normalize = True, dropna = False).sort_index(ascending = False)
def most_n_tweet_dates(df): $     return df.sort_values('Tweet_counts',ascending = False) $ dates_by_tweet_count = most_n_tweet_dates(tweet_counts_stock_df)
df2 = df2.merge(dfc, on='user_id') $ df2.head()
for dataset in combine: $     dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int) $ train_df.head()
dict_1=request.json() $ dict_1.get('dataset_data')
rnd_reg_2.fit(X_train_2, y_train_2)
titles_list = temp_df2['titles'].tolist()
kickstarter.isnull().sum()
simple = TextBlob("Simple is better than complex. Complex is better than complicated.") $ simple.translate(to='it')
df_full.drop(['index'],axis=1,inplace=True)
disag_filename = join(data_dir, 'disag_gjw_co.hdf5') $ output = HDFDataStore(disag_filename, 'w') $ co.disaggregate(mains,output,sample_period=1) $ output.close()
from datetime import date $ df_new['last_active'] = df_new['current_date'] - df_new['updated_at'] $ df_new['last_active'].astype(str) $
sns.set(font_scale=1.5, style='whitegrid') $ sns.set_palette(sns.cubehelix_palette(rot=-.4))
import dill $ with open('./server/model.pk', 'wb') as f: $     dill.dump(gs, f)
yt = youtube_api.YoutubeDataApi(key)
import matplotlib.pyplot as plt $ plt.figure() $ plt.imshow(image) $ plt.show()
avg_data = tickerdata.groupby(pd.TimeGrouper(freq='12M')).mean()
store_items.dropna(axis = 0)
path_ids = list(map(lambda x: x.split('\t')[0], res.json())) $ path_url_human = list(map(lambda x: 'http://rest.kegg.jp/get/' + x.replace('path:map', 'hsa') + '/kgml', path_ids)) $ pp(path_url_human)
df_others = pd.DataFrame(list_others) $ df_others.head()
pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
lsi.save('trump.lsi') $ lsi = models.LsiModel.load('trump.lsi')
the_sites = [] $ the_sites.append(a) $ the_sites
new_df = pd.DataFrame(df_final[['created_time', 'total_likes', 'total_comments']]) $ new_df.columns
( 1/np.exp(-0.0149), np.exp(0.0506), np.exp(0.0408))
data_df = pd.read_csv('../datasets/flight_delays_data.csv') $ data_df.shape
list(festivals.columns.values)
to_be_predicted_Day4 = 21.28690176 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
daily_vol = [] $ for day in dict_17: $     daily_vol.append(dict_17[day]['Traded Volume']) $ avg_volume= sum(daily_vol)/len(daily_vol) $ round(avg_volume,2)
df.query("group == 'treatment' & landing_page != 'new_page'").count()[0]
df_protest.dtypes=='object'
autos.price.describe()
non_categorical_attributes = ['name', 'age', 'sibsp', 'parch', 'fare', 'body', 'home.dest']
archive.name.value_counts()
prcp_12monthsDf.info()
test_str = news_titles_sr.iloc[0]
s.str.repeat(2)
fig = sns.FacetGrid(trip_data, row = "subscription_type", aspect = 4) $ fig.map(sns.countplot, 'start_hour')
images_df.reset_index().tweet_id.nunique(), len(images_df)
young_co_numbers = very_young_pscs.company_number $ temp_df = active_companies[active_companies.CompanyNumber.isin(young_co_numbers)].groupby(['first_and_postcode'])['CompanyNumber']\ $         .agg(lambda x: len(x.unique())).sort_values(ascending=False).head(10) $ temp_df
gs_rfc_under.score(X_train, y_train_under)
columns = [col for col in X_train.columns if col not in ['msno','bd_c']]
a[a.find(':') + 1:]
targetUserItemInt=userItemInt.join(targetUsers.set_index(['user_id']),on='user_id',how='inner') $ print targetUserItemInt.shape $ targetUserItemInt.head()
df_old = df2[df2['landing_page'] == 'old_page'] $ converted = df_old['converted'] $ old_page_converted = np.random.choice(converted,n_old)
output= "Create view ViewDemo as select user_id, tweet_content, retweets from tweet as t inner join tweet_details as td where t.tweet_id=td.tweet_id order by td.retweets desc;" $ cursor.execute(output) $
data = pd.read_csv('time_series.csv', index_col = 'time')
def scrape_headlines(page_html): $     titles =  re.findall('div class="art_title">(.*?)<\/div', page_html) $     return titles
page.exists()
print("Probability an individual recieved new page:", $       df2['landing_page'].value_counts()[0]/len(df2))
!curl -L -s "https://dl.dropboxusercontent.com/u/16006464/DwD_Fall2014/Restaurants.xlsx" -o Restaurants.xlsx
df.drop(['Unnamed:_13', 'addressee', 'delivery_line_2', 'ews_match', 'suitelink_match', 'urbanization', 'extra_secondary_number', 'extra_secondary_designator', 'pmb_designator', 'pmb_number'], axis = 1, inplace=True) $ df.drop([326625], axis=0, inplace=True)
tmp1 = air_reserve.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'}) $ tmp2 = air_reserve.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'}) $ air_reserve = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])
van_out = van15_fin.drop(['diffs','pagetitle','stiki_mean','cluebot_mode'],axis=1)
sampled_contirbutors_human_agg_by_gender_and_proj_df.show()
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_ipb_brace_crown[1][0])) $ perr = np.sqrt(np.diag(pcov_ipb_brace_crown[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
auth = tweepy.OAuthHandler(api_key, api_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, wait_on_rate_limit_notify=True, wait_on_rate_limit=True)
get_topic_distribution(sandwich_train_model, sandwich_dictionary, sandwich_test.iloc[2]['reviews_without_rare_words'])
import pandas as pd $ pd.__version__
twitter_archive_clean.loc[(twitter_archive_clean.rating_denominator> 10),"rating_denominator"]=10
import clipboard $ clipboard.copy('A wild zebra') $ clipboard.paste()
data_2018= data_2018.set_index('time')
model3 = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK','ab_page']]) $ model3.fit().summary()
experiment_run_details = client.experiments.get_run_details(experiment_run_uid) $ print(experiment_run_details)
post_df = pd.DataFrame(snapshotted_posts) $ posts_filename = "r_worldnews_posts_02.18.2017.recovery.csv" $ post_df.to_csv(os.path.join("outputs",posts_filename))
data1[data1['y_flag'] == 1].groupby(by = ['dataset_location']).size()
X = pd.get_dummies(X, drop_first=True)
prog_lang = df[df.Year != 'Year'] $ prog_lang
len(ben_final['userid'].unique())
max_trip = int((end_2017 - datetime.datetime(1970, 1, 1)).total_seconds())
net_loans_exclude_US_outstanding.loc[KR_select].head()
sns.countplot(y="action",data=firstWeekUserMerged) $ plt.show()
%matplotlib inline $ import matplotlib.pyplot as plt
df3.sort_values(by='DepDelay', ascending=False).head(5)
breed_ratings = breed_ratings.sort_values(ascending = False)
df2.query('converted==1')['user_id'].count()/df2['user_id'].count()
merged2.head()
lm = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ result = lm.fit()
df.groupby("pickup_hour")["cancelled"].mean()
uids = uids.reshape((len(uids), 1)) $ mids = mids.reshape((len(mids),1)) $ rates = rates.reshape((len(rates),1))
df['Updated Shipped diff'].max()
result= logit.fit() $ result.summary()
tfav.plot(figsize=(16,4), label="Likes", legend=True) $ tret.plot(figsize=(16,4), label="Retweets", legend=True);
LT906474.head()
dr = pd.to_datetime(pd.to_datetime(merged_df.index.date)) $ cal = calendar() $ holidays = cal.holidays(start=dr.min(), end=dr.max()) $ merged_df['holiday'] = dr.isin(holidays)
store_items.fillna(method = 'ffill', axis = 0)
df.asfreq('W', method='ffill')
edge_types_DF = pd.read_csv('network/recurrent_network/edge_types.csv', sep = ' ') $ edge_types_DF
df2.head()
print 'A (no fillna() and values cut 18<=x<80):' $ print bnbA.shape $ print 'B (values cut 18<=x<80):' $ print bnbB.shape
most_active = list(np.where(indexed_activity["station_count"]==max_activity)[0]) $ station_max = indexed_activity.iloc[most_active] $ station_max =str(station_max.index[0]) $ print("Station with the most observations recorded: "+station_max+ $       ", with a count of {:,} observations".format(max_activity))
nbar_clean.time
X_train, X_test, y_train, y_test = train_test_split(df_model, df_outcomes, test_size = 0.2, stratify=df_outcomes)
new_df = df[df["landing_page"]=="new_page"] $ new_df1 = new_df[new_df["group"]=="control"] $ new_df2 = df[df["landing_page"]=="old_page"] $ new_df3 =  new_df2[new_df2["group"]=="treatment"] $ print("the no. of times new_page and treatment don't line up = ",len(new_df1) + len(new_df3))
labels = df.author $ true_k = np.unique(labels).shape[0]
url = '{}&start_date={}&end_date={}'.format(base_url, '2017-01-01', '2017-12-31') $ r = requests.get(url) $ if r.status_code != requests.codes.ok: $     print('API ERROR: ({}) -{} '.format(r.status_code, r.text))
df2[df2['converted']==1].shape[0]/df2.shape[0]
df2['DepTime'].count()
cotradicted_pairs = pd.read_csv('Extracted_Contradicted_Tweet_Pairs.csv',encoding='utf-8')
frame3.T
new_reps.apply(lambda x : x.isnull().sum())
results = log_mod1.fit() $ results.summary()
df2_control = df2.query("group == 'control'")
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 5 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
X_train = train_df.drop(['target', 'true_grow'], 1) $ y_train = train_df.true_grow
data = {'Integers': [1, 2, 3], $         'Floats': [4.5, 8.2, 9.6]} $ df = pd.DataFrame(data, index=['label 1', 'label 2', 'label 3']) $ df
loan_stats["revol_util"] = loan_stats["revol_util"].ascharacter() $ loan_stats["revol_util"] = loan_stats["revol_util"].gsub(pattern = "%", replacement = "") $ loan_stats["revol_util"] = loan_stats["revol_util"].trim() $ loan_stats["revol_util"] = loan_stats["revol_util"].asnumeric() $ loan_stats["revol_util"].head(rows = 2)
p_received_new_page = df2.query('landing_page=="new_page"').user_id.nunique()/df2.user_id.nunique() $ p_received_new_page
sort_dict(xgb_learner.best_model.get_fscore())
swinbounds = swPos[swPos.inbound == True]
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17, linear.coef_) + linear.intercept_, c='red') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
afl_data.tail(3)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
df_total['US_new'] = df_total['US']*df_total['ab_page'] $ df_total['UK_new'] = df_total['UK']*df_total['ab_page'] $ df_total['CA_new'] = df_total['CA']*df_total['ab_page'] $ df_total.head()
chunk_review=pd.read_csv("ign.csv",chunksize=14) $ r=pd.Series({chunk.iloc[0].title:chunk.score for chunk in chunk_review}) $ r
np.info(np.unravel_index)
doctype_by_day.iloc[:, doctype_by_day.columns.isin(doctype_by_day.min().sort_values(ascending=False)[:10].index)]
snow.drop_table("nk_als_ref")
new_page_cr = new_page_converted.sum()/new_page_converted.size $ old_page_cr = old_page_converted.sum()/old_page_converted.size $ diff = new_page_cr - old_page_cr $ diff
p_val_twotail = 0.19 $ p_val_lefttail = p_val_twotail/2 $ p_val_rightttail = 1 - p_val_lefttail $ p_val_rightttail $
uber_14.head()
keeping = ['C/A','UNIT','SCP','STATION','DATE','TIME','ENTRIES'] $ ttTimeEntry = ttTimeEntry[keeping] $ ttTimeEntry.head()
bnb2 = bnb[bnb['age']<1000] $ bnb2[bnb2['age']>80].plot(kind='hist', y='age', bins=20)
df.cumsum()
archive_df_clean['date'] = archive_df_clean['timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ archive_df_clean['time'] = archive_df_clean['timestamp'].apply(lambda time: time.strftime('%H:%M'))
df_prep8 = df_prep(df8) $ df_prep8_ = pd.DataFrame({'date':df_prep8.index, 'values':df_prep8.values}, index=pd.to_datetime(df_prep8.index))
guinea_columns=["Date", "Country", "New Cases of Suspects", "New Cases of Probables", "New Cases of Confirmed", $                 "Total new Cases", "Total deaths of suspects", "Total deaths of probables", "Total deaths of confirmed", $                 "Total deaths (suspects + probables + confirmed)", "Total new correct", "Total deaths correct"] $ guinea_df=pd.DataFrame(columns=guinea_columns)
pprint(q_mine.metadata(reload=True))
giss_temp.columns
plt.hist(p_diffs); $ plt.title('Graph of p_diffs') #title of graphs $ plt.xlabel('Page difference') # x-label of graphs $ plt.ylabel('Count') # y-label of graphs $ plt.axvline(x= obs_diff, color='r');
old_page_converted = df2.sample(n_old, replace = True) $ p_old = old_page_converted.converted.mean() $ p_old
df_goog['change'] = df_goog.Close - df_goog.Open
states.set_index(['day', 'location'])
batcmd = thecmd $ result = subprocess.check_output(batcmd, shell=True)
cvec.build_analyzer()
for day in days[:1]: $     df = df4[df4.day == day] $     print(df.placeId.unique().shape, df.shape) $
cat_outcomes['outcome_subtype'] = np.where(pd.isnull(cat_outcomes['outcome_subtype']), $                                            cat_outcomes['outcome_type'], $                                            cat_outcomes['outcome_subtype'])
pgh_311_data.index = pd.to_datetime(pgh_311_data['CREATED_ON']) $ pgh_311_data.head()
s2.value_counts()
a = dat.ward.unique() $ a.sort() $ a
df['game_date'] = pd.to_datetime(df['game_date'])
df_questionable['link.domain'].value_counts()
props.info()
gps_df.to_csv('gps_coords_df.csv',index=False)
top20_brands_mean_price = {} $ for item in top20_brands: $     selected_rows = autos[autos.brand == item] $     v = round(selected_rows.price.mean()) $     top20_brands_mean_price[item] = v
results.summary()
s =[1,2,2,3] $ list(map(lambda x:(s.count(x)),s))
df_new[us_uk]=pd.get_dummies(df_new['country'])[us_uk]
prop_caba_gba.head()
data['Incident Zip'] = data['Incident Zip'].apply(fix_zip)
complete_df = complete_df.join(pd.get_dummies(complete_df['country'])) $ complete_df.head()
data = quandl.get_table('WIKI/PRICES', ticker = tickers, $                         qopts = { 'columns': ['ticker', 'date', 'adj_close'] }, $                         date = { 'gte': '2011-11-31', 'lte': '2017-12-31' }, $                         paginate=True)
test.columns
daily_averages['Weekday'] = daily_averages.index.weekday
df.select('post_creation_date').show()
old_page_converted = np.random.binomial(1,0.1196,145274) $ old_page_converted
import datetime as dt $ listStart=dt.datetime(2016,11,19) $ dwk=dt.timedelta(days=7) $ asdf['WkEnd']=asdf.apply(lambda x: listStart+dwk*(x.wk-1),axis=1)
df_test = text_classifier.predict(df_test) $ df_test.head()
image_predictions.tail()
stock['next_day_open'] = stock.open.shift(-1) 
data.head(5) $
train.pivot_table(values = 'Fare', index = 'Title', aggfunc=np.mean)
dates = pd.date_range('2018-01-01', '2018-05-23') $ dates
forecast.head()
item_lookup = shopify_data_simple[['Lineitem sku', 'Lineitem name']].drop_duplicates() # Only get unique item/description pairs $ item_lookup['Lineitem sku'] = item_lookup['Lineitem sku'].astype(str) # Encode as strings for future lookup ease
learn.freeze_to(-1)
to_be_predicted_Day2 = 36.50976151 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
ratings.loc[2436]
tweet_archive_clean['rating_denominator'].value_counts()
obj4.index.name = 'state'
TrainData_ForLogistic = dum.drop(['City_Code', 'Employer_Code', 'Customer_Existing_Primary_Bank_Code', 'Source', $                                  'DOB', 'Lead_Creation_Date'], axis=1)
df2 = df2.drop(df2.index[2893])
plt.plot(df['Gross Sales'],df['Units Sold'],'.')
tweets_clean.to_csv('tweets_clean.csv', index=False)
taxi_sample.head().to_pandas()
pd.set_option('display.max_columns', None) $
payments_all_yrs.tail()
df.converted.mean()
df.groupby("cancelled")["pickup_dow"].mean()
percipitation_2017_df = pd.DataFrame(percipitation_2017[(percipitation_2017['Date'] >= '2016-08-01')\ $                                 & (percipitation_2017['Date'] <= '2017-08-23')].set_index('Date')) $ percipitation_2017_df $
import spacy $ nlp = spacy.load('en_core_web_sm') $
text_classifier.get_step_params_by_name("text_word_ngrams")        
coinbase_btc_eur_min=coinbase_btc_eur.groupby('Timestamp', as_index=False).agg({'Coin_price_EUR':'mean', 'Coin_volume':'sum'})
p_new = df2.converted.mean() $ p_new $
df.to_json('data\8oct_pre_processed_stemmed_polarity.json', orient='records', lines=True)
weather_dt = np.array(weather_dt)
filled.dropna(axis=0)  # each row containing a NaN is dropped
lat = rhum_nc.variables['lat'][:] $ lon = rhum_nc.variables['lon'][:] $ time = rhum_nc.variables['time'][:] $ rhum = rhum_nc.variables['rhum'][1,:,:] $ np.shape(rhum)
all_simband_data['group_type'] = all_simband_data['subject_id'].apply(lambda x: subject_id_to_group[x])
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.scatter(x=df_twitter_archive_master['favorite_count'],y=df_twitter_archive_master['retweet_count'],c='red',alpha=.5) $ plt.title('Scatter plot between favorite count and retweet count') $ plt.xlabel('favorite count') $ plt.ylabel('retweet count') $ plt.show()
df_by_donor.columns
df = df.reset_index()
wb.search('cell.*%')
rows_no=ab_file.shape[0] $ print('No. of rows in dataset are : ', rows_no)
tw.tail()
prob_control = (df2['group'] == 'control').sum()/unique_users_2 $ prob_control_and_converted = (df2[  (df2['group'] == 'control')& (df2['converted'] == 1)].shape[0])/unique_users_2 $ prob_convert_given_control = prob_control_and_converted/prob_control $ prob_convert_given_control
def find_most(column_name): $     most = results[["ID", column_name, "Author", "Date created", "Message"]] $     most = most.sort_values(by=[column_name], ascending=False)[:5].reset_index(drop=True) $     most.index = most.index + 1 $     return most
df['production_companies'].head(15)
df['waiting_days'].describe()
df2[df2.duplicated(['user_id'], keep=False)]
df.pivot_table(values='Postal Code', index='Updated Shipped ranges',columns='carrier',aggfunc='count')
df2['converted'].mean()*100
df.groupby('converted').count()['user_id']/df.count()['user_id']
autos.columns
time_to_close = pandas.DataFrame(data_list, columns=["launchpad_issue", "gerrit_id_ticket", "launchpad_opening_date", "gerrit_closing_date", "time2close", "current_status", "gerrit_tracker"]) $ time_to_close.columns.values.tolist()
alias_tokens = word_tokenize(unicode.decode('utf-8').lower()) $ print(alias_tokens)
import pandas as pd $ import matplotlib.pyplot as plt $ import datetime
pd.DataFrame({'count':tweet_df.tweet_type.value_counts(), $               'percentage':tweet_df.tweet_type.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})
liquor['Sale (Dollars)'] = [s.replace("$","") for s in liquor['Sale (Dollars)']] $ liquor['Sale (Dollars)'] = [float(x) for x in liquor['Sale (Dollars)']] $ liquor_state_dollars = liquor['Sale (Dollars)']
pd.pivot_table(more_grades, index="name", values="grade", columns="month", margins=True)
df.set_index(['Timestamp'], inplace = True)
flights = pd.read_csv("./data/flight_sample.csv")
glm_model.std_coef_plot(num_of_features = 10) $ print(glm_model.confusion_matrix(valid = True))
clean_df.to_csv('clean_tweet.csv',encoding='utf-8')
bucket_name = buckets[0] $ bucket_obj = cos.Bucket(bucket_name)
new_page = df2.query('landing_page == "new_page"') $ total_pages = df2['landing_page'] $ new_page.count()[0]/total_pages.count()
%%time $ nodeInDegreeDict = network_friends.in_degree() $ nodeOutDegreeDict = network_friends.out_degree()
dfExport.to_csv(path_or_buf=fileOut, sep=',', na_rep='', header=True, index=False, encoding='utf-8', mode='w', quotechar='"', line_terminator='\n', decimal='.')
fundret.idxmax(), fundret[fundret.idxmax()]
subs_and_comments.head()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
n_user_days = pax_raw[['seqn', 'paxday']].drop_duplicates().groupby('seqn').size()
tables[4].head()
p_diffs = [] $ for _ in range (10000): $     new_page_converted = np.random.choice([0,1], 145310, p=[p_mean,(1-p_mean)]) $     old_page_converted = np.random.choice([0,1], 145274, p=[p_mean,(1-p_mean)]) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())   
total_users = len(clean_users) $ total_users_mailing = clean_users['opted_in_to_mailing_list'].sum()
null_vals=np.random.normal(0,p_diffs.std(),len(p_diffs)) $ plt.hist(null_vals) $ plt.axvline(x=obs_diff,color='red')
vectorized = cv.fit_transform(shows['stemmed_keywords']).todense() $ vectorized = pd.DataFrame(vectorized, columns=cv.get_feature_names())
plt.plot(dfdaycounts['created_date'], dfdaycounts['count_n']) $ plt.xticks(rotation='vertical')
DT_yhat = DT_model.predict(test_X) $ print("DT Jaccard index: %.2f" % jaccard_similarity_score(test_y, DT_yhat)) $ print("DT F1-score: %.2f" % f1_score(test_y, DT_yhat, average='weighted') )
stocks_happiness=stocks_happiness.dropna(axis=0) $ stocks_happiness $
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23NYCC').items(100): $     results.append(tweet) $ print(len(results))
log_reg_pred = log_reg.predict(test)
df['Agency'].value_counts()
groups = intervention_history.groupby('INSTANCE_ID')
search.timestamp = pd.to_datetime(search.timestamp) $ search.trip_start_date = pd.to_datetime(search.trip_start_date) $ search.trip_end_date = pd.to_datetime(search.trip_end_date)
plt.plot(df['field5'])
store_items = store_items.rename(columns={'bikes': 'hats'}) $ store_items
tweets = pd.read_csv('https://raw.githubusercontent.com/organisciak/Scripting-Course/master/data/voldemort_tweets.csv') $ tweets.head()
df2_treatment = df2.query("group == 'treatment'") $ convereted_rate_new = round(df2_treatment.converted.mean(),4) $ print(convereted_rate_new)
df.drop(df[((df.state == '') & (df.city != ''))].index, inplace=True)
zipShp = gpd.read_file(os.environ.get('PUIDATA') + '/zipcodes/ZIP_CODE_040114.shp')
df = df.rename(columns={'dvsz':'vzs', 'hdxxh':'xhth'}) $ df2 = df[['zv','vzs']] $ curr_d = tuple(set(df2['current_device']))
model = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']]).fit()
df.groupby(df.index.hour).count().plot(y='Complaint Type')
autos['date_crawled'] = autos['date_crawled'].str[:10] $ autos['date_crawled'].value_counts(normalize=True, dropna=False).sort_index()
pd.value_counts(ac['Filing Date']).head()
df_link_yt['video_title'].value_counts().head(10)
old_page_converted = pd.DataFrame(np.random.choice([0,1],n_old,p=[1-CRold,CRold])) $
ax = df['price'].hist() $ ax.set_title('Price range') $ df['price'].plot.hist(alpha=0.99, bins=15) $ ax.set_ylabel('Number of house') $ ax.set_xlabel('price') $
george = relevant_data[relevant_data['User Name'] == 'George Liu'] $ df = george['Event Type Name'].value_counts() $ df_george = pd.Series.to_frame(df) $ df_george.columns = ['Count_George'] $ df_george
df_all_loans.head()
old_page_converted=np.random.binomial(n_old, p_null)
first_month = grouped_months_new.get_group(8) $ second_month = grouped_months_new.get_group(9) $ third_month = grouped_months_new.get_group(10) $
df_old_true = df2.query('group == "treatment"')['converted'].mean() $ obs_diff = df_new_true - df_old_true $ df_old_true
p_new = p_old = df2['converted'].mean() $ p_new
os.listdir(B.paths['subset_directory'])
SD_mailing = np.sqrt(2*(total_users_mailing/total_users) * (1-total_users_mailing/total_users) / total_users)
import numpy as np $ data['float_time'] = data['processing_time'].apply(lambda x: $                                                   x/np.timedelta64(1,'D')) $ data
df.head()
(data.isnull().sum()/len(data)).sort_values() #percentage of missing values $
female_journalists_mention_summary_df[['mention_count']].describe()
X_train, X_test, y_train, y_test = train_test_split(stock.drop(['target'], 1), stock['target'], test_size=0.3, random_state=42)
conn_b.commit()
data.columns
train.info()
plt.figure(figsize=(12, 8)) $ plt.plot(df[df['Close'] < 3], label='Close')
sns.regplot(x="qstot_0", y="y", data=psy_native).set_title("Depressive Symptomatology") $
gender[gender == -1].index
results = model.fit() $ results.summary()
html = driver.page_source
station_data = session.query(Stations).first() $ station_data.__dict__
committees_NNN.head()
print("The user that has been active the longest, has been on the site for %.2f years" %(users['DaysActive'].max() / 365))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
import seaborn as sns $ sns.set(style="white") $ sns.pairplot(matrix) $ plt.show()
pd.DataFrame(records.loc[:, records.columns != 'ID']).hist(); $ plt.savefig('images/histograms.png')
comm_merge.head()
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ results = log_mod.fit()
titanic.dtypes
dates = session.query(Measurement.date, Measurement.prcp).order_by(Measurement.date.desc()).limit(365).all() $ s_dates = pd.DataFrame(dates) $ s_dates.head(1)
to_be_predicted_Day4 = 38.291767 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df = df4[df4.day == days[0]]
sns.factorplot('notRepairedDamage',data=autodf,kind='count')
ticker = 'MSFT' $ msft = quandl.get('%s/%s' % (exchange, ticker)) $ msft.head(1)
pokemon.drop(["Total"],inplace=True,axis=1) $ plt.figure(figsize=(10,6)) #manage the size of the plot $ sns.heatmap(pokemon.corr(),annot=True) #df.corr() makes a correlation matrix and sns.heatmap is used to show the correlations heatmap $ plt.show()
y_fit_proba = rf.predict_proba(test_data_features)
data_sets['1min'][data_sets['1min']['interpolated_values'].notnull()].tail()
c.most_common(3)
rf_grid.fit(X_train,y_train)
opened_prs = PullRequests(github_index).get_cardinality("id_in_repo").by_period() $ print("Trend for month: ", get_trend(get_timeseries(opened_prs))) $ opened_pr = PullRequests(github_index).get_cardinality("id_in_repo").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(opened_pr)))
df.head()
for the_year in range(initial_year,final_year + 1): $     abc = payments_by_year[the_year].sort_values( ascending=[False]) $     abc.head() $     plot_top_DRGs(abc,5,the_year,'payment',False)
autos['offer_type'] = autos['offer_type'].replace('Angebot', 'offer')
ccl.set_index("Date", inplace=True)
df_release.isnull().sum()
one_hot_domains_questionable = df_questionable.groupby('user.id')[media_classes].sum().fillna(0) $ one_hot_domains_questionable = one_hot_domains_questionable.apply(normalize, axis=1).fillna(0)
df = pd.DataFrame(video_ids) $ df.head()
writer = pd.ExcelWriter('ch_fp7.xlsx') $ ch.to_excel(writer) $ writer.save() $ writer.close()
df_final.columns
(autos["ad_created"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
(p_diffs>diff).mean()
rh = weather_yvr['Relative Humidity (%)'] $ rh.head()
year_averages = session.query(Measurement.date, Measurement.prcp).\ $     filter(func.strftime("%Y-%m-%d", Measurement.date) > "2016-08-23").\ $     order_by(Measurement.date).all() $
learn.lr_find()
mask_XX_century = (deaths_by_decade.reset_index()['decade'] >= 1900) & (deaths_by_decade.reset_index()['decade'] <= 2000)
import pickle $ filename = 'IMDB_model.pkl' $ pickle.dump(gs_lr_tfidf, open(filename, 'wb')) $
first_status['user'].keys()
print(((new_page_converted == 1).mean()) - ((old_page_converted == 1).mean())) $ print(p_new - p_old)
gp.aggregate(lambda x: [i for l in x for i in l ])
all_data_merge.to_csv('Nestle_normal_march.csv', index=False, sep=',', encoding='utf-8')
count_vect = CountVectorizer(vocabulary=unique_feature_list, tokenizer=lambda x: x.split(','))
df2.groupby([df2['group']=='treatment',df2['converted']==1]).size().reset_index()[0].iloc[3]/df2.query('landing_page == "new_page"').user_id.nunique()
import matplotlib.pyplot as plt $ plt.style.use('ggplot')
reddit_data.date = pd.to_datetime(reddit_data.date, unit = 's')
df['year']=list(map(lambda x:x.year,df.reviewTime)) $ df['length_review']=list(map(lambda x:len(x),df.reviewText)) $ df['length_title_reviews']=list(map(lambda x:len(x),df.summary)) $ df['number of reviewers']=list(map(lambda x:len(df[df.year<=x].reviewerID.unique()),df.year))
new_df.head()
X= preprocessing.StandardScaler().fit(X).transform(X) $ X[0:5]
titanic[titanic.survived == 1].head()
def get_residual_sum_of_squares(input_feature, output, intercept, slope): $     predictions = input_feature * slope + intercept $     residuals = predictions - output $     RSS = (residuals ** 2).sum() $     return(RSS)
textgroupby = talks.groupby('label')['text']
liberiaFullDf = pd.concat(liberiaFrameList)
from sklearn.model_selection import train_test_split $ from keras.utils import np_utils $ train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=0.2, stratify = y)#, stratify = y) $
pd.read_csv("../data/microbiome/microbiome_missing.csv").head(20)
lm_uk = sm.OLS(df_new['converted'], df_new[['intercept_uk', 'ab-page']]) $ results_uk = lm_uk.fit() $ results_uk.summary()
df.converted.values.sum()/row_num
fig, ax = plt.subplots(1,1, figsize=(16,8)) $ colors =[None, 'black', 'red', 'blue'] $ for key, vals in scoresdf.items(): $     plt.plot(vals['min_df'], vals['score'], color=colors[key], label=key) $ plt.legend(fontsize=18)
def dataAggregator(df,column,name): $     new = df.groupby(['Year','Month','Day'])[column].agg({'no_{}'.format(name):'count'}) $     new.reset_index(level=[0,1,2], inplace=True) $     new['datetime'] = pd.to_datetime((new.Year*10000+new.Month*100+new.Day).apply(str),format='%Y%m%d') $     return new
import zipfile $ zip_ref = zipfile.ZipFile('./names.zip', 'r') $ zip_ref.extractall('./names/') $ zip_ref.close()
preds = pd.DataFrame(y_test) $ preds['knn'] = knn_pred $ preds['rf'] = rf_pred $ preds['lr'] = lr_pred
average_polarity.reset_index(inplace=True)
shared_bucket = storage.Bucket('inpt-forecasting') $ for obj in shared_bucket.objects(): $   if obj.key.find('/') < 0: $     print(obj.key)
Y = data3.sales
max_mean_km = price_vs_km["mean_odometer_km"].max() $ price_vs_km.loc[price_vs_km["mean_odometer_km"] == max_mean_km, :]
gDateEnergy = itemTable.groupby([pd.Grouper(freq='1W', key='Date'),'Energy'], as_index=True) $ gDateEnergy_content = gDateEnergy['Content'] $
print('Accuracy baseline is: {:.2f}%'.format(100*(1-full['<=30Days'].mean()))) $
df.head()
type(git_log.timestamp[0])
sess = bp.Session()
van.revtime = ben_final.revtime.str[:19]
twitter_dataset=twitter_dataset.drop(columns=['date_time','img_num','user_followers','user_favourites','in_reply_to_status_id','in_reply_to_user_id','source','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp','expanded_urls'])
mismatch_grp1 = ab_file.query("group == 'treatment' and landing_page == 'old_page'") $ print("Times treatment group user lands incorrectly on old_page is {}".format(len(mismatch_grp1))) $ mismatch_grp2 = ab_file.query("group == 'control' and landing_page == 'new_page'") $ print("Times control group user incorrectly lands on new_page is {}".format(len(mismatch_grp2))) $ print("Times new_page and treatment don't line up is {}".format(len(mismatch_grp1) + len(mismatch_grp2)))
np.median(df2.ltv)
datetime.datetime.now().year
googletrend.loc[googletrend.State=='NI', "State"] = 'HB,NI'
s.str.upper()
properati.loc[:,'state_name'] = states
df.info() # looks like all rows have values
s.index[[2,3]]
total_rows = len(df.index) $ converted = (df['converted'] == 1).sum() $ print(converted / total_rows)
s1 = np.random.normal(0, 1, 2000) $ s2 = np.random.normal(9, 2, 2000) $ v = pd.Series(np.concatenate([s1, s2])) $ v.hist(bins=100, alpha=0.4, color='B', normed=True) $ v.plot(kind='kde', style='k--')
def initialize_analytics(): $     credentials = ServiceAccountCredentials.from_json_keyfile_name( $         KEY_FILE_LOCATION, SCOPES) $     analytics = build('analytics', 'v3', credentials=credentials) $     return analytics
for index, row in after.iterrows(): $     if (row.home_team in teams or row.away_team in teams): $         continue $     else: $         after.drop(index, inplace = True)
df_joined.columns
df['col1'].map(lambda x:x*100)
test_text3 = df['text'][11] $ print(test_text3) $ re.sub(r'[:=;] [oO\-]?[D\)\]\(\]/\\OpP]', '', test_text3)
active_countries[active_countries['All'] > 20].sort_values('% of Total')['% of Total'].tail(15).plot('barh', color = 'g') $ plt.title('Top 15 Active Countries') $ plt.xlabel('% of Active Users in Country') $ plt.ylabel('')
col_to_drop = ['text', 'extended_tweet','ideo_score', 'friends', 'followers', 'listed', 'screen_name', 'id', 'name'] $ clean_madrid = df_madrid.drop(col_to_drop, axis=1)
cont = json.loads(s)
X=df.titles $ y=df.target
companies = data['Company'].value_counts() $ print('Number of Companies compained about: {}'.format(len(companies)))
import pandas_datareader.data as web $ import datetime $ df = web.DataReader('UN_DEN', 'oecd', end=datetime.datetime(2012, 1, 1)) $ df.columns
gene_df.sort_values('length').head()
stationActiveTemp_df = pd.read_sql("SELECT * FROM measurement where station = 'USC00519281'", conn) $ lowestTemp = stationActiveTemp_df['tobs'].min() $ highestTemp = stationActiveTemp_df['tobs'].max() $ avgTemp = stationActiveTemp_df['tobs'].mean() $ print(f"Lowest temp: {lowestTemp}\nHighest temp: {highestTemp}\nAverage temp: {avgTemp}")
kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix)
pd.set_option('display.max_colwidth', -1) $ df[['text', 'favorite_count', 'date']][df.favorite_count == np.max(df.favorite_count)]
import sys $ reload(sys) $ sys.setdefaultencoding('utf8')
filterdf = result.query("0 <= best <= 1 and fileType == 'csv' and teamCount < 1000") $ filterdf.corr()['best'].sort_values()
(sub_df.groupby('Rating').size()/sub_total).plot(kind='bar') $ plt.title("Propotion of Review Rating") $ plt.xlabel("Rating") $ plt.ylabel("Propotion of reviews") $ plt.xticks(rotation=0)
red_4['created_utc'] = red_4['created_utc'].astype('datetime64[s]') $ red_4['time fetched'] = red_4['time fetched'].astype('datetime64[s]') $ red_4.head()
from sklearn.linear_model import LogisticRegression $ from sklearn.model_selection import train_test_split $ from sklearn.feature_selection import RFE
df1=pd.DataFrame(dict(id=range(4),age=np.random.randint(18,31,size=4))) $ df2=pd.DataFrame(dict(id=list(range(3))+list(range(3)),score=np.random.random(size=6))) $ print(df1) $ df2
min_lat = weather_df["Lat"].min()//10*10 $ max_lat = weather_df["Lat"].max()//10*10+15 $ tick_locations = np.arange(min_lat -10, max_lat +10, 10)
top50 = pd.read_csv('../top50visited.csv', sep=';') $ top50['dataset_slug'] = [x.split('www.data.gouv.fr/datasets/')[1] for x in top50.dataset] $ top50['dataset_id'] = top50.dataset_slug.map(datasets_slug_id)
gen2 = dta.t[(dta.b==1) & (dta.c==2)]
df.count(), len(df.columns)
NewConverted = df2.query('converted == 1')['user_id'].count() $ HAlte = (NewConverted/NewTotalUser) $ print("Alternate Convert Rate: ",HAlte)
melted = pd.melt(s4p, id_vars=['Date','Symbol']) $ melted[:5]
df2.loc[df2.user_id.duplicated(keep = False),:]
df_with_ctr = pd.merge(df, ctr, how="left", on=["Rank", "Device"])
csvData['country'].value_counts()
slFullDf = pd.concat(slFrameList,axis=0) $ slFullDf.head()
df_local_website = df[(~df.domain.isnull()) & $                       (df.domain != 'facebook.com') & $                       (df.domain != 'comettv.com')].drop_duplicates(subset=['domain'])
pods.notebook.display_plots('quadratic_basis{num_basis:0>3}.svg', $                             directory='../slides/diagrams/ml', $                             num_basis=IntSlider(0,0,2,1))
X.to_pickle(data_file_path+'X.pkl') $ y1.to_pickle(data_file_path+'y1.pkl') $ y2.to_pickle(data_file_path+'y2.pkl') $ master_df.to_pickle(data_file_path+'master_df.pkl') $ print("... saved as pickle")
dfNew=pd.concat([df['Country'],df],axis=1) $ df[dfNew.columns.unique()]
name = pet[0].find('div', attrs = {'class': 'views-field views-field-field-pp-animalname'}) $ primary_breed = pet[0].find('div', attrs = {'class': 'views-field views-field-field-pp-primarybreed'}) $ secondary_breed = pet[0].find('div', attrs = {'class': 'views-field views-field-field-pp-secondarybreed'}) $ age = pet[0].find('div', attrs = {'class': 'views-field views-field-field-pp-age'})
merge_event["hasphone"] = merge_event.device.str.contains('phone')
conf_matrix = confusion_matrix(y_tfidf_test, svc.predict(X_tfidf_test), labels=[1,2,3,4,5]) $ conf_matrix # this is very bad, all results are classified as 5 point.
data.head()
np.exp(-0.0674), np.exp(0.0118), np.exp(0.0783), np.exp(0.0175), np.exp(0.0469)
df2.loc[1, 'b'] = np.nan
df_yt.head(2)
tweets['daysFromStart'] = (tweets['fromStart'] / np.timedelta64(1, 'D')).apply(np.floor) $ tweets['hoursFromStart'] = (tweets['fromStart'] / np.timedelta64(1, 'h')).apply(np.floor)
pd.DataFrame({'features': X.columns, 'LasoCoefficients': lasso.coef_})
twitter_data.head()
plt.figure(figsize=(16,5)) $ plt.plot(daily)
df_clean3.loc[1254, 'text']
old_page_converted = np.random.binomial(1, p_old, size = n_old)
vect1.vocabulary_
np.random.seed(12347)
cust_demo.ndim
testing.head(10)
df.describe()
df.rating_category.value_counts()
Test.MakeConnection(UnLockedPort)
user_answered_counts = df.set_index('user_id').index.value_counts().sort_index()
df_flight_paths = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv') $ df_flight_paths.head()
old_page_converted = np.random.normal(p_old,np.std(p_old), n_old)
print(array.max()) $ print(array.min()) $ print(array.mean(axis=0))
logit_mod = sm.Logit(df_merge['converted'], df_merge[['intercept', 'ab_page', 'UK', 'US', 'interaction_UK', 'interaction_US']]) $ res = logit_mod.fit() $ res.summary()
sns.regplot(x=df2['age'], y=df2['ltv'])
facts_metrics.groupby('dimensions_date_id').size().describe()
%matplotlib inline $ sentiment_df.plot.bar("index", 0)
top_10_3 = scores.loc[7].argsort()[::-1][:11] $ trunc_df.loc[list(top_10_3)]
outcomes = [0,1] ### 0 is not converted and 1 is converted $ probs = [1-c_rate_null,c_rate_null] ### probability of 0 is one minus conversion rate at null, and probability of 1 is c_rate_null $ new_page_converted = np.random.choice(outcomes, size= nnew,replace = True, p = probs) $ new_page_converted
df_trump_tweets = pd.read_csv('Trump_Tweets.csv') $ text_data = df_trump_tweets['text'] $ text_data.fillna('', inplace=True) $ list_text = list(text_data) $ trump_words = preprocessing(list_text)
df_sample=df.sample(n=len(df)/200) $ print "The original size is:",len(df) $ print "The sample size is:",len(df_sample)
threeoneone_geo = threeoneone_geo[threeoneone_geo['Latitude'].notnull()] $ threeoneone_geo = threeoneone_geo[threeoneone_geo['fix_time_sec'].notnull()] $ threeoneone_geo = threeoneone_geo[threeoneone_geo['fix_time_sec']>0]
bonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]]) $ bonus_points = pd.DataFrame(bonus_array, columns=["oct", "nov", "dec"], index=["bob","colin", "darwin", "charles"]) $ bonus_points
ecg_data = test_df[test_df.field == 'ecg'] $ ecg_data['relative stdev'].mean()
supertrimmed_sorted_degree = sorted_degree_map(supertrimmed_degrees) $ d = DataFrame(supertrimmed_sorted_degree) $ d.set_index([0], inplace=True) $ Table.display(d)
ex2.sort_values(ascending = False) $ ex2
a.ndim  #Number of dimensions.
url = "http://space-facts.com/mars/"
scaler.fit(x_test)
n_new = df2[df2.group == 'treatment'].count()[0] $ n_new
for k in range(len(imagelist)): $     send2trash.send2trash(imagelist[k]) $ imagelist = [i for i in os.listdir() if i.endswith(".pdf")  ] $ len(imagelist)
training.index = range(608) $ test.index = range(153) $ training.head()
df1 = df.copy()
import numpy as np $ stock_change = stocks.apply(lambda x: np.log(x) - np.log(x.shift(1))) # shift moves dates back by 1. $ stock_change.head()
tweets_clean['retweet_count'] = 0 $ tweets_clean['favorites_count'] = 0 $ tweets_clean.head() $
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, inspect, func, desc
dfgts = pd.DataFrame() $ for i in range(len(datafiles)): $   dftmp = pd.read_csv(datafiles[i]) $   dfgts = dfgts.append(dftmp)
autos["registration_year"].value_counts(normalize=True)
final_df = dt_features.join(tree_c_features)
import pandas as pd $ with pd.option_context("max.rows", 10): $     print(dta.results.value_counts())
from dotce.visual_roc import roc_curve, precision_recall_curve
df_grp= df.groupby('group') $ df_grp.describe()
df_methods = [({'id':x['id'],'carrier':x['carrier'],'name':x['name']}) for x in json.loads(response.content)['shipping_methods']] $ df_methods = pd.DataFrame(df_methods)
city = pd.read_sql_query('select * from city', engine) $ city.head()
from scipy.stats import norm $ critical_value = norm.ppf(1-(0.05)) $ print('The critical value of our z-score is : {:.4f}'.format(critical_value))
startups_USA = startups_USA.set_index('permalink')
df2 = pd.read_csv('ab.csv') #reading the new dataset file
nmf3 = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(transformed3)
sess = data_t[data_t['Session'] == data_t.ix[0, 'Session']] $ fg, ax = plt.subplots() $ sess.plot(y='X', ax=ax) $
fin_r = fin_r.reindex(df.resample('D').index, method='ffill') #df.resample('D').index $ assert (fin_r.index == r_top10_mat.index).all()
nonzero = grades.drop(grades.index[(grades.Mark == 0) | (grades.ATAR == 0)]) $ nonzero.plot(x='ATAR', y='Mark', color='red', kind='scatter')
print('The Jupyter notebook stores information in the "Kernel".\ $       \nRestart the Kernel to clear noteook memory.')
from sklearn.decomposition import TruncatedSVD $ tfidf_svd_v2 = TruncatedSVD(n_components=n_comp, n_iter=100, random_state=2) $ review_reduced_v2 = tfidf_svd_v2.fit_transform(tfidf_vecs)  $ print(tfidf_svd_v2.explained_variance_ratio_) $ print(tfidf_svd_v2.singular_values_)
boolean_test_frame = ((loan_stats['loan_status'] == 'Charged Off')|(loan_stats['loan_status'] == 'Default')) $ loan_stats['loan_status'] = boolean_test_frame.ifelse('Default','Fully Paid')
avg_km_by_brand = dict.fromkeys(brands) $ for name in brands: $     brand_km = autos.loc[autos["brand"] == name, "odometer_km"] $     brand_avg_km = round(brand_km.mean()) $     avg_km_by_brand[name] = brand_avg_km
senateAll[["leg_id", "filerIdent"]].to_csv("../data/senateIDs.csv") $ houseAll[["leg_id", "filerIdent"]].to_csv("../data/houseIDs.csv")
import pandas as pd $ items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']), $          'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])} $ print(type(items))
cand_date_df.shape
autos['abtest'].value_counts()
proj_df['Project Grade Level Category'].unique()
close_month.loc[month].nlargest(2)
print('Voted Classifier Accuracy:', nltk.classify.accuracy(voted_classifier, test_set) * 100, '%')
r_json = r.json() $ data = r_json['dataset_data']['data'] $ keys = r_json['dataset_data']['column_names']
events_filtered_1 = events_enriched_df[events_enriched_df['yes_rsvp_count']>50] $ events_filtered_2 = events_df[(events_df['yes_rsvp_count']>50) & (events_df['venue.city']=='Chicago')]
autodf.monthOfRegistration = autodf.monthOfRegistration.astype('category')
combined = exportOI.set_index('join_col').join(exportID.set_index('join_col'), lsuffix="exportOI", rsuffix="exportID", how = "inner")
data_set.tail(5)
pd.get_dummies(grades_ord,drop_first= True)
dataFrame.head(5)
!hdfs dfs -put CC_records.csv {HDFS_DIR}/Consumer_Complaints.csv
payload_online = {"name": "test", "description": "test", "type": "online"} $ response_online = requests.post(endpoint_deployments, json=payload_online, headers=header) $ print(response_online) $ print(response_online.text)
dfUK = final_df.query('country == "UK"') $ dfUS = final_df.query('country == "US"') $ dfCA = final_df.query('country == "CA"')
log_status_to_count_df = status_to_count_df.withColumn('log(count)', sqlFunctions.log(status_to_count_df['count'])) $ data = log_status_to_count_df.drop('count').collect() $ data
station_activity = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                         order_by(func.count(Measurement.tobs).desc()).all() $ station_activity
weather.info()
df1.drop_duplicates(subset = ['first_name', 'last_name'])
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local') $
import pandas_datareader.data as web  #Not 'import pandas.io.data as web' as in the book.
carmileage={} $ for brand in top20.index: $     mile=autos[autos["brand"]==brand]['odometer_km'].mean() $     carmileage[brand]=mile $ print(carmileage)   
scores[3.5:].sum()/total
train.grade.value_counts()
conn.setsessopt(caslib='otherlib') $
data_demo["upvotes"].describe()
aapl.index
df_new = df_new.join(pd.get_dummies(df_new['group']))
temp_df2.info()
X = pd.get_dummies(X, columns=['subreddit'], drop_first=True)
sequences = pd.DataFrame(index=range(seqsize), columns=users)
oppose.head(1)
sample_survey.loc[:, ['i_Education', 'wf']].groupby('i_Education').sum().plot.bar();
cur_a.close() $ conn_a.close() $ cur_b.close() $ conn_b.close()
results.to_csv(path_or_buf=path + '/NFL_Fantasy_Search_2016_PreSeason.csv')
df.head() # default --> prints first 5 rows
twitter_archive_clean[twitter_archive_clean.tweet_id==883482846933004288]
index = pd.date_range(end='%s-23:00'%date, periods=7*24, freq='h') $ prices = df[::2].transpose().stack() $ prices.index = index
plt.title("homework") $ plt.xlabel("Date") $ plt.ylabel("Prcp")
results.head()
tfidf_vect = TfidfVectorizer(analyzer = clean_text) $ X_tfidf = tfidf_vect.fit_transform(tweets_1['text']) $ Y_tfidf = tfidf_vect.transform(tweet_table['text'])
movies['released'] = pd.to_datetime(movies['released']) $ movies.info()
eug_counts = aqi.groupby(['AQI Category_eug']).size() $ (eug_cg_counts / eug_counts).unstack(fill_value=0)
archive_clean.info()
logs.columns
autos.info() $ autos.head()
submission2[['proba']].median()
meaning = pd.Series(42, ["life", "universe", "everything"]) $ meaning
intake['IntakeDate'] = pd.to_datetime(intake['DateTime'],format ='%m/%d/%Y %I:%M:%S %p' ) $ intake = intake.drop(['DateTime','MonthYear','Found Location'],axis=1)
autos[['date_crawled','last_seen','ad_created']].head()
forcast_out=int(math.ceil(.01*len(df1))) $ forcast_out $
conn = engine.connect()
sub = pd.DataFrame(np.column_stack((y_id, y_test_log_pred)), columns=['listing_id'] + le.classes_.tolist())
In this case, the p value obtained from the ztest and hypothesis testing comes to .90. whereas in the Logistic regression we can see $ the p value of 0.19. Still the p value is more and is not statistically significant to predict the value of convertion
dogs.at['Rex', 'gender'] = 'M' $ dogs.at['Rover', 'gender'] = 'M' $ dogs.at['Polly', 'gender'] = 'F' $ dogs
ps.index
pd.date_range(start, periods=5, freq=pd.tseries.offsets.BDay())
ssstext = [s.encode("ascii","ignore") for s in sstext] $ ststext = [s.encode("ascii","ignore") for s in tstext] $ ssstext[:10]
df1['Week'] = week.values $ df1.loc[0:10, ['SerNo', 'By', 'Week']]
test_post_words = test_post.split() $ print(test_post_words[:10]) # tokenize and lower case $ print(len(test_post_words))
result = api.search(q='%23H2P')
session.query(func.count(Sta.name)).all()
p_diffs = [] $ for i in range(10000): $     p_diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_new, 10000)/n_old $
rt.sample(5)
from sklearn.ensemble import RandomForestClassifier
from bokeh.io import output_notebook $ output_notebook()
(mydata / mydata.iloc[0] * 100).plot(figsize = (15, 8)); # 15 deals with the width and 8 deals with the price. $ plt.show() # this function always plots the price using matplotlib functions. $
first_cluster.transform(docs).shape
recommendation_df['hacker_count'].describe()
dfPriceCalculations = pd.DataFrame(columns=['Peak Price','Base Price']) $ dfPriceCalculations['Peak Price'] = peakPricePerDay $ dfPriceCalculations['Base Price'] = basePricePerDay $ dfPriceCalculations.head() # verify correct creation of consolidated data frame
df2.head(3)
trump[trump['is_retweet']==True].text[0:5]
bow_df_df.shape
df_clean = df_clean[df_clean['in_reply_to_status_id'].isnull()]
df.drop(['id', 'id_region', 'country', 'name_region'],inplace=True, axis = 1)
Raw_Forecast.set_index("Date_Monday").groupby( $     [pd.TimeGrouper('M'), "Product_Motor", "Part_Number"]).sum().fillna(0)[["Qty"]]
ins.groupby("type").size()
data['Created Date'] = data['Created Date'].apply(lambda x: datetime.datetime. $                                                   strptime(x,'%m/%d/%Y %H:%M'))
"We have {0} Bible passages.".format(len(D2))
from sagemaker.tensorflow import TensorFlowPredictor $ predictor = TensorFlowPredictor('tensorflowgendermodel571') $ sagemaker.Session().delete_endpoint(predictor) $
prods_user2 =pd.merge(priors_product_purchase_spec,priors_product_reordered_spec,how="outer") $ prods_user2['reorder_ratio']=prods_user2['reordered_count_spec']/prods_user2['purchase_count_spec'] $ prods_user2.head()
(trainingData, testData) = training.randomSplit([0.8, 0.2], seed = 100)
dat_hcad_1 = dat_hcad['blk_range'] $ dat_hcad_2 = dat_hcad['77002'] $ dat_hcad_zip = pd.concat([dat_hcad_1,dat_hcad_2],axis=1) $ dat_hcad_zip.shape
conn.fetch(table=dict(name='data.iris', caslib='casuser'), to=5, $            sortby=['sepal_length', 'sepal_width'])
df.dtypes # Tells us what data type each column is!
data.dropna(thresh=4)
train_df.info()
namesarray = list(df.index.values[0:20])
df_columns.head() $
print(f'Got {len(rentals):,.0f} listings')
users.columns
reddit_comments_data.groupby('link_id').count().orderBy('count', ascending = False).show(100, truncate = False)
sessions_summary = sessions_summary.merge(right=train_users, how="left", left_on=["user_id"], right_on=["id"]) $ sessions_summary.shape[0]
from scipy.interpolate import *
n_new = df2[df2.landing_page == 'new_page'].shape[0] $ n_new
df3 = df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'], $                     'salary': [70000, 80000, 120000, 90000]}) $ df3
df1=df['author'].groupby(df['author']).count() $ df1.head()
re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])
print('Total number of telemetry records: {}'.format(len(telemetry.index))) $ print(telemetry.head()) $ telemetry.describe()
df_predict = df_final[['p1_conf', 'p2_conf', 'p3_conf']] $ df_predict.corr()
tokenize.fit_on_texts(train_posts) # only fit on train $ x_train = tokenize.texts_to_matrix(train_posts) $ x_test = tokenize.texts_to_matrix(test_posts)
np.exp(results.params)
product_searches_df['ga:eventAction'].value_counts()
temperature = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $     filter(Measurement.station == busiest_station).\ $     filter(Measurement.date > last_year).\ $     order_by(Measurement.tobs).all() $ temperature
au.find_some_docs(uso17_coll,sort_params=[("id",1)],limit=3)
temp = temp.drop(["year","month"],axis =1)
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set()
train_df, y, nas = proc_df(train_data, 'totals.transactionRevenue')
sub_df.to_csv("dnn100_sub.csv", index=False)
dfg = df[df.source=='google']
import json $ test_sample = json.dumps({"data": test_df.to_json(orient='records')}) $ prediction = aci_service.run(input_data = test_sample) $ print(prediction)
stocks.head()
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK', 'US']] $ df_new.head()
Imagenes_data_v2.head()
Measurements = Base.classes.Measurements $ Station = Base.classes.Station $
df1 = pd.DataFrame(np.random.randn(10,3),columns=['col1','col2','col3']) $ df1
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
plt.figure(figsize=(5,5)) $ sns.countplot(auto_new.Custom)
data.head(-2)
results = model.fit() $ results.summary()
new = new.join([model, year, country, custom, payment, hand, body, purch])
p_mean = np.mean([p_new, p_old]) $ p_mean
first_name = first_movie.h3.a.text $ first_name
index_to_change = df3[df3['group'] == 'treatment'].index $ df3.set_value(index = index_to_change, col='ab_page', value = 1) $ df3.set_value(index = df3.index, col ='intercept', value=1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
df2.groupby('group').describe()
matched_df = df.loc[df['Match']=='Match'].copy() $ matched_df[:5]
import sentlex $ import sentlex.sentanalysis
count_all = Counter() $ terms_all = [term for term in list(itertools.chain.from_iterable(obama_cleaned_words))] $ count_all.update(terms_all) $ print(count_all.most_common(5))
!pip install -r requirements.txt
to_be_predicted_Day3 = 55.25522979 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df_arch_clean['dog_stage'] = df_arch_clean['text'].str.extract('(doggo|floofer|pupper|puppo)', expand=True) $
sim = df.iloc[0] $ b1 = phimage(sim.name)
descr_text = df.description.tolist() $ cleaned_description = [] $ for tweet in descr_text: $     cleaned_description.append(clean_tweet(tweet))
df.iloc[[1, 3, 5], [1, 3]]
atdist_opp_dist_noinfo_valid_responses = ['Sure! I would be happy to go out of my way!'] $ atdist_opp_dist_noinfo_valid_count_prop_byuser = compute_valid_count_prop_byuser(atdist_opp_dist[~atdist_opp_dist['infoIncluded']], $                                                                                users_opp_dist, 'vendorId', 'emaResponse', $                                                                                atdist_opp_dist_noinfo_valid_responses) $ atdist_opp_dist_noinfo_valid_count_prop_byuser.head()
iso3166 = {'SLE': 'Sierra Leone', 'NGA': 'Nigeria', 'LBR': 'Liberia' } $ print("These are all the keys in this dictionary: {}".format(iso3166.keys())) $ print("This country is {}".format(iso3166['LBR']))
temp_df.info()
if 'elasticsearch-hadoop-6.1.1' not in os.listdir(): $     es_hadoop = urllib.request.URLopener() $     es_hadoop.retrieve("http://download.elastic.co/hadoop/elasticsearch-hadoop-6.1.1.zip", "es-hadoop.zip") $     with zipfile.ZipFile("es-hadoop.zip","r") as zip_ref: $         zip_ref.extractall()
pd = %sql select * from runtime.queries limit 1 $ pd.DataFrame().head()
dict(list(r_close.items())[0:10])
mask = (youthUser3["creationDate"] > '2017-01-01') & (youthUser3["creationDate"]<= '2017-12-31') $ youthUser2017 = (youthUser3.loc[mask]) $ youthUser2017.head()
temps_df.loc['2014-07-03']
crsr.execute("describe targets")
trump_originals = trump[trump.is_retweet == False] $ trump_originals.shape $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value
top_supports.head(5).to_csv("top_supporters.csv")
perc_df.map(two_digits) $
old_page_converted = np.random.choice([0,1],n_old, p=(p_old,1-p_old))
import numpy as np $ date = np.array('2015-07-04', dtype=np.datetime64) $ date
rule_one_below = df[df['X'] < x_chart_lcl] $ for i in range(0, rule_one_below.shape[0], 10): $     display_html(rule_one_below.iloc[i:i+10].T)
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount" , ascending=False)
taxa_labels=list(set(merged_data.taxa)) $ plot_labels=list(set(merged_data.plot_id)) $ print(taxa_labels) $ print(plot_labels)
shifted_leaderboard = plate_appearances.groupby(['game_year','batter_name', 'batter', 'bats'])['is_shift'].agg(['mean', 'count', 'sum']) $ shifted_leaderboard.loc[shifted_leaderboard['count']>150,].sort_values('mean', ascending=False).head(15)
df.sort_values('prob', ascending=False).head(10)
all_simband_data.to_csv('all_simband_data.csv', sep=',')
pd.read_sql_query('SELECT * FROM samples', con)
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old])
df_station = df.groupby(('STATION', 'DATE')).sum().reset_index() $ df_station
replies.columns
print(len(df_proj)) $ df_proj.head()
obs_data = session.query(Measurement.tobs, Measurement.station).\ $     filter(func.strftime("%Y-%m-%d", Measurement.date) > "2016-08-23").\ $     filter(Measurement.tobs == active_stations[0]).\ $     order_by(Measurement.tobs).all() $
dictionary = Dictionary(train_texts) $ corpus = [dictionary.doc2bow(text) for text in train_texts]
train_df['parcelid'].nunique()
df2.tail() $
df.corr()
re.match('^\w+@[a-zA-Z_]+?\.[a-zA-Z]{2,3}$', 'abc@gmail.com').group()
df_msg = pd.DataFrame.from_records(ods.messages)
start = datetime.now() $ modelrf100 = RandomForestClassifier(n_estimators=50, n_jobs=-1).fit(Xtr.toarray(), ytr) $ print(RandomForestClassifier(n_estimators=50, n_jobs=-1).fit(Xtr.toarray(), ytr)\ $     .score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]}) $ X_new.head()
import pandas as pd $ df = pd.read_csv("../Data/teaminfo.csv")
article.download() $ article.parse()
round(len(df_twitter[df_twitter.dog_label == 'puppo']) / len(df_twitter.dog_label), 2)
df.loc[1:4,"Date"]
train.iloc[[0, 1, 4]]
pickup_kmeans = KMeans(n_clusters=num_pickup_clusters, random_state=1).fit(pickup_coords)
 X = pd.merge(X,title_tokens, left_index=True, right_index=True)
archive_copy.loc[archive_copy['tweet_id'] == tweet_id_11, 'new_rating_denominator'] = 10 $ archive_copy.loc[archive_copy['tweet_id'] == tweet_id_13, 'new_rating_denominator'] = 10 $ archive_copy.loc[archive_copy['tweet_id'] == tweet_id_13a, 'new_rating_denominator'] = 10 $ print (archive_copy['new_rating_denominator'].value_counts()) $
tweets_df.columns
df1=pd.DataFrame({'key':['K0','K1','K2','K3'],'A':['A0','A1','A2','A3'],'B':['B0','B1','B2','B3']}) $ df2=pd.DataFrame({'key':['K0','K1','K2','K3'],'C':['C0','C1','C2','C3'],'D':['D0','D1','D2','D3']}) $ print(df1) $ print(df2)
import statsmodels.api as sm $ convert_old = df2.query("group=='control' & converted==1").count()[0] $ convert_new = df2.query("group=='treatment' & converted==1").count()[0] $ n_old = size_c $ n_new = size_t
smart_authors.head()
sum(twitter_df_clean.rating_denominator > 10)
n_old = (df2['landing_page'] == 'old_page').sum()
atloc_4x_valid_responses = ['Valid Response'] $ atloc_4x_valid_count_prop_byuser = compute_valid_count_prop_byuser(atloc_4x, users_4x, 'vendorId', 'remappedResponses', $                                                                    atloc_4x_valid_responses) $ atloc_4x_valid_count_prop_byuser.head()
with pd.ExcelWriter("test.xls") as writer: $     giss_temp.to_excel(writer, sheet_name="GISS temp data") $     pd.DataFrame({"Full Globe Temp": full_globe_temp}).to_excel(writer, sheet_name="FullGlobe temp data")
len(directed_by_popular_person(officers,100).company_number.unique())
sets_columns_names = [column[DATA].name for column in get_child_data_nodes(sets_node).values()] $ display(sets_columns_names)
building_pa_prc_shrink.dtypes
results.summary()
model = AuthorTopicModel.load('/tmp/model.atmodel')
gen3 = dta.t[(dta.b==1) & (dta.c==3)]
df3['timestamp'].max()-df3['timestamp'].min() $
y.head()
app_pivot = df[['is_application','ab_test_group', 'email']].groupby(['ab_test_group','is_application']).count().reset_index() $ app_pivot
sp500.iloc[[0, 2]]
client.experiments.monitor_logs(experiment_run_uid)
Image(url='http://s29.postimg.org/86323s15j/Python_Data_Types.png')
cust_age = pd.read_csv('../data_clean/customer_age.csv') $ cust_age.head()
to_be_predicted_Day4 = 22.34273663 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
sum(twitter_archive.tweet_id.duplicated())
countries_df.head()
df2.query('converted == "1"').user_id.nunique() / df2['user_id'].nunique()
sunspots.info()
agg_function = {'budget':np.mean, 'reading_score':np.mean, 'math_score': np.mean, 'size': np.mean, 'passing_reading': np.sum, 'passing_math': np.sum}
df3.head(5)
(df['margin_val'] < 0).sum()
festivals_clean['Time'] =  pd.to_datetime(festivals_clean['Time'])
requests.saveAsTextFiles("/dir/requests")
df = item.to_pandas() $ df.tail()
df['text']=df['title'].str.replace('\d+', '') $
stc.checkpoint("checkpoint10")
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print(z_score, p_value)
languages = [] $ def lang_detection(tweet): $     for tweet in tweet_textblob_de: $         lang = translator.detect(tweet).lang $         languages.append(lang)
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) $ trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  $ bigram_mod = gensim.models.phrases.Phraser(bigram) $ trigram_mod = gensim.models.phrases.Phraser(trigram) $ print(trigram_mod[bigram_mod[data_words[0]]])
eclf1 = VotingClassifier(estimators=[('lr', alg), ('calC', alg7)], voting='soft') $ eclf1.fit(X_train, y_train) $ probs = eclf1.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"])
merged2 = merged1.copy()
plt.hist(p_diffs) $ plt.axvline(actual_diff, c='r');
lr_best = gs.best_estimator_
df = pd.DataFrame(result, columns = ["date", "prcp"]) $ df['date'] = pd.to_datetime(df['date']) $ df = df.set_index("date") $ df.head()
print(len(df)) $ print(df.head()) $ print(topics.head())
df_significant_feature = pd.DataFrame(significant_features, index=range(len(significant_features)), columns=['xfeature'])
old_page_converted = np.random.choice([0, 1], size=n_old, p=[1-p_old_null, p_old_null])
sns.distplot(filter_iphone(orig_tweets[orig_tweets['date'].dt.year==2016])['hour'], color='b') $ sns.distplot(filter_android(orig_tweets[orig_tweets['date'].dt.year==2016])['hour'], color='g')
ls *.ipynb
df_2014.dropna(inplace=True) $ df_2014
group_brands = df2.groupby('brand').agg({"price": [min, max, mean]}) $ group_brands.columns = ["_".join(x) for x in group_brands.columns.ravel()] $ group_brands
Results_ZeroFill.to_csv('soln_logistic_zerofill.csv', index=False)
with open ("auth.txt", "r") as f: $     cred=f.readlines() $ cred = [i.rstrip() for i in cred] $ auth = tweepy.AppAuthHandler(cred[0],cred[1]) $ saveTweets(auth,query,5000)
joined.dtypes.filter(items=['Frequency_score'])
p_old_diff = (old_page_converted.sum() / len(old_page_converted)) $ p_old_diff
df_4_test.columns =  ['Hospital', 'Provider ID', 'State', 'Period', 'Claim Type', 'Avg Spending Hospital', 'Avg Spending State', 'Avg Spending Nation', 'Percent Spending Hospital', 'Percent Spending State', 'Percent Spending Nation'] $ print df_4_test
def get_genre(x): $     return x.replace("'", ' ').replace(',', ' ').split() $ df.Genre = df.Genre.apply(get_genre)
import statsmodels.api as sm $ mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = mod.fit()
prcp_12monthsDf.set_index('date').describe()
ks_goal_failed = ks_goal_failed.sort_values(by = ['counts'], ascending = False) $ ks_goal_failed.head(10)
for idx, row in df_trips.iterrows(): $     df_trips.loc[idx, "trip_started"] = row["trip_requested"] + np.random.randint(60, 60 * 15)
data.iloc[1]
merged_data.drop('due', axis=1, inplace=True)
from pyspark.sql.functions import to_timestamp $ df_city_reviews = df_city_reviews.withColumn('date', to_timestamp(df_city_reviews['date'], 'yyyy-mm-dd'))
tt_final.dropna(inplace=True) $ tt_final.info()
meal_addon_counts_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meal_addon_counts.csv')['Body'].read().decode('utf-8') $ meal_addon_counts = pd.read_csv(StringIO(meal_addon_counts_csv_string), header=0, delimiter='|')
cursor = con.execute('SELECT * FROM samples') $ cursor.fetchall()
BroncosBillsPct = winpct.loc[winpct['Game Title Date'] == 'Broncos vs. Bills  2017-09-24'][['playId','text','homeWinPercentage']]
pd.concat([s1, s2, s3], axis=0)
to_be_predicted_Day1 = 26.40 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
cars.isnull().sum()
print(autos["price"].unique().shape) $ print(autos["price"].describe()) $ autos["price"].value_counts().head(20)
ifcdf = pd.concat([ifc2df(row) for row in cachedf.itertuples()])
tabulation.columns
print ("Data Frame with Forward Fill limiting to 1:") $ df2.reindex_like(df1,method='ffill',limit=1)
print(df_w_topics[['title', 'author gender', 0]].sort_values(by=[0], ascending=False))
print(len(labels.keys()))
def normalize(row): $     return row / row.sum()
soup.title.contents
df.iloc[0:5,:12]
secret_corporate_pscs = active_psc_records[(active_psc_records.secret_base == True) & (active_psc_records.kind == 'corporate-entity-person-with-significant-control') & (active_psc_records['identification.legal_form'] != 'Public Limited Company')].copy() $ len(secret_corporate_pscs.company_number.unique())
len(df[df.isnull().any(axis=1)])
win_list = [pd.read_csv(win_paths[i], index_col='date', parse_dates=True) for i in range(len(win_paths))]
features_regress_vect = vectorizer.transform(features_regress)
saved_model = ml_repository_client.models.save(model_artifact)
props = MetaProps({MetaNames.AUTHOR_NAME:"IBM", MetaNames.AUTHOR_EMAIL:"ibm@ibm.com"})
xgb_learner.initialize()
score_variable = 'correct.answers'
def getFullScoreGraph(by_month_df, by_week_df, subreddit, notable_dates): $     plt.gcf().subplots_adjust(bottom=0.25) $     getScoreGraph(by_month_df, subreddit, notable_dates) $     getScoreGraph(by_week_df, subreddit, [], alpha=0.4)
month_year_crimes = crimes.groupby(['year', 'month']).size()
imgp.describe()
corM =  pd.concat([X , y ], axis =1).corr()
from scipy.stats import norm $ z_score, p_value = \ $     sm.stats.proportions_ztest( [ convert_new, convert_old ], \ $                                 [ nnew, n_old ], \ $                                 alternative='larger' ) $
nfl.shape
txns[txns['token']=='DAI'].describe()
df['timestamp'] = pd.to_datetime(df['timestamp'])
import numpy as np $ dist = np.sum(train_data_features, axis=0) $ for tag, count in zip(vocab, dist): $     print(tag, count)
samples_query.result_set
X_valid, y_valid = X_valid.head(90), y_valid.head(90)
sns.barplot(x='Total_Num_Comments', y='Subreddit', orient='h', data=subred_num_tot[:10])
unique_users.info()
mis_1 = df[(df.group == "control") & (df.landing_page == "new_page")].count()[0] $ mis_2 = df[(df.group == "treatment") & (df.landing_page == "old_page")].count()[0] $ print("Count of the group 'Control' misaligned with landing page 'New Page' is: %d" %mis_1) $ print("Count of the group 'Treatment' misaligned with landing page 'Old Page' is: %d" %mis_2) $ print("Number of times the 'New Page' and 'Treatment' don't line up is: %d" %(mis_1 + mis_2))
value_counts = combined_df['State'].value_counts(dropna = False) $ count_results = pd.DataFrame({'State':value_counts.index, 'Count':value_counts.values}) $ count_results = count_results[['State', 'Count']] $ print(count_results)
data_volumn.iloc[0:100]
tm_2040 /= 1000 $ tm_2040_norm = tm_2040 ** (10/11) $ tm_2040_norm = tm_2040_norm.round(1) $ tm_2040_alpha = tm_2040 ** (1/3) $ tm_2040_alpha = tm_2040_alpha / tm_2040_alpha.max().max()
text=[tweet.text for tweet in all_tweets] $ text[:5]
df1_clean.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis = 1, inplace=True)
df.info()
dfg = dfg.sort_values(['discharges'], ascending=[False]) $ dfg = dfg.reset_index(['drg3']) $ dfg.head()
frame.apply(f)
logit_mod3 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'ab_CA']]) $ results3 = logit_mod3.fit() $ results3.summary()
pred.info()
control_cnv = df2.converted.mean() $ control_cnv
linkNYC['lonlat'] = zip(linkNYC.longitude,linkNYC.latitude) $ linkNYC['geometry'] = linkNYC['lonlat'].apply(lambda x:shapely.geometry.Point(x))
    forest.fit(X_train,y_train) $     y_pred = forest.predict(X_test) $     y_pred = pd.DataFrame({'ans': y_pred}) $     test = pd.DataFrame({'close':clean_prices.tail(90).reset_index().close, 'ratio':y_pred['ans']}) $     test['pred_price']= test['close'] * test['ratio']
nitrogen['HydrologicEvent'].unique()
df.dropna(subset=['model','brand'],axis=0, how='any', thresh=None, inplace=False)
engine.make_classifier("nhtsa_classifier", $                        "SingleTag", $                        "SimpleFreqDist", $                        "NaiveBayesClassifier")
reviews.groupby('variety').price.agg([min,max])
data.loc[data['hired']==1].groupby('category').hourly_rate.mean()
cursor = con.execute('SELECT * FROM samples') $ rows = cursor.fetchall() $ rows
np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)
LSTM_traininput = [np.array(LSTM_trainin) for LSTM_trainin in LSTM_traininput] $ LSTM_traininput = np.array(LSTM_traininput) $ LSTM_testinput = [np.array(LSTM_testin) for LSTM_testin in LSTM_testinput] $ LSTM_testinput = np.array(LSTM_testinput)
tst_lat_lon_df = pd.read_csv("testset_unique_lat_and_lon_vals.csv", index_col=0)
season_team_groups.aggregate(np.mean).sort_values(by = "Tm.Pts", ascending = False).head(5)
baseball1_df.drop(baseball1_df.index [8107], inplace=True) $ baseball1_df.loc[baseball1_df['ageAtDebut'].idxmin()]
mean_price_by_brand = {} $ for brand in brands: $     price_by_brand = autos.loc[autos["brand"] == brand, "price"] $     mean_price_by_brand[brand] = price_by_brand.mean() $ mean_price_by_brand
ann_ret_SP500[0].head(10)
q2_results = session.query(Stations.name,func.count(Measurements.date)).filter(Stations.station == Measurements.station)\ $             .group_by(Measurements.station).all() $
noise.head(2)
gs_k150_under.score(X_test, y_test_under)
X_test = count_vect.transform(df_test.text)
active_companies['sum_red_flags'] = active_companies[red_flag_columns].sum(axis=1) $ active_companies.sum_red_flags.value_counts()
pd.Series({2:'a', 1:'b', 3:'c'})
df_clean = df_clean.sort_values(by='ID') # the bigger the ID, the most recent the tweet 
tag_df.values
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn
import statsmodels.formula.api as smf $ lm = smf.ols(formula='sales ~ TV', data=data).fit() $ lm.params
autos = autos.drop("number_of_photos",axis=1) $ autos.head(3)
df.dateCrawled = pd.to_datetime(df.dateCrawled) $ df.dateCreated = pd.to_datetime(df.dateCreated) $ df.lastSeen = pd.to_datetime(df.lastSeen)
damaged_map = {} $ for c in ['yes', 'no']: $     damaged_map[c] = autos[autos['unrepaired_damage'] == c]['price'].mean() $ damaged_map    
!rm world_bank.json.gz -f $ !wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=5000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
merged1 = merged1.rename(columns={'Name':'OfficeName', 'id_x':'id'})
sns.barplot(Data2015['Month'], Data2015['Sale (Dollars)'], estimator = sum)
df_gt.head()
tweet_full_df.info()
print(f'shape of issue body array: {vectorized_body.shape}') $ print(f'shape of issue title array: {vectorized_title.shape}')
df.describe()
atloc_opp_dist_valid_responses = ['Valid Response'] $ atloc_opp_dist_valid_count_prop_byuser = compute_valid_count_prop_byuser(atloc_opp_dist, users_opp_dist, 'vendorId', $                                                                          'remappedResponses', atloc_opp_dist_valid_responses) $ atloc_opp_dist_valid_count_prop_byuser.head()
def timestamp2datetime(timestamp): $     try: $         return  pd.to_datetime(timestamp,unit='s') $     except AttributeError: $         return timestamp 
full_orig = full.copy()
dataFrame.describe()
c = Counter(tree_labels) $ pprint.pprint(c.most_common(10))
dfChile.head()
control_proportion = df2.query('group=="control" & converted==1')['user_id'].count()/df2.query('group=="control"')['user_id'].count() $ treatment_proportion = df2.query('group=="treatment" & converted==1')['user_id'].count()/df2.query('group=="treatment"')['user_id'].count() $ obs_diff = treatment_proportion - control_proportion $ obs_diff
df2 = df2.append(df.query("group == 'treatment' and landing_page == 'new_page'"))
pro_con = df['converted'].mean()*100 $ print("Proportion of users converted are: {}%".format(round(pro_con,2)))
df.iloc[99:104]
df.index
df['dateCrawled'] = df['dateCrawled'].astype('datetime64') $ df['dateCreated'] = df['dateCreated'].astype('datetime64') $ df['lastSeen'] = df['lastSeen'].astype('datetime64[ns]')
twitter_ar['source_url'] = twitter_ar['source_url'].map(lambda x: x.lstrip('<a href=').rstrip('" ')) $ twitter_ar['source_type'] = twitter_ar['source_type'].map(lambda x: x.lstrip(' ="nofollow">').rstrip('</a>'))
pd.get_dummies(months)
train_data.describe()
DataSet['userTimezone'].value_counts()
test_clean = test_data.drop("Name", axis=1) $ test_clean = test_clean.drop("Color", axis=1) $ test_clean = test_clean.drop("Breed", axis=1) $ test_clean = test_clean.drop("AgeuponOutcome", axis=1) $ print test_clean.head()
brands=["bmw", "audi", "mercedes_benz", "porsche","land_rover", "mini", "opel", "toyota"] $ for b in brands: $     print("models to be sold percentage of manufacturer {}".format(b)) $     print(autos[autos.brand==b].model.value_counts(normalize=True).head(5))
B2.print_all_paths() $
merged_df.shape
tweets.to_csv('tweets.csv')
stocks.duplicated().sum()  # Checking for duplicated data 
0.118920 #. From the above table
prediction.sample(3)
output.count()
conn.columninfo(table=dict(name='crops', caslib='casuser'))
df_count_clean.info()
ts_utc = ts.tz_localize('UTC') $ ts_utc
df_new.groupby('country').count()
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set() $ goog.plot()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
iplot(data.groupby('merged_by.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, kind='bar', dimensions=(750, 500)))
column_abbreviations = pd.read_csv("data/afl_data_columns_mapping.csv") $ column_abbreviations
df_combined.shape
df.to_sql('toronto_health_inspections', conn, if_exists='append', index=False) $ inspector = inspect(conn) $ print(inspector.get_table_names())
expected_cashflows.shape
df.to_json("json_data_format_records.json", orient="records") $ !cat json_data_format_records.json
index_group2 = user_group['Group'].apply(checkGroup, number=2) $ user_group2 = user_group[index_group2] $ print(len(user_group2)) $ user_group2.head()
idx_drop = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].index $ df2 = df.drop(idx_drop)
trip_data.head()
c6 = CayleyColorsGroup('r') $ c7 = CayleyColorsGroup('g') $ print("If I have ", c6, 'and', c7, 'then', c6, '*', c7, '=') $ c8 = CayleyColorsGroup(c6.get_cayley_table()[c6.get_value(), c7.get_value()]) $ print(c8)
old_page_converted = df2.sample(n_old, replace = True).converted $ old_page_converted.mean()
import matplotlib.cm as cm $ dots_c, line_c, *_ = cm.Paired.colors
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY $ r = requests.get(url)
finalData=dat.append(Stockholm_data) $ X.f = vectorizer.fit_transform(finalData['tweet_text']) $ y.f = finalData['class'] $
df2=df[df.Trip_distance < 100] $ df2['Avg_speed'].describe()
ddp.to_csv("twitter_dataset_v2.csv", float_format='{:f}'.format)
c.find_one({'name.middle': 'Joseph'})
series.index = list('abcdefghij')
for name, in session.query(User.name).\ $ ...             filter(User.fullname=='Ed Jones'): $ ...    print(name)
degree_dict = dict(G.degree(G.nodes())) $ nx.set_node_attributes(G, degree_dict, 'degree')
oldPage_df = df2.query('landing_page == "old_page"') $ n_old = oldPage_df.shape[0] $ n_old
pred5 = nba_pred_modelv1.predict(g5) $ prob5 = nba_pred_modelv1.predict_proba(g5) $ print(pred5) $ print(prob5)
a = criteria.values $ so.iloc[a].head()
df.head()
def on_base_percentage(df): $     return ( df.h + df.bb + df.hbp )/( df.ab + df.bb + df.hbp + df.sf + 1e-6) $ obp = on_base_percentage(baseball) # equivalent to obp = baseball.apply(on_base_percentage, axis=1) $ obp.sort_values(ascending=False).round(4)
frequency_list = california_house_dataframe.iloc[:, 2].value_counts() $ print(type(frequency_list)) # Series Type $ print(sorted(california_house_dataframe.iloc[:, 2].unique())) $ frequency_list.hist() $ pd.DataFrame(california_house_dataframe.iloc[:, 2]).hist() $
(StockData['Dell'] / StockData['Facebook']).tail()
sns.factorplot(x='call_type',y='length_in_sec',col='call_day',data=calls_df,kind='bar')
femalebydatenew  = femalebydate[['Sex','Offense']].copy() $ femalebydatenew.head(3) $
null_vals = np.random.normal(0, diffs.std(), diffs.size)
bnb.age.isnull().sum()
autos.price.value_counts().sort_index(ascending=False).tail(100)
df3 = df2.join(df_countries.set_index('user_id'), on='user_id') $ df3.head()
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets']))) $ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df3[['control', 'treatment']] = pd.get_dummies(df['group']) $ df3.head()
df = pd.read_sql('SELECT COUNT(*) FROM city;', con=conn) $ df
prop57.committee_position.value_counts()
tweet_df_raw.head(5)
train=mydf.sample(frac=0.9,random_state=200) $ test=mydf.drop(train.index)
final_df = final_df.join(n_features)
sales_2016 = df_2016.groupby(by=["store_number"], as_index=False)
prob=df2['converted'].mean() $ print("Probability of an individual converting regardless of the page they receive is "+str(prob))
print(f.__name__) $ print(print_hello.__name__)
df2 = df2.drop_duplicates()
str(datetime.now())
data = pd.DataFrame(data=[tweet.text for tweet in results], columns=['Tweets'])
print(df['Borough'].value_counts(dropna=False))
pd.Series(data=y5_train).value_counts()
support.sort_values('amount', ascending= False)
high_low_diff = TenDayMeanDifference(inputs=[USEquityPricing.high, USEquityPricing.low])
Most_active_stations = session.query(Measurement.station).\ $                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).limit(1).scalar() $ print ( "Station which has the highest number of observations is  " + str(Most_active_stations))
balance_df.shape
timestamp = pd.to_datetime('now')-pd.Timedelta(hours=5) $ path='C:\\Users\\Christopher\\Google Drive\\TailDemography\\outputFiles\\' $ filename = path + 'cleaned CC data 2000-2017_' + '.csv' $ df.to_csv(filename,index = False) $ filename
df[df['Promo2SinceYear'] == 1900].head()
batting_df.head(10)
from google.datalab.ml import TensorBoard $ for pid in TensorBoard.list()['pid']: $   TensorBoard().stop(pid) $   print 'Stopped TensorBoard with pid {}'.format(pid)
train_df['FareBand'] = pd.qcut(train_df['Fare'], 4) $ train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)
tlen.plot(figsize = (16,4), color = 'r')
finaldf.congress = finaldf.congress.astype(int) $ finaldf.head()
%%time $ model.fit(train_x, train_y, epochs=100)
!apt-key add /var/cuda-repo-8-0-local-ga2/7fa2af80.pub
full['PastPCPVisits'].plot(kind='box',figsize=(12,4),vert=False) $ plt.xlabel('visits') $ plt.title('Distribution of Past PCP Visits',size=15)
list(c.find({}, {'name.first': 1, $                  'born': 1, $                  '_id': 0}))
new_w = np.zeros((vs, em_sz), dtype=np.float32) $ for i,w in enumerate(itos): $     r = stoi2[w] $     new_w[i] = enc_wgts[r] if r>=0 else row_m
df.head()
response.json()
df_prep9 = df_prep(df9) $ df_prep9_ = pd.DataFrame({'date':df_prep9.index, 'values':df_prep9.values}, index=pd.to_datetime(df_prep9.index))
df_goog.Open.resample('M').mean()
locationing.polarity.plot(kind='hist', normed=True) $ range = np.arange(-1.5, 1.5, 0.001) $ plt.plot(range, norm.pdf(range,0,1))
appleNegs = companyNeg[companyNeg['author_id'].str.contains('AppleSupport') | companyNeg['text'].str.contains('AppleSupport')]
test = pd.DataFrame(raw_df.groupby('Date')['DA-price'].mean()) $ test['DA-price'].values
Beverage_name = ["Pepsi","Tropicana Lemonade","Water","Sobe Yumberry Pomegranate","Aquafina Sparkling","Tropicana Pink Lemonade","Diet Pepsi","Diet Sierra Mist","Brisk Raspberry Iced Tea","Sierra Mist","Mountain Dew","Rug Root Beer","Sweet Tea","Diet","LMN","CHR","STW","VAN"] $ new_data['drink2'] = new_data['drink2'].fillna("no") $ new_data['drink3'] = new_data['drink3'].fillna("no")
for f in train_preprocessed.columns: $     print f
data.resample('M').mean().plot()
unique_member_ids = votes_by_party['member_id'].unique() $ print(len(unique_member_ids))
fig, ax  = plt.subplots(figsize = (10,8)) $ aapl["Adj. Open"].plot(ax = ax) $ plt.show()
autos.columns $
new_page_converted= np.random.binomial(1, p=p_new, size=n_new) $ new_page_converted
np.exp(final_results.params)
tweets_unique = tweets092815.append(tweets092315.append(tweets.append(oldtweets)))
df_clean = df_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id','retweeted_status_id', $               'retweeted_status_user_id', 'retweeted_status_timestamp', 'source' ], axis = 1)
combined_df[combined_df['classifier'].isnull() == True]
h = plt.hist(tag_degrees.values(), 100) #Display histogram of node degrees in 100 bins $ plt.loglog(h[1][1:],h[0]) #Plot same histogram in log-log space
df.query('group=="treatment" and landing_page=="old_page"').count() + df.query('group=="control" and landing_page=="new_page"').count()
df_Q1_3 = df_Q123.set_index('Time')
rstd_SPY = df['SPY'].rolling(window=20).std()
all_data_vectorized = body_pp.transform_parallel(all_data_bodies)
investors_df['most_recent_active_year'] = investors_df['most_recent_investment'].dt.year $
print(a) $ a.remove('dog')  # Remove first matching instance of element $ print(a) $ del a[-1]  # Remove element at index; implementedby list.__del__
pd.to_datetime(0)
pred_df_logmodel = pd.concat([X_test, pd.DataFrame(logmodel_predictions, columns = ['logmodel_predictions'])], axis = 1) $ pred_df_logmodel.head(10)
park['named_drug'] = park.abstract.apply(named_drug)
data2 = pd.DataFrame(data=[tweet.text for tweet in lista], columns=['Tweets'])
class StdDev(CustomFactor): $     def compute(self, today, asset_ides, out, values): $         out[:] = np.nanstd(values, axis=0)
tweet.user.location $ tweet.user.time_zone $ tweet.created_at $ tweet.user.name $ tweet.user.screen_name
df_stackoverflow.drop $ df_launchpad.drop $
df_bkk.describe()
logit_mod3 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'US', 'UK', 'ab_page_US', 'ab_page_UK']]) $ results3 = logit_mod3.fit() $ results3.summary()
def clean_it(time): $     return re.sub('days+-\s', '')
raw_data.columns
autos["date_crawled"].sort_index() #sort_index default is ascending, to do descending do this: sort_index(ascending=False) $
NYG = pd.read_excel(url_NYG, $                     skiprows = 8)
import nltk.data $
pd.get_dummies(df_goog.Open > df_goog.Close)
print(results_baseline.summary()) $ print(results_clusters.summary())
print('Unique number of users notified: {}'.format(len(atloc_4x['vendorId'].unique())))
learner.sched.plot_loss()
np.unique(bdata.index.time)
S3loc = sagemaker_session.upload_data(path=local_pickel_root, bucket=bucket, key_prefix='visa-kaggle/data') $ print(S3loc) $ !aws s3 ls cyrusmv-sagemaker-demos/visa-kaggle/data/ --recursive #use the output from your own S3loc
filename = patient+'_mvarmodel_results.hdf' $ adjmat_load = loadarray(filename) $ print adjmat_load.data.shape $ print adjmat_load.metadata.keys()
display('df8', 'df9', 'pd.merge(df8, df9, on="name", suffixes=["_L", "_R"])')
got_new_page = df2[df2['landing_page'] == 'new_page'] $ prob_new_page = got_new_page.shape[0] / df2.shape[0] $ print(prob_new_page)
train = df[df.index < '2014-01-01'] $ test = df[df.index > '2013-12-31']
co.steady_states.head() $
model.wv.syn0.shape
country_data.describe()
figure = df_parsed['Open'].plot();
a = ['d', 'h', 'g'] $ b = ['1', '2', '3'] $ c = list(map(str.__add__, a, b))
df_mismatch1 = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_mismatch2 = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ total_mismatch = df_mismatch1.shape[0] + df_mismatch2.shape[0] $ print total_mismatch
df = df[df.first_message_month < 10] $ df = df[df.first_message_month > 2] $ print(min(df.first_message_timestamp)) $ print(max(df.first_message_timestamp)) $ print("... cropped date to Jan through Sept 2017")
IMDB_df.head()
normed = df/df.iloc[0,:] $ normed.head()
cur.execute('SELECT LangCd ,count(*) FROM surveytabl WHERE LangCd="QB";') $ cur.fetchall()
import statsmodels.api as sm $ convert_old = df2.query('landing_page=="old_page" and converted==1').count()[0] $ convert_old $
df4[['CA','UK','US']] = pd.get_dummies(df4['country']) $ df4 = df4.drop('CA', axis=1) $ df4.head()
prcp_df.count()
ss  = StandardScaler() $ logreg = LogisticRegression() $ rfc = RandomForestClassifier(n_estimators=14, random_state=42) $ adb = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
df_con.head()
datetime.strptime('09/Aug/1995:09:22:01 -0400',DATETIME_PARSE_PATTERN).weekday()
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closerToBotOrTop'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], 'FromTopWell', 'FromBotWell')
train_data[train_data['totals.transactionRevenue'].notnull()].groupby(['device.browser']).agg({'visitNumber': 'count'}).reset_index().set_index("device.browser",drop=True).plot.bar()
old_converted = np.random.binomial(n_old, p_old, )
pd.DataFrame(A)
to_be_predicted_Day4 = 31.29300329 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
dd=cfs.diff_abundance('Subject','Control','Patient')
old_page_converted = np.random.choice([1, 0], size=n_old_page, p=[p_mean, (1-p_mean)]) $ print(old_page_converted.mean())
for dataset in full_data: $     dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 $ print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())
n_old = df2.query(('landing_page == "old_page"')).count()[0] $ n_old
df[df['yearOfRegistration'] > 2016].head(1) $
workspace_data.columns
data2 = copy.deepcopy(df[['dates','sales']])
RunSQL(sql_query) $ actor = pd.read_sql_query(sql_query, engine) $ actor.head()
df.isnull() $ df.isnull().values.any()
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ assert os.getcwd() != today,'Current directory should be TODAY' $
df.head(2)
census_withdata = census.merge(all_boroughs_cleaned,how='inner',left_on='GEOID_tract',right_on='GEO.id2')
print(scores.mean())
componentTable = pd.DataFrame(index=list) $ for x in selectedComponents: $     componentTable['PC' + str(x+1)] = pd.Series(pca.components_[x],index=list)
import pandas as pd $ data_1 = pd.read_csv(url_1) $ data_2 = pd.read_csv(url_2)
lr_balanced = lr_grid.best_estimator_ $ lr_balanced.fit(X_train,y_train) $ lr_balanced_pred = lr_balanced.predict(X_test) $ lr_balanced_predp = lr_balanced.predict_proba(X_test)
plot = ggplot(visual_df, aes(x='ORt', y='DRt', size='wins')) + \ $     geom_point() + \ $     geom_hline(y=[visual_df['DRt'].mean()], size = 1, color='black') + \ $     geom_vline(x=[visual_df['ORt'].mean()], size = 1, color='black') + \ $     labs(title='Season Long Offensive and Defensive Rating for the 2016-2017 NBA Season', y='Defensive Rating', x='Offensive Rating')
df["screen_name"].value_counts().head(10)
np.all(southern_sea_level.year == northern_sea_level.year)
mean_vecs = np.mean(hidden_states, axis=1) $ max_vecs = np.max(hidden_states, axis=1) $ sum_vecs = np.sum(hidden_states, axis=1) $ train_target_emb = mean_vecs
twitter_archive.tail()
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.choice([0,1], size=n_new, p=[p_new, 1-p_new]).mean() $     old_page_converted = np.random.choice([0,1], size=n_old, p=[p_old, 1-p_old]).mean() $     p_diffs.append(old_page_converted - new_page_converted) $
FREEVIEW.plot_fixation_durations(raw_freeview_df)
kmeans = KMeans(n_clusters=2)  $ kmeans.fit(X)  
twitter_ar.head(2)
maint = pd.read_csv('maint.csv', encoding='utf-8') $ maint.head()
help(results.remove)
y.shape
test.head()
date_ny = day_ny.groupby(['day','month','year'])['count'].apply(shannon).reset_index()
list(set(df17).intersection(list(df16)))
pos_tweets = ioDF[ioDF.all_sent_x >= .5]
r.html.absolute_links
df_us_.drop(['category','creator','location','profile','deadline'], axis=1, inplace = True)
file4=file3.join(loc, file3.pole_id == loc.pole, "left_outer").drop(loc.pole) $ file4.show(3)
tf_idf = gensim.models.TfidfModel(corpus) $ print(tf_idf)
week46 = week45.rename(columns={322:'322'}) $ stocks = stocks.rename(columns={'Week 45':'Week 46','315':'322'}) $ week46 = pd.merge(stocks,week46,on=['322','Tickers']) $ week46.drop_duplicates(subset='Link',inplace=True)
clean_rates = we_rate_dogs.merge(tweet_fav_counts, how='inner', on='tweet_id').copy()
df1.dtypes
celgene_df.head()
from sklearn.model_selection import train_test_split $ X = Data['train'].drop(['store_id', 'visit_date', 'res_store_date_partial_sum-0'], axis = 1).as_matrix() $ y = Data['train'][['res_store_date_partial_sum-0']].as_matrix() $ X_learn, X_eval, y_learn, y_eval = train_test_split(X, y, test_size=0.25, random_state=42)
education_2011_2015.reset_index(inplace=True)
classes = ['actif', 'churn', 'lost'] $ conf_matrix = confusion_matrix(y_true, y_pred) $ plot_confusion_matrix(conf_matrix, classes, 'True label', 'Predicted label', title='Confusion matrix', cmap=plt.cm.Blues)
new_data = dict() $ for table in files_to_manage: $     new_data[table] = pd.DataFrame()
df_enhanced = df_enhanced.drop(df_enhanced.columns[8:12], axis=1) $ df_enhanced.head()
model.most_similar('awful')
y_test.reset_index(drop = True, inplace = True)
model_w_country = sm.Logit(df2['converted'], $                            df2[['intercept', 'ab_page', 'CA', 'UK']]) $ results_w_country = model_w_country.fit() $ results_w_country.summary()
elm.nanosecond
df['dateCreated'] = pd.to_datetime(df['dateCreated']) $ display(df.head(), df.dtypes)
Measurement = Base.classes.measurements
data.head(20)
testy.head()
ex4.drop("corn", axis = 1)
sessions['SessionDate']=pd.to_datetime(sessions['SessionDate']) $ sessions.merge(users,how='inner', left_on=['UserID','SessionDate'],right_on=['UserID','Registered'])
data['Agency'].unique()
df2.drop_duplicates('user_id', keep='first', inplace=True) $ df2[df2.duplicated('user_id')]
results = [] $ for tweet in tweepy.Cursor(api.search, geocode="-6.205537,106.83942540285217,20km").items(2500): $     results.append(tweet) $ print len(results)
df_RSV = df_full[df_full.Field4 == "RSV"]
dfFull['MasVnrAreaNorm'] = dfFull['MasVnrArea']/dfFull['MasVnrArea'].max()
new_df.head()
data_ar = np.array(data_ls) $ data = pd.DataFrame(data_ar, columns = ["fr_id","login_via_fb","wepay_status","has_benefactor", $                                         "use_video_as_main_img","title_length","desc_length","has_city", $                                         "images","videos","org_total_shares","updates","active"])
df2.loc[df2["user_id"] == 773192,]
q2 = pd.Period('2018Q2', freq='Q-JAN') $ q2
link = "https://www.jpl.nasa.gov" $ featured_image_url = link + extension
ac['Filing Date'].groupby([ac['Filing Date'].dt.year]).agg('count')
x.iloc[:3,:]
df_not4new = df.query('landing_page == "new_page"') $ df_43 = df_not4new.query('group != "treatment"') $ df_43.nunique()
logistic_mod_full = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','US', 'wk2', 'wk3']]) $ results_full = logistic_mod_full.fit() $ results_full.summary()
kinda_old_by_date = not_so_recent.sort_values(by='Days Elapsed', ascending=False) $ kinda_old_by_date.head(30)
first_values_liberia = grouped_months_liberia.first() $ first_values_liberia=first_values_liberia.rename(columns = {'National':'First_v_T'}) $ first_values_liberia.head()
save_n_load_df(joined, 'joined_holidays.pkl')
len(x), y.size
gDateEnergy_plot = gDateEnergy_content.count().unstack().plot.bar(stacked=True, figsize=(10, 10)) $ gDateEnergy_plot.set_title('Task Completion (Sorted by Energy Level)') $ gDateEnergy_plot.set_ylabel("Tasks Completed")
t=longitudinal.take(1)[0] $ print "One client id: {}".format(t.cid) $ print "First five ssd for this client id (unordered): {}".format(t.ssd[:5]) $ print "First five num_ssd for this c_id  (unordered): {}".format(t.num_ssd[:5])
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new],alternative='smaller') $
purch = pd.get_dummies(auto_new.Purchased) $ purch = purch.drop(["No"], axis=1) $ purch = purch.rename(columns={"Yes":"Purchased"}) $ purch.head()
archive_clean.timestamp = pd.to_datetime(archive_clean.timestamp)
len(dfXy['user_id'].unique())/len(dfXy)
df_archive.text[0]
model = sklearn.linear_model.LinearRegression() #use the scikit linear regression model
model.summary()
p = fp7.append(h2020) $ fp7 = 0 $ h2020 = 0
x_train = x_train.reshape((len(x_train),1)) $ print(x_train.shape, y_train.shape)
Sort1 = stores.sort_values(by = "TotalSales")
loans.columns
a=pd.DataFrame() $ a['1']=ins['score'].value_counts(normalize=True).index $ a['2']=ins['score'].value_counts(normalize=True).values $ a
donors.loc[donors['Donor Zip'].isnull(), 'Donor State'].value_counts(normalize=True).head()
df_adjusted['adjusted_numerator']=  df_adjusted.rating_numerator_y.iloc[0]* df_adjusted.rating_numerator_x/df_adjusted.rating_numerator_y
df2[['control','treatment']]=pd.get_dummies(df2['group']) $ df2[['new_page','old_page']]=pd.get_dummies(df2['landing_page']) $ df2['intercept']=1 $ df2.head()
display(df.columns, df.head())
merged_df.head()
X = df_modeling_categorized.drop('final_status', axis=1) $ y = df_modeling_categorized['final_status']
young = users[users.age<21]
len(users) $ users.drop(users[users['followers_count'] == 'followers_count'].index, inplace=True) $ len(users)
print([", ".join(pin2)])
new_page_ind = df2.query('landing_page == "new_page"').shape[0] $ print('Probability and individual got the NEW PAGE: {:.4f}'.format(new_page_ind/df2.shape[0]))
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG_max =(grouped.aggregate(np.max)['medicare_payment']) $ grouped_by_year_DRG_max.head()
test_df_01.to_pickle("dogscats_pred.pkl")
df_subset['Borough'] = df_subset['Borough'].astype(str) 
data["type"].unique()
X_train = pd.read_json("../Data//train.json").sort_values(by="listing_id") $ X_test = pd.read_json("../Data//test.json").sort_values(by="listing_id")
print(autos['odometer_km'].unique().shape) $ print(autos['odometer_km'].describe()) $ print(autos['odometer_km'].value_counts().sort_index(ascending=False))
fld = 'Promo' $ df = df.sort_values(['Store', 'Date']) $ get_elapsed(fld, 'After') $ df = df.sort_values(['Store', 'Date'], ascending=[True, False]) $ get_elapsed(fld, 'Before')
s.index
result_3 = pd.concat([df1, df3], ignore_index = True) # same as option 1 but with reset index $ result_3
cityID = '0e2242eb8691df96' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Henderson.append(tweet) 
df.index[0]
df2.user_id.drop_duplicates(inplace = True, keep = 'first')
df = pd.DataFrame({ $     'A': s, $     'B': ['a', 'b', 'c', 'd', 'e', 'f'] $ }) $ df
df_twitter_copy.loc[375].text
len(train_data[train_data.abtest == 'test'])
mismatched = ab.query('(group == "treatment" & landing_page == "old_page") | (group == "control" & landing_page == "new_page")')
df[(df['county_number'].isnull())&(df['county'].isnull())].head()
z_score, p_value = sm.stats.proportions_ztest ( [ convert_new , convert_old ] , [ n_new, n_old ] ,  alternative= 'larger' ) $ z_score
train_df.info
top10.plot.pie();
INC = pd.read_csv('s3://datapd/SS_DUN_Incidents_Dem_Zone_join.csv') $
dtc_features = sorted(list(zip(test_features, dtc.feature_importances_)), key=lambda x: x[1], reverse=True) $ dtc_features
df["booking_application"][~mask].value_counts() / df["booking_application"][~mask].count()
s_t = np.sqrt(((n1-1)*n1*sd1+(n3-1)*n3*sd3)/(n1+n3-2)) $ t = (m3-m1)/(s_t*np.sqrt(1/n1+1/n3)) $ tscore = stats.t.ppf(.95,n1+n3-2) $ print("t stats is {0}; 95% t score is {1}".format(t,tscore))
from stacking import stacking_regression $ from sklearn.metrics import mean_squared_error $ import numpy as np
df.plot(kind='scatter', x='B', y='C', title = 'Scatterplot')
price_data = heading.append(price_data) $ price_data.columns = price_data.iloc[0]
data['Time'] = pd.to_datetime(data['Time'], format='%H:%M').dt.time
res['cost'] = res['quantity'] * res['price'] $ res_agg = res.groupby('id').agg({'description': ['nunique'], 'quantity': ['sum'], 'cost': ['mean', 'sum','median','max','min']}) $ res_agg.columns = ['unique_items','total_quantity', 'mean_cost', 'total_cost','median_cost','most_exp_cost','least_exp_cost'] $ res_agg.reset_index(inplace=True) $
results = session.query(Station.station, Station.name).count() $ print(f"There are {results} stations.")
tweets_clean[tweets_clean.retweet_count == 0].index
test_scores = run(q_agent, env, num_episodes=100, mode='test') $ print("[TEST] Completed {} episodes with avg. score = {}".format(len(test_scores), np.mean(test_scores))) $ _ = plot_scores(test_scores, rolling_window=10)
bookings.columns
new_page_converted = np.random.choice([1, 0], size=len(df2_treatment.index), p=[df2.converted.mean(), (1-(df2.converted.mean()))])
image_clean['p1'] = image_clean['p1'].str.lower() $ image_clean['p2'] = image_clean['p2'].str.lower() $ image_clean['p3'] = image_clean['p3'].str.lower()
autos["model_brand"].value_counts(normalize = True).head(20)
stringlike_instance_2.content
data.area
noise = noise.drop('Unnamed: 0',axis=1)
s = BeautifulSoup(doc,'html.parser') $ print(s.prettify())
lst_tickers.remove('CMA')
prods=pd.merge(priors_product_purchase,priors_product_reordered, on='product_id') $ prods['prod_reorder_rate']=prods['reordered_count']/prods['purchase_count'] $ prods.head()
df_link_meta['link.domain_resolved'].value_counts().head(25)
stores.columns
from datetime import datetime $ def convert_date_to_datetime(string_time): $     return datetime.strptime(string_time,'%Y-%m-%d') $ df['datetime'] = df.date.map(convert_date_to_datetime)
df_new['morning']= pd.get_dummies(df_new['day_part'])['morning']
gen.c.unique()
sentiment_df.head()
print(trump_tweets[0]._json['created_at'])
p_old_std = (df2['converted']==1).std()
df = pd.concat([sanders_df, dtrump_df, jimcramer_df], ignore_index=True) # ignore index = True it will reset all the $
sox['season'] = pd.DatetimeIndex(sox.date).year
annual_returns.plot(figsize = (15, 6)) $ plt.show() $
def get_list_of_file_name(the_posts): $     list_of_file_name = [] $     for i in list_Media_ID: $         list_of_file_name.append(the_posts[i]["the_fname"]) $     return list_of_file_name   
df = df[df.sentiment != 'Neutral'] $ df['text'] = df['text'].apply(lambda x: x.lower()) $ df['text'] = df['text'].apply(lambda x: x.replace('rt', ' ')) $ df['text'] = df['text'].apply(lambda x: re.sub('[^a-zA-z0-9\s]', '', x))
MergeMonth['Date'] = MergeMonth['Date'].dt.to_period("M")
enc_weights = to_np(weights['0.encoder.weight']) $ row_m = enc_weights.mean(0)
get_freq(series_obj=raw.income_flag)
shows1 = pd.read_csv('scraped_data4.csv') $ shows1.shape
new_items = [{'pants': 30}] $ new_store = pd.DataFrame(new_items, index=['store 3']) $ new_store
snow.select ("select * from ST_ADPKD_PTS limit 5")
pd.concat([msftA[:3], aaplA[:3]], ignore_index=True)
jail_census.loc['2017-02-01'].groupby('Race')['Age at Booking'].mean()
male_journalists_mention_summary_df[['mention_count']].describe()
def save_tweets(tweets, path): $     ...
df2.query('landing_page == "new_page"').count()[0] / df2.shape[0]
df_image_clean2.info()
z_score, p_val = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ ('p-value:',p_val,'z-score:', z_score)
with pd.option_context("max.rows", 10): $     grouper = dta.groupby((dta.address, dta.dba_name)) $     print(grouper.size().describe())
Pop_df = pd.DataFrame(Population) 
%store -r keys_0430 $ keys = keys_0430.copy()
buckets_to_df(commits.get_cardinality("hash").by_period().fetch_aggregation_results()['aggregations']['0']['buckets'])
df_new.head()
dummies = pd.get_dummies(df_new.country) $ df_new = df_new.join(dummies) $ df_new.head()
print train.shape, test.shape
df_a.join(df_b, how = "inner") # inner join (see above for definition)
df_pivot1 = df_pivot1[(df_pivot1[11].notnull() & df_pivot1[12].notnull()) | (df_pivot1[14].notnull() & df_pivot1[15].notnull())] $ df_pivot1.shape
print(data.shape)
pd.concat([city_loc, city_pop], ignore_index=True)
idx = pd.IndexSlice $ df.loc[idx[:, :, 'x'], :]
from features.build_features import remove_invalid_data $ df = remove_invalid_data(pump_data_path) $ df.shape
!gunzip -c GCA_002079225.1_ASM207922v1_feature_table.txt.gz | head -n 4
df.head()
from sklearn.neighbors import KNeighborsClassifier
tizibika[tizibika['likes']==tizibika['likes'].max()]
cc['loghigh']=np.log(cc['high']) $ plt.hist(cc['loghigh']) $ plt.show()
corrplot(corr=r[coins_infund].loc[start_date:end_date].corr()) $ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date) $ plt.show()
f = lv_workspace.get_data_filter_object(step=1, subset='A') $
svm_model.fit(X_train, y_train)
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False) $ url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ browser.visit(url)
reddit = reddit.drop_duplicates(subset=['Titles', 'Subreddits'], keep='last', inplace=False) #dropping those $ reddit.head() $ reddit.shape #now I see those specific rows have been dropped $
weather_mean.iloc[5:15:3, 0]
contribs.committee_name.value_counts().reset_index()
clf = LogisticRegression() $ clf.fit(X_train, y_train)
df['intercept']=1 $ df[['control', 'ab_page']] = pd.get_dummies(df['group']) $ df = df.drop(['control'], axis=1) $ df.head()
from datetime import datetime $ date = nc.num2date(time, 'hours since 1800-01-01 00:00:0.0') $ ts = pd.Series(date, index = date) $ print(ts.head(), ts.tail())
df2 = df2.join(countries.set_index('user_id'), on='user_id') $ df2.head()
print ("The Number of time the 'UK' Conversion is better than 'CA' : {} ".format(np.exp(0.0507))) $ print ("The Number of time the 'US' Conversion is better than 'CA' : {} ".format(np.exp(0.0408)))
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2','floor']])
os.chdir(temp_path) $ data_sets = {} $ data_sets['15min'] = pd.read_pickle('final_15.pickle') $ data_sets['30min'] = pd.read_pickle('final_30.pickle') $ data_sets['60min'] = pd.read_pickle('final_60.pickle')
print() $ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
pres_df['hour_aired'].value_counts()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger') $ z_score, p_value
df.info()
globalCityRequest = requests.get(globalCity_pdf, stream=True) $ print(globalCityRequest.text[:1000])
r = pd.DataFrame(q, columns = ['cat','score']) $ r.head()
<blockquote class="instagram-media" data-instgrm-captioned data-instgrm-permalink="https://www.instagram.com/p/BeBwOoND4Cc/" data-instgrm-version="8" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:658px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);"><div style="padding:8px;"> <div style=" background:#F8F8F8; line-height:0; margin-top:40px; padding:62.5% 0; text-align:center; width:100%;"> <div style=" background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC); display:block; height:44px; margin:0 auto -44px; position:relative; top:-22px; width:44px;"></div></div> <p style=" margin:8px 0 0 0; padding:0 4px;"> <a href="https://www.instagram.com/p/BeBwOoND4Cc/" style=" color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;" target="_blank">Tattoo done by Bunny. #bunnydontinstagram #deluxetattoochicago</a></p> <p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;">A post shared by <a href="https://www.instagram.com/deluxetattoochicago/" style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;" target="_blank"> Deluxe Tattoo</a> (@deluxetattoochicago) on <time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2018-01-16T22:38:44+00:00">Jan 16, 2018 at 2:38pm PST</time></p></div></blockquote> <script async defer src="//platform.instagram.com/en_US/embeds.js"></script>
mlp = MLPRegressor(activation='relu',solver='adam', alpha=0.001, hidden_layer_sizes=(60,60,60,60,60,60,60), tol=0.00001, verbose=False, learning_rate_init=0.01, max_iter=100000000, random_state=42) $ mlp.fit(x_train,y_train)
from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.naive_bayes import MultinomialNB $ tfidf_vectorizer = TfidfVectorizer(min_df = 5, max_df = 1000)
new_page_converted=np.random.choice([0,1],n_new,p=[1-p_new,p_new]) $
new_converted_simulation.mean() - old_converted_simulation.mean()
round(df2['converted'].mean(), 4)
conditions_counts = conditions.value_counts()
df_count_clean.info()
index = similarities.MatrixSimilarity(lsi[corpus])
df = pd.DataFrame ( $     {'A': ['A1', 'A2', 'A3','A1','A3','A1'], $      'B': ['B1','B2','B3','B1','B1','B3'], $      'C': ['C1','C2','C3','C1',np.nan,np.nan]}) $ df
structured.add_datepart(date_df, 'Date', drop=False)
%%time $ dask_df.body.apply(lambda x: sleep(.0001)).head()
scores.IMDB.mean()
df_cal.apply(lambda x: sum(x.isnull()), axis = 0)
last12_df.describe()
transactions.merge(users, how='outer', on=['UserID'])
closed_issues = Issues(github_index) $ closed_issues.is_closed() $ closed_issue_age = closed_issues.fetch_results_from_source('time_to_close_days', 'id_in_repo', dataframe=True) $ print(closed_issue_age)
experiment_run_details = client.experiments.get_run_details(experiment_run_uid) $ training_run_uids = client.experiments.get_training_uids(experiment_run_details) $ for i in training_run_uids: $     print(i)
p_old = df2[(df2.group == 'control')&(df2.converted == 1)].shape[0]/df2[df2.group == 'control'].shape[0] $ p_old
df_indices = df_sp_500.unionAll(df_nasdaq).unionAll(df_russell_2k) $ df_indices.sample(False, 0.01).show()
OldPage = df2.query('landing_page=="old_page"')['user_id'].count() $ OldPage
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth) $     return api
from unidecode import unidecode $ def get_entitie(obj, key, name): $     return [unidecode(entity.get(key, u'').lower()) for entity in obj[name]] $ hashtags_series = df.entities.apply(lambda obj: get_entitie(obj, 'text', 'hashtags')).dropna() $ mentions_series = df.entities.apply(lambda obj: get_entitie(obj, 'screen_name', 'user_mentions')).dropna()
url_CIN = "https://bengals.seasonticketrights.com/Images/Teams/CincinnatiBengals/SalesData/Cincinnati-Bengals-Sales-Data.xls"
sqlite> SELECT key, COUNT(key) as count $    ...> FROM nodes_tags $    ...> GROUP BY key $    ...> ORDER BY count DESC $    ...> LIMIT 20;
resdf.iloc[:,114:129].to_sql('demotabl',conn)
df_img_predictions.tweet_id.nunique()
history = model.fit(X_dense_train, y_train, epochs=30, validation_split=0.2, batch_size=64)
print('Multinomial Naive Bayes Accuracy:', nltk.classify.accuracy(MNB, test_set) * 100, '%')
data_dummies.to_csv('invoices_dummies.csv')
df2.info()  
old_page_converted = np.random.binomial(n_old , p_old)
station_distance.shape $ station_distance.columns $ station_distance.tail()
os.listdir()
r1 =first_cluster.model.print_topics() $ r1
grinter1.number_int.max()
telemetry.describe()
cmask = hdf.index=='child' $ hdf.loc[cmask, 'Age'].head()
significant_features.remove('')
%%time $ _dict_word_count = {} $ for sentence in dfRegMet["sentences"]: $     for word in sentence: $         _dict_word_count[word] = _dict_word_count.get(word,0) + 1
count6df = pd.DataFrame(kochbar10) $ count6df = count6df.drop_duplicates(subset=['name', 'user']) $ count6df.info()
odm = pd.read_csv('data/odm2.csv')
ts.shift(-2)
weather = weather[weather.zip_code == 94107]
numPurchU = train.groupby(by='User_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesU'}) $ train = train.merge(numPurchU, on='User_ID', how='left') $ test = test.merge(numPurchU, on= 'User_ID', how='left')
X_train.head()
plt.plot(bitcoin_df['price'])
df_t["courses"] = df_t["course_key"] + ', ' + df_t["nd_key_formatted"] $ df_t.head()
r.json()
new_read['is_read'] = (new_read['read_time_x']/new_read['read_time_y'] > 0.7).astype(int)
bow_df = pd.DataFrame(bagofwords.todense()) $ tfidf_df = pd.DataFrame(tfidfdata.todense())
INDEX = np.random.choice(len(text[mask]), 5000)
df_ud889 = pd.read_sql(sql_ud889,conn_hardy) $ df_ud889.groupby(['course_enrolled','course_enrollment','converted'])['applicant_id'].count()
weather_all.info() # Double check
groceries.drop('apples', inplace=True) $ groceries
df2 = df.copy() $ df2 = df2.query('(landing_page == "new_page" & group == "treatment") \ $ | (landing_page == "old_page" & group == "control")') $ df2.info()
non_cancel_df = data_df[~(data_df['delay_time'] == "Cancelled")].copy() $ non_cancel_df['is_delayed'] = non_cancel_df['delay_time'].apply(lambda x: float(x) >= 3.0) $ non_cancel_df = non_cancel_df.sort_values(['Departure', 'Arrival', 'Airline', 'flight_year', 'flight_month', 'flight_day', 'std_hour'],ascending=False)
data.shape
data_vi[['VIOLATIONS']].plot(figsize=(8,4), $                                style=['-']);
y_pred_mdl = mdl.predict(X_test) $ y_train_pred_mdl=mdl.predict(X_train) $ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(mdl.score(X_test, y_test)))
avg_da = pd.DataFrame('AVG_DA':raw_df.groupby('Date')['DA-price'].mean().values) $ avg_da
tweet_ids_image_predictions.shape[0]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print('The z_score is {} and the p value is {}.'.format(z_score,p_value))
prop.head()
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit.fit() $
logit_mod4 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'UK', 'ab_UK']]) $ results4 = logit_mod4.fit() $ results4.summary()
cX_test.drop(['Thai', 'Italian', 'Indian', 'Chinese', 'Mexican', 'Text_length'], axis=1, inplace=True)
df2.info()
df.seqid.unique().shape
print((p_diffs > difference).mean())
df2.plot.
prng.to_timestamp('M', 'e')
(ggplot(raw_large_grid_df.groupby(['eyetracker','posx','posy'],as_index=False).mean(),aes(x="posx",y="posy",size="rms"))+geom_point()+facet_wrap("~eyetracker"))+coord_fixed()
code_n_name = session.query(Stock.stock_code, Stock.company).all()
print(result.f_test('SP500=0, Intercept=0'))
df_merge = pd.merge(df_schoo11, df_students1, on='school_name') $ df_merge.drop(['School ID', 'Student ID'], axis = 1, inplace=True)
bild.head(3)
df_rand['ch_json'] = df_rand.name.apply(search_org) $ df_rand['ch_postcodes'] = df_rand.ch_json.apply(ch_postcodes)
df['intercept'] = 1 $ df['ab_page'] = 0 $ df['ab_page'] = df.apply(lambda row: get_ab_page(row), axis = 1) $ df.head()
plt.hist(drt_avg17, bins=20, align='mid'); $ plt.xlabel('Defensive Rating') $ plt.ylabel('Count') $ plt.title("Histogram of Teams' Defensive Rating, 2017-2018 Season\n");
print("Unique bikes that are in this dataset: {}".format(len(data.imei.unique())))
datatest.loc[datatest.place_name == 'Abril Club de Campo', 'lat'] = -34.802221 $ datatest.loc[datatest.place_name == 'Abril Club de Campo', 'lon'] = -58.164291
views = containers[0].find("li", {"class":"views"}).contents[1][0:3] $ views
mask = combined.country.isin(['Dominican Republic', 'Colombia']) $ combined.loc[mask, 'zika_cases'] /= 10
list_of_exclusion_words = list(tweet_archive_clean[tweet_archive_clean.name.map(lambda x: not x[0].isupper())].name.value_counts().index) $ list_of_exclusion_words.extend(['None'])
mars_html_table = mars_table.to_html() $
parse_dict['profile'].head(5) $
img_path = os.getcwd()+ '/images/'
df_events['prefecture'] = df_events.prefecture.map(prefecture_dict) $ df_users['prefecture'] = df_users.prefecture.map(prefecture_dict) $ df_users['gender'] = df_users.gender.map(gender_dict) $ df_log['payment_method'] = df_log.payment_method.map(payment_method_dict) $ df_events['interest'] = df_events.interest.map(interest_dict) $
from pathlib import Path $ data_dir = Path('data') $ my_zip.extractall(data_dir)
for c in ccc[:2]: $     for i in spp[spp.columns[spp.columns.str.contains(c)==True]].columns: $         spp[i] /= spp[i].max()
etsamples_100hz = etsamples_100hz.query("smpl_time>%.2f"%(max(etsamples_100hz.groupby(["eyetracker"]).smpl_time.agg(min))))
new_page_converted=np.random.binomial(1,pnew,nnew) $ s_pnew=new_page_converted.mean()
tsla_neg_cnt = tsla_neg.count()*100 $ print "{:,} users have no activity before {} ({:.2%} of DAU)"\ $       .format(tsla_neg_cnt, D0.isoformat(), tsla_neg_cnt*1./dau)
pt.to_csv('C:\\Users\\ChandraMouli\\Desktop\\summary.csv')
print("Number of edited comments: ",len(df['edited'].value_counts()))
df.head(2)
train[target].value_counts()
from sagemaker.predictor import csv_serializer, json_deserializer $ linear_predictor.content_type = 'text/csv' $ linear_predictor.serializer = csv_serializer $ linear_predictor.deserializer = json_deserializer
import statsmodels.formula.api as sm $ result = sm.ols(formula="pr_rf ~ mr_rf", data=tbl).fit() $ result.summary()
filter_df['start_time'].min(), filter_df['start_time'].max()
df.head(10)
autos['kilometer'].value_counts()
digitalgc_tweets = api.search(q="#digitalgc", $                               count=100, lang="en", $                               since="2018-01-01") $
tweet = result[0] $ for param in dir(tweet): $     if not param.startswith("_"): $         print ("%s : %s\n" % (param, eval('tweet.'+param)))
df = pd.DataFrame.from_dict(dst_cap, orient="index") $ df.columns = ['Fequency']
department_df_sub.apply(np.sum, axis = 1)# apply function to each row 
one_station['week_num']=week_num $ one_station.head(20)
df.tail(3)
stations_total = (stations.groupby('STATION')['DAILY_ENTRIES'].sum().reset_index().sort_values("DAILY_ENTRIES", ascending=False)) $ stations_total.head()
df2=df.query('misaligned==False')
sns.distplot(train.loan_amnt);
urls["domain"].value_counts()
reddit['Morning Hours'] = reddit['Hours'].apply(lambda x: 1 if x<9 and x>4 else 0)
df = pd.read_csv('weather_data_austin_2010 (1).csv') $ df.head()
data.T
df['score'].corr(df['comms_num'])
df_members.info()
dbdmh = dmh.DBAPI(sqlite3, c, nrecs=10)
df["booking_user_agent"][~mask].value_counts() / df["booking_user_agent"][~mask].count()
combined_df4.keys()
data.isnull().sum()
def cat_preproc(dat): $     return cat_map_fit.transform(dat).astype(np.int16) $ def contin_preproc(dat): $     return contin_map_fit.transform(dat).astype(np.float32)
df.median() - 1.57 * (df.quantile(.75) - df.quantile(.25))/np.sqrt(df.count())
from sklearn.metrics import classification_report $ print(classification_report(y_test, y_pred))
bptt,em_sz,nh,nl = 70,400,1150,3 $ vs = len(itos) $ opt_fn = partial(optim.Adam, betas=(0.8, 0.99)) $ bs = 48
driver = webdriver.Chrome(executable_path="./chromedriver")
trn_df = train_data[usecols] $ val_df = val_data[usecols] $ trn_y = train_data.is_attributed $ val_y = val_data.is_attributed
crimes.columns = crimes.columns.str.replace(' ', '_') $ crimes.columns
genre_vectors.shape
data.plot(x='batch_start_time', y='error', kind='line')
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.axvline(x=obs_diff, color='green'); $
allVars = read.getVariables() $ variables_df = pd.DataFrame.from_records([vars(variable) for variable in allVars], index='VariableID') $ variables_df.head(10)
INQ2016.columns
guineaFileList = glob.glob("Data/ebola/guinea_data/*.csv") $ frameList = [pd.read_csv(file,usecols=['Description','Date','Totals'],index_col=['Description','Date']) for file in guineaFileList] $ len(guineaFileList)
db.fetch_all_data()
recipes.iloc[0]
future = m.make_future_dataframe(periods= int(len(test_p) * 1.1),freq= '1MIN')
df = df[df["name_region"].str.contains("Uppsala")] $ display(df.head())
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','OriginationPhase']).closed_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True, grid=False) $
df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})
data3.head()
data_sources = lift.get_all_data_sources()
pd.read_csv("msft.csv", nrows=3)
pd.read_csv("Data/microbiome.csv", header=None).head()
def print_rmse(model, name, input_fn): $   metrics = model.evaluate(input_fn=input_fn, steps=1) $   print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['loss'])) $ print_rmse(model, 'validation', get_valid())
df_user = pd.read_csv("Data/yammer_users.csv") $ df_event = pd.read_csv("Data/yammer_events.csv") $ df_email = pd.read_csv("Data/yammer_emails.csv") $ df_period = pd.read_csv("Data/dimension_rollup_periods.csv") $
ts = pd.Series(np.random.randn(3), index=dr)
result = pd.DataFrame(list(results), columns=['tags', 'values']) $ result
s.value_counts()
properati[properati['zone'] == ""]['place_name'].value_counts(dropna=False)
autos['odometer'] = autos['odometer'].str.replace('km', '').str.replace(',', '').astype(int)
user.head(3)
df_final[df_final['cluster'] == 0].head()
plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101[(df_nd101['ud730']>=0)&(df_nd101['ud730']<=1)].ud730)
loan_stats.set_name('loan_status', 'loan_result') 
mb.swaplevel('Patient', 'Taxon').head()
posts.count()
from scipy.stats.mstats import winsorize $ train_df['price'] = winsorize(train_df['price'], limits=[0.01, 0.01]) $ sns.distplot(train_df.price.values, bins=100, kde=True)
importances = pd.DataFrame(xgb.feature_importances_) $ importances['labels'] = X_train.columns $ importances.sort_values(0, ascending=False)
auth = tweepy.OAuthHandler(consumerKey, consumerSecret) $ api = tweepy.API(auth)
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ results = log_mod.fit() $ results.summary()
from sklearn import model_selection, preprocessing, ensemble $ from sklearn.metrics import log_loss $ import xgboost as xgb
new_page_converted.sum()/len(new_page_converted) - old_page_converted.sum()/len(old_page_converted) $
total = df.shape[0] $ df[df.converted == 1].shape[0]/total
data['temp']=data['Date'].astype(basestring)+'-1999' $ data.head(5) $
train_small_data.is_attributed.mean(), val_small_data.is_attributed.mean()
X = reddit['Titles'] $ y = reddit['Above_Below_Median'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
newdf.reset_index(inplace=True)
SCN_BDAY.head()
pprint.pprint(treaties.find_one({"_id": treaty_id}))
Google_stock.isnull().any()
stock = pd.DataFrame(np.log(data_tickers['adj_close'][ticker]).diff().fillna(0)) $ ts_mean = ts_mean.join(stock) $ ts_mean.head()
print(data.columns)
plt.rcParams['figure.figsize'] = (15,5) $ timezones.plot(kind='bar')
YS1517.Close.plot(kind='kde')
model_preds = pd.DataFrame(index=docs_string)
print(df.shape) $
n_old = df2[df2['landing_page']=='old_page'].count()[0] $ n_old
pax_raw['minute_of_day'] = pax_raw.paxhour*60 + pax_raw.paxminut $ pax_raw.head()
toggl = TogglPy.Toggl() $ toggl.setAPIKey(APIKEY) 
len(df_meta['collection'].unique())
results = logit.fit() $ results.summary()
X_train['age'] = df_imputed.iloc[:,0].values
p_old = (old_page_converted == 1).sum()/n_old $ p_new = (new_page_converted == 1).sum()/n_new $ p_new - p_old
merge[merge.columns[48]].value_counts()
users = users[users.country_destination != 'NDF']
df2_control=df2.query('group=="control"') $ p_control=df2_control['converted'].sum()/len(df2_control) $ p_control
autos['last_seen'].str[:10].value_counts(normalize = True, dropna = False).sort_index()
converted_old = df2.query("group == 'control' and converted == '1'").count()[0]/df2.query("group == 'control'").count()[0] $ converted_old
from cfg import rebalance_month $ importlib.reload(util) $ tagged_df = util.tag_rebalance_date(target_pf, period = rebalance_month.rebalance_period)
npath = out_file3 $ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
df2 = df.drop(df.query('(group == "treatment" and landing_page == "old_page") or (group == "control" and landing_page == "new_page")').index)
ben_fin['stiki_percent']=ben_fin['stiki_mean']<0.1
leadDollarsClosedPerMonth = segmentData[['opportunity_won_amount', 'opportunity_month_year', 'lead_source']].pivot_table( $                                 values='opportunity_won_amount', $                                 index='opportunity_month_year', $                                 columns='lead_source', aggfunc='sum')
INT.loc[:,'MONTH'] = INT['Create_Date'].apply(lambda x: "%d" % (x.month))
from bs4 import BeautifulSoup $ r = requests.get('http://www.aflcio.org/Legislation-and-Politics/Legislative-Alerts') $ page = r.content $ page[:1000]
autos['odometer_km'].unique().shape
plt.xticks(rotation=25) $ plt.plot(data['created_at'],data['rate'],'b.',alpha=0.5) $ plt.plot(data['created_at'],data['timestamp']*slope + intercept,'r-', linewidth=3) $ plt.show()
save_n_load_df(df, 'filled_elapsed_events_df.pkl')
major_cities_l1_features = major_cities_l1_fset.features $ len(major_cities_l1_features)
test.head(1)
negatives = month['PERIOD ENTRIES'] < 0 $ print negatives.sum() $
data.loc[data.surface_total_in_m2 < 1, 'surface_total_in_m2'] = np.NaN
menu_dishes_about_latent_features_with_menu_ids = np.insert(menu_dishes_about_latent_features, 0, np.array([menu_dishes_to_analyze.menu_id.values]), axis=1) $
cr_under_null = df2['converted'].mean()    # Under the null, use both groups. $ cr_under_null $
df_crea = parse_dict['creator'].drop(['avatar','id','slug','urls'],axis=1).rename(columns={'name':'creator_name', $                                                                                  'is_registered':'creator_registered'})
week51 = week50.rename(columns={357:'357'}) $ stocks = stocks.rename(columns={'Week 50':'Week 51','350':'357'}) $ week51 = pd.merge(stocks,week51,on=['357','Tickers']) $ week51.drop_duplicates(subset='Link',inplace=True)
if metadata['data_ignore_value'] in reflRaw: $     print('% of Points with No Data: ', $          np.round(number_of_values_where_NoData * 100 / total_number_of_values_in_array,1)) $     nodata_ind = np.where(reflClean == metadata['data_ignore_value']) $     reflClean[nodata_ind] = np.nan
tag_df= pd.get_dummies( $     tag_df.stack()).sum(level=0)
from sklearn.linear_model import LogisticRegression $ LR_model = LogisticRegression(C=0.01).fit(X_train,y_train) $ LR_model
results2 = reg.fit()
train_geo.columns
y_predict = knn_model.predict(X_test)
final_df_columns=["Date" , "Day", "Month", "Country" , "Number of new cases" , "Total number of deaths"] $ final_df=pd.DataFrame(columns=final_df_columns)
sum(df.duplicated())
metadata_df = pd.read_excel(DATA_FOLDER+'/microbiome'+'/metadata.xls') $ metadata_df
cleaned_sampled_contirbutors_human_df_saved = non_blocking_df_save_or_load( $     cleaned_sampled_contirbutors_human_df, $     "{0}/human_data_cleaned/sampled_contirbutors_cleaned".format(fs_prefix)) 
df_twitter.text.loc[767]
list(df_.dropna(thresh=int(df_.shape[0] * .9), axis=1).columns) #set threshold to drop NAs
type(prcp_analysis_df.precipitation[0])
my_query = "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES" $ my_result = limsquery(my_query) $ my_result
API.rate_limit.get_limit("/statuses/show/:id")
accuracy = accuracy_score(y_test, y_pred) $ print('Accuracy: {:.1f}%.'.format(accuracy * 100.0))
1/np.exp(-0.0469), np.exp(0.0134), 1/np.exp(-0.0206), 1/np.exp(-0.0175), 1/np.exp(-0.0057)
(p_diffs > diff_obs).mean()
df_cen = pd.merge(df_tot, df_geo, on='Tract', how='outer') $ df_cen.sample(5)
dfChile = dfChile[dfChile["latitude"] < -17.000000]
merged = pd.merge(pop, abbrevs, how='outer', $                  left_on='state/region', right_on='abbreviation') $ merged = merged.drop('abbreviation', 1) # drop duplicate info $ merged.head()
df2[((df2.group == 'treatment') == (df2.landing_page == 'new_page')) == False].shape[0]
train.created.describe()
calls_df[calls_df["call_type"]=="Ignore"].count()
exportID['avg_duration'] = exportID['event.longSum_sum_duration']/exportID['event.longSum_total_records']
df.tail() $
vals = bacteria_data.value.copy() $ vals[5] = 1000 $ bacteria_data
svc = SVC() $ scores = cross_validate(svc, X_tfidf, y_tfidf, cv=10, n_jobs=-1, return_train_score=True) $ print ('The mean of test_accuracy of the model with the default hyperparameter of SVC model:') $ scores['test_score'].mean()
list(filter(lambda x:"commit" in x, jdf.columns))
autos['price'].value_counts().head()
ab_groups = pickle.load(open("ab_groups.p", "rb"))
gMapAddrDat.buildOutDF(tst_lat_lon_df[0:50])
auto_new.head()
measurement_results = session.query( $     Measurements.station, Measurements.date, Measurements.prcp, Measurements.tobs).all()
cleaned = cleaned.dropna(subset=['facname','address']) $ cleaned.info()
print('Most positive tweets:') $ for t in senti.sort_values('polarity', ascending=False).head()['text']: $     print('  ', t)
table_1c = table_1c.where(table_1c[u'Collected from port'] < store_time)
small_train = train[train['is_booking'] == 1] $ small_train.shape
idx = df_providers[ (df_providers['id_num']==10001)].index.tolist() $ print('length of IDX',len(idx)) $ assert len(idx) > 0, 'Length of IDX is NULL' $ print( df_providers.loc[idx[0],'name'] ) $
learning_periods = 255 $ rebalance_frequency = 5 $ agent.learn(learning_periods, rebalance_frequency)
temp_df2.sort_values(by='timestamp', ascending=True).head()
plt.scatter(X2[:, 0], X2[:, 1], c = labels, cmap='rainbow'); $ plt.colorbar()
inspector = inspect(engine) $ columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"])
outputFileHeaders = os.path.join(edfDir, "fileheaders.csv") $ outputChanHeaders = os.path.join(edfDir, "chanheaders.csv") $ outputannotations = os.path.join(edfDir, 'annotations.csv') $ edfmetatocsv(inputFile, outputFileHeaders, outputChanHeaders, outputannotations, VERBOSE=True)
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ print(act_diff)
df_ab.info() #From the output we know that the answer is no, all the rows are complete
jobs_data.nunique() $
s_lyc_repeats_gff = "%s/ITAG3.2_RepeatModeler_repeats_light.gff" % DATA_PATH $ s_lyc_genome_fasta = "%s/S_lycopersicum_chromosomes.3.00.fa" % DATA_PATH $ s_lyc_repeats_fasta = "%s/ITAG3.2_RepeatModeler_repeats_light.fasta" % OUT_PATH
X_train_matrix = cvec.fit_transform(X_train)
no_psc_top_of_chain = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*0..]-(c2:Company)-[:STATES]-(s:Statement)\ $ WHERE s.statement CONTAINS 'no-individual-or-entity-with-signficant-control'\ $ RETURN DISTINCT (c1.company_number)").data()) $ print(len(no_psc_top_of_chain)) $ print("Additional companies due to network effect: " + str(len(no_psc_top_of_chain) - len(active_psc_statements[active_psc_statements.statement.str.contains('no-individual-or-entity-with-signficant-control')].company_number.unique())))
resumePath = "resumes/jacky.txt"
testo.detect_language()
festivals_clean['Start_Date'] =  pd.to_datetime(festivals_clean['Start_Date'])
google_stock.head()
df.iloc[ [0,3] ]    # by row numbers
def reindexer(df): $     return df.reindex_axis(sorted(df.columns), axis =1) $ v_item_hub = reindexer(v_item_hub) $ item_hub = reindexer(item_hub)
df2['E'] =le.transform(df2['E'])
engine = create_engine("sqlite:///Resources/hawaii.sqlite", echo=False)
times = pd.DatetimeIndex(df_en['date']) $ grouped = df_en['text'].groupby([times.hour, times.minute]).agg(['count']) $ grouped.plot() $ price_df.plot()
Z = np.random.randint(0,2,(6,3)) $ T = np.ascontiguousarray(Z).view(np.dtype((np.void, Z.dtype.itemsize * Z.shape[1]))) $ _, idx = np.unique(T, return_index=True) $ uZ = Z[idx] $ print(uZ)
pd.set_option('max_colwidth', 100)
exiftool -csv -createdate -modifydate cisuabg7/cisuabg7_cycle1.MP4 cisuabg7/cisuabg7_cycle4.MP4 cisuabg7/cisuabg7_cycle6.MP4 > cisuabg7.csv
df_326_table = pd.read_sql(sql_326_table,conn_laurel) $ df_326_table.sort_values(by='Values',ascending=False)
QUIDS_wide["y"] = QUIDS_wide[['qstot_12','qstot_14']].apply(lambda x: x['qstot_14'] if np.isnan(x['qstot_12']) $                                                             else x['qstot_12'], axis=1)
rt.head()
countries_df.head()
INC.shape
tabulation.index
pd.concat([s1, s2], verify_integrity=True)
xmlData.set_value(89, 'address', u'290th Avenue Southeast, Hobart, King County, Washington, 98025, United States of America') $ xmlData.loc[89, 'address']
print(df.info()) $ print('\nNo missing rows in dataframe.')
pd.set_option('display.max_columns', None) $ customers.head()
def tokenize(text): $     tokens = re.split('\W+', text) $     return tokens $ infinity['text_tokenized'] = infinity['text_clean'].apply(lambda x: tokenize(x.lower()))
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn as sns
df_times['Time of daily highs'].value_counts().max()
df1.tail(5)
df_city_restaurants.select('name','city','categories').limit(10).toPandas()
actual_payments.columns
geo_enable = dataset['User_Geo_Enabled'] $ chart_title = 'Proportion of user has enabled the possibility of geotagging of their tweets' $ geo_enable.value_counts().plot.pie(label='', fontsize=11, autopct='%.2f', figsize=(5, 5), title=chart_title, legend=True);
plt.scatter(xs,ys,color='#003F72') $ plt.plot(xs, regression_line) $ plt.show()
tweet_archive_master.columns
cc.volume.describe()
pd.DataFrame(df.groupby(['Parcel Status Id','Parcel Status'])['Announced At'].count())
df_hour['hour'] = map(lambda x: x.hour, df_hour.created_at)
meals = pd.read_csv('./raw_data/meals.csv') $ meals['Meal Categories'].fillna('', inplace=True) $ meals['Cuisine Type'].fillna('', inplace=True)
df2['intercept']=1 $ df2[['control', 'ab_page']]=pd.get_dummies(df2['group']) $ df2.drop(labels=['control'], axis=1, inplace=True) $ df2.head()
interactions1Data.interaction_type.unique()
mydict = r.json()['dataset'] $ dict(list(mydict.items())[0:10])
act_diff =  (prob_treat-prob_control).mean() $ print(act_diff)
P_old = df2.converted.mean() $ print("The convert rate for p-old under the null is {}.".format(P_old))
imagelist = $ [ 'BARNES JEWISH HOSPITAL title_page.pdf', $  'BARNES JEWISH HOSPITAL ALL DRGs.pdf', $  'BARNES JEWISH HOSPITAL  Top 10 DRGs -    2015  Pay vs. Discharges .pdf', $  'BARNES JEWISH HOSPITAL 260032.pdf']
logreg = LogisticRegression() $ logreg.fit(X_train, y_train) $ logreg.score(X_test, y_test)
df[df.num_likes == max(trumpint.num_likes)]
bacteria_data.index[0] = 15
RF.score(X_train,Y_train)
X_train, X_test, y_train,y_test = train_test_split(post_list, $                                                    np.array(r6s["popular"]),train_size=0.8,random_state=42)
for row in nps.itertuples(): $     if row.GNIS_ID == '302659': $         print (row.PARKNAME + '\n') $         print (str(row.geometry))
selection3 = ['Fund', 'Bitcoin', 'Litecoin', 'Ripple'] $ frac_volume_m.loc['2014':, selection3].plot() $ plt.ylabel('Fraction of volume') $ plt.title('Fund captures more volume') $ plt.show()
df = df.drop(['mintempm', 'maxtempm'], axis=1) $ X = df[[col for col in df.columns if col != 'meantempm']] $ y = df['meantempm']
store_info = pd.pivot_table(df, index=['Store Number', 'City'], values=['Zip Code', "County Number"]) $ store_info.reset_index(inplace=True) $ store_info.set_index('Store Number', inplace=True) $ store_info
dtc = DecisionTreeClassifier() $ param_grid = {'max_leaf_nodes': np.arange(2,50000, 1000)} $ CV_dtc = GridSearchCV(dtc, param_grid) $ CV_dtc.fit(X_train, y_train) $ CV_dtc.best_params_
'NORDITROPIN' in fda_drugs.DrugName.values
indeed.columns
Lab7_RevenueEPS = Lab7_RevenueEPS0.drop('Dividends', axis=1)
temps_maxact = session.query(Measurements.station, Measurements.tobs).filter(Measurements.station == max_activity[0], Measurements.date > year_before).all()
finals[finals.pts_l==1].shape
tx = tx[tx["filerTypeCd"] == 'COH'] $ tx = tx.rename(columns={"filerHoldOfficeDistrict": "district"})
tickers = ["VALE3.SA", "PETR4.SA", "ITUB4.SA", "BBAS3.SA", "LREN3.SA"] $ start = '2010-01-01' $ end = '2017-11-14' $ data = pdr.get_data_yahoo(tickers, start=start, end=end)['Adj Close'] $ data.plot(); $
df_archive_clean.head()
df_new2=df.query('landing_page=="old_page" & group=="treatment" ') $ df_new2.tail(10) $ df_new2.nunique()
ab_data.timestamp.describe()
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page'")['converted'].sum() $ convert_new = df2.query("landing_page == 'new_page'")['converted'].sum() $ n_old = df2.query("landing_page == 'old_page'").shape[0] $ n_new = df2.query("landing_page == 'new_page'").shape[0]
df_int = (df / df.shift(1)) - 1 $ df_int.iloc[0] = 0 $ df_int.head()
model.most_similar("awful")
df.pivot_table(values='(kW)', index='YEAR', columns='Make', aggfunc=[np.mean,np.min], margins=True)
pizza_poor_reviews.head(2)
type(pd.crosstab(index=mydf.comp, columns='counter'))
scores.head()
df_temp_by_USC00519281 = pd.read_sql_query("select tobs from measurement where station = 'USC00519281' AND date > '2016-08-22';", engine) $ df_temp_by_USC00519281
tb_melt = pd.melt(tb, id_vars=['country', 'year']) $ tb_melt['gender'] = tb_melt.variable.str[0] $ tb_melt['age_group'] = tb_melt.variable.str[1:] $ print(tb_melt.head()) $
summary_bystatus = df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9]).T $ summary_bystatus
from scipy.stats import norm $ print(norm.cdf(z_score)) #significance of z-score $ print(norm.ppf(1-(0.05/2))) #critical value at 95% confidence
df_elect.head()
test=dict(r.json())
cars.loc[cars.yearOfRegistration < 1960].count()['name'] $
tips.sort_values(["sex", "day"]).set_index(["sex", "day"]).head(12) $
from pyspark.sql.functions import monotonically_increasing_id $ modeling1 = (train $              .withColumn('id', monotonically_increasing_id()) $              .drop('click_time', 'attributed_time')) $ print("modeling size: ", modeling1.count())
daily.asfreq('H',method='ffill')
print(r.json()['dataset_data']['column_names']) $ print(r.json()['dataset_data']['data'][0]) $ print(r.json()['dataset_data']['data'][-1])
points = pd.read_csv("points.csv") $ points.head()
from carto.tables import TableManager $ help(TableManager) $ table_manager = TableManager(auth_client) $
df['stemmed'] = df["message_tokens"].apply(lambda x: [stemmer.stem(y) for y in x])
honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['Time stamp'].str.split("mhn").tolist(), columns = ['time_stamp1','time_stamp2','time_stamp3'])],axis = 1) $ honeypot_df = pd.concat([honeypot_df,pd.DataFrame(honeypot_df['time_stamp3'].str.split("T").tolist(), columns = ['date','time'])], axis = 1)
cX_test['prob'] = M_NB_model.predict_proba(X_test_term)[:,1]
All_tweet_data_v2.expanded_urls.fillna("Unknown", inplace=True)
temp_df2.sort_values(by='timestamp', ascending=False).head()
archive_df.sample(5)
dateCounts = pd.DataFrame(all_dates.value_counts().reset_index()) $ dateCounts.columns = ['dates','countsOnDate'] $ dateCounts.head()
lm_country = sm.OLS(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ country_results = lm_country.fit() $ country_results.summary()
output= "SELECT * from user where user_id='@Pratik'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Followers','Following','TweetCount'])
yc_trimmed = pd.read_csv('http://cosmo.nyu.edu/~fb55/data/yc200902_tripdata_trimmed.csv') $ yc_trimmed.head()
n_old = df2[df2.landing_page == 'old_page']['user_id'].count() $ n_old
train = train.reset_index(drop=True) $ train = train.merge(p,left_index=True,right_index=True)
prediction_modeling2 = lr_best_model.transform(hashed_modeling2).cache() $ prediction_modeling2.show(1, truncate=False)
apiv.head()
dfh.columns
type(df), len(df), df.shape
evaluator = keras_entity_recognizer.evaluate(predictions)
columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"]) $
dfjoined.head()
autos["price"] = autos["price"].str.replace('$','').str.replace(',','').astype(float) $ autos["odometer"] = autos["odometer"].str.replace('km','').str.replace(',','').astype(float) $ autos.rename(columns={"odometer":"odometer_km"}, inplace=True) $ print(autos[["price","odometer_km"]])
for col in var_cat: $     cats = taxi_sample[col].unique()[1:]  # drop first $     taxi_sample = taxi_sample.one_hot_encoding(col, prefix=col, cats=cats) $     del taxi_sample[col]
topic_ratings_Tesla_tweets = [] $ for i in range(len(corpus_Tesla)) : $     ratings = ldamodel_Tesla.get_document_topics(corpus_Tesla[i]) $     topic_ratings_Tesla_tweets.append(ratings) $ topic_ratings_Tesla_tweets[0:5] 
.5 / 1.6
df = pd.DataFrame(rng.randint(0, 10, (3, 4)), $                  columns=['A', 'B', 'C', 'D']) $ df
res = requests.get('http://elasticsearch:9200') $ r=json.loads(res.content) $ r
fig = sns.countplot(x = 'weekday', hue = 'subscription_type', data = trip_data) $ plt.xticks(np.arange(0,7),['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
lq[('Category')] = lq[('Category')].astype(int)
df_goog.plot(y = 'Close')
df_archive_clean["source"] = df_archive_clean["source"].replace('<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>', $                                                                "Twitter for iPhone")
store_states.head()
df['Created Date'] = pd.to_datetime(df['Created Date'])
tok_trn = np.load(LM_PATH / 'tmp' / 'tok_trn.npy') $ tok_val = np.load(LM_PATH / 'tmp' / 'tok_val.npy')
crs = {'init': 'epsg:4326'} $ geometry = df_geo_segments['geometry'] $ geo_segments_all = gpd.GeoDataFrame(df_geo_segments, crs=crs,geometry = geometry)
np.exp(-1.7227), np.exp(-1.3968), np.exp(-1.4422) $
reviewsDF = pd.read_csv('ABT.csv', index_col=False, encoding='UTF-8')
twitter_archive_clean.head(1)
def clean_tweet(tweet): $     return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())
m3 = np.dot(np.transpose(m2), m)  #taking the dot product of the A and B $ print("m3: ", m3)
X.shape
print(multi.shape, pp.shape)
apache_people_df.schema
training.head()
df_int = df.copy() $ df_int[1:] = (df[1:] - df[:-1].values) / df[:-1].values $ df_int.iloc[0] = 0 $ df_int.head()
p_new = df2['converted'].mean() $ p_new
pd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)
sns.pairplot(intervention_train, hue='target')
df.month[:5]
df_sb['cleaned_text'] = df_sb['text'].apply(lambda x : text_cleaners(x))
import functions.init_logger $ import logging $ logger = logging.getLogger(__name__) $ logger.setLevel(logging.CRITICAL)
to_be_predicted_Day1 = 51.60 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df_carto.to_csv('obike_carto.csv')
names = d[d.name != 'None']
df_concat.rename(columns={"likes.summary.total_count" : "likes_total", $                           "comments.summary.total_count" : "comments_total" }, inplace = True)
html = browser.html $ soup = bs(html, 'html.parser')
display(data[['error', 'relative_error']].describe(include='all'))
max((maxi - stock).abs())
df_small.info()
df2.timestamp = pd.to_datetime(df2.timestamp) $ df2.sort_values(['timestamp'], inplace=True) $ df2.iloc[-1, 1] - df2.iloc[0,1]
answer.head()
obj.sort_index()
data.patient
bigram = gensim.models.Phrases(text_list) $ bigram_text = [bigram[line] for line in text_list]
def day_of_week(date): $     days_of_week = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'} $     return days_of_week[date.weekday()] $ day_of_week(lesson_date)
new_page_converted=np.random.binomial(nnew,nullrate) $ new_page_converted
with open('test.csv') as f: $     size=len([0 for _ in f]) $     print("Records in test.csv => {}".format(size))
r.xs('UA', level=1)[['weight']].plot()
def display_features(features, feature_names): $     df = pd.DataFrame(data=features, $                       columns=feature_names) $     print (df)
gt_enrollment_data = pd.read_csv('../data/external/gatech_enrollment.csv') $ gt_enrollment_data.head()
df2['user_id'].nunique()
count_google = target_google.groupby(['date'], as_index=False).channel.count() $ count_google.head()
talks_train.shape
projects.actual_hours.sum()
outliers_age = airbnb_od.outlier_detection_serie_1d('age', cutoff_params=airbnb_od.strong_cutoff) $ outliers_age.head(10)
parse_dates = ['Created Date', 'Closed Date'] $ com311 = pd.read_csv('../data/rawdata/311_Service_Requests_heathotwater_20162017heatingseaosn.csv' $                      , dtype = {'Unique Key': str}, parse_dates=parse_dates)
pd.options.display.max_colwidth = 100 $ data_df[data_df.nwords == 1]['clean_desc'].head(15)
np.exp(0.0507), np.exp(0.0408)
df[df['diff_log_EBAY'].isnull()==True]
full.groupby(['Age'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False)
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17, linear.coef_) + linear.intercept_, c='red') $ plt.plot(X, np.dot(X_17,ridge.coef_) + ridge.intercept_, c='b') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
x_axis = 1 $ trip = [76.0, 80.25, 82.0] $ error = trip[2] - trip[0] $ fig, ax = plt.subplots() $ ax.errorbar(x_axis, trip[1], trip[0], trip[2], fmt='o')
reddit2.dropna(axis=0, inplace=True) 
np.sum(df["pickup_hour"].isin([7, 8, 9, 10, 11, 12]))
system = system.supersize(3,3,3) $ print(system)
data_final.head()
import statsmodels.api as sm $ convert_old = df2.query('group=="control"').query('converted == 1')['user_id'].count() $ convert_new = df2.query('group=="treatment"').query('converted == 1')['user_id'].count() $ n_old = df2.query('landing_page == "old_page"')['user_id'].count() $ n_new = df2.query('landing_page == "new_page"')['user_id'].count()
(train_4.shape, y_train.shape)
churn_array = df.reset_index().groupby('user_id')['event_leg'].max().value_counts().sort_index() $ plt.figure(figsize=(10,10)) $ plt.plot(range(1,len(churn_array)+1),churn_array.cumsum()/sum(churn_array)*100,'-ok') $ plt.ylabel('Cumulative percent that have left') $ plt.xlabel('Month')
plt.plot(pipe.tracking_error)
sum(trumpint.num_likes),sum(cliint.num_likes)
testmx2.shape
HOU_analysis2 = HOU_analysis["Per Seat Price"].mean() # Takes the average of each year/in-season vs. offseason
a = df1.io_state[2][-3:] $ int(a,16)
intersections_irr.head()
full['Age'].unique().shape[0]
autos.odometer.unique()
from sklearn import feature_extraction $ sklearn.__version__
logit_mod = sm.Logit(df_new.converted, df_new[['intercept', 'new_page', 'UK', 'US']]) $ results = logit_mod.fit() $ results.summary()
model = gensim.models.Word2Vec(sentences, min_count=1)
mta_census.dropna(inplace=True) $ mta_census['tragettraffic'] = mta_census['ENTRIES_MORNING']*mta_census['betw150kand200k']/100 $ target_by_station = mta_census.sort_values('tragettraffic', ascending=False).head(10) $ target_by_station = target_by_station.reset_index() $ target_by_station
changes = [] $ for lobbyist in newl: $     changes.append([lobbyist[0], lobbyist[3], 1]) $     changes.append([lobbyist[1], lobbyist[3], -1]) $
merged.groupby('committee_name_x')['amount'].sum().reset_index().sort_values('amount', ascending = False)
apple = pd.read_csv('appl_1980_2014.csv') $ apple.head()
dsg.axes('T')
df_license_opening.head(2)
merged_NNN.amount.sum()
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_values()        
tsne_vectors[u'word'] = tsne_vectors.index
df = df.drop(['Unnamed: 0','Unnamed: 1'],axis=1) $ df.head()
pickle.dump(lsa_tfidf_data, open('iteration1_files/epoch3/lsa_tfidf_data.pkl', 'wb'))
s.tail(1)
users_visits.visits =(users_visits.visits.fillna(0)/ np.timedelta64(1, 'D')).astype(int) $ users_visits.head()
X.head()
data.isnull().any()
import pandas as pd $ df_test = pd.read_json('data/test.json') $
df_train = pd.DataFrame({'text': train_texts, 'labels': train_labels}, columns=col_names) $ df_val = pd.DataFrame({'text': val_texts, 'labels': val_labels}, columns=col_names)
(df2['landing_page'] == "new_page").mean()
data_sets = {} $ data_sets['1min'] = pd.read_pickle('fixed_1min.pickle') $ data_sets['3min'] = pd.read_pickle('fixed_3min.pickle')
now = datetime.datetime.now() $ dt = relativedelta(years=1) $ year_ago = now - dt $ year_ago_str = year_ago.strftime("%Y-%m-%d") $ print(year_ago_str)
leadPerMonth.mean().plot.bar(figsize={12,6}) $ plt.title('Average Leads per month');
data = data.dropna() $ data.info()
df2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']) $ df2
rng.day
df_fuel_type =  pd.get_dummies(df['fuelType']) $ df_gear_box =  pd.get_dummies(df['gearbox']) $ df_vehicleType =  pd.get_dummies(df['vehicleType']) $ df_ = pd.concat([df, df_fuel_type,df_gear_box,df_vehicleType], axis=1)
sns.countplot(y="source_cleaned_2", data=tweet_table, palette="Greens_d")
df_t_best['Shipped At'] = df_t_best['Shipped At'].apply(lambda x: x.date()) $ df_best_chart = pd.DataFrame(df_t_best[df_t_best['Place Name'].isin(['Leeuwarden','Valkenswaard','Rokkeveen-West'])] $                     .groupby(['Place Name','State','Latitude','Longitude','Shipped At'])['Updated Shipped diff_normalized'] $              .mean()).sort_values(by='Place Name',ascending=True)
results = rcf_inference.predict(df_numpy[:10]) $ print(results['scores'])
from pyspark import SparkContext $ from pyspark.sql import SQLContext $ from pyspark.sql.functions import * $ print (pyspark.__version__)
r2 = yfs2.session.get(url, params={'format': 'json'}) $ r2.status_code
cityID = 'fa3435044b52ecc7' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Newark.append(tweet) 
end = pd.datetime(2012, 1, 1)
date = sorted_precip.index $ print(date)
df_master.head()
visual_df.index.name = 'team'
built_df['category'].value_counts().plot.bar()
df.head()
data.values
from sklearn.cross_validation import KFold, train_test_split $ train, test= train_test_split(table3, test_size= 0.3, random_state= 42)
my_house_sqft = 2650 $ estimated_price = get_regression_predictions(my_house_sqft, sqft_intercept, sqft_slope) $ print('The estimated price for a house with {} squarefeet is {:.2f}'.format(my_house_sqft, estimated_price))
musk.head()
df.loc[df.bottle_vol_ml == 750, 'consolidated_bottle_ml'] = '750ml' $ df.loc[df.bottle_vol_ml == 1000, 'consolidated_bottle_ml'] = '1000ml' $ df.loc[df.bottle_vol_ml == 1750, 'consolidated_bottle_ml'] = '1750ml' $ df.head()
net.save_edges(edges_file_name='edges.h5', edge_types_file_name='edge_types.csv', output_dir=directory_name)
outcols = ['site_name', 'user_location_country', 'user_location_region', 'user_location_city', $            'is_mobile', 'is_package', 'channel', 'srch_adults_cnt', 'srch_children_cnt', $            'srch_destination_type_id', 'hotel_continent', 'hotel_country', 'hotel_market', 'hotel_cluster'] $ dfXy = dfXy[outcols].copy() $ dfXy.to_csv(dataurl+'train200thdrop.csv', sep=',', encoding='utf-8', index=False)
run txt2pdf.py -o '2018-06-22 2012 FLORIDA HOSPITAL Sorted by payments.pdf'  '2018-06-22 2012 FLORIDA HOSPITAL Sorted by payments.txt' $
 twitter_merged_data = pd.read_csv('twitter_archive_master.csv') 
predictions = reg.predict(X_train) $ dev_predictions = reg.predict(X_dev)
df2=df2.drop(df2.index[1899]) $ df2.user_id.nunique()
os.environ["asset_id"] = "users/resourcewatch/cit_018_monthly_no2_concentrations_in_atmosphere_201701" $ !earthengine upload image --asset_id=%asset_id% %Zgs_key%
df_goog.resample("M").sum()
change_index = df3[df3['group'] == 'treatment'].index $ df3.set_value(index=change_index, col='ab_page', value=1) $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3[['intercept', 'ab_page']] == df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
dupli.index[0]
baseball1_df.loc[baseball1_df['ageAtDebut'].idxmin()]
new_values = [] $ for i in X.values: $     new_values.append(" ".join(i))
index_ts.head()
from carto.auth import APIKeyAuthClient $ USERNAME="wri-rw" $ USR_BASE_URL = "https://{user}.carto.com/".format(user=USERNAME) $ auth_client = APIKeyAuthClient(api_key=carto_api_token, base_url=USR_BASE_URL)
print(autos.columns) $
users_df = get_lims_dataframe("SELECT id, login FROM users WHERE login = 'aarono'") $ users_df.head()
shows['release_weekday'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%w')) $ shows['release_weekday'] = shows['release_weekday'].dropna().apply(lambda x: str(x)) $ shows['release_weekday'] = shows['release_weekday'].dropna().apply(lambda x: int(x))
df['destination'] = [1 if x == 'US' else 0 for x in df.country_destination]
matches['home_team_win_or_draw'] = 0 $ matches.loc[matches['home_team_goal'] >= matches['away_team_goal'], 'home_team_win_or_draw'] = 1
states.columns
url = 'http://www.basketball-reference.com/teams/ORL/executives.html'
for place in meanLon.index: $     datatest.loc[((datatest.lon.isnull()) & (datatest.place_name == place)), 'lon'] = meanLon.loc[place]['lon'] $     datatest.loc[((datatest.lat.isnull()) & (datatest.place_name == place)), 'lat'] = meanLat.loc[place]['lat']
rhum_fine = np.zeros((844, 26, 59)) $ for i in range(844): $     rhum_mon = rhum_us_full[i] $     interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, rhum_mon) $     rhum_fine[i] = interp_spline(grid_lat, grid_lon)
train['dot_org'] = train.url.str.contains('.org', case=False, na=False, regex=False).astype(int) $ train.groupby('dot_org').popular.mean()
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
baseball1_df.info()
plt.figure(figsize=(8,3)) $ plt.plot( $     group_2.loc[group_2['STATION'] == '103 ST']['DATETIME'], $     group_2.loc[group_2['STATION'] == '103 ST']['ENTRIES'])
df_test_2.set_index('user_id') $ df_test_2.head()
obj = pd.Series([4.5, 7.2, -5.4, 3.6], index=['d', 'b', 'a', 'c'])
raw_data.objective = pd.Categorical(raw_data.objective)
s.dt.minute # extract minute as integer
dulp_files = [f for f in os.listdir(folder) if re.match('Dul.*.phys.*.csv', f)] $ dulp_files
min_IMDB = scores.IMDB.min()
sns.factorplot('brand',kind='count',data=autodf,size=50)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1 - p_old)])
months = pd.concat([nov, dec]) $ print nov.shape, dec.shape, months.shape
Measurement = Base.classes.measurements $ Station = Base.classes.stations $ session = Session(engine)
df_countries.head(10)
twitter_archive_full = pd.merge(twitter_archive_with_json, image_prediction_clean, how='left', on=['tweet_id'])
df.quantile([.01, .05, .1, .25, .5, .75, .9, .95, .99])
X = data_sub["ratio"] $ y = data_sub["retweets"] $ model = sm.OLS(y, X).fit() $ predictions = model.predict(X) # make the predictions by the model $ model.summary() $
from sklearn.linear_model import LinearRegression $ regression_model = LinearRegression() $ regression_model.fit(X_train, y_train)
monte.str.lower()
plots.proto_distribution(traffic_type = 0,scale = "log") #Normal traffic log scale
ideas['Time'].head(1)  # Inspecting progress
wed11 = facts_metrics[facts_metrics['dimensions_date_id']==(datetime.date(2018, 4, 14))]
ice_df.dtypes
df = pd.read_csv("pgh_meetup_groups_Jan_27_2018.csv") $ df.head()
grades.plot('ATAR', 'Mark', kind='scatter')
logit = sm.Logit(df3['converted'],df3[['ab_page','intercept']]) $ result = logit.fit() $
ps.to_timestamp('D', how='start')
DataAPI.write.update_secs_industry_gics(industry='A_GICSL1', trading_days=trading_days, override=False)
fcst_trips = fcst_trips.replace(np.nan, '', regex=True) #replace NaN with blank $ fcst_weekly = fcst_weekly.replace(np.nan, '', regex=True) #replace NaN with blank
pclass2_survived = df_titanic.loc[df_titanic['pclass'] == 2, 'survived'] $ print(pclass2_survived.value_counts()) $ pclass2_survived.value_counts().plot(kind='pie', autopct='%.2f', fontsize=20, figsize=(6, 6))
merged_gdf = gp.sjoin( $     nyc_census_tracts, $     complaints2016_geodf, $     how = "inner" $ )
read_table("TechTickerSampleData.txt", sep="\t", skiprows=7, names=["stock", "timestamp", "close", "high", "low", "open", "volume"], $            index_col=1)
songLink = 'https://www.youtube.com/embed/'+soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href'][9:] $ HTML('<iframe width="560" height="315" src='+songLink+' frameborder="0" allowfullscreen></iframe>')
old_page_converted=np.random.binomial(n_old,p_old)
pd.to_datetime(['04-01-2012 10:00'], dayfirst=False)
df.to_csv('edited_ab_data.csv',index=False)
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv')
df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True) $ df.head()
_ = ok.grade('q15') $ _ = ok.backup()
control_group_sample = df2[df2.group == 'control'] $ num_of_converted_in_control = len(control_group_sample[control_group_sample.converted == 1]) $ p_control_converted = num_of_converted_in_control/len(control_group_sample) $ p_control_converted
damage_count = faa_data_pandas['DAMAGE'].value_counts() $ print(damage_count)
Fraud_Data.groupby('country')['class'].mean().sort_values(ascending=False)[:10]
avgPurchU = train.groupby(by='User_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseU'}) $ train = train.merge(avgPurchU, on='User_ID', how='left') $ test = test.merge(avgPurchU, on= 'User_ID', how='left')
plantlist_for_efficiency_analysis = plantlist $ plantlist_for_efficiency_analysis = plantlist_for_efficiency_analysis.dropna(subset=['efficiency_net'])
got_data = df_final $ got_data.head()
meeting_status.info()
df['speaker_party'].value_counts()
lostintranslation_wiki = wikipedia.page("Lost in translation (film)") $ print lostintranslation_wiki.title $ print lostintranslation_wiki.url $ print lostintranslation_wiki.content
revs.tail()
pd.set_option('display.max_colwidth', 200)  $ pd.set_option('display.max_rows', 50) $ yxe_tweets
df_archive_clean.rating_denominator.value_counts()
b_cal.head(2)
df = df[(df['First Date'] < pd.Timestamp("2015-01-19")) & (df['Last Date'] > pd.Timestamp("2016-03-17"))]
a.extend([3,4])
stats = loans.groupby('client_id')['loan_amount'].agg(['mean', 'max', 'min']) $ stats.columns = ['mean_loan_amount', 'max_loan_amount', 'min_loan_amount'] $ stats.head()
nba_df.dtypes
%timeit pd.eval('df1 + df2 + df3 + df4')
df_agg = tmdb_movies.groupby('day_of_week').agg({'revenue': np.sum}) #aggregate $ df_agg.sort_values(by=['revenue'], ascending = False).head()
col = col.dropna(subset = ['LOCATION'])
my_gempro.get_msms_annotations()
combined_df3.shape
plt.plot(data.moving_avg)
from statsmodels.tsa.arima_model import ARIMA $ model_713 = ARIMA(dta_713, (2, 2, 0)).fit() $ model_713.forecast(5)[:1] 
clf_lr2 = linear_model.LogisticRegression(penalty = 'l2',C=0.01)
site_values = ulmo.cuahsi.wof.get_values(wsdlurl, location, variable, $                                          start=dataset['begin_date'], end=dataset['end_date'])
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) $ model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)
tweet_archive_enhanced_clean.loc[1627,'text']
list_users = dict(df_ta_caps_user["user name"].value_counts()).keys() $ list_capsule = dict(df_ta_caps_user["capsule name"].value_counts()).keys()
%%sql $ select * from hive.entities.repos limit 10
old_page_converted = np.random.choice([0,1],size=n_old,p=[(1-p_old_null),p_old_null])
x = store_items.isnull() $ print(x) $
df = df[['Country', 'Indicator_id', 'Publication Status', 'Year', 'WHO Region', 'World Bank income group', 'Sex', 'Display Value', 'Numeric','Low', 'High', 'Comments']] $ df.head()
df.head(5)
df_city_restaurants = df_restaurants.filter(df_restaurants.city == 'Edinburgh')
test = a.strip().replace("'","").split(",") $ type(test[0])
p_old = df2['converted'].mean(); $ p_old
load2017.isnull().sum() 
reqs = pd.read_csv(r'311_service_requests.zip') $ reqs.head()
rides_urban = urban['fare'].count() $ rides_suburban = suburban['fare'].count() $ rides_rural = rural['fare'].count() $ total_ride = [rides_urban, rides_suburban, rides_rural] $
tweet_data_df["date"]=pd.to_datetime(tweet_data_df["date"]) $ tweet_data_df.head() $
fig, ax1 = plt.subplots(1,1, figsize=(8,3)) $ totals.T.plot(ax=ax1) $ ax1.set_xlim(1944,2018) $ fig.savefig(os.path.join(output_dir, 'Sox_totals.png'), dpi=200, bbox_inches='tight' )
age1_mean = records4[(records4['Graduated'] == 'Yes') & (records4['Gender'] == 'Male')]['Age'].mean() $ age2_mean = records4[(records4['Graduated'] == 'Yes') & (records4['Gender'] == 'Female')]['Age'].mean() $ age3_mean = records4[(records4['Graduated'] == 'No') & (records4['Gender'] == 'Male')]['Age'].mean() $ age4_mean = records4[(records4['Graduated'] == 'No') & (records4['Gender'] == 'Female')]['Age'].mean() $ age1_mean, age2_mean, age3_mean, age4_mean
from sklearn.model_selection import train_test_split $ X = df.drop('Label', axis=1) $ y = df['Label'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=31)
incorr_oldpage = df.query("group == 'treatment' and landing_page == 'old_page'") $ print("Number of times when treatment group user lands incorrectly on old_page is {}".format(len(incorr_oldpage))) $ incorr_newpage = df.query("group == 'control' and landing_page == 'new_page'") $ print("Number of times when control group user lands incorrectly on new_page is {}".format(len(incorr_newpage))) $ print("Number of times when new_page and treatment don't line up is {}".format(len(incorr_oldpage) + len(incorr_newpage)))
meal_cuisine_types_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meal_cuisine_types.csv')['Body'].read().decode('utf-8') $ meal_cuisine_types = pd.read_csv(StringIO(meal_cuisine_types_csv_string), header=0, delimiter='|')
relevant_data['Event Type Name'].value_counts().plot(kind='barh')
df.loc['20180103', ['A','B']]
import pandas as pd $ df = pd.DataFrame(data) $ df
trips_data['percentile'] = trips_data['duration'].rank(pct=True) # using rank method of pandas to assign percentile to each duration values $ pct_95_trips_data= trips_data[trips_data['percentile']<=0.95] # to remove outliers, say, we accept to consider only 95% of the data observed
prediction.tail(10)
print('\n'*2 ) $ print('df_everything_about_DRGs.loc[0]') $ print('\n'*2 ) $ print(df_everything_about_DRGs.loc[0])
pca.explained_variance_ratio_
json_data['dataset'].keys()
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index() * 100
top10_topics_list = top10_topics_2.head(10)['topic_id'].values $ top10_topics_list
tweets_df.head() 
autos["registration_year"].value_counts(normalize=True).sort_index(ascending=False).head(20)
sns.set_style('whitegrid') $ sns.distplot(data_final['countPublications'], kde=False,color="red") # bins=30, $
RF = RandomForestRegressor() $ RF.fit(team_names[predictor_cols],team_names.regular_occurrences)
df_likes_grouped['Total'] = df_likes_grouped.sum(axis=1) $ df_likes_grouped = df_likes_grouped.loc[:, (df_likes_grouped != 0).any(axis=0)]
in_out_dict = {"left":{"in":[],"out":[],"inx":[],"outx":[]},"middle":{"in":[],"out":[],"inx":[],"outx":[]}, $                "right":{"in":[],"out":[],"inx":[],"outx":[]}} $ for t in tnrange(len(rsv)): $     in_out_dict[rsv["Position"].iloc[t]][rsv["in_out"].iloc[t]].append(rsv["time"].iloc[t]) $     in_out_dict[rsv["Position"].iloc[t]][rsv["in_out"].iloc[t]+'x'].append(rsv.index[t]) $
faa_data_pandas = pd.read_csv("Pennsylvania_Condensed.csv") # Bringing in the Facebook message data $ faa_data_pandas.head()
df_group_by['gender'] = df_group_by['gender'].fillna(0)  #Filled na with zero $ df_group_by['city'].fillna(df_group_by['city'].mode()[0], inplace=True)  #Filled na with mode $ df_group_by['bd_c'].fillna(df_group_by['bd_c'].mode()[0], inplace=True) #Filled na with mode $ df_group_by['bd'].fillna((df_group_by['bd'].mean()), inplace=True)   #Filled na with mean $ df_group_by['registered_via'].fillna(df_group_by['registered_via'].mode()[0], inplace=True)  #Filled na with mode
autos["odometer_km"].head()
tweet_data['text_tokenized'] = tweet_data['text'].apply(lambda x: word_tokenize(x.lower())) $ tweet_data['hash_tags'] = tweet_data['text'].apply(lambda x: hash_tag(x)) $ tweet_data['@_tags'] = tweet_data['text'].apply(lambda x: at_tag(x))
po= df2.query("converted==1").count()[0]/len(df2) $ po
vidsByCountry_df = youtube_df[youtube_df["title"].isin(title_namesC)] $ vidsByCountry_dfMax = vidsByCountry_df.loc[vidsByCountry_df.groupby(["title"])["views"].idxmax()] $ vidsByCountry_dfMax
new_reps.newDate[new_reps.Cruz.isnull()]
odds.head(3)
csv_df = pd.read_csv("images_aranjuez.csv", sep="|", parse_dates=True)
final_topbikes['Distance'].mean()
data.loc[data.floor.notnull(), 'floor*price'] = data['floor']*data['price_aprox_usd']
df.tail()
file_Path_To_Store_Data = 'Delhi_Proper_Places.pkl' $ list_Places_Delhi = [] $ with open(file_Path_To_Store_Data, 'rb') as f: $     list_Places_Delhi = pickle.load(f)
totvolume.shape
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
shows.dtypes
noaa_data.loc[:,'PRECIPITATION'].groupby(pd.Grouper(freq='W')).sum()
s1 = "SELECT * FROM paudm.production as prod where prod.pipeline='memba' order by - prod.id  limit 1" $ d= pd.read_sql(s1,engine)
actual_payments['iso_date'].dtype
df_birth.info()
def Lemmatize_tokens(tokens): $     doc = ' '.join(tokens) $     Lemmatized_tokens = [token.lemma_ for token in nlp(doc)] $     return Lemmatized_tokens
saved_tweets.head()
subwaydf.iloc[117329:117333] # this high & correseponding low number seems to be because entries&exits messes us.
df_msg[df_msg["attachments"]] $ "attachments", "edited", "reactions" $ mentions_with_id = pd.io.json.json_normalize(resp.json(), record_path='mentions', meta='id', $                                      record_prefix='mentions.') $ mentions_with_id.head()
store_items = store_items.drop(['store 3'], axis = 0) $ store_items
snow.select("select * from RWD_DB.RWD.RAVEN_CLAIMS_SUBMITS_PROCEDURE limit 3")
daily_returns.hist(bins=100) $ plt.show()
loans.loan_nr[loans.id_loan==815]
s[criteria].head()
trend_de = googletrend[googletrend.file == 'Rossmann_DE']
tcga_target_gtex_expression[["GTEX-146FH-1726-SM-5QGQ2", $                              "GTEX-WZTO-2926-SM-3NM9I", $                              "TCGA-ZS-A9CE-01", "TCGA-AB-2965-03"]].apply( $     np.exp2).apply(lambda x: x - 0.001).sum()
np.exp(results_new.params)
gene_to_seq_dict = {'Rv1295': 'MTVPPTATHQPWPGVIAAYRDRLPVGDDWTPVTLLEGGTPLIAATNLSKQTGCTIHLKVEGLNPTGSFKDRGMTMAVTDALAHGQRAVLCASTGNTSASAAAYAARAGITCAVLIPQGKIAMGKLAQAVMHGAKIIQIDGNFDDCLELARKMAADFPTISLVNSVNPVRIEGQKTAAFEIVDVLGTAPDVHALPVGNAGNITAYWKGYTEYHQLGLIDKLPRMLGTQAAGAAPLVLGEPVSHPETIATAIRIGSPASWTSAVEAQQQSKGRFLAASDEEILAAYHLVARVEGVFVEPASAASIAGLLKAIDDGWVARGSTVVCTVTGNGLKDPDTALKDMPSVSPVPVDPVAVVEKLGLA', $                     'Rv2233': 'VSSPRERRPASQAPRLSRRPPAHQTSRSSPDTTAPTGSGLSNRFVNDNGIVTDTTASGTNCPPPPRAAARRASSPGESPQLVIFDLDGTLTDSARGIVSSFRHALNHIGAPVPEGDLATHIVGPPMHETLRAMGLGESAEEAIVAYRADYSARGWAMNSLFDGIGPLLADLRTAGVRLAVATSKAEPTARRILRHFGIEQHFEVIAGASTDGSRGSKVDVLAHALAQLRPLPERLVMVGDRSHDVDGAAAHGIDTVVVGWGYGRADFIDKTSTTVVTHAATIDELREALGV'} $ my_gempro.manual_seq_mapping(gene_to_seq_dict)
a_result = df1.append(df2) $ a_result
url = form_url(f'organizations/{org_id}/teams', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
tweets_df.info()
y_score_test = gbm.predict_proba(X_test)[:,1] $ metrics.roc_auc_score(y_test, y_score_test)   # 0.69
from sklearn.datasets import fetch_20newsgroups $ dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes')) $ documents = dataset.data
df2.loc[:,df2.any()]
joined_df.usage_duration.to_frame().head()
featured_image_url = browser.url
old_page_converted = np.random.choice(a=[1,0], size=n_old, replace=True, p=[p_old, 1-p_old])
priority=[] $ for t in t_open_resol: $     i=t['index'][1] $     priority.append(data_issues_transitions['priority'][i]) $
for i in lst: $     for j in i.keys(): $         if len(j) == 6: $             print(i[j]['acct_type'])
payments_all_yrs.head()
def my_scaler(col): $   return (col - np.min(col))/(np.max(col)-np.min(col)) $ data_scaled = data_numeric.apply(my_scaler)
adopted_cats.head(5)
%sql \ $ SELECT twitter.url,twitter.heats, twitter.user_id \ $ FROM twitter \ $ ORDER BY twitter.heats DESC LIMIT 1;
plt = sns.boxplot(data=df, x="race_desc", y="charge_count", hue="race_desc", dodge=False) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) $ plt.set_xticklabels(plt.get_xticklabels(), rotation=45);
df2[df2['landing_page']=="new_page"].count()/df2.shape[0]
df = pd.read_sql('SELECT last_name, COUNT(*) FROM actor GROUP BY last_name ORDER BY last_name', con=conn) $ df
data = [go.Histogram(x=twitter_final['length'])] $ iplot(data, filename='basic histogram')
from sklearn.model_selection import GridSearchCV
match = pattern.search('<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>')
type_list = list(pokemon['Type 1'].unique()) $ pokemon_test = pd.DataFrame(columns = pokemon.columns) $ for i in type_list: $     pokemon_test = pokemon_test.append((pokemon[pokemon['Type 1'] == i]).sample(frac=0.05))
dd2=cfs.diff_abundance('Subject','Control','Patient', fdr_method='bhfdr', random_seed=2018)
df = pd.DataFrame(datalist)
con = sqlite3.connect('db.sqlite') $ df=pd.read_sql_query("SELECT * from sqlite_master", con) $ con.close() $ df
df_joined = df_country.set_index('user_id').join(df2.set_index('user_id'), how = 'inner') $ df_joined.head()
mean_of_threshold_recall = np.array(mean_of_threshold_recall) $ for r in mean_of_threshold_recall: $     print(r)
pd.DataFrame(selectfrom_coef, columns=selectfromcols).T.sort_values(0, ascending=False).tail(10)
X.head()
dfs.tail()
run txt2pdf.py -o"2018-06-14 2148 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by Payments.pdf"  "2018-06-14 2148 UNIVERSITY HOSPITALS OF CLEVELAND Sorted by Payments.txt"
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative= 'smaller') $ z_score, p_value
df2.query('group == "control"').converted.mean()
min_risk = minimize(risk_, W, args=(ER, COV), method='SLSQP', bounds=b_, constraints=c_) $ if not min_risk.success: $     print 'risk_ not optimized: ', min_risk.message $ Results = Results.join(pd.Series(min_risk.x, name='min_risk').round(4)) $
df4['tobs']=df4['tobs'].astype(int) $ tobs_mean = df4['tobs'].mean() $ tobs_max = df4['tobs'].max() $ tobs_min = df4['tobs'].min() $
week24 = week23.rename(columns={168:'168'}) $ stocks = stocks.rename(columns={'Week 23':'Week 24','161':'168'}) $ week24 = pd.merge(stocks,week24,on=['168','Tickers']) $ week24.drop_duplicates(subset='Link',inplace=True)
df2 = df.groupby(['neighbourhood'])[['id']].count().reset_index() $ len(df2[df2.id > 10].sort_values('id',ascending=False ) )
df = df.drop('fullgender', 1)
def machine_learning(x): $     if 'Machine Learning' in x: $         return 1 $     return 0 $ df_more['Machine Learning'] = df_more['Title'].apply(machine_learning)
test_df = pd.read_csv("test.csv", dtype=dtypes) $ test_df.head()
np.shape(prec_us_full)
(CLAS_PATH/'tmp').mkdir(exist_ok=True) $ np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn) $ np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val) $ np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels) $ np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)
pred.info()
p_old =df2[df2['landing_page'] == "old_page"].converted.mean() $ p_old
test_clean_token = tc.clean_corpus(test_corpus, string_line=False) $ train_clean_token = tc.clean_corpus(train_corpus, string_line=False) $ test_bow, test_word_freq = tc.get_bow(test_clean_token) $ train_bow, train_word_freq = tc.get_bow(train_clean_token)
nnew = (df2['landing_page'] == 'new_page').sum() $ print(nnew)
parsed_liberia_df = pd.concat([liberia_df_new_cases, liberia_df_new_deaths]) $ parsed_liberia_df.rename(columns={'Date': DEFAULT_NAME_COLUMN_DATE, $                                   'Variable': DEFAULT_NAME_COLUMN_DESCRIPTION, $                                   'National': DEFAULT_NAME_COLUMN_TOTAL}, inplace=True) $ parsed_liberia_df[DEFAULT_NAME_COLUMN_COUNTRY] = countries['liberia']
with tb.open_file(filename='data/file1.h5', mode='w') as f: $     f.create_array(where='/',  name='array1', obj=[0, 1, 2, 3])
tweets_clean[tweets_clean.doggo == 'doggo']
distance_list = [] $ for i in range(0, len_start_coord_list): $     distance_list.append(get_distance())
f.visititems?
df_new['intercept'] = 1 $ lm = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','US','interaction_us_ab_page','CA','interaction_ca_ab_page']]) $ results = lm.fit() $ results.summary()
from statsmodels.tsa.arima_model import ARIMA $ arima10 = ARIMA(dta_713,(1,0,0),freq='Q').fit() $ arima10.summary() $
a_submission_data.to_csv("a_submissions_clean.csv", index=False, sep="|") $ d_submission_data.to_csv("d_submissions_clean.csv", index=False, sep="|") $ a_comment_data.to_csv("a_comments_clean.csv", index=False, sep="|") $ d_comment_data.to_csv("d_comments_clean.csv", index=False, sep="|")
ftp = ftplib.FTP("ftp.star.nesdis.noaa.gov")
youthUser4 = youthUser3.astype(object).fillna("UNKNOWN") $ youthUser4.head()
es = Elasticsearch('elasticsearch:9200') $ if es.indices.exists('stream-test'): $     es.indices.delete('stream-test') $     es.indices.create('stream-test')
suspects_with_25_1[suspects_with_25_1['is_trip']].index.values
ac['If No Eligibility, Why?'].describe()
sentiment_df = pd.DataFrame.from_dict(sentiment_freq,  orient='index') $ sentiment_df = sentiment_df.reset_index() $ sentiment_df
from profootballref.Parsers import TeamStats $ year = 2015 $ df = TeamStats.TeamStats().defense(year)
stn_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).filter(Measurement.station == busiest_stn).filter(Measurement.date > data_oneyear).order_by(Measurement.date).all() $ stn_temp
dfgrouphc = df.groupby('hotel_cluster').count() $ serhc = dfgrouphc['date_time'] $ sns.set(font_scale=1.5) $ f = serhc.plot(figsize=(12,6)) $ _ = f.set_ylabel('Number of Examples')
asf_people_human_raw_df = session.read.format("csv").option("header", "true") \ $                 .option("inferSchema", "true").load( $     "{0}/human_data/asf_people".format(fs_prefix))
df_protest.loc[2].head()
s1.sample(3)
column_created.keys()
df.head()
norm.cdf(z_score)
liquor['State Bottle Cost'] = [s.replace("$","") for s in liquor['State Bottle Cost']] $ liquor['State Bottle Cost'] = [float(x) for x in liquor['State Bottle Cost']] $ liquor_state_cost = liquor['State Bottle Cost']
(df2["converted"] == 1).mean() ### assumed that is  pnew  and  pold  are equal
a = (df['landing_page'] == 'new_page').sum() + (df['group'] == 'treatment').sum() $ b = len(df.query('landing_page == "new_page" & group == "treatment"')) $ a -2*b
femalemoon = pd.concat([moon, femalebydatenew], axis=1) $ femalemoon.head(3)
pd.Series(['a','b','c','d','e'], index=[10,20,30,40,50])
data.simple_features
groupby_imputation.head()
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ with open('image-predictions.tsv', mode = 'wb') as file: $     file.write(response.content)
legHouse["first_name"] = legHouse["first_name"].fillna(indexed_df["first_name_y"])
sfrom = company_vacancies[company_vacancies.salary_from > 10000].salary_from.min() $ sto = company_vacancies.salary_to.min() $ sfrom, sto
query_result1.features[0].geometry
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])
!wget ftp://ftp.solgenomics.net/tomato_genome/annotation/ITAG3.2_release/ITAG3.2_RepeatModeler_repeats_light.gff -P $DATA_PATH
extractor = twitter_setup() $ user = extractor.get_user('racheldyap') $ print(user.screen_name) $ print(user.id)
lm.score(x_test, y_test > 0),np.mean(y_test > 0)
data = {} $ data['name'] = 'pratap' $ json_obj = json.loads('{"names": {"name1":"pratap","name2":"swetha"}}') $ json_data = json.dumps(data) $ print (json_obj['names'])
myopener = MyOpener() $ page = myopener.open('http://www.google.com/search?q=python') $ html=page.read()
people_person['date'] = people_person['date'].astype('str')
df_compare=pd.read_csv('Unique Providers Jan 3, 2017.csv') $
test_y[0:202].shape
cig_data.columns
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
stats.apply(np.median)
stops_per_crime_per_month.index
lst= [abs(df.Close[i] - df.Close[i+1]) for i in range(0,len(df.Close)-1)] $ print("The largest change between any two days is %3.3f"%(max(lst)))
last_12_precip_df = pd.DataFrame(data=last_12_precip) $ last_12_precip_by_date_df  = last_12_precip_df.set_index("date") $ last_12_precip_by_date_df.head(2500)
df = df.merge(shifts, on='atbat_pk', suffixes=('_old', ''))
for tweet in islice(tweets, 20): $     if tweet != None: $         print json.loads(tweet)['text'][:20] $     else: $         print 'Timeout.'
print(result2.summary())
contract_history.dtypes
X = df.loc[:,"Timestamp":"Volume_(Currency)"] $ y = df["Weighted_Price"]
print(df2['converted'].mean())
bd.reset_index()
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.grid() $ plt.axvline(p_diffs.mean(), color='r', label='mean') $ plt.legend();
prob_kNN500x = kNN500x.predict_proba(Test_extra) $ prob1_kNN500x = [x[1] for x in prob_kNN500x] $ Results_kNN500x = pd.DataFrame({'ID': Test.index, 'Approved': prob1_kNN500x}) $ Results_kNN500x = Results_kNN500x[['ID', 'Approved']] $ Results_kNN500x.head()
con = psycopg2.connect(dbname=dbname, user=user, host=host, $                        password=password) $ cur = con.cursor() $ cur.execute('SET search_path to {}'.format(schema))
a.shape #Number of rows and columns.
df_ab.converted.sum()/df_ab.count()[0]
import re $ spice_df = pd.DataFrame(dict((spice, recipes.ingredients.str.contains(spice, re.IGNORECASE)) $                             for spice in spice_list)) $ spice_df.head()
df.drop( columns=df.columns[[3,4,5]] )   # delete columns by list of column number
np.nan == np.nan
print(voters.columns) $ print(households.columns)
new_page_sim = (np.random.choice([1, 0], size=n_new, p=[p_mean/100, (1-p_mean/100)])).mean() $ output1 = round(new_page_sim, 4) * 100 $ print(output1,'%')
df_Measures        = pd.read_csv("~/Downloads/radiation.measurements.sample.csv")
suburb = [None] * len(train_df4) $ suburb[:1000] = [geolocator.reverse(x).address for x in train_df4['location-ll'][:1000]]
clean_appt_df = train_set.copy() $ clean_appt_df.shape
%matplotlib inline $ df.resample(rule='M').count().plot(y='Complaint Type')
time_hour_for_file_name = datetime.datetime.strptime(df_TempJams['timeStamp'][0],"%Y-%m-%d %H:%M:%S").hour
h2o_train.to_csv('stacking_input/h2o_train_kfld.csv',index=False)
c2.sum()
rfc_features = sorted(list(zip(test_features, rfc.feature_importances_)), key=lambda x: x[1], reverse=True) $ rfc_features
fb.uuid
table.head(20)
nnew = len(df2[(df2.landing_page=='new_page')]) $ print(nnew)
joined.dropna(inplace=True) $ len(joined[joined.Close.isnull()])
length = [len(h.tweets) for h in airlines]
temps_df.Missoula - temps_df.Philadelphia
year15 = driver.find_elements_by_class_name('yr-button')[14] $ year15.click()
df2.drop(drop_rows,inplace=True)
strMaxEndDate = maxEndDate.strftime('%Y-%m-%d') $ df_new = df[df['endDate'] > strMaxEndDate] $ df_new
archive_copy['dog_description'].unique()
model_name = 'mdl_helpdesk_resolution_time' $ model_version = 'v1' $ storage_bucket = 'gs://' + google.datalab.Context.default().project_id + '-datalab-workspace/' $ storage_region = 'us-central1'
All_tweet_data_v2.rating_denominator=10
bus.set_index('zip_code').loc[["94545", "94602", "94609"]]
assert mcap_mat.shape[0] < 100
SCC_med_inc.reset_index(level=0,inplace=True) $ SCC_med_inc.columns = ['Year','Median_HH_Income'] $ SCC_med_inc
portfolio.df_returns()
pd.date_range('2005', periods=4, freq='Q')
newdf['score'].fillna(0.187218571, inplace=True)
ed = dt.datetime.today()
X.shape
old_page_converted= np.random.binomial(1, p=p_old, size=n_old) $ old_page_converted
pd.crosstab(train.CIA, train.L2_ORGANISATION_ID)
range(0, af.length.max(), 100000)
def append_wait_indicator(): $     df['wait'] = '' $     df.loc[df['waiting_days'] > 0, 'wait'] = 1 $     df.loc[df['waiting_days'] == 0,'wait'] = 0 $     print("Appended binary indicator to 'wait' column")
results = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]).fit()
plt.hist(null_vals); $ plt.axvline(x=obs_diff, color = 'red');
df = pd.DataFrame({'temp':pd.Series(28 + 10*np.random.randn(10)), $                    'rain':pd.Series(100 + 50*np.random.randn(10)), $                    'location':list('AAAAABBBBB') $ }) $ print(df.head(2))
priceData = priceData.loc[:,('Name', 'BestBuyYesCost', 'BestSellYesCost', 'BestBuyNoCost', 'BestSellNoCost','TickerSymbol')] $ priceData.sort_values(by = 'TickerSymbol', ascending=True, inplace=True) $ priceData.reset_index(drop=True, inplace=True)
between_my_posts.describe()
print(filou.buddy.name) $ print(filou.buddy.age)
ab.converted.sum()/ab.user_id.nunique()
data.head()
print("Number of Malware in Mobile ATT&CK") $ print(len(all_mobile['malware'])) $ df = all_mobile['malware'] $ df = json_normalize(df) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
aqmdata.to_excel('aqmdata.xlsx', index=False)
cars[cars.Model.str.contains('Toyota')]
likes = pd.read_csv('../data/cleanInput/likes.csv') $ likes.head()
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
gsutil cp s3://wri-public-data/resourcewatch/raster/dis_007_landslide_susceptibility_map/dis_007_landslide_susceptibility_map_edit.tif gs://resource-watch-public/resourcewatch/raster/dis_007_landslide_susceptibility_map/dis_007_landslide_susceptibility_map_edit.tif
df_wm.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
mentions_df["date"] = mentions_df["epoch"].apply(lambda x: time.strftime('%Y-%m-%d', time.localtime(x))) $ mentions_df["time"] = mentions_df["epoch"].apply(lambda x: time.strftime('%H:%M:%S', time.localtime(x)))
df.groupby("cancelled")["created_as_guest"].mean()
df.head()
df2[df2.group == 'treatment'].converted.mean() $
dfcsv = df.loc[df['fileType'] == 'csv'] $ dfcsv['fileCount'].describe()
n_old = len(df2.query("group == 'control'" )) $ n_old
cand_date_df['state'].value_counts()
from dateutil.tz import tzutc $ enrollment_start = datetime.datetime(2017, 7, 19, 18, 40, tzinfo=tzutc()) $ assert(enrollment_start.isoweekday() == 3)
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
plt.plot(glons, glats, marker='.', color='k', linestyle='none') $ plt.show()
from sklearn import svm $ SVM = svm.SVC() $ SVM.fit(X_train, y_train) 
sst.shape
df_cal['start_date'] =  pd.to_datetime(df_cal['start_date'], format='%Y-%m-%d %H:%M:%S.%f')
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1 - convert_mean), convert_mean])
activeWikis = dfClean[(dfClean['stats.activeUsers']>=1)&(dfClean['users_1']>0)] $ inactiveWikis = dfClean[(dfClean['stats.activeUsers']<1)|(dfClean['users_1']==0)]
c.execute('SELECT city FROM weather where cold_month = "January" LIMIT 2 OFFSET 3') $ print(c.fetchall())
rng.asi8[0]
trunc_df.loc[list(airbnb_10)]
p_tnl = therm_abs_rate / (therm_abs_rate + thermal_leak) $ p_tnl.get_pandas_dataframe()
sentiment_df.to_csv("sentiment.csv", encoding = "utf-8-sig", index = False)
print(open('submission_metadata.txt').read())
snow.upload_dataframe(df, "nk_adpkd_ref")
pred_df_dtmodel = pd.concat([X_test, pd.DataFrame(dtmodel_predictions, columns = ['dtmodel_predictions'])], axis = 1) $ pred_df_dtmodel.head(10)
total_features.columns
for i in range(len(df_enhanced)): $     df_enhanced['source'][i] = df_enhanced['source'][i].split('<')[1].split('>')[-1]
E = np.logspace(1, 4, 10) * u.GeV $ print(E.to('TeV'))
blame.timestamp.describe()
result3.summary()
sensor = hp.find_sensor('53b1eb0479c83dee927fff10b0cb0fe6') $ sensor
results_country.summary()
num_rdd = access_logs_raw.count() $ num_rdd
crimes.tail()
n_new = treatment_df.shape[0] $ n_new
df_intermediate  = df_[selected_feature] $ df_norm = (df_intermediate - df_intermediate.mean()) / (df_intermediate.max() - df_intermediate.min()) $ df_std = (df_intermediate - df_intermediate.mean()) / df_intermediate.std()
df_protest.loc[df_protest.loc[df_protest.Rural==-1].index, 'Rural'] = np.nan
import re $ letters_only = re.sub("[^a-zA-Z]",           # The pattern to search for $                       " ",                   # The pattern to replace it with $                       example1.get_text() )  # The text to search $ print(letters_only)
countries = pd.read_csv("country.csv", encoding = "utf-16") $ print (countries.head())
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1 - convert_mean), convert_mean])
pd.Series(np.random.randint(0,10,10)).plot();
all_simband_data.field_stream.nunique()
base_dict_by_place = pd.concat([twitter_count, base_dict_by_place], axis=1)
X_train, X_test, y_train, y_test = train_test_split(features,regression_price,test_size=0.2)
df[['text', 'retweet_count', 'date']][df.retweet_count == np.min(df.retweet_count)]
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df['group'])
tizibika.info()
%store -r extract_deduped_0501 $ extract_deduped = extract_deduped_0501.copy()
df["Dif 50-day"] = df["50-day moving average"] - df["Closing Price"]  $ df["Dif 50-day"].min()  $ df[df["Dif 50-day"] <=0].head(3) $ df[df["Dif 50-day"] <=0].shape
df_predictions_clean.info()
preds = pd.concat([X_test,preds],axis=1)
prcp_12monthsDf['date'] = pd.to_datetime(prcp_12monthsDf['date'])
train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) $ test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])
df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)
df_master.drop(['Unnamed: 0'],axis=1,inplace=True) $ df_master.tweet_id = df_master.tweet_id.astype(str)
pd.Index(unique_vals).get_indexer(to_match)
df.info()
sns.distplot(questions_scores[:int(len(questions_scores)*0.99)])
age_hist = pd.crosstab(index=goodreads_users_df[goodreads_users_df['age'].notnull()]['age'], columns="count") $ age_hist['age_freq'] = age_hist['count'] * 100 / age_hist.sum()['count'] $ age_hist = age_hist.sort_index(ascending=True) $ age_hist.head(10)
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
crimes_by_type.reset_index(inplace=True)
data = {'date': ['2014-05-01 18:47:05.069722', '2014-05-01 18:47:05.119994', '2014-05-02 18:47:05.178768', '2014-05-02 18:47:05.230071', '2014-05-02 18:47:05.230071', '2014-05-02 18:47:05.280592', '2014-05-03 18:47:05.332662', '2014-05-03 18:47:05.385109', '2014-05-04 18:47:05.436523', '2014-05-04 18:47:05.486877'], $         'battle_deaths': [34, 25, 26, 15, 15, 14, 26, 25, 62, 41]} $ df = pd.DataFrame(data, columns = ['date', 'battle_deaths']) $ df
iowa_fc_item.layers
clean_colnames = [clean_string(colname) for colname in df_oncstage_dummies.columns] $ df_oncstage_dummies.columns = clean_colnames
df[['age']].describe()
tweets.drop(['retweeted','favorited'],1,inplace=True)
to_be_predicted_Day1 = 34.69 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df_loan2=pd.DataFrame(df_loan)
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'CA']]) $ results = logit_mod.fit() $ results.summary() $
x = my_df.text $ y = my_df.target
pd.options.display.max_colwidth = 280 $ for _, row in cleanedData.iterrows(): $     row.text = ftfy.fix_text(row.text) $ irows = cleanedData['text'].str.contains("&amp;") $ cleanedData.loc[irows, 'text'] = cleanedData.loc[irows, 'text'].str.replace('&amp;', '&')
All_tweet_data_v2.info()
image_soup = BeautifulSoup(html, 'html.parser') $ print(image_soup.prettify())
df_predictions.Q1.value_counts()
print(new_page_converted.mean() - old_page_converted.mean())
reddit['title_len'] = reddit['title'].apply(lambda x: len(x.split()))
autos = pd.read_csv("autos.csv", encoding='Latin-1') $ autos
media_user_results_df = pd.DataFrame.from_dict(results_list) $ media_user_results_df.head(10)
tweet_counts_by_month.plot()
test_post = re.sub('[^a-zA-Z]',' ',test_post) $ print(test_post) # remove special character
autos.head(3)
model.wv.similarity('king', 'man')
image_predictions.describe()
df_city_restaurants.cache()
df2.drop('ab_page2', axis = 1, inplace = True) $ df2.head()
sorted(autos["registration_year"].unique())
df.first_device_type.value_counts()
grid_lat = np.arange(24, 50.0, 1) $ grid_lon = np.arange(-125.0, -66, 1) $ glons, glats = np.meshgrid(grid_lon, grid_lat)
life = TextBlob("I love life") $ life.sentiment
np.array(actual_payments.iso_date,'datetime64[D]')[0:10]
recommendation_df.drop(['hacker_count', 'challenge_count'], axis = 1, inplace = True)
%run '../forecasting/helpers.py' $ %run '../forecasting/main_functions.py' $ %run '../forecasting/ForecastModel.py'
base = 'https://www.coursera.org/browse/' $ categories = ['arts-and-humanities', 'business', 'computer-science', 'data-science', 'information-technology', $                 'life-sciences', 'math-and-logic', 'personal-development', 'physical-science-and-engineering', $                 'social-sciences', 'language-learning']
datatmp=data[["Postal Code","Total Gallons"]][data["Customer Class"]=="Residential"] $ datatmp.index=datatmp.index.map(lambda x: datetime.strftime(x,"%Y")) $ datatmp.pivot_table(index=datatmp.index,columns="Postal Code",values="Total Gallons",aggfunc=sum).plot(legend=False) $ plt.show() $
print(vip_reason.sum(axis=0).sum()) $ print() $ print(vip_reason.sum(axis=0))
dfRegMet.info()
df.show()
raw_large_grid_df.query("subject=='VP4'&eyetracker=='pl'").duration-raw_large_grid_df.query("subject=='VP4'&eyetracker=='el'").duration
usersDf.hist(column=['followers_count'],bins=50) $
len(df[(df['Complaint Type'] == 'Homeless Encampment')&(df.index.month.isin([6,7,8]))])
for index, text in enumerate(all_text): $     cv = re.compile('crossvalidation') $     all_text[index] = re.sub(cv,'cross validation',text) $     all_text[index] = re.sub(quote,'',text)
stories[['created_dow', 'score']].corr()
zip_1_df.rename(columns = {0:"Count"},inplace=True) $ zip_2_df.rename(columns = {0:"Count"},inplace=True) $ zip_1_sns.rename(columns = {0:"Count"},inplace=True) $ zip_2_sns.rename(columns = {0:"Count"},inplace=True)
for (Year, Quarter), Data in StockData.groupby(['Date-Year','Date-Quarter']): $     print("{}-Q{} mean prices".format(Year, Quarter)) $     print(Data[StockNames].mean()) $     print()
pd.concat([s1,s2,s3],axis=1)
try: $     c.iloc[c>0.7] $ except: $     print("nah") $     print(c.iloc[c.values>0.7])
graphlab.__VERSION__
to_be_predicted_Day5 = 50.98684784 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
import pandas as pd $ data = pd.read_csv('data/data_mixed.csv') $ print(data)
pd.date_range('2018 May 1st', '2018 Jul 3', freq = 'D')
authors.sort_values('count', ascending=False).head()
from sklearn.ensemble import RandomForestRegressor $ from sklearn.ensemble import GradientBoostingRegressor
print("Select single value at index 2, x[2]=",x[2]) $ print("Select slice index 2:3, x[2:4]=",x[2:4]) $ print("One can reference the (first) index of a vlaue e.g. x.index(6) is",x.index(6))
df['country code'] = df['country code'].astype(str) $ df.info()
df_restaurants.select('name','categories').limit(10).toPandas()
sns.regplot(x=final["BTC Volume"], y=final['Crypto Compound'], fit_reg=False) $ sns.regplot(x=final["BTC Volume"], y=final['Crypto Negative'], fit_reg=False, color = 'r') $ sns.regplot(x=final["BTC Volume"], y=final['Crypto Positive'], fit_reg=False, color = 'g')
pd.set_option('display.max_colwidth', -1) $ data_2017_12_14_iberia["text_2"].head()
sum(adjectives.values()) # number of adjective types
zipcode = list(set(trips.zip_code.unique())) $ zipcode = sorted(zipcode) $ for i in zipcode: $     print i,
log_reg_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_reg_mod.fit()
id = 'realDonaldtrump' $ new_tweets = api.user_timeline(screen_name = id,count=20) $ print(len(new_tweets)) $ print(new_tweets)
collection.list_snapshots()
for column in ['Announced At','Created At','Shipped At','Updated At']: $     df[column] = pd.to_datetime(df[column])
print(df.shape) $ df.tail()
nold = df2[df2['group'] == 'control'].shape[0] $ nold
prec_wide_df = pd.concat([grid_df, prec_df], axis = 1) $ prec_wide_df.head()
loan_requests_indebtedness_web.to_clipboard()
my_df.info()
import numpy as np $ extraRecordsForLag = pd.DataFrame({'ID': ['HeatPump', 'S Cmprsr', 'N Cmprsr', 'Chiller', 'Boiler'] * 720,'DATE':[np.nan, np.nan, np.nan, np.nan, np.nan] * 720, 'TEMP':[ np.nan, np.nan, np.nan, np.nan, np.nan] * 720,'X':[np.nan, np.nan, np.nan, np.nan, np.nan] * 720, 'Z':[ np.nan, np.nan, np.nan, np.nan, np.nan] * 720, 'Y':[ np.nan, np.nan, np.nan, np.nan, np.nan] * 720}) $ extraRecordsForLag = extraRecordsForLag[['ID','DATE','TEMP','X','Y','Z']] $ print extraRecordsForLag;
driver.quit() # Always remember to close your browser!
df_sale_price =df_sale_price.transpose()          ##Transposing the dataframe in order to better analyze the data $ df_sale_price =df_sale_price.iloc[5:,]            ##removing unnecessary columns $ df_sale_price.reset_index(level=0,inplace=True) $ df_sale_price.columns=['Date','Sale_Price']    ##renaming the columns to more meaningful names $ df_sale_price.head(5)
now = datetime.datetime.now() $ time_stamp =  str(now.strftime("%Y%m%d%H%M%d")) $ csv_name = 'bridge_data_' + time_stamp + '.csv' $ print('csv name: ', csv_name) $
df = tables[0] $ df.columns = ['Description', 'Values'] $ df
month_year_crimes.plot()
(null_val < actual_diff).mean()
print(c_df.isnull().sum()) $
model_rf_20_14 = RandomForestClassifier(min_samples_leaf=20, max_depth=14, random_state=42) $ model_rf_20_14.fit(x_train,y_train) $ print("Train: ", model_rf_20_14.score(x_train,y_train)*100) $ print("Test: ", model_rf_20_14.score(x_test,y_test)*100) $ print("Differnce between train and test: ", model_rf_20_14.score(x_train,y_train)*100-model_rf_20_14.score(x_test,y_test)*100)
cercanasAfuerteApacheEntre50Y75mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 50) & (cercanasAfuerteApache['surface_total_in_m2'] < 75)] $ cercanasAfuerteApacheEntre50Y75mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre50Y75mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre50Y75mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
round_count = pd.DataFrame(df.groupby('funding_rounds').size()).reset_index() $ round_count.columns = ['funding_rounds','count']
from sklearn.model_selection import train_test_split $ y_train_full = train_df['Winner'] $ x_train_full = train_df.drop('Winner', axis=1) $ from sklearn.model_selection import train_test_split $ x_train, x_cv, y_train, y_cv = train_test_split(x_train_full, y_train_full, test_size=0.25, random_state=42)
needles = re.findall(r'\d{1,2}\.(mp3|wav)', haystack) $ print(needles)
df_new['US_ab_page'] = df_new['US'] * df_new['ab_page'] $ df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new.head()
df_twitter_copy = df_twitter_copy[df_twitter_copy.expanded_urls.notnull()]
df3.at[d[3],'A'] = 0 $ df3
df_uro_dd_dummies_no_sparse = df_uro_dd_dummies.drop(columns=ls_sparse_cols)
autos = autos[autos["price"].between(1,350000)] $ autos["price"].describe()
points = points.reindex(mindex) $ points
word = "best" $ try: $     print(model.most_similar(word)) $ except: $     print(word, "not in the vocab")
com_grp.ngroups
advancedmodel = LogisticRegression() $ advancedmodel = advancedmodel.fit(advancedtrain, train["rise_in_next_week"])
ab.user_id.nunique()
S_lumpedTopmodel.forcing_list.filename
to_be_predicted_Day1 = 14.46 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
unsegmented_users.groupby(['cohort_availability']).user_id.count()
with open(join(DATA_FOLDER, 'gold.json'), 'r') as f: $     gold = json.load(f)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=16000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
iris.groupby('Species')['Species'].count()
d = pd.date_range('11-Sep-2017', '17-Sep-2017', freq='2D') $ d + pd.Timedelta('1 days 2 hours')
total_new_page = (df2['landing_page'] == 'new_page').sum() $ total_new_page/unique_users
from pandas import * $ from pandas.io.json import json_normalize
gmm.fit(X) $ labels = gmm.predict(X) $ labels
points[[p.endswith("a") for p in points.index]]
final_valid_pred_nbsvm1 = valid_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-08-19&end_date=2018-08-19&api_key={}'.format(API_KEY), auth=('user', 'pass'))
log_mod= sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results= log_mod.fit()
pd.concat(g for _, g in df_click.groupby("user_session") if len(g) > 1).shape # just an oberservation
df["Complaint Type"].value_counts().head(5).sort_values().plot(kind='barh')
pd.Timestamp('2010/11/12')
url='http://www.ign.com/articles/2017/10/26/super-mario-odyssey-review?watch' $ html=requests.get(url).content $ soup=BeautifulSoup(html,'html5lib') $ article = soup.find_all('article') $ plist = article[0].find_all('p',text=True)
unassembled_human_genome_length = gdf[gdf['type'] == 'supercontig'].length.sum() $ percentage_incomplete = (unassembled_human_genome_length / human_genome_length)*100 $ print("{}% of the human genome is incomplete.".format(round(percentage_incomplete, 4)))
noloc_df = df[(df.city == '') & (df.state == '') & (df.zipcode_initial == '')].copy() $ df = df[~((df.city == '') & (df.state == '') & (df.zipcode_initial == ''))].copy()
cityID = '1661ada9b2b18024' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Wichita.append(tweet) 
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
with tb.open_file(filename='data/file2.h5', mode='w') as f: $     f.create_array(where='/',  name='array2', obj=[4, 5, 6, 7])
mnb = MultinomialNB() $ param_grid = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], $               'fit_prior':[True, False]} $ mnb_gd = GridSearchCV(estimator=mnb, param_grid=param_grid, cv=5, scoring='f1', n_jobs = -1) $ mnb_gd.fit(X_train, y_train)
merged=pd.merge(train,items,on='item_nbr', how='left') $ print("Rows and columns:",merged.shape) $ pd.DataFrame.head(merged)
until_nye.days
train_df["num_description"]= train_df["description"].apply(num_description) $
rf_tfidf.fit(X_train, y_train) 
writer = pd.ExcelWriter('auditfile.xlsx') # de auditfile kunnen we exporteren naar excel om vervolgens te gebruiken voor je doeleinden. $ auditfile.to_excel(writer, 'export_af') $ writer.save()
df_en['polarity_vader'].plot.hist(figsize=(10,5), bins=70, title="Vader Histogram")
print(df2['timestamp'].max()) $ print(df2['timestamp'].min())
df.nunique()
df4.head()
events_top10_df = events_enriched_df[events_enriched_df['topic_id'].isin(top10_topics_list)].copy() $ events_enriched_df.shape[0], events_top10_df.shape[0]
x1 = pd.DataFrame(df.groupby(['donor_id','zipcode']).zipcode.nunique()) $ x1[x1.zipcode != 1]
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("P of conversion of old page (p_old):", p_old)
df_clean.drop(df_clean[(df_clean.rating_numerator > 20)].index,inplace=True);
df_cust_data['Registration Date'] = pd.to_datetime(df_cust_data['Registration Date'])
who_purchased.columns = ['BOUGHT_'+str(col) for col in who_purchased.columns]
from pyramid.arima import auto_arima
gender.value_counts()
df.dtypes
print(model_data.groupby('adopted_user').count())
retweets = deep_learning_tweet.retweets() $ len(retweets)
df2.query('user_id == 773192')
clf.fit(digits.data[:-2], digits.target[:-2])
pop = {'Nevada': {2001:2.4, 2002: 2.9}, $       'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}
l_t = [i for i in dummy_features_test if i not in train.columns.tolist()] $ print('%d dummy_features in test not in train.'%len(l_t))
race_vars.columns = race_vars.columns.str.replace(' ', '_') $ race_vars.columns = race_vars.columns.str.replace('/', '_') $ race_vars.columns = race_vars.columns.str.lower()
%matplotlib inline $ cat_group_counts = df.groupby("category").size().sort_values(ascending=False)[:10] $ cat_group_counts.plot(kind="bar", title="Top 10 Meetup Group Categories")
grouped.columns
np.random.seed(123456) $ ps = pd.Series(np.random.randn(12),mp2013) $ ps
soup.find_all('div','g') # find all links $ for x in soup.find_all('div','g'): $     print(urllib.parse.parse_qs(urllib.parse.urlsplit(x.find('a')['href']).query)['q'][0]) ; print()
df_users_test = df_users.iloc[:2, :] $ df_users_test.created_on[1] = '2017-09-20' $ df_users_test
image_1 = np.expand_dims(x_test[0], axis=2) $ image_2 = np.expand_dims(x_test[1], axis=2)
data['fico_diff'] = data['fico_range_high'] - data['fico_range_low'] $ data[~data['last_pymnt_d'].isnull()].groupby(['loan_status']).agg({'fico_diff':'mean', 'member_id':'count'})
act_stations = session.query(Measurement.station,func.count(Measurement.station)).\ $                group_by(Measurement.station).\ $                order_by(func.count(Measurement.station).desc()).all() $ act_stations
g=sns.lmplot(y='favorite_count', x='rating', data=matrix, $            palette="muted", size=4,scatter_kws={"s": 10, "alpha": 1}) $ g.set_axis_labels( 'Rating',"Favoriate").set(xlim=(0,2.5),ylim=(0,30000))
ax = ign.groupby('release_year').size().plot(kind = 'bar') $ ax.set_ylabel('Games released')
df.dropna()
pd.read_sql('select * from sessions limit 5;',cnx)
sns.countplot(data=data, x='OutcomeType',hue='Name', palette="Set3")
en_translation_counts = en_es.groupby(by='en').size() $ en_translation_counts[en_translation_counts > 1].hist(bins=10)
returns = generate_returns(close) $ helper.plot_returns(returns, 'Close Returns')
new_page_converted = np.random.choice([0, 1], size=nnew, p=[1-pnewnull, pnewnull])
pd.Series(d)
tags = pd.read_csv('data/tags.csv', index_col = 0) $ tags.head()
metrics.accuracy_score(classes, resultados)
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $ filter(Measurement.station == 'USC00519281').all() $
knn.fit(train[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2','floor']], train['rooms'])
weblogs_partitioned.count()
old_page_converted=np.random.choice([0,1],n_old,p=[1-p_old,p_old])
print("Percentage of positive tweets: {}%".format(data_spd.query('SA>1').shape[0]*100/len(data_spd['tweets']))) $ print("Percentage of neutral tweets: {}%".format(data_spd.query('SA==1').shape[0]*100/len(data_spd['tweets']))) $ print("Percentage of negative tweets: {}%".format(data_spd.query('SA<1').shape[0]*100/len(data_spd['tweets'])))
a = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].count() $ print('The number of times new_page and treatment dont line up:{}'.format(a['user_id'])) $
twitter_data = twttr_data() $ Sentiment_pd = sentiment(twitter_data) $ sentiment_pd.to_csv('../Data_Sources/twitter_sentiments.csv') $ twitter_data.to_csv('../Data_Sources/twitter_data.csv')
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count() $ commits_per_year.rename(columns={'author': 'num_commits'}, inplace=True) $ commits_per_year.head(5)
df_full["Field4"] = df_full.Field4.fillna("None")
department_df_sub.applymap(lambda x: '%.2f' % x) # insert 2 decimal places for each element in the Series 
punct_re = '[^\w\s]' $ trump['no_punc'] = trump['text'].str.replace(punct_re, " ") $
most_active = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $     filter(Measurement.station == "USC00519281").\ $     order_by(Measurement.date).all() $ most_active = pd.DataFrame(most_active) $ most_active["tobs"].describe()
train.head(3)
exiftool -csv -createdate -modifydate cisnwh8/cisnwh8_cycle1.MP4 cisnwh8/cisnwh8_cycle2.MP4 cisnwh8/cisnwh8_cycle3.MP4 cisnwh8/cisnwh8_cycle4.MP4 cisnwh8/cisnwh8_cycle5.MP4 cisnwh8/cisnwh8_cycle6.MP4 > cisnwh8.csv
age_gender.shape $ age_gender.head(5) $
len(df[~(df.groups == {})])
ts_split_over = TimeSeriesSplit(n_splits=3).split(X_train)
data_2018.to_csv('/Users/annalisasheehan/Dropbox/Climate_India/Data/climate/CPC/cpc_global temperature/minimum temperature/extracted_data/tmin.2018.csv')
ticket3 = data_df.clean_desc[22] $ parsed3 = nlp(ticket3) $ svg1 = displacy.render(parsed3, style='dep', jupyter=True, options={'distance':140})
for res_key, df in entso_e.items(): $     logger.info(res_key + ': %s', df.shape)
p_old = df2['converted'].mean()
df_city_restaurants.select('business_id', 'business_idn').take(5)
df.head()
for i, image in enumerate([x_test[0], x_test[1]]): $     plt.subplot(2, 2, i + 1) $     plt.axis('off') $     plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
we_rate_dogs = pd.read_csv('twitter-archive-enhanced.csv')
dfa_1=dfa["driver_count"].sum() $
log_m = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = log_m.fit()
headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'} $ r = requests.get(url, headers=headers) $
taxi_hourly_df.shape
prediction = pd.DataFrame() $ for modelName in models: $     print(modelName) $     prediction[modelName] =  cross_val_predict(models[modelName] , X.fillna(-1) , y , cv = skf, verbose =2, method='predict_proba') $
lbl = preprocessing.LabelEncoder() $ lbl.fit(list(drace_df['manager_id'].values)) $ drace_df['manager_id'] = lbl.transform(list(drace_df['manager_id'].values))
df[df.client_event_time >= datetime.datetime(2018,4,2,0,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
df2.query('group == "treatment" and converted == 1').user_id.count() / df2.query('group == "treatment"').user_id.count() $
dow_columns=pd.get_dummies(df2['dow']) $ dow_rate=pd.DataFrame([(lambda x:(df2[x] * df2.converted).sum()/df2[x].sum()) (x) for x in dow_columns], index=list(pd.get_dummies(df2['dow']).columns), columns=['conversion_rate']) $ dow_rate
df.groupby("newsOutlet")["compound"].max()
it_df.columns
user_info_df = pd.read_csv('source_data/user_info_lookup.csv', names=['user_id', 'name', 'organization', 'position', $                                             'gender', 'followers_count', 'following_count', 'tweet_count', $                                             'user_created_at', 'verified', 'protected'], $                           dtype={'user_id': str}).set_index(['user_id']) $ user_info_df.count()
intersections_irr['estimated_number_vehicles'] = [get_number_of_vehicles_per_segment(row['avg_traffic_flow'],row['speed'],row['SHAPE_Leng']) for index,row in intersections_irr.iterrows()]
Counter(orgs['primary_role']).most_common(10)
import statsmodels.api as sm $ logit_mod2 = sm.Logit(df4['converted'], df4[['intercept','UK','US']]) $ results2 = logit_mod2.fit()
cFrame = cumFrame.groupby(('Client','Date')).sum() $ cFrame.head(20)
p_new = new_page_converted.sum() / len(new_page_converted) $ p_old = old_page_converted.sum() / len(old_page_converted) $ print('p_new - p_old = ', p_new - p_old)
year_prcp_df.describe()
len(df2.query('converted==1').index)/len(df2.index)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt" $ mydata = pd.read_table(path, sep= '\t') $ mydata.head(5)
bad_dul_isbns = pd.read_table("C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/Duluth invalid ISBNs.txt") $ bad_dul_isbns = bad_dul_isbns['ISBN'] $ bad_dul_isbns.size
lr=df2.copy() $ lr['intercept']=1 $ lr[['control','ab_page']]=pd.get_dummies(lr['group'])
df_goog[['Open', 'High', 'Low', 'Close', 'Adj Close']].plot()
dfJobs['DTSTART'] = pd.to_datetime(dfJobs['DTSTART']) $ dfJobs['DTEND'] = pd.to_datetime(dfJobs['DTEND']) $ dfJobs['DTSTAMP'] = pd.to_datetime(dfJobs['DTSTAMP']) $ dfJobs['CREATED'] = pd.to_datetime(dfJobs['CREATED']) $ dfJobs['LAST MODIFIED'] = pd.to_datetime(dfJobs['LAST MODIFIED'])
df = df[df['SUMLEV']==50] $ df.set_index(['STNAME','CTYNAME'], inplace=True) $ df.rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'})
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
lsi.print_topics(1) $
print("The proportion of users converted: {}".format(df["converted"].mean()))
tipsDF.dtypes
tweet=results[16] $ for param in dir(tweet): $     if not param.startswith("_"): $         print "%s : %s" % (param, eval("tweet." + param))
sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old]) $ sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])
image_predictions.head()
pd_aux['permit_number_notM']=pd_aux['permit_creation_date'].apply(lambda x: str(x)[0:10].replace('-',''))+pd_aux['permit_number'].apply(lambda x: x[3:]) $ pd_aux.head(5)
ma_8.plot() $ ma_34.plot()
list_to_merge = list(db.tweets.find({},{"id": 1, "user": 1,"text": 1,"hashtags":1, "_id": 0}))
sns.distplot(answers_scores[:int(len(answers_scores)*0.95)])
df_group_by.columns = ['msno','payment_method_id','payment_plan_days','not_auto_renew','msno_count','is_cancel','is_discount','amount_per_day','membership_duration','membership_expire_date']
test['visitors'] = preds3 $ test['visitors'] = np.expm1(test['visitors']).clip(lower=0.) $ sub1 = test[['id','visitors']].copy() $ sub1.to_csv('sub_single_xgb_nocv.csv',index=False)
df.iloc[[11,24, 37]]
genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId']) $ genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1) $ genreTable.head()
articles = db.get_sql(sql) $ articles.head()
mood=Stockholm_data_final.iloc[:,10] $ pd.value_counts(mood).reset_index()
np.mean(df2.days_active)
df2['abtest'].value_counts().plot(kind='bar') $ print(df2['abtest'].value_counts())
df2_control = df2.query('group == "control"') $ agg_df2_control = df2_control.query('converted == "1"').user_id.nunique() / df2_control.user_id.nunique() $ agg_df2_control
re_split_raw = re.findall(r"\w+(?:[-']\w+)*|'|[-.(]+|\S\w*", raw) $ print(re_split_raw[100:150])
s = pd.Series(np.random.np.random.randn(5), index=list('abcde')) $ s
df.index
session.query(func.count(User.id)).scalar()
test_pred_svm = lin_svc_clf.predict(test_cont_doc)
pnew = df2['converted'].mean() $ print(pnew)
ac['Dispute Resolution End'].groupby([ac['Dispute Resolution End'].dt.year]).agg('count')
tweet_data.info()
dataframe.groupby('year').daily_worker_count.agg(['count','min','max','sum','mean'])
scidat = pandas.read_csv('./scientists_1.csv') $ print(scidat.columns) $
us.loc[condition, ['cityOrState']] = us[condition]['cityOrState'].str.extract(r'([A-Z]{2})') $ us['cityOrState'].value_counts(dropna=False)
1/np.exp(results.params)
xmlData['sqm.living_area'] = pd.to_numeric(xmlData['sqm.living_area'], errors = 'raise') $ xmlData['sqm.lot_area'] = pd.to_numeric(xmlData['sqm.lot_area'], errors = 'raise') $ xmlData['sqm.upper_area'] = pd.to_numeric(xmlData['sqm.upper_area'], errors = 'raise') $ xmlData['sqm.basement_area'] = pd.to_numeric(xmlData['sqm.basement_area'], errors = 'raise')
b1 = phimage('btime7') $ phi = b1.phi2('phi.1769') $ plt.ion() $ plt.plot(np.arange(len(phi[-1])), phi[-1])
for r in k.groupby('Gender'): $     print r
annotations_df = pd.merge(raw_annotations_df.drop('answers', axis=1), answers_df, left_index=True, right_index=True) $ annotations_df.tail(5)
stations_df  = pd.DataFrame(stations,columns=["Station Name", "Station ID"]) $ stations_df
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ print (convert_old , convert_new , n_old , n_new)
df["cancelled"].mean()
plt.hist(Profit, bins=50,alpha=0.5) $ plt.hist(RandomPercentage, bins=50,alpha=0.5) $ plt.ylabel("Frequency") $ plt.xlabel("percentual change used for profit") $ plt.show()
retweets = cleanedData['text'].str.startswith("RT @") $ print("Num retweets: ", sum(retweets)) $ cleanedDataNoRetweets = cleanedData[~retweets]
df.shape
print(X_train.shape, y_train.shape) $ print(X_test.shape, y_test.shape) $
df_new['ab_page_CA']=df_new['ab_page']*df_new['CA'] $ df_new['ab_page_UK']=df_new['ab_page']*df_new['UK'] $ log_mod_new2=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','ab_page_CA','ab_page_UK']]) $ results_new2=log_mod_new2.fit() $ results_new2.summary2() $
df.ix[306]['yob']
plot_q_table(q_agent_new.q_table)
from pyspark.sql import SQLContext, Row $ import datetime $ from collections import namedtuple $ import numpy as np $ import pandas as pd
train_orders=orders[orders['eval_set']=='train'] $ trains=pd.merge(order_products_train, train_orders,how='left', on='order_id') $ trains.head()
indexes_insert_1 = df3.query('group == "treatment"').index $ df3.set_value(index = indexes_insert_1, col ='abs_page', value=1) # sets value 1 only for the selected indexes $ df3.set_value(index = df3.index, col ='intercept', value=1) # sets value 1 for all rows with no preselected indexes $ df3.head() $ df3.isnull().sum()
Counter(df2.Language)
import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline $ pd.set_option('display.max_colwidth', -1)
df2[['CA', 'UK','US']]=pd.get_dummies(df2['country']) $ df2.head(1)
conv_ind = df2.query('converted == 1') $ num_conv = conv_ind.shape[0] $ print('Probability of converted individual regardless of the page: {:.4f}'.format(num_conv/df2.shape[0]))
abc = Grouping_Year_DRG_discharges_payments.groupby(['year','drg3']).\ $ agg({'discharges':[np.size,np.sum, np.mean,np.max]}) $ abc.head()
transactions = transactions.head(n = 500)
pd.value_counts(ac['Registration Date']).head()
hs.info()
from pyspark.ml.evaluation import MulticlassClassificationEvaluator $ evaluator = MulticlassClassificationEvaluator( $     labelCol="label", predictionCol="prediction", metricName="accuracy") $ accuracy = evaluator.evaluate(predictions) $ print("Accuracy = %g " % (accuracy))
pd.crosstab(result['timestampDate'], result['type']).head()
lr_model_newton = LogisticRegression(C=0.01, class_weight='balanced', max_iter=50, solver='newton-cg')
ttTimeEntry.head()
print (" The text: %s \n The grade in rating_numerator: %.1f \n" % (df['text'].ix[695], df['rating_numerator'].ix[695]))
reader = pd.read_csv(dataurl+'train.csv.gz', sep=',', compression='gzip', chunksize=100000)
X_sample = X.loc[4232] $ y_sample = y.loc[4232] $ X_sample, y_sample
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.join(item_onpromotion, 'item_nbr', 'left_outer') $
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "CNN"]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "CNN"]["tweet_vader_score"],fit_reg=False,marker = "^",scatter_kws={"color":"purple","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (CNN News)",fontsize = 12) $ plt.savefig('Sentiment_CNN.png')
sns.heatmap(temp)
observations_node = columns["fields"]["oSets"]["oPoints"]["observations"] $ [item.name for item in get_child_column_data(observations_node)]
df.columns
company_vacancies['weekday'] = company_vacancies['created'].apply(lambda x: x.weekday() + 1) $ company_vacancies['hour'] = company_vacancies['created'].apply(lambda x: x.hour)
df_control_group = df2.query('group == "control"') $ conversion_rate_control = df_control_group.query('converted == 1').shape[0] / df_control_group.shape[0] $ print('The probability of an individual from the control group converting is {}'.format(conversion_rate_control))
dfs = pd.read_html(html_string) $ dfs
model2 = sm.Logit(df_new.converted, df_new[['intercept','US', 'CA']]) $ result2 = model2.fit() $ result2.summary()
hdb.head()
test_portfolio['weight'] = 0.5 $ test_portfolio['capital'] = 10000 $ test_portfolio['share'] = test_portfolio['weight'] * test_portfolio['capital'] / test_portfolio['price'] $ test_portfolio['net'] = test_portfolio['price'] * test_portfolio['share']
thecmd = 'curl -v -F file=@'+curDir+'/'+dataDir+'processing/new-york_new-york_points.csv "https://'+USERNAME+'.carto.com/api/v1/imports/?api_key="'+APIKEY $ os.system(thecmd) #run the command to curl the input file, this should work as its not GDAL/OGR
((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idx, df, yl.astype(np.float32))
df_final['total_cust'] = df.groupby('user_id', as_index=False)['customer_number'].nunique()
austin['yr_mo'] = austin['started_on'].apply(lambda x:x.strftime('%Y-%m'))
df3=df2.copy() $ df3[['ab_page','old_page']] = pd.get_dummies(df3['landing_page']) $ df3['intercept']=1 $ df3=df3.drop(['old_page'], axis=1) $ df3.head(5) $
df2 = df.drop(index_to_remove)
np.arange(10, 50)
columns2=['eval_set','purchase_count_spec','reordered_count_spec'] $ test_orders_prodfill.drop(columns2, inplace=True, axis=1)
df_gnis = pd.read_csv(file_name+'_20180601.txt', sep='|', encoding = 'utf-8')
parsed = stream.map(lambda x: format_sample(x))
df_X_train.head()
pd.value_counts(dr_new['ReasonForVisitName'])
os.environ["s3_key"] = "s3://" +s3_bucket+"/"+ s3_key_builtup_merge_nodata_set_to_zero $ os.environ["gs_key"] = "gs://resource-watch-public/" + s3_key_builtup_merge_nodata_set_to_zero $ os.environ["asset_id"] = "users/resourcewatch/cit_014_built_up_areas_nodata_set_to_zero" $ !gsutil cp $s3_key $gs_key
output.head()
store_items.insert(4, 'shoes', [8,5,0])
records.loc[records['Gender'].isnull(), 'Gender'] = gender
%%time $ dictionary = corpora.Dictionary.load(join(DATA_FOLDER, 'fltrd_nbelow10_nabove0.3_keepn100000.dict')) $ corpus = corpora.MmCorpus(join(DATA_FOLDER, 'fltrd_nbelow10_nabove0.3_keepn100000_corpus.mm'))
new_page_converted  = np.random.choice([1, 0], size=n_new, p=[convert_p_new, (1-convert_p_new)])
df_carto.head()
df2['intercept'] = 1 $ df2['ab_page'] = df2.group.map(lambda x: 1 if x == 'treatment' else 0)
from scipy.stats import norm $ print('Critical value:'+str(norm.ppf(1-(0.05)))) $
session.query(func.count(Mea.date)).all()
tfidf.get_feature_names()
pings.count()
df_new['intercept'] = 1 $ lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ results = lm.fit() $ results.summary()
1/np.exp(-0.0674)
sb.pairplot(cats_df[cats_df.columns[:]].dropna())
tweet_data_v2.tweet_id.unique().shape
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     f.create_array(where='/gr1/gr2/gr3', $                    name='some_not_yet_existing_array', $                    obj=[4, 5, 6, 7], $                    createparents=True)
array_masked = np.ma.masked_values(array, 99.9) $ print(array_masked[0,:])
from statsmodels.stats.diagnostic import acorr_ljungbox $
df_vow['Close'].unique()
pgh_311_data_merged['Category'].value_counts(ascending=True).plot.barh(figsize=(10,10))
software_techniques = lift.get_techniques_used_by_software('BITSAdmin')
print(df.head()) $
baseball_h = baseball.set_index(['year', 'team', 'player']) $ baseball_h.head(10)
brand_names = autos["brand"].value_counts().index
X_train.isnull().sum()
skip.head(4)
week5 = week4.rename(columns={35:'35'}) $ stocks = stocks.rename(columns={'Week 4':'Week 5','28':'35'}) $ week5 = pd.merge(stocks,week5,on=['35','Tickers']) $ week5.drop_duplicates(subset='Link',inplace=True)
fig = goes_lightcurve.peek() $ fig = lyra_lightcurve.peek()
filename = 'data/pulled_tweets/PT_all_airlines_df' $ filehandler = open(filename,"wb") $ pickle.dump(df, filehandler)
results=logistic_model3.fit() $ results.summary()
asf_agg_by_gender_df.count()
def flight_type(x): $     if 'light search:' in x: $         return re.search(r'(^.*)( search:)', x).group(1).lower()
disag_filename = join(data_dir, 'disag_gjw_hart.hdf5') $ output = HDFDataStore(disag_filename, 'w') $ h.disaggregate(mains,output,sample_period=1) $ output.close()
df_clean3.loc[1202, 'text']
df = pd.read_pickle(pretrain_data_dir+'/pretrain_data_02.pkl')
ca_pl_path = cwd + '\\data_for_pl_ca_simu.csv' $ ca_pl_all = pd.read_csv(ca_pl_path) $ print(ca_pl_all.shape)
tweet_archive.info()
df_measures = pd.read_csv('../data/interim/df_measures.csv', encoding="utf-8")
ad_group_performance.loc[4, 'Date']
len(lxml.html.tostring(_html))
df['day'] = df.day.map({1: 'mon', 2: 'tues', 3: 'weds', 4:'thurs', 5:'fri'   })
type(customer_visitors.DateCol.dt.dayofweek)
other_stopwords = ['http://', 'https://www.', '\xe2\x80\xa6'] $ stop_words = set(stopwords.words('english')) $ stop_words |= set(other_stopwords)
(df['converted'].value_counts()[1].sum()) / ((df['converted'].value_counts()[0]) + (df['converted'].value_counts()[1]))
groups = contract_history[['INSTANCE_ID', 'UPD_DATE']].merge(intervention_train[['INSTANCE_ID', 'CRE_DATE_GZL']])
for u in range(len(U_B_df)): $     ixs = np.unique([a.strip() for a in U_B_df.loc[u,'cameras']]) $     df_slice = USER_PLANS_df.loc[ixs] $     USER_PLANS_df.loc[str(ixs)] = (pd.Series([list(chain(*df_slice[c])) for c in df_slice.columns],index=USER_PLANS_df.columns)) $     USER_PLANS_df.drop(ixs,inplace=True)
loans_act_origpd_xirr=cashflows_act_origpd_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
for idx,file in enumerate(files): $     dat = pd.read_excel(dir+file) $     if idx == 0: $         datAll = dat $     datAll = pd.concat([datAll,dat])
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
TEXT = data.Field(lower=True, tokenize="spacy")
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] == 'control', 0, 1) $ df2.tail()
result.head() $
user = user.set_index("name") $ user.head(3)
df[df.index.month.isin([12,1,2])]['Complaint Type'].value_counts().head()
precioPromedioM2 = properati[['state_name','price_per_m2','price_usd_per_m2','price_aprox_usd']].groupby('state_name').agg(['mean','sum'])\ $         .sort_values(('price_usd_per_m2','sum'),ascending=False)[('price_usd_per_m2','mean')]\ $             .plot(kind='bar',title='Promedio de precios por m2 en USD en distinta zona',figsize=(18,8)) $ precioPromedioM2.set_xlabel("Zona") $ precioPromedioM2.set_ylabel('Promedio de precio en USD')
def date_index(df, coin): $     return df[df['symbol'] == coin].set_index('date') $ pricesLTC = date_index(prices, 'LTC') $ pricesETH = date_index(prices, 'ETH') $ pricesXRP = date_index(prices, 'XRP')
full_data = pd.concat([jobs_data1, jobs_data2, jobs_data3, jobs_data4], ignore_index=True)
weather_sorted = weather_all.sort_values('Temp (deg C)') $ weather_sorted.head()
plt.hist(p_diffs, edgecolor='gray'); $ plt.axvline(x=obs_diff, color='k',linestyle='dashed');
(autos['ad_created'] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
countDict=Counter([word.lower() for word in blob.words])
import matplotlib.pyplot as plt $ sales_data_group.sort_values(['Email', 'Paid at']) $ sales_data_group['days since start'].hist(bins = 20)
df['AgeVideo'][df['AgeVideo']>30].count()
to_be_predicted_Day1 = 36.61 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
cols = ['doggo', 'floofer', 'pupper', 'puppo'] $ archive_df[cols] = archive_df[cols].replace('None', '') $ archive_df['doggolingo'] = archive_df[cols].apply(lambda x: ';'.join(filter(None, x)), axis=1) $ archive_df.drop(cols, axis='columns', inplace=True, errors='ignore')
df_z= df_cb.groupby(["landing_page","group"]).count() $ df_z $
df['marks']= zero_labels $ df.tail()
precip_data_df.head(3) $
master_file.to_csv(os.curdir + '/master.csv', index=False)
!which chromedriver $ executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False) $ url_hemispehere = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(url_hemispehere) $
loan_requests1['indebt:actual'].notnull().sum()
db=client.tweets
nba_df.head(2)
auth = tweepy.OAuthHandler(consumer_token, consumer_secret) $ auth.set_access_token(access_key, access_secret)
testObjDocs.outDF.drop(testObjDocs.outDF.index[985:990], inplace=True)
tmp_df.reset_index().to_file('geocoded_evictions_deidentified.geojson', driver='GeoJSON')
dedups.isnull().sum()
X_train = np.concatenate((train[simple_features].values, X_train_feature_counts.toarray()), axis=1)
old_page_converted = np.random.choice([1,0],size = len(old),p = [convert,1-convert])
newp = len(df2.query('landing_page == "new_page"')) $ total = len(df2) $ newp_pr = newp / total $ newp_pr
print("Number of Groups in PRE-ATT&CK") $ print(len(all_pre['groups'])) $ df = all_pre['groups'] $ df = json_normalize(df) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
n_old = df2[df2['group'] == 'control'].shape[0] $ n_old
sm.stats.durbin_watson(df['NASDAQ.AAPL'])
sb.heatmap(components2)
type(df.columns)
word_freq_df.head()
stockItems = sqlContext.sql("select distinct stockCode, description from retailPurchases") $ stockItems.registerTempTable("stockItems") $ sqlContext.sql(query).toPandas()
AAPL.iloc[0:200,0:4].diff().hist(bins=25)
t = final_df.loc[max_vals_idx] $ t.website_url[:20]
tweets = pol_tweets.append(troll_tweets) $ users = pol_users.append(troll_users)
df['Name'].apply(returnName).value_counts().head(20)                                   
np.save('myfile.npy', a)  #Save `a` as a binary .npy file.
all_number_classes = [] $ for item in piotroski_univ_sets: $     all_number_classes.append(item.string)
[x for x in tweets_df.userLocation if 'Costa Rica' in x] $
final = pd.merge(merged, areas, on='state', how='left') $ final.head()
periods = 31 * 24 $ hourly = Series(np.arange(0,periods),pd.date_range('08-01-2014',freq="2H",periods=periods)) $ hourly
X = stock.iloc[925:-1].drop(['volatility', 'volume', 'high', 'low', 'close', 'open'], 1) $ y = stock.iloc[925:-1].volatility
tweets ['apple'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'apple' in x else False) $ tweets ['samsung'] = (tweets['hashtags'] + tweets['user_mentions']).apply(lambda x: True if 'samsung' in x else False)
!gsutil cp gs://solutions-public-assets/smartenup-helpdesk/ml/issues.csv $CSV_FILE
american_counts.head(3)
df2 = df2.drop_duplicates(subset = 'user_id')
df1.tail(36)
res = [] $ for url in urls: $     res.append(get_detail(url))
clients.merge(stats, left_on = 'client_id', right_index=True, how = 'left').head(10)
actual_diff = p_conv_treat - p_conv_ctrl $ plt.hist(p_diffs) $ plt.axvline(actual_diff, c='red');
tweet_df.head()
test_embedding=test_embedding.rename({'DETAILS3':"WordVec"}, axis=1)
data['sepal_area'] = data.sepal_length * data.sepal_width $ data['abbrev'] =  data.species.apply( lambda x:  x.replace('Iris-','') ) $ print(data.iloc[:5, 4:])
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2014 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2014 Percentiles.txt"
donors_c.loc[donors_c['Donor Zip'].notnull(), 'Donor Zip'].value_counts().tail(20)
bad_iv.groupby(['Strike']).count()['Expiration']
y.info()
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ GROUP BY twitter.tag_text \ $ ORDER BY count DESC LIMIT 10;
sum(autos["registration_year"].between(0,2016))
c.execute('SELECT city FROM weather where warm_month = "July"') $ print(c.fetchall())
model = ols("happy ~ age + income", training).fit() $ model.summary()
df.info()
golden_size = lambda width: (width, 2. * width / (1 + np.sqrt(5)))
post_creation = {} $ for row in db_engine.execute(text("select * from posts WHERE subreddit_id = '2qh13';")): $     post_creation[row['id']] = row['created']
df = pd.DataFrame.from_records(mylist) $ df.head()
movielen=len(movies['movieId'].unique()) $ userlen=len(ratings['userId'].unique()) $ sparsity=(len(ratings)*100)/(movielen*userlen) $ print(sparsity)
response.status_code, response.url
df2[(df2.group == 'treatment')&(df2.converted == 1)].shape[0]/df2[df2.group == 'treatment'].shape[0]
pd.read_json(path_or_buf='http://api.github.com/events')
my_model_q9 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label') $ my_model_q9.fit(X_train, y_train) $ base_model_relation, base_accuracy_comparison = my_model_q9.base_model_eval()
meeting_status.head()
le_data = le_data_all.reset_index().pivot(index="country", $                                           columns="year") $ le_data.ix[:,0:3] $
model.save_weights('best.hdf5')
df.dtypes
df_h1b_mv = df_h1b_mv.drop([123031])
MICROSACC.plot_default(microsaccades,subtype="amplitude mean")
test_tfidf = pd.DataFrame(X_tfidf_test) $ test_tfidf['y'] = y_tfidf_test $ test_tfidf.columns = variable_name $ test_tfidf.head(10) # Test set with tf-idf
trainer = NaiveBayesClassifier.train $ classifier = sentim_analyzer.train(trainer, training_set)
tempX = pd.concat([X_trainfinal, X_testfinal]) $ tempy = pd.concat([y_train, y_test]) $ print tempX.shape, tempy.shape
sp['day_ago_high'] = sp.High.shift(periods = 1) $ sp['week_ago_high'] = sp.High.shift(periods = 7)
plot_results = actual_value_second_measure.join(predicted_probs_first_measure) $
requirements = {'Job_A': pd.Series(data = [2, 300000], index = ['FTE', 'Budget']), $                  'Job_B': pd.Series(data = [6, 2], index = ['FTE', 'Programmers'])}
print 'Create a DatetimeIndex even from gratuitously mixing date formats.' $ print 'An invalid type is convereted to NaT, which means Not A Time' $ dti = pd.to_datetime(['Aug 1, 2014', '2014-08-02', '2014.8.3', None]) $ dti
train.head()
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session, load_only $ from sqlalchemy import create_engine, func, inspect, and_
train.shape
result.loc[(result['timestamp'] >= 1.4e+13) & (result['timestamp'] <= 1.6e+15), 'timestampCorrected' ] = result['timestamp']/1000
for col in missing_info: $     num_missing = data[data[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing))
gs_pca_over.score(X_test_pca, y_test_over)
mydata.loc['1995-01-03'] # Specifies the date that you want
daily_cases.unstack().T.fillna(0).head()
page = BeautifulSoup(logged_out.content) $ with open('logged_out'+str(2)+'.html', 'w') as html_file: $     html_file.write(page.prettify('utf-8'))
type(twitter_archive_df_clean.timestamp[0])
arparams = np.array([.75, -.25]) $ maparams = np.array([.65, .35])
gatecount_station_line_gameless = gatecount_station_line[gatecount_station_line.service_day.isin(all_game_dates) == False]
len(df2['user_id'].unique())
roc_auc_score(predictions, fb_test.popular)
columns = ['BEML_weight','Glaxo_weight', 'Infy_weight','Unitech_weight','mean_return', 'std_returns'] $ portfolios_df = pd.DataFrame(data=results[0:,0:],columns=columns) $ portfolios_df
len(users) $ users.drop(users[users['friends_count'].str.contains(r'[A-Za-z]') == True].index, inplace=True) $ len(users)
autos[~autos["power_ps"].between(50,1000)]["power_ps"].value_counts()
AFX_open = [item[1] for item in AFX_X_2017['dataset_data']['data'] if item[1] != None] $ AFX_min = str(min(AFX_open)) $ print("In 2017, the lowest opening price was $" + AFX_min + '.') $ AFX_max = str(max(AFX_open)) $ print("In 2017, the highest opening price was $" + AFX_max + '.') $
gainax, phaseax = bode(omega, G) $ gainax.scatter(freqs, gains, color='red') $ phaseax.scatter(freqs, phases, color='red')
segments.info()
Pipeline = dfDay[['Date', 'Contract Value (Daily)', 'Weighted Value']] $ Pipeline = Pipeline.groupby(['Date'])[['Contract Value (Daily)', 'Weighted Value']].sum().reset_index()
pd.io.json.json_normalize(playlist['tracks']['items'][2])
information_ratio.loc[['Manager_A', 'Manager_B']] * 1.5
gas_df = gas_df.sort_index().loc['2016-01-01':'2017-12-31'] $ gas_df.head()
print("Number of Techniques in Enterprise ATT&CK") $ print(len(all_enterprise['techniques'])) $ df = all_enterprise['techniques'] $ df = json_normalize(df) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'data_sources'], axis=1)[0:5]
american_train,american_test = train_test_split(cat_american,0.9)
df = pd.read_csv('trump_lies.csv', parse_dates=['date'], encoding='utf-8') 
yhat_lr = lr.predict(X_test)
kickstarter.describe().drop(["ID", "pledged"], axis = 1)
accuracy = accuracy_score(y_test, y_pred) $ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
p_conv_control = df2.query('group == "control"')['converted'].mean() $ p_conv_control
pd.DataFrame(sanders).head()
print(model) $ print(model.wv.vocab)
a = WholeDf.groupby(by="Rating")
df2.query("group == 'treatment'")["converted"].mean()
x_test = np.array(data)
list(map(range, a.shape))
df2.dtypes $ df2['intercept']=1 $ df2[['dummy','ab_page']]=pd.get_dummies(df2['group']) $ df2 = df2.drop('dummy',axis=1) $ df2.head()
likes.groupby(['month']).size().plot(kind='bar')
X_kaggle = test.drop(drop_cols, axis=1, errors='ignore') $ if kaggle: id_test = test['id'].values
data = data[data.property_type != "store"]
data = data.drop(["price_max","quantity_max"],axis=1)
np.cov(Xs.T)
e_p_b_two.TimeCreate = e_p_b_two.TimeCreate.apply(lambda x:x.date())
colmns=['category', 'launched_year', 'launched_quarter', 'goal_cat_perc', 'participants'] $ ks_particpants.columns=colmns
api_df.head()
dfSF.tail()
df.dtypes
score = model.evaluate(x_test, y_test, $                        batch_size=batch_size, verbose=1) $ print('Test score:', score[0]) $ print('Test accuracy:', score[1])
Z = np.arange(11) $ Z[(3 < Z) & (Z <= 8)] *= -1 $ print(Z)
df_twitter_archive_master.tweet_id = df_twitter_archive_master.tweet_id.astype(str)
contribs.head()
table_rows = driver.find_elements_by_tag_name("tbody")[10].find_elements_by_tag_name("tr") $
season_groups.aggregate(np.sum).sort_values(by = "Tm.3PA", ascending = False)
top_brand_mileages = dict() $ for brand in top_brands: $     top_brand_mileages[brand] = autos.loc[autos['brand']==brand, 'odometer_km'].mean() $ top_brand_mileages
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = df2.query("group == 'control'")['converted'].count() $ n_new = df2.query("group == 'treatment'")['converted'].count()
def call(address, sig, args, result_types): $     data = _encode_function(sig, args) $     data_hex = data.encode('hex') $     response = eth_call(to_address=address, data=data_hex) $     return decode_abi(result_types, response[2:].decode('hex'))
dates = ["date_crawled","month_of_registration","year_of_registration","ad_created","last_seen"] $ autos[dates].info() $
len(df_measurement['station'].unique()) $
LARGE_GRID.display_fixation_centered(raw_large_grid_df)+xlim((-10,10))+ylim((-10,10))
reg_mod_ca = sm.OLS(df_all['converted'], df_all[['intercept', 'CA']]) $ analysis_ca = reg_mod_ca.fit() $ analysis_ca.summary()
tweets.head()
data = aapl.get_near_stock_price(expiry=aapl.expiry_dates[0:3]) $ data.iloc[0:5:, 0:5]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)
type(g.get_group('106.152.115.161'))
df3 = df3.join(pd.get_dummies(df3['country'])) $ df3.head()
df.groupby('key').sum()
autos['price']=autos['price'].str.replace('$','').str.replace(',','').astype(int) $
fps.working_dir
conn = sqlite3.connect('twitter_testing.sqlite') $ cur = conn.cursor() $
os.getcwd() $ os.chdir('../')
data_with_dates['post'] = 1 $ data_with_dates.head()
df = pd.read_csv('http://ix.cs.uoregon.edu/~lowd/aqi-lanecounty-2012-2017.csv') $ df.head()
aapl = pd.read_excel("../../data/stocks.xlsx", sheetname='aapl') $ aapl.head()
test_df[(test_df.labels>.4) & (test_df.labels<.5)].text.iloc[10]
pres_df['ad_length_tmp'] = pres_df['ad_length_tmp'] / np.timedelta64(1, 's') $ pres_df['ad_length_tmp'].head(10)
historicalPriceC.to_csv('C.csv') 
df.fillna({'text': 'NaN'}, inplace=True) $ df['health'] = df.text.str.contains('Health', case=False).astype(int)
data["Age (Years)"].mean()
print("LOCATION") $ DataSet["userLocation"].value_counts() $ print("TIMEZONE") $ DataSet["userTimezone"].value_counts()
payments_per_year_common_to_all_yrs = (df_sites_common_DRGs.groupby(['id_num','year'])[['disc_times_pay']].sum()) $ payments_per_year_common_to_all_yrs = payments_per_year_common_to_all_yrs.reset_index() $ payments_all_yrs = (df_providers.groupby(['id_num','year'])[['disc_times_pay']].sum()) $ payments_all_yrs = payments_all_yrs.reset_index()
model = sm.Logit(df3.converted, df3[['intercept','ab_page']]) $ result = model.fit()
movies.count()
user1 = df.query('group == "treatment" & landing_page == "old_page"').count $ print (user1) $ user2 = df.query('group == "control" & landing_page == "new_page"').count $ print (user2) $
r.summary2()
df.floofer.value_counts()
lyr_url = 'http://sampleserver3.arcgisonline.com/ArcGIS/rest/services/SanFrancisco/311Incidents/FeatureServer/0' $ layer = FeatureLayer(lyr_url) $ layer
!( \ $   echo DOI,PID && \ $   cat crossref-by-doi/*.json \ $     | jq -r '.message | select(has("alternative-id")) | "\(.DOI),\(.["alternative-id"][0])"' \ $ ) > crossref-pid-from-doi.csv
tweets_df.head()
clusters = list(km.labels_)
grouped_publications_by_author.columns = grouped_publications_by_author.columns.droplevel(0) $ grouped_publications_by_author.columns = ['authorId', 'authorName','publicationTitles','authorCollaboratorIds','authorCollaborators','countPublications','publicationKeys','publicationDates']
stores_tran_nulls['date'].unique()
df.isnull().values.any() $
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']#['link_weight'].loc[97528]
df2.info() $ df2['user_id'].unique() $ print('There are {} unique user ids'.format(len(df2['user_id'].unique())))
reddit.drop_duplicates(subset='title', keep='first', inplace=True)
the_frame["time"] = the_frame.set_index("local_15min").index.tz_localize(pytz.timezone('America/Chicago'), ambiguous = True) $ the_frame["date"] = [ dt.datetime(d.year,d.month,d.day,0,0,0,0) for d in the_frame['time'] ] $
cityID = 'e4a0d228eb6be76b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Philadelphia.append(tweet) 
df_predictions_clean['p1'] = df_predictions_clean['p1'].str.title() $ df_predictions_clean['p2'] = df_predictions_clean['p2'].str.title() $ df_predictions_clean['p3'] = df_predictions_clean['p3'].str.title() $
stocks.dtypes # Inspecting datatypes
csv_df[csv_df['timestamp'] == csv_df['timestamp'].min()]['url'].values[0]
Z = np.arange(9).reshape(3,3) $ for index, value in np.ndenumerate(Z): $     print(index, value) $ for index in np.ndindex(Z.shape): $     print(index, Z[index])
len(df2.query('group == "treatment" and converted == 1')) / len(df2.query('group == "treatment"'))
folium.Marker(location=[site_dct['Latitude'], site_dct['Longitude']]).add_to(smap)
new_page_converted = np.random.choice([1,0], size = n_new, p = [p_new,(1-p_new)])
horror_readings = horror_readings.groupby('date').agg('count') $ horror_readings
groups = sentiments.groupby([sentiments['created_at'].dt.date])
autos["odometer_km"].value_counts().sort_index(ascending=True).head(15)
w_change[coins_top10].sum().plot.bar() $ plt.title('Sum of weight changes') $ plt.show()
df_total[['CA','UK','US']] = pd.get_dummies(df_total.country)
1/np.exp(-0.0099), np.exp(-0.0506)
ufos_df = spark_df.toPandas()
merged_data.head()
x.loc[:,["B","A"]]
data = pd.read_csv('sample.csv')
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger')
glm_binom_feat_1 = H2OGeneralizedLinearEstimator(family='binomial', solver='L_BFGS', model_id='glm_v4') $ glm_binom_feat_1.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)
p_diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_old, 10000)/n_old $
ibm_hr_cat_dum = spark.createDataFrame(pd_cat) $ ibm_hr_cat_dum.show(3)
train = train.astype({'From':'category', 'To':'category', 'Title':'category',\ $                         'From-To':'category', 'Hour':'category', 'Age_bin':'category'}) $ test = test.astype({'From':'category', 'To':'category', 'Title':'category',\ $                        'From-To':'category', 'Hour':'category', 'Age_bin':'category'})
logit_control_2 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','UK_int_ab_page', 'US_int_ab_page']]) $ result_control_2 = logit_control_2.fit()
params = {'figure.figsize': [8,8],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(RN_PA_duration, params=params, freq=31, title='RN/PA Decomposition')
df_wm = pd.read_csv("walmart_all.csv", encoding="latin-1")
pd.read_csv(r'C:\Users\Patrik\Downloads\webrobots.iokickstarter-datasets\Kickstarter_2017-10-15T10_20_38_271Z\Kickstarter016.csv').info()
df["Source"].unique()
df_geo = pd.read_csv('https://www.aggdata.com/download_sample.php?file=nl_postal_codes.csv') $ df_geo.head()
for tract in sorted(tract_to_logrecno['ak'].keys()): $     print "%s %s" % (tract_to_logrecno['ak'][tract], logrecno_to_row[tract_to_logrecno['ak'][tract]])
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings_v1.csv', sep=',', encoding='utf-8', header=True)
id_list2 = df_clean3[df_clean3.tweet_id.isin(df_image_tweet.tweet_id)].tweet_id $ len(id_list)
df.head(6)
combined_df[combined_df['classifier_summary'].isnull() == True]
train['NTACode'] = train.NTACode.fillna('Non-NYC')
outfile_RSV = "Data_RSV_20180312.txt" $ outfile_full = "Data_All_20180312.txt" $ df_RSV.to_csv(outfile_RSV, sep="|", index=None) $ df_full.to_csv(outfile_full, sep="|", index=None)
print("Feature importances") $ for f, i in sorted(zip(features, bdt.feature_importances_), key=lambda x: x[1]): $     print(f.ljust(20), i)
h2o_age = h2o.H2OFrame(df_age)
new = df[df['landing_page']=='new_page']['user_id'].unique().shape[0] $ tot = df['user_id'].unique().shape[0] $ print(new/tot)
df_input = pd.read_csv('pydata_vw_tweets.csv') $ display(df_input.head()) $ display(df_input.tail()) $ display(df_input.describe())
display(HTML('<div style="border-style:solid; border-width: 1px; border-color: gray; padding: 10px 10px 10px;"> <b><u>Findings </u></b> </br> Comparatively there is no big gap between positive sentiments and negative sentiments, although nearly half of the comments are neutral. However, when looking at the negative percentage, it is not a good sign for company\'s performance as 1/5th of comments are negative. Further analysis on negative tweets are required in order to identify key issues why customers are unhappy. </div>'))
twitter_archive_df_clean['expanded_urls'].fillna("Unsure", inplace=True)
archive_clean.info()
asf_agg_by_gender_and_proj_df = non_blocking_df_save_or_load_csv( $     group_by_project_count_gender(cleaned_asf_people_human_df_saved).repartition(1), $     "{0}/asf_people_cleaned_agg_by_gender_and_proj_3c".format(fs_prefix))
def kDist(data, k): $     nData = data.shape[0] $     pDistMat = squareform(pdist(data)) $     kDistArray = [ sorted(pDistMat[i,:])[k-1] for i in range(nData)] $     return sorted(kDistArray, reverse=True)
print('Num entries where name is >= 2 and < 3:\t{}'.format( $         org_counts[(org_counts >= 2) & (org_counts < 3)].sum()))
injuries_hour.rename(columns={'NUMBER OF PERSONS INJURED':'injuries'},inplace=True)
mean = np.mean(data['len']) $ print("The average length in tweets: {}".format(mean))
MSE_train=mean_squared_error(y_train, y_pred_train) $ MSE_test=mean_squared_error(y_test, y_pred_test) $ print("Training Set MSE:               {:.2f}".format((MSE_train))) $ print("Test Set MSE:                   {:.2f}".format((MSE_test))) $
twitter_goodreads_users_df.isnull().sum()
url = "http://oglobo.globo.com" $ html_txt = urllib.request.urlopen(url).read() $ soup = bs(html_txt, "html.parser") $ [line.get('href') for line in soup.find_all('a')][0:10]    
es_rdd = es_rdd.map(lambda x: x[1])
from sklearn import metrics $ print("Train set Accuracy: ", metrics.accuracy_score(y_train_knn, neigh.predict(X_train_knn))) $ print("Test set Accuracy: ", metrics.accuracy_score(y_test_knn, yhat_knn))
all_df.shape
df1.head()
with graph.as_default(): $     train_epoch = 58 $     train_batch = 0 $     saver = tf.train.Saver(var_list=tf.global_variables()) $     saver.restore(sess,"models/{}/model_{}".format(model_name,1))
dfTemp.to_csv('../data/train_step1.csv')
rain_df = pd.DataFrame(rain_score) $ rain_df.set_index('date').head() $ rain_df.head()
df2['intercept'] = 1 $ df2[['ab_page', 'old_page']] = pd.get_dummies(df2['landing_page']) $ df2.head()
df.info()
automl.leader.predict(test_data=test)
data[(data > 0.3) & (data < 0.8)]
data.set_index('zipcode', inplace=True) $ data.to_json('heating.json')
plt.clf() $ fig = plt.figure(figsize =(15,10)) $ mean_dry_for_plots = dry_ndvi.mean(dim =('x','y')) $ plt.scatter(mean_dry_for_plots.time.data,mean_dry_for_plots.data) $ plt.show()
"University of Pretoria".split()
twitter_df_clean.groupby('source').source.count()
df2=df.copy() $ print(id(df),sep='\n') $ print(id(df2),sep='\n')
data.columns = ['West', 'East'] $ data['Total'] = data.eval('West + East')
print(autos['registration_year'].value_counts(normalize = True).sort_index(ascending = False)) $ print(autos['registration_year'].describe())
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_15min'], axis=1) $ dfY = data['corrCost']
autos['ad_created'] = autos['ad_created'].str[:10] $ (autos['ad_created'] $ .value_counts(normalize=True,dropna=False) $ .sort_index())
X_train_df = pd.DataFrame(X_train_matrix.todense(), $                          columns=tvec.get_feature_names(), $                          index=X_train.index)
data.head(1)
countries_df.info()
X = df[['word_count','sentiment','subjectivity','domain_d','post_duration','title','text']] $ le=LabelEncoder() $ y_cm = le.fit_transform(df['com_label'])#Comments $ y_sc = le.fit_transform(df['score_label'])# Score $
learner.get_layer_groups()
sf_small_grouped.reset_index(inplace=True)
df_enhanced['name'].describe()
from pyspark.ml.regression import RandomForestRegressor, \ $                                   GBTRegressor, \ $                                   GeneralizedLinearRegression $ from pyspark.ml.tuning import ParamGridBuilder, CrossValidator $ from pyspark.ml.evaluation import RegressionEvaluator
scipy.stats.kruskal(df2["tripduration"], df4["tripduration"])
os.chdir(root_dir + "data/") $ df_fda_drugs_reported = pd.read_csv("filtered_fda_drug_reports.csv", header=0)
pgh_311_data.info()
(df2.query('group == "treatment"')['converted'] == 1).mean()
disk_engine = create_engine('sqlite:///311_8M.db') # Initializes database with filename 311_8M.db in current directory
data = pd.Series(["quick", "brown", "fox"], name="Fox") $ data
extract_all.loc[(extract_all.application_month=='2018-04') $                 &(extract_all.app_branch=='NV0848') $                 &(extract_all.APP_PRODUCT_TYPE=='PL'),['APPLICATION_DATE_short','DEC_LOAN_AMOUNT1','DEC_FINAL_DECISION']]
pd.read_csv("Data/microbiome.csv", chunksize=15)
idxs = get_cv_idxs(n, val_pct=150000/n) $ joined_samp = joined.iloc[idxs].set_index("Date") $ samp_size = len(joined_samp); samp_size
import re $ letters_only = re.sub("[^a-zA-Z]",           # The pattern to search for $                       " ",                   # The pattern to replace it with $                       example1.get_text() )  # The text to search $ print(letters_only)
DT_feature_impt = pd.DataFrame({'features':features.columns, 'importance':model_dt.feature_importances_}).sort_values('importance', ascending=False) $ DT_feature_impt.head(20)
df_vow.describe()
OAUTH_SERVER_URL = 'https://accounts.team.blastmotion.com/oauth2/token'
trainheadlines = train["text"].values $ advancedvectorizer = CountVectorizer(ngram_range=(2,2)) $ advancedtrain = advancedvectorizer.fit_transform(trainheadlines)
full_orig[['PrevPrimaryDx','PrevDx2','PrevDx3']] = full_orig[full_orig['Readmitted'] == 1].groupby('Patient').transform('shift')[['PrimaryDx','Dx2','Dx3']].fillna('')
data.to_csv('prepared_data_with_cuts.csv', index = True) $
f_yt_resolved = '/scratch/olympus/projects/ideology_scaling/congress/youtube_links_resolved.tsv' $ df_yt_resolved = pd.read_csv(f_yt_resolved, sep='\t') $ f_yt_raw = '/scratch/olympus/projects/ideology_scaling/congress/youtube_links_raw.csv' $ df_yt = pd.read_csv(f_yt_raw)
df_site.head()
(combined[['location','country','date','zika_cases']] $  .groupby('country') $  .sum() $   .sort_values('zika_cases',ascending=False)) $
highRate.value_counts()
encoder_model_inference.predict(encoder_input_data[0:1]).shape
nitrodata = pd.merge(left=sites, $                      right=nitrogen, $                      how='right', $                      on='MonitoringLocationIdentifier') $ nitrodata.shape
microsoft = docx.Document('C:\\Users\\Andrew\\content-analysis-2018\\1-Intro\\2017_Annual_Report.docx')
yhat_prob = loanlr.predict_proba(X_test) $ yhat_prob
graph01 = df['complaint'].value_counts().head(3).sort_values(ascending=True).plot(kind='barh') $ print("Top 3 popular type of complaint\n") $ print(df['complaint'].value_counts().head(3).sort_values(ascending=False)) $ graph01
limit = 1 $ for i, tweet in enumerate(collect.get_iterator()): $     if i < limit: $         print(json.dumps(tweet, indent=4))
test.info()
from src.pipeline import pipeline_json $ pj = pipeline_json('../data/data.json')
grinter_day0.head()
extended_tweets.isnull().sum()
ether = df[df['tweet'].str.contains("Ethereum") | df['tweet'].str.contains("eth") | df['tweet'].str.contains("ETH")] $ ether = ether.reset_index(drop=True) $ ether.info()
s_mean_df = s_mean_df.set_index('date') $ print(len(s_mean_df.index)) $ s_mean_df.info() $ s_mean_df.head(5)
page = pb.Page(commons_site, 'File:Parlament Europeu.jpg') $ page.exists()
df.head()
from pyspark.sql.functions import log $ df_input_clean = df_input_clean.withColumn("log_Resp_time", log(df_input_clean.Resp_time))
df2.drop_duplicates(subset='user_id',inplace=True) $ len(df2)
X = pj.convert_to_df(scaling=True, filtered=True)
users_meta = df_meta.groupby('user_id').first().reset_index()
def split_vals(a,n): return a[:n].copy(), a[n:].copy()
details.head(5)
srcdf[srcdf.barename == 'acq1']   # barename 'acq1' appears four times
samples_query.execute_sql('SELECT ID, Name, DOB, SSN FROM Sample.Person')
interestlevel_df.to_csv('interest_level_df.csv',index=False)
data.head()
from sklearn import metrics $ print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train))) $ print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))
def plot_metrics(metrics): return metrics.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)
df['text_no_urls_names_nums'] = remove_by_regex(df_1, re.compile(r"\s?[0-9]+\.?[0-9]*"))['text']
dates= pd.date_range('20180831', periods=50, freq='M') $ dates
twitter_df_clean['ratio']=twitter_df_clean.rating_numerator/twitter_df_clean.rating_denominator
x=DataFrameSummary(joined).summary() $ x.loc['missing'][x.loc['missing']>0] $ print('\n') $ joined.filter(like='Comp', axis=1).min() $ joined.filter(like='Promo', axis=1).min()
df_293_table = pd.read_sql(sql_293_table,conn_laurel) $ df_293_table.sort_values(by='Values',ascending=False)
print("(rows, columns) for teams is = ", df.shape)
df_concat_2["time"].str.split(':') $ df_concat_2["time"] = df_concat_2["time"].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
data.whitelist_status.unique()
S.decision_obj.simulStart.value = "2006-07-01 00:00" $ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
help(np.linspace)
type(df2['account_created'][0].month)
vacation_data = session.query(Measurement.station, Measurement.date, Measurement.prcp, Measurement.tobs).\ $     filter(Measurement.date >= lst_year_arrive).\ $     filter(Measurement.date <= lst_year_leave).\ $     order_by(Measurement.station).all()
avg_vol = np.average(dataset["Traded Volume"]) $ print("Average Daily Treading Volume : " + str(avg_vol))
sample = pd.read_csv('data/sample_submission.csv')
regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]' $ def compress(word): $     pieces = re.findall(regexp, word) $     return ''.join(pieces) $ print(nltk.tokenwrap(compress(w) for w in tokens[10:85]))
df.iloc[1:5, 2:4]
cross_val_scores = cross_val_score(rf,x,y,cv=5) $ print "5-fold accuracies:\n",cross_val_scores $ print "Mean cv-accuracy:",np.mean(cross_val_scores) $ print "Std of cv-accuracy:",np.std(cross_val_scores)
fig = plt.figure(figsize=(15,8)) $ ax = fig.add_subplot(111) $ ax = resid_713.plot(ax=ax);
df['Temperature'].unique()
mean = np.mean(data['len']) $ print("The average length of tweets: {}".format(mean))
nuevo_df =  df.loc[:,['seller','offerType','vehicleType','yearOfRegistration','gearbox','powerPS', $                      'kilometer','monthOfRegistration','fuelType','notRepairedDamage','abtest']] $ nuevo_df = pd.get_dummies(nuevo_df, columns=['brand','seller','gearbox','fuelType','offerType','vehicleType','notRepairedDamage']) $ print "Nuevas dimensiones del dataset:" ,nuevo_df.shape $ nuevo_df.head()
data.to_csv('data/Building_Permits_proc_2.csv')
np.std(p_diffs)
if 1 == 1: $     news_titles_sr = pd.read_pickle(news_period_title_docs_pkl)
pp.pprint(emoji_lis(text)) $ line() $ print('Emojis: ', emoji_count(text))
fed_reg_dataframe[fed_reg_dataframe.index > '2017-01-20']
b = a.json()
v_train = nltk.classify.apply_features(extract_features,sentiment_train_tweets) $ v_train_full = nltk.classify.apply_features(extract_features,sentiment_train_tweets_full) $ v_validation = nltk.classify.apply_features(extract_features,sentiment_validation_tweets) $ v_test  = nltk.classify.apply_features(extract_features,sentiment_test_tweets)
plt.plot(x, y) $ X=np.linspace(1,len(x),len(x)) $ X = X.reshape((len(x),1)) $ X.shape
my_model.summary()
pnls = df.map( lambda r: {'date': r.date, $                           'neutral': r.neutral, $                           'var': float(var(r.scenarios.array, neutral_scenario=r.neutral))})
most_common_model_by_brand = {} $ for brand in brands: $     most_common_model_by_brand[brand] = autos.loc[autos["brand"] == brand, "model"].value_counts() $ most_common_model_by_brand
df = pd.DataFrame.from_records(sf_json)
df2['country'].unique()
df.head(2)
sum(contractor.address1.isnull())
df2['intercept']=1 $ df2[['control','treatment']]=pd.get_dummies(df2['group']) $ df2.head() $ df2.rename(columns={'treatment':'ab_page'},inplace=True)
df2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']) $ df2
stats_cols = ['backers', 'usd_pledged_real', 'usd_goal_real'] $ df[stats_cols].describe()
archive.floofer.value_counts()
cat 1st_flask_app_2/static/style.css
talks.reset_index(inplace = True, drop = True)
table1.describe()
from nltk.stem.porter import PorterStemmer $ stemmer = PorterStemmer() $ words_no_stop_stem = [stemmer.stem(w) for w in words_no_stop] $ print words_no_stop_stem
with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f: $     f.create_group(where='/', name='my_group')    
new_page_converted.converted.mean() - old_page_converted.converted.mean()
pd.concat([d1, d2], axis=1)
nullCity = youthUser4.loc[youthUser4["cityName"]=="UNKNOWN"] $ nullCity.head()
Table = [] $ for i in range(0, len(mars_facts)): $     temp=list(mars_facts[i].values()) $     Table.append(temp) $ print(Table)
owns = pd.read_pickle('data/pickled/new_subset_owns.pkl') $ stars = pd.read_pickle('data/pickled/new_subset_starred.pkl')
day_access = log_with_day.groupBy('dayOfWeek').count().sort('dayOfWeek', ascending=True) $ day_access.show()
(df16.count().mean() * 90) / (df1.count().mean() * 120)
plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue') $ plt.xlabel("Engine size") $ plt.ylabel("Emission") $ plt.show()
treatment_cr = df2.query('group == "treatment"').converted.mean() $ treatment_cr
ds = df.loc[df['item_id'] == 11] $ ai = df.loc[df['item_id'] == 23]
file_writer = tf.summary.FileWriter('./graph/bilstm_crf_elmo_bio_multi', tf.get_default_graph())
trump_tweets = clinton_tweets = get_tweets_with_cache("realDonaldTrump", "keys.json") $ trump_df = make_dataframe(trump_tweets)
out.to_csv('./data/train_for_map.csv', encoding='utf-8', sep=',')
countries = pd.read_csv('countries.csv') $ countries.head()
All_tweet_data_v2.timestamp= All_tweet_data_v2.timestamp.str.replace(' \+0000','') $ All_tweet_data_v2.timestamp.head() $ All_tweet_data_v2.info()
tweets.head(3)
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']]) $ results=mod.fit() $ results.summary()
aviation.sort_values('Deaths', ascending = False).head(1)['Date']
import scipy $ corr_coeff = scipy.stats.pearsonr(data_compare['SA_mix'], data_compare['SA_textblob_de']) $ print('Der Korrelationskoeffizient zwischen dem Mix und TextBlob ist:') $ print('----------------------------------------------------------------------------') $ print(corr_coeff[0]) $
temp_df['Order_Qty'].corr(temp_df['reorder_interval_group'])
djb_url = 'http://www.sosuave.net/forum/threads/most-commonly-asked-newbie-questions-answered-right-here.56520/' $ djb_content = requests.get(djb_url) $ djb_save = 'djb.html' $ with open(djb_save, mode='w', encoding='utf-8') as f: $     f.write(djb_content.text)
followup.columns
pd.DataFrame(records2.describe().loc['mean', ['Age', 'GPA', 'Days_missed']])
data.plot(kind='scatter', x='TV', y='sales') $ plt.plot(X_new, preds, c='red', linewidth=2)
hour_tweet_count=tizibika['hour'].value_counts() $ df_tweet_hour_count=pd.DataFrame(data=hour_tweet_count)
pickle.dump(tfidf_df,open('../proxy/dataset3_texttf','wb'))
from sklearn.linear_model import LogisticRegression $ from sklearn.feature_selection import SelectKBest, SelectFromModel
df['overall_rating'].corr(df['penalties'])
fwd.head(20)
Meter1.AutoRun(5, 'Sec')
df['dealowner'] = df['dealowner'].str.split(expand = True)[0]
df['tweet_ats'] = df.text.str.extract('(\s@\w*)')
grouped = word_freq_df.groupby('Word_stem', as_index=False).sum()
data.sample(5)
ts.resample('5Min', how='sum')
weekly_gtrends_data.head()
import plotly.plotly as py $ import plotly.graph_objs as go $ py.sign_in('adarsh108', 'zGjLFPqCsMfBaQdwZLnp')
print("Number of NAN values before: ",df_usa.isnull().sum())    #printing number of NaN values $ df_usa = df_usa.fillna(0) $ print("Number of NAN values after: ", df_usa.isnull().sum())    #printing the values after the NaN deletion
df_link_meta[df_link_meta['link.domain'] == 'bit.ly']['link.domain_resolved'].value_counts().head(25)
df_sched[df_sched.Initiation < 0 ]
RNPA_existing_8_to_16wk_arima = RNPA_existing_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']] $ RNPA_existing_8_to_16wk_arima.index = RNPA_existing_8_to_16wk_arima.index.date
cfg_fnames = list(glob.glob('test-data/output/sample-xls-case-badlayout1.xlsx-*.csv')) $ len(cfg_fnames) $ c = d6tstack.combine_csv.CombinerCSV(cfg_fnames, all_strings=True) $ c.combine().head() $
train_df.info()
inputFile = os.path.join(edfDir, "pt1sz2_eeg.csv") $ outputFile = os.path.join(edfDir, "chanheadersOut.json") $ outputFile = os.path.join(edfDir, "pt1sz2_eeg.json") $ outputFile = os.path.join(edfDir, "pt1sz2_eeg.npy")
consumerKey= 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX' $ consumerSecret='XXXXXXXXXXXXXXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, $                            consumer_secret=consumerSecret) $ api=tweepy.API(auth)
df.head(1)
dr_2018 = dr_2018.resample('W-MON').sum() $ RNPA_2018 = RNPA_2018.resample('W-MON').sum() $ ther_2018 = ther_2018.resample('W-MON').sum()
tweet_archive_clean['new'].unique()
def clean_hex(d): $     return hex(d).rstrip('L')
fps = DirFileMgr(dr_id_str) $ fps.create_all_dr_fps(new_setup='N') $ fps.create_all_modeling_fps(mod_id_str) $ fps.add_fp('pyLDAvis')
np.multiply(validation_labels[:1] , 100) + val_prediction[:1]
t = len(np.concatenate(trn_lm)) $ t, t // 64
df_timeseries.set_index('date', inplace=True)
d_dates.head()
train. = 'NAN' if train.ORIGINE_INCIDENT not in 
sns.distplot(posts[(posts['ViewCount'].notnull() & $                     (posts['ViewCount'] < 4000))]['ViewCount'], kde = False, bins = 30) $ plt.xlim((0,4000))
df_cont = pd.read_csv('contact.csv', index_col=None)
tmp = test_df.join(tt) $
print soup.prettify()[0:500]
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, (1-p_old)]) $ old_page_converted
repeat_user = df2.user_id.value_counts()[0:1] $ print('The repeat user is {}'.format(repeat_user))
groups_df.dropna(subset=[ 'MSA_CODE' ], inplace=True) $ metro_groups_df = groups_df[ groups_df['MSA_NAME'].str.contains('Metro') ] $ groups_by_metro = metro_groups_df.groupby([ 'MSA_CODE', 'MSA_NAME' ], as_index=False)
data.info()
autos = autos[autos["price"].between(1, 350000)]
treat_old = df.query("group == 'treatment' and landing_page == 'old_page'") $ ctrl_new = df.query("group == 'control' and landing_page == 'new_page'") $ treat_old['user_id'].count()+ctrl_new['user_id'].count()
rate_pold_null = sum(df2['converted'])/df2.shape[0] $ rate_pold_null
df2['DepDelay'].describe()
aapl['open'].plot(color='b', style='.-', legend=True) $ aapl['close'].plot(color='r', style='.', legend=True) $ plt.axis(('2001','2002',0, 100)) $ plt.show()
DT_yhat = DT.predict(test_X) $ print("DT Jaccard index: %.2f" % jaccard_similarity_score(test_y, DT_yhat)) $ print("DT F1-score: %.2f" % f1_score(test_y, DT_yhat, average='weighted') )
rr = pd.concat([fundret, r[coins_top10]], axis=1)
samples_query.count_records()
r.xs('AMD', level=1)[['share']].plot()
tweets_rt.head()
df.dropna()
blink= condition_df.get_condition_df(data=(etsamples,etmsgs,etevents),condition="BLINK")
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
sandag_df_w_pred = sandag_df.reset_index() $ sandag_df_w_pred['homevals_pred'] = pd.Series(y_2_pred) $ sandag_df_w_pred = (sandag_df_w_pred. $                     set_index(['year', 'tract']).sort_index()) $ sandag_df_w_pred.head()
criteria = s > 10 $ criteria.head()
users.country_destination.value_counts()
text.upper
data3 = pd.concat(dfs, ignore_index=True) $ data3.shape
y_train = np.where(y_train == 'Charged Off', 1, 0)
autos["price"].unique().shape
events_pd = events.toPandas() $ events_pd['event_type'].value_counts().plot.bar()
archive_clean.drop(['Unnamed: 0'], axis=1, inplace = True) $ images_clean.drop(['Unnamed: 0'], axis=1, inplace = True) $ popularity_clean.drop(['Unnamed: 0'], axis=1, inplace = True)
own_star.drop(['created_at_own'], axis=1, inplace=True)
plan_repaid.columns
print('Preview dataset info') $ print('Unique questions:', df['question_id'].nunique()) $ print('Unique users:', df['user_id'].nunique()) $ print('Total predictions:', df.shape[0])
DBPATH = 'results.db' $ conn = sqlite3.connect(DBPATH) $ cur = conn.cursor()
injury_df['Date'] = injury_df['Date'].astype(int) $ injury_df.info()
dt_features_test['deadline'] = pd.to_datetime(dt_features_test['deadline'],unit='s')
df['cowbell'] = df['body'].apply(lambda x: len([x for x in x.split() if '(((' in x]))
posts.groupby(['from', 'hostname_clean'])['post'].aggregate(sum)
salesdec.head()
autos["brand"].unique()
db_path = os.path.join(os.getcwd(), 'crawler_app', 'crawler_app', 'db', 'crawler.sqlite') $
from IPython.core.display import HTML $ css_file = '../style/style.css' $ HTML(open(css_file, "r").read())
data = {'Bob': pd.Series([245, 25, 55]), $          'Alice': pd.Series([40, 110, 500, 45])} $ df = pd.DataFrame(data) $ df
_ = ok.grade('q05c') $ _ = ok.backup()
tweet_hour.loc[1340,'tweet_text'].split()
mention_df['tweet_id'].unique().size
import requests $ import json $ url = "https://www.dcard.tw/_api/posts?popular=false&before=228084271" $ resp = requests.get(url, headers=headers)
df_clean2['Year']  = temp.year $ df_clean2['Month'] = temp.month $ df_clean2['Date']  = temp.date
df_archive_clean.source.value_counts()
x.to_csv('data_cleaned')
df.state_retail.mean()
print train_data.shape, validation_data.shape, test_data.shape
centre_lat = df.iat[0, 1] #df.iat[row, col] $ centre_lon = df.iat[0, 2]
stories['created_hour'] = stories.created_at.map(lambda x: x.hour)
import pandas as pd $ import numpy as np $ from io import StringIO $ import re $ from IPython.display import display,clear_output
df.index.hour.value_counts().head()
autos.head(10)
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     print(repr(f.root.multidimensional_data.array_3d[2:7, 1, :]))
popularity.info()
f = '/scratch/olympus/womens_march_2017/data/womens_march_2017_data__01_18_2018__00_00_00__23_59_59.json.bz2' $ collect = JsonCollection(f, compression='bz2', throw_error=0)
filter_df = all_df.copy() $ filter_df.head(2)
df['id'] = df['id'].astype('category') $ df['sentiment'] = df['sentiment'].astype('category') $ df['text'] = df['text'].astype('string')
own_star['uniqueID'] = own_star.repo_id.astype(str) + own_star.user_id.astype(str)
df5 = df2.query("group == 'treatment'") $ df6 = df5['converted'].mean()
p_diffs = [] $ pn = np.random.binomial(n=n_new, p=p_null, size=10000) / n_new $ po = np.random.binomial(n=n_old, p=p_null, size=10000) / n_old $ p_diffs.append(pn - po)
predict.predict_score('Wikipedia')
print(autos.columns)
a + 1.2
b = pd.DataFrame(np.random.random((6, 3)), columns=['one', 'two', 'three']) $ b.iloc[2, 2] = np.nan $ b
filter_df = all_df.copy() $ filter_df.head(2)
old_page_converted = np.random.binomial(n=n_old, p=p_null) / n_old $ old_page_converted
dule2 = dule[~dule['ElectronicCollection'].isin(dul_ecolls_to_exclude['ecoll'])] $ dule2
train_downsampled = training.sampleBy('label', fractions={0.0: 0.135, 1.0: 1.0}, seed=123).cache() $ train_downsampled.groupby('label').count().show() $ testing.groupby('label').count().show() $
hs.setAccessRules(C_resource_id, public=True)
df2.to_csv('caderma_clean.csv')
tweets_df = pd.DataFrame(status)
negative.head()
dftemp1 = dftemp[dftemp['Variable Name'].isin(['National Rainfall Index (NRI)']).where(dftemp['Value']> 950, dftemp['Value']<900)] $ dftemp2 = dftemp1[-6:] $ dftemp2['Year']     #printing the years for the given condition
reddit_comments_data.select(min("score")).show(truncate=False)
classes.fillna(0,inplace = True)
mis_match = [] $ for i in range(len(image_predictions)): $     if (image_predictions['tweet_id'][i]) not in list(twitter_archive['tweet_id']): $         mis_match.append(image_predictions['tweet_id'][i]) $ print(len(mis_match))
conn = psycopg2.connect("host=108.45.74.15 port=5432 dbname=emoji_db user=insight password=") $ cur = conn.cursor() $
driver = webdriver.Chrome(executable_path='./chromedriver') $ driver.get(URL) $ html = driver.page_source $ soup = BeautifulSoup(html, 'lxml') $ html[:2500]
df2.query("group == 'treatment'").converted.mean()
ts = pd.Series(np.random.randn(9) * 10 + 500, index=timestamps)
p_diffs=[] $ for i in range(10000): $     new=np.random.choice([0,1],n_new,p=[1-p_new,p_new]) $     old=np.random.choice([0,1],n_old,p=[1-p_old,p_old]) $     p_diffs.append(new.sum()/len(new)-old.sum()/len(old)) $
data_scaled.head()
dr_new_data_plus_forecast.to_csv('./data/dr_new_patients_arimax_forecast.csv') $ dr_existing_data_plus_forecast.to_csv('./data/dr_existing_patients_arimax_forecast.csv')
(autos["registration_year"] $  .value_counts(normalize=True) $  .sort_index(ascending=False).head(10))
building_pa_prc_fix_issued=pd.read_csv('buildding_03.csv',parse_dates=['permit_creation_date','issued_date'])
path_to_array1 = '{f1path}:/array1'.format(f1path=os.path.abspath(os.path.join(data_dir, 'file1.h5'))) $ path_to_array2 = '{f2path}:/array2'.format(f2path=os.path.abspath(os.path.join(data_dir, 'file2.h5'))) $ print(path_to_array1) $ print(path_to_array2)
iv = options_frame[options_frame['Expiration'] == '2016-03-18'] $ iv_call = iv[iv['OptionType'] == 'call'] $ iv_call[['Strike', 'ImpliedVolatilityMid']].set_index('Strike').plot(title='Implied volatility skew')
naimp.nacolcount()
df.to_csv('trump_lies.csv', index=False, encoding='utf-8')
newgeo.sort_index(inplace=True)
import statsmodels.api as sm $ convert_old = df2[(df2['group'] == 'control') & (df2['converted'] == 1)]['user_id'].count() $ convert_new = df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)]['user_id'].count() $
txt_count = sc.accumulator(0)
import sklearn.neural_network $ ann = sklearn.neural_network.MLPRegressor() $ ann.fit(train[features], train[target].values.ravel()) $ p = ann.predict(test[features]) $ print("Loss", np.mean((p - test[target])**2) / baseline_loss)
dates = pd.date_range('2010-01-01', '2010-12-31') $ df1=pd.DataFrame(index=dates) $ print(df1.head())
p_diffs = np.random.binomial(n_new, p_new, 10000)/n_new - np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs
result = clf_LR_tfidf.predict(test_data_features_tfidf) $ result_prob = clf_LR_tfidf.predict_proba(test_data_features_tfidf) $ output = pd.DataFrame(data={"id":test["id"], "sentiment":result,})# "probs":result_prob[:,1]}) $ output.to_csv(os.path.join(outputs,'LR_tfidf_model.csv'), index=False, quoting=3)
tweet_df.rename(columns={'id':'tweet_id'}, inplace=True) $ final_df = pd.merge(df, img_df, how = 'left', on = ['tweet_id'] ) $ final_df = pd.merge(final_df, tweet_df, how = 'left', on = ['tweet_id']) $ final_df.info() $ fin_df=final_df.copy()
iris_data_path = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/iris/iris.csv" # load demonstration data
sales = graphlab.SFrame('Data/kc_house_data.gl/') $ sales.head()
old_page_converted = np.random.choice([0,1], size=control_group.shape[0], p=[1-p_old, p_old]) $ print(old_page_converted.mean())
df_clean = df_clean[df_clean['retweeted_status_id'].isnull()] $ df_clean = df_clean[df_clean['in_reply_to_status_id'].isnull()]
tweets_kyoto = tweets_kyoto[np.isfinite(tweets_kyoto['ex_lat'])]
hand = pd.get_dummies(auto_new.Hand_Drive) $ hand.head()
apple.set_index('Date', drop=True, inplace=True) $ apple.head()
header = {'Authorization': 'Bearer {0}'.format(access_token)}
SANDAG_ethnicity_df = pd.read_csv(open(SANDAG_ethnicity_filepath), dtype={'TRACT': str}) $ SANDAG_ethnicity_df.set_index(['YEAR', 'TRACT'], inplace=True) $ SANDAG_ethnicity_df.sort_index(inplace=True) $ SANDAG_ethnicity_df.head()
df2[df2['landing_page'] == 'new_page'].shape[0]/df2.shape[0]
tmp_2 = linear_model.BayesianRidge() $ tmp_2.fit(X_training_2,y_training) $ y_pred_health = tmp_2.predict(X_final_test_2) $ r2_score(y_final_test, y_pred_health)
joined=join_df(train,store,"Store") $ joined_test=join_df(test,store,"Store") $ sum(joined['State'].isnull()),sum(joined_test['State'].isnull())
L = [0, 1, 0, 1, 2, 0] $ display('df', 'df.groupby(L).sum()')
calls_nocontact_2017.to_csv("311_calls_new.csv")
df.tail(3)
df['Descriptor'][df['Complaint Type'] == 'Street Condition'].value_counts().head()
prediction_and_counts=image_predictions_clean.merge(tweet_scores_clean, how='left',on='tweet_id')
type(data.value)
cig_data['tar'].value_counts()
df2.i
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger') $ z_score, p_value $
def read_data(filename): $     data = pd.read_csv(filename, sep='\t') $     data['tags'] = data['tags'].apply(literal_eval) $     return data
p_diff = new_page_converted.mean()-old_page_converted.mean() $ print(p_diff)
Adsdata = load_AdsData ('../data/SalesDataA.xlsm', sheetname = 'Ads') $ Adsdata.head()
Base.prepare(engine, reflect=True) $
df = pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv ') $ df.head(2)
data_sets['60min'].head()
gs.best_params_
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
subs_and_comments.shape
X_train.head(3)
print(tweet1.text)
treatment_convert = treatment[treatment['converted']==1].user_id.nunique()/treatment['user_id'].nunique() $ treatment_convert
permits_df['CENSUS_TRACT'] = pd.Series.from_csv('building_permits_by_census_tract.csv') if os.path.exists('building_permits_by_census_tract.csv') else get_census_tracts_series(permits_df['LAT_LONG_COORDS'], 'building_permits_by_census_tract.csv') $ permits_df.head()
print(real_diff) $ plt.hist(p_diffs) $ plt.axvline(x = real_diff,color = 'red');
my_string = str('my text') $ my_string.__str__()
pd.DataFrame(mnnb.feature_log_prob_, columns = cvec.get_feature_names())
stocks.head()
df.sum().plot() $
query = spark_hive.sql('create table reddit_comments_data_hive as select * from tmp_reddit_df') $ query.show()
new_page_converted = np.random.choice(2, size=n_new ,p=[p_new,1 - p_new]) $ new_page_converted.mean()
treatment_group = df2.query('group == "treatment"') $ pconversion_treatment = treatment_group['converted'].mean() $ pconversion_treatment
df_vow['Low'].unique()
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.hist(data['comments'], color='orange');
overal_topic(date_df=df[df.placeId == place], total_base_dict=total_base_dict_by_place.loc[place]['dict'], $              total_N=total_base_dict_by_place.loc[place]['base_twitter_count'])
timeindex = pd.Series(['1/1/2018', '1/4/2018', '1/5/2018', '1/7/2018', '1/8/2018']) $ timeindex = pd.to_datetime(timeindex) $ data_to_interp = [1, np.nan, 5, np.nan, 8] $ df_to_interp = pd.DataFrame(data_to_interp, index=timeindex) $ df_to_interp
parse_dict['creator'].head(5) $
horror_readings['date'] = horror_readings.tracking_time.dt.date
print(pd.merge(df2,df3))
prods_user =pd.merge(priors_product_purchase_spec,priors_product_reordered_spec, on="userprod_id",how="outer") $ prods_user['reorder_ratio']=prods_user['reordered_count_spec']/prods_user['purchase_count_spec'] $ prods_user.head()
scaler = MinMaxScaler(feature_range=(0,1)) $ cols_to_scale = [col for col in df_total.columns if 'weekday' not in col and 'month' not in col] $ scaled_cols = scaler.fit_transform(df_total[cols_to_scale]) $ df_total[cols_to_scale] = scaled_cols
symbol='IBB' $ benchmark2 = web.DataReader(symbol, 'yahoo' , start_date ,end_date)
url = 'https://www.cgv.id/en/schedule/cinema/041/2018-02-22' $ request = urllib.request.Request(url) $ page = urllib.request.urlopen(request) $ soup2 = BeautifulSoup(page,"lxml") $ soup2.find_all('div', class_='schedule-container')[0].select('.disabled')
print(data.head()) $ print(data.tail(3)) $ print(data.shape) $
X = users_usage_summaries.iloc[:, users_usage_summaries.columns!='churned'].values $ y = users_usage_summaries.loc[:,'churned'].values $ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.05)
X= preprocessing.StandardScaler().fit(X).transform(X) $ X[0:5]
p.asfreq('M', how='end')
segmentData = pd.read_excel('2017 08 take home data.xlsx')
old_page_converted = np.random.choice([0,1], n_old, p=[1-p_old, p_old]) $ old_page_converted
m_plus = 0.9 $ m_minus = 0.1 $ lambda_ = 0.5
df_first_published_at.first_published_at = df_first_published_at.first_published_at.replace({'1970-01-01 00:00:00.000009999': 'NaN'})
df4.sort_values(by='BG')
train.age.describe()
dr_2018.head()
my_query = "SELECT * FROM well_known_files LIMIT 10" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element.keys()
clean_tokens = [] $ for t in tokens: $     if t not in sr: $         clean_tokens.append(t) $ clean_tokens[:10]
df2.user_id.duplicated().sum()
rolled_returns['2017-06-07':].plot()
df_2 = pd.read_csv('bitcoin_data_one_minute_2017_2018.csv', parse_dates = ['timestamp']).set_index('timestamp')
1/np.exp(results_4.params)
ifs = pd.read_csv('journalimpactfactors2017.csv') $ ifs['Journal'] = ifs['Journal'].str.lower() $ ifs = ifs[ifs['Journal Impact Factor']!='Not Available']
companies = pd.read_csv("companies.csv") $ companies[companies['trc_industry_group'] == "Investment Banking & Investment Services"]
(pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days
list(user_politics.values())[0]
new_pages = df2.query('landing_page == "new_page"').user_id.count() $ new_pages/tc
d = {'Flavor': ['Strawberry', 'Vanilla', 'Chocolate'], 'Price': [3.50, 3.00, 4.25]} $ icecream = pd.DataFrame(data=d) $ icecream
twtter_count1.columns = ['base_twitter_count'] $ twtter_count2.columns = ['base_twitter_count'] $ twtter_count3.columns = ['base_twitter_count']
df2['intercept'] = 1 $ df2[['control','treatment']]=pd.get_dummies(df2['group']) $
import matplotlib.pyplot as plt
%%time $ rf_v1.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)
converted_first_week = first_week_day_num.asfreq("B") # class only happens on business days $ converted_first_week
!rm SIGHTINGS.csv -f $ !wget https://www.quandl.com/api/v3/datasets/WIKI/IBM.csv
group.shift(1)[['inspection_date', 'results']]
hashed_train_sample = hashingTF.transform(parsed_train_sample).select('id', 'label', 'features').cache() $ hashed_train1 = hashingTF.transform(parsed_train1).select('id', 'label', 'features').cache() $ hashed_dev1 = hashingTF.transform(parsed_dev1).select('id', 'label', 'features').cache() $ hashed_modeling2 = hashingTF.transform(parsed_modeling2).select('id', 'label', 'features').cache() $ hashed_test = hashingTF.transform(parsed_test).select('id', 'label', 'features').cache()
df_ad_airings_5['location'].value_counts()
dr_new_patient_data_plus_forecast.columns
data = item.data $ data
es = es.entity_from_dataframe(entity_id = 'clients', dataframe = clients, $                               index = 'client_id', time_index = 'joined')
import statsmodels.api as sm $ convert_old = df2[df2['group'] == 'control']['converted'].sum() $ convert_new = df2[df2['group'] == 'treatment']['converted'].sum() $ n_old = df2[df2['group'] == 'control'].shape[0] $ n_new = df2[df2['group'] == 'treatment'].shape[0]
logins['datetime'] = pd.to_datetime(logins.login_time)
trip_data_q5["hour"] = trip_data_q5.lpep_pickup_datetime.apply(lambda x: pd.to_datetime(x).hour) $ trip_data_q5["hour"] = trip_data_q5["hour"].astype('category') $ mean_hour = trip_data_q5.groupby("hour").agg(["mean"])
snap_df = collection.item('AAPL', snapshot='snapshot_name') $ snap_df.to_pandas().tail()
c.execute(query) $ results = c.fetchall() $ print "Accommodations in Leiden database:", results[0][0]
factors = web.DataReader("Global_Factors", "famafrench") $ factors
ser4[np.isnan(ser4)] #numpy has isnan function to identify Not a Number values
frame.loc[('b', 2), 'Colorado']
autos.info()
no_outliers.to_csv('data/processed.csv', index=False)
c = sqlc.cursor()
daily_returns['SPY'].hist(bins=20,label='SPY') $ daily_returns['XOM'].hist(bins=20,label='XOM') $ plt.legend(loc='upper right') $ plt.show()
tweet_data.head(3)
pm.forestplot(trace)
data.retained.value_counts()
sl_columns=["Date", "Country", 'new_suspected', 'new_probable', 'new_confirmed', 'death_suspected', 'death_probable' , $             'death_confirmed'] $ sl_df=pd.DataFrame(columns=sl_columns)
df.head()
def create_emb_cols(data, cats): $     for c in cats: $         cat2emb = {v:k for k, v in enumerate(data[c].unique())} $         data[c] = data[c].map(cat2emb) $     return data
tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True) $ raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1] $ raw_tfidf 
maint_spark = spark.createDataFrame(maint, $                               verifySchema=False) $ del maint $ maint_spark.printSchema()
smpl_join = smpl.join(contest_cr, F.col('end_customer_party_ssot_party_id_int_sav_party_id')==F.col('party_id'))#.select('party_id','decision_date_time','party_name','postal_code','address1','street_name','sales_acct_id').cache()
freqword = defaultdict(list) $ for word, freq in Counter(wordlist).items(): $     freqword[freq].append(word) $ for freq in sorted(freqword): $     print('count {}: {}'.format(freq, sorted(freqword[freq])))
from sklearn.neighbors import KNeighborsClassifier $ from sklearn.model_selection import GridSearchCV
df_goog.resample('Q').mean()
def create_df_grouped_by_date( tweets_df ): $     return tweets_df.groupby( Grouper( 'date' )).mean()
full_monte['info'].str.get_dummies('|')
feature_imp_RF=pd.DataFrame(list(zip(features,trained_model_RF.feature_importances_))) $ column_names_RF= ['features','RF_imp'] $ feature_imp_RF.columns= column_names_RF
df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E']) $ df1.iloc[0:2,6] = 1. $ df1
giss_temp = giss_temp.drop("Year.1", axis=1) $ giss_temp
x_test = x_test.reshape((len(x_test),1)) $ print(x_test.shape, y_test.shape)
td = td.fillna(0)
corrplot(rr.loc[start_date:end_date].corr(), annot=True) $ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date) $ plt.show()
city_fare=table_grouped_type["fare"].sum() $ city_fare
dtObj = binary_sensors_df['last_changed'].iloc[0] $ device_id = 'device_tracker.robins_iphone' $ print("At {} {} is {}".format(dtObj, device_id, get_device_state(parsedDF, device_id, dtObj)))
call_hist.iloc[-1]
pnew = df2['converted'].mean() $ pnew
results = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >='2016-09-01').\ $ filter(Measurement.prcp != None).\ $ order_by(Measurement.date.desc()).all() $ results $
df_licenses.head(2)
df_playlist_videos = pd.DataFrame() $ for playlist in tqdm(df_playlists['playlist_id'].tolist()): $     playlist = yt.get_video_urls_from_playlist_id(playlist, key, verbose=0) $     df_ = pd.DataFrame(playlist) $     df_playlist_videos = df_playlist_videos.append(df_, ignore_index=False)
freq_titles = final_data.groupby(['clean_titles']).size().reset_index(name='counts').sort_values('counts', ascending=False).head(200) $ freq_titles
submission = reddit.submission(url='https://www.reddit.com/r/CryptoCurrency/comments/7upe01/daily_general_discussion_february_2_2018/')
df_pd.sort_values(by='timestamp') $ train_frame = df_pd[0 : int(0.7*len(df_pd))] $ test_frame = df_pd[int(0.7*len(df_pd)) : ] $
autos["price"].value_counts().sort_index(ascending=False)
plt.figure() $ plt.hist(word_count, bins = 100)
images_clean.set_index(images_clean.columns[0], inplace = True) $ images_clean.head()
new_converted = np.random.choice([1, 0], size=len(df_new), p=[P_mean, (1-P_mean)]) $ new_converted.mean()
df= pd.read_csv('../PANDAS/data/DataForTutorial3.csv', parse_dates=['Date'], usecols=list(range(1,8)))
file = 'data/pickled/Emoticon_NB4/full_emoji_dict.obj' $ emoji_dict = gu.read_pickle_obj(file)
n_old = df2[df2['group']=='control']['user_id'].count() $ n_old
df_nodates = df_EMR_with_dummies.select_dtypes(exclude=['<M8[ns]']) $ df_EMR_dd_dummies = pd.concat([df_dd, df_nodates], axis=1) $ df_EMR_dd_dummies.shape $ df_EMR_dd_dummies.head()
bnb.shape
(autos["date_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
trace = go.Scatter(x=df['Date'], y=df['Likes'])
success_classifier = DecisionTreeClassifier() $ success_classifier.fit(X_train, y_train) $ predictions = success_classifier.predict(X_test) $ accuracy_score(y_true = y_test, y_pred = predictions)
iris.loc[::30,"Species"]
if not os.path.exists(ml_1m_path): $     os.makedirs(ml_1m_path)
msft.dtypes
origin=table.find(text='Flight origin').find_next('td').text $ origin
grid_search.best_estimator_
view = events.pivot_table(index=["distance", "swimstyle", "category"], columns='course', values='time').reset_index() $ view.sample(5)
df.head()
df = pd.read_csv("https://raw.githubusercontent.com/YingZhang1028/practicum/master/Data_DataMining/train_score.csv") $ train_df["description_score"] = df["description_score"] $ train_df["description_score"].ix[np.isnan(train_df["description_score"]) == True] = train_df["description_score"].mean()
oppstage.loc[1].plot(kind='bar', stacked=True, figsize=(12,6));
filename = 'nba_pred_modelv1.sav' $ pickle.dump(clfgtb, open(filename, 'wb'))
df2.head() $
plt.savefig('News Mood Analysis_Channel.png')
submit.to_csv('log_reg_baseline.csv', index = False)
c1 = covtype_df[covtype_df['Cover_Type'] == 'class_1'] $ c2 = covtype_df[covtype_df['Cover_Type'] == 'class_2'] $ df_b = c1.rbind(c2)
mean_year_first = df.groupby(df.date.dt.year)['first_day_pctchg'].mean()
df.plot.hist(bins=12, color='#99cc00') $ plt.tight_layout() $ plt.show() $ plt.savefig('station_temp.png')
irl.drop(['bill', 'congress'], axis=1, inplace=True) $ irl.head()
import matplotlib.pyplot as plt $ plt.plot(close_arr) $ plt.show()
spark_df = hc.createDataFrame(table) $ spark_df.write.saveAsTable('tweet_table')
analyze_set[analyze_set['tweet_id']==666102155909144576].jpg_url
plt.hist(p_diffs);
dsg.vatts('station')
pipeline2 = Pipeline([ $     ('lr', LogisticRegression())]) $ pipeline2.fit(X_trainfinal, y_train) $ pipeline2.score(X_testfinal, y_test)
users = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv') $ sessions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv') $ products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv') $ transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv')
offseason07["InorOff"] = "Offseason" # This assigns an identifier as to whether this is inseason or offseason.
r = requests.get(url_api) $ print(r.status_code)
data_df.tail()
twitter_data = pd.DataFrame(list(twitter_coll_reference.find())) $ twitter_data.head()
total_base_dict_by_place = base_dict_by_place.copy()
number_rows = len(df) $ print('The number of rows in the dataset is {}'.format(number_rows))
del Train_extra['Source_Category'], Test_extra['Source_Category'] $ Train_extra.index = Train_extra.ID $ Test_extra.index = Test_extra.ID $ del Train_extra['ID'], Test_extra['ID'] $ Train_extra.head()
actual_diff = df2_treatment['converted'].mean() - df2_control['converted'].mean() $ p_diffs = np.array(p_diffs) $ (p_diffs > actual_diff).mean()
autos.price.unique()
football=pd.read_csv("results.csv") $ football.head()
minute_return.head()
repos[repos.name.isnull()]
train.shape
f_close_clicks_app_train = spark.read.csv(os.path.join(mungepath, "f_close_clicks_app_train"), header=True) $ f_close_clicks_app_test = spark.read.csv(os.path.join(mungepath, "f_close_clicks_app_test"), header=True) $ print('Found %d observations in train.' %f_close_clicks_app_train.count()) $ print('Found %d observations in test.' %f_close_clicks_app_test.count())
len_bins = pd.groupby(df['max_len'], by=[df.index.year, df.index.month]) $ print(len(len_bins)) $
pt_after.sort_values(by=['uid'])
consumer_key = os.environ["consumer_key"] $ consumer_secret = os.environ["consumer_secret"] $ access_token = os.environ["access_token"] $ access_token_secret = os.environ["access_token_secret"]
classification_df.corr()['best'].sort_values()
tweets_original.info()
def join_df(left, right, left_on, right_on=None, suffix='_y'): $     if right_on is None: right_on = left_on $     return left.merge( $         right, how='left', left_on=left_on, right_on=right_on, $         suffixes=('', suffix))
x_train.head()
rf_classifier = RandomForestClassifier(n_estimators=250, criterion='gini', $                                        class_weight='balanced_subsample', bootstrap=True, oob_score=True) $ rf_classifier.fit(x_train, y_train)
df.shape $
title = soup.find_all('div', class_="content_title")[1].text.strip() $ description=soup.find_all('div', class_="rollover_description_inner")[1].text.strip()
Genres=movie_rating['genres'].values.tolist()
df_NYPD['Complaint Type'].value_counts().head(1)
print(teamAttr.shape) $ teamAttr.head(3)
df3['intercept']=pd.Series(np.zeros(len(df3)),index=df3.index) $ df3['ab_page']=pd.Series(np.zeros(len(df3)),index=df3.index) $ df3.head()
import datetime $ import pandas as pd $ date_now = pd.to_datetime('2018-07-27') $ multiple_party_votes_all['days_ago'] = [(date_now - date).days for date in multiple_party_votes_all['date']]
df.loc[0:2, ['A', 'C']]
op_ed_articles.shape $
df_copy.drop(df_copy[df_copy['retweeted_status_id'].notnull()].index,inplace=True) $
len(session.query(Station.station).all())
sdf = sdf.assign(Norm_summary = normalize_corpus(sdf.summary))
y = tf.placeholder(shape=[None], dtype=tf.int64, name="y")
dfd.zones.value_counts()
autos_pr = autos_p[autos_p['registration_year'].between(1910, 2016)]
twitter_archive_master[twitter_archive_master.rating_numerator < 10].head()
roc_auc = metrics.auc(fpr, tpr)
backers = databackers.pivot_table(index = 'backers', columns='successful',aggfunc=np.size) $ backers = backers.rename(columns= {1: 'Successful', 0:'Failed'}) $ backers[backers['Successful'] == backers['Successful'].max()]
finals = pd.concat([finals, pd.get_dummies(finals['type'])], axis=1) $ finals.head()
wo_rt_df = en_tweets_df[~en_tweets_df['full_text'].str.contains('RT @')]
df2_vif = pd.DataFrame() $ df2_vif["VIF Factor"] = [vif(df_teamX_scaled.values, i) for i in range(df_teamX_scaled.shape[1])] $ df2_vif["features"] = teamX.columns $ df2_vif
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
b.find_by_xpath('//*[@id="day-section"]/div/div[3]/div[8]/div[2]/ul/li[6]/button').click()
plt.hist(null_vals) $ plt.axvline(x=obs_diff, color='red') $
logit = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ results = logit.fit()
extract_deduped..groupby('application_month').size()
v_to_c['time'] = v_to_c.checkout_time - v_to_c.visit_time $ print(v_to_c)
tranny = df.T $ tranny
stats.ttest_ind(clean_users[clean_users['active']==1]['account_life'].dt.days,clean_users[clean_users['active']==0].dropna(how = 'any')['account_life'].dt.days,equal_var = False)
index = dframe_team['Draft_year'] $ dframe_team.set_index(index, inplace = True) # sets the column 'Draft_year' as the index' $ dframe_team.head()
df.groupby('Updated Shipped ranges').aggregate({'Updated Shipped diff':['count','mean','std','min','max']}) $ df_updated_stats = df.groupby('Updated Shipped ranges').aggregate({'Updated Shipped diff':['mean','std']})
latest_df.annotations.head(1)
Dataset.from_source('telemetry').schema
train=pd.get_dummies(train,columns=['air_genre_name'])
month=pd.Period('Oct-2020', freq='M') $ month
PATTERNS2 = {k:re.compile(v) for k, v in PATTERNS.items()}
df_prep10 = df_prep(df10) $ df_prep10_ = pd.DataFrame({'date':df_prep10.index, 'values':df_prep10.values}, index=pd.to_datetime(df_prep10.index))
autos["price_euro"].value_counts().head(20)
print(compressed_data[0].head(5)) $ print(compressed_data[0].dtypes)
people.groupby(lambda x:GroupColFunc(people,x,'b')).std()
from dateutil.parser import parse $ from datetime import timedelta
auth = tweepy.OAuthHandler(consumer_key=consumerKey, $     consumer_secret=consumerSecret) $ api = tweepy.API(auth) $
print(f"{urls[2]} returns:") $ ux.is_short(urls[2])
df.replace({1000:11,2000:61})
object.__prepare__?? $
title = 'Line chart of Number of created accounts' $ acc_created_date.plot(x='Date', y='Freq', title=title, fontsize=11, grid=True, figsize=(16,6))
reddit_info.titles.duplicated()
posts_df['processed_text'] = [process_text(s.lower()) for s in tqdm(posts_df['Body'])]
df['Trip_duration']=df['Trip_duration'].astype('timedelta64[s]')
ad_source.columns = ['AD_'+str(col) for col in ad_source.columns]
df_q = pd.read_sql(query, conn, index_col='Matrix_UID') $ df_q.head(5)
df2.head(2)
adopted_cats.loc[adopted_cats['Color']=='Gray/Tortie','Color'] = 'Gray Tortie'
data = scale(df_select) $ noOfClusters = 4 $ model = KMeans(init='k-means++', n_clusters=noOfClusters, n_init=20).fit(data) $
article_project = pd.read_csv('data_old/article_project.csv') $ print(article_project.shape) $ article_project.head()
station_stat = session.query(Measurement.station, Station.name, func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $ filter(Measurement.station == 'USC00519281').all() $ station_stat
group_by_month = sorted_by_date.groupby(['contb_receipt_dt']).sum().reset_index() $ group_by_month.contb_receipt_dt = group_by_month.contb_receipt_dt.apply(lambda date: dt.datetime.strptime(date, '%d-%b-%y').month) $ group_by_month = group_by_month.groupby(['contb_receipt_dt']).sum().reset_index() $ group_by_month.contb_receipt_amt
df.loc[df.full_sq < df.life_sq,'life_sq'] = np.NaN
recs = sess.get_data(['ibm us equity','aa us equity'],['best analyst recs bulk','px last','chg pct ytd']) $ recs
data.head()
x_train = train_data.values
nnew = df2.query('group == "treatment"').shape[0] $ print(nnew)
X.shape
fig1=plt.figure(figsize=[8,8]) $ ax = fig1.add_subplot(111) $ ax.bar(hour_coef.index,hour_coef[0],width=1) $
rt_max = data['RTs'].max() $ rt_tweet = data[data['RTs'] == rt_max] $ print("The tweet with more retweets is: \n{}".format(np.array(rt_tweet['Tweets'])[0])) $ print("Number of retweets: {}".format(rt_max)) $ print("{} characters.\n".format(np.array(rt_tweet['len'])[0]))
result1 = -df1 * df2 / (df3 + df4) - df5 $ result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5') $ np.allclose(result1, result2)
problem_combos.cache()
status = status.favorite() $ status.favorited, status.favorite_count
blurb_SVD = tSVD.fit_transform(blurbs_to_vect)
output_location = 's3://{}/{}/output'.format(bucket, prefix) $ print('training artifacts will be uploaded to: {}'.format(output_location))
df.dtypes
van_final['diffs'] = van_final['diffs'].fillna(0)
empInfo["sender"] = empInfo.ID $ empInfo["receiver"] = empInfo.ID
temp_hist_data = pd.DataFrame(session.query(Measurement.tobs).filter(Measurement.station == "USC00519281").filter(Measurement.date > '2016-08-23').all()) $ temp_hist_data.head()
state = environment.reset() $ state, reward, done=environment.execute(env.action_space.sample()) $ state.shape
rfc = RandomForestClassifier() $ model=rfc.fit(X_train, y_train) # just to run the next line
top_songs.head()
from sklearn.cluster import KMeans $ from sklearn import metrics $ from sklearn.decomposition import TruncatedSVD $ from sklearn.pipeline import make_pipeline $ from sklearn.preprocessing import Normalizer
lrs.shape
primary_share = IsPrimaryShare() $ common_stock = Fundamentals.security_type.latest.eq('ST00000001') $ not_depositary = ~Fundamentals.e
polarity = pd.DataFrame(df.groupby(['polarityFeeling', 'profile']).size().rename('counts')).astype(int) $ polarity['percent'] = polarity['counts'].apply(lambda x: x/polarity.sum()) $ polarity $
df2[df2.duplicated(['user_id'], keep=False)] #displaying the row information for the repeated user_id
for c, f in zip(tips_model.coef_[0], features.columns.values): $     print(f'{c:5.2f} * {f}')
all_tables_df.loc[[2, 3, 4, 10, 2000], ['OBJECT_NAME', 'OBJECT_TYPE']]
sum(tw.expanded_urls.isnull())
clean_stations.drop(['name'], axis=1, inplace=True)
df_new['page_c1'] = df_new['ab_page']*df_new['c1'] $ df_new['page_c2'] = df_new['ab_page']*df_new['c2'] $ logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','page_c1','page_c2']]) $ res = logit_mod.fit() $ res.summary() $
soup.body.ul("li")
theta3 = np.mean(sample[:, :2], 0) $ g = np.mean(sample[:, 2:], 0) $ outliers = (g < 0.5)
splits1=['payout_quarter', 'rating_base',  ('rating_switch', 'rating_base')] $ xirrd={} $ for split in splits1: $     xirrd[split]=pd.DataFrame({'actual':xirr_actual[split], 'expected':xirr_expected[split]})
pd.version.version
df2['intercept']=1 $ df2[['control','treatment']] = pd.get_dummies(df2['group']) $ df2.head()
df2 = df2[~df2.user_id.duplicated(keep='first')] $ df2.shape #total rows minus one 'dup' - done!
bets = games_to_bet[games_to_bet['bet_either'] == 1].copy()
trading.df.head()
tmp_cov = cov_df.loc[i].copy() $ tmp_cov
twitter_archive_clean.info()
tmax_day_2018.values
precipitation_df = pd.read_sql(session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23").statement,session.bind) $ precipitation_df.head(15)
from sklearn.model_selection import train_test_split $ Xtr, Xts, Ytr, Yts = train_test_split(X, Y, train_size=500)
chart = top_supporters.head(5).amount.plot.bar()
quant_995 = claps.quantile([0.90]) $ print(quant_995) $ data = data[data["claps"] < int(quant_995)] $ data.shape
logisticRegr.fit(train_ind[features], train_dep[response]) $ zip(features,logisticRegr.coef_[0]) $ train_features_df= train_ind[features] $ train_features_arr= train_features_df.values $ zip(features,(np.std(train_features_arr)*logisticRegr.coef_[0]))
bb_df = bb_df.dropna(how='any') $ len(bb_df)
start_error_set = error_set['timestamp'].min() $ end_error_set = error_set['timestamp'].max() $ print('The ERROR started on the {} and has been ended on {}'.format(start_error_set,end_error_set))
no_of_rows = df.shape[0] $ no_of_rows
def combine_DfOfAllWells_with_knnDf(df_all_wells_basic,knn_df): $     df_all_wells_wKNN = pd.merge(df_all_wells_basic, knn_df, on='UWI') $     return df_all_wells_wKNN
talks.sort(inplace = True, columns = 'id')
last_hours.to_period()
model_2 = graphlab.linear_regression.create(train_data, target = 'price', $                                                   features = model_2_features, $                                                   validation_set = None)
bacteria2.fillna(method='bfill')
inputs = {"score_input": SampleDefinition(DataTypes.PANDAS, df)} $ json_schema = generate_schema(run_func=run, inputs=inputs, filepath=SCHEMA_FILE) $ out = json.dumps(json_schema) $ with open(SCHEMA_FILE, 'w') as f: $     f.write(out)
train_doc = doc_id_list[['training' in doc_id for doc_id in doc_id_list]] $ print("train_doc is created with following document names: {} ...".format(train_doc[0:5]))
loans_df.verification_status.value_counts()
plt.show()
trump_time = df[["time","count_trump"]][df["count_trump"]>0][1:] $ trump_time.head()
df.rename(columns={'Indicator': 'Indicator_id'}, inplace=True) # Now, it is permanent $ df.head(2)
vip_crosstab_percentage.plot() $ plt.ylabel('Proportion') $ ax = plt.gca() $ ax.set_title("Finish Type by Proportion Over Time") $ plt.show()
df["RNG_INGRESO_BRUTO_COD"] = df["RNG_INGRESO_BRUTO"].map(ingreso_rng_dict)
topicmax = np.array(topicmax)
df.head()
Test.SetFlowValues(9.25, 12.00, 9.25)
display(kelsey.head(5))
s519397_df["prcp"].min()
validation.analysis(observation_data, distributed_simulation)
list.remove(3) $ print(list)
dataset[['customer_id','order_id']].values
len(train_data[train_data.offerType == 'Gesuch'])
percent_quarter[mask].plot()
seen.shape
df['start'] = pd.to_datetime(df['start']) $ df['finishg'] = pd.to_datetime(df['finish']) $ df['created'] = pd.to_datetime(df['created']) $ df['duration'] = pd.to_timedelta(df['duration'].apply(format_duration)) $ df['pause'] = pd.to_timedelta(df['pause'].apply(format_duration))
pickle.dump(df, open("C:/Users/Anke/Masterarbeit/Pythonstuff/df_manipulated_2.pickle", "wb"))
def daily_normals(date): $     sel = [func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)] $     return session.query(*sel).filter(func.strftime("%m-%d", Measurement.date) == date).all() $ print(daily_normals('07-15'))
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ convert_rate_new = np.mean([p_new, p_old]) $ convert_rate_new
target = ('../datasets/CSVs/1500000 Sales Records.hdf') $ target
prices = pd.read_csv("prices.csv" , parse_dates= ['date'])
joined = join_df(train, store, "Store") $ joined_test = join_df(test, store, "Store") $ len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])
plt.hist(p_diffs);
df_new['country_page_interact'] = df_new['country'] + '_' + df_new['landing_page'] $ print(df_new['country_page_interact'].value_counts()) $ df_new[['CA_new_page', 'CA_old_page', 'UK_new_page', 'UK_old_page', 'US_new_page', 'US_old_page']] = pd.get_dummies(df_new['country_page_interact']) $ df_new = df_new.drop('US_new_page', axis=1) $
from plotly import __version__ $ from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot $ import plotly.graph_objs as go $ print(__version__) # requires version >= 1.9.0 $ init_notebook_mode(connected=True)
conf_matrix = confusion_matrix(y_tfidf_test, lr2.predict(X_tfidf_test), labels=[1,2,3,4,5]) $ conf_matrix
print(df2.converted.value_counts()) $ print(df2.landing_page.value_counts())
test_number_con = convers_con.groupby('added', as_index=False).count() $ test_number_con.head(2)
print(station_availability_df.describe())
def convert(x): $     return x.encode('utf8')
image = Image.open('/Users/i351707/Desktop/tesseract_wrong/1268-test.png') $ image
data.describe()
df_user_info['marketing_source'].fillna("unknown").value_counts()
df2[df2.group == 'treatment'].converted.mean()
prices.plot()
logrecnos = data['LOGRECNO'] $ col_names = data.columns.values[6:] $ col_names
stories['created_dow'] = stories.created_at.map(lambda x: x.weekday())
jobs = pd.merge(jobs, fairshare[['Group', 'MAXCPUS']], how = 'left')
matrix_det = tf.matrix_determinant(matrix2)
from collections import defaultdict $ data_dict=defaultdict(list) $ for i in range(len(price)): $     for j in range(11): $         data_dict[map_label[j]].append(price[i][j])
trains_fe2_x= trains_fe1.drop(['days_since_prior_order','add_to_cart_order','reordered','eval_set'], axis=1) $ trains_fe2_x.head() #12 features as the predictor variables
for hyperlink in s.find_all('a'): $     print(hyperlink.get('href'))
hr[hr["icustay_id"]==14882][:100].plot(x="new charttime", $                                  y=["chart delta"])
MATTHEWKW = pd.DataFrame(MATTHEWKEYWORD) $ MATTHEWKW['date'] = MATTHEWKW.created_at.apply(lambda x: x.date()) $ print(len(df))
df_d=pd.DataFrame(day_prcp,columns=['date','Prcp']) $ df_d['date'].head()
snow.select("select count(distinct patient_id) from nk_albatross_psp where left(patient_id, 5) != 'XXX -' and patient_id in (select distinct patient_id from nk_albatross_ftd)")
grader.status()
newdf.drop('yearmonth', axis=1, inplace=True)
imgp.info()
df.dtypes
df['time_elapsed'] = df['time_elapsed'].astype('timedelta64[D]') * 24 $ display(df['time_elapsed'].head())
data2['target']=np.where(data2['close']>data2['open'],1,0)
y.start_time # Tells us that the object/period starts on 1st Jan 2016
pd.read_excel('PPB_gang_records_UPDATED_100516.xlsx',sheetname=1).head()
counts_by_campaign_date = ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'] $ ).count() $ counts_by_campaign_date
for v in cat_vars: joined[v] = joined[v].astype('category').cat.as_ordered() $ for v in contin_vars: joined[v] = joined[v].astype('float32') $ dep = 'Sales' $ joined = joined[cat_vars + contin_vars + [dep, 'Date']]
datetime.datetime.strptime("Mar 03, 2010", "%b %d, %Y")
print("The following is the row information for the repeat user_id:  ") $ df3.head()
p_diffs = [] $ for _ in range(10000): $     old_converted = np.random.binomial(1, size = n_old, p= p_old) $     new_converted = np.random.binomial(1, size = n_new, p=p_new) $     p_diffs.append(new_converted.mean() - old_converted.mean())
df.std() #standard deviation
c = dft.groupby(['stamp_round'])['log_followers_count'].sum()
breed_predict_df_clean['tweet_id'] = breed_predict_df_clean.tweet_id.apply(str) $
for x in range(10): $     %sql INSERT INTO temps3 (device,datetime,temp,hum) VALUES('pi222',date(now()),73.2,22.0);
pd.crosstab(test["rise_nextday"], predictions, rownames=["Actual"], colnames=["Predicted"])
browser.click_link_by_partial_href('photojournal.jpl.nasa.gov/jpeg')
minMaxDate['minDateTS'] = minMaxDate['minDate'].map(lambda x: np.datetime64(x)) $ minMaxDate['diffTS'] = minMaxDate['diff'].map(lambda x: np.timedelta64(x))
autos.columns
pumpkin = data["author"] == "PumpkinDevourer" $ pd.options.display.max_colwidth = 100 $ data[pumpkin].sort_values(by="num_comments", ascending=False)[0:9][["author", "num_comments", "score", "title"]] $
top_10_authors = git_log['author'].value_counts().iloc[:10] $ print(top_10_authors)
Labels_majority = (((CurrentA1.iloc[:,3:14] + CurrentA2.iloc[:,3:14] + CurrentA3.iloc[:,3:14])/3)>0.3).astype("int32") $ Class_frame_majority = pd.concat([CurrentA1.iloc[:,0:2],Labels_majority],axis=1) $ Class_frame_majority.to_csv("union_voting_elisa.csv")
all_feature_list = all_features.split(',')
df2['intercept'] = 1 $ df2[['treatment', 'ab_page']] = pd.get_dummies(df2['group']) $ print(df2.info()) $ df2.drop(['treatment'], axis=1, inplace=True) $ df2.head()
tweets.describe(include='all')
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old, n_new],alternative='smaller') $ z_score, p_value
twitter_archive_clean[twitter_archive_clean.rating_denominator > 14]
tweet_archive_enhanced_clean =  pd.merge(tweet_archive_enhanced_clean,tweet_json[['retweet_count','favorite_count','id']], how ='left', left_on ='tweet_id',right_on ='id')
nnew = df2.query('group == "treatment"').user_id.nunique() $ nnew
df.head()
image_copy.sample(5)
train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ $     train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)
df['TOTAL_PAYMENT'].describe().astype(int)
import nltk $ nltk.download('stopwords') $ from nltk.corpus import stopwords
pd.set_option('display.max_columns',100)
offseason14 = ALL[(ALL.index > '2014-02-02') & (ALL.index < '2014-09-04')]
sum(tw_clean.duplicated(subset=['expanded_urls']))
fig, ax = plt.subplots(1, figsize=(12,4)) $ plot_with_moving_average(ax, 'Seasonal AVG Therapists', therapist_duration, window=52)
brand_mean_km = {} $ for brand in brands: $     mean_km = autos.loc[autos["brand"]==brand,"odometer_km"].mean() $     brand_mean_km[brand] = mean_km $ brand_mean_km
df_cal['is_all_day'].hist(bins=4)
results = model_selection.cross_val_score(gnb, X_test, Y_test, cv=kfold) $ results.mean()
df['date']=df['date']-pd.Timedelta('4 hours')
p = getpass.getpass() $ r = requests.get('https://api.github.com/user', auth=('rsouza', p)) $ r.status_code
df = pd.read_csv('data_stocks.csv')
cityID = '5c2b5e46ab891f07' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Las_Vegas.append(tweet) 
pos_data_train['rating'].count()
learner.save('lm1')
doctype = billstargs[billstargs.billtext.str.contains('DOCTYPE')] $ doctype.shape
train[train['is_rushhour'] == 1].head(3)
director_df.dir_agross = pd.to_numeric(director_df.dir_agross.replace("\$","", regex = True).str.strip()) $ director_df.dir_gross = pd.to_numeric(director_df.dir_gross.replace("\$","", regex = True) $                                       .replace(",","", regex = True).str.strip()) $ director_df.dir_nmovies = pd.to_numeric(director_df.dir_nmovies) $ director_df.info()
nba_df.tail(10)
df1 = pd.DataFrame(list('ABC'), columns=['c1']) $ df2 = pd.DataFrame(list('DEF'), columns=['c2']) $ pd.merge(df1, df2, left_index=True, right_index=True)
df.T #transpose
dtunit = np.array(data_dict['Traded Volume'],dtype='float64')
feedex = pd.read_sql_query('select * from "events_feedexes"',con=engine)
s.head()  # default to length 5
week38 = week37.rename(columns={266:'266'}) $ stocks = stocks.rename(columns={'Week 37':'Week 38','259':'266'}) $ week38 = pd.merge(stocks,week38,on=['266','Tickers']) $ week38.drop_duplicates(subset='Link',inplace=True)
plt.scatter(rets.SCTY,rets.TAN)
Quantile_95_disc_times_pay = df.groupby(['drg3','year']).quantile(.95) $ Quantile_95_disc_times_pay.head(15) $
for sent in fullData['change'].value_counts().keys(): $     subset = fullData[fullData['change'] == sent] $     plt.plot(subset['compound']**2, label=sent) $ plt.legend()
df.groupby('Single Name').sum()[df.groupby('Single Name').count()['Name']>6]
blah = test_collection.find({"URL": feral_url}) $ test_collection.find({"URL": data[1]['display_url']}).count() $ if test_collection.find({"URL": feral_url}).count() == 0: $     print('URL not in database, add it')
df['Descriptor'][df['Complaint Type'] == 'Noise - Residential'].value_counts().head()
age_up70.head()
results["units"].head()
cityID = '0562e9e53cddf6ec' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Santa_Ana.append(tweet) 
sns.jointplot(x='figure_density', y='prosecution_period', data=figure_density_df[figure_density_df.figure_density < 5], kind="kde", color="green") $ plt.show()
result[1][0.1].plot(label = 'Modelled') $ plt.plot(df['10'].values[0:n_iterations], label = "Measured") $ plt.fill_between(np.linspace(0,n_iterations-1, n_iterations), df['10'].values[0:n_iterations] - 0.5, df['10'].values[0:n_iterations] + 0.5, $                  edgecolor = 'Orange',facecolor ='Orange', alpha = 0.5) $ plt.legend();
sub_df.iloc[0]['date_created'].to_datetime()
overlapping_trip_passenger_ids = trips_sorted_passenger[trips_sorted_passenger["overlap"] == 1]["id"].values $ df_trips = df_trips.drop(overlapping_trip_passenger_ids)
all_data_merge = pd.merge(all_data, mapping, on = ['rpc']) $ all_data_merge.shape
authors_by_project_and_commit_df.schema
%%bash $ aws s3 ls s3://olgabot-maca/lung_cancer/sourmash/ | cut -f 9 -d ' ' | cut -f 1 -d'.' > samples_with_signatures.txt $ ls -lh | grep --fixed-strings --file samples_with_signatures.txt -v | grep log > failed_samples.txt
df_raw = pd.read_feather(file_name)
treatment = df2.query("group == 'treatment'") $ prop_conv_treat = len(treatment.query("converted == '1'")) / treatment.shape[0] $ print('Given that an individual was in the treatment group, the probability they converted is {}'.format(prop_conv_treat))
autos["price"].value_counts().sort_index(ascending=False).head(15)
validation_size = 0.30 $ seed = 2018 $ X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size = validation_size, random_state = seed)
twitter_data.describe()
test=pd.get_dummies(test,columns=['air_genre_name'])
shows['stemmed_keywords'] = shows['keywords'].dropna().apply(split_and_stem2)
display(data2.head(10))
print('Tokenization') $ en_df['tokens'] = en_df.combined.apply(lambda x: x.split(' ')) $ en_df['num_words'] = en_df.tokens.apply(lambda x: len(x)) $ en_df.head()
df[df['Descriptor'] == 'Pothole']['Unique Key'].groupby(df[df['Descriptor'] == 'Pothole'].index.dayofweek).count().plot()
BroncosBillsTweets = BroncosBillsTweets.drop_duplicates(subset=['text'])
df.loc['1975-01-01']
wrd_full.query('favorite > 9447')['hour'].value_counts()
with pdfplumber.open('../pdfs/collections.pdf') as pdf: $     test = pdf.pages[1] $     table = test.extract_table() $     print(table)
contribs = pd.read_csv('http://www.firstpythonnotebook.org/_static/contributions.csv')
restaurants = pd.read_csv("/home/ubuntu/data/restaurant.csv", dtype=unicode, index_col=["CAMIS"], encoding="utf-8") $ restaurants
from sklearn.feature_selection import SelectFromModel $ from sklearn import pipeline $
low_odometer = (autos["odometer_km"] < odometer_LF) == True $ odometer_outliers = autos.loc[low_odometer, :].index
df_twitter = pd.read_csv('data/twitter-archive-enhanced.csv') $ df_twitter_copy = df_twitter.copy() $ df_twitter_copy.head()
webmap.basemap = 'gray'
pd.set_option('display.max_columns',100) $ df.head(10000)
outname = 'user_dataset_results.csv' # You can rename this file to whatever you'd like. Using the same output filename multiple times could cause overwriting, be careful! $ fullpath = os.path.join('./output', outname)  $ combined_df.to_csv(fullpath, index=False)
gdax_trans_btc.loc[(gdax_trans_btc['Type'] == 'match') & (gdax_trans_btc['Trade_amount']>0),'Type'] = 'Buy' $ gdax_trans_btc.loc[(gdax_trans_btc['Type'] == 'match') & (gdax_trans_btc['Trade_amount']<0),'Type'] = 'Sell' $ gdax_trans_btc.loc[(gdax_trans_btc['Type'] == 'transfer') & (gdax_trans_btc['Trade_amount']>0),'Type'] = 'Fund' $ gdax_trans_btc.loc[(gdax_trans_btc['Type'] == 'transfer') & (gdax_trans_btc['Trade_amount']<0),'Type'] = 'Withdraw'
from datetime import datetime as dt $ datAll['Date'] = pd.to_datetime(datAll['Date']) $ datAll = datAll[(datAll['Date']>='2010-01-01') & $                 (datAll['Date']<='2018-01-01')]
Historical_Raw_Data.set_index("Date_Monday")["2017"].groupby( $     [pd.TimeGrouper('M'), "Product_Motor"]).sum().unstack(level=0).fillna(0)['Qty']
now = datetime.now() $ now.hour, now.year
number_of_users = df.user_id.nunique() $ number_of_users
y_class_baseline = demo.get_class(y_pred_baseline) $ cm(y_test,y_class_baseline,['0','1'])
df['intercept'] = 1 $ df[['ab_page', 'temp']] = pd.get_dummies(df['landing_page']) $ df = df.drop('temp', axis=1) $ df.head()
graf_counts.columns
del_id = list(df_never_moved['id']) $ df['Timestamp +2'] = df['Timestamp'].apply(addtwo)
popC15.groupby(by='content').size()
autos['registration_year'].sort_values().head(10)
ytd_measure.describe()
failures['failure'].value_counts().plot(kind='bar') $ plt.ylabel('Number of failures') $ plt.show()
import pip $ pip.main(['install','quandl'])
Sandbox.head()
session.query(Measurement.station,func.count(Measurement.station))\ $       .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
bird_names = pd.unique(bird_data.bird_name) $ print(bird_names)
treatment_group = df2.query('group == "treatment"') $ converted_treatment_group = converted_group.query('group == "treatment"') $ print(converted_treatment_group.user_id.nunique() / treatment_group.user_id.nunique())
df_substrate = df_mysql[df_mysql.endDate==max(df_mysql.endDate)] $ df_substrate = df_mysql[['barrelID', 'substrate']] $ def getSubstrate(barrelID): $     return df_substrate.substrate[df_substrate.barrelID==barrelID].values[0]
theft.sort_values('DATE_OF_OCCURRENCE', inplace=True, ascending=False) $ theft.head()
print(np.info(np.ubyte))
run txt2pdf.py -o"2018-06-14 2148 OROVILLE HOSPITAL Sorted by Payments.pdf"  "2018-06-14 2148 OROVILLE HOSPITAL Sorted by Payments.txt"
scoring_url = client.deployments.get_scoring_url(deployment_details) $ print(scoring_url)
X.shape, y.shape
url = 'http://www.espn.com/nfl/game?gameId=400999172' $ src = requests.get(url) $ score = extract_game_data(src.text) $ df_afc_champ2018 = convert_gamedata_to_df(score)
url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vRlXVQ6c3fKWvtQlFRSRUs5TI3soU7EghlypcptOM8paKXcUH8HjYv90VoJBncuEKYIZGLq477xE58C/pub?gid=0&single=true&output=csv' $ df_hourly = pd.read_csv(url,parse_dates = ['time'],infer_datetime_format = True,usecols = [0,3]) $ df_hourly.head()
vol = vol.groupby(vol.index.day).transform(lambda x: x/x.sum()) $ vol.head()
df2.query('group == "treatment" and converted == 1').count()[0]/df2[df2['group'] == 'treatment'].count()[0]
calls_df[["length_in_sec"]].plot(kind="density")
import codecs $ def save_file(text, file_name): $     with codecs.open(file_name, 'wb', 'utf8') as outfile: $         outfile.write(text) $
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-(0.05/2))
df_more.head()
data['timestamp'] = pd.to_datetime(data['Date'], unit='s')
result.summary2()
html = requests.get('http://as.nyu.edu/sociology/people/faculty.html').text $ slices = html.split('filtered-items-item js-filter-item')
df_cprc.plot(kind='hist', alpha=0.5, bins=20)
import multiprocessing $ cores = multiprocessing.cpu_count() $ print(cores) $ assert gensim.models.word2vec.FAST_VERSION > -1, "This will be painfully slow otherwise"
train_holiday_oil = train_holiday.join(oil_spark, 'date', 'left_outer') $ train_holiday_oil.show()
search = tw.Cursor(api.search, q='#donaldtrump').items(400)
from sklearn.decomposition import PCA $ pca = PCA() $ pca.fit(data2Scaled[list]) $ pcaData = pca.transform(data2Scaled[list])
data_type = df.dtypes
df = df.loc[df['lang'].str.startswith('en') == True, ['tweetId','screenName', 'processed']]
df_columns['Complaint Type'].value_counts() # Homeless $ df_columns[df_columns['Complaint Type'].str.contains('Homeless')].index.month.value_counts().sort_index().plot() $
PRIOR = pd.DataFrame({'Prior to Jan 2016': x , 'Prior to Jan 2017': y , 'Prior to Jan 2018': z},index = [0])
Ralston["TMAX"].mean() $
f_lr_hash_test = (prediction_test $                   .withColumn('f_lr_hash_inter2_2p18_noip', get_pred_udf(col('probability'))) $                   .select('id', 'f_lr_hash_inter2_2p18_noip')) $ f_lr_hash_test.show(3)
no_name_list = df_twitter_copy[df_twitter_copy.name.str.contains('(^[a-z])')].name.tolist()
df2.query('landing_page == "new_page"').converted.count()
betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality $ nx.set_node_attributes(G, betweenness_dict, 'betweenness') $
train_data = gl.SFrame(submissions[submissions['solved']==1])
local_hol=local_holidays[['date','description','locale','locale_name']] $ local_hol.columns=['date','description_local_hol','locale_local','city'] $ pd.DataFrame.head(local_hol)
perturb = (np.random.randint(0, 20, size=N) - 10) * 0.25
train['default'] = np.where(train.loan_status == 'Charged Off', 1, 0)
all_cards = pd.DataFrame(data = None, columns = all_cards_columns) $ all_cards.rename_axis("name", inplace = True) $ all_cards.head()
df_clean.info()
pd.DataFrame(X.toarray()).head()
crimes_date_cnt = crimes.groupby('Date')[['IncidntNum']].count() $ crimes_date_cnt.reset_index(inplace=True) $ crimes_date_cnt.rename(columns={"Date": "incident_date"}, inplace=True)
ip['p1_conf'].describe()
df[(df['abuse_type']=='A') & (df['public']=='offline')].count()[0]
example_model = graphlab.linear_regression.create(train_data, target = 'price', $                                                   features = example_features, $                                                   validation_set = None)
client.repository.list_definitions()
length = [len(h.tweets) for h in heap]
a_list.remove(another_list) $ a_list
pd.read_csv("msft.csv", skiprows=100, nrows=5, header=0, $            names=['open', 'high', 'low', 'close', 'val', 'adjClose'])
some_numbers = [10,8,3,1,5,-5,2,-15,-4,5,-2,-1,-3,-5] $ pd.Series(some_numbers).replace(range(-5,-1,2),0)
tweet_df_clean.head(3)
df3=pd.read_pickle('./city_service_requests_neighborhoods')
df2[df2.duplicated(['user_id'], keep=False)] #identifying the repeated user_id $
options_frame['ModelError'].hist() $
accounts.head()
data_compare['SA_mix'].mean(), data_compare['SA_google_translate'].mean(), data_compare['SA_textblob_de'].mean()
np.exp(result4.params)
pandas_df = young.toPandas()
support=merge[merge.committee_position=="SUPPORT"]
basePricePerDay = dfEPEXbase.groupby(dfEPEXbase.index.date).aggregate('mean')['Price'] $ basePricePerDay.head() # verify calculation
from sklearn.preprocessing import LabelEncoder
pumaPop.head()
test['srch_ci'].fillna(test['date_time'],inplace=True) $ test['srch_co'].fillna(test['date_time'],inplace=True)
maxitems = 10 $ print "Brighton tweets retrieve testing" $ print '----------------------------------' $ for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_B).items(maxitems): $     print tweet.text
analyze_set['date']=pd.to_datetime(analyze_set['date']) $ analyze_set['year']=analyze_set['date'].dt.year $ analyze_set['year'].value_counts()
for j in range(agg_providers.shape[0]): $     if len(agg_providers.loc[j,'name']) <=70: $         agg_providers.loc[j,'name'] = agg_providers.loc[j,'name'] + (70 - len(agg_providers.loc[j,'name']))*' ' $ agg_providers.head() $
my_instance.special_print = print() $ try: $     my_instance.special_print()  # <- These parentheses are not accepted $ except: $     print("No, unfortunately, functions added from external scopes don't accept arguments.")
p_new = new_page_converted.sum() / new_page_converted.shape[0] $ p_old = old_page_converted.sum() / old_page_converted.shape[0] $ p_new - p_old
pd.Series(np.random.rand(5))
import datetime $ multiple_party_votes_all['date'] = pd.to_datetime(multiple_party_votes_all['date'])
eia_total_monthly.head()
len(cats_in['Animal ID'].unique())
df_byzone.shape
dup_labels2 = pd.Index(['bar', 'a'])
tweets_df.retweet_count
timezone_df['gmt_hour'] = timezone_df['gmt_offset'].apply(lambda x:x/3600)
merged2.dropna(subset=['Specialty'], how='all', inplace=True)
iowa = pd.read_csv(iowa_csv) $ iowa.head()
new_data = data*4.3 $ np.savetxt('data/new_data.csv', new_data)
ks_name_lengths = ks_projects.groupby(['name_length', "state"]).size().reset_index(name='counts') $ ks_name_success = ks_name_lengths.drop(ks_name_lengths.index[ks_name_lengths.state != 'successful']) $ ks_name_success.set_index('name_length', inplace=True) $
raw_scores = df.rating_score[df.rating_score.notnull()]
fig = plt.figure(figsize=(10,4)) $ ax = fig.add_subplot(111) $ ax = resid_701.plot(ax=ax);
logit_mod=sm.Logit(df2['converted'],df2[['intercept','new_page']]) $ results=logit_mod.fit()
people_with_one_or_zero_collab['authorId'].nunique()
df.groupby('Year')['Points'].agg([np.sum, np.mean, np.std])
csvDF.head(10) $ csvDF.tail() $ csvDF.tail(10)
init_ind = 82 $ trans = acc.transactions.loc[82] $ trans
print pearsonr(weather.max_wind_Speed_mph[weather.max_gust_speed_mph >= 0], $                weather.max_gust_speed_mph[weather.max_gust_speed_mph >= 0])
weather['temp'].mean()
df.plot() $ plt.show()
speeches_metadata = speeches_features.merge(speeches_similarity_df, on = 'index')
headers = pd.read_csv(r'./weather_headers.csv',header=None,squeeze=True)
search_url = "https://api.cognitive.microsoft.com/bing/v7.0/search"
subset.to_csv('subset.csv')
for subreddit in reddit.subreddits.default(limit=None): $     print(subreddit)
net_loans_exclude_US_outstanding.head()
imageids = list(range(1, avg_preds.shape[0]+1)) $ print (type(imageids), len(imageids), imageids[0:5], imageids[-5:])
df_arch_clean['source'] = df_arch_clean['source'].str.replace('<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>', 'iPhone') $ df_arch_clean['source'] = df_arch_clean['source'].str.replace('<a href="http://vine.co" rel="nofollow">Vine - Make a Scene</a>', 'Vine') $ df_arch_clean['source'] = df_arch_clean['source'].str.replace('<a href="http://twitter.com" rel="nofollow">Twitter Web Client</a>', 'Web') $ df_arch_clean['source'] = df_arch_clean['source'].str.replace('<a href="https://about.twitter.com/products/tweetdeck" rel="nofollow">TweetDeck</a>', 'TweetDeck')
df_vow.columns = ['Date','Open','High','Low','Close','Volume'] $ df_vow.columns.values $
s2 = pd.Series(range(3)) $ s1[1:] + s2[:4]
m3.fit(lrs, 7, metrics=[accuracy], cycle_len=2, cycle_save_name='imdb2')
print('Records in train %d.'%len(train)) $ print('Records in test %d.'%len(test))
len(stfvect.get_stop_words())
one_hot_domains_questionable = df_questionable_3.groupby('user.id')[media_classes].sum().fillna(0) $ one_hot_domains_questionable = one_hot_domains_questionable.apply(normalize, axis=1).fillna(0) $ tsne = TSNE(n_components=2, learning_rate=150, verbose=2).fit_transform(one_hot_domains_questionable)
stocks.to_sql('stocks', engine, index = False, if_exists = 'append')
shots_df[['PKG', 'PKA']] = shots_df['PKG/A'].str.split('/', expand=True) $ shots_df.drop('PKG/A', axis=1, inplace=True)
autos['price'].describe()
df_null_acct_name = df[df['LinkedAccountName'].isnull()]
run txt2pdf.py -o"2018-06-18  2015 460 discharges.pdf"  "2018-06-18  2015 460 discharges.txt"
fig, ax = plt.subplots(figsize=(14, 5)) $ ax.set(title='File Sizes', xlabel='File length in bytes', ylabel='Count') $ ax.hist(af.length, bins=range(0, af.length.max(), 100000)) $ ax.set_yscale('log')
archive_df_clean.loc[archive_df_clean['name'] == 'Bella'] $
X_loadtest= preprocessing.StandardScaler().fit(X_loadtest).transform(X_loadtest) $ X_loadtest[0:5]
df = pd.read_csv('./data/FB.csv')
df2.to_csv("../../data/msft_modified.csv",index_label='date')
df_developer_blog=df_developer_blog.parse('Sheet1')
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ countries_df.head() $ df_new.groupby('country')['country'].count()
ufos_df3 = sqlContext.sql("select Count, Reports, substr(Reports, 1, 4) as year from ufo_sightings limit 10") $ ufos_df3.show(3)
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2016-12-31&end_date=2018-01-01',auth = ('User','Key')) $ r_limit = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?limit=1',auth = ('User','Key'))
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])  $ results = lm.fit()
pax_raw.head()
first_time = hr["realtime"][0] $ time_delta = first_time - datetime.datetime.now() $ print(time_delta.days)
df_variables = df_variables.loc[~(df_variables["_merge"] == "left_only")] $ df_variables = df_variables.loc[:,df_variables.columns != "_merge"] $ print("Data entries variables: " + str(len(df_variables.index)))
X_predict = recent.iloc[:, 1:8].as_matrix() $ y = regr.predict(X_predict) $ recent["Predictions"] = pd.DataFrame(y) $ ordered = recent.sort_values("Predictions", ascending = False)
yhat = LR.predict(X_test) $ yhat
exported_pipeline.named_steps['gradientboostingregressor'].feature_importances_
new_df = twitter_df.ix['2018-04-01':'2018-07-16']
df_passengers.head(5)
print ("After renaming the rows and columns:") $ df1.rename(columns={'col1' : 'c1', 'col2' : 'c2'}, $ index = {0 : 'apple', 1 : 'banana', 2 : 'durian'})
converted.head()
df_chdesc = pd.read_csv('http://chilp.it/ad0b6e4') $ print 'Marketing Channels: ', df_chdesc['marketing_channel'].unique(), '\n' $ print 'DataFrame df_chdesc: ', df_chdesc.shape $ df_chdesc
today = dt.date.today
metrics.accuracy_score(y_holdout.iloc[:,2], lr.predict(X_holdout))
artists_info[0].keys()
conn_aws = engine.connect()
mask=df.isnull() $ df=df.drop(range(390,398),axis=0) $ df=df.drop('Md',axis=1) $
table_info = con.execute('PRAGMA table_info(samples);').fetchall() $ table_info
eug_cg_counts.unstack(fill_value=0).plot(kind='bar', stacked=True);
data['version'].value_counts().sort_index().plot(kind='bar', figsize=[25, 5])
df_new = df_new.drop('CA', axis=1)
daily_averages['2014-10-2':'2014-10-7'].head()
df.info(memory_usage='deep')
gc = gspread.authorize(credentials) $ dashboard = gc.open("Tracking NBA Prediction Models").worksheet("Logistic Model")
print('The RSS of predicting Prices based on Square Feet is : {}'. $       format(get_residual_sum_of_squares(train_data['sqft_living'], train_data['price'], sqft_intercept, sqft_slope)))
pax_raw = pax_raw.merge(keep_days, on=['seqn', 'paxday'], how='inner')
session.query(func.count(Station.station)).all()
autos = pd.read_csv('../data/ebay-autos.csv', encoding='Latin-1') $ autos.info() $ autos.head()
data.info()
merged.isnull().any()
np.all(x < 8, axis=1)
stock_return.plot(grid = True).axhline(y = 1, color = "black", lw = 2)
master_list.sort_values(by='Count', ascending=True).head(10)
tvec_df.head(25)
client = MongoClient(port=12345) # this is the port set by the SSH tunnel $ db = client.research_papers $ db.collection_names()
pd.read_pickle('data/wx/tmy3/proc/703950.pkl', compression='bz2').head()
(dummies_df.keys())
words_hash_sp = [term for term in words_sp if term.startswith('#')] $ corpus_tweets_streamed_profile.append(('hashtags', len(words_hash_sp))) # update corpus comparison $ print('List and total number of hashtags: ', len(words_hash_sp)) #, set(terms_hash_stream))
print("The probability of individual in the control group converting is: {}".format(df2[df2['group'] == 'control']['converted'].mean()))
df = DataFrame(randn(len(rng), 3), rng, ['X', 'Y', 'Z']) $ df.head()
print("End Time (If only it will ever end): ") $ str(datetime.now())
data['processing_time'].describe() $
tagged_df.head()
X_new = new.title_author $ pipe.fit(X, y) $ new_pred_prob = pipe.predict_proba(X_new)[:, 1] $ metrics.roc_auc_score(new.popular, new_pred_prob)
np.exp(0.0507)
cur = conn.cursor() $ cur.execute('ALTER TABLE actor DROP COLUMN IF EXISTS middle_name;') $ df = pd.read_sql('SELECT * FROM actor', con=conn) $ df.head()
autos=autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).sort_index(ascending=True).head(10)
master_output_variables = S.modeloutput_obj.read_master_file()
list_of_files = [i for i in os.listdir() $              if i.endswith(".txt")  ] #and  i.startswith("MOAF disc_times_pay") $ list_of_files.sort(reverse = True) $ print(list_of_files) $ list_of_files[0].split('.txt')[0].split('-')[3]
df_image = pd.read_csv('image-predictions.tsv',sep='\t')
df_tx_claims.groupby(['Hospital', 'Claim Type']).agg({'Percent Spending Hospital': [np.mean, np.max]})
final_rankings = rt[rt.games_played>30].sort_values(['season', "games_played"]) $ final_rankings.head()
plt.figure(figsize=(10,4)) $ plt.plot(Year2_df.Month, Year2_df.Sales_in_CAD, 'bo', Year1_df.Month, Year1_df.Sales_in_CAD, 'ko', $          Year3_df.Month, Year3_df.Sales_in_CAD, 'ro') $
db.summarize(load_buf_array)[:]
noaa_data[(noaa_data.index >= '2018-05-27') & (noaa_data.index < '2018-06-03')].loc[:,'AIR_TEMPERATURE']
plt.rcParams['figure.figsize'] = (15, 5) $ timezones.plot(kind='bar') $ plt.xlabel('Timezones') $ plt.ylabel('Tweet Count') $ plt.title('Top 10 Timezones tweeting about #TomPetty')
len(final_data.item.unique())
df_meta.info()
obj.value_counts()
r1 = requests.get("https://API_KEY@www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31")
load2017['timestamp'] = load2017.apply(lambda r : pd.datetime.combine(r['date'],r['hour']),1) $ load2017['timestamp'].head()
QUIDS = pd.read_table("qids01.txt", skiprows = [1])
data.occupation.unique()
ideas['Time'] = pd.to_datetime(ideas['Time'],errors='coerce') $ stocks['Date'] = pd.to_datetime(stocks['Date'],errors='coerce')
precip = session.query(Measurement.date, Measurement.prcp).group_by(Measurement.date).\ $                     having(Measurement.date.like('2016%')).all() $ precip $
cov=np.cov(x,y) $ print(cov[0][1])
def train_classifier(X_train, y_train): $     clf = OneVsRestClassifier(LogisticRegression(penalty='l2', C=5)) $     clf.fit(X_train,y_train) $     return clf
df.loc[(df["date"].dt.hour > 6) & (df["date"].dt.hour < 19), 'day'] = 1 #this is day $ df.loc[(df["date"].dt.hour <= 6) | (df["date"].dt.hour >= 19), 'day'] = 0 #this is night
grouped = writers.groupby('Country') $ grouped.first() $
learner.save('lm_last_ft')
df.drop([5,12,23,56], axis = 0).head(13)
elon.nlp_text = elon.nlp_text + ' ' $ elon = elon.groupby('date')['nlp_text'].sum() $ elon = pd.DataFrame(elon) $ elon.columns = ['elon_tweet']
twitter_archive_enhanced.info()
mydata.plot(figsize =(15 ,6)) $ plt.show()
bkk_lat = 13.685780 $ bkk_long = 100.484605 $ diff_wid = 0.1/grid_size
go_no_go = 1 #NO_GO - ie do NOT run all the long run-time items.  Note that some of these long $
seen_click_read_action = pd.merge(seen_click_read, new_action, how='left', left_on=['article_id','project_id','user_id','user_type'], right_on = ['article_id','project_id','user_id','user_type']) $ print(seen_click_read_action.shape) $ seen_click_read_action.head()
df1.info()
old_samp = df2.sample(df2.shape[0], replace=True) $ old_page_converted = new_samp.query('landing_page == "old_page"').converted.mean()
diffs_evening =np.array(diffs_evening)
json_df['post_patch'].value_counts()
corrplot(rr.loc[start_date:].corr().round(1), annot=True) $ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date) $ plt.savefig('output/corr-fund_top10-monthly_largesize.png', dpi=750) $ plt.savefig('output/corr-fund_top10-monthly_smallsize.png') $ plt.show()
from sklearn.model_selection import train_test_split
speakers.set_index('id',drop = True,inplace = True)
type(ts.index)
(ggplot(all_lum_binned.query("lum>0&subject=='VP3'"),aes(x="td",y="gy"))+geom_smooth(method='loess'))+facet_wrap("~eyetracker")+xlim(-1,4)
zip_borough_filename = './data/zip_borough.csv' $ zip_borough = pd.read_csv(zip_borough_filename, dtype=str) $ data_2017_subset = data_2017_subset.merge(zip_borough, how='left', on=['incident_zip']) $ data_2017_subset['borough'] = data_2017_subset.apply(lambda x: x['zip_borough'] if x['borough'] == 'Unspecified' else x['borough'], axis=1)
for x in range(0,9): $         low_temps[x] = low_temps[x].drop(low_temps[x][(low_temps[x]['Low Temperature'] < 10.28) $                                                       | (low_temps[x]['Low Temperature'] > 10.32)].index) $         print (low_temps[x].head(1))
text = "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?" $ text = re.sub(r'[^a-zA-Z0-9]', ' ', text.lower()) $ words = text.split() $ print(words)
twitter_df_clean['source'].unique()
from datetime import datetime $ date = datetime.strptime(logins.login_time.iloc[0], '%Y-%m-%d %H:%M:%S') $ print(type(date), date.weekday)
ActiveStations = session.query(Measurement.station,func.count(Measurement.station)). \ $                  group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $ ActiveStations
pd.value_counts(ac['Sector']).head()
df_archive_clean = df_archive_clean.sort_values('tweet_id', ascending = True)
calls.head().T
print(df["created_at"].min()) $ print(df["created_at"].max())
abcd = pd.read_csv('Completed sepsis pass.csv') $ abcd.head()
from google.colab import files $ files.download("iou_grid4_submission.csv")
X_test_dtm.head()
MAX_DELAY_S = 60 * 60 * 96.0 $ HOUR_IN_S = 60 * 60.0 $ CHANNELS = ['release', 'beta', 'aurora', 'nightly']
newdf.head()
chromosomes_list = [str(i) for i in range(1, 23)] + ['X', 'Y', 'MT'] # list comprehensions $ gene_df = gene_df[gene_df['seqid'].isin(chromosomes_list)] $ chromosome_gene_count = gene_df.groupby('seqid').count().iloc[:, 0].sort_values().iloc[::-1] $ chromosome_gene_count
submit = pd.DataFrame({'id':id_test,'pred':p})
df_uniname.rate.hist() $ plt.title('one-word name users (rate)') $ plt.show()
p_old = df2[df2['landing_page'] == "old_page"]['converted'].mean() $ p_old
ser5 = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e']) $ ser5
def check_duplicate_rows(df, onColumns): $     duplicate = df.duplicated(subset = onColumns) $     return duplicate[duplicate == True] $ check_duplicate_rows(complete_wind_df, ['DateTime']) $ check_duplicate_rows()
X_train = ss.fit_transform(X_train) $ X_test = ss.fit_transform(X_test)
twitter_archive.sample(4)
data_scrapped.to_csv('data_scraped.csv', index = False, encoding='utf-8')
likes = data[data['Likes'] == likes_max].index[0] $ print(likes) $ retweets  = data[data.RTs == retweet_max].index[0] $ print(retweets)
page = re=json.load(urllib2.urlopen(url)) $ page = pd.DataFrame.from_dict(page)
df.to_html('table.html')
plt.figure(figsize=(10,10)) $ plt.scatter(df.Longitude, df.Latitude, alpha=0.2, s=0.4) $ plt.axis('off') $ plt.show()
norm.ppf(1-(0.05))
device.get_sensors('gas')
df.query('group == "treatment" and landing_page != "new_page"').shape[0]+df.query('group != "treatment" and landing_page == "new_page"').shape[0]
print("The most susceptible is patient {} with a total of {} hours extra sleep".format( $     ordered_by_susceptibility.index[0], ordered_by_susceptibility.iloc[0]))
StockData.head()
pd.Period('2011-01')
nan_ind = np.argwhere(np.isnan(w)) $ refl_band = sercb56 $ refl = copy.copy(sercRefl) $ metadata = copy.copy(sercRefl_md)
df.query('group=="treatment" and landing_page != "new_page" or group=="control" and landing_page=="new_page"').count()
type(t2.tweet_id.iloc[2])
df = df[df.userTimezone.notnull()] $ len(df)
df.apply(lambda x: x.max() - x.min())
read_twittertoken <- function() { $     readRDS(Sys.getenv("TWITTER_PAT")) $ } $ read_twittertoken()
len(genre_vectors.columns)
df['zip'].replace(regex=True,inplace=True,to_replace=r'\D',value=r'')
o = pd.read_sql_query(QUERY, conn) $ o
lr_fit.summary()
pitcherFirst = input("Please enter a pitcher's first name: ") $ pitcherLast = input("Please enter a pitcher's last name: ") $ pid = pyb.playerid_lookup(pitcherLast,pitcherFirst).iloc[0]['key_mlbam'] $ pid $
train_y = total_y $ print np.asarray(train_y).shape $ train_y[-2:]
ideas.columns  # Inspecting progress
attr_bench = 'prob_HDWPSRRating' $ df_result_payout = compute_simple_payout(dfX_hist, attr_model=attr_bench, ascending=False, bet_amount=1.0) $ advantage_HDWPSRRating = compute_advantage(df_result_payout)
raw_optim_etf_weights = helper.solve_qp(xtx.values, xty.values) $ raw_optim_etf_weights_per_date = np.tile(raw_optim_etf_weights, (len(returns.columns), 1)) $ optim_etf_weights = pd.DataFrame(raw_optim_etf_weights_per_date.T, returns.index, returns.columns)
import pandas $ from pandas.io import gbq $ df = gbq.read_gbq(query=query, dialect='standard', project_id=os.environ['PROJECT'], verbose=False) $ df.head()
cvec.fit(X_train) $ X_train_matrix = cvec.transform(X_train) $ print(X_train_matrix[:5])
ss = StandardScaler() $ X = ss.fit_transform(X) $ type(X) $ X
boro_counts = restaurants['BORO'].value_counts() $ boro_counts
city_eco["economy"].cat.categories
p_converted_ca = df_new[df_new.country == 'CA'].converted.mean() $ p_converted_us = df_new[df_new.country == 'US'].converted.mean() $ p_converted_uk = df_new[df_new.country == 'UK'].converted.mean() $ print("CA: {0}, US: {1}, UK: {2}".format(p_converted_ca, p_converted_us, p_converted_uk))
station_availability_df.loc[station_availability_df['status_key']==1,'status_value'] = "In Service" $ station_availability_df.loc[station_availability_df['status_key']==3,'status_value'] = "Not In Service" $ station_availability_df.status_value.value_counts()
iris = pd.read_csv('data/iris.csv') $ print(iris.shape)
train['PassengerId'].describe()
hdf['Age'].mean(level=[0, 1]).head()
reddit_comments_data.groupby('edited').count().orderBy('count', ascending = False).show(100, truncate = False)
new_page_converted = np.random.binomial(nnew, p = pnew)
old_page_converted = np.random.binomial(1, p_null, n_old) $ old_page_converted.mean()
noise_graf.head(2)
to_be_predicted_Day2 = 52.31714025 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
newping_shutdown = deduped_docid.filter(lambda p: p.get("payload/reason") == "shutdown")
bagofwords = countvec.fit_transform(train_df['desc']) $ tfidfdata = tfidfvec.fit_transform(train_df['desc'])
bet_under  = [x[1] > .6 for x in pred_probas_under]
scr_churned_bool = pd.Series([USER_PLANS_df.loc[uid]['status'][np.argmax(SCR_PLANS_df.loc[uid]['scns_created'])] =='canceled' for uid in SCR_PLANS_df.index],index=SCR_PLANS_df.index)
len(df2[df2['landing_page']=='new_page'].user_id.unique())/len(df2['user_id'].unique())
import datetime $ import pandas as pd $ speeches_df3['month'] = [datetime.datetime.strptime(month, '%B').strftime('%m') for month in speeches_df3['month']] $ speeches_df3['date'] = pd.to_datetime(speeches_df3[['year','month','day']])
df_userid = pd.DataFrame({"UserID":users["UserID"]}) $ df_Tran = pd.DataFrame({"ProductID":products["ProductID"]}) $ df_userid['Key'] = 1 $ df_Tran['Key'] = 1
df_gt[[u'Equipment Type', u'Equipment Sap Code']].head()
print('Click the + symbol above to add new cells, and the up and down arrows to re-order cells.')
authors = EQCC( git_index) $ previous_month_date = end_date - timedelta(days=31) $ authors.since(start=previous_month_date).until(end=end_date) $ authors.get_terms(field="author_name") $ print(buckets_to_df(authors.fetch_aggregation_results()['aggregations'][str(authors.parent_id-1)]['buckets']))
information_ratio = pd.Series([40,50], ['Manager_A', 'Manager_B']) $ print(information_ratio)
active_stations = session.query(func.distinct(Measurement.station), func.count(Measurement.station)).\ $                 group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $ active_stations
r,q,p = sm.tsa.acf(resid_713.values.squeeze(), qstat=True) $ data = np.c_[range(1,41), r[1:], q, p] $ table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"]) $ print(table.set_index('lag'))
%sql SELECT twitter.tag_text FROM twitter \ $ WHERE twitter.key_word LIKE 'Roger Federer%'
def calcPercentStockReturns(stockPrices = np.empty(0), n_days=1):   $     n_day_percent_returns = np.empty(len(stockPrices) - n_days) $     for i in range(len(stockPrices) - n_days):     #n-day training return; use "-n" to omit oldest n days that won't have a pair with which to compare $         n_day_percent_returns[i] = (float(stockPrices[i]) - float(stockPrices[i+n_days])) / float(stockPrices[i+n_days]) *100 $     return n_day_percent_returns
pd.to_datetime("2018/04/23")
new_user = len(df.query("group == 'treatment'")) $ users=df.shape[0] $ new_user_p = new_user/users $ print(new_user_p)
a = pd.DataFrame(DataAPI.schema.get_schema("factor")).T $ factors = a.sort_values(['level']).index.tolist()
pd.datetime(2012, 5, 1)
!wget https://pjreddie.com/media/files/yolov3.weights $
autos['price'].value_counts().sort_index().head(20)
df2.quarter_opened.value_counts()
Y_tweet.idxmax()
pc = pd.DataFrame(tt1.groupby('ZIPCODE').size()) $ pc.columns=['count'] $ pc_gt100 = pc[pc['count']>100].sort('count', ascending = False) $ pc_gt100
old_page_converted = np.random.binomial(1,p_old, n_old); $ old_page_converted
flights2.loc[[(1949,"December"),(1950,"January")]]
terms = tf.get_feature_names()
loans.shape
y3 = df3['converted'] $ X3 = df3[['intercept', 'ab_page']] $ X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.20, random_state=0)
archive_df.name.value_counts()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').query('converted == 1').count()[0] $ convert_new = df2.query('landing_page == "new_page"').query('converted == 1').count()[0] 
y.name = "huhu" $ y = y.rename("jsdfjkdsfhsdfhdsfs") $ y
df_ec2 = df_cols[df_cols.ProductName == 'Amazon Elastic Compute Cloud'] # narrow down to EC2 charges $ df_ec2_instance = df_ec2[df_ec2.UsageType.str.contains('BoxUsage:')] #narrow down to instance charges $ df_tte = df_ec2_instance[df_ec2_instance['LinkedAccountName'] == target_account] $
black_scholes_call_value_six_months = black_scholes_call_value(S_, K, r, 0.5, vol) $ black_scholes_call_value_three_months = black_scholes_call_value(S_, K, r, 0.25, vol) $ black_scholes_call_value_one_month = black_scholes_call_value(S_, K, r, 1.0/12.0, vol) $ call_payoff_at_expiration = call_payoff(S_, K)
df2.head()
def tokenizer(text): # create a tokenizer function $     return [tok for tok in spacy_en.tokenizer(text)]
present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm), $                               name="present_error_raw") $ present_error = tf.reshape(present_error_raw, shape=(-1, 10), $                            name="present_error")
w.index_handler.booleans['step_0'][subset_uuid]['step_1']['step_2']['SE654470-222700'].keys()
tag_df.head()
run txt2pdf.py -o"2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2013 Percentiles.pdf" "2018-06-12-0815 MAYO CLINIC HOSPITAL ROCHESTER - 2013 Percentiles.txt"
df.reset_index(inplace=True, drop=True)
df_test_user = df_users.iloc[0, :] $ df_test_user
print("Percentage of positive tweets: {}%".format(len(pos_tweets) * 100/len(twitter_data['OriginalTweets']))) $ print("Percentage neutral tweets: {}%".format(len(neu_tweets) * 100 / len(twitter_data['OriginalTweets']))) $ print("Percentage negative tweets: {}%".format(len(neg_tweets) * 100 / len(twitter_data['OriginalTweets'])))
df1.describe()
fin_pivot_table_tr = pd.pivot_table(fin_df, values = 'totalRevenue', index = ['symbol'], aggfunc = np.mean) $ fin_pivot_table_tr = fin_pivot_table_tr.rename(index=str, columns={"totalRevenue": "Avg Total Revenue"}) $ fin_pivot_table_tr.sort_values(by = ['Avg Total Revenue'], ascending = False)
predictions = model.predict(x_test, batch_size=1024, verbose=1)[0:len(X_test)] $ prediction_df = pd.DataFrame(predictions, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]) $ combined_df = new_comments_df.join(prediction_df) # join the comment dataframe with the results dataframe $ combined_df.head(15)
df_new.country.unique()
df_new = df2.query('landing_page == "new_page"') $ newpage_proportion = len(df_new)/len(df2) $ newpage_proportion
gc.collect()
merged_copy = df_merged.copy() $ merged_copy.drop([0, 1, 8], inplace = True) $ regression2 = np.corrcoef(merged_copy['repair_time'], merged_copy['2017_homeAverage']) $ regression2
assert len(target_docs) == 200000, 'target_docs should be truncated to the first 200k rows to use the cached model.' $ fname = get_file(fname='kdd_lm_v2.h5', origin='https://storage.googleapis.com/kdd-seq2seq-2018/kdd_lm_v2.h5', ) $ model = load_model(fname)
tweet_archive_clean['tweet_id'].isin(tweet_df_clean['tweet_id']).value_counts()
df_ab_raw.shape
debate_evolution.head()
df.shape
category = pb.Category(commons_site, 'Category:Files by User:Rodelar') $ category.categoryinfo
git_log.index = git_log['timestamp']
df.index.weekday_name
df.isnull().any().any()
retweet_sum.sample(3)
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
resolved_links, errors = ux.multithread_expand(short_urls, $                                                chunksize=1280, $                                                n_workers=64, $                                                cache_file='tmp.json', $                                                return_errors=True)
prices = aapl.loc[dates] $ prices
new_page_converted = np.random.choice([1,0],size = len(new),p = [convert,1-convert])
ozzy.setBuddy(filou)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(therapist_duration, params=params, lags=30, alpha=0.05, \ $     title='Weekly Therapists Hours Partial Autocorrelation')
load2017['actual'] = load2017['actual'].astype(int)
station_count = session.query(Station.id).count() $ station_count
np.random.seed(1) $ s1 = pd.Series(np.random.randn(100)) $ s1
f = sql.udf.register("fadd", lambda x: (np.array(x[3]) * 3.1).tolist(), ArrayType(FloatType())) $ fagg = sql.udf.register("fagg", lambda x,y: (np.array(x[3]) + np.array(y[3])).tolist(), ArrayType(FloatType()))
projects_csv.count()
payments_total_yrs.shape
print 'Percentage of amount for unknown (YY) state : {:.2f}'.format(100*df[df.state == 'YY'].amount.sum()/df.amount.sum())
df_total.to_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_Oncology\01_Oncology_EMR_cleaned_with_dummies.csv', index=False)
df_kws.plot(kind='area')
adds["YBP sub-account"].replace(195099, 590099, inplace= True) $ adds
plt.plot(d.variables['time'], d.variables['depth']) $ plt.gca().invert_yaxis() $ plt.show()
lm = sm.formula.ols(formula='Sales_in_CAD ~ Date_of_Order', data = New_df).fit() $ lm.params
payments_all_yrs = pd.read_csv('Payments with DRG percentiles. (ID, 25,50,75,Number DRGs).csv',index_col=0) $ payments_all_yrs = payments_all_yrs.sort_values(['disc_times_pay'], ascending=[False]) $ payments_all_yrs = payments_all_yrs.reset_index(drop=True) $ payments_all_yrs.head()
from sklearn.preprocessing import StandardScaler $ sc = StandardScaler()  # default values $ psy[psy.columns] = sc.fit_transform(psy[psy.columns])
session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\ $     order_by(desc(func.count(Measurement.station))).all()
tbl2 = pd.concat([amzn,gspc], axis=1, join='outer')[1:] $ tbl2.columns = ['AMZNr','GSPCr'] $ tbl2['250beta'] = tbl2['GSPCr'].rolling(window=250).cov(tbl2['AMZNr'])/tbl2['GSPCr'].rolling(window=250).var() $ tbl2['250alpha'] = tbl2['AMZNr'].rolling(window=250).mean() - tbl2['250beta'] * tbl2['GSPCr'].rolling(window=250).mean() $ tbl2.tail()
gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_
for df in (train,test): $     df.loc[df.CompetitionDaysOpen<0, "CompetitionDaysOpen"] = 0 $     df.loc[df.CompetitionOpenSinceYear<1990, "CompetitionDaysOpen"] = 0
weekday = 2 $ transit_df_byday = transit_df_merged.loc[transit_df_merged.index.weekday==weekday] $ transit_df_byday.info() $ transit_df_byday.head()
pd.Timestamp('2017-03-31')
to_be_predicted_Day2 = 34.15678836 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
salary_df2 = pd.read_csv('../Data/playersalary.csv')
cfs=ca.read_amplicon('data/chronic-fatigue-syndrome.biom', $                      'data/chronic-fatigue-syndrome.sample.txt', $                      normalize=10000,min_reads=1000)
airbnb_df.memory_usage(deep=True)
n_old = len(df2.query("landing_page == 'old_page'")) $ print("old_page count: {}".format(n_old))
df_geo_unique_asn.shape
hawaii_station_df = pd.DataFrame(results_station[:], columns=['Station', 'StationName', 'Latitude','Longitute','Elevation']) $ hawaii_station_df.set_index('Station', inplace=True, ) $ hawaii_station_df 
writer = pd.ExcelWriter('NaN_table.xlsx') $ for res_key, df in nan_tables.items(): $     df.to_excel(writer, res_key) $ writer.save()
prediction = naive_model.predict(df_test) $ indic = df_test['Id'] $ res = pd.DataFrame(indic) $ res['Sales'] = prediction $ res.to_csv('data/submission_naive.csv', index = False)
X_d.head(2)
score_100 = score[score["score"] == 100] $ score_100.shape[0]
print("Age:-",result_set.age,"workclass:-",result_set.workclass)
df.columns
sqlContext.sql("SELECT id FROM my_data WHERE Height > 161").collect()
s[s.ID < 9].groupby(['group','ID']).size().unstack().plot.bar(stacked=True) $ plt.xlabel('Group') $ plt.ylabel('Number of people targeted') $ plt.legend(title='Sms\'s') $ plt.title('Frequency of sms each person received');
live = preg[preg.outcome == 1]
df2 = df2.drop(df2[(df2.user_id == 773192) & (df2['timestamp'] == '2017-01-09 05:37:58.781806')].index)
sim_ret = pd.DataFrame(sigma*np.random.randn(ndays,nscen)+r, index=dates) $ sim_ret
df2.query('converted == "1"').count()[0]/df2.shape[0]
positive_topic_dataframe.to_csv("positive_topics.csv")
for column_name in data.columns: $     display(column_name.split(SEPARATOR))
from sklearn.metrics import confusion_matrix $ confusion_matrix = confusion_matrix(y_test, y_pred_rf) $ print(confusion_matrix)
new_c2 = float(df2.query('converted == 1 and  landing_page == "new_page"')['user_id'].nunique()) $ new_users2 =float(df2.query('landing_page == "new_page"')['user_id'].nunique()) $ print(" Given that an individual was in new landing page, the probability they converted is {0:.2%}".format(new_c2 /new_users2))
revisions = [revision for revision in page.revisions(reverse=True, content=True)] $ revisions[0]
breed_predict_df.head(1)
california['bins'] = pd.qcut(california['FIRE_SIZE'],4) #used .qcut to cut the fire sizes from california into $ california['bins'].value_counts()                       #quartiles
df[['location_id', 'prev_location_id']].head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\olympics.dta" $ df = pd.read_stata(path) $ df.head(5)
users[(users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))) & $      (users['DaysActive'] < 10)]['DaysActive'].plot(kind = 'hist') $ plt.title('Histogram of Number of Active Days \n for Users that Left Permanently') $ plt.xlim((0,9)) $ plt.xlabel('Days Active')
clusters = np.array(km.labels_)
Car.wheels
stuttgart.info()
department_df_sub = department_df.loc[:, ["People", "Revenue"]] $ revenue = department_df["Revenue"]
pd.Series([30, 35, 40], index=['2015 Sales', '2016 Sales', '2017 Sales'], name='Product A')
contractor_clean = contractor.copy()
print(df['lang'].value_counts()) $ df['lang'].value_counts().plot(kind='bar')
full_clean_df.groupby('type')['rating_numerator'].mean().plot(kind='bar') $ plt.ylabel('Dog Rating') $ plt.xlabel('Dog Type') $ plt.title("Who's the Goodest Boy?")
df_code_activations.head(3)
ec550 = adm.where(in_domain(adm), drop=True).ec550 $ ec550
x = Gauss_Option(96, 100, msft_mu, msft_sigma, 20, .05) $ x.payoff_fx = x.put_payoff # set the proper payoff for a put option $ x.simulate_many(ap = 0.01, rp = .01, ns = 100000) 
news_sentiment_df.to_csv("twitter_news_org_sentiment.csv", encoding="utf-8", index=False)
station_availability_df.to_csv('station_availability_df')
contrib_state.amount = contrib_state.amount.astype(int)
df['comments'].value_counts()/len(df['comments'])
df.plot.hist()
len(train[train.id.isin(session.id.unique())].id.unique())
shifted_forward = msftAC.shift(1) $ shifted_forward[:5]
ingreso_rng_dict = {v: k for k, v in enumerate(df["RNG_INGRESO_BRUTO"].unique().tolist())} $ ingreso_rng_dict
INQ2017.Create_Date.dt.month.value_counts().sort_index()
Manhattan_gdf = newYork_gdf[newYork_gdf.boro_name == 'Manhattan'] $ Manhattan_gdf.crs = {'init': 'epsg:4326'}
df_master.p1.value_counts() $ df_master.p2.value_counts() $ df_master.p3.value_counts()
loan_fundings1[(loan_fundings1.fk_loan==350)&(loan_fundings1.fk_user==88)].investment_yield
result_df.columns
%run twitter_creds.py
tweets_clean.to_csv('tweets_clean_final.csv', index = False)
articles.head()
data_df.clean_desc[23]
round(len(df_twitter[df_twitter.dog_label == 'pupper']) / len(df_twitter.dog_label), 2)
df_new['week1'] = np.where(df_new['timestamp'] <= '2017-01-09', 1, 0) $ df_new['week2'] = np.where((df_new['timestamp'] > '2017-01-09') & (df_new['timestamp'] <= '2017-01-16'), 1, 0) $ df_new['week3'] = np.where(df_new['timestamp'] > '2017-01-16', 1, 0)
plot_data = options_data[options_data['IMP_VOL'] > 0.0]
btc_forum_df['price_change'] = 0.0 $ btc_forum_df['mins_before'] = 0.0 $ btc_forum_df['mins_after'] = 0.0
percipitation_year = session.query(Measurements.date,func.avg(Measurements.prcp)) \ $              .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \ $              .group_by(Measurements.date).all()
google_after_april.tail()
students.sort_index(axis=0)
driver.title
df3_new.groupby('country')['converted'].mean()
df2.head()
data2010 = final.query("year == 2010 & ages == 'total'") $ data2010.head()
train.columns
actual_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ actual_diff $ (actual_diff < p_diffs).mean()
wrd_clean.query('name == "an"')
print(f"Fit2 shape: {fit2.shape}, Fit2 non-nulls: {fit2.nnz}") $ print(f"Non-null fraction of total: {'{:.10f}'.format(fit2.nnz/(fit2.shape[0] * fit2.shape[1]))}")
TestData_ForLogistic.to_csv('test_logistic.csv')
nb = MultinomialNB() $ nb.fit(X_train_dtm, y_train) $ y_pred_class = nb.predict(X_test_dtm)
liberiaCasesSuspected = liberiaFullDf.loc['New Case/s (Suspected)'] $ liberiaCasesProbable = liberiaFullDf.loc['New Case/s (Probable)'] $ liberiaCasesConfirmed = liberiaFullDf.loc['New case/s (confirmed)'] $ liberiaCasesSuspected.info()
h1 = qb.History(360, Resolution.Daily) $ h1;
alpha_range = 10.**np.arange(-2, 3) $ alpha_range
df_twitter_archive_master.groupby(['p1']).mean()['retweet_count'].sort_values(ascending=False).head(5)
pd.DataFrame(data, columns=['joe', 'dora'])
path = './' $ teamname = 'MadeInJerusalem' $ out_name = path + teamname + '_submission.csv'
from sqlalchemy import func $ num_stations = session.query(Stations.station).group_by(Stations.station).count()
(np.array(p_diffs)>diff_ab_data).mean()
normalizedDf = finalDf $ normalizedDf = normalizedDf.drop('kmLabels', axis=1);
datetime.datetime.fromtimestamp(0)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.csv" $ df = pd.read_table(path, sep =',', na_values=[' ?']) $ df.head(5)
def date(x): $     return x.split("T")[0] $ def time(x): $     return x.split('T')[1][:-1]
results = model.fit() $ results.summary()
exportOI['avg_distance'] = exportOI['event.longSum_sum_distance']/exportOI['event.longSum_total_records']
states.set_index(['day', 'location'], append=True)
frames = [dummy_var_df, data['cat'], data['funny'], data['love'], dfcv] $ features_df = pd.concat(frames, axis=1, ignore_index=True )
trump_month_distri.plot(kind='bar', figsize=(10,5), rot= 45,title="# of Twitters of Donald Trump") $ plt.savefig('fig/trump_month.png');
donald_trump_tweets=tweets[tweets.text.str.lower().str.contains('donald trump|president trump')==True] $ donald_trump_tweets[:5]
smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name)['HyperParameterTuningJobStatus']
returns.corrwith(returns.IBM)
all_data_wide = all_data_long.pivot(columns='access', values='views') $ all_data_wide[:3]
import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline
s.values
rate_change=df_master[['date', 'retweet_count', 'favorite_count', 'rating_numerator', 'rating_denominator']].copy()
ctas = "create table icu_sensor_util as \ $ select * from icu_sensor_24 where dt_cancel = '' \ $ and CONCAT(cast(id_nda as VARCHAR),cast(TO_DATE(dt_deb) as VARCHAR)) in \ $ (select distinct(CONCAT(cast(id_nda as VARCHAR),j1)) from nda_j1_deces)" $ conn.execute(ctas)
df2.head()
train_labels = [reuters.categories(doc_id)for doc_id in doc_id_list if 'training' in doc_id]
countries_df = pd.read_csv('countries.csv') $ df_ab_cntry = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
weather_df.info()
import numpy as np $ df_test = df_raw_tweet[~msk] $ docs_test = tweet_test_data $ predicted = text_clf.predict(docs_test) $ np.mean(predicted == tweet_test_target) 
df4.info()
unique_users = df.user_id.unique().size $ unique_users
test.to_excel(r"C:\Users\A130893\notebooks\tests\HH_test.xlsx", index=False, encoding='utf-8')
pca.explained_variance_ratio_
s1.iloc[0]
df2[df2.duplicated(['user_id'],keep=False)]
df2['user_id'].drop_duplicates(keep = 'first',inplace = True) $
pd.PeriodIndex(start='2014-01', freq='3M', periods=4)
df.Opened  = pd.to_datetime(df.Opened , format="%m/%d/%Y %H:%M:%S %p") $ df.Closed  = pd.to_datetime(df.Closed , format="%m/%d/%Y %H:%M:%S %p") $ df.Updated = pd.to_datetime(df.Updated, format="%m/%d/%Y %H:%M:%S %p")
pizza_train_model.print_topics(num_topics = 5 ,num_words = 10)
for df in (joined, joined_test): $     for c in df.columns: $         if c.endswith('_y'): $             if c in df.columns: df.drop(c, inplace=True, axis=1)
samples_query.execute_sql('SELECT ID, Name, DOB, SSN FROM Sample.Person')
df2.drop(df2[df2.duplicated(['user_id'],keep=False)].index[0],inplace=True)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closTopBotDist']/0.25 $ df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['rowsToEdge'].astype(int)
conn.execute(sql) $
rain = session.query(Measurements.date, Measurements.prcp).\ $     filter(Measurements.date > last_year).\ $     order_by(Measurements.date).all()
df_session.shape
dominique = relevant_data[relevant_data['User Name'] == 'Dominique Luna'] $ df = dominique['Event Type Name'].value_counts() $ df_dominique = pd.Series.to_frame(df) $ df_dominique.columns = ['Count_Dominique'] $ df_dominique
g = sns.FacetGrid(data=dataset, col='rating') $ g.map(plt.hist, 'text length', bins=50) $
moving_average = close_ys.rolling(center=False,window=10).mean() $ moving_average
finals['type'].value_counts()
is_single = dfd.zones.str.lower().str.startswith('single') $ dfd.loc[is_single, 'zones'] = 'Single' $ dfd.loc[~is_single, 'zones'] = 'Multi' $ dfd.zones.value_counts()
t.converted.sum()/t_rows
images.info()
week3 = week2.rename(columns={21:'21'}) $ stocks = stocks.rename(columns={'Week 2':'Week 3','14':'21'}) $ week3 = pd.merge(stocks,week3,on=['21','Tickers']) $ week3.drop_duplicates(subset='Link',inplace=True)
expirations = options_frame['Expiration'].unique()[-5:] $ iv_multi = options_frame[options_frame['Expiration'].isin(expirations)] $ iv_multi_call = iv_multi[iv_multi['OptionType'] == 'call'] $ iv_pivoted = iv_multi_call[['DaysUntilExpiration', 'Strike', 'ImpliedVolatilityMid']].pivot(index='Strike', columns='DaysUntilExpiration', values='ImpliedVolatilityMid').dropna() $ iv_pivoted.plot() $
data = [trace] $ py.iplot(data, filename='line')
df_goog['Closed_Higher'] = pd.get_dummies(df_goog.Open > df_goog.Close) $ df_goog.head(2)
from lazy_helpers import * $ bcast_pool = sc.broadcast(LazyPool) $ bcast_pool.value
df=pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv", usecols=['Created Date','Closed Date','Agency','Complaint Type','Descriptor']) $ df.head()
for col in ['date_crawled', 'ad_created', 'last_seen']: $     autos[col] = autos[col].str[:10] $     autos[col] = autos[col].str.replace('-','').astype(int) $ autos.head()
ax2.plot(anomalies.index, anomalies.score, 'ko') $ fig
print(df_aggregate.index.is_unique)
dates = pd.date_range('2014-07-01', '2014-07-06') $ dates
from sklearn.metrics import roc_curve, roc_auc_score $ logPredprob= logreg.predict_proba(test.ix[:, test.columns != 'class']) $ logregFpr, logregTpr, rocThresh= roc_curve(test['class'],logPredprob[:,1])
dfMonth.set_index('Date').to_csv('PipelineInventorySales_Monthly.csv')
S_1dRichards = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_1dRichards.txt')
plans_set = set() $ [plans_set.add(plan) for plans_combination in np.unique(USER_PLANS_df['scns_array']) for plan in plans_combination ] $ plans_set
df2.drop([1899], inplace=True); $ df2.count()
thisWeek.head()
def remove_stop_words(words): $     for token in words: $         if token in stopwords.words('english'): $             words.remove(token) $     return words
print('The largest change between any two days is ' + str(max(max((stock - mini).abs()),max((maxi - stock).abs()))))
beijing.index = beijing['Date']
appendedFrames = noHandReliableData.append(extraRecordsForLag); $ appendedFrames.shape
serc_pixel_df = pd.DataFrame() $ serc_pixel_df['reflectance'] = sercRefl[500,500,:] $ serc_pixel_df['wavelengths'] = sercRefl_md['wavelength']
from datetime import datetime $ sdate = datetime.strptime("2016-10-23 21:50:00.000", "%Y-%m-%d %H:%M:%S.%f") $ print(sdate)
df_mes[df_mes['DOLocationID'].astype('int64') >= 266]
medals_data.head()
vio['new_date'] = pd.to_datetime(vio["date"], format='%Y%m%d', errors='ignore') $ vio['year']     = pd.to_datetime(vio["date"], format='%Y%m%d', errors='ignore').dt.year $ vio2016 = vio[vio['year']==2016] $ vio2016
tabulation.columns.values
dataA = pd.read_csv('data/dataPorUbicacion_Anios_tmin.csv', header=None)
Q96P20=Graph().parse(format='ttl', $                      data=turtle) $
log_mod=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results=log_mod.fit() $ results.summary2()
df_datecols = df_EMR_with_dummies.select_dtypes(include=['<M8[ns]']).drop(columns=['index_date', 'lookback_date']) $ df_datecols.shape
temp_df=df_small.groupby('order_num').sum().reset_index() $ temp_df.head()
contractor_clean=contractor_clean[contractor_clean['contractor_id'].isin([139,140,228,236,238]) & $     contractor_clean['contractor_version']!=1 ] $ contractor_clean=contractor_clean.loc[~contractor_clean.contractor_id.isin([373,374,378])]
timezones = DataSet['userTimezone'].value_counts()[:10] $ print(timezones)
df8 = pd.read_csv('2008.csv')
lagged.at_time('9:30')
df2['intercept'] = 1 $ df2[['ab_page', 'old_page']] = pd.get_dummies(df2['landing_page']) $ df2.drop(['old_page', 'landing_page', 'group'], axis = 1, inplace = True) $ df2.head()
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new])
f = open(data_path+"metadata.json","r") $ the_posts = json.load(f)
autos.columns #print all the existing column names
df['domain'].value_counts().head(20).plot(kind='bar') $ plt.xticks(rotation=70);
volt_prof_before['Bus']=volt_prof_before['Bus'].apply(lambda x:x.lower()) $ volt_prof_after['Bus'] =volt_prof_after['Bus'].apply(lambda x:x.lower())
data = pd.read_csv("datosSinNan.csv") $ test = pd.read_csv("datatestSinNan.csv")
query_date_2yearsago = dt.date.today() - dt.timedelta(days = 730) $ print(query_date_2yearsago)
candidates.election_date = pd.to_datetime(candidates.election_date)
train.describe()
result1 = -df1 * df2 / (df3 + df4) - df5 $ result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5') $ np.allclose(result1, result2)
df_adjusted.adjusted_numerator.mode(), df_adjusted.adjusted_numerator.describe()
sites_on_net = sites[sites['On Zayo Network Status'] != 'Not on Zayo Network'].groupby(['Account ID'])['Building ID'].count().reset_index().sort_values(by='Building ID', ascending=False)
ftfy.ftfy(u'facebook\u2019s')
autos['offer_type'].value_counts()
d5.columns = d5.columns.droplevel(level = 0) $ d5
dfSF.head()
periods = [pd.Period('2012-01'), pd.Period('2012-02'), pd.Period('2012-03')] $ periods
display(observations_ext_node[DATA].dropna().head(9))
df = pd.DataFrame(rng.rand(1000,3), columns=['A', 'B', 'C']) $ df.head()
autos.loc[max_price, :"price"]
ind = pd.read_csv('170418_3_cbg_w_foursquare_prep.csv')
from arcgis.features import FeatureLayer
hm_sub.shape
local.import_from_quilt("dsdb/processing_test")
best_model = h2o.get_model(gbm_grid_cart.model_ids[0]) $ best_model
data = res.json()   
import zipfile $ def unzipfile(filename,unzippath): $   with zipfile.ZipFile(filename,"r") as zip_ref: $     zip_ref.extractall(unzippath) $ print("data extracted")
rep = re.findall(r'[-]{2} ([\d]{4}-[\d]{2}-[\d]{2}) [a-z]{3} (.[\d]).+ = ([\d]+)', data) $ rep
import numpy as np $ from sklearn.linear_model import LinearRegression $ from sklearn import linear_model $ x = hours[['hour']] $ y = hours.start
autos = autos[autos["price"].between(1,351000)]
linkNYC.shape
fruits= pd.Series(data = [10, 6, 3,], index = ['apples', 'oranges', 'bananas']) $ fruits
wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)
np.exp(result_interact.params)
def top_n_tweet_day_plot(n,df): $     top_n_tweet_day = top_n_tweet_days(df,n) $     top_n_tweet_day.plot(kind='bar', x=top_n_tweet_day.index,y='Tweet_counts') $     return top_n_tweet_days
gMapAddrDat.testConnection()  ## uses default test record to just ensure connection is working
dfDay = dfDay.drop(dfDay[(dfDay['Sales Stage'] == 'Work without contract') & (dfDay['Likelihood of Win'] == 'Certain (100%)')].index) $ dfDay = dfDay[dfDay['Sales Stage'] != 'Potential']
f_ip_device_clicks.show(1)
DataAPI.write.update_index_contents(index_code="A", trading_days=trading_days, override=False, log=False)
df_goog['good_month'].describe()
df_goog.head()
for treaty in treaties.find({"reinsurer": "AIG"}): $     pprint.pprint(treaty)
autos["date_crawled"].str[:10].\ $ value_counts(normalize=True,dropna=False).\ $ sort_index(ascending=True)
driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[1]').send_keys('Avengers: Infinity War') $ driver.find_element_by_xpath('//*[@id="leftnav"]/li[2]/form/input[2]').click()
user_week_day_frequency = user_df["week_day"].value_counts() $ ax = plt.subplot(111) $ ax.bar(user_week_day_frequency.index, user_week_day_frequency.data) $ plt.show()
countries_df.info()
train.head()
countries_df.country.unique()
data_FCInspevnt_latest = data_FCInspevnt.loc[data_FCInspevnt['Inspection_number'] == 1] $ data_FCInspevnt_latest = data_FCInspevnt_latest.reset_index(drop=True) $ data_FCInspevnt_latest.head(15)
week26 = week25.rename(columns={182:'182'}) $ stocks = stocks.rename(columns={'Week 25':'Week 26','175':'182'}) $ week26 = pd.merge(stocks,week26,on=['182','Tickers']) $ week26.drop_duplicates(subset='Link',inplace=True)
df.drop(["urlname"], axis = 1, inplace=True) $ top_rsvp.drop(["urlname"], axis = 1, inplace=True) $ top_rsvp.head(5)
tweet_texts=[x.text for x in results] $ len(tweet_texts)==len(set(tweet_texts)) $ len(set(tweet_texts)) $ len(results) 
fig, ax = plt.subplots(1, 2, figsize=(14, 6)) $ pivoted.T[labels == 0].T.plot(legend=False, alpha=0.1, ax=ax[0]); $ pivoted.T[labels == 1].T.plot(legend=False, alpha=0.1, ax=ax[1]); $ ax[0].set_title('Purple Cluster') $ ax[1].set_title('Red Cluster')
All_tweet_data_v2.date_time=All_tweet_data_v2.date_time.str.replace(' \+0000','') $ All_tweet_data_v2.date_time=pd.to_datetime(All_tweet_data_v2.date_time)
g_geo.head()
stockdftest = stockdftest.sort(['Date'])
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK']]) $ lm.fit().summary2()
pres_df.rename(columns={'subject_count': 'subject_count_test'}, inplace=True) $ pres_df.head(2)
collection.write('AAPL', aapl[:-1], metadata={'source': 'Quandl'})
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head() $ df_new['country'].astype(str).value_counts()
y_predit = exported_pipeline.predict(X)
hist.drop('UserID',axis=1, inplace=True)
myindex = pd.date_range(end=pd.Timestamp.today(), normalize=True, periods=100, freq='B') $ P = 20+np.random.randn(100).cumsum()  #Make up some share prices. $ aapl = pd.Series(P, name="AAPL", index=myindex) $ aapl.tail()
df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2[['old_page', 'new_page']] = pd.get_dummies(df2['landing_page']) $ df2['intercept'] = 1
def create_dictionary(post_body): $     body_vectors_filtered=[post_to_filtered_word_vector(body) for body in tqdm(post_body)] $     dictionary = corpora.Dictionary(body_vectors_filtered) $     return dictionary,body_vectors_filtered
usage_ct = pd.crosstab(combined_df5['llpg_usage'],combined_df5['bin_label']) $ usage_ct['neg_pctg']=usage_ct[1]/usage_ct[0]*100 $ usage_ct.sort_values('neg_pctg',ascending=False)
old_page_converted=np.random.binomial(n=1,p=P_old,size=n_old) $ old_page_converted $
df_sp = df_data[df_data.CIDADE=='S.PAULO']
train_data['notRepairedDamage_int'] = train_data['notRepairedDamage'].apply(get_integer6) $ test_data['notRepairedDamage_int'] = test_data['notRepairedDamage'].apply(get_integer6) $ del train_data['notRepairedDamage'] $ del test_data['notRepairedDamage']
frame2['debt'] = np.arange(6.)
BASE_URL = 'https://orgdemo.blastmotion.com'
precipitation_df = pd.DataFrame(precipitation, columns=['date', 'prcp']) $ precipitation_df.set_index('date', inplace=True) $ precipitation_df.head()
tia['date'] = tia['date'].apply(lambda x: x[14:]) $ tia['date'][0]
df.info() # studying the dataframe 
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='two-sided') $ print (z_score, p_value)
df = pd.DataFrame({'ImageId': imageids, 'Label': predCategories}) $ print (df.shape) $ print (df.head()) $ print (df.tail())
text_classifier.set_step_params_by_name("text_char_ngrams", ngram_range =(3,4)) $ text_classifier.get_step_params_by_name("text_char_ngrams")
twitter_data.head(1)
len(users) $ users.drop_duplicates(subset='screenName', keep='last', inplace=True) $ users.reset_index(inplace=True, drop=True) $ len(users)
print(pd.Series.unique(df1['Area'])) $ print(pd.Series.unique(df1['Variable Name'])) $ print(pd.Series.unique(df1['Year']))
df3.head()
df.groupby('converted').nunique()['user_id']/df.nunique()['user_id']
from sklearn.ensemble import RandomForestClassifier $ rf = RandomForestClassifier(n_estimators=45) $ rf.fit(X_train, y_train)
hp.find_device('FL03001562')
print(pd.value_counts(train_df['ip'])[:20])
np.mean(score)
from sklearn.linear_model import LogisticRegression $ from sklearn import metrics $ from sklearn.cross_validation import train_test_split
cachedf = dir2df(cachedir, fnpat='\.ifc$', addcols=['dirname', 'barename'])
data['Race'].value_counts()
response = requests.get(url) $ soup = BeautifulSoup(response.text, 'html.parser') $ print(soup.prettify())
festivals_clean.info()
active_raw_sample_sizes = active_num_authors_by_project.withColumn( $     "sample_size_1", $     compute_num_required_sample_1("author_count")).persist()
no_lined_up = df.query('(group == "treatment" and landing_page != "new_page") or (group == "control" and landing_page != "old_page")') $ no_lined_up.shape[0]
matrix = tf.fit_transform(corpus)
VIC = pd.DataFrame(cvModel2.bestModel.featureImportances.toArray(), columns=["values"]) $ features_col = pd.Series(["AgencyVec", "CompTypeVec", "BoroughVec", "HOD"])  $ VIC["features"] = features_col $ VIC
f = "/home/sala/Work/Data/SwissFEL/Storage/list.all-files.bz2" $ headers = ["Inode number", "gen number", "Snapshot ID", ] $ headers += ["kb_allocated", "sep1", "filesize", "sep2", "user_id", "sep3", "fileset_name", "sep4", "creation_date", "creation_time"] $ headers += ["Seperator", "Filename"] $ df = pd.read_csv(f, sep=r"\s+", names=headers, ) $
autos['last_seen'].describe()
accuracy_np(*m3.predict_with_targs())
df_test_2 = df2.merge(countries_df, left_on = 'user_id', right_on = 'user_id', how='inner') $ df_test_2.head()
measurement_df['station'].value_counts().count()
print ("Probability that individual was in the control group,and they converted: %0.4f" % (df2.query('converted == 1 and group == "control"').shape[0]/df2.shape[0]))
ws = Workspace.create(name = workspace_name, $                       subscription_id = subscription_id, $                       resource_group = resource_group, $                       location = workspace_region) $ ws.get_details()
p_value = (p_diffs>diff).mean() $ p_value
pred.p1_dog.value_counts()
df2_new[['CA','UK']]=pd.get_dummies(df2_new['country'])[['CA','UK']]
pd.MultiIndex(levels=[['a','b'], [1,2]], $              labels=[[0,0,1,1], [0,1,0,1]])
prediction_proba = model_test.predict_proba(X_test) $ prediction_proba = [p[1] for p in prediction_proba] $ auc = roc_auc_score(y_test, prediction_proba) $ print(auc)
train.info()
RNPA_new_8_to_16wk_arima = RNPA_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']] $ RNPA_new_8_to_16wk_arima.index = RNPA_new_8_to_16wk_arima.index.date
matthew['matthew160'] = matthew.text.apply(lambda text: pd.Series([x in text for x in MATTHEW_WORDS_160]).any()) $ matthew['matthew92']  = matthew.text.apply(lambda text: pd.Series([x in text for x in MATTHEW_WORDS_95]).any())
df3 = pd.merge(campaign_cost, df2, left_on=['id_partner', 'campaign'], right_on=['id_partner', 'name']).sort_values('Costs')
col_names_full = ['Field1', 'Field2', 'Field3', 'Field4', 'Field5', $                   'Field6', 'Field7', 'Field8', 'Field9', 'Field10', $                   'Field11', 'Field12', 'Field13', 'file'] $ df_full.columns = col_names_full
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = log_mod.fit()
for column in ls_other_columns: $     df_uro[column].unique()
def project_link(project_id): $     try: $         return (str(projectTable.index[projectTable['id'] == project_id][0])) $     except: $         return ('Personal')
import statsmodels.api as sm $ df_reg['intercept']=1 $ logit_mod= sm.Logit(df_reg['converted'], df_reg[['intercept','ab_page']]) $ results= logit_mod.fit() $ results.summary()
df_roll = df.set_index("posted_date") $ df_roll = df_roll.resample("1h").sum().fillna(0).rolling(window=3, min_periods=1).mean() $ df_roll.reset_index(inplace=True)
train['text'] = train['text'].apply(process_tweet)
sensors_list = [entity[0] for entity in entity_id_list if entity[0].split('.')[0] == 'sensor'] # Print only the sensors $ sensors_list
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs);
amanda_tweets = pd.DataFrame(amanda) $ amanda_tweets
import numpy as np $ a = np.array([1, 2, 3, 4]) $ a.dtype
noHandReliableData.shape
inspector = inspect(engine) $ scolumns = inspector.get_columns('station') $ for c in scolumns: $     print(c['name'],c['type'])
from statsmodels.tsa.stattools import adfuller as ADF $ print() $ print(ADF(resid_6201.values)[0:4]) $ print() $ print(ADF(resid_6201.values)[4:7])
ds_train.Schema
twitter_data.info()
pd.crosstab(test["rise_in_next_week"], predictions, rownames=["Actual"], colnames=["Predicted"])
for remove in map(lambda r: re.compile(re.escape(r)), $                   [",", ":", "\"", "=", "&", ";", "%", "$", "@", "%", "^", "*", "(", ")", "{", "}", $                    "[", "]", "|", "/", "\\", ">", "<", "-", "!", "?", ".", "'", "--", "---", "#", "..."] $                  ): $     df_1.loc[:, "text"].replace(remove, " ", inplace=True)
from bokeh.plotting import figure, show, output_notebook $ from bokeh.models import HoverTool, ColumnDataSource, value $ output_notebook()
cur.execute("CREATE DATABASE IF NOT EXISTS test;") $ cur.execute("USE test;") $ cur.execute("show tables;") $ for r in cur.fetchall(): $    print(r)
data = spark.read.csv('../datasets/ATM_CleanData.csv', header='true',inferSchema='true') $ data.toPandas().head()
trains_scaled[0].head(n=2)
total.iloc[:3]
top10_df_pd=df.toPandas() $ top10_df_pd.head(10)
import numpy as np $ vectorized_body = np.load('body_vectors.npy') $ vectorized_title = np.load('title_vectors.npy')
props.head()
X_all = pd.concat((X, X_num), 1) $ X_all.shape
s.values?
df_CLEAN1A['AGE_groups'] = df_CLEAN1A['AGE_groups'].astype('category')
n_duo = pd.Categorical((df.landing_page.astype(str)+df.group.astype(str))).value_counts() # number of each duo (newpage_control, new_pagetratment, etc) $ dontlineup = n_duo['new_pagecontrol']+n_duo['old_pagetreatment'] $ print ("The number of times the new_page and treatment do not line up is: {}".format(dontlineup))
n_new = treatment.shape[0] $ n_new
no_match = df[(df['group'] == 'treatment') != (df['landing_page'] == 'new_page')].shape[0] $ print('Number of times that "new_page" and "treatment" do not match: ', no_match)
q = pd.read_json(jsonx2, typ='frame')
np.exp(log_mod_results.params)
mnnb.score(X_test_dtm, y_test)
tweet_full_df.head()
len(df.user_name.unique()) # number of users writing these tweets
df_ab_page.reset_index().head()
samsung_SA = samsung.groupby('SA').agg({'SA':'count'}).rename(columns=({'SA': 'Count'})) $ samsung_SA ['% of total'] = samsung_SA['Count']/samsung_SA['Count'].sum() $ samsung_SA ['Sentiment'] = ['Negative', 'Neutral', 'Positive'] $ samsung_SA = samsung_SA.reindex(columns = ['Sentiment', 'Count','% of total']) $ samsung_SA
type.__call__
twitter_archive_clean.to_csv('twitter_archive_master.csv') $ image_predictions_clean.to_csv('image_predictions_master.csv')
df_test.head()
idx = df_providers [(df_providers['drg3']==470) | (df_providers['state'] == 'CA')].index.tolist() $ len(idx)
led_pin = 18 $ GPIO.setmode(GPIO.BCM) $ GPIO.setwarnings(False) $ GPIO.setup(led_pin,GPIO.OUT)
acs_df.head()
from sklearn.metrics import classification_report, confusion_matrix $ print classification_report(y_test, rf_yhat)
df_breed.info()
input_data = input_data[input_data.year >= 2010] $ input_data.shape
d = ts.resample('1D') $ d
df2 = df2.drop(df2[df2['user_id'].duplicated() == True]['user_id'].index[0])
del(df['Time']) $ df.head()
np.sqrt(np.mean((df_y_pred.values - df_y_actual.values)**2))
negative=Stockholm_data_final[Stockholm_data_final.iloc[:,10]==0] $ positive=Stockholm_data_final[Stockholm_data_final.iloc[:,10]==1] $ negative = negative.sort_values('tweet_created_at') $ positive = positive.sort_values('tweet_created_at')
n_new = len(df2.query('landing_page=="new_page"')) $ n_new   #displaying the number of individuals receiving new page       
images_clean.info()
basicmodel = LogisticRegression() $ basicmodel = basicmodel.fit(basictrain, train["rise_in_next_week"])
sales_update_nan = sales_update.dropna() $ sales_update_nan.isnull().sum()
df_archive_clean["tweet_id"].describe()
corr_aggs = [] $ corr_aggs.extend(manual_aggs) $ corr_aggs.extend(computed_aggs)
autos['odometer'].head()
plt.rcParams['axes.unicode_minus'] = False $ dta_52.plot(figsize=(15,5)) $ plt.show()
twitter_final.shape
assert trn_df.shape[0] == train_nrows $ assert val_df.shape[0] == val_nrows $ assert test_df.shape[0] == test_nrows
aux = aux.join(pd.get_dummies(aux['country'])) $ aux.drop(['user_id', 'country', 'CA'], axis=1, inplace=True) $ aux.head()
text1.concordance("monstrous")
indexed_activity = active_df.set_index("station")
czn = pd.read_csv('Cuisine.txt')
pop = pd.Series([5.7, 82.7, 17.0], name='Population'); pop  #The descriptive name is optional.
df_vow = pd.read_csv('./datasets/vow.csv')
m1.test_assumptions('1%')
coming_next_reason = questions['coming_next_reason'].str.get_dummies(sep="'")
index_outliers_age = cserie(outliers_age.is_outlier==1, index=True)
p_diffs = np.array(p_diffs) $ (p_diffs > actual_diff).mean()
print("train X shape", trX.shape) $ print("train Y shape", trY.shape) $ print("test X shape", tsX.shape) $ print("test Y shape", tsY.shape)
test_prediction = rdf_clf.predict(X_test[columns])
%%time $ ent_count = defaultdict(int) # reset defaultdict $ for doc in nlp.pipe(texts): # ['parser','tagger','ner'] $     matcher(doc) # match on your text $ print(ent_count)
recommendation_df = recommendation_df.sort(['hacker_id', 'score'], ascending = [False, False])
learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)
stations = (turnstiles_daily.groupby(['STATION', 'DATE_TIME']).DAILY_ENTRIES.sum().reset_index()) $ stations.head()
autos.loc[((autos['registration_year'] < 1910) | (autos['registration_year'] > 2016)), 'registration_year'].value_counts().sort_index()
articles_list = pd.read_csv('nyt_scrape/articles_list_w_date.csv',parse_dates=['date'])
(taxiData2.Fare_amount == 0).any() # This Returns True, meaning there are values that equal to 0
monthly_gain_summary = pd.DataFrame() $ monthly_gain_summary['Infy'] = infy_df.gain_perc.resample('M').mean() $ monthly_gain_summary['Glaxo'] = glaxo_df.gain_perc.resample('M').mean() $ monthly_gain_summary['BEML'] = beml_df.gain_perc.resample('M').mean() $ monthly_gain_summary['Unitech'] = unitech_df.gain_perc.resample('M').mean()
endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url') $ print(endpoint_published_models) $
index_to_change = df[df['group']=='treatment'].index $
doc = "Let's do great things together." $ vec_bow = bag.doc2bow(doc.lower().split()) $ vec_lsi = lsi[vec_bow] $ sims = index[vec_lsi] $ print(list(enumerate(sims)))
isinstance(rng, pd.Index)
from sklearn.metrics import accuracy_score
mock_data = mock_data[(mock_data['ABPm_x'].isnull() == False) & (mock_data['ABPm_y'].isnull() == False) & (mock_data['ABPm'].isnull() == False) & (mock_data['ABPm'] > 0)].head(20) $ mock_data.to_json()
pred_clean.info()
part = data[['created', 'freq']].copy() $ part = part.set_index('created').resample('60Min').sum().plot(kind="bar", figsize=[25, 10]) $
movie_containers = html_soup.find_all('div', class_ = 'lister-item mode-advanced') $ print(type(movie_containers)) $ print(len(movie_containers))
(p_diffs > actual_diff).mean()
from sklearn.metrics import accuracy_score $ accuracy_score()
%%time $ df['closed_at'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %X %p')
aqmdata.info()
print(true_file.shape) $ print(test_file.shape) $ display(true_file.head()) $ display(test_file.head())
print cust_data.assign(Dups = cust_data.duplicated()).head(5) $
df_new['night']= pd.get_dummies(df_new['day_part'])['night']
def find_email(slice): $     return re.findall('mailto:(.*?)"', slice)[0]
first_year = first_year.text $ first_year
le_data_all.index.levels[0]
sim_prop_new_converted = new_page_converted.mean() $ sim_prop_old_converted = old_page_converted.mean() $ print(sim_prop_new_converted - sim_prop_old_converted)
%%bash $ curl -v -X GET "https://api.crunchbase.com/v/3/organizations/origami-logic/funding_rounds?user_key=7c32f123949eb66c6fb19d362490f752"
df_clean = df_clean.drop('variable', axis = 1) $ df_clean = df_clean.rename(columns = {'value':'dog_stage'}) $
url_df_full.head()
country_dummies=pd.get_dummies(df_new['country']) $ df_new=df_new.join(country_dummies) $ df_new.head()
merged.groupby("committee_name_x")
temp_cat.codes
stationweekday = data.groupby(['STATION','SaturdayWeekEndingDate','DayOfWeek'])['EntriesDifference'].agg(pd.np.sum) $
!hadoop fs -ls -h stocks.json
df2 = df2.drop_duplicates(['user_id'],keep='first')
no_outliers_forecast_exp = np.exp(no_outliers_forecast[['yhat','yhat_lower','yhat_upper']]) $ no_outliers_forecast_exp.index = no_outliers_forecast['ds'] $ no_outliers_error = no_outliers_forecast_exp['yhat'] - df_orig['y'] $ MAPE_no_outliers_model = (no_outliers_error/df_orig['y']).abs().sum()/n *100 $ print ("No outliers model error: ",round(MAPE_no_outliers_model,2))
y.sum()
import quandl $ quandl.ApiConfig.api_key = 'RGYoyz3FAs5xbhtGVAcc'
data[ $     get_child_column_names(observations_node) + $     get_child_column_names(observations_ext_node) $ ].T
count1df = pd.DataFrame(kochbar02) $ count1df = count1df.drop_duplicates(subset=['name', 'user']) $ count1df.info()
print("accuracy for train data is",best_model_lr.score(train_ind[features],train_dep[response])) $ print("accuracy for test data is",best_model_lr.score(test_ind[features],test_dep[response])) $ print("accuracy for complete data is",best_model_lr.score(kick_projects_ip_scaled_ftrs,kick_projects_ip[response])) $ print (" Confusion matrix on complete data is", confusion_matrix(kick_projects_ip[response], kick_projects_ip["Pred_state_LR"]))
a.all
xml_in.shape
df2[df2.duplicated(['user_id'], keep=False)]['user_id'] $ df2[df2.duplicated(['user_id'], keep=False)]
old_page_converted = np.random.binomial(1, p_old, n_old) $ plt.hist(old_page_converted) $ plt.axvline(x=p_old, color="red");
logit_mod4=sm.Logit(df3['converted'],df3[['ab_page','intercept', 'CA','UK', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday' ]]) $ fit4=logit_mod4.fit() $ fit4.summary()
tensor_2d = np.arange(16).reshape(4, 4) $ print(tensor_2d) $ tf_tensor = tf.placeholder(tf.float32, shape=(4, 4)) $ with tf.Session() as sess: $     print(sess.run(tf_tensor, feed_dict={tf_tensor: tensor_2d}))
df.filter(a_sevenPointSeven).count()
df2[df2['landing_page'] == 'new_page'].count()[0]/df2.count()[0]
df=df.rename(columns = {'Parent?':'flag'}) $ d=len(df.com_id) $ print("Total Number of Comments Scraped: ") $ d
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
f1_score(Y_valid_lab, val_pred_svm, average='weighted', labels=np.unique(val_pred_svm))
old_page_converted = np.random.binomial(n_old,Pold) $ old_page_converted
plt.hist(p_diffs); $ plt.axvline(x= obs_diff, color = 'red')
titanic_df = pd.read_excel(DATA_FOLDER+'/titanic.xls') $ titanic_df.head()
y_prob = gnb.predict_proba(X_clf) $ print(y_prob.shape)
cust_demo.shape
festivals_clean['End_Date'] =  pd.to_datetime(festivals_clean['End_Date'])
lr_best = LogisticRegression(C = 4.641588, penalty = 'l2' $                                   , random_state = 42, n_jobs = -1) $ lr_best.fit(X_train, y_train) $ test_preds = lr_best.predict(X_test) $ display_f1(y_test, test_preds)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner')
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 1) & (jobs.Group == 'p_meiler')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
import sys $ list(sys.modules.keys())[:20]
def clean_mentions(text): $     return re.sub(r"@\w+", "", text)
giss_temp.tail()
combined_item_df = FRStimSessionSummary.combine_sessions(session_summaries) $ combined_item_df.head()
from h2o.estimators.gbm import H2OGradientBoostingEstimator $ from h2o.estimators.random_forest import H2ORandomForestEstimator $ help(H2ORandomForestEstimator) $ help(h2o.import_file)
season_type_groups.groups
data.tail()
reddit_comments_data.groupBy('author').agg({'subjectivity':'mean'}).orderBy('avg(sentiment)').show()
pickle.dump(nmf_tfidf, open('iteration1_files/epoch3/nmf_tfidf.pkl', 'wb'))
tweet_archive.head(2)
df[(df['group'] == 'treatment') != (df['landing_page'] == 'new_page')].shape[0] $
archive_copy.loc[archive_copy['name'] == 'None']
pred=pd.read_csv('image-predictions.tsv',sep='\t')
search['search_weekday'] =  search.timestamp.dt.dayofweek+1 $ search['trip_start_date_weekday'] =  search.trip_start_date.dt.dayofweek+1 $ search['trip_end_date_weekday'] =  search.trip_end_date.dt.dayofweek+1
all_noms[all_noms["agency"] == "Foreign Service"]["nom_count"].sum()
df_columns = pd.read_csv("/data/311_Service_Requests_from_2010_to_Present.csv", error_bad_lines=False, usecols=['Created Date','Closed Date','Agency','Complaint Type','Descriptor','Borough']) $
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 2) & (jobs.GPU == 1) & (jobs.Group == 'cms')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
n_old = df2.query('group == "control"')['user_id'].count() $ n_old = int(n_old) $ n_old
pd.read_excel(cfg_fnames[0]).head()
from sklearn.cross_validation import cross_val_score
%%time $ gc_conn = psycopg2.connect(user='postgres', password='Xypherium-0', $                         dbname='jpstat', $                         host='35.224.240.50') $ print(gc_conn.closed) #0 when open
len(df.index) - df.count()
df2['landing_page'].value_counts()['new_page']/df2['landing_page'].count()
rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})] $
log_mod_3 = sm.Logit(df_joined['converted'], df_joined[['intercept', 'ab_page', 'US', 'CA']]) $ results = log_mod_3.fit() $ results.summary()
with pdfplumber.open('../pdfs/collections.pdf') as pdf: $     pages_with_data = pdf.pages[1:] $     for page in pages_with_data: $         df_to_append = page_to_df(page) $         df = df.append(df_to_append, ignore_index=True)
merge[merge.columns[7:22]].head(3)
train = train.merge(data, on=['cust_id','order_date'],how='left')
train_small_data.head(1)
main.wl_ta.value_counts(normalize=True)
s6 = pd.Series([83, 68], index=["bob", "alice"], name="weights") $ s6
exogx = np.column_stack([df[x]])
y_predicted = fit4.predict(X4) $ plt.plot(y4, y_predicted, 'b.') $ plt.title('Actual Gross Outcome vs. Budget') $ plt.xlabel('Budget') $ plt.ylabel('Domestic Gross Total')
data = pd.Series(["quick", None, "fox"], name="Fox") $ data
vlc = df[(df.country == "es") & (df.city == "Valencia") & (df.status == "active")]
df_master.describe()
allocs=[0.4,0.4,0.1,0.1] $ allocs=np.array(allocs) $ alloced=(normed*allocs) $ alloced.head()
print(g1800s.describe())
np.unique(noaa_data[noaa_data['AIR_TEMPERATURE'] < -273.15]['AIR_TEMPERATURE'].index.date)
import pandas as pd $ df = pd.DataFrame(all_records, columns=['date', 'lie', 'explanation', 'url'])  $
n_new = df2[df2['landing_page']=='new_page'].shape[0] $ p_new = df2.converted.mean() $ new_page_converted = np.random.choice([0,1], n_new, p= [1-p_new, p_new]) $ unique, counts = np.unique(new_page_converted, return_counts=True) $ dict(zip(unique, counts))
df.select(functions.lower(df.hashtag)).show()
scn_genesis = pd.to_datetime(min(BID_PLANS_df['scns_created'])[0])
stop = stopwords.words('english') $ stop += ['just', 'like', 'know', 'want', 'really', 'got', 'people', 'feel', 'would'] $ stop += ['would', 'could', 'should', 'us', 'we', 'great', 'good', 'might', 'may'] $ stop += ['get', 'go', 'anyone', 'one', 'take', 'too', 'also', 'going', 'getting'] $ stop = set(stop)
autos['price'].describe()
df2.tail()
a_fav_total = a_df['tweetFavoriteCt'].sum() $ a_rt_total = a_df['tweetRetweetCt'].sum() $ b_fav_total = b_df['tweetFavoriteCt'].sum() $ b_rt_total = b_df['tweetRetweetCt'].sum()
df_input_clean.groupby('isLate').count().show()
adopted_cats.loc[adopted_cats.Color.str.contains('Silver'),'Color'] = adopted_cats.Color.str.replace('Silver','Gray')
archive_df[archive_df.name == 'None'].sample(20, random_state=1210)
transactions_items.info()
n_old = len(df2.query("group == 'control'")); $ n_old
package.descriptor['resources'][1]
open_list=[] $ for lis in answer1: $     open_list.append(lis[1])
df_goog.Open.resample('M').plot() $
df.columns = df.columns.str.strip() $ df.columns
dtObj = binary_sensors_df.iloc[0]['last_changed'] $ device_id = 'device_tracker.robins_iphone' $ print("At {} {} is {}".format(dtObj, device_id, get_device_state(parsedDF, device_id, dtObj)))
seaborn.countplot(vacancies.hour)
nba_df.set_index("Date", inplace = True)
profit_calculator(stock.iloc[1640:], 'model_predict',-1)
discounts_table.Country.unique()
df[(df['landing_page'] == 'new_page') & (df['group'] == 'control')].landing_page.count()
dd_df.head()
def sentiment_finder_vader(comment): $     analysis = analyser.polarity_scores(comment) $     return analysis['compound']
s.loc['b':'d']
home = pathlib2.Path.home() $ full_path = home/"documents"/"stash"/"ps-metadata-analysis"/"data"/"patch_seq_log_mouse.csv" $ full_path = str(full_path) $ full_path $ initial = pd.read_csv(full_path)
net_loans_exclude_US_outstanding.Beschreibung.unique()
tbl.head()
for column in titanic: $     print(titanic[column].describe()) $     print()
nar2.head()
holdout_results.groupby(['wpdx_id', 'second_measurement'])['status_binary','cv_predictions'].sum().unstack().head()
print(read.shape) $ read.head()
df2.head()
plt.figure() $ plt.title("Trip Avg Temp") $ plt.bar(.5, tavg, yerr=(tmax-tmin))
df_archive_clean.info()
url_reputation = grouped['reputation_score'].agg({'total_reputation': 'sum', 'avg_reputation': 'mean'})
pd.set_option('display.max_colwidth',-1)
df.shape
plt.style.use('ggplot') $ bb['close'].apply(rank_performance).value_counts().plot(kind='pie', legend=True)
noaa_data.loc[:,'AIR_TEMPERATURE'].groupby(pd.Grouper(freq='W')).mean()
df.query("landing_page == 'new_page'").count()['landing_page']/df['landing_page'].count()
rfc.fit(features_class_norm, overdue_transf)
unzipfile("glove.6B.zip","wordvec")
df.sample(20)
sp['day_ago_open'] = sp.Open.shift(periods = 1) $ sp['week_ago_open'] = sp.Open.shift(periods = 7)
autos.head()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
from shapely.geometry import Polygon, Point $ closed_sakhalin_contour = np.vstack([contour_sakhalin, contour_sakhalin[-1]]) # Polygon should be closed to check inclusions $ sakhalin_poly = Polygon(closed_sakhalin_contour)
df_clean = df.copy() $ preds_clean = pred.copy() $ api_clean = api_df.copy()
season_type_groups.ngroups
X.shape
without_condition_heatmap.add_child(plugins.HeatMap(final_location_ll[:40000], radius=15)) $ without_condition_heatmap
df.info()
building_pa_prc_zip_loc.head(5)
'{:,.2f}'.format(datetime.datetime(2099,12,31,0,0).timestamp()*1000)
page.text
s = df.iloc[3] $ df.append(s, ignore_index=True)
x = np.array([1, 2, 3, 4, 5])
math.exp(4.165)
s = pd.Series([1,3,5,np.nan,6,7,8], index=dates).shift(2) $ s
!cp kaggle.json ~/.kaggle/
pop_cat.describe()
def get_tuple(series): $     return tuple(series) $ grouped_authors_by_publication = xml_in_sample.groupby(['publicationKey'], as_index=False)[['authorName', 'authorId']].agg({'authorName': get_tuple, $                                                                                                                      'authorId': get_tuple}) 
df.sort_values(by=['Year'],ascending=True, inplace = True) $ df.head(5)
adjust_cols = ['StateBottleCost','StateBottleRetail','SaleDollars'] $ for col in adjust_cols: $     liquor[col] = pd.to_numeric(liquor[col].str.replace('$',''),errors='coerce')
table2.head(3)
im_clean.query('p1_dog == True').p1.value_counts()[0:3]
json_tweets = pd.DataFrame(df_list, columns = ['tweet_id', 'favorites', 'retweets', $                                                'user_followers', 'user_favourites', 'date_time']) $ json_tweets.to_csv('tweet_json.txt', encoding = 'utf-8', index=False)
word = 'Data' $ it = iter(word) $ print(*it) $ print('again?...') $ print(*it)
print (collData.collect()) $ print (collData.reduce(lambda x,y: x+y)) $ z =collData.map(lambda x: x**2) $ print (z.collect()) $
mention_pairs["FromType"] = "Person" $ mention_pairs["ToType"] = "Person" $ mention_pairs["Edge"] = "Mentioned" $ mention_pairs.rename(columns={"screen_name":"FromName","mention":"ToName"},inplace=True) $ mention_pairs.head()
print(np.info(np.dtype))
campaign_cost = Cost.iloc[:, 0:3].groupby(['id_partner', 'campaign'], as_index=False).sum() $ display(campaign_cost.sort_values('Costs', ascending=False))
c1.sum()
cohort_churned_df.to_csv('churned_cohorts_518.csv')
df['before_or_after_election'] = df['created_at'].apply(lambda x: 'after' $                                                         if x > datetime.datetime(2016,11,9) $                                                         else 'before')
df.shape, noloc_df.shape
all_tables_df.loc[:, 'OBJECT_NAME']
measures = ureg.Quantity(np.random.random(100), 'volts') $ print(measures)
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 2) & (jobs.GPU == 1)].groupby(['Group']).JobID.count().sort_values(ascending = False)
pattern = re.compile('AA') $ print(pattern.sub('BB', 'AAbcAA')) $ print(pattern.sub('BB', 'bcAA'))
result.route
df_concensus.head(20)
from sklearn.feature_extraction.text import TfidfVectorizer
def fill_missing_airline(row): $     if not isinstance(row['Airline'], str) and np.isnan(row['Airline']): $         row['Airline'] = row['flight_no'][:2] $     return row $ data_df = data_df.apply(fill_missing_airline, axis=1)
from shutil import copyfile $ source = 'targetarticles' $ destination = 'missing' $ for fh in missing: $     copyfile('./'+ source +'/PMC' + fh +'.nxml', './'+ destination +'/PMC' + fh +'.nxml')
santos_tweets.loc[santos_tweets['date'] == '2017-09-08']['text'].unique() # only 2017-09-08 Tweets
dftemp = df1[(df1['Area'] == "Iceland")] $ dftemp.head(10) $
for i in range(artificial_returns.shape[1]): $     plt.plot(returns.iloc[:,i], color=colors[i]) $ plt.legend(loc=3, bbox_to_anchor=(1.0,0.5)) $ plt.show()
autos['odometer_km'].value_counts().sort_index(ascending=False)
building_pa_prc_shrink.to_csv("buildding_02.csv",index=False) $ building_pa_prc_shrink=pd.read_csv('buildding_02.csv',parse_dates=['permit_creation_date'])
final_df = merged_df.merge()
df_subset.dtypes
a = zip(cvec.get_feature_names(), log_model.coef_[0]) $ a
master_df.loc[master_df.rating_numerator_normal<1776, ['rating_numerator_normal']].hist();
from sklearn.metrics import mean_absolute_error $ mean_absolute_error(y_test, pred)
promo_df['after_onpromotion'].value_counts()
trump.columns
df = df.loc[(df['Direction'] == 1)]
test = Plot['mLayerLiqFluxSoil'].data $ dates = Plot['time'].data $ test = np.squeeze(test) $ df = pd.DataFrame(data = test, index=dates) $ df.replace(to_replace=-9999.0, value = 0, inplace=True)
plt.hist(p_diffs, alpha = 0.5)
page.status_code
access_logs_df.count()
df_reg=df2 $ df_reg.loc[df_reg['group']== 'control','ab_page']= 0 $ df_reg.loc[df_reg['group']== 'treatment','ab_page']= 1
breakfastlunchdinner['totals'] = (breakfastlunchdinner['breakfast'] + $                                   breakfastlunchdinner['lunch + brexits'] + $                                   breakfastlunchdinner['dinner']) $ breakfastlunchdinner.sort_values('totals',ascending=False, inplace=True) $ breakfastlunchdinner.reset_index()
data[(data['author_flair'] == 'Broncos') & (data['win_differential'] >= 0.9)].comment_body.head(15) $
data.drop(['Unnamed: 0'], axis=1, inplace=True) $ data = data.dropna()
[sample_pivot_table,test] = split_data(order,70,100) $ similarity_weight = cosine_similarity(sample_pivot_table) $ prefiltering_sw = prefiltering_of_neighbors(similarity_weight, 0.1) $ predicted_table =  cs_classification_predicted_score_user_based(sample_pivot_table,prefiltering_sw,[0,1])
df_clean['body_length'].hist(range = (0,100))
oppose_NNN.sort_values("amount", ascending=False).head()
iris.head().iloc[:,0]
sanders = miner.mine_user_tweets(user="berniesanders") $ donald = miner.mine_user_tweets(user="realDonaldTrump")
twitter_archive.info()
 df.describe(include='all')
status = api.destroy_favorite(status.id) $ status.favorited, status.favorite_count
history.to_csv('../data/merged_data/history.csv')
ex4.drop([1,2])
df_final_.category.nunique()
plt.hist(null_value) $ plt.axvline(obs_diffs, c="r")
fulldata = pd.concat(seg_data)
from azureml.api.schema.dataTypes import DataTypes $ from azureml.api.schema.sampleDefinition import SampleDefinition $ import azureml.api.realtime.services as amlo16n
about.attrs
def day_of_week(times): $     datetime_df = pd.DataFrame(times)
crimes.PRIMARY_DESCRIPTION[3:10]
sim_pnew_minus_pold = (new_page_converted.mean() - old_page_converted.mean()) $ sim_pnew_minus_pold
Lab7_Equifax = Lab7_Redesign.loc[Lab7_Redesign['ENTITY'] == 'Equifax']
train.info()
model = sm.Logit(reg_df['converted'], reg_df[['intercept', 'ab_page', 'is_US', 'is_UK', 'page_UK', 'page_US']])
df.columns
con = pd.concat([tasks,emails,site_landings],axis=0)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ (z_score, p_value)
retweet_max = np.max(data['RTs']) $ retweets = data[data.RTs == retweet_max].index[0] $ print("Tweet with most retweets is: \n{}".format(data['Tweets'][retweets])) $ print("Number of retweets: {}".format(retweet_max)) $ print("{} characters.\n".format(data['len'][retweets]))
department_df.groupby(["Department", "City"], as_index = False).sum()
api_token = os.environ['GITHUB_API_TOKEN'] $ headers = {'Authorization': f'token {api_token}'}
pd.DataFrame(hillary)
g_influxdbconn.insert('Host01', 300, 20, 10, 1, 30, 20)
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.plot(y,z) $
b = R17df.rename({'Create_Date': 'Count-2017'}, axis = 'columns') $
csvname = queryfile[:-3] + 'csv' $ print('The CSV, by default will be "{}".\n'.format(csvname)) $ print('To change this, uncomment and change "example path" below')
type_city=pd.Series(totol_fare["type"]) $ fare_city=pd.Series(totol_fare["fare"]) $ colors = ["gold","lightskyblue","lightcoral"] $ explode = (0, 0, 0.1)
tb = pd.crosstab(index=mydf.dept, columns=mydf.grp, margins=True) $ tb
learn.save('clas_0')
import numpy as np $ ids = np.random.choice(small_train['user_id'].unique(), 100000) $ small_train = small_train[small_train['user_id'].isin(ids)] $ small_train.shape
Merge.head()
%matplotlib inline $ g=sns.lmplot(x='favorite_count', y='retweet_count', data=matrix, $            palette="muted", size=4,scatter_kws={"s": 10, "alpha": 1}) $ g.set_axis_labels("Favoriate Count", "Retweet Count")
s4 =pd.Series(np.random.randn(5), index=list('abcde'), name='S4') $ print s4
asf_people_human_df = non_blocking_df_save_or_load( $     rewrite_human_data(asf_people_human_raw_df), $     "{0}/human_data_cleaned/asf_people".format(fs_prefix)) 
yhat = LR.predict(X_test) $ yhat [0:5]
user_summary_df = user_summary_df[user_summary_df.tweets_in_dataset != 0] $ user_summary_df.count()
type.__dict__['__call__'].__get__(type)
df.fillna(0, inplace=True)
is_08A = (restaurants["VIOLATION CODE"] == "08A") $ violations08A = restaurants[is_08A] $ violations08Acounts = violations08A['BORO'].value_counts() $ violations08Acounts
plt.scatter(med['longitude'],med['latitude'])
cluster_labels = g.predict(cluster)
df["2016-04-28":"2016-05-27"].mean()
%%time $ grid_tfidf.fit(X_tfidf, y_tfidf)
URL = "http://www.reddit.com"
final_xgb = xgb.train(best_params, dtrain, num_boost_round = num_boost)
tweet_hour.drop(columns=tweet_hour.columns[3:],axis=1,inplace=True)
geocode = ["geocode:35.5150,137.8214,10km", "geocode:33.4629,132.4233,10km"] $ name = ["Iida", "Yawatahama"]
%store -r keys_0423 $ keys = keys_0423.copy()
df1 = df1.drop(0) $ df2 = df2.drop(0) $ df2 = df2.drop(columns=["PERIOD", "Name"])
for i in files_to_manage: $     print(load_file_into_my_dropbox(account, '{}_2017.csv'.format(i), folder="nba games/test_ws_workflow/"))
sanfran.layers
df_person.merge(df_grades, how="inner", left_index=True, right_index=True)
test = [char_index[char] if char in vocab else char_index['specialchar'] for char in titleslist[0].lower()] $ for i in range(maxlength-len(title)): $         test.append(char_index["END"])
print("The simulated difference is: %.4f" %(new_page_converted - old_page_converted))
autos = autos[autos["price"].between(1,351000)] $ autos["price"].describe()
tweet_json_clean.rename(index=str, columns={"id": "tweet_id"}, inplace=True)
print("Type of data in cell (row 3, col 2):"), $ print(sheet.cell_type(3, 2))
((revenue[revenue.cum_revenue_pct <= 0.81].fullVisitorId.nunique())/revenue.shape[0])
df_unique_providers.to_csv('Unique Providers December 31, 2017.csv')
msft_cum_ret['2012-01'].mean()
noise = [np.random.normal(0,noise_level*p,1) for p in weather.power_output]
pvalue=(p_diffs > obs_diff).mean() $ pvalue
df.rename(columns={"PUBLISH STATES":"Publication Status","WHO region":"WHO Region"},inplace=True)
generate_chart(roc_curve, # we imported this method 2 cells above. $                "", # filename is blank. $                "injured", # the name of the report is "injured." $                [('a', our_nb_classifier), ]) # not sure what 'a' is for. $ plt.show()
pd.options.display.max_columns
len([premiePair for premiePair in BDAY_PAIR_qthis.pair_age if premiePair < 0])/BDAY_PAIR_qthis.pair_age.count()
url_authors = grouped['author'].agg({'mentions': 'count', 'num_authors': 'nunique','authors': lambda x: np.unique(x)})
df_main.info()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
p_old = df2.converted.mean() $ print("{:.4f}".format(p_old))
keyli = token.sender.tolist() + token.receiver.tolist()
trips.head()
url_p1 = 'https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-08-24&end_date=2018-08-24&api_key=' $ url_test = url_p1 + API_KEY $ r_test = requests.get(url_test) $ data_test = r_test.json()
es_rdd.take(1)
old_page_converted = np.random.choice([0,1],size=n_old, p=(cr_old_null,1-cr_old_null))
import pandas_profiling as pp $ pp.ProfileReport(train)
TestData_ForLogistic.columns
df2.head()
aggregate_by_name = pd.concat(g for _, g in df_Providers.groupby("name") if len(g) > 1) $ aggregate_by_name.head()
y_train.value_counts(normalize=True)
print('input dim = {}'.format(input_image.ndim)) $ print('input shape = {}'.format(input_image.shape))
df.loc[:4,['id','price_doc']]
plotly_fig = tls.mpl_to_plotly(mat_plt_plt) $ iplot(plotly_fig, filename='myplot')
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']] $ df_new.head()
names = list(vedanta_data.keys()) $ values = list(vedanta_data.values())
q1 + 1 # Gives you the Q2...also ending in December!?
result = pd.DataFrame(eclf3.predict_proba(test[features]), index=test.index, columns=eclf3.classes_) $ result.insert(0, 'ID', mid) $ result.to_csv("gaurav5.csv", index=False)
df[df['Complaint Type']=='Street Condition']['Descriptor'].value_counts()
%run process_twitter2tokens.py -i ../data/Test_relate_unrelated.csv -ot ../data/Test_relate_unrelated.txt -oc ../data/Test_relate_unrelated_tokenized.csv -co text
pd.DataFrame(data_for_model.groupby(['final_status','country'])['goal'].count())
highmeans = cc.groupby(['name'])['high'].mean()
rows = df.shape[0] $ df = df.dropna()
pd.pivot_table(more_grades, index=("name", "month"), margins=True)
text = text.lower() $ print(text)
writer = pd.ExcelWriter("../visualizations/uber_day_of_month.xlsx")
print(training_features.shape, training_target.shape, test_features.shape, test_target.shape)
store_items.pop('new watches') $ store_items
new_page_prob = (df2[df2["landing_page"] == "new_page"]["user_id"].count())/(df2["landing_page"].count()) $ new_page_prob
to_drop = 'Date Artist Album Genre Score Author Abstract Article Text Tokenized Score_Binary'.split()
nold = len(df2[(df2.landing_page=='old_page')]) $ print(nold)
if not os.path.exists('training_data'): $     os.mkdir('training_data')
cur.execute("show databases;") $ for r in cur.fetchall(): $    print(r)
df2[df2['landing_page']=='new_page'].user_id.count()/df2['landing_page'].count()
suspects_with_25_1.apply(adding_to_map, axis=1)
datatest.loc[datatest.place_name == "Solar del Bosque",'lat'] = -34.905555 $ datatest.loc[datatest.place_name == "Solar del Bosque",'lon'] = -58.507635
threeoneone.columns
BAL_analysis = team_analysis.get_group("BAL").groupby("Category") # Pulls only team transactions from sample, then groups $
idx = pd.IndexSlice $ df.loc[idx['a', 'ii', 'z'], :]
discGrouped.head(10)
dfD.to_sql('distances', conn)
n_old = old.count() $ n_old
data[(data.value>1000) & (data.phylum.str.endswith('bacteria'))]
np.exp(reg_country.params)
for i in train_dum.columns: $     if i not in test_dum.columns: $         print i
t1.year
from quilt.data.dsdb import secondary_processing $ secondary_processing
df.groupby('raw_character_text')['episode_id'].nunique().head()
ids = df2["user_id"] $ df2[ids.isin(ids[ids.duplicated()])]
url = form_url(f'teams/{team_id}/squads', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
df.groupby('brand').count().reset_index().sort('price',ascending=False)
abtest_list = list(set(train_data.abtest))
X_prepro = psy_prepro.drop(labels=["y"], axis =1) $ y_prepro = psy_prepro["y"]
df['ts_dayofweek']=df['timestamp'].apply(lambda x:x.dayofweek)
train_set.topic.value_counts()
data = pd.read_csv("Data.csv")
(taxiData2.Fare_amount < 0).any() # This Returns True, meaning there are values that are negative
check_int('2016-11-06')[check_int('2016-11-06').number_repeat >= 7]
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ new_converted_simulation - old_converted_simulation $
!hdfs dfs -mkdir hw3 $ os.getcwd() 
foxnews_df = constructDF("@FoxNews") $ display(constructDF("@FoxNews").head())
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Measurement = Base.classes.measurements
get_freq(series_obj=raw.education)
df = pd.read_json("C://employee.json")
sc.stop()
column_names=list(All_tweet_data_v2.columns.values) $ stages=list(list(All_tweet_data_v2.columns.values)[i] for i in [13, 14,15,16,33]) $ Remaining_columns=[names for names in column_names if names not in stages]
df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv", $                  usecols= ['Borough', 'Agency', 'Closed Date', 'Created Date','Complaint Type', 'Descriptor'])
mentions_raw, user_names_raw, user_screen_names_raw, num_tweets_raw, num_retweets_raw = au.get_mentions(uso17_coll)
data_full.isnull().sum()
prediction = pd.read_csv('image-predictions.tsv', sep='\t') $ prediction.head()
varianceMonthlyReturns = dict( { 'Infy': monthly_gain_summary.Infy.var(), 'Glaxo': monthly_gain_summary.Glaxo.var(), $                           'BEML': monthly_gain_summary.BEML.var(),'Unitech': monthly_gain_summary.Unitech.var()  } )
result.inserted_id
failures['datetime'] = pd.to_datetime(failures['datetime'], format="%Y-%m-%d %H:%M:%S") $ failures.count()
from sklearn.multiclass import OneVsRestClassifier $ from sklearn.linear_model import LogisticRegression, RidgeClassifier
clean_users.head()
titles = [] $ for item in soup.find_all('div',class_='text-section'): $     if item.div is not None: $         title = item.find_all('h3')[0].string $         titles.append(title)
train['day'] = train.created_at.dt.weekday $ train.groupby('day').popular.mean()
testing = df.text[226].decode("utf-8-sig") $ testing
dum = pd.get_dummies(TestData, sparse=True, drop_first=True, dummy_na=True, $                     columns=['Gender', 'City_Category', 'Employer_Category1', 'Primary_Bank_Type', 'Contacted', $                             'Source_Category', 'Employer_Category2', 'Var1']) $ dum.head()
giss_temp = giss_temp.set_index("Year") $ giss_temp.head()
training_active_listing_dummy.shape
aggreg.to_excel('aggreg.xlsx', index=False)
archive.head()
test_data = pd.read_csv('../data/test.csv', parse_dates=['DateTime']) $ test_data.isnull().sum()
pd.value_counts(appointments[appointments['Specialty'] == 'doctor']['Provider'])
final_word_df.tail()
print('number of observations:',len(data)) $ col_names = list(data.columns) $ print('features:', col_names)
all_acct_ids = contest_savm.where\ $ ((F.col('postal_code').rlike("^95630$"))  & (F.col('start_date')>=test[0][2]))#.take(50) $ actual_acct_id = all_acct_ids.where(F.col('sales_acct_id')==test[0][1]) $ print actual_acct_id.count(),'/',all_acct_ids.select('sales_acct_id').count() $ print 'Distinct sales_acct_ids: ',all_acct_ids.select('sales_acct_id').distinct().count()
np.power(information_ratio,2)
df_enhanced['source'].nunique()
random_walk = random_steps.cumsum()
ctd_df.dtypes
df.mean(1)
ins['new_date'] = pd.to_datetime(ins['date']) $ ins.head(5)
from scipy.optimize import curve_fit $ popt, pcov = curve_fit(sigmoid, xdata, ydata) $ print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
x = slack.channels.list()
df_cleaned_train = df['text_cleaned'].tolist()
merged_data['overdue'].value_counts()
has_text[has_text.grade_levels==6.0]
1/np.exp(-0.0150)
users_conditions.drop_duplicates(subset = ['user_id'], keep = 'first', inplace = True) $ users_conditions.reset_index(drop = True, inplace = True)
n_old = df2.query("landing_page == 'old_page'")['user_id'].count() $ n_old
for model_name in nbsvm_models.keys(): $     test_probs[model_name] = nbsvm_models[model_name].predict_proba(test_cont_doc)
validation.analysis(observation_data, BallBerry_simulation)
new_set.purchase_value.mean()
print df.shape[0] + noloc_df.shape[0]
YS1315 = pd.read_csv('yahoostocks1315.csv',parse_dates = True,engine ='c',index_col = 0, sep = ',') $ YS1315
ins.head(3)
churned_unordered.head()
liberia_data1.shape
df.head(2) # See the top 2 rows
graph_stats = [] $ for split in date_splits[1:]: $     partial_df = mentions_df[mentions_df["date"] < split] $     graph_stats += [au.get_graph_stats(partial_df)] $ graph_stats_df = pd.DataFrame(graph_stats, columns=["nodes","edges","weak_components","strong_components"])
train.show(3)
df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'], $                     'supervisor': ['Carly', 'Guido', 'Steve']}) $ df4
feature_importance = exported_pipeline.named_steps['gradientboostingregressor'].feature_importances_ $ feature_importance = 100.0 * (feature_importance / feature_importance.max()) $ print(names) $ print(feature_importance)
autos.odometer = (autos.odometer.str.replace('km','') $                                 .str.replace(',','') $                                 .astype(int)) $ autos.odometer.value_counts() $
df_new = df.filter(['email','public_repos','followers','hireable','company','updated_at'], axis=1)
data = data.dropna(how = 'any')
week9 = week8.rename(columns={63:'63'}) $ stocks = stocks.rename(columns={'Week 8':'Week 9','56':'63'}) $ week9 = pd.merge(stocks,week9,on=['63','Tickers']) $ week9.drop_duplicates(subset='Link',inplace=True)
tweet_archive_clean.loc[tweet_archive_clean.name == 'O', 'name']="O'Malley" $ tweet_archive_clean.loc[tweet_archive_clean.name == 'Gin', 'name']="Gin & Tonic"
print data_df.clean_desc[26]
actual_new_page_converted=df2.query('landing_page=="new_page"')['converted'].value_counts()[1]/\ $ df2.query('landing_page=="new_page"').shape[0]
treatment_gp = df2.query('group == "treatment"') $ prob_treatment_converted = treatment_gp[treatment_gp["converted"] == 1].count()  / (treatment_gp[treatment_gp["converted"] == 0].count() + treatment_gp[treatment_gp["converted"] == 1].count()) $ prob_treatment_converted = prob_treatment_converted[0] $ prob_treatment_converted
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
df_tweet.head()
data=part.resample('5T').median() $ plt.plot(data['field4']) $ plt.show()
df_test = full_text_classifier.predict(df_test) $ df_test.head()
model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)
len(df[katespadeseries].userid.unique())
km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100) $ km.fit(X)
pnew = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("the convert rate for  pnew =", pnew)
tokens = pickle.load(open("pickle_files/tokens.p","rb"))
y.end_time # Tells us the end of the period
df2['user_id'].nunique() $ df2[df2['user_id'].duplicated() == True]
mar = pd.read_excel(mar_file)
eval_sklearn_model(real_test_df.true_grow, real_test_df.model_predict)
pd.read_sql('SELECT * FROM experiments', conn, index_col='experiment_id')
topauthors = datacamp[datacamp["publishdate"]>='2017-01-01']["author"].value_counts(sort=True, ascending=False)[:10].index
columns.keys()
df.head()
week18 = week17.rename(columns={126:'126'}) $ stocks = stocks.rename(columns={'Week 17':'Week 18','119':'126'}) $ week18 = pd.merge(stocks,week18,on=['126','Tickers']) $ week18.drop_duplicates(subset='Link',inplace=True)
samples_query.skip_records(20) $ samples_query.display_records(10)
df_students.columns
sel = [Measurements.date, func.avg(Measurements.prcp)] $ initial_date = format_latest_date - timedelta(days=365) # This will be start date from 12 months before final date of 8/23/17 $ prcp_data = session.query(*sel).filter((Measurements.date >= initial_date)).group_by(Measurements.date).order_by(Measurements.date).all()
translated_expression = cpi_sdmx.translate_expression('1.1.40055+115524+97558.10.Q') $ translated_expression
df.groupby(df.date.dt.year)['first_day_pctchg'].mean()
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False]['user_id'].count()
(autos["date_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         ) $
rnd_reg.oob_score_
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(y="Borough")
goog.head()
df.rolling(2).sum() # rolling sum with window length of 2 (sum of every 2 values) - ignored NaN
from sklearn.model_selection import train_test_split $ x = df.drop(['active'],axis = 1) $ y = df['active'] $ x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = .2,random_state = 3)
prop = df2.groupby(["converted","group"]) $ prop.count()
len(df2[df2.duplicated(['user_id'],keep=False)])
lmdict[lmdict.Positive != 0].head()
df.shape
jobPostDF.date.max() - timedelta(days=730)
Raw_Forecast.set_index("Date_Monday").groupby([pd.TimeGrouper('M'),"Product_Motor","Part_Number"]).sum().fillna(0)[["Qty"]]
pop_df = pd.DataFrame({'total': pop, $                       'under18': [2346345,234245234, $                                  36341435,23452453, $                                  234234235,34563453]}) $ pop_df
bad_bathrooms = raw_full_df.bathrooms==112 $ raw_full_df[bad_bathrooms]
def near(x, keep = 5): $     return x.tail(keep) $
train_holiday_oil_store_transaction_item_test_003 = train_holiday_oil_store_transaction_item_test_002.filter(train_holiday_oil_store_transaction_item_test_002['transferred'] == 'False') $
fig, ax = plt.subplots() $ ax.set_xlabel("Polarity,points") $ ax.set_ylabel("Bitcoin Price, dollars") $ ax.scatter(pscore, btc_price) $ fig
trump_source_counts = trump_df['source'].value_counts() $ trump_source_counts.plot.barh()
vals2 = np.array([1, np.nan, 3, 4]) $ vals2.dtype $
df2['country'].value_counts()
df['yob'].idxmax(axis=1)
x = api.GetUserTimeline(screen_name="una_dab", count=20, include_rts=False) $ x = [_.AsDict() for _ in x]
joined.drop('cluster', axis=1, inplace=True)
sns.set(style="darkgrid") $ f, ax = plt.subplots(figsize=(10, 10)) $ sns.set_color_codes("pastel") $ sns.barplot(x="Average_Num_Comments", y="Subreddit", data=subred_num_avg[:10], color="b")
!wget download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz $ !ls
twitter_archive_full.loc[(twitter_archive_full.stage == 'None') & (twitter_archive_full.text.str.contains('doggos')), 'stage'] = 'doggo'
ffr_recentM = ffr_recent.resample("M").first() $ vc_M = vc.resample("M").pad()
containers[0].find("li", {"class":"paid-amount"}).span.contents[-1].split()[2]
new_page_converted = np.random.choice([0,1], size=n_new, p=[1-p_new, p_new]) $ print(new_page_converted.mean())
small_train = small_train.dropna() $ small_train
mysom.plot_feat_grid(100)
df.loc[df.following.idxmax()]
device.key
year_prcp_df.plot() $ plt.show()
import re $ re.sub(r'@[A-Za-z0-9]+','',df.text[343])
grouped.get_group(2011).shape
n_old = df.query('group == "control" and landing_page == "old_page"').nunique()[0] $ print(n_old)
promo_df = joined[['date', 'item_nbr', 'store_nbr', 'onpromotion']].copy()
import sys $ !{sys.executable} -m pip install pandas-profiling
dfOther = df[df['Memo'].apply(returnCategory)=="Other"]
tokens['one_star'] = tokens.one_star + 1 $ tokens['five_star'] = tokens.five_star + 1
kimanalysis.listids('model-driver', extended=True)
df2 = ab_data[((ab_data.group == "treatment") & (ab_data.landing_page == "new_page")) | $                   ((ab_data.group == "control") & (ab_data.landing_page == "old_page")) ] $ df2.info()
cross_correlation = np.correlate(lyra_lightcurve.data['CHANNEL3'], $                                 lyra_lightcurve.data['CHANNEL4']) $ print(cross_correlation)
explotions['decade'] = explotions['Date'].map(stringDate_to_decade)
tweet_archive_enhanced_clean['rating_numerator'].sum()
pd.read_sql('SELECT * FROM customer_gt_90_dollars;', conn)
cb = pc.CrunchBase(key)
sessions.head()
amount.describe()
week19 = week18.rename(columns={133:'133'}) $ stocks = stocks.rename(columns={'Week 18':'Week 19','126':'133'}) $ week19 = pd.merge(stocks,week19,on=['133','Tickers']) $ week19.drop_duplicates(subset='Link',inplace=True)
overallYearBuilt = pd.get_dummies(dfFull.YearBuilt)
import json $ with open("credentials.json", "r") as file: $     credentials = json.load(file) $     toggl_cr = credentials['toggl'] $     APIKEY = toggl_cr['APIKEY']
from sklearn.linear_model import LogisticRegression $ from sklearn.metrics import confusion_matrix $
payload = { $     'location': [40.746054, -111.847987], $     'snappingOn': True, $     'dataSource': 'nhd'} $ json_dat = json.dumps(payload)
df.to_csv('data/Airline+Weather_data.csv',index=False)
sns.set(style="whitegrid") $ ax = sns.barplot(x=df['main_category'].value_counts(), y=df['main_category'].value_counts().index)
results.summary2()
mu = ret_aapl.mean().AAPL $ sigma = ret_aapl.std().AAPL $ r = 0.0160/252 $ mu, sigma, r
cityID = '3f3f6803f117606d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lubbock.append(tweet) 
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1').shape[0] $ convert_new = df2.query('group == "treatment" and converted == 1').shape[0] $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
def drop_index(cur, index_name): $     cur.execute(sql)
plt.hist(tickets.ticket_price, bins=100) $ plt.show()
dates = pd.DatetimeIndex(pivoted.columns) $ dates[(labels == 1) & (dates.dayofweek<5)]
df.head()
def sentlex_analysis(text): $     SWN = sentlex.SWN3Lexicon() $     classifier = sentlex.sentanalysis.BasicDocSentiScore() $     classifier.classify_document(text, tagged=False, L=SWN, a=True, v=True, n=False, r=False, negation=False, verbose=False) $     return classifier.resultdata
list(first_result.children)[1][1:-2]
latest_timelog = pd.DataFrame.from_dict(latest_time_entries)
df.plot()
df2[df2['landing_page'] == 'new_page'].count()[0]/df2.shape[0]
iowa_features = iowa_fset.features $ iowa_features[0].geometry
for k in ['linear', 'poly', 'rbf', 'sigmoid']: $     clf = svm.SVR(kernel = k) $     clf.fit(X_train, y_train) $     confidence = clf.score(X_test, y_test) $     print("Confidence for kernel - ", k, "is ", confidence)
team_names['noun_occurrences'] = [c[name.lower()] for name in names] $ team_names.sort_values(['noun_occurrences'],ascending=False).head()
twitter_archive_clean.shape[0]
twitter_archive_clean['dog_type'] = twitter_archive_clean['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True)
data['title_len'].value_counts()
dfcounts['created_date'] = pd.to_datetime(dfcounts['created_date']) $ dfrecent['created_date'] = pd.to_datetime(dfrecent['created_date'])
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs)
locale_name_local_hols=local_holidays['locale_name'].unique() $ print(locale_name_local_hols)
bruins['season'] = (pd.DatetimeIndex(bruins.date) - np.timedelta64(9,'M')).year
p_diffs= [] $ for i in range(10000): $     new_converted = np.random.binomial(length_of_new, p_equal_old_new, 10000)/length_of_new $     old_converted = np.random.binomial(length_of_old, p_equal_old_new, 10000)/length_of_old $     p_diffs = new_converted - old_converted
model.wv.vectors.shape
pd.date_range('2018 May 1st', '2018 Jul 3', freq = 'B')
zone_train.corr()
new_page_converted = np.random.binomial(1,p_new,n_new) $ new_page_converted.mean()
df_1.shape
regr = linear_model.LinearRegression() $ regr.fit(X_features, y_features) $ print (regr.coef_)
pickle.dump(nmf_cv_data, open('iteration1_files/epoch3/nmf_cv_data.pkl', 'wb'))
details.head()
pd.crosstab(df_protest.loc[:, 'Violent_or_non_violent'], df_protest.loc[:, 'Type']).T
page.text
data = pd.read_csv(datafile,index_col='Unique Key')
to_be_predicted_Day5 = 31.34448268 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
excel=pd.rea_excel("File Name")
null_p_old = df2.converted.mean() $ print(null_p_old)
data["e1_len"] = data["project_essay_1"].str.len() $ data["e2_len"] = data["project_essay_2"].str.len() $ data["ttl_len"] = data["project_title"].str.len() $ data["summ_len"] = data["project_resource_summary"].str.len()
df_A2=pd.read_csv("classA.csv") $ df_A2.index
df = pd.concat([df1, df2], sort=True)
autos["brand"].unique() $ top20=autos["brand"].value_counts(normalize=True, dropna=False).head(20) $ top20
df2 = df.drop(df.query('(landing_page == "new_page" & group == "control") or (landing_page != "new_page" & group != "control")').index)
df = pd.DataFrame(np.random.randn(12,4), index = d, columns = list('ABCD')) $ df
print('No of repeated entries: {}'.format(sum(df.duplicated()))) $ n_unique_users = df['user_id'].unique().shape[0] $ print (n_unique_users)
from nltk.corpus import stopwords $ stopword_list = set(stopwords.words("english")) $ def remove_stopwords(tokens): $     filtered_tokens = [token for token in tokens if token not in stopword_list] $     return filtered_tokens
tweet_archive_clean[tweet_archive_clean.name.isin(["O'Malley", "Gin & Tonic"])]
from bs4 import BeautifulSoup $ soup = BeautifulSoup(r.text, 'html.parser')
ser = pd.Series(np.arange(3.))
fda_drug_names = set(fda_drugs.DrugName.values)
asdf['Trump']=asdf['text'].apply(lambda x: 'TRUMP' in x.upper()) $ colsToUse=['wk','WkEnd','Trump','n'] $ aDict={'WkEnd':'max','n':'count','Trump':'sum'} $ wkTrump=asdf[['wk','WkEnd','Trump','n']].groupby('wk').agg(aDict)
soup = bs(response.text, 'html.parser')
tail = df.copy() $
!protoc object_detection/protos/*.proto --python_out=.
from sklearn.feature_extraction.text import CountVectorizer $ count = CountVectorizer(stop_words='english', $                         max_df=.1, $                         max_features=5000) $ X = count.fit_transform(df['review'].values)
print(''.join(re_split_raw[100:150])) $ print() $ print('|'.join(re_split_raw[100:150]))
trend_de.head()
reddit_df.head()
df2[df2['user_id'].duplicated()]
autos.groupby(['odometer_group'])['price'].mean()
y_pred_lr = lr.predict_proba(X_test[['avg_shifted_against', 'def_shift_pct']])[:, 1] $ fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_lr)
saved_model = ml_repository_client.models.save(model_artifact)
tweetsS17 = pd.read_sql_query("SELECT created_at, extracted FROM tweets_info;", connS17, parse_dates=['created_at'] ) $ tweetsS17['created_at'] = tweetsS17['created_at'].dt.tz_localize("UTC").dt.tz_convert("Europe/Berlin") $ print("Number of Tweets: %s" %len(tweetsS17)) $ tweetsS17.head()
table.to_csv(report_path+'\%s.csv' % day.strftime('%Y-%m-%d'), encoding='utf-8', index=False)
BroncosBillsPct['text30'] = BroncosBillsPct['text'].apply(lambda x: x[:30])
os.getcwd() $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS/2018-06-13') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $
tweet_summary_df = pd.DataFrame(tweet_df.sum(), columns=['tweet_count']).drop('total') $ tweet_summary_df['percentage'] = tweet_summary_df.tweet_count.div(tweet_df.count()).mul(100).round(1).astype(str) + '%' $ tweet_summary_df
varianceDailyReturns = dict( { 'Infy': infy_df.gain_perc.var(), 'Glaxo': glaxo_df.gain_perc.var(), $                           'BEML': beml_df.gain_perc.var(), 'Unitech': unitech_df.gain_perc.var()  } )
converted = df.groupby(['converted']).nunique().user_id[1] # Number of converted users $ converted_proportion = (converted / number_of_users) * 100  # Get converted users proportion $ converted_proportion
articles.to_csv('super.json')
plt.clf() $ NDVI_means = nbart_allsensors.NDVI.mean(dim =('x','y')) $ NDVI_means.plot() $ plt.show()
props.head(1)
def played(x): $     dr = x['ab'] + x['bb'] + x['hbp'] + x['sf'] $     return dr!=0 $ thesePlayed = played(baseball) $ thesePlayed.head()
tweet_archive_enhanced_clean['timestamp'] = pd.to_datetime(tweet_archive_enhanced_clean['timestamp']) $
df = pd.DataFrame.from_dict(last_12_months) $ df = df.set_index('date') $ df.head()
commits = pd.read_pickle('data/pickled/commits.pkl') $ commits.head()
p_new = df2.query('converted == "1"').count()[0]/df2.shape[0] $ new_page_convert = np.random.choice([0,1],n_new, p=(1-p_new, p_new))
df.set_index("stamp",inplace=True) $ df.head()
cnf_matrix[1:].sum() / cnf_matrix.sum()
result2[result2['dt_deces'].notnull()].shape
df_all_wells_wKNN.head()
train_df4 = train_df3.copy() $ train_df4['location-ll'] = [str(x) + ', ' + str(y) for x,y in zip(train_df4.latitude, train_df4.longitude)] $ del train_df4['latitude'] $ del train_df4['longitude']
e.__class__
stations = session.query(func.count(distinct(Measurement.station))).all() $ print(f"Number of stations: {stations}")
autos[["date_crawled", $        "last_seen", $        "ad_created", $        "registration_month", $        "registration_year"]].info()
def isapple(string): $     return string == 'iPhone' $ data.phone = data.phone.apply(isapple)
tUnderweight = len(df[df['bmi']< 18.5]) $ tUnderweight
df_us = df_parsed.query('country == "US"').reset_index() $ df_us.shape
idx2 = pd.IndexSlice $ health_data_row.loc[idx2[2013:2015, 1, 'Bob':'Guido'], 'Temp']
user_logs[user_logs.num_25 <0]
month2 = oanda.get_history(instrument_1, $                           start = '2018-2-1', $                           end = '2018-3-1', $                           granularity ='M10', $                           price = 'A')
n_old = df2[df2.landing_page =='old_page'].count()['user_id'] $ n_old
result_df[['CA','UK','US']]=pd.get_dummies(result_df['country']) $ result_df=result_df.drop('CA',axis=1) $ result_df.head()
top10_topics_1 = events_enriched_df[['event_id','topic_id','topic_name']].groupby(['topic_id','topic_name']).count() $ top10_topics_1.rename(columns={'event_id':'count_event_per_topic'}, inplace=True) $ top10_topics_1.reset_index(inplace=True) $ top10_topics_1.head(5)
test.Page.value_counts().shape
old_page_converted = np.random.choice([1,0], size = n_old, p = [0.1196,1-0.1196]) $ old_page_converted
dfEtiquetas["latitude"] = dfEtiquetas["place"].apply(lambda p: p["location"]["latitude"]) $ dfEtiquetas["longitude"] = dfEtiquetas["place"].apply(lambda p: p["location"]["longitude"])
sentiment_df["Date"]=dates $ sentiment_df.head()
groupby_x = df['y'].groupby(df['x']) $ round(groupby_x.describe(), 3)
datatest['rooms'] = datatest['rooms'].apply(lambda x: (float)((int)(x)))
len(df2.query('converted == 1')) / len(df2)
feature_cols = ['TV', 'radio', 'newspaper'] $ X = data[feature_cols] $ y = data.sales
df2[['ab_page2', 'ab_page']] = pd.get_dummies(df2['group'])
old_page_converted = np.random.choice([0, 1], size=n_old, p=[1-p_old, p_old]) $ p_old_sim = old_page_converted.sum()/len(old_page_converted) $ p_old_sim
model = models.HdpModel(corpus, id2word=dictionary)
cityID = 'ac88a4f17a51c7fc' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Portland.append(tweet) 
Difference_mean = (new_page_converted.mean() - old_page_converted.mean() ) $ Difference_mean
pd.Timestamp('now', tz='US/Eastern')
df_person.merge(df_grades, how="right", left_on = "id", right_on="person_id")
from pyspark.sql.functions import col $ usage_400hz_filter = usage_400hz.where(col("pk_ramp").isin({'B15', 'B23', 'B31', 'E09','E20', 'E24', 'H02', 'H04', 'H06'}) == False) \ $ .where(col("pk_ramp").isin({'B16', 'B20','B24', 'B28', 'B32', 'B36'}) == False) \ $ .where(col("actype").isin({'DASH 8-400', 'FK50', 'EMB 145', 'EMB-145'}) == False) \ $ .where(col("pk_ramp").isin({'H02', 'E17', 'D55', 'E17', 'G02', 'D18', 'B35', 'D04', 'D04', 'D53', 'D53', 'D07', 'D07', 'E09', 'D24', 'H03', 'E09', 'G05', 'E20', 'E20', 'G05', 'G08', 'G08', 'F04', 'F04', 'F07', 'F07', 'D14', 'B15', 'B31', 'H04', 'E18', 'E18', 'E18', 'D41', 'E18', 'D02', 'D02', 'D51', 'D51', 'D08', 'D08', 'D57', 'D57', 'E07', 'E07', 'G03', 'G03', 'D10', 'G06', 'G06', 'D43', 'G09', 'E24', 'D26', 'D43', 'H05', 'E24', 'G09', 'G09', 'B27', 'G09', 'F05', 'F05', 'F08', 'F08', 'D16', 'B17', 'E19', 'E19', 'D03', 'D03', 'H06', 'H01', 'D22', 'D05', 'B23', 'E05', 'E05', 'E08', 'E08', 'G04', 'G04', 'E22', 'E22', 'G07', 'D12', 'D49', 'D47', 'B13', 'D47', 'F03', 'F03', 'D28', 'H07', 'F06', 'F06', 'F09', 'F09'}) == True) $
temps_df.loc['2018-05-02'].index
matthew = pd.DataFrame(TWEETS) $ matthew['date'] = matthew.created_at.apply(lambda x: x.date()) $ print(len(df))
len(train_data[train_data.fuelType == 'cng'])
df.num_comments.median()
negatives = NB_results.loc[NB_results.sentiment == 'N'] $ sample_size = 10 $ for tweet in negatives.tweet.sample(sample_size): $     print(tweet)
groceries.drop('apples')
sn.boxplot(data = monthly_gain_summary, palette="Set2")
season_team_groups = nba_df.groupby(["Season", "Team"], as_index = False)
bad_dtype_df = pd.read_csv('multi_col_lvl_output.csv', header=[0, 1], index_col=[0, 1, 2, 3, 4, 5], $                             skipinitialspace=True).head(3) $ display(bad_dtype_df) $ display(index_level_dtypes(bad_dtype_df))
location = pd.read_csv('data_old/AdWords_API_Location_Criteria.csv') $ location[location['Name'] == 'Bangkok'].head()
df_2 = pd.read_csv('./scraping_results.csv')
with open('files/approved users.txt') as fin: $     approved_users = [] $     for line in fin: $         approved_users.append(line.rstrip('\n')) $ print(approved_users)
df.drop(['seller','offerType','abtest','dateCrawled','nrOfPictures','lastSeen','postalCode','dateCreated'], axis='columns', inplace=True)
saveToFile = os.path.join(PROCESSED_PATH, 'Opportunities_with_Potential_Accounts.csv') $ zero_rev_acc_opps.to_csv(saveToFile, index = False)
baseball.hr.rank(method='first')
df = db.call_non_select_stored_proc(DBConnections.FRAME_USER, '[Frame_User].[FinFC].[USP_HHdataVarAnalysis]', params=(4463, '2017-10-01', '2018-01-01', 'Forecast', 'all'), print_sql=True)
averages = means.reset_index(drop=False) $ averages
from sklearn.tree import DecisionTreeClassifier $ DT = DecisionTreeClassifier(criterion="entropy", max_depth = 4) $ DT.fit(X_train,y_train) $ DT
df_users_6.shape
df_clean = ea.copy()
cols = ['A','B'] $ pd.get_dummies(df[cols])
nodereader.head()
ts = pd.Series(np.random.randn(9) * 10 + 500, index=timestamps)
for index, row in groupedvalues.iterrows(): $     print(row.Memory)
doesnt_meet_credit_policy.head(rows=2)
g8_aggregates = g8_groups.agg({ $     'area': ['min', 'max'], $     'population': ['mean', 'std'], $     'gdp': ['mean', 'std'], $ })
ldamodel = models.ldamodel.LdaModel(doc_term_mat, $                                     num_topics=num_topics, $                                     id2word=dict_tokens, $                                     passes=25, alpha=1)
str(datetime.datetime.now())
df2['account_created'][0].month
print keywords(data_df.clean_desc[23], ratio=0.5)
census['GEOID_tract']=census.apply(lambda x: x['GEOID'][:11],axis=1)
model.fit(X_train, y_train)
turnaround_planes_df.count()
html = browser.html $ image_soup = BeautifulSoup(html, 'html.parser')
cm = our_nb_classifier.conf_matrix(our_nb_classifier.data_results['out_y'], $                             our_nb_classifier.data_results['out_n']) $ print(ascii_confusion_matrix(cm))
convo1.shape
for f in potentialFeatures: $     related = df['overall_rating'].corr(df[f]) $     print("%s: %f" % (f,related)) $
varx=np.var(x) $ vary=np.var(y) $ print(varx) $ print(vary)
df2['ab_page']=pd.get_dummies(df2['group'])['treatment']
optimal_weights, optimal_mus, optimal_sigmas = calculate_frontier(returns.values)
for f in read_in["files"]: $     fp = getattr(processing_test.files, f) $     print(json.load(open(fp.load())))
topics_doc_count = docs_by_topic.count()[['TopicNum']] $ topics_doc_count
test_features = bag_of_words_vectorizer.transform(test_corpus)
temp_dict.keys()
new_df_dummies = pd.get_dummies(new_df, columns=['ordered']) $ new_df_dummies.head()
df2[df2.landing_page == 'new_page'].shape[0]/df2.shape[0]
from sklearn.cross_validation import train_test_split $ x_train, x_test, y_train, y_test = train_test_split(x_train_data, y_train_data, test_size=0.1, random_state=1337)
students[(students.gender == 'F') & (students.weight >= 140)]
grouped = df.groupby(['product_type', 'state'])['price_doc'].size() $ grouped
import statsmodels.api as sm   #stats model is imported to implement OLS and z-test methods $ convert_old = len(df2.query('landing_page == "old_page" & converted == 1')) $ convert_new = len(df2.query('landing_page == "new_page" & converted == 1')) $ convert_old,convert_new
cruise_data_file = 'https://alfresco.oceanobservatories.org/alfresco/d/d/workspace/SpacesStore/976c0631-48c8-4add-9d04-c95a066f56c4/ar18a009.asc' $ cruise_data = pd.read_table(cruise_data_file, delim_whitespace=True, header=None, skiprows=1) $ cruise_data = cruise_data.rename(columns={0:'Pressure', 1:'Temperature', 13:'Salinity'}) $ cruist_data.head
def collect_sents(matcher, doc, i, matches): $     match_id, start, end = matches[i]  # indices of matched term $     span = doc[start:end]              # extract matched term $     print('span: {} | start_ind:{:5} | end_ind:{:5} | id:{}'.format( $         span, start, end, match_id))
new_crs.mean()
calls = pd.read_csv('./data/CallsRingCentral.csv')
max_value = rankings['cur_year_avg'].idxmax() $ rankings.iloc[max_value]
print(lr.intercept_)
num_vars = list(df.dtypes[df.dtypes=='int64'].index) + list(df.dtypes[df.dtypes=='float64'].index)
qW = ps.queen_from_shapefile(os.getenv('PUIDATA')+'/pumashplc.shp')
posts.head()
yc.eval({y_true: uu})
corn.size().index.values
p_treatment = df2.query('group=="treatment"').converted.mean() $ p_treatment
sns.countplot(data=data, x='OutcomeType',hue='Breed', palette="Set3")
df2=df.select("name", "favorite_color") $ df2.show()
temps_df.loc['2016-04-05']
model.most_similar("queen")
Helper().what_is_on_the_fly_selector()
import statsmodels.api as sm
df_arch_clean[df_arch_clean.text.str.contains(r"([0-9]+\.[0-9]*\/\d+)")]
trip_index_25_1 = suspects_with_25_1['timestamp'][suspects_with_25_1['timestamp'] - suspects_with_25_1['timestamp'].shift(1) > "12:00:00"].index.values
df_subset['Total Est. Fee'].head()
import test_package.print_hello_direct
df_imputed_mean_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.mean())
pickle_in = open("C:/Users/Fabian/Documents/FinancialForecasting/Data/X_score-2018725.pickle", "rb") $ X_score = pickle.load(pickle_in)
tr_old = df.query("group == 'treatment' and landing_page == 'old_page'") $ con_new = df.query("group == 'control' and landing_page == 'new_page'") $ print(len(tr_old) + len(con_new))
def tokenize_text(text): $     word_tokens = nltk.word_tokenize(text) $     tokens = [token.strip() for token in word_tokens] $     return tokens
n_new = ab_df2.query('landing_page == "new_page"').shape[0] $ n_new
graf_train, graf_test=train_test_split(graf, test_size=.33, random_state=42)
other_inter_recr = pd.read_csv(folderData + 'interactions_recruiter.csv')
df2[df2.duplicated("user_id")]
df2[df2['group'] == "control"]['converted'].mean()
jaja.values
m = pd.read_sql_query(QUERY, conn) $ m
y=df['loan_status'].values $ y[0:5]
lr = 6e-4 $ learn.fit(lr, 20, cycle_len=1, use_clr=(10,10))
for i in range(100000): $     results1.set_value(i,'correct',outlier(results1.get_value(i,'max1stdetectwssc'),results1.get_value(i,'max1stdetectwssd'),results1.get_value(i,'max1stdetectwsse'),results1.get_value(i,'max1stdetectwssf')))    $
archive_copy['tweet_id']= archive_copy['tweet_id']. astype('str') $ type(archive_copy['tweet_id'].iloc[0])
for label,val in ser7.iteritems(): $     ser7.set_value(label, val+2) $     print(val,label)
import datapackage $ package = datapackage.Package()
year2 = driver.find_elements_by_class_name('yr-button')[1] $ year2.click()
dfs[index_max]['Time'][dfs[index_max]['Outside Temperature'] == dfs[index_max]['Outside Temperature'].max()]
df.reset_index(inplace=True) $ df.groupby('dog_type')['timestamp'].describe()
n_old = df2.query('group == "control"').user_id.count() $ n_old
plt.hist(np.log(threeoneone_census_complaints['median_income_new']+1),bins=100) $ plt.show()
print('The In/Out counter indicates the order in which cells have been executed.')
package.infer('./solutions/data/**/*.csv')
march_2016.start_time
numPurchP = train.groupby(by='Product_ID')['Purchase'].count().reset_index().rename(columns={'Purchase': 'NumPurchasesP'}) $ train = train.merge(numPurchP, on='Product_ID', how='left') $ test = test.merge(numPurchP, on= 'Product_ID', how='left')
df_agg_op_rand.head()
for fashion_index, label in enumerate(model.classes_): $     fashion['%s-proba' % label] = np.round(probas[:, fashion_index], 4)
SCN_BDAY.birthdate.describe()
comments = [x.text for x in soup.find_all('a', {'class':'bylink comments may-blank'})]
channel_meta['topic_ids_json'] = json.loads(channel_meta['topic_ids']) $ channel_meta['topic_ids_json']
run txt2pdf.py -o "NEW YORK HOSPITAL MEDICAL CENTER OF QUEENS  Sepsis.pdf"   "NEW YORK HOSPITAL MEDICAL CENTER OF QUEENS  Sepsis.txt"
selfharmm_topic_names_df = mf.compile_topics_df([nmf_cv, lsa_cv, lda_cv], [nmf_tfidf, lsa_tfidf, lda_tfidf], cv_fitted, tfidf_fitted, 20)
df.signup_app.value_counts()
conn.upload('https://github.com/sassoftware/' $             'sas-viya-programming/blob/master/data/class.csv')
eval_df = create_evaluation_df(predictions, test_inputs, HORIZON, y_scaler) $ eval_df.head()
categories = list(train_users.columns.values) $ categories = categories[4:len(categories)-1] $ categories.remove('age') $ categories.remove('gender') $ categories
url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ browser.visit(url) $ html = browser.html $ soup = BeautifulSoup(html, "html.parser")
custom = pd.get_dummies(auto_new.Custom) $ custom.head()
talks.text[0]
fig,axes = plt.subplots(1, 2, figsize = (16,4), sharey= True) $ axes[0].plot_date(x=obama.created_at, y = obama.n_uwords,linestyle = '-',marker='None') $ axes[1].plot_date(x=trump.created_at, y = trump.n_uwords,linestyle='solid',marker='None') $ plt.savefig("fig/n_uword_comparison.png")
pivoted_data.cumsum().plot(figsize=(10,10))
ax = df['DepDelay'].hist(bins=500) $ ax.set_xlim(-100, 500) $ plt.title('Num flights per delay bin') $ plt.yscale('log') $
df_new=df2.query("landing_page=='new_page'") $ n_new=len(df_new) $ n_new
order_data['OrderPeriod'] = order_data.created.apply(next_weekday)
users.info()
def format_cust_campaign(x): $     file = re.split("[_\-.]",x.lower()) $     file = file[1:-1] $     return file
df.min()
df2 = df.copy() $ df2.drop(df2.query("group=='treatment' & landing_page == 'old_page'").index,inplace=True) $ df2.drop(df2.query("group == 'control' and landing_page == 'new_page'").index, inplace=True) $ df2.head()
autos["registration_year"].describe()
log_preds,y = learn.TTA() $ accuracy(log_preds,y)
bwd = df[['store_nbr']+ holidays].sort_index().groupby('store_nbr').rolling(7, min_periods=1).sum() $ fwd = df[['store_nbr']+ holidays].sort_index(ascending=False).groupby('store_nbr').rolling(7, min_periods=1).sum()
FREEVIEW.plot_number_of_fixations(raw_fix_count_df, option='eyetracker')
p_new=df2['converted'].mean() $ p_new
Train_extra['ID'].head()
print(df_by_donor.head())
tags = db.get_tags() $ tags.head()
df_new['interaction1'] = df_new['CA']*df_new['new_page'] $ df_new['interaction2'] = df_new['UK']*df_new['new_page'] $ loginter = sm.Logit(df_new['converted'],df_new[['intercept','interaction1','interaction2','new_page','CA','UK']]) $ results2=loginter.fit() $ results2.summary()
session.query(func.count(measurement.date)).all()
s_n_s_epb_one.rename(columns = {0:"Count"},inplace=True)
q_agent_new.scores += run(q_agent_new, env, num_episodes=50000)  # accumulate scores $ rolling_mean_new = plot_scores(q_agent_new.scores)
import scipy.stats as stats $ x = df.contain_kw $ y = df.successful $ stats.pearsonr(x, y)
temp_dir.cleanup() # clean up of temporary data
doc = uso17_coll.find_one()
df_log.user_id.nunique()
text = data["name"] $ vect = CountVectorizer(stop_words="english", min_df=3) $ X = vect.fit_transform(text) $ X = pd.DataFrame(X.toarray(), columns = vect.get_feature_names()) $
print("Job status for " + jobId + ": " + sqlClient.get_job(jobId)['status'])
df_to_interp.interpolate(method='index')  # notice how the data obtains the "right" values
hist = model.fit(Xtr_scale, Ytr_scale, epochs=20, $                  validation_data=(Xts_scale,Yts_scale), $                  verbose=0)
pl.hist(yc_new4['tipPC'], bins=30) $ pl.ylabel('N') $ pl.xlabel('tipPC') $ pl.title('Distribution of Tips as a percentage of Fare, under 100% and over 1%') $ print('The first moment is 20.44 and the second moment is 9.04')
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean()
input_data.head(10)
print('Sample:') $ print('Polarity:', sampleDF.Polarity.mean()) $ print('Subjectivity:', sampleDF.Subjectivity.mean())
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "CBS News"]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "CBS News"]["tweet_vader_score"],fit_reg=False,marker = "+",scatter_kws={"color":"skyblue","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (CBS News)",fontsize = 12) $ plt.savefig('Sentiment_CBS.png')
data = data.drop_duplicates(subset=['name'], keep=False)
wk_output.to_csv('all_business_licence_feedback.csv')
df_inter_2 = pd.DataFrame({'A': [1, 2.1, np.nan, 4.7, 5.6, 6.8], $                            'B': [.25, np.nan, np.nan, 4, 12.2, 14.4]}) $ df_inter_2
print(prec_long_df['date'].min(), prec_long_df['date'].max())
df2 = pd.read_csv('ab_data_cleaned.csv')
prob_kNN5 = kNN5.predict_proba(Test) $ prob_kNN5
to_be_predicted_Day3 = 14.52670269 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
tfav.plot(figsize=(16,10), label="Likes", legend=True) $ tret.plot(figsize=(16,10), label="Retweets", legend=True)
df_pol['domain'].value_counts().head(20).plot(kind='bar')
for table in engine.table_names(): $     print(table) $     print(Track)
data=data.dropna(subset=['lat-lon']).reset_index(drop = True)
df_dem.head(100)
cohort_churned_df.to_csv('cancelation_churned_subscribers.csv')
weather_yvr.head()
sc = SparkContext(appName="PythonSparkStreaming")  $ sc.setLogLevel("WARN") 
df_western.genres.str.contains(r'Western').sum() == df_western['id'].count()
cust_data.iloc[:,3:7:2] # 4th, 6th column. 
IMDB_dftouse_dict = {feature: IMDB_dftouse[feature].values.tolist() for feature in IMDB_dftouse.columns.values} $ fp = open("IMDB_dftouse_dict.json","w") $ json.dump(IMDB_dftouse_dict, fp) $ fp.close()
sentiments.groupby('listing_id')['sentiment_score'].mean()[:5]
prop = df2.agg("converted") $ prop.sum()/unique()
page.is_categorypage()
secondary_processing.files
honeypot_input_data = "2018-01-26-mhn.log"
ts = pd.Series(np.random.randn(3), periods) $ ts
for k, v in co_occurence_on_top50.items(): $     ds_json = {k: v} $     filename = '../../datasets_reco/reco_json/{}.json'.format(k) $     with open(filename, 'w') as f: $         json.dump(ds_json, f)
dtrump = miner.mine_user_tweets(user='realDonaldTrump', max_pages=10) $ print (dtrump[0]['text']) $ dtrump_df = pd.DataFrame(dtrump)
properati[properati['place_name'] == "Bs.As. G.B.A. Zona Norte"]
df['Complaint Type'].groupby(by=df.index.month).value_counts()
df = df.withColumn("post_creation_date", df["post_creation_date"].cast(TimestampType()))
!gunzip -c GCA_900186905.1_49923_G01_feature_table.txt.gz | head -n 4
df.describe()
3 // 2.0
len(tweet_vector.get_feature_names())
np.count_nonzero(x < 6)
twitter_archive_clean = twitter_archive_enhanced.copy() $ image_prediction_clean = image_predictions.copy() $ tweet_json_clean = tweet_json.copy()
dfNiwot.mean()
print(summarizer.summarize(gen2str, words=200))
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05))) $
cur.execute(query)
size_control= df2.query('group == "control"').user_id.nunique() $ size_treatment = df2.query('group == "treatment"').user_id.nunique() $
learn.sched.plot_loss()
vio.head(5)
releases = pd.read_csv('../input/jail_releases.csv') $ bookings = pd.read_csv('../input/jail_bookings.csv')
model_df['loss_gain'] = model_df.close - model_df.open $ model_df['spread'] = model_df.high - model_df.low
pd.Period('10/23/2016', freq = 'A').end_time
session.query(Station.station).all()
bitinfocoins = ['eth', 'ltc', 'xrp', 'bch', 'dash', 'xmr', 'etc', 'rdd', 'zec', 'doge'] $ for coin in bitinfocoins: $     all_coins_df = all_coins_df.merge(my_cryptory.extract_bitinfocharts(coin), on="date", how="left") $ all_coins_df
df[df['rating'] <= 14]['rating'].describe()
day_of_week15 = uber_15["day_of_week"].value_counts().to_frame()
(df2.query('group == "control"')['converted']==1).mean()
question_1_dataframe_sorted = question_1_dataframe.reset_index().sort_values(['borough','count'], ascending=False).set_index(['borough','complaint_type']) $ question_1_dataframe_sorted
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
areas = pd.read_csv(areas_csv_path)
tweet_json.sample(5)
df.to_csv('twitter_archive_master.csv', encoding = 'utf-8')
col['date_time'] = col.index.map(str) + " " + col["TIME"] $ col['date_time'] = pd.to_datetime(col['date_time']) $ col['date_time']=pd.to_datetime(col.date_time.dt.date) + pd.to_timedelta(col.date_time.dt.hour, unit='H')
for n in [0.01, 0.001]: $     print("Words with importance greater than {}".format(n)) $     imp_words = np.argwhere(rfc_feat_sel.feature_importances_ >= n) $     for i in imp_words: $         print(vect.get_feature_names()[i[0]])
sub1.shape
prcp_df = pd.DataFrame(prcp_scores, columns = ['Date', 'prcp']) $ prcp_df.set_index('Date' , inplace=True) $ prcp_df.head()
less_hacker_list = list(less_hackers['hacker_id'].unique())
n_new = df2.query('landing_page == "new_page"').count()[0] $ n_new
cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=cvEvaluator) $ cvModel = cv.fit(trainingData)
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = "smaller") $ print("The computed z-score is: {} and the p-value is: {}".format(z_score, p_value))
%timeit pd.eval('df1 + df2 + df3 + df4')
df['label'] = df[forecast_col].shift(-forecast_out)
p_null_new = np.random.binomial(1, p_new, 10000) $ p_null_old = np.random.binomial(1, p_new, 10000) $ p_diffs = p_null_new - p_null_old $ print(p_diffs.mean())
churned_df['end_date'] = pd.to_datetime(end_date).strftime('%Y-%m')
new_page_converted = np.random.binomial(1, p_new, n_new)
df.head()
g2_filtered = g_kick_data.filter(lambda x: len(x) <= 1).sort_values(['blurb','launched_at']) $ len(g2_filtered) $
day_change = [] $ for close2, close1 in zip(close_day2, close_day1): $     day_change.append(close2 - close1) $ print("Largest consecutive 1 day change in 2017: " + str(round(max(day_change), 2)))
right = pd.DataFrame({'key' : ['foo', 'foo'], 'rval': [4,5]}) $ right
most_recent_investment.head()
pubs = db.get_publications() $ feeds = db.get_feeds()
df['text'] = df['text'].astype(str).str.lower() $ Counter(" ".join(df["text"]).split()).most_common(100)
googClose.plot(alpha=0.5, style='-') $ googClose.resample('BA').mean().plot(style=':') $ googClose.asfreq('BA').plot(style='--'); $ plt.legend(['input', 'resample', 'asfreq'], loc='upper left')
percent_quarter = (qtrclosePrice / qtrclosePrice.shift(4)) - 1
cryptos.loc[0]
train_agent_and_comment.to_csv('train_agent_and_comment.csv') $ test_agent_and_comment.to_csv('test_agent_and_comment.csv')
year_ago_ppt = dt.date.today() - dt.timedelta(days =365) $ year_ago_ppt $
df_new['intercept']=1 $ lm=sm.Logit(df_new['converted'],df_new[['intercept','UK','US']]) $ res=lm.fit() $ res.summary()
p_old = df2[df2['converted'] == 1]['user_id'].count() / df2['user_id'].count() $ p_old
p = data_s[data_s['isvalid']>0].groupby('customer')['date'].agg({"first": lambda x: x.min(),"last": lambda x: x.max()}).reset_index()
p_diffs = [] $ for _ in range(10000): $     new_page_convert = np.random.binomial(n_new, p_new)/n_new $     old_page_convert = np.random.binomial(n_old, p_old)/n_old $     p_diffs.append(new_page_convert - old_page_convert)
xml_in_sample.shape
result6 = sm.ols(formula="NFLX ~ GSPC", data=tbl4).fit() $ result6.summary()
new_page_conv = np.random.binomial(n_new, p_new,  10000)/n_new $ old_page_conv = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_page_conv - old_page_conv 
df_twitter_extract_copy['tweet_id'] = df_twitter_extract['tweet_id'].astype(str)
import  statsmodels.api  as sm $ logit = sm.Logit(df_regression['converted'],df_regression[['intercept','ab_page']]) $ result=logit.fit()
output_file = root_folder + 'temperature_spectrum.csv' $ new_df.to_csv(output_file, header=False)
sample_size = compute_sample_size(prop1 = 0.04, min_diff = 0.01, significance = 0.05, power = 0.95) $ print('sample size required per group:', sample_size)
df15 = pd.read_csv('2015.csv')
menus_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/menus.csv')['Body'].read().decode('utf-8') $ menus = pd.read_csv(StringIO(menus_csv_string), header=0)
compound_sub1 = compound_wdate_df0.append(compound_wdate_df1)
!cp ./stockdemo-model/myenv.yml ./
ks = scipy.stats.ks_2samp(df.ageM, df.ageF) $
(tech/tech.ix[0]-1).plot()
conn.commit()
control_group_user_count = df2[df2['group'] == 'control']['user_id'].count() $ converted_control_user_count = df2[(df2['group'] == 'control') & (df2['converted'] == True)]['user_id'].count() $ p_control_converted = converted_control_user_count / control_group_user_count $
matches = re.findall('\d+', $     'the recipe calls for 10 strawberries and 1 banana') $ print(matches) $
ways_tags.head()
expAndCoinByUser = firstWeekUserMerged.groupby('userid')["exp","coin"].sum().reset_index() $ print(expAndCoinByUser.head(5)) $ print(expAndCoinByUser.shape)
df_twitter_archive_master.groupby(['p1']).mean()['favorite_count'].sort_values(ascending=False).head(5) $
writers = ['Charles M. Blow','David Brooks','Frank Bruni','Roger Cohen','Gail Collins', $           'Ross Douthat','Maureen Dowd','Thomas L. Friedman','Michelle Goldberg','Nicholas Kristof', $           'Paul Krugman','David Leonhardt','Andrew Rosenthal','Bret Stephens'] $ writers = [x.upper() for x in writers] $ print(writers)
user_group = df.groupby('user') $ size = user_group.size() $ size.sort_values(inplace=True) $ size.plot() $
df_ad_airings_filter_3.head(2)
ls ../../outputs-git_ignored/gpu_mod_run_1
store[['StoreType_a', 'StoreType_b', 'StoreType_c', 'StoreType_d']] = pd.get_dummies(store['StoreType']) $ store[['Assortment_a', 'Assortment_b', 'Assortment_c']] = pd.get_dummies(store['Assortment']) $ store.drop('StoreType', axis=1, inplace=True) $ store.drop('Assortment', axis=1, inplace=True)
df = sqlContext.jsonFile("file:/path/file.json")
df.group.value_counts()
print(soup.get_text()[:500])
bins = pd.cut(train['unique_items'], [i for i in range(1,101,10)]) $ bin2 = pd.cut(train['mean_cost'], list(np.logspace(0,6.1,10))) $ pros = train.groupby([bins,bin2])['project_is_approved'].agg('mean')
df_clean = df_clean.loc[df_clean['expanded_urls'].str.extract(pat=r'(photo)', expand=False).notnull(), :]
logit_mod2=sm.Logit(df_new['converted'],df_new[['intercept','UK','US']]) $ results2=logit_mod2.fit() $ results2.summary2()
s3 = pd.Series(['red', 'green', 'blue'], index=[0, 3, 5]) $ s3
weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
pd.isnull(df)                     $ df.drop(df.tail(8).index,inplace=True)    #droping the last 8 rows $ df = df.drop(columns = ['Md'])    #removing Md column
y = list(train_1m_ag.is_attributed) $ X = train_1m_ag.drop(['is_attributed'],axis=1)
user = user.reset_index() $ user.head(3)
testing.info()
violation_counts[:10].plot(kind='bar')
df['Complaint Type'].value_counts().head(10)
with open('data/PN_and_QTY.csv', 'wb') as f: $     w = csv.DictWriter(f, ['PN', 'QTY']) $     w.writeheader() $     w.writerows([{'PN': key, 'QTY': val} $                  for key, val in pn_and_qty_no_duplicates.items()])
df[race_cols].head()
knn_model = KNeighborsClassifier(n_neighbors=7) $ knn_model.fit(X_train, y_train)
df_protest.TownCity_Name.unique()
p = pd.Period('2012', freq='A')
df.loc[0]['skills'], df.loc[closest_id]['skills']
logreg = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
leadPerMonth = segmentData[['opportunity_month_year', 'lead_source']].pivot_table( $                                 index='opportunity_month_year', $                                 columns='lead_source', aggfunc=len)
neigh = KNeighborsClassifier(n_neighbors = 9).fit(X_train,y_train) $ yhat_knn = neigh.predict(X_test)
sales_diff = sales_change1.merge(sales_change2, on = 'Store Number') $ sales_diff.head() $ sales_diff['2016_sales'] = total_sales['2016_q1_sales'] $ sales_diff.dropna(inplace = True) $ sales_diff.tail()
freq = all_acct_ids.stat.freqItems(['sales_acct_id'], 0.25) $ freq.collect()[0]
print("Number of Techniques in Mobile ATT&CK") $ techniques = lift.get_all_mobile_techniques() $ print(len(techniques)) $ df = json_normalize(techniques) $ df.reindex(['matrix', 'id','tactic', 'technique', 'tactic_type','contributors'], axis=1)[0:5]
(null_vals > p_observed).mean()
import tweepy $ auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
matt2 = dta.t[(dta.b==40) & (dta.c==2)]
dataframe = dataframe.sort_index()
df11 = pd.read_csv('2011.csv')
measure.info()
plt.rcParams['figure.figsize'] = (15, 5) $ timezones.plot(kind='bar') $ plt.xlabel('Timezones') $ plt.ylabel('Tweet Count') $ plt.title('Top 10 Timezones tweeting about #Trump')
imagehash.hex_to_hash('e6e6c9791516c2c3') - imagehash.hex_to_hash('e6e6c9791516c2e2')
ratings = ratings.unstack() $ ratings = pd.concat([ratings.numerator.mean(axis=1).rename('rating_numerator'), $                      ratings.denominator.mean(axis=1).rename('rating_denominator')], $                     axis = 1)
df = data.groupby('Date').sum()
%%writefile file32.txt $ foo foo quux labs foo bar quux
open_idx = afx['dataset']['column_names'].index('Open') $ print('The index of the "Open" column in the dataset is {}\n'.format(open_idx)) $ afx_data_len = len(afx['dataset']['data']) $ open_values = [entry[open_idx] for entry in afx['dataset']['data'] if entry[open_idx]] $
w = 'zionist' $ model.wv.most_similar (positive = w)
%load "solutions/sol_2_34b.py"
tweets_clean.favorites_count.mean()
Magic.__dict__['__repr__'].__get__(m)
not_numbers = data_read.genre_ids.astype(str).apply(lambda x: x.isnumeric()) == False $ data_read["genre_ids"][not_numbers.values].sample(10)
dr.save_screenshot('/Users/ryadom/seminars_spring2018/seminars/11/sc.png')
census_finaldata.sample(100).plot()
train = pd.read_json('./data/train.json')
def keras_rnn_predict(samples, empty=human_vocab["<pad>"], rnn_model=m, maxlen=30): $     data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty) $     return rnn_model.predict(data, verbose=0)
train_set = [(find_features(tokens, word_features), cat) for tokens, cat in zip(X_train.Text_Tokenized, y_train)] $ test_set = [(find_features(tokens, word_features), cat) for tokens, cat in zip(X_test.Text_Tokenized, y_test)]
verify_response = requests.get(url, auth=auth)
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs))\ $     .filter(Measurement.station == 'USC00519281').all()
feature_freq.to_csv('./data/feature_freq.csv', encoding='utf-8')
odometer_counts = autos['odometer_km'].value_counts() $ print(odometer_counts.sort_index(ascending=True))
plt.hist(df_elect['Polarity'], bins=15) $ plt.title('Tweet #Election2018') $ plt.show()
q2 = "select count(distinct s.id_nda) \ $     from icu_sensor_24 s \ $     where s.id_measure_type in (10102, 10120, 11, 12, 14, 15) \ $     and s.dt_cancel = ''" $ df_from_query(conn, q2)
df2['ab_page'] = df2.apply (lambda row: label_abpage (row),axis=1) $ df2['intercept'] = 1 $
print("Probability of control group converting:", $       df2[df2['group']=='control']['converted'].mean())
requests.get(saem_women)
built_df = built_df.loc[built_df['category'] != ""][:20].reset_index() $ built_df
ts.asfreq('B')
bigdf.to_hdf('Combined_Comments.h5', $              'comments', $              format='table', $              mode='w')
df_raw = pd.read_feather(file_name)
def YearColumn(text): $     if isinstance(text,str): $         time1 = parser.parse(text) $         return time1.year $     return np.nan
X_svd = np.hstack((id_dense,blurb_SVD2, goal_dense, duration_dense))
full_dataset.to_excel('rolling_dataset.xlsx')
sns.regplot(x=dfz.rating_numeric, y=dfz.favorite_count)
new_texas_city["Measurement_date"] = pd.to_datetime(new_texas_city["Measurement_date"])
appendMe = "\nThis is a new line" $ appendFile = open("exampleFile.txt","a") $ appendFile.write(appendMe) $ appendFile.close() # Don't forget to close the file
pca=decomposition.PCA() $ stocks_pca_t4= pca.fit_transform(stocks_pca_m4)
n_old = df2[df2['group'] == 'control']['user_id'].count() $ print('Elements of treatment group n_old: ',n_old)
google_username = "vivek.menon@jptbwa.com" $ google_password = "Juniper.park1" $ nfl = TrendReq(google_username, google_password) $ nflcont = TrendReq(google_username, google_password)
loan_requests1[loan_requests1.fk_user==137886]
df_count_clean["tweet_id"] = df_count_clean["tweet_id"].astype(str)
counts.plot.barh()
transformed_six_month.count()
new_misalign = df.query('landing_page == "new_page" & group != "treatment"').count()[0] $ old_misalign  = df.query('landing_page == "old_page" & group != "control"').count()[0] $ print(new_misalign + old_misalign)
def getlastid(my_soup): $     return my_soup.find(id=re.compile("thing"))['id'][6:]
%%time $ svc = SVC(random_state=20, C=10, decision_function_shape='ovo', kernel= 'rbf') $ svc.fit(X_tfidf, y_tfidf) $ svc.score(X_tfidf_test, y_tfidf_test)
from selenium import webdriver $ from selenium.webdriver.support.ui import Select $ from selenium.webdriver.common.by import By $ from selenium.webdriver.support.ui import WebDriverWait $ from selenium.webdriver.support import expected_conditions as EC
for item in bottom_three: $     print('{} has a std of {}.'.format(item[0], item[1]))
df1 = pd.read_csv("{}MN_17_3616_MJ_Table2.csv".format(data_dir)) $ df2 = pd.read_csv("{}MN_17_3616_MJ_Table4.csv".format(data_dir))
mit.date = pd.to_datetime(mit.date)
control_df = df2.query('group == "control"') $ control_pro = control_df.query('converted == 1').user_id.nunique() / control_df.user_id.nunique() $ control_pro
df.party.value_counts()
tweet_json_clean['followers_count'].sample(3)
! head -n 5 ../../data/msft.csv # OS/Linux $
res_val = gridCV.predict_proba(X_val) $ res_val = res_val[:,-1] $ res_val[res_val>0.45] = 1 $ res_val[res_val!=1] = 0 $ print(res_val)
df.describe()
first_words.saveAsTextFile("file:/path/file")
df_twitter_copy[df_twitter_copy.rating_numerator == '3 1']
area = pd.Series({'Alaska': 123234, 'Texas': 234523, $                  'California': 457623}, name='area') $ population = pd.Series({'Texas': 234523, 'California': 457623, $                 'New York': 24523}, name='population')
s.get('d') # ---> Returns '13'
cityID = '813a485b26b8dae2' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Albuquerque.append(tweet)  
df8 = df[df['hired'] ==0] $ df8.groupby('category')['position','hourly_rate','num_completed_tasks'].agg({'median','mean','min','max'}).reset_index() \ $ .rename(columns={'category': 'category', 'position': 'position_stats_not_hired', \ $                     'hourly_rate':'hourly_rate_stats_not_hired',  'num_completed_tasks':'num_completed_tasks_stats_not_hired'}) $
autos['date_crawled'].str[:10] $ autos['date_crawled'].value_counts().sort_index(ascending=False).head(20)
pred1 = pd.read_csv("prob1.txt") $ pred2 = pd.read_csv("prob2.txt").set_index('id') $ pred3 = pd.read_csv("prob3.txt").set_index('id')
my_data = sc.textFile(filePath) $ my_data.foreach(lambda line: if '.txt' in line: txt_count.add(1))
df[(df.full_sq>10)&(df.full_sq<1500)] $ df.query('full_sq>10 and full_sq<1500') $
plt.plot(precipitation_df["hw_date"],precipitation_df["prcp"]) $ plt.show()
insert = users.insert() $ print (insert) $ insert = users.insert().values(name = 'kim', fullname = "Anonymous kin") $ print (insert)
learn.freeze_to(-2)
df = file.to_dataframe(mode='pivot') $ df.head()
rh_frac = rh / 100 $ rh_frac.head()
df2.groupby(['group'],as_index=False).mean()
df = pj.convert_to_df(scaling=True, filtered=True) $ df.info()
my_model_q3_label = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_nb, training='label') $ my_model_q3_label.fit(X_train, y_train) $ my_model_q3_label.stackData.head()
stats.ttest_ind(df_h1b_nyc_ft.pw_1,df_h1b_mv_ft.pw_1, equal_var = False)
session.query(Stations.station, Stations.name, Stations.latitude, $               Stations.longitude, Stations.elevation).all()
df.columns
trump.info()
telecom3 = telecom2.drop(['mobile_number', 'churn'], axis=1) $ plt.figure(figsize = (20,20))        # Size of the figure $ sns.heatmap(telecom3.corr())
df.index.name = 'DateTime' $ df
data.tail(10)
df.columns
ac['Eligibility Date'].groupby([ac['Eligibility Date'].dt.year]).agg('count')
test.fillna(0, axis=1, inplace=True) $ train.fillna(0, axis=1, inplace=True)
df = pd.DataFrame(np.random.randn(100, 4), index=ts.index, columns=['A', 'B', 'C', 'D']) $ df = df.cumsum() $ plt.figure(); df.plot(); plt.legend(loc='best') $ plt.show()
monthly_portfolio_average = round(np.sum(MonthlyReturns * stock_weights),2) $ monthly_portfolio_average
taxiData[Tip_percentage_of_total_fare != 0].Tip_percentage_of_total_fare.mean() # tip percentage mean is around 0.25 $ print("MEAN of Tip as a percentage of the total fare = {}" $       .format(taxiData[Tip_percentage_of_total_fare != 0].Tip_percentage_of_total_fare.mean()))
Meta3 = type.__new__(Meta,'M3',(),{}) $ print_classes(Meta3)
data.dtypes
xirrs_overall =pd.DataFrame( {'actual':np.nan, 'expected': np.nan}, index=selected_reporting_dates)
%time df_csv['Total Profit'].sum().compute()
m.plot(forecast);
df['production_companies'] = df['production_companies'].apply(lambda x: x.split('|')[0])
merged2.shape
y_hat=model.predict(X_test)
id_pickup_label.shape, id_dropoff_label.shape
api = tweepy.API(auth, $                  parser = tweepy.parsers.JSONParser(), $                  wait_on_rate_limit = True, $                  wait_on_rate_limit_notify = True)
flights = flights[-((flights['origin_airport'].str.contains('[0-9]')) & $                     (flights['destination_airport'].str.contains('[0-9]')))] $ flights.head()
b.iloc[0:1]
lab.info()
top_songs['Date'].isnull().sum()
tweets_df.loc[tweets_df.language == 'und', :] $
gdf_gnis.shape
ax = tz_cat['tweetRetweetCt'].plot(kind='barh', title ="Cat Timezone Retweet Count", figsize=(10, 25), $                                     legend=True, fontsize=10) $ ax.set_ylabel("Timezone", fontsize=12) $ ax.set_xlabel("Count", fontsize=12) $ plt.show()
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country'])
df = pd.DataFrame(bmp_series, columns = ['mean_price']) $ df
treatment_converted = df2[((df2['group'] == 'treatment') &(df2['converted'] == 1))].count()[0] $ treatment_all = df2[((df2['group'] == 'treatment') )].count()[0] $ print('Given that an individual was in the treatment group, the probability they converted is {}'.format(round((treatment_converted/control_all),4))) $ print('\nConverted in treatment is {} '.format(treatment_converted)) $ print('Total number of treatment group {} '.format(treatment_all))
log_US = sm.Logit(new['converted'],new[['intercept','country_US']]) $ r = log_US.fit() $ r.summary()
tree = DecisionTreeClassifier() $ tree.fit(X_train, y_train) $ tree.score(X_test, y_test) $
no_images=[] $ for i in range(len(tweet_json)): $     if 'media' not in list(tweet_json['entities'][i].keys()): $         no_images.append(i) $ print("There are {} tweets that do not have an image".format(len(no_images)))
autos['price'].unique().shape
pvt.reset_index(inplace=True) $ pvt
df.sample(5)
st.pearsonr(df_final.rt_count, df_final.img_num)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
y_ls.shape
(df2.query('group == "control"')['converted'].mean() - df2.query('group == "treatment"')['converted'].mean())
tweet_archive.head()
df.head(2)
((val,trn), (y_val,y_trn)) = split_by_idx(val_idx, df.values, yl)
sub_dataset[ sub_dataset['WordCount'] == 0 ]['WordCount'].count()
suburban_driver_total = suburban_type_df.groupby(["city"]).mean()["driver_count"] $ suburban_driver_total.head()
test_df.field.value_counts()
image_pred.drop(['jpg_url','img_num','p1','p1_conf','p1_dog','p2','p2_conf','p2_dog','p3','p3_conf','p3_dog'],inplace=True,axis=1)
import pyspark.sql.functions as func $ parsed_test.groupBy().agg(func.max(col('id'))).show()
store[objectFeaturesStore].head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old,n_new], alternative='smaller') $ z_score,p_value
ab_data[((ab_data.group == 'treatment') & (ab_data.landing_page != 'new_page')) | $         ((ab_data.group != 'treatment') & (ab_data.landing_page == 'new_page')) $        ].shape[0]
df['kw-cliton'] = df['text'].apply(lambda t: 'cliton' in str(t).lower()) $ df['kw-debate'] = df['text'].apply(lambda t: 'debate' in str(t).lower()) $ df['kw-blacklivesmatter'] = df['text'].apply(lambda t: 'blacklivesmatter' in str(t).lower())
sales_2015 = df_2015.groupby(by=["store_number"], as_index=False)
df.isnull().sum(axis=1)
from sklearn.cluster import KMeans $ num_clusters = 5 $ km = KMeans(n_clusters=num_clusters) $ km.fit(tfidf_doctopic)
tweets[0]._json['retweet_count'], tweets[0]._json['favorite_count'], tweets[0]._json['entities']['media'][0]['media_url']
prepared_train = prepared_train[prepared_train.views < 600000]
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == "1"').count() $ convert_new = df2.query('group == "treatment" and converted == "1"').count() $ convert_old[0] , convert_new[0], n_old , n_new
df_nona['create_date'] = df_nona.created.map(pd.to_datetime) $ app_usage = df_nona.groupby('create_date').count().app_id $ app_usage.resample('w', sum).plot(title='Adoption through time') $
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
p_new_std = (df2['converted']==1).std() $ p_new_std
df = pd.read_csv("prepped_data.csv"); print(len(df)) $ vs = 52234 # manually set this to match above $ import ast $ df["numerized_tokens"] = df["numerized_tokens"].apply(lambda x: ast.literal_eval(x))
prob_treat_converted = df2[df2['group'] == 'treatment']['converted'].mean() $ print (prob_treat_converted)
y_train.clip(0, 10).max() $ y_test.clip(0, 10)
data2.head()
def print_rmse(model, name, input_fn): $   metrics = model.evaluate(input_fn=input_fn, steps=1) $   print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['loss'])) $ print_rmse(model, 'validation', make_input_fn(df_valid))
knn.score(x_test,y_test > 0)
train_data['visitStartTime'] = pd.to_datetime(train_data['visitStartTime'],unit='s') $ train_data['date'] = pd.to_datetime(train_data['date'].astype('str')) $ test_data['visitStartTime'] = pd.to_datetime(test_data['visitStartTime'],unit='s') $ test_data['date'] = pd.to_datetime(test_data['date'].astype('str'))
l_norm=(l-np.min(l))/(np.max(l)-np.min(l))
df["grade"].cat.categories = ["excellent", "good", "okay", "bad", "miserable", "fail"] $ df
autos['price'].value_counts().sort_index(ascending=False).head(50)
pred_probas_rfc_over = gs_rfc_over.predict_proba(X_test)
df.index.month
max(c, key=c.get)
rounds['announced_on'] = pd.to_datetime(rounds['announced_on'], errors = 'coerce')
df_users.sum()
arg = [] $ for i in test: $     argmax = df_campaigns['Title'].apply(lambda x: \ $                                          len(list(set(re.split("[ _\-.]",x.lower())).intersection(i)))).argmax() $     arg.append(argmax)
df_aggregate = (df[['date','serial_number','failure']].groupby('serial_number',as_index = True) $                 .agg({'date':'count', 'failure':'sum'}) $                 .rename(columns={'date': 'date_count', 'failure': 'failure_sum'}) $                 .sort_values(by=['failure_sum'],axis=0,ascending=False)) $ df_aggregate.head(5)
rspmhigh['location'].value_counts()
logmodel.fit(X_train, y_train) $ logmodel_predictions = logmodel.predict(X_test) $ num_of_logmodel_pred = collections.Counter(logmodel_predictions) $ num_of_logmodel_pred
oil_spark = spark.createDataFrame(oil_pandas) $ oil_spark.show()
All_tweet_data_v2[All_tweet_data_v2.expanded_urls.isnull()]
df = df[df["city"]=="Pittsburgh"] $ df.reset_index(drop=True, inplace=True) $ df.shape
tf_idf.shape
old_page_converted = np.random.choice([0, 1], p=[(1-p_old), p_old], size=n_old)
lgb_model = lgb.LGBMRegressor(objective='mape',n_estimators=300, learning_rate=0.2, num_leaves=45, random_state=1)
pm_data.isnull().sum(axis=0)
releases.columns
spark_df.write.parquet("/user/mapr/eds/nda_j1_deces")
temp_cat.codes
zstats,pvalue=sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old],alternative="larger") $ print("Zstats",zstats) $ print("pvalue",pvalue) $
old_page_converted = np.random.choice(2, n_old, p=[1-p_old, p_old])
list(soup.children)
from sklearn.cluster import KMeans $ kmeans = KMeans(n_clusters=5, random_state=1).fit(X_feat) $ X_cluster = kmeans.predict(X_feat) $ df_tweets = pd.DataFrame({'text': X_tweets, 'cluster': X_cluster, 'position': txt_tweets_position}) $ df_tweets.cluster.value_counts(normalize=True)
station_cluster['DATE'] = pd.to_datetime(station_cluster['DATE'], format='%y/%m/%d')
k.groupby('Gender').agg(lambda row: (', '.join(row.Name), int(row.Age.mean())))
pgh_311_data_merged['Category'].value_counts()
df = pd.merge(ffr, vc, left_index=True, right_index=True, how="left") $ df_info(df)
data = data[data.state != 'undefined'] $ data.info()
tweet_image_clean = tweet_image_clean[tweet_image_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']) == True]
tweets_master_df.groupby(tweets_master_df["timestamp"].apply(lambda x: x.year))['timestamp'].count()
selected_reacts = [r for r, c in unique_reacts.most_common(20)]
html_hemi = requests.get(url_hemispehere).text $ soup_hemi = BeautifulSoup(html_hemi, 'lxml') $ thumbs = soup_hemi.find_all('a', class_='product-item')
for team in list: $     team.set_index("Date", inplace = True) # Sets date as the index
%%HTML $ <iFrame src="https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior" width=900 height400></iFrame>
(df.query('group == "treatment" & landing_page != "new_page"').converted.count() + $  df.query('group == "control" & landing_page != "old_page"').converted.count())
print autodf.kilometer.count() $ print max(autodf.kilometer) $ print min(autodf.kilometer) $ sns.boxplot(autodf.kilometer)
contract_history['NUM_CAMPAGNE'] = contract_history['NUM_CAMPAGNE'].map(lambda x: float(x) if x not in ['N', ''] else np.nan)
taxi_weather_df.head()
1/np.exp(-0.0150), np.exp(0.0202), np.exp(0.0252)
X_test.shape
log_model = sm.Logit(y, X)
temp_df.to_csv('users_gh_emailHash_All.csv')
with_condition_heatmap = folium.Map([41.90293279, -87.70769386], $                zoom_start=11) $ with_condition_heatmap.add_child(plugins.HeatMap(final_location_ll[:40000], radius=15)) $ with_condition_heatmap
T = tf.one_hot(y, depth=caps2_n_caps, name="T")
yc_merged_drop.shape
temp_df2 = temp_df.drop_duplicates()
dates = pd.date_range('2018-05-01', '2018-05-06') $ temps1 = Series([80, 82, 85, 90, 83, 87], index = dates) $ temps1
g = sns.barplot(count_categories.index, count_categories) $ for item in g.get_xticklabels(): $     item.set_rotation(90)
train.isnull().sum()
df.describe()
dpickle.dump(ppm_title, open( "ppm_title.dpkl", "wb" ) )
df3.head()
pd.scatter_matrix(df, diagonal='kde', color='k', alpha=0.5, figsize=(12, 6)) $ tight_layout()
archive_version = None  # i.e. '2016-07-14'
sns.distplot(virginica) $ sns.distplot(versicolor)
X_test.head()
ax = pdf.plot() $
tranny.values
tfidf_corpus = matutils.Sparse2Corpus(tfidf_vecs.transpose()) $ id2word = dict((v, k) for k, v in tfidf.vocabulary_.items()) $ id2word = corpora.Dictionary.from_corpus(tfidf_corpus, $                                          id2word=id2word)
df = df.rename(index = str, columns = {'new.recap':'newRecap'})
import gensim $ import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) $ model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)
aux = df.groupby(['group', 'landing_page']).agg({'user_id':'count'}) $ aux.reset_index(inplace=True) $ aux
arthritis_edge_weights = [edata['weight'] for f,t,edata in arthritisgraph.edges(data=True)] $ print "count of arthritis edge weights: %s" % len(arthritis_edge_weights) $ trimmed_arth_weights = [x for x in arthritis_edge_weights if x > 2] $ plt.hist(trimmed_arth_weights, bins=100)
df.info() 
goal_rate['n_proj'].plot.bar(title='Number of projects vs goal');
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata  = pd.read_csv(path, sep =' ') $ mydata.head(5)
payload = {'start_date':'2017-01-01','end_date':'2017-12-31','api_key':API_KEY} $ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X', params=payload) $
np.all(x < 10)
closeSeriesQ.pct_change().tail()
df2[df2['landing_page']=='old_page']['converted'].mean()
giss_temp.Aug.interpolate().tail()
test_df.head(2)
part1_flt = part1_flt.sort_values(by=["campaign_spend"]) $
df.head()
en_es.head()
df_json_tweets.head()
df3[['US','UK','CA']] = pd.get_dummies(df3['country'])
model_df.groupby('score_str').size() #just take a look at the count of topics for each score
domain = 'steemit.com'
containers = page_soup.find_all("section", {"class":"ref-module-paid-bribe"})
log_mod_3 = sm.Logit(df_new['converted'], df_new[['ab_page','CA', 'UK', 'intercept']]) $ result_3 = log_mod_3.fit() $ result_3.summary()
so.iloc[so['viewcount'] > 100000]
extract_deduped_with_elms_v2.loc[(~extract_deduped_with_elms_v2.ACCOUNT_ID.isnull()) $                              &(extract_deduped_with_elms_v2.LOAN_AMOUNT.isnull())].shape
df_new['US_page'] = df_new['US'] * df_new['ab_page'] $ df_new['UK_page'] = df_new['UK'] * df_new['ab_page'] $ df_new.head()
reference_frame = proc_rxn_time[['Subject First Name', 'Subject Last Name', 'subject_id']] $ reference_frame.columns = ['Subject First Name', 'bhc_id', 'subject_id']
organisation = pd.read_csv(data_repo + 'organisation.csv', **import_params)
df2['landing_page'].value_counts()
data[(data.phylum.str.endswith('bacteria')) & (data.value>1000)]
pd.DataFrame({'Social Networks' : [x[1] for x in HARVEY_92_USERS_SN]}).hist(bins=[1,2,3,4,5,6,7,8,10,100,200,1000])
names.str.capitalize()
sns.countplot(x="month",data=twitter_final)
image_predictions=pd.read_csv('image_predictions.tsv', sep='\t') #load the data of predictions $ image_predictions.head()
events_df['event_hour'] = events_df['event_time'].apply(lambda d: d.hour)
df_archive['rating_denominator'].value_counts()
averaged_data = dry_ndvi.groupby('time.month').mean(dim='time')
log.info(("loading predictions={} with accuracy={} into pandas dataframe") $          .format(len(predictions), accuracy)) $ train_results_df = pd.DataFrame(predictions) $ log.info(("columns in dataframe with columns={}" $           .format(train_results_df.columns.values)))
titanic.sex.value_counts(sort=False).plot(kind='bar')
col.head(2)
page_mismatch_df = df.query("group == 'treatment' and landing_page == 'old_page'") $ group_mismatch_df = df.query("group == 'control' and landing_page == 'new_page'") $ total_mismatch_number = page_mismatch_df.count()[0] + group_mismatch_df.count()[0] $ total_mismatch_number $
col = pd.read_csv('NYPD_Motor_Vehicle_Collisions.csv', index_col = 0, low_memory = False)
my_data.count()
%%time $ df.body.apply(lambda x: sleep(.0001)).head()
print('Unique number of users notified: {}'.format(len(atdist_opp_dist[atdist_opp_dist['infoIncluded']]['vendorId'].unique())))
result3 = df.eval('(A + B) / (C - 1)') $ np.allclose(result1, result3)
df_wm['sentiment'] = df_wm['cleaned_text'].apply(sentiment_calc)
rshelp.query("SELECT parking_spot_id, ev_charging FROM postgres_public.parking_spot ORDER BY RANDOM() LIMIT 100;")
df_group=df2.groupby('group') $ df_group.describe() $
year_labeled= $ year_predict= $ description_labeled = df[df.year==year_labeled]['description'] $ description_predict = df[df.year==year_predict]['description']
twitter_archive.name.value_counts()
rng.tz_localize('Etc/GMT-3')
tesla_days.plot(y='Close') $ plt.show()
cars[cars.Model.str.contains(r'[0-9][0-9]+')]
df_members.head()
x_train, x_test, y_train, y_test = train_test_split(trainDataVecs, y, test_size=0.999) $ nb = MultinomialNB() $ nb.fit(x_train, y_train) $ nb.score(x_test, y_test)
conn.fetch(table=dict(name='iris_df', caslib='casuser'), to=5)
addOne = cylHPData.mapValues(lambda x: (x, 1)) $ print (addOne.collect()) $
for a in JournalStories['Article']: $     print("".join(a))
dta.b.unique()
sns.jointplot(train_data["pp_neg_words"], target_data['Prediction'], $               kind='reg', ylim=5000, size=6, space=0, color='b')
autos.shape[0]
print(autodf.groupby('seller').size()) $ autodf = autodf[autodf['seller'] != 'gewerblich'] $ autodf = autodf.drop('seller',axis = 1) $ print( "\nSize of the dataset - " + str(len(autodf)))
%timeit StockData = pd.read_csv(StockDataFile, index_col=['Date'], parse_dates=['Date'])
ks_name_failed = ks_name_lengths.drop(ks_name_lengths.index[ks_name_lengths.state != 'failed']) $ ks_name_failed.set_index('name_length', inplace=True)
class_merged_hol=pd.merge(class_merged_hol,local_hol,on=['date','city'],how='left') $ print("Rows and columns:",class_merged_hol.shape) $ pd.DataFrame.head(class_merged_hol)
dates = pd.DatetimeIndex(pivoted.columns) $ dates[(labels == 0) & (dayofweek < 5)]
rv_params = { $     'resultID': ','.join(resultids) $ }
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data.TAB.txt" $ mydata = pd.read_csv(path, sep= '\t') $ mydata.head(5)
multi_table([df_metric[df_metric['count']>30].head(n=3),df_metric[df_metric['count']>30].tail(n=3)])
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x, np.dot(x3, model3.coef_) + model3.intercept_, color = 'dimgrey', linewidth = '5.0')
df['Name'].value_counts().head(35)
train_df['Domain'].nunique()
from h2o.estimators.gbm import H2OGradientBoostingEstimator
data.head()
train_data['country_destination'].value_counts()
med_comments = reddit['Comments'].median() $ reddit['Above_Below_Median'] = np.where(reddit['Comments']>=med_comments, 'Above', 'Below')
df.info();
my_list = ["a", "b", "c", "d"] $ print(len(my_list))            # For a 4 item-long list, the length is naturally '4' $ print(my_list[0:len(my_list)]) # <-- This is the equivalent of '[0:4]'. $
numberFilter = tweets["retweet_count"] > 10000 $ tweets[numberFilter].head()
datetime.date(datetime(2014,12,14))
temp_cat.categories = ['sejuk','sederhana','panas'] $ temp_cat   # original category object categories is changed
print(classification_report(model.predict_classes([nn_X_test]),[np.argmax(value) for value in nn_y_test]))
(new_page_converted).mean() - (old_page_converted).mean()
prop_users_converted = round(df.converted.mean() * 100) $ print('The proportion of users converted is {}%.'.format(prop_users_converted))
dfFull['GrLivAreaNorm'] = dfFull.GrLivArea/dfFull.GrLivArea.max()
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new, n_old],alternative='larger') #larger one-sided $ z_score, p_value
census_withdata.head()
token_sendReceiveAvg_month = token_sendReceiveAvg_month.fillna(0)
import scipy $ corr_coeff = scipy.stats.pearsonr(data_compare['SA_textblob_de'], data_compare['SA_google_translate']) $ print('Der Korrelationskoeffizient zwischen TextBlob_DE und Google Translation ist:') $ print('----------------------------------------------------------------------------') $ print(corr_coeff[0])
driver = webdriver.Firefox(executable_path = "C:/libs/web_driver_for_crawl/geckodriver") $ driver.wait = WebDriverWait(driver, 10)
df_pd.sort_values(by='timestamp') $ df_pd.set_index("timestamp") $ train_frame = df_pd[0 : int(0.7*len(df_pd))] $ test_frame = df_pd[int(0.7*len(df_pd)) : ]
stem_freq_dict = grouped.set_index('Word_stem')['Frequency'].to_dict()
prcpDF.describe()
dealer_avg_order_intervals = copy.deepcopy(result) $ %store dealer_avg_order_intervals
jobs_data.nunique()
control = df2.query('group=="control"') $ control_converted = control.query('converted==1') $ prob_control_converted = control_converted.shape[0]/control.shape[0] $ prob_control_converted
records3.loc[(records3['Graduated'] == 'Yes') & (records3['Days_missed'].isnull()), 'Days_missed'] = grad_days_mean $ records3.loc[(records3['Graduated'] == 'No') & (records3['Days_missed'].isnull()), 'Days_missed'] = non_grad_days_mean
pd.merge(staff_df, student_df, how='left', left_index=True, right_index=True)
df_goog['good'] = df_goog.Close > df_goog.Open $ df_goog
results=logit_mod.fit();
properati['currency'].value_counts(dropna=False)
df = pd.read_json('twitter_store.json', lines=True) $ df = df.assign(user_location = df.user.apply(lambda d: d['location']))
def clean_text(text): $     text = "".join([char for char in text if char not in string.punctuation]) $     tokens = re.split('\W+', text) $     text = [ps.stem(word) for word in tokens if word not in stopword] $     return text
df.describe(include='all')
jobs_data.tail()
s = lv_workspace.get_step_1_object('A')
gs_rfc_over.score(X_test, y_test_over)
df = df[pd.isnull(df['retweeted_status_id'])] $ df.shape[0]
train_df.groupby(['Domain'])['Tag'].nunique().value_counts(normalize=True)
np.save("series",series)
twitter_dataset.info()
df.sum()
chefdf = chefdf.drop_duplicates(subset=['name', 'user']) $ len(chefdf.name)
twitter_Archive.drop(twitter_Archive[twitter_Archive['retweeted_status_id'].notnull()== True].index,inplace=True) $ twitter_Archive.info() $
tables = [] $ for table in think_tanks_soup.find_all('a'): $     think_tank = " ".join(table.text.split()) $     tables.append(think_tank)
shelter_df_only_idx = shelter_df_idx $ shelter_df_only_idx = shelter_df_only_idx.drop('OutcomeType', 'AnimalType', 'SexuponOutcome', 'Breed', 'Color') $
df_centered.cache()
locale_name_regional_hols=regional_holidays['locale_name'].unique() $ print(locale_name_regional_hols)
Raw_Forecast.to_csv("SG_MasterFile",encoding = "utf-8",index = False)
web.DataReader("NFLX", 'yahoo', start, end)[['Adj Close']]['2012-07-20':'2013-07-23'].plot(figsize=(20,10))
s = pd.Series([99, 32, 67],list('abc')) $ s.isin([67,32])
train_data.to_feather('data/train_data') $ test_data.to_feather('data/test_data')
questions = pd.concat([questions.drop('coming_next_reason', axis=1), coming_next_reason], axis=1)
print ('The mean of test_accuracy of the model with the default hyperparameter:') $ scores['test_accuracy'].mean()
places = mydata[['location.name','location.displayName','location.twitter_country_code','location.geo.coordinates']] $ places = places.drop_duplicates(subset = ['location.name']) $ places.reset_index(drop = True) $ places
from skmultilearn.adapt import MLkNN $ classifier = MLkNN(k=20) $ classifier.fit(X_tr[0:len(X_train)-40-1], y_ls) $ classifier.score(X_tr[0:len(X_train)-40-1], y_ls)
rounds.dtypes
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'], $        'year': [2000, 2001, 2002, 2001, 2002, 2003], $        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}
session.query(Adultdb).filter_by(occupation="?").delete(synchronize_session='fetch') $ session.commit()
directory_name = 'network/source_input/' $ input_nodes_file        = directory_name + 'nodes.h5' $ input_models_file       = directory_name + 'node_types.csv' $ input_edge_types_file   = directory_name + 'edge_types.csv' $ input_edges_file        = directory_name + 'edges.h5'
df['time_open_hours'] = df['time_open'].apply(lambda x: x.total_seconds()) / 60 / 60
tweets_master_df.ix[309, 'text']
tweet_archive_enhanced_clean['name'].replace(to_replace =['a','an','the'],value = 'None',inplace = True)
import statsmodels.api as sm $ convert_old = df2[df2['group'] == "control"]['converted'].sum() $ convert_new = df2[df2['group'] == "treatment"]['converted'].sum() $ n_old = df2['group'].value_counts()[1] $ n_new = df2['group'].value_counts()[0]
list_acceptable_exts.sort() $ list_acceptable_exts = list_acceptable_exts[5:] $
indexed_df = movie_df.set_index("title")
df_training = df_centered[df_centered.seq < df_centered.numOfReviews] $ df_test = df_centered[df_centered.seq == df_centered.numOfReviews]
poly_features = PolynomialFeatures(2, include_bias=False) $ poly_features.fit(X_train['Pending Ratio'].values.reshape(-1, 1)) $ training_pending_ratio = poly_features.transform( $     X_train['Pending Ratio'].values.reshape(-1, 1)) $ print(training_pending_ratio[0:5, :]) $
plt.hist(review_length, bins =100) $ plt.show() # most reviews are short, only few reviews are very long.
url = "https://www.analyticsvidhya.com/" $ r = requests.get(url) $ r
measure_avg_prcp_year_df = data_year_df.groupby('Date', as_index=False, sort=False)['Prcp'].mean() $ measure_avg_prcp_year_df.rename(columns={'Prcp':'Avg Prcp'}, inplace=True) $ measure_avg_prcp_year_df.head()
data['freq']=np.where(data['online'] == False, 1, 0)
sl[sl.index.isin(sl_train.index)].shape
score_0 = score[score["score"] == 0] $ score_0.shape[0]
groceries.drop('apples') # returns groceries without apples
graf_counts['AFFGEOID'].dtypes
f(132.52012385, True)
db.list_collection_names()
import requests $ import quandl $ from pandas.io.json import json_normalize $ quandl.ApiConfig.api_key = API_KEY 
resp_json = resp.json()
curr = pd.read_sql("SELECT emp_id_seq.currval FROM dual",xedb) $ next = pd.read_sql("SELECT emp_id_seq.nextval FROM dual",xedb) $ print(curr) $ print(' ') $ print(next)
last_year = dt.date(2017, 6, 2) - dt.timedelta(days=365) $ print(last_year)
conn.addtable(table='iris_db', caslib='casuser', $               **dbdmh.args.addtable)
season09 = ALL[(ALL.index >= '2009-09-10') & (ALL.index <= '2010-02-07')]
data.dropna(axis=1)
gbm_2 = joblib.load(r'C:\Users\nkieu\Desktop\Python\Loan data\2018-04-09\gbm_0409.pkl')
df['order_date'].max()
data_archie = data_archie[data_archie['cur_sell_price'].notnull()]
results = soup.find_all('span', attrs={'class':'short-desc'}) $ results = soup('span', attrs={'class':'short-desc'}) $ results = soup('span', class_='short-desc')
learner.load_encoder('adam3_10_enc')
df_new[['US', 'CA', 'UK']] = pd.get_dummies(df_new['country']) $ new_log_m = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'US', 'CA']]) $ new_results = new_log_m.fit() $ new_results.summary()
tweet_df_polarity = my_tweet_df.groupby(["tweet_source"]).mean()["tweet_vader_score"] $ pd.DataFrame(tweet_df_polarity)
rng_pytz.tz == tz_pytz
p_old = p_new # df2[df2['landing_page']=='old_page']['converted'].mean() $ p_old
elms_all_0611.loc[(elms_all_0611.duplicated('ACCOUNT_ID'))].shape
num_vars.remove('id')
reddit['comm_range'].value_counts()
paired_df = pd.DataFrame(list(pairwise_count.items()), columns=['paired_datasets', 'co_occurence']) $ paired_df.sort_values('co_occurence', ascending=False, inplace=True) $ paired_df.head()
proj_df['Project Need Statement'].str.startswith('I need').value_counts()
closed_pr = PullRequests(github_index) $ closed_pr.since(start=start_date).until(end=end_date) $ closed_pr.is_closed() $ closed_pr.get_percentile("time_to_close_days").by_period() $ print(pd.DataFrame(closed_pr.get_ts()))
df_image_clean[df_image_clean['tweet_id'] == int(df_tweet_clean['id'].sample().values)]
horizAAPL = AAPL.sort_index(axis=1) $ horizAAPL.head()
sqlContext.sql("select sum(count) from pcs").show()
ripc = ripc[np.isfinite(ripc['w_sentiment_score'])] $ ripc.info()
tuna_neg = mapped.filter(lambda row: row[4] < 0) $
x = df.day_of_year.value_counts(sort=False) $ vc = pd.DataFrame(x)
postsM.reset_index(inplace=True)
appointments.to_csv('./data/appointments_full.csv')
df = df.sort_values('DATE')
new = len(df2.query("landing_page == 'new_page'")) $ new
twitter_archive_clean[twitter_archive_clean.text.str.contains('&amp;', na=False, regex=True)]
df_passengers.head(3)
df000001.tail()
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
pd.DataFrame(eligible_posts).to_csv("feminism/feminism_posts_06.30.2017-12.31.2107.csv") $ pd.DataFrame(eligible_comments).to_csv("feminism/feminism_comments_on_posts_with_body_06.30.2017-12.31.2107.csv")
from subprocess import call $ call(['python', '-m', 'nbconvert', 'wrangle_act.ipynb']) $
import findspark $ findspark.init() $ from pyspark.sql import SparkSession $ spark = SparkSession.builder.master("local[*]").getOrCreate()
NoOfRows = df.shape[0] $ NoOfRows
commits_per_weekday.plot.bar();
daily.rolling(50, center=True, $              win_type='gaussian').sum(std=10).plot(style=[':', '--', '-'])
train['Month']     = train["date"].dt.month $ train['Day']       = train["date"].dt.day $ train['DayOfWeek'] = train["date"].dt.dayofweek
plot_mnist_sample(mnist_train.train_data, $                   sample_idx=[i for i in range(10)]) $ plot_mnist_sample(mnist_test.test_data, size=10) $
overallMoSold = pd.get_dummies(dfFull.MoSold)
rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)
bitcoin_price.head()
new_page_converted = np.random.binomial(n_new,p_new)
n_old = df2.query('group =="control"').shape[0] $ n_old
closes.plot(figsize=(8,6));
AFX_X_2017 = r.json()
sdf.plot(kind='map', map_widget=webmap, renderer_type='o')
engine.execute(f'drop view if exists {view}')
league.name.unique()
X.head()
reddit_info = reddit_info.drop('index', axis = 1)
!ls -lah | grep .dpkl
type_df.columns = ['column_name','column_type'] $ type_df.groupby('column_type').aggregate('count').reset_index() $
(loan_requests.postcheck_data!='null').sum()
precipitation_measurement_df = pd.DataFrame(precipitation_year[:], columns=['Date','prcp',]) $ precipitation_measurement_df['Date'] =  pd.to_datetime(precipitation_measurement_df['Date']) $ precipitation_measurement_df.set_index('Date', inplace=True) $ precipitation_measurement_df.head()
users = users[email_bool] $ users.info()
p_control_converted = df2[df2['group'] == 'control']['converted'].mean() $ print('The probability of an individual in the control group converting: ', p_control_converted)
validation.analysis(observation_data, BallBerry_resistance_simulation_0_25)
mars_weather = Weather_soup.find('p',class_="TweetTextSize").text $ print(mars_weather)
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'useless'
cohort_activated_df.to_csv('activation_cohorts_518.csv')
IBMpandas_df = pd.read_csv("IBM.csv") $ IBMpandas_df.head()
plt.plot(actual, imputed, 'go')
session_df['delta_recall'] = session_df['session_summary'].apply(normalized_item_delta_recall) $ session_df.head()
subwaydf['DIVISION'].value_counts()
df_test['dayofweek'] = df_test['effective_date'].dt.dayofweek $ df_test['weekend']= df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0) $ df_test.head()
ngrams_summaries = cvec_4.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
team_unique=results.date.astype(str)+results.home_team+results.away_team $ result_copy=results.copy() $ result_copy.index=team_unique $ result_copy.head()
knn = knn_grid.best_estimator_ $ knn.fit(X_train,y_train) $ knn_pred = knn.predict(X_test)
from sklearn.neighbors import KNeighborsRegressor $ knn = KNeighborsRegressor(n_neighbors = 3, p = 1, weights = 'distance')
plt.figure(figsize=(10,3)) $ plt.plot(one_station['DATE'],one_station['DAILY_ENTRIES'])
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop(['CA'], axis=1) $ df_new.head()
userArtistDF.agg(min("userID"),max("userID")).show()
fitted_pdf = norm.pdf(x,loc = parameters[0],scale = parameters[1])
R16df = pd.DataFrame([R16]).T $ R17df = pd.DataFrame([R17]).T $ R18df = pd.DataFrame([R18]).T
pd.Series(data=y10_train).hist()
merged = winpct.merge(pbptweets, how='left', on=['text'])
agent.trade(5)
goog.index = goog.index.to_datetime()
f_counts_week_app = spark.read.csv(os.path.join(mungepath, "f_counts_week_app"), header=True) $ print('Found %d observations.' %f_counts_week_app.count())
(null_vals< ab_dif).mean()
print('Merchant plan offer distribution: ') $ print(data['Merchant_Plan_Offer'].value_counts())
merged_data.drop(redundant_features, axis=1, inplace=True)
datatest['floor'] = datatest['floor'].apply(lambda x: (float)((int)(x)))
word_freq_df[word_freq_df.Word=='table']
control_new = df.query('group == "control" and landing_page == "new_page"') $ treatment_old = df.query('group == "treatment" and landing_page == "old_page"') $ unmatched = len(control_new) + len(treatment_old) $ print(unmatched)
August_July = df["2015-07":"2015-08"] $ August_July['Complaint Type'].value_counts().head(5)
tweet_df.tweet_created_at.max()
pd.concat([df1.drop(['booking_items'], axis=1),df2], axis=1)
prcp_df = pd.DataFrame(prcp_data, columns=['Precipitation Date', 'Precipitation']) $ prcp_df.set_index('Precipitation Date', inplace=True) # Set the index by date
all_data = import_data('data1.csv') $ display(all_data.head()) $ display(all_data.tail())
fruits = pd.Series(data = [10, 6, 3], index = ['apples', 'oranges', 'bananas']) $ fruits
np.random.randn(5, 5)
df_test = text_classifier.predict(df_test) $ df_test.head()
clf = GradientBoostingClassifier() $ clf.fit(X_train, y_train) $ print(clf.__class__.__name__) $ print(accuracy_score(y_test, clf.predict(X_test)))
s.index
data.Suburb = data.Suburb.str.lower().str.replace('[^a-z\s]', ' ').str.split().str.join(' ').str.lstrip(' ').str.rstrip(' ') $ labels = data.Suburb.value_counts() $ labels.shape
jobs.loc[(jobs.FAIRSHARE == 24) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
df.loc['r_one'] # first row
to_be_predicted_Day1 = 55.05 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
import pickle $ filename = 'finalized_automl.sav' $ pickle.dump(automl, open(filename, 'wb'))
locationing['subjectivity'].describe()
unique_users = set(orders.user_id.unique()) $ selected_users = random.sample(unique_users, 20000)
%matplotlib notebook $ import matplotlib.pyplot as plt $ import seaborn as sns $ sns.set()
ml=clean_prices.groupby(['name']).tail(1).loc[:,'EMA_ratio':'5dayvol']
building_pa_prc=pd.read_csv("buildding_00.csv")
df_clean3.nlargest(10, 'rating_denominator')[['text', 'rating_numerator', 'rating_denominator']]
line_up1=df.query('group=="treatment" & landing_page=="old_page"')['user_id'].count() $ line_up2=df.query('group=="control" & landing_page=="new_page"')['user_id'].count() $ line_up=line_up1+line_up2 $ print("The number of times the new_page and treatment don't line up is ="+str(line_up))
pumashp.head()
corpus = [dictionary.doc2bow(text) for text in texts] $ corpora.MmCorpus.serialize('bible.mm', corpus) $ print(corpus)
diff_specs['azure.com:automation-certificate']
df['State Bottle Cost'] = df['State Bottle Cost'].str.extract('([^$][0-9.]*)').astype(float) $ df['State Bottle Retail'] = df['State Bottle Retail'].str.extract('([^$][0-9.]*)').astype(float) $ df['Sale (Dollars)'] = df['Sale (Dollars)'].str.extract('([^$][0-9.]*)').astype(float) $ df.head()
1/np.exp(-0.0149)
conv_count_each_country = df_new.groupby('country')['converted'].sum() $ conv_rate_each_country = conv_count_each_country / countries_count $ print(conv_rate_each_country) $ conv_rate_each_country.plot.bar()
users.query("created_at < '1984-01-01 00:00:00'")
df.to_pickle(pretrain_data_dir+'/pretrain_data_02.pkl')
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_index().plot(kind="bar", title="Last_seen", colormap="Blues_r")
rural_avg_fare = rural_type_df.groupby(["city"]).mean()["fare"] $ rural_avg_fare.head() $
from matplotlib.pyplot import figure $ figure(num=None, figsize=(17, 4), dpi=80, facecolor='w', edgecolor='k') $ places = list(tweet_place_hist.index) $ frequencies = list(tweet_place_hist.place_freq) $ plt.bar(places, frequencies)
posts.find({"reinsurer": "AIG"}).count()
volume.sort() $ print(volume[int(len(volume)/2 +1)])
plt.hist(np.log(threeoneone_census_complaints[threeoneone_census_complaints['complaint_density']>0]['complaint_density']+1),bins=100) $ plt.show()
print('WITHOUT resetting the random seed:') $ print('%d different bacteria between the two function calls' % len(set(dd.feature_metadata.index)^set(dd2.feature_metadata.index)))
n_old = df2.query('landing_page == "old_page"').user_id.count() $ print(n_old)
feature_matrix_duplicated.drop_duplicates().shape
train.shape
r_aux=[] $ for i in range(0,l2): $     if i not in seq: $         r_aux.append(resilience[i]) $ col.append(np.array(r_aux))
students.values
data.to_csv('time_series_arsenal.csv')
merged_feature_df.to_csv('../datasets/flight_delays_data_transformed_new.csv')
%run returns.py
%%bash $ module load afni $ cd /data/NCR_SBRB/simplex/xhmm $ for f in `ls *.DOC.sample_summary`; do tail -1 $f; done | cut -f 3 | 1d_tool.py -show_mmms -infile -
c = bnbAx[bnbAx['language']=='es'].first_browser.value_counts()/len(bnbAx[bnbAx['language']=='es']) $ c.head()
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_new, (1 - p_new)]) $ p_old_sim = old_page_converted.mean() $ p_old_sim
c = c.applymap(lambda x: x if isinstance(x, list) else [])
inspector = inspect(engine) $ inspector.get_table_names() $
file_path = "/mnt/idms/temporalNodeRanking/data/filtered_timeline_data/tsv/usopen/usopen_mentions.csv" $ au.recode_and_export_mentions(file_path,mentions_df,user_names)
xstep = a * 2**0.5 / 4  # atomic column separation along [10-1] direction $ xnum = 100 $ halfwidth = 2 $ x, disreg = am.defect.pn_arctan_disregistry(xstep=xstep, xnum=xnum, burgers=b, halfwidth=halfwidth)
morning_rush.iloc[:5]
df2[['CA','UK','US']] = pd.get_dummies(df2['country']) $ df2 = df2.drop(['US'],axis=1) $ df2.head()
f_ip_device_hour_clicks.show(1)
google['high'].apply(custome_roune).value_counts().sort_index()
df_countries['country'].unique()
from sklearn import metrics $ import matplotlib.pyplot as plt $ print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_testset, predTree))
_ = ok.submit()
n = len(qa_data) $ m = len(field_data) $ print('qa_data.csv has {} entries'.format(n), $      'fields_data.csv has {} entries'.format(m),sep='\n')
treatment_grp = df2.query('group == "treatment"') $ treatment_grp_prop = len(treatment_grp.query('converted == 1'))/len(treatment_grp) $ print ("probablility of the treatment group converted: ", treatment_grp_prop)
print('size of control group is {}'.format(len(df2_control))) $ print('size of treatment group is {}'.format(len(df2_treatment)))
shiny = pd.read_csv('\\\\allen\\programs\\celltypes\\workgroups\\rnaseqanalysis\\shiny\\patch_seq\\mouse_patchseq_VISp_20171204_collapsed90\\mapping.df.with.bp.90.csv')
out = pd.concat((out_train,out_test))
df = pd.DataFrame(results[:], columns=['Date', 'Precipitation']) $ df.set_index('Date', inplace=True, ) $ df.head()
import pandas as pd $ houses_train = pd.read_csv('../Data/encoded_houses_train.csv') $ houses_test = pd.read_csv('../Data/encoded_houses_test.csv')
building_pa_prc_zip_loc['permit_type'].unique()
df2= df.drop(delte, axis=0)
test = old_test.append(new_test).reset_index() $
test_norm.head(3)
df_new['country_US'] = df_new['country'].replace(('US', 'UK', 'CA'), (1, 0, 0)) $ lm_us = sm.OLS(df_new['converted'], df_new[['intercept', 'country_US']]) $ results_us = lm_us.fit() $ results_us.summary()
print(DataSet_sorted['tweetText'].iloc[3])
y_pred = lgb_model.predict(val)
stock_data.describe()
df_norm = (df - df.mean()) / df.std()
import statsmodels.api as sm $ import scipy.stats as stats $ logit = sm.Logit(df2['converted'],df2[['intercept' ,'ab_page']]) $ results = logit.fit()
with open('continentdict.txt', 'w') as g: $     g.write(str(continentdict)) $ with open('continentdict.txt', 'r') as f: $     print f.read()
df.drop_duplicates(['text'], keep='first',inplace=True) $ df.reset_index(drop=True, inplace=True)
store_items.fillna(method = 'backfill', axis = 0)
mgdata = finnal_data.merge(spire_event,on = "event") $ mgdata.to_csv("ungrouped_spire.csv")
df[df['Complaint Type'] == 'Noise - Residential']['Descriptor'].unique()
train.head()
plt.hist(p_diffs,bins=30) $ plt.axvline(x=(treatment_convert-control_convert),color="r");
model_lr = LinearRegression().fit(X_train, y_train) $ print('Training set accuracy: {:.2f}:'.format(model_lr.score(X_train, y_train))) $ print('Test set accuracy: {:.2f}:'.format(model_lr.score(X_test, y_test)))
plt.hist(p_diffs) $ plt.xlabel('Difference') $ plt.ylabel('Frequency') $ plt.title('Simulated difference of New Page and Old Page');
le = LabelEncoder() $ joined['CHURN']= le.fit_transform(joined['Churn']) $ joined = joined.drop(['Churn'], axis = 1) $ joined.head()
features = X.columns $ feature_importances = model.feature_importances_ $ features_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances}) $ features_df.sort_values('Importance Score', inplace=True, ascending=False) $ features_df
calendar['modeled_prices'] = np.nan
df_countries[['CA', 'UK', 'US']] = pd.get_dummies(df_countries['country'])
f_close_clicks_train.show(3)
autos["num_photos"].value_counts()
from pyspark.sql import Window, functions as F $ w1 = Window.partitionBy('user_id').orderBy('date') $ df_city_reviews = df_city_reviews.withColumn('seq', F.row_number().over(w1))
clean_users[clean_users['active']==0]['creation_source'].value_counts()
data.iloc[2, [3, 0, 1]]
new_scaled = data_scaled.join([model, year, country, custom, payment, hand, body, purch])
cand_date_df = cand_date_df[cand_date_df['sponsor_class'] != 'Other'] $ cand_date_df.head(2)
autos=autos[autos["price"].between(500,350000)]
r2_score(y_dev, best_model['model'].predict(X_dev))
grp1=df.query("group == 'treatment' and landing_page == 'new_page'") $ print('Total rows for correct landing of treatment group',len(grp1)) $ grp2=df.query("group == 'control' and landing_page == 'old_page'") $ print('Total rows for correct landing of control group',len(grp2)) $ print('Number of times new_page and treatment dont line up :: ',df.shape[0]-(len(grp1)+len(grp2))) $
client.experiments.list_runs()
twitter_archive_full[['retweet_count','favorite_count']].info()
n_old = df[df.landing_page=='old_page'].shape[0]
df.tail() $
tweets.shape
avg_per_seat_price_seasonsandteams = seasons_and_teams["Per Seat Price"].mean() #Takes the average of those transactions
fig, ax = plt.subplots(figsize=(12, 8)) $ sar.plot_diagnostics(fig=fig);
duplicates = df2[df2.duplicated(['user_id'])] $ duplicates.head()
learn.fit(0.01, 3, cycle_len=1)
encoder_model_inference = extract_encoder_model() $ encoder_model_inference.summary()
frame2.columns
max_open = [] $ for i in r_lol: $     if not i[1] is None: $         max_open.append(i[1])
list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists] $ df = pd.DataFrame(list_of_dicts) $ print(type(df)) $ print(df.head()) $
df2[df2.duplicated('user_id',keep=False)==True]
from sklearn.feature_extraction.text import CountVectorizer $ from sklearn.model_selection import cross_val_score, StratifiedKFold $ from sklearn.naive_bayes import MultinomialNB $ from sklearn.linear_model import LogisticRegression $ from sklearn.ensemble import RandomForestClassifier
X = reddit_master[['age', 'subreddit']].copy(deep=True) $ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
import numpy as np $ data['float_time'] =data['processing_time'].apply(lambda x:x/np.timedelta64(1, 'D'))
codes.shape
sns.jointplot(x = "negative_ratio", y = "wow_ratio", data = news_df)
print("Arrange multiple column values in ascending order") $ df.sort_values(by=['Indicator_id','Country','Year','WHO Region','Publication Status'], axis=0, inplace=True) $ df.sort_index(inplace=True) $ df[['Indicator_id','Country','Year','WHO Region','Publication Status']].head(3)
model = gensim.models.Word2Vec(texts_trigrams, size=100, window=10, min_count=30, workers=4)
dict1 = {k: g["VALUE"].tolist() for k,g in df.groupby("TUPLEKEY")} $ dict1[('A002','R051','02-00-00','LEXINGTON AVE')]
s3.reindex(np.arange(0,7), method='ffill')
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)])
result = pd.DataFrame(eclf2.predict_proba(test[features]), index=test.index, columns=eclf2.classes_) $ result.insert(0, 'ID', mid) $ result.to_csv("gaurav6.csv", index=False)
df2['intercept'] = 1 $ df2[['new_page', 'old_page']] = pd.get_dummies(df['landing_page']) $ df2.head()
dfNYC = pd.read_csv("311_Service_Requests_from_2010_to_Present.csv")
vect = CountVectorizer(stop_words=stop_words, ngram_range=(2,4), min_df=0.0001) $ X = vect.fit_transform(fashion.text)
merge_event.loc[(merge_event.location == 'United States')].groupby('nweek')['user_id'].nunique().plot(label="in US") $ merge_event.loc[(merge_event.location != 'United States')].groupby('nweek')['user_id'].nunique().plot(label="out US") $ plt.legend() $ plt.show()
y_pred = lassoreg.predict(X_test) $ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
plots.top_n_port_plots(10,traffic_type = 1) #Malicious traffic top 10 ports
df_input.toPandas().info()
float(result2[result2['dt_deces'].notnull()].shape[0]) / float(result2.shape[0]) * 100.
cust_demo.count() # no of non null values per column
print min(train_data['created'].apply(lambda d: d[:10])) $ print max(train_data['created'].apply(lambda d: d[:10]))
result = dta.groupby((dta.results, dta.dba_name)).size()
X_test = preprocessing.StandardScaler().fit(X_test).transform(X_test) $ X_test[0:5]
jackard_udf = F.udf(jaccard_similarity,types.DoubleType()) $ join_b = join_a.withColumn("name_similairity", jackard_udf(join_a["party_name_orig"],join_a["party_name"])) $ join_b = join_b.withColumn("address_similairity", jackard_udf(join_a["address1_orig"],join_a["address1"])) $ join_b = join_b.withColumn("st_name_similairity", jackard_udf(join_a["street_name_orig"],join_a["street_name"]))
df2.head()
clusterer = KMeans(n_clusters=3, random_state=324) $ cluster_labels = clusterer.fit_predict(cluster) $ cluster['cluster_labels'] = cluster_labels
number_failures = len(nyt_df[nyt_df.text=='']) $ number_successes = len(nyt_df[nyt_df.text!='']) $ success_percent = float(number_successes)/(number_failures+number_successes) $ print 'Percent of Articles successfully scraped:', success_percent $ print 'Number of Articles successfully scraped:', number_successes
crimes['month'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.month)
fulldata_copy['RSI'] = rsi['RSI'] $ fulldata_copy.columns = ['AUDUSD','RSI']
autos = autos[autos["registration_year"].between(1900, 2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
student_info = [(100, 62, 'F'), (120, 66, 'M'), (140, 68, 'M'), (110, 62, 'F'), (160, 70, 'M'), (140, 63, 'F'), (140, 66, 'F'), (110, 63, 'F'), (180, 72, 'M'), (190, 72, 'M'), (200, 73, 'M')] $ names = ['Mary', 'Mike', 'Joe', 'Janet', 'Steve', 'Alissa', 'Alison', 'Maya', 'Ryan', 'Paul', 'Michael'] $ students = pd.DataFrame(student_info, columns = ['weight', 'height', 'gender'], index = names) $ students
index_missin_hrafter6_before2016 = taxi_hourly_df.loc[(taxi_hourly_df.missing_dt == True) & $                                                     (taxi_hourly_df.index.hour > 6), : $                                                    ].index
df2[df2.landing_page == 'new_page'].count()/df2.shape[0]
num_of_mentions_by_day = mentions_df["date"].value_counts().sort_index()
convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'")) $
all_df.dtypes
sns.countplot(y="objecttype",data=firstWeekUserMerged) $ plt.show()
item_onpromotion = train_holiday_oil_store_transaction_item_test.select("item_nbr", *exprs)
df_new.columns
station_distance = TripData_merged3 $ station_distance.iloc[:2] $ station_distance.shape
test_vecs = proc.transform(test_docs)
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending = False) $
__ = pd.read_xxx(filepath_or_buffer='./data/FB.csv')
msft = pd.read_csv('msft.csv') $ msft.head()
images.info()
%%bash $ wget "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" $ ls
print("Probability of treatment group converting is", $       df2[df2['group']=='treatment']['converted'].mean())
df['intercept']= 1 $ df[['control','treatment']] = pd.get_dummies(df['group'])
col_info = pd.DataFrame() $ df = data_sets['60min'] $ for level in df.columns.names: $     col_info[level] = df.columns.get_level_values(level) $ col_info
pold = df2[df2['landing_page']=='old_page']['converted'].mean() $ pold
ad_group_performance.loc[:5]
df.info()
mc_results_x = ssm.MultiComparison(df['y'], df['x']) $ mc_results_x_tukey_hsd = mc_results_x.tukeyhsd() $ print(mc_results_x_tukey_hsd)
new = collections.Counter(number_list) $ new
df_complete['dog_stage'].value_counts()
y.value_counts(normalize=False).apply(lambda x: len(y)/(100*x)).to_csv('classweights.csv', header=None)
master_file = pd.read_csv(os.curdir + '/master.csv', encoding='iso-8859-1', dtype=object)
for i in files_to_manage: $     main_tables[i].to_csv('{}_2017.csv'.format(i))
print(area.index) $ print(population.index) $ print(states.index)
twitter_archive_master.loc[(twitter_archive_master.name == 'None'),'name'] = 'Unknown'
plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS,  color='blue') $ plt.xlabel("Engine size") $ plt.ylabel("Emission") $ plt.show()
pandas_df = df.toPandas().join(df_loc)
df_more[df_more.Manager != 0]
baseball.describe()
df_more = df_more.join(df_more['Location'].str.split(',', 1, expand=True).rename(columns={0:'City', 1:'State'}))
threeoneone_census_complaints.columns
noNulls = df.dropna(how='any')
data = pd.read_csv("train.csv")
autos['ad_created'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True).hist()
fav_max = data['Likes'].max() $ fav_tweet = data[data['Likes'] == fav_max] $ print("The tweet with more likes is: \n{}".format(np.array(fav_tweet['Tweets'])[0])) $ print("Number of likes: {}".format(fav_max)) $ print("{} characters.\n".format(np.array(fav_tweet['len'])[0]))
p_new_sim = new_page_converted.mean() $ p_old_sim = old_page_converted.mean() $ print('diff: ', p_new_sim - p_old_sim)
len(table3['device_id'].unique())
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Categorical(temp, ordered=True) $ temp_cat
df[df.index.month.isin([6,7,8])]['Complaint Type'].value_counts().head()
for remove in map(lambda r: re.compile(re.escape(r)), $                   [",", ":", "\"", "=", "&", ";", "%", "$", "@", "%", "^", "*", "(", ")", "{", "}", $                    "[", "]", "|", "/", "\\", ">", "<", "-", "!", "?", ".", "'", "--", "---", "#", "..."] $                  ): $     test.replace(remove, " ", inplace=True)
emotion_big_df.head()
np.mean(labels)
from_sql = pd.read_sql_query("SELECT * FROM Tweets_London;", conn)
dfRegMet = df[df["latitude"] > -34.164060]
resdatasep=[] $ for i in range(len(resdata)): $     for j in range(len(resdata[i]['restaurants'])): $         resdatasep.append(resdata[i]['restaurants'][j])
df.drop(df[df.zipcode_initial.isin(['GU214ND','94000'])].index, axis=0, inplace=True)
print(p.shape) $ print(p)
tmp = combined.sort_values('zika_cases',ascending=False).head(1000) $ tmp.cm_type.value_counts()
X_train.fillna(X_train.mean(), inplace=True)
access_logs_df.limit(10).show()
cust_clust = crosstab.copy() $ cust_clust['cluster'] = c_pred $
%run ./train.py --logtostderr --train_dir=images/ --pipeline_config_path=images/ssd_mobilenet_v2_coco.config
m = np.mean(x, axis=0); m  #m has shape (3,).
pd.options.display.max_columns = 200
lm2 = smf.ols(formula='Delta ~ IP', data=fx).fit() $ lm2.params $ lm2.summary()
df_2012.dropna(inplace=True) $ df_2012
pca.explained_variance_ratio_
more_200 = tweet_df['username'].value_counts() > 200 $ dublicated_users = more_200[more_200].index
recommendation_df['challenge_count'] = recommendation_df.groupby(['challenge_id'])['score'].transform('count')
df2.query('group == "control"') $ df2.query('group == "control" and converted == 1') $
results.summary() $
treat2 = treat[treat['landing_page'] == 'new_page'] $ contr2 = contr[contr['landing_page'] == 'old_page'] $ df2 = treat2.append(contr2) $ df2.head(), df2.tail()
q = ['what', 'when', 'where', 'while', 'who', 'why', 'which', 'whom', 'whose', 'how'] $ qfd = nltk.FreqDist([word for word in monty if word in q]) $ display(qfd)
stream = ssc.textFileStream('sample/')
 j = pd.concat([a, b], axis=1, join_axes=[a.index])
dfm = df.median(axis=0)
def get_hdfs_client(): $     return InsecureClient("pheno0.phenovari-utwente.surf-hosted.nl:50070", user="pheno", $          root="/")
dataset_ja.set_index(['customer_id']) $ dataset_ja.loc[dataset_ja['customer_id'] == "000011265b8a3727c4cc77b494134aca"]
d = tran_time_diff[tran_time_diff.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro='] $ d
conv_treat_users = df2.query('converted == 1 and group == "treatment"').shape[0] $ treatment_users = df2.query('group == "treatment"').shape[0] $ p3 = conv_treat_users /treatment_users $ print(" Given that an individual was in the treatment group, the probability they converted is {:.4f}".format(p3))
user_ips = log_data \ $     .map(lambda line: line.split()) \ $     .map(lambda words: (words[IP_ADDRESS],words[USER_ID])) \ $     .groupByKey()
ols_model = statsmodels.formula.api.ols('sentiment ~ group', data=name_sentiments).fit() $ ols_model.summary().tables[0]
final_feat = telemetry_feat.merge(error_count, on=['datetime', 'machineID'], how='left') $ final_feat = final_feat.merge(comp_rep, on=['datetime', 'machineID'], how='left') $ final_feat = final_feat.merge(machines, on=['machineID'], how='left') $ print(final_feat.head()) $ final_feat.describe()
modeling1.show(5)
np.exp(0.0507), np.exp(0.0408)
merged_NNN = pd.merge(committees_NNN, contributions, on="calaccess_committee_id")
page = requests.get('https://www.coursera.org/learn/songwriting-lyrics') $ soup = BeautifulSoup((page).content, 'html.parser') $ result = soup.find("div", {'class': 'XdpApp'})
autos['price'] = autos['price'].str.replace("$","").str.replace(",","").astype(int) $ autos['price'].head()
%%timeit -n 100 $ summary = np.sum(ser7)
weights_A=pd.Series([85.1, 90.2, 76.8, 80.4,78.9],index=['s1','s2','s3','s4','s5'])
stations = session.query(func.count(Station.id)).scalar() $ print(stations)
result.sum() $ result.mean() $ result.mean(skipna=False) $ result.describe()
def web3_sha3(data): $     data = str(data).encode('hex') $     return _call('web3_sha3', [data])
res.summary()
with open('example.com.txt', 'w', encoding='utf-8') as f: $     f.write(html)
cluster["ZIPCODE"] = cluster.index
grouped.size()
got_data.sort_values('total_likes', ascending=False, inplace=True)
sns.countplot(x="inactivePeriod",data=X) $ plt.show() $ sns.countplot(x="activeDay",data=X) $ plt.show()
dummy = pd.get_dummies(furniture, columns=['category'],drop_first=True)
df_big = df_big.drop(columns=['doggo', 'floofer', 'pupper', 'puppo'])
lm = sm.Logit(df_new['converted'],df_new[['intercept','treatment','UK','US']]) $ results = lm.fit() $ results.summary()
def finish_check(name): $     b = phimage(name) $     l1 = b.time_steps() $     print(f"{name}: last = {l1[-1]}, btime = {b.BTime()}")
new_features = tfidf_vectorizer.transform(sdf.Norm_reviewText) $ new_features_tfidf = new_features.todense()
nold=df2[df2.landing_page=="old_page"].count()[0] $ nold
browser.get("http://stats.nba.com/scores/#!/{}".format(date_url)) $ time.sleep(10) $ soup = bs(browser.page_source, "html5lib")
df_293 = pd.read_sql(sql_293,conn_laurel) $ df_293.groupby(['accepted','paid','preview_clicked','preview_watched','preview_finished'])['applicant_id'].count().unstack().fillna(0)
ls_not_numeric = [not pd.api.types.is_numeric_dtype(dtype) for dtype in df_uro.dtypes] $ prog = re.compile('DATE[0-9]*$') $ ls_not_date = [not bool(prog.search(colname)) for colname in df_uro.columns] $ ls_both = [num and date for num, date in zip(ls_not_numeric, ls_not_date)] $ df_uro.loc[:,ls_both].nunique()
mean=a.mean() $ print(mean)
df.to_csv('../output/releases_with_demographics.csv')
s.index.freqstr
weather.loc[weather.precipitation_inches.isnull(), $             'precipitation_inches'] = weather[weather.precipitation_inches.notnull()].precipitation_inches.median()
df=pd.read_csv("dataset_quora/quora_train_test.csv")
tweets_df.head()
tizibika.head()
new_page_converted = np.random.normal(p_new,np.std(p_new), n_new)
def sentiment_scores(doc): $     snt = analyser.polarity_scores(doc) $     return snt
preds_df.describe()
access_logs_raw = spark.sparkContext.textFile('data/apache.log')
company_page.content
chk=pd.read_csv('sub_average_5models_nocv.csv') $ chk.head(10)
offseason09 = ALL[(ALL.index > '2009-02-01') & (ALL.index < '2009-09-10')]
never_moved = df_change_count[df_change_count['Count'] == 1]['Count'].count()
print(len(df2.query("landing_page == 'new_page'")) / df2.shape[0])
df_hmeq_card['Percent_Missing']  = df_hmeq_card._NMISS_ / df_hmeq_card._NOBS_ * 100 $ df_hmeq_card.plot.bar('_VARNAME_','Percent_Missing', title='Percentage of Missing Values', legend=False)
joined.to_feather(f'{PATH}joined') $ joined_test.to_feather(f'{PATH}joined_test')
agency_borough['BRONX'].sort_values(ascending=False)[:5] 
SeriesJota = pd.Series(ListJota) $ SeriesJota
closes.ix['MSFT'][:3]
country_lm = sm.Logit(df3.converted, df3[['intercept','ab_page','US','UK']]) $ country_result = country_lm.fit() $ country_result.summary()
predictions = model.predict(test_words)
criteria = so['ans_name'].isin(['Scott Boston', 'Ted Petrou', 'MaxU', 'unutbu']) $ so.loc[criteria].head()
df_predictions_clean.p3 = df_predictions_clean.p3.str.title()
print('Jumlah data: ', len(df)) $ print('Jumlah fitur: ', len(df.columns)-1)
closingPrices.nlargest(5)
autos['seller'].value_counts()
train_dum.drop(['Source_S130', 'Source_S135', 'Source_S140', 'Source_S154', 'Source_S160', $                'Customer_Existing_Primary_Bank_Code_B056'], axis=1, inplace=True) $ test_dum.drop(['Source_S126', 'Source_S131', 'Source_S132', 'Source_S142'], axis=1, inplace=True)
old_page_converted=np.random.choice([1,0],p=(P_old,1-P_old),size=n_old) $ len(old_page_converted)
referred_users = clean_users[clean_users['invited_by_user_id'] > 0] $ unreferred_users = clean_users[clean_users['invited_by_user_id'].isnull()]
print("tweet archive contains {} entries.".format(len(tweet_archive_df.tweet_id))) $ print("tweepy api retrieved {} entries.".format(len(tweet_json_df.id)))
import matplotlib.pyplot as plt $ df[['Date','GasPrice']].set_index('Date').plot()
model2=sm.Logit(df_new['converted'],df_new[['intercept','treatment','CA','UK']]) $ result_2=model2.fit() $ result_2.summary()
print(autos.info()) $ autos.head()
X_valid = mnist.validation.images $ y_valid = mnist.validation.labels
df2 = df.set_index('key') $ mapping = {'A': 'vowel', 'B': 'consonant', 'C': 'consonant'} $ display('df2', 'df2.groupby(mapping).sum()')
spider = api.Crawler('demoowshq', 'teste', '2017-11-11')
from nltk.tokenize import TweetTokenizer
url =  "http://www.basketball-reference.com/teams/ATL/executives.html"
train.to_csv('Hold/train2.csv', index=False) $ test.to_csv('Hold/test2.csv', index=False)
paragraphs = tree.xpath('//p') $ for p in paragraphs: $     print(p.text)  
X_btc.head(5)
from scipy.stats import norm $ norm.cdf(z_score)
df_meta = pd.read_table('data/meta_data.tsv', encoding='latin-1') $ df_meta.head()
%matplotlib inline $ import matplotlib.pyplot as plt
for i in files_to_manage: $     print i $     account.files_delete(path="/nba games/test_ws_workflow/{}_2017.csv".format(i))
df['Market'].value_counts()
print('Accuracy on the training data:', grid.score(Xtrain, ytrain)) $ print('Accuracy on the testing data:', grid.score(Xtest, ytest))
cityID =  '5a110d312052166f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Francisco.append(tweet) 
with rio.open(merge_file, "r") as src: $     print(src.profile) $     window = ((10000, 100000), (10000, 60000)) $     data = src.read(indexes=4, window=window)
ridgereg = Ridge(alpha=.1) $ ridgereg.fit(X_train, y_train) $ y_pred2 = ridgereg.predict(X_test) $ print np.sqrt(metrics.mean_squared_error(y_test, y_pred1))
plt.plot(ds_ossm['time'],ds_ossm['met_salsurf'],'r.') $ plt.title('CP04OSSM, Sea Surface Salinity') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
df.count()
filter = titanic.notnull() $ filter.head()
run txt2pdf.py -o '2018-06-22 2012 FLORIDA HOSPITAL Sorted by discharges.pdf'  '2018-06-22 2012 FLORIDA HOSPITAL Sorted by discharges.txt' $
df.puppo.value_counts()
p_new=df2.converted.mean() $ p_new
grid.best_params_
n_user_days.value_counts().sort_index()
plt.figure(0) $ source_counts = df['sourceurl'].value_counts().head(10) $ source_counts.plot.bar()
print('date, previous_wednesday') $ for test_date_offset in range(16): $     dt = datetime.date(2017,7,18) + datetime.timedelta(days=test_date_offset) $     offset = (dt.isoweekday() + 4) % 7 $     print(dt, dt - datetime.timedelta(days=offset))
tweets['hashtags'] = tweets['hashtags'].apply(lambda x: x.strip('[]').lower().replace("'",'').split(', ')) $ tweets['user_mentions'] = tweets['user_mentions'].apply(lambda x: x.strip('[]').lower().replace("'",'').split(', '))
df_countries.head()
data_splitted = data.split("\n") $ print(data_splitted[0])
df.dealowner.unique()
df3 = df.copy() $ df3 = df3.drop('ENTRIES',1) $ df3 = df3.drop('EXITS',1) $
sample.asfreq('H',method='bfill')
df_new.drop(binary_cols, axis=1, inplace=True)
plt.hist(daily_trade_volume) $ plt.ylabel('Daily Trading Volume') $ plt.show()
engine = enginegetter.getEngine()
zone_props = [["upw.ss",0], ["rch.rech",0],["rch.rech",1]] $ k_zone_dict = {k:zn_array for k in range(m.nlay)} $
soup.ul.contents
master_df.name.isnull().sum()
data.drop(['ceil_10min_x','ceil_10min_y'], axis=1,inplace=True)
tweetering.corr()
device_browser = train_data.groupby(['device.browser']).agg({'totals.transactionRevenue': 'sum'}).reset_index() $ device_browser = device_browser.assign(pct = (device_browser["totals.transactionRevenue"]/device_browser["totals.transactionRevenue"].sum())) $ device_browser.set_index("device.browser",drop=True)["pct"].plot.bar()
foot=football.set_index(['date']) $ foot.head()
text=soup.get_text().encode('ascii','ignore') $ print text
archive_clean.info()
DataSet = DataSet[DataSet.userTimezone.notnull()] $ len(DataSet) $ len(DataSet)/5000*100 $
subs = subs[subs.veredict == 'AC'] $ subs.describe()
df.apply(lambda a: sum(a.isnull()),axis=0) 
flood_reports.created_date.dt.dayofyear.nunique()
df_ll.head(2) $ df_ll.isDuplicated.value_counts() $ df_ll.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
df[df['public']=='offline'].count()[0]
my_columns = list(data.columns) $ my_columns
cityID = '0eb9676d24b211f1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cleveland.append(tweet) 
karma_per_user = scores.groupby('author')['score'].sum() $ karma_per_user.sort_values(ascending=False)
maxData = flowerKV.reduceByKey(lambda x, y: max(float(x),float(y))) $ print (maxData.collect()) $
apple.set_index('Date', inplace=True) $ apple.head()
data1Scaled['Label'] = np.array(data1['adj close'].shift(-(daysToForecast)).values)
speeches_cleaned.columns
train_data.notRepairedDamage.fillna('nein', inplace = True) $ test_data.notRepairedDamage.fillna('nein', inplace = True)
train.isnull().sum()[train.isnull().sum()>0]
pd.read_pickle('data/city-util/proc/misc_info.pkl', compression='bz2')
logit_ab_country = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA', 'ab_page','US_ab_page', 'CA_ab_page']]) $ results_ab_country = logit_ab_country.fit()
nt.columns
print('Conversion Rate Considerations (against US)') $ print('Canada: {}X more likely to convert than US'.format(np.exp(.0099))) $ print('UK: {}X less likely to convert than US'.format(1/np.exp(-.0408)))
import os # To use command line like instructions $ if not os.path.exists(DirSaveOutput): os.makedirs(DirSaveOutput)
top_20_breed_stats = top_20_breed_archive.groupby('dog_breed').mean()
two_day_sample.reset_index(inplace=True)
c_df.describe()
url = '/Users/jennawhite/documents/wild_west/abnb_project/data/train_users_2.csv' $ bnb = pd.read_csv(url, index_col='id') $ bnb.shape $
def clean_stopwords(text): $     stopwords = set(nl.corpus.stopwords.words('portuguese')) $     words = [i for i in text.split() if not i in stopwords] $     return (" ".join(words))
movies.shape
API_KEY = 'MHso4itbSsk44SDbsyWv' $ import requests $ import numpy as np $ url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-05-12&end_date=2018-05-12&API_KEY'
df = pd.DataFrame(results, columns=['date', 'precipitation']) $ df.set_index(df['date'], inplace=True) $ df.tail()
rfe = RFE(lg, n_features_to_select=None,step = 1, verbose=2 ) $ rfmodel = rfe.fit(X_train,y_train)
df.rename(columns={'id_x':'Matching Parcel status', $                    'id_y':'Matching Shipping Method id', $                    'name':'Shipping Method name', $                    'message':'Parcel Status'}, $           inplace=True)
suburban_summary_table = pd.DataFrame({"Average Fare": suburban_avg_fare, $                                     "Total Rides": suburban_ride_total}) $ suburban_summary_table.head()
df['com_label'] = np.where(df['comms_num']>=np.median(df.comms_num), 'High', 'Low') $ df['score_label'] = np.where(df['score']>=np.median(df.score), 'High', 'Low')
bnbx.shape
db.collection_names(include_system_collections=False)
df.duplicated().sum()
df1.describe()
sns.jointplot(x='sepal_length', y='sepal_width', data=data, size=4)
df = pd.read_csv('./Data/AAPL.csv', index_col=0) $ df.head()
Base = automap_base() $ Base.prepare(engine, reflect=True)
last_year.shape
telemetry['datetime'] = pd.to_datetime(telemetry['datetime'], format="%Y-%m-%d %H:%M:%S")
df2 = pd.io.json.json_normalize(data=data) $ df2.head() $ df2.columns
fpr, tpr, threshold = metrics.roc_curve(y_test, probability [:,1])
result.to_csv(r'X:\data_report\01.Result\Karir.com\result.csv', encoding='utf-8', index=False)
hmeq_scored_gb.head()
Zn1Zn = (model.transmat_.T * Zn_post).T
df.life_sq.isnull().sum()
sets_node = fields_node['oSets'] $ sets_node.keys()
plt.show()
np.exp(results.params)
interest_dict = {'low':0, 'medium':1, 'high':2} $ data.replace(to_replace=interest_dict, inplace=True)
zero_age = cats_df[cats_df['age at death'] <= 0]['age at death'] $ cats_df['remove'].iloc[zero_age.index] = True $ del zero_age
error_set = df.query('group == "treatment" & landing_page !="new_page" | group != "treatment" & landing_page =="new_page"' ) $ error_set.head(2)
print len(hpdcom) $ print len(hpdpro) $ hpd = pd.merge(hpdpro, hpdcom, how='outer', on='ComplaintID') $ print len(hpd)
print(clf.class_count_)
tweet_archive_clean.name
to_be_predicted_Day3 = 50.63320999 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
print(dd.feature_metadata.columns)
chart = top_supporters.head(5).amount.plot.barh()
tweet_counts_by_hour = pd.concat([tweet_counts_by_hour.groupby('timestamp').sum(), $                                   tweet_counts_by_hour.groupby('timestamp').count()['favorite_count'].rename('tweet_count')], $                                  axis = 1) $ tweet_counts_by_hour.index.names = ['hour of day'] $ tweet_counts_by_hour.plot()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}".format(API_KEY), $                  auth=('user', 'pass'))
data_vi.index.weekday
from sklearn.ensemble import ExtraTreesRegressor $ auto_model = ExtraTreesRegressor(random_state=23) $ auto_model = auto_model.fit(ind_train, dep_train) $ print('Score = {:.1%}'.format(auto_model.score(ind_test, dep_test)))
joined.head()
print(regression_model.coef_) $ print(regression_model.intercept_)
eg.index.values
(autos["last_online"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
import pandas as pd $ pd.read_json('http://api.open-notify.org/iss-now.json')
import gcsfs $ import google.datalab.bigquery as bq $ import pandas as pd
trump.describe()
finals[finals.PLAYER_NAME.str.contains("Durant")]
marijuanamatches = props[props.prop_name == 'PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE.'] $ marijuanamatches.head()
df = df.set_index('email')
os.chdir('input')
ins.head(5)
autos.info()
sc.stop()
df_new['intercept'] = 1 $ lm = sm.OLS(df_new['converted'], df_new[['intercept','ab_page','uk','us']]) $ results = lm.fit() $ results.summary()
!wget https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv $ !head -n 2 Consumer_Complaints.csv
w_mat, b_mat = create_wbl(mcap_mat, p=c) $ coins_all = b_mat.sum().index $ coins_infund = coins_all[b_mat.sum() > 1] $ print("Coins that once has been in top 10 \n", coins_infund)
import pandas as pd $ iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)
obj.rank(ascending=False, method='max')
df_raw['list_date'] = pd.to_datetime(df_raw.list_date, errors='coerce')
builder.where(F.col('endcustomerlinefixed_data')=="HERC RENTALS").select(builder.endcustomerlinefixed_data,builder.decision_date_time_data,builder.sales_acct_id_data,builder.sales_acct_id_savm).count()
description_predict_clean = clean_corpus(description_predict) $ predicted_2018 = text_clf.predict(description_predict_clean)
movies['Position'].head()
df.sample(10)
cand_date_df['sponsor_class'].value_counts()
education=pd.read_csv('data/crime/final_education.csv')
optim_etf_returns = generate_weighted_returns(returns, optimal_single_rebalance_etf_weights) $ optim_etf_cumulative_returns = calculate_cumulative_returns(optim_etf_returns) $ project_helper.plot_benchmark_returns(index_weighted_cumulative_returns, optim_etf_cumulative_returns, 'Optimized ETF vs Index') $ optim_etf_tracking_error = tracking_error(np.sum(index_weighted_returns, 1), np.sum(optim_etf_returns, 1)) $ print('Optimized ETF Tracking Error: {}'.format(optim_etf_tracking_error))
df_main = pd.merge(df_main, preds_clean,on='tweet_id', how='inner')
df_goog.info()
for Quarter in range(1,5): $     StockData['Date-Quarter{}'.format(Quarter)] = (StockData['Date'].dt.quarter == Quarter).astype(int)
import numexpr $ mask_numexpr = numexpr.evaluate('(x > 0.5) & (y < 0.5)') $ np.allclose(mask, mask_numexpr)
lims_query = "SELECT name, barcode FROM specimens \ $ WHERE specimens.ephys_roi_result_id IS NOT NULL" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
df2 = df[['FlightDate', 'DayOfWeek', 'Carrier', 'TailNum', 'FlightNum', 'Origin', $          'OriginCityName', 'OriginStateName', 'Dest', 'DestCityName', 'DestStateName', $          'DepTime', 'DepDelay', 'AirTime', 'Distance']] $ df2.sample(5)
covtype_df.head()
df_q = pd.read_sql(query, conn, index_col=None) $ df_q.head(10)
def get_duration_career(input_): $     return max(input_) - min(input_) $ grouped_publications_by_author['duration_career'] = grouped_publications_by_author['publicationDates'].apply(get_duration_career) $
df2 = pd.read_csv('ab_data2.csv')
def preprocess(text): $     stemmer = PorterStemmer() $     split1 = ' '.join([word for word in re.split('\W+', text) if word.isalpha()]) $     split2 = ' '.join([stemmer.stem(word) for word in re.sub('(?!^)([A-Z][a-z]+)', r' \1', split1).split()]) $     return split2
result = api.search(q='%23HarryPotter') $ len(result)
session_data = results_dict['session_summaries'][0] $ print("Data is saved as a {} object.\nSession Completed On: {}".format(type(session_data), $                                                                        session_data.session_datetime))
Z = np.ones((5,5)) $ Z = np.pad(Z, pad_width=2, mode='constant', constant_values=0) $ print(Z)
df.loc[df.userLocation == 'Youngstown, Ohio', :]
regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols, $                                       hidden_units=[50, 50], $                                       model_dir='tf_wx_model')
pd.to_datetime('11/12/2010', format='%d/%m/%Y')
df.to_csv(data_path+"/master.csv")
to_be_predicted_Day5 = 48.64925054 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df_postgres = psql.read_sql('SELECT * FROM postgres LIMIT 5;', con=conn) $ df_postgres.head()
df_big = df.loc[(df['doggo'] == 'None') & (df['floofer'] == 'None') & (df['pupper'] == 'None') & (df['puppo'] == 'None')]
train_df.dtypes
reviews.groupby('price').points.max().sort_index()
data_full = data_full.drop(['customer_id', 'invoice_id', 'created_at', 'due', 'last_payment_date', 'paid_status_PARTIAL', 'paid_status_UNPAID', 'customer_created_at'], axis = 1)
df.groupby('Team').groups
p_mean=np.mean([p_new, p_old]) $ new_page_converted= np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)]) $ new_page_converted.mean()
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3]) $ ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6]) $ pd.concat([ser1, ser2])
auto = auto.drop("DatePosted", axis=1) $ auto = auto.drop("Difference_Date", axis=1)
b_cal_q1.dtypes
old_page_converted=np.random.choice([1,0],size=nold,p=[pold,1-pold])
sibsp = df_titanic['sibsp'] $ print(sibsp.describe()) $ sibsp.value_counts()
movies.describe()
to_be_predicted_Day2 = 31.2931565 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
a = df2['landing_page'] == 'new_page' $ new_page_received = len(df2[a])/len(df2) $ print('probability that an individual received the new page:{}'.format(new_page_received))
models = autos["model_brand"].value_counts(normalize = True) $ represented_models = models[models > .02].index $ print(represented_models)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') $ print ('z-score:',z_score) $ print ('p-value:',p_value)
authors_grouped_by_id_saved.show()
user1 = user1[(user1.CallDate.dt.dayofweek == 5) | (user1.CallDate.dt.dayofweek == 6) ] $
goalkeeping_df.to_pickle('goalkeeping_df.pkl')
microsaccades = MICROSACC.detect_microsaccades(etsamples=etsamples,etmsgs=etmsgs,etevents=etevents)
unique_companies_in_secret_psc_count = secret_corporate_pscs.groupby(['first_and_postcode'])['name']\ $         .agg(lambda x: len(x.unique())).sort_values(ascending=False)
df_clean3.loc[2038, 'text']
education_data.head(14)
origin_dataset = api.create_dataset(source)
df.info()
dt.datetime.strptime("2016-W52" + '-0', "%Y-W%W-%w")
df['speaker'].value_counts()
df_breed= df[['breed_fin','rating_numerator', 'favorite_count', 'retweet_count']].groupby('breed_fin').mean() $ df_breed= df_breed.sort_values('rating_numerator', ascending=False) $ df_breed= df_breed.iloc[:-1,] #this seemed to be an outlier skewing the graph uneccessarily
control_grp = df2.query('group == "control"') $ control_grp_prop = len(control_grp.query('converted == 1'))/len(control_grp) $ print ("probablility of the control group converted: ", control_grp_prop)
from sklearn.linear_model import SGDClassifier
by_party_type = candidates.groupby(["party_type", "election_year"]).size().reset_index()
tweetsDf.lang.hist()
dfdaycounts.head()
BLINK.plot_duration(blink,option="facet_subjects")
np.full((2,2),7)
accuracy_score(model.predict(test_vectors), test_targets)
load2017inh= load2017[load2017.index % 4 == 0] $ load2017inh.head()
lasso = Lasso(alpha=0.0002) $ lasso.fit(train_data, train_labels)
df = pd.concat([df, race_vars], axis=1) $ df = pd.concat([df, gender_vars], axis=1)
week27 = week26.rename(columns={189:'189'}) $ stocks = stocks.rename(columns={'Week 26':'Week 27','182':'189'}) $ week27 = pd.merge(stocks,week27,on=['189','Tickers']) $ week27.drop_duplicates(subset='Link',inplace=True)
lq.dtypes
extract_all.loc[(extract_all.APP_SSN==149640712)]
df2[df2['group'] == 'treatment'].mean()['converted']
cX_test.head()
deaths_by_decade = pd.DataFrame(explotions.groupby('decade')['Deaths'].sum())
df1 = df.drop(['Area Id','Variable Id','Symbol'],axis=1)
to_be_predicted_Day5 = 48.60641021 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
ratings.loc[ratings.rating_denominator > 10].head(3)
details.to_pickle('../../data/processed/Movies_Details.pkl')
another_set = {2, 2.0, 'two', 'Two', 100, 'one hundred'}
pd.set_option('display.max_colwidth', -1)
new_page_converted = np.random.choice([0,1] , size = n_new, p=[p_new, (1-p_new)]) $ len(new_page_converted)
%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)), dtype=x.dtype, count=len(x))
df_master.info()
print len(stations.name.unique()) $ print stations.shape
df['gender']=df['gender'].replace({'Male': 0, 'Female': 1}) $ for col in ['Partner','Dependents','PhoneService','PaperlessBilling','Churn']: $      df[col].replace({'Yes': 1, 'No': 0}, inplace=True)
intervention_train[intervention_train.index.duplicated(keep=False)]
uber_14["hour_of_day"].value_counts()
ts.truncate(before='10/31/2011', after='12/31/2011')
autos["kilometer"] = autos["kilometer"].str.replace(",","") $ autos["kilometer"] = autos["kilometer"].str.replace("km","") $ autos["kilometer"] = autos["kilometer"].astype(int) $ autos["kilometer"].head(3)
temperature_sensors_list = [sensor for sensor in sensors_list if 'temperature' in sensor.split('.')[1]] $ temperature_sensors_list
pd.date_range(end='2012', periods=7*12, freq='M')
comments.columns
df.to_csv('top40_processed.csv', date_format="%Y-%m-%d", index=False)
table3= table1.copy() $ for elem in table3['source'].unique(): $     table3[str(elem)] = table3['source'] == elem $ table3.head(3)
kick_projects = kick_projects.replace({'country': 'N,0"'}, {'country': 'NZERO'}, regex=True)
plt.figure() $ x = [1,3,4,6,8,9,0] $ y = [2,5,7,10,4,7,2] $ plt.scatter(x,y,c = 'red', s=0,80) $ plt.show()
graf_counts2 = pd.DataFrame(graffiti2['graffiti_count'].groupby(graffiti2['precinct']).sum())
motion_sensors_df = parsedDF[parsedDF['entity_id'].isin(motion_sensors_list)] $ motion_sensors_df['state'] = motion_sensors_df['state'].apply(lambda x: binary_state(x)) # Binarise $ motion_sensors_df= motion_sensors_df[motion_sensors_df['state']!=0]                      # Get only motion events $ motion_sensors_df.head()
driver = webdriver.Firefox(executable_path = '/usr/local/bin/geckodriver')
unseen_predictions_df = pd.concat([pd.DataFrame(unseen_predictions, columns=['Predictions'], index=X_unseen.index), y_unseen], axis=1) # Test predictions $ unseen_predictions_df $
from sklearn import model_selection $ kfold = model_selection.KFold(n_splits=10, shuffle=True) $ loocv = model_selection.LeaveOneOut()
list = [] $ for paragraph in microsoft.paragraphs: $     list.append(paragraph.text) $ microsoftAnnual = pandas.DataFrame({'Paragraph Content':list}, index = range(len(list)) ) $ print( microsoftAnnual[:80])
df2['intercept'] = 1 $ df2[['ab_page_to_drop', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop(['ab_page_to_drop'], axis=1) $ df2.head()
model_country = Logit(df2['converted'], $                            df2[['intercept', 'CA', 'UK']]) $ results_country = model_country.fit() $ results_country.summary()
S_lumpedTopmodel.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
nba_df[nba_df.isnull().any(axis=1)].loc[:, ["Season", "Team", "G", "Opp", "Home.Attendance", "Referee3"]]
hashtags = df.text.str.extractall(r"#(\w+)") $ mask = hashtags.loc[:, 0].value_counts() > 4 $ hashtags.loc[:,0].value_counts()[mask]
df.game_type.value_counts()
station_count["Count"].hist(bins=12, color="darkblue") $ plt.title("Histogram: Observation count by station") $ plt.savefig("Histogram Observation count by station") $ plt.show()
regr.score(experiment_X, experiment_y)
output= "SELECT * from user where user_id='@Pratik'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Followers','Following','TweetCount'])
ranked.set_index('date').resample('M').count()
dat_dow.vgplot.line(value_name='Hospital mortality rate')
active_psc_records.month_year_birth.hist(figsize=(20,5),bins=50)
df = df.drop_duplicates().dropna()
sample_mean = new_page_converted.mean() - old_page_converted.mean() $ sample_mean
autos["ad_created"].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True)
num_newpage = df2[df2['landing_page'] == 'new_page']['user_id'].count() $ num_total = df2['user_id'].count() $ p_newpage = num_newpage / num_total $ print('The probability of an individual receiving the new page: ', p_newpage)
ppt = session.query(Measurement.date,Measurement.prcp).filter(Measurement.date.between ('2016-08-23','2017-08-23')).order_by(Measurement.date).all() $ ppt $
data['manufacturer'].value_counts().sort_index()
questions = pd.concat([questions.drop('bands', axis=1), bands], axis=1)
data.info()
api = Api() $ print(api.auth_time)
! wc -l data/tweets.csv
<img src="/images/MongoDB1.png" alt="[img: MongoDB view]" title="MongoDB View" />
x_train_numerical = x_train[:, 0:2].toarray() # We only want the first two features which are the numerical ones. $ x_test_numerical = x_test[:, 0:2].toarray()
selected_leases = proxy.GetLeases( $     auth, $     {'>t_from' : ifrom, '<t_from' : iuntil} $ ) $ print(f"there have been {len(selected_leases)} reservations made during the period")
df_user['user.name'].value_counts()[:10]
sns.regplot(filtered_df['number_of_reviews'],filtered_df['availability_365'],ci=95)
trends = pd.read_csv('../datasets/sp500searches.csv', skiprows=1) $ print(trends.shape) $ trends.head()
print (new_df_left['rawcensustractandblock'].dtype) $
train_data, test_data = sales.random_split(.8, seed = 0)
np.random.seed(123456) $ dates = ['2014-08-01','2014-08-02'] $ ts = pd.Series(np.random.randn(2),dates) $ ts
autos["registration_year"].describe()
pd.read_csv("Data/microbiome_missing.csv").head(20)
df2['DepTime'].head()
import os $ ratings_file = os.path.join(ml_data_dir, 'ratings.csv') $ movies_file = os.path.join(ml_data_dir, 'movies.csv') $ ratings = pd.read_csv(ratings_file) $ movies = pd.read_csv(movies_file)
flights2.loc[[1950, 1951]]
joined['dcoilwtico']=joined['dcoilwtico'].astype(np.int8)
last_values = grouped_months.last() $ last_values=last_values.rename(columns = {'Totals':'last_v_T'}) $
df_interests = pd.read_csv('user_interests.csv') $ df_interests.head(3)
from scipy.stats import norm $ norm.cdf(z_score) $ norm.ppf(1-(0.05/2))
s2 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd']) $ s2
twitter.rating_denominator.value_counts()
inter_by_date = niners.groupby('Date')['InterceptionThrown'].sum() $ inter_by_date;
jobs_data4 = json_normalize(json_data4['page']) $ jobs_data4.head(5)
pd.value_counts(ac['Bank'].values, sort=True, ascending=False)
sub1.head()
support.amount.sum()
sorted(data.date.tolist())[-1], sorted(data.date.tolist())[-1][0:4]
p_new = df2['converted'].mean() $ print(p_new)
data['pickup_cluster_demand'].fillna(value=0, inplace=True) $ data['dropoff_cluster_demand'].fillna(value=0, inplace=True) $ data['ride_cluster_demand'].fillna(value=0, inplace=True) $ data.drop(['ceil_15min'], axis=1,inplace=True)
df = df.loc[(df['retweeted_status'].isnull()) & (df['lang'] == "en")]
first_result.find('strong').text
twitter_archive_master[(twitter_archive_master['rating_denominator'] == 50) & (twitter_archive_master['rating_numerator'] == 50)].iloc[0].name
autos["registration_year"].describe()
weekly = logins.groupby('weekday').count()/15
df.head(5)
data.loc[[pd.to_datetime("2016-12-01"), pd.to_datetime("2016-12-03")], ["TMAX", "TMIN"]]
data = np.zeros((2,), dtype=[('A', 'i4'),('B', 'f4'),('C', 'a10')]) $ data[:] = [(1,2.,'Hello'), (2,3.,"World")] $ pd.DataFrame(data, index = ['first','second'], columns=['C','B','A'])
exp_convert=df2.query('group=="treatment"').converted.mean() #Proportion of treatment group converted
new_page = df.query('landing_page == "new_page"')['user_id'].count() $ total = df.shape[0] $ new_page_prob = new_page / total $ new_page_prob 
px.isnull().sum()
S_lumpedTopmodel.decision_obj.groundwatr.options, S_lumpedTopmodel.decision_obj.groundwatr.value
session.query(Measurement.id, func.avg(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
df2 = df2[df2.duplicated(subset='user_id', keep='first') == False] $ df2.info()
df_columns.index.month.value_counts().sort_index() $
arr3d = np.random.random((10, 3, 5)).astype('float32') $ arr3d
n_new = df2[df2['landing_page']=='new_page'].count()[0] $ n_new
plt.hist(diffs);
pf_rdd = sc.parallelize([('RF1', 1.), ('RF2', 2.)]) $ dfpf = sqlContext.createDataFrame(pf_rdd, ['rf', 'qty'])
located_data.head(3)
test2 = df_main.tweet_id == 670319130621435904 $ df_main[test2]
np.shape(rhum_us_full)
import matplotlib.pyplot as plt $ % matplotlib inline $ plot=twitter_data.rating_numerator.hist(bins=50) $ plot.set_ylim([0,10])
model = DecisionTreeClassifier() $ X = data[train_cols] $ y = data['Churn'] $ model.fit(X, y)
df2['ab_page'] = pd.Series(np.zeros(len(df2)), index=df2.index); $ trt_ind = df2[df2['group']=='treatment'].index; $ df2.set_value(index=trt_ind, col='ab_page', value=1); $ df2['ab_page'] = df2['ab_page'].astype(int); $ df2.head()
from sklearn.ensemble import GradientBoostingClassifier $ gbdt_params = { $ } $ gbdt_pipe = make_pipeline(IncidentPreprocessor(), GradientBoostingClassifier()) $ gbdt_grid = GridSearchCV(gbdt_pipe, param_grid=gbdt_params, cv=3)
AAPL['close'].ewm(alpha=.9, adjust=False).mean().plot()
print('Bernoulli Naive Bayes Accuracy:', nltk.classify.accuracy(BNB, test_set) * 100, '%')
lasso.score(X_test,Y_test)
is_08A_manhattan = (restaurants["VIOLATION CODE"] == "08A") & (restaurants["BORO"] == "MANHATTAN") $ inspections08A_in_manhattan = restaurants[is_08A_manhattan] $ inspections08A_in_manhattan["DBA"].value_counts()[:10].plot(kind='bar')
gridCV.fit(X_train, y_train)
bc_props = [] $ for iper in range(m.nper): $     bc_props.append(["wel.flux",iper])
df.loc['2018-05-30'::2,['Open','Volume']]   # What is that we are trying to do here?
with open('potus_wiki_bios.json','r') as f: $     bios = json.load(f) $ print("There are {0} biographies of presidents.".format(len(bios)))
autos["price_dollars"].value_counts().sort_index(ascending=False).tail(200)
xmlData.drop('address', axis = 1, inplace = True)
autos= autos[autos["price"].between(1,351000)] $ autos["price"].describe()
cnf_matrix = confusion_matrix(y_test, yhat_tree, labels=['PAIDOFF','COLLECTION']) $ np.set_printoptions(precision=2) $ print (classification_report(y_test, yhat_tree)) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')
USvideos['like_ratio'] = USvideos['likes'] / (USvideos['likes'] + USvideos['dislikes']) $ USvideos['like_ratio'].describe() $
df.sort_index(axis = 1, ascending = False)
newdf.groupby([newdf.Hour_of_day]).Trip_distance.median()
knn= KNeighborsClassifier( n_neighbors=3,  $                            weights='uniform') $ scores = cross_val_score(knn,  X_test, y_test,  cv=5) $ np.mean(scores), np.std(scores)    # scoring on my Testing Data set the same so similar to the training data set 
n_net3.score(x_test,y_test)
sp500.ix[['MSFT', 'ZTS']]
AAPL.iloc[25:30,0:4].plot.bar()
frac = 0.9
tcga_target_gtex_expression_hugo[ $     [ $         "GTEX-146FH-1726-SM-5QGQ2", "GTEX-WZTO-2926-SM-3NM9I", "TCGA-AB-2965-03" $     ]].apply(np.exp2).apply(lambda x: x - 0.001).sum()
with open('nmf_doc_top_5_29.pkl', 'rb') as pikkle3: $     doc_top = pickle.load(pikkle3)
missing_tweets.code.value_counts()
go2genes = {} $ for row in df.itertuples(): $     go2genes[row[1]] = row[2].item()
inception_opt_wgt = [0.5, 0.5] $ inception_date = lst_date_to_set[0] $ capital = 8000
ax = sns.barplot(x= "countCollaborators", y = "countCollaborators", data = data_final, palette='rainbow', $                  estimator = lambda countCollaborators: len(countCollaborators) / len(data_final) * 100) $ ax.set(ylabel="% of authors with a given number of collab.") $ ax.set(xlabel="Number of collab. by author") $ ax.set(xlim=(0, 20))
under_thresh_dict = {} $ under_winnings_dict = {} $ for thresh in np.linspace(.5, .66, 16): $     under_thresh_dict[thresh] = sum([x[1] > thresh for x in pred_probas_under_fm]) $     under_winnings_dict[thresh] = np.mean(y_test_under[[x[1] > thresh for x in pred_probas_under_fm]]) $
score.head()
reviews.loc[reviews.price.isnull()]
df = pd.read_csv('trump_state_of_union_2018.csv.gz') $ df['parse'] = df['text'].apply(unescape).apply(nlp)
new_page_converted = np.random.binomial(n_new, p_new)
lsi.print_topics()
(data_2017_12_14[data_2017_12_14['text'].str.contains("instantaneamente", case = False)])["text"].head()
old_page_converted = np.random.binomial(145274, df2['converted'].mean())
fdonor.plot(kind='bar') $ plt.ylim((0,100000))
pd.unique(tweetsDF.time_zone)
commits.get_cardinality("hash").by_period().get_timeseries(dataframe=True)
ldamodel = models.ldamodel.LdaModel( $     terms_matrix, num_topics=10, id2word=dictionary, passes=5)
start = datetime.datetime(2010, 12, 31) $ end = datetime.datetime(2013, 12, 31) $ gspc = web.DataReader("^GSPC", 'yahoo', start, end)[['Adj Close']].pct_change() $ amzn = web.DataReader("AMZN", 'yahoo', start, end)[['Adj Close']].pct_change()
big_df_count.to_csv('polarity_results_LexiconBased/season/polarity_count_season.csv',index=None) $ big_df_avg.to_csv('polarity_results_LexiconBased/season/polarity_avg_season.csv',index=None)
merged2.shape
last_date=df1.iloc[-1].name $ last_date
glm_multi_v3.confusion_matrix(valid_f)
test_results_final = cvModel2.bestModel.transform(test_data)
df_TempIrregular.info()
P = type.__new__(LetsGetMeta,'S',(),{}) $ P.__class__.__class__
(autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
os.chdir(Base_Directory) $ os.chdir(today) $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $
numbers = {'integers': [1,2,3], 'floats': [1.1, 2.1, 3.1]} $ numbers_df = pd.DataFrame(numbers) $ numbers_df
import heapq $ import numpy $ a = numpy.array([1, 3, 2, 4, 5]) $ b = numpy.array([0, 3, 2, 4, 5])
P_new = df2.converted.mean() $ print("The convert rate for p-new under the null is {}.".format(P_new))
len(df.index)  # number of tweets with treatment
bus['postal_code_5'] = bus['postal_code'].str[:5] $ bus
yr15 = 271/243 $ total = 1759/2034 $ print('Ratios for: yr15 = ',yr15, '...and total = ', total)
enroute_4x_valid_responses = ['Valid Response'] $ enroute_4x_valid_count_prop_byuser = compute_valid_count_prop_byuser(enroute_4x, users_4x, 'vendorId', 'remappedResponses', $                                                                      enroute_4x_valid_responses) $ enroute_4x_valid_count_prop_byuser.head()
from sqlalchemy import func $ stmt1 = session.query(func.count('*')).select_from(employee).scalar() $ stmt2 = session.query(func.count(employee.c.id)).scalar() $ print(stmt1) $ print(stmt2)
autos["brand"].value_counts(normalize=True)
rodelar.gender()
data_year=data_nonan_temp.groupby(['Year']) $ data_year['Temperature(C)'].mean().plot()
Jarvis_ET_Combine = pd.concat([Jarvis_rootDistExp_1, Jarvis_rootDistExp_0_5, Jarvis_rootDistExp_0_25], axis=1) $ Jarvis_ET_Combine.columns = ['Jarvis(Root Exp = 1.0)', 'Jarvis(Root Exp = 0.5)', 'Jarvis(Root Exp = 0.25)']
(details.Genres.isnull()).value_counts()
food["created_date"]=food["created_datetime"].apply(date)
df[ df["systemmessage"] & df["text"].str.contains("Vous avez") ] $
df['body_tokens'] = df['body_tokens'].apply(nltk.word_tokenize) $ print(df['body_tokens'])
df_data.visa_class.value_counts()
weather = weather_soup.find('div', class_='js-tweet-text-container') $ mars_weather= weather.p.text.lstrip() $ print(mars_weather)
%run -i 'label_image/label_image.py' --graph='/tmp/output_graph.pb' --labels='/tmp/output_labels.txt' --input_layer='Mul' --output_layer='final_result' --input_mean=128 --input_std=128 --image='test/Colin_Powell.jpg'
df_ab.count()[0]
with open('start.pkl', 'rb') as f: $     start_df = pickle.load(f)
pd.isna(df).any()
dataA = pd.read_csv("data/dataPorUbicacion_Anios_tmax.csv", header=None)
np.exp(results.params) #np.exp is used to calculate the exponential of all elements in the input array, .params gives the linear coefficients that minimize the least value criterion
a.capitalize().lower()
twitter_archive_clean = twitter_archive.copy() $ image_predictions_clean = image_predictions.copy() $ tweet_json_clean = tweet_json[['id','retweet_count','favorite_count']].copy()
average = labmt.happiness_average.mean() $ happiness = (labmt.happiness_average - average).to_dict()
dfagency = pd.get_dummies(cp311['agency'])
autos.ad_created_year.value_counts(normalize=True)
db = client.test_database $ collection = db.test_collection $ collection
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['personalize'])
inter = mr * vol $ inter = inter.between_time('9:30', '16:00') $ lagged_inter = inter.tshift(1, 'min').between_time('9:30', '16:00') $ pd.ols(y=mr, x=lagged_inter)
w2 = Window.partitionBy('user_id') $ df_selected = df_city_reviews\ $ .withColumn('numOfReviews', F.count("seq").over(w2))\
ss_sparse = (~df_EMR_dd_dummies.isnull()).sum() < 3 $ ls_sparse_cols = ss_sparse[ss_sparse].index.tolist()
df_test[0:5].T
data = pd.DataFrame({"time": pd.date_range("2018-01-01", periods=120, freq="2H"), $                      "value": np.random.random(120),                            $                      "category": pd.Categorical(list("abcdef" * 20))}) $ data.head()
Stations = Base.classes.stations $ Measurements = Base.classes.measurements
df_playlist_videos.head(2)
type(ffr.index)
import urllib.request $ import json $
linear_svc = LinearSVC() $ linear_svc.fit(X_train, Y_train) $ Y_pred = linear_svc.predict(X_test) $ acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2) $ acc_linear_svc
(df_new.ab_page == df_new.new_page).mean()
np.exp(0.0506), np.exp(0.0408)
w,h = 16,16 $ I = np.random.randint(0,2,(h,w,3)).astype(np.ubyte) $ F = I[...,0]*(256*256) + I[...,1]*256 +I[...,2] $ n = len(np.unique(F)) $ print(n)
columns = inspector.get_columns('measurements') $ for c in columns: $     print(c['name'], c["type"]) $
unique_urls.sort_values('total_payout', ascending=False)[0:50][['url', 'total_payout']]
df.rename(columns={"PUBLISH STATES":"Publication Status","WHO region":"WHO Region"},inplace=True) $ df.head(2)
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit_mod.fit()
movies.head()
target = np.array(fraud_data_updated['class']) $ features = np.array(fraud_data_updated.drop(['class'],axis=1))
tweet_archive_clean.name = tweet_archive_clean.name.where(~tweet_archive_clean.name.isin(list_of_exclusion_words))
%matplotlib inline $ from matplotlib import pyplot as plt $ import numpy $ x=numpy.linspace(0,1,1000)**1.5
archive_copy['name'].loc[archive_copy['name'] == 'None'] 
from google.cloud import bigtable $ client     = bigtable.Client.from_service_account_json(JSON_SERVICE_KEY,project=project_id, admin=True) $ instance   = client.instance(instance_id) $
engine = engine.connect() $ engine
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyser = SentimentIntensityAnalyzer()
index_group1 = user_group['Group'].apply(checkGroup, number=1) $ user_group1 = user_group[index_group1] $ print(len(user_group1)) $ user_group1.head()
rf = rf_grid.best_estimator_ $ rf.fit(X_train,y_train) $ rf_pred = rf.predict(X_test)
model.wv.similarity('model', 'system')
grp_tweet["content"].count()
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
final.index = final.Date
print(type(data.points)) $ data[['points']] $
df = df[df.Tract.notnull()].copy()
grouped1.sort_values(ascending=False)
import collections $ nardil = df_fda_drugs_reported[df_fda_drugs_reported['simple_name'].str.lower() == 'phenelzine'] $ a = [y for x in nardil['adverse_effects'].str.split("; ") for y in x] $ counter=collections.Counter(a) $ [(eff, p/len(counter)) for (eff, p) in counter.most_common(20)]
port_val=pos_vals.sum(axis=1) $ port_val.head()
ppm_body = preProcessor_in_memory(hueristic_pct=.61, keep_n=6000, maxlen=60) $ vectorized_body = ppm_body.fit_transform(data_to_clean_body)
data = data.loc[(data.lat<-34) & (data.lat>-35) & (data.lon>-60) & (data.lon<-57.5)]
autos["registration_year"].describe()
df = df.swaplevel('Description', 'UPC EAN') $ df.head(3)
df =pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv') $ df.head(2)
cfs=cfs.cluster_features()
dfp = pd.DataFrame(np.random.randn(600,1), columns=['A'], index=pd.period_range('2013-01-01 9:00', periods=600, freq='T'))
doc = r.text
df_new.shape $ df_new.sample(3)
df_user = df_user.set_index('user_id') $ df_churn = np.floor((pd.to_datetime(df_user['last_seen'])  - pd.to_datetime(df_user['registration_date'])).dt.total_seconds()/(3600*24*30)) $ df_churn = df_churn.rename('churn_leg') $ df_churn.head()
mr = vwap / bars.open - 1 $ mr = mr.between_time('9:30', '16:00') $ lagged = mr.tshift(1, 'min').between_time('9:30', '16:00') $ pd.ols(y=mr, x=lagged)
fh_4 = FeatureHasher(num_features=uniques.iloc[6, 1], input_type='string', non_negative=True) $ %time fit4 = fh_4.fit_transform(train.C14.map(lambda x: str(x))) $ %time fit4_test = fh_4.transform(test.C14.map(lambda x: str(x))) $ print((fit4.shape, fit4_test.shape)) $ print(fit4.nnz)
wash_matrix_df = pd.DataFrame(wash_park_matrix.toarray(), $                                        index=important_tweets.index, $                                        columns=count_vectorizer.get_feature_names())
poly20 = PolynomialFeatures(degree=20) $ X_20 = poly20.fit_transform(X)
rng.to_pydatetime()
relevant_data['User Name'].value_counts().plot(kind='barh')
sp.str.get(-1) 
url= 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json' \ $      '?start_date=2017-01-01&end_date=2017-12-31&api_key={}' \ $         .format(API_KEY) $ r = requests.get(url)
df_new['intercept'] = 1 $ models=sm.Logit(df_new['converted'],df_new[['US','UK']]) $ results=models.fit() $ results.summary()
appleInitialNegs = neg_tweets[neg_tweets.author_id_y == 'AppleSupport']
p = model.predict(X_kaggle, num_iteration=model.best_iteration)
df.to_pickle('Data/NLP.pkl')
print('The probability of conversion:', df2['converted'].mean())
df3.describe()
df = df.drop(7)
session.query(Measurement.station, func.count(Measurement.station).label('count')).\ $                                   group_by(Measurement.station).\ $                                   order_by('count DESC').all()
dataset['text length'] = dataset['text'].apply(len)
image_predictions = pd.read_csv('image_predictions.tsv',sep='\t')
to_be_predicted_Day1 = 22.17 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
date.strftime('%A')
git_log['author'].value_counts().size
api.data
graffiti = graffiti.replace(np.nan, 0)
weather['ice_pellets'] = weather['ice_pellets'].astype('int') $ weather['hail'] = weather['hail'].astype('int')
n_users = df.user_id.nunique() $ n_users
train_set.columns = ['tweet_id', 'tweetText', 'polarity_value', 'polarity_type', 'topic','set']
year4 = driver.find_elements_by_class_name('yr-button')[3] $ year4.click()
dul_final.to_csv(folder + "\\" + "Duluth-all.txt", sep="\t", index = False)
pk_planes.count()
total=df_data.merge(final, on='chips', how='left') $ total=total[['chips','time','ra','dec','redshift','min_air','best_time','priority']]
Amazon = Amazon.reset_index() $ print('The maximum Close price was:',format(Amazon.loc[Amazon['Close'].argmax(),['Date','Close']][1]),'\n','On:',format(Amazon.loc[Amazon['Close'].argmax(),['Date','Close']][0]))
condos.shape
df_change_count[df_change_count['Count'] == 1].count()
poverty_data.head()
from nltk.tokenize import wordpunct_tokenize, TweetTokenizer, RegexpTokenizer
mentions_df = pd.read_csv("/mnt/idms/fberes/network/ausopen18/data/ao18_mentions_with_names.csv",sep="|") $ mentions_df.head()
search = api.GetSearch("bitcoin") # Replace happy with your search $ for tweet in search: $     print(tweet.id, tweet.text)
newdf.head(12)
analyze_set.describe()
strfilter1 = "CAPITAL|ASSET MANAGEMENT" $ strfilter2 = "CAPITAL|ASSET MANAGEMENT|ADVISORS|ADVISERS|HOLDINGS|FINANCIAL|HEDGE|SECURITIES" $ hedgeprop1 = bigdata[bigdata['Company Name'].str.upper().str.contains(strfilter2)].sort_values('CIK') $ hedgeprop1
pd.set_option("display.max_columns", 65) $ df
df_combined.head()
facts_metrics.groupby('dimensions_item_id').sum()
train = pd.merge(train_data, date_info, how='left', on=['visit_date']) $ test = pd.merge(test_data, date_info, how='left', on=['visit_date']) 
cust_demo.columns
df.instagrams
data.plot() $ plt.ylabel('Hourly Bicycle Count');
tags = pd.read_csv('tag_categories.csv')
df.loc[df.userTimezone == 'Mountain Time (US & Canada)', 'tweetText'] $
res = es.search(index="logs*", body={"query": {"match_all": {}}}) $ print("Got %d Hits:" % res['hits']['total']) $ for hit in res['hits']['hits']: $     print("Timestamp: {}, \tmetric: {}, \tresult: {}\n".format(hit["_source"]["phenomenonTime"], $             hit["_source"]["Datastream"]["name"], hit["_source"]["result"]))
df.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep=',', encoding='utf-8', header=True)
c.execute('SELECT * FROM iris')
df_merge.info()
tbl2[['250beta','250alpha']]['2012':].plot()
for i in levelmean.columns[1:]: $     tokendata[i+"_byLevel"] = round(100*tokendata[i] / levelmean[i],2)
test_dum_clean.head()
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "fantastic ms."]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "fantastic ms."]["tweet_vader_score"],fit_reg=False,marker = "o",scatter_kws={"color":"teal","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (fantastic ms.)",fontsize = 12)
df_arch_clean[df_arch_clean['retweeted_status_timestamp'].notnull()] $
print pd.concat([s1, s2, s3], axis=1, keys=['S1', 'S2', 'S3'])
sizes = tdf['size'].as_matrix() $ print(f'Number of instances = {sizes.shape[0]}') $ print(f'Mean (population) = {np.mean(sizes):5.3f}') $ print(f'Standard deviation (population) = {np.std(sizes):5.3f}')
df = df.groupby(('C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME')).sum().reset_index() $ df
test.describe()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df_pr = pd.DataFrame([r for r in results if r is not None])
df_geo.sample(10)
datatest.loc[datatest.place_name == "Terralagos",'lat'] = -34.907001 $ datatest.loc[datatest.place_name == "Terralagos",'lon'] = -58.514885
search_df = pd.DataFrame(data['data'])
sentences = [['first', 'sentence'], ['second', 'sentence']] $ model = gensim.models.Word2Vec(sentences, min_count=1)
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(kind = 'bar', y="Borough")
retweets = twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()==False] $ twitter_archive_clean.drop(retweets.index, inplace=True) $ twitter_archive_clean.reset_index(drop=True,inplace=True)
tweet.user.location $ tweet.created_at $ tweet.user.name $ tweet.user.screen_name
df.info()
save_dat.to_csv(save_pth, float_format='%g', date_format='%Y/%m/%d') #write select data
import os $ print("Running all tests...") $ _ = [ok.grade(q[:-3]) for q in os.listdir("ok_tests") if q.startswith('q')]
df_cos = np.genfromtxt('out.dat')
len(vc.index.tolist())
df = web.DataReader('TSLA', 'google', datetime(2016, 1, 1)) $ df
stanford_word_list.head()
x_min_max = pd.DataFrame({'x': [df['x'].min(), df['x'].max()]}) $ x_min_max
week33 = week32.rename(columns={231:'231'}) $ stocks = stocks.rename(columns={'Week 32':'Week 33','224':'231'}) $ week33 = pd.merge(stocks,week33,on=['231','Tickers']) $ week33.drop_duplicates(subset='Link',inplace=True)
f_close_clicks_device_train = spark.read.csv(os.path.join(mungepath, "f_close_clicks_device_train"), header=True) $ f_close_clicks_device_test = spark.read.csv(os.path.join(mungepath, "f_close_clicks_device_test"), header=True) $ print('Found %d observations in train.' %f_close_clicks_device_train.count()) $ print('Found %d observations in test.' %f_close_clicks_device_test.count())
pattern = re.compile('AA') $ print([x for x in pattern.finditer('AAbcAA')]) $ print([x for x in pattern.finditer('bcAA')])
x2 = test_number_con.added $ y2 = test_number_con.booking_rate
df_new[['CA','UK', 'US']]= pd.get_dummies(df_new['country']) $ df_new.head()
(~autos["price"].between(200,155000)).sum() / autos.shape[0] #In this code ~ invert operator selects $
autos["price"].head() #head() displays first few rows, probably first 5
data.info()
impressions_by_algRef = pd.DataFrame(impressions.groupby(['algRef'])['session'].count()) $ impressions_by_algRef.columns.values[0] = 'impressions' $ impressions_by_algRef = impressions_by_algRef.sort_values(by='impressions', ascending=False) $ impressions_by_algRef
RandomTwoDF = RowedRDD.toDF() $ RandomTwoDF.registerTempTable("RandomTwo") $ print type(RandomTwoDF)
pd.to_datetime(['2009/07/31', 'asd'], errors='ignore')
y_test_under[rfc_bet_under].sum()
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','Status']).closed_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False) $
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_autocorrelation(RN_PA_duration.diff()[1:], params=params, lags=30, alpha=0.05, \ $     title='Weekly RN/PA Hours First Difference Autocorrelation')
cercanasAfuerteApacheEntre100Y125mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 100) & (cercanasAfuerteApache['surface_total_in_m2'] < 125)] $ cercanasAfuerteApacheEntre100Y125mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre100Y125mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre100Y125mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
linkNYC = pd.read_csv('linkNYClocations.csv') $ linkNYC = linkNYC.iloc[:,1:]
weather.dtypes
import crowdtruth $ from crowdtruth.configuration import DefaultConfig
autos.loc[autos["registration_year"]>2018, "registration_year" ].shape
queryfile = 'EXAMPLE.sql' $ numegs = 200 $ initialdays = 1 $ uniqueid = 'id'
yah = ha.accounts.manual_current('Yahoo', path=os.path.join('data', 'manual_accounts'), $                                  currency='YHOO')
co = CombinatorialOptimisation() $ co.train(mains,cols=[('power','active')]) $
table.info()
log_mod=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results=log_mod.fit()
to_seconds(between_my_posts).plot( $     kind='hist', $     range=(0, 100_000) $ )
with SAS7BDAT('./data/in/adtteos.sas7bdat') as file: $     df_dm = file.to_data_frame() $ df_dm.head()
GPU_CORE = [0] $ BATCH_SIZE = 512 $ BEGINING_LR = 0.01 $ model_name = 'supervisord_model' $ data_dir = '../data/imsa-cbf/'
sampled_authors = active_distinct_authors_latest_commit.sampleBy( $     "project_name", $     fractions=dict(map(lambda r: (r[0], min(1.0, r[1])), local_sample_fractions)), $     seed=42)
df2['intercept'] = 1 $ df2[['control','ab_page']]= pd.get_dummies(df2['group'])
pd.DataFrame(tweetsDf.columns)
geo_irregularities.to_file("geo_irregularities_20180601_to_20180607.geojson", driver='GeoJSON')
model.most_similar('man')
fit = log_model.fit()
df['profit']=(df.state_retail-df.state_cost)*df.bottles_sold $ df['ppb']=(df.state_retail-df.state_cost) $ df['profit_margin']=(df.ppb/df.state_retail)*100
pickle.dump(cv_fitted, open('iteration1_files/epoch3/cv_fitted.pkl', 'wb'))
gdf = gdf[gdf['seqid'].isin(chromosomes_list)] $ gdf.drop(['start', 'end', 'score', 'strand', 'phase', 'attributes'], axis=1, inplace=True) $ gdf.sort_values('length').iloc[::-1]
twitter_archive_clean.head(1)
X = data3.drop('sales', axis = 1)
df_B3=pd.read_csv("classB.csv",header=None) $ df_B3.shape
modern_balk_king.head(15)
df_ad_airings_5['location'][0].split(",")
tlen = pd.Series(data=data['len'].values, index=data['Date'])
df_data_1.describe(include = 'all')
invoice = invoice.str.replace("$","").astype(float) $ invoice.sum()
df = pd.read_csv('data/btc-market-price.csv')
tweet_json_clean.head()
df_weather.to_csv("../data/weather_fixed.csv", index=False)
pdp.corr()
p_diffs = np.array(p_diffs) $ (p_diffs > actual_diff).mean() $
sh50_df.to_sql(con=conn_helloDB, name='sh50', if_exists='replace', index=False)
pgn2value = dict(pd.read_csv('../data/resultlist.csv').values[:,1:])
not_in_dsi = dsi_me_1_df.merge(data_science_immersive_df, how='left', on='name')
df.T
labels = ['AG_0_30','AG_30_50','AG_50_70','AG_70_UP'] $ df_CLEAN1A['AGE_groups'] = pd.cut(df_CLEAN1A['AGE'], bins=[0, 30, 50, 70, 100], labels=labels) $
rt_count.head(20).to_csv('retweets.csv')
stn_tempobs = session.query(Measurement.station, Measurement.tobs).\ $     filter(Measurement.station == busiest_stn).\ $     filter(Measurement.date >= data_oneyear).all() $ stn_tempobs
free_data.iloc[:5]
print('The Best parameter set: %s' % gs_lr_tfidf.best_params_) $
official_data = pd.DataFrame.from_csv("data/merged-census.csv", index_col=None) $ official_data['Census Tract'] = official_data["Census Tract"].apply(fix_tract) $ official_data = official_data.set_index("Census Tract") $ official_data
r6s = r6s[['created_utc', 'num_comments', 'score', 'title', 'selftext']] $ r6s['created'] = pd.to_datetime(r6s['created_utc'],unit='s') $ r6s = r6s[r6s['created']>datetime.date(2017,12,1)]
summed = a + b $ summed.iloc[4, 0] = np.nan $ summed
df_new = df2.merge(df3, on='user_id', how='left') $ df_new.head()
tub=tub.set_index(['Date','Type','Scoop','Qty','Scoops/Tub','Proportion']).stack().reset_index() $ tub
year_string = 'extracted_data/*.csv'
goodreads_users_df.replace('None', np.nan, inplace=True)
df['Complaint Type'][df['Agency'] == 'DOT'].value_counts().head()
(df_new.style $     .set_table_styles(styles))
df3.plot(x=0,y=4) $ plt.show()
lims_query = "SELECT ephys_roi_results.id, specimens.id AS cell_id, specimens.name, specimens.ephys_roi_result_id \ $ FROM ephys_roi_results JOIN specimens ON specimens.ephys_roi_result_id = ephys_roi_results.id" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
print(data["Time Stamp"][0].day) $ print(data["Time Stamp"][0].month) $ print(data["Time Stamp"][0].year)
loc = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load('treas_parking_meters_loc_datasd.csv') $ loc.show(3)
events_popularity_summary.fillna(0)
df.to_csv('Tweets_sports.csv')
new_pred = pred1.join(pred2 , on='id', rsuffix='_2').join(pred3 , on='id', rsuffix='_3') $ new_pred['pred']=new_pred[['any_spot','any_spot_2','any_spot_3']].mean(axis=1).astype(int) $ new_pred = new_pred.drop(['any_spot','any_spot_3'], axis=1).rename(columns={'pred': 'any_spot'})
tmp_df.reset_index().to_file('geocoded_evictions_deidentified.shp')
learner.sched.plot()
re_json.get('statuses', [{}])[0] #advanced
predictions = model_rf.transform(test_data) $ evaluatorRF = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy") $ accuracy = evaluatorRF.evaluate(predictions) $ print("Accuracy = %g" % accuracy)
tweets_per_day_df.plot.line(x=tweets_per_day_df['date'],y='number_of_tweets',figsize=(15,8),lw=1)
active_ix = [a for a in USER_PLANS_df.index if a not in churned_ix]
[(s.id, s.user.name, s.user.screen_name) for s in retweets]
test.head()
data.describe(include = 'all')
mw.to_csv('mw.csv',sep='~',index=False)
data.plot(x='Salinity(psu)', y='Temperature(C)',kind='scatter')
for i in cpi_all.Measure.cat.categories.tolist(): $     print i
weather_features = weather_features.resample('D').sum()
weathertweet = soup.find('div', class_="js-tweet-test-container") $ first_para = soup.find('p', class_="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text") $ weathertext = soup.find('p', class_="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text").text $ print("mars_weather = " + weathertext)
ab_df.shape[0]
print(convo1.text)
writer = pd.ExcelWriter("../visualizations/uber_day_of_week.xlsx")
re_split_raw = re.findall(r'\w+|\S\w*', raw) $ print(re_split_raw[100:150])
tweet_df = df_tweet.filter(['id_str','text','created_at','in_reply_to_status_id_str','in_reply_to_user_id_str','in_reply_to_screen_name','lang'],axis=1)
duration_df.info()
top_songs['Position'].dtype
df_new['current_date'] = '2018-06-24' $ df_new['current_date']= df_new['current_date'].apply(pd.to_datetime) $ df_new['current_date']
print('x, y, z coords:', keto_coord.values[0]) $ distances = pmol.distance(keto_coord.values[0])
mb_file = pd.ExcelFile('../data/microbiome/MID1.xls') $ mb_file
s.asfreq('3B', method='bfill').head(15)
%%bash $ cd data $ ls model*.index
from sklearn import svm $ clf = svm.LinearSVC() $ clf.fit(X_train, Y_train) $ print (clf) $ print(clf.score(X_test, Y_test))
image_preview = pd.read_csv('image_preview.tsv', sep='\t')
groupby_month = df_master.groupby(['year','month'],sort=False)['tweet_id'].count().plot(kind='bar',figsize=(10, 5),fontsize=12) $ plt.title('Number of original tweets per month',fontsize=15) $ plt.ylabel('Count') $ plt.xlabel('') $ plt.show() $
df_members['gender'] = df_members['gender'].fillna(0) #Filling the null values in the column
cpq_status = cpq_business.loc[cpq_business['On Zayo Network Status'] == 'Not on Zayo Network']
top20_merged = pd.merge(top20_mostfav, top20_mosttweeted, on='prediction_1') $ top20_merged = top20_merged.sort_values('tweet_count', ascending = False).copy() $ display(top20_merged) $ top20_merged.to_csv('top20_merged.csv')
pd.value_counts(appointments['Provider']), len(pd.value_counts(appointments['Provider']))
from sklearn.preprocessing import LabelEncoder, OneHotEncoder $ from collections import defaultdict
twitter_df = twitter_df.groupby('Date').mean() $ twitter_df.head()
df.loc[df.toes.str.match(pattern3)==True]
print(tweet_archive[tweet_archive.tweet_id.isnull()]) $ print(tweet_archive[tweet_archive.tweet_id.duplicated()])
results = pd.read_json('/Users/thomasmulhern/Downloads/pitchesDataJson/result.json')
building_pa.to_csv("buildding_00.csv",index=False)
shmuel = relevant_data[relevant_data['User Name'] == 'Shmuel Naaman'] $ df = shmuel['Event Type Name'].value_counts() $ df_shmuel = pd.Series.to_frame(df) $ df_shmuel.columns = ['Count_Shmuel'] $ df_shmuel
stock_return.plot(grid = True).axhline(y = 1, color = "black", lw = 2) $
import kipoi $ models_df = kipoi.list_models() $ models_substr = ["HAL", "MaxEntScan", "labranchor", "rbp"] $ models_df_subsets = {ms: models_df.loc[models_df["model"].str.contains(ms)] for ms in models_substr}
data.groupby(['Date received', 'State'])['Company'].agg([ 'count']).pivot_table('count', index = 'Date received', columns='State', fill_value=0).sum().sort_values(ascending=False)
df.plot("genre", "avg_rating", "bar", title = "Barplot of avg rating by genre")
draw_tree(m.estimators_[0], raw_train, precision=3)
print(metrics.classification_report(testy, ada_predict)) $
height.isna()
df.loc['a':'d']
lda = LatentDirichletAllocation(n_topics=20, max_iter=5, random_state=1) $ lda.fit(X)
df.to_pickle('salary_df.pkl')
1/np.exp(-0.0408) $
print df.shape $ df.index
click_condition_meta.dvce_type.unique()
df_ad_airings_5.info()
train[['ld','md','mc','jd','vd','sd','dc']] = pd.get_dummies(train['DayOfWeek']) $ test[['ld','md','mc','jd','vd','sd','dc']] = pd.get_dummies(test['DayOfWeek'])
c = MongoClient().sb1.music
print(est.summary())
df_master = pd.read_csv("../01_data preprocessing/data new/masterdata.csv",encoding="utf-8",sep=",", $                         parse_dates=["ValidityDate"],usecols=[0,1,11],dtype={"CardID":object}) $ df_master1 = df_master.sort_values(["CustID","ValidityDate"]).loc[~df_master.duplicated(subset=["CustID"],keep="first")] $ df_master1 = df_master1.loc[:,["CustID","ValidityDate"]]
price_cleaned = ( autos["price"].str.replace("$","") $                                 .str.replace(",","") $                                 .astype(float)) $ autos["price"]=price_cleaned $ autos["price"].value_counts()
df.loc[0:4:2] $
df.head()
joined.head()
long_list = list(top_bike['Long']) $ last_elem = long_list[-1] $ long_list.append(last_elem) $ long_list.pop(0) $ top_bike['newLong'] = long_list
frame = pd.DataFrame(np.arange(12).reshape(( 4, 3)), $                   index =[['a', 'a', 'b', 'b'], [1, 2, 1, 2]], $                   columns =[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']]) $ frame
df_daily4=df_daily.groupby(["C/A", "UNIT", "STATION"]).DAILY_ENTRIES.sum().reset_index() $ df_daily4.head(5) $
documents = [x.split(' ') for x in documents]
newdf.columns
html_table = df.to_html() $ html_table
real_old = len(df2.query('group == "control" & converted == 1'))/len(df2.query('group == "control"')) $ print(real_old)
weather.drop(['STATION', 'NAME'], axis=1, inplace=True)
y_train.shape
df['statement_type'].value_counts()
tt_final.to_csv('twitter_archive_master.csv', index=False)
df.loc[:,"Date"] = 
n_periods = 1 $ df_survival.loc[:, 'Churn Date'] = pd.to_datetime(obs_end_date)-df_survival.loc[:, 'Frequency of Donations']*n_periods
month_year_crimes.plot()
to_be_predicted_Day1 = 43.22 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
gbm_best = gbm.set_params(**rs_gbm.best_params_).fit(X_train, y_train) $ gbm_best_pred = gbm_best.predict(X_test) $ print classification_report(gbm_best_pred, y_test) $ print 'Accuracy: ', accuracy_score(gbm_best_pred, y_test) $ print 'ROC AUC: ', roc_auc_score(gbm_best_pred, y_test)
prophet_model.fit(prophet_df)
voters = pd.read_csv('data_raw_NOGIT/voters_district3.txt', sep='\t') $ households = pd.read_csv('data_raw_NOGIT/households_district3.txt', sep='\t') $ households_with_count = pd.read_csv('data_raw_NOGIT/AK_hhld_withVoterCounts.txt', sep='\t')
daily_window = Window.partitionBy('day').orderBy(functions.desc('count'))
extract_deduped.loc[(e)]
df_max.columns = [dict_names[x] for x in df_max.columns]
session.query(Measurement.station, func.count(Measurement.station)).\ $     group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
uber_14["month"].value_counts()
random_sample = trainset.sample(5000)
p_treatment = df2.query('group == "treatment"').converted.mean() $ p_treatment
by_day_by_hour15.to_json("../visualizations/by_day_by_hour15.json", orient="split")
def sunday(s): $     return s.split("-")[1] $ ccl["Date"] = ccl["Date"].apply(sunday)
trainData.groupBy("label").count().show()
%sql \ $ SELECT avg(retweet_count + twitter.like_count) AS heat \ $ FROM twitter \ $ WHERE user_id = 42869268;
df_predictions.to_csv("~/GermanElections.csv", index=False)
col.index
from sklearn.metrics import classification_report,confusion_matrix, f1_score $ print(confusion_matrix(y_test, dummy_predict)) $ print(classification_report(y_test, dummy_predict)) $ f1=[f1_score(y_test, dummy_predict, average=avg) for avg in ['micro', 'macro', 'weighted']] $ print(f1)
data = fat.add_ema_columns(data, 'Close', [3, 6, 12, 26, 50])
print('Scores') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.score),np.median(df.score))) $ print('Number of coments') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.comms_num),np.median(df.comms_num))) $
def calc_polarity(i): $     tweet = TextBlob(dataset['Text'][i]) $     return tweet.polarity $ dataset['Sent_Polarity'] = [calc_polarity(i) for i in range(len(dataset))]
autos['last_seen'].str[:10].value_counts(normalize = True, dropna = False).sort_index()
Numerator=df2.query("landing_page=='new_page'").shape[0] $ print("The probality that he recieves new page equals",Numerator/df2.shape[0])
dicttagger_price = DictionaryTagger(['price.yml'])
df.rename(columns={'PUBLISH STATES': 'Publication Status', 'WHO region': 'WHO Region' }, inplace=True) $ df.head(2)
(etsamples_engbert,etmsgs_engbert,etevents_engbert) = be_load.load_data(algorithm='') $ raw_freeview_df, raw_fix_count_df  = condition_df.get_condition_df(data=(etsamples_engbert,etmsgs_engbert,etevents_engbert),condition='FREEVIEW')
d=df2.query('group=="treatment"').converted.mean()-df2.query('group=="control"').converted.mean()
from scipy.stats import norm $ print(norm.ppf(1-0.05))
df_transactions['amount_per_day'] = df_transactions['amount_per_day'].fillna(0)
df.rename(columns={'CMPLNT_FR_DATE_YEAR':'YEAR', $  'CMPLNT_FR_DATE_MONTH':'MONTH', $  'LAW_CAT_CD':'CRIME_CAT', $  'OFNS_DESC':'OFFENSE', $  'CMPLNT_NUM_count':'COUNT'}, inplace = True)
len(df[df.location_id != df.prev_location_id])
rmse_scores = np.sqrt(mse_scores) $ print(rmse_scores)
word = 'convolution' $ fig = w2v.create_2d_tsne_plot(word, number_closest_words=25)
archive_clean['rating'] = archive_clean.apply(get_rating, axis=1)
yelp_dataframe.drop('index', axis=1, inplace=True)
raw_sample_sizes = num_authors_by_project.withColumn( $     "sample_size_1", $     compute_num_required_sample_1("author_count")).persist()
print(train_df['Tag'].value_counts()) $ print(train_df['Tag'].value_counts(normalize=True))
x1 = poly1.fit_transform(x)
census_finaldata = census[['GEOID_tract','geometry']].merge(threeoneone_census_complaints.drop('geometry',1),how='outer',left_on='GEOID_tract',right_on = 'GEOID_tract') $ census_finaldata = gpd.GeoDataFrame(census_finaldata.drop('geometry',1),geometry=census_finaldata['geometry'],crs=census.crs)
ratings = (spark.read.format("csv") $ .options(header = True, inferSchema = True) $ .load(home_dir + "ratings.csv") $ .persist())
libraries_df.head()
first_result.contents
flight.write.parquet(pq_file_name)
prices = prices.ffill(limit=3) $ prices = prices.bfill(limit=3)
df_protest.info()
sns.barplot(x=top_sub['id'], y=top_sub.index) # challenge: annotate values in the plot $ plt.xlabel("number of posts") $ plt.title("Top 5 active subreddits by # of posts");
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d') $ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
print('The API status code is {}'.format(ng_stor.response)) $ print('The URL sent to the API is {}'.format(ng_stor.response.url)) $ print('The API header is {}'.format(ng_stor.response.headers))
plots.top_n_port_plots(10,traffic_type = 1) #Normal traffic top 10 ports
X=dataset.iloc[:,1].values
full_data.dtypes
country_dummies = pd.get_dummies(df3['country']) $ df3_new = df3.join(country_dummies) $ df3_new['intercept'] = 1 $ df3_new.head()
df_twitter_archive_copy.name.value_counts()
from carto.datasets import DatasetManager $ dataset_manager = DatasetManager(auth_client) $ datasets = dataset_manager.all()
conditions_clean = conditions_m.str.lower().str.strip() $ conditions_clean
sns.boxplot(data.antiguedad.values) $ plt.show()
tweet_df['place'].unique()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"')['converted'].sum() $ convert_new = df2.query('landing_page == "new_page"')['converted'].sum() $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn as sns; sns.set() $ import numpy as np
df_afc_champ2018.plot(x='playnumber', y='awayWinPercentage', figsize=(12,5))
y_final = trainDF['ARR_DELAY'].values
df_control = df2.query('group=="control"') $ y_control = df_control["user_id"].count() $
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Plot of 10K simulated p_diffs'); $
tweets_kyoto_filter = tweets_kyoto[tweets_kyoto['ex_lat']<35.031461] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_lat']>34.931461] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_long']<135.785300] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_long']>135.685300] $
autos.loc[autos["registration_month"] == 0, :].describe(include='all')
print(store['prealn/queue'].shape[0]) $ add_table(store, 'prealn/queue', data=problems) $ print(store['prealn/queue'].shape[0])
for school in bigten: $     c = tp.Cursor(api.search, q=school, lang="en") $     for status in c.items(100): $         tempDict = {"School":school, "Text": status.text, "Location":status.geo, "Time":status.created_at} $         df = df.append(tempDict, ignore_index=True) $
stop_words = nltk.corpus.stopwords.words('portuguese')
for search_result in search: $     print(search_result.text) $     print(search_result.created_at) $     print(search_result.user.location)
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir(Base_Directory) $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir(today) $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)
austin.isnull().sum() $
new_page_converted = np.random.choice([0,1], n_new, p=[1-p_new, p_new]) $ new_page_converted
print("There are {} in this collection of Tea Party tweets".format(len(df)))
for topic_idx, topic in enumerate(lda.components_): $     print "Topic %d:" % (topic_idx) $     print " ".join([tf_feats[i] $                     for i in topic.argsort()[:-10 - 1:-1]])
all_sets = pd.read_json("AllSets.json", orient = "index") $ all_sets.head()
p_old = df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique() $ p_old
multiple_party_votes_all.drop(columns = ['index', 'votes_hour', 'time', 'district', 'date'], inplace = True)
df2.drop(df2.index[2893], inplace=True) $ df2.head()
freqs = (autos['brand'] $  .value_counts(normalize=True)) $ freqs
s.str[1]  # return char-1 (second char) of every item
from scipy.stats import norm $ print(norm.cdf(z_score)) #Z-score significance $ print(norm.ppf(1-(0.05))) # It tells us what our critical value at 95% confidence is $
'xx'.split('x')
from sklearn import metrics $ print("Accuracy: %.3f" % # TODO $          )
engine.execute('SELECT * FROM measurements limit 10').fetchall()
mask = (nullCity["creationDate"] > '2016-01-01') & (nullCity["creationDate"]<= '2016-12-31') $ nullCity2016 = (nullCity.loc[mask]) $ nullCity2016.head()
df_clean3.rating_denominator.value_counts()
result_control_1.summary()
useless_variables = [ 'CONTRACT_NUMBER', 'CONTRACT_MODIFICATEUR', 'CRE_DATE', $        'CONDITION_REGLEMENT', 'MOTIF_RESILIATION', 'RENOUVELLEMENT_AGENCE', $        'PRIX_FORMULE', 'PRIX_OPTION', 'NUM_CAMPAGNE', 'DATE_RESILIATION'] $ contract_history.drop(useless_variables, axis=1, inplace=True)
print(df2[df2['group']=='control']['converted'].mean())
non_null_counts = df.count() $ non_null_counts[non_null_counts < 1000]
df.info()
first_result.contents
response.text
reg.fit(X_.astype(float), y_.astype(float))
sel_df = sel_df.dropna(subset=['BlockRange'])
gcv.best_estimator_.fit(fb_train.message, fb_train.popular)
df_date[michaelkorsseries].groupby("date").sum().sort_values(by="postcount",ascending=False)['postcount'][:10]
X_train_vals = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1])) $ X_valid_vals = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))
stock.head()
df_t[df_t['Shipping Method name']==271]['Updated Shipped diff'].hist() $ pd.DataFrame(df_t[df_t['Shipping Method name']==271]['Updated Shipped diff'].describe())
df_h1b_mv_ft = df_h1b_mv[df_h1b_mv.full_time_pos=='Y'] $ print('There are {:.0f} visa applications for full-time jobs located in Mountain View in this dataset.'.format(df_h1b_mv_ft.shape[0])) $
Probas = pd.DataFrame(estimator.predict_proba(X1), columns=["Proba_Una", "Proba_Amanda"])
linearReg = LinearRegression() $ linearReg.fit(X_train, y_train) $ predicted_linear_y = linearReg.predict(X_test) $ print("OLS Mean Squared error:",mean_squared_error(y_test, predicted_linear_y )) $ print("OLS R2 Score:",linearReg.score(X_test, y_test))
shelter_df_only_category = shelter_df_idx $ shelter_df_only_category = shelter_df_only_category.drop('DateTime', 'OutcomeType_idx', 'AnimalType_idx', 'SexuponOutcome_idx', 'Breed_idx', 'Color_idx','AgeuponOutcome_binned', 'Ageuponoutcome') $ shelter_cleaned_df = shelter_df_only_category.toPandas() $ shelter_cleaned_df
cityID = '960993b9cfdffda9' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Bakersfield.append(tweet) 
s1 = pd.Series(['A1','A2','A3','A4']) $ s2 = pd.Series(['B1','B2','B3','B4']) $ s3 = pd.Series(['C1','C2','C3','C4']) $ df = pd.DataFrame({ 'A': s1, 'B': s2}) $ df
pysqldf("select * from genes left join essential_genes on genes.symbol = essential_genes.Gene where genomic_accession = 'LT906474.1' and start > 3252780 and end < 3302485")
df.head()
tweets_df["retweet_mean"] = tweets_df.retweet_count.mean()
yah.add_transaction('23.04.2007', agent='me', desc='buy', amount=145, $                     t_type='buying rate: 28.70 USD') $ yah.add_transaction('01.12.2008', agent='me', desc='sell (need cash)', amount=-45, $                     t_type='selling rate: 12.20 USD') $ yah.transactions
totalfare_drivers_by_city= cityfare_driver.groupby(cityfare_driver['type']).sum().reset_index() $ citylabels = totalfare_drivers_by_city['type'] $ totalarea_fares = totalfare_drivers_by_city['fare']
autos.shape
hc.sql('drop table if exists asm_wspace.final_400hz_2017q4')
df2.user_id.drop_duplicates(inplace=True)
round((model_x.rsquared), 3)
cats_df['breed'].value_counts()
menus['about'].fillna('', inplace=True) $ menus_to_analyze = menus[menus['id'].isin(X['master_menu_id'])]
treatment_df = df2.query('group == "treatment"') $ treatment_cr = treatment_df.query('converted == 1').user_id.nunique()/treatment_df.user_id.nunique() $ new_page_converted = treatment_cr $ new_page_converted
mb.loc['Proteobacteria']
pd.date_range('2017-01', periods=4, freq='W-MON')  # First 4 Mondays in Jan 2017
df.query('(group == "treatment" and landing_page != "new_page") or (group == "control" and landing_page == "new_page")').user_id.count()
a.split(':')
df_birth.tail()
crime_wea['Date']=pd.to_datetime(crime_wea['Date']) $ crime_wea.index=crime_wea['Date']
r.json()
test = pd.read_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/ratings.csv', sep = ',', $                    encoding='utf-8', low_memory=False)
df.groupby('key').aggregate({'data1': 'min', $                             'data2': 'max'})
import os.path as op $ my_gempro.save_json(op.join(my_gempro.model_dir, '{}.json'.format(my_gempro.id)), compression=False)
type_df = new_df_left.dtypes.reset_index() $
print("Number of survey results :", len(data)) $ print("Number of Different locations :", len(set(data["SiteId"].values))) $ print("Number of Different game types :", len(set(data["GameId"].values)))
graf=df.copy()
unordered_df.head()
items.old[items.old.code_ogr.isin(obsolete_items)].tail()
cohort_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
cast_data.head()
obj2=Series([1.5,-2.5,0],index=index) $ obj2.index is index
ab_df_new[['CA','UK','US']] = pd.get_dummies(ab_df_new.country)
esp = draft_df[draft_df.language == 'es']
%%timeit -n1 -r2 $ concessions = mr['description'].map_partitions(get_rental_concession_vec) $ T5 = concessions.compute()
cm = metrics.confusion_matrix(y_val, res) $ print(cm) $ print(classification_report(y_pred=res,y_true=y_val)) $ print(np.round(f1_score(y_pred=res,y_true=y_val),3))
df['US'] = 0 $ df['UK'] = 0 $ df.loc[df.country == 'US', 'US'] = 1 $ df.loc[df.country == 'UK', 'UK'] = 1 $ df.head()
session.query(Measurement.station).distinct().count()
gMapAddrDat.set_ServerTimeout(9)
top_conf = summary.groupby('week_id')sort_values(by=['week_id', 'confidence'], ascending=[True, False])
import statsmodels.api as sm $ convert_old = len(df2_control[df2_control['converted'] == 1]) $ convert_new = len(df2_treatment[df2_treatment['converted'] == 1]) $ n_old = len(df2_control.index) $ n_new = len(df2_treatment.index)
reddit.nunique()
n_new = df[df.landing_page=='new_page'].shape[0]
hn = pd.read_csv('../data/HN_posts_year_to_Sep_26_2016.csv', index_col='id', parse_dates=['created_at']) $ hn.dtypes
file = open(resumePath, "r") $ resume = file.read() $ corpus.insert(0, resume)
plt.hist(tobs_data, bins=12, label='tobs', color = 'skyblue') $ plt.xlabel("Temparture Observation Data") $ plt.ylabel("Frequency") $ plt.savefig("station_analysis.png") $ plt.show()
pickle.dump(history_list, open('/mnt/data2/shalaby/history_list_sample_0.0001.pickle','w'))
with open('./data/out/xport_demo.xpt', 'wb') as f: $     xport.from_dataframe(_xport_dm2, f)
print(etsamples.subject.unique()) $ print("There are %i subjects"%(etsamples.subject.unique().shape))
apache_people_df.schema
v_id = df_link_yt['video_id'].value_counts().index[1] $ df_link_yt[df_link_yt['video_id'] == v_id]['twitter_id'].unique()
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(n_new, p_new) $     old_page_converted = np.random.binomial(n_old, p_old) $     p_diffs.append(new_page_converted/n_new - old_page_converted/n_new) $
movies=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\movies.csv')
np.eye(3)
print(cc['open'].describe()) $ print(cc['spread'].describe())
df.loc[:,'G'] = np.array([5] * len(df)) $ df
stop_words = set(stopwords.words('english')) $ for i in range(0,len(twitter_final)): $     data= ''.join(twitter_final['text']) $ words = data.split()
control_conv = conv_ind.query('group == "control"').shape[0] $ control_group = df2.query('group=="control"').shape[0] $ print('Probability of CONTROL page converted individual: {:.4f}'.format(control_conv/control_group))
Entity_name = Lab7['ENTITY'].tolist() $ for y in Entity_name: $     ticker.append(get_symbol(y)) 
h + pd.offsets.Hour(1) # Will give us the same result as the h + 1 sum above
pd.set_option('display.max_rows', 20) $ df
au.find_some_docs(uso17_coll,limit=3)
betas_mask = np.zeros(shape=(mcmc_iters, n_bandits)) $ betas_mask[np.arange(mcmc_iters), betas_argmax] = 1
dfClientes=pd.read_csv(clientes_path, header=0, sep=',', encoding='latin1', dtype={"CODIGOCLIENTE":str}) $ dfTransacciones=pd.read_csv(transacciones_path, header=0, sep=',', encoding='latin1', dtype={"CODIGOCLIENTE":str})
df_t_worst['Shipped At'] = df_t_worst['Shipped At'].apply(lambda x: x.date()) $ df_worst_chart = pd.DataFrame(df_t_worst[df_t_worst['Place Name'].isin(['Diemen','Stadscentrum','Eindhoven'])] $                     .groupby(['Place Name','State','Latitude','Longitude','Shipped At'])['Updated Shipped diff_normalized'] $              .mean()).sort_values(by='Place Name',ascending=True)
sales_corr = sales_update.corr() $ sales_corr
pair_info = potential.pair_info(system.symbols) $ print(pair_info)
df_mes = df_mes[(df_mes['extra']==0)|(df_mes['extra']==0.5)|(df_mes['extra']==1)|(df_mes['extra']==1.5)] $ df_mes.shape[0]
gender_comp = free_data.groupby('sex')['v1','v2','v3','v4','v5','v6'].sum()
testObjDocs.outDF.tail()
actual_value_second_measure = holdout_results[holdout_results.second_measurement==1].\ $ groupby('wpdx_id').status_binary.sum() $ actual_value_second_measure.head()
graf_train=pd.concat([graf_train, train_topics_df], axis=1) $ graf_test=pd.concat([graf_test, test_topics_df], axis=1)
scores = model.evaluate(X_train, y_train, verbose=0) $ print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100)) $ test_sample=np.array([145,183,1902,1795,178]).reshape(1,5) $ sample_output=model.predict_classes(test_sample) $ print(sample_output) $
no_outliers_model = Prophet(yearly_seasonality =True,weekly_seasonality= True,daily_seasonality = True) $ no_outliers_model.fit(df1_no_outliers); $ no_outliers_future = no_outliers_model.make_future_dataframe(periods= 6, freq = "m") $ no_outliers_forecast = no_outliers_model.predict(no_outliers_future)
state_DataFrames['OH_R_con'].head(30)
genes.head()
fish = pandas.DataFrame({'size': [100, 120, 70], $                          'weight': [20, 30, 25]}, $                         index = ['Brown Trout', 'Atlantic Salmon', 'Chinook Salmon'])
avg_day_of_month15 = day_of_year15.groupby("avg_day_of_month").mean() $ avg_day_of_month15.head()
image_pred_df = pd.read_csv('image-predictions.tsv', sep='\t') $ image_pred_df.head(2)
properati.country_name = properati.place_with_parent_names.apply(lambda x : x.split('|')[1])
plt.legend(handles=[Precipitation], loc="best") $ plt.savefig("date_Fre.png")
van_final[ben_final['pagetitle'].str.contains('/')]
df2.user_id.count()
score_rate = sudptable[["author", "score"]] $ score_rate = pd.merge(score_rate, posts, how="left", on="author") $ score_rate["rate"] = score_rate["score"] / score_rate["links"] $ score_rate.sort_values(inplace=True, by="links", ascending=False) $ score_rate[0:9].sort_values(by="rate", ascending=False)
livetweets.find_one()
cust_data1.drop('MonthlySavings1', axis=1).head(2) # it creates new data $ cust_data.head(2)
sampled_authors_saved.count()
driver = webdriver.Chrome('/Users/daesikkim/Downloads/chromedriver', chrome_options=options) # chrome_options=options $ driver.implicitly_wait(3)
area_dict = {'California': 23453, 'Texas': 3456, 'New York': 234503409, 'Florida': 23453634, 'Illinois': 2345342} $ area = pd.Series(area_dict) $ area
raw = fat.get_price_data(ticker) $ ohlcv = raw
pd.Series(index = feats_used, data = rf.feature_importances_).sort_values().plot(kind = 'bar')
log_mod3 = sm.Logit(df_joined['converted'], df_joined[['intercept', 'ab_page', 'CA', 'UK', 'it_CA', 'it_UK']]) $ results3 = log_mod3.fit() $ results3.summary()
compound_df = compound_df.reset_index()
run txt2pdf.py -o"2018-06-19 2015 MAYO CLINIC Sorted by discharges.pdf"  "2018-06-19 2015 MAYO CLINIC Sorted by discharges.txt"
n_new = df2.query("landing_page == 'new_page'")['user_id'].count() $ n_new
with open('../data/channel_topic.json', 'r') as f: $     topics = json.load(f)
X = np.array(df_ohc.iloc[:,4:]) $ Y = np.array(df_ohc[['trip_num','trip_dur_avg']])
dumping_dealers = np.unique(np.asarray(DUMPINGS.showroom_id)) $ dumping_dealers
BDAY_PAIR_df.set_index('uid',inplace=True)
tz_dateutil = dateutil.tz.gettz('Europe/London')
outcome.shape
not_creditworthy_user.shape
week4 = week3.rename(columns={28:'28'}) $ stocks = stocks.rename(columns={'Week 3':'Week 4','21':'28'}) $ week4 = pd.merge(stocks,week4,on=['28','Tickers']) $ week4.drop_duplicates(subset='Link',inplace=True)
from IPython.core.magic import (register_line_magic, register_cell_magic, $                                 register_line_cell_magic) $ @register_cell_magic $ def sql(line, cell): $     return presto2df(cell)
df_new[['Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', 'Wednesday']] = pd.get_dummies(df_new['day_of_week'])[['Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', 'Wednesday']] $ df_new.head()
df.iloc[-1].name
df3=df3.merge(df2,how="inner") #Merging by Common User Id $ df3.head(3) $ df3[["UK","US"]]=pd.get_dummies(df3['country'],drop_first=True)
old_page_converted = np.random.choice([1, 0], size=n_new, p=[p_old, (1-p_old)]) $ print(old_page_converted)
index = pd.date_range('2000-1-1', periods=1000, freq='M')
train_fs2=kd_utils.load_data('../feature_data/train_fs2.pkl') $ test_fs2=kd_utils.load_data('../feature_data/test_fs2.pkl')
df_final.corr(method='pearson')
compiled_data[pd.isnull(compiled_data['botometer'])==True][0:2]
cashflows_act_investor_20150430[(cashflows_act_investor_20150430.id_loan==27) & (cashflows_act_investor_20150430.fk_user_investor==2190)].to_clipboard()
comps_df = comps[(comps.entity_uuid.isin(df.uuid)) & (comps.competitor_uuid.isin(df.uuid))].copy() $ comps_df = pd.merge(comps_df,df[['uuid','company_name']],left_on = 'entity_uuid',right_on = 'uuid',how = 'left') $ comps_df = pd.merge(comps_df,df[['uuid','company_name']],left_on = 'competitor_uuid',right_on = 'uuid',how = 'left') $
proj_df['lit_and_lang'] = proj_df['Project Subject Category Tree'].fillna('').apply(has_literacy_and_language) $ proj_df.lit_and_lang.sum()
graf_counts.head()
d1 = df[(df['group']=='treatment') & (df['landing_page']=='new_page')]    # to create new dataframe $ d2 = df[(df['group']=='control') & (df['landing_page']=='old_page')] $ df2 = pd.concat([d1,d2]) $ df2.head(5)
print((data["Time Stamp"][0] - data["Time Stamp"][200])) $ print((data["Time Stamp"][0] - data["Time Stamp"][200]).days) $ print((data["Time Stamp"][0] - data["Time Stamp"][200]).seconds)
mustang.model
nitrogen = results[mediaMask & hydroMask & charMask & sampFracMask] $ nitrogen.shape
max_IMDB = scores.IMDB.max()
from sklearn import metrics $ print metrics.accuracy_score(y_test, predicted) $
dict_urls['web']['project'].split('?')[0]
p_diffs = np.zeros((10000)) $ for i in range(10000): $     sam_new = np.random.binomial(1, pnew, nnew) $     sam_old = np.random.binomial(1, pold, nold) $     p_diffs[i] = ((sam_new).mean()) - ((sam_old).mean())
def getHour(date): $     time = date.split(" ")[1] $     hour = time.split(":")[0] $     return int(hour)
sns.countplot(x="dayofweek",data=twitter_final)
df_arch_clean["tweet_id"] = df_arch_clean["tweet_id"].astype("str")
plt.scatter(df['B'], df['C']) $ plt.title('Scatterplot of X and Y')
import time $ recent_comment_df = pd.DataFrame(recent_comments_included) $ recent_comments_filename = "r_worldnews_comments_02.18.2017.a.csv" $ recent_comment_df.to_csv(os.path.join("outputs",recent_comments_filename))
df.shape
cohort_retention_df.head()
df = df [new_colorder] $ df.set_index('empID',inplace=True) $ df
p = getpass.getpass() $ try: $     conn = psycopg2.connect("dbname='postgres' user='rsouza' host='localhost' password='{}'".format(p)) $ except: $     print("I am unable to connect to the database")
bb[['low','high']].plot()
time_series.info()
tweets_possible_bots = tweets.query("snsuserid.isin(@possible_bots.snsuserid.values)") $ tweets_possible_bots.shape
df.fillna(0.0,inplace=True) $ dfg = df.groupby("date").mean() $ dfg.rename(columns={"prcp":"precipitation"},inplace=True) $ dfg.head(5)
daily_returns.plot(kind='scatter',x='SPY',y='XOM') $ plt.plot(daily_returns['SPY'],daily_returns['SPY']*beta_XOM + alpha_XOM,'-',color='r') $ plt.show()
 new_profile = {'user_id': 213, 'reinsurer': 'XL American'} $  duplicate_profile = {'user_id': 235, 'reinsurer': 'SCOR S.E'} $  result = db.profiles.insert_one(new_profile)  # This is fine. $  result = db.profiles.insert_one(duplicate_profile)
oppose.sort_values("amount", ascending=False).head(10)
hru_rootDistExp = rootDistExp.open_netcdf()
plt.hist(p_diffs); $ plt.ylabel('Frequency') $ plt.xlabel('p_diffs')
users.to_csv('users_cleaned.csv')
other_text_feature.toarray()[0,:]
s519281_df.describe()
prices = prices.join(announcement_dates)
vader_df = pd.DataFrame(vader_scores)[['text', 'created_at','compound', 'neg', 'neu', 'pos']] $ vader_df = vader_df.sort_values('compound', ascending=True) $ vader_df.head(7)
def last_word(string, word_separator=' '): $     words = string.split(word_separator) $     return words[-1]
removing_features_train = X_train[:, :5] $ removing_features_validate = X_validate[:, :5] $ X_train = np.delete(X_train, [0, 1, 2, 3, 4], axis=1) $ X_validate = np.delete(X_validate, [0, 1, 2, 3, 4], axis=1)
prob_rf = rf.predict_proba(Test) $ prob_rf
aml = H2OAutoML(max_runtime_secs = 300, # 5 minutes $                 seed=2055) $ aml.train(x = predictors, y = Y, $           training_frame = train)
autodf = autodf.drop('abtest',1)
df.head()
train.drop('Date of Birth', axis = 1, inplace = True) $ test.drop('Date of Birth', axis = 1, inplace = True)
resultvalue_df['valuedtoff'] = resultvalue_df.apply(lambda x: pd.to_datetime(x['valuedatetime']).tz_localize(dateutil.tz.tzoffset(None, $                                                                                                                                int(x['valuedatetimeutcoffset'])*60*60)).astimezone(pytz.UTC), axis=1)
train.head()
dedup[dedup.job_type == 'Organic'].hash.value_counts()[0:3]
old_page_converted = np.random.choice([0,1], n_old, [(1-p_old), p_old])
word_freq_df = pd.read_csv('word_freq.txt', delim_whitespace=True) $ word_freq_df.columns = ['Frequency','Word','Parts_of_Speach','Num_doc_occurences'] $ word_freq_df = (word_freq_df[~word_freq_df.Word.isnull()])
df_place = pd.DataFrame(list_places) $ df_user = pd.DataFrame(list_users) $ df_coor = pd.DataFrame(list_coor)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ result = logreg.fit() $ result.summary()
pd.pivot_table(more_grades, index="name", values=["grade","bonus"], aggfunc=np.max)
c.index
top_10_authors = git_log['author'].value_counts()[:10] $ top_10_authors
news_df = pd.read_json("Newschannel_tweets_df.json").sort_values('Source Acc.') $ news_df.head()
df = pd.read_csv('../Created CSVs/Cleaned_For_Analysis_All_Data.csv')
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ] $ pd.pivot_table(games_2017, values = "Opp.Pts", index = "Team", aggfunc = np.mean).sort_values(by = "Opp.Pts", ascending = False).head(5)
df_business = df_average.join(df_categories_svd) $ print df_business.shape $ df_business.head()
weather['DATE']=pd.to_datetime(weather['DATE']) $ weather['date']=weather['DATE'].astype(str) $ weather.info() $ weather.head()
soup = bs(facts_res.text, 'html.parser')
df2['FlightDate'].head()
old_page_converted = np.random.choice([1,0],n_old,p=[p_old,(1-p_old)]) $ old_page_converted
ts.info()
sentiments.tail()
print(my_df["xcoordinate"].describe()) $ print() $ print(my_df["ycoordinate"].describe())
from sklearn.metrics import log_loss $ for col in train_agent_and_comment.columns: $     print(log_loss(y ,train_agent_and_comment[col]))
test = pd.read_csv(dirs[2], dtype={"ip":"int32", "app":"int16", "device":"int16", "os":"int16", "channel":"int16"})
intake.head()
df_archive["floofer"].value_counts()
Month = df['order_date'].groupby(df.order_date.dt.to_period("M")).agg('count').plot()
df_countries = pd.read_csv('./countries.csv') $ df_countries.groupby('country').count()
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country'])
conn.droptable('data.iris', caslib='casuser')
plt.title('Overall ngram', fontsize=18) $ overall_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/overall_ngram.jpg')
twitter_archive_clean=twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()]
image_copy.info()
line = next(file.generateParsedLines()) $ line
len(repos.id.unique())
dfChile = dfChile[dfChile["longitude"] > -76.000000]
p_stats.head(1)
c.find_one({'albums.released': 1982})
len(df2.loc[df['landing_page'] == 'new_page'])/len(df2['user_id'])
for idx, row in df_trips.iterrows(): $     passengers_from_planet = passenger_planets[passenger_planets["planet"] == row["planet"]] $     df_trips.loc[idx, "passenger"] = row["passenger"] = np.random.choice(passengers_from_planet["passenger"]) $ df_trips["passenger"] = df_trips["passenger"].astype("int64")
sns.countplot(x="fluctuation", data=bitc, palette="Greens_d")
y_hat = model.predict(X_test)
from sklearn.feature_selection import SelectFromModel $ model = SelectFromModel(lm,prefit = True) $ lm_new = model.transform(x_test) $ lm.score(x_test,y_test)
zscore_fxn = lambda x: (x - x.mean()) / x.std() $ features['f09'] =prices.groupby(level='symbol').close.apply(zscore_fxn) $ features.f09.unstack().plot.kde(title='Z-Scores (not quite accurate)')
plot_confusion_matrix(cm_dt, classes=['COLLECTION', 'PAIDOFF'],normalize=False, title="Confusion matrix for knn", cmap=plt.cm.Blues)
trigram_model = prep.get_trigram_model(from_scratch=False):
p6_result = p3_table.sort_values('Profit', ascending=True) $ p6_result.head()
df_new[['ca', 'uk', 'us']] = pd.get_dummies(df_new['country']) $ df_new.head()
for x in range(0,9): $     avg_values.append(25)
grouped.describe()
logit_results.summary()
print("Training took {:.2f}s".format(t1 - t0)) $
CHI = pd.read_excel(url_CHI, $                     skiprows = 8)
X_test_dtm = stfvect.transform(X_test) $ X_test_dtm.shape
plt.figure() $ plot_confusion_matrix(ConfustionMx[mean_acc.argmax()], classes=['PAIDOFF','COLLECTION' ],title='Confusion matrix, with normalization',normalize=True)
plot_autocorrelation(series=dr_num_new_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of new patients')) $ plot_autocorrelation(series=dr_num_existing_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr number of existing patients'))
p=pd.Panel({"ClassA":df_A,"ClassB":df_B})
(np.array(null_vals) > obs_diff).mean()
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = logit_mod.fit() $
1 / np.exp(-0.0150)
test_features = users[["timestamp_first_active", "signup_flow", "age"]].values
preci_df = pd.DataFrame(preci_data) $ preci_df.head()
ngrams_summaries = cvec_3.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
region_cat_return_amt = returned_orders_data.groupby(['Region', 'Category'])['Sales'].sum() $ print(region_cat_return_amt)
own_star_2csv = own_star.drop(['forked_from_repo_id', 'owned', 'created_at', 'starred'], axis=1) $ own_star_2csv = own_star_2csv.rename(columns={'repo_id': 'item', 'user_id': 'user'}) $ own_star_2csv[['user', 'item', 'rating']].to_csv('data/new_subset_data/ratings_data.csv', sep='\t', index=False)
train_df.corr()
data_for_model.corr()
pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))
ks_projects['deadline'] = ks_projects['deadline'].apply(lambda x: x.split(" ")[0]) $ ks_projects['launched'] = ks_projects['launched'].apply(lambda x: x.split(" ")[0]) $ ks_projects['deadline'] = ks_projects['deadline'].apply(lambda x: datetime.strptime(x, "%Y-%m-%d")) $ ks_projects['launched'] = ks_projects['launched'].apply(lambda x: datetime.strptime(x, "%Y-%m-%d"))
USER_PLANS_df = BID_PLANS_df.copy(deep=False) $ USER_PLANS_df.head()
tweets = treino["text"].values # obtendo texto dos tweets $
len(station_tobs) $ print(" There are {} tobs within the data set".format(len(station_tobs)))
hpdpro[hpdpro['ComplaintID']=='8338184']['StatusDescription'].tolist()
U_B_df = query_df(U_B_URL) $ U_B_df.head()
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
places = api.geo_search(query="Edinburgh", granularity="city") $ place_id_E = places[0].id $ print('Edinburgh id is: ',place_id_E)
converted_usersU = sum(dfU.converted == 1)/len(unique_usrs) $ converted_usersU
df_day=df[(df["day"] == 1)] $ df_night=df[(df["day"] == 0)] $ scipy.stats.ks_2samp(df_day["tripduration"],df_night["tripduration"])
tweet_data.info()
data_scrapped.head()
m1 = pd.merge(reading, stories, on='story_id',how='inner') $
pd.Series(data=predicted10).hist()
df2.tail()
plt.scatter(USvideos['likes'], USvideos['views'])
model.summary()
closed_issues = Issues(github_index).is_closed().get_cardinality("id_in_repo").by_period(field="closed_at") $ print("Trend for month: ", get_trend(get_timeseries(closed_issues))) $ closed_issues = Issues(github_index).is_closed().get_cardinality("id_in_repo").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(closed_issues)))
sinplot() $ sns.set_style("ticks") $ sns.despine(offset=10, trim=True)
rent_db3.describe()
from statsmodels.stats.diagnostic import acorr_ljungbox $
urllib.request.urlretrieve(link, "../datasets/CSVs/bigcsv.zip")
ret_aapl = calc_daily_ret(closes_aapl) $ ret_aapl.plot(figsize=(8,6))
sentiment_df = pd.DataFrame(save_sentiment) $ sentiment_df.to_csv('sentiment.csv', sep=',', header=True, index=True, index_label=None) $ sentiment_df
ca_pl_all.loc[(ca_pl_all.FIRST_ACCOUNT!=ca_pl_all.LAST_ACCOUNT)].shape
speeches = [] $ speeches.append(mandelaSpeechDict) $ SpeechDF = pandas.DataFrame(speeches) $ SpeechDF = mandelaSpeechDF[['speech-title', 'speech-text']] $ SpeechDF
for t in range(1, T+1): $     train_shifted[str(T-t)] = train_shifted['load'].shift(T-t, freq='H')
shannon_petropavlovsk_relative = shannon_petropavlovsk / np.log2(len(petropavlovsk_freq)) $ shannon_petropavlovsk_relative
df_total.tail()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'larger') $ z_score, p_value
temperatura_global = pd.read_csv('GlobalTemperatures.csv')
bg3.columns # find out what the column headers are
(day_counts.groupBy('week', 'hashtag') $            .avg('count') $            .withColumnRenamed('avg(count)', 'average') $            .sort('average', ascending=False) $ ).show(20)
start = 0 $ start = pd.to_datetime(start, unit='s') $ print(start)
archive_clean[archive_clean['tweet_id'] == 680494726643068929].rating_numerator
not_lined_up_df = df.query("(group == 'treatment' and landing_page != 'new_page') or (landing_page == 'new_page' and group != 'treatment')") $ print("The number of times the new_page and treatment don't line up: {}".format(len(not_lined_up_df)))
with open(os.path.join(outputs, 'clean_reviews.pkl'),'wb') as f: $     pickle.dump((clean_train_reviews, $                  clean_test_reviews, $                  clean_train_reviews_sw, $                  clean_test_reviews_sw),f)
data['Created Date'] = pd.to_datetime(data['Created Date'], errors='coerce') $ data['Created Year'] = data['Created Date'].dt.year
df_max.head()
data.treatment = 1 $ data
old_page_converted = df2.sample(n_old, replace = True)
df_new = df.query('group == "treatment" and landing_page == "old_page" or group == "control" and landing_page == "new_page"  ') $ df_new.shape[0]                 
df.filter(like='year')
repos.id.sort_values().head()
1/np.exp(results.params[3])
shows.dtypes
add_datepart(scaled, 'Date') $ scaled.head()
df.drop(index=100, inplace=False)       # single row
df.query('(group=="treatment" and landing_page=="old_page") or (group=="control" and landing_page=="new_page")').shape[0]
sp500.ix[[10, 200, 450]]
ff3['Date'] = ff3['Date'].apply(lambda x: str(pd.to_datetime(str(x), format= '%Y%m'))[0:7])
props = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
s1 = pd.Series(np.arange(1, 6, 1)) $ s2 = pd.Series(np.arange(6, 11, 1)) $ pd.DataFrame({'c1': s1, 'c2': s2})
import seaborn as sns $ sns.set() $ sns.set_context("talk")
wednesdays = pd.date_range('2014-06-01','2014-08-31', freq='W-WED') $ wednesdays.values
df_ab_raw['line_up'] = np.where(((df_ab_raw['group'] == 'treatment') & (df_ab_raw['landing_page'] != 'new_page')) | $                                 ((df_ab_raw['group'] != 'treatment') & (df_ab_raw['landing_page'] == 'new_page'))  , 1, 0)
from nltk.sentiment.vader import SentimentIntensityAnalyzer $ sid = SentimentIntensityAnalyzer() $ for tw in tweets: $     print(tw) $     print(sid.polarity_scores(tw))
from sklearn.tree import DecisionTreeClassifier $ DT_model = DecisionTreeClassifier(criterion="entropy", max_depth = 4) $ DT_model.fit(X_train,y_train) $ DT_model
plt.scatter(df_2015['state_bottle_retail'], df_2015['state_bottle_cost']) $ plt.xlabel('Sales $'); $ plt.ylabel('Bottles Sold'); $ plt.title('Bottles Sold x Sales $')
m.lr_find()
clf = clf.partial_fit(X_test, y_test)
df2[df2.user_id==773192]
df_master.drop(['tweet_id'], axis=1).describe()
(df2['landing_page']=='new_page').mean()
df_copy['stage'].value_counts()
def listening_longevity(x): $     x['listening_longevity'] = (x.iloc[-1].date - x.iloc[0].date).days $     return x $
r.html.links
X = tfidf_vectorizer.fit_transform(df['article'])
import statsmodels.api as sm $ convert_old = df2[df2['group'] == 'control'].converted.sum() $ convert_new = df2[df2['group'] == 'treatment'].converted.sum() $ n_old = df2[df2['group'] == 'control'].converted.size $ n_new = df2[df2['group'] == 'treatment'].converted.size
utils.add_coordinates(data)
df2.groupby('landing_page').user_id.count()[0]/df2.user_id.count()
gMapAddrDat.set_statusMsgGrouping(12)
mu = ret_aapl.mean().AAPL $ sigma = ret_aapl.std().AAPL $ ndays = (datetime(2018,6,15).date()-datetime(2018,3,21).date()).days $ nscen = 10000 $ K = 180
print(len(free_data.country.unique()))
df_timeseries = df_history[['symbol','date','open','high','low','close','marketvalue','quantity','movement_delta','peaks_delta','cryptdex_value','cryptdex_quantity','cryptdex_index']].copy()
df.query('landing_page=="new_page" & group=="control"').shape[0] + df.query('landing_page=="old_page" & group=="treatment"').shape[0]
merged_df = df_genre.join(dtm_tfidf_df, how = 'right', lsuffix='_x') $ merged_df
save_model('model_logistic_reg_v1.mod', lgr_grid)
df1 =pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
df_master[df_master.retweet_count == [df_master['retweet_count'].max()]]
from nltk.corpus import conll2000 $ from nltk import conlltags2tree, tree2conlltags $ train_labels[3]
from pandas.tseries.offsets import Hour, Minute $ from datetime import timedelta
df_likes_grouped.sort_values('Total',ascending=False)
df2 = df.query('landing_page == "new_page" & group == "treatment"') $ df2 = df2.append(df.query('landing_page == "old_page" & group == "control"'))
def calculate_cumulative_returns(returns): $     cumulative_returns = (returns.sum(axis=1)+1).cumprod() $     return cumulative_returns $ project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)
contractor.tail()
len(orig_iphone_tweets[orig_iphone_tweets['date'].dt.year == 2016])
edu_columns=education.columns.tolist()
model_rf_14 = RandomForestClassifier(max_depth = 14, random_state=42) $ model_rf_14.fit(x_train,y_train) $ print("Train: ", model_rf_14.score(x_train,y_train)*100) $ print("Test: ", model_rf_14.score(x_test,y_test)*100) $ print("Differnce between train and test: ", model_rf_14.score(x_train,y_train)*100-model_rf_14.score(x_test,y_test)*100)
comunas = pd.read_csv('datasets/comunas.csv', sep=',', error_bad_lines=False, low_memory=False) $ comunas.info()
!head -n 10 p32cf_results.txt
nb.class_count_
browser = webdriver.Chrome('chromedriver.exe') $ browser.get(url_schiaparelli) $ time.sleep(5) $ html = browser.page_source $ soup = BeautifulSoup(html, "html.parser")
xpdraft2.columns = ('name','status','count') $ xpdraft2.head()
to_be_predicted_Day3 = 36.47754126 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
mb1 = mb_file.parse("Sheet 1", header=None) $ mb1.columns = ["Taxon", "Count"] $ mb1.head()
df_columns.set_index('created_at', inplace = True) $
data = pd.read_csv("links.csv") $ data["created_utc"] = pd.to_datetime(data["created_utc"]) $ data.head(10)
m3.clip = 25.
c_df['Date']= c_df.index $
store1_open_data[['Customers']].plot()
df_coun.head() $ mdf = pd.merge(df2, df_coun, how="left", on="user_id")
P_old = ab_df2['converted'].mean() $ print(P_old)
np.exp(0.0783), np.exp(0.0469)
pd.concat([closes_aapl,sim_closes]).plot(figsize=(8,6));
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df_sorted = df_measurement.groupby(['station'],sort=True).count() $ df_sorted.sort_values('id',ascending=False) $
x = data2_df.BikeID.resample(sample_rate).count() $ x.name = 'Count' $ y =  data2_df.resample(sample_rate).mean() $ y = y.drop(columns=['BikeID', 'ID', 'Lat', 'Lon', 'Casual', 'Member', 'Fall', 'Spring', 'Summer', 'Winter']) $ R_trip = pd.concat([x, y], axis=1)
psy_df5 = HAMD.merge(psy_df4, on='subjectkey', how='right') # I want to keep all Ss from psy_df $ psy_df5.shape
mod2 = sm.Logit(df_new['converted'], df_new[['intercept', 'UK','CA','ab_page']]) $ results2 = mod2.fit()
subred_counts.subreddit.values
support = merged[merged.committee_position == "SUPPORT"]
autos['odometer_km'].value_counts().sort_index(ascending = False)
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2) $ store_items
pnew=df2['converted'].mean() $ pnew
id_dense = np.reshape(df_cat_stat.main_cat_id.tolist(), (-1, 1)) $ goal_dense = np.reshape(df_cat_stat.goal.tolist(), (-1, 1)) $ duration_dense = np.reshape(df_cat_stat["duration"].tolist(), (-1, 1)) $ print(id_dense.shape)
with open(os.path.join(folder_name, url.split('/')[-1]), mode='wb') as file: $     file.write(response.content)
df2.groupby('landing_page').mean()['converted']
iphone = trump.loc[trump['source'] == "Twitter for iPhone", :] $ android = trump.loc[trump['source'] == "Twitter for Android", :] $ ax = sns.kdeplot(iphone['hour'], label="Twitter for iPhone") $ sns.kdeplot(android['hour'], label="Twitter for Android") $
df.describe()
hashed_train_sample.show(5, truncate=False)
len(graffiti2[graffiti2['precinct']==1.0])
(autos["ad_created"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().head(10) $ )
tz_dog.describe()
todaysFollowers.to_csv('Data/todaysFollowers_'+date+'.csv',sep=';',index=False)
w = 'holocaust' $ model.wv.most_similar (positive = w)
archive_copy.floofer.unique()
stops["arrest_type"].value_counts()
tweet_data = pd.read_csv('tweet_json.txt', encoding = 'utf-8')
conv_users = df2.query('converted == 1').shape[0] $ p1 = float(conv_users/df2.shape[0]) $ print("The probability of an individual converting regardless of the page is {:.4f}".format(p1)) $
for t in tables: display(t.head())
df['Precipitation'].describe()
linked.to_csv('linked_emailHash.csv')
"ISSID" in df_protest.columns
df_ab.user_id.nunique()
soup.find_all('a', 'tltle')
dog_ratings['source'] = dog_ratings.source.str.extract('<a[^>]*>([^<]*)</a>', expand=True) $ dog_ratings.source = dog_ratings.source.astype('category') $ dog_ratings.source.value_counts()
df_archive_clean.rating_denominator = 10
df3 = pd.DataFrame(df, index = ['b', 'c', 'd', 'a']) $ df3
raw_data.isna().sum()
twitter_archive[twitter_archive['tweet_id'].duplicated()]
tweets_original['full_text'] = tweets_original['full_text'].str.decode('utf-8') $ tweets_original['created_at'] = tweets_original['created_at'].str.decode('utf-8')
dset['surface_flag'].plot()
data['y'] = data['y'].apply(lambda y: 'yes' if y == True else 'no') $ model_data = pd.get_dummies(data)
logreg = LogisticRegression() $ logreg.fit(X_train, Y_train) $ Y_pred = logreg.predict(X_test) $ acc_log = round(logreg.score(X_test, Y_test) * 100, 2) $ acc_log
X = lsa.fit_transform(X)
sensor.system
(3).__add__(4)
users.info()
df['date'] = pd.to_datetime(df['date'])  
index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=12)
Grouping_Year_DRG_discharges_payments.head()
last_year_temp = session.query(Measurement.tobs).filter(Measurement.station=="USC00519281")\ $         .filter(Measurement.date>='2016-08-01').filter(Measurement.date<'2017-08-01').all()
df_merge['interaction_UK'] = df_merge['ab_page'] * df_merge['UK'] $ df_merge['interaction_US'] = df_merge['ab_page'] * df_merge['US'] $ df_merge.head()
density.sort_values(ascending=False, inplace=True) $ density.head()
df = pd.read_csv('Reddit06022018.csv',index_col ='Unnamed: 0' , engine='python')
posts.find_one({"_id": post_id_as_str}) # No result
sb.heatmap(correlation_matrix,annot = True,linewidths = 0.5) $ plt.show()
print ('Shape of the dataset after transformation:' + str(master_file.shape)) $ master_file.head()
byYear = df.resample('y').count()['id'] $ byYear
b = [str(i).split(';')[1] for i in a]
print('{:6}Hi'.format(42)) $ print('{:<6}Hi' .format(42))
import statsmodels.api as sm $ convert_old = df2.query("group == 'control' and converted == 1").count()[0] $ convert_new = df2.query("group == 'treatment' and converted == 1").count()[0] $ n_old = df2.query('group == "control"').count()[0] $ n_new = df2.query('group == "treatment"').count()[0]
df.groupby('Complaint Type').count().sort(desc("count")).show(10)
data = fat.add_bollinger_bands(data, 'Close')
from sklearn.cluster import KMeans $ kmeans_model = KMeans(n_clusters=7) $ kmeans_model.fit(df)
date = '2017-10-11' $ url = urllib.request.urlopen('https://www.epexspot.com/en/market-data/dayaheadauction/auction-table/%s/de'%date) $ url = url.read() $ soup = BeautifulSoup(url,"lxml")
Amazon = web.DataReader('AMZN', 'google', start, end) $ Amazon.head()
df['Market'].head(1)
WholeDf['date'] = pd.DatetimeIndex(datecreated).normalize()
from sklearn.linear_model import LogisticRegression
import awrams.utils.wiski.interface as W $ wcds = W.get_station_list("Water Course Discharge") # Get discharge volume (m3/sec) from wiski for gauging stations $ stns = W.find_id_in_station_list('105001',wcds) $ stns  #['105001B', '105001A']
df6 = pd.read_csv('2006.csv')
df = pd.merge(df, tran_time_diff, on='msno', how='left').drop_duplicates(['msno','order_number_rev'])   $
top_songs.to_csv('top_songs_clean.csv')
facts_metrics.head()
api.rate_limit_status()
p_treat = df2.query('group=="treatment"').converted.mean() $ p_treat
archive_clean.info() $
data.head()
y_pred = model.predict(X_test) $ y_test_rescaled = scaler.inverse_transform(y_test) $ y_pred_rescaled = scaler.inverse_transform(y_pred)
slicer = df.iloc[:, 0:5] $ slicer
knn = KNeighborsClassifier( n_neighbors=3,  $                            weights='uniform') $ scores = cross_val_score(knn,  X_train, y_train,  cv=5) $ model = knn.fit(X_train, y_train) $ np.mean(scores), np.std(scores)   # scoring on my Training Data set was really worst
states = properati.place_with_parent_names.apply(lambda x : x.split('|')[2])
segmentData.opportunity_conversion.value_counts()
def scorecard_by_symbol(df): $     return df.groupby(level='symbol').apply(calc_scorecard).T $ print(scorecard_by_symbol(df))
twitter_archive_master.head()
pizza_train_model_final.print_topics(num_topics = 10 ,num_words = 10)
new_1 = df.query("landing_page != 'new_page' and group == 'treatment'") $ new_2 = df.query("landing_page == 'new_page' and group != 'treatment'") $ times =  new_1.shape[0] + new_2.shape[0] $ print("The number of times the new_page and treatment don't line up is: {}".format(times))
grid_id = np.arange(1, 1535,1) $ grid_id_array = np.reshape(grid_id, (26,59)) $ grid_id_flat = grid_id_array.flatten()
s = '3.14159' $ type(s)
plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Freq') $ plt.title('10 k  simulate'); $ plt.axvline(x=(pnew-pold), color='r'); $
('threshold, F1 score : {0}').format(gbm_model.F1(valid=True)[0])
df_select.head()
users.to_csv('data_table/user.csv', index = False)
mentions_df_begin.head()
Xs.to_csv('just_subreddit', index=False)
G8.index = ['CAN', 'FRA', 'DEU', 'ITA', 'JPN', 'RUS', 'GBR', 'USA'] $ G8
tfa_train=train.timestamp_first_active.value_counts() $ tfa_test=test.timestamp_first_active.value_counts() $ tfa_train.unique()
last_values_liberia = grouped_months_liberia.last() $ last_values_liberia=last_values_liberia.rename(columns = {'National':'last_v_T'}) $
eia_facility['fuel'].unique()
df_merged = pd.merge(df_clean, tweet_df_clean, on='tweet_id', how='inner') $ df_merged.info()
ltc = pd.read_json('ltc.json', lines=True) $ xrp = pd.read_json('xrp.json', lines=True) $ eth = pd.read_json('eth.json', lines=True)
firstday_df.dtypes
df = data.groupby('Date').sum() $ df['mood']= df['SA'].rolling(window=20).apply(lambda x : np.sum(x)) $ fig = plt.subplots(1,1,figsize=(8,8)) $ plt.plot(df.index,df['mood']) $ plt.title('The Mood of the President',fontsize=14); $
1.0*sum(df2['converted']==1)/ n_valid_users
df_archive_clean["timestamp"] = pd.to_datetime(df_archive_clean["timestamp"],format='%Y-%m-%d %H:%M:%S ',utc = True)
p = p.set_index(["frameworkProgramme"])
df_wu = pd.read_csv('weather_kdca0917.csv', index_col='date', parse_dates=True) $ df_wu.dtypes
df_data = pd.read_csv(CSV_FILE, dtype=str) $ print '%d rows' % len(df_data) $ df_data.head()
def sumthis(a, b): $     return a+b $ list(map(lambda x, y: sumthis(x, y), [i for i in df.a], [j for j in df.b]))
data.nlargest(10, 'Z-Score') # This could be later put into a button for easy access. 
df = kickstarter_failed_successful.sample(5000) $ df
val_size = 0.05 $ state = 20 $ experiment_X, validation_X, experiment_y, validation_y = train_test_split(X, y, test_size=val_size, random_state = state)
from sklearn.decomposition import PCA $ PCA(2)
legHouse = legHouse.set_index("TLO_id")
twitter_archive_df.info()
from scipy.stats import norm $ z_sig = norm.cdf(z_score) # Tells us how significant our z-score is $ crit_val = norm.ppf(1-(.05/2)) # Tells us what our critical value at 95% confidence is $ print(z_sig, crit_val)
games_2017_sorted.loc[:, ["Team", "Opp", "Home", "Home.Attendance"]].tail(10) # games are duplicated 
total_df.head()
ts_utc.tz_convert('US/Eastern')
areas_dataframe = areas_dataframe.sort_values(by='rate', ascending=False) $ areas_dataframe = areas_dataframe.reset_index() $ areas_dataframe = areas_dataframe.drop('index', 1) $ areas_dataframe[areas_dataframe.rate >= .5]
old_conver_rate = df2.converted.mean() $ (old_conver_rate, new_conver_rate)
for v in data.values(): $     if 'Q3' not in v['answers']: $         v['answers']['Q3'] = ['Other']
condition = (users['cityOrState'].isna() == False) & (users['country'].isna() == False) $ users[condition].head()
df2['ab_page']=pd.get_dummies(df2['group'])['treatment'] $ df2.head()
my_house_price = 800000 $ estimated_squarefeet = inverse_regression_predictions(my_house_price, sqft_intercept, sqft_slope) $ print('The estimated squarefeet for a house worth {} is {:.2f}'.format(my_house_price, estimated_squarefeet))
air_store_3d7 = air_store[air_store['air_store_id'] == 'air_0f0cdeee6c9bf3d7'] $ air_store_april = air_store_3d7[air_store_3d7['visit_date'] >  '2017-04-01' ] $ air_store_april.head(10)
from sklearn.neighbors import KNeighborsClassifier $ k = 4 $ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train_knn,y_train_knn) $ neigh
z_score, p_value = sm.stats.proportions_ztest(count = [convert_new,convert_old], nobs = [n_new,n_old], alternative ='larger' ) $ print ("z_score:",z_score) $ print("p_value:",p_value)
%matplotlib notebook $ plt.plot([1,2,3,4]) $ plt.ylabel('some numbers') $ plt.show()
liberiaCases = liberiaCasesSuspected + liberiaCasesProbable + liberiaCasesConfirmed $ liberiaCases.head()
team_names.drop([6,24],inplace=True)  # Drop Knicks, Nets
train.head()
autos['ad_created'] = autos['ad_created'].str[:10].str.replace('-', '').astype(int) $ autos['ad_created'].head()
mw['page_ptr_id'] = pd.Series(range(1,189))
communities = community.best_partition(G)
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred_rf))
def replace_repaid(actual_cashflows,plan_cashflows): $     return pd.concat([actual_cashflows[actual_cashflows.payback_state!='payback_complete'], $                       plan_cashflows[plan_cashflows.payback_state=='payback_complete']],ignore_index=True)
fm = pd.merge(crime_df, weather_df, how='left', on=None, left_on='Date_Time', right_on='Date_ TimeCST', $       left_index=False, right_index=False, sort=True, $       suffixes=('_x', '_y'), copy=True, indicator=False)
test_probs = pd.DataFrame(columns=Y_valid_df.columns, index=test_df.index)
f_ip_app_minute_clicks.show(1)
train_df.head(2)
def f(x): $     return pd.Series([x.min(), x.max()], index=['min', 'max'])
yellow_3112_do = ytx_do $ yellow_3112_do.to_csv('data_pre_yellow/yellow_3112_do.csv')
(d < p_diffs).mean()
yhat = SVM.predict(X_test) $ yhat
datetime(-1200, 1, 1).toordinal()
server_response = urlopen(within_sakhalin_request_url) $ sakhalin_data_in_bbox = pd.DataFrame(json.loads(server_response.read().decode('utf-8'))['data'])
import os $ from geopy.geocoders import Nominatim $ geolocator = Nominatim() $ from geopy.exc import GeocoderTimedOut $ import time $
pd.to_datetime('14/04/2018')
count_by_source.plot.pie(fontsize=11, autopct='%.2f', figsize=(6, 6));
tren = pd.read_csv('../Datos Capital/estaciones-de-ferrocarril.csv',low_memory=False)
grouped_publications_by_author['countCollaborators'] = grouped_publications_by_author['authorCollaboratorIds'].apply(len)
df['day'] = '01' $ df['date']= df.apply(lambda x:datetime.strptime("{0} {1} {2}".format(x['year'],x['month'], x['day']), "%Y %m %d"),axis=1)
page.contributors()
m_df['interest_level_ishigh'] = (m_df.interest_level == 'high')*1 $ m_df['interest_level_islow'] = (m_df.interest_level == 'low')*1
df['created_at'].head()
train_view.sort_values(by=6, ascending=False)[0:10]
ocsvm_stemmed_bow.fit(trump_stemmed_bow, y = y_true_stemmed_bow) $ prediction_stemmed_bow = ocsvm_stemmed_bow.predict(test_stemmed_bow) $ prediction_stemmed_bow
new_page_converted = np.random.binomial(1, p_new, n_new) $ len(new_page_converted)
import matplotlib.pyplot as plt $ %matplotlib inline $ ax = delays_by_origin.plot.scatter('mean', 'count', alpha=.2) $
def clean_city_name(city): $     return city.decode('utf-8').lower().title() $ def gender_to_str(gender): $
ppm_body.token_count_pandas().head()
y = y.apply(lambda x: 0 if x <= 67 else 1)
json_data = r.json() $ json_data
MIN_TIME_LIMIT = min(min(overview_table["Dsmith time (hrs)"]), $                      min(overview_table["CLSmith time (hrs)"])) $ print(f"The minimum hours on a device is {MIN_TIME_LIMIT:.1f} hrs")
active_org_id = clean_users[clean_users['active']==1][['org_id','active']].groupby('org_id').count()
len([user_group for user_group in U_B_df.cameras if str(user_group) not in USER_PLANS_df.index])
print(train_data.gearbox.isnull().sum()) $ print(test_data.gearbox.isnull().sum())
pos_tags = pickle.load(open("pickle_files/pos_tags.p","rb"))
new_page_converted = np.random.choice([0,1], size=n_new, replace=True, p=[1-p_new,p_new]) $ np.bincount(new_page_converted)
blame.author = pd.Categorical(blame.author) $ blame.author = blame.author.str.replace(",", ";") $ blame.timestamp = pd.Categorical(blame.timestamp) $ blame.timestamp = pd.to_datetime(blame.timestamp) $ blame.info()
df2 = df2.join(countries_df.set_index('user_id'),on='user_id')
data = pd.read_csv('pop_hot_raw.csv') $ data.head()
import os $ os.makedirs(PATH) $ os.chdir(PATH) $ !wget http://files.fast.ai/part2/lesson14/rossmann.tgz $
nyse_filter = exchange.eq('NYS') $ nyse_filter
df.isnull().sum()
articles.head()
group_techniques = lift.get_techniques_used_by_group('APT12')
results.home_score.rank(ascending=False)
df_image_tweet.head()
df['Forecast'] = np.nan
page_interaction_result.summary()
print('Slope: Efthymiou vs experiment: {:0.2f}'.format(popt_ipb_brace_crown[2][0])) $ perr = np.sqrt(np.diag(pcov_ipb_brace_crown[2]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df[df.index.month.isin([8,9,10])]['Complaint Type'].value_counts().head(10).plot(kind='bar', color='yellow', figsize=(18,5)) $ df[df.index.month.isin([12,1,2])]['Complaint Type'].value_counts().head(10).plot(kind='bar', figsize=(18,5), alpha=0.5)
for col_name in ['project_id','category','country','location_type','usd_goal']: $     if col_name in df_fail_success.columns: $             print('removed column {}'.format(col_name)) $             df_fail_success=df_fail_success.drop([col_name], axis=1)
ppm_title = preProcessor_in_memory(hueristic_pct=.99, append_indicators=True, padding='post', keep_n=4000, maxlen=12) $ vectorized_title = ppm_title.fit_transform(data_to_clean_title)
dateCounts = df['Created Date'].value_counts().reset_index() $ dateCounts.columns = ['dates','countsOnDate'] $ dateCounts.head()
p_new_null = df.converted.mean() $ p_new_null
p_new = df2['converted'].mean() $ print(p_new)
graph = tf.get_default_graph() $ x = graph.get_tensor_by_name("input_seq:0") $ seqlen = graph.get_tensor_by_name("input_len:0") $ pred_prob = graph.get_tensor_by_name("pred_prob:0")
emails_dataframe['institution'] = emails_dataframe['address'].str.split("@").str.get(1) $ emails_dataframe
pred_probas_under_fm = gs_from_model_under.predict_proba(X_test) $ fm_bet_under = [x[1] > .62 for x in pred_probas_under_fm]
notus.loc[notus['country'] == 'Canada', 'cityOrState'].value_counts()
print(len(model[model.wv.index2word[0]]))
df_t1 = df.query('landing_page == "new_page"').query('group == "treatment"') $ df_t2 = df.query('landing_page == "old_page"').query('group == "control"') $ df2 = df_t1.append(df_t2, ignore_index=True)
Google_stock.isnull()
plt.show()
server = ENCODED('www.encodeproject.org') $ spreadsheet_name = os.path.expanduser('~diane/woldlab/ENCODE/C1-mouse-forlimb-submission-201804.ods') $ server.load_netrc() $ validator = DCCValidator(server)
df['Lat'].value_counts(ascending=False).head(5)
education_data.drop(list(range(539, 546)), inplace=True)
writers.groupby("Country").all()
import pyspark $ sc = pyspark.SparkContext.getOrCreate() $ sc $
p = getpass.getpass() $ r = requests.get('https://api.github.com/user', auth=('rsouza', p)) $ r.status_code
string.punctuation
liberiaDfOld = liberiaDf.copy()
dictionary["a"]="..." $ print(dictionary)
mb.ix['Proteobacteria']
daily_averages.head()
sqlContext.sql("select * from example2").toPandas()
df_actor.head()
tables = pd.read_html(url) $ tables
offset.rollforward(d)
baseball.corr()
twitter_archive_master['doggo'] = (twitter_archive_master['full_text'].str.extract('([Dd]ogg?o?)').astype(str) != 'nan').astype(int) $ twitter_archive_master['puppo'] = (twitter_archive_master['full_text'].str.extract('([Pp]uppo)').astype(str) != 'nan').astype(int) $ twitter_archive_master['floofer'] = (twitter_archive_master['full_text'].str.extract('([Ff]loofe?r?)').astype(str) != 'nan').astype(int) $ twitter_archive_master['pupper'] = (twitter_archive_master['full_text'].str.extract('(pupper|[Pp]up+[^\w]|[Pp]upp[^o])').astype(str) != 'nan').astype(int)
print 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))
len(fraud_data.user_id.unique()) == len(fraud_data)
set(user_df.columns).intersection(stories.columns)
col_drop = ['id', 'athlete_count', 'avg_speed', 'type', 'latlng', 'avg_hr', 'pace'] $ zone_train = zone_train.drop(col_drop, axis=1) $ zone_test = zone_test.drop(col_drop, axis=1) $ zone_train.head()
to_be_predicted_Day5 = 17.67853523 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
lead_classes.fillna(0,inplace = True) $ lead_classes[lead_classes == 't'] = 1
column_check = inspector.get_columns('station') $ for check in column_check: $     print(check['name'],check['type'])
train_data.vehicleType.fillna('kleinwagen', inplace = True)
messages_with_dates_ext.to_csv("messages.csv")
result = api.search(q='%23data')
geometry = openmc.Geometry(root_universe)
or_list = df.query("group=='treatment'| landing_page=='new_page'").index
first_result.find('strong').text[0:-1] + ', 2017'
df2_control = df2[df2['group'] == 'control']['converted'].mean() $ print('The probability that an individual in the control group converted is: {}'.format(round(df2_control, 4)))
unique_users_count = df['user_id'].nunique() $ print(unique_users_count)
from scipy.stats import norm $ norm.ppf(1-(0.05/2))
x.body
pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
print(parser.HHParser.boundaries)
rmse_ebay=np.sqrt(mean_squared_error(df['diff_log_EBAY'][1:-1],predEbay))
n_rows, n_columns = df.shape $ print(f'There are {n_rows:,} rows and {n_columns} columns in this data.') $ data_to_clean_body = df.body.tolist() $ data_to_clean_title = df.issue_title.tolist() $ df.head()
deaths_per_year = defaultdict(int) $ for row in df_valid["Died"]: $     deaths_per_year[row.year] += 1 $ deaths_sorted = OrderedDict(sorted(deaths_per_year.items()))
merged_portfolio_sp['Equiv SP Shares'] = merged_portfolio_sp['Cost Basis'] / merged_portfolio_sp['SP 500 Initial Close'] $ merged_portfolio_sp.head()
old_page_converted=np.random.choice([1,0],size=n_old,p=[p_old,1-p_old]) $ old_page_converted.mean()
df_new.groupby(['country', 'group'], as_index =False).count()[['country','group', 'landing_page']]
df.take(1)
E_grid = np.zeros((100, 100)) $ for i in range(100): $     for j in range(100): $         E_grid[i, j] = ((y - m_grid[i, j]*x - c_grid[i, j])**2).sum()
T = 0 $ folder = 'trainW-'+str(T) $ train = pd.read_csv('../../input/preprocessed_data/trainW-{0}.csv'.format(T))[['msno']] $
obj.drop(['d', 'c'])
jobs_data.drop_duplicates(subset=['record.company_name','clean_titles'], keep='first', inplace=True)
!less 'data/taxi+_zone_lookup.csv'
model.wv.most_similar("cantonese")
df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
df_treat= df2[df2['group'] == 'treatment']['converted'].mean() $ print("{} is the probability they converted.Thus, given that an individual was in the treatment group.".format(df_treat))
df.converted.sum()/df.converted.count()
data[['Sales']].resample('D').mean().rolling(window=15).mean().plot()
df['converted'].value_counts()[1]/len(df)
df2['landing_page'].value_counts()["new_page"]/len(df2)
ideas.dtypes  # Inspecting progress
temp_df.groupby('reorder_interval_group')['Order_Qty'].mean()
df["text"] = df["text"].apply(lambda x: normalise(x).replace("\n", " "))
precip_data = session.query(Measurements).first() $ precip_data.__dict__
treatment_convert = df2.query('group =="treatment"').converted.mean() $ print("Probability of treatment group converting is :", treatment_convert)
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X' $ r = requests.get(url) $ data = r.json()
prng = pd.period_range('1/1/2011', '1/1/2012', freq='M')
newdf.head(12)
stock_ids = soup.select('a[href*="/item/main.nhn"]')
mentioned_bills_all['last_vote'] = pd.to_datetime(mentioned_bills_all['last_vote'], format = '%Y-%m-%d') $ mentioned_bills_all['votes_date'] = pd.to_datetime(mentioned_bills_all['votes_date'], format = '%Y-%m-%d')
df_providers[['id_num','name','year','drg3','discharges','discharge_rank',\ $               'disc_times_pay','payment_rank']].tail()
df2.rename( columns= { "comments.summary.total_count" : "total_comments"} , inplace=True) $ df2.rename( columns= { "likes.summary.total_count" : "total_likes"} , inplace=True)
accuracy_eval = pipeline.predict(X_eval) $ accuracy = np.mean(np.array(accuracy_eval['PredictedLabel'].astype(int)) == y_eval) $ print('evaluation accuracy: ' + str(accuracy))
from pyspark.sql import Row $ test_df = spark.createDataFrame(sc.range(180).map(lambda x: Row(degrees=x)), ['degrees']) $ sin_rad = functions.sin(functions.radians(test_df.degrees)) $ test_df.select(sin_rad).show()
targetUserItemInt=targetUserItemInt.join(targetUsersRank['norm_rank'],on='user_id') $ print targetUserItemInt.shape $ targetUserItemInt.head()
retweets=twitter_archive[~twitter_archive.retweeted_status_id.isnull()].tweet_id.values $
knn.fit(train[['property_type', 'lat', 'lon','surface_covered_in_m2','surface_total_in_m2']], train['floor'])
df_transactions['membership_duration']  = df_transactions['membership_duration']  / np.timedelta64(1, 'D')
sort_a_desc = noNulls['a'].desc()
df.drop(df.query("group =='treatment' and landing_page =='old_page'").index, inplace = True) $ df.drop(df.query("group =='control' and landing_page =='new_page'").index, inplace =True)
auth = {'AuthMethod' : 'password', $         'Username' : account, $         'AuthString' : password} $ import xmlrpc.client $ url = "https://r2labapi.inria.fr:443/PLCAPI/"
x_dict = {'text':X.values, 'joined_text':new_values}
total_sales['2015_q1_sales_bottlenorm'] = total_sales["2015_q1_sales"].values/total_sales.bottles_total.values $ total_sales['2016_q1_sales_bottlenorm'] = total_sales['2016_q1_sales'].values/total_sales.bottles_total.values
extractor = twitter_setup() $ sname = "realDonaldTrump" $ tweets = extractor.user_timeline(screen_name=sname, count = 200) $ print("Tweets Extracted: {}.\n".format(len(tweets)))
repl = r"notus['country'] = notus['country'].str.replace(" $ repl
df.boxplot('MeanFlow_cms');
y = op_ed_articles['byline'] $ X = op_ed_articles.drop(['byline','subjects'],axis=1) $ from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.25) $
df_test_index_2 = event_list $ df_test_index_2 = pd.merge(df_test_index_2[event_list['event_start_at'] > df_test_user_2['created_on']], $                             log_user2[event_list['event_start_at'] > df_test_user_2['created_on']], on='event_id', how='left') $ df_test_index_2
with pd.option_context('max.rows', 15): $     print(result["Pass"].sort_values(ascending=False))
df2[['CA', 'UK', 'US']] = pd.get_dummies(df2['country']) $ df2.drop(['country', 'US'], axis = 1, inplace = True) $ df2.head()
ga = pd.read_sql_query('select * from "events_gas"',con=engine)
corTable['Variable1'].unique()
crimes.groupby('BEAT').mean()
grouped_authors_by_publication.rename(columns = {'authorName':'authorNames_in_given_publication'}, inplace = True) $ grouped_authors_by_publication.rename(columns = {'authorId':'authorIds_in_given_publication'}, inplace = True)
np.exp(0.0099)
print("Test After", test['StateHoliday'].unique()) $ test.replace({"StateHoliday": {"0": 0}}, inplace=True) $ print("Test After", test['StateHoliday'].unique())
[name.endswith('bacteria') for name in bacteria.index]
%matplotlib notebook $ sns.set_style('white')
review_title = review_df.loc[:, 'review_title'] $ review_body = review_df.loc[:, 'review_body']
ioDF.shape
sample_autos=autos.sample(n=10000, random_state=1)
df.head()
obj = pd.Series(range(3), index=['a', 'b', 'c'])
old_page_converted = np.random.choice([1,0],size=n_old,p=[p_old,(1-p_old)]) $ old_page_converted.mean()
user.is_enabled = user.is_enabled.astype(int)
bnbAx.country_destination.value_counts() $
train_dum.shape
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(columns=['control'], inplace=True) $ df2.head()
titanic[filter.ticket & filter.cabin].head()
tipsDF.shape
wnl = nltk.WordNetLemmatizer() $ print([wnl.lemmatize(t)  for t in tokens[600:650]])
injuries_hour.columns
df.groupby('category')['position','hourly_rate','num_completed_tasks'].agg({'median','mean','min','max'}).reset_index() \ $     .rename(columns={'category': 'category', 'position': 'position_stats_overall','hourly_rate':'hourly_rate_stats_overall', \ $                  'num_completed_tasks':'num_completed_tasks_stats_overall'})
reddit.head()
plt.figure(figsize=(12,10)) $ plt.plot(cluster_label_group_1.ix[:,0:21].T) $ plt.title("Cluster Centers Over Time: Gaussian Mixture") $ plt.legend([0,1,2])
print('Merchants with more than one e-shop') $ print(data['Merchant_ID'].duplicated().sum())
monthly_gain_summary.head(10)
dfNYC.tail()
df2.drop(146212,axis=0,inplace=True);
def get_list_userID_profile_pic(the_posts): $     list_userID_profile_pic = [] $     for i in list_Media_ID: $         list_userID_profile_pic.append(the_posts[i]['shortcode_media']['owner']['profile_pic_url']) $     return list_userID_profile_pic
autos['registration_year'].describe()
autos.head()
df[['Match','Exact']].describe()
df.to_csv('./movie_data.csv', index=False) $ df = pd.read_csv('./movie_data.csv') $ df.head(3) # (notice reindexing)
df = pd.DataFrame(bmp_series, columns=['mean_price']) $ df
final.to_csv('tweets.csv', index=False) $ tweetsPerDay.to_csv('tweetsPerDay.csv', index=False)
city_eco["economy"] = city_eco["eco_code"].astype('category') $ city_eco
grouped2 = df_cod3.groupby(["Death year", "Cause of death"]) $ grouped2.size()
tweet_json_clean['id'] = tweet_json_clean['id'].astype('str')
logit_new = sm.Logit(df_new['converted'],df_new[['intercept','CA','US','treatment']]) $ results_new = logit_new.fit() $ results_new.summary()
os.environ["s3_key"] = "s3://wri-public-data/" + s3_key_merge $ os.environ["gs_key"] = "gs://resource-watch-public/" + s3_key_merge $ !gsutil cp $s3_key $gs_key
from bs4 import BeautifulSoup $ soup = BeautifulSoup(response.text, 'html')
y_train = np.array(test_df['Log_Price_Sqft'][mask]) $ y_test = np.array(test_df['Log_Price_Sqft'][~mask]) $ print len(y_train),len(y_test) $ print y_test
grouped_by_year_DRG_max.tail()
pd.Series({'a' : 0., 'c' : 1., 'b' : 2.},index = ['a','b','c','d'])  # from Python Dict, index specified, no auto sort
tweets.head()
np.sqrt(information_ratio)
pd.read_csv("./parsed_tweets.csv")
df_a.join(df_b, how = "left") # same as above (default)
temp_series_paris_naive = temp_series_paris.tz_localize(None) $ temp_series_paris_naive
baseball_newind.query('ab > @min_ab')
np.datetime64('2015-07-04 12:59:59.50', 'ns')
flight6 = spark.read.parquet("/home/ubuntu/parquet/flight6.parquet") $
new_page_converted = np.random.binomial(1,p_new, n_new) $ new_page_converted
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
result_set.DataFrame()
model.most_similar("awful")
forked = forked[forked.repo_id != 0] $ forked.info() $ forked.head()
codes.head()
df_p3.loc[df_p3["CustID"].isin([customer])]
print("pulling historical weather from a single zip code") $ weather_json = skafos.engine.query("SELECT * from weather_noaa WHERE zipcode = 23250").result()
daily_df.reset_index(inplace=True)
elms_all_0611.ORIG_DATE.max()
pd.read_csv
archive_clean.info()
lm2 = sm.Logit(df_new['converted'],df_new[['ab_page','US','UK']]) $ results2=lm2.fit() $ results.summary()
df2[df2.duplicated('user_id', keep=False) == True]
tweets.head()
import sys $ sys.path.append('..') $ from utils.bikeshare import download_bikeshare_data $ download_bikeshare_data(2016, 1, '../data/')
df['created_at'] = pd.to_datetime(df.created_at)
df3[df3['group']=='treatment'].head()
def findthreshold(fpr,tpr,thresholds): $     i = np.arange(len(tpr)) # index for df $     roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)}) $     return roc.ix[(roc.tf-0).abs().argsort()[:1]].reset_index()['thresholds'][0]
WholeDfNonNULL =WholeDf[WholeDf.Rating == WholeDf.Rating]
cols = data_df.columns.tolist() $ cols = cols[-1:] + cols[:-1] $ cols
for file in s3_key_origs: $     os.environ["s3_key"] = "s3://wri-public-data/" + file $     os.environ["gs_key"] = "gs://resource-watch-public/" + file $     !gsutil cp $s3_key $gs_key
df_potholes = df[df['Descriptor'] == 'Pothole'] $ df_potholes.groupby(df_potholes.index.weekday).apply(lambda x: len(x)).plot()
df.drop_duplicates(subset=['last_name'], keep='last') $
rspmcum = rspm.cumsum() $ rspmcum.plot()
data['rooms'] = data['rooms'].apply(lambda x: (float)((int)(x)))
df_campaigns['Send Date'] = pd.to_datetime(df_campaigns['Send Date'])
weekly.plot()
melted_total = melted_total.dropna(subset=['Categories'])
col_arr=df["WHO Region"].values $ col_arr
npath = save_filepath+'/pysumma/sopron_2018_notebooks/pySUMMA_Demo_Example_Fig8_right_Using_TestCase_from_Hydroshare.ipynb' $ hs.addContentToExistingResource(resource_id, [npath])
subs_and_comments = subs_and_comments.merge(op_add_comms, on='id', how='outer')
index_list = [] $ for x,y in zip(df_2.index, df_2.title): $     if y not in index_list: $
alldata = pd.concat([malenewnew, femalenew], axis=1) $ alldata.head(10)
ks_goals = ks_projects.groupby(["goal", "state"]).size().reset_index(name='counts') $ ks_goal_success = ks_goals.drop(ks_goals.index[ks_goals.state != 'successful']) $ ks_goal_success = ks_goal_success.drop(ks_goal_success.index[ks_goal_success.counts < 300]) $ ks_goal_success = ks_goal_success.sort_values(by = ['goal'], ascending = True) $ ks_goal_success.set_index('goal', inplace=True)
cars= cars[(cars.price >= 500) & (cars.price <=160000) & (cars.yearOfRegistration >=1950) &(cars.yearOfRegistration <=2016) & (cars.powerPS >=10) & (cars.powerPS<=500)] $ cars.info()
st = dt.datetime(1982, 1, 1)
os.environ['PROJECT'] = PROJECT $ os.environ['BUCKET'] = BUCKET $ os.environ['REGION'] = REGION $ os.environ['REPO'] = REPO
import matplotlib.pyplot as plt $ from matplotlib.ticker import FuncFormatter $ import pandas as pd $ import numpy as np $ import os
api_copy.info()
s.get(2) # ---> Returns '12'
df_new.head()
!wc -l < /home/denton/Downloads/2013_311_Service_Requests.csv # Number of lines in dataset
df2.user_id.duplicated().sum()
volume_m.shape
def brandNumber(brand): $     return brand, len(data[data.brand==brand])
hate_pos = df_hate[df_hate['Polarity'] >= 0] $ hate_pos['Polarity'].count()
sconf = SparkConf() \ $     .setAppName("Word Count") \ $     .set("spark.ui.port","4141") $ sc = SparkContext(conf=sconf)
df.info()
X_cat.fillna(0, inplace=True)
a=contractor_clean.groupby(['contractor_id','contractor_bus_name'])['contractor_bus_name'].nunique() $ print a[a >1] $ contractor_clean[contractor_clean.contractor_number.duplicated() == True]
(combined.to_csv('../map/zika_places.csv', $             sep=',', $             index=False, $             date_format='%Y-%m-%d %H:%M:%S')) $ ! head ../map/zika_places.csv
start = datetime.now() $ modelrf1k = RandomForestClassifier(n_estimators=1000, n_jobs=-1) $ modelrf1k.fit(Xtr.toarray(), ytr) $ print(modelrf1k.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret)
data3.columns.values
p_new = new_page_converted.sum()/len(new_page_converted) $ p_old = old_page_converted.sum()/len(old_page_converted) $ print("p_new - p_old = ",p_new - p_old) $ answer_of_g = p_new - p_old # used in j part
predictions = final_pipeline.predict_proba(df_target.text.values) $ df_target['BlockChainPredictions'] = [j for i,j in predictions] $
ans = pd.pivot_table(df, values='E', index=['A','B'], columns=['C']) $ ans
df.mean() # same as df.mean(0)
df = tables[0] $ df.columns = ['Parameters', 'Values'] $ df.head()
df['Ranking Full URL on Sep  1, 2017'] = df['Ranking Full URL on Sep  1, 2017'].fillna('')
mod2 = sm.Logit(final_df['converted'], final_df[['ab_page','country_CA','country_UK','intercept']]) $ fit2 = mod2.fit() $ fit2.summary()
train_data = data1[data1['dataset'] == 'train'] $ train_data = train_data.drop('dataset', axis = 1) $ train_data.sample(3)
corpus = [dictionary.doc2bow(text) for text in texts] $ corpus
archive_copy['name'].loc[archive_copy['name'].str.islower()].count()
client.repository.list_models()
tp = np.dtype([('id', 'i8'), ('mat', 'f8', (3, 3))]) $ X = np.zeros(1, dtype=tp) $ print(X[0]) $ print(X['mat'][0])
%matplotlib inline $ import matplotlib.pyplot as plt $ import numpy as np
movie_ratings.loc[:, 'year'] = movie_ratings['year'].str[-5:-1].astype(int)
df_json_tweet['tweet_id'] = df_json_tweet['tweet_id'].astype('int64') $ df_json_tweet.info()
df2.groupby('group').group.value_counts() $
df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis=1) $ df2.head()
(autos["ad_created"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().tail(10) $ )
dta.loc[(dta.risk == "Risk 1 (High)") | (dta.risk == "Risk 1 (Medium)")].head()
for row in df2.iterrows(): $     row = row[1] $     if row['Chernoff_Bound'] < 0.499 or row['Chernoff_Bound'] > 0.5: $         print(row['ra'], row['rb'], row['Chernoff_Bound'])
df = df.sort_values(by=['date']) $ df = df.rename(columns={'prcp':'precipitation'}) $ df.head(10)
s.str.len()
lr = 1e-2 $ learn.fit(lr, 20, cycle_len=1, use_clr=(10,10))
final_data.groupby('rating').agg(['count'])
print("Average item-level delta recall: {:.2f}%".format(np.nanmean(np.array(normalized_delta_recall)))) $ print("Biggest item-level delta recall: {:.2f}%".format(max(normalized_delta_recall))) $ print("Biggest adverse item-level delta recall: {:.2f}%".format(min(normalized_delta_recall)))
def strip_singlequote_and_dateformat2datetime(text): $     try: $         return  pd.to_datetime(text.strip('\''),format='%d-%m-%Y %H:%M') $     except AttributeError: $         return text 
scaler_M7 = StandardScaler().fit(training_X)
commits = EQCC(git_index) $ commits.since(start=start_date).until(end=end_date) $ commits.get_cardinality("author_uuid").by_period() $ print(pd.DataFrame(commits.get_ts()))
df_archive_clean["name"].value_counts()[0:15]
df['positive_tokens'] = df['body_tokens'].apply(lambda x: len([word for word in x if word in positive_words])) $ df['negative_tokens'] = df['body_tokens'].apply(lambda x: len([word for word in x if word in negative_words])) $ print(df[['token_count', 'positive_tokens', 'negative_tokens']])
df = df.groupby('msno').apply(make_order_number) $ df
iris.plot(x="Petal.Length",y="Sepal.Width", kind="scatter", c=np.array(["black", "cyan", "orange"]))
k1.head()
scoring_data = {'values': [x1.tolist(), x2.tolist()]}
users= pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv') $ sessions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv') $ products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv') $ transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv')
random_series.cumsum().plot()
dd_df['fuelType'].fillna('fueltype_unknown', inplace=True)
df2.head()
df3.groupby(df3.index).median()
t = pd.DataFrame(t.groupby(['Date', 'location'])['tweetCount'].sum()) $ t.to_csv('count.csv')
total.iloc[-3:]
d = datetime.date(1492, 10, 12) $ d.strftime('%A')
index = pd.date_range('2018-01-01 20:30:40', periods=10, freq='2S') $ index
logit_mod = sm.Logit(df_joined['converted'],df_joined[['intercept','ab_page','UK','CA']]) $ results_3 = logit_mod.fit() $ results_3.summary()
loans_df.earliest_cr_line = pd.to_datetime(loans_df.earliest_cr_line) $ loans_df.earliest_cr_line.value_counts()
p_diffs = [] $ for _ in range(int(1e4)): $     p_n_s = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new]).mean() $     p_o_s = np.random.choice([0, 1], size=n_old, p=[1-p_old, p_old]).mean() $     p_diffs.append(p_n_s - p_o_s) $
sample_index = honeypot_df['src'].sample(10,random_state = 0).reset_index(drop = True) $ sample_regular = regular_traffic['id.orig_h'].sample(10, random_state = 0).reset_index(drop = True) $ print(sample_index) $ print(sample_regular)
df_parsed.country.value_counts()
new_page = df2.query('landing_page == "new_page"')['user_id'].count() $ total = df2.shape[0] $ new_page_prob = new_page / total $ new_page_prob 
print(ha.accounts.credit_subaccount.__init__.__doc__)
affordability['Ratio'] = affordability.Median_Sales_Price/affordability.Median_HH_Income $ affordability
from src.image_manager import ImageManager $
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(new_df, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
tfav_k2.plot(figsize=(16,4), label="Likes", legend=True) $ tret_k2.plot(figsize=(16,4), label="Retweets", legend=True);
df.text[0]
sns.barplot(x=sentiment_avg_pd['Media Source'], y = sentiment_avg_pd.Compound, $             ) $ plt.title("Overall Media Sentiment based on Twitter (%s)" %datetime.date.today()) $ plt.show() $
sns.boxplot(score["score"])
group_sizes.set_index('variation_group', inplace=True)
unsegmented_users.groupby(['segment']).user_id.count()
payload_online = {"name": "Product Line Prediction", "description": "My Cool Deployment", "type": "online"} $ response_online = requests.post(endpoint_deployments, json=payload_online, headers=header) $ print response_online $ print response_online.text
ca_de.shape
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05/2)))
print("Converted users proportion is "+str(df['converted'].mean()*100))
noise = pd.read_csv('ny_cb_shp_w_noise_counts')
first_words = my_data.map(lambda line: line.split()[0])
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key='+API_KEY\ $ +'&start_date=2017-01-01&end_date=2017-12-31' $ r = requests.get(url)
type.__call__(MetaClass,'Example', (), {})
df.resample('M').count()
business_df['prop']=business_df['prop'].apply(str) $ business_df['prop'].head()
glm_multi_v1.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)
twitter_archive_clean=twitter_archive.copy() #creating copy of each data set. $ image_predictions_clean=image_predictions.copy() $ tweet_scores_clean=tweet_scores.copy()
btc_wallet.plot(kind='line',x='Timestamp',y='BTC_balance_GBP', grid=True);
df.to_pickle("clean_df.pkl")
NB_results['sentiment'].value_counts().plot(kind='pie', title="Distribution of Colombian Election Twitter Sentiment", figsize=(6,6))
tweets_clean.drop(columns = ['pupper', 'doggo', 'puppo', 'source'], inplace=True) $ tweets_clean.info()
from IPython.display import Image
df_master.keys()
plot.save(filename='winvisualization.png',dpi=500)
temp_series_freq_2H = temp_series.resample("2H").min() $ temp_series_freq_2H
gmap.draw('incidents3.html')
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df_place.head()
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'),how='inner')
tt4Dict.head()
train_users_non_en = train_users[train_users.language != "en"]
lims_df.columns
df_filtered = df[df["Market"].str.contains("GB-en")==False | $                  df["Device"].str.contains("desktop")==True]
df.count()
brands = autos["brand"].value_counts(normalize=True).head(9) $ brands = brands.index $ brands
S_1dRichards.decision_obj.simulStart.value, S_1dRichards.decision_obj.simulFinsh.value
data.loc[data.surface_covered_in_m2 > data.surface_total_in_m2, 'surface_covered_in_m2'] = np.NaN
cohort_activated_df = pd.DataFrame(index=daterange,columns=daterange)
intersections.head(1)
df_comb.head()
fulldata_copy['Returns'] = np.log(fulldata_copy['AUDUSD'] / fulldata_copy['AUDUSD'].shift(1))
archive_clean.drop(archive_clean[archive_clean['retweeted_status_id'].notnull() == True].index, inplace = True)
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05)))
dft.columns.name = 'date' $ dft.index.name = 'hero'
test_.head()
with pd.option_context('display.max_rows', 150): $     print(news_period_df.groupby(['news_collected_time']).size())
del df['index'] $ print 'Number of nulls in the data set: \n', df.isnull().sum(),'\n' $ df.head()
data = pd.read_csv('data_science_challenge_samp_18.csv',parse_dates=[1],keep_date_col = True)
df[df.topicmax == 2].subreddit.value_counts()[:10]
X_test = test_pixelData.reshape((test_pixelData.shape[0], 28, 28)) $ print ("X_test.shape:", X_test.shape)
df_train.corr()
recommendations = model.recommendProducts(2093760, 5) $ recArtist = set([r[1] for r in recommendations])
for key,value in deaths_sorted.items(): $     print( "Deaths in " + str(key) + " = " + str(value))
btime3.std1()
from pysumma.Simulation import Simulation
s.iloc[[1,3]] 
autos["ad_created"].str[:10].value_counts(normalize=True,dropna=False).sort_index(ascending=True)
p_new_null = df2.converted.mean() $ p_new_null
pred = session.run(Ypred, feed_dict={X:x_data}) $ plt.plot(x_data, pred) $ plt.plot(x_data, y_data, 'ro') $ plt.xlabel("# Chirps per 15 sec") $ plt.ylabel("Temp in Farenhiet") $
old_page_converted = np.random.binomial(n_old, p_old) $ print(old_page_converted)
pd.Timestamp('9/2/2016 8:10AM') + pd.Timedelta('12D 3H')
xt = np.arange(0,24) $ counts.plot(xticks=xt)
import tensorflow as tf $ sess=tf.Session()    $ saver = tf.train.import_meta_graph('/tmp/testing/stock_prediction_12_21/model.ckpt-1000.meta') $
factor = 1.0/sum(country_dict.values()) $ weights = dict() $ for elem in country_dict: $     weights[elem] = country_dict[elem] * factor * 10 $
logistic_model2=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']])
avg_per_seat_price_inoroffseason_teams = inoroffseason_teams["Per Seat Price"].mean() $ avg_per_seat_price_inoroffseason_teams # Acts as a benchmark for how expensive a team's PSLs are during in or off-season.
malenewnew  = malenew[['Male']].copy() $ malenewnew.head(3) $
ben_scores = ben_final.loc[:,['userid','stiki_score','cluebotRevert']] $ van_scores = van_final.loc[:,['userid','stiki_score','cluebotRevert']]
job_requirements.shape $
agg_trips_data = trips_data.groupby('start_station_id').agg({'id':'count','duration':['mean','sum']}).reset_index() # aggregatig data $ agg_trips_data.columns= ['start_station_id','count','mean_duration','total_duration'] #to overwrite multi-column indexe with single index
sydney_friends['userFriendsCt'].plot(kind = 'bar') $ plt.title('Sydney Friends') $ plt.xlabel('Time Zone') $ plt.ylabel('Number of Friends') $ plt.show()
import pandas as pd $ row_df = pd.DataFrame(rows) $ row_df.head() $ row_df.to_excel("comparison.xls")
X.shape
df.groupby('Store Number').describe()
import statsmodels.api as sm $ logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result=logit.fit()
paired_df_grouped = paired_df.groupby('dataset_1')['dataset_2', 'co_occurence'].agg(lambda x: list(x)) $ paired_df_grouped.reset_index(inplace=True) $ paired_df_grouped['all_co_occurence'] = paired_df_grouped.apply(lambda x: tuple(zip(x.dataset_2, x.co_occurence)), axis=1) $ paired_df_grouped.head(2)
nodes_lst = [] $ for x,y,z,aa in zip(most_followed_tweeters.screen_name, location_india, normalized_followers, most_followed_tweeters.followers): $      nodes_lst.append({"id":x, "group":y, "value":int(z), "followers":int(aa)}) $ for x,y,z in zip(retweet_lst_names,fols_values, retweet_fol): $      nodes_lst.append({"id":str(x), "group":2, "value":int(y), "followers":int(z)}) $
noise_data_2 = noise_data.join(hour) $ noise_data_2.head()
f_ip_os_minute_clicks.show(1)
selected_leases.sort(key=lambda lease: lease['t_from'])
health_data_row.loc[2013, 1, 'Guido']  # index triplet
GSW_2017["Tm.3PA"].mean()
try: $     redirect_url = auth.get_authorization_url() $ except tweepy.TweepError: $     print('Error! Failed to get request token.')
wrd_api_clean.head(3)
twitter_archive_master=prediction_and_counts.merge(twitter_archive_clean, how='inner',on='tweet_id')
df[3] = np.nan $ df
e=data.iloc[2]
pct_outliers = len(pax_raw[pax_raw.paxstep>step_threshold]) / len(pax_raw)*100 $ print(f'Percent of minutes with > {step_threshold} steps: {pct_outliers}')
contractor_clean['address1'] = contractor_clean['address1'] .map(lambda x: x.lstrip('Attn:')) $ contractor_clean['address1'] =contractor_clean['address1'].str.replace('-', ' ') $ contractor_clean['address2'] =contractor_clean['address2'].str.replace('-', ' ')
mu1, sd1 = np.mean(feature1), np.std(feature1) $ mu2, sd2 = np.mean(feature2), np.std(feature2) $ mu3, sd3 = np.mean(feature3), np.std(feature3) $ print sd1, sd2, sd3
xmlData.drop('county', axis = 1, inplace = True)
commons_site = pb.Site("commons", "commons")
sqldmh = dmh.SQLTable('iris', eng)
df = df[(df.age >= 18) & (df.age <= 105)] $ df.head(2)
image_labels = (loadmat('imagelabels.mat')['labels'] - 1).tolist()[0]
futures_data.info()
bitc = bitc[np.isfinite(bitc['w_sentiment_score'])] $ bitc.info()
cityID = 'd98e7ce217ade2c5' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Stockton.append(tweet) 
dfA_stn.to_csv('dfA_stn.csv')
print(sorted(df_mes['payment_type'].unique()))
n_new = df2_treatment.shape[0] $ n_new
url ="https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ response = requests.get(url)
df_goog['good_month'] = df_goog['Close'] > df_goog['Open'] $ df_goog.head()
df = spark.read.parquet('/datasets/twitter_hashtags')
imin = amount.idxmin()
prop_conv = df['converted'].mean() $ output = round(prop_conv, 4) $ print("The proportion of Converted users is {}%".format(output*100))
lm = sm.OLS(df3['converted'], df3[['intercept', 'UK', 'US']]) $ results = lm.fit() $ results.summary()
df2 = df[['id_partner', 'name', 'Amount']].groupby(['id_partner', 'name'], as_index=False).sum()
[1] # <-- this syntax should be used when converting to a list. $
people = pd.DataFrame(np.random.randn(5, 5), columns=['a', 'b', 'c', 'd', 'e'], index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis']) $ people
frame.sort_index()
df.query('group == "treatment"')['converted'].mean()
print('if we spend 50k on TV ads, we predict to sell:', round(lm.predict(X_new)[0],2), ' thousand units')
session.query(Stations.station).count()
import matplotlib.pyplot as plt $ df.drop('Volume', axis=1).plot(figsize=(10,7))
tips_sentiment.head()
female_journalists_retweet_summary_df = journalists_retweet_summary_df[journalists_retweet_summary_df.gender == 'F'] $ female_journalists_retweet_summary_df.to_csv('output/female_journalists_retweeted_by_journalists.csv') $ female_journalists_retweet_summary_df[journalist_retweet_summary_fields].head(25)
data_archie.isnull().sum()
model_uid='training-WWmHAB5ig' $ saved_model_details = client.repository.store_model(model_uid, {'name': 'Keras LSTM model'})
rspmhigh['state'].value_counts()
np.empty((1,4))
a.string[a.start(): a.end()]
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = len(df2.loc[(df2['landing_page'] == 'old_page')]) $ n_new = len(df2.loc[(df2['landing_page'] == 'new_page')])
from nltk.stem.wordnet import WordNetLemmatizer $ lemmed = [WordNetLemmatizer().lemmatize(w) for w in words] $ print(lemmed)
nonc =  df[df['converted']==0].converted.count() $ nonc
keyword = 'Bitcoin' $ pytrend = TrendReq(hl='de-CH') $ pytrend.build_payload(kw_list=[keyword], timeframe='today 1-m', geo='CH', gprop='news') $ interest_over_time_df = pytrend.interest_over_time() $ print(interest_over_time_df.head())
result3 = result3.reset_index() $ result3.head(20)
abc['xxx'] = abc['xy'] +  abc['ab'] +  abc['dc'] $ abc = abc.sort_values(['sepsis_overall'], ascending=[False]) $ abc = abc.reset_index(drop=True) $ abc.head()
closingprice = [] $ for ele in r.json()['dataset']['data']: $     closingprice.append(ele[4]) $ change = max(closingprice)-min(closingprice) $ print('The largest change between any two days based on Closing Price: ' +str(change))
userproduct.merge(transactions,how='outer')[['UserID','ProductID','Quantity']].fillna(0).astype('int64')
df2.c2_bin.value_counts()
old_page_converted = np.random.binomial(1, p_equal_old_new, length_of_old) $ old_page_converted.mean()
print(tree.getroot()) $ print(etree.tostring(tree.getroot(), pretty_print=True, method="html"))
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifier.score(X_test,y_test )))  ### using logistic.score method
ml['5dayforecast'] = forest.predict(ml) $
filtered_df[numeric_cols].dtypes
date_df.head()
stat_info_city = stat_info[1].apply(fix_space) $ print(stat_info_city)
type(ts.index)
teatment_and_not_new_page = (df.group == 'treatment') & (df.landing_page != 'new_page') $ not_teatment_and_new_page = (df.group != 'treatment') & (df.landing_page == 'new_page') $ not_lined_up = teatment_and_not_new_page | not_teatment_and_new_page $ len(df[not_lined_up])
idx = pd.IndexSlice
bankID = pd.read_csv('data/bankID.csv', index_col=0) $ bankID
legos = {} $ for name in files: $     filename = 'legos/{}.csv'.format(name) $     file = pd.read_csv(filename) $     legos[name] = file
df.first_day_pctchg = (df.first_day_close-df.offer_price)/df.offer_price $ df['pct_chg_opencls'] = ((df['dollar_chg_opencls']/df['open_price'])) $ df['dollar_chg_opencls'] = (df['dollar_change_close'] - df['dollar_change_open'])
df2[['afternoon', 'evening', 'morning']] = pd.get_dummies(df2['time_of_day']) $ df2 = df2.drop('morning', axis=1)
skewed = ['capital-gain', 'capital-loss'] $ features_log_transformed = pd.DataFrame(data = features_raw) $ features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1)) $ vs.distribution(features_log_transformed, transformed = True)
test= test.reset_index(drop = True) $ test['expenses'] = pd.Series(predictions) $ datatest = pd.concat([train, test]) $ datatest = datatest.reset_index(drop = True)
%run __init__.py
df_mes = df_mes[df_mes['improvement_surcharge']>=0] $ df_mes.shape[0]
df.replace('\n', ' ', inplace=True, regex=True) $ df.lic_date = pd.to_datetime(df.lic_date, errors='coerce') $ df = df.sort_values('lic_date', ascending=False)
combined_df2 = pd.merge(combined_df, balance_df,  how = 'left', left_on = 'acct_id', right_on = 'account_id', suffixes =['','_2']) $ combined_df2.shape
observations_per_station = session.query(hi_measurement.STATION, func.count(hi_measurement.PRCP)).\ $     group_by(hi_measurement.STATION).\ $     order_by(func.count(hi_measurement.STATION).desc()).\ $     all() $ observations_per_station
from sklearn.ensemble import RandomForestRegressor $ auto_model = RandomForestRegressor(random_state=23) $ auto_model = auto_model.fit(ind_train, dep_train) $ print('Score = {:.1%}'.format(auto_model.score(ind_test, dep_test)))
log_model = sm.Logit(y, X)
import pandas as pd $ import numpy as np $ from datetime import date $ df = pd.DataFrame(np.random.randn(3,4), columns = list('ABCD')) $ df
from sklearn.model_selection import GridSearchCV $ param_grid = { $                  'n_estimators': [5, 10, 15, 20], $                  'max_depth': [2, 5, 7, 9], $              }
r = 0.0162/360
testObj.buildOutDF(tst_lat_lon_df)  ## some tests not shown performed ahead of this run $
df.head()
git_log.head(10)
bacteria_data.drop('treatment', axis=1)
new_page_converted.mean() - old_page_converted.mean() $
USER_PLANS_df.head()
X_train[X_train.building_id==-1].head(3)
df['Long'].value_counts(ascending=False).head(5)
n_new = df2.query(('landing_page == "new_page"')).count()[0] $ n_new
df_test.dtypes
df_afc_champ2018 = df_afc_champ2018.sort_values('playnumber')
df.head()
fundret.loc['2017-04':].cumsum()
total_max_prc_mon_day = hawaii_measurement_df[["Percipitation"]] \ $ .groupby([ \ $     hawaii_measurement_df["Date"].dt.strftime('%m-%d') $ ]).max() $ total_max_prc_mon_day.head()
df2 = df.drop(df.index[(df['group'] == 'control') & (df['landing_page'] == 'new_page')]) $ df2 = df2.drop(df2.index[(df2['group'] == 'treatment') & (df2['landing_page'] == 'old_page')])
session = tf.Session() $ session.run(tf.global_variables_initializer())
normals=[] $ for item in date_list: $     normals.append(daily_normals(item)[0])
print(len(BroncosBillsTweets['text30'])) $ print(len(BroncosBillsTweets['text30'].unique()))
top_brands = autos["brand"].value_counts().sort_values().index $ print(autos["brand"].value_counts().sort_values(ascending=False).head(10))
max_dif = max(dif_dict.values()) $ for key, value in dif_dict.items(): $     if value == max_dif: $         print key, value
linkpp.reset_index(inplace=True)
df_archive.info()
to_be_predicted_Day4 = 21.20401743 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
train.shape
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42) $ X_train.shape , y_train.shape,  X_test.shape, y_test.shape
list(squares.items())
images[images.duplicated]
df['subject'].value_counts()
salary_df2.columns = salary_df2.columns.str.lower() $ salary_df2.head(10)
goog.index + timedelta(weeks=1)
bymin = walk.resample("1Min") $ bymin.resample('S').mean()
plt.hist(p_diffs); $ plt.ylabel('# of Simulations') $ plt.xlabel('p_diffs') $ plt.title('Plot of 10,000 Simulated p_diffs');
name_error_rows =[] $ for i in range(len(twitter_archive)): $     if twitter_archive['name'][i] !="None" and twitter_archive['name'][i] not in twitter_archive['text'][i]: $         name_error_rows.append(i) $ print(name_error_rows)
rows = normalizedDf.index.tolist() $ for i in rows: $     normalizedDf.loc[i] = (normalizedDf.loc[i] - normalizedDf.loc[i].mean()) / normalizedDf.loc[i].std() $ normalizedDf.head()
train = train[train.price < 100000]
me = api.me() $ type(me)
MAX_DEPTH = 4   # choose a MAX_DEPTH based on cross-validation... $ print("\nChoosing MAX_DEPTH =", MAX_DEPTH, "\n")
col_totals=pd.DataFrame(col.groupby(['date_time'])['NUMBER OF PERSONS INJURED'].sum()) $ col_totals['date_time']=col_totals.index $ col_totals.head(2)
nba_df.loc[28:34]
P_n=(len(treatment_df2)) / (df2.shape[0]) $ print("Probability that an individual received the new page is {}".format(P_n))
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(N_new, P_new) $     old_page_converted = np.random.binomial(N_old, P_old) $     p_diffs.append((new_page_converted/N_new) - (old_page_converted/N_old))
notes = notes.dropna() $ notes = notes.drop(['Unnamed: 0','ScanType','ScanDate','ClinInfo','Technique','Comparison', $                         'ElecSig','Patient','RawText'], 1) $ notes = notes.reset_index(drop=True) $
goo.head()
par = pst.parameter_data #get a ref to the parameter data dataframe $ wpars = par.pargp=="welflux" $ par.loc[wpars] $
master_df.to_csv('twitter_archive_master.csv')
no_of_unique_users_in_the_data_set = df.user_id.nunique() $ print("no: of unique users in the data set = {}".format(no_of_unique_users_in_the_data_set))
autos["odometer_km"].unique().shape
hawaii_measurement_df = pd.DataFrame(results_measurement[:], columns=['Station', 'Date', 'Percipitation','Temperature']) $ hawaii_measurement_df['Date'] =  pd.to_datetime(hawaii_measurement_df['Date']) $ hawaii_measurement_df.set_index('Station', inplace=True) $ hawaii_measurement_df $
print("Number of Tools in Mobile ATT&CK") $ print(len(all_mobile['tools'])) $ df = all_mobile['tools'] $ df = json_normalize(df) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
Xs = StandardScaler().fit_transform(X) $ logreg_sentiment = LogisticRegression(random_state=42) $ scores = cross_val_score(logreg_sentiment, Xs, y, cv=skf) $ print 'Cross-validated LogReg scores based on sentiment:', scores $ print 'Mean of scores:', np.mean(scores)
tweet_scores_clean[tweet_scores_clean.retweet_count=='failed']
def log_density_ratio_gaussians(z, q_mu, q_sigma, p_mu, p_sigma): $     r_p = (z - p_mu) / p_sigma $     r_q = (z - q_mu) / q_sigma $     return np.sum(np.log(p_sigma) - np.log(q_sigma) + $                   .5 * (r_p**2 - r_q**2), axis=-1)
sl[sl.status_binary==0].groupby('wpdx_id')[['status_binary','today_preds','one_year_preds','five_year_preds']].sum().sum()
data_final.to_csv('Data_final.csv')
train_saved = train.copy(deep=True) $ test_saved = test.copy(deep=True)
pd.timedelta_range(0, periods=9, freq="2H30T")
df.tail(50)
os.system('zcat ' + file_path + ' | grep "#CHROM" | head -n 1 > /tmp/samples')
conn.execute(addresses.insert(),[ $     {"user-id":1, "email_address":"anonymous.kim@test.com"} $ ])
from gensim.models import Word2Vec
df_twitter_archive_copy.source.dtype
control_gp = df2.query('group == "control"') $ prob_control_converted = control_gp[control_gp["converted"] == 1].count()  / (control_gp[control_gp["converted"] == 0].count() + control_gp[control_gp["converted"] == 1].count()) $ prob_control_converted = prob_control_converted[0] $ prob_control_converted
national_hol=national_holidays[['date','description','locale']] $ print("Rows and columns:",national_hol.shape) $ pd.DataFrame.head(national_hol)
Celsius.temperature.__class__
df_train.columns.values
df.unstack()
import urllib $ f = urllib.urlopen("http://music.naver.com/search/search.nhn?query="+keyword+"&x=0&y=0") $ mydata = f.read();
df_vow.describe()
house_data['date'] = pd.to_datetime(house_data['date'])
path = os.path.join('Data for plots', 'Quarterly generation.csv') $ eia_gen_quarterly.to_csv(path, index=False)
model_preds.sort_values('prob_off').tail(100)
df2 = df2[df2['timestamp'] != (duplicated.iat[1,1])] $ df2.info()
df2.head(5)
data_volumn = df_range.sort_values(by = ['volumn', 'prob_1'], ascending = True)#.head(50)
stopWords = set(stopwords.words('english')) | set(stopwords.words('spanish'))
df2 = df2.join(countries.set_index('user_id'), on='user_id')
df['used_month'] = (2017 - df['yearOfRegistration']) * 12 + (4 - df['monthOfRegistration']) $ features = df.columns.values $ features
logit_control_4 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','CA_int_ab_page', 'UK_int_ab_page']]) $ result_control_4 = logit_control_4.fit() $ result_control_4.summary()
df[12].plot()
modifications_over_time = commits[['author', 'timestamp', 'modifications']].groupby( $     [commits['author'], $      pd.Grouper(freq=TIME_FREQUENCY)]).sum().reset_index() $ modifications_over_time.head()
timestamp_duplicate = '2017-01-14 02:55:59.590927' $ df2 = df2[df2.timestamp != timestamp_duplicate] $ df2['user_id'].duplicated() $ sum(df2['user_id'].duplicated())
geo_db.head()
oto.plot(x="created_utc", y="score", figsize=(18,10)) $ plt.show()
precipitation_measurement_df.describe()
from numpy import pi $ f = np.linspace(0,pi,100) $ sinf=np.sin(f) $ print("f: ", f)
threshold=20 $ counts = combined_df4['split_llpg2'].value_counts() $ repl = counts[counts <= threshold].index $ combined_df4['split_llpg2_edited']=combined_df4['split_llpg2'].replace(repl, 'Other') $
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17, linear.coef_) + linear.intercept_, c='red') $ plt.plot(X, np.dot(X_17,ridge2.coef_) + ridge2.intercept_, c='b') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
p = np.mean([pold, pnew]) $ print("the convert rate for  pnew =", p)
condo_6.shape
plt.hist(p_diffs) $ plt.axvline(obs_diff, color='r')
df.tail()
df_subset['Borough'] = df_subset['Borough'].astype('category') 
plt.hist(null_values) $ plt.axvline(obs_diff, c='red')
criteria_1 = so['ans_name'].isin(['Scott Boston', 'Ted Petrou', 'MaxU', 'unutbu']) $ criteria_2 = so['score'] > 30 $ criteria_all = criteria_1 & criteria_2 $ so[criteria_all].tail()
sorted_features = reversed(sorted([(index, x) for index, x in enumerate(clf.feature_importances_)], key=lambda x: x[1]))
result.shape
AAPL.info()
import seaborn as sns $ sns.factorplot('sex', data=titanic3, kind='count')
def answer_six(): $     Top15 = answer_one().reset_index() $     s = Top15.loc[Top15['% Renewable'].idxmax()] $     return tuple(s[['Country','% Renewable']]) $ answer_six()
autos = autos[autos["registration_year"].between(1900,2016)] $ autos['registration_year'].value_counts(normalize=True)
tweet_archive_clean['timestamp'] = pd.to_datetime(tweet_archive_clean['timestamp'])
df1 = df1.dropna()
grouped_dpt.nth(2) # nth row
print(model.params)
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', $                                           'ab_page']]) $ results = logit_mod.fit()
crimeData.to_csv('/home/hoona/Python/mpug/checkIndexIsWritten.csv')
dfData.describe()
df.head()
import pandas as pd $ import numpy as np $ np.random.seed(2018)
p_new = round(float(df2.query('converted == 1')['user_id'].nunique())/float(df2['user_id'].nunique()),4) $ p_new
public_tweets = api.search('StarWarsDay')
df2[df2.duplicated('user_id', keep=False)]
with open('.CB_key', 'r') as f: $     key = f.read().replace('\n', '')
afl_data.tail(3)
autos["kilometer"].head(3)
print(autos['price'].unique()) $ print(autos['odometer'].unique())
total_prob_of_conv=df2['converted'].mean() $ print('The probability of an individual converting regardless of page is:  ' + str(total_prob_of_conv)) $
posts_groupby.mean().ups.plot(kind='barh', figsize=[8,8])
model_gb_20 = GradientBoostingClassifier(min_samples_leaf=20, random_state=42) $ model_gb_20.fit(x_train,y_train) $ print("Train: ", model_gb_20.score(x_train,y_train)*100) $ print("Test: ", model_gb_20.score(x_test,y_test)*100) $ print("Difference between train and test: ", model_gb_20.score(x_train,y_train)*100-model_gb_20.score(x_test,y_test)*100)
'{0:.6f}%'.format((act_diff < p_diffs).mean() *100)
with open( "title_id2token.dpkl", "rb" ) as file: $     title_id2token = dpickle.load(file) $ with open( "title_token2id.dpkl", "rb" ) as file: $     title_token2id = dpickle.load(file)
new_page_converted.mean()
testing.Open.fillna(1,inplace=True) $ testing.Open= testing.Open.astype(int) $ print(testing.Open.value_counts())
sns.barplot(eg.index[:15].values,eg[:15].values)#,eg) $ plt.title('Top Application Frequencies By State')
ts.resample('D').sum().plot()
complete_signup_mf = build_hist_dict(signup_completed, feature='gender_str') $ all_mf = build_hist_dict(train_data_df, feature='gender_str') $ for gender in ['m', 'f']: $     print 'signup completed rate for {} gender: {}'.\ $         format(gender, float(complete_signup_mf[gender])/all_mf[gender]) $
control_gp=df2[df2['group']=='treatment'] $ control_gp['converted'].mean()
num_unique_users = len(df.user_id.unique()) $ print("Number of unique users: {}".format(num_unique_users))
df_timeseries['cryptdex100'] = df_timeseries.groupby('date')['cryptdex_index'].transform('sum')
p.to_timestamp('M', 'e')
talks_train.ix[1300,'name']
matt_tweets = pd.DataFrame(Matt) $ matt_tweets
sphere.subjectivity
precipitation_year = session.query(Measurements.date,func.avg(Measurements.prcp)) \ $              .filter(Measurements.date >= '2016-08-23').filter(Measurements.date <= '2017-08-23') \ $              .group_by(Measurements.date).all()
archive_df[archive_df.tweet_id.duplicated()] $
print(df_w_topics[['title', 'author gender', 1]].sort_values(by=[1], ascending=False))
index = obj.index
reddit_comments_data.select('body','sentiment','subjectivity').show(10)
autos.head()
df_merged.drop(['created_at'], axis=1,inplace=True);
def get_rebalance_cost(all_rebalance_weights, shift_size, rebalance_count): $     assert shift_size > 0 $     assert rebalance_count > 0 $     return None $ project_tests.test_get_rebalance_cost(get_rebalance_cost)
X = np.array(df.drop(['label'], 1))
pol_users.drop('array_agg', axis=1, inplace=True)
sm.delete_endpoint(EndpointName=xgboost_endpoint)
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ act_diff $ print('Proportion of p_diffs greater than act_diff is {}'.format((act_diff < p_diffs).mean()))
week2_df.keys()
df2_copy['intercept'] = 1 $ df2_copy[['landing_page_new', 'landing_page_old']] = pd.get_dummies(df2_copy['landing_page']) $ df2_copy[['ab_page_control','ab_page_treatment']] = pd.get_dummies(df2_copy['group'])
old_page = np.random.binomial(1,p,df2.query('landing_page == "old_page"').shape[0])
os.remove('myfile.csv')  #Clean up.
appleNegs.shape
fashion = pd.read_pickle('../data/fashion.pkl')
df = genre_avg_rating.toPandas() $ df.head()
print('The proportion of users converted: ' ,df['converted'].sum()/len(df))
subrcvec1 = pd.DataFrame(cvecdata1.todense(),      $              columns=cvec.get_feature_names())
code, start, end = '1', '2017/08/01', '2017/11/10' $ sql = "select * from hundred_stocks_twoyears_daily_bar where code=%s"% (code) $ df000001 = pd.read_sql(sql, conn_helloDB)
to_be_predicted_Day5 = 52.45280379 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
for t in df[df.sentiment == df.sentiment.max()].text[:10]: $     print(t + '\n')
df.loc['2018-05-21',['Open','Volume']]
rsp.json()
model.docvecs.most_similar()
for e in res: $     print e.text
c.execute(query) $
grid_tfidf.cv_results_['mean_test_score']
cnx = create_engine('postgres://jessica:123@localhost:5432/rpred', isolation_level='AUTOCOMMIT') $ conn = cnx.connect()
intervention_history['CRE_DATE_GZL'].min(), intervention_history['CRE_DATE_GZL'].max()
topics = pd.DataFrame(interests)
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ response
swPos.shape
p_old = len(df2.query("converted=='1'"))/ len(df2) $ p_old
scores = cross_val_score(model, X, y, scoring='roc_auc', cv=5) $ print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))
trump['text'] = trump['text'].str.lower() $
events[events['type'] == 'WatchEvent'].head()
metrics, predictions = pipeline.test(X_train, y_train, output_scores=True) $ print("Performance metrics on training set: ") $ display(metrics) $ print("Individual scores: ")
df.groupby("cancelled")[["web_booking", "iphone", "android"]].mean()
cfg_fnames = list(glob.glob('test-data/excel_adv_data/sample-xls-case-badlayout1*.xlsx')) $ print(len(cfg_fnames))
car_brands = autos['brand'].value_counts(normalize=True).index
inputs, targets = zip(*dataset) $ inputs = np.array([string_to_int(i, 20, human_vocab) for i in inputs]) $ targets = [string_to_int(t, 20, machine_vocab) for t in targets] $ targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))
pd.Series(np.random.randn(1000)).hist(bins=20, alpha=0.4);
s.dt.time   # extract time as time Object
DD= df['domain'].value_counts().head(30).index.tolist() $ df['domain_d'] = [type_ if type_ in DD $                       else "OTHER" for type_ in df['domain']] $ print(df['domain_d'].nunique())
predictions.show(20)
us_rhum = rhum_fine.reshape(844,1534).T #.T is for transpose $ np.shape(us_rhum)
old = df2.query('landing_page == "old_page"') $ new = df2.query('landing_page == "new_page"')
crime_10H40_10_17 = sel_df[sel_df.Beat =='10H40'] $ crime_10H10_10_17 = sel_df[sel_df.Beat =='10H10'] $ crime_10H60_10_17 = sel_df[sel_df.Beat =='10H60'] $ crime_10H70_10_17 = sel_df[sel_df.Beat =='10H70']
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1 - p_new), p_new]) $ new_page_converted.mean()
rhum_us_full = rhum_nc.variables['rhum'][:, lat_li:lat_ui, lon_li:lon_ui]
state_keys_list = sorted(list(state_DataFrames.keys())) $ state_keys_list
y_test_under[rfc_bet_under].mean()
bmp_series = pd.Series(brand_dict)
indices = related_docs_indices.copy() $ indices = [x - 1 for x in related_docs_indices]
Base = automap_base() $ Base.prepare(engine, reflect=True) $
df.index
X_train.shape, X_test.shape
dulp.columns
R_trip['Season'] = [get_season(xi) for xi in R_trip.index.dayofyear] $ R_trip.Season = R_trip.Season.map({'Winter': 4, 'Fall': 3, 'Summer': 2, 'Spring': 1}) $ R_trip.head()
x = [1, 2, 3] $ y = [4, 5, 6] $ z = [7, 8, 9] $ np.concatenate([x, y, z])
def join_df(left, right, left_on, right_on=None, suffix='_y'): $     if right_on is None: right_on = left_on $     return left.merge(right, how='left', left_on=left_on, right_on=right_on, $                       suffixes=("", suffix))
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%s" % (x.year, str(x.week).zfill(2))) $ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%s" % (x.year, str(x.week).zfill(2)))
tweet_json_clean.rename(index = str, columns={"id": "tweet_id"}, inplace = True)
extra_bases = baseball[['X2b','X3b','hr']].sum(axis=1) $ extra_bases.sort_values(ascending=False)
print (" The text: %s \n The grade in rating_numerator: %.1f \n" % (df_clean['text'].ix[695], df_clean['rating_numerator'].ix[695]))
prob_converted = df2.converted.mean() $ print (prob_converted)
df_person = pd.read_sql_query("SELECT * FROM person", conn) $ df_grades = pd.read_sql_query("SELECT * FROM grades", conn) $ df_person.merge(df_grades, how="left", left_on = "id", right_on="person_id")
correlation_matrix = np.corrcoef(data_matrix.T)
deck_data['within_six_months'] = deck_data['created_at_dt'].apply(lambda x: 1 if (datetime.today() - x).days <= 180 $                                                                  else 0) $ six_months_deck = deck_data.copy() $ six_months_deck = six_months_deck.loc[(six_months_deck['within_six_months'] == 1)] $ six_months_deck.head()
from datetime import datetime $ from sqlalchemy import create_engine $ from sqlalchemy import Table, Column, Integer, String, DateTime, ForeignKey $ from sqlalchemy.orm import relationship, backref, sessionmaker $ from sqlalchemy.ext.declarative import declarative_base
bgf = BetaGeoFitter(penalizer_coef=0.0) $ bgf.fit(m['frequency'], m['recency'], m['T']) $ print(bgf) $
bs = BeautifulSoup(res["html"], "html.parser") $ " ".join([str(x) for x in bs.find("blockquote").children])
conn_hardy = psycopg2.connect("dbname='analytics' user='awsdatapipeline' host='udacity-segment.c2zpsqalam7o.us-west-2.redshift.amazonaws.com' port='5439' password='Know.it.13818'")
my_query = "SELECT * FROM specimens LIMIT 1" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element.keys() $
properati[(pd.isnull(properati['place_name']))].zone.value_counts()
articles = soup.find_all('article', class_='carousel_item')
df_nona.groupby('district_id').district_size.mean().hist(bins=12) $ df_nona['segment'] = pd.cut(df_nona.district_size, bins=[0,1000,4000,120000]) $ df_nona.groupby('segment').district_id.nunique()
df[(df.group =='treatment') & (df.landing_page != 'new_page') | (df.group !='treatment') & (df.landing_page =='new_page')].count()['landing_page']
price_odometer = pd.DataFrame(price, columns = ["mean_price"])
df2.drop(df2.index[2862], inplace=True)
n_page_converted = np.random.binomial(1, p_new, n_new) $ print('The new_page convert rate: {}.'.format(n_page_converted.mean())) $ print('The new_page convert rate: {}.'.format(round(n_page_converted.mean(), 4)))
precip_data_df.head(3)
reddit_df.head()
list_of_hospitals.sort()
m3.load_cycle('imdb2', 4)
df = cust_df.drop('Address', axis=1) $ df.head()
YS1517['Adj Close'].rank()
old_page_converted = np.random.choice([0, 1], size = n_old, p = [1 - p_old_null, p_old_null])
pivoted.T.shape
fb_desc = fb.description
weather_data3.columns = weather_data1.columns.values; weather_data3.head()
from keras.models import load_model $ y_pred = model.predict_classes(val_x) $ y_true = val_y
global_sea_level = global_sea_level.set_index("year") $ global_sea_level["msl_ib_ns(mm)"]
s2.iloc[1]
reviews = df['review'].tolist() $ print("The file {} contains {} reviews.".format(reviews_file_name, len(reviews))) $ print() $ print("This is the last review:") $ print(reviews[-1])
tmp = data[data['loan_status']!='Current'].copy() $ tmp.ix[tmp['loan_status']=='Current', 'target']=-1 $ tmp.ix[tmp['loan_status']=='Charged Off', 'target']=0 $ tmp.ix[tmp['loan_status']=='Fully Paid', 'target']=1
df2['intercept']=1 $ ab_page = ['treatment', 'control'] $ df2['ab_page'] = pd.get_dummies(df2.group)['treatment']
pd.pivot_table(users, values = 'DaysActive', $                index = 'OneDayUser', $                columns = 'Active', $                margins = True)
senateAll["filerIdent"] = senateAll["filerIdent"].astype(object)
df_tot[['Tract', 'Poverty', 'Income', 'Unemployment', 'Percent_Male']] = \ $ df_tot[['Tract', 'Poverty', 'Income', 'Unemployment', 'Percent_Male']].apply(pd.to_numeric)
print("UK_ind_ab_page: " + str(np.exp(results_m.params[1]))) $ print("US_ind_ab_page: " + str(1/np.exp(results_m.params[2]))) $ print("Based on the results there is not a strong correlation between country and conversion.") $
kickstarter_failed_successful.sort_values(by='difference', ascending=False)
autos = autos[autos["registration_year"].between(1910,2016)] $ autos["registration_year"].value_counts(normalize=True).head(30)
ls ../../outputs-git_ignored/gpu_mod_run_1
names = ["John Smith", "Alan Alda"] $ for i in filter(lambda s: s[0].upper() == s[-1].upper(), names): $     print(i)
print(survey.describe(include='all')) $ categoricos = survey.select_dtypes(include=['object']).copy() $ categoricosx = categoricos.astype(str) $
LogisticModel_ZeroFill.fit(X, y)
breaches.groupby('IsVerified').size()
ab_file2.info()
fig, axarr = plt.subplots( 1, 1, figsize=(20,10) ) $ plt.hist(users.control["revs"], alpha = 0.5, range = (0, 15), log = True, bins = 15, label = "Source editor only") $ plt.hist(users.treatment["revs"], alpha = 0.5, range = (0, 15), log = True, bins = 15, label = "Both editors") $ plt.legend(loc="upper right") $ plt.show()
sns.heatmap(prec_us)
print(tweets.loc[300074]["content"]) $ tweets.loc[300074]
%timeit pd.read_sql(f'explain {sql_on}', engine).head()
df.groupby(['country code', 'website']).price.mean()
data[['returns', 'strategy']].cumsum().apply(np.exp).plot(figsize=(10, 6))
model_rf_imp=RandomForestRegressor(max_depth=19, n_estimators=100, random_state=42) $ model_rf_imp.fit(X_train_new,Y_train) $ print("Random Forest (max_depth=19, n_est=100, importances)") $ print("Train: ", model_rf_imp.score(X_train_new,Y_train)*100) $ print("Test: ", model_rf_imp.score(X_test_new,Y_test)*100) $
sum(df.isnull().any())
(df2.landing_page == "new_page").mean()
sub_df.drop('created', axis=1, inplace=True)
dimension = vec_code.shape[-1] $ index = AnnoyIndex(dimension) $ for i, v in enumerate(vec_code): $     index.add_item(i, v) $ index.build(10)
print wikipedia.summary("Lost in translation (film)")
weather_df = pd.read_sql('select * from weather limit 10;', conn)
brand_counts = autos["brand"].value_counts(normalize=True) $ common_brands = brand_counts[brand_counts > .05].index $ print(common_brands)
business_df.shape, address_df.shape
episcopalian = df_final[df_final['tweet_id']==666287406224695296].index $ df_final.drop(episcopalian, inplace=True) $ df_final.reset_index(drop=True,inplace=True)
from sklearn.linear_model import LogisticRegression $ from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score $ from sklearn.model_selection import train_test_split
facebook_urls = unique_urls[(unique_urls.domain == domain) & (unique_urls.url.str.count('/') == 3)] $ facebook_urls.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
!wget -O loan_test.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv
dr = pd.date_range('1/1/2010', periods=3, freq='3B')
posi = 0 $ posj = 3 $ evol = pd.DataFrame([[s[i] if len(s) > posj-1 else 0 for s in steps] for i in range(posi, posj)], $                     index=range(posi, posj)) $ evol.T.plot()
df['btime_h'] = df.apply(lambda x: x['dt']*x.btime*x.h0/1000/(x.Da*(x.hmax_sim-1))/U0(1,33,x.h0/1000, x.hmax/1000), axis =1)
import matplotlib.pyplot as plt $ plt.matshow(ibm_hr_target_small.select("*").toPandas().corr())
df['duration'].describe()
twtter_count1 = df1.groupby('placeId').count()[['id']] $ twtter_count2 = df2.groupby('placeId').count()[['id']] $ twtter_count3 = df3.groupby('placeId').count()[['id']]
dfFull['GarageAreaNorm'] = dfFull.GarageArea/dfFull.GarageArea.max()
  hp.dtypes
crimes['year'] = crimes.DATE_OF_OCCURRENCE.map(lambda x: x.year)
CSV_FILE_PATH = os.path.join('pims_cloudpbx_subset_201806051550_1million.csv') $ GEOLITE_ASN_PATH = os.path.join('GeoLite2-ASN.mmdb') $ GEOLITE_CITY_PATH = os.path.join('GeoLite2-City.mmdb')
user_tweet_count_df = tweet_df[['user_id', 'tweet_type']].groupby(['user_id', 'tweet_type']).size().unstack() $ user_tweet_count_df.fillna(0, inplace=True) $ user_tweet_count_df['tweets_in_dataset'] = user_tweet_count_df.original + user_tweet_count_df.quote + user_tweet_count_df.reply + user_tweet_count_df.retweet $ user_tweet_count_df.count()
df_user_count = df_stars.groupby('user_id').size() $ print(df_user_count )
autos = autos.drop(["num_photos", "seller", "offer_type"], axis=1)
cityID = '4fd63188b772fc62' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Laredo.append(tweet) 
df_test_1 = countries_df.join(df2, how='inner') $ df_test_1.head()
average_chart_lower_control_limit = average_of_averages - 3 * d_three * average_range / \ $                                     (d_two * math.sqrt(subgroup_size))
need_to_slice = sp.get_tally(name='need-to-slice') $ slice_test = need_to_slice.get_slice(scores=['scatter'], nuclides=['H1'], $                                      filters=[openmc.CellFilter], filter_bins=[(moderator_cell.id,)]) $ slice_test.get_pandas_dataframe()
df['man_on_first'] = np.where(df['on_1b'] > 0 , 1, 0) $ df['man_on_second'] = np.where(df['on_2b'] > 0 , 1, 0) $ df['man_on_third'] = np.where(df['on_3b'] > 0 , 1, 0) $ df['men_on_base'] = df['man_on_first'] + df['man_on_second'] + df['man_on_third']
df = pd.merge(train.to_frame(), stock["rise_in_next_day"].to_frame(), left_index=True, right_index=True)
plt.figure(figsize=(15,6)) $ plt.title('Distribution of Crimes per day', fontsize=16) $ plt.tick_params(labelsize=14) $ sns.distplot(crimes.resample('D').size(), bins=60);
connection.close()
contractor_clean.head() #these columns have been dropped
import psycopg2 as pg2 $ conn = pg2.connect(dbname='ChicagoViolencePredictor', user='postgres', host='localhost')
import csv $ with open('Other Files\\ZomatoData.csv', 'w',encoding='utf-8') as csv_file: $     for a in mrow: $         csv_file.write(a)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df['VALUE'] = zip(df['DIVISION'], df['DATE'], df['TIME'], df['DESC'], df['ENTRIES'], df['EXITS']) $ df['VALUE'] = df['VALUE'].apply(lambda x: list(x))
def remove_stopwords(tokenized_list): $     text = [word for word in tokenized_list if word not in stopword] $     return text $ infinity['text_nostop'] = infinity['text_tokenized'].apply(lambda x: remove_stopwords(x))
video_ids = yt.get_video_urls_from_playlist_id(playlist_id, key, $                                                cutoff_date=datetime.datetime(2017,1,1))
data.info()
tweets_original = pd.DataFrame(ndarray_original)
pd.to_datetime(data.last_trip_date).hist()
df.iloc[1:4,0:3]
autos.head()
borehole.to_sql(con=engine, name='borehole', if_exists='replace', flavor='mysql',index=False)
twitter_df[twitter_df.in_reply_to_status_id.notnull()]['in_reply_to_status_id'][0:5]
new_page_converted = np.random.choice(2,n_new,p=[0.8804,0.1196])
tweet_clean.columns
merged_df['Month'] = merged_df.index.month
info_cut = parks_info[['parkid','acreage','supdist']] $ info_cut.parkid = info_cut.parkid.values.astype(int) $ info_cut.head(2)
results.n_311_flooding_reports = flood_reports.shape[0] $ flood_reports.shape
unique_users = df.nunique()['user_id'] $ print("The number of unique users in the dataset is: {}".format(unique_users)) $
names = [] $ counts = [] $ for i in range(len(fdist_top)): $     names.append((fdist_top[i])[0]) $     counts.append((fdist_top[i])[1])
unique_users_count = df.user_id.nunique() $ print("Number of unique users: {}".format(unique_users_count))
tweets_kyoto_filter = tweets_kyoto[tweets_kyoto['ex_lat']<35.031461] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_lat']>34.931461] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_long']<135.785300] $ tweets_kyoto_filter = tweets_kyoto_filter[tweets_kyoto_filter['ex_long']>135.685300]
autos['price'].value_counts().sort_index(ascending = False).head(20)
it_df['bidDeadline'] = pd.to_datetime(it_df['bidDeadline'], errors='coerce') $ it_df["buyer_name"] = it_df.buyers.apply(getMainBuyerName)
apple.dtypes
goog.Open.resample('M').plot() $ goog.Open.asfreq('D',method='backfill').plot()
test_df = us_companies.copy()
conversion_df2 = df2['converted'].mean() $ conversion_df2
print "Last version of the production as from the first row: ", df2.id[0] $ df2
tweet_archive_clean.head(2)
!h5ls -r 'data/my_pytables_file.h5'
df_selected = df_selected.filter("numOfReviews > 4")
absorption.export_xs_data(filename='absorption-xs', format='excel')
df.drop( columns=df.columns[3:6] )       # delete columns by range of column number
def engineer(x): $     if 'Engineer' in x: $         return 1 $     return 0 $ df_more['Engineer'] = df_more['Title'].apply(engineer)
questions['zipcode'].unique() $
df.plot(y = 'Close')
D2 = [tuple(x) for x in worldbible.to_records(index=False)]
index_weighted_returns = generate_weighted_returns(returns, index_weights) $ etf_weighted_returns = generate_weighted_returns(returns, etf_weights) $ helper.plot_returns(index_weighted_returns, 'Index Returns') $ helper.plot_returns(etf_weighted_returns, 'ETF Returns')
percipitation_measurement_df = pd.DataFrame(percipitation_year[:], columns=['Date','prcp',]) $ percipitation_measurement_df['Date'] =  pd.to_datetime(percipitation_measurement_df['Date']) $ percipitation_measurement_df.set_index('Date', inplace=True) $ percipitation_measurement_df.head()
clean_stations.columns = ['station', 'latitude', 'longitude', 'elevation', 'name', 'country', 'state']
lead_classes = labels_dedupe.pivot('funding_round_uuid','investor_uuid','is_lead_investor')
ndvi_of_interest02= ndvi.sel(time = time_slice02, method='nearest') $ ndvi_of_interest02
df["Complaint Type"].value_counts().head(1)
plt.scatter(x_axis,y_axis) $ plt.title('Total Retweet Distribution',fontsize=16) $ plt.xlabel('Follower Count',fontsize=14) $ plt.ylabel('Retweet Count',fontsize=14) $ plt.show()
adj_close_pivot = adj_close_acq_date_modified.pivot_table(index=['Ticker', 'Acquisition Date'], values='Adj Close', aggfunc=np.max) $ adj_close_pivot.reset_index(inplace=True) $ adj_close_pivot
print ('Summary:') $ print (summarize(text, ratio=0.5))
df.loc[df['edition']=='PunditFact'].head()
yc = np.array(y)
round((model_x.rsquared_adj), 3)
df.iloc[:,[0,3]]
data[data['Body_Count']>200].sort_values(by='Body_Count', ascending=False)
options_frame.info()
x.dropna()
df[["PREV_DATE", "PREV_ENTRIES"]] = (df $                                                        .groupby(["C/A", "UNIT", "SCP", "STATION"])["DATE", "ENTRIES"] $                                                        .transform(lambda grp: grp.shift(1))) $ df.head()
with open('library-Al.meam', 'w') as f: $
trunc_df.head()
temp_series_freq_2H = temp_series.resample("2H").mean() $ temp_series_freq_2H
returns.corr()
train_data = train_data[(train_data.yearOfRegistration >=1970) & (train_data.yearOfRegistration <= 2017) & (train_data.price >100) & (train_data.price < 200000)] $ train_data.shape
print(autos.columns)
dat_missing_zip.shape
twitter_archive_enhanced = pd.read_csv(data_folder + 'twitter-archive-enhanced.csv')
reqs.set_index(['Unnamed: 0', 'Agency', 'Complaint Type', 'Descriptor', 'Location Type', 'Bridge Highway Direction'], inplace=True) $ reqs.head()
check_presence = multiindex_output.isin(source_table_to_compare[['Email', 'Lineitem sku']]) $ check_presence[check_presence['step_no'] == True]
pnls = df_res.map( lambda r: {'date': r.date, $                           'neutral': r.neutral, $                           'var': float(var(r.scenarios.array, neutral_scenario=r.neutral))}).toDF().toPandas()
obs_diff_treatment = df2.query('group == "treatment"').query('converted == 1')['user_id'].count() / df2.query('group == "treatment"')['user_id'].count() $ obs_diff_treatment
resampler = sess.resample('5s') $ print(type(resampler)) $ sess_r = resampler.bfill() $ sess_r.head() $ sess_r.plot(y='X')  # Timedeltas seem to still have some bugs regarding plotting
df2[df2['landing_page']=='old_page'].shape[0]/df.shape[0]
archive_copy.info()
spark.createDataFrame(moviesCleanRdd, schema).show(10, False)
p0_new = df2.query("converted==1 & group=='treatment'").shape[0]/df2.query("group=='treatment'").shape[0]
sns.distplot(calls_df["length_in_sec"])
result['id_ndaj1'].unique().shape
ts.asfreq('45min',method='pad')
weekday_df = pd.get_dummies(df_total.index.weekday, prefix='weekday') $ weekday_df.index = df_total.index
df['atbat_pk'] = df['game_pk'].astype(str) + df['at_bat_number'].astype(str) $ df['is_shift'] = np.where(df['if_fielding_alignment'] == 'Infield shift', 1, 0)
mask = ((combined.state=='Colombia') & combined.city.str.startswith('Pe')).pipe(np.invert) $ combined = combined.loc[mask]
prophet_df = pd.DataFrame() $ prophet_df['y'] = df[target_column] $ prophet_df['ds'] = df['date']
tweets_clean.timestamp = pd.to_datetime(tweets_clean.timestamp, yearfirst = True, infer_datetime_format = True)
closed_daily = data.set_index('closed_at').resample('D').size() $ closed_monthly_mean = closed_daily.resample('M').mean() $ closed_monthly_mean.index = closed_monthly_mean.index.strftime('%Y/%m') $ iplot(closed_monthly_mean.iplot(asFigure=True, dimensions=(750, 500), vline=['2017/09', '2017/03', '2016/09', '2016/03', '2015/09', '2014/12', '2014/04', '2013/10']))
cust_new.head(3)
male_journalists_retweet_summary_df = journalists_retweet_summary_df[journalists_retweet_summary_df.gender == 'M'] $ male_journalists_retweet_summary_df.to_csv('output/male_journalists_retweeted_by_journalists.csv') $ male_journalists_retweet_summary_df[journalist_retweet_summary_fields].head(25)
cats_df['breed'].value_counts()
df_dd = pd.DataFrame() $ for date_col in df_datecols: $     dd_col = date_col + '_index_dd' $     df_dd[dd_col] = (df_EMR['index_date'] - df_datecols[date_col]).dt.days
(taxiData2.Tip_amount < 0).any() # This Returns False, proving we have successfully changed the values with no negative
precipitation_df.sort_values("date", inplace = True) $ precipitation_df.head(15)
output= " select tag_id, sum(retweets) as crt from tweet_details as td inner join tweet_tags as tt where td.tweet_id=tt.tweet_id group by tag_id order by crt desc limit 8; " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['tag_id','Sum'])
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
ab = AdaBoostRegressor(base_estimator=GradientBoostingRegressor(n_estimators = 2000, \ $                                                                 max_depth = 5, min_samples_split = 3), \ $                         n_estimators=50, learning_rate=0.1, loss='exponential', random_state=3)
twitter_archive_full = twitter_archive_full[twitter_archive_full.in_reply_to_status_id_x.isna()].copy() $ twitter_archive_full = twitter_archive_full[twitter_archive_full.retweeted_status.isna()].copy() $
file_dir = os.path.dirname(os.path.abspath("__file__")) $ parent_dir = os.path.dirname(file_dir) $ newPath = os.path.join(parent_dir, '/home/student/data/weather/2017weatherClean.csv') $ dfW17 = pd.read_csv(newPath, delimiter=',')
!cat data/legend.csv
brand_names = np.unique(np.asarray(data.brand[:])) $ model_names = np.unique(np.asarray(data.model[:]))
print(np.exp(results.params)) $ print(1/np.exp(results.params))
column_mean = df.mean(1) $ result1 = df['A'] + column_mean $ result2 = df.eval('A + @column_mean') $ np.allclose(result1, result2)
import statsmodels.api as sm $ convert_old = df2.query('group=="control"').converted.sum() $ convert_new = df2.query('group=="treatment"').converted.sum() $ n_old = df2.query('group=="control"').shape[0] $ n_new = df2.query('group=="treatment"').shape[0]
df2.head()
plt.plot(pipe.tracking_error)
states = pd.DataFrame({'population': population, $                        'area': area}   ) $ states
difference = [] $ for entry in d["dataset_data"]["data"]: $     difference.append(entry[2]-entry[3]) $ print("The maimum difference in a given was $" + str(max(difference)))
def percents( date_time , date_series , data_frame , look_back = 365 , quantiles = [25,75]): $     look_back = datetime.timedelta(look_back) $     mask = (-look_back < date_series - date_time) & (date_series - date_time <= datetime.timedelta(0)) $     nums = np.percentile( data_frame[mask],q = quantiles,axis = 0) $     return nums $
lr_all_user[lr_all_user['indebt:actual'].notnull()].to_clipboard()
news_df.head()                                                                # displays dataframe
example_time = tweet_archive_clean.timestamp[0] $ datetime.strptime(example_time,'%Y-%m-%d %H:%M:%S %z')
for item in result_set: $     print(item.index,item.relationship)
obj.index.is_unique
shows['stemmed_plot'] = shows['stemmed_plot'].dropna().apply(reassemble_plots)
zipincome.head()
table_rows = driver.find_elements_by_tag_name("tbody")[14].find_elements_by_tag_name("tr") $
from sklearn.linear_model import LogisticRegression
df_weekly['polarity_average'].describe()
session.query(Measurements.station,func.count(Measurements.date)) \ $              .group_by(Measurements.station).order_by(func.count(Measurements.date).desc()).all() $
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=s6zxiCjBcnTwZdtz4mEQ') $ rj = r.json() $ rj['dataset']['data'][1]
df_campaigns.describe()
pd.get_option('display.max_colwidth')
pred.info() $
stock_change.plot(grid = True).axhline(y = 0, color = "black", lw = 2)
set(clean_rates.name.sample(20).tolist())
leadConvpct = leadConvpct.filter(like='convertedLead').rename('leadConversionPercent').to_frame().reset_index()
df3 = df3.drop('CA', axis=1) $ df3.head()
len(users) $ users.drop(users[users['friends_count'].isna()].index, inplace=True) $ len(users)
joined_hist.reset_index(level=0, inplace=True)
saved = savedict.copy() $ ex = saved['2017-11-15'] $ print("Choose Campaign Name: ",ex['Campaign Name'].unique()) $
data.to_csv('data/Building_Permits_proc_1.csv')
( $     autos["registration_year"].value_counts(normalize=True) $     .head(10) $ )
index_vector = df.source == 'GRCh38' $ gdf = df[index_vector] $ gdf.shape
extract_all.app_id_short.unique()
validation.analysis(observation_data, Jarvis_simulation)
grouped = df_providers.groupby(['year','drg3']) $ (grouped.count) $ grouped.quantile
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s') $ print(git_log.describe())
tz_cat = timecat_df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ tz_cat.head()
explode = [0.1,0,0] $ colors = ["gold", "lightblue", "lightcoral"] $ labels = ["Urban", "Suburban","Rural"]
test_data["Age no Nans"] = test_data["AgeuponOutcome"].fillna(value="2 years") $
df_members['bd'] = df_members['bd'].clip_upper(100).clip_lower(10)
def set_position(lots): $     position=store['POSITION'] $     position.write('Current',lots)
temps1.mean()
image_clean.head(5)
model = Model(inputs=base_model.input, outputs=predictions) $ for layer in base_model.layers: layer.trainable = False $ model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
top_20['retweet_count'].sort_values().plot.barh(figsize=(10, 8))
all_mobile = lift.get_all_mobile()
pd.date_range(start='2018-01-01', periods=12, freq='WOM-3SAT')
percent_success = round(kickstarters_2017["state"].value_counts() / len(kickstarters_2017["state"]) * 100,2) $ print("State Percent: ") $ print(percent_success)
active_with_type = gbq.read_gbq(query3+query4, project_id=project_id,dialect = "standard")
repeated_user_id = df2.user_id.value_counts().reset_index().query('user_id > 1')['index'][0] $ repeated_user_id
actor = pd.read_sql_query('select last_name, first_name from actor \ $                            where last_name like "%%LI%%" \ $                            order by last_name, first_name', engine) $ actor.head()
np.shape(temp_us_full)
df2.info()
s1 = pd.Series([3,4,5,6,7,8,9,10,11,12,13,14], index = pd.date_range('20170101', periods = 12)) $ s1
nnew = df2.query('landing_page == "new_page"').count()[0] $ print ("The population of Newpage is : {}".format(nnew))
x_train, x_test, y_train, y_test = train_test_split(xarr, yarr, $                                                     test_size=0.3, random_state=1)
np.random.binomial(size_treatment, converted_prob, 10000).mean()
sub = pd.DataFrame() $ sub['click_id'] = test_df['click_id'] $ print("Sub dimension "    + str(sub.shape))
pd.concat([timeseries.loc['2018-01-01':],future_forecast],axis=1).plot(figsize= (12,8))
e_p_b_two.index = e_p_b_two.TimeCreate $ del e_p_b_two['TimeCreate'] $
df = pd.read_hdf('data/games.hdf','df') $ df
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score,p_value
data.groupby(data.index.time).mean().plot();
theft.sort_values('DATE_OF_OCCURRENCE', inplace=True, ascending=True) $ theft.head()
train_df8 = train_df7.copy() $ train_df8['fn_categorical'] = ['fna' if x<=2 else( 'fnb' if x<6 else 'fnc') for x in train_df7['features_number']] $ del train_df8['features_number'] $ train_df8.head()
weather_df["weather_main"] = weather_df.weather_main.str.lower() $ weather_df["weather_description"] = weather_df.weather_description.str.lower()
autos['registration_year'].describe()
page.text
df_tot[df_tot.Population==0] = df_tot[df_tot.Population==0].fillna(0) $ df_tot.Poverty.fillna(0, inplace=True) $ df_tot.Income.fillna(df_tot.Income.median(), inplace=True)
coin_data.head(5)
startups_USA = df_companies[df_companies['country_code'] == 'USA'] $ startups_USA.head()
%store page.text > barclay_banks.txt
def load_keys(path): $     ...
lr_predicted = lr_model_newton.predict(X_final)
os.chdir(dir) $ average_crimes_current.to_csv("rolling_crime_avg_by_zip.csv")
df_sched.fillna(value= '1900-01-01',inplace=True)
X_df = classify_df.drop(['Y'],axis=1)
conn.execute("select * from Measurements limit 10 ").fetchall()
df_r['intercept']=1 $ df_r[['control', 'treatment']] = pd.get_dummies(df['group'])
import re $ letters_only = re.sub("[^a-zA-Z]",           # The pattern to search for $                       " ",                   # The pattern to replace it with $                       example1.get_text() )  # The text to search $ print (letters_only)
[(f.user.name, f.text[:15].replace("\n", " ")) for f in favorites]
c_df = c_df.dropna(axis=1,how='all') $ c_df.size
print(firsts.mean(), others.mean())
!head data/sessions.csv
dfd = pd.get_dummies(dfm, columns=['country'], drop_first=True) $ dfd = pd.get_dummies(dfd, columns=['cat_type'], drop_first=True)
to_be_predicted_Day2 = 48.59906715 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
station.head()
gDate_vProject = gDateProject_content.unstack(level=1, fill_value=0) $
last_tweet = browser.find_by_id('stream-items-id')[0]
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall()
nltk_text.concordance("PETA") $ print $ nltk_text.concordance("protest") $
df.set_index('hash_id', inplace=True)
df.set_index('datetime',inplace=True)
big_df_avg.to_csv('polarity_results_LexiconBased/days7/polarity_avg_days.csv',index=None) $ big_df_count.to_csv('polarity_results_LexiconBased/days7/polarity_count_days.csv',index=None)
house_data = house_data.sort_values(by = ['listing_id'])
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'weekday']]) $ results = log_mod.fit() $ results.summary()
contribs.head(10)
conditions_counts.plot(kind='barh');
print(pd.date_range('2018-01-01',periods=30,freq='D'))
url = "https://twitter.com/marswxreport?lang=en" $ response = requests.get(url) $ soup = BeautifulSoup(response.text, 'html.parser') $ result = soup.find('div', class_="js-tweet-text-container") $ print(result)
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df2['group'])
"my string".find('s')
rnd_reg.fit(X_train, y_train)
df.values  # Display the values only
df_merged['text'].iloc[228]
facts_metrics.dimensions_item_id.nunique()
kick_projects.groupby(['main_category','state']).size() $
tweet1.created_at
udois = dataset["article_doi"].drop_duplicates() $ invalid_dois = udois[~udois.apply(re.compile(r"10(\.\d+)+/.+$").match).apply(bool)] $ invalid_dois
inflex_words = en_translation_counts[en_translation_counts < 2] $ print("Total: {} unique words".format(len(inflex_words))) $ inflex_words.sample(10)
raw_full_df['building_id_iszero'] = raw_full_df.building_id == '0'
p_new_minus_p_old = new_page_converted.mean() - old_page_converted.mean() $ print(p_new_minus_p_old)
data.info()
with open('image_predictions.tsv', 'wb') as file: $     file.write(response_tweet_image.content)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])   
plt.hist(reddit['Comments'], range=(0,150)) $ plt.xlabel('Number of Comments',fontsize='large') $ plt.ylabel('Number of Reddit Posts',fontsize='large') $ plt.title('About half are above or below the number of comments!', fontsize='large') $ plt.show()
newdf.info()
clean_appt_df['Weekday'] = clean_appt_df['AppointmentDay'].dt.weekday $ clean_appt_df.groupby('Weekday')['No-show']\ $     .value_counts(normalize=True)\ $     .loc[:,'Yes']\ $     .plot.bar()
linkNYC.crs = from_epsg(4326) $ linkNYC = linkNYC.to_crs(epsg=2263) $ linkNYC.crs
number_of_commits = len(git_log) $ number_of_authors = len(git_log.query("author != ''").groupby('author')) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
author_data.loc["leyroft"]
projects = formatted_sample_pq_saved.groupBy( $     formatted_sample_pq_saved.project_name).agg(F.first("project_name")).select("project_name")
tweet_hour['tweet_text'].apply(tweet_tokenizer.tokenize).head()
mask = mask1 | mask2 $ df2=df[~mask].copy()
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1-p_old), p_old])
index = df[(df.donor_id == '-28K0T47RF') & (df.donation_date == '2007-11-30') & (df.city == 'Cupertino')].index $ df.ix[index,'state'] = 'CA' $ index = df[(df.donor_id == '9F4812A118') & (df.donation_date == '2012-06-30') & (df.city == 'San Juan')].index $ df.ix[index,'state'] = 'WA' $ df.ix[index,'zipcode'] = 98250
 from imblearn.over_sampling import SMOTE $ sm = SMOTE(random_state=42) $ X_res, y_res = sm.fit_sample(X,y)
cats_df = cats_df[cats_df['remove']==False] $ cats_df = cats_df.drop('remove', axis=1)
dul.drop_duplicates(inplace=True) $ dul
row_to_drop = df2[df2.user_id == duplicate_userid].index[0] $ df2 = df2.drop(row_to_drop)
details['Released'] = pd.to_datetime(details['Released'])
obj.values  # array representation of the Series
cg_u= df[df['group']=='control'] $ cg_u_prob_of_conv = cg_u['converted'].mean() $ print('The probability of a control group user converting is:  ' + str(cg_u_prob_of_conv))
def developer(x): $     if 'Developer' in x: $         return 1 $     return 0 $ df_more['Developer'] = df_more['Title'].apply(developer)
top_supporters["contributor_cleanname"]=top_supporters.apply(combine_names, axis=1)
input_nodes_DF = nodes_table('network/source_input/nodes.h5', 'inputNetwork') $ input_nodes_DF[1:5]
userProfile = userGenreTable.transpose().dot(inputMovies['rating']) $ userProfile
df7 = pd.read_csv('2007.csv')
logit = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit.fit() $
week45 = week44.rename(columns={315:'315'}) $ stocks = stocks.rename(columns={'Week 44':'Week 45','308':'315'}) $ week45 = pd.merge(stocks,week45,on=['315','Tickers']) $ week45.drop_duplicates(subset='Link',inplace=True)
df_nd101_d = df_pivot_days[df_pivot_days['nd_key_formatted'] == 'nd101'] $ df_nd101_d.head()
X.fillna(0, inplace=True)
df.ix['2015-09-03 11:00:00+01:00':'2015-09-03 12:00:00+01:00'].plot()# select a time range and plot it $
attend_with.to_csv('../data/attend.csv')
df2_cntrl = df2.query('group == "control"') $ df2_cntrl_conv = df2_cntrl[df2_cntrl['converted'] == 1].count() $ total = df2_cntrl['converted'].count() $ prop_cntrl = (df2_cntrl_conv['converted'] / total) $ print(prop_cntrl)                   $
f $ mask = f >=.5 $ print(mask) $ print(f[mask])
csvData.head(5)
log_UK = sm.Logit(new['converted'],new[['intercept','country_UK']]) $ r = log_UK.fit() $ r.summary()
tweets_kyoto_filter.describe()
train.head()
cbd = CustomBusinessDay(holidays=cal.holidays()) $ datetime(2014,8,29) + cbd
df_h1b_nyc = df_h1b_nyc.drop([60049,396316,275446])
my_gempro.set_representative_sequence() $ print('Missing a representative sequence: ', my_gempro.missing_representative_sequence) $ my_gempro.df_representative_sequences.head()
data = data.sort_index() $ data
hpd.head()
ev_cumsum = pca_full.explained_variance_ratio_.cumsum() $ for i in np.arange(0.5, 0.9, 0.05): $     print('{0:.2f}: {1:}'.format(i, np.sum(ev_cumsum <= i))) $ for i in np.arange(0.9, 1.01, 0.01): $     print('{0:.2f}: {1:}'.format(i, np.sum(ev_cumsum <= i)))
np.vstack((a, a))
precip_data_df.set_index("date",drop=True,inplace=True) $ precip_data_df.columns $ precip_data_df.tail()
X = pd.get_dummies(X, columns=['subreddit'], drop_first = True)
n = 10 $ np.random.choice(n, size=n)
train_data.vehicleType.isnull().sum()
import pandas as pd $ df = pd.read_csv("software1.csv") $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ df.head()
[topics.get(c) for c in channel_meta['topic_ids_json']]
train.head()
properati.shape
df_new['strfrmt']= df_new.strfrmt.apply(lambda x: datetime.datetime.strptime(x, "%Y-%m-%d %H:%M:%S.%f"))
resdata=[] $ for i in range(len(restaurantlist)): $     resdata.append(requests.get('https://developers.zomato.com/api/v2.1/search?entity_type=city&entity_id='+str(cityid)+'&q='+restaurantlist[i],headers=headers).json()) $     time.sleep(0.3)
! ls ./data/raw-news_tweets-original/dataset1/tweets/2014-11-18/
result2.summary2()
vals2 = np.array([1, np.nan, 3, 4]) $ vals2.dtype
trans.groupby('msno').count().sort_values('is_cancel').tail()
df2.query("landing_page == 'new_page'").shape[0] / df2.landing_page.shape[0]
df2.head() # delete later
indeed.isnull().sum()
df_CLEAN1A.head(5)
duplicated_user_df = df2[df2.duplicated(['user_id'], keep=False)] $ duplicated_user_df
def load_json(value): $     tasks = json.loads(value) $     assert len(tasks) == 1 $     return tasks[0]
automl.refit(X_test.copy(), y_test.copy()) $ print(automl.show_models())
yc_new2.dropna() $ yc_new2.isnull().sum()
p3_table = profits_table.groupby(['Segment']).Profit.sum().reset_index() $ p3_result = p3_table.sort_values('Profit', ascending=False) $ p3_result.head()
actual_pnew=(df2.query('group=="treatment"')['converted']==1).mean() $ actual_pnew
secondary_temp_dat.tail(3)
p_old=df2['converted'].mean() $ p_old
test = "1 review for cleanliness" $ test.split(maxsplit=1) $ test.split(maxsplit=1)[1] $ test.split(maxsplit=1)[1].replace(' ','_')
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA', 'US']] $ df_new.head()
rounded_labels_over = [round(x, 2) for x in over_thresh_dict.keys()]
for h in heap: $     h.company = [t.author_id for t in h.tweets if t.author_id in names][0]
params = {} $ params['penalty'] = ['l1','l2'] $ params['C'] = [.001,.01,.1,.5,1,5,10]#[.03,.05,.1]
user_project = pd.read_csv('data_old/user_project.csv') $ print(user_project.shape) $ user_project.head()
control_convert = df2.query('group == "control"')['converted'].mean() $ control_convert
user_extract.account_name.value_counts().head()
pmol.df['element_type'] = pmol.df['atom_type'].apply(lambda x: x.split('.')[0]) $ pmol.df['element_type'].value_counts().plot(kind='bar') $ plt.xlabel('element type') $ plt.ylabel('count') $ plt.show()
sl_data.columns
ordered_by_susceptibility = sleep.groupby('ID').sum()['extra'].sort_values(ascending=False) $ ordered_by_susceptibility
pd.groupby(vlc, by=vlc.visited.dt.year).size() \ $ .plot(kind="bar", title="Volume of Members Visits per Year", color="green", figsize=(12,4))
1/np.exp(-0.0150)
df.loc['EUR-CHF','Value']
res.text
import pandas as pd $ def get_mean_volume(symbol): $     df = pd.read_csv("data/{}.csv".format(symbol))  # read in data $     return df['Volume'].mean() #Compute and return the mean value. $ def test_run(): $
shows = pd.read_pickle("ismyshowcancelled_raw_pull.pkl")
X = pivoted.fillna(0).T.values $ X.shape
greater = (p_diffs > actual_diff).mean() $ print('Proportion of simulated differences that are greater than actual difference:', greater)
oppose.amount.sum()
print("mean_new:",np.mean(df_concat_2.message_likes_rel))
public_tweets = api.user_timeline(target_user,count=1)
yt.get_channel_id_from_user('munchies')
tweet_archive_clean.loc[2436]
ds_ossm = xr.open_mfdataset(data_url2) $ ds_ossm = ds_ossm.swap_dims({'obs': 'time'}) $ ds_ossm = ds_ossm.chunk({'time': 100}) $ ds_ossm = ds_ossm.sortby('time') # data from different deployments can overlap so we want to sort all data by time stamp. $ ds_ossm
print(autos.price.value_counts().head(10).sort_index(ascending=True))
dfMeanFlow.head()
[ list(calendar.day_name)[i] for i in customer_visitors.DateCol.dt.dayofweek]
feed=feed.dropna(axis=1 ,how='all') $ feed=feed.dropna(axis=0 ,how='all')
skip_idx = random.sample(range(1, num_lines), num_lines - size)
young.groupBy("gender").count()
df2.info()
result_df = pd.DataFrame(test['comment_id']) $ result_df['is_fake'] = prediction[:, 1] $ result_df.to_csv(path_join(DATA_DIR, 'submission.csv'), index=False)
df_ = df_merge[df_merge['Geography'].isin(['WY', 'AZ', 'MA', 'NY'])] $ df_ = pd.crosstab(df_['Geography'], df_['Medium'])
!pip install wget $ link = 'https://github.com/meethariprasad/phd/raw/master/series.npy'
c.loc["c":"e"]
drg_string_value = '001' $ year = 2015 $ x = df_everything_about_DRGs[(df_everything_about_DRGs['drg3_str'] == drg_string_value) &\ $                         (df_everything_about_DRGs['year'] == year)].index.tolist() $ print('There are',len(x),'values for DRG',drg_string_value,'in',year)
tweets_df_clean.columns = ['tweet_id', 'retweet_count', 'favorite_count']
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value
X_train = train.drop('overdue', axis=1) $ y_train = train['overdue']
df_test['is_test'] = np.repeat(True, df_test.shape[0]) $ df_train['is_test'] = np.repeat(False, df_train.shape[0]) $ df_total = pd.concat([df_train, df_test]) $ df_total.info()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative='smaller') $ z_score, p_value $
crime_df.tail()
conf_matrix = confusion_matrix(y_tfidf_test, svc.predict(X_tfidf_test), labels=[1,2,3,4,5]) $ conf_matrix $
fin_coins_r.shape
token_text = [token.orth_ for token in parsed_review] $ token_pos = [token.pos_ for token in parsed_review] $ pd.DataFrame(zip(token_text, token_pos), $              columns=['token_text', 'part_of_speech'])
le_data = le_data_all.reset_index().pivot(index='country',columns='year') $ le_data.iloc[:,0:3]
from IPython.core.display import HTML $ display(HTML('<div style="border-style:solid; border-width: 1px; border-color: gray; padding: 10px 10px 10px;"> <b><u>Findings</u></b></br> Please note that this is a demo. In order to arrive at sound conclusions, you need a larger data set. </br> </br> The most liked tweet implies that people tend to go for brunch at McDonalds. It is interesting to analyse sales at each time during the day, in order to boost company\'s profits. The most retweeted tweet implies that the company is investing on expansions. Further analysis on the article referred by the URL shows that location is a good fit as the new McDonalds will be set up in Ohio CitiCenter. </div>'))
result1 = df[(df.A < 0.5) & (df.B < 0.5)] $ result2 = pd.eval('df[(df.A < 0.5) & (df.B < 0.5)]') $ np.allclose(result1, result2)
f_device_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_device_hour_clicks"), header=True) $ print('Found %d observations.' %f_device_hour_clicks.count())
search['trip_start_date'] = search['message'].apply(trip_start_date)
df[(df['group'] != 'treatment') & (df['landing_page'] != 'new_page')].sum()
import StringIO $ from StringIO import StringIO  # got moved to io in python3.
reg_traffic_with_flags.head()
(p_diffs > (p_new-p_old)).mean()
cust_group_date = df.groupby(['cust_id','order_date'])['cust_id'].count().reset_index(name="count") $ cust_group_date $
fld_student = np.array(np.unique(d['If I could be any type of scientist when I grow up, I would want to study:'])) $ fld_student_all = d['If I could be any type of scientist when I grow up, I would want to study:']
autos["price"].value_counts()
qqqq = pd.merge(c,d , on =['msno','transaction_date']) $ qqqq
date_null_agg_merged=pd.merge(date_null_agg,date_agg, on='date',how='left') $ date_null_agg_merged['percent_missing']=date_null_agg_merged['frequency_x']/date_null_agg_merged['frequency_y']*100 $ date_null_agg_merged=date_null_agg_merged.sort_values(by='date') $ pd.DataFrame.head(date_null_agg_merged) $ date_null_agg_merged.plot('date','percent_missing',kind='bar', color='r', figsize=(15,5))
c_df.groupby('DEVICE_MODEL').size().shape[0]
print("Number of Groups in PRE-ATT&CK") $ groups = lift.get_all_pre_groups() $ print(len(groups)) $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
cov = pyemu.Cov.from_ascii(os.path.join(new_model_ws,m.name+".pst.prior.cov"))
analyze_set['date']=pd.to_datetime(analyze_set['date']) $ analyze_set['month']=analyze_set['date'].dt.month $ analyze_set['month'].value_counts()
decade = 10 * (planets['year'] // 10) $ decade = decade.astype(str) + 's' $ decade.name = 'decade' $ planets.groupby(['method', decade])['number'].sum().unstack().fillna(0)
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(RN_PA_duration.diff()[1:], params=params, lags=30, alpha=0.05, \ $     title='Weekly RN/PA Hours First Difference Partial Autocorrelation')
new_df= preprocessing.StandardScaler().fit(new_df).transform(new_df) $ new_df[0:5]
r.headers['content-type']
df_subset.describe()
print ("Column headers:\n- " + "\n- ".join(p.columns))
df_clean.rating_denominator.value_counts()
wrongly_objected = ['L2', 'L3', 'LRank', 'Lsets', 'W2', 'W3'] $ for w in wrongly_objected: $     data.loc[:, [w]] = np.float64(data[w][:]) $ data.describe(include=['O'])
disag_hart_elec = disag_hart.buildings[building_number].elec $ disag_hart_elec
pprint.pprint(posts.find_one({"reinsurer": "AIG"}))
churned_unordered['start_date'] = pd.to_datetime(churned_unordered['scns_created'].apply(lambda x:x[np.argmin(x)])).dt.strftime('%Y-%m')
my_query = "SELECT * FROM ephys_roi_results LIMIT 1" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element.keys() $
from sklearn.model_selection import train_test_split $ train, val = train_test_split(sample, test_size=0.2, random_state=42)
roc_auc_score(preds, fb_test.popular)
train[train.age>117].age.shape
df_Q1_3.groupby('Date')['Outside Temperature'].nlargest(10)
vwg = pd.read_excel('input/Data.xlsm', sheet_name='53', usecols='A:Y', header=13, skipfooter=10)
gnb.fit(X_clf, y_clf)
recipes.name[np.argmax(recipes.ingredients.str.len())]
tree = ET.parse(file) # mocht je een andere map hebben dan kan je het pad hier aanpassen $ root = tree.getroot() $ namespaces = {'xsd':"http://www.w3.org/2001/XMLSchema", 'xsi':"http://www.w3.org/2001/XMLSchema-instance" } $
print(ca_de.shape)
%matplotlib inline $ import matplotlib.pyplot as plt $ import datetime as datetime $ df_tweet = pd.read_csv('twitter_archive_master.csv', encoding = 'utf-8')
ng_stor_json = ng_stor.response.json()
p_old = (df2['converted'] == 1).mean() $ p_old
new_page_converted = np.random.binomial(N_new, P_new) $ new_page_converted
loan_stats["credit_length_in_years"] = loan_stats["issue_d"].year() - loan_stats["earliest_cr_line"].year() $ loan_stats["credit_length_in_years"].head(rows=3)
pred_probas_under = log_reg_under.predict_proba(X_test)
print(api.rate_limit_status()['resources']['application']) $ print(api.rate_limit_status()['resources']['statuses']['/statuses/user_timeline'])
np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm) $ np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm) $ pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))
pd.get_dummies(df.A, prefix='col')
print("Number days without rain:      ", np.sum(inches == 0)) $ print("Number days with rain:         ", np.sum(inches != 0)) $ print("Days with more than 0.5 inches:", np.sum(inches > 0.5)) $ print("Rainy days with < 0.2 inches  :", np.sum((inches > 0) & $                                                 (inches < 0.2)))
pred = xgboost.predict(cvecogXfinaltemp) $ print classification_report(pred, ogy) $ print confusion_matrix(ogy, pred) $ print "cross val score accuracy :"+str(sum(cross_val_score(xgboost, cvecogXfinaltemp, ogy))/3) $ print 'roc_auc score :'+str(sum(cross_val_score(xgboost, cvecogXfinaltemp, ogy, scoring='roc_auc'))/3) $
new_page_converted = np.random.binomial(1, pnew, nnew)
stats.norm.cdf((active_mailing - inactive_mailing) / SD_mailing)
autos["brand"].value_counts(normalize=True)[:10]
logit_mod_2 = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','US']]) $ results_2 = logit_mod_2.fit() $ results_2.summary()
df_users.index
values = df[df['converted'] == 1] $ values_counts = values["converted"].count() $ values_total = df['converted'].count() $ proportion = ((values_counts / values_total) * 100).round() $ print('The proportion of users converted is {}%.'.format(proportion))
google_stock.head() $ google_stock.head(10)
poverty.head(10)
df.shape #Original Dataset
graf_test=pd.concat([graf_test, test_embedding], axis=1)
p(sys.getdefaultencoding)
bacteria_data = bacteria_data.T $ bacteria_data
updated_df = multi_col_lvl_df.copy() $ updated_df.loc[idx[:, :, 'Beer'], idx['Dollars', 'Store 1':'Store 1']] = "We changed this" $ updated_df.head(10)
alerts = soup.find_all('div', class_='content-details') $ print(len(alerts)) $ type(alerts)
brand_counts.head(4)
df.sample(2)
raw_df.replace({'\n': ' '}, inplace=True, regex=True) $ raw_df.head(5)
naimp.get_isna_ttest('age', type_test='ttest')
print('reduce memory') $ utils.reduce_memory(transactions) $ transactions.info()
df.rename(columns={'Price':'House_Price'},inplace=True) # renaming inplace i.e. in that same data frame $ df.head()
dollars_per_unit = multi_col_lvl_df['Dollars'] / multi_col_lvl_df['Units'] $ dollars_per_unit.sample(10)
reviews_sample.shape
eclf2 = VotingClassifier(estimators=[('lr', alg), ('calC', alg7), ("ranFor", alg2)], voting='soft') $ eclf2.fit(X_train, y_train) $ probs = eclf2.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
errors['datetime'] = pd.to_datetime(errors['datetime'], format="%Y-%m-%d %H:%M:%S") $ errors.count()
df_test = df_clean.loc[df_clean['rating_numerator'] > 10] $ df_test[['text','rating_numerator']].head()
print("Unique values from odometer_km column:", autos["odometer_km"].unique().shape[0]) $ print("Unique values from price column:",autos["price"].unique().shape[0])
df2.query("landing_page == 'new_page'")['landing_page'].count() / df2['landing_page'].count()
act_diff = df[df['landing_page'] == 'new_page']['converted'].mean() -  df[df['landing_page'] == 'old_page']['converted'].mean() $ (act_diff < p_diffs).mean()
payments_all_yrs.to_csv('Line_item_Site_Year.csv',index=False)
df_goog.index + timedelta(days=4380, hours=1, seconds=43)
import plotly $ import plotly.plotly as py $ import plotly.graph_objs as go $ plotly.tools.set_credentials_file(username='[INSERT USERNAME]', api_key='[INSERT API KEY]')
df = df.loc[df['statement_type']!='Flip']
image_predictions_clean.head(3)
forest_model = RandomForestClassifier() $ forest_model.fit(X_train,y_train) $ forest_y_pred = forest_model.predict(X_train)
df_arch_clean[df_arch_clean['retweeted_status_timestamp'].notnull()]
hashtag_selection = hashtags['hashtag'][hashtags['created_at'].dt.week == 3].value_counts()[0:20] $ print(hashtag_selection)
Aussie_df = Aussie_df[Aussie_df.userTimezone.notnull()] $ len(Aussie_df) $ len(Aussie_df)/100*100 $
PTrueNegative = (PNegativeTrue * PTrue) / PNegative $ "%.2f" % (PTrueNegative * 100) + '%'
Project['Date'] = pd.to_datetime(Project['Date'])
df['release_dt'] = pd.to_datetime(df['release_dt']) $ df['booking_dt'] = pd.to_datetime(df['booking_dt'])
X.shape
open('test_data//write_test.txt', mode='w')
pd.concat(df_tests).to_clipboard()
df2.user_id.drop_duplicates(inplace = True)
r.xs('UA', level=1)[['share']].plot()
demographics = my_df_free1.iloc[:,0:3] $ scores = my_df_free1.iloc[:,5:]
df = pd.read_csv('orders.csv', parse_dates=['created_at_date']) $ df.columns
builder.count()
to_be_predicted_Day5 = 26.69302484 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
(y_hat.shape, y_train.shape)
from pyspark.sql.functions import monotonically_increasing_id $ modeling1 = (train $              .withColumn('id', monotonically_increasing_id()) $              .drop('click_time', 'attributed_time')) $ print("modeling1 size: ", modeling1.count())
weather_missing = weather_all.isnull() $ weather_missing.head()
EPEXvolumes = dfEPEX[1::2].transpose().stack() # take every second row starting one row in $ EPEXvolumes.index = index # assign generated index to volume data $ EPEXvolumes.head() # verify extracted volume data
model.fit([sources], targets, epochs=10, batch_size=512, validation_split=0.1) $
data.shape, test.shape
my_df_free1.iloc[100:110]
run txt2pdf.py -o "VIDANT MEDICAL CENTER  Sepsis.pdf"   "VIDANT MEDICAL CENTER  Sepsis.txt"
df.head(50)
df_null = df[(df['group'] == 'control') & (df['landing_page'] == 'old_page')] $ df_alt = df[(df['group'] == 'treatment') & (df['landing_page'] == 'new_page')] $ df_dummy = pd.concat([df_null, df_alt]) $ df_count = df['converted'].count() - (df_null['converted'].count() + df_alt['converted'].count()) $ print (df_count) $
pd.Timestamp(2018, 4, 23)
siteFeatures = read.getSamplingFeatures(type='Site') $ df = pd.DataFrame.from_records([vars(sf) for sf in siteFeatures if sf.Latitude])
import statsmodels.api as sm $ lm = sm.Logit(df['converted'], df[['intercept', 'ab_page']]) $ results = lm.fit()
learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)
AAPL.describe()
Results_kNN1000 = Results_kNN1000[['ID', 'Approved']] $ Results_kNN1000.head()
d1 = ['2 June 2013', 'Aug 29, 2014', '2015-06-26', '7/12/16'] $ ts3 = pd.DataFrame(np.random.randint(10, 100, (4,2)), index=d1, columns=list('ab')) $ ts3
users_visits = pd.merge(users_visits, Relations, how='outer', on=['name', 'id_partner']) $ users_visits = users_visits[['visits', 'regs', 'chanel']]
test_date = input("Enter date to calculate daily normals for (use MM-DD format): ") $ normals_list = [row for row in sq.daily_normals(test_date)] $ print("\n" $       f"Daily normals for {test_date} are: \n\n" $
vader_df = pd.DataFrame(vader_scores)[['text', 'compound', 'created_at']] $ vader_df = vader_df.sort_values('compound', ascending=True) $ vader_df.head(7)
for x in tweets_clean.dog_class.unique(): $     print('Mean rating numerator for ' + str(x) + ' class:' + str(tweets_clean[tweets_clean.dog_class == x].rating_numerator.mean()))
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') #larger for > sign in alternate hypothesis $ z_score, p_value
cleaned = facilities.drop_duplicates(subset=['facname','address','idagency']) $ cleaned.info()
data_read = pd.read_csv("data_music_replay_train") $ data = data_read.copy()
print("Unique users:", len(ab_file2.user_id.unique())) $ print("Non-unique users:", len(ab_file2)-len(ab_file2.user_id.unique()))
data_activ = data_activ.groupby('new_date').count().reset_index()[['new_date','id']]
all_data_merge.groupby('brand').size()
df_t[df_t['Shipping Method name']=='DHLForYou Drop Off']['Updated Shipped diff'].hist() $ pd.DataFrame(df_t[df_t['Shipping Method name']=='DHLForYou Drop Off']['Updated Shipped diff'].describe())
flattened_pandas_df.groupby('tag').size().nlargest(20).sort_values(ascending=False)
item_categories.head()
categorical_features = df.select_dtypes(include=[np.object]) $ categorical_features.columns $
results=[] $ for tweet in tweepy.Cursor(api.search,q="vegan").items(10): $     results.append(tweet)
poly17= PolynomialFeatures(degree=17) $ X_17 = poly17.fit_transform(X) $ linear = linear_model.LinearRegression() $ linear.fit(X_17, y2) $ (linear.coef_, linear.intercept_)
Google_stock.tail()
print(result.inserted_primary_key)
paired_df_grouped.n_best_co_occurence.describe()
subwaydf.iloc[28630:28636] #this high number seems to be because entries and exits messes up
(df2.query('group == "treatment"')['converted'] == 1).sum()/(df2.query('group == "treatment"')['converted'] >= 0).sum()
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp',freq='AS')).count() $ print(commits_per_year.head())
y.value_counts()
pres_df['subject_count_tmp'].value_counts(dropna=False) $
url = 'https://raw.githubusercontent.com/yinleon/LocalNewsDataset/master/data/local_news_dataset_2018.csv' $ df = pd.read_csv(url)
spark = pyspark.sql.SparkSession(sc) $ df = spark.read.csv('../data/somenulls.csv', header=True)
cityID = '2409d5aabed47f79' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Rochester.append(tweet) 
vocab = vectorizer.get_feature_names() $ print(vocab)
stream_measures.sort_values(by=['mean_occurances'], ascending=False).head(n=15)
math.factorial(5)
year_info = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date.between('2016-08-23', '2017-08-23')).all() $ year_info_df = pd.DataFrame(year_info) $ year_info_df.set_index("date").head() $
FREEVIEW.plot_histogram(raw_fix_count_df)
X_cat = pd.concat([tfidfdummiee, skill_dummies, aldf[['salary_predict','duration_int']]], axis=1) $ X_cat.shape
df2['intercept']=1 $ df2[['old_page','ab_page']] = pd.get_dummies(df2['group']) $ df2.head()
y_pred = ridgeregcv.predict(X_test) $ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
df_clean = df.copy() $ pred_clean = pred.copy() $ tweet_df_clean = tweet_df.copy()
df.rename(columns={ $     'Ranking Full URL on Sep  1, 2017':'2017-09-01', $     'Ranking Full URL on Jan  1, 2017':'2017-01-01' $ }, inplace=True)
( $     autos["ad_created"].str[:10] $     .value_counts(normalize=True,ascending=False) $     .head(10) $ )
major_cities_l1_features[0].geometry
df_predictions['p3_dog'].value_counts()
testcols = ['site_name', 'is_mobile', 'is_package', 'channel', 'srch_adults_cnt', $                 'srch_children_cnt', 'srch_destination_type_id', 'hotel_continent'] $ dftest = pd.read_csv(dataurl+'test.csv.gz', sep=',', compression='gzip')
timestamps = [] $ for k in range(len(bird_data)): $     timestamps.append(datetime.datetime.strptime(bird_data.date_time.iloc[k][:-3], '%Y-%m-%d %H:%M:%S')) $ print(timestamps[:5])
sessions_path = '../../Data/sessions.csv' $ sessions = pd.read_csv(sessions_path) $ print(len(sessions)) $ sessions.head()
watch_table = watch_table.drop_duplicates() $ print watch_table.shape
joined_with_project_info.show()
p_diff = (new_page_converted/N_new) - (old_page_converted/N_old) $ print("The p-difference for the simulated values is: {}".format(p_diff))
df, err = ek.get_data(['GOOG.O', 'MSFT.O', 'FB.O', 'AMZN.O', 'TWTR.K'], $                       ['TR.Revenue.date','TR.Revenue','TR.GrossProfit'], $                       {'Scale': 6, 'SDate': 0, 'EDate': -2, 'FRQ': 'FY', 'Curn': 'EUR'}) $ df
df.iloc[:,0:3]
b1 = total_base_dict_by_place.sort_values(by='base_twitter_count')
print(loan_stats["revol_util"].na_omit().min()) $ print(loan_stats["revol_util"].na_omit().max()) $
trading.coint_test()
promo_df.sort_values(['item_nbr','store_nbr', 'date'], ascending=[True, True, False], inplace=True)
pivoted.head()
df_similar_items = content_rec.get_similar_items().to_dataframe() $ df_similar_items.head(20)
tickerdf.to_csv('ticker_data_LAB3.csv')
raw_df.head(5)
    X['num']= X.groupby(level=1).cumcount(ascending=False) $     clean_prices.describe
season_groups = nba_df.groupby("Season") $ team_groups = nba_df.groupby("Team") $ season_type_groups = nba_df.groupby("GameType")
print('First rows of dataset:') $ tw.head(2)
old_position=get_current_position() $ new_position=lots.ix[-1]
countries_df  = pd.read_csv('./countries.csv') $ countries_df.head()
df_goog.Open.resample('a').plot() $
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.hour).count().plot(y='Agency') $
len(df.index)  # number of total tweets
X.shape
df4=df_new.copy()
Not_same = df_3['user_id'].count() + df_43['user_id'].count() $ Not_same $
pd.get_dummies(grades_cat)
df['text'] = df['text'].apply(lambda x: x.decode("utf-8") ) $ df['datetime'] = pd.to_datetime(df['datetime'])
sents = [] $ for i in range(30): $     sents.append(' '.join([k for val in train_x[i] for k,v in words.items() if v==val])) $ sents
from pyspark.ml.feature import HashingTF $ num_hash_buckets = 2 ** 18 $ hashingTF = HashingTF(inputCol="rawFeatures", outputCol="features", numFeatures=num_hash_buckets)
top_songs['Track Name'].unique()
stateonly_df = df[(df.city == '') & (df.zipcode_initial == '')].copy() $ stateonly_df.state = '' $ noloc_df = pd.concat([noloc_df, stateonly_df]) $ df = df[~((df.city == '') & (df.zipcode_initial == ''))].copy()
df.head()
df_twitter_copy.ix[df_twitter_copy.name.isin(no_name_list), 'name'] = 'None'
print('Average Daily trading volume during 2017 is :',np.mean(dtunit))
df_EMR_dd_dummies.to_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_combined\02_EMR_combined_with_dd_with_dummies.csv') $ df_EMR_dd_dummies_no_sparse.to_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_combined\02_EMR_combined_with_dd_with_dummies_no_sparse.csv')
cell_df.columns
train_score = math.sqrt(mean_squared_error( y_train_i.reshape(1,-1)[0], X_train_predict_i.reshape(1,-1)[0])) $ print('train_score: {0}'.format(train_score)) $ test_score = math.sqrt(mean_squared_error( y_test_i.reshape(1,-1)[0], X_test_predict_i.reshape(1,-1)[0])) $ print('test_score: {0}'.format(test_score))
btc_price_df.index = pd.to_datetime(btc_price_df.index)
df.to_csv('ab_data_mod.csv')
btypedums = pd.get_dummies(data.bill_type) $ data.drop(['bill_id', 'bill_type'], axis=1, inplace=True) $ data.billtext = map(str.lower, data.billtext) $
year_tobs_df = pd.DataFrame(year_tobs, columns=['date', 'tobs'])
mars_news_url = "https://mars.nasa.gov/news/"
driver.quit() # Always remember to close your browser!
tf_idf = tfidf_vectorizer.fit_transform(df['body'])
has_text.sort_index(by='grade_levels', axis=0).tail(20)
mini = stock.cummin()
data.head()
first_result.find('a')['href'] 
df2.query('landing_page == "new_page"').count()[0] / df2['user_id'].nunique()
twitter_archive_master = twitter_archive_master.drop(['doggo','floofer','pupper','puppo'],axis = 1)
autos["odometer_km"].value_counts()
new_page_converted = np.random.choice([0,1],size = n_new, p = [1-p_new,p_new]) $ new_page_converted
df.head(3)
energy_indices.pct_change().mul(100)
nnew = len(df2.loc[(df2['landing_page'] == 'new_page')])#updated $ nnew
dark_sky = temperature_sensors_df[ $     temperature_sensors_df['entity_id']=='sensor.darksky_sensor_temperature'] $ dark_sky = dark_sky.set_index('last_changed') $ dark_sky.head()
train['visit_date']=pd.to_datetime(train['visit_date'])
reddit_body = data.drop(['created', 'is_video', 'thumbnail', 'url', 'timestamp', $                     'time_up', 'time_up_sec', 'time_up_clean'], axis=1)
image_predictions.info()
transfer_holidays=holidays_events[(holidays_events.type=='Transfer') & (holidays_events.transferred==False)] $ print("Rows and columns:",transfer_holidays.shape) $ pd.DataFrame.head(transfer_holidays)
state_party_df.shape
csvData = pd.read_csv('data_s1_ass4.csv')
crimes.PRIMARY_DESCRIPTION[5:16]
df2.reset_index(inplace=True) $ df2.drop(df2.index[df2[df2['user_id'].duplicated(keep=False)]['user_id'].index.values[:-1]], inplace=True); $
tweet_df_clean.head()
data[data['processing_time']<datetime.timedelta(0,0,0)]
corr_matrix = numerical.corr() $ fig, ax = plt.subplots(figsize=(10,10)) $ sns.heatmap(corr_matrix, ax=ax);
zip_counts = bus.fillna("?????").groupby("postal_code").size().sort_values(ascending=False) $ zip_counts.head(15)
ab_df2.loc[ab_df2.user_id.duplicated(),:]
df2['duplicate'] = df2['user_id'].duplicated() $ user_id_duplicate = df2['user_id'].loc[df2['duplicate'] == True] $ print("The one user_id repeated in df2 is : {} ".format(user_id_duplicate.iloc[0]))
data = pd.read_sql("SELECT * FROM session_privs",xedb) $ print(data) $
df_final_ = df_final.query('loc_country == country').reset_index().drop('index',axis=1) $ print(df_final_.head(5)) $ print(df_final_.shape)
import matplotlib.pyplot as plt $ from matplotlib import style $ style.use('ggplot')
p_converted = df.query('converted == 1').user_id.nunique()/num_users $ p_converted
map_estimate['betas_race_interval__'].reshape((-1,1)).T.dot(X[:,1:5].T).shape
log_mod = sm.Logit(df_combined['converted'], df_combined[['intercept', 'country_CA', 'country_UK']]) $ results = log_mod.fit() $ results.summary()
click_percentage = (train['is_click'].value_counts()[1] / train.shape[0]) * 100 $ print('Percentage of total emails that get clicked: {0:.2f}%'.format(click_percentage))
X = df_mes2.loc[0:1000000,cols] $ Y = df_mes2.loc[0:1000000,['tip_amount']]
pickle.dump(selfharmm_topic_names_df, open('iteration1_files/epoch3/selfharmm_topic_names_df', 'wb'))
master_list['Count'].describe()
for i in Train_extra.columns: $     if i not in Test_extra.columns: $         print i
df_arch.info() $
for tweet in results: $     data = {} $     data['tweet'] = tweet.text.encode('utf-8') $     data['datetime'] = tweet.created_at $     tweets.insert_one(data)
response.headers
pgh_311_data['REQUEST_ID'].resample("M").count().plot()
tags = tweets[['hour','hashtag','text']][tweets.hashtag != 'No Hashtag'] $ item = tags.groupby(['hour','hashtag']).agg('count').unstack(1) $ yt = np.arange(0,500,50) $ xt = np.arange(0,24) $ item.plot(subplots=True, figsize=(10,10),yticks=yt, xticks=xt)
p_new=df2['converted'].mean() $ print("p_new under null:  " + str(p_new)) $
y_pred=knn_10.predict(X_test)
QUIDS_wide $ QUIDS_wide.dtypes $ QUIDS_wide["qstot_0"] = pd.to_numeric(QUIDS_wide["qstot_0"], errors='raise', downcast='integer') $ QUIDS_wide["y"] = pd.to_numeric(QUIDS_wide["y"], errors='raise', downcast='integer')
def classify(train_features,train_labels,test_features): $     clf = SVC(kernel='rbf', verbose=True) $     clf.fit(train_features, train_labels) $     print("\ndone fitting classifier\n") $     return clf.predict(test_features)
dpth = os.getcwd() $ dbname_sqlite = "ODM2_Example2.sqlite" $ sqlite_pth = os.path.join(dpth, os.path.pardir, "data/expectedoutput", dbname_sqlite)
print('Number of SLPs that have declared at least one statement: ' + str(len(active_psc_statements[active_psc_statements.company_number.isin(slps.CompanyNumber)].company_number.unique()))) $ print('Proportion of SLPs in active companies dataset that declared a statement: ' + str(round(len(active_psc_statements[active_psc_statements.company_number.isin(slps.CompanyNumber)].company_number.unique()) / len(slps.CompanyNumber.unique()),2)))
tlen_k2 = pd.Series(data=kelsey['len'].values, index=kelsey['Date']) $ tfav_k2 = pd.Series(data=kelsey['Likes'].values, index=kelsey['Date']) $ tret_k2 = pd.Series(data=kelsey['RTs'].values, index=kelsey['Date'])
lr_model_newton.fit(X_train, y_train)
writer= pd.ExcelWriter('results.xlsx') $ resdf.iloc[:,114:129].to_excel(writer,'Demo') $ survey.to_excel(writer,'Survey') $ writer.save()
offseason07["Category"] = "2007 Offseason" # This assigns a season identifier to the transactions.
n_new = len(df2_treatment.index) $ n_new
control = df2[df2["group"] == 'control'] $ control_conv = control[control["converted"] == 1] $ control_conv_prob = control_conv.shape[0]/control.shape[0] $ control_conv_prob
feature_imp = pd.read_csv(r"C:\Users\nkieu\Desktop\Python\Loan data\2018-04-02\Feature importance 0420.csv") $ xx = feature_imp.loc[feature_imp.mult_gbm < 6, ['name','mult_gbm']] $ X_reduced = X[xx.name.values] # shape is around 42 $ X_test_reduced = X_test[xx.name.values] $ X_reduced.shape $
pd.concat([per_tweet_archive_by_month, ave_ratings_over_time], axis=1).plot(subplots=True)
loss = 10 / np.linspace(1, 100, a.size) $ loss.shape
gdax_trans['Timestamp'] = pd.to_datetime(gdax_trans['Timestamp'], format="%d/%m/%Y %H:%M:%S")
access_logs_df = access_logs_parsed.toDF() $ access_logs_df.printSchema()
scores = scores[['date','home team', 'home pts', 'away team', 'away pts', $                  'winner', 'home team 41 game win%', 'home team 8 game win%', $                  'away team 41 game win%', 'away team 8 game win%',]] $ scores.head(3)
unseen_predictions_df.to_csv('/Users/mattelgazar/Desktop/unseen_predictions.csv')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
stocks = stocks.rename(columns={'Close':'Opening Price','Date':'Time'}) $ opening = pd.merge(stocks,ideas,on=['Time','Tickers']) $ opening.drop_duplicates(subset='Link',inplace=True)
weather2.head()
scattering_to_total = scattering.xs_tally / total.xs_tally $ scattering_to_total.get_pandas_dataframe()
github_data.shape
tweet_df_clean.info()
df3[['other','ab_page']]= pd.get_dummies(df3['group'])
terms_hash = [term for term in list(itertools.chain.from_iterable(obama_cleaned_words)) $               if term.startswith('#')] $ count_hash = Counter() $ count_hash.update(terms_hash) $ print(count_hash.most_common(5))
os.listdir()
display(data[sets_columns_names].head())
n_new = df2[df2['landing_page']=='new_page']['user_id'].nunique() $ n_new 
p_old = (df2.query('converted == 1')['user_id'].nunique())/(df2.user_id.nunique()) $ p_old
net_loans_exclude_US_outstanding_user.head()
batting_and_salary = batting_df2.join(salary_df2)
w_counts = Counter(lemmatized_words) $ df = pd.DataFrame(w_counts.most_common(100), columns=['Word', 'Count']) $ df.to_csv('word_counts.csv')
dataframe = pd.DataFrame(np.random.randn(10,5)) $ dataframe
breaches.DataClasses.value_counts()
import mcpi $ from mcpi.minecraft import Minecraft
Lab7.head()
week12 = week11.rename(columns={84:'84'}) $ stocks = stocks.rename(columns={'Week 11':'Week 12','77':'84'}) $ week12 = pd.merge(stocks,week12,on=['84','Tickers']) $ week12.drop_duplicates(subset='Link',inplace=True)
sample_fractions = filtered_active_sample_sizes.withColumn( $     "sample_fraction", $     (filtered_active_sample_sizes.sample_size_1+0.5) / filtered_active_sample_sizes.author_count)
dr = pd.date_range('1/1/2010', periods=3, freq=3 * pd.datetools.bday)
from sklearn.model_selection import train_test_split $ X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train_knn set:', X_train_knn.shape,  y_train_knn.shape) $ print ('Test_knn set:', X_test_knn.shape,  y_test_knn.shape)
df3=df2.copy()
url = 'https://maps.googleapis.com/maps/api/distancematrix/json?origins={}&destinations={}&key={}'.format(origin, dest, key)
station_counts = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                 order_by(func.count(Measurement.tobs).desc()).all() $ for station, count in station_counts: $     print(f"Station: {station}, Observations: {count}")
style.use('ggplot')
story_sentence = f'Of {record_count:,} licensed debt collectors in Colorado, {action_count:,} ({pct_whole:0.2f}%) have been subject to some form of legal or administrative action, according to an analysis of Colorado Secretary of State data.' $ print(story_sentence)
df.columns.values # underlying values are numpy.ndarray
dedups['notRepairedDamage'].fillna(value='not-declared', inplace=True) $ dedups['fuelType'].fillna(value='not-declared', inplace=True) $ dedups['gearbox'].fillna(value='not-declared', inplace=True) $ dedups['vehicleType'].fillna(value='not-declared', inplace=True) $ dedups['model'].fillna(value='not-declared', inplace=True)
response = requests.get(url2)
len(features.columns)
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set()
df2.head()
new_page_convertered = np.random.binomial(1, p=p_new, size=n_new)
final_topbikes['Distance'].count()
new_ticket = 'It seems like I have followed the set up directions but I see the set up WEPAY \ $                 and am not sure if I have completed everything..thank you' $
pdiff = new_page_converted.mean() - old_page_converted.mean() $ pdiff
conv_mean = df2.converted.mean() $ conv_mean
unicode_tokens = word_tokenize(demoji.lower()) $ print(unicode_tokens)
df = pd.read_csv('data/test1.csv', parse_dates=['date'], index_col='date') $ df
df_geo_unique = df_geo.drop_duplicates() $ df_geo_unique.shape
def indexed_return(returnvector): $     r = returnvector.replace([np.inf, -np.inf], 0) $     r = r.fillna(0) $     return 100*np.exp(r.cumsum())
tweets.columns
rounds = 10 $ steps = [('mapper', mapper),('XGBClassifier', XGBClassifier)] $ pipeline = sklearn.pipeline.Pipeline(steps) $ model = (pipeline.fit(X_train, y_train, XGBClassifier__eval_metric='error', XGBClassifier__early_stopping_rounds=rounds, $         XGBClassifier__eval_set=[((mapper.fit_transform(X_train), y_train)),(mapper.fit_transform(X_test), y_test)]))
scores.IMDB.describe()
print("123".isdigit()) $ print("1X3".isdigit()) $ print("NOOOOooo".isupper())
f.all_filters
autos['date_crawled'] = autos['date_crawled'].str[:10] $ date_crawled_count_norm = autos['date_crawled'].value_counts(normalize=True, dropna=False) $ date_crawled_count_norm.sort_index()
df_never_moved['Long'].value_counts().head(5)
Feature=df[['Principal','terms','age','Gender','weekend']] $ Feature=pd.concat([Feature,pd.get_dummies(df['education'])], axis=1) $ Feature.drop(['Master or Above'], axis = 1,inplace=True) $ Feature.head() $
b2 = a.sort_values('id').tail(20)
events.where( col("user_id").isNull() ).show()
sorted(scores.loc[168],reverse = True)
price_outliers = autos.loc[expensive, :].index
query = "SELECT DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) as created_date, temp, wdsp, mxpsd, gust, max, min, prcp, sndp, snow_ice_pellets FROM `bigquery-public-data.noaa_gsod.gsod20*` WHERE _TABLE_SUFFIX BETWEEN '10' AND '17' AND stn = '725053' AND wban = '94728'" $ weather = read_gbq(query=query, project_id='opendataproject-180502', dialect='standard')
asf_agg_by_gender_and_proj_df.select("*").show()
sh_max_df.dtypes
p = re.compile('^(is)') $ bicols = [c for c in df.columns if p.search(c)] $ bicols
df = pd.read_csv('ab_data.csv') $ df.head()
high_dollar_volume = dollar_volume.percentile_between(90,100) $ latest_close = USEquityPricing.close.latest $ above_20 = latest_close > 20 $ is_tradeable = latest_close & above_20
!head data/test_users.csv
print 'https://{}:8888/'.format(dns)
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country']) $ lm = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ results = lm.fit() $ results.summary()
fi = pd.DataFrame({'cols':col_names, 'imp':gb.feature_importances_}) $ fi = fi.sort_values('imp', ascending =False)[:15] $ fi
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=870) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
sns.distplot(df['comms_num']); $
kimanalysis.listfiles(model)
df = train[columns].append(test[columns])
agency_borough.size().unstack().plot(kind='bar')
beforeUsers=fullDf[fullDf.index<pd.datetime(2015,4,25)].user
old_folder = folder + "\\last batch" $ old_TC_file = [f for f in os.listdir(old_folder) if re.match('.*TC.*', f)][0] $ old_TC_file = old_folder + "\\" + old_TC_file $ old_TC_file
ttDaily.head()
df_experiment.info()
loaded_text_classifier = TextClassifier.load(model_file) $ from tatk.feature_extraction import NGramsVectorizer $ word_ngram_vocab = NGramsVectorizer.load_vocabulary(word_vocab_file_path) $ char_ngram_vocab = NGramsVectorizer.load_vocabulary(char_vocab_file_path)
archive_df_clean=archive_df_clean[archive_df.in_reply_to_status_id.isnull() == True]
(df.set_index('STNAME').groupby(level=0)['CENSUS2010POP'] $     .agg({'avg': np.average, 'sum': np.sum}))
properties.head(1)
from statsmodels.tsa.stattools import acf, pacf
p_aux=[] $ for i in range(0,l2): $     if i not in seq: $         p_aux.append(priority_num[i]) $ col.append(np.array(p_aux))
a = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 9]]) $ c = np.array([[1], [2], [3]]) $ print(a) $ print(c)
move_34p23s34p = (breakfastlunchdinner.iloc[1, 1] $                + breakfastlunchdinner.iloc[3, 2] $                + breakfastlunchdinner.iloc[1, 3]) * 0.002 $ move_34p23s34p
df.describe()
df1.plot( kind='line', x='timestamp', y='Polarity',title='Polarity by date') $ axes = plt.gca() $ plt.xticks(rotation='vertical', fontsize=11) $ plt.show()
points.loc[:, 'c1']
nrows = df.shape[0] $ print(nrows)
temp['c'] = temp['contents'].str.split()
assert (ebola >= 0).all().all()
df['comments'] = df['comments'].str.replace(',', '')
autos_p['registration_year'].value_counts().sort_index(ascending = False).head(10)
df.head()
learn.load_encoder('lm1_enc')
result = requests.get(url) $ c = result.content $ soup = BeautifulSoup(c, "lxml")
challenge_contest = challenge_contest.drop_duplicates()
df.head()
SVPOL(data/'realdata'/'MV8.dat').to_dataframe().head()
scr_retention_df.head()
props.head()
interactions_deleted = pd.read_csv(folderData + 'interactions_deleted.csv')
X.info()
%%sql $ UPDATE facts $ SET Status_Change_Date_key = hour.hour_key $ FROM hour $ WHERE TO_CHAR(facts.status_change_date, 'YYYY-MM-DD HH24:00:00') = hour.hour; $
tvec = TfidfVectorizer(stop_words='english') $ X_train_counts = tvec.fit_transform(X_train) $ X_test_counts = tvec.transform(X_test)
r = requests.get(word_docx, stream=True) $ d = docx.Document(io.BytesIO(r.content)) $ for paragraph in d.paragraphs[:7]: $     print(paragraph.text)
forecast = prophet_model.predict(future_dates)
news_df.to_csv('output/news_analysis.csv')
[media['media_url'] for media in tweets[0]._json['entities']['media'] if media['type'] == 'photo']
my_data.to_csv('examples/simple/data/fixed_process_data.csv', index=False)
dfTransacciones.shape
from sklearn.feature_extraction.text import CountVectorizer $ countvec = CountVectorizer() $ sklearn_dtm = CountVectorizer().fit_transform(df.body) $ print(sklearn_dtm)
df.reset_index(inplace=True)
old_page_converted = np.random.choice([0, 1], size=n_old, p=[converted_rate, 1 - converted_rate])
squares.head(2)
print("Number of duplicated rows: "+ str(df.duplicated().sum())) #There are no duplicated rows $ print("Number of duplicated user id's: " + str(df["user_id"].duplicated().sum())) $ df.loc[df["user_id"].duplicated(keep = False), :] 
x = store_items.isnull() $ print(x)
s.iloc[2:3]
new_page_converted = np.random.choice([0,1],n_new,p_new); $ new_page_converted
top_20 = data['Company'].value_counts()[0:20] $ top_20[::-1].plot(kind='barh');
file_name = 'LSTM_stock_prediction_{}.h5'.format(str(datetime.date.today())) $ model.save(file_name)
df.query('landing_page == "new_page" and group == "control"').count()[0] + df.query('landing_page == "old_page" and group == "treatment"').count()[0]
%run returns.py
Path=f'data/' $ train_path=Path+"train.csv" $
df['id'].value_counts()    # this will help us to see if there is repetition on the titles $
print(wcPerf1_df.loc[wcPerf1_df['Score'] == 4])
autos[autos["price"].between(50,1000000)]["price"].\ $ value_counts()
test_df.isnull().sum()
tickerdata = tickerdata.ix['Close']
geocoded_addresses_df = addresses_df.reset_index()[['Case.Number', 'Defendant.Addr.Line.1']].reset_index().join(stacked_geo_df.set_index('case_idx')) $ geocoded_addresses_df.to_csv('geocoded_addresses.csv', encoding='utf-8') $ geocoded_addresses_df.head()
predictions = client.deployments.score(scoring_url, scoring_data) $ print("Scoring result: " + str(predictions))
dataframe['year'] = dataframe.index.year $ dataframe['day_of_month'] = dataframe.index.day $ dataframe['month'] = dataframe.index.month $ dataframe['day_of_week'] = dataframe.index.dayofweek $ dataframe['quarter'] = dataframe.index.quarter
comps.drop_duplicates(['entity_uuid','competitor_uuid'],inplace = True)
B2_NTOT_WINTER_SETTINGS.settings.mapping_water_body['N m Bottenvikens kustvatten']
condos.info()
print(autos["price"].unique().shape) $ print(autos["price"].describe()) $ autos["price"].value_counts().head(20)
pro_result['MY'] = pro_result['Month Name'] + pro_result['Year'].astype(str) $ pro_result
new_page_converted=np.random.choice([0,1],size=145310, p=[mean2, 1-mean2])
soup.find("span", {"class":'next-button'}).a['href']
n_old = df_control.nunique()['user_id']
pres_df.shape
df = pd.read_csv('../input/crime.csv') $ df.head()
df1 = tier1_df.reset_index() $ df1 = df1.rename(columns={'Date':'ds', 'Incidents':'y'}) $ df_orig = df1['y'].to_frame() $ df_orig.index = df1['ds'] $ n = np.int(df_orig.count())
p_diffs = [] $ new_sim = np.random.binomial(n_new,p_new,10000)/n_new $ old_sim = np.random.binomial(n_old,p_old,10000)/n_old $ p_diffs = (new_sim - old_sim) $ p_diffs = np.array(p_diffs)
df.drop(df.query("group != 'treatment' and landing_page == 'new_page'").index, axis = 0,inplace = True) $ df.drop(df.query("group == 'control' and landing_page != 'old_page'").index, axis= 0,inplace = True) $ df.drop(df.query("group != 'control' and landing_page == 'old_page'").index,axis = 0,inplace = True)
googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0] $ googletrend['State'] = googletrend.file.str.split('_', expand=True)[2]
liberiaFileList = glob.glob("Data/ebola/liberia_data/*.csv") $ liberiaFrameList = [pd.read_csv(file,usecols=['Variable','Date','National'],index_col=['Variable','Date']) for file in liberiaFileList] $ len(liberiaFileList)
adj_close_acq_date['Date Delta'] = adj_close_acq_date['Date'] - adj_close_acq_date['Acquisition Date'] $ adj_close_acq_date['Date Delta'] = adj_close_acq_date[['Date Delta']].apply(pd.to_numeric)  $ adj_close_acq_date.head()
list_of_issues_dict_data = [json.loads(line) for line in open('SPM587SP18issues.json')]
df.shape
iter_visits = pd.read_csv(visits_path, iterator=True, chunksize=1000000) $ users_visits = Users.assign(visits=lambda x: 0) $ users_visits.head() $
df2[df2.landing_page == 'new_page'].shape[0]/ df2.shape[0]
new_conv_rate = df2.query('group == "treatment"')['converted'].mean() $ new_conv_rate
reviewsDFslice.tail(50)
run txt2pdf.py -o"2018-06-18  2015 291 disc_times_pay.pdf"  "2018-06-18  2015 291 disc_times_pay.txt"
while raw_text.find(' KPMG Advisory N.V ., registered') > -1: $     raw_text = raw_text.replace(raw_text[raw_text.find('KPMG Advisory N.V ., registered'):raw_text.find('thumbnail.jpeg')+len('thumbnail.jpeg')],'')
reg_traffic_with_flags['Time_stamp'] = reg_traffic_with_flags['ts'].apply(lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(pd.to_numeric(x))))
test_df = dogscats_df.loc[dogscats_df['pred_00'].notnull()] $ test_df.loc[test_df['pred_00'] != test_df['label'], 'imagePath']
plt.scatter(X2[:, 0], X2[:,1], c=dayofweek, cmap='rainbow') $ plt.colorbar()
X_test = ["this is an innocent comment", "stfu baddie", "shit fuck piss"] # create list of strings to test $ comment_df = pd.DataFrame(data=X_test,columns=["comment_text"]) # create dataframe from this list for later $ X_test=tok.texts_to_sequences(X_test) $ x_test=sequence.pad_sequences(X_test,maxlen=maxlen) $ predictions = model.predict(x_test, batch_size=1024, verbose=1)[0:len(X_test)]
df_sentiment[df_sentiment["len_user_mentions"] == 50].head()
df.loc[:, ["last_name"]]
lm = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ r = lm.fit()
mean_new=df2[df2['converted']==1].count()[0]/df2.shape[0] $ mean_new
X.shape
call.iloc[-1]
data.info()
new_order.isnull().sum(axis=0)
subs_and_comments['thanks'].fillna(False, inplace=True)
Base = automap_base() $ Base.prepare(engine, reflect=True) $
repeated_user_id=df2.user_id[df2.user_id.duplicated()].values[0] $ repeated_index=df2[df2.user_id.duplicated()].index.tolist()[0] $ print('The repeated user id is '+str(repeated_user_id)+' and index is '+str(repeated_index)+'.')
events['payload'] = events['payload'].str.encode('ascii', 'ignore') $ events['payload'] = events['payload'].apply(json.loads) $ events[['repo_id', 'user_id', 'archive_id']] = events[['repo_id', 'user_id', 'archive_id']].applymap(pd.to_numeric) $ events['created_at'] = events['created_at'].apply(pd.to_datetime) $ events.shape
related_docs_indices = results.argsort()[::-1][1:11]
df2[df2['GrossOut'] > df2['GrossIn']].index
os.chdir("../data/pubs") $ os.listdir()
public_tweets.head()
history_data = prepare_data(intervention_history_df).drop(drop_columns, axis=1)
ts.tshift(5, freq='D')
kfpd.plugin = DataSynthesizerPlugin(mode='independent_attribute_mode') $ fdf = time_method(kfpd, verbose = True, rerun_query = False, repetitions = 10) $ fdf.head()
feature_layer.properties.drawingInfo.renderer.type
cols = ['dow', 'created', 'is_weekend'] $ df[cols].head()
duplicated_sites = all_sites_with_unique_id_nums_and_names['id_num'].duplicated() $ indices_duplicated_sites = [i for i, x in enumerate(duplicated_sites) if x] $ print('There are',len(indices_duplicated_sites),'sites that have duplicates') $ print('For example:')
df2['intercept'] = 1 $ df2['ab_page'] = df2['group'].map({ 'treatment': 1, 'control': 0 })
print("Number of Techniques in ATT&CK") $ techniques = lift.get_all_techniques() $ print(len(techniques)) $ df = json_normalize(techniques) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'data_sources'], axis=1)[0:5]
logit = sm.Logit(df2['converted'],df2[['intercept' ,'treatment']]) $ results = logit.fit()
MyList2 = np.array(MyList) $ %timeit np.max(MyList2)
sales_update = sales.ix[:,'2015 Sales':] $ sales_update.head()
x_test = scaler.transform(x_test)
learn = ConvLearner.pretrained(arch, data, precompute=True)
nt["Date"] = pd.to_datetime(nt.date, format="%Y-%m-%d", errors='ignore')
party_crosstab = party_crosstab.reset_index() $ party_crosstab.columns = party_crosstab.columns.droplevel(0) $ party_crosstab = party_crosstab.rename_axis(None, axis=1) $ party_crosstab = party_crosstab.set_index("")
df.columns
top_brands_mean_prices = pd.Series(top_brand_prices).sort_values(ascending=False) $ top_brands_mean_mileages = pd.Series(top_brand_mileages).sort_values(ascending=False)
p_stats = pd.DataFrame() $ for i in range(1986, 2017): $     tmp = pd.read_csv("main_players_{}.csv".format(i)) $     tmp['season'] = i $     p_stats = p_stats.append(tmp)
df = condensed_xs.get_pandas_dataframe(xs_type='micro') $ df
bigdf_read.shape
GBR.score(X_train,Y_train)
def pandas_smape(df): $     df.fillna(0, inplace=True) $     df["SMAPE"] = 200 * np.abs(df["Visits"] - df["pred_Visits"]) / (df["Visits"] + df["pred_Visits"]) $     df["SMAPE"].fillna(0, inplace=True) $     return np.mean(df["SMAPE"])
f_close_clicks_train = spark.read.csv(os.path.join(mungepath, "f_close_clicks_train"), header=True) $ f_close_clicks_test = spark.read.csv(os.path.join(mungepath, "f_close_clicks_test"), header=True) $ print('Found %d observations in train.' %f_close_clicks_train.count()) $ print('Found %d observations in test.' %f_close_clicks_test.count())
fp7 = fp7_proj.merge( $     fp7_part, $     on=["projectRCN","projectID","projectAcronym"] $ )
import matplotlib.pyplot as plt $ from matplotlib import style $ %matplotlib inline
pd.Series([1,2,9])
path_leaderboard = '../metadata/leaderboards.json' $ leaderboard = pd.read_json(path_leaderboard) $ leaderboard['best'] = pd.to_numeric(leaderboard['best'], errors='coerce') $ leaderboard.head()
autos.describe(include='all') $ carname = pandas.Series(autos["name"]) $ autos["car_model"] =autos["name"] + "-" + autos["model"] $ autos["car_model"].value_counts() $
results = pd.concat([pd.Series(preds).reset_index(drop=True), Y_test.reset_index(drop=True)], axis = 1) $ results.columns = ["predicted", "actual"] $ results["diff"] = (results["predicted"] - results["actual"])/results["actual"]
data.groupby('category').position.mean()
total_users = df2['user_id'].nunique() $ total_users
df_new = transactions.join(users.set_index('UserID'), on='UserID', how = 'left') $ df_new[df_new["User"].isnull()]
class Bug: $         return 'an'if 'aeiouAEIOU'.find(name[0]) >= 0 else 'a' $     def article(self):  # 'public' instance method $
a1.b=0
X_native = psy_native.drop(labels=["y"], axis = 1) $ y_native = psy_native["y"] $ X_train, X_test, y_train, y_test = train_test_split(X_native, y_native, random_state=1) $ feat_names_Series = pd.Series(X_train.columns) $ feat_type =feat_names_Series.apply(lambda x: 'Categorical' if x in cat_vars else 'Numerical') $
from dateutil.relativedelta import relativedelta
converted = (df.query('converted == 1').count()[0]) $ total = df.shape[0] $ print ("The number of user converted is {}".format(float(converted) /  float(total)))
df_clean['dog_nick'] = df_clean[['doggo','floofer','pupper','puppo']].mask(df_clean[['doggo','floofer','pupper','puppo']].eq('None')).fillna('').sum(axis=1)
html = html.decode('utf-8') $ print(html)
tweet_archive_clean['name'] = tweet_archive_clean['name'].replace('None', np.NaN)
df2.drop_duplicates(inplace=True)
pred_probas_under_fm = gs_from_model_under.predict_proba(X_test) $ fm_bet_under = [x[1] > .6 for x in pred_probas_under_fm]
from scipy import stats $ resid = model_arima121.resid $
festivals.at[2,'longitude'] = -87.7035663 $ festivals.head(3) $
likes.groupby(by='content').size()
%timeit pd.read_sql(f'explain {sql_sub}', engine).head()
all_df.onpromotion.fillna(False, inplace=True)
df.iloc[[0, 3], 1] = np.NaN  # Not a number $ df
hours.shape
np.median(investors_df.investment_count)
bc_consumption.day = pd.to_datetime(bc_consumption.day,infer_datetime_format=True) $ plot_period(bc_consumption,None,day='2016-09-07',pd=3, meters = meters)
X_test = data3[302:].drop('sales', axis = 1)
for p in plist: $     print(p.contents[0].strip())
sm.Logit(complete_df['converted'], complete_df[['intercept', 'ab_page', 'CA', 'UK', 'treatment_CA', 'treatment_UK']]).fit().summary()
res=AgglomerativeClustering(linkage='ward', n_clusters=3).fit(crosstab_transformed)
counts.reset_index(inplace=True) $ counts.head()
a.upper()
results = AlgorithmResultsList(res_list) $ results.add_benchmark('../data/benchmark/IBOV.csv') $ results.plot()
x=df2[['intercept','new','ab_page']] $ y=df2['converted'] $ x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)
cleanedAutos=autos $ cleanedAutos.isnull().sum()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer()
response = open_prs.get_cardinality("id_in_repo")\ $                    .by_authors("author_name")\ $                    .fetch_aggregation_results()['aggregations'] $ open_prs_by_authors = response['0']['buckets'] $ print(buckets_to_df(open_prs_by_authors).head())
client = Socrata("data.cityofnewyork.us", $                  MyAppToken, $                  username=username, $                  password=password)
injury_df.sort_values('Date', ascending=False).head()
def normalize_user_properties_column(df): $     df_user_properties = json_normalize(data=df.user_properties) $     df_user_properties = df_user_properties.set_index(df.index) $     standardize_column_names(df_user_properties) $     return df.merge(df_user_properties, left_index=True, right_index=True) $
nitrodata['MonitoringLocationIdentifier'].nunique()
plt2 = results["diff"].hist(range=[-0.5, .5], density=True, cumulative=True, figsize=(8, 4))
df_prep16 = df_prep(df16) $ df_prep16_ = pd.DataFrame({'date':df_prep16.index, 'values':df_prep16.values}, index=pd.to_datetime(df_prep16.index))
collocate = lambda model, obs: model.load().sel( $     lon=obs.lon, lat=obs.lat, time=obs.time, method='nearest' $ )
search_booking = search1[search1.booking == 1] $ search2 = search1.append([search_booking]*4,ignore_index=True)
googletrend.head()
new_page_converted=np.random.binomial(n=1,p=pnew,size = n_new) $ new_page_converted
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ with open('image_predictions.tsv', 'wb') as file: $     file.write(response.content) $ predictions = pd.read_csv('image_predictions.tsv', sep='\t')
data = pandas.read_csv("MSFT.csv", index_col='Date') $ data.index = pandas.to_datetime(data.index)
excelDF.groupby(['Ship Mode','Region']).Sales.mean()
np_x = np.random.rand(1000) $ np_target = 0.96*np_x + 0.24
to_be_predicted_Day3 = 38.49419985 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df2.country.unique()
users = context.load("s3n://path/to/users.json", "json")
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ p_diffs[:10]
contract_history.head(50)
no_outliers_forecast_exp[(no_outliers_forecast_exp.index >= '2018-06-05') & (no_outliers_forecast_exp.index <= '2018-12-31')].astype(int)
jobs_data['clean_description'] = jobs_data['record.description'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", str(x)).split() if i not in s_words]).lower())
df.head(5)
twitter_final = pd.read_csv('twitter_archive_master.csv',encoding='latin-1')
startups_USA.to_csv('data/startups_pre_processed.csv')
tokens = nltk.regexp_tokenize(text.lower(), '[a-z]+') $ tokens[:10]
df2['intercept'] = 1 $ df2['ab_page'] = 0 $ df2.loc[(df2["group"] == "treatment"), "ab_page" ] = 1
data.iloc[[1,10,3], [2, 3]]
null_values = np.random.normal(0, p_diffs.std(), 10000) $ plt.hist(null_values) $ plt.axvline(pop_diff, c='red') $ plt.show()
count = 0 $ for item in df.health: $     count += item $ percentage_health = count/(len(df.health)) $ print(percentage_health)
assert sorted(troll_tweets.columns) == sorted(expected_tweet_cols) $ assert sorted(pol_tweets.columns) == sorted(troll_tweets.columns)
df2['intercept'] = 1 $ df2['ab_page'] = df2.apply(lambda row: 1 if row.group == 'treatment' else 0, axis=1) $ df2.head()
print(23 / 5)     $ print(23 / 3.0)   # In Python 2, you must divide int / float -> float $ print(23 // 5.0 ) # explicit integer division $ print(23 % 5)     # remainder $ print(2 ** 7)     # 2 to the power of 7
df = pd.DataFrame(['05SEP2014:00:00:00.000'],columns=['Mycol']) $ df['Mycol'] = df['Mycol'].apply(lambda x: dt.datetime.strptime(x,'%d%b%Y:%H:%M:%S.%f')) $ df
print(archive_copy.shape[0], predictions_copy.shape[0], tweet_data_copy.shape[0])
import matplotlib.cm as cm $ dots_c, vhlines_c, *_ = cm.Paired.colors
dusseldorf.info()
df = pd.DataFrame(stationtotals)
print("Probability of treatment group converting:", $       ab_file2[ab_file2['group']=='treatment']['converted'].mean())
vi['ENDDATE'] = pd.to_datetime(vi['ENDDATE'], format='%Y-%m-%d %H:%M:%S')
xplot = np.linspace(0.5,2.5,21) $ yplot = f(xplot) $ plt.plot(xplot, yplot) $ plt.plot(min_result.x,min_result.fun,'ro') $ plt.show()
store_items.fillna(method = 'backfill', axis = 1)
LT906474.head(2)
(null_value > obs_diff).mean()
df.is_shift.value_counts()
country = pd.read_sql_query('select * from country', engine) $ country.head()
import matplotlib.pyplot as plt $ %matplotlib inline $ df.plot()
num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl') $ num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')
cust_data.plot(kind='scatter', x='RevolvingUtilization', y='MonthlyIncome', title = 'Scatterplot', color='R')
predicted = model2.predict(x_test) $ print predicted
top20.sort_values(by = 'mean_price', ascending = False)
merged_df = df_ad_state_metro_2.join(df_state_victory_margins)
injury_df.DL_length.value_counts()
mnb.fit(tfidf_X_train, tfidf_y_train) $ print('Training set score:', mnb.score(tfidf_X_train, tfidf_y_train)) $ print('\nTest set score:', mnb.score(tfidf_X_test, tfidf_y_test)) $ print('\nCross Val score:',cross_val_score(mnb, tfidf_X_test, tfidf_y_test, cv=5))
tweet_archive_enhanced_clean.info()
X.head(5)
df_birth.Continent.value_counts(dropna=False)
diffs = [] $ new_crs = [] $ old_crs = [] $ size = df2.shape[0]
func_vals = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).filter(Measurement.station == "USC00519281").all() $ func_vals $
clean_archive.head(10)
np.random.seed(500) $ x=np.linspace(1,10,100)+ np.random.uniform(low=0,high=.5,size=100)   #purpose of this command???? $ y=np.linspace(1,20,100)+ np.random.uniform(low=0,high=1,size=100) $ print ('x = ',x) $ print ('y= ',y)
browser.visit(url) $ html = browser.html
df.xs(key='x', level=2)
print(soup.prettify())
import os $ os.environ['BIGML_USERNAME'] = "EFETOROS" $ os.environ['BIGML_API_KEY'] = "7e5fc6a649fd0f8517fc8ecf2ebd30151c5d4fb4"
merge[merge.columns[37:40]].head(3)
df_countries = pd.read_csv('./countries.csv') $ df_countries.head(2)
print(f"lowest_temp = {min(tobs_data)} \ $         highest_temp = {max(tobs_data)}\ $         avg_temp_of_most_active_station = {np.mean(tobs_data)}")
df_mas.timestamp = pd.to_datetime(df_mas.timestamp) $ df_mas.retweeted_status_timestamp = pd.to_datetime(df_mas.retweeted_status_timestamp)
df['operator'].unique()
pd.scatter_matrix(cust_demo.select_dtypes(include = ['number']), color='k', alpha=0.5, figsize=(12, 6)) $ tight_layout()
All_tweet_data_v2.rating_numerator[All_tweet_data_v2.rating_numerator>30].value_counts() $
lines.take(10)
dollar_volume_decile = AverageDollarVolume(window_length = 10).deciles() $ top_decile = (dollar_volume_decile.eq(9)) $
wks1 = gc.open("00 forecasting").worksheet('Daily') $ wks2 = gc.open("00 forecasting").worksheet('Weekly')
with open(os.path.join(folder_name,url.split('/')[-1]) ,mode='wb') as file: $     file.write(response.content)
!head data/age_gender_bkts.csv -n 25
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn as sns
diff_of_means = dfNiwot["TMAX"].mean() - dfNiwot["TMIN"].mean() $ print("Diff of Mean Temps = {:.3f}".format(diff_of_means))
df[df['group']=='treatment'].groupby('landing_page').count()
n_old = df2.query('landing_page=="old_page"').count()[0] $ n_old $
pd.DataFrame(df.date.str.split(' ').tolist(), columns = "datepart timepart".split())
data.occupation_husb.unique()
[i for i in b1.index if i not in b2.index]
%%time $ tcga_target_gtex_expression_hugo_tpm = tcga_target_gtex_expression_hugo \ $     .apply(np.exp2).subtract(0.001).groupby(level=0).aggregate(np.sum).add(0.001).apply(np.log2)
kyt_lat = 34.955205 #35.005205 $ kyt_long = 135.675300 #135.7353 $ diff_wid = (135.795300 - 135.675300)/grid_size $
pd.set_option("display.max_colwidth", 160)
transactions.merge(users, how='inner', on='UserID')
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
articles.to_csv('data_table/article.csv', index = False)
pns = data_from_store[u'Part Number'] $ pn1c = data_from_1c[u'Part no.']
snt_sorted = list(snt["time"].sort_values()) $ [snt_sorted[len(snt_sorted)//4], $  snt_sorted[len(snt_sorted)//2], $  snt_sorted[3 * len(snt_sorted)//4]]
factors = [] $ DataAPI.write.update_factors_return(['GROWTH'], trading_days)
reddit_info.shape
queensland_data = pd.read_csv('../data/2-data-collection/2013_Queensland_floods-tweets_labeled.csv') $ topic_preds = [topics[p] for p in np.argmax(lda.transform(count_vectorizer.transform(queensland_data[' Tweet Text'])), axis=1)]
data.head(2)
stores = pd.merge(stores, air_store_info, how='left', on=['air_store_id'])
df_ind_site = df_providers[['id_num','drg3','discharges','disc_times_pay']].sort_values(['id_num'],\ $               ascending=[True]) $ df_ind_site.head() $ df_sites_to_eliminate = (df_providers.groupby(['id_num','name','drg3','discharges','year'])[['disc_times_pay']].sum()) $ df_sites_to_eliminate.head()
ids = data[ids_col].values $ label = data[label_col].values $ data = data.drop([ids_col, label_col], axis = 1) $ print('labels distribution:', np.bincount(label) / label.size)
teams_sorted_by_wins = df.sort_values(by=['wins']) $ teams_sorted_by_wins.head(10)
churned_df.iloc[18:30]
learn.save('clas_2')
df.iloc[1] # second row
path = os.path.join('Data for plots', 'Monthly generation.csv') $ eia_gen_monthly.to_csv(path, index=False)
weekly.sort_values(by=['weekday'], inplace=True)
len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)
reg.fit(X_train,Y_train)
revenue.tail()
comps_df.rename(columns={'company_name_x': 'company_name','company_name_y': 'competitor_name'}, inplace=True)
b = bnbAx[bnbAx['language']=='fr'].first_browser.value_counts()/len(bnbAx[bnbAx['language']=='fr']) $ b.head()
pickle.dump(lsa_cv_data, open('iteration1_files/epoch3/lsa_cv_data.pkl', 'wb'))
print 'Most Onion-like:', tweets_pp[tweets_pp.handle == 'TheOnion'].sort_values('The_Onion_Prob', ascending=False).text.values[0] $ print 'Least Onion-like:', tweets_pp[tweets_pp.handle == 'TheOnion'].sort_values('The_Onion_Prob', ascending=True).text.values[0]
def named_drug(row): $     for d in drugs: $         if d.lower() in row.lower(): $             return(d)        
plt.style.use('ggplot') $ bb['close'].apply(rank_performance).value_counts().plot(kind='bar')
vals2.sum(), vals2.min(), vals2.max()  # runtime warnings, not exceptions
autos = autos.drop(['nr_of_pictures','seller','offer_type'],axis = 1)
data.drop(columns=['affairs'],axis=1,inplace=True) # after binary target variable is created we delete affairs variable
plot = sb.regplot(x='Price', y='Volume', data=dfEPEXbase, fit_reg=True) $ plot.set_title('Relationship between Price and Volume')
param_test2 = {'max_depth':list(range(2,16,2)), 'min_samples_split':list(range(200,1001,10))} $ gsearch2 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=499, max_features='sqrt', subsample=0.8, random_state=10), $ param_grid = param_test2, scoring='r2',n_jobs=4,iid=False, cv=5) $ gsearch2.fit(prop[predictors], prop[target]) $ gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_
to_be_predicted_Day5 = 50.62473966 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df_temperature.head()
df['user_id'].value_counts().index $ newdf = df.drop_duplicates('user_id') $ len(newdf[newdf["converted"]==1])/len(newdf.index)
xmlData.head(5)
url_HOU = "https://texans.seasonticketrights.com/Images/Teams/HoustonTexans/SalesData/Houston-Texans-Sales-Data.xls"
r=df.resample('3D')
df_discharges_totals.head()
df['timestamp'] = pd.to_datetime(df['timestamp']) $ df['day'] = df['timestamp'].dt.day $ df_converted = df.query('converted == 1 & timestamp > "2017-01-03 00:00:00.00" & timestamp < "2017-01-24 00:00:00.00"') $
out_train.shape, out_test.shape
tmp.head()
df.describe(include='all')
treatment_converted = df2[df2['group']=="treatment"]['converted'].mean() $ treatment_converted
old_page_converted_mean=old_page_converted.mean() $ new_page_converted_mean=new_page_converted.mean() $ new_page_converted_mean-old_page_converted_mean
precip_stations_data = session.query(Measurement.station,func.count(Measurement.id)).\ $                 group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all() $ precip_stations_data $
twitter_archive.name.value_counts()
df2 = df $ mismatch_index = mismatch_df.index $ df2 = df2.drop(mismatch_index)
countries_df = pd.read_csv('countries.csv') $ reg_df = reg_df.merge(countries_df, on='user_id') $ reg_df[['is_CA', 'is_UK', 'is_US']] = pd.get_dummies(reg_df['country']) $ reg_df.head()
sp500.iloc[[i1, i2]]
start_end_points_df.to_csv('start_end_gps_from_garmin.csv')
dt_features['state_changed_at'] = pd.to_datetime(dt_features['state_changed_at'],unit='s')
image_predictions_df.info()
conn.commit()
users_fin=users_van.append(users_ben) $ users_fin.head()
sum(df2.user_id.duplicated())
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=17000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
len(train_preprocessed[train_preprocessed.isnull().any(axis=1)])
df.head()
wikipedia_base_url = 'https://en.wikipedia.org' $ wikipedia_marvel_comics = 'https://en.wikipedia.org/wiki/Marvel_Comics' $ marvel_comics_save = 'wikipedia_marvel_comics.html' $ globalCity_pdf = 'http://journals.sagepub.com/doi/pdf/10.1177/0042098007085099'
lm = sm.OLS(df_new['converted'], df_new[['intercept', 'US', 'UK']]) #using only 2 dummy variables from country $ results = lm.fit() $ results.summary()
autos['vehicleType'].unique()
df.drop(['ARR_TIME','AIR_TIME','ACTUAL_ELAPSED_TIME','ARR_HOUR'],axis=1,inplace=True)
print(csgo_profiles.info())
!rm hawaii.sb $ !rm hawaii.sqlite
my_tweet_df["tweet_date"] = pd.to_datetime(my_tweet_df["tweet_date"]) $ my_tweet_df.sort_values("tweet_date", inplace=True) $ my_tweet_df.reset_index(drop=True, inplace=True) $ my_tweet_df.head()
print('No. of unique users: ', df['user_id'].nunique())
max_features = 1000 $ tfidf_vec = TfidfVectorizer(max_df=0.95, min_df=2, $                             max_features=max_features, stop_words="english") $ tfidf = tfidf_vec.fit_transform(comment_sentences) $ tfidf_feats = tfidf_vec.get_feature_names()
grp = data.groupby(['product','issue'])['text'].count() $ grp['Bank account or service'].plot(kind='barh',title = 'Complaints Per Issue',color='black',figsize=(6,6)) $ plt.xlabel('# Of Complaints') $ plt.ylabel('Issue') $ plt.show()
df[df['Agency Name'].isin(['Department of Transportation', 'DOT'])].index.month.value_counts().sort_index().plot() $ df[df['Agency Name'].isin(['New York City Police Department', 'NYPD'])].index.month.value_counts().sort_index().plot()
def split1(elem): $     elem = elem.replace('POINT (', '') $     elem = elem.replace(')', '') $     return elem.split(' ')[0]
df_station = pd.DataFrame(list(station_zipcode.items()), columns = ['station', 'zipcode']) $ df_station['zipcheck']=df_station.zipcode.apply(lambda x:len(x)) $ df_station[df_station['zipcheck']!=5] $
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func, inspect $ from flask import Flask, jsonify
table_rows = driver.find_elements_by_tag_name("tbody")[8].find_elements_by_tag_name("tr") $
pd.Period('2012', freq='A') - pd.Period('2002', freq='A')
y_test_pred = model.predict(X_test.as_matrix()) $ utils.metrics(y_test, y_test_pred)
dates=pd.date_range(start='1/Oct/2020', periods=5, freq='M') $ print(dates) $ dates.to_period()
list(pd.read_csv(projFile, nrows=1).columns), list(pd.read_csv(schedFile, nrows=1).columns), list(pd.read_csv(budFile, nrows=1).columns)
tweets['chars'] = tweets['full_text'].str.len()
df2.head()
sns.boxplot(calls_df["length_in_sec"],orient='v')
sns.heatmap(data.corr()) $ plt.show()
len(df_enhanced.query('retweeted_status_id != "NaN"'))
control_converting = df2[df2['group'] == 'control']['converted'].mean() $ print('Probability of an individual in the control group converting:') $ print(control_converting)
df.columns
p1_table = profits_table.groupby(['Product']).Profit.sum().reset_index() $ p1_result = p1_table.sort_values('Profit', ascending=False) $ p1_result.head()
df_questionable[df_questionable['bias'] == 1]['link.domain_resolved'].value_counts(25).head(25)
us['country'] = us['country'].str.replace(r'.','') $ us['cityOrState'] = us['cityOrState'].str.replace(r'.','') $ us['country'] = us['country'].str.strip()
base3 = df3[['placeId', 'hashtags']].groupby('placeId').aggregate(lambda x: [i for l in x for i in l ])
with ZipFile('{0}.zip'.format(datapath / zipfile), 'r') as myzip: $     with myzip.open('2013_ERCOT_Hourly_Load_Data.xls') as myfile: $         pd_excel = pd.read_excel(myfile) $ pd_excel.head()
points2=pd.Series(points_dic,index=["Spain","India","China","France"]) $ points2
individuals_metadata_df.shape
df3.dtypes
df['director'] = df['director'].fillna('Unknown') $ df['production_companies'] = df['production_companies'].fillna('Unknown') $ df['genres'] = df['genres'].fillna('Unknown')
yt.get_upload_playlist_id(channel_id)
old_page_converted = np.random.binomial(nold, p = pold)
df_new['CA_ab_page'] = df_new['ab_page'] * df_new['CA'] $ df_new['UK_ab_page'] = df_new['ab_page'] * df_new['UK'] $ df_new.head()
driver.get("http://www.reddit.com") $ time.sleep(2) $ html = driver.page_source
autos['price'].head()
results = logit_mod.fit() $ results.summary()
wk_prob = draft_df[draft_df.license_issue == True] $ to_remove = ['vehicle', 'motor', 'rmv', 'car', 'road', 'driver', 'drivers', 'plates', 'birth', 'child'] $ wk_prob = wk_prob[~wk_prob.explain.str.contains('|'.join(to_remove))]
from keras.preprocessing.text import Tokenizer $ from keras.preprocessing.sequence import pad_sequences
df_new['intercept']=1 $ log_mod= sm.Logit(df_new['converted'], df_new[['intercept','CA', 'UK']]) $ results= log_mod.fit()
%%time $ df['closed_at'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %I:%M:%S %p')
full = pd.merge(admit,claims,on=['Patient','AdmitDate']) $ full['AdmitDate'] = pd.to_datetime(full['AdmitDate']) $ full.head()
df_twitter_copy.head()
classes = df["sentiment"].values
twitter_archive_clean=twitter_archive_clean[twitter_archive_clean.rating_denominator==10]
maxActiveDay = max(activityByUserByday["activeDay"]) $ print("\nThe highest number of active day is :",maxActiveDay,"days") $ numberOfUser = activityByUserByday[activityByUserByday["activeDay"]==maxActiveDay].userid.count() $ print("There are",numberOfUser,"users who keep playing the mobile application for", maxActiveDay,"days") $ X = pd.merge(X, activityByUserByday[['userid','activeDay']] , on ="userid")
! wc -l failed_samples.txt
project_human_df = non_blocking_df_save_or_load( $     rewrite_human_data(project_human_raw_df), $     "{0}/human_data_cleaned/projects".format(fs_prefix))                                  
train_col.train_model(num_epochs=2)
daily.head()
vocab = vect.get_feature_names()
reviews.rename(columns={'region_1':'region','region_2':'locale'})
autos['offer_type'].value_counts()
df2 = df2.drop_duplicates(subset='user_id') $ df2.shape
git_blame['component'] = git_blame.path.str.split("/", n=2).str[:2].str.join(":") $ git_blame.head()
print(df_result.sort('loglikelihood', ascending=False).head(50))
edftocsv.edftocsv(inputFile, outputFileHeaders, outputChanHeaders, outputData, True)
station_counts['Precip'].idxmax()
all_sets.cards["XLN"].head()
df=pd.read_csv('DonutologyKC_facebook_statuses.csv') $
df_congress = ux.datasets.load_congress_twitter_links() $ print(f'The dataset has {len(df_congress)} rows') $ df_congress.tail(2)
print(train.columns.values)
df_clean['rating_numerator'] = df.text.str.extract('(\d+)', expand=True) $ df_clean['rating_numerator'] = pd.to_numeric(df_clean['rating_numerator']) $ print (" The text: %s \n The new grade in rating_numerator: %.1f \n The old grade: %.1f" % (df_clean['text'].ix[2326], df_clean['rating_numerator'].ix[2326],df['rating_numerator'].ix[2326])) $ print (" The text: %s \n The new grade in rating_numerator: %.1f \n The old grade: %.1f" % (df_clean['text'].ix[1689], df_clean['rating_numerator'].ix[1689],df['rating_numerator'].ix[1689]))
t_conv = gb.loc['treatment', 1][0] $ t_no_conv = gb.loc['treatment', 0][0] $ p = t_conv/(t_conv + t_no_conv) $ p
groupby_breed.head()
countries_df = pd.read_csv('./countries.csv') #reading countries data $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head() $
filtered_content_words = [] $ for word in tqdm(content_words): $     if content_counts[word] > 2: $         if not word in filtered_content_words: $             filtered_content_words.append(word)
fill = A.stack().mean() $ A.add(B, fill_value=fill)
petropavlovsk_filtered.shape
pd.read_html('https://httpbin.org/basic-auth/myuser/mypasswd')
preds = xgb.predict_proba(X_test) $ print('AUC-ROC Score:', round(roc_auc_score(y_test, [item[1] for item in preds]) * 100, 2), '%')
tweets_master_df.ix[309, 'expanded_urls']
import os $ for file in os.listdir('.'): $     if file.endswith('.py'): $         print(file)
sale_prod_table = avg_sale_table.rename(columns = {'Sale Price':'SalePrice'}) $ sale_prod_table.head()
df2 = ab.query('(landing_page == "new_page" & group=="treatment") | (landing_page == "old_page" & group=="control")') $ df2.shape[0]
(autos['last_seen'] $          .str[:10] $          .value_counts(normalize=True, dropna=False) $          .sort_index() $         )   
months = pd.Categorical(['Jun', 'Jun', 'Mar', 'Apr'], $                         categories=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', $                                     'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], $                         ordered=True) $ months
from pyspark.ml.feature import StringIndexer $ t1 = StringIndexer(inputCol="business_id", outputCol="business_idn").fit(df_city_restaurants) $ df_city_restaurants = t1.transform(df_city_restaurants)
df.head()
logit = sm.Logit(df3['converted'],df3[['intercept','ab_page']]) $ r = logit.fit()
calls_nocontact.issue_type.value_counts()
contractor_clean['updated_date'].head() $
np.sqrt(np.mean((actual.values - imputed.values)**2))
b_cal_q1.head()
fin_r.isnull().sum()
AAPL.index
giss_temp.boxplot();
well_data['time'] = days $ print(well_data.head())
ax = sns.lmplot(x='year', y='temp_c', fit_reg=True, order = 2,  data = mean_temp) $ ax.set(xlabel = 'Year', ylabel = 'Yearly Average Temperature C')
pd.Series(bnbB.age).isnull().any()
Logit_mod = sm.Logit(dfX['converted'],dfX[['intercept','ab_page']]) $ results = Logit_mod.fit() $
df=pd.read_csv("../UserData/1000ShareAllColumns.csv") $ df.dtypes
pattern = re.compile('AA') $ print(pattern.search('AAbcAA')) $ print(pattern.search('bcAA'))
s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] $ d2 = defaultdict(list) $ for k, v in s: $     d2[k].append(v) $ d2
obs_diff= new_page_converted.mean() - old_page_converted.mean()# differences computed in from p_new and p_old $ obs_diff
print sqlContext.sql(query).toPandas()
table_name = "Nathan_Carto_SQL_API_test" $ res = sql_api(carto_url, truncate_table_sql, carto_api_token) $ print(res.text)
df_new['intercept'] = 1 $ lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK', 'ab_page']]) $ results = lm.fit() $ results.summary()
p_old = round(df2['converted'].mean(),4) $ print(p_old)
noise_df= df[df['Complaint Type'].str.contains("Noise")]
data.describe()
classweights = pd.read_csv('classweights.csv', sep=',', header=None) $ classweights.set_index(0, inplace=True) $ classweights[1].to_dict() $
mcqueen_ng.plot(kind='barh', figsize=(20,16));
df_merge.head()
timestamp = tweets092315['created_at'].values $ ts = [pd.Timestamp(t).tz_convert('US/Eastern') for t in timestamp] $ tweets092315.index = ts
convo3 = companyNeg[companyNeg.group_id == 'group_96810' ]
us['cityOrState'].value_counts(dropna=False)
header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} $ response_get = requests.get(endpoint_published_models, headers=header) $ print response_get $ print response_get.text
from sklearn.svm import SVC
df_moved.sort_values(by='Count', ascending=False).head()
predictions = knn.predict(test[['property_type', 'lat', 'lon']])
items.head()
wildfires_df['FIRE_YEAR'] = wildfires_df['FIRE_YEAR'].astype('str') #use .astype function to convert to a string $ Info_Dataframe(wildfires_df)
lm=smf.ols(formula='y ~ h0+h1+h2+h3+h4+h5+h6+h7+h8+h9+h10+h11+h12+h13+h14+h15+h16+h17+h18+h19+h20+h21+h22+h23+wet+low_vis',data=df_reg).fit() $ lm.summary()
sns.kdeplot(pm_final['obs_date'], shade=True).set_title("Distribution of cumulative date for readings")
control_cr = df2.query('converted == 1').user_id.nunique()/df2.user_id.nunique()
df.groupby("cancelled")["pickup_week"].mean()
for i, k in d_par.items(): $     if k > 0.8: $         print (i, df_rank_kw.keywords[i[0]], df_rank_kw.keywords[i[1]])
treat_new_pageUU = df2.loc[(df2['group'] == 'treatment')] $ len(treat_new_pageUU[(treat_new_pageUU['converted']==1)] )/ treat_new_pageUU.shape[0]
P1_Graphs = pickle.load(open('A4_graphs','rb')) $ P1_Graphs
grouped_dpt_city.size()
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ results = log_mod.fit() $ results.summary()
avg_stops_window_crimes =  int(np.average(stops_month_window['sum_window_crimes'])) $ avg_monthly_crimes = int(np.average(crime_month_window['sum_crimes'])) $ avg_bi_weekly_crimes = int(np.average(crime_two_week_window['sum_crimes'])) $ avg_weekly_crimes = int(np.average(crime_week_window['sum_crimes'])) $ avg_daily_crimes = int(np.average(crime_day_window['sum_crimes']))
pgh_311_data_merged['CREATED_ON'] = pd.to_datetime(pgh_311_data_merged['CREATED_ON']) $ pgh_311_data_merged.info()
old_page_converted=np.random.choice([0,1],size=nold[0],p=[pold,1-pold]) $ print(len(old_page_converted))
%%R $ GrouperFunc <- function(df, ...) df %>% regroup(list(...))
df = df[['Adj. Open',  'Adj. High',  'Adj. Low',  'Adj. Close', 'Adj. Volume']] $ df['HL_PCT'] = (df['Adj. High'] - df['Adj. Low']) / df['Adj. Close'] * 100.0 $ df['PCT_change'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.0 $ df = df[['Adj. Close', 'HL_PCT', 'PCT_change', 'Adj. Volume']] $ print(df.head())
nypd = df[df['Agency'] == 'NYPD'] $ dot = df[df['Agency'] == 'DOT'] $ ax = nypd.groupby(by=nypd.index.hour).count().plot(y='Unique Key', label='NYPD') $ dot.groupby(by=dot.index.hour).count().plot(y='Unique Key', label='DOT', ax=ax)
customer_emails.sort_values('Paid at', inplace=True) $ customer_emails['Previous Transaction'] = customer_emails.groupby(['Email'])['Paid at'].shift() $ customer_emails['Days Between'] = customer_emails['Paid at'] - customer_emails['Previous Transaction'] $ customer_emails['Days Between Int'] = (customer_emails['Days Between'].dropna()/ np.timedelta64(1, "D")).astype(int) $ customer_emails['Buy Count'] = customer_emails.groupby(['Email'])['Paid at'].cumcount()+1
pclass3_survived = df_titanic.loc[df_titanic['pclass'] == 3, 'survived'] $ print(pclass3_survived.value_counts()) $ pclass3_survived.value_counts().plot(kind='pie', autopct='%.2f', fontsize=20, figsize=(6, 6))
df.sum(1)
op_ = model.predict([q1_data1,q2_data2])
conflicting_df1 = df.query('group == "treatment" & landing_page == "old_page"') $ conflicting_df2 = df.query("group == 'control' & landing_page == 'new_page'") $ conflicting_df1.user_id.nunique() + conflicting_df2.user_id.nunique()
for i in range(0,len(data)): $     data['TweetID'].iloc[i] = i
new_model.wv.vocab
au.plot_user_popularity(au.filter_for_support(popular_trg_df, min_times=5),day_list)
arrows = pd.read_csv('input/data/arrow_positions.csv', encoding='utf8', index_col=[0,1])
to_week = lambda x: x.dt.week
df2.head(2)
np.mean(df.num_comments)
hourly_data = trip_data[["Trip_distance","hours"]] $ hourly_data.groupby("hours").agg(['mean', 'median'])
import datetime $ now = datetime.datetime.now() $ print now
df["loc"] = df["start station latitude"].astype(str)+","+" "+df["start station longitude"].astype(str)
def range_calc(x): $     return x.max() - x.min()
atdist_opp_dist_info_valid_responses = ['Yes! This info is useful, I\'m going now.'] $ atdist_opp_dist_info_valid_count_prop_byuser = compute_valid_count_prop_byuser(atdist_opp_dist[atdist_opp_dist['infoIncluded']], $                                                                                users_opp_dist, 'vendorId', 'emaResponse', $                                                                                atdist_opp_dist_info_valid_responses) $ atdist_opp_dist_info_valid_count_prop_byuser.head()
plt.hist(name_array) $ plt.title('The distribution of the names\' length') $ plt.show() $
twitter_df_clean['favorite_count'].corr(twitter_df_clean['retweet_count'])
pd.read_sql_query("SELECT * FROM person LEFT JOIN grades ON person.id = grades.person_id" , conn)
g_hash = spike_tweets.groupby('hashtags')['id'].count().sort_values(ascending=False)[:5] $ print(g_hash)
salida_individual = procesaPosts(salida) $ for i in range(ncomm): $     df = pd.DataFrame(list(scrapComments(i)), index=['comm_id','comm_msg','comm_date']).transpose() $     df_comms = df_comms.append(df, ignore_index=True) $
df3['intercept'] = pd.Series(np.zeros(len(df3)), index = df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index = df3.index)
hundred_stocks_df.tail()
type(fish.stack())
print "Cantidad de sellers:" ,len(set(df.seller)) $ print "Cantidad de gearbox:" ,len(set(df.gearbox)) $ print "Cantidad de modelos:" ,len(set(df.model)) $ print "Cantidad de tipos de combustibles:" ,len(set(df.fuelType)) $ print "Cantidad de marcas:" ,len(set(df.brand))
a = df.iloc[4] $ df.append(a, ignore_index = True)
df_user_count.apply(np.log).hist() $ plt.show()
merged.info()
tizibika['time_date']=pd.to_datetime(tizibika['timestamp'])
company_data.reset_index(inplace=True)
df[df['Descriptor'] == 'Loud Music/Party'].resample('M').count().head()
df_countries = pd.read_csv('countries.csv') # reading 'countries.csv' $ df_countries.shape[0] == df2.shape[0] # checking if they have the same number of rows
p + pd.tseries.offsets.Hour(2)
months_2016 = pd.period_range("2016", periods=12, freq="M") $ one_day_after_last_days = months_2016.asfreq("D") + 1 $ last_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay() $ last_bdays.to_period("H") + 9
year_with_most_commits = commits_per_year["author"].idxmax().year $ print(year_with_most_commits)
transactions.join(users.set_index('UserID'), on='UserID', how = 'outer')
df[df['Descriptor'] == 'Loud Music/Party'].index.weekday.value_counts().sort_index().plot(kind='bar')
datAll = datAll[pd.notnull(datAll['zip'])]
tfidf_doctopic = clf.fit_transform(tf_idf)
score_a.shape[0] / score.shape[0]
all_data = pd.concat(datasets).reset_index(drop=True) $ all_data.head()
from sklearn.model_selection import train_test_split $ train, test = train_test_split(users, test_size = 0.2)
LABELS = ["Underweight", "Normal", "Overweight", "Obese"] $ plt.bar(x, y, 0.5) # call the plot $ plt.xticks(x, LABELS) # format the X axis with labels $ plt.show()
run txt2pdf.py -o "2018-06-12-1308 FLORIDA HOSPITAL ALL DRGs.pdf"  "2018-06-12-1308 FLORIDA HOSPITAL ALL DRGs.txt"
df.head()
new_page_converted = np.random.binomial(n_new,Pnew) $ new_page_converted
data_train = pd.read_json('train.json') $ data_test = pd.read_json('test.json')
counts = count_vectorizer.transform(final_tweets).transpose()
df_en.to_csv('btc_eng.csv')
autos['num_photos'].value_counts() $ autos = autos.drop(['num_photos', 'seller', 'offer_type'], axis=1)
twelve_months_prcp.head()
aa=ins.groupby(["business_id",'year']).size().to_frame().reset_index() $ inspections_by_id_and_year = aa.rename({0:"count"}, axis='columns').set_index(["business_id","year"]) $
intr = df_noblends[y_col].value_counts(1) $ plot = intr.plot(kind='bar', figsize=(16,8)); $ plot.set_xticklabels(intr.index, {'rotation' : 90});
pd.Series(gbm_pred).isnull().sum(), pd.Series(y_test).isnull().sum()
df.index[2]
autos['odometer_km'].describe()
btc_price = df_combined['Close Price'] $ btc_price = btc_price.values $ btc_price[:5]
print '1 hr ahead:', date_now + timedelta(hours=1) $ print '1 hr ahead:', date_now - timedelta(days=3) $ from dateutil.relativedelta import relativedelta $ print '1 hr ahead:', date_now + relativedelta(years=1) + timedelta(days=3, seconds=2)
thisWeek.head()
people.dtypes
temp_df['reorder_interval_group'].replace('', np.nan, inplace=True) $ temp_df.dropna(subset=['reorder_interval_group'], inplace=True)
lm=sm.Logit(df2['converted'], df2[['intercept','ab_page','Friday', 'Monday', 'Saturday', 'Thursday', 'Tuesday', $        'Wednesday']]) $ results=lm.fit() $ results.summary()
records['Gender'] = records['Gender'].apply(str.capitalize)
df2[['CA','UK','US']] = pd.get_dummies(df2['country']) $ df2.head()
df3 = df2.merge(df_countries, on='user_id', how='left') $ df3.head()
industries.head()
mask = obj.isin(['b', 'c'])
top_songs.shape
def kelvin_to_celsius(temp): $     return temp - 273.15
type(room_temp).__dict__['temperature'].__delete__(room_temp)
h2o.init(min_mem_size="8G")
pres_df['time_from_creation_tmp'] = pres_df['start_time'] - pres_df['date_created'] $ pres_df['time_from_creation_tmp'].head(10)
df['datetime'] = df.index.values
n_new = treatment_df.shape[0] $ print('The number of individuals in the treatment group is n_new = {}.'.format(n_new))
df.describe()
train_data.isnull().sum()
test_orders_prodfill_final2=test_orders_prodfill_final[['order_id','product_id']] $ test_orders_prodfill_final2['products']=test_orders_prodfill_final2['product_id'] $ test_orders_prodfill_final2.head()
df_mes['PULocationID'].unique().shape
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, $                                                     random_state = 321)
for df in (joined,joined_test): $     df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32) $     df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32) $     df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32) $     df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)
coarse_groups = openmc.mgxs.EnergyGroups(group_edges=[0., 20.0e6]) $ coarse_mgxs_lib = mgxs_lib.get_condensed_library(coarse_groups)
two_day_sample.groupby('date')[['steps']].apply(compute_max_rolling_k, k=30)
first_main = main_deduped.filter(lambda p:\ $                                     p.get("payload/info/previousSessionId") == None and\ $                                     p.get("payload/info/reason") == "shutdown")
raw_data.dtypes
data_for_model.info()
prop57.info()
df.hist(bins=100, figsize=(11,8),color='black') $ plt.show()
vectorized_text_labeled = vectorizer.fit_transform( ... )
serious_count = 0 $ for row in data: $     if re.search("[\[\(][Ss]erious[\]\)]", row[0]): $         serious_count = serious_count + 1 $ print(serious_count)
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_6201.values.squeeze(), lags=40, ax=ax1)
month4 = oanda.get_history(instrument_1, $                         start = '2018-3-31', $                         end = '2018-4-30', $                         granularity = 'M10', $                         price = 'A')
nytimes_df = constructDF("@nytimes") $ display(constructDF("@nytimes").head())
sp500.index
a = (df.query('landing_page == "new_page"')['group'] != "treatment").sum() $ b = (df.query('group == "treatment"')['landing_page'] != "new_page").sum() $ a + b
mod = sm.Logit(df2['converted'], df2[['ab_page', 'intercept']]) $ res = mod.fit()
df['Unique Key'].groupby(by=df.index.hour).count().sort_values(ascending = False).head(1)
learner.save_encoder('lm1_enc')
dt_today = datetime.today().replace(minute=0, second=0) $ print dt_today
dataset_rows = df.shape[0] $ print(dataset_rows)
nutables=tables.copy(); train, store, store_states, state_names, googletrend, weather, test = nutables
df['release_dt'].describe()
mv_lens.head()
df_geo_unique_asn.head()
df.set_index(rng, inplace=True)
autos.columns.tolist()
from sklearn.cross_validation import StratifiedShuffleSplit, cross_val_score
idx = df_providers[ (df_providers['id_num']==30137)].index.tolist() $ '${0:10,.0f}'.format(np.sum(df_providers.loc[idx,'disc_times_pay'])) $
log_mod1 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $
twitter_archive_full[twitter_archive_full.tweet_id.isin(duplicated_list)][['tweet_id','stage']].sort_values('tweet_id')
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.dayofweek).count().plot(y='Agency') $
df.to_csv("prepped_data.csv", index=False)
unique, counts = np.unique(y_hat, return_counts=True) $ print(unique, counts)
pgrid_reg = ParameterGrid({'learning_rate': [0.1] , 'n_estimators': [200], 'random_state': [1] $                         , 'max_depth': [4], 'min_child_weight': [4], 'colsample_bytree': [0.6] $                         , 'reg_alpha': [0.001, 0.001, 0.1, 0, 1.0], 'reg_lambda': [0.001, 0.001, 0.0, 0.1, 1.0] $                         , 'gamma': [0, 0.001, 0.01, 0.1, 1] $                       })
results.iloc[0,3]
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']]
StockData.head(10)
plt.boxplot(daily_trade_volume) $ plt.ylabel('Daily Trading Volume') $ plt.show()
from scipy import stats $ stats.describe(MaxPercentage)
idx = df_providers[ (df_providers['id_num']==287560)].index.tolist() $ print('length of IDX',len(idx)) $ assert len(idx) > 0, 'Length of IDX is NULL' $ print( df_providers.loc[idx[0],'name'] ) $
df['treatments'] = df['tokens'].apply(find_treatments)
autos["vehicle_type"].unique() $
%matplotlib inline $ train.num_points.value_counts(normalize=True)[0:20].plot()
grads = K.gradients(loss, [w,b]) $ updates = [(w, w-lr*grads[0]), (b, b-lr*grads[1])]
X = train[feature_cols] $ y = train.popular
df_combined = countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_combined.head()
autos['datecrawled'].str[:10].value_counts(dropna=False).sort_index()
arraycontainer = loadarray('test.hdf') $ adjmats = arraycontainer.data
s.str.lower()
print('The number of unique users in the dataset is {}.'.format(df['user_id'].nunique()))
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth) $     return api
twitter_df_merged.info()
df.index
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-0.05)
df_final.to_csv('final_dataframe.csv')
VI = pd.DataFrame(cvModel1.bestModel.featureImportances.toArray(), columns=["values"]) $ features_col = pd.Series(["AgencyVec", "CompTypeVec", "BoroughVec", "HOD"])  $ VI["features"] = features_col $ VI
df_cod2["Cause of death"].unique()
df = pd.read_sql_query("select count(*) as cantidad, application.site_id from tracks where ds >= '2018-05-13' and ds < '2018-05-14' and path='/checkout/congrats' and application.site_id in ('MLA', 'MLC', 'MLU', 'MCO', 'MLM') group by application.site_id", con)
logit = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'US','UK', 'US_page', 'UK_page']]) $ results = logit.fit() $ results.summary()
ts = next(dyn_data) $ df = pd.DataFrame(ts) $ charts.plot(df, stock=True, show='inline')
df_master = df2.merge(df_select, on='id', how='outer')
df_dummies = pd.get_dummies(df_categorical)
country_result.summary()
trump.text.describe()
df2 = df.drop(index_of_wrong_mapped_cols, axis=0)
y_q.value_counts()
deployment_details = client.deployments.create(model_uid, 'Retail_Churn_with_XGBoost')
tweet_archive_clean.loc[516]
avg_stops_window_crimes_ =  int(np.average(stops_month_window_['sum_window_crimes'])) $ avg_monthly_crimes_ = int(np.average(crime_month_window_['sum_crimes'])) $ avg_bi_weekly_crimes_ = int(np.average(crime_two_week_window_['sum_crimes'])) $ avg_weekly_crimes_ = int(np.average(crime_week_window_['sum_crimes'])) $ avg_daily_crimes_ = int(np.average(crime_day_window_['sum_crimes']))
api_copy = df_filtered.copy() $ api_copy.info()
s519397_df["date"] = pd.to_datetime(s519397_df["date"], format='%Y-%m-%d') $ s519397_df.info()
def calPercentage(dataframe, tokenCount, categoryList): $     for category in categoryList: $         dataframe[category] = float("{0:.2f}".format(dataframe[category] / dataframe[tokenCount]  * 100)) $     return dataframe
reviewsDFslice = reviewsDF[:25924] $ reviewsDFslice.tail(50)
ab_df_new.head()
w.get_step_object(step = 2, subset = subset_uuid).indicator_data_filter_settings
df = pd.read_csv("../data/sales_report.csv", sep=',', encoding='utf-8') $ df.head()
f_ip_os_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_os_clicks"), header=True) $ print('Found %d observations.' %f_ip_os_clicks.count())
from scipy.stats import norm $ print(1- norm.cdf(z_score)) $ print(norm.ppf(1-0.05))
Quandl_DF.info() $ Quandl_DF.tail(5)
sys.path
import nltk $ nltk.download()  # Download text data sets, including stop words
def get_lat_long_series(lat, long): $     return pd.Series(list(zip(long, lat)))
with open(p_file, 'rb') as fin: $     new_airports = pickle.load(fin) $ print(new_airports[0:5])
USvideos.info()
filtered_tweet_file = 'tweets_filtered.csv.gz' $ collect.dump_to_csv(filtered_tweet_file, $                     input_fields = ['user.id', 'text', 'created_at'], $                     compression = 'gzip')
cm = metrics.confusion_matrix(y_val, res) $ print(cm) $ print(classification_report(y_pred=res,y_true=y_val)) $ print(np.round(f1_score(y_pred=res,y_true=y_val),3))
d + pd.tseries.offsets.Week(normalize=True)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=26500) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
pd.merge(left=users, right=sessions, how='inner', left_on=['UserID', 'Registered'], right_on=['UserID', 'SessionDate'])
predictions_table.count()
raw[raw.job_type == 'Organic'].hash.value_counts()[0:5]
intervention_history.reset_index(inplace=True) $ intervention_history.set_index(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
df_user_product_ids=pd.merge(left=df_userIds,right=df_productIds,how="outer",on="Key")[["UserID","ProductID"]]
frame = pd.DataFrame({'b':[4, 7, -3, 2], 'a':[0, 1, 0, 1]})
print(autos["registration_year"].value_counts(normalize=True) $                           .sort_index())
table_list = pd.read_html("http://www.psmsl.org/data/obtaining/")
from IPython.display import SVG $ from keras.utils.vis_utils import model_to_dot $ SVG(model_to_dot(decoder_model_inference).create(prog='dot', format='svg'))
xmlData.describe()
df_twitter_copy.to_csv('data/twitter_archive_master.csv', index = False, encoding='utf-8') $ df_user_extract_copy.to_csv('data/user_extract_master.csv', index = False, encoding = 'utf-8')
df2 = df2.rename(columns ={"control":"ab_page"})
subred_counts = pd.DataFrame(reddit['subreddit'].value_counts()) $ subred_counts
autos.columns = snake_case
vip_crosstab = pd.crosstab(vip_df['Month'], vip_df['Finish_Type']) $ vip_crosstab
model = smf.ols(formula='revenue ~ day_of_week', data=tmdb_movies) $ results = model.fit() $ print (results.summary())
age_range_breakdown.sum()
%%sql $ UPDATE facts $ SET Close_Date_key = hour.hour_key $ FROM hour $ WHERE hour.hour  = TO_CHAR(facts.Close_Date, 'YYYY-MM-DD HH24:00:00')
for key, grp in raw_data.groupby('objective'): $     print "{0: <20}{1: <20}{2}".format(key, len(grp), float(len(grp))/len(raw_data))
n_new = gb_page.loc['new_page'][0] $ n_new
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ] $ games_2017_sorted = games_2017.sort_values(by = ["Home.Attendance"], ascending = False)
df = df.query('latitude > 59.787105 & latitude < 59.887502 & \ $               longitude < 17.677434 & longitude > 17.630056')
df.describe(include=['object','number'])
df_mysql.head()
pystore.get_path()
id2word = corpora.Dictionary(data_lemmatized) $ texts = data_lemmatized $ corpus = [id2word.doc2bow(text) for text in texts] $ print(corpus[:1])
df_onc_no_metac['ONC_LATEST_N'].unique()
Y_ols = pymc.Normal(name='Y_ols', mu=y_hat_ols, tau=np.sqrt(train_std), value=y_train, observed=True)
model_df.news_text = model_df.news_text.fillna('') $ model_df.tesla_tweet = model_df.tesla_tweet.fillna('') $ model_df.elon_tweet = model_df.elon_tweet.fillna('')
df.loc[:, 'posted_datetime'] = df.loc[:, 'posted_datetime'].str[1:] 
type(ds[4]) $ ds[4].head()
clf = RandomForestClassifier(max_depth=5, random_state=0) $ clf.fit(X_train, y_train) $ clf.score(X_test,y_test)
df2.drop(2893);
df['Agency Name'].value_counts()
df.sort_values(by=['Company','Product'],inplace=True)
import emot $ print(emot.emoji(text)) $ print(emot.emoticons(text)) $
r.summary()
converted_users = df[df['converted']==1] $ df['converted'].count() $ proportion_converted = round(converted_users['user_id'].nunique()/df['user_id'].nunique(),2) $ print('The proportion of converted users is: ' +  str(proportion_converted )) $
twitter_ar.head(2)
overall3SsnPorch = pd.get_dummies(dfFull['3SsnPorch'])
import statsmodels.api as sm $ convert_old = df2_control.converted.sum() $ convert_new = df2_treatment.converted.sum() $ n_old = len(df2_control) $ n_new = len(df2_treatment)
red_test = pca.transform(test_float_pca) $ red_test
polarity_avg.fillna('No Data',inplace=True) $ polarity_count.fillna('No Data',inplace=True)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=123,stratify=y)
Results_ZeroFill = pd.DataFrame({'ID': Test.index, $                                 'Approved': prob1_ZeroFill}) $ Results_ZeroFill.head()
log_mod_new = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ results_new = log_mod_new.fit() $ results_new.summary()
df_madrid = pd.read_csv('madrid_df.csv') $ demo = df_madrid['text'][:5] # demo testing with 10 texts $ demo
soup.find_all('div', attrs={'class':'art_title'})
os.getcwd() $ os.chdir(local_folder) $ os.environ["local_orig"] =local_orig $ os.environ["local_edit"] =local_edit
quarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters) $ quarterly_revenue
top_supporters.groupby( $     "contributor_cleanname" $ ).amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
Z = np.random.uniform(0,1,10) $ z = 0.5 $ m = Z.flat[np.abs(Z - z).argmin()] $ print(m)
df.index.get_level_values('Subcategory').unique() $
p_treatment_converted = df2.query('group == "treatment"')['converted'].mean() $ p_treatment_converted
df_vow.columns.values $
n_old = df2.query('landing_page == "old_page"').user_id.count() $ n_new = df2.query('landing_page == "new_page"').user_id.count()
noatar_grades = noatar.groupby('Grade').size() $ print(noatar_grades) $ noatar_grades.plot.bar()
uber_14["day_of_year"].value_counts().head()
logit = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results = logit.fit() $ results.summary()
Grouping_Year_DRG_discharges_payments, Year_levels, DRG_levels =\ $         Generate_Grouping_Year_DRG_discharges_payments(False) $ Grouping_Year_DRG_discharges_payments.head()
grouped_df = xml_in.groupby(['authorId', 'authorName'], as_index=False)['publicationKey'].agg({'countPublications': 'count'})
df_ad_airings_5.drop('state',inplace=True, axis=1)
df5[(df5.priority=='B') & ((df5.ra>315) | (df5.ra<195))].sort_values(by='ra')
daily.rolling(50, center=True, $               win_type='gaussian').sum(std=10).plot(style=[':', '--', '-']);
combined_df4['empty_prop']=combined_df4['empty_prop'].fillna(0) $ combined_df4['empty_prop']=combined_df4['empty_prop'].replace(['Y','V'],1) $ combined_df4.head()
result2 = result[result['age']>=18] $ result2.shape
def to_json(dicts): $     with open('specs210618.json', 'w') as fp: $         json.dump(dicts,fp)
df['timestamp'].astype('datetime64[ns]').hist(xrot=90); $ plt.title('Tweets Over Time');
projects = jp.projects() $ boards = jp.boards()   #to see the boards $ sprints = jp.sprints(board_id=18)  #18 for PMT $ sprint = jp.sprint_info(board_id=18,sprint_id=74)  #74 is the last for PMT $ issues = jp.search_issues('project = PMT AND status not in ("Rejected")') $
df3 = df2.copy() $ df3['intercept'] = 1 $ df3[['ab_page','old_page']] = pd.get_dummies(df3['landing_page']) $ df3.head(5)
station_tobs = session.query(measurement.station, station.station.name, measurement.date, measurement.tobs).\ $                filter(measurement.station == (Active_stations[0][0])).\ $                filter(measurement.date.between('2016-08-23', '2017-08-23')).all() $ len(station_tobs) $
freq_port = train_df.Embarked.dropna().mode()[0] $ freq_port
itemTable['Content'] = itemTable['Content'].str.lower() $ meeting_hash = itemTable['Content'].str.contains("meeting") $ print ("Total Meeting Count : " + str(meeting_hash.sum()))
X = pd.DataFrame(data.data, columns=data.feature_names) $ X
ffr.index.month
members = pd.read_csv(f'{PARTITION_DIR}/members.csv', $                       parse_dates=['registration_init_time'], infer_datetime_format = True) $ trans = pd.read_csv(f'{PARTITION_DIR}/transactions.csv', $                    parse_dates=['transaction_date', 'membership_expire_date'], infer_datetime_format = True) $ logs = pd.read_csv(f'{PARTITION_DIR}/logs.csv', parse_dates = ['date'])
gdax_trans_btc = pd.merge(gdax_trans_btc, coinbase_btc_eur_min.iloc[:,:2], on="Timestamp", how="left")
top_songs.columns
crimes_df.T
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) $ X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=0.2)
negative_topic_dataferame.to_csv("negative_topics.csv")
smart_beta_tracking_error = tracking_error(np.sum(index_weighted_returns, 1), np.sum(etf_weighted_returns, 1)) $ print('Smart Beta Tracking Error: {}'.format(smart_beta_tracking_error))
price_data = json_normalize(data, [['dataset', 'data']]) $ heading = json_normalize(data_test, [['dataset', 'column_names']]).transpose()
df_hate.hist() $ plt.show()
def parseAndComputeTopics(df): $     res_df = pd.DataFrame(index=df.index, columns=topics) $     res_df.loc[:, topics[0]:topics[-1]] = df.apply(getTopics, axis=1) $     return res_df
raw_full_df.building_id.value_counts()[:10]
ufos_df.plot(kind='bar', x='Reports', y='Count', figsize=(12, 5))
new_count = df2.query('landing_page == "new_page"').count() $ old_count = df2.query('landing_page == "old_page"').count() $ print(new_count/(old_count + new_count))
search['days_plan_ahead'] = (search['trip_start_date'] - search['timestamp']).dt.days+1
token_sendcnt = token.groupby(["sender","month"]).size().reset_index(name= "sendcount")
ac['Status'].describe()
end_time = datetime.now() $ (end_time - start_time).total_seconds()
data['text_sentiment'] = data['text'].apply(lambda x: sid.polarity_scores(x)['compound'])
sample_submission.head(5)
res = df_exp.join(dfpf, dfpf.rf == df_exp.rf)
dfFull['BsmtFinSF2Norm'] = dfFull['BsmtFinSF2']/dfFull['BsmtFinSF2'].max()
df_dd = pd.DataFrame() $ for date_col in df_datecols: $     dd_col = date_col + '_index_dd' $     df_dd[dd_col] = (df_uro['index_date'] - df_datecols[date_col]).dt.days
access_logs_parsed = access_logs_raw.map(parse_apache_log_line,lambda x: x**2).filter(lambda x: x is not None) $ access_logs_parsed
df_questionable_3[df_questionable_3['state_PA'] == 1]['link.domain_resolved'].value_counts()
kochdf = pd.merge(kochdf, koch12df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_12'))
html_table = df.to_html() $ html_table.replace('\n', '') $ df.to_html('templates/table.html') $ !open table.html
grp_tweet = tweets.groupby("lan")
df_mas = fin_df.sort_values('dog_stage').drop_duplicates('tweet_id', keep = 'last')
serc_mapInfo = serc_refl['Metadata']['Coordinate_System']['Map_Info'] $ print('SERC Map Info:',serc_mapInfo.value)
outliersDict = {key: df[abs(df['value'] - np.mean(df['value'])) > 3 * np.std(df['value'])] for key, df in typesDict.items()} $ outliersDict.keys()
weather.loc[weather.max_gust_speed_mph.isnull(), 'max_gust_speed_mph'] = weather.groupby('max_wind_Speed_mph').max_gust_speed_mph.apply(lambda x: x.fillna(x.median()))
import requests $ url = "https://en.wikipedia.org/" $ r = requests.get(url) $ print(r.text)
test = only_cards["XLN"] $ test.head()
test_cleaned_bow = vect_cleaned_bow.transform(test_cleaned) $ test_stemmed_bow = vect_stemmed_bow.transform(test_stemmed) $ test_cleaned_tfidf = vect_cleaned_tfidf.transform(test_cleaned) $ test_stemmed_tfidf = vect_stemmed_tfidf.transform(test_stemmed)
old_page_converted = np.random.choice([1,0], size=n_old, p=(p_old, 1-p_old)).mean() $ old_page_converted
df_gnis = df_gnis.replace(np.nan, '', regex=True) $ geometry = [Point(xy) for xy in zip(df_gnis.PRIM_LAT_DEC, df_gnis.PRIM_LONG_DEC)] $ gdf_gnis = gpd.GeoDataFrame(df_gnis, crs={'init':'epsg:4269'}, geometry=geometry)
parsed_events["team_size"] = parsed_events.team_size.fillna(1).astype(int) $ parsed_events.distance = parsed_events.distance.astype(int) * parsed_events.team_size $ parsed_events.assign(original = raw.event).sample(5)
txSenate.info()
replacements = { $ 'location': {'A':'Hyderabad', 'B':'Mumbai'} $ } $ df = df.replace(replacements, regex=True) $ print(df.head(2))
ab_dataframe['converted'].mean()
plt.hist([comment['first.comment.week.diff'] for comment in eligible_comments]) $ plt.title("How many weeks previously was this comment posted") $ plt.show()
conn = pymysql.connect(host='localhost', port=3306, user='root', password='pythonetl', db='pythonetl')
sentences = df.text $ dates = df.created_at
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer() $
all_data_wide = all_data_wide.reindex_axis(sorted(all_data_wide.columns), axis=1)
df_compare=pd.read_csv('Unique Providers December 31, 2017.csv') $
driller.to_sql(con=engine, name='driller', if_exists='replace', flavor='mysql',index=False)
crime_10H40_10_17.to_csv('crime_data_clean/crime_10H40_10_17.csv') $ crime_10H10_10_17.to_csv('crime_data_clean/crime_10H10_10_17.csv') $ crime_10H60_10_17.to_csv('crime_data_clean/crime_10H60_10_17.csv') $ crime_10H70_10_17.to_csv('crime_data_clean/crime_10H70_10_17.csv') $
xgb_learner.params['nthread'] = 4
gearbox_dict = {'manuell':'manual', $                 'automatik':'automatic'    $ } $ autos['gearbox'].replace(gearbox_dict,inplace=True) $ autos['gearbox'].fillna('unspecified',inplace=True)
click_condition_meta.info()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_key, access_secret) $ api = tweepy.API(auth, $                  wait_on_rate_limit=True, $                  wait_on_rate_limit_notify=True)
g_test = data_test[data_test['isvalid']>0].groupby('customer').apply(lambda x: x['isvalid'].sum()).reset_index() $ g_test.columns = ['customer','validOrders'] $ p_test = data_test[data_test['isvalid']>0].groupby('customer')['date'].agg({"first": lambda x: x.min(),"last": lambda x: x.max()}).reset_index()
df_gauge.to_csv('gauge_data.csv')
popC15 = pd.DataFrame(yr15.groupby(by=['contact','content']).size()) $ popC15.columns = ['counts'] $ popC15 = popC15.reset_index('content') $ popC15.sort_values(by='counts', ascending=False).head(10)
df_img_algo_clean.tweet_id = df_img_algo_clean.tweet_id.astype(str)
pprint(ldamodel.print_topics())
com_grp.agg([np.mean,np.sum])
closingPrices.tail()
prob_temp = df2.converted.mean() * 100 $ print("Probability of an individual converting regardless of the page they receive: {}%".format(round(prob_temp, 2)))
b_cal_q1.head(2)
scores.info()
df_loc['country'].fillna('Unknown').head()
results = Geocoder.reverse_geocode(31.3372728, -109.5609559) $ results.coordinates
new_query = "SELECT id, recording_date, stage1_reviewer_id, workflow_state FROM ephys_roi_results" $ err_df = get_lims_dataframe(new_query) $ err_df.head()
%%capture $ girls_shuffled.loc['HARJIT':'HARKIRAN']
twitter_ar['text'] = twitter_ar.text.apply(lambda row: removeHttp(row))
df.rename(index={'Goose Island - Honkers Ale - 6 Pack': 'We changed this'}).head(15) $
pd.read_csv("Data/microbiome.csv", chunksize=14)
full_data.to_pickle('D:/CAPSTONE_NEW/jobs_data_full.pkl')
prediction_url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ with open(prediction_url.split("/")[-1],"wb") as file: $     file.write(requests.get(prediction_url).content)
treatment_df2=df2.query('group=="treatment"') $ P_t=treatment_df2['converted'].mean() $ print("Given that an individual was in the treatment group,the probability they converted is {}".format(P_t))
parameters = {'kernel':['rbf'], 'C': np.arange(1,40,2), 'gamma': np.linspace(0.0, 0.2, 11)}
df['timestamp'] = df['timestamp'].str[0:10] $ df_TimeStamp = pd.to_datetime(df['timestamp'])
geometry.export_to_xml()
dat.plot.scatter('longitude','latitude') $ print ' '
ser.sum()
gbm_perf = gbm_model.model_performance(valid = True) $ gbm_perf.plot()
hs_path = utils.install_test_cases_hs(save_filepath)
sns.distplot(df['yearOfRegistration']) $ plt.xlabel("yearOfRegistration " , fontsize=20) $ plt.ylabel("percentage" , fontsize=20) $ plt.title('Vehicle yearOfRegistration', fontsize=20)
params =  {'reg_alpha': 0.1, 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'min_child_weight': 1 $           , 'n_estimators': 300, 'reg_lambda': 1.0, 'random_state': 1, 'max_depth': 4} $ reg_final = xgb.XGBRegressor(**params).fit(X, y)#**params $
reframed.drop(reframed.columns[[7,8,9,10,11]], axis=1, inplace=True) $ print(reframed.head())
test_count_vect = CountVectorizer(vocabulary=significant_features, tokenizer=lambda x: x.split(',')) $ X_test_feature_counts = count_vect.fit_transform(test['feature_list']) $ X_test_feature_counts
for tweet in results[0:5]: $     print(tweet.user.screen_name,"Tweeted:",tweet.text)
twitter_archive_clean.iloc[bad_indexes,[3,7,8,9,10]]
df.isnull().sum().sum()
titanic_df.head()
df.isnull().any()
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_ipb_chord_crown[1][0])) $ perr = np.sqrt(np.diag(pcov_ipb_chord_crown[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
wed11.head()
key, secret = open('api_key.txt').read().splitlines()
Recent_Measurements = pd.DataFrame(Recent_Measurements) $ Recent_Measurements.set_index(Recent_Measurements['date'], inplace=True) $ Recent_Measurements = Recent_Measurements.sort_values('date') $ Recent_Measurements.plot()
subred_num_avg.rename(index=str, columns={'subreddit': 'Subreddit', 'num_comments': 'Average_Num_Comments'}, inplace=True)
df2.loc[df2.user_id.duplicated(keep = False),:]
c.execute('SELECT * FROM cities') $ print(c.fetchall())
total_df[['cat1','cat2','cat3']] = total_df['Categories'].apply(lambda x : pd.Series(x))
print("x es una {}".format(type(x)) , "y es un {}".format( type(y)))
ddf.max().sort_values(ascending=False)
from scipy.stats import norm $ print(norm.cdf(z_score)) # Tells us how significant our z-score is $ print(norm.ppf(1-(0.05))) # Tells us what our critical value at 95% confidence is
def calc_sub(i): $     tweet = TextBlob(dataset['Text'][i]) $     return tweet.subjectivity $ dataset['Sent_Subjectivity'] = [calc_sub(i) for i in range(len(dataset))]
from pysumma.Plotting import Plotting $ from jupyterthemes import jtplot $ import matplotlib.pyplot as plt $ import pandas as pd $ jtplot.figsize(x=10, y=10)
df.head()
rng = pd.date_range('1/1/2018', periods=120, freq='S') $ len(rng)
prog = re.compile('\d{3}-\d{3}-\d{4}') $ result = prog.match('123-456-7890') $ print(bool(result)) $ result = prog.match('1123-456-7890') $ print(bool(result)) $
len([b for b in BDAY_PAIR_df.pair_age if b<0 ])
results = session.query(Measurement.tobs).all() $ tobs_values = list(np.ravel(results)) $ tobs_values
!wget --directory-prefix=downloads ftp://ftp.ensembl.org/pub/release-85/gff3/homo_sapiens/Homo_sapiens.GRCh38.85.gff3.gz
pd.Period(pd.datetime.today(), 'D') - pd.Period('1/1/1970', 'D')
learner.fit(lrs, 1, wds=wd, use_clr=(20, 10), cycle_len=15)
for dim in d.dimensions: $     print('%s:\t%s' % (dim, d.dimensions[dim]))
dff=df2.drop('aa_page', axis=1)
df['venue_name_exits'] = df['venue_name'].isnull().replace([False, True], [0,1])
df_dateorder = df.sort_values('timestamp', ascending=True) $ df_dateorder.head(), df_dateorder.tail()
df['x'].unique()
returns.head()
providers_schedules.head()
df2 = df2.drop_duplicates(['user_id'], keep='first') $ df2 $
print(google_stock.max()) $ print('\n', google_stock.mean()) $ print('\n', google_stock['Close'].min())
s = pd.Series(['EUR', 'DKK', 'EUR', 'GBP'], index=['NL', 'DK', 'DE', 'UK'], name='Currency') $ data.join(s)  #Add a new column from a series or dataframe.
filename = 'First_production_model.sav' $ joblib.dump(reg_logit, filename)
ac['Compliance Review Start'].describe()
ffr2.plot()
print(np.min(lat), np.max(lat), np.min(lon)-360, np.max(lon)-360)
oldfile.shape
engine.execute("SELECT * FROM stations").fetchall()
df.groupby(['ab_test_group']).count().reset_index()
person = "UTHornsRawk" $ person = df[df["screen_name"]==person] $ person.head(100)
intervention_train.isnull().sum()
type(df.Date[0])
precip_data_null= precip_data_df[precip_data_df.isnull().any(axis=1)] $ precip_data_null.tail(3) $ precip_data_df = precip_data_df.dropna(how="any") $ precip_data_df = precip_data_df.sort_values("date") $ precip_data_df.info() $
X_train, X_test, y_train, y_test = train_test_split(X, y, $                                                    test_size=0.3, $                                                    stratify=y, $                                                    random_state=123)
tf_idf.shape
train_texts.shape
df2 = df.copy() $ df2 = df.drop(df[(df.group == "control") & (df.landing_page == "new_page") | (df.group == "treatment") & (df.landing_page == "old_page")].index)
from sagemaker.predictor import csv_serializer $ predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)
dfdummy = pd.get_dummies(data = df_countries, columns=['country'], drop_first = True) $ df4 = dfdummy.merge(df3, on='user_id') $ df4 = df4[['user_id', 'timestamp', 'group', 'landing_page', $            'ab_page', 'country_UK', 'country_US', $            'intercept', 'converted']]
temperature_2016_df["tobs"].hist(bins=12, color="darkblue") $ plt.title("Histogram: Temperature from Station with highest observation") $ plt.savefig("Histogram: Temperature from Station with highest observation") $ plt.show()
lag_list = list(df.columns.values)
daily_returns.hist(bins=20) $ plt.axvline(mean, color='w', linestyle='dashed', linewidth=2) $ plt.axvline(std, color='r', linestyle='dashed', linewidth=2) $ plt.axvline(-std, color='r', linestyle='dashed', linewidth=2) $ plt.show()
squares = pd.Series([1, 4, 9, 16, 25]) $ print(squares.name) $ squares
dfn.head(20)
session.query(User).filter(text("id<:value and name=:name")).\ $ ...     params(value=224, name='fred').order_by(User.id).one()
df_columns['Agency'].value_counts() $
y_client = authenticate('yelp_api_keys.json')
data['Closed Date'] = data['Closed Date'].apply(lambda x:datetime. $                                                 datetime. $                                                 strptime(x,'%m/%d/%Y %H:%M')) $
df_full = pd.read_csv(path) $ df = df_full#.sample(n=100) $
y_preds = knn_grid.best_estimator_.predict(X_test) $ knn_scores = show_model_metrics('K-NN', knn_grid, y_test, y_preds)
df['id'] = df['id'].astype('category') $ df['sentiment'] = df['sentiment'].astype('category') $ df['created_at'] = pd.to_datetime(df['created_at'])
c = R18df.rename({'Create_Date': 'Count-2018'}, axis = 'columns')
df.head()
station_count = session.query(Station.station).count() $ print(f"There are a total of {station_count} stations in this dataset") $
os.chdir(DIR) $ clfs = [] $ for file in glob.glob("*.pickle"): $     clfs.append(file)
import numpy as np $ ok.grade('q06')
dummy_IntServ = pd.get_dummies(df['InternetService'], prefix='IntServ') $ print(dummy_IntServ.head())
negTrend=pd.value_counts(negative.tweet_created_at).reset_index() $ negTrend=negTrend.sort_values("index") $ posTrend=pd.value_counts(positive.tweet_created_at).reset_index() $ posTrend=posTrend.sort_values("index")
geo_countries = [] $ for item in sub_data["place"]: $     geo_countries.append(item["country_code"])
df2_treatment = df[df.group == 'treatment'] $ (df2_treatment.converted == 1).sum()/len(df2_treatment)
for key in sorted(counter_clinton, key=counter_clinton.get, reverse=True): $     print(key,counter_clinton[key])
t0 = t0.reshape((4,1))
df2['ab_page']=df2.group $ df2.ab_page.replace(('treatment','control'), (1,0), inplace=True)
top_views = doctype_by_day.loc[:,doctype_by_day.min().sort_values(ascending=False)[:10]] $ ax = top_views.plot() $ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
act_diff = df2[df2['group']=='treatment']['converted'].mean() - df2[df2['group']=='control']['converted'].mean() $ (act_diff < p_diffs).mean()
df = df.reset_index()
associations = pd.read_csv('Data/associations.csv') $ users = pd.read_csv('Data/users3.csv')
from sklearn.metrics import accuracy_score
df = $ df.head()
weather1.head()
top_challenges = [] $ for i in (sorted(chal_counts)[-10:]): $     j = recommendation_df[recommendation_df['challenge_count']==i].head(1)['challenge_id'].values[0] $     top_challenges.append(j)
p_conv_ctrl = df2.query('converted == "1" and group == "control"').user_id.nunique() / df2.query('group == "control"').user_id.nunique() $ p_conv_ctrl
top_supports['contributor_cleanname'] = top_supports.apply(combine_names, axis=1)
df['HOUR'].fillna(99, inplace = True) $ df['NEIGHBOURHOOD'].fillna('N/A', inplace = True) $ df['HUNDRED_BLOCK'].fillna('N/A', inplace = True)
lgb_mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', $           objective = 'binary') $
stacked_image_predictions.drop('is_dog', axis=1, inplace=True) $ stacked_image_predictions.head()
df.describe()
display(data[data['age'] == data['age'].min()]) $ display(data[data['age'] == data['age'].max()])
mask_H = clintondf.text.str.contains(" -H")
tree_features_df.loc[:,'p_hash'] = tree_features_df['p_hash'].apply(imagehash.hex_to_hash) $ tree_features_df.loc[0,'p_hash']
from nltk.book import *
sp500.Price
body = re.sub(r'[^\x00-\x7F]+',' ', body) $ body
subset1 = df[5500:6500] $ subset1.plot(x='timestamp', y='value')
logit_reg_all = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','Canada','UK']]) $ result_all = logit_reg_all.fit() $ result_all.summary()
cid = areas_dataframe.company_id[0]
a.iloc[[3]]
n_valid_users = df2['user_id'].unique().shape[0] $ print (n_unique_users)
df_users_mvp=df_users_mvp.rename(columns={'field_first_name_value':'first_name','field_last_name_value':'last_name'})
df_loc = df_loc.loc[:,['country','input_string']]
n_old = df2.query("group == 'control'").user_id.nunique() $ n_old
all_used_by_group = lift.get_all_used_by_group('APT12')
df = df[df['unknown'] == 0] $ df.drop('unknown', axis=1, inplace=True)
rows=df.shape[0] $ print("The total number of rows are : {}".format(rows))
number_of_rts = pd.Series(data=data['RTs'].values, index=data['Date']) $ number_of_rts.plot(figsize=(16,4), label="Retweets", legend=True);
df[df['Agency'] == 'DOT']['Complaint Type'].value_counts().head()
disposition_df.columns
validation.analysis(observation_data, Jarvis_resistance_simulation_0_25)
print('Proportion of users converted:', 100*df.converted.mean()) $
df = pd.read_excel("../../data/stocks.xlsx") $ df.head()
df['goal_date_ratio'] = df['date_diff'] / df['goal'] $ df.loc[df['date_diff'] == 0] = 1 $ df['goal_pledge_date'] = (df['pledged'] / df['goal']) / df['date_diff']
x_test.shape
days = ['09-28', '09-29', '09-30', '10-01', '10-02', '10-03', '10-04', '10-05'] $ normals = [] $ for day in range(days): $     normals.append(daily_normals(days[day])) $
converts = df['converted'].sum() $ proportion = converts / uni_users $ proportion
clf = decomposition.NMF(n_components=n_topics, random_state=1)
df.describe()
integratedData.to_csv('Jose_2866049_Ass4_data.csv', index = False)
weather_yvr.plot()
from sklearn.naive_bayes import MultinomialNB
first_mscore = first_movie.find('span', class_ = 'metascore favorable') $ first_mscore = int(first_mscore.text) $ print(first_mscore)
fwd.drop('Store',1,inplace=True) $ fwd.reset_index(inplace=True)
svc.score(X_tfidf_test, y_tfidf_test)
rain.to_csv('rain.csv')
s3.isnull()
ss_sparse = (~df_uro_dd_dummies.isnull()).sum() < 3 $ ls_sparse_cols = ss_sparse[ss_sparse].index.tolist()
summer = beijing.loc[datetime(2014,6,1) : datetime(2014,8,31)]
seen_click_read = pd.merge(new_seen_and_click, new_read, how='left', left_on=['article_id','project_id','user_id','user_type'], right_on = ['article_id','project_id','user_id','user_type']) $ seen_click_read[0:10]
p0 = df2['converted'].mean() $ p0
CVtrain_X, CVtest_X, CVtrain_Y, CVtest_Y = train_test_split(train_X, train_Y, test_size = 0.2, $                                                            random_state = 42) $ CVtrain_X, CVtest_X = pd.get_dummies(CVtrain_X), pd.get_dummies(CVtest_X)
df_cont = pd.read_csv("contact.csv", index_col=None)
autos['price'].value_counts().sort_index(ascending=False).head(30)
FAFRPOSRequest = requests.get(FAFRPOS_pdf, stream=True) $ print(FAFRPOSRequest.text[:1000])
template.format(4.5560, 'Argentine Pesos', 1)
os.makedirs(package_path, exist_ok=True)
support.amount.sum() / merged.amount.sum()
df2['landing_page'].value_counts()[0]/df2.shape[0]
fig, axs = plt.subplots(1, 1, figsize=(7, 6), sharey=True) $ axs.bar(names, values) $ fig.suptitle('Pollution charge calculated for Vedanta-CEMS1')
df_master.describe()
model = tree.DecisionTreeClassifier() $ print ('Decision tree') $ reg_analysis(model,X_train, X_test, y_train, y_test)
favorites = api.favorites()
def indexed_price(pricevector): $     r = pricevector / pricevector.shift(1) - 1 $     r = r.replace([np.inf, -np.inf], 0) # this implies you buy after 1 full day of existence, i.e. 12h or 24h after ico begins $     r = r.fillna(0) $     return 100*np.exp(r.cumsum())
temp_mean.sort_values()
df_goog['Closed_Higher'] = df_goog.Open > df_goog.Close $ df_goog.head(2)
(p_diffs > p_diff_real).mean() $
browser = Browser('chrome', headless=True) $ url = 'https://twitter.com/marswxreport?lang=en' $ browser.visit(url)
df_new = df2[df2['landing_page'] == 'new_page'] $ converted = df_new['converted'] $ new_page_converted = np.random.choice(converted,n_new)
comments.head()
noNulls.orderBy(sort_a_desc, sort_b_desc).show(5)
red_flag_columns = active_companies.columns[-8:] $ temp_df = pd.DataFrame(active_companies[red_flag_columns].sum(axis=0)) $ temp_df.columns = ['number_of_companies_with_red_flag'] $ temp_df.index.name = 'red_flag' $ temp_df
column_dataloaders = {x: torch.utils.data.DataLoader(column_datasets[x], batch_size=128, $                                              shuffle=True, num_workers=0) $               for x in ['train', 'val']}
df_full = df_full[list(DATA_L2_HDR_DICT.values()) + ['school_type']]
print(type(data.values)) $ data.values
path = '' $ df = pd.read_excel(path+'data.xls', index_col=0) $ df.head(3)
dat.tail()
data = pd.concat([approved, rejected], ignore_index=True)
data.groupby('Agency').size().sort_values(ascending=False).plot(kind='bar', $                                                                figsize=(20,4))
print WorldBankdf.printSchema()
def load_tweets(path): $     ...
df_CLEAN1B.head(5)
agency_borough = data.groupby(['Agency', 'Borough']).size().unstack()
train.printSchema()
(np.random.binomial(1, p_new, n_new).mean()) - (np.random.binomial(1, p_old, n_old).mean())
df = pd.DataFrame({'one' : pd.Series(np.random.randn(3), index=['a', 'b', 'c']), $     'two' : pd.Series(np.random.randn(4), index=['a', 'b', 'c', 'd']), $     'three' : pd.Series(np.random.randn(3), index=['b', 'c', 'd'])}) $ df
s = pd.Series([1,2,3,4,5], index = ['a', 'b', 'c', 'd', 'e']) $ s
top100ratings=ratings.groupby('movieId').count().nlargest(100,'rating').reset_index()
evaluator.get_metrics('micro_avg_accuracy')
countries_df.country.unique()
karma_per_comment = (karma_per_user / comments_per_user) $ karma_per_comment.sort_values(ascending=False)
X_counts_df.columns = count_vect_sample.get_feature_names() $ X_counts_df
CO_profit.head()
vocab_google = Google_model.vocab.keys() $ print len(vocab_google)
X_train, X_test, y_train, y_test = train_test_split(features,classification_price,test_size=0.2)
trends_per_year = defaultdict(list) $ for key,value in deaths_per_year.items(): $     df = df_valid[(df_valid.Died >= datetime.strptime(str(key), "%Y")) & (df_valid.Died < datetime.strptime(str(key + 1), "%Y"))] $     trends_per_year[key] = df.Trends.tolist()
y2, X2 = patsy.dmatrices('DomesticTotalGross ~ Constant + Budget', data=df, return_type="dataframe") $ model = sm.OLS(y2, X2, missing='drop') $ fit2 = model.fit() $ fit2.summary()
snow.select ("select count (distinct patient_id) from st_ftd_psp")
df['DAY_OF_WEEK'] = df['DATETIME'].dt.dayofweek
df1 = pd.read_csv('2001.csv')
adopted_cats.loc[adopted_cats['Color']=='Tortie/White','Color'] = 'White Tortie' $ adopted_cats.loc[adopted_cats['Color']=='White/White','Color'] = 'White' $ adopted_cats.loc[adopted_cats['Color']=='Calico/White','Color'] = 'White Calico' $ adopted_cats.loc[adopted_cats['Color']=='White/Calico','Color'] = 'White Calico' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Calico','Color'] = 'Calico'
df2[df2['user_id'].duplicated()]
tweetsDf.is_retweet.hist()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').converted.sum()  # old page == control group $ convert_new = df2.query('landing_page == "new_page"').converted.sum()  # new page == treatment group $ n_old = df2.query('landing_page == "old_page"')['user_id'].nunique() $ n_new = df2.query('landing_page == "new_page"')['user_id'].nunique()
fig, ax = plt.subplots(figsize=(20,10)) $ BTC.plot(ax=ax, kind='line',x='Timestamp',y='Balance', grid=True) $ ax.set_ylabel("BTC Balance", size=14) $ ax.set_xlabel("Date", size=14); $
def get_closest_tweets(X_trans, clust, max_t=10): $     idx = np.argsort(X_trans[:, clust])[::-1][:50] $     tweets = X_tweets.iloc[idx].drop_duplicates() $     return tweets[:max_t]
df.groupby('Sentiment', axis=0).mean()
shelter_pd[['DateTime', 'OutcomeType', 'AnimalType', 'SexuponOutcome', 'AgeuponOutcome', 'Breed', 'Color']] = shelter_pd[['DateTime', 'OutcomeType', 'AnimalType', 'SexuponOutcome', 'AgeuponOutcome', 'Breed', 'Color']].astype(str) $ shelter_df = sqlContext.createDataFrame(shelter_pd) $
msftAC = msft['Adj Close'] $ msftAC.head(3)
data_issues.columns
pd.value_counts(['a', 'a', 'b'], sort=False)
(null_vals > obs_diff).mean()
twitter_archive_clean['source'].value_counts()
noise_graf.head()
delimited_hourly['text']=delimited_twitter_df.groupby([pd.Grouper(freq="H"), 'company'])['text'].apply(lambda x: ' '.join(x)) $ delimited_hourly['Number_of_Users'] = delimited_twitter_df.groupby([pd.Grouper(freq="H"), 'company'])['user_name'].nunique() $ delimited_hourly = delimited_hourly.reindex(delimited_hourly.index.rename(['Time', 'Company'])) $ delimited_hourly.head()
plt.plot(model_output.history['loss'],c='k',linestyle='--') $ plt.plot(model_output.history['val_loss'],c='purple',linestyle='-') $ plt.show;
train.dropna().shape
tweet_json.info()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" & converted == 1').count()['user_id'] $ convert_new = df2.query('landing_page == "new_page" & converted == 1').count()['user_id'] $ n_old = df2[df2['landing_page'] == 'old_page'].count()['user_id'] $ n_new = df2[df2['landing_page'] == 'new_page'].count()['user_id']
model_att.fit([sources], targets, epochs=1, batch_size=12, validation_split=0.1)
df["retweeted_screen_name"].value_counts().head(20)
complete_df.loc[complete_df['Totals'].apply(lambda x: int(x)) < 0, 'Totals'] = 0
airbnb_df['room_type'] = airbnb_df['room_type'].cat.set_categories(['Shared room', 'Private room', 'Entire home/apt'], $                                                                    ordered=True)
df.head()
from sklearn.ensemble import RandomForestRegressor $ from sklearn.metrics import mean_squared_error, r2_score $ clf = RandomForestRegressor(n_estimators=20, max_depth = 50) $ clf.fit(X_train, y_train) $ print "RMSE of prediction for test set is: ", np.sqrt(mean_squared_error(y_test, clf.predict(X_test)))
for id,row in tweets.loc[tweets.snspostid.isin(least_retweeted.parentPost.astype(np.str).values)].iterrows(): $     print(row.text) $     print('')
df_new = df_new[['startDate', 'endDate','barrelID','trapType','lure','uvled','funnel','substrate','males','females','notes']] $ df_new.sort(['endDate','barrelID'],inplace=True) $ df_new.reset_index(drop=True, inplace=True) $ df_new
bacteria2.isnull()
df2['ab_page_and_UK'] = df2['ab_page']*df2['UK'] $ df2['ab_page_and_CA'] = df2['ab_page']*df2['CA'] $ df2.head(10)
try: $     search_results = twitter.search(q='"orchard road" -filter:retweets', count=20) $ except TwythonError as e: $     print(e)
gs_k150_under.score(X_train, y_train_under)
archive_df = pd.read_csv('twitter-archive-enhanced.csv') $ image_df = pd.read_csv('image-predictions.tsv', sep='\t') $ status_df = tweet_df.copy() $ status_df.rename(columns = {"id": "tweet_id"}, inplace = True)
mentions_begin, user_names_begin, user_screen_names_begin, num_tweets_begin, num_retweets_begin = au.get_mentions(ao18_qual_coll)
df.loc[df['survey'] == '201702']
words_hash_scrape = [term for term in words_scrape if term.startswith('#')] $ corpus_tweets_scraped.append(('hashtags', len(words_hash_scrape))) # update corpus comparison $ print('Total number of hashtags: ', len(words_hash_scrape)) #, set(terms_hash_stream))
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("The probability of conversion for the old page is: " + str(p_old))
knn = KNeighborsClassifier(n_neighbors=5) $ knn.fit(x_train, y_train) $ pred = knn.predict(x_test) $ print(metrics.accuracy_score(y_test, pred)) $ print(knn.predict_proba(x_test))
df_family = df[(df.srch_children_cnt) > 0 & (df.srch_adults_cnt > 0)]
feature_importances = rnd_reg.feature_importances_ $ feature_importances
dp = docProcessor(rtnType='word',stopwordList=stops) $ testParsedPD = [" ".join(dp.transform(s)) for s in dfCleaningData[dfCleaningData['geo_code'] == 'TheHill']['property_description']] $ print 'len(testParsedPD): ' + str(len(testParsedPD))
players_list = pd.read_csv('~/dotaMediaTermPaper/data/players_df.csv')['Name'].tolist()
col = col[col.bike_involved == 1]    # drop rows where no bike involved
from sklearn.model_selection import train_test_split $ x, y = the_data, range(the_data.shape[0]) $ x_train, x_test, y_train, y_test = \ $     train_test_split(x, y, test_size = 0.1, random_state=23) $ tmp_df.tmp_idx = np.array(y)
titleslist[0].lower()
sess.get_data('eur curncy','fwd curve')
F_hc = a_hc[score_variable].var() / m_hc[score_variable].var() $ degrees1hc = len(m_hc[score_variable]) -1 $ degrees2hc = len(a_hc[score_variable]) -1
s519397_df = pd.DataFrame(s519397) $ print(len(s519397_df.index)) $ s519397_df.info()
education.reset_index(drop=True, inplace=True)
unique_brands = autos['brand'].value_counts(normalize=True) $ unique_brands
births = births.query('(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)')
datatest['rooms'] = datatest['rooms'].apply(lambda x: 1.0 if x<1 else x)
VXcnn = np.expand_dims(VX, axis=2) $ VXcnn $ scores = model.evaluate(VXcnn, $                         y_label_test_OneHot, verbose=0) $ scores[1]
weekday_agg = trips_data.groupby(['weekday','end_station_id']).duration.mean().reset_index() $ weekday_agg.head() # each weekday, each end station id average duration per trip
people["age"] = 2016 - people["birthyear"]  # adds a new column "age" $ people["over 30"] = people["age"] > 30      # adds another column "over 30" $ birthyears = people.pop("birthyear") $ del people["children"] $ people
from quilt.data.aics import random_sample $ random_sample
print("The probability of converted in control group:", len(df2.query('landing_page == "new_page"'))/len(df2))
b = df_2016.sale_dollars.sum() $ b
for name, importance in zip(x.columns,clf.feature_importances_): $     print(name,importance) $
trump_o.drop("text", axis = 1) $ trump_con = trump_o.drop(["favorite_count", "text", "id_str", "is_retweet", "retweet_count", "source", "created_at"], axis = 1) $ trump_con.shape
weather_dt = c.fetchall() $ print 'fetched'
ndvi_grid = np.array(np.meshgrid(lon_us, lat_us)).reshape(2, -1).T $ np.shape(ndvi_grid)
y_axis = output['retweeted_status.retweet_count'].astype(int).values.tolist()
import matplotlib.pyplot as plt $ from sklearn.datasets import fetch_mldata $ from sklearn.neural_network import MLPClassifier $ import numpy as np $ import pandas as pd $
df2['intercept'] = 1  #adding intercept column to the dataset $ dummy = pd.get_dummies(df['group']) $ df2['ab_page'] = dummy['treatment'] $ df2.head()
X_train_term.shape
df_new.groupby(['ab_page'], as_index=False).mean()
df_new['campaign'].value_counts()
list(itertools.permutations('ABCD', 2))
A.add(B, fill_value=0)
ticket4 = data_df.clean_desc[26] $ parsed4 = nlp(ticket4) $ for ent in parsed4.ents: $     print 'Entity: {}'.format(ent.text) $     print '\t Entity label: {}'.format(ent.label_) $
first_movie.a
regex = re.compile('[^A-Za-z ]+')
results = pd.read_csv('player_stats/{}_results.csv'.format(team_accronym), parse_dates=['Date']) $ results_postseason = pd.read_csv('player_stats/{}_results_postseason.csv'.format(team_accronym), parse_dates=['Date'])
notus.loc[notus['country'].isin(canada)]
data.drop(['Permit_Type_Definition', $            'Existing_Construction_Type_Description', $            'Proposed_Construction_Type_Description'], $           axis=1, $           inplace=True)
total = scores.sum() $ scores[:2.75].sum()/total
tweets_df = tweets_df[tweets_df.userTimezone.notnull()] $ len(tweets_df) $
sentiments_pd["Date"] = pd.to_datetime(sentiments_pd["Date"]) $ sentiments_pd.sort_values("Date", inplace=True) $ sentiments_pd.reset_index(drop=True, inplace=True) $ sentiments_pd.head()
df_new.info()
the_year = 2015 $ Grouping_Year_DRG_discharges_payments_for_specific_year = \ $        Grouping_Year_DRG_discharges_payments.loc[the_year] $ Grouping_Year_DRG_discharges_payments_for_specific_year.head()
users.to_pickle('data/pickled/new_subset_users.pkl')
p_mean = np.mean([p_new, p_old]) $ print("Probability of conversion udner null hypothesis (p_mean):", p_mean) $
df.iloc[:4]
features=list(kick_projects_ip) $ features.remove('state') $ response= ['state']
pct = pct.drop('precinct',axis=1)
sample = df2.sample(df2.shape[0], replace=True) $ old_page_converted = sample.query('landing_page == "old_page"')['converted'] $ old_page_converted.mean()
print(gs.best_estimator_)
train = train.merge(weather, on = train.date)
twitter_archive_clean = twitter_archive_clean.dropna(subset=['expanded_urls'])
beta_1 = 0.10 $ beta_2 = 1990.0 $ Y_pred = sigmoid(x_data, beta_1 , beta_2) $ plt.plot(x_data, Y_pred*15000000000000.) $ plt.plot(x_data, y_data, 'ro')
data_FCInspevnt["Inspection_number"] = data_FCInspevnt.groupby("brkey")["in_modtime"].rank(ascending=False) $ data_FCInspevnt['Inspection_number'] = data_FCInspevnt['Inspection_number'].astype('int64') $ data_FCInspevnt.head(15)
equipment.dtypes
trt_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][1] $ print("P(trt_con) = %.4f" %trt_con)
yt.get_channel_metadata(channel_id, key)
all_df.head()
df_new.head() $ df_new['interaction_us_ab_page'] = df_new.US *df_new.ab_page $ df_new['interaction_ca_ab_page'] = df_new.CA *df_new.ab_page $ df_new.head() $
fuel_xs = fuel_rxn_rates / flux $ fuel_xs.get_pandas_dataframe()
data = np.zeros(4, dtype={'names':('name', 'age', 'weight'), $                           'formats':('U10', 'i4', 'f8')}) $ print(data.dtype)
df_agg = pd.DataFrame() $ for letter in list('ABCDEFGH'): $     df_agg[letter] = ab_groups[letter].sum() $ df_agg = df_agg.round().T $ df_agg[['page', 'viewreclog', 'clicklog', 'transaction', 'revenue']]
obj2.index
for train_index, test_index in splits.split(training_X): $     if len(train_index) >= min_train: $         print(train_index, test_index)
all_df.shape
df.hist(column = "funding_rounds", by = 'founded_year', figsize = (20,16),layout = (5,6))
reddit_comments_data.groupBy('author').agg({'score': 'mean'}).orderBy('avg(score)', ascending = False).show()
current_time = time.strftime("%m-%d-%Y") $ df.to_csv('master_'+current_time+'(hotposts).csv')
tweets_df = pd.read_csv("tweets_sample.csv", index_col=False, compression='gzip', encoding='utf-8') $ tweets_df
df2.info() $ df2.head()
df_complete.iloc[790]['text']
%time headdtm2 = stfvect.fit_transform(revs.head(20000))
np.exp(results4.params) $
model.predict(training_data[:5])
sales_change2.head()
(val_texts $  .groupby('SDG') $  .count())
ct = pd.crosstab(titanic['survived'],titanic['sex']) $ ct
%writefile /tmp/test.json $ {"dayofweek": "Sun", "hourofday": 17, "pickuplon": -73.885262, "pickuplat": 40.773008, "dropofflon": -73.987232, "dropofflat": 40.732403, "passengers": 2}
print(dfh['Centrally Ducted or Ductless'].unique()) $ dfh['Centrally Ducted or Ductless'].value_counts()
df.groupby("three_day_reminder_profile_incomplete_sent")["cancelled"].mean()
df = pd.read_csv('Tweets_retweet_count.csv') $ df.head()
mins = np.array([str(datetime.now() + timedelta(seconds=el)) for el in range(100)], $                dtype=np.datetime64) $ mins[:3]
!$TARGETCALIBPATH/apply_calibration -i refdata/Run17473_r0.tio -p refdata/Run17438_ped.tcal -x
print additional_limit.shape $ print additional_limit_paid_off.shape $ print additional_limit_outstanding.shape
print(len(props.prop_name.unique()))
weekdays = [] $ for i in range(0, len(date_df)): $     weekdays.append(date_df.iloc[i]['date'].strftime('%A'))
contentParagraphs = [] $ for pTag in contentPTags: $     contentParagraphs.append(re.sub(r'\[\d+\]', '', pTag.text)) $ contentParagraphsDF = pandas.DataFrame({'paragraph-text' : contentParagraphs}) $ print(contentParagraphsDF)
current_len = len(youtube_urls.items()) $ print(current_len)
df3['ab_page'] = pd.get_dummies(df3.group)['treatment']
df.show(5)
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'CA']]) $ results = log_mod.fit() $ results.summary()
potholes['Unique Key'].groupby(by= potholes.index.dayofweek).count().sort_values(ascending = False).head(1)
giss_temp.info()
sfs1.subsets_
from sklearn.model_selection import cross_validate $ from sklearn.model_selection import cross_val_score
contribs[contribs.calaccess_committee_id==1371855].info()
energy_indices.pct_change().mul(100).plot.bar(figsize=(7.5,5));
groceries.drop('apples', inplace = True) $ groceries
train.head()
c=table.find(text='Crew').find_next('td').text $ crews=re.search(r'\d+', c).group() $ crews
tweets_raw["date"] = tweets_raw["date"].apply(lambda d: parse(d))
print bnb2[bnb2['age']>80].shape $ print bnb[bnb['age']>80].shape $ print bnb['age'].shape $ 2771/213451
autos.info() $
tweet_archive_clean.timestamp.head()
db_users = pd.read_sql_query ("SELECT * FROM \"user\"", conn) $
sgdScore = SGDC.score(x_test, y_test) $ print ("Model Accuracy: %f" % sgdScore)
archive_copy['timestamp'] = pd.to_datetime(archive_copy['timestamp']) $ archive_copy.info()
combined_df2.fin_year.value_counts()
p=np.mean([p_old,p_new]) $ p
float(result[result['dt_deces'].notnull()].shape[0]) / float(result.shape[0]) * 100.
model_df['target'] = model_df.close.shift(-1) $ model_df['next_day_open'] = model_df.open.shift(-1) $ model_df['true_grow'] = model_df[['target', 'next_day_open']].apply(lambda x: 1 if x[0] - x[1] >= 0 else 0, axis=1)
all_data_wide.fillna(0).to_csv(filename_output, index=False)
print data_df.clean_desc[20] $ print 'becomes this ---' $ print text_list[20]
df = tweet_archive_clean.copy() $ df.set_index('timestamp', inplace=True) $ df.describe()
intervention_train.reset_index(inplace=True) $ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL'], inplace=True)
from datetime import datetime $ df['created_at'] = df['created_at'].apply(lambda x: datetime.strptime(x , "%a %b %d %H:%M:%S %z %Y"))
df3[['CA','UK','US']] = pd.get_dummies(df3['country']) $ df3.head()
df.isnull().sum() #Counts all null values
overall_topics.head()
pres_df['subjects'].nunique() # unique combinations of ad topics
print(donations['Donation Included Optional Donation'].value_counts()) $ print('percentage') $ print(donations['Donation Included Optional Donation'].value_counts(normalize=True))
response = Query(git_index).get_min("author_date")\ $                                    .by_authors("author_name")\ $                                    .fetch_aggregation_results() $ print(buckets_to_df(response['aggregations']['0']['buckets']).head())
plt.hist(shows['fixed_runtime'].dropna()) $ plt.title('Distribution of Runtime') $ plt.ylabel('Frequency') $ plt.xlabel('Runtime')
old_features = train.columns.tolist()
df.shift(-1)
channels = ['BBC','CNN','ABC'] $ for index,channel in enumerate(channels): $     mean = dfs[index]['len'].mean() $     print('Mean nuumber of characters for ' + channel + ':' ,mean)
from sklearn import linear_model $ logistic = linear_model.LogisticRegression(penalty='l1', C=0.5) $ logistic.fit(X_reduced, y) $ joblib.dump(logistic, "logistic"+'0410'+ '.pkl')
df2['user_id'].count()
df.loc[df.userLocation == 'Manchester, PA', 'tweetText'] $
df = df.sort_values(by=['day']) $ df.head()
contractor_merge['contractor_bus_name'].head() $
RFC_grid.best_estimator_
fig, axarr = plt.subplots( 1, 1, figsize=(20,10) ) $ plt.hist(editors.control["revs"], alpha = 0.5, range = (1, 21), normed = 1, bins = 20, label = "Source editor only") $ plt.hist(editors.treatment["revs"], alpha = 0.5, range = (1, 21), normed = 1, bins = 20, label = "Both editors") $ plt.legend(loc="upper right") $ plt.show()
log_user2 = df_log[df_log['user_id'].isin([df_test_user_2['user_id']])] $ print(log_user2)
grad_days_mean = records3[records3['Graduated'] == 'Yes']['Days_missed'].mean() $ grad_days_mean
data_temp10 = data[data['Temperature(C)'] >10] $ data_temp10.describe()
kick_projects[['goal', 'pledged', 'backers','usd_pledged','usd_pledged_real','usd_goal_real','state']].corr()
bm_model = DecisionTreeClassifier(**bg, random_state=101) $ bm_model.fit(X_train, y_train)
new_dems.Sanders.isnull().sum()
to_be_predicted_Day4 = 50.62613762 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
for c in cols: df[c]=df[c].astype(np.int16) $ df.info()
twitter=pd.read_csv('twitter-archive-enhanced.csv')
df[['beer_name', 'simple_style', 'brewery_name', 'brewery_country']][df.rating_score == worst_score]
testObj.outDF.tail()
csvData[csvData['street'].str.match('.*Avenue.*')]['street']
df.iloc[4]
stars.value_counts()[3]
!head -3 train.csv
p_new = df2[df2['converted'] == 1].user_id.nunique()/df2.user_id.count() $ p_new
_ = ok.submit()
weather.shape
backup = clean_rates.copy() $ clean_rates.cuteness = clean_rates.cuteness.astype(str) $ clean_rates.cuteness = clean_rates.cuteness.str.replace('nan', 'none') $ clean_rates.cuteness = clean_rates.cuteness.astype('category')
import os $ import pandas as pd $ import matplotlib.pyplot as plt $ import numpy as np
temp.plot(kind='bar', figsize=(10, 3));
new_df = pd.merge(df,orgs[['uuid','funding_rounds','funding_total_usd','status','homepage_url']],left_on = 'company_uuid',right_on = 'uuid') $ trunc_df = new_df[new_df.funding_rounds > 1].copy() $ trunc_df.to_csv('/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data/unsupervised/trunc_clustering.csv')
pca = PCA() $ a = pca.fit_transform(full_orig[['PastPCPVisits','Age','LOS']])
most_recent = train["Time Stamp"][(train["Next Order Date"].isnull())&(train["Product"]==38) & (train["Restaurant"] == 84)].all() $ display(most_recent)
df.head(2)
fire_pred = log_mod.predict_proba(climate_vars)
menu_about_latent_features = pd.DataFrame(menu_about_latent_features_with_menu_ids, columns=menu_about_latent_features_column_names)
dem["date"] = dem.apply(lambda line: datetime.strptime(str(line['date']),"%m/%d/%y").date(),axis=1)
df.info()
top_songs = pd.read_csv('./data.csv')
autos['registration_year'].describe()
from sklearn.linear_model import LinearRegression $ model = LinearRegression()
df.describe(percentiles=[0.9,0.3,0.2,0.1])
df_archive_clean["source"] = df_archive_clean["source"].replace('<a href="https://about.twitter.com/products/tweetdeck" rel="nofollow">TweetDeck</a>', $                                                                "TweetDeck")
tweets.retweeted.value_counts()
without_condition_heatmap = folium.Map([41.90293279, -87.70769386], $                zoom_start=11) $ print(without_condition_heatmap) $ without_condition_heatmap
tt1['RECORDDATE'] = pd.to_datetime(tt1['RECORDDATE'], format='%Y-%m-%d %H:%M:%S')
returns = price.pct_change()
S.decision_obj.stomResist.value = 'Jarvis' $ S.decision_obj.stomResist.value
my_dict = first_element $ for my_key in my_dict.keys(): $     my_val = my_dict[my_key] $     print my_key, ":", my_val
engine.execute('SELECT * FROM stations LIMIT 10').fetchall()
df[df['Descriptor'] == 'Loud Music/Party']['Unique Key'].groupby(df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot()
reddit_comments_data.count()
for i in range(1000): $     new_page_converted = np.random.choice([1,0], size=df_newlen, p=[pnew,(1-pnew)]).mean() $     old_page_converted = np.random.choice([1,0], size=df_oldlen, p=[pold,(1-pold)]).mean() $     diff = new_page_converted.mean() - old_page_converted.mean() $     p_diffs.append(diff)
for cat in dfm['cat_type'].unique(): $     dfm['goal_'+cat] = dfm['goal_log'] $     dfm.loc[dfm.cat_type != cat, 'goal_'+cat] = 0.0 $ dfm = dfm.drop('goal_log', axis=1)
dcrime["crime_inc"] = 1 $ dcrime_gb = dcrime.loc[dcrime_in,["long","lat","crime_inc"]].groupby(["long","lat"],as_index=False).sum()
r = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv') $ code = r.status_code $ print(code) $ open('image-predictions.tsv', 'wb').write(r.content) $
treatment_df = df2.query('group == "treatment"') $ treatment_conv = treatment_df.query('converted == 1').user_id.nunique() / treatment_df.user_id.nunique() $ treatment_conv
bsfr.describe()
archive_copy['dog_description'] = archive_copy['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True)
unique_words_sk = set(words_sk) $ corpus_tweets_streamed_keyword.append(('unique words', len(unique_words_sk))) # update corpus comparison $ print('Number of unique terms: ', len(unique_words_sk))
lowest = session.query(func.min(Measurement.tobs)).\ $     filter(Measurement.station == "USC00519281").all() $ print(f"Lowest Temp: {lowest}")
autos[autos['price'] > 200000].sort_values(by='price')
train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)
df_onc_no_metac[ls_other_columns] = df_onc_no_metac[ls_other_columns].applymap(clean_string)
print list(label_encoder.inverse_transform([0,1,2])) $ model.predict_proba(np.array([0,50])) #first value is the intercept
df.groupby('season')['episode_id'].nunique().agg(['min', 'mean', 'max'])
plt.figure(1, figsize=(10,5)) $ plt.plot(lyra_lightcurve.data.index, lyra_lightcurve.data['CHANNEL3'], color='b') $ plt.plot(lyra_lightcurve.data.index, lyra_lightcurve.data['CHANNEL4'], color='r') $ plt.ylabel('Flux (Wm^2)') $ plt.show()
new_albums = pd.read_csv("new_albums.csv") $ top_200 = pd.read_csv("top_200_table.csv",sep="\t",index_col = 0) $ del new_albums["Unnamed: 0"]
fig1 = (taxi_sample["tip_pct"] $          .to_pandas() $          .hist(figsize=(8, 4), range=[0, .4], density=True, cumulative=True) $ ) $ plt.show(fig1)
max_NASDAQ=stocks_happiness_rev1['NASDAQ'].max() $ max_DJIA=stocks_happiness_rev1['DJIA'].max() $ max_SP500=stocks_happiness_rev1['SP500'].max() $ max_Russell1000=stocks_happiness_rev1['Russell1000'].max() $
firefox_binary = FirefoxBinary(tor) $ if sys.platform=="win32": $     browser = webdriver.Firefox(firefox_binary=firefox_binary, capabilities=capabilities) $ else: $     browser = webdriver.Firefox(firefox_binary=firefox_binary)
b. #need to execute last declaretion at first
drop_cols = ['project_is_approved','id','teacher_id'] $ X = train.drop(drop_cols, axis=1) $ y = train['project_is_approved'] $ feature_names = list(X.columns)
store_items.count()
print(len(auto_new.CarModel.unique())) $ print(auto_new.CarModel.unique())
print("Number of Mitigations in Enterprise ATT&CK") $ mitigations = lift.get_all_enterprise_mitigations() $ print(len(mitigations)) $ df = json_normalize(mitigations) $ df.reindex(['matrix', 'mitigation', 'mitigation_description', 'url'], axis=1)[0:5]
serious_start_count = 0 $ for row in data: $     if re.search("^[\[\(][Ss]erious[\]\)]", row[0]): $         serious_start_count = serious_start_count + 1 $ print("[1] serious_start_count: " + str(serious_start_count))
n = 4 $ GM=GaussianMixture(n_components=n) $ GM.fit(finalDf.iloc[:,:-1]) $ res_Mix=GM.predict(finalDf.iloc[:,:-1])
from pandas.tools.plotting import scatter_matrix $ scatter_matrix(mean_sea_level, figsize=LARGE_FIGSIZE);
old_df1 = ab_data[(ab_data['group']=='treatment') & (ab_data['landing_page']=='new_page')] $ old_df2 = ab_data[(ab_data['group']=='control') & (ab_data['landing_page']=='old_page') ] $ df2 = old_df1.append(old_df2) $ n_rows - df2.count() $
cityID = 'a307591cd0413588' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Buffalo.append(tweet) 
%%time $ out_degree_centrality = convert_dictionary_to_sorted_list(nx.out_degree_centrality(network_friends))
df_archive.iloc[19,:]
merged_df_cut.shape
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'pure_scorers'
df['date_added'] = pd.to_datetime(df['date_added'])
more_recs = pd.read_csv("../data/microbiome/microbiome_missing.csv").head(20)
results=results.to_frame().reset_index() $ results.columns=['item_id','users'] $ results.head()
print(temp_long_df['date'].min(), temp_long_df['date'].max())
combined_df['Returned'] = combined_df['Returned'].fillna('No') $ print(combined_df.head(5))
train_ratio = 0.75 $ train_size = int(samp_size * train_ratio); train_size $ val_idx = list(range(train_size, len(df)))
df_merge.favorite_count.max()
fig, axarr = plt.subplots(1, 2, figsize=(12, 6)) $ _ = df_passengers["created"].plot(kind="density", ax=axarr[0]) $ _ = axarr[0].set_title("Passenger Account Creation Density") $ _ = df_pilots["created"].plot(kind="density", ax=axarr[1]) $ _ = axarr[1].set_title("Pilot Account Creation Density")
bsf = before_sherpa["Date"].value_counts() $ bsf.to_csv("GG.csv") $ cols = ['Day', 'Count'] $ bsfr = pd.read_csv("GG.csv", header=None, names=cols) $
results.summary()
df_protest = df_protest.dropna(subset=["Rural"])
import seaborn as sns $ planets = sns.load_dataset('planets') $ planets.shape
data['intercept'] = 1.0
sources = data_set["source"].value_counts()[:5][::-1] $ plt.barh(range(len(sources)), sources.values) $ plt.yticks(np.arange(len(sources)) + 0.4, sources.index) $ plt.show()
holdout_results.head()
P_new = df2[df2['landing_page'] == 'new_page']['converted'].mean() $ print("P_new under null: ", P_new)
filtered_hashtags = set([el.hashtag for el in filtered_greater_100.select('hashtag').collect()])
S_lumpedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_lumpedTopmodel.txt')
info.info()
df['Views-PercentChange'][df['Views-PercentChange']==0].count()
from nltk.tokenize import word_tokenize $ import string $ import ftfy
nold_sim = df2.loc[(df2['landing_page'] == 'old_page')].sample(nold,replace = True) $ old_page_converted = nold_sim.converted $ old_page_converted.mean()
cars['vehicleType'].fillna('none', inplace=True) $ cars['gearbox'].fillna('none',inplace=True) $ cars['model'].fillna('none',inplace=True) $ cars['fuelType'].fillna('none',inplace=True) $ cars['notRepairedDamage'].fillna('none',inplace=True)
bp = USvideos.boxplot(column='like_ratio', by='category_name', vert = False) $
df_ = df.groupby('msno').apply(within_n_days,T, n = 90).reset_index(drop = True)
temp_df['avg'].plot(kind='bar', color='coral', alpha=0.7, yerr=temp_df['max'] - temp_df['min']) $ plt.title("Trip Avg Temp") $ plt.savefig("Resources/trip_avg_temp.png") $ plt.xticks([])
y = x.loc[:,"A"] $ y.mean() $ np.mean(y)
first_result.contents[1]
df.query('Confidence == "A:e"')
results["workers"].head()
ok.grade('q01')
import numpy as np $ print(np.info(np.fromiter))
df.year.unique()
red_4.isna().sum()
twitter_archive_df_clean.name.value_counts()[:20]
datatmp.pivot_table(index=datatmp.index,values="Total Gallons",aggfunc=sum)#.plot(legend=False) $
df_latest.head(1)
from pandas_datareader import wb $ all_indicators = wb.get_indicators() $ all_indicators.ix[:,0:1]
hist(df3.tripduration, bins = 20, color="red", label = "Female", normed = 1) $ plt.xlabel("Trip Duration in seconds", fontsize=12) $ plt.ylabel("Amount of trips", fontsize=12) $ plt.title("Trip Duration Histogram female users", weight='bold', fontsize=14) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
sp_500_adj_close.tail()
df = df[(np.log(df.price)>12) & (np.log(df.price)<14)]
df2_copy.shape == df2.shape
p_diffs = np.array(p_diffs) $ (p_diffs > obs_diff).sum()/len(p_diffs)
aapl.plot()  # plots ALL Series at one $ plt.show()
difflib.SequenceMatcher(None, 'BARCLAYS CAPITAL', 'BARCLAYS CAPITAL, INC.').ratio()
df_grouped_mean = df.groupby('year_month').mean().reset_index()[['year_month', 'rating_numerator', 'favorite_count', 'retweet_count']]
x = x[x.index.minute == 20] $ x = x[x.index.hour == 10] $ x
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old,n_new],alternative='smaller') $ z_score, p_value
autos["odometer_km"].value_counts().sort_index(ascending=False)
one_station.sort_values(by=['DATE', 'weekday'], inplace=True)
recipes.ingredients.str.contains('[Cc]inamon').sum()
df_stars_cleaned = df_stars.set_index('user_id').loc[df_users.index].reset_index() $ df_stars_cleaned.head()
transactions.info()
file_name = "./data/train_preprocessed2.csv" $ train_df2 = pd.read_csv(file_name, low_memory = False) $ train_df2.head()
fb_day_time.head()
df_Left_trans_users = pd.merge(transactions,users,how="left",on="UserID") $ df_Left_trans_users
df.iloc[2893]
station_distance.columns.tolist()
avg_price= df2.pivot_table(values=["price"], index=['vehicleType'], aggfunc=np.mean) $ print(avg_price)
sample_groups_sorted_by_members = sample_groups.sort_values([ 'MSA_CODE', 'members' ], ascending=False) $ sample_groups_sorted_by_members.groupby([ 'MSA_NAME']).apply(lambda x: x.head(5))
pd.read_html("http://www.imdb.com/chart/top?ref_=nv_wl_img_3")[0]
X_tfidf_df.head(10)
np.savetxt(r'text_preparation/reviews.txt', reviewsDFslice['text'], fmt='%s')
df.rename(columns={'PUBLISH STATES':'Publication Status', 'WHO region':'WHO Region'}, inplace=True) $ df.head(2)
re.findall('\s\w\w\w\w?\s', synopsis)
%%bash $ p2o.py --enrich --index git_raw --index-enrich git -e http://localhost:9200 --no_inc --debug git https://github.com/grimoirelab/perceval.git
results = lrm.fit() $ results.summary()
%run billboard.py $ billboard = pd.DataFrame(billboard) $ billboard.columns = ['year','artist','track','time','date.entered','week','rank'] $ billboard.info()
rng = pd.date_range(px.index[0], periods=len(px) + N, freq='B')
afx = dict(r.json())
repos = pd.read_csv('data/new_subset_data/new_subset_repos.csv', sep='\t') $ users = pd.read_csv('data/new_subset_data/new_subset_users.csv', sep='\t') $ owns = pd.read_csv('data/new_subset_data/new_subset_owns.csv', sep='\t') $ stars = pd.read_csv('data/new_subset_data/new_subset_stars.csv', sep='\t') $ final_data = pd.read_csv('data/new_subset_data/ratings_data.csv', sep='\t') $
dfs_resample.reset_index(inplace = True) $ dfs_resample['TIME'] = pd.to_datetime(dfs_resample['DATE_TIME']).dt.hour $ dfs_morning = dfs_resample[(dfs_resample['TIME'] == 6) | (dfs_resample['TIME'] == 12)]
title_sum = preproc_titles.sum(axis=0) $ title_counts_per_word = list(zip(pipe_cv.get_feature_names(), title_sum.A1)) $ sorted(title_counts_per_word, key=lambda t: t[1], reverse=True)[:]
print(list(dfClientes.columns))
sample.loc[case_mask, ['date', 'variable', 'National']]
intervention_history['next_incident_type'] = groups.INCIDENT_TYPE_NAME.transform(lambda s: s.shift(-1))
c.info()
cat_sz = [(c, len(joined_samp[c].cat.categories)+1) for c in cat_vars]
print(joined.info())
click_condition_meta.info()
df = sqlContext.createDataFrame(rows)
chars_not_used = pca_plot(model, characters,'Word2Vec: Characters') $ print('Characters not used:') $ print(', '.join(chars_not_used))
print('This is the list of folders in your directory for this HydroShare resource.') $ test = [each for each in os.listdir(homedir) if os.path.isdir(each)] $ print(test)
fig = plt.figure(figsize=(14,10)) $ ax = fig.add_subplot(111, projection="aitoff") $ ax.scatter(coord[in_region].l.radian, coord[in_region].b.radian)
permits_df = permits_df[['LAT', 'LON', 'IssuedDate']] $ permits_df['YEAR'] = permits_df['IssuedDate'].apply(get_year) $ permits_df['LAT_LONG_COORDS'] = pd.Series(list(zip(permits_df['LON'], permits_df['LAT']))) $ permits_df.head()
autos["price"] = autos["price"].str.replace("$", "").str.replace(",", "").astype(int) $ autos.rename({"price" : "price_euro"}, axis = 1, inplace = True)
samp311.agency.value_counts()
tweet_archive_clean.loc[tweet_archive_clean['tweet_id'] == '681340665377193984',['tweet_id', 'text', 'rating_numerator', 'rating_denominator']]
df_sched2.insert(loc=0, column='ProjectId', value=df_sched['ProjectId'])
retention.set_index(['CohortGroup', 'CohortPeriod'], inplace=True)
html = load_file('moby_22_2.html')
temp1=df.query('group =="treatment" and landing_page =="old_page"') $ temp2=df.query('group =="control" and landing_page =="new_page"') $ temp1.shape[0]+temp2.shape[0]
absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus), $                              name="absent_error_raw") $ absent_error = tf.reshape(absent_error_raw, shape=(-1, 10), $                           name="absent_error")
autos["last_seen"].str[:10].value_counts(normalize=True,dropna=False).sort_index(ascending=True)
np_diff = np.diff(close) $ max(np_diff) $
baseball.ab.corr(baseball.h)
test_predicted = model.predict([Q1_test, Q2_test])
pd.date_range('2017-12-30', periods=4)  # 4 values using the default frequency of day
X_score.shape
train_downsampled = sqlContext.read.parquet(S3_HOME + '/train_downsampled.parquet') $ testing = sqlContext.read.parquet(S3_HOME + '/testing.parquet') $
data.plot()
import os $ REPO = "/content/datalab/gcp-data-analyst" $ os.listdir(REPO)
train.groupby('popular').num_comments.describe().unstack()
ward = joblib.load('./data/Ward.pkl')
dfMonth = dfMonth.groupby(['Date', 'Project Name',   $                          'Estimated Start', 'Estimated End', 'Estimated Duration (Days)',  $                          'Sales Stage', 'Likelihood of Win', 'Likelihood Percent', 'Contract Value', $                          'JTBD', 'Big Bet', 'Client Type: Direct Impact', 'Client Type: Funder', 'New/Repeat Client', $                          'Contract Band Size'])[['Weighted Value', 'Contract Value (Daily)']].sum().reset_index()
np.linspace(0, 100, 5)
df = pd.DataFrame() $ print(df)
df1.dropna(inplace=True)
res = res[~res.match_0.isnull()] $ len(res)
trunc_df.company_name = trunc_df.company_name.str.lower() $ trunc_df[trunc_df['homepage_url'].str.contains("airbnb.com",na=False) & trunc_df['company_name'].str.contains("airbnb",na=False)].index[0]
measure.tail()
investors_df.hist(column = 'most_recent_active_year',bins = 20, figsize = (20,8))
new_df['index'] = pd.to_datetime(new_df['index'])
logit3 = sm.Logit(df4['converted'], $                            df4[['intercept', 'ab_page', 'country_UK', 'country_US']]) $ result3 = logit3.fit() $ result3.summary()
from scipy import stats # to fix bug "AttributeError: module 'scipy.stats' has no attribute 'chisqprob'" $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) #bug fix  "AttributeError: module 'scipy.stats' has no attribute..." $ logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = logit_mod.fit()
law.to_csv(folder + "\\" + "law-all.txt", sep="\t", index=False)
df_geo_segments.head(3)
X = pd.get_dummies(X, prefix='sub')
data.info(verbose=False)
pd.Series(dates)
airbnb_df = pd.read_csv('data/listings.csv', usecols=cols)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new],[n_old, n_new]) $ (z_score, p_value)
import gensim, logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) $ sentences = [x.split(" ") for x in tt] $
bListenCount = sc.broadcast(trainData.map(lambda r: (r[1], r[2])).reduceByKey(lambda x,y: x+y).collectAsMap()) $ def predictMostListened(allData): $      return allData.map(lambda r: Rating(r[0], r[1], bListenCount.value.get( r[1] , 0.0))) $
temp_long_df.describe()
model_unigram.most_similar(positive=['house','pool'],topn=5)
result1 = df[(df.A < 0.5) & (df.B < 0.5)] $ result2 = pd.eval('df[(df.A < 0.5) & (df.B < 0.5)]') $ np.allclose(result1, result2)
df['time']=pd.DataFrame(df.date.str.split(' ').tolist(), columns = "date time".split())['time'].values $ df
bars = ticks.Price.resample('1min', how='ohlc') $ bars
liquor2016_q1_days = pd.DataFrame.from_dict(liquor2016_q1.Date.groupby(liquor2016_q1.StoreNumber).unique().apply(lambda x: len(x))) $ liquor2016_q1_days.columns = ['Days'] $ liquor2016_q1_days.tail()
twitter_archive_master['rating_denominator'].value_counts()
manager.image_df[manager.image_df['filename'] == 'image_sitka_spruce_71.png'] $
math.sqrt(((lmscore.predict(X_test)-y_test)**2).mean())
print("Saving dictionary to elasticsearch - please be patient") $ es.saveToEs(df_dict,index=es_dictindex,doctype=es_dicttype)
df['review'] = df['review'].apply(preprocessor) #use the apply method and send in the preprocessor function (applys the function to each row) $
%timeit df.set_index(['date', 'item']).unstack()
S_distributedTopmodel.meta_basinvar.filename
import pyLDAvis.gensim $ pyLDAvis.enable_notebook() $ data = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False) $ pyLDAvis.save_html(data, 'data/lda.html')
weblogs = spark.table("weblogs") $ weblogs.limit(10).toPandas()
from sklearn.linear_model import LogisticRegression $ logreg = LogisticRegression() $ print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())
transactions['membership_expire_date']  = transactions.membership_expire_date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d')) $ transactions['transaction_date']  = transactions.transaction_date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d')) $
ryanair = df[df["text_3"].str.contains("ryanair", case = False)] $ ryanair.text_3.str.split(expand=True).stack().value_counts().head(10).reset_index()
trump_origitnals["lText"] = trump_o["text"].map(lambda x: x if type(x)!=str else x.lower())
twitter_archive_enhanced_clean = twitter_archive_enhanced.copy()
pipeline.steps[1][1].feature_importances_
weather_yvr_dt = weather_yvr_dt.set_index('Datetime') $ weather_yvr_dt.head()
twitter_ar['date'] = twitter_ar['timestamp'].apply(lambda row: row.split(' ')[0]) $ twitter_ar['time'] = twitter_ar['timestamp'].apply(lambda row: row.split(' ')[1])
import numpy as np $ result=results.home_score $ result.apply(np.median)
from sklearn.linear_model import LogisticRegression
prec_pred.head()
def var_summary(x): $     return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.quantile(0.01), x.quantile(0.05),x.quantile(0.10),x.quantile(0.25),x.quantile(0.50),x.quantile(0.75), x.quantile(0.90),x.quantile(0.95), x.quantile(0.99),x.max()], $                   index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX']) $ df.apply(lambda x: var_summary(x)).T
click_condition_meta.loc[click_condition_meta.dvce_type == "Unknown"].head(2)
df_all.describe()
no_specialty = merged1[appointments['Specialty'].isnull()] 
for item in shopify_full_size_only[(shopify_full_size_only['Email']=='alpa.poddar@gmail.com')]['child_sku'].unique(): $     print(item in get_items_purchased('alpa.poddar@gmail.com', product_train, customers_arr, products_arr, item_lookup)['child_sku'].unique()) $
station_columns = inspector.get_columns('station') $ for column in station_columns: $     print(column["name"], column["type"])
treatment_converted = (df2.query('group == "treatment" and converted == 1').count()[0]) $ treatment_total = (df2.query('group == "treatment"').count()[0]) $ treatment_prob = float(treatment_converted) /  float(treatment_total) $ print ("Probability of Treatment Group converted is {}".format(treatment_prob))
test = tmax_day_2018.sel(lat=16.75, lon=81.25).to_dataframe()
autos['brand'].unique() $ print(len(autos['brand'].unique()))
all_brands = autos["brand"].unique() $ print("Unique brands in autos dataset:\n{}".format(all_brands))
grp[['Age','Salary']].transform('sum')
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
start = time.time() $ with Pool(2) as p: $     print(p.map(sum_prime, [300000, 600000, 900000])) $ print("Time taken = {0:.5f}".format(time.time() - start))
borough_population = question_3_dataframe.\ $     drop_duplicates(['borough', 'incident_zip', 'population']).\ $     groupby('borough')['population'].\ $     agg('sum') $ borough_population
engine = create_engine("sqlite:///hawaii.sqlite")
total_order_per_user = prior_orders.groupby('user_id').size().reset_index(name='total_order_per_user') $ total_order_per_user.head()
df_new['stack_reputation'] = u.reputation.format()
projects.head(3)
apps_by_dist = df_nona.groupby(['segment', 'district_id']).app_id.nunique() $ print apps_by_dist.groupby(level='segment').mean() $ apps_by_dist.groupby(level='segment').describe() $
poverty_2011_2015.head()
df_events.head()
y_pred_df = pd.DataFrame(y_pred)
df_with_test_names = df.copy() $ df.drop(['name'], axis=1, inplace=True) $ df.drop('abtest', axis=1, inplace=True)
r6s.shape
%time $ test_5 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop.copy() $ test_5 = dd.from_pandas(test_5, npartitions=50)
aug2014 = pd.Period('2014-8',freq='M') $ aug2014
sitetype = 'spring' $ pd.read_csv("http://vocabulary.odm2.org/api/v1/sitetype/{}/?format=csv".format(sitetype))
brand_pct = dict(autos["brand"].value_counts(normalize=True))
df2[['xx','ab_page']]=pd.get_dummies(df['group']) $ del df2['xx'] $ df2['intercept']=1 $ df2.head()
auth_key = 'yZVrH8Esn8zs7vGPN2zJ' $ main_df = pd.DataFrame # Lets create a blank dataframedf = quandl.get("FMAC/HPI_KOKIN", authtoken=auth_key)
data1.head()
df_ca[df_ca['ab_page'] == 0]['converted'].mean()
new_sample = pd.concat([new_sample,pd.get_dummies(new_sample['created_date'].apply(lambda x: x.month))],axis=1)
svm_predicted = svm_model.predict(X_test)
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
df.isnull().sum()
df = pd.read_sql_query('SELECT ComplaintType, Descriptor, Agency ' $                        'FROM data ' $                        'WHERE Agency IN ("NYPD", "DOB")' $                        'LIMIT 10', disk_engine) $ df.head()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)]) $ old_page_converted.mean()
deployment_details = client.deployments.create(name="k_flwr_test3", model_uid=model_guid)
aggregates['Email Address'] = aggregates['Email Address'].apply(lambda x: str(x.lower().strip())) $ aggregates['Email Address'] = aggregates['Email Address'].astype(str)
df.rename(columns={'Indicator':'Indicator_Id'}).head(2)
df2['datetime'].min(), df2['datetime'].max()
df2['intercept'] = 1 $ lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = lm.fit()
print(np.random.normal(168.156, 2))
np.corrcoef(data['int_rate'], data['fico_range_low'])
interactions_recruiter = pd.read_csv(folderData + 'interactions_recruiter.csv')
pd.Series(['a', 'a', 'b', 'c'] * 4).describe()
df.select('latitude').distinct().sort('latitude', ascending=True).show(10)
ab_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ p_diff = np.array(p_diff) $ print("The proportion of the p_diffs greater than difference observed in ab_data.csv is {}".format((ab_diff < p_diff).mean() $ ))
df_CV = pd.DataFrame(X) $ df_CV['y'] = y $ df_CV.columns = variable_name $ df_CV.head(10) # training set without tf-id
c_conv = gb.loc['control', 1][0] $ c_no_conv = gb.loc['control', 0][0]
user_total = df.nunique()['user_id'] $ print("all the Unique users are : {}".format(user_total))
zf = zipfile.ZipFile(path) $ df = pd.read_excel(zf.open('Sample_Superstore_Sales.xlsx')) $ df.head(5)
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set()
df_A[(df_A.Student_height>160) & (df_A.Student_weight<80)].head(1)
df3[['CA','UK','US']] = pd.get_dummies(df3['country']) $ df3 = df3.drop(df3['CA'])
keydf = pd.DataFrame(pd.Series([i for i in keyli if not bool(re.search("[^0-9]", str(i)))]).unique())
sess = tf.InteractiveSession()
foo = pd.Series([np.nan, -3, None, 'foobar']) $ foo
autos["price"] = autos["price"].str.replace("$", "").str.replace(",", "").astype(int) $ autos["odometer"] = autos["odometer"].str.replace("km", "").str.replace(",", "").astype(int) $ autos.rename({"odometer": "odometer_km"}, axis = 1, inplace = True)
url = form_url('handednesses', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
merge.info()
run txt2pdf.py -o"2018-06-18  2015 853 discharges.pdf"  "2018-06-18  2015 853 discharges.txt"
merge_left = pd.merge(url_authors, url_votes) $ merge_left = pd.merge(merge_left, url_payouts) $ unique_urls = pd.merge(merge_left, url_reputation)
df_int = (df / df.iloc[0]) - 1 $ df_int.head()
dataA.head()
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_index()
df[['life_sq','num_room']].apply(lambda x:x[0]/x[1],axis=1)
mean_weekday = daily_averages.groupby('Weekday').mean() $ mean_weekday.plot(kind='bar', ylim=(0, 5000))
%%HTML $ <iframe src="date.htm" width=1250 height=250>
intersections_irr.info()
print('Row zero, column zero:', data.iloc[0,0])
df_cont.apply(lambda c: sum(c.isnull()),axis=0) 
hstotal[hstotal.notnull()]
my_model_q8 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='label', useEntireData='True') $ my_model_q8.fit(X_train, y_train) $ y_pred = my_model_q8.predict(X_valid) $ accuracy = metrics.accuracy_score(y_valid, y_pred) $ print("Accuracy: " +  str(accuracy))
results.summary2() # since in my system results.summary() is giving an error i have used results.summary2()
vocabulary_expression['component_2'].sort_values(ascending=False).head(7) $
learn.metrics = [accuracy, auc]
df_20180113_filtered = filter_clean(df_20180113) $ df_20180113_filtered.head()
grades.mean()
df.Date.dtype
errors = pd.DataFrame.from_dict(d, orient="index").rename(columns={0:"mae_vals"}) $ errors.head()
load2017.isnull().sum()
origin = pd.read_csv('5to5.csv')
temp1 = np.array([ $         ['', '', ''], $         ['', '', ''], $         ['1', '2', '3'] $     ], str)
from bisect import bisect $ def commission_rate(sales_value, tier_ceiling=[1000, 2000, 3000], $                     rates=[0, 0.01, 0.03, 0.05]): $     index = bisect(tier_ceiling, sales_value) $     return rates[index]
df_new.hist(figsize=(10,5)) $ plt.show() $
ex2.sort_values(ascending = False, inplace = True) $ ex2
coefs = zip(predictor_cols,model.coef_) $ for coef in coefs: $     print(f"Our model predicts that for a unit increase in {coef[0]}, a team will receive **{round(coef[1],2)}** more mentions in the NYT during the regular season.\n")
df_treatment_group = df.query('group == "treatment"') $ df0 = df_treatment_group[df_treatment_group.landing_page == "new_page"] $ df_control_group = df.query('group == "control"') $ df1 = df_control_group[df_control_group.landing_page == "old_page"] $ df2 = pd.concat([df0, df1])
df = pd.read_csv('/Users/yaru/Downloads/dataset/last_2_years_restaurant_reviews.csv')
init = tf.global_variables_initializer() $ sess.run(init)
raw_df = data.import_raw_data() $ raw_sample_df = raw_df.sample(4, random_state=42) $ display(raw_sample_df.set_index('Name').T)
df3 = df2.copy() $ df3['intercept'] = 1 $ df3[['remove','ab_page']] = pd.get_dummies(df['group']) $ df3.drop('remove', axis=1, inplace=True) $ df3.head()
plt.plot(ticks, dataset)
from quantopian.pipeline.data import Fundamentals $ from quantopian.pipeline.filters.fundamentals import IsPrimaryShare
df1.head()
session.query(func.count(User.name), User.name).group_by(User.name).all()
pd.to_datetime(pd.Series(['Jul 31, 2009', 'Nov 22 1985', '2005/11/22']))
frame = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])
probs_planets = [0.05, 0.03, 0.18, 0.34, 0.10, 0.16, 0.14]
dicttagger_beverages = DictionaryTagger(['beverages.yml'])
from matplotlib_venn import venn2 $ venn2(subsets={'10': n_titles-n_titles_tags, '01': n_tags-n_titles_tags, '11': n_titles_tags}, set_labels = ('Titles', 'Tags')) $ print "Total of",len(bag_tags.intersection(bag_title)),"common keywords btw Tags and Title from a total of",\ $       len(bag_tags.union(bag_title)),"different keywords"
loans_df.isnull().sum()
i_aux=[] $ for i in range(0,l2): $     if i not in seq: $         i_aux.append(interventions[i]) $ col.append(np.array(i_aux))
all_39s_from_2011 = Grouping_Year_DRG_discharges_payments.loc[(slice(2011), slice(39)),:] $ all_39s_from_2011.loc[:,'discharges'][:5]
dfU.shape[0] - (1257+420+2629)
df_unit_after=pd.merge(df_unit,df_users_6_after[['uid']],left_on='uid',right_on='uid',how='inner')
m3.save('split_model_v1')
dfs[index_max]['Time'][dfs[index_max]['Outside Temperature'] == dfs[index_max]['Outside Temperature'].max()].values[0]
twitter_archive_clean[twitter_archive_clean['retweeted_status_user_id'].isnull()==False].shape[0]
old_page_converted = np.random.binomial(1, pold, nold)
employee = pd.read_csv('../../data/employee.csv') $ employee.head()
repos.info() $ print "-----------------------------------" $ users.info()
dfs[index_max].head()
import xgboost $ from xgboost import DMatrix $ from xgboost import XGBClassifier $ from sklearn.multiclass import OneVsRestClassifier
cols = tweet_json_df.columns.tolist() $ cols
train.info()
kimanalysis.queryitems('test', driver='ClusterEnergyAndForces__TD_000043093022_001')
full_dataset = full_dataset.sort_values('date')
ax = statistics['power_consumption'].plot(y='sum') $ ax.set_xlabel('Date') $ ax.set_ylabel('Total Power Consumption')
df.apply(np.mean,axis=1)
fillna(test_df) $ categoricalize(test_df) $ preds = pd.DataFrame({'Id' : range(1461,2920), 'SalePrice' : pd.Series(automl.predict(test_df))}) $ preds.to_csv('house/pred.csv', index=False)
prob_control = df2[df2.group == 'control']['converted'].mean() $ prob_control
twitter_ar.sample(2)
details.shape
db.domains
df['Unique Key'].groupby(df.index.hour).count().plot()
intersections_final.info()
house_data['grade'].unique()
df_clean3.loc[315, 'text']
import sys $ print(sys.version)
to_be_predicted_Day3 = 56.17900512 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
calculated_average_df = calculated_average_df[['Country', 'Month', $                                                'Death monthly average', 'Case monthly average']] $ calculated_average_df.head()
test.drop(columns = 'month_created' , inplace = True) $ train = train[list(test.columns) + ['is_fake']] $ train['user_agent'] = train['user_agent'].fillna('no_info')
injury_df['First_Name'] = injury_df['Relinquished'].apply(lambda y: y.split()[0]) $ injury_df['Last_Name'] = injury_df['Relinquished'].apply(lambda y: y.split()[1] if len(y.split())>1 else 'Unknown')
list_of_non_rle_and_secret_base_pscs = non_rle_pscs[non_rle_pscs.secret_base == True].company_number.unique().tolist() $ non_rle_company_and_secret_base_at_top_of_chain = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*0..]-(c2:Company)\ $ WHERE c2.uid IN {list_of_non_rle_companies_with_secret_base}\ $ RETURN DISTINCT (c1.company_number)",list_of_non_rle_companies_with_secret_base=list_of_non_rle_and_secret_base_pscs).data()) $ len(non_rle_company_and_secret_base_at_top_of_chain)
df_weather.head()
r = result_df.set_index(['metric_date', 'ticker'])
df_gnis.head()
%matplotlib inline $ modifications_per_authors_over_time.plot(kind='area', legend=None, figsize=(12,4))
txSenate = tx[tx['filerHoldOfficeCd'].isin(['STATESEN'])][tx['filerFilerpersStatusCd'] != "NOT_OFFICEHOLDER"] $ txHouse = tx[tx['filerHoldOfficeCd'].isin(['STATEREP'])][tx['filerFilerpersStatusCd'] != "NOT_OFFICEHOLDER"]
min_val = df_noblends[df_noblends[y_col] == 'pinot noir'].shape[0] $ print(min_val) $ is_pinot = df_noblends[df_noblends[y_col] == 'pinot noir'].sample(min_val) $ not_pinot = df_noblends[df_noblends[y_col] != 'pinot noir'].sample(min_val) $ ndf = pd.concat([is_pinot, not_pinot])
hm_sub = grid_heatmap[grid_heatmap['pr_fire'] >= 0.25] $ fire_hm = hm_sub[['glat', 'glon', 'pr_fire']].values
tweets['retweeted'].value_counts()
extraction_path = "/home/ubuntu/s3/flight_1_5/extracted" $ d = read_from_json_file(json_file_name) $ jsonl_file_name = json_file_name.replace('.json','.jsonl') $ with open(jsonl_file_name,'w') as f: $     json.dump(d, f) $
cvec_df.head(25)
import re $ s = dfn.News.iloc[0] $ wordList = re.sub("[^\w]", " ",  s).split()
new_page_converted = df2.sample(n_new, replace = True).converted $ new_page_converted.mean()
clusterNum = 3 $ k_means = KMeans(init = "k-means++", n_clusters = clusterNum, n_init = 12) $ k_means.fit(X) $ labels = k_means.labels_ $ print(labels)
train_df = model_df.iloc[1:725] $ test_df = model_df.iloc[725:-1]
data = pd.read_csv('data_redditv2.csv')
df_input_clean = df_input_clean.filter("`Resp_time` > 0") $ df_input_clean.toPandas().info()
np.sum(PyReverseDims(jArr), 0)
html = browser.html $ soup = bs(html, 'html.parser')
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print("z_score:",z_score,"p_value:", p_value)
sqlCtx = pyspark.SQLContext(sc) $ sdf = sqlCtx.createDataFrame(df.astype(str)) $ sdf.show(5)
job_requirements.size
print("Probability of control group converting is", $       df2[df2['group']=='control']['converted'].mean())
run txt2pdf.py -o"2018-06-14 2148 OROVILLE HOSPITAL Sorted by Discharges.pdf"  "2018-06-14 2148 OROVILLE HOSPITAL Sorted by Discharges.txt"
def power_cohort(row): $     if row['atleast_one_course_completed_or_not']=='Completed atleast one course (Course started after 1st July 2018)': $         row['cohort']='Power User' $     return row['cohort'] $ df_users_6['cohort']=df_users_6.apply(power_cohort,axis=1)
pivoted_data = popular_centers.pivot(columns="center_name", values="attendance_count") $ pivoted_data.head()
filtered_tweets = [] $ for tweet in collect.get_iterator(): $     filtered_tweets.append(tweet) $ len(filtered_tweets)
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'US', 'UK']]) $ result = logm2.fit() $ result.summary2()
noise_graf = noise_graf.rename(columns={"Descriptor":"noise_count"})
min_max_avg = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).all() $ min_max_avg
print("MSE:", metrics.mean_squared_error(y_test, predictions))
5 - 7 $ 12 + 21.5 $ 5 <= 2
df2.nunique()['user_id']
g = g.dropna(subset=['X Coordinate (State Plane)', 'Y Coordinate (State Plane)'])
preci_data = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date > last_year).\ $     order_by(Measurement.date).all()
my_zip =zipfile.ZipFile(dest_path,'r') $ list_names = [f.filename for f in my_zip.filelist] $ list_names $
df2[(df2['landing_page']=='new_page')].landing_page.count()/df2.landing_page.count()
print('Slope: Efthymiou vs experiment: {:0.2f}'.format(popt_ipb_chord_crown[2][0])) $ perr = np.sqrt(np.diag(pcov_ipb_chord_crown[2]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
for dimension in ['Parcel Status']: $     multi_table([ $     pd.DataFrame(df[df['Shipped At' ] > df['Updated At']].groupby(dimension)['Created At'].count()), $     pd.DataFrame(df[df['Shipped At'] <= df['Updated At']].groupby(dimension)['Created At'].count())])
week1_df.rename(columns={'24 - 28 July 2017' : 'title'}, inplace=True) $ week1_df['date'] = '24 - 28 July 2017' $ week1_df
col = ["num_25", "num_50", "num_75", "num_985", "num_100", "num_unq", "total_secs"] # remove date# ignore time dimension $ d = user_logs.groupby(["msno"], as_index=False)[col].sum()    $ d[d.msno == '29V0Jm3Xli1dy9UFeEL/BH2EMOr62DgeGLeKAKfE07k=']
crs = {'init': 'epsg:4326'} $ geometry = df_TempIrregular['lineString'] $ geo_irregularities = gpd.GeoDataFrame(df_TempIrregular, crs=crs,geometry = geometry)
for each_field in ca_de.columns.values: $     if each_field.find('HIT')>-1: $         print(each_field)
print(df2[df2['group']=='treatment']['converted'].mean())
autos["ad_created_year"] = autos["ad_created"].dt.year $ autos["ad_created_month"] = autos["ad_created"].dt.month $
file = root + 'gas/2018-2008_monthly_gas_NYC.csv' $ gas_df = pd.read_csv(file, header=0, skipinitialspace=True) $ gas_df.info() $ gas_df
bnb.columns
building_pa.columns[building_pa.columns != building_pa.columns.str.extract(r'^(\w+)$')] 
scores[scores.IMDB == min_IMDB]
topC['date_simple'] = pd.DataFrame(topC.date.dt.date) #not ideal python syntax, but for now it's ok
cat_sz = [(c, len(full_data[c].unique())) for c in cats]
Y_tweet = dft.groupby(['stamp'])['log_followers_count'].sum()
commits_per_repo[['user_id', 'repo_id', 'log10_commits']].to_csv('data/new_subset_data/entire_commits.csv', index=False)
df = df.loc[df['job_id'] == '2572']
p_diffs=[] $ for _ in range(10000): $     new_page_converted = np.random.binomial(1, p_new, n_new) $     old_page_converted = np.random.binomial(1,p_old, n_old) $     p_diffs.append(new_page_converted.mean()-old_page_converted.mean())
responsesJsonList = entities.apply(getJson)
worldbible.head()
opt_fn = partial(optim.Adam, betas=(0.7, 0.99))
after.head()
train_reduced.show(3)
listings['host_response_time'].value_counts()
df['day_of_week'] = df['date'].dt.dayofweek $ days = {0:'Mon',1:'Tues',2:'Wed',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}
training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=42)
img_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $
print("\nThere are {} toe marks with spaces surrounding hypens.".format(df.loc[df.toes.str.match(pattern1)==True].shape[0])) $ print("\nThere are {} toe marks with spaces surrounding numbers.".format(df.loc[df.toes.str.match(pattern2)==True].shape[0])) $ print("\nThere are {} toe marks with an ' preceeding the entry.".format(df.loc[df.toes.str.match(pattern3)==True].shape[0]))
df_train[df_train['labels'] != 2].to_csv(CLAS_PATH / 'train.csv', header=False, index=False) $ df_val.to_csv(CLAS_PATH / 'test.csv', header=False, index=False) $ (CLAS_PATH / 'classes.txt').open('w').writelines(f'{o}\n' for o in CLASSES) $ df_train.head()
lm=sm.OLS(dff['user_id'], dff[['intercept','ab_page']]) $
dateindex = pd.date_range(taxi_hourly_df.index.min(), taxi_hourly_df.index.max(), freq="1H")
pickle.dump(lda_cv_data, open('iteration1_files/epoch3/lda_cv_data.pkl', 'wb'))
df.head(5)
listings = pd.read_csv('../final_project/listings.csv.gz', nrows=100, compression='gzip', $                    error_bad_lines=False) $ print(listings)
unique_topic = page_details["topic"].unique() $ print(unique_topic) $ print("Number of distinct topics: ", unique_topic.shape[0])
status_dummies = pd.get_dummies(fb.status_type, dummy_na=True, prefix="st") $ fb_features = pd.concat((fb,status_dummies), axis=1) $ fb_features['message_character_count'] = fb_features['message'].apply(lambda x: len(x)) $ fb_features['message_word_count'] = fb_features['message'].apply(lambda x: len(x.split()))
bb_pivot_table_2.plot() $ plt.ylabel('Inches, Weight') $ ax = plt.gca() $ ax.set_title("Min & Max NBA Player Height & Weight Over Time") $ plt.show()
promo_df['after_onpromotion'] = promo_df['after_onpromotion'].astype(np.int16) $ promo_df['before_onpromotion'] = promo_df['before_onpromotion'].astype(np.int16)
to_be_changed = df2[df2['group']=='treatment'].index $ df2.loc[to_be_changed, 'ab_page'] = 1 $ df2[['intercept', 'ab_page']] = df2[['intercept', 'ab_page']].astype(int)
project_meetups_rdd = committee_names_df.repartition(500).rdd.map(lambda x: x.project).map(lambda name: lookup_relevant_meetup(name, max_meetup_events)) $ project_meetups_rdd.setName("Meetup Data RDD")
bkk[9].head()
print ('training_active_listing_dummy - Shape: ', training_active_listing_dummy.shape)
rent_db.sort_values(by='price',ascending=False, inplace=True) $ rent_db.head(20)
control_group = df2.query('group == "control"') $ print(control_group.shape[0])
baseball_newind.sort_index().head()
h5.groups()
df =  pd.DataFrame(list(sorted_posts))
santos_winpct.loc[santos_winpct['date'] == '2017-09-07']['playtext'].unique()
log_with_day.groupBy('ipAddress', 'dayOfWeek').count().sort(log_with_day.ipAddress, log_with_day.dayOfWeek).show(30)
log_mod1 = sm.Logit(df3['converted'], df3[['intercept','ab_page' ,'country_UK','country_US']]) $ results1 = log_mod1.fit()
print(train["review"][0]) $ print() $ print(example1.get_text())
us_companies.head()
autos["brand"].unique()
X_colset = set(X_cols) $ y_cols = [col for col in df_train.columns if col not in X_colset] $ y = df_train[y_cols]
no_outliers_forecast_exp2[(no_outliers_forecast_exp2.index >= '2018-06-01') & (no_outliers_forecast_exp2.index <= '2018-12-31')].astype(int)
pres_df.dropna(subset=['location'], inplace=True) $ pres_df.shape
grouped_by_date_df = full_df.groupby('created')['listing_id'].count().reset_index().copy() $ grouped_by_date_df.columns = ['created_date','count_of_listings'] $ grouped_by_date_df.head(5)
autos['price'].hist()
df.head() $
test = df["converted"].mean() $ test
modeling2.show(3)
def sigmoid(z): $     s =  1.0 / (1.0 + np.exp(- z)) $     return s
buckets_to_df(commits.get_cardinality("hash")\ $                      .by_period()\ $                      .fetch_aggregation_results()['aggregations']['0']['buckets']).head()
df_pd_trimmed = df_pd[df_pd["timestamp"] > datetime(2018,6,16,3,14)] $ df_pd_trimmed = df_pd_trimmed[df_pd_trimmed["timestamp"] < datetime(2018,6,21,3,14)] $ train_frame = df_pd_trimmed[0 : int(0.7*len(df_pd_trimmed))] $ test_frame = df_pd_trimmed[int(0.7*len(df_pd_trimmed)) : ]
train, test = data.randomSplit([0.8,0.2], seed=6) $ train.cache() $ test.cache()
cities.iloc[0:5]
for c in df_test.columns: $     print(c, df_test[c].dtype)
df.user_id.nunique() # to get the unique user id counts
ghana.sort('Mean TemperatureC')
grid_size = 15 $ diff = 0.1/grid_size
d.groupby('label')['tweet_id'].count()
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score, p_value)
df = pd.merge(df,df_avg_DEP,how='left',on='ORIGIN') $ df = pd.merge(df,df_avg_ARR,how='left',on='DEST')
df3.head()
help( pd.tseries.offsets.DateOffset)
df_kyt.head()
html = browser.html $ soup = BeautifulSoup(html, 'html.parser')
random_tokens = random_sample['tokens'].tolist() $ random_corpus = createCorpus(random_tokens)
logit_mod_joined2 = sm.Logit(df_joined_dummy.converted, \ $                            df_joined_dummy[['intercept', \ $                                             'ab_page_new_CA', 'ab_page_new_UK', 'ab_page_new_US']]) $ logit_mod_joined2_result = logit_mod_joined2.fit() $ logit_mod_joined2_result.summary()
df_new.loc[df_new['last_active'] <= 7 , 'score_activity_1'] = 5 $ df_new.loc[ (df_new['last_active'] > 8) & (df_new['last_active'] <= 15) , 'score_activity_1'] = 3 $ df_new.loc[ (df_new['last_active'] > 15) & (df_new['last_active'] <= 30) , 'score_activity_1'] = 1 $ df_res['score_activity'] = df_new['score_activity_1'] $
fi.sort_values(by = 'Importance', ascending=True).plot.barh(figsize = (10,6), legend = False, title = 'Feature Importance')
datAll['blk_rng'] = datAll['Block_range'].map(str)+' '+datAll['Street_name'].map(str)
df.describe().transpose()
day_of_year15 = uber_15["day_of_year"].value_counts().sort_index().to_frame() $ day_of_year15.head()
datascience_tweets[datascience_tweets["text"].str.contains("RT")==False]['text'].count() # 895
combined_df.head()
df2 = df2.drop_duplicates(subset=['user_id']) $ df2.info()
ffr.resample("M").first().head()
RatingSampledf=pd.DataFrame()
sentiment_df["Date"] = pd.to_datetime(sentiment_df["Date"]) $ sentiment_df["Date"]
output= "SELECT * from user where user_id='@Pratik'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Following','Followers','TweetCount'])
ax = ign.release_month.value_counts().sort_index().plot(kind = 'bar') $ tick = plt.xticks(range(0,13),['January','February','March','April','May','June', \ $                                'July','August','September','October','November','December']) $ ax.set_ylabel('Games released')
df_goog.index + timedelta(days=1)
json_dict = r1.json() $ type(json_dict)
df_cleaned3_grouped_top = df_cleaned3_grouped \ $ .sort_values('TOTALTRAFFIC',ascending=False) \ $ .head(20)
print('distinct_storeType:', store['StoreType'].unique(), '\ndistinct_assortment: ', store['Assortment'].unique(), '\ndistinct_promoInterval: ', store['PromoInterval'].unique())
crime['Sex'].isnull().sum()
authors_per_file = git_blame.groupby(['path']).author.nunique() $ authors_per_file[authors_per_file == authors_per_file.max()]
fig, axarr = plt.subplots(1, 2, figsize=(12, 6)) $ _ = pd.DataFrame(df_trips["passenger"].value_counts()).plot(kind="hist", bins=11, ax=axarr[0]) $ _ = axarr[0].set_title("Distribution of Passenger Trips Taken") $ _ = pd.DataFrame(df_trips["pilot"].value_counts()).plot(kind="hist", bins=11, ax=axarr[1]) $ _ = axarr[1].set_title("Distribution of Pilot Trips Given")
temp.columns
old_page_converted = np.random.choice([0,1],size=n_old,p=[(1-p_old),p_old])
for table_info in crsr.tables(tableType='TABLE'): $     print(table_info.table_name);print(table_info.table_info.)
pre_svc, rec_svc, thr_svc= metrics.precision_recall_curve(1-y_test, predictions_proba_svc[:,1]) $ pre_dtc, rec_dtc, thr_dtc= metrics.precision_recall_curve(1-y_test, predictions_proba_dtc[:,1]) $ pre_rfc, rec_rfc, thr_rfc= metrics.precision_recall_curve(1-y_test, predictions_proba_rfc[:,1])
df = pd.read_csv(dp('tmdb_5000_movies.csv'), encoding='utf-8') $ df
dfWeek['Date'] = dfWeek['Date'].dt.to_period("W").dt.start_time
df2['Change'].abs()  
free_data.describe()
projects.to_csv('data/toggl-current-projects.csv')
control_df = df2.query('group == "control"') $ control_cr = control_df.query('converted == 1').user_id.nunique()/control_df.user_id.nunique() $ old_page_converted = control_cr $ old_page_converted 
hist(df1.tripduration, bins = 20, color = "Grey", label = "Total trips", normed = 1) $ plt.xlabel("Trip Duration in seconds", fontsize=12) $ plt.ylabel("Amount of trips", fontsize=12) $ plt.title("Trip Duration Histogram by gender", weight='bold', fontsize=14) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
print(cat_group_counts.keys().tolist())
reddit.columns
clean_prices.head()
ax = data.resample('D').sum().rolling(365).sum().plot() $ ax.set_ylim(0,None);
loans_df.issue_d = pd.to_datetime(loans_df.issue_d) $ loans_df.issue_d.value_counts()
final_titles_list = [] $ for title in titles_list : $     title = re.sub('[^a-zA-Z]', ' ', title) $     final_titles_list.append(title)
url_with_author_data = pd.merge(url_data, author_data, left_on='author', right_on="name") $ url_with_author_data.head()
ncptable = data.pivot_table(index=["author"], values=["num_comments"], aggfunc=sum).reset_index() $ ncptable.sort_values(inplace=True, by="num_comments", ascending=False) $ ncptable[0:9]
duplicated_user = df2[df2.duplicated(['user_id'], keep=False)]['user_id'] $ duplicated_user
c_df = new_df.dropna(how='all') $ c_df.size
grouped = df_providers.groupby(['year' ]) $ type(grouped.get_group(2011)) $
import pydata_simpsons $ df = pydata_simpsons.clean_simpsons_script_lines(df)
autos["fuel_type"].unique()
DataSet.tweetText[0]
def apply_cats(df, trn): $     for n,c in df.items(): $         if (n in trn.columns) and (trn[n].dtype.name=='category'): $             df[n] = pd.Categorical(c, categories=trn[n].cat.categories, ordered=True)
data_archie = data_archie[data_archie['user_id'].notnull()] $
unordered_df = USER_PLANS_df.iloc[unordered_timelines]
prop = donors['Donor Is Teacher'].value_counts(normalize=True) $ print(prop)
winpct = pd.read_csv('All_Games_Win_Pct.csv') $ winpct['text'] = winpct['playtext'] $ winpct['date'] = pd.to_datetime(winpct['Game Date'])
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=500000) $ df.head()
sum([1 for row in U_B_df.cameras if len(row) > 2])
p_conv = df['converted'].mean() $ p_conv
df["pickup_hour"] = df["pickup"].dt.hour $ df["pickup_dow"] = df["pickup"].dt.dayofweek $ df["pickup_week"] = (df["pickup"].dt.day - 1) // 7 + 1 $ df["pickup_month"] = df["pickup"].dt.month $ df["pickup_year"] = df["pickup"].dt.year
topic_dist_df = pandas.DataFrame(topic_dist) $ df_w_topics = topic_dist_df.join(df_lit) $ df_w_topics
(df_state_votes.hill_trump_diff > 0).value_counts()
model = sm.Logit(df2['converted'], df2[['ab_page', 'intercept']]) $ result = model.fit()
kk = pd.read_csv('data/kktv/data-001.csv') $ kk.head(30)
def rank_performance(stock_price): $     if stock_price > 11.18766746411483: $         return 'Above' $     else: $         return 'Below'
positives = NB_results.loc[NB_results.sentiment == 'P'] $ sample_size = 10 $ for tweet in positives.tweet.sample(sample_size): $     print(tweet)
autos = pd.read_csv("autos.csv", encoding="Latin-1") $ autos.head()
upvotes = dh[dh["author"].isin(topauthors)].groupby(['author'], as_index=False).agg({'posts':"sum",      $                                      'upvote': "sum"})
stream_listener = StreamListener() $ stream = tweepy.Stream(auth=api.auth, listener=stream_listener, tweet_mode="extended")
print(people.groupby(lambda x: GroupColFunc(people, x, 'a')).mean()) $ print(people.groupby(lambda x: GroupColFunc(people, x, 'a')).std())
overallPoolArea = pd.get_dummies(dfFull.PoolArea)
pd.DataFrame([ [1,2,3],[5,6,7] ])
df2_control.query('converted == 1')['user_id'].count()/df2_control['user_id'].count()
X[['temp_c', 'prec_kgm2', 'rhum_perc']].plot(kind='density', subplots = True, $                                              layout = (1, 3), sharex = False)
datacamp[datacamp["publishdate"]>='2017-01-01']["author"].value_counts(sort=True, ascending=False)[:10].plot(kind='bar')
df2 = pd.read_csv('data_stocks.csv')
print rawdata[pt1.includedchans,:].shape $ print adjmats.shape $ chanlabels = list(pt1.chanlabels) $ print timepoints $ savearray(adjmats, 'test.hdf', chanlabels, timepoints)
displacy.serve(doc, style='ent')
df_everything_about_DRGs.insert(0, 'drg3_str', '') $ df_everything_about_DRGs['drg3_str'] =df_everything_about_DRGs['drg'].apply(lambda x: x[:3])
model.add(Conv1D(30, kernel_size = 30, $                  padding='same', activation='relu')) $ model.add(MaxPooling1D(pool_size=(30))) $ model.add(Dropout(rate=0.25))
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline', $                         xls_dir='scen2xls', file_name='data.xlsx', verbose=False)
salesDF.show(3)
stations = session.query(Measurement).group_by(Measurement.station).count() $ print("There are {} stations.".format(stations))
listings_column_names = pd.DataFrame((listings.columns)) $ listings_column_names_list = list(listings.columns) $ object_first_observation = [listings[x].head(1) for x in listings_column_names_list] $ observations = pd.DataFrame(object_first_observation)
monthly_dataframes = [april_acj_data, may_acj_data, june_acj_data] $ ajc_data = pd.concat(monthly_dataframes) $ ajc_data.sample(5)
df = df[(df['weekday']!='Sat') & (df['weekday']!='Sun')] $ df.head(2)
    !rm SIGHTINGS.csv -f $     !wget https://www.dropbox.com/s/iqf7w9xon14du2e/SIGHTINGS.csv
cities = csvData['city'].value_counts().reset_index() $ cities.columns = ['cities', 'count'] $ cities[cities['count'] < 5]
df2 = df2.drop(duplicate_row_indexes[1])
archive_clean[archive_clean['tweet_id'] == 778027034220126208].rating_numerator 
import pickle $ output = open('speeches_metadata_evidence.pkl', 'wb') $ pickle.dump(speeches_metadata_evidence2, output) $ output.close()
michaelkorsseries.sum()
drop_list = ['FEDFUNDS','DGS1MO','DGS3MO','DGS6MO','DGS1','DGS2','DGS3'] $ for drop_x in drop_list: $     df.drop(drop_x, axis=1, inplace=True)
train = full_data.loc[full_data.order_date <= pd.to_datetime('2016-03-27',format ='%Y-%m-%d' )] $ test = full_data.loc[full_data.order_date > pd.to_datetime('2016-03-27',format ='%Y-%m-%d' )]
pickle.dump(lda_cv_df, open('iteration1_files/epoch3/lda_cv_df.pkl', 'wb'))
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
np.exp(-0.0507), np.exp(-0.0099)
train.num_points.describe()
df['text_split'] = df['text'].str.split()
'my string my'.index('ng')
print(with_countries['country'].unique()) $ with_countries[['CA','UK','US']] = pd.get_dummies(with_countries['country']) $ with_countries.head()
cond_1 = (df['group'] == 'treatment') & (df['landing_page'] == 'new_page'); $ cond_2 = (df['group'] == 'control') & (df['landing_page'] == 'old_page'); $ df2 = pd.DataFrame(df[cond_1|cond_2]);
dhp = dh[dh["author"].isin(topauthors)].pivot_table(index="publishdateone",values="posts",columns="author", aggfunc=np.sum) $ fig, ax = plt.subplots(figsize=(15,7)) $ dhp.plot(ax=ax, kind='bar', stacked=True) $ ticklabels = [item.strftime('%Y %b') for item in dhp.index] $ ax.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels)) $
Lab7 = Lab7.drop(Lab7.index[[0]])
from sklearn.linear_model import LinearRegression $ lin = LinearRegression() $ lin.fit(x_train,y_train)
page_old = df2.converted.mean() $ page_old
url =  'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=Tr4sDaRC5eTmZYfipkh3&start_date=2017-01-01&end_date=2017-12-31' $ r = requests.get(url) $ afxdata = r.json()['dataset']
print("The number of rows in the dataset is:  " + str(df.shape[0])) $
print('Installed Offshore Capacity:',capa2017offshore['Current Installed Capacity [MW]'].sum(),'MW') $ print('Installed Onshore Capacity:',capa2017onshore['Current Installed Capacity [MW]'].sum(),'MW') $ print('Total:', capa2017offshore['Current Installed Capacity [MW]'].sum()+capa2017onshore['Current Installed Capacity [MW]'].sum())
columns = inspector.get_columns('station') $ for c in columns: $     print(c['name'], c["type"])
data.groupby(['longitude', 'latitude']).count().idxmax()
df_predictions_clean.p3_dog.value_counts()
yhat = loanlr.predict(X_test) $ yhat
df_users_6.loc[df_users_6['created']>='2017-07-01','DSA_account_created_before_or_after']='After'
df.groupby(by = ['County', 'Zip Code', 'City']).mean()[[5]].sort_values(by = ['State Bottle Retail'], ascending = False)
df2.groupby('group')['converted'].value_counts()/df.groupby('group')['converted'].count()
file = 'boulder_weather.csv' $ weather = pd.read_csv(file)
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", $                          start='1980', $                          end='2014') $ le_data_all
scaler = MinMaxScaler() $ data[intFeatures] = scaler.fit_transform(data[intFeatures])
lm=sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results=lm.fit() $
temps_df[temps_df.Missouri > 82]
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)]) $ new_page_converted.mean()
n_old = len(df2.query('landing_page=="old_page"'))
df.text[175]
tweet_image_clean.dtypes
bnbAx[bnbAx['language_english']==0].head()
archive_copy['source'].nunique()
data_final.columns
no_conv, yes_conv = df2.converted.value_counts() $ yes_conv/(no_conv + yes_conv)
tx, ty = tsne[:,0], tsne[:,1] $ tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx)) $ ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty)) $ pd.DataFrame(list(zip(tx,ty))).plot.scatter( $     x=0, y=1, alpha = .4);
df_questionable_3[df_questionable_3['state_MI'] == 1]['link.domain_resolved'].value_counts()
P_diffs = np.array(p_diffs) $ P_diffs $
df3 = pd.get_dummies(df2, columns=['landing_page']) \ $         .rename(columns={'landing_page_new_page': 'ab_page'}) \ $         .drop(columns='landing_page_old_page') $ df3['intercept'] = 1 $ df3.head()
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ print('Size of new_page_converted: ', len(new_page_converted))
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df2.drop([2893], inplace=True)
display(data.head(10))
postings.creation_date[postings.creation_date > datetime.datetime(2016, 8, 1)].value_counts().plot();
sentiment = stock['tesla_tweet'].apply(sia.polarity_scores) $ sent = pd.DataFrame(list(sentiment)) $ sent.index = stock.index $ sent.columns = ['telsa_compound', 'tesla_neg', 'tesla_neu', 'tesla_pos'] $ stock = pd.merge(stock, sent, how='left', left_index=True, right_index=True)
pvt.to_csv('./output/ganalytics_transactions_and_associated_products.txt', sep='|', index=False, quoting=csv.QUOTE_NONE)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p0, (1-p0)])
name = '@realDonaldTrump' $ nbr_tweets = 400 $ results = myapi.user_timeline(id=name, count=nbr_tweets)
roberts = nobel[nobel['Full Name'].map(lambda x: x.startswith('Robert'))] $ print("There were {} winners named Robert".format(len(roberts)))
p_new_sim = new_page_converted.mean() $ p_old_sim = old_page_converted.mean() $ p_diff = p_new_sim - p_old_sim $ p_diff
items.loc[[228], ['title', 'dateCreated']]
df_usnpl['state_level_media'] = df_usnpl['state_level_media'].replace(remap_vals)
driver = selenium.webdriver.Safari() # This command opens a window in Safari $ driver.get("https://xkcd.com/")
bc['newdate'] = pd.DatetimeIndex(bc.date).normalize()
df[df.reviewerName.isnull()]
bnb.first_affiliate_tracked.value_counts()
users.head(2) 
df_new_npage = df_new.query('ab_page == 1') $ df_new_npage.tail(10)
house_data.groupby(['bedrooms'])['bedrooms'].count()
props.prop_name.value_counts().reset_index()
dfNYC["Year"] = dfNYC["Created Date"].apply(YearColumn)
a =R16df.rename({'Create_Date': 'Count-2016'}, axis = 'columns') $
total_students_with_passing_reading_score = len(df_students.loc[df_students['reading_score'] > 69]) $ total_students_with_passing_reading_score
ax = df[['A']].plot(figsize = (12,2)) $ for index, row in df.iterrows(): $      if row['A'] > 1: $         ax.annotate(row['A'],xy=(index,row['A'])) $ plt.show()
nltk.download('wordnet')
no_newpage_treatment.isnull().sum().sum()
ser7 = pd.Series([9,8,7,6,5,34,2,98]) $ ser7.head(7) $ ser7.tail(2)
m_true = 1.4 $ c_true = -3.1
stemmer = SnowballStemmer('english') $ s_words = stopwords.words('english')
df_final[features].describe()
ex4.drop(ex4.index[-1])
data_year_df.info()
pos_data_test['rating'].count()
from sklearn.neighbors import KNeighborsClassifier
print(linreg.coef_) $ print("Function: " + " + ".join(["%.8f" % linreg.coef_[i] + "x^" + str(i) for i in range(len(linreg.coef_))])) $ print(linreg.score(quadratic, y)) # Figure out the R^2 "score" of our prediction (out of 1.0)
gMapAddrDat.set_timeDelay(0)
scr_end_date = pd.Series(pd.to_datetime(scr_end_date).strftime('%Y-%m'),index=scr_churned_ix)
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs); $ plt.axvline(x=obs_diff, color='r');
df = pd.read_sql('select * from hundred_stocks_twoyears_daily_bar', conn_helloDB) $ df.tail()
df.axes
hours.dropna(subset=['Specialty'], how='all', inplace=True)
len(data.url.unique())
logit_mod2 = sm.Logit(df_ab_cntry['converted'], df_ab_page[['intercept','US_ind_ab_page','CA_ind_ab_page']]) $ results2 = logit_mod2.fit()
corpusDF = corpusDF.groupby("URI").first() $ len(corpusDF)
promo_df.sort_values(['item_nbr','store_nbr', 'date'], inplace=True)
df.loc[  100:103  ]  # by row label range
df_combined['point_diff'] = df_combined['home_weighted_points'] - df_combined['away_weighted_points'] $ df_combined.head()
students[(students.gender == 'F') | (students.weight >= 140)]
ts = pd.Timestamp(datetime.datetime(2016,5, 12, 3, 42, 56)) $ ts
result[['id_ndaj1', '11_x', '12_x', '14_x', '15_x', '10102_x', '10120_x']].hist(figsize=(15, 10), bins=50, alpha=0.7)
path = "http://www.principlesofeconometrics.com/stata/consumption.dta" $ df = pd.read_stata(path) $ df.head(5)
Y_df1 = Y_df.copy() $ Y_df1['interest'] = [0 if x=='low' else(1 if x=='medium' else(2)) for x in Y_df.interest_level] $ Y_df1.head()
def parsear_zona(row): $     mylist = row.loc['place_with_parent_names'].split("|") $     return mylist[3] $ prop_caba_gba.loc[:,'neighborhood'] = prop_caba_gba.apply(parsear_zona, axis=1)   
pivoted.plot(legend=False, alpha=0.1)
columns = [m.key for m in Measurement.__table__.columns] $ print(columns)
very_pop_df = au.filter_for_support(popular_trg_df, max_times=5, min_times=3) $ au.plot_user_dominance(very_pop_df)
tf = logs.fm_ip == logs.to_ip
mc_results_w = ssm.MultiComparison(df['y'], df['w']) $ mc_results_w_tukey_hsd = mc_results_w.tukeyhsd() $ print(mc_results_w_tukey_hsd)
print(len(df[df.Address == 'Not associated with a specific address'])) $ df.Address.replace('Not associated with a specific address', np.nan, inplace=True)
del adj_close_acq_date['Quantity'] $ del adj_close_acq_date['Unit Cost'] $ del adj_close_acq_date['Cost Basis'] $ del adj_close_acq_date['Start of Year'] $ adj_close_acq_date.sort_values(by=['Ticker', 'Acquisition Date', 'Date'], ascending=[True, True, True], inplace=True)
plt.scatter(sing_fam.larea.values, sing_fam.rp1lndval.values);
token_send_add_receiveAvg_month.columns = ["ID","sendReceiveCntAvg_mon"]
%%sql mysql://user1:logger@172.20.101.81/pidata $ select * from temps3;
filepath = os.path.join('input', 'input_plant-list_SI.csv') $ data_SI = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None) $ data_SI.head()
taxiData.head(5)
"Rural" in df_protest.columns
colArr=df['WHO Region'].get_values() $ colArr
df3['timestamp'] = pd.to_datetime(df3['timestamp']) $ df3.info()
fraq_volume_m_sel = b_mat.as_matrix() * fraq_volume_m_coins $ fraq_fund_volume_m = fraq_volume_m_sel.sum(axis=1) $
g8_aggregates.columns
pt_all=pd.DataFrame.pivot_table(df_users_6,index=['cohort'],values=['uid'],aggfunc='count',fill_value=0)
match_results = pd.read_csv("data/afl_match_results.csv") $ odds = pd.read_csv("data/afl_odds.csv") $ player_stats = pd.read_csv("data/afl_player_stats.csv")
Type_date = Exchange_df[['New_or_Returning', 'Sales_in_CAD']] $ Type_date.head()
df['2015-06'].resample('D').sum().plot()
fig = plt.figure(figsize = (5,5)) $ sns.distplot(reddit_master['num_comments'], kde = False) $ plt.xlabel("Number of comments") $ plt.ylabel('Frequency') $ plt.title('Number of Comments Distribution (master scrape)');
weather.head()
y,X = dmatrices('Lottery ~ Literacy + Wealth + Region', data=df, return_type='dataframe') $ mod = sm.OLS(y, X)    # Describe model $ res = mod.fit()       # Fit model $ res.summary()         # Summarize model
my_gempro.blast_seqs_to_pdb(all_genes=True, seq_ident_cutoff=.9, evalue=0.00001) $ my_gempro.df_pdb_blast.head(2)
np.exp(results_mod.params)
S = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure08/summa_fileManager_riparianAspenPerturbRoots.txt')
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.contains('^[(a-z)]'))].value_counts()
complete_df = df2.merge(country_df, on = 'user_id', how = 'inner') $ complete_df.head()
df_weather.dtypes
recipes.description.str.contains('[Bb]reakfast').sum()
percipitation_measurement_df.plot() $ plt.show()
contractor[contractor.contractor_number.duplicated() == True]
theft.dtypes
df.dropna(inplace=True) $ df['County Number'] = df['County Number'].astype(int) $ df['Category'] = df['Category'].astype(int)
df.info()
reduced_trips_data['duration'].hist() # still this does not help in obserbving the distribution of duration
random_series = pd.Series(np.random.randn(1000), $                           index=pd.date_range('1/1/2000', periods=1000)) $ random_series.plot();
people.iloc[1:3]
n_new = df2.query('landing_page == "new_page"').user_id.count() $ print(n_new)
import calendar $ df['Month'] = df['Date'].apply(lambda x: calendar.month_abbr[x.month]) $ df['Year']  = df['Date'].apply(lambda x: x.year)
with open('gdp_model.pkl', 'wb') as fid: $     pickle.dump(fit, fid)    
df.hist(column='income', bins=50)
url = 'https://api.twitter.com/1.1/account/verify_credentials.json'
tweets.set_index('Serial',inplace=True)
import numpy as np $ np.sqrt(fruits)
autos["registration_month"].value_counts().sort_index(ascending=True)
df2.groupby(df2['landing_page']=='new_page').size().reset_index()[0].iloc[1]/df2['landing_page'].count()
old_page_converted = np.random.choice(2, size=n_old ,p=[p_old,1 - p_old]) $ old_page_converted.mean()
data.head()
order_item = pd.read_csv('../data_clean/order_item_merge.csv') $ order_item.head()
n_old = df2.query('landing_page == "old_page"').landing_page.count() $ n_old
ab_data.group.value_counts()
import statsmodels.api as sm $ convert_old = df2.query('group == "control" & converted == 1').user_id.nunique() $ convert_new = df2.query('group == "treatment" & converted == 1').user_id.nunique() $ n_old = df2.query('group == "control"').user_id.count() $ n_new = df2.query('group == "treatment"').user_id.count()
scores.shape
print(autos['seller'].unique()) $ print(autos['vehicle_type'].unique()) $ print(autos['gearbox'].unique()) $ print(autos['fuel_type'].unique()) $ print(autos['unrepaired_damage'].unique())
(~autos["year_of_registration"].between(1910,2016)).sum() / autos.shape[0]
error_set.groupby('converted').nunique()['user_id']/error_set.nunique()['user_id']
df3.summary()
f0.include_list_filter
data = pd.get_dummies(df5['andrew_id_hash']) $ data = data.drop([col for col, val in data.sum().iteritems() if val <= 10], axis=1) $ data = data.join(pd.get_dummies(df5['new_id'])) $ data = data.join(df5['HT']) $ print(data.info())
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ old_page_converted.mean()
sns.violinplot(autos["registration_year"])
parsed_liberia_df.head()
players = pd.read_sql_query('select * from Player', conn)  # don't forget to specify the connection $ print(players.shape) $ players.head()
gmap.heatmap(data['Latitude'],data['Longitude'])
dates =  pd.date_range(dt.datetime(2014,01,01),dt.datetime(2017,01,01))
proj_df.apply(lambda r: 1 if r.math_science and r.lit_and_lang else 0, axis=1).sum()
archive_clean[archive_clean['tweet_id'] == 883482846933004288].rating_numerator
m3 = m3.round(decimals=2) $ print("m3: ", m3)
average_polarity.reset_index(inplace=True) $ count_polarity.reset_index(inplace=True)
df2.head()
f.include_list_filter
BDAY_PAIR_df.head()
litecoin_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
print("Get a slice of values in column 3, from rows 1-3:",) $ print(sheet.col_values(3, start_rowx=1, end_rowx=4))
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17,ridge3.coef_) + ridge3.intercept_, c='b') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
tips.sort_values(["tip","size"],ascending=False).head(10)
lgComp_df = wcPerf1_df.groupby(['lgID','name']).sum() $ lgComp_df
dt.replace(minute=0, second=0)
colorless = all_cards.loc[all_cards.colors.isnull()] $ all_cards.loc[colorless.index, "colors"] = colorless.colors.apply(lambda x: [])
array_train = pokemon_train.values $ array_test = pokemon_test.values $ Y_train = array_train[:,1] $ Y_test = array_test[:,1]
git_log.timestamp = pd.to_datetime(git_log['timestamp'], unit='s') $ git_log.timestamp.describe()
dtc = DecisionTreeClassifier(max_leaf_nodes=1002) $ scores = cross_val_score(dtc, X, np.ravel(y,order='C'), cv=10) $ print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
from sklearn.feature_extraction.text import TfidfVectorizer
df = pd.read_csv("training.1600000.processed.noemoticon.csv", $                  header=None, names=cols, encoding='latin-1')
import gmplot $ gmap = gmplot.GoogleMapPlotter.from_geocode("New York",10)
top10 = git_log.author.value_counts().head(10) $ top10
df_raw.sort_values(by=['year','list_crawl_num', 'row_crawl_num']) # look at lists in original order
p.asfreq('M', how='start')
tweet_data.retweeted.value_counts()
emails_dataframe['institution'].value_counts()
list(xp.columns.values)
df.head()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
rfr = RandomForestRegressor(random_state=0) $ cv_score = cross_val_score(rfr, features_regress_vect, overdue_duration, $                            scoring='neg_median_absolute_error', cv=5) $ 'MedianAE score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
(logs.lat - logs.long).apply(round).value_counts()
x_train, x_test, y_train, y_test, msk = demo.prep_for_model(snowshoe_df)
features_df.head(10)
at.ADF(pair[stock2 + '_return'])
temp = list(zip(*df['Ranking Full URL on Sep  1, 2017'].map(get_url_parts))) $ for i, c in enumerate(columns): $     df[c] = temp[i]
byMonth = df.groupby('Mo')
(df_ind_site.groupby(['id_num', 'year'])['drg3'].count())[100007]
DataSet = DataSet[DataSet.userTimezone.notnull()] $ len(DataSet)
df_uk[df_uk['ab_page'] == 1]['converted'].mean()
vocabulary_expression['component_1'].sort_values(ascending=False).head(7) $
df_q = pd.read_sql(query, conn, index_col=None) $ plt.figure() $ df_q.plot(x = 'UNIQUE_DATE') $ df_q.head(5)
cityID = '2a93711775303f90' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Milwaukee.append(tweet) 
potholes.groupby(by= potholes.index.hour).count().plot(y='Unique Key')
topsales=df15[[store in top500 for store in df15.store_number]].groupby('store_number').agg(lambda x:x.value_counts().index[0]) $ topsales
support.groupby(["contributor_lastname","contributor_firstname"]).amount.sum().reset_index().sort_values("amount",ascending=False).head(10)
image_predictions.head()
df_birth.describe()
p(locale.getpreferredencoding)
countries_df.country.unique()
bikedataframe['snowtotal'] = np.where(bikedataframe['DAILYSnowfall'] > bikedataframe['DAILYSnowDepth'], $                                       bikedataframe['DAILYSnowfall'], bikedataframe['DAILYSnowDepth']) $
temp = pd.DatetimeIndex(df_clean2['timestamp'])
test_df.head().T
df.groupby("cancelled")["pickup_hour"].mean()
cur.execute('SELECT material_type, COUNT(*), AVG(alpha), MAX(beta) FROM materials GROUP BY material_type') $ cur.fetchmany(2)  # use fetchmany() with size parameter, just for fun
vectorizer = TfidfVectorizer(stop_words='english',ngram_range =(1,4)) $ X = vectorizer.fit_transform(df.loc[:,'text'])
m3.unfreeze() $ m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)
machines = pd.read_csv('machines.csv', encoding='utf-8') $ machines.head()
grouped = positive_results.groupby('user_id') $ successful_campaigns_per_user = dict(grouped.campaign_id.count()) $ successful_users = OrderedDict(Counter(successful_campaigns_per_user).most_common(20)) $ successful_users
stmt = stmt.columns(User.name, User.id, User.fullname, User.password)
print("Probability of individual converting:", df2.converted.mean())
datasets_slug_id.get('contrats-de-milieu-guadeloupe-onm', 'key not found :(')
df_us[df_us['ab_page'] == 0]['converted'].mean()
autos['odometer_km'].unique().shape
food["created_time"]=food["created_datetime"].apply(time)
past_year = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ past_year
reddit.head()
clf = decomposition.NMF(n_components=6, random_state=1)
trump_con.insert(0, "conspiracy", trump_con["lText"].isin(TC)) $ print (trump_con)
birth_dates.head()
df_concat = pd.concat([bild, spon]) $
df_test['due_date']=pd.to_datetime(df_test['due_date']) $ df_test['effective_date']=pd.to_datetime(df_test['effective_date']) $ df_test.head()
gs.best_params_
print('Most LaziestCanine: {}'.format(tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values('laziestcanine_pp', ascending=False).text.values[0])) $ print('Least LaziestCanine: {}'.format(tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values('laziestcanine_pp', ascending=True).text.values[0]))
rating_and_retweet['score'].corr(rating_and_retweet['retweet_count'])
df.groupby(['Agency Name'])['Complaint Type'].value_counts()
left = pd.DataFrame({'key' : ['foo', 'foo'], 'lval': [1,2]}) $ left
SelectedOpenClose = AAPL.iloc[200:210, [1,3]] $ SelectedOpenClose
df['release_dt'].describe()
Measurements = Base.classes.measurements $ Stations = Base.classes.stations
full_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ full_image = "https://www.jpl.nasa.gov/spaceimages/details.php?id=PIA16726" $ more_info_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars"
with open('health_tweets.pkl', 'rb') as f: $     health_tweets = pickle.load(f)
from sklearn.linear_model import LogisticRegression $ log_reg = LogisticRegression(random_state=42) $ log_reg.fit(X_train, y_train)
train_df.head()
dfd = dfd[~dupes_to_delete].copy() $ len(dfd)
train.MODELE_CODE.value_counts()
temp_obs = session.query(Measurement.date, Measurement.tobs).\ $ filter(Measurement.station == mostactive_station).\ $ filter(Measurement.date >= '2016-08-18').\ $ order_by(Measurement.date).all()
nouns.most_common(10)  # most frequent nouns
logit_countries_model2 = sm.Logit(df4['converted'], df4[['abs_page','country_UK', 'country_US', 'intercept']]) $ results_countries_2 = logit_countries_model2.fit()
dictionary = {} $ for i in range(len(list_atlas_prob_stat)): $     dictionary[df_map[i,0]]=[mvoid_to_bson_id(a)   for a in df_map.A[i,1:] if mvoid_to_bson_id(a) != mvoid_to_bson_id('000000000000000000000000')]
session.query(Stations.station, Stations.name).all()
pass_students = grades[(grades.Mark >= 50)] $ fail_students = grades[(grades.Mark < 50)] $ print("PASS:", pass_students.ATAR.mean()) $ print("FAIL:", fail_students.ATAR.mean())
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1-mean_of_p), mean_of_p])
filtered_df = classification_df $ plt.figure() $ plt.title('Team Count') $ plt.scatter(filtered_df['teamCount'], filtered_df['best'], marker= 'o', s=20) $ plt.show()
sep2014 = aug2014 + 1 $ sep2014,sep2014.start_time, sep2014.end_time
df.head()  # let's see some samples
from sklearn.model_selection import train_test_split $ tips_train, tips_test = train_test_split(tips, test_size=0.2, random_state=123) $ tips_train.shape, tips_test.shape, tips.shape
df = pd.merge(df_twitter_archive_copy, df_img_predictions_copy, on='tweet_id') $ df = df.merge(df_tweet_data_copy, on='tweet_id')
df_uro = df_uro.drop(columns = ls_metac_colnames)
creations.groupby("creator_autoconfirmed").size()
df_sb['blobw'] = df_sb['cleaned_text'].apply(TextBlob) $ df_sb['sentiment'] = df_sb['cleaned_text'].apply(sentiment_calc) $ df_sb['Polarity'] = df_sb['cleaned_text'].apply(polarity_calc) $ df_sb['Subjectivity'] = df_sb['cleaned_text'].apply(subjectivity_calc)
auso18_size = db.command("collstats", "ausopen18")["count"] $ auso18_qual_size = db.command("collstats", "ausopen18_qual")["count"]
y_preds = gbdt_grid.best_estimator_.predict(X_test) $ gbdt_scores = show_model_metrics('Gradient Boosting Decision Trees', gbdt_grid, y_test, y_preds)
news_df = guardian_data.copy()
jail_census.groupby('Gender')['Age at Booking'].mean()
old_page_converted = np.random.binomial(1, p_old, n_old) $ len(old_page_converted)
df3 = df2.assign(intercept=np.ones(len(df2), dtype=int)) $ df3['ab_page'] = pd.get_dummies(df['group'])['treatment'] $ df3.head()
fig, axarr = plt.subplots(1, 2, figsize=(12, 6)) $ _ = df_passengers["rating"].plot(kind="hist", bins=11, ax=axarr[0]) $ _ = axarr[0].set_title("Distribution of Passenger Ratings") $ _ = df_pilots["rating"].plot(kind="hist", bins=11, ax=axarr[1]) $ _ = axarr[1].set_title("Distribution of Pilot Ratings")
tweets_df.head(10)
autos['last_seen'] = autos['last_seen'].str[:10] $ autos['last_seen'].value_counts(normalize=True, dropna=False).sort_index()
list(data.dropna(thresh=int(data.shape[0] * .9), axis=1).columns)
ndf = ndf.assign(Norm_summary = normalize_corpus(ndf.summary))
outlier_detection.strong_cutoff
for i in cpi_all.Region.cat.categories.tolist(): $     print i
pickle.dump(merged, open('data/merged.dat', 'wb'))
team_groups.ngroups
df[df['SA'] < 0].subreddit.value_counts()[:3]
autos["price"].value_counts().sort_index(ascending=False).head(20)
for col in missing_info: $     num_missing = df_[df_[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing)) $
with open('2015--Pascuet-M-I--Al--LAMMPS--ipr1.json', 'w') as f: $
my_list = [1, 2, 3] $ my_list.insert(1, "insert") $ print(my_list)
train_set.tweetText[0]
def dropyears(df_): $     return(df_.truncate(before = "1980-7"))  
example_document = index.document(index.document_base()) $ print(example_document)
logit = sm.Logit(df2.converted, df2[['intercept', 'new_page', 'CA_new', 'UK_new', 'CA', 'UK']]) $ result = logit.fit() $ result.summary()
closest = np.array(pwd).argsort()[:17]
test_df.head(15)
treatment_group = len(df2.query('group=="treatment" and converted==1'))/len(df2.query('group=="treatment"')) $ treatment_group
lmscore.resid.head(4)
df2.set_index('user_id', inplace = True) $ df_c.set_index('user_id', inplace = True) $ df2 = df2.join(df_c)
hp.find_site(1)
grid = GridSearchCV(logreg, param_grid, cv=5, scoring='roc_auc') $ grid.fit(X, y) $ grid.grid_scores_
new = auto_new.drop(["CarModel", "CarYear", "Country", "Custom", "Payment_Option", "Hand_Drive", "Body_Type", "Purchased"],axis=1)
index =['Region', 'date', 'periodstart', 'Period', 'periodtype'] $ df_new = df[RunIDs['NewScenario']].sort_values(by=['Region', 'periodstart'], ascending=[True,True]).set_index(index) $ df_old = df[RunIDs['OldScenario']].sort_values(by=['Region', 'periodstart'], ascending=[True,True]).set_index(index)
store_items = store_items.drop(['store 1', 'store 2'], axis=0) $ store_items
df.info()
tb = pd.crosstab(index=mydf.comp, columns=[mydf.dept, mydf.grp]) $ tb
grid_search.fit(train, train['Visits'])
path="../output/positve_model.pickle"
target_google['date'] = target_google['date'].astype('str')
pd.tseries.offsets.BDay()
df.to_csv('intent.csv',sep=',')
HTML(string=html_out).write_pdf('../reports/report_css.pdf', stylesheets=["../templates/assets/css/style.css"])
elnet2 = ElasticNet(alpha=0.0001, l1_ratio=0.25) $ elnet2.fit(train_data, train_labels)
tfav.plot(figsize=(16,4), label="Likes", legend=True,color='r')
df['min'] = df.index - datetime.timedelta(seconds=1) $ df['max'] = df.index + datetime.timedelta(seconds=1) $ df.head()
new_page_converted.mean() - old_page_converted.mean()
df2.head()
documents = tweets.text_fmt.values
from nltk.stem.porter import PorterStemmer $ porter = PorterStemmer() $ def tokenizer_porter(text): $     return [porter.stem(word) for word in text.split()] $ tokenizer_porter('runners like running and thus they run')
weather.dtypes
td = s - pd.Series(pd.date_range('2011-12-29', '2011-12-31'))
geo_segments_all_simple.head(100)
def get_avg_value( d, val_by_key ): $     get_valuebykey = lambda key1: get_float_value( d = d, key1 = key1, key2 = val_by_key )    $     return sum( map( get_valuebykey, d.keys() ) ) / len(d)
dfClientes.shape[1]
users.email.value_counts()
girls_by_name = girls.set_index('Name') $ girls_by_name.head()
new_page_converted = np.random.choice([1,0], size = nNew, p=[pMean,oneMinusP]) $ new_page_converted.mean()
grinter1.head()
run txt2pdf.py -o"2018-06-19 2013 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2013 FLORIDA HOSPITAL Sorted by payments.txt"
y = np.asarray(churn_df['churn']) $ y [0:5]
top10_topics_2 = top10_topics_1.sort_values(by='count_event_per_topic', ascending=False) $ top10_topics_2.head(10)
df_protest.index[0]
comments = pd.read_csv('data/comments.csv', index_col = 0, parse_dates = [2]) $ comments.head()
tweet_archive.name.value_counts().head(5) $
Test.ReadFlows() $
most_retweeted_tweeps_sorted=most_retweeted_tweeps_df.sort_values(by='number_of_retweets') $ most_retweeted_tweeps_sorted.tail(5)
file = 'https://assets.datacamp.com/production/course_2023/datasets/tips.csv' $ tips = pd.read_csv(file) $ tips.info()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
datatest.to_csv("csvs/datatestSinNan.csv", index = False)
samples_query.print_records(40)
nameList = ["DR.", "WERB"]  # DR. = Dr Axselrod   Dentists $ nameAmtDict = {} $ for name in nameList: $     updateDict(name) $ nameAmtDict
temp_cat = pd.Categorical(temp, categories=['low','medium','high']) $ temp_cat
logit_mod = sm.Logit(df3['converted'], df3[['ab_page', 'UK', 'US', 'intercept']]) $ df3_results = logit_mod.fit()
json_data = json_response.json() $ print(json_data['dataset']['column_names']) $ print(json_data['dataset']['data'][0]) $ print(json_data['dataset']['data'][-1]) $
top_supports.head(5)
taxi_hourly_df.index.min()
exportparams = urllib.parse.urlencode({ $     'action': 'tweet-export', $     'format': 'csv'})
newdf.yearmonth = newdf.yearmonth.str.replace('(','').str.replace(')','').str.replace(',','').str.replace(' ','')
autos.odometer_km.value_counts().sort_index(ascending=True).head(13)
df_users_mvp.shape
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn; seaborn.set()  # set plot styles
%matplotlib inline $ fig, ax =viewer.draw(nrn) 
df_int.plot() $ plt.show()
new_page_converted = np.random.binomial(1, p_equal_old_new, length_of_new) $ new_page_converted.mean()
results_countries_2.summary()
X_train_predict_i = scaler.inverse_transform(X_train_predict) $ y_train_i = scaler.inverse_transform(y_train) $ X_test_predict_i = scaler.inverse_transform(X_test_predict) $ y_test_i = scaler.inverse_transform(y_test)
forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 1 $ forecast_df['male_pct'] = forecast_df['male_pop'] / forecast_df['pop'] * 100 $ forecast_df.drop('male_pop', axis=1, inplace=True) $ forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 0 $ forecast_df.head()
df1['day_of_week'].value_counts() $
clf_y_score = rfc.predict_proba(X_test)[:, 1] #[:,1] is formatting the output $ clf_y_score
t1 = df_clean2['timestamp'].sample() $ t1 = pd.DatetimeIndex(t1)
if save_n_load_df(joined, 'joined_roll_tran.pkl'): del(tr_roll)
check_point_file_path='../../data/resource/checkpoints.geojson' $ with open(check_point_file_path) as cp_file: $     check_point_data = cp_file.read() $ check_points = json.loads(check_point_data)
pd.read_sql('desc actor', engine)
for letter in list('GH'): $     sparkline(ab_groups[letter].rpv, letter)
z_stat, p_val = stats.ranksums(virginica, versicolor) $ print('MWW RankSum p-value %0.15f' % p_val)
df_group = df.groupby(['breed'])['retweet_count', 'favorite_count'].mean() $ df_group = df_group.sort_values(['retweet_count', 'favorite_count'], ascending=False) $ df_group.iloc[0:14,].plot(kind='bar');
df.head(10)
df['artist'].value_counts()
features_df.head(10)
len([earlyScn for earlyScn in SCN_BDAY_qthis.scn_age if earlyScn < 3])
new_article_project = article_project.drop_duplicates(subset=['article_id', 'project_id'], keep='first') $ new_article_project.shape
labels[np.argmax(sample_y1[0])]
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
df_users = df_user_count[df_user_count > 5] $ df_users.count()
to_be_predicted_Day4 = 22.34270784 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
mydata = quandl.get("FRED/GDP") $ mydata.head()
senate = pd.merge(legSenate, openSenate.reset_index(), how='left', on='TLO_id')
ts.index
df2.drop(labels = 1899, axis=0, inplace=True)
(null_value > obs_diffs).mean()
df_columns['Time value'] = pd.to_datetime(df_columns['Created Date'], format="%m/%d/%Y %H:%M:%S %p") $ df_columns['Day of the week'] = df_columns['Time value'].dt.weekday_name $ df_columns.head() $
for tweet in public_tweets: $     print(tweet.text) $     analysis = tb(tweet.text) $     print(analysis.sentiment.polarity) $
reddit['Date Only'] = reddit['Time'].apply(lambda x:x.date().strftime('%m-%d-%y'))
test_portfolio = ew_portfolio_ts[ew_portfolio_ts['ticker'].isin(['AMD', 'UA'])].copy()
holiday_model = Prophet(holidays=holidays_df,yearly_seasonality =True,weekly_seasonality= True,daily_seasonality = True) $ holiday_model.fit(df3_holidays); $ holiday_future = holiday_model.make_future_dataframe(periods= 6, freq = "m") $ holiday_forecast = holiday_model.predict(holiday_future) $
autos['ad_created'].str[:10].value_counts(normalize=True, dropna=False).head(10)
knn = KNeighborsClassifier(n_neighbors = 3) $ knn.fit(X_train, Y_train) $ Y_pred = knn.predict(X_test) $ acc_knn = round(knn.score(X_train, Y_train) * 100, 2) $ acc_knn
data.info()
df2.head()
n_old = df2[df2['landing_page']=='old_page'].user_id.count() $ n_old
hold = hold_dict.keys() $ my = my_dict.keys() $ for key in my_dict.keys(): $     if key not in hold and key != my_dict[key][0]: $         print "my_dict['{}'] = {}".format(key, my_dict[key])
body = body.replace('\n', ' ') $ body
warnings.simplefilter('ignore')
baseball_newind.index.is_unique
number_pos = data_df[data_df['is_high_val'] == 1].shape[0] $ number_all = data_df.shape[0] $ print(f'Target labels of class \'1\': {number_pos} or {(number_pos/number_all)*100:.2f}% over all.')
import statsmodels.formula.api as smf $ model = smf.ols('VIX ~ SP500', data=df) $ result = model.fit(cov_type="HAC", cov_kwds={'maxlags':5}) $ print(result.summary2())
n_old = df2.query('group=="control"').shape[0] $ n_old
hr["chart delta"] = \   #line- continuation $ hr.apply(lambda x: (x["charttime"] - $                         x["realtime"]).total_seconds(), axis=1) $ hr.head()
station_count = session.query(Stations.id).count() $ print ("Total Number of Stations are: "+ str(station_count))
try: $     r=table.find(text='Registration').find_next('td').text $ except: $     reg='No data' $ reg
sub1=test[['id','visitors']].copy()
st_hp_in=pd.merge(internet,stocks_happiness_rev1,on='dates',how='inner')
df_twitter_archive.rating_denominator.value_counts()
soup_p = soup.find_all('p') $ soup_p[2].get_text()
kick_projects.head()
utility_patents_subset_df['prosecution_period'] = utility_patents_subset_df.grant_date - utility_patents_subset_df.filing_date $ utility_patents_subset_df.prosecution_period = utility_patents_subset_df.prosecution_period.apply(lambda x: x.days) $ utility_patents_subset_df.prosecution_period.describe()
dfMovies.to_csv('tmdb_5000_movies_out_final.csv', sep=',', header=True)
ex2 = pd.Series(["b","a","c"])
from sklearn.neural_network import MLPRegressor $ n_net = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(1000, 500, 250, 100, 50), $                       random_state=1, verbose = True) $ n_net.fit(x_train,y_train)
sns.distplot(utility_patents_subset_df['number-of-figures'], color="red") $ plt.show()
tweets['hour'] = tweets['created_at'].dt.hour $ tweets['day_of_week'] = tweets['created_at'].dt.dayofweek
bikedataframe = bikedataframe.loc[bikedataframe['count'] > 0] $ bikedataframe.shape
prop.head()
mentioned_bills_all['votes_api_url_2'] = mentioned_bills_all['votes_api_url'].fillna('')
logit_mod3 = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA', 'ab_page', 'page_US', 'page_CA']]) $ results = logit_mod3.fit() $ results.summary2()
tobs_stn_281 = session.query(Measurement.date, Measurement.station, Measurement.tobs).\ $                 filter(Measurement.station == 'USC00519281').\ $                 filter(Measurement.date > '2016-08-23').\ $                 filter(Measurement.date <= '2017-08-23').all() $
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
zero_weight = cats_df[cats_df['weight'] == 0]['weight'] $ cats_df['remove'].iloc[zero_weight.index] = True $ del zero_weight
xgb = XGBRegressor(gamma=0, learning_rate=0.02, max_depth=3, n_estimators=100) $ xgb.fit(X_train, y_train) $ accuracy = metrics.r2_score(y_train, xgb.predict(X_train)) $ print("R2 Accuracy:", accuracy)
ec550.to_netcdf(lustre%"adm_domain.nc") $ aod550.to_netcdf(lustre%"emep_colloc.nc")
print('EXP(X) = \n', np.exp(fruits))
treatment = df2.query('group=="treatment"') $ treatment_converted = treatment.query('converted==1') $ prob_treatment_converted = treatment_converted.shape[0]/treatment.shape[0] $ prob_treatment_converted
log_clf2 = LogisticRegression(multi_class="multinomial", solver="lbfgs", random_state=42) $ t0 = time.time() $ log_clf2.fit(X_train_reduced, y_train) $ t1 = time.time()
df_users.last_session_creation_time.fillna(method='ffill',inplace=True)
sc.textFile("file:/path/file.txt") \ $     .filter(lambda line: ".txt" in line) \ $     .count()
topics = topics.drop(['out_date', 'due_date', 'semester'], axis=1) $ print(topics.info())
array = np.loadtxt('RaceRocksDailySalTemp.txt', skiprows=3 )
df2['intercept'] = 1 $ df2_dummies = pd.get_dummies(df2['landing_page']) $ df2 = df2.join(df2_dummies) $ df2['ab_page'] = pd.get_dummies(df['group'])['treatment'] $
n = tweets_df.favorite_count.hist() $ n.set(yscale="log")
test_number_con['booking_rate'] = ( $     test_number_con.booked_at/test_number_con.total_number_conversations) * 100
ab_page_index = df2[df2['group']=='treatment'].index
typesub2017.dtypes
df2.all(axis=1)
annotations_df.filters.apply(lambda x: len(x) == 0).all()
n_new=df2[df2['landing_page']=='new_page']['user_id'].count() $ n_new
areas.loc[areas['OPSD area'].notnull(),:'EIC'].fillna('')
result.summary()
def adder(ele1,ele2): $     return ele1+ele2 $ df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) $ df
data['section'].value_counts()
print sqlContext.sql("select * from ufo_sightings limit 10").collect()
df_pagecounts_mobile.head()
data=datasets.load_iris()
datastore[0].shape, datastore[1].shape
weather_mean.loc['HALIFAX':'OTTAWA', 'Rel Hum (%)':'Wind Spd (km/h)']
plt.plot(data.moving_avg)
sql = "SELECT * FROM paudm.production as prod where prod.pipeline='memba' order by - prod.id  limit 5" $ df2 = pd.read_sql(sql,engine)
df = df.drop_duplicates(subset="tweet") $ df
from scipy import stats $ chi2,p,dof,expected = stats.chi2_contingency(obs) $ p
tmp.full_text
df['status'].unique()
'Remember when we learned about split?'.split('we')
random_integers = rng.randint(1, 10, size = 16).reshape(4, 4)
print "{:,} total users".format(filteredPingsDF.groupBy("cid").count().count()*100) # ~830M users
p_old = df2['converted'].mean() $ print("{} is the convert rate for Pold under the null.".format(p_old))
df \ $     .groupby('name') \ $     .count() \ $     .collect()
customer_with_purchases_3ormore = df_r1["Counter"].isin(["3"]) $ df_r2 = df_r1.loc[customer_with_purchases_3ormore,:]
archive_clean=pd.merge(twitter_archive_clean, image_predictions_clean, on='tweet_id', how='left')
dutchPhoneNumber = re.compile(r'(^\+[0-9]{2}|^\+[0-9]{2}\(0\)|^\(\+[0-9]{2}\)\(0\)|^00[0-9]{2}|^0)([0-9]{9}$|[0-9\-\s]{10}|[0-9]{2}-[0-9]{7}|[0-9]{3}-[0-9]{6}$)') $ for number,quantity in phoneNumbers: $     strippedNumber = number.replace(" ", "") $     if not dutchPhoneNumber.match(strippedNumber): $         print strippedNumber
pprint(q_pathdep_obs.metadata(reload=True))
df['created_at'].tail()
pd.DataFrame(pdata)
data['publish_date'] = pd.to_datetime(data['publish_date'])
url = "ftp://ftp.star.nesdis.noaa.gov/pub/sod/mecb/crw/data/outlook/v4/nc/v1/outlook/{:%Y}/{}".format(today_UTC,lfiles[-1])
from arcgis.features import FeatureLayerCollection
user_logs['is_satisfied'] = user_logs.completed_songs_ratio.apply(lambda x: 1 if x > 0.5 else 0)
HMM('GOOG', datetime.datetime(2016,1,1), datetime.datetime(2016,5,1),True)
sns.countplot(y="lang", data=tweet_table, palette="Blues")
daily_ret_mean = daily_ret.mean() $ daily_ret_mean
df = pd.read_excel("msft.xls") $ df.head()
twitter_archive_full[(twitter_archive_full.text.str.contains('doggos'))][['tweet_id','stage','text']]
pres_df.drop('time_from_creation_tmp', inplace=True, axis=1) $ pres_df.head()
df2['intercept'] = 1 $ df2['ab_page'] = pd.Series(np.zeros(len(df2)), index=df2.index)
session_df.groupby(by='experiment').agg({'delta_recall': ['mean', 'min', 'max']})
plt.hist(np.nan_to_num(_delta.values()), bins=10) $ plt.tight_layout()
hits_df = hits_df.reindex(til_today)
cursor.execute("SELECT complaint, descriptor, created_date, closed_date FROM dot_311") $ dot311 = cursor.fetchall()
field_data =  pd.read_csv("fields_data.csv")
maeG = np.sum(np.absolute(predGdf.values-xGR.values)) $ rmseG = np.sqrt(((predGdf.values-xGR.values) ** 2).mean()) $ print "FIT G  :  RMSE  \  MAE  = ", rmseG, "  \  ", maeG, "\n"
sqlite> SELECT value, COUNT(*) as sum $    ...> FROM nodes_tags $    ...> WHERE key='highway' $    ...> GROUP BY value;
df1.info()
tls.set_credentials_file(username='ddeloss', api_key='0uwykwidtt')
t.minute
print("Identify Injured by Keyword") $ print(df.cdescr.str.contains('INJUR|HURT').sum()) $ print(len(df))
for key, value in close.items(): $     if value == min_dif_Q5: $         print key, value
words = [w for w in words if not w in stopwords.words("english")] $ print(words)
pf.cost.sum()/100
agency_borough = data.groupby(['Agency','Borough']).size().unstack() $ agency_borough
df2 = ab_data[((ab_data.group == 'treatment') & (ab_data.landing_page == 'new_page')) | $               ((ab_data.group == 'control')   & (ab_data.landing_page == 'old_page'))]
gsmodel.best_params_
df3['timestamp'] = pd.to_datetime(df3['timestamp']) $
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall()
corn["price"].max() $ corn[["price", "sold"]].max()
dates = pd.date_range('2010-01-01', '2010-12-31') $ df = get_data(symbols=['XOM'],dates=dates) $ df.head()
damd['created'] = pd.to_datetime(damd['created']) $ damd['created'].head(3)
del df_aggregate, df_broken, df_ok, selected_disks $ gc.collect()
print ("Probability that individual was in the control group,and they converted: %0.5f" % (df2.query('converted == 1 and group == "control"').shape[0]/df2.query('group == "control"').shape[0]))
options_frame['BidAskSpread'] = options_frame['Ask'] - options_frame['Bid'] $ errors_20_largest_by_spread = options_frame.ix[sorted_errors_idx.index] $ errors_20_largest_by_spread[['BidAskSpread', 'ModelError']].sort_values(by='BidAskSpread').plot(kind='bar', x='BidAskSpread')
import pandas as pd $ pd.read_csv('network/mthalamus_mcortex_edge_types.csv', sep=' ')
def cost_computation(theta, X, y): $     hx = sigmoid(np.dot(X, theta)) # predicted probability of label 1 $     cost = (-y)* np.log(hx) - (1-y)*np.log(1-hx) # log-likelihood vector $     J = cost.mean() $     return J
timeMDict['health_bmiDF'].head(n = 5)
df_tsv.duplicated()
tfidfnmf_topics.head()
df_release = df_release.dropna(axis=0, subset=['actual']) $ df_release.shape
bp.rename(columns={"value1num":"systolic", "value2num":"diastolic"}, $           inplace=True)
day_of_month15 = uber_15["day_of_month"].value_counts().sort_index().to_frame() $ day_of_month15.head()
df_goog['Year']   =  df_goog.index.year $ df_goog['Month']  =  df_goog.index.month $ df_goog['Day']    =  df_goog.index.day
fraud_data.loc[0,"ip_address"]
with open("data/nouns_twitter.json", 'w') as fh: $     json.dump(nouns, fh) $ with open("data/adjectives_twitter.json", 'w') as fh: $     json.dump(adjectives, fh)
mnnb = MultinomialNB() $ mnnb.fit(X_train_dtm, y_train)
pd.set_option('display.max_colwidth', -1) $ df.loc[df.Sentiment==1, ['description','Sentiment']].head(10)
testing_array.shape
desc_stats = desc_stats.transpose().unstack(level=0).transpose() $ desc_stats
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
van15_fin['stiki_percent']=van15_fin['stiki_mean']<0.1
lda_tfidf.print_topics(num_topics=10, num_words=7) $
df2[df2.duplicated(subset = 'user_id', keep = False)]
df2[df2.duplicated(['user_id'],keep=False)== True]
Xs.sum().sort_values(ascending = False).head(10)
plt.hist(p_diffs) $ plt.axvline(actual_diff, color = 'r');
tobs_df.hist(bins=12) $ plt.legend(["tobs"],loc='best') $ plt.ylabel("Frequency") $ plt.show()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger') $ print(z_score) $ print(p_value)
diff_weekly_mean.show(10)
user_ids = df2['user_id'] $ df2[user_ids.isin(user_ids[user_ids.duplicated()])]
df.T.to_csv('../outputs/CSFV2_outlook_weekly_90th_per_summary_table_from_{:%Y%m%d}.csv'.format(dates_range[0]))
lr = LogisticRegressionCV() $ lr.fit(train_Features, train_species)
!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.8,en;q=0.6" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/6322/train-jpg.tar.7z?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1532362578&Signature=bEVBoqLUCBiiz7J%2F%2FjIley%2F6k4eQ273wCeqvGZ3l40AMu03p3yJEFy%2FoWM6Xp5sLLZm5jrxNEW0lZbY7qEf3Fkj1KqnhYt7FsjRUcngewQ6vpGH79Oq0A4%2B9y6J%2F8gcL%2BtMZKTHkeh7HpLUGegji8T3l7lCv4LoVSsdpnT4aWpuvVp2UtNOLvGxW%2FSm1GKWIu6rqmxvixzH6S3zeSmTkBvxwZyDdCFT38MYqvn6Djr9VuIbMM6FvX0ZLL47o0VW%2BW3shEcumO8wdC1Srl2fayyhflUT%2BLkTqsXdQMCYV4KGRAH5xWJbd4cLvixpOymjduZGsbwEJb94V6tr1H6AL1w%3D%3D" -O "train-jpg.tar.7z" -c
s.empty
qends = pd.date_range('2014-01-01','2014-12-31',freq='BQS-JUN') $ qends.values
query2 = feature_layer.query(where="POP2010 > 1000000") $ query2.sdf
df_all['datetime_utc'] = pd.to_datetime(df_all['created_utc'], unit='s') $ df_all['datetime_utc'] = pd.DatetimeIndex(df_all['datetime_utc']) $ print(df_all['datetime_utc'][0:5])
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())
train_df.Q2_primary_target_of_opinion.value_counts()
df = pd.read_csv('anova_one_factor.csv')
test_dum_clean = test_dum[['ID'] + [col for col in train_dum.columns if 'Source_' in col or $                       'Customer_Existing_' in col]]
p_old = df2.query('converted == 1').user_id.count() / df2['user_id'].count() $ print(p_old)
airbnb_df['host_is_superhost'].value_counts(dropna=False)
df2.drop(df2.index[1876], inplace=True)
twelve_months_prcp.plot(figsize = (12, 8), rot = 45, fontsize=20, use_index = True, legend=False) $ plt.ylabel('Precipation', fontsize=20) $ plt.xlabel('Date', fontsize=20) $ plt.title("Precipition in Hawaii from %s to %s" % (twelve_months_prcp.index.min(),twelve_months_prcp.index.max()), fontsize=24) $ plt.show()
details.dtypes
p_old = ab_file2['converted'].mean() $ print(p_old)
df.notnull().count()
samples_query.execute_call('Person', 'ByName', 'A') $ samples_query.display_records(5)
joblib.dump(nmf, './data/NMF.pkl') $
df2.columns
p_new = df2.query('landing_page == "new_page"')['converted'].mean() $ print('The convert rate for the new page:', p_new)
reddit = data.drop(['body','id','created', 'is_video', 'thumbnail', 'url', 'timestamp', $                     'time_up', 'time_up_sec', 'time_up_clean'], axis=1)
to_be_predicted_Day1 = 17.37 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
y.tail()
train.drop(['Class', 'Gender'], axis = 1).corr()
df2[df2['user_id'].duplicated(keep=False)]
chk = joined.loc[joined['state_hol']==1] $ holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
df[((df['group']=='treatment') &(df['landing_page']!='new_page'))|((df['group']!='treatment') &(df['landing_page']=='new_page'))].group.count()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value
sites_in_region= (df_providers.groupby(['drg','year'])[['disc_times_pay']].sum()) $     payments_by_DRG_by_YEAR = payments_by_DRG_by_YEAR.reset_index() $     payments_by_DRG_by_YEAR = payments_by_DRG_by_YEAR.drop(payments_by_DRG_by_YEAR.index[0]) $     payments_by_DRG_by_YEAR.to_csv(str(today) + ' Payments by DRG by YEAR.csv',index=False) $
print("Output: Count of missing data\n") $ for col in missing_info: $     num_missing = df_users_first_transaction[df_users_first_transaction[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing))    
plt.scatter(X2[:, 0], X2[:, 1], c = day_of_week, cmap='rainbow'); $ plt.colorbar();
html = browser.html $ mars_facts = bs(html, 'html.parser')
my_features.shape
lm=smf.ols('tripduration ~ gender',data=df1).fit() $ lm.summary()
from sklearn.preprocessing import StandardScaler $ X = df.values[:,1:] $ X = np.nan_to_num(X) $ Clus_dataSet = StandardScaler().fit_transform(X) $ Clus_dataSet
y_pred_lgr =logreg.predict(X_test) $ y_train_pred_lgr=logreg.predict(X_train) $ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(logreg.score(X_test, y_test)))
len(out.index.unique()), out.shape
response = urllib.request.urlopen('http://example.com/') $ html = response.read() $ print(html)
print(mbti_text_collection.info())
check_int('2016-11-06').number_repeat.max()
from sklearn.pipeline import Pipeline
old_page_converted = np.random.choice([0,1], size=n_old, p=[p_old, (1-p_old)])
df['margin_val'].min()
df.dist_km.mean()
y = X.dollar_chg_opencls $ X = X.drop('dollar_chg_opencls', axis = 1) $ X_train, X_test = X[:2800], X[2800:]
df2[df2.duplicated(['user_id'], keep=False)]
mars_weather = weather.text.strip() $ mars_weather
fig1, axes1 = plt.subplots(2, 2, figsize=(12, 4), sharex=True, sharey=True) $ for i in range(2): $     for j in range(2): $         axes1[i, j].hist(np.random.randn(500), bins=15, alpha=0.4, color='c') $ plt.subplots_adjust(wspace=0.1, hspace=0.1) $
t1 = t1.T $ t0 = t0.T $ t0.shape
eur_usd = df.loc['EUR-USD','Change'] #eur_usd points to the value inside the dataframe $ df.loc['EUR-USD','Change'] = 1.0 #Change the value in the view $ print(eur_usd) #eur_usd is changed (because it points to the view) $ print(df.loc['EUR-USD']['Change']) #The dataframe has been correctly updated
pd.Series(pd.DatetimeIndex(pivoted.T[labels==0].index).strftime('%a')).value_counts().plot(kind='bar');
In [10]: df = pd.read_csv(StringIO(data), index_col=0,parse_dates=['Quradate']) $ In [11]: df.head()
df = pd.read_sql_query('SELECT DISTINCT City FROM data', disk_engine) $ df.head()
news_df.head()
xyz = json.dumps(youtube_urls, separators=(',', ':')) $ with open('youtube_urls.json', 'w') as fp: $     fp.write(xyz) $
list(purchases.columns)
intervention_train = intervention_train.merge(update_date, left_on=['INSTANCE_ID', 'CRE_DATE_GZL'], right_index=True)
country_tz = country_df.merge(zone_df,on='country_code')
!hadoop fs -ls -h stocks.parquet
stories.head()
df_final.drop(df_final.columns[[0,1,2]], axis=1, inplace=True)
tfav.plot(figsize = (16, 4), color ='b') $
df2[df2['user_id'].duplicated()] $
twitter_ar['source_url'] = twitter_ar['source'].apply(lambda row: row.split('rel')[0]) $ twitter_ar['source_type'] = twitter_ar['source'].apply(lambda row: row.split('rel')[1])
import os $ files = os.listdir("/data/measurements") $ print('\n'.join(files))
pca=decomposition.PCA() $ stocks_pca_t1= pca.fit_transform(stocks_pca_m1)
pred1 = nba_pred_modelv1.predict(g1) $ prob1 = nba_pred_modelv1.predict_proba(g1) $ print(pred1) $ print(prob1)
m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range) $ lr = 1e-3
X_train = pad_sequences(sequences, maxlen=21) $ X_train.shape
variables = pd.DataFrame.from_records(var)
f_lr_hash_modeling2 = spark.read.csv(os.path.join(mungepath, "logisitc_regression/lasso/f_lr_hash_inter2_2p18_noip_modeling2_split"), header=True) $ f_lr_hash_test = spark.read.csv(os.path.join(mungepath, "logisitc_regression/lasso/f_lr_hash_inter2_2p18_noip_test"), header=True) $ print('Found %d observations in f_lr_hash_modeling2.' %f_lr_hash_modeling2.count()) $ print('Found %d observations in f_lr_hash_test.' %f_lr_hash_test.count())
import getpass $ !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL $ vcode = getpass.getpass() $ !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}
verify_response.json()['screen_name']
df8_lunch.mean() # mean is a bit higher than the smaller df6 data set
tweet_clean['retweeted_status'] = tweet_clean.retweeted_status.str.replace('This is a retweet' , '')
df['Close'].pct_change() #One timeperiod percent change
rng = pd.date_range('1/1/2011', periods=72, freq='H') $ type(rng)
lrm = sm.Logit(df_regression['converted'],df_regression[['intercept', 'ab_page']])
csv_filename = "/Users/miguel/Jottacloud/devel/osqf2015/data/scenarios2.csv"
sqlContext.sql(query).toPandas()
daily_cases.sort_values(ascending=False) $ daily_cases.head(10)
v_invoice_link.columns.tolist()
tweet_image.head()
print 'Mean {} STD +/- {}'.format(np.mean(score),np.std(score))
print(most_informative_features(our_nb_classifier.classifier, n=10))
csvData['yr_renovated'].value_counts()
p_old = .1196
reg_traffic_with_flags['Hour'] = pd.to_datetime(reg_traffic_with_flags.Time_stamp).dt.hour $ reg_traffic_with_flags['Day'] = pd.to_datetime(reg_traffic_with_flags.Time_stamp).dt.day $ reg_traffic_with_flags['Month'] = pd.to_datetime(reg_traffic_with_flags.Time_stamp).dt.month
m.fit(lr, 3, metrics=[exp_rmspe])
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=amQDZBVZxsNFXgn8Dmpo') $ r.json()['dataset']['data'][1]
raw_authors_by_project_and_commit_df = git_project_data_df.select("project_name", "data.Author", "data.CommitDate") $
mask = data['listing_id'].apply(lambda x : 1 if str(x) in house_id else 0)
x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)
df_max = df.groupby('date').head(1) $ df_count = df.groupby(['date'] ,as_index=False).count() $ df_mean = df.groupby(['date'], as_index=False).mean()
plt.title('Stella Mccartney ngram', fontsize=18) $ stellamccartney_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/Stella_Mccartney_ngram.jpg')
sets_node['date'][DATA].head(12)
annual_precip_df['month'] = annual_precip_df.index.month $ annual_precip_by_month = annual_precip_df.groupby('month').sum() $ annual_precip_by_month.iloc[:].plot(kind='bar', title="Annual Precipitation by Month") $ plt.show()
closemeans.sort_values(ascending = False)
f"So many fields about {column_name.split(SEPARATOR)[0]}, I tell you!"
pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))
tbl3 = pd.concat([ff,hp2], axis = 1, join='outer') $ tbl3.columns = ['Mkt_RF','SMB','HML','RF','HPr'] $ tbl3['HPr_RF'] = tbl3['HPr'] - tbl3['RF'] $ tbl3.head()
date_now.toordinal()
vocabulary_expression['component_4'].sort_values(ascending=False).head(7) $
df.affiliate_channel.value_counts()
len(M7_pred),len(M7_actual),len(dfM.DATE[16:-12])
grid_lat = np.arange(24, 50.0, 1) $ grid_lon = np.arange(-125.0, -66, 1) $ glons, glats = np.meshgrid(grid_lon, grid_lat)
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLA.csv" $ tesla = pd.read_csv(url, index_col=0, parse_dates=True) # index_col = 0, means that we want date to be our index $ tesla
print('{0:.2f}%'.format((scores[0:2.5].sum() / total) * 100))
df_birth.info()
frame.loc[('b',2), 'Colorado']
result['lastUpdatedUTC'] = pd.to_datetime(result['lastUpdated'], unit='ms')
r = requests.get(url1) $ json_data = r.json()
df_predictions_clean.head(15)
neo_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
df = pd.read_csv(r'C:\Users\Patrik\Downloads\webrobots.iokickstarter-datasets\Kickstarter_2015-11-01T14_09_04_557Z\Kickstarter007.csv') $ df.head()
titanic['is_adult'] = titanic.apply(lambda row: row['age'] >= 18, axis=1) $ titanic.head()
df2 = df[((df.group == 'treatment') & (df.landing_page == 'new_page'))|((df.group == 'control')&(df.landing_page == 'old_page'))]
segments[segments.st_time.dt.month==2].head()
pred_labels = lasso2.predict(test_data) $ print("Training set score: {:.2f}".format(lasso2.score(train_data, train_labels))) $ print("Test set score: {:.2f}".format(lasso2.score(test_data, test_labels))) $ print("Number of features used: {}".format(np.sum(lasso2.coef_ != 0)))
conn.columninfo(table=dict(name='data.iris', caslib='casuser'))
df.reset_index('date', inplace = True) $ df.head()
session.query(func.min(Measurement.tobs), $               func.max(Measurement.tobs), $               func.avg(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
df=df.rename(columns={"product": "item"}) $ df
user1 = api.get_user(1242977138) $ print(user1.name, user1.screen_name, str(user1.statuses_count), str(user1.friends_count), str(user1.followers_count))
cityID = 'a592bd6ceb1319f7' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Diego.append(tweet) 
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']['authorCollaborators']#.loc[97545]
store1_data = data[data.Store == 1]
response = Query(github_index).get_terms("author_name")\ $                                    .by_organizations("user_orgs")\ $                                    .fetch_aggregation_results() $ pprint(response['aggregations']['0']['buckets']) $
pd.set_option('display.max_colwidth', 100) $ clinton_df.head()
tweets['hour'] = tweets['created_at'].dt.hour $ tweets['day_of_week'] = tweets['created_at'].dt.dayofweek $ days_of_week = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
pivoted.T[labels == 1].T.plot(legend=False, alpha=0.1)
stopwords = get_stop_words('spanish') $ stopwords.append("rt") $ stopwords.append("https") $ stopwords.append("http") $ stopwords.append("co")
to_be_predicted_Day4 = 21.38790489 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df_birth.shape
All_tweet_data=pd.merge(twitter_data_v2, tweet_data_v2, on='tweet_id', how='left') $ All_tweet_data.shape
SVPOL(data/'realdata'/'H.dat').to_dataframe().head()
df2.drop_duplicates(subset='user_id', inplace = True) $ if (df2.shape[0] == df2.user_id.nunique()): $     print( "No Duplicate(s) found!") $ else: $     print("Duplicate(s) not removed!")
dmh.CASDataMsgHandler.__subclasses__()
one_star_token_count = nb.feature_count_[0, :] $ five_star_token_count = nb.feature_count_[1, :]
probs_test[:, 1].mean()
score_merkmale.to_clipboard(encoding='UTF-8')
seats_per_hour = seats_per_hour.reindex(taxi_weather_df.index)
d.month
gjw.store.window = TimeFrame(start='2015-09-03 00:00:00+01:00', end='2015-09-05 00:00:00+01:00') $ gjw.set_window = TimeFrame(start='2015-09-03 00:00:00+01:00', end='2015-09-05 00:00:00+01:00') $ elec = gjw.buildings[building_number].elec $ mains = elec.mains() $ mains.plot() $
twitter_final1 = twitter_final[(twitter_final.dogType != 'None') & (twitter_final.rating_num>7)].groupby(['dogType','rating_num'])['tweet_id'].size().unstack().plot(kind='bar', stacked=True)
data.loc[data.expenses.notnull(), 'expenses/price'] = data['expenses']/data['price_aprox_usd']
with open(os.path.join(outputs, 'clean_reviews.pkl'),'rb') as f: $     (clean_train_reviews, $      clean_test_reviews, $      clean_train_reviews_sw, $      clean_test_reviews_sw) = pickle.load(f)
data.sort_values(by = ['lat-lon','description'], inplace=True)
vec1.get_feature_names() #feature names in test sample. $
CLIENT_ID = os.environ['FITBIT_CLIENT_ID'] $ CLIENT_SECRET = os.environ['FITBIT_CLIENT_SECRET']
stocks = pd.concat([AAPL, GOOGL]) $ stocks
dat[dat.x.isnull()]
berlin = ts_utc.tz_convert('Europe/Berlin')
df['CashRatio'] = df['CashRatio'].map(transform1)
bigdf.to_csv('Combined_Comments-Fixed.csv')
framesOut = [] $ for row in df.iloc[startOut:endOut].iterrows(): $         framesOut.append(np.array(row[1]).reshape(8,8).astype('float')) $
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.join(stores_df, 'store_nbr', 'left_outer') $
import matplotlib.pyplot as plt                                              # pyplot module from matplotlib library
df2.head()
dft[pd.datetime(2013, 1, 1):pd.datetime(2013, 2, 28)]
weather_all.sort_values('Temp (deg C)', ascending=False).head()
new_trends.related_queries()
cumFrame = pd.DataFrame(changes, columns=('Date', 'Client', 'Change')) $ cumFrame = cumFrame.sort_values('Date') $ print(cumFrame.loc[cumFrame['Client'] == 'AT&T']) $
np.exp(-0.0149), 1/(np.exp(-0.0149))
df_unit.head()
True == tab.find({str(527017):{'$exists':True}}).limit(1).count(True)
highest_change = r.json()['dataset_data']['data'][0][2] - r.json()['dataset_data']['data'][0][3] $ for ele in r.json()['dataset_data']['data']: $     if ele[2] - ele[3] > highest_change: $         highest_change = ele[2] - ele[3] $ print('Highest change based on High and Low prices - {}'.format(highest_change))
pats_chiefs_nov8_tweets = pats_chiefs_nov8_tweets.loc[pats_chiefs_nov8_tweets['UTC_Datetime'] <= '09-10-2017']
import statsmodels.api as sm $ convert_old = df2.query("converted==1 and landing_page=='old_page'").shape[0] $ convert_new = df2.query("converted==1 and landing_page=='new_page'").shape[0] $ n_old = df2.query("group=='control'").shape[0] $ n_new = df2.query("group=='treatment'").shape[0]
pres_df.rename(columns={'subject_count_test': 'subject_count'}, inplace=True) $ pres_df.head(2)
df.head()
obama_month_distri.plot(kind='bar', figsize=(10,5), rot= 45,title="# of Twitters of Brack Obama") $ plt.savefig('fig/obama_month.png');
final_word_df = final_word_df.reset_index()
html = browser.html $ soup = bs(html,'html.parser') $ get_full_img = browser.find_by_xpath('//*[@id="fancybox-lock"]/div/div[2]/div/div[1]/a[2]').first.click() $ time.sleep(3)  #allow time for page to load $
cursor = db.TweetDetils.aggregate([ {"$group" : {"_id":"$user_name", "score":{"$sum":"$retweet_cnt"}}},  {"$sort":{"score" : -1}},{"$limit":5}]) $ for rec in cursor: $     print(rec["_id"], rec["score"])
l = [[12,12],[11,11],[1,10],[10,1],[99,99]] $ l.sort(key=lambda x : x[1], reverse=True) $ l
year9 = driver.find_elements_by_class_name('yr-button')[8] $ year9.click()
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
pd.crosstab(test["rise_in_next_day"], advpredictions, rownames=["Actual"], colnames=["Predicted"])
data.columns
noise_graph= noise_df.groupby(noise_df.index.month).count().plot(y='Unique Key', legend=False) $ noise_graph.set_xticks([1,2,3,4,5,6,7,8,9,10,11, 12]) $ noise_graph.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']) $ noise_graph.set_ylabel("Noise Complaints")
main_cat_id_sparse = csr_matrix(df_cat_stat.main_cat_id.to_dense()).T $ goal_sparse = csr_matrix(df_cat_stat.goal.to_dense()).T $ duration_sparse = csr_matrix(df_cat_stat['duration']).T
series.values # values is the raw data
df.equals(df_from_hdf)
american_test.iloc[2]['reviews_text']
sfs1.k_feature_names_
sc_cut = parks_sc.groupby(['ParkID'])[['Score']].mean() $ sc_cut.reset_index(inplace=True) $ sc_cut.rename(columns={"ParkID": "parkid"}, inplace=True) $ sc_cut.head()
history = model.fit([nn_X_train], nn_y_train, initial_epoch=0, epochs=25, batch_size=100, validation_data=([nn_X_test],nn_y_test), shuffle=True)
s1 = pd.Series( [100,200,300,400,500] ) $ s2 = pd.Series( [10, 20, 30, 40, 50] )
bb.plot()
list_Media_ID = the_posts.keys()
bacteria2.dropna()
np.count_nonzero(na_df.isnull()) # total number of missing values
salesDF = sqlContext.read.format('jdbc').\ $           options(url='jdbc:db2://dashdb-entry-yp-dal09-10.services.dal.bluemix.net:50000/BLUDB:user=dash6431;password=wXDVudMyrnia;',\ $                   dbtable='GOSALES.BRANCH').load()
df2.sort_values('fatalities',ascending=False).head(5) #top 5 fatalities
td_bag = vect_tfidf.fit_transform(bag).toarray()
year12 = driver.find_elements_by_class_name('yr-button')[11] $ year12.click()
aaplA01 = aapl['2012-01'][['Adj Close']] $ withDups = pd.concat([msftA01[:3], aaplA01[:3]]) $ withDups
merge[merge.columns[16]].value_counts().sort $
therm_abs_rate = openmc.Tally(name='therm. abs. rate') $ therm_abs_rate.scores = ['absorption'] $ therm_abs_rate.filters = [openmc.EnergyFilter([0., 0.625])] $ tallies_file.append(therm_abs_rate)
df4 = pd.read_csv('2004.csv')
with open("data1.csv") as f: $     print(f.readline())
X_train_desc_vect = desc_count.fit_transform(train['description']) $ X_test_desc_vect = desc_count.transform(test['description'])
engine.execute("SELECT  * FROM contractor").fetchall()
search = Search(using=es_client, index='git') $ search.aggs.bucket('author', 'terms', field='author_name', size=100)\ $            .metric('first_commit', 'min', field='author_date')\ $            .metric('total_commits', 'cardinality', field='hash') $ results = search.execute()
pd.DataFrame(result)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ results.summary()
bplt.output_file('weeklyRatio.html')
dfpred = pd.DataFrame(pred) $ serpred = dfpred.apply(logtorec, axis=1) $ serpred.name = 'hotel_cluster' $ serpred.to_csv('testoutput.csv', sep=',', index_label='id', header=True) $ serpred.head()
dfstationmeanlunch[['STATION','totals']].nlargest(10, ['totals'])
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(dfX.set_index('user_id'), how='inner') $ df_new.head()
!find / -name index.html > links.txt $ !cat links.txt
df.isnull().sum(axis=0)   
atoms.df()
df.query('group=="treatment" and landing_page=="old_page"').count() $
GRID_accuracy_table = pd.concat( $     [LARGE_GRID.make_table_accuracy(raw_large_grid_df).assign(        algorithm='hmm_nosmooth'), $      LARGE_GRID.make_table_accuracy(raw_large_grid_df_engbert).assign(algorithm='engbert'     )]) $ GRID_accuracy_table
df.head()
watershed = gpd.GeoDataFrame.from_features(result['watershed'], crs={'init': 'epsg:4326'})
first_commit_timestamp = pd.to_datetime('2005-04-16 22:20:36') $ last_commit_timestamp = pd.to_datetime('2017-10-03 12:57:00') $ corrected_log = git_log[(git_log.timestamp >= first_commit_timestamp) & $                         (git_log.timestamp <= last_commit_timestamp)] $ corrected_log['timestamp'].describe()
df_new['intercept'] = 1 $ df_new[['CA','UK','US']]= pd.get_dummies(df_new['country']) $ df_new = df_new.drop('CA',axis = 1) $ df_new.head(5)
training.info()
dup_user = df2.groupby('user_id') $ dup_user.filter(lambda x: len(x) > 1) $
week42 = week41.rename(columns={294:'294'}) $ stocks = stocks.rename(columns={'Week 41':'Week 42','287':'294'}) $ week42 = pd.merge(stocks,week42,on=['294','Tickers']) $ week42.drop_duplicates(subset='Link',inplace=True)
df_new['intercept'] = pd.Series(np.zeros(len(df_new)), index=df_new.index) $ df_new['ab_page'] = pd.Series(np.zeros(len(df_new)), index=df_new.index)
roc_auc_score(predictions, fb_test.popular)
log_mod_new=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']]) $ results_new=log_mod_new.fit() $ results_new.summary2()
from sklearn import svm $ clf = svm.SVC(gamma=0.001, C=100.)
users['Month'] = users['CreationDate'].apply(lambda x: x.month)
surveys_df = pd.read_csv("surveys.csv", keep_default_na=False, na_values=[""]) $ for year in surveys_df['year'].unique(): $     surveys_year = surveys_df[surveys_df.year == year].dropna() $     filename ='surveys' + str(year) + '.csv' $     surveys_year.to_csv(filename)
df_test_index.action_type.value_counts()
logistic = SklearnClassifier(LogisticRegression()) $ logistic.train(train_set)
tweet = result[0] $ for param in dir(tweet): $     if not param.startswith("_"): $         print("%s : %s\n" % (param, eval('tweet.'+param)))
options_frame.info()
df2 = df2.rename(columns={'date': 'ESPN_Play_Count'})
def getjson(url): $     req = requests.get(url) $     specification = req.json() $     return specification
df.timestamp.drop_duplicates(inplace=True) $ df.timestamp.nunique()
old_page_converted = np.random.choice([1, 0], size=len(df2_control.index), p=[df2.converted.mean(), (1-(df2.converted.mean()))])
results.mean()
print fs.get_last_version("scansmpl.pdf").uploadDate
no_specialty = merged1[appointments['Specialty'].isnull()] 
cm = metrics.confusion_matrix(y_test, y_pred) $ print(cm)
df1 = clinton_df[clinton_df['source'] == "Twitter Web Client"].head(10) $ df2 = clinton_df[clinton_df['source'] == "TweetDeck"].head(10) $ pd.concat([df1, df2])
pd.merge(dfleft,dfright,on=['key1','key2'])
df.drop(todrop1, inplace=True)
autos = autos.drop(["nr_of_pictures", "seller", "offer_type"], axis = 1)
result['class'] = result['class'].map(dict1) $ result[:20]
one_hot_domains_questionable = df_questionable_2.groupby('user.id')[media_classes].sum().fillna(0) $ one_hot_domains_questionable = one_hot_domains_questionable.apply(normalize, axis=1).fillna(0) $ tsne = TSNE(n_components=2, learning_rate=150, verbose=2).fit_transform(one_hot_domains_questionable)
reason_counts = subset.map(lambda p: p.get("payload/reason")).countByValue() $ for reason, count in reason_counts.iteritems(): $     print("'new-profile' pings with reason '{}':\t{:.2f}%".format(reason, pct(count, ping_count)))
wedate=data['Week Ending Date'] $ wedate.head(10)
df2.user_id.duplicated().sum()
print(pd.date_range('2018-01-01',periods=4,freq='BQ'))
1/(np.exp(-0.0425))
daily_returns.hist() $ plt.show()
nntest = pd.DataFrame(labeled_features.loc[labeled_features['datetime'] >= pd.to_datetime('2015-10-01 00:00:00')]) $ pred = model.predict(test_x) $ pred = np.argmax(pred, axis=1) $ nntest['predicted_failure'] = enc.inverse_transform(pred)
TBL_FCInspevnt_query = "SELECT brkey, scourcrit, suff_rate, nbi_rating, in_modtime FROM TBL_FCInspevnt" $ data_FCInspevnt = pd.read_sql(TBL_FCInspevnt_query, cnxn) $ data_FCInspevnt
from sklearn.feature_extraction.text import CountVectorizer $ from sklearn.feature_extraction.text import TfidfVectorizer $ cvec = CountVectorizer(stop_words='english', max_features=100) $ tvec = TfidfVectorizer(stop_words='english', max_features=100)
ts.resample('D').mean()
temperature = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $     filter(Measurement.station == most_active).\ $     filter(Measurement.date > year_ago).\ $     order_by(Measurement.tobs).all() $ temperature
def normalize_data_column(df): $     df_first_event = json_normalize(df.data) $     df_first_event = df_first_event.set_index(df.index) $     df_first_event['first_event'] = [False if pd.isnull(e) else e for e in df_first_event.first_event] $     return df.merge(df_first_event, left_index=True, right_index=True)
rr.to_csv('output/ret_annualized_forcalendarfig.csv')
rng = pd.date_range('1/1/2011', periods=72, freq='H')
import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline $
from sklearn.mixture import GaussianMixture $ gmm = GaussianMixture(2) $ gmm.fit(X) $ labels = gmm.predict(X) $ labels
weather_warm['Station Name'].value_counts()
archive_copy.expanded_urls.isnull().sum()
for c in ccc: $     for i in ved[ved.columns[ved.columns.str.contains(c)==True]].columns: $         ved[i] /= ved[i].max()
pd.merge(df1,df3,how='left')
df2['converted'].sum() / df2.shape[0]
df.shape
actor = pd.read_sql_query('select actor_id, first_name, last_name from actor where first_name="Joe"', engine) $ actor.head()
rng.asobject.values[0]
from_api.info()
unique, counts = np.unique(y_hat, return_counts=True) $ print(unique, counts)
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
import numpy as np $ np.random.randint(1,10,len(rng)) $
np.count_nonzero(np.any(nba_df.isnull(), axis = 1))
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'us', 'uk', 'us_page', 'uk_page']]) $ result = log_mod.fit() $ result.summary()
np.exp(cig_data_SeriesCO)
df2.converted.sum()/df2.count()[0]
df_control = df2[df2.group == 'control'] $ sum(df_control.converted==1) / df_control.shape[0]
df_else=df[~df.BCLASS.isin(condoclass)] $ df = df_condo.append(df_else) $ df.shape
z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new, convert_old]), np.array([n_new, n_old]), alternative = 'larger')
robust_cov_matrix_b= pd.DataFrame(np.insert((np.insert(skcov.ShrunkCovariance().fit(daily_ret).covariance_,len(tickers),0,axis=0)),len(tickers),0,axis=1) $ ,columns=daily_ret_b.columns,index=daily_ret_b.columns) $ robust_cov_matrix_b
archive_clean['rating_numerator'].replace(5, 13.5, inplace = True) $ archive_clean['rating_numerator'].replace(75, 9.75, inplace = True) $ archive_clean['rating_numerator'].replace(27, 11.27, inplace = True) $ archive_clean['rating_numerator'].replace(26, 11.26, inplace = True)
y_pred = knn_pred $ print('precision: {:.2f}\nrecall: {:.2f}\naccuracy: {:.2f}'.format(precision_score(y_test,y_pred), $                                                        recall_score(y_test,y_pred), $                                                        accuracy(y_test,y_pred)))
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comments', ''))
error_count = telemetry[['datetime', 'machineID']].merge(error_count, on=['machineID', 'datetime'], how='left').fillna(0.0) $ error_count.describe()
import scipy $ corr_coeff = scipy.stats.pearsonr(data_compare['SA_mix'], data_compare['SA_google_translate']) $ print('Der Korrelationskoeffizient zwischen dem Mix und Google Translation ist:') $ print('----------------------------------------------------------------------------') $ print(corr_coeff[0]) $
tweet_archive_clean.stage.unique()
data.head() $
df.describe()  # Show the basic statistics
response_url = requests.get(url) $ parsed_json = json.loads(response_url.text) $ print(parsed_json['value']['timeSeries'][0]['values'][0]['value'][0]['value']) $ print(parsed_json['value']['timeSeries'][0]['values'][0]['value'][3]) $
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/mydata.json" $ df = pd.read_json(path) $ df.head(5)
total = flattened_pandas_df[['title','body','tag']].reset_index(drop=True)
control_conversion = df2[(df2['group'] == "control") & (df2['converted'] == 1)].count() $ total_control = df2[(df2['group'] == "control")].count() $ print(control_conversion/total_control) $
Nnew = df2.query('group == "treatment"')['user_id'].nunique() $ print (Nnew)
crimes[(crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE')&(crimes['SECONDARY_DESCRIPTION']=='TO PROPERTY')].head() $
c.execute('SELECT city, max(average_high) FROM weather') $ print(c.fetchone())
y1 = tweets1['handle'].map(lambda x: 1 if x == 'itsamandaross' else 0).values $ print max(pd.Series(y1).value_counts(normalize=True))
autos['price'].value_counts()
diz = {'pippo':3, 'pluto':4, 'topolino':"ciao"} $ s2 = pd.Series(diz) $ s2
pop.unstack(level=1)
import pandas as pd $ import numpy as np $ df = pd.DataFrame(np.random.randn(4,3),columns = ['col1','col2','col3']) $ df
setup = people.set_index('Id').loc[ids.keys(), ['Street', 'SetId', 'StartTimeStamp', 'EndTimeStamp']]
print(cc['name'].describe())
tsla_30_cnt = tsla_30.count()*100 $ print "{:,} users have tsla > 30 ({:.2%} of DAU)"\ $       .format(tsla_30_cnt, tsla_30_cnt*1./dau)
users_visits = users_visits.groupby('chanel', as_index=False).sum() $ users_visits = users_visits.assign(retention=lambda x:x.visits/x.regs).sort_values('retention') $ users_visits.head()
from sklearn.svm import LinearSVR $ model = LinearSVR() $ print ('SVR') $ reg_analysis(model,X_train, X_test, y_train, y_test)
train['date'] = train['date_x'] $ train.drop(['date_y','date_x'],1, inplace= True)
df2.drop(df2[df2['user_id'].duplicated(keep = False)].index[0], inplace = True)
plt.hist(weather.Precipitation.values, range=(0, 1.4)) #, bins = np.arange(0, 1.4, 0.1) $ plt.show()
site.get_sensors('electricity')
train_1m_ag = train_1m_ag.drop(['ip'],axis=1) $ train_10m_ag = train_10m_ag.drop(['ip'],axis=1) $ train_50m_ag = train_50m_ag.drop(['ip'],axis=1)
model_tree.fit(x_train,y_train) $ model_logit.fit(x_train,y_train) $ model_rf.fit(x_train, y_train) $ model_gb.fit(x_train,y_train) $ model_nn.fit(x_train, y_train)
df1 =pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
df_node = pd.read_sql(query_node,engine) $ df_node=df_node[['nid','title']] $ df_download = pd.read_sql(query_download,engine) $ df_download['date']=df_download['date'].apply(lambda x: datetime.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d')) $ df_download['date']=pd.to_datetime(df_download['date'])
ab_data.describe()
df['Views-PercentChange'].apply(np.log).hist()
print('There are {:.0f} visa applications for jobs located in NYC in this dataset.'.format(df_h1b_nyc.shape[0]))
df2.head(3)
day = pd.tseries.offsets.Day()
total = df2.nunique() $ pct_conv = df2.query('converted == 1').user_id.nunique()/total $ print(pct_conv)
A = pd.DataFrame(np.random.randint(0, 20, (2, 2)), $                  columns=list('AB')) $ A
ioDF.author_id_x.nunique()
cold_tweets = all_raw_tweets[all_raw_tweets['tweet'].str.lower().str.contains("cold")]
data = [{'a': 1, 'b': 2},{'a': 5, 'b': 10, 'c': 20}] $ df1 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b']) $ df2 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b1']) $ print(df1) $ print(df2)
raw_readings = {}    $ for row in rows:    $     raw_readings.setdefault(tuple(row[:4]), []).append(tuple(row[4:])) 
sns.violinplot(x='precipType', $                y='subjectivity', $                data=twitter_moves[twitter_moves['precipType'] != 'snow'])
news_df.shape
iris.head().loc[:,:"Sepal.Length"] $
autos.rename({"odometer" : "odometer_km"},axis=1,inplace = True) $ autos["odometer_km"] = autos["odometer_km"].str.replace(",","").str.replace("km","").astype(float)
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_index()        
%%gcs read --object gs://inpt-forecasting/Inpatient Census extract\t-\tPHSEDW\t71118.csv --variable icd
z_score, p_value = sm.stats.proportions_ztest([convert_new[0], convert_old[0]], [n_old, n_new], alternative='larger') $ z_score , p_value
df2[df2['user_id'].duplicated() == True]
from scipy.stats import norm $ norm.ppf(0.025)
bixi_hourly=bixi_hourly.resample('1H', how={'duration_sec': np.mean, $                                             'distance_traveled': np.mean, 'is_member':np.sum, $                                             'number_of_trips':np.sum})
Moscow_food['delivery_string'] = Moscow_food['delivery_lat'].map(str) + ',' + Moscow_food['delivery_long'].map(str) $ unique_delivery_strings = Moscow_food.delivery_string.unique()
result=results.set_index(['date','home_team','away_team']) $ result.head()
df_lda = pd.DataFrame(data_lda) $ print(df_lda.shape) $ df_lda = df_lda.fillna(0).T $ print(df_lda.shape)
users = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv') $ users.head()
df_copy=df_copy.drop(columns=['doggo', 'floofer','pupper','puppo'])
for row in nocachedf.itertuples(): $     print(row)
tweet_data_clean.set_index(tweet_data_clean.columns[0], drop = False, inplace = True) $ tweet_data_clean.head()
print df.pivot(index='date', columns='item', values='status')
dfFull['1stFlrSFNorm'] = dfFull['1stFlrSF']/dfFull['1stFlrSF'].max()
def fix_runtime(runtime): $     return runtime.split(' ')[0]
import matplotlib.pyplot as plt $ %matplotlib inline
dfClientes.shape[0]
df = pd.read_sql('SELECT * FROM users_ukr', engine).set_index('id')
cercanasAfuerteApacheEntre150Y200mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 150) & (cercanasAfuerteApache['surface_total_in_m2'] < 200)] $ cercanasAfuerteApacheEntre150Y200mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre150Y200mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre150Y200mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
last_unix=last_date.timestamp() #it is a function of the time module that can convert the time to $
pd.date_range(start, periods=1000, freq='M')
result.head(10)
test_data.columns
from sklearn.decomposition import PCA $ pca = PCA(n_components=2) $ pca.fit(data_float_pca) $ red = pca.transform(data_float_pca) $
daily = count_15.resample('1d').sum()
twitter_archive_master.name.value_counts()
df_groupby_date = DF_filtered.groupby('date')
station_count2 = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ station_count2
df_providers_pared = df_providers_pared[['id_num', 'drg', 'name', 'street', 'city', 'state', 'zipcode', 'region', $        'discharges', 'avg_charges', 'total_payment', 'medicare_payment', $        'year', 'drg3', 'disc_times_pay', 'Zip', 'zip_length']] $ df_providers_pared.head()
cpi_sdmx.lookup_code('','Time',component='TimeDimension') $
fat.plot_daily_ticker(ohlcv)
horror_readings=horror_readings.groupby('visitor_id').size()
df.iloc[:].plot(title="Precipitation By date") $ plt.xticks(rotation=70) $ plt.tight_layout() $ plt.savefig("precipitation.png") $ plt.show() $
df_corr = result.groupby(['type', 'scope'])['site'].sum().reset_index() $ display(df_corr.sort_values('site',ascending=False).head(10)) $ plot2D(df_corr, 'type', 'scope','site')
df_uro[ls_other_columns] = df_uro[ls_other_columns].applymap(clean_string)
suspects_with_25_1['in_cp'].value_counts()
new = df2['landing_page'] $ new_converted_null = df2.query('converted == 1') $ p_new = new_converted_null.count()[0]/new.count() $ p_new
session.query(Measurement.station).group_by(Measurement.station).count() $
train_data['brand_int'] = train_data['brand'].apply(get_integer5) $ test_data['brand_int'] = test_data['brand'].apply(get_integer5) $ del train_data['brand'] $ del test_data['brand']
tfidf.get_feature_names()[75]
print(prec_nc) $ for v in prec_nc.variables: $     print(prec_nc.variables[v])
x.drop([0, 1])
(trn_texts $  .groupby('SDG') $  .count())
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['p1_dog'] == False].index)
from nltk.stem.porter import PorterStemmer $ porter = PorterStemmer() $ def tokenizer_porter(text): $     return [porter.stem(word) for word in text.split()]
if 0 == 1: $     news_titles_sr.to_pickle(news_period_title_docs_pkl)
dateparse = lambda x: pd.datetime.strptime(x, '%Y%m') $ ff = pd.read_csv('F-F_Research_Data_Factors.csv',index_col=0, parse_dates=True, date_parser=dateparse)['2011':'2013'] $ ff = ff / 100 $ ff.head()
wk_output = wk_prob[['explain', 'info_found', 'url', 'created_at', 'node_id']] $ wk_output = wk_output.drop_duplicates()
X_train, X_test, y_train, y_test = train_test_split(X_features,tweets_1['sentiment'], test_size = .2) $ rdf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1) $ rdf_model = rdf.fit(X_train, y_train) $ y_pred = rdf_model.predict(X_test) $ precision, recall, fscore, support = score(y_test, y_pred, pos_label = 4, average = 'binary')
tweet= api.user_timeline(target_user, count=1)[0]
names = [n.replace('open.', '') for n in price_mat.columns]
twitter_archive_clean.head()
twitter[twitter.tweet_id.duplicated()] $
autos["odometer_km"].value_counts()
back_to_h2o_frame = h2o.H2OFrame(pandas_small_frame) $ print(type(back_to_h2o_frame)) $ back_to_h2o_frame
for i in df2.user_id.unique(): $     h.remove(i)        
np.exp(-0.0150)
reviews_w_sentiment['sentiment_score_scaled'] = reviews_w_sentiment['sentiment_score']*5 + 5 $ reviews_w_sentiment[['sentiment_score_scaled', 'sentiment_score']].describe() 
df2[df2.user_id.duplicated(keep=False)].iloc[0,0]
appointments['Specialty'].loc[appointments['Specialty'].isin(doctors)]= 'doctor' $ appointments['Specialty'].loc[appointments['Specialty'].isin(RN_PAs)] = 'RN/PA' $ appointments['Specialty'].loc[appointments['Specialty'].isin(therapists)] = 'therapist'
a.argmin(1)
props = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
import csv $ data_with_header = list(csv.reader(open("test_data//askreddit_2015.csv", encoding="utf8"))) $ data = data_with_header[1:len(data_with_header)]
s = pd.Series(todays_datetimes) $ s.diff().mean()
from sklearn.linear_model import LogisticRegression
def closest_users_data(df, close_user_list): $     return df[df.user_id.isin(close_user_list)] $ close_user_df = closest_users_data(user_df, smaller_closer_user) $ print(close_user_df)
y = df['comments'] $ X = df['title'] $ X = X.apply(lambda x: PorterStemmer().stem(x))
p_value_obs_lt = (p_diffs < obs_p_diff).mean() $ _, p_value_ztest_lt = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='smaller') $ p_value_obs_lt, p_value_ztest_lt
posts.to_excel('digitalreport.xlsx')
plt.show()
df_2003['bank_name'] = df_2003.bank_name.str.split(",").str[0] $
committees = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
df['critic'].value_counts()
nltk.download('stopwords') $ from nltk.corpus import stopwords $ stop = stopwords.words('english')
f.visititems(ls_dataset)
groupby_w = df['y'].groupby(df['w']) $ round(groupby_w.describe(), 3)
df_c2[['CA', 'UK', 'US']] = pd.get_dummies(df_c2['country'])
df1=pd.DataFrame.from_records(raw,index='id_loan_request')
df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %I:%M:%S %p')
df.dtypes
(obs_diff < p_diffs).mean()
p_diffs = np.array(p_diffs) $ p_diffs
cabs_df_byday = cabs_df.loc[cabs_df.index.weekday == weekday] $ cabs_df_byday.info() $ cabs_df_byday.head()
df3['DETAILS']=df3['DETAILS'].fillna("")
scr_activated_df.head()
df_old = df2[(df2['landing_page'] == 'old_page')] $ converted = df_old['converted'] $ old_page_converted = np.random.choice(converted, n_old)
gscv.best_estimator_
wellTrans.to_sql(con=engine, name='pumpingtests', if_exists='replace', flavor='mysql',index=False)
combined_df.to_csv('../Created CSVs/Cleaned_For_Analysis_Compressed_By_Day.csv')
df.groupby("cancelled")["created_as_guest"].value_counts()
from matplotlib import pyplot as plt $ temp = reader.monitor.select_column("TM_T_SIPM") $ plt.hist(temp, bins=100)
SANDAG_jobs_df.loc[(2012, '1'), 'JOBS'].sum()
d.hour
import os  $ os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars elasticsearch-hadoop-6.1.1/dist/elasticsearch-spark-20_2.11-6.1.1.jar pyspark-shell'  
autos["ad_created_10"].value_counts(normalize = True, dropna = False).sort_index(ascending = False)
%matplotlib inline $ fb['2012-04-13':].resample('W').count()['message'].plot(figsize=(18,6))
reg_traffic_with_flags.duration.isna().value_counts()
q_pathdep_obs = c.submit_query(PTOQuerySpec().time("2018-06-10", "2018-06-12").set_id(0xc) $                                              .condition("ecn.multipoint.negotiation.path_dependent"))
arma_mod20 = sm.tsa.ARMA(dta_713, (2,0)).fit(disp=False) $ print(arma_mod20.params)
to_be_predicted_Day5 = 31.29300322 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
table3.dtypes
all_sites_with_unique_id_nums_and_names.loc[indices_duplicated_sites[:5],:]
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
postsM.head(2)
import calendar $ may_2016 = calendar.month(2016, 5) $ print(may_2016.encode('latin1').decode('cp1251'))
sakhalin_freq = sakhalin_filtered.genus.value_counts() / len(sakhalin_filtered) $ sakhalin_freq
cityID = '1c69a67ad480e1b1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Houston.append(tweet) 
pl[['elapsed','cost']].plot()
df_pol_matrix = tvec.transform(df_pol_t['text'])
model = sm.GLM(y2_train, X_train, family=sm.families.Poisson(statsmodels.genmod.families.links.log)) $ results2 = model.fit()
oppose.amount.sum()
sl[sl.status_binary==0][(sl.today_preds==1)].shape
metrics.name.value_counts()
df_convsbyCountry.head() $ totalConvs_month_byMC = pd.DataFrame(df_convsbyCountry.groupby(['marketing_channel', 'year', 'month', 'day'], as_index=False).sum()) $ print 'DataFrame totalConvs_month_byMC: ', totalConvs_month_byMC.shape $ totalConvs_month_byMC.head()
def fetch_project_github_data(org, project): $         result["project_name"] = project $         return result $     return list(map(append_project_info, gh_backend.fetch()))
sgd_predict = sgd.predict(testx)
y = combined_df4[['bin_label']].copy() $ X = dummies_df.drop(['bin_label','account_bal','bill_bal'], axis=1)
md2 = TextData.from_splits(PATH, splits, bs)
SurveySummary_flt=SurveySummary_raw[SurveySummary_raw['CYCLE']==1.0].rename(columns={'CYCLE':'SURVEY_CYCLE'}).drop(drop_list, axis=1) $ print(list(SurveySummary_flt)) $ SurveySummary_flt.head()
df = df[df.user_location_country.notnull() & df.user_location_region.notnull()& df.srch_ci.notnull() & df.srch_co.notnull()]
image_predictions_copy.info()
print(rhum_long_df['date'].min(), rhum_long_df['date'].max())
logs[['fm_ip', 'to_ip']].head(12)
content_rec = graphlab.recommender.item_content_recommender.create(sf_business, 'business_id')
X = df[['word_count','sentiment','subjectivity','domain_d','post_duration']] $ y_cm = LabelEncoder().fit_transform(df['com_label'])#Comments $ y_sc = LabelEncoder().fit_transform(df['score_label'])# Score $
npath = out_file2 $ resource_id = hs.addResourceFile('1df83d07805042ce91d806db9fed1eeb', npath)
df_clean.sample(3)
errors['errorID'].value_counts()
newdf.loc[newdf['score'] == 0.187218571]
data_compare.plot()
df_user.head()
df_test = pd.read_csv('loan_test.csv') $ df_test.head()
X_test.head()
p_new = df2[df2['converted'] == 1].user_id.nunique() / df2.user_id.nunique() $ p_new
df2['timestamp'] = pd.date_range('8/8/2018', periods=len(df2['MATCHKEY']), freq='D')
sp.head(10)
df_2004.dropna(inplace=True) $ df_2004
from sklearn.svm import SVC $ from sklearn import svm $ estimator = SVC(kernel='linear') $ from sklearn.model_selection import GridSearchCV
pd.Series(bnb.first_affiliate_tracked).isnull().any()
our_nb_classifier = engine.get_classifier("nhtsa_classifier")
stopword_list = stopwords.words("german")   #saves German stop words in a list $ print(len(stopword_list),"stop words in the list.")   #Prints number (len()) of elements in a list. $
lliberia = [pd.read_csv(filename) for filename in glob.glob("data/ebola/liberia_data/*.csv")] $ liberia_data = pd.concat(lliberia, join='outer')
dates = df_turnstile['DATE'].tolist() $ dates
!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz  --directory-prefix=data
df2 = df2[(df2.group == "treatment") == (df2.landing_page == "new_page")] $ df2
!wget https://cdn.pixabay.com/photo/2017/03/21/17/27/daffodils-2162825_1280.jpg
df.info() $ df.isnull().sum()
df[df.duplicated(['user_id'], keep=False)]
filename = processed_dir+'pulledTweetsProcessedAndClassified_df' $ gu.pickle_obj(filename,pulledTweets_df)
S_1dRichards.forcing_list.filename
pokemon_train = pokemon[~pokemon['Name'].isin(pokemon_test['Name'])]
user.ix["Trump", [1,2,4,5]]
rng + pd.tseries.offsets.BQuarterEnd()
X = dd_df.drop(['price'], axis=1, inplace=False) $ y = dd_df['price'] $ plt.figure(figsize=(10, 10)) $ y.hist(grid=True, bins=50)
twitter.rating_numerator.value_counts()
fed_reg_dataframe['token_text'] = fed_reg_dataframe['str_text'].apply(lambda x: word_tokenize(x.lower()))
for name, fullname in session.query(User.name, User.fullname): $ ...     print(name, fullname)
%%time $ df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv", nrows=50000, $                  usecols= ['Borough', 'Agency', 'Closed Date', 'Created Date','Complaint Type', 'Descriptor'])
var_df = pd.concat([GenVolMvt, GenCostMvt, CMVolMvt, SraMTMmvt], axis=1)
X = stock.iloc[915:-1].drop(['target', 'target_class', 'volatility', 'high', 'low', 'close', 'volume', 'open', 'news_sources', 'news_text', 'tesla_tweet', 'elon_tweet', 'daily_gain', 'daily_change'], 1) $ y = stock.iloc[915:-1].volatility
twitter_archive.rating_denominator.value_counts()
UK_treatment_conversion = df_c_merge[(df_c_merge['group']== 'treatment') & (df_c_merge['country'] == 'UK')]['converted'].mean() $ UK_treatment_conversion
merkmale.iloc[3]
fields = [num for num in xrange(15)] $ weather_df = pd.read_csv(weather_file, parse_dates=[['Date', ' TimeCST']], usecols=fields) $ print "Weather data loaded." $ weather_df.head()
lm = sm.Logit(df2['ab_page'], df2[['intercept', 'CA', 'UK']]) $ result = lm.fit() $ result.summary()
notus['cityOrState'].value_counts(dropna=False)
mgxs_lib.by_nuclide = True
meets_credit_policy = doesnt_meet_credit_policy.logical_negation() $ meets_credit_policy.head(rows=2)
pd.datetime.today()
tweets.groupby("related_attack").sum()["frequency"]
active_df['start_date'] = pd.to_datetime(active_df['scns_created'].apply(lambda x:x[0])).dt.strftime('%Y-%m')
wine_indices = {} $ for wine in wines: $     indices = dataset_filtered.index[dataset_filtered['variety'] == wine].tolist() $     wine_indices[wine] = indices
desktop.filter(lambda p: len(p['adapters'])> 1).take(1)
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit = 's') $ git_log['timestamp'].describe()
twitter_archive_master.sort_values(by=['favorite_count'], ascending = False).head()[['created_at','favorite_count','retweet_count','name','url','rating_num','dog_stage','breed' ]]
autos_p['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True)
pd.merge(df1, df3, left_on="employee", right_on="name").drop('name', axis=1)
old = (df2.query('group =="control"')['converted']==1).mean() $ old
vocab = np.array(vectorizer.get_feature_names())
df = df.iloc[2:] $ df.head()
df = df.set_index(df.Date)
tweet_json_clean['followers_count'].value_counts()
serc_pixel_df.tail(5)
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0) & (jobs.Group == 'cms')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
print(res.summary())
yxe_tweets['source'] = yxe_tweets.source.str.replace("\<a href\=\".+\"\s*rel\=\"nofollow\"\>", '') $ yxe_tweets['source'] = yxe_tweets.source.str.replace("\<\/a\>", '') $
ltcc = ltcc[np.isfinite(ltcc['w_sentiment_score'])] $ ltcc.info()
dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015', '2015-Jul-6', '07-07-2015', '20150708']) $ dates
print(trump.favorite_count.sum()) $ print(" ") $ print(trump.retweet_count.sum())
health_data.loc[:, ('Bob', 'HR')]
eng = dmh.SQLTable.create_engine('sqlite:///iris.db')
shifts = pd.DataFrame(df.groupby('atbat_pk')['is_shift'].sum()).reset_index() $ shifts.loc[shifts.is_shift > 0, 'is_shift'] = 1
hp = houseprint.Houseprint() $
grouped_dpt_city = department_df.groupby(["Department", "City"])
dfjoined.sort_values(['count_type_day'], ascending = 0).head()#very cold days have heat complaints
xmlData.set_value(296, 'zipcode', ' 98011') $ print xmlData.loc[296]
columns = ['doggo', 'floofer', 'pupper', 'puppo'] $ archive_copy.drop(columns, axis=1, inplace=True)
pd.date_range('2018-01-01', '2018-01-03', freq='T')
ann_ret_SP500[9].head(10)
total_word_count = 0 $ for text in all_text: $     total_word_count += len(text.split()) $ print('There are {0} words total.'.format(total_word_count))
sns.jointplot(x='number_of_claims', y='prosecution_period', data=utility_patents_subset_df, kind="kde") $ plt.show()
ls ../outputs-git_ignored/ff_50_15p
final = pd.concat([joined,joined1]) $ final['Issue Date and Time'] = pd.to_datetime(final['Issue Date and Time']).dt.month $ final.head().T
p_old = (df2.converted).mean() $ p_old
x = range(0,160000) $ y = range(0,160000) $ plt.plot(x, y) $ scatter(RandomForestRegressor_model.predict(X_test),y_test)
itemTable.shape[0]
moviesRdd = sc.textFile("movielens/movies.csv") $ moviesRdd.take(10)
df_state_victory_margins = pd.read_csv('./data/state_victory_margins_data.csv') $ df_state_victory_margins
model_data = df_users.copy()
df_tweet_clean2 = df_tweet_clean[df_tweet_clean.tweet_id.isin(id_list)] $ df_image_clean2 = df_image_clean[df_image_clean.tweet_id.isin(id_list)]
kfpd.plugin = KDEPlugin(verbose = False) $ fdf = time_method(kfpd, verbose = False, rerun_query = False, repetitions = 10) $ fdf.head()
!wget https://s3.amazonaws.com/arrival/embeddings/wiki.multi.fr.vec -O /tmp/wiki.multi.fr.vec $ !wget https://s3.amazonaws.com/arrival/embeddings/wiki.multi.en.vec -O /tmp/wiki.multi.en.vec
df_goog.dtypes
trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy') $ val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy') $ itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))
df_2002['bank_name'] = df_2002.bank_name.str.split(",").str[0] $
measurement_df.groupby(['station']).count().sort_values(by='id', ascending=False) $
from sklearn.feature_extraction.text import CountVectorizer $ def bow_extractor(corpus, ngram_range=(1,1)): $     vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range, max_features = 5000) $     features = vectorizer.fit_transform(corpus) $     return vectorizer, features
import matplotlib.pyplot as plt $ %pylab inline $ from datetime import datetime $ df.plot(y='lux',color='r', marker='.', linestyle="")
max_words = 1000 $ tokenize = text.Tokenizer(num_words=max_words, char_level=False)
med_dist = reddit['Above_Below_Median'].value_counts()/len(reddit) $ med_dist
chefdf = pd.merge(chefdf, chef02df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_02'))
data.values
df_t[df_t['Shipping Method name']=='PostNL with signature 0-30kg']['Updated Shipped diff'].hist() $ pd.DataFrame(df_t[df_t['Shipping Method name']=='PostNL with signature 0-30kg']['Updated Shipped diff'].describe())
%timeit df_census.ward.astype(int)
uber1 = uber[0:99] $ uber2 = uber[100:199] $ uber3 = uber[200:] $ print(uber1.head()) $ print(uber3.tail())
ohlc = walk.resample("H").ohlc() $ ohlc
pd.to_datetime("03/28/2018") + 3*bday_us
predict_actual[predict_actual.predicted_label==1.0]
(autos["date_crawled"].str[:10] $  .value_counts(normalize=True) $  .sort_index(ascending=True) $ )
train.popular.value_counts(normalize=True)
n_old = df2.query('group == "control"')['user_id'].count() $ print(n_old)
df.shape
imagelist = [i for i in os.listdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS')\ $              if i.endswith('Percentiles.txt') ] $ imagelist.sort() $ for k in range(5): $     print(imagelist[k])
wheels.merge(onehot, left_index=True, right_index=True).drop('drive_wheels', axis=1)
bwd.drop('store_nbr', 1, inplace=True) $ bwd.reset_index(inplace=True) $ fwd.drop('store_nbr', 1, inplace=True) $ fwd.reset_index(inplace=True)
loans_df.loan_status.value_counts()
min_div_stock=df.iloc[df["Dividend Yield"].idxmin()] $ min_div_stock $ print("The stock with the minimum dividend yield is %s with yield %s" % (min_div_stock['Company Name'],min_div_stock['Dividend Yield']))
z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new], nobs=[n_old, n_new] ) $ z_score, p_value
df2.query('group == "control" and converted == 1').count()['user_id']/df2.query('group == "control"').count()['user_id']
model = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA','UK']]) $ results = model.fit() $ results.summary()
autos[date_cols].head(3)
High_temp_station = session.query(func.min(measurements.tobs), $                                   func.max(measurements.tobs), $                                   func.avg(measurements.tobs)).\ $                     filter(measurements.station == active_stations[0][0]).all() $ High_temp_station
volumes = df[1::2].transpose().stack() $ volumes.index = index $ volumes.plot()
dff2 = pd.melt(dff,["index"], var_name="team", value_name="gts") $ dff2 = dff2.rename(columns={'index':'date'})
live_weights.value_counts()
def rmsle(y, y_pred): $     return np.sqrt(mean_squared_error(np.log(y), np.log(y_pred)))
max_date=session.query(func.max(Measurement.date)).all() $ max_date
utils.serialize_data(data)
plt.hist(p_diffs); $ plt.axvline(-0.00127,color='red') $
Cleaneddata = clean_data(Salesdata) $ Cleaneddata.head()
model = sm.tsa.ARIMA(train, (1, 1, 0)).fit() $ predictions = model.predict(dynamic=True) $ print("Mean absolute error: ", mean_absolute_error(test, predictions)) $ model.summary()
dataframe.drop(['Date'],inplace=True,axis=1) $ scaler = MinMaxScaler(feature_range=(0, 1)) $ scaled = scaler.fit_transform(dataframe)
from sklearn.metrics import classification_report $ print(classification_report(y_true=y, y_pred=model.predict_proba(X)[:,0]>0.9))
autos["brand"].value_counts()
pivoted.T[labels==0].T.plot(legend=False, alpha = 0.1);
engine = create_engine("sqlite:///hawaii.sqlite", echo=False)
print('{:10} | {:15}'.format('LABEL','ENTITY')) $ for ent in doc.ents[0:20]: $     print('{:10} | {:50}'.format(ent.label_, ent.text))
df.describe()
cityID = '52445186970bafb3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chandler.append(tweet) 
kochdf = pd.merge(kochdf, koch03df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_03'))
df.drop(['ORIGIN_STATE_ABR','ORIGIN_AIRPORT_ID','DEST_AIRPORT_ID','DEST_STATE_ABR'],axis=1,inplace=True) $ df.drop(['Unnamed: 27','CARRIER_DELAY','WEATHER_DELAY','NAS_DELAY','SECURITY_DELAY','LATE_AIRCRAFT_DELAY'],axis=1,inplace=True)
prop_users_converted = df.converted.mean() $ prop_users_converted
n_new = ab_file2[ab_file2['landing_page']=='new_page'].shape[0] $ print(n_new)
df = pd.read_csv('./Data/AAPL.csv')  # try to explore several options of read_csv
gnb.__dict__
contrib_state = contribs.groupby('contributor_state')['amount'].sum().reset_index()
rng = pd.date_range(end='2018-01-19', periods=200, freq='BM') $ type(rng)
names = Counter(df_main.name) $ names
new_page_filtered = df[(df["landing_page"] == 'new_page') == (df["group"] != 'treatment')].shape[0] $ new_page_filtered
db.collection_names(include_system_collections=False)
first_week_day_num = pd.Series(np.arange(1,8), index = first_week) $ first_week_day_num
rf_pred = m.predict(X_test)
create_all_topic_vecs=True $ if create_all_topic_vecs: $     all_top_vecs = [lda.get_document_topics(serial_corp[n], minimum_probability=0) \ $                     for n in range(len(serial_corp))]
tbl[tbl.msno == '++mnT+fA+ilPMzHgEhsyS91vWjM7iRy7//aq/eVTm6s=']
df3.head()
df_count.sample(5)
tstLoc1 = gMapAddrDat.getGeoAddr(40.699100, -73.703697, test=True)  ## use .raw on output during testing $ print(type(tstLoc1))                                                ## to view JSON structure of Location obj $ tstLoc1.raw
template = '%.2f %s are worth $%d'
X_train = train.drop(axis=1, labels=['loan_status', 'default']) $ y_train = train.loan_status
df_twitter_copy['timestamp'] = pd.to_datetime(df_twitter_copy['timestamp'])
r.json()
merged.groupby(["contributor_firstname", "contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount", ascending=False).head()
autos['date_created'].str[:20].value_counts(normalize=True, dropna=False).sort_index() * 100
daily = counts.resample('d').sum() $ daily['Total'] = daily.sum(axis=1) $ daily = daily[['Total']] # remove other columns
table = driver.find_element_by_xpath('//*[@id="body"]/table[2]') $ table.get_attribute('innerHTML').strip()
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0] $ autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head()
session.query(Measurements.date).order_by(Measurements.date.desc()).first()
clean_weather = pd.read_csv('weather_clean.csv') $ clean_weather.head()
autos["price"].sort_index(ascending=False) #sorts according to index aka row positioning, so if its descending it will show from bottom up. $
births['decade'] = 10 * (births['year'] // 10) $ births.pivot_table('births', index='decade', columns='gender', aggfunc='sum')
autos.registration_year.describe()
merged1 = merged1.set_index('AppointmentDate')
import os $ os.chdir(dir) $ files = [f for f in os.listdir('.') if os.path.isfile(f)] $ files = [f for f in files if '.xls' in f]
doi_regex = re.compile(r"(?:.*?10\.?|^)(\d+(?:.\d+)+/.+)$") $ invalid_dois.apply(doi_regex.search).apply(lambda val: val and "10." + val.groups()[0])
df.loc[n]['skills']
import tensorflow as tf $ tf.logging.set_verbosity(tf.logging.ERROR)
positive_words=pos_sent.split('\n') $ negative_words=neg_sent.split('\n') $ print(positive_words[:10]) $ print(negative_words[:10]) $ positive_words
df = pd.read_sql("select * from noise_311", conn)
df_archive_clean = df_archive_clean.drop(["in_reply_to_status_id", "in_reply_to_user_id", $                                           "retweeted_status_id","retweeted_status_user_id"],axis="columns")
response=r.json() $ print(response)
ga, pa = bode(omega, G) $ bode(omegafft, Gfft, ga, pa, numpy.pi/4) $ plt.xlim(1e-3, 1e-2) $ ga.set_ylim(0.01, 1) $ pa.set_ylim(-numpy.pi/2, 0)
dd2=cfs.diff_abundance('Subject','Control','Patient')
print(testing_active_listing_dummy[0:5],testing_active_listing_dummy.mean())
autos.shape
inputNetwork.save_nodes(nodes_file_name='nodes.h5', node_types_file_name='node_types.csv', output_dir=directory_name)
train_topics_df=pd.DataFrame([dict(ldamodel[item]) for item in train_corpus], index=graf_train.index) $ test_topics_df=pd.DataFrame([dict(ldamodel[item]) for item in test_corpus], index=graf_test.index) $
df_small = raw_data[raw_data.num_tubes < sets_threshold] $ df_sets = raw_data[raw_data.num_tubes >= sets_threshold]
a = fs.put("Hello World", text='I am a tiny document', tags=['foo', 'bar'], username='skywalker') $ print 'put returned:\t', a $ print 'database contains:\t', fs.get(a).read()
autos = autos[autos["year_of_registration"].between(1910,2016)] $ autos["year_of_registration"].value_counts(normalize=True).head(10)
fundret.index
data.iloc[:, :3][data.three > 5]
res['predicted_clv'].to_csv('result.csv', sep=',')
old_page_converted = np.random.choice([0, 1], size=n_old, p=[1-convert_old, convert_old]) $ old_page_converted
if False: $     plt.scatter(train.diff_lat, train.duration) $     plt.show()
autos.rename(columns={"odometer":"odometer_km"}, inplace = True) $ autos.drop(["seller", "offer_type", "nr_of_pictures"], axis = 1, inplace = True) $ autos["price"] = autos["price"].str.replace("$","").str.replace(",","").astype(int) $ autos["odometer_km"] = autos["odometer_km"].str.replace("km","").str.replace(",","").astype(int) $ autos.head()
score_c.shape[0] / score.shape[0]
try: $     import pymysql $     pymysql.install_as_MySQLdb() $ except ImportError: $     pass
autos['price'].unique().shape[0]
actual_pold=(df2.query('group=="control"')['converted']==1).mean() $ actual_pold
apple = tweets[tweets['apple'] == True] $ apple.head()
plt.hist(p_diffs); $
model = clf.fit(train_data, train_labels)
order_with_data = pd.merge(order_item,product, how='left', left_on=['MATNR'], right_on =['MATNR']) $ paid_success_with_data = order_with_data[order_with_data['STATUS'] == "PAID_SUCCESS"] $ paid_success_with_data.head()
public_tweets = api.user_timeline(target_user)
output.to_csv( "randomforest.csv", index=False, quoting=3 )
pd.merge(d1, d2, on='city')
search.drop('message', axis=1).to_csv('tripactions_model_ready.csv')
year17 = driver.find_elements_by_class_name('yr-button')[16] $ year17.click()
r.json()
plt.title('startup distribution for Windows') $ plt.ylabel('count') $ plt.xlabel('log10(firstPaint)') $ frame['Windows_NT'].plot(kind='hist', bins=50, figsize=(14, 7))
answer.summary2()
groceries.iloc[2]  # explicity using a numeric index
summer['Mean TemperatureC'].plot(grid=True, figsize=(10,5))
fix_comma = lambda x: pd.Series([i for i in reversed(x.split(','))])
dmap = {0:'Mon',1:"Tue",3:"Wed",4:"Thurs",5:"Fri",6:"Sat",7:"Sun"} $ twitter_final['dayofweek'] = twitter_final['dayofweek'].map(dmap)
s.get(url, headers=headers)
def coord_tuple(located_data): $     return (located_data['lon'], located_data['lat']) $ located_data['coordinates'] = located_data.apply(coord_tuple, axis = 1)
from sklearn.ensemble import GradientBoostingClassifier
for (_, event) in df.iterrows(): $     write_event(event, cals, service)
save_n_load_df(promo_df, 'promo_df6.pkl')
std_for_each_weekday = delineate_in_weekdays.std().reset_index() $ std_for_each_weekday = std_for_each_weekday.rename(columns = {'duration(min)':'std(duration)'}) $ std_for_each_weekday
' '.join([sent.string.strip() for sent in spacy_tok(review[0])])
predictions = dtModel.transform(testData)
f = lambda x: x.max() - x.min()
app_ver_map['1.0.4']
import itertools $ import requests $ from urllib.parse import urlparse $ import pandas as pd
my_df.insert(2,'target','0') $ my_df.head()
df = pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv') $ df.head(2)
poiModel.summary()
speeches_df4.drop_duplicates(inplace = True)
transactions.head()
flight.count()
c.to_csv('../data/processed/crime_data_clean.csv', encoding='utf-8')
subway2_df.info()
tickerdf = tickerdf.dropna(axis=0,how='any')
cc.high.describe()
bad_dates_cnt = bad_dates.count()*100 $ print "{:,} users have a bad date format somewhere ({:.2%} of DAU)"\ $       .format(bad_dates_cnt, bad_dates_cnt*1./dau)
collection.list_items()
Z = np.array([1,2,3,4,5]) $ nz = 3 $ Z0 = np.zeros(len(Z) + (len(Z)-1)*(nz)) $ Z0[::nz+1] = Z $ print(Z0)
df_all_wells_wKNN.info()
data.drop_duplicates('id', keep = 'last', inplace = True)
week17 = week16.rename(columns={119:'119'}) $ stocks = stocks.rename(columns={'Week 16':'Week 17','112':'119'}) $ week17 = pd.merge(stocks,week17,on=['119','Tickers']) $ week17.drop_duplicates(subset='Link',inplace=True)
weather_df = weather_df.groupby("date").agg({"temp":"mean", "temp_min":"mean", "temp_max":"mean", "humidity": "mean", $                                 "wind_speed":"mean", "weather_main" : lambda x: concat_weather_types(x), $                                 "weather_description" : lambda x : concat_weather_types(x) })
print("Number of rows and features", tipsDF.shape)
df_model = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_model.head()
from scipy import optimize $ max_a = optimize.minimize(lambda x: -f(x), 37) $ print(max_a) $ f(max_a.x, True)
df_merged['retweet_count'].corr(df_merged['favorite_count'])
lsl = [pd.read_csv(filename) for filename in glob.glob("data/ebola/sl_data/*.csv")] $ sl_data = pd.concat(lsl, join='outer')
train["hotel_cluster"].value_counts(1)
Pop_df.columns = ['Date', 'Population']
rec_items('Shirley_a_louis@yahoo.com', product_train, item_vecs, user_vecs, customers_arr, products_arr, item_lookup)
precision = float(precision_score(fy, rf.predict(fX))) $ recall = float(recall_score(fy, rf.predict(fX))) $ print "Flight searches model -" $ print("The precision is {:.1f}% and the recall is {:.1f}%.".format(precision * 100, recall * 100))
import numpy as np $ np.random.seed(0) $ df = df.reindex(np.random.permutation(df.index))
retweet_and_count = sns.factorplot(data=tweets_df, x="name", y="retweet_count", kind="box") $ plt.xticks(rotation=60)
news_p = soup.find_all('p') $ for paragraph in news_p: $     print(paragraph.text)
maxVocab = 100000 $ wordsList = list(modelWord2Vec.vocab.keys())[:maxVocab]
to_be_predicted_Day1 = 21.18 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
df.groupby('type')['login'].nunique()
reliableData.describe(include = 'all')
np.arange(5, 10)
tweet_image_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']).value_counts()
(df $  .groupby(["C/A", "UNIT", "SCP", "STATION", "DATETIME"]) $  .ENTRIES.count() $  .reset_index() $  .sort_values("ENTRIES", ascending=False)).head(1)
y_.median()
vocab = vectorizer.get_feature_names() $ print(len(vocab)) $ print(vocab[:10])
len(calls.location.unique())
temp.head()
df.Opened.dt.year.value_counts().sort_index()/len(df)
path_to_places = 'python_data/moves_angelos/moves_export/csv/daily/places/' $ path_to_summary = 'python_data/moves_angelos/moves_export/csv/daily/summary/' $ csv_files_places = [single_csv for single_csv in os.listdir(path_to_places) if single_csv.endswith('.csv')] $ csv_files_summary = [single_csv for single_csv in os.listdir(path_to_summary) if single_csv.endswith('.csv')]
set(df.donor_id.values).intersection(noloc_df.donor_id.values)
ts = pd.Series(np.random.randn(3), periods)
df_new.groupby('country').nunique() $ df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new['intercept'] = 1
churned_ordered.index
%timeit regex.sub('','hello 1923worl 234 shad54awo')
data.registerTempTable("my_data")
print (df.loc[ [101,103], ['name','year1'] ], '\n')  # by list of row label and column names $ print (df.loc[  101:104 ,  'name':'year1'  ], '\n')  # by range of row label and column names
intersections_final.tail()
app_ids = ls_data.loc[(ls_data.Campaign.isin(['LS418','SFL418PA'])) $                      &(ls_data.Approved_Flag==1) $                      &(ls_data.funded_ind==0)].key.unique()
reviews.groupby('taster_name').points.mean()
trigram_model_filepath = paths.trigram_model_filepath
learner.save_encoder('adam1_enc')
from nltk import word_tokenize $ from nltk.corpus import stopwords $ ENG_STOPWORDS = stopwords.words('english') $ def text_preprocessing(original_text): $
fuel_type_dict = {"lpg": "lp_gas", "benzin": "gasoline", "cng": "natural_gas", $                   "hybrid": "hybrid", "elektro": "electric", "andere": "other"} $ autos["fuel_type"].replace(fuel_type_dict,inplace=True) $ autos["fuel_type"].fillna("not_specified", inplace=True)
image_predictions_clean = image_predictions_df.copy()
!cp gts*.csv drive/NBA_Data_Hackathon
ax = pre_analyzeable['prior_sims_phet'].value_counts().plot(kind='bar') $ for p in ax.patches: $     ax.annotate(str(int(p.get_height())), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize=25)
unique_users_count = df.user_id.nunique() $ print(unique_users_count)
def stageWonMap(x): $     p = 0 $     if x.opportunity_stage == 'Closed Won': p = x.opportunity_amount $     return p
crsr.execute(breakdown)
d = np.stack([a, b]); d  #Concatenate along a new axis (e.g., vectors to matrix).
merged_index_month = merged2.index.month
mod3 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'US', 'CA']]) $ mod3.fit().summary()
len(df2.query('group=="treatment"'))/len(df2)
ethereum_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
store.info()
twitter_archive=pd.read_csv('twitter-archive-enhanced.csv') $ tweet_id_all=twitter_archive['tweet_id']
df.head()
y = np.array(hours['Duration (ms)']) $ print(y)
ax=df_series.plot(title="Original Data") $ ax.set_ylabel("value") $ plt.show()
tvec = TfidfVectorizer(max_features = 2000,ngram_range=(1,3)) $ tvec.fit(X_train,y_train)
response_scoring = requests.post(scoring_url, json=payload_scoring, headers=header) $ print(response_scoring.text)
print(metrics.f1_score(testy, sgd_predict)) $ print(metrics.recall_score(testy, sgd_predict))
bacteria[bacteria.index.str.endswith('bacteria')]
grouped = writers.groupby(lambda x: age_groups(writers,x,'Age')) $ grouped.groups
print("Percentage of positive Federer's tweets: {}%".format(len(pos_tweets_rf)*100/len(data_rf['Tweets']))) $ print("Percentage of neutral Federer's tweets: {}%".format(len(neu_tweets_rf)*100/len(data_rf['Tweets']))) $ print("Percentage de negative Federer's tweets: {}%".format(len(neg_tweets_rf)*100/len(data_rf['Tweets'])))
df = pd.read_csv('Sales-2017-12.csv')
for file in files: $     temp = pd.read_csv('{fname}'.format(fname=file)) $     data = data.append(temp) $     print('Processed {fname}'.format(fname=file))
data.describe()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
join_a = join_a.na.fill('a',["party_name_orig","party_name","address1_orig","address1","street_name_orig","street_name"])
now = datetime.datetime.now() $ now $ then = datetime.datetime.now() - timedelta(days=4) $ then $ print(then.date())
articles['updated_at'] = articles.apply((lambda row: dateparser.parse(row.date[1]) if len(row.date) > 1 else None), axis=1)
def explore_topic(topic_number, topn=25): $     print u'{:20} {}'.format(u'term', u'frequency') + u'\n' $     for term, frequency in lda.show_topic(topic_number, topn=25): $         print u'{:20} {:.3f}'.format(term, round(frequency, 3))
shows.info(memory_usage='deep')
df2 = df2[df2['timestamp'] != "2017-01-14 02:55:59.590927"] $ sum(df2['user_id'].duplicated())
df2['new_sales_perc'] = df2.new_sales.divide(df2.sales) $ df2.head()
neg_words =  lmdict.loc[lmdict.Negative != 0, 'Word'].str.lower().unique() $ pos_words =  lmdict.loc[lmdict.Positive != 0, 'Word'].str.lower().unique()
def get_list_username(the_posts): $     list_username = [] $     for i in list_Media_ID: $         list_username.append(the_posts[i]['shortcode_media']['owner']['username']) $     return list_username
twitter_df_clean = twitter_df_clean.drop(['retweeted_status_id', 'retweeted_status_user_id','retweeted_status_timestamp'], axis=1)
df2=df2.drop('cntry_CA',axis=1)
df_ = pd.DataFrame(op_)
salary_df.head(10)
model = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = model.fit()
tweets_l_scrape = d_scrape['text'].tolist() # create a list from 'text' column in d dataframe $ print(tweets_l_scrape[-1:])
norm.ppf(1-(0.05/2))
commits = Query(git_index).get_cardinality("hash")\ $                           .since(start=start_date)\ $                           .until(end=end_date)\ $                           .by_period() $ print(get_timeseries(commits, dataframe=True).tail())
train_4_reduced.shape, y_train.shape
measurement_results = session.query(Measurements.station,Measurements.date,Measurements.prcp, Measurements.tobs).all() $
np.abs(-100)
df_final = df_sub.sort_values(by=['Prediction'],ascending=False) $ df_final.to_csv("result")
user_dropped = df2.user_id.duplicated().sum() $ print('There are {} duplicate rows in df2.'.format(user_dropped))
st_streams.close()
df2.drop_duplicates('user_id', inplace=True) $ df2[df2['user_id']==773192]
evaluator = full_text_classifier.evaluate(df_test) $ print(evaluator.get_metrics("ALL")) $ evaluator.plot_confusion_matrix() $ evaluator.get_metrics("macro_f1")
recommended_vids = yt.get_recommended_videos(vid)
(df2.query('landing_page == "new_page"')['converted'] >= 0).sum()/df2.shape[0]
score_data = x[int(0.9*samples_count): ] $ predictions = loadedModelArtifact.model_instance().predict(score_data)
!cat /notebooks/.log/20170704/20170704-071647-0970.log
loan_stats["revol_util"].head(rows=2)
test_sentence = test_sentence.replace(re.compile(r"http.?://[^\s]+[\s]?")) $ test_sentence[0]
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ r= requests.get(url) $
df['sentiment'] = df.cleaned_text.apply(lambda x: get_tweet_sentiment(' '.join(x)))
categories_feat = df.groupby(['business_id']).categories.apply(np.unique) $ categories_feat.head()
print(train_data.notRepairedDamage.isnull().sum()) $ print(test_data.notRepairedDamage.isnull().sum())
baseball_h.index.is_unique
df_true.head(10)
plt.figure(figsize=(20,20)) $ sns.heatmap(df.corr(),annot=True)      # looking at the variables r/ship
df.head()
zipcodes_listings = listings['mainzip'].value_counts() $ zipcodes_listings.plot()
df_unit_after.head()
print("Predicting...") $ sub['is_attributed'] = model_lgb.predict(test_df[predictors]) $ print("Writing...") $ sub.to_csv('submission.csv.gz',index=False,compression='gzip') $ print("Done...")
req.text
labels = train.trips $ train = train.drop(['trips', 'date'], 1)
gcv.best_params_
INC.head()
posts.plot(kind='scatter',x='postsCount',y='a')
df5 = pd.concat([df3, Senate], axis=1, join='inner') $ df5.columns = ['tweet_sentiment', 'Senate_sentiment']
a = np.arange(10,20) $ b = np.linspace(1,7,10) $ print("a: ", a) $ print("b: ", b)
hsi["Date"] = pd.to_datetime(hsi.Date, format="%b %d, %Y", errors='ignore') $ hsi = hsi.set_index("Date")
plt.figure(figsize=(8, 5)) $ plt.scatter(train_df.favs_lognorm, train_df.views); $ plt.title('The distribution of the favs_lognorm and number of the views'); $
from datetime import datetime $ datetime.fromtimestamp(1382189138.4196026).strftime('%Y-%m-%d %H:%M:%S')
pred.p2_dog.value_counts()
prob_rf200 = rf200.predict_proba(Test) $ prob1_rf200 = pd.Series(x[1] for x in prob_rf200) $ Results_rf200 = pd.DataFrame({'ID': Test.index, 'Approved': prob1_rf200}) $ Results_rf200 = Results_rf200[['ID', 'Approved']] $ Results_rf200.head()
from statsmodels.stats.diagnostic import acorr_ljungbox $
t2.tail(10)
dsdf = pd.DataFrame.from_records(ds)
not_needed = ['p1_conf', 'p2_conf', 'p3_conf'] $ tweets_clean.drop(columns = not_needed, inplace = True) $ tweets_clean.head()
list(itertools.combinations('ABCD', 2))
5/9*(5.88)
dfChile = dfChile[dfChile["longitude"] < -66.000000]
df_new2['intercept'] = 1 $ df_new2.head()
tweets = pd.DataFrame(ndarray)
df.head(5)
import numpy as np $ X_feat = np.zeros((len(text_tw), 300)) $ X_feat[:,:] = lines $ X_tweets = text_tw
import statsmodels.api as sm $ convert_old = conv_giv_ctrl $ convert_new = conv_giv_trt $ n_old = n_old $ n_new = n_old
params = {'figure.figsize': [4,3],'axes.grid.axis': 'both', 'axes.grid': True, 'axes.labelsize': 'Medium', 'font.size': 12.0, \ $ 'lines.linewidth': 2} $ plot_partial_autocorrelation(series=RNPA_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of RNPA hours new patients')) $ plot_partial_autocorrelation(series=RNPA_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of RNPA hours existing patients'))
for col in ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']: $     df_goog[col].plot() $     plt.show()
df_episodes = lds.dataframes['simpsons_episodes'] $ len(df_episodes)
print(results.summary())
df_A.groupby('Gender').mean()
file = 'https://assets.datacamp.com/production/course_2023/datasets/airquality.csv' $ airquality = pd.read_csv(file) $ airquality.info()
github_data = github_data[github_data.user_year != '2018'] $ github_data = github_data[github_data.project_year != '2018'] $ github_data.reset_index(inplace=True, drop=True)
p.to_csv('/home/student/columns66.csv')
d = datetime.datetime.now()
hdf5_file = h5py.File(refl_filename,'r') $ hdf5_file
df_pol_d=df_pol_d.drop('text',axis=1)
import pandas as pd $ test = pd.read_csv("datatestSinNan.csv")
plt.hist(p_diffs_alt) $ plt.grid() $ plt.axvline(p_diffs_alt.mean(), color='r', label='mean') $ plt.legend();
cbg = cbg.to_crs({'init':'epsg:4269'})
autos = autos.drop(["seller", "offer_type"], axis=1)
temp_mean.plot(kind='bar', figsize=(12, 6), fontsize='large', $                colormap='tab10', $                title='24-hr Mean Temperatures (deg C)');
dftopcomplaint = dfjoined.groupby('created_date').first().reset_index()
movies.dtypes
df_twitter_archive_master.drop(['Unnamed: 0'],axis=1,inplace=True)
df[df.handle == 'Jim Cramer'].head()
station = pd.read_csv('station.csv', sep=',', parse_dates=['installation_date'], $                       infer_datetime_format=True,low_memory=False)
autos.columns = columns
path = '/Users/atharvabhandarkar/Downloads/citi_trips_sep2017.csv' $
for col in time_vals: $     flights.loc[:, col] = flights.loc[:, col].apply(lambda x: time_to_str(x)) $ flights[time_vals].head()
x_train.head() $
df2['ab_page_US'] = df2['ab_page']*df2['US'] $ df2['ab_page_UK'] = df2['ab_page']*df2['UK'] $ df2.head()
url = form_url(f'organizations/{org_id}/teamTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
print 'After dropping the anonymous donor, total amounts from the unknown state as a percentage of all amounts is: '\ $     , thousands_sep(100*df[(df.state == 'YY')].amount.sum()/df.amount.sum()), '%'
df_input = df.select(["Unique Key", "Created Date", "Closed Date", "Agency", "Complaint Type", "Status", "Borough", \ $                       "Park Borough", "Resp_time", "HOD"])
data = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True) $ data.head()
df_link_meta.head(3)
round(df2.query('landing_page=="new_page"').shape[0] / df2.shape[0], 4)
df_payments_totals.head()
(autos["date_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
df_ud208 = pd.read_sql(sql_ud208,conn_hardy) $ df_ud208.groupby(['course_enrolled','course_enrollment','converted'])['applicant_id'].count()
sample_df['first_genre'].unique()
df = pd.concat([df, sentiments_df], axis=1) $ df.to_csv('file_output\\news_mood.csv') $ df.head()
converted_users_count = df[df['converted'] == 1].user_id.count() $ print(converted_users_count) $ (converted_users_count / unique_users_count) * 100
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ p_diffs = np.array(diff1) $
lang_detection(tweet) $ lang_occur = pd.DataFrame() $ lang_occur['language'] = languages
lbs = [x for x in range(5, 15, 5)] $ mws = (np.array(lbs) / 100).tolist() $ for i, l in enumerate(lbs): $     for j, w in enumerate(mws): $         print(i, j)
df.to_csv('data/usnpl_newspapers.csv', index=False)
proj_df = pd.read_csv('io/Projects.csv', engine='python')  # dataset available on kaggle/donorschoose
p_diffs = np.random.binomial(NewPage, p_new, 10000)/NewPage - np.random.binomial(OldPage, p_old, 10000)/OldPage
transactions.merge(users,how="inner",on="UserID")
closep=np.array(data_dict['Close'],dtype='float64')
all_colnames = [clean_string(colname) for colname in df_total.columns] $ df_total.columns = all_colnames $ df_total.head()
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
pyLDAvis.enable_notebook() $ vis = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary) $
sample_weight = np.array([1 if not p else 1.25 for p in joined_train.perishable])
dates=[tweet.created_at for tweet in all_tweets] $ dates[:5]
for column in has_stage_archive.iloc[:,8:12].columns: $     avg_retweet_count = has_stage_archive[has_stage_archive[column] == 1]['retweet_count'].mean() $     print('The average retweet count for {}s is {}'.format(column, avg_retweet_count))
df.columns
from sklearn.linear_model import LinearRegression $ lr = LinearRegression() $ lr.fit(new_array, y_train) $ print(lr.score(new_array, y_train))
cont_probability = df2[df2['group'] == 'control']['converted'].mean() $ cont_probability
data['Population']  #data.Population works too
hc.sql('create table asm_wspace.usage_400hz_2017_Q4 as select * from tmp_400hz_usage')
df2 = df1.loc[lambda x: pd.notnull(x['Embarked'])] $ df2.shape
df = pd.read_sql_query('show tables', engine)
final_topbikes.groupby(by=final_topbikes.index.weekday)['Distance'].median().plot(kind='bar', figsize=(12,6))
start = df['timestamp'].min() $ end = df['timestamp'].max() $ print('Our experiment started on the {} and has been ended on {}'.format(start,end))
result_df=pd.merge(df2,countries,on='user_id') $ result_df.head()
converted_users = df[df.converted == 1]['converted'].count() $ prop_conv = converted_users/unique_users $ prop_conv
autos["date_crawled"].str[:10].value_counts(normalize=True, dropna=False).sort_index().plot(kind="bar", title="Date_crawled", colormap="Blues_r")
fig, ax = plt.subplots(1, figsize=(12,4)) $ plot_with_moving_average(ax, 'MA Doctors', doc_duration, window=16) $ fig.savefig('./images/dr_MA16.png')
pd.io.json.json_normalize(playlist['tracks']['items'][2]['track'])
df2[df2['user_id'].duplicated() == True]
df['Complaint Type'].groupby(by=df.index.month).count().plot()
df.set_index('Timestamp', inplace=True)
print('US is having {:0.4f} times more converted users than UK, all other features held constant.'.format(1/np.exp(-0.0408)))
lr = LinearRegression() $ cv_score = cross_val_score(lr, features_regress_vect, overdue_duration, $                            scoring='neg_median_absolute_error', cv=5) $ 'MedianAE score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
condos.info()
d + pd.tseries.offsets.Week(weekday=4)
people["pets"] = pd.Series({"bob": 0, "charles": 5, "eugene":1})  # alice is missing, eugene is ignored $ people
m_grid, c_grid = np.meshgrid(m_vals, c_vals)
df_by_donor.rename(columns = {'Donor ID': 'Donor ID', 'Donation Amount min': 'Minimum Donation', 'Donation Amount max': 'Maximum Donation', 'Donation Amount mean': 'Mean Donation', 'Donation Amount sum': 'Total Donations', 'Donation ID count': 'Count of Donations by Donor', 'Donation Received Date max': 'Most Recent Donation'}, inplace=True) $ print(df_by_donor.head())
fpath = 'twitter_marketplace_data.csv' $ raw_data = pd.read_csv( $     fpath, $     parse_dates=[1] $ )
recommend["answer"] = pd.to_numeric(recommend["answer"]) # Converts the Series to `as_numeric`
new_page_converted = np.random.choice([1, 0], size = nnew, p = [p_mean, (1-p_mean)]) $ print(len(new_page_converted))
df1.resample('D', how='ohlc')
from helper_code import mlplots as ml $ ml.confusion(l_test.reshape(l_test.shape[0]), $              y_pred, labels, 3, $              'Random Forest Classification')
full_orig['MonthAdmit'] = full_orig['AdmitDate'].apply(lambda x: x.month)
park.loc[park.named_fda_drug.notnull()][:5]
cols_to_drop = const_cols + col_to_remove + ["sessionId"] + ["fullVisitorId"] $ cols_to_drop = [x for x in cols_to_drop if x not in ["totals.transactionRevenue","date"]] $ cols_to_drop_test = [x for x in cols_to_drop if x not in ["trafficSource.campaignCode","totals.transactionRevenue","date"]] $ train_data = train_data.drop(cols_to_drop, axis=1) $ test_data = test_data.drop(cols_to_drop_test, axis=1)
X_d.shape
x = x.assign(A2 = x["A"]**2) $ x
np.std(pipe.tracking_error)
stat_info = stations['name'].apply(fix_comma) $ print(stat_info)
output_folder = '../data/sync/'+experiment_name+'/schedule_event/' $ checkDirectory(output_folder) $ event_schedule.to_csv(output_folder+'schedule_event_'+experiment_name+'_.csv',index=False)
df3.join(highly_delayed.head(5), on='Origin', how='inner')['Carrier'].unique()
twitter_master2 =  pd.merge(twitter_master1, twitter_master, left_on='tweet_id', right_on='tweet_id')
p + pd.tseries.offsets.timedelta(minutes=120)
query.first()
uniqueArtists = userArtistDF.select("artistID").distinct().count() $ print("Total n. of distinct artists: ", uniqueArtists) $
htmldmh = dmh.HTML('https://www.fdic.gov/bank/' +         $                    'individual/failed/banklist.html', index=0)
products.head(2) 
train_orig=train.copy() $ test_orig=test.copy()
print(df_users.shape) $ df_users.isnull().sum()
pandas_df['tags'] = [x.split('><') for x in pandas_df['tags']]
data['comments'] = data['comments'].astype(int)
train.sample(5)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller' ) $ z_score, p_value
df[df2.columns[df2.columns.str.upper().str.contains('ORIGIN')]].sample(5)
stringlike_instance.content
pop_con = len(df2[df2.group == "control"]) $ pop_con
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA']]) $ results = logit_mod.fit() $ results.summary()
list_of_id_num_sites_not_done = df_sites_not_yet_done.index $ print('There are',len(list_of_id_num_sites_not_done),'sites not yet done') $ print('The first 10 are:',[list_of_id_num_sites_not_done[i] for i in range(10)])
(lv_workspace.get_filtered_data(step = 0)[['DIN', 'SALT_BTL']].dropna().count() > 0).all()
log_model_uk = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','UK']]) $ result = log_model_uk.fit() $ result.summary()
df[df['Descriptor'] == 'Pothole']['Unique Key'].groupby(df[df['Descriptor'] == 'Pothole'].index.hour).count().plot()
first_comb.head(5)
df_person.merge(df_grades, how="outer", left_on = "id", right_on="person_id")
Pold=df2['converted'].mean() $ Pold
days_alive / pd.Timedelta(nanoseconds=1)
from sklearn.model_selection import train_test_split as tts
client.experiments.get_status(experiment_run_uid)['state']
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
log_reg_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'country_CA', 'country_UK']]) $ results = log_reg_mod.fit() $ results.summary()
shopping_carts.ndim
compared_resuts = ka.predict(test_data, results, 'Logit') $ compared_resuts = Series(compared_resuts)  # convert our model to a series for easy output
genes = pd.concat( [ LT906474,CP020543 ] )
pwd = os.getcwd() $ df_users = pd.read_csv( pwd+'/users.052317.csv', encoding='utf-8') #user, need to find a way to link them, since it is only individual record. $
train.head(1)
tweet_df.head()
la_tz = pytz.timezone('America/Los_Angeles') $ la_dt = la_tz.normalize(utc_dt) $ la_dt
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_713.values.squeeze(), lags=40, ax=ax1)
print(soup.prettify()[28700:30500])
tweetsIn22Mar.index = pd.to_datetime(tweetsIn22Mar['created_at'], utc=True) $ tweetsIn1Apr.index = pd.to_datetime(tweetsIn1Apr['created_at'], utc=True) $ tweetsIn2Apr.index = pd.to_datetime(tweetsIn2Apr['created_at'], utc=True)
from gensim import models $ models = models.Word2Vec.load_word2vec_format((r'C:\Users\User\Desktop\670\7_Topic_Modeling\data\text8.csv'))
plantlist_uba = getubalist(url_uba) $ plantlist_uba.head()
march_2016.end_time
blacklist = ["#rstats", "rstats", "Python", "Datasets", "GANs", "AI", "Causal Models", "Thoughts"] $ sub_headline_date_count = sub_headline_date_count[ $     (sub_headline_date_count > 1) & ([ $        x not in blacklist for x in sub_headline_date_count.index.get_level_values(1).tolist()])] $ sub_headline_date_count
norm.ppf(1-(0.05/2))
logit_mod2=sm.Logit(df_new['converted'],df_new[['ab_page','intercept','CA','UK']]) $ fit2=logit_mod2.fit() $ fit2.summary()
df.apply(lambda x: x.mean() - x.max())
data.dtypes
raw.columns
prod_ch.input_production.max()
open_price = [x[1] for x in data2 if x[1]!=None]  # needs to exclude None. $ print('The highest opening price during 2017 is : ',max(open_price)) $ print('The lowest opening price during 2017 is : ',min(open_price))
S.decision_obj.stomResist.value = 'Jarvis' $ S.decision_obj.stomResist.value
df_inner_users_sess = pd.merge(users,sessions,how='left',on='UserID') $ df_inner_users_sess[df_inner_users_sess['Registered']== df_inner_users_sess['SessionDate'] ].reset_index().drop('index',axis=1)
df.loc[7,:] = ('Moore', 'Andrew') $ df
store_items = store_items.rename(index={'store 3': 'fun'}) $ store_items
monte.str[0:3]
X = pivoted.fillna(0).T.values $ X.shape
students.loc['Ryan']
i=random.randrange(len(train_x)) $ print_query(i)
user._json
from sklearn.cross_validation import cross_val_score $ scores = cross_val_score(logit, X_train, y_train, cv=10, scoring='neg_log_loss')
df_ratings.drop('video_watched_type', axis=1, inplace=True) $ df_ratings.drop('rating_date_creation', axis=1, inplace=True) $ df_ratings
f, ax = plt.subplots(figsize=(15, 5)) $ sns.stripplot(data=samp311, x="complaint_type",y="agency", hue='borough',size = 10);
xgb = XGBClassifier() $ xgb.fit(X_train, y_train)
np.exp(-2.0300), np.exp(0.0506), np.exp(0.0408)
ab_file2 = pd.read_csv('ab_edited.csv')
df.axes
total_unique_conv = df.query('converted == "1"').user_id.nunique(); $ total_unique_conv
train_size = 11538 #use 80% of data for training $ train_df = tweets_df[:train_size].copy() $ test_df = tweets_df[train_size:].copy()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
save_model('model_gbdt_v1.mod', gbdt_grid)    
df_boost_csv = df_boost.to_csv('model1.csv') 
individuals_metadata_df.to_csv("output/individuals_{}.tsv".format(date), index=False, sep="\t", na_rep="NA")
print('Examples of tweets (with only text and hashtag column):') $ tw6[['text', 'hashtag']].head(3)
from sklearn.svm import SVC
df3 = pd.DataFrame({'key': np.repeat(1, users.shape[0]), 'UserID': users.UserID}) $ df4 = pd.DataFrame({'key': np.repeat(1, products.shape[0]), 'ProductID': products.ProductID}) $ pd.merge(df3, df4,on='key')[['UserID', 'ProductID']]
df.resample('M').count().plot(y="Unique Key")
Y, X = dmatrices('Y ~ X', data=df2) $ mod3 = sm.OLS(Y, X) $ res3 = mod3.fit() $ res3.summary2()
jobPostDFSample = jobPostDFSample.drop("index", axis=1)
dup_id = df2[df2.duplicated(['user_id'], keep=False)]['user_id'].reset_index()
df1 = pd.DataFrame(np.random.randn(6,3),columns=['col1','col2','col3']) $ df2 = pd.DataFrame(np.random.randn(2,3),columns=['col1','col2','col3']) $ print(df2.reindex_like(df1))
columns = list(df_all_wells_wKNN.columns.values)
uniqID = df['user_id'].nunique() $ print('The data set contains ' + str(uniqID) + ' unique users')
coinbase_btc_eur_min.plot(kind='line',x='Timestamp',y='Coin_price_EUR', grid=True);
avg_day_of_month15 = avg_day_of_month15.rename(columns={"day_of_year":"rides"}) $ avg_day_of_month15.head()
for col in data: $     print col $     print data[col].value_counts() $
jimcramer = miner.mine_user_tweets(user='jimcramer', max_pages=10)
my_person.firstname
pres_df['ad_length_tmp'] = pres_df['end_time'] - pres_df['start_time'] $ pres_df['ad_length_tmp'].head(10)
CRold = df2.converted.sum()/df2.count()[0] $ CRold
(grades == 10).any(axis = 1)
tag_df.stack().head()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]); $ results2 = logit_mod.fit() $ results2.summary()
store_items = store_items.rename(index={'store 3': 'last store'}) $ store_items
data.to_pickle("experiment/2_post_corpus.p")
data.tasker_id.value_counts().head()
df_vow['Volume'].unique()
monte.str.startswith('T')
pattern = re.compile('AA') $ print(pattern.findall('AAbcAA')) $ print(pattern.findall('bcAA'))
for col in ["campaign_spend", "campaign_budget", "bid", "charged"]: $     print "{0: <20}{1}".format(col, raw_data[col].map(lambda x: x < 0).any())
AAPL.tail(3)
kick_projects['goal_cat_perc'] =  kick_projects.groupby(['category'])['goal'].transform( $                      lambda x: pd.qcut(x, [0, .35, .70, 1.0], labels =[1,2,3]))
targetUserItemInt=targetUserItemInt.join(userData.set_index(['id'])['premium'],on='user_id',how='left') $ print targetUserItemInt.shape $ targetUserItemInt.head()
rng = pd.date_range(start = '6/1/2017', end = '6/30/2017', freq = 'B') $ rng
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
result=logit.fit()
poverty_data_columns=poverty_data.columns.tolist()
prec_us_full = prec_nc.variables['pr_wtr'][:, lat_li:lat_ui, lon_li:lon_ui]
m.fit(lr, 3, metrics=[exp_rmspe], cycle_len=1)
df2 = df[((df["landing_page"] == "new_page") & (df["group"] == "treatment")) $          | ((df["landing_page"] == "old_page") & (df["group"] == "control"))] $ df2.count()
s.loc['c'] $
df_pol[(df_pol['domain'] == 'nationalreview.com' )]
df.mean()
import warnings $ warnings.simplefilter("ignore", UserWarning) $ warnings.simplefilter("ignore", FutureWarning)
data_issues_csv.head()
top_apps = df_nona.groupby('app_id').accounts_provisioned.sum() $ top_apps.sort(inplace=True, ascending=False) $ top_apps.head(10)
merged1.drop('Id', axis=1, inplace=True)
np.sqrt(grades)
percentage = nypd_unspecified/nypd_complaints_total*100 $ print("%1.2f"%percentage)
trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt) $ val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt) $ md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)
from sklearn.ensemble import RandomForestClassifier $ random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)
print('Shape of the dataset after transformation:', str(filtered_file.shape), 'Out of:', str(master_file.shape), '\n') $ print(filtered_file.iloc[:, 28:-1].axes[1], '\n') $ print(filtered_file['TECTONIC SETTING'].value_counts())
snow.upload_dataframe(als_ref, "nk_als_ref")
df = df[df['duration'].astype('timedelta64[h]') < 100]
two_day_sample.head()
xmlData = pd.DataFrame(recordSeries)
file_dict = {} $ for file in files: $     file = file $     num_files = len(glob.glob('{}/*'.format(file))) $     file_dict[file] = num_files
h5py.File?
plt.figure(figsize=(15,15)) $ nx.draw_networkx(G2, with_labels=True, node_size=10)
autos["price"].head()
s = so['commentcount'] $ s.head()
df_tweet.duplicated().sum()
df2[['old_page','new_page']] = pd.get_dummies(df2['landing_page']) $ df2[['treatment', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('new_page', axis=1) $ df2 = df2.drop('treatment', axis=1)
exiftool -csv -createdate -modifydate ciscij10/CISCIJ10_cycle2.mp4 ciscij10/CISCIJ10_cycle3.mp4 ciscij10/CISCIJ10_cycle4.mp4 ciscij10/CISCIJ10_cycle5.mp4 ciscij10/CISCIJ10_cycle6.mp4 > ciscij10.csv
dictionary $ shape = [len(a) for a in dictionary.values()]
future_pd_30360_origpd_all[(future_pd_30360_origpd_all.fk_loan==574) & (future_pd_30360_origpd_all.fk_user_investor==31192)].to_clipboard()
for dataset in combine: $     dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 $ train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)
df4 = df2.set_index("user_id").join(df3.set_index("user_id"))
df["Date"] = pd.to_datetime(df.Date)
import pickle $ data = [1, 1.2, 'a', 'b'] $ with open(outputs / 'data.pkl', 'wb') as f: $     pickle.dump(data, f)
exploration_airbnb.print_infos('consistency')
y = df2.converted $ X = df2[['intercept', 'ab_page']]
results = session.query(measurement.station, measurement.tobs).\ $     filter(measurement.station == 'USC00519281').\ $     filter(measurement.date >= prev_year).all()
X = X.dropna()
dfMonth = dfMonth.rename(columns = {'Weighted Value': 'Weighted Value (Monthly)', $                                  'Contract Value (Daily)': 'Contract Value (Monthly)'})
df = pd.read_sql_query(q , conn) $ df # giving error $
df.groupby('main_category')['ID'].count()
image_df_clean.sample(10)
result = pd.DataFrame(eclf1.predict_proba(test[features]), index=test.index, columns=eclf1.classes_) $ result.insert(0, 'ID', mid) $ result.to_csv("gaurav7.csv", index=False)
sales.head(1)
my_df.columns = ['text', 'polarity'] $ my_df.head()
observations.to_csv('datasample.csv')
features.shape
data4.crs= "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
old_page_converted = np.random.choice([0, 1], $                                       p=[1. - p_convert_control, p_convert_control], $                                       size=n_control, $                                       replace=True) $ old_page_converted.mean()
def _weekday(date): $     return datetime.strptime(date, DATETIME_PARSE_PATTERN).weekday() $ weekday2 = udf(_weekday, IntegerType())
dfsunrise.loc['2017-05-17': '2017-05-20']
fig,ax=plt.subplots(1,2,figsize=(15,3)) $ ax[0].boxplot(joined['CompetitionOpenSinceYear'],vert=False) $ ax[1].boxplot(joined['CompetitionDaysOpen'],vert=False)
stories[['created_hour', 'score']].corr()
twitter_archive_master.p1.value_counts()
pd.read_sql("SELECT dest from related_content where content_id=696", conn).iloc[0]['dest']
print('There are {:.0f} visa applications in this dataset.'.format(df_data.shape[0]))
tweet_json = pd.read_json('tweet_json.txt',lines= True) $ tweet_archive_enhanced = pd.read_csv('twitter-archive-enhanced.csv') $ tweet_image_predictions =  pd.read_csv('image_predictions.tsv',sep = "\t")
new_cases=dropped.loc['Total new cases registered so far'] $ new_cases.head()
1/np.exp(-0.0149), 1/np.exp(-0.0408), np.exp(0.0099)
date_news['month'] = date_news.dates.dt.month $ date_news['year'] = date_news.dates.dt.year $ two16 = date_news.loc[date_news.year == 2016] $ date_news.groupby('year').count()
y_hat = map_estimate['alpha_interval__'] + map_estimate['betas_race_interval__'].reshape((-1,1)).T.dot(X[:,1:5].T) +\ $ map_estimate['beta_income_interval__'].reshape((-1,1)).T.dot(X[:,5:6].T)+ \ $ map_estimate['beta_complaint_interval__'].reshape((-1,1)).T.dot(X[:,6:7].T) $ plt.scatter(y_hat.T,y) $ plt.show()
powerlifting_meets = pd.read_csv("../data/powerlifting-database/meets.csv") $ powerlifting_competitors = pd.read_csv("../data/powerlifting-database/openpowerlifting.csv") $ powerlifting_meets.set_index('MeetID').join(powerlifting_competitors.set_index('MeetID'))
grinter1 = inter1.groupby(['user_id','item_id']).size().reset_index() $ grinter1.columns = ['user_id','item_id','number_int'] $ grinter1.shape
apple_ticker = 'AAPL' $ project_helper.plot_stock(close[apple_ticker], '{} Stock'.format(apple_ticker))
df.info()
jobs.loc[(jobs.FAIRSHARE == 180) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
new_page = df2.query('landing_page=="new_page"') $ prob_new_page = new_page.shape[0]/df2.shape[0] $ prob_new_page
df_c = pd.read_csv('countries.csv')
y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled) $ accuracy_score(y_test, y_pred)
injury_df['team_id'] = injury_df['Team'].apply(lambda x: team_slugs_dict[process.extract(x, team_slugs_dict.keys(), limit=1)[0][0]])
pd.to_datetime(1)
from io import StringIO $ trains = pd.read_table(StringIO(data_string)) $ trains['size'] = pd.cut(trains['weight_tons'], 3, labels=['Small','Medium','Big']) $ trains
import matplotlib.pyplot as plt
station_distance.tail() $ station_distance.shape
country_dummies = pd.get_dummies(df_countries['country']) $ df_new = df_countries.join(country_dummies)
grp = company.groupby('Company') $ grp.size()
from sklearn.metrics import mean_squared_error $ ac_predictions = lin_reg.predict(ac_tr_prepared) $ lin_mse = mean_squared_error(ac_tr_label, ac_predictions) $ lin_rmse = np.sqrt(lin_mse) $ lin_rmse
clean_rates.source.value_counts()
lr.drop('control',axis=1,inplace=True)
movies.to_pickle('../../data/processed/movies.pkl')#stored the data to a pickle file
recipes.head()
len(df.iloc[len(df) - len(df_test) : len(df)+1])
url = form_url(f'organizations/{org_id}/members', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
userModel.summary()
noaa_data.head()
h2o.save_model(aml.leader)
company_count = {} $ for item in twitter_data.itertuples(): $     for company in item[1]: $         company_count[company] = company_count.get(company,0)+1
filtered_df.shape
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=00) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df2 = pd.read_csv("../../data/msft.csv",usecols=['Date','Close'],index_col=['Date']) $ df2.head()
pd.date_range(start = '08/18/2017', end = '08/25/2017', freq = myc2)
df2.head()
df = df.loc[df['offerType'] == 'Angebot'] $ df = df.drop('offerType',axis=1) $ df.shape
tweet._json['entities']["media"][0]["type"]
first_commit_timestamp = git_log.timestamp.iloc[-1] $ last_commit_timestamp = pd.to_datetime('today') $ corrected_log = git_log[(git_log.timestamp>=first_commit_timestamp) & (git_log.timestamp<=last_commit_timestamp)] $ corrected_log.timestamp.describe()
df.drop(['YR_RMDL', 'ROOMS'], axis=1, inplace=True)
df_h1b_nyc.pw_1.describe()
unique = df2.user_id.nunique() $ print("{} unique 'user_id' could be found in the dataset 'df2'.".format(unique))
transformed.shape
df_new['UK_ind_ab_page'] = df_new['UK']*df_new['ab_page']#Creating new dummy variables by multiplying each country and ab_page columns $ df_new['US_ind_ab_page'] = df_new['US']*df_new['ab_page'] $ Log_mod = sm.Logit(df_new['converted'],df_new[['intercept','US_ind_ab_page','UK_ind_ab_page']]) $ results = Log_mod.fit() $ print(results.summary())
tmp.user
import pandas as pd $ tweets = pd.read_csv ("./twitter.csv", header=None) $ tweets.head()
price = autos['price'] $ ax = sns.boxplot(y= price)
word_count = df.groupby(['subreddit'])['word_count'].sum()
df.sort_values('date', inplace=True) $ df.head()
sourcedict = {'source':source, 'system id': systemid, 'system':system, 'pls':pls, 'source type': sourcetype, $           'source use':sourceuse, 'win':win, 'wrnum':wrnum, 'DEHN source id':sourcecode, 'source id':sourceid} $ sources = pd.DataFrame(sourcedict)
autos['odometer_km'].describe()
pd.value_counts(ac['Bank'].values, sort=True, ascending=False) $
print('Get subset data of rows excluding rows no. 5, 12, 23, and 56') $ lst_exclude = [5, 12, 23, 56] $ df[~df.index.isin(lst_exclude)].head(15)
test = contest_data_sample.sample(False,0.002).select('end_customer_party_ssot_party_id_int_sav_party_id','sales_acct_id','decision_date_time').take(1) #Randomly picking an entry $ test[0]
data.tail()
committees_NNN = committees[committees.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
mean_price_serie = pd.Series(mean_price_by_brand) $ mean_mileage_serie = pd.Series(mean_mileage_by_brand) $ mean_df = pd.DataFrame(mean_price_serie, columns=['mean_price']) $ mean_df["mean_mileage"] = mean_mileage_serie $ mean_df
all_df.shape, cand_date_df.shape
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
soup.li.parent.name
brands = autos['brand'].value_counts(normalize = True) $ most_common_brands = brands[brands > 0.05].index $ most_common_brands
coins_top10 = mcap_mat.iloc[-2].sort_values(ascending=False).head(10).index.tolist() $ coins_top10
fig = plt.subplots(1,1,figsize=(14,8)) $ plt.plot(df.index,df['mood']) $ plt.title('The Mood of the President',fontsize=14);
jobs_data['clean_titles'].head(5)
optimizer_col= optim.Adam(model.parameters(),lr=0.001) $ lr_scheduler_col = lr_sched.myCosineAnnealingLR(optimizer_col,6297,cycle_mult=1.0) $ train_col.train_model(num_epochs=3,optimizer=optimizer_col,scheduler=lr_scheduler_col)
tweets_clean.info()
for ratingValue in value: $     filtereddf = ratings.loc[lambda x: x['rating'] == ratingValue] $     sampledf=filtereddf.sample(sampleSize) $     RatingSampledf=pd.concat([RatingSampledf, sampledf],axis=0) $ print("Sample created")
bathrooms = train_df['bathrooms'].value_counts() $ x = bathrooms.index $ y = bathrooms.values $ sns.barplot(x, y )
image_path2 = "daffodils-2162825_1280.jpg" $ img2 = pil_image.open(image_path2) $ img2 = img2.convert('RGB')
data.last_trip_date = pd.to_datetime(data.last_trip_date) $ data.signup_date = pd.to_datetime(data.signup_date)
recipes['ingredients'].str.len().describe()
df.loc[df['offerType'] != 'Angebot'].shape
intervention_test.dtypes
color = np.dtype([("r", np.ubyte, 1), $                   ("g", np.ubyte, 1), $                   ("b", np.ubyte, 1), $                   ("a", np.ubyte, 1)])
np.exp(-0.0150)  #ab_page coefficient = -0.0150
e = abs(y_test_pred - y_test) $ for th in [1,2,3]: $     e_ = e[e <= th] $     print('Accuracy for error less or equal to {} year: {}'.format(th,np.round(len(e_)/len(e),2)))
df2_treatment = df2.query('group == "treatment"') $ df2_treatment['converted'].mean()
df[df.Miles > 0].Day_of_week.value_counts().plot(kind='bar')
model.add(Dense(16, input_shape=(4,))) $ model.add(Activation('sigmoid'))
a1 = np.array([1, 2, 3, 4]) $ a2 = np.array([4, 3, 2, 1]) $ a1 + a2
breed_predict_df = pd.read_csv('image_predictions.tsv', sep = '\t') $
print('The repeated user_id in df2 is {}.'.format(df2[df2.user_id.duplicated()].user_id.iloc[0]))
sess_rm = sess.rolling(window=50, center=True).mean() $ sess_rm.head(10) $ sess_rm.plot()
import statsmodels.api as sm $ convert_old = sum((df2.group == 'control')&(df2.converted == 1)) $ convert_new = sum((df2.group == 'treatment')&(df2.converted == 1)) $ n_old = sum(df2.landing_page == 'old_page') $ n_new = sum(df2.landing_page == 'new_page')
encoded_df = pd.get_dummies(df, columns=['category', 'fileType']) $ encoded_df.shape
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['closTopBotDist'] = np.where(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell']<=df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'], df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromTopWell'], df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['FromBotWell'])
cohorts_unstacked = cohorts['TotalUsers'].unstack(0).head(10) $ cohorts_unstacked_nonblank = cohorts_unstacked.drop(cohorts_unstacked.columns[0], axis=1) $ cohorts_unstacked_nonblank
stories[['karma', 'score']].corr()
with open(outputs / 'data.pkl', 'rb') as f: $     new_data = pickle.load(f) $ print(new_data)
crimes=pd.read_csv('data/crime/crimes_chi.csv', parse_dates=True)
total_users = df.nunique()['user_id'] $ print("Number of unique users are : {}".format(total_users))
df1 = pd.DataFrame(last_year, columns=['date', 'precipitation'])
total_users = df2['user_id'].nunique() $ converted_users = df2[df2['converted'] == 1].count() $ conversion_prob = converted_users/total_users $ print(conversion_prob)
users[users.id == 9236]
vectorizer = CountVectorizer(stop_words=custom_stop_words) $ X = vectorizer.fit_transform(dfn['News'])
sale2_table = sale_prod_table.groupby(['Product', 'Country']).SalePrice.mean() $ sale2_table
df_all_users[df_all_users['Email Address'] == 'caro_bellemans@yahoo.com']
np.unique(train_small_sample.click_timeDay), np.unique(train_small_sample.click_timeHour)
log_reg_under = LogisticRegressionCV(Cs=[0.1, 0.6], scoring='neg_log_loss') $ log_reg_under.fit(X_train, y_train_under)
tweet_train_data = df_train['Tweet_text'].tolist() $ tweet_train_target = df_train['Attribute'].tolist() $ tweet_test_data = df_test['Tweet_text'].tolist() $ tweet_test_target = df_test['Attribute'].tolist()
respond = requests.get("https://en.wikipedia.org/wiki/List_of_best-selling_music_artists") $ soup = BeautifulSoup(respond.text) $ l = soup.find_all('tr') $ pprint(l[1].text)
print("The average daily trading volume is %3.3f"%(df["Traded Volume"].mean()))
def date_reformat(x): $     s = x.split() $     new = "%s-%s-%s" % (s[5], s[1], s[2]) $     return new $ QS = TwitterSQLService.QueryShell()   $
df.groupby("used_promo")["cancelled"].mean()
p_cont = df2.query('group=="control"').converted.mean() $ p_cont
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
for i in categories: $     df[i][df.project_subject_categories.str.contains(i)] = 1        
hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f') $ NAmer = hs.content['NAmer_dem_list.shp']
df_users_6_mvp['created_year']=df_users_6_mvp['created'].apply(lambda x:x.year) $ df_users_6_mvp['created_month']=df_users_6_mvp['created'].apply(lambda x:x.month) $ df_users_6_after_mvp['created_year']=df_users_6_after_mvp['created'].apply(lambda x:x.year) $ df_users_6_after_mvp['created_month']=df_users_6_after_mvp['created'].apply(lambda x:x.month)
df.size
print(r.text[0:500])
rain_df = pd.DataFrame(rain_result) $ rain_df.head() $ rain_df.set_index('date').head()
from sklearn.ensemble import RandomForestRegressor $ from sklearn.grid_search import GridSearchCV
index=pd.Index(np.arange(3)) $ index
import statsmodels.api as sm $ model=sm.Logit(df2['converted'],df2[['intercept','treatment']]) $ result=model.fit()
len(data_2012['date'].unique())
most_active = station_obs_df.loc[station_obs_df["Observation counts"].idxmax()] $ x = most_active["Station name"] $ y = most_active["Observation counts"] $ print(f"Station {x} had the most observation counts being {y}")
users["gender"].value_counts()
tweets_master_df.ix[774, 'text']
a = raw.values.reshape(1,-1)
display(heading('Good:'), $         sheets.keys() - sheets_with_bad_column_names.keys())
df = test_results_final["isLate", "prediction"].toPandas() $ df["dummy"] = 1 $ pd.pivot_table(df, index=["isLate", "prediction"], values="dummy", aggfunc="count")
table_store = data_from_store.ix[pns == pn].reset_index(drop=True)
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','ab_page']]) 
all_cards.describe()
details = details[~details['Popularity'].str.contains('E')]
pred_test=m.predict(True)
top_10_zipcodes_by_population_columns = ['incident_zip', 'population'] $ top_10_zipcodes_by_population = question_2_dataframe[top_10_zipcodes_by_population_columns].\ $     drop_duplicates().\ $     nlargest(10, 'population')['incident_zip'].values $ top_10_zipcodes_by_population
models_series = pd.Series(represented_models) $ models_series
flights2 = flights.set_index(["year", "month"])["passengers"] $ flights2.head()
twitter_archive_enhanced[twitter_archive_enhanced['name'].str.islower()].name.unique()
tweets = pd.read_csv('LaManada_new/tblposts.csv',sep=SEP) $ tweets.shape
mydata.iloc[0] 
ppdb = rdb.conn(driver=rdb.drivers()[8], $                 server='Freshdb03', $                 database='permission_pass_db', $                 uid='sa')
pn_o= new_page_converted.mean() - old_page_converted.mean() $ pn_o
sns.factorplot('sex', data=titanic3, hue='pclass', kind='count')
df.info()
plt.scatter(X2[:, 0], X2[:, 1], c=labels, cmap='rainbow') $ plt.colorbar();
time2 = '2017-04-05 00:00:00' $ df2 = df1.set_index(['id_container', 'time_in']) # , 'id_container', 'time_in', $ op_before = df2[df2['time_move']<time2] $ op_after = df2[df2['time_move']>time2] $ at_snapshot = op_before.index.intersection(op_after.index) $
c = pd.read_csv('countries.csv') $ c.head()
filter1 = train['PassengerId'] > 800 $ filter1.any()
fb.message = fb.message.fillna(fb.story)
blight_gb["long"] = pd.np.nan $ blight_gb["lat"] = pd.np.nan $
new_years_eve = dt.date(2016, 12, 31)
pd.isnull(pd.read_csv("Data/microbiome_missing.csv")).head(20)
random_integers = pd.DataFrame(random_integers, columns=['a', 'b', 'c', 'd'])
start = time.time() $ lrmodel = lr.fit(trainData) $ end = time.time() $ print(end - start)
subway2_df.columns = subway2_df.columns.str.strip()
df_vimeo_selected.columns
lassoreg = Lasso(alpha=0.01, normalize=True) $ lassoreg.fit(X_train, y_train) $ print(lassoreg.coef_)
a[a.find(':') + 1:].rstrip()
df.iloc[5]['Unnamed: 1']
df.listed.head()
df_drug_counts.AMANTADINE.plot(kind = 'bar', color = 'b')
df_clean.text.str.extract(pat='(\d+)/10')
test_movie = "sen to chihiro no kamikakushi" $ pwd = pairwise_distances(tfidf_matrix, tfidf_matrix[title_list_lower.index(test_movie)].reshape(1,-1), metric='cosine')
rshelp.query("SELECT id, ev_charging FROM postgres_public.ratings_amenities LIMIT 100;")
!./flow --model cfg/tiny-yolo-voc-3c.cfg --train --dataset "../darkflow_data/MY20173c/JPEGImages" --annotation "../darkflow_data/MY20173c/Annotations" --epoch 5 --trainer adam --load bin/tiny-yolo-voc.weights --gpu 0.85
twitter_df_clean[['favorite_count','retweet_count']].dtypes
years = data.set_index("year") $ years.head()
price2017.head()
del(StockData['Date-1st']) $ StockData = StockData.drop(['Date-DaysInMonth'], axis=1) $ StockData.drop(['Date-MonthStart', 'Date-MonthEnd'], axis=1, inplace=True)
pickle.dump(tfidf_data, open('iteration1_files/epoch3/tfidf_data', 'wb'))
?pd.date_range
from collections import Counter $ Counter(df_2.titles)
train = train.sort_values(['Store','Date'],ascending=[True,False]) $ test = test.sort_values(['Store','Date'],ascending=[True,False])
my_prediction = my_tree_one.predict(test_features)
test_kyo3 = tweets_kyoto_filter[tweets_kyoto_filter['ex_lat']<35] $ test_kyo3 = test_kyo3[test_kyo3['ex_long']<135.715300]
dfgts.head()
autos['last_seen'] = autos['last_seen'].str[:10] $ last_seen_count_norm = autos['last_seen'].value_counts(normalize=True, dropna=False) $ last_seen_count_norm.sort_index()
txt_tweets = txt_tweets[~txt_tweets.str.startswith('RT')] $ txt_tweets_position = tweets[~tweets['text'].str.startswith('RT')]['position'] $ txt_tweets_position.value_counts()
count = 0 $ for year in df.year: $     if year == 2017: $         count += 1 $ print(count)
print(points) $ print(points2) $ print(points+points2) $ (points+points2).dropna()
stations = session.query(Measurement).group_by(Measurement.station).count() $ print (f"There are {stations} stations")
plt.scatter(x, y) $ plt.show()
data = res.json()
h5.close()
autos[['date_crawled','last_seen', $       'ad_created','registration_month', $       'registration_year']].info()
archive_copy.pupper.unique()
g.ngroups
df.loc[closest_id]['skills'].values.tolist()
my_data.shape $
import statsmodels.api as sm $ logit_model = sm.Logit(teamY, teamX) $ result = logit_model.fit() $ print(result.summary2())
test_portfolio[['date', 'ticker', 'weight', 'price', 'share', 'net']].head()
dict_tokens = corpora.Dictionary(tokens)
test['answer_dt'].describe()
def classify_score(input_text): $     SWN = sentlex.SWN3Lexicon() $     classifier = sentlex.sentanalysis.BasicDocSentiScore() $     return classifier.classify_document(input_text, tagged=False, L=SWN, a=True, v=True, n=False, r=False, negation=False, verbose=False)
loans.payback_state[loans.fk_loan.isin(filtered_de_payments)]
pods.notebook.display_plots('regression_sgd_contour_fit{num:0>3}.svg', $     directory='../slides/diagrams/mlai', num=IntSlider(0, 0, num_plots, 1))
print(grouped_merged.head())
flight_phase.to_excel('flight_phase.xlsx') $ print("Done Writing!")
cc['name'].describe()
number_unpaids.hist(bins=20, figsize=(20, 5))
import datetime $ import matplotlib.pyplot as plt $ from matplotlib import style
val_idx = np.flatnonzero( $     (df.index<=datetime.datetime(2014,9,17)) & (df.index>=datetime.datetime(2014, 8, 1)))
print('The number of unique users:', df['user_id'].nunique())
df.set_index('UPC EAN', append=True, inplace=True) $ df.head(3)
to_be_predicted_Day2 = 14.7576603 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
una = miner.mine_user_tweets(user="una_dab") $ amanda = miner.mine_user_tweets(user="itsamandaross")
df_c2['country'].value_counts()
(null_vals>actual_diff).mean()
p_new = .1188 $ p_new
df.drop(df.query("landing_page == 'new_page' and group == 'control'").index, inplace=True) $ df.drop(df.query("landing_page == 'old_page' and group == 'treatment'").index, inplace=True) $ df.info()
import tensorflow as tf $ tf.test.gpu_device_name()
for  feature  in  ['country']: $     print("{}: {}".format(feature, countries_df[feature].unique()))
grouped.describe()
to_be_predicted_Day1 = 34.26 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
hk["catfathername"].unique()
pros.plot()
df2.iloc[146678] $
tw_clean.name = tw_clean.name.replace("a", "None")
autos["odometer"].astype(int)
df.iloc[:4]
lr_final.fit(scl.fit_transform(X_train),y_train) $ weights = pd.Series(lr_final.coef_[0], index=df_alg.columns[:-1]) $ weights.plot(kind='bar') $ plt.show() $
df2_countries['country'].unique()
def evening_mapper(x): $     if x.hour>17  and x.hour<23 : $         return 1 $     else: $         return 0 $
pystore.delete_store('mydatastore') $
weather.loc[:, 'PRCP':'SNWD']=weather.loc[:, 'PRCP':'SNWD'].fillna(value=0) $ weather.loc[:, 'TAVG': 'TMIN']=weather.loc[:, 'TAVG': 'TMIN'].fillna(method='ffill') $ weather.info()
y_test_over[fm_bet_over].sum()
therapist_duration.head()
df_goog.set_index('Date', inplace=True) $ df_goog.head()
def get_sec(time_str): $     h, m, s = time_str.split(':') $     return int(h) * 3600 + int(m) * 60 + int(s) $ def dur2sec(row): $     return get_sec(row['Duration']) $
x = x.rename({"A2": "A_sq"}, axis=1) $ x
metrics.accuracy_score(actual_value_second_measure, predicted_outcome_first_measure) $
systemuseData.to_sql(con=engine, name='systemuse', if_exists='replace', flavor='mysql',chunksize=10000)
df_questionable[df_questionable['unreliable'] == 1]['link.domain_resolved'].value_counts(25).head(25)
predictions_train=predictions_train.withColumn('scores',explode('rec')) $ predictions_train=predictions_train.withColumn('repo_id',predictions_train.scores['item'])\ $                        .withColumn('score',predictions_train.scores['score']) $ predictions_train=predictions_train.select('user_id','repo_id','score') $
latest_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first()[0]
Magic.__repr__
df_symbols.head()
lgComp_df = wcPerf1_df.groupby(['lgID']).sum() $ lgComp_df
sales_records = list(map(tuple, df[['item_name', 'date_str', 'median_sell_price', 'quantity']].values))
price2017 = price2017[['Date', 'Time', 'Germany/Austria/Luxembourg[Euro/MWh]']] $ price2017.columns = ['Date', 'Time', 'DE-AT-LUX'] $ price2017.head()
old_page_converted = np.random.choice([0,1], size=n_old, p=[p_old, 1-p_old]) $
(apple.index[-1] - apple.index[0]).days
win_draw_team = matches['home_team_win_or_draw'].groupby(matches['home_team_api_id']) $ ratio_win_home_team = win_draw_team.apply(lambda x: x.sum() / len(x)) $ print(ratio_win_home_team.head()) $ print(ratio_win_home_team.shape)
conv_treat_prob = len((df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)])) / (df2['group'] == 'treatment').sum() $ print(conv_treat_prob)
df_Inner_trans_users = pd.merge(transactions,users,how="inner",on="UserID") $ df_Inner_trans_users
positive.head()
yelpMonth = pd.read_csv('2016-09-02.csv') $ yelpMonth.head(10) #only display the first 10 $ yelpMonth.groupby("key").size() $
autos.columns $
indices = df.query('group != "treatment" and landing_page == "new_page" or group == "treatment" and landing_page != "new_page"').index $ df2 = df.drop(indices)
df_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis=1, inplace= True)
run txt2pdf.py -o '2018-06-22  2015 872 discharges.pdf'  '2018-06-22  2015 872 discharges.txt'
import statsmodels.api as sm $ convert_old = df2.query('group == "control"')['converted'].sum() $ convert_new = df2.query('group == "treatment"')['converted'].sum() $ n_old = df2[df2['group']=='control'].shape[0] $ n_new = df2[df2['group']=='treatment'].shape[0]
plt.hist(p_diffs) $ plt.xlabel('p_new - p_old') $ plt.ylabel('Frequency') $ plt.axvline(act_diff, color='red') #Red Line showing actual difference $ plt.show();
!head '../data/image_log_20180205.csv'
result_1 = pd.concat([df1, df3], axis = 1) # concatenate one dataframe on another along columns $ result_1
google_stock.head(8)
foursquare_data_dict['response'].items()[2][1][0]
run txt2pdf.py -o "CENTRA  HEALTH, INC  Sepsis.pdf"   "CENTRA  HEALTH, INC  Sepsis.txt"
type(s1), s1.dtype, len(s1), s1.shape
print(filtered_df['name'][0].split(' '))
full_globe_temp = pd.read_table(filename, sep="\s+", names=["year", "mean temp"], $                                 index_col=0) $ full_globe_temp
engine.execute('SELECT * FROM station LIMIT 5').fetchall()
frames = [train_users, test_users] $ users = pd.concat((frames), axis=0, ignore_index=True)
converted_users = (df2.converted == 1).sum() #Converted users $ prob_conv = (converted_users/unique_users)
import pickle $ output = open('speeches_cleaned.pkl', 'wb') $ pickle.dump(speeches_df4, output) $ output.close()
weather = twitter_news.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text') $ weather
brand_pminfo["mean_mileage"] = mmileage_series $ brand_pminfo.sort_values(by='mean_price', ascending=False)
frame3.columns
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.head()
options_frame[abs(options_frame['ModelError']) >= 1.0e-4].plot(kind='scatter', x='BidAskSpread', y='ModelError')
X_train.f = X.f[0:30892,] $ y_train.f = y.f[0:30892] $ X_test.f = X.f[30893:32315,]
data_df = pd.concat([R_trip, R_weather], axis=1)
df_agg = tmdb_movies_production_countries_revenue.groupby('production_countries').agg({'revenue': np.sum}) #aggregate
df2_old_page = len(df2.query("landing_page == 'old_page'")) / df2.shape[0] $ print('The probability that an individual received the old page is: {}.'.format(round(df2_old_page, 4)))
import nltk $ from nltk.corpus import stopwords  #gets German stopwords from the nltk corpus
results = session.query(Station.name).limit(5) $ for result in results: $     print(result)
[tweet for tweet in df_clusters[df_clusters.cluster_cat==40].text[:10]]
contribs.iloc[imax].committee_name
rent_db.columns
invoice_sat.columns[~invoice_sat.columns.isin(v_invoice_sat.columns)]
lobbyFrame.sort_values(by='Start') $ lobbyFrame.head(10)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, wait_on_rate_limit=True)
categories_feat = categories_feat.str.join('').apply(lambda x: x[1:-1])
data_df.groupby('tone')['ticket_id'].nunique()
df_members = df_members[df_members.registered_via != -1]
kick_projects_ip_copy= kick_projects_ip.copy()
missing_conditions.sum()
annual_returns.head() $
grades = pd.read_csv("files/grades.csv") $ grades.head()
' '.join(tok_trn[0])
Helper().why_adaptive_selection()
t1 = pd.to_datetime('20:24:27 10/05/2018', format='%H:%M:%S %d/%m/%Y') $ t2 = pd.to_datetime('21:24:27 10/05/2018', format='%H:%M:%S %d/%m/%Y')
print(norm.ppf(1-(0.05)))
startups_USA.head()
large_df.info()
y = df_new.converted $ X_cols = ['intercept'] $ for i in range(unique_countries.size - 1): $     X_cols.append(unique_countries[i]) $ X = df_new[X_cols]
print(v_to_c.time.mean)
cell_df = cell_df[pd.to_numeric(cell_df['BareNuc'], errors='coerce').notnull()] $ cell_df['BareNuc'] = cell_df['BareNuc'].astype('int') $ cell_df.dtypes
df[df['Agency']=='DOT']['Complaint Type'].value_counts()
pd.DataFrame(data, index=['orange', 'red'])
dataframe.pct_change()
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ logit_mod = sm.Logit(df2.converted, df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
tmdb_movies['day_of_week'].head()
log_mod2 = sm.Logit(df_joined['converted'], df_joined[['intercept', 'ab_page', 'CA', 'UK']]) $ results2 = log_mod2.fit() $ results2.summary()
y_dist = list(np.zeros(394)) + list(np.ones(394)) $ y_dist = np.asarray(y_dist) $ print(y_dist)
cgm = cgm.rename(columns={"value": "mmol_L"}) $ cgm["mg_dL"] = (cgm["mmol_L"] * 18.01559).astype(int) $ cgm.mg_dL.head()
unique_stores = test_data['air_store_id'].unique() $ stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)
df.loc[[11,24,37]]
control_convert_rate = df2.query('group == "control"').converted.mean() $ print(' Given that an individual was in the control group, what is the probability they converted is {}'. $       format(control_convert_rate))
df2.head()
max_features = 1000 $ tfidf_vec = TfidfVectorizer(max_df=0.95, min_df=2, $                             max_features=max_features, stop_words="english") $ tfidf = tfidf_vec.fit_transform(comment_column) $ tfidf_feats = tfidf_vec.get_feature_names()
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_under_null, p_under_null]) $     old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_under_null, p_under_null]) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean()) 
thecmd = 'curl -v -F file=@'+curDir+'/'+dataDir+'input/beh_nyc_walkability.csv "https://'+USERNAME+'.carto.com/api/v1/imports/?api_key="'+APIKEY $ os.system(thecmd) #run the command to curl the input file, this should work as its not GDAL/OGR
volume_weather=general_volume.merge(df, left_on='ds' , right_on='Date') $ volume_weather.head() $ volume_weather=volume_weather[~volume_weather.T_avg.isnull()]
talks['text'] = talks['text'].apply(lambda x: re.sub("<.*?>", "", x))    
null_rows = kickstarter.isnull().any(axis=1) $ kickstarter[null_rows]
df_new['day'] = np.where(pd.to_datetime(df_new['timestamp']).dt.dayofweek >= 5, 'weekend', 'weekday')
from sklearn.svm import SVC
view["additional_turnpoints"] = ((view.distance / 25) - 1) - ((view.distance / 50) - 1) $ view["additional_time_per_turn"] = (view.short - view.long) / (view.additional_turnpoints) $ view = view[~view.swimstyle.str.contains('relay')].copy() $ view.sample(5)
diabetes_risk = high_hba1c.select('person_ref') \ $                           .union(diabetes_conditions.select('person_ref')) \ $                           .distinct() $ diabetes_risk.limit(10).toPandas()
airbnb_df['room_type'].value_counts()
df2_conv = df2.converted.mean() $ df2_conv
new_df = flatten_plot_df.loc[flatten_plot_df['tag'].isin(flatten_plot_df['tag'].value_counts()[:10].index)]
season_team_groups.size().sort_values(ascending=False).head(5)
vals = data.value.copy() $ vals[5] = 1000 $ data
%matplotlib inline $ plt.plot(data.sepal_length, data.sepal_width, ls ='', marker='o')
url_c = "https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv" $ c = pd.read_csv(url_c) $ c
df_tweet_json_clean = df_tweet_json_clean.drop('hashtags', axis = 1)
test_df["num_description"]= test_df["description"].apply(num_description) $
df.Date[0]
to_be_predicted_Day2 = 81.77623296 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
annual_returns = returns.mean() * 250 $
p_diff = new_page_converted.mean()-old_page_converted.mean() $ print('p_new - p_old for our simulated values is:',p_diff)
df.dropna(inplace=True)
diff = p_control - p_treatment $ actual_diff = diff.user_id $ actual_diff
twitter_Archive['date'] = twitter_Archive['timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ twitter_Archive['time'] = twitter_Archive['timestamp'].apply(lambda time: time.strftime('%H:%M'))
start = datetime.datetime(2012, 1, 3) $ end = datetime.date.today() $ amzn = web.DataReader("AMZN", 'morningstar', start, end)
X_train.info()
train_df['interest_level'].value_counts()*1.0/len(train_df)
stock.shape
probability = df2["converted"].mean() $ print("The probability is {}.".format(probability))
gnbtest['predicted_failure'] = model.predict(test_x)
df_city_reviews = df_reviews.join(df_city_restaurants, on="business_id").drop(df_city_restaurants["stars"])
git_log['timestamp'].describe()
df.loc['r2','B']
active_distinct_authors_latest_commit.schema
text = 'My cat is a great cat' $ tokens = text.lower().split() $ print('Words in the our text:', tokens)
cat_pizza.head(2)
learner = md.get_model( $     opt_fn, em_sz, nh, nl, dropouti=0.05, dropout=0.05, $     wdrop=0.1, dropoute=0.02, dropouth=0.05) $ learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)
df.shape
pred_cols = features $ df2 = sl.copy() $ df2=df2[pred_cols]
bnb[bnb['age']>80].head() $
df_amznnews_clsfd_2tick = df_amznnews_clsfd_2tick[['textblob_sent', 'vs_compound']].resample('1D').mean() $ print(df_amznnews_clsfd_2tick) $
import sys $ sys.path.insert(0,"../Resources/Modules")
logit_mod = sm.Logit(df_joined['converted'], df_joined[['intercept','ab_page', 'US', 'UK']]) $ results = logit_mod.fit()
print hpdcom['BuildingID'].describe() $ print hpdvio['BuildingID'].describe()
df_vow.head()
results3.summary()
df.ix[0].A = 1 $ store['df'] = df $ pd.HDFStore('store.h5')['df'].head(2)
pd.set_option("display.max_columns", 100)
merged.groupby("committee_name_x")
le_data_all.index.levels[0]
claims_std_dev = utility_patents_df.number_of_claims.std() $ claims_median = utility_patents_df.number_of_claims.median() $ utility_patents_subset_df = utility_patents_df[utility_patents_df.number_of_claims <= (claims_median + 3*claims_std_dev)].copy()
model1 = linear_model.LinearRegression() $ model1.fit(x1, y) $ (model1.coef_, model1.intercept_)
ny_df=pd.DataFrame.from_dict(foursquare_data_dict['response'])
pd.get_dummies(df2.landing_page).new_page.mean()
data.head() $
tfidftran = TfidfTransformer() $ def_values = tfidftran.fit_transform(df_normalized) $ print(def_values.toarray())
class_data['Census Tract'].value_counts().plot.barh()
dff2.to_csv("drive/NBA_Data_Hackathon/nba_gts.csv", index=None) 
df2['nrOfPictures'].hist()
prob1_kNN100 = pd.Series(x[1] for x in prob_kNN100) $ Results_kNN100 = pd.DataFrame({'ID': Test.index, 'Approved': prob1_kNN100})
myIP= table1['ip_address'].values $ low= table2['lower_bound_ip_address'].values $ high= table2['upper_bound_ip_address'].values $ lowDiff= [min(myIP[i]-low) for i in range(0,len(myIP))]
df_merged = pd.merge(df2, df_countries, how='inner', left_index=True, right_index=True) $ df_merged.head()
last_date = df.iloc[-1].name $ last_unix = last_date.timestamp() $ one_day = 86400 $ next_unix = last_unix + one_day $ next_unix
news_df.to_csv('news_analysis.csv')                                           # exports dataframe to csv
tt1['ZIPCODE'] = tt1['ZIPCODE'].astype('int')
df.head()
df[df.category != 'top'].head()
mentionsfreq = FreqDist(words_mention_sk) $ print("20 most common mentions: ", mentionsfreq.most_common(20)) $ hashfreq = FreqDist(words_hash_sk) $ print("20 most common hashtags: ", hashfreq.most_common(20))
from pylab import * $ plot(contour_sakhalin[:,0], contour_sakhalin[:,1]) $ gca().set_aspect('equal') $ title('Sakhalin Island') $ show()
dbcon = sqlite3.connect("mobiledata.db") $
stocks.index
portfolio_df = pd.read_excel('Sample stocks acquisition dates_costs.xlsx', sheetname='Sample') $ portfolio_df.head(10)
STD_reorder_stats.shape
cohort_churned_df.head()
time2 = parser.parse(dfnychead["Created Date"][1]) $ print "time2 =", time2 $ print "time2.year =", time2.year
cutoff_times = generate_labels('5fPXqLcScoC93rH/gCPK+5Soj+XdNMXX9S3LhV5dJjM=', trans, $                                label_type = 'MS', churn_period = 30) $ cutoff_times.head()
mb.index
matthew.head()
df2['converted'].sum()/df2.shape[0]
reviews.info() $ reviews=pd.read_csv("ign.csv",index_col=['Unnamed: 0','score_phrase']) $ reviews.head()
df_twitter_archive.rating_numerator.value_counts()
gs_rfc_under.score(X_test, y_test_under)
ftp.cwd("pub/sod/mecb/crw/data/outlook/v4/nc/v1/outlook/{:%Y}".format(today_UTC))
df_group1=df.query("landing_page=='old_page'and group=='control'")
plot_confusion_matrix(cm=cm, classes=['COLLECTION', 'PAIDOFF'], $                           normalize=False, $                           title='Confusion matrix', $                           cmap=plt.cm.Blues)
print(repos_users[repos_users['language'] == 'Python'].shape) $ repos_users[repos_users['language'] == 'Python'].head()
from sklearn.ensemble import ExtraTreesClassifier $ adult_model = ExtraTreesClassifier(random_state=23) $ adult_model = adult_model.fit(d_train, l_train)
from sklearn.model_selection import train_test_split
countries = pd.get_dummies(df_new['country']) $ df_new = df_new.join(countries) $ df_new.drop(['CA'],axis=1,inplace=True) $ df_new.head()
pc = pd.DataFrame(tt1.groupby('CUISINECODE').size()) $ pc.columns=['count'] $ pc_gt100 = pc[pc['count']>100].sort('count', ascending = False)[:75] $ pc_gt100
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new],[n_old, n_new])
goals_df.describe()
df2.user_id.value_counts().reset_index().iloc[0,0]
intersections_irr.head()
df_html_extract.info()
p__old = df2['converted'].mean() $ p__old
s.resample('Q', 'last').head()
typesub2017.head()
components2= pd.DataFrame(pca.components_, columns=['SP500','DJIA','NASDAQ','happiness'])
all_data.reindex(all_data.index[::-1])
discounts_table.groupby(['Discount Band'], sort=False)['Discount %'].max()
abc.loc[2011][('discharges', 'size')][:5]
y_pred=knn5.predict(X_test)
A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')]) $ A
import matplotlib.pyplot as plt $ import matplotlib.dates as mdates $ plt.gca().xaxis.set_major_locator(mdates.DayLocator()) $ plt.plot(df['order_date'],total_spend) $ plt.gcf().autofmt_xdate()
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])
ratio.describe()
n_old = control.shape[0] $ print(n_old)
from sklearn.neighbors import KNeighborsRegressor
df['age'] = 2017 - df['birth year'] $ df['age'].dropna(inplace= True)
us['country'].value_counts(dropna=False)
aTL.loc[(aTL.BRANCH_STATE.isnull())].shape
!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.9,en;q=0.8" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/8076/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1519109908&Signature=mwFZu2%2B%2FQ6UMu%2BQP4SZDmhFP%2BSyF0LfJTtKH74a2P0JWhc7K640LR88mcETS2oX7%2B4CVs1PG1U48fIKCWn0fQp%2Bz3t%2BrplrMl%2BMOUmXCiq6akD9pXPa6Vb6zV9aZoaOOSpAKO8dsU%2BNdOjKMlMoLfnfvFLHcIwzmZtN2KtiG6AG1Qjb%2BeD4DlC8plsQFqgLRetZ%2BqZqkQH4FAGhzVQiG5gDbEzsqHTharjIz2TzqhJHvxOeMDv4EBiI84Rx5nA3UokpJU5iB8niywL4%2FeV4r2xi9xynpjr5xbbMtwEyok6mbX6f3OpaYFWSFap%2FZLX3QcWYng7fwELGYKRYQr0NfkQ%3D%3D" -O "train.csv.zip" -c
import os $ import sys $ os.environ['PYSPARK_PYTHON'] = sys.executable $ os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
conversion_rate_null = (df2['converted'] == 1).sum() / len(df2.index) $ p_new = conversion_rate_null $ p_new
sns.rugplot(dataset['FollowerCount']) $ plt.title("Rug Plot")
df_ad_airings_filter_3.shape
tfav_a.plot(figsize=(16,4), label="Likes", legend=True) $ tret_a.plot(figsize=(16,4), label="Retweets", legend=True);
countries_df = pd.read_csv('countries.csv') $ ab_df_new = countries_df.set_index('user_id').join(ab_df2.set_index('user_id'), how='inner')
def initialize_analyticsreporting(): $     credentials = ServiceAccountCredentials.from_json_keyfile_name( $         KEY_FILE_LOCATION, SCOPES) $     analytics = build('analyticsreporting', 'v4', credentials=credentials) $     return analytics
import urllib.request
def get_tweet_sentiment(tweet): $     return TextBlob(tweet).sentiment.polarity
mb.loc[('Firmicutes', 2)]
notus.loc[notus['country'].isin(canada), 'country'].value_counts(dropna=False)
data[['comments', 'subreddit']].groupby('comments').count()
df = pd.read_csv('data/cornwall_phones.csv') $ df.head()
bgCan = [[ 5.6 , 7.8, 6.0 ], [ 12.2, 4.4, 6.7 ]] # same ndarray as the NumPy lesson $ df = pd.DataFrame(bgCan, index=['Monday', 'Tuesday'], columns=['Breakfast', 'Lunch', 'Dinner']) $ df
to_be_predicted_Day4 = 26.71780383 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df[df.province==400][df.city<=16].groupby("city").sum().sort_values(by="postcount",ascending=False)["postcount"][:10]
locationsDF.crs = londonDFSubset.crs $ mergedLocationDF = gpd.sjoin(locationsDF, londonDFSubset, how="left", op='within') $ mergedLocationDF.head()
np.dtype({'names':('name', 'age', 'weight'), $           'formats':((np.str_, 10), int, np.float32)})
tweet_json.source.value_counts()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ print(len(old_page_converted)) 
weekly=one_station.groupby(['week_num', 'weekday'])['DAILY_ENTRIES'].sum().reset_index() $ weekly.head(10)
msftCVR = pd.merge(msftAR,msftVR) $ msftCVR[:5]
graph.number_of_edges()
goog.Open.plot() $ goog.Open.asfreq('D', method='bfill').plot()
df_h1b_ft_US = df_h1b_ft_US.fillna(0).drop([47419, 80529])
s_1 = "SELECT * FROM paudm.photoz_bcnz as bcnz " $ s_2 = "JOIN paudm.cosmos " $ s_3 = "ON bcnz.ref_id = cosmos.paudm_id " $ s = s_1 + s_2 + s_3 $ data = pd.read_sql(s,engine)
from sklearn.linear_model import LogisticRegression
processing_test.files
pd.DataFrame(file_lines).dmac.value_counts()
params = clf.best_params_
users.location.fillna('\\N',inplace = True) $ users.company.fillna('\\N',inplace = True)
df_country = pd.read_csv("countries.csv") $ df_country.head(3) $ df3 = df2.merge(df_country,how='left',on='user_id') $ df3_country_dummies = pd.get_dummies(df3.country,prefix = 'country').iloc[:,1:] $ df3 = df3.join(df3_country_dummies)
df.loc[['a','b','e'],'A']
df = df[(df.Opened > '01/01/2009')]
logit1 = sm.Logit(df4['converted'],df4[['intercept', 'country_CA','country_UK', 'ab_page', 'page_CA','page_UK']]) $ result1 = logit1.fit() $ result1.summary()
con = sqlite3.connect('db.sqlite') $ pd.read_sql_query("SELECT * from sqlite_master", con)
prob_converted = df.converted.mean() $ print('The probability of conversion is: ' + str(prob_converted))
dc_counter_frame.to_csv('dc_count.csv') $ tm_counter_frame.to_csv('tm_count.csv') $
test_labels = [reuters.categories(doc_id)for doc_id in doc_id_list if 'test' in doc_id]
open_list = [value for key, value in price_dict['Open'].items()] $ highest_open = max(open_list) $ lowest_open = min(open_list) $ print('Highest open is: ' + str(highest_open)) $ print('Lowest open is: ' + str(lowest_open))
ctv = CountVectorizer() $ project_cats = train.project_subject_categories.apply(lambda x: scrub(x)) $ x = ctv.fit_transform(project_cats) $ train = train.reset_index
data_for_model.head()
resultItemUsersLen=topUserItemDocs.groupby(['item_id']).size() $ print min(resultItemUsersLen) $ print max(resultItemUsersLen) $ print np.median(resultItemUsersLen) $ print np.mean(resultItemUsersLen)
import urllib3, requests, json, base64, time, os $ warnings.filterwarnings('ignore')
investors.loc[0]
pres_date_df.sort_values('start_time', ascending=True, inplace=True) $ pres_date_df.head(10)
Base.classes.keys() $
sns.violinplot(wildfires_df['FIRE_SIZE'])
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ act_diff 
df = pd.read_sql_query('SELECT Agency, COUNT(*) as `num_complaints`' $                        'FROM data ' $                        'GROUP BY Agency ' $                        'ORDER BY -num_complaints', disk_engine) $ py.iplot([Bar(x=df.Agency, y=df.num_complaints)], filename='311/most common complaints by agency')
sandag_df = pd.read_csv('./Datasets/SANDAG_Cleaned.csv') $ sandag_df = sandag_df.set_index(['year', 'tract']).sort_index() $ sandag_df = sandag_df[['pop', 'age', 'male_pct', 'white_pct']] $ sandag_df.head()
commits_per_year = corrected_log.resample('AS',on='timestamp')['author'].agg('count') $ print(commits_per_year.head())
best_val = ModelCheckpoint('model_{epoch:02d}.h5', save_best_only=True, mode='min', period=1)
with sns.axes_style('white'): $     p = sns.jointplot('retweet','likes', data=twitter_archive_master) $     p.ax_joint.plot(np.linspace(0,20000), $                    np.linspace(0,40000))
df_final = pd.concat([df_big, df_small])
convert_old = df2.query("group == 'control'")['converted'].sum() $ convert_new = df2.query("group == 'treatment'")['converted'].sum() $ n_old = df2.query("group == 'control'")['converted'].count() $ n_new = df2.query("group == 'treatment'")['converted'].count()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
df_all['datetime'] = pd.to_datetime(df_all['created'], unit='s') $ df_all['datetime'] = pd.DatetimeIndex(df_all['datetime']) $ print(df_all['datetime'][0:5])
support.amount.sum()
df_elect.shape
best_dt = pickle.load(open('xgb_model.pkl', 'rb'))
events_df[events_df['event_day']==events_df['event_day'].min()]
mydata = quandl.get("FSE/AFX_X", start_date="2017-01-01", end_date="2017-12-31")
svg2 = displacy.render(parsed4, style='ent', jupyter=True)
x = df2[df2.group == 'control'] $ x["converted"].mean() * 100
archive_clean['doggo'].replace(np.nan,'', inplace=True) $ archive_clean['floofer'].replace(np.nan,'', inplace=True) $ archive_clean['pupper'].replace(np.nan,'', inplace=True) $ archive_clean['puppo'].replace(np.nan, '', inplace=True) $ archive_clean['stage'] = archive_clean[['doggo', 'floofer','pupper','puppo']].apply(lambda x: ''.join(set(x.astype(str))), axis=1) $
for i, r in bug_count.iteritems(): $     md_id = 'mdb' + str(df_bthlst[df_bthlst["booth_id"]==i]["md_id"].values[0]) $     df_selparams.loc[md_id, 'n_faults'] = bug_count[i]
tfa = np.vstack(df_all.timestamp_first_active.astype(str).apply(lambda x: list(map(int, [x[:4],x[4:6],x[6:8],x[8:10],x[10:12],x[12:14]]))).values) $ df_all['tfa_year'] = tfa[:,0] $ df_all['tfa_month'] = tfa[:,1] $ df_all['tfa_day'] = tfa[:,2] $ df_all = df_all.drop(['timestamp_first_active'], axis=1)
data.iloc[:1,11:]
gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_
pt_download=pd.DataFrame.pivot_table(df_download_node,index=['uid'],values=['nid'],aggfunc='count',fill_value=0) $ pt_download=pt_download.reset_index() $ pt_download=pt_download.rename(columns={'nid':'no_of_downloads'}) $ pt_download['downloaded_or_not']='Downloaded after 1st July 2018'
dropped =guinea[['Date', 'Totals','country']] $ dropped =dropped.fillna(0) $ dropped.head()
df.tail() # default --> prints last 5 rows
df.zone.fillna('Unknown', inplace=True) $ df.county_name.fillna('Alameda', inplace=True)
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, 1-p_old])
def tvd(col1, col2): $     return 0.5*(np.sum(np.abs(col1 - col2))) $ observed_tvd = tvd(clinton_pivoted['aides_proportion'], clinton_pivoted['clinton_proportion']) $ observed_tvd
accuracy=100*metrics.accuracy_score(y_test, y_hat) $ print(accuracy)
titanic.pivot_table('survived', index=['sex', age], columns='class')
df_weather_origin.groupby("ORIGIN").count()
active_psc_records = get_company_name_from_number(active_psc_records) $ politician_pscs = pd.merge(add_suffix_to_cols(active_psc_records,'ch'),add_suffix_to_cols(every_politican,'ep'),on='join_id')
tweet_archive_enhanced_clean['name'].value_counts()
df[['Pending Ratio']].head()
net.save_nodes(nodes_file_name='nodes.h5', node_types_file_name='node_types.csv', output_dir=directory_name)
questions = pd.concat([questions.drop(['purchased'], axis=1), who_purchased], axis=1)
pd.date_range(start = '8/01/2018', end = '8/31/2018', freq = sm)
date_cols = ['CRE_DATE_GZL', 'SCHEDULED_START_DATE', 'SCHEDULED_END_DATE'] $ date_cols_history = ['ACTUAL_START_DATE', 'ACTUAL_END_DATE'] $ intervention_test = pd.read_csv(data_repo + 'intervention_test.csv', sep='|', encoding='latin-1', parse_dates=date_cols) $ intervention_history = pd.read_csv(data_repo + 'intervention_history.csv', sep='|', encoding='latin-1', parse_dates=date_cols+date_cols_history) $ intervention_train = pd.read_csv(data_repo + 'intervention_train.csv', sep='|', encoding='latin-1', parse_dates=date_cols)
import matplotlib.pyplot as plt $ %matplotlib inline
fig = plt.figure(figsize=(5,5)) $ plt.pie(list(passengers_per_cabin_floor.values()), labels=passengers_per_cabin_floor.keys(), autopct='%1.0f%%') $ plt.axis('equal') $ plt.title('Passengers per floor') $ plt.show()
my_employee.__str__()
lo = sm.Logit(df2['converted'], df2[['intercept','ab_page']])
sns.boxplot(data=sample)
op = model.predict([Q1_test, Q2_test])
intersections_irr.info()
missing_sample.dropna()
df_SP = pd.merge(df_sched, df_proj_agg, on='ProjectId', how='inner')
repeated_user = df2[df2['user_id'].duplicated()] $ repeated_user['user_id']
def getStartDate(collectionDate): $   startDate = max(df_epi.index[df_epi.index<collectionDate]) $   return startDate.strftime('%Y-%m-%d')
groups = groups.loc[groups.UPD_DATE <=  groups.CRE_DATE_GZL, :].groupby(['INSTANCE_ID', 'CRE_DATE_GZL'])
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
%run process_twitter2tokens.py -i ../data/Training_relate_unrelated.csv -ot ../data/Training_relate_unrelated.txt -oc ../data/Training_relate_unrelated_tokenized.csv -co text
twitter_master1 = pd.merge(tweet_info, image_pred, left_on='id', right_on='tweet_id')
top_songs[top_songs['Track Name'].isnull()]
df = df[['Adj. Open',  'Adj. High',  'Adj. Low',  'Adj. Close', 'Adj. Volume']]
df1['date'] =  pd.to_datetime(df1['date']) $ df1['Commodity']=df1['Commodity'].str.lower() $ df1['Commodity'] = df1['Commodity'].str.replace('bhagar/vari','bhagar-vari') $ df1['Commodity'] = df1['Commodity'].str.replace('thymol/lovage','thymol-lovage') $ df2['commodity']=df2['commodity'].str.lower()
y_train[1459] $ y_train.shape
basket.shape
df_ham['place'] = df_ham['place'].apply(lambda x: translate(x,'en')) $ df_ham['text'] = df_ham['text'].apply(lambda x: translate(x,'en'))
countries_df = pd.read_csv('./countries.csv') $ countries_df.head()
autos["price"]=autos["price"].str.replace("$","").str.replace(",","").astype(float)
p_new = df2[(df2.group == 'treatment')&(df2.converted == 1)].shape[0]/df2[df2.group == 'treatment'].shape[0] $ p_new
small_ratings_file = os.path.join(dataset, 'ml-latest-small', 'ratings.csv') $ print('small_ratings_file: '+ small_ratings_file) $ small_ratings_raw_data , small_ratings_raw_data_header = read_file(small_ratings_file)
df_users_2.shape
cats_df['number of vet visits'] = cats_df['number of vet visits'].apply(lambda x: np.nan if x < 0 else x) $ cats_df[cats_df['number of vet visits'].isnull()]
Xfinal = X_main.iloc[holdout_idx:, :][cols_final] $ yfinal = y_main[holdout_idx:]
suspects_with_1T_25['imsi'].unique()
(df.groupby('episode_id')['number'].max() + 1).head()
response = requests.post(OAUTH_SERVER_URL, $                          data={'username': USER_NAME, 'password': PASSWORD, $                               'scope': 'cloud_api', 'grant_type': 'password'}, $                          auth=(CLIENT_ID, CLIENT_SECRET)) $ print_status(response)
data.info()
os.chdir('/Users/AlexandraDing/Desktop') $ pickle.dump( top_movies_list, open( "top_movies_100_year2016_list.p", "wb" ) ) $
df2 = df2.round({'new_sales_perc':4}) $ df2.head()
reset_graph() $ X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X") $ y = tf.placeholder(tf.int64, shape=(None), name="y")
stocks_info_df.tail()
image_predictions_clean=image_predictions_clean[~image_predictions_clean['tweet_id'].isin(retweets)] $
def predict (x): $     return predict_house_price.slope * x + predict_house_price.intercept
twitter_df_clean.loc[twitter_df_clean.rating_denominator != 10, 'rating_denominator'] = 10
poverty_data.columns=poverty_data_columns
df['timestamp'] = pd.Timestamp('20180101') $ df
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator $ paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build() $ cvEvaluator = MulticlassClassificationEvaluator() $
broadband.to_csv('broadband.csv')
df.index = pd.DatetimeIndex(df["created_at"])
topics_df.to_csv("topics_words.csv",encoding="utf8",index=False)
prob_conv_control = df2[df2['group']=='control'].sum()['converted'] / len(df2[df2['group']=='control']) $ prob_conv_control
test['air_genre_name']=test['air_genre_name'].fillna('Other')
plt.rcParams['figure.figsize'] = (15, 5) $ DataSet['userTimezone'].value_counts().plot(kind='bar') $ plt.xlabel('Timezones') $ plt.ylabel('Tweet Count') $ plt.title('Top 10 Timezones tweeting about #Australia')
tags.head(25)
extracted_rating=twitter_archive_clean.text.str.extract('([0-9]+\.?[0-9]*/[0-9]+)', expand=False) $ extracted_numerator=extracted_rating.apply(lambda x: x.split('/')[0]) $ twitter_archive_clean['rating_numerator']=extracted_numerator
payments_all_yrs.tail()
df_pol.sample(10)
fuel_therm_abs_rate = sp.get_tally(name='fuel therm. abs. rate') $ therm_util = fuel_therm_abs_rate / therm_abs_rate $ therm_util.get_pandas_dataframe()
train_df["description_score"] = train_df['description'].apply(process)
train = data.groupby(pd.Grouper(freq='W'))['text'].apply(lambda x: "%s" % ' '.join(x)) $ train.tail()
cc.info()
df_2014 = pd.read_csv(DATA_FILE_2014) $ df_2015 = pd.read_csv(DATA_FILE_2015) $ df_2016 = pd.read_csv(DATA_FILE_2016)
url = 'https://twitter.com/marswxreport?lang=en' $ response = requests.get(url) $ soup = bs(response.text, 'lxml')
(16753 + 655 + 2173 + 9516)/.30
fb.head(20)
test_pl2.describe()
(null_vals > obs_diff).mean()
df_clean.info()
count_vect = CountVectorizer(vocabulary=significant_features, tokenizer=lambda x: x.split(',')) $
nltk_text.concordance("marina") $
df2017=df1.iloc[0:165,]  #line 165 is 2016 so we need 1 more than line 164 ie 165 $ df2017.tail()
not_in_misk.to_csv('./not_in_misk.csv', index=False)
logit_mod = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'CA_pages']]) $ results = logit_mod.fit() $ results.summary()
columns = ['bedrooms', '1 bathroom', '2 bathrooms', '3 bathrooms'] $ bed_df = pd.DataFrame( columns=columns) $ bed_df = bed_df.fillna(0) $ print(bed_df)
df2.drop_duplicates(subset='user_id', keep='first', inplace=True)
os.getcwd()
p_diffs = [] $ new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
r.html.search('Python is a {} language')[0]
Sort3 = stores.sort_values(by = ["Location","TotalSales"], ascending = [True,False])
dt.date()
open_yearly = open_prices.groupby(open_prices.index.year).mean() $ open_yearly
pd.concat([msftAV[:5], aaplAV[:3]], axis=1, keys=['MSFT', 'AAPL'])
data_sets['15min'][data_sets['15min']['interpolated_values'].notnull()].tail()
df_protest.loc[df_protest.TownCity_Name=='Cape Town'].head().index
df2['converted'].mean() # probability of conversion for all groups in the experiment is 0.11959708724499628 $
word_counts.head()
pred_probas_under_k150 = gs_k150_under.predict_proba(X_test)
print('RMSE LGBMRegressor: ', RMSLE(np.log1p(train['visitors'].values), lgbmrscv.predict(train[col])))
titanic.groupby(['sex','embark_town'])['survived'].mean()
top10_df_pd=top10_df.toPandas() $ top10_df_pd.head(10)
df.shape
data_df.desc[15]
plt.plot(x, y, 'r.', markersize=10) # plot data as red dots $ plt.xlim([-3, 3]) $ mlai.write_figure(filename="../slides/diagrams/ml/regression.svg", transparent=True)
s.ix[:3].resample('W', fill_method='ffill')
temp_wide_df = pd.concat([grid_df, temp_df], axis = 1) $ temp_wide_df.head()
weekly_hashtag_window = Window.partitionBy('week', 'hashtag')\ $                             .orderBy(functions.desc('count'))\ $                             .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit()
df.drop_duplicates(subset='hash_id')[race_cols].sum()/len(df.drop_duplicates(subset='hash_id'))*100
df[df['Complaint Type'].str.contains('firework', case=False)]['Complaint Type'].unique()
plt.rcParams['axes.unicode_minus'] = False $ dta_713.plot(figsize=(15,5)) $ plt.show()
LT906474.rename(columns={'# feature':'feature'}, inplace=True) $ CP020543.rename(columns={'# feature':'feature'}, inplace=True)
import os $ os.chdir('models/research/')
df_sample_day=df_sample[(df_sample["day"] == 1)] $ df_sample_night=df_sample[(df_sample["day"] == 0)] $ scipy.stats.ks_2samp(df_sample_day["tripduration"],df_sample_night["tripduration"])
jobs_data['clean_description'].head(5)
new_fan.to_csv('../data/new_fan.csv') $ return_fan.to_csv('../data/return_fan.csv')
fig, ax = plt.subplots(1, figsize=(12,4)) $ plot_with_moving_average(ax, 'Seasonal AVG Doctors', doc_duration, window=52)
import seaborn as sns $ ax = df['DepDelay'].hist(bins=500) $ ax.set_xlim(-100, 500) $ plt.title('Number of flights per delay bin') $ plt.show()
valid_scores = scores[scores['date']<'2018-03-12'] $ valid_scores.shape
for key,value in positive_deaths_sorted.items(): $     print( "Trend Ratios in " + str(key) + " greater than 1 = " + str(value))
df2 = df2.drop_duplicates()
autos.head()
train[simple_features].head(1)
df_new[['CA','UK','US']]=pd.get_dummies(df_new['country']) $ df_new.head(1)
preds_2016 = lr.predict(x_2016)
val_small_data.info("deep")
max_return = minimize(return_, W, args=(ER, COV), method='SLSQP', bounds=b_, constraints=c_) $ if not max_return.success: $     print 'return_ not optimized: ', max_return.message $ Results = Results.join(pd.Series(max_return.x, name='max_return').round(4)) $
lr = LogisticRegression(random_state = 42) $ param_grid = {'penalty': ['l1', 'l2'], $               'C':np.logspace(0, 2, 10)} $ lr_gd = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, scoring='f1', n_jobs = -1) $ lr_gd.fit(X_train, y_train)
tmp_df[np.logical_not(tmp_df['Geo.Lon.Lat'].isnull())].sample(5000).plot()
(temp_df.emailHash == '44d0dc437936b13f7cea2f77053806bd').sum()
oil_interpolation.index.names=[None] $ oil_interpolation.set_index('date',inplace=True) $ pd.DataFrame.head(oil_interpolation)
treatment_but_not_new = df[(df['group'] == 'treatment') & (df['landing_page'] != "new_page")] $ new_but_not_treatment = df[(df['group'] != 'treatment') & (df['landing_page'] == "new_page")] $ dont_line_up_1 = pd.concat([treatment_but_not_new,new_but_not_treatment]) $ print('The number of times the new_page and treatment don\'t line up is:' + str(dont_line_up_1.shape[0])) $ dont_line_up_1.head()
df = pd.merge(df,rsvp_df,how="left",on="urlname") $ df.head()
allqueryDF.head()
joined.country.value_counts()
my_df["day_from"] = my_df["date"] - min(my_df["date"]) $ my_df["day_from"] = my_df["day_from"].dt.days $ my_df["time"] = my_df["timestamp"].dt.hour * 60 + my_df["timestamp"].dt.minute $ print(my_df.head())
df2_with_country[['CA', 'US','UK']] = pd.get_dummies(df2_with_country['country'])[['CA','US','UK']] $ df2_with_country[['old_page','new_page']] = pd.get_dummies(df2_with_country['landing_page'])[['old_page','new_page']] $ df2_with_country.head() $
totalMeetingTime = ((itemTable['Time'] * meeting_hash).sum()) / 60 $ print ("Total Meeting Time : " + str(round(totalMeetingTime)) + " " + "Hours")
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG_max =(grouped.aggregate(np.max)['disc_times_pay']) $ grouped_by_year_DRG_max.head()
erfurt.info()
np.count_nonzero(nba_df.isnull())
df2['timestamp']=pd.to_datetime(df['timestamp'],format='%Y-%m-%d %H:%M:%S') $ df2.dtypes
cb_funds.head()
print type(topic_word) $ print topic_word.shape $ print topic_word[0][:20] $ print topic_word[0].sum()
df_US = df_new.query('country == "US"')
df.info()
data.dtypes
extract_nondeduped_cmp = extract_all[f_remove_extract_fields(extract_all.sample()).columns.values].copy()
uniqueusers=df.user_id.value_counts() $ uniqueusers.size $
full['LOS'].hist(bins=15,figsize=(12,4)) $ plt.title('Distribution of length of stay',size=15) $ plt.xlabel('days') $ plt.ylabel('count')
df_new.groupby(['country','ab_page'], as_index=False).mean()
df_new[df_new['Retraction_time'].notnull()]
t.head()
train.StateHoliday.head()
sum(trumpint.num_comments),sum(cliint.num_comments)
df['TYPE'].value_counts().sort_index()
def get_important_features(all_words, n=3000): $     sred = sorted(all_words.items(), key=lambda x : x[1]) $     sred = sred[::-1] $     word_features = [word.lower() for (word, count) in sred[:n]] $     return word_features
frames = [df_group1,df_group2] $ df2=pd.concat(frames) $
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth) $     return api $
final_word_df.sort_index(by='Frequency').tail()
dates = pd.date_range('2018-01-01', '2018-05-23', freq='M') $ dates
print(r.json()['dataset']['column_names'])
model_forest = RandomForestRegressor(n_estimators=6).fit(X_train, y_train) $ print('Training set accuracy: {:.2f}:'.format(model_forest.score(X_train, y_train))) $ print('Test set accuracy: {:.2f}:'.format(model_forest.score(X_test, y_test)))
feature_importances.head(20)
sb.upload_file_to_item(new_item, outfile_zip)
p_mean = np.mean([p_new, p_old]) $ print("P of conversion under NH (p_mean):", p_mean)
import scipy.io as sio $ sio.savemat('my_filename3', {'a':mydata['a'], 'b':mydata['b']}) # a and b are still 0! $ loaded_variables = sio.loadmat('my_filename3.mat') $ loaded_variables
df_more = pd.read_csv('/Users/aakashtandel/Desktop/Indeed_Project_3_df_cleaned.csv', index_col=0) $ df_more = df_more.reset_index(drop=True) $ df_more.head()
github_data['project_year'] = github_data.project_creation_date.apply(lambda x: x[:4]) $ github_data['user_year'] = github_data.user_creation_date.apply(lambda x: x[:4])
op = [] $ for row in json_data['dataset_data']['data']: $     if row[1] != None: $         op.append(row[1]) $ print (max(op), min(op))
X=X.reshape(-1, 1)
grouper2.head()
new_read = pd.merge(read, article[['id','read_time']], how='left', left_on=['article_id'], right_on = ['id']) $ print(new_read.shape) $ new_read[0:10] 
retweets = status.retweets() $ retweets
testObj.rtn_null = True           ## change error handling: bad records will not simply get blank Address values $ testObj.set_statusMsgGrouping(10) ## get status message about every 10 records (this will be a small test) $ testObj.set_timeDelay(0)          ## remove time delay (this increases risk of errors) $
import matplotlib.pyplot as plt $ from plotnine import * $ from edx_stats import grouped_confidence_intervals $ from edx_plot import geom_confidence, theme_confidence
print("replacing null value to zero is done") $ compiled_data['botometer']=compiled_data.botometer.replace(np.NaN, 0) $ print("there is null value on column botometer, this null indicated this account already deactivate their account :") $ len(compiled_data[pd.isnull(compiled_data['botometer'])==True])
df.head(10) $ df.sort_values(by=['Year']).head(10)
sqlClient.delete_result(jobId)
scoring_url = client.deployments.get_scoring_url(deployment_details) $ print(scoring_url)
model.wv.similarity('man', 'women')
gb = convert_game_board(testset.filelist[ind],testset.feature_list,pgn2value) $
with ZipFile('{0}.zip'.format(datapath / zipfile), 'r') as myzip: $     myzip.extractall(datapath)
pook_url = "http://www.djbible.classicalgasemissions.com/book_of_pook.pdf" $ pook_dl = requests.get(pook_url, stream = True)
fi = pd.DataFrame({'cols':col_names, 'imp':rf.feature_importances_}) $ fi = fi.sort_values('imp', ascending =False)[:15] $ fi
writers.groupby("Country").mean()
parsed.pprint()
df_kws.divide(df_kws.sum(axis=1), axis=0).plot(kind='area')
fig,ax = plt.subplots(figsize=(8,6)) $ forecast_data[['yhat_lower', 'yhat', 'yhat_upper']].plot(ax=ax)
df.groupby("cancelled")["awards_referral_bonus"].value_counts()
data['Borough'].unique()
df['tt_lat'] = df['tweet_location_coord'].apply(lambda x: None if type(x) is float else x['lat']) $ df['tt_lng'] = df['tweet_location_coord'].apply(lambda x: None if type(x) is float else x['lng'])
dup_df = df2[df2['user_id'].duplicated(keep = False)] $ a = dup_df.groupby('user_id').apply(lambda x: list(x.index)) $ a
df_archive.sample(15)
TestData.head()
birth_dates.set_index("BirthDate_dt").loc["2014-01-01":,:]
tmp_df = tmp_df.drop('Case.Number.1', axis=1)
pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))
import xgboost as xgb $ from xgboost.sklearn import XGBClassifier $ XGB_model = xgb.XGBClassifier(objective='multi:softprob')
datacamp[datacamp["publishdate"]>='2017-01-01'].sort_values(by="publishdate", ascending=True).groupby([datacamp['publishyymm']],sort=False).size().plot(kind='bar', figsize=(15,7), color='b')
expiry = datetime.date(2015, 1, 5) $ msft_calls = Options('MSFT','yahoo').get_call_data(expiry=expiry) $ msft_calls.iloc[0:5,0:5]
most_retweeted_tweeps_sum=tizibika.groupby('user')['retweets'].sum()
gene_df['gene_name'].unique().shape
dogscats_df = pd.DataFrame(np.load('dogscats_features.npy')) $ dogscats_df.columns = ["X_" + str(col) for col in dogscats_df.columns] $ predictors = dogscats_df.columns.tolist() $ dogscats_df['label'] = np.load('labels_sample.npy') $ dogscats_df['imagePath'] = np.load('imagePaths_sample.npy')
import numpy as np $ print(np.info(np.tile))
print(csgo_profiles.info()) $ print('Null object :',csgo_profiles.isnull().any().sum()) $ print('Shape'+str(csgo_profiles.shape))
r6s.head()
count = 0 $ for elem in recent["Predictions"]: $     count += elem $ factor = 1.0/count $ ordered["Odds"] = ordered["Predictions"] * factor * 100
new_page_converted = np.random.choice([0, 1], p=[(1-p_new), p_new], size=n_new)
df[['beer_name', 'rating_score', 'simple_style', 'brewery_name', 'brewery_country']][df.rating_score < 2].sort_values('rating_score')
reviews.country.value_counts()
unsegmented_users.groupby('course_id').user_id.count()
v_item_hub.columns.tolist()
import datetime $ options_data['TTM'] = (pd.to_datetime(options_data['MATURITY']) - pd.to_datetime(options_data['DATE']))/datetime.timedelta(days=365)
df['group'].value_counts()
raw = pd.read_json('data/raw_data.json') #not needed $ object_check = raw['object_id'][0]
result['modifiedBy'].nunique()
tf_vec = CountVectorizer(max_df=0.95, min_df=2, $                          max_features=max_features, stop_words="english") $ tf = tf_vec.fit_transform(comment_column) $ tf_feats = tf_vec.get_feature_names()
year_prcp_df = pd.DataFrame(year_prcp, columns=['date', 'precipitation']) $ year_prcp_df.head()
adj_close_latest.set_index('Ticker', inplace=True) $ adj_close_latest.head()
 print(triplet_model.summary())
rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, "a" * x)) $ rdd.saveAsSequenceFile("dataRdd.txt") $ sortedRDD = sorted(sc.sequenceFile("dataRdd.txt").collect()) $ print(sortedRDD)
for k in range(len(imagelist)): $     send2trash.send2trash(imagelist[k]) $ imagelist = [i for i in os.listdir() if i.endswith(".pdf")  ] $ len(imagelist)
df_2018 = df[(df.year == 2018)]
import ibmcloudsql $ from pixiedust.display import * $ import pandas as pd $ targeturl=''
autos["price"].describe().apply(lambda x: format(x, 'f'))
ffr.index.year
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
def filter_images_and_links(text): $     return re.sub('!?\[[-a-zA-Z0-9?@: %._\+~#=/()]*\]\([-a-zA-Z0-9?@:%._\+~#=/()]+\)', '', text) $ filter_images_and_links('Lookat ![j kjds](wehwjrkjewrk.de), yes [iii](jlkajddjsla), and ' $                         '![images (17).jpg](https://steemitimages.com/DQmQF5BxHtPdPu1yKipV67GpnRdzemPpEFCqB59kVXC6Ahy/images%20(17).jpg)')
pumashplc = pumashp.merge(linkpp,on='puma',how='outer').fillna(0) $ pumashplc['linkNYCp100p'] = pumashplc['date_link_']/pumashplc['Pop']*100
files8['Tenure']=files8.EndDate-files8.StartDate $ files8.head()
pd.DataFrame(item_prediction_df.loc[25451]).sort_values(25451,ascending=False)
cities = xmlData['city'].value_counts().reset_index() $ cities.columns = ['cities', 'count'] $ cities[cities['count'] < 5]
train.head()
full.groupby(['PrimaryDx'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False) $
len(df2.query('converted == 1')) / len(df2)
df_DST=pd.DataFrame() $ df_DST['Datetime']=pd.to_datetime(datetime) $
subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})]
pax_raw.paxstep.hist(bins=30)
txHouse.info()
dfdates=df.set_index('Created Date') $ dfdates
autos = autos[autos["price"].between(1,351000)] $ autos["price"].describe()
src_map = dict(zip(mentions_df["src"],mentions_df["src_str"])) $ trg_map = dict(zip(mentions_df["trg"],mentions_df["trg_str"])) $ src_map.update(trg_map) $ user_names = src_map
tweet_length = pd.Series(data=data['len'].values, index=data['Date']) $ tweet_favourite = pd.Series(data=data['Likes'].values, index=data['Date']) $ tweet_retweet = pd.Series(data=data['RTs'].values, index=data['Date'])
pd.concat([msftA[:5], aaplA[:3]], axis=1, join='inner', keys=['MSFT', 'AAPL'])
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url)
df_pilots.head(5)
test1=pd.merge(test,aggreg, how='inner', on=['acreg', 'schd_park_end'])
autos.head()
actual_payments[actual_payments.fk_loan==27]
def fully_conn(x_tensor, num_outputs): $     return tf.layers.dense(x_tensor, num_outputs, activation=tf.nn.relu) $ tests.test_fully_conn(fully_conn)
plt.plot(glons, glats, marker='.', color='k', linestyle='none') $ plt.show()
posts.reset_index(inplace=True)
result_copy.index.is_unique
from sklearn.linear_model import LogisticRegression $ logreg = LogisticRegression() $ cross_val_score(logreg, X, y, cv=5, scoring='roc_auc').mean()
weekdays_avg=pd.concat([weekdays_avg_2012,weekdays_avg_2013,weekdays_avg_2014,weekdays_avg_2015,weekdays_avg_2016], axis=1) $ weekdays_count=pd.concat([weekdays_count_2012,weekdays_count_2013,weekdays_count_2014,weekdays_count_2015,weekdays_count_2016], axis=1) $ weekends_avg=pd.concat([weekends_avg_2012,weekends_avg_2013,weekends_avg_2014,weekends_avg_2015,weekends_avg_2016], axis=1) $ weekends_count=pd.concat([weekends_count_2012,weekends_count_2013,weekends_count_2014,weekends_count_2015,weekends_count_2016], axis=1) $
df2 = df2.join(country_dummies);
df_date_vs_count = reddit_comments_data.groupBy('created_date').count().orderBy('created_date').toPandas()
df2['converted'][(df2['group'] == 'treatment')].sum()/df2['converted'][(df2['group'] == 'treatment')].count()
user_repeated = df2[df2.duplicated(['user_id'])]['user_id'].unique() $ print('The repeated user_id is {}'.format(user_repeated))
data.info()
t2.info()
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='API_KEY $ r = requests.get(url)
'US POWER SECTOR CO2 EMISSIONS INTENSITY'.title()
df2['css_count'] = pd.to_numeric(df.css_count)
Xholdout = X_main.iloc[:holdout_idx,:] $ yholdout = y_main[:holdout_idx]
dfWords.head()
con = sqlite3.connect('db.sqlite') $ print(pd.read_sql_query("SELECT * FROM temp_table ORDER BY year ASC, total DESC", con)) $ con.close()
np.random.seed(42) $ new_page_converted=np.random.binomial(n=1,p=P_new,size=n_new) $ (new_page_converted)
sns.violinplot(x='precipType', $                y='polarity', $                data=twitter_moves[twitter_moves['precipType'] != 'snow'])
daily = data_df.groupby(pd.Grouper(key='datetime', freq='D'))['04_00060'].sum() $ daily.plot() $ plt.xlabel("Date Time") $ plt.ylabel("Total Daily Stream Discharge") $ plt.show()
dfRegMet.to_pickle(data+"dfRegMet.p")
pd.Timestamp('now', tz='Asia/Riyadh')
print(f"Fit shape: {fit.shape}, Fit non-nulls: {fit.nnz}") $ print(f"Non-null fraction of total: {'{:.10f}'.format(fit.nnz/(fit.shape[0] * fit.shape[1]))}")
sq81= "CREATE TEMPORARY TABLE  newtable_111 ( SELECT * FROM NBA_homepage order by 0.4*Fav+0.6*retweets DESC limit 50)" $ sq82="SELECT word, COUNT(*) total FROM ( SELECT DISTINCT Num, SUBSTRING_INDEX(SUBSTRING_INDEX(text,' ',i+1),' ',-1) word FROM newtable_111, ints) x where word like'%#%'GROUP BY word HAVING COUNT(*) > 0 ORDER BY total DESC, word;"
gdf_gnis = gpd.read_file('gnis_20180601.geojson')
s - pd.tseries.offsets.Day(2)
prediction.describe()
freq = pd.Series(' '.join(df['body']).split()).value_counts()[-10:] $ freq
df.sort_values("time")
def get_num(x): $     return int(''.join(ele for ele in x if ele.isdigit())) $ print(get_num("2,000")) $ print(get_num("100+")) $ print(get_num("63[a]"))
top.to_csv('top50tweets.csv', index=False)
df[df.rating_denominator > 10]
vectorizer = CountVectorizer(analyzer='word', stop_words='english', tokenizer=lambda text: text, $                              lowercase=False, binary=True, min_df=5) $ spmat = vectorizer.fit_transform(x_tokens)
df['source'].value_counts()
common_names.plot(kind = 'bar', figsize = (8, 8)) $ plt.title ('Top 5 most common dog names') $ plt.xlabel ('Dog name') $ plt.ylabel ('Amount of names');
df.columns = ['Timestamp', 'Price']
[len(sheet.columns) for sheet in sheets.values()]
n = 120 $ closest_id, urls = sim.get(df.loc[n]['skills'])
p_new = df2['converted'].mean(); $ p_new
df_mod.head()
ipAddress_to_country.head()
b2b_df = pd.read_csv(data_fp, header=None, names=['brain_weight', 'body_weight']) $ b2b_df.head()
print(' '.join(TEXT.vocab.itos[int(w)] for w in batch[0][:,1]))
iplot([{'x': prices.index, 'y': prices['CBA.AX']}])
df_episodes.info()
percipitation_2017_df.describe()
xml_in.head(5)
sgd = SGDClassifier() $ sgd.fit(X_train, Y_train) $ Y_pred = sgd.predict(X_test) $ acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2) $ acc_sgd
ch.setLevel(logging.WARNING)
del train_data $ del val_data $ gc.collect()
from sklearn.datasets import fetch_california_housing
import sentiment_impact as si $ np.random.seed(50) $ a = np.random.normal(size=2000) $ a = a/max(abs(a)) $ print('The sentiment is {}'.format(si.overall_sentiment(a)))
inspector = inspect(engine) $ columns = inspector.get_columns('measurements') $ for c in columns: $     print(c['name'], c["type"]) $
from pyspark.sql.functions import isnan, when, count, col $
pop.unstack(level=0)
import datetime $ datetime.datetime.now()
abc['sepsis_overall'] = abc['870'] +  abc['871'] +  abc['872'] $ abc = abc.sort_values(['sepsis_overall'], ascending=[False]) $ abc = abc.reset_index(drop=True) $ abc.head()
import math $ from sklearn.metrics import mean_squared_error $ math.sqrt(mean_squared_error(pred, y_test))
wrd_full2 = wrd_full.copy() $ prediction_clean3 = prediction_clean.copy() $ wrd_prediction = wrd_full2.merge(prediction_clean3, on='tweet_id', how = 'left') $ wrd_prediction.drop(['source','in_reply_to_status_id','in_reply_to_user_id', 'retweeted_status_id', 'retweeted_status_timestamp','retweeted_status_user_id'],axis=1,inplace= True) $ wrd_prediction.head()
cityID = 'ab2f2fac83aa388d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Oakland.append(tweet) 
print('Training the random forest...') $ from sklearn.ensemble import RandomForestClassifier $ forest = RandomForestClassifier(n_estimators=100) $ forest = forest.fit(train_data_features, train['sentiment'])
import nltk $ nltk.download('punkt')
test.head(1)
department_df_sub.apply(lambda x: x.max() - x.min()) # apply function to each column 
extract_all.loc[(extract_all.application_month=='2018-04') $                &(extract_all.APP_SOURCE=='LEADSMKT') $                &(extract_all.app_branch_state=='MO') $                &(extract_all.LEAD_MIN_SELL_PRICE==15)].APPLICATION_DATE_short.min()
df['OpenData Unique Key'] = df['OpenData Unique Key'].str.replace(',', '') $ df['Agency'] = 'HPD' $ df['Complaint Type'] = 'HEAT/HOT WATER'
random_forest = RandomForestClassifier(n_estimators=100) $ random_forest.fit(X_train, Y_train) $ Y_pred = random_forest.predict(X_test) $ acc_random_forest = round(random_forest.score(X_test, Y_test) * 100, 2) $ acc_random_forest
def run_model(formula): $     y, X = dmatrices(formula, data=m_df, return_type='dataframe') $     mod = sm.OLS(y, X) $     res = mod.fit() $     print res.summary()
def label_encoding(df, col_name): $     df[col_name] = c_df[col_name].astype('category') $     df[col_name+"_CAT"] = c_df[col_name].cat.codes $     return $
df_cust_data['Registration Date'] = df_cust_data['Registration Date'].as_matrix()
df["booking_user_agent"] = df["booking_user_agent"].str.split("/", expand=True)[0]
train_binary_dummy = pd.get_dummies(train_binary, columns = categorical) $ train_binary_dummy.head()
p + np.timedelta64(7200, 's')
stocks.index.names = ['name', 'date']
import pandas as pd $ data_for_model = pd.read_pickle('data_for_model')
df['Miles'] = df.Miles[df.Miles > 0] $ df.head(10)
no_specialty.columns
weather['PV_noise'] = np.absolute(np.asarray(noise))
yc_new1 = yc_new1.merge(zipincome, left_on='zip_dest', right_on='ZIPCODE', how='inner') $ yc_new1.head()
hmeq.summarize(cardinality=dict(name='hmeq_card', replace=True))
to_be_predicted_Day5 = 55.27881761 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df.shape
datelist = pd.date_range(end=pd.datetime.today(), periods=n, normalize=True).tolist() $ my_data = pd.DataFrame({'date': datelist, 'input1': x, 'input2': z, 'input3': u1, 'input4': u2, 'target': y}) $ my_data = my_data.drop(0) $ my_data.head(5)
print('Total number of manually extracted contradicted tweet pairs is:  ',cotradicted_pairs.shape[0], 'pairs')
df.shape
archive.info()
ppt_date.describe() $
cur.execute('INSERT INTO materials VALUES ("EVA", "ethylene vinyl acetate", 0.123, 4.56, "polymer")') $ conn.commit()  # you must commit for it to become permanent $ cur.rowcount  # tells you how many rows written, sometimes, it's quirky
df.set_index('datetime',inplace=True) $ df.index
tweet_df.describe()
pd.DataFrame(sanders)
s.str.count('m')
image_clean.head(5)
with open('../data/data.json', 'r') as f: $         json_raw = f.read() $ with open('../data/example.json', 'r') as f: $         json_ex = f.read()
df.groupby('author').score.agg(['min', 'max'])[:10]
i = int(dogscats_df.shape[0] * 0.75) $ dogscats_df.loc[i:, 'pred_00'] = np.load('predictions.npy') $
pumaPop['public use microdata area'] = pumaPop['public use microdata area'].astype('str')
ccp_df = cc_df[coins].div(market_cap_df['Total Market Cap'], axis=0) $ ccp_df = ccp_df[::-1]    # Reverse order of df $ ccp_df.head()
plt.figure(1) $ actor_counts = df['Actor1Name'].value_counts().head(10) $ actor_counts.plot.bar()
fixed_bonus_points.loc["alice"] = 0 $ fixed_bonus_points
df_hi_temps = pd.concat(hi_temps)
df.select("column_name")
data.head()
autos["num_photos"].describe()
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'control']]) $ results = logit_mod.fit()
for var in filtered_df[numeric_cols]: $     filtered_df.replace({var: {'\$': ''}}, regex=True, inplace=True) $     filtered_df.replace({var: {'\,': ''}}, regex=True, inplace=True) $     filtered_df[var] = filtered_df[var].astype('float64', copy=False) $     filtered_df[var] = filtered_df[var].fillna(0)
prob_convert = df2.converted.mean() $ print("Probability of individual converting is :", prob_convert)
titles = [div.find('a').text for div in soup.find_all('div', {'class': 'event-description-content-dev'})]
np.exp(0.0408)
rational_da['Sentiment'] = np.array(labels)
interactions_replied = pd.read_csv(folderData + 'interactions_replied.csv')
actual_diff = df2.converted[df2.group == 'treatment'].mean() - df2.converted[df2.group == 'control'].mean() $ (actual_diff < p_diffs).mean()
transactions[(transactions.transaction_date < datetime.strptime('2017-01-01', '%Y-%m-%d'))]
df_onc_no_metac[ls_other_columns].head()
X_test.shape
all_tables_df.loc[0]
!cat /notebooks/.log/20170704/20170704-071348-0190.log
quartiles = np.percentile(births['births'], [25, 50, 75]) $ mu = quartiles[1] $ sig = 0.74 * (quartiles[2] - quartiles[0])
year= train_data.date_account_created[0][:4] $ print year $ month= train_data.date_account_created[0][5:7] $ print month
drivers = data_merge.groupby("type") $ drivers_total = drivers['driver_count'].sum() $ drivers_totals = [drivers_total['Urban'], drivers_total['Suburban'], drivers_total['Rural']]
df2.converted.mean()
drop_columns = ['discount' , 'plan_list_price', 'actual_amount_paid', 'transaction_date','is_auto_renew' ] $ df_transactions = df_transactions.drop(drop_columns, 1)
lr_grid.fit(X_train,y_train)
msft = pd.read_csv("msft.csv", $                     dtype = { 'Volume' : np.float64}) $ msft.dtypes
X = best_worst.text $ y = best_worst.stars $ X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4) $ X_train_dtm = stfvect.fit_transform(X_train) $ X_train.shape
print("example of a shift: NEM out, IOTA in") $ print(b[b>0].iloc[-3,].sort_values()[0:11]) $ print(b[b>0].iloc[-4,].sort_values()[0:11])
pnew = df2[df2['landing_page']=='new_page']["converted"].mean() $ pnew
dd_df = pd.concat([dd_df.drop(['gearbox', 'notRepairedDamage', 'fuelType'], axis=1), dd_df_gearbox, $                    dd_df_notRepairedDamage, dd_df_fuelType], axis=1)
ferrocarriles_caba.head(10)
items.shape
popups = soup.findAll('a', {'aria-haspopup': 'true'})
video_meta = [] $ for chunk in chunks(video_id, n=40): $     vm_ = yt.get_video_metadata(chunk, key, P.parse_video_metadata) $     video_meta.extend(vm_) $ len(video_id)
bc_consumption = pd.merge(detailed_bc, losses_bc, how='inner', on = 'heartbeat_end') $ bc_consumption = bc_consumption[bc_consumption.kilowatt_hours>0] $ print ("dataset size: " + str(len(bc_consumption))) $ bc_consumption.head(5)
traindf.to_csv('train.csv', header=False, index=False, encoding='utf-8', sep='|') $ evaldf.to_csv('eval.csv', header=False, index=False, encoding='utf-8', sep='|')
to_be_predicted_Day3 = 21.38790453 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
total_docks = [] $ for day in train.date: $     total_docks.append(sum(stations[stations.installation_date <= day].dock_count))
duration_train_df.index
tweet_archive_clean.head(2)
pd.value_counts(appointments['Specialty'])
new_page_converted = np.random.choice([1, 0], p=[p_new, 1-p_new], size=n_new) $ new_page_converted.mean()
print(training_target.value_counts()) $ print(test_target.value_counts())
converted_proportion = (ab_data["converted"]==1).mean() $ converted_proportion
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page','US','UK','US_ab','UK_ab']]) $ results = logit_mod.fit() $ results.summary()
twitter_archive_master.rating_numerator.plot(kind='box');
survey = resdf.iloc[:,:113] $ survey.insert(2,'LangCd',resdf.iloc[:,120]) $ survey.to_sql('surveytabl',conn)
donors[donors['Donor Zip'] == 606 ]['Donor State'].value_counts()
for key, sheet in sheets.items(): $     print(f"{key}, {type(sheet)!r}")
my_tweet_df["tweet_source"].unique()
merged.groupby(["contributor_firstname", "contributor_lastname","committee_position"]).amount.sum().reset_index().sort_values("amount", ascending=False) $
archive_copy['full_text'] = archive_copy['full_text'].str.replace('&amp;', '&')
series2 = df2['FlightDate'].head() + ' ' + df2['DepTimeStr'].head() $ pd.to_datetime(series2)
conn.caslibinfo(caslib='research')
pold=df2.converted.mean() $ pold
linear = linear_model.LinearRegression() $ linear.fit(x_15, y) $ (linear.coef_, linear.intercept_) $
states.median(level='day')
for dtype in ['object', 'int']: $     print("dtype =", dtype) $     %timeit np.arange(1E6, dtype=dtype).sum() $     print()
results2.summary()
class newHandler(handler.SVPOLHandler): $     _parser = newParser
df_usa = df_usa.rename(columns = {'Gross Domestic Product (GDP)':'GDP','National Rainfall Index (NRI)':'NRI', $                                  'Population density':'PD','Total area of the country':'Area','Total population':'Population'}) $ df_usa.head()
param_test4 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]} $ gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80,min_samples_leaf=30,max_depth=11,min_samples_split=241, max_features='sqrt'),\ $                         param_grid = param_test4, scoring='log_loss',n_jobs=4,iid=False, cv=5) $ gsearch4.fit(drace_df[feats_used],y)
import numpy as np $ import matplotlib.pylab as plt $ %matplotlib inline $ from sklearn import linear_model
df_clean3.rating_numerator.value_counts()[:10]
hour_coef=pd.DataFrame(lm.params[1:]) $ type(hour_coef) $ hour_coef $
suspects_with_25_1['is_trip'] = suspects_with_25_1['timestamp'] - suspects_with_25_1['timestamp'].shift(1) > "12:00:00"
result.street_number
df.describe()
df_ad_airings_5['location'].isnull().sum()
d.tz
lq.dtypes
pd.Series(pd.date_range("2018-07-04", periods=3, freq="D"))
results_dict = load_existing_results(subject='R1389J', experiment='catFR5', $                                      sessions=[1], stim_report=True, $                                      db_loc=report_db_location, rootdir=rhino_root).compute() $ list(results_dict.keys())
dfEPEXbase.groupby(dfEPEXbase.index.week).aggregate('mean').plot(y=['Price', 'Volume'], secondary_y='Volume', title='Weekly Average by Calendar Week') # plot time series of weekly means
senateAll = senateAll.set_index('TLO_id') $ houseAll = houseAll.set_index('TLO_id')
spark.sql("select id, borrower from world_bank limit 2").toPandas()
temp_series_ny = temp_series.tz_localize("America/New_York") $ temp_series_ny
df.expanded_urls.value_counts()
accuracy = accuracy_score(y_test, predictions) $ print('Accuracy: {:.1f}%'.format(accuracy * 100.0))
for column in ls_cprc_columns: $     df_cprc[column].plot(kind='hist') $ plt.show()
vec = TfidfVectorizer() $ X_train_cont_doc = vec.fit_transform(X_train['Content']) $ X_valid_cont_doc = vec.transform(X_valid['Content']) $ test_cont_doc = vec.transform(test_df['Content'])
df1 = pd.DataFrame(np.arange(9.0).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])
pd.value_counts(ac['Filer']).head(20)
pages=pd.read_csv('pages.tsv') $ pages=pd.DataFrame(pages) $ pages.head(10)
df_daily.dropna(subset=["PREV_DATE"], axis=0, inplace=True) $ df_daily.head(5)
tweet_archive.name.value_counts().head()
print(reg1.score(X_train, y_train)) $ print(reg1.score(X_test, y_test))
faulty_names = archive_clean[archive_clean.name.str.islower()].name $ print(faulty_names)
Z = np.random.random((5,5)) $ Zmax, Zmin = Z.max(), Z.min() $ Z = (Z - Zmin)/(Zmax - Zmin) $ print(Z)
df.shape
print(autos['odometer_km'].max()) $ print(autos['odometer_km'].min())
new_page_converted=np.random.binomial(n_new,p_new) $
from nltk.corpus import stopwords $ print(stopwords.words('english'))
if search_results is not []: $     print(json.dumps(search_results[0]._json, indent=2))
df2.query("landing_page == 'old_page'")['landing_page'].count()
msft.head()
s.resample('30D', fill_method='ffill').head(10)
prob_temp = len(df2[df2["landing_page"] == "new_page"]) / len(df2) $ print("Probability that an individual received the new page: {}%".format(round(prob_temp, 2)))
all_cards.loc["Umezawa's Jitte"]
df.head()
archive_copy['in_reply_to_status_id'] = archive_copy['in_reply_to_status_id'].astype('str') $ archive_copy['in_reply_to_user_id'] = archive_copy['in_reply_to_user_id'].astype('str') $ archive_copy['timestamp'] = pd.to_datetime(archive_copy['timestamp']) $ archive_copy['dog_description'] = archive_copy['dog_description'].astype('str')
talks_train.head()
convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = len(df2.query('group == "control"')) $ n_old = len(df2.query('group == "treatment"')) $
sgd = SGDClassifier(loss="log", alpha=0.01, $                                          learning_rate='optimal')
c=pd.Series([630,25,26,255]) $ print(c)
au.find_some_docs(uso17_qual_coll,sort_params=[("id",1)],limit=3)
print("Train:", model.evaluate(x = X_train.reshape(shapeX), y = Y_train_bound, verbose = False) ) $ print("Dev  :", model.evaluate(x = X_dev.reshape(shapeX),   y = Y_dev_bound,   verbose = False) ) $ print("Test :", model.evaluate(x = X_test.reshape(shapeX),  y = Y_test_bound,  verbose = False) ) $
import matplotlib.pyplot as plt $ plt.style.use('ggplot') $ df[df['Agency'] == 'DOT']['Complaint Type'].resample(rule='M').count().plot() $ df[df['Agency'] == 'NYPD']['Complaint Type'].resample(rule='M').count().plot()
top10.plot.bar()
old_page_converted =  np.random.binomial(1, p = p_old,size = n_old) $ old_page_converted
import test_package $ test_package.print_hello_function_container.print_hello_function()
import codecs $ import spacy $ nlp = spacy.load('en')
joined['intercept'] = 1 $ joined[['CA', 'US']] = pd.get_dummies(joined['country'])[['CA', 'US']] $ joined.head(10)
df_ab_cntry[['UK','US','CA']] = pd.get_dummies(df_ab_cntry['country'])
logit_result = logit_mod.fit() $ logit_result.summary()
import locale $ locale.setlocale(locale.LC_ALL, "en_GB.utf8")
tweet_json_df.info()
weather_warm = weather_all[warm] $ weather_warm.head()
df.loc[(df.company == False) & (df.other == False), 'other'] = df[(df.company == False) & $                                                      (df.other == False)].name.isin(other_list)
df_joined['country'].unique()
df_countries = pd.read_csv('countries.csv') $ df_countries.sort_values(['user_id'], inplace = True) $ df_countries.reset_index(drop = True, inplace = True)
mv_lens.title.value_counts().head()
df[df.D > 0]
s1.loc['Tues']
logodds[logodds.index.str.contains('you')]
Customers_df.boxplot(by = 'New_or_Returning', column='Sales_in_CAD', figsize=(10,7)) $ plt.ylim(0, 45000) $ plt.title('Sales to New vs. Returning customers from 2010 - 2015') $ plt.xlabel('Type of Customer') $ plt.ylabel('Sales in CAD') $
unique, counts = np.unique(cities, return_counts=True) $
title_list_lower = list(full_data.titlelower) $ title_list = list(full_data.title)
criteria = so['answercount'] > so['score'] $ so[criteria].head()
to_be_predicted_Day5 = 81.95538051 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
model.compile(optimizer='RMSprop', loss='mse')
def ylabelloc(compoundScore): $     if(compoundScore > 0): $         return compoundScore + 0.01  #if compoundscore is positive, value should be displayed above bar $     else: $         return compoundScore - 0.01  #if compoundscore is negative, value should be displayed below bar
data['Sales'].resample('D').mean().autocorr(lag=1) $
loans_df.emp_length.value_counts()
ab_data.info()
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
train_topics_df=train_topics_df.fillna(0) $ test_topics_df=test_topics_df.fillna(0)
rt_max = ... $ rt_tweet = ... $ print("The tweet with more retweets is: \n{}".format(data['Tweets'][rt_tweet])) $ print("Number of retweets: {}".format(rt_max)) $ print("{} characters.\n".format(data['len'][rt_tweet]))
active_listing_count_binarizer = Binarizer(10000) $ active_listing_count_binarizer.fit(X_train[['Active Listing Count ']].values) $ training_active_listing_dummy = active_listing_count_binarizer.transform( $     X_train[['Active Listing Count ']].values) $ print(training_active_listing_dummy[0:5], training_active_listing_dummy.mean())
pd.get_dummies(df[cols],drop_first=True)
df3[df3['converted']==1].count()[0]/df3.count()[0]
soup.find_all('div', {'class': 'thing'})['data-fullname']
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ (z_score, p_value)
print train_sents[0] $ print during_v $ print vocab.id_to_word
model.doesnt_match("france england germany berlin".split())
dfcounts.sort_values(['created_date'], ascending = 0).head()
weekly = data.resample('W').sum() $ weekly.plot(style=[':', '--', '-']) $ plt.ylabel('Weekly bicycle count');
qualConvpct.head()
df_predictions.describe()
new_page_converted = np.random.choice([0, 1], n_new, p = [p_new, 1-p_new])
def make_col_vector(array): $     return sm.add_constant(make_col_vector(array), prepend=False)
OR = np.exp(-1.9888-0.0150) / np.exp(-1.9888) $ print (OR)
print ("The feature 'id' is not unique on pagaviews",len(pageviews.id.unique())) $ print ("The feature 'id' is not unique on impressions",len(impressions.id.unique())) $ print ("The feature 'id' is not unique on clicks",len(clicks.id.unique())) $ print ("I would use the feature 'vrlId' on clicks to join with its respective impression, but the feature 'id' on impression is not unique",len(clicks.vrlId.unique()))
x_test.shape
df[~df.amount_initial.str.startswith('-$') & ~df.amount_initial.str.startswith('$')]
client.training.get_details('training-WWmHAB5ig')
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.listdir()
user_logs = pd.read_csv('../input/user_logs.csv') # 392,106,544 rows --> nealy 400 millions rows
with open('tweet_json.txt') as f: $     data = json.load(f)
plt.hist(p_diffs) $ plt.title('Simulated difference : New & Old Page') $ plt.xlabel('Page Difference') $ plt.ylabel('Frequency');
df_twitter_archive_copy.gender.value_counts()
train = train.sort('date') $ train.reset_index(drop=True, inplace=True)
preds = xgb_learner.best_model.predict(dtest)
p_val = (p_diffs > p_diff_obs).mean() $ p_val
pods.notebook.display_plots('regression_contour_fit{num:0>3}.svg', directory='../slides/diagrams/ml', num=IntSlider(0, 0, num_plots, 1))
ratings = archive_copy['text'].apply(lambda x: re.findall(r'(\d+(\.\d+)|(\d+))\/(\d+0)', x)) $ print(ratings)
df['dated'] = pd.to_datetime(df['dated'],unit='s') $ df['edited'] = pd.to_datetime(df['edited'],unit='s') $ df=df.rename(columns = {'flag':'Parent?'}) $ b=df.loc[:,['com_id','author','content','dated','upvotes','score','depth','distinguished','removal_reason','gilded','edited','Parent?']] $ b
s1.index
sessions.info(null_counts = True)
mentioned_bills_all.shape
df.loc['r1']
for index, row in df.iterrows(): $     df.loc[index,'is_ch_company'] = row.postcodes in row.ch_postcodes
df_nulls = df[df.district_size == 0] $ df_nulls.groupby('app_id').count().created.plot(kind='barh') $ plt.figure() $ df_nulls.groupby('district_id').count().created.plot(kind='barh') $
import pycountry $ country_mapping = {country.alpha_2: country.alpha_3 for country in pycountry.countries} $ geo_code = [] $ for code in geo_countries: $     geo_code.append(country_mapping[code])
df.head()
num_of_converted = len(df2[df2.converted == 1]) $ float(num_of_converted)/len(df2)
pd.get_option("display.max_columns")
new_stock_data = stock_data.drop('volume', axis = 1) $ print(display(new_stock_data.head()))
data1.dropna(inplace=True)
df_train.median(axis=0)
df = pd.read_sql('SELECT * FROM booking_contacts', con=conn_a) $ df
rGraphData = pd.DataFrame({'to': MultData.to_account, $                            'from': MultData.from_account, $                            'deposit': MultData.deposited, $                            'withdraw': MultData.withdrawn})
print('Unique number of users notified: {}'.format(len(atdist_4x['vendorId'].unique())))
preds.mean()
p_mean = np.mean([p_new, p_old]) $ print("Probability of conversion according to null hypothesis (p_mean) is", $       p_mean)
def remove_duplicate_index(df): $     df = df.set_index('insert_id') $     return df[~df.index.duplicated(keep='first')] $
print(data["Time Stamp"].min()) $ print(data["Time Stamp"].max()) $
for dataset in full_data: $     dataset['IsAlone'] = 0 $     dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1 $ print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())
res = sts.query(qry)
old_numbers = [10,8,3,1,5,-5,2,-15,-4,5,-2,-1,-3,-5] $ new_numbers = pd.Series(old_numbers).replace(range(1,10,1),0) $ pd.concat([pd.Series(old_numbers),pd.Series(new_numbers)],axis=1)
def format_time(x): $     return datetime.strptime(x, '%m/%d/%Y').strftime('%Y-%m-%d')
data_2 =pd.read_excel
pd.groupby(vlc, by=[vlc.visited.dt.year,vlc.visited.dt.month]) \ $ .size() \ $ .plot(kind="bar", title="Visits distributed by Month", color="green", figsize=(12,4))
df2[df2['country'].unique()] = pd.get_dummies(df2['country']) $ df2.head()
import numpy as np $ np.where(lv_workspace.index_handler.subset_filter)
df.query('group != "treatment" and landing_page == "new_page"').count()
autos.columns = new_col_names
%%bash $ less mydata.csv
import  statsmodels.api  as sm $ log_mod = sm.Logit(df2['converted'],df2[['intercept','treatment']]) $ results = log_mod.fit()
twitter_merged_data.plot('stage', 'rating', kind='bar'); $ plt.title('Stage vs Rating Bar Plot') $ plt.xlabel('Stage') $ plt.ylabel('Rating');
df = pd.DataFrame(data=members) $ df.set_index(['id']); $ df.visited = (df.visited.values/1000).astype('datetime64[s]') $ df.joined  = (df.joined.values/1000).astype('datetime64[s]') $ df.columns
stat_data['working_exp'] = processed_working_exp.working_exp.mean() $ stat_data
writers.groupby('Country').all()
from sklearn.preprocessing import MinMaxScaler $ dataset = np.reshape(dataset, (len(dataset), 1)) $ scaler = MinMaxScaler(feature_range=(0,1)) $ scaled_dataset = scaler.fit_transform(dataset) $ dataset[:5],  scaled_dataset[:5]
model.userFeatures().first() $
brands_norm_count = autos['brand'].value_counts(normalize=True) $ brands_norm_count.sort_values()
archive_df_clean.info()
df[df['Complaint Type'] == 'Homeless Encampment']['Unique Key'].resample('M').count().plot()
edgereader.head()
from pyspark.sql.types import IntegerType $ df = df.withColumn("HOD", df["call_hour_of_day"].cast(IntegerType())).drop("call_hour_of_day")
df_final_edited.describe()
Actual_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ Actual_diff
X = Train.drop('Approved', axis=1) $ y = Train['Approved']
tweetsDF.dtypes
df_clean['rating_numerator']=df.text.str.extract('(\d+\.\d+)', expand=True) $ df_clean['rating_numerator']= pd.to_numeric(df_clean['rating_numerator']) $ print (" The text: %s \n The new grade in rating_numerator: %.1f \n The old grade: %.1f" % (df_clean['text'].ix[2326], df_clean['rating_numerator'].ix[2326],df['rating_numerator'].ix[2326])) $ print (" The text: %s \n The new grade in rating_numerator: %.1f \n The old grade: %.1f" % (df_clean['text'].ix[1689], df_clean['rating_numerator'].ix[1689],df['rating_numerator'].ix[1689]))
out_df['high'] = out_df['high']*.077788/.042513 $ out_df['medium'] = out_df['medium']*.227529/.195824 $ out_df['high'] = out_df['high']*.694683/.761663
wrQualified.map(lambda p: p["gfx"]["features"]["compositor"]).countByValue()
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23puravida').items(100): $     results.append(tweet) $ len(results)
bin_data.head()
feature_matrix_duplicated.reset_index().drop_duplicates().shape
gm_df = gm_df.rename(columns = {'Draft_year':'Draft_Yr'}) $ gm_df.head()
other_list = list(org_counts[org_counts >= 5].index) $ other_list.remove('unlisted') $ other_list.remove('unknown')
writers.groupby('Country').sum()
precipitation_measurement_df.plot() $ plt.title("12 Months of Precipitation") $ plt.ylabel("Precipitation") $ plt.show()
from IPython.core.display import HTML $ css = open('../static/style-table.css').read() + open('../static/style-notebook.css').read() $ HTML('<style>{}</style>'.format(css))
sql="SELECT * FROM %s.%s ORDER BY dict_field ASC" % (schema, dict_table) $ HTML(hc.sql(sql).toPandas().to_html())
pd.Timestamp(datetime(2018, 1, 1))
mask = df2['user_id'].duplicated(keep=False)
ab_df2['intercept'] = 1 $ ab_df2['ab_page'] = pd.get_dummies(ab_df2.landing_page)['new_page'] $ ab_df2.head() 
df3 =pd.get_dummies(df3,prefix=['country'], columns=['country']) $ df3.drop('country_CA', inplace=True, axis=1) $ df3.head()
prediction_clean['p1'] = prediction_clean['p1'].apply(lambda x: x.lower()) $ prediction_clean['p2'] = prediction_clean['p2'].apply(lambda x: x.lower()) $ prediction_clean['p3'] = prediction_clean['p3'].apply(lambda x: x.lower())
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True).hist()
print(tweet_archive[tweet_archive.rating_numerator == 0])
titanic.groupby(['sex', 'class'])['survived'].mean()
n_new=df2.query('landing_page == "new_page"').user_id.nunique() $ n_new
dfData['rqual_score10'].value_counts()
pipeline_tfidf = Pipeline([('posf', PartOfSpeechFilter()), $                      ('tfidf', TfidfVectorizer()) $                    ]) $ pipeline_tfidf.set_params(posf__stop_words=stop_words_list, tfidf__lowercase=True, tfidf__max_df=0.95, $                     tfidf__min_df=0.01, tfidf__stop_words=stop_words_update, tfidf__norm='l2', tfidf__use_idf=True)
payment_plans_combined[(payment_plans_combined.fk_loan==34) & (payment_plans_combined.fk_user_investor==38)].to_clipboard()
lr_pipe.fit(X_train, y_train) $ lr_pipe.score(X_test, y_test)
bp[bp["icustay_id"]==14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"]) $ bp[bp["icustay_id"]!=14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"]) $ bp.head()
tweet_full_df.rename(columns={'name':'dog_name'},inplace=True) $ tweet_full_df = tweet_full_df[['tweet_id','date','year','month','day','time','hour','dog_name','dog_stages', $                                'dog_breed','breed_conf','rating_numerator','rating_denominator','img_num', $                                'jpg_url','source','text','favorite_count','retweet_count']] $
df_concat['date'] = pd.DatetimeIndex(df_concat.date_series).normalize() $ df_concat.date.head()
browser.click_link_by_partial_text('Valles Marineris Hemisphere Enhanced') $ html = browser.html $ soup = BeautifulSoup(html, 'lxml')
df.describe()
all_tweet_polarity = [] $ for tweet in tweets: $     all_tweet_polarity.append(...) $ all_tweet_polarity
ending = pd.Series(data=end, index=dates) $ ending
conv = df[df['converted']==1]['user_id'].shape[0] $ ratio = conv/total $ ratio
dummies = pd.get_dummies(plate_appearances['bb_type']).rename(columns=lambda x: 'bb_type_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1) $ plate_appearances.drop(['bb_type'], inplace=True, axis=1)
ts = pd.Series(np.random.randn(len(rng)), index=rng).resample('D').mean() $ ts
plt.clf() $ plt.scatter(predictions_sm, y_test, s=30, c='r', marker='+', zorder=10) $ plt.show()
n_new = df2.query('landing_page == "new_page"').user_id.count() $ n_new
population.apply(lambda val: val > 1000000)
df2.query("group=='treatment'").count()
print(topics, topics.status_code, topics.headers['content-type']) $ print(topics.url) $ top50=topics.json() $ top50[0]['trends']
print(DataSet_sorted['tweetText'].iloc[-4])
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem, $                             stop_words=stopwords.words('english'), $                             max_df=0.5, $                             min_df=0.1, $                             lowercase=True)
auc_modeling_next = round(bcEval.evaluate(prediction_modeling2),6) $ print('Estimated AUC modeling next:', round(auc_modeling_next,6))
%%R $ flightsDB <- na.omit(flightsDB)
idxs_selected = X_new.get_support(indices=True) $ features_dataframe_new = fs_X.columns[idxs_selected]
hawaii_measurement_df.head()
? pd.DataFrame
twitter_archive[twitter_archive.duplicated()]
df2[((df2['group'] == 'control') == (df2['landing_page'] == 'old_page')) == False].shape[0]
time_to_close["time2close"].plot(kind="hist")
num_uni_user = df.nunique()['user_id'] $ num_uni_user
authors[authors['count'] >= 10].plot(kind='scatter', x='count', y='mean', alpha=0.3)
X_train_feature_counts = count_vect.fit_transform(train['feature_list'])
tfav = pd.Series(data=data['Likes'].values, index=data['Date']) $ tret = pd.Series(data=data['RTs'].values, index=data['Date'])
newdf["win"] = 1 * (newdf["home_score"] > newdf["away_score"]) + 0.5 * (newdf["home_score"] == newdf["away_score"]) $
tmdb_movies.head()
lg = sm.Logit(df_new["converted"], df_new[[ "intercept", "ab_page", "CA", "UK",]]) $ res = lg.fit() $ res.summary()
pd.Series(df_train.values.ravel()).unique()
Trump_week_android = Trump_week[Trump_week["source"] == "Twitter for Android"] $ Trump_week_android.shape
df_train["totals.transactionRevenue"].fillna(0, inplace=True)
com_grp.agg(np.mean)
import lightgbm as lgb $ lgbm_train = lgb.Dataset(X, label = y , free_raw_data = False ) $ n_rounds = 10000
test_df.head()
pd.to_datetime(eth['UnixTimeStamp']).head()
df_copy.info()
for n in fromNodes: $     dot.node(n)
pd_train_filtered['date'] = pd.to_datetime(pd_train_filtered['date']) $ pd_train_filtered['day'] = pd_train_filtered['date'].dt.weekday_name $ pd_train_filtered = pd_train_filtered.drop('date', axis=1)
Measurement = Base.classes.Measurement $ Station = Base.classes.Station
pmol.df['distances'] = distances $ pmol.df.head()
matplotlib.style.use('seaborn')
corn.groups
def hash_tag(text): $     return re.findall(r'(#[^\s]+)', text) $ def at_tag(text): $     return re.findall(r'(@[A-Za-z_]+)[^s]', text)
cc['loglow'] = np.log(cc['low']) $ plt.hist(cc['loglow']) $ plt.show()
food=pd.read_csv("en.openfoodfacts.org.products.tsv", sep="\t", nrows=20000)
from shapely.geometry import Point $ geometry = [Point(xy) for xy in zip(df_journey.lon, df_journey.lat)] $ df = df_journey.drop(['lon', 'lat'], axis=1) $ crs = {'init': 'epsg:4326'} $ geo_df = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)
from imblearn.over_sampling import SMOTE $ train_x, train_y = SMOTE().fit_sample(train_x, train_y)
data.keys()
pickle.dump(final_df,open('../proxy/dataset1','wb'))
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 7)) $ for i, (title, group) in enumerate(calculated_average_df.groupby(DEFAULT_NAME_COLUMN_COUNTRY)): $     group.plot.bar(x='Month', title=title, ax=axes.flat[i])
image_predictions[image_predictions.jpg_url.duplicated()].jpg_url #duplicates $ image_predictions[image_predictions.jpg_url=='https://pbs.twimg.com/media/CdHwZd0VIAA4792.jpg']
df = pd.read_csv(fileIn, encoding = 'utf-8') $ df.head()
for household_name, household_dict in households.items(): $     data_households[household_name] = validate(data_households[household_name], household_name, $                                                household_dict['feeds'], headers, output=verbose)
joined = joined.merge(trend_de, 'left', ["Year", "Week"], suffixes=('', '_DE')) $ joined_test = joined_test.merge(trend_de, 'left', ["Year", "Week"], suffixes=('', '_DE')) $ len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])
print pd.concat([s1, s2, s3], axis=1)
df2 = df.query('(landing_page == "new_page" and group == "treatment") or \ $               (landing_page == "old_page" and group == "control")')
red.shape
%%timeit $ for i in range(10000): $     b = pattern.sub('BB', the_str)
print(len(set(contract_history.INSTANCE_ID) & set(intervention_train.INSTANCE_ID))) $ print(len(set(contract_history.INSTANCE_ID) & set(intervention_test.INSTANCE_ID))) $ print(len(set(contract_history.INSTANCE_ID) & set(intervention_history.INSTANCE_ID)))
x = x.values.reshape(-1, 1) $ y = y.values.reshape(-1, 1) $ linereg = LinearRegression().fit(x, y) $ print('m: ', linereg.coef_[0][0]) $ print('c: ', linereg.intercept_[0])
about.find('a')
loan_stats['loan_status'].isna().sum()
import statsmodels.api as sm $ convert_old = len(df2.query("landing_page=='old_page' and converted=='1'")) $ convert_new = len(df2.query("landing_page=='new_page' and converted=='1'")) $ n_old = len(df2.query("landing_page=='old_page'")) $ n_new = len(df2.query("landing_page=='new_page'"))
test_portfolio.columns
save_n_load_df(joined, 'oil_joined.pkl')
cityID = '018929347840059e' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Indianapolis.append(tweet) 
c['code'] = c['code'].fillna('0000').astype(str) $ c['code'] = c['code'].apply(lambda x: x[:4]).astype(int) $ c['code'].unique()
ptweets = schumer[schumer["screen_name"].isin(popular)] $
for para in soup.find_all('p'): $     print(para.get_text())
f_new[['CA','US']]=pd.get_dummies(f_new['country'])[['CA','US']]
RunSQL(sql_query) $ actor = pd.read_sql_query(sql_query, engine) $ actor.head()
pd.DataFrame({('A', 'a1'): {('Y', 'y1'): 1, ('Y', 'y2'): 2}, $               ('A', 'a2'): {('Y', 'y1'): 3, ('Y', 'y2'): 4}, $               ('A', 'a3'): {('Z', 'z1'): 5, ('Z', 'z2'): 6}, $               ('B', 'b1'): {('Z', 'z1'): 7, ('Z', 'z2'): 8}, $               ('B', 'b2'): {('Z', 'z1'): 9, ('Z', 'z2'): 10}})
a_list.extend(another_list) $ a_list
cls = joined.columns.tolist()
data.describe()
 df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0] $ df.query('group == "treatment" and landing_page != "new_page" or group != "treatment" and landing_page == "new_page" ' ).shape[0]
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $
import os $ import glob $ from ramutils.reports.summary import FRStimSessionSummary $ fr5_session_summary_locations = glob.glob(os.path.join(report_db_location, '*FR5*session_summary*')) $ print("Found {} FR5 or catFR5 session summaries".format(len(fr5_session_summary_locations)))
df['converted'].mean()
high_rev_acc_opps_net.columns
movie_df.head()
df_archive_clean["retweeted_status_id"] = df_archive_clean["retweeted_status_id"].astype(str) $ row_drop = df_archive_clean[df_archive_clean["retweeted_status_id"]!="nan"]["tweet_id"].index $ df_archive_clean.drop(row_drop, inplace=True) $ df_archive_clean["retweeted_status_id"].replace("nan", np.nan, inplace=True)
station_df =pd.read_sql('station',engine) $ station_unique_df = station_df['station'] $ station_unique_df.nunique() $
warm.value_counts()
len(cold_tweets[cold_tweets["tweet"].apply(lambda x: any([str(y) in x.lower() for y in known_places]))])
df_vow.index + timedelta(days=1)
Ralston.mean() $
popularity_clean.info()
df_modeling_categorized = pd.get_dummies(df_modeling, columns=['country', 'currency'])
squares.mean()
df = df.dropna().copy()
df.head()
clf = LinearRegression() $ clf.fit(X_train, y_train) $ confidence  = clf.score(X_test, y_test) $ print("Confidence our Linear Regression classifier is: ", confidence)
daily_unit_df = all_turnstiles.groupby(['STATION','C/A','UNIT','DATE'], as_index=False).sum().drop(['ENTRIES','EXITS'], axis=1) $ daily_unit_df.sample(5)
jail_census.drop("_id", axis=1, inplace=True) $ jail_census
p_old = df2['converted'].mean() $ p_old
beijing = beijing.rename(columns={'WindDirDegrees<br />' : 'WindDirDegrees'})
def is_sorted(l): $     return all(l[i] <= l[i+1] for i in xrange(len(l)-1)) $ for key, grp in part1_flt.groupby(["campaign_id"]): $     print "{0: <20}{1}".format(key, is_sorted(grp.campaign_spend.values))
data.head()
tweets2=pd.read_json(r'C:\Users\shampy\Desktop\project\RedCarpetUp\socialmediadata-tweets-of-congress-november\2017-11-02.json') $ tweets2.shape
hours = appointments.copy()
for follower in tweepy.Cursor(api.friends).items(): $     print(follower.name)
calls_df[calls_df["phone number"]==5600724140].head()
from sklearn.neighbors import KNeighborsClassifier $ knn = KNeighborsClassifier() $ knn.fit(X_train_all, y_train) $ knn.score(X_test_all, y_test)
df = pd.read_csv('combinedtweets.csv', encoding='utf-8') $ df.head()
tlen.plot(figsize=(16,4), color='r');
regular_posters = [(k,count) for k, count in df['Username'].value_counts().iteritems() if count > 3] $ regular_posters = pd.DataFrame(regular_posters, columns= ['Username', 'Number Of Posts'])
df_test['due_date']=pd.to_datetime(df_test['due_date']) $ df_test['effective_date']=pd.to_datetime(df_test['effective_date'])
coins.shape
excutable = utils.download_executable_lubuntu_hs(save_filepath)
weather.date = pd.to_datetime(weather.date, format='%m/%d/%Y')
merged_DA_power_df = pd.read_excel(data_folder_path + '/temp/day_ahead_merge_power.xlsx')
planevisits_df=planevisits.toPandas() $ planevisits_df
df = pd.merge(ffr_recentM, vc_M, left_index=True, right_index=True, how="left") $ print(df.head(6)) $ print("\n\n", df.tail(8))
plt.show()
result=mod.fit()
import pickle $ pickle.dump((fraud_data_updated),open('preprocess.p', 'wb')) $
df2['intercept'] = 1 $ df2[['drop_this', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('drop_this', axis=1) $ df2.head()
random_normal = norm.rvs(size=10000) $ plt.hist(random_normal); $ plt.axvline(x=z_score, color='red'); $ plt.axvline(x=critical_value, color='black');
corr_df = corr_df.sort_values(by= ['date','score'], ascending = [True,True]) $ corr_n = corr_df[corr_df.score<0] $ corr_n.user.value_counts().head(15)
gs_from_model_under.score(X_test, y_test_under)
autos['ad_created'].str[:10].value_counts(normalize = True, dropna = False).sort_index()
master_df.name.value_counts().hist();
xd = pd.get_dummies(x)
obj = pd.Series([7, -5, 7, 4, 2, 0, 4])
temp_long_df = pd.melt(temp_wide_df, id_vars = ['grid_id', 'glon', 'glat'], $                       var_name = "date", value_name = "temp_c") $ temp_long_df.head()
plt.scatter(X2[:, 0], X2[:, 1], c=dayofweek, cmap='rainbow') $ plt.colorbar();
model.doesnt_match("input is lunch he sentence cat".split())
df_new[['US', 'UK', 'CA']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop(['CA'], axis=1)
dfnew1=df.loc[(df['landing_page']=='new_page') & (df['group']=="treatment"),] $ dfnew2=df.loc[(df['landing_page']=='old_page') & (df['group']=="control"),] $ df2=pd.concat([dfnew1,dfnew2],axis=0)   #Concanating the Rows
df_q = pd.read_sql(query, conn, index_col='work_order_id') $ df_q.head(5)
cust_demo.info()
test.isnull().sum()
obs = df_gp_hr.groupby('level_0').mean() $ observation_data = obs['Observation (aspen)']
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count() $ print(commits_per_year.apply(lambda x: x.head())[0:5])
pd.merge(df1, df5)
backup = clean_rates.copy() $ names = clean_rates.text.str.extract(r'(?P<intro>is|Meet|to|named) (?P<name>[A-Z]\w+)(\.| )', expand=True) $ names.loc[names.name.isnull(),'name'] = 'None' $ clean_rates.loc[:, 'name'] = names.loc[:, 'name'].copy()
df.head()
def Info_Dataframe(dataframe): $     return dataframe.info()
xmlData['sold_date'] = pd.to_datetime(xmlData['sold_date'], format = "%Y-%m-%d", errors = 'raise')
extract_nondeduped_cmp.loc[(extract_nondeduped_cmp.APP_SOURCE=='SFL') $                           &(extract_nondeduped_cmp.APPLICATION_DATE_short>=datetime.date(2018,6,1)) $                           &(extract_nondeduped_cmp.app_branch_state=='CA')].groupby('APP_PROMO_CD').size()
date2= plt.plot_date(data=date_retweets2, x="created_at", y="retweet_count") $ plt.xticks(rotation=90) $ ax = plt.subplot() $ ax.xaxis.set_major_formatter(mdates.DateFormatter("%d-%m-%Y")) $
convert_old = df2[(df2["landing_page"] == "old_page") & (df2["converted"] == 1)]["user_id"].count() $ convert_new = df2[(df2["landing_page"] == "new_page") & (df2["converted"] == 1)]["user_id"].count() $ n_old = n_old $ n_new = n_new
print df.shape[0] + noloc_df.shape[0]
def Delta_days(x): $     seconds=x.seconds+x.microseconds/1000 $     total=x.days+seconds/(24*3600) $     return total
session.query(Adultdb).filter_by(education="9th").delete(synchronize_session='fetch')
for c in tqdm_notebook(cols): $     siim[c] = encod.inverse_transform(siim[c])
df3 = df2.merge(df_countries) $ df3.head()
contribs.head()
df_A.iloc[:, lambda x : [0,2]]
full_data.shape
import pandas as pd $ Mars_Facts_URL = 'https://space-facts.com/mars/' $ Mars_tables = pd.read_html(Mars_Facts_URL) $ Mars_tables[0]
mean_days_diff_due_payment = np.mean((data['last_payment_date'] - data['due']).astype('timedelta64[D]')) $ for feature in data_features: $     if np.isnan(feature.get('mean_days_diff_payment_due_date', False)): $         feature['mean_days_diff_payment_due_date'] = mean_days_diff_due_payment
titanic.pivot_table('survived', index='sex', columns='class')
print(df2.query('user_id == 773192'))
cumulative_stats = predictions_pdf.groupby(['predictedLabel']).count() $ product_data = [go.Pie(labels=cumulative_stats.index, values=cumulative_stats['GENDER'])] $ product_layout = go.Layout(title='Predicted product line client interest distribution') $ fig = go.Figure(data=product_data, layout=product_layout) $ iplot(fig)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" & converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" & converted == 1').shape[0] $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
print(AFX_X_06082017_r.json())
train = pd.read_csv(dirs[-2], dtype={"ip":"int32", "app":"int16", "device":"int16", "os":"int16", "channel":"int16"})
def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)
print("Feature types", tipsDF.dtypes)
tweet_archive_enhanced_clean.loc[200,'dog_stage']='floofer'
soup = bs(response.text, "html.parser")
df['fullVisitorId'] = df['fullVisitorId'].astype('str')
df2_countries[['UK','US']] = pd.get_dummies(df2_countries['country'])[['UK', 'US']] $ df2_countries.head() $
sess.get_historical_data('ibm us equity','px last',periodicity='MONTHLY')
df_clean.info()
def cython2(s): $     return s.ewm(com=com,adjust=True).mean()
df_weekly.sort_values(by='retweets_average', ascending=False)
df2 = pd.read_csv('ab_updated.csv')
offer_type = autos.groupby("offer_type").size() $ offer_type
df2.shape[0]
(df.shape[0] - df2.shape[0]) == 3893
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $     filter(Measurement.station == 'USC00519281').all() $
trends_per_year_avg_ord = OrderedDict(sorted(trends_per_year_avg.items()))
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('US', axis=1) $ df_new['intercept'] = 1
help(gwt.hh)  # Start using the gwt.hh function below!!! $ help(gwt.orbitalFreq) $ help(gwt.hhModes)  # Use this one!!!
print("Average item-level delta recall: {:.2f}%".format(session_df['delta_recall'].mean())) $ print("Biggest item-level delta recall: {:.2f}%".format(session_df['delta_recall'].max())) $ print("Biggest adverse item-level delta recall: {:.2f}%".format(session_df['delta_recall'].min()))
dcw = dc.groupby(['YearWeek'], as_index=False).mean() $ tmw = tm.groupby(['YearWeek'], as_index=False).mean() $ dcw.head(5) $ tmw.head(5) $
retweet_pairs["FromType"] = "Person" $ retweet_pairs["ToType"] = "Person" $ retweet_pairs["Edge"] = "Retweeted" $ retweet_pairs.rename(columns={"screen_name":"FromName","retweeted_screen_name":"ToName"},inplace=True) $ retweet_pairs.head(10)
new_prob_mean = new_page_converted.mean() $ old_prob_mean = old_page_converted.mean() $ print(new_prob_mean) $ print(old_prob_mean) $ print(new_prob_mean - old_prob_mean)
stationbydate = data.groupby(['STATION','DATE'])['EntriesDifference'].agg(pd.np.sum) $
print(autos['nrOfPictures'].sum())
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = n_old $ n_new = n_new
NER_raw_comparison = pd.read_csv('Notebooks/NER_vs_raw_strings.csv') $ NER_raw_comparison
stop_words_list = ['flashlight', 'light'] $ pipeline.set_params(posf__stop_words=stop_words_list, cv__stop_words=stop_words_update)
sns.violinplot(x="borough", y="complaint_type",data = samp311)
act_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ p_value = (p_diffs > act_diff).mean() $ print('The proportion of the p_diffs greater than the actual difference observed is: {}.'.format(p_value))
sorted(first_status.keys())
fig, ax = plt.subplots(figsize=(10,6)) $ sns.distplot(data2.agreeableness) $
import datetime $ outdir = "/Users/jeriwieringa/Dissertation/drafts/data/word-lists/" $ with open("{}{}-Base-Word-List-SCOWL&KJV.txt".format(outdir, str(datetime.date.today())), 'w') as outfile: $     for each in spelling_dictionary: $         outfile.write("{}\n".format(each))
df.info() #no: of non null values and no: of rows remains the same and hence it can be concluded that no rows have missing values
df.head()
test_data.loc[(test_data.gearbox == 'manuell') & (test_data.vehicleType.isnull()), 'vehicleType'] = 'kleinwagen'
col = list(X_trainfinal.columns) $ col[2]= '1hr' $ X_trainfinal.columns = col $
np.shape(temp_fine)
csvWriter = csv.writer(csvFile)
automl_feat = pickle.load(open(filename, 'rb'))
doi = "10.1016/S0034-4257(00)00169-3" $ title = "Classification and change detection using Landsat TM data: When and how to correct atmospheric effects?"
metrics.recall_score(testy,predict_y)
df[(df.company == True) & (df.other == True)].index.notin(other)
X_final_test_2 = X_reduced.iloc[indices_test] $ X_training_2 = X_reduced.iloc[indices_train]
to_plot = customer_emails[['Email', 'Days Between Int']] $ plt.title("Actual number of days between purchases") $ sb.distplot(to_plot['Days Between Int'].dropna()) $
df1 = pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
df_imput = MICE(n_imputations=200, impute_type='col', verbose=False).complete(X_mice_dummify.as_matrix())
S_1dRichards.decision_obj.bcLowrSoiH.options, S_1dRichards.decision_obj.bcLowrSoiH.value
df['due_date']=pd.to_datetime(df['due_date']) $ df['effective_date']=pd.to_datetime(df['effective_date']) $ df.head()
table = Table.read("../datasets/catalogs/fermi/gll_psc_v16.fit.gz") $
conditions_counts.index
df.plot(figsize=(16, 9), title='Bitcoin Price 2017-2018')
df_CLEAN1A.info() $
train['created'] = pd.to_datetime(train['created'], format='%Y-%m-%d %H:%M:%S') $ import datetime as dt $ train['day_created'] = train['created'].dt.weekday $ train['month_created'] = train['created'].dt.month $ train['hour_created'] = train['created'].dt.hour
tips.groupby(["sex", "day"]).mean().reset_index()
tfav.plot(figsize=(16,4), label='Likes', legend=True) $ tret.plot(figsize=(16,4), label='Retweets', legend=True)
pd.get_dummies(df2['landing_page']).head()
pd.value_counts(ac['IFI Support']).head(10)
v = portfolio_metrics(pdf) $ p_template.format(v[0], v[1], v[2])
from client.api.notebook import Notebook $ ok = Notebook('hw2.ok') $ _ = ok.grade('q02')
conn = MySQLdb.connect( host=hostname, user=uid, passwd=pwd, db=database ) $ cur = conn.cursor()
Raw_Forecast.to_csv("Master File_CA", encoding="utf-8", index=False)
p = df2[df2['converted']==1]['converted'].count()/df2['converted'].count() $ p
app_counts = df.groupby(['ab_test_group', 'is_application']).count().reset_index() $
bytes_to_write = combined_preds.to_csv(None).encode() $ fs = s3fs.S3FileSystem() $ with fs.open(filepath, 'wb') as f: $     f.write(bytes_to_write)
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Base.classes.keys()
count = np.array([convert_new, convert_old]) $ nobs = np.array([n_new, n_old]) $ z_score, p_value = sm.stats.proportions_ztest(count,nobs, alternative='larger') $ z_score, p_value
np.exp(-1.9865),np.exp(-0.0469), np.exp(0.0314)
soup.find_all(class_='chorus')
df_json_tweets.info()
df[df.converted == 1].user_id.count()/df.shape[0] $ df.converted.mean()
import statsmodels.api as sm $ convert_old = df2[df2['group'] == 'control']['converted'].sum() $ convert_new = df2[df2['group'] == 'treatment']['converted'].sum()
df.at[dates[0],'A']
house_data.describe()
df.head()
df.info() $ print("There are no missing values in the dataset")
df_atr = df_atr.drop(['X'],1)
df.iloc[3:6, 2:3]
full_globe_temp.dropna()
internet.head()
df1=df1[['Adj. Close','volatility','PCT_Change','Adj. Open','Adj. Volume']] $ df1.head()
uber.short_description
df.plot() $ plt.show()
def get_citipy_data(): $     for index, row in cities_df.iterrows(): $         cities_df.loc[index,"City"] = citipy.nearest_city(row["Lat"],row["Lon"]).city_name $         cities_df.loc[index,"Country Code"] = citipy.nearest_city(row["Lat"],row["Lon"]).country_code $     cities_df.drop_duplicates(["City","Country Code"]) $
from sklearn.svm import SVC
aussie_search = api.search(q='%23Aussie') $ len(aussie_search)
geom = [Point(xy) for xy in zip(new_sample['Longitude'],new_sample['Latitude'])] $ crs = {'init':'epsg:4326'} $ threeoneone_geo = gpd.GeoDataFrame(new_sample,crs = crs,geometry=geom)
mnb_best = MultinomialNB(alpha = 0.2, fit_prior = False) $ mnb_best = mnb_best.fit( X_train, y_train )
try: $     import test_package.print_hello_function_container.print_hello_function $ except: $     print("A function cannot be imported directly with dot notation")
pred.p3_dog.value_counts()
com_grp = company.groupby(['Company','Department'])
(autos['date_crawled'] $ .value_counts(normalize=True,dropna=False) $ .sort_index(ascending=True))
mapInfo = refl['Metadata']['Coordinate_System']['Map_Info'].value $ mapInfo
users.head()
from sklearn import tree $ clf = tree.DecisionTreeClassifier(random_state = 100, $                                max_depth=3, min_samples_leaf=5) $ clf.fit(X_train,y_train)
eth = pd.read_csv('data/eth-price.csv', parse_dates=True) $ print(eth.dtypes) $ eth.head()
getTextEN().head()
for d in ['DOB_clean', 'Lead_Creation_Date_clean']: $     Train[d] = map(date.toordinal, Train[d]) $     Test[d] = map(date.toordinal, Test[d])
to_be_predicted_Day3 = 82.26272357 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
core_samples_mask = np.zeros_like(db.labels_, dtype=bool) $ core_samples_mask[db.core_sample_indices_] = True $ core_samples_mask
chefdf = pd.merge(chefdf, chef03df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_03'))
old_page_converted = np.random.choice([0,1] ,size = n_old, p=[(1-p_old), p_old]) $ old_page_converted.mean()
dfEtiquetas.dropna(subset=["created_time"], inplace=True)
import json $ with open('config.json') as config: $     config_file = json.load(config) $ spks = np.loadtxt('output/spikes.csv') $ print(spks[:10, :])
from IPython.display import Image $ url = "https://s3.amazonaws.com/leiwen/dmfa/star_schema.png" $ Image(url=url)
ab_data['user_id'].nunique()
for trigram_sentence in it.islice(trigram_sentences, 350, 370): $     print u' '.join(trigram_sentence) $     print u''
before_sherpa = df.loc[df["index"] <= 1685.0] $ after_sherpa = df.loc[df["index"] > 1685.0] $
df = pd.read_sql_query('SELECT ComplaintType, Descriptor, Agency ' $                        'FROM data ' $                        'WHERE Agency = "NYPD" ' $                        'LIMIT 10', disk_engine) $ df.head()
baseball_swing_action_type_id = 1 $ url = form_url(f'actionTypes/{baseball_swing_action_type_id}/eventMarkerTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_status(response)
flight.show(100)
automl = pickle.load(open(filename, 'rb'))
df2.drop(1899, inplace=True) $ df2[df2.duplicated(subset="user_id", keep=False)]
p_new=df2.query('converted == 1').user_id.nunique()/df2['converted'].count() $ p_new
def get_last_time_series_val(row): $     flight_dt = row['flight_dt'] - timedelta(hours=1) $     return int(flight_dt.strftime("%Y%m%d%H")) $ feature_df['flight_ts'] = feature_df.apply(get_last_time_series_val, axis=1)
print(all_complaints['Descriptor'].nunique())
hour = trump["est_time"].astype(str).str.slice(11,13).astype(int) $ minute = trump["est_time"].astype(str).str.slice(14,16).astype(int) $ second = trump["est_time"].astype(str).str.slice(17,19).astype(int) $ trump['hour'] = hour + minute/60 + second/60**2
poverty.drop([386, 387, 388, 389, 390], inplace=True)
vals = data.value $ vals
dfd.query("zones == 'Multi'").hspf.hist()
transactions.join(users.set_index('UserID'), on='UserID')
temp_cat.get_values()
timezones = DataSet['userTimezone'].value_counts()[:10] $ print(timezones)
df_all_repaid_latest.index.names=['date','rating_base']
lm = sm.OLS(df2['converted'], df2[['intercept', 'ab_page']]) $ results = lm.fit() $ results.summary()
stations = session.query(Measurement).group_by(Measurement.station).count() $ stations
tweets_df.isnull().sum()
plt.figure(figsize=(20,6)); $ plt.plot(df['MeanFlow_cfs']); $ plt.axvline(x='1980-01-01',color='red',ls='--'); $ plt.title("Neuse River Near Goldsboro, NC"); $ plt.ylabel("Discharge (cfs)");
answer2={} $ for lis in answer1: $     answer2[answer1.index(lis)]=lis
df = pd.read_sql('SELECT COUNT(*) FROM address;', con=conn) $ df
s = pd.Series(['a_b_c', 'c_d_e', np.nan, 'f_g_h'])
season08 = ALL[(ALL.index >= '2008-09-04') & (ALL.index <= '2009-02-01')]
X = Florida['Description'] $ stop = set(stopwords.words('english'))
df2.head()
df.head()
df10 = pd.read_csv('2010.csv')
menus_dishes_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/menu_dishes.csv')['Body'].read().decode('utf-8') $ menu_dishes = pd.read_csv(StringIO(menus_dishes_csv_string), header=0)
df = pd.read_json('/Users/eampo/Desktop/InfinityWars/infinitywars.json', lines = True)
from templates.invoicegen import create_invoice
min_x = openmc.XPlane(boundary_type='reflective', x0=-0.63) $ max_x = openmc.XPlane(boundary_type='reflective', x0=0.63) $ min_y = openmc.YPlane(boundary_type='reflective', y0=-0.63) $ max_y = openmc.YPlane(boundary_type='reflective', y0=0.63)
dem_in = dem.apply(within_area,axis=1) $ dem = dem.loc[dem_in] $ dem["grid"]= dem.apply(build_grid_index,axis=1)
df.shape
twitter_count = pd.concat([twtter_count1, twtter_count2, twtter_count3], axis=1).fillna(0).apply(lambda x: np.sum(x), axis=1)
from IPython.display import Image $ Image(url=csv_df[csv_df['timestamp'] == csv_df['timestamp'].min()]['url'].values[0], width=600)
cust_data.loc[:,'ID'].head(2)
iris.drop(['Id'], inplace=True, axis=1) $ print(iris.shape)
eval_RGF_tf_tts = clf_RGF_tf.score(X_testcv_tf, y_testcv_tf) $ print(eval_RGF_tf_tts)
out = conn.addtable(table='crops', caslib='casuser', $                     **exceldmh.args.addtable) $ out
df2.converted.mean()
ser5.loc["a"]
len(twitter_archive_df[pd.isnull(twitter_archive_df['expanded_urls'])])
df2.drop(labels=1899, axis=0, inplace=True)
missing_sample = pd.read_csv("../data/microbiome/microbiome_missing.csv", $                              na_values=['?', -99999], nrows=20) $ missing_sample
df.head(1)
sm.stats.proportions_ztest([converted_new, converted_old],[n_new, n_old],alternative ='larger')
df3 = df / df.iloc[0, :] $ df3.plot() $ plt.show()
data = ['peter', 'Paul', None, 'MARY', 'gUIDO'] $ for s in data: $     print(s.capitalize())
sim_closes = closes_aapl.iloc[-1].AAPL*np.exp(sim_ret.cumsum()) $ sim_closes
reduced = TSNE(n_components=2).fit_transform(lsi_out[list(range(50))])
new_page_converted.sum()/len(new_page_converted)-old_page_converted.sum()/len(old_page_converted)
model.most_similar("man") $
import statsmodels.api as sm $ lm = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = lm.fit() $
from itertools import islice $ list(islice(answer2.iteritems(), 2))
from sklearn.linear_model import LinearRegression $ LR = LinearRegression() $ LR.fit(X_train, y_train) $ pred = LR.predict(X_test)
decision_tree = DecisionTreeClassifier() $ decision_tree.fit(X_train, Y_train) $ Y_pred = decision_tree.predict(X_test) $ acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2) $ acc_decision_tree
merged_df_flight_cancels = pd.concat([merged_df_flightdelays, flight_cancels], axis=1)
wed11.shape
def filter_html_tags(text): $     return re.sub('</?[a-z]{1,11}>', '', text) $ filter_html_tags('<jkdjdksakd>hi</img>')
s5 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd']) $ print(s5) $ s6 = pd.Series([4, 3, 2, 1], index=['d', 'c', 'b', 'a']) $ print(s6)
count_bldg_opps.head()
trains_fe1=pd.merge(trains_fe,users_fin) $ trains_fe1.head()
dtypes={'store_nbr':np.int64,'city':np.str,'state':np.str,'type':np.str,'cluster':np.int64} $ stores = pd.read_csv('stores.csv',dtype=dtypes) # opens the csv file $ print("Rows and columns:",stores.shape) $ pd.DataFrame.head(stores)
df.rolling(window=3).mean()
archive_clean['name'].replace('the', np.nan, inplace=True) $ archive_clean['name'].replace('a', np.nan, inplace=True) $ archive_clean['name'].replace('None', np.nan, inplace=True) $ archive_clean['name'].replace('an', np.nan, inplace=True) $ archive_clean['name'].replace('my', np.nan, inplace=True)
model_name = "300features_40count_10context" $ model.save(model_name)
pd.Series(bnb.first_affiliate_tracked).value_counts()
corr_matrix = telecom3.corr().abs() $ upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) $ to_drop = [column for column in upper.columns if any(upper[column] > 0.80)] $ print(to_drop) $ telecom3 = telecom3.drop(set(to_drop), axis=1)
df_mes[(df_mes['mta_tax']!=0.5)&(df_mes['mta_tax']!=0)].shape[0]
gain_df.corr()
print "modelType: " + saved_model.meta.prop("modelType") $ print "trainingDataSchema: " + str(saved_model.meta.prop("trainingDataSchema")) $ print "creationTime: " + str(saved_model.meta.prop("creationTime")) $ print "modelVersionHref: " + saved_model.meta.prop("modelVersionHref") $ print "label: " + saved_model.meta.prop("label")
le.fit(df2['E'])
ms_df = mt_df[mt_df['mortality_salience'] == 1] $ print("HI tweets between 18:07-18:21 AM 2019-01-13 labelled as mortality_salience: {}".format(len(ms_df))) $ print("percent mortality salience: {}".format(len(ms_df)/len(mt_df)))
recipes.iloc[135598]['ingredients']
appleinbounds.to_csv('../data/allAppleNeg.csv')
daily_averages['2014-10'].head()
sq_index = df.query('full_sq<life_sq').index
flight = spark.read.json(jsonl_file_name) $ flight.printSchema()
tweets['text'] = tweets['fixed'] $ tweets.drop('fixed', 1, inplace=True)
df.groupby("cancelled")["new_customer"].mean()
sum(tw_clean.expanded_urls.isnull())
print pd.pivot_table(data=df, $                      index='date', $                      columns='item', $                      values='status', $                      aggfunc='mean')
for i in image_predictions_rows: $     if i in list(image_predictions_clean.index): $         print("Failed, {} still in dataframe".format(i)) $         break
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','treatment']]) $ res = logit.fit()
df_geo_unique_asn = df_geo_unique[['a_saddr_asn','a_saddr_asorg','a_saddr_lat','a_saddr_long', $                                    'b_saddr_asn','b_saddr_asorg','b_saddr_lat','b_saddr_long']].drop_duplicates()
from wordcloud import WordCloud $ wordcloud = WordCloud().generate(''.join(negativeTweets['Tweets'])) $ import matplotlib.pyplot as plt $ plt.imshow(wordcloud, interpolation='bilinear') $ plt.axis("off")
data.head()
trip_data_sub = trip_data_sub.drop("Ehail_fee", axis = 1)
z_values, _ = vae_latent(nn_vae, mnist_test_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ recon_x = vae_recon(nn_vae, z_values) $ plot_mnist_sample(recon_x)
df_temperature = df_temperature.replace('M', 0) $ df_precipitation = df_precipitation.replace('M', 0)
data.pivot(columns="center_name", values="attendance_count").head()
conn.rollback() $ c.execute('SELECT * FROM cities') $ print(c.fetchall())
askreddit['num_comments'].value_counts().sort_index(ascending=False)[:10]
category_dummies = pd.get_dummies(aldf['category']) $ skill_dummies = aldf['skills'].str.join(sep='*').str.get_dummies(sep='*') #Wow what magic I wish I can remember this somewhere
autos.head()
rng.tz_localize('Europe/Berlin')
def prep_roc(ctable,predname,predp): $ prep_roc("nn_scored","_NN_PredName_","_NN_PredP_") $ prep_roc("HMEQ_SCORED_GB","_GBT_PredName_","_GBT_PredP_") $ hmeq_scored_gb.head() $
tw_clean = tw_clean.drop(tw_clean[tw_clean.expanded_urls.isnull()].index)
high_ranked_question_threshold = np.percentile(questions_scores, 90) $ high_ranked_question_threshold
df4 = df2.query('group == "treatment"') $ len(df4.query('converted == 1'))/len(df4)
batting_and_salary[['home_runs','salary']].head(10)
jpl_logo_href = jpl_soup.find_all('div', class_='jpl_logo') $ print(jpl_logo_href)
early = df['created_at'].min() $ recent = df['created_at'].max() $ print("the earliest tweet of this set is {} and the most recent is {}.".format(early,recent))
zip_1_df = two_zips_df[two_zips_df.Zip==70117] $ zip_1_df = zip_1_df.groupby(zip_1_df.TimeCreate).size().reset_index() $ zip_2_df = two_zips_df[two_zips_df.Zip==70119] $ zip_2_df = zip_2_df.groupby(zip_2_df.TimeCreate).size().reset_index()
twitter_archive_clean.info()
df2 = df2.drop(1899, axis=0)
p_new = df2[df2['converted']==1]['user_id'].count() / df2.shape[0] $ p_new
def duration(time_dur): $     diff_in_secs =  time_dur.total_seconds() $     return round(diff_in_secs,2)
date_df.iloc[0]['date'].strftime('%A')
df2_clean.jpg_url.duplicated().value_counts()
compound_final.head()
eval_data = pd.concat([predictions, actuals]) $ eval_data['day'] = eval_data.index
data['update']= [datetime.strptime(x , '%d/%m/%Y %H:%M:%S') for x in data['update']]
bruins_postgame.to_csv('../../../../../CS 171/cs171-gameday/data/bruins_postgame.csv',   index=False) $ celtics_postgame.to_csv('../../../../../CS 171/cs171-gameday/data/celtics_postgame.csv', index=False) $ sox_postgame.to_csv('../../../../../CS 171/cs171-gameday/data/sox_postgame.csv',         index=False)
output2 = (act_diff < p_diffs).mean() $ print('Proportion of greater than actual difference: {}%'.format(output2*100)) $
goo = pd.read_csv('data/goog.csv', encoding='utf_8')
stat, p, med, tbl = scipy.stats.median_test(df2["tripduration"], df4["tripduration"]) $ print p $ print tbl
df2['intercept']=1 $ df2[['control','treatment']]  = pd.get_dummies(df2['group']) $ df2.rename(columns={'treatment':'ab_page'},inplace=True) $ df2.head()
pd.timedelta_range(0, periods=10, freq='H')
highest_open=df['Open'].max()
MATTHEWKW['matthew160'] = MATTHEWKW.text.apply(lambda text: pd.Series([x in text for x in MATTHEW_WORDS_160]).any()) $ MATTHEWKW['matthew92']  = MATTHEWKW.text.apply(lambda text: pd.Series([x in text for x in MATTHEW_WORDS_95]).any())
hp.save('new_houseprint.pkl')
df_clean.head(5)
df_merge = pd.merge(archive_clean, tweet_clean, on = 'tweet_id', how = 'inner' )
index_missing = taxi_hourly_df.loc[(taxi_hourly_df.missing_dt == True) & $                                                     (taxi_hourly_df.num_passengers.isnull()) , : $                                                    ].index
for row in selfharmm_topic_names_df.iloc[3]: $     print(row)
cashflows_act_investor_EOM[(cashflows_act_investor_EOM.id_loan==34) & (cashflows_act_investor_EOM.fk_user_investor==38)].to_clipboard()
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://www.tensorflow.org/images/softmax-nplm.png") $
print 'Pecentage of total amount from donations with no location: ', 100*sum(df[(df.city == '') & (df.state == '') & (df.zipcode_initial == '')].amount)/sum(df.amount)
dummies = pd.get_dummies(plate_appearances['events']).rename(columns=lambda x: 'event_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)
tweets = api.statuses_lookup([str(deep_learning_tweet_id), deep_learning_tweet_id]) $ deep_learning_tweet2 = tweets[0]
pred = gbm_model.predict(test)
df2.landing_page.value_counts()[0] / df2.shape[0]
knn = KNeighborsClassifier(n_neighbors=5) $ knn.fit(X_train, y_train) $ knn.score(X_test, y_test)
run txt2pdf.py -o "SOUTHCOAST HOSPITAL GROUP, INC  Sepsis.pdf"   "SOUTHCOAST HOSPITAL GROUP, INC  Sepsis.txt"
words = [w for w in words if not w in stopwords.words("english")] $ print (words)
trn_y.mean(), val_y.mean()
p_old = df2['converted'].value_counts()[1]/len(df2) $ p_old
filled.dropna(axis=1)  # each column containing NaN is dropped
shows3.shape
df.tail()
df.join(d).head(10)
p_old = df2['converted'].mean() $ print (p_new)
scoring_url = client.deployments.get_scoring_url(deployment_details) $ print(scoring_url)
rng = pd.date_range(start = '1/1/2017', periods = 72, freq = 'B') $ rng
urls = GetPositionLinks.Position('passing').player_links(2015, 2017)
profits.to_csv('profits.csv')
parsed_locations = raw.location.str.extract(r'(?P<country>[\w ]+), (?P<city>[\w ]+) ! \2, \1', expand = True) $ parsed_locations.assign(original = raw.location).sample(5)
y.loc[931], y.loc[1582]
gender.plot.bar() $ plt.title("Students per Gender", fontdict={'fontsize': 14}); $ plt.savefig('images/barplot_gender.png')
nfl.build_payload(kw_nfl, geo="US", timeframe="2016-01-01 2016-09-01") #2016-09-01 2016-12-31 $
gear_box_dict = {"manuell": "manual", "automatik": "automatic"} $ autos["gear_box"].replace(gear_box_dict,inplace=True) $ autos["gear_box"].fillna("not_specified", inplace=True)
mod_model.xls2model(new_version='new', annotation=None) $ scenario = mod_model.model2db() $ scenario.solve(model='MESSAGE', case='GHD_hospital')
vip_category = pd.pivot_table(vip_df, values = 'Payment', index = ['Month'], columns = ['Category'], aggfunc = 'count')
bnbAx.timestamp_first_active.plot.hist()
sfs1.k_feature_idx_
daily_hashtag = english_df.select( $     functions.month(english_df.date).alias('month'), $     functions.weekofyear(english_df.date).alias('week'), $     functions.dayofyear(english_df.date).alias('day'), $     english_df.hashtag)
gcp_credentials = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX' $ destination = './gcp_credentials.json' $ download_file_from_google_drive(gcp_credentials, destination)
df.loc[0]
store_items.interpolate(method='linear', axis=0)
from nltk.stem.snowball import SnowballStemmer $ stemmer = SnowballStemmer("german")
chambers.rename(columns = {'index': 'state'}, inplace = True)
merged_portfolio_sp = pd.merge(merged_portfolio, sp_500_adj_close, left_on='Acquisition Date', right_on='Date') $ merged_portfolio_sp.head()
csvData.head()
shannon_sakhalin_relative = shannon_sakhalin / np.log2(len(sakhalin_freq)) $ shannon_sakhalin_relative
print('Given that an individual was in the \'control\' group, the probability they converted is 0.120386')
x = cat_outcomes.filter(items=['sex_upon_outcome', 'breed', 'color', 'coat_pattern', $                                'domestic_breed', 'dob_month', 'age_group', 'outcome_month', $                                'outcome_weekday', 'outcome_hour', 'Cat/Kitten (outcome)'])
trainwdummies = pd.concat([train, dummy_categories], axis=1) $ trainwdummies = trainwdummies.drop('Unnamed: 0', axis=1).copy() $ trainwdummies
irradiance_clear_df = irradiance_clear_df.loc['2018-07-08 07:00' : '2018-07-08 22:00'] $ irradiance_cloudy_df = irradiance_cloudy_df.loc['2018-07-12 07:00' : '2018-07-12 22:00']
df5_lunch = df5.between_time('11:00:00', '13:00:00') # one of my favourite methods so far in pandas $ df5_lunch 
pres_df['split_location_tmp'].head()
validation.analysis(observation_data, simple_resistance_simulation_0_25)
sns.barplot(x='user',y='number_of_likes',data=most_liked_tweets.tail(5))
def tvd(dist1, dist2): $     return 0.5*(np.sum(np.abs(dist1 - dist2)))
track_generator = openmoc.TrackGenerator(openmoc_geometry, num_azim=32, azim_spacing=0.1) $ track_generator.generateTracks() $ solver = openmoc.CPUSolver(track_generator) $ solver.computeEigenvalue()
logm = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']]) $ logres = logm.fit()
df_users_6.shape
files = glob.glob(os.path.join(input_folder, year_string))
df_lineup = df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')") $ print("{} times the new_page and treatment don't line up.".format((df_lineup.shape[0]) )) $
train.columns
av = lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_data_filter_settings['ntot_winter'].allowed_variables $ lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_data_filter_settings['ntot_winter'].settings.df[av]
n_new = df2[df2['landing_page'] == 'new_page']['user_id'].count() $ print('n_new: ', n_new)
plt.boxplot(train_data['price'].values)
df.columns
 result = db.profiles.create_index([('user_id', pymongo.ASCENDING)], $                                   unique=True) $  sorted(list(db.profiles.index_information()))
(df2.query('group == "control"')['converted'] == 1).sum()/(df2.query('group == "control"')['converted'] >= 0).sum()
import pandas_datareader.wb as wb $ all_indicators = wb.get_indicators() $
store_items.dropna(axis=0)
autos["odometer_km"].unique().shape
Test.dtypes
All_tweet_data_v2=pd.melt(All_tweet_data_v2, id_vars=Remaining_columns, value_vars=stages, var_name="Remove", value_name='Stage')
for col in ['bottle_vol_ml', 'bottles_sold', 'sale_dollars', 'volume_sold_lt', $             'state_bottle_retail', 'state_bottle_cost']:    $     df = iowa[(iowa[col] < np.mean(iowa[col]) + 3 * np.std(iowa[col])) $               & (iowa[col] > np.mean(iowa[col]) - 3 * np.std(iowa[col]))]
import torch $ import torch.autograd $ from torch.autograd import Variable
data.order_date.min(),data.order_date.max()
page.text
recoveries[recoveries.fk_loan==921]
df.groupby("same_location")["cancelled"].mean()
y_axis = output['retweeted_status.retweet_count'].astype(int).values.tolist() $ print (y_axis)
print(pd.to_numeric(countdf['number_ratings']).sum()) $ print(pd.to_numeric(countdf['number_ratings']).sum()-pd.to_numeric(count1df['number_ratings']).sum()) $ print(pd.to_numeric(countdf['number_ratings']).sum()-pd.to_numeric(count6df['number_ratings']).sum())
match_results.head(3)
U_B_df.cameras[0]
import ogh $ homedir = ogh.mapContentFolder(str(os.environ["HS_RES_ID"])) $ print('Data will be loaded from and save to:'+homedir)
hp.search_sensors(type='electricity', direction='Import')
coords = merge[["RA", "Dec"]].apply(to_skycoord, axis=1)
print('.text = ', tweet.text, '\n') $ print('.lang = ', tweet.lang, '\n') $ print('.favorite_count = ', tweet.favorite_count, '\n') $ print('.source_url = ', tweet.source_url, '\n')
train.reset_index(inplace=True) $ train.drop('index',axis=1,inplace=True) $ train.head()
from scipy import stats $ chi2, p, dof, expected = stats.chi2_contingency(ct.values) $ p
df_goog = df_goog.set_index('Date')
for category in page.categories() : $     print (category.title())
allFiles = glob.glob('./data/geocoded_loc/geocode_*.csv') $ dataframes = (pd.read_csv(f, index_col=None, dtype=object) for f in allFiles) $ df_geo = pd.concat(dataframes, ignore_index=True).drop('Unnamed: 0', axis=1)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],value=None, alternative='smaller', prop_var=False) $ z_score, p_value
error_as_pandas_df = errors.as_data_frame(use_pandas = True) $ print(error_as_pandas_df.head()) $ %matplotlib inline $ import matplotlib.pyplot as plt $ plt.plot(error_as_pandas_df, 'ro')
gender = list(df_members['gender']) $ gender_null=['not given' if x is np.nan else x for x in gender] $ gender_counts = Counter(gender_null) $
secclintondf = clintondf[mask_H] $ secclintondf = secclintondf.reset_index(drop=True)
pst.parameter_data
accident_counts_per_segment = df.groupby('segment_id').size().reset_index(name='accident_counts').set_index('segment_id')
df.columns
val.shape
writer.save()
tokendata = pd.merge(token_sendReceiveAvg_month,token_send_add_receiveAvg_month,how="left",on="ID")
twitter_df.head(2)
x = store_items.isnull().sum().sum() $ print(x)
with open('./data/processed/y_train.pkl', 'rb') as picklefile: $     y_train = pickle.load(picklefile)
words, values = list(zip(*new_words.items())) $ data = {'word':words, 'weight1':values} $ new_df = pd.DataFrame(data, columns=['word', 'weight1']) $ new_df.head(5)
n_bandits = gb.size().shape[0] $ test_labels = [i for i in gb.size().index] $ mcmc_iters = 25000
len(df_enhanced.query('retweeted_status_id != "NaN"'))
save_n_load_df(joined, 'joined_stores.pkl')
type2017.isnull().sum() 
import re $ regex_1 = re.compile('\w[A-Z]+')
tweets = tweets.loc[tweets['created_at'] > startDay]
a = 1.23456 $ print(a) $ print(f'{a:.2f}')  # Float restricted to two decimal places $ print(f'{a:06.2f}')  # Float restricted to two decimal places and padded with leading zeroes if less than 6 chars
df.groupby(('C/A', 'UNIT', 'SCP', 'STATION', 'DATE')).sum() $ df
after.head()
np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas) $ np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)
look_back_dt = dt.datetime.strptime(valid_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1) $ valid = energy.copy()[(energy.index >=look_back_dt) & (energy.index < test_start_dt)][['load']] $ valid.head()
index_replies = pd.Series(clean_archive[np.isnan(clean_archive['in_reply_to_status_id']) != True].index) $ index_retweets = pd.Series(clean_archive[np.isnan(clean_archive['retweeted_status_id']) != True].index) $ merged_indexes = pd.concat([index_replies,index_retweets]) $ clean_archive.drop(merged_indexes, inplace=True) $ clean_archive.reset_index(drop=True, inplace=True)
zone_df = zone_df.merge(tz_tmp,on='zone_id')
lm=sm.Logit(df2['converted'], df2[['intercept', 'CA', 'UK']]) $ results=lm.fit() $ results.summary()
seqsize = subs.user_id.value_counts(normalize=False)[0] $ print ("the longuest sequence is %d accepted submissions long" % seqsize )
%%script false $ data = get_dummy(ibm_hr_final2, categorical_no_target ,numerical, label) $ data.show(5)
df2.rename(columns={'activity_date_time_c': 'median_activities_per_gallery'}, inplace=True) $ df2.groupby(df2.index).median()
df_finlexs = {} $ df_finlex = pd.read_csv('l2_lexicon.csv', delimiter=';') $ df_finlex.sample(5, random_state=0)
from nltk.corpus import stopwords $ portuguese_stop_words = stopwords.words('portuguese')
rent_db2.boxplot(column='price')
glm_binom_v1 = H2OGeneralizedLinearEstimator( $     model_id='glm_v3', $     solver='L_BFGS', $     family='binomial') $ glm_binom_v1.train(covtype_X, covtype_y, training_frame=train_b, validation_frame=valid_b)
ts_mean = ts.resample(sampling, how='mean') $ ts_mean = pd.concat([nposts, ts_count, ts_mean], axis=1)
df2[df2.duplicated(['user_id'], keep=False)]['user_id'].unique()
cluster_pd.head(22)
df = df[df["current_state"].isin(["finished", "cancelled", "no_showed", "started", "payment_declined_cancelled"])]
more_grades.columns = ["name", "month", "grade"] $ more_grades["bonus"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0] $ more_grades
jobs.MAXCPUS = pd.to_numeric(jobs.MAXCPUS)
pd.DataFrame.from_dict(sbs.get_metric_dict()).T
df.groupby('episode_id').count().head()
n_old = df2.query('group == "control"').count()[0] $ print(n_old)
OldPage = np.random.choice([1, 0], size=old, p=[pMean, (1-pMean)]) $ old_avg = OldPage.mean() $ print()
reviews_w_sentiment=reviews_w_sentiment[reviews_w_sentiment.comments != ' '] $ reviews_w_sentiment[reviews_w_sentiment['date']>='2018-01-01'].head() $ len(reviews_w_sentiment)
df_con.shape
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
df4.head()
bonus = pd.DataFrame(d) $ bonus.columns = ['Date', 'Increment', 'Total_Score']
training_pending_ratio.shape
plt.figure(figsize=(8, 5)) $ plt.scatter(train_df.favs_lognorm, train_df.comments); $ plt.title('The distribution of the favs_lognorm and number of the comments'); $
frames_liberia = [first_values_liberia, last_values_liberia, count_liberia] $ result_liberia = pd.concat(frames_liberia, axis=1) $
data_for_model = pd.read_pickle('data_for_model') $ df_modeling = data_for_model.copy(deep=True)
cust_demo.memory_usage()
dftestdum = trainsparsify(pd.get_dummies(dftest[testcols].astype(str))) $ print(dftestdum.shape)
y_pred = rf.predict(X_test)
stringIndexer_label = StringIndexer(inputCol="PRODUCT_LINE", outputCol="label").fit(df_data) $ stringIndexer_prof = StringIndexer(inputCol="PROFESSION", outputCol="PROFESSION_IX") $ stringIndexer_gend = StringIndexer(inputCol="GENDER", outputCol="GENDER_IX") $ stringIndexer_mar = StringIndexer(inputCol="MARITAL_STATUS", outputCol="MARITAL_STATUS_IX")
np.exp(-0.0099), np.exp(-0.0506)
df = pd.merge(dfleavetimes, dftrips, how='left', on=['TripID', 'DayOfService'], suffixes=('_leave', '_trips'))
year14 = driver.find_elements_by_class_name('yr-button')[13] $ year14.click()
n_old = df2.query("landing_page == 'old_page'").shape[0] $ n_old
from sklearn.model_selection import cross_val_score, cross_val_predict $ from sklearn import metrics
weather_warm['Station Name'].nunique()
autos["odometer"].head()
df2.query("group=='treatment'").converted.mean()
distance_from_sun = [149.6, 1433.5, 227.9, 108.2, 778.6] $ planets = ['Earth','Saturn', 'Mars','Venus', 'Jupiter'] $ dist_planets = pd.Series(data = distance_from_sun, index = planets) $ time_light = dist_planets / 18 $ close_planets = time_light[time_light < 40] $
uber_15["month"].value_counts()
pd.date_range(start, periods=10, freq='1D10U')
def calc_mom(price, lookback, lag): $     mom_ret = price.shift(lag).pct_change(lookback) $     ranks = mom_ret.rank(axis=1, ascending=False) $     demeaned = ranks.subtract(ranks.mean(axis=1), axis=0) $     return demeaned.divide(demeaned.std(axis=1), axis=0)
df2.iloc[5,3]=np.inf
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_axial_chord_saddle[1][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_saddle[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
prs.to_csv("prs2.csv") $ pr_comments.to_csv("pr_comments2.csv") $ issues.to_csv("issues2.csv") $ issue_comments.to_csv("issue_comments2.csv")
tweet1 = result[1] #get the data of the first tweet... $ dir(tweet1)
slist = [s1, s2, s3] $ for item in slist: $     item.name == 'NAME': $
adopted_cats.loc[adopted_cats['Color']=='Chocolate','Color'] = 'Brown' $ adopted_cats.loc[adopted_cats['Color']=='Chocolate Point','Color'] = 'Brown Point' $ adopted_cats.loc[adopted_cats['Color']=='Chocolate/White','Color'] = 'Brown/White' $ adopted_cats.loc[adopted_cats['Color']=='Chocolate Point/White','Color'] = 'Brown Point/White'
autos['price'].value_counts().sort_index(ascending=True).head(10)
data['len']  = [len(x.text) for x in tweets]  # number of characters in a tweet (think length) $ data['ID']   = [x.id for x in tweets] $ data['Source'] = [x.source for x in tweets] $ data['Likes']  = [x.favorite_count for x in tweets] # favorite_count $ data['RTs']    =  [x.retweet_count for x in tweets]  # retweet count
S_distributedTopmodel.initial_cond.filename
df_byzone.drop('time_stamp', axis = 1, inplace = True)
names = pd.Series(['Alice', 'Bob', 'Carol']) $ phones = pd.Series(['555-123-4567', '555-987-6543', '555-245-6789']) $ dept = pd.Series(['Marketing', 'Accounts', 'HR']) $ staff = pd.DataFrame({'Name': names, 'Phone': phones, 'Department': dept})  # 'Name', 'Phone', 'Department' are the column names $ staff
gs.best_score_
display('df', "df.groupby(df['key']).sum()")
bus['zip_code'] = bus['postal_code'].str[:5] $ bus['zip_code'].value_counts(dropna=False)
twitter_master1.info()
df2.drop(df.index[1899], inplace=True)
df2[df2.duplicated(subset = ['user_id']) == True]
df_archive_clean[["timestamp","retweeted_status_timestamp"]].info()
train_cols = ["Bachelor's degree","High school degree","Master's degree","PhD" $               ,"General interest in the topic (personal growth and enrichment)","Grow skills for my current role" $               ,"Help move from academia to industry","Help prepare for an advanced degree" $               ,"Start a new career in this field"] $ logit = sm.Logit(data['submitted'],data[train_cols])
def day_of_week(date): $     days_of_week = {0: 'monday', 1: 'tuesday', 2: 'wednesday', 3: 'thursday', 4: 'friday', 5: 'saturday', 6: 'sunday'} $     return days_of_week[date.weekday()] $ day_of_week(lesson_date)
adj_close_start = adj_close[adj_close['Date']==end_of_last_year] $ adj_close_start.head()
def to_seconds(time): $     return time / np.timedelta64(1, 's')
pk_planes = hc.table('asm_wspace.parking_planes') \ $ .where("pk_start between '{0}' and '{1}'".format(start_date, end_date))
Google_stock.isnull().any()
last_seen =  max(df.lastSeen) $ last_seen
Google_stock.head()
all_tweets = df_Tesla['text'].values $ print(all_tweets[2])
users.head(20)
df = pd.read_sql('SELECT * FROM customer ORDER BY title, member_credit', con=conn_b) $ df
tweet_text1 = tweets1['text'].values $ clean_text1 = [preprocess_text(x, fix_unicode=True, lowercase=True, no_urls=True, no_emails=True, no_phone_numbers=True, no_currency_symbols=True, $                               no_punct=True, no_accents=True) $               for x in tweet_text1]
people.query("age > 30 and pets == 0")
landing_b = df[(df.destination_landing == 'landing_b')]
data.shape
stores_df[['state']].drop_duplicates()
pold = df2['converted'].mean() $ print(pold)
sample.asfreq("H")
s.pct_change()
top_membership = df.sort_values("members", ascending=False).reset_index(drop=True) $ top_membership.head(5)
logs.date
df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True) $ yl = np.log(y)
lst = "University of Pretoria".split() $ for i in lst: $     print(i)
cnf_matrix = confusion_matrix(y_test, yhat_SVM, labels=['PAIDOFF','COLLECTION']) $ np.set_printoptions(precision=2) $ print (classification_report(y_test, yhat_SVM)) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')
lr_all_user[lr_all_user.id_loan_request==120973].T.to_clipboard()
zero_rev_acc = accounts[accounts[' Total BRR '] == 0] $ zero_rev_acc_opps = pd.merge(zero_rev_acc, opportunities_not_lost,\ $                                     on=['Account ID'],\ $                                     how='inner').sort_values(by='Building ID', ascending=False)
try: $     'my string'.index('x') $ except Exception as error_message: $     print(error_message)
final_log_mod = sm.Logit(df3['converted'], df3[['intercept', 'new_page', 'UK_new_page', 'US_new_page', 'UK', 'US']]) $ final_results = final_log_mod.fit() $ final_results.summary()
df2.drop(index=2893,axis=0,inplace=True)
new_df['weight1'].apply(lambda x:float(x))
top_10_authors = git_log.author.value_counts().nlargest(10) $ top_10_authors
sample=train.head(100).copy() $
cv.get_feature_names()
df2 = df2.drop('treatment', axis = 1) $ df2.head()
wikis[["arwiki", "jawiki", "zhwiki"]].plot();
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\metrics.sas7bdat" $ df = pd.read_sas(path) $ df.head(5)
p_new_sample = new_page_converted.mean() $ p_old_sample = old_page_converted.mean() $ p_new_sample - p_old_sample
offseason12 = ALL[(ALL.index > '2012-02-05') & (ALL.index < '2012-09-05')]
df_template = pd.DataFrame(index=datetimes) $ df_template.index.name = 'dt'
df2[df2.duplicated(['user_id'],keep=False)].index[0]
inspector = inspect(engine) $ inspector.get_table_names()
tweets[0]._json
%%bash $ grep -A 15 "add_engineered(" taxifare/trainer/model.py
r = requests.get('http://www.contextures.com/SampleData.zip') $ ZipFile(io.BytesIO(r.content)).extractall()
session.query(func.count(Station.station)).all() $
tcat_df.size $ tdog_df.size
fuel_therm_abs_rate = openmc.Tally(name='fuel therm. abs. rate') $ fuel_therm_abs_rate.scores = ['absorption'] $ fuel_therm_abs_rate.filters = [openmc.EnergyFilter([0., 0.625]), $                                openmc.CellFilter([fuel_cell])] $ tallies_file.append(fuel_therm_abs_rate)
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 7 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
list("1" "2" "abc" "Two words.")
with open("data/twitter-swisscom/geo/geo_info_total.pickle", "rb") as h: $     geo = pickle.load(h)
edgereader = sample_df[['series_or_movie_name','encrypted_customer_id']] $ edgereader.columns = ['source','target']
print (festivals.loc[2]['latitude']) $ print (festivals.loc[2]['longitude']) $ print(festivals.index) $ festivals.head(3) $
dfstationmeanbreak[['STATION','totals']].nlargest(10, ['totals'])
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
coin_hour = get_historical_price_hour(COIN) $ print(coin_hour.head())
tar_counts = cig_data['tar'].value_counts()
print("The probability of converted in treatment group:", df2.query('group == "treatment"')['converted'].mean())
lq = lq.dropna(how = 'any')
pd_data.payPrice = pd_data.payPrice.map(lambda x: float(x)/100 if x else 0)
df = df.drop(['seller','name'],axis=1) $ df.head(3)
zc['zipcode'] = zc['zipcode'].astype(int)
autos['price'].unique().shape[0]
logistic_countries = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results2 = logistic_countries.fit()
df_other_dummies = pd.get_dummies(df_with_metac_with_onc[ls_other_columns]) $ dummy_colnames = df_other_dummies.columns $ dummy_colnames = [clean_string(colname) for colname in dummy_colnames] $ df_other_dummies.columns = dummy_colnames
from sklearn.model_selection import RandomizedSearchCV $ from scipy.stats import reciprocal, uniform $ param_distributions = {"gamma": reciprocal(0.001, 0.1), "C": uniform(1, 10)} $ rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2) $ rnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])
filter_df = filter_df[filter_df['start_time'] >= datetime(2016, 8, 1, 0, 0)] $ filter_df.head(2)
stock['forecast'] = results.predict(dynamic=False)
!cat /notebooks/.log/20170704/20170704-071448-0119.log
nd1=dfSPY.values $ print(nd1[0:5])
indexed_return(fundret['2017-04':]).head()
data = {'name': ['Alice', 'Bob', 'Charles', 'David', 'Eric'], $         'year': [2017, 2017, 2017, 2018, 2018], $         'salary': [40000, 24000, 31000, 20000, 30000]} $ df = pd.DataFrame(data, index = ['Acme', 'Acme', 'Bilbao', 'Bilbao', 'Bilbao']) $ df
loans_plan_all_xirr=cashflows_plan_investor_all.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
df_h1b_ny = df_h1b[map(lambda x: x[1].lca_case_workloc1_state=='NY' $                        or x[1].lca_case_workloc2_state=='NY', $                        df_h1b.iterrows())] $ print('There are {:.0f} visa applications for jobs located in NY in this dataset.'.format(df_h1b_ny.shape[0]))
(labels.loc[labels['churn'] == 1, 'cutoff_time']).values[0]
data = spark.read.csv('sensor_data.csv',header=True)
def get_data(sz): $     tfms=tfms_from_model(f_model=f_model,sz=sz,aug_tfms=transforms_top_down,max_zoom=1.05) $     dat= ImageClassifierData.from_csv(PATH,'train-jpg',val_idxs=val_idxs,tfms=tfms, $                                        csv_fname=labels,suffix='.jpg') $     return dat
out_temp_columns = [s for s in daily_dat.columns if primary_temp_column in s] #only save select temperature columns $ save_name=Glacier.lower()+ Station + "_daily_"+"LVL2.csv" #filename $ save_pth=os.path.join(save_dir, save_name) #location to save file
knn_10.score(X_train, y_train)
len(df.DATE.unique()), len(df)
limited_contingency =  np.array(pd.crosstab(index=intervention_train_extract['target'], columns=intervention_train_extract['ORIGINE_INCIDENT']))
station_activity = session.query(Measurement.station, func.count(Measurement.station)).\ $                     group_by(Measurement.station).\ $                     order_by(func.count(Measurement.station).desc()).all() $ station_activity $
taxiData.tail(5)
transit_df = transit_df.drop_duplicates() $ transit_df = transit_df.dropna()
intervention_train.loc[:, columns_with_nan].head(100)
x.dropna(axis=1)
all_noms = pd.read_csv("all_nominations_confirmations_late_june.csv") $ all_noms.head()
for i in cpi_all['Frequency'].cat.categories.tolist(): $     print i    
countries.user_id.nunique()
lines = sc.textFile(csv_filename) $ parts = lines.map(lambda l: l.split(",")) $ rows = parts.map(parse)
store_items.pop('new watches') $ store_items
log_mod_w_country = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ log_mod_country_results = log_mod_w_country.fit() $ log_mod_country_results.summary()
ts.plot(figsize=(12, 6))
ab.loc[mismatched.index].isnull().sum()
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2) $ store_items
model = sm.Logit(df_new['converted'], df_new[['UK', 'US', 'ab_page', 'intercept']]) $ result = model.fit()
df = pd.DataFrame(np.random.randn(8,4), columns=['A','B','C','D']) $ df
dfrecent.head()
df["extended_tweet"] = df["extended_tweet"].apply(lambda x: x["full_text"]  if type(x) == dict else x )
(trainingData, testData) = output2.randomSplit([0.7, 0.3])
twitter_archive_master.likes.value_counts()
df2.jpg_url.duplicated().value_counts()
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-pnew, pnew]) $ print(new_page_converted)
df.head()
df_gateways = pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False) $ df_gateways.head(10)
print loan_requests1.shape $ print loan_requests_indebtedness.shape
titanic.fare
lda_df = pd.concat([lda_nra_convention, lda_gunviolence, lda_antigun, lda_school_shooting, lda_guncontrol, lda_nra]) $ lda_df.to_csv('lda_data.csv')
df[df['Complaint Type'] == 'Noise - Residential']['Descriptor'].unique()
s = lv_workspace.get_subset_object('B').indicator_objects['dip_winter'] $ s.get_filtered_data(subset = 'B', step = 'step_2')
twitter_Archive=twitter_Archive.merge(df_tsv, left_on='tweet_id', right_on='tweet_id',how='left', left_index=True) $ twitter_Archive.info()
df.truncate(after='5/3/2014')
for i in range(len(resdatasep)): $     cid=str(resdatasep[i]['restaurant']['location']['country_id']) $     if(cid in countrydata.keys()): $         resdatasep[i]['restaurant']['location']['country_name']=countrydata[cid]
random.seed(1234) $ old_page_converted = np.random.choice([1, 0], size = n_old, p = [p_mean, (1-p_mean)])
table_rows = driver.find_elements_by_tag_name("tbody")[6].find_elements_by_tag_name("tr") $
events_df.head(5)
search = []    $ for values in archive_df_clean['text']: $     search.append(re.search(r'([0-9]+(\.)?([0-9])*)', values).group()) $ archive_df_clean['clean_numerator'] = search $
by_day_by_hour15 = uber_15.groupby(["day_of_week", "hour_of_day"]).size() $ by_day_by_hour15.head()
gaming_products = pd.read_csv("../data/top-things/reddits/g/gaming.csv") $ gaming_products['subreddit'] = "r/gaming" $ movie_products = pd.read_csv("../data/top-things/reddits/m/movies.csv") $ movie_products['subreddit'] = "r/movies" $ pd.concat([gaming_products,movie_products])
building_pa_prc.shape
df_DST['DST Index']=df['Dst Index'] $ df_DST.index=df_DST['Datetime'] $ df_DST=df_DST.drop('Datetime',axis=1)
image_df_tomerge = image_df_clean.loc[:, ['tweet_id', 'prediction_1', 'prediction_1_confidence','prediction_1_result']] $ image_df_tomerge['tweet_id'] = image_df_tomerge['tweet_id'].astype(str) $ twitter_df_merged = pd.merge(pd.merge(archive_df_clean, status_df, on='tweet_id'), image_df_tomerge, on='tweet_id')#pd.merge(status_df, image_df_clean, on = 'tweet_id', how='inner')
data = utils.load_data()
df_complete_b.shape[0]
mc = Motorcycle(make='Honda', miles=2000, model='Hawk', sold_on="01-Feb-2008",year=2007) $ mc.vehicle_type()
workspaces_list = toggl.request("https://www.toggl.com/api/v8/workspaces")
train_dates = ['2014-06-01', '2014-03-01', '2015-05-01', '2015-07-01'] $ myCViterator = [] $ for i in train_dates: $     trainIndices, valIndices = create_validation(train, i) $     myCViterator.append((trainIndices, valIndices))
df_agg_click_rand.head()
vectors = pd.concat([pos_vectors, neg_vectors]) $ targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index]) $ labels = list(pos_vectors.index) + list(neg_vectors.index)
df = pd.read_csv('LoanPaymentsDataClean.csv') $ df.head()
session['action'].value_counts().sort_values(ascending=False).head(20)
p_diffs.mean()
df.set_index('created_at', inplace=True) $
print( X_train.shape, X_test.shape)
date_retweets_sample=date_retweets.sample(5000)
data['signup_time'] = pd.to_datetime(data['signup_time']) $ data['purchase_time'] = pd.to_datetime(data['purchase_time']) $ data['difference'] = data['purchase_time'] - data['signup_time'] $ data['difference'] = data['difference'].astype('timedelta64[m]') $ data.head()
from pyspark.sql.functions import *
import catboost as ctb $ ctb_model = ctb.CatBoostClassifier(iterations = 40, learning_rate=0.05, depth=4, eval_metric='Logloss', $                                        random_seed=42, rsm=0.8, l2_leaf_reg=2, od_wait=50, od_type='Iter' , verbouse = 0)
twitter_df_clean.loc[200]
obs_new = df2.query('group == "treatment" and converted == 1').count()/df2[df2['group'] == "treatment"].count() $ obs_new[0]
lsi_out = np.hstack(([np.array(lsimodel[tfidf[dictionary.doc2bow(trigram[bigram[doc]])]])[:, 1][:, np.newaxis] for doc in corpus])).T $ lsi_out = pd.DataFrame(lsi_out, columns=np.arange(lsi_out.shape[1]), index=np.arange(lsi_out.shape[0])) $ lsi_out['key'] = df['key']
print('web service hosted in ACI:', aci_service.scoring_uri)
autos["odometer_km"].unique()
result1 = (df['A'] + df['B']) / (df['C'] - 1) $ result2 = pd.eval("(df.A + df.B) / (df.C - 1)") $ np.allclose(result1, result2)
len(df2[df2['converted'] == 1])/df2.shape[0]
for i,doc in enumerate(founddocs): $     with open(doc.filename+'_'+str(i), 'w') as f: $       f.write(fs.get(doc._id).read())
np.sum(df2.landing_page=="new_page")/len(df2.landing_page)
test_df = pd.read_csv('loan_test.csv') $ test_df.head()
precip_df.plot(title="Hawaii Precipitation 2016") $ plt.tight_layout() $ plt.ylabel("prcp") $ plt.show()
df_small.head(2)
corpus = corpora.MmCorpus('/home/david/Desktop/guardian.mm') $
df.query("group == 'control' & landing_page == 'new_page'").count()[0]
lsa = TruncatedSVD(n_components=10,n_iter=100) $ lsa.fit(X)
lemma = WordNetLemmatizer() $ def lemmatize(text): $     return ' '.join(lemma.lemmatize(word) for word in text.split()) $ texts = [text for text in cleaned_texts.values if len(text) > 1] $ lemmatized_texts = [lemmatize(text).split() for text in texts]
print(X.shape,y.shape) $ print(X_train.shape,y_train.shape) $ print(X_test.shape,y_test.shape)
new_page_converted = np.random.choice([0,1],n_new,p=(p_new,1-p_new))
nodes = pd.read_csv("nodes.csv",parse_dates=['timestamp']) $ ways = pd.read_csv("ways.csv",parse_dates=['timestamp']) $ nodes_tags = pd.read_csv("nodes_tags.csv") $ ways_tags = pd.read_csv("ways_tags.csv")
from sklearn.metrics import mean_absolute_error $ preds = forecast_data["yhat"] $ actuals = forecast_data["y"] $ mean_absolute_error(preds, actuals)
tsprior = pd.Timestamp('1/1/2016') $ tsprior2 = pd.Timestamp ('1/1/2017') $ tsprior3 = pd.Timestamp('1/1/2018')
df1a = df1.set_index('employee') $ df2a = df2.set_index('employee') $ display('df1a', 'df2a')
week15 = week14.rename(columns={105:'105'}) $ stocks = stocks.rename(columns={'Week 14':'Week 15','98':'105'}) $ week15 = pd.merge(stocks,week15,on=['105','Tickers']) $ week15.drop_duplicates(subset='Link',inplace=True)
df.to_csv('gang_list_for_visualization.csv')
all_noms[(all_noms["agency"] == "Foreign Service") & (all_noms["confirmed"] == "yes")]["nom_count"].sum()
df_reader = pd.read_csv(file_wb, chunksize = 10) $ print(next(df_reader)) $ print(next(df_reader)) $
d.reshape(3,-1)  #3 rows, number of columns determined automatically.
airquality_dup = pd.concat([airquality_melt, airquality_melt])
pickle.dump(nmf_cv, open('iteration1_files/epoch3/nmf_cv.pkl', 'wb'))
Y, X = dmatrices('Y ~ X', data=df) $ mod2 = sm.OLS(Y, X) $ res2 = mod2.fit() $ res2.summary2()
n_old = df2.query('landing_page == "old_page"').group.count() $ n_old
output_variables = S.modeloutput_obj.read_variables_from_file()
titanic.duplicated().sum()
print('* Cleaning texts...') $ cleaned_docs = spacy_docs $ cleaned_docs = [[token.lemma_ for token in doc if not _unwanted_token(token)] for doc in cleaned_docs]
X.head()
altitude_only.info()
df.loc[:,['name','year']]
session.query(func.count(station.station)).all()
df2[df2.duplicated(subset='user_id', keep=False)]
y = train_df['Tag'].values $ X_train, X_valid, Y_train, Y_valid = train_test_split(train_df, y, test_size = 0.1, stratify=y)
iris.loc[:,"Species"].cat.categories
future = m.make_future_dataframe(periods=52*3, freq='w') $ future.tail()
df0 = df[df['message_likes_dummy']==0].sample(n=3000).reset_index(drop=True) $ df1 = df[df['message_likes_dummy']==1].sample(n=3000).reset_index(drop=True)#gets shuffled sample of each subset $ df = pd.concat([df0, df1])
dates = list(map(parser.parse, dates))
tesla = pd.read_csv('twitter/Tesla_tweets.csv')
dogscats_h2o = h2o.H2OFrame(dogscats_df) 
botoresult = pd.DataFrame() $ for screen_name, result in bom.check_accounts_in(asu): $     botoresult=botoresult.append(result, ignore_index=True)
autos['gear_box'].value_counts()
df_planets = pd.DataFrame({"name": planets, "id": range(len(planets))}) $ df_planets
tweets_clean = pd.read_csv('tweets_clean.csv')
lm.rsquared
vc_M.head()
DataAPI.write.update_factors(factors=factors,trading_days=trading_days , override=False, log=False)
with open(os.path.join(outputs, 'train_test_data_features.pkl'),'rb') as f: $     (train_data_features_tf, $     test_data_features_tf, $     train_data_features_tfidf, $     test_data_features_tfidf) = pickle.load(f)
error_treatment = df[(df['group'] == "treatment") & (df['landing_page'] == "old_page")].count() $ error_control = df[(df['group'] == "control") & (df['landing_page'] == "new_page")].count() $ print(error_treatment) $ print(error_control) $ print (1965 + 1928)
df.to_csv('data/dibeBot_ff_tweets.csv', index=False, sep='\t', encoding='utf-8-sig')
%time $ test5result = ddf_test5.compute() $
print('From the above, what I can make out is it is not worth investing into new pages since the figures that we find here are not that great and pointing at the fact that adding new pages would serve the purpose.')
cats = shelter_cleaned_df.loc[shelter_cleaned_df['AnimalType'] == 'Cat'] $ cats.describe() $
nold = df2.query('landing_page == "old_page"').count()[0] $ print ("The population of Oldpage is : {}".format(nold))
train_shifted.head(3)
df.to_json('/tmp/coindesk.json')
weather_df.to_csv("Weather data.csv", encoding = "utf-8-sig", index = False)
googletrend.head()
team_attributes['buildUpPlayDribbling'].groupby(team_attributes['team_api_id']).fillna(team_attributes['buildUpPlayDribbling'].mean())
pres_df['subjects'].isnull().sum()
%time dot.render('02')
part1_flt[['campaign_id', 'bid', 'matched_targeting']].drop_duplicates()
np.random.seed(1) $ s = pd.Series(np.random.randn(5)) $ s
!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.8,en;q=0.6" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/6322/train_v2.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1532362274&Signature=ilplDkEyN7y1SAZo52C%2Br3aNH%2FXDkZTGBpamwuU3eyWInNqtouIXH3SJcCSsPs1M7zEyGZqlDKzrimvexYNM9qotATf1jwG6%2FXUk2agw%2F95B3gur7w6uC9zoiKghLdGkT3NSYQum7YeCYpa8FlvAcMlbPO64z9eARMj5LZfo8MTbi1TOr5l4RX9UCLFxjJamdXfJogylJZ1VblalFPjrv5Jv8Bp07kZ6u1FFEF8bJaIAVqRaw6%2F3AhNk74AcsiXa1DCiZJwN1%2BTnbZe8UPNx4dwgrxYPYxWVHVLIyezV2qe1X7VDhf237RBgAvF7LSaE3tH3knSAmzWIQPXGyp5tVQ%3D%3D" -O "train.zip" -c -d /data
pold = df2.converted.mean() $ print(pold)
july_1 = pd.to_datetime('7/1/2016') $ july_1
print("The maximum value of artistID:") $ userArtistDF.agg(max("artistID")).show()
apple_tweets2 = ioDF.loc[ioDF.author_id_y == 'AppleSupport'] \ $     .loc[ioDF.date_x > datetime(2017, 11, 8)] \ $         .loc[ioDF.date_x < datetime(2017, 11, 10)]
temps_df[temps_df.Missoula > 82]
df_train = df['text'].tolist()
template = envi.get_template("report_template_columns.html") $ html_out = template.render(template_vars) $ HTML(string=html_out).write_pdf('../reports/report_columns.pdf')
 result = db.profiles.create_index([('reinsurer_id', pymongo.ASCENDING)], $                                   unique=True) $  sorted(list(db.profiles.index_information()))
df.head()
Base.prepare(engine, reflect=True)
def drop_constraint(cur, table_name, constraint_name): $     cur.execute(sql)
bad_iv = options_frame[np.isnan(options_frame['ImpliedVolatilityMid'])]
grouped.unstack('product_type')
cnx.commit() $ c.fetchall()
taxi_hourly_df["missing_dt"] = np.NaN
temps_df.Missoula > 82
train['number_of_features'] = train['features'].map(len) $ test['number_of_features'] = test['features'].map(len)
data_other = tmpdf.index[tmpdf[tmpdf.isin(DATA_SUM2_KEYS)].notnull().any(axis=1)].tolist() $ data_other
print('Max train ID: %d. Max test ID: %d' % (np.max(NYT_train_raw['UniqueID']), np.max(NYT_test_raw['UniqueID']))) $ joined = NYT_train_raw.merge(NYT_test_raw, how = 'outer')
mr = minute_return.between_time('9:30', '16:00') $ mr.head()
df[ $     (df['datetime'] > datetime(2016, 1, 1)) & $     (df['datetime'] < datetime(2017, 1, 1)) $ ].set_index('datetime').resample('1d').aggregate('count').plot()
data_final.head(2)
from sklearn.cross_validation import train_test_split
import random $ L=[random.random() for i in range(100000)] $ print('sorting an unsorted list:') $ %time L.sort()
df = orgs[(orgs.founded_year >= 1990) & (orgs.founded_year <= 2016)].copy()
apple.resample('BM').mean().head()
volume = ticks.Volume.resample('1min', how='sum') $ value = ticks.prod(axis=1).resample('1min', how='sum') $ vwap = value / volume
last_date_of_ppt = session.query(Measurement.date).group_by(Measurement.date==year_ago_ppt).order_by(Measurement.date.desc()).first() $ print(last_date_of_ppt) $
df2 = df2.join(countries.set_index('user_id'), on='user_id') $ df2.head()
S.decision_obj.simulStart.value = "2006-07-01 00:00" $ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
df = pd.read_sql_query('SELECT ComplaintType, Descriptor, Agency ' $                        'FROM data ' $                        'LIMIT 10', disk_engine) $ df
time_step=list() $ for i in range(0,len(values)): $         time_step.append(i);
new_page_converted = np.random.binomial(1, p, n_new)
words_mention_sp = [term for term in words_sp if term.startswith('@')] $ corpus_tweets_streamed_profile.append(('mentions', len(words_mention_sp))) # update corpus comparison $ print('List and total number of mentions: ', len(set(words_mention_sp))) #, set(terms_mention_stream))
temps_df.columns
EOM_dates=pd.date_range('2014-01-01', '2015-05-31', freq='M') $ minimum_vintage=pd.tseries.offsets.MonthEnd(3) $ pd.DataFrame({'d':EOM_dates,'e':EOM_dates -minimum_vintage})
p_new = df2['converted'].mean() $ print('Convert rate for p_new under the null :: ',p_new)
raw_full_df[['building_id_iszero','is_train']].groupby('is_train').mean()
geocoded_df.loc[idx,['Case.File.Date','Judgment.Date','Case.Duration']].head()
dinw_filter_set = w.get_step_object(step = 2, subset = subset_uuid).get_indicator_data_filter_settings('din_winter')
df = df.join(cluster.Cluster_Labels_GaussianMixture)
wrd.query('name == "a"')['text']
classification_data.loc[0]
compare = np.where(data.suma != data.view_count) $ print(compare[-1])
difference = total.xs_tally - absorption.xs_tally - scattering.xs_tally $ difference.get_pandas_dataframe()
pd.set_option('display.multi_sparse', True)
google_stock.tail()
total = ac.isnull().sum().sort_values(ascending=False) $ percent = (ac.isnull().sum()/ac.isnull().count()).sort_values(ascending=False) $ missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) $ missing_data
from sklearn.svm import LinearSVR $ model = LinearSVR() $ print ('SVR') $ reg_analysis(model,X_train, X_test, y_train, y_test)
total = ALL.groupby("Team") # This groups our final sample by team.
sm.graphics.plot_partregress
image_pred.info()
plt.scatter(mario_game.NA_Sales, mario_game.EU_Sales) $ print('The pearson correlation between the NA sales and EU sales is {}' \ $       .format(pearsonr(mario_game.NA_Sales, mario_game.EU_Sales)[0]))
print('CV Accuracy: %.3f' $      % gs_lr_tfidf.best_score_) $ clf = gs_lr_tfidf.best_estimator_ $ print('Test Accuracy: %.3f' % clf.score(X_test, y_test)) $
db_grade =pd.read_sql_query( "SELECT * FROM vote_grade",conn) #final score of student=ranking $ db_rating =pd.read_sql_query( "SELECT author_id, AVG(value) as avg_rating from rating GROUP BY author_id ORDER by avg_rating DESC",conn)
url = 'http://www.straitstimes.com/asia/east-asia/now-its-japans-turn-to-brace-for-a-monster-storm-as-typhoon-lan-nears' $ article = Article(url)
import re $ re.sub("Hi", "Hello", "Hi world")
df_arch.describe() $
data['title_len'].sort_values(ascending= False)
switch_times = numpy.concatenate([[0], numpy.cumsum(P*nperiods)])
df_archive['source'].value_counts()
top_bc = df["0"].values[0:n_iterations] $ bot_bc = df['20'].values[0:n_iterations]
print(pd.DataFrame({'effect': params.round(0), $                     'error': err.round(0)}))
print("customer_transaction.shape",customer_transaction.shape) $ print("data.shape",data.shape)
last_12_precip_df = pd.DataFrame(last_12_precip, columns=['date', 'precipitation']) $ last_12_precip_df = last_12_precip_df.set_index('date') $ last_12_precip_df.head()
treatment_df = df2.query('group=="treatment"') $ converted_treatment = treatment_df.query('converted == 1') $ p_new_act = float(converted_treatment.user_id.nunique()/treatment_df.user_id.nunique()) $ print('Prob. new page actual {}'.format(p_new_act))
shows['plot_cleaned'] = shows['plot'].dropna().apply(reassemble_plots)
actual_payments_combined.columns
submit.head(1)
def tweets_in_period(period_start): $     mask = (df_input['created_at'] >= period_start) & (df_input['created_at'] <= period_start + timedelta(hours=1)) $     tweets = df_input.loc[mask]['text'].tolist() $     return {'tweets': tweets}
baseball.reindex(id_range, method='ffill', columns=['player','year']).head()
sunspots.iloc[10:20, :]
ticket = df_titanic['ticket'] $ print(ticket.describe()) $
from pandas.tseries.offsets import BDay $ pd.date_range('2015-07-01', periods=5, freq=BDay())
df_max['date'] = df_max['date'].dt.date $ df_count['date'] = df_count['date'].dt.date $ df_mean['date'] = df_mean['date'].dt.date
X_svd = np.hstack((id_dense,blurb_SVD, goal_dense, duration_dense))
import pickle $ import problem_unittests as tests $ import helper $ valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))
first_imdb = float(first_movie.strong.text) $ first_imdb
df3.query('country == "US"').converted.mean(),df3.query('country == "UK"').converted.mean(),df3.query('country == "CA"').converted.mean()
tweet_json.nunique()
list_of_files = [i for i in os.listdir()] $ print('There are',len(list_of_files),'Files in',os.getcwd()) $ list_of_py = Find_files_with_a_given_extension(os.getcwd(),'py',False) $ list_of_py
DF_filtered = ds_complete_temp_CTD_1988.to_dataframe()
sn.distplot(train_binary_dummy['unique_device'])
import matplotlib.image as mp_image $ filename = "../imgs/keras-logo-small.jpg" $ input_image = mp_image.imread(filename)
browser = webdriver.Chrome('chromedriver.exe') $ browser.get(url_syrtis) $ time.sleep(5) $ html = browser.page_source $ soup = BeautifulSoup(html, "html.parser")
df.groupby("cancelled")[["past_rides", "past_cancellations", "past_percent_cancelled"]].mean()
url='https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response=requests.get(url) $ with open(url.split('/')[-1],mode='wb')as file: $     file.write(response.content)
c2 = file.to_dataframe(mode='pivot', aggfunc='size') $ c2.head()
df_comment['time']= df_comment.datetime.dt.hour $ df_comment['datetime_moscow']= df_comment.datetime +  pd.DateOffset(hours=15) $ df_comment['time_moscow']= df_comment.datetime_moscow.dt.hour
p_diffs=[] $ for i in range(10000): $     new_page_converted = np.random.choice([1, 0], size=nnew, p=[np.mean([pnew, pold]), (1-np.mean([pnew, pold]))]).mean() $     old_page_converted = np.random.choice([1, 0], size=nold, p=[np.mean([pnew, pold]), (1-np.mean([pnew, pold]))]).mean() $     p_diffs.append(new_page_converted - old_page_converted) $
dfFull.LotFrontage = dfFull.LotFrontage.fillna(dfFull.LotFrontage.mean())
tfav.plot(figsize=(16,4), label="Likes", legend=True) $ tret.plot(figsize=(16,4), label="Retweets", legend=True);
X_test = pd.DataFrame(im.transform(X_test),columns=X_train.columns)
plt.xticks([-4*pi,-2*pi,0,2*pi,4*pi], ['$-4\pi$','$-2\pi$','$0$','$2\pi$','$4\pi$']) $ plt.yticks([0,1], ['$0$','$1$']) $ plt.plot(x, np.sin(x)/x)
json_data = r.json() $ print(json_data)
datetimes = pd.date_range(DATA_STARTTIME, DATA_ENDTIME, freq='min') $ datetimes[0:10]
data2.keys()
n_new = (df2['landing_page'] == 'new_page').sum() $ print(n_new)
pd_austin = pd_builder[pd_builder['endcustomerlinefixed_data']=="DILLARD'S, INC."]
loans_df.emp_length.value_counts()
df_jse.loc[:, 'Volume'].resample('2W').agg([np.mean, np.sum])
jeff.withdraw(100.0)   # Instruction to withdraw $ jeff.balance           # Shows 900.0
bnbAx = pd.concat([bnbAx, pd.get_dummies(bnbAx['language'], prefix='lang_')])
us_grid_id = (pd.read_csv('../data/model_data/us_grid_id.csv', index_col = 0) $               .groupby('grid_id').count().reset_index()) $
df['created_at']=pd.to_datetime(df['created_at'],format='%Y-%m-%d %H:%M:%S') $ df['Month'] = df['created_at'].dt.month $ df['Year'] = df['created_at'].dt.year $ df['Day'] = df['created_at'].dt.day
BroncosBillsTweets.loc[4365]['text30']
match.groups()
pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5')
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
padres_id = '16c14652-0309-42a8-a58d-61e0b3977198' $ url = form_url(f'teams/{padres_id}') $ response = requests.get(url, headers=headers) $ print_status(response)
df[df.is_oc_company == True]
df.columns
s4 = pd.Series([10, 0, 1, 1, 2, 3, 4, 5, 6, np.nan]) $ len(s4)
check_null = df.isnull().sum(axis=0).sort_values(ascending=False)/float(len(df)) $ print(np.sum(check_null.values) == 0.0)
hundred_stocks_df.to_sql(con=conn_helloDB, name='hundred_stocks_twoyears_daily_bar', if_exists='replace', index=False)
sum((df.group=='control') & (df.landing_page=='new_page')) + \ $ sum((df.group=='treatment') & (df.landing_page=='old_page'))
rows_in_table = len(ins['business_id']) $ unique_ins_ids = len(ins['business_id'].unique()) $
donors_c['Donor Zip'] = donors_c['Donor Zip'].astype(str)
sqlContext.sql("select * from pcs").show()
gbm = xgb.XGBClassifier(max_depth=5, gamma=.8, n_estimators=1200, $                         learning_rate=0.01, objective='binary:logistic', seed=1984) $ gbm.fit(x_train_advanced, y_train_advanced)
def scoring(clf): $     scores = cross_val_score(clf, X_train, y_train, cv=15, n_jobs=1, scoring = 'neg_median_absolute_error') $     print np.median(scores) * -1
np.outer(u, v)
dataset.head(2)
print(a.lower()) $ print(a.upper()) $ print(a.capitalize())  # Capitalize first letter
print(stock.head()) $ print(mini.head())
trips_data['duration'].hist() # displays histogram for duration of trips $
import statsmodels.api as sm $ logit=sm.Logit(df2['converted'],df2[['intercept','treatment']]) $ result=logit.fit()
tweets_df.retweet_count.describe()
aapl = pd.read_csv(file_name, index_col='Date') $ aapl
most_recent_investment = pd.DataFrame(investment_dates.groupby('investor_uuid')['announced_on'].max()).reset_index()
series1 = df2['DepTime'].apply(deptime_to_string) $ series1.head(10)
k_var_concat.head()
df.iloc[0] #using internal integer indexes 
df_1.info()
conv_prob_control = df2[df2.group == 'control'].converted.mean() $ print('Given that an individual was in the control group, the probability they converted is {}.'.format(conv_prob_control))
data.drop('Unnamed: 0', axis =1, inplace = True)
def prefiltering_of_neighbors(old_similar_table, thershold): $     new_similar_table = old_similar_table.copy() $     new_similar_table[new_similar_table < thershold] = 0 $     return new_similar_table
control_new_page.set_index('timestamp') $ treatment_old_page.set_index('timestamp');
control_convert = (control[control['converted']==1].user_id.nunique())/(control['user_id'].nunique()) $ control_convert
data.loc[(100, slice(None), 'put'),'Vol'].head()
tweets_sample = tweets_1[0:20] $ count_vect_sample = CountVectorizer(analyzer = clean_text) $ X_counts_sample = count_vect_sample.fit_transform(tweets_sample['text']) $ print(X_counts_sample.shape) $ print(count_vect_sample.get_feature_names())
measure.describe()
avgPurchP = train.groupby(by='Product_ID')['Purchase'].mean().reset_index().rename(columns={'Purchase': 'AvgPurchaseP'}) $ train = train.merge(avgPurchP, on='Product_ID', how='left') $ test = test.merge(avgPurchP, on= 'Product_ID', how='left')
(autos['date_crawled'] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
old_n = df2.query('landing_page == "old_page"').shape[0] $ old_p = df2.converted.mean() $ old_page_converted = np.random.choice([0, 1], size=old_n, p=[1-old_p, old_p])
Trump_week_iphone = Trump_week[Trump_week["source"] == "Twitter for iPhone"] $ Trump_week_iphone = pd.concat([Trump_week_iphone.iloc[0:35],pd.DataFrame({"year":2016,"week":46,"id":0,"source":"Twitter for iPhone","negative":0},index=[20]),Trump_week_iphone.iloc[35:]]) $ Trump_week_iphone.shape
import pprint $ pp = pprint.PrettyPrinter(indent=0) $ pp.pprint(the_posts['BfHzGJdA5FZ'])
ad_group_performance['Conversions'].median()
df2['converted'].sum() / df2.shape[0]
city_eco.sort_values(by="economy", ascending=False)
df["tokens"] = df["text"].apply(tokenise)
print("The shape of the dataframe is:", sl_data.shape) $ print() $ print("Print the head of the new created dataframe:", '\n', sl_data.head())
oppstage.columns = ['lead_mql_status', 'opportunity_month_year', 'opportunity_stage', 'counts']
df_schools.shape
pd.DataFrame(usersDf.columns)
age.loc[[True, False, True, False, True, False]]
logit_mod = sm.Logit(df2['ab_page'],df2[['intercept','converted']])
model.show_topics(10)
auto_new = auto.dropna()
df.state.unique()
fig, axarr = plt.subplots(1,1, figsize =(12,8)) $ df_prod_wide.Production[['AG', 'AI,AR', 'BE,JU', 'BL,BS']].plot(ax=axarr, colormap=customcolors.deloitte_colormap()) $ axarr.set_ylabel('Energy produced per day [kWh/day]') $ axarr.set_xlabel('Time');
dates = [pd.Timestamp('2012-05-01'), pd.Timestamp('2012-05-02'), pd.Timestamp('2012-05-03')]
hs.columns
cb_offices.head()
df2.set_index('user_id').index.get_duplicates()
region = pd.DataFrame(df.pop("region").tolist()) $ df = df.join(region, rsuffix="_region")
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') #Merging 2 data frames with index as user_id $ df_new.head()
team_groups.groups
df_clean.sample(10)
model.wv.similarity('like', 'love')
autos["price"].describe()
brand_mileage = pd.Series(mean_mileage)
autos['registration_year'].value_counts(normalize=True)
df['AQI Category'] = df['AQI Category'].astype('category') $ df['AQI Category'].cat.categories
model = sm.Logit(reg_df['converted'], reg_df[['intercept', 'ab_page']])
s.str.split('_',expand=True, n=5)  # limit expansion into n columns
w_counts=[] $ for t in t_open_resol: $     i=t['index'][1] $     w_counts.append(data_issues_transitions['watch_count'][i])
df_clean3.loc[2091, 'text']
posTrend.head(10)
brands = autos["brand"].value_counts(normalize=True)[autos["brand"].value_counts(normalize=True)>0.02].index.tolist() $ brands
LUM.plot_mean(all_lum)
date_df = pd.DataFrame({'Date': [datetime.datetime(2017, 3, 1), datetime.datetime(2017, 3, 2)]})
house_data = data.loc[mask == 1]
km = joblib.load('./data/KMeans.pkl')
tweets_clean.rating_numerator.mean()
PD=df_usa['Population']/df_usa['Area'] $ df_usa['PD2']=PD $ df_usa.head()
dfTemp=df.rename(columns={'Indicator':'Indicator_ID'}) $ print(dfTemp.info()) $ print ("----------------------df not reflecting the change-------------") $ print(df.info())
cityID = '512a8a4a4c4b4be0' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Charlotte.append(tweet) 
master_copy = archive_clean.copy()
tweet_archive_clean['text'] = tweet_archive_clean['text'].apply(lambda x: x.split('https')[0])
frac_volume_m = pd.concat([fraq_volume_m_coins, $                            fraq_fund_volume_m], axis=1)
scaled = data.div(data['Close'], axis=0) $ scaled.head()
speeches_cleaned.shape
mini_df = data[['Order_Qty','wait','month']] $ mini_df['wait'] = pd.to_numeric(mini_df['wait'])
df.set_index('Parameter', inplace=True) $ df.head()
monthly_clim = data_month['Temperature(C)'].mean() $ monthly_clim.index = monthly_clim.index.map(int) $ monthly_clim = monthly_clim.sort_index() $ monthly_clim.plot()
model_dir = "top20_len20_prebase_ps20_it100" $ with open(LIBRARY_PATH+"saved_models/"+model_dir+"/model_spec.txt") as f: $     model_spec = json.loads(f.read()) $
api = tweepy.API(auth)
prod_ch = pd.read_csv("2188_production_nomment.csv",sep=",",comment="#")
season07 = ALL[(ALL.index >= '2007-09-06') & (ALL.index <= '2008-02-03')] # This means every transaction between 9-6-07 and $
%%sql $ UPDATE facts $ SET Created_Date_key = hour.hour_key $ FROM hour $ WHERE hour.hour  = TO_CHAR(facts.Created_Date, 'YYYY-MM-DD HH24:00:00')
twitter_merged_data.hist(column='favorites'); $ plt.title('Favorites Histogram') $ plt.xlabel('Favorites') $ plt.ylabel('Count');
%matplotlib inline $ plt.plot(games[games['TEAM_ABBREVIATION_HOME'] == 'ATL'].sort_values(by='GAME_DATE_HOME')['CumulFT_HOME']) $ plt.ylim(0,1.0)
df_comb['country'].value_counts()
feature_freq = pd.DataFrame(sorted(freqs, key = lambda x: -x[1]))
session.query(measurement.station,func.count(measurement.station)).group_by(measurement.station).order_by(func.count(measurement.station).desc()).all()
df_concat.drop(df_concat.columns[[0,1,2]], axis=1, inplace= True)
to_be_predicted_Day1 = 21.25 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
p_fnl = (abs_rate + thermal_leak) / (abs_rate + leak) $ p_fnl.get_pandas_dataframe()
tweets_clean.pupper.value_counts()
total1=total.loc[:,['name','RA0','DEC0','priority']]
data.head()
digits.target[5] $ clf.predict(digits.data[5:6])
p_actual_old = df2.query("landing_page == 'old_page'")['converted'].mean() $ p_actual_old
hemispheres_url= "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(hemispheres_url) $ html = browser.html $ soup = bs(html, "html.parser") $ mars_hemisphere = []
s_n_s_epb_one.Date = pd.to_datetime(s_n_s_epb_one.Date,format="%Y%m%d")
S_distributedTopmodel.decision_obj.hc_profile.options, S_distributedTopmodel.decision_obj.hc_profile.value
df = pd.read_csv('basic_graphics_single_column_data.csv')
sm.get_values()
likes['year'] = pd.DataFrame(likes.date.dt.year) $ likes['month'] = pd.DataFrame(likes.date.dt.month)
data.sort_index(inplace=True) $ print(data) $ print(data['a':'b'])  # now it works
number_rows = df.shape[0] $ print("Number of rows: {}".format(number_rows)) $
logodds.drop_duplicates().sort_values(by=['count']).to_csv('logsodd.csv')
ppt_df = pd.DataFrame(ppt) $ ppt_df.head() $ ppt_date = ppt_df.set_index('date') $ ppt_date.head(10)
dfn = df.loc[df.period].copy() $ dfo = df.loc[~df.period].copy() $ dfn.shape, dfo.shape
dt_features_test['launched_at'] = pd.to_datetime(dt_features_test['launched_at'],unit='s')
verify_response.headers
my_url = "http://www.ipaidabribe.com/reports/all#gsc.tab=0"
S_1dRichards.decision_obj.thCondSoil.options, S_1dRichards.decision_obj.thCondSoil.value
twitter_names = df[~df['Twitter_Name'].isnull()]['Twitter_Name'].unique()
from fastai.structured import * $ from fastai.column_data import * $ np.set_printoptions(threshold=50, edgeitems=20) $ PATH='data/Rossmann/'
df.select('CRIME_CATEGORY_DESCRIPTION').distinct().show(truncate=False)
Pop_df = Pop_df.set_index('Date')
from sklearn.model_selection import train_test_split  # model selection rather than cross validation?
margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name="margin_loss")
y_train = train_data['price'].values
gatecount_game_stations.head()
df = df.loc[df['game_type'] == 'R',]
excelDF.info()
billboard.head() $ tracks = billboard[['year','artist','track','time']] $ print(tracks.info()) $ tracks_no_duplicates = tracks.drop_duplicates() $ print(tracks_no_duplicates.info()) $
vehicleType_list = list(set(train_data.vehicleType))
viajes_sin_destino = trips[trips['start_station_name'] == trips['end_station_name']].shape[0] $ print str(viajes_sin_destino) + " \"viajes\""
for i in categorical: $     train_binary[i] = train_binary[i].astype('category')
filePath = '/green-projects/project-waze_transportation_network/workspace/share/Data/20180601103403.json'
from bson.objectid import ObjectId $ def get(post_id): $     document = client.db.collection.find_one({'_id': ObjectId(post_id)})
BroncosBillsTweets = tweets_gametitle.loc[tweets_gametitle['Game Title Date'] == 'Broncos vs. Bills  2017-09-24']
test.shape,train.shape,full_data.shape
interestlevel_df = train_df.groupby('interest_level')['listing_id'].count().reset_index().copy() $ interestlevel_df.columns = ['interest_level','count_of_listings'] $ interestlevel_df = interestlevel_df.sort_values(by='count_of_listings', ascending=False) $ interestlevel_df.head()
X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)
twitter_archive_enhanced_clean.timestamp.max
new_page_converted = df2.query('converted == "1"').user_id.nunique()
ts = pd.DataFrame(np.random.randn(758, 4), columns=list('ABCD'), index=dates)
poly_features = PolynomialFeatures(2, include_bias=False) $ poly_features.fit(X_test['Pending Ratio'].values.reshape(-1, 1)) $ test_pending_ratio = poly_features.transform( $     X_test['Pending Ratio'].values.reshape(-1, 1)) $ print(test_pending_ratio[0:5, :])
so_hashlist.columns = ["emailhash"]      $ gh_hashlist.columns = ["emailhash"]      
df_prep13 = df_prep(df13) $ df_prep13_ = pd.DataFrame({'date':df_prep13.index, 'values':df_prep13.values}, index=pd.to_datetime(df_prep13.index))
df = web.DataReader('2317.tw','yahoo',datetime(2017,1,1))
1 / np.exp(-0.0408)
grouped_dpt.describe() # summary statistics 
ExponentStories = pd.DataFrame(ExponentStories)
df_drug_counts.LEVODOPA.plot(kind = 'bar', color = 'r')
minimum = date_price[date_price==date_price.min()].index[0] $ maximum = date_price[date_price==date_price.max()].index[0] $ print(f'The lowest average price was on {minimum}') $ print(f'The highest average price was on {maximum}')
adopted_cats.loc[adopted_cats.Color.str.contains('Lynx Point'),('white')] = 1 $ adopted_cats.loc[adopted_cats.Color.str.contains('Lynx Point'),('gray')] = 1 $ adopted_cats.loc[adopted_cats.Color.str.contains('Seal Point'),('white')] = 1 $ adopted_cats.loc[adopted_cats.Color.str.contains('Lynx Point'),('black')] = 1
col_to_remove = list(set(train_data.columns).difference(set(test_data.columns)))
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==666287406224695296]
intersection = set(coins_mcap_today[50:].index) & set(coins_infund) $ len(intersection)
df_countries = pd.read_csv('countries.csv') $ df_countries.country.unique()
users["created_at"] = users["created_at"].astype("datetime64")
trump.groupby("source").describe()
(df $  .groupby('SDG') $  .count())
test_df = test_df.loc[test_df['lang'] == 'en'] $ en_test_df = test_df[['created_at', 'text']] $ print(en_test_df.shape) $ en_test_df.head()
plt.rcParams['figure.figsize'] = [14, 6] $ plt.style.use('seaborn-whitegrid') $ matplotlib.rc('xtick', labelsize=11) $ matplotlib.rc('ytick', labelsize=11) 
df['SALEDATE'] = pd.to_datetime(df['SALEDATE'], errors='coerce')
RE_GENE_NAME = re.compile(r'Name=(?P<gene_name>.+?);') $ def extract_gene_name(attributes_str): $     res = RE_GENE_NAME.search(attributes_str) $     return res.group('gene_name') $ gene_df['gene_name'] = gene_df['attributes'].apply(extract_gene_name)
review = pd.read_excel('Product Reviews.xlsx') $ review
pd.set_option('max_columns', 60)
archive_copy.doggo.unique()
df_cal = pd.read_csv("calendar.csv",index_col=None)
y=np.ravel(y)
data_final['authorId'].nunique()
LR_prob=logisticRegr.predict_proba(train_ind[features]) $ roc=roc_auc_score(test_dep[response], best_model_lr.predict_proba(test_ind[features])[:,1])
model_data = model_data.drop(['creation_time','name','email','last_session_creation_time','invited_by_user_id','org_id'], axis=1)
print(tweets[0].id) $ print(tweets[0].created_at)
other_list = list(org_counts[org_counts >= 3].index) $ other_list.remove('unlisted') $ other_list.remove('unknown') $ other_list.remove('bae systems')
portalcuahsi_json_pth = os.path.join(portal_pth, portal_json_1) $ with open(portalcuahsi_json_pth, 'r') as fj: $     portalcuahsi_json = json.load(fj)
df.pop('Seconds')
ab_df.converted.mean()
np.exp(results_2.params)
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s') $ print(git_log.timestamp.describe())
ds_meta = build_ds_meta() $ for ds_meta_item in ds_meta: $     print('Quantity of %s: %s' % (ds_meta_item, ds_meta[ds_meta_item].count()[0]))
cc.close.describe()
dup_id = df2.user_id.value_counts().argmax() $ print('Duplicated user_id: {}'.format(dup_id))
logging.info("Closing connection to mediaflux.") $ con.close()
X_trainset.shape $ y_trainset.shape $
regr2.fit(X2,y)
now = Timestamp("now") $ local_now = now.tz_localize('UTC') $ now, local_now
X_mice.shape
date_cols = ['approx_payout_date', 'event_created', 'event_created', 'event_end', \ $              'event_published', 'event_start', 'user_created'] $ for col in date_cols: $     a[col] = pd.to_datetime(a[col], unit='s') $ a
afl_data.tail(3)
with open(os.path.join(folder_name, url.split('/')[-1]), mode = 'wb')as file: $       file.write(r.content)
future_df.head()
first_result.find('strong')
traindf, testdf = train_test_split(pd.read_csv('github_issues.csv').sample(n=2000000), $                                    test_size=.10) $ print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns') $ print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns') $ traindf.head(3)
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2014-01-01&end_date=2014-01-02&api_key=S9bQCgmEaGHM8HSDLBNo') $ r.status_code
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ r = requests.get(url).content $ images = pd.read_csv(io.StringIO(r.decode('utf-8')), sep='\t') $ images.to_csv('image-predictions.csv', encoding='utf-8', index=False)
index = mydict['date'].index('2017-06-15') $ openprice = mydict['open'][index] $ highprice = mydict['high'][index] $ lowprice = mydict['low'][index] $ print(openprice, highprice, lowprice)
volumn = pd.DataFrame({ticker: data['Volume'] for ticker, data in all_data.items()})
data[data.density > 10]
df2 = df2.drop([2893])
df.rename(columns={'PUBLISH STATES':'Publication STATUS','WHO region':'WHO Region'},inplace=True) $ df.info()
train_shifted = train.copy() $ train_shifted['y_t+1'] = train_shifted['load'].shift(-1, freq='H') $ train_shifted.head(10)
weekday = austin['weekday'].value_counts() $ weekday= weekday.reindex(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']) $ weekday
file = '../Data_sources/coinbase_historical_pricechange.csv' $ df = pd.read_csv(file) $ df.head()
df.resample('M').mean()
num_plots = plot.regression_contour_fit(x, y, diagrams='../slides/diagrams/ml')
auto_new.Country.unique()
exploration_airbnb.sign_summary() # Get sign summary (look for -1 na encoded value for example)
df_archive_clean = df_archive_clean.drop(["doggo","floofer","pupper","puppo"], axis = "columns")
train_cleaned_df = train_df[['shop_id', 'item_id', 'item_category_id'] + list(range(34))] $ train_cleaned_df.head()
ehash = pd.read_csv('email_hash_list_gh.csv', header=None)
mentions_df = pd.read_csv("/mnt/idms/fberes/network/usopen/data/uso17_mentions_with_names.csv",sep="|") $ mentions_df.head()
final_feat = telemetry_feat.merge(error_count, on=['datetime', 'machineID'], how='left') $ final_feat = final_feat.merge(comp_rep, on=['datetime', 'machineID'], how='left') $ final_feat = final_feat.merge(machines, on=['machineID'], how='left') $ final_feat.describe()
pd.read_sql('desc country', engine)
client.repository.list_definitions()
cats_df[cats_df.isnull().any(axis=1)]
final_.index = final_.Date
learner.save_encoder('adam1_10_enc')
usnpl_clean_url = 'https://raw.githubusercontent.com/yinleon/usnpl/master/data/usnpl_newspapers_twitter_ids.csv' $ df_usnpl = pd.read_csv(usnpl_clean_url) $ len(df_usnpl)
Pnew = df2.converted.mean() $ Pnew
jobs.loc[(jobs.MAXCPUS == '600') & (jobs.GPU == 0)].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
fulldata_copy.plot();
merged_test["T"] = (pd.datetime(2017,1,1) - merged_test['first']).astype('timedelta64[W]') $ merged_test['recency'] = (merged_test['last'] - merged_test['first']).astype('timedelta64[W]') $ merged_test['frequency'] = merged_test['validOrders'] - 1 $ merged_test.shape
store_items.fillna(0)
number_of_commits = git_log['timestamp'].count() $ number_of_authors = len(git_log['author'].dropna().unique().tolist()) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
ins_named[ins_named['business_id'].isin(['71440','2044','73978'])]
baseball.hr.corr(baseball.X2b)
df.set_index('Currency',inplace=True) $ print(df)
priors_product_purchase= priors_product.groupby(["product_id"]).size().reset_index(name ='purchase_count') $ priors_product_purchase.head()
df_new[['CA', 'US']] = pd.get_dummies(df_new['country'])[['CA','US']] $ df_new['country'].astype(str).value_counts()
base1_url = "https://en.wikipedia.org" $ index1_ref = "/wiki/May_1923_Air_Union_Farman_Goliath_crash" $ index1_html = urlopen(base1_url + index1_ref) $ soup2 = BeautifulSoup(index1_html, "lxml")
mailing_list_posts_in_reply_to = mailing_list_posts_mbox_df_saved.filter( $     mailing_list_posts_mbox_df_saved.in_reply_to.isNotNull()).alias("mailing_list_posts_in_reply_to")
open_day('2016-11-06')[(open_day('2016-11-06').user_id == 1545665) & (open_day('2016-11-06').item_id == 30425)]
base_dict_by_place['dict'] = base_dict_by_place['hashtags'].apply(lambda x: count_hash_tags(pd.Series([x])))
import json $ with open(slices_filename) as feed: $     all_slices = json.loads(feed.read()) $     print("OK")
rent_db3 = rent_db2[rent_db2.price < 7000] $ rent_db3.boxplot(column='price')
df.info()
type(df.loc['Mon'])
sns.countplot(x='badge_#', data=df)
weekly_gtrends_data = pd.read_csv("data/multiTimeline_2014-12-01_to_2018-06-27_weekly.csv")
df.iloc[[11,24,37]]
geo_db.head()
twitter_ar.info()
details = client.repository.get_experiment_details(experiment_uid)
import statsmodels.api as sm $ convert_old =  len(df2[(df2['landing_page'] == 'old_page')& (df2['converted'] == 1)]) $ convert_new = len(df2[(df2['landing_page'] == 'new_page')& (df2['converted'] == 1)]) $ n_old = n_old = df2[df2['landing_page']=='old_page'].user_id.count() $ n_new = n_new = df2[df2['landing_page']=='new_page'].user_id.count()
people_csvt = CSVTable.CSVTable("People", "People.csv", ["playerID"]) $ people_csvt.load()
sortedDF = dateCounts.sort_values('dates') $ print(max(sortedDF['countsOnDate']),min(sortedDF['countsOnDate'])) $ sortedDF[0:5]
tl_2020 /= 1000 $ tl_2020_norm = tl_2020 ** (10/11) $ tl_2020_norm = tl_2020_norm.round(1) $ tl_2020_alpha = tl_2020 ** (1/3) $ tl_2020_alpha = tl_2020_alpha / tl_2020_alpha.max().max()
timestring = datetime.datetime.fromtimestamp(time.time()).strftime('%m%d%H%M') $ pivot.to_csv('pivot_' + freq[:5] + '_' + timestring + '.csv')
(autos['last_seen'] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
data_donald_replies.to_csv("ddr_following_and_hashtag.csv")
weather.describe(include='all')
joined_test = joined_test.set_index("Date")
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)
df_SP.to_pickle('DDC_data_cleaned.pkl')
plt.scatter(res, y_pred) $ plt.show()
bnbAx.head()
cur = conn.cursor()
import statsmodels.api as sm $ convert_old = df2.query('landing_page=="old_page"').query('converted==1')['user_id'].count() $ convert_new = df2.query('landing_page=="new_page"').query('converted==1')['user_id'].count() $ n_old = df2.query('landing_page=="old_page"')['user_id'].count() $ n_new = df2.query('landing_page=="new_page"')['user_id'].count()
test= test.reset_index(drop = True) $ test['covered/total'] = pd.Series(predictions) $ datatest = pd.concat([train, test]) $ datatest = datatest.reset_index(drop = True)
fed_reg_dataframe = pd.DataFrame.from_records(tuple_lst, columns=['date','str_text'], index = 'date')
clim_train, clim_test, size_train, size_test = train_test_split(x_climate, y_size, $                                                                 test_size=0.25, $                                                                 random_state = 321)
fraud_df['purchase_time']= pd.to_datetime(fraud_df['purchase_time']) $ fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time']) $ fraud_df['delay_time'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).astype('timedelta64[h]')
strip_func = lambda x: x.strip() if isinstance(x, str) else x $ df = df.applymap(strip_func)
df.select((a == 0).alias('aIsZero')).show(5)
changes['date'] = changes['date'].fillna(method = "ffill") $ changes.iloc[6:10]
%reload_ext autoreload $ %autoreload 2 $ from ktext.preprocess import processor
!$TARGETCALIBPATH/generate_ped -i refdata/Run17438_r0.tio
active_stations = session.query(func.count(Measurement.tobs),Measurement.station).\ $     group_by(Measurement.station).\ $     filter(func.strftime("%Y-%m-%d", Measurement.date) > "2016-08-23").\ $     order_by(func.count(Measurement.tobs).desc()).all() $ print(active_stations)
sentiment = stock['elon_tweet'].apply(sia.polarity_scores) $ sent = pd.DataFrame(list(sentiment)) $ sent.index = stock.index $ sent.columns = ['elon_compound', 'elon_neg', 'elon_neu', 'elon_pos'] $ stock = pd.merge(stock, sent, how='left', left_index=True, right_index=True)
autos.describe(include='all')
from pyspark.mllib.recommendation import MatrixFactorizationModel $ model_path = os.path.join('..', 'models', 'movie_lens_als_20m') $ new_ratings_model.save(sc, model_path) $ same_model = MatrixFactorizationModel.load(sc, model_path)
tsla_neg = mapped.filter(lambda row: row[3] < 0) $
cursor.execute(" select distinct issue_id from ticket_issue ")
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new.converted,df_new[['intercept','CA','UK','new_page']]) $ results = log_mod.fit() $ results.summary()
test_df.head()
ols_model = pymc.Model([Y_ols, beta0_ols, beta1_ols, beta2_ols, beta3_ols]) $ ols_map = pymc.MAP(ols_model) $ ols_map.fit()
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
print len(df) $ print len(df['Master SR #'].unique()) $ print len(df['SR #'].unique())
StockData.loc[(StockData['Date-Fri'] == 1) & (StockData['Date-Q2'] == 1), 'Twitter'] $
imp = importances(RF,team_names[predictor_cols],team_names.regular_occurrences) # permutation $ plot_importances(imp)
df_characters.merge( $     df.groupby( $         'raw_character_text')['episode_id'].nunique().reset_index(), $     on='raw_character_text', $     how='left').head()
P_old=df2['converted'].mean() $ P_old
agg_trips_data.plot.scatter(x='start_station_id',y='count', s= 20, c= 'r') $ agg_trips_data.plot.scatter(x='start_station_id',y='mean_duration') $ agg_trips_data.plot.scatter(x='start_station_id',y='total_duration') $
properati[(pd.isnull(properati['place_name']))]
stn_tempobs_df=pd.DataFrame(stn_tempobs,columns=['Station','Temperature (Deg. Fah.)']) $ stn_tempobs_df
print ('Population mean:', weather.TMAX.mean()) $ for i in range(10): $     sample = pd.Series(weather.TMAX.sample(frac=(i+1)/10, replace=False)) $     sample.mean() $     print ('Sample size:',sample.count(),'Sample mean:', sample.mean(),  'Difference:', sample.mean() - weather.TMAX.mean())
image_predictions.info()
df_ec2[df_ec2['AvailabilityZone'].isnull()]['UsageType'].unique()
train.drop(['Age','Flight Date', 'Booking Date', 'Name'], axis = 1, inplace = True) $ test.drop(['Age', 'Flight Date', 'Booking Date', 'Name'], axis = 1, inplace = True)
turnaround_planes_df.to_excel('turnaround_planes_df1.xlsx', index=False)
df.head() # our raw twitter data from json file
loans_df.drop(axis=1, labels=['earliest_cr_line', 'issue_d'], inplace=True)
journalists_mentioned_by_male_summary_df = journalist_mention_summary(journalists_mention_df[journalists_mention_df.gender == 'M']) $ journalists_mentioned_by_male_summary_df.to_csv('output/journalists_mentioned_by_male_journalists.csv') $ journalists_mentioned_by_male_summary_df[journalist_mention_summary_fields].head(25)
df2.query('landing_page == "new_page"').count()/df2.shape[0]
df_h1b_nyc_ft.pw_1.describe()
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit() $
users.columns
temp_df =  sqlContext.sql("select * from world_bank limit 2") $ print type(temp_df) $ print "*" * 20 $ print temp_df
plt.plot(ds_ossm['time'],ds_ossm['met_salsurf_qc_results'],'.') $ plt.title('CP04OSSM, OOI QC Executed SSS') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
r = df.pct_change(1).dropna() $ r.head(5)
random_integers.max(axis=1)
print(kmeans.cluster_centers_)  $
include_stations = [] $ include_years = [] $ w.set_data_filter(step=0, filter_type='include_list', filter_name='STATN', data=include_stations) $ w.set_data_filter(step=0, filter_type='include_list', filter_name='MYEAR', data=include_years) 
df_btc = pd.read_csv('C:\\Users\\dashs\\Downloads\\cryptocurrency_social-master\\bitcoinprice.csv')
plt.scatter(df_joy['Polarity'], df_joy['Subjectivity'], alpha=0.1, color='purple') $ plt.title('Tweet #joy, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
df = df[df['Reg_date'] + pd.DateOffset(6) == df['Order Date']]
clf = LogisticRegression(fit_intercept=True) $ clf.fit(X_transform, y)
Lags = MSUStats.groupby(level=1).shift(periods=1)  $ Lags.columns = [c+'Lag1' for c in Lags.columns] $ Lags.head()
p_new = df2['converted'].value_counts()[1]/len(df2) $ p_new
tweet_data.head()
celgene_df.info()
np.random.randint(1,100, 10)
def filter_punctuation(text): $     return re.sub('[;,.?!]+', '', text) $ filter_punctuation('hi. my. name. is yolo!;;k;')
!cat crossref-by-doi/*.json | jq -r .message.DOI | grep ';' | wc -l
XGB_model.fit(X_train, y_train)
ab_file.info()    # Displaying information about the dataset
filename = 'Production_model_v2.sav' $ joblib.dump(gbm, filename) $
m, b = np.polyfit(ages, weights,1) $ print(m,b)
lmdict[lmdict.Negative != 0].head()
train['From-To'] = train.apply(lambda row: row[1]+'-'+row[2], axis = 1) $ test['From-To'] = test.apply(lambda row: row[1]+'-'+row[2], axis = 1)
iris.groupby('Species')['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'].describe()
with pd.option_context('display.max_rows', 150): $     print(news_period_df[news_period_df['news_entities'] == ''].groupby(['news_collected_time']).size())
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new,n_old],alternative='larger') $ z_score,p_value
df.isnull().any() #there is no missing values
df2 = df.loc[(df["group"]!="treatment") & (df["landing_page"]=="new_page") ,  ["group","landing_page"]] $ df3 = df.loc[(df["group"]=="treatment") & (df["landing_page"]!="new_page") ,  ["group","landing_page"]] $ df2+df3
plt.plot(backers['Successful'][:100]);
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_2 =  pd.read_pickle("df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p")
tweets_df.possibly_sensitive.describe()
pd.DataFrame(tfidf_vecs.todense(), $              columns=tfidf.get_feature_names() $              ).head()
df_new[['CA','UK','US']]= pd.get_dummies(df_new['country']) $ df_new.head()
plt.barh(ai['title'], ai['quantity'], align='center') $ plt.show()
user_retention = (cohorts['TotalUsers'] $                   .unstack('CohortGroup') $                   .divide(cohorts_size, axis = 1)) $ user_retention.head()
json_data = req.json() $
access_logs_df.count()
df1['Adj. Close'].plot() $ plt.xlabel('Date') $ plt.ylabel('Price') $
label_map = {val: idx for idx, val in enumerate(ndf[y_col].unique())} $ y = df_noblends[y_col].replace(label_map).values
def var(scenarios, level=99, neutral_scenario=0): $     pnls = scenarios - neutral_scenario $     return - np.percentile(pnls, 100-level, interpolation='linear')
df_hdf = dd.read_hdf(target, '/data') $ df_hdf.head()
df2=pd.read_csv('C:\\Users\\Christopher\\Google Drive\\TailDemography\\csvFiles by year\\xCC2017x.csv') $ df2.loc[df2.Toes=='43085',]
xml_in[xml_in['publicationDate'].isnull()].count()
type(watch_df.SubjectCode[0])
dates = pd.read_sql_query('select * from "dimensions_dates"',con=engine)
ttarc_clean = ttarc.copy() $ imgp_clean = imgp.copy() $ tt_json_clean = tt_json.copy()
tesla.plot(y='Adj Close') # plotting by indicating which column we want the values from... $ plt.show()
y = x.loc[:,"A"] $ y
googClose.plot()
eval_pred = linear_model.predict(eval_x) $ print(eval_pred[7:15]) $ print(np.array(eval_y[7:15]))
tf.reset_default_graph()
_,ax=plt.subplots(1,2,figsize=(12,7)) $ sns.countplot(calls_df["call_day"],ax=ax[0]) $ sns.countplot(calls_df["call_time"],ax=ax[1])
lr = LogisticRegression(random_state=20) $ lr.fit(X, y)
notRepairedDamage_list = list(set(train_data.notRepairedDamage))
extract_deduped_with_elms.loc[(~extract_deduped_with_elms.ACCOUNT_ID.isnull()) $                              &(extract_deduped_with_elms.LOAN_AMOUNT.isnull()) $                              &(extract_deduped_with_elms.application_month=='2018-05')].groupby('app_branch_state').size()
s4.unique()
dj_df =  getTextFromThread(urls_df.iloc[0,0], urls_df.iloc[0,1]) $ for i in range(1,5): $         dj_df.append(getTextFromThread(urls_df.iloc[i,0], urls_df.iloc[i,1]), ignore_index = True)
integratedData.to_csv('merged.csv', index = False) $ integratedData.drop('True', axis = 1, inplace = True) $ integratedData.head()
duration = df['dauer'] $ duration.head()
for df in (joined, joined_test): $   df['CompetitionMonthsOpen']=df['CompetitionDaysOpen']//30 $   df.loc[df['CompetitionMonthsOpen']>24,'CompetitionMonthsOpen']=24 $ joined['CompetitionMonthsOpen'].unique()   
nb_class = nb_classifier.train(v_train_full) $ test_predict = [nb_class.classify(t) for (t,s) in v_test]
assembler = VectorAssembler(inputCols = feature_col, outputCol = "features") $ assembled = assembler.transform(ibm_train)
df_enhanced.info()
help(ogh.getDailyWRF_salathe2014)
test = pd.read_csv('test.csv', sep=',') $ print(type(test)) $ test.head(15)
titanic.pivot_table('survived', index='sex', columns='class')
twitter_archive_clean['source'].value_counts()
fileURL = "https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv" $ taxiData = pd.read_csv(fileURL)
props.head(7)
data.drop(['view_count_by_category', 'suma'], axis=1, inplace=True)
coefs = pd.DataFrame(lr_pipe.named_steps['lr'].coef_[0], index = lr_pipe.named_steps['cvec'].get_feature_names(), $                      columns = ['coef']) $ coefs['coef'] = np.exp(coefs['coef']) $ coefs.sort_values(by='coef', ascending = False, inplace=True) $ coefs.head()
from sklearn import linear_model $ from sklearn import datasets, linear_model, metrics $ from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict $ from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV $ lm = linear_model.LinearRegression()
convert_me = "2016bbb12---15" $ datetime.datetime.strptime(convert_me, "%Ybbb%m---%d")
db_connection = ittconnection('prodcopy') $ query = (" SELECT * FROM signal_signal WHERE `signal` = 'ANN_simple' ") $ signals_df = pd.read_sql(query, con=db_connection) $ signals_df.tail(5) $
fig, ax1 = plt.subplots(figsize=(10, 6)) $ ax1.set_ylim(-20, 240) $ bp = plt.boxplot(groups) $ labels = plt.setp(ax1, xticklabels=hours)
df1_normal = df1.copy() $ df1_normal['y'] = np.log(df1_normal['y'])
cdata.describe()
test.datetime.min(), test.datetime.max() # there is one day of overlapping between data and test
incl_Ss.shape
!./flow --imgdir "../darkflow_data/MY20173c_test/" --model cfg/tiny-yolo-voc-3c.cfg --load -1 --lr 0.0001 --batch 32 --gpu 0.8
pgh_311_data.resample("M").count()
repeated_user_id = df2[df2.duplicated('user_id')]['user_id'].values[0] $ repeated_user_id
session.query(Measurements.date).all()
feature_names = tfidf_vectorizer.get_feature_names() $ display_features(features_tfidf[0:10], feature_names)
linked.size
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit() $ results.summary()
df.loc[df['full_sq'] < df['life_sq'] , 'life_sq'] = np.nan 
print(well_data.head())
defs=actual_payments_combined.loc[(actual_payments_combined.iso_date==datetime.date(2015,4,30))& (actual_payments_combined.in_arrears_since_days_30360>90),['fk_loan','iso_date','in_arrears_since_days_30360']].fk_loan.unique() $ defs
data.keys()
from sklearn.feature_extraction.text import CountVectorizer
linkNYC.head()
missing_info = list(df_.columns[df_.isnull().any()]) $ missing_info
session.query(Measurements.station,func.strftime("%m-%d", Measurements.date),func.avg(Measurements.prcp)) \ $        .group_by(Measurements.station,func.strftime("%m-%d", Measurements.date)).limit(5).all()
from sklearn.linear_model import LogisticRegression $ model = LogisticRegression() $ model = model.fit(X, y) $ model.score(X, y)
twitter_ar['rating_num'] = (twitter_ar.rating_numerator/twitter_ar.rating_denominator)*10
AFX_traded_volume = [item[6] for item in AFX_X_2017['dataset_data']['data'] if item[1] != None] $ Avg_AFX_trade_volume = sum(AFX_traded_volume) / len(AFX_traded_volume) $ Avg_AFX_trade_volume = str(Avg_AFX_trade_volume) $ print("The average daily trading volume for 2017 was $" + Avg_AFX_trade_volume + ".")
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_6203.values.squeeze(), lags=40, ax=ax1)
print ("Probability that individual received the new page: %0.4f" % (df2.query(' landing_page == "new_page"').shape[0]/df2.shape[0]))
old_converted = np.random.choice([1,0], size=nold, p=[pmean, (1-pmean)]) $ old_converted.mean()
df2[['control','ab_page']]=pd.get_dummies(df2['group'])
data[data.language.isnull()]
logit_mod = sm.Logit(lr_df['converted'], lr_df[['intercept', 'ab_page']]) $ results = logit_mod.fit()
submissionctb =  pd.DataFrame(  list(abc3) , index = test['comment_id'] , columns = ["is_fake"] ) $ submissionctb.to_csv('submissionctb_F.csv')
print(g1800s.info())
BDAY_PAIR_qthis.pair_age.describe()
m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
print(np.isfinite(testDataVecs).all())
np.mean(n_net.predict(x_test))
headers = {'X-Api-Key': '[Enter your API Key Here]'} $ r = requests.get(url1, headers= headers)   $ r.status_code
logit_mod5=sm.Logit(df3['converted'],df3[['ab_page','intercept', 'CA','UK','weekend']]) $ fit5=logit_mod5.fit() $ fit5.summary()
AAPL.columns  # returns the names of its columns
print('Number of sites: {}'.format(len(portalcuahsi_json)))
print('Index zero, header Date:', data.loc[0,'Date'])
trips=trips3 #get historical data $ trips['day']=pd.to_datetime(trips['day']) # create datetime dates $ trips=trips.set_index('day') #index it on time $ del trips.index.name $ fcst_trips = pd.concat([fcst_trips, trips], axis=1) #join data to commbine history+forecasts
 df.apply(lambda x: sum(x.isnull()),axis=0) 
d - pd.tseries.offsets.Week()
train_data.fuelType.fillna('benzin', inplace = True) $ test_data.fuelType.fillna('benzin', inplace = True)
autos['price'].value_counts().sort_index(ascending=True).head(5)
from pyspark.sql.types import DoubleType, IntegerType $ for col_name in data.columns: $     data = data.withColumn(col_name, data[col_name].cast(DoubleType()))
iris.loc[iris["Species"].isin(["setosa","versicolor"]),:].sample(10)
df_train['totals.pageviews'] = df_train['totals.pageviews'].fillna('0') $ df_test['totals.pageviews'] = df_test['totals.pageviews'].fillna('0')
df = pd.DataFrame(web_stats) # Converted dictionary to a dataframe
sns.lmplot(x="income", y="happy", data=training, x_estimator=np.mean, order=1)
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
!curl -s https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py | python -
from pandas_datareader import data $ ebay = data.DataReader('EBAY', start='2017', end='2018', data_source='iex')['close'] $ ebay.plot()
bb['close'].apply(rank_performance).value_counts().plot()
df[df['Complaint Type'].str.contains('homeless', case=False)]['Complaint Type'].unique()
api.retweeters(id=981924371647393793,count = 10)
from IPython.display import Markdown, display $ def printmd(string): $     display(Markdown(string)) $ p_new = df2['converted'].mean() $ printmd("**P*new***: {:0.4}".format(p_new))
model.fit(x_tr,y_tr,batch_size=32,epochs=6,validation_data=(x_val,y_val))
tweet_archive_enhanced_clean.to_csv('twitter_archive_master.csv',sep =',',encoding = 'utf-8',index= False)
df.describe()
rng = np.random.RandomState(20180326)
preprocessor=preprocessing(word_embedder.vector_size,word_embedder) $ preprocessor
len(archive_clean[archive_clean.name.isnull()]) == len(faulty_names)
df2.query("converted==1").count()[0]/len(df2)
for c in np.unique(res.labels_): $     print('{}: {}'.format(c, np.sum(c_pred==c)))
tf_mat1 = tf.constant(matrix1) $ tf_mat2 = tf.constant(matrix2)
autos["ad_created"].str[:10].value_counts(normalize=True, $                                    dropna = False $                                   ).sort_index()
df['is_rank_1'] = False $ df.loc[df['Rank'] == 1, 'is_rank_1'] = True
twitter_df_clean.loc[twitter_df_clean.rating_numerator > 10, 'rating_numerator'] = 10
join_c.orderBy(F.desc("party_id_orig"),F.desc("aggregated_prediction")).select('party_id_orig','aggregated_prediction','predicted').show(500)
df2.query('group=="control"').converted.sum()/df2.query('group=="control"').count()[0]
dfv = pd.Series(documents)
DataDescriptionFileName = "DataDescription.json" $ with open(DataDescriptionFileName, 'w') as f: $     json.dump(DataDescription, f, indent=4) $ print("Saved data description to {}".format(DataDescriptionFileName))
approved.head()
calories_df = df_summary.groupby('Date').sum( $ ).reset_index().sort_values(by='Date', ascending=1) $ calories_df = calories_df.filter(items=['Date', 'Calories']) $ calories_df.rename(columns={'Calories': 'ttl_cal_burnt'}, inplace=True) $ calories_df['day_no']= calories_df.index + 1
417*0.061028*6.77
df2.query('group=="treatment"').converted.sum()/df2.query('group=="treatment"').count()[0]
first_commit_timestamp = git_log.loc[699070,'timestamp'] $ last_commit_timestamp = pd.to_datetime('today') $ corrected_log = git_log[(git_log.timestamp >= first_commit_timestamp) & (git_log.timestamp <= last_commit_timestamp)] $ corrected_log['timestamp'].describe()
fig,ax1 = plt.subplots(1,1) $ ax1.plot(data[['Close','Bollinger High','Bollinger Low']]) $ y = ax1.get_ylim() $ plt.show(); $
intervention_history.columns
injuries_hour.columns
filter_df = filter_df[filter_df['start_time'] <= datetime(2016, 11, 8, 23, 59, 59)] $ filter_df.head(2)
inter = mr * dev $ inter = inter.between_time('10:00', '15:59') $ pd.ols(y=mr, x=inter.tshift(1, 'min'))
datetime.now().toordinal() - datetime(1987, 1, 7).toordinal()
subreddits.keys()
df_low_temps = pd.concat(low_temps)
tweet_df = pd.read_csv('tweet_json.txt') $ tweet_df.info()
unique_domains.sort_values('total_payout', ascending=False).head()
fig, axes = plt.subplots(2,figsize=(20,10),sharex=True) $ axes[1].plot(tbl2.index,tbl2['250alpha']) $ axes[1].set_title('Rolling Alpha') $ axes[0].plot(tbl2.index,tbl2['250beta']) $ axes[0].set_title('Rolling Beta')
image_predictions = pd.read_csv('data/image-predictions.tsv', sep = '\t') $ image_predictions_copy = image_predictions.copy() $ image_predictions_copy.head()
df.head()
cleaned_df_y = cleaned_df.pop('acct_type')
calls_df.head()
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US_ind_ab_page', "US","ab_page"]]) $ results = log_mod.fit() $ results.summary()
Q2 = pd.merge(df_low_temps, df_hi_temps, how='left', on=['Date'])
df2['intercept'] = 1 $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
print '{} districts w bad data'.format(len(df_nulls.district_id.unique())) $ print "{} districts w bad data that don't have good data".format(len(np.setdiff1d(df_nulls.district_id.unique(), $                                                                                   df_nona.district_id.unique()))) $ print 'This is {:.1f}% of all districts'.format(float(len(df_nulls)) / len(df_nona) * 100)
INQ2016.head(1)
new_page_converted = np.random.choice([0,1], size = (145310), p = [0.88, 0.12]) $ p_new = (new_page_converted == 1).mean() $
assert (twitter_archive_clean[twitter_archive_clean['in_reply_to_status_id'].isnull()==False].shape[0]==0 $         and $         twitter_archive_clean[twitter_archive_clean['in_reply_to_user_id'].isnull()==False].shape[0]==0)
def lemmatizing(tokenized_text): $     text = [wn.lemmatize(word) for word in tokenized_text] $     return text $ infinity['text_lemmatized'] = infinity['text_nostop'].apply(lambda x: lemmatizing(x))
knn = KNeighborsClassifier(n_neighbors=20) $ print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())
import numpy as np $ x = np.array([1, 2, 3, 4, 5, 6, 7, 8]) $ y = x $ plt.figure() $ plt.scatter(x, y)
df.info() $
rent_db2 = rent_db[rent_db.price < 1000000] $ rent_db2.head()
df.head()
old_page_converted = np.random.choice([1, 0], size=n_new, p=[page_old, (1-page_old)])
df = pd.concat([prices, volumes], axis=1) $ df.columns = ["prices", "volumes"] $ df.head()
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=50000) $ df.head() $
eval_sklearn_model(stock.true_grow, stock.predict_grow)
assortativity = nx.attribute_assortativity_coefficient(multiG, 'politicalSpectrum') $ print assortativity
latest_timelog.head()
df[df['Complaint Type'] == 'Illegal Fireworks'].index.dayofyear.value_counts().sort_index().plot()
X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.2, random_state=23)
df_usa = df1[df1['Area'].isin(['United States of America'])]    #getting data where area is USA $ df_usa  = df_usa.set_index(['Year'])                            #setting Year as index
names.groupby('name')['retweets'].sum().sort_values(ascending=False).head(10)
df['ra'].unique()
data.head()
processed_tweets_with_obs['t_ob'] = processed_tweets_with_obs['obs'].apply(lambda x: json.loads(x[2:-1])['currently']['temperature']) $ processed_tweets_with_obs['flt_ob'] = processed_tweets_with_obs['obs'].apply(lambda x: json.loads(x[2:-1])['currently']['apparentTemperature'])
c.find_one() # any object
frame.rank(axis='columns')
df_date_vs_sentiment = reddit_comments_data.groupBy('created_date').agg({'sentiment':'mean'}).orderBy('created_date').toPandas() $ plt.clf() $ df_date_vs_sentiment.plot(x='created_date', y= "count",kind='line') $ plt.show()
num_users = df.nunique()['user_id'] $ print("The number of unique users in the dataset is : {}".format(num_users))
glm_binom_feat_3 = H2OGeneralizedLinearEstimator(family='binomial', model_id='glm_v6', lambda_search=True) $ glm_binom_feat_3.train(covtype_X, covtype_y, training_frame=train_bf, validation_frame=valid_bf)
svm_clf = SVC(decision_function_shape="ovr") $ svm_clf.fit(X_train_scaled[:10000], y_train[:10000])
tablename='email_templates' $ pd.read_csv(read_inserted_table(dumpfile, tablename), $             delimiter=",", $             error_bad_lines=False).head(10)
predicted = tips_model.predict(ind_test) $ score = 100.0 * accuracy_score(dep_test, predicted) $ print(f'Smoker [Numeric & Categoric] classification score = {score:4.2f}%') $ print(classification_report(dep_test, predicted))
log_model_2 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','US','UK']]) $ result = log_model_2.fit() $ result.summary()
df.head(10)
z_score, p_value = sm.stats.proportions_ztest([17489,17264], [145274, 145310],alternative = 'larger')
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
filt_dt=pd_aux2.issued_date.isna() $ pd_aux2['new_date']=pd_aux2['issued_date'] $ pd_aux2.loc[filt_dt,['new_date']]=pd_aux2.permit_creation_date[filt_dt] $ pd_aux2.head(10)
gender = records[records['Gender'].isnull()]['First_name'].apply(get_gender)
df_twitter_extract_copy.info()
Imagenes_data.tweet_id.unique().size
received_old_page = len(df2.query('landing_page == "old_page"')) $ received_old_page
pyLDAvis.gensim.prepare(lmlist[8], corpus, dictionary)
RF.score(X_test,Y_test)
search_fc = gis.content.search("title:AVL_Direct_FC", item_type='Feature Collection') $ iowa_fc_item = search_fc[0] $ iowa_fc_item
msft_monthly_cum_ret = msft_cum_ret.resample('M') $ msft_monthly_cum_ret
set(df[df['author'] == 'send_nasty_stuff'].subreddit)
print('\nAverage value of each column:\n', google_stock.mean())
dataAnio.to_csv("data/dataPorUbicacion_Anios_tmax.csv")
so = (np.sort(-m3, axis=None)) $ rsf = so.reshape(2, int((m3.size)/2)) $ sorted_m3 = -rsf $ print("sorted m3: ", sorted_m3)
def get_next_scrape_times(min_time, max_time): $     start_time = datetime.now() # TO SUBTRACT LATER $     scrape_time = random.randint(min_time, max_time) # HOW LONG WE'LL SCRAPE FOR BEFORE TAKING A BREAK $     return start_time, scrape_time $
linear = linear_model.LinearRegression() $ linear.fit(x, y) $ linear.coef_, linear.intercept_
d=soup.find('li').get_text() $ des
obj=Series(range(3),index=['a','b','c']) $ index=obj.index $ index
mask = (nullCity["creationDate"] > '2018-01-01') & (nullCity["creationDate"]<= '2018-12-31') $ nullCity2018 = (nullCity.loc[mask]) $ nullCity2018.head()
clean_trips = trips $ clean_trips = clean_trips[clean_trips['duration'] <= 86400 ] $ clean_trips = clean_trips[clean_trips['duration'] > 120 ] $ clean_trips.duration.describe()
deployment_details = client.deployments.create(model_guid, name="MNIST keras deployment")
tt_final.drop(['id'], axis = 1, inplace= True) $
df=pd.read_csv('LibCon03-06-2018.csv') $ df.shape
strategy.df_data().head()
for i in range(len(twitter["user"])): $     twitter["compound"].append(SentimentIntensityAnalyzer().polarity_scores(twitter["text"][i])["compound"]) $     twitter["positive"].append(SentimentIntensityAnalyzer().polarity_scores(twitter["text"][i])["pos"]) $     twitter["neutral"].append(SentimentIntensityAnalyzer().polarity_scores(twitter["text"][i])["neu"]) $     twitter["negative"].append(SentimentIntensityAnalyzer().polarity_scores(twitter["text"][i])["neg"])
print(new_model) $ print(model.wv.vocab)
df = df.dropna() $ df = df[df!=0] $ df = df.dropna() $ df.describe()
print train_data[['building_id','display_address','listing_id','manager_id', $                  'street_address']].apply(lambda col: col.nunique())
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam', $                   'Eric Idle', 'Terry Jones', 'Michael Palin'])
data.head()
sql("describe formatted orders").toPandas()
archive_clean['dog_stages'] = archive_clean['text'].str.extract('(puppo|pupper|floofer|doggo)', expand=True) $ archive_clean.drop(['doggo','floofer','pupper','puppo'],axis=1,inplace=True)
dataset.info()
df_group_by.head(20)
iso_gdf.intersects(iso_gdf_2)
for i in range(50): $     print('TWEET:\n', pulledTweets_df.date.values[i], '\n', pulledTweets_df.text.values[i], $           '\n\nSentiment Prediction NB:\n', pulledTweets_df.sentiment_predicted_nb.values[i], $           '\nSentiment Prediction LR:\n', pulledTweets_df.sentiment_predicted_lr.values[i], $           '\n')
twitter_archive_master.info()
word_ids = vocab.loc[tokens].values $ word_ids
my_dictionary = { 'first':1, 'second':2, 'third':3 } $ my_dictionary['first_number'] = my_dictionary['first'] # Although this adds the key as the the last item of the dictonary ... $ print(my_dictionary)                                   # ... dictionaries do not have an order, so that is not a problem. $ del my_dictionary['first'] $ print(my_dictionary)
roc_auc_score(preds, fb_test.popular)
confusion_matrix(fashion.index, fashion.predicted)
PSN = df[df['start_station_name']== 'Pershing Square North'] #create df with only rows that have PSN as start station $ PSN_grouped = PSN.groupby('end_station_name').count().reset_index() #group them by end station names $ PSN_grouped.end_station_name.sort_values(ascending = False).head() #and sort in descending order
test.date.value_counts().shape
df2['intercept'] = 1 $ df2[['ab_page', 'control_page']] = pd.get_dummies(df2['landing_page']) $ df2.head()
tfav.plot(figsize = (16,4), label = "Likes", legend = True) $ tret.plot(figsize = (16,4), label = "Retweets", legend = True);  $
df.head()
ab_data.shape
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index()
bet_under  = [x[1] > .62 for x in pred_probas_under]
def get_child_frame(node: dict, frame: pandas.DataFrame = None) -> pandas.DataFrame: $     if frame is not None: $         return frame[get_child_column_names(node)] $     return pandas.DataFrame(data=get_child_column_data(node)).T
data["author"].describe()
def compute_perf(m, xts, yts): $     ypred = m.predict(xts.reshape(xts.shape[0],24,-1)) $     py = np.squeeze(np.reshape(ypred, newshape= [-1,1]), axis=1) $     r2 = mt.r2_score(yts, ypred) $     return r2
daily_returns.kurtosis()
sorted(get_freq(series_obj=raw.minutes_spent.apply(lambda x: round(x, 0))))
stock['target'] = stock.daily_change.shift(-1) $ stock.head()
print("Number of mislabeled points out of a total %d points : %d" % \ $       (X_clf.shape[0], (y_clf != y_pred).sum()))
kick_projects_ip.head()
inactive_creation_ratio = [x/len(clean_users[clean_users['active']==0]) for x in list(clean_users[clean_users['active']==0]['creation_source'].value_counts())] $ inactive_creation_ratio
n_new = len(df2.query('landing_page=="new_page"')) $ n_new
import matplotlib.pyplot as plt
a = pd.DataFrame(DataAPI.schema.get_schema("indicator")).T $
from scipy.stats import norm $ norm.cdf(z_score)
twitter_archive_clean.name.value_counts()
measurements_year = session.query(Measurements.date,Measurements.prcp).limit(10).all() $ for mammal in measurements_year: $     print(mammal)
rng2 = pd.date_range(start = '07/01/2017', end = '07/21/2017', freq = usb) $ rng2[0:3]
twitter_url = "https://twitter.com/marswxreport?lang=en" $ browser.visit(twitter_url)
robin_df = devices_df[devices_df['entity_id'] == 'device_tracker.robins_iphone'].set_index('last_changed').drop_duplicates() # Can set this index as will be unique for single device $ robin_df[0:10]
lr2 = LogisticRegression() $ params2 = {'penalty': ['l1', 'l2'], 'C':np.logspace(-5,0,100)} $ gs_2 = GridSearchCV(lr2, param_grid=params2, cv=10, verbose=1) $ gs_2.fit(X2, y2)
df.isnull().sum()[0]
treatment_mean = (df2.query('group == "treatment"')['converted']==1).mean() $ treatment_mean
import thesis.utils.metrics
weather_df.T
df_ad_airings_filter_3 = df_ad_airings_2.copy()
fmt = "%Y-%m-%d %H:%M:%S" $ data_sample['startDate'] = pd.Series([datetime.datetime.strptime(i, fmt) for i in data_sample['startDate']])
percent_quarter.plot()
df_uro_no_metac = df_uro.drop(columns = ['MET_DATE1']) $ df_uro_no_metac.head()
busy_stations = session.query(Measurement.station, func.count(Measurement.tobs)).filter(Station.station == Measurement.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ busy_stations
print('Original Grocery List:\n', groceries) $ groceries.drop('apples', inplace = True) $ print() $ print('Grocery List after removing apples in place:\n', groceries)
d6tstack.utils.read_excel_advanced(cfg_fnames[0], $                                    sheet_name='Sheet1', header_xls_range="B2:E2").head()
own_star['rating'] = own_star[['starred','owned']].sum(axis=1) $ own_star.head(20)
column_dataloaders = {x: torch.utils.data.DataLoader(column_datasets[x], batch_size=128, $                                              shuffle=True, num_workers=0) $               for x in ['train', 'val']}
full_data.head()
X = df.drop(['body', 'closed_date', 'completed_by', 'created_date', 'title', 'comments'], axis=1) $ print(X.shape) $ preds = classifier.predict(X) $ print(preds)
media_user_results_df.to_csv("MediaTweetsData.csv", encoding="utf-8", index=False)
autos['odometer_km'].head()
g_geo = g_geo.drop('index',axis=1)
df.head(5)
internet=pd.read_csv('internet.csv' ,  dtype = {'dates': str,'percentage': float})
Stockholm_data_final = pd.concat([Stockholm_data, pd.DataFrame(y_predict_bayes)], axis=1)
intervention_train['CRE_DATE_GZL'].min(), intervention_train['CRE_DATE_GZL'].max()
mean = np.mean(data['len']) $ print("The lenght's average in tweets: {}".format(mean))
plt.figure(figsize=(15, 5)); $ plt.plot(np.sin(np.arange(50)), 'c-*');
data.set_index("date", inplace=True) $ data.head()
n_old = df2.query('group == "control"').user_id.nunique() $ n_old
%matplotlib notebook $ fig, ax = viewer.draw(nrn, mode='3d') $ ax.set_autoscale_on(True) $ fig.set_size_inches(10,10)
!grep -A 50 "build_estimator" taxifare/trainer/model.py
%%time $ X_test_term = vectorizer.transform(X_test['text'])
control_df = df2.query('group == "control"') $ p_control = control_df.query('converted == 1').user_id.nunique()/control_df.user_id.nunique() $ print('Probability of conversion for the control group is {}.'.format(round(p_control,4)))
metadata['bad_band_window2'] = refl.attrs['Band_Window_2_Nanometers'] $ metadata['bad_band_window2']
prop = props [props.prop_name == 'PROPOSITION 063- FIREARMS. AMMUNITION SALES. INTIATIVE STATUTE.'] 
print("actual difference in convert rate: {}".format(obs_diff))
liquor2016_q1 = liquor2016[liquor2016.Date.dt.quarter == 1]
data = grouped_publications_by_author.copy()
ben_final['FirstMeta'] = ben_final.groupby('userid')['pagetitle'].first().str.contains('/')
scaled_X = Df_X.drop(['id', 'author', 'text', 'spacy_text','lemma', 'created_at'], 1) $ scaled_X = scalar.fit_transform(scaled_X) $ y = Df_X.author $ X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, random_state=24)
print(surveys_df.columns) $ print(species_df.columns)
wkrng[0].dayofweek
filtered_active_sample_sizes.groupby().agg(F.sum("sample_size_1")).show()
gs_rfc_under = GridSearchCV(pipe_rfc, param_grid=params_rfc, cv=ts_split_under, scoring='roc_auc') $ gs_rfc_under.fit(X_train, y_train_under)
baseball_h.index[:10]
our_nb_classifier.predict("The car lights turned off and THROTTLE did not work when driving for a long time")
user1 = api.get_user(129111088) $ print(user1.name, user1.screen_name, str(user1.statuses_count), str(user1.friends_count), str(user1.followers_count))
test_orders_prodfill=pd.merge(test_orders_prod3,prods, on='product_id') $ test_orders_prodfill.head()
old_page_converted = np.random.binomial(1, p, n_old)
print(y_train.value_counts()) $ print(y_val.value_counts())
new_eng.drop(['visited_y','count'],axis=1,inplace=True)
data = pd.Series([0.25, 0.5, 0.75, 1.0], $                 index=['a','b','c','d']) $ data
graf_counts['AFFGEOID'] = graf_counts['AFFGEOID'].astype(str)
df_new = df2.query('landing_page == "new_page"') $ n_new = df_new.shape[0] $ n_new
prev_year_start = dt.date(2017, 3, 4) - dt.timedelta(days=365) $ prev_year_end = dt.date(2017, 3, 11) - dt.timedelta(days=365) $ tmin, tavg, tmax = calc_temps(prev_year_start.strftime("%Y-%m-%d"), prev_year_end.strftime("%Y-%m-%d"))[0] $ print(tmin, tavg, tmax)
dd.plot(sample_field='Subject',gui='jupyter',bary_fields=['_calour_diff_abundance_group'])
clean_measure = measure.fillna(0)
predictions_clean.head()
def fnCleanSentence(text): $     text = re.sub(stripNA, "", text) $     text = text.replace("<br />", " ") $     return text
metrics, predictions = pipeline.test(X_eval, y_eval, output_scores=True) $ print("Performance metrics on test set: ") $ display(metrics)
result_co.summary()
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
sensors = hp.search_sensors(type='electricity', system='solar') $ print(sensors) $ df = hp.get_data(sensors=sensors, head=head, tail=tail, diff=True, unit='W') $ charts.plot(df, stock=True, show='inline')
product_color_grouped = paid_success_with_data.groupby(['MVGR6']).size().sort_values(ascending=False) $ product_color_grouped = product_color_grouped[product_color_grouped > 1] $ product_color_grouped.head()
def sort_dict_values(d, reverse=False): $     return sorted(d.items(), key=lambda x: x[1], reverse=reverse)
filtered_events = events.where( col("user_id").isNotNull() ) $ events_grouped_no_nulls = filtered_events.groupBy(["user_id", "event_date"]).count()
mytext=list(map(lambda tweet: remove_stop_words(tweet), support_or_not.text)) $ support_or_not.text=mytext
iris_new = pd.DataFrame(iris_mat, columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','Species']) $ iris_new.head(20)
print(f'dataframe shape: {league.shape}') $ league.head()
len(taxi_hourly_df.index.unique())
temp_cat.get_values()  #array
df_new['country'].unique()
prediction_modeling2.unpersist()
df_columns[df_columns['Complaint Type'].str.contains('Noise')]['Complaint Type'].value_counts().plot(kind='barh') $
pd.Series([1,2,3,np.nan]).mean(skipna=False)
plt.hist(p_diffs); $ plt.axvline(obs_diff, color='r', linewidth=2);
bet_over  = [x[1] > .6 for x in pred_probas_over]
gdax_trans_btc['Timestamp'] = gdax_trans_btc['Timestamp'].map(lambda x: x.replace(second=0))
sp.describe()
bmp_series = pd.Series(brand_mean_prices) $ mile_series = pd.Series(mean_mileage) $ brand_df = pd.DataFrame(bmp_series, columns = ['mean_prices']) $ brand_df['mean_mileage'] = mile_series $ brand_df
df2_dup['group'].iloc[0],df2_dup['landing_page'].iloc[0],df2_dup['converted'].iloc[0]
g = sns.FacetGrid(train, col ='loan_status') $ g.map(sns.distplot, "loan_amnt");
import zipfile $ with zipfile.ZipFile(ml_1m_path, "r") as z: $     z.extractall(dataset) $ with zipfile.ZipFile(ml_20m_path, "r") as z: $     z.extractall(dataset)
temp_stations = pd.DataFrame(session.query(Measurement.date, Measurement.tobs).\ $                               filter(Measurement.date >= '2016-08-23').\ $                               filter(Measurement.station == 'USC00519281').order_by(Measurement.date).all()) $ temp_stations.set_index('date', inplace = True) $ temp_stations.hist(bins=12) $
os.mkdir('submissions')
df_likes = pd.merge(df_likes,df_authors,left_on='liker',right_on='authorId')
(hdf['Age'].mean(level=1) $  .reset_index(name='AvgAge') $  .sort_values('AvgAge', ascending=False) $  .nsmallest(10, 'AvgAge'))
pd.value_counts(RNPA_existing['ReasonForVisitName'])
im = pd.read_csv("image-predictions.tsv", sep='\t') $ im.info()
baseball_newind.sort_index(axis=1).head()
with pm.Model() as model: $     alpha = 1.0/summary.mean()[0] $     lambda_1 = pm.Exponential("lambda_1", alpha) $     lambda_2 = pm.Exponential("lambda_2", alpha) $     tau = pm.DiscreteUniform("tau", lower = 0, upper = n_count_data - 1)
temp_df2['titles'] = temp_df2['titles'].astype(str)
first_movie.h3
ftfy.ftfy(fb_cleaned)
import scipy.sparse $ scipy_sparse_matrix = scipy.sparse.random(5,2) $ corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix) $ scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)
forcast_col='Adj. Close'
for i in files: $     tracks = pd.read_csv("data/"+i,sep="\n") $     top_tracks[i.replace(".txt","")] = tracks $
url = ('http://dccouncil.us/calendar') $ browser = webdriver.Firefox() $ browser.get(url) $ wait = WebDriverWait(browser, 30)
samp_size = n $ joined_samp = joined.set_index("Date")
fulldf.describe()
shows['genres'].fillna('missing', inplace=True)
d3 = dt.datetime(2012,12,21,12,21,12,21) $ d3.replace(month=11,day=10,hour=9,minute=8,second=7,microsecond=6)
data_archie.head(5)
alg3 = KNeighborsClassifier(5) $ alg3.fit(X_train, y_train) $ probs = alg3.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
ctd = gcsfs.GCSFileSystem(project='inpt-forecasting') $ with ctd.open('inpt-forecasting/Census Tool Data Pull CY2017- May 2018CONFIDENTIAL.xls.xlsx') as ctd_f: $   ctd_df = pd.read_excel(ctd_f)
df_twitter_clean.head()
autos['price'].value_counts().sort_index(ascending=True).head(30)
df = pd.DataFrame(np.arange(12).reshape(4, 3), columns = ['a', 'b', 'c']) $ df
classify_df['neutered'] = 0 $ classify_df.loc[classify_df['Neutered']=='Spayed','neutered'] = 1 $ classify_df.loc[classify_df['Neutered']=='Neutered','neutered'] = 1
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').shape[0] $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
mft = most_followed_tweeters.followers $ normalized_followers = np.round((np.abs(np.array(mft)-np.array(mft).mean())/mft.std())*10, 0)
df2.head()
d.weekday()
test_data = prepare_data(intervention_test_df).drop(drop_columns, axis=1)
df_hourly.rename(columns={'time': 'TimeStamps'}, inplace=True) $ df_hourly.rename(columns={'CarCount': 'VehicleCountperHour'}, inplace=True) $ df_hourly['TimeStamps'] = df_hourly['TimeStamps'].values.astype('datetime64[s]') $ df_hourly = df_hourly.reindex(['TimeStamps','VehicleCountperHour'], axis=1) $ df_hourly.tail()
pold = df2['converted'].mean() $ pold
def our_function(input): $     return np.sqrt(input)
start = datetime.now() $ modelxg_t1000 = xgb.XGBClassifier(max_depth=50, learning_rate=.1, n_estimators=1000, n_jobs=-1) $ modelxg_t1000.fit(Xtr.toarray(), ytr) $ print(modelxg_t1000.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
df.sum()
commits = Query(git_index).get_cardinality("hash").by_period() $ print("Trend for month: ", get_trend(get_timeseries(commits))) $ commits = Query(git_index).get_cardinality("hash").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(commits)))
twitter_archive_full.to_csv(data_folder+'twitter_archive_master.csv', index=False)
sl_data4 = sl_data3.set_index(['Description', 'Date']).sort_index() $ sl_data4.columns = ['sl'] $ sl_data4
new_dems.newDate.head()
def strip_insertintologs(text): $     try: $         return int(text.replace('INSERT INTO `logs` VALUES (',"")) $     except AttributeError: $         return text 
recommendations = (model.recommendProducts(1059637, 5)) $ recommendations[: 5] $ recArtist = set(list(elt[1] for elt in recommendations)) $
print("Showing %s items."%len(resp['search-results']['entry'])) $ total_count = resp['search-results']['opensearch:totalResults'] $ start_index = resp['search-results']['opensearch:startIndex'] $ print("A total number of %s results found in Scopus; Start index is %s."%(total_count, start_index)) $ print("\033[43m%s\033[0m"%resp['search-results']['entry'][0])
test= test.reset_index(drop = True) $ test['covered/total'] = pd.Series(predictions) $ datatest = pd.concat([train, test, rest]) $ datatest = datatest.reset_index(drop = True) $ datatest.loc[datatest['surface_covered_in_m2'].isnull(),'surface_covered_in_m2'] = datatest['surface_total_in_m2']*datatest['covered/total']
y = df.iloc[:,7].values $ X = df.iloc[:,[0,1,2,3,4,5,6,8,9,10,11,12]].values
import pickle $ pickle.dump(rf_reg,open('random_forest.sav','wb'))
df.groupby('episode_id')['id'].nunique().hist()
master = df.merge(context, how='left', left_on='Recipient Email Address', right_on='email') $ master.head()
!./container/send_container_to_amazon.sh $(image)
df.dtypes
pst.npar,pst.nobs
print("Probability of treatment group converting:", df2[df2['group']=='treatment']['converted'].mean())
mid = (quotes['ask'] + quotes['bid'])/2 $ mid.name = 'mid' $ temp = mid.to_frame().join(result.possession) $ temp = temp.join(result.capital/result.capital[0]) $ temp.plot(subplots=True, figsize=(10,10));
eth = pd.read_csv("Ether-chart.csv", sep=',') $ eth['date'] = ' ' $ eth.info() $
df = pd.read_csv("song_count.csv",index_col=0) $ plt.bar(range(len(df.Count)),df.Count,align="center") $ plt.xticks(range(len(df.Count)),df.index,rotation=70)
ts = soup.find_all('div', attrs={'class':'art_title'}) $ for i in ts: $     print i.contents[0] $
with open("./minisom.save", "w") as f: $     f.write("SOM {} {} {} {} {} {}\n".format(20, 30, 8292, 100, 0.5, 0.3)) $     for i in som.get_weights(): $         for j in i: $             f.write(" ".join(map(str, j)) + "\n")
from dateutil.relativedelta import relativedelta $ twelve_months = dt.datetime.today() + relativedelta(months=-12) $ twelve_months = twelve_months.strftime('%Y-%m-%d') $ twelve_months
p_new = df2.converted.mean() $ p_new $
prcp_df.head() # Display the top 5 records of the dataframe
p_new= df2['converted'].mean() $ p_new
def load_tweets(path): $     with open(path) as f: $         x = json.load(f) $     return x $
example.head(5)
tt.sum()*1./len(test_df)
df.to_csv("crypto_sent.csv")
from test_package.print_hello_function_container import print_hello_function as print_hello $ print_hello() $ from test_package.print_hello_function_container import print_hello_function $ print_hello_function() $
acc.find(vdate='20161103')
dfList.head()
ridge2 = linear_model.Ridge(alpha=0.5) $ ridge2.fit(x3, y) $ (ridge2.coef_, ridge2.intercept_)
predictions = model.predict(x_test)
reg.score(X_train,Y_train)
df.groupby(['Team','Year']).groups
print('reduce memory') $ utils.reduce_memory(df) $ df.drop(['t-1_date'], axis = 1, inplace = True)# saving memory $ df = df.tail(n = df.shape[0] -1 )
p_new_page= df2.query('landing_page == "new_page"').count()[0]/df2.count()[0] $ print('The probability that an individual received the new page is {}'.format(p_new_page))
type2017.isnull().sum()
df_merged.boxplot(column='favorite_count');
x = topics_data.comms_num $ y = topics_data.score $ print("Correlation between Number of Comments and Total Score is:", round(np.corrcoef(x, y)[0,1], 2)) $
with open(lgb_xgb_combined_pred_pickle_file_name, 'rb') as f: $     pred = pickle.load(f) $ print(pred[:10])
df.shape[0] #Returns the number of rows and columns
print fs.list() $ print fs.find({"filename" : "scansmpl.pdf"}).count()
cleaned_sampled_contirbutors_human_df_saved.schema
df_pageviews_mobile = copy.copy(df_pageviews_mobile_web) $ df_pageviews_mobile.views = df_pageviews_mobile.views + df_pageviews_mobile_app.views $ df_pageviews_mobile.head()
weather = weather.groupby("Date").mean()
len(df.index)
xml_in['authorId'].nunique()
averaged_data.mean(dim=('x','y')).plot() $ plt.show()
tq1.remove('question1') $ tq2.remove('question2')
df = pd.read_csv('house/train.csv') $ df
conn = psycopg2.connect('host=stuffed dbname=cusp')
df[df.B > 0]
df.landing_page.value_counts()
df[df['Descriptor'] == 'Pothole'].index.hour.value_counts().head()
store_items.toPandas().head()
topics_indexes = {x[0]:index for index,x in enumerate(tags_freq.most_common(number_of_topics))}
convert_rate_p_old = df2.converted.mean() $ print('Convert rate of p_old under the null is:{}'.format(convert_rate_p_old))
import statsmodels.api as sm $ df2['intercept']=1 $ df2[['aa_page','ab_page']]=pd.get_dummies(df2['group']) $ df2.head()
%matplotlib inline $ data4.plot()
m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)
eval = pd.read_csv(eval_file, header=0, sep='\t', na_values=[''], keep_default_na=False) $ X_eval = eval.iloc[:,features_index] $ y_eval = eval['Label'] $ np.all(eval.columns == train.columns)
lats[:] = np.arange(-89.5,89.6)
shows.isnull().sum()
df_clean3.nsmallest(10, 'rating_numerator')[['text', 'rating_numerator', 'rating_denominator']]
df.dropna(subset=["PREV_DATE"], axis=0, inplace=True)
StockData.describe()  # pandas.DataFrame.describe() shows statistics on each numeric column
lr2 = LogisticRegression() $ %time lr2 = lr2.fit(train_4_reduced, y_train) $ %time y_hat_lr2 = lr2.predict(train_4_reduced) $ print(roc_auc_score(y_train, y_hat_lr2)) $ print(log_loss(y_train, y_hat_lr2))
no_conv, yes_conv = df2.query('group == "treatment"').converted.value_counts() $ yes_conv/(no_conv + yes_conv)
x = store_items.isnull().sum() $ print(x)
total_rows = df.shape[0] $ print(total_rows)
indeed.head()
(act_pay.fk_loan==36).sum()
val dbConnection = ... $ lines.map(... dbConnection.createStatement(...) ...) $ dbConnection.close() // Wrong!
table3.head(3)
np.abs(df2['Change'])
pd.read_html(html_string, header=0)[0]
knn_predicted = knn_model.predict(X_final)
tmp = data_df.copy() $ tmp.columns = [x if re.search("size", x) else "data_{}".format(x) for x in tmp.columns]
z_values, labels = vae_latent(nn_vae, mnist_test_loader) $ plt.plot(z_values[:,0], z_values[:,1], ".")
df_2015 = df.sort_values('store_number') $ df_2016 = df.sort_values('store_number')
draft_df['license_issue'] = draft_df['words'].apply(mh_to_cat, args = (license_words,)) 
dff.head()
unsorted_Google_data = pd.read_csv(Google_data_file) $ unsorted_Google_data.head()
srctake = srctake[srctake['level_2'].isin(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])] $ srctake['my'] = srctake[['Year','level_2']].apply(lambda x: pd.to_datetime(str(int(x[0]))+' '+str(x[1]), format='%Y %b'),1) $ srctake.columns = ['sourceid','Year','Month','Use','datetime'] $ srctake['systemid'] = srctake['sourceid'].apply(lambda x: str(x)[0:9],1)
autos['price'].value_counts().sort_index(ascending=True).head(10)
dfRegMet =pd.read_pickle(data + "dfRegMet.p")
news.symbol.value_counts()[:15]
acc.find(t_type='46')
dfgts.shape
sn.kdeplot(train_binary_dummy['age'])
databreach_2017 = databreach_2017.dropna(axis=0,how='any')
pca.explained_variance_ratio_
def calculate_embeddings(tokens): $     token_embdgs = [embeddings.get(t) for t in tokens] $     token_embdgs = [t for t in token_embdgs if t is not None] $     return numpy.mean(token_embdgs, axis=0)
dates = pd.date_range(date.today(), periods=2) $ dates
1/np.exp(-0.0408),np.exp(0.0099)
M_data = session.query(Measurement).first() $ M_data.__dict__
austin.drop(258, inplace=True)
to_be_predicted_Day1 = 51.40 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
new_data["flavor"] = new_data["LMN"]+new_data["CHR"]+new_data["STW"]+new_data["VAN"] $ new_data["flavor"] = new_data["flavor"].map(lambda x: 1 if x >0 else 0)
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
print("Mean squared error: %.2f" $       % mean_squared_error(y_test, y_pred))
equipment = pd.read_csv(data_repo + 'equipment.csv', dtype={'CODE_POSTAL': object}, **import_params)
df.loc[:'s2']
for col in autos.columns: $     print(col)
df2.query("group=='treatment' & converted==1").count()[0]/df2.query("group=='treatment'").count()[0]
%timeit data['age'] $ %timeit data_rec['age'] $ %timeit data_rec.age
watch_df.loc[watch_df.SubjectCode == 1050.0]
df.info()
testObjDocs.buildOutDF(tst_lat_lon_df[985:990])
with open('total_review_apps_eng_lower.pickle', 'rb') as d: $     total_review_apps_eng_lower = pickle.load(d)
sky = TextBlob("The sky is blue") $ sky.sentiment
df[df.client_event_time < datetime.datetime(2018,4,1,23,0)][['client_event_time', 'client_upload_time', 'event_time', 'server_received_time']].sort_values('client_event_time').head()
d311_gb.to_csv(processed_path+"c311_incident_count.csv")
autos['odometer_km'].sort_index(ascending=True).head()
joined=load_df('joined_elapsed_events.pkl')
api = twitter.Api(consumer_key=twitter_key, $                   consumer_secret=twitter_secret, $                   access_token_key=twitter_token, $                   access_token_secret=twitter_token_secret)
print(type(coin_data.index)) $ coin_data.columns
dum.columns
byName = df.groupby('Name') $ byName.sum()
x["sum"]=x[list(x.columns)].sum() $ x
def top_retweets(inputfile,k,n): $     rt =  inputfile[inputfile['retweeted_status.retweet_count']>=n] $     rt_sorted = rt.sort_values('retweeted_status.retweet_count',ascending=False) $     return rt_sorted[:k]
plt.figure(figsize=(10, 7)) $ plt.plot(range(0,n_comp), list(tfidf_svd_v2.singular_values_)) $ plt.xlabel('Reduced Feature Space') $ plt.ylabel('SVD Singular Values') $ plt.title('Elbow Plot for TFIDF-SVD(' + str(n_comp) + ' reduced features)');
RN_PA_duration.tail()
dark_sky['state'].plot(figsize=(20,5)); $ plt.ylabel("Temperature $^\circ$C");
tweet_archive_enhanced_clean['floofer'].value_counts()
type(ds) $ ds.keys()
weather_features = pd.DataFrame(index=weather_data.index)
    clean_prices = prices.loc[(prices['EMA_200'].notnull() & prices['5_day_target'].notnull())] $
test_features = spark.read.csv(os.path.join(mungepath,"model_data/20180504/rf_lr_lasso_inter2_noip/test_features/*"), header=True) $ print("Number of observations in test :", test_features.count())
MAX_DOCUMENT_LENGTH = 800 $ processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH) $ x_train = np.array(list(processor.fit_transform(x_train))) $ number_of_words = len(processor.vocabulary_) $ x_test = np.array(list(processor.transform(x_test)))
twitter_archive_clean[twitter_archive_clean.rating_denominator != 10]
data_lda = {i: OrderedDict(ldamodel.show_topic(i,15)) for i in range(total_topics)}
print(df2['landing_page'].value_counts(normalize=True)) $ print(df2['landing_page'].value_counts())
users_converted = df.converted.sum() $ prop_users_converted = users_converted / total_rows $ print(prop_users_converted)
num_missing = df.isnull().values.ravel().sum() $ print("Number of row with missing values - {}".format(num_missing))
df_new.query('group == "treatment" and country =="CA"')['converted'].mean()
new_page = df2['landing_page'] == 'new_page' $ df2[new_page]['user_id'].count()/df2['user_id'].count()
df.describe(include='all')
all_res = pd.read_csv("/Users/sdas/GoogleDrive/Papers/DeepLearning/DLinFinance/SP_Data_shared/DLIndex_Random_results_30_10000_30.csv") $ all_res.head() $ all_res.describe()
mlp_pc_fp = 'data/richmond_median_list_prices_percent_change.csv' $ mlp_pc.to_csv(mlp_pc_fp, index=True) # Pass index=True to ensure our DatetimeIndex remains in the output
appointments.shape
gamma.delta_line_plot(vect=[0.5, 0.5, -1], length_unit=length_unit) $ plt.show()
results = reg1.fit() $ type(results)
fig = data["author"].value_counts()[:].plot.pie(labels = None) $ plt.show()
table = pd.read_html(url_mars_facts) $ table[0]
poo = df[df.Year_of_Release.isin(df.release_year)==False] $ poo[['Year_of_Release','release_year', 'Name','Platform']].head()
df_final_edited_10.plot(x='rating_numerator', y='retweet_count', kind='scatter');
titanic.pivot_table('survived', index='sex', columns='class', aggfunc='sum')
df_goog.sort_values('Date', inplace=True) $ df_goog.set_index('Date', inplace=True) $ df_goog.index = df_goog.index.to_datetime() $
d = datetime.datetime(2018, 2, 18)
vectorized_text_predict = vectorizer.transform( ... ) $ vectorized_text_predict.toarray()
replies = pd.read_csv('LaManada_new/tblreplies.csv',sep=SEP,quotechar='"') $ replies.shape
df_z_obs_CTD_1988 = ds_z_obs_CTD_1988.to_dataframe() $ df_temp_casts_CTD_1988 = ds_temp_casts_CTD_1988.to_dataframe() $ df_temp_obs_CTD_1988 = ds_temp_obs_CTD_1988.to_dataframe()
dicttagger_sentiment = DictionaryTagger(['positive.yml','negative.yml'])
df = pd.read_csv("../../data/msft.csv",header=0,names=['open','high','low','close','volume','adjclose']) $ df.head()
stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ results.summary()
n_new = df2[df2.group == "treatment"].count()[0] $ print("The population of user under treatment group: %d" %n_new)
sales = pd.DataFrame(sale2_table) $ sales
plt.hist(old_page_converted);
journalists_retweet_df = retweet_df.join(user_summary_df['gender'], how='inner', on='retweet_user_id', rsuffix='_retweet') $ journalists_retweet_df.rename(columns = {'gender_retweet': 'retweet_gender'}, inplace=True) $ journalists_retweet_df.count()
git_blame.path.value_counts().head()
total_stations = session.query(Station.station).distinct(Station.station).count() $ print('Total number of stations is ' + str(total_stations))
Grouping_Year_DRG_discharges_payments.groupby('drg3').get_group(871)[:4]
ddf = dd.read_csv('test-data/output/sample-xls-case-badlayout1.xlsx-*.csv') $ ddf.compute().head() # dask breaks! use d6tstack.combine_csv $
calls_df.describe(include=["object"])
precipitation_count = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23").count() $ print(" There are {} results being analyzed over the past 12 months".format(precipitation_count))
results.head()
df_bkk.describe()
final_data = final_data[jobs_data['clean_titles'].isin(freq_titles['clean_titles'])] $ final_data.nunique()
player_attributes = pd.read_sql_query('select * from Player_Attributes', conn)  # don't forget to specify the connection $ print(player_attributes.shape) $ player_attributes.head()
df.shape
tweet_df.iloc[:,15:].sample(5)
df1.shape, df2.shape
brand_dict = {} $ for b in brands: $     brand_dict[b] = autos[autos['brand'] == b]['price'].mean() $ brand_dict    
tweet_df_clean.head(2)
learner = md.get_model( $     opt_fn, em_sz, num_hidden, num_layers, dropouti=drops[0], dropout=drops[1], $     wdrop=drops[2], dropoute=drops[3])
df_questionable = pd.merge(left= df_links, left_on= 'link.domain', $                            right= df_os, right_on= 'domain', how= 'inner')
News_outlets_df['Date'] = pd.to_datetime(News_outlets_df['Date'])
df_weather.head(5)
df2['zeros']=0  # created a column of zeros with zero values $ df2.head()
tweets_pp = pd.concat([multi.reset_index(), pp.reset_index()], axis=1) $ tweets_pp.head(2)
x = np.arange(-2,2,0.01) $ plt.plot(x,x*x, 'blue', label='$x^2$') $ plt.plot(x,abs(x), 'orange', label='|x|') $ plt.legend(loc="upper center") $ plt.show()
pivot_thershold = prefiltering_of_neighbors(pivot_corr, 0.1) $ pivot_thershold
from tensorforce.execution import Runner $ runner = Runner(agent=agent, environment=environment)
for index in df_wrong_rating.index: $     df_enhanced['rating_10_scaled'][index] =  df_wrong_rating['rating_10_scaled'][index]
type(model.wv.syn0) $ len(model.wv.vocab) $ model.wv.syn0.shape
cust_data.columns
consumer_key, consumer_secret, access_token, access_token_secret = (open("../../credentials.txt") $                                                                     .read().split("\n"))
length = len(df2.query("group == 'control'")) $ pr=len(df2.query("group=='control' & converted == 1")) $ prob = pr/length $ prob
simband_time_df.set_index('subject_id', inplace=True)
df2=pd.read_csv('ab_data.csv')
index.save('/tmp/deerwester.index') $ index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')
ml.TensorBoard.stop(tensorboard_pid)
di = pd.to_datetime("December 25, 2017") + pd.to_timedelta(np.arange(4*7), 'D') $ di
train_df.info()
LabelsReviewedByDate = wrangled_issues_df.groupby(['closed_at','Category']).closed_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'purple', 'red'], grid=False)
model.wv.vocab
output= "SELECT user_id, user_name from user limit 10" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','user_name'])
df2['day']=df2['datetime'].dt.day
image_clean.info()
auto.head()
data.set_index(['Record ID'], inplace=True)
def corr_matrix(df): $     df = df[['prices','volume','delta_day', 'compound', 'numcomments']] $     return df.corr(method='pearson') $ corr_matrix(eth)
data = json_normalize(r.json(), [['fantasy_content', 'leagues', '0', 'league']]) $ data
uniq_sorted_churned_plans_counts = sorted(uniq_churned_plans_counts,key=lambda x:x[0].tolist())
gdp_df=gdp_df.rename(index=str, columns={"TIME_PERIOD": "Year", "All industry total [Detroit-Warren-Dearborn MI (Metropolitan Statistical Area)]": "Detroit GDP"})
image_pred_df.query('tweet_id == 884441805382717440')
def calc_temps(start, end): $     results = session.query(Measurements.date, func.avg(Measurements.tobs), func.min(Measurements.tobs), func.max(Measurements.tobs)) \ $         .filter(Measurements.date >= start).filter(Measurements.date <= end) \ $         .group_by(Measurements.date).all() $     return pd.DataFrame(results, columns=["Date", "Avg_temp", "Min_temp", "Max_temp"]).set_index('Date')
df2.groupby('converted').count() $
master_df = archive_df.join(tweets_df) $ master_df = master_df.join(images_df)
autodf = rawautodf.drop('name',axis = 1) $ print( "Size of the dataset - " + str(len(autodf)))
for post in test_collection.find(): $     pprint.pprint(post)
pd.read_clipboard()
lr = LogisticRegression(random_state=20, max_iter=10000) $ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr','multinomial'], 'solver':['saga','newton-cg', 'lbfgs']} $ grid = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
df_vow.sort_values('Date', inplace=True) $ df_vow.set_index('Date', inplace=True) $ df_vow.index = df_vow.index.to_datetime()
cutoff_times = generate_labels('/7/KMLZlMBnmWtb9NNkm3bYMQHWrt0C1BChb62EiQLM=',  trans, $                                label_type = 'SMS', churn_period = 14) $ cutoff_times[cutoff_times['churn'] == 1].head()
df['Sale (Dollars)'] = df['Sale (Dollars)'].str.replace(r'[$,]', '').astype('float')
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
ndays = 123 $ nscen = 10 $ dates = pd.date_range('2018-02-13', periods = ndays) $ dates
twitter_archive_master.to_csv('./WeRateDogs_data/twitter_archive_master.csv')
ind = ['NY.GDP.PCAP.KD', 'IT.MOB.COV.ZS'] $ dat = wb.download(indicator=ind, country='all', start=2011, end=2011).dropna() $ dat.columns = ['gdp', 'cellphone'] $ print(dat.tail())
repos[repos.id == 6]
series1.cov(series2)
%%time $ df.head()
suspects_with_1T_27['imsi'].value_counts()
df = pd.merge(ign, salesdec, on=['Name','Standard_Plat'], how='outer',indicator=True) #merge both upon standardization $ df.isnull().any()
df_valid = pd.read_json("valid.json", orient="records")
print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ imagelist = [i for i in os.listdir() if i.endswith(".pdf")  ] $ imagelist
df.iloc[[0, 56, 1033, -1]]
StationDailyClean = (ttDailyClean $                      .groupby(['C/A','UNIT','STATION','DATE']) $                      .Corrected_Entry.sum().reset_index()) $ StationDailyClean.DATE = pd.to_datetime(StationDailyClean.DATE) $ StationDailyClean.head()
free_data.mean(),free_data.count()
header_names = {'1. symbol':'sym', $                 '2. price':'price_str', $                 '3. volume':'vol'}
close_month.loc[month]
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==666287406224695296,'rating_numerator']=9 $ twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==666287406224695296,'rating_denominator']=10
X_conv_train.shape
df_columns['Day in the year'] = df_columns['Created Date'].str.extract(r"(\d\d/\d\d)/\d\d\d\d ", expand=False) $ df_columns.head() $
cig_data  = pd.read_csv('cigarette.csv', index_col = 0, engine ='c', sep = ',') $ cig_data
daily_returns.plot(kind='scatter',x='SPY',y='XOM') $ plt.show()
excelDF.Quantity.max()
rng.values[1]
cityID = '84229b03659050aa' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Virginia_Beach.append(tweet) 
datAll['Block_range'] = np.where(~pd.isnull(datAll['Block Range']),datAll['Block Range'],np.nan) $ datAll['Block_range'] = np.where(~pd.isnull(datAll['BlockRange']),datAll['BlockRange'],datAll['Block_range'])
start = pd.datetime(2010, 1, 1) $ end = pd.datetime.today() $ p = web.DataReader("^GSPC", 'yahoo', start, end)  #S&P500 $ p.tail()
m = RandomForestClassifier(n_estimators=1, max_depth=4, bootstrap=False, n_jobs=-1, random_state=42, min_samples_leaf=10) $ m.fit(X_train, y_train) $ print_score(m)
df_schoo11 = df_schools.rename(columns={'name':'school_name'}) $ df_schoo11.head()
pd.DataFrame(features['MONTH(joined)'].head())
assists_df.to_pickle('assists_df.pkl')
df = data.dropna() $ df
df['name']= df['name'].apply(convert)
len(b_cal_q1.groupby('listing_id').mean().index)
(new_reps.newDate[1] - max(new_reps.newDate[new_reps.Cruz.isnull()])).components.days $
import statsmodels.api as sm  $ logit_model=sm.Logit(Y_train,X_train) $ result=logit_model.fit() $ print(result.summary2());
df = pd.read_sql('SELECT * FROM actor WHERE actor_id = 172', con=conn) $ df
y.mean()
now = datetime.now() $ print(now)
ab_df_new['treatment_US'] = ab_df_new.ab_page * ab_df_new.US $ ab_df_new['treatment_CA'] = ab_df_new.ab_page * ab_df_new.CA $ ab_df_new.head()
membership['new_date'] = membership['new_date'].apply(lambda x:pd.to_datetime(x)) $ membership['new_date'] = membership['new_date'].apply(lambda x:x.strftime('%B-%Y'))
temp_df.groupby('reorder_interval_group').corr()
pd.DataFrame({'Yes': [50, 21], 'No': [131, 2]})
url = "https://data.wprdc.org/datastore/dump/76fda9d0-69be-4dd5-8108-0de7907fc5a4" $ pgh_311_data = pd.read_csv(url) $ pgh_311_data.head()
df = df[df['type']=='User'] $ df.drop(['type'], axis=1, inplace=True)
tweet_archive_enhanced_clean['rating_denominator'].value_counts()
y = new["Price"] $ x = new.drop("Price",axis=1)
from config import config $ CONFIGS = config.Config.get(env='prod', caller_info=False)
red_inter_recr.to_pickle(folderPartial + 'red_inter_recr.pkl')
dd_df = dd_df.drop(['manuell'], axis=1)
df.shape    
twitter_archive_clean = twitter_archive.copy() $ image_predictions_clean = image_predictions.copy() $ tweet_json_clean = tweet_json.copy()
not_fit = ['pp_xoffset_min', 'pp_yoffset_min', 'pp_xoffset_max', 'pp_yoffset_max', 'tstamp', $           'collector_tstamp_x', 'etl_tstamp', 'collector_tstamp_y', 'dvce_created_tstamp', 'event', 'domain_userid', $           'domain_sessionid'] $ consistent_with_action = ['action_label', 'action_type'] $ click_condition_meta.drop(not_fit + consistent_with_action, axis = 1, inplace = True)
p_new = df2['converted'].mean() $ p_new
cars.yearOfRegistration.unique() $
dedup[dedup.job_type == 'Sponsored'].hash.value_counts()[0:3]
users.head()
stackedCloses.loc['2012-01-03', 'AAPL']
analyze_set.info()
df_new[['ca', 'uk', 'us']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('ca', axis = 1) $ df_new.head()
ratings_df = ratings_df.drop('timestamp', 1) $ ratings_df.head()
df_clean = df_clean[df_clean['retweeted_status_id'].isnull()]
few_recs.ix[0]
learn.load("clas_0")
tweet_data_copy.info()
df2.query('landing_page == "new_page"').user_id.count() / df2.user_id.count()
people.groupby(lambda x:GroupColFunc(people,x,'a')).mean()
train = train.fillna(-999) $ test = test.fillna(-999)
last_date = df.iloc[-1].name $ last_unix = last_date.timestamp() $ one_day = 86400 $ next_unix = last_unix + one_day
score['is_false_positive'] = np.where((score['pred_lr']==1) & (score['is_shift']==0), 1, 0) $ score['is_false_negative'] = np.where((score['pred_lr']==0) & (score['is_shift']==1), 1, 0) $ score['is_true_positive'] = np.where((score['pred_lr']==1) & (score['is_shift']==1), 1, 0) $ score['is_true_negative'] = np.where((score['pred_lr']==0) & (score['is_shift']==0), 1, 0)
df_tick_sent = df_tick.join(df_amznnews_2tick)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&limit=1&api_key=" + API_KEY $ r = requests.get(url)
type(1.0)
from nltk.tokenize import TweetTokenizer $ from nltk import pos_tag $ from nltk.stem import WordNetLemmatizer $ import re
y=pd.DataFrame(y)
len(df.query("group == 'treatment' & landing_page == 'old_page'")) + len(df.query("group == 'control' & landing_page == 'new_page'"))
df_weather.head(5)
raw_full_df[raw_full_df.building_id=='96274288c84ddd7d5c5d8e425ee75027'].head()
df_precipitation = pd.melt(df_precipitation, id_vars=['Year'], var_name='Month', value_name='Precipitation')
my_list = [0.25, 0.5, 0.75, 1.0] $ data = pd.Series(my_list) # "the constructor" $ data
num_dont_treatment = df.query("group == 'treatment' and landing_page == 'old_page'") $ num_dont_newpage = df.query("group == 'control' and landing_page == 'new_page'") $ print(len(num_dont_treatment)) $ print(len(num_dont_newpage))
to_be_predicted_Day2 = 17.69737131 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
words_only_sp_freq = FreqDist(words_only_sp) $ print('The 100 most frequent terms (terms only): ', words_only_sp_freq.most_common(20))
data['Week Ending Date'] = pd.to_datetime(data['Week Ending Date']) $ data['year'] = data['Week Ending Date'].dt.year $ data.head(5)
csv_data = pd.read_csv("Returns.csv") $ print(csv_data)
z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new, convert_old]), $                                               np.array([n_new, n_old]),value=0.0 , alternative = 'larger') $ print(z_score) $ print(p_value)
df.empty
train['PassengerId'].max()
autos.describe(include = 'all')
slFullDf.index.get_level_values(0).value_counts()
df_2016['bank_name'] = df_2016.bank_name.str.split(",").str[0] $
df.iloc[0:3, [0, 2]]
x_test = np.float32(x_test) $ y_test = np.float32(y_test)
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']]) $ results=lm.fit() $ results.summary()
df[df['Agency Name'] == 'New York City Police Department']['Complaint Type'].value_counts().head().plot(kind='bar') $ df[df['Agency Name'] == 'Department of Transportation']['Complaint Type'].value_counts().head().plot(kind='bar')
print(str(autos["odometer_km"].unique().shape[0]) + " unique values") $ print(autos["odometer_km"].describe())
for urlTuple in otherPAgeURLS[:3]: $     sociologistParagraphsDF = sociologistParagraphsDF.append(getTextFromWikiPage(*urlTuple),ignore_index=True) $ sociologistParagraphsDF $
df_clean.drop(df_clean[(df_clean.rating_denominator != 10)].index,inplace=True);
print("Number of Mitigations in Enterprise ATT&CK") $ print(len(all_enterprise['mitigations'])) $ df = all_enterprise['mitigations'] $ df = json_normalize(df) $ df.reindex(['matrix','mitigation', 'mitigation_description', 'url'], axis=1)[0:5]
vectorizer.vocabulary_
store_info.CompetitionDistance.fillna(store_info.CompetitionDistance.median(),inplace=True) $ store_info.CompetitionDistance.isnull().any()
dfRegMet2016.info()
from mako.template import Template
x = np.arange(6).reshape((2, 3)); x  #x has shape (2,3).
import re $ p=re.compile('href="(http://.*?)"') $ nodes=p.findall(_html) $ for node in nodes: $     print node
date_price.head(1)
mean_database = pd.DataFrame(mmbb_series, columns=['mean_mileage']) $ mean_database
df16 = pd.read_csv('2016.csv')
dd_df_gearbox = pd.get_dummies(dd_df['gearbox']) $ dd_df_notRepairedDamage = pd.get_dummies(dd_df['notRepairedDamage']) $ dd_df_fuelType = pd.get_dummies(dd_df['fuelType'])
train.pivot_table(values = 'Fare', index = 'Hour', aggfunc = np.mean)
breakfastlunchdinner.sort_values(['dinner'], ascending=False)
df2.columns = ['c','d'] $ df2
print (df.iloc[ [1,3], [1,3]],'\n' )   # by individual rows/columns $ print (df.iloc[  1:3 ,  1:3], '\n')    # by range
np.set_printoptions(precision=2)
user = api.get_user(twitter_names[0])
df_train['totals.newVisits'] = df_train['totals.newVisits'].fillna('0') $ df_test['totals.newVisits'] = df_test['totals.newVisits'].fillna('0') $
pd.Series([1, 2, 3, 4, 5])
df2 = df.copy() $ df2[df2.isnull()] = 0 $ df2
from sklearn.naive_bayes import GaussianNB $ nbc = GaussianNB() $ nbc = nbc.fit(d_train_sc, l_train) $ score = 100.0 * nbc.score(d_test_sc, l_test) $ print(f"Gaussian Naive Bayes accuracy = {score:5.1f}%")
volume_idx = afx['dataset']['column_names'].index('Traded Volume') $ traded_volume = [entry[volume_idx] for entry in afx['dataset']['data'] if entry[volume_idx]] $ print(traded_volume[:5]) $ sum(traded_volume) / len(traded_volume) $
(df_final[df_final['Scorepoints'] == 160]).head()
meritPTags = wikiMeritSoup.body.findAll('p') $ for pTag in meritPTags[:5]: $     print(pTag.text)
move_34p14u34p = (breakfastlunchdinner.iloc[1, 1] $                + breakfastlunchdinner.iloc[5, 2] $                + breakfastlunchdinner.iloc[1, 3]) * 0.002 $ move_34p14u34p
image_predictions.p1.value_counts()
%matplotlib inline $ best.plot.box(grid=True,subplots=True, figsize=(15, 10));
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
dd_df_fuelType.head()
df = pd.read_csv('tweet_csvs/realDonaldTrump_tweets.csv', index_col = None, header = 0, $                      parse_dates=['created_at'], infer_datetime_format = True, dayfirst = True)
sel_df.Beat.value_counts(dropna=False)
df[0:1000]['AQI'].mean()
result['goal_usd'] = result.goal * result.rate $ result['pledged_usd'] = result.pledged * result.rate
autos = autos[autos["registration_year"].between(1900,2016)] $ print(autos["registration_year"].describe()) $ print(autos["registration_year"].value_counts(normalize = True).head(20))
np.zeros(5)
ratesuccess = df[['main_category', 'successful']].groupby(by='main_category').mean().sort_values( $     by='successful', ascending=False) $ ratesuccess.plot.bar(title='Rate of success by category');
actual_diffs = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ actual_diffs
df2.head(3) $
sql="SELECT * FROM %s.%s LIMIT 3" % (schema, data_table) $ HTML(hc.sql(sql).toPandas().to_html())
session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).order_by(func.count(Measurement.id).desc()).all()
nycshp.ZIPCODE = nycshp.ZIPCODE.astype(int)
df_providers_pared.columns
review_df = review_df.rename(columns={'overall': 'Rating'}) $ print ("Total data:", str(review_df.shape)) $ review_df.head()
to_be_predicted_Day2 = 56.14260511 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
plt.scatter(training_pending_ratio[:,0], training_pending_ratio[:,1], color ='g') $ plt.xlabel('Actual Pending Ratios') $ plt.ylabel('Polynomial Pending Ratios') $ plt.title('Plotting the Pending Ratios V/s Polynomial Pending Ratios', fontweight = 'bold') $ plt.tight_layout()
stations.dtypes
dfs = [] $ for time, month_range in times.items(): $     dfs.append(wiki_table_to_df(time, month_range)) $ df = pd.concat(dfs) $ print('We have {} registered attacks from January 1st, 2011 up to today (November 28th, 2017)'.format(df.shape[0]))
sp['day_ago_low'] = sp.Low.shift(periods = 1) $ sp['week_ago_low'] = sp.Low.shift(periods = 7)
p_old = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ print('p old convert rate: ', p_old)
titanic.dropna().describe()
frame.reindex(columns=['Texas', 'Utah', 'California'])
answer1 = [] $ for lis in inner_list: $     if datetime.strptime(lis[0], '%Y-%m-%d').year == 2017: $         answer1.append(lis)
x =  store_items.isnull().sum().sum() $ print('Number of NaN values in our DataFrame:', x) $
dr.shape
new_page_df = df2.query('landing_page == "new_page"')
rtc = rtc.set_index('Export Region') $ rtc = rtc.drop(['Year'], axis=1)
X_extra = Train_extra.drop('Approved', axis=1) $ y_extra = Train_extra['Approved']
print(contribs.info())
tweets_df.head()
autos = autos[autos["registration_year"].between(1900, 2016)]
train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)
twitter_Archive.head()
%%time $ grid_svc.fit(X_tfidf, y_tfidf)
from collections import Counter $ Counter(" ".join(df1['tweet_text']).split()).most_common(10)
import statsmodels.api as sm $ df2['intercept']=1; $ logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]); $ results = logit_mod.fit();
inter1.head()
xtx, xty = get_covariance(returns, index_weighted_returns) $ xtx = pd.DataFrame(xtx, returns.index, returns.index) $ xty = pd.Series(xty, returns.index) $ helper.plot_covariance(xty, xtx)
for post in got_data.head(10)['id']: $     print("https://facebook.com/"+post)
tzs = DataSet['userTimezone'].value_counts()[:10] $ print(tzs)
vectorizer = TfidfVectorizer(token_pattern=r'\w{2,}', ngram_range=(1,2), stop_words='english', sublinear_tf=True) $ desc_matrix = vectorizer.fit_transform(final_data['clean_description']) $ feature_names = vectorizer.get_feature_names() $ print(feature_names[:20])
ratings.sample(5)
tokendata.head()
props = pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
expectancy_for_least_country = le_data.min(axis=0) $ expectancy
troll_users.drop(columns=diff_user_cols, axis=1, inplace=True)
data_month = data_nonan_temp.groupby(['Month']) $ data_month.describe()
logit_countries2 = sm.Logit(df4['converted'], $                            df4[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ result3 = logit_countries2.fit()
autos = autos[(autos["registration_year"]>=1886)&(autos["registration_year"]<=2016)] $ autos.head()
twitter_archive_full.info()
sum(pd.isnull(newdf['score']))
english_df = df.filter(df.lang == 'en').select( $     df.timestamp_ms, $     df.lang, $     lowercase(df.hashtag).alias('hashtag'), $     functions.from_unixtime(df.timestamp_ms / 1000, 'yyyy-MM-dd HH:mm:ss').alias('date'))
nodes.head()
df2.plot(figsize=(10,10)) $ plt.show()
df_parsed.head()
t1.astimezone(timezone('EST')) - t2
df2[df2['user_id'].duplicated()].index
data[['returns', 'strategy']].std() * 252 ** 0.5
y = analyze_set['date'] $ count = Counter(y) $ count.most_common(1)
journalist_mention_gender_summary(journalists_mention_df[journalists_mention_df.gender == 'F'])
X_train, X_test, y_train, y_test = train_test_split(X_dfgrb, y_dfgrb, test_size=0.2) $ params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 2, $           'learning_rate': 0.01, 'loss': 'ls','random_state':42 } $ grbreg = GradientBoostingRegressor(**params) $ grbreg.fit(X_train, y_train) $
print type(rdd_example2) $ print rdd_example2.collect() $
s.asfreq('Q').head()
np.power(fruits, 2)
rmse_CSCO=np.sqrt(mean_squared_error(df2['diff_CSCO_log'][1:-1],pred_CSCO))
adds.to_csv(folder + "\\" + TC_adds, sep="\t", index = False) $ deletes.to_csv(folder + "\\" + TC_deletes, sep="\t", index = False)
spyder_etf = get_pricing(dwld_key, start_date.strftime(date_fmt)) $ spyder_etf.name = dwld_key + ' ETF Index' $ s_etf = (spyder_etf.pct_change() + 1).cumprod()
from sklearn.externals import joblib $ ab = joblib.load('adaboost.pkl') 
words = text.split() $ print(words)
brand_counts = autos["brand"].value_counts(normalize = True) $ popular_brands = brand_counts[brand_counts>0.05].index
df2.landing_page.value_counts()
Chem.rdMolDescriptors.CalcMolFormula(mol)
clean_rates.to_csv('twitter_archive_master.tsv', index=False, sep='\t', encoding='utf8')
print('Data imported at: ', datetime.datetime.now())
pd.DataFrame(np.random.rand(3, 2), $             columns=['foo', 'bar'], $             index=['a', 'b', 'c'])
tweet_df["tweet_date"] = pd.to_datetime(tweet_df["tweet_date"]) $ tweet_df.sort_values("tweet_date", inplace=True) $ tweet_df.reset_index(drop=True, inplace=True) $ tweet_df.head()
imdb_df.drop(axis=1, labels='Unnamed: 0', inplace=True) $ imdb_df.drop(axis=1, labels='Unnamed: 4', inplace=True) $ imdb_df $
xgb_learner.num_rounds
print(dfd.hspf.describe()) $ dfd.hspf.hist()
tweets_clean.retweet_count.mean()
data.head()  #Show the first few rows. data.tail shows the last few.
grouper = dta.groupby(dta.results)
print(df2.sort_values(['timestamp']).iloc[0]) $ print(df2.sort_values(['timestamp']).iloc[-1])
diabetes_without_wellness = diabetes_risk.join(wellness_visits, $                                                ['person_ref'], $                                                'left_anti') $ diabetes_without_wellness.limit(10).toPandas()
pkl_file = open('think_tanks.pkl', 'rb') $ think_tank_party_dict = pickle.load(pkl_file) $ pkl_file = open('speeches_metadata.pkl', 'rb') $ speeches_cleaned = pickle.load(pkl_file)
dftopcomplaint.sort_values(['count_complaints_day'], ascending = 0, inplace = True)
from sklearn.metrics import accuracy_score $ from sklearn.linear_model import LogisticRegression $ from sklearn.pipeline import Pipeline $ from time import time $ from  sklearn.feature_extraction.text import CountVectorizer
df_bill_data[df_bill_data['patient_id'] == sample_repeat]['amount'].sum()
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
import statsmodels.api as sm $ convert_old = ab_df2.query('group == "control"')['converted'].sum() $ convert_new = ab_df2.query('group == "treatment"')['converted'].sum() $ n_old = ab_df2.query('group == "control"').shape[0] $ n_new = ab_df2.query('group == "treatment"').shape[0]
from keras.models import Sequential $ from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Embedding, Reshape, merge, Merge $ from keras.callbacks import Callback, EarlyStopping
file.latest_file_info
print(json.dumps(res_json, indent=4))
df5 = pd.read_csv('2005.csv')
ab_df2.converted.mean()
(p_diffs > p_diff_actual).mean()
import pandas as pd $ import numpy as np $ url = 'small_train.csv' $ small_train = pd.read_csv(url, parse_dates=[0, 11, 12])
autos.price.value_counts().sort_index(ascending = False).head(10)
from sklearn.ensemble import RandomForestClassifier $ forest = RandomForestClassifier(n_estimators = 110) $ forest = forest.fit( X, y )
df3 = df3.groupby(['month','name']).count() $ df3 = df3.reset_index('name') $ df3.rename(columns={'activity_date_time_c': 'average_activity_count_per_liaison'}, inplace=True) $
ax = merge_.plot(x='index', y='ratio', legend=False, $                  title='Ratio of Questionable Links Shared by Russian Trolls', $                  figsize=(16,6)) $ ax.xaxis.label.set_visible(False) $ fig = ax.get_figure();
df_clean3[df_clean3.rating_denominator != 10].sample(5)
model = tf.global_variables_initializer()  # model is used by convention
adopted_cats['Y']=0 $ adopted_cats.loc[adopted_cats['DaysInShelter']<29,'Y']=1
df.set_index(['operator', 'part'], inplace=True)
print(lr.coef_)
np.exp(-1.9888)
DummySwapRepricing = ((df_old['DummySwapMW'] / -2) * delta(df_new, df_old, ['RealPrice'])).rename('DummySwapRepricing') $ Exposure = (df_old['ExposureMWH'] * delta(df_new, df_old, ['RealPrice']) - df_new['ExposureMWH'] * df_new['CapCurve']).rename('Ex[psure]') $ DummyCapRepricing = ((df_old['DummyCapMW'] / -2) * delta(df_new, df_old, ['CapCurve'])).rename('DummyCapRepricing') $ CapVolMvt = ((df_new['DummyCapMW'] + df_new['CapMW'] - df_old['DummyCapMW'] - df_old['CapMW']) * df_new['CapCurve'] *-1).rename('CapVolMvt')
scoring_ind['response_x'][(scoring_ind['response_root'].isnull())].unique()
festivals.reindex(festivals.index.drop('Fest'))
master_list[master_list['Count'] == 1]['Count'].describe()
odds.tail(3)
x_normalized.head()
logit_mod2 = sm.Logit(df_new['converted'],df_new[['intercept', 'control','UK','US']]) $ result2 = logit_mod2.fit() $ result2.summary()
points.iloc[6] $
autos["odometer"].shape #autos[column name] will always give 1-d array which is a.k.a as Series in pandas, just like 1-d array in Numpy is known as vector $
query = 'CREATE TABLE new_york_new_york_points_int_ct10 AS (SELECT geoid, osm_id, latitude, longitude FROM beh_nyc_walkability, new_york_new_york_points WHERE ST_INTERSECTS(beh_nyc_walkability.the_geom, new_york_new_york_points.the_geom))' $
vect.stop_words_
elms_all_0604.ORIG_DATE.max()
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ null_vals = np.random.normal(0,np.std(p_diffs),10000)#Converting to be a distribution under tha null
prepared_train = prepared_train[prepared_train.comments < 1100]
p_old = df2['converted'].mean() $ p_old
ks_projects.columns
test_df['Domain'].nunique()
data = {'empID':  [100,      101,    102,      103,     104], $         'year':   [2017,     2017,   2017,      2018,    2018], $         'salary': [40000,    24000,  31000,     20000,   30000], $         'name':   ['Alice', 'Bob',  'Charles', 'David', 'Eric']} $ pd.DataFrame(data)
reviews_w_sentiment = pd.merge(reviews, pd.DataFrame(sentiments, columns=['sentiment_score']),left_index=True, right_index=True) $ reviews_w_sentiment['date']  = pd.to_datetime(reviews_w_sentiment['date']) $ len(reviews_w_sentiment)
print("total number of records:", len(df)) $ print("\rnumber of records with 0-day waiting interval:", len(df.loc[df['waiting_days']==0]), "\t{0:.3%}".format(len(df.loc[df['waiting_days']==0])/len(df)) ) $ print("\rnumber of records with at least 1 day waiting interval:", len(df[df['waiting_days'] > 0]), "\t{0:.3%}".format(len(df.loc[df['waiting_days']>0])/len(df)) )
wrd['name'].value_counts()[:10]
crimes.drop(['Ward', 'Community Area'], axis=1, inplace=True) $ crimes.dropna(inplace=True)
freq = session.query(Measurement.date, Measurement.tobs).group_by(Measurement.date).\ $                     having(Measurement.date.like('2016%')).filter(Measurement.station == 'USC00519281').all() $ freq $
ioDF.columns.tolist()
sex.value_counts().sort_index().plot(kind='bar') $ sex.value_counts()
fb.tail()
%sql \ $ SELECT twitter.user_id, twitter.tweet_text, twitter.heats FROM twitter \ $   WHERE twitter.tweet_text REGEXP 'Rafael|Nadal|Rafael Nadal|daviscup' \ $   ORDER BY heats DESC \ $ LIMIT 5;
full_model_file = os.path.join(working_dir, 'sk_model_full.zip')    $ full_text_classifier.save(full_model_file) $ full_model_file
spark.udf.register('simple_function',simple_function,pyspark.sql.types.IntegerType())
B2.paths['directory_paths']['indicator_settings'] $
df_test['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True) $ df_test.head()
posts.plot(figsize=(15,10)) $
df1 = df1.reindex_like(df2) #this will reindex same like df2 $ df1
df_cs.isDuplicated.value_counts() $ df_cs.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
df['Created Date'].resample('MS').count().plot(kind="bar")
pickle.dump(lda_tfidf, open('iteration1_files/epoch3/lda_tfidf.pkl', 'wb'))
df.sample(frac=0.7, replace=True).duplicated().value_counts()
x.iloc[[0,2,1],:]
df.head()
hp.get_sensors('water')[:3]
multipoint_nego_pivot = pivot_condition_time(multipoint_df, "ecn.multipoint", "negotiation", $                                              ('succeeded','failed','reflected','path_dependent','unstable')) $ multipoint_nego_pivot
X_train, y_train, X_test, y_test = utils.get_train_test_fm(feature_matrix,.75) $ y_train = np.log(y_train + 1) $ y_test = np.log(y_test + 1)
lg_model = LogisticRegression() $ lg_model.fit(X_train,y_train) $ lg_y_pred = lg_model.predict(X_train)
conn.caslibinfo()
df1.index.values
images.info()
model.load_weights(weights_path) $ model.save('wikigrader/data/nn_model.hdf5')
!wget -O drug200.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/drug200.csv
station_distance.tail(2) $ station_distance.shape
data = pd.read_csv('./bitcoin_ticker.csv')
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2012-02-28', '2012-03-05'))
datetime1 = datetime.datetime.strptime(date_str[:-3], '%Y-%m-%d %H:%M:%S')
pd.read_csv('../data/ebola/sl_data/2014-08-12-v77.csv').head()
malebydate = male.groupby(['Date','Sex']).count().reset_index() $ malebydate.head(3)
result0 = pd.merge(files1,files6, on='candidateid',how='left') $ result00 = pd.merge(result0,files7, on='jobcandidate',how='left') $ result = pd.merge(result00,files5, on='candidateid',how='left') $ files2=pd.merge(files1,files2, on='candidateid', how='left')
d = ddf.groupby('key').value.sum() $ %timeit d.compute()
null_mean = np.mean([p_new,p_old]) $ null_mean
auto_new.info()
(autos['last_seen'] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
%load "solutions/sol_2_32.py"
class DateRangeFreq(Enum): $     D = 'days' $     M = 'months' $     Y = 'years' $ rng = pd.date_range('1/1/2018',periods=100, freq=pd.DateRangeFreq.D)  # doesn't actually work...
df.head()
BTCprice = sns.regplot(x=final["BTC Price"], y=final['Crypto Positive'], fit_reg=False, color = 'g') $ BTCvol = sns.regplot(x=final["BTC Volume"], y=final['Crypto Positive'], fit_reg=False, color = 'g')
with open('../data/flutter/flutter_issues_open.json') as json_data: $     issues = json.load(json_data) $ print("Number of open issues: " + str(len(issues)))
type(U_B_df.cameras[0])
df2 = df.copy() $ df2['one'] = 0 $ df2
modern_combos = modern_pings.flatMap(get_problem_combos)
hp.sync_tmpos()
n_old = df2.query('group == "control"')['user_id'].nunique() $ n_old
error = df['tmax'].sub(df['tmin']) $ df.plot.bar(y='avg',yerr=error, color='#ff3399') $ plt.savefig('average_trip_temperature.png')
df_variables.to_csv("../01_data preprocessing/data new/variables.csv", encoding="utf-8", sep=",", index=False)
df_predictions = pd.DataFrame(data) $ for i, row in df_predictions.iterrows(): $     if row["Q0"]=="NO" : $         df_predictions.at[i,"Q1"] = "NULL" $ df_predictions
fit3.resid.hist();
preprocessor.subtypes
final_topbikes['Timestamp +2'] = final_topbikes['Timestamp'].apply(addtwo)
data_test = data_s[data_s['date'] < datetime(2017,1,1)] $ data_test['date'].max()
twelve_months_prcp.describe()
archive_df_clean['tweet_id'] = archive_df_clean.tweet_id.apply(str)
kmf.plot() $ plt.title('Kaplan Meier Fitter estimates')
import matplotlib.pyplot as plt
p + pd.tseries.offsets.MonthEnd(3)
temps_df.Missoula
submissionxg =  pd.DataFrame(  modelXg.predict_proba(test_X  )[:,1], index = test['comment_id'] , columns = ["is_fake"] ) $ submissionxg.to_csv('submissionxg_Xg.csv')
naimp.plot_corrplot_na()
INT.loc[:,'M'] = INT['Create_Date'].apply(lambda x: "%d" % (x.month))
na_df.dropna(axis=0) # remove rows that have NA values 
import urllib $ iris_url="http://aima.cs.berkeley.edu/data/iris.csv" $ urlRequest=urllib.request.Request(iris_url) $ iris_file=urllib.request.urlopen(urlRequest) $ iris_fromUrl=pd.read_csv(iris_file,sep=',',header=None,decimal='.',names=['sepal_length','sepal_width','petal_length','petal_width','target'])
to_be_predicted_Day2 = 50.6689895 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
years_of_prcp = session.query(extract('year', hi_measurement.DATE), func.count(hi_measurement.PRCP)).\ $     filter(hi_measurement.PRCP != "0.0").\ $     group_by(extract('year', hi_measurement.DATE)).\ $     order_by(func.count(hi_measurement.PRCP).desc()).\ $     all()
pmean = np.mean([pnew,pold]) $ pmean
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit()
pred9 = nba_pred_modelv1.predict(g9) $ prob9 = nba_pred_modelv1.predict_proba(g9) $ print(pred9) $ print(prob9)
new_page_converted = np.random.choice([0,1], size=treatment_group.shape[0], p=[1-p_new, p_new]) $ print(new_page_converted.mean())
results = logm.fit() $ results.summary()
labels=[i.replace(i[0: i.index('.')+1], '') for i in labels]
df_data.DESCRICAOLOCAL.value_counts()
host = '192.168.31.49' $ user = 'touchtv' $ password = 'op@touchtv' $ database = 'touchtv_dev' $ port = 3306
yar = train.project_subject_categories.apply(lambda x:yup(x)) $ p = pd.get_dummies(yar.apply(pd.Series).stack()).sum(level=0).reset_index(drop=True) $ del p['Warmth']
df_goog['Year'] = df_goog.index.year $ df_goog['Month'] = df_goog.index.month $ df_goog['Day'] = df_goog.index.day
y_pred_rf = rf.predict_proba(X_test)[:, 1] $ fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)
sleep.groupby('group').mean()['extra']
plt.figure(figsize=(8, 5)) $ train_df.polling.value_counts().plot.bar(); $ plt.title('Number of #hab by polling') $ plt.xticks(rotation='horizontal'); $
data['Date']=data['Date'].apply(lambda x : pd.to_datetime(pd.to_datetime(x).strftime('%Y%m%d')))
df_log = df_log.fillna(9999) $ df_users = df_users.fillna(9999) $ df_events = df_events.fillna(9999)
temp_df.groupby('reorder_interval_group')['Order_Qty'].var()
model.summary()
stacked.columns = ['name', 'info', 'value']
shows.dtypes
mean = np.mean(data['len']) $ print("The lenght's average in tweets: {}".format(mean))
ac['Compliance Review End'].describe()
dups = df.duplicated().sum() $ print("Our dataframe contains {} duplicated rows".format(dups))
(p_diffs > -0.001576).mean()
df.to_csv(DATAPATH+'submit_most_popular_category.csv', index=False) $
maxitems = 10 $ print "Edinburgh tweets retrieve testing" $ print '---------------------------------' $ for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_E).items(maxitems): $     print tweet.text
autos['unrepaired_damage'].unique()
type(df_master.tweet_id[1])
df2[df2['group']=='control']['converted'].mean()  #Probability of the users in control group who coonverted
damd['created'].head(3)
sp500[sp500.Price < 100]
archive_copy.name.sort_values()
learner.save_encoder('adam3_20_enc')
n_old=145273 $ old_page_converted=np.random.binomial(1, mean_old, n_old)
with rio.open(local_edit) as src: $     data = src.read(indexes=1) $     pyplot.imshow(data)
x = pd.read_sql_query(q, conn) $ x $
table.head() # refined into the table we would like to store
e = user_log_counts.groupby(["msno"], as_index=False)["logs_count"].sum() $ e[e.msno == '29V0Jm3Xli1dy9UFeEL/BH2EMOr62DgeGLeKAKfE07k=']
res = res.json() $ res
holdout_results.loc[holdout_results.wpdx_id == ('wpdx-00102032') ]
tweet_counts_by_hour.plot(subplots=True)
P_Control_Converted = (df2.query('converted == 1')['group'] == 'control').mean() $ P_Control = (df2.group=='control').mean() $ P_Converted_Control = (P_Control_Converted * P_Converted) / P_Control $ P_Converted_Control
df_final.head(1)
ct_df.head()
df.loc['r_five']=[5,10,15,20,False] $ df
hr2007.add(hr2006, fill_value=0)
df_ud013 = pd.read_sql(sql_ud013,conn_hardy) $ df_ud013.groupby(['course_enrolled','course_enrollment','converted'])['applicant_id'].count()
from sklearn.metrics import precision_score,recall_score,accuracy_score,confusion_matrix $ print( precision_score(y_test,y_preds)) $ print( recall_score(y_test,y_preds)) $ print( accuracy_score(y_test,y_preds)) $ confusion_matrix(y_test,y_preds)
df2[df2.duplicated('user_id')]['user_id']
df.query('landing_page == "new_page"').shape[0] / df.shape[0]
np.dot(a, b)
oblist=[52,62,63,132,143,83,182,187,24,26,27,28,32,194,197,211,213,216,217,218,222,117,123,136,137,159,166,179,89,99,135,173,174,33,44,53,54,55,68,82,154,202,204,109,196,91,35,56,57,71,73,201,205,206,208,75,86,92,101,102,219,29,151,198,66,190] $ name_obs=["Field%03i"%x for x in oblist] $ name_obs=pd.DataFrame({'name':name_obs}) $ observed=name_obs $ observed.head()
reddit_data.to_csv('bitcoin_reddit.csv', index=False, sep=";")
df.resample('D').mean().plot()
driver.get("http://www.reddit.com") $ time.sleep(1)
allqueryDF.info()
import matplotlib.pyplot as plt $ %matplotlib nbagg
combined_df3.keys()
!ls -l $ !head DataDescription.json
datatmp[].plot() $ plt.show()
df.loc[ 0:3 , ['name','age'] ]
fda_act_ingr = set(fda_drugs.ActiveIngredient.values)
newdf.index = newdf.yearmonth
list0 = [1,2,4,3,'k'] $ ''.join(str(x) for x in list0)
preds = nb.predict(X_test)
potential_accounts_buildings_info = (pd.merge(sites_on_net, sites_no_net,\ $          on=['Account ID'],\ $          how='outer').sort_values(by='# Buildings on Net', ascending=False)).fillna(0) $ potential_accounts_buildings_info['# Buildings not on Net'] = potential_accounts_buildings_info['# Buildings not on Net'].astype(int)
R_sq=1.0-(SSE/SST) $ R_sq
plt.figure(figsize = (6,6)) $ kmeans.fit_predict(X) $ plt.scatter(X[:,0],X[:,1], c = kmeans.labels_, cmap='rainbow')  $ plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,0], color='black')  
tables = pd.read_html(fact_url) $ tables
df.set_index('Day') # this returns the new data frame with index set as 'Day' but does not modify the existing df dataframe $ df.head()
df_enhanced[['dog_name']].groupby(['dog_name'])['dog_name'].size()
pd.value_counts(ac['Issues']).head(10)
blob.noun_phrases
test_df["labels"] = np.array(probs)[:,1] $ test_df.head(500)
wikiMeritRequest = requests.get(wikipedia_meritocracy) $ print(wikiMeritRequest.text[:1000])
df3[df3['E'].isin(['t','f'])]
goodTargetUserItemInt=goodTargetUserItemInt.join(intForMaxTS,on=['user_id','item_id'],how='left') $ print goodTargetUserItemInt.shape $ goodTargetUserItemInt.head()
df_tweet_json_clean = df_tweet_json.copy()
df = pd.get_dummies(df)
precipitation_df['date'] = pd.to_datetime(precipitation_df["date"]).dt.date $ precipitation_df.set_index(["date"], inplace = True)
pd.concat(pieces)
for i in range(0,200): $     random_id = (dat.COMMITTEE_ID.unique()[random.randint(0,len(dat.COMMITTEE_ID.unique()))]) $     committee_test = comm_merge[comm_merge.COMMITTEE_ID.isin([random_id])].groupby(by=['COMMITTEE_ID','CAND_ID','TRANSACTION_DATE']).size() $     dat_test = dat[dat.COMMITTEE_ID.isin([random_id])].groupby(by=['COMMITTEE_ID','CAND_ID','TRANSACTION_DATE']).size() $     assert len(dat_test[dat_test>1]) <= len(committee_test[committee_test>1]), 'there are more observations in the merged dataframe than the original for id ' + random_id $
most_common_registered_addresses.sort_values(ascending=False).head(10)
old_con = ab_data[(ab_data['group']=='control') & (ab_data['landing_page']=='old_page') ].count()
from scipy.stats import norm $ norm.cdf(1.3109241984234394), norm.ppf(1-(0.05))
aapl_opt.loc[(aapl_opt.Expiry=='2018-03-16') &(aapl_opt.Type=='call') & (aapl_opt.Strike==180)].JSON[638]
t=mini_df.groupby(['wait', 'month']).mean()
dep = ha.depot('demo depot', path='data', currency='EUR') $ dep.add_account(acc) $ dep.account_infos
r_top10_mat_na = returns_calc(price_mat[coins_top10], fill_na_0=False) $ fin_coins_r = pd.concat([fin_r_monthly.loc[start_date:end_date], #financial assets $            r_top10_mat_na.loc[start_date:end_date], # top 10 crypto assets $                          fundret[start_date:end_date] # fund return $                         ], axis=1, join='inner')
df = df.set_index('datetime') $ df = df.resample('H').sum()
SCN_BDAY_qthis.scn_age.describe()
model = Sequential() $ model.add(Dense(LATENT_DIM, activation="relu", input_shape=(T,))) $ model.add(Dense(HORIZON))
df.breed.value_counts()[0:19].plot(kind='bar');
seaborn.countplot(company_vacancies.hour)
train_data.head()
hn['popular'] = (hn.num_points > 5).astype(int)
questions = pd.concat([questions.drop('vip_reason', axis=1), vip_reason], axis=1)
charge_counts.head()
naimp.get_isna_ttest('age', type_test='ks')
df[df['status_type']=='photo'].groupby('dayofweek').status_id.count()
stock.head()
most_active = station_counts[0][0] $ active_averages = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $     filter(Measurement.station == most_active).all() $ print(active_averages)
bnbAx.first_device_type.value_counts()
max(twitter_archive['tweet_id'].value_counts())
unique_instance.content $
nx.info(tweetnet)
afx_2017_url = base_url+"?start_date=2016-12-31&end_date=2018-01-01"+api_url $ r = requests.get(afx_2017_url)
x = "dort" $ if x in stopword_list: $     print("Yes!" ,x, "is in the list." )    #Prints "Yes!", if x in list. $ else: $     print("No!",x, "is not in the list.")   #Prints "No!" if not.
new_df = pd.DataFrame(X.loc[[515, 4232]], columns=X_train.columns)
df_never_moved['Lat'].value_counts().head(10)
print(r.json()['dataset_data']['column_names'])#['start_date'])#
autos.head()
num_closures = re.search(r'[0-9]{3}', data) $ num_closures = int(num_closures.group()) $ num_closures == faa_length
autos['odometer_km'].value_counts().sort_index()
prop['bathroomcnt'].value_counts().sort_values()
repeated_user = df_counts[df_counts.counts > 1].index.values[0] $ df2.query('user_id == @repeated_user')
aaplA = aapl[['Adj Close']] $ pd.concat([msftAV, aaplA])
print('There are {:.0f} visa applications for jobs located in Mountain View in this dataset.'.format(df_h1b_mv.shape[0]))
airbnb_df['room_type'] = airbnb_df['room_type'].astype('category')
train_df.describe()
cityID = '01c060cf466c6ce3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Long_Beach.append(tweet) 
weather_all = pd.read_csv('data/weather_airports_24hr_snapshot.csv') $ weather_mean = weather_all.groupby('Station Name').mean() $ weather_mean.head()
Q96P20=Graph().parse(format='ttl', $                      data=turtle) $ for row in qres: $     print("%s is a protein with a gene %s that has %s names" % row)
_ = ok.grade('q02') $ _ = ok.backup()
df_merge = pd.merge(df_merge, predictions_clean, on = 'tweet_id', how = 'inner') $ df_merge.head()
autos["odometer_km"].value_counts().sort_index(ascending=True)
df_clean4.head()
data = pd.read_csv('./fake_company.csv') $ data
dat.isnull().sum() $
auth = tweepy.OAuthHandler(creds['consumer_key'], creds['consumer_secret']) $ auth.set_access_token(creds['access_token'], creds['access_token_secret']) $ api = tweepy.API(auth) $ tweets = api.user_timeline('everycolorbot', count=24*30) # seems to be limited to 200 as of 2018-05-30
holding_file_name = 'Holding Folder - Number 2' $ holding_file_name = input('What is the name of the Folder you want to create:  ') $ if not (os.path.exists(holding_file_name)): $     print('creating a new folder in',os.getcwd()) $     os.makedirs(str(holding_file_name))
start = datetime(year=2018,month=1,day=29) $ td = dt.timedelta(days=45) $ end = start + td $ p=((f.loc[start][0]-f.loc[end][1])/f.loc[start][0])*100 $ print('The closing price of '+ ticker_var +' has changed by '+ str(p) + ' % between ' + str(start) + ' and '+ str(end) +'.')
start_date = pd.Timestamp("20150101") $ end_date = pd.Timestamp("20151231") $ mask2015= (df['Date'] >= start_date) & (df['Date'] <= end_date) $ Data2015 = df[mask2015].sort_values(by = 'Date') $
sel = [Measurements.station, func.count(Measurements.tobs)] $ active_stations_data = session.query(*sel).group_by(Measurements.station).order_by(desc(func.count(Measurements.tobs))).all() $ active_stations_data
new_page_converted = np.random.binomial(1, treatment_cnv, treatment_num) $ new_page_converted.mean()
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
unit_size_map = {} $ for next_type in sim_types: $     mdf = pd.DataFrame(market_data_map[next_type]['trades'], index=[x['time'] for x in market_data_map[next_type]['trades']]) $     unit_size_map[next_type] = int(max(1, mdf.groupby(pd.TimeGrouper('D')).volume.sum().mean() * 0.1 * 0.1))
train.head()
cols = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest','vehicle_type', 'registration_year', 'gear_box', 'power_ps', 'model','odometer', 'registration_month', 'fuel_type', 'brand','unrepaired_damage', 'ad_created', 'nr_of_pictures', 'postal_code','last_seen'] $ autos.columns = cols $ autos.info()
df.plot()
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Series(temp, dtype='category') $ print (type(temp_cat))       # Series object $ print (type(temp_cat.cat))   # Categorical Accessor
exploration_titanic.print_infos('consistency', print_empty=False) $
temp_pred.shape
ST_hist = sim_closes_hist.tail(1) $ ind = np.digitize(ST_hist.values[0],np.array([0, K, ST_hist.values.max()+1])) $ freq = (np.array([sum(ind==1),sum(ind==2)])/nscen)*100
signals[signals.index.isin(['2014-04-25','2014-04-28','2014-04-29'])]
if not os.path.isdir('output/pv_production'): $     os.makedirs('output/pv_production')
flight2.unpersist()
conditions.value_counts()
    example1_df.registerTempTable("world_bank")
for attr in ncTest.ncattrs(): $     print('%s: %s' % (attr, ncTest.getncattr(attr)))
DataSet = DataSet[DataSet.userTimezone.notnull()] $ len(DataSet) $
sessions_summary = sessions_summary.reset_index() $ sessions_summary["len"] = sessions_summary.apply(lambda l: len(l["action"]), 1) $ sessions_summary["len"].value_counts()
conversion_rates = total_Visits_Convs_month_byMC[['marketing_channel', 'conversion_rate']] $ print'conversion_rates', conversion_rates.shape $ conversion_rates.head()
day_of_week = pd.DatetimeIndex(pivoted.columns).dayofweek
df_archive_clean = df_archive.copy()
with open ('tweet_json.txt', 'w') as file: $     file.write(json.dumps(my_list_of_dicts, indent = 4))
Results_kNN100.to_csv('soln_kNN100.csv', index=False)
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['show'])
url = "https://api.census.gov/data/2016/acs/acs1/variables.json" $ resp = requests.request('GET', url) $ aff1y = json.loads(resp.text)
tweets_timeline = tweets.groupby(by=['hour', 'day_of_week'])['full_text'].count()
plt.figure(figsize=(16,8)) $ dateCreated_count = df2['dateCreated'].value_counts().plot(kind='bar') $ dC = dateCreated_count.set_xticklabels(dateCreated_count.get_xticklabels(), rotation=90) $ len(df2['dateCreated'].unique())
df_link_yt['video_category'].replace(video_categories).value_counts()
S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
from bs4 import BeautifulSoup $ from scrapy.spidermiddlewares.httperror import HttpError $ from twisted.internet.error import DNSLookupError $ from twisted.internet.error import TimeoutError, TCPTimedOutError
df_closed = df_bug[date_cols] $ df_closed.head()
results = lm.fit() $ results.summary()
print('Number of unique symbols: {}'.format(len(news.symbol.unique()))) $ print('Average number of news per symbol: {}'.format(np.mean(news.symbol.value_counts())))
dsi_me_1_df.to_csv('./dsi_me_1.csv', index_label='id')
cityID = 'a84b808ce3f11719' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Omaha.append(tweet) 
with_condition_heatmap_query3 = folium.Map([41.90293279, -87.70769386], $                zoom_start=11) $ with_condition_heatmap_query3.add_child(plugins.HeatMap(final_location_ll[:40000], radius=15)) $ with_condition_heatmap_query3 $
scr_retention_df = scr_retention_df.append(scr_retention_sum)
dic_a = {"a":1,"b":2,"c":3,"d":4} $ ser3 = pd.Series(dic_a) $ ser3
lm = sm.Logit(df2['converted'],df2[['intercept','ab_page', 'US', 'CA']]) $ result = lm.fit() $ result.summary()
np.exp(0.0043)
sagemaker_session.delete_endpoint(endpoint_name)
lr = LogisticRegression(class_weight='balanced') $ sc = preprocessing.StandardScaler() $ Xfinal = sc.fit_transform(Xfinal) $ clf = lr.fit(Xfinal, yfinal) 
import statsmodels.api as sm $ convert_old = num_of_converted_in_control $ convert_new = num_of_converted_in_treatment $ n_old = len(control_group_sample) $ n_new = len(treatment_group_sample)
df_pol[df_pol['pol_id']=='Liberal']['domain'].value_counts().head(12).plot(kind='bar');
os.getcwd() $ os.path.abspath(holding_file_name)
stadium_arr.arrests.plot(kind='hist', title='Distribution of Arrests per Game') $ plt.savefig('distributionOfArrestsPerGame.png', bbox_inches='tight')
labeled_features.isnull().values.any() 
df_train_pca = pca.fit_transform(X_train) $ print(df_train_pca.shape) $ df_test_pca = pca.transform(X_test) $ print(df_test_pca.shape)
snow.select("select count(distinct patient_id) from st_pharma_cohort_a")
joined = join_df(joined, weather, ["State","Date"]) $ joined_test = join_df(joined_test, weather, ["State","Date"]) $ len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])
def predict_output(feature_matrix, weights): $     predictions = np.dot(feature_matrix, weights) $     return(predictions)
df[['favorites', 'retweets']].plot(style = '.', alpha = 0.4) $ plt.title('Favorites and Retweets with Time') $ plt.xlabel('Date') $ plt.ylabel('Count');
parks_don['fq'] = parks_don.close_date.apply(lambda x: 'FY'+ str(x.strftime('%y')) + 'Q' + str((x.month-1)//3)) $ parks_don['year'] = parks_don.close_date.apply(lambda x: x.year) $ parks_don.head(2)
print(np.exp(results.params))
handler.to_dataframe().head()
data.iloc[1:3]
manager.image_df['p_hash'].isin(tree_features_df['p_hash']).head(20)
df_unique_providers = df_unique_providers.drop_duplicates(['id_num','name']) $ df_unique_providers = df_unique_providers.reset_index(drop=True) $ df_unique_providers = df_unique_providers.drop('year',axis=1) $
finals[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 1)].PLAYER_NAME.value_counts()
df2[df2.duplicated(['user_id'], keep = False)]
s2 = pd.Series([68, 83, 112, 68], index=["alice", "bob", "charles", "darwin"]) $ s2
ctc = ctc.round(1)
df_new[['US','UK','CA']] = pd.get_dummies(df_new['country']) $ df_new.head()
q1_data1 = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH) $ q2_data2 = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH) $ print('Shape of question1 data tensor:', q1_data1.shape) $ print('Shape of question2 data tensor:', q2_data2.shape) $
B_INCR2 = 0.293 $ B_DECR2 = -0.293
df3=pd.read_csv('countries.csv') $ df3.head(3)
size_pred = rf.predict(climate_vars)
df_ab_converted_portion.head()
mean_sea_level.index.name = "date" $ mean_sea_level
import urllib2 $ import requests $ response = requests.get("http://api.coindesk.com/v1/bpi/currentprice.json") $ bitcoin_response = response.json() $ print bitcoin_response['bpi']['EUR']['rate_float']
cleaned_df = split_df.na.fill({'content_size': 0}) $ exprs = [count_null(col_name) for col_name in cleaned_df.columns] $ cleaned_df.agg(*exprs).show()
raw_train_df.shape
df4=df_new.drop(['CA','aa_page'], axis=1) $ df4['intercept']=1 $ lmm=sm.OLS(df4['ab_page'],df4[['intercept','UK','US']]) $ result1=lmm.fit() $ result1.summary()
season07["Category"] = "2007 Season" # Again, this assigns aseason identifier to the transactions.
data_set.head(5)
convert_count = df2.groupby(['group', 'converted']).size().reset_index(name='count') $ convert_count
df_transactions['is_discount'] = df_transactions.discount.apply(lambda x: 1 if x > 0 else 0)
dfDsmall = pd.read_sql_query("select * from distances limit 5;", conn_aws)
train_df = effectiveness(train_df) $ print(train_df.head(5))
m2T=m2.T $ m3= m2T.dot(m) $ print("m3: ", m3)
target_google = target_google.sort_values('date').copy() $ target_google.head(2)
bb.plot(y='volume')
for df in reader.iterate_over_chunks(chunksize=4000): $     break $ df
print(today.hour) $ print(today.month) $ print(today.day) $ print(today.year)
num_portfolios = 200000 # Cantidad de portafolios a simular $ r=0.015/252             # Tasa libre de riesgo $ num_stocks = len(tickers)
print targetUserItemInt.shape $ targetUserItemInt.head()
np.random.seed(31) $ random.seed(31) $ tf.set_random_seed(31)
len(data.thumbnail.unique())
simple_glm = smf.glm('hospital_expire_flag ~ C(inday_icu_wkd)', $                      data=data, family=sm.families.Binomial()).fit() $ simple_glm.summary2() $
len(df_all_wells_wKNN.UWI.unique())
params = pd.Series(model.coef_, index=X.columns) $ params
engine.execute('SELECT * FROM measurements LIMIT 10').fetchall() $
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
treatment = df2.query("group == 'treatment'")
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
p_old = df2[df2['converted'] == 1].user_id.nunique()/df2.user_id.count() $ p_old
station_distance.head() $ station_distance.shape
pd.to_datetime(df['Date']).dt.day
f.exclude_list_filter
x.drop(["sum"], axis=1)
cov = cov.x $ cov[cov==0] = np.NaN $ plt.imshow(cov)
stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)}) $ len(itos)
tree_features_df[~(tree_features_df['p_hash'].isin(manager.image_df['p_hash']) | tree_features_df['filename'].isin(manager.image_df['filename']))]
for v in X_train.toarray()[0][:20]: $     print v
def expand_counts(source_counts): $     return np.repeat(np.arange(len(source_counts)), source_counts) $
df_test.cache()
importances = zip(X.columns, gbm.feature_importances_) $ importances.sort(key=lambda x: x[1], reverse=True) $ importances
uso17_size = db.command("collstats", "usopen17")["count"] $ uso17_qual_size = db.command("collstats", "usopen17_qual")["count"]
results = api.search(q="'HIV'")
site = hp.find_site(1) $ site
print(df.shape) $ print(df2.shape) $ print(both_dfs.shape)
session.query(measurements.date)[-1]
vip_df.info()
actual=test_y[0:203] $ actuals_dataframe=pd.DataFrame(actual,columns=['actual']) $ actuals_dataframe.shape
git_log = git_log.sort_index(ascending=False)
engine.execute('SELECT * FROM measurement WHERE DATE > "2016-08-22"').fetchall()
contour_sakhalin = np.array(sakhalin_shp.shapes()[0].points)  #convert contour points to numpy array
import sys $ sys.stdout.write('1') $ sys.stdout.write(' ') $ sys.stdout.write('2')
about=r.html.find('#about',first=True)
df_twitter_copy[df_twitter_copy.expanded_urls.isnull()]
df.columns
forecast = m.predict(future) $ forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()
r.encoding
to_be_predicted_Day4 = 36.48514248 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
pricePublication = list(zip(*it_df.finalPrice.apply(getfinalPriceANDpublicationDate))) $ it_df["price"] = pricePublication[0] $ it_df["publicationDate"] = pricePublication[1]
bwd = df[['Store']+columns].sort_index().groupby("Store").rolling(7, min_periods=1).sum()
topics_data.sample(5) # take a look at the data
p_new = df2['converted'].mean() $ print(p_new)
df_clean.describe()
times.head(30)
departureZip = departureZip[['Unnamed: 0', 'ZIPCODE']] $ departureZip.rename(columns={'ZIPCODE':'zip_depart'}, inplace=True) $ departureZip.head()
errors['errorID'].value_counts().plot(kind='bar') $ plt.ylabel('Number of Errors') $ plt.show()
linkNYC['days'] = pd.to_datetime(linkNYC.date_link_)
resp = requests.get('https://lobste.rs/hottest.json') $ stories = pd.read_json(resp.content) $ stories = stories.set_index('short_id') $ stories.to_json('hottest.json') $
import json $ from pymongo import MongoClient $ client = MongoClient(port=12345) $ db = client.stocks $ tweets = db.stock_whisperer
archive_copy[['tweet_id', 'id_str']].head()
df2.head()
p=read_cvs_by_pands(path_database,'pragraph_index_20.csv',None,0) $ index_p_p=p['index'] $
cryptos.iloc[0]
len(train_data[train_data.fuelType == 'hybrid'])
X_train.columns.values
df_new['CA_new_page'] = df_new.CA * df_new.new_page $ df_new['UK_new_page'] = df_new.UK * df_new.new_page $ df_new.head()
print("The minimum value of artistID:") $ userArtistDF.agg(min("artistID")).show()
unique_Dx = [] $ for col in Dx_cols: $     for Dx in full_orig[col].unique(): $         unique_Dx.append(Dx)
daily_df_subset = daily_df[['Time','Company','Day Prediction','Price_Change']] $ daily_df_subset.head()
!convert materials-xy.ppm materials-xy.png $ Image(filename='materials-xy.png')
news_p = soup.find('div', class_='image_and_description_container').text.strip() $ news_p
df_twitter_copy = df_twitter_copy[df_twitter_copy['in_reply_to_status_id'].isnull()]
m3.clip=25. $ lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])
stations.installation_date = pd.to_datetime(stations.installation_date, format = "%m/%d/%Y").dt.date
plt.boxplot(data) $ plt.ylim(0, 20000) $ plt.xlabel('Number of images') $ plt.ylabel('Number of retweets') $ plt.title('Number of Images vs Retweets');
baseball1_df = baseball_df[['playerID','birthYear','birthMonth','birthDay','debut','finalGame']].dropna() $ baseball1_df.reset_index(inplace = True) $ baseball1_df.tail() $
wrd_full.query('favorite > 9447')['hour'].value_counts().rename_axis('hour').reset_index(name='counts').sort_values(by=['hour']).plot(x="hour",y="counts");
model = model.fit(X,y)
sw_tweets.shape
(~autos['registration_year'].between(1900,2016)).sum()/autos.shape[0]
df1 = df1[df1['Title'].str.contains(blacklist) == False] $ df1.shape
r.summary2()
td_amb = st_streams['/streams/amb'].index.values[-1] - st_streams['/streams/amb'].index.values[0] $ td_door = st_streams['/streams/door'].index.values[-1] - st_streams['/streams/door'].index.values[0] $ td_mcu = st_streams['/streams/mcu'].index.values[-1] - st_streams['/streams/mcu'].index.values[0] $ td_relay = st_streams['/streams/relay'].index.values[-1] - st_streams['/streams/relay'].index.values[0] $ td_temp = st_streams['/streams/temp'].index.values[-1] - st_streams['/streams/temp'].index.values[0]
pickle.dump(nmf_tfidf_data, open('iteration1_files/epoch3/nmf_tfidf_data.pkl', 'wb'))
print("dfMonth = ",dfMonth['Contract Value (Monthly)'].sum(), "dfMonth Project Count = ", dfMonth['Project Name'].nunique()) $ print("dfDay = ",dfDay['Contract Value (Daily)'].sum(), "dfDay Project Count = ", dfDay['Project Name'].nunique())
autos["price"].value_counts().sort_index(ascending = True).head(20)
dayofweek = df.groupby('Day_of_week') $ print(dayofweek)
print("cost is: {}" .format(cost), "revenue is: {}" .format(revenue), sep=", ")
companyNeg.shape
df = gbq.read_gbq(query=query, dialect='standard', project_id=os.environ['PROJECT'], verbose=False) $ df
merged = merged.dropna(axis=0, how='any');
for col in x: $     x[col] = x[col].astype('category') $ y = cat_outcomes['outcome_subtype'].astype('category')
from nltk.corpus import stopwords $ stop = stopwords.words('english')  # stop words have little meaning eg. a, is, and, has, etc. $ [w for w in tokenizer_porter('a runner likes running and runs a lot') $  if w not in stop]
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.join(holiday_events_df, 'date', 'left_outer') $
os.getcwd()
df_new['afternoon']= pd.get_dummies(df_new['day_part'])['afternoon']
df.head(3)
figure_density_df = utility_patents_subset_df.dropna() $ sns.distplot(figure_density_df.figure_density, color="red") $ plt.show()
financial_crisis.drop('Spain defaults 7x', inplace = True) $ print(financial_crisis)
year16 = driver.find_elements_by_class_name('yr-button')[15] $ year16.click()
import pandas as pd $ airport_lookupDF = pd.read_csv('https://s3.amazonaws.com/jsw.dsprojects/AirlinePredictions/Airport_Lookup.csv', $                                header = 0) # Airport codes $ trainDF = pd.read_csv('FinalFlightsNumeric.csv', header = 0) # Data from R
pop_treat = len(df2[df2.group == "treatment"]) $ pop_treat
def yearly_data_csv_writer(start_year, end_year, all_data, path, name): $     for year in range(start_year, end_year+1): $         one_year_csv_writer(year, all_data, path, name) $ yearly_data_csv_writer(1998, 1998, surveys_df, './data/', 'function_surveys')
all_slices.sort(key = lambda slice: slice['expires']) $ print(f"Found {len(all_slices)} slices")    
r = requests.get('https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html')  
nnew = len(df2.query('group =="treatment"')) $ nnew
%matplotlib inline $ import matplotlib.pyplot as plt $ plt.xlabel("Karma") $ plt.ylabel("Comments") $ plt.scatter(karma_per_user, comments_per_user, s=karma_per_comment.values, alpha=0.3)
new_df = cs.apply(lambda x : np.min(x[cols_select]), axis = 1) $ new_df.head()
x = pandas.read_csv("data1.csv")
unprocessed_list = list() $ for i in l: $     unprocessed_list.append(i.text)
1/np.exp(-0.0099)
f = open('origin-of-species.txt', 'r') $ content = f.read() $ f.close()
reddit_comments_data.groupBy('author').agg({'sentiment':'mean'}).orderBy('avg(sentiment)').show()
result5 = sm.ols(formula="HPr_RF ~ Mkt_RF", data=tbl3).fit() $ result5.summary()
simul=pd.concat([closes.T, simdata.T]).T $ simul.plot(figsize=(8,6),legend=False);
trump['est_time'] = pd.Index(trump['time']).tz_localize("UTC").tz_convert("US/Eastern") $ trump.head()
pd.DataFrame(dog['SexuponOutcome'].value_counts()).plot(kind='bar')
gmap.draw('incidents_311.html')
train_X = train_df[fealist].as_matrix() $ test_X = test_df[fealist].as_matrix() $ train_y = np.array(train_df['interest_level']) $ preds_simple, model = runXGB_sklearn(train_X, train_y, test_X,num_rounds=3000) $
client.repository.list_experiments()
mar.info()
if parameters['doc_type'] == 'resume': $     data = pd.read_csv(directory+'02_resumes_work.csv') $     data.rename(columns = {'descript':'summary_text'}, inplace=True) $ if parameters['doc_type'] == 'postings': $     data = pd.read_csv(directory+'02_job_posts.csv')
_ = ok.submit()
df_test_index.iloc[6260, :]
training_size = int(len(scaled_df) * 0.8) $ training_size
pd.set_option('display.max_colwidth', -1) $ twitter_archive_full[twitter_archive_full.rating_numerator > 20][['tweet_id','text','rating_numerator']]
df_copy=df.copy() $ image_copy=image.copy() $ tweet_data_copy=tweet_data.copy()
test = import_all(data_repo + 'intervention_test.csv')
from sklearn.model_selection import StratifiedShuffleSplit $ split = StratifiedShuffleSplit(n_splits=1, test_size=20000, random_state=1234) $ for train_index, test_index in split.split(clean_appt_df, clean_appt_df['No-show']): $     train_set = clean_appt_df.iloc[train_index] $     test_set = clean_appt_df.iloc[test_index] $
targetItemData=itemData.join(targetItems.set_index(['item_id']),on='id',how='inner').reset_index(drop=True) $ print targetItemData.shape $ targetItemData.head()
rsp = requests.get(api_url_base + ','.join(vid_list))
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
dfm = dfn.drop(['usd_pledged','goal','state','slug','currency','deadline','state_changed_at','created_at','backers_count','spotlight','period'], axis=1).copy()
df_hate.describe()
users_visits.sort_values('visits', ascending=False).head()
from patsy import dmatrices $ from statsmodels.stats.outliers_influence import variance_inflation_factor
d = pd.merge(trend, geo, on='placeId', how='left').set_index('date') $ d['has_event'] = d['1'].apply(lambda x: type(x) == str)
std_for_each_weekday = std_for_each_weekday.sort_values(by=['day_order']) $ std_for_each_weekday
read_df=pd.read_csv("test_df_to_csv.csv") $ print(read_df)
df0 = pd.read_csv('2000.csv')
rhum_long_df = pd.melt(rhum_wide_df, id_vars = ['grid_id', 'glon', 'glat'], $                       var_name = "date", value_name = "rhum_perc") $ rhum_long_df.head()
dog_stage_list_all = list(twitter_archive_clean['dog_stage'].value_counts().index) $ dog_stage_list = list(filter(lambda x:x!='None',dog_stage_list_all)) $ for dog_stage in dog_stage_list: $     assert twitter_archive_clean[dog_stage].value_counts()[dog_stage] == twitter_archive_clean['dog_stage'].value_counts()[dog_stage]
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], alternative='smaller', $                                               nobs=[n_new, n_old]) $ print("z-score:", z_score,"\np-value:", p_value)
first_result.find('strong').text 
bacteria2.fillna(0)
get_req = requests.get(get_url(job=json_out['job']), headers=headers)
ax = dfg.plot(rot=45) $ plt.tight_layout() $
import datetime as dt $ NOW = dt.datetime(2016,3,27)
pd.merge(test, session, how=left)
features.head()
con_logit = sm.Logit(df2['ab_page'],df2[['intercept','converted','UK','US']]) $ result = con_logit.fit() $ result.summary()
hashtags = pd.Series(hashtags_series.sum()) $ mentions = pd.Series(mentions_series.sum())
archive_clean[archive_clean['tweet_id'] == 786709082849828864].rating_numerator
config = tf.ConfigProto() $ try: $     sess = tf.Session(config=config) $ except: $     sess = tf.Session(config=config)
pd.read_csv(r'./CRNS0101-05-2018-NC_Durham_11_W.txt').head()
full.groupby(['Dx3'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False) $
from nltk.probability import FreqDist $ fdist = FreqDist(filtered_words) $ fdist
start= '2017-1-1' $ start_date = dt.datetime.strptime(start, '%Y-%m-%d') $ session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).all() $
repos = pd.read_pickle('data/pickled/new_subset_repos.pkl') $ users = pd.read_pickle('data/pickled/new_subset_users.pkl') $
festivals['Index'] = range(1, len(festivals) + 1) $ list(festivals.columns.values) $ festivals.head(3) $
df_agg_unsub_rand.head()
df.index $
columns = ['contributors', 'coordinates', 'geo', 'quoted_status', 'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink'] $ archive_copy.drop(columns, axis=1, inplace=True)
plot_reorder_stat(STD_reorder_stats['mean'], $                   [i*3 for i in range(100)], $                   False, 'average days between reorders', $                   'count','cadence of reorder: STD dealers',sample_mean=41.255567)
pre_strategy_google.head(1)
gbm_v1.hit_ratio_table(True, False, True)
twitter_archive.rating_denominator.value_counts()
data_archie.user_id = (data_archie['user_id']).astype(int)
df.info()
trump.index=pd.DatetimeIndex(trump['created_at']) $ trump.drop('created_at', axis = 1, inplace =True) $ trump.shape
df = sqlCtx.read.format("es") $
data_dir = os.path.join(os.getcwd(), 'data') $ print(data_dir)
df2[df2.duplicated('user_id')] $
station.shape
session.query(Measurement.id, func.max(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
train.head()
print(df['approx_payout_date'].min(), $ df['approx_payout_date'].max())
run txt2pdf.py -o"2018-06-19 2015 OROVILLE HOSPITAL Sorted by payments.pdf"  "2018-06-19 2015 OROVILLE HOSPITAL Sorted by payments.txt"
data = pdr.get_data_yahoo(classes, start="2017-11-01", end="2017-11-28")
int_ab_page = 1/np.exp(-0.0149) $ int_ab_page
top_5_beats = cfs_df['Beat'].groupby(cfs_df['Zip']).value_counts() $ top_5_beats.groupby(level=0).nlargest(5)
analyze_set.plot(kind='scatter',x='favorites',y='retweets', alpha = 0.4) $ plt.xlabel('Favorites') $ plt.ylabel('Retweets') $ plt.title('Retweets and Favorites') $ plt.savefig('a.png')
restaurants = pd.read_csv("/home/ubuntu/data/restaurant.csv", dtype=unicode, encoding="utf-8")
delta.days, delta.seconds 
sentiments_groupby = sentiments_pd.groupby(['Products']) $ sentiments_mean = sentiments_groupby['Tweet Polarity'].mean() $ sentiments_final = pd.DataFrame(sentiments_mean).reset_index() $ sentiments_final.head()
grp = data.groupby(by=[data.datetime_col.map(lambda x : (x.hour, x.minute))]) $ grp.count()
intersections_final_for_update_no_dupe.head()
support = merged[merged.committee_position == 'SUPPORT'] $ oppose = merged[merged.committee_position == 'OPPOSE'] $ oppose.amount.sum()
print (archive_copy['new_rating_denominator'].value_counts()) $ archive_copy[archive_copy.new_rating_denominator == 'Error']['tweet_id'] $
df['Has_Media_Link'] = np.where(df['Media URL'].isnull(), True, False) $ df.drop('Media URL', axis=1, inplace=True)
incorrect_naics_indices = df_h1b_ft_US_Y.lca_case_naics_code[map(lambda x: x!=8, $                                                                  map(lambda x: len(str(x)), $                                                                      df_h1b_ft_US_Y.lca_case_naics_code))].index
df = pd.DataFrame(TWEETS) $ df['date'] = df.created_at.apply(lambda x: x.date()) $ print(len(df))
autos = pd.read_csv("autos_2.csv", encoding = "Latin-1") $ autos.head()
df2['intercept'] = 1 $ df2['ab_page'] = [1 if x == 'treatment' else 0 for x in df2['group']] $ df2.head()
df2.converted.mean()
links = hems_soup.find_all("a", {"class": "itemLink"}) $ links
outliers = abs(daily_deltas - daily_deltas.mean()) > 2.5*daily_deltas.std()
all_zipcodes = pd.merge(df, zipcodes, on='zipcode', how='left') $ all_zipcodes[pd.isnull(all_zipcodes.city_x)].head()
real_test_df = model_df.iloc[725:-1].copy() $ real_test_df['model_predict'] = np.NaN
tableViolationCodes = restaurantsExcelFile.parse(sheetname='Violation');
train_data.gearbox.fillna('manuell', inplace = True) $ test_data.gearbox.fillna('manuell', inplace = True)
y_pred = xgb_model.predict(X_test)
plt.figure(figsize=(20, 5)) $ distplot = sns.distplot(df['question_answer'], bins=101, color='darkblue', kde=False)
weather['precipitation_inches'].unique()
scoresdf.set_index('n_estimators')['score'].plot(figsize=(16,8))
pods.notebook.display_plots('independent_height_weight{fig:0>3}.png', '../slides/diagrams/ml', fig=IntSlider(0, 0, 8, 1))
df_merge = pd.merge(df_merge, df_imgs, on='tweet_id', how='inner')
ssc.start()
import plotly.plotly as py $ import plotly.graph_objs as go $ df3.drop('tweet_id', axis = 1) $ df3.drop('url', axis = 1) $ df3.sort_values(by= ['created_at']) $
pp.barplot(df=df, filters={'technology': tecs, 'variable': ['ACT']}, $            title='ACT - light')
aml.leader
df_country=pd.get_dummies(data=countries_df, columns=['country']) $ df_country.head()
births_by_date.index = [pd.datetime(2012, month, day) $                        for (month, day) in births_by_date.index] $ births_by_date.head()
paid_invoices['overdue'] = pd.to_datetime(paid_invoices['due'])
from sklearn.ensemble import RandomForestClassifier $ rnd_clf = RandomForestClassifier(random_state=42)
df3[['inv', 'ab_page']] = pd.get_dummies(df3['group']) $ df3.tail() $
cur.description   # Carries info regarding the tables. With index 0 containining headers
%matplotlib inline $ import seaborn as sns $ import matplotlib.pyplot as plt $ sns.set_style("whitegrid") $ sns.set(color_codes=True)
from scipy.stats import norm $ print(norm.cdf(z_score)) # 0.905058312759 $ print(norm.ppf(1-(0.05))) # 1.64485362695 $
g = open('../data/faa.txt').readlines()
df = pd.read_sql('SELECT a.address_id, a.city_id, c.city FROM address a FULL OUTER JOIN city c ON a.city_id = c.city_id ORDER BY a.address_id DESC;', con=conn) $ df
token_sendreceiveCnt = token_sendreceiveCnt.fillna(0)
v_item_hub.drop(v_item_hub_dropper, axis =1 , inplace = True)
results1.summary()
percentage = nypd_unspecified/nypd_complaints_total*100 $ print("%1.2f"%percentage)
data['Date'] = pd.to_datetime(data['Date']) #we like dates because you can query against dates $ data.set_index('Date', inplace=True) $ data['Year'] = data.index.year $ data['Month'] = data.index.month
data = [{'a': i, 'b':2 * i} $        for i in range(3)] $ pd.DataFrame(data)
try: $     donors_c['Donor Zip'] = donors_c['Donor Zip'].astype(int).astype(str) $ except: $     print("ValueError: invalid literal for int() with base 10: 'nan'")
df_precipitation.head()
df.head()
df_experiment = pd.read_csv('data/experiment_details_new.csv') # to-do: need to use "_new" dataset? $ df_experiment.head()
x = df2[df2['group']=='control']['converted'].mean() $ print("{:.2%}".format(x))
print soup.prettify()[28700:30500]
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ res = logit_mod.fit()
plt.axvline(diff_actual, c='red') $ plt.hist(p_diffs)
print cust_data[-cust_data.duplicated()].head(5) $
df1 = pd.read_csv('data/BMO CC2017.csv') $ df1.head(15)
df_clean.describe()
df_concat.head()
from sklearn.ensemble import RandomForestRegressor $ from rfpimp import *
df.dropna().shape == df.shape, df.shape
Labels = pd.read_hdf('//FS2.smpp.local\RTO\CIS-PD MUSC\decoded_forms\\form509.h5') $ watch_df = Labels[['SubjectCode','Q146_UTC']]
conn = sqlite3.connect("fires.sqlite") $ fires.head(10).to_sql("fires", conn)
df2=df.drop(index_values)
X_train = prepared_train[prepared_train.published <= '2016-04-01 00:00:00'] $ X_test = prepared_train[prepared_train.published > '2016-04-01 00:00:00'] $ y_train, y_test = X_train.favs_lognorm, X_test.favs_lognorm
i=random.randrange(len(train_trees)) $ print(train_trees[i])
text = ','.join(wordlist) $ wordcloud = WordCloud().generate(text) $ plt.imshow(wordcloud, interpolation='bilinear') $ plt.axis("off") $ plt.show()
df.head()
df_n.applymap(Cube)
autos["registration_year"].describe()
(autos['last_seen'] $         .str[:10] $         .value_counts(normalize = True, dropna = False) $         .sort_index() $         )
print(train.shape) $ print(test.shape)
new_page_count = df2.query('landing_page == "new_page"').count() $ new_page_prob = new_page_count / df2.shape[0] $ new_page_prob = new_page_prob[0] $ new_page_prob
from pyspark.sql import Row $ rddRow = rdd.map(lambda f: Row(f)) $ spark.createDataFrame(rddRow).toDF("col1").show()
con = sqlite3.connect('db.sqlite') $ print(pd.read_sql_query("SELECT * FROM temp_table ORDER BY total DESC", con)) $ con.close()
df = pd.read_pickle(pretrain_data_dir+'/pretrain_data_01.pkl') $ loaded_shape = df.shape $ print ("Data shape: ",loaded_shape) $ df = df.sort_values(by=['serial_number','date'],axis=0,ascending=True)
cens_key = open(os.getenv('PUIDATA')+'/census_key.txt', 'r+') $ myAPI = cens_key.readlines()[0] $
from sklearn.metrics import classification_report $ print(classification_report(daily_df_subset['Day Prediction'], daily_df_subset['Price_Change']))
fin_p.columns = ['SP500', 'OMXS30', 'EURSTOXX', 'Gold']
sample_df = df[df['encrypted_customer_id'].isin(toy['encrypted_customer_id'].unique())]
data.head()
hc.clearCache()
vader_df.dtypes
result = df2.groupby('group') $ result.describe()
os.mkdir('AV')
ac['Compliance Review End'].groupby([ac['Compliance Review End'].dt.year]).agg('count')
utils.read_sas_write_hdf(source_paths, data_dir, 'nhanes.h5')
s = pd.Series(['Tom ', ' William Rick', 'John', 'Alber@t']) $ s
load2017['hour'] = load2017['time'].str.slice(11,16) $ load2017['hour'] = pd.to_datetime(load2017['hour'], format = '%H:%M').dt.time $ load2017['hour'].head()
s[(s > 10) & (s < 15)].head()
features = afl_data[['date', 'game', 'team', 'opponent', 'venue', 'home_game']].copy()
df=pd.read_csv('twitter-archive-enhanced.csv')
with ZipFile('{0}.zip'.format(datapath / zipfile), 'r') as myzip: $     with myzip.open('2013_ERCOT_Hourly_Load_Data.xls') as myfile: $         pd_excel = pd.read_excel(myfile) $ pd_excel.head()
print("Shape: " + str(cleanedDataNoRetweets.shape)) $ print(cleanedDataNoRetweets.head())
print(data.shape) $ print(testdata.shape)
plate_appearances['Month'] = plate_appearances['game_date'].dt.month $ dummies = pd.get_dummies(plate_appearances['Month']).rename(columns=lambda x: 'Month_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1) $ dummies = pd.get_dummies(plate_appearances['game_year']).rename(columns=lambda x: 'Year_' + str(x)) $ plate_appearances = pd.concat([plate_appearances, dummies], axis=1)
fn_ss = 'neep_ashp_data.xlsx' $ dfh = pd.read_excel(f'data/heat-pump/raw/{fn_ss}', skiprows=6) $ dfh.head()
df_loan2[['fk_loan',0]].head()
bbc_df = constructDF("@BBC") $ display(constructDF("@BBC").head())
trial_parameter_nc = Plotting(S_distributedTopmodel.setting_path.filepath+S_distributedTopmodel.local_attr.value) $ trial_parameter = trial_parameter_nc.open_netcdf() $ trial_parameter['HRUarea']
talks_train = pd.read_json('train2.json')
plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101_d[df_nd101_d['ud730']>0].ud730)
highest_obs_temps_df = pd.DataFrame(highest_obs_temps, columns=['Station', 'date', 'temp']) $ highest_obs_temps_df.set_index('Station', inplace=True) $ highest_obs_temps_df.head()
def time_to_str(time): $     if time != time: $         return time $     else: $         return '{:04}'.format(int(time))
res = sm.tsa.seasonal_decompose(events_per_day['count_event_day'].values,freq=7,model='multiplicative')
p_new = ab_file2['converted'].mean() $ print(p_new)
train_df.describe() $ train_df['transaction_month'] = train_df['transactiondate'].dt.month $ train_df
Grouping_Year_DRG_discharges_payments.head() $
df = pd.DataFrame({'A': np.arange(50), $                'B': np.arange(50) + np.random.randn(50), $                'C': np.sqrt(np.arange(50)) + np.sin(np.arange(50)) }) $ print df[:10]
s.__class__
data = pd.Series([1, None, 3]) $ data
building_pa[building_pa.duplicated()]
from nltk.corpus import stopwords $ nltk_stops = stopwords.words()
df.ndim   # number of dimension
rng_utc = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz=dateutil.tz.tzutc())
len(w.data_handler.get_all_column_data_df())
file = 'https://assets.datacamp.com/production/course_2023/datasets/ebola.csv' $ ebola = pd.read_csv(file) $ ebola.info()
X_train.info()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new,convert_old], nobs=[n_new,n_old],alternative='larger')
data.info()
active_users.shape # there are only 7479 active users out of 22884 signed users $
df.sort_values('dealid', ascending = True, inplace = True)
train.index
poverty.drop(385, inplace=True)
new_non_discover_sale_transaction = post_discover_sales[~post_discover_sales['Email'].isin(new_customers_test['Post Launch Emails'].unique())] $ new_non_discover_sale_transaction['Total'].mean()
keyed_hist = ( $     pings_dataset $     .select(redirects='payload.keyedHistograms.NETWORK_HTTP_REDIRECT_TO_SCHEME.https.values') $     .records(sc, sample=0.05) $ )
merkmale.Merkmal_typ[merkmale.Merkmalcode=='RK'].unique()
for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']: $     main["{}_d".format(i)] = main['{}_ta'.format(i)] - main['{}_tb'.format(i)]
predictions = knn.predict(test[['property_type', 'lat', 'lon','surface_covered_in_m2']])
evaluator.get_metrics('tag_level_results')
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2, index=['store 1', 'store 2']) $ store_items
tweet_df[['total']].count()
kickstarter.head()
df.head()
highc = closep.max() $ lowc = closep.min() $ print('Largest change bewteen any two days (not consecutive) is:',highc-lowc)
C = pd.merge(A,B, on = 'team', how = 'left',left_index=True, right_index=True) $ C
delimited_hourly.reset_index(inplace=True) $ delimited_hourly.head()
df = scattering.get_pandas_dataframe() $ df.head(10)
sorted(modern_combos.map(lambda c: (c[0], 1)).countByKey().iteritems(), key=lambda x: x[1], reverse=True)
df = pp.create_plotdata(results) $ tecs = ['fluorescent_tube', 'bulb', 'halogen_bulb', 'led_lamp']
mgxs_lib.load_from_statepoint(sp)
len(tract_block_indexes.keys())
unhappy_teams = df[(df['wins'] > (df['loses'] + 20)) & (df['rank'] > 1)] $ unhappy_teams.sort_values(by=['wins'], ascending=False).head(10)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
p_diffs=[] $ new_convert=np.random.binomial(n_new, p_new, 10000)/n_new $ old_convert=np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs=new_convert-old_convert
ratings.describe()
df2[(df2['landing_page']=='new_page')].count()[0]
aapl.loc['2001':'2004',['open','close','high', 'low']].plot() $
responsesJsonList[1:5] # Citigroup in no.3
sortdf.loc[:first_value_greater_50]
hour = pd.tseries.offsets.Hour(normalize=True)
for i in ["List_Of_Users_ID", "List_Of_Comments", "List_Of_Hashtags_Caption", "List_0f_Hashtags_Comments"]: $     df.loc[df[df[i].apply(lambda x: len(x)) == 0][i].index,i] = np.NaN $ df.loc[df[df["Main_Caption"] == 'No caption']['Main_Caption'].index,\ $    'Main_Caption'] = np.NaN
history.to_csv('../data/merged_data/history.csv')
user_actions = get_action_counts(session,top_actions).reset_index() $ user_actions.head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata = pd.read_csv(path, sep ='\s+', nrows=80, skiprows=(1,2,5,10,60), usecols=(1,3,5)) $ print("mydata.head(5)\n",mydata.head(5)) $ print("Length of mydata = ",len(mydata))
hawaii_measurement_df = pd.read_csv(r"\Users\Josue\Desktop\\hawaii_measurements.csv")
total_rows2 = df2.shape[0] $ users_converted2 = df2.converted.sum() $ prob_user_conversion = users_converted2 / total_rows2 $ print(prob_user_conversion)
def day_of_week(date): $     days_of_week = dict(zip(range(7), ['monday','tuesday','wednesday','thursday','friday','saturday','sunday'])) $     return days_of_week[date.weekday()] $ day_of_week(lesson_date)
plt.hist([h.company for h in airlines])
review['Category'].unique()
df.isna().sum()
plt.scatter(sing_fam.rooms.values, sing_fam.rp1lndval.values);
with open('tweet_json.txt', 'w') as outfile:  $     json.dump(df3_list, outfile)
tweets_df.created_at
re.findall('mailto:neal.caren@gmail.com so', text)
df_members.head()
temp_data = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs),\ $     func.avg(Measurement.tobs)).\ $     filter(Measurement.station == 'USC00519281').all() $ print(temp_data)
convert_old =  sum(df2.query("group == 'control'")['converted']) $ convert_new =  sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
df_groups = pd.read_csv('groups.csv') $ df_groups.head()
xgb_predictor.content_type = 'text/csv' $ xgb_predictor.serializer = csv_serializer
logs['date'] = logs.date_time.apply(date_only)
run txt2pdf.py -o"2018-06-14 2148 FLORIDA HOSPITAL Sorted by Payments.pdf"  "2018-06-14 2148 FLORIDA HOSPITAL Sorted by Payments.txt"
bands.head()
finals[finals.pts_l==1].sort_values(['season', "PTS"], ascending=[False, False]).head()
print(lgb_cv.best_params_) $ print(lgb_cv.best_score_)
sns.distplot(reddit_master['age'], kde = False) $ plt.xlabel("Age (hours)") $ plt.ylabel('Frequency') $ plt.title('Post Age Distribution');
count_non_null(geocoded_df, 'Judgment.Date')
loan_plan_nar=payment_plans_idx.loc[payment_plans_idx.interval!=0,['interest_amount','initial_principal_amount']].groupby(level=(0,1)).sum()
os.getcwd()
traffic_df_rsmpld = traffic_df_rsmpld.sort_index().loc[idx['2016-01-01':'2017-12-31',:],:].sort_index() $ traffic_df_rsmpld.info() $ traffic_df_rsmpld.head()
prob_convert_t = df2_treatment.converted.mean() $ prob_convert_t
calories_df.to_csv('python_data/calories_from_moves.csv', index=False)
post_id = posts.insert_one(post) #insere dados $ post_id $
x.head()
df.topicmax.value_counts()
df_test[['id','visitors']].copy().to_csv('lgbmtscv_out_tscv_s2.csv',index=False)
test_rank.sort_values("Total Orders", ascending=False).head()
no_specialty.shape
df_total.head()
ozzy.bark()
shmuel = relevant_data[relevant_data['User Name'] == 'Shmuel Naaman'] $ df = shmuel['Event Type Name'].value_counts() $ df_shmuel = pd.Series.to_frame(df) $ df_shmuel.columns = ['Shmuel N'] $ df_shmuel
df_questionable[df_questionable['clickbait'] == 1]['link.domain_resolved'].value_counts(25).head(25)
soup.li.parent
eligible_by_week = {} $ for key in set([x['first.comment.week.diff'] for x in eligible_comments]): $     eligible_by_week[key] = [x['author'] for x in eligible_comments if $                              x['first.comment.week.diff'] == key]
auth_endpoint = 'https://iam.bluemix.net/oidc/token'
df2['intercept']=1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2.head()
df_master.loc[rate_change[rate_change['rating']==177.6].index]
station_count = session.query(func.count(distinct(Measurement.station))).all() $ station_count $
import statsmodels.api as sm $ df2['intercept'] = 1 $ log_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = log_mod.fit() $
mask = (nullCity["creationDate"] > '2014-01-01') & (nullCity["creationDate"]<= '2014-12-31') $ nullCity2014 = (nullCity.loc[mask]) $ nullCity2014.head()
weather_yvr['Temperature (C)'].plot();
EPEXprices = dfEPEX[::2].transpose().stack() # take every second row $ EPEXprices.index = index # assign generated index to price data $ EPEXprices.head() # verify extracted price data
sites_no_net = sites[sites['On Zayo Network Status'] == 'Not on Zayo Network'].groupby(['Account ID'])['Building ID'].count().reset_index().sort_values(by='Building ID', ascending=False)
twitter_archive_full[(twitter_archive_full.in_reply_to_status_id_x.isna()==False) | $                      (twitter_archive_full.retweeted_status.isna()==False)]
X = train.drop(['Page','date','Visits'], axis = 1) $ y = train['Visits'] $ X_test = test.drop(['Page','date','Visits'], axis = 1) $ y_test = test['Visits']
pst.template_files
foil_538 = pd.read_csv("data/uber/uber-foil.csv", parse_dates=["date"]) $ foil_538.head(3)
startlist = ['2015', '2016', '2017', '2016-04', '2017-04'] $ for s in startlist: $     returns_vol_tbl(start=s, assets=assets_).plot.barh() $     plt.title("start @" + s) $     plt.show()
df['MeanFlow_mps'] = df['MeanFlow_cfs'] * 0.028316847
LOC.shape
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=4000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
all_df.head(2)
strategy.marketdata('BTC').df().head()
hour.apply(pd.Timestamp('2014-01-01 22:00'))
timedog_df['tweetFavoriteCt'].max() $ timedog_df.index[timedog_df['tweetFavoriteCt'] == timedog_df['tweetFavoriteCt'].max()].tolist() $ timedog_df['userLocation'][6825] #this one is empty $ timedog_df['Hashtag'][6825]      #hashtag: dog
fatalities = data ['Fatalities'].sum() $ injured = data['Injured'].sum()
txns[txns['token']=='DAI'].amount.apply(lambda x: np.clip(np.log(x), -5,15)).hist()
len(image_predictions_df[image_predictions_df.p1_dog == False])
np.info(np.diag)
random_crashes_df.shape
old_page_converted = np.random.choice([0,1],size = n_new, p = [1-p_old,p_old]) $ old_page_converted
from sklearn.metrics import classification_report, confusion_matrix $ import itertools
pct_95_trips_data.duration.hist() # now we can visualize the duration and make sense of it $ plt.show() # to dsiplay the plot $
submission2 = pd.DataFrame(submission, index=submission['id']) $ submission2[['proba']] = preds[:,1] $ submission2.drop(['id', 'proba2'], inplace=True, axis=1) $ submission2.to_csv('sub.csv')
vio.head(10)
params = {'figure.figsize': [4,3],'axes.grid.axis': 'both', 'axes.grid': True, 'axes.labelsize': 'Medium', 'font.size': 12.0, \ $ 'lines.linewidth': 2} $ plot_partial_autocorrelation(series=dr_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of dr hours new patients')) $ plot_partial_autocorrelation(series=dr_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of dr hours existing patients'))
treatment_convert = df2.query('group == "treatment"').converted.mean() $ treatment_convert
old_page_converted = np.random.binomial(n_old,convert_rate_p_old)
df_countries = pd.read_csv('countries.csv') $ df_full = pd.merge(df_countries, df2, how='outer') $ df_full.head()
station_df = pd.read_sql("SELECT * FROM station", conn) $ station_df.head(10)
x_train, x_test, y_train, y_test = train_test_split(bow_df, amazon_review['Sentiment'], shuffle= True, test_size=0.2)
to_datetime = lambda x: pd.to_datetime(x)
bigdf.loc[bigdf['comment_fullname'] == 't1_drdvqmo']
plt.figure(figsize=(10,5)) $ plt.plot(df_cluster.cluster, df_cluster.cluster_error) $ plt.xlabel('Number of cluster') $ plt.ylabel('cluster errors')
s.index[0]
twitter_df.dtypes
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113529266152000.json', encoding="utf-8-sig") as data_file: $     json_data4 = j.load(data_file)
serious_end_count = 0 $ for row in data: $     if re.search("[\[\(][Ss]erious[\]\)]$", row[0]): $         serious_end_count = serious_end_count + 1 $ print("[2] serious_end_count: " + str(serious_end_count))
auto.describe().transpose()
mask = percent_quarter.abs().apply(lambda x: x > 1) $ percent_quarter[mask].nlargest(4)
pandas_ds["time2close"].plot(kind="hist", logy=True)
df[df.state.isin(['56', 'AB', 'BC', 'CF', 'Ca', 'Co', 'HY', 'IO', 'Ny', 'PR', 'UK', 'VI', 'ja'])]
from statsmodels.stats.weightstats import ztest $ ztest(hm_data.weight.values, value=85., alternative="two-sided")
autos["price"] = autos["price"].str.replace("$","") $ autos["price"] = autos["price"].str.replace(",","") $ autos["price"] = autos["price"].astype(int) $ autos["price"].head(3)
joined_train_df = pd.read_pickle(slowdata+'joined_train_df.pkl') $ joined_test_df = pd.read_pickle(slowdata+'joined_test_df.pkl')
r.json()['dataset_data']
raw_df = pd.concat([raw_df,pd.DataFrame(columns=([3]))]) $ raw_df[3] = False
np.datetime64('2015-07-04')
woba_leaderboard = df.groupby(['batter_name', 'batter'])['woba_value'].agg(['mean', 'count']) $ woba_leaderboard.loc[woba_leaderboard['count']>100,].sort_values('mean', ascending=False).head()
y_ = df2["user_id"].count() $
diff=df.index[7]-df.index[2] $ diff.seconds
df['date'] = pd.to_datetime(df['date'])
m = Prophet() $ m.fit(df1);
dates =  pd.DatetimeIndex(pivoted.columns) $ dates[(labels == 1) & (dayofweek < 5)]
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['tweets']))) $ print("Percentage de negative tweets: {}%".format(len(neg_tweets)*100/len(data['tweets'])))
nufission = xs_library[fuel_cell.id]['nu-fission'] $ nufission.print_xs(xs_type='macro', nuclides='sum')
cleanedData = allData $ cleanedData[cleanedData['text'].str.contains("\&amp;")].shape[0]
print(len(pbptweets['text'])) $ print(len(pbptweets['text'].drop_duplicates()))
stores = stores.withColumnRenamed("type", "store_type") $ train_holiday_oil_store = train_holiday_oil.join(stores, 'store_nbr', 'left_outer') $ train_holiday_oil_store.show()
df2.drop_duplicates(subset=['user_id'], keep='first',inplace=True) $ df2.shape $
import pandas as pd $ df = pd.read_csv('Consumer_Complaints.csv', encoding = "ISO-8859-1") $ df.head()[['Product', 'Consumer complaint narrative']]
intersections_irr['date_time_hour'] = [datetime.datetime.strptime( dateStr,'%Y-%m-%d %H:%M:%S').time().hour for dateStr in intersections_irr['updateTimeStamp'] ]
wellness_visits.limit(10).toPandas()
ADBC = AdaBoostClassifier(n_estimators=50) $ ADBC.fit(X, y) 
inspector = inspect(engine) $ inspector.get_table_names()
def func8(x): $     return pd.Series([x.min(), x.mean(), x.max()], $                   index=['MIN.', 'MEAN.', 'MAX.']) $ df.apply(lambda x: func8(x))
cust_count.head(2)
geolocated_tweets = tweets_df[tweets_df.geo.notna() == True] $ geolocated_tweets.geo.values $ coordinates_df = pd.DataFrame(list(geolocated_tweets.geo.values)) $ coordinates_df.coordinates
tuna_90 = mapped.filter(lambda row: (row[4] > 0 and row[4] <= T1)) $ tuna_90_DF = tuna_90.toDF(["cid","ssd","num_ssd","tsla","tuna"]) $ tuna_90_pd = tuna_90_DF.toPandas()
df['hashtags'] = df.text.str.extract('(#\w*)')
p_convert = df2.converted.mean() $ p_convert
df_pageviews_mobile_web.head()
weather_date = weather_data['Date'].apply(format_time)
sns.barplot(x='hour',y='number_of_tweets',data=tweets_per_hour_df)
df.to_csv('./data/wiki_stock_price.csv', sep=',')
pt = jdfs.pushed_at.apply(lambda x: time.mktime(x.timetuple())) $ npt = pt - pt.min()
df['Shipping Method name'] = df['Shipping Method name'].fillna(df['Shipping Method Id'])
df2 = df2.drop_duplicates(subset=['user_id']) $
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_values()
transConcat = trans["Improvements"] $ data.update(transConcat) $ data[data["SiteName"] == "Tokyo Joypolis"]["Improvements"].iloc[:20] $
own_star[own_star.duplicated('uniqueID', keep=False)].shape
twitter_archive_master[twitter_archive_master.rating_numerator > 15].head()
therm_fiss_rate = sp.get_tally(name='therm. fiss. rate') $ fast_fiss = fiss_rate / therm_fiss_rate $ fast_fiss.get_pandas_dataframe()
niners_td.groupby('Jimmy').Touchdown.sum()
temp_df[temp_df.total_companies > 100].proportion_no_psc.describe()
sample_size_old_page = df2.query('landing_page == "old_page"').shape[0] $ print('Sample size old_page: {}'.format(sample_size_old_page)) 
df_users_6['unit_flag'].unique()
cov_df.loc[lst_dates[0]].copy()
print("The probability of individual in the treatment group converting is: {}".format(df2[df2['group'] == 'treatment']['converted'].mean())) $
open_issues = Issues(github_index).is_open().get_cardinality("id_in_repo").by_period() $ print("Trend for month: ", get_trend(get_timeseries(open_issues))) $ open_issues = Issues(github_index).is_open().get_cardinality("id_in_repo").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(open_issues)))
print("the probabilty that the indvidual was in treaatment group = ",df2[df2['group']=='treatment']['converted'].mean())
df.head(30)
train['number_of_features'] = train['features'].map(len)
dog_stages = Counter(df_clean.dog_stage) $ dog_stages
pd.get_dummies(df.A)
k['Count-2018'] =k['Count-2018'].astype(int) $ k
train.shape
y, X = dmatrices('converted ~ ab_page + CA +UK' , df2_new, return_type='dataframe') $ vif = pd.DataFrame() $ vif["VIF Factor"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] $ vif["features"] = X.columns $ vif
lasso = Lasso() $ lasso.fit(X1_train, y1_train) $ predicted_lasso_y = lasso.predict(X1_test) $ print("Lasso Mean Squared error:",mean_squared_error(y1_test, predicted_lasso_y)) $ print("Lasso R2 Score:",lasso.score(X1_test, y1_test))
df.ID # Shows us the elements inside the column named, "ID"
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2014 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2014 Percentiles.txt"
reg = sm.Logit(df_joined['converted'], df_joined[['intercept', 'US', 'UK']]) $ reg_fit = reg.fit() $ reg_fit.summary()
recipes.ingredients.str.len().describe()
class A(object, metaclass=type): $     pass $ a = A() $ print(f'a.__class__ = {a.__class__}') $ print(f'A.__class__ = {A.__class__}')
null_valls = np.random.normal(0,p_diffs.std(),p_diffs.size) $ plt.hist(null_valls); $ plt.axvline(x=actual_obs_diff, color = 'g', linewidth = 1);
scores = dataset['points'].tolist()
trump.shape
xmlData['bedrooms'] = pd.to_numeric(xmlData['bedrooms'], errors = 'raise') $ xmlData['bathrooms'] = pd.to_numeric(xmlData['bathrooms'], errors = 'raise') $ xmlData['floors'] = pd.to_numeric(xmlData['floors'], errors = 'raise')
targettraffic['weekday'] = targettraffic['DATE_TIME'].apply(lambda x:x.weekday()) $ targettraffic['weekdayname'] = targettraffic['DATE_TIME'].apply(lambda x:x.weekday_name)
plt.hist(null_vals);
archive_clean.info()
pred.describe()
pd.to_datetime('2018-03-23')
df_old = df2.query('landing_page == "old_page"') $ n_old = df_old.shape[0] $ n_old
z_score, p_value = sm.stats.proportions_ztest([convert_old * n_old, convert_new * n_new], [n_old, n_new], alternative = 'smaller') $ z_score, p_value 
f_ip_app_clicks.show(1)
(df_merged['text'].iloc[1150],df_merged['text'].iloc[602],df_merged['text'].iloc[201])
df_test4_promotions['AwardType'].fillna('Discount', inplace=True);
row_idx = (slice(None), slice(None), slice('Bob', 'Guido'))  # all years, all visits, Bob + Guido $ health_data_row.loc[row_idx, 'HR']
cols = {i:[] for i in range(300)} $ index = [] $ for line in lines: $     row_split(line, cols, index) $ df = pd.DataFrame(data=cols, index=index).astype(float) $
shelter_pd.describe(include = "all")
df.query('group != "treatment" and landing_page == "new_page"').count()[0]
r = yfs.session.get(url, params={'format': 'json'}) $ r.status_code
import statsmodels.api as sm $ from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)#workaround for issues with logistics regression in statsmodel.api $ logit_mod=sm.Logit(df2['converted'],df2[['ab_page','intercept']]) $ results=logit_mod.fit()
train_probs = model.predict(xgb.DMatrix(X_train)) $ validation_probs = model.predict(xgb.DMatrix(X_validation)) $ print('Error on training set ', rmspe(np.exp(get_prob(train_probs))-1, y_train.values)) $ print('Error on validation set ', rmspe(np.exp(get_prob(validation_probs))-1, y_validation.values))
df = df.loc[df['treatments'].notnull()]
shifted_backwards = msftAC.shift(-2) $ shifted_backwards[:5]
df_ind_site.groupby(['id_num', 'year','drg3'])['disc_times_pay'].agg([np.sum, np.mean, np.std, np.median, $                      np.var, np.min, np.max, percentile(50), percentile(95)])
green_line = data[:training_split_cut] $ purple_line = [] $ for iter_x in np.arange(training_split_cut): $     purple_line.append(np.nan) $ purple_line = purple_line + list(reg1.predict(X_test))    
df.dropna()
binary_sensors_df.head()
mismatch1 = df.query('landing_page == "new_page" and group =="control"') $ print("No. of times where control group user incorrectly lands on new_page is {}".format(len(mismatch1))) $ mismatch2 = df.query('landing_page == "old_page" and group =="treatment"') $ print("No. of times where treatment group user lands incorrectly on old_page is {}".format(len(mismatch2))) $ print("No. of times where new_page and treatment don't line up is {}".format(len(mismatch1) + len(mismatch2)))
ctr.head(5)
scores[scores.RottenTomatoes == scores.RottenTomatoes.max()]
df['Complaint Type'] [df['Agency'] =='DOT'].value_counts().head()
gfD.to_sql('distances', conn) $ gfD.iloc[:,0:15].to_sql('distances', conn)
ax = tm_week_df['Incident_number'].loc[day_order].plot(kind="bar", $                                                        legend=False, $                                                    figsize=(8,5)); $ ax.set_ylabel("Number of vegetation fires") $ ax.set_xlabel("Day of the week")
path = r'X:\CIS-PD Videos\timestamp' $ fname = 'video_utc_timestamp.csv' $ filename = os.path.join(path, fname) $ with open(filename,'wb') as f: $     result.to_csv(filename, sep=',')
print(Meter1.Mode) $ Meter1.ModeSet('DC_V') $ print(Meter1.Mode)
df2[df2.duplicated(['user_id'], keep=False)]
import os $ KAGGLE_USERNAME = os.environ.get("KAGGLE_USERNAME") $ print(KAGGLE_USERNAME)
print(loan_stats["issue_d"].head(rows=2)) $ print(loan_stats["issue_d"].year().head(rows=2))
pulledTweets_df['Processed_tweet'] = pulledTweets_df['text'].apply(cleaner) $ pulledTweets_df['Processed_tweet'] = pulledTweets_df['Processed_tweet'].apply(remove_stopwords) $ pulledTweets_df['Processed_tweet'] = pulledTweets_df['Processed_tweet'].apply(lemmatize)
live_weights.value_counts().sort_index()
df3.head()
df = df.sort_values(by='retweets', ascending=False)
autos["price"].value_counts().sort_index(ascending = False).head(20)
autos = autos.drop(['seller','offer_type','nr_of_pictures'],axis=1)
new_page_converted = np.random.binomial(1, p_new,size = n_new)
tweet_data.info()
lossprob2 = fe.bs.smallsample_loss(2560, poparr2, yearly=256, repeat=500, level=0.90, inprice=1.0) $
new_df[['US', 'CA']] = pd.get_dummies(new_df['country'])[['US', 'CA']]
df_img_algo_clean.head()
rolled_returns['2017-06-07':].mean()
temperature_kelvin = temperature_celsius + 273.15 $ print(temperature_kelvin)
df_Sessions.head(n=10)
mdl2 = lm.LinearRegression(normalize=False)
test_portfolio['date'].unique()[:1]
p_diffs = [] $ new_convert = np.random.binomial(n_new,p_new,10000)/n_new $ old_convert = np.random.binomial(n_old,p_old,10000)/n_old $ p_diffs = new_convert - old_convert
base_date = dt.datetime.strptime("2017-08-23", "%Y-%m-%d") $ YrFrombd = base_date - dt.timedelta(days=365) $ print(YrFrombd)
df_clean['date'] = pd.to_datetime(df_clean['date']) $ df_clean['time'] = pd.to_datetime(df_clean['time'])
ghana['WindDirDegrees'] = ghana['WindDirDegrees'].astype('float64')   
test_data.text
gps__interestlevel_df = train_df[['latitude','longitude','interest_level']].copy() $ gps__interestlevel_df.head(5)
cur.execute('SELECT COUNT(*) FROM materials') $ print(cur.fetchone())
doc = [''] $ bow_vector = dictionary.doc2bow(doc) $ print([(dictionary[id], count) for id, count in bow_vector])
train = pd.merge(train, train_geo[['row_index','NTACode']], how='inner', left_index=True, right_on='row_index')
df_movies = pd.merge(df, xml_in_sample, left_on=['movieId'], right_on=['authorId'],  how='left')[['movieId', 'authorName']] $ df_movies.drop_duplicates(subset=None, keep='first', inplace=True)
detroit_census2=detroit_census.drop("Fact Note", axis=1) $ detroit_census2=detroit_census2.drop("Value Note for Detroit city, Michigan", axis=1)
all_labels = [] $ for i in train_labels: $     all_labels.extend(i) $ for i in test_labels: $     all_labels.extend(i)
prec_group = precipitation.groupby(['valid']).agg({"precip_in" : np.max})
git_blame.sha = pd.Categorical(git_blame.sha) $ git_blame.path = pd.Categorical(git_blame.path) $ git_blame.author = pd.Categorical(git_blame.author) $ git_blame.info(memory_usage='deep')
df.select(a.alias('sepal_width')).show(5)
sn.distplot(a.A.flatten()[:],kde=False,norm_hist=True,bins=900) $ plt.xlim((0.,0.5))
bg_df2.dtypes # list columns with data types
tweet_archive_df.head(3)
features_to_use=[cols for cols in train.columns if cols not in ['readingScore','index']] $ features_to_use
shows.isnull().sum()
sales_df[sales_df['Product_Id'] == 5].groupby(['Country']).sum()['Quantity'].sort_values(ascending=False) $
data_archie = data_archie.drop(['fullname','longitude','latitude','source'] , 1)
fval = 7.243 $ fval2 = 6.78e-5
import pandas as pd $ sports = [('Soccer', None, 'Tennis', 3), (None, None, None, None), ('Soccer', None, 'Lacrosse', 5), ('Soccer', 'Basketball', 'Lacrosse', 4), (None, 'Basketball', 'Lacrosse', 2)] $ sports = pd.DataFrame(sports, index = ['Lauren', 'Paul', 'Jack', 'Max', 'Lucy'], columns = ['fall', 'winter', 'spring', 'varsity letters']) $ sports
cityID = '5d231ed8656fcf5a' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Petersburg.append(tweet) 
df.head(10)
df.sort_values("Year",ascending=True)
confidence = clf.score(X_test, y_test)
df.head()
df['upper'] = df['body'].apply(lambda x: len([x for x in x.split() if x.isupper()])) $ df[['body','upper']].head()
reddit_comments_data.select(mean("score")).show(truncate=False)
dci['weekEnergy'].max()
df_c_merge = countries_df.set_index('user_id').join(df_regression.set_index('user_id'), how='inner') $ df_c_merge.head()
twitter_archive_df_clean.head(3)
unique_urls.query('mentions >= 10').sort_values('avg_payout', ascending=False)[0:20][['url', 'total_payout']]
csvData['yr_built'].value_counts()
df.gender.value_counts().plot(kind='bar');
data = read_root('./preprocessed_files.root') $ data.head()
quantiles = rfmTable.quantile(q=[0.25,0.5,0.75]) $ quantiles = quantiles.to_dict()
new_avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_newReview, model=model, num_features=5000) $ print (np.round(new_avg_word_vec_features, 3))
bt.plot_trades() $ ohlc.C.rolling(short_ma).mean().plot(c='green') $ ohlc.C.rolling(long_ma).mean().plot(c='blue') $ plt.legend(loc='upper left') $ pass
df_2017['bank_name'] = df_2017.bank_name.str.split(",").str[0] $
ValidEvents.head(3)
print(type(dicts)) $ print(dicts.keys())
df.to_csv('ab_clean.csv', index=False) $ df2 = pd.read_csv('ab_clean.csv')
daily.asfreq("H")
def load_csv(csv): $     p=os.path.join("data/", csv) $     print (p) $     data=pd.read_csv(p, encoding = "ISO-8859-1", engine='python') $     return data 
df2.query('landing_page == "new_page"').count()['user_id']/df2.shape[0]
df_centered.limit(5).toPandas()
(reddit["engagement"] > 0).sum()
city_dict = {} $ for city in cities_list: $     city_dict[city] = api.geo_search(query = city, wait_on_rate_limit=True, granularity = 'city')[0].id
data_2017_subset.head(5)
stfvect.get_stop_words()
collection.list_items()
r.groupby('cat').count()
df_ad_state_metro_1[df_ad_state_metro_1['candidates'] =='Donald Trump, Hillary Clinton'].head(5)
class Insect(Bug): $     def __init__(self): $         super().__init__(name='insect') $ Insect()
data.groupby('category').position.agg(pd.Series.mode)
people_person['date_joined'] = pd.to_datetime(people_person['date_joined'])
cust_demo.age.plot.box()
print(df_A.index) $ print(df_A)
df.sort_values(by='B', ascending=True)
np.sqrt(fruits)
from ramutils.classifier.utils import reload_classifier $ classifier_container = reload_classifier('R1387E', 'catFR5', 1, mount_point='/Volumes/RHINO/') $ classifier_container.features.shape # n_events x n_features power matrix
train_Y = train['Fare'] $ train_X = train.drop('Fare', axis = 1) $ test_X = test
bg_df2 = pd.DataFrame(bg3) # create new variable explicitly for it being a DataFrame in pandas $ bg_df2 $
df[df['group']=='control'].groupby('landing_page').count()
pernan = 0.999999 $ nbart_allsensors = nbart_allsensors.dropna('time',  thresh = int(pernan*len(nbart_allsensors.x)*len(nbart_allsensors.y)))
no_atmention = re.sub(r'@[\w_]+','', no_urls) $ print(no_atmention)
def get_data_lee_background_file(): $     test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) $     lee_train_file = test_data_dir + os.sep + 'lee_background.cor' $     return lee_train_file
data.index
print("Leavetimes shape:") $ dfleavetimes.shape
p_new = (df2.query('converted == 1')['user_id'].nunique())/(df2.user_id.nunique()) $ p_new
x2 = api.GetUserTimeline(screen_name="JNkappers", count=20, include_rts=False) $ x2 = [_.AsDict() for _ in x2]
numberly = pd.read_csv('numberly.csv') $ ttest(numberly["Many"], numberly["A lot"])
stacked=football.stack() $ stacked.head() $ stacked.unstack().head()
tweets = pd.read_csv("tweets.csv")
session.query(func.count(Measurement.id)).all()
df.head(30)
import pandas as pd $ train = pd.read_csv( "../data/train.csv", header=0, nrows = 10000) $ test = pd.read_csv( "../data/test.csv", header=0, nrows = 10000) $ print("Read %d labeled train comments and %d labeled test comments\n" % (train["comment_text"].size, test["comment_text"].size))
four_d = dt.timedelta(days = 4) $ creations["creator_autoconfirmed"] = ( $     (creations["user_edit_count"] >= 10) & $     (creations["creation_timestamp"] >= (creations["user_registration"] + four_d)) $     )
duration = np.array(pd.to_datetime(df.timestamp).sort_values(ascending=True)) $ td = duration[-1] - duration[0] $ days = td.astype('timedelta64[D]') $ days / np.timedelta64(1, 'D')
with pd.option_context('display.max_colwidth', 130): $     print(news_title_docs_high_freq_words_df['high_freq_words'])
details.isnull().sum()
model_avg=(0.2*gb_pred_best+0.2*knn_pred_best+0.3*xgb_pred_best+0.1*rf_pred_best+0.2*lgb_pred_best)
df_station_group = mta.groupby(['STATION', mta.DATE_TIME.dt.date])['NEW_ENTRIES'].sum() $ df_station_group.tail(20)
sensor.type
plt.rcParams['axes.unicode_minus'] = False $ dta_701.plot(figsize=(15,5)) $ plt.show()
tweets_list = tweets_list.str.split().values.tolist()
retweets['id'].value_counts()
pd.to_datetime(eth['Date(UTC)']).head()
svc=svm.SVC().fit(X_train,y_train)
plt.scatter(rets.RGSE,rets.TAN)
url_df=pd.DataFrame({'url':urls})
trips_completed = stats.binom.rvs(n=1, p=0.95, size=len(df_trips)) $ df_trips["trip_completed"] = trips_completed
feature_by_missing = df_train.columns[df_train.isnull().sum()/df_train.shape[0] >= 0.4]
plt.hist(new_page_converted); $ plt.axvline(x=p_new, color="red");
vow[['Open', 'High', 'Low', 'Close']].plot()
bus = cX_test.loc[p_flag & t_flag,['name', 'text', 'city', 'prob']].copy() $ bus = bus.sort_values(by='prob', axis=0, ascending=False) $ bus.head(10)
example_pd = posts.find() $ df_pandas =  pd.DataFrame(list(example_pd)) $ df_pandas
data = data.set_index('time')
data_donald_replies = pd.read_csv("ddr_following_and_hashtag.csv", index_col=0, dtype={'reply_id':str}) $ data_donald_replies.head()
parent_input_count_hist = cached.map(lambda x: x[1]['parent_input']).histogram(range(20)) $ draw_histogram("Parent Input Hangs Distribution", parent_input_count_hist)
df_twitter_archive_master[df_twitter_archive_master['tweet_id']=='749981277374128128'].jpg_url
earlier_timestamps = len(timestamp_left_df[timestamp_left_df['time_delta_seconds'] > 0]) $ ratio = earlier_timestamps / len(timestamp_left_df) *100 $ print("There are %.2f%% objects in which the 'timestamp' are even earlier than events were created." % ratio)
from sklearn.preprocessing import StandardScaler $ scaler = StandardScaler() $ X_train_scaled = scaler.fit_transform(X_train) $ X_test_scaled = scaler.transform(X_test)
mismatch = df.query("group == 'treatment' and landing_page == 'old_page'").count() + df.query("group == 'control' and landing_page == 'new_page'").count() $ print(mismatch)
liberiaDeaths = liberiaFullDf.loc['Newly reported deaths'] $ liberiaDeaths.head()
num_dr_new.index
p_diffs = [] $ for _ in range(10000): $     new_page_converted_samples = np.random.choice([1,0], size=n_size, p=[p_new, (1 - p_new)]) $     old_page_converted_samples = np.random.choice([1,0], size=o_size, p=[p_old, (1 - p_old)]) $     p_diffs.append(new_page_converted_samples.mean() - old_page_converted_samples.mean())
energy_cpi_with_id.loc[:,'Frequency'] = energy_cpi_with_id.loc[:,'Frequency'].map(lambda i: i.split(':')[0]) $ energy_cpi_with_id
rdb.drivers()
print 'First observation period :', min(cpi_all['Time'].cat.categories.tolist()) $ print 'Last observation period:', max(cpi_all['Time'].cat.categories.tolist())
df.query("converted==True").shape[0]/df.shape[0]
prob_contr =df2.query("group=='control'").converted.mean() $ print('The probality of an individual converting in the control group converting is {}'.format(prob_contr))
import datetime as dt $ now = dt.date(2018,1,1)
print "The unique number of devices is :%d" %(fraud_df.device_id.nunique()) $ print "the total number of user id is :%d" %(fraud_df.user_id.nunique())
df2['user_id'].nunique()
onespeaker = talks_train.speaker_ids.apply(lambda x:len(x)==1)
data_2017_subset['incident_zip'] = data_2017_subset.incident_zip.str.extract(r'([0-9]{5})', expand=False)
train_x,  test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.3) $ train_x.shape, test_x.shape
vto_be_predicted_Day5 = 83.28804904 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
cercanasA1_11_14Entre100Y125mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 100) & (cercanasA1_11_14['surface_total_in_m2'] < 125)] $ cercanasA1_11_14Entre100Y125mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre100Y125mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre100Y125mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
test_df.dropna(inplace=True)
nold = df2.query('group == "control"').shape[0] $ print(nold)
df.isnull().values.any()
predictions_clean['p1'] = predictions_clean['p1'].str.replace('_', ' ') $ predictions_clean['p2'] = predictions_clean['p2'].str.replace('_', ' ') $ predictions_clean['p3'] = predictions_clean['p3'].str.replace('_', ' ')
average_polarity.fillna('No Data',inplace=True) $ count_polarity.fillna('No Data',inplace=True)
print(full_df.dtypes)
autodf.model = autodf.model.astype('category')
people.loc["charles"]
seed = 2210 $ (train_sample, validation) = modeling2.randomSplit([0.9,0.1], seed=seed)
NumberofCompanies = 88 $ PredictionTimepoints = 2 $ FirstIndex = PredictionTimepoints $ MaxPoints = mydata.shape[0]-FirstIndex $ print(MaxPoints)
hc.sql('drop table if exists asm_wspace.usage_400hz_2017_Q4')
print(type(df.groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011'])) $ print(type(df.groupby(level=0)['POPESTIMATE2010']))
df[df['Agency'] == 'DOT'].resample('M').count().plot(y='Created Date')
LinkNYCpLagQ10 = ps.Quantiles(LinkNYCpLag,k=5)
plt.hist(null_vals); $ plt.axvline(obs_diff,c='red')
b_cal.available.unique()
candidate_gender = pd.Series(gender_top.candidate_id, index=['Female', 'Male', 'Unknown'], name='Gender in Company') $ plt.rcParams['patch.edgecolor'] = 'white' $ candidate_gender.plot.pie(figsize=(6, 6), colors=['lightskyblue', 'peru', 'skyblue'], autopct='%.2f%%')
df_user_engagement = pd.read_csv('takehome_user_engagement.csv')
df2.head()
p_diff_orig = df[df['landing_page'] == 'new_page']['converted'].mean() -  df[df['landing_page'] == 'old_page']['converted'].mean() $ print('p_diff from ab_data.csv :: ',p_diff_orig)
pd.unique(tag_df.values.ravel())
frame2['debt']=16.5
all_noms[(all_noms["agency"] != "Foreign Service") & (all_noms["confirmed"] == "yes")]["nom_count"].sum()
s.resample('Q', closed='left').head()
df_train['totals.newVisits_bool'] = [ bool(c==1) for c in df_train['totals.newVisits']] $ df_test['totals.newVisits_bool'] = [ bool(c==1) for c in df_test['totals.newVisits']]
df['statement_context'].value_counts()
dtrain = xgb.DMatrix(X_resampled, label=y_resampled, feature_names=feature_names) $ dval = xgb.DMatrix(X_val, label=y_val, feature_names=feature_names) $ dtest = xgb.DMatrix(test_features, label=test_target, feature_names=feature_names)
food["created_date"].head()
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'CA']]) $ results = mod.fit() $ results.summary()
w = 'hitler' $ model.wv.most_similar (positive = w)
ROI_per_chanel_path =  output_path + '\\ROI_per_chanel.csv' $ ROI.to_csv(ROI_per_chanel_path)
test_x = test_encodedlist $ test_y = list(youTubeTitles.values[2500:3000,1]) + list(pornTitles.values[2500:3000,1]) $ print np.asarray(test_x).shape $ print np.asarray(test_y).shape
dates = [pd.datetime(2012, 5, 1), pd.datetime(2012, 5, 2), pd.datetime(2012, 5, 3)]
df3.info()
data2_df = pd.concat([start_df, pd.get_dummies(start_df.UserType)], axis=1) $ data2_df = pd.concat([data2_df, pd.get_dummies(start_df.Season)], axis=1)
arima12= ARIMA(dta_713,[2,2,0],freq='Q').fit() $ arima12.summary()
query = pgh_311_data_merged['Category'] == "Road/Street Issues" $ pgh_311_data_merged[query]['Issue'].value_counts(ascending=True).plot.barh(figsize=(10,10))
import numpy as np $ import pandas as pd $ import json $ import psycopg2 as pg2 $ from psycopg2.extras import RealDictCursor, Json
for instance in session.query(User).order_by(User.id): $ ...     print(instance.name, instance.fullname)
distance = pd.Series(distance_list)
print('What items are in Store 1:\n', store_items.loc[['store 1']])
df.sum()
sp['day_ago'] = sp.Date.shift(periods=1) $ sp['week_ago'] = sp.Date.shift(periods = 7)
missing_values = np.sum(avi_data.isna()) $ percent_missing_values = np.round((missing_values/avi_data.shape[0])*100,2) $ missing_data = np.vstack((missing_values.values,percent_missing_values)) $ pd.DataFrame(data = missing_data.T,index=missing_values.index, columns = ['absolute_no','percentage %'])
plt.rcParams['axes.unicode_minus'] = False $ dta_53.plot(figsize=(15,5)) $ plt.show()
df.to_csv('clean_df.csv')
from scipy.stats import norm $ norm.ppf(0.95) $
from sklearn.model_selection import KFold
' '.join(sorted([n for n in names if len(n)<7 and len(n)>2]))
df.iloc[40:90].head()
    modules = Series(index=Index(Path(deathbeds.__file__).parent.glob('*.ipynb')));
pd.merge(df1,df2)
carprice={} $ for brand in top20.index: $     price=autos[autos["brand"]==brand]['price'].mean() $     carprice[brand]=price $ print(carprice)   
percentage_open_by_no_emails = train.groupby('no_of_emails')['is_open'].agg(np.mean) $ percentage_open_by_no_emails
df_merge = campaign_data.merge(train, on = 'campaign_id') $ campaigns = df_merge.groupby('campaign_id')['is_open','is_click'].agg(np.mean) $ campaigns['click_given_open'] = campaigns['is_click'] / campaigns['is_open'] $ campaigns['is_open'].sort_values(ascending = False)[0:5]
ghana.index = ghana['GMT']
data = pd.read_csv('../valencia-data-projects/valenbisi/vb_data/data.csv')
max_day = df.created_at.dt.date.value_counts().sort_values(ascending=False).index[0] $ max_day_beers = df.created_at.dt.date.value_counts().sort_values(ascending=False).values[0] $ max_day_string = max_day.strftime('%B %d, %Y') $ print('On {} I drank a whopping {} beers! Let\'s hope they were just samplers!'.format(max_day_string, max_day_beers))
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)])
smo = SMOTE(random_state=0) $ X_resampled, y_resampled = smo.fit_sample(X_train, y_train)
df = pd.merge(left=df,right=df_parcel,how='left',left_on='Parcel Status Id',right_on='id',validate="many_to_one") $ df = pd.merge(left=df,right=df_methods,how='left',left_on='Shipping Method Id',right_on='id',validate="many_to_one")
recommend = survey[survey.question.str.contains('On a scale of one to ten, how likely are you to recommend this to your friends?')].copy()
print final0.shape $ print final0.drop(duplicate_id).shape $ final=final0.drop(duplicate_id)
len(kochdf.loc[kochdf['date'] == max_date])
geo_file_name = "geo_segments_" + str(time_hour_for_file_name) + ".geojson"
for cd in root.findall('CD'): $     title = cd.find('TITLE').text $     artist = cd.find('ARTIST').text $     country = cd.find('COUNTRY').text $     print('{}\t{}\t{}'.format(artist, country, title))
from classiflib.container import ClassifierContainer $ retrained_classifier_loc = glob.glob(os.path.join(report_db_location, 'R1387E_catFR5_all_retrained_classifier*'))[0] $ classifier_container = ClassifierContainer.load(retrained_classifier_loc) $ classifier_container.features.shape
def cosine_similarity(table): $     from sklearn.metrics.pairwise import cosine_similarity $     sample_pivot_table[np.isnan(table)] = 0 $     user_sim = cosine_similarity(table) $     return user_sim $
title = soup.find('div', class_='content_title').text $ news_p = soup.find('div', class_="rollover_description_inner").text
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop =  pd.read_pickle('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p')
brandValues= addOne \ $     .reduceByKey(lambda x, y: (int(x[0]) + int(y[0]), \ $     x[1] + y[1])) $ print (brandValues.collect()) $
train.groupby('campaign_id')['day_of_week'].unique()
log_mod2 = sm.Logit(df3['converted'], df3[['intercept','ab_page', $                                            'country_UK', 'country_US','us_new','uk_new']]) $ results2 = log_mod2.fit()
movie=pd.DataFrame({'Date':mdate,'Name':mname,'Point':mpoint}) $ import numpy as np $ munique=pd.pivot_table(movie,index=['Name'],aggfunc=np.sum) $ mbest=munique.sort_values(by='Point',ascending=False) $ mbest.head()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=20000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
start = "2005-04-01" $ end = "2018-06-31" $ trading_days = fk.get_monthly_last_trading_days(start=start, end=end) $ trading_days = sorted(trading_days)
breakdown[breakdown != 0].sort_values().plot( $     kind='bar', title='Russian Trolls Number of Links per Topic' $ );
old_page_converted = np.random.choice([1,0], size = n_old, p = [p_old, 1-p_old]);
(df_final[df_final['Scorepoints'] == 150]).head()
s2 = pd.Series([10,100,1000,10000],subset.index) $ s2
s_mean_df["avg_prcp"].describe()
Test.ALLOFF()
datatest.loc[datatest.expenses >50000,'expenses'] = np.NaN
twitter_dataset=pd.merge(tweet_data_copy,image_copy,on='tweet_id',how='inner') $ twitter_dataset=pd.merge(twitter_dataset,df_copy,on='tweet_id',how='inner')
honeypot_df['ts_unix'] = honeypot_df['time'].apply(lambda x: pd.to_datetime(x).timestamp())
contractor_merge['month_year'].tail()
df1.rdiv(1)
breed_predict_df.info()
df_unique = pd.DataFrame(unique_feature_list)
from_api = api_clean.copy()
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt" $ mydata = pd.read_table(path, sep= '\t') $ mydata.head(5)
created = pd.tslib.Timestamp(pd.datetime(2017, 7, 15, 1, 3, 59))
df_clean.loc[mask, 'name'] = 'None'
df.loc['1998-09-10':'1998-09-15']
obs_diff = np.mean(df.query('group == "treatment"')['converted']) - np.mean(df.query('group == "control"')['converted']) $ obs_diff
df = df.drop(['nrOfPictures','postalCode','abtest'], axis=1)
df["past_percent_cancelled"] = df["past_percent_cancelled"].fillna(df["cancelled"].mean())
daily_rank = functions.rank() \ $                       .over(daily_window) \ $                       .alias('rank')
crime_gpd = gpd.read_file('phillydata/Crime.geojson') $ print(crime_gpd.shape) $ crime_gpd.head().T
X= df_loantoVal.drop(['First_Payment_Date' ,'Maturity_Date','LoanToValue'], axis=1) $ y = df_loantoVal['LoanToValue']
invoice_hub.drop(['deleted_at','fk_s_change_context_id_cr', 'fk_s_change_context_id_dl','fk_x_billing_account_hub_id', 'fk_x_subscription_hub_id', 'sk_id'],axis = 1, inplace = True)
prob_treatment = df2.query('group == "treatment"').converted.mean() $
df.query("group == 'treatment' and landing_page != 'new_page'").shape[0] + df.query("group != 'treatment' and landing_page == 'new_page'").shape[0]
categorised_df.head(5)
pprint(q_multi.metadata(reload=True))
santos_tweets['date'] = pd.to_datetime(santos_tweets['date'])
r6s = r6s[~r6s['selftext'].isin(['[removed]','[deleted]'])] $ r6s = r6s.dropna(subset=['title','selftext']) # drop when the content is null $ r6s = r6s[r6s['selftext'].str.len() > 50] $ r6s.shape
g_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ p_diff = np.array(p_diff) $ (g_diff < p_diffs).mean() $
s = df_usa.idxmax() $ r = df_usa[df_usa['NRI']>0.1].idxmin()   #finding the min and max values $ print(s['NRI']) $ print(r['NRI'])
max_calc = df.loc[df["station"] == "USC00519397"].groupby('station').max() $ min_calc = df.loc[df["station"] == "USC00519397"].groupby('station').min() $ mean_calc = df.loc[df["station"] == "USC00519397"].groupby('station').mean() $
(df_res.style $     .applymap(color_negative_red, subset=['total_score']) $     .set_table_styles(styles))
data.to_csv("csvs/datosSinDuplicados.csv", index = False)
USvideos.head(10) #first 10 rows
r=session.get('https://reddit.com') $ for html in r.html: $     print(html)
df = df.reindex(np.random.permutation(df.index)) $ df.head(2)
len([earlyScr for earlyScr in SCN_BDAY_qthis.scn_age if earlyScr < 3])/SCN_BDAY_qthis.scn_age.count()
processed_tweets_with_obs['flt_ob'] = processed_tweets_with_obs['flt_ob'].apply(lambda x: (x - 32) * (5/9))
rnd_search_cv.best_estimator_
df.boxplot('MeanFlow_cms',by='status');
dollars_per_unit.columns = pd.MultiIndex.from_product([['Dollars per Unit'], dollars_per_unit.columns]) $ pd.concat([multi_col_lvl_df, dollars_per_unit], axis='columns').head(3)
(df.xs(symbol,level='symbol')['2011':].flow*100.).rename(symbol).plot.hist(bins=61, $ title='{}: Daily Change in Shares (%)'.format(symbol),figsize=(10,4),xlim=(-10,10),alpha=0.75)
(null_vals > act_diff).mean()
df[['Footnote']].isnull().sum() $
autos['unrepaired_damage'].value_counts()
soup = BeautifulSoup(page, 'html.parser')
scaled['Gain +1d'] = scaled['Close -1d'].shift(-1) < 1.0 $ scaled.head()
df.info()
tweet_archive_enhanced_clean.loc[2338,'text']
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
monte.str.findall(r'^[^AEIOU].*[^aeiou]$')
s = pd.Series(['A','B','C',np.nan,'Dog']) $ s.str.lower()
browser_dummy = pd.get_dummies(fraud_data_updated['browser']) $ fraud_data_updated = pd.concat([fraud_data_updated,browser_dummy],axis=1)
(autos["date_crawled"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index() $ )
results = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ for row in results: $     print(f"The most recent date recorded is {row}.")
months= austin['yr_mo'].value_counts().sort_index()
mention_pairs[["FromType","FromName","Edge","ToType","ToName","Weight"]][mention_pairs["Weight"]>1].to_csv("For_Graph_Commons2.csv",index=False)
cnn_fox = [] $ cnn_fox.append(cnn_data) $ cnn_fox.append(fox_data) $ cnn_fox_data = pd.concat(cnn_fox)
path = pathlib.Path('deep-review-stats.json') $ path.write_text(stats_str)
bs = pd.Series(range(0,10)) $ bs
row_df.to_excel("comparison.xls")
left = pd.DataFrame({'key': ['yes', 'no'], 'lval': [7, 2]}) $ left
vocab = np.array(vectorizer.get_feature_names())
adopted_cats.columns
all_topics_words = lda.print_topics(num_topics=lda.num_topics) $ all_topics_words
contractor_merge['contractor_bus_name'] = contractor_merge['contractor_bus_name']+" - "+contractor_merge['contractor_number'].astype(str)
f = np.dot(X, w) # np.dot does matrix multiplication in python
ax = perf.prices.to_drawdown_series().plot(subplots=True, color='blue',figsize=(16, 10))
%%timeit -n 100 $ summary = ser7.sum()
lm = sm.OLS(df_new['converted'],df_new[['a/b_page','uk_intercept']]) $ result = lm.fit() $ result.summary()
df_tot.replace('-', np.nan, inplace=True)
df = pd.read_excel('accounts-annotations.xlsx', encoding='cp1252')
saying = ['After', 'all', 'is', 'said', 'and', 'done', ',', $           'more', 'is', 'said', 'than', 'done', '.'] $ for word in saying: $     print(word, '(' + str(len(word)) + '),', end=' ')
twitter_archive_master['Rating'] = twitter_archive_master['rating_numerator'].astype(str) + '/' + twitter_archive_master['rating_denominator'].astype(str)
ValidPrediction=model.predict_classes(VXcnn)
a = np.cos(np.pi) $ a # will be printed as a default in Jupyter without using a `print` command. 
learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn) $ learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ learn.clip=25. $ learn.metrics = [accuracy]
dates = ["date_crawled","ad_created","last_seen"] $ autos[dates].head(10) $ autos["date_crawled"].str[:10].value_counts(normalize=True,dropna=False).sort_index().head(100) $ autos["ad_created"].str[:10].value_counts(normalize=True,dropna=False).sort_index().head(100) $ autos["last_seen"].str[:10].value_counts(normalize=True,dropna=False).sort_index().head(100) $
filtered_df.dropna(axis=0,subset=[['name','summary']],inplace=True) $ values = {'host_is_superhost':'f','host_has_profile_pic':'f','host_identity_verified':'f','host_listings_count':1,'bathrooms':1,'bedrooms':1,'beds':1} $ filtered_df.fillna(value = values, inplace=True) $ print (filtered_df.apply(num_missing, axis=0))
df_CLEAN1A.tail()
temp_mean.plot(kind='bar')
pop.loc['California': 'New York']
df.fillna(method='pad')
df_max.index.name = 'month'
reviewsDF.head()
cmap = plt.get_cmap('Set3') $ colors = [cmap(i) for i in np.linspace(0, 1, df['Year'].nunique())]
dat[dat.ward.isnull()]
grouped = df.groupby('Year') $ for name,group in grouped: $     print(name) $     print(group)
data_2017_12_14_iberia = data_2017_12_14.loc[data_2017_12_14.text_2.str.contains("iberia", case = False, na = False)]
with open('data/data_complex.txt', 'r') as f: $     data = f.readlines() $ for line in data: $     print(line)
pd.merge(left=users,right=sessions,how="inner",left_on=['UserID','Registered'],right_on=['UserID','SessionDate'])
type.__new__(LetsGetMeta,'S',(),{})
df.loc[df['names'] == 'Nameless', 'names'] = None $ df.loc[df['breed'] == 'Unidentifiable', 'breed'] = None $ df.loc[df['dog_type'] == 'None', 'dog_type'] = None $ df.loc[df['rating'] == 0.0, 'rating'] = np.nan $ df.loc[df['confidence'] == 0.0, 'confidence'] = np.nan
result_set=session.query(Adultdb).filter_by(relationship='Not-in-family').all()
data2['lat'] = data2.coordinates.apply(lambda x:x['coordinates'][1]) $ data2['lon'] = data2.coordinates.apply(lambda x:x['coordinates'][0])
model=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results=model.fit() 
df2_dummy.tail(1)
df.shape[0]
merged_predictions_archive = pd.merge(left=twitter_archive_clean, right=most_confident_predictions, how='inner', on=['tweet_id']).copy()
pd.value_counts(ac['Status'].values, sort=True, ascending=False)
Google_stock.head() $ Google_stock.tail() $
prediction_and_counts.head()
df_users_first_transaction=users.merge((transactions.groupby('UserID').first()), how ="left", on="UserID") $ df_users_first_transaction
to_be_predicted_Day5 = 48.74674055 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
learner.fit(3e-3, 1, wds=1e-6, cycle_len=10)
df1.loc[df1['ENTRY_DIFFS'] < 0, 'ENTRY_DIFFS'] = 0 $ df1.loc[df1['ENTRY_DIFFS'] > 100000, 'ENTRY_DIFFS'] = 0 $ df1.loc[df1['EXIT_DIFFS'] < 0, 'EXIT_DIFFS'] = 0 $ df1.loc[df1['EXIT_DIFFS'] > 100000, 'EXIT_DIFFS'] = 0
df_mes['DOLocationID'].unique().shape
resid = (y-f) $ E = np.dot(resid.T, resid) # matrix multiplication on a single vector is equivalent to a dot product. $ print("Error function is:", E)
plt.figure(figsize=(8, 5)) $ plt.hist(train_df.views_lognorm); $ plt.title('The distribution of the property views_lognorm');
twitter_archive[twitter_archive.text.str.contains(r'(\d+(\.\d+))\/(\d+)')]
log_loss(y_test, clf.predict_proba(X_test))
cat_pizza.head(2)
df = pd.DataFrame({"accuracy": {"baseline": acc_baseline, "embedding": acc_embedding}}) $ plt.rcParams['figure.figsize'] = (7,4) $ df.plot(kind="bar")
print '{} cases where district did not provision any accounts, assume deployed but no usage'.format(len(df[df.install_rate==0]))
df_precep_dates_12mo.set_index('date')
test_data = pd.read_csv("trainingandtestdata/testdata.manual.2009.06.14.csv", encoding="latin1", names=colnames)
def find_DRGs_in_given_year(year): $     idx = df_providers[(df_providers['year']==year)].index.tolist() $     list_DRG_year = df_providers.loc[idx,'drg'].unique() $     return list_DRG_year
df2[df2['landing_page'] == 'new_page']['user_id'].count() / df2.shape[0]
sites.shape
gbm_v1 = H2OGradientBoostingEstimator( $     model_id="gbm_covType_v1", $     seed=2000000 $ ) $ gbm_v1.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)
df_titanic_temp.age = categories $ df_titanic_temp.head()
m_ms = df_ms[df_ms['hour'] < df_ms['hour'].median()] $ a_ms = df_ms[df_ms['hour'] >= df_ms['hour'].median()] $ m_hc = df_hc[df_hc['hour'] < df_hc['hour'].median()] $ a_hc = df_hc[df_hc['hour'] >= df_hc['hour'].median()]
LUM.plot_time_diff(all_lum_binned,subject="VP3")
nf = pd.read_csv('https://bit.ly/2mFxANJ') $ nf.head(2)
history.to_pickle('../data/merged_data/history.pkl')
f_ip_os_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_os_hour_clicks"), header=True) $ print('Found %d observations.' %f_ip_os_hour_clicks.count())
pd.Timedelta('2D 3H')
df2 = df2.drop('control', axis=1)
import statsmodels.api as sm $ convert_old = df2.query('group=="control" & converted==1')['converted'].count() $ convert_new = df2.query('group=="treatment" & converted==1')['converted'].count() $ n_old = 145274 $ n_new = 145310
float(0.0)
train_body_raw = traindf.body.tolist() $ train_title_raw = traindf.issue_title.tolist() $ train_body_raw[0]
frankfurt.info()
com311.head()
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"C":["C1", "C2"], $                     "D":["D1","D2"]},index=[1,2]) $ pd.concat([df1,df2], axis=1)
df.describe(include=['object'])
for n in [tup[0] for tup in term_tops]: $     print (lda.print_topic(n)) $     print ("")
df_goog.sort_values(['Date'])
new_page_converted = np.random.choice([0,1], size=145310, p=[1-0.1196, 0.1196]); new_page_converted
to_be_predicted_Day5 = 14.85524742 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
sm.Logit(complete_df['converted'], complete_df[['intercept', 'ab_page', 'CA', 'UK']]).fit().summary()
df = pd.DataFrame(historic_data) $ df
new_page_p = new_page_converted.mean() $ old_page_p = old_page_converted.mean() $ new_page_p - old_page_p
cfModel.summary()
display(HTML("<style>.cell { font-size: 12px; width:900px }</style>")) $ display(HTML("<style>.input { margin-top:2em, margin-bottom:2em }</style>")) $
fouls_df.info()
print(len(autos.groupby('name').size()))
%matplotlib inline $ import matplotlib.pyplot as plt
final['state'][final['area (sq. mi)'].isnull()].unique()
stats.norm.cdf((active_marketing - inactive_marketing) / SD_marketing)
p_diffs=[] $ for _ in range(10000): $     s_new_t=np.random.binomial(1,pnew,nnew).mean() $     s_pold_t=np.random.binomial(1,pold,nold).mean() $     p_diffs.append(s_new_t-s_pold_t)
raw_data = raw_data.loc[raw_data['date'] >= pd.to_datetime(start_date)] $ raw_data = raw_data.loc[raw_data['date'] <= pd.to_datetime(end_date)]
def comment_count(df): $     return df.set_index('datetime').resample('D').size() $ ltccomment['numcomments'] = comment_count(ltc) $ xrpcomment['numcomments'] = comment_count(xrp) $ ethcomment['numcomments'] = comment_count(eth) $
from scipy.stats import norm $ norm.cdf(z_score)
x_normalized.columns = normalized_columns
df1 = df[df.isnull().any(axis=1)] # axis=1 specifies rows instead of columns $ df1.shape
pd.pivot_table(train, index=['srch_destination_id'], columns=['hotel_cluster'], values='cnt', aggfunc='count', fill_value=0) $
auth = tweepy.AppAuthHandler(consumer_key, consumer_secret) $ auth.secure = True $ api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
got_sentiment = rdf_model.predict(Y_tfidf) $ got_sentiment_df = pd.DataFrame(got_sentiment) $ got_sentiment_df.columns = ['sentiment']
calls_df["call_day"]=calls_df["call_date"].dt.weekday_name $ calls_df["month_day"]=calls_df["call_date"].dt.day
autos["odometer_km"].describe()
print('The number of unique users is {}'.format(df.user_id.nunique()))
autos['date_crawled'].str[:10].describe()
xmlData['statezip'] = xmlData[['state', 'zipcode']].apply(lambda x: ''.join(x), axis = 1) $ xmlData.drop('state', axis = 1, inplace = True) $ xmlData.drop('zipcode', axis = 1, inplace = True)
stat_info_merge = pd.concat([stat_info[1], stat_info_st[[0,1]]], axis=1)
ventas_por_tipo = prop_caba_gba.groupby(['year','property_type']).count()['created_on']
mg_data = pd.concat([finnal_data.iloc[:,1:24],finnal_data.iloc[:,135:144]],axis = 1) $ spire_data = spire_event.iloc[:,6:] $ spire_data.head(1)
Train.describe()
for index, dtype in enumerate(df.dtypes): $     if dtype==np.int64 and df.columns[index] in x+y: $         df[df.columns[index]] = df[df.columns[index]].astype(float)
df2.info()
autos["date_crawled"].str[:10].\ $ value_counts(normalize=True,dropna=False).\ $ sort_index(ascending=True).describe()
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ the_files = [i for i in os.listdir() $              if i.endswith("Sepsis.pdf") and  i.startswith(starts_with_digit) and not i.startswith(today) ] $ the_files $
stats=[len(x) for x in results['users']] $ print min(stats) $ print max(stats) $ print np.median(stats) $ print np.mean(stats)
full.groupby(['Dx2'])['<=30Days'].agg({'sum':'sum','count':'count','mean':'mean'}).sort_values(by='mean',ascending=False)
tweet_2 = pd.read_json('tweet_json.txt') $
dfJobs.ix[4009]
log_reg_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'country_CA', 'country_UK', 'ab*CA', 'ab*UK']]) $ results = log_reg_mod.fit() $ results.summary()
bnbAx.country_destination.value_counts().plot.bar()
api.api2dataset(data, 'api-dataset.csv', ["", "Palavras-chave", "Nome do Arquivo"])
df5 = pd.DataFrame({'group': ['Accounting', 'Accounting', $                              'Engineering', 'Engineering', 'HR', 'HR'], $                    'skills': ['math', 'spreadsheets', 'coding', 'linux', $                              'spreadsheets', 'organization']}) $ display('df1', 'df5', "pd.merge(df1, df5)")
plt.figure() $ qplt.contourf(temperature_cube, 25) $ plt.gca().coastlines() $ plt.show()
import shapefile $ import numpy as np # Note: numpy is a part of Pandas pack: you can access numpy via pandas.np or pd.np $ sakhalin_shp = shapefile.Reader("shapefiles/sakhalin.shp")
company_count_df = pd.DataFrame.from_dict(company_count, orient='index') $ company_count_df.columns=['Count'] $ company_count_df.sort_values(by='Count', ascending=False).plot(kind='bar', figsize=(16,8), cmap='Set3') $
assert(tw[tw.text.str.startswith('RT @')].shape[0] == sum(tw.retweeted_status_id.notnull()))
plt.show()
iris.head().iloc[:,0].values
report_score(y_test, y_fit_proba, 0.3)
start_coord_list = station_distance['Start Coordinates'].tolist() $ end_coord_list = station_distance['End Coordinates'].tolist()
df.plot(x='playId', y='homeWinPercentage', figsize=(15,5))
len(data.created_at.unique())
result_concat = pd.concat([city_loc, city_pop]) $ result_concat
test = quotes_historical_yahoo_ochl( $     "AAPL", datetime.date(2016, 4, 1), datetime.date(2016, 4, 2))
meal_inferred_types_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meal_inferred_types.csv')['Body'].read().decode('utf-8') $ meal_inferred_types = pd.read_csv(StringIO(meal_inferred_types_csv_string), header=0, delimiter='|')
df.head(10)
newdf.drop(newdf.columns[0],axis=1,inplace=True)
g_live_kd915_df = live_kd915_df.groupby(['blurb']) $ live_woutcome_unfilt = g_live_kd915_df.filter(lambda x: len(x) > 1).sort_values(['blurb','launched_at']) $ live_woutcome_unfilt[['name','category_name','blurb','blurb_count','goal_USD','backers_count','launched_at',\ $                'state_changed_at','days_to_change','state']]
ttTimeEntry.drop_duplicates(subset = mainkey + ['DT'], inplace = True)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0] $
stringlike_instance_3.content
df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique()
tweet_df_clean.info()
to_be_predicted_Day3 = 48.31596998 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
fields_frame.head(10)
exploration_titanic.numeric_summary() # you can access to numeric
df_wna.sort_values(by='n_faults').head(24)
channel_meta = yt.get_channel_metadata(channel_id) $ channel_meta
df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country']) $ df3 = df3.drop(df3['CA']) #or could also drop doing df3 = df3.drop('CA', axis=1)
closeSeriesQ = closingPrices.resample('Q-sep').mean()
df2.head()  # what the above did is just returned an object after dropping but change in actual object didn't take place
filtered_brewery[['beer_name', 'brewery_name', 'rating_score']][filtered_brewery.brewery_name == bottom_three[0][0]]
dates[dates.isin(search_dates)]
d + pd.tseries.offsets.DateOffset(months=4, days=5)
df.time.unique().shape
df = pd.DataFrame({'one' : pd.Series(np.random.randn(3), index=['a', 'b', 'c']), $     'two' : pd.Series(np.random.randn(4), index=['a', 'b', 'c', 'd']), $     'three' : pd.Series(np.random.randn(3), index=['b', 'c', 'd'])}) $ df
reliableData['ID'].unique()
twitter_ar.info()
from sklearn.naive_bayes import GaussianNB
df_time = df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ df_time
nold = len(df2.loc[(df2['landing_page'] == 'old_page')]) $ nold
pd.Timestamp(2018, 1, 1)
num_lines = access_logs_raw.count() $ num_lines
transactions['TransactionDate']=pd.to_datetime(transactions['TransactionDate']) $ transactions
%writefile ./test.json $ {"pickuplon": -73.885262,"pickuplat": 40.773008,"dropofflon": -73.987232,"dropofflat": 41.732403,"passengers": 2}
climate_df.plot(kind ="line", figsize=(8,6)) $ plt.yticks(np.arange(0, 4.5, step=0.5)) $ plt.legend(loc="upper left") $ plt.tight_layout() $ plt.savefig("hawaii_Date_Precipitation.png",bbox_inches="tight")
result.to_csv("gaurav3.csv", index=False)
subset = subset.filter(lambda p:\ $                        p["payload/info/subsessionLength"] is not None\ $                        and p["meta/Timestamp"] is not None\ $                        and p["meta/creationTimestamp"] is not None)
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new],[n_old,n_new], alternative = "smaller") $ print(z_score, p_value)
n_old = df2['group'].value_counts()[1] $ n_old $
f_counts_week_device = spark.read.csv(os.path.join(mungepath, "f_counts_week_device"), header=True) $ print('Found %d observations.' %f_counts_week_device.count())
test_sentence_stemmed = test_sentence.apply(lambda sequence: ' '.join(stemmer.stem(word) for word in sequence.lower().split())) $ test_sentence_stemmed[0]
net.add_nodes(pop_name='excitatory', $               ei='e', $               model_type='population', $               model_template='dipde:Internal', $               dynamics_params='excitatory_pop.json')
transactions.merge(users, how='left', on='UserID')
measure_avg_prcp_year_df.set_index('Date',  inplace=True) $ measure_avg_prcp_year_df.head() $
import pandas as pd $ import os.path as op
ssc.stop()
ride_kmeans = KMeans(n_clusters=num_ride_clusters, random_state=1).fit(ride_coords)
(df.doggo.value_counts(), df.floofer.value_counts(), df.pupper.value_counts(), df.puppo.value_counts())
active.head()
xgb_learner.params
features_df.tail()
guineaCases = guineaFullDf.loc['Total new cases registered so far'] $ guineaCases.head()
malemoon = pd.concat([moon, malebydatenew], axis=1) $ malemoon.head(3)
path = '/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data/csv_export' $ orgs = pd.read_csv(path + '/organizations.csv')
pred = pipeline2.predict(cvecogXfinaltemp) $ print classification_report(pred, ogy) $ print confusion_matrix(ogy, pred) $ print "cross val score accuracy :"+str(sum(cross_val_score(pipeline2, cvecogXfinaltemp, ogy))/3) $ print 'roc_auc score :'+str(sum(cross_val_score(pipeline2, cvecogXfinaltemp, ogy, scoring='roc_auc'))/3) $
dsg.vatts('u')
station_activity = session.query(Measurement.station, Station.name, func.count(Measurement.tobs)).\ $ filter(Measurement.station == Station.station).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all()
for drink in Beverage_name: $     new_data[drink] = 0 $     new_data.loc[new_data['drink1'].str.contains(drink), drink] = 1 $     new_data.loc[new_data['drink2'].str.contains(drink), drink] = 1 $     new_data.loc[new_data['drink3'].str.contains(drink), drink] = 1
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.country.value_counts()
data = [go.Scatter(x=a_df.index,y=a_df["Crime Count"])] $ plotly.plotly.iplot(data)
data['cat'].value_counts()
pickle_full = "Data Files/sorted_stays.p"
np.exp(-0.0333)
pos_data_train = data_train.query("rating >= 4") $ pos_data_test = data_test.query("rating >= 4")
tia['salary_clean'] = tia['salary'].apply(clean_salary) $ tia.head()
pred = clf.predict(x_test) $ print(metrics.accuracy_score(y_test, pred))
print item_sat.shape $ print v_item_sat.shape
BLINK.plot_duration(blink)
df.info()
merged1 = pd.merge(left=merged1, right=offices, how='left', left_on='OfficeId', right_on='id')
df_users.last_session_creation_time.isnull().values.ravel().sum()
nb_pipe_2.fit(X_train, y_train) $ nb_pipe_2.score(X_test, y_test)
CAres.summary()
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['landing_page'])['new_page'] $ df2.head()
autos.loc[autos.price<50.0,'price']=np.nan
segmentData.opportunity_amount.describe()
print(result.summary2())
df_NYPD = df[df['Agency Name'].isin(['New York City Police Department', 'NYPD'])]
merged1['AppointmentCreated'] = pd.to_datetime(merged1['AppointmentCreated'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d') $ merged1['AppointmentDate'] = pd.to_datetime(merged1['AppointmentDate'], errors='coerce')#.apply(lambda x: x.date()) #, format='%Y-%m-%d')
station_distance.iloc[:2, [0,3,6,7]] $ station_distance.iloc[:2, 0:4]
ts.resample("Min").sum() # 1 minute totals 
sdf_wide = sdf_resampled.groupBy("Start").pivot("Cantons").agg(avg("Sum Production")) $ sdf_wide.printSchema() $ sdf_wide.take(2)
monthly_tweets.groupby('source').sum()
fig, ax = plt.subplots(figsize=(10,8)) $ g = sns.distplot(condo_6.PRICE, rug=True, kde=True, ax=ax) $ t = g.set_title("Distribution of Sale Prices")
to_be_predicted_Day2 = 82.26004376 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
forked = repos[['owner_id', 'forked_from', 'created_at']] $ forked = forked.rename(columns={'owner_id': 'user_id', 'forked_from': 'repo_id'}) $ forked['forked'] = 4 $ forked.head() $ forked.info()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
vocabulary_expression['component_6'].sort_values(ascending=False).head(7) $
df2.drop(labels=1899, axis=0, inplace=True) $ df2[df2['user_id']==773192]
weekly = data.resample('W').sum() $ weekly.plot(style=[':', '--', '-']) $ plt.ylabel('Weekly bicycle count')
df3.head(3)
abc.to_csv('test example.csv')
plt.hist([guest['ama.week.diff'] for guest in ama_guests.values()]) $ plt.title("How many weeks previously was this AMA posted") $ plt.show()
x_scaled = min_max_scaler.fit_transform(x_normalized)
!head -n 10 p32cfr_results.txt
api_url_base = 'https://api.bilibili.com/x/article/archives?ids='
cityID = '00ae272d6d0d28fe' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lexington_Fayette.append(tweet) 
tmp["asset_name"] = [re.sub(".zip$", "", x) for x in tmp["data_asset_name"]]
month1 = oanda.get_history(instrument_1, $                           start = '2017-12-31', $                           end = '2018-2-1', $                           granularity = 'M10', $                           price = 'A')
obs_mean = df2.converted.mean() $ obs_mean
y_train_est_probs = model.predict_proba(X_train)[:,0]
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer         # import vader for sentiment analysis $ analyzer = SentimentIntensityAnalyzer()
for n in t: $     rec, spec, sp_re, coefs, ypred = eval_fits_by_thresh(clf, sc, Xholdout, yholdout, n) $     recs.append(rec) $     specs.append(spec) $     sp_rec.append(sp_re)
print(sentiments['comments'].iloc[3])
dataset.to_csv(output_path, sep=",")
link = 'http://eforexcel.com/wp/wp-content/uploads/2017/07/1500000%20Sales%20Records.zip'
null_vals = np.random.normal(0, np.std(p_diffs), p_diffs.size) $ p_null = df2.query('landing_page == "new_page"').converted.mean() -\ $ df2.query('landing_page == "old_page"').converted.mean() $ plt.hist(null_vals); $ (null_vals > p_null).mean() $
hm_clean.info()
df_archive_clean = df_archive_clean.drop(labels = errors_index,axis = "index")
titanic_dummy_df.head()
def top_n_tweet_days(df,n): $     return df.head(n) $ top_20_tweet_days = top_n_tweet_days(dates_by_tweet_count,20)
df_concensus_uaa.head()
bg2 = pd.read_csv('Libre2018-01-03.txt', sep='\t', nrows=100) # local $ print(bg2) $ print(type(bg2))
df_columns[df_columns['Complaint Type'] == 'Noise - Residential'].index.month.value_counts().sort_index().plot() $
finals['type'].value_counts()
old_page_converted=np.random.binomial(1,pold,nold) $ s_pold=old_page_converted.mean()
df_ad_airings_5['location'].unique()
transactions.merge(users, how='outer', on='UserID')
all_news.to_pickle("all_news.pkl")
non_usa_states = ['ON', 'AP', 'VI', 'PR', '56', 'HY', 'BC', 'AB', 'UK', 'KA'] $ print 'Total amount for locations outside USA: ', sum(df[df.state.isin(non_usa_states)].amount) $
test['loan_result'].cbind(pred).head(rows=3)
print(type(r.json())) $ json_dict = r.json() $
df.drop('min', axis=1, inplace=True) $ df.drop('max', axis=1, inplace=True) $ df
corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tf.mm'), corpus_tf) $ corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tfidf.mm'), corpus_tfidf)
from sklearn.preprocessing import OneHotEncoder  $ encoder = OneHotEncoder(sparse=False)  $ y_onehot = encoder.fit_transform(y)  $ y[0], y_onehot[0,:] $
df.tail(10)
mean_mileages = pd.Series(mileages_by_brand).sort_values(ascending=False) $ mean_prices = pd.Series(price_by_brand).sort_values(ascending=False) $ brand_info = pd.DataFrame(mean_mileages, columns=['mean_mileage']) $ brand_info['mean_price'] = mean_prices $ brand_info
d_utc.tz_localize('US/Eastern')
X = reddit_master['title'] $ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0) $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
store_items = store_items.drop(['watches', 'shoes'], axis=1) $ store_items
taxa_count=grouped_by_plot['taxa'].nunique() $ taxa_count.head()
df2 = df.drop(df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')].index) $ df2 = df2.drop(df2[(df2['group'] != 'treatment') & (df2['landing_page'] == 'new_page')].index) $ df2.head()
df2['intercept']=1 $ df2[['ab_extra','ab_page']]=pd.get_dummies(df['group'])#creating dummy varibles $ df2.drop(['ab_extra'],axis=1, inplace=True)#droping one of them for maintaining the rank $ df2.head()
df_term = pd.read_csv(f_term) $ df_term.head(10)
image_copy['p1']=image_copy['p1'].str.replace('_',' ') $ image_copy['p2']=image_copy['p1'].str.replace('_',' ') $ image_copy['p3']=image_copy['p1'].str.replace('_',' ')
pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
wrd_clean['puppo'].value_counts()[:10]
v_invoice_link.columns[~v_invoice_link.columns.isin(invoice_link.columns)]
tfidf_doctopic = clf.fit_transform(tf_idf)
df_json_tweets['tweet_id'] = df_json_tweets['tweet_id'].astype(str) $ df_json_tweets.info()
from sklearn.metrics import confusion_matrix
import pickle $ output = open('mentioned_bills.pkl', 'wb') $ pickle.dump(mentioned_bills_all, output) $ output.close()
sessions.dtypes
user = pd.read_csv("users.csv") $ user.head(3)
actual_payments['iso_date_next']=pd.to_datetime(actual_payments.iso_date)+pd.
X_all = df.iloc[:,41:82].values        # iloc == "integer locations" of rows/cols $ y_all = df[ 'beat' ].values    
b = x[x.Name == 'Grand Theft Auto V'] $ b[['Name', 'Platform','Global_Sales','_merge']]
from pandas.tseries.holiday import AbstractHolidayCalendar, Holiday, nearest_workday $ class myBirthDay(AbstractHolidayCalendar): $     rules = [ Holiday('Noushad Khan Birthday', month=8, day=20) ] $ myc = CustomBusinessDay(calendar = myBirthDay()) $ myc
df.select('b', b_threeish).show(5)
properati.country_name.value_counts(dropna=False)
topVideo_df = pivotTab_toDF(dataset = youtube_df, value="views", \ $                             indices = "title", aggregFunct = np.sum)[:10] $ print("Top 10 Youtube Videos:\n") $ topVideo_df
print('\n'.join((f"{key}, {type(sheet)!r}" for key, sheet in sheets.items())))
df.nunique()['user_id'] $
s_n_s_epb_two.index = s_n_s_epb_two.Date $ del s_n_s_epb_two['Date']
log_mod4 = sm.Logit(df3['converted'], df3[['intercept','ab_page','evening','eve_new']]) $ results4 = log_mod4.fit() $ results4.summary()
alldata.to_pickle('test')
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop_3.to_hdf('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.h5', key='df', mode='w')
sample_groups_category_count = sample_groups.groupby(['MSA_CODE', 'MSA_NAME', 'category.name']).count().reset_index() $ sample_groups_category_count.rename(columns={ 'id': 'count'}, inplace=True) $ group_category_counts_by_msa = sample_groups_category_count[ ['MSA_NAME', 'count', 'category.name']].groupby('MSA_NAME') $ group_category_counts_by_msa.apply(lambda x: x.sort_values('count', ascending=False).head(5)).drop('MSA_NAME', axis=1)
df_questions_read.head(3)
! python player_stats/team_results_scrape.py PHI
apple_tweets.to_pickle('../data/apple.pkl')
coefs.loc['age', :]
df['price'].fillna(df['price'].mean(),inplace = True)
for k in range(10): $     show_how_many_DRGs_received_payment(payments_all_yrs.loc[k,'name'],payments_all_yrs.loc[k,'year'],50)
prob_kNN100 = kNN100.predict_proba(Test) $ prob_kNN100
def filter_date(df, after, before): $     return df.ix[after:before] $ pricesLTC = filter_date(pricesLTC, '2016-01-01','2017-12-31') $ pricesETH = filter_date(pricesETH, '2016-01-01','2017-12-31') $ pricesXRP = filter_date(pricesXRP, '2016-01-01','2017-12-31')
df2_new['intercept'] = pd.Series(np.zeros(len(df2_new)), index=df2_new.index) $ df2_new['ab_page'] = pd.Series(np.zeros(len(df2_new)), index=df2_new.index)
sorted_unigram = sorted(unigrams_fd.items(), reverse = True, key=lambda x: x[1])
df.query("group == 'treatment' and landing_page == 'old_page'").count()[0] + df.query("group == 'control' and landing_page == 'new_page'").count()[0]
df_archive_clean["in_reply_to_user_id"] = df_archive_clean["in_reply_to_user_id"].astype(str) $ row_drop = df_archive_clean[df_archive_clean["in_reply_to_user_id"]!="nan"]["tweet_id"].index $ df_archive_clean.drop(row_drop, inplace=True) $ df_archive_clean["in_reply_to_user_id"].replace("nan", np.nan, inplace=True)
close_arr = aapl['close'].values
new_page_converted = np.random.binomial(1, p_new, Nnew) $ print(new_page_converted)
tweets['fixed'] = [ftfy.fix_text(tweet) for tweet in tweets['text'].values.tolist()]
convo1 = companyNeg[companyNeg.group_id == 'group_98805' ]
filled = summed.fillna(summed.mean()) $ filled
amts = amts.rename(columns={'LOAN_AMOUNT':'LOAN_AMOUNT_'}) $ extract_deduped_with_elms_v2 = extract_deduped_with_elms.merge(amts, how='left', on='ACCOUNT_ID') $ extract_deduped_with_elms_v2['LOAN_AMOUNT'] = [lamt if str(lamt) not in ['nan','(null)',''] else lamt_ $                                                for (lamt, lamt_) in zip(extract_deduped_with_elms_v2.LOAN_AMOUNT, $                                                                         extract_deduped_with_elms_v2.LOAN_AMOUNT_)]
bnbAx[bnbAx['language']=='en'].country_destination.value_counts().plot.pie()
fit = sm.tsa.ARIMA(df[y], (1,0,1), exog = exogx).fit()
df_ab_raw.user_id.nunique()
new_eng = pd.merge(df_user_engagement,df_eng_mult_visits, on="user_id") $
sns_path = "../Datasets/Beat_Data/stop_search_beat.csv" $ s_n_s_df =  pd.read_csv(sns_path)
sns.boxplot(x = 'loan_status', y = 'loan_amnt', data=train);
lr=np.array([1e-4,1e-3,1e-2])
df.sort_values('fatalities',ascending=False).head(5) #top 5 fatalities
df_new.sample(5)
education_data.index=education_data['Category']
r = r[r.duration < 3000]
df_repub_2016.head(100)
samples_query.execute_sql("CALL Sample.Person_Extent") $ samples_query.display_records(5)
support.sort_values("amount", ascending=False).head()
fig = plt.figure(figsize=(10,10)) $ ax = fig.add_subplot(111) $ ax.axis('off') $ pumashp.plot(edgecolor='black',color='white',linewidth=2.0,ax=ax)
sample_x1,sample_y1,sample_value = testflow.next()['data']
df2_converted = df2["converted"].mean() $ df2_converted #roughly equal to no_of_unique_converted_users calculated above
with open(PredClass.save_model, 'wb') as w: $     pickle.dump(reg_final, w)
converted_group = df2.query('converted == 1') $ print(converted_group.user_id.nunique() / df2.user_id.nunique())
format_latest_date = dt.strptime(latest_date,"%Y-%m-%d")
topics_data.author = topics_data.author.apply(lambda x: str(x)) $ topics_data.flair_text = topics_data.flair_text.apply(lambda x: str(x)) # convert columns to string type
data = data[data['Tag'].isnull() != True]
example['hour'] = example.index.hour $ example.head(5)
cohort_retention_df.fillna(0,inplace=True)
df2[df2['group']=='treatment'].converted.mean()
df.tail()
df['favorite_count'].describe()
data['data'].keys()    # getting the data Keys
df2.user_id.nunique(), df2.shape
df.to_feather(f'{PATH}df')
import pandas as pd $ df_resolved_links = pd.DataFrame(resolved_links) $ df_resolved_links.tail(2)
week49 = week48.rename(columns={343:'343'}) $ stocks = stocks.rename(columns={'Week 48':'Week 49','336':'343'}) $ week49 = pd.merge(stocks,week49,on=['343','Tickers']) $ week49.drop_duplicates(subset='Link',inplace=True)
predictors = titanic.drop('survived', axis = 1).values $ predictors
tsd = gcsfs.GCSFileSystem(project='inpt-forecasting') $ with tsd.open('inpt-forecasting/Transplant Stemsoft data -040117 to 061518.xlsx') as tsd_f: $   tsd_df = pd.read_excel(tsd_f)
dot.attr('node', fillcolor='lightgreen', shape='oval')
twitter_archive_master.loc[(twitter_archive_master.name == 'a'),'name'] = 'Alpha' $ twitter_archive_master.loc[(twitter_archive_master.name == 'his'),'name'] = 'His'
kimanalysis.getitem('LammpsExample__TD_567444853524_004')
evaldf['inspection_result'].value_counts()
lr_cv.best_params_
p_treatment_converted = df2.query('group == "treatment" & converted == 1').count()['user_id']/df2.query('group == "treatment"').count()['user_id'] $ print('The probability of someone converting given that an individual was in the treatment group, disregaring the page he/she visited is {:2f}'.format(p_treatment_converted)) $
data.plot() $ plt.ylabel('Hourly Bicycle Count')
print("Number of Mitigations in ATT&CK") $ print(len(all_attack['mitigations'])) $ mitigations = all_attack['mitigations'] $ df = json_normalize(mitigations) $ df.reindex(['matrix','mitigation', 'mitigation_description','url'], axis=1)[0:5]
unemp.columns = ['quarter', 'rate']
def lemmatiz(comment): $     return [lemmatizer.lemmatize(w) for w in comment.split(' ')]
for col in ["power", "toughness", "loyalty"]: $     all_cards.loc[:, col] = all_cards.loc[:, col].infer_objects()
date.strftime("%A")
m = Base.classes.measurement $ s = Base.classes.station $
plt.bar(icecream['Flavor'],icecream['Price'])
print(contribs.head())
df['open'].describe()
pickle.dump(lsa_tfidf_df, open('iteration1_files/epoch3/lsa_tfidf_df.pkl', 'wb'))
df['encrypted_customer_id'].nunique()
steemit_urls = unique_urls[(unique_urls.domain == domain)] $ steemit_urls.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
g8_groups['area'].mean()
df_playlist_videos['channel_id'].unique()
merged2['AppointmentDate'] = pd.to_datetime(merged2.index,format='%Y-%m-%d')
json_data = r.json() $ json_limit = r_limit.json() $ pp.pprint(json_limit)
my_list = [0.25, 0.5, 0.75, 1.0] $ data = pd.Series(my_list) $ data
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] == 'treatment', 1, 0) $ df2.head()
df1 = pd.DataFrame(data01.load_data()) #load data into df $ df2 = pd.DataFrame(data02.load_data()) #load data into df
results_distributedTopmodel, output_DT = S_distributedTopmodel.execute(run_suffix="distributedTopmodel_hs", run_option = 'local')
display((p1 - actual_next_order).days) $ display((p2 - actual_next_order).days)
QUIDS_wide = QUIDS.pivot_table(index='subjectkey', columns = 'week', values='qstot') $ QUIDS_wide.reset_index(inplace=True) $ QUIDS_wide.columns $ QUIDS_wide.columns = ['subjectkey', 'qstot_0', 'qstot_12','qstot_14'] $
data.to_csv('TwitterSentimentData.csv')
washpark_matrixdf = pd.DataFrame(document_term_matrix.toarray(), $                                        index=important_tweets.index, $                                        columns=tfidf_vectorizer.get_feature_names())
df.tail()
df.apply(np.cumsum)
my_model_q6 = SuperLearnerClassifier(clfs=[clf_base_knn,clf_base_svc, clf_base_nb], stacked_clf=clf_stack_lr, training='label') $ my_model_q6.fit(X_train, y_train) $ y_pred = my_model_q6.predict(X_test) $ accuracy = metrics.accuracy_score(y_test, y_pred) $ print("Accuracy: " +  str(accuracy))
minimum = df['time_open'].min() $ print(minimum) $
shiftlog_entries_df.info()
print(y_test.mean(), y_train.mean())
zipcodesdetail.loc[(zipcodesdetail.city == 'Frisco') & (zipcodesdetail.state == 'TX') & (pd.isnull(zipcodesdetail.county)), 'county'] = 'Denton'
users = users.sort_values(by=['avg_score'], ascending=False) $ top_25_users = users.index[1:50].tolist()
plt.plot(close_series) $ plt.show()
from pixiedust.display import * $ display(data)
import locale $ import sys $ def p(f): $     print('%s.%s():%s'%(f.__module__,f.__name__,f()))
df.head()
ps0 = ps.drop(labels = ["Post patch?", "PCR cycles", "SM_QC_PF", "Bad dates"], axis = 1) $ ps0.head() $
data = pd.read_json('data/data.json')
from sklearn.metrics import confusion_matrix $ confusion_matrix = confusion_matrix(y_test, y_pred_lgr) $ print(confusion_matrix)
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(n_new, p_new) $     old_page_converted = np.random.binomial(n_old, p_old) $     p_diffs.append(new_page_converted/n_new - old_page_converted/n_old) $
model.add(Dense(3)) $ model.add(Activation('softmax'))
filing_ts_df.filings['2009':].plot(color='red') $ plt.title("Filings Per Day Over Time") $ plt.ylabel("# of filings") $ plt.show()
zipincome['ZIPCODE'] = zipincome['ZIPCODE'].astype(float)
autos.price.unique().shape
AppleURL = MakeYahooStockURL('AAPL', datetime.date(2016, 3, 1), datetime.date(2016, 3, 7))  # 7 days of data $ AAPL = pd.read_csv(AppleURL)
print 'Set date strings to be month or day first.  Month first (AMERICA) is default.' $ dti1 = pd.to_datetime(['8/1/2014']) $ dti2 = pd.to_datetime(['2/8/2014'], dayfirst=True) $ dti1[0], dti2[0]
ndf.shape
from bs4 import BeautifulSoup $ example1 = BeautifulSoup(df.text[279], 'lxml') $ print(example1.get_text())
train_df = pd.DataFrame(train_df_dict) $ test_df = pd.DataFrame(test_df_dict)
mean_sea_level = pd.DataFrame({"northern_hem": northern_sea_level["msl_ib(mm)"], $                                "southern_hem": southern_sea_level["msl_ib(mm)"], $                                "date": northern_sea_level.year}) $ mean_sea_level
df2_treatmentconverted = df2.query('group=="treatment"').converted.mean() $ df2_treatmentconverted
new_page_converted = np.random.binomial(n_new,p_new) $ print('The new_page_converted is: {}.'.format(new_page_converted))
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group']) $
archive_clean.loc[archive_clean['tweet_id'].isin(remove_list),:]
tree_features_df = pd.read_csv('../data/tree_images.csv', sep='|', index_col=0) $ tree_features_df.head()
matt1 = dta.t[(dta.b==40) & (dta.c==1)] $
walmart.asfreq('M', how='start') # We are changing the freq from quartely to monthly using the start date
df.tail(2)
train, test = dogscats_h2o.split_frame(ratios=[0.7])
mean_sea_level = pd.DataFrame({"northern_hem": northern_sea_level["msl_ib(mm)"], $                                "southern_hem": southern_sea_level["msl_ib(mm)"]}, $                                index = northern_sea_level.year) $ mean_sea_level
import pandas as pd $ import numpy as np $ import seaborn as sns $ import matplotlib.pyplot as plt $ from datetime import datetime
dfChile.shape
with pd.option_context('display.max_colwidth', 130): $     print(news_titles_sr)
df_r.head()
sentiments_vader = sentiments_pd['Tweet Polarity'].head(20) $ sentiments_vader
round(autos["odometer_km"].describe())
twitter_ks = df[(~df.twitter.isnull()) & (df.state == 'KS')] $ twitter_ks.twitter.unique()
print("Printing the vector of 'inc': {} ...".format(model['inc'][:10])) $ print("Printing the similarity between 'inc' and 'love': {}"\ $       .format(model.wv.similarity('inc', 'love'))) $ print("Printing the similarity between 'inc' and 'company': {}"\ $       .format(model.wv.similarity('inc', 'company')))
save_n_load_df(promo_df, 'promo_df4.pkl')
lsi_tf.show_topic(1)
stat_range = lambda x: x.max() - x.min() $ stats.apply(stat_range)
twitter_status = unique_urls[(unique_urls.domain == domain) & (unique_urls.url.str.contains('/status/'))] $ twitter_status.sort_values('num_authors', ascending=False)[0: 50][['url', 'num_authors']]
import spacy $ print('* Loading SpaCy "en_core_web_md" corpus...') $ nlp = spacy.load('en_core_web_md') $ print('* Success.') $ print('-'*116)
datetime_index = pd.to_datetime(df['created_at']).copy() # copy, because reference is deleted next $ df = df.set_index(datetime_index).drop('created_at', axis=1) # set index and drop redundant variable $ df.index.names = ['date_time'] # rename index $ df.head()
p_x = x.copy()
overall_ng         = all_text.sum().sort_values(ascending=False)[5:40] $ gucci_ng           = all_text[all_text.index == "gucci"].sum().sort_values(ascending=False).head(25) $ mcqueen_ng         = all_text[all_text.index == "Alexander McQueen"].sum().sort_values(ascending=False).head(25) $ stellamccartney_ng = all_text[all_text.index == "Stella McCartney"].sum().sort_values(ascending=False).head(25) $ burberry_ng        = all_text[all_text.index == "Burberry"].sum().sort_values(ascending=False).head(25)
b_vs_bshift = b.shift(1) != b $ fund_nr_coinswitches = b_vs_bshift.T.sum() $ fund_nr_coinswitches = fund_nr_coinswitches[1:] # delete first row
plt.hist(changediff[np.isfinite(changediff)].flatten(), 25, density=True) $ plt.show()
validation.analysis(observation_data, lumped_simulation)
projects = pd.DataFrame() $ for i in list(range(0, len(workspaces_list))): $     projects_list = toggl.request("https://www.toggl.com/api/v8/workspaces/" + str(workspaces_list[i]['id']) + "/projects") $     projects_df_temp = pd.DataFrame.from_dict(projects_list) $     projects = pd.concat([projects_df_temp, projects])
rollcorr_daily.plot() $ plt.show()
print('full: {} rows and {} columns'.format(*full.shape))
tweet.keys()
rng_berlin = rng_utc.tz_convert('Europe/Berlin')
sns.set()                                                                    # switches to seaborn default display
pd.reset_option("display.max_rows") $ pd.get_option("display.max_rows")
df_new = df2[(df2['landing_page'] == 'new_page')] $ converted = df_new['converted'] $ new_page_converted = np.random.choice(converted, n_new)
total_sales[['2015_q1_sales', '2016_q1_sales', 'bottles_total', '2015_q1_sales_bottlenorm']].corr()
gender_freq_hist = pd.crosstab(index=goodreads_users_df["gender"], columns="count") $ gender_freq_hist['gender_freq'] = gender_freq_hist['count'] * 100 / gender_freq_hist.sum()['count'] $ gender_freq_hist = gender_freq_hist.sort_values('gender_freq', ascending=False) $ gender_freq_hist.head(10)
with open('myfile.csv', 'r') as f: $     print(f.read())
merged = pd.merge(prop, contribs, on="calaccess_committee_id")
print(df.tail())
samples_query.columns
new_page_converted = np.random.choice([0, 1], num_new, p=[1 - null_p_new, null_p_new]) $ print(new_page_converted[:10])
ins2016.head()
p_diffs = [] $ new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation $ p_diffs = np.array(p_diffs)   
df['name'].value_counts()
fig = plot.get_figure()
[each for each in df2.columns if 'ORIGIN' in each.upper()]
percentage_loc = sum(df['Picture_Location'].isnull())/len(df) $ print("Percentage of non-empty location: " + "{0:.2f}%".format((1-percentage_loc) * 100))
cityID = 'c47c0bc571bf5427' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Honolulu.append(tweet) 
df_twitter_extract = pd.read_csv('data/df_twitter_extract.csv') $ df_twitter_extract_copy = df_twitter_extract.copy() $ df_twitter_extract_copy.head()
store_b_a_left = pd.merge(store_b, store_a, how='left') $ print(store_a_b_left) $ print(store_b_a_left)
df_tweets.reset_index().to_pickle("../tweets_extended.pkl")
X_test = test.drop(axis=1, labels='loan_status')
tesla['Close'].mean() # getting the mean by just calling the mean function
X = np.random.rand(5, 10) $ Y = X - X.mean(axis=1, keepdims=True) $ Y = X - X.mean(axis=1).reshape(-1, 1) $ print(Y)
duplicate_row = df2.loc[df2.duplicated('user_id') == True] $ duplicate_row
bufferdf = bufferdf.groupby([bufferdf.RateCodeID])
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=58000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
train['late_night'] = ((train.hour <= 5) | (train.hour >= 23)).astype(int) $ train.groupby('late_night').popular.mean()
logit_mod = sm.Logit(df['converted'], df[['intercept','treatment']]) $ results = logit_mod.fit() $
my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994) $ individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD) $ individual_movie_rating_RDD.take(1)
print ("Probability that individual was in the treatment group,and they converted: %0.5f" % (df2.query('converted == 1 and group == "treatment"').shape[0]/df2.query('group == "treatment"').shape[0]))
reset_graph() $ X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X") $ y = tf.placeholder(tf.int64, shape=(None), name="y") 
ab.fit(data[['expenses', 'floor', 'lat', 'lon', 'property_type', \ $              'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']], \ $         data[['price_aprox_usd']].values.ravel())
yxe_tweets['userHandle'].value_counts().head(10)
ds[4].ix['2013-01-01']
df_gt.columns
plt.scatter(df_customers['number of customers'],df_customers['profits']) $ plt.xlabel('number of customers') $ plt.ylabel('profits') $ plt.show()
autos['fuelType'].unique()
results = logit.fit() $ results.summary2()
dfLookup = dfChat[['userFromName','userFromId']] $ dfLookup = dfLookup.drop_duplicates() $ dfLookup.columns = ['userToName','userToId'] $
indices = range(len(total_ridership)) $ plt.bar(indices, total_ridership);
!curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv $ !curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv $ !curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-abbrevs.csv
twitter_final.drop(['expanded_urls','source_url','id','name1'],inplace=True,axis=1) $ twitter_final.head(2)
print dfSF.columns $ print len(dfSF.columns)
df2 = df.copy() $ df2 = df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) != False]
top_supporters.head(5).amount.plot.bar()
autos['odometer'] = autos['odometer'].str.replace("km","").str.replace(",","").astype(int) $ autos['odometer'].head(10)
df2.query('group == "treatment"')['converted'].mean()
pd.to_datetime(next_day).date()
df.groupby(['COUNTRY','PRODUCT','PRIMARY VIOLATION']).size().nlargest(5)
TEST_DATA_PATH = SHARE_ROOT + 'test_dataframe.pkl' $ test_df.to_pickle(TEST_DATA_PATH)
df['Close'].pct_change(n).rolling(21)
ebola_melt_1 = ebola_melt.iloc[:,0:4] $ print(ebola_melt_1.head()) $ status_country = ebola_melt.iloc[:,5:] $ print(status_country.head())
Featured_image = image_soup.find('img',class_="fancybox-image") $ print (Featured_image) $
store.initialize_library('POSITION') $ position=store['POSITION']
full['<=30Days'].value_counts()#.mean()
plt = sns.boxplot(data=df, x="race_desc", y="charge_count", hue="race_desc", dodge=False) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) $ plt.set_xticklabels(plt.get_xticklabels(), rotation=45)
new_dataframe = pd.read_sql("SELECT * \ $                  FROM temps", $                  con=conn) $ conn.close()
old_page_converted = np.random.binomial(1, p_old, Nold) $ print(new_page_converted)
top_songs['Day'] = top_songs['Date'].dt.day
import matplotlib.pyplot as plt $ df3_month.columns
df_treatment = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_control = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ mismatch = len(df_treatment) + len (df_control) $ mismatch_df = pd.concat([df_treatment, df_control]) $ mismatch $
(p_diffs<p_diffs_actual).mean()+(p_diffs>p_diffs+(n-p_diffs_actual)).mean()
!wget 'https://datahack-prod.s3.amazonaws.com/sample_submission/sample_submission_poy1UIu.csv'
n_old = len(df2.query('landing_page=="old_page"')) $ n_old #displaying the number of individuals receiving old page
p_diff = (new_page_converted.sum()/nold) - (old_page_converted.sum()/nold) $ print (p_diff)
stationlist = ['1 AVE', '103 ST', 'FULTON ST', 'YORK ST'] $ datecondition = (df4['DATETIME'] >= '08-01-2015 00:00:00') & (df4['DATETIME'] <= '09-01-2015 00:00:00') $ stationcondition = (df4['STATION'].isin(stationlist)) $ df4[datecondition & stationcondition].groupby(['STATION']).sum()
print(ng_m1.wv.similarity('men', 'women')) $ print(ng_m3.wv.similarity('men', 'women')) $ print(ng_m4.wv.similarity('men', 'women')) $ print(ng_m5.wv.similarity('men', 'women')) $ print(ng_m6.wv.similarity('men', 'women'))
from scipy import stats $ stats.describe(Profit)
Desc_active_stations = session.query(Measurement.station, func.count(Measurement.prcp)).\ $                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).all() $ Desc_active_stations
US_cases=match_id(merkmale, merkmale.Merkmalcode.isin(['US'])) $ US_cases.Merkmalcode.unique()
pnew = new_page_converted.sum() / len(new_page_converted) $ pold = old_page_converted.sum() / len(old_page_converted) $ p = pnew - pold $ print('pnew - pold = ', p)
colnames = ["polarity", "id", "date", "query", "user", "text"] $ data = pd.read_csv("trainingandtestdata/training.1600000.processed.noemoticon.csv", encoding="latin1", names=colnames)
df.iloc[:, 0]
lm=sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results=lm.fit() $ results.summary()
plt.plot(mydf6.index[:10000],mydf6.fuelVoltage[:10000]);
to_be_predicted_Day2 = 48.63080568 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
joined_df = df_usage.join(df_users.set_index('id'),on='id')
subjectivity = pd.DataFrame(df.groupby(['subjectivityFeeling', 'profile']).size().rename('counts')).astype(int) $ subjectivity['percent'] = subjectivity['counts'].apply(lambda x: x/subjectivity.sum()) $ subjectivity
df.columns
ls -l *.csv
samples_query.execute_call('Person', 'Extent')
i_c = df3[df3['group']=='treatment'].index $ df3.set_value(index=i_c, col='ab_page', value=1) $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
autos['date_crawled'].str[:10].value_counts(dropna=False).sort_index()
df = pd.read_csv('data/btc-market-price.csv', header=None) $ df.columns = ['Timestamp', 'Price'] $ df['Timestamp'] = pd.to_datetime(df['Timestamp']) $ df.set_index('Timestamp', inplace=True)
contribs.info()
lda.print_topics()
np.random.random((3, 3, 3))
df.to_pickle('df_organisation_type.p')
print df.shape $ ab_counts = df.groupby('ab_test_group').count().reset_index() $ ab_counts $
prob_converted = df2['converted'].mean() $ prob_newPage = len(df2.query('landing_page=="new_page"'))/len(df2) $ prob_oldPage = 1 - prob_newPage $ prob_converted_GivenNewPage = df2.query('group=="treatment"')['converted'].mean() $ prob_converted_GivenOldPage = df2.query('group=="control"')['converted'].mean()
avg_comments = reddit['Comments'].mean() $ reddit['Above_Below_Mean'] = np.where(reddit['Comments']>=avg_comments, 'Above', 'Below')
user_extract['user_id'] = user_extract['user_id'].astype(str) $ user_extract['created_at'] = pd.to_datetime(user_extract['created_at'])
from pandas.tools.plotting import lag_plot $ dataSeries = pd.Series(Q3['Average Temperature'])
csv_df[csv_df['uploader_registration'].isin([np.nan])]['uploader'].unique()
pprint(ldamallet.show_topics(formatted=False)) $ coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=tweets_list, dictionary=dictionary, coherence='c_v') $ coherence_ldamallet = coherence_model_ldamallet.get_coherence() $ print('\nCoherence Score: ', coherence_ldamallet)
closingPrices.head()
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
data = np.array(['a','b','c','d']) $ ser2 = pd.Series(data,index=[101,102,103,104]) #manually passing the index of the array $ ser2
bnbB.age.plot.hist()
witf = open("latlong_test1.txt","w", encoding="utf-8") $ for i in range(len(test_kyo1)): $     witf.write("{location: new google.maps.LatLng(%f, %f)},\n" % (test_kyo1['ex_lat'][i],test_kyo1['ex_long'][i])) $ witf.close()
tweet_json.info()
props.prop_name.value_counts()
reviews.isnull().sum()
ac.dtypes
s_mountain = Series(np.arange(0,5),index=pd.date_range('2014-08-01', periods=5, freq="H",tz="US/Mountain")) $ s_eastern = Series(np.arange(0,5),index=pd.date_range('2014-08-01', periods=5, freq="H",tz="US/Eastern")) $ s_mountain
x = ['monetary', 'frequency', 'recency','total_cust', 'join_days', 'total_profit'] $ data = df_final[x] $ correlation = data.corr()  $ plt.figure(figsize=(10, 7))  $ sns.heatmap(correlation, vmax=1, square=True, annot=True, cmap='cubehelix')
walk['2014-08-01 00:00'].mean()
pre_submission_authors = set(writing_commit_df.query("authored_datetime <= '2017-05-28'").author_name) $ post_submission_authors = set(writing_commit_df.query("'2017-05-28' < authored_datetime <= '2018-03-12'").author_name) $ new_authors = post_submission_authors - pre_submission_authors $ new_authors
nnew = len(df2.query("group == 'treatment'")) $ print(nnew)
s.index
hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284') $ shapefile = hs.content['wbdhuc12_17110006_WGS84.shp']
engine = sqlalchemy.engine.create_engine("mysql+pymysql://root:pythonetl@localhost:3306/pythonetl")
nps.head(8)
'id' in df_protest.columns
hdf.head()
plt.hist(movies.vote_average) $ plt.title('Movie Ratings in Dataset') $ plt.xlabel('Average Rating') $ plt.ylabel('Count') $ sns.despine()
hours.shape $
nfl.head()
processed_tweets_with_obs = processed_tweets_with_obs[processed_tweets_with_obs['obs'] != 'FAILED'] $ processed_tweets_with_obs = processed_tweets_with_obs.drop(processed_tweets_with_obs.index[[201]])
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
unstackedCloses = stackedCloses.unstack() $ unstackedCloses
m3.load('split_model_v1')
autos = autos.drop(["num_photos", "seller", "offer_type"], axis=1)
basicmodel = LogisticRegression() $ basicmodel = basicmodel.fit(basictrain, train["rise_in_next_day"])
df.set_value(index=index_to_change, col='ab_page', value=1)
D2 = [(str(v), str(t).replace("|","")) for v,t in D2] $ D2[0:5]
df["Age"].sum() #Specific column sum
subred_num_tot.head(10)
t.label, ' '.join(t.text[:16])
df2['Title'] = df2.Title.fillna(' N.A.') $ df2['Title'] = [x[1:] for x in df2.Title.values] $ df = pd.merge(df2, df1, on=['Cinema'], how='left') $ df = pd.merge(df, df3, on=['Title'], how='left') $ df.head()
logit_mod2 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US', 'week2', 'week3']]); $ results3 = logit_mod2.fit() $ results3.summary()
users = subs.user_id.unique() $ print "%d Users" % len(users)
lr = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ result = lr.fit()
for df in list_to_change: $     df.drop("Compaction", axis = 1, inplace = True) $     df.drop("diff_", axis = 1, inplace = True) $ addicks.head() # to see the dropped columns
for word in sorted(fdist): $     print('{0}->{1};'.format(word, fdist[word]), end=' ')
troll_tweets.head()
df_n_feature = pd.read_csv('data/photo_url.csv') $ df_n_feat = df_n_feature['photos'].astype(str) $ df_nn = df_n_feat.apply(lambda x: x.replace('u','').replace('[','').replace(']','').replace("'", '').split(', '))
logreg = LogisticRegression() $ scores = cross_val_score(logreg,  X_test, y_test,  cv=5) $ np.mean(scores), np.std(scores)    # scoring on my Testing Data under performs also
(-1 * close_month).loc[month].nlargest(2) *-1
df.head()
common_brands = brand_counts[brand_counts > .05].index $ common_brands
df.head(10) $ df[['Indicator_ID','Country','Year','WHO Region','Publication STATUS']].sort_index().head(3)
df1_clean.sample(5)
old_page_converted = np.random.choice([1, 0], size=n_old, p=[convert_rate_old, (1-convert_rate_old)]) $ old_page_converted.mean()
merged1['Specialty'].value_counts()
new_job_descriptions.cleaned_job_title.nunique()
obs_mean = new_page_converted.mean() - old_page_converted.mean() $ print ("The Value of  pnew_pold is {}".format(obs_mean))
index = pd.MultiIndex.from_product( $     list(map(range, a.shape)), $     names=['epoch', 'batch', 'datapoint'] $ )
df.info() $ df.converted.unique() $ df.landing_page.unique() $ df.group.unique()
train_df.shape
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv') $ df.head()
df2.drop(index = df2.query('landing_page =="new_page" and group=="control"').index,inplace = True, axis = 0)
from sklearn.externals import joblib $ joblib.dump(pca, '../models/pca_20kinput_6858comp.pkl') $ np.save('../models/crosstab_40937.pkl', crosstab_transformed)
gDate_content_count = gDateProject_content.groupby(level=0) $ gDate_content_count = gDate_content_count.sum() $
countries_df=pd.read_csv('countries.csv') $ countries_df.head()
messy['Conditions (standardized)'] = conditions_clean $ messy.head()
len(cats_out['Animal ID'].unique())
vals = billtargs.bill_type.get_values() $ np.unique(vals)
sorted_ct.select('count').toPandas().hist(bins=15)
flights2.loc[:,"January"]
fit_time = (end_fit - start_fit) $ print(fit_time/60.0)
cutoff = datetime.datetime(2015, 3, 30, 11, 38, 5, 291165)
control_converted = (df2.query('group == "control" and converted == 1').count()[0]) $ control_total = (df2.query('group == "control"').count()[0]) $ control_prob = float(control_converted) /  float(control_total) $ print ("Probability of Control Group converted is {}".format(control_prob))
len(df_unit_after['uid'].unique())
comments = pickle.load(open('data/comments_with_post_ids.dat', 'rb'))
teama_merge = my_elo_df.merge(final_elo, how='left', left_on=['Date', 'Team A'], right_on=['Date', 'Team']) $ teama_merge[teama_merge['Team A'] == 'Cloud9'].tail(7)
df.head()
df.to_csv('../data/processed/processed_data.csv')
setup['station'] = setup.index.map(lambda x: ids[x])
twitter_Archive.drop(['retweeted_status_user_id', $                'retweeted_status_id', $                'retweeted_status_timestamp'], axis=1,inplace=True)
image_predictions.info()
df2[df2['converted'] == 1].count()/df2.shape[0]
pd.period_range('2005', '2012', freq='A')
accts[accts['parent_id'].notnull()].to_sql('account', engine, chunksize=1000, if_exists='append', index=False)
data.tail()
example1_df = sqlContext.read.json("./world_bank.json.gz")
data[['clean_text','issue']].head(5)
tw.sample(5)
init() $ pred=run(json.dumps({"data": test_df.to_json(orient='records')})) $ print(pred)
new_page_converted = np.random.choice([1,0],size=n_new,p=[p_mean,(1-p_mean)]) $ new_page_converted.mean()
df.groupby(['userid', 'website']).sum()
re.findall('title="(.*?)"', slices[7])[0]
df_newpage = df2.query('landing_page =="new_page"') $ x_newpage = df_newpage["user_id"].count() $
df_users.gender.value_counts()
%matplotlib inline $ closeSeriesP.plot();
pets.to_csv('pets.csv')
dataset=json_data['dataset_data']['data'] $ dataset= [dataset[i] for i in range(len(dataset)) if dataset[i][0].startswith('2017')] 
tweet_archive_clean.name = tweet_archive_clean.name.apply(lambda x: 'None' if x in wrong_name_list else x)
variance(Returning_df.Sales_in_CAD, len(Returning_df.Sales_in_CAD))
import pandas as pd $ df = pd.read_csv("autos2.csv") $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ df.head()
cust_data2.info()
for index, row in susp.iterrows(): $     print(row.link)
sercRefl, sercRefl_md = neon_hs.aop_h5refl2array('../data/Day1_Hyperspectral_Intro/NEON_D02_SERC_DP3_368000_4306000_reflectance.h5')
df2[df2.user_id == dup_id[0]]
[i for i in train.columns if i not in test.columns],[i for i in test.columns if i not in train.columns]
basket = (order $           .groupby(['ID', 'Lineitem name'])['Lineitem quantity'] $           .sum().unstack().reset_index().fillna(0) $           .set_index('ID'))
model = LinearRegression() $ model.fit(X_train, y_train) $ utils.cross_val_metrics(X_train, y_train, LinearRegression())
joined_logit_mod = sm.Logit(joined['converted'], joined[['intercept', 'US', 'CA']]) $ country_result = joined_logit_mod.fit()
df.message.apply(word_tokenize)
files.remove('.ipynb_checkpoints')
test = vec1.fit_transform(df.message[1]) #takes 2. row in df for testing $ for i in test: $     print(i) $
plt.hist(null_vals); $ plt.axvline(x=diff, color='red');
new_page_converted = df2.sample(n_new, replace = True) $ p_new = new_page_converted.converted.mean() $ p_new
postsM = postsW $ postsM.reset_index().head(2)
ss = StandardScaler() $ Xs = ss.fit_transform(X) 
kick_projects['launched_date'] = pd.to_datetime(kick_projects['launched'], format='%Y-%m-%d %H:%M:%S') $ kick_projects['deadline_date'] = pd.to_datetime(kick_projects['deadline'], format='%Y-%m-%d %H:%M:%S')
shows[['primary_genre','secondary_genre']] = shows[['primary_genre','secondary_genre']].apply(lambda x: x.astype('category')) $ shows.to_pickle("ismyshowcancelled_final.pkl") $ shows.to_csv("ismyshowcancelled_final.csv")
pca.explained_variance_ratio_[0]
demographics_in_oa = openaccess_df[openaccess_df['PMCID'].isin(PMCIDs_in_demographics)] $ print(len(demographics_in_oa)) $ print(demographics_in_oa[:10]) $ demographics_in_oa = demographics_in_oa.set_index('PMCID') $ demographics_in_oa.head()
twitter_Archive.duplicated()
pprint.pprint(train_set[0][:3]) $ print('#'*50) $ pprint.pprint(train_set[1][:3]) $ print('#'*50) $ pprint.pprint(train_set[2][:3])
dtrain = DMatrix(X_tr[0:len(X_train)-40-1], label=y_ls) $ param = {'gamma':2.0,'nthread':8, 'max_depth':15, $          'eta':0.000000003, 'silent':1, 'objective':'multi:softprob', $          'eval_metric':'auc' ,'num_class':105}
tablename='gateway' $ pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False).head(10)
url = "https://d5tgnm602g.execute-api.us-east-1.amazonaws.com/beta/la?long="\ $       + var_lon + "&lati=" + var_lat
merged[['CA', 'UK', 'US']] = pd.get_dummies(merged['country']) $ merged.head()
df = pd.DataFrame(np.random.randn(10,4)) $ df
sqlContext.sql("select * from RandomOne").toPandas()
blame.timestamp = blame.timestamp.astype('int64') $ blame.head()
to_be_predicted_Day3 = 55.22955286 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
from scipy.stats import norm $ print("Singificance of z-score: ",norm.cdf(z_score)) $ print("Critical Value: ",norm.ppf(1-(0.05/2))) $
n_new = df_new.shape[0] $ print(n_new)
df.head(8)
df2.rename(columns={'ID':'id'}, inplace=True)
df3['day'] = df3['timestamp'].dt.day $ df3.head()
norm.ppf(1-(0.05))
 new_profile = {'user_id': 213, 'reinsurer': 'XL American'} $  duplicate_profile = {'user_id': 212, 'reinsurer': 'SCOR S.E'} $  result = db.profiles.insert_one(new_profile)  # This is fine. $  result = db.profiles.insert_one(duplicate_profile)
with open("Valid_events_eps0.7_5days_500topics","rb") as fp: $     Valid_events = pickle.load(fp)
csvDF=csvDF.rename(columns = {'CustID':'CustId'})
so[(so['score'] >= 100) | (so['answercount'] >= 10)].head()
prop = props[props.prop_name=="PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
All_tweet_data_v2[['rating_numerator','rating_denominator']][All_tweet_data_v2.rating_denominator!=10]
actual_diff = df2.converted[df2.group == 'treatment'].mean() - df2.converted[df2.group == 'control'].mean() $ (actual_diff < p_diffs_alt).mean()
archive[np.isnan(archive['in_reply_to_status_id']) != True].sample(5)
sample.head(5)
n=13 $ df['Close'].pct_change(n) #n timeperiods percent change
for trace in traces: $     trace['y'] = 100.*trace['y']/sum(trace['y'])
a.lower()
from utils import fetch_and_cache $ data_url = 'http://www.ds100.org/sp18/assets/datasets/old_trump_tweets.json.zip' $ file_name = 'old_trump_tweets.json.zip' $ dest_path = fetch_and_cache(data_url=data_url, file=file_name) $ print(f'Located at {dest_path}')
group_vT = df2.groupby('vehicleType').agg({"powerPS": [min, max, mean]}) $ group_vT.columns = ["_".join(x) for x in group_vT.columns.ravel()] $ group_vT
print("There are %d documents in this collection." % (index.maximum_document() - index.document_base()))
automl.fit(X_train.copy(), y_train.copy(), dataset_name='house', $            feat_type=feature_types) $ print(automl.show_models()) $ predictions = automl.predict(X_test) $ print("R2 score:", sklearn.metrics.r2_score(y_test, predictions))
authors = train.groupby('author').popular.agg(['count', 'mean']) $ authors.shape[0]
df.head()
ctd_df.head(10)
vecs.head()
page.oldest_revision['user']
y_tree_predicted = tree.predict(X_test)
trump['net_polarity'] = all_tweet_polarity $ trump.head()
image_predictions.info()
crimes.drop('LOCATION', axis=1, inplace=True)
s4p = combined.reset_index() $ s4p[:5]
investors_df.head()
engagement =[] $ for sublist in outputList: $     for item in sublist: $         engagement.append(item) $ print("There are " + str(len(engagement)) +  " observations in the output")
age_up70.shape
df_new = country_df.set_index('user_id').join(df2.set_index('user_id'),how='inner') $ df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new.country) $ df_new.head()
for tweet in tweets: $     print(tweet.text)
y_cat.value_counts() $
t = pd.datetime.today() - pd.Timestamp('1/1/1970')
agency_group = data.groupby('Agency') $ agency_group.size().plot(kind='bar')
class_test.groupby('PrimaryTopic').sum()[['thanks']]
predictions_train.show()
df_kyt['place'] = df_kyt['place'].apply(lambda x: translate(x,'en')) $ df_kyt['text'] = df_kyt['text'].apply(lambda x: translate(x,'en'))
z3.head()
df2['intercept'] = 1 $ df2[['control','ab_page']] = pd.get_dummies(df2['group']) $ df2.head()
minute_range = bars.high - bars.low $ minute_range.describe()
theft.sort_values('DATE_OF_OCCURRENCE', inplace=True, ascending=True) $
phone = "2004-959-559 # This is Phone Number" $ num = re.sub(r'#.*$', '', phone) $ print('Phone Num : {}'.format(num))
search['one_way'] = search.apply(lambda x: 0 if x['trip_end_loc'] == x['trip_start_loc'] else 1, axis=1)
print('There are',len(df_providers['id_num'].unique()),'unique site IDs') $ df_sites = (df_providers.groupby(['id_num'])[['disc_times_pay']].sum()) $ df_sites = df_sites.reset_index() $ df_sites.head()
temp = dog_rates.groupby(['cuteness']).agg({'favorite_count':'mean', 'retweet_count':'mean'}) $ temp.sort_values(['favorite_count'],ascending=False)
new_page_converted= new_page_df.query('converted == True').user_id.nunique() / new_page_df.query('converted == False').user_id.nunique()
%%time $ data_demo["num_child"] = data_demo["comment_id"].apply(lambda x: data_demo[data_demo["parent_id"]==x].shape[0])
team_list = list(final_elo['Team'].unique()) $ print(len(team_list)) $ fig = plt.figure(figsize=(14,14)) $ plt.plot(pd.to_datetime(final_elo[final_elo['Team'] == 'Cloud9']['Date']).interpolate(method='time'), final_elo[final_elo['Team'] == 'Cloud9']['Elo'].interpolate(method='cubic') ) $ plt.show()
writer = pd.ExcelWriter("../visualizations/uber_avg_day_of_month.xlsx")
brand_count = autos["brand"].value_counts(normalize=True) $ sel_brand = brand_count[brand_count >= 0.05].index
calls_df=calls_df.drop(["postal_code"],axis=1)
import IPython  # for displaying parse trees inline $ for tree in parser.parse(sentence): $     IPython.display.display(tree)  # instead of tree.draw()
df2.drop_duplicates(subset = ['user_id'],  inplace=True) $ df2[df2['user_id']==773192]
autos['odometer_km'].sort_index(ascending=False).head()
mentions_df["src_str"] =  mentions_df["src"].apply(lambda x: user_names[x]) $ mentions_df["trg_str"] =  mentions_df["trg"].apply(lambda x: user_names[x]) $ mentions_df["src_screen_str"] =  mentions_df["src"].apply(lambda x: user_screen_names[x]) $ mentions_df["trg_screen_str"] =  mentions_df["trg"].apply(lambda x: user_screen_names[x])
X_train_all=X_train_all.drop('text',axis=1) $ X_test_all=X_test_all.drop('text',axis=1)
conf = result.conf_int() $ conf['OR'] = params $ conf.columns = ['2.5%', '97.5%', 'OR'] $ print(np.exp(conf)) # confidence intervals (we get the odds ratio from the log odds by exponentiating the coefficient)
robust_cov_matrix= pd.DataFrame(skcov.ShrunkCovariance().fit(daily_ret).covariance_,columns=daily_ret.columns,index=daily_ret.columns) $ robust_cov_matrix
menu_about_vect = menu_about_vectorizer.fit_transform(menus_to_analyze['about']) $ menu_about_feature_names = menu_about_vectorizer.get_feature_names() $ menu_about_latent_features = run_sklearn_nmf(menu_about_vect, menu_about_feature_names, 7)
type_driver=pd.Series(driver_count["type"]) $ count_driver=pd.Series(driver_count["driver_count"]) $ colors = ["gold","lightskyblue","lightcoral"] $ explode = (0, 0, 0.1)
save_n_load_df(promo_df, 'promo_df1.pkl')
pd.to_datetime(['04-01-2012 10:00'])
targetUsersRank['norm_rank']=(targetUsersRank['label']-targetUsersRank['label'].min())/(targetUsersRank['label'].max()-targetUsersRank['label'].min()) $ print targetUsersRank.shape $ targetUsersRank.head()
df = all_tables_df.tail()
def id_annotation(row): $     new_row = row.copy() $     new_row['annotations']['classification_id'] = new_row['classification_id'] $     return new_row
im.tail()
first_year = first_movie.h3.find('span', class_ = 'lister-item-year text-muted unbold') $ first_year
number_of_topics = 150 $ posts_df["Tags"] = posts_df["Tags"].map(lambda x: [] if (x is np.nan) else x.replace("<","").replace(">",",").strip(",").split(",")) $ tags = reduce(lambda x,y: x+y,posts_df["Tags"]) $ tags_freq = Counter(tags) $ print(tags_freq.most_common(number_of_topics))
near = np.concatenate([px.values, px.values[-1] + walk])
lda = np.linspace(20, 50, 300) $ dist = [f(a) for a in lda] $ plt.figure(figsize=(10, 5)) $ _ = plt.plot(lda, dist)
autos['registration_year'].describe()
A = np.arange(25).reshape(5,5) $ A[[0,1]] = A[[1,0]] $ print(A)
pgrid_tree = ParameterGrid({'learning_rate': [0.1] , 'n_estimators': [100, 200, 300], 'random_state': [1] $                         , 'max_depth': [3, 4, 5], 'min_child_weight': [1, 2, 3, 4] $                         , 'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9] $                       })
properati['zone'] = properati.place_with_parent_names.apply(lambda x : x.split('|')[3])
def Cube(x): $     return x ** 3 $ df_n = df.iloc[:,1:] $ df_n.apply(Cube)
combined_df.shape
model.fit(train_Features, train_species_ohe, epochs=100, batch_size=1, verbose=0);
df3 = df2.copy() # Take a copy for the new regression approach. $ df3.head()
hpd['StatusDescription'].value_counts()
print('Number of rows in customers = {}'.format(customers.CustomerID.count())) $ print('Number of rows in trips = {}'.format(transactions.CustomerID.count()))
cat_outcomes = cat_outcomes.loc[(cat_outcomes['outcome_type'] == 'Adoption') | $                                  (cat_outcomes['outcome_type'] == 'Transfer')]
! head readme.html 
df_comment[df_comment.authorName=='The Zeus'].groupby('date').count()
historicalPriceEFX.to_csv('EFX_stock.csv')
data_archie = data_archie[data_archie.cur_sell_price != 0.0]
df = pd.DataFrame(['Fred', 'Alice', 'Joe'], columns=['Name']) $ df
excelDF.Region.unique()
from datetime import datetime $ def get_time_since_last_publication(input_): $     now = datetime.now() $     return (now - max(input_)).days $ grouped_publications_by_author['time_since_last_publication'] = grouped_publications_by_author['publicationDates'].apply(get_time_since_last_publication) $
search2 = search2.sample(frac=1)
autos['odometer'] = autos.odometer.str.replace(',','').str.replace('km','').astype(float)
plt.hist(p_diffs,bins=25) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Plot of 10K simulated p_diffs');
tweet_df.head(2)
pd.read_csv("../../data/msft.csv", skiprows=100, nrows=5, header=0,names=['open','high','low','close','vol','adjclose'])
df2.loc[df2.duplicated('user_id') == True]['user_id']
lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_data_filter_settings['ntot_winter'].settings.df $
giss_temp.columns
%%time $ PredClass.process_raw()
plt.figure(figsize=(16, 16)) $ sns.heatmap(corr, annot=True) $ plt.title('Correlation in Heat Map')
import pandas as pd $ from sqlalchemy import create_engine $ db_connection = create_engine('sqlite:///data/chinook.db') $ df = pd.read_sql_query('SELECT * FROM albums', db_connection) $ df.head(2)
df['intercept'] =1 $ df[['control', 'treatment']] = pd.get_dummies(df['group'])
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head(n=4)
ab_df2.query('group == "treatment"')['converted'].mean()
houses_test.SalePrice.head()
x=numpy.memmap('columncache/acs2015_5year_tract2010/B00001_001.float32', dtype=numpy.float32, mode='r')
df_loan3.to_clipboard()
mask = (nullCity["creationDate"] > '2017-01-01') & (nullCity["creationDate"]<= '2017-12-31') $ nullCity2017 = (nullCity.loc[mask]) $ nullCity2017.head()
prog_lang[prog_lang.Name == 'Python']
from bs4 import BeautifulSoup             $ example1 = BeautifulSoup(train["review"][0], 'html.parser')  
autos.drop(['dateCrawled','dateCreated','lastSeen','postalCode'],axis='columns',inplace=True)
premio_total_sena = data["Rateio_Sena"].sum() $ premio_total_quina = data["Rateio_Quina"].sum() $ premio_total_quadra = data["Rateio_Quadra"].sum() $ soma_premios = premio_total_quadra + premio_total_quina + premio_total_sena $ print(soma_premios)
pop_dog = twitter_archive_master.groupby('p1')['likes'].mean().reset_index() $ pop_dog.sort_values('likes', ascending=False).head()
data.isnull().sum()
df.plot(x='favorite_count', y='retweet_count', title='Favorite Count by Retweet Count', kind='scatter'); $ plt.savefig('retweet_vs_favorite.png')
tweets = tweets_raw.copy() $ tweets["retweet"] =  tweets["content"].map(lambda s : s[0:4] == "RT @") #Is it a retweet?
for item in all_simband_data.subject_id.unique(): $     if item not in proc_rxn_time.subject_id.unique(): $         print(item)
all_tables_df.OWNER.nunique()
msno.matrix(titanic)
df_lm.filter(regex='tch_attempt|last_month').boxplot(by='last_month', figsize=(10,10),showfliers=False)
%%time $ df['closed_at'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %X %p') $
data = ['peter', 'Paul', None, 'MARY', 'gUDIO'] $ [s.capitalize() for s in data]
y_train.T[0:5]
train.AUTEUR_INCIDENT.value_counts() $
grid_lat = np.arange(24, 50.0, 1) $ grid_lon = np.arange(-125.0, -66, 1) $ glons, glats = np.meshgrid(grid_lon, grid_lat)
df_img_algo_clean['prediction_1'] = df_img_algo_clean['prediction_1'].str.lower() $ df_img_algo_clean['prediction_2'] = df_img_algo_clean['prediction_2'].str.lower() $ df_img_algo_clean['prediction_3'] = df_img_algo_clean['prediction_3'].str.lower()
df4[['Canada','UK','US']] = pd.get_dummies(df4['country'])
cryptos['market_cap_usd_billions'] = cryptos.market_cap_usd / 1e9  $ cryptos.head()
df = pd.DataFrame(fb_vec.todense(), columns=cv.get_feature_names())
df.Visitors
import seaborn as sns $ sns.set(style='white') $ plt.figure(figsize=(12, 8)) $ plt.title('Cohorts: User Retention') $ sns.heatmap(user_retention.T, mask=user_retention.T.isnull(), annot=True, fmt='.0%');
df.loc[df['edition']=='NBC', 'edition'] = 'National'
mean = np.mean(data['len']) $ print("The lenght's average in tweets: {}".format(mean))
p_diffs = np.array(p_diffs) $ (p_diffs > obs_diff).mean()
model.wv.index2word[0:10]
Y_test = data3[302:].sales
temp = pd.read_csv("loans_2007.csv", nrows=1000) $ print("Memory usage (1000 observations): {}".format(round(temp.memory_usage(deep=True).sum()/1048576,2))) $ temp = pd.read_csv("loans_2007.csv", nrows=3000) $ print("Memory usage (3000 observations): {}".format(round(temp.memory_usage(deep=True).sum()/1048576,2))) $ print(temp.shape)
s1.index
faa_data_substantial_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "S"] $ print(faa_data_substantial_damage_pandas.shape) $ faa_data_substantial_damage_pandas.head()
us_grid_id.head()
df_wm.head(2)
def getDictFrom2ColDataFrame(df, index_col, other_col): $     direction = df[[index_col, other_col]] $     direction_dict = {k: list(v)[0] for k, v in direction.groupby(index_col)[other_col]} $     return direction_dict
np.shape(tables)
parts = legos['parts'].copy() $ parts.rename(columns = {'name': 'part_name'}, inplace = True)
run txt2pdf.py -o '870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS.pdf'  '870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS.txt'
contribs.info()
n_new=df2.landing_page.value_counts()[0] $ n_new
urls = read_csv("urls.csv") $ urls.head()
day = lambda x: datetime.date(int(x[0]),int(x[1]),int(x[2])).isocalendar()[1] $ emotion_big_df['date']=emotion_big_df['date'].apply(day)
print('Support Vector Classifier Accuracy:', nltk.classify.accuracy(SVC, test_set) * 100, '%')
tweet_en.describe()
df_group=df2.groupby(['group','landing_page']) $ df_group.describe() $ prob=(145310/(145274+145310)) $ print("Probability that an individual received the new page is "+str(prob))
url = "https://space-facts.com/mars/" $ table = pd.read_html(url) $ print(table)
appointments.head()
df_merge = pd.read_csv('df_merge.csv')
np.exp(-0.0408), 1/(np.exp(-0.0408))
dict_creator = json.loads(df.loc[row,'creator']) $ pprint(dict_creator)
pd.value_counts(ac['External ID'].values, sort=True, ascending=False) $
active_station_temps = session.query(func.max(Measurement.tobs), \ $                                      func.min(Measurement.tobs), func.avg(Measurement.tobs)).\ $                                     filter(Measurement.station == 'USC00519281').all() $ active_station_temps
actor = pd.read_sql_query('select concat(ucase(first_name)," ", ucase(last_name)) as "Actor Name" from actor', engine) $ actor.head()
old_page_converted = np.random.binomial(1, 0.1196, 145274)
len(missing_rcn) + h2020.shape[0] == h2020_part.shape[0]
clicking_conditions.loc[clicking_conditions.user_id == '379881d5-32d7-49f4-bf5b-81fefbc5fcce']
df.groupby(df.index.hour)['Created Date'].count().plot()
data.groupby("Gender").mean()["Age (Years)"]
xml_in.dropna(subset = ['venueName', 'publicationKey', 'publicationDate'], inplace = True) $
df_goog.Open.asfreq('D', method='backfill').plot()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
gc.collect()
df_twitter_archive.describe()
suspects_with_25_1 = suspects_with_25_1.reset_index()
n_old = df2.query("group == 'control'").shape[0] $ n_old
df.to_csv("ccl_clean.csv")
X_traincvCT, X_testcvCT, y_traincvCT, y_testcvCT = model_selection.train_test_split(train_centroids, $                                                                                     train["sentiment"], $                                                                                     test_size=0.2, $                                                                                     random_state=0)
h2020 = h2020_proj.merge( $     h2020_part, $     on=["projectRCN", "projectID", "projectAcronym"] $ )
!head -n 40 example_data/clinvar_donor_acceptor_chr22.vcf
joined2.head()
keep = tag_pairs["FromName"].value_counts() $ keep = keep[keep>5] $ keep.shape
plt.hist(p_diffs, alpha=0.5); $ plt.axvline(act_diff, color='red');
",".join(list(grouped_df.get_group('2011-07-12')['Grouped Corrected Common Name']))
twitter_archive.loc[(twitter_archive['name'].str.islower()) & (twitter_archive['text'].str.contains('named'))]
not_in_misk.head(3)
pd.io.json.json_normalize(list(c.find({}, {'_id': 0})))
df.plot()
raw.columns
com_grp.nth(-1)    # retireve last row from each group
data = json.loads(rs)
archive_clean = twitter_archive.copy() $ predictions_clean = images_predictions.copy() $ tweet_clean = tweet_json.copy()
df3_holidays = df3.copy() $ df3_holidays['y'] = np.log(df3_holidays['y'])
hp.init_tmpo() $ hp._tmpos.debug = False $ hp.sync_tmpos()
print(automl_feat.show_models())
for leg in plan['plan']['itineraries'][0]['legs']: $     print('distance = {:,.2f} | duration = {:.0f} | mode = {} | route = {} | steps = {}'.\ $ format(leg['distance'], leg['duration'], leg['mode'], leg['route'], len(leg['steps'])))
query = 'SELECT count(distinct x.uid) FROM (SELECT uid FROM ways UNION ALL SELECT uid FROM nodes) as x' $ c.execute(query) $ results=c.fetchall() $ print results[0][0]
dfs = pd.read_html('https://en.wikipedia.org/wiki/Timeline_of_programming_languages')
nobel[nobel['Sex'] == 'Female'].groupby('Category').size().sort_values(ascending=False)
for i in image.list(workspace = ws,tag = TICKER): $     print('{} {}(v.{} [{}]) stored at {} with build log {}'.format(i.id, i.name, i.version, i.creation_state, i.image_location, i.image_build_log_uri))
all_cols = list(train_data.columns)
plt.clf() $ plt.plot(res.seasonal[:7],color = 'black') $ plt.show()
df5 = df4.set_index("user_id").join(df3.set_index("user_id"), how="inner") $ df5.head()
df.loc[df.followers.idxmax()]
weather_yvr.head()
AAPL_array=df["log_AAPL"].dropna().as_matrix() 
pd_data.head()
image_predictions.describe()
pets.to_pickle('petpickle.pkl')
len(df) $ len(df[df['processed2'].str.count(r'\w+') >= 3]) $ df = df[df['processed2'].str.count(r'\w+') >= 3]
import plotly as pltly $ import plotly.tools as tls
fill_zipcode = lambda x: '0'*(5-len(str(x))) + str(x) $ x1 = pd.DataFrame([[1, '8820'], [2, 8820]], columns=['a','b']) $ x1.b = x1.b.apply(fill_zipcode) $ x1
joined = customers.join(transactions.set_index('CustomerID'), on='CustomerID', how='inner').reset_index(drop=True)
noaa_data['AIR_TEMPERATURE'].groupby(noaa_data.index.dayofweek).mean()
plt.hist(null_values) #Simulation from the null $ plt.axvline(ControlConverted-TreatmentConverted,color="red")
bg3 = bg2.drop(bg2.index[0]) # drop first row $ bg3
Test.FlowVals
unordered_timelines = list(chain(*np.where([min(USER_PLANS_df.iloc[i]['scns_created']) != USER_PLANS_df.iloc[i]['scns_created'][0] for i in range(len(USER_PLANS_df))])))
center_attendance_pandas.groupby('center_name')['attendance_count'].sum().sort_values(ascending=False) $
youthUser1['demographicInfodob'] = youthUser1['demographicInfodob'].fillna(0) $ youthUser1['demographicInfodob'] = youthUser1['demographicInfodob'].apply(date) $ youthUser1.describe()
print(advancedtrain.shape)
df_concensus['esimates_count'].describe() $
print(tweet_archive.name.value_counts())
df2.types
[sumthis(x, y) for x, y in zip(df.a, df.b)]
by_day_by_hour14.to_json("../visualizations/by_day_by_hour14.json", orient="split")
geo_irregularities.info()
tweet_archive_clean.head()
image_predictions_df[(image_predictions_df.p1_dog == False) & $                      (image_predictions_df.p2_dog == False) & $                      (image_predictions_df.p3_dog == False)]
df_sqr = df_final[features2] $ sqrt_val = df_sqr.apply(np.sqrt) $ sqrt_val.tail()
df = pd.read_csv('data/test1.csv', parse_dates=['date']) $ df
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_6201.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_6201, lags=40, ax=ax2)
training, test = train_test_split(review_df, test_size=0.2, random_state=233) $ print(len(training), "train +", len(test), "test")
df_proc = pd.read_csv("../01_data preprocessing/data new/procdata.csv",encoding="utf-8",sep=",", $                       parse_dates=["BookingDate"],dtype={"CustID":"int64"}) $ df_proc["Counter"] = df_proc.groupby("CustID").cumcount(ascending=False) + 1 $ df_proc1 = df_proc.loc[:,["CustID","CardID","Store","BookingDate","Counter"]]
plt.plot(x, x ** 2) $ plt.plot(x, -1 * (x ** 2))
top_10_authors = git_log['author'].value_counts().head(10) $ top_10_authors
df2.dtypes
condos.MAR_WARD.value_counts()
r,q,p = sm.tsa.acf(resid_6203.values.squeeze(), qstat=True) $ data = np.c_[range(1,41), r[1:], q, p] $ table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"]) $ print(table.set_index('lag'))
path_to_zips = '../data/us_dot/otp/'
new_page_converted = np.random.choice([1, 0], size=n_new, p=[page_new, (1-page_new)])
reg.score(X_test,Y_test)
df2.query('landing_page == "new_page"').user_id.nunique()  / df2.user_id.count()
X_train, X_test, y_train, y_test = train_test_split(X_c, y_cm, test_size=0.5, random_state=42)
rj = r.json()
sentiment_df.plot(x="News_Source", y="Compound", title = "News Mood Analysis_Channel", kind = "bar") $ plt.show()
df2[["treatment", "control"]] = pd.get_dummies(df2["group"])
data['online'].value_counts()
mod = sm.Logit(df3['ab_page'], df3[['converted','country_code_CA','country_code_UK','country_code_US','intercept']]) $ res = mod.fit() $ res.summary()
brands = autos["brand"].value_counts(normalize = True) $ represented_brands = brands[brands > .02].index $ print(represented_brands)
import urllib3, requests, json, base64, time, os $ warnings.filterwarnings('ignore')
states = kickstarter.groupby('state').size() $ states
df_user_product_ids.head(5)
contentPTags = SAEMSoup.body.findAll('p') $ for pTag in contentPTags[:3]: $     print(pTag.text)
plt.figure() $ plt.title('Team count') $ plt.scatter(filterdf['teamCount'], filterdf['best'], marker= 'o', s=20) $ plt.show() $
pd.Series(5, index=[100,200,300])
total_cars = autos['registration_year'].shape[0] $ correct_range_cars = autos[autos['registration_year'].between(1886, 2016)].shape[0] $ correct_range_percentage = correct_range_cars / total_cars $ correct_range_percentage
df = pd.read_sql_query("select path, type, ds from tracks where ds>='2018-01-01' and ds<'2018-01-02' limit 10", conn)
datacamp = pd.read_csv("c:\\users\\ssalahuddin\\documents\\datacamp130818.csv", parse_dates=["publishdate"], infer_datetime_format=True)
print('Under the null hypothesis p_old is the same conversion rate regardless of the page: {}.'.format(conversion_rate_all_pages))
trans_data.head()
pickle.dump(selfharmmm_final_df, open('iteration1_files/epoch3/selfharmmm_final_df.pkl', 'wb'))
ufos_df2.show(3)
data[[name.endswith('bacteria') for name in data.phylum] & $     (data.value > 1000)] $ data
twitter_archive_clean[['tweet_id','in_reply_to_status_id','in_reply_to_user_id']] = twitter_archive_clean[['tweet_id','in_reply_to_status_id','in_reply_to_user_id']].astype('object') $ twitter_archive_clean['timestamp'] = pd.to_datetime(twitter_archive_clean['timestamp']) $ image_predictions_clean['tweet_id'] = image_predictions['tweet_id'].astype('object')
grbreg.score(X_test, y_test)
le.fit(df_members['bd_c'])
filled = filtered.fillna(method='pad', limit=1) $ filled.ix['2011-11-03':'2011-11-04'].head(20)
rmse_CBoE=np.sqrt(mean_squared_error(df['diff_log_CBoE'][1:-1],predCBoE))
top_supporters.to_csv("top_supporters.csv")
hdf = df.set_index(['AgeBins', 'Industry']) $ hdf.head()
summer = df[df.index.month.isin([6, 7, 8])]
points.name="WorldCup" $ points.index.name="Previous Points" $ points $
us_temp = temp_fine.reshape(843,1534).T #.T is for transpose $ np.shape(us_temp)
df_all_wells_basic.isnull().sum()
data[['Sales']].resample('D').mean().expanding().mean().head() $
obs_diff = (df2.query("group == 'treatment'")['converted'] == 1).mean() - (df2.query("group == 'control'")['converted'] == 1).mean() $ obs_diff
print(1/np.exp(-0.0408),1/np.exp(0.0099))
import numpy as np $ a=np.random.randn(3, 3) $ a
print("Mean squared error: %.2f" $       % mean_squared_error(y_test, y_pred))
for c in ccc: $     vhd[c] /= vhd[c].max()
grouped = df.groupby(['product_type','state']).size()
print(model_CBoE.aic) $ print(model_CBoE.bic)
df_tweet_clean.duplicated('id').value_counts()
forest_clf = RandomForestClassifier(random_state=0) $ forest_clf.fit(X_train, Y_train)
import statsmodels.api as sm $ convert_old = df2.query('group == "control"').converted.sum() $ convert_new = df2.query('group == "treatment"').converted.sum() $ n_old = df2.query("landing_page == 'old_page'").shape[0] $ n_new = df2.query("landing_page == 'new_page'").shape[0]
tweets_predictions_all = tweets_predictions_all[tweets_predictions_all['jpg_url'].notnull()]
sns.set_style("darkgrid") $ plt.figure(figsize=(8, 4)) $ errors['errorID'].value_counts().plot(kind='bar') $ plt.ylabel('Count')
cpi_sdmx.lookup_code('housing','Index')
pd.Series([2, 4, 6])
all_data_merge = all_data_merge[clm] $ all_data_merge.rename(columns={'store_x':'store','Sub_store':'substore','Brand':'brand', 'url_x':'url'}, inplace=True) $ all_data_merge.columns
logout_page = BeautifulSoup(logged_out.content) $ bool(logout_page.find_all(string=re.compile('All cookies cleared!')))
iso_join.plot();
df_clean3.loc[1779, 'text']
sel = [Measurements.tobs] $ temperature_data = session.query(*sel).\ $     filter(Measurements.date >= initial_date).\ $     filter(Measurements.station == highest_station).all() $
df.isnull().sum(), df.isnull().sum()/float(len(df)) $
train.head(3)
compound_df.plot(kind='scatter', x='index', y='@BBCWorld', subplots=False) $ plt.show()
to_be_predicted_Day3 = 25.11485584 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
n_old = control_df.shape[0] $ n_old
val_docs = glob(join(DATA_FOLDER, 'validate/*.txt'))
mlp_fp = 'data/richmond_median_list_prices.csv' $ mlp_df = pd.read_csv(mlp_fp) $ mlp_df.head()
merged.amount.sum()
train.timestamp_first_active.head(5)
store_items = store_items.drop(['bikes'], axis=1)  # bikes column $ store_items
data_with_dates = pd.DataFrame(df_final[['created_time', 'total_likes', 'total_comments']]) $ splits = data_with_dates['created_time'].str.split('T') $ data_with_dates['created_time'] = splits.str[0] $ print(data_with_dates.head()) $ d_dates =  data_with_dates.groupby('created_time').sum()
percipitation_measurement_df.describe()
text_clf.fit(tweet_train_data, tweet_train_target)  
p_new = df2.query('landing_page == "new_page"').converted.mean() $ p_new
data['User_Score'] = data['User_Score'].apply(pd.to_numeric, errors='coerce') $ data['User_Score'] = data['User_Score'].mask(np.isnan(data["User_Score"]), data['Critic_Score'] / 10.0)
acc.get_last(10)
aggDF.columns
index_change = df_new[df_new['group']=='treatment'].index $ df_new.set_value(index=index_change, col='ab_page', value=1) $ df_new.set_value(index=df_new.index, col='intercept', value=1) $ df_new[['intercept', 'ab_page']] = df_new[['intercept', 'ab_page']].astype(int) $ df_new = df_new[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
import pandas as pd $ git_log['timestamp']=pd.to_datetime(git_log['timestamp'],unit='s') $ git_log.timestamp.describe() $
logreg_words = LogisticRegression(random_state=42) $ scores = cross_val_score(logreg_words, X_words, y, cv=skf) $ print 'Cross-validated LogReg scores based on certain words:', scores $ print 'Mean of scores:', np.mean(scores)
fda_drugs.loc[fda_drugs.DrugName.str.contains('AMANTADINE')]
top_supports.amount.plot.bar()
googletrend.head()
from IPython.display import IFrame $ IFrame('https://dms2203.carto.com/builder/e18a2cd4-0edb-412b-9b40-36b08216fe5f/embed?state=%7B%22map%22%3A%7B%22ne%22%3A%5B40.34445080136368%2C-74.66445922851564%5D%2C%22sw%22%3A%5B41.05760862509861%2C-73.35433959960939%5D%2C%22center%22%3A%5B40.702244436175114%2C-74.01008605957033%5D%2C%22zoom%22%3A10%7D%2C%22widgets%22%3A%7B%2257f9458a-b853-444f-a937-0c2bbb5738a6%22%3A%7B%22autoStyle%22%3Atrue%2C%22normalized%22%3Atrue%7D%2C%2210db2f20-bdb2-42dd-a518-cfbb0a34c211%22%3A%7B%22normalized%22%3Atrue%7D%7D%7D', width='100%', height=700)
imgp_clean.info()
joined.head(2)
my_query = "SELECT * FROM specimens LIMIT 10" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print sorted(first_element.keys())
measure_val_2014_to_2017.count()
Image(filename='D:\kagglelearn\pics\wine_reviews.png')
Meter1.AutoRun(1.5, 'Sec', ReGenPlot=True)
reg_lm.summary()
block_populations = numpy.load('columncache/census2010_block2010/p001001.numpy') $ print 'block_populations has', sum(block_populations), 'total people'
autos = autos[autos["registration_year"].between(1900,2016)]
1/np.exp(results.params[2])
'https://www.reddit.com/r/CryptoCurrency/search?q=flair%3AGeneral+Discussion&restrict_sr=on&sort=new&t=all&count=+'str(num)'&after='str(lastid)
save_filepath = '/media/sf_pysumma' $ hs_path = save_filepath+'/a0105d479c334764ba84633c5b9c1c01/a0105d479c334764ba84633c5b9c1c01/data/contents' $ S = Simulation(hs_path+'/summaTestCases_2.x/settings/wrrPaperTestCases/figure07/summa_fileManager_riparianAspenSimpleResistance.txt')
import statsmodels.api as sm $ convert_old = df2.converted[df.landing_page == 'old_page'].sum() $ convert_new = df2.converted[df.landing_page == 'new_page'].sum() $
p_diffs = pd.DataFrame({'col': p_diffs}) $ p_diffs.plot(kind='hist')
no_line_up = first_comb.count() + second_comb.count() $ no_line_up
metadata['reflectance_scale_factor'] = float(refldata.attrs['Scale_Factor']) $ metadata
tweet_archive_clean[tweet_archive_clean.tweet_id == 855862651834028034]
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)
df3[['new', 'old']] = pd.get_dummies(df3['landing_page']) $ df3.head()
predicted = model.predict(test_data) $ score = metrics.accuracy_score(test_labels, predicted) $ print("Accuracy = {:.2f}".format(score))
merged2.columns
liberiaFullDf.index.get_level_values(0).value_counts()
quarters.asfreq("M")
outcomes = [0,1] $ probs = [1-c_rate_null,c_rate_null] ### same as above $ old_page_converted = np.random.choice(outcomes, size= nold,replace = True, p = probs) $ old_page_converted
weather_mean.values
coins = pd.DataFrame(np.random.randint(0, 2, (100, 10)), $                      columns=list('abcdefghij')) $ coins.head()
autos["brand"].value_counts(normalize = True)
pd.get_dummies(df.C,dummy_na=True)
adopted_cats['Breed'].value_counts()
df_img_algo = pd.read_csv("file_2", sep= "\t") $ df_img_algo.head() $
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
rm_SPY.plot(label="Rolling Mean",ax=ax)
from keras.utils import to_categorical $ target = to_categorical(titanic.survived) $ target
stand_err = std_w/np.sqrt(len(hm_data)) $ z_score = (mean_w - 85.)/stand_err $ print stand_err, z_score
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
ben_fin.shape
print(ndvi_nc) $ for v in ndvi_nc.variables: $     print(ndvi_nc.variables[v])
import pickle $ output = open('speeches_features.pkl', 'wb') $ pickle.dump(speeches_cleaned, output) $ output.close()
print('       Sponsored Jobs: ' + str(len(raw[raw.job_type == 'Sponsored']))) $ print('Unique Sponsored Jobs: ' + str(raw[raw.job_type == 'Sponsored'].hash.nunique()) + '\n') $ print('         Organic Jobs: ' + str(len(raw[raw.job_type == 'Organic']))) $ print('  Unique Organic Jobs: ' + str(raw[raw.job_type == 'Organic'].hash.nunique()))
df_image_tweet2 = df_image_tweet[df_image_tweet.tweet_id.isin(id_list2)] $ df_clean4       = df_clean3[df_clean3.tweet_id.isin(id_list2)]
tweets_p1_month = twitter_archive_master.loc[:, ['Month', 'Year', 'Frist_pred_conf']]
learner.fit(3e-3, 4, wds=1e-6, cycle_len=10, cycle_save_name='adam3_10')
%matplotlib inline $ plt.hist(data.sepal_length, bins=25)
print(store['prealn/alignment_bad'].shape[0]) $ remove_chunk(store, 'prealn/alignment_bad', problems.srr) $ print(store['prealn/alignment_bad'].shape[0])
br_pr = pd.Series(brand_price) $ br_mi = pd.Series(brand_mileage)
data.head(10)
pd.Timestamp('1/2018')
df.shape
saved = df['Market'].head(1)
df2['converted'][(df2['group'] == 'control')].sum()/df2['converted'][(df2['group'] == 'control')].count()
sum((df2.group == 'control')&(df2.converted == 1)) / sum(df2.group == 'control')
df.shape  # there are 8 columns and 20k rows, read is successful
columns = ['feature','price_no',"price_yes"] $ fprice_df = pd.DataFrame(columns=columns) $ fprice_df = fprice_df.fillna(0) $ print(fprice_df)
pandas.read_csv('Movies_awards.csv') $
print(temp_nc) $ for v in temp_nc.variables: $     print(temp_nc.variables[v])
r=session.get('https://python.org/')
archive_clean.info()
b_threeish = df['b'].between(2.8, 3.2)
null_vals = np.random.normal(0, diffs.std(), diffs.size)
ax = hits_df.plot(title="GitHub search hits for {} days".format(len(hits_df)), figsize=figsize) $ ax.set_xlabel('Date') $ ax.set_ylabel('# of ipynb files');
avg_preds = all_preds.mean(axis=0)
df[df['Complaint Type'].str.contains('Noise')]['Complaint Type'].value_counts()
autos.describe()
sum(df["user_id"].isin(rl))
import matplotlib.pyplot as plt $ df[['Date','GrossGasProd']].set_index('Date').plot()
varx=x.var() $ vary=y.var() $ print(varx) $ print(vary)
p_converted = convert/df.shape[0] $ p_converted
likes.dtypes
print('Outliers are points below {} or above {}.'.format((scores_firstq - (1.5 * IQR)), (scores_thirdq + (1.5 * IQR))))
cities_list = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose', 'Detroit', 'Jacksonville', 'Indianapolis', 'San Francisco', 'Columbus', 'Austin', 'Memphis', 'Fort Worth', 'Baltimore', 'Charlotte', 'El Paso', 'Boston', 'Seattle', 'Washington', 'Milwaukee', 'Denver', 'Louisville', 'Las Vegas', 'Nashville', 'Oklahoma City', 'Portland', 'Tucson', 'Albuquerque', 'Atlanta', 'Long Beach', 'Fresno', 'Sacramento', 'Mesa', 'Kansas City', 'Cleveland', 'Virginia Beach', 'Omaha', 'Miami', 'Oakland', 'Tulsa', 'Honolulu', 'Minneapolis', 'Colorado Springs', 'Arlington', 'Wichita', 'Raleigh', 'St. Louis', 'Santa Ana', 'Anaheim', 'Tampa', 'Cincinnati', 'Pittsburgh', 'Bakersfield', 'Aurora', 'Toledo', 'Riverside', 'Stockton', 'Corpus Christi', 'Newark', 'Anchorage', 'Buffalo', 'St. Paul', 'Lexington-Fayette', 'Plano', 'Fort Wayne', 'St. Petersburg', 'Glendale', 'Jersey City', 'Lincoln', 'Henderson', 'Chandler', 'Greensboro', 'Scottsdale', 'Baton Rouge', 'Birmingham', 'Norfolk', 'Madison', 'New Orleans', 'Chesapeake', 'Orlando', 'Garland', 'Hialeah', 'Laredo', 'Chula Vista', 'Lubbock', 'Reno', 'Akron', 'Durham', 'Rochester', 'Modesto', 'Montgomery', 'Fremont', 'Shreveport', 'Arlington', 'Glendale']
query = pgh_311_data_merged['Category'] == "Road/Street Issues" $ pgh_311_data_merged[query]['Issue'].value_counts()
df.A
for city,count in uniqueCities: $     if not checkCityExistence(city): $         print "Error found with city {0}".format(city)
sns.jointplot(x="age", y="fare", data=titanic)
x = [5,6,3,2,6] $ x.append(9) $ x.insert(1,200) $ x.remove(2) # remove by value $ del x[3] # remove by index $
all_zeros = 1 - val_y.mean() $ all_zeros
apple.resample('M').mean()
df2 = df.drop(df_new_page_not_treatment.index).copy() $ df2.drop(df_old_page_not_control.index, inplace=True) $ print('df2 should have: {} rows\n'.format(df.shape[0]-(new_page_not_treatment+old_page_not_control)) ) $ df2.info()
agg_trips_data.head()
s.axes #print the labels $
merged2['Specialty'].value_counts()
convo_frame.iloc[top5.index]['q']
tweet_data.tweet_id.unique().size
t3 = tweet_df.copy()
observation_counts = session.query(Measurements.station, func.count(Measurements.station)).group_by(Measurements.station).\ $ statement $ obs_counts_df = pd.read_sql_query(observation_counts, session.bind) $ obs_counts_df.sort_values(by = ["count_1"], ascending=False, inplace=True) $ obs_counts_df
ttarc_clean['floofer'] = ttarc_clean['floofer'].map({'floofer': 4, 'None': 0}) $ ttarc_clean['doggo'] = ttarc_clean['doggo'].map({'doggo': 3, 'None': 0}) $ ttarc_clean['pupper'] = ttarc_clean['pupper'].map({'pupper': 2, 'None': 0}) $ ttarc_clean['puppo'] = ttarc_clean['puppo'].map({'puppo': 1, 'None': 0})
dfWeek = dfWeek.rename(columns = {'Weighted Value': 'Weighted Value (Weekly)', $                                  'Contract Value (Daily)': 'Contract Value (Weekly)'})
temp_fine = np.zeros((13, 26, 59)) $ for i in range(13): $     temp_mon = temp[i] $     interp_spline = interpolate.RectBivariateSpline(sorted(lat), lon, temp_mon) $     temp_fine[i] = interp_spline(grid_lat, grid_lon)
np.exp(0.0506),np.exp(0.0408)
df.shape
vectorizer = TfidfVectorizer(max_df=1.0, min_df=0.0, sublinear_tf=True, $                              ngram_range=(1,3), stop_words=stoplist) $ X = vectorizer.fit_transform(train) $ X.shape
p_new = df2.query('converted == 1').user_id.count() / df2['user_id'].count() $ print(p_new)
aru_df.head()
plt.hist(p_diffs); #Was expecting a normal distribution, this is fairly close to one
df.iloc[2218]
autos = autos[autos['price'].between(1,351000)] $ autos['price'].describe()
%%bash $ cp -R models/research/object_detection/utils/. utils
df.fundedDate = [var[-1:-2]+"-"+var[0:-2] for var in df.fundedDate]
df_us = df_new[df_new['country'] == 'US'] $ df_us['converted'].mean()
df['Unique Key'] = df['OpenData Unique Key'] $ df['ComplaintID'] = df['Master SR #'] $ df['ProblemID'] = df['SR #'] $ df = df[['Unique Key', 'ComplaintID', 'ProblemID', 'Created Month']] $ df.head(2)
train_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_train_feature20180716mask_0.1.txt" $ valid_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_test_feature20180716mask_0.1.txt" $ eval_file = r"D:\Git\Sellthrough2\STR_Pipeline\temp_data\data20180716\feature20180716\mdata_noopen\Data_Train_FeatureTactic_emk_feature20180716mask_0.txt"
words_mention_scrape = [term for term in words_scrape if term.startswith('@')] $ corpus_tweets_scraped.append(('mentions', len(words_mention_scrape))) # update corpus comparison $ print('Total number of mentions: ', len(words_mention_scrape)) #, set(terms_mention_stream))
items.new[items.new.code_ogr.isin(new_items)].head()
import pandas as pd $ Google_stock = pd.read_csv('./GOOG.csv') $ print('Google_stock is of type:', type(Google_stock)) $ print('Google_stock has shape:', Google_stock.shape)
df.head(3)
df.plot('DATE_TIME','ENTRIES', figsize=(15,7)).set(ylabel='Entries')
with open("/Users/jess/.ssh/encrypted_pw.txt") as f: $     pw = f.read().strip()
df['category_main'] = [json.loads(x)['urls']['web']['discover'][:].split('/')[5] for x in df['category']] $ df['category_main'] = df['category_main'].replace({'film%20&%20video': 'film_and_video'})
airquality.info() $ oz_mean = airquality.Ozone.mean() $ airquality['Ozone'] = airquality.Ozone.fillna(oz_mean) $ print(airquality.info()) $
df.donation_date = df.donation_date.apply(pd.to_datetime) $ df.charitable = df.charitable.apply(bool) $ df.amount = df.amount.apply(int)
items = {'Bob' : pd.Series(data = [245, 25, 55], index = ['bike', 'pants', 'watch']), $          'Alice' : pd.Series(data = [40, 110, 500, 45], index = ['book', 'glasses', 'bike', 'pants'])} $ print(type(items))
grid_pr_size.plot.scatter(x='glon',y='glat', c='size_val', $                            colormap = 'RdYlGn_r')
%%bash $ python3 1st_flask_app_2/app.py
from datetime import datetime $ small_train['in_advance'] = (small_train['srch_ci'] - small_train['date_time']) / np.timedelta64(1, 'D') $ small_train['duration'] = (small_train['srch_co'] - small_train['srch_ci']) / np.timedelta64(1, 'D') $ small_train['trip_month'] = pd.DatetimeIndex(small_train['srch_ci']).month 
analyzer = SentimentIntensityAnalyzer() $ scores = [analyzer.polarity_scores(tweet)['compound'] for tweet in tweets['text'].values.tolist()]
dsdf.head()
cur.execute(my_query)
yf.pdr_override()
df_ab_cntry.country.unique()
autos['num_photos'].value_counts()
df2['intercept'] = 1 $ df2[['control','ab_page']]= pd.get_dummies(df2['group']) $ df2 = df2.drop('control',axis = 1)
grouped_dpt.aggregate([np.sum, np.mean, np.std]) # this is similar to describe() above 
skf = StratifiedKFold(n_splits=5) $ skf.get_n_splits(X, y) $ folds = [(tr,te) for (tr,te) in skf.split(X, y)]
to_be_predicted_Day3 = 43.12236678 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
for i in forecast_set: $     next_date = datetime.datetime.fromtimestamp(next_unix) $     next_unix += 86400 $     df.loc[next_date] = [np.nan for _ in range(len(df.columns)-1)]+[i]
df.plot(y = 'Close')
np.shape(prec_fine)
twitter_data.info() $
sorted(CheckNull.items(), key=operator.itemgetter(1), reverse=False)
s3 = pd.Series({'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}) $ s3
df = pd.read_csv('data/goog.csv', parse_dates=['Date']) $ df
cursor = tab.find({'527017':'$lt'}) $ for document in cursor: $     pprint.pprint(document)
seed=2210 $ train_sample = modeling1.sample(fraction=0.01, withReplacement=True, seed=seed) $ train_sample.show(3)
np.sum(x < 6, axis=1)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
sq.set_height(20) $ print(f"New dimensions: {sq.x, sq.y} - not a square.")
count_vect = CountVectorizer(analyzer=clean_text) $ X_counts = count_vect.fit_transform(tweets_1['text'])
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative="larger") $ print(f"z-score: {z_score} \np-value: {p_value}")
eug_cg_counts = aqi.groupby(['AQI Category_eug', 'AQI Category_cg']).size() $ eug_cg_counts.unstack(fill_value=0)
clean_users[clean_users['active']==1]['account_life'].mean()
to_be_predicted_Day4 = 34.57130151 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
type(target_pf['date'].iloc[0])
xgb.plot_importance(xgb_model) $ plt.show()
bounds = track.get_bounds()
data = pd.read_csv('2016-donations.csv', index_col=False)
mask = (x > 0.5) & (y < 0.5)
mars_table = table[0] $ mars_table.columns = ["Parameter", "Values"] $ mars_table.set_index(["Parameter"]) $
users = api.GetFriends() $ print([u.name for u in users])
trends_per_year_avg = defaultdict(float) $ for key,value in trends_per_year.items(): $     tr = trends_per_year[key] $     trends_per_year_avg[key] = sum(tr)/ float(len(tr))
p_diffs = [] $ for _ in range(10000): $     new_page_converted_sample = np.random.binomial(1, p_new_sample, n_new) # e $     old_page_converted_sample = np.random.binomial(1, p_old_sample, n_old) # f $     p_diffs.append(new_page_converted_sample.mean() - old_page_converted_sample.mean()) # g $
df_mes = df_mes[df_mes['trip_distance']>0] $ df_mes.shape[0]
conv_control_prob = len((df2[(df2['group'] == 'control') & (df2['converted'] == 1)])) / (df2['group'] == 'control').sum() $ print(conv_control_prob)
testObj.outDF.tail()  ## this check shows default behavior $
bus.describe() $ ins.describe() $ vio.describe() $
segments=pd.read_csv("transit_segments.csv") $ segments.head()
train.drop('second',1,inplace=True) $ train.drop('minute',1,inplace=True) $ train.drop('action',1,inplace=True) $
temperature.describe()
stocks_info_df.to_sql(con=conn_helloDB, name='company_info', if_exists='replace', index=False)
data.shape
df["ab_test_group"] = ["A" if date is not None else "B" for date in df.fitness_test_date] $ df.head(3)
%%time $  db = detective.HassDatabase(DB_URL)
control = df2.query("group == 'control'") $ prop_conv_ctrl = len(control.query("converted == '1'")) / control.shape[0] $ print('Given that an individual was in the control group, the probability they converted is {}'.format(prop_conv_ctrl))
user_logs[user_logs.msno == '29V0Jm3Xli1dy9UFeEL/BH2EMOr62DgeGLeKAKfE07k=']
h = pd.read_sql_query(QUERY, conn) $ h $
autos['odometer_km'].unique().shape
Quantile_95_disc_times_pay = df.groupby(['drg3','year']).quantile([0.1,0.9]) $ Quantile_95_disc_times_pay.head()
measure_nan = measure[measure.isnull().any(axis=1)]
tweet_archive_enhanced = pd.read_csv('twitter-archive-enhanced.csv')
indA = pd.Index([1, 3, 5, 7, 9]) $ indB = pd.Index([2, 3, 5, 7, 11])
tweet_archive_enhanced_clean = tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['rating_denominator'].notnull()]
tmp.value_counts()
pd.period_range(start="2018-04-23", end = "May-31-2018", freq = "M")
logit = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit.fit()
df_weather.drop_duplicates(['STATION_NAME','YEAR','MONTH','DAY_OF_MONTH','HOUR'],inplace=True) $ df_weather.drop('DATE',axis = 1,inplace=True)
pickle.dump(combo_models_list, open('iteration1_files/epoch3/combo_models_list.pkl', 'wb'))
subway5.reset_index(inplace=True)
df_wide = pd.DataFrame(np.random.randint(0, 100, 25).reshape(5, 5), $                     index=list('abcde'), $                     columns=list('vwxyz')) $ df_wide
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ the_files = [i for i in os.listdir() $              if i.endswith("Sepsis.pdf") and  i.startswith(starts_with_digit) ] $ the_files.sort() $ the_files
import numpy as np $ convert_old = df2.query('group == "control" & converted==1').count()[0] $ convert_new = df2.query('group == "treatment" & converted ==1').count()[0] $ n_old = df2[df2['group'] == "control"].count()[0] $ n_new = (df2[df2['landing_page']=='new_page']).count()[0]
plt.figure(figsize=(16, 5)) $ plt.plot(news.date, news.bullets.apply(len));
pd.concat([df1,df2],axis=0,join='inner')
svm_parameters = [ $                   {'C': [1, 10, 100, 1000], 'kernel': ['linear']}, $                   {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']} $                   ]
for t in tables: display(DataFrameSummary(t).summary())
lr = LinearRegression() $ scores = cross_val_score(lr, X, y, cv=5) $ print('Cross-validated scores:', scores.mean())
scr_end_date = [SCR_PLANS_df.loc[cid,'canceled_at'][np.argmax(SCR_PLANS_df.loc[cid,'scns_created'])] if SCR_PLANS_df.loc[cid,'cancel_at_period_end'][np.argmax(SCR_PLANS_df.loc[cid,'scns_created'])] == False else SCR_PLANS_df.loc[cid,'current_period_end'][np.argmax(SCR_PLANS_df.loc[cid,'scns_created'])] if SCR_PLANS_df.loc[cid,'cancel_at_period_end'][np.argmax(SCR_PLANS_df.loc[cid,'scns_created'])]==True else None for cid in scr_churned_ix]
df2_new['intercept']=1 $ logit_c=sm.Logit(df2_new['converted'],df2_new[['intercept','CA','UK']]) $ result_c=logit_c.fit() $ result_c.summary()
df2.to_csv('random_data.csv')
x = np.arange(-10, 11)
investors_df = investors_df[(investors_df.most_recent_investment >= '2015-10-15') & $                             (investors_df.investment_count > 5)].copy()
train['discussion_other'] = ((train.url.isnull()) & (~train.title.str.contains(' HN: '))).astype(int) $ train.groupby('discussion_other').popular.mean()
full['Readmitted'] = full.groupby(['Patient'])['Patient'].transform('count') $ full['Readmitted'] = full['Readmitted'].map({2:1,1:0})
properati.loc[(properati['zone'] == "") &\ $               ((properati['place_name'] == "Bs.As. G.B.A. Zona Norte") | (properati['place_name'] == "Bs.As. G.B.A. Zona Oeste") | \ $               (properati['place_name'] == "Bs.As. G.B.A. Zona Sur")),\ $               'zone'] = "G.B.A"
dfRegMet2016.head()
jscores = jcomplete_profile['scores'] $ sdf = pd.DataFrame.from_dict(jscores) $ print(sdf[['score_code','model_scope_forecast_horizon','effective_date', 'score_value']])
times = df['Timestamp +2'].value_counts().reset_index() $ times['date'] = times['index'].apply(lambda x: $                                     dt.datetime.strptime(x,'%Y.%m.%d %H:%M:%S')) $ date_list = list(times.sort_values(by='date')['index'])
day_of_month15.to_excel(writer, index=True, sheet_name="2015")
sum(df2['user_id'].duplicated()) $ print("only {} user_id duplicated in df2".format(sum(df2['user_id'].duplicated()))) $ df_rep = df2[df2.duplicated(['user_id'])]['user_id'].unique() $ print("{} is the repeated user_id in df2 ".format(df_rep))
WorldBankdf.take(2) $
ts.shift(3, freq='M')
tb.loc['All'] # column total, return a Series
df_trips.head(3)
to_be_predicted_Day4 = 31.23097052 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df_combined = df_combined[df_combined['tournament'].str.contains('World')]
twitter_archive_full[twitter_archive_full.rating_numerator == 204][ $     ['tweet_id', 'rating_numerator', 'rating_denominator'] $ ]
logs.name.value_counts()
new_page_samples = np.random.binomial(n=n_new, p=p_new, size=10000) $ new_page_samples = new_page_samples / n_new $ old_page_samples = np.random.binomial(n=n_old, p=p_old, size=10000)/n_old $ p_diffs = new_page_samples - old_page_samples $
esp.to_csv('spanish_language_feedback.csv')
df.head()
nt.head()
day_zero = weather['date'].min()
tweet_archive_enhanced_clean.iloc[221]
train_set.head(4)
prev1week_index_price = (df_byzone.time_stamp_local >= pd.datetime(2017, 12, 25)) & \ $                         (df_byzone.time_stamp_local <= pd.datetime(2017, 12, 31, 23)) $ df_byzone = df_byzone.loc[prev1week_index_price, :]
url_votes = grouped['net_votes'].agg({'total_votes': 'sum', 'avg_votes': 'mean'})    $
temp_fine = np.zeros((843, 26, 59)) $ for i in range(843): $     temp_mon = temp_us_full[i] $     interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, temp_mon) $     temp_fine[i] = interp_spline(grid_lat, grid_lon)
sigma_est = sim_closes.iloc[-1].std() $ (call.iloc[-1].Prima-1.96*sigma_est*np.exp(-r*ndays)/np.sqrt(nscen),call.iloc[-1].Prima+1.96*sigma_est*np.exp(-r*ndays)/np.sqrt(nscen))
stupid = df[df["text"].str.contains("liberal")] $ stupid['text']
df_tsv.info()
googClose = goog.reset_index(level=0, drop=True) $ googClose = googClose['Close'] $ googClose
cars = pd.read_csv('../Day2_old/Day 2 - For Corey/data/mtcars.csv') $ cars.head()
investors_df[investors_df.investment_count > 0].hist(column = 'investment_count',bins = 100, figsize = (20,8))
np.exp(-0.0150)
df.ix[0]
temp_df2['titles'] = temp_df2['titles'].str.lower()
drop_cols = ['project_is_approved','id','teacher_id'] $ X = train.drop(drop_cols, axis=1) $ y = train['project_is_approved'] $ feature_names = list(X.columns) $ del train
model.wv.doesnt_match("input is lunch he sentence cat".split())
dt_features_test['created_at'] = pd.to_datetime(dt_features_test['created_at'],unit='s')
lv_workspace.get_available_indicators(subset = 'B', step = 'step_2')
weather.head()
os.listdir()
cats.value_counts()
df_clean[df_clean.duplicated()]
results_df.describe(percentiles=None, include=None, exclude=None)
year_tobs_df.hist(bins=12) $ plt.ylabel("Frequency") $ plt.tight_layout() $ plt.show()
df_archive_clean.to_csv("twitter_archive_master.csv")
plt.scatter(e_neg['Polarity'], e_neg['Subjectivity'], alpha=0.3, color='green') $ plt.title('Negative tweets #Election2018, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
tweets_clean = tweets.copy() $ tweet_data_clean = tweet_data.copy() $ images_clean = images.copy() 
loss, accuracy = model.evaluate(test_Features, test_species_ohe, verbose=0) $ print("Accuracy = {:.2f}".format(accuracy))
dif = new_page_converted.mean() - old_page_converted.mean() $ dif
c.find_one({'born': datetime(1958, 8, 29)})
f1_A.exclude_list_filter
results.summary()
import sagemaker $ sagemaker.Session().delete_endpoint(linear_predictor.endpoint)
overlap = list(set.intersection(set(targetarticles_files_list),set(pmcids_list))) $ print(len(overlap)) $ missing = list(set(targetarticles_files_list)-set(pmcids_list)) $ print(missing)
o_data = OrderedDict(sorted(data.items(), key=lambda t:t[0]))
finaldf.to_csv(path_or_buf='SepPM.csv', sep=',', na_rep='NaN', index=False)
parks_info.head(2)
adf_check(dfs['Seasonal Difference'].dropna())
df2_prop_new = df2.query('landing_page == "new_page"').user_id.nunique() / df2.user_id.nunique() $ df2_prop_new $
twitter_df_clean.dtypes
secret_company_at_top_of_chain = pd.DataFrame(graph.run("MATCH p=(c1:Company)<-[:CONTROLS*0..]-(c2:Company)\ $ WHERE c2.uid IN {list_of_secretly_controlled_companies}\ $ RETURN DISTINCT (c1.company_number)",list_of_secretly_controlled_companies=list_of_secretly_controlled_companies).data()) $ len(secret_company_at_top_of_chain)
df_protest.loc[df_protest.towncity_name=='Johannesburg', 'start_date'].min()
from pydataset import data $ quakes = data('quakes') $ quakes.head() $ quakes.tail()
for row in reversed(station_activity):                                       # extracts station with most activity $     most_active_station=row.station $     most_active_name=str(row.name)
conmat = confusion_matrix(y_test,y_pred) $ confusion = pd.DataFrame(conmat, $                         index=['is_Bronx', 'is_Brooklyn','is_Mnhtn','is_Queens','is_StatenIsland'], $                         columns= ['predicted_BoogieDown','predicted_BK','predicted_MoneyMaken', $                                   'predicted_TheBridge','predict_Shoalin'])
df_cs['sentiment'] = df_cs['cleaned_text'].apply(sentiment_calc) $ df_cs['Polarity'] = df_cs['cleaned_text'].apply(polarity_calc) $ df_cs['Subjectivity'] = df_cs['cleaned_text'].apply(subjectivity_calc)
bus_new=bus.iloc[:,0:3] $ ins_named = ins.merge(bus_new, how="left", left_on='business_id', right_on='business_id') $ ins_named $
p_old = df2.converted.mean() $ p_old $
flights_month_airline = flights.groupby( ["AIRLINE", "MONTH"] ) $ num_flights_month_airline = flights_month_airline.size() $ num_flights_month_airline.head()
dr.columns
_ = df_pilots["rating"].plot(kind="hist", bins=11)
df_2.head()
df1=data $ del df1['day'] $ df1.head(5) $
status = api.update_status(status='1, Updating using OAuth authentication via Tweepy!') $ type(status)
df2.drop_duplicates('user_id', inplace=True) $ df2.info()
no_answer = so['ans_name'].isnull() $ no_answer.head(6)
import matplotlib.pyplot as plt $ from sklearn.metrics import roc_curve, auc $ actual =perf_test[['Default']] $ predi = predictions
ldl_values.describe().toPandas()
words = lines.flatMap(lambda line: line[0].split(" ")) $ hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1)) $ hashtag1 = hashtags.reduceByKey( lambda a, b: a + b ) $ df2 = hashtag1.map( lambda rec: Tweet(rec[0], rec[1])).toDF()
r.json()
n_new = df2[df2['landing_page'] == "new_page"]['converted'].count() $ n_new
plots.top_n_IPs(5)
users= pd.merge(avg_reorder_days,avg_usercart_size) $ users_fin=pd.merge(users,total_order_per_user) $ users_fin.head()
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ os.getcwd()
fig = plt.figure(figsize=(10,10)) $ ax = fig.add_subplot(111) $ ax.axis('off') $ pumashp.plot(column='pcBB',ax=ax,legend=True)
iberia_values = df_iberia.text_3.str.split(expand=True).stack().value_counts()\ $     .reset_index().rename(columns={'index': 'iberia_word', 0:'iberia_value'}) $ NOT_iberia_values = df_NOT_iberia.text_3.str.split(expand=True).stack().value_counts()\ $     .reset_index().rename(columns={'index': 'NOT_iberia_word', 0:'NOT_iberia_value'})
countries.country.unique()
if 'Sat' in s1.index: $     print(s1.loc['Sat'])
ben_final['revtime'] = pd.to_datetime(ben_final['revtime'])
air_rsrv.tail(10)
d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), $      'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} $ df = pd.DataFrame(d) $ df
n = 10 $ p = 3 $ Z = np.zeros((n,n)) $ np.put(Z, np.random.choice(range(n*n), p, replace=False),1) $ print(Z)
df.std()
age_table = train.pivot_table(index = 'Age', values = 'Fare', aggfunc=np.mean)
commits['log10_commits'] = np.log10(commits['commits'])
tweets['created_at'] = pd.to_datetime(tweets['created_at'])
pcaData = pd.DataFrame(data=pcaData) 
json_ex1 = json_df.iloc[0,:].to_json() $ json_ex2 = json_df.iloc[1,:].to_json() $ print json_ex1 $ print $ print json_ex2
my_df["company_create"].plot() $ my_df["company_active"].plot() $ plt.ylabel("Ncompany") $ plt.legend() $ plt.show()
loan_requests_indebtedness.head()
average_math_score = df_students['math_score'].mean() $ average_math_score
reddit['Time']=(pd.to_datetime(reddit['Time'],unit='ms')) 
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724 = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM $ pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724, open( "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p", "wb" ) )
max_price = autos["price"] == autos["price"].max()
traffic_df_rsmpld = traffic_df_byday.reset_index().groupby('LINK_ID').apply(lambda x: x.set_index('DATETIME').resample('1M').median()).swaplevel(1,0) $ traffic_df_rsmpld.info() $ traffic_df_rsmpld.head()
yt.get_featured_channels(channel_ids, key)
import statsmodels.api as sm $ logit   = sm.Logit(ab_file2['converted'], ab_file2[['intercept','ab_page']]) # creating logistics for our query $ results = logit.fit()
bad_iv_post.groupby(['Strike']).count()['Expiration']
pd.date_range('2017-12-30', periods=4, freq='h')  # 4 values using hourly frequency 
df2.reset_index(inplace=True);
train_ratio = 0.75 $ train_size = int(samp_size * train_ratio); train_size $ val_idx = list(range(train_size, len(df)))
df.at['20180101','D'] = 0.3 $ df
print('waiting...', end='') $ time.sleep(2) $ print('done!', end='')
df3 = sm.Logit(df2['converted'], df2[['intercept','ab_page']])  #Implementing OLS model to fit to the regression line. $ df3 = df3.fit()
pd.Series(pd.Categorical(iris["Species"])).sample(5)
(df_final[df_final['Scorepoints'] == 200]).head()
converted_users = df2.converted.mean() $ print("The probability of an individual converting regardless of the page they receive is {0:.4}".format(converted_users))
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2, index = ['store 1', 'store 2']) $ store_items
s = pd.Series([2, 3, 1, 5, 3], index=['a', 'b', 'c', 'd', 'e']) $ s.plot()
bins = np.arange(10, 99, 5) $ axM = df.age.groupby(pd.cut(df.age, bins)).agg([count_nonzero]).plot(kind='bar', $                                                                 legend=False) $ axM.set_title("Riders"); $
pd.to_datetime(['2005/11/23', '2010.12.31'])
features['f16'] = features['f05'].apply(np.sign) $
test = single_patient[single_patient.field_stream == 'gsr'] $ test.reset_index(drop=True, inplace=True) $ test.set_index('parsed_ts', inplace=True) $ test = test.cumsum()
idx = pd.IndexSlice $ hdf.loc[idx[['adult', 'child'], ['Alcoholic Beverage', 'Choc/Cocoa Prod']], :].head()
review_length = [] $ for i in range(len(review_body)): $     count = len(re.findall(r'\w+', review_body.loc[i])) $     review_length.append(count)
for p in popups: $     if p.findChild().text == 'MLS Player Salaries': $         salary_folder = p.findParent()
connection = sqlite3.connect("stocks.sqlite") $ query = "SELECT * FROM STOCK_DATA WHERE Volume>29200100 AND Symbol='MSFT';" $ items = pd.io.sql.read_sql(query, connection, index_col="index") $ connection.close() $ items
def search(inp): $   inp = vectorize_string(inp) $   ids, dist = index.get_nns_by_vector(inp.squeeze(), 10, include_distances=True) $   for i, dist in zip(ids, dist): $     print(f'dist: {dist:.2f}\n{source_docs[i]}\n')
test_data.isnull().sum()
countries = pd.get_dummies(df2['country']) $ df2 = df2.join(countries)
cercanasA1_11_14Entre50Y75mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 50) & (cercanasA1_11_14['surface_total_in_m2'] < 75)] $ cercanasA1_11_14Entre50Y75mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre50Y75mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre50Y75mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
prcp_df = pd.DataFrame(prcp_data, columns=['Date', 'Precipitation']) $ prcp_df.set_index('Date', inplace=True) $ prcp_df.head()
ben_out.to_csv('benign_out.csv') $ van_out.to_csv('vandal_out.csv')
def replace_newlines(text): $     return re.sub('\s+', ' ', text) $ replace_newlines('Hi \n\n\tee')
SeriesPandas1 = pd.Series([1,'a',2,3,np.nan,5,6,7]) $ SeriesPandas1
imbalance_df = pd.read_csv(data_folder_path + '/imbalance/tennet16to18.csv')
writers.groupby('Country').first()
j.head()
ins.groupby('year').count()
df2.query('landing_page == "old_page" and group == "treatment"')
dates=pd.date_range('1/Oct/2020', '5/Oct/2020') $ print(dates)
score_b = score[(score["score"] < 90) & (score["score"] >= 80)] $ score_b.shape[0]
my_tag={'name':'img','title':'Sunset boulevard', $        'src':'sunset.jpg','cls':'framed'} $ tag(**my_tag)
import re $ def clean_tweet(tweet): $         return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())
filtered_greater_100.count()
data['Created Date'].head()
X_train, X_test, y_train, y_test = train_test_split(X_, y_q, test_size=0.2, random_state=2)
len(df.query('group=="treatment" and landing_page !="new_page"')) + len(df.query('group!="treatment" and landing_page =="new_page"'))
old_page_converted = np.random.choice([1, 0], n_new, (p_old, 1-p_old))
!pip3 install Pillow==4.0.0
import matplotlib.pyplot as plt $ %matplotlib inline $ aapl[['Close', 'Adj Close']].plot(figsize=(8,6));
from sklearn.linear_model import LogisticRegression $ from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
GSW_2017["Tm.3PM"].mean()
hour_graph= df.groupby(df.index.hour).count().plot(y='Unique Key', legend=False) $ hour_graph.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]) $ hour_graph.set_ylabel("Complaints") $ hour_graph.set_xlabel("Time")
s2.index = s2.index.values.astype(int) $ s1 + s2
pokemon['Legendary'] = np.where(pokemon['Legendary'] == True, 1, 0) $
datAll['zip'] = datAll['zip'].str.replace(' ','')
p.to_timestamp()
act_diff = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ print("{} is the actual difference observed in ab_data.csv. ".format(act_diff)) $ p_diffs = np.array(p_diffs) $ print("{} is the proportion of the p_diffs are greater than the actual difference .".format((act_diff < p_diffs).mean())) $
train_X = sparse.hstack([train_df[features_to_use], tr_sparse,tr_sparse1]).tocsr() $ test_X = sparse.hstack([test_df[features_to_use], te_sparse,te_sparse1]).tocsr() $ target_num_map = {'high':0, 'medium':1, 'low':2} $ train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x])) $ print(train_X.shape, test_X.shape)
odds = vis["Points"].plot(kind = "bar", figsize = (25, 5), title = "Total FIFA Points by Country", rot = 90, legend = True) $ odds.set_ylabel("FIFA Official Ranking Points", fontsize = 15) $ plt.show()
one_yr_tobs = session.query(measurements.tobs).\ $     filter(measurements.station == active_stations[0][0]).\ $     filter(measurements.date >= year_ago).all() $ bins_df = pd.DataFrame(one_yr_tobs, columns=['tobs']) $ bins_df.head()
new=df2.query('landing_page=="new_page"') $ new_samp=new.sample(new.shape[0],replace=True) $ new_page_converted=new_samp.query('converted==1')['user_id'].count()/new_samp['user_id'].count() $ new_page_converted
! ls ./data/raw-news_tweets-original/dataset1/news
n_old = len(df2.query("group == 'control'")) $ (n_old)
dtm_pos['pos_proportion'] = dtm_pos['pos_count']/dtm_df.sum(axis=1) $ print(dtm_pos['pos_proportion']) $ df['pos_prop']
a.find('t')
labels2idx = {label: i for i, label in enumerate(clf.classes_)} $ sub = pd.DataFrame() $ sub["listing_id"] = test_df["listing_id"] $ for label in ["high", "medium", "low"]: $     sub[label] = test_prediction[:, labels2idx[label]]
theft = crimes[theft_bool] $ theft.head()
print("Mean absolute percentage error: %.3f" % mean_absolute_percentage_error(y_test, y_pred) + '%')
df2_tr=df2[df2['group']=="treatment"] $ prob2 = df2_tr[df2_tr['converted']==1].shape[0]/df2_tr.shape[0] $ prob2
df.text.isnull().sum() $ df.text.fillna('', inplace=True)
X_train[X_train.isna().any(axis=1)]
top10.plot.pie( $     figsize=[5,5], $     title="Top 10 Authors", $     label="");
clean_appt_df.groupby('Gender')['No-show'].value_counts(normalize=True).plot.bar()
data = pd.read_csv('time_series.csv', index_col = 'time')
department_df["Revenue_Total"] = grouped_dpt["Revenue"].transform('sum') # transform revenue to be total departmental revenue $ department_df
df.rename(columns={'Indicator':'Indicator_id'}, inplace=True) $ df.head(2)
df5.country.value_counts()
control_group = df2.query('group == "control"') $ pconversion_control = control_group['converted'].mean() $ pconversion_control
plt.clf() $ plt.plot(res.seasonal[6:13],color = 'black') $ plt.show()
from dateutil.parser import parse $ parse('Sept 15th 2020')
plt.figure(figsize=(15,5)) $ sns.countplot(auto_new.Body_Type)
low = out_df.copy() $ low['low']=1 $ low['medium']=0 $ low['high']=0 $ low.to_csv('all_low.csv',index=False)
combined_df3=combined_df2[combined_df2['fin_year']==201718.0]
neigh = KNeighborsClassifier(n_neighbors = 9).fit(X,y)
data_compare['SA_textblob_de'].mean()
print (tweets.shape)
time_spent_by_date = merged2.groupby([merged2.index.date, 'Specialty'])['DurationHours'].sum()
df_meta.loc[df_meta.user_id == '379881d5-32d7-49f4-bf5b-81fefbc5fcce'].head(3)
archive_copy['name'].loc[archive_copy['name'].str.islower()]
fakeNews = trump.loc[trump['text'].str.contains("fake"),:] $ ax = sns.kdeplot(fakeNews['year'], label="'Fake' word  usage") $
pd.read_csv("../data/microbiome/microbiome.csv", skiprows=[3,4,6]).head()
print "Mean time for closing a ticket in 2015: %f hours" % (time2close_2015.mean()/3600.0) $ print "Median time for closing a ticket in 2015: %f hours" % (time2close_2015.median()/3600.0) $ print "Quantiles: " $ print time2close_2015.quantile([0.25, 0.5, 0.75])
df.columns[non_null_counts < 1000]
plt.figure(2) $ df['GoldsteinScale'].plot.hist(bins=(20),legend=True) $ plt.figure(3) $ df['GoldsteinScale'].plot.kde(legend=True)
float(trips_sorted_pilot["overlap"].sum()) / len(trips_sorted_pilot)
print(seen_and_click.shape) $ seen_and_click[0:20]
base_url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json" $ api_url = "&api_key="+API_KEY $ url = base_url+"?limit=1"+api_url $ r = requests.get(url)
Z = np.zeros((5,5), [('x',float),('y',float)]) $ print(Z) $ Z['x'], Z['y'] = np.meshgrid(np.linspace(0,1,5), $                              np.linspace(0,1,5)) $ print(Z)
autos.info()
data.to_csv("csvs/datosConDuplicados.csv", index = False)
nba_df.describe()
mock_data = mock_data[(mock_data['ABPm_x'].isnull() == False) & (mock_data['ABPm_y'].isnull() == False) & (mock_data['ABPm'].isnull() == False) & (mock_data['ABPm'] > 0)].head(10) $ mock_data.head(2)
model1 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results1 = model1.fit()
plt.style.use('seaborn') $ breed_ax = breed_conf[0:51].plot(kind = "bar",figsize=(20,8)) $ breed_ax.set_title("Breeds with Highest Image Recognition", fontsize = 24) $ breed_ax.set_xlabel("") $ plt.savefig('plots/breed_ax.png', bbox_inches='tight')
df_clean3.loc[1635, 'text']
df.query('converted == 1').count()/df['user_id'].nunique()
plt.hist(taxiData.Trip_distance, bins = 100, range = [taxiData.Trip_distance.min(),50]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
df.head()
db = client['instagram-london'] $ coll = db.posts
plt.plot(subway['datetime'], subway['Hourly_Entries'])
df4['country'].value_counts()
final_rm=final.drop(target0.index) $ X=create_time_list(final_rm)
json_data = r.json()
np.array(df[['Visitors','Bounce_Rate']])
movie_ratings['year'].head(3)
spark.sql('select * from my_analysis_work.example_timeseries limit 10').toPandas()
df[df['C'].isin(['can', 'fish'])]
np.percentile(p_diffs,2.5), np.percentile(p_diffs,97.5)
b.find_by_xpath('//*[@id="day-section"]/div/div[3]/div[1]/div/ul/li[5]/button').click()
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=56000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
for feed in feeds: $     print("feed {}".format(feed)) $     agency = json.loads(requests.get('{}index/agencies/{}'.format(base_url, feed)).text) $     print(agency) $     print('') $
print "The mean of the preTest scores is " + str(np.mean(df.preTest)) $ print "The mean of the postTest scores is " + str(np.mean(df.postTest))
fix_space = lambda x: pd.Series([i for i in reversed(x.split(' '))])
stock.target_class.value_counts()
df_archive_clean.text[24]
page_soup.title
price = autos['price'] $ odometer = autos['odometer_km'] $ price.unique().shape
df.simple_style.value_counts()[:20].plot(kind='barh');
top_rsvp = df.sort_values("last_rsvp", ascending=False).reset_index(drop=True) $ top_rsvp.head(5)
for dimension in ['Parcel Status']: $     multi_table([ $     pd.DataFrame(df[df['Created At' ] > df['Shipped At']].groupby(dimension)['Created At'].count()), $     pd.DataFrame(df[df['Created At'] <= df['Shipped At']].groupby(dimension)['Created At'].count())])
data1.shape
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=18000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df_customers.tail()
meany_train, stdevy_train, y_train = normy(joined_train.as_matrix(columns = ['unit_sales'])) $ meany_valid, stdevy_valid, y_valid = normy(joined_valid.as_matrix(columns = ['unit_sales']))
def predict_row(row, theta): $     hx = sigmoid(np.dot(row, theta)) $     return hx
from sklearn.preprocessing import Imputer $ trainDataVecs = Imputer().fit_transform(trainDataVecs)
titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()
df2 = pd.DataFrame(np.arange(12.0).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])
poverty.reset_index(drop=True, inplace=True)
p_diff = p_new - p_old $ print("Difference in probability of conversion(not under H_0) is {}".format(p_diff))
tweet_df_clean.rename(columns={'id':'tweet_id'},inplace=True)
result = api.search(q='%23Australia')  # "%23" == "#" $ len(result)
plt.hist(p_diffs, bins=21) $ plt.axvline(p_convert_obs_diff, color='green') $ plt.axvline(np.percentile(p_diffs, 95), color='red');
row = df2.loc[df2['user_id']==773192].index $ print("The row information for the repeat user_id is : {} ".format(row[1]))
All_tweet_data_v2.rating_numerator.value_counts().sort_index()
import numpy as np $ data['position'] = np.where(data['SMA1'] > data['SMA2'], 1, -1) $ data.dropna(inplace=True) $ data['position'].plot(ylim=[-1.1, 1.1], title='Market Positioning')
temps_df.ix[1].index
number_unpaids = data.groupby('customer_id').agg({'score': max, 'paid_status': lambda x: np.sum(x == 'UNPAID')})
a_list = [1, 1.0, 'one', '1', 100] $ a_list
GroupZip.corr()
pandas_ds["time2close"].plot(kind="hist")
users_visits = users_visits.groupby('chanel', as_index=False).sum() $ users_visits = users_visits.assign(logins=lambda x: x.visits/x.user) $ users_visits
merged_df.plot(x='state', y='Percent margin',  kind='bar') $ merged_df.plot(x='state', secondary_y=True, y='ad_duration_secs',  kind='bar')
api = twitterUtils.getTwitterApi() $ twitterUtils.filterTwitterData(twitterUtils.getCurrentTwitterData(api), "(F|f)acebook")
git_log.timestamp.head()
df2_new.country.value_counts()
sox['hour'] = sox.START_TIME.str.slice(0,2).astype(np.int) $ sox['minute'] = sox.START_TIME.str.slice(3,5).astype(np.int) $ sox.ix[sox.START_TIME.str.contains(' PM'),'hour'] += 12 $ sox.minute = (sox.minute / 15).astype(np.int) * 15 $ sox['time'] = sox['hour'].astype(np.str).str.pad(2, fillchar='0') + ':' + sox['minute'].astype(np.str).str.pad(2, fillchar='0') + ':00'
df8 = df7.set_index(pd.DatetimeIndex(df7['Date'])) $ df8 = df8[['BG']].copy() $ df8_lunch = df8.between_time('11:00:00', '13:00:00') $ df8_lunch
dfLikes.dropna(subset=["created_time"], inplace=True)
model.resid.plot()
con = sqlite3.connect('db.sqlite') # open connection $ con.close() # close connection, important to prevent data losses
croppedFrame.to_csv('./data/lobbyistByClient.csv', index = False)
S_1dRichards.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
get_response('I mean I didn\'t actually name it.')
import itertools $ model = tf.contrib.learn.LinearRegressor( $       feature_columns=make_feature_cols(), model_dir='taxi_trained') $ preds_iter = model.predict(input_fn=make_input_fn(df_valid)) $ print list(itertools.islice(preds_iter, 5)) # first 5
afl_modelling_v2.create_predictions()
cur.close() $ conn.close()
grid_svc.best_params_
pd.DataFrame(np.array([[10, 11], [20, 21]]))
print(j.json()) $
new_page_converted = np.random.choice(a=[0, 1], $                                       p=[1-p_new, p_new], $                                       size=n_new) $ new_page_converted
male_journalists_retweet_df = journalists_retweet_df[journalists_retweet_df.gender == 'M'] $ male_journalists_retweet_by_gender_df = pd.merge(user_summary_df[user_summary_df.gender == 'M'], male_journalists_retweet_df.groupby(['user_id', 'retweet_gender']).size().unstack(), how='left', left_index=True, right_index=True)[['F', 'M']] $ male_journalists_retweet_by_gender_df.fillna(0, inplace=True) $ male_journalists_retweet_by_gender_df['all'] = male_journalists_retweet_by_gender_df.F + male_journalists_retweet_by_gender_df.M $ male_journalists_retweet_by_gender_df.describe()
print(input_hash_tag.value)
print "Mean time for closing a ticket in 2014: %f hours" % (time2close_2014.mean()/3600.0) $ print "Median time for closing a ticket in 2014: %f hours" % (time2close_2014.median()/3600.0) $ print "Quantiles: " $ print time2close_2014.quantile([0.25, 0.5, 0.75])
df_en.to_csv('bitcoin_eng.csv') # this has all 4 bitcoins $
tweet = pd.read_csv("tweet_json.txt", encoding = 'utf-8') $ tweet.info()
forecast_data = forecast.set_index('ds') $ forecast_data = forecast_data[forecast_data.index.date > (dt(2018, 1, 1).date())]
default = donations['Donation Amount'].value_counts().head(10) $ default.plot(kind='bar') $ plt.title('Donation Amount') $ plt.ylabel('frequency') $ plt.xlabel('Dollars Donated')
stop = np.floor(len(X)/4).astype(int) $ model.fit(X[:stop], y[:stop], epochs=200, batch_size=128*2) $
df = pd.DataFrame() $ variables = ['text','created_at','source','user'] $ df = pd.DataFrame([[getattr(i,j) for j in variables] for i in tweets], columns = variables)
train_df.shape
head = pd.Timestamp('20150617') $ tail = pd.Timestamp('20150628') $ df=site.get_data(sensortype='electricity', head=head,tail=tail, diff=True, unit='kW') $ charts.plot(df, stock=True, show='inline')
df_new.country.value_counts()
weather_norm = weather_features.apply(lambda c: 0.5 * (c - c.mean()) / c.std())
clicks.head(2)
pd.datetime(2016,6,2) - pd.datetime(2016,6,22)
clean_madrid['created_at'] = pd.to_datetime(clean_madrid.created_at).dt.date $ clean_madrid['created_at'] = pd.to_datetime(clean_madrid.created_at, errors='coerce')
from tatk.operationalization.csi.csi_web_service import CsiWebService $ url = "<please insert the url here>" $ key = "<please insert the key here>" $ web_service = CsiWebService(url, key)
tm['energy'].mean() $ tm['energy'].var() $ tm['friendliness'].mean() $ tm['friendliness'].var()
dfHaw_Discharge['flow_MGD'] = dfHaw_Discharge['meanflow_cfs'] * 0.64631688969744
offseason17 = ALL[(ALL.index > '2017-02-05') & (ALL.index < '2017-09-07')]
sunspots.index = sunspots['year_month_day'] $ sunspots.index.name = 'date'  # change the col name to date $ sunspots.info()
xgb = XGBRegressor(gamma=0, learning_rate=0.02, max_depth=3, n_estimators=100) $ xgb.fit(X_train, y_train)
TEXT.vocab.itos[2]
df_api.head()
np.exp(-0.0150) $ 1/np.exp(-0.0150)
s.index
df.loc[df.PredictedIntent == "Problem",["display_time","Notes","PredictedIntent"]].head(20)
stars = pd.read_csv('data/new_subset_data/new_subset_stars.csv', sep='\t') $ stars.info() $ stars.head()
old_page_converted = np.random.choice([0,1],n_old,p_old); $ old_page_converted
n_new = df2[ df2['landing_page'] == 'new_page' ]['user_id'].count() $ n_new
df_A=pd.DataFrame({"Student_height":heights_A,"Student_weight":weights_A})
LR_yhat = LR.predict(test_X) $ LR_yhat_prob = LR.predict_proba(test_X) $ print("LR Jaccard index: %.2f" % jaccard_similarity_score(test_y, LR_yhat)) $ print("LR F1-score: %.2f" % f1_score(test_y, LR_yhat, average='weighted') ) $ print("LR LogLoss: %.2f" % log_loss(test_y, LR_yhat_prob))
df_all_columns.head()
stocks = r.json()['dataset'] $ print(stocks.keys()) $ print(stocks['column_names']) $ print(stocks['data'][0])
print(pandas_list_2d_rename.loc[:, 'Name'])
model.predict_proba(np.array([2,50]))
vlc[pd.isnull(vlc.bio.str.strip())].id.size
print(psy_hx.shape) $ psy_hx = psy_hx[psy_hx["subjectkey"].isin(incl_Ss)] $ psy_hx.shape $
lower_case = letters_only.lower()        # Convert to lower case $ words = lower_case.split()               # Split into words $ words[:10]
coef_df = pd.DataFrame.from_records(coef) $ coef_df.plot.barh() $ plt.legend([]) $ plt.yticks(range(0, len(coef_df)), coef_df[0])
df_positive.loc[169,['Tweets']]
witf = open("latlong_test.txt","w", encoding="utf-8") $ for i in range(len(tweets_kyoto_filter)): $     witf.write("{location: new google.maps.LatLng(%f, %f)},\n" % (tweets_kyoto_filter['ex_lat'][i],tweets_kyoto_filter['ex_long'][i])) $ witf.close()
df_clean['timestamp'].dtype
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_axial_chord_saddle[0][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_saddle[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
log_with_day = access_logs_df.withColumn('dayOfWeek',weekday('dateTime')) $
df_tweet_json.head()
df1['Year'] = pd.to_datetime(pd.Series(df1['Year']).astype(int),format='%Y').dt.year $ df1.tail()
df.drop(df[df.amount == 0].index, axis=0, inplace=True)
pct.to_csv('pct_data_4_18.csv')
noise = df[df["Complaint Type"].str.contains("Noise")] $ noise["Complaint Type"].value_counts()
df[df.index.month.isin([1,2,3])].head()
churned_unordered = unordered_df.loc[churned_unord]
oil_prices.drop(labels='dcoilwtico', axis=1, inplace=True)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
df_2.shape
capa2017onshore.head()
!h5ls 'data/file1.h5'
list(tweet_archive_clean.columns.values)
(null_vals > -0.001576).mean()
plt.bar(rscale, numpeo) $ plt.xlabel("Rating") $ plt.ylabel("Number of People") $ plt.title("Number of ratings by rating", weight = 'bold', fontsize = 15) $ plt.grid(False)
def append_element(some_list, element): $     some_list.append(element)
val.encode('latin1') $ val.encode('utf-16') $ val.encode('utf-16le')
jpncodes = ["CRDQJPAPABIS","DDDM01JPA156NWDB","QJPHAM770A","DDOI02JPA156NWDB","MKTGDPJPA646NWDB", "DEXJPUS"] $ startj = dt.datetime(1975,1,1) $ japcolumns = ["Credit_2NonFinSec","Mkt_Cap2GDP", "Household_Credit", "Deposits_2GDP", "GDP", "Yen2USD" ] $ japandata= data.DataReader(jpncodes, "fred", startj) $ japandata.columns = japcolumns
questions = pd.concat([questions.drop(['month_bought'], axis=1), month], axis=1)
df['intercept'] = 1 $ log_mod = sm. Logit (df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'US']]) $ results = log_mod.fit() $ results.summary()
ufrom =txns['from'].unique() $ uto = txns['to'].unique() $ uboth = [val for val in ufrom if val in uto] $
pf.create_full_tear_sheet(returns, positions=positions, transactions=transactions, $                           gross_lev=gross_lev, live_start_date='2016-08-31', round_trips=True)
drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15]) * 0.7
from sklearn.linear_model import LogisticRegressionCV $ lr = LogisticRegressionCV(Cs=5, n_jobs=-1, random_state=42)
le_data_all = wb.download(indicator="SP.DYN.LE00.IN",country=countries['iso2c'],start='1980',end='2012') $ le_data_all
train.pivot_table(values = 'Fare', index = 'From-To', aggfunc=np.mean)
cleaned2 = pd.DataFrame(tweets)
from fastai.structured import * $ from fastai.column_data import * $ np.set_printoptions(threshold=50, edgeitems=20) $ PATH='data/'
train['NTACode'] = train.NTACode.fillna('Non-NYC') $ test['NTACode'] = test.NTACode.fillna('Non-NYC')
twitter_archive_master['name'] = twitter_archive_master['full_text'].str.extract('([^\.\?]\s([A-Z][a-z]+)\s[^A-Z]|^Meet\s([A-Z][a-z]+)\.|^This\sis\s([A-Z][a-z]+)\.)').astype(str)
niners_td = nfl[((nfl["HomeTeam"] == 'SF') | (nfl["AwayTeam"] == 'SF')) $              & (nfl["sp"] == 1) & (nfl["Touchdown"] == 1) & (nfl["DefensiveTeam"] != 'SF') & pd.isnull(nfl["Interceptor"]) ]
df = final_annotations_df.dropna() $ df[df.young_present]['choice'].value_counts()
train[train['is_holiday'] == 1].head(3)
tsne_input.head()
from sklearn.metrics import r2_score $ predictions = automl.predict(X_test) $ print("R2 score:", r2_score(y_test, predictions))
pd.DataFrame( $     np.array(status_keys + [""]).reshape(5, 5) $ )
url = 'https://mars.nasa.gov/news/'
typesub2017['MTU2'] = typesub2017.MTU.str[:16] $ typesub2017['DateTime'] = pd.to_datetime(typesub2017['MTU2'])
le = preprocessing.LabelEncoder()
fb = cb.organization('Facebook')
scalable_variables = ['unit_sales','dcoilwtico'] $ for var in scalable_variables: $     mini, maxi = pd_train_filtered[var].min(), pd_train_filtered[var].max() $     pd_train_filtered.loc[:,var] = (pd_train_filtered[var] - mini) / (maxi - mini)
autos.head()
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','c1','c2']]) $ res = logit_mod.fit() $ res.summary()
for i in ['vo_propdescrip','empty_prop', $                    'llpg_usage', 'Ward', 'paymeth_code']: $     print(i,(pd.get_dummies(combined_df4, columns=[i])).shape)
data = pd.DataFrame(np.arange(16).reshape((4, 4)), $                    index=['Ohio', 'Colorado', 'Utah', 'New York'], $                    columns=['one', 'two', 'three', 'four'])
filepath = os.path.join('input', 'input_plant-list_SE.csv') $ data_SE = pd.read_csv(filepath, encoding='utf-8', header=0, index_col=None) $ data_SE.head()
df_train = df_train.drop(high_null_columns, axis=1) $ df_test = df_test.drop([ c for c in high_null_columns if c in df_test.columns], axis=1)
df2_page = df2['landing_page'].value_counts() $ df2_page
!cat crossref-by-doi/*.json | jq -r .message.DOI | grep ',' | wc -l
bigram_sentences_filepath = paths.bigram_sentences_filepath
df2[df2.user_id.duplicated() == True]
test.shape
cbs_df = constructDF("@CBS") $ display(constructDF("@CBS").head())
full_data = my_data.append(my_data2) $ len(full_data)
train_user['timestamp_first_active'] = pd.to_datetime(train_user['timestamp_first_active'], format='%Y%m%d%H%M%S')
np.count_nonzero(np.any(nba_df.isnull(), axis = 0))
vol.describe()
cog_simband_times = cog_simband_times.drop(labels=['ST-OTS BHC0066-1'])
df_subset['Existing Zoning Sqft'].describe()
df9 = pd.read_csv('2009.csv')
for row in nocachedf.itertuples(): $     print(row.fname_wav)
df2 = df.drop(df[(df.landing_page == "new_page") & (df.group != "treatment")].index) $ df2 = df2.drop(df[(df.landing_page == "old_page") & (df.group != "control")].index) $
k_data1 = pd.read_csv('Kickstarter_2016-07-15/Kickstarter038.csv') $ print k_data1.columns.values $ k_var1 = k_data1[['name','blurb','launched_at','state_changed_at','state']] $ k_var1.head() $ len(k_var1)
extract_all['system.record_id'].unique()
contribs.amount.sum()
tobs_values_df=pd.DataFrame([tobs_values]).T $ tobs_values_df.head()
list5 = [1, 2, 4, 3, 'k'] $ print ''.join(map(str, list5))
folium.GeoJson(watershed).add_to(m);
train_df.nunique()
diff = new_page_converted.mean() - old_page_converted.mean() $ print(diff)
training_set, test_set = newdf[newdf['date']<split_date], newdf[newdf['date']>=split_date] $ training_set = training_set.drop('date', 1) $ test_set = test_set.drop('date', 1)
xs_df = valid_scores[['home team 41 game win%','away team 41 game win%', 'home team 8 game win%', $                       'away team 8 game win%']] $ xs_df.head()
state = "PA" $ data = {'state': state, 'city': cities} $ df = pd.DataFrame(data) $ print(df) $
predictortVar = ['intercept', 'ab_page'] $ logReg = sm.Logit(df2['converted'], df2[predictortVar]) $ answer = logReg.fit()
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new],[n_old,n_new])
labels =list(crf.classes_) $ y_pred = crf.predict(X_valid) $ metrics.flat_f1_score(y_valid, y_pred, $                       average='weighted', labels=labels)
df_transactions['amount_per_day'] = df_transactions['amount_per_day'].replace([np.inf, -np.inf], 0)
temp_df['reorder_interval_group'].replace('', np.nan, inplace=True) $ temp_df.dropna(subset=['reorder_interval_group'], inplace=True) $ temp_df.reorder_interval_group.apply(int)
tag_pairs = hashtags[["id","screen_name","hashtag"]].groupby(["screen_name","hashtag"]).agg({"id":"count"}).rename(columns={"id":"Weight"}) $ tag_pairs.reset_index(inplace=True) $ tag_pairs.head()
df['text_stemming'] = df['text_cleaned'].apply(lambda sequence: ' '.join(stemmer.stem(word) for word in sequence.lower().split()))
active_stations = session.query(measurements.station, func.count(measurements.station)).\ $             group_by(measurements.station).\ $             order_by(func.count(measurements.station).desc()).all() $ active_stations
donations.head()
sns.lmplot(x='hour', y='start', data=hours, aspect=1.5, scatter_kws={'alpha':0.2})
cleaned_asf_people_human_df_saved.count()
A = pd.Series(index=posts_w['date_week'],data=posts_w['postsCount'].values)
weather_yvr_dt['Temperature (C)'].plot();
100.0*df_outcomes.sum()/len(df_outcomes)
result.tostring()
df_joined.groupby('group').mean()['converted']
summary_ms = df_measures_users[df_measures_users['ms']==1][score_variable].describe() $ print('Average Score of MS Group: {} (SD {})'.format( $     round(summary_ms.loc['mean'],2), round(summary_ms.loc['std'],2)))
df.index = df['Difference']
train['frequent_author'] = train.author.isin(frequent_authors.index).astype(int) $ train.groupby('frequent_author').popular.mean()
dist = np.sum(X, axis=0) $ for tag, count in zip(vocab, dist): $     print (count,tag)
data.resample('3T').mean().plot()
treatment_conversion = df2[df2['group'] == 'treatment']['converted'].mean() $ print(treatment_conversion) $ treatment_rounded =round(treatment_conversion, 4) $ print(treatment_rounded)
pd.Timestamp('2014-12-15')
y_test_over[k150_bets_over].mean()
tfv = TfidfVectorizer(ngram_range=(1,5), max_features=2000) $ X2 = tfv.fit_transform(clean_text2).todense() $ print X2.shape
week11 = week10.rename(columns={77:'77'}) $ stocks = stocks.rename(columns={'Week 10':'Week 11','70':'77'}) $ week11 = pd.merge(stocks,week11,on=['77','Tickers']) $ week11.drop_duplicates(subset='Link',inplace=True)
train['year'] = pd.to_datetime(train['date']).dt.year $ train['month'] = pd.to_datetime(train['date']).dt.month $ train['weekday'] = pd.to_datetime(train['date']).dt.weekday
tweets_possible_bots.query("isaRetweet==1").shape
str(containers[0].find("li", {"class":"transaction"}).a["title"])
tweets_df[~(tweets_df.place.isnull())]
results = pd.read_csv('../data/result.csv', $                       low_memory=False      #This is required as it's a large file... $                      )
for o in ['Before', 'After']: $     for p in columns: $         a = o + p $         df[a] = df[a].fillna(0).astype(int)
vocabulary_expression['component_5'].sort_values(ascending=False).head(7) $
criteria = so['ans_name'] == 'Scott Boston' $ so[criteria].head()
h5.close()
kmeans = KMeans(n_clusters=3) $ kmeans.fit(X_scale) $ y_kmeans = kmeans.predict(X_scale) $ y_kmeans
rdd.map(lambda x: x**2 + really_large_dataset).repartition(4).map( $ lambda x: x ** 2 + really_large_dataset).collect()
excelDF['Ship Date'] = excelDF['Ship Date'].apply(lambda x: x.strftime('%Y-%m-%d'))
r.json()['dataset']
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred_clf))
import numpy as np $ import matplotlib.pyplot as pp $ import pandas as pd $ import seaborn $ %matplotlib inline
order = pd.read_csv('Orders_with_country.csv')
rfc_best = RandomForestClassifier(n_estimators = 300, max_features = 'log2' $                                   , random_state = 42, n_jobs = -1) $ rfc_best.fit(X_train, y_train) $ test_preds = rfc_best.predict(X_test)
autos["gear_box"].unique() $
df.isnull().any()
p_treatment = df2.query("group == 'treatment'")['converted'].mean() $ p_treatment
localized = rng.tz_localize('US/Eastern')
dset = dset.squeeze()
joined_patient.limit(10).toPandas()
len(topUserItemDocs['user_id'].unique()) $
ok.grade('q03')
import time $ from datetime import datetime $ at = datetime.strptime("2016-10-23 21:50:00.000", "%Y-%m-%d %H:%M:%S.%f") $ current_time = at.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ') $ datetime.timestamp(at)
prediction_df = pd.DataFrame(y_pred, columns=["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]) $ prediction_df.head(15)
pd.DataFrame({'one' : pd.Series([1.]), 'two' : pd.Series([1., 2.], index=['a', 'b'])}, columns = ['one', 'two', 'three'])
julyAugSept = summer.ix[datetime(2015,7,1) : datetime(2015,9,30)] $ julyAugSept[['Mean TemperatureC', 'Max TemperatureC']].plot(grid=True, figsize=(10,5))
ListJota= [1,3,5,np.nan,6,8] $ ListJota
PFalseNegative = (PNegativeFalse * PFalse) / PNegative $ "%.2f" % (PFalseNegative * 100) + '%'
parameters = {'classification__mlpclassifier__solver': ['sgd'], 'classification__mlpclassifier__max_iter': [1500], $               'classification__mlpclassifier__alpha': 10.0 ** -np.arange(1, 7), $               'classification__mlpclassifier__hidden_layer_sizes':[[30, 30, 30, 30]], $              'classification__mlpclassifier__momentum':[0, 0.3, 0.6, 1]} $ parameters
url_cpi_all = 'http://stat.data.abs.gov.au/sdmx-json/data/CPI//all?detail=DataOnly&dimensionAtObservation=AllDimensions' $ print url_cpi_all
pd.Timestamp("2018-04-23")
tweets_df[tweets_df['id'].duplicated()]
ax1.plot(np.log(np.arange(50)), 'r', label='log(x)') $ ax1.plot(np.sqrt(np.arange(50)), 'b*--', label='sqrt(x)') $ ax1.legend(loc='best') $ plt.savefig('threePlots.png')
df_regression=df2 $ df_regression.head() $ df_regression.group.unique() $ df_regression['intercept']=1 $ df_regression[['control', 'treatment']] = pd.get_dummies(df['group']) $
tweets = r.iter_lines()
treatment_diff = (df2[df2.group == 'treatment'].converted).mean()
b = tweets_df.retweet_count.hist() $ b.set(yscale="log") $ plt.annotate("max", xy=(1000, 20000), xytext= (3000, 1000), arrowprops=dict(facecolor='black', shrink=0.05))
df.reset_index()
for col in b_cal.columns: $     print(f'{col}....{len(b_cal[col].unique())}')
df_wna = df_selparams.dropna()
datetime=pd.DataFrame() $ datetime= pd.DataFrame({'year': df['Year'], $                        'month': df['Month'], $                        'day': df['Date'], $                        'hour':df['Time(Hrs)']})
logit_model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = logit_model.fit()
print('\n Row x Columns of imported data:') $ print(dfa.shape) $ print(type(dfa))
autos_p.describe(include = 'all')
parties['Unique Key'].groupby(by= parties.index.hour).count().head(1)
pmol = PandasMol2().read_mol2('./data/40_mol2_files.mol2.gz')
df.drop('Ehail_fee', axis=1, inplace=True) $ df.columns
pd.options.display.max_colwidth = 100 $ it_df[~it_df.publications.isnull()][["publications", "finalPrice", "bidDeadline","title"]].head()
Results_kNN100.head()
merged.groupby(["contributor_firstname", "contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount" , ascending=False)
df.isnull().sum()
df1.head()
os.listdir()
data.to_csv('311_parsed.csv', encoding = 'utf-8', index = False)
df.index = pd.DatetimeIndex(df['DATE'])
test_df = pd.concat([test_df,  get_flat_features(test_df)], axis=1)
trump_df.head(3)
cercanasAfuerteApacheEntre125Y150mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 125) & (cercanasAfuerteApache['surface_total_in_m2'] < 150)] $ cercanasAfuerteApacheEntre125Y150mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre125Y150mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre125Y150mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
text_file = sc.textFile("hdfs:///datasets/tweets-leon")
shops.head()
minTemp = func.min(Measurement.tobs).label("Lowest Temperature") $ maxTemp = func.max(Measurement.tobs).label("Highest Temperature") $ avgTemp = func.avg(Measurement.tobs).label("Average Temperature") $ session.query(minTemp, maxTemp, avgTemp).filter_by(station="USC00519281").all() $
images_clean.index
autos["price"].value_counts()
df_mes = df_mes[df_mes['fare_amount']>0] $ df_mes.shape[0]
data_2018 = data_2018.rename(columns={'Unnamed: 0':'time'})
df[df['Complaint Type'] == 'Illegal Fireworks']['Unique Key'].groupby(df[df['Complaint Type'] == 'Illegal Fireworks'].index.dayofyear).count().plot()
df3.country.unique()
archive_copy['source'].head()
points["GoalRate"] = points["Goals for"] / points["Matches Played"] $ points["ConcedeRate"] = points["Goals against"] / points["Matches Played"] $ points.head()
from sklearn.svm import SVC
latest_date = session.query(Measurement.date).\ $ order_by(Measurement.date.desc()).first()[0] $ latest_date
df = pd.read_csv("Example Car SERPs.csv")
np.average(new_page_converted)- np.average(old_page_converted)
df_B.index=[ 's6', 's7', 's8', 's9', 's10'] $ df_B['Gender']=['F', 'M', 'F', 'F', 'M'] $ df=pd.concat([df_A,df_B]) $ df
grouped_dpt.tail(1)
df_dont_line_up = df.query("(landing_page == 'new_page' and group == 'control') or (landing_page == 'old_page' and group == 'treatment')") $ print('The number of times the new_page and treatment don\'t line up is {}.'.format(df_dont_line_up.shape[0]))
print('Original Grocery List:\n', groceries) $ print() $ print('We remove apples (out of place):\n', groceries.drop('apples')) $ print() $ print('Grocery List after removing apples out of place:\n', groceries)
print(autos.columns) $ autos['unrepaired_damage'].value_counts()
dr_num_new_patients = dr_num_new_patients.astype('float') $ dr_num_existing_patients = dr_num_existing_patients.astype('float')
dataloader_arguments = {"fasta_file": "example_data/hg19_chr22.fa"}
jobs_in_tech = jobs[jobs.category_name=='Technology'] $ by_day = jobs_in_tech.groupby('created_date')
b.quit()
tesla.nlp_text = tesla.nlp_text + ' ' $ tesla = tesla.groupby('date')['nlp_text'].sum() $ tesla = pd.DataFrame(tesla) $ tesla.columns = ['tesla_tweet']
autos["brand"] = autos["brand"].str.replace("sonstige_autos","other")
df2[df2['group']=='control']['converted'].mean() $
logit_mod = sm.Logit(df3_new['converted'], df3_new[['intercept','ab_page', 'UK', 'US']]) $ results = logit_mod.fit() $ results.summary()
display('x', 'y', "pd.concat([x, y], keys=['x', 'y'])")
food.dtypes
df = pd.read_csv("data/airline_data.csv") 
tier2 = getcrimesby_tier(store1,Property) $ tier2_df = tier2.copy()
cs.hist(cumulative=True, normed=100, bins=100, histtype='step')
shown = data.tasker_id.value_counts() $ hired = data.groupby('tasker_id').hired.sum()
measurement_df = pd.read_sql("SELECT * FROM measurement", conn) $ measurement_df.head()
store.info()
pd.MultiIndex.from_product([['a', 'b'], [1, 2]])  # Cartesian product
processed_tweets.processed_tweet_features[150]
df_new = df2.query('landing_page == "new_page"') $ p_new = df2['converted'].mean() $ print(p_new)
crime_wea.loc[:, ['Arrest', 'Domestic']]=crime_wea.loc[:, ['Arrest', 'Domestic']].astype('bool')
def is_clinton(tweet): $     suffix = "-H" $     return tweet.text.endswith(suffix) $ clinton_df['is_personal'] = [is_clinton(tweet) for tweet in clinton_tweets]
nr_rev ='the cheese pizza and garlic bread were bleh. wish the beer was colder and the service really sucked.\ $ i would say stay away not worth it did not taste fresh. even the salad was wilted' $ vectorizer.transform([nr_rev]) $ M_NB_model.predict_proba(vectorizer.transform([nr_rev]))
stats.chi2_contingency(crosstab)
X = pd.get_dummies(cats_df, columns=['breed'], drop_first=True)
np.exp(results.params)
result.summary2() 
sns.countplot(calls_df["call_type"])
print("Number of Techniques in Enterprise ATT&CK") $ techniques = lift.get_all_enterprise_techniques() $ print(len(techniques)) $ df = json_normalize(techniques) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'data_sources','contributors'], axis=1)[0:5]
df_only_headline[df_only_headline["tags"].apply(lambda x: "nlp" in set(x))].groupby(["headline", "date"]).size()
fig, ax = plt.subplots() $ price2017['DE-AT-LUX'].plot(ax=ax, title="Market Price 2017 (DE-AT-LUX)", lw=0.7) $ ax.set_ylabel("Market Price") $ ax.set_xlabel("Time")
df.num_comments.min()
pd.options.display.max_colwidth = 2000 $ jobs.new[jobs.new.code_ogr.isin(new_jobs)][['code_ogr', 'libelle_appellation_long', 'code_rome']]
tweets_clean.info()
%time fullDf['region']=fullDf.latLong.apply(lambda x:hitDetector.getRegion(x)[0])
vertices.printSchema() $ edges.printSchema() $
df2[df2['group'] == 'treatment']['converted'].sum()/len(df2[df2['group'] == 'treatment'])
tweettext = df.text.tolist()
pprint.pprint(posts.find_one({"_id": post_id}))
df2 = df[(((df.group == 'treatment') & (df.landing_page == 'new_page')) | ((df.group == 'control') & (df.landing_page == 'old_page')))]
twitter_archive_master[twitter_archive_master.rating_denominator != 10].head()
sublist = [BAL, CHI, HOU, PIT] # These are the four teams with an original Per Seat Price column uploaded. $ for team in sublist: $     team.drop(["Per Seat Price"], axis = 1, inplace = True) # Drops the column
users_van = van_final.dropna(axis=0) $ users_van.apply(lambda x: sum(x.isna()))
msftAR = msftA.reset_index() $ msftVR = msft[['Volume']].reset_index() $ msftAR[:3]
other = [1375, 10225] $ mask = df['other'].isin(other) $ df.loc[(~mask) & (df.company == True) & (df.other == True), 'other'] = False
telemetry = pd.read_csv('telemetry.csv', encoding='utf-8') $ telemetry.head()
def remove_punct(text): $     text_nopunct = "".join([char for char in text if char not in string.punctuation]) $     return text_nopunct $ infinity['text_clean'] = infinity['text'].apply(lambda x: remove_punct(x))
df2['intercept'] = 1 $ df2[['new_page', 'old_page']] = pd.get_dummies(df['landing_page']) $ df2['ab_page'] = pd.get_dummies(df['group']) ['treatment'] $ df2.head()
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page'] == 'old_page') & (df2['converted'] == 1)]) $ convert_new = len(df2[(df2['landing_page'] == 'new_page') & (df2['converted'] == 1)]) $ n_old = len(df2[df2['landing_page'] == 'old_page']) $ n_new = len(df2[df2['landing_page'] == 'new_page'])
tweet1.id
aussie_results = [] $ for tweet in tweepy.Cursor(api.search, q='%23Aussie').items(2500): $     aussie_results.append(tweet) $ print(len(aussie_results))
camelCase = autos.columns
df.shape
bus.head(5)
smith_tokens = " ".join(train[train['label']==0]['text']).split(" ") $ smith_dict = Counter(smith_tokens)
classifier=SVC(kernel='linear', random_state=2) $ classifier.fit(xy_train,z_train) $ z_pred=classifier.predict(xy_test) $ cm=confusion_matrix(z_test, z_pred) $ cm
import numpy.linalg as la
grades_ord = df['Grades'].astype(dtype= 'category', categories = ['D-', 'D', 'C-', 'C', 'C+', 'B-', "B", "B+", 'A-', $                                                                   'A', 'A+'], ordered = True) $ grades_ord
h.steady_states $
vals = Inspection_duplicates.index.values $ vals = list (vals) $ vals
columns = ['doggo', 'floofer', 'pupper', 'puppo'] $ twitter_archive_clean = twitter_archive_clean.drop(columns, axis=1)
1/np.exp(result_2.params)
temp_df.hist("tobs",bins=12, color="blue", grid=True) $ plt.ylabel("Frequency", fontsize = 15) $ plt.xlabel("Temperature", fontsize = 15) $ plt.title("Last 12 Months Temperature Histogram", fontsize = 15) $
tweets.loc[tweets['isRetweet'] == True, 'isRetweet'] = 1 $ tweets.loc[tweets['isRetweet'] == False, 'isRetweet'] = 0 $ tweets.loc[tweets['truncated'] == True, 'truncated'] = 1 $ tweets.loc[tweets['truncated'] == False, 'truncated'] = 0
display('df1a', 'df2a', 'df1a.join(df2a)')
from pyspark.sql.window import Window $ import pyspark.sql.functions as func $ window = Window.partitionBy('patient_id').orderBy('year', 'month') $ predict_chd = joined_patient.withColumn('next_chd', func.lead('chd').over(window)) $ predict_chd.limit(10).toPandas()
raw.gender = raw.gender.apply(lambda x: gender_mappings.get(int(x[1])) ) $ raw.gender.head()
ip = pd.read_csv('image-predictions.tsv', sep = '\t')
subwaydf.iloc[114053:114065] #this high number seems to come because the time increments from 4/2 to 4/4.
pystore.list_stores()
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs)
tlen.plot(figsize=(16,4), color='r')
for row in reader.iterate_over_rows(): $     break $ row
pn = df2.query("converted==1").count()[0]/len(df2) $ pn
subtes = pd.read_csv('datasets/estaciones-de-subte.csv', sep=',', error_bad_lines=False, low_memory=False) $ subtes.info()
x_axis = np.arange(0,len(target_users)) $ x_axis
df.sort_index(axis = 1, ascending = True)
df3 = df2[['andrew_id_hash', 'WT', 'HT', 'topic_id']] $ print(df3.info()) $ print(df3.head()) $ print(topics.head())
LR2 = LogisticRegression(C=0.01, solver='sag').fit(X_train,y_train) $ yhat_prob2 = LR2.predict_proba(X_test) $ print ("LogLoss: : %.2f" % log_loss(y_test, yhat_prob2)) $
df_new[['new_page','old_page']]=pd.get_dummies(df_new['landing_page'])
df[::2].head()
Games = put_game_ids() $ Games.head()
nltk.download('vader_lexicon')
state_party_df['National_D']['2016-08-01':'2016-08-07'].sum() / 7
games_to_bet[games_to_bet.bet_either == True].describe()
df2[['control','ab_page']] = pd.get_dummies(df['group']) $ df2 = df2.drop(['control'], axis = 1) #df.drop(['B', 'C'], axis=1) $ df2['intercept'] = 1
mod2 = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ mod2.fit().summary()
autos['price'].unique().shape
data.head()
opensorces_clean_url = 'https://raw.githubusercontent.com/yinleon/fake_news/master/data/sources_clean.tsv' $ df_os = pd.read_csv(opensorces_clean_url, sep='\t')
print('Total number of different vote types recorded: {}'.format(len(v['vote'].value_counts(dropna=False)))) $ v['vote'].value_counts(dropna=False).nlargest(10)
plt.figure(figsize=(10,5)) $ sns.heatmap(data = test_data.isnull(), yticklabels=False, cbar = False)
merged1.drop_duplicates(inplace=True)
lda_tf = models.LdaModel.load(os.path.join(outputs, 'model_tf.lda')) $ corpus_lda_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf.mm')) $ lda_tfidf = models.LdaModel.load(os.path.join(outputs, 'model_tfidf.lda')) $ corpus_lda_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tfidf.mm'))
tweet_archive_clean.loc[tweet_archive_clean['new'].apply(find_dot) == True]
if not (os.path.isfile('txt2pdf.py')): $     destination = os.getcwd() $     os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $     shutil.copy('txt2pdf.py',destination) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END)
engine.execute("select * from measurement limit 5").fetchall()
from tempfile import mkstemp $ fs, temp_path = mkstemp("gensim_temp")  $ model.save(temp_path)  
player_unique = baseball.player + baseball.team + baseball.year.astype(str) $ baseball_newind = baseball.copy() $ baseball_newind.index = player_unique $ baseball_newind.head()
for row in selfharmm_topic_names_df.iloc[0]: $     print(row)
url = "https://data.wprdc.org/dataset/a8f7a1c2-7d4d-4daa-bc30-b866855f0419/resource/7794b313-33be-4a8b-bf80-41751a59b84a/download/311-codebook-request-types.xlsx" $ pgh_311_codes = pd.read_excel(url) # parse the excel sheet $ pgh_311_codes.sample(10) # pull ten random rows
1/np.exp(-0.0783), 1/np.exp(-0.0314), 1/np.exp(-0.0118), 1/np.exp(0.0057)
sqlContext.sql("select * from pcs where count > 1").show()
iso_join = gpd.overlay(iso_gdf, iso_gdf_2, how='union')
data[["Class","Amount"]].groupby(["Class"]).mean()
data = access_logs_df.select("contentSize").toPandas() $ data.plot.kde()
data.describe(include=['O'])
svm_classifier.estimator #estimator svm
serc_pixel_df = pd.DataFrame()
file = '/Users/sumad/Documents/DS/Python/UM Spcialization/DS_with_Python' + '/mpg.csv' $ with open(file) as con: $     df = pd.read_csv(con)
Y.shape
demand.loc[:,'value'] = demand.loc[:,'value'].copy() *1.5 $ demand.head()
print "The probability of an individual converting in the control group is {0:.4f}%".format(float(control_con[0])/sum(df2['group']=='control')*100)
df2.head() #Output for quick check
print ("The number Unique userid in dataset is {}".format(df.user_id.drop_duplicates().count()))
df.drop(["join_mode"], axis = 1, inplace = True)
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit()
giss_temp.index.dtype
df.loc[df['edition']=='Global News Service'].head()
train.describe(include='all')
%%bash $ grep -A 20 "INPUT_COLUMNS =" taxifare/trainer/model.py
top_brand = {} $ for brand in top20: $     top = autos[autos['brand'] == brand] $     top_brand[brand] = np.mean(top['price'])
HYB, STD = set(HYB_customer_order_intervals), set(STD_customer_order_intervals) $ for customerID in HYB.intersection(STD): $     print(customerID)
userArtistDF.agg(min("artistID"),max("userID")).show()
tweets_predictions_all.info()
nf.shape
y.is_leap_year # Is this time period a leap year?
ts.asfreq(pd.tseries.offsets.BDay(), method='pad')
brand_model_database = pd.DataFrame(models_series) $ brand_model_database_real = pd.DataFrame(brand_model_database[0].str.split(" ", expand = True)) $ brand_model_database_real.rename(columns={0: "model" , 1: "brand"}, inplace = True) $ brand_model_database_real
df_img_predictions.describe()
d.head()
plt.style.available
data['MinW'] = data[odds_W].min(axis=1) $ data['MinL'] = data[odds_L].min(axis=1) $ display(data.MinW.head()) $ display(data.MinL.head())
c_trt = df2.query('group == "treatment"')['converted'].mean() $ c_trt
df2.query('converted == 1').count()[0]/df2.count()[0]
db = client.mars_db $ collection = db.marsnews $ news_url = 'https://mars.nasa.gov/news/' $ response = requests.get(news_url) $ soup = BeautifulSoup(response.text, 'lxml') $
%matplotlib inline $ twitter_archive_full[twitter_archive_full.stage != 'None'].stage.hist().set_title('Distribution by stage') $ twitter_archive_full[twitter_archive_full.stage != 'None'].groupby('stage').tweet_id.count()
df["Views-PercentChange"].hist() $ plt.show()
path = os.path.join('Data for plots', 'Quarterly index.csv') $ quarterly_index.to_csv(path, index=False)
data = api.concatFields(data, ['keyphrases']) $ pd.DataFrame(data).head()
data["Postal Code"]=data["Postal Code"].astype(str) $ data.info()
s.index
utils.fix_coordinates(data)
print ("The convert rate for  p_old  under the null is: {:.4f}".format(convert_p_old))
autos['price'].value_counts().sort_index(ascending=True).head(20)
display(raw_data[(raw_data['name'].isna())]) $ raw_data[(raw_data['desc'].isna())]
%%time $ test3 = model.predict_generator(cs, steps=100, workers=10, use_multiprocessing=True)
providers_schedules['ProviderId'].unique(), len(providers_schedules['ProviderId'].unique())
cityID = '7d62cffe6f98f349' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         San_Jose.append(tweet) 
ins_named.sort_values('score').iloc[0]
df_json = pd.read_json('https://api.github.com/repos/pydata/pandas/issues?per_page=5') $ df_json
sns.regplot(x=final["ETH Price"], y=final['Crypto Compound'], fit_reg=False) $ sns.regplot(x=final["ETH Price"], y=final['Crypto Negative'], fit_reg=False, color = 'r') $ sns.regplot(x=final["ETH Price"], y=final['Crypto Positive'], fit_reg=False, color = 'g').invert_yaxis()
df_users_5['units_completed_flag'].unique()
dfTemp=transactions.merge(users, how='outer',left_on='UserID',right_on='UserID') $ dfTemp
from cfg import mv_buy_hold $ (target_pf.iloc[-1]['date'] - target_pf.iloc[0]['date']).days $ period = 'month' $
mtcars_filtered = mtcars[1:12] $ mtcars_filtered.head()
texts_trigrams = [] $ for text in texts: $     texts_trigrams.append(text + ngrams(text,2) + ngrams(text,3))
tweet_archive_clean.rating_numerator.value_counts().sort_index().head(10)
sf_small_grouped.plot('start_date', 'duration', figsize = (12,4), marker = 'o', linestyle = '-', legend = None) $ for i in list(rain_period.index): $     plt.axvline(i,alpha = 0.5) $ plt.ylabel('Rides') $ plt.xlabel('Date')
from sklearn.metrics import confusion_matrix, classification_report $ print(confusion_matrix(y_test, preds)) $ print('\n') $ print(classification_report(y_test, preds))
df.info()  # return text output
most_common_registered_postcodes_slps = active_companies[active_companies['company_type'] == 'Limited Partnership for Scotland'].groupby(['RegAddress.PostCode'])['CompanyNumber']\ $     .agg(lambda x: len(x.unique())).sort_values(ascending=False).head(20) $ most_common_registered_postcodes_slps.to_excel('data/for_further_investigation/top_20_slp_postcodes.xlsx') $ most_common_registered_postcodes_slps
df_tweets["oembed"] = oembeds $ df_tweets.sample()
scr_retention_df.fillna(0,inplace=True)
print(df2.loc[df2['user_id']==773192])
model.add(Dense(4, activation='softmax'))
df2['intercept']=1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])
merge_DA_imb_power_df.to_excel(data_folder_path + '/final-merged-df.xlsx', index = False)
df_twitter_copy[df_twitter_copy.retweet_count.isnull()]
van15_fin['stiki_percent'].value_counts()
y.mean()
from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV $ skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=42 )
old_page_converted = np.random.binomial(1,Pold,Nold) $ old_page_converted
dr.shape, RNPA.shape, ther.shape
df2.converted.mean() $
psy_hx = psy_hx.dropna(axis=1, how='all') $
session.query(Measurement.station, func.count(Measurement.id)).group_by(Measurement.station).\ $                        order_by(func.count(Measurement.id).desc()).all()
train.Page.value_counts().shape
n_new = (df2['landing_page'] == 'new_page').sum(); $ n_new
entity_relations = $ counter_entities = $ for sent in doc.sents: $     sent = $     entities = $
mentions_count.plot(kind='bar')
trimmed_degrees = trimmed.degree() #sort by degree $ metrsorted_degree = sorted_degree_map(trimmed_degrees) $ plt.hist(trimmed_degrees.values(), 100) #Display histogram of node degrees in 100 bins
users['Registered'] = pd.to_datetime(users['Registered']) $ users['Cancelled'] = pd.to_datetime(users['Cancelled']) $ sessions['SessionDate'] = pd.to_datetime(sessions['SessionDate']) $ transactions['TransactionDate'] = pd.to_datetime(transactions['TransactionDate']) $
obs_diff = new_page_converted.mean() - old_page_converted.mean()
link = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ r = requests.get(link, auth=('user', 'pass'))
stats = baseball[['h','X2b', 'X3b', 'hr']] $ diff = stats - stats.loc[89521] $ diff[:10]
stats_cols = ['Backers', 'Pledged (USD)', 'Goal (USD)', 'Funding %'] $ desc_stats = df[df.State.isin(['Successful', 'Failed'])].groupby('State')[stats_cols].describe() $ desc_stats
to_be_predicted_Day4 = 56.18021778 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
np.save('../data/medline_abstracts/npQuarantineArticles_CloselyAssociatedDiseases', quarantineArticles) $ np.save('../data/medline_abstracts/npWhiteListArticles_CloselyAssociatedDiseases', whitelistedArticles)
k1.head()
df_train['totals.bounces'] = df_train['totals.bounces'].fillna('0') $ df_test['totals.bounces'] = df_test['totals.bounces'].fillna('0')
import matplotlib.pyplot as plt $ from wordcloud import WordCloud, STOPWORDS $
tobs_data = [] $ for row in temperature_data: $     tobs_data.append(row[0])
import gensim, logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
plt.hist(p_diffs) $ plt.axvline(x=diffs, color= 'red');
c = pd.concat([a1,a2],axis=0) $ c
autos["odometer"].value_counts()
lm = smf.ols(formula='sales ~ TV + radio + newspaper', data=data).fit() $ lm.params
results.summary()
df.drop('cylinders',1,inplace=True) $ x = df.values.tolist() $ data = x $ label = y
print ('Mean difference:', Ralston["T_DELTA"].mean()) $
!subl /usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ $
(loans_xirr.loc[mth3_loans]*100).hist()
!python -m textblob.download_corpara
years = df.groupby(pd.TimeGrouper(freq="AS")).count() $ years
df_new = df2[df2['landing_page'] == 'new_page'] $ n_new = df_new.user_id.count() $ n_new
n_new = len(df2.loc[new_page]) $ n_new
image_clean.drop(['p1','p1_conf','p1_dog','p2','p2_conf','p2_dog','p3','p3_conf','p3_dog'],axis=1,inplace=True)
dfq115.groupby(['county','store_number',dfq115['date'].dt.month]).agg({'sale':['sum']})
with open('new_reddit_topics.pkl', 'rb') as pikkle2: $     sub_df = pickle.load(pikkle2)
collection.write('AAPL', aapl[['Close', 'Volume']], $                  metadata={'source': 'Quandl'}, $                  overwrite=True) $ df = collection.item('AAPL').to_pandas() $ df.tail()
plt.scatter(cdf.FUELCONSUMPTION_COMB, cdf.CO2EMISSIONS,  color='blue') $ plt.xlabel("FUELCONSUMPTION_COMB") $ plt.ylabel("Emission") $ plt.show()
df = pd.read_pickle('5sbo03_parsed_comments.pickle') $ ddf = dd.from_pandas(df, npartitions=8)
to_be_predicted_Day2 = 81.77623296 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
nba_df.loc[nba_df.loc[:, "Tm.Pts"] > 145, "Team"].tolist()
reddit['Date Only'].unique()
twitter_archive_master[(twitter_archive_master['rating_denominator'] == 11) & (twitter_archive_master['rating_numerator'] == 7)].iloc[0].name
dfs = sqlContext.createDataFrame(scenarios_rdd)
twitter_merged_data.plot('stage',  kind='bar'); $ plt.title('Stage Bar') $ plt.xlabel('Stage') $ plt.ylabel('Rating');
pd.unique(dump["country"]) #Again, making sure I'm not crazy
ScoreLabel = 'HDWPSRRating' $ A = ScoreToProbViaIntegral(Score, ScoreLabel) $ dfX_hist['prob_'+ScoreLabel] = dfX_hist.groupby('race_id')[ScoreLabel].transform(lambda x:A(x))
dfSF = pd.read_csv("Case_Data_from_San_Francisco_311__SF311_.csv")
com_luz = ((df_data.PERIDOOCORRENCIA_TIPO==1) | (df_data.PERIDOOCORRENCIA_TIPO==2)) $ sem_luz = ((df_data.PERIDOOCORRENCIA_TIPO==3) | (df_data.PERIDOOCORRENCIA_TIPO==4)) $ print('Com Luz Solar: ', com_luz.sum()) $ print('Sem Luz Solar: ', sem_luz.sum())
data_2018 = pd.read_csv(filename_2018)
lbl = preprocessing.LabelEncoder() $ lbl.fit(list(df_train['manager_id'].values)) $ df_train['manager_id'] = lbl.transform(list(df_train['manager_id'].values))
parsed_events[parsed_events.swimstyle.isnull()]
predictions = mlb.inverse_transform(y_pred)
street_conditions = df[df['Complaint Type'] == 'Street Condition'] $ street_conditions['Descriptor'].value_counts()
reg.coef_
print(f"Number of stations is {sq.station_count()}")
df_sizes = pd.DataFrame(sizes, columns = ['size'], index= newdf.index) $ newdf = newdf.join(df_sizes, how = 'inner')
londonDFSubsetWithCounts = londonDFSubset.merge(grouped, on='OA11CD')
first_commit_timestamp = pd.to_datetime('2005-4-16 22:20:36') $ last_commit_timestamp = pd.to_datetime('today') $ boolIndex = (git_log['timestamp'] >= first_commit_timestamp) & (git_log['timestamp'] < last_commit_timestamp) $ corrected_log = git_log[boolIndex] $ corrected_log['timestamp'].describe()
df2.query( $     'group=="treatment" & converted==1').user_id.count() / df2.query( $     'group=="treatment"').user_id.count()
con = sqlite3.connect('db.sqlite') $ con.execute("INSERT INTO tbl VALUES ('en','sqlite',1,2,3)")  $ con.commit() $ con.close()
X = std_scaler.fit_transform(nuevo_df.values)
fulldf.info()
merged_df_cut.head(5)
twitter_df["user"].unique()
p_old = df2['converted'].mean() $ print('p_old: ',p_old)
fig, ax = plt.subplots(1, figsize=(10,3)) $ plot_linear_trend(ax, title='Therapists', series= doctors_detrended) $ plt.title('Therapists data, linearly detrended') $ plt.tight_layout()
def tweet_extend (tweet_id): $     return api.get_status(tweet_id, tweet_mode='extended')._json['full_text']
data.columns.values
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"')['converted'].sum() $ convert_new = df2.query('landing_page == "new_page"')['converted'].sum() $ n_old = (df2.landing_page == 'old_page').sum() $ n_new = (df2.landing_page == 'new_page').sum()
ax=gdp_df.plot(x='Year', y="Detroit GDP", title='Detroit GDP Recovery 2001-2016 \n', color='green') $ ax.set_xlabel("Year") $ ax.set_ylabel("Detroit GDP (in Millions USD)") $ ax.spines["right"].set_visible(False) $ ax.spines["top"].set_visible(False)
pbptweets.head()
autos_pr['registration_year'].value_counts(normalize = True)
df_schools.columns.tolist()
from sklearn.cross_validation import cross_val_score
tvecdata= tvec.transform(X_train,y_train)
df4 = bg_df2[['Date', 'BG']].copy() $ df4 # whew, this way works fine $
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
df_schools.head()
most_retweets = yxe_tweets.sort_values('retweet count', ascending=False) $ most_retweets.head(10)[['userHandle', 'text', 'retweet count']]
df_new.head()
DummyDataframe2 = DummyDataframe[["Tokens","Token_Count"]].copy() $ DummyDataframe2 = DummyDataframe2[["Tokens","Token_Count"]].groupby('Date').agg({'Tokens': 'sum', 'Token_Count': 'sum'}) $ DummyDataframe2
final_data = pd.read_pickle('D:/CAPSTONE_NEW/jobs_data_clean.pkl')
regional_holidays=actual_holidays[actual_holidays.locale=='Regional'] $ print("Rows and columns:",regional_holidays.shape) $ pd.DataFrame.head(regional_holidays)
p_old = len(df2.query('converted == 1'))/len(df2) $ p_old
y_pred = gnb.predict(X_clf)
min_IMDB = scores.IMDB.min() $ min_IMDB
cbg = cbg.loc[cbg['COUNTYFP'].isin(['061', '081', '085', '047', '005'])]
plt.figure(figsize=(12,8)) $ sns.violinplot(x=trump.source, y= 'num_unique_words', data= trump) $ plt.xlabel(' Source of Tweet', fontsize=12) $ plt.ylabel('NUmber of Unique Words', fontsize=12) $ plt.title('Number of Unique Words per Tweet by Source', fontsize=15)
result_list = [] $ for item in profile_ids: $     num_event = num_events_between_two_dates(df_cal, item, '2012-10-01', '2017-10-30') $     result_list.append(num_event)
sessions.head(2) 
breed_predict_df_clean = breed_predict_df.copy()
airquality_pivot = airquality_pivot.reset_index() $ print(airquality_pivot.index)
for c, v in zip(logreg_words.fit(X_words, y).coef_[0].round(3)[-4:], ['Curse','Fun','Happy','Vice']): $     print v, c
import matplotlib.pyplot as plt   $ %matplotlib inline $ %pylab inline $ pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots $ appl["Adj Close"].plot(grid = True) 
dt.fit(data[['expenses', 'floor', 'lat', 'lon', 'property_type', \ $              'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']], \ $         data[['price_aprox_usd']])
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(n_new, p_new) $     old_page_converted = np.random.binomial(n_old, p_old) $     p_diffs.append(new_page_converted/n_new - old_page_converted/n_old) $
jobs_data['clean_titles'].replace({' state farm agent team member':''}, regex=True, inplace=True)
%%time $ model = GaussianNB() $ model.fit(train_x, train_y)
!wget http://files.fast.ai/part2/lesson14/rossmann.tgz --directory-prefix={PATH}
df_archive_clean.info()
df2 = df2.drop(rep_row) $
df_enhanced = df_enhanced.drop('name', axis = 1)
doctopic = clf.fit_transform(dtm)
with tb.open_file('data/file1.h5', mode='a') as f1, tb.open_file('data/file2.h5', mode='a') as f2: $     f1.create_external_link(where='/', name='external_link_to_array2', target=path_to_array1) $     f2.create_external_link(where='/', name='external_link_to_array1', target=path_to_array2)
images = mars_hemispheres.find('div', class_='collapsible results') $ print(images.prettify())
X1 = pd.DataFrame(X[['Loan_Amount','Property_Value']])
treatg = df2.query('group == "treatment" & converted ==1').user_id.count() $ greatttl = df2.query('group == "treatment"').user_id.count() $ treatment_convert = treatg/greatttl $ treatment_convert
tweets[0].text
np.random.random((5,5))*np.identity(5)
diffs = np.array(p_diffs)
n = ds.make_one_shot_iterator().get_next() $ sess = tf.Session() $ sess.run(n)
cols = [desc[0] for desc in cur.description] $ cols
def outlier_eliminator(df): $     essendon_filter_criteria = ~(((df['team'] == 'Essendon') & (df['season'] == 2016)) | ((df['opponent'] == 'Essendon') & (df['season'] == 2016))) $     df = df[essendon_filter_criteria].reset_index(drop=True) $     return df
least = pd.DataFrame(data={'Country':country_with_least_expectancy.values, $                           'Expectancy':expectancy_for_least_country.values}, $                     index= country_with_least_expectancy.index.levels[1]) $ least
from sklearn.ensemble import RandomForestClassifier
SVM_yhat = SVM_model.predict(test_X) $ print("SVM Jaccard index: %.2f" % jaccard_similarity_score(test_y, SVM_yhat)) $ print("SVM F1-score: %.2f" % f1_score(test_y, SVM_yhat, average='weighted') )
saved_tweets=pd.read_csv("../output/clean_data_4_18_14_40.csv",sep=",",encoding="utf-8",engine="python")
linear_predictor.content_type = 'text/csv' $ linear_predictor.serializer = csv_serializer $ linear_predictor.deserializer = json_deserializer
frame.sort_values(by='b')
np.vstack((a, a))[0:6:2,:] # by steps of 2.
X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid $                                                     , y_train_valid $                                                     , test_size=0.20)
y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag) $ y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag) $ y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf) $ y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)
df_users_mvp=pd.merge(df_users_mvp,df_first_name[['entity_id','field_first_name_value']],left_on='uid',right_on='entity_id',how='left') $ df_users_mvp=pd.merge(df_users_mvp,df_last_name[['entity_id','field_last_name_value']],left_on='uid',right_on='entity_id',how='left')
car_data = pd.read_csv('training_car_x_y_train.csv') $ test_data = pd.read_csv('test_car_x_test.csv')
energy['2014-07-01':'2014-07-07'].plot(y='load', subplots=True, figsize=(15, 8), fontsize=12) $ plt.xlabel('timestamp', fontsize=12) $ plt.ylabel('load', fontsize=12) $ plt.show()
selection1=df.loc[(df.group=='treatment')&(df.landing_page!='new_page')].count()+df.loc[(df.group!='treatment')&(df.landing_page=='new_page')].count() $ selection1[0]
! head wikipedia_test_basedocs.txt 
engine = create_engine('postgresql+psycopg2://aact:aact@aact-db.ctti-clinicaltrials.org:5432/aact') $ df = pd.read_sql_query("SELECT * FROM studies WHERE plan_to_share_ipd != 'Null'", engine)
air_rsrv_by_date = air_rsrv.groupby(['air_store_id','visit_date'], as_index=False).agg({"air_rsrv_visitors": "sum"}) $ hpg_rsrv_by_date = hpg_rsrv.groupby(['hpg_store_id','visit_date'], as_index=False).agg({"hpg_rsrv_visitors": "sum"}) $ [air_rsrv_by_date.head(),hpg_rsrv_by_date.head()] $
members = [] $ dict_temp = year_count(df_chi, 'joined') $ for key in tqdm(year): $     members.append(dict_temp[key]) $ members_count.append(members)
with fs.open(filepath, 'rb') as f: $     df2 = pd.read_csv(f, index_col = 0)
trump_df.head()
extractor = twitter_setup() $ tweets_dalai = extractor.user_timeline(screen_name="Pontifex", count=200) $ print("Number of tweets extracted: {}.\n".format(len(tweets_neta))) $ tweets_pope = extractor.user_timeline(screen_name="DalaiLama", count=200) $ print("Number of tweets extracted: {}.\n".format(len(tweets_erdog)))
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ df_imgs = pd.read_csv(url, sep='\t') $ urllib.request.urlretrieve(url, 'image-predictions.tsv')
df3['timestamp'] = df3['timestamp'].astype('datetime64[ns]') $ df3_sorted = df3.sort_values(by='timestamp') $ df3_sorted.timestamp.describe()
plt.hist(p_diffs); $ plt.axvline(diff, color='red')
results_df.to_csv(csv_file_name)
corn.size()
new_dems.Clinton.describe() $
cbg.head(2)
for dataset in combine: $     dataset['IsAlone'] = 0 $     dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1 $ train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()
days_traded = len(r) $ n_years = days_traded / avg_annual_days_traded $ cagr = (end_value / start_value) ** (1 / n_years) $ print("Compound annual growth rate (CAGR) = {:5.4}%".format((cagr - 1) * 100))
pd.Timestamp('2018-01-01') + Hour(3) + Minute(5)
df3[['ab_page', 'not_ab_page']] = pd.get_dummies(df3['landing_page']) $ df3 = df3.drop('not_ab_page', axis=1) $ df3['intercept'] = 1
data['floor'] = data['floor'].apply(lambda x: (float)((int)(x)))
air_store = join_df(air_store, air_rsrv_by_date, "air_store_id") $ air_store.head() $
df.columns = [column.strip() for column in df.columns] $ df.columns
m2 =m[:,[1,2]] $ m=np.delete(m,[1,2],1) $ print(m.shape) $ print(m2.shape) $ print("m2: ", m2)
ftr_imp_rf=zip(features,trained_model_RF.feature_importances_) $ for values in ftr_imp_rf: $     print(values)
print total_sales.q1_pct_change.describe() $ print np.sum(total_sales.q1_pct_change > 3)
top_brand_info = pd.DataFrame(mean_mileage, columns = ['mean_mileage']) $ top_brand_info
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group']) $ df.rename(columns={'treatment':'ab_page'},inplace=True)
my_query = "SELECT * FROM donors LIMIT 1" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element.keys() $
speeches_metadata = speeches_cleaned.merge(metadata_df_all, left_on = 'id', right_on = 'accessids', how = 'inner')
autos['price'] = autos['price'].str.replace("$","").str.replace(',', '') $ autos['price'] = autos['price'].astype(int) $ autos['price'].head()
df2 = df2.drop('control', axis=1)
breakfastlunchdinner.plot(x="STATION", y=["breakfast", "lunch + brexits", "dinner"], figsize = (20,9), kind="bar", rot=30, fontsize=15, title="Average Daily Weekday Traffic for Top 10 Stations at B/L/D");
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, 1-p_old]) $ old_page_converted.mean()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=12000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
lm.fit(X_train,y_train) $ predictions = lm.predict(X_test) $ plt.scatter(predictions,y_test)
print(os.path.abspath(holding_file_name)) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir('../') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $
grouped = df_providers.groupby(['year']) $ (grouped.aggregate(np.sum)['disc_times_pay'])
g_kd915 = kd915[(kd915.state != 'live') & (kd915.state != 'suspended') & (kd915.state != 'canceled')].groupby(['blurb']) $ kd915_filtered = g_kd915.filter(lambda x: len(x) <= 1).sort_values(['blurb','launched_at'])
df[df['Updated At'] < df['Shipped At']].shape
refinedData.query()
rng + pd.tseries.offsets.DateOffset(months=2)
dr_test_data = dr_test_data.resample('W-MON').sum() $ RN_PA_test_data = RN_PA_test_data.resample('W-MON').sum() $ therapist_test_data = therapist_test_data.resample('W-MON').sum()
if True: $     from sklearn.model_selection import train_test_split $     train_model, validation_model = train_test_split(train_norm, test_size=0.2, random_state=73777) $     train_model = validation_model.copy(deep=True)
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x , np.dot(x1, model1.coef_), + model1.intercept_, color = 'orangered', linewidth = 5.0 )
print(df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False]) $ df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
autos['odometer_km'].unique().shape
fish.index
properati[properati['zone'] == ""]['place_name'].value_counts(dropna=False)
oldest_date = pd.to_datetime(pd.Series([t["created_at"] for t in trump_tweets])).min() $ oldest_date
print(autos.columns)  $ autos.columns = autos.columns.str.replace('([a-z0-9])([A-Z])', r'\1_\2').str.lower()
print('Change the names of multiple columns permanently') $ df.rename(columns = {'PUBLISH STATES':'Publication Status','WHO region':'WHO Region'},inplace=True) $ df.head(2)
gb = df2.groupby('landing_page').count() $ gb
archive_clean=pd.merge(archive_clean, tweet_json_clean, left_on='tweet_id', right_on='id')
np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)
iowa.dtypes
trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt) $ val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt) $ md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)
hashed_test.count()
data.describe().transpose()
features_rolling_averages.tail()
user_df = my_df[my_df["user"] == 0] $ print(user_df.shape) $ print(user_df.head())
data['yyyymm'].head()
xmlData.drop('living_area_unit', axis = 1, inplace = True) $ xmlData.drop('lot_area_unit', axis = 1, inplace = True) $ xmlData.drop('upper_area_unit', axis = 1, inplace = True) $ xmlData.drop('basement_area_unit', axis = 1, inplace = True)
poverty_data.index=multi_index
hp.listdate = hp.listdate.astype('datetime64[ns]', inplace=True)
r.aggregate({'A' : np.sum,'B' : np.mean}) #Apply Different Functions to Different Columns of a Dataframe
fdist.plot(100, cumulative=False)
filtered_flatten_plot_df = flatten_plot_df[flatten_plot_df['tag'].isin(flatten_plot_df.groupby(['tag']).size().nlargest(10).index)]
(ggplot(raw_large_grid_df.groupby(['eyetracker','posx','posy'],as_index=False).mean(),aes(x="posx",y="posy",size="accuracy"))+geom_point()+facet_wrap("~eyetracker"))+coord_fixed()
a = np.array([1, 2, 3]); b = np.array([4, 5, 6])
user_total = df.nunique()['user_id'] $ print("Number of unique users is : {}".format(user_total))
result_control_2.summary()
df.stemmed[:10]
import urllib $ from bs4 import BeautifulSoup as bs $ import requests $ import lxml.html $ import getpass
abc = pd.read_csv('Line_item_Site_Year.csv') $ abc.head()
ts.head()
df_2018.columns
import os $ dataframes = [ pd.read_csv( report_path+'\\'+f ) for f in os.listdir(report_path)] # add arguments as necessary to the read_csv method $ merged = reduce(lambda left,right: pd.merge(left,right,on='variable', how='left'), dataframes)
twitter_json = r'data/twitter_01_20_17_to_3-2-18.json' $ tweet_data = pd.read_json(twitter_json)
df3[df3['STATION']=='1 AVE'].pivot_table(index = 'DAY', columns = 'WEEK', values = 'INCR_ENTRIES').plot(figsize=(10,3))
pp = pd.DataFrame(pp, columns=['dril_pp', 'laziestcanine_pp', 'ch000ch_pp'])
Base = automap_base() $ Base.prepare(engine, reflect=True) $ hawaii = Base.classes.measurement
l_aux=[] $ for i in range(0,l2): $     if i not in seq: $         l_aux.append(lifetime[i]) $ col.append(np.array(l_aux))
print(X_val.info())
spy_df['range'] = ( to_numeric( spy_df['high'] ) - $                     to_numeric( spy_df['low'] )) $ spy_df.sort_index( ascending=False, inplace= True ) $ spy_df.index = to_datetime( spy_df.index ) $ spy_df.head()
idx = data['dataset_data']['column_names'].index('Open') $ open_price = [day[idx] for day in data['dataset_data']['data'] if day[idx]] $ print('Highest and lowest opening prices in 2017 were {} and {}'.format(max(open_price), min(open_price)))
result = customer_visitors.groupby('Yearcol').mean() $ result
r,q,p = sm.tsa.acf(resid_701.values.squeeze(), qstat=True) $ data = np.c_[range(1,41), r[1:], q, p] $ table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"]) $ print(table.set_index('lag'))
df2['converted'].mean()
shannon_sakhalin = - sum(np.log2(sakhalin_freq.values) * sakhalin_freq.values) $ shannon_sakhalin
' '.join(str(o) for o in trn_lm[0])
type(twitter_Archive['timestamp'].iloc[0]) $
X_tfidf_df = pd.DataFrame(X_tfidf.toarray()) $ X_tfidf_df.columns = tfidf_vect.get_feature_names()
countries['haversine_distance'] = countries.apply(lambda r: haversine.haversine(([36.966427, -95.844030]), $                                                                   (r['lat_destination'], r['lng_destination'])), axis=1) $ countries
np.array([1,2,3,4,5]) $
DummyDataframe2 = DummyDataframe2.apply(lambda x: update_values_category(x, "Tokens"), axis=1) $ DummyDataframe2
for unigram_sentence in it.islice(unigram_sentences, 230, 240): $     print u' '.join(unigram_sentence) $     print u''
def clean_links(text): $     return re.sub(r"http\S+", "", text)
cityID = '6ba08e404aed471f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Riverside.append(tweet) 
metadata_df[1]['Variable'].fillna(method='ffill', inplace=True) $ metadata_df[1].head()
doc_duration = doctors.groupby(doctors.index.date)['DurationHours'].sum() $ RN_PA_duration = RN_PA.groupby(RN_PA.index.date)['DurationHours'].sum() $ therapist_duration = therapists.groupby(therapists.index.date)['DurationHours'].sum()
print (np.exp(-0.0099)) $ print (np.exp(-0.0506))
from gensim.models import Word2Vec $ model = Word2Vec.load("300features_40count_10context") $ type(model.wv.syn0)
%matplotlib inline $ x_data, y_data = (df["Chirps"].values,df["Temp"].values) $ plt.plot(x_data, y_data, 'ro') $ plt.xlabel("# Chirps per 15 sec") $ plt.ylabel("Temp in Farenhiet") $
tweet_image_clean.head(10)
S.decision_obj.stomResist.value = 'BallBerry' $ S.decision_obj.stomResist.value
%time preds = np.stack([t.predict(X_valid) for t in m.estimators_]) $ np.mean(preds[:,0]), np.std(preds[:,0])
fulldata_copy[['AUDUSD','RSI']][:500].plot(figsize=(10,6),secondary_y = 'RSI');
print("Value in cell (row 3, col 2):", ) $ print(sheet.cell_value(3, 2))
convert_new = df2.query('landing_page=="new_page" and converted==1').count()[0] $ convert_new $
data['results']['companies'][0].keys()
df.shape
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23Australia').items(2500): $     results.append(tweet) $ print(len(results))
soup.find_all('span', {'class':'next-button'})[0]
engine.execute("SELECT * FROM measurements limit 5").fetchall()
results = results.fillna(0)
index = pd.date_range('2018-01-01 00:00:00', periods=4, freq='D') $ index
sql = SparkSession.builder.master("local").appName("wm").getOrCreate()
schedId_in_proj = [x for x in df_sched.ProjectId if x in list(df_proj.ProjectId)] $ schedId_in_bud = [x for x in df_proj.FMSID if x in list(df_bud.project_id)]
matthew.head()
pd.date_range('2005', periods=7*12, freq='M')
noloc_df = noloc_df.append(df[(df.state == 'YY')]) $ df = df[~(df.state == 'YY')]
con.activity_type = con.activity_type.astype(str) $ con.activity_type = con['activity_type'].apply(str.lower) $ con.activity_type.unique()
X_test_df = pd.DataFrame(X_test_matrix.todense(), $                         columns=tvec.get_feature_names(), $                         index=X_test.index)
df2['Open'].apply(lambda x:2*x/32)
students = students.append(newstudents) $ students
joined_test[['Id','Sales']].to_csv(csv_fn, index=False)
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
feature_layer.query(where='POP2010>1000000', return_count_only=True)
tbl = df[df.date_diff == 1].groupby('msno').date_diff.size().to_frame() $ tbl.columns = ['num_listen_music_in_a_row_count'] $ tbl['num_listen_music_in_a_row_ratio'] = tbl.num_listen_music_in_a_row_count / df.groupby('msno').date_diff.apply(len) $ tbl.reset_index(inplace = True) $ tbl
invoice = shoppingCart.map(priceCategory) $ invoice
import pandas as pd $ names = ['Rover', 'Rex', 'Polly', 'Putin'] $ pets = [(100, 'german shepard', 'dog'), (20, 'labrador', 'dog'), (5, None, 'bird'), (15, 'siamese', 'cat')] $ pets = pd.DataFrame(pets, index = names, columns = ['weight', 'breed', 'species']) $ pets
print(r_test.json())
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\mydata.json" $ df = pd.read_json(path) $ df.head(5)
bg2 = pd.read_csv('Libre2018-01-03.txt') # when saved locally $ print(bg2) $ type(bg2) # at bottom we see it's a DataFrame $
items = find_rome_dataset_by_name(rome_data, 'item') $ new_items = set(items.new.code_ogr) - set(items.old.code_ogr) $ obsolete_items = set(items.old.code_ogr) - set(items.new.code_ogr) $ stable_items = set(items.new.code_ogr) & set(items.old.code_ogr) $ matplotlib_venn.venn2((len(obsolete_items), len(new_items), len(stable_items)), (OLD_VERSION, NEW_VERSION));
number_of_favs = pd.Series( $     data=data['Likes'].values, $     index=data['Date']) $ number_of_favs.plot(figsize=(16,4), color='r'); $
from sklearn.ensemble import RandomForestClassifier $ random_forests_params = { $         "randomforestclassifier__n_jobs" : [-1]} $ pipe = make_pipeline(IncidentPreprocessor(), RandomForestClassifier()) $ random_forests_grid = GridSearchCV(pipe, param_grid=random_forests_params, cv=3)
japandata.dropna(inplace = True)
small_movies_data.take(3)
run txt2pdf.py -o "2018-06-18-1552 FLORIDA HOSPITAL title_page.pdf"  "2018-06-18-1552 FLORIDA HOSPITAL title_page.txt"
reviews.points.median()
crimes['year'] = crimes.DATE__OF_OCCURRENCE.map(lambda x: x.year)
df.head()
btc_price_df['mean'] = btc_price_df.loc[:,["close", 'open', 'low', 'high']].mean(axis = 1)
countries["country"].unique()
np.random.seed(42) $ trn_idx = np.random.permutation(len(trn_texts)) $ val_idx = np.random.permutation(len(val_texts)) $ trn_texts = trn_texts.reset_index(drop = True).loc[trn_idx,:] $ val_texts = val_texts.reset_index(drop = True).loc[val_idx,:]
import pandas as pd $ df = pd.DataFrame.from_dict(js, orient='columns')
pd.DataFrame.from_dict(sbs.get_metric_dict(confidence_interval=0.90)).T
a = sqlContext.createDataFrame(pnls)
master.to_csv("Pega_Bouncebacks_UserUnknow_012616_dates_joined.csv", index=False)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],value=None, alternative='two-sided', prop_var=False) $ (z_score,p_value)
df_nd101_d_b = df_pivot_days_b[df_pivot_days_b['nd_key_formatted'] == 'nd101'] $ df_nd101_d_b.head()
volume = [float('nan') if item[6] is None else item[6] for item in afx['dataset_data']['data']] $ avg_volume = sum(volume) / len(volume) $ print('Average daily trading volume = ', '{:,.0f}'.format(avg_volume))
senate = senate.rename(index=str, columns={"last_name_x": "last_name", "first_name_y": "first_name"}) $ senate = senate.drop(['first_name_x', 'last_name_y'], axis=1)
print('Accuracy Metrics for Random Forest') $ score[['is_false_positive', 'is_false_negative', 'is_true_positive', 'is_true_negative']].mean()
weather_mean.index
data = data.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x) $ data.loc[data['opinion'].str.contains("best"), 'pos_count'] = data['pos_count'] + 1 $ data.loc[data['opinion'].str.contains("worst"), 'pos_count'] = data['pos_count'] + 1 $ data
trump_bow.shape, trump_cleaned_bow.shape, trump_stemmed_bow.shape
infinity.head(10)
big_df_avg.head()
json_data= response.json()['dataset_data'] $ print(json_data) $
tweet_archive_df[tweet_archive_df.rating_numerator > 20].sort_values(by=['rating_numerator'],  ascending=False)[['tweet_id','rating_numerator', 'rating_denominator', 'text']][0:10]
commit_df = pandas.read_table('commits.tsv') $ commit_df.tail(2)
vals1 = np.array([1, None, 3, 4]) $ vals1
df.tail(8)
shows['release_date'].head()
Y_TFIDF_PRED = rdf_model.predict(Y_tfidf)
print("All Tweets: {0} | Users: {1}".format(len(df), df.user.nunique())) $ print("Tweets in the Harvey 160 Collection: ", len(df.query('harvey160'))) $ print("Users in the Harvey 160 Collection: ", df.query('harvey160').user.nunique())
t2.astimezone(timezone('EST'))
hashed_test.show(5, truncate=False)
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1-(0.05/2)) $
ax=nypd.groupby(by=nypd.index.hour).count().plot(y='Unique Key', label='NYPD') $ dot.groupby(by=dot.index.hour).count().plot(y='Unique Key', ax=ax, label='DOT') $ dpr.groupby(by=dpr.index.hour).count().plot(y='Unique Key',ax=ax, label='DPR') $ hpd.groupby(by=hpd.index.hour).count().plot(y='Unique Key', ax=ax, label='HPD') $ dohmh.groupby(by=dohmh.index.hour).count().plot(y='Unique Key', ax=ax, label='DOHMH')
sentences_text = nltk.sent_tokenize(text) $ len(sentences_text)
df2.converted.mean()
data_get["HAYN"].Close.plot()
s1.shape, s1.size
!head _talks/2008-03-01-tamu-communicative-ethnography.md
from pandas.tseries.holiday import USFederalHolidayCalendar $ cal = USFederalHolidayCalendar() $ holidays = cal.holidays('2012', '2016') $ daily = daily.join(pd.Series(1, index=holidays, name='holiday')) $ daily['holiday'].fillna(0, inplace=True)
sentim_analyzer = SentimentAnalyzer() $ allNeg = sentim_analyzer.all_words([mark_negation(doc) for doc in train['trainTuple']])
author_data.head()
historicFollowers = pd.concat([historicFollowers,todaysFollowers[date]],axis=1) #add todays column
sqlContext.sql("select count(person) from pcs").show()
stmt = text("SELECT name, id, fullname, password " $ ...             "FROM users where name=:name")
appointments['Specialty'].unique()
df.head()
X = dfX[fNames].values $ y = dfY.values
inspector = inspect(engine) $ inspector.get_table_names()
df2['US_ab'] = np.multiply(df2['ab_page'],df2['US']) $ df2['UK_ab'] = np.multiply(df2['ab_page'],df2['UK']) $ df2.head()
p_value_obs_gt = (p_diffs > obs_p_diff).mean() $ _, p_value_ztest_gt = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ p_value_obs_gt, p_value_ztest_gt
admit = pd.read_csv('../Cyft/readmissions/admissions.csv') $ claims = pd.read_csv('../Cyft/readmissions/claims.csv')
class_merged.to_csv('class_merged_excl_hol.csv',sep=',')
start = datetime.now() $ modelxg5 = xgb.XGBClassifier(max_depth=10, learning_rate=.05, n_estimators=250, n_jobs=-1) $ modelxg5.fit(Xtr.toarray(), ytr) $ print(modelxg5.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
df['comments'] = [1 if x >= per_75 else 0 for x in df['num_comments']] $ df['comments'].value_counts()
log_opt = log_with_day.groupBy('dayOfWeek', 'ipAddress').count().sort('dayOfWeek', ascending=False) $ max_per_day = log_opt.groupBy('dayOfWeek').max('count') $ max_per_day = max_per_day.withColumn("count", col('max(count)')) $ filtered = log_opt.join(max_per_day, on=['count', 'dayOfWeek']) $ filtered.select(['ipAddress', 'dayOfWeek', 'count']).sort('dayOfWeek').show() $
tweets.head()
payment_plans.sort(['fk_loan','interval'],inplace=True) #prob unnecc? but need sorted for rebase
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit_mod.fit()
print cust_data.head(5) $ print cust_demo.head(5) $ print cust_new.head(5) $
data.index[0] = 15
old_page_converted = np.random.choice([0, 1], num_old, p=[1 - null_p_old, null_p_old]) $ print(old_page_converted[:10])
sample_submission = read_csv_zip("data/sample_submission_NDF.csv")
for tweet in trumpTweets: $     cleaner_tweets.append({'id': tweet.id, 'text': tweet.text, 'created_at': tweet.created_at, 'profile': tweet.user.screen_name}) $
results.keys()
i = (np.arange(0, df3[0].iloc[-1]-df3[0].iloc[0], 43.259))/1000
np.std(diffs)
print "Probability of treatment group converting:",df2.groupby(['group'], as_index = False)['converted'].mean().iloc[1,1]
app_pivot = app_pivot.pivot(index='ab_test_group', columns='is_application', values='email') $ app_pivot
df2csv2gdb(lithlog,fileloc,r'C:/GIS/lithlogs.csv','lithlogs')
machin[(machin.user_id == 1881755) & (machin.item_id == 1928479) & (machin.interaction_type == 5)]
print(p.shape) $ p.head()
df.to_csv(DATAPATH+'submit_MNIST_keras_CNN99.csv', index=False)
tables = pd.read_html("https://dev.twitter.com/overview/api/response-codes")
df2.drop(index = df2.query('landing_page =="old_page" and group=="treatment"').index,inplace = True, axis = 0)
test = pd.Series(test)
df['datetime'] = pd.to_datetime(df['datetime'],format=('%Y-%m-%d')) $ df.dtypes
prob_treatment = df2[df2['group'] == 'treatment']['converted'].mean() $ prob_treatment
(df2.landing_page == 'new_page').sum() / df2.user_id.count()
nb.fit(X_train_dtm, y_train)
dt.minute
import os $ os.path.exists("classA.csv")
ps["post patch class"].value_counts()
model2.compile(loss='categorical_crossentropy', $           optimizer= "adam", $           metrics=[f1])
df.to_excel("msft.xls", sheet_name="msft")
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ new_page_converted.mean()
np.unique(predicted_table)
numeric_df.select("`nonmale_percentage`")
df_stock.head()
df_temp_1 = df.query('landing_page =="new_page" and  group=="treatment"') $ df_temp_2 = df.query('landing_page =="old_page" and  group=="control"') $ df2= df_temp_1.append(df_temp_2)
StockData = pd.read_csv(StockDataFile + '.gz', index_col=['Date'], parse_dates=['Date']) $ StockData.head(10)
df_state_victory_margins.plot(x='state', y='Percent margin', kind='bar')
converted_users = df.query('converted == 1').user_id.nunique() $ print(converted_users) $ proportion = converted_users / number_of_users $ print(proportion)
pd.set_option('display.precision',4) $ pd.set_option('display.float_format', lambda x: '%.2f' % x) $ df_norm.describe()
to_be_predicted_Day1 = 21.37 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
predicted_logpdp95 = 4.63 + 0.53 * 7.07 $ predicted_logpdp95
from sklearn.model_selection import train_test_split $ from sklearn.metrics import r2_score
df.shape 
data.groupby('bandit')['expid', 'latency'].mean()
plt.grid() $ plt.bar(x,Y) $ plt.plot(x,Y) $
covtype_df = h2o.import_file(os.path.realpath("../DATA/covtype.full.csv"))
breakdown[breakdown != 0].sort_values().plot( $     kind='bar', title='Sean Hannity Number of Links per Topic' $ )
from sklearn.cross_validation import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) $ model2 = LogisticRegression() $ model2.fit(X_train, y_train)
twitter_archive.rating_denominator.describe()
df2 = pd.DataFrame(np.array([[10, 11], [20, 21]]), columns=['a', 'b']) $ df2
df_test[['id','visitors']].to_csv('h2o_automl_pubmine.csv',index=False)
hdf = df.set_index(['AgeBins', 'Industry']) $ hdf.loc[('adult', 'Cosmetics'), :].head()
pd.crosstab(cust_demo.Martial_Status,cust_demo.Own_House).plot(kind='bar', color=['R','G'],alpha=0.5, stacked=True)
x = make_df('AB', [0, 1]) $ y = make_df('AB', [2, 3]) $ y.index = x.index # make duplicate indices! $ display('x', 'y', 'pd.concat([x, y])')
testObj.outDF.head()
contractor_clean=contractor_clean.drop(['fax', 'email','ignore','address3'], axis=1)
data[['Sales']].resample('M').apply(['median', 'mean']).head()
Station = Base.classes.stations
af.length.describe()
s.dt.hour  # extract hour as integer
df.describe(exclude = np.number)
from sklearn.metrics import classification_report
W.WISKI_CODES
df_archive_clean["timestamp"].unique()
with open('example.com.txt', 'r', encoding='utf-8') as f: $     html = f.read() $ print(html)
df_questionable_3[df_questionable_3['state_NJ'] == 1]['link.domain_resolved'].value_counts()
to_be_predicted_Day3 = 34.13953228 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df_ad_airings_5['metro_area_state'] = df_ad_airings_5['location'].map(lambda x: x.split(","))
tweet_archive_enhanced_clean['in_reply_to_status_id'].value_counts()
logr = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK', 'ab_page']]) $ result = logr.fit() $ result.summary()
df_final_edited = pd.read_csv('twitter_archive_master_edited.csv', sep=',')
weekly_variance = diff_weekly_mean.groupBy('week','hashtag')\ $                                   .sum('sq_diff')\ $                                   .withColumnRenamed('sum(sq_diff)', 'variance')\ $                                   .orderBy('week','variance', ascending=[True, False])\ $                                   .cache()
actual_diff = df2[df2.group=='treatment']['converted'].mean() - df2[df2.group=='control']['converted'].mean() $ actual_diff $ (p_diffs > actual_diff).mean()
total_base_dict_by_place.loc[place]['hashtags']
suspect = pymol.atoms[0] $ suspect.type
train_df[["Parch", "Survived"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)
full['WillBe<=30Days'] = full['WillBe<=30Days'].fillna(0)
q = [] $ for row in c.execute('SELECT * from weather'): $     q.append(row[4]) $     print(row) $
display(results.summary()) $
new_treatment = df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page == "new_page")')['user_id'].count() $ print("number of times the new_page and treatment don't line up : {}".format(new_treatment))
print(autos[~autos["registration_year"].between(1900,2016)]["registration_year"].value_counts())
so.loc[so['favoritecount'].between(30, 40), 'title'::3].head()
Q96P20=Graph().parse(format='ttl', $                      data=turtle) $ for row in qres: $     print("%s is a protein with an annotation of type %s starting at %s and ending at %s on sequence %s" % row) $
!head data/sample_submission_NDF.csv
df_providers.loc[0,'drg3'] 
try: $     be = CustomBusinessDay(weekmask = 'Sun Mon Tue Wed Thu', holidays = ['2018/08/01']) $     be $ except Exception as e: $     print(f'{type(e).__name__}:--{e}')
cfs=cfs.filter_abundance(10)
prob_conv_treatment = df2[df2['group']=='treatment'].sum()['converted'] / len(df2[df2['group']=='treatment']) $ prob_conv_treatment
logit2_countries = sm.Logit(newset['converted'], $                            newset[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ result_final = logit2_countries.fit()
df1 = pd.DataFrame(pv1.to_records()) $ df1 = df1.set_index("Death year") $ df1.plot() $ plt.show()
sorted(data['data'].keys())     # sorting  the data dictionary
df_step1 = w.get_filtered_data(step = 1, subset = subset_uuid) $ df_step1.columns
top500=df15.groupby(df15.store_number).sum().nlargest(500,'sale').index
df['text'].values
df_new['country'].astype(str).value_counts()
All_tweet_data_v2.retweeted_status_timestamp=pd.to_datetime(All_tweet_data_v2.retweeted_status_timestamp) $ All_tweet_data_v2.retweeted_status_timestamp.dropna().head()
s = df_sms.groupby(['group','ShopperID'])['ID'].count().reset_index() $ s.groupby(['ID','group']).size();
logit_mod = sm.Logit(df2.converted, df2[['intercept','ab_page']]) $ results = logit_mod.fit()
news_titles_sr = news_period_df.resample('D', on='news_collected_time')['news_title'].apply(lambda x: '\n'.join(x))
result['modifiedBy'].value_counts()
np.exp(-0.408), np.exp(0.0099), np.exp(0.0149)
indeed[indeed['summary'].isnull()] $
input_data.groupby('year').num_commits.mean()
test[test['Open'].isnull()]
yt.get_subscriptions(channel_id, key)
test.show(5)
mask_group = df.group == 'control' $ mask_page = df.landing_page == 'old_page' $ df2 = df[mask_group == mask_page]
information_ratio.iloc[1] - 2
colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels))) $ colors
df_tweet_data.tweet_id.nunique()
dump["country"].value_counts() # Returns the number of times a single country appears in the column
from sklearn.model_selection import KFold $ cv = KFold(n_splits=100, random_state=None, shuffle=True) $ estimator = Ridge(alpha=8000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
n = p_diffs.mean()
with tf.name_scope("eval"): $     correct = tf.nn.in_top_k(logits, y, 1) $     accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
!tail -n 10 p32cf_results.txt
summaries = "".join(df.titles) $ ngrams_summaries = cvec.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
df.to_excel("msft_temp.xlsx")
print() $ print(df.groupby('Class').size())
df[df.Predicted == 5]
lin_svc_clf = LinearSVC(random_state=2018, C = 0.055)
events = pd.get_dummies(weather.events)
pd.Timestamp('2018-01-01')
loans_act_xirr=cashflows_act_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
result_country.summary()
url=("/Users/maggiewest/Projects/detroit_gdp.csv") $ gdp_df = pd.read_csv(url)
nan_sets = {} $ for res_key, df in data_sets.items(): $     data_sets[res_key], nan_table = find_nan(df, headers, res_key, patch=True) $     nan_sets[res_key] = nan_table
df['converted'].mean() $
x.sum()
details.dropna(subset=['Runtime'], inplace = True)
df.loc[row,'source_url']
!head msft_temp.csv
pd.value_counts(df['completed_by']).plot.bar()
bd.columns.name = "Data" $ bd
park = load_data('../../static/parkinson_1960tonow.csv')
twitter_archive_clean[~twitter_archive_clean['retweeted_status_id'].isnull()]
plt.scatter("obs_count","status", data=pm_final,marker='o', alpha = 0.25) $ plt.xlabel("Cumulative number of readings") $ plt.ylabel("Status") $ plt.title('Failure over time') $ plt.show()
pixel, charge = reader.select_columns(['pixel', 'charge']) $ charge = charge.values # Convert from Pandas Series to numpy array $ pixel = pixel.values # Convert from Pandas Series to numpy array $ print('charge = ', charge) $ print('pixel = ', pixel)
r = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)') $ pd.read_html(r.text, header=0)[1]
R16df.head()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY)
facts_metrics.dimensions_date_id.nunique()
news.shape
journalists_retweet_df.head()
!pip install patsy $ from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt $ import matplotlib.pyplot as plt $ import pandas as pd
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
print 'Series from a PeriodIndex' $ np.random.seed(123456) $ ps = pd.Series(np.random.randn(12), index=mp2013) $ ps
df_tweet_clean.drop(df_tweet_clean[df_tweet_clean['retweeted_status'].isnull()==False].index,inplace=True)
df2csv2gdb(waterlevels,fileloc,r'H:/GIS/waterlevels.csv','waterlevels')
nyc_census_tracts.crs
diff_ = pumaBB.merge(bbpc, right_on="gid", $                      left_on="public use microdata area")[["pcBB", "HC01"]] $ diff_["diff"] = np.abs(diff_["pcBB"] - diff_["HC01"].astype(float)) $ diff_.describe()
df_2010.dropna(inplace=True) $ df_2010
smallmap = morning_rush.iloc[:1000][['latitude', 'longitude']].get_values()[0:10] $ smallmap.data.tolist()
import pandas as pd $ import matplotlib.pyplot as plt $ import seaborn as sns # package for nice plotting defaults $ sns.set()
state_DataFrames['OH_D_con'].head(30)
df['index1'] = df.index
data_year_df = pd.DataFrame.from_records(data_year, columns=('Date','Station','Prcp')) $ data_year_df.head()
reviews_sample.to_csv('text_preparation/abt_text_analysis_prepared.csv') $
df_all['date'] = temp.date $ df_all['time'] = temp.time $ print(df_all['date'][0:2]) $ print(df_all['time'][0:2])
dates_station = df_station[df_station.STATION == '1 AV'].groupby('DATE').sum().reset_index().DATE $ counts_station = df_station[df_station.STATION == '1 AV'].groupby('DATE').sum().reset_index().ENTRIES $ plt.figure(figsize=(15, 8)) $ sns.barplot(dates_station, counts_station)
users.dtypes
print(np.any(df["dropoff"] < df["pickup"])) $ print(np.any(df["updated_at"] < df["created_at"]))
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'], df2[['intercept','ab_page']])
gdax_trans_btc['Balance'] = gdax_trans_btc['Trade_amount'].cumsum();
autos["odometer_km"].value_counts().sort_index(ascending=False).head(15)
nold=df2[df2['group']=='control'].count()[0] $ nold
test_id = pax_raw.seqn.values[1] $ pax_sample = pax_raw[pax_raw.seqn==test_id].copy()
precip_data_df1.tail(5)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old,n_new], alternative='smaller') $ print('z_score : {}, p_value : {}'.format(str(z_score),str(p_value)))
np.save(embedding_save_path, embeddings_matrix) $
new_txt = r_t_test.replace("outstanding", "<UNK>") $ print(new_txt)
for row in coll.find(): $     print(row)
y = np.log(df_nuevo['price'].values) $ df_nuevo.drop(['price','id','date','zipcode',],axis=1,inplace=True) $ X = df_nuevo.values
table_name = "Nathan_Carto_SQL_API_test" $ res = sql_api(carto_url, select_all_sql, carto_api_token) $ print(res.text)
if 0 == 1: $     news_title_high_freq_words_csv_file = os.path.join(config.HR_DIR, 'news_title_high_freq_words.csv') $     news_title_docs_high_freq_words_df = pd.read_pickle(news_title_docs_high_freq_words_df_pkl) $     news_title_docs_high_freq_words_df.to_csv(path_or_buf=news_title_high_freq_words_csv_file, columns=['high_freq_words'], sep='\t', header=True, index=True)
bottom10 = tt_final.sort_values(['rating_numerator'], ascending=True,).head(10) $ bottom10[['expanded_urls', 'rating_numerator', 'jpg_url', 'text', 'all_p']]
input_data['normalized_commits'] = input_data.groupby('year').num_commits.transform(lambda x: x/x.mean())
pandas.read_csv('Movies_release_date.csv')
%%time $ extract_deduped = f_extract_dedup(extract_all.copy())
set_themes = legos['sets'].merge(legos['themes'], left_on = 'theme_id', right_on = 'id') \ $     .merge(legos['inventories'], on = 'set_num') \ $     .merge(legos['inventory_parts'], left_on = 'id_y', right_on = 'inventory_id')
data.keys()
results_page.summary()
plt.clf()
_delta=(np.abs(volt_prof_before[' pu1']-volt_prof_after[' pu1'])+ $         np.abs(volt_prof_before[' pu2']-volt_prof_after[' pu2'])+ $         np.abs(volt_prof_before[' pu3']-volt_prof_after[' pu3']))
from IPython.display import HTML
prcp_results_df = pd.DataFrame(prcp_results, columns=['date', 'precipitation']) $ prcp_results_df.set_index('date', inplace=True) $ prcp_results_df.head()
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head() $
plot_chernoff_data(df01, 1e-6, 1, "Nth = 0.1") $ plt.savefig('../output/g_perr_vs_M_01.pdf', bbox_inches='tight')
new_df02 = pd.merge(_GoogOut_forgbatmn02, _GoogOut_forgbatmn00,  how='outer', left_on=['trade2','Address'], right_on = ['trade2','Address'],indicator =True) $ pd.crosstab(new_df._merge,new_df.zip_keep) $ pd.crosstab(new_df02._merge,new_df02.zip_keep) $
imagehash.hex_to_hash('ff6042c59f3870e2') - imagehash.hex_to_hash('fe6046c59f3870e2')
(df2.query("converted >= 0")['landing_page'] == 'new_page').mean()
pd.merge(df_a, df_b, on='mathdad_id', how='inner')
df.last_name.isnull().sum()
print("Min: {}, max: {}, mu: {}, std: {}".format( $ df["result"].min(), df["result"].max(), df["result"].mean(), df["result"].std())) $ df["result"].plot.hist(bins=100) $ plt.show()
print('Columns:\n\t'+'\n\t'.join(map(str,[col for col in full.columns])))
clients.to_csv('data/toggl-clients.csv')
pd.set_option('display.max_columns', 100) $ tmpdf
p_diffs = np.random.binomial(n_new, CRnew, 10000)/n_new - np.random.binomial(n_old, CRold, 10000)/n_old $
df = pd.DataFrame(np.random.randn(8, 4), columns = ['A', 'B', 'C', 'D']) $ df
ben_fin.head()
results = session.query(measurement.date, measurement.prcp).filter(measurement.date >= prev_year).all()
pca=decomposition.PCA() $ stocks_pca_t= pca.fit_transform(stocks_pca_m) $
sub1 = sub1.sort(['hacker_id', 'time'], ascending = [False, False])
df_clean.rating_denominator = 10
df.columns = [dict_names[x] for x in df.columns]
against.amount.sum()
df2[['ab_page', 'treatment']] = pd.get_dummies(df2['group']) $ df2.head()
joined = pd.read_sql_query(sql_query, engine) $ joined.head()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_secret) $ api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)
datAll.shape
df_merge.info()
input_node_types_DF = pd.read_csv('network/source_input/node_types.csv', sep = ' ') $ input_node_types_DF
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
 type(model.wv.syn0)
first_result.find('strong').text
df_tweets.sample(10)
cold1 = cold.sample(frac=0.5) $ cold2 = cold.sample(frac=0.5) $ ttest(cold1, cold2)
autos["model"].value_counts()
spark = SparkSession.builder.appName('nyc311').getOrCreate()
ga_sum = GA_profit['Profit Including Build Cost'].sum() $ buildings_ga = len(GA_profit)
plt.figure(figsize=(10,5)) $ sns.countplot(auto_new.Country)
f = plt.figure(figsize=(12, 5)) $ ax1 = f.add_subplot(1, 1, 1) $ ax1.plot(4 + 2 * np.sin(np.arange(50)), 'g--', label='4 + 6*sin(x)')
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ print(os.getcwd()) $ Does_Current_Date_Folder_Exist(os.getcwd())
autos.head()
logit_mod2 = sm.Logit(df_new.converted, df_new[['intercept', 'CA', 'US', 'new_page']]) $ result2 = logit_mod2.fit() $ result2.summary()
print(len(forecast)) $ forecast.tail(3)
v_invoice_link.merge(v_item_hub, left_on = 'fk_x_invoice_item_hub_id', right_on = 'sk_id')
import pickle $ filename = 'automl_feat.sav' $ pickle.dump(automl_feat, open(filename, 'wb'))
S.decision_obj.simulStart.value = "2007-07-01 00:00" $ S.decision_obj.simulFinsh.value = "2007-08-20 00:00"
treatment = pd.Series([0]*4 + [1]*2) $ treatment
Google_stock['Open'].mean()
df.loc[row,'blurb']
states_df.columns = ['State','Abrv','State-hood','Capital','CapitalSince','Area','PopMunic','PopMetro','Rank','Notes'] $ states_df.drop([0,1], axis=0, inplace=True) $ states_df
np.any(x > 8)
df_train['id'].unique()
record_counts = pd.DataFrame(active_psc_records.groupby('company_number')['name'].count()).reset_index() $ record_counts.columns = ['company_number','count'] $ temp_df = pd.merge(record_counts,active_companies_psc_applies_df,left_on='company_number',right_on='CompanyNumber',how='right') $ temp_df['count'] = temp_df['count'].fillna(0) $ temp_df['count'].describe()
store_items.loc[['store 1']]
run txt2pdf.py -o"2018-06-18  2015 470 disc_times_pay.pdf"  "2018-06-18  2015 470 disc_times_pay.txt"
tweets_clean.rating_numerator.unique()
def df_norm(df) : $         newdf = (df - df.mean()) /(df.max() - df.min()) $         return newdf - newdf.min()
session.query(measurement.date).\ $     filter(measurement.station == 'USC00519281').\ $     order_by(measurement.date.desc()).first()
cityID = 'b463d3bd6064861b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Detroit.append(tweet) 
n_new = sum(df2.landing_page == 'new_page') $ n_new
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').converted.sum() $ convert_new = df2.query('landing_page == "new_page"').converted.sum() $ n_old = df2[df2['landing_page'] == 'old_page'].user_id.count() $ n_new = df2[df2['landing_page'] == 'new_page'].user_id.count()
customers = pd.read_table(url_customers,sep=',') $ transactions = pd.read_table(url_transactions,sep=',')
ctrl_con = df2.groupby('group', as_index=False).describe()['converted']['mean'][0] $ print("P(ctrl_con) = %.4f" %ctrl_con)
error_filtered = error_as_pandas_df[error_as_pandas_df['Reconstruction.MSE'] > 0.0025]
to_be_predicted_Day3 = 83.26256668 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
clf = svm.SVC(kernel='rbf')
print((df_pol_d.select_dtypes(include=['O']).columns.values))
labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=stringIndexer_label.labels)
v.purchase_price()
data.shape
y_test.head()
lr.predict(X)
com_grp.get_group(('C1','D3'))
animals = pd.crosstab(df['Type'], df['Outcome']) $ rate = animals.div(animals.sum(1).astype(float), axis=0)*100 $ rate.plot(kind='barh',stacked=True) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
intersections_final_for_update_no_dupe = intersections_final_for_update[~intersections_final_for_update.index.duplicated(keep='first')]
import datetime $ dNow = datetime.datetime.now() $ AcqDate = dNow.strftime("%Y-%m-%d")
remote_data = xr.open_dataset('http://hydromet-thredds.princeton.edu:9000/thredds/dodsC/UrbanRainfall/Phoenix.nc') $ print remote_data
codes = access_logs_df.groupby("responseCode").count()
df_twitter_copy['score_rating'] = df_twitter_copy.rating_numerator / df_twitter_copy.rating_denominator
terror.type.unique()
merged_df_flightdelays = pd.concat([merged_df_prec, flight_delays], axis=1)
df.set_index('Day', inplace=True) # 'inplace = True' will modify the existing dataframe $ df.head()
from pyspark.ml import Pipeline $ from pyspark.ml.feature import OneHotEncoder, VectorAssembler $ from pyspark.ml.regression import DecisionTreeRegressor $ from pyspark.ml.evaluation import RegressionEvaluator
path = "/Users/Yujiao/Desktop/twitter_Ikea/Data/IKEA_database/" $ Stockholm_data = pd.read_csv(path + "stockholm.csv")
from sklearn.base import BaseEstimator, TransformerMixin
to_be_predicted_Day4 = 25.10824769 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df.head()
df_questionable_3[df_questionable_3['state_DC'] == 1]['link.domain_resolved'].value_counts(25)
people_person.channel.value_counts()
np.exp(0.0352),np.exp(0.0108) , np.exp(0.0171)
df.to_csv("test_df_to_csv.csv",index=False)
from sklearn.preprocessing import LabelEncoder $ le = LabelEncoder() $ for i in var_cat: $     df[i] = le.fit_transform(df[i])
austin= pd.merge(left= df, right= df2, on='RIDE_ID')
autos['price'] = (autos['price'] $                   .str.replace('$','') $                   .str.replace(',','') $                   .astype(int)) $ autos.price.head()
plot_aop_refl(ndvi,sercRefl_md['spatial extent'], $               colorlimit = (np.min(ndvi),np.max(ndvi)), $               title='SERC Subset NDVI \n (VIS = Band 58, NIR = Band 90)', $               cmap_title='NDVI', $               colormap='seismic')
temp = train.groupby("Block")["any_spot"].mean() $ train['Block_more_parking'] = train["Block"].isin(temp[temp>0.55].keys()) $ train['Block_less_parking'] = train["Block"].isin(temp[temp<0.2].keys()) $ val['Block_more_parking'] = val["Block"].isin(temp[temp>0.55].keys()) $ val['Block_less_parking'] = val["Block"].isin(temp[temp<0.2].keys())
df1 = crimes.assign(Temperature = [temp[0],temp[1],temp[2],temp[3],temp[4],temp[5],temp[6],temp[7],temp[8],temp[9], $                                    temp[10],temp[11]]) $ df2 = df1.assign(Precipitation = [precip[0],precip[1],precip[2],precip[3],precip[4],precip[5],precip[6],precip[7], $                                  precip[8],precip[9],precip[10],precip[11]]) $ df2
users = sql.read.option("inferSchema", "true").option("header", "true").csv(data_dir + "/users.txt.bz2") $ events = sql.read.option("inferSchema", "true").option("header", "true").csv(data_dir + "/events.txt.bz2") $ store_items = sql.read.option("inferSchema", "true").option("header", "true").csv(data_dir + "/store_items.txt.bz2") $ stores = sql.read.option("inferSchema", "true").option("header", "true").csv(data_dir + "/stores.txt.bz2") $ promotions = sql.read.option("inferSchema", "true").option("header", "true").csv(data_dir + "/promotions.txt.bz2")
Results_kNN1000.to_csv('soln_kNN1000.csv', index=False)
g.columns = ['customer','validOrders'] $ print(g["validOrders"].sum()) $ print(g.shape) $ g.head()
learning_rate = 0.01 $ with tf.name_scope("train"): $     optimizer = tf.train.GradientDescentOptimizer(learning_rate) $     training_op = optimizer.minimize(loss)
s1 = pd.Series([0, 1, 2, 3]) $ s2 = pd.Series([0, 1, 2, 3]) $ s3 = pd.Series([0, 1, 4, 5]) $ d = pd.concat([s1, s2, s3], axis=1) $ d
festivals.index
pd.get_dummies(df.C)
df_transactions['discount'].unique()
user_project_grouped = user_project.groupby(['user_id','project_id']).count() $ user_project_grouped.head()
p_newpage = df2.query('landing_page == "new_page"').count()['user_id']/df2.count()['user_id'] $ print('The probability of someone landing on the New page is {:2f}'.format(p_newpage)) $
symbols = re.compile("[^a-zA-Z]") $ word_freq_df = word_freq_df[~word_freq_df.Word.str.contains(symbols)]
with open("vis/interactive_streamgraph_template.html", "r") as template: $     content = template.read() $     content = content.replace("${FILENAME}", "modifications" + FILENAME_SUFFIX + ".csv") $     with open(FILENAME_PREFIX + "modifications" + FILENAME_SUFFIX + ".html", "w") as output_file: $         output_file.write(content)
xgb.fit(X_train, y_train)
df_all['US'] = df_all['country'].replace(('US', 'UK', 'CA'), (1, 0, 0)) $ df_all['UK'] = df_all['country'].replace(('US', 'UK', 'CA'), (0, 1, 0)) $ df_all['CA'] = df_all['country'].replace(('US', 'UK', 'CA'), (0, 0, 1))
to_be_predicted_Day3 = 26.68060428 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
staff = staff.set_index('Name') $ staff
mean_for_each_weekday = mean_for_each_weekday.sort_values(by=['day_order']) $ mean_for_each_weekday
for s in sentences: $     scores = analyzer.polarity_scores(s) $     print("\"{0}\" ==> {1}".format(s, scores['compound']))
points_df.to_csv('all_gps_from_garmin.csv')
train.pivot_table(values = 'Fare', index = 'Hours', aggfunc=np.mean)
plt.plot(h.t, h.logdict['T1'])
most_confident_predictions = pd.merge(left=highest_confidence, right=stacked_image_predictions, how='left',on=['tweet_id','confidence'])
x = np.zeros(4, dtype=int)
pd.Period('1/2016')
sum(data.tag.value_counts().unique())
p_diffs = [] $ for x in range(10000): $     new_page_converted = np.random.binomial(1, p_new, n_new) $     old_page_converted = np.random.binomial(1, p_old, n_old) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean()) $
df.rename(columns={"Indicator":"Indicator_Id"},inplace=True)
df_new['CA_ab_page'] = df_new['CA']*df_new['ab_page'] $ df_new['UK_ab_page'] = df_new['UK']*df_new['ab_page'] $ df_new['US_ab_page'] = df_new['US']*df_new['ab_page'] $ df_new.head(1)
vueling = df[df["text_3"].str.contains("vueling", case = False)] $ vueling.text_3.str.split(expand=True).stack().value_counts().head(10).reset_index()
tweets_df.retweeted.value_counts()
sum(df.query('group == "treatment"')['landing_page'] == 'old_page') + sum(df.query('group == "control"')['landing_page'] == 'new_page')
y_pred= classifier.predict(X_test) # predicting test set 
count_polarity.reset_index(inplace=True)
medals_data = medals.assign(oecd=medals.index.isin((oecd_year[oecd_year<1997]).index).astype(int))
df_clean = pd.melt(df_clean, id_vars = ['tweet_id', 'timestamp', 'text', 'expanded_urls', $                       'rating_numerator', 'rating_denominator', 'name' ]) $ df_clean.info() $
n_old = df2.loc[(df2.landing_page == "old_page")].user_id.nunique()
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("Probability of conversion for new page (p_new) is", p_new)
clean_users[clean_users['active']==0].dropna(how='any')['account_life'].mean()
data=get_data(128) $ learn.freeze() $ learn.fit(lr,3,cycle_len=1,cycle_mult=2)
print(df['one'].isnull(),df['one'].isnull())
df.dtypes
post_df = pd.DataFrame(data) $ post_df.head()
from sklearn.model_selection import train_test_split as tts $ X_train, X_test, Y_train, Y_test = tts(X,Y, test_size=0.2, random_state = 56)
months['date'] = pd.to_datetime(months['starttime']) $ months.head()
X_train.shape, X_val.shape
filter_df = filter_df[filter_df['race'] == 'PRES'] $ filter_df.head(2)
search_key_words = "\"" + search_key_words.replace(" ","+") + "\"" $ df['newspaper_search'] = df['newspaper'].apply(lambda x: x.replace("/","%2F"))
autos = autos[autos["price"].between(1, 350000)]
blocksBID=blocks.merge(df, left_on='bid_id', right_on='org_id', how='inner') $ blocksBID.head()
chakin.download(number=2, save_dir='.', )
input_ids = [n.node_id for n in inputNetwork.nodes()] $ from bmtk.utils.io.spike_trains import PoissonSpikesGenerator $ psg = PoissonSpikesGenerator(gids=input_ids, firing_rate=0.5, tstart=0.0, tstop=10000.0) $ psg.to_hdf5(file_name='network/source_input/poission_input_spk_train.h5')
data_mean.mean(axis=1, level='type')
ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'] $ )['CampaignId'].count()
x = pd.DataFrame(autodf.notRepairedDamage.value_counts()) $ print "Percentage of cars not repaired = " + str(round( x.ix['ja'] * 100 /x.ix['nein'],2)) + " %"
df.head()
plt.plot(df['Gross Sales'],df['Units Sold']**2,'.')
ax.set_xlabel('Time') $ ax.set_ylabel('Price') $ ax.legend(loc='upper left') $ plt.show()
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $
hours = bikes.groupby('hour_of_day').agg('count') $ hours['hour'] = hours.index $ hours.start.plot(color = 'lightgreen') $
model.add(Conv1D(filters=10, kernel_size=10, $                  activation='relu', padding='same'))
freq = nltk.FreqDist(tokens) $ pd.Series(freq).sort_values(ascending=False).head(20)
archive_df = archive_df[archive_df.retweeted_status_id.isna() & $                         ~archive_df.expanded_urls.isna()] $ len(archive_df)
df2.drop_duplicates('user_id',inplace=True) $ df2.info()
y.values
c.execute(query) $ results = c.fetchall() $ df = pd.DataFrame(results) $ users_df = df.rename(columns={0:'User', 1:'Contributions', 2:'First contribution', 3:'Last contribution'}) $ users_df
pd.date_range('2005', periods=3, freq='A-JUN')
psy_hx.drop_duplicates(subset=None, keep='first', inplace=True) $ psy_hx.shape
ndExample = df.values $ ndExample
_ = ok.grade('q05') $ _ = ok.backup()
candidate = [r for r in all_results('/candidate/H0CA27085/', {})] $ candidate_df = pd.DataFrame(candidate) $ candidate_df.head()
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, 1-p_new]) $ new_page_converted.mean()
talks[['id','text']].to_json('talks_other_text.json')
df.groupby("pickup_month")["cancelled"].mean()
parking_planes_df.to_excel('parking_planes_df1.xlsx', index=False)
corr_matrix=df3.corr()
openmc_geometry = openmc.Geometry(root_universe) $ openmc_geometry.export_to_xml()
users.tail()
twitter_archive_master=twitter_archive_master.replace('None', np.NaN) $ twitter_archive_master.rating_numerator=twitter_archive_master.rating_numerator.astype(float) $ twitter_archive_master.retweet_count=twitter_archive_master.retweet_count.astype('int64') $ twitter_archive_master.favorite_count=twitter_archive_master.favorite_count.astype('int64')
kmeans.labels_
ben_fin.describe()
%matplotlib inline $ import seaborn; seaborn.set() $ import matplotlib.pyplot as plt $ data.plot() $ plt.ylabel("Hourly Bicycle Count");
archive.puppo.value_counts()
tm_2050 = pd.read_csv('input/data/trans_2050_m.csv', encoding='utf8', index_col=0)
pd.to_datetime(3e9)
df = pd.read_csv('data/goog.csv') $ df
df_new['Codebuddy_rank'] = a[0].values $ df_new['Codebuddy_problem_solved'] = a[1].values $ df_new['Codebuddy_points'] = a[2].values $ (df_new.style $     .set_table_styles(styles)) $
model_3 = graphlab.linear_regression.create(train_data, target = 'price', $                                                   features = model_3_features, $                                                   validation_set = None)
SparkSession.builder.getOrCreate().stop()
submission = reddit.submission(url='https://www.reddit.com/r/funny/comments/3g1jfi/buttons/') $ submission.comment_sort = 'new'   #sort from newest $ submission.comments.replace_more(limit=None)
modelOutputDir = '../model-output/ann' $ modelcheckpoint = ModelCheckpoint(filepath=output_dir+"/weights.{epoch:02d}.hdf5") $ if not os.path.exists(output_dir): $     os.makedirs(output_dir)
for row in session.query(user_alias, user_alias.name).all(): $ ...    print(row.user_alias)
print(train.values[0][1]) $ print(train.values[-1][1]) $ print(test.values[0][1]) $ print(test.values[-1][1])
df_vimeo_selected.to_csv('/Users/nikhil.mogare/Desktop/DSA_Reporting/Week3_Sep19/CSVs/vimeo_selective.csv')
from sqlalchemy.orm import Session $ session = Session(bind=engine)
pclass.value_counts().sort_index().plot(kind='bar') $ pclass.value_counts()
july = summer.ix[datetime(2014,7,1) : datetime(2014,7,31)] $ july[['Mean TemperatureC', 'Precipitationmm']].plot(grid=True, figsize=(10,5))
autos['brand'].value_counts(normalize=True)
df.sort_values(['Country','Year','WHO Region', 'Publication Status']).sort_index().head(3)
col['VEHICLE TYPE CODE 1'] = col['VEHICLE TYPE CODE 1'].fillna('') $ col['VEHICLE TYPE CODE 2'] = col['VEHICLE TYPE CODE 2'].fillna('') $ col['VEHICLE TYPE CODE 3'] = col['VEHICLE TYPE CODE 3'].fillna('') $ col['VEHICLE TYPE CODE 4'] = col['VEHICLE TYPE CODE 4'].fillna('') $ col['VEHICLE TYPE CODE 5'] = col['VEHICLE TYPE CODE 5'].fillna('')
for f in files_txt: $     data = pd.read_csv(f, sep="|", header=None) $     df_list.append(data) $ df_full = pd.concat(df_list, axis=0)
df = pd.DataFrame(np.random.randn(1000, 4), index=random_series.index, columns=list('ABCD')) $ df.head()
%%time $ X_df.iloc[:, 5] = X_df['text'].apply( lambda rev: rev.replace('_', '') )
tweets_df.in_reply_to_screen_name.describe()
from gensim.models.lsimodel import LsiModel $ from gensim import corpora $ from gensim.matutils import corpus2dense $ from sklearn.ensemble import RandomForestRegressor $
with open("../data/reputation.txt") as f: $     data = f.read()
autos.info()
from sklearn import preprocessing $ max_abs_scaler = preprocessing.MaxAbsScaler() $ X_train = max_abs_scaler.fit_transform(pokemon_train.iloc[:,2:8]) $ X_test = max_abs_scaler.fit_transform(pokemon_test.iloc[:,2:8])
loans_df.emp_length.replace(replace_dict, inplace=True)
print(df2,'\n') $ print(df2[df2['E'].isin(['test'])])  # select E column=='test' only 
n_old=df2.query('group == "control"').shape[0] $ n_old
df['timestamp']=pd.to_datetime(df['timestamp'], format='%Y-%m-%d')
print("Accuracy = {:.2f}".format(lr.score(test_Features, test_species)))
accuracy = metrics.r2_score(y_train, xgb.predict(X_train)) $ print("R2 Accuracy:", accuracy)
p_new_minus_p_old = new_page_convertered.mean() - old_page_converted.mean() $ p_new_minus_p_old
heights_A=pd.Series([176.2, 158.4, 167.6, 156.2,161.4],index=['s1','s2','s3','s4','s5'])
file = path + "/ks-projects-201612.csv" $ ks_projects = pd.read_csv(file, encoding = 'latin1') $
live_weights = weight.dropna() $ print(len(live_weights))
mtcars = h2o.import_file(path = os.path.realpath("../data/mtcars.csv"))
students.sort_index(axis=1)
target1.size(0)
Gc=df_usa['GDP']/df_usa['Population'] $ df_usa['GDP/capita']=Gc $ df_usa.head()
pred=model.predict(nn_X_test)
tweet_df["Date_tweeted"] = pd.to_datetime(tweet_df["Date_tweeted"]) $ tweet_df.sort_values("Date_tweeted", inplace=True) $ tweet_df.reset_index(drop=True, inplace=True) $ tweet_df.head()
df['token_count'] = df['body_tokens'].apply(lambda x: len(x)) $ print(df[['body_tokens','token_count']])
df.shape
test.shape
y = pd.Period('2016') $ y
url1 = "https://api.propublica.org/congress/v1/members/R000435/votes.json"
es.indices.get_settings(index='dealinfos')
pt_unit=pd.DataFrame.pivot_table(df_unit,index=['course_name','unit_name'],values=['uid'],aggfunc='count',fill_value=0) $ pt_unit_top=pt_unit.sort_values(by='uid',ascending=0)
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['rating_numerator'] >= 15].index)
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s')
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
import numpy as np $ data = np.array(['a','b','c','d']) $ ser1 = pd.Series(data) $ print(ser1)
df['Duration'].plot.hist(xlim=(0,100)) # doesn't do what we want
test_data.head()
shifts_w_pauses = df[df['pause'].astype('timedelta64[m]') > 0] $ user_counts = shifts_w_pauses.groupby('user').count()['duration'] $ print('Users who use pauses: {}\n\n'.format(len(user_counts))) $ print(user_counts)
df2["timestamp"] = pd.to_datetime(df2["timestamp"]) $ min_time = min(df['timestamp']) $ max_time = max(df['timestamp']) $ print("earliest available date = {} & latest available date = {}".format(min_time,max_time))
for k, v in d.items(): $     print(k, v)
weather_mean = weather_all.groupby('Station Name').mean() $ weather_mean
idxx = 562 $ station_bounds.iloc[idxx][2]
df[((df['group'] == 'control') == (df['landing_page'] == 'old_page')) == False]["user_id"].count()
df_archive_clean["source"].unique()
df.apply(np.cumsum)
res3 = df_test3_promotions.groupby('PromotionID') \ $     .agg({'PromotionRuleCount':'sum', 'AwardAmount':'sum', 'ShopperID':'nunique'}) $ res3['AwardAmount'] = res3['AwardAmount'] / 100 $ res3
X_test_all.shape
import datetime $ austin['started_on'] = pd.to_datetime(austin['started_on']) $ dayOfWeek={0:'Mon', 1:'Tue', 2:'Wed', 3:'Thu', 4:'Fri', 5:'Sat', 6:'Sun'} $ austin['weekday'] = austin['started_on'].dt.dayofweek.map(dayOfWeek)
n_net.score(x_test,y_test)
HMM('LNKD', datetime.datetime(2016,4,1)-timedelta(211), datetime.datetime(2016,4,1),True)
df2.drop(labels=2893,axis=0,inplace=True) $ df2[df2['user_id']==773192]
autos.info()
pd.to_datetime(df['Timestamp']).head()
p_new = df2['converted'].mean() $ print("The convert rate for p_new under the null: ",p_new)
hourly_df['Open_Price_Change'].value_counts()
t_user = len(df2.query("group == 'treatment'")) $ users = df2.shape[0] $ new_p = t_user/users $ output = round(new_p,4) * 100 $ print("The probability that an individual receives the new page is: {}%".format(output))
df_new.reset_index(inplace=True) $ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new.head()
(ab_data.converted == 1).mean()
new_trump_df.head(10)
sns.countplot(auto_new.Payment_Option)
df_predictions_clean.head()
GenresString=Genres.replace('|',',') $ GenresString=GenresString.replace(',',',\n')
logit = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result=logit.fit() $
neo_github_issues_url = blockchain_projects_github_issues_urls[2] $ neo_github_issues_df  = pd.read_json(get_http_json_response_contents(neo_github_issues_url))
pvt['ga:date'] = pd.to_datetime(pvt['ga:date'], format='%Y%m%d', errors='coerce') $ pvt.rename(columns={'ga:transactionId': 'transactionId', 'ga:date': 'date'}, inplace=True) $ pvt
plt.plot(ds_cnsm['time'],ds_cnsm['met_salsurf_qc_results'],'.') $ plt.title('CP01CNSM, OOI QC Results SSS') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show() $
goog.describe()
logm2 = sm.Logit(df_con['converted'], df_con[['intercept', 'ab_page','US', 'UK']]) $ result = logm2.fit() $ result.summary2()
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] == 'treatment', 1,0) $ df2.head()
autos.info()
titanic.dropna().head()
p_diffs = [] $ for _ in range(10000): $     new_page_converted=np.random.binomial(n_new,p_new) $     old_page_converted=np.random.binomial(n_old,p_old) $     p_diffs.append(new_page_converted/n_new-old_page_converted/n_old)
charge = reader.select_column('charge', start=0, stop=100) $ charge = charge.values # Convert from Pandas Series to numpy array $ charge
df.isnull().any(axis=1).any()
print("Probability of user converting:", df2.converted.mean()) #using mean to find the  probability of an individual converting regardless of the page they receive
model.wv["flower"][0:10]
df2 = df.query('~((group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page == "new_page"))') $ df2.shape[0]
plt.hist(taxiData.Trip_distance, bins = 50, range = [150,taxiData.Trip_distance.max()]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
print(about.text)
active_df.sort_values(by="station_count", ascending=False, inplace=True)
np.log(points)
TestData_ForLogistic.shape
datasets_co_occurence = paired_df_grouped[['dataset_1', 'best_co_occurence']].set_index('dataset_1').to_dict()['best_co_occurence']
df_transactions.hist(figsize=(15,12),color = 'y') $ plt.show()
op_add_comms.head(10)
two_day_sample.groupby('date').steps.max()
tweet_model.wv.similar_by_word('Sheeran')
from sklearn.preprocessing import LabelEncoder $ le = LabelEncoder() $ cat_var = ["VendorID","Store_and_fwd_flag","RateCodeID","Trip_type ","Payment_type"] $ for var in cat_var: $     trip_data_sub[var] = le.fit_transform(trip_data_sub[var].astype('category')) 
eastern = ts_utc.tz_convert('US/Eastern')
with tf.Session() as sess: $     saver.restore(sess, final_model_path) $     accuracy_val = accuracy.eval(feed_dict={X: X_test, y: y_test})
uber_14["month"] = uber_14["Date/Time"].apply(lambda x: int(x[0:1])) $ uber_14["day_of_month"] = uber_14["Date/Time"].apply(lambda x: int(x.split("/")[1])) $ uber_14["day_of_year"] = uber_14["Date/Time"].apply(lambda x: x.split(" ")[0]) $ uber_14.head()
sum(df.duplicated())
df = pd.DataFrame(data=data1['adj close']) $ df['Forecasted close(SVR)'] =np.concatenate((np.array([np.nan for x in range(daysToForecast)]), predictSVR),axis=0) $ df['Forecasted close(ANN)'] =np.concatenate((np.array([np.nan for x in range(daysToForecast)]), predictANN),axis=0)#one $ df.dropna(inplace=True)
result['teamId'].nunique()
time.gmtime().tm_year
log_mod = sm.Logit(df3.converted,df3[['intercept','country_UK','country_US']]) $ result_2 = log_mod.fit() $ result_2.summary()
fig, ax = plt.subplots(figsize = (20,15)) $ plt.hist(np.log(df.num_comments),bins = 100,range = (0,25));
c.execute('UPDATE cities SET state="CA" WHERE state="Californ-I-A"') $ c.execute('SELECT state FROM cities') $ conn.commit() $ print(c.fetchall())
df.to_csv('Crimes_-_2001_to_present_v2.csv', index=False)
print('dataframe shape: {}'.format(country.shape)) $ country.head()
df_cryptdex = df_history[['symbol','date','close','movement_delta']].copy()
len(model.wv.vocab)
tr_roll = roll_17.merge(roll_90, 'left', ['date', 'store_nbr']).merge(roll_360, 'left', ['date', 'store_nbr'])
dfp.groupby('chamber').bill_type.count().plot(kind='bar', color='r') $ plt.xlabel("Chamber the Bill was Introduced") $ plt.title("") $ plt.show()
d = {'Game': games, 'Prediction':[pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9, pred10], 'Probability (1, 2)': [prob1, prob2, prob3, prob4, prob5, prob6, prob7, prob8, prob9, prob10], 'Actual Result': [2 , 2, 2, 1, 1, 1, 2, 1, 2, 1]} $ df3 = pd.DataFrame(data = d) $ df3
data.drop(columns=['deadline', 'launched_at'], axis=1, inplace=True)
df_twitter_archive_copy.source.value_counts()
data['subreddit_other'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
grouped_by_origin['DepDelay'].mean().sort_values(ascending=False).head()
df_opc = df2[(df2['landing_page'] == 'old_page')] $ conv = df_opc['converted'] $ old_page_converted = np.random.choice(conv, nold)
writer = pd.ExcelWriter("test.xls")
num = re.sub(r'\D', '', phone) $ print('Phone Num : {}'.format(num))
%%time $ results_single = ppm.process_text(data_to_clean[:benchmark_n])
bnbAx[bnbAx['language']=='fr'].country_destination.value_counts().plot.pie()
import numpy as np $ import matplotlib.pyplot as plt $ import pandas as pd $ df=pd.read_csv('inf.csv')
date_list = [] $ for day in days: $     date_list.append(day.strftime("%m-%d"))
old_page_converted = np.random.binomial(n_old, p_old) $ old_page_converted
authors = db.get_sql(sql) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
test['Month']     = test["date"].dt.month $ test['Day']       = test["date"].dt.day $ test['DayOfWeek'] = test["date"].dt.dayofweek
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True) $ df.info()
ek.set_app_id('D163218EE154B9D1851F8C9')
flight6.cache()
df_2008.dropna(inplace=True) $ df_2008
print(pd.DataFrame(test_bow).head())
list_of_df = predictor.predict(pred_data[:5]) $ actual_data = test_data[:5]
p_diffs = np.asarray(p_diffs) # converting p_diffs to numpy array
multi_bulk = sess.get_data(['ibm us equity','aa us equity'],['best analyst recs bulk','dvd hist all','px last','px open']) $ multi_bulk
with open(INPUT_FILE_PATH,encoding='utf8') as f: $     contentData = f.readlines()
treatment = pd.Series([0]*4 + [1]*2) $ treatment
br = pd.DataFrame(br_pr, columns=['mean_price']) $ br
print(stations.head())
predictions_proba_svc = svc.predict_proba(X_test_scaled) $ predictions_proba_dtc = dtc.predict_proba(X_test_scaled) $ predictions_proba_rfc = rfc.predict_proba(X_test_scaled)
sensors_num_df.entities
cnx.commit() $ c.fetchall()
most_yards[['Date','PlayType','Yards.Gained','qtr','desc','Rusher','Receiver', 'Jimmy']][:10]
DataSet['tweetSource'].value_counts()
bg_df = pd.DataFrame(bg_s) # create dataFrame $ bg_df # a nice name to remind you this one is a DataFrame
plt.figure(figsize=(10, 20)) $ plt.title('Cohorts: User Retention') $ heat_map = sns.heatmap(retention_10_per, cmap=sns.color_palette('Blues', 40)) $ figure = heat_map.get_figure()    $ figure.savefig('user retention.png', dpi=1000)
from scipy.stats import norm $ norm.cdf(z_score),norm.ppf(1-(0.05/2))
v=sns.factorplot(data=tweets_df, x="name", y="favorite_count", kind="box") $ plt.xticks(rotation=60) $ v.set(yscale="log")
tweetsIn22Mar.head() $ tweetsIn1Apr.head() $ tweetsIn2Apr.head()
cityID = '629f4a26fed69cd3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Hialeah.append(tweet) 
df_new[(df_new['group'] == 'control')]['converted'].mean() $
from sklearn.decomposition import PCA $ pca = PCA(n_components=0.95) $ X_train_reduced = pca.fit_transform(X_train)
api_clean = from_api.copy()
m[0].bs = 1 $ m.eval() $ m.reset() $ res, *_ = m(t) $ m[0].bs=bs
import re $ def clean_tweet(tweet): $     return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split()) $
fwd = df[['Store'] + columns]\ $     .sort_index(ascending=False)\ $     .groupby("Store")\ $     .rolling(7, min_periods=1).sum()
df2.query('user_id =="773192"')
first_movie.strong
import seaborn as sns $ sns.set(style="darkgrid") $ ax = sns.countplot(x="AGE_groups", data=df_CLEAN1A) $
from Classes.TweetsProcessor import TweetsProcessor $ allImages = TweetsProcessor.getAllImages(tweetsDuringEvent, 'image_url')
close_idx = afx['dataset']['column_names'].index('Close')
df_subset['Initial Cost'] = pd.to_numeric(df_subset['Initial Cost'].str.replace('$',''), $                                           errors = 'coerce') $ df_subset['Total Est. Fee'] = pd.to_numeric(df_subset['Total Est. Fee'].str.replace('$',''), $                                           errors = 'coerce') 
df2.groupby(['group', 'converted']).describe().iloc[:4, :1]
type(date_df.created_time.loc[0])
goog.plot();
new_df = pd.DataFrame(columns=['movieId','title','genre','year'])
pd.crosstab(aqi['AQI Category_eug'], aqi['AQI Category_cg'], margins=True)
for x in prop.columns: $     print (x,prop[x].isnull().sum())
newest=pd.merge(detroit_census2, portland_census2, how='inner', $                          left_on = "Fact", right_on = "Fact", indicator = True) $ population=newest.drop(newest.index[1:]) $ population.head()
h2o.init(max_mem_size = "2G")             #specify max number of bytes. uses all cores by default.\n $ h2o.remove_all()                          #clean slate, in case cluster was already running
df_hours = df_min[(df_min['CumulativeEngineHours'] >= 0)] $ AO = pd.Series(df_hours['CumulativeEngineHours'].values, index=df_hours['StartDateTime']) $ AO.plot()
df['dayofweek']=df['effective_date'].dt.dayofweek $ NiceHist('dayofweek',df)
urban_driver_total = urban_type_df.groupby(["city"]).mean()["driver_count"] $ urban_driver_total.head() $
target_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=12000, padding ='post') $ target_vecs = target_proc.fit_transform(target_docs)
prob_for_new_page = (df2.landing_page == 'new_page').mean() $ print('Probability that an individual received the new page: ') $ print(prob_for_new_page)
def error_func(allocs): $     data = allocate_equities(allocs=allocs) $     r= -sharpe_ratio(data,daily_rf=0.00037828653,portfolio=True) $     return r
joined.reset_index(inplace=True)
after['count'] = after.groupby('hashtags')['hashtags'].transform(pd.Series.value_counts) $ after.sort('count', ascending=False) $ after.hashtags.dropna().head()
total_hacker_list = list(submissions['hacker_id'].unique())
c.execute(query) $ results = c.fetchall() $ df = pd.DataFrame(results) $ users_df = df.rename(columns={0:'User', 1:'Contributions'}) $ users_df
tbl.head()
random_forests_grid.fit(X_train,y_train) $
top_songs['Artist'].dtype
df.head().to_json("msft.json") $ !cat msft.json
categorical = free_data.dtypes[free_data.dtypes == "object"].index $ free_data[categorical].describe()
soup = BeautifulSoup(page, 'html5lib') $ print soup.prettify()[:1000]
import os.path as op $ my_gempro.save_pickle(op.join(my_gempro.model_dir, '{}.pckl'.format(my_gempro.id)))
df[df['converted'] == 1].shape[0]/df.shape[0]
new_page_converted.mean()
df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner') $ df_new.head()
print (df.shape) $ df2 = df.drop(df.index[[5, 12, 23, 56]]) $ print (df2.shape) $
y_true, y_pred = y_test, gs.predict(X_test)
df.query('group == "treatment" and landing_page != "new_page"').shape[0] + df.query('group != "treatment" and landing_page == "new_page"').shape[0]
ls_other_columns = df_onc_no_metac.loc[:, ls_both].columns $ for column in ls_other_columns: $     print(column) $     df_onc_no_metac[column].unique()
df = pulledTweets_df[pulledTweets_df.airline != 'USAirways']
plt.hist(commits.log10_commits, bins=50) $ plt.xlim((0.5,3)) $ plt.title('Distribution of Commits per Repo') $ plt.xlabel('Log10(commits per repo)') $ plt.ylabel('Count');
sess.get_data('ibm us equity','g:ohlc')
known_shorteners = ux.constants.all_short_domains.copy() $ known_shorteners[:25]
%%timeit $ scipy.optimize.broyden2(globals()[function_name], 2, f_tol=1e-14)
properati['state_name'].value_counts()
import statsmodels.api as sm $ convert_old = df2.query('group == "control"').sum()['converted'] $ convert_new = df2.query('group == "treatment"').sum()['converted'] $ n_old = df2.query('landing_page == "old_page"').count()['user_id'] $ n_new = df2.query('landing_page == "new_page"').count()['user_id']
gaussian = GaussianNB() $ gaussian.fit(X_train, Y_train) $ Y_pred = gaussian.predict(X_test) $ acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2) $ acc_gaussian
time6MDict = averagebytime(typesDict, '6M') $ time6MDict.keys()
len(digits.target)
data = trends.interest_over_time()
topics_top_doc = docs_by_topic.idxmax()[['TopicWt']] $ topics_top_doc['DocText'] = [submissions.iloc[x]['text'] for x in topics_top_doc['TopicWt']] $ topics_top_doc
shannon_petropavlovsk = - sum(np.log2(petropavlovsk_freq.values) * petropavlovsk_freq.values) $ shannon_petropavlovsk
logit = sm.Logit(data['Churn'], data[train_cols]) $ result = logit.fit()
d.head()
doi_pid.shape[0], doi_pid.drop_duplicates().shape[0]
predicted_values = model_NB.predict_proba(X_test)
dfjoined = dfrecent.merge(dfcounts, how = 'inner', on = ['created_date'])
ms_columns = inspector.get_columns('measurements') $ for c in ms_columns: $     print(c['name'], c["type"]) $
X_train, X_test, y_train, y_test = train_test_split(X_features,tweets_1['sentiment'], test_size = .2) $
p_old=df2.query('converted==1').user_id.count()/df.shape[0] $ p_old
walk = walk.cumsum()
twitter_archive.info()
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import StandardScaler $ from sklearn_pandas import DataFrameMapper
wb.save('most_excellent.xlsx')
df.info()
airports_by_city.map(lambda x: len(x)).sort_values(ascending=False).head(10)
result = api.search(q='%23puravida') #%23 is used to specify '#' $ len(result)
n = training_data.shape[0] $ kf = KFold(n_splits=5, random_state=123, shuffle=True) $
season14 = ALL[(ALL.index >= '2014-09-04') & (ALL.index <= '2015-02-01')]
df3.index
likes.groupby('reaction').size()
address_fields = ['address.address_line_1', 'address.address_line_2', 'address.country', 'address.postal_code'] $ active_psc_records.groupby(address_fields)[['company_number']]\ $     .agg(unique_company_count).sort_values(by='company_number',ascending=False).head(10)
data_frame_lengths = [len(sheet) for sheet in sheets.values()] $ display(data_frame_lengths) $ sum(data_frame_lengths)
mb.sum(level='Taxon')
df.dtypes[0]
model = sm.tsa.ARIMA(train, (1, 1, 0)).fit() $ print(model.params)
import seaborn as sns $ import matplotlib.pyplot as plt $ from matplotlib.colors import ListedColormap $ %matplotlib inline
multiindex_output = table_to_compare.set_index(keys=['email', 'SKU']) $ multiindex_output
geo_TempJams.info()
data.loc[data.surface_covered_in_m2.notnull(), 'surface/price'] = data['surface_covered_in_m2']*data['price_aprox_usd']/data['surface_total_in_m2']
output= "SELECT * from user where user_id='@Pratik'" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['User_id','User Name','Followers','Following','TweetCount'])
lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics = topics)
park.named_fda_drug.notnull().sum()
dfX_hist = pd.read_csv('df_factors_PILOT.csv')
column_list = sheets['Erl'].columns ^ sheets['Tyler'].columns $ display(column_list)
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 1 $ forecast_df['white_pct'] = forecast_df['white_pop'] / forecast_df['pop'] * 100 $ forecast_df.drop('white_pop', axis=1, inplace=True) $ forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 0 $ forecast_df.head()
users.head()
modern_pings = Dataset.from_source("telemetry") \ $     .where(docType='main') \ $     .where(submissionDate="20170716") \ $     .records(sc, sample=0.01)
import os $ print("Running all tests...") $ _ = [ok.grade(q[:-3]) for q in os.listdir("ok_tests") if q.startswith('q')]
scr_churned_df = pd.DataFrame(index=daterange,columns=daterange).fillna(0)
treehouse_expression_hugo_tpm_pruned = \ $     treehouse_expression_hugo_tpm.loc[:, treehouse_labels_pruned.index] $ print(treehouse_expression_hugo_tpm_pruned.shape) $ assert(tcga_target_gtex_expression_common.index.equals( $     treehouse_expression_hugo_tpm_pruned.index))
JAR_FILE = "/usr/lib/hadoop-mapreduce/hadoop-streaming.jar" $ HDFS_DIR = "/user/root/hw3" $ HOME_DIR = os.getcwd() 
ekos.load_workspace(user_id, alias = workspace_alias) $
matches = tweets.text.str.contains('dead') $ tweets[matches]
options_frame.head()
missing_values = missing_values_table(pay_train) $ missing_values.head(20)
fraud_df.head(3)
pi = 3.1415927 $ print(round(pi)) $ print(round(pi, 3))
QUIDS_wide.dropna(subset =["qstot_0"], axis =0, inplace=True)
df_t['Postal Code Int'] = df_t['Postal Code'].apply(lambda x: int(x[:4]))  # get the integer part of the postal code $ df_t = pd.merge(left=df_t,right=df_geo,how='left',left_on='Postal Code Int',right_on='Postal Code',validate="many_to_one")
df3_month = df3[['Donation Received Month-Year', 'Donor ID', 'Donation Amount']].groupby(by='Donation Received Month-Year', as_index=False).agg({'Donation Amount': ['min', 'max', 'mean', 'sum', 'count'], 'Donor ID': pd.Series.nunique}) $ df3_month.columns = [' '.join(col).strip() for col in df3_month.columns.values] $ df3_month.rename(columns = {'Donor ID nunique': 'Count of Unique Donors', 'Donation Amount min': 'Minimum Donation', 'Donation Amount max': 'Maximum Donation', 'Donation Amount mean': 'Mean Donation', 'Donation Amount sum': 'Total Donations', 'Donation Amount count': 'Count of Donations'}, inplace=True)
x = (df[(df.group == 'treatment') & (df.landing_page == 'old_page')]).count() $ y= (df[(df.group == 'control') & (df.landing_page == 'new_page')]).count() $ (x+y).converted
users = users.join(newgeo)
elnet = ElasticNet(alpha=0.0001, l1_ratio=0.25) $ elnet.fit(train_data, train_labels)
for df in (joined,joined_test): $     df["CompetitionMonthsOpen"] = pd.to_timedelta(df["CompetitionDaysOpen"]).dt.days//30 $     df.loc[df.CompetitionMonthsOpen>24, "CompetitionMonthsOpen"] = 24 $ joined.CompetitionMonthsOpen.unique()
cgc2 = CensusGeoCoder(template='tmp/geo-try2-{}', concurrency=20) $ cgc2.geocode_addresses(fail_df)
df_DST.to_csv('DST(Time-Series Format).csv')
rows = session.query(Adultdb).filter_by(education="9th").all() $ print("-"*100) $ print("Count of rows having education as '9th' after delete: ",len(rows)) $ print("-"*100)
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-0.05)) $
trip=(calc_temps('2017-05-10','2017-05-13')) $ y = [trip[0][1]] $ trip_df = pd.DataFrame.from_dict(y) $ errors = [trip[0][0]-trip[0][2]] $ trip_df.plot.bar(yerr=errors,title='Trip Avg Temp', legend=False)
countries = pd.read_csv('countries.csv') $ countries.sample(5) $ combined_df = countries.merge(df2, left_on = 'user_id', right_on = 'user_id', how = 'inner') $ combined_df[['UK', 'CA']] = pd.get_dummies(combined_df['country'])[['UK','CA']] $ combined_df.sample(5)
df[df['processed'].isna() == True]
print([X.shape[0],y.shape[0]])
merged2[["article_publisher_id", "PID"]] \ $     [merged2["article_publisher_id"] == merged2["PID"]] \ $     .drop_duplicates() \ $     .shape[0]
import statsmodels.api as sm
dtypes={'date':np.str,'type':np.str,'locale':np.str,'locale_name':np.str,'description':np.str,'transferred':np.bool} $ parse_dates=['date'] $ holidays_events = pd.read_csv('holidays_events.csv', dtype=dtypes, parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",holidays_events.shape) $ pd.DataFrame.head(holidays_events)
bnbAx[bnbAx['language']=='fr'].first_browser.value_counts().plot.pie()
nyt_df = pd.DataFrame.from_dict(my_old_data, orient='index')
two_zips_df = cfs_df[(cfs_df.Zip==70117) | (cfs_df.Zip==70119)]
data.drop('id', axis=1, inplace=True)
logit_mod1 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'CA','US']]) $ result1 = logit_mod1.fit() $ result1.summary()
df = arcgis.features.SpatialDataFrame().from_featureclass(collisions_path)
ann_ret_bond[9].describe()
el = dr.find_element_by_id('text')
customer_visitors_new.index = pd.MultiIndex.from_tuples([(x[0], list(calendar.day_name)[x[1]]) for x in customer_visitors_new.index]) $ customer_visitors_new
contain_geo_search = collection_reference.find({'geo' : { '$ne' : None}}) $ contain_geo_search
!curl -I 'http://www.scielo.br/scielo.php?script=sci_arttext&pid=S1414-32832017000200349&lng=en&nrm=iso&tlng=pt&debug=xml'
pd.concat([msftAV, aaplA], join='inner')
len(df)*0.5/(60*60*24) #Convert to days
df2[df2.duplicated('user_id')]
crime_wea.info(null_counts=True)
df.query('group == "treatment" and landing_page != "new_page"').count()
from sklearn.externals import joblib $ filename = 'IMDBmodel.pkl' $ joblib.dump(gs_lr_tfidf, filename, compress=9) $
df['ruling'].value_counts()
url_template = "http://www.basketball-reference.com/teams/{team}/executives.html"
df4.sort_values(by='ra') $ (df4[df4.ra>315].sort_values(by='ra')).append((df4[df4.ra<195].sort_values(by='ra')))
doc_top_mat['MaxTopicWt'] = doc_top_mat.iloc[:, :15].max(axis=1) $ doc_top_mat['PrimaryTopic'] = [topic_list[x] for x in doc_top_mat['TopicNum']]
results_baseline = smf.ols('favorite_count ~ dow + time_group + has_media + has_hashtags + has_urls', data=df_clusters[(df_clusters.favorite_count>0)]).fit() $ results_clusters = smf.ols('favorite_count ~ dow + time_group + has_media + has_hashtags + has_urls+cluster_cat', data=df_clusters[(df_clusters.favorite_count>0)]).fit()
my_stream.filter(track=['data'])
print("Row information of repeated user_id:") $ df2[df2.user_id.duplicated(keep=False)]
station_activity = df.groupby('station').count().sort_values(['id'], ascending=False) $ station_activity
for col in b_list.columns: $     print(f'{col}...{len(b_list[col].unique())}')
y_preds = random_forests_grid.best_estimator_.predict(X_test) $ random_forests_scores = show_model_metrics('Random Forests', random_forests_grid, y_test, y_preds)
[c for c in nar2.columns if c not in [ u'in_arrears_since',       u'in_arrears_since_days', $        u'in_arrears_since_days_30360',                      u'bucket', $                          u'bucket_pd',               u'payback_state']]
cb_offices.head()
rounds_df.loc[0]
predict_y = randomforest.predict(testx)
df["booking_user_agent"][mask].value_counts() / df["booking_user_agent"][mask].count()
places = api.geo_search(query="London", granularity="city") $ place_id_L = places[0].id $ print('London id is: ',place_id_L)
datAll.shape
df_favored = df_stars[df_stars.stars > 4] $ user_favored = df_favored[df_favored.user_id == df_favored.user_id.iloc[0]] $ df_similar_items[df_similar_items['business_id'].isin(user_favored.business_id)].sort_values('score',ascending=False).similar[:5]
template = 'Lee wants a {} right now' $ menu = ['sandwich', 'pizza', 'hot potato'] $ for snack in menu: $     print(template.format(snack))
df_con.to_csv('df_con', index=False)
autos["seller"].value_counts()
[column.name for column in get_child_column_data(observations_node)]
s[s > 100].head()
display(data.head()) $ display(pd.DataFrame(testdata).head())
df.tweet_id = df.tweet_id.to_string() $ df.timestamp = pd.to_datetime(df.timestamp, yearfirst=True)
list(Station.__table__.columns)
df.groupby(['group', 'landing_page']).count()
list(twitter_Archive.columns.values)
df.to_csv('DataSet_clean.csv', index=False)
print('No. of uniques users in dataset:',df.user_id.nunique())
def get_mappings(col_name, val_range): $     return { $         i: key_dict[(col_name, "A{}".format(i))] for i in val_range $     } $ get_mappings("education", range(1, 14))
grouped_df.to_csv('results.csv', index=False)
data_df.head()
train_data.head() $
to_be_predicted_Day1 = 17.56 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
fullDf[fullDf.levelIndices=='CR'].level.value_counts()
Pold = df2['converted'].mean() $ Pold
plt.show()
result_mwu.add_benchmark('../data/benchmark/IBOV.csv') $ result_mwu.add_benchmark('../data/benchmark/DynamoCougar.csv') $ result_mwu.plot() $ result_mwu.plot_yrly_result()
print("{}'s most favorited tweet is: \n {}".format(target_user,data['tweets'][fav])) $ print("Number of favorites: {}\n\n".format(fav_max)) $ print("{}'s most retweeted tweet is: \n {}".format(target_user, data['tweets'][rt])) $ print("Number of retweets: {}".format(rt_max))
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1-mean_of_p), mean_of_p])
shifted_leaderboard = df.groupby(['batter_name', 'batter', 'bats'])['is_shift'].agg(['mean', 'count']) $ shifted_leaderboard.loc[shifted_leaderboard['count']>1500,].sort_values('mean', ascending=False).head(15)
df = lds.dataframes['simpsons_script_lines']
transactions[~transactions['UserID'].isin(users['UserID'])]
data.columns
c.execute("SELECT state FROM cities") $ print(c.fetchall())
typesub2017['Solar'] = typesub2017['Solar'].multiply(4) $ typesub2017['Wind Offshore'] = typesub2017['Wind Offshore'].multiply(4) $ typesub2017['Wind Onshore'] = typesub2017['Wind Onshore'].multiply(4)
np.sum(ser6)
rider_rat = austin['rider_rating'].value_counts().sort_index() $ driver_rat = austin['driver_rating'].value_counts().sort_index() $ print(rider_rat) $ print(driver_rat)
df2.shape[0]
temps1.mean()
pred_probas_over_k150 = gs_k150.predict_proba(X_test)
import numpy as np $ AAPL.iloc[::3,-1] = np.nan  # assinging scalar value to column, slice broadcast value to each row $ AAPL.head(7)
to_be_predicted_Day5 = 52.54311172 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
ser5.iloc[1]#Purely integer-location based indexing for selection by position.
df3 = df.iloc[df.query('group == "control" and landing_page == "old_page"').index.values]
a_diff=df2.groupby('group')['converted'].mean()['treatment']-df2.groupby('group')['converted'].mean()['control'] $ a_diff
grp.transform( lambda x:x+10 )
autos["number_of_photos"].value_counts()
Irregularities_data = [] $ time_hour_for_file_name = 0 #datetime.datetime.now().time().hour $
Measurement_df = pd.read_sql("SELECT * FROM measurement", conn) $ Measurement_df.head(10)
df=pd.DataFrame(np.random.randn(1000,3),columns=['A','B','C'],index=pd.date_range('1/1/2012',periods=1000,freq='h'))
autos.rename(columns={'odometer':'odometer_km'}, inplace=True) $ autos.rename(columns={'price':'price_$'}, inplace=True)
df=pd.io.json.json_normalize(data_jsn)
combined = pd.concat([reddit, cvec_df], axis=1, join_axes=[reddit.index])
reg_country.summary()
countries.country.nunique()
sns.distplot(df['score']);
df_enhanced.info()
df_h1b_nyc_ft.groupby(['lca_case_employer_name'])['pw_1'].mean()
df_clean.info()
cercanasA1_11_14Entre125Y150mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 125) & (cercanasA1_11_14['surface_total_in_m2'] < 150)] $ cercanasA1_11_14Entre125Y150mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre125Y150mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre125Y150mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
updated_experiment_details = client.repository.update_experiment(experiment_uid, experiment_metadata)
df2['intercept'] = 1 $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = log_mod.fit() $ result.summary()
mb.to_csv("mb.csv")
df_clean.loc[:, 'rating_numerator'] = df_clean['rating_numerator'].astype(float)
rl = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv" $ u = urllib.request.urlopen(url) $ rawdata = u.read()
len(df.index) / n_time_series
engine.execute('SELECT * FROM measurement LIMIT 5').fetchall()
X = pd.merge(X, expAndCoinByUser, on="userid") $ X.head(5)
yhat = DT.predict(X_test) $ yhat
pd.date_range(start, end, freq='BM')
class_merged.isnull().sum()
df_ab_cntry.drop(['UK'], axis=1, inplace=True)
df['2015-04-01']['Complaint Type'].value_counts().head(3)
plt.plot(spks[:,0], spks[:, 1], '.k') $ plt.xlim(500, 1500) $ plt.ylim(-1, 7.5) $ plt.xlabel('Time (ms)') $ plt.ylabel('Neuron Number')
dummy_clf = DummyClassifier() $ dummy_fit = dummy_clf.fit(X_train, y_train) $ dummy_predict = dummy_clf.predict(X_test)
plt.rcParams['axes.unicode_minus'] = False $ dta_58.plot(figsize=(15,5)) $ plt.show()
p_old = df2.query("converted==1").user_id.nunique()/df2.user_id.nunique() $ p_old $
country.head()
y_estimates = lm.predict(x_min_max) $ y_estimates
crimes.isnull().sum()
s1 = active_psc_records.groupby('company_number').size() $ s1.to_csv('data/viz/distribution_of_psc_count_per_company.csv',index=False) $ sns.distplot(s1)
dt_obj = datetime.datetime.strptime(df.index[0], '%Y-%m-%dT%H:%M:%S.%f') $ print(dt_obj) $ print(type(dt_obj))
df_hate.shape
screen_names=[tweet.author.name for tweet in all_tweets] $ screen_names[:5]
def try_request(url): $     html = urlopen(url) $     time.sleep(1) $     return html
df['closed_at']=pd.to_datetime(df['Closed Date'], format="%m/%d/%Y %I:%M:%S %p") $ df['closed_at']
mini_df.groupby(['wait', 'month']).var()
large_image_url = browser.find_by_xpath('//*[@id="page"]/section[1]/div/article/figure/a/img')["src"] $ print(large_image_url)
df_weekly = df_mean.merge(df_count[['date', 'id']], suffixes=('_average','_count'), on='date').merge( $     df_max[['date', 'week', 'text', 'polarity', 'negative', 'retweets']], on='date', suffixes=('_average','_max'))
df_students.describe()
b = pd.DataFrame(a) $ print b
url = form_url(f'organizations/{org_id}') $ print(url) $ response = requests.get(url, headers=headers) $ print_status(response)
uber_15["hour_of_day"] = uber_15["Pickup_date"].apply(lambda x: getHour(x)) $ uber_15.head()
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA
active_stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\ $                                 order_by(func.count(Measurement.station).desc()).all() $ active_stations $
linear_prediction = linear_model.predict(X_test) $ SVR_prediction = SVR_model.predict(X_test) $ group_A_predict = (linear_prediction+SVR_prediction)/2
datetime_counts = {turnstile: [(time, count) $                                for (time, count, _) in rows $                                if 0 <= count <= 5000] $                    for turnstile, rows in datetime_count_times.items()} 
dictionary = corpora.Dictionary(text_list) $ dictionary.save('dictionary.dict') $ print dictionary
old_Page_converted=sum(old_page_converted)/old_page_converted.shape[0] $ new_Page_converted=sum(new_page_converted)/new_page_converted.shape[0] $ print("Conversion of  new to old",new_Page_converted-old_Page_converted)
misk_df.to_csv('./misk.csv', index_label='id')
prob_treat = (df2.query('group=="treatment"')['converted']==1).mean() $ print('Probability converted given that individual in treatment group: ' + str(prob_treat))
 1 - (election_data['pop_2012'].isnull().sum() / len(election_data['pop_2012'])) $
historicalPriceEFX = pdr.DataReader('EFX', 'yahoo', "2016-01-01", "2017-10-06")
df = pd.read_csv('banklist.csv') $ df['year'] = pd.to_datetime(df['Closing Date']) $ df['year'] = df['year'].dt.strftime('%Y') $ df['fail_date'] = pd.to_datetime(df['Closing Date'], format='%d-%b-%y') $ df.head()
reflClean = reflRaw.astype(float) $ reflClean
autos = autos[autos.registration_year.between(1920,2016)] $ autos.shape
df_tweet_clean.info()
result1[['last_v_T']].sub(result1['First_v_T'], axis=0) $ result1['Guinea_deaths Mean'] = result1['last_v_T']/result1['count_v_T'] $ result1
crimes.head()
data.to_csv("output2.csv",index=False)
n_new = df2['group'].value_counts()[0] $ n_new $
df_clean.rating_denominator.value_counts()
ff3.head()
sub_dataset.groupby(["NewsDesk", "SectionName", "Popular"]).size()
result = lo.fit()
df_new.groupby(['country','landing_page']).converted.mean().plot(kind='bar');
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
df2 = winpct.groupby('Game Title Date')['date','Team1'].count()
df2[df2['group']=="treatment"].count()[0]
poverty.drop(rows_todrop_1, inplace=True)
genre_vectors.columns
with open('./data/out/xport_demo.sas7bdat', 'wb') as f: $     xport.from_dataframe(_xport_dm2, f)
from matplotlib.pyplot import figure $ figure(num=None, figsize=(30, 4), dpi=80, facecolor='w', edgecolor='k') $ sources = list(tweet_source_hist.head(15).index) $ frequencies = list(tweet_source_hist.head(15).source_freq) $ plt.bar(sources, frequencies)
to_be_predicted_Day4 = 48.31143151 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
sharpe_ratio(data,daily_rf=0.00037828653,portfolio=True)
filtered_collapsed_all_simband_data.parsed_ts.nunique()
df.groupby(['group', 'landing_page']).nunique()
df[df['Descriptor'] == 'Pothole'].index.weekday.value_counts()
print(df,'\n') $ s = pd.Series([1,3,5,np.nan,6,8],index=dates).shift(2) # shift 2 rows down (2 NaN at top) $ print(s,'\n') $ print(df.sub(s,axis='index'))
twitter_final['tweet_id'] = twitter_final.tweet_id.apply(lambda row: str(row))
utility_patents_df = all_patents_df[all_patents_df.application_type == 'utility'] $ utility_patents_df.info()
msk = np.random.rand(len(df)) < 0.8 $ print(msk) $ train = cdf[msk] $ test = cdf[~msk]
y_score_test_rfc = rfc.predict_proba(X_test)[:,1] $ metrics.roc_auc_score(y_test, y_score_test_rfc) 
training = dfM[dfM.YEAR <= 2015].dropna() #first row will be dropped $ holdout = dfM[dfM.YEAR == 2016].dropna()
tw.dtypes
data.sort_values('fix_duration', ascending=False)
from scipy.optimize import minimize $ fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, learning_rate),  $                 method='TNC', jac=True, options={'maxiter': 250}) $ fmin  
def data_scientist(x): $     if 'Data Scientist' in x: $         return 1 $     return 0 $ df_more['Data Scientist'] = df_more['Title'].apply(data_scientist)
df = df4[df4.day == days[0]] $ place = total_base_dict_by_place.index[20] $ total_base_dict_by_place.loc[place]
deaths['Totals'] = deaths['Totals'].astype(int) $ deaths['Date'] = deaths['Date'].apply(lambda d: datetime.strptime(d, '%Y-%m-%d')) $ grouped_months = deaths.groupby(deaths['Date'].apply(lambda x: x.month)) $
tips_sentiment = tips_sentiment.rename(columns={'Positve Percentage': 'Tip Positve Percentage', $                                                 'Negative Percentage': 'Tip Negative Percentage', 'Business_Focus': 'Tip Business Focus'})
topics_by_time(period=topics_data.timestamp.dt.weekday, period_str='Weekday (0= Monday, 6=Sunday)')
twitter_archive.describe()
vec = vectorizer.fit_transform(convo_frame['q'])
treatment = pd.Series([0]*4+[1]*2)#INDEX $ treatment $ data['treatment'] = treatment $ data
events.type = events.type.astype("category") $ events.doc_type = events.doc_type.fillna("UNKNOWN").astype("category")
bnbA.age.plot.hist()
donors['Donor City'].value_counts().head(20).plot(kind='barh') $ plt.title('Top 20 Cities with most Donors') $ plt.xlabel('Number of Donors') $ plt.gca().invert_yaxis()
pprint.pprint(test_collection.find_one())
data2.dates = pd.to_datetime(data2.dates) $ data2.TRDATETIME = pd.to_datetime(data2['dates'],  dayfirst= True, format='%d%b%y:%H:%M:%S')
df_users_5.head()
list = [1,3,4,30] $ list.append(21) $ print(list)
print(sp500['Price'].head(3)) $ print(sp500[['Price','Sector']].head(3))
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline $ from sklearn.neighbors import KNeighborsRegressor
stocks.loc['Apple']
length = [] $ for x in sequences: $     length.extend(x) $ max(length)
df_users_5=pd.merge(df_users_4,pt_unit_resume,left_on='uid',right_on='uid',how='left') $ df_users_5.loc[df_users_5['no_units_resume'].isnull(),'no_units_resume']=0 $ df_users_5.loc[df_users_5['units_resume_flag'].isnull(),'units_resume_flag']='Did not start any unit'
pd.pivot_table(bb_df, values = ["Wt"], index = ['Pos'], aggfunc = np.mean).sort_values(by = ['Wt'], ascending = False)
merged_data['invoices_creation_date'].max() # 19th of Mar, 2017 - end date 11:24:45
fig1, ax1 = plt.subplots() $ ax1.pie(stage_summary['count'], labels = stage_summary['stage']) $ fig1.savefig('Stage Pie Plot') $
n_new = df2.query('landing_page == "new_page"').count()[0] $ print("Number of users with new page :",n_new)
Ralston.loc[:,"TMAXc"] = (5/9)*(Ralston.loc[:,"TMAX"] - 32) $ Ralston.loc[:,"TMINc"] = (5/9)*(Ralston.loc[:,"TMIN"] - 32)
tt_final.shape
from sklearn.metrics import mean_absolute_error, mean_squared_error $ from math import sqrt $ from pysumma.Validation import validation
customer_emails['Days Between'].describe()
rec, spec, sp_re, coefs, ypred = eval_fits_by_thresh(clf, sc, X_main, y_main, t[np.argmax(sp_rec)])
df = df_1.reset_index().set_index('user_id').join(df_churn) $ df = df[df['event_leg']<=df['churn_leg']] $ df = df.drop('churn_leg',axis=1) $ df = df.groupby(['user_id','event_leg']).sum() $ df.head() $
siteInfo = siteInfo.query('Range >= 10 & EndYear >= 2017') $ siteInfo
dfData.dtypes
df3['timestamp'].max(),df3['timestamp'].min()
reviews_recent20 = reviews_w_sentiment.groupby(['listing_id']).tail(20)
stars.info()
df.columns
autos.brand.value_counts(normalize=True) $
train_features_tokenized = vectorizer2.fit_transform(articles_train)
emojis_db.emoji_category.unique()
nu_fiss_xs = fuel_xs.get_values(scores=['(nu-fission / flux)']) $ print(nu_fiss_xs)
convert_new_ct = df2.query("group=='treatment'and converted==1")['user_id'].count() $ print(convert_new_ct) $
df2=pd.merge(df2,country, on=['user_id']) $ df2.head()
data.loc[1:3]
from h2o.estimators.glm import H2OGeneralizedLinearEstimator $ help(H2OGeneralizedLinearEstimator) $ help(h2o.import_file)
df.sort_values('total_value', ascending=False, inplace=True) $ display(df.head(n=10))
from IPython.display import FileLink $ FileLink('../develop/a.ipynb')
df_protest.loc[df_protest.TownCity_Name=='Cape Town', ['TownCity_Name', 'Violent_or_non_violent']].head()
a = np.array([0, 1, -1]) $ print (a) $ print (a.shape)
gmm = GaussianMixture(2).fit(X) $ labels = gmm.predict(X)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner') $ df_new.head(10)
merge_email = df_email.merge(df_user, on="user_id", how = 'inner')
brand_pminfo = pd.DataFrame(mpbrand_series, columns=['mean_price'])
education.columns=education.iloc[0,:]
to_be_predicted_Day3 = 38.62438598 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
print(tipsDF.groupby('likes').count())
right(np.array([3, 0]), np.array([0, 4]))
train.bedrooms.value_counts()
from scipy.stats import norm $ norm.cdf(-1.3109241984234394)# where 0.002 is Zscore calculated by ztest
to_remove = creations[ $     (creations["length_at_creation"] < 100) & $     (creations["creator_autoconfirmed"] == True) $ ] $ creations = creations.drop(to_remove.index)
prcp_analysis_df = pd.read_sql("SELECT date, prcp FROM measurements", con=engine, columns=[["date"],["prcp"]])
[scores.apply(sum), scores.apply(np.mean)]
tweet_archive.rating_denominator.value_counts()
fuel_factors = {'NG' : ef.loc['NG', 'Fossil Factor'], $                    'PEL': ef.loc[['DFO', 'RFO'], 'Fossil Factor'].mean(), $                    'PC' : ef.loc['PC', 'Fossil Factor'], $                    'COW' : ef.loc[['BIT', 'SUB'], 'Fossil Factor'].mean(), $                    'OOG' : ef.loc['OG', 'Fossil Factor']}
class_items=pd.DataFrame(group['item_nbr'].agg('count')) $ pd.DataFrame.head(class_items)
import copy $ right = weather_df $ left = copy.deepcopy(crime_df.iloc[[100, 200, 300, 400, 500]]) $ left['Date_ TimeCST'] = left['Date_Time'].apply(lambda x: nearestDate(weather_df['Date_ TimeCST'], x)) $ left
sdf.select('_c0', date_format('_c0', 'HH:mm').alias('time'), date_format('_c0', 'E').alias('weekday'), date_format('_c0', 'u HH:mm').alias('weekdaytime')).show()
multiple_party_votes_all = multiple_party_votes_all.loc[:,~multiple_party_votes_all.columns.duplicated()]
z=loan_principals #repaid_loans_cash $ z[z.fk_loan==loan_test].T
sessions.action.value_counts()#['active']
rain_score = session.query(Measurement.prcp, Measurement.date)\ $                .filter(Measurement.date > past_year).\ $                 order_by(Measurement.date).all()
new_df = pd.DataFrame(tweet_archive_clean.loc[tweet_archive_clean['new'].apply(find_dot) == True].new.str.split('/').tolist(), columns=['rating_numerator', 'rating_denominator'])
datetime.time(datetime(2015,12,14,15,17,30))
treatment_conv = df[df['group'] == 'treatment'].converted.mean() $ treatment_conv
df['time_detained'].head()
df.head()
rows = train.shape[0] $ columns = train.shape[1] $ print("The train dataset contains {0} rows and {1} columns".format(rows, columns))
compound_df.columns = target_terms $ compound_df.head()
machines['model'] = machines['model'].astype('category') $ print("Total number of machines: %d" % len(machines.index)) $ machines.head()
ser6=pd.Series([1,2,3,4,5,6]) #example of summation $
proj_df['Project Subject Category Tree'].value_counts()
print(df.isnull().sum().all()) $ print('None of the rows have missing values.')
sentiments = df[['created_at', 'neg', 'pos']] $ sentiments = sentiments.sort_values(by="created_at") $ sentiments = sentiments.reset_index(drop=True) $ sentiments.head()
df.loc[:, ['B', 'D']]
df = df.fillna(0, subset=features_numeric) $ df = df.fillna("Unknown", subset=features_categorical) $ print(df['fault_code_type_3',].head(3)) $
treatment_mismatch = df.query("group == 'treatment' and landing_page == 'old_page'") $ control_mismatch = df.query("group == 'control' and landing_page == 'new_page'") $ print('The number of times the new_page and treatment dont line up is {}'.format(len(treatment_mismatch))) $ print('The number of times the old_page and control dont line up is {}'.format(len(control_mismatch))) $ print('Total dont line up is {}'.format(len(control_mismatch)+ len(treatment_mismatch)))
df2.query('group =="treatment"')['converted'].mean()
archive_copy['text'][archive_copy['text'].str.contains('&amp;')]
rounds_df.hist(column = 'announced_year',bins = 27, figsize = (20,8))
try: $     list(1) $ except: $     print("Error: Integer is not iterable.")
xgb_model.fit(X_train, y_train)
pd.merge(df1, df3, left_on="employee", right_on="name" )
unique_urls.sort_values('mentions', ascending=False).head()
models_df = pd.DataFrame.from_dict(models_dict['classifiers']); models_df
brands_np = np.asarray(brands) $ models_np = np.asarray(models)
df_metadata = pd.read_excel(DATA_FOLDER + "/microbiome/metadata.xls")
pd.DataFrame({'one' : [1., 2., 3.], 'two' : [3., 2., 1.]})
scr_active_ix = [s for s in SCR_PLANS_df.index if s not in scr_churned_ix]
x = data.loc[:,'Cl.thickness':'Mitoses'] $ print(x.head()) $ y = data.loc[:,'Class'] $ print(y.head())
dict_user_means = ...
p_diff = new_page_converted - old_page_converted $ p_diff.mean()
xyz = json.dumps(youtube_urls, separators=(',', ':')) $ with open('youtube_urls.json', 'w') as fp: $     fp.write(xyz) $
log_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = log_mod.fit() $ results.summary()
qw = qgrid.show_grid(all_tables_df, show_toolbar=True)
lm=sm.OLS(df2_by_day['new_rate']-df2_by_day['old_rate'], df2_by_day[['intercept', 'day']]) $ results=lm.fit() $ results.summary()
type2017_2 = type2017[type2017.index % 4 == 0] # we'll address to this later
calls_nocontact_2017 = calls_nocontact_simp.loc[mask]
final['BTC Price Change'] = final['BTC Price'].pct_change()*100 $ final['ETH Price Change'] = final['ETH Price'].pct_change()*100 $
txt_exception_folder = '/home/ubuntu/s3/comb/txt_exception/' $ print(txt_exception_folder) $ flightv1_1 = spark.read.json(os.path.join(txt_exception_folder, "flight_15_13_price_2017-05-11*.txt")) $ flightv1_1.count() $
result_df = pd.DataFrame({'profile_id':profile_ids}) $ result_df['num_events'] = result_list
df.to_csv('ab_cleaned.csv', index=False) $ df2 = pd.read_csv('ab_cleaned.csv')
oppstage.loc[0].plot(kind='bar', stacked=True, figsize=(12,6));
new_reps.Trump.astype("float64").describe()
festivals.rename(columns={'TempFest': 'Fest'}, inplace=True)
ca_de_xml.shape
trump.to_csv("trump_all.csv",index=False) $ schumer.to_csv("schumer_all.csv",index=False)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq,df)
sentiment_df.head()
investors_df[investors_df.investment_count < 200].hist(column = 'investment_count',bins = 100)
df2.head()
string_list = ["[hi]", "[Hi]", "[hello]", "[Hello]"] $ string_list_new = [] $ for row in string_list: $     string_list_new.append(re.sub("\[[Hh]i\]|\[hello\]", "[Hello]", row)) $ print("[0] string_list: " + str(string_list_new))
n = -100 $ similar = LSI_model.get_dists(corpus[n]) $ print(corpus[n]) $ df.loc[list(similar.keys())]['desc'].values.tolist()
plt.hist(p_diffs) $ plt.axvline(obs_diff, c='r');
movies.head(5) # Brings up the first 5 rows
StockData.describe()
import numpy as np $ high = np.array(dataset["High"]) $ low = np.array(dataset["Low"]) $ daily_change = high - low
df3 = df2.unstack(1) #['var'].plot(subplots=True) $ df3
quarters.asfreq("M", how="start")
mydata.json()['dataset_data']['data'][1]
session_v2.shape
cbow_m1 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=100) $ cbow_m2 = gensim.models.Word2Vec(train_clean_token, min_count=5, workers=2, window = 5, size=100) $ cbow_m3 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 10, size=100) $ cbow_m4 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=300)
fields_frame = get_child_frame(fields_node, data)[crop.notnull()]
labels=poverty.iloc[label_rows, 0].tolist()
train = pickle.load( open("../Data/train.csv", "rb") )
print 'Total amount for the unknown state excluding outliers: ', df[(df.state == 'YY') & (df.amount < 45000)].amount.sum() $ print 'Total amount for the unknown state: ', df[(df.state == 'YY')].amount.sum() $ print 'Total amount: ', df.amount.sum()
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df['group']) $ df2.head()
s = pd.Series(lst) $ s.isnull()
metrics.completeness_score(labels, km.labels_)
df_clean3.nlargest(10, 'rating_numerator')[['text', 'rating_numerator', 'rating_denominator']]
diffs = new_page_converted.mean() - old_page_converted.mean() $ print("Difference between mean of each scenarios probability:",diffs)
n_new = df2.query('group == "treatment"')['user_id'].count() $ n_new = int(n_new) $ n_new
new_doc = repos[4] $ new_vec = dictionary.doc2bow(new_doc) $ repos_langs.iloc[4, 0] # Para testar nossa hipotese vamos encontrar usuarios similares ao usuario 4
df1['Year'] = pd.to_datetime(pd.Series(df1['Year']).astype(int),format='%Y').dt.year  #converting the year to pandas date time $ df1.tail()
users = pd.read_csv('data/users.csv', index_col = 0, parse_dates = [1,2]) $ users.head()
df_concensus.info()
!wget http://files.fast.ai/part2/lesson14/rossmann.tgz
jobs_data3 = json_normalize(json_data3['page']) $ jobs_data3.head(5)
na_df.loc["a", "four"] = np.nan # fills as None since replacing a str $ na_df.loc["b", "four"] = None # fills as None $ na_df
df1_clean.head(1)
train_df.describe().T
df_list = pd.read_html('http://www.bloomberg.com/markets/currencies/major') $ print(len(df_list))
print (df2.loc[df2['user_id'] == 773192])
master_copy.describe()
joined.head()
filled.ix['2011-11-03':'2011-11-04'].plot() $ plt.ylim(103.5, 104.5)
cashflows_plan_origpd_noshift_all[(cashflows_plan_origpd_noshift_all.id_loan==210)&(cashflows_plan_origpd_noshift_all.fk_user_investor==25151)].to_clipboard()
grouped_by_day_df = full_df.groupby('day')['listing_id'].count().reset_index().copy() $ grouped_by_day_df.columns = ['day_of_month','count_of_listings'] $ grouped_by_day_df.head(3)
low_ranked_answers_threshold = np.percentile(answers_scores, 10) $ low_ranked_answers_threshold
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)
file1 = '/Users/gta/dev/hw-4/schools_complete.csv' $ file2 = '/Users/gta/dev/hw-4/students_complete.csv'
df.iloc[99:110,3:]
new_query = "SELECT id, recording_date, stage1_reviewer_id, workflow_state \ $ FROM ephys_roi_results WHERE recording_date > '2017-01-01'" $ err_df = get_lims_dataframe(new_query) $ err_df.head()
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df.head(2)
students.weight.values
room_temp.temperature =3
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
to_be_predicted_Day4 = 43.11800827 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
from scipy.sparse import coo_matrix
print(df.head()) $ print('\n Types Data: ') $ print(df.dtypes)
conf_matrix = confusion_matrix(new_y, lr2.predict(new_x), labels=[1,2,3,4,5]) $ conf_matrix # most of the results are classified as 5 point $
import nltk $ nltk.download() $ global user_date
df_members.head()
df = pd.merge(releases, bookings, on='hash_id', how='left')
X_test_reduced = pca.transform(X_test) $ y_pred = rnd_clf2.predict(X_test_reduced) $ accuracy_score(y_test, y_pred)
parser = etree.HTMLParser() $ tree = etree.parse(StringIO(html), parser) $ tree
weather.ice_pellets.value_counts()
ggplot(mres.data.assign(idx=list(range(mres.data.shape[0]))),aes(x="idx",y="residuals",color='subject',shape="et"))+geom_point()
dfpf.collect()
df2['intercept'] = 1 # adding intercept column $ dummy = pd.get_dummies(df2['group'],drop_first=True) ## creating dummy $ dummy['ab_page'] = dummy['treatment'] $ df2 = df2.join(dummy['ab_page']) # joining dummy column to dataframe
useless_variables = ['CODE_POSTAL', 'ETAGE', 'PAYS', 'POINTS_FIDEL', 'STOP_PHONING'] $ equipment.drop(useless_variables, axis=1, inplace=True)
new_page_user = len(df2.query("group == 'treatment'")) $ total_users = df2.shape[0] $ print("The probability that an individual received the new page: " + str(new_page_user/total_users)) $ print("As a percentage: " + str((new_page_user/total_users)*100) + "%")
df[['TMAX']].plot(figsize=(8,4), style=['-']);
autos['odometer_group'] = autos['odometer_km'] / 10000.0 $ autos['odometer_group'].value_counts()
twitter_data.rating_numerator.value_counts()
data_sample['value'] = [i.replace(',', '') for i in data_sample['value']] $ data_sample['value'] = pd.to_numeric(data_sample['value'])
dtest = xgb.DMatrix(test.drop("click_id", 1)[dtrain.feature_names])
df.shape
pickle_off = open("df_meta.pickle","rb") $ dfMeta = pickle.load(pickle_off)
mta[mta.STATION == '59 ST'].groupby(mta.DATE_TIME.dt.date)['NEW_ENTRIES']\ $     .sum().plot(kind = 'line', figsize = (12,7)); $ plt.ylabel('Entries') $ plt.xlabel('Date') $ plt.title('Entries at 59th Street Subway Entrance');
summary = player.groupby('segment')[['event']].sum() $ summary['event'] = summary['event']
station_count = session.query(Station.station).count() $ station_count
p_value = (null_vals>p_obs_diff).mean() $ p_value
print(stemmer.stem('human')) $ print(stemmer.stem('humans')) $ print(stemmer.stem('cannibal')) $ print(stemmer.stem('cannibals')) $ print(stemmer.stem('cannibalism'))
random_numbers['2015-01-01':'2015-04-30'].idxmax() # for first 4 months
df2.to_csv('./Data/changes.csv')
all_colnames = [clean_string(colname) for colname in df_total.columns] $ df_total.columns = all_colnames $ df_total.head()
n_old = (df2['landing_page'] == 'old_page').sum() $ print(n_old)
n_new = df2.query('landing_page == "new_page"').landing_page.count() $ n_new
pred_labels = elnet.predict(test_data) $ print("Training set score: {:.2f}".format(elnet.score(train_data, train_labels))) $ print("Test set score: {:.2f}".format(elnet.score(test_data, test_labels))) $ print("Number of features used: {}".format(np.sum(elnet.coef_ != 0)))
type(digits.data)
df_tweet_json_clean.created_at = pd.to_datetime(df_tweet_json_clean.created_at)
for i in df['questions_48542_why_is_this_your_preferred_brand']: $     if i $ df['questions_48542_why_is_this_your_preferred_brand'].apply(correct_spelling)
embarked.value_counts().sort_index().plot(kind='bar') $ embarked.value_counts()
stc.checkpoint("checkpoint11")
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index() * 100
today = pd.to_datetime("Today") $ today
tips.head()
df_pol_d=pd.get_dummies(df_pol_t, columns=['domain_d'], drop_first=True)
ffr.resample("M").last().head()
pd.DataFrame(X_train_scaled).head()
ebay.index
import string $ strings = string.ascii_lowercase[1:26] $ print(strings)
run txt2pdf.py -o "METHODIST HOSPITAL  Sepsis.pdf"   "METHODIST HOSPITAL  Sepsis.txt"
totals.fillna(0, inplace = True)
y = df['hospitalmortality'].values $ X = df.drop(['hospitalmortality'], axis=1).values $ X_header = df.drop(['hospitalmortality'],axis=1).columns
c.find_one({'$or': [{'name.first': 'Michael'}, $                      {'name.last': 'Bowie'}]})
convert_new = df2[(df2['landing_page']=='new_page') & (df2['converted']==1)].count()[0] $ convert_new
assert vecs.shape[0] == len(target_docs)
import statsmodels.api as sm $ from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq,df) $ mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = mod.fit() 
print('Number of rows with invalid values = {}'.format(len(joined[joined.isnull().any(axis=1)]))) $ joined[joined.isnull().any(axis=1)]
content_wed11.tail(20)
bininfo = json.loads(bres.read())
tmp_df.columns.sort_values()
df_mes2 = df_mes.sample(n=1000000, random_state=0) #.iloc[0:1000000,:]
e_p_b_one.TimeCreate = e_p_b_one.TimeCreate.apply(lambda x:x.date())
pgh_311_data['NEIGHBORHOOD'].value_counts()
df = pd.read_sql('SELECT first_name, last_name, member_credit FROM customer WHERE last_name LIKE \'Smith%\'', con=conn_b) $ df
start = datetime.now() $ model_svm = SVC(C=1.0, kernel='rbf', degree=3, gamma=.05) $ model_svm.fit(Xtr.toarray(), ytr) $ print( model_svm.score(Xte.toarray(), yte)) $ print((datetime.now()-start).seconds)
k.index
df['processing_time'].describe()
test = np.load('./pt1sz2.npy') $ print test.shape
sig_sq = np.var(d_3) $ mu = np.mean(d_3) $ scipy.stats.norm.interval(0.95, loc=mu, scale=sig_sq)
api.get_user('florentinawne')._json
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep ='\s+') # Index in the first column "col=0" and header on the first "row" $ df.head(5) $
df2.groupby(df2['group']=='control')['converted'].mean()[1]
archive.rating_denominator.value_counts()
conn.fetch(table=dict(name='data.iris', caslib='casuser'), to=5)
autoDf.createOrReplaceTempView("autos") $ SpSession.sql("select * from autos where hp > 200").show()
n_old=df2[df2['group'] == "control"].shape[0] $ print(n_old)
print('Number of unique users in dataset: {}'.format(df['user_id'].nunique()))
df.eval('D = (A + B) / C', inplace= True) $ df.head()
with open ('apipass.json') as f : $     keys = json.loads(f.read()) $ reddit = praw.Reddit(client_id= keys['client_id'], client_secret=keys['client_secret'], $                      username= keys['username'], password= keys['password'], $                      user_agent= keys['user_agent'])
norm.ppf(1-(0.05/2)) $
shopping_carts = pd.DataFrame(items) $ shopping_carts
df.to_csv('')
df14 = pd.read_csv('2014.csv')
my_df.info()
foursquare_data_dict.keys()
print(autos[~autos["registration_year"].between(1900,2018)]["registration_year"].value_counts()) $
train.columns
df_new[['CA', 'UK', 'US' ]] = pd.get_dummies(df_new['country']) $ model2 = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ model2.fit().summary() $
X = pd.merge(X, meal_inferred_types[['meal_id', 'is_inferred_by_text_columns_breakfast', 'is_inferred_by_text_columns_brunch', 'is_inferred_by_text_columns_lunch', 'is_inferred_by_text_columns_dinner']], on='meal_id', how='inner')
p_diffs = np.array(p_diffs) $ p_val = (p_diffs > actual_diff).mean() $ p_val
!tar -xzf data/aclImdb_v1.tar.gz -C data
sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)
df_bug = df_gt[(df_gt[u'Service Location'].isin(bthlst)) & \ $                (df_gt[u'Equipment Sap Code'].str.match('^PPG-R')) & \ $                (df_gt[u'Request Status'] == 'Closed')][my_cols]
df = pd.read_sql('SELECT last_name, COUNT(*) FROM actor GROUP BY last_name HAVING COUNT(*) >= 2 ORDER BY last_name', con=conn) $ df
start = datetime.now() $ modelgb = GaussianNB() $ modelgb.fit(Xtr.toarray(), ytr) $ print(modelgb.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
prop = props[props.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
df2[['control', 'ab_page']] = pd.get_dummies(df2['group'])
def source_text(html): $     source = BeautifulSoup(html, 'lxml') $     return source.a.string $ df_twitter_copy['source'] = df_twitter_copy.apply(lambda row: source_text(row['source']), axis = 1)
base_url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?api_key={}'.format(API_KEY) $ url = '{}&start_date={}&end_date={}'.format(base_url, '2018-08-29', '2018-08-29') $ r = requests.get(url) $ if r.status_code != requests.codes.ok: $     print('API ERROR: ({}) -{} '.format(r.status_code, r.text))
print("Nr of months with a coin switch:", sum(fund_nr_coinswitches > 0), "out of", len(fund_nr_coinswitches)) $ print("Median coins switches during a month:", fund_nr_coinswitches.median())
old_page_converted = np.random.binomial(1, p_old,n_old) $ old_page_converted.mean()
abc = pd.read_csv('DRG Categorical Payments by YEAR.csv',index_col=0)
train.columns
tweets.info()
ra_min_ogle_fix = 250.0; ra_max_ogle_fix = 283.0; $ dec_min_ogle_fix = -40.0; dec_max_ogle_fix = -15.0;
finaldf['bill_id'] = np.nan
pred_df_knnmodel = pd.concat([X_test, pd.DataFrame(knnmodel_predictions, columns = ['knnmodel_predictions'])], axis = 1) $ pred_df_knnmodel.head(10)
reviews_recent20['listing_id'].unique()[-10:]
total_rows = len(df2.index) $ total_rows_with_new_page = (df2['landing_page'] == 'new_page').sum() $ total_rows_with_new_page/total_rows
df_clean['in_reply_to_status_id'].notnull().sum()
np.random.seed(1)
vessels.type.value_counts()
data = df.values
page_html= uClient.read() $ uClient.close()
np.array(df.index)
df_signup.pivot_table('country_destination', ['signup_method', 'signup_app'], aggfunc="count")
plt.scatter(house_data['sqft_living'], house_data['price'])
import matplotlib.pyplot as plt $ sb.heatmap(components)
n_new, n_old = df2['landing_page'].value_counts() $ print("n_new:", n_new, "\nn_old:", n_old)
df.plot() $
avi_data = pd.read_csv('AviationData.csv', encoding='ISO-8859-1') $
rfc = RandomForestClassifier(random_state = 42) $ param_grid = {'n_estimators' : [100, 200, 300], $               'max_features' : ['log2', 'sqrt']} $ rf_gd = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, scoring='f1', n_jobs = -1) $ rf_gd.fit(X_train, y_train)
investor_fut_bucket_30360_xirr_orig.hist(bins=np.linspace(-.5,.5,110))
sampled_contirbutors_human_raw_df = session.read.format("csv").option("header", "true") \ $                 .option("inferSchema", "true").load( $     "{0}/human_data/sampled_contirbutors".format(fs_prefix))
bday = datetime(1991, 5, 1).toordinal() $ date_now.toordinal() - bday
output = pd.DataFrame(data={"id":test["id"], "rating":predictions}) $ output.to_csv("svm.csv", index=False, quoting=3)
plt.scatter(cc['open'],cc['high']) $ plt.title("Opening vs High Value") #Change x and y axis scale $ plt.show()
print ("# 6a. Cross tab calculations") $ d=pd.crosstab(dfL.OEMModel, dfL[predictColumn].astype(bool)) $ d.plot(kind="bar") $ HTML(d.to_html()) $ plt.legend(loc='upper right');
twitter_archive_df_clean.rating.head()
sym.dsolve(eqn, C(t)) # Resolver
corpus = [] $ corpusDF["AllDescription"].apply(lambda row: corpus.append(str(row))) $ len(corpus)
merged.info()
extractor = twitter_setup() $ tweets_neta = extractor.user_timeline(screen_name="netanyahu", count=200) $ print("Number of tweets extracted: {}.\n".format(len(tweets_neta))) $ tweets_erdog = extractor.user_timeline(screen_name="RT_Erdogan", count=200) $ print("Number of tweets extracted: {}.\n".format(len(tweets_erdog)))
variance(New_df.Sales_in_CAD, len(New_df.Sales_in_CAD))
xmlData['renovate_year'].value_counts()
print(address_df['nndr_prop_ref'].value_counts().head(3))
user_logs['completed_songs_ratio'] = user_logs.num_completed_songs/ (user_logs.num_incompleted_songs + user_logs.num_completed_songs)
t0 = time() $ model = MatrixFactorizationModel.load(sc, "lastfm_model.spark") $ t1 = time() $ print("finish loading model in %f secs" % (t1 - t0)) $
from sklearn.ensemble import RandomForestRegressor
tweets['full_text'] = tweets['full_text'].str.decode('utf-8')
df['year'] = pd.DatetimeIndex(df['date']).year $ df
df['pg_ratio'] = df['pledged'] / df['goal'] $ df['pg_avg'] = df[['pledged', 'goal']].mean(axis=1) $
liberia_columns=["Date", "Country", 'New Case/s (Suspected)', 'New Case/s (Probable)', 'New case/s (confirmed)', $                  'Total death/s in suspected cases', 'Total death/s in probable cases' ,'Total death/s in confirmed cases', $                  'Total death/s in confirmed, probable, suspected cases', 'Total deaths correct'] $ liberia_df=pd.DataFrame(columns=liberia_columns)
query = 'INSERT INTO samples VALUES(?, ?, ?, ?)' $ con.executemany(query, few_recs.values[1:])
new_array.shape
autos['odometer_km'].value_counts()
x = len(df2.query("landing_page == 'new_page'")) / df2.shape[0] $ print("{:.2%}".format(x))
ranked_variance.show(90, False)
sales = np.random.randn(10) * 10 + 500 $ sales
treat_converted = df2[df2['group'] == 'treatment'].converted.mean() $ treat_converted
sns_plot = sns.lmplot(x='score',y='favorite_count',data=rating_and_retweet,fit_reg=False,scatter_kws={'alpha':0.05}) $ sns_plot.savefig("score_vs_favorite.jpg")
labels=f'{PATH}train_v2.csv' $ n=len(list(open(labels)))-1 $ val_idxs=get_cv_idxs(n)
joined['onpromotion']=joined['onpromotion'].astype(np.int8)
nrows, ncols = 100000, 100 $ rng = np.random.RandomState(42) $ df1, df2, df3, df4 = (pd.DataFrame(rng.rand(nrows, ncols)) for i in range(4)) $ %timeit df1 + df2 + df3 + df4 $ %timeit pd.eval("df1 + df2 + df3 + df4") $
df.query('group == "treatment" and landing_page == "old_page"').count()['user_id'] + df.query('group == "control" and landing_page == "new_page"').count()['user_id']
rdf = rdf.set_index('segment_id').join(accident_counts_per_segment)
train = pd.read_csv('train.csv', sep=',') $ print(type(train)) $ train.head(15)
result1.summary2() $
hr2006 = baseball.loc[baseball.year==2006, 'hr'] $ hr2006.index = baseball.player[baseball.year==2006] $ hr2007 = baseball.loc[baseball.year==2007, 'hr'] $ hr2007.index = baseball.player[baseball.year==2007]
data.head()
data.whitelist_status.unique()
df.plot(kind='bar', stacked=True, figsize=(12, 5)) $ plt.savefig('stackedBarcharts.jpeg')
lg=sm.Logit(df2['converted'], df2[['intercept', 'ab_page' ]]) $ results = lg.fit() $ results.summary()
df_ll = pd.read_csv("loblaws_all.csv", encoding="latin-1")
yhat = SVM_model.predict(X_test) $ yhat
np.clip(bag, 0, 1, out=bag)
ffr.resample("7D").max().head(10)
rf.score(X_train, y_train)
actual_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ (actual_diff < p_diffs).mean()
events = events.resample('H').asfreq().fillna(0)['count']
df_newlen = len(df2.query("group =='treatment'")) $ df_newlen 
df['dog_count'].value_counts()
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, inspect, func, distinct
patient_times['total_time'] = patient_times.stop_time - patient_times.start_time
df_new.country.unique() $ df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country']) $ df_new['intercept'] = 1 $ df_new.head()
evaluator.plot_confusion_matrix(type='word_level', normalize = True)
active_creation_ratio =  [x/len(clean_users[clean_users['active']==1]) for x in list(clean_users[clean_users['active']==1]['creation_source'].value_counts())] $ active_creation_ratio
T[T.notnull()].head(10)
idx = df_sites[ (df_sites['disc_times_pay'] < 1000000)].index.tolist() $ len(idx) $ df_sites.loc[idx,:].head()
autos['price'] = autos['price'].str.replace('\$', '') $ autos['price'] = pd.to_numeric(autos['price'].str.replace(',', '')) $ autos['odometer'] = (autos['odometer'].str.replace(',', '')) $ autos['odometer'] = pd.to_numeric(autos['odometer'].str.replace('km', '')) $ autos = autos.rename(index = str, columns = {'odometer' : 'odometer_km'})
some_articles = newsapi.get_everything(domains='wsj.com', language='en') $ some_articles['articles'][0]
from statsmodels.tsa.seasonal import seasonal_decompose $ decomposition = seasonal_decompose(timeseries, freq = 5) $ fig = plt.figure()  $ fig = decomposition.plot()  $ fig.set_size_inches(15, 8)
df2_treatment.query('converted == 1')['user_id'].count()/df2_treatment['user_id'].count()
template % (4.5560, 'Argentine Pesos', 1)
users.toPandas().head()
draft_df.language.value_counts()
learner.fit(3e-3, 1, wds=1e-6, cycle_len=20, cycle_save_name='adam3_20')
n_new = df2[df2["landing_page"] == "new_page"].count() $ n_new = n_new[0] $ n_new
daily_data.head()
from sklearn.feature_extraction.text import TfidfVectorizer $ tfidf = TfidfVectorizer()
is_aux=[] $ for i in range(0,l2): $     if i not in seq: $         is_aux.append(issue_types_num[i]) $ col.append(np.array(is_aux))
sum(df2['user_id'].duplicated()) $ df2[df2.duplicated(['user_id'], keep=False)]['user_id'] $ df2[df2.duplicated('user_id')]
individuals_metadata_df.to_excel(writer, "individuals", index=False)
stations=session.query(Station.station, Station.name).all()
print(int(y_forecast[0]), "bike counts forecast for May 17, 2017.") $ print(int(y_forecast[1]), "bike counts forecast for May 18, 2017.") $ print(int(y_forecast[2]), "bike counts forecast for May 19, 2017.") $ print(int(y_forecast[3]), "bike counts forecast for May 20, 2017.")
list(c.elements())
corrplot(fin_r_monthly.loc[start_date:end_date].corr(), annot=True) $ plt.title('Correlation matrix - monthly data \n from ' + start_date + ' to ' + end_date) $ plt.show()
train['Hour'] = train.apply(lambda row: hour_bins(row[12]), axis = 1) $ test['Hour'] = test.apply(lambda row: hour_bins(row[11]), axis = 1)
df.head()
norm.ppf(1-(0.05)) #critical value of 95% confidence
newdf.yearmonth = newdf.yearmonth.astype("str")
sorted(users.age.unique())
set(tcga_target_gtex_labels.disease).intersection(treehouse_labels_pruned.disease)
test_delta = test['date'].max() - test['date'].min() # 61 days $ train_delta = train['date'].max() - train['date'].min() # 243 days
df2['converted'].sum()/df2['converted'].count()
merged.groupby(["contributor_firstname", "contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount", ascending=False)
with_condition_heatmap_query2 = folium.Map([41.90293279, -87.70769386], $                zoom_start=11) $ with_condition_heatmap_query2.add_child(plugins.HeatMap(final_location_ll[:40000], radius=15)) $ with_condition_heatmap_query2
df2 = pd.read_csv('new_edited.csv')
$SPARK_HOME/conf/log4j.properties.template
sampled_contirbutors_human_df = non_blocking_df_save_or_load( $     rewrite_human_data(sampled_contirbutors_human_raw_df), $     "{0}/human_data_cleaned/sampled_contributors".format(fs_prefix)) 
s.str. startswith ('T')
results = logit_mod.fit() $ results.summary()
df['Datetime'].max()
output= "SELECT tweet_id,retweets, favourate, retweets+favourate as SUM from tweet_details limit 15  " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['tweet_id','Retweets','favourate','SUM'])
v_invoice_hub.columns.tolist()
times.dt.year
y_pred = model.predict(x_test)
logit2 = sm.Logit(df4['converted'], $                            df4[['intercept', 'country_UK', 'country_US']]) $ result2 = logit2.fit() $ result2.summary()
df_master.dog_name = df_master.dog_name.astype('category') $ df_master.stages = df_master.stages.astype('category') $ df_master ['p1'] = df_master.p1.astype('category') $ df_master ['p2'] = df_master.p2.astype('category') $ df_master ['p3'] = df_master.p3.astype('category')
scaler_fit.inverse_transform(predict_actual_df).shape
clean_rates.shape
def create_soup(x): $     return ''.join(x['categoryname']) + ', ' + ''.join(x['eventname']) + ', ' + ''.join(x['location'])
loans_df = loans_df.loc[:, pd.notnull(loans_df).sum() > int(len(loans_df)*(1 - frac))]
type(df_clean['tweet_id'].iloc[0]) $ type(image_clean['tweet_id'].iloc[0]) $ type(tweet_clean['tweet_id'].iloc[0])
destinations_url = '~/Documents/Expedia/destinations.csv' $ dests = pd.read_csv(destinations_url)
sites = pd.read_csv('../data/station.csv', $                    dtype={'HUCEightDigitCode':'str'})
df.isnull().sum()
n_new = len(df2.query("landing_page == 'new_page'")) $ print("new_page count: {}".format(n_new))
conn.get_tables()
X = [re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', string) for string in sample]
c = df.groupby(['education', 'purpose']).agg({'applicant_id': lambda x: len(set(x))}).reset_index() $ c['education+purpose'] = c['education']+c['purpose'] $ sns.barplot(data=c, x='education+purpose', y='applicant_id')
building_pa.columns = building_pa.columns.str.replace(' ', '_') $ building_pa.columns = building_pa.columns.str.replace('-', '_') $ building_pa.columns = building_pa.columns.str.lower() $ building_pa.head()
goals_df.info()
festivals.set_index('Index', inplace=True)
pd.crosstab(df.launched_year, df.State)
autos.odometer.value_counts()
df.dtypes.index
print(pd.isnull(dfx))
neg_lda = models.LdaModel(neg_bow, id2word=neg_dic, num_topics=3, chunksize=10000, passes=4, iterations=100) $ neg_lda.show_topics(formatted=False)
df.head()
df.info()
clf_RF_tfidf.predict_proba(X_testcv_tfidf)
import numpy as np $ unsegmented_users['segment'] = np.random.randint(num_user_segments, size=len(unsegmented_users))
model = 'MO_080526771943' $ print(kimanalysis.shortcode(model)) $ print(kimanalysis.shortid(model)) $ print(kimanalysis.extendedid(model))
BAL_analysis2 = BAL_analysis["Per Seat Price"].mean() # Takes the average of each year/in-season vs. offseason
1/np.exp(.0506), 1/np.exp(.0408)
emb, summ = seq2seq_inf.generate_issue_title(txt) $ summ
price = pd.DataFrame({ticker: data['Adj Close'] for ticker, data in all_data.items()})
df_train.describe()
df.columns
import statsmodels.api as sm $ convert_old = df2.query('group=="control" & converted==1')['user_id'].count() $ convert_new = df2.query('group=="treatment" & converted==1')['user_id'].count() $ n_old = n_old $ n_new = n_new
X.head(1)
Temperature_year = session.query(Measurements.date,Measurements.tobs) \ $              .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \ $              .filter(Measurements.station == 'USC00519281').all() $ Temperature_year
rate.dtypes # Check the dtypes of the rate dataframe
df.word_count.sum()
plt.plot(data,color='k',linestyle='--',lw=1.25) $ plt.plot(purple_line, color='purple',lw=2.0,alpha=0.85) $ plt.plot(green_line,color='green',lw=1.75) $ plt.grid(True) $ plt.legend(['actual','pred','train'], loc='best')
dcrime_in = dcrime.apply(within_area,axis=1) $ print "Number of incidents within area",sum(dcrime_in),"total",len(dcrime_in) 
total_df['Res_id'].value_counts().head()
df=iris_fromUrl[iris_fromUrl.target=='setosa'] $ df
dt.strftime('%m/%d/%Y %H:%M')
df = pd.merge(data, tags, left_on=['tag'], right_on=['Tags'], how='outer')
import datetime $ def mdy_hm(datetimestring): $     return datetime.datetime.strptime(datetimestring, $                             '%m/%d/%Y %H:%M') $ df['Start Date'] = df['Start Date'].apply(mdy_hm) # element-wise
tweet_df.tail(5)
base_df.show(truncate=False)
df2.to_excel('./Data/chg.xls')
type(ts.index[0])
lm2 = sm.Logit(df2['converted'], df2[['intercept', 'UK_ab_page','US_ab_page']]) $ results2 = lm2.fit() $ results2.summary()
StockData.reset_index(inplace=True) $ StockData.index.values
run txt2pdf.py -o '2013 Snapshot.pdf' '2013 Snapshot.txt' $
from sklearn.preprocessing import PolynomialFeatures $ poly15 = PolynomialFeatures(degree=15) $ x_15 = poly15.fit_transform(x)
import statsmodels.api as sm $ from scipy import stats $ stats.chisqprob = lambda chisq, df2: stats.chi2.sf(chisq, df2) $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit()
table.colnames
keys_0604.integration_date.values
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_mean, (1-p_mean)])
p_new = df2['converted'].mean() $ print('p_new: ',p_new)
train_small_data.to_feather("../../../data/talking/train_small_data.csv.feather") $ val_small_data.to_feather("../../../data/talking/val_small_data.feather")
cand_date_df['sponsor_class'].value_counts()
_dm2 = pd.read_sas('./data/in/adtteos.sas7bdat', format='sas7bdat') $ _dm2.head()
Salesdata = load_data ('../data/SalesDataA.xlsm', sheetname = 'Sales') $ Adsdata = load_data ('../data/SalesDataA.xlsm', sheetname = 'Ads') $ Adsdata.head()
if download: $     cmd = "curl --silent {} -o {}/{}".format(url, opath, filename) $     r = call(cmd, shell=True)
learner.save_encoder('adam3_10_enc')
import pickle $ output = open('votes_by_party.pkl', 'wb') $ pickle.dump(multiple_party_votes_all, output) $ output.close()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
X_test.info()
try: $     temp_series_paris_naive.tz_localize("Europe/Paris") $ except Exception as e: $     print(type(e)) $     print(e)
scraped_batch6_top.to_csv('batch6_top.csv', encoding='utf-8') $ scraped_batch6_sec.to_csv('batch6_sec.csv', encoding='utf-8')
df.head(2)
typesub2017['Solar'] = typesub2017['Solar'][typesub2017['Solar']!='-'] $ typesub2017['Wind Onshore'] = typesub2017['Wind Onshore'][typesub2017['Wind Onshore']!='-'] $ typesub2017['Wind Offshore'] = typesub2017['Wind Offshore'][typesub2017['Wind Offshore']!='-'] $ typesub2017['Solar'] = typesub2017['Solar'].dropna()
X = data[['avg_dist', 'avg_rating_by_driver', 'avg_rating_of_driver', 'avg_surge', 'city', 'phone', $           'surge_pct', 'trips_in_first_30_days', 'ultimate_black_user', 'weekday_pct']] $ y = data.retained
companies_disclosing_individual_psc_count = len(active_psc_records[active_psc_records.kind == 'individual-person-with-significant-control'].company_number.unique()) $ companies_disclosing_individual_psc_count
train['date'] = pd.to_datetime(train['date'])
pd.DataFrame(features['MEAN(loans.loan_amount)'].head(10))
s.resample('Q', label='left').head()
cityID = '30344aecffe6a491' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fremont.append(tweet) 
CLIENT_ID = 'f5ee75104c694db8b49b46424b04e205' $ CLIENT_SECRET = 'F1DDE71D081844F799D83A12D8BC3C33' $ USER_NAME = 'org_data_analyst@blastmotion.com' $ PASSWORD = '#47sT>@Aa9'
weather['created_date'] = pd.to_datetime(weather['created_date'], errors = 'coerce')
kimanalysis.listfiles(result)
print(article.shape) $ article.head()
print(kickstarters_2017.shape) $ print(kickstarters_2017.info()) $ print(kickstarters_2017.describe()) $ print(kickstarters_2017.nunique())
model = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US', 'ab_page']]) $ results_2 = model.fit() $ results_2.summary()
building_pa_prc_shrink.loc[pd_aux.index, 'permit_number']=pd_aux.permit_number_notM
archive_df.rating_denominator.value_counts()
spark.table("orders").show(10)
topC = popC15[popC15.content == 'photo'].sort_values(by='counts', ascending=False).head(1) $ topC.reset_index(inplace=True) $ topC = topC.contact[0] $ topC = likes.loc[(likes['contact'] == topC) & (likes['content'] == 'photo')] $ topC.head()
import numpy as np $ import h5py $ import gdal, osr, os $ import matplotlib.pyplot as plt
acs_df_2 = acs_df[['pop', 'age', 'pct_male', 'pct_white', 'homeval']] $ X_2 = acs_df_2.drop('homeval', axis=1).values $ y_2 = acs_df_2['homeval'].values
tweets_df.in_reply_to_screen_name.value_counts().plot()  # alseyaseyah has people reply more than other newspapers but retweets less than other people $ plt.title("reply")                                                       # the cause may be that the people is replying with somthing they didn't like about the tweet 
scipy.stats.spearmanr(df["tripduration"],df["day"])
tweets.plot()
manager.image_df.loc[1,:] #This image should be in both dataframes
df_rand.head()
shift_entries['TimeElapsed'] = (shift_entries.SE_TIMESTAMP - shift_entries.SE_START).apply( $     lambda x: x.total_seconds() / 3600.)
for i, (label, col) in enumerate(agency_borough.iteritems()): $     print(i, label, col)
weather_features.interpolate(method='linear',axis=0,inplace=True)
class_merged=pd.merge(class_sales,class_items,on=['date','store_nbr','class','family'],how='left') $ class_merged=pd.merge(class_merged,class_perishables,on=['date','store_nbr','class','family'],how='left') $ class_merged.columns=['date','store_nbr','class','family','sum_unit_sales','no_items','no_perishable_items'] $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
df = pd.DataFrame(np.random.randn(50,4), index=dates, columns=list('JUAN')) $ df
scaled = scaled.dropna() $ scaled.head()
closeSeriesR = closingPrices.resample('5D').ffill()
word = 'supercalifragilisticexpialidocious' $ print(re.findall(r'[aeiou]', word))
sql("drop table orders").show()
log_1.columns
exiftool -csv -createdate -modifydate cisuabe5/cisuabe5_cycle1.MP4 cisuabe5/cisuabe5_cycle2.MP4 cisuabe5/cisuabe5_cycle3.MP4 cisuabe5/cisuabe5_cycle4.MP4 > cisuabe5.csv
filelist = [os.path.join('../../icyElephant/',i) for i in filelist]
merged_df['team_id_x'].apply(lambda x: 1 if x.isnull() == False)
lower_case = letters_only.lower()        # Convert to lower case $ words = lower_case.split()               # Split into words
X_train = data3[:302].drop('sales', axis = 1) $ Y_train = data3[:302].sales
min_df = 0.005 # minimum frequency of words needed to be a part of the model $ max_df = 1.0 # max frequency of words to take into account as part of the model $ num_k = 10 # number of clusters $ data = test $ model_iteration(min_df, max_df, num_k, data)
merged = posts_renamed.merge(comments_renamed, left_on='post_id', $                              right_on='comment_parent_post_id')
min([len(h.tweets) for h in heap])
clean_train_df.head()
print("Number of Malware objects in Enterprise ATT&CK") $ print(len(all_enterprise['malware'])) $ df = all_enterprise['malware'] $ df = json_normalize(df) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
airbnb_df['listed_year_month'] = airbnb_df['date_listed'].dt.to_period('M')
plt.hist(taxiData.Trip_distance, bins = 50, range = [0,10]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
print len(y_train) $ y_train = y_train.reshape(64155,1)
codes = pd.read_csv('data/icd-main.csv') $ codes = codes[(codes['code'] != codes['code'].shift())].set_index('code')
trump['source'].value_counts()
cats_df = cats_df.set_index(['Unnamed: 0']) $ cats_df.index.name = 'seq'
train['Survived'].describe()
ananas = TextBlob("I hate pineapples") $ ananas.sentiment
train_col.train_model(num_epochs=2,optimizer=optimizer_col)
stock["Date"] = pd.to_datetime(stock.Date, format="%Y-%m-%d", errors='ignore')
twitter_master2.to_csv('twitter_archive_master.csv',index=False)
plt.bar(range(len(unique)),counts) $ for i in range(len(unique)): $     print unique[i], counts[i]
oppose=merged[merged.committee_position=="OPPOSE"]
df.drop(1899)
df_ad_state_metro_1['candidates'].value_counts()
prediction_clean.head(3)
cur.fetchmany(5)
pd.merge(Games, Teams, on='TeamID', how='outer').head() # full outer join
df=pd.read_csv("outputs/allV_info.csv", sep='\t', index_col=0, keep_default_na=False, na_values="") $ df.shape
final_df = df3.join(dummy_country) $ final_df.head() $
autos["postal_code"].value_counts()
issues = pd.DataFrame(repos, columns=['number', 'title', 'createdAt'])
df.Date = pd.to_datetime(df.Date)
df1_after_df2 = df2.append(df1) $ df1_after_df2
from scipy.stats import norm $ norm.ppf(1-0.05) $
mean_weekday.plot(kind='bar', ylim=(0, 5000))
pd.set_option('display.max_colwidth', 500) $ paired_df_grouped['n_best_co_occurence'] = paired_df_grouped.best_co_occurence.apply(lambda x: len(x)) $ paired_df_grouped[['dataset_1', 'all_co_occurence', 'best_co_occurence', 'n_best_co_occurence']].head(10)
df.groupby("cancelled")["winter_pickup"].mean()
n_new = len(df2.query('landing_page=="new_page"'))
(v_invoice_link.loc[:, invoice_link.columns] == invoice_link).sum()
theft.head()
print("The total no of unique users are",df['user_id'].nunique())  #Many Users have same user id
scoring_url = json.loads(response_online.text).get('entity').get('scoring_url') $ print(scoring_url)
average_of_averages = df['average'].mean()
bnbAx['language_english'] = np.where(bnbAx['language']=='en', 1, 0) $
cityID = '0570f015c264cbd9' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Louis.append(tweet) 
air_visit.head(3)
df_h1b_nyc_ft.groupby(['lca_case_employer_address'])['lca_case_employer_name'].unique()
from sklearn.metrics import accuracy_score
baseline_temp = 291.483 $ weather_features['HDD'] = weather_data.temp.apply(lambda x: np.max([(baseline_temp-x)/24,0])) $ weather_features['CDD'] = weather_data.temp.apply(lambda x: np.max([(x-baseline_temp)/24,0])) $
master_df = pd.read_csv('twitter_archive_master.csv', parse_dates=['timestamp'])
sub_df.head()
challenges = pd.read_csv('challenges.csv')
le=LabelEncoder() $ X = df[['word_count','sentiment','domain_d','title','post_duration','text']] $ y = le.fit_transform(df['subreddit']) $
p_old = len(df2.query('converted==1'))/len(df2.index) $ p_old
df3.to_csv('output/last_100_tweets.csv', index=False)
test_predictions = xgb.predict(X_test) $ eval_sklearn_model(y_test, test_predictions, model=xgb, X=X_test)
df_merged.boxplot(column='rating_numerator');
liquor_2016 = liquor[~date_mask].sort_values(by=['Date']) $ bottles_sold = liquor_2016[['Bottles Sold','Month']].groupby(['Month']).sum() $ bottles_vs_month = bottles_sold.plot(kind='bar', figsize=(8,8)) $ bottles_vs_month.set_ylabel('Bottles Sold') $ bottles_vs_month.set_title('Total Bottles Sold per Month in 2016 (January 4 to  March 31)')
trains_fe2_y= trains_fe1[['reordered']] $ trains_fe2_y.head()
df_country=pd.read_csv('countries.csv') $ df_country.country.unique()
day_of_week14.to_excel(writer, index=True, sheet_name="2014")
train_data.dtypes
predictions_clean['p1'] = predictions_clean['p1'].str.title() $ predictions_clean['p2'] = predictions_clean['p2'].str.title() $ predictions_clean['p3'] = predictions_clean['p3'].str.title()
pandp_top_vec = all_top_vecs[bks.pandp[0]] $ print(bks.pandp) $ pandp_top_vec
usage_400hz_filter.head()
borough = {"BROOKLYN", "QUEENS", "MANHATTAN", "BRONX", "Unspecified", "STATEN ISLAND"} $ df_input_clean = df_input.filter(df_input["Borough"].isin(borough) == True)
user_logs[user_logs.msno == 'Pz51LVoS9ENG1kNHQyrJ3gG8A163pyHi+gyvN2p+1nM=']
plots.frequency_of_attack("DAY")
station_total.set_index('STATION').plot(kind='bar',figsize=(16,6),fontsize=3);
speeches_df3.head()
df.shape
post_url = 'https://api.instagram.com/oembed?url=http://instagr.am/p/fA9uwTtkSN/' $ response = requests.get(post_url)
def add_percentiles(df, quantiles = [25,75], look_back = 365): $     data_frame = df.copy() $     return data_frame
init = tf.global_variables_initializer() $ saver = tf.train.Saver()
train.show(3)
graf['DETAILS2']=graf['DETAILS'].progress_apply(text_process)
from pandas.util.testing import assert_frame_equal $ assert_frame_equal(df1, df2, check_dtype=False)
faa_data_minor_damage_pandas['AIRPORT'].value_counts()
transactions.merge(transactions,how='inner',on='UserID').head(5)
q=len(af.userid) $ print("Total Number of Users: ") $ q
obs_diff = p_treatment - p_control $ print('The observed difference in the conversion rates is {}.'.format(round(obs_diff,4)))
data.drop_duplicates()
pd.set_option("max.rows", 10) $ result
transactions[~transactions['UserID'].isin(users['UserID'])]
from sklearn.dummy import DummyClassifier $ dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X, y) $ y_predict_dummy = dummy_majority.fit(X, y)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, wait_on_rate_limit = True)
plt.figure(figsize=(16,8)) $ vT_gb = sns.countplot(x='vehicleType', hue='gearbox', data=df2) $ vg = vT_gb.set_xticklabels(vT_gb.get_xticklabels(), rotation=90)
stocks_pca_m4=np.delete(stocks_pca_m2,2,axis=1) #Removed NASDAQ
proj_df['Project Grade Level Category'].value_counts()
raw_full_df.bathrooms.value_counts()
chart = top_supporters.head(5).amount.plot.barh() $ chart.set_yticklabels(top_supporters.contributor_lastname)
p_diffs = [] $ for _ in range(0,10000): $     p_diffs.append(np.random.choice([1, 0], p=[p_new, 1-p_new], size=n_new).mean() - $                    np.random.choice([1, 0], p=[p_old, 1-p_old], size=n_old).mean())
print('Endpoint name: {}'.format(rcf_inference.endpoint))
print(non_member['duration_sec'].describe())
sns.set(font_scale=1)  # ONLY WAY TO MAKE IT READABLE LOL $ print np.sum(one_hot_encode_genres.values, 1) $ print 'Average number of genres a movie belongs to:', np.mean(np.sum(one_hot_encode_genres.values, 1)) $ print np.sum(one_hot_encode_genres.values, 0) $ print one_hot_encode_genres.columns
sns.heatmap(ndvi_us, vmin = -.1, vmax=1)
window = pdf.loc['2008-1-1':'2009-3-31'] $ portfolio_metrics(window) $ window.plot(); $
cand_date_df['sponsor_class'] = cand_date_df['sponsors'].map(mapping_dict) $ cand_date_df.head()
trump.head()
fld = 'StateHoliday' $ df = df.sort_values(['Store', 'Date']) $ get_elapsed(fld, 'After') $ df = df.sort_values(['Store', 'Date'], ascending=[True, False]) $ get_elapsed(fld, 'Before')
titanic.describe()
df['Approximate Age at Designation'] = df['membership_date'].apply(lambda x: x.year)-df['yob']
df_r1.loc[~df_r1["Counter"].isin(["3"])].to_csv("data excluded/excluded 7_customers without less than 3 purchases.csv", $                                        encoding="utf-8", sep=",") $ print("Customers with less than 3 purchases: " + str(len(df_r1.loc[~df_r1["Counter"].isin(["3"])])))
to_be_predicted_Day4 = 52.4480597 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
final_df = final_df[pd.isnull(final_df.retweeted_status_id)] $ final_df = final_df.dropna(subset = ['jpg_url']) $ fin_df=final_df.copy()
eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual'] $ eval_df.groupby('h')['APE'].mean()
100*np.exp(fundret.loc[date:].cumsum())
print("The propotion of users converted are",sum(df['converted'])/df.shape[0])
h2o.cluster().shutdown()
s = pd.Series() $ print (s) $ type(s)
db.collection_names()
test_pl.describe()
weekly_window = Window.partitionBy('week').orderBy(functions.desc('variance')) $ weekly_rank = functions.rank().over(weekly_window).alias('rank')
print(store_info.CompetitionDistance.describe()) $ store_info.CompetitionDistance.plot.hist();
df.tail(2)
store_items.fillna(method = 'backfill', axis = 0)
plt.plot(ages, weights,'.') $ plt.xlabel("mother's age") $ plt.ylabel("birth weight") $
beirut = beirut.rename(columns={'WindDirDegrees<br />' : 'WindDirDegrees'}) $ beirut['WindDirDegrees'] = beirut['WindDirDegrees'].str.rstrip('<br />') $ beirut['WindDirDegrees'].head(3)
import urllib $ iris_url="http://aima.cs.berkeley.edu/data/iris.csv" $ urlRequest=urllib.request.Request(iris_url) $ iris_file=urllib.request.urlopen(urlRequest) $ irisIterator=pd.read_csv(iris_file,sep=',',header=None,decimal='.',names=['sepal_length','sepal_width','petal_length','petal_width','target'],iterator=True)
from sklearn.neighbors import KNeighborsClassifier $ from sklearn.metrics import confusion_matrix
df_attacks= pd.DataFrame(columns=['id','userid','username','targetip','targeturl','duration','port','type','date']) $ df_plans=pd.DataFrame(columns=['planid','planname','plandescr','price','maxboottime','concurrency']) $ df_servers=pd.DataFrame(columns=['id','ip']) $ df_settings=pd.DataFrame(columns=['url','sitename','siteemail']) $ df_webshells=pd.DataFrame(columns=['id','url','status','lastchecked','attacktype'])
df.head()
convert=df2.query('converted=="1"').user_id.nunique()/df["user_id"].nunique() $ convert
autos.head() $ autos.info()
excelDF[["Sales","Region"]].head() $
tesla.head()
tweets_text = tweets_df.text.value_counts().index
labels = investments.copy() $ labels['invested'] = 1
df['tweet_location_coord'] = df['tweet_location_coord'].astype('object') $ df['user_timezone_coord'] = df['user_timezone_coord'].astype('object')
departures.head()
tsla_30 = mapped.filter(lambda row: row[3] > T0) $ tsla_30_DF = tsla_30.toDF(["cid","ssd","num_ssd","tsla","tuna"]) $ tsla_30_pd = tsla_30_DF.toPandas()
p_old = df2.query('landing_page == "old_page"')['converted'].mean() $ print('The convert rate for the new page:', p_old)
clf=clf.fit(X_train, y_train) # perform training $ y_predict = clf.predict(X_test) $ print(accuracy_score(y_test, y_predict)*100)
df_vow.head()
df2.query('group == "control"').user_id.nunique()/df2.user_id.count()
f = open('my_filename.txt','w') $ f.write(str(a)) $ f.close()
b.find_by_xpath('//*[@id="day-section"]/div/div[3]/div[8]/div[2]/ul/li[2]/button').click()
coin_mean = coins.mean() $ coin_mean
mentionsfreq = FreqDist(words_mention_scrape) $ print("20 most common mentions: ", mentionsfreq.most_common(20)) $ hashfreq = FreqDist(words_hash_scrape) $ print("20 most common hashtags: ", hashfreq.most_common(20))
print("Actual difference:" , p_diff) $ p_greater_than_diff = len(greater_than_diff)/len(p_diffs) $ print('Proportion greater than actual difference:', p_greater_than_diff) $ print('As a percentage: {}%'.format(p_greater_than_diff*100))
import statsmodels.api as sm $ convert_old = df2.groupby([df2['landing_page']=='old_page',df2['converted']==1]).size().reset_index()[0].iloc[3] $ convert_new = df2.groupby([df2['landing_page']=='new_page',df2['converted']==1]).size().reset_index()[0].iloc[3] $ n_old = df2.query('landing_page == "old_page"').user_id.nunique() $ n_new = df2.query('landing_page == "new_page"').user_id.nunique() $
TestData.shape
df.head()
import statsmodels.api as sm $ convert_old = sum(df2[df2.landing_page == 'old_page'].converted) $ convert_new = sum(df2[df2.landing_page == 'new_page'].converted) $ n_old = df2[df2.landing_page == 'old_page'].shape[0] $ n_new = df2[df2.landing_page == 'new_page'].shape[0]
g = sns.JointGrid(x='retweet_count',y='favorite_count',data=df) $ g = g.plot(sns.regplot, sns.distplot)
fig, ax = plt.subplots() $ typesub2017['Solar'].plot(ax=ax, title="Solar Energy Generation 2017 (DE-AT-LUX)", lw=0.7) $ ax.set_ylabel("Solar Energy") $ ax.set_xlabel("Time")
tweet_df.total.value_counts()
tesla.text = tesla.text.str.lstrip('b') $ tesla.created_at = pd.to_datetime(tesla.created_at) $ tesla['date'] = tesla.created_at.apply(lambda x: x.date())
autos.head()
image_predictions_df[image_predictions_df.tweet_id == 668623201287675904]
df['Department'] = pd.Series({0: 'HR', 2: 'Marketing'}) $ df
prob_conv = len(df2.query('converted == 1')) / len(df2['converted']) $ print('The  probability of an individual converting regardless of the page they receive is {}'.format(prob_conv))
nnew = df2[df2['group'] == 'treatment'].shape[0] $ print(nnew)
googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0] $ googletrend['State'] = googletrend.file.str.split('_', expand=True)[2] $ googletrend.loc[googletrend.State=='NI', "State"] = 'HB,NI'
scores[:1.625].sum()
df2.loc[1899] #check if first index has the same userid
grid_search.best_params_
sns.distplot(autodf.price)
NB.fit(X_train.f, y_train.f) $ y_predict_bayes = NB.predict(X_test.f)
reddit_comments_data.select('parent_id').distinct().count()
yc_merged = yc_trimmed.merge(yc_sd, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner')
bigdf.columns
leadDollarsClosedPerMonth.tail()
to_be_predicted_Day2 = 22.24028033 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
scoring_data = {'values': [image_1.tolist(), image_2.tolist()]}
S_distributedTopmodel = Simulation(hs_path + '/summaTestCases_2.x/settings/wrrPaperTestCases/figure09/summa_fileManager_distributedTopmodel.txt')
textList = tweets['text'][0:1000] $ textList= textList.tolist()
from sklearn.ensemble import RandomForestRegressor $ regressor=RandomForestRegressor(n_estimators=10, random_state=0) $ regressor.fit(y,z)
from sklearn.naive_bayes import MultinomialNB
autos['odometer'] = autos['odometer'].str.replace(',', '').str.replace('km', '') $
sns.regplot(df[asset2], df[asset1],  scatter_kws={'color':'r', 's':5})
total.last_valid_index()
counts_df_clean = counts_df.copy()
pd.isnull(jobPostDFSample.JobRequirment[0])
df['quantity'] = df['quantity'].astype(int) $ df
old_page_converted = np.random.choice(2,n_old,p=[0.8804,0.1196])
submit = pd.DataFrame(test.id, index=test.index) $ submit['click'] = 0.0 $ submit['id'] = submit['id'].astype(np.uint64) $ submit.dtypes
tz_cat['tweetRetweetCt'].max() $ tz_cat.index[tz_cat['tweetRetweetCt'] == tz_cat['tweetRetweetCt'].max()].tolist()
airbnb_df['host_acceptance_rate'] = np.random.randint(70,100, size=(airbnb_df.shape[0],1)) $ airbnb_df['host_response_rate'] = np.random.randint(70,100, size=(airbnb_df.shape[0],1))
partition = community.best_partition(G) $ print('Modularity: ', community.modularity(partition, G))
print iowa.info() $ iowa.describe() $
rpi_home = pd.DataFrame(rpi_home) $ rpi_away = pd.DataFrame(rpi_away)
np.exp(results_model.params)
(p_diffs>real_diff).mean()
len(np.unique(df['user_id']))
for c in ccc[:2]: $     spp[c] /= spp[c].max()
connection = sqlite3.connect("../../data/stocks.sqlite") $ query = "SELECT * FROM STOCK_DATA WHERE Volume > 29200100 AND Symbol='MSFT';" $ items = pd.io.sql.read_sql(query,connection,index_col='index') $ connection.close() $ items
df_mas['name'] = df_mas['name'].replace('None', np.NaN)
sqlContext.sql("select * from pcs where count > 1").show()
%matplotlib inline $ from xgboost import plot_tree $ from matplotlib.pylab import rcParams $ rcParams['figure.figsize'] = 80,50 $ plot_tree(XGBClassifier)
p_old = df2.converted.mean() $ print(p_old)
len(ibm_train.columns), len(feature_col)
train_df['num_photos'].ix[train_df['num_photos']>=16] = 16 $ num_photos = train_df['num_photos'].value_counts() $ x = num_photos.index $ y = num_photos.values $ sns.barplot(x, y )
fh_1 = FeatureHasher(input_type='string', non_negative=True) # so we can use NaiveBayes $ %time fit = fh_1.fit_transform(train.device_model)
IDX_train = df_train.iloc[0:,[0,2]].values $ IDX_train = IDX_train[0::sample_days] $ IDX_test = df_test.iloc[0:,[0,2]].values $ IDX_test = IDX_test[0::sample_days]
x = api.GetUserTimeline(screen_name="berniesanders", count=20, include_rts=False) $ x = [_.AsDict() for _ in x]
raw_df.sample(5)
obj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])
ts = [t.assign(category = c, course = i) for t, c, i in zip(record_tables, category, course)] $ raw = pd.concat(ts).reset_index(drop=True) $ raw.sample(5)
metrics = client.experiments.get_latest_metrics(experiment_run_uid)
pd.date_range('2015-07-03', '2015-07-10')
co = CombinatorialOptimisation() $ co.train(elec,cols=[('power','active')]) $
sns.distplot(a=fraud_data_updated.purchase_value)
mgxs_lib.build_library()
MergeMonth = Merge.copy(deep=True)
interp_spline = interpolate.RectBivariateSpline(sorted(lat_us), lon_us, temp_us)
events['type'].unique()
	df_ = df.groupby('msno').apply(within_n_days, T, n = 7).reset_index(drop = True) $ 	tbl = df_[df_.date_diff == 1].groupby('msno').date_diff.size().to_frame() $ 	tbl.columns = ['listen_music_in_a_row_count_during_t_7'] $ 	tbl['listen_music_in_a_row_ratio_during_t_7'] = tbl.listen_music_in_a_row_count_during_t_7 / df_.groupby('msno').date_diff.apply(len) $ 	tbl.reset_index(inplace = True) $
reg = linear_model.LinearRegression(fit_intercept=False, normalize=False)
selected_features=selected.index $ x_train_new=x_train[selected_features] $ x_test_new=x_test[selected_features]
df.loc[df.sex.isin(sex2m)]
p_converted = df['converted'].mean() $ print('Proportion of converted users: ',p_converted)
df_new.head()
for t in tables: display(t.name, t.head(), DataFrameSummary(t).summary())
words = ['Data Science','DataScience','datascience','Data Scientist','data science','data','data scientist','Data scientist'] $ pat = '|'.join(words) $ datascience_tweets = megmfurr_tweets[megmfurr_tweets['text'].str.contains(pat)] $ megmfurr_tweets[megmfurr_tweets['text'].str.contains(pat)]['text'].count()
test_kyo2 = test_kyo1.rename(columns={"ex_lat":"end_lat", "ex_long":"end_long"}) $ test_bkk2 = test_pl1.rename(columns={"ex_lat":"start_lat", "ex_long":"start_long"})
len(df_merged[0].unique())
db_tables=inspector.get_table_names() $ db_tables
sns.countplot(x="created_day", hue="interest_level", hue_order=['low', 'medium', 'high'], data=train_df)
import statsmodels.api as sm $ convert_old = df2[(df2.group == 'control')&(df2.converted == 1)].shape[0] $ convert_new = df2[(df2.group == 'treatment')&(df2.converted == 1)].shape[0] $ n_old = df2[(df2.group == 'control')].shape[0] $ n_new = df2[(df2.group == 'treatment')].shape[0]
np.exp(0.0408), np.exp(0.0506)
np.exp(results.params)
plt.plot(ds_cnsm['time'],ds_cnsm['met_salsurf'],'r.') $ plt.title('CP01CNSM, Sea Surface Salinity') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show() $
np.exp(logit_mod_joined2_result.params)
df_A.loc[df_A.index.str.endswith(('1','4'))] 
news_p = news_items[0].find(class_='rollover_description_inner').text $ print (news_p)
full_data.to_feather("../../../data/talking/nn_small_sample_full_data.feather")
cityID = '67b98f17fdcf20be' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Boston.append(tweet) 
lons, lats = np.meshgrid(lon_us, lat_us) $ plt.plot(lons, lats, marker='.', color='k', linestyle='none') $ plt.show()
dog_stage_stats[['favorite_count', 'retweet_count']].sort_values(by='favorite_count', ascending=False).plot(kind='bar', subplots=True)
df = df.drop(['cast', 'homepage', 'tagline', 'keywords', 'overview', 'imdb_id'], axis=1)
display(DataFrameSummary(ph).summary(), ph.info(), ph.head())
etsamples_100hz.iloc[1000]
df = pd.merge(train, $ transactions[(transactions.transaction_date < datetime.strptime('2017-03-01', '%Y-%m-%d'))], $ on='msno', $
p_new = (df2['converted']==1).mean() $ p_new
goals_df = pickle.load(open('goals_df.pkl', 'rb'))
z1=match_id(net_loans_exclude_US_outstanding,net_loans_exclude_US_outstanding.Beschreibung==u'R\xfcckgabe vom Inkassounternehmen/von einer internen Inkassostelle an den Gl\xe4ubiger') $ z1
print(df.query('group=="treatment" & landing_page!="new_page"').head()) $ print(df.query('group=="treatment" & landing_page!="new_page"').shape) $ print(df.query('group=="control" & landing_page!="old_page"').shape) $ 1965+1928
os.getcwd()
train_df.dropna(subset=['text','Q0_RELEVANT'], inplace=True)
pnewnull = df2.query('converted == 1').shape[0] / df2.shape[0] $ pnewnull
print(model.aic,model.bic,model.hqic)
iris_dataframe.head()
future_dates = [dfs.index[-1] + DateOffset(days=x) for x in range(0,30) ]
autos["date_crawled_10"]= autos["date_crawled"].str[:10] $ autos["ad_created_10"]=autos["ad_created"].str[:10] $ autos["last_seen_10"]=autos["last_seen"].str[:10]
conn_str = ( $     r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};' $     r'DBQ=C:\users\rws\google drive\mydocuments lenovo backup\sr24imported.mdb;' $     )
merged.head()
first_datetime = df1['DATETIME'].min() $ df_day1 = df1[(df1['DATETIME'] > first_datetime)&(df1['DATETIME'] <= first_datetime + timedelta(days=1))] $ df_day1.head(5)
limit = 2 $ for i, tweet in enumerate(collect.get_iterator()): $     if i < limit: $         print(json.dumps(tweet, indent=4))
ip.img_num.describe()
df_new[['UK','US']] = pd.get_dummies(df_new['country'])[['UK','US']]
y_retweet = df['retweet_count'] #1 Output $ y_fav = df['favorite_count']    #2 Output $ x.head()
b[b.T.sum()==c].index.min()
joined.to_feather(f'{PATH}joined') $ joined_test.to_feather(f'{PATH}joined_test')
image_predictions = re.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
for s in re.finditer(r'\d{1,2}\.(mp3|wav)', haystack): $     print(haystack[s.start():s.end()]) $     print('  [file {}]'.format(s.group(1)))
obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']
yhat = clf.predict(X_test) $ yhat [0:5]
p_conv = df2['converted'].mean() $ print('The probability of someone converting, disregaring the page he/she visited is {:2f}'.format(p_conv))
scaler = preprocessing.MinMaxScaler()
df.iloc[0]
autos['price'].unique().shape
 newdf.isnull().any()
affkeys = np.array(aff1y['variables'].keys())
old_page = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ print(len(old_page))
plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments) $ plot.set_ylim([-10, 10])
monthly_medication_df.second_month_adherence.mean()
train_data, test_data = df_input_final.randomSplit([0.7, 0.3])
xgb = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, $        gamma=0.4, learning_rate=0.1, max_delta_step=0, max_depth=3, $        min_child_weight=1, missing=None, n_estimators=100, nthread=-1, $        objective='binary:logistic', reg_alpha=0, reg_lambda=1, $        scale_pos_weight=1, seed=0, silent=True, subsample=1) 
db = db_connect.SpectrumDB()
crosstab.shape
import pandas as pd $ %time hundred_stocks_df = pd.read_csv('../data/hundred_stocks_twoyears_daily_bar.csv')
!head -n 5 p32c_results.txt
con.execute('INSERT INTO samples VALUES(\'{}\',{},{},{})'.format(*few_recs.ix[0]))
FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH) $ md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)
import datetime as dt $ df_concat["day"] = pd.to_datetime(df_concat["date"]) $ df_concat["day"] = df_concat["day"].map(dt.datetime.toordinal) $ df_concat["day"].head()
import datetime $ day = lambda x: x.split(' ')[0].replace('-',',') $ data_2012['date']=data_2012['createdAt'].apply(day)
resp = urllib2.urlopen(req) $ resp = myopener.open(req) $ html = resp.read()
openaccess_df = pd.read_csv('oa_file_list.csv') $ print(openaccess_df.shape) $ openaccess_df['PMCID'] = openaccess_df['Accession ID'].str[3:] $ print(openaccess_df.count(axis=0)) $ openaccess_df.head()
autos['price'].unique().shape
z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]),np.array([n_new,n_old]), alternative = 'larger') $ print ("z_score: "+ str(z_score) + "   p_value: "+str(p_value))
pd.Timestamp('2014-12-14 17:30')
age = pd.cut(titanic['age'], [0, 18, 100])  # Assume no-one is over 100 $ age.head()
archive_df.rating_denominator.value_counts()
for i, incorrect in enumerate(incorrect[0:9]): $     plt.subplot(3,3,i+1) $     plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none') $     plt.title("Predicted {}, Class {}".format(predicted_classes[incorrect], y_true[incorrect])) $     plt.tight_layout()
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
activityByUserByday = firstWeekUserMerged.groupby('userid')["day_time_stamp2"].value_counts().index.tolist() $ activityByUserByday = pd.DataFrame(activityByUserByday,columns=["userid","activeDay"]) $ activityByUserByday = activityByUserByday.groupby('userid')["activeDay"].count().reset_index() $ activityByUserByday.head(5)
new_page_converted=np.random.binomial(1,Conversion_Rate,Conversion_No) $
countries_log_reg = sm.Logit(df4['converted'], df4[['country_UK', 'country_US', 'intercept']]) $ country_result = countries_log_reg.fit()
tokaise.shape
df_train_time['month'] = df_train_time.loc[:, 'fulldate2'].map(lambda x: x.month)
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs),func.avg(Measurement.tobs)).\ $               filter(Measurement.station==active_station).all()
r.head()
import pandas as pd $ test_data = pd.read_csv("relex_example.csv") $ test_data.head()
learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))
new_set_two.purchase_value.mean()
model.fit(X,y) #fit the regression model using OLS
print("All Tweets: {0} | Users: {1}".format(len(matthew), matthew.user.nunique())) $ print("Tweets in the Matthew 160 Collection: ", len(matthew.query('matthew160'))) $ print("Users in the Matthew 160 Collection: ", matthew.query('matthew160').user.nunique())
df=df[["tripduration","date"]] $ df.head()
train[['StateHoliday_0', 'StateHoliday_a', 'StateHoliday_b', 'StateHoliday_c']] = pd.get_dummies(train['StateHoliday']) $ test[['StateHoliday_0', 'StateHoliday_a']] = pd.get_dummies(test['StateHoliday']) $ train.drop('StateHoliday', axis=1, inplace=True) $ test.drop('StateHoliday', axis=1, inplace=True)
df['intercept']=1 $ df['ab_page'] = pd.get_dummies(df['group']) ['treatment'] $
np.exp(results.params)
constructor.sort_values(by='price',ascending=True)
git_blame[git_blame.line == git_blame.line.max()]
week10 = week9.rename(columns={70:'70'}) $ stocks = stocks.rename(columns={'Week 9':'Week 10','63':'70'}) $ week10 = pd.merge(stocks,week10,on=['70','Tickers']) $ week10.drop_duplicates(subset='Link',inplace=True)
y_pred = regr.predict(X_test)
df2.query('group == "control" and converted == 1').count()[0]/df2[df2['group'] == 'control'].count()[0]
test = pd.read_csv('../input/sample_submission_v2.csv')
df_json_tweets['date_timestamp'] = pd.to_datetime(df_json_tweets['date_timestamp']) $ type(df_json_tweets['date_timestamp'].iloc[0])
y_test_under[bet_under].mean()
print("The user id repeated was",df2['user_id'].value_counts().head(1))
import datetime $ data['yyyymm'] = data['Created Date'].apply(lambda x:datetime.datetime.strftime(x,'%Y%m'))
frames = [data['title_len'], dfcv] $ my_features = pd.concat(frames, axis=1, ignore_index=True )
ex4 = pd.DataFrame({"vegetable": ["lettuce", "brocolli", "carrot"], $                    "corn": ["beef", "tofu", "fish"], $                    "stable": ["rice", "pasta", "soba"], $                    "remark": ["sold out","sold out", None]}) $ ex4
titanic.age.fillna(titanic.age.mean(), inplace=True)
df1.isnull().sum()
sex = df_titanic['sex'] $ print(sex.describe()) $ df_titanic['sex'] = df_titanic.sex.astype('category')
datetime_cumulative = {turnstile: [(datetime.strptime(date + time,'%m/%d/%Y%X'),int(in_cumulative)) $                                    for _, _, date, time,_, in_cumulative, _ in rows] $                        for turnstile, rows in raw_readings.items()}    
result['cycle'] = result.videoname.str.replace('6_part1', '6') $ result['cycle'] = result.cycle.str.replace('6_part2', '7') $ result['cycle'] = result.cycle.apply(reversetextsplitter,sep='cycle') $ result['cycle'] = result.cycle.apply(textsplitter,sep='_') $ result['cycle'] = result.cycle.apply(textsplitter,sep='.')
len(train_data[train_data.vehicleType == 'andere'])
plt.figure(figsize=(8, 5)) $ train_df.groupby('flow').favs.median().plot.bar() $ plt.title('Median of the #favs by flow'); $ plt.xticks(rotation='horizontal');
data_sets['15min'].head()
path = '/home/gabriel/share/Prestashop/' $ data = pd.read_csv(path+"Prestashop_Ready_dataset.csv")
df1.info()
time.mktime(jdfs.pushed_at[0].timetuple())
result.index
df2['user_id'].drop_duplicates(inplace=True) $ df2['user_id'].duplicated().count()
df.drop(df[df.state.isin(non_usa_states)].index, axis=0, inplace=True)
data.iloc[[1,100,20], [6,7,8]].plot(kind='bar', stacked=True)
data.iloc[9]["Improvements"]
random_numbers.groupby(pd.Grouper(freq='M')).mean() #calender months with each months mean values
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs); $ plt.axvline(x=p_new-p_old,c='r',label="real diff") $ plt.axvline(x=(p_diffs.mean()),c='y',label="simulated diff")
atom = tb.UInt8Atom(shape=(2,)) $ atom
merged.sort_values('amount', ascending=False)
pd.merge(GameResults, Boxscores, on=['TeamName', 'Date']).head()
mgxs_lib = openmc.mgxs.Library(geometry) $ mgxs_lib.energy_groups = groups
train_session = pd.merge(session_v2,train, on='user_id', how='inner')
df.columns=df.columns.str.strip() #strip off trailing and leading whitespace
quotes = yahoo_finance.download_quotes("GILD") $ print quotes
df_mk.CreatedDate = df_mk.CreatedDate.map(lambda x: np.datetime64(x)) $ df_mk['quarter'] = df_mk.CreatedDate.dt.to_period('Q-SEP') $ df_mk['month'] = df_mk.CreatedDate.dt.to_period('M') $ df_mk['updated_leadSource'] = df_mk.leadSource $
logit_mod2 = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ results = logit_mod2.fit() $ results.summary2()
train.corr()["hotel_cluster"]
modeling1.show(5)
type(df_clean['date'].iloc[0]) $ type(df_clean['time'].iloc[0])
gdf = gdf.copy() $ gdf['length'] = gdf['end'] - gdf['start'] + 1 $ gdf.head()
c_ctrl = df2.query('group == "control"')['converted'].mean() $ c_ctrl
len(sel_df[sel_df.BlockRange.isnull()])
df_survival['Churn Flag'] = np.where(df_survival['Churn Date'].isnull() | (df_survival['Churn Date'] > df_survival['Most Recent Donation']) , np.where(df_survival['Churn Date'].isnull(), 'None', 'Y'), 'N') $ df_survival['Churn Flag'].value_counts()
components3= pd.DataFrame(pca.components_, columns=['internet','happiness','SP500','DJIA'])
skip_idx.sort()
conversion_mean = df['converted'].mean() $ conversion_mean
logit_model = sm.Logit(df3['converted'],df3[['intercept','CA','UK']]) $ results = logit_model.fit() $ results.summary()
d_bnbAx.head()
flat = TextBlob("The Earth is flat") $ flat.sentiment
df1.head()
season10 = ALL[(ALL.index >= '2010-09-09') & (ALL.index <= '2011-02-06')]
y.summary()
autos['brand'].value_counts(normalize=True)
sns.set(style="whitegrid") $ f, ax = plt.subplots(figsize=(10, 10)) $ sns.set_color_codes("pastel") $ sns.barplot(x="Total_Num_Comments", y="Subreddit", data=subred_num_tot[:10], $             label="Comments", color="b")
df.shift(1)
google_stock.corr()
contributions.info()
model.show_topic(0)
df.groupby(['landing_page', 'group', 'converted']).count()
duration_df.columns
import statsmodels.api as sm $ logit = sm.Logit(df3['converted'],df3[['intercept','ab_page']]) $ results = logit.fit()
cityID = 'adc95f2911133646' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Colorado_Springs.append(tweet) 
data.count(axis=0)
df_uro_no_cat = df_uro_no_metac.drop(columns = ls_other_columns)
y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name="y_pred")
loans_act_arrears_latest_paid_xirr=cashflows_act_arrears_latest_paid_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf)) $ loans_act_origpd_latest_paid_xirr=cashflows_act_origpd_latest_paid_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
numeric_df.agg(*corr_aggs).toPandas()
tweet_df['lang'] = tweet_df['lang'].apply(lambda x: x.strip())
lgbm_train.to_csv('stacking_input/lgbm_tscv_train.csv',index=False)
snow.select("select count(patient_id) from nk_mnd_index where left(patient_id, 5) = 'XXX -'")
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=31000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
free_data[free_data['educ']>5].groupby('age_cat').describe()
url_SEA = "https://seahawks.strmarketplace.com/Images/Teams/SeattleSeahawks/SalesData/Seattle-Seahawks-Sales-Data.xls"
initialize_op = tf.global_variables_initializer() $ sess.run([initialize_op])
dfSummary = pd.concat([summary_all, summary_bystatus],axis=1) $ dfSummary.columns = ("1930-2017","1930-1980","1984-2017") $ dfSummary
X = X[X.columns[rfe.get_support()]]
gs.fit(X,y)
datAll['Offense Type'] = datAll['Offense Type'].str.strip() $ datAll['Offense Type'] = np.where(datAll['Offense Type']=="AutoTheft",'Auto Theft',datAll['Offense Type'])
lm = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA','UK']]) $ logit_result = logit_model.fit() $ logit_result.summary()
%timeit re.findall(r'<p>.*?</p>', html, re.DOTALL) $ %timeit tree.xpath('//p') $ %timeit soup.find_all('p')
df2.join([df1, df3, df4], how='outer').fillna('')
pd.merge(df1, df3, left_on="employee", right_on="name" ).drop("name", axis=1)
full_data.order_date.min(),full_data.order_date.max()
dimension = hidden_states.shape[-1] $ index = AnnoyIndex(dimension) $ for i, v in enumerate(sum_vecs): $     index.add_item(i, v) $ index.build(10)
df_vow[['Open','High','Close','Low']].plot()
df2= df2.drop_duplicates(['user_id'],keep='first');
top_songs.rename(columns={'Region': 'Country'}, inplace=True)
tfidf_vect = TfidfVectorizer(analyzer = clean_text) $ X_tfidf = tfidf_vect.fit_transform(tweets_1['text']) $ print(X_tfidf.shape)
print pd.pivot_table(data=cust_data, index='SeriousDlqin2yrs', values='No_of_RealEstateLoans',aggfunc='mean' )
train['Embarked'].mode()
conf_matrix = confusion_matrix(y_test, lr1.predict(X_test), labels=[1,2,3,4,5]) $ conf_matrix
type.__new__(type,'A',(),{"a":1})
np.ones(5)
data['funny'].value_counts()
df_all.age.value_counts()
sns.distplot(reddit['Hour of Day'], kde=True, bins=10, color='green')
run txt2pdf.py -o '2012 Snapshot.pdf' '2012 Snapshot.txt' $
Genres=",".join(Genres).join(("",""))
not_creditworthy_user=merkmale.groupby(level=['dwh_country_id','id'])['not_creditworthy', $                                                                'not_creditworthy_US', $                                                                'not_creditworthy_KRMLKWKX_US'].max() $ not_creditworthy_user.sum()
root_dir = "/Users/gogrean/Documents/Insight_Fellowship/Research/Mental_Health/NHANES_Survey/" $ os.chdir(root_dir) $ faers_data_dir = "data/FAERS/" $ faers_data_subdirs = [d[0] for d in os.walk(faers_data_dir) if d[0][-5:]=='ascii']
set_renv_token("data_sci_8001_token.rds", override = TRUE)
autos['lastSeen'] = autos['lastSeen'].str[:10] $ (autos['lastSeen'] $ .value_counts(normalize=True,dropna=False) $ .sort_index(ascending=True))
frame.sort_values(by=['a', 'b'])
d10 = d9.unstack(level = (0,1)) $ d10
my_query = "SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES" $ my_result = limsquery(my_query) $ my_result
autos.price = (autos.price $                            .str.replace('$','') $                            .str.replace('.','') $                            .str.replace(',','') $                            .astype(float)) $
print(server.server) $ book = gcat.get_file(spreadsheet_name, fmt='pandas_excel') $ experiments = book.parse('Experiments', header=0) $ created = server.post_sheet('/experiments/', experiments, verbose=True, dry_run=True, validator=validator) $ print(len(created))
for index, row in df.iterrows(): $     df.loc[index,'is_oc_company'] = row.postcodes in row.oc_postcodes
input_data['categories'] = pd.cut(input_data['normalized_commits'], bins=[0,0.95,3.7], labels=[0,1])
from quantopian.pipeline import CustomFactor, Pipeline $ import numpy as np
icecream.plot.bar(x='Flavor', y='Price', legend=True)
print("Number of Tools in ATT&CK") $ print(len(all_attack['tools'])) $ tools = all_attack['tools'] $ df = json_normalize(tools) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
rGraphData.tail()
spark.sql('create database if not exists my_analysis_work') $ spark.sql('drop table if exists my_analysis_work.example_timeseries') $ joined_patient.write.saveAsTable('my_analysis_work.example_timeseries')
places = api.geo_search(query="Brighton", granularity="city") $ place_id_B = places[0].id $ print('Brighton id is: ',place_id_B)
df[df['Complaint Type'] == 'Homeless Encampment'].sort_index().resample('M').count().plot(y='Complaint Type')
df = df.sort_values(by=['seq_id','work_day'],axis=0,ascending=[True, True]) $ df.to_pickle(pretrain_data_dir+'/pretrain_data_03.pkl')
from pyensae.finance import StockPrices $ stock = StockPrices("MSFT", folder=".", url="yahoo") $ stock.tail()
print('Slope: Efthymiou vs experiment: {:0.2f}'.format(popt_opb_chord_saddle[2][0])) $ perr = np.sqrt(np.diag(pcov_opb_chord_saddle[2]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
for field_name, dtype in df.select_dtypes(include=categories).items(): $     print(field_name) $     df[field_name] = pd.Series(pd.Categorical(df[field_name]).codes)
sorted_time = df2['timestamp'].sort_values() $ sorted_time[0] - sorted_time[len(sorted_time)-1]  $
kmeans = KMeans(k=3, seed=1) $ model = kmeans.fit(feature_sel)
z_score, p_value = sm.stats.proportions_ztest([17739, 17498], [147239, 147239]) $ z_score, p_value
ALLbyseasons = offseason07.append([season07, offseason08, season08, offseason09, season09, offseason10, season10, offseason11, $                                   season11, offseason12, season12, offseason13, season13, offseason14, season14, offseason15, $                                   season15, offseason16, season16, offseason17, season17, offseason18]) $ ALLbyseasons.head() # Checks the headers
data['Incident Zip'].unique()
df["stamp"] = to_datetime(df["created_at"],format='%a %b %d %H:%M:%S +0000 %Y') $ df.head()
weather.loc[weather.NAME == 'RALSTON RESERVOIR, CO US'].boxplot(column="TMAX");
prepared['goal'].hist(bins=50,log=True) $ plt.figure() $ prepared['backers_count'].hist(bins=50,log=True) $ plt.figure()
df_ad_airings_4.drop('ad_duration',inplace=True, axis=1)
df3['Donation Received Date'] = pd.to_datetime(df3['Donation Received Date']) $ df3['Most Recent Donation'] = pd.to_datetime(df3['Most Recent Donation'])
datatest.loc[datatest.surface_total_in_m2 == 0, 'surface_total_in_m2'] = np.NaN
df_users_6_after.shape
prob_rf1000 = rf1000.predict_proba(Test) $ prob1_rf1000 = pd.Series(x[1] for x in prob_rf1000) $ Results_rf1000 = pd.DataFrame({'ID': Test.index, 'Approved': prob1_rf1000}) $ Results_rf1000 = Results_rf1000[['ID', 'Approved']] $ Results_rf1000.head()
r2.json()
df_dates_final.shape[0] - df_dates_new.shape[0] $
pca.fit(scaled_data)
s = pd.Series(['a1', 'b2', 'c3']) $ s
submission2[['proba']].mean()
authors_per_file[authors_per_file == 1].count()
df_enhanced = df_enhanced.rename(index=str, columns={"rating_numerator": "rating_10_scaled"})
data = fat.add_sma_columns(data, 'Close', [6,12,20,200])
df_archive_clean.text[334]
df_train = pd.concat((df_train, pd.read_csv('C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/train_v2.csv',dtype={'is_churn' : np.int8} )), axis=0, ignore_index=True).reset_index(drop=True)
closed_prs = PullRequests(github_index) $ closed_prs.is_closed() $ num_closed_prs = closed_prs.get_cardinality("id_in_repo").get_aggs() $ closed_prs.get_cardinality("id_in_repo").by_authors("author_name") $ response = closed_prs.fetch_aggregation_results()['aggregations']
train_data[(train_data.latitude == 0) | (train_data.longitude == 0)][ $     ['latitude','longitude','interest_level']].groupby( $     'interest_level').count()
df[['Price Increase Count M/M']].tail(100) $ df[['Price Increase Count Y/Y']].tail(100) $ df[['Pending Listing Count M/M']].tail(100) $ df[['Pending Listing Count Y/Y']].tail(25) $
Users_first_tran=pd.merge(users,transactions,how="left",on="UserID").groupby("UserID").min().reset_index()
import nltk $ from nltk.corpus import stopwords $ import  string
house_data.columns
test = pd.read_json('./data/test.json')
props.info()
plt.figure(figsize = (15,8)) $ plt.plot(New_df.Date_of_Order, New_df.Sales_in_CAD, 'ro', Returning_df.Date_of_Order, Returning_df.Sales_in_CAD, 'go') $ plt.ylim(-500, 50000) $ plt.ylabel('Sales in CAD') $ plt.title ('Sales to Returning customers vs. New customers over time')
X.info()
usage_400hz_filter.head()
df.head(2)
ser5.iloc[[1,2,3]]
def plotfreqs(ax): $     for freq in freqs: $         ax.axvline(freq)
pd.merge(df1,df2)
stoplist = stopwords.words('english') $ stoplist.append('free') $ print(stoplist)
rng_dateutil.tz
tweets_df.head()
tables = pd.read_html(url) $ tables
suspects_with_25_2['timestamp'] = pd.to_datetime(suspects_with_25_2['timestamp'])
fit4.resid.hist();
model.doesnt_match("paris berlin london austria".split())
stations_des=session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ stations_des
df_c_merge[['CA', 'UK', 'US']] = pd.get_dummies(df_c_merge['country']) $ df_c_merge.head()
rng = np.random.RandomState(23) $ sample_size = 50 $ rng.choice(sizes, sample_size, replace=True)
views_data_clean.to_csv('data/views_data_clean.csv',index=False)
most_yards = niners_offense.sort_values(by='Yards.Gained', ascending=False)[:50]
joined['onpromotion'].value_counts()
Results_rf200.to_csv('soln_rf200.csv', index=False)
df.info
data.popular.value_counts()
df_new.loc[ df_new['public_repos'] >= 30 , 'score_con_1'] = 5 $ df_new.loc[ (df_new['followers'] < 30) & (df_new['followers'] >= 10) , 'score_con_1'] = 3 $ df_new.loc[ (df_new['followers'] < 10) & (df_new['followers'] >= 3) , 'score_con_1'] = 1 $ df_res['score_contributions'] = df_new['score_con_1'] $ df_res
xml_in.dtypes
geocoded_evictions_df.head()[['Geo.Address','Defendant.Addr.Line.1']]
scalingDF = trainDF[['DISTANCE', 'HDAYS']].astype('float') # Numerical features $ categDF = trainDF[['MONTH', 'DAY_OF_MONTH', 'ORIGIN_AIRPORT_ID', $                    'DEST_AIRPORT_ID', 'ARR_HOUR', 'DEP_HOUR', $                    'CARRIER_CODE', 'DAY_OF_WEEK']] # Categorical features $
orgs.hist(column = 'founded_year')
import statsmodels.api as sm $ convert_old = sum(df2.query('group == "control"')['converted']) $ convert_new = sum(df2.query('group == "treatment"')['converted']) $ n_old = len(df2.query('group == "control"')) $ n_new = len(df2.query('group == "treatment"'))
%%time $ c = collection.find({}, {'_id':0, 'id':0, 'building_id':0, 'manager_id':0, 'listing_id':0, 'photos':0}) $ raw_df = pd.DataFrame(list(c)) $ raw_df.interest_level = raw_df.interest_level.astype('category', categories=['low', 'medium', 'high'])
trainData = trainData.filter(lambda line : line[1] != 1034635)
df['Vader'] = np.array([ sentiment_finder_vader(comment) for comment in df['body'] ])
so.loc[so['viewcount'] > 20000, ['creationdate', 'viewcount', 'ans_name']].head(10)
z1=cashflows_fut_bucket_30360[cashflows_fut_bucket_30360.payback_state!='payback_complete'] $ xirr(z1.payment, z1.dcf)
from sklearn.feature_extraction.text import CountVectorizer
result.index.names
noise.columns
worst_score = df.rating_score.min() $ print('The worst beer got a {}. Ouch!'.format(worst_score))
df['statement_type'].unique()
for c in ccc: $     vhd[c] = vhd[vhd.columns[vhd.columns.str.contains(c)==True]].sum(axis=1)
collect.get_iterator()
z1.head()
from matplotlib.pyplot import figure $ figure(num=None, figsize=(14, 3), dpi=80, facecolor='w', edgecolor='k') $ joined = list(joined_hist.joined) $ values = list(joined_hist.joined_freq) $ plt.plot(joined, values, linewidth=1.0, marker='o', linestyle='-')
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df['group']) $ df2.head()
activesel = [func.min(Measurement.tobs), $        func.max(Measurement.tobs), $        func.avg(Measurement.tobs)] $ activeStationStat = session.query(*activesel).filter(Measurement.station == "USC00519281").all() $ activeStationStat
sharpe_ratio(data,daily_rf=0.00037828653,portfolio=True)
rf = RandomForestClassifier(n_estimators=400, n_jobs=4, min_samples_split=5) $ rf.fit(train_x[variable_indices[:10]], train_y) $ prediction =rf.predict_proba(test_x[variable_indices[:10]])
b.loc[0]
coefs = pd.DataFrame(log_reg_over.coef_, columns=X_train.columns) $ coefs.T.sort_values(0).head().T
plt.scatter(aqi['AQI_eug'], aqi['AQI'], alpha=0.2) $ plt.xlabel('Eugene/Springfield'); plt.ylabel('Oakridge');
df_arch_clean.info()
normal_model = Prophet(yearly_seasonality =True,weekly_seasonality= True,daily_seasonality = True) $ normal_model.fit(df1_normal); $ normal_future = normal_model.make_future_dataframe(periods= 6 , freq = 'M')    $ normal_forecast = normal_model.predict(normal_future) $ plotzero = normal_model.plot(normal_forecast) $
products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv') $ products.head()
print('Ridge Coefficient: \n', ridgemodel.coef_) $ print('Ridge Intercept: \n', ridgemodel.intercept_)
df.head()
Bow_train_X = train_Bow.drop([0, 1, 2, 4, 5, 6], 1) $ Bow_train_X = scalar.fit_transform(Bow_train_X) $ Bow_train_y = train_Bow[3].astype(int) $ Bow_X_train, Bow_X_test, Bow_y_train, Bow_y_test = train_test_split(Bow_train_X, Bow_train_y, random_state=24)
prediction_plot = prediction.plot(x= 'Prediction 1', y= 'difference 1', style=".", label = "prediction 1", color = 'blue') $ prediction.plot(x= 'Prediction 2', y= 'difference 2', style=".", color = 'red', ax=prediction_plot, label = "prediction 2") $ prediction.plot(x= 'actual order date', y= 'difference 1', style=".", color = 'green', ax=prediction_plot, label = "actual")
pred = pipeline.predict(ogXfinaltemptf) $ print classification_report(pred, ogy) $ print confusion_matrix(ogy, pred) $ print "cross val score accuracy :"+str(sum(cross_val_score(pipeline, ogXfinaltemptf, ogy))/3) $ print 'roc_auc score :'+str(sum(cross_val_score(pipeline, ogXfinaltemptf, ogy, scoring='roc_auc'))/3) $
counts_df_clean.info()
with open('file_2', 'w') as f: $     writer = csv.writer(f) $     reader = csv.reader(r.text.splitlines()) $     for row in reader: $             writer.writerow(row) $
segmentData.opportunity_size.value_counts()
print(DataSet_sorted['tweetText'].iloc[-3])
dfdaycounts['created_date'] = pd.to_datetime(dfdaycounts['created_date'], errors = 'coerce')
All_tweet_data_v2=All_tweet_data_v2.drop('Remove', 1)
round(date_df['duration(min)'].describe(), 2)
liberia_data.Description.replace('Newly Reported Cases in HCW', 'Total new cases registered so far', inplace=True) $ liberia_data.Description.replace('Newly Reported deaths in HCW', 'New deaths registered', inplace=True)
autos.head()
features = pd.merge(features, form_btwn_teams.drop(columns=['margin']), on=['game', 'team', 'opponent'])
df_full['school_type'] = df_full['school_type'].map(DATA_L1_HDR_DICT)
X=df.loc[:,'age':'language_zh'] $ X
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
train, val = train_test_split(merged_data, test_size=0.2)
run txt2pdf.py -o "2018-06-18 1525 2014 872 discharges.pdf"  "2018-06-18 1525 2014 872 discharges.txt"
data.loc[:'Illinois', :'pop']
n_old = df2[df2['group']=='control']['user_id'].nunique() $ n_old
image_predictions_clean[image_predictions_clean.jpg_url.duplicated()].jpg_url $
tasks = {k: v for k, v in result.groupby('taskid')} $ print('Number of tasks '+str(len(list(tasks)))) $ print(list(tasks)[0]) $ display(tasks[list(tasks)[0]])
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
price_vs_km["mean_odometer_km"].describe()
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
room_temp.temperature = 5
df_noise = df[df['Complaint Type'].str.contains("Noise")] $ df_noise.groupby([df_noise.index.hour, "Complaint Type"])['Created Date'].count().unstack().plot(kind="bar", stacked=True, figsize=(10,6))
print(f"Now {urls[2]} returns:") $ ux.is_short(urls[2], list_of_domains=known_shorteners)
apple = pd.read_csv(url) $ apple.head()
from scipy.stats import norm $ norm.ppf(1-(0.05))
airline_df = pd.read_csv('data/unitedAIRLINES.csv') $ airline_df
binary_sensors_list = [entity[0] for entity in entity_id_list if entity[0].split('.')[0] == 'binary_sensor'] # Print only the sensors $ binary_sensors_list
ab_dataframe.isnull().any().any()
df.columns
import statsmodels.api as sm
pd.get_dummies(df)
duration = df_TimeStamp.max() - df_TimeStamp.min() $ print("The duration of the test is {} days.".format(duration.days))
data = pd.read_csv('csvs/datosConDuplicados.csv', low_memory=False)
p_old = df2.converted.sum()/len(df2) $ p_old
num_open_issues = Issues(github_index).is_open().get_cardinality("id_in_repo") $ print("Number of currently open issues: ", get_aggs(num_open_issues))
data_donald_replies.head()
print(df['score'].mean())
combined.iloc[:,18:] #only going to use the words and not the numbers
player_details = pd.read_csv("detailed_player_stats_2017.csv").drop_duplicates() $ player_details.rename(columns={"PERSON_ID":"PLAYER_ID"}, inplace=True)
trainDF.drop('UNIQUE_CARRIER', axis = 1, inplace = True)
forecast_df['under_20'] = 0 $ for ind, row in SANDAG_age_df[SANDAG_age_df['AGE_RANGE'] == '10 to 19'].iterrows(): $     forecast_df.loc[ind, 'under_20'] = row['POPULATION'] $ forecast_df.head()
training_df.tail()
df_capsule.to_csv("BaseCapsulePSE")
from pandas import ExcelWriter $ with ExcelWriter("multisheets.xls") as writer: $     df.to_excel(writer, sheet_name="sheet1") $     df.to_excel(writer, sheet_name="sheet2")
with open(datafile, "r") as f: $     data2 = csv.DictReader(f) $     for row in data2: $         print(row)
CMVolMvt = (delta(df_new, df_old, ['CMDemandMW'], ConvertToMWH=True) * (df_old['CM_TP'] - df_new['RealPrice'])).rename('CMVolMvt') $ CMTPMvt = (delta(df_new, df_old, ['CM_TP']) * (df_new['CMDemandMW'] / 2)).rename('CMTPMvt')
df['CIK']=df['CIK'].map(lambda x:str(x).zfill(10))
life.polarity
n_old = df2[df2['landing_page']=='old_page'].shape[0] $ p_old = df2.converted.mean() $ old_page_converted = np.random.choice([0,1], n_old, p= [1-p_old, p_old]) $ unique, counts = np.unique(old_page_converted, return_counts=True) $ dict(zip(unique, counts))
new_read['is_read'] = ((new_read['read_time_x']/new_read['read_time_y'] > 0.7) & (new_read['read_length'] > 70)).astype(int)
df3[['CA', 'UK', 'US']] = pd.get_dummies(df3.country) $ df3
tweet_pickle_path = r'data/twitter_01_20_17_to_3-2-18.pickle' $ tweet_data.to_pickle(tweet_pickle_path)
sum(autos["registration_year"].between(2019,99999))
to_be_predicted_Day5 = 36.4845596 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
snow.select (" select * from RWD_DB.RWD.ALBATROSS_EHR_PROBLEMS limit 3")
missalignments = df.query('group == "control" and landing_page != "old_page" or \ $                         group != "control" and landing_page == "old_page"') $ missalignments.shape[0]
expx=np.mean(x) $ expy=np.mean(y) $ expx, expy
dfTemp = getDataFiltering(df_test) $ dfTemp.shape[0] $ dfTemp.to_csv('../data/test_step1.csv')
log_file = "hdfs://localhost/user/logs/*" $ filtered_data = sc.textFile(log_file)\ $     .filter(lambda line: any(item in line for item in broadcast_list_sc.value)) $ filtered_data.take(10)
print(np.isfinite(trainDataVecs).all())
lm=sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'CA', 'UK']]) $ results=lm.fit() $ results.summary() $
walmart= pd.Period('2017Q1', freq='Q-JAN') # We have to specify Q ending in January
exiftool -csv -createdate -modifydate cisuabd4/cisuabd4_cycle1.MP4 cisuabd4/cisuabd4_cycle2.MP4 cisuabd4/cisuabd4_cycle3.MP4 cisuabd4/cisuabd4_cycle4.MP4 cisuabd4/cisuabd4_cycle5.MP4 cisuabd4/cisuabd4_cycle6.MP4 > cisuabd4.csv
pd.read_csv('twitter_archive_master.csv').head()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) $ X_train.shape
df.neu = df[["COMPANY", "DISPOSITION_TYPE", "TOTAL_PAYMENT", "DATE"]] $ df.neu
swinbounds.to_csv('../data/swPositive.csv')
p_new = df2['converted'].mean() $ print "Convert rate of an individual received the new page:",p_new
b = regr.intercept_[0] $ b
data_matrix = num_data.as_matrix()
pandas_small_frame = small_frame.as_data_frame() $ print(type(pandas_small_frame)) $ pandas_small_frame
calls_df.info()
import pandas as pd $ import numpy as np $ from sklearn.linear_model import LinearRegression
plate_appearances = plate_appearances.loc[plate_appearances.events.isnull()==False,]
conv_mean = df2['converted'].mean() $ conv_mean
p_conv = df2.converted.mean() $ p_conv
dot818_dup = [486782526] $ fields = ['APP_APPLICATION_ID','APPLICATION_DATE','APP_SOURCE','APP_SSN','APP_LAST_NAME','APP_FIRST_NAME','DEC_FINAL_DECISION'] $ extract_all.loc[(extract_all.APP_SSN.isin(dot818_dup)), fields]
flight.show(2)
cfd_pairs = [(cv, w) for w in wsj $             for cv in re.findall(r'[a-z][a-z]', w.lower())] $ cfd_index = nltk.Index(cfd_pairs)
text = np.array(text)
data = data.rename(columns={0:'time',1:'sentiment'})
max_retweet = DataSet['tweetRetweetCt'].max() $ max_retweet
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     print(f.root.dangling_link[...]) $     print(f.root.dangling_link.is_dangling())
df["mail ID"] = df["mail ID"].apply(lambda x: x.split('.')[0]) $ df = df.set_index("mail ID")
df_columns.dtypes $
weights['0.encoder.weight'] = T(new_w) $ weights['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w)) $ weights['1.decoder.weight'] = T(np.copy(new_w))
sales['Commission Rate'] = sales['Sales'].apply(commission_rate) $ sales['Commission'] = round(sales['Commission Rate'] * sales['Sales'], 2) $ sales.head(20)
weather_mean.iloc[4:8, 5:]
duration_df.isnull().sum()
df_weather['HOURLYPrecip'].fillna(value=0,inplace=True) $ df_weather['HOURLYVISIBILITY'].fillna(df_weather['HOURLYVISIBILITY'].mean(),inplace=True)
order_drop.columns
merged_portfolio.reset_index(inplace=True)
df.index = df['dataset_datetime'] $ rolling_mean = df.groupby('dataset_location')['SpO2'].rolling('180s').mean().reset_index() $ test = df.groupby('dataset_location')['SpO2'].rolling_apply(tr)
dframe_team = pd.io.html.read_html(url) $ dframe_team = dframe_team[0] $ dframe_team.head()
dtypes={'date':np.str,'store_nbr':np.int64,'class':np.str,'family':np.str,'sum_unit_sales': np.float64,'no_items':np.float64,'no_perishable_items':np.float64,'items_onpromotion':np.float64} $ parse_dates=['date'] $ class_merged = pd.read_csv('class_merged.csv', dtype=dtypes,parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
data_df.index = data_df['date'] $ consumption_df = data_df.loc[:,['consumption']]
calculated_average_df.plot.bar(x=['Month',DEFAULT_NAME_COLUMN_COUNTRY], figsize=(20,7))
df = df[df['lang'] == 'en'].drop('lang', axis=1) $ df.head()
pernan = 0.80 $ nbart_allsensors = nbart_allsensors.dropna('time',  thresh = int(pernan*len(nbart_allsensors.x)*len(nbart_allsensors.y)))
so_score_10_or_more.shape
for k in ['linear','poly','rbf','sigmoid']: $     clf = svm.SVR(kernel=k) $     clf.fit(X_train, y_train) $     confidence = clf.score(X_test, y_test) $     print(k,confidence)
sc.master
linear_model_p3 = sm.OLS(df_c_merge['converted'], df_c_merge[['intercept', 'ab_page', 'US', 'UK']]) $ lin_results = linear_model_p3.fit() $ lin_results.summary()
import pickle $ with open('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.pckl', 'rb') as f: $     my_saved_gempro = pickle.load(f)
port_val.plot() $ plt.show()
excutable = '/media/sf_pysumma/a5dbd5b198c9468387f59f3fefc11e22/a5dbd5b198c9468387f59f3fefc11e22/data/contents/summa-master/bin' $ S.executable = excutable +'/summa.exe'
old_page_converted = np.random.binomial(1, p_old, n_old) $ old_page_converted.mean()
import ijson $ filename = "md_traffic.json" $ with open(filename, 'r') as f: $     objects = ijson.items(f, 'meta.view.columns.item') $
mv_stats = mv_lens.groupby('title').agg({'rating': [np.size, np.mean]}) $ rating_count = 20 $ top_movies = mv_stats['rating']['size'] >= rating_count $ mv_stats[top_movies].sort_values(by=('rating', 'mean'), ascending=False).head()
exportID['avg_distance'] = exportID['event.longSum_sum_distance']/exportID['event.longSum_total_records']
df['new_date']=df['DATE'].apply(lambda x: datetime.fromtimestamp(x).strftime("%Y-%m-%d %H:%M:%S"))
All_tweet_data_v2.rating_numerator.value_counts().sort_index()
client_credentials_manager = SpotifyClientCredentials(client_id, client_secret) $ sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)
df_concensus_uaa = df_concensus_uaa.sort_index()
counts_df_clean['tweet_id'] = counts_df_clean.tweet_id.apply(str) $
wrd_clean['rating_numerator'] = wrd_clean['rating_numerator'].astype('str').apply(lambda x: x.split('/')[0]).astype('float') $ wrd_clean['rating_denominator'] = wrd_clean['rating_denominator'].astype('str').apply(lambda x: x if x == 'nan' else x.split('/')[1]).astype('float')
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func $ from sqlalchemy import desc
prediction = naive_model.predict(df_test) $ indic = test['Id'] $ res = pd.DataFrame(indic) $ res['Sales'] = prediction $ res.to_csv('data/submission_naive.csv', index = False)
omegafft = numpy.fft.rfftfreq(len(Q1), 1) $ Gfft = y/u
df2.sample(5)
cur.close() $ con.close()
tweets.find_one()
df.get_dtype_counts()
red = [child['data'] for child in data['data']['children']] $ red = pd.DataFrame(red) $ time = pd.Timestamp.utcnow() $ red['time fetched'] = time $ red.head()
autos_pr['brand'].value_counts()
coeffs = pd.Series(model.coef_, index = X.columns)
srf.rainfall.isel(time=50).plot()
source = browser.page_source $ soup = BeautifulSoup(source, 'lxml')
prob_conrol_converted = df2[df2['group'] == 'control']['converted'].mean() $ print (prob_conrol_converted)
options_data['IMP_VOL'] = 0.0
countries.user_id.nunique(), countries.user_id.shape[0], countries.user_id.nunique() == countries.user_id.shape[0], countries.user_id.shape[0] == df2.shape[0]
from scipy.stats import norm $ z_significance = 1 - norm.cdf(z_score) $ critical_value = norm.ppf(1-(0.05)) $ critical_value, z_significance
df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=400000)
target_pf.iloc[:2]
mnnb.feature_log_prob_.shape
gdf['SiteTypeCV'].value_counts()
weather_df.shape
fullfilename_list = glob.glob(os.path.join(data_path, '*.txt')) $ filename_list = [os.path.split(f)[-1] for f in fullfilename_list if os.path.split(f)[-1] != 'vocab.txt'] $ fullfilename_list = [os.path.join(data_path, f) for f in filename_list] $ print("number of files: {}".format(len(fullfilename_list))) $ print filename_list[:10]
pd.Series(np.random.rand(5),index=['a','b','c','d','e'])
X = np.array(df.drop(['label'], 1)) $ y = np.array(df['label'])
new.describe()
(p_diffs > 0.000913).mean()
df_twitter_copy['retweet_count'] = df_twitter_copy['tweet_id'].map(df_twitter_extract_copy.set_index('tweet_id')['retweet_count']) $ df_twitter_copy['favorite_count'] = df_twitter_copy['tweet_id'].map(df_twitter_extract_copy.set_index('tweet_id')['favorite_count'])
obj4 = obj3.reindex(range(6), method='ffill')
df_final.head()
top10_categories = melted_total['Categories'].value_counts()[:10]
%matplotlib inline $ import matplotlib.pyplot as plt
graffiti = gpd.sjoin(cbg, g_geo, how='left')
from sklearn.manifold import TSNE
submit.head()
reddit_comments_data.select('link_id').distinct().count()
def reddit_scrapper(website): $     for i in range(25, max_results, 25): $         url_template = "http://www.reddit.com/?count={}&after={}".format(i, next_page) $         driver.get(url_template)
df.diff()
pipe.get_params().keys()
from sklearn.metrics import confusion_matrix
solar_wind_df = pd.read_excel(weather_folder+'/imputed_solar_wind.xlsx')
json.dumps(obj)
twitter_archive_full.drop(twitter_archive_full[(twitter_archive_full.tweet_id.isin(duplicated_list)) & $                                               (twitter_archive_full.stage != 'doggo')].index, inplace=True)
text.split(maxsplit=2, sep=" ")
pivot_discover_first = pd.melt(discover_first, $                                id_vars='email', $                                value_vars=list(discover_first.columns[-11:-1]), $                                var_name='step_no', $                                value_name= 'SKU')
sub = pd.read_csv('../input/sample_submission.csv') $ sub['Tag'] = le.inverse_transform(test_pred_svm) $ sub.to_csv('../submissions/svc.csv', index=False)
df.sort_values(by = 'C')
convert=df2.query('converted=="1"').user_id.nunique()/df["user_id"].nunique() $ convert
print("nb strong outliers : {}".format(outliers_age.is_outlier.sum()))
old_trump_tweets[0].keys()
data_df.groupby('nwords')['ticket_id'].nunique()
cell_df.cell_name[0]
data[data['authorName'] == 'Lunulls A. Lima Silva']#['link_weight']#.loc[3]
pickle.dump(lsa_tfidf, open('iteration1_files/epoch3/lsa_tfidf.pkl', 'wb'))
top_lev = summary.groupby('week_id').head(n).sort_values(by=['week_id', 'leverage_ratio'], ascending=[True, False])
sandwich_train_model.print_topics(num_topics = 10 ,num_words = 10)
rng.to_period('D')
import re $ pattern = re.compile('AA') $ the_str = 'AAbc'
control_convert = df2[df2['group']== 'control'].converted.mean() $ print(control_convert)
(p_diffs >actual_diff).mean()
s = w.get_subset_object(subset_uuid).indicator_objects['din_winter'] $ s.get_filtered_data(subset = subset_uuid, step = 'step_2')
ideas.drop(['Authors', 'Link', 'Tickers', 'Title', 'Strategy'],axis=1,inplace=True) $ ideas = ideas.applymap(to_datetime)
old_page_converted = np.random.choice([0, 1], size=df_o.shape[0], p=[(1-p_old), p_old])
bigdf.loc[bigdf['comment_body'].str.contains('\r\r')]
sent.columns = ['news_compound', 'news_neg', 'news_neu', 'news_pos'] $ stock = pd.merge(stock, sent, how='left', left_index=True, right_index=True)
df[['VOL','FLOW','IC','SF','ACQ','REVENUE']].describe()
full_clean_df.to_csv('twitter_archive_master.csv', index=False)
df.head()
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])
t.index = [101,102,103,104,105] $ t
tip_sample.head
membership_report = membership.groupby('new_date').sum().reset_index()[['new_date','Price']]
from scipy import stats $ instance.initialize(parameters)
cluster_pd['prediction'].plot(kind='hist', xticks=[0,1,2], color='g', alpha=0.3)
unsorted_df.sort_index(axis=1)
chinadata.tail()
model_guid = client.repository.get_model_uid(saved_model_details) $ print("Saved model guid: " + model_guid)
retweet_and_count = sns.factorplot(data=tweets_df, x="name", y="retweet_count", kind="box") $ retweet_and_count.set(yscale="log") $ plt.xticks(rotation=60)     # alwatan have above average number of retweets and alseyassah have below average number of retweets $
def var(scenarios, level=99, neutral_scenario=0): $     pnls = scenarios - neutral_scenario $     return - np.percentile(pnls, 100-level, interpolation='linear')
keywords_dataframe = pandas.DataFrame(Counter(keywords).most_common(40), columns=['keyword', 'count'])
df2.query('converted == 1').user_id.nunique() / df2.user_id.nunique()
df=pd.DataFrame([['frank','M',29],['mary','F',23],['tom','M',35],['ted','M',33],['jean','F',21],['lisa','F',20]]) $ df
Data.tail()
volume_m.to_csv('output/volume_monthly.csv', na_rep='NaN')
titanic.fare.describe()
for i in ['Updated Shipped diff']: $     t = df[i].hist(bins=500) $     t.set_xlim((0,500)) $     t.set_ylim((0,2000))
results.summary2()
(visit_num.reset_index(level=[0, 1], drop=True) $  .to_frame() $  .rename( $     columns={'inspection_date': 'num_fails'} $ ))
df = pd.read_csv('data/311_sf.csv') $ df.info()
porn_df.to_csv('data/porn_users.csv',index=False, encoding='utf-8-sig')
sqlContext.sql("select * from df_example3").toPandas()
test_post = test_post.replace('.','. ') $ print(len(sent_tokenize(test_post)), end='\n\n') # number of sentences $ for sent in sent_tokenize(test_post): $     print(sent, end='\n\n')
dfData.count()
X_train = pd.DataFrame(im.transform(X_train),columns = X_train.columns)
dog_ratings.to_csv('twitter_archive_master.csv', index=False, encoding = 'utf-8')
df_unique_providers.tail()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=16000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
data.dropna(axis = 0).shape
n_old = df2[df2['landing_page']=='old_page'].landing_page.count() $ n_old
segments = pd.read_csv("transit_segments.csv", parse_dates=['st_time', 'end_time']) $
model_rf_20 = RandomForestClassifier(min_samples_leaf=20, random_state=42) $ model_rf_20.fit(x_train,y_train) $ print("Train: ", model_rf_20.score(x_train,y_train)*100) $ print("Test: ", model_rf_20.score(x_test,y_test)*100) $ print("Differnce between train and test: ", model_rf_20.score(x_train,y_train)*100-model_rf_20.score(x_test,y_test)*100)
intervention_train.sort_index(inplace=True) $ intervention_history.sort_index(inplace=True)
treatment_df = df2[df2['group']=='treatment'] $ n_treatment = treatment_df.shape[0] $ 1.0*sum(treatment_df['converted']==1)/ n_treatment
df2['intercept'] = 1 $ df2[['ab_page', 'old_page' ]] = pd.get_dummies(df2['landing_page']) $
top_brand_mean_price = {} $ for b in common_brands: $     top_brand_mean_price[b] = int(autos['price'][autos['brand'] == b].mean()) $ dict(sorted(top_brand_mean_price.items(), key=lambda x: x[1], reverse=True))
top50_city[(top50_city.user_location == "Sweden")| (top50_city.user_location == "New York, NY")] $
def businessyear(x, y): $     Month = x.dt.month
df_archive_clean = pd.merge(df_archive_clean,df_image_clean,how = "left", on = "tweet_id")
tuned_forest_model = grid_search.best_estimator_
df_joined[['UK', 'US', 'CA']] = pd.get_dummies(df_joined['country']) $ log_mod_2 = sm.Logit(df_joined['converted'], df_joined[['intercept', 'US', 'CA']]) $ results = log_mod_2.fit() $ results.summary()
traffic_df = df_traffic.dropna() $ traffic_df_byday = traffic_df.loc[traffic_df.index.weekday==weekday] $ traffic_df_byday.info() $ traffic_df_byday.head()
w.get_available_indicators(subset= 'A', step=2)
pd.Period('2012-05', freq='A')
regr.score(X, y)  # when we fit all of the data points
len(para), len(tags), len(bullets), len(images), len(links), data_scrapped.shape
df2=df2.append(df.query('group=="treatment"').query('landing_page=="new_page"'));
df_clean.info()
p_new = (df2['converted']).mean() $ print(p_new)
userMovies = moviesWithGenres_df[moviesWithGenres_df['movieId'].isin(inputMovies['movieId'].tolist())] $ userMovies
eval_data['tavg'] = 2. * eval_data['tavg_norm'] * weather_features['tavg'].std() + weather_features['tavg'].mean() 
xgb_learner = XGBLearner(dtrain, evals, True, 0.01, 'binary:logistic', 'auc', seed=7)
df_not_categorical = df_EMR.select_dtypes(include=['float64', 'datetime64', 'int64']) $ df_not_categorical.shape $ df_not_categorical.head()
collection_reference.count_documents({'geo' : { '$ne' : None}})
users.head()
clean_users[clean_users['active']==1]['creation_source'].value_counts()
top_supports = support.groupby(["contributor_firstname", "contributor_lastname"]).amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
fwd = sess.get_data(['gbp curncy','hkd curncy','eur curncy'],'fwd curve') $ fwd
Station = Base.classes.measurement
resultValues = read.getResultValues(resultids=resultIDList) $ resultValues.head()
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted==1 ").converted.count() $ convert_new = df2.query("landing_page == 'new_page' and converted==1 ").converted.count() $ n_old = df2.query("landing_page == 'old_page'").converted.count() $ n_new = df2.query("landing_page == 'new_page'").converted.count()
print('Method 1: ', '\n') $ for i in set(csv_df['names']): $     print(i, ": ", csv_df['platform'][(csv_df['names']==i) & (csv_df['platform']=='Facebook')].value_counts(), '\n') $ print('Method 2 ', '\n') $ csv_df.groupby(['names', 'platform'])['date'].count()
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 8 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
titanic.embarked.value_counts(sort=False).plot(kind='bar')
print('Number of unique users in dataset {}.'.format(df.user_id.nunique()))
clean_train_df.Q1_mood_of_speaker.value_counts()
op_add_comms = subs_and_comments[subs_and_comments['op_comment']].groupby('id')['body'].apply(lambda x: ' '.join(x)).reset_index().reset_index()
ticks.shift(-1).head()
essential_genes.head()
idx_high = data['dataset_data']['column_names'].index('High') $ idx_low = data['dataset_data']['column_names'].index('Low') $ change = [day[idx_high]-day[idx_low] for day in data['dataset_data']['data']] $ print('The largest change in any one day in 2017 was {:.2f}'.format(max(change)))
trip_arrive = dt.date(2018, 4, 1) $ trip_leave = dt.date(2018, 4, 15) $ last_year = dt.timedelta(days=365) $ temp_avg_lst_year = (calc_temps((trip_arrive-last_year), (trip_leave-last_year))) $ print(temp_avg_lst_year)
train_groupped = pd.DataFrame(train.groupby(by = ['Page', 'date']).agg({'Visits' : np.mean})) $ test_groupped = pd.DataFrame(test.groupby(by = ['Page', 'date']).agg({'Visits' : np.mean})) 
logit_mod = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'CA', 'UK']]) $ results = logit_mod.fit() $ results.summary()
df.plot(kind='bar')
df["ZIPCODE"] = df.index
model_bigram.most_similar(positive=['foundation','electrical'], topn=5)
tweets[tweets['retweeted'] == 1]['chars'].describe()
df2.query('landing_page=="new_page"').user_id.count()/df2.shape[0]
grouped = s4g.groupby('Symbol') $ type(grouped.groups) $ grouped.groups
chk = joined.loc[joined['nat_event']==1].head() $ holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
print(data.columns.values) $ print('-'*40) $ data.drop(odds_W, axis=1, inplace=True) $ data.drop(odds_L, axis=1, inplace=True) $ print(data.columns.values)
b.find_by_xpath('//*[@id="day-section"]/div/div[3]/div[8]/div[2]/ul/li[4]/button').click()
s519397_df["prcp"].describe()
plt.savefig('aapl.png') $ plt.savefig('aapl.jpg') $ plt.savefig('aapl.pdf') $ plt.show()
s.reset_index()
df_tick_clsfd_sent = df_tick.join(df_amznnews_clsfd_2tick) $ df_tick_clsfd_sent.info()
df_tweets2['bot_status'] = np.random.uniform(0,1, size=len(df_tweets2)) $ df_tweets2['bot_ori_status'] = np.random.uniform(0,1, size=len(df_tweets2))
df_h1b_nyc[df_h1b_nyc.pw_1.isnull()].pw_1
tweet_image_predictions_clean.info()
simdata=(closes.loc['2016-12-30',:].AAPL)*np.exp(simret.cumsum()) $ simdata
csgo_profiles.fillna(0, inplace=True)
df.rename( columns={'year.1':'year1', 'year.2':'year2'}, inplace=True) $ df.head(2)
to_be_predicted_Day1 = 35.82 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
iso_gdf.plot();
counts_compare = df1.join(df2)
test_data.tail()
plot_compare_generator(['mention_count', 'hashtag_count'],"Comparing the two counts" ,DummyDataframe.index, DummyDataframe, intervalValue= 1, saveImage=True, fileName="compare")
tweets['created'] = pd.to_datetime(tweets['created'])
print(type(plan["plan"])) $ print(plan['plan'].keys())
airbnb_df = airbnb_df[airbnb_df['city']=='Seattle'] $ airbnb_df['city'].value_counts(dropna=False)
list(soup.ul.children)
collection_reference.count_documents({})
test['visitors'] = 0.2*preds1+0.2*preds2+0.3*preds3+0.1*preds4+0.2*preds5 $ test['visitors'] = np.expm1(test['visitors']).clip(lower=0.) $ sub1 = test[['id','visitors']].copy()
summer[summer['Complaint Type'] == 'Homeless Encampment']['Unique Key'].count()
new_page_converted = np.random.binomial(1, p_new,n_new) $ p_new_sample = new_page_converted.mean() $ p_new_sample $
df_2011['bank_name'] = df_2011.bank_name.str.split(",").str[0] $
sub_df.head()
grouped = mergedLocationDF.groupby('OA11CD').agg({'count': np.sum}).reset_index() $ grouped.head()
r = kimanalysis.rawquery('obj', query={"type":'tr'}, limit=1)
prediction['p1'].value_counts()[:10]
temps_df.iloc[[1,3,5]]['Difference']
for i in cols: $     print("Average length in {} is {} words".format(i,str(round(train[i].str.len().mean()))))
df.head()
SMOOTH.plot_init_latency(smoothresult,option="difference")
nt["catfathername"].unique()
df_c_merge['intercept'] = 1 $ lm = sm.Logit(df_c_merge['converted'], df_c_merge[['intercept', 'US', 'CA']]) $ results_countries = lm.fit() $ results_countries.summary()
df = df[df['PView'] > 0] $ df = df[df['ViewCount'] > 0] $ df['Views-Difference'] = df['ViewCount'] - df['PView'] $ df['Views-PercentChange'] = df['Views-Difference'] / df['PView'].astype(np.float) $ df.dropna()
def Describe_Data(dataframe): $     return dataframe.describe()
df2 = df.fillna(0) $ df2
df2[df2['converted'] == 1].count()[0]/df2.shape[0]
df.query('salary>30000 & year==2017')
df.sort_index(axis=0, ascending=False)  # Sort by index
counts_df_clean.head()
filtered.shape
odo_counts = odometer.value_counts() $ print(odo_counts) $ odo_counts.sort_index(ascending=False) $
merged=pd.merge(merged,oil,on='date', how='left') $ print("Rows and columns:",merged.shape)
df.corr()['ratio_win_home_team'].abs().sort_values(ascending = False)
plt.figure(figsize=(16,8)) $ dendrogram(features['cnn']['linkage'], orientation='top', $           p=300, truncate_mode='lastp', no_labels=True, color_threshold=0) $ plt.axes().get_yaxis().set_visible(False) $ plt.show()
def split_and_add_to_set(column_values, unique_structure, split_character): $     for val in column_values.split(split_character): $         unique_structure.add(val) $ def clean_value(val): $     return val.replace('/', '_').replace(' ', '_').lower()
lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
Ralston = weather.loc[weather.NAME == 'RALSTON RESERVOIR, CO US'] $ Ralston.head() $
print(autos.info()) $ print(autos.head())
e_p_b_one.index = e_p_b_one.TimeCreate $ del e_p_b_one['TimeCreate'] $
feature_imp= feature_imp.sort_values('XGB_imp',ascending=False)
now_start = datetime.datetime.now() $ time_start = now_start.strftime("%Y-%m-%d (yyyy-mm-dd); %H:%M hrs.") $ print "# Starting time of computing: %s"%time_start
autos.describe(include="all")
bus.head(3)
df_ab_raw.info()
pd.DataFrame(random_integers, index=[3, 2, 0])
prs = pd.read_csv("prs.csv") $ pr_comments = pd.read_csv("pr_comments.csv") $ issues = pd.read_csv("issues.csv") $ issue_comments = pd.read_csv("issue_comments.csv")
df.to_csv("0016_clean.csv")
doglist = doglist.fillna(50)
dr_existing_8_to_16wk_arimax = dr_existing_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']] $ dr_existing_8_to_16wk_arimax.index = dr_existing_8_to_16wk_arimax.index.date
autos['date_crawled'] = autos['date_crawled'].str[:10].str.replace('-', '').astype(int) $ autos['date_crawled'].head()
scoreCts = ins2016.groupby('score').count().reset_index() $ plt.bar(scoreCts['score'],scoreCts['type']) $ plt.show() $
dfSPY=pd.read_csv("data/SPY.csv") $ print(dfSPY.head(1))
precipitation_2yearsago = session.query(Measurement.prcp, Measurement.date).filter(Measurement.date > query_date_2yearsago).order_by(Measurement.date.desc()).all() $ print(precipitation_2yearsago)
most_common_registered_addresses_slps = active_companies[active_companies['company_type'] == 'Limited Partnership for Scotland'].groupby(['first_and_postcode'])['CompanyNumber']\ $     .agg(lambda x: len(x.unique())).sort_values(ascending=False).head(20) $ most_common_registered_addresses_slps.to_excel('data/for_further_investigation/top_20_slp_addresses.xlsx') $ most_common_registered_addresses_slps
n_old = df_control.user_id.nunique() $ n_old
rng.tz_localize('Etc/GMT-1')
df0 = df.query('group == "treatment" and landing_page == "new_page"') $ df1 = df.query('group == "control" and landing_page == "old_page"') $ df2 = df0.append(df1, ignore_index = True)
print(train_trees[random.randrange(len(train_trees))])
print "modelType: " + saved_model.meta.prop("modelType") $ print "creationTime: " + str(saved_model.meta.prop("creationTime")) $ print "modelVersionHref: " + saved_model.meta.prop("modelVersionHref") $ print "label: " + saved_model.meta.prop("label")
p.crs
df.index[7]
sum(image_predictions.tweet_id.duplicated())
learner.load('lm_last_ft')
images = pd.read_csv('image-predictions.tsv', sep ='\t')
n_old = df2[df2['group']=='control'].user_id.count() $ n_old
df2 = pd.DataFrame(data)
df2.drop(labels=['control'], axis=1,inplace=True) $ df2.head()
serc_plot = plt.imshow(b56,extent=serc_ext,cmap='Greys',clim=(0,0.4)) $ plt.title('SERC Band 56 Reflectance');
my_query = "SELECT * FROM donors LIMIT 10" $ my_result = limsquery(my_query) $ first_element = my_result[0] $ print first_element.keys()
engine = create_engine('mysql+mysqlconnector://root:{}@localhost:3306/github'.format(password), echo=False)
lda_tf2.save(os.path.join(outputs, 'model_tf2.lda')) $ corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tf2.mm'), corpus_lda_tf2)
oil_pandas = oil_pandas.fillna(method='bfill') $ oil_pandas = oil_pandas.fillna(method='ffill') $ oil_pandas.head()
import pandas as pd, numpy as np $ import matplotlib.pyplot as plt $ periods=60 $ dates = pd.date_range('20130101', periods=periods) $ dates
df_int.plot() $ plt.show()
dictionary = corpora.dictionary.Dictionary.load(os.path.join(outputs, 'reviews.dict'))
h = np.where(scores.values >=4, 'high','no') $ h
trd_clean = [] $ for t in trd: $     if not t is None: $         trd_clean.append(t) $
from sklearn.cross_validation import cross_val_score
df.tail(5)
df_old = df2.query('landing_page == "old_page"') $ p_old = df2['converted'].mean() $ print(p_old)
df3=pd.read_csv('countries.csv') $ print(df3['country'].unique())
ggplot(err,aes(x="eyetracker",y="avg"))+geom_jitter()+geom_hline(yintercept=[0.5,1.5])#+facet_wrap("~subject")
idx = data['dataset_data']['column_names'].index('Close') $ closing = [day[idx] for day in data['dataset_data']['data']] $ change_two_days = [abs(closing[index]-closing[index-1]) for index, value in enumerate(closing) if index > 0] $ print('The largest chagne between any two days in 2017 was {:.2f}'.format(max(change_two_days)))
import pymysql $ conn = pymysql.connect(host='localhost', port=3306, user='root', password='pythonetl', db='pythonetl') $ cur = conn.cursor()
date = '2017-06-01' $ browser.find_element_by_id('cal-date-select-dev').clear() $ browser.find_element_by_id('cal-date-select-dev').send_keys(date) $ browser.find_element_by_xpath('//td//input[contains(@class, "site-button-small-dev submit")]').click()
allqueryDF['domain'].unique()
print('source embedding shape on training set: ', train_source_emb.shape) $ print('target embedding shape on training set: ', train_target_emb.shape)
result = result.rename(columns={'Price':'Revenue'})
(taxiData2.Tip_amount < 0).any() # This Returns True, meaning there are values that are negative
preds = gcv.best_estimator_.predict(fb_test.message)
print(DataSet_sorted['tweetText'].iloc[4])
discConvpct.head()
workclass = pd.read_sql(q, connection) $ workclass.head(10)
trees = model_outer.get_params()['n_estimators'] $ lr = model_outer.get_params()['learning_rate'] $ depth = model_outer.get_params()['max_depth'] $ clf = sk.ensemble.GradientBoostingClassifier(learning_rate=lr, n_estimators=trees, max_depth=depth).fit(X_train, y_train)
jpl_link = links[1].strip('/') $ print(jpl_link)
from textblob import TextBlob $ from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer()
tweet_json.iloc[0]
df_usertran = pd.merge(users,transactions,how='left',on='UserID') $ df_ = df_usertran.drop_duplicates(subset='UserID') $ df_ = df_.reset_index(drop=True) $ df_ $
df.shape
test_df.dtypes
def txt_pragraphs(str): $     pragraphs = str.split("\n\n") $     return pragraphs
grouped = data[['float_time','Agency']].groupby('Agency') $ grouped.mean().sort_values('float_time',ascending=False)
artist = list() $ for x, i in enumerate(unprocessed_list): $     if (unprocessed_list[x][1]): $         artist.append(unprocessed_list[x][1])
main = pd.merge(pd.merge(main, ma, on=['game_id', 'season', 'team_id_ta'], how='left'), $                 mb, on=['game_id', 'season', 'team_id_tb'], how='left') $ for i in ['pts_la', 'ast_la', 'blk_la', 'reb_la', 'stl_la', 'pts_lb', 'ast_lb', 'blk_lb', 'reb_lb', 'stl_lb']: $     main[i] = main[i].fillna(0)
df_326 = pd.read_sql(sql_326,conn_laurel) $ df_326.groupby(['accepted','paid','preview_clicked','preview_watched','preview_finished'])['applicant_id'].count().unstack().fillna(0)
from nltk.corpus import stopwords $ stop_words = stopwords.words(['english','danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']) $ stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0] $ googletrend['State'] = googletrend.file.str.split('_', expand=True)[2] $ googletrend.loc[googletrend.State=='NI', "State"] = 'HB,NI'
sc.stop()
my_gempro.uniprot_mapping_and_metadata(model_gene_source='TUBERCULIST_ID') $ print('Missing UniProt mapping: ', my_gempro.missing_uniprot_mapping) $ my_gempro.df_uniprot_metadata.head()
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan,'CABA', 'dog', 'cat']) $ s
TH=0.45 $ results=topUserItemDocs[topUserItemDocs['score']>TH].groupby(['item_id']).apply(lambda g: [x[0] for x in sorted(zip(g['user_id'],g['score_weight']),key=lambda t: -t[1])[0:100]]) $ print results.shape $ print results.head()
states.set_index([['i', 'ii', 'iii', 'iv', 'v', 'vi'], 'day'])
df['intercept']=1 $ df['ab_page'] = pd.get_dummies(df['group'])['treatment']
actual_value_second_measure.status_binary.unique()
biology.plot()
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, func
new_page_converted = df2.sample(n_new, replace = True)
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3]) $ ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6]) $ pd.concat([ser1, ser2])
DATA_DIR = "./data" $ cama_url = "https://opendata.arcgis.com/datasets/d6c70978daa8461992658b69dccb3dbf_24.csv" $ cama_file = os.path.join(DATA_DIR, "cama-condo.csv")
data = {'Bob' : pd.Series([245, 25, 55]), $         'Alice' : pd.Series([40, 110, 500, 45])} $ df = pd.DataFrame(data) $ df
df_arch_clean['rating_value'].head() $
twitter_archive_master.rating_numerator.plot(kind='hist');
from pyspark.sql import SparkSession, SQLContext, Row $ spark = SparkSession \ $     .builder \ $     .appName("Spark SQL") \ $     .getOrCreate()
td_by_date = niners.groupby('Date')['Touchdown'].sum() $ td_by_date;
df.duration.describe()
import lightgbm as lgb $ mdl =lgb.LGBMClassifier(boosting_type ='gbdt',objective ='binary', num_leaves =80, learning_rate =0.1) $ mdl.fit(X_train,y_train)
ins.describe()
from scipy.stats import norm $ z_signift = norm.cdf(z_score) $ CV_95CI = norm.ppf(1 - 0.05) $ z_signift, CV_95CI $
station_distance.insert(loc=1, column='Trip Duration(Minutes)', value=tripduration_minutes)
results_3 = pd.DataFrame(list(results3), columns=['tags', 'values'])#, index=[1,2,3,4]) $ results_3.head(20)
dc['YearWeek'] = dc['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week)) $ tm['YearWeek'] = tm['created_at'].apply(lambda x: "%d/%d" % (x.year, x.week))
!head ../../data/msft2.csv  # Linux
all_cards = all_cards.loc[~(all_cards.printings.map(lambda x: bool(set(invalid_sets) & set(x))) & all_cards.supertypes.map(lambda x: x != ["Basic"]))]
prediction=model_arima121.predict()
import glob $ filenames = {data_dir[:data_dir.find('_')]: glob.glob('../data/ebola/{0}/*.csv'.format(data_dir)) for data_dir in ebola_dirs[1:]}
stations_info = session.query(Station.name, Station.station ).all() $ stations_info = pd.DataFrame(stations_info) $ stations_info.head()
autos["price"].unique().shape
regression = sm.Logit(df['converted'],df[['intercept','treatment']])
van15_fin.describe()
data_scraped.head(3)
RFC_grid.best_params_
ave_ratings_over_time.plot()
p_old = df2['converted'].mean() $ print(p_old)
df.head()
predictions = [model.predict(start=start_date, end=end_date, dynamic=True) for model in models]
svm_clf = SVC(kernel="linear") $ X = X_train $ y = y_train $ svm_clf.fit(X, y)
lda.components_.shape
for topic_idx, topic in enumerate(nmf.components_): $     print "Topic %d:" % (topic_idx) $     print " ".join([tfidf_feats[i] $                     for i in topic.argsort()[:-10 - 1:-1]])
df.shape
bListenCount = sc.broadcast(trainData.map(lambda r: (r[1], r[2])).reduceByKey(lambda a,b: a+b).collectAsMap()) $ def predictMostListened(allData): $     return allData.map(lambda r: Rating(r[0], r[1], bListenCount.value.get(r[1] , 0.0))) $
autos['ad_created'] = autos['ad_created'].str[:10] $ ad_created_count_norm = autos['ad_created'].value_counts(normalize=True, dropna=False) $ ad_created_count_norm.sort_index() $
ltc.info() $ eth.info() $ xrp.info()
print("Bayesian odds against Bob winning: %i to 1" %((1. - bayes_prob) / bayes_prob))
a = dat['community area'].unique() $ a.sort() $ a
TX_profit.head()
old_page_converted = np.random.binomial(nold, pold)
diff_real = 0.118808 - 0.120386  $ p_diffs = np.array(p_diffs) $ (p_diffs > diff_real).mean()
df['rating_numerator'].mode()
rolled_returns = (prices.pct_change() * weights).sum(1)
hr["chart delta"] = \ $ hr.apply(lambda x: (x["charttime"] - $                         x["realtime"]).total_seconds(), axis=1) $ hr.head()
df = pd.read_sql('SELECT a.address_id, a.city_id, c.city FROM address a LEFT JOIN city c ON a.city_id = c.city_id ORDER BY a.address_id DESC;', con=conn) $ df
jobs.loc[(jobs.FAIRSHARE == 2) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'davis_lab')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
plt.plot(f_c_dow.num_fire_incids);
mappingfile = ogh.treatgeoself(shapefile=shapefile, NAmer=NAmer, folder_path=os.getcwd(), outfilename='monkeysonatree.csv', buffer_distance=0.00) $ print(mappingfile)
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2011 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2011 Percentiles.txt"
template_df = pd.concat([X_valid, y_valid], axis=1) $ template_df['is_test'] = np.repeat(True, template_df.shape[0])
data_df.clean_desc[15]
train.head()
sns.set_style("darkgrid") $ plt.figure(figsize=(8, 4)) $ maint['comp'].value_counts().plot(kind='bar') $ plt.ylabel('Count')
y_test_under[fm_bet_under].mean()
precipitation_df.isnull().sum()
from scipy.stats import norm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print('z_score', z_score) $ print('norm.cdf', norm.cdf(z_score)) $ print('norm.ppf', norm.ppf(1-(0.05/2)))
df = pd.merge(train,user_logs, on = 'msno', how = 'left') $ df.sort_values(by = ['msno','date'], inplace = True)
df2["converted"].mean() * 100
contractor_clean.loc[contractor_clean['contractor_id'].isin([382,383,384,385,386,387]), $                      'contractor_bus_name'] ='Cahaba Government Benefit Administrators, LLC'
bus.groupby("business_id").count() $
pd.options.display.max_rows = 6 $ all_coins_df = my_cryptory.extract_bitinfocharts("btc") $ all_coins_df
active_marketing = clean_users[clean_users['active']==1]['enabled_for_marketing_drip'].sum()/active_count $ inactive_marketing = clean_users[clean_users['active']==0]['enabled_for_marketing_drip'].sum()/inactive_count_with_na
results1.summary()
records4 = records.copy()
tweet_archive_enhanced.head()
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all"
pd_data.paidTime = pd_data.paidTime.map(lambda x: pd.to_datetime( $     x, unit='ms', utc=True).astimezone('Asia/Shanghai').strftime('%Y-%m-%d %H:%M:%S') if x else '')
training_data,holdout = train_test_split(lq2015_combined,test_size=0.10,random_state=123)
p_diffs = np.random.binomial(df2.query('landing_page == "new_page"').shape[0], p, 10000)/df2.query('landing_page == "new_page"').shape[0] - np.random.binomial(df2.query('landing_page == "old_page"').shape[0], p, 10000)/df2.query('landing_page == "old_page"').shape[0]
df_tweets2.to_csv("export_dummy.csv", sep=',', encoding='utf-8')
np.random.seed(1) $ model = models.LdaMulticore(corpus, id2word=dictionary, num_topics=10, workers=5,passes=200, eval_every = 1)
sorted(avg_price_by_brand.items(), key=lambda x:x[1], reverse=True)
np.std(p_diffs)
testObjDocs.buildOutDF(tst_lat_lon_df)
SVC = SklearnClassifier(SVC()) $ SVC.train(train_set)
n_old=df[df['group']=='control'].shape[0] $ n_old
ds.tail()
print("Probability an individual recieved new page is", $       df2['landing_page'].value_counts()[0]/len(df2))
X_train_sc = ss.fit_transform(X_train) $ X_test_sc = ss.transform(X_test)
n_converted = df.query('converted == 1').user_id.nunique() $ p_converted = n_converted / n_users $ p_converted
merged_portfolio_sp_latest_YTD_sp = pd.merge(merged_portfolio_sp_latest_YTD, sp_500_adj_close_start $                                              , left_on='Start of Year', right_on='Date') $ merged_portfolio_sp_latest_YTD_sp.head()
pumashplc.to_file(os.getenv('PUIDATA')+'/pumashplc.shp')
new_crs = np.array(new_crs)
to_be_predicted_Day4 = 48.73986814 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
X = test_df.drop(['Log_Price_Sqft','ZIP CODE'], axis=1)
nypd_unspecified = data[(data['Borough']=='Unspecified') & (data['Agency']=="NYPD")]['Borough'].count() $ nypd_unspecified
df_countries.country.nunique()
df_local_website.sample(3, random_state=303)
print("\nThe types of weather events in the 'events' column are:") $ evnts = [str(x) for x in df_weather.events.unique()] $ print("".join([str(i+1) + ". " + evnts[i] + "\n" for i in range(len(evnts))]))
df = df[df.PRICE.notnull()]
print(metrics.classification_report(testy, predict_y))
means1_table = avg1_table.rename(columns = {'Units Sold':'UnitsSold'}) $ means2_table = means1_table.groupby(['Product', 'Country']).UnitsSold.mean() $ means2_table
len(soup.find_all('div', class_="content_title"))
browser = webdriver.Chrome('chromedriver.exe') $ browser.get(url) $ time.sleep(5) $ html = browser.page_source $ soup = BeautifulSoup(html, "html.parser") $
df_pol.pol_id.value_counts()
subway2_df[['Hourly_Entries', 'Hourly_Exits']].sample(100) #this is what clued me in to the error in how the function was parsing ridership.
conn.fileinfo('data', caslib='casuser')
element = driver.find_element_by_xpath('//*[@id="comic"]/img') $ element.get_attribute("title")
from scipy.stats import linregress
train.
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'
plt.scatter(X2[:, 0], X2[:,1], c=labels, cmap='rainbow') $ plt.colorbar()
session['action_detail'].value_counts().sort_values(ascending=False).head(20)
posts = got_data.head(10)[['id', 'message']] $ posts['link'] = 'https://facebook.com/'+posts['id'] $ for post in posts['link']: $     print(post)
openmoc_geometry = get_openmoc_geometry(sp.summary.geometry)
engine.execute("SELECT COLUMN_NAME FROM information_schema.COLUMNS WHERE TABLE_NAME =  'contractor';").fetchall()
df = ayush['Event Type Name'].value_counts() $ df_ayush = pd.Series.to_frame(df) $ df_ayush.columns = ['Count_Ayush'] $ df_ayush
p_old = df2['converted'].mean() $ (p_old)
Z = np.random.randint(0,10,50) $ print(np.bincount(Z).argmax())
one_station.sort_values(by=['DATE'], inplace=True)
joined[['Frequency_score']] = joined[['Frequency_score']].astype(int) $ joined.dtypes.filter(items=['Frequency_score'])
deep_learning_tweet1 = api.get_status(str(deep_learning_tweet_id)) $ type(deep_learning_tweet1)
notes = pd.read_csv("Z:/notes v2/cohort1_deid_df.csv")
df3 = df2[df2['user_id'].duplicated()] $ print('The one user_id that is repeated is:  ' + str(df3.iloc[0][0])) $
taxi_weather_df.shape
stocks_happiness = stocks_happiness.reset_index(drop=True)
rdf.loc[pd.isna(rdf.accident_counts),'accident_counts'] = 0.0
weather.events.unique()
len(train_data[train_data.gearbox == 'manuell'])
guineaFullDf.index.get_level_values(0).value_counts()
len(df[df.gender=="f"])/float(len(df[df.gender=="f"])+len(df[df.gender=="m"]))
%bash $ gsutil cat "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" | head
coarse_fuel_mgxs = coarse_mgxs_lib.get_mgxs(fuel_cell, 'nu-fission') $ coarse_fuel_mgxs.get_pandas_dataframe()
resDir  = "results/basic weekly move predict quaterly train results.csv" $
geocoded_evictions_df = eviction_df.join(geocoded_addresses_df.drop(['Defendant.Addr.Line.1', 'index'], axis=1).set_index('Case.Number')) $ geocoded_evictions_df.to_csv('geocoded_evictions.csv', encoding='utf-8') $ geocoded_evictions_df.head()
x,y=X[0:train],Y[0:train] $ print (x.shape,y.shape) $ model.fit(x,y,epochs=150,batch_size=10,shuffle=True)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
data_sets = {} $ data_sets['1min'] = pd.read_pickle('final_1min.pickle') $ data_sets['3min'] = pd.read_pickle('final_3min.pickle') $ data_sets['15min'] = pd.read_pickle('final_15min.pickle') $ data_sets['60min'] = pd.read_pickle('final_60min.pickle')
req_3d_df = filtered_df['l_req_3d'] $ pay_3d_df = filtered_df['M_pay_3d'].fillna(0) $ pay_13d_df = filtered_df['M_pay_13_3d'].fillna(0) $ price_usd = filtered_df['fst_price_per_hour_USD']
df1['DATETIME'] = df1['DATE'] + ' ' + df1['TIME'] $ df1['DATETIME'] = pd.to_datetime(df1['DATETIME']) $ df1.sample(3)
df_train['a'] = list(map(lambda x: 1 if x == value else 0, df_train.DAYS_EMPLOYED)) $
materials_list = ("EVA", "PMMC") $ cur.execute('SELECT alpha, beta FROM materials WHERE material_id IN (?, ?)', materials_list) $ [(mat, cur.fetchone()) for mat in materials_list]  # use the cursor fetchone() method to get next item
df_train = pd.merge(train, store, left_on='Store', right_on='Store') $ df_test = pd.merge(test, store, left_on='Store', right_on='Store')
crime.head()
df_archive_clean.info()
!wget http://files.fast.ai/data/aclImdb.tgz --directory-prefix=data
top20_brands_mean_price = pd.DataFrame(top20_brands_mean_price,columns = ['mean_price'])
pd.read_sql(f'explain {sql}', engine).head()
archive_df_clean.sample(10)
vect = TfidfVectorizer(min_df=10, ngram_range=(1, 2), sublinear_tf=True, norm="l2") $ print(df_cat_stat.blurb.shape) $ blurbs_to_vect = vect.fit_transform(df_cat_stat.blurb) $ print(blurbs_to_vect.shape)
import pandas as pd $ dfs = pd.read_html('https://en.wikipedia.org/wiki/Timeline_of_programming_languages', header=0) $ df = pd.concat(dfs[4:12]) $ prog_lang = df[df.Year != 'Year']
labels = pd.read_csv('response5.csv',header =None )
titanic_dummy_df = pd.get_dummies(titanic_df, columns=['pclass', 'sex', 'embarked','cabin_floor'])
mismatch_group1 = df.query("group == 'treatment' and landing_page == 'old_page'") $ mismatch_groupp2 = df.query("group == 'control' and landing_page == 'new_page'") $ len(mismatch_group1) + len(mismatch_groupp2)
cols_to_export = ["epoch","src","trg","src_str","src_screen_str","trg_str","trg_screen_str","lang","text"] $ mentions_df.to_csv("/mnt/idms/fberes/network/usopen/data/uso17_mentions_with_names_and_text.csv",columns=cols_to_export,sep="|",index=False)
def f(x): $     y = (x-1.5)**2 + 0.5 $     print(x,y) $     return y
pings.take(2)
logreg = LogisticRegression(penalty='l2', C=1.0) $ logreg.fit(X, y) $ new_pred_prob = logreg.predict_proba(X_new)[:, 1] $ new_pred_prob
def get_list_of_comments(the_posts): $     list_of_comments = [] $     for i in list_Media_ID: $         list_of_comments.append(the_posts[i]["comm_texts"]) $     return list_of_comments   
data["Visit Type"].value_counts()
df_times['Time of daily highs'].value_counts().idxmax()
autos.registration_year.describe()
indeed[indeed.duplicated(subset=['company','title'], keep=False)]
tw.query('rating_denominator != 10')
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page']=='old_page')&(df2['converted']==1)]) $ convert_new = len(df2[(df2['landing_page']=='new_page')&(df2['converted']==1)]) 
df.info()
ds_complete_temp_CTD_1988['date'] = ds_complete_temp_CTD_1988.date.astype(int)
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], $                                               nobs=[n_new, n_old], alternative='larger') $ print(z_score) $ print(p_value)
cell_df.dtypes
df = df[df["status"] == "active"] $ df.drop(["status"], axis = 1, inplace = True) $ df.shape
squared_errors_quadratic = [(y_hat[i] - y[i]) ** 2 for i in range(len(y))] $ print("Total sq error is {0}".format(sum(squared_errors_quadratic))) $ print("Average sq error is {0}".format(sum(squared_errors_quadratic)/len(squared_errors_quadratic)))
feral_url = 'https://scontent-atl3-1.cdninstagram.com/vp/28607fb4ca62fc1cfe265fecffd76904/5AE6B6C4/t51.2885-15/e35/26066485_169583613655603_4826747482048299008_n.jpg' $ data[0]['display_url']
page_dummies = pd.get_dummies(df2['landing_page']) $ df_page = df2.join(page_dummies) $ df_page.head() $ df_page['new_page'].mean()
def getResults(JsonReponse): $     return JsonReponse.get('ResultSet').get('Result')
type(AAPL.index)  # returns a special kind of index called Datetime Index
sns.violinplot(calls_df["length_in_sec"],orient='v')
dc['created_at'].dt.hour.hist(density=True, color="blue")
new_page_converted = np.random.choice(2, n_new, p=[1-p_new,p_new]) $ new_page_converted
pt_weekly = pt.resample('W').sum() $ pt_weekly.head(10)
import pandas as pd $ income = pd.read_excel('Household Income Year 2013 to 2017 Aff.xlsx') $ income
reddit['Hour of Day'] = np.abs(reddit.Time.dt.hour - 5) 
yc_new2[yc_new2.tipPC > 100]
stocks.loc[('Apple', '2017-12-29')]
lm = sm.Logit(df_new["converted"],df_new[["intercept","control","CA","UK"]]) $ results = lm.fit() $ results.summary() $
t.second
sel_df.info()
from sklearn.feature_extraction.text import TfidfVectorizer $ vect_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english') $ vect_cleaned_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english') $ vect_stemmed_tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words = 'english') $
origin=origin.drop_duplicates()
not_in_misk.head(3)
mb = pd.read_excel('Data/microbiome/MID2.xls', sheetname='Sheet 1', header=None) $ mb.head()
df.dropna(how='any', inplace=True) $
with open(embedding_save_path, 'w') as f: $     f.write(json.dumps(embeddings_matrix)) $
new_page_converted = np.random.choice(a=[1,0], size=n_new, replace=True, p=[p_new, 1-p_new]) $
comps_count[comps_count.competitor_count <20].competitor_count.hist(bins = 21)
pd.Period('1/1/21', 'D') - pd.Period(pd.datetime.today(), 'D')
age_hist.age = pd.to_numeric(age_hist.age) $ age_hist.head() $ age_hist.info()
recipes.iloc[135598]
future_df = pd.concat([dfs,future_dates_df])
import seaborn as sns $ sns.set(style='whitegrid', palette='pastel') $ plt.figure(figsize=(8,6)) $ plt.hist(p_diffs) $ plt.show();
movies.isnull().any()
import statsmodels.api as sm $ df2.head(5) 
print("Proportion of converted users is {}%".format((ab_file['converted'].mean())*100))
from sklearn.feature_extraction.text import CountVectorizer $ vectorizer = CountVectorizer(token_pattern=r'\b\w+\b') # This token pattern keeps single-letter words $ train_matrix = vectorizer.fit_transform(train['text_clean']) $ val_matrix = vectorizer.transform(val['text_clean'])
sgd.fit(trainx,trainy)
day_of_week14 = uber_14["day_of_week"].value_counts().to_frame()
run txt2pdf.py -o "2018-06-18 1525 2015 872 discharges.pdf"  "2018-06-18 1525 2015 872 discharges.txt"
nan_samples = cats_df[cats_df.isnull().any(axis=1)] $ cats_df['remove'].iloc[nan_samples.index] = True $ del nan_samples
new_page_converted = np.random.binomial(1, size = n_new, p=p_new) $ new_page_converted
predicted = model2.predict(X_test) $ print predicted
a.clear()  # empty the list $ print(a)
n = len(df2[(df2.landing_page=='new_page')]) $ print(n) $ ntot = len(df2.index) $ prob_3 = n/ntot $ print(prob_3)
notus['country'].value_counts(dropna=False)[:45]
sess.get_data(sess.chain(['ibm us equity','indu index'],'chain tickers',{'chain points ovrd':10}), $               'opt implied volatility mid')
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
keep_day_threshold = 4 $ n_user_days = n_user_days.loc[n_user_days >= keep_day_threshold].reset_index() $ n_user_days = n_user_days[['seqn']]
ben_fin['reverted_mode'].value_counts()
X_train_dtm = pd.DataFrame(cvec.fit_transform(X_train.title).todense(), columns=cvec.get_feature_names())
releases.head()
df = final_annotations_df $ df[df.choice.str.contains('WOLF') & (df.how_many > 1)]
result = result.sort_values('taskid') $ display(result.head(5))
plt.figure(figsize=(12,8)) $ sns.violinplot(x=trump.source, y= 'num_char', data= trump) $ plt.xlabel(' Source of Tweet', fontsize=12) $ plt.ylabel('Number of Characters', fontsize=12) $ plt.title('Number of Characters per Tweet by Source', fontsize=15)
df2 = ab_df.query('(group == "control" and landing_page == "old_page") or (group == "treatment" and landing_page == "new_page")')
lr.score(preproc_training_test, training_test_labels)
vect = TfidfVectorizer(ngram_range=(2,4), stop_words='english') $ summaries = "".join(josh_tweets['text']) $ ngrams_summaries = vect.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
term_tops = lda.get_term_topics(rand_word, minimum_probability=.0001) $ term_tops, get_topic_desig(term_tops)
master_file['TECTONIC SETTING'].value_counts()
TotalNameEvents.head(1)
StockNames = [StockName for _, StockName in Stocks] $ print(StockNames) $ StockData['MeanPrice'] = StockData[StockNames].mean(axis=1)  # axis=1 require to generate means per row $ StockData['AMZN/AAPL'] = StockData['Amazon'] / StockData['Apple'] $ StockData[StockNames + ['MeanPrice', 'AMZN/AAPL']].head()
%bash $ gsutil cp *.csv gs://${BUCKET}/data/
%%time $ average_neighbor_degree = convert_dictionary_to_sorted_list(nx.average_neighbor_degree(network_friends))
test_url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2018-08-21&end_date=2018-08-21&api_key=' + API_KEY $ test_data = requests.get(url)
import statsmodels.api as sm $ convert_old = df2.query("group == 'control' and converted == True").shape[0] $ convert_new = df2.query("group == 'treatment' and converted == True").shape[0] $ n_old = df2.query('group== "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
summaries = "".join(df.title) $ ngrams_summaries = cvec.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
meals['cuisine_type'].fillna('', inplace=True) $ meals_with_cuisine_type = meals[meals['cuisine_type'].apply(lambda x: len(x) > 0)]['id'] $ X = X[X['meal_id'].isin(meals_with_cuisine_type)]
df_max.T.to_csv('../outputs/retrospective/CSFV2_outlook_monthly_90th_per_summary_table_from_{:%Y%m%d}.csv'.format(dates_range[0]))
run txt2pdf.py -o '2015 Snapshot.pdf' '2015 Snapshot.txt'
department_df_sub.sum(axis = 1)
profile_ids = df_cal['profile_id'].unique()
USvideos.describe() $ USvideos.head()
print("Combing the bag of words and the w2v vectors...\n") $ train_bwv = hstack([train_bow, train_w2v]) $ test_bwv = hstack([test_bow, test_w2v])
df = pd.read_csv('cars.csv') $ len(df)
model_data_df.to_csv('data/Random_Model_Data_All_One_Hot.csv')
run txt2pdf.py -o "METHODIST HEALTHCARE MEMPHIS HOSPITALS  Sepsis.pdf"   "METHODIST HEALTHCARE MEMPHIS HOSPITALS  Sepsis.txt"
count_non_null(geocoded_df, 'Case.File.Date')
twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %I:%M:%S %p')
for files in df.files[:10]: $     print('---') $     for f in files: $         print(f['name'])
for c in np.unique(c_pred): $     print('{}: {}'.format(c, np.sum(c_pred==c)))
!cat ../data/microbiome/microbiome.csv
network_simulation[network_simulation.generations.isin([])] $
desparse = dtm.toarray() $ word_list = cv.get_feature_names() $ dtm_df = pd.DataFrame(columns=word_list, data=desparse) $ dtm_df.head()
plot_df = df[['compound', 'datetime', 'Account']] $ plot_df = plot_df.rename(columns={'compound': 'Compound Polarity Score', 'datetime': 'Timestamp of Tweet'}) $ plot_df = plot_df.sort_values('Timestamp of Tweet') #sort tweets by date/time they were posted $ plot_df['Tweets Ago'] = [500-x for x in list(range(0,500))] $ plot_df.head()
df = pd.read_excel("../01-data/external/NO2_apr_mei_2016_notval.xlsx", na_values=[-9999])
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ results = mod.fit() $ results.summary()
print(ldamodel.print_topics(num_topics=10, num_words=11))
bt.trdplot['2004':'2007'] $ ohlc.C['2004':'2007'].rolling(short_ma).mean().plot(c='green') $ ohlc.C['2004':'2007'].rolling(long_ma).mean().plot(c='blue') $ pass
autos["registration_year"].describe()
new_seen_click_read_action_project_article.to_csv('data_report/report.csv', index = False)
train_cols = data.columns[1:] $ print(train_cols.values) $
c=True $ for _ in df.groupby('Store Number')['Zip Code'].unique().values: $     if len(_)!=1: $         c=False $ assert c
proc.token_count_pandas().head(20)
autos.shape
c = transactions.columns.tolist()
df_weather.head(5)
mi = s4g.set_index(['Symbol', 'Year', 'Month']) $ mi
predicted_table.head()
df.set_index('id', inplace=True)
gs.grid_scores_
avg_usercart_size = priors_reordered.groupby(["user_id"])['add_to_cart_order'].aggregate('count').reset_index(name='avg_user_cart_size') $ avg_usercart_size.head()
df_countries.nunique()
df_proc.loc[df_proc["CustID"].isin([customer])]
(LM_PATH / 'tmp').mkdir(exist_ok=True)
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
zipcodes = xmlData['zipcode'].value_counts().reset_index() $ zipcodes.columns = ['zipcodes', 'count'] $ zipcodes[zipcodes['count'] < 5]
autos['gearbox'].value_counts()
np.mean(high_pdiff)
pd.Series({'a': 42, 'b': 13, 'c': 2})
ddf=df.drop(df.columns[[0,1,2,3,5]], axis=1)
from sklearn.feature_extraction import text $ a = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index)) $ b = text.ENGLISH_STOP_WORDS $ set(a).issubset(set(b))
segmentData['opportunity_conversion'] = segmentData.apply(oppConversion, axis=1)
df.head(5)
codes = access_logs_df.groupBy('responseCode').count() $ codes
appleinbounds = appleNeg[appleNeg.inbound == True]
ax = sns.barplot(x='Single Name', y='Amount',data=dfmean.reset_index(),estimator=sum)
ins.groupby(["year"]).size()
items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'shirts': 15, 'shoes': 8, 'suits': 45}, $          {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants': 5, 'shirts': 2, 'shoes': 5, 'suits': 7}, $          {'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4, 'shoes': 10}] $ store_items = pd.DataFrame(items, index=['store 1', 'store 2', 'store 3']) $ store_items
from dllib.core import V $ test1 = V(test1)
blocks= pd.read_csv("../Data/blocks_clean.csv", usecols=range(1,23)) $ blocks.head()
max_ch_ol2 = max(abs(u.close-v.close) for u,v in zip(list(o_data.values()),list(o_data.values())[1:])) $ print('Another one liner using islice: {:.2f}'.format(max_ch_ol2))
def date_to_timestamp_ms(date_obj): $     timestamp_in_nanoseconds = date_obj.astype('int64') $     timestamp_in_ms = (timestamp_in_nanoseconds / 1000000).astype('int64') $     return timestamp_in_ms
df_state_votes.sort_values("hill_trump_diff", ascending=False).head(10)
titanic_class_sex_age = df_titanic_temp.groupby(['age', 'sex','pclass']) $ print(titanic_class_sex_age.describe())
tweets.tail()
model = load_model('wikigrader/data/nn_model.hdf5', custom_objects={'r2': utils.r2})
y.value_counts().values[0]/float(len(y))
Imagenes_data.jpg_url.value_counts() $ Imagenes_data.jpg_url.unique().size
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], value=0, alternative='smaller') $ print(z_score) $ print(p_value)
cum_cont = pd.DataFrame(data={"cum_contr":np.cumsum(contributions_by_day.contb_receipt_amt),"date":contributions_by_day.contb_receipt_dt}) $ cum_cont = cum_cont.sort_values('date', ascending=True).reset_index() $ cum_cont $ contributions_by_day
df.assign( $     UNK_count=df.CATS_Counter.apply(lambda x: x.get("UNK", 0)) $ ).groupby("is_controversial").UNK_count.agg([np.sum, np.mean])
part = total[datetime(2017, 6, 30, 11, 0, 0):datetime(2017, 6, 30, 13, 59, 59)] $ plt.plot(part['field4']) $ plt.show()
pd.read_csv("../../data/msft.csv",nrows=3)
loan_principals.head()
autos = autos[autos['price'].between(1,351000)] $ autos['price'].describe()
graffiti2.head(2)
mean = np.mean(data['len']) $ m = "{0:.4g}".format(mean) $ print("{} has an average tweet length of {} characters\n".format(target_user,m))
X_train, X_test, y_train, y_test = train_test_split(data[['expenses', 'floor', 'lat', 'lon',\ $                                                           'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']], \ $                                                     data[['price_aprox_usd']], test_size=0.1, random_state=0)
n_old = df_old.shape[0] $ print(n_old)
df.tail(20)
result_df.head()
coo_matrix((df2['D'], (df2['D'], df2['F']))).toarray()
df.head()
babynames = pd.read_csv('NationalNames.csv.zip', compression='zip')  # Pandas can unzip the data for you $ babynames.head()
df_train['dow_visitStartTime'] = df_train['visitStartTime'].dt.day_name() $ df_test['dow_visitStartTime'] = df_train['visitStartTime'].dt.day_name()
prophet_model.plot_components(forecast)
corr_matrix = my_df[["user", "day_from", "time", "week_day", "distance"]].corr() $ print(corr_matrix)
stock_tickers = companies.index.tolist() $ stock_tickers
2003 in frame3.index
sample.asfreq('H',method='ffill')
results = session.query(Measurement.station, func.count(Measurement.station))\ $             .group_by(Measurement.station)\ $             .order_by(desc(func.count(Measurement.station))) $ for row in results: $     print(row[0], row[1])
df['dog_description'].value_counts()
pipe.fit(train, labelsTrain)
log_reg = LogisticRegression() $ log_reg.fit(X_train_counts, y_train) $ log_reg.score(X_test_counts, y_test)
(etsamples_grid,etmsgs_grid,etevents_grid) = be_load.load_data(algorithm='hmmnosmooth_') $ raw_large_grid_df = condition_df.get_condition_df(data=(etsamples_grid,etmsgs_grid,etevents_grid),condition='LARGE_GRID')
z=cashflows_act_investor_20150430_def $ z[(z.id_loan==85) & (z.fk_user_investor==9614)].to_clipboard()
df.dtypes
tickets['listed_days'] = (tickets['meal_date'] - tickets['meal_created_date']) $ tickets['listed_days'] = tickets['listed_days'].apply(lambda listed_days: listed_days.days) $ X['meal_listed_days'] = tickets['listed_days']
month_max_loc = df.month.value_counts().sort_values(ascending=False).index[0] $ month_max = df.month.value_counts().sort_values(ascending=False).values[0] $ month_max_loc = df.created_at[df.month == month_max_loc].iloc[0] $ month_string = month_max_loc.strftime('%B %Y') $ print('I drank the most beers during the month of {} with {} total beers.'.format(month_string, month_max))
station_data = session.query(Stations).first() $ station_data.__dict__
data5_1 = pd.DataFrame(data=[tweet.text for tweet in tweets_pope], columns=['Tweets']) $ data5_2 = pd.DataFrame(data=[tweet.text for tweet in tweets_dalai], columns=['Tweets'])
len(grouped.sum())
dfcopy.sort_values('date').tail()
first_week = pd.date_range("2018/04/23", periods = 7, freq = "D") # first week of class $ first_week
ben_fin['stiki_percent'].value_counts()
g = sns.jointplot(y="PRICE", x="LIVING_GBA", data=condo_6, kind="hex", size=8)
pop_tot_cv = len(df2[df2.converted == 1]) $ print(pop_tot_cv)
temp_df2['timestamp'] = pd.to_datetime(temp_df2['timestamp'],infer_datetime_format=True)
df.select(a).show(5)
jobs.head()
file_cap = pyshark.FileCapture('captures/botnet-sample.pcap')
ind = pd.Index([2, 3, 5, 7, 11]) $ ind
response = requests.get('http://example.com') $ response
P_new=df2['converted'].mean() $ P_new $
p_new = df2.query('converted==1').user_id.nunique()/df2.user_id.nunique() $ p_new
p_old = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("Probability of conversion for old page (p_old):", p_old)
df2[df2.index==1899].index
doc_input = Input(shape=(DOC2VEC_SIZE,), name='doc_input') $ hidden = Dense(NN_HIDDEN_NEURONS, activation='relu', name='hidden_layer')(doc_input) $ softmax_output = Dense(NN_OUTPUT_NEURONS, activation='sigmoid', name='softmax_output')(hidden) $ model = Model(input=doc_input, output=softmax_output) $ model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', 'fbeta_score', theano_coverage_error])
itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb')) $ stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})
au.find_some_docs(ao18_qual_coll,sort_params=[("id",1)],limit=3)
cleaned_sampled_contirbutors_human_df = sampled_contirbutors_human_df.select( $     "*", $     clean_gender_field_udf("Answer_gender").alias("cleaned_gender"))
master_list[master_list['Count'] < 5]['Count'].describe()
def dateConverter(s): $     time_format = "%d/%m/%Y" $     converted = datetime.strptime(s,time_format) $     return converted
from pandas_datareader.data import Options $ aapl = Options('AAPL','yahoo') $ data = aapl.get_all_data() $ data.iloc[0:6,0:4]
df = df_questions[df_questions.columns-["test attempt","user id"]]
df_new[['US', 'UK', 'CA']] = pd.get_dummies(df_new['country']) $ df_new.head()
calls_nocontact = calls[calls.location != '(530187.70942, 3678691.8167)']
df_treatment = df.query("group == 'treatment'") $ df_treatment.converted.mean()
taxi_hourly_df.head()
files = [local_orig, local_edit] $ for file in files: $     with rio.open(file, 'r') as src: $         profile = src.profile $         print(profile)
head = pd.Timestamp('20150101') $ tail = pd.Timestamp('20160101') $ df = hp.get_data(sensortype='water', head=head, tail=tail, diff=True, resample='min', unit='l/min') $
pd.Series({2:'a', 1:'b', 3:'c'})
from microsoftml_scikit.utils.exports import img_export_pipeline $ fig = img_export_pipeline(pipeline, ds_train) $ fig.render('pipeline_visual') $ fig
pres_df['metro_area'].value_counts()
merged[['US', 'UK', 'CA']] = pd.get_dummies(merged['country'])
new_page_converted = np.random.binomial(1, p_new,n_new)
tl_2050 = pd.read_csv('input/data/trans_2050_ls.csv', encoding='utf8', index_col=0)
plot_confusion_matrix(cm,title='Confusion matrix', cmap=plt.cm.Blues)
ts['year'] = ts.index.year
loan_stats.columns
p_new = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ p_new 
from sklearn import preprocessing $ scaler = preprocessing.StandardScaler() $ data1Scaled = pd.DataFrame(scaler.fit_transform(data1),columns=featureList)
package.descriptor['resources'][1]['name'] = 'winemag-reviews' $ package.descriptor['resources'][1]
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
boat = df_titanic['boat'] $ print(boat.describe())
from scipy.stats import norm $ sign_z_score = norm.cdf(z_score) # Tells us how significant our z-score is $ crit_val_95conf = norm.ppf(1-(0.05/2)) # Tells us what our critical value at 95% confidence is $ sign_z_score, crit_val_95conf
merged_NNN.groupby(["contributor_firstname", "contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount", ascending=False)
pd.MultiIndex.from_product([['a','b'], [1,2]])
df2.loc[df['user_id'] == 773192]
new_items = [{'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4}] $ new_store = pd.DataFrame(new_items, index=['store 3']) $ new_store
cleansed_search_df['SearchTerm'] = np.where(cleansed_search_df['SearchCategory'] == "Plant", cleansed_search_df['SearchTerm'].str.rpartition('-')[2].str.strip() , cleansed_search_df['SearchTerm']) $ cleansed_search_df.loc[cleansed_search_df['SearchCategory'] == "Plant"]
X = tfidf.transform(text)
datetime.strptime('1.14.86', '%m.%d.%y')
multi.handle.value_counts()/multi.shape[0]
slack.files.upload('first_plot.png', channels = ['testing_exp', 'slack_interaction']);
stories[['comment_count', 'score']].corr()
g=sns.clustermap(df_lda.corr(), center=0, cmap="RdBu", metric='cosine', linewidths=.75, figsize=(12, 12)) $ plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0) $ plt.show()
from sklearn import linear_model $ model = linear_model.LinearRegression() $ print ('Linear Regression') $ reg_analysis(model,X_train, X_test, y_train, y_test)
dfgts_sorted.to_csv('drive/NBA_Data_Hackathon/NBA_gts_161718.csv', index=None)
df4 = df.drop('Cabin', axis=1) \ $     .loc[lambda x: pd.notnull(x['Embarked'])] \ $     .fillna(30)
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new,(1-p_new)]) $ new_page_converted.mean()
finnal_data.to_csv("finnal_data.csv")
selection = ['SP500', 'Gold', 'Fund', 'Ethereum'] $ pd.rolling_corr(fin_coins_r[selection], window=12).to_csv('output/rollcorr_tradvscoins.csv', na_rep='NaN')
df2[df2['landing_page'] == "new_page"]['converted'].sum()
print(applications['application_created_at'].min()) $ print(applications['application_created_at'].max())
np.arange(0,2,9)
df_tweet_data.describe()
print('Shifts with a pause: {}/{}'.format(len(df['pause'][df['pause'].astype('timedelta64[m]') != 0]), len(df)))
submissions['date_created'].apply(lambda x: x.week)
browser = Browser('chrome', headless=False) $ weather_url = 'https://twitter.com/marswxreport?lang=en' $ browser.visit(weather_url) $ time.sleep(1)
cgm = data[data["type"] == "cbg"].copy() $ cgm.head()
knn_reg.score(x_test,y_test),np.mean(y_test)
zone = 'HUDVL' #'GENESE'#'DUNWOD'#'CENTRL' #'CAPITL' $ df_byzone = pd.read_csv('nyiso_' + zone + '_price.csv', parse_dates=['time_stamp', 'time_stamp_local'])
s_n_s_epb_two.Date = s_n_s_epb_two.Date.str.replace('-',"") $ s_n_s_epb_two.Date = pd.to_datetime(s_n_s_epb_two.Date,format="%Y%m%d")
df_tsv['tweet_id'] = df_tsv['tweet_id'].astype(str) $ df_tsv.info()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=YxSY4z-2vyxvJ15WjtFa&start_date=2017-01-01&end_date=2017-12-31" $ req = requests.get(url)
slDf.index.is_unique
appointments['Specialty'].isnull().sum()
print ("Train Accuracy :: ", accuracy_score(train_dep[response], trained_model_RF.predict(train_ind[features]))) $ print ("Test Accuracy  :: ", accuracy_score(test_dep[response], trained_model_RF.predict(test_ind[features]))) $ print ("Complete Accuracy  :: ", accuracy_score(kick_projects_ip[response], trained_model_RF.predict(kick_projects_ip_scaled_ftrs))) $ print (" Confusion matrix of complete data is", confusion_matrix(kick_projects_ip[response],kick_projects_ip["Pred_state_RF"]))
tst_lat_lon_df.tail() ## final records in the input
dfg.describe()
df_CLEAN1A = pd.read_csv(url_CLEAN1A,sep=',') $ df_CLEAN1B = pd.read_csv(url_CLEAN1B,sep=',') $ df_CLEAN1C = pd.read_csv(url_CLEAN1C,sep=',')
df2 = df2.query("index != '1899'") $
store_items.size - store_items.count().sum()
batch = next(iter(md.trn_dl))
job_requirements.index
churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']] $ churn_df['churn'] = churn_df['churn'].astype('int') $ churn_df.head() $
not_found_paths_df = not_found_df.select('path') $ unique_not_found_paths_df  = not_found_paths_df.distinct() $ print('404 URLs: ') $ unique_not_found_paths_df.show(truncate=False, n=40)
((ab_data.group == "treatment") & (ab_data.landing_page != "new_page") \ $  | (ab_data.group != "treatment") & (ab_data.landing_page == "new_page")).sum()
df = df.dropna(subset=['title']) $ df_x_test = df['title'][:200] $ df_x_test = np.array(list(processor.transform(df_x_test)))
out = conn.loadtable('data/iris.csv', caslib='casuser', $                      casout=dict(name='mydata', caslib='casuser')) $ out
df.tail()
len(twitter_archive_df_clean[pd.isnull(twitter_archive_df_clean['expanded_urls'])])
absorption_to_total = absorption.xs_tally / total.xs_tally $ absorption_to_total.get_pandas_dataframe()
1/np.exp(results.params[1])
1/np.exp(-0.0150),np.exp(0.0506),np.exp(0.0408)
%time geo_segments_all.to_file("data/geo_segments_by_hours/" + geo_file_name, driver='GeoJSON')
total_df['Review_count'].plot.hist(bins = 20) $ plt.title('Overall Review Count Distribution') $ plt.ylabel("Frequency") $ plt.xlabel("Review Count")
donald_sentiment_score_df = create_df_grouped_by_date( donald_tweets_df ) $ donald_sentiment_score_df.head()
print "Score(happy): ", happiness['happy'] $ print "Score(miserable): ", happiness['miserable'] $ print "Best score: ", max(happiness.values()) $ print "Worst score: ", min(happiness.values())
len(df2.query('group=="control"').query('converted==1'))/len(df2.query('group=="control"'))
df.describe()
df['Market'].head(1) $ df['Market'].head(2) $ df['Market'].head(3) $ "Only I will print."
spotify_url = 'https://open.spotify.com/user/spotify/playlist/37i9dQZF1DX0XUsuxWHRQd' $ client_id = '2f2a2d3c5b2f4621ae1758439dae20c4' $ client_secret = 'client_secret' $
station_distance.loc[:, ['Start Station Name', 'End Station Name', $                          'Start Coordinates', 'End Coordinates']].head(2)
logit_model = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ logit_result = logit_model.fit() $ logit_result.summary() $
user_retention = cohorts_unstacked_nonblank.divide(cohort_group_size, axis=1) $ user_retention.head(10)
old_page_df = df2.query('landing_page =="old_page"')
types_of_complaints.to_csv('../output/call-descriptor-counts.csv')
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'],df2[['intercept', 'old_page']]) $ result = logit.fit()
y=df['target'] $ X=df.scores
user = user[user.is_enabled >= 0]
day_of_month14 = uber_14["day_of_month"].value_counts().sort_index().to_frame() $ day_of_month14.head()
plt.subplots(figsize=(15, 6)) $ sn.barplot(actions_top10['index'],actions_top10['action'], palette="Set3")
a_set.intersection(another_set)
args = mfclient.XmlStringWriter("args") $ args.add("where", "namespace=/projects/proj-hoffmann_data-1128.4.49/individuals") $ args.add("action", "get-path") $ args.add("size", "infinity") $ individuals_query = con.execute("asset.query", args.doc_text())
matches._get_numeric_data().columns.tolist()
weights.ix['2017-09-12':'2017-09-21']
measurements_df.head()
to_be_predicted_Day2 = 81.77623296 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
counts1 = np.cumsum(np.random.randint(low = 1,high = 40,size= 9)) $ counts2 = np.cumsum(np.random.randint(low = 10,high = 40,size= 9))
df2[df2.duplicated(subset="user_id", keep=False)] #duplicated() returns the index of of the cuplicate columns
print(gs_rfc_under.best_score_) $ print(gs_rfc_under.best_params_)
from sklearn.svm import LinearSVR $ model = LinearSVR() $ print ('SVR') $ reg_analysis(model,X_train_, X_test_, y_train_, y_test_)
df.set_index('State', inplace=True) $ df.head()
links = [] $ for link in JPL_soup.find_all('a'): $     links.append(link.get('href')) $ print(links)
status.retweeted, status.favorited
hashtag_counts.plot(kind='bar')
data[data['y_flag'] == 1][['dataset_datetime', 'dataset_location', 'SpO2', 'SpO2_change', 'Inspired Tidal Volume', 'Inspired Tidal Volume_change', 'Mean Airway Pressure_change', 'Respiratory Rate_change']].head(20)
train_user.loc[pd.isnull(train_user.first_affiliate_tracked)]
dataPath = os.path.join("..", "example-data") $ data = pd.read_csv(os.path.join(dataPath, "example-from-j-jellyfish.csv")) $ data.head()
def ts_interval_to_years(ts_interval): $     return float(ts_interval)/(3600000*24*365) $ train_data_df ['estimated_age_at_creation'] = train_data_df.apply(lambda item:\ $                                 ts_interval_to_years(item['created'] - item['birthdate']), axis=1)
(autos["last_seen"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().tail() $ )
sampled = cleaned_sampled_contirbutors_human_df_saved \ $   .withColumn( $     "Input_projects", $     cleaned_sampled_contirbutors_human_df_saved.Input_project_name)
from sklearn.model_selection import train_test_split $ train, test = train_test_split(loans_df, test_size=0.30, random_state=1)
df_prep5 = df_prep(df5) $ df_prep5_ = pd.DataFrame({'date':df_prep5.index, 'values':df_prep5.values}, index=pd.to_datetime(df_prep5.index))
base=pd.read_csv('/Users/taweewat/Documents/xray_project/red_sequence/chips_all_obj.csv', index_col=0) $ df4=base[(base['panstar']==True) & (base['pisco']==False) & (base['outside_pisco']==True) & (base['dec']<-27)] $ df5=base[(base['panstar']==False) & (base['pisco']==False)] $ test1=df4.append(df5)
lst_columns= list(df.columns) $ lst_columns.pop(lst_columns.index('Country')) $ lst_columns.insert(0,'Country') $ df[lst_columns].head(5)
sns.distplot(titanic.age)
data_sets = {} $ data_sets['15min'] = pd.read_pickle('final_15.pickle') $ data_sets['30min'] = pd.read_pickle('final_30.pickle') $ data_sets['60min'] = pd.read_pickle('final_60.pickle')
autos = autos[autos["registration_year"].between(1900,2016)] $ autos.shape[0]
autos = autos.drop(["seller","offer_type"], axis=1)
support=merged[merged.committee_position=="SUPPORT"]
print('\nPerplexity: ', ldamodel.log_perplexity(doc_term_matrix)) $ coherence_model_lda = CoherenceModel(model=ldamodel, texts=tweets_list, dictionary=dictionary, coherence='c_v') $ coherence_lda = coherence_model_lda.get_coherence() $ print('\nCoherence Score: ', coherence_lda)
trump['est_time'] = ( $     trump['time'].dt.tz_localize("UTC") # Setting initial timezone to UTC $                  .dt.tz_convert("EST") # Convert to Eastern Time $ ) $ trump.head()
index = [('California', 2000), ('California', 2010), ('New York', 2000), ('New York', 2010), ('Texas', 2000), ('Texas', 2010)] $ index = pd.MultiIndex.from_tuples(index) $ index $
import math          # import the math module $ print(math.sqrt(720), '\n')    # square root of 720 $ print(math.pow(5, 2), '\n')    #5 to the power 2
co_occurence_on_top50 = {ds: [(el[0], str(el[1])) for el in datasets_co_occurence[ds]] for ds in top50.dataset_id.tolist()}
df[df.A > 0]  # Boolean indexing
top_songs['Artist'].unique()
class_merged.to_csv('class_merged.csv',sep=',')
ratings.loc[ratings.rating_denominator > 10]
mit.info()
from nltk.classify import ClassifierI
data[data['Borough']=='Unspecified'].groupby('Agency').count()
df = pd.read_csv('GageData.csv', dtype={'site_no':'str'}) 
image_pred.head(2)
from statsmodels.graphics.tsaplots import plot_acf $ plot_acf(dataBPL['Total_Demand_KW'], lags=5209) $
access_logs_parsed = access_logs_raw.map(lambda r: parse_apache_log_line(r)).filter(lambda x : x is not None) $
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42) $ X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25,random_state=42)
Zn1_post = np.sum(Zn1Zn, axis = 0)
init = tf.global_variables_initializer() $ saver = tf.train.Saver()
date = pd.to_datetime(logins.login_time.iloc[0]) $ print(type(date), date.weekday)
check_int('2017-01-17')[check_int('2017-01-17').number_repeat == 4999]
new_page_converted.mean() - old_page_converted.mean() 
airport_count.to_excel('airport_count.xlsx') $ print("Done Writing!")
fig, ax = plt.subplots() $ load2017.iloc[:96].plot(ax=ax, title="Actual load 2017 (DE-AT-LUX)", x='timestamp', y='actual', lw=0.7) $ ax.set_ylabel("Actual Load") $ ax.set_xlabel("Time") $
df2[df2['user_id'].duplicated()]
grouped_by_plot = merged_data.groupby('plot_id')
volume = [] $ for ele in r.json()['dataset']['data']: $     volume.append(ele[6]) $ print('The average daily trading volume during this year: {} '.format(sum(volume)/float(len(volume))))
from sklearn.metrics import confusion_matrix $ confusion_matrix = confusion_matrix(y_test, y_pred_mdl) $ print(confusion_matrix)
w.get_step_object(step = 3, subset = subset_uuid).indicator_objects['din_winter'].get_ref_value(type_area = '1s', salinity = 25)
df[['Principal','terms','age','Gender','education']].head()
from sklearn.linear_model import LinearRegression as LR $ model = LR(fit_intercept=True, normalize=True, n_jobs=-1) $ model.fit(X,y) $ print model.coef_ $ print model.score(X,y)
RDDTestScorees.map(lambda entry: (entry[0], entry[1] * 0.9)).collect()
top.groupby('breed')['rating'].describe()
rdd = sc.parallelize([ $     Row(c1 = 1.0, c2 = None, c3 = None), $     Row(c1 = None, c2= "Apple", c3 = None)]) $ df = spark.createDataFrame(rdd, samplingRatio=1.0) # samplingRatio = 1 forces to see all records $ df.printSchema()
chk = joined.loc[joined['nat_hol']==1].head(); chk
yhat = neigh.predict(X_test) $ yhat[0:5]
f_yt_resolved = '/scratch/olympus/projects/ideology_scaling/congress/youtube_links_resolved.tsv' $ df_yt_resolved.to_csv(f_yt_resolved, index=False, sep='\t') $ shutil.chown(f_yt_resolved, group='smapp')
df[df.province<99].groupby("province").sum().sort_values(by="postcount",ascending=False)["postcount"][0:10]
df = pd.read_table("msft.csv", sep=",") $ df.head()
coming_next_reason = coming_next_reason.drop(['[', ']'], axis=1) $ coming_next_reason = coming_next_reason.drop(coming_next_reason.columns[0], axis=1)
mod.fit(trainx, trainy)
soup.find_all('div', class_='schedule-container')[0].select('.active')
r['BTC Price'].plot()
tret.plot(figsize = (16,4), color = 'r')
train['author_popularity'] = train.author.map(authors['mean'])
trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn]) $ val_clas = np.array([[stoi[o] for o in p] for p in tok_val])
plotdf.head()
cust_demo.duplicated().value_counts() $
obama.to_hdf('result/n2.h5','obama',table=True,mode='a') $ trump.to_hdf('result/n2.h5','trump',table=True,mode='a') $ with shelve.open('result/vars2') as db: $     db['obama'] = obama[['created_at', 'text']] $     db['trump'] = trump[['created_at', 'text']]
autos["price"].describe().apply(lambda x: format(x, 'f')) #Found this lambda to better view info.
autos["price"]= autos["price"].replace('[$,]','',regex=True).astype(int) 
print("max:",time_series.index.min(), "min:",time_series.index.max()) $
randomforest.fit(trainx, trainy)
store_items.insert(5, 'shoes', [8, 5, 0]) $ store_items
df_twitter_archive.tweet_id.nunique()
model = RandomForestRegressor(n_estimators=200, max_leaf_nodes=1000, $                               max_features=0.1, n_jobs=-1, verbose=1, $                               random_state=42)
tscv = model_selection.TimeSeriesSplit(n_splits=4) $ tscv_cv = tscv.split(train)
normals = [] $ for trip in trips: $     normals.append(daily_normals(trip))
path = data/'realdata'/'HH.dat'
def vectorize_string(txt): $   vec = proc.transform([txt])[:,1:] $   emb = np.mean(embedding_model.predict(vec), axis=1) $   return emb
notus.loc[notus['country'].isin(brazil), 'country'] = 'Brazil' $ notus.loc[notus['cityOrState'].isin(brazil), 'country'] = 'Brazil' $ notus.loc[notus['cityOrState'].isin(['Brasil', 'Brazil']), 'cityOrState'] = 'Brazil' $ notus.loc[notus['country'] == 'Brazil', 'cityOrState'].value_counts(dropna=False)
pvt['customerId'] = pvt['ga:dimension2'].str.rpartition('-')[0].str.strip() $ pvt['customerName'] = pvt['ga:dimension2'].str.rpartition('-')[2].str.strip() $ pvt
df2 = df $ df2.drop(mismatch_all.index,inplace=True)
tweet_df.info() $
LARGE_GRID.display_fixations(raw_large_grid_df, option='fixations')
df2.groupby('group')['converted'].value_counts()/df.groupby('group')['converted'].count()
g = sns.lmplot(x="Wins", y="regular_occurrences", data=team_names,fit_reg = True, legend = True) $ g.set_axis_labels(x_var = "Number of Wins", y_var = "Mentions") $ g.axes[0,0].set_xlim(0,82) $ plt.show()
print(dfx.dropna(how='any'))
year10 = driver.find_elements_by_class_name('yr-button')[9] $ year10.click()
top_allocs = hist_alloc.loc[pd.to_datetime(intervals)].sum(axis=0).sort_values(ascending=False) $ top_allocs[:10], top_allocs[-10:]
data[data['ComplaintID'].notnull()][['ComplaintID']].head()
print("Row names for berlin_weather_oldest.csv:") $ df1.index.values
RE_GENE_ID = re.compile(r'gene_id=(?P<gene_id>ENSG.+?);') $ def extract_gene_id(attributes_str): $     res = RE_GENE_ID.search(attributes_str) $     return res.group('gene_id') $ gene_df['gene_id'] = gene_df['attributes'].apply(extract_gene_id)
((null_values<obs_diff).mean())
print(array.shape) $ print(array.size) $ print(array.ndim)
cust_demo.age.plot(kind='hist', bins=50, color='R');
geoCodeList = dfCleaningData['geo_code'].unique() $ geoCodeList
n_conts = len(full_data.columns) - len(cats)
data2.drop('dates', axis = 1, inplace = True)
train_rounds = classification_data[(classification_data.announced_on < '2015-08-01')].funding_round_uuid $ test_rounds = classification_data[(classification_data.announced_on >= '2015-08-01')].funding_round_uuid
aussie_tweet_ids=[x.id for x in aussie_results] $ aussie_orig_tweet_ids=set(aussie_tweet_ids) $ len(aussie_tweet_ids) == len(aussie_orig_tweet_ids) $ len(aussie_tweet_ids)
overallQual = pd.get_dummies(dfFull.OverallQual)
convert_new = df2.query("landing_page == 'new_page'")['converted'].sum() $ convert_new
for i in range(15): $     print("{} (top topic: {})".format(text[mask][i], doc_topic[i].argmax()))
sp.str[-1]
average_polarity=pd.concat([average_polarity_2012,average_polarity_2013,average_polarity_2014 $                            ,average_polarity_2015,average_polarity_2016], axis=1)
accuracies.mean()
r=requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv') $ with open('image_predictions.tsv','wb') as file: $     file.write(r.content)
sns.lmplot(x="income", y="satisfied", data=training, x_estimator=np.mean, order=1)
overallFullBath = pd.get_dummies(dfFull.FullBath)
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'],df2[['intercept','treatment']])
subwaydf.iloc[114433:114439] #this low number seems to be because entries and exits resets
GroupZip = df.groupby(by = ['Zip Code']).mean()
'Ohio' in frame3.columns
if PredClass.toln: $     df_debias = make_debias(y, predict_final) $     PredClass.save_debias(df_debias)
print("Number of observations in train set :", train1.count()) $ print("Number of observations in dev set :", dev1.count()) $ print("Number of observations for next model training:", modeling2.count())
test_tfidf.shape, test_cleaned_tfidf.shape, test_stemmed_tfidf.shape
df_new[['CA','UK','US']]= pd.get_dummies(df_new['country']) $ df_new.head()
gid.get_repo_version('a')
df_tweets2['sentiment'] = np.random.uniform(-1,1, size=len(df_tweets2)) $ df_tweets2['sentiment_ori'] = np.random.uniform(-1,1, size=len(df_tweets2))
house = elec['fridge'] #only one meter so any selection will do $ df = house.load().next() #load the first chunk of data into a dataframe $ df.info() #check that the data is what we want (optional) $
pd.value_counts(appointments[appointments['Specialty'] == 'RN/PA']['Provider'])
import sqlalchemy $ from sqlalchemy.ext.automap import automap_base $ from sqlalchemy.orm import Session $ from sqlalchemy import create_engine, inspect, func, desc
recommendationTable_df = ((genreTable*userProfile).sum(axis=1))/(userProfile.sum()) $ recommendationTable_df.head()
iplot(data.groupby('assignee.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, kind='bar', dimensions=(750, 500)))
live_kd915_filt_df = pd.concat([live_df, kd915_filtered], axis = 0)
df_user_extract_copy['user_id'] = df_user_extract_copy['user_id'].astype(str) $ df_user_extract_copy['created_at'] = pd.to_datetime(df_user_extract_copy['created_at']) $ df_user_extract_copy = df_user_extract_copy.drop('Unnamed: 0', axis = 1) $ df_user_extract_copy['location'] = df_user_extract_copy['location'].fillna("not given")
df['misaligned']=((df.group=='treatment') & (df.landing_page=='old_page')) | ((df.group=='control') & (df.landing_page=='new_page'))
to_be_predicted_Day5 = 22.39046301 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
print(len(set(equipment.INSTANCE_ID) & set(intervention_train.INSTANCE_ID))) $ print(len(set(equipment.INSTANCE_ID) & set(intervention_test.INSTANCE_ID))) $ print(len(set(equipment.INSTANCE_ID) & set(intervention_history.INSTANCE_ID)))
import matplotlib.pyplot as plt $ import seaborn as sns $ % matplotlib inline $ df_input_pd = df_input_clean.toPandas()
mean_temp.head()
sns.boxplot(x='rating', y='text length', data=dataset) $
ps = pd.Series(np.random.randn(len(prng)), prng)
exportOI['avg_duration'] = exportOI['event.longSum_sum_duration']/exportOI['event.longSum_total_records']
week32 = week31.rename(columns={224:'224'}) $ stocks = stocks.rename(columns={'Week 31':'Week 32','217':'224'}) $ week32 = pd.merge(stocks,week32,on=['224','Tickers']) $ week32.drop_duplicates(subset='Link',inplace=True)
billstargs.drop(['congress', 'number', 'chamber', 'committee_ids', 'introduced_on', 'last_action_at', 'last_vote_at', 'official_title', 'urls'], axis=1, inplace=True)
area_dict = {'Illinois': 149995, 'California': 423967, $              'Texas': 695662, 'Florida': 170312, $              'New York': 141297} $ area = pd.Series(area_dict) $ area
retweet_pairs[retweet_pairs["Weight"]>1].shape
p_diffs = np.array(p_diffs) $ (p_diffs > obs_diff).mean()
json_data_2017 = request_data_2017.json()
plt.plot(xs)
tweets_stream_data = [] $ tweets_stream_file = open('tweets_stream.json','r') $ for line in tweets_stream_file: $     tweet = json.loads(line) $     tweets_stream_data.append(tweet) $
keras.metrics.categorical_accuracy(y_test, avg_preds).eval()
precip_df.plot() $ plt.title("Precipitation Analysis - Last 12 mos") $ plt.savefig("Images/precipitation_analysis.png") $ plt.show()
dfbreakfast = df[(df['TIME'] == '11:00:00') | (df['TIME'] == '12:00:00')] $ dfbreakfast.head(2)
pd.crosstab(result['timestampUTC'].dt.year, result['type'])
print('Most chuuch: {}'.format(tweets_pp[tweets_pp.handle == 'chuuch'].sort_values('ch000ch_pp', ascending=False).text.values[0])) $ print('Least chuuch: {}'.format(tweets_pp[tweets_pp.handle == 'chuuch'].sort_values('ch000ch_pp', ascending=True).text.values[0]))
capa2017['Production Type'].unique()
from nltk.stem import WordNetLemmatizer
Measurement.__table__
log.info(("loading New predictions={} with accuracy={} into pandas dataframe") $          .format(len(new_predictions), new_accuracy)) $ new_train_results_df = pd.DataFrame(new_predictions) $ log.info(("columns in dataframe with columns={}" $           .format(new_train_results_df.columns.values)))
df_TempIrregular.head(100)
team_slugs_mini = team_slugs_df[['new_slug','nickname']] $ team_slugs_mini.set_index('nickname', inplace=True) $ team_slugs_dict = team_slugs_mini.to_dict()['new_slug']
tweet_json_df.head(3)
autos["fuel_type"].unique()
establecimientos_educativos = pd.read_csv('datasets/establecimientos-educativos.csv', sep=';', error_bad_lines=False, low_memory=False) $ establecimientos_educativos.info()
beta_GLD , alpha_GLD = np.polyfit(daily_returns['SPY'],daily_returns['GLD'],1) $ print(beta_GLD , alpha_GLD) $ daily_returns.plot(kind='scatter',x='SPY',y='GLD') $ plt.plot(daily_returns['SPY'],daily_returns['SPY']*beta_GLD + alpha_GLD,'-',color='r') $ plt.show()
pd.merge(transactions,transactions,how='outer',on='UserID')
result_final.summary2()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print("Z_score:",z_score) $ print("P_value:",p_value)
df1.columns
s = c.retrieve_set(setid=0xb) $ pprint(s.metadata())
p_convert_control = df2.query('group == "control"').converted.mean() $ p_convert_control
contractor_clean['last_updated'] = pd.to_datetime(contractor_clean.last_updated) $ contractor_clean['updated_date'] =contractor_clean['last_updated'].dt.strftime('%m/%d/%Y') $ contractor_merge['month_year'] =contractor_merge['last_updated'].dt.to_period('M')
df['SSL'].isin(aru_df['SSL']).value_counts()
users["age"] = users["age"].fillna(0) $ users["country_destination"] = users["country_destination"].fillna("NDF") $ users["date_first_booking"] = users["date_first_booking"].fillna("-unknown-") $ users["first_affiliate_tracked"] = users["first_affiliate_tracked"].fillna("untracked")
subred_num_avg.head(1)
fb_data = graph.get_object(id='DonutologyKC/', fields=req_fields) $ type(fb_data)
local_holidays=actual_holidays[actual_holidays.locale=='Local'] $ print("Rows and columns:",local_holidays.shape) $ pd.DataFrame.head(local_holidays)
trainwdummies.columns
res = skafos.engine.create_view( $     "weather_noaa", {"keyspace": "weather", $                       "table": "weather_noaa"}, DataSourceType.Cassandra).result() $ print("created a view of NOAA historial weather data")
cp311.columns = [ x.lower().replace(' ','_') for x in cp311.columns]
print('Before: ', kickstarter.shape) $ kickstarter = kickstarter.dropna(axis=0, how='any') $ print('After: ', kickstarter.shape) $ kickstarter.isnull().sum()
mlp_pc = mlp_df.pct_change() $ mlp_pc.head()
bixi_hourly=bixi_hourly.set_index('start_date')
weather_warm.shape
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
total2=total[total['priority']=='A'] $ total2.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/objsA_fall2016.csv',index=False)
goo2 = pd.read_csv('data/goog.csv',index_col='Date', parse_dates=['Date'])
pd.DataFrame( $     np.array(retweets_status_keys + [""]).reshape(5, 5) $ )
df3['Donation Received Month'] = df3['Donation Received Date'].dt.month $ df3['Donation Received Year'] = df3['Donation Received Date'].dt.year $ df3['Donation Received Month-Year'] = df3['Donation Received Date'].dt.to_period('M') $ df3.sort_values(by='Donation Received Date', inplace=True, ascending= True)
df_new=df_new.drop('CA',axis=1) $ df_new.head()
kraken_orderbook.head(10)
pp.pprint(tweet._json)
ticks.tshift(1, 'min').head()
cur.execute('SELECT count(Comments_Ratings) FROM surveytabl WHERE Comments_Ratings is not null;') $ cur.fetchall()
airbnb_df.filter(like='host', axis=1).head()
unknown_users_log = df_log[~df_log['user_id'].isin(df_users['user_id'])]
train_labels = np.load('train_labels.npy') $ train_word = np.load('train_word.npy') $ train_char = np.load('train_char.npy')
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis=1) $ df2.head()
converted = df.query('group == "control" and converted == 1')['user_id'].count() $ total = df.query('group == "control"')['user_id'].count() $ old_conv_rate = converted / total $ old_conv_rate
df_ab_cntry.head()
ice = gcsfs.GCSFileSystem(project='inpt-forecasting') $ with ice.open('inpt-forecasting/Inpatient Census extract - PHSEDW 71118.csv') as ice_f: $   ice_df = pd.read_csv(ice_f)
best_150_cols = X_train.columns[gs_k150.best_estimator_.named_steps['kbest'].get_support()] $ k150_coef = gs_k150.best_estimator_.named_steps['logreg'].coef_ $ pd.DataFrame(k150_coef, columns=best_150_cols).T.sort_values(0, ascending=False).head()
new_page_converted = np.random.choice(2,size = 145310,p = [p_new,(1 - p_new)]) $ p_new_bootstrapped = new_page_converted.mean() $ print(p_new_bootstrapped)
twitter_merged_data.hist(column='favorites', bins =60); $ plt.title('Favorites Histogram') $ plt.xlabel('Favorites (Bins=60)') $ plt.ylabel('Count');
df2.query('group=="control"').query('converted==1')['user_id'].count()/df2.query('group=="control"').shape[0]
mojog_df["unemp_rate"] = 4.1 $ mojog_df.head()
plt.scatter(scaled_matrix[:,0],scaled_matrix[:,1], c=kmeans.labels_, cmap='rainbow')  $ plt.scatter(centers[:,0] ,centers[:,1], color='black') 
file = 'injuries.csv' $ injury_df = pd.read_csv(file)
df2['intercept'] = 1 $ df2['a/b_page'] = df2['group'].replace(('control','treatment'),(0,1)) $ df2.head()
col_list = list(df.columns.values) $ print(col_list)
far = np.concatenate([px.values, px.values[-1] + walk + perturb])
prec_long_df = pd.melt(prec_wide_df, id_vars = ['grid_id', 'glon', 'glat'], $                       var_name = "date", value_name = "prec_kgm2") $ prec_long_df.head()
s2.iloc[1:3]
sumTable.index
mars_html_table = mars_df_table.to_html(classes='marsdata') $ mars_table = mars_html_table.replace('\n', ' ') $ mars_table
n_old = df2.query('landing_page == "old_page"').count()['user_id'] $ n_old
svc = SVC() $ svc.fit(X_train, Y_train) $ Y_pred = svc.predict(X_test) $ acc_svc = round(svc.score(X_train, Y_train) * 100, 2) $ acc_svc
df_new.country.unique()
df2['intercept'] = 1 $ df2[['ab_page_to_drop', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop(['ab_page_to_drop'], axis=1) $ df2.head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\FourFiles.zip" $ zf = zipfile.ZipFile(path) $ df = pd.read_csv(zf.open('vehicles.csv')) $ df.head(5)
my_model_q4 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_lr, training='label') $ my_model_q4.fit(X_train, y_train) $ y_pred = my_model_q4.predict(X_test) $ accuracy = metrics.accuracy_score(y_test, y_pred) $ print("Accuracy: " +  str(accuracy))
avg_values = [] $ for x in range(0,30): $     avg_values.append(dfs[x]['Outside Temperature'].mean()) $ print (avg_values)
df1.index = range(len(df1)) $ df2.index = range(len(df1))
list1 = list(df2_clean.p1.unique()) $ list2 = list(df2_clean.p2.unique()) $ list3 = list(df2_clean.p3.unique()) $ full_list = list1 + list2 + list3 $ any(word[0].islower() for word in full_list)
calls_df["length_in_sec"].isnull().sum()
df_new['us_intercept'] = df_new['country'].replace(('US','UK','CA'),(1,0,0)) $ lm = sm.OLS(df_new['converted'],df_new[['intercept','us_intercept']]) $ lm.fit().summary()
%matplotlib inline $ fig= date_df.groupby(date_df.created_time.dt.month).count().plot(kind="bar",title="posts par mois")
doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]
transactions.head(2)
df.min()
ben_fin = ben_dummy.groupby('userid').nth(1)
twitter_archive.head()
pres_df['ad_length'].describe()
update_records('tbl1', 2, 'price', 50.25)
df['log_time_detained'] = np.log(df['time_detained'])
df.head(50)
test.head()
new_page = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ print(len(new_page))
nb_class.show_most_informative_features(10)
print(DataSet_sorted['tweetText'].iloc[-1])
sns.distplot(basic_smapes, label='Basic model') $ sns.distplot(complex_smapes, label='Complex model') $ plt.legend(loc='upper right') $ plt.savefig('smape_basic_vs_complex.svg') $ plt.show()
tweetsIn22Mar = tweetsIn22Mar.loc['2018-03-22'] $ tweetsIn1Apr = tweetsIn1Apr.loc['2018-04-1'] $ tweetsIn2Apr = tweetsIn2Apr.loc['2018-04-2']
plt = df2.plot(legend=False) $ plt.set_ylabel("complaint count") $ plt.set_xlabel("DOTCE predicted grade school level (high school 9-12)");
df_ser_dict["Three"] = pd.Series([10,20,30,40],index = ["A","B","C","D"]) $ df_ser_dict
len(set(df_2.title))
data.head()
chefdf.info()
autos['odometer_km'].describe()
non_na_df.groupby('hour').count()['sender_qq'].plot(kind='bar', figsize=(18,6)) $ plt.tight_layout() $ plt.title('Activity by aHour') $ plt.ylabel('Total Chat Count') $ plt.xlabel('Hour (24h)')
for i in range(1000): $     print clintondf.text[i] $     print 
twitter_Archive['timestamp'] = pd.to_datetime(twitter_Archive['timestamp']) $ type(twitter_Archive['timestamp'].iloc[0])
df3.drop('other',axis=1,inplace=True)
%%time $ test2 = model.predict_generator(custom_generator(), steps = 100, $                                workers=1, use_multiprocessing=False)
old_page_converted = np.random.choice([0,1], n_old, replace=True, p=[1-p_old,p_old]) $ np.bincount(old_page_converted)
from scipy.stats import norm $ norm.cdf(z_score) $ norm.ppf(1-(0.05/2)) $
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.hist(results["units"]["uqs"]) $ plt.xlabel("Video Fragment Quality Score") $ plt.ylabel("Video Fragment")
for row in example1_df.take(2): $     print row $     print "*" * 20
with pd.option_context('display.max_colwidth', 100): $     display(news_period_df[['news_collected_time', 'news_title']])
goog['Month'] = goog.index.month $ goog['Year'] = goog.index.year
data_from_1c = pd.read_csv(r'data/WOSS.csv') $ data_from_1c.columns = ['Part no.'] + list(data_from_1c.columns[1:]) $ data_from_1c.columns
converted_controlusers2 = float(df2.query('converted == 1 and group == "control"')['user_id'].nunique()) $ control_users2 =float(df2.query('group == "control"')['user_id'].nunique()) $ cp2 = converted_controlusers2 /control_users2 $ print(" Given that an individual was in the control group, the probability they converted is {0:.2%}".format(cp2))
lr1 = LogisticRegression() $ params1 = {'penalty': ['l1', 'l2'], 'C':np.logspace(-5,0,100)} $ gs_1 = GridSearchCV(lr1, param_grid=params1, cv=10, verbose=1) $ gs_1.fit(X1, y1)
rain_df = pd.DataFrame(rain) $ rain_df.head()
len(df) #Afters cleansing, df contains 9840 cases.
df = pd.DataFrame([ [1,2,3],[5,6,7] ], columns = ["col_1", "col_2","col_3"]) $ df
s.str.strip()
tt = pd.get_dummies(test_df['pred'],prefix='pred')
baseball[baseball['team'].isin(['LAN', 'SFN'])]
google_after_april = target_google[window] $ google_after_april.head()
tag_pairs[["FromType","FromName","Edge","ToType","ToName","Weight"]].to_csv("For_Graph_Commons3.csv",encoding="utf-8",index=False)
specsJson["nebl.io"]
for file in my_zip.namelist(): $     print(ds100_utils.head(data_dir/file, 5))
df.loc['r2']['A']
from dateutil.parser import parse $ def date_parse(da): $     a=parse(da) $     return a
idx = pd.IntervalIndex.from_arrays(df2.Start, df2.End, closed='both') $ idx
brand_info = pd.DataFrame(mean_mileage,columns=['mean_mileage']) $ brand_info
sqlContext.sql("select count(person) from pcs").show()
P_old = df2[df2['landing_page'] == 'old_page']['converted'].mean() $ print("P_old under null: ", P_old)
def add_dollar(value): $     value = str(value * 100.00) + '%' $     return value
tweets_clean.head()
filenames = glob.glob(os.path.join(input_folder, glob_string))
b = la.solve(a, c) $ b
wrd_tidy['hour'] = wrd_tidy['timestamp'].apply(lambda x: datetime.strptime(x, "%Y-%m-%d %H:%M:%S +0000").hour)
y = np.array(df['label'])
fact_train = pd.read_csv('facturation_train.csv') $ print('Training data shape: ', fact_train.shape) $ fact_train.head(40)
import matplotlib.pyplot as plt
random.randint(0,9)
hemispheres_url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(hemispheres_url)
fulldata_copy.head()
tmp = [i.lower() for i in df_repub_2016['text']]
twitter_archive_clean['timestamp']= twitter_archive_clean['timestamp'].apply(convert_to_datetime)
g.reset_index(inplace=True) $ g.head(2)
X_d = pd.get_dummies(X, columns=['domain_d'], drop_first=True)
test_ind["Pred_state_XGB"] = xgb_model.predict(test_ind[features]) $ train_ind["Pred_state_XGB"] = xgb_model.predict(train_ind[features]) $ kick_projects_ip["Pred_state_XGB"] = xgb_model.predict(kick_projects_ip_scaled_ftrs)
advancedmodel = LogisticRegression() $ advancedmodel = advancedmodel.fit(advancedtrain, train["rise_in_next_day"])
nypd_df_raw = pd.read_csv(data_folder+'/nypd_7_major_felony_incidents.csv') $ nypd_df = nypd_df_raw[['Created Date', 'Closed Date', 'Agency', 'Complaint Type', 'Descriptor', 'Location Type', \ $                        'Community Board', 'Borough', 'Latitude', 'Longitude']] $ nypd_df = nypd_df[nypd_df.Descriptor == "Blocked Sidewalk"] $ nypd_df.head()
df1['PCT_Change']=(df1['Adj. Close']-df1['Adj. Open'])/df1['Adj. Open'] $ df1['PCT_Change'].head()
temp_df.plot.hist(bins=12)
df['domain_d'].value_counts().head(30).plot(kind='bar') $ plt.xticks(rotation=70);
before_filter = kickstarter.shape $ kickstarter = kickstarter[(kickstarter["state"] == "successful") | (kickstarter["state"] == "failed")] $ assert list(kickstarter["state"].unique()) == ["failed", "successful"] $ after_filter = kickstarter.shape $ print("Shape before: {0}; Shape After: {1}".format(before_filter, after_filter))
from gensim.models import Word2Vec $ model = Word2Vec.load("300features_40minwords_10context")
pd.Series([1,2,3,np.nan]).mean()
sns.countplot(x='approved',data=data, palette='hls') $ plt.show()
cust_data['MonthlySavings1'] = cust_data['MonthlyIncome'] * 0.15 $ cust_data.head(2)
_ = ok.grade('q08b') $ _ = ok.backup()
tl_2040 /= 1000 $ tl_2040_norm = tl_2040 ** (10/11) $ tl_2040_norm = tl_2040_norm.round(1) $ tl_2040_alpha = tl_2040 ** (1/3) $ tl_2040_alpha = tl_2040_alpha / tl_2040_alpha.max().max()
layer = item.layers[0] $ layer
itos2 = pickle.load((WT_PATH / 'itos_wt103.pkl').open('rb')) $ stoi2 = collections.defaultdict(lambda: -1, {v: k for k, v in enumerate(itos2)})
data.tail(3)
df.info()
perf_train.head(10)
Sun_index  = pd.DatetimeIndex(pivoted.T[labels==0].index).strftime('%a')=='Sun' $ pd.DatetimeIndex(pivoted.T[labels==0].index)[Sun_index]
twitter_archive[twitter_archive['rating_denominator'].astype(int)>10] $
print(df_by_donor.columns.get_level_values(0)) $ print(df_by_donor.columns.get_level_values(1))
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ print(last_year)
sumPre = dfPre['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9]) $ sumPost = dfPost['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9])
archive[np.isnan(archive['retweeted_status_id']) != True].sample(5)
mask = (x > 0.5) & (y < 0.5)
model.most_similar("man") $
components = ['PC' + str(x) for x in range(1,len(pca.explained_variance_)+1)] $ varianceTable = pd.DataFrame({'Variance':pca.explained_variance_,'Variance %': pca.explained_variance_ratio_*100},index=components)
df_reg['y']=df_reg1['y'].astype(float)
store_items.interpolate(method='linear', axis=0)
hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val), $                  callbacks=[RocAuc], verbose=1)
df2 = df2.drop_duplicates(subset='user_id')
df.to_csv('cleaned_vg_df')
autos["price"].value_counts().sort_index(ascending=False).head(20)
jdf.git_commits_url[10]
X_cvec_synos = pd.concat([df_vec, df_city_dummy, df_state_dummy, df_vec_synos], axis=1) $ X_cvec_synos.head()
sns.barplot(x="MILLESIME", y="target", data=df)
%run ~/source/repos/twitter_credentials.py $
CryptoComm['CommentTime'] = pd.to_datetime(CryptoComm['CommentTime'], format='%Y-%m-%d %H:%M:%S') $ CryptoComm['PostTime'] = pd.to_datetime(CryptoComm['PostTime'], format='%Y-%m-%d %H:%M:%S')
mcg = s4g.groupby(['Symbol', 'Year', 'Month'])
femalebydate = female.groupby(['Date','Sex']).count().reset_index() $ femalebydate.head(3)
tweetsDf.favorite_count.describe() $
fin_df.isnull().any()
top_songs['Artist'].isnull().sum()
blob.polarity
p_new = df2.converted.mean() $ p_new
df.source.value_counts()
users=df2.shape[0] $ new_page_user = len(df[df['landing_page'] == 'new_page']) $ new_page_user $ new_page_user/users
df.head()
CONTENT_COUNT = len(np.unique(filtered_content_words))
experiment_details = client.experiments.get_details(experiment_uid)
df_user = pd.read_csv(f_user) $ df_user.head(3)
sum(tweetsDF.location.value_counts())
def mvp_cohort(row): $     if row['atleast_one_course_completed_or_not']=='Completed atleast one course (Course started after 1st July 2018)' and row['downloaded_or_not']=='Downloaded after 1st July 2018' and row['active_or_inactive']=='Active': $         row['cohort']='MVP User'   $     return row['cohort'] $ df_users_6['cohort']=df_users_6.apply(mvp_cohort,axis=1)
jobs.loc[(jobs.FAIRSHARE == 24) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'cgg')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
result.params
agg_churned_plans_counts = [(key,sum(num for _,num in value)) for key, value in groupby(uniq_sorted_churned_plans_counts,lambda x:x[0].tolist())]
data.loc['AT'] = [386.4, 8.7, 'German', 'EUR']  #Add a row with index 'AT'. $ s = pd.DataFrame([[511.0, 9.9, 'Swedish', 'SEK']], index=['SE'], columns=data.columns) $ data = data.append(s)  #Add a row by appending another dataframe. May create duplicates. $ data
n_new=df2.query('group == "treatment"').shape[0] $ n_new
tweets_kyoto.head()
annual_precip_df = pd.DataFrame(annual_precip, columns=['date', 'prcp']) $ annual_precip_df.set_index(pd.DatetimeIndex(annual_precip_df['date']), inplace=True) $ annual_precip_df.describe()
Bronx_gdf = newYork_gdf[newYork_gdf.boro_name == 'Bronx'] $ Bronx_gdf.crs = {'init': 'epsg:4326'}
old_DUL_file = [f for f in os.listdir(old_folder) if re.match('.*Dul.*', f)][0] $ old_DUL_file = old_folder + "\\" + old_DUL_file $ old_DUL_file
df.info() #checking the dataset
The Values are not Statistically Significant. It is difficult to predict based on only these values. $ The P value is much higher than expected. We have to increase the variables. There is no relation between the country $ and convertion rates
conditions_clean = conditions_lower.str.strip() $ conditions_clean
contractor[contractor.duplicated() == True]
rate = api.rate_limit_status() $ rate_df = pd.io.json.json_normalize(rate).T $ rate_df = rate_df[rate_df.index.str.endswith("remaining")] $ rate_df = rate_df[rate_df.index.str.contains("search")] $ rate_df
y_error = np.array(y-y_predicted)
df.sort_values('prob_off').tail(50)
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['rating_denominator'] != 10].index)
pt_unit_top.head(20)
cfs.plot(sample_field='Subject',gui='jupyter')
subred_num_tot.rename(index=str, columns={'subreddit': 'Subreddit', 'num_comments': 'Total_Num_Comments'}, inplace=True)
excelDF.head()
top_20_breed_stats[['favorite_count','retweet_count']].sort_values(by='favorite_count', ascending=False).plot(kind='bar', subplots=True)
autos.columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', $        'vehicle_type', 'registration_year', 'gearbox', 'power_ps', 'model', $        'odometer', 'registration_month', 'fuel_type', 'brand', $        'unrepaired_damage', 'ad_created', 'nr_of_pictures', 'postal_code', $        'last_seen']
cc.close_ratio.describe()
X = df.drop('num_comments', axis = 1) $ y = df.num_comments
atom = tb.UInt8Atom(shape=(2,)) $ atom
joined['dcoilwtico'].interpolate(inplace=True, limit_direction='both')
rng.tz is None
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
1/np.exp(results_2.params)
df["weekend_pickup"] = (df["pickup_dow"].isin([4, 5, 6])).astype(int)
to_be_predicted_Day2 = 43.30506441 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
print [logit.coef_, logit.intercept_]
f_device_hour_clicks.show(1)
train_df = stats_diff(train_df) $ print(train_df.head(5))
df['is_test'] = (np.random.rand(len(df.index)) < 0.8)
df = pd.DataFrame(np.array([ht,h0,cv]).T, columns = ['ht','h0','cv'], index = folder_names) $ dfPeDa = pd.DataFrame(list(df.apply(lambda x: list(PeDa(1,33,x.h0/1000)), axis = 1)), index = folder_names, columns = ['Pe','Da']) $ df = pd.concat([df,dfPeDa], axis = 1) $
X_train_2, X_test_2, y_train_2, y_test_2 = ( $   train_test_split(X_2, y_2, test_size=0.20))
guineaDf.head()
autos.info()
m.lr_find()
support_NNN.amount.sum() / merged_NNN.amount.sum()
station_cnts_desc = session.query(Measurement.station,func.count(Measurement.station).label("scount")).\ $                     group_by(Measurement.station).\ $                     order_by(desc("scount")).\ $                     all() $ station_cnts_desc          
test_df.tail(5)
init = tf.global_variables_initializer() $ saver = tf.train.Saver()
vow.head()
brics.index = ["BR", "RU", "IN", "CH", "SA"] $ brics
h2o.init() $
titanic.groupby('sex')[['survived']].mean()
print(len(df_sched)) $ df_sched.head()
sns.countplot(x='currency', data=data_for_model, hue='final_status')
better_counting_dict = collections.Counter(tallies) $ better_counting_dict
import numpy as np $ rng = np.random.RandomState(42) $ x = rng.rand(1E6) $ y = rng.rand(1E6) $ %timeit x + y
os.listdir()
clicking_conditions.info()
dfMonth['Date'] = dfMonth['Date'].dt.to_period("M")
yhat=neigh.predict(X_test) $ yhat[0:5]
def insertIntoPath(newPath): $     if newPath not in sys.path: $         sys.path.insert(0, newPath)
class Review_Item(scrapy.Item): $     author = scrapy.Field() #author of review $     date = scrapy.Field() #date published $     rating = scrapy.Field() #rating (1-5) $     text = scrapy.Field() #content of review
infered_relevant_info.groupBy(infered_relevant_info.infered_gender).agg(F.count("*")).show()
all_games = bruins.append(celtics).append(sox) $ all_game_dates = all_games.date.unique()
flights = pd.read_csv('flights/flights_sm_raw.csv') $ flights.head(3)
df.loc[dates[0]]
(p_diffs>p_diff_obs).mean()
today = datetime.now().strftime('%m/%d/%Y') $ print(type(today))
table_store.ix[5:]
print(train["comment_text"][0]) $ print(example1.get_text()) $
prod_paudm.to_csv("production_paudm.csv") $ id_pau = prod_paudm["input_production_id"] $ print id_pau $ prod_paudm
old_page_converted = np.random.binomial(1, P_old, n_old) $ old_page_converted
segmented_rfm['RFMScore'] = segmented_rfm.r_quartile.map(str) + segmented_rfm.f_quartile.map(str) + segmented_rfm.m_quartile.map(str) $ segmented_rfm.head()
autos.odometer_km.value_counts().sort_index()
print('{} / {} '.format(data['TIDF Compliance'].notna().sum(), len(data['TIDF Compliance']))) $ print('{:.3f} % '.format(data['TIDF Compliance'].notna().sum() * 100 / len(data['TIDF Compliance']))) $ data[data['TIDF Compliance'].notna()]['TIDF Compliance']
owns = repos[['id', 'owner_id', 'forked_from', 'created_at']] $ owns = owns.rename(columns={'id': 'repo_id', 'owner_id': 'user_id', 'forked_from': 'forked_from_repo_id'}) $ owns['owned'] = 4
surge.plot() $ plt.suptitle('Apple Quarterly Stock Price Surge > 100%') $ plt.xlabel('Year') $ plt.ylabel('% Surge')
com_grp.count()  # return panda DataFrame object
df['Avg_speed']= (df['Trip_distance'] / df['Trip_duration']) #miles/second $ df['Avg_speed']=df['Avg_speed']*3600  #miles/hour $ df['Avg_speed'].describe()
df.groupby("pickup_dow")["cancelled"].mean()
Xcnn = np.expand_dims(X_resampled, axis=2) $
cust_data._get_numeric_data().apply(lambda x: var_summary(x)).T
df = pd.DataFrame(np.random.randn(4, 3), index=index, columns=['A','B','C']) $ df
data = pd.DataFrame(np.arange(16).reshape((4, 4)), $                     index=['Ohio', 'Colorado', 'Utah', 'New York'], $                     columns=['one', 'two', 'three', 'four'])
autos=autos.drop('nrOfPictures',1)
code_n_name = dict(code_n_name[0:5]) # Adjust number of codes in the future. For development purpose, codes are limited to 5 
df_airports = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2011_february_us_airport_traffic.csv') $ df_airports.head()
df_data['VITIMAFATAL_BOOL'] = df_data.VITIMAFATAL.apply(yes_or_no_to_bool) $ df_data.head()
p_value = scipy.stats.chi2_contingency(limited_contingency)[1] $ print(p_value)
datatest.loc[datatest.surface_total_in_m2.isnull(),'surface_total_in_m2'] = datatest['surface_covered_in_m2']/datatest['covered/total'] $ del datatest['covered/total']
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new],[n_old, n_new], alternative='smaller') $ z_score, p_value $
print('Number of rows in dataset :: ',df.shape[0]) $ print('dataset information :: ') $ df.info()
ga.shape
logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = logit_mod.fit()
sorted = grouped.sort( grouped['count'].desc() ) $ sorted.show()
image_predictions_df[image_predictions_df.img_num != 1][0:5]
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Categorical(temp) $ temp_cat
display_code('models.py',[248,252])
train_nrows, val_nrows, test_nrows = train_small_sample.shape[0], val_small_sample.shape[0], test.shape[0]
wrd.shape[0]
yc_depart = yc_merged_drop.merge(departureZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner') $ yc_depart.shape
import test_package.print_hello_class_container                          # Import module that the class is in $ classified = test_package.print_hello_class_container.Print_hello_class  # Extract the class from the module and assign it to a variable $ print_hello_instance = classified()                                      # Instantiate the class $ print(print_hello_instance.name)                                         # Print a property of the instance $ print_hello_instance.print_hello_method_within_class()                   # Call a method of the instance
games_2017_sorted.loc[:, ["Team", "Opp", "Home", "Home.Attendance"]].head(10) # games are duplicated 
ma_8 = df['Close'].pct_change(n).rolling(window=8).mean() $ ma_13= df['Close'].pct_change(n).rolling(window=13).mean() $ ma_21= df['Close'].pct_change(n).rolling(window=21).mean() $ ma_34= df['Close'].pct_change(n).rolling(window=34).mean() $ ma_55= df['Close'].pct_change(n).rolling(window=55).mean()
df_prcp = pd.DataFrame(results, columns=['date', 'prcp'])
data = [] $ for f in fileList: $     with open(f) as data_file:    $         data.append(json.load(data_file)["results"])
df1.head(5)
bigdf.shape
m3 = np.round(m3, decimals = 2)  #rounding the decimal upto two points $ print("m3: ", m3)
sp500 = pd.read_csv('sp500.csv', index_col='Symbol', usecols=[0,2,3,7]) $ sp500.head()
df_mes['tpep_pickup_weekday'] = pd.DatetimeIndex(df_mes['tpep_pickup_datetime']).weekday $ df_mes['tpep_dropoff_weekday'] = pd.DatetimeIndex(df_mes['tpep_dropoff_datetime']).weekday
model.fit(x_train, fb_train.popular)
yhat = DT_model.predict(X_test) $ yhat
df1_clean.drop('test', axis=1, inplace=True)
log_user1 = df_log[df_log['user_id'].isin([df_test_user['user_id']])] $ print(log_user1)
np.mean(p_diffs > obs_diff)
final_df_test = final_df_test.join(n_features_test)
data_df.info()
pickle.dump(skills, open('./data/skills.pickle.dat', 'wb'))
top5_days= df['Unique Key'].resample('D').count().sort_values(ascending=False).head(5) $ top5_days
df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1]/100) for tup in pops_list] $ df_pop_ceb.plot(kind='scatter', x='Year', y='Total Urban Population') $ plt.show()
search['prefetch_m'] = search['prefetch'].apply(lambda x: 1 if x >= 0 else 0)
import pandas as pd $ import numpy as np $ h5 = pd.HDFStore('vstoxx_march_2014.h5', 'r')
logit = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','US','UK','US_x_ab_page','UK_x_ab_page']]) $ result2 = logit.fit()
pumashplc.head()
croppedFrame = cFrame[cFrame.Date < lastDay] $ print(croppedFrame.loc[cFrame['Client'] == 'AT&T']) $ croppedFrame.tail() $
non_align_1 = df.query('group == "treatment" and landing_page != "new_page"') $ non_align_2 = df.query('group != "treatment" and landing_page == "new_page"') $ non_align = non_align_1.count()[0] + non_align_2.count()[0] $ print('{} number of times new_page and treatment do not line up'.format(non_align))
rate["answer"].mean()
clf3 = Pipeline([ $     ('vectorizer', TfidfVectorizer(token_pattern=r'\w{2,}', ngram_range=(1,2), stop_words='english', sublinear_tf=True)), $     ('classifier', LinearSVC(C=1.0, penalty='l1', max_iter=1000, dual=False)), $ ]) $ svm_model = train(clf3, model_data['clean_description'], model_data['clean_titles_cat']) $
html_table = df.to_html() $ html_table
df.head()
response = requests.get(url)
df2 = pd.DataFrame(np.random.randn(2,3),columns=["Col1","Col2","Col3"]) $ df2
import pandas as pd $ pd.crosstab(test_set[1], predictions, rownames=['actuals'], colnames=['predictions'])
np.sort(df2.date.unique())
gmaps = googlemaps.Client(key='xxxxxxxxx')
ax =  datePlanGroupByCountPivot.plot(title = "Willingness to Share Individual Participant Data") $ ax.set_xlabel("The record first received in clinicaltrials.gov (month)") $ ax.set_ylabel("Frequency") $ ax.legend(title='') $ plt.show()
import sys $ !{sys.executable} -m pip install nltk
ethPrice = ethPrice.sort_values(by='date') $ ethPrice.set_index('date',inplace=True)
import matplotlib.pyplot as plt $ import datetime $ from matplotlib import style $ style.use('ggplot')
with_countries['CA_ab_page'] = with_countries['ab_page']* with_countries['CA'] $ ca_new_page = sm.Logit(with_countries['converted'], with_countries[['intercept', 'ab_page', 'CA_ab_page', 'CA']]).fit() $ print(ca_new_page.summary())
df2 = pd.read_csv('ab_cleaned.csv')
precip_data_df1.plot(x="date",y="Precipitation",kind="line",ax=None,legend=True, $                      title="Hawaii - Date vs precipitation ") $ plt.savefig("Images\datevsprecip_vsk.png") $ plt.show()
clean_archive[(np.isnan(clean_archive['in_reply_to_status_id']) != True) | (np.isnan(clean_archive['retweeted_status_id']) != True)]
kNN500.fit(X_extra, y_extra)
loc_date_str = soup.find('p', attrs={'class' : 'article-location-publishdate'}).get_text() $ loc_date_str
classification_df = rounds_df[['company_name','country_code','state_code','region','city','company_category_list', $                               'funding_round_type','funding_round_code','announced_on','raised_amount_usd', $                               'company_uuid','funding_round_uuid']].copy()
print(gridCV.best_params_) $ print(gridCV.best_score_)
df_proj[df_proj.duplicated(subset='ProjectId', keep=False)]
m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, $            dropout=0.1, dropouti=0.4, wdrop=0.5, dropoute=0.05, dropouth=0.3) $ m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ m3.load_encoder(f'adam3_20_enc')
X.stay.describe()
conf_gain = x.loc[x['pred'] > 0.75] $ conf_gain = pd.DataFrame(conf_gain, columns=['pred','pred_std', 'Gain +1d']) $ len(conf_gain.loc[conf_gain['Gain +1d'] == True])/len(conf_gain)
cityID = '249bc600a1b6bb6a' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Aurora.append(tweet) 
import statsmodels.api as sm $ convert_old = df2.loc[~treatment]['converted'].sum() $ convert_new = df2.loc[treatment]['converted'].sum() $ n_old = len(df2.loc[~new_page]) $ n_new = len(df2.loc[new_page])
deadline = pd.to_datetime('2016-08-01')
multi.handle.value_counts()
lat = ndvi_nc.variables['latitude'][:] $ lon = ndvi_nc.variables['longitude'][:] $ time = ndvi_nc.variables['time'][:] $ ndvi = ndvi_nc.variables['NDVI'][0,:,:] $ np.shape(ndvi)
df['water_year2'] = df['datetime'].apply(lambda x: x.year if x.month < 10 else x.year + 1)
train_data.head()
df = pd.read_csv('data/clean_data.csv') $ df = df.drop(['Unnamed: 0'], axis=1)   # I don't care for the first column that was imported
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,shuffle=False)
negative_list=list(support_or_not.loc[support_or_not["compound"]<=neg_threshold].text)
from pyspark.sql.types import * $ from pyspark.sql import SQLContext $ sc.stop() $ sc = pyspark.SparkContext(appName="ml") $ sqlContext = SQLContext(sc)
pats_chiefs_nov8_plays = winpct.loc[winpct['Game Title'] == 'Chiefs vs. Patriots ']
affair_yrs_married = pd.crosstab(data.yrs_married, data.affair.astype(bool)) $ affair_yrs_married
TEXT.vocab.stoi['the']
archive_clean.drop(['rating_numerator', $                     'rating_denominator'], $                   axis=1, inplace=True)
DataSet = toDataFrame(results) $ DataSet.head()
import pandas as pd $ pd.read_csv('network/mcortex_node_types.csv', sep=' ')
df_tests=[z[z.fk_loan==loan_test].groupby(['dcf']).payment.sum().reset_index() for z in [loan_principals,loan_payments,residuals,recoveries, repaid_loans_cash]]
twitter_archive_master[(twitter_archive_master['rating_denominator'] == 11) & (twitter_archive_master['rating_numerator'] == 9)].iloc[0].name
look_back_dt = dt.datetime.strptime(test_start_dt, '%Y-%m-%d %H:%M:%S') - dt.timedelta(hours=T-1) $ test = energy.copy()[test_start_dt:][['load']] $ test.head()
(p_diffs > act_diffs).mean()
S_2017 = active_2017_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']] $ S_2016 = active_2016_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']] $ S_2015 = active_2015_06.drop_duplicates('PropID')[['PropID', 'Lat', 'Lng']]
rural_df = pd.read_csv('county_rural.csv', dtype={'2015 GEOID':str}) $ rural_dict = dict(zip(rural_df['2015 GEOID'], rural_df['2010 Census \rPercent Rural'])) $ broadband['Rural_Pct'] = broadband['county_fips'].map(rural_dict)
intersections['avg_traffic_flow'].head()
df_activity_prediction = \ $     df_active_user_prediction[df_active_user_prediction['user_id'].isin(active_users)]\ $     .merge(df_active_user_metrics,on="user_id", how="left")
elms_all_0611 = elms_all_0604.append(elms_all, ignore_index=True) $ elms_all_0611.drop_duplicates('ACCOUNT_ID',inplace=True)
files = os.listdir("data")
instance = FindUnusualExamples(df,uniqueid) $ instance.parameterTemplate()
guess_ages = np.zeros((2,3)) $ guess_ages
df2 = pd.DataFrame(np.random.randn(7,3),columns=['col1','col2','col3']) $ df2
df_complete_a['name'].value_counts().to_csv('names.csv')
tweets_df.tweet_text.head(10)
df_merge.head()
df.isnull().values.any()#any missing value
url = "https://www.cgv.id/en/movies/info/17009800" $ request = urllib.request.Request(url) $ page = urllib.request.urlopen(request) $ soup = BeautifulSoup(page,"lxml") $ soup.find('div', class_='movie-info-title').string
df_h1b_ft_US_Y = df_h1b_ft_US_Y.drop(incorrect_naics_indices)
data.shape
graffiti.isnull().sum()
df = pd.DataFrame.from_csv("metoo_full_backup_3M.csv")
ip['p2_conf'].describe()
calls = pd.read_csv('./data/CallsRingCentral.csv')
drace_df.columns
store_items.fillna(method='ffill', axis=0) # filled with previous value from the column
encoder_model_loaded = load_model('encoder_model_inference.h5')
df_wb = pd.read_csv('https://assets.datacamp.com/production/course_1531/datasets/world_ind_pop_data.csv') $ print(type(df_wb)) $ print(df_wb.head())
np.exp(logit_mod_joined_result.params)
files = os.listdir(os.getenv('PUIDATA') + '/archives')
autos["odometer"] = autos["odometer"].str.replace("km", "") $ autos["odometer"] = autos["odometer"].str.replace(",", "")
autos["odometer_km"].value_counts()
S_lumpedTopmodel.decision_obj.bcLowrSoiH.options, S_lumpedTopmodel.decision_obj.bcLowrSoiH.value
train_df["created_year"] = pd.to_datetime(train_df["created"], coerce=True).dt.year $ train_df["created_month"] = pd.to_datetime(train_df["created"], coerce=True).dt.month $ train_df["created_day"] = pd.to_datetime(train_df["created"], coerce=True).dt.day
from sklearn.manifold import TSNE $ tsne = TSNE(n_components=2, random_state=0) $ smallest_author = 0  # Ignore authors with documents less than this. $ authors = [model.author2id[a] for a in model.author2id.keys() if len(model.author2doc[a]) >= smallest_author] $ _ = tsne.fit_transform(model.state.gamma[authors, :])  # Result stored in tsne.embedding_
pd.Series([2, 4, 6], index=['a','b','c'])
import csv $ data = pd.read_csv("McDonaldstweets.csv") $ display(data)
df_clean.reset_index(inplace=True)
Station = Base.classes.station $ Measurements = Base.classes.measurements
InfinityWars_PRED_df = pd.DataFrame(Y_TFIDF_PRED)
df - df.iloc[0]
df['PRCP'].sum()
df =pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv') $ df.head(2)
batters = df.batter $ names = playerid_reverse_lookup(batters) $ names['batter_name'] = names['name_first'] + ' ' + names['name_last']
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'])
if os.path.isfile(pickle_full): $     print("loading pickle") $     df = pd.read_pickle(pickle_full) $ else: $     raise Exception("Did you run 1 - Raw Data Visualisation?")
autos['odometer'].value_counts()
(null_vals - obs_diff).mean()
year_with_most_commits = ... 
df_img_predictions_copy.info()
rows = data[["t", "close"]].itertuples() $ rows = map(lambda r: (r.t, r.close, r.t+1), rows) $ list(rows)[:2]
scowl = load_from_txt('/Users/jeriwieringa/Dissertation/drafts/data/word-lists/SCOWL-wl/words.txt')
df["result"].plot() $ plt.show()
corn.count()
df1.shape
df_main.info()
pd.concat([msftA01.head(3),msftA02.head(3)])
test_X.head()
pd.set_option('display.max_colwidth', -1) $ sqlClient.get_jobs().head(100)
forecast_set = clf.predict(X_lately) $ len(forecast_set)
check_measurements = session.query(Measurements).first()
daily_sales.plot('date','dcoilwtico',kind='bar', color='r', figsize=(15,5))
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
save_n_load_df(joined, 'joined_elapsed_events.pkl')
df.to_excel("../../data/msft2.xlsx")
df_providers.shape $ idx = df_providers[ (df_providers['id_num']==id_num)].index.tolist() $ print('length of IDX',len(idx)) $ assert len(idx) > 0, 'Length of IDX is NULL' $ print( df_providers.loc[idx[0],'name'] ) $
plt.pie(ab_counts.first_name.values, autopct='%0.2f%%') $ plt.legend(['A', 'B']) $ plt.axis('equal') $ plt.savefig('ab_test_pie_chart.png') $ plt.show() $
logreg_sentiment.fit(Xs, y).coef_[0].round(3)
shows[['stemmed_keywords', 'keywords']].head(10)
from pysumma.hydroshare import hydroshare $ hs = hydroshare.hydroshare()
dummy_IntServ = pd.get_dummies(df['InternetService'], prefix='IntServ') $ dummy_Contract = pd.get_dummies(df['Contract'], prefix='Contract') $ dummy_PayMethod = pd.get_dummies(df['PaymentMethod'], prefix='PayMethod') $ dummy_Contract = pd.get_dummies(df['Contract'], prefix='Contract') $ print(dummy_IntServ.head()) $
soup.title.text
raw_df.head(2)
props.prop_name
S.decision_obj.stomResist.value = 'Jarvis' $ S.decision_obj.stomResist.value
appointments['Specialty'].loc[appointments['Provider'].isin(dr_ID)]= 'doctor' $ appointments['Specialty'].loc[appointments['Provider'].isin(RNPA_ID)] = 'RN/PA' $ appointments['Specialty'].loc[appointments['Provider'].isin(ther_ID)] = 'therapist'
lm_withsubID_export_path = cwd+'\\LeadGen\\Ad hoc\\SubID\\LM Sig Loans with SubID.xlsx' $ lm_withsubID.to_excel(lm_withsubID_export_path, index=False)
findNumbers = r'\d+' $ regexResults = re.search(findNumbers, 'not a number, not a number, numbers 2134567890, not a number') $ print(regexResults.group(0))
df3[['CA','UK']] =pd.get_dummies(df3['country'])[['CA','US']] $
tl_2050 /= 1000 $ tl_2050_norm = tl_2050 ** (10/11) $ tl_2050_norm = tl_2050_norm.round(1) $ tl_2050_alpha = tl_2050 ** (1/3) $ tl_2050_alpha = tl_2050_alpha / tl_2050_alpha.max().max()
obj = json.loads(rec) $ type(obj)
df.head()
apple_tweets2.to_pickle('../data/apple2.pkl')
count_by_insertid = Counter(df.insert_id) $ insertid_freq = {x : count_by_insertid[x] for x in count_by_insertid if count_by_insertid[x] > 1 } $ print('Duplicated insert_ids: {}'.format(len(insertid_freq.keys())))
lithlog.to_sql(con=engine, name='lithlog', if_exists='replace', flavor='mysql',index=False, chunksize=1000)
pandas_list_2d_rename = pandas_list_2d.rename(columns={0 : 'Name', 1: 'ID', 2 : 'State'}) $ print(pandas_list_2d_rename)
dataset.set_index(['customer_id']) $ dataset.loc[dataset['customer_id'] == "450e1c2cbd21687780153995f1be0c23"]
sources = data_set["source"].value_counts()[:5][::-1] $ plt.barh(xrange(len(sources)), sources.values) $ plt.yticks(np.arange(len(sources)) + 0.4, sources.index) $ plt.show()
retailDf = sqlContext.sql(query) $ retailDf.registerTempTable("retailDf") $ sqlContext.sql("select * from retailDf limit 10").toPandas()
ocsvm_tfidf.fit(trump_tfidf, y = y_true_tfidf) $ prediction_tfidf = ocsvm_tfidf.predict(test_tfidf) $ prediction_tfidf
import numpy as np $ data = list(np.ravel(calc_temps('2012-02-28', '2012-03-05'))) $ data
print fullDf.shape[0] $ fullDf.region.value_counts()
GBR=GradientBoostingRegressor(alpha=0.001,n_estimators=200, random_state=34)
sum(df2.duplicated())
crime = crime.dropna(axis = 0, subset=['Sex'])
sns.heatmap(viscorr, $  cmap = 'Blues')
merged.groupby(["contributor_firstname", "contributor_lastname"]).amount.sum().reset_index().sort_values("amount", ascending=False)
df_joy.hist() $ plt.show()
type(dataset[['customer_id','order_id']].values)
clinton_df['source'].value_counts().plot.barh()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\FourFiles.zip" $ stories_zip = zipfile.ZipFile('C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\FourFiles.zip') $ print("",stories_zip.namelist()) $ stories_zip.close()
df = df.iloc[3:] $ df
df3['intercept'] = pd.Series(np.zeros(len(df3)), index=df3.index) $ df3['ab_page'] = pd.Series(np.zeros(len(df3)), index=df3.index)
y_hat = model.predict(X)
df2[df2.duplicated(["user_id"])]
vectorizer_method1 = TfidfVectorizer(tokenizer=lemmatize,stop_words="english",smooth_idf=True,norm=None,vocabulary=voca_tf) $ cty = data['text'] $ corpus = cty.copy() $ tfs_method1 = vectorizer_method1.fit_transform(corpus)
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df2['group'])
actual_new = df2.query('converted == 1 and landing_page=="new_page"').count()[0]/n_new
aggregates.info()
hourly_volume.plot(kind='bar')
df.loc[:, "last_name"]
twitter_archive_full[twitter_archive_full.favorite_count == 143537][ $     ['tweet_id', 'stage','favorite_count']]
obj3 = pd.Series(sdata)
fe.stat(lossprob)
data_issues_csv=pd.read_csv('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-issues.csv')
df_combined['won'] = (df_combined['home_score'] - df_combined['away_score']) > 1 $ df_combined.drop(['date', 'city', 'country','home_score','away_score'], axis=1, inplace=True)
df_temp = df_measurement[['station','tobs']].groupby(['tobs']).min() $ df_temp.head(1) $ df_temp.tail(1)
data_df.describe()
support[["amount", "committee_name_x"]].sort_values("amount", ascending=False).head(20)
fig, ax = plt.subplots(nrows = 1, ncols = 1) $ chinadata.plot(ax = ax,) # So this ploted gdp growth using the axes generated above.... $ ax.set_title("Key Economic Indicator Growth", fontweight = "bold") $ plt.show()
df=pd.read_csv('../reddit-dataset/gaming_minecraft.csv')
df.dtypes
df.values # numpy.ndarray
'issid' in df_protest.columns
test = np.where(predicted_probs_first_measure > .2, 1,0) $ print(metrics.confusion_matrix(actual_value_second_measure, test)) $ print(metrics.recall_score(actual_value_second_measure, test)) $ print(metrics.recall_score(actual_value_second_measure, predicted_outcome_first_measure))
df = sm.datasets.get_rdataset("Guerry", "HistData").data   # this is a familiar Pandas dataframe $ df.head()
sorted_bigram = sorted(bigrams_fd.items(), reverse = True, key=lambda x: x[1])
predict_actual_df.columns
posts = pickle.load(open('data/posts_with_age_gender.dat', 'rb')) $ comments = pickle.load(open('data/comments_with_ratings_df.dat', 'rb'))
twitter_archive_master = twitter_archive_master.drop(['tweet_id'],axis = 1)
test_tokens = testset['tokens'].tolist() $ test_corpus = createCorpus(test_tokens)
column_check = inspector.get_columns('measurement') $ for check in column_check: $     print(check['name'],check['type'])
obs_diff_control = df2.query('group == "control"').query('converted == 1')['user_id'].count() / df2.query('group == "control"')['user_id'].count() $ obs_diff_control
model.accuracy('./datasets/questions-words.txt')
(p_diffs>obs_diff).mean()
sns.distplot(df['age'], kde= False) $ plt.axvline(df['age'].mean(), color='k', linestyle='dashed', linewidth=1) $ plt.xlabel("Post age by minutes") $ plt.ylabel("Frequency");
X_train.head(10)
props.prop_name.value_counts().reset_index()
news_title_docs_high_freq_words_df = pd.read_pickle(news_title_docs_high_freq_words_df_pkl) $ with pd.option_context('display.max_colwidth', 100): $     display(news_title_docs_high_freq_words_df)
historical_scores['weighted_sum'].plot()
is_08A = (restaurants["VIOLATION CODE"] == "08A") $ inspections08A = restaurants[is_08A] $ inspections08A["DBA"].value_counts()[:10]
xml_in_sample.shape
plt.hist(p_diffs) $ plt.axvline(x = real_diff, color='red')
tmp_cov_2 = tmp_cov_1.where(idx) $ tmp_cov_2
rm_SPY = df['SPY'].rolling(window=20).mean()
np.exp(log_mod_country_results.params)
df2[['CA','UK','US']] = pd.get_dummies(df2['country'])
mentions_df_begin = pd.DataFrame(mentions_begin,columns=["epoch","src","trg","lang","text"]) $ print(len(mentions_df_begin))
from sklearn.metrics.pairwise import cosine_similarity $ dist = 1 - cosine_similarity(X2)
pc = pd.DataFrame(tt1_ok.groupby('VIOLCODE').size()) $ pc.columns=['count'] $ pc_fl = pc[pc['count']>=100].sort('count', ascending = False) $ pc_fl['VIOLATION_PROB'] = pc_fl['count'] / sum(pc_fl['count']) $
data_ms.dtype.names
df2.head()
pd.set_option('display.float_format', lambda x: '%.3f' % x)
poverty.tail(10)
df_os.head()
iplot((monthly_mean - merged_monthly_mean).iplot(asFigure = True, vline=['2017/09', '2017/03', '2016/09', '2016/03', '2015/09', '2014/12', '2014/04', '2013/10'], dimensions=(750, 500)))
props.info()
SAEMRequest = requests.get(saem_women) $ print(SAEMRequest.text[:100])
for_plt = tweets.set_index('created_at', drop=False, inplace=False) $ for_plt.created_at.groupby(pd.Grouper(freq='H')).count().plot(kind='bar', figsize=(20, 6)) $
df4[['new','old']] = pd.get_dummies(df4['landing_page']) $ df4.head()
gs_rfc_over = GridSearchCV(pipe_rfc, param_grid=params_rfc, cv=ts_split_over, scoring='roc_auc') $ gs_rfc_over.fit(X_train, y_train_over)
yhat_prob = lr.predict_proba(X_test) $ yhat_prob
converted = df.query('group == "treatment" and converted == 1')['user_id'].count() $ total = df.query('group == "treatment"')['user_id'].count() $ new_conv_rate = converted / total $ new_conv_rate
priors_reordered = priors_product[priors_product['reordered'] == 1] $ priors_reordered.head()
new_fan = questions.loc[questions['years_attend'] == 0] $ return_fan = questions.loc[questions['years_attend'] != 0]
df_control = df2[(df2['group'] == 'control')] # create new DF with only control group $ df_control['converted'].mean() #probablility of conversion for  is 0.1203863045004612
db = client.insight_database $ collection = db.posts
df = pd.DataFrame({'data1' : np.random.randn(5), $                    'data2' : np.random.randn(5)}) $ df
tweet_full_df['rating_denominator']=tweet_full_df['rating_denominator'].astype(float)
subprocess.check_output('python dist_me.py',shell=True)
cluster = cluster.dropna()
dsg.getncattr('featureType')
import re $ df2['rx_requested'] = re.sub(r'[^\w]', ' ', df2['rx_requested'])
run txt2pdf.py -o"2018-06-19 2015 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2015 FLORIDA HOSPITAL Sorted by payments.txt"
new_page_converted = np.random.choice(2, size=145310, replace=True, p=[0.8804, 0.1196])
mysom.plot_hit_histogram(data.values) $
example_measures = np.array([[4,2,1,1,1,2,3,2,1],[4,2,1,1,1,2,3,2,1]]) $ example_measures = example_measures.reshape(len(example_measures), -1) $ prediction = clf.predict(example_measures) $ print(prediction)
tweet_data_copy.info()
containers[0].find("span",{"class":"date"}).contents[0]
np.random.random((3,4))
ts.tshift(5,freq="H")
tweets.info()
Image("/Users/jamespearce/repos/dl/data/dogscats/train/dog.9126.jpg")
bar=sess.get_intraday_bar_data('ibm us equity', event_type='TRADE', bar_interval=60, $                                start_datetime=datetime.datetime.now() - datetime.timedelta(days=7))
avg_day_of_month15.to_excel(writer, index=True, sheet_name="2015")
bitcoin_github_issues_df[['title', 'html_url', 'created_at','updated_at']].head()
df = pd.DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])
df.loc[df['Match']=='No_Match'].to_csv('geocoding_failures.csv', encoding='utf-8')
data['cumreturns']=1+(data['strategy'].cumsum()) $ data.head()
yc200902_short['Trip_Pickup_DateTime'] = pd.to_datetime(yc200902_short['Trip_Pickup_DateTime'])
order_pay_num = pd_data.loc[lambda pd_data: pd_data.pay > 0,:].groupby(['appCode','orderType','year','month'])['pay'].count()
clf = svm.SVR()
df.shape
pres_df['location'].unique()
model = ols("happy ~ age + income", training).fit() $ model.summary()
print('Actual price of the 1st house: {:.1f}'.format(test_data['price'][0]))
out_df.to_csv('stacking_gbm_quant.csv', index=False)
X_test['Text_Tokenized'] = X_test.Text.apply(process) $ X_test.head()
df2 = pd.concat([df2a, df2b], axis=0)
DatePrcp = session.query(Measurement.hw_date, Measurement.prcp)
twitter_archive_master = clean_archive.merge(data_tweets) #automatically merge on the intersection of the columns $ twitter_archive_master= twitter_archive_master.merge(clean_predictions)
unsegmented_users.groupby(['course_id', 'segment']).user_id.count()
os.makedirs('data/tmp', exist_ok=True) $ scaled.to_feather(file_name)
from scipy.stats import norm $ critical_value = norm.ppf(1-0.05) $ critical_value
df.info()     # looking on the details of the information and memory size
example1_df.registerTempTable("world_bank")
dataset = pd.read_csv("inner_join_2018-06-04.csv", $                       dtype=str, $                       keep_default_na=False) \ $             .drop_duplicates()
twitter_archive_clean.info()
cursor.execute("DELETE FROM person where id = 7;") $ pd.read_sql_query("SELECT * from person;", conn, index_col="id")            
db.op_count(db.filter(load_buf_array, 'a is not null and attribute_no=2513'))[:]
print(df_users['bio2'].value_counts())
ins['new_date'] = ins['date'].apply(convert) $ ins['year']     = ins['new_date'].apply(lambda x: x.year) $
arparams = np.r_[1, -arparams] $ maparams = np.r_[1, maparams] $ nobs = 250 $ y = arma_generate_sample(arparams, maparams, nobs)
head = "https://wwww.codechef.com/users/" $ var = user $ URL = head + user $ page  = requests.get(URL) $ soup = BeautifulSoup(page.content,'html.parser')
sorted_o = df.sort_values(['customer_id', 'created_at_date']).groupby(['customer_id', 'order_id'])['created_at_date'].max() $ interval_c = sorted_o.groupby('customer_id').diff().groupby('customer_id').max().dt.days.rename(('created_at_date', 'diff', 'interval'))
df_graph = new_df_dummies[new_df_dummies['click_rec_menues']==1] $ df_graph.head()
Test_extra.head()
pprint.pprint(parser.HHParser.format)
x = range(len(df.index)) $ x
ticket6 = data_df.clean_desc[9] $ parsed6 = nlp(ticket6) $ for token in parsed6: $     print token.text $
import sqlite3 $ conn = sqlite3.connect("database.db") $ cursor = conn.cursor() $
paragraphs = soup.find_all('p', ) $ paragraphs
urban_ride_total = urban_type_df.groupby(["city"]).count()["ride_id"] $ urban_ride_total.head()
df['ruling'].unique()
predictions.show(5)
!python3 -m nltk.downloader all
print(df.size) $ df.size == df.shape[0] * df.shape[1]
AAPL.head(5)
ridg = Ridge() $ ridg.fit(X_train, y_train) $ predicted_ridg_y = ridg.predict(X_test) $ print("Ridge Mean Squared error:",mean_squared_error(y_test, predicted_ridg_y)) $ print("Ridge R2 Score:",ridg.score(X_test, y_test))
! find -type f -name "flight_1_5_price_2017-05-10_2017-06-10*.txt"
%pwd $ os.chdir(os.getcwd() + os.sep + "STARD")
len(twitter_archive_df['retweeted_status_id'].unique())
for tables in files_to_manage: $     new_data[tables] = new_data[tables][new_data[tables].GAME_ID.isin(on_date_games)].sort_values("GAME_ID") $     new_data[tables]["GAME_ID"] = new_data[tables]["GAME_ID"].astype(int)
import pandas_datareader.data as web $ df = web.DataReader("tran_sf_railac", 'eurostat') $ df
h2o.ls()
abc.head()
reddit['Subreddits'].sort_values(ascending=False).head(25) #just seeing the top 25 subreddits
week = df2.loc[idx.get_indexer(df1.By), 'Week'] $ week[0:10]
np.random.seed(123456) $ bymin = pd.Series(np.random.randn(24*60*90),pd.date_range('2014-08-01','2014-10-29 23:59',freq='T')) $ bymin
sns.countplot(x='country', data=data_for_model, hue='final_status')
g_filtered.head()
from sklearn.ensemble import RandomForestClassifier $ from sklearn.linear_model import LogisticRegression
age.iloc[1]
exiftool -csv -createdate -modifydate cisrol12/cycle1_MVI_0032.mp4 cisrol12/cycle1_MVI_0033.mp4 cisrol12/cycle1_MVI_0034.mp4 cisrol12/cycle2_MVI_0035.mp4 cisrol12/cycle2_MVI_0036.mp4 cisrol12/cycle2_MVI_0037.mp4 cisrol12/cycle2_MVI_0038.mp4 cisrol12/cycle2_MVI_0039.mp4 cisrol12/cycle2_MVI_0042.mp4 cisrol12/cycle2_MVI_0043.mp4 cisrol12/cycle2_MVI_0044.mp4 cisrol12/cycle2_MVI_0045.mp4 cisrol12/cycle2_MVI_0046.mp4 cisrol12/cycle2_MVI_0047.mp4 cisrol12/cycle2_MVI_0048.mp4 cisrol12/cycle2_MVI_0049.mp4 cisrol12/cycle2_MVI_0050.mp4 cisrol12/cycle2_MVI_0051.mp4 cisrol12/cycle2_MVI_0052.mp4 cisrol12/cycle2_MVI_0053.mp4 cisrol12/cycle2_MVI_0054.mp4 cisrol12/cycle2_MVI_0055.mp4 cisrol12/cycle4_MVI_0075.mp4 cisrol12/cycle4_MVI_0076.mp4 cisrol12/cycle4_MVI_0078.mp4 cisrol12/cycle4_MVI_0079.mp4 cisrol12/cycle4_MVI_0080.mp4 cisrol12/cycle4_MVI_0081.mp4 cisrol12/cycle4_MVI_0082.mp4 cisrol12/cycle4_MVI_0083.mp4 cisrol12/cycle4_MVI_0084.mp4 cisrol12/cycle4_MVI_0085.mp4 cisrol12/cycle4_MVI_0086.mp4 cisrol12/cycle4_MVI_0089.mp4 cisrol12/cycle5_MVI_0095.mp4 cisrol12/cycle5_MVI_0096.mp4 cisrol12/cycle5_MVI_0097.mp4 cisrol12/cycle5_MVI_0098.mp4 cisrol12/cycle5_MVI_0100.mp4 cisrol12/cycle5_MVI_0101.mp4 cisrol12/cycle5_MVI_0102.mp4 cisrol12/cycle5_MVI_0106.mp4 > cisrol12.csv
stellamccartney_ng.plot(kind='barh', figsize=(20,16));
np.median(spmL)
above_5percent_autos.groupby("brand").agg({"price":np.mean}).sort_values("price",ascending=False)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep =' ') $ df.head(5)
avg_retweet = DataSet['tweetRetweetCt'].mean() $ median_retweet = DataSet['tweetRetweetCt'].median() $ print("Average number of retweets =", avg_retweet) $ print("Median number of retweets =", median_retweet)
lm = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','US']]) $ results_lm = lm.fit() $ results_lm.summary()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative = 'smaller') $ z_score,p_value
S.modeloutput_obj.remove_variable('scalarCanopyTranspiration')
Year3_df = Distribution_df.drop(Distribution_df.index[:24]) $ Year1_df = Distribution_df.drop(Distribution_df.index[12:]) $ Year2_df = Distribution_df.iloc[12:24]
autos.loc[autos["offer_type"] == 'Gesuch']
by_year = candidates.groupby("election_year").size().reset_index()
df2=df2.apply(qnt)
df1, df2, df3, df4, df5 = (pd.DataFrame(rng.randint(0, 1000, (100, 3))) $                           for i in range(5))
data_final.head(30)
df['date'] =  pd.to_datetime(df['date'], format='%m/%d/%y') $ df
cityID = '01fbe706f872cb32' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Washington.append(tweet) 
import pandas as pd $ t=trumpint.describe() $ c=cliint.describe() $ tc=pd.concat([t, c],keys=['Trump','Clinton'], axis=1 ) $ tc
delte = [] $ for i in or_list: $     if(i not in and_list): $         delte.append(i) $
ser_obj = pd.Series(range(10, 20)) $ print(type(ser_obj))
df_range = select_range(df, 10, 60) $ df_range.sort_values(by = 'prob_1', ascending = False)#.iloc[100:200]
from pandas.tseries.offsets import BDay $ pd.date_range('2015-07-01', periods=5, freq='B')
def return_sentiment_scores(keyword): $     sentence = ' '.join(df_tweets[df_tweets.keyword == keyword].text.values) $     snt = analyzer.polarity_scores(sentence) $     return(snt)
dreamRequest = requests.get(dreamophone_url) $ dreamSoup = bs4.BeautifulSoup(dreamRequest.text, 'html.parser') $ with open('dreamFormat.html', mode='w', encoding='utf-8') as f: $     f.write(dreamRequest.text)
joined_train_df.to_pickle(slowdata+'joined_train_df.pkl') $ joined_test_df.to_pickle(slowdata+'joined_test_df.pkl')
pd.Timestamp('17:55')
autos = autos[autos['price'] < 500000] $ autos.price.describe()
old_columns_names = autos.columns
df.columns
print(props.info())
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, (1 - p_new)]) $ p_new_sim = new_page_converted.mean() $ p_new_sim
predY = bikescore.run(tsX[0:5,:]) $ trueY=str(pd.DataFrame(data={"Actual Values":np.squeeze(np.reshape(tsY[0:5], newshape= [-1,1]), axis=1)}))
lsi_tf.print_topics(10)
cols = ['tweetId', 'screenName', 'text', $         'replyToSN', 'truncated',  'isRetweet', 'retweetCount', 'favoriteCount',    $         'created', 'longitude', 'latitude', 'sentiment_score', 'sentiment'] $ tweets[cols].to_csv('Data/tweets3.csv', index=False, encoding='utf-8')
paragraphs = re.findall(r'<p>(.*)</p>', html) $ paragraphs
df_protest.describe()
archive_clean['name'].replace('None',np.nan,inplace= True)
dta.info()
df.hist(column = 'founded_year',bins = 20)
yc_new.head()
from sklearn.model_selection import cross_val_score, StratifiedKFold $ from sklearn.tree import DecisionTreeClassifier $ from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier $ from sklearn.linear_model import LogisticRegression $ from sklearn.preprocessing import StandardScaler $
bands.to_csv('../data/bands.csv')
tweet_scores.head()
print('Top Ten Paths:') $ paths_df.show(truncate=False, n=10) $ paths_df.take(10)
print('Last run:', datetime.datetime.utcnow(), 'GMT')
shots_df.info()
Lab7 = Lab7.rename(columns = {'Entity':'ENTITY','records lost':'RECORDS LOST' })
jdf = avg_pd.join(ifs.set_index('Journal'),how='inner',on='Journal') $ jdf.drop_duplicates(inplace=True)
df_gt = pd.read_excel('../../data/essentials/Report By Category.xls', index_col=[0])
dat_hcad_zip.drop_duplicates(subset='blk_range',keep="last",inplace=True) $ dat_hcad_zip.columns = ['blk_range','zip'] $ dat_hcad_zip.shape
optim_etf_returns = generate_weighted_returns(returns, optim_etf_weights) $ optim_etf_cumulative_returns = calculate_cumulative_returns(optim_etf_returns) $ helper.plot_benchmark_returns(index_weighted_cumulative_returns, optim_etf_cumulative_returns, 'Optimized ETF vs Index') $ optim_etf_tracking_error = helper.tracking_error(index_weighted_cumulative_returns, optim_etf_cumulative_returns) $ helper.plot_tracking_error(optim_etf_tracking_error, 'Optimized ETF Tracking Error')
total = df2[df2['group']=='treatment']['user_id'].unique().shape[0] $ conv = df2[((df2['converted']==1) & (df2['group']=='treatment'))]['user_id'].unique().shape[0] $ probab = conv/total $ print(probab)
prob = df2.groupby('group') $ prob_control = prob.mean()['converted']['control'] $ print("The probability of an individual converted, being in the 'control' group is - {}".format(prob_control))
from sklearn.decomposition import LatentDirichletAllocation
dj_posts = [] $ for post in dj_content: $     dj_posts.append(post.text) $ postsDF = pd.DataFrame({'post' : dj_posts}) $ print(postsDF)
test_data.vehicleType.fillna('kleinwagen', inplace = True)
plt.scatter(y=dftouse.opening_gross, x=dftouse.star_avg)
pd.__version__
class_codes.head()
plt.rcParams['figure.figsize'] = (15, 5) $ tzs.plot(kind='bar') $ plt.xlabel('Timezones') $ plt.ylabel('Tweet Count') $ plt.title('Top 10 Timezones tweeting about #Australia')
yc_merged_drop.head()
lr_df[['control', 'ab_page']] = pd.get_dummies(lr_df['group']) $ lr_df = lr_df.drop('control', axis = 1) $ lr_df['intercept'] = 1.0 $ lr_df.head()
shows.iloc[row:row+50,:]
url = "https://mars.nasa.gov/news/" $ response = requests.get(url)
stock.columns
y_test = np.where(y_test == 'Charged Off', 1, 0)
run txt2pdf.py -o '2018-06-22 2013 FLORIDA HOSPITAL Sorted by payments.pdf'  '2018-06-22 2013 FLORIDA HOSPITAL Sorted by payments.txt'
X = telecom3 $ y = telecom2['churn'] $ tel = pd.concat([X, y], axis=1) $ tel.head() $
t2.tweet_id=t2.tweet_id.astype(str)
week6 = week5.rename(columns={42:'42'}) $ stocks = stocks.rename(columns={'Week 5':'Week 6','35':'42'}) $ week6 = pd.merge(stocks,week6,on=['42','Tickers']) $ week6.drop_duplicates(subset='Link',inplace=True)
train_small_data.reset_index(drop=True, inplace=True) $ val_small_data.reset_index(drop=True, inplace=True)
print(airquality_pivot.head())
(df2['converted'] == 1).mean()
data['results'].keys()
regressie = linereg.predict(y) $ plt.scatter(x,y) $ plt.plot(x,regressie, color='blue') $ plt.xlabel('lat') $ plt.ylabel('lon')
df_new = df2.set_index('user_id').join(countries_df.set_index('user_id')) $ df_new.head()
dropCols = (['CustomerID', 'Invest', 'Educ', 'MARITAL', 'TimeYears', 'lasttrans', 'current', 'Monetary_score']) $ joined.drop(dropCols, axis=1, inplace=True) $ joined.head()
user.iloc[1, 1:5]
error_spark = spark.createDataFrame(errors, $                                verifySchema=False) $ del errors $ error_spark.printSchema()
start = "2005-01-01" $ end = "2018-06-31" $ trading_days = fk.get_monthly_last_trading_days(start=start, end=end) $
dummies_df = pd.get_dummies(combined_df4, columns=['Ward', 'paymeth_code']) $ dummies_df = dummies_df.drop(['vo_propdescrip','llpg_usage'],axis=1)
df2 = df2.drop_duplicates(subset=['user_id'], keep='first') $ df2[(df2.user_id == 773192)]
import logging $ import daiquiri $ daiquiri.setup(level=logging.INFO) $ logger = daiquiri.getLogger("bootcamp") $ logger.info("It works and logs to stderr by default with color!")
spreadsheet = '1LTXIPNb7MX0qEOU_DbBKC-OwE080kyRvt-i_ejFM-Yg' $ wks_name = 'CleanedData' $ d2g.upload(df_dn,spreadsheet,wks_name,col_names=True,clean=True)
nlp = spacy.en.English() $ df['parsed'] = df.text.apply(nlp)
print(metrics.classification_report(actual_value_second_measure, predicted_outcome_first_measure))
sns.plt.plot(range(10))
df_ad_airings_4.head(3)
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).head(5)
d = pd.date_range('11-Sep-2017', '17-Sep-2017', freq='2D') $ len(d[d.isin(pd.to_datetime(['12-09-2017', '15-09-2017']))])
frac_conv = df[df['converted'] == 1].shape[0] / rows $ frac_conv
avg_preds.shape
len([earlyPair for earlyPair in BDAY_PAIR_qthis.pair_age if earlyPair < 3])
df.loc[1, "last_name"]
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], $                                               nobs=[n_new, n_old],alternative='larger') $ print("z-score:", z_score,"\np-value:", p_value)
clf = LogisticRegression(class_weight="balanced", solver="newton-cg", tol = 1e-3) $ clf.fit(X, y) $ predictions = clf.predict(test_data_features) $ output = pd.DataFrame( data={"id":test["id"], "rating":predictions} ) $ output.to_csv( "regression.csv", index=False, quoting=3 )
aux = df2[df2.group == 'control'] $ aux[aux.converted == 1].shape[0] / aux.shape[0]
meals_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/meals.csv')['Body'].read().decode('utf-8') $ meals = pd.read_csv(StringIO(meals_csv_string), header=0)
old_page_converted = np.random.choice([0, 1], size=n_old, p=[1-p_new, p_new])
blocksBID.to_csv("../Data/block_dummies_BIDS.csv", encoding='utf8')
user = user.set_index("screen_name") $ user.head(3)
fiddy_states = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states')
allqueryDF.shape
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page']=='old_page')&(df2['converted'])]) $ convert_new = len(df2[(df2['landing_page']=='new_page')&(df2['converted'])]) $ print(f"convert_old: {convert_old}\nconvert_new: {convert_new}\nn_old: {n_old}\nn_new: {n_new}")
df_characters.sort_values('num_lines', ascending=False).head(10)
df['Source'][df['Source']=='Buffer'].count()
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_pred), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
df_mes[(df_mes['extra']!=0)&(df_mes['extra']!=0.5)&(df_mes['extra']!=1)&(df_mes['extra']!=1.5)].shape[0]
tweets_df = tweets_df[tweets_df['Text'].apply(lambda x: ner.is_tweet_about_country(x, 'FR'))] $ tweets_df
h_aux=[] $ for i in range(0,l2): $     if i not in seq: $         h_aux.append(histories_b5[i]) $ col.append(np.array(h_aux))
print('Very short time travels number %d.'%len(train[train['duration'] <= 60]))
df.head(5)
surveys_df = pd.read_csv("surveys.csv", keep_default_na=False, na_values=[""]) $ species_df = pd.read_csv("species.csv", keep_default_na=False, na_values=[""])
autos[['date_crawled', 'ad_created', 'last_seen']].head()
df.head()
newdf.groupby([newdf.Hour_of_day]).Trip_distance.mean()
arma_mod30 = sm.tsa.ARMA(dta_713, (3,0)).fit(disp=False) $ print(arma_mod30.params)
slCasesSuspected = slFullDf.loc['new_suspected'] $ slCasesProbable = slFullDf.loc['new_probable'] $ slCasesConfirmed = slFullDf.loc['new_confirmed'] $ slCasesSuspected.info()
active = session.query(Measurement.station, func.count(Measurement.station)).\ $     group_by(Measurement.station).\ $     order_by(func.count(Measurement.station).desc()).all() $ for station in active: $     print(f"{station[0]} with {station[1]} entries.")
twitter_archive.loc[(twitter_archive['name'].str.islower()) & (twitter_archive['text'].str.contains('name is'))]
start = dt.datetime(2004,1,1) $ end = dt.datetime(2017,6,6) $ data_source = 'yahoo' $ tickerdata = pdr.DataReader(ticker, data_source, start, end)
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth, wait_on_rate_limit = True) $     return api
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.choice([0,1], size = n_new, p=[1-p_new, p_new]) $     old_page_converted = np.random.choice([0,1], size = n_old, p=[1-p_old, p_old]) $     p_diffs.append(new_page_converted.mean()-old_page_converted.mean())
joined = joined[joined.Sales!=0]
archive_df.shape,image_file.shape,tweet_df.shape
from pandas.io.json import json_normalize $ user_info = json_normalize(tweets_clean['user']) $ user_info.columns
plt.plot(n_new_temp_df['Time'], n_new_temp_df['high'], n_new_temp_df['Time'], n_new_temp_df['close'])
df = pd.read_sql('SELECT city_id, count(*) as address_cnt FROM address GROUP BY city_id ORDER BY address_cnt DESC;', con=conn) $ df.head()
import scrapy $ class XRateItem(scrapy.Item): $     OneUnitinDollar = scrapy.Field() $     OneDollarEquivalent = scrapy.Field() $     Type = scrapy.Field() $
iplot(data.groupby('user.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, kind='bar', dimensions=(750, 500)))
tia[tia['duration']>dt.timedelta(30)]
sns.lmplot('FLOW', 'REVENUE', log_df3) $ plt.show()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], $                                               [n_old, n_new], alternative='smaller') $ z_score, p_value
df_daily = df_daily.groupby(['STATION','DATE']).sum() $ df_daily.head()
prcp_gb_query = session.query(func.strftime(measurements.date), func.avg(measurements.prcp)).\ $ filter(measurements.date <= last_date, measurements.date>= last_date-relativedelta(months=12)).\ $ group_by(measurements.date).all() $ prcp_gb_query
x_data = new_job_descriptions.summary_text $ y_labels = new_job_descriptions.cleaned_job_title
iv = options_frame[options_frame['Strike'] == 130.0] $ iv_call = iv[iv['OptionType'] == 'call'] $ iv_call[['Expiration', 'ImpliedVolatilityMid']].set_index('Expiration').plot()
df['full_text'] = df['full_text'].apply(lambda x: x.replace('\r', ' ')) $ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\n', ' ')) $ df['full_text'] = df['full_text'].apply(lambda x: x.replace('\t', ' '))
pct = pd.merge(pct, graf_counts2, left_on='Precinct', right_on='precinct')
retweet_sum = twitter_df_merged.groupby('date', as_index = False).sum() $ retweet_sum = retweet_sum.sort_values(by = "date", ascending=False) $ retweet_sum['date'] = pd.to_datetime(retweet_sum['date'], format = "%m-%d-%Y") $ retweet_sum = retweet_sum.sort_values(by = "date") $ retweet_sum.set_index('date', inplace=True) $
ser2 = pd.Series(np.arange(3.), index=['a', 'b', 'c'])
print('Probability that an individual received the new page:',(df2['landing_page'] == 'new_page').mean())
filtered_df[['accommodates','bedrooms']].corr()
autos = autos[autos["price_dollars"].between(1,4000000)]
data.describe()
pd.Period('2012-1-1 19:00', freq='H')
len(df2[(df2['landing_page'] == 'new_page')]) / df2.shape[0]
p_new = df2['converted'].mean() $ p_new
mars_facts_df.columns = ['Characteristic','Data'] $ mars_df_table = mars_facts_df.set_index("Characteristic") $ mars_df_table
inoroffseason_teams = ALLbyseasons.groupby(["InorOff", "Team"]) # Groups by In or Offseason, then by team
df_onc_no_metac = df_onc_no_metac.drop(columns = ['METAC_SITE_NM1', 'MET_DATE1'])
df["message_tokens"] = df.message.apply(word_tokenize) #Tokenize and remove stop words first. $ df.loc[:,"message_tokens"]= df['message_tokens'].apply(lambda x: [item for item in x if item not in stopword_list])
df2[(df2.user_id == 773192)]
dataframe.loc[rows,columns] # .loc can take row ID $ dataframe.iloc[row_ix,columns_ix]
avg_arrests = stadium_arr.pivot_table(index='season', values='arrests', aggfunc='mean') $ avg_arrests
confuse = pd.crosstab(gbctest['failure'], gbctest['predicted_failure']) $ confuse
tweets.plot(x='created_at', y='chars')
merged.info()
pd.get_dummies(df[cols], prefix=['colA','colB'])
wlactivity = pd.read_csv(route+'wlactivity.txt')
revs.head()
tweets_master_df['tweet_id'] = tweets_master_df['tweet_id'].astype(object, copy=False) $ tweets_master_df['timestamp'] = pd.to_datetime(tweets_master_df['timestamp']) $ tweets_master_df['retweet_count'] = tweets_master_df['retweet_count'].astype(int, copy=False) $ tweets_master_df['favorite_count'] = tweets_master_df['favorite_count'].astype(int, copy=False)
df.groupby('site_admin')['login'].nunique()
cur = conn.cursor() $ cur.execute('UPDATE actor SET first_name = \'HARPO\' WHERE first_name ilike \'Groucho\' and last_name ilike \'Williams\';') $
con.commit()
df_group_by.head()
df0 = df[ (df['group'] == 'control') & (df['landing_page'] == 'old_page')  ] $ df1=  df[(df['group'] == 'treatment') & (df['landing_page'] == 'new_page')] $ df2=pd.concat([df0,df1]) $ unique_users_2=df2['user_id'].unique().shape[0] $ df2.head() $
from matplotlib.dates import MonthLocator, WeekdayLocator, DateFormatter $ %pylab inline $ pylab.rcParams['figure.figsize'] = (15, 9) $
orig_ct = len(dfd) $ dfd = dfd.query('capacity_5F_max >= 5000. and capacity_5F_max <= 70000.') $ print(len(dfd) - orig_ct, 'eliminated')
import pandas as pd $ bild = pd.io.json.json_normalize(data=bild) $ spon = pd.io.json.json_normalize(data=spon)
print(autos['price'].describe()) $ autos['price'].value_counts().head()
id_of_tweet = 932626561966247936 $ tweet = (api.get_status(id_of_tweet, tweet_mode='extended')._json['full_text']) $ print(tweet)
model_ft = FastText.load_fasttext_format('C:/Users/edwin/Desktop/FastText/wiki.zh.bin') $
weather_data['Date'] = weather_date; weather_data.head()
datAll['zip'].value_counts(dropna=False)[0]/len(datAll)
followup = pd.read_json('/Users/thomasmulhern/Downloads/pitchesDataJson/followup.json')
coin_idx_list = df_new_coins[['symbol','url_ref']].values.tolist()
images.sample(10)
TestIndex = SampleIndex[NumTrain:] $ StockData.loc[TestIndex, 'Set'] = 'test'
%matplotlib inline $ import matplotlib.pyplot as plt $ plt.style.use('ggplot')
conn.fetch(table=dict(name='iris_sql2', caslib='casuser'))
print(sl.mindate.min()) $ print(sl.maxdate.max())
gdp = pd.Series([3494.898, 769.930], name='Nominal GDP in Billion USD', index=['DE', 'NL']); gdp
facebook_urls = unique_urls[(unique_urls.domain == domain)] $ facebook_urls.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
constructor.sort_values(by='mileage',ascending=False)
conditions_m.unique()
new = df.groupby(['Year','Month','hashtags'])['hashtags'].agg({'no':'count'}) # do a groupby and count each hashtag $ new.reset_index(level=[0,1,2], inplace=True) # reset the indexes $ new['datetime'] = pd.to_datetime((new.Year*10000+new.Month*100+1).apply(str),format='%Y%m%d') # convert back to a datetime with a default day of 1 $ idx = new.groupby(['datetime'])['no'].transform(max) == new['no'] # find the indexes of those max hashtags per month $ new[idx] # return the result
events.groupBy("event_type").count().toPandas().head(50).plot(kind="bar")
hits_df.tail(3)
df.shape
ax.set_xlabel('Time') $ ax.set_ylabel('Price') $ ax.legend(loc='upper left') $ plt.show()
excelDF.columns
data = data.dropna()
repos = pd.read_csv("data/2017/repos-dump.csv", quotechar='"', skipinitialspace=True) $ print('Shape before dropping duplicates', repos.shape) $ repos = repos.drop_duplicates(subset='full_name', keep='last') $ print('Shape after  dropping duplicates', repos.shape) $ repos.head()
sports_domain_id = 2 $ url = form_url(f'domains/{sports_domain_id}') $ response = requests.get(url, headers=headers)
df.shape $ df.shape[0]
year = 2015 $ drg_selected =100 $ print(unique_DRG_names[unique_DRG_numbers.index(drg_selected)]) $ DRG = drg_selected $ Grouping_Year_DRG_discharges_payments.loc[(year,DRG)][:5]
df_customers[['number of customers','MA_3','MA_6']].plot()
topCat_df = pivotTab_toDF(dataset = youtube_df, value="views", \ $                           indices = "category_name", aggregFunct = np.sum)[:5] $ print("Top 5 Youtube Categories:\n") $ topCat_df
logit_mod2 = sm.Logit(df2['converted'], df2[['intercept','new_page','country_uk','country_us']]) $ results2 = logit_mod2.fit() $ results2.summary()
session.head()
preds = pipe.predict(test) $ for (sample, pred) in zip(test, preds): $     print(sample, ":", pred) $ print("\n","Accuracy:", accuracy_score(labelsTest, preds))
data.view_count_by_category[0]
crimes.DATE__OF_OCCURRENCE = pd.to_datetime(crimes.DATE_OF_OCCURRENCE)
new_page = df.query('landing_page=="new_page"') $ new_page_not_treatment = new_page.query('group!="treatment"') $ treatment = df.query('group=="treatment"') $ treatment_not_new_page = treatment.query('landing_page!="new_page"') $ new_page_not_treatment.shape[0]+treatment_not_new_page.shape[0]
len(df.Genre.value_counts())
df.num_comments = df.num_comments.astype(int)
query_df = pd.DataFrame.from_dict(json_,  orient='index') $ query_df = query_df.transpose()
import statsmodels.api as sm $ convert_old = df2.query('converted == 1 and landing_page=="old_page"').count()[0] $ convert_new = df2.query('converted == 1 and landing_page=="new_page"').count()[0] $ n_old = df2[df2['landing_page']=="old_page"].count()[0] $ n_new = df2[df2['landing_page']=="new_page"].count()[0]
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=3w2gwzsRAMrLctsYLAzN') $ json_data = r.json() $ print(type(json_data))
plt.plot(y_validation.as_matrix()[0:50], '+', color ='blue', alpha=0.7) $ plt.plot(predictions[0:50], 'ro', color ='red', alpha=0.5) $ plt.show()
def stemming(tokenized_text): $     text = [ps.stem(word) for word in tokenized_text] $     return text $ infinity['text_stemmed'] = infinity['text_nostop'].apply(lambda x: stemming(x))
flood_reports.head()
datAll['zip'] = datAll['zip'].apply(pd.to_numeric, errors='coerce')
latest_df['annotations'] = latest_df.annotations.apply(load_json)
embedding_save_path = os.path.join(pathModel, "data") $ data_get = np.load(embedding_save_path)
from nltk.corpus import stopwords # Import the stop word list $ print(stopwords.words("english"))
import numpy as np $ np.exp(s)
len([prenatalScn for prenatalScn in SCN_BDAY_qthis.scn_age if prenatalScn < 0])
! head failed_samples.txt
print(len(sentiments[sentiments['sentiment_score']<0.05])) $ neg_sent_by_listing = sentiments[sentiments['sentiment_score']<0.05].groupby('listing_id').head(20) $ neg_sent_by_listing[:5]
df_train.info()
cars.isnull().sum()/cars.shape[0] * 100
session.query(User).from_statement(stmt).params(name='ed').all()
df[null_columns].isnull().sum()
ibm_hr_int = ibm_hr_target.select(numerical) $ ibm_hr_int.show(3)
df_dummies = pd.get_dummies(df_uro[ls_other_columns]) $ dummy_colnames = df_dummies.columns $ dummy_colnames = [clean_string(colname) for colname in dummy_colnames] $ df_dummies.columns = dummy_colnames $ df_dummies.head()
df.info()
location_scaffolds = {} $ for index, row in data_4x['LocationTypeMetadata'].iterrows(): $     location_scaffolds[row['locationType']] = row['scaffold']
guinea_data3 = guinea_data1[['Description', 'Date', 'Totals']] $ guinea_data3.head()
HTML(open('/home/ola/.ipython/profile_newprofile_34/static/custom/custom.css', 'r').read())
uniq_churned_plans = [np.unique(churned_plan) for churned_plan in churned_plans]
spark_df = context.createDataFrame(pandas_df)
plt.figure(figsize=(8,8)); $ plt.boxplot(tt_final['rating_numerator'], labels=['rating_numerator']);
tweets_clean.columns
vol = vol.replace(0, 1) $ vol.describe()
for tweet in trump_df.text: $     clean_tweet(tweet)
to_be_predicted_Day2 = 38.4923792 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'game_winners'
x.insert(2, 'A') $ print(x) $ x.insert(3, [1, 2])  # Note: insert() is for elements, so [1, 2] is a single element, not expanded $ print(x)
plt.figure(figsize=(12,8)) $ sns.violinplot(x='source', y= 'num_words', data= trump) $ plt.xlabel(' Source of Tweet', fontsize=12) $ plt.ylabel('NUmber of Words', fontsize=12) $ plt.title('Number of Words per Tweet by Source', fontsize=15)
round(np.sqrt(model_x.scale), 3)
raw = pd.read_csv('jobs_raw.tsv', sep='\t')
speeches_metadata.shape
plt.figure(figsize=(12,6)) $ sns.barplot(x='lead_source', y= 'qualConversionPercent', data=qualConvpct) $ plt.xticks(rotation=90) $ plt.title('Qualification Stage to Discovery Percentage');
nba_df.drop([30,31], axis=0, inplace= True) $ nba_df.loc[28:34]
p_diffs = np.array(p_diffs) $ (p_diffs > p_diff).mean() $ print('As a percentage: {}%'.format((p_diffs > p_diff).mean()*100))
rt_count = df_rt[['text', 'keyword']].groupby(['text', 'keyword']).size().reset_index() $ rt_count.columns = ['text', 'keyword', 'count'] $ rt_count.sort_values(by = ['count'], ascending = False, inplace = True)
driver = selenium.webdriver.Safari() # This command opens a window in Safari $ driver.get('https://www.boxofficemojo.com')
reddit_comments_data.createOrReplaceTempView('tmp_reddit_df')
Measurement = Base.classes.measurement $ Station = Base.classes.station
a=len(df.query("group =='treatment' & landing_page =='new_page' | group != 'treatment' & landing_page !='new_page'")) $ n_dont_line_up = n-a $ print(n_dont_line_up) $
df.neu.rename(columns={'DATE': 'YEAR'}, inplace=True) $ df.neu
airbnb_od.strong_cutoff
output= "SELECT user_id, followers from user where followers>150000 order by followers desc " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','followers'])
nfea = Xtr_scale.shape[1]
df.columns  # Display the column names only
df_archive_clean.info()
lasso.fit(X_train,Y_train)
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ response
df.groupby('acct_type')['approx_payout_date'].count().reset_index().sort_values('approx_payout_date', ascending = False)
merge.tail() $
mean_sea_level.plot(subplots=True, figsize=(16, 12));
auto_new.CarYear.value_counts()
from sklearn.svm import SVR $ model = SVR(kernel='rbf', C=1e3, gamma=0.1) $ print ('SVR') $ reg_analysis(model,X_train_, X_test_, y_train_, y_test_)
X["inactivePeriod"] = endDate - X['recentTime'] $ X["inactivePeriod"] = X.inactivePeriod.dt.days $ X["memberPeriod"] = startDate - X["createdtm"] $ X["memberPeriod"] = X.memberPeriod.dt.days $ X.head(5)
plt.plot(pipe.tracking_error)
url = "https://space-facts.com/mars/"
df.drop(df['lang']!='en',inplace = True)
fb.head()
merged.amount.sum()
upper_band.plot(label="Upper Band",ax=ax) $ lower_band.plot(label="Lower Band",ax=ax)
df_modified["modified"] = df_modified["updated_at"].dt.date > df_modified["created_at"].dt.date $ df_modified["modified_time"] = (df_modified["updated_at"] - df_modified["created_at"]).dt.total_seconds() / 86400
print('{0:.2f}%'.format((scores[4.0:5.0].sum()/total) * 100))
data["name"] = data["name"].str.replace("\d", "")
gc.collect()
raw.head()
y_fit = lr_grid.predict_proba(clean_X_test) $ report_score(y_test, y_fit, threshold = 0.20, proba=True)
assert 'expanded_urls' not in twitter_archive_clean.columns
nfl['Jimmy'] = np.where( nfl['Date']>=pd.datetime(2017,12,3), 'yes', 'no')
1-autos["registration_year"].between(1900,2016).sum()/autos.shape[0]
lengths = [] $ for x in articles['body']: $     lengths.append(len(x)) $ plt.hist(lengths) $ plt.show()
invalid_reg_car = df2[(df2['yearOfRegistration']>2018)] $ print('Number of listed cars with invalid registration year: ', invalid_reg_car['brand'].count()) $ invalid_reg_car[['yearOfRegistration','brand']].head()
(autos["last_seen"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().head() $ )
pd.Timestamp('2018-01')
na_df.loc["a", "two"] = None # fills as NaN since replacing an int $ na_df.loc["a", "three"] = np.nan # fills as NaN $ na_df
df.head()
pd.to_datetime('11/12/2010')
df2= df.drop(unwanted.index, axis=0)
df_495_table = pd.read_sql(sql_495_table,conn_laurel) $ df_495_table.sort_values(by='Values',ascending=False)
day_counts.show(5)
df.shape
df['ts_year'] = df['timestamp'].dt.year $ df['ts_dayofweek'] = df['timestamp'].dt.dayofweek
experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)
race_vars.head()
df[df['Descriptor'] == 'Loud Music/Party'].index.hour.value_counts().head()
train_data.loc[(train_data.vehicleType == 'kleinwagen') & (train_data.gearbox.isnull()), 'gearbox'] = 'manuell' $ test_data.loc[(test_data.vehicleType == 'kleinwagen') & (test_data.gearbox.isnull()), 'gearbox'] = 'manuell'
com311.head(2)
dtypes={'id': np.int64,'date':np.str,'store_nbr':np.int64,'item_nbr':np.int64,'unit_sales': np.float64,'onpromotion':np.float64} $ parse_dates=['date'] $ train = pd.read_csv('train.csv', dtype=dtypes,parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",train.shape) $ pd.DataFrame.head(train)
data_df.head()
Group=pd.get_dummies(df2['group'],drop_first=True)  #Forming dummy Variables and dropping the first Cloumn $ Group=Group.rename(columns={'treatment':'ab_page'})   $ df2['Intercept']=1 $ df2=pd.concat([df2,Group],axis=1)       #Joining by column $ df2.head(3)
team_search_count_df = pd.DataFrame.from_dict(team_search_count, orient='index').reset_index() $ team_search_count_df.columns = ['name', 'search_count'] $ teams_df = pd.read_csv('~/dotaMediaTermPaper/data/teams_df.csv') $ teams_df = pd.merge(team_search_count_df, teams_df, on='name', how='right')
pandas_candlestick_ohlc(apple.loc['2017-10-04':'2018-01-15',:], otherseries = ["20d", "upperband", "lowerband"])
feature_columns = fraud_data_updated.columns.values
df.index
order_statistics=[] $ for key, value in order_num.items(): $     order_statistics.append(key[0], key[1], key[2] + "-" + key[3], value, order_pay_num[key], order_pay_price_sum[key])
df.query('MeanFlow_cfs < 50')
n = -100 $ similar = LSI_model.get_dists(corpus[n]) $ print(df.iloc[n]['desc']) $ print(corpus[n]) $ df.loc[list(similar.keys())]['desc'].values.tolist()
url = "https://www.predictit.org/api/marketdata/ticker/RDTS." + str(endDay.month).rjust(2, '0') + str(endDay.day + 7).rjust(2, '0') + str(endDay.year)[-2:]
dfCleaningData = dfData.dropna() $ dfCleaningData.count()
def download_file(volume, issue): $     url = 'http://mobilizationjournal.org/toc/maiq/' + str(issue) + '/' + str(issue) $     html= requests.get(url).text $     return html
results = log_reg.fit() $ results.summary()
joined = gpd.sjoin(tazs,sw_m, how="right", op='contains') $ del joined['index_left'] $ del joined['geometry'] $ joined = joined.set_geometry('linestring') $ zones = joined['VTA_TAZ'].unique()
features.tail(3)
import pandas as pd $ dfpolicies = pd.read_csv("../data/policies.csv")
reg_df['page_US'] = reg_df['is_US'] * reg_df['ab_page'] $ reg_df['page_UK'] = reg_df['is_UK'] * reg_df['ab_page'] $ reg_df.head() $
california['bins'] = pd.qcut(california['FIRE_SIZE'],4) $ california['bins'].value_counts()
reviews=pd.read_table("ign.csv", sep=',') $ reviews.head()
for comb in itertools.combinations(nums, 2): $     print(comb)
sns.regplot(x=dfz.retweet_count, y=dfz.favorite_count)
tw_clean.stage.value_counts()
%sql \ $ SELECT avg(twitter.retweet_count) AS viral \ $ FROM twitter \ $ WHERE user_id = 42869268;
model_data = clean_users.drop(['creation_time','last_session_creation_time', 'max_login_per_7'],axis = 1) $ model_data['account_life'] = model_data['account_life'].dt.days $ model_data['invited_by_user_id'] = model_data['invited_by_user_id'] > 0 $ model_data
writer = pd.ExcelWriter("output/mediaflux_output_{}.xlsx".format(date))
len(df.query('(group == "treatment" & landing_page != "new_page") | (group != "treatment" & landing_page == "new_page")'))
test_clean["Month"] = test_clean["DateTime"].apply(month_slicer) $ list_of_test_features.append(("Month", test_clean["Month"]))
log_mod = sm.Logit(df_new.converted,df_new[['intercept','CA','UK','new_page','CA_new_page','UK_new_page']]) $ results = log_mod.fit() $ results.summary()
hu.varinfo(tips)
df2 = df1.groupby(['Destination', 'Protocol'], sort=False).sum()
BLINK.plot_count(blink,option="facet_subjects")
X_train.info()
print(pandas_list_2d.iloc[:, 0])
def date_prep(train): $     train['datetime_year'] = train['project_submitted_datetime'].dt.year $     train['datetime_month'] = train['project_submitted_datetime'].dt.month $     return(train)
df_churned = joined_df[(joined_df['churned']==True) & (joined_df['accepted']==True)] $ df_churned.head()
engine.execute('SELECT *FROM station LIMIT 10').fetchall()
hm_clean = hm.dropna()
tweet_text = result.p.text $ tweet_text
s.resample('Q').head()
display('df6', 'df7', "pd.merge(df6, df7, how='outer')")
df3['US_con'] = df3['ab_page']*df3['US'] $ df3['UK_con'] = df3['ab_page']*df3['UK'] $ df3['CA_con'] = df3['ab_page']*df3['CA'] $ df3.head()
old_page_converted.mean()
avg_data.head()
july = summer.ix[datetime(2014,7,2) : datetime(2014,8,28)] $ july[['Mean TemperatureC', 'Precipitationmm']].plot(grid=True, figsize=(10,5))
a = np.arange(10); a
col = ['msno','plan_list_price','payment_plan_days','actual_amount_paid','payment_method_id','transaction_date','membership_expire_date'] $ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions', col) # 20,000,000 $
grinter_day0 = inter_day0.groupby(['user_id','item_id']).size().reset_index() $ grinter_day0.columns = ['user_id','item_id','number_int'] $ grinter_day0.shape
col = list(X_testfinal.columns) $ col[2]= '1hr' $ X_testfinal.columns = col $
returned_orders_data.to_csv("returned_orders_data.csv")
ebola_melt.head()
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ results = log_mod.fit() $ results.summary()
df = pd.merge(df, zipcodesdetail, on=['state', 'city', 'zipcode'], how='inner', suffixes=('_x', ''))[newcols]
ax = pre_analyzeable['prio_sims_other'].value_counts().plot(kind='bar') $ for p in ax.patches: $     ax.annotate(str(int(p.get_height())), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize=25)
c>0.7
elms_all = elms_all_0611.copy()
with open('image-predictions.tsv',mode='wb')as file: $     file.write(response.content)
PDSQ = PDSQ.dropna(axis=1, how='all')
multiple_party_votes_all.reset_index(inplace = True)
import shutil $ shutil.rmtree(fp_ex) $ db_store = pathlib.Path("/active/examples/local_database/") $ shutil.rmtree(db_store)
scores[scores.IMDB == scores.IMDB.min()]
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])
bm1 = list(map(int, benchmark11)) $ print('benchmark1 score is: {}'.format(np.around(f1_score(actual1, bm1, average='weighted'), decimals=5))) $
!cp ../../drive/ColabNotebooks/AV_innoplexus_html/train_cont.zip .
train_data = pd.read_csv('train.csv') $ test_data = pd.read_csv('test.csv') $
df2.shape[0] $ print("Number of rows in df2 : {}".format(df2.shape[0]))
season17 = ALL[(ALL.index >= '2017-09-07') & (ALL.index <= '2018-02-07')]
games_df_v2.head()
autos['date_crawled'].str[:10].value_counts().sort_index()
df_byzone.head(2)
by_area['AQI Category'].value_counts()
tweet_data = pd.DataFrame(columns = ['id','retweet_count','favorite_count']) $ for line in lines: $     x = json.loads(line) $     tweet_data = tweet_data.append({'id':x['id'], 'retweet_count':x['retweet_count'],'favorite_count':x['favorite_count']}, ignore_index = True)
val.dtype
train = pd.merge(train, count_comments(train), how='left', on='user_id') $ test = pd.merge(test, count_comments(test), how='left', on='user_id')
from pyspark.ml import Pipeline $ from pyspark.ml.classification import DecisionTreeClassifier $ from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer
df.Close.pct_change().plot(figsize=(15,5));
ibm_hr_target_small.stat.corr("Age", "DailyRate")
(p_diffs > ob_diffs).mean()
numerized_tokens = [[stoi[o] for o in p] for p in df.tokenized_text]
(extract_all.shape, extract_deduped.shape)
store_items.fillna(method = 'ffill', axis = 1)
results_station = session.query(Stations.station,Stations.name,Stations.latitude, Stations.longitude,Stations.elevation).all() $ results_station
scores = cross_val_score(lr, X_train, y_train, cv=6) $ print("Cross-validated scores:", scores) $ print("Average: ", scores.mean())
adopted_cats.loc[adopted_cats['Color']=='Cream Tabby/Cream Tabby','Color'] = 'Cream Tabby' 
my_df[my_df.isnull().any(axis=1)].head()
from IPython.core.display import HTML $ HTML(filename=DATA_FOLDER+'/titanic.html')
df['car_type'] = df['Keyword'].str.extract("(bmw|audi|ford)", re.IGNORECASE, expand=False)
csvFile = open('ua.csv', 'a')
n_old = (df2.landing_page == "old_page").sum() $ n_old
df.shape
df3.head(3)
ser8 = pd.Series(np.random.randint(0,1000,10000)) $ for label, value in ser8.iteritems(): $     ser8.loc[label]= value+2  
df.head()
raw[raw.job_type == 'Sponsored'].hash.value_counts()[0:5]
bufferdf.Fare_amount.mean()
crimes.columns = crimes.columns.str.replace('__', '_') $
df.loc['2018-05-21']
keydf.columns = ["ID"] $ keydf.ID = keydf.ID.astype(int)
pd.DataFrame(data={'goles_a_favor': goles_favor, $               'goles_en_contra': goles_contra} $             )
plt.hist(a['2']) $ plt.show()
df['date'] = pd.to_datetime(df['date']) $ df
complete_df = complete_df.fillna(0)
conn.close() $
USvideos['trending_date'] = pd.to_datetime(USvideos['trending_date'],infer_datetime_format = True, format = '%y.%d.%m') $ USvideos.info() $ USvideos['time_to_trend'] = (USvideos['trending_date'] - USvideos['publish_time']).astype('timedelta64[h]')
print "Fit booths are: ", set(bthlst) ^ set(df_bug[u'Service Location'].unique())
def get_time_period_snapshot_urls_df(urls_df, time_period_start, time_period_end): $     start_dto = datetime.strptime(time_period_start, '%y-%m-%d_%H-%M-%S') $     end_dto = datetime.strptime(time_period_end, '%y-%m-%d_%H-%M-%S') $     urls_df = urls_df[start_dto:end_dto] $     return urls_df
X = stock.iloc[915:-1].drop(['target', 'target_class', 'news_sources', 'news_text', 'tesla_tweet', 'elon_tweet'], 1) $ y = stock.iloc[915:-1].target_class
August_deaths_Guinea = result1.iloc[0]['Guinea_deaths Mean'] $ September_deaths_Guinea = result1.iloc[1]['Guinea_deaths Mean'] $ October_deaths_Guinea = result1.iloc[2]['Guinea_deaths Mean'] $ October_deaths_Guinea
doctype_by_day.min().sort_values(ascending=False)[:10].index
age = pd.Series( $     [35, 29, 27, 29, 18, 21], $     index=['Bob', 'Alice', 'Peggy', 'Victor', 'Frank', 'Erin'] $ )
n_f_df['ones'] = np.ones(len(t_df)) $ f_arr = np.array(n_f_df) $ t_arr = np.array(t_df) $ alpha = 0.1 $ num_iter = 50
hashtag_counts = hashtags.value_counts().head(10) $ hashtag_counts
plt.hist(y_res) $ plt.hist(y) $ plt.show()
np.exp(results.params)
m.start_time
g1800s.shape
conn_a.commit()
moves_dataframe['date'] = moves_dataframe['date'].apply(lambda x: datetime.strptime(x,"%Y%m%d")) $ moves_dataframe.index = moves_dataframe['date'] $ twitter_moves = twitter_mean.join(moves_dataframe,lsuffix='_twitter',rsuffix='_moves',how='right')
df1 = df.drop('Cabin', axis=1) $ df1.shape
unrepaired_damage_dict={'nein':'no', $                         'ja':'yes' $ } $ autos['unrepaired_damage'].replace(unrepaired_damage_dict,inplace=True) $ autos['unrepaired_damage'].fillna('unspecified',inplace=True)
from sklearn.feature_extraction.text import CountVectorizer
s = pd.Series([9.2, 'hello', 89]) $ s.dtype
jpn = ['Japan', 'Tokyo', 'JAPAN', 'Tokyo-to', 'Osaka', 'Okinawa', 'Chiba', 'Okinawa-ken', 'Kyoto', 'Miyazaki'] $ notus.loc[notus['country'].isin(jpn), 'country'] = 'Japan' $ notus.loc[notus['cityOrState'].isin(jpn), 'country'] = 'Japan' $ notus.loc[notus['country'] == 'Japan', 'cityOrState'].value_counts(dropna=False)
c.execute('SELECT city, min(average_high) FROM weather') $ print(c.fetchall())
bucket.upload_dir('data/heat-pump/raw/', 'heat-pump/raw', clear_dest_dir=True)
first_result.find('a')
from nltk import pos_tag $ sentence = word_tokenize('I always lie down to tell a lie.') $ pos_tag(sentence)
datelist = pd.date_range(start=pd.datetime.today(), periods=n, normalize=True).tolist() $ my_data2 = pd.DataFrame({'date': datelist, 'input1': x, 'input2': z, 'input3': u1, 'input4': u2, 'target': y}) $ my_data2 = my_data2.drop(0) $ my_data2.head(5)
print('Molecule ID: %s' % pmol.code) $ print('\nRaw MOL2 file contents:\n\n%s\n...' % pmol.mol2_text[:500])
df[(abs(df['Open']-df['High'])<0.2 ) | (abs(df['Close']-df['Low'])<0.2)]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.999) $ nb = MultinomialNB() $ nb.fit(x_train, y_train) $ nb.score(x_test, y_test)
control = df2[df2['group'] == 'control'] $ control[control['converted'] == 1].shape[0] / control.shape[0]
happiness_df=pd.DataFrame(columns=['dates','happiness']) $ for i in range(0, len(happiness_array)): $     happiness_df.loc[i]=([happiness_time[i],happiness_array[i]]) $
reviews['listing_id'].value_counts().plot(kind='hist', figsize = (20,10), fontsize = 16, title = 'Frequency of Reviews') $ axes = plt.gca() $ axes.set_xlim([0,250]) $ axes.set_ylim([0,2500]) $ plt.savefig('freq reviews.jpg') $
df.groupby('STNAME').agg({'CENSUS2010POP': np.average})
tweet_archive_clean.stage.value_counts()
precipitation_data = session.query(Measurement.date, Measurement.prcp).filter(Measurement.date >= "2016-08-23")
df_vow.head()
1 + 1.0
bnb[bnb['age']>1000].age.head(10)
image_pred.shape 
most_yards.groupby("Jimmy").Jimmy.value_counts()
s.index = ['a', 'b', 'c', 'd', 'e'] $ s
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=["accuracy"]) $
percent_null_columns = [(c,df_train[c].count()/len(df_train)*100) for c in df_train.columns if df_train[c].isnull().any()] $ for t in percent_null_columns: $     print (t)
b_cal_q1.loc[:,'price'] = new_price
import csv $ data = list(csv.reader(open("test_data//dummy.csv"))) $ print(data)
complete_table = pd.merge(mean_for_each_weekday, std_for_each_weekday) $ complete_table[['weekday', 'duration(min)', 'std(duration)']]
df.drop(holidays, 1, inplace=True)
user_summary_df[user_summary_df.gender == 'F'][['followers_count', 'following_count', 'tweet_count', 'original', 'quote', 'reply', 'retweet', 'tweets_in_dataset']].describe()
dividends = yahoo_finance.download_dividends("GILD") $ print dividends[0]
df['in_reply_to_status_id'].value_counts()
df_plot = pd.DataFrame(columns = ['date','click_amount',]) $ for k,i in df_a.iterrows(): $     df_plot.loc[len(df_plot)] = [k,i['click_rec_menues']]
df2.plot() $ plt.show()
print("Probability of control group converting:", $       df2[df2['group']=='control']['converted'].mean())
dataframes = [data_NL_Q1, data_NL_Q2, data_NL_Q3, data_NL_Q4] $ data_NL = pd.concat(dataframes)
archive.info()
pn = 'B012828' $ table_store = data_from_store[data_from_store['Part Number'] == pn] $ table_1c = data_from_1c[data_from_1c['PO no.'] == pn] $ pn_qty[pn]['storeinfo'] = [] $ pn_qty[pn]['1cinfo'] = []
df_map_name.to_csv("BaseMapPSE")
trainset = dataset_filtered.iloc[tr_indices,:].copy() $ testset = dataset_filtered.iloc[t_indices,:].copy()
y.value_counts()
sorted_errors_idx = options_frame['ModelError'].map(abs).sort_values(ascending=False).head(50) $ errors_20_largest_by_strike = options_frame.ix[sorted_errors_idx.index] $ errors_20_largest_by_strike[['Strike', 'ModelError']].sort_values(by='Strike').plot(kind='bar', x='Strike') $
people.columns
girls_by_name.loc['HARJIT':'HARKIRAN']
accuracy = metrics.r2_score(y_test, predictions) $ print("Cross-Predicted Accuracy:", accuracy)
pd.Timestamp("now")
f_top_domains = '/scratch/olympus/projects/ideology_scaling/congress/top_domains.json' $ with open(f_top_domains , 'w+') as f: $     f.write(json.dumps(domain_counter)) $ shutil.chown(f_top_domains, group='smapp')
extended_tweets.info() $ get_real_types(extended_tweets)
texts = df[df.section_text.str.contains('Southwest Airlines')].section_text $ len(texts)
os.listdir(os.getcwd() + "/fireeye")[1]
df = pd.DataFrame(np.random.randint(10,size=(10,4)), $ index = pd.date_range('1/1/2000', periods=10), $ columns = ['A', 'B', 'C', 'D']) $ df
def convert_to_one_hot(Y, C): $     Y = np.eye(C)[Y.reshape(-1)] $     return Y
final_topbikes['Distance'].count() / len(list(set(list(final_topbikes['id'])))) / 14
conf = result.conf_int() $ conf['OR'] = params $ conf.columns = ['2.5%', '97.5%', 'OR'] $ print(np.exp(conf)) # confidence intervals (we get the odds ratio from the log odds by exponentiating the coefficient)
columns = ['feature','including', 'excluding'] $ feature_df = pd.DataFrame(columns=columns) $ feature_df = feature_df.fillna(0)
model_artifact = MLRepositoryArtifact(model_rf, training_data=train_data, name="Product Line Prediction") $ model_artifact.meta.add("authorName", "Team XX")
cbg['COUNTYFP'] = cbg['COUNTYFP'].astype(str)
restaurants.head(10)
os.chdir('..')
rawautodf.head()
pred=svc.predict(X_test)
print('Number of rows', reviewsDF.shape)
some_data = ac_tr.iloc[:5] $ some_labels = ac_tr_label.iloc[:5] $ some_data_prepared = full_pipeline.transform(some_data) $ print("predictions:\t", lin_reg.predict(some_data_prepared)) $ print("Labels:\t\t", list(some_labels))
def nearestDate(dates, pivot): $     return min(dates, key=lambda x: abs(x - pivot)) $ crime_list = [] $ i = 0 $ %time crime_list.append(nearestDate(weather_dates, crime_data[i][3])) $
run txt2pdf.py -o"2018-06-19 2014 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2014 FLORIDA HOSPITAL Sorted by payments.txt"
sns.boxplot(x=df.author, y=df.created_at.dt.weekday); $ plt.xticks(np.arange(10), ('collins', 'hclinton', 'hirono', 'hoeven', 'mccain', 'obama', 'ryan', 'sanders', 'schwarzenegger', 'trump'), rotation = 50); $ plt.yticks(np.arange(7), ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'));
df4[['CA', 'UK', 'US']] = pd.get_dummies(df4['country']) $ df4.head()
lda_model.show_topics()
contract_history.STS_CODE.value_counts()
q_multipoint = c.retrieve_query(url='https://v3.pto.mami-project.eu/query/9d0a16b4fbb090b661e33140c847b999a8907a49058f21795ab68cdeb6784c95') $ multipoint_df = q_multipoint.results() $ multipoint_df.columns = ["count","condition","time"] $ multipoint_df["time"] = pd.to_datetime(multipoint_df["time"]) $ q_multipoint.metadata()
my_columns = list(df_.columns) $ print(my_columns)
v_invoice_sat.columns[~v_invoice_sat.columns.isin(invoice_sat.columns)]
building_pa_prc.sample(5)
sns.distplot(df_predictions_clean.img_num) $ plt.xlabel("Image Number") $ plt.ylabel("Count")
print(autos.loc[autos['unrepaired_damage'] == 'no', 'price'].mean()) $ print(autos.loc[autos['unrepaired_damage'] == 'yes', 'price'].mean())
old_page_converted=np.random.choice([0,1],size=145274, p=[mean2, 1-mean2])
from sklearn.svm import LinearSVR $ model = LinearSVR() $ print ('SVR') $ SVR_model = reg_analysis(model,X_train, X_test, y_train, y_test)
rankings_USA.query("rank >= 30")['rank_date'].count() $
writer.save()
payments_all_yrs.head()
X = np.arange(8) $ Y = X + 0.5 $ C = 1.0 / np.subtract.outer(X, Y) $ print(np.linalg.det(C))
classifier_data = results_dict['classifier_evaluation_results'][0] $ print("Data is saved as a {} object.\nAUC for this session: {}".format(type(classifier_data), classifier_data.auc))
purchases['Zipcode'].apply(lambda x: len(x)).max()
year_labeled= 2017 $ year_predict= 2018 $ description_labeled = df[df.year==year_labeled]['description'] $ description_predict = df[df.year==year_predict]['description']
df2['intercept']=1 $ model=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ result=model.fit() $
weather.Date = weather.Date.apply(lambda x:x.month)
def calculate_cumulative_returns(returns): $     return None $ project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)
def get_action_counts(frame,action_list): $     action_subset = frame[frame['action'].isin(action_list)] $     action_counts = pd.crosstab(action_subset['user_id'],action_subset['action']) $     return action_counts
itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb')) $ stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)}) $ len(itos)
QUIDS = QUIDS.loc[(QUIDS['week'] == 0) |(QUIDS['week'] == 12)  |(QUIDS['week'] == 14)] $ QUIDS = QUIDS[["subjectkey", "qstot", "week"]].sort_values(['subjectkey', 'week'])
!cat Data/microbiome.csv
np.exp(results.params)
for tweet in negativeTweets['Tweets'].iteritems(): $     print (tweet)
filename = 'subway_data.txt' $ subway2_df = pd.read_csv(filename)
autos['price'] = autos['price'].str.replace("$","") $ autos['price'] = autos['price'].str.replace(",","") $ autos['price'] = autos['price'].astype(float)
source = api.create_source('data/Airline+Weather_data.csv')
ADP.dtype
df = df[(df.price < 150000) & (df.price > 100) ]
merged_df.groupby(["plot_id", "taxa"]).count()["record_id"].unstack().plot(kind='bar', stacked=True) $ plt.xlabel("Plot ID") $ plt.ylabel("Number") $ plt.legend(loc='upper center', ncol=5, bbox_to_anchor=(0.5, 1.15)) $ plt.show()
dataAnio.to_csv('data/dataPorUbicacion_Anios.csv')
df.loc[dates[0],'A']
df2 = df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index) $ df2 = df2.drop(df2.query("group == 'control' and landing_page == 'new_page'").index)
for letter in list('ABCDEFGH'): $     ab_groups[letter].drop(ab_groups[letter].tail(1).index, inplace=True)
df2.duplicated().sum()
def MA(df, n):  $     MA = pd.Series(pd.rolling_mean(df['Close'], n), name = 'SMA_' + str(n))  $     df = df.join(MA)  $     return df $ dataframe1=MA(dataframe,30) $
xs = np.array([1,2,3,4,5], dtype=np.float64) $ ys = np.array([5,4,6,5,6], dtype=np.float64)
jobs_data['clean_titles'] = jobs_data['record.title'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", str(x)).split() if i not in s_words]).lower())
search_response = requests.get(search_url, params=params, auth=auth)
reddit.shape
df_categories.category_id.unique().shape
iterator.df.head()
type(df.date[0])
ABT_tip = pd.merge(reviewsDFslice, tips_sentiment, how = 'left', on = ('business_id'))
ser_1 = pd.Series([1,2,3,4],index = ["A","B","C","D"]) $ ser_2 = pd.Series([5,4,6,7],index = ["A","B","C","E"]) $ dict_ser ={"One":ser_1,"Two":ser_2} $ df_ser_dict = pd.DataFrame(dict_ser) $ print(df_ser_dict)
convo = df.iloc[:,1] $ convo
pd.DataFrame(tweet_df.groupby('username')['lang'].value_counts()).add_suffix('_Count').reset_index()
nypd_df['create_date_time'] = pd.to_datetime(nypd_df['Created Date']) $ nypd_df['closed_date_time'] = pd.to_datetime(nypd_df['Closed Date'])
dfs['Stock First Difference'].plot()
gid.get_repo_version?
IBMspark_df.registerTempTable("IBM") $ print sqlContext.sql("select * from IBM limit 10").collect()
model.load_weights('model') $ loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0) $ print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))
df.interpolate()
dfAll = pd.read_csv('All.csv',index_col=0) $ df_ohc = pd.get_dummies(dfAll,columns=['events','day_of_week']) $ df_ohc.head()
df.set_value(306, 'yob',1998)
parsed_times = raw.time.str.extract("(?P<m>\d{1,2})?:?(?P<s>\d{2})\.(?P<ms>\d{2})", expand = True) $ time_seconds = parsed_times.m.astype(float).fillna(0) * 60 + parsed_times.s.astype(float) + parsed_times.ms.astype(float) / 100 $ raw.assign(time_seconds = time_seconds)[["event", "time_seconds"]].sample(10)
df_nullcount.plot(kind='bar')
df2.converted.sum() / len(df2)
df_enhanced.info()
df['LOT'].value_counts()
suburban_avg_fare = suburban_type_df.groupby(["city"]).mean()["fare"] $ suburban_avg_fare.head() $
extract_all['app_id_short'] = [np.nan if str(x)=='nan' else str(x)[0:16] for x in extract_all['system.record_id'].values]
df = pd.DataFrame(emails, columns = ['email_address']) $ df
counted_store_events = no_null_events.groupBy("store_id").count()
sim_prop_new_converted = np.random.binomial(num_new, null_p_new, 10000) / num_new $ sim_prop_old_converted = np.random.binomial(num_old, null_p_old, 10000) / num_old $ p_diffs = sim_prop_new_converted - sim_prop_old_converted
grouped = order_data.groupby(['CohortGroup', 'OrderPeriod']).agg({'userId': pd.Series.nunique})
fullDf[fullDf.country=='NP'].levelIndices.value_counts()
c.find_one({'name.first': 'Michael', $              'name.last': 'Bowie'})
conditions_m.value_counts(dropna=False)
group_one = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ group_two = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ print("Number of times 'treatment' group don't line up with 'old_page' - {}".format(len(group_one))) $ print("Number of times 'control' group don't line up with 'new_age' - {}".format(len(group_two))) $ print("Number total of times the 'new_page' and 'treatment' don't line up - {}".format((len(group_one)+len(group_two))))
merged2.index
player_frame = baseball[baseball['team'].isin(['LAN', 'SFN'])] $ print(player_frame['player']) $ print('The number of records is: ',len(baseball[baseball['team'].isin(['LAN', 'SFN'])]))
map_osm.choropleth(geo_path=check_point_file_path)
df.to_excel("msft.xls")
print(calls_df[["length_in_sec"]].hist(bins=50,figsize=(20,10)))
df.plot()
integratedData = pd.merge(xmlData, csvData, how = 'outer', indicator = 'True', on = [u'date', u'price', u'bedrooms', u'bathrooms', u'floors', u'waterfront', $        u'view', u'yr_built', u'yr_renovated', u'sqft_above', u'sqft_basement', $        u'sqft_living', u'sqft_lot', u'street', u'city', u'statezip', $        u'country'])
from sklearn.cross_validation import KFold
print(example_model.get('coefficients'))
s1 = pd.Series(np.random.randn(5),index=['a','b','c','d','e']) $ s1
scores_mode = scores.sort_values(ascending=False).index[0] $ print('The mode is {}.'.format(scores_mode))
df1['FuelVoltage']= df1.io_state.apply(lambda x: int(x[-3:],16)) $
kamchatka_bbox = [151.1, 47.8, 172.0, 58.3] $ petropavlovsk_coords = (53.145992, 158.683548)
smart_authors = authors[(authors['count'] >= 10) & (authors['mean'] >= 0.5)] $ smart_authors.shape[0]
print("the day of the month is: ", now.day) $ print("we are curretly in month number", now.month) $ print("The year is", now.year)
autos["price"].unique().shape
r = requests.get('https://httpbin.org/basic-auth/myuser/mypasswd') $ r.status_code
new_page_p = ab_df2.landing_page.value_counts()[0]/ab_df2.shape[0] $ old_page_p = ab_df2.landing_page.value_counts()[1]/ab_df2.shape[0] $ print(('new page probability', new_page_p),('old page probability', old_page_p))
df.dropna(inplace=True)
loans.columns.values
df_course_after=pd.merge(df_course,df_users_6_after,left_on='uid',right_on='uid',how='inner')
life_ship_top_vec = all_top_vecs[bks.life_ship[0]] $ twentyk_leagues_top_vec = all_top_vecs[bks.twentyk_leagues[0]] $ diff_vec2 = vec_lst_add_subtract(twentyk_leagues_top_vec, life_ship_top_vec, mode='subtract') $ find_most_similar(diff_vec2, all_top_vecs, title_lst, vec_in_corp='N')
tokendata = tokendata.drop("level",axis=1)
treatment_group = df2.query('group == "treatment"') $ converted_treatment = treatment_group[treatment_group.converted == 1] $ p_treatment = converted_treatment.count()/treatment_group.count() $ p_treatment
X_train.shape
merged_data['due_day'] = merged_data['due'].dt.day $ merged_data['due_month'] = merged_data['due'].dt.month $ merged_data['due_year'] = merged_data['due'].dt.year
autos['ad_created'].str[:10].value_counts(normalize = True).sort_index()
print("Probability of treatment group converting:", $       df2[df2['group']=='treatment']['converted'].mean())
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = lm.fit()
import re $ liquor.columns = [re.sub("[^a-zA-Z]+", "", x) for x in liquor.columns]
age_hist.head(10)
groupby_breed.tail()
newdf.drop(newdf.columns[7],axis=1,inplace=True)
from nltk.tokenize import word_tokenize $ words = word_tokenize(text) $ print(words)
X_train.shape, y_train.shape
df_train = train.sort_values(by='date_int').copy() $ df_test=test.sort_values(by='date_int').copy()
routing_weights = tf.nn.softmax(raw_weights, dim=2, name="routing_weights")
calls_nocontact_simp['ticket_closed_date_time'] = pd.to_datetime(calls_nocontact_simp['ticket_closed_date_time'])
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_autocorrelation(doc_duration.diff()[1:], params=params, lags=30, alpha=0.05, \ $     title='Weekly Doctor Hours First Difference Autocorrelation')
users = users.query("created_at > '1984-01-01 00:00:00'") #removing those 2 outliers
df_sites = (df_providers.groupby(['id_num','name','year'])[['discharges']].sum()) $ df_sites.head()
house_data.head(20)
r = requests.get(data_request_url, params=params, auth=(username, token)) $ r
pd.Series(42)
autos["price"] = autos["price"].astype(int) $ autos["odometer"] = autos["odometer"].astype(int)
tweets_df = pd.DataFrame(tweets_df, columns = ['id', 'retweeted','retweet_count','favorite_count'])
blob.sentences[0].polarity
np.mean(b)  # Get the mean of all the elements
print('Change the column Indicator to Indicator_id permanently') $ df.rename(columns = {'Indicator':'Indicator_id'},inplace=True) $ df.head(2)
rng = pd.date_range('3/6/2012 00:00', periods=15, freq='D')
print(knn5.score(X_train, y_train), knn5.score(X_test, y_test))
tt_json_df.info()
pop_cat = timecat_df.groupby('userLocation')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ pop_cat.head()
p=df2['converted'].mean() $ p
from gensim import corpora, models, similarities $ dictionary = corpora.Dictionary.load('/var/folders/2m/d1x8p4k179j6433svzq39zqh0000gn/T/deerwester.dict') $ corpus = corpora.MmCorpus('/var/folders/2m/d1x8p4k179j6433svzq39zqh0000gn/T/deerwester.mm') # comes from the first tutorial, "From strings to vectors" $ print(corpus)
logreg = LogisticRegression() $ logreg.fit(X_train,y_train) $ y_preds=logreg.predict(X_test) $ metrics.accuracy_score(y_test,y_preds)
nba_df.index[0:5]
json.dumps(mi_diccionario)
n_new, n_old = df2['landing_page'].value_counts()
dataUrl = 'http://gstore.unm.edu/apps/rgis/datasets/7bbe8af5-029b-4adf-b06c-134f0dd57226/nps_boundary.original.zip' $ fileName = 'nps_boundaries.zip' $ ur.urlretrieve(dataUrl, fileName)
df_archive.describe()
newdf = pd.DataFrame(taxiData.Trip_distance)
df_birth.boxplot(column='population', by = 'Continent') $ plt.show()
days = pd.Series(pd.Categorical(tips.day, $                          ordered=True, $                          categories=["Thu","Fri","Sat","Sun"] $                         )).sample(10) $ days.sort_values()
final.dropna(inplace=True) $ final.head()
total_sales = total_sales.dropna() $ total_sales.head()
df_master_select.shape
df_master.head()
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_secret) $ api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, parser=tweepy.parsers.JSONParser())
N_new = df2.query("landing_page == 'new_page'")["user_id"].count() $ print("The dataset consists of {} new pages.".format(N_new))
metrics.homogeneity_score(labels, km.labels_)
df.iloc[:2]
cursor.execute(sq72) $ cursor.execute(sq73) $ results2 = cursor.fetchall() $ results2
twitter_df.rating_numerator.nunique()
print('Maximum values of each column:\n', google_stock.max())
df.isnull().values.any() #check if the dataset has any null value
df2 = df.drop(df_nomatch.index)
import os $ covtype_df = h2o.import_file(os.path.realpath("../__data/covtype.full.csv"))
n_old = df2.query('landing_page == "old_page"').shape[0] $ n_old
drawPoliGraph(multiG,0.01)
baby_scn_postgen['SCN_CREATED'].min()
sale_country_sort = sale_average[sale_average.Country == 'Canada'] $ sale_country_sort
p_diffs=np.array(p_diffs) $ (actual_difference < p_diffs).mean()
autos.isnull().sum()
cpq.columns
df2.describe() $
sub_dataset['NewsDesk'].value_counts(sort=True, normalize=True)
os.chdir(folder_source_of_files) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir('../') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $
BID_PLANS_df.set_index('baby_uid',inplace=True)
y = y.map(lambda x: 1 if x > 1 else 0)
userArtistDF.groupBy("userID").sum("playCount").where("sum(playCount) <= 50").orderBy('sum(playCount)', ascending=0).show(10)
df_mas=df_mas[df_mas.rating_numerator>9] $ df_mas=df_mas[df_mas.rating_numerator<15]
result.head(5)
data.index
missing_hours = taxi_hourly_df[(taxi_hourly_df.missing_dt == True)].index.hour
sp_companies = pd.read_html("http://en.wikipedia.org/wiki/List_of_S%26P_500_companies", header = 0)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old]) $ z_score,p_value
post_to_user_map = {k:v for k,v in zip(posts_df['Id'],posts_df['OwnerUserId'])} $ user_to_num_of_posts_map = Counter(list(post_to_user_map.values())) $ post_counts=sorted(list(user_to_num_of_posts_map.values())) $ sns.distplot(post_counts[:int(len(post_counts)*0.99)])
sampleSize=ratings.groupby("rating").count().reset_index()['userId'].min() $ sampleSize
data5 = df_signup.groupby(['signup_method', 'country_destination']).size() $ data5
df['days'].max() - df['days'].min()
for url in soup.find_all('a'): $     print(url.get('href'))
n_new = len(df2.query("group == 'treatment'" )) $ n_new
top_tracks.head()
c.find_one(result.inserted_id)
age.loc['Bob':'Peggy']
pd.DataFrame ([{"name":"Yong", "id":1,"zkey":101},{"name":"Gan","id":2}])
engine = create_engine("sqlite:///./Resources/hawaii.sqlite")
train_corpus = list(read_corpus(path_play + "/all_demo_train")) $ model = gensim.models.doc2vec.Doc2Vec(vector_size=80, min_count=2, epochs=100) $ model.build_vocab(train_corpus) $ %time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
def calculate_reorder_rate(): $     len(HYB_customer_order_intervals) / len(df.loc[df['SalesOffice']])
file_path = "/mnt/idms/temporalNodeRanking/data/filtered_timeline_data/tsv/ausopen/ausopen_mentions.csv" $ au.recode_and_export_mentions(file_path,mentions_df,user_names)
print(temp_nc) $ for v in temp_nc.variables: $     print(temp_nc.variables[v])
jobs_data = pd.read_pickle('D:/CAPSTONE_NEW/jobs_data_full.pkl')
learner = md.get_model(opt_fn, em_sz, nh, nl, $                dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05) $ learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ learner.clip=0.3
import dask.dataframe as dd $ from time import sleep $ dask_df = dd.from_pandas(data=df, chunksize=10000) $ dask_df = dask_df.repartition(npartitions=8) $ dask_df.head()
reddit['engagement'].value_counts(normalize=True)
df_questionable[df_questionable['conspiracy'] == 1]['link.domain_resolved'].value_counts(25).head(25)
df['date'] = pd.to_datetime(df['starttime'])
import pyspark.sql.functions as func $ test.groupBy().agg(func.max(col('id'))).show()
neg_tweets = ioDF[ioDF.all_sent_x <= -.5]
us_grid = np.array(np.meshgrid(grid_lon, grid_lat)).reshape(2, -1).T $ np.shape(us_grid)
nypd_complaints_total = data[data['Agency']=='NYPD']['Borough'].count() $ nypd_unspecified = data[(data['Borough']=='Unspecified') & (data['Agency']=="NYPD")]['Borough'].count() $ percentage = nypd_unspecified/nypd_complaints_total*100 $ print("%1.2f"%percentage)
z_score, p_value = sm.stats.proportions_ztest([ convert_new, convert_old ],[ n_new, n_old ], alternative = 'larger') $ z_score, p_value
nold=df2.query("group == 'control'") $ n_old=nold.shape[0] $ n_old
X = vectorizer.fit_transform(text)
transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv') $ transactions.head()
df_c2 = df_c.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_c2.head()
a=pd.DataFrame(general_volume_types.groupby('complaint_type').f1_.sum()) $ a.sort_values('f1_', ascending=False)[0:26]
log_all = sm.Logit(new['converted'],new[['intercept','country_US','country_UK','country_CA']]) $ r = log_all.fit() $ r.summary()
comp_leader = pd.merge(competitions, leaderboard, on='ref') $ df = pd.merge(comp_leader, metrics, on='ref') $ df.head()
twitter_archive_clean.head(3)
import urllib3, requests, json $ headers = urllib3.util.make_headers(basic_auth='{username}:{password}'.format(username=wml_credentials['username'], password=wml_credentials['password'])) $ url = '{}/v3/identity/token'.format(wml_credentials['url']) $ response = requests.get(url, headers=headers) $ mltoken = json.loads(response.text).get('token')
model.doesnt_match("paris berlin london austria".split()) $
cityID = 'b71fac2ee9792cbe' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Sacramento.append(tweet) 
common_dictionary.head()
merge_email.loc[(merge_email.language == 'english') & (merge_email.action == "email_clickthrough")].groupby('nweek')['user_id'].nunique().plot(label="english") $ merge_email.loc[(merge_email.language != 'english') & (merge_email.action == "email_clickthrough")].groupby('nweek')['user_id'].nunique().plot(label="non-english") $ plt.legend() $ plt.show() $
fig = df['violation_desc_long'].value_counts()[:10].plot('barh') #title='Top Citations by Violation') $ plt.savefig("by_violation_desc.png")
missing_info = list(data.columns[data.isnull().any()]) $ missing_info
df.groupby('raw_character_text')['episode_id'].nunique().reset_index().head()
word_list = [] $ for word in model_ft.wv.vocab: $     word_list.append(word)
log.info("starting job") $ new_predictions_response = client.run_job(body=req_body) $ log.info("done with job")
raw_freeview_df, raw_fix_count_df = condition_df.get_condition_df(data=(etsamples_grid,etmsgs_grid,etevents_grid), condition='FREEVIEW')
num_id = df.nunique()['user_id'] $ print("{} unique users in the dataset.".format(num_id))
googletrend.head()
grinter1[grinter1.number_int > 50]
df_A.loc[['s2','s5','s1']]
cats_in = intake.loc[intake['Animal Type']=='Cat'] $ cats_in.shape
cityID = '8173485c72e78ca5' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Atlanta.append(tweet) 
df_columns.index.month.value_counts().sort_index().plot() $
out = conn.loadtable('data/iris.csv', caslib='casuser') $ out
df2.drop(['UK', 'country'], axis = 1, inplace = True)
response = requests.get("http://api.open-notify.org/iss-now.json") $ pd.read_json(response.content)
df2['intercept']=1 $ model=sm.Logit(df2['converted'],df2[['intercept','ab_page','US','UK']]) $ result=model.fit() $ result.summary()
image_file.info()
sum(image_predictions.jpg_url.duplicated())
%%bash $ while IFS='' read -r line || [[ -n "$line" ]]; do $     grep "It works!" $line $ done < links.txt
import pickle $ with open("rows.pkl", 'wb') as f: $     pickle.dump(rows, f)
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
date_agency = data.groupby(['yyyymm','Agency']) $ date_agency.size().unstack().plot(kind='bar',figsize=(15,15))
prices = df[::2].transpose().stack()
url_template.format(start,)
rf_words = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1) $ scores = cross_val_score(rf_words, X, y, cv=skf) $ print "Cross-validated RF scores based on words:", scores $ print "Mean score:", scores.mean()
start = datetime.now() $ modelxg3 = xgb.XGBClassifier(max_depth=3, learning_rate=.05, n_estimators=250, n_jobs=-1) $ modelxg3.fit(Xtr.toarray(), ytr) $ print(modelxg3.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
RF = RandomForestClassifier(criterion="entropy",n_estimators=10,n_jobs=-1,max_depth=50) $ RF.fit(X_resampled, y_resampled)
to_be_predicted_Day4 = 25.12100163 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
ax = users.created_at.hist(bins=4380) $ ax.set_xlabel('Date') $ ax.set_ylabel('# Users') $ ax.set_title("Users' account creation per day")
sentiments_pd.to_excel("NewsMood.xlsx", encoding="UTF-8")
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner')
plt.figure(figsize=(10,5)) $ sns.heatmap(data = train_data.isnull(), yticklabels=False, cbar = False)
doctype_by_day.columns.isin(doctype_by_day.min().sort_values(ascending=False)[:10].index)
df_new['intercept'] = 1 $ df_new['UK_ind_treatment'] = df_new['UK']*df_new['treatment'] $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','treatment', 'US', 'UK', 'UK_ind_treatment']]) $ results = logit_mod.fit() $ results.summary()
festivals.at[2,'latitude'] = 41.9028805 $ festivals.head(3)
countries_df = pd.read_csv('countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
predictions = pd.Series.from_array(model.predict(res_c_i.as_matrix())) $ res = res_c_i $ res['predicted_clv'] = model.predict(res_c_i.as_matrix())
substr='BRANCH' $ for each_field in extract_all.columns.values: $     if str.upper(each_field).find(substr)>-1: $         print(each_field)
to_plot_df = df.groupby(['original_language']).mean() $ to_plot_df
%matplotlib inline $ plt.plot(data.sepal_length, data.sepal_width, ls ='', marker='o', label='sepal'); $ plt.plot(data.petal_length, data.petal_width, ls ='', marker='o', label='petal')
rng.asobject
bloomfield_pothole_data.info()
train.male.value_counts()
data_activ = pd.read_csv(r'X:\data_report\01.Result\jobsid\applications\result.csv', chunksize=None)
throwaway_df.select(throwaway_df['count']).show()
wildfires_df['STATE'].unique()
movies[['Position', 'Const']].head()
sample_dic.values()
(pf_data / pf_data.iloc[0] * 100).plot(figsize=(10, 5))
for ele in r.json().keys(): $     print(ele)
df_date[katespadeseries].groupby("date").sum().sort_values(by="postcount",ascending=False)['postcount'][:10]
print (user.index.values.tolist()[:5])
Z = np.ones((10,10)) $ Z[1:-1,1:-1] = 0 $ print(Z)
pickle.dump(final_df_test,open('../proxy/dataset1_test','wb'))
df.drop(columns,1,inplace=True)
df3_results.summary()
import collections $ parnate = df_fda_drugs_reported[df_fda_drugs_reported['simple_name'].str.lower() == 'tranylcypromine'] $ a = [y for x in parnate['adverse_effects'].str.split("; ") for y in x] $ counter=collections.Counter(a) $ [(eff, p/len(counter)) for (eff, p) in counter.most_common(20)]
len(most_freq.imei.unique())
node_names = [i for i in nodereader.iloc[:,0]]
def convert_to_datetime(dt_str): $     fmt = "%Y-%m-%d %H:%M:%S +0000" $     return dt.strptime(dt_str,fmt)
pred.head()
user_frequency = my_df["user"].value_counts() $ print("Shape:", user_frequency.shape) $ print(user_frequency.head(10)) $ print(user_frequency.tail(10))
data_file_sheet_names = sorted(data_file.sheet_names)
display(data.columns[9])
reddit_data = reddit_data.set_index('date')
X_reduced = X[coefs_sorted.head(best_number_of_variables).index] $ X_reduced.shape $ X_reduced['health (real)'] = X['health (real)'] $ X_final_test_3 = X_final_test_2[[c for c in X_final_test_2.columns if "foreign born" not in c]]
rfreg = RandomForestRegressor(random_state=42) $ rfreg.fit(X_train, y_train) $ rfreg.score(X_test, y_test)
plotone = normal_model.plot_components(normal_forecast)
url = form_url(f'actions/{action_id}/calibratedSensorData') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
import pickle $ with open(path, 'bw') as f: $     pickle.dump(positive_model, f)
list(tweet_archive_clean.columns.values)
comments = pickle.load(open('data/comments_with_ratings_df.dat', 'rb'))
df['PCT_change'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.0 $
cur.execute('SELECT material_id, long_name FROM materials WHERE alpha < 1 LIMIT 2') $ for c in cur: print('{} is {}'.format(*c))  # user the cursor as an iterator
Magic.__call__
celtics.head()
df_img_algo.head()
trip_arrive = dt.date(2018, 4, 1) $ trip_leave = dt.date(2018, 4, 15) $ last_year = dt.timedelta(days=365) $ temp_avg_lst_year = (calc_temps((trip_arrive-last_year), (trip_leave-last_year))) $ print(temp_avg_lst_year)
londonDFSubsetWithCounts.ix[:, ['OA11CD', 'count', 'lng', 'lat', 'POPDEN', 'WD11NM_BF']].to_csv('instagram-london-counts-by-OA.csv')
len(df.query('group == "treatment" and landing_page != "new_page"'))+len(df.query('group != "treatment" and landing_page == "new_page"'))
merkmale.columns
intervention_test[intervention_test.index.duplicated(keep=False)]
to_seconds(between_all_posts).plot( $     kind='hist', $     range=(0, 100_000) $ )
df['state_cost'].replace('\$','',regex=True,inplace=True) $ df['state_retail'].replace('\$','',regex=True,inplace=True) $ df['sale'].replace('\$','',regex=True,inplace=True)
autos[["ad_created", $        "date_crawled", $       "last_seen"]].head()
import itertools $ list(itertools.chain.from_iterable(example_nested_list))
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s') $ git_log.describe()
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1 - (0.05/2))
plt.hist(null_vals) $ plt.axvline(x=obs_diff, color = 'red')
twitter_final.info() 
df['landing_page'].value_counts()
infinity.head()
print('Predicted price of the 1st house: {:.1f}'.format(prediction_simple[0])) $ print('Predicted RSS: {}'.format(np.sum((prediction_simple - test_output_simple)**2)))
index = np.where(np.isin(timedifference, max(timedifference)))[0] $ df[['text', 'created_at']].iloc[[1605, 1606]]
!head -n 5 msft.csv
df_joined[sorted(list(df_joined['country'].unique()))] = pd.get_dummies(df_joined['country']) $ df_joined.head()
df = spark.createDataFrame(rdd, schema) $ df.printSchema()
import folium $ from folium.plugins import MarkerCluster $ from folium import IFrame
df2=df2[['TRANSACTION_CURRENCY','VOL','FLOW','REVENUE']] $ df2.head()
y_train.shape
df_ad_airings_5[df_ad_airings_5['location'].isnull()]
df_cleaned3_grouped = df_cleaned3 \ $ .groupby('STATION').sum() \ $ .drop(['ENTRIES','EXITS',"TOTALENTRIES",'TOTALEXITS'], axis=1) \ $ .reset_index()
test_kyo2 = tweets_kyoto_filter[tweets_kyoto_filter['ex_lat']<35.1] $ test_kyo2 = test_kyo2[test_kyo2['ex_long']<135.725300] $ test_kyo2 = test_kyo2[test_kyo2['ex_lat']>34.05] $ test_kyo2 = test_kyo2[test_kyo2['ex_long']>135.705300]
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret)
high_rev_acc_opps_net.rename(columns={'Building ID': 'Total Buildings'}, inplace=True)
df.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATETIME"], inplace=True)
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.title('The normal distribution of mean differents of new_page and old_page converted');
train_col.lr_find(steps=30)
rfc = RandomForestClassifier() $ model=rfc.fit(X_train, y_train)     # fitting the training data
df.date[0]
mean_price = pd.Series(top_brand) $ top20 = pd.DataFrame(mean_price, columns = ['mean_price'])
run txt2pdf.py -o"2018-06-19 2015 CLEVELAND CLINIC HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 CLEVELAND CLINIC HOSPITAL Sorted by discharges.txt"
j = tweets["date"].iloc[2] $ terror["type"] = terror["type"].fillna("Unkown")
postags = sorted(set([pos for sent in train_trees for (word, pos) in sent.leaves()])) $ print(unigram_chunker.tagger.tag(postags))
vow.plot()
np.identity(5)
git_timed = git_log.set_index('timestamp').author $ git_timed.head()
monthly_sales_2015 = liquor_2015[['Sale (Dollars)','Month']].groupby(['Month']).sum() $ month_sales_15 = monthly_sales_2015.plot(kind='bar',figsize=(8,8)) $ month_sales_15.set_ylabel('Sale ($)') $ month_sales_15.set_title('Total Sales Per Month in 2015') $
import missingno as msno $ import seaborn as sns $ sns.set()
df2.info()
p_new = df2[df2['landing_page'] == "new_page"]['converted'].mean() $ p_new
from bson.objectid import ObjectId $ fs.find_one({'_id': ObjectId('56fe67a3b143762514358154')})._id
new_page_converted = np.random.choice([0, 1], size = n_new, p = [1 - p_new_null, p_new_null])
full_globe_temp == -999.000
logistic_model3=sm.Logit(df4['converted'],df4[['intercept','UK','CA','ab_page','UK_page','CA_page']])
unconstrained_costs = get_rebalance_cost(all_rebalance_weights, shift_size, returns.shape[1]) $ print(unconstrained_costs)
model_arma = sm.tsa.ARMA(AAPL_array, (2,2)).fit() $ print(model_arma.params)
%matplotlib notebook $ df1 = var_per_portfolio.toPandas() $ df2 = df1.set_index(['date', 'portfolio']) $
precipitation_df.describe()
print metrics.conftusion_matrix(y_test, predicted) $ print metrics.classification_report(y_test, predicted) $
plt.figure() $ precip_df.plot(kind = 'line', x = 'Date', y = 'Precip') $ plt.legend(loc='best') $
a_mismatch = df.query('group == "control" and landing_page =="new_page"' ) $ b_mismatch = df.query('group == "treatment" and landing_page =="old_page"' ) $ a_mismatch.shape[0] + b_mismatch.shape[0]
df_2016['sales_jan_mar'] = [y if ((x.month >=1) & (x.month <=3)) else 0.0 for x, y in zip(df['Date'],df['sale_dollars'])]
df_link_yt['channel_title'].value_counts().head(30)
autos.rename({'yearOfRegistration' : 'registration_year','monthOfRegistration':'registration_month','notRepairedDamage':'unrepaired_damage','dateCreated':'ad_created','dateCrawled':'date_crawled','offerType':'offertype','vehicleType':'vehicletype','fuelType':'fueltype','nrOfPictures':'nrofpictures','postalCode':'postalcode','lastSeen':'last_seen'},axis=1,inplace=True) $
df_2.drop_duplicates('title', inplace = True)
PRE_LM_PATH = WT_PATH / 'fwd_wt103.h5'
ax2 = sns.boxplot(x = tweets_clean['retweet_count']) $ ax2.set(ylabel = 'tetweet_tweets')
data['Body_Count'].hist(bins=20) # histogram the data with 20 bins. $ plt.title('Histogram of Film Kill Count')
df2.drop(['group', 'landing_page', 'old_page'], axis = 1, inplace = True)
major_cities_layers = major_cities_item.layers $ major_cities_layers
data = data[data.price_aprox_usd.notnull()]
cleansed_search_df = product_searches_df $ cleansed_search_df['CustomerId'] = cleansed_search_df['ga:dimension2'].str.rpartition('-')[0].str.strip() $ cleansed_search_df['CustomerName'] = cleansed_search_df['ga:dimension2'].str.rpartition('-')[2].str.strip() $ cleansed_search_df
df = pd.read_csv('healthy_breakfast.csv', sep='\t') $ df
data.groupby(['Year'])['Salary'].mean()
1-(p_diffs>realdiff).mean()
api.ok(source)
pd.Period("2018-04") # inferred as Month
nx.draw(G, pos=nx.spring_layout(G)) $ plt.show()
temp["time"]=temp.year.apply(lambda x: str(x)+ "-") +temp.month.apply(lambda x: str(x))
segmentData.count()
datatest.info() $
melted_total.head()
X_train, X_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.2)
cpq_status.rename(columns={' X36 MRC List ': 'X36 MRC', ' X36 NRR List ': 'X36 NRR', ' X36 NPV List ': 'X36 NPV'}, inplace=True)
avg_att_2015_MIL = nba_df.loc[(nba_df["Season"] == 2015) & (nba_df["Team"] == "MIL"), "Home.Attendance"].mean() $ round(avg_att_2015_MIL, 0)
apnew = df2.query('group == "treatment"').converted.mean() # actual p new $ apold = df2.query('group == "control"').converted.mean() # actual p old $ actualdiff = apnew - apold $ actualdiff
shiftlog_entries_df.head()
oneDayData = pd.DataFrame(list(cursor))
df_ctrl = df[df['group'] == 'control'] $ df_treat = df[df['group'] == 'treatment'] $ num_ctrl_new_pages = (df_ctrl['landing_page'] == 'new_page').sum() $ num_treat_old_pages = (df_treat['landing_page'] == 'old_page').sum() $ print("The number of times of mismatch: ", num_ctrl_new_pages + num_treat_old_pages)
from sklearn.metrics.pairwise import cosine_similarity
autos.registration_year.value_counts(normalize = True).sort_index()
df_users_products=pd.merge(left=df_user_product_ids,right=transactions,how="left",left_on=["UserID","ProductID"],right_on=["UserID","ProductID"])[["UserID","ProductID","Quantity"]]
from sklearn.preprocessing import PolynomialFeatures $ poly1 = PolynomialFeatures(degree=5) $ x = np.array(hours['hour']).reshape(-1,1) $ print(x)
print(len(countdf['user'].unique())) $ print(len(countdf['user'].unique())-len(count1df['user'].unique())) $ print(len(countdf['user'].unique())-len(count6df['user'].unique()))
date_info.holiday_flg = date_info.holiday_flg!=0 $ date_info.head()
dataframes = [b_cal, b_list, b_rev, s_cal, s_list, s_rev] $ x = 0 $ for frame in dataframes: $     print(frame.shape)
qr1 = df.query("group == 'control' and landing_page == 'new_page'") $ qr2 = df.query("group == 'treatment' and landing_page == 'old_page'") $ print("The number of times the new_page and treatment don't line up : " + str(len(qr1) + len(qr2)))
dupl_row = df2_duplicated_row.index.values # retrieving index of duplicated row $ df2.drop(dupl_row,inplace=True) # dropping duplicated row from df2
final.to_csv('final.csv')
df_new.head(10)
import datetime $ year = datetime.date.today().year $ print year $ print datetime.date.today()
!wget http://files.fast.ai/models/wt103/bwd_wt103.h5 --directory-prefix={WT_PATH} $ !wget http://files.fast.ai/models/wt103/bwd_wt103_enc.h5 --directory-prefix={WT_PATH} $ !wget http://files.fast.ai/models/wt103/fwd_wt103.h5 --directory-prefix={WT_PATH} $ !wget http://files.fast.ai/models/wt103/fwd_wt103_enc.h5 --directory-prefix={WT_PATH} $ !wget http://files.fast.ai/models/wt103/itos_wt103.pkl --directory-prefix={WT_PATH}
movies.shape
((~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0])*100
archive_copy.info()
impact_effort = pd.read_csv('impact_effort.csv')
df.to_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext.csv", sep=',')
TrainData_ForLogistic.shape
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
df['y'].plot.box()
pd.Series(pd.date_range(start='2018-4-14',periods = 10,freq = '12H'))
with open(os.path.join(outputs, 'train_test_data_features.pkl'),'wb') as f: $     pickle.dump((train_data_features_tf, $                  test_data_features_tf, $                  train_data_features_tfidf, $                  test_data_features_tfidf),f)
unidentified = list(gender[gender == -1].index)
results_countries.summary()
maint['datetime'] = pd.to_datetime(maint['datetime'], format="%Y-%m-%d %H:%M:%S") $ maint['comp'] = maint['comp'].astype('category') $ print("Total number of maintenance records: %d" % len(maint.index)) $ maint.head()
poverty_2011_2015.columns=poverty_columns_new
df=pd.read_table("../../data/msft.csv",sep=',') $ df.head()
def fund_vs_coins(date, coins, logy=True): $     iprice_fund = 100*np.exp(fundret.loc[date:].cumsum()).shift(1).fillna(1) # tmp fix. todo make it nicer $     iprice_coins = indexed_price(price_mat.loc[date:, coins]) $     fund_and_coins = pd.concat([iprice_fund, iprice_coins], axis=1) $     return fund_and_coins.plot(logy=logy)
order.columns
X.head()
df_archive_clean["doggo"].add(df_archive_clean["floofer"], $                               fill_value="").add(df_archive_clean["pupper"], $                                                  fill_value="").add(df_archive_clean["puppo"],fill_value="").unique()
X = df3[["ab_page", "c2", "c3"]] $ df3['intercept'] = 1 $ logit_mod = sm.Logit(df3['converted'], X) $ results = logit_mod.fit() $ results.summary()
len(reason_for_visit['Name'].unique())
search['trip_end_weekday_m'] = search.apply(trip_end_weekday_m, axis=1)
df_customers['number of customers'].mean()
liberiaDf.head()
import pandas as pd $ from datetime import timedelta $ %pylab inline $ df_goog = pd.read_csv('CSV/goog.csv')
run txt2pdf.py -o"2018-06-19 2015 MAYO CLINIC HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 MAYO CLINIC HOSPITAL Sorted by discharges.txt"
rfc = RandomForestClassifier() $ rfc.fit(X_train, np.ravel(y_train,order='C')) $ predictions = rfc.predict(X_test) $ accuracy_score(y_true = y_test, y_pred = predictions)
print(irisIterator.get_chunk(10).shape) $ mychunk=irisIterator.get_chunk(10) $ print(mychunk)
duration_train_df.to_csv('./data/hours_training_data.csv')
from fastai.core import BasicModel
number_of_commits = len(git_log) $ number_of_authors = git_log.loc[:, 'author'].dropna().nunique() $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
dr = webdriver.Chrome()
y_pred = logreg.predict(X_test) $ print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))
df['G'] = (df['Rating'] == 'G').astype(int) $ df['PG'] = (df['Rating'] == 'PG').astype(int) $ df['PG13'] = (df['Rating'] == 'PG13').astype(int) $ df['R'] = (df['Rating'] == 'R').astype(int) $ df
ET_Combine = pd.concat([simResis_hour, BallBerry_hour, Jarvis_hour], axis=1) $ ET_Combine.columns = ['Simple resistance', 'Ball-Berry', 'Jarvis']
PANDAS_ODF = os.path.expanduser('~/src/odf_pandas') $ if PANDAS_ODF not in sys.path: $     sys.path.append(PANDAS_ODF) $     from pandasodf import ODFReader
df['date_diff'] = [i.days for i in (df.date - df['date'].shift(1))] $ print ('shape of df:', df.shape) $ df.reset_index(drop = True, inplace = True) $ df.head()
df_questionable = pd.merge(left= df_merged, left_on= 'link.domain_resolved', $                            right= df_os, right_on= 'domain', how= 'left') $ df_questionable['tweet.created_at'] = pd.to_datetime(df_questionable['tweet.created_at'])
df_birth.describe()
df_ab_page.head()
bigram_transformer = Phrases(sentences=flatSentenceList) $ model_bigram = models.Word2Vec(bigram_transformer[flatSentenceList], iter=5)
soup = BeautifulSoup(page.text) $ text= soup.get_text() $ print text
tw_clean.rating_denominator.value_counts()
np.dot(a, a)
df2 = df[((df.group == 'treatment') & (df.landing_page == 'new_page')) | $          (df.group == 'control') & (df.landing_page == 'old_page')] $ df2.shape[0]
old_page_converted = np.random.choice(2,size = 145274,p = [p_old,(1 - p_old)]) $ p_old_bootstrapped = old_page_converted.mean() $ print(p_old_bootstrapped)
score_b.shape[0] / score.shape[0]
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','US','UK','ab_page']]) $ results=logit_mod.fit() $ results.summary()
totalVisits_month_byMC = pd.DataFrame(df_visitsbyCountry.groupby(['marketing_channel', 'country_code', 'year', 'month', 'day'], as_index=False).sum()) $ print 'DataFrame totalVisits_month_byMC: ', totalVisits_month_byMC.shape $ totalVisits_month_byMC.head()
from sklearn.metrics import confusion_matrix $ confuse = pd.crosstab(rfctest['failure'], rfctest['predicted_failure']) $ confuse
companies = pd.read_csv('companies.csv') $ companies.set_index('Ticker', inplace=True) $ companies['tweet_ticker']=companies.index.map(lambda x: '$'+x) $ companies
bb['close'].mean()
dfDay = df.copy(deep=True)
knn.fit(train[['property_type', 'lat', 'lon','surface_total_in_m2']], train['covered/total'])
trump_time.to_csv("trump_time.csv") $ schumer_time.to_csv("schumer_time.csv")
datetime.now().time()
for row in df_Sessions: $     df_Sessions['rt'] = df_Sessions['text'].str[0:2] $ df_Sessions.head(n=20) $ df_Sessions['retweet?'] = np.where(df_Sessions['rt'] == 'rt',1,0) $ df_Sessions.head(n=5)
df1_clean.rating_denominator.value_counts()
for df in list_to_change: $     df["diff_"] = df.Compaction.diff() $     df.fillna(0, inplace = True) $ addicks.head()
np.sum(df["same_location"] == 0)
print(df_cat.shape) $ print(df_crea.shape) $ print(df_loc.shape) $ print(df_us_.shape)
gwt.ggSimp(1,0.0)
ls Dropbox/aula_nc/*.nc $
lsa_tfidf_topic6_sample = mf.random_sample(selfharmmm_final_df, criterion1='non_lda_max_topic', value1='lsa_tfidf_topic6', use_one_criterion=True)
assert rows.pop(0) == ['C/A', 'UNIT', 'SCP', 'STATION', 'LINENAME', $                        'DIVISION', 'DATE', 'TIME', 'DESC', 'ENTRIES', $                        'EXITS'] 
index = pd.DatetimeIndex(['2014-07-04', '2014-08-04', $                           '2015-07-04', '2015-08-04']) $ data = pd.Series([0, 1, 2, 3], index=index) $ data
from nltk.corpus import stopwords $ stop = set(stopwords.words('english')) $ clubs = pd.read_sql("SELECT DISTINCT subreddit FROM comments", con=engine)['subreddit'] $ clubs
year_with_most_commits = commits_per_year.sort_values(by='author', ascending=False).head(1) $ year_with_most_commits = 2017
autos[["odometer_km", "price_euro"]].head()
df.isnull $ df.drop(['Md'], axis=1, inplace=True) $ df = df[:-8] $ df
tweets_pp = pd.concat([multi.reset_index(), pp.reset_index()], axis=1) $ tweets_pp.drop('index', axis=1, inplace=True) $ tweets_pp.head(10)
df_msg[df_msg["subtype"].isnull()].to_csv("messages.csv") $
cust_data1.columns
%time tsvd = tsvd.fit(train_4)
df3[['CA','UK', 'US']] = pd.get_dummies(df3['country']) $ df3 = df3.drop('CA', axis = 1) $ df3.head()
lrf=learn.lr_find() $
df_namb = df_namb[df_namb.index != 'mdb68'] $ df_ndoor = df_ndoor[df_ndoor.index != 'mdb68'] $ df_nmcu = df_nmcu[df_nmcu.index != 'mdb68'] $ df_nrelay = df_nrelay[df_nrelay.index != 'mdb68'] $ df_ntemp = df_ntemp[df_ntemp.index != 'mdb68']
(events.query('doc_type=="CNI" & index < "20170110" & ~status') $       .head())
fashion.columns
p=df2['converted'].mean() $ p
from pyspark.ml.clustering import KMeans $ kmeans = KMeans(featuresCol='features', predictionCol='prediction',k=4) $ pipeline = Pipeline(stages=[assembler, kmeans]) $ PipelineModel = pipeline.fit(data) $ predictions = PipelineModel.transform(data)
%%time $ PredClass.add_dummies()
df.head()
unique_users = df.user_id.nunique()
rng = pd.bdate_range(start, end)
retweets_status_keys = list(retweets[0]._json.keys()) $ len(retweets_status_keys)
pn_and_qty_with_duplicates = pd.read_csv('data/PN_and_QTY_with_duplicates.csv')
import os $ REPO = "/content/datalab/training-data-analyst" $ os.listdir(REPO)
main = pd.read_csv("1986_2016_seasons_shifted_v1.csv") $ main.shape
data_countries = pd.read_csv("./data/countries.csv") $ data_age_gender_bkts = pd.read_csv("./data/age_gender_bkts.csv")
df.iloc[0, 2] = 100 $ df
test.head()
archive_clean.drop('timestamp',axis=1,inplace=True)
file_name = 'campaign_finance_clean_data.csv' $ dat.to_csv(path_or_buf=file_name,sep=',')
pd.to_datetime(['2009/07/31', 'asd'], errors='raise')
tweets_kyoto_filter.reset_index(inplace=True)
periods = [pd.Period('2012-01'), pd.Period('2012-02'), pd.Period('2012-03')]
titanic.drop('deck', axis=1, inplace=True)
marketdata_df = pd.read_sql_table(table_name="marketdata", con=engine) $ marketdata_df.set_index('CARVER', inplace=True) $ marketdata_df.head()
S_1dRichards.meta_basinvar.filename
archive_clean = pd.read_csv('archive_clean.csv') $ images_clean = pd.read_csv('images_clean.csv') $ popularity_clean = pd.read_csv('popularity_clean.csv')
p_housing=portland_census2.drop(portland_census2.index[:26]) $ p_housing.head(6)
pd.Series(data=predicted5).value_counts()
ldamodel.save("lda.model")
mustang.wheels
from sklearn.feature_extraction.text import CountVectorizer $ cvec = CountVectorizer() $ cvec.fit(my_df.text)
df1=pd.concat([df['author'], df['edited'].sub(df['dated'])], axis=1) $ df1.columns=['author','Duration'] $ df1 = df1[df1.Duration.notnull()] $ df1.nsmallest(5, 'Duration')
popC15[popC15.content == 'photo'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
df_vow['Date'] $ type(df_vow['Date'].loc[0])
%matplotlib inline $ counts_compare.sort_values('Count_Diff', ascending=False)
rngE = pd.date_range(start = '08/01/2018', end = '08/20/2018', freq = bEgypt) $ rngE
twitter_ar.drop(['retweeted_status_id','source','rating_numerator','rating_denominator','timestamp','in_reply_to_status_id','in_reply_to_user_id','retweeted_status_user_id','retweeted_status_timestamp'],axis=1,inplace=True)
df = df[ ['Department', 'Lottery', 'Literacy', 'Wealth', 'Region'] ] $ df.tail(5)
(a + b).describe() $
scaled_df = scale_df(source_df, ['date']) $ scaled_df.tail() $ scaled_df.pop('date')
weather_data1 = pd.read_csv('201402_weather_data.csv'); weather_data1.head()
log_mod2 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','US','new_US', 'UK', 'new_UK']]) $ results_log2 = log_mod2.fit() $ results_log2.summary()
autos.describe(include='all') $
twitter_df.query('tweet_id==884441805382717440')
results3 = logistic_countries2.fit() $ results3.summary()
ds_train = FileDataStream.read_csv(train_file, collapse=False, header=False, names=columns, numeric_dtype=np.float32, sep='\t', na_values=[''], keep_default_na=False) $ print(repr(ds_train.Schema)) $ print(ds_train.Schema)
import examples.simple.backtest_pkg.PipelineControl as pc $ pipe = pc.PipelineControl(data_path='examples/simple/data/fixed_process_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=True) $ pipe.runPipeline()
stars.info() $ owns.info()
try: $     pd.concat([x, y], verify_integrity=True) $ except ValueError as e: $     print('ValueError:', e)
df3.groupby('OriginCityName')['Origin'].count().head() # number of flights per Origin
clinton_pivoted.plot.barh()
green_line = data[:training_split_cut] $ purple_line = [] $ for iter_x in np.arange(training_split_cut): $     purple_line.append(np.nan) $ purple_line = purple_line + list(X_test_predict_i[:,0])    
dfsunrise = pd.read_pickle('sunrise120120102017.pkl')
local.get_dataset(3)
print(X.todense()) $ print("") $ print("Words for each feature:") $ print(vectorizer.get_feature_names())
from datetime import datetime $ import pyspark.sql.functions as functions
df['log_price'] = np.log(df['price_doc']) $ df.head()
n_new, n_old = df2['landing_page'].value_counts() $ print("new:", n_new, "\nold:", n_old)
(v_invoice_hub.loc[:, invoice_hub.columns] == invoice_hub).sum()
ac['Monitoring End'].groupby([ac['Monitoring End'].dt.year]).agg('count')
f0.exclude_list_filter
n_old = df2[df2['landing_page'] == 'old_page'].user_id.count() $ n_old
obs_diff = p_treatment- p_control $ (p_diffs > obs_diff).mean()
df_users_1.shape
res = es.search() $ print(res["_shards"]) $
Active_stations = session.query(measurement.station, station.name, measurement.date, func.count(measurement.tobs)).\ $                           join(station, station.station == measurement.station).\ $                           group_by(measurement.station).\ $                           order_by(func.count(measurement.tobs).desc()).all() $ Active_stations $
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
ttarc.info() #The others are fine in terms of completeness, so I must address the missing data first, which occur $
shirt_1.price
pd_data['year']=pd_data.createdAt.map(lambda x:time.strftime("%Y",time.localtime(int(x)/1000)) if x else x) $ pd_data['month']=pd_data.createdAt.map(lambda x:time.strftime("%m",time.localtime(int(x)/1000)) if x else x) $ pd_data['week']=pd_data.createdAt.map(lambda x:time.strftime("%w",time.localtime(int(x)/1000)) if x else x) $ pd_data['day']=pd_data.createdAt.map(lambda x:time.strftime("%d",time.localtime(int(x)/1000)) if x else x) $ pd_data['pay']=pd_data.status.map(lambda x: 0 if x in ['0','1','2'] else 1)
movies.overview[6]
df.drop(['County', 'Category Name', 'Vendor Number', 'Item Description', 'Volume Sold (Gallons)'], axis=1, inplace=True)
P.plot_1d_layer('mLayerVolFracWat')
print(len(train_df['os'].unique()))
aapl = quandl.get('WIKI/AAPL') $ aapl.head()
from scrapy.crawler import CrawlerProcess $ import scrapy
stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)}) $ len(stoi)
data_chunks = pd.read_csv("Data/microbiome.csv", chunksize=14) $ mean_tissue = pd.Series({chunk.Taxon[0]: chunk.Tissue.mean() for chunk in data_chunks}) $ mean_tissue
df_h1b_nyc = df_h1b_nyc.drop([33359, 66488, 104536, 166920, 279967, 342136,426706])
df_sites.loc[125:135] $ id_numbers_to_drop = df_sites.loc[:len(idx_places_less_million),'id_num'] $ id_numbers_to_drop.sort_values()[:10]
df = table[0] $ df.columns = ['Parameter', 'Values'] $ df.head()
autos['make'].cat.categories
old_page_converted= np.random.binomial(1, p_old, n_old)
df2.drop_duplicates(inplace=True)
df3 = pd.DataFrame(columns=['to','fr','ans']) $ df3.fr = [pd.Timestamp(df_new.timestamp.min())] $ df3.to = [pd.Timestamp(df_new.timestamp.max())] $ df3.ans=(df3.to-df3.fr).astype('timedelta64[D]') $ df3.head()
df_train['hour_of_day'] = tsne_train_hourOfDay $ df_test['hour_of_day'] = tsne_test_hourOfDay $ print(df_train.shape) $ df_train.head()
df5 = make_df('ABC', [1, 2]) $ df6 = make_df('BCD', [3, 4]) $ display('df5', 'df6', 'pd.concat([df5, df6])')
categories = total['tag'].unique()
for elem in titles: $     print elem.text
c_array = np.concatenate([training_active_listing_dummy, training_pending_ratio], axis=1) $ print(c_array.shape) $ print(c_array[0:5, :])
lm2 = sm.Logit(df['converted'], df[['intercept', 'ab_page', 'US', 'UK']]) $ results2 = lm2.fit() $ results2.summary()
df4 = df3.reindex(index=d[0:4], columns=list(df.columns) + ['E']) $ df4.loc[d[0]:d[1],'E'] = 1 $ df4
shows['cancelled'].value_counts() $
df_questionable_3[df_questionable_3['state_TX'] == 1]['link.domain_resolved'].value_counts()
np.save(LM_PATH / 'tmp' / 'train_ids.npy', trn_lm) $ np.save(LM_PATH / 'tmp' / 'val_ids.npy', val_lm) $ pickle.dump(itos, open(LM_PATH / 'tmp' / 'itos.pkl', 'wb'))
ax = mains.plot() $ co.steady_states['active average'].plot(style='o', ax = ax); $ plt.ylabel("Power (W)") $ plt.xlabel("Time");
n_new = df2.query('group == "treatment"').user_id.count() $ n_new
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit() $
support.amount.sum()/merge.amount.sum()
path = pathlib.Path('deep-review/content/metadata.yaml') $ with path.open() as read_file: $     metadata = yaml.load(read_file)
msft = pd.read_csv('msft.csv', index_col=0) $ msft.head()
tcat_df.to_csv('cat_tweets.csv') $ tdog_df.to_csv('dog_tweets.csv')
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].count()
optimizer_col= optim.Adam(model.parameters(),lr=0.001)
search = "SELECT * FROM specimen_tags" $ search_result = limsquery(search) $ first_element = search_result[0] $ print first_element.keys()
import random $ x = [random.randint(0,5) for x in range(100)] $ y = [random.randint(20,50) for x in range(100)]
log_data = ParseBroLogs("2018-05-26/conn.00_00_00-01_00_00.log") $ regular_traffic = pd.DataFrame(json.loads(log_data.to_json()))
brands = autos['brand'].unique()
all_sets.loc[:, ["name", "releaseDate", "setSize"]].sort_values(["releaseDate"]).tail()
datAll['Street_name'] = np.where(~pd.isnull(datAll['Street Name']),datAll['Street Name'],np.nan) $ datAll['Street_name'] = np.where(~pd.isnull(datAll['StreetName']),datAll['StreetName'],datAll['Street_name'])
plt.hist(null_vals); $ plt.axvline(x=obs_diff, color='red') $ plt.show() $
seaborn.boxplot(areas_dataframe.rate)
prop_caba_gba = propiedades[propiedades['state_name'].str.contains('Capital Federal') | propiedades['state_name'].str.contains('G.B.A.')] $ prop_caba_gba.sample()
df2[df2.converted == 1].shape[0]/df2.shape[0]
with open(marvel_comics_save, mode='w', encoding='utf-8') as f: $     f.write(wikiMarvelRequest.text)
station_counts = station_df.groupby('Station ID').count().sort_values(by = 'Precip', ascending = False) $ station_counts
joined_patient.corr('avg_ldl', 'max_triglycerides')
top_songs['Date'].dtype
tweet_archive_clean.name.value_counts().head()
unique_urls.sort_values('total_votes', ascending=False)[0:20][['url', 'total_votes']]
autos = autos[autos['registration_year'].between(1900,2018)] $ autos['registration_year'].value_counts(normalize=True).head(20)
twitter_archive_clean.describe()
planets.dropna().describe()
v_invoice_hub.merge(v_item_hub, left_on='fk_s_change_context_id_cr', right_on='fk_s_change_context_id_cr', how='inner').shape
out.applymap(lambda x: type(x))
save_n_load_df(joined, 'joined_promo_bef_af.pkl')
joined.head()
pop_result = pop_rec.recommend()
hashtaglist = [] $ for w in wordlist: $     if w.startswith("#"): $         print(w) $         hashtaglist.append(w)
temp_obs_df = pd.DataFrame(temp_obs) $ temp_obs_df.head()
sentiment_df.sort_values(by="Date",inplace=True,ascending=True)
result_1.summary()
df2[df2.user_id == 773192]
(p_diffs > (treat_convert - control_convert)).mean()
df_train.head(887)
np.random.seed(30) $ data = list(zip(character_list, date_list, game_mode_list, league_list, match_id_list, ranked_list, win_list)) $ update = pd.DataFrame(data = data, columns=['character', 'date', 'game_mode', 'league', 'match_id', 'ranked', 'win']) $ update = pd.concat([ranked, update], ignore_index=True) $
df2['quarter_opened'] = df2['account_created'].map(lambda x: int((x.month+2)//3))
a = df1.io_state[7] $ a.zfill(8)
e_p_b_one = cfs_df[cfs_df.Beat=='5K02'] $ e_p_b_one = e_p_b_one.groupby(e_p_b_one.TimeCreate).size().reset_index()
s_n_s_epb_one.Date = s_n_s_epb_one.Date.str.replace('-',"")
(d + pd.tseries.offsets.Week(weekday=4)).weekday()
np.exp(-0.0150), np.exp(-0.0408),np.exp(0.0099)
dfg['cluster'].value_counts()
features = list(status_dummies.columns) + ['message_character_count', 'message_word_count']
AAPL.tail()
df.isnull().any() 
data.show() $ data.printSchema() $
def add_constant_index_level(df: pd.DataFrame, value: Any, level_name: str): $     return pd.concat([df], keys=[value], names=[level_name]) $ df = add_constant_index_level(df, "Booooze", "Department") $ df = df.reorder_levels(order=['Date', 'Store', 'Department', 'Category', 'Subcategory', 'UPC EAN', 'Description']) $ df.head(3) $
print(label((predicted_image)>180).max())
print(arma_mod20.aic, arma_mod20.bic, arma_mod20.hqic) $ print(arma_mod30.aic, arma_mod30.bic, arma_mod30.hqic) $ print(arma_mod40.aic, arma_mod40.bic, arma_mod40.hqic) $ print(arma_mod50.aic, arma_mod50.bic, arma_mod50.hqic)
flight_delays = pd.read_csv("../clean_data/delays_per_hour.csv")
df['age'] = 2017-df['birth year'] $ df.age.describe()
print(train_data.shape) $ print(test_data.shape)
data.head(10)
kNN100.fit(X, y)
for i in range(artificial_returns.shape[1]): $     plt.plot(returns.iloc[:,i].cumsum(), color=colors[i]) $ plt.legend(loc=3, bbox_to_anchor=(1.0,0.5)) $ plt.show()
Grouping_Year_DRG_discharges_payments.groupby(['A']).sum() $ df.groupby(['A']).size()
results.sort_index().head() $ results.sort_index(ascending=False).head() $ results.sort_index(axis=1).head() $ result.sort_values() $ results[['home_score','away_score','home_team']].sort_values(ascending=[False,True],by=['home_score','away_score']).head()
balance = r.groupby(level=0)[['net']].sum()
baseline = 1 - reddit['comm_range'].mean() $ print 'Baseline accuracy is:', round(baseline, 2)
start_date = "2017-07-01"; $ end_date = "2017-07-10" $ trip_temp = calc_temps(start_date,end_date) $ trip_temp_df = pd.DataFrame(trip_temp,columns = ['Max-Temp','Min-Temp','Mean-Temp']) $ trip_temp_df $
cust_count = data.cust_id.value_counts().reset_index() $ cust_count.columns = ['cust_id','cust_count']
top20_branks_mean_price_mileage.sort_values(by='mean_price')
tc = pd.concat([tce2, tcp], ignore_index=True) $ tc
move_1_23st = sale_lost(breakfastlunchdinner.iloc[1, 1], 20) $ move_2_23st = sale_lost(breakfastlunchdinner.iloc[3, 2], 20) $ adjustment_1 = move_1_23st + move_2_23st $ print('Adjusted total for route: ' + str(move_34p23s34p - adjustment_1))
sns.barplot(x=['Actual 0', 'Pred 0', 'Actual 1', 'Pred 1'], $             y=[y_unseen.value_counts()[0], pd.Series(unseen_predictions).value_counts()[0], $                y_unseen.value_counts()[1], pd.Series(unseen_predictions).value_counts()[1]]) $ sns.set(rc={'figure.figsize':(15.5, 10.27)})
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['index'])
sns.barplot(x='user',y='retweets',data=highly_retweeted)
aSL.loc[(aSL.BRANCH_STATE.isnull())].shape
merged_predictions_archive.head(1)
extract_all.loc[(extract_all.duplicated('APP_SSN')==True) $                &(extract_all.APP_SOURCE=='DOT818') $                &(extract_all.application_month=='2018-04'), $                fields].sample(3)
agency_borough = data.groupby(['Agency','Borough']) $ agency_borough.size().unstack().plot(kind='bar',title="Incidents in each Agency by Borough",figsize=(15,15)) $
idx_names = all_sites_with_unique_id_nums_and_names['name'].unique() $ print (color.RED + color.BOLD + 'check_df ' + color.END) $ print('Contains: \n',len(idx_names),'unique site names') $ idx_id_nums = check_df['id_num'].unique() $ print('and \n',len(idx_id_nums),'unique site ID numbers') $
%timeit articles['tokens'] = articles['content'].map(nltk.tokenize.RegexpTokenizer('[A-Za-z]+').tokenize)
weather.Date = pd.to_datetime(weather.Date)+MonthEnd()
g[['blurb','launched_at','state_changed_at','days_to_change','state']].filter(lambda x: len(x) > 1).sort_values('blurb')
dftop.head()
np.exp(0.0507)
from pandas.plotting import scatter_matrix $ axes = scatter_matrix(data.loc[:, "TMAX":"TMIN"])
d4 = df.iloc[[0, 3], [1, 2]]
l=len(col) $ for i in range(1,l): $     X=np.column_stack((X,col[i]))
dfLikes.sort_values(by="date", ascending=True, inplace=True)
%%javascript $ $('.math>span').css("border-left-color","transparent")
autos[['date_crawled','last_seen','ad_created','registration_month','registration_year']].dtypes
week13 = week12.rename(columns={91:'91'}) $ stocks = stocks.rename(columns={'Week 12':'Week 13','84':'91'}) $ week13 = pd.merge(stocks,week13,on=['91','Tickers']) $ week13.drop_duplicates(subset='Link',inplace=True)
df_new.set_index('email') $ (df_new.style $     .set_table_styles(styles)) $
cityID = '04cb31bae3b3af93' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Miami.append(tweet) 
filtered_keys = [] $ for key, item in wine_count.items(): $     if item < threshold: $         filtered_keys.append(key)
df.head()
lm=sm.Logit(df2['converted'],df2[['intercept','cntry_US','cntry_UK']]) $ results=lm.fit() $ results.summary()
full['DaysSinceAdmission'] = full[full['Readmitted'] == 1].groupby(['Patient']).diff()['AdmitDate']
df4.describe()
iowa_fset = iowa_fc.query()
X_test.shape
to_be_predicted_Day4 = 82.26274673 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
tweets.dtypes
df['usr_lat'] = df['user_timezone_coord'].apply(lambda x: None if type(x) is float else x['lat']) $ df['usr_lng'] = df['user_timezone_coord'].apply(lambda x: None if type(x) is float else x['lng'])
df = pd.DataFrame(data = pd.Series(range(12)).reshape(3, 4), columns = list('abcd')) $ df
pMean = np.mean([pnew, pold]) $ NewPage = np.random.choice([1, 0], size=new,p=[p, (1-p)]) $ new_avg = NewPage.mean() $ print(new_avg)
complaints2016_geodf.crs = nyc_census_tracts.crs
train_data["totals.transactionRevenue"] = train_data["totals.transactionRevenue"].astype('float') $ revenue = train_data.groupby("fullVisitorId")["totals.transactionRevenue"].sum().reset_index()
print(model) $ print(model.wv.vocab)
rfc = ensemble.RandomForestClassifier() $ rfc.fit(tfidf_X_train, tfidf_y_train) $ print('Training set score:', rfc.score(tfidf_X_train, tfidf_y_train)) $ print('\nTest set score:', rfc.score(tfidf_X_test, tfidf_y_test)) $ print('\nCross Val score:',cross_val_score(rfc, tfidf_X_test, tfidf_y_test, cv=5))
for p in mp2013: $     print("{0} {1} {2} {3}".format(p,p.freq,p.start_time,p.end_time))
stats['commit'] = commit_df.commit.iloc[-1]
families = train_holiday_oil_store_transaction_item_test.select("family").distinct().rdd.flatMap(lambda x: x).collect() $ exprs = [F.when(F.col("family") == family, 1).otherwise(0).alias('family' + family) $          for family in families] $ train_holiday_oil_store_transaction_item_test.select("item_nbr", *exprs).show() $ item_families = train_holiday_oil_store_transaction_item_test.select("item_nbr", *exprs)
chart = top_supporters.head(5).amount.plot.barh() $ chart.set_yticklabels(top_supporters.contributor_fullname)
logit = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','UK','US']]) $ result=logit.fit()
df['converted'].mean()
writer = pd.ExcelWriter('output.xlsx') $ frame.to_excel(writer,'Sheet1') $ writer.save()
user1 = user1[(user1.CallTime < "06:00:00") | (user1.CallTime > "22:00:00") ] $
file = open('tweets/tweets_Trump.csv', 'w') $ for tweet in results: $     file.write("%s\n" % tweet)
df2.loc[2893]
p_diff=(new_page_converted.mean()) - (old_page_converted.mean()) $ p_diff
autos['odometer_km'].hist()
tree = DecisionTreeClassifier(criterion='entropy',max_depth=5) $ tree.fit(X_train,y_train)
sort_b_desc = noNulls['b'].desc()
idx_close = r.json()['dataset']['column_names'].index('Close')
df.tail(2) # prints the last 2 rows
data.describe()
geocode_endpoint = 'https://maps.googleapis.com/maps/api/geocode/json?'
clients['join_month'] = clients['joined'].dt.month $ clients['log_income'] = np.log(clients['income']) $ clients.head()
country=pd.read_csv('countries.csv')
np.exp(results3.params)
train_df[train_df.author.isnull()].head(1)
my_list = [0.25, 0.5, 0.75, 1.0] $ data = pd.Series(my_list, index=[1,2,3,4]) $ data
sydney_friends=sydney.groupby('userTimezone')[['userFriendsCt','userFollowerCt']].mean() $ sydney_friends=sydney.sort_values(['userFriendsCt'],ascending=False) $ sydney_friends.head(10) $ len(sydney_friends)
import matplotlib.pyplot as plt $ %matplotlib inline
print(pd.merge(All_tweet_data, Imagenes_data_v2, on='tweet_id', how='inner').shape) $ print(pd.merge(twitter_data_v2, tweet_data_v2, on='tweet_id', how='inner').shape)
surprise_slice.iloc[0]
df['date'] = pd.to_datetime(df['date'])
store_items = store_items.append(new_store, sort=False) $ store_items
fashion.head()
final_test_pred_nbsvm1 = test_probs.idxmax(axis=1).apply(lambda x: x.split('_')[1])
fname = "C://Users/dirkm/Meditationszeiten_ab_Okt_2017.csv" $ f1name = path.expanduser(fname) $ df = pd.read_csv(f1name) $ df.head()
df.describe()
df.head(1)
df.columns
dfM = df.copy()
treatment = df2[df2['group']=='treatment'] $ treatment.head(5)
[list(np.ravel(item)) for item in normals] 
print(spmat.shape)
sum(tw.retweeted_status_id.notnull())
print community.modularity(politiciansPartyDict, Gun, weight='weight') $ print community.modularity(partitions, Gun, weight='weight') $ print len(set(partitions.values()))
train.head()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger')
np.mean(new_page_converted) - np.mean(old_page_converted)
some_rdd = sc.parallelize([Row(name=u"John", age=19), $                            Row(name=u"Smith", age=23), $                            Row(name=u"Sarah", age=18)]) $ some_rdd.collect()
pd.Timestamp('2018-01-01') + Hour(3) + (5 * Minute())
for div in soup.find_all('div',class_='body'): $     print(div.get_text())
%matplotlib inline $ import warnings $ warnings.filterwarnings('ignore')
xx,yy = fractionate(readertest, 0.1)
df.fillna(value=-1)  # Fill missing data with a value (not in-place)
df_joined = df2.join(df3.set_index('user_id'),on ='user_id') $
station.dtypes
knn.fit(train[['property_type', 'lat', 'lon','surface_covered_in_m2']], train['covered/total'])
p_diffs = np.array(p_diffs) $
x = [] $ with open("data1.csv") as f: $     reader = csv.reader(f) $     for line in reader: $         x.append(line)
imdb_df.drop(axis=1, labels='Your Rating', inplace=True) $ imdb_df
tweet_info.info()
from IPython.display import display $ pd.options.display.max_columns = None $ twitter_archive_master.head()
import numpy as np $ cs = np.log(df['Size']) $ cs = cs.reset_index()
print('The number of unique species in 200-km circle around the Petropavlovsk-Kamchatsky city:', len(petropavlovsk_filtered.species_id.unique()))
for i,x in top_likes.iteritems(): $     print ('https://www.facebook.com/'+x )
dataA.head()
files = [] $ for filename in glob.glob('data/*.csv'): $     print filename $     files.append(filename)   
df_merge.groupby(['school_name','grade']).reading_score.mean().unstack()
print('Bitcoin movement predictors accuracy score(with weights)') $ logreg1.score(A_test,b_test)
(df2.query('landing_page == "new_page"').count()[0])/df2.shape[0]
daily_revenue = daily_revenue.select(col("date").alias("date"), col("sum(trans_amt)").alias("daily_rev_sum")) $ daily_revenue.show()
data_vi.describe()
n_old_page = len(df2.query("group == 'control'")) $ print(n_old_page)
wrd_clean['name'].value_counts()[:10]
dfFull['LotFrontageNorm'] = dfFull['LotFrontage']/dfFull['LotFrontage'].max()
learner.sched.plot_loss() $
members_states['members_congress'] = members_states['members_congress'].map(float)
doc = "Human computer interaction" $ vec_bow = dictionary.doc2bow(doc.lower().split()) $ vec_lsi = lsi[vec_bow] # convert the query to LSI space $ print(vec_lsi)
plot_df2017 = age_plot.unstack('age').loc[:, 'Sales'] $ plot_df2017.index = pd.PeriodIndex(plot_df.index.tolist(), freq='0') $ plot_df.plot()
df2 = df2.drop_duplicates(['user_id'], keep='first')
Sheet10_Meta = pd.read_excel(DATA_FOLDER +'microbiome/metadata.xls') $ allFiles = glob.glob(DATA_FOLDER + 'microbiome' + "/MID*.xls") $ allFiles
df.count() $ df.nunique()
hemis_url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(hemis_url)
drop_columns = ['membership_expire_date' , 'registration_init_time', 'reg_mem_duration'] # to reduce memory usage $ df_group_by = df_group_by.drop(drop_columns, 1)
train.groupby("DOW")["any_spot"].mean()
fashion['created'] = pd.to_datetime(fashion.created) $ fashion.set_index('handle', inplace=True) $ fashion.dtypes
check_wait_corr().groupby('wait').mean()
data = pd.read_csv(datafile,index_col='Unique Key')
unique_df = df2.drop_duplicates('user_id') $ unique_df.nunique()
np.array(df1.index)
clf = decomposition.NMF(n_components=n_topics, random_state=1)
df.dtypes
seaborn.countplot(company_vacancies.weekday)
np.sqrt(mse)
box[0:box.index(')')]
learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)
df2['new_page'] = pd.get_dummies(df2.landing_page)['new_page']
import matplotlib.pyplot as plt $ % matplotlib inline $ plot=twitter_data.rating_numerator.hist(bins=50) $ plot=plot.set_xlim([0,200])
grouped_merged = merged.groupby(['contributor_firstname','contributor_lastname','committee_position'])['amount'].sum().reset_index().sort_values('amount', ascending = False)
r = requests.get('https://httpbin.org/basic-auth/myuser/mypasswd', auth=('myuser', 'mypasswd')) $ r.status_code
print(df['converted'].mean()*100)
MAE_score = metrics.mean_absolute_error(y_holdout, holdout_preds) $ MSE_score = metrics.mean_squared_error(y_holdout, holdout_preds) $ RMSE_score = np.sqrt(metrics.mean_squared_error(y_holdout, holdout_preds)) $ R2_score = lr.score(x_holdout, y_holdout)
blame = \ $   blame_raw.raw.str.extract( $       "(?P<sha>.*?) (?P<path>.*?) \((?P<author>.* ?) (?P<timestamp>[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2} .[0-9]{4}) *(?P<line>[0-9]*)\) .*", $       expand=True) $ blame.head()
nltk.FreqDist(list(tweet_hour['tweet_text'].apply(tweet_tokenizer.tokenize)))
req_test.text
c.execute(query) $ results = c.fetchall() $ df = pd.DataFrame(results) $
df = pd.read_csv("/data/311_Service_Requests_from_2010_to_Present.csv", error_bad_lines=False) $
np.sum(sample_x1[0],axis=-1)
twitter_archive_enhanced[twitter_archive_enhanced.duplicated('tweet_id')]
print("Number of Relationships in ATT&CK") $ print(len(all_attack['relationships'])) $ relationships = all_attack['relationships'] $ df = json_normalize(relationships) $ df.reindex(['id','relationship', 'source_object', 'target_object'], axis=1)[0:5]
Z = np.random.randint(0,3,(3,10)) $ print((~Z.any(axis=0)).any())
n_old = df2.query('landing_page == "old_page" and group == "control"').user_id.nunique()
sample_sizes = non_blocking_df_save_or_load( $     raw_sample_sizes, $     "{0}/sample_sizes_10".format(fs_prefix))
csv_file2 = "countries.csv" $ df3 = pd.read_csv(csv_file2) $ df3.head()
excelref_df.head(1)
week_analysis_df = calc_temps('2016-08-23', '2016-09-05') $ week_analysis_df.head()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'country_UK','country_US']]) $ results = logit_mod.fit() $ results.summary()
df = pd.read_csv('basic_statistics_single_column_data.csv')
tmp = new_df['meantempm'] - gd_predictions $ plt.style.use('seaborn-whitegrid') $ plt.hist(tmp, bins=200, alpha = 0.75) $ plt.show()
print('N recalls:', len(recalls)) $ print('Percentage: {0:.2f}%'.format((len(recalls) / len(df)) * 100)) $ recalls.groupby('ACTION REQUESTED').size()
df.groupby(level=0).count()
import matplotlib.pyplot as plt $ mat_plt_plt=plt.figure() $ plt.plot(predict_actual_df['prediction'][0:200]) $ plt.plot(predict_actual_df['actual'][0:200])
yc_new = yc_depart.merge(destinationZip, left_on='Unnamed: 0', right_on='Unnamed: 0', how='inner') $ yc_new.shape
train.drop('Hours', axis = 1, inplace = True) $ test.drop('Hours', axis = 1, inplace = True)
builder = contest_data.select(*(F.col(x).alias(x + '_cd') for x in contest_data.columns)) $ builder = contest_savm.join(builder, builder.sales_acct_id_cd == contest_savm.sales_acct_id, how = 'inner') # Exploding 68 times
df['Descriptor'].value_counts().head(10)
linear_svc = LinearSVC() $ linear_svc.fit(X_train, Y_train) $ Y_pred = linear_svc.predict(X_test) $ acc_linear_svc = round(linear_svc.score(X_test, Y_test) * 100, 2) $ acc_linear_svc
ti_jd.rename(columns={'review_image':'image_url','#comments':'review_image','harvest_product_description':'product_description','retailer_product_code':'rpc','user_id':'username'}, inplace=True) $ ti_jd['store'] = 'JD' $ ti_jd = ti_jd[ti_clm] $ ti_jd.shape
input_data = input_data[[x for x in input_data.columns if x!='snow_depth']]
future_dates = prophet_model.make_future_dataframe(periods=forecast_steps, freq='W') $ future_dates.tail(3)
speakers.set_index('id',inplace = True)
df2.query('user_id=="773192"')
jobs_data2 = json_normalize(json_data2['page']) $ jobs_data2.head(5)
offices.info()
dfEPEXbase = pd.DataFrame() $ dfEPEXbase['Price'] = EPEXprices $ dfEPEXbase['Volume'] = EPEXvolumes $ dfEPEXbase.head() # verify generated data frame
converted_df = df.query('converted==1') $ converted_df.user_id.nunique()/df.user_id.nunique()
a = np.arange(1, 5); np.sqrt(a)
combined_df[['Row ID']] = combined_df[['Row ID']].astype(object) $ combined_df = combined_df.rename(columns={'Quantitie' : 'Quantity'}) $ print(combined_df.dtypes)
print('Slope: Efthymiou vs experiment: {:0.2f}'.format(popt_opb_brace_saddle[2][0])) $ perr = np.sqrt(np.diag(pcov_opb_brace_saddle[2]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
lm2=sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results=lm2.fit() $ results.summary()
ah = list(t.follower_ids(user="ahernandez85b")) $ rl = list(t.follower_ids(user="rlyor")) $ ma = list(t.follower_ids(user="mandersonhare1")) $ print(len(ah),len(rl),len(ma))
df['w'].unique()
YS1517['Adj Close'].corr(YS1315['Adj Close'],method = 'pearson')
to_be_predicted_Day3 = 14.84307103 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
old_page_converted = np.random.choice(2, size=145274, replace=True, p=[0.8804, 0.1196])
!hdfs dfs -cat 321_sorted-results-output/part-0000* > 321_sorted-results-output.txt $ !tail 321_sorted-results-output.txt
idx = pd.IndexSlice $ health_data_row.loc[idx[:, :, 'Bob'], :]  # very close to the naive implementation
reliableData = df_data_1[df_data_1.DATE >= "2017-07-03 00:00:00"] $ print reliableData
people.groupby(lambda x: GroupColFunc(people, x, 'a')).groups
df_mes = df_mes[df_mes['travel_time'] > 0] $ df_mes.shape[0]
interact_oil = interact(plot_oil,qi_oil= (1, 10000, 50), b_value= (0, 2, 0.01), initial_decline_rate= (0, 1.5, 0.01), exp_yes_no = False, start_of_exp= (1, well_data.time.max(), 1), exp_decline_rate=(0, 0.02, 0.001))
df = pd.read_csv("results.csv") $ print (df.shape) $ df.head()
blob = TextBlob(text.encode('ascii','ignore'))
df2.tail(2)
tweets = megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False] $ megmfurr_tweets[megmfurr_tweets["text"].str.contains("RT")==False]['text'].count() # 895
df_t_worst = df_t[df_t['Shipping Method name'].isin(['PostNL service point 0-30kg',53,'DPD Pickup Point'])]. \ $             groupby(['Place Name','State']).filter(lambda x: len(x) >= 5)
page_dummies = pd.get_dummies(df_new.landing_page) $ df_new = df_new.join(page_dummies) $ df_new['CA_new_page'] = df_new.CA * df_new.new_page $ df_new['US_new_page'] = df_new.US * df_new.new_page $ df_new.head()
token_sendreceiveCnt = pd.merge(token_sendcnt,token_receivecnt,on=["ID","month"],how="outer")
sub1.drop('solved', axis = 1, inplace = True)
x.mean(axis=0)
even_counter.value
driver.close()
people.shape
plt.bar(years,trend_data) $ plt.title('Bar Graph: Average Celebrity Death Trends per year') $ plt.show()
stats = session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $ filter(Measurement.station=="USC00519281").group_by(Measurement.station).all() $ stats
if(any(df.isnull())):        #any computes True if any value in the return structure is True $     print("Rows does have null value") $ else: $     print("Rows does not contain any null value")
twitter_archive_clean['date'] = twitter_archive_clean['timestamp'].apply(lambda x: x.date()) $ twitter_archive_clean['date'] = pd.to_datetime(twitter_archive_clean['date']) $ twitter_archive_clean['hour'] = twitter_archive_clean['timestamp'].apply(lambda x: x.hour) $ twitter_archive_clean.drop(columns='timestamp',inplace=True)
subway3_df[['datetime','Hourly_Entries', 'Hourly_Exits']].sample(50) #this is what clued me in to the error in how the function was parsing ridership.
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(1, p_new, n_new) $     old_page_converted= np.random.binomial(1, p_old, n_old) $     p_diffs.append(np.average(old_page_converted)-np.average(new_page_converted)) $
authors_with_name.filter(authors_with_name.parsed_info.title != "").select("parsed_info.first", "parsed_info.title", "Author").count()
s.holidays()
data['affair'].value_counts()
health_data_row.loc[2013:2017]  # 2017 doesn't exist, but Python's slicing rules prevent an exception here $
c = d6tstack.convert_xls.XLStoCSVMultiSheet(cfg_fnames[0],output_dir = 'test-data/output',logger=PrintLogger()) $ c.convert_all(header_xls_range="B2:B2") $
condos = pd.merge(condos, mar, left_on='FULLADDRESS',  right_on='full_address')
for df in (joined,joined_test): $     df["CompetitionMonthsOpen"] = df["CompetitionDaysOpen"]//30 $     df.loc[df.CompetitionMonthsOpen>24, "CompetitionMonthsOpen"] = 24 $ joined.CompetitionMonthsOpen.unique()
build_visual_explainer(r_t, relevance_scores_df, highlight_oov=True, file_name="rendered", $                        title="GroundTruth: {}\n".format(y_test[1]), enable_plot=True)
series + pandas.Series({'a': 2, 'c': 2})
alpha = 0.05 $ np.random.seed(999) $ dist_n = (np.random.randn(10000) + 5) * 4 # +5 fixes mean, *4 fixes stdev
df_new1=df.query('landing_page=="new_page" & group=="control" ') $ df_new1.tail(10) $ df_new1.nunique()
speakers.whotheyare = speakers.whotheyare.apply(lambda x: re.sub("<.*?>", "", x))
MergeWeek = Merge.copy(deep=True)
tweet.lang $ tweet.text $ tweet.retweet_count $ tweet.place $ tweet.geo
import pandas as pd $ import numpy as np $ import matplotlib.pyplot as plt $ import seaborn as sns
df_final.head()
dates.to_period('D')
reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[11577]]) $ reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[12957]]) $ reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[15437]]) $ reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[16839]]) $ reviewsDFslice = reviewsDFslice.drop(reviewsDFslice.index[[23361]])
s.resample('Q', label='left', loffset='-1D').head()
z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old], alternative = 'larger') $ print('z score is '+ str(z_score)) $ print('p_value is '+str(p_value))
results.summary()
table.info('stats')
n_old = df2.query('landing_page == "old_page"')['user_id'].count() $ print(n_old)
reddit_comments_data = reddit_comments_data.withColumn('subjectivity',subj_udf(reddit_comments_data.body))
poverty.drop(poverty.columns.tolist()[1:8], axis=1, inplace=True)
f, ax = plt.subplots(figsize=(5,5)) $ regression_contour(f, ax, m_vals, c_vals, E_grid) $ mlai.write_figure(filename='../slides/diagrams/ml/regression_contour.svg')
df_norm(data[103*30:(104)*30]).plot() $ df_norm(data[104*30:(105)*30]).plot()
newpage_Pconverted_simulation = np.random.binomial(n_new, p,  10000)/n_new $ oldpage_Pconverted_simulation = np.random.binomial(n_old, p,  10000)/n_old $ p_diffs = newpage_Pconverted_simulation - oldpage_Pconverted_simulation
from nltk.corpus import stopwords $ all_stopwords = [x for x in 'abcdefghijklmnopqrstuvwxyz'] $ for j in ['student','students','education',]: $     all_stopwords.append(j) $ all_stopwords += stopwords.words("english")
table2.head()
import ta # technical analysis library: https://technical-analysis-library-in-python.readthedocs.io/en/latest/ $ features['f13'] = ta.momentum.money_flow_index(prices.high, prices.low, prices.close, prices.volume, n=14, fillna=False) $ features['f14'] = features['f13'] - features['f13'].rolling(200,min_periods=20).mean() $
all_tables_df.iloc[:, 1]
contractor.info()
print("Only %d objects in the dataset have 'timestamp'. " % len(timestamp_left_df))
temp_df = pd.DataFrame({'count': [active_psc_records_unique_company_count,active_companies_psc_applies_unique_count - active_psc_records_unique_company_count]\ $             },index=['Number of companies that have filed at least one PSC','Number of companies that have not filed a PSC']) $ temp_df.to_csv('data/viz/number_of_companies_to_which_psc_applies.csv') $ temp_df['proprotion'] = temp_df['count'] / active_companies_psc_applies_unique_count $ temp_df
Magic.__dict__['__repr__'].__get__(None, Magic) $
from sklearn.model_selection import train_test_split $ X_train,X_test,y_train,y_test = train_test_split(X,y, random_state=42)
plt.hist(p_diffs) $ plt.axvline(x = diff_obs,color = "r");
len(df2.query('group == "control" and converted == 1')) / len(df2.query('group == "control"'))
articles = pd.read_csv('data_old/sc_report_topics1.csv') $ print(articles.shape) $ articles.head()
rain_df.set_index('date').head() $
staff_df = staff_df.reset_index() $ student_df = student_df.reset_index() $ pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name')
results_2 = pd.DataFrame(list(results2), columns=['tags', 'values'])#, index=[1,2,3,4]) $ results_2.head(20)
public_table.to_sql(name='public_events', con=con, if_exists='append', index=False)
print('WITH resetting the random seed:') $ print('%d different bacteria between the two function calls' % len(set(dd.feature_metadata.index)^set(dd2.feature_metadata.index)))
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['likes'] == 0].index)
autos['price'].value_counts().sort_index(ascending=False).head(5)
spp_plot.set_index('term', inplace=True) $ ved_plot.set_index('term', inplace=True) $ vhd_plot.set_index('term', inplace=True) $ vwg_plot.set_index('term', inplace=True)
stream.filter(follow=my_followers, stall_warnings=True, languages=["en"], async=True) $ print("Streaming has started.") $ sleep(DURATION) $ stream.disconnect()
df.toPandas().info()
twitter_archive_enhanced_clean = twitter_archive_enhanced_clean[twitter_archive_enhanced_clean.in_reply_to_user_id.isna()] $ twitter_archive_enhanced_clean = twitter_archive_enhanced_clean.drop(['in_reply_to_status_id','in_reply_to_user_id'], axis = 1)
print("Training took {:.2f}s".format(t1 - t0))
convert_mean = df2.query('converted == 1')['user_id'].nunique() / total_users $ convert_mean
df_Outter_trans_users = pd.merge(transactions,users,how="outer",on="UserID") $ df_Outter_trans_users
def to_skycoord(coords): $     coord = str_merge(*coords) $     c = SkyCoord(coord, unit=(u.hourangle, u.deg)) $     return c
x_scaled = 0 $ if len(x_normalized) > 1: $     x_scaled = min_max_scaler.fit_transform(x_normalized)
df.drop('log_price',axis=1,inplace=True)
import statsmodels.api as sm $ convert_old = control_converted.shape[0] $ convert_new = treatment_converted.shape[0] $ n_old = df2.query('landing_page=="old_page"').shape[0] $ n_new = df2.query('landing_page=="new_page"').shape[0]
le.fit(df2['category']) $ df2["cat_num"] = le.fit_transform(df2['category'])
fav1 = twitter_final.filter(['retweet_count','dog_species']).sort_values(by='retweet_count',ascending=False).head(5) $ fav1.plot.bar('dog_species')
df2['landing_page'].value_counts(normalize=True)
locations.State.unique()
print(summarizer.summarize(text, words=200))
honeypot_df.drop(['time_stamp1','time_stamp2','time_stamp3'], axis = 1, inplace = True)
stops = stopwords.words('english') $ more = "united states whereas section country subsection shall act paragraph countries ii usc aa aaaa aaiiiiiaabb ag bb aab aai aaiii aaii aaiiiii ab iii iv b c d e".split() $ stops += more
twitter_archive.info()
df_track.to_sql('track_db', cnx) $ df_artist.loc[:,:].to_sql('artist_db', cnx)
to_be_predicted_Day5 = 25.12171815 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
rdf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1) $ rdf_model = rdf.fit(X_train, y_train)
store_items = store_items.rename(columns = {'bikes': 'hats'}) $ store_items
josh_tweets = pd.DataFrame(Josh) $ josh_tweets
breakfastlunchdinner.sort_values(['lunch + brexits'], ascending=False)
s2 = pd.Series(data = [3, 0, 1, 4], index = ['dogs', 'cats', 'fish', 'birds']) $ s2
df.drop(index=[100,103], inplace=False)   # multiple rows
model = tf.contrib.learn.LinearRegressor( $       feature_columns=make_feature_cols(), model_dir='taxi_trained') $ preds_iter = model.predict(input_fn=make_input_fn(df_valid)) $ print list(itertools.islice(preds_iter, 5)) # first 5
full['<=30Days'] = full['WillBe<=30Days'] $ full.drop('WillBe<=30Days',axis=1,inplace=True)
print contractor_clean['zip_part1'].head(); contractor_clean['zip_part2'].head() $
print('There are {} news articles'.format(len(news))) $ print('Timewise, we have news from {} to {}'.format(min(news.date), max(news.date)))
df_clean.drop(['doggo','floofer','pupper','puppo','expanded_urls', 'source','name','in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id', $                'retweeted_status_user_id','retweeted_status_timestamp'], axis=1,inplace=True) $
df['fileType'].value_counts()
from gensim import corpora $ dictionary = corpora.Dictionary(texts) $ dictionary.save('bible.dict') $ print(dictionary)
topics_top_doc_wt = docs_by_topic.max()[['TopicWt']] $ topics_top_doc_wt
df_group = df.groupby('Ranking URL on Sep  1, 2017').agg({ $     'Keyword': 'count', $     'Rank': 'mean' $ }).reset_index()
model_df['forecast'] = results.predict(dynamic=False)
survey = pd.read_csv("./survey.csv")
f, ax = plt.subplots() $ user_counts.plot(ax=ax, style='o') $ ax.set_xticks(np.arange(1, df['user'].max() + 1, 2)) $ ax.set_xlim(0, 37) $ ax.set_yticks(np.arange(1, user_counts.max() + 1));
file = 'https://assets.datacamp.com/production/course_1975/datasets/titanic_all_numeric.csv' $ titanic = pd.read_csv(file) $ titanic.head()
pipeline = Pipeline(stages=stages_with_decision_tree) $ model = pipeline.fit(trainingData) $ predictions = model.transform(testData)
fb_cleaned = re.sub(link_pattern, '', fb_desc)
diff_weekly_mean_comp = ((day_counts['count'] - functions.avg('count').over(weekly_hashtag_window))**2)
total_users = df2['user_id'].count() $ converted_users = df2[df2['converted'] == True]['user_id'].count() $ converted_users / total_users $
fwd_train = roll_train_df.sort_index(ascending=False).rolling(7,min_periods=1).sum() $ fwd_test = roll_test_df.sort_index(ascending=False).rolling(7,min_periods=1).sum()
pd.Timestamp('2017-02-13') - pd.Timestamp('2017-02-12')
sub_df.head()
url_data.head()
prob_group3 = df2.query("landing_page == 'new_page'")["user_id"].count() $ prob = prob_group3 / df2.shape[0] $ print("The probability that an individual received the new page is {}.".format(prob))
df.withColumn("user_age", user_age_udf("created_at", "user_since")).head()
top_supports.head(10)
logit_model = sm.Logit(df3['converted'],df3[['intercept','ab_page','CA','UK']]) $ results = logit_model.fit() $ results.summary()
merged1 = merged1.rename(columns={'MeetingReasonForVisitId': 'ReasonForVisitId', 'Name':'ReasonForVisitName', 'Description':'ReasonForVisitDescription'})
sox.loc[sox.hour >= 18,'late'] = 1
d_housing=detroit_census2.drop(detroit_census2.index[:24]) $ d_housing=d_housing.drop(d_housing.index[5:]) $ d_housing
X_test['predicted zone'] = model_forest.predict(X_test) $ X_test
df_goog.Open > df_goog.Close
users.creation_source.value_counts()
merged_NNN.committee_position.value_counts()
pd.date_range('2005', periods=4, freq='Q-NOV')
coins.loc[:, coins.mean() > .5].head()
print( "\nSize of the dataset - " + str(len(autodf))) $ autodf = autodf[(autodf.yearOfRegistration >= 1990) & (autodf.yearOfRegistration < 2017)] $ print( "\nSize of the dataset - " + str(len(autodf)))
dir(scipy.optimize)
so[~(so['score'] <= 100)].head()
fig,axes = plt.subplots(1, 2, figsize = (16,4), sharey= True) $ axes[0].plot_date(x=obama.created_at, y = obama.n_chars,linestyle = '-',marker='None') $ axes[1].plot_date(x=trump.created_at, y = trump.n_chars,linestyle='solid',marker='None') $ plt.savefig("fig/n_char_comparison.png")
import os $ os.getcwd()
df_nodates = df_uro.select_dtypes(exclude=['<M8[ns]']) $ df_uro_dd_dummies = pd.concat([df_dd, df_nodates], axis=1) $ df_uro_dd_dummies.shape $ df_uro_dd_dummies.head()
print("there is null value on column botometer, this null indicated this account already deactivate their account :") $ len(compiled_data[pd.isnull(compiled_data['botometer'])==True])
def sentiment_finder_partial(comment): $     analysis = TextBlob(comment) $     return analysis.sentiment.polarity
u = np.array([1, 2, 3])  #u has shape (3,). $ v = np.array([4, 5, 6, 7])  #v has shape (4,). $ w = u[:, np.newaxis]  #w has shape (3, 1); a matrix with 3 rows and one column. $ w*v  #(3, 1) x (4,); starting from the back, 4 and 1 are compatible, and 3 and 'missing' are too -> (3, 4).
(group.shift(1).results == 'Fail').cumsum()
data.resample('T').mean().plot()
train_data_features = vectorizer.fit_transform(X)
soup.find_all(id='third')
pred_series = pd.Series(np.squeeze(predictions)) $ pred_series.describe()
plantlist[plantlist['commissioned'].isnull()]
df = pd.DataFrame({'date': (list(pd.date_range('2000-01-03', '2000-01-05')) * 4), $           'item': (list('ABCD'*3)), $           'status': (np.random.randn(12))}) $ print df
df2 = df.copy() $ df2.iloc[3] = 0 $ df2
sl['second_measurement'] = np.where((sl.new_report_date==sl.maxdate) & (sl.mindate!=sl.maxdate),1,0)
m.fit(train_p)
package.get_resource('chile').read(keyed=True)
count6df = pd.DataFrame(chef10) $ count6df = count6df.drop_duplicates(subset=['name', 'user']) $ count6df.info()
regression_model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = regression_model.fit()
import statsmodels.api as sm $ df_new['intercept'] = 1 $ logit3 = sm.Logit(df_new['converted'], df_new[['intercept','new_page','US','CA']]) $ results3= logit3.fit() $ results3.summary() $
plt.plot(df[base_col].rolling(window=12).std(),color='green',lw=2.0,alpha=0.4) $ plt.plot(df[base_col].rolling(window=3).std(),color='purple',lw=0.75) $ plt.show;
games.head()
len_1 = len(df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')]) $ len_2 = len(df[(df['group'] != 'treatment') & (df['landing_page'] == 'new_page')]) $ print('The number of times "new_page" and "treatment" do not line up is {}.'.format(len_1+len_2))
pgh_311_data = pd.read_csv("https://data.wprdc.org/datastore/dump/40776043-ad00-40f5-9dc8-1fde865ff571", $                            index_col="CREATED_ON", $                            parse_dates=True) $ pgh_311_data.head()
df_twitter_archive[df_twitter_archive.text.duplicated()]
plt.figure(figsize=(15,10)) $ sns.countplot(auto_new.CarModel)
s1 = pd.Series([0, 1, 2], index=[0, 1, 2]) $ s2 = pd.Series([3, 4, 5], index=['0', '1', '2']) $ s1 + s2
df=df.assign( tmp = np.nan ) $ df=df.assign( nrmembers = np.nan )
df['word_count'].value_counts().sort_index().plot(color='b', alpha=.5);
authors_grouped_by_id.schema
list_caps_prob_stat = sum(dictionary.values())
raw_df = pd.read_csv('./wild-wolf-watch-classifications.csv')
df_with_ctr.head(2)
levelmean = pd.merge(pd.DataFrame(tokendata.level),groupmean,how="left",on="level")
unique_votes = mentioned_bills_all['votes_api_url_2'].unique()
step_counts = pd.Series(step_data, name='steps') $ print(step_counts)
def get_list_of_Media_Body_URL(the_posts): $     list_of_Media_Body_URL = [] $     for i in list_Media_ID: $         list_of_Media_Body_URL.append(the_posts[i]["object_url"]) $     return list_of_Media_Body_URL
df.plot()
es = EarlyStopping(patience=100) $ history = model3.fit([X_train_text, X_train_uid], y_train, validation_split=0.1, $                         epochs=100, verbose=1, callbacks=[es])
session = Session(engine) $ conn = engine.connect()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative="larger") $ print(z_score, p_value)
df_precep_dates_12mo = pd.read_sql_query("select date, prcp from measurement where date > '2016-08-22';", engine) $ df_precep_dates_12mo
mydata=pd.read_csv("Data-5year-2012-2017.csv")#,index_col="Date") $ mydata[['Date']] = mydata[['Date']].apply(pd.to_datetime, errors='ignore') $ mydata=mydata.set_index(mydata["Date"]) $ mydata = mydata.drop("Date",axis=1)
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn as sns
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True).head(10)
unemp = unemp.dropna().reset_index(drop=True) $ unemp.rate = unemp.rate.str.replace('%', '').astype(float)
volume_list = [] $ for j in data2: $     if j[6] != None: $         volume_list.append(j[6])
topics_data = [] $ for i in range(number_of_topics): $     topics_data.append([i]+[x[0] for x in ldamodel.show_topic(i)]+[x[1] for x in ldamodel.show_topic(i)]) $ topics_df = pd.DataFrame(topics_data,columns=["topic_id"]+["word_%s"%i for i in range(10)]+["word_%s_weight"%i for i in range(10)])
possible_features = [val for val in X.columns if val in X_unseen.columns] $ possible_features
priors_product_reordered_spec= priors_reordered.groupby(["user_id","product_id"]).size().reset_index(name ='reordered_count_spec') $ priors_product_reordered_spec['userprod_id']=priors_product_reordered_spec['product_id'] + priors_product_reordered_spec['user_id'] *100000 $ priors_product_reordered_spec.head(10)
pax_raw.to_hdf(os.path.join(data_dir, hdf_path), 'pax_raw')
def ls_dataset(name,node): $     if isinstance(node, h5py.Dataset): $         print(node)
df_twitter_archive.name.isnull().sum()
df2[df2['landing_page']=='new_page'].count()
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/EON_X?start_date=2018-06-01&end_date=2018-06-01&api_key=") $
import warnings $ warnings.filterwarnings('ignore')
print('Prediction for index 0: {:.2f}'.format(example_model.predict(train_data)[0]))
df_by_donor.columns = [' '.join(col).strip() for col in df_by_donor.columns.values]
analyze_set['retweets'].corr(analyze_set['favorites'])
stations = session.query(Measurement.station).distinct().all() $ print(len(stations))
soup = bs(response.text, 'html.parser')
df19.to_csv("AppleTweets19.csv",index=False)
%%bash $ wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz $ tar xvzf ssd_mobilenet_v1_coco_11_06_2017.tar.gz
trump.iloc[[0, -1], :]
raw.shape
df3.loc[:,'C'] = np.array([2] * len(df3)) $ df3
image_predictions.sample(5)
archive_clean.drop(archive_clean[archive_clean.retweeted_status_id.notnull()].index, axis=0,inplace=True)
cats_df.head()
help(es.search)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
users[users['friends_count'].isna()]
df.query('director == "Unknown"').head(20) $ df.query('production_companies == "Unknown"').head(20) $ df.query('genres == "Unknown"').head(20)
not_lineup = len(df[((df['group'] == 'treatment') == (df['landing_page'] != 'new_page')) ]) $ print('The new_page and treatment dont line up {} times'.format(not_lineup))
authors = xml_in['authorName'].unique().tolist() $ random_authors = np.random.choice(authors, 100000) $ random_authors_final = random_authors.tolist()
users[users['friends_count'].str.contains(r'[A-Za-z]') == True]
rfr.fit(features_regress_vect, overdue_duration)
from scipy.stats import norm $ norm.cdf(z_score) #how significant our z_score is
df_h1b_mv_ft.pw_unit_1.value_counts()
indexed_return(fundret.loc['2017-03':])
tabs = Tabs(tabs=[Panel(child=layout, title=layout_name) for layout_name, layout in table_layouts.items()])
to_be_predicted_Day3 = 43.4200283 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df_tweet_data.info()
idx = df_providers[ (df_providers['id_num']==id_num)].index.tolist() $ print('length of IDX',len(idx)) $ assert len(idx) > 0, 'Length of IDX is NULL' $ print( df_providers.loc[idx[0],'name'] ) $
filepath = os.path.join('extracts', 'monthly_reports')
transactions.merge(users, how='left')
test_collection.insert_one(temp_dict)
from rain import *   # tool in this repo $ df = ds.to_dataframe() $ e = Event(df)
logmod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logmod.fit() $
twitter_df_clean.groupby('stage_name')['stage_name'].count()
imputed = np.exp(df_imputed.iloc[na_index,0])
data.shape
joined.dtypes
!tar -xzf data/aclImdb.tgz -C data
all_df.to_pickle('data/all_political_ads.pickle') $
mean_vecs = np.mean(hidden_states, axis=1) $ max_vecs = np.max(hidden_states, axis=1) $ sum_vecs = np.sum(hidden_states, axis=1)
df['AgeVideo'][df['AgeVideo']<700].count()
len(r.json())
df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_downsampled, df_thermal, $                                                                 test_size=0.2, random_state=42)
iris.head().iloc[:,[0]]
df_train.reset_index(drop=True, inplace=True) $ df_valid.reset_index(drop=True, inplace=True)
data_for_model.to_pickle('data_for_model')
df2.nunique()
boston = df[df["text"].str.contains("boston")] $ obama =df[df["text"].str.contains("obama")] $ gop = df[df["text"].str.contains("gop")] $ candidate = df[df["text"].str.contains("candidate")]
regr.fit(X,y)
print(rows) $ df.isnull().any().any(), df.shape
conn_str = ( $     r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};' $     r'DBQ=C:\Users\Daniel.Aragon\Desktop\DJA_TEMP\FortCollins\COFC Bridge Mgmnt DB_MASTER_1.0_2018.accdb;' $     r'UID=FCADMIN;' $     r'PWD=password;')
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.drop('type', 'locale', 'locale_name', 'description', 'transferred') $ train_holiday_oil_store_transaction_item_test_004.show()
intervention_train.loc[:, columns_with_nan].apply(nan_check, axis=0)
path2 = '../Data/nyse_20170718_20170721_1minute' $ X_test, y_test = get_data(df_test, path2, 0, len(df_test))
df_new = pd.merge(df2, df_countries, on='user_id') $ df_new.head()
 import datetime $ expiry = datetime.date(2016, 1, 1) $ data = aapl.get_call_data(expiry=expiry) $ data.iloc[0:5:, 0:5]
hawaii_station_df.head(20)
Google_stock['Open'].max()
test.property_type.value_counts()
donors[donors['Donor Zip'].isnull()].count()
eclf4 = VotingClassifier(estimators=[('lr', alg), ('calC', alg7), ("ranFor", alg2), ("DT", alg6), ("Ada", alg4)], voting='soft') $ eclf4.fit(X_train, y_train) $ probs = eclf4.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
outdir = './output' $ if not os.path.exists(outdir): $     os.mkdir(outdir)
df = pd.read_sql('SELECT * FROM actor WHERE actor_id = 172', con=conn) $ df
yhat.shape, X_test.shape,y_test.shape
T4 = get_rental_concession_vec(rentals['description']) $ T4[T4.any(1)].head()
cursor1=db.bookings.find({"consignee.city":"Indore"},{'booking_id':1,'_id':0,'booking_items.is_cod':1})
control_converted=df2.query('group=="control"').converted.mean() $ control_converted
os.chdir(Base_Directory) $ print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END)
xmlData.describe(include = ['O'])
data3=data2.sort_values(by=["date"]) $ data3.head(10)
mileage = {} $ for brand in top_brand: $     top = autos[autos['brand']==brand] $     mileage[brand] = top['odometer_km'].mean() $
titanic[~filter.fare]
jobPostDF.head(5)
remove_list = df_errors.tweet_id $ archive_clean = archive_clean.loc[~archive_clean['tweet_id'].isin(remove_list),:]
df = df.sample(frac=1).reset_index(drop=True)
p_mean = np.mean([p_new, p_old])
converted_users2 = float(df2.query('converted == 1')['user_id'].nunique()) $ p2 = converted_users2/float(df2.shape[0]) $ print("The probability of an individual converting regardless of the page they receive is {0:.4}".format(p2)) $
AAPL.std()
df2_control = df2.query('group == "control"') $ p_control = df2_control['converted'].sum() / df2_control.shape[0] $ p_control
ax=contractor_merge.groupby(['month_year'])['contractor_number'].nunique().plot(kind='Line', title ="Bar Chart for Frequency of Contractor Updates by year", figsize=(15, 10), legend=True, fontsize=12,) $ ax.set_xlabel("Month_Year", fontsize=12) $ ax.set_ylabel("Count of Contractor Updates", fontsize=12) $ plt.show()
data = data[data['Incident Zip'].notnull()]
df["winter_pickup"] = (df["pickup_month"].isin([1, 12])).astype(int)
archive_clean[archive_clean['in_reply_to_status_id'].notnull()]
for df in (joined, joined_test): $   df.loc[df['CompetitionDaysOpen']<0,'CompetitionDaysOpen']=0 $   df.loc[df['CompetitionOpenSinceYear']<1900,'CompetitionDaysOpen']=0
ct_df = pd.read_csv('./countries.csv') $ ct_df.head()
columns_to_merge = tweet_json[['id','retweet_count','favorite_count']].copy() $ twitter_archive_clean = twitter_archive_clean.merge(right=columns_to_merge,how='left',left_on='tweet_id',right_on='id') $ twitter_archive_clean = twitter_archive_clean.drop(columns='id')
wd = "/Users/zy/Documents/2017Spring/DataMinning/Project/data" $ train_df = pd.read_json(wd+"/train.json") $ train_df = train_df.reset_index(drop=True) $ test_df = pd.read_json(wd+"/test.json") $ test_df = test_df.reset_index(drop=True)
df_new.country.value_counts() # confirm how many of each country are in the dataset
df.head()
plt.hist(new_page_converted);
df.info() # No missing rows
rng_eastern[5].tz_convert('Europe/Berlin')
authors_to_github_username_saved.schema
df2['landing_page'].value_counts(normalize=True)
df_wp[df_wp.day_created.isin(days_unmatched)].sort_values('create_time')
df2.converted.mean()
df1.shape
statuses = api.GetUserTimeline(screen_name='TIME') $ print([s.text for s in statuses])
perf_train.dtypes.value_counts()
countries_df.head()
df_new.query('group == "treatment" and country =="US"')['converted'].mean()
df_2017 = df[df.year==year_labeled] $ X_train = df_2017[['description', 'label']] $ X_train.head() $ description_2017 = X_train['description'] $
Distance = [] $ for i in range(0,len(values)): $     row = values.ix[i] $     Distance.append(calculateDistance(row[0], row[1], row[2], row[3]))
check_rhum = rhum_fine[1] $ sns.heatmap(check_rhum)
blob.polarity
df.head()
sns.lmplot(x='avg_hr', y='pace', data=df_run)
data['SA_diff'] = sa_diff $ for i, row in data.iterrows(): $     if row[12] > 0.2: $         print row[0], ' , ', row[6], ' , ', row[5]
df.query('converted == 1').nunique()['user_id']/unique_user
df_users.groupby('creation_source').count() 
Mars_df.to_html('Mars_Facts.html') $ !open Mars_Facts.html
pca_df.plot(kind='scatter', x='PC1', y='PC2') $ plt.xlabel('PC1 - {}%'.format(per_var[0])) $ plt.ylabel('PC2 - {}%'.format(per_var[1])) $
import matplotlib.pyplot as plt $ %matplotlib inline $ import seaborn as sns
a[a.find(':'):]
mig_l12 = mi.groupby(level=['Symbol', 'Year', 'Month']) $ print_groups(mig_l12)
chefdf = chefdf.dropna()
result_Sl[['last_sl_T']].sub(result_Sl['First_sl_T'], axis=0) $ result_Sl['Mean deaths'] = result_Sl['last_sl_T']/result_Sl['count_sl_T'] $ result_Sl
def ngrams(input_list,n): $     ngrams = list(zip(*[input_list[i:] for i in range(n)])) $     return [*map('_'.join, ngrams)]
articles['next_pair_id'] = articles.apply(next_nearest_id, axis=1)
plt.plot(ds_ossm['time'],ds_ossm['met_salsurf_qc_executed'],'b.') $ plt.title('CP04OSSM, OOI QC Executed SSS') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
pprint(q_specific.metadata(reload=True))
fires.dtypes
from sklearn.preprocessing import StandardScaler $ scaler = StandardScaler() $ X = scaler.fit_transform(X) $ X = pd.DataFrame(X,columns=col_names) $ X.head(5)
ts=df15.groupby('day_of_week').agg({'sale':['sum']}) $ ts
closeSeriesR.pct_change().nlargest(5)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == True')['converted'].count() $ convert_new = df2.query('landing_page == "new_page" and converted == True')['converted'].count() $ n_old = df2.query('landing_page == "old_page"')['landing_page'].count() $ n_new = df2.query('landing_page == "new_page"')['landing_page'].count()
conv_prop = df.query('converted == "1"').user_id.nunique() / df.user_id.nunique() * 100 $ print("The proportion of users converted is", round(conv_prop, 4)) $
mb.head()
dul = pd.concat([dule2, dulp], ignore_index=True) $ dul
df2.head(3)
daily = data.resample('D').sum() $ daily.rolling(30, center=True).sum().plot(style=[':', '--', '-']) $ plt.ylabel('mean hourly count');
n_new = df2.query('landing_page == "new_page"').count()[0] $ n_new
df2[df2['user_id'].duplicated() == True]
df['home_win'] = df.home_win.astype(int)
content_performance_bytime.index = content_performance_bytime['date']
session.query(Measurement.station, Station.station).limit(10).all()
pre.head(2)
team_attributes = pd.read_sql_query('select * from Team_Attributes', conn)  # don't forget to specify the connection $ print(team_attributes.shape) $ team_attributes.head()
df1_clean.replace('None', np.NaN, inplace=True)
feature_importances.columns
corpora.SvmLightCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.svmlight'), corpus) $ corpora.BleiCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.lda-c'), corpus) $ corpora.LowCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.low'), corpus)
my_columns = list(df_.columns) $ print(my_columns)
df[df["stamp"]>= "02-20-2018"].head()
table_rows = driver.find_elements_by_tag_name("tbody")[16].find_elements_by_tag_name("tr") $
pres_df['state'].unique()
dat.status[dat.completed.notnull()].unique()
pax_raw.columns = [x.lower() for x in pax_raw.columns]
final_.head()
model = sm.OLS.from_formula('arrests ~ game_hour', stadium_arr) $ results = model.fit() $ print(results.summary())
fig, ax = plt.subplots() $ load2017.plot(ax=ax, title="Actual load 2017 (DE-AT-LUX)", x='timestamp', y='actual', lw=0.7) $ ax.set_ylabel("Actual Load") $ ax.set_xlabel("Date")
story_url = "https://www.gutenberg.org/files/14838/14838-h/14838-h.htm" $ storyContentrequest = requests.get(story_url) $ storyContentSoup = bs4.BeautifulSoup(storyContentrequest.text, 'html.parser') $ storyContentSoup
sel = [Measurements.tobs] $ temperature_data = session.query(*sel).\ $     filter(Measurements.date >= last_year_date).\ $     filter(Measurements.station == highest_station).all()
vectorizer = CountVectorizer(analyzer='word', stop_words='english', tokenizer=lambda text: text, $                              lowercase=False, binary=True)#, min_df=10) $ spmat = vectorizer.fit_transform(x_tokens)
df_temp = df.query("group == 'treatment'") $ x = df_temp.query("landing_page == 'old_page'").shape[0] $ df_temp = df.query("group == 'control'") $ y = df_temp.query("landing_page == 'new_page'").shape[0] $ x + y
odometer_IQR = autos["odometer_km"].quantile(.75) - autos["odometer_km"].quantile(.25) $ odometer_LF = autos["odometer_km"].quantile(.25) - (1.5 * odometer_IQR) $ print(odometer_LF)
bedrooms_sim = pickle.load(open('bedroom_sim_mat.pickle','rb')) $ bathrooms_sim = pickle.load(open('bathroom_sim_mat.pickle','rb')) $ living_rooms_sim = pickle.load(open('living_room_sim_mat.pickle','rb')) $ kitchens_sim = pickle.load(open('kitchen_sim_mat.pickle','rb')) 
actual_old_page_converted=df2.query('landing_page=="old_page"')['converted'].value_counts()[1]/\ $ df2.query('landing_page=="old_page"').shape[0]
pd.value_counts(dr_existing['ReasonForVisitName'])
print(users.c.id == addresses.c.user_id)
merged2.head()
df.resample('D').mean()
out = conn.upload('/u/username/data/iris.tsv', $                   importoptions=dict(filetype='csv', delimiter='\t'), $                   casout=dict(name='iris_tsv', caslib='casuser')) $ out
import qgrid $ q_df = qgrid.show_grid(df)
%%time $ results_parallel = ppm.parallel_process_text(data_to_clean[:benchmark_n])
def only_upper(s): $     return "".join(c for c in s if c.isupper())
y2_train = y_train.apply(lambda x : x // 150) $ model = sm.GLM(y2_train, X_train, family=sm.families.Poisson(statsmodels.genmod.families.links.log)) $ model_res = model.fit()
df_columns['Day of the week'].value_counts().plot(kind='barh') $
clean_rates.info()
df.head()
cov_df = factor_ts.rolling(250).cov().dropna()
json.loads(page.text)
textFilter = tweets["screen_name"].str.contains("Tiffany") $ tweets[textFilter].head()
url = 'https://api.github.com/graphql'
df.loc[:, topics[0]:topics[-1]] = df.apply(lambda x: \ $                                            pd.Series([t in x['tags'] for t in topics], index=topics), axis=1)
d = nc.Dataset('data/level1_7cf4_8c17_87af_7609_1fb9_c55c.nc', 'r') # set the filemode to 'r' for read
np.exp(-0.015) * np.exp(-1.9888)
wikiMeritSoup = bs4.BeautifulSoup(wikiMeritRequest.text, 'html.parser') $ print(wikiMeritSoup.text[:200])
twitter_merged_data.corr()
from sklearn.metrics import classification_report $ print(classification_report(y_test,y_pred_mdl))
df['time'] = df['closed_date']-df['created_date'] $ df.head(3)
url_cerberus = "https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced" $ url_schiaparelli = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced' $ url_syrtis = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced' $ url_valles = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced'
df_con=pd.concat([df_con, Xs], axis=1)
plt.hist(freq_df['tobs'], bins=12) $ plt.tight_layout() $ plt.ylabel("Frequency") $ plt.show() $
df2[df2.duplicated(subset='user_id')==True]
holdout_preds = lr.predict(x_holdout)
df_goog.sort_values('Date', inplace=True) $ df_goog.set_index('Date', inplace=True) $ df_goog.index = df_goog.index.to_datetime()
print 'Percentage of total amount for data with City but no state: {:.3f}'.format(100*sum(df[df.state == ''].amount)/sum(df.amount)) $ df[((df.state == '') & (df.city != ''))][['city','zipcode','amount']].sort_values('city', ascending=True).to_csv('out/0/City_No_State.csv')
reconstruction_mask = tf.one_hot(reconstruction_targets, $                                  depth=caps2_n_caps, $                                  name="reconstruction_mask")
liberia_data.Description.value_counts()
measure_df.info() $
df_more[df_more.Senior != 0]
df_new[['ca','uk','us']] = pd.get_dummies(df_new['country']) $ df_new.drop(['ca'], axis=1, inplace=True) $ df_new.head()
df.head(2)
result3 = sm.ols(formula="AMZNr ~ GSPCr", data=tbl2[-250:]).fit() $ result3.summary()
all_categories = set(c for categories in events.categories.tolist() for c in categories) $ all_categories
df_tweets.reset_index().to_pickle("../tweets_extended.pkl")
master.tail(10)
week22 = week21.rename(columns={154:'154'}) $ stocks = stocks.rename(columns={'Week 21':'Week 22','147':'154'}) $ week22 = pd.merge(stocks,week22,on=['154','Tickers']) $ week22.drop_duplicates(subset='Link',inplace=True)
tablename='boots_list' $ pd.read_csv(read_inserted_table2(dumpfile, tablename), $             delimiter=",", $             error_bad_lines=False).head(5)
posts = data["author"].value_counts().reset_index() $ posts.rename(columns = {"index" : "author", "author": "links"}, inplace=True) $ posts[0:9]
breed_predict_df_clean.info()
count_15 = logins.set_index(keys='datetime').resample('15min').count()
gen1str = '\n'.join(all_gen1_verse)
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", start='1980',end='2014') $ le_data_all
WT_PATH = Path('./models/wt103') $ WT_PATH.mkdir(exist_ok=True)
props.prop_name.value_counts()
train_fs2.shape
X = np.reshape(dataX, (dataX.shape[0], n_chars, 1)) $ max_int = max(char_to_int.values()) $ y = dataY/n_chars $ X = X/n_chars
LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')
df = pd.DataFrame.from_dict(gender_counts, orient='index') $ df.plot(kind='bar',color = 'y') $ pyplot.xlabel('Gender')  $ pyplot.ylabel('count') 
y = df_nuevo.price.values $ plt.hist(np.log(y)) $ plt.show()
print cust_data1.columns $ cust_data1=cust_data1.rename(columns={'RevolvingUtilization':'Rev_Utilization', 'SeriousDlqin2yrs':'SeriousDlq'}).head(100)
devtest_cut = pd.to_datetime('2017-10-01')
features.shape
y_pred = rfmodel.predict(X_test)
from IPython.display import IFrame $ IFrame('data/lda.html', width=1220, height=860)
null_vals = np.random.normal(0, p_diffs.std(), 10000)
full_globe_temp.values
len(" ".join(dc['text']).split(" "))
33**2*df.h0/1e3/df.Da
train.to_pickle('../data/merged_data/train.pkl')
forecast = stepwise_model.predict(n_periods=29)
def append_element(some_list, element): $     some_list.append(element)
vlc[vlc.other_services.map(lambda x: bool(x))][["name","visited", "other_services"]]
cols = ['GP', 'GS', 'MINS', 'G', 'A', 'SHTS', 'SOG', 'GWG', 'HmG', 'RdG', \ $         'G/90min', 'SC%', 'Year', 'PKG', 'PKA'] $ goals_df[cols] = goals_df[cols].apply(pd.to_numeric)
from sklearn.ensemble import RandomForestClassifier
top_tracks = pd.DataFrame(columns=[i.replace(".txt","") for i in files])
print('No. of unique users in Dataset:',df.user_id.nunique())
repos.created_at = pd.to_datetime(repos.created_at)
temp_df.tail(-100)
lr2 = LogisticRegression(random_state=20, max_iter=10000, C= 1, multi_class='ovr', solver='saga') $ lr2.fit(X_tfidf, y_tfidf)
cig_data.dtypes
autos.columns
import pandas as pd $ df = pd.read_csv("kc_house_data.csv") $ df.dropna(how='any',inplace=True)    #to drop if any value in the row has a nan $ df.head()
parsed_names = raw.name.str.extract(r"(?P<last_name>[\w ]+), (?P<first_name>[\S ]+)\2 \1", expand = True) $ parsed_names_simple = raw.name.str.extract(r"^(?P<last_name>[\w]+) (?P<first_name>[\w ]+)$", expand = True) $ q = parsed_names.first_name.isnull() & parsed_names_simple.first_name.notnull() $ parsed_names.loc[q] = parsed_names_simple.loc[q] $ parsed_names.assign(original = raw.name).sample(10)
from sklearn.model_selection import train_test_split $ x_train, x_test, y_train, y_test = train_test_split(xs, ys) $ y_train = y_train.ravel() $ y_test = y_test.ravel() $ print(len(x_train),len(x_test))
print("Probability an individual recieved new page:", df2['landing_page'].value_counts()[0]/len(df2))
df_bthlst.head()
strategy.df_pnl().head()
testData.groupBy("label").count().show()
conn_a.commit()
data.shape
df_final.to_csv('twitter_archive_master.csv', header=True, sep=',', encoding='utf-8')
hawaii_measurement_df = hawaii_measurement_df.replace(np.nan, 0)
df_geo_count = df_geo.groupby("geo_code").count() $ dict_geo_count = df_geo_count.to_dict()["id_str"]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new],[n_old, n_new], alternative='smaller') $ z_score, p_value
df_sum_by_stage = twitter_archive_full[twitter_archive_full.stage !='None'].groupby('stage')['retweet_count','favorite_count'].sum() $ df_sum_by_stage.plot(y = ['retweet_count','favorite_count'], kind='bar').set_title('Distribution of retweet and favorite by stage'); $
dat_wkd = data.groupby(['admission_type','inday_icu_wkd'])['hospital_expire_flag'].mean().reset_index() $ dat_wkd = dat_wkd.pivot(index='inday_icu_wkd', columns='admission_type', values='hospital_expire_flag') $ dat_wkd.head()
converted_users = sum(df.converted == 1)/len(unique_usrs) $ converted_users #proportion of users converted
unique_users_cnt=len(ab_data.user_id.unique()) $ unique_users_cnt
archive_clean = archive.copy() $ images_clean = images.copy() $ popularity_clean = popularity.copy()
token.columns = ["_type","sender","date","receiver"]
print(df2[df2['group'] == 'control']['converted'].mean())
round(autos["price"].describe())
df2 = df2.drop(temp.head(1).index)
p_diffs = [] $ for _ in range(10000): $     p_new = np.random.choice([0,1], size=n_new, p=[1-p_new, p_new]).mean() $     p_old = np.random.choice([0,1], size=n_old, p=[1-p_old, p_old]).mean() $     p_diffs.append(p_new - p_old)
df = pd.read_sql('SELECT * FROM customer_female', con=conn_b) $ df
grouped.unstack('state')
journalists_retweet_summary_df[['retweet_count']].describe()
y=dataframe1['Close'] $ plt.plot(y) $ plt.show()
tmax_day_2018.coords
just_apps = df[df.is_application == "Application"] $ just_apps.head(3)
people_person.head()
df.groupby('Status').count().sort(desc("count")).show(10)
merge['Weatherstr'] = merge.EVENTS.apply(str) $ merge[merge.columns[51]].value_counts().sort
nnew_sim = df2.loc[(df2['landing_page'] == 'new_page')].sample(nnew,replace = True) $ new_page_converted = nnew_sim.converted $ new_page_converted.mean()
rng_pytz = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz=tz_pytz)
df.reset_index(inplace=True)
Y_test.iloc[0:5]
df.rename(columns={'85235_00060_00003':'MeanFlow_cfs','85235_00060_00003_cd':'Confidence'},inplace=True) $ df.head()
df.select('station','year','measurement').show(5)
cursor = db_connection.cursor() $ cursor.execute(update_1_query) $ db_connection.commit() $ cursor.close() $ db_connection.close()
new_x.head()
preprocessor = PreProcessor(titanic, copy=True) $ print("We made a copy so id titanic :  {} different from id preprocessor.data {}".format( $         id(titanic),id(preprocessor.data)))
cityID = '488da0de4c92ac8e' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Plano.append(tweet) 
pd.concat([d1, d3], join='inner')
actions = pd.DataFrame(session['action'].value_counts().reset_index()) $ actions_top10 = actions.head(10) $ actions_top10
sub_df = pd.concat([sub_df, op_sent], axis=1)
arrPriceList.head(1)
evaluator.evaluate(cvPredictions)
dfs[0].rename(columns={"VALUE": my_data["FRED/DFF"]}, inplace=True) $ df_info(dfs[0])
sample_df['offer_group_desc'].unique()
bad_comments = bigdf.loc[(bigdf.index == 'dogx9hr') | (bigdf.index == 'dpprl85') | (bigdf.index == 'dof7gzj') | (bigdf.index == 'drdvqmo')]['comment_body']
df_new = df2.set_index('user_id').join(countries_df.set_index('user_id')) $ df_new[['CA','US', 'UK']] = pd.get_dummies(df_new['country'])[['CA','US', 'UK']] $ df_new.head()
model_lm2 = LogisticRegression() $ model_lm2.fit(X2_train, y2_train) $ y2_preds = model_lm2.predict(X2_test) $ confusion_matrix(y2_test, y2_preds)
load2017['actual'].isnull().sum()
import datetime as dt $ mydata = dc2015.copy() $ mydata['new'] = np.where((mydata['created_at'].dt.time >= '0:00') & (mydata['created_at'].dt.time < '12:00'), 'morning', 'evening')
from zipfile import ZipFile $ zip_file = ZipFile('/content/us-consumer-finance-complaints.zip') $ fields= ['product','consumer_complaint_narrative'] $ data=pd.read_csv(zip_file.open('consumer_complaints.csv'), usecols=fields) $ data.head()
loanTree.fit(X,y)
df["TOTAL_PAYMENT"].sort_values(ascending=False)
posts_in_response_to_user_collected.show()
graf_counts2.head()
analyze_set=pd.read_csv('Twitter_archive_master.csv')
train.StateHoliday = ( train.StateHoliday != '0' ) $ test.StateHoliday = ( test.StateHoliday != '0')
0.1196
autos_p['last_seen'].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True).head(100)
my_df["user"] = my_df["user"].astype("object") $ print(my_df["user"].describe())
b_cal_q1.loc[:,'date'] = pd.to_datetime(b_cal_q1['date'])
%%time $ degree_centrality = convert_dictionary_to_sorted_list(nx.degree_centrality(network_friends))
df.year.unique()
Ytr_m = np.mean(Ytr[:,:7],axis=0) $ Ytr_std = np.std(Ytr[:,:7],axis=0) $ Ytr_scale = (Ytr[:,:7]-Ytr_m)/Ytr_std $ Yts_scale = (Yts[:,:7]-Ytr_m)/Ytr_std
small_train.isnull().sum(axis=0) # show columns with counts of null values
import statsmodels.api as sm $ convert_old = df2.query('group=="control"').converted.sum() $ convert_new = df2.query('group=="treatment"').converted.sum() $ n_old = df2.query('group=="control"').converted.shape[0] $ n_new = df2.query('group=="treatment"').converted.shape[0]
Measurements = Base.classes.hawaii_measurement $ Stations = Base.classes.hawaii_station
data_chunks = pd.read_csv("Data/microbiome.csv", chunksize=15) $ mean_tissue = pd.Series({chunk.Taxon[0]: chunk.Tissue.mean() for chunk in data_chunks}) $ mean_tissue
p_convert_treatment = df2.query('group == "treatment"').converted.mean() $ p_convert_treatment
env = Environment(dm.get_slice_from_pair('EUR/USD'), dp.state) $ agent = Agent(dp.state.columns.size, 3) $ env.run(agent) $ agent.brain.model.save("brain.h5")
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\adult.data2.csv" $ names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship',\ $          'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'] $ df = pd.read_csv(path, sep =',', header=None, names = names) $ df.head(5)
r = df['upvotes'].groupby(df['author']).sum() $ r.head()
for i in range(0, seconds_len): $     minutes_list.append(get_min())
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], value=0, alternative='larger') $ z_score, p_value
clean = pd.concat([raw, ideal_index_df], axis=1)
df2.drop(2893, inplace=True) #Drop the second value from the output above
df.info() $ df.isnull().values.any()
net.add_nodes(pop_name='inhibitory', $               ei='i', $               model_type='population', $               model_template='dipde:Internal', $               dynamics_params='inhibitory_pop.json')
df.sum().plot(kind='bar')
norm.ppf(1-(0.05))
df_joined.groupby(['country','ab_page_new_page']).converted.mean()
hp.get_devices()[:4]
stations_dict = stations_df.to_dict(orient='records') $ stations_dict
data = pd.DataFrame({'Qu1': [1, 3, 4, 3, 4], $                     'Qu2': [2, 3, 1, 2, 3], $                     'Qu3': [1, 5, 2, 4, 4]})
n_old = df2.query("landing_page == 'old_page'")['landing_page'].count() $ n_old
plot_confusion_matrix(cm_svm, classes=['COLLECTION', 'PAIDOFF'],normalize=False, title="Confusion matrix for svm", cmap=plt.cm.Blues)
df = pd.read_sql('SELECT UPPER(first_name || \' \' || last_name) as "Actor Name" FROM actor', con=conn) $ df.head()
repaid_loans_cash[(repaid_loans_cash.fk_loan==36) & (repaid_loans_cash.fk_user_investor==51)].to_clipboard()
c.execute(query) $
df_result[df_result.loglikelihood > 0].iloc[0]
exploration_titanic.nearzerovar()
autos['ad_created'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
conn.execute("UPDATE bonus_episodes set blog_id=398 WHERE episode_id=24")
import pandas as pd $ pd.read_csv('thalamus_spikes.csv', sep=' ')
pd.bdate_range('11-Sep-2017', '17-Sep-2017', freq='2D')
posts.find_one({"author": "Mike"})
temp_series_freq_2H.plot(kind="bar") $ plt.show()
r = q_specific.results() $ r
retweet_pairs[["FromType","FromName","Edge","ToType","ToName","Weight"]].to_csv("For_Graph_Commons.csv",index=False)
X_extra.dtypes
products['key']=0 $ users['key']=0 $ userproduct=pd.merge(users,products,on='key')[['ProductID','UserID']] $ users.drop('key',1,inplace=True) $ userproduct
t_counts = Counter(trump_words) $ df = pd.DataFrame(t_counts.most_common(100), columns=['T_Words', 'Count']) $ df.to_csv('t_word_count.csv')
pd.Timestamp('9/1/2016 10:05AM')
pitches = pd.read_json('/Users/thomasmulhern/Downloads/pitchesDataJson/pitch.json')
T = 0 $ train = pd.read_csv('../../input/preprocessed_data/trainW-{0}.csv'.format(T))
search['trip_end_date'] = search['message'].apply(trip_end_date)
adopted_cats.info()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], $                                               nobs=[n_new, n_old], alternative = 'larger') $ print("z-score:", z_score, $      "\np-value:", p_value)
month = month.sort_values(['LOCATION', 'DATE/TIME']) $ month = month.reset_index() $ month = month.drop('index', axis = 1) $ month
c=contractor[contractor['contractor_number'].isin([11004,11201,11202])] $ c[['contractor_id','contractor_type_id','contractor_version','contractor_bus_name', $    'contractor_number','last_updated']].sort_values(by='last_updated', ascending=False)
df_ad_airings_filter_3['start_time'].max() $
names_count = authors_with_name.groupBy("parsed_info.first").agg(F.count("*").alias("names_count")) $ names_count.sort(names_count.names_count.desc()).show()
import scipy.stats as stats $ t_stat, p_val = stats.ttest_ind(dftop['temp'],weather['temp'], equal_var=False)
df3 = df2.set_index('user_id').join(df_countries.set_index('user_id')) $ df3.head()
ozzy.doginfo() $ skippy.doginfo() $ filou.doginfo()
from sklearn import tree $ from sklearn.metrics import accuracy_score $ clf = tree.DecisionTreeClassifier() $ clf.fit(x_train,y_train) $ accuracy_score(clf.predict(x_test),y_test)
cityID = '8e9665cec9370f0f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Minneapolis.append(tweet) 
lda = models.LdaModel(corpus=corpus, num_topics=15, id2word=id2word, minimum_probability=.03, passes=10)
to_be_predicted_Day2 = 21.20397968 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
os.environ['PROJECT'] = PROJECT $ os.environ['BUCKET'] = BUCKET $ os.environ['REGION'] = REGION $ os.environ['TFVERSION'] = '1.8' 
coll_ref.count()
extract_deduped_with_elms = extract_deduped_with_elms_v2.copy()
clean_appt_df.to_csv('processed_data/clean_appt_df.csv', index=False)
df_image_clean["tweet_id"] = df_image_clean["tweet_id"].astype(str)
conn.addtable(table='myclass', caslib='casuser', $               **mydmh.args.addtable)
pd.merge(transactions,transactions,how='outer',on='UserID')
weather_yvr_dt = weather_yvr.copy()
def convert(x): $     return pd.datetime(divmod(x, 10000)[0], divmod(x, 100)[0] % 100, x % 100)
autos["price"].describe()
nrn = nm.load_neuron(example_swc)
con_cols = [i for i in full_data.columns if (full_data.dtypes[i]=='float64')|(full_data.dtypes[i]=='int64')] $ cat_cols = [i for i in full_data.columns if (i not in con_cols) & (i!='id')] $ id_col = 'id' $ target_col = 'country_destination'
ds_info = ingest.upload_dataset(database=db, dataset=test) $ ds_info
cats_df.head()
summed.fillna(0)
gs_lr_tfidf.fit(X_train, y_train) 
df[['beer_name', 'brewery_name', 'rating_score']][(df.brewery_name.str.contains('Arcadia')) & (df.beer_name.str.startswith('IPA'))]
print(X_dense_train.shape) $ print(X_conv_train.shape)
par.loc[wpars,"parubnd"] = 1.1 $ par.loc[wpars,"parlbnd"] = 0.9 $ pst.parameter_data
for i in places.index: $     results = Geocoder.reverse_geocode(places[i]['coordinates'][0], places[i]['coordinates'][1]) $     print(results.city)
intervention_train.reset_index(inplace=True) $ intervention_train.set_index(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
print(res.response)
offseason11 = ALL[(ALL.index > '2011-02-06') & (ALL.index < '2011-09-08')]
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'], $                    'data': range(6)}, columns=['key', 'data']) $ df
points_df = pd.DataFrame(all_points, columns = ['Time', 'Lat', 'Lon']) $ points_df.set_index('Time', inplace = True) $ points_df.head()
edgesWeight = pd.read_csv('../CreatedCSVs/edgesWeightned'+country+'.csv', sep=',', decimal='.', header=0) $ edgesWeight.Weight.describe()
temp_long_df['date'] = pd.to_datetime(temp_long_df['date']) $ temp_long_df['year'] = temp_long_df['date'].dt.year $ mean_temp = temp_long_df.groupby(['year'])['temp_c'].mean().reset_index() $ mean_temp = mean_temp[mean_temp['temp_c'] > 8]
ax = pre_analyzeable['prior_number_virtual_labs'].value_counts().plot(kind='bar') $ for p in ax.patches: $     ax.annotate(str(int(p.get_height())), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize=25)
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df3 = df2[df2['group'] == 'treatment'] $ num_group = df3.user_id.count() $ num_converted = df3[df3['converted'] == 1].user_id.count() $ p_new_actual = num_converted / num_group $ p_new_actual
shows3 = pd.read_csv('scraped_data5.csv')
dflong = pd.read_csv("SP500_Long_V4.CSV") $ print dflong.shape $
ts.shift(5, freq='BM')
S_distributedTopmodel.forcing_list.filename
df_western.groupby(['release_decade'], as_index = False)['popularity'].mean()
new = (df2.query('group == "treatment"')['converted']==1).mean() $ new
df.head()
joined_df.head()
!tar -xvzf rossmann.tgz -C data/rossmann/
tags = {} $ for i, el in rentals['description'].iteritems(): $     tags[i] = get_rental_concession(el) $ T = pd.Series(tags)
df = w.data_handler.all_data $ df[df.WATER_BODY_NAME.notnull()].loc[:,['DEPH', 'SALT_BTL', $        'SDATE', 'VISS_EU_CD', 'WATER_BODY_NAME', 'SEA_BASIN', $        'SECCHI', 'TEMP_BTL', 'VISS_EU_ID', 'WATER_DISTRICT', 'WATER_TYPE_AREA', 'MONTH']] $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ z_score, p_value
df.describe()
sys.path.insert(0, '/Users/pedrohserrano/neuroscience/utils') #internal packages $ import buildms as ms $ import statsms as stms $ color_ms = '#386cb0' #blue, This is the color chosen for patients with Multiple Sclerosis $ color_hc = 'red'#This is the color chosen for health control participants
details = pd.read_csv('../../data/raw/AllMoviesDetailsCleaned.csv', sep=';', engine = 'python')
weather.events
pd.Series( 1, index = ['a','b','c','d'])
query = select([users]) $ result = conn.execute(query) $ for row in result : $     print(row)
df.dropna(how='any')  # Drop the rows with any missing data (not in-place)
from sklearn.linear_model import LinearRegression $ lin_reg = LinearRegression() $ lin_reg.fit(ac_tr_prepared, ac_tr_label)
props.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."
doctype_grouped = content_performance_bytime.groupby(['document_type', pd.Grouper(freq='D')])['pageviews'].sum()
articles['content_short'] = articles['tokens'].map(lambda s: ' '.join(s))
results2.summary()
scr_retention_df.to_csv('retention_paid_518.csv')
new_page_converted = np.random.choice([1,0], size = n_new, p = [p_new, 1-p_new]);
s = 'download_Janney.Nestle.reviews.adhoc.F180401T180410.2018_0411_1613.xlsx.csv' $ s.split('.')[4] + "hello"
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ old_page_converted = old_page_converted.shape[0] $ old_page_converted
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').converted.sum() $ convert_new = df2.query('landing_page == "new_page"').converted.sum() $ n_old = df2.query('landing_page == "old_page"').converted.count() $ n_new = df2.query('landing_page == "new_page"').converted.count()
from lxml.cssselect import CSSSelector $ sel = CSSSelector('table[summary] > tbody > ._tracklist_move > .name > a.title') $ nodes = sel(_html)
test.dropna().shape
mnb_classifier = MultinomialNB() $ mnb_classifier.fit(X=X_train,y=y_train)
vip_pivot_table = pd.pivot_table(vip_df, values = 'Payment', index = ['Month'], aggfunc = np.sum) $ vip_pivot_table
knn = KNeighborsClassifier() $ pipe_knn = make_pipeline(cvec, knn) $ pipe_knn.fit(X_train, y_train) $ pipe_knn.score(X_test, y_test)
tokenized_reviewText = [nltk.word_tokenize(text) for text in sdf.Norm_reviewText] $ tokenized_newReview = [nltk.word_tokenize(text) for text in ndf.Norm_reviewText]                        
import requests $ r=requests.get(url) $ _html=r.text
import nltk $ nltk.download('stopwords')
autos[['price', 'odometer_km']].corr()
donald_trump_tweets['screen_name'].value_counts()
pd.options.display.max_colwidth = 1000
acs_df.dropna(inplace=True) $ acs_df = acs_df[['pop', 'age', 'pct_male', 'pct_white', 'income', 'unemployment', 'pct_bachelors', $                   'crime_count', 'permit_count', 'homeval']]
funded.describe(include='all')
beirut.index = beirut['EET']
bus.head(10) $ ins.head(10) $ vio.head(10) $
tweet_json_df_clean['tweet_id'] = tweet_json_df['tweet_id'].astype(str) $ twitter_df_clean['tweet_id'] =  twitter_df['tweet_id'].astype(str)
df2 = pd.merge(donations, df_by_donor, how='inner', on='Donor ID') $ df3 = pd.merge(df2, donors, how='inner', on='Donor ID') $ print(df3.head())
data_path = r'd:\WMIND\temp\pos_dataset_by_user.csv'
merged[merged.contributor_state=="CA"].amount.sum() / merged.amount.sum()
archive.name.value_counts()
success_order_with_cust = order_with_cust[order_with_cust['STATUS'] == "PAID_SUCCESS"] $ success_order_with_cust.head()
import seaborn as sns $ sns.set_style("whitegrid") $ sns.barplot(x=lang_occur.groupby('language').size().index, y=lang_occur.groupby('language').language.size())
len(pd.unique(ratings['movieId'].ravel()))
idx = test_df[ (test_df['Provider Id']==260096)  ].index.tolist() $ idx
BallBerry_ET_Combine_Graph = BallBerry_ET_Combine.plot(color=['blue', 'green', 'orange']) $ BallBerry_ET_Combine_Graph.invert_yaxis() $ BallBerry_ET_Combine_Graph.scatter(xvals, df_gp_hr['Observation (aspen)'], color='black') $ BallBerry_ET_Combine_Graph.set(xlabel='Time of day (hr)', ylabel='Total evapotranspiration (mm h-1) ') $ BallBerry_ET_Combine_Graph.legend()
%time train_4 = ss.fit_transform(train_4)
raw_data.isnull().sum()
from scipy.special import beta $ bayes_prob = beta(6 + 1, 5 + 1) / beta(3 + 1, 5 + 1) $ print("P(B|D) = %.2f" %bayes_prob)
from keras.utils import plot_model $ import pydot
test = pd.DataFrame(test_d)
print(reframed.shape) $ print(bitcoin_market_info.shape)
i1 = sp500.index.get_loc('MMM') $ i2 = sp500.index.get_loc('A') $ i1, i2
from sklearn.decomposition import PCA $ import sklearn
test_array = np.concatenate([test_active_listing_dummy, test_pending_ratio_dummy], axis=1) $ print(test_array.shape)
X_age_test_dummy, _ = custom_dummify(X_age_test, 0.01, train_cols) $ y_pred_test = grid.predict(X_age_test_dummy) $ rmse = np.sqrt(np.mean((y_pred_test[:,np.newaxis]-y_age_test)**2)) $ print ('RMSE of age imputation via random forest is {}'.format(rmse))
goals_df.to_pickle('goals_df.pkl')
archive_df.info()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative = 'smaller') $ print(' z-score = {}.'. format(round(z_score,4))) $ print(' p-value = {}.'. format(round(p_value,4))) $
pd.set_option('max_colwidth',150) $ df_en['text']
ben_final.revtime = ben_final.revtime.str[:19] $
people.plot(kind = "line", x = "body_mass_index", y = ["height", "weight"]) $ plt.show()
%matplotlib inline $ import matplotlib.pyplot as plt $ plt.scatter(iris["Petal.Length"], iris["Sepal.Width"], $            c=np.array(["black", "cyan", "orange"]))
df['fileCount'].describe()
%%time $ df = pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv", usecols=['Unique Key', 'Created Date', 'Closed Date', 'Agency', 'Complaint Type', 'Descriptor', 'Location Type', 'Borough'])
kickstarters_2017[cont_vars].corr()
with open(tweet_file, 'w') as f: $     for tweet in user_tweets: $         f.write(json.dumps(tweet) + '\n')
print (r.json())
my_model_q3_proba = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_knn, training='probability') $ my_model_q3_proba.fit(X_train, y_train) $ my_model_q3_proba.stackData.head()
df2.query('user_id == {}'.format(dup_id)).head()
tweet = api.get_status(id = '892177421306343426', tweet_mode='extended')
so.loc[:, col_bools].head()
lm=sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results=lm.fit() $ results.summary()
from rl.callbacks import ModelIntervalCheckpoint $ save_path= 'outputs/agent_portfolio-ddpg-keras/agent_{}_weights.h5f'.format('portfolio-ddpg-keras-rl')
c = df[df.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro='] $ c
full_data.cust_id.value_counts().unique()
total_min_prc_mon_day = hawaii_measurement_df[["Percipitation"]] \ $ .groupby([ \ $     hawaii_measurement_df["Date"].dt.strftime('%m-%d') $ ]).min() $ total_min_prc_mon_day.head()
beforeUsersAfter=fullDf[(fullDf.user.isin(beforeUsers.values))&(fullDf.index>pd.datetime(2015,4,25))]
engine.execute('SELECT * FROM Station LIMIT 5').fetchall()
import numpy as np $ np.random.randint(1,10,len(rng)) $
import pyspark.sql.functions as func $ hashed_test.groupBy().agg(func.max(col('id'))).show()
AAPL[['close', 'volume']].merge(GOOGL[['close', 'volume']], $                       left_index=True, $                       right_index=True, $                       suffixes=['_AAPL', '_GOOGL'])
data.shape
from nltk.corpus import stopwords $ english = stopwords.words('english')
top = pd.DataFrame({'Word':names, 'Count': counts}) $ cols = ['Word', 'Count'] $ top = top[cols] $ top
control_df = df2.query('group == "control"') $ control_df.user_id.nunique()
new_converted_rate = np.random.binomial(n_new, p_new, 10000)/n_new $ old_converted_rate = np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs = new_converted_rate - old_converted_rate $
temp_cat.categories
predictions = dt.predict(test[['expenses', 'floor', 'lat', 'lon', 'property_type',\ $                                 'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']])
df_ad_airings_5['state'].value_counts()
fromId = dfUsers['userFromId'].unique() $ dfChat = dfUsers[dfUsers['userToId'].apply(lambda x: x in fromId)]
stations = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).order_by(func.count(Measurement.date).desc()).all() $ stations
committee = [r for r in all_results('/committee/C00403592/', {})] $ committee_df = pd.DataFrame(committee) $ committee_df.head()
df['Product'].value_counts().head()
df.to_csv('attacks.csv')
feature_df.to_csv('feature_df.csv',index=False)
predTree = drugTree.predict(X_testset)
au.find_some_docs(ao18_coll,limit=3)
p_old = df2['converted'].mean() $ print ("convert rate for p_old under the null :{} ".format(round(p_old, 4)))
cust_data1['Deciles']=cust_data1['Deciles'].astype('str')
d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Smith','Jack']), $    'Age':pd.Series([25,26,25,23,30,29,23]), $    'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8])} $ df = pd.DataFrame(d)
tweet_archive_clean.tweet_id.dtypes
print(pd.unique(df1['Area'])) $ print(pd.unique(df1['Variable Name'])) $ print(pd.unique(df1['Year']))
excelDF['Order Date'] = excelDF['Order Date'].apply(lambda x: x.strftime('%Y-%m-%d'))
df_total.to_feather(r'F:\Projects\Pfizer_mCRPC\Data\Raw_data\Oncology_EMR\Oncology_EMR_cleaned_with_dummies.feather') $ df_total.to_csv(r'F:\Projects\Pfizer_mCRPC\Data\Raw_data\Oncology_EMR\Oncology_EMR_cleaned_with_dummies.csv', index=False)
"In the last {0} tweets, I've been postive {1} percent, negative {2} percent and neutral {3} percent".format(polarity.sum().counts, percentConvert(polarity.loc["positive"].percent), percentConvert(polarity.loc["negative"].percent), percentConvert(polarity.loc["neutral"].percent))
valid = pd.read_csv(valid_file, header=0, sep='\t', na_values=[''], keep_default_na=False) $ X_valid = valid.iloc[:,features_index] $ y_valid = valid['Label'] $ np.all(valid.columns == train.columns)
state = env.reset() $ state, reward, done, info=env.step(env.action_space.sample()) $ state.shape
data = data.iloc[:,:7]
df_concat_2.message_likes_rel = np.where(df_concat_2.message_likes_rel > 10000, 10000, df_concat_2.message_likes_rel)
eval_df[eval_df.timestamp<'2014-11-08'].plot(x='timestamp', y=['prediction', 'actual'], style=['r', 'b'], figsize=(15, 8)) $ plt.xlabel('timestamp', fontsize=12) $ plt.ylabel('load', fontsize=12) $ plt.show()
to_be_predicted_Day2 = 21.3690037 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df_ad_airings_filter_3['start_time'].min()
target = ["loan_status"] $ features = [x for x in list(df.columns) if x not in target]
df = [child['data'] for child in data['data']['children']]     # to get the first 25 posts $ df = pd.DataFrame(df)      # defining DataFrame $ fetch_time = pd.Timestamp.utcnow()    # new datetime representing UTC day and time instead of taking the given time on reddit $ df['fetched time'] = fetch_time       # this will show the fetch time $ df.head()       #  checking the scrapped df top 5 rows by default
print('The probability of an individual converting regardless of the pay they receive is %.5f'%(df2.query('converted==1').shape[0]/df2.shape[0]))
df_providers.head() 
data_sampling_rate = 0.05
small_df = df.head(10) $ small_df.iloc[0]['account'] $ list(small_df.index)
final_ticker_data.head(2)
df_mes.info()
classes = labels_dedupe.pivot('funding_round_uuid','investor_uuid','invested')
val_small_data.head(1)
%%sql $ COPY facts FROM '/home/ubuntu/project4/Data.csv' $ CSV $ HEADER;
result.acknowledged
import statsmodels.api as sm $ convert_old = df2[(df2['group'] == 'control') & (df2['converted'] == 1)].shape[0] $ convert_new = df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)].shape[0] $ n_old = df2[(df2['landing_page'] == 'old_page')].shape[0] $ n_new = df2[(df2['landing_page'] == 'new_page')].shape[0]
negative_examples.head()
df['acct_type'].value_counts()
yerror = (max_temp - min_temp)/2 $ fig, ax = plt.subplots() $ ax.bar(x = 'Trip Temp', height = avg_temp, yerr = yerror) $ ax.set_title('Trip Avg Temp')
voters.LastVoted.value_counts(dropna=False) $
for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']: $     main["{}_ta_d".format(i)] = main['{}_ta'.format(i)] - main['{}_ta_opp'.format(i)] $ for i in ['pts', 'oreb', 'dreb', 'reb', 'ast', 'stl', 'blk', 'to']: $     main["{}_tb_d".format(i)] = main['{}_tb'.format(i)] - main['{}_tb_opp'.format(i)]
50.238 - 13.602
m_s_df_ = stops_per_crime_per_month_.groupby(stops_per_crime_per_month_.index.month).mean().reset_index() $ m_s_df_['month'] = ['Jan','Feb','Mar','Apr','May','Jun','July','Aug','Sep','Oct','Nov','Dec'] $ m_s_df_.index = m_s_df_.month $ del m_s_df_['month']
filename = processed_dir+'pulledTweetsCleanedLemmaEmEnc_df' $ gu.pickle_obj(filename,pulledTweets_df)
SCR_PLANS_df.head()
df_sb.head(2)
dict_profile = json.loads(df.loc[row,'profile']) $ pprint(dict_profile)
base_hems_url= "https://astrogeology.usgs.gov" $ links[0].attrs.get('href') $ links_visit = ["https://astrogeology.usgs.gov" + x.attrs.get('href') for x in links] $ links_visit $
expiry = datetime.date(2015,1,17) $ aapl_calls = aapl.get_call_data(expiry=expiry) $ aapl_calls.iloc[0:5,0:4]
pandas_df = pd.read_csv("./SIGHTINGS.csv") $ pandas_df.head()
finals.loc[(finals["pts_l"] == 1) & (finals["ast_l"] == 1) & (finals["reb_l"] == 1), 'type'] = 'useless'
cd ..
user.query("followers_count > 100 and friends_count > 200").head(3)
s - pd.tseries.offsets.DateOffset(months=2)
meanMonthlyReturns = dict( { 'Infy': monthly_gain_summary.Infy.mean(), 'Glaxo': monthly_gain_summary.Glaxo.mean(), $                           'BEML': monthly_gain_summary.BEML.mean(),'Unitech': monthly_gain_summary.Unitech.mean()  } )
print("Converted users proportion are {}%".format((df['converted'].mean())*100))
active_psc_records[active_psc_records.company_number.isin(slps.CompanyNumber)].nationality.value_counts().head(10)
RollingSums = Lags.groupby(level=1).apply(pd.rolling_mean, window=3, min_periods=1) $ RollingSums.columns = [c[:-4]+'3GameAvg' for c in RollingSums.columns] $ RollingSums.head()
high_rev_acc = accounts[accounts[' Total BRR '] >= 500000] $ high_rev_acc_opps = pd.merge(high_rev_acc, count_bldg_opps, on=['Account ID'], how='inner') $ high_rev_acc_opps_net = pd.DataFrame(high_rev_acc_opps[high_rev_acc_opps['On Zayo Network Status'] == 'On Zayo Network'])
db.bookings.update_many({"status":"Inititated"},{"$set":{"status":"Init"}})
from ramutils.utils import extract_report_info_from_path $ metadata = extract_report_info_from_path(fr5_session_summary_locations[0]) $ metadata
df1['forcast']=np.nan
%%bash $ git clone https://github.com/pjreddie/darknet $ cd darknet $ make
df.select('a', a_sevenPointSeven).show(5)
df2 = df1[df1['timestamp'] == '2018-01-23'] $ df2.plot( kind='line', x='Hour', y='Polarity',title='Polarity on 23/01/2018') $ axes = plt.gca() $ plt.xticks(rotation='vertical', fontsize=11) $ plt.show()
df_new.ab_page.mean()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').query('converted == 1')['user_id'].count() $ convert_new = df2.query('landing_page == "new_page"').query('converted == 1')['user_id'].count() $ n_old = df2.query('landing_page == "old_page"')['user_id'].count() $ n_new = df2.query('landing_page == "new_page"')['user_id'].count()
a = np.random.random((2, 2)) $ b = np.random.random((2, 2)) $ print(a) $ print(b)
df = df.sort(columns=['Year', 'Month ID'])
dfJobs['DESCRIPTION'].ix[4009].split('\\n')
browser = webdriver.Chrome('chromedriver.exe') $ browser.get(url_valles) $ time.sleep(5) $ html = browser.page_source $ soup = BeautifulSoup(html, "html.parser")
cohorts_size = cohorts['TotalUsers'].groupby(level = 'CohortGroup').first() $ cohorts_size.head()
session.query(func.count(station.id)).scalar()
out = conn.addtable(table='dfcrops', caslib='casuser', $                     **exceldmh.args.addtable) $ out
user_logs_copy = df.groupby('msno').apply(listening_longevity) $
simple_resistance_simulation_1 = sim_ET_Combine['simResist(Root Exp = 1.0)'] $ simple_resistance_simulation_0_5 = sim_ET_Combine['simResist(Root Exp = 0.5)'] $ simple_resistance_simulation_0_25 = sim_ET_Combine['simResist(Root Exp = 0.25)']
pm_data = pd.concat([pm_data.drop('operational_setting_3', axis = 1), $                     pd.get_dummies(pm_data.operational_setting_3, $                                    prefix = 'operational_setting_3')[['operational_setting_3_High']]], $                     axis = 1).drop('index', axis = 1)
print ufos_df.take(5)
age_category1 = df_titanic_temp.loc[df_titanic_temp['age'] < age_median, 'age'] $ age_category2 = df_titanic_temp.loc[df_titanic_temp['age'] >= age_median, 'age'] $ print(len(age_category1)) $ print(len(age_category2))
print('R^2 train: %.3f' % (r2_score(y, y_predit)))
mario_game.dropna(subset = ['Year_of_Release'], inplace = True) $ mario_game.Year_of_Release = mario_game.Year_of_Release.astype('int')
frame2 = frame.reindex(['a', 'b', 'c', 'd'])
for subreddit in like_the_donald: $     if subreddit not in subreddits.keys(): $         print("{0} not in politics subreddits".format(subreddit))
startups_USA['funding_rounds'].hist(bins=range(1,10)) $ plt.title("Histogram of the number of funding rounds") $ plt.ylabel('Number of companies') $ plt.xlabel('Number of funding rounds')
msft_cum_ret.resample("M").ohlc()
pca=decomposition.PCA() $ stocks_pca_t3= pca.fit_transform(stocks_pca_m3)
failed.describe(include='all') $
df.zipcode = df.zipcode.apply(fill_zipcode)
ac.describe()
import calendar $ month2int = {v.lower():k for k,v in enumerate(calendar.month_name)} $ month2int    
df.plot.box()
neg_tweets = ioDF[ioDF.all_sent_x < -.5]
old_page_converted = np.random.binomial(1,p_old,n_old)
n_old = df2.query('landing_page == "old_page"').nunique()['user_id'] $ n_old
df_arch_clean.info()
fig_2= plt.figure(figsize=(10,5)) $ b2tools.plot_synapses(conn['L4exc_L4exc'].i,conn['L4exc_L4exc'].j,marker='.')
loan = ha.accounts.credit_subaccount(acc, '13EX5i', 1.3, 82)
c="r,g,b,c,m,y,k,w".split(",") $ scatter_matrix(dfL, alpha=1, figsize=(10,10), s=100, c=df[predictColumn].apply(lambda x:c[x]));
df[df.state_cost==425] $ df[df.item_number==995381].item_descript
Ser1.iloc[]
sl.describe()
print(soup.title)
dr_new_hours.values[1].dtype
logit = sm.Logit(df4['converted'],df4[['intercept','country_CA','country_UK']]) $ result = logit.fit() $ result.summary()
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','US','CA','US_ab','CA_ab']]) $ results = logit_mod.fit() $ results.summary()
avg_order_intervals = np.fromiter(result.values(), dtype=float) $ avg_order_intervals.size
autos["registration_year"].value_counts().sort_index()
megmfurr_tweets = pandas.read_csv('@megmfurr_tweets.csv') $ megmfurr_tweets
y_test_over[k150_bets_over].sum()
import statsmodels $ print(statsmodels.__version__)
def mean_absolute_percentage_error(y_true, y_pred): $     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
img_url_valles = soup.find('div', 'downloads').a['href']
time2 = time_utc - start_time $ for i in range(0,l): $     ind = int(np.floor((time2[i])/40)) $     labels.loc[ind, 'Key_id'] = key_ids[i] $     labels.loc[ind, 'Key'] = keys[i]
run txt2pdf.py -o"2018-06-18  2015 291 discharges.pdf"  "2018-06-18  2015 291 discharges.txt"
crosstab = pd.crosstab(index=purchase_history.user_id, columns=purchase_history.product_id)
merged.groupby(["contributor_firstname","contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount", ascending=False)
df1 = newdf.drop(['Area Id','Variable Id','Symbol'],axis=1) $ df1
top_songs.shape
dr_new_patient_8_to_16wk_arima = dr_new_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']] $ dr_new_patient_8_to_16wk_arima.index = dr_new_patient_8_to_16wk_arima.index.date
a = df_protest.loc[0:10, 'start_date'].weekday()
pd.merge(df3,df4)
np.random.seed(123456) $ ts = Series([1,2,2.5,1.5,0.5],pd.date_range('2014-08-01',periods=5)) $ ts
collection.create_snapshot('snapshot_name') $ collection.list_snapshots()
data_vi_week.ix[days].plot(lw=3, figsize=(6,4), kind='bar'); $ plt.ylim(179000);  # Set the bottom axis to 179000.
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = logit_mod.fit() $
tweet = result[0] $ for item in dir(tweet): $     if not item.startswith("_"): $         print("%s : %s\n" % (item, eval('tweet.'+item)))
df2['new_sales'] = df2.new_sales.apply(add_dollar) $ df2['sales'] = df2.sales.apply(add_dollar) $ df2['cust_avail_v3'] = df2.cust_avail_v3.apply(make_perc) $ df2['new_conv'] = df2.new_conv.apply(make_perc) $ df2['new_sales_perc'] = df2.new_sales_perc.apply(make_perc)
users_cost.Costs = users_cost.Costs.fillna(0) $ users_cost.Costs = users_cost.Costs / users_cost.num_users $ users_cost = users_cost[['id', 'Reg_date', 'id_partner', 'name', 'Costs']] $ users_cost.head()
df_subset2.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70) $ plt.show()
user_log_counts.head()
actions.columns = ['BUY=0_SELL=1_HOLD=2'] $ result.join(actions).plot(subplots=True, figsize=(15,10));
df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium", "good", "very good"]) $ df["grade"]
new_df['index'] = new_df['index'].apply(conv)
with tf.Session(): $     print(T.eval(feed_dict={y: np.array([0, 1, 2, 3, 9])}))
p_new = len(df.query('converted == 1'))/len(df)
main_dir = "/Volumes/LaCie/Research/NAC/" $ data_dir = "{}data/".format(main_dir) $ out_dir = "{}output/".format(main_dir)
from scipy.stats import norm $ print('CDF of Z-Score: ', norm.cdf(z_score)) $ print('Z-Score: ', z_score, ' < ',norm.ppf(1-0.05))
sales_df.groupby('Country').count()['Revenue'].sort_values(ascending=False) $
rfbestgini = RandomForestClassifier(max_depth=14, n_estimators=10, criterion = 'gini') $ rfbestgini.fit(X_trainfinaltemp, y_train)
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_6204.values.squeeze(), lags=40, ax=ax1)
overdue = data_full.query('timeliness_overdue == 1 & paid_status_PAID == 1')
times = pd.DatetimeIndex(data.datetime_col) $ grouped = df.groupby([times.hour, times.minute])
Station_data = session.query(Measurement).first() $ Station_data.__dict__
multi_app = picker[picker>1] $ print len(multi_app) $ df_nona[df_nona.district_id.isin(multi_app.index)].install_rate.hist() $
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date'], axis=1) $ dfY = data['corrCost']
dat.status[dat.completed.isnull()].unique()
gdax_trans['Balance']= 0.00
drace_df.manager_skill.describe()
plt.figure(figsize=(20,6)); $ plt.plot(df['MeanFlow_cfs']); $ plt.axvline(x='1980-01-01',color='red',ls='--'); $ plt.title("Neuse River Near Goldsboro, NC"); $ plt.ylabel("Discharge (cfs)");
df_complete = pd.merge(left=merged_predictions_archive, right=df_json_tweet, how='inner', on=['tweet_id']).copy()
np.array(df[['Visitors','Bounce_Rate']]).tolist()
np.array_equal([d[0] for d in reps], [d[0] for d in threaded_reps])
df.title.astype(str)
df_pol['pol_id']= le.inverse_transform(df_pol['y_hats'])
cur.execute("SELECT * from tweet_dump order by id DESC limit 10;") $ result = cur.fetchall() $
X = np.array(df1.drop(['label'], axis=1)) $ y = np.array(df1['label'])
NewPage = df2.query('landing_page=="new_page"')['user_id'].count() $ NewPage
heights_B=pd.Series(np.random.normal(170,25,5),index=['s1','s2','s3','s4','s5']) #pd.Series(170+25*np.random.randn(5),index=['s1','s2','s3','s4','s5']) $ weights_B=pd.Series(np.random.normal(75,12,5),index=['s1','s2','s3','s4','s5']) #pd.Series(75+12*np.random.randn(5),index=['s1','s2','s3','s4','s5'])
df.loc['2018-05-21','Open'] 
finals[finals.pts_l==1].sort_values("PTS", ascending=False).head()
graf['DETAILS']=graf['DETAILS'].str.replace('\n', ' ')
loans_df.loan_status.replace(inplace=True, to_replace='Default', value='Charged Off')
popCon = pd.DataFrame(likes.groupby(by=['contact','content']).size()) $ popCon.columns = ['counts'] $ popCon = popCon.reset_index() $ popCon.sort_values(by='counts', ascending=False).head(10)
test_corpus = [" ".join([t for t in reuters.words(test_doc[t])]) $                for t in range(len(test_doc))] $ print("test_corpus is created, the first line is: {} ...".format(test_corpus[0][:100]))
churn_df.columns
df.info()
tweet_data = pd.read_json(twitter_json) $ tweet_data.set_index('created_at', drop=True, inplace= True) $ pd.to_datetime(tweet_data.index)
video_meta = yt.get_video_metadata(video_id, key)
X_train.columns[220:240]
if not os.path.isdir('output/wind_generation'): $     os.makedirs('output/wind_generation')
def download_recent_tweets_by_user(user_account_name, keys): $     import tweepy $     ...
writer.close()
def print_full(x): $     pd.set_option('display.max_rows', len(x)) $     print(x) $     pd.reset_option('display.max_rows') $
autos["price"] = autos["price"].str.replace("$", "").str.replace(",", "").astype(int) $ autos["price"].head()
mentioned_bills_all['congress'].value_counts().sort_index()
df = pd.read_csv("sample50.csv")
df2.shape
fig, ax1 = plt.subplots() $ ax2 = ax1.twinx() # twinning the axis that is common $ df_filter.plot(kind='line',x='start_date', y='id_x', ax=ax1,c='g') $ df_filter.plot(kind='line',x='start_date', y='cloud_cover',ax=ax2,c='r') $ plt.show()
joined['QorE'] = joined['Headline'].str.contains(r'\!|\?').astype(int) $ joined['Q&A'] = joined['Headline'].str.contains(r'Q\. and A\.').astype(int)
cityID = 'a3d770a00f15bcb1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Corpus_Christi.append(tweet) 
n_old=df2[df2['group']=='control'].shape[0] $ print(n_old)
url = "http://sealevel.colorado.edu/files/2015_rel2/sl_ns_global.txt" $ global_sea_level = pd.read_table(url, sep="\s+") $ global_sea_level
df.dtypes
all_scores = [] $ for k in range(10,200,10): $     knn_reg = KNeighborsRegressor(n_neighbors = k) $     knn_reg.fit(x_train,y_train) $     all_scores.append(knn_reg.score(x_test,y_test))
cust_demo.get_dtype_counts()
df['domain'].value_counts().plot(kind='hist') $ plt.xticks(rotation=70);
rcf_inference = rcf.deploy( $     initial_instance_count=2, $     instance_type='ml.c5.xlarge', $ )
ad_group_performance['Clicks'].sum()
menu_about_latent_features_with_menu_ids = np.insert(menu_about_latent_features, 0, np.array([menus_to_analyze.id.values]), axis=1)
engine.table_names()
Y, W = dmatrices('Y ~ W', data=df) $ mod = sm.OLS(Y, W) $ res = mod.fit() $ res.summary2()
mainstream_facts_metrics.shape
pd.Period("2018-04-23") # inferred as Day
archive_clean[(archive_clean['rating_numerator'] == 0) | (archive_clean['rating_numerator'] == 1)].tweet_id
tweet_df.retweeted.value_counts() $
my_df['target'] = my_df['target'].astype(int)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=100, random_state=None, shuffle=True) $ estimator = Ridge(alpha=7000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df8_lunch.count()
df3 = countries_df.merge(df2,on='user_id') $ df3.head() $ df3.groupby('country').mean() $
temp_data_query = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
WorldBankLimited =  sqlContext.sql("select * from worldbank limit 2") $ print type(WorldBankLimited) $ print "*" * 20 $ print WorldBankLimited
prices.head()
for df in (joined, joined_test): $     for c in df.columns: $         if c.endswith('_y'): $             if c in df.columns: df.drop(c, inplace=True, axis=1)
iex_coll_reference.count()
ofav.plot(figsize=(16,4), label="Likes", legend=True,title="Popularity of Barack Obama over time") $ oret.plot(label="Retweets", legend=True) $ orep.plot(label="Replies", legend=True) $ plt.xlabel('Time') $ plt.savefig("fig/obama_popularity_time.png");
df_merged.hist(column='retweet_count',bins=100);
temp_df.plot(kind='bar', figsize=(10, 3));
ebola_melt['country'] = ebola_melt.str_split.str.get(1) $ ebola_melt.head()
picker = df_nona.groupby('district_id').app_id.nunique() $ single_app = picker[picker==1] $ df_nona[df_nona.district_id.isin(single_app.index)].install_rate.hist()
pd.options.display.float_format = '{:,.2f}'.format $ feature_imp = pd.DataFrame({'name': X.columns, 'imp': gbm.feature_importances_}).sort_values(by = 'imp', ascending = False) $ feature_imp['mult_gbm'] = feature_imp.imp.max() / feature_imp['imp'] $ feature_imp['mult_rfc'] = rfc.feature_importances_.max()/rfc.feature_importances_ $ feature_imp $
autos['price'].value_counts().sort_index(ascending = True).head(15)
files.upload()
text = 'My email address is mailto:neal.caren@gmail.com so be sure to send me notes.'
s519397_df = s519397_df.set_index('date') $ print(len(s519397_df.index)) $ s519397_df.info() $ s519397_df.head(5)
save_n_load_df(joined, 'joined_promo_bef_af2.pkl')
lgb_model = lgb.LGBMRegressor(objective='mape',n_estimators=420, learning_rate=0.2, num_leaves=75, random_state=1)
balk_iron_man.head(15)
texasurl = 'https://pubs.usgs.gov/sim/3365/tables/sim3365_table4G.xlsx' $ new_texas_city = pd.read_excel(texasurl,skiprows = [0, 1, 2], skip_footer = 2, parse_cols=[0,2])
new_model = gensim.models.Word2Vec(min_count=1)  # an empty model, no training $ new_model.build_vocab(sentences)                 # can be a non-repeatable, 1-pass generator     $ new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.iter)                       $
df_joined_dummy = pd.get_dummies(data=df_joined, columns=['country']) $ df_joined_dummy.head()
for row in mw.itertuples(): $     rtp = RichTextPage(content=row[4]) $     rtp.save() $
((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idx, df, yl.astype(np.float32))
likes.groupby(['year']).size().plot(kind='bar')
city_holidays_df.head()
plt.hist (returns, bins = 30)
offseason15 = ALL[(ALL.index > '2015-02-01') & (ALL.index < '2015-09-10')]
to_pickle('Data files/clean_dataframe.p',adopted_cats) $ adopted_cats=from_pickle('Data files/clean_dataframe.p')
calls_df["dial_type"].unique()
SAEMSoup = bs4.BeautifulSoup(SAEMRequest.text, 'html.parser') $ print(SAEMSoup.text[100:1000])
x = api.GetUserTimeline(screen_name="HillaryClinton", count=20, include_rts=False) $ x = [_.AsDict() for _ in x]
train_data = pd.read_feather('data/train_data') $ test_data = pd.read_feather('data/test_data')
print(df2[df2['user_id'].duplicated()])
autos.head(10)
temp_series.plot(kind="bar") $ plt.grid(True) $ plt.show()
data = pd.concat([gdp, pop], axis=1); data  #Concatenate two Series to a DataFrame.
features = X.columns $ feature_importances = model.feature_importances_ $ features_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances}) $ features_df.sort_values('Importance Score', inplace=True, ascending=False) $ features_df
tweet = 'I have lived in China all my life, but I was born in Lisbon, Portugal' $ print("{} :  {}".format(color_formatting(ner.is_tweet_about_country(tweet, 'PT')), tweet)) $ print("\t\t| \n\t\t|-> {}".format(ner.get_countries_from_content(tweet)) )
r=rq.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
rf_sentiment = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1) $ scores = cross_val_score(rf_sentiment, X, y, cv=skf) $ print "Cross-validated RF scores based on sentiment:", scores $ print "Mean of scores:", scores.mean()
if not os.path.isdir('output/heat_demand'): $     os.makedirs('output/heat_demand')
gender = records['Gender'].value_counts() $ gender['Missing'] = len(records) - gender['Male'] - gender['Female'] $ gender
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_17, linear.coef_) + linear.intercept_, c='red')
df=pd.DataFrame(d,index=['r_one','r_two','r_three','r_four']) $ df
np.savetxt('myfile.csv', a, delimiter=',')  #Save `a` as a CSV file (comma seperated values, can be read by MS Excel).
df2= df.copy() $ df2.head()
df_temp_z_obs_CTD_1988 = pd.concat([df_temp_obs_CTD_1988, df_z_obs_CTD_1988], axis=1)
from swat.cas import datamsghandlers as dmh
df_tot.info()
df1['vehicleType'].fillna(value='not-available', inplace=True) $ df1['gearbox'].fillna(value='not-available', inplace=True) $ df1['model'].fillna(value='not-available', inplace=True) $ df1['fuelType'].fillna(value='not-available', inplace=True) $ df1['notRepairedDamage'].fillna(value='not-available', inplace=True)
xmlData['country'].replace({' United States of America': 'USA'}, inplace = True)
1-stats.norm.cdf((active_mean - inactive_mean)/diffSD)
new_page_converted = np.random.binomial(1, p_new,n_new) $ new_page_converted
scores_mean = np.mean(raw_scores) $ scores_std = np.std(raw_scores) $ print('The mean is {:.5} and the standard deviation is {:.5}.'.format(scores_mean, scores_std))
mumbai_data = df.loc[df.location.str.contains('umb'),:] $ print(mumbai_data.head(2))
cols = ['GP', 'GS', 'MINS', 'G', 'A', 'SHTS', 'SOG', 'PKG', 'PKA', 'SC%', 'Year'] $ shots_df[cols] = shots_df[cols].apply(pd.to_numeric)
df_usnpl_one_hot = pd.get_dummies(df_usnpl[['domain', 'state_level_media']], columns=['state_level_media'])
def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None): $     with tf.name_scope(name, default_name="safe_norm"): $         squared_norm = tf.reduce_sum(tf.square(s), axis=axis, $                                      keep_dims=keep_dims) $         return tf.sqrt(squared_norm + epsilon)
glm_binom_feat_2.accuracy(valid=True)
RawDF = pd.read_json('try1.json', orient='columns' $              'records', convert_axes=True, convert_dates=True, keep_default_dates=True, $              numpy=False, precise_float=False, date_unit=None, encoding=None) $ display(RawDF[:5])
df.get_dtype_counts() # return Series
corn_vege = ex5.groupby(["corn", "vegetable"])
weather = weather.drop(['events','zip_code'],1)
test.visitors=0
modifications_over_time['modifications_norm'] = modifications_over_time['modifications'].clip_upper( $     modifications_over_time['modifications'].quantile(0.99)) $ modifications_over_time[['modifications', 'modifications_norm']].max()
1/np.exp(-0.0507)
adds.to_csv(folder + "\\" + law_adds_file, sep="\t", index = False)
df_usa = df1[(df1['Area']=='United States of America')] $ df_usa = df_usa.set_index('Year') $ df_usa.head()
print(testObjDocs.buildOutDF.__doc__) # buildOutDF
final = pd.merge(df3, rchilli, on = 'candidateid', how='left')
Y_df = classify_df['Y'] $ Y_df.shape
from sklearn.ensemble import RandomForestClassifier $ from sklearn.grid_search import GridSearchCV $ clf = RandomForestClassifier(class_weight={0 : 1, 1 : int(len(y_train)/sum(y_train))}) $ parameters = {'n_estimators' : [2**n for n in range(0,8)]} $ gscv = GridSearchCV(clf, parameters, scoring='precision')
a = news_df[news_df['Source Acc.'] == 'BBC'] $ a.head() $ print(a['Compound Score'].sum())
df = pd.read_csv('data/btc-market-price.csv', header=None)
pipe = pc.PipelineControl(data_path='examples/simple/data/fixed_process_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=False) $ pipe.runPipeline()
importances = zip(X.columns, rsskb_gbm_best.feature_importances_) $ importances.sort(key=lambda x: x[1], reverse=True) $ importances
guinea_data['Description'].replace('New deaths registered today', 'New deaths registered', inplace=True)
tickerdata.head(5)
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]); $ results = logit_mod.fit()
df['pre_clean_len'] = [len(t) for t in df.text]
p_diffs = np.array(p_diffs)
ibm_hr_final = ibm_hr_int.join(ibm_hr_cat_dum) $ for c in ibm_hr_final.columns: $     ibm_hr_final = ibm_hr_final.withColumn(c, ibm_hr_final[c].cast(IntegerType())) $ ibm_hr_final.printSchema()
log_mod = sm.Logit(df_combined['converted'], df_combined[['intercept', 'ab_page', 'country_CA', 'country_UK']]) $ results = log_mod.fit() $ results.summary()
rnd_search_cv.best_score_ $
plt.style.use('default') $ %pprint
titles_index = title_list_lower.index(test_movie.lower()) $ good_indices = [i for i in range(len(clusters)) if clusters[i] == clusters[titles_index]]
X_today = X2.copy() $ X_today['age_well_years'] = X_today.age_well_years + X_today.time_since_meas_years
sqlContext.sql("select * from pcs where person like 'Abdullah%'").show()
mngr = dsdb.ConnectionManager(dsdb.LOCAL, user="jacksonb") $ local = mngr.connect(dsdb.LOCAL) $ local._deep_print()
result = api.search(q = '%23NYCC') #Get tweets with #NYCC (New York Comic Con, which is happening today) $ len(result)
autos = pd.read_csv("autos.csv", encoding = 'Latin-1') $
cust_data['No_of_30_Plus_DPD'] = cust_data['No_of_30_59_DPD']+cust_data['No_of_60_89_DPD']+cust_data['No_of_90_DPD'] $ print cust_data1.head(2)
embeddings_index2 = dict() $ for i in range(len(word_list)): $     word = word_list[i] $     embeddings_index2[word] = np.asarray(model_ft[word], dtype='float32') $
new = pd.DataFrame(coord) $ new.columns = ['Longitude','Latitude'] $ new.head().T
v = tmdb_movies.production_countries.apply(json.loads)
converted = (df['converted'] == 1).sum() $ prop_converted = converted/row_count $ prop_converted $
exiftool -csv -createdate -modifydate MVI_0011.mp4 MVI_0012.mp4 > out.csv
f_channel_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_channel_hour_clicks"), header=True) $ print('Found %d observations.' %f_channel_hour_clicks.count())
feature_matrix = df_more.copy(deep=True) $ feature_matrix.drop(['Title', 'Location', 'Company', 'Synopsis', 'Low', 'High', 'Average', 'City', 'State', 'Above Median', 'State Initials'], axis=1, inplace=True) $ print (feature_matrix.shape) $ feature_matrix.head()
train.loc[10000,'feature_list']
print(stopwords.words('english')[:10]) $ print(stopwords.words('portuguese')[:10]) $ print(stopwords.words('german')[:10])
learn.save("dnn60")
csvDF.head()
s519281 = session.query(weather.date, weather.tobs).\ $  filter(and_(weather.date.between('2015-01-01','2015-12-31'), weather.station == 'USC00519281')).\ $  order_by(weather.date.asc()).all() $ s519281
lm = sm.Logit(df4['converted'],df4[['intercept','UK','US']]) $ results = lm.fit() $ results.summary()
pd.DataFrame(random_integers, columns=['b', 'a'])
data_df[['clean_desc','tone']][data_df.tone == 'neutral'][2:7]
for tweet in tweepy.Cursor(api.user_timeline).items(10): $     print(tweet.text)
bar.head()
y.shape
df.Category.value_counts()
Fireworks['Complaint Type'].groupby(by= Fireworks.index.date).count().sort_values(ascending= False).head(1)
df.loc[:, 'comments'] = df.loc[:, 'comments'].str[1:] 
print(autos["price"].unique().shape) $ print(autos["price"].describe()) $ autos["price"].value_counts().head(20)
df[df.location_id==0].head()
average_crimes_current.head()
temps1 = pd.Series([80,82,85,90,83,87], index = dates) $ temps1
pd.Period(pd.datetime.today(), 'A') - pd.Period('1/1/1970', 'A')
nb_classifier = nltk.classify.NaiveBayesClassifier $ nb_class = nb_classifier.train(v_train) $ print ("Accuracy of the model = ", nltk.classify.accuracy(nb_class, v_validation))
from urllib import parse $ from urllib import request $ data = bytes(parse.urlencode({'word':'hello'}),encoding='utf8') $ response = request.urlopen('http://httpbin.org/post',data=data) $ print(response.read())
df.groupby('episode_id').location_id.shift().fillna(0).astype('int64').head()
train[(train.age<15)].age.value_counts()
lr_predicted = lr_model_newton.predict(X_test)
df_goog.Date = pd.to_datetime(df_goog.Date)
df.T  # Show the transpose
senti_vader = df.groupby('subreddit')['Vader'].mean()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
props.prop_name.value_counts()
TestRatio = 0.2
twitter_archive_df_clean['name'].replace(to_replace=['None', 'a', 'the', 'an'], value="Unsure", inplace=True)
autos['date_crawled'].str[:10].head()
df2[df2['group']=='treatment']['converted'].mean()  #Probability of the users in control group who coonverted
text = "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?" $ print(text)
df = pd.DataFrame(results_list)
for p in mp2013: $     print("{0} {1}".format(p.start_time,p.end_time))
git_log['sha'].count()
print(len(train_df['app'].unique()))
norm.ppf(1-(0.05/2))# Calculating value at 95% 
output= "Select * from ViewDemo where retweets=3439" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
cityID = 'c7ef5f3368b68777' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Baton_Rouge.append(tweet) 
raw_df.head(5)
import random $ sample=movie1['ceiled_ratings'].tolist() $ x = np.linspace(1,5,100)
desc_stats.transpose().unstack(level=0)
tweets_df_lang = tweets_df.groupby('language')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ tweets_df_lang $
sharpelist = [] $ for coin in coins_top10: $     print(coin, sharpe(r[coin])) $ sharpe(rr.Fund)
archive_df_clean['timestamp'] = archive_df_clean.timestamp.str[:19] $ archive_df_clean['timestamp'] = pd.to_datetime(archive_df_clean['timestamp'], format = "%Y-%m-%d %H:%M:%S")
sns.violinplot(autos["registration_year"])
Count_Row=df.shape[0] $ Count_Col=df.shape[1] $ print("There are %s rows in this file" % (Count_Row)) $ print("There are %s columns in this file" % (Count_Col))
plt.figure(figsize=(8, 5)) $ plt.scatter(prepared_train.favs_lognorm, prepared_train.comments); $ plt.title('The distribution of the favs_lognorm and number of the views without outliers');
tweetsDF.full_text[0]
shows.columns
ca_all_path = cwd + '\\LeadGen\\Ad hoc\\SubID\\CA_SubID_from_python.csv' $ ca_all.to_csv(ca_all_path, index=False)
results = [] $ for tweet in tweepy.Cursor(api.search, q='%40northernlionlp').items(100): $     results.append(tweet)
teams_df.to_csv('~/dotaMediaTermPaper/data/teams_df.csv')
one_day_users = users[(users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))) & $       (users['DaysActive'] == 0)] $ p_one_day_users = one_day_users.shape[0] / users.shape[0] * 100 $ print('Number of users that left the same day of signing in (one-day-users):', one_day_users.shape[0]) $ print('Percent of one-day-users out of total users: %.2f' % p_one_day_users + '%')
db.collection_names(include_system_collections=False)
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', usecols=['Unique Key','Created Date', 'Closed Date', 'Agency', 'Complaint Type', 'Descriptor', 'City']) $
y_pred_clf =clf.predict(X_test) $ y_train_pred_clf=clf.predict(X_train) $ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(clf.score(X_test, y_test)))
lr.score(test_array, y_test) $
if not database_exists(engine.url): $     create_database(engine.url) $ print(database_exists(engine.url))
df_drug_counts.to_csv('../../static/dash_data/named_drugs.csv')
c = np.sort(a.A.flatten()) $ c[len(c)-50:]
df['log_price']=np.log(df['price_doc'].values)
plt.hist(p_diffs, bins=25) $ plt.title('simulated differances') $ plt.xlabel('page diff') $ plt.ylabel('Frequency') $ plt.show()
todaysTweets[todaysTweets[date].isnull()] #check nulls
df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))
intervention_train = intervention_train.merge(contract_history, on='UPD_DATE')
df_clean.drop(df_clean[df_clean['retweeted_status_id'].notnull()== True].index, inplace= True) $ df_clean.shape[0]
if pattern.search('AAbc') is not None: $     print('asdfdsf')
r_json = r.json()
a=[0,1] $ new_page_converted = np.random.choice(a,145310,p=[0.8804,0.1196]) $ print(new_page_converted.mean())
logit4 = sm.Logit(df_new['converted'], df_new[['intercept','new_page','CA_new_page','US_new_page','CA','US']]) $ results4 = logit4.fit() $ results4.summary() $
n_new=df[df['group']=='treatment'].shape[0] $ n_new
df2['converted'].mean()    #convertion rate of people regardless of the page they receive
print hpdvio[hpdvio['BuildingID']==1]['NOVDescription'].tolist()
incomes = data.loc[data.renta.notnull(),:].groupby("total").agg({"renta":{"MedianIncome":np.mean}}) $ incomes.sort_values(by=("renta","MedianIncome"),inplace=True) $ incomes.reset_index(inplace=True) $ incomes.total = incomes.total.astype("category", categories=[i for i in data.total.unique()],ordered=False) $
import matplotlib.pyplot as plt $ import seaborn as sns $ %matplotlib inline $ plt.hist(x = np.log10(result["timestamp"]))
lin2 = sm.OLS(df_new['converted'], df_new[['intercept','US','UK']]) $ result2 = lin2.fit()
plt.hist(p_diffs) $ plt.axvline(x=a_diff,color='red')
df_twitter.info()
countries_grouped = df4.groupby(["country"]) $ countries_grouped.count()
tweet_df['metadata'] = tweet_df['created_at'] $ tweet_df['category'] = 'tweet' $ sou_df['metadata'] = sou_df['president'] + ': ' + sou_df['date'] $ sou_df['category'] = sou_df.president.apply(lambda x: 'trumpsou' if x == 'Donald J. Trump' else 'othersou') $ df = pd.concat([tweet_df, sou_df])
future_forecast = stepwise_model.predict(n_periods=len(test))
(Actual_diff < P_diffs).mean()
p_new =df2['converted'].mean() $ p_new
to_be_predicted_Day5 = 48.27868824 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
 !csvstat --count Data.csv
ncomm = len(salida_comments) $ for i in range(ncomm): $     df = pd.DataFrame(list(scrapComments(i)), index=['comm_id','comm_msg','comm_date']).transpose() $     df_comms = df_comms.append(df, ignore_index=True) $
sfs1 = sfs1.fit(X_df, y_series)
stations = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).\ $ order_by(desc(func.count(Measurement.station))).all() $ stations
df.groupby('episode_id')['raw_character_text'].nunique().agg(['min', 'mean', 'max'])
cityID = '42e46bc3663a4b5f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fort_Worth.append(tweet) 
reason_for_visit.head()
df2_new['intercept']=1 $ logit4=sm.Logit(df2_new['converted'],df2_new[['intercept','ab_page','CA','UK','ab_page_UK','ab_page_CA']]) $ result4=logit4.fit() $ result4.summary()
s.str.cat(sep='_')
grouped_modern.count()
vc.plot(kind='bar', x='date', y='count')
df2.query('user_id==773192')
idxs = get_cv_idxs(n, val_pct=150000/n) $ joined_samp = joined.iloc[idxs].set_index("Date") $ samp_size = len(joined_samp); samp_size
USres.summary()
f_counts_week_ip = spark.read.csv(os.path.join(mungepath, "f_counts_week_ip"), header=True) $ print('Found %d observations.' %f_counts_week_ip.count())
df2.columns
chart = top_supports.head(5).amount.plot.barh() $ chart.set_yticklabels(top_supports.contributor_fullname)
S_lumpedTopmodel.meta_basinvar.filename
df3['DATE'] = df3['DATE'].apply(lambda x: dateutil.parser.parse(x))
print(plan['plan']['itineraries'][0]['legs'][0]['steps'][0].keys())
to_be_predicted_Day5 = 22.34273663 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
text_area = driver.find_element_by_id('textarea') $ text_area.send_keys("print('Robots controlling robots.')")
df['Consumer complaint narrative'] = df['Consumer complaint narrative'].str.lower() $ df.head()[['Product', 'Consumer complaint narrative']]
store.head()
stocks_ts = pd.read_sql("select distinct(ticker) from daily_price where instrument_type='stock'", engine) $
ebola_melt['type'] = ebola_melt.str_split.str.get(0) $ ebola_melt.head()
session.query(Measurement.date).order_by(Measurement.date.desc()).first()
df2 = df2.join(countries_df.set_index('user_id'), on='user_id') $ df2.head()
df2[((df2['group'] == 'treatment') & (df2['converted'] == 1))].user_id.count() / df2.user_id.count()
tw.describe()
df.to_gbq('Instagram.scraping_mx5', 'copacabana-dl-dev')
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist(null_vals) $ plt.axvline(x=obs_diff, color='red'); $ pvalres = (null_vals > obs_diff).mean() $ print(f'Proportion greater:  {pvalres}')
usernodes = list( db.osm.find({"created.user": "Bot45715"})) $ print len(usernodes) $ usernodes[:10]
len(df2[df2['user_id'].duplicated()])
df1 = pd.read_csv('twitter-archive-enhanced.csv')
unigram_sentences_filepath = paths.unigram_sentences_filepath
ab_df.shape[0]
dfSPY=pd.read_csv("data/SPY.csv") $ print(dfSPY.head())
labels.remove(labels[-1])
print len(df_nona) $ df_nona.install_rate.hist(range=(0,1.1), bins=11) $
hours.head(10)
bnbAx.language.value_counts()
df['Descriptor'].value_counts() #descriptions
df12 = pd.read_csv('2012.csv')
data.pivot(columns="center_name", values="attendance_count").resample("Y").sum().plot(figsize=(20,10))
Staten_Island_gdf = newYork_gdf[newYork_gdf.boro_name == 'Staten Island'] $ Staten_Island_gdf.crs = {'init': 'epsg:4326'}
from datetime import datetime $ df3['DAY'] = df3['DATE'].apply(lambda x: x.weekday()) $ df3['DATE'] = df3['DATE'].apply(pd.datetools.normalize_date)
df_clean.to_csv('twitter_archive_master.csv', encoding='utf-8', index=False) $
tz_cat.describe()
df['year'] = df.index.year
df2[df2['landing_page']=='old_page'].shape[0]
train_data.columns
x = df.copy() $ x['pred_std'] = np.std(preds, axis=0) $ x['pred'] = np.mean(preds, axis=0)
senti = ...
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata  = pd.read_csv(path, sep ="\s+") $ mydata.head(5) $
print("Row names for data-text.csv:") $ df.index.values
closeSeriesP = closeSeriesQ.pct_change()
df.set_index("Description", inplace=True)
appointments = appointments.set_index('AppointmentDate')
df_master.drop('created_at',axis =1, inplace = True) $ df_master.info()
image_predictions_clean.p1.value_counts()
k = 3 $ neigh = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)
comment = bigdf.loc[bigdf.index =='dlgb7lh']['comment_body'][0]
weedmaps = 'https://weedmaps.com/deliveries/westsideorganic/menu_items.json' $ df = pd.read_json(weedmaps) $ df
Daily_Price.tail()
from IPython.display import Image $ url = "https://s3.amazonaws.com/leiwen/dmfa/too_big.jpg" $ Image(url=url)
mask = (youthUser3["creationDate"] > '2014-01-01') & (youthUser3["creationDate"]<= '2014-12-31') $ youthUser2014 = (youthUser3.loc[mask]) $ youthUser2014.head()
strategy.df_trades().head()
b = 'hmmm...what does this do to a?' $ print a
from IPython.display import HTML, Image $ Image(url='test3.gif')
mig_l12.agg(np.mean)
dummies = pd.get_dummies(df_merge['country']) $ df_merge[['CA', 'UK', 'US']] = dummies $ df_merge.drop(['CA'], axis = 1, inplace = True) $ df_merge.head()
df2 = df.drop(idx)
msftAC.tshift(1,freq='D')
filename_2018 = os.path.join(input_folder, string_2018)
df_merge.drop(df_merge[df_merge.jpg_url.isnull()].index,axis=0,inplace=True) $
!cat /datalab/web/node_modules/is2/jsdoc/index.html
path = './Data/microbiome' $ doc_names = glob.glob(path + '/MID*.xls')
log_mod = sm.Logit(df_new['converted'], df_new[['intercept','treatment','CA', 'UK']]) $ results = log_mod.fit() $ results.summary()
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ] $ GSW_2017 = games_2017.loc[games_2017.loc[:, "Team"] == "GSW", ] $ rest_2017 = games_2017.loc[games_2017.loc[:, "Team"] != "GSW", ]
r = requests.post('https://stream.twitter.com/1.1/statuses/filter.json', $                  params = {'track': '#data'}, $                  auth=auth, $                  stream=True) # important
(actual_payments.iso_date==EOM_date.date()).sum()
n_old = df2.query("landing_page == 'old_page'")['landing_page'].count() $ n_old
future_forecast = pd.DataFrame(future_forecast,index = test.index,columns=['Prediction'])
df[df.Year > 1975]
for key in list(mi_diccionario.keys()): $     print(mi_diccionario[key])
df_final['join_days'] = (now - df_final['first_trx']).dt.days
cols = ['Respiratory Rate', 'Mean Airway Pressure', 'Inspired Tidal Volume', 'SpO2', 'Heart Rate', 'Extrinsic PEEP', 'Pulse Rate'] $ data1, features = create_features(data, t_before='300s', t_moving='180s', n_before = 6, cols = cols) $ data1 = data1[data1['SpO2'].isnull() == False] $ print(data1.groupby(['y_flag']).size())
import pandas as pd $ import re $ from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
temps_df.iloc[[1, 3, 5]].Difference
corpora.BleiCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.lda-c'), corpus)
diff = new_page_converted.mean() - old_page_converted.mean() $ diff
res=driver.find_elements_by_class_name('conference-item')
    ufo_pandas = pandas_df.toPandas() $     ufo_pandas.describe()
image_predictions_copy['tweet_id'] = image_predictions_copy['tweet_id'].astype(str)
trips.shape[0]  # Tells us how many trips are in the dataset
with open('./data/model/age_prediction_sk.pkl', 'rb') as picklefile: $     grid = pickle.load(picklefile)
df.info()
autos["odometer_km"].value_counts()
len(df2[(df2['landing_page']=='new_page')])/df2.shape[0]
forest.score(X_train_matrix, y_train)
sns.barplot(y = X.columns, x = clf.feature_importances_)
X = np.array(pcaData[selectedComponents]) $ y = np.array(data2Scaled['Label'])
df_ca = df_new[df_new['country'] == 'CA'] $ df_ca['converted'].mean()
index_of_top5 = new_users['CUSTOMER_ID'].apply(checkCustomer) $ top5_data = new_users[index_of_top5] $ top5_data
df2['intercept'] = 1 $ df2[['control_page','ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(['control_page'],axis=1,inplace=True) $ df2.head()
grouped = df.groupby('Team') $ score = lambda x: x*10 $ grouped.transform(score)
files = mysdc.FileNames() $ print(*files, sep='\n')
production_df = pd.merge(future_predictions, features, on=['Date', 'HomeTeam', 'AwayTeam', 'season'])
df_users_3.shape
df = normalize_data_column(df) $ df = normalize_user_properties_column(df) $ df = normalize_event_properties_column(df) $ df.drop(['group_properties', 'groups', 'data', 'user_properties', 'event_properties'], axis=1, inplace=True)
print((3270+1253+329 - m_test['predicted_purchases'].sum())/m_test['predicted_purchases'].sum()) $ m_test['predicted_purchases'].sum()
_ = ok.grade('q05c') $ _ = ok.backup()
lin_mod = sm.OLS(df_new['converted'], \ $     df_new[['intercept', 'ab_page', 'CA', 'abp_CA', 'US', 'abp_US']]) $ results = lin_mod.fit() $ results.summary()
plt.plot(df['id'], df['inputR1'], 'ro') $ plt.xlabel('Mechanism ID') $ plt.ylabel('Input Rotation on X axis') $ plt.show()
browser = Browser('chrome', headless=False) $ image_url  = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars' $ browser.visit(image_url ) $ time.sleep(1)
df_long = df_wide.stack()
(df2.query('group == "treatment"').converted).mean()
poverty_data.head(10)
for n in toNodes: $     dot.node(n)
df_new.head(5)
sns.violinplot(x="status", y="borough",data = samp311);
p_new = df2.converted.mean() $ print("{:.4f}".format(p_new))
test_features = bind_features(test, train_test="test").cache() $ test_features.count()
acc.find(description='12', amount=1500)
my_df.insert(2,'target','0') $ my_df.head() $
cv_score.mean()
import lightgbm as lgb $ mdl7 =lgb.LGBMClassifier(boosting_type ='gbdt',objective ='binary', num_leaves =80, learning_rate =0.1) $ mdl7.fit(X_train,y_train)
tweet_en[tweet_en['text'].apply(lambda x: "I'm at" in x)]
tw_clean.timestamp.head()
val_pred_svm = lin_svc_clf.predict(X_valid_cont_doc)
unique_users = df.user_id.nunique() $ print('The number of unique users in the dataset is {}'.format(unique_users))
df2.drop(labels=1899, axis=0, inplace=True)
df.currency.unique()
data_chunks = pd.read_csv("../data/microbiome/microbiome.csv", chunksize=15) $ data_chunks
with graph.as_default(): $     saver = tf.train.Saver(var_list=tf.global_variables()) $     saver.save(sess,"../data/prepare_weight/2018-06-20_09-00-14".format(model_name,2))
df.shape
import requests $ import pandas as pd $ import numpy as np $ import json $ from pandas.io.json import json_normalize
mit.sort_values('num_commits', ascending=False).head(20)
df[['text', 'retweet_count', 'date']][df.retweet_count == np.max(df.retweet_count)]
affair=data.pop('affair')
nt = pd.read_csv(path+"datas/new_territories.csv", index_col=0)
ea = df_clean.copy()
a = 'this is a string' $ a[10] = 'f' $ b = a.replace('string', 'longer string') $ b
f_ip_device_minute_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_device_minute_clicks"), header=True) $ print('Found %d observations.' %f_ip_device_minute_clicks.count())
for column in ideas: $     ideas[column] = ideas[column].dt.week
results = model.transform(test) $ results=results.select(results["label"],results["prediction"],results["FRAUD"],results['predictedLabel'],results["probability"]) $ results.toPandas().head(10)
print("original 'image_predictions_df' entries: {}".format(len(image_predictions_df))) $ print("updated 'image_prediction_clean' entries: {}".format(len(image_predictions_clean)))
p_diffs = [] $ for _ in range(10000): $     bs_new_page_c = np.random.binomial(1, p_new, n_new) $     bs_old_page_c = np.random.binomial(1, p_old, n_old) $     p_diffs.append(bs_new_page_c.mean()-bs_old_page_c.mean())
plotly_df.groupby('month').sum()
amenities = list(db.osm.aggregate([{"$match":{"amenity":{"$exists":1}}},{"$group":{"_id": "$amenity",\ $             "count":{"$sum": 1}}}, {"$sort": {"count": -1}}])) $ print len(amenities) $ amenities
to_be_predicted_Day2 = 83.14924533 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
stocks_pca_m3= np.zeros(shape=(len(stocks_pca_SP500),3)) $ for i in range(0,len(stocks_pca_happiness)): $     stocks_pca_m3[i][0]=float(stocks_pca_SP500[i]) $     stocks_pca_m3[i][1]=float(stocks_pca_DJIA[i]) $     stocks_pca_m3[i][2]=float(stocks_pca_happiness[i])
df_byzone.tail(2)
yhat_tree = tree.predict(X_test)
acc.find(description='DeScription 12', case=True)
number_of_commits = git_log.timestamp.count() $ number_of_authors = git_log.author.value_counts(dropna=True).count() $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
complete_df = complete_df.join(pd.get_dummies(complete_df['group_country'])) $ complete_df.head()
df[df.index.month.isin([6,7,8])].head() $
medals_data.to_csv("../data/medals.csv", index=False)
old_page_converted=np.random.binomial(n_old,p_old) $ old_page_converted
to_be_predicted_Day2 = 36.44907637 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
students.weight
np.exp(-0.015)
prob_new =len(df2.query("landing_page == 'new_page'"))/len(df2) $ print('The probality of an individual receiving a new page is {}'.format(prob_new))
df_modeling_categorized.head()
sources.to_sql(con=engine, name='sources', if_exists='replace', flavor='mysql', index=False)
!mkdir -p out/0 $ df.to_pickle('out/0/donations.pkl') $ noloc_df.to_pickle('out/0/donations_noloc.pkl')
results = log_mod.fit() $ results.summary()
pickle.dump(sample_weight, open(slowdata + 'sample_weight.pkl', 'wb'))
df.head()
df_converted = df.converted.mean() $ print("The proportion of users converted is {}%.".format(round(df_converted * 100)))
df_train.to_csv("data/df_1.csv", index=False) $ df_test.to_csv("data/df_1_test.csv",index=False)
ddf = dd.read_csv('test-data/output/sample-xls-case-multifile1.xlsx-*.csv') $ ddf.compute().head() $
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from sklearn.metrics import roc_auc_score
data3.to_csv('heating.csv')
X = df.drop('num_comments', axis = 1)
df_unique_providers = df_unique_providers.drop_duplicates(['id_num','name']) $ df_unique_providers = df_unique_providers.reset_index(drop=True) $ df_unique_providers = df_unique_providers.drop('year',axis=1) $
mydata.iloc[0] # specificies the actual row in terms of index type
from sklearn import metrics $ metrics.roc_auc_score(new.popular, new_pred_prob)
autos["registration_year"].describe()
PHI = pd.read_excel(url_PHI, $                     skiprows = 8)
sandwich_test.iloc[2]['reviews_text']
zipShp.shape
crushing_it = sales_vs_targets[sales_vs_targets.revenue > sales_vs_targets.target] $ crushing_it
df_characters[df_characters.character_id>0].sort_values( $     'num_lines', ascending=False).head(10)
train_geo = pd.read_csv('./data/geo_train.csv')
filters = tb.Filters(complevel=5, complib='zlib')
df_comment.liked_by = df_comment.liked_by.apply(ast.literal_eval) $ s = df_comment.apply(lambda x: pd.Series(x['liked_by']),axis=1).stack().reset_index(level=1, drop=True) $ s.name='liker'
sourcetake = pd.concat(sourcet) $ sourcetake.reset_index(inplace=True) $ sourcetake.rename(columns={'level_0':'systemid'},inplace=True) $ sourcetake.set_index(['systemid','Year'],inplace=True) $ sourcetake.drop(['level_1','Measuring','MeasuringMethod','Unnamed: 15','Mea','Meth','Ann'],axis=1,inplace=True)
states = pd.DataFrame(locations['State'].value_counts()) $ top10states = states[0:10] $ top10states.plot(kind='bar') $ plt.show()
data_year_df['Date'] = pd.to_datetime(data_year_df['Date']) $ data_year_df.head() $
c1 = file.to_dataframe(mode='pivot', aggfunc='count') $ c1.head()
con = sqlite3.connect('db.sqlite') $ print(pd.read_sql_query("SELECT * FROM temp_table", con)) $ con.close()
mb = pd.read_csv("data/microbiome.csv") $ mb
df_significant_feature.to_csv('./data/significant_features.csv', index=False)
SVPOL(data/'realdata'/'DD.Linux.dat').to_dataframe().head()
bow_2 = bow_transformer.transform([review_2]) $ bow_2
video_ids = df_yt['yt_id'].unique().tolist()
flight_cancels = flight_cancels.reindex(taxi_hourly_df.index) $ flight_cancels.fillna(0, inplace=True)
df.head()
df_clean['doggo'].value_counts(),df_clean['floofer'].value_counts(),df_clean['pupper'].value_counts(),df_clean['puppo'].value_counts()
np.unique(raw_train_y.values, return_counts=True)
df.columns.values
! python /src/myhome/code/Keras-genomics/main.py -d $data_folder/pep_expt01 -m /src/myhome/code/Keras-genomics/example/model.py -y -t -e
SelectedHighLows = AAPL.loc["2017-06-20":"2017-07-20", ["high", "low"]] $ SelectedHighLows
traded_volume_idx = json_data['column_names'].index('Traded Volume') $ traded_volumes = [datum[traded_volume_idx] for datum in json_data['data']] $ mean_traded_volumes = sum(traded_volumes) / len(traded_volumes) $ print("Average daily trading volume: {}".format(mean_traded_volumes))
cooks_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/cooks.csv')['Body'].read().decode('utf-8') $ cooks = pd.read_csv(StringIO(cooks_csv_string), header=0)
print df_nona.groupby('segment').apply(lambda v: float(sum(v.accounts_provisioned))/sum(v.district_size)) $ df_nona.groupby('segment').install_rate.describe() $
api = twitter.Api(consumer_key = consumer_key, consumer_secret=consumer_secret, $                  access_token_key=access_token,access_token_secret = access_token_secret, $                   sleep_on_rate_limit=False)
S_check2 = session.query(Station).statement $ S_df = pd.read_sql(S_check2,session.bind) $ S_df.head()
df.drop('country_destination',axis=1,inplace=True)
emails_dataframe.to_csv("email-data-with-institution.csv", index=False)
uber = cb.organization('uber')
df2.groupby('group').describe()
hp.search_sites(inhabitants=5)
df["grade"].cat.categories = ["very good", "good", "very bad"] $ df["grade"]
results=[] $ for tweet in tweepy.Cursor(api.search,q="ivanka").items(10): $     results.append(tweet)
session.query(Station.name).count()
B4JAN16 = TERM2017.loc[TERM2017.Create_Date <= tsprior,:] $ B4JAN17 = TERM2018.loc[TERM2018.Create_Date <= tsprior2,:] $ B4JAN18 = TERM2019.loc[TERM2019.Create_Date <= tsprior2,:]
def predict(classifier, x_test): $     pred_y = [] $     for prediction in classifier.predict(x_test, as_iterable=True): $         pred_y.append(prediction['class']) $     return pred_y
s.ix[:3].resample('W')
data.info()
pd.set_option('display.max_columns', 50) $ tt_final.head()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=32000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
reddit = [child['data'] for child in data['data']['children']] $ reddit = pd.DataFrame(reddit) $ fetch_time = pd.Timestamp.utcnow() $ reddit['fetched time'] = fetch_time $ reddit.head(2)
n_old = df2[df['group']=='control'].shape[0] $ n_old
print(len(fdist.hapaxes()))
tce2 = tce[~tce['ElectronicCollection'].isin(tc_ecolls_to_exclude['ecoll'])] $ tce2
vect = TfidfVectorizer(ngram_range=(2,4), stop_words='english') $ summaries = "".join(matt_tweets['text']) $ ngrams_summaries = vect.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
df_amznnews_clsfd_2tick = df_amznnews_clsfd[['publish_time','textblob_sent', 'vs_compound']] $ df_amznnews_clsfd_2tick.head()
cp311 = pd.concat([cp311,dftype],axis=1,join='inner').copy()
ml.drop(columns='7dayforecast',inplace=True)
goal_pace = 5.5 $
Most_active_stations_tobs_results = session.query(Measurement.station, Measurement.tobs).\ $                                    filter(Measurement.date.between('2017-08-01', '2018-07-31')).\ $                                    filter(Measurement.station == Most_active_stations).all() $ Most_active_stations_tobs_results
combined_df5 = combined_df4.copy() $ threshold=20 $ counts = combined_df5['llpg_usage'].value_counts() $ repl = counts[counts <= threshold].index $ combined_df5['llpg_usage']=combined_df5['llpg_usage'].replace(repl, 'Other') $
df_tweet_clean.info()
state_keys_list = sorted(list(state_DataFrames.keys())) $ state_keys_list
pd.Series({2:'a', 1:'b', 3:'c'})
data.to_csv('/home/sb0709/github_repos/bootcamp_ksu/Data/data.csv', sep = ',')
country = pd.DataFrame(g_all.loc[0:259,'Life expectancy']) $ country.columns = ['Life expectancy'] $ print(country.shape) $ print(country.head())
probability = rdf_clf.predict_proba(X_test[columns])
jobs.loc[(jobs.FAIRSHARE == 132) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'h_vuiis')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
df.describe()
from pandas import scatter_matrix $ scatter_matrix(train_data.get(["nb_words_content", "pp_uniq_words"]), alpha=0.2, $                figsize=(6, 6), diagonal='kde')
yhat1 = neigh.predict(X_test) $ yhat2 = tree.predict(X_test) $ yhat3 = clf.predict(X_test) $ yhat4 = lr.predict(X_test)
titanic.isnull().head()
store_items.interpolate(method = 'linear', axis = 0)
len(pd.unique(f['user_id']))
weather_dates = [] $ for pair in xrange(len(weather_dt)): $     weather_dates.append(datetime.datetime.combine(weather_dt[pair][0], weather_dt[pair][1])) $
data.head()
print(sorted(list(pol_users.columns))) $ print() $ print(sorted(list(troll_users.columns)))
from dateutil import parser $ date = parser.parse("4th of July, 2015") $ date
vwap.ix['2011-11-03':'2011-11-04'].plot() $ plt.ylim(103.5, 104.5) $ vol.ix['2011-11-03':'2011-11-04'].plot(secondary_y=True, style='r')
sample_single_treat = treatment_group.sample(size_treatment, replace = True) $ new_page_converted = (sample_single_treat['converted'] == 1).sum() / sample_single_treat.user_id.count() $ new_page_converted
import seaborn as sns $ sns.lmplot(x='hour', y='start', data=hours, aspect=1.5, scatter_kws={'alpha':0.2})
pd.Period('2012-1-1', freq='D')
import pandas as pd $ df = pd.DataFrame(complaints) $ df.columns = ['unique_key', 'complaint', 'created_date', 'closed_date'] $ df_original = df.copy()
df_twitter_copy = df_twitter_copy.drop(['rating_numerator', 'rating_denominator'], axis = 1)
print(df[df['Product']=='Wine - Gato Negro Cabernet']['Quantity'].mean()) $ print(df[df['Product']=='Pork - Tenderloin, Frozen']['Price'].mean()) $ print(df[df['Product']=='Wine - Gato Negro Cabernet']['Quantity'].mean()) $ print(df[df['Product']=='Pork - Tenderloin, Frozen']['Price'].mean())
df1 = df.loc[:, ('Close')].reset_index().rename(index=str, columns={"Date": "ds", 'Close': 'y'})
ridge3 = linear_model.Ridge(alpha=-0.6) $ ridge3.fit(X_17, y2) $ (ridge3.coef_, ridge3.intercept_)
train, valid = h2o_age.split_frame([0.8], seed=1234) $ X = h2o_age.col_names[:-1] $ y = h2o_age.col_names[-1]
for el in twitter_archive_master['name'].head(10): $     if el[0] in ['T', 'M']: $         print(el.split(' ')[-1][:-1]) $     elif el != 'nan': $         print(el.split(' ')[-2])
df2.drop(labels=1899, axis=0, inplace=True)
bucket.upload_dir('data/wx/tmy3/proc/', 'wx/tmy3/proc', clear_dest_dir=True)
tfav_b.plot(figsize=(16,4), label="Likes", legend=True) $ tret_b.plot(figsize=(16,4), label="Retweets", legend=True);
scores = cross_val_score(model, X_train, y_train, cv=5)  # cross Validation $ np.mean(scores), np.std(scores) #  scoring the performance of training data
tweets_df.id.describe()
CVIterator = [] $ for i in dates: $     trainIndices, valIndices = create_validation(train, i) $     CVIterator.append( (trainIndices, valIndices) )
for i in vectorized.columns: $     print i, vectorized[i].sum()
likes.groupby(by='content').size()
balance.resample('M', convention='end').asfreq()
output.show(2)
transactions_to_products_df.info()
print(dtm_tfidf_df.max().sort_values(ascending=False)[0:20])
end_date = pd.Series(pd.to_datetime(end_date).strftime('%Y-%m'),index=churned_ix)
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_opb_chord_saddle[1][0])) $ perr = np.sqrt(np.diag(pcov_opb_chord_saddle[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
grid_search.best_score_
start = datetime.now() $ print(start) $ recall, precision, f1_score = main(train_features,trainset,test_features,testset) $ end = datetime.now() $ print(end)
sm_model = sm.Logit(df_new['converted'] , df_new[['intercept','ab_page_UK','ab_page_CA','CA','UK','ab_page']]) $ results_model=sm_model.fit() $ results_model.summary()
intersections_final.head()
original_values = buckets_to_df(response['aggregations']['1']['buckets']) $ original_values = original_values.fillna(0)
df.describe()
cust_data.NewColumn.head(10)
np.exp(result_c.params)
rnd_reg_2.oob_score_
def analyst(x): $     if 'Analyst' in x: $         return 1 $     return 0 $ df_more['Analyst'] = df_more['Title'].apply(analyst)
page_soup.title.string
merged.groupby(["committee_name_x","contributor_firstname","contributor_lastname"]).amount.sum().reset_index().sort_values("amount", ascending=False).head()
print(df.loc[sd:ed][['GOOG','GLD']].head()) #more pythonic? $ print(df[sd:ed][['GOOG','GLD']].head())
actual_diff = df.query("group == 'treatment'").converted.mean() - df.query("group == 'control'").converted.mean() $ p_diffs = np.array(p_diffs) $ (actual_diff < p_diffs).mean()
final_df.corr()["ground_truth_crude"][names]
high_rev_acc_opps_net.drop([' DandB Revenue ', 'DandB Total Employees'], axis=1, inplace=True)
data.dropna(axis=1)
for col in b_rev.columns: $     print(f'{col}....{len(b_rev[col].unique())}') $
df = pd.read_csv('data/goog.csv', parse_dates=['Date'], index_col='Date') $ df
stock_data.head(3)
active_distinct_authors_latest_commit = authors_grouped_by_id_saved.filter( $     (F.date_sub(F.current_date(), 365)) < authors_grouped_by_id_saved.latest_commit)
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %X %p')
dforders = ml4t.build_orders(dfprediction, abs_threshold=0.01, startin=False, symbol='USD-BTC') $ dforders
df_new['intercept'] = 1 $ logit2 = sm.Logit(df_new['converted'],df_new[['intercept','CA','US']]) $ r2 = logit2.fit() $ r2.summary()
properati.info()
df_tweet_clean['tweet_id'] = df_tweet_clean['id'].astype(str) $ df_tweet_clean             = df_tweet_clean.drop('id', axis=1) $ df_tweet_clean.head()
df = tables[0] $ df.columns = ['State', 'Abr.', 'State-hood Rank', 'Capital', $               'Capital Since', 'Area (sq-mi)', 'Municipal Population', $               'Metropolitan Population', 'Population Rank In State', 'Population Rank in US', 'Notes'] $ df.head()
parties['Unique Key'].groupby(by= parties.index.dayofweek).count().plot()
print('the one user_id repeated is {}'.format(df2[df2.duplicated(['user_id'], keep=False) ]['user_id'].iat[0])) $
sl_data.Description.replace('new_confirmed', 'Total new cases registered so far', inplace=True) $ sl_data.Description.replace('etc_new_deaths', 'New deaths registered', inplace=True)
p_overall = df2.converted.mean() $ p_overall
df_master_select.rename(columns={'state': 'Status', 'category': 'sub_category', 'launched_atYM': 'month_launched'}, inplace=True) $ df_master_select.head()
reddit_comments_data.orderBy('score', ascending = False).select('score').show(10)
join_c.registerTempTable("x") $ predictions_table = sqlContext.sql("SELECT party_id_orig,aggregated_prediction,predicted  FROM( SELECT *, ROW_NUMBER()OVER(PARTITION BY party_id_orig ORDER BY aggregated_prediction DESC) rn FROM x) y WHERE rn = 1").cache()
pd.Timestamp('now', tz='Etc/GMT-3')
sample_sizes.groupby().agg(F.sum("sample_size_1")).show()
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = lm.fit() $ result.summary()
def qualConversion(x): $     p = '' $     if type(x.opportunity_qualified_date) == pd.tslib.Timestamp: p = 'convertedQual' $     return p
regions = [label(up > 128) for up in upsampled_predictions] $ regions[25].max()
team_analysis = ALLbyseasons.groupby(["Team"]) # Groups our sample by team
BID_BDAY_URL = 'https://nanit-bi:pcjxg72f3yat@redash.nanit.com/api/queries/176/results.json?api_key=UNoGTb6vTkzt2Bg7h6dkD7xSnAvib8WpnNjWYIbD'
ccl["rise_in_next_week"] = (ccl["CCL"].shift(-1)/ccl["CCL"] >= 1).astype("int")
breed_predict_df_clean.head(1)
from collections import Counter $ words = set() $ word_counts = data['Tweets'].apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0) $
df.xs(key=('a', 'ii', 'z'))
investment_dates = pd.merge(investments,rounds[['funding_round_uuid','announced_on']], on = 'funding_round_uuid')
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot $ import plotly.graph_objs as go $ from plotly import tools $ import plotly
cityID = '7068dd9474ab6973' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Toledo.append(tweet) 
weights = np.random.random(num_assets) $ weights /= np.sum(weights) $ weights
%matplotlib inline $ (autos.unrepaired_damage.value_counts(normalize=True, $                                      dropna=False) $                         .plot.bar()) $
train.head(3)
building_pa_specs = pd.read_csv('DataDictionaryBuildingPermit.csv') $ building_pa_specs
prior = float(y_train.sum())/len(y_train)
learn.unfreeze() $ learn.fit(lrs,3,cycle_len=1,cycle_mult=2)
dataset['User_Created_At'] = dataset['User_Created_At'].dt.strftime('%m/%Y') $ acc_created_date = pd.DataFrame({ $     'Date': pd.to_datetime(dataset['User_Created_At'].value_counts().index.tolist()), $     'Freq': dataset['User_Created_At'].value_counts() $ }) $
vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 2))  $ transformed = vectorizer.fit_transform(cleaned_texts.values) $ features = vectorizer.get_feature_names()
y1t = y1[~((y1-y1.mean()).abs()>10*y1.std())] $ y2t = y2[~((y2-y2.mean()).abs()>10*y2.std())]
sheets.keys()
df.replace({'appeal': {'0': ''}}, inplace=True) $ df.appeal.fillna('', inplace=True) $ df.fund.fillna('', inplace=True)
autos['ad_created'].str[:7].value_counts().sort_index()
df = pd.read_csv("https://raw.githubusercontent.com/YingZhang1028/practicum/master/Data_DataMining/test_score.csv") $ test_df["description_score"] = df["description_score"] $ test_df["description_score"].ix[np.isnan(test_df["description_score"]) == True] = test_df["description_score"].mean()
docs = text_cleaner.clean(texts)
tweet_data_copy[['id', 'id_str']].head()
todaysTweets_json = todaysTweets.to_json(orient='records') $ with open('Data/todaysTweets_'+date+'.json','w') as fp: $     json.dump(todaysTweets_json,fp)
tmp = data['deadline'] - data['launched_at'] $ data['duration'] = pd.to_timedelta(tmp, unit='D') $ data.head()
twitter_archive_master.columns
terms_single = set(terms_all) $ count_single = Counter() $ count_single.update(terms_single) $ print(count_single.most_common(5))
final.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/all_objs.csv',index=False)
theft.DATE_OF_OCCURRENCE = pd.to_datetime(theft.DATE_OF_OCCURRENCE)
gDateProject_all = itemTable.groupby([pd.Grouper(freq='1W', key='Date'),'Project'], as_index=True) $ gDateProject_all.groups $ gDateProject_all.count() $ gDateProject_content = gDateProject_all['Content'] $ gDateProject_content = gDateProject_content.count() $
mlb = MultiLabelBinarizer(classes=list(set(all_labels))) $ train_labels_bin = mlb.fit_transform(train_labels) $ test_labels_bin = mlb.transform(test_labels)
volumes[['Bitcoin', 'Ethereum', 'Ripple']].plot(logy=True) $ plt.ylabel('Volume') $ plt.show()
import statsmodels.api as sm $ reg1 = sm.OLS(endog=df1['logpgp95'], exog=df1[['const', 'avexpr']], missing='drop') $ type(reg1)
rate.head()
conn = pymysql.connect(host='mysql.guaminsects.net',user='aubreymoore',passwd='canada12',db='oryctes') $ cursor = conn.cursor() $ cursor.execute(s) $ cursor.close() $ conn.close()
df_wm['Sentiment_class'] = df_wm.apply(conditions, axis=1)
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=900000) $
autos.head()
daily_df = pd.concat([predictions_daily, stock_daily], axis=1, join='inner') $ daily_df.head()
cov_matrix_b = daily_ret_b.cov() $ cov_matrix_b
weekdays_avg.to_csv('polarity_results_LexiconBased/weekdaysVSweekends/polarity_avg_weekdays_2012_2016.csv', index=None) $ weekdays_count.to_csv('polarity_results_LexiconBased/weekdaysVSweekends/polarity_count_weekdays_2012_2016.csv', index=None) $ weekends_avg.to_csv('polarity_results_LexiconBased/weekdaysVSweekends/polarity_avg_weekends_2012_2016.csv', index=None) $ weekends_count.to_csv('polarity_results_LexiconBased/weekdaysVSweekends/polarity_count_weekends_2012_2016.csv', index=None)
for col in ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']: $      df_raw[col].replace({'Yes': 1, 'No': 0, 'No internet service': 2}, inplace=True)
from nltk.tokenize import word_tokenize #Import word tokenize funktion from NLTK.
def time_between(times): $     sorted_times = times.sort_values() $     time_between = sorted_times - sorted_times.shift(1) $     return time_between
df_uk[df_uk['ab_page'] == 0]['converted'].mean()
austin = austin.drop(discrp.index)
df_final[features2].describe()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
tw.sample(1)
grid_search.train(X, $                   y, $                   training_frame = train, $                   validation_frame = valid)
df.head(1)
print(len(df)) #Print number of cases. $ df.head()
spark_df = sqlContext.createDataFrame(pandas_df)
x = soup2.find_all('div', class_='schedule-container')[0].select('.disabled') $ str(type(x[0].get('attr-mov'))) == "<class 'NoneType'>"
def percentdonationoptional(num): $     print('\n', f'{num} dollars') $     print(donations.loc[donations['Donation Amount'] == num, 'Donation Included Optional Donation'].value_counts()) $     print('percentage') $     print(donations.loc[donations['Donation Amount'] == num, 'Donation Included Optional Donation'].value_counts(normalize=True))
values=['2020Q1', '2020Q2', '2020Q3', '2020Q4'] $ index=pd.PeriodIndex(values, freq='Q') $ index
lims_query = "SELECT cell.name as cell_name, donors.full_genotype, cell.donor_id, donors.id \ $ FROM specimens cell JOIN donors ON cell.donor_id = donors.id \ $ WHERE cell.ephys_roi_result_id IS NOT NULL" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
df2 = df.query("(group == 'control' and landing_page == 'new_page') or (group == 'treatment' and landing_page == 'old_page')") $ df2.shape[0]
df_reg = df2.copy()
print 'Can pass partial datetime values and get sensible output:' $ msft.loc['2012-02':'2012-03'] # Feb and March 
print ('temperatura maxima',temperatura_global.LandAverageTemperature.astype(float).max()) $ print ('temperatura minima',temperatura_global.LandAverageTemperature.astype(float).min())
all_cards = all_cards[~all_cards.index.duplicated(keep = "first")]
value=ratings['rating'].unique() $ value
another_rdd = sc.parallelize([("John", 19), ("Smith", 23), ("Sarah", 18)]) $ schema = StructType([StructField("person_name", StringType(), False), $                      StructField("person_age", IntegerType(), False)]) $ another_df = sqlContext.createDataFrame(another_rdd, schema) $ another_df.printSchema() $
weather.zip_code.unique()
station = combined_turnstile.groupby(["DATE"]) ["TOTALTRAFFIC"].sum().reset_index() # without this reset_index DATE column would be an index $ station
df2 = df.query("group == 'treatment' and landing_page == 'new_page' or group == 'control' and landing_page == 'old_page'") $ df2.head()
tweet_df_clean = tweet_df_clean[tweet_df_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']) == True]
payments_total_yrs.tail() $ payments_total_yrs.to_csv('Top Total Payees More than 1 Million Total.csv') $
colNames = autos.columns
from pyaerocom.io.read_aeolus_l2b_data import ReadAeolusL2bData $ ADM = ReadAeolusL2bData(verbose=True)
print(sorted(list(pol_users.columns))) $ print() $ print(sorted(list(troll_users.columns)))
n_rows = ab_data.shape[0] $ n_rows
rank_meters['afternoon_12_17'].most_common(11)[1:]
json.dumps(aoi)
archive.rating_numerator.value_counts().sort_values()
len(df['user_id'].unique())
tweets_unique = tweets_unique.drop_duplicates(subset='id') $ tweets_unique = tweets_unique.sort('id')
missings.sum()
df.iloc[1]
print(X_train.shape) $ print(y_train.shape) $ print(X_test.shape) $ print(y_test.shape)
df_twitter_archive.info()
import seaborn as sns $ sns.set() $ sns.set_style('dark')
final = final.loc[final['candidateid']!=61285] $ final['cv_max_tenure'] = final['cv_max_tenure'].astype('timedelta64[s]')/86400 $ final['willingtoRelocate'] = final['willingtoRelocate'].fillna(0) $ final['willingtoRelocate'] = final['willingtoRelocate'].astype(int)
p(sys.getfilesystemencoding)
xgb = XGBRegressor(gamma=0, learning_rate=0.02, max_depth=3, n_estimators=100)
df_old=df2.query("landing_page=='old_page'") $ n_old=len(df_old) $ n_old
print dfNYC.columns $ print len(dfNYC.columns)
print(autos.groupby('seller').size())
pts_df.stack()  # back to a series
logit_mod2 = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'CA', 'UK']]) $ results2 = logit_mod2.fit() $ results2.summary()
datetime(1, 1, 1).toordinal()
d1 = pd.DataFrame([['A1', 'B1'],['A2', 'B2']], columns=['A', 'B']) $ d2 = pd.DataFrame([['C3', 'D3'],['C4', 'D4']], columns=['A', 'B']) $ d3 = pd.DataFrame([['B1', 'C1'],['B2', 'C2']], columns=['B', 'C']) $ pd.concat([d1, d2])
pd.datetime.strptime('2017-03-31', '%Y-%m-%d')
pd.Period('2017-01')
for res_key, df in entso_e.items(): $     entso_e[res_key], nan_tables[res_key + ' ENTSO-E'] = find_nan( $         df, res_key, headers, patch=True)
grad_age_mean = records3[records3['Graduated'] == 'Yes']['Age'].mean() $ grad_age_mean
plt.bar(np.arange(1), [temp_ranges[2]], yerr=(temp_ranges[1]-temp_ranges[0]),xerr=None,color=['orange']) $ plt.xticks([]) $ plt.ylabel("Temp (F)") $ plt.title("Trip Avg Temp") $ plt.show()
autos["odometer"] = autos["odometer"].astype("int") $ autos = autos.rename(index=str, columns={"odometer": "odometer_km"})
stocks.drop(['Week','Month'],axis=1,inplace=True)   # Dropping columns - new week column to be engineered
users = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/users.csv' ) $ sessions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv' ) $ products = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/products.csv' ) $ transactions = pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/transactions.csv') 
from scipy.stats import norm $ print("The significance of our z-score is: {}".format(norm.cdf(z_score))) $ print("Our critical value at 95% confidence is: {}".format(norm.ppf(1-(0.05))))
df['Trip_duration'].dt.components
old_n = df2[df2['group'] == 'control'].shape[0] $ old_n
seventh.to_csv('data/word_rates.csv')
ufos_df.toDF().registerTempTable("ufo_withyear")
difference_locations = np.where(resuld_df != checkDF) $ changed_from = result_df.values[difference_locations] $ changed_to = checkDF.values[difference_locations] $ pd.DataFrame({'from': changed_from, 'to': changed_to})
count_new_p = df2[df2.landing_page == "new_page"].count()[0] $ count_users = df2.user_id.nunique() $ print("P(new_page_onload) = %.4f" %(count_new_p/count_users))
df.to_csv("clean_df.csv")
top_10_complaint_types = data_2017_subset.complaint_type.value_counts().nlargest(10) $ top_10_complaint_types
stocks_happiness_rev=stocks_happiness.replace('',float('nan'))
pd.set_option('max_columns',None)
df1.rating_denominator.value_counts()
print(autos["brand"].value_counts().head(10))
sum(1 for tweet in collect.get_iterator())
Lab7_Redesign.tail()
df_image_tweet2.head()
df2 = df.drop(df[(df.user_id == 773192) & (df.timestamp == '2017-01-14 02:55:59.590927')].index, inplace=False) $ df2.loc[df['user_id'] == 773192]
revenue.map(lambda x: '%.2f' % x) # insert 2 decimal places for each element in the Series 
from datetime import datetime $ date = nc.num2date(time, 'hours since 1800-01-01 00:00:0.0') $ ts = pd.Series(date, index = date) $ print(ts.head(), ts.tail())
np.linspace(0,2,9)
model.evaluate_word_pairs(test_data_dir + 'wordsim353.tsv')
p_new_old_diff = new_page_converted.mean() - old_page_converted.mean() $ p_new_old_diff
all_words = [] $ for words in X_train.Text_Tokenized: $     all_words += words $ all_words = nltk.FreqDist(all_words)
df_prep17 = df_prep(df17) $ df_prep17_ = pd.DataFrame({'date':df_prep17.index, 'values':df_prep17.values}, index=pd.to_datetime(df_prep17.index))
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ r = req.get(url, allow_redirects=True) $ open('image-predictions.tsv', 'wb').write(r.content)
df_date_vs_count.plot(x='created_date', y= "count",kind='line') $ plt.show()
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK','new_CA','new_UK']]) $ results=lm.fit() $ results.summary()
num = it_df.isnull().sum() $ den = len(it_df) $ a = round(num/den, 2) $ a.sort_values()
np.array(p_diffs).mean()
df_mean_time_diff = mean_time_diff.to_frame() $ df_mean_time_diff = df_mean_time_diff.reset_index(drop=False) $ df_mean_time_diff.rename(columns={0: 'Frequency of Donations'}, inplace=True) $ df_survival = df_survival.merge(df_mean_time_diff, how='inner', on= 'Donor ID')
s_n_s_df.columns
np.exp(output.params)
grouped_ms = ms_df.groupby("user_screen_name") $ grouped_ms.size()
len(full_df.listing_id.unique())
weblogs_clean = weblogs.withColumn("time_clean", expr(r"from_unixtime(UNIX_TIMESTAMP(TIME, '[dd/MMM/yyyy:HH:mm:ss Z]'))")) $ weblogs_clean.select("time", "time_clean").show(10, False)
elec = transition_rate(0, 0.2, default_start=2016, default_end=2025) $ heat = transition_rate(0, 0.1, default_start=2016, default_end=2025) $ cool = transition_rate(0, -0.2, default_start=2016, default_end=2025) $ pr   = transition_rate(0, 0.3, default_start=2016, default_end=2025)
dac = np.vstack(df_all.date_account_created.astype(str).apply(lambda x: list(map(int, x.split('-')))).values) $ df_all['dac_year'] = dac[:,0] $ df_all['dac_month'] = dac[:,1] $ df_all['dac_day'] = dac[:,2] $ df_all = df_all.drop(['date_account_created'], axis=1)
dftop2 = dftop.groupby(['complaint_type']).size().reset_index(name = 'days_top_complaint')
new_page_converted = np.random.binomial(1, p_new, n_new) $ sum(new_page_converted)
df["tweet_source"].value_counts().head(10)
df.groupby('raw_character_text')['episode_id'].nunique().reset_index().head()
df_final_edited['rating_denominator'].value_counts() $
def print_tweet(tweet): $     print "@%s - %s (%s)" % (tweet.user.screen_name, tweet.user.name, tweet.created_at) $     print tweet.text $ tweet=results[1] $ print_tweet(tweet)
control_df=df.query('group=="control"') $ uuc_old=control_df.query('landing_page=="old_page"').user_id.nunique() $ uuc_new=control_df.query('landing_page=="new_page"').user_id.nunique() $ print("Number of unique users in the control group landed in old page is :{}".format(uuc_old)) $ print("Number of unique users in the control group landed in new page is :{}".format(uuc_new))
formula = "log_time_detained ~ imm_hold + charge_count + male + black + hispanic + amer_indian_alaskan + asian_pacific_island" $ reg = smf.ols(formula = formula, data = data).fit() $ reg.summary()
complaints2016 = all_complaints[ $     all_complaints['Created Date'].dt.year == 2016 $ ]
import datetime $ print(datetime.datetime.now())
inst_order = dfm.sort_values().index
pd.Series({'square of 1':1, 'square of 2':4, 'square of 3':9, 'square of 4':16, 'square of 5':25})
df2.query("group == 'control'").count()['group']
ozzy.age = 3
print('Groceries has shape:', groceries.shape) $ print('Groceries has dimension:', groceries.ndim) $ print('Groceries has a total of', groceries.size, 'elements')
n = pd.read_sql_query(QUERY, conn) $ n
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger')
p_diffs = np.array(p_diffs) $ (actual_diffs < p_diffs).mean()
%%bash $ cd /data/LNG/Hirotaka/ASYN $ cut -f 1,4-6 PPMI1_all | sed 's/_/\t/g' | LANG=C sort |\ $     LANG=C join -t$'\t' - maf001rsq3/PPMImaf05rsq8sorted.info > PPMI1_all_maf05rsq8
StockData.columns
usage_400hz = hc.table('asm_wspace.usage_400hz_2017_Q4') $
df.rename(columns={'Indicator':'Indicator_id'},inplace=True) $ df.head(2)
df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('MA/Manchester', 'NH') $ df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('DC/Hagerstown', 'VA') $ df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('Ohio', 'OH') $ df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('Iowa', 'IA') $ df_ad_airings_5['state'] = df_ad_airings_5['state'].str.replace('CA', 'National')
slp_timeseries = active_companies[active_companies['company_type'] == 'Limited Partnership for Scotland'].set_index('incorporation_date_formatted').copy() $ temp_timeseries = slp_timeseries.resample('Q').count()['CompanyNumber'] $ temp_timeseries['2009':'2017'].to_csv('data/viz/slp_timeseries.csv') $ temp_timeseries.plot(figsize=(20,10),xlim=('2009-01-01','2018-01-01')) $ print('Drop from peak SLP incorproation to last quarter of 2015: '  + str(1 - (temp_timeseries['2017-12-31'] / temp_timeseries['2015-12-31'])))
imax = amount.idxmax()
dedup = raw.copy()
acc.get_monthly_balance('2016-11')
output = pd.DataFrame(data={"id":test.id, "rating":predictions}) $ output.to_csv( "new_naive.csv", index=False, quoting=3 ) $
X_test.shape
actual_payments.pivot(index='fk_loan',columns='iso_date',values='bucket')
df1.dropna(how='any')
well_data.drop(columns=['api number', 'water_bbl'], inplace=True) $ print(well_data.head())
train["comment_text"].fillna("fillna") $ test["comment_text"].fillna("fillna") $ X_train = train["comment_text"].str.lower() $ y_train = train[["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]].values $ X_test = test["comment_text"].str.lower()
specific_hrefs = tree.xpath('//a[@href="http://www.non-existing-domain.org/"]') $ specific_hrefs
chilly = weather_mean['Temp (deg C)'] < 10 $ weather_mean.loc[chilly, ['Pressure (kPa)', 'Temp (deg C)']]
engine.execute('SELECT * FROM measurements LIMIT 15').fetchall() $
print(reviews_recent20.shape) $ reviews_recent20.head()
costs = 0.1 $ df_customers['profits'] = (df_customers['price']-costs)*df_customers['number of customers']
val logs = ssc.socketTextStream(hostname, port)
z_score, p_value = sm.stats.proportions_ztest(np.array([convert_new,convert_old]),np.array([n_new,n_old]), alternative = 'larger') $ print(z_score, p_value)
mask = (youthUser3["creationDate"] > '2018-01-01') & (youthUser3["creationDate"]<= '2018-12-31') $ youthUser2018 = (youthUser3.loc[mask]) $ youthUser2018.head()
autos["odometer"] = (autos["odometer"].str.replace("km", "") $                                      .str.replace(",", "") $                                      .astype(int)) $ autos.rename({"odometer": "odometer_km"}, axis=1, inplace = True) $ autos["odometer_km"].head()
df.resample('M').mean()
result_set =session.query(Adultdb).filter(Adultdb.education.in_(['Masters', '11th'])).all()
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"C":["C1", "C2"], $                     "D":["D1","D2"]},index=[1,2]) $ pd.concat([df1,df2], axis="col")
to_be_predicted_Day3 = 48.64887236 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
df_prep2 = df_prep(df2) $ df_prep2_ = pd.DataFrame({'date':df_prep2.index, 'values':df_prep2.values}, index=pd.to_datetime(df_prep2.index))
top_supporters.head()
income_T = income.transpose() $ income_T.head()
autos = autos.drop(["num_photos", "seller", "offer_type"], axis=1)
pc = pd.DataFrame(tt1.groupby('BORO').size()) $ pc.columns=['count'] $ pc_order = pc.sort('count', ascending = False) $
frame2.values[0].dtype
house = house.rename(index=str, columns={"TLO_id_x": "TLO_id"}) $ house = house.drop(['TLO_id_y'], axis=1)
grpConfidence = df.groupby(['Confidence'])
shows['fixed_runtime'].value_counts()
Top15=answer_one() $ Top15['Estimated population'] = Top15['Energy Supply'] / Top15['Energy Supply per Capita'] $ Top15['Estimated citable docs per person'] = Top15['Citable documents'] / Top15['Estimated population'] $ Top15['Estimated citable docs per person'].astype('float64').corr(Top15['Energy Supply per Capita'].astype('float64'))
newdf = cumret1.join(df3, how = 'inner')
sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'CA_ab_page', 'UK', 'UK_ab_page']]).fit().summary()
subwaydf['4HR_Entries'].idxmax()
df[["FINE", "FORFEITURE_DISGORGEMENT"]].head(2)
groceries = pd.Series(data = [30, 6, 'Yes', 'No'], index = ['eggs', 'apples', 'milk', 'bread'])
g8_groups = G8.groupby('continent')
print (ser.index) $ print (ser.size) $ print (ser.dtype) $ print (ser.values)
html = requests.get(url)
df_user[df_user['user.name'] == 'Marco Rubio']
Base = automap_base() $ Base.prepare(engine, reflect=True)
import pandas $ import pyodbc $ [x for x in pyodbc.drivers() if x.startswith('Microsoft Access Driver')]
atloc_opp_loc_tabledata = atloc_opp_loc_count_prop_byloc.reset_index() $ create_study_table(atloc_opp_loc_tabledata, 'locationType', 'remappedResponses', $                    location_remapping, atloc_response_list)
pres_df['day_of_week'] = pres_df['start_time'].map(lambda x: x.strftime("%A")) $ pres_df.head()
hours['AppointmentDate'] = pd.to_datetime(hours.index,format='%Y-%m-%d')
from sklearn.preprocessing import StandardScaler $
nba_df.index
tags = sentiments_df['User'].unique() $ print(tags) $ tweet_count = sentiments_df.groupby('User').Text.agg(['count']) $ print(tweet_count) $ print(type(tweet_count))
(df2.query('group == "treatment"')['converted']==1).mean()
df['user_id'].nunique()
tweets=api.GetUserTimeline('826036134', include_rts=False, count=200) $ print(tweets[0]) $ print(len(tweets))
matplotlib.pyplot.hist(y_pred)
gender_counts = nobel['Sex'].value_counts() $ female_pct = round(gender_counts['Female'] * 100.0 / len(nobel), 2) $ "{}% of winners are female".format(female_pct)
df3 = df2.fillna(30) $ df3.shape
for dataset in combine: $     dataset['Embarked'] = dataset['Embarked'].fillna(freq_port) $ train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)
client = pymongo.MongoClient() $ tweets = client['twitter']['tweets'].find() $ latest_tweet = tweets[200]
lagged = minute_return.tshift(1, 'min').between_time('9:30', '16:00') $ lagged.at_time('9:30')
groups_topics_df.shape, groups_topics_unique_df.shape
list1= np.arange(1,10) $ list2= np.arange(5,15)
csvData[csvData['street'].str.match('.*Court.*')]['street']
SCN_BDAY.scn_age.describe()
data.to_csv('time_series_manu.csv')
ax=nypd.groupby(by=nypd.index.week).count().plot(y='Unique Key', label='NYPD') $ dot.groupby(by=dot.index.week).count().plot(y='Unique Key', ax=ax, label='DOT') $ dpr.groupby(by=dpr.index.week).count().plot(y='Unique Key',ax=ax, label='DPR') $ hpd.groupby(by=hpd.index.week).count().plot(y='Unique Key', ax=ax, label='HPD') $ dohmh.groupby(by=dohmh.index.week).count().plot(y='Unique Key', ax=ax, label='DOHMH')
df_movies.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/movies.csv', sep=',', encoding='utf-8', header=True)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
wb.search('income').head()
print hashlib.md5("\\N").hexdigest()
df2_control = df2.query('group == "control"')
nobel = pd.read_excel('data/nobel_prizes.xlsx', sheetname='nobel_prizes') $ population = pd.read_excel('data/nobel_prizes.xlsx', sheetname='population')
cfModel.compile(loss='mse', optimizer='adam')
vbm['LOA DATE'] = vbm['LOA DATE'].values.astype('float') $ X, y = vbm.iloc[:,0].values, vbm.iloc[:,1].values $ X = X.reshape(-1,1) $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21)
df = pd.merge(df, df_cen, on=['CaseID'], how='left') $ df.sample(5)
class_merged_hol=pd.merge(class_merged_hol,regional_hol,on=['date','state'],how='left') $ print("Rows and columns:",class_merged_hol.shape) $ pd.DataFrame.head(class_merged_hol)
from bmtk.analyzer import plot_potential, plot_calcium $ plot_potential(cell_vars_h5='output/cell_vars.h5') $ plot_calcium(cell_vars_h5='output/cell_vars.h5')
fitted.score(X_test, y_test)
content_wed11 = pd.merge(wed11, mainstream, how='inner', $                          left_on='dimensions_item_id', $                          right_on='id')
for e in sumToRG['deposit'].iteritems(): $     (f, t), v = e $     dot.edge(t, f, taillabel=str(v))
df_clean.source = df_clean.source.str.extract(pat = pattern)
clfgtb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train, y_train) $ clfgtb.score(x_test, y_test)
stat, p, med, tbl = scipy.stats.median_test(df2["tripduration"], df3["tripduration"]) $ print p $ print tbl
high_idx = afx['dataset']['column_names'].index('High') $ low_idx = afx['dataset']['column_names'].index('Low') $ change_values = [entry[high_idx] - entry[low_idx] for entry in afx['dataset']['data'] if entry[high_idx] and entry[low_idx]]
pnew =  df[df.landing_page == 'new_page']['converted'].mean() $ print("Probablity of conversion for new page = ",pnew) $ pold = df[df.landing_page == 'old_page']['converted'].mean() $ print("Probablity of conversion for old page = ",pold)
X_cols_stacked = [col for col in df_train.columns if '_past_' in col] $ X_cols_caldata = [col for col in df_train.columns if 'weekday_' in col or 'month_' in col or 'year' in col] $ X_cols = X_cols_stacked + X_cols_caldata $ X = df_train[X_cols]
df2['active_bin'] = df2['days_active'].map(lambda x: 1 if x > 16.38 else 0)
all_tweets.iloc[0:10,:]
s_lyc_transcript_assemblies_dir = OUT_PATH + "/S_lyc_transcriptome_assembly" $ s_lyc_all_transcripts_concat = transcript_assemblies_dir + "/all_transcripts_concat.fasta"
loans_df.issue_d.value_counts()
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35, 'shirts': 15, 'shoes':8, 'suits':45}, $ {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5, 'shirts': 2, 'shoes':5, 'suits':7}, $ {'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4, 'shoes':10}] $ store_items = pd.DataFrame(items2, index = ['store 1', 'store 2', 'store 3'])
expiry = Series(expiry).sort_values()
df.groupby('group').mean()['converted']
df.iloc[0:4]
tx_sum = TX_profit['Profit Including Build Cost'].sum() $ buildings_tx = len(TX_profit)
s.data_filter.all_filters
comment_column = feedback.iloc[:, 0] $ comment_column = [x.replace("'", "") for x in comment_column] $ comment_column = [x.replace("-", "") for x in comment_column]
fig = plt.figure(figsize=(10,4)) $ ax = fig.add_subplot(111) $ ax = resid_6203.plot(ax=ax);
df2.head(2)
import glob $ import pandas as pd $ pattern = 'uber?.csv' $ csv_files = glob.glob(pattern) $ print(csv_files)
y.value_counts()
np.random.seed(42) $ train_idx = np.random.permutation(len(train_texts)) $ val_idx = np.random.permutation(len(val_texts))
access_logs_df = access_logs_parsed.toDF() $ access_logs_df.cache()
df.groupby(['group', 'landing_page']).count() $ control_newpage = df.query('group == "control" & landing_page == "new_page"').count()[0] $ treatment_oldpage = df.query('group == "treatment" & landing_page == "old_page"').count()[0] $ control_newpage + treatment_oldpage $
bow_corpus  = [dictionary.doc2bow(text) for text in list(repos)] $ index = SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12418)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger') $ z_score, p_value
df2.query('group == "control"').user_id.size #145274 147202
vow.head()
sum(twitter_df_clean.rating_numerator > 10)
k ['MONTH'] = k.index.map(dict)
df_final.sort_values(by='Pct_Passing_Overall', ascending=False).tail(5)
words_only_sk_freq = FreqDist(words_only_sk) $ print('The 100 most frequent terms (terms only): ', words_only_sk_freq.most_common(30))
df2['converted'].mean()
df_members.hist(figsize=(15,12),color = 'y') $ plt.show()
trump['est_time'] = ( $     trump['time'].dt.tz_localize("UTC") # Set initial timezone to UTC $                  .dt.tz_convert("EST") # Convert to Eastern Time $ ) $ trump.head()
nf.set_index(rng2, inplace = True) $ nf.head(5)
train['CompetitionDistance']= train['CompetitionDistance'].fillna(train['CompetitionDistance'].median()); $ test['CompetitionDistance']= test['CompetitionDistance'].fillna(test['CompetitionDistance'].median())
elms_all_0611.loc[range(1048575)].shape[0] + elms_all_0611.iloc[1048575:].shape[0]
index_of_non_unique_user_id = df2[df2['user_id']==duplicated_user_id].index.values.astype(int)[0] $ index_of_non_unique_user_id $ df2.drop(index_of_non_unique_user_id, inplace=True) $ df2.query("user_id == @duplicated_user_id") $ df2.shape[0] #test - this value has to be 290584, which is one less than (df2_size = 290585)
FileLink("dnn100_sub.csv")
words_sk = [term for term in all_tokens_sk if term not in stop and not term.startswith('http') and len(term)>2] $ corpus_tweets_streamed_keyword.append(('meaningful words', len(words_sk))) # update corpus comparison $ print('Total number of meaningful words (without stopwords and links): ', len(words_sk))
screendf.to_sql(con=engine, name='wellscreens', if_exists='replace', flavor='mysql',index=False)
calls_nocontact.ticket_status.value_counts()
chunker.tagger.classifier.show_most_informative_features(15)
df_columns.head() $
pp.pprint(metadata_dict)
cpi_sdmx.names
dfa=new_table.groupby("type")
Z = np.random.uniform(0,1,(10,10)) $ U, S, V = np.linalg.svd(Z) # Singular Value Decomposition $ rank = np.sum(S > 1e-10) $ print(rank)
df.groupby('domain').count().sort_values('id',ascending=False).head(25)
user_actions_with_profile = user_actions.join(user_profile)
model.fit(predictors, target)
 user_profiles = [ $      {'reinsurer_id': 228, 'reinsurer': 'Atlantic Re'}, $      {'reinsurer_id': 235, 'reinsurer': 'XL Re'}] $  result = db.profiles.insert_many(user_profiles)
df_clean.expanded_urls.sample().tolist()
tweets_clean.index
df.select('longitude').distinct().sort('longitude', ascending=True).show(10)
activeDF.take(10)
to_be_predicted_Day5 = 55.25233774 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
versiData = irisRDD.filter(lambda x: "versicolor" in x) $ versiData.count()
df.head()
stock_dict = {} $ for data in r.json()['dataset']['data']: $     stock_dict[data[0]] = dict(zip(r.json()['dataset']['column_names'][1:], data[1:]))
df2 = pd.read_csv('image-predictions.tsv', sep='\t')
files8.head()
ebay.head()
tweets.describe(include='all')
loans_df.home_ownership.value_counts()
nearest_id = lambda row: int(((pair.DATETIME - row).abs() / np.timedelta64(1, 's')).argsort()[:1])
ac['Dispute Resolution Start'].groupby([ac['Dispute Resolution Start'].dt.year]).agg('count')
CD.describe() $
from pysumma.utils import utils $ import os
df2.query('group == "treatment"') $ df2.query('group == "treatment" and converted == 1') $
pn_counter = {} $ for pn in pn_qty.iterkeys(): $     pn_counter[pn] = counter[pn]
from dfast.jupyterutils.togglecode import hideCode $ hideCode()
params = {'sender_address':'all'} $ response = requests.get( $   'https://panel.sendcloud.sc/api/v2/shipping_methods', $     auth=('key', 'secret_key'),params=params)
s.describe()
df.info() $ print ("no values are missing!")
pscore = pscore.reshape(-1, 1) $ btc_price = btc_price.reshape(-1, 1) $ print(pscore[:5]) $ print(btc_price[:5])
tfidfnmf_topics['TopicNum'] = tfidfnmf_topics.idxmax(axis=1) $ tfidfnmf_topics['TopicWt'] = tfidfnmf_topics.iloc[:, :15].max(axis=1) $ tfidfnmf_topics['PrimaryTopic'] = [topic_list[x] for x in tfidfnmf_topics['TopicNum']]
DD= df['domain'].value_counts().head(30).index.tolist() $ df['domain_d'] = [type_ if type_ in DD $                       else "OTHER" for type_ in df['domain']] $ df['domain_d'].value_counts()
ADMCPATIENTDRUGCLASSES_20170512 = pd.read_csv('data/ADMCPATIENTDRUGCLASSES_20170512.csv') $ print(len(ADMCPATIENTDRUGCLASSES_20170512[ADMCPATIENTDRUGCLASSES_20170512['Phase']=='ADNI1'])) $ print(len(ADMCPATIENTDRUGCLASSES_20170512[ADMCPATIENTDRUGCLASSES_20170512['Phase']=='ADNI2'])) $ print(len(ADMCPATIENTDRUGCLASSES_20170512[ADMCPATIENTDRUGCLASSES_20170512['Phase']=='ADNI3'])) $ print(len(ADMCPATIENTDRUGCLASSES_20170512[ADMCPATIENTDRUGCLASSES_20170512['Phase']=='ADNIGO']))
Distribution_df['Month'] = Distribution_df['Date_of_Order'].dt.month $ Distribution_df.head()
repos.loc[repos.forked_from == '\N', 'forked_from'] = '0'
aapl.loc["2008-03-28"]
hist(df4.tripduration, bins = 20, color="green", label = "Unknown", normed = 1) $ plt.xlabel("Trip Duration in seconds", fontsize=12) $ plt.ylabel("Amount of trips", fontsize=12) $ plt.title("Trip Duration Histogram unknown gender", weight='bold', fontsize=14) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled, $                       transpose_a=True, name="agreement")
words = lc_review.split(" ") $ words_no_stop = [w for w in words if w not in stopwords.words("english")]
people.index.is_unique
exiftool -csv -createdate -modifydate ciscid4/CISCID4_cycle2.mp4 ciscid4/CISCID4_cycle3.mp4 ciscid4/CISCID4_cycle4.mp4 ciscid4/CISCID4_cycle5.mp4 ciscid4/CISCID4_cycle6.mp4 > ciscid4.csv
dir(friends_n_followers) $ help(friends_n_followers.sort_values)
mgxs_lib.dump_to_file(filename='mgxs', directory='mgxs')
cursor1 $ df1 =  pd.DataFrame(list(cursor1)) $ df1
vectorizer = TfidfVectorizer(stop_words=stops, use_idf=True, ngram_range=(1,3), max_df=0.2) $ X = vectorizer.fit_transform(testParsedPD) $ X[0]
c0vm = df5['HT'].where(df5['new_id'] == 'c0vm').dropna() $ plt.hist(c0vm, bins=50)
x_train = scaler.transform(x_train) $ x_test = scaler.transform(x_test)
print("Approximatively {}  hours to run".format(10 * len(tweets)/3600) )
[(type(nd), nd.shape) for nd in read_in["ndarrays"]]
df_to_interp.interpolate()  # the index values aren't taken into account
earliest_date = min([x['created_at'] for x in list(all_posts.values())]) $ latest_date = max([x['created_at'] for x in list(all_posts.values())]) $ ranking_latest_date = max([x['created_at'] for x in list(all_posts.values())]) + datetime.timedelta(days=1) $
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['CA', 'US']]) $ results = log_mod.fit() $ results.summary()
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyzer = SentimentIntensityAnalyzer()
top_brands = autos["brand"].value_counts(normalize=True).head(20).index $ top_brands
nodes = nodereader.values.tolist() 
d9 = d8.unstack() $ d9
df.iloc[3,] # is this a row or column?
df_predictions_clean.img_num.value_counts()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df.groupby('episode_id').id.nunique().head()
SCN_BDAY_qthis = SCN_BDAY.loc[list(set(SCN_BDAY.index) & set(BDAY_PAIR_qthis.index))]
state_party_df.fillna(0, inplace=True) $ state_party_df.head(20)
from sklearn import svm $ SVM_model = svm.SVC() $ SVM_model.fit(X_train, y_train) 
df.info()
df_archive_clean = df_archive_clean.replace("None",np.NaN)
game_info = create_game_info_df("data/weekly_game_info.csv")
data["is_video"] = data["is_video"].astype(str)
df2.query("user_id=='773192'")
sentiment_df = pd.read_csv("sentiment.csv")
scoring_input_data['DATE'] = pd.to_datetime(scoring_input_data['DATE']) $ scoring_input_data['ID'] = scoring_input_data['ID'].astype('category') $ scoring_input_data.dtypes
scenarios_rdd.takeSample(False, 1,0)
aggreg1=turnaround_planes_df.groupby(['dep_ramp']) $ aggreg1=aggreg1.aggregate({'acreg':{'Count' : 'count'}}) $ aggreg1.columns=aggreg1.columns.droplevel() $ aggreg1.reset_index(inplace=True) $ aggreg1=aggreg1[aggreg1['Count']>1]
save_n_load_df(joined, 'joined_items.pkl')
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     return tweepy.API(auth)
X.head()
print(data.shape) $ data = data.groupby(['cust_id','order_date']).head(1) $ print (data.shape)
importlib.reload(util)
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" and converted == 1').count()[0] $ convert_new = df2.query('landing_page == "new_page" and converted == 1').count()[0] $ n_old = df2.query('group == "control"').count()[0] $ n_new = df2.query('group == "treatment"').count()[0]
weather_data['zip'].unique()
len(df.columns)
intersections_irr['CO2_grams'] = [get_CO2_emission_per_segment(row['speed'],row['estimated_number_vehicles'],row['SHAPE_Leng']) for index,row in intersections_irr.iterrows()]
df_trips.head(5)
len(df2.query('landing_page=="new_page"'))/len(df2.index)
df.describe(include=[np.number])
archive_copy['full_text'][archive_copy['full_text'].str.contains('&amp;')]
tweet = result[0] #Get the first tweet in the result $ for param in dir(tweet): $     if not param.startswith("_"): $         print "%s : %s\n" % (param, eval('tweet.'+param)) $
all_cards = pd.concat(list(only_cards)) $ all_cards.sample(10)
stamp=datetime(2020,10,1) $ stamp.strftime('%Y-%m-%d')
a[a.find(':') + 1:].lstrip()
df1.sample(5)
pd.Series(['a','b','c','d','e'])           # from Python list
scoring_url = client.deployments.get_scoring_url(deployment_details) $ print(scoring_url)
combined['total_avg_duration'] = combined['avg_durationexportID'] + combined['avg_durationexportOI'] $ combined['total_avg_distance'] = combined['avg_distanceexportID'] + combined['avg_distanceexportOI'] $ combined['total_load'] = (combined['event.hyperUnique_unique_agentsexportOI'] + combined['event.hyperUnique_unique_agentsexportID'])/2
training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=42)
new_page_converted = np.random.choice([0,1],n_new, p=(p_new,1-p_new))
from scipy.stats import norm $ norm.cdf(z_score)  # significance of our z_score
df = pd.read_html(str(table[1]), header=0, encoding='utf8')[0] $ df.head()
results.summary()
weather_data3 = pd.read_csv('201508_weather_data.csv'); weather_data3.head()
print('The number of unique genera in 200 km circle around the Petropavlovsk-Kamchatsky city:', len(petropavlovsk_filtered.genus.unique()))
tokens.head(10)
data.tail(10)
x = df2[df2['group']=='treatment']['converted'].mean() $ print("{:.2%}".format(x))
img = mpimg.imread('input/c(DE-DK-NO-SE).png')
df.index
from keras.models import load_model
pd.merge(msftAR0_5, msftVR2_4, how='outer')
df.index.weekday_name
v_invoice_hub.shape
frame['e'].map(format)
from sklearn.pipeline import make_pipeline $ poly_model = make_pipeline(PolynomialFeatures(7), $                            LinearRegression())
df2['tripduration'].describe()
station_count= session.query(Measurement.station ).group_by(Measurement.station ).count() $ print(f"There are {station_count} available in dataset")
YS1517.plot(subplots = True, figsize=(10,10))
validation.analysis(observation_data, Simple_resistance_simulation)
tweet_data.retweet_count.hist(bins=70)
r,q,p = sm.tsa.acf(resid_6201.values.squeeze(), qstat=True) $ data = np.c_[range(1,41), r[1:], q, p] $ table = pd.DataFrame(data, columns=['lag', "AC", "Q", "Prob(>Q)"]) $ print(table.set_index('lag'))
twitter_archive_clean.drop(dog_stage_list, axis=1, inplace=True)
df_hour.groupby("hour").sum().sort_values(by="postcount",ascending=False)['postcount'][:10]
print (np.sqrt(-1)) $ print(np.sqrt(-1) == np.emath.sqrt(-1))
referee_1_groups = nba_df.groupby(["Referee1", "Referee2", "Referee3"]) $ referee_1_groups.size().sort_values(ascending = False).head(30)
engine = create_engine("sqlite:///hawaii.sqlite")
subs = df.groupby(['subreddit'])['subreddit_subscribers'].max()
tweet_archive_clean['url'] = tweet_archive_clean.text.str.extract('(?P<url>https://.*/+[a-zA-Z0-9]+)', expand=True)
a = t[(t['group'] == 'treatment') & (t['landing_page'] == 'old_page')] $ b = t[(t['group'] == 'control') & (t['landing_page'] == 'new_page')] $ c=len(a)+len(b) $ c_t = pd.concat([a, b]) $ c
dict_category = json.loads(df.loc[row,'category']) $ pprint(dict_category)
daily_ret_b = calc_daily_ret(closes) $ daily_ret_b.loc[:,'BOND'] = 0.015/252 $ daily_ret_b
rf = RandomForestClassifier(n_estimators=500,n_jobs=-1) $ rf.fit(X_train,y_train) $ rf_pred = rf.predict(X_test)
ids = df2['user_id'] $ df2[ids.isin(ids[ids.duplicated()])]
df_train['moy_date'] = df_train['date'].dt.month_name() $ df_test['moy_date'] = df_train['date'].dt.month_name() $
df_countries = pd.read_csv("countries.csv") $ df_countries.head()
list_of_genre_1990s_1 = list() $ for x in list_of_genre_1990s[0]: $     list_of_genre_1990s_1 = x.strip('').split('\n')
from sklearn.model_selection import cross_val_score $ scores = cross_val_score(fitted, X, y, cv=5)
autos["abtest"].value_counts()
np.power(b, 2)  # Raise every element to second power
df2 = df[['sentiment', 'state']] $ df3 = df2.pivot_table(index='state', aggfunc='mean') $ df3 = df3.drop(['', 'Burger King', 'IHOP', 'United States', 'Yogurtland']) $ pd.set_option("display.max_rows", 500) $ df3
df_test_user_2 = df_test_user.copy() $ df_test_user_2['created_on'] = '2017-09-20 00:00:00'
nread_read=horror_readings.reset_index(name='n.readings/reader')
gbm_regressor.train(x=list(range(1,iris_df.ncol)), y=0, training_frame=iris_df)
ordered_df = USER_PLANS_df.iloc[ordered_timelines]
sum(~twitter_archive['retweeted_status_id'].isnull())
test_id = np.array(test_clean["ID"])[:,None] $ print test_id[:10] $
!pip install patsy $ from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt $ import matplotlib.pyplot as plt $ import pandas as pd
law_adds_file = "LAW-adds.txt" $ law_deletes_file = "LAW-deletes.txt"
col=col[['ZIP CODE','LONGITUDE','LATITUDE','NUMBER OF PERSONS INJURED','date_time']] $ col.head()
train_holiday = train_test_set.join(holidays_events, 'date', 'left_outer') $ train_holiday.show(2)
def get_tweets_with_cache(user_account_name, keys_path): $     ...
building_pa_prc_zip_loc.sample(20)
id_most_act_station = act_stations[0][0] $ data_most_act_temp = session.query(func.min(Measurement.tobs),func.max(Measurement.tobs),func.avg(Measurement.tobs)).\ $                 filter(Measurement.station == id_most_act_station).\ $                 filter(Measurement.date > another_1_year_ago).all() $ data_most_act_temp
list_of_secretly_controlled_companies = secret_corporate_pscs.company_number.unique().tolist()
classification_data.groupby('primary_role').size()
pax_sample[pax_sample.paxday==1]\ $     .set_index('minute_of_day')\ $     .paxstep.rolling(window=50, win_type='triang')\ $     .mean()\ $     .plot(figsize=(25,8)) $
country_dummies = pd.get_dummies(df['country']) $ currency_dummies = pd.get_dummies(df['currency']) $ delivery_dummy = pd.get_dummies(df['delivery_method'])
!cp ../submissions/nvsvm.csv ../../drive/ColabNotebooks/AV_innoplexus_html
address_df['nndr_prop_ref']=address_df['nndr_prop_ref'].apply(str) $ address_df['nndr_prop_ref'].head(5)
df2 = cgc2.as_dataframe() $ df2[['Match','Exact']].describe()
data['Sales'].dtypes
df2 = df1.query("(landing_page == 'new_page' and group == 'treatment') or \ $                  (landing_page == 'old_page' and group == 'control')")
for post in posts.find(): $     print(post)
LUM.plot_diff(all_lum)
wrd_clean['rating_numerator'] = wrd_clean['rating_numerator'].astype('str').apply(lambda x: x.split('/10')[0]).astype('float') $ wrd_clean['rating_denominator'] = wrd_clean['rating_denominator'].astype('str').apply(lambda x: x if x == 'nan' else x.split('/')[1]).astype('float')
rng = pd.date_range('4/23/2018', periods=100, freq='S') # range of 100 seconds $ ts = pd.Series(np.arange(len(rng)), index = rng) # fill in values of 0 - 100 $ ts.head()
df_train.head()
df.describe()
submission_full.describe(percentiles=[.1, .2,.3, .4,.5])
date_ny.head()
len(slps.CompanyNumber.unique())
movies['scoreRank'] = movies.score.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False) $ movies['grossRank'] = movies.gross.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)
df_daily[df_daily["ENTRIES"] < df_daily["PREV_ENTRIES"]].head()
cp = c1.sum()/c1.index.size $ cp
get_items_purchased('alpa.poddar@gmail.com', product_train, customers_arr, products_arr, item_lookup)
df_cal['start_date'].dtypes
calls_df["phone number"].nunique()-840
stations = session.query(Measurement).group_by(Measurement.station).count() $ print(stations)
MostHourlyExits.to_csv('Largest_hourly_exits.csv')
options_frame = pandas.read_pickle('options_frame.pickle')
dframe_team['Draft_year'] = (dframe_team['start_year']+1)[(dframe_team['Start'] >= dframe_team['start_cut'])] # This gives the GM's first year of the draft as the year after he started if he started after July 1 of the previous year $ dframe_team['Draft_year'] = dframe_team['Draft_year'].fillna(dframe_team['start_year']) $ dframe_team.head()
df_link_yt = df_yt.merge(df_yt_resolved, left_on = 'yt_id', right_on='video_id', how='left')
df_day.info()
print("P-CG-converting:", $       df2[df2['group']=='control']['converted'].mean())
treatment_converted=df2.query('group=="treatment"').converted.mean() $ treatment_converted
tips_analysisDF.head()
df['created_at'] = pd.to_datetime(df['created_at'])
fire_size = pd.read_csv('../data/model_data/1979-2016_fire_size.csv', index_col=0) $ fire_size.dropna(inplace=True) $
type(series.values)
events.to_feather("./data/swimming-records.feather")
htest.head()
wrd_clean.info()
results_ab_country.summary()
labels = tf.one_hot(targets, len(vocab)) $ loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels) $ loss = tf.reduce_mean(loss) $ train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)
df2 = df[((df.group == 'treatment') & (df.landing_page == 'new_page')) | ((df.group == 'control') & (df.landing_page == 'old_page'))]
strategy.df_orders().head()
df1, df2, df3, df4, df5 = (pd.DataFrame(rng.randint(0, 1000, (100, 3))) for i in range(5))
df_new['country'].unique()
first = alerts[0] $ print(first.find('time').get_text()) $ print(first.a.find('span').get_text()) $ print(first.a['href'])
len(master_df[['name', 'tweet_id']].groupby('name').filter(lambda x: len(x)==1))
plt.hist(p_diffs); $ plt.title('Simulated Difference of New Page and Old Page Converted Under the Null'); $ plt.xlabel('Page difference'); $ plt.ylabel('Frequency');
learn.save('multi-label')
cars.head() $
g = df_A.groupby(df_A.index.str.len()) $ g.filter(lambda x: len(x) > 1)
feed_ids = feeds[feeds['publication_id'] == pubs[pubs['name'] == 'Fox News'].id.values[0]]['id'] $ print(feed_ids) # these can be used as indices for feeds_items
prob_conv_cont = ((pop_cont_con / pop_con)*0.5) / 0.5 $ prob_conv_cont
df_city_reviews.limit(5).toPandas()
repos.info() $ repos.head()
con_df = pd.read_csv('.\countries.csv') $ con_df.head()
from scipy.stats import norm $ norm.cdf(z_score) $ norm.ppf(1-(0.05/2)) $
df_total = pd.concat([df_no_cat, df_dummies], axis=1)
df_dummies = pd.get_dummies(df_countries,columns=['country']) $ df4 = df3.merge(df_dummies,on='user_id') $ df4.head()
import numpy as np $ print 'Originally there were ', len(df.columns) $ _, i = np.unique(df.columns, return_index=True) $ print 'Now there are ', len(np.unique(df.columns)), ' columns' $
ts_df = site_series_values_to_df(site_values, variable_name) $ ts_df.tail()
first_result.contents[2]
dt_features['created_at'] = pd.to_datetime(dt_features['created_at'],unit='s')
(df_final[df_final['R'] == 10]).head()
nold = df2[df2['group'] == 'control'].shape[0] $ print(nold)
end_time = dt.datetime.now() $ (end_time - start_time).total_seconds()
txns[txns['token']=='MKR'].describe()
learn.data.test_dl = test_dl $ log_preds_test = learn.predict(is_test=True);
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
y_test = test.loan_status
m.fit([X], Y, epochs=10, batch_size=16, validation_split=0.1) $
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=2500) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
import logging $ logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\ $     level=logging.INFO)
playlists = yt.get_playlists(channel_id, key) $ df_playlists = pd.DataFrame(playlists) $ df_playlists.head(2)
df2['intercept'] = 1 $ df2[['ab_page', 'drop_me']] = pd.get_dummies(df2['landing_page']) $ df2.head()
for df in list_to_change: $     df.columns = ["Compaction"] $ addicks.head() # to see the final product
analyze_set['rating_numerator'].value_counts()
y_test.values
df_tte_all[df_tte_all['UsageType'].str.contains('UGW')]['ItemDescription'].unique()
n = len(dataBPL.Total_Demand_KW) $ train = dataBPL.Total_Demand_KW[:int(.75*n)] $ test = dataBPL.Total_Demand_KW[int(.75*n):]
pd.set_option('display.max_colwidth', 100)
transactions.info()
n_old = df2[df2['group']=='control'].count()[0] $ n_old
croppedFrame.describe()
sqlc = sqlite3.connect('iris.db')
between_all_posts.describe()
dfRegMet.to_pickle("dfRegMet.p")
topic_words = [] $ for topic in clf.components_: $     word_idx = np.argsort(topic)[::-1][0:n_top_words] $     topic_words.append([tfidf_vocab[i] for i in word_idx])
with open('dropbox/github/Thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_continent-90m.csv', 'rU') as inputFile: $     for line in inputFile: $         print(line)
plt.hist(final_data.rating) $ plt.title('Starred, Owned, and Both') $ plt.ylabel("Count") $ plt.savefig('/home/ubuntu/PROJECT/github-collaborator/matplots/star_own_distribution.png');
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.getcwd() $
HC = [] $ for tweet in tw: $     HC.append(parse_tweets(tweet)) $
z_values, labels = create_latent(nn_aae.nn_enc, train_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ plt.scatter(z_values[:,0], z_values[:,1], s=1)
frame2.year
df_city_reviews.select('user_id','date','seq').orderBy('user_id','seq').limit(40).toPandas()
control_df2 = df2.query('group == "control"') $ control_conversion = control_df2['converted'].mean() $ control_conversion
evals_mean = evals.mean(axis=0)
autos[autos.price > 500000].loc[:,["name", "odometer_km","price" ]] $
df = pd.read_csv('data_test_2m.esc', sep='\3')
date.
tweets = pd.concat([tweets_rt, tweets_original])
df_piotroski_all = pd.concat(frames, ignore_index=True)
list_feature_names = list(feature_data['feature_names']) $ train_data = pd.read_csv('data/kaggle_data/train.csv', header=None, sep=" ", names=list_feature_names) $ train_data.head(5)
result = cur.fetchall() $
pd.merge(pd.concat([df_a, df_b]), df_c, on = "mathdad_id") # concatenate 2 mathdad df and then merge with skill level df
df2 = df2.drop(df2.index[df2.user_id.duplicated()==True].tolist(), axis=0)
import sys, os $ import pandas as pd $ import numpy as np $ import xml.etree.ElementTree as ET $ from datetime import datetime
RNPA_existing_data_plus_forecast.columns
s_mean_df["date"] = pd.to_datetime(s_mean_df["date"], format='%Y-%m-%d') $ s_mean_df.info()
(p_diffs > act_diff).mean()
X = tf.placeholder(tf.float32, shape=(x_data.size)) $ Y = tf.placeholder(tf.float32,shape=(y_data.size)) $ m = tf.Variable(3.0) $ c = tf.Variable(2.0) $ Ypred = tf.add(tf.multiply(X, m), c) $
merged1 = pd.merge(left=appointments, right=reason_for_visit, how='left', left_on='MeetingReasonForVisitId',\ $                   right_on='Id')
unsegmented_users = enrollments[['user_id','course_id','cohort_availability','username']].copy()
df.head()
autos['brand'].value_counts(normalize = True)
psy = pd.get_dummies(psy, columns = cat_vars, drop_first=False)
sakhalin_data_in_bbox.shape
ticker_price = get_adj_price('TSLA') $ ticker_price = ticker_price[ticker_price.index > start_date.strftime("%Y-%m-%d")] $ ticker_price = ticker_price[ticker_price.index < end_date.strftime("%Y-%m-%d")] $ ticker_price.head()
reddit_comments_data.orderBy('score').select('score').show(10)
df = pd.DataFrame() $ df
activeDF = get_active_on_date(filteredPingsDF_str, D0.isoformat()) $ activeDF_str = "activeDF" $ sqlContext.registerDataFrameAsTable(activeDF, activeDF_str)
txt.head()
ben = pd.merge(combined,users,how='left',on='username') $ ben.head(10)
filtered_tweets_df.to_csv("../../data/clean/filtered_tweets.csv",index=False)
test_features = vectorizer.transform(articles_test) $ clf.score(test_features, authors_test)
df_prep4 = df_prep(df4) $ df_prep4_ = pd.DataFrame({'date':df_prep4.index, 'values':df_prep4.values}, index=pd.to_datetime(df_prep4.index))
df2['intercept'] = 1 $ df2[['intercept','ab_page']]= pd.get_dummies(df2['group']) $ df2.head()
from numpy import median $ width, height = 12, 6 $ plt.figure(figsize=(width, height)) $ ax = sns.barplot(x="Memory", y="Wait", data=jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'cms')], estimator = median, ci= None) $ ax.set_yscale('log') $
print(spreadsheet_name) $ book = gcat.get_file(spreadsheet_name, fmt='pandas_excel') $ libraries = book.parse('Libraries', header=0) $ created = server.post_sheet('/libraries/', libraries, verbose=True, dry_run=True, validator=validator) $ print(len(created))
df.loc[:,'name':'year1']
train = pd.read_json('./data/train.json')
df[df.bottles_sold==2508]
forecast_col = 'Adj. Close' $ df.fillna(value=-99999, inplace=True) $ forecast_out = int(math.ceil(0.01 * len(df)))
x_final = sparse.hstack((scalingDF_sparse, categDF_encoded))
for k in ['linear','poly','rbf','sigmoid']: $     clf = svm.SVR(kernel=k) $     clf.fit(X_train, y_train) $     confidence = clf.score(X_test, y_test) $     print(k,confidence)
gene_df = sub_gene_df[sub_gene_df['type'] == 'gene'] $ gene_df = gene_df.copy() $ gene_df.sample(10).attributes.values
print(data["Product"].value_counts()) $ print(data["Product"][data["Restaurant"] == 8].value_counts()) $ print(data[" Quantity"].min()) $ print(data[" Quantity"].max()) $ print(data[" Quantity"].mean())
cityID = 'c3f37afa9efcf94b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Austin.append(tweet) 
prop.plot(kind='bar') $ plt.title('Is the Donor a Teacher?') $ plt.show()
station_analysis_df = tobs_df.rename(columns={0: "station", 1: "name", 2: "date", 3: "tobs"}) $ station_analysis_df.head()
terror.loc[1390]
df = austin.pivot_table(index='yr_mo', columns='weekday', values='started_on', aggfunc='count') $ df = df.reindex_axis(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'], axis=1) $ print(df)
confuse = pd.crosstab(gnbtest['failure'], gnbtest['predicted_failure']) $ confuse
print(df_A.loc['s1']) $ print(df_A.iloc[1]) $
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(lr.set_index('user_id'), how='inner')
retweet_pairs["FromType"] = "Person" $ retweet_pairs["ToType"] = "Person" $ retweet_pairs["Edge"] = "Retweet" $ retweet_pairs.rename(columns={"screen_name":"FromName","retweeted_screen_name":"ToName"},inplace=True) $ retweet_pairs.head()
"From {0} to {1}".format(dataset["PubDate"].min(), dataset["PubDate"].max())
def prepare(ticket): $     one = cleaning(ticket).split() $     two = dictionary.doc2bow(one) $     return two
df_estimates_false['points'].hist(bins=50, figsize=(10,5)) $ plt.show() $
state_party_df['National_R']['2016-08-01':'2016-08-07'].sum() / 7
stock_data.info()
systemuseData.groupby('Year')[['Total','Domestic','Industrial','Commercial']].sum().plot() $ plt.xlim(1980,2020) $ plt.ylabel('Use (ac-ft)')
df_twitter[df_twitter.p1 == "Saluki"]
shows['release_date'] = shows['release_date'].dropna().apply(lambda x: datetime.strptime(x, '%d %b %Y'))
btc_forum_df = pd.read_csv('btc_forum_cleaned_new_pos_neg_sub.csv') $ btc_forum_df.set_index('timestamp', inplace=True) $ btc_forum_df.index = pd.to_datetime(btc_forum_df.index)
goog.head()
destination = os.path.abspath(holding_file_name) $ os.chdir(destination) $ print('The current directory is ' + color.RED + color.BOLD + os.getcwd() + color.END) $ print('Number of files in',destination,':',len(os.listdir()))
estimator.fit(prepared_train, prepared_train.favs_lognorm);
df.as_matrix() # not a matrix, however, just numpy array
np.random.seed(500) $ x=np.linspace(1,10,100)+ np.random.uniform(low=0,high=.5,size=100) $ y=np.linspace(1,20,100)+ np.random.uniform(low=0,high=1,size=100) $ print ('x = ',x) $ print ('y= ',y)
autos.info()
print('There are {} unique Dxs in the dataset'.format(len(unique_Dx))) $ unique_Dx = [Dx for Dx in unique_Dx if len(Dx) == 3]
(df2['converted']).mean()
temp_frame = get_filtered_op_frame(OP_TYPE) $ temp_frame = temp_frame.set_index(temp_frame.timestamp) $ temp_frame = temp_frame[['timestamp','value']]
nmf_tfidf_topic6_sample = mf.random_sample(selfharmmm_final_df, criterion1='non_lda_max_topic', value1='nmf_tfidf_topic6', use_one_criterion=True)
print(data.columns) $ print(type(data)) $ data.describe()
oppose.sort_values("amount", ascending=False).head()
archive_clean.head(5)
history.head()
crimes['2011-06-15']['HOUR'].value_counts().head(5)
pd.DatetimeIndex(pivoted.columns).dayofweek
t_indices = [] $ for _, indices in test_indices.items(): $     t_indices = t_indices + indices
cancel_df = data_df[(data_df['delay_time'] == "Cancelled")].copy() $ cancel_df['is_cancel'] = cancel_df['delay_time'].apply(lambda x: x == "Cancelled") $ cancel_df = cancel_df.sort_values(['Departure', 'Arrival', 'Airline', 'flight_year', 'flight_month', 'flight_day', 'std_hour'],ascending=False)
cust, c = generate_labels('/YOByLTq4vE1SJWjpjPFnO+npiOGxK15Y8QbtcW+RBE=', trans, $                           label_type = 'SMS', churn_period = 30, return_cust = True) $ c[c['days_to_next_churn'] < 15] $ cust.iloc[:4, 6:]
sns.distplot(filter_iphone(orig_tweets[(orig_tweets['date'].dt.year==2017)&(orig_tweets['date'].dt.month==1)])['hour'], color='b') $ sns.distplot(filter_android(orig_tweets[(orig_tweets['date'].dt.year==2017)&(orig_tweets['date'].dt.month==1)])['hour'], color='g')
shows['plot'] = shows['plot'].dropna().apply(lambda x: x.lower()) $ shows['plot'] = shows['plot'].dropna().apply(remove_punctuation)
! aws s3 ls s3://flight.price.11/flight_1_5 --recursive $ ! aws s3 sync s3://flight.price.11/flight_1_5 .
import matplotlib.pyplot as plt $ %matplotlib inline $ labels.set_index('cutoff_time')['days_to_next_churn'].plot(); $ plt.axvline(x = (labels.loc[labels['churn'] == 1, 'cutoff_time']).values[0], color = 'r'); $ plt.ylabel('Days to Next Churn');
sum(pred.duplicated())
X_train.shape
X.reset_index(inplace=True, drop=True)
response = requests.get(CSV_URL) $ with open(os.path.join("./", $                       CSV_URL.split("/")[-1]), mode='wb') as file: $     file.write(response.content)
total_sales = total_sales.dropna()
df3.query('group == "treatment"').head()
data_after_first_filter = w.get_filtered_data(step=0) # level=0 means first filter $ print('{} rows matching the filter criteria'.format(len(data_after_first_filter)))
bnbAx.language.value_counts().plot.bar()
for df in list_to_change: $     df["cumsum_"] = df.diff_.cumsum() $ addicks.head() # to see the cumsum_ column.
import sys $ sys.getsizeof(items)  # 48 bytes - not nearly enough to hold 1000 items
segmentData.opportunity_amount.hist(bins=30, figsize=(12,7));
print cust_data[cust_data.duplicated()] $
data['register_date'] = data.register_date.apply(pd.to_datetime) $ data['register_month'] = data.register_date.apply(lambda date: date.month) $ data['register_year'] = data.register_date.apply(lambda date: date.year) $ data['register_year_week'] = data.register_date.apply(lambda date: str(date.year)+'/'+str(date.week)) $ data['is_weekend'] = data.register_date.apply(lambda date: True if date.weekday() in [4,5,6] else False)
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==835246439529840640]
print(list(festivals.columns.values)) $ festivals = festivals[['Index', 'Date', 'Location', 'latitude', 'longitude', 'Website']] $ festivals.head(3)
tweets_df.head().T
df_test.head()
stock_df = stock_df.pct_change()[1:] $ stock_df.head()
df.to_csv('df_ancestry.csv') $ df_model = h2o.import_file(path='df_ancestry.csv')
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score
log_lm = sm.Logit(df2['converted'], df2[['ab_page', 'intercept']]) $ results = log_lm.fit() $ results.summary()
conversion_rate_null = (df2['converted'] == 1).sum() / len(df2.index) $ p_new = conversion_rate_null $ print(p_new)
ab_data_p_diff = df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean() $ p_diffs = np.array(p_diffs) $ (p_diffs>ab_data_p_diff).mean()
sns.lmplot(x="age", y="happy", data=training, x_estimator=np.mean, order=1)
image_clean.head()
set(test[test['date_time'].isnull()].index)
top_songs.head()
avg_comp_score = pd.DataFrame(news_df_grouped['Compound Score'].mean()) $ avg_comp_score = avg_comp_score.rename(columns = {'Compound Score':'Avg Compound Score'}) $ avg_comp_score.round(2)
free_mo_churns = [ix for ix in USER_PLANS_df.index if USER_PLANS_df.loc[ix,'scns_array']==['Free-Month-Trial'] ]
a_vs_p_diffs = (np.array(p_diffs) > a_diffs).mean() $ print (a_vs_p_diffs)
train = pd.merge(train, air_reserve, how='left', on=['air_store_id','visit_date']) $ test = pd.merge(test, air_reserve, how='left', on=['air_store_id','visit_date'])
speakers.head()
weather_mean[['Wind Dir (deg)', 'Wind Spd (km/h)']] = 0 $ weather_mean.head()
df3.groupby('country').mean()
(merged_visits $  .groupby(('results', 'address', 'dba_name')) $  .size()).head()
set(train_data.notRepairedDamage)
print "R^2 score of prediction for test set is: ", r2_score(y_test, clf.predict(X_test))
df_image_clean['img_num'].value_counts()
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], value=None, alternative='larger',prop_var=False) $ z_score, p_value
print("Trips shape:") $ dftrips.shape
users = df['user_id'].nunique() $ users
eth_df = general_info.groupby('ethnicity').count()['candidate_id'].sort_values(ascending=False) $ ethnicity_plot = eth_df.plot.bar(figsize=(13,8), color=['green']) $ for p in ethnicity_plot.patches: $     ethnicity_plot.annotate(str(p.get_height()), (p.get_x() * 1.025, p.get_height() * 1.005))
df_t1 = df_closed[[u'Created Date', u'Close Date', u'Service Location']].reset_index(drop=True) $ df_t1.columns = ['Start', 'Finish', 'Task'] $ df_t1.head()
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://image.slidesharecdn.com/whatisword2vec-150901142702-lva1-app6891/95/what-is-word2vec-21-638.jpg?cb=1441117721 ")
ethereum_github_issues_url = blockchain_projects_github_issues_urls[1] $ ethereum_github_issues_df  = pd.read_json(get_http_json_response_contents(ethereum_github_issues_url))
energy[energy.index < valid_start_dt][['load']].rename(columns={'load':'original load'}).plot.hist(bins=100, fontsize=12) $ train.rename(columns={'load':'scaled load'}).plot.hist(bins=100, fontsize=12) $ plt.show()
image_predictions_clean.info()
pd.crosstab(test["rise_in_next_day"], predictions, rownames=["Actual"], colnames=["Predicted"])
days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'] $ for i in range(7): $     daily[days[i]] = (daily.index.dayofweek == i).astype(float)
import statsmodels.api as sm $ convert_old =  df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new =  df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = df2.loc[df2["landing_page"]=="old_page",].shape[0] $ n_new = df2.loc[df2["landing_page"]=="new_page",].shape[0]
df = table[0] $ df.columns = ["Parameters", "Values"] $ df.head()
! gcloud ml-engine predict --model $MODEL_NAME --version $VERSION_NAME --json-instances more_than_50K.json
altitude_only = search_df[search_df.altitude != ''].copy()
A = np.array([1, 0, 1, 0, 1, 0], dtype=bool) $ B = np.array([1, 1, 1, 0, 1, 1], dtype=bool) $ A | B
DataAPI.write.update_index_contents(index_code="A", trading_days=trading_days, override=False) $ DataAPI.write.update_index_contents(index_code="000300.SH", trading_days=trading_days, override=False) $ DataAPI.write.update_index_contents(index_code="000016.SH", trading_days=trading_days, override=False) $ DataAPI.write.update_index_contents(index_code="000905.SH", trading_days=trading_days, override=False)
wine_reviews.head().to_csv("wine_reviews.csv")
indexes=df.query('group=="treatment" and landing_page!="new_page" or group=="control" and landing_page!="old_page"').index.values $ indexes
kchart.draw(df)
my_q = vectorizer.transform(['Hi. My name is Piush.'])
del df['OutcomeSubtype'], df['AnimalID'] # drop extraneous data  columns $ df = df.dropna(subset=['SexuponOutcome']) # drop the one nan row in the sex column. Not a large impact to analysis. $ df = df.reset_index() # reset index to ensure further operations are executed correctly
store_items = store_items.drop(['store 1'], axis=0) $ store_items
suspects_with_1T_27['loc'] = suspects_with_1T_27.apply(to_loc, axis=1)
mit.describe(include='all')
giss_temp = giss_temp.where(giss_temp != "****", np.nan)
len(mod.coef_)
index = pd.bdate_range('2012-1-1', periods=250)
npd.close()
movies['HarMean'] = ((movies['HarMean']-min(movies['HarMean']))/(max(movies['HarMean'])-min(movies['HarMean'])))#nomalising the harmonic with the standard formula.
labels_dedupe = labels.drop_duplicates(['funding_round_uuid','investor_uuid']) $ labels_dedupe = labels_dedupe[labels_dedupe.investor_uuid.isin(investors_df.uuid)].copy()
print (temp_cat_more) $ temp_cat_more.remove_unused_categories()
c['code'].unique()
merged_df['Day'] = merged_df.index.dayofweek
data.to_csv('{fname}/final_data/min_temp_Hyderabad.csv'.format(fname=folder))
PFalsePositive = (PPositiveFalse * PFalse) / PPositive $ "%.2f" % (PFalsePositive * 100) + '%'
print df_vow.index.weekday_name[:5] $
f(137.5, True)
trip_arrive = dt.date(2018, 4, 1) $ trip_leave = dt.date(2018, 4, 15) $ last_year = dt.timedelta(days=365) $ temp_avg_lst_year = (calc_temps((trip_arrive-last_year), (trip_leave-last_year))) $ print(temp_avg_lst_year)
test_data_features_tf = vectorizer_tf.fit_transform(clean_test_reviews) $ test_data_features_tf = test_data_features_tf.toarray() # Numpy arrays are easy to work with $ print(test_data_features_tf.shape)
cpi_sdmx.codes['Dimension'][1]
train.head()
autos.info()
df2[['new_page','old_page']] = pd.get_dummies(df2['landing_page']) $ lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = lm.fit() $
y = K.dot(x, W) + b $ loss = K.categorical_crossentropy(y, target)
sp500[(sp500.Price < 10) & (sp500.Price > 0)][['Price']]
stmt1= session.query(employee).filter(employee.c.id=='Male').first() $ stmt2= session.query(employee).filter(employee.c.hours_per_week==45).first()
avg_km_series = pd.Series(avg_km_by_brand, dtype=int) $ price_vs_km["mean_odometer_km"] = avg_km_series $ price_vs_km
df.isnull().any().any() $
pd.unique(tweetsDF.location)
centromicrocentro = CABAPorBarrioYAnioNoIndex.loc[CABAPorBarrioYAnioNoIndex.place_name.str.contains("Centro / Microcentro"), :] $ centromicrocentro
scenarios_rdd = sc.textFile(csv_filename).map(lambda l: l.split(",")).map(parse2)
add_plane_data = pk_planes.join(grouped, ['pk_id'], 'left_outer')
def my_function(x, y, z=1.5): $     if z > 1: $         return z * (x + y) $     else: $         return z / (x + y)
full_clean_df.groupby('type')['rating_numerator'].mean()
print('band width between first 2 bands =',(wavelengths.value[1]-wavelengths.value[0]),'nm') $ print('band width between last 2 bands =',(wavelengths.value[-1]-wavelengths.value[-2]),'nm')
!grep -A 20 "INPUT_COLUMNS =" taxifare/trainer/model.py
qualification['qual_conversion'] = qualification.apply(qualConversion, axis=1)
twitter_archive_enhanced_clean.info()
to_be_predicted_Day3 = 21.20401737 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
log_2 = pd.read_csv('log_2.csv') $ log_2.head()
print(f'Proportion of users converted by total count:  {users_converted/users_total:.4f}') $ print(f'Proportion of users converted by unique count:  {users_conv_uniq/users_unique:.4f}')
interact(plot_data, date=data.index)
fig, ax = plt.subplots(1, 1) $ station_sample_group = station_by_week[station_by_week.STATION == '1 AV'].groupby(['Week_Number']) $ for key, grp in station_sample_group: $     grp.plot("Week_day", "entries_diff", ax=ax, label=key) $
from nltk.stem.snowball import SnowballStemmer $ stemmer = SnowballStemmer('english')
g = sns.FacetGrid(bb_df, col="Pos", margin_titles=True) $ g.map(plt.hist, "Wt", color="steelblue") $ plt.subplots_adjust(top=0.5) $ g.fig.suptitle("Distribution of NBA Player Weight by Position") $ plt.show()
pd.cut(titanic.age, [0,10,20,30,40,50,60,70,80]).value_counts(sort=False).plot(kind='bar')
test_train = np.random.random(word_vecs.shape[0]) < .8 $ Xtr = tf_idf_matrix[test_train] $ Xte = tf_idf_matrix[~test_train] $ ytr = y[test_train] $ yte = y[~test_train]
pred_labels = lr2.predict(test_data) $ print("Training set score: {:.2f}".format(lr2.score(train_data, train_labels))) $ print("Test set score: {:.2f}".format(lr2.score(test_data, test_labels)))
investors_df.shape[0]
apple["20d"] = np.round(apple["close"].rolling(window = 20, center = False).mean(), 2) $ apple_rstd = np.round(apple['close'].rolling(window = 20, center = False).std(), 2) $ apple['upperband'] = apple['20d'] + 2 * apple_rstd $ apple['lowerband'] = apple['20d'] - 2 * apple_rstd $ pandas_candlestick_ohlc(apple.loc['2017-08-25':'2018-03-13',:], otherseries = ["20d", "upperband", "lowerband"])
data1.corrwith(data2)
kl["catfathername"].unique()
cols = list(df_mes2.columns) $ cols.remove('tip_amount')
import matplotlib.pyplot as plt $ %matplotlib inline $ plt.style.use("ggplot") $ deaths_XX_century.set_index('decade').plot()
df.head()
c_df.size
last_seen_count_norm.describe()
prob_treat =df2.query("group=='treatment'").converted.mean() $ print('The probality of an individual converting in the treatment group converting is {}'.format(prob_treat))
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results_1 = logit_mod.fit() $
data.drop(['TIDF Compliance', 'Voluntary Soft-Story Retrofit'], axis=1, inplace=True)
df2['intercept'] = 1 # add intercept $ df2[['control','treatment']] = pd.get_dummies(df2['group']) #create dummy variables from group $ df2['ab_page'] = df2['treatment'] # create ab_page column by duplicating treatment column $ df2 = df2.drop(['control','treatment'], axis=1) # drop original 'control' and 'treatment' dummy variables $ df2.sample(5)
autos["brand"].value_counts(normalize=True)
df_cond.shape
pd.options.display.max_rows = 200 $ pd.options.display.max_columns = 200 $ leg
Z = np.diag(1+np.arange(4),k=-1) $ print(Z)
sep = train_delta*0.3 $ separizer = train['date'].max() - sep # Timestamp('2016-06-19 02:24:00')
autos['price'].sort_index(ascending=False).head(9)
frame.sort_index(axis=1, ascending=False)
x_axis1 = [] $ y_axis1 = [] $ for item in output1: $     x_axis1.append(item[0]) $     y_axis1.append(item[1])
payments_total_yrs.head(2)
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ new_page_converted
df.show()
tweet_archive_clean.columns
import statsmodels.api as sm $ convert_old = df2[df2['group']=='control']['converted'].sum() $ convert_new = df2[df2['group']=='treatment']['converted'].sum() $ n_old = (df2['landing_page']=='old_page').sum() $ n_new = (df2['landing_page']=='new_page').sum()
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_hat), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
new_page_converted.mean() - old_page_converted.mean()
df['ab_test_group'] = df['fitness_test_date'].apply(lambda x: "B" if pd.isnull(x) else "A") $ df.head(5) $
!tar -zxvf {PATH}rossmann.tgz -C {PATH}
item_item_rec_result = item_item_rec.recommend(k=10, verbose=False) $ item_item_rec_result
l_2 = [3,5,4,6,5,7] $ l_2.sort() $ l_2
rules.to_csv('Association_result.csv')
diffs_evening = [] $ for _ in range(10000): $     new_evn_con = np.random.choice(a=[1,0], size=eve_new, replace=True, p=[eve_prob, 1-eve_prob]).mean() $     old_evn_con = np.random.choice(a=[1,0], size=eve_old, replace=True, p=[eve_prob, 1-eve_prob]).mean() $     diffs_evening.append(new_evn_con - old_evn_con) $
df_archive_clean["tweet_id"] = df_archive_clean["tweet_id"].astype(str)
site_valsdf.plot(y='datavalue', x='valuedatetime', label=varcode)
print(DataSet_sorted['tweetText'].iloc[1])
journalist_mention_gender_summary(journalists_mention_df[journalists_mention_df.gender == 'M'])
dat=dat_before_fill.copy() #reset 'dat' to be un-interpolated data $ for temp_col in temp_columns: $     dat.loc[:,temp_col]=dat[temp_col].interpolate(method='linear', limit=12) #<= 3hrs;  at a 15 min interval, 3 hrs is 12 measurements
n_old = sum(df2.group == 'control') $ n_old
grouped_dpt.size()
score_c = score[(score["score"] < 80) & (score["score"] >= 70)] $ score_c.shape[0]
cvPredictions = cvModel.transform(testData)
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs),func.avg(Measurement.tobs)).filter(Measurement.station=="USC00519281").all()
autos.rename({'odometer': 'odometer_km'}, axis= 1, inplace=True)
df_merge.head()
re.findall('title="(.*?)"', slices[2])
ttarc.describe()
data = pd.read_csv("output2.csv")
tfidf_vocab = np.array(tfidf_vectorizer.get_feature_names())
import statsmodels.discrete.discrete_model as sm $ from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
Raw_Forecast.Qty = Raw_Forecast.Qty.apply(lambda x: pd.to_numeric(x))
from sightengine.client import SightengineClient $ client = SightengineClient("737618018", "bstrJ5VzARavYy5FsELN") $ output = client.check('face-attributes').set_url('https://instagram.fprg2-1.fna.fbcdn.net/vp/aa7a6811e2fbc814c91bb94e92aa8467/5B197538/t51.2885-19/s150x150/11875444_527830867370128_1019973931_a.jpg')
theft.head()
print(IPython.sys_info())
t_likes.plot(title='Donald Trump', $              figsize=(12,6), $              legend=True, $              label='Likes'); $ t_retweets.plot(legend=True,label='Retweets');
dtypes={'date':np.str,'dcoilqtico': np.float64} $ parse_dates=['date'] $ oil = pd.read_csv('oil.csv', dtype=dtypes, parse_dates=parse_dates) # opens the csv file $ print("Rows and columns:",oil.shape) $ pd.DataFrame.head(oil)
autos["date_crawled"].str[:10].value_counts(normalize=True,dropna=False).sort_index(ascending=True)
import statsmodels.api as sm $ logit = sm.Logit(df2_new['converted'], df2_new[['ab_page', 'intercept']]) $ results=logit.fit()
cities.reindex([2, 0, 1])
df1['Description'].value_counts().head(15)      
trunc_df.loc[list(top_10)].to_csv('Civatech_Oncology.csv')
import statsmodels.api as sm $ convert_old = df2[(df2['landing_page']=='old_page')&(df2['converted']==1)].user_id.count() $ convert_new = df2[(df2['landing_page']=='new_page')&(df2['converted']==1)].user_id.count() $ n_old = length_of_old $ n_new = length_of_new $
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.title("Simulated differences in conversion rates under null hypothesis", fontsize=14) $ plt.xlabel('Difference in Probability', fontsize=12) $ plt.axvline(treat_convert - control_convert, color='r')
tr, _ = acc.get_month('2016-10') $ tr
df.groupby(by = ['City']).sum().sort_values(by = ['Sale (Dollars)'], ascending = False).head()
spark.app.name  App Name $ spark.ui.port   4141 $ spark.master    spark://localhost:7077
a.append(4) $ b
dfjoined.sort_values(['count_type_day'], ascending = 0).head()
dataset.head()
firstWeekUserMerged = firstWeekUserMerged.drop(["gender","birth_year"], axis=1) $ firstWeekUserMerged.isnull().sum()
fire_month = dff['index'].resample('M').sum().to_period(freq='M') $ fire_month.rename(columns={'index': 'sum'}, inplace=True) $ fire_month.head() $ weekends = fire_month.index.month.isin([12,1])
events = events[events.repo_id.isin(repos_ids.repos_ids)] $ events.shape
twitter_archive_df.sample(3)
vocab = vectorizer.get_feature_names() $ print (vocab)
sb.distplot(cats_df['age at death'].dropna()) $ plt.show()
ax = x.host_since_year.value_counts().sort_index().plot(figsize=(8, 5))
wash_park_matrix = count_vectorizer.fit_transform(important_tweets.text)
z_value,p_value=sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old],alternative='larger') $ print(z_value,p_value)
df=pd.read_csv("data/311_Service_Requests_from_2010_to_Present.csv", usecols=['Created Date','Closed Date','Agency','Complaint Type','Descriptor']) $ df.head() $ %time
combined_df[['CA', 'US']] = pd.get_dummies(combined_df['country'])[['CA','US']] $ combined_df.head()
run txt2pdf.py -o"2018-06-19 2011 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2011 FLORIDA HOSPITAL Sorted by discharges.txt"
autos["odometer_km"].value_counts()
morning_rush.iloc[:5]['longitude']
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
model_name = "{}features_{}minwords_{}context".format(num_features, min_word_count, context) $ model.save(os.path.join(outputs,model_name)) $ model = Word2Vec.load(os.path.join(outputs,model_name))
df = pd.read_csv( $     os.path.join(cachedir, 'subj1/trial1', 'acq1.ifc'), $     sep='\t' $ ).assign(relpath='subj1/trial1', barename='acq1') $ df.head()
model.wv.most_similar(positive=['pasta','chinese'], negative=['italian']) $
data['SMA2'] = data['AAPL'].rolling(200).mean() $ data.tail()
archive_clean.head()
p_new_abtest=df2.query('group == "treatment"')['converted'].mean() $ p_new_abtest
weather_all.head(3)
ftp.close()
results = log_mod.fit() $ results.summary()
new_page_converted=np.random.choice([0,1],size=nnew[0],p=[pnew,1-pnew]) $ new_page_converted = new_page_converted[:145274] $ print(len(new_page_converted))
autos['price'] = (autos['price'].str.replace('$','').str.replace(',','').astype(int)) $ autos['odometer'] = (autos['odometer'].str.replace('km','').str.replace(',','').astype(int)) $ autos.rename({'price':'price_dollars'}, axis=1, inplace=True) $ autos.rename({'odometer':'odometer_km'}, axis=1, inplace=True)
print(Counter(ent.text for ent in doc.ents if 'GPE' in ent.label_))
pumaBB.head()
learn.fit(1e-2, 2)
file_name = 'Pairs of names and ID numbers.csv' $ all_sites_with_unique_id_nums_and_names.to_csv(file_name, index=False) $
pd.to_numeric(new_dems.Sanders).describe() $
df2.tail()
import  statsmodels.api as sm $ lm= sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = lm.fit() $
future_df[['Close', 'forecast']].loc['2018-01-01':].plot(figsize=(12, 8)) $ plt.legend(('Market data','Prediction' ))
train_cols=df2.columns[5:7] $ print(train_cols) $ logit = sm.Logit(df2['converted'],df2[train_cols]) $ result = logit.fit()
session.query(measurement.date).order_by(measurement.date.desc()).first()
stations = session.query(func.count(Station.station)) $ station_count = stations[0] $ station_count
suspects_with_27_1['loc'] = suspects_with_27_1.apply(to_loc, axis=1)
cust['gap'] = (cust['transaction_date'].shift(-1) - cust['membership_expire_date']).dt.days $ cust.iloc[-15:, 1:]
indices_of_interest = df_protest.loc[df_protest.TownCity_Name=='Cape Town'].head().index $ df_protest.loc[indices_of_interest]
df_new['US_ind_ab_page'] = df_new['US']*df_new['ab_page'] $ df_new['CA_ind_ab_page'] = df_new['CA']*df_new['ab_page'] $ logit_h = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'US', 'CA', 'US_ind_ab_page', 'CA_ind_ab_page']]) $ results = logit_h.fit() $ results.summary()
print('There are {} patients readmitted in the 30 day window'.format(np.sum(full['Patient'].value_counts()>1))) $
p_new = df2['converted'].mean() $ print("{} is the convert rate for  Pnew under the null.".format(p_new))
extract_all.loc[(extract_all.APPLICATION_DATE_short>=datetime.date(2018,5,1)) $                &(extract_all.APP_PRODUCT_TYPE.isin(['TL','RL'])) $                &(extract_all.DEC_FINAL_DECISION.isin(['A','J','D']))].sort_values('app_branch_state').app_branch_state.unique()
us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar()) $ business_days = pd.DatetimeIndex(start=train.date.min(), end=train.date.max(), freq=us_bd)
df.groupby('userid')['price'].sum()
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('US', axis = 1) $ df_new.head() $
n_new=df2[df2['landing_page']=="new_page"].shape[0] $ n_new
tweetsDf.describe()
print "The probability that an individual received the new page is {0:.4f}%".format(float(sum(df2['landing_page'] == 'new_page'))/df2['landing_page'].shape[0]*100)
bus['zip_code'] = bus['zip_code'].str.replace("94602", "94102") $
totalVisits_month = pd.DataFrame(df_visits_master.groupby(['year', 'month'], as_index=False).sum()) $ totalVisits_month.drop('day', axis=1, inplace=True) $ print 'DataFrame totalVisits_month: ', totalVisits_month.shape $ totalVisits_month
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index()[35:].plot(kind="bar", title="Ad_created (35th-last)", colormap="Blues_r")
df_new.groupby(['country','group']).converted.mean()
small_train.to_csv('small_train.csv', index=False) 
grouped = df_cod2.groupby(["Death year", "Cause of death"]) $ grouped.size()
print('Number of unique users in the dataset :: ',df['user_id'].nunique())
finalDf.head()
keep_cols = ['Follow up Telepsychiatry', 'Follow up', 'Therapy Telepsychiatry', 'Returning Patient', 'Returning Patient MD Adult'] $ dr_existing = dr_existing[dr_existing['ReasonForVisitName'].isin(keep_cols)]
print(data.loc[123,:]['Tweets'])
corr_coins = ['BTC','ETH','BCH','ETC','XRP','NEO','LTC','IOTA','BTG'] $ mask = np.logical_or.reduce([(df_cryptdex.symbol == coin) for coin in corr_coins]) $ df_cryptdex = df_cryptdex[mask]
joined = joined.merge(trend_de, 'left', ["Year", "Week"], suffixes=('', '_DE')) $ joined_test = joined_test.merge(trend_de, 'left', ["Year", "Week"], suffixes=('', '_DE')) $ len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])
df[df['Descriptor'] == 'Pothole'].groupby(by=df[df['Descriptor'] == 'Pothole'].index.hour).count().plot(y="Borough")
onlyfiles = [f for f in os.listdir(datapath3) if os.path.isfile(datapath3 / f) and f.endswith('.png')] $ onlyfiles.sort() $ print('Files in the folder:') $ for i, w in enumerate(onlyfiles): $     print(i+1, '--' ,w)
plt.figure(figsize=(10,5)) $ plt.plot(my_data_test[::1],'k.') $ plt.show()
model.fit([train_word, train_char], train_labels,batch_size=20, epochs=20,callbacks=[metrics])
weather_data_null.groupby([weather_data_null.index.month]).agg('count').head(12)
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23tompetty').items(100): $     results.append(tweet) $ print(len(results))
pred = train.predict(X_test) $ df = pd.DataFrame(pred, index=users_usage_summaries_test.index.astype(str), columns=['churned'], dtype=str) $ df.to_csv(out_name, header=True, quoting=csv.QUOTE_NONNUMERIC) 
df['fetched time'] = df['fetched time'].astype('datetime64[s]') $ df['created_utc'] = df['created_utc'].astype('datetime64[s]')
display(data.head(10))
norm.ppf(1-(0.05/2))
states.plot(kind='bar') $ plt.title('Counts for States of All Kickstarter Campaigns');
df[((df.landing_page=='new_page')==(df.group=='treatment'))== False].shape[0]
n_new=145310 $ new_page_converted=np.random.binomial(1, mean_new, n_new) $
mean1 = pd.DataFrame(df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(pd.TimeGrouper(freq = '15Min'))['ABPm'].mean().reset_index()) $ max1 = pd.DataFrame(df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(pd.TimeGrouper(freq = '15Min'))['ABPm'].max().reset_index()) $ min1 = pd.DataFrame(df[df['dataset_location'] == 'MICU2-6FL-B1'].groupby(pd.TimeGrouper(freq = '15Min'))['ABPm'].min().reset_index()) $ mock_data = pd.merge(mean1, max1, how = 'left', on = ['dataset_datetime']) $ mock_data = pd.merge(mock_data, min1, how = 'left', on = ['dataset_datetime'])
status.isnull().any()
from bs4 import BeautifulSoup $ think_tanks_response = requests.get('http://www.citizensource.com/Opinion&Policy/ThinkTanks.htm') $ think_tanks_text = think_tanks_response.text $ think_tanks_soup = BeautifulSoup(think_tanks_text, "lxml")
active_fire_zone_df.shape
S.decision_obj.stomResist.options
plt.scatter(low['longitude'],low['latitude'])
output = pipeline.fit(flight7).transform(flight7) $ output = output.withColumnRenamed('price_will_drop_num', 'label') $ output.cache()
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])
alpha = 0.0005 $ loss = tf.add(margin_loss, alpha * reconstruction_loss, name="loss")
van_dummy = van_final.loc[:,['diffs','userid','pagetitle']]
import numpy as np $ print("mean:",np.mean(df_concat_2.message_likes_rel)) $ print("max:",max(df_concat_2.message_likes_rel)) $ print("min:",min(df_concat_2.message_likes_rel)) $ print("std:",np.std(df_concat_2.message_likes_rel))
plt.rcParams['axes.unicode_minus'] = False $ dta_56.plot(figsize=(15,5)) $ plt.show()
from datetime import datetime, timedelta $ date = datetime.now() + timedelta(days=3) $ print(date) $ ntime = date2index(date,times,select='nearest') $ print('index = %s, date = %s' % (ntime, dates[ntime]))
df_predictions.info()
cols = df_os.columns.tolist() + df_usnpl_one_hot.columns.tolist() $ media_classes = [c for c in cols if c not in ['domain', 'notes']] $ breakdown = df_questionable_2[media_classes].sum(axis=0) $ breakdown.sort_values(ascending=False)
from pymer4.models import Lmer $ mres = Lmer("accuracy~1+et+(1+et|subject/block)",data=raw_large_grid_df) $ mres.fit()
df_archive_clean["source"] = df_archive_clean["source"].replace('<a href="http://twitter.com" rel="nofollow">Twitter Web Client</a>', $                                                                "Twitter Web Client")
from scipy.stats import norm $ print(norm.ppf(1-(0.05)))
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_701.values.squeeze(), lags=40, ax=ax1)
f_close_clicks_device_train.show(3)
for tweet in tweepy.Cursor(api.home_timeline).items(10): $     print(tweet.text)
df2[['control','ab_page']]=pd.get_dummies(df["group"]) $ df2=df2.drop('control',axis=1) $ df2["intercept"]=1 $ df2.head()
x1 = df2[df2['group']=='treatment']['converted'].mean() $ x2 = df2[df2['group']=='control']['converted'].mean() $ difference = x1 - x2 $ proportion = (p_diffs > (difference)).mean() $ printmd("**Proportion is**: {:0.2%}".format(proportion))
csvData['date'] = pd.to_datetime(csvData['date'], format = "%Y%m%dT%H%M%S", errors = 'raise')
(mismatch['match'] == 'mismatch').sum()
print d.variables['time'][0:10]
result['ownerId'].nunique()
df['year3'] = df.year1 $ df
df_new.head() $
filepath = wellpath + '*.txt'
pd.isnull(reviews).head()
twitter_archive_clean.drop(columns='name',inplace=True)
weather.boxplot()
(eug_cg_counts / eug_counts).unstack(fill_value=0).plot(kind='bar', stacked=True);
shows2.shape
act_diff = df2[df2['group'] == 'treatment']['converted'].mean() - df2[df2['group'] == 'control']['converted'].mean() $ act_diff $
df = pd.read_csv('teleCust1000t.csv') $ df.head()
expenses_df.melt(id_vars = ["Day", "Buyer"], value_vars = ["Amount"])
pp = PostProcess(run_config='config/run_config_notebook.yml', $                  model='MESSAGE_GHD', scen='hospitals baseline', version=None)
df.head(3)
techmeme.original_title = techmeme.original_title.apply(lambda x: x.replace('\n', '')) $ techmeme.extra_titles = techmeme.extra_titles.apply(lambda x: x.lstrip('[').rstrip(']')) $ techmeme['titles'] = techmeme.original_title + ' ' + techmeme.extra_titles
df_numpy = df.value.as_matrix().reshape(-1,1) $ print(df_numpy[:10])
conditions.nunique()
le_indicators = wb.search("life expectancy") $ le_indicators.iloc[:3, :2]
stream = tweepy.Stream(auth, l)
merged2['AppointmentHours'] = merged2['AppointmentDuration'] /60.0
df_new['CA'] = pd.get_dummies(df_new['country'])['CA']
twitter_status = unique_urls[(unique_urls.domain == domain) & (unique_urls.url.str.count('/') == 3)] $ twitter_status.sort_values('num_authors', ascending=False)[0:50][['url', 'num_authors']]
USvideos['like_ratio'] = USvideos['likes'] / (USvideos['likes'] + USvideos['dislikes']) $ USvideos['comment_intensity'] = USvideos['comment_count'] / USvideos['views'] $ USvideos['thumbs'] = USvideos['likes'] + USvideos['dislikes'] $ USvideos[['like_ratio']].describe()
compdf.Company.value_counts()
import matplotlib.pyplot as plt $ plt.plot(y,model.predict(X),'.') $ plt.show()
print('Average altitude is {} meters above sea level'.format(altitude_only.altitude.apply(pd.to_numeric, args=('coerce',)).mean()))
v_item_hub.columns[~v_item_hub.columns.isin(item_hub.columns)]
notus = geo[np.logical_not(index)]
very_pop_df = au.filter_for_support(popular_trg_df, min_times=7) $ au.plot_user_dominance(very_pop_df)
lst = tab.find({'_id':{'$exists':True}}) $
dcw.plot(x='YearWeek', y='energy',  color='blue') $ dcw.plot(x='YearWeek', y='friendliness',  color='blue') $ tmw.plot(x='YearWeek', y='energy',  color='orange') $ tmw.plot(x='YearWeek', y='friendliness',  color='orange')
%sql \ $ SELECT twitter.user_id, twitter.tweet_text, twitter.heats FROM twitter \ $   WHERE twitter.tweet_text REGEXP 'Roger Federer|Tennis' \ $     AND heats>5 \ $   ORDER BY heats DESC ;
uber_15["day_of_year"].value_counts().head()
lst = [1, 2, np.nan, 3] $ for i in lst: $     print(i==i)
prcp_analysis_df.set_index(['Date'], inplace=True) $ prcp_analysis_df.head()
right(np.array([0, 1]), np.array([1, 2]))
ac['Registration Date'].groupby([ac['Registration Date'].dt.year]).agg('count')
youthUser4['year'].groupby(youthUser4['cityName']).describe()
!find {VAL} -name '*.txt' | xargs cat | wc -w
suspects_with_27_1['loc'].value_counts()/suspects_with_27_1['loc'].value_counts().sum()*(suspects_with_27_1['loc'].value_counts()/suspects_with_27_1['loc'].value_counts().sum()).apply(log10)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
data_archie.columns.values
train.info()
c_df=c_df.drop_duplicates(keep='first') $ c_df.size
test_target_monthly = test_target['255_elec_use'].resample('M').sum() $ test_preds_monthly = test_preds_df['kwh_pred'].resample('M').sum()
def percentile(n): $     def percentile_(x): $         return np.percentile(x, n) $     percentile_.__name__ = 'percentile_%s' % n $     return percentile_
! mkdir census_data $ ! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data --output census_data/adult.data $ ! curl https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test --output census_data/adult.test
prod_sort = averages[averages.Product == 'Amarilla'] $ prod_sort
%matplotlib inline $ new_df.describe() $ new_df[['created_time', 'total_likes']].plot(type='') $ new_df[['created_time', 'total_comments']].plot()
df2 = df2.drop(df2.index[2893])  #dropped one value of the duplicated value $ df2.shape
images_df.describe(include="all")
text = re.sub(r'[^a-zA-Z0-9]', ' ', text) $ print(text)
from sklearn.ensemble import RandomForestClassifier
day2int = {v.lower():k for k,v in enumerate(calendar.day_name)} $ day2int
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new,n_old], alternative='larger') $ z_score, p_value
f_counts_week_ip.show(1)
df = pd.DataFrame.from_records(results) $ df.shape
data.groupby(['Year'])['Salary'].sum()
calls_df.loc[(calls_df["list_id"]==998) | (calls_df["list_id"]==999),"dial_type"]="MANUAL" $ calls_df.loc[(calls_df["dial_type"]!="MANUAL"),"dial_type"]="AUTO"
actual = X_age_notnull.iloc[na_index,1]
model.fit(x=training_data, y=training_labels, $           validation_data=(validation_data, validation_labels), $           nb_epoch=1, verbose=1)
corr = pandas.DataFrame(test.corr()) $ corr.to_csv(path_or_buf=path + '/fantasy_results_corr.csv')
re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['pr'][13])
panel_summary.to_excel('dna_stats.xlsx')
with pd.option_context('display.max_rows', None): $     print(df)
hs2017=results.loc[results.date>"2017-01-01",'date'] $ hs2018=results.loc[results.date>"2018-01-01",'date'] $ hstotal=hs2017+hs2018 $ hstotal
mentions_begin, user_names_begin, user_screen_names_begin, num_tweets_begin, num_retweets_begin = au.get_mentions(uso17_qual_coll)
timezones = DataSet['userTimezone'].value_counts()[:10] $ timezones
energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12) $ plt.xlabel('timestamp', fontsize=12) $ plt.ylabel('load', fontsize=12) $ plt.show()
fp7_int_org_file = "fp7_DiscartededInternationalOrgInSwitzerland.xlsx" $ international_orgs_in_CH = pd.read_excel( $     fp7_int_org_file, $     header=0, $ )
motion_df.set_index('UTC', append = True, inplace = True) $ motion_df.head()
go_no_go_date = go_no_go.groupby('subject_id').Date.unique() $ simp_rxn_time_date = simp_rxn_time.groupby('subject_id').Date.unique() $ proc_rxn_times_date = proc_rxn_time.groupby('subject_id').Date.unique()
pd.bdate_range(start=s,end=e,weekmask=weekmask,holidays=holidays)
p_converted_user = df.query('converted==1').user_id.nunique()/i_unique_user $ p_converted_user
pca_df = pd.DataFrame(pca_data, index = pivoted.T.index.values,columns=labels)
days_list=data_2012['date'].tolist()
len(seen_and_click.groupby(['article_id','user_type','user_id','project_id']).groups)
m = model_attention_nmt(len(human_vocab), len(machine_vocab)) $ m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) $ print(m.summary())
train.loc[train.Purchase > train.AvgPurchaseP, 'High'] = 1 $ train.loc[train.Purchase < train.AvgPurchaseP, 'High'] = 0 $ propHighU = train.groupby('User_ID')['High'].mean().reset_index().rename(columns={'High': 'PropHighU'}) $ train = train.merge(propHighU, on='User_ID', how='left') $ test = test.merge(propHighU, on='User_ID', how='left')
image_df_clean.prediction_1 = image_df_clean.prediction_1.str.title() $ image_df_clean.prediction_2 = image_df_clean.prediction_2.str.title() $ image_df_clean.prediction_3 = image_df_clean.prediction_3.str.title() $
tw_clean = tw.copy() $ im_clean = im.copy() $ rt_clean = rt.copy()
dates_by_tweet_count['Tweet_counts'].corr(dates_by_tweet_count['Volume'])
print("P-tG-converting:", $       df2[df2['group']=='treatment']['converted'].mean())
control_converted = df2[((df2['group'] == 'control') & (df2['converted'] == 1))].count()[0] $ control_all = df2[((df2['group'] == 'control') )].count()[0] $ print('Given that an individual was in the control group, the probability they converted is {}'.format(round((control_converted/control_all),4))) $ print('\nConverted in control is {} '.format(control_converted)) $ print('Total number of control group {}'.format(control_all))
train = pd.concat([train, percent_change_features, df['Close'].rolling(30).mean()], axis=1) $ train.dropna(inplace=True) $ train.head()
precip_df.head()
data.treatment
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new]) $ print("The z score calculated is = ", z_score) $ print("The P value calculated is = ", p_value)
to_be_predicted_Day5 = 25.10814835 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
dyn_data = hp.get_data_dynamic(sensortype='electricity', head=head, tail=tail)
df.Visitors.tolist()
noise_graf.to_csv('noise_graf_counts_cbg.csv')
X_reduced =X_train[X_train.columns[6:]] $ X_reduced.info() $ X_reduced.head()
df_vow.head()
print("Mean squared error: %.2f" $       % mean_squared_error(y_test, y_pred))
df.query('breed == "Not identified"').shape[0]
with open('clean_tweets_full.pkl', 'wb') as picklefile: # wb: write, binary $     pickle.dump(fullDF, picklefile) #dump data into pickle file
goodTargetUserItemInt=targetUserItemInt[targetUserItemInt['label']>0.0] $ print goodTargetUserItemInt.shape
 evaluator = text_classifier.evaluate(df_test)          
df_users_6['atleast_one_course_completed_or_not'].unique()
print(rmse_scores.mean())
merged_df = pd.merge(stool_df, tissue_df, how='outer', on=['Description', 'GROUP']) $ merged_df = pd.merge(merged_df, na_df, how='outer', on=['Description', 'GROUP']) $ merged_df.head()
df_merged.info()
twitter_archive_clean[twitter_archive_clean['in_reply_to_status_id'].isnull()==False]
if os.path.isfile(pickle_full): $     print("You've already pickled!") $ else: $     sorted_stays.to_pickle(pickle_full)
df_twitter_copy['rating_numerator'] = df_twitter_copy['rating_numerator'].astype(float) $ df_twitter_copy['rating_denominator'] = df_twitter_copy['rating_denominator'].astype(float)
df_temperature = pd.melt(df_temperature, id_vars=['Year'], var_name='Month', value_name='Temperature')
airbnb_od.structure()
n_new = df2[df2['landing_page']=="new_page"].count()[0] $ n_new
print(f'dataframe shape: {playerAttr.shape}') $ print(playerAttr.columns) $ playerAttr.head(3)
print("Column names before change") $ df.head(2)
df_us.sample(5) $ df_us[df_us['ab_page'] == 1]['converted'].mean()
df.groupby('episode_id')['raw_character_text'].nunique().agg(['min', 'mean', 'max'])
null_p_new = df2.converted.mean() $ print(null_p_new)
archive_df_clean['clean_numerator'] = archive_df_clean.clean_numerator.astype(float)
low_ranked_question_threshold = np.percentile(questions_scores, 10) $ low_ranked_question_threshold
pickle.dump(nmf_cv_df, open('iteration1_files/epoch3/nmf_cv_df.pkl', 'wb'))
domain = 'facebook.com'
etr.fit(X_train, y_train)
for img in soup("img"): $     print img.attrs
posts = pd.read_csv('data/posts.csv', index_col = 0, parse_dates= [5, 6, 7]) $ posts.head()
d = {'one': pd.Series([100, 200, 300], ['apple', 'orange', 'banana'], 'float32'), $       'two': pd.Series([111, 222, 333], ['apple', 'orange', 'clock'], 'float32')}
feature_names = vectorizer.get_feature_names() $ class_labels = clf.classes_ $ print(class_labels)
tau_p = 150 $ K_p = 0.33 $ theta = 15
techmeme = pd.read_csv('web_scraping/techmeme.csv', index_col=0)
print(bixi.head())
data = data[data.price_aprox_usd!=0]
df['created'] = pd.to_datetime(df['created']) $ df['dow'] = df['created'].apply(lambda x: x.date().weekday()) $ df['is_weekend'] = df['created'].apply(lambda x: 1 if x.date().weekday() in (5,6) else 0)
s1 = pd.Series(np.random.randn(3).round(2), index=list('abc'), name='S1') $ s2 = pd.Series(np.random.randn(5).round(2), index=list('cdefg'), name='S2') $ s3 = pd.Series(np.random.randn(4).round(2), index=list('fghi'), name='S3') $ print s1, '\n\n S2:\n', s2, '\n\n S3:\n', s3
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old , n_new], $                                              )
zip_counts=bus["business_id"].groupby(bus["postal_code"]).size().sort_values(ascending=False) $
baseball.shape
df.head().T
google_stock.isnull().any()
import statsmodels.api as sm $ convert_old = df2.query('group == "control" & converted == 1')['user_id'].nunique() $ convert_new = df2.query('group == "treatment" & converted == 1')['user_id'].nunique() $
dfs_morning[["PREV_DATE", "PREV_ENTRIES"]] = (dfs_morning $                                                     .groupby(["STATION", "DATE"])['DATE','ENTRIES'] $                                                     .transform(lambda grp: grp.shift(1))) $ dfs_morning.dropna(inplace=True) $ dfs_morning['ENTRIES_MORNING'] = dfs_morning['ENTRIES'] - dfs_morning['PREV_ENTRIES']
students.weight.plot.hist(rwidth=.9, bins=np.arange(95,205,10))
df.head(n=2)
n_old = len(df2_control.index) $ n_old
def sort_files(f): $     return int(f.split('_')[4][4:])
total_control = (df2['group'] == 'control').sum() $ control_converted = len((df2[(df2['group'] == 'control') & (df2['converted'] == 1)])) $ print((control_converted / total_control))
building_pa_prc_shrink.sample(5)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, p_value
yhat_prob = LR.predict_proba(X_test) $ yhat_prob
max([len(h.tweets) for h in heap])
data.head()
path = '/Volumes/Development/Stock/Twits_res.csv'
engine.execute('SELECT *FROM measurement LIMIT 20').fetchall()
active_stations = session.query(Measurement.station, func.count(Measurement.id)).\ $ group_by(Measurement.station).\ $ order_by(func.count(Measurement.id).desc()) $ for station, count in active_stations: $     print(station, count)
cityID ='5c62ffb0f0f3479d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Phoenix.append(tweet) 
df3 = df_tweet_json_clean.copy() $ df3.sort_values(by = ['created_at']) $
df2 = df2.drop(1899)
gatecount_game_stations = gatecount[gatecount.name.isin(fenway_stations+td_stations)].copy() $ gatecount_game_stations['team'] = 'sox' $ gatecount_game_stations.ix[gatecount_game_stations.name.isin(td_stations),'team'] = 'bc' $ gatecount_game_stations = gatecount_game_stations[['team','service_day','day_of_week','service_time','service_datetime','entries']] $ gatecount_game_stations = gatecount_game_stations.groupby(['team','service_day','day_of_week','service_time','service_datetime']).sum().reset_index(drop=False)
print("The proportion of users converted is: {}".format(df['converted'].mean()))
df_oncstage_dummies = pd.get_dummies(df_with_metac1['ONC_LATEST_STAGE'])
print('number of contributions with missing candidate name: ',len(dat[dat.CAND_NAME.isnull()==True])) $ print('number of candidate ids for contributions with missing candidate name: ',len(pd.unique(dat[dat.CAND_NAME.isnull()==True].CAND_ID)))
cust_data1.info()
re.sub('https?://[A-Za-z0-9./]+','',df.text[0])
autos['price'] = autos['price'].str.replace("$","").str.replace(",","").astype(int) $ autos['price'].head(10)
df=pd.DataFrame(data, columns=col_names)
merged.groupby(["contributor_firstname", "contributor_lastname", "committee_position"]).amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
df_new = pd.merge(data_df, shiny, left_on = 'patched_cell_container', right_on = 'Unnamed: 0')
census_pd_complete.shape
subject_id_to_group.keys()
logit_countries2 = sm.Logit(df4['converted'], $                            df4[['ab_page', 'US', 'UK', 'intercept']]) $ result3 = logit_countries2.fit() $
to_be_predicted_Day4 = 22.39046488 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
extractor.followers("wendywchapman")
%%time $ treehouse_expression_hugo_tpm = treehouse_expression.apply(np.exp2).subtract(1.0).add(0.001).apply(np.log2)
consumerKey = 'XXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
dfJobs.to_csv('VectorMovingJobs.csv')
teams = pd.read_sql_query('select * from Team', conn)  # don't forget to specify the connection $ print(teams.shape) $ teams.head()
lims_query = "SELECT specimens.name as cell_name, specimens.barcode, specimens.donor_id, donors.id \ $ FROM specimens JOIN donors ON specimens.donor_id = donors.id \ $ WHERE specimens.ephys_roi_result_id IS NOT NULL" $ df = get_lims_dataframe(lims_query) $ df.tail()
print('Most negative tweets:') $ for t in senti.sort_values('polarity').head()['text']: $     print('  ', t)
posts = db.music $ tweets = api.search(q = "#facebook", count = 200) $ for tweet in tweets: $     posts.insert_one(tweet._json)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old]) $ norm_ppf = norm.ppf(1-(0.05/2)) $ print('p-value (two-sided): {}'.format(p_value)) $ print('z-score: {} \nPercent point function: {}'\ $      .format(z_score, norm_ppf)) 
english_dict = enchant.Dict("en_US")
question_2_dataframe_in_top_zips = question_2_dataframe_in_top_zips.groupby(['incident_zip', 'complaint_type'])
tweets_p1_month['M_Y'] = tweets_p1_month.loc[:, 'Month'].astype(str) + '-' + tweets_p1_month.loc[:, 'Year'].astype(str)
zero_rev_acc_opps.rename(columns={'Building ID': 'Total Buildings'}, inplace=True)
transform = TfidfVectorizer(lowercase=False, min_df=.01) $ tf_idf_matrix = transform.fit_transform(back2sent.values) $ tf_idf_matrix.shape
df_2015 = df[df['Date'].dt.year==2015] $ df_2016 = df[df['Date'].dt.year==2016]
sale_average = sales.reset_index(drop=False) $ sale_average
mismatch_g1 = df.query('group == "treatment" and landing_page == "old_page"') $ len(mismatch_g1)
df2[(df2['group'] == 'control')].shape
s4.count()
summed.fillna(method='pad')  # The NaN column remained the same, but values were propagated forward $
twitter_df.info()
youtube_urls = {} # key will be title/artist and value will be YouTube URL $ for title, artist in unique_title_artist[:min(batch_size, len_unique_title_artist)]: $     youtubeurl = urllib.parse.quote_plus(YOUTUBE_URL_TEMPLATE.format(gc.search(title +' '+artist)), safe='/:?=') $     youtube_urls[str((title, artist))] = youtubeurl
user_logs_iter = pd.read_csv("../../input/user_logs_v2.csv", chunksize=50000, $                 iterator=True, low_memory=False, parse_dates=['date']) $
vulnerability_type_histogram = cved_df.groupby(by=['vulnerability_type','cwe_id'])['cve_id','n_exploits'].count() $ print(vulnerability_type_histogram) $ vulnerability_list = np.unique(cved_df['vulnerability_type']) $ vulnerability_by_month = cved_df.groupby(by=['vulnerability_type','month'])['cve_id'].count() $
telecom1 = telecom[total_rech_amt_6_7 >= amont_70_pc] $ print('Dataframe Shape: ', telecom1.shape); print_ln();
srcdir = '../resource/postproc/orig_data' $ cachedir = '../resource/postproc/cache'
customer_emails = sales_data_clean[['Email', 'Paid at']].drop_duplicates() $ customer_emails.dropna(inplace=True)
query_date = dt.date(2017, 8, 23) - dt.timedelta(days=7) $ print("Query Date: ", query_date)
(day_counts.select('day', 'count', 'hashtag', daily_rank) $            .filter('rank <= 5') $            .sort('day', 'rank') $            .show(20))
new_page_converted.mean()-old_page_converted.mean()
df[['Gross Sales','Units Sold']].iloc[[0,2],[0,1]]
data = data.dropna(subset=['name'], how='any') $ data.info()
set(list(building_pa_specs['Column name'].values))==set(list(building_pa.columns))
new = ins2016.groupby("business_id").count() $ new["inspections"] = new["score"].values $ numIns2numIDs = new.groupby("inspections").count()["score"].to_dict() $ numIns2numIDs
filing_ts_df.running_total.plot() $ plt.title("Cumulative Total Filings Over Time") $ plt.ylabel("Cumulative Filings") $ plt.show()
d7 = d6.stack() $ d7
user.loc["Trump"]
eia_extra.head()
index_change = df2_new[df2_new['group']=='treatment'].index $ df2_new.set_value(index=index_change, col='ab_page', value=1) $ df2_new.set_value(index=df2_new.index, col='intercept', value=1) $ df2_new[['intercept', 'ab_page']] = df2_new[['intercept', 'ab_page']].astype(int) $ df2_new = df2_new[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
my_df["user_create"].plot() $ my_df["user_active"].plot() $ plt.ylabel("Nuser") $ plt.legend() $ plt.show()
cityID = 'a6c257c61f294ec1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Greensboro.append(tweet) 
from google.colab import files $ uploaded = files.upload()
n_old = len(df2[df2.group == 'control']) $ n_old
a.iloc[:3]
!tail -n 10 p32cfr_results.txt
beirut['Mean Humidity'].mean(), summer['Mean Humidity'].mean()
ser.mean()
df2[df2.duplicated('user_id') == True]
Test.EndAutoRun()
tweets['chars'].plot(kind='hist')
tweets_filtered = [term for term in tweets if term not in stop] $ terms_bigram = bigrams(tweets_filtered) #exclude stopwords $ count_all_bg = Counter() $ count_all_bg.update(terms_bigram) $ count_all_bg.most_common(5)
crime_df = pd.read_csv(crime_file) $ print "Crime data loaded." $ crime_df.head()
n_old = df2.query('group == "control"')['converted'].count() $ n_old
nitrodata['Year'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).year $ nitrodata['Month'] = pd.DatetimeIndex(nitrodata['ActivityStartDate']).month
us.loc[us['country'].isna(), 'country'] = us.loc[us['country'].isna(), 'cityOrState']
df.index
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
df_mas.in_reply_to_status_id_x = df_mas.in_reply_to_status_id_x.astype(str) $ df_mas.in_reply_to_user_id_x = df_mas.in_reply_to_user_id_x.astype(str) $ df_mas.retweeted_status_id = df_mas.retweeted_status_id.astype(str) $ df_mas.retweeted_status_user_id = df_mas.retweeted_status_user_id.astype(str)
sentiments_pd.to_csv("NewsMood.csv", encoding="UTF-8")
articles_train, articles_test, authors_train, authors_test = train_test_split( $     articles, authors, test_size=0.3, random_state=42, stratify=authors)
dfFull.BsmtFinSF1 = dfFull.BsmtFinSF1.fillna(dfFull.BsmtFinSF1.mean())
subte = pd.read_csv('../Datos Capital/estaciones-de-subte.csv',low_memory=False)
PIT = pd.read_excel(url_PIT, $                     skiprows = 8)
systems.to_sql(con=engine, name='systems', if_exists='replace', flavor='mysql',index=False)
logit_new = sm.Logit(df_mod['converted'], df_mod[['intercept','US', 'UK','ab_page','UK_ab_page']]) $ model_new = logit_new.fit() $ model_new.summary()
data = [['Alex',10],['Bob',12],['Clarke',13]] $ df = pd.DataFrame(data,columns=['Name','Age']) $ print(df)
nba_df.shape
results.plot(kind='area', stacked=False) $ plt.show()
reviewsDF = reviewsDF.drop(reviewsDF.columns[0], axis = 1)
url = "http://www.reddit.com"
targets = ['1D', '1W', '1M', '3M'] $ df_symbols = df.loc[df_symbols.index] $ df_symbols.drop(targets, axis=1, inplace=True) $ df_symbols.dropna(axis=1, inplace=True) $ df_symbols.head()
department_df["Revenue_Range"] = grouped_dpt["Revenue"].transform('max') - grouped_dpt["Revenue"].transform('min') $ department_df
df['Descriptor'].value_counts()
df2[df2['user_id'].duplicated(keep = False)]
df_mas.rating_numerator.value_counts()
df['y'].plot.box(notch=True, showmeans=True)
youthUser2["cityName"] = youthUser2['name'] $ youthUser3 = youthUser2[['_id_x','creationDate','namefirst','namelast','contactInfoemail','contactInfophone', $                          'demographicInfodob','demographicInfoethnicity','demographicInfogender', $                         'demographicInfozipcode','loginDetailslastLoginTimestamp1','cityName']].copy() $ youthUser3.head()
saveToFile = os.path.join(PROCESSED_PATH, 'Opportunities_with_Current_High_Revenue_Accounts_On_Net.csv') $ high_rev_acc_opps_net.to_csv(saveToFile, index = False)
Image("SD_districts.jpg")
df2.shape[0]
from sklearn.linear_model import LogisticRegression
df['user_id'].nunique()  #Identify the number of unique elements
id_numbers_to_drop $ idx = df_providers[ df_providers['id_num'].isin(id_numbers_to_drop)].index.tolist() $ len(idx) $ df_providers.loc[idx[:10],'id_num'] 
n_old=df2[df2['landing_page']=='old_page']['converted'].count() $ n_old
output.summary()
id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())
ts2 = pd.Series(np.random.randn(len(rng))) $ ts2
new_reps.Kasich.astype("float64").describe()
newfile = pd.read_excel('export new.xlsx') $ oldfile = pd.read_excel('export old.xlsx')
df_control = df.query("group == 'control'") $ df_control.converted.mean()
viscov = viscov.drop(columns = ['BTC Price', 'BTC Price Change', 'BTC Volume', 'ETH Price', $                                   'ETH Price Change', 'ETH Volume']) $ viscov = viscov.drop(viscov.index[-4:]) $ viscov
np.exp(information_ratio)
s = pd.Series([89.2, 76.4, 98.2, 75.9], index=list('abcd')) $ 'b' in s
match[match.iloc[:,66 :77].notnull().any(axis=1)].iloc[:5,66 :77] # get rows with non-nulls in columns 66 :77
movies_df.loc[movies_df['movieId'].isin(recommendationTable_df.head(20).keys())]
confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_hat), $                                               columns=['predicted_High(1)', 'predicted_low(0)'], $                       index=['is_High(1)', 'is_Low(0)'])
parsed_test.printSchema()
contributions = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
body = soup.find('div', attrs={'class' : 'article-body'}).get_text() $ print(body)
users.columns.tolist()
rng_eastern = rng_utc.tz_convert('US/Eastern')
print("Number of merchants for each class: ") $ disabled, active = df_selection['Shop_Status'].value_counts() $ print("Disabled accounts: "+str(disabled)) $ print("Active/Created accounts: "+str(active))
wkTrump['Trump Percent']=wkTrump['Trump']/wkTrump['n']*100 $ wkTrump['MA T Percent']=wkTrump['Trump Percent'].rolling(window=3,center=True).sum().fillna(0.)/3.
(df2.landing_page == 'new_page').sum() / len(df2)
(df_merged['p1'].iloc[1150], df_merged['p1_conf'].iloc[1150],df_merged['p1'].iloc[602],df_merged['p1_conf'].iloc[602],df_merged['p1'].iloc[201],df_merged['p1_conf'].iloc[201])
sheet_name = 'DataFrame 0' $ sheet_num = 1 $ xls_toc_label = '=HYPERLINK("#{}!A1","{}")'.format(str(sheet_num),sheet_name) $ print(xls_toc_label)
autos['year_of_registration'].value_counts(normalize = True).sort_index()
X_train.columns
df.iloc[[11,24,37]]
trainx.shape
revenue["totals.transactionRevenue"].plot.hist(bins = 100)
d + pd.tseries.offsets.YearEnd(month=6)
u = so.user('1600172') $ print('reputation is' , u.reputation.format()) $ print('no of questions answered - ', u.answers.count)
answer = model.predict(df_germany)
y_test = df_test['loan_status'].values $ y_test[0:5]
plt.style.use('ggplot') $ fig = plt.figure() $
first_price = trade_data_btc.df_h.loc[Y_btc_test.index[0]] $ last_price = trade_data_btc.df_h.loc[Y_btc_test.index[-1]] $ (last_price - first_price) / first_price
reputation_score(author_data.iloc[39]['reputation'])
test_orders_prodfill_final=test_orders_prodfill[test_orders_prodfill['reordered']==1] $ test_orders_prodfill_final.head()
teams_df.head()
Station = Base.classes.station $ Measurements = Base.classes.measurements
b_rev.head(2)
def get_list_ts(the_posts): $     list_ts = [] $     for i in list_Media_ID: $         list_ts.append(the_posts[i]['activity'][-1]['scrape_ts']) $     return list_ts
archive_clean['name'].replace(archive_clean[archive_clean.name.str.islower()].name ,np.nan, inplace=True) $
not_in_misk = not_in_misk.copy()[not_in_misk.copy()['full_name_y'].isnull()][['name', 'forks_url_x', 'forks_count_x', 'created_at_x']] \ $     .rename(columns={'forks_url_x': 'forks_url', 'forks_count_x': 'forks_count', 'created_at_x': 'created_at'})
ny_cities_list = sorted(df_h1b_ny.lca_case_workloc1_city.unique())
y = api.GetUserTimeline(screen_name="HillaryClinton", count=20, max_id=935706980643147777, include_rts=False) $ y = [_.AsDict() for _ in y]
surprise = pd.Series([1000, 1001, 1002, 1003]) $ surprise
coin_data = quandl.get("BCHARTS/ITBITSGD") $
train = timeseries.loc[:'2018-05-31'] ## creating training data without the last month $ test = timeseries.loc['2018-06-01':]  ## Will test the model using the last month data
twitter_archive_master[(twitter_archive_master.iloc[:,8:12].sum(axis=1) == 1) & (twitter_archive_master['has_stage'] == 0)]
rent_db3.boxplot(column='price', by='bedrooms')
to_be_predicted_Day3 = 31.29300657 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
dfHashtags.rename(columns={ 0:"date", 1:'word', 2:'latitude', 3:"longitude" }, inplace=True)
another_list = [1.0, 1, 'two', 2, 2.0]
print finalGoodTargetUserItemInt.shape $ print finalGoodTargetUserItemInt.columns $ finalGoodTargetUserItemInt.head()
plt.bar(['not a hit', 'hit'], data['y'].value_counts()) $ plt.show()
df.loc[df.Sentiment==-1, ['description','Sentiment']].head(10)
a_result_2 = df1.append([df3], ignore_index = True) # same as option 4 above $ a_result_2
idx = df_providers[ (df_providers['id_num']==260096) &\ $                    (df_providers['year']==2011) &\ $                    (df_providers['drg3']==39) ].index.tolist() $ idx $
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets']))) $ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
vectorized_text_predict = vectorizer.transform(description_predict) $ vectorized_text_predict.toarray()
weight_is_relevant = 2*1/(np.sum(article_isRelevant)/ len(article_isRelevant)) $ weight_is_not_relevant = 1 $ weights = {0:weight_is_not_relevant, 1:weight_is_relevant}
sets_node = columns['fields']['oSets'] $ sets_node['date'].keys()
QUIDS_wide.shape
fashion = pd.read_pickle('./data/fashion.pkl')
glm_binom_feat_1.accuracy(valid=True)
c = pd.concat([base1, base2, base3], axis=1)
temp_cat.value_counts()
df.dropna(0, subset=['processed'], inplace=True)
autos['brand'].value_counts(normalize=True) $ brand_unique = autos['brand'].unique()[:6] $
pd.Series.iloc?
festivals.head(3)
pd.Series(['San Francisco', 'San Jose', 'Sacramento'])
df.sort_index(axis=0, ascending=True)
mask = df2.user_id.duplicated() $ df2.user_id[mask]
df['change'] = df.Close - df.Open
reddit["thumbnail_size"].fillna(0, inplace=True)
date_ny[['date','shannon']].set_index('date').plot()
data.info()
predictions = keras_entity_recognizer.predict(df_test) $ predictions[['text', 'label', 'prediction', 'confidences']][:5]
logit_mod = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'US_pages']]) $ results = logit_mod.fit() $ results.summary()
sub1 = sub1.drop_duplicates().reset_index(drop = True)
autos['date_crawled'].str[:10].value_counts(normalize = True).sort_index()
loans_fut_bucket_30360_xirr_groups.to_clipboard()
details.isnull().sum()
df.head()
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42) $ X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25,random_state=42)
stations_all=session.query(Measurement.station).all() $ q_stations=session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).all() $ q_stations
for row in session.query(Measurements).limit(5).all(): $     print(row) $
GA_profit.head()
cruise_data = pd.read_table(data_file, delim_whitespace=True, header=None, skiprows=1) $ cruise_data = cruise_data.rename( columns={ 0:'Pressure', 1:'Temperature', 13:'Salinity' } ) $ cruise_data[0:5]
print(parser.HHParser)
merge[merge.columns[11]].value_counts() $
image_clean.shape
df2[['ab_page', 'intercept']] = pd.get_dummies(df2['group']) $ df2['intercept'] = 1 $ df2.head()
for tweet in lista[:5]: $     print(tweet.text) $     print()
print(dfx.fillna(value=-999.25),'\n') $ print(dfx) # original data remains unchanged. 
LR_grid.best_estimator_
weather.to_csv(path_or_buf='weather_clean.csv', sep=',',index_label=False)
def full_DRG_name(drg_number): $     idx = df_DRGs[ (df_DRGs['drg_num']==drg_number)].index.tolist() $     assert len(idx) > 0, 'Length of IDX is NULL' $     return( df_DRGs.loc[idx[0],'DRG_name'] )
start = pd.datetime(2011, 1, 1)
df = pd.read_csv('./meal_seats.csv', delimiter='|', names=['meal_id', 'meal_created_date', 'meal_date', 'ticket_price',  'seats_available', 'seats_sold',  'percentage_seats_sold']) $ df['meal_year'] = pd.to_datetime(df.meal_date).apply(lambda x: x.year) $ df = df[(df.ticket_price < 90) & (df.ticket_price > 20) & (df.meal_year == 2016)]
twitter_master = pd.read_csv('twitter_archive_edited.csv',encoding='latin-1')
from PIL import Image $ import requests $ url = user['profile']['image_192'] $ im = Image.open(requests.get(url, stream=True).raw) $ plt.imshow(im)
X_test=tok.texts_to_sequences(X_test) $ x_test=sequence.pad_sequences(X_test,maxlen=maxlen)
features = [i for i in train_norm.columns.tolist() if i != 'duration']
categories = pd.cut(df_titanic_temp.age, [0,age_median,80], labels=['category1','category2']) $ print(categories.value_counts())
unique, counts = np.unique(countries, return_counts=True)
num_not_aligned = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0] $ print('The number of times the new_page and treatment don\'t line up is {}.'.format(num_not_aligned))
x_new = pd.DataFrame({'Date_of_Order' : range(1,700)})
my_town = TextBlob("I love my town, I live in Lisbon") $ my_town.sentiment
prob_new_page = df2.query('landing_page== "new_page"')['user_id'].count()/df2.shape[0] $ print("Probability of individual receiving new page is :", prob_new_page)
cog_simband_times = pd.concat([simband_time_df, all_test_times_dates], axis = 1)
by_time = data.groupby(data.index.time).mean() $ hourly_ticks = 4 * 60 * 60 * np.arange(6) $ by_time.plot(xticks=hourly_ticks, style=[':', '--', '-']);
season_team_groups.aggregate(np.mean).sort_values(by = "Tm.3PM", ascending = False).head(5)
autos['odometer_km'].unique()
new_price = np.array([price[1:] for price in b_cal_q1['price']]) $ print(new_price) $ print(len(new_price))
oil_nulls=pd.DataFrame(daily_sales[pd.isnull(daily_sales.dcoilwtico)]) $ dates_with_nulls=len(oil_nulls['date'].unique()) $ all_dates=len(daily_sales['date'].unique()) $ dates_with_nulls/all_dates
new_page_converted = np.array(df2.sample(received_new_page, replace=True).converted)
temps_df['Difference'] = temps_df.Missouri - temps_df.Philadelphia $ temps_df
print(s['2015-01-01':'2015-04-30'].idxmax()) #Printing the max value from January, 2015 to April, 2015 $ print(s['2015-05-01':'2015-08-31'].idxmax()) #Printing the max value from May, 2015 to August, 2015 $ print(s['2015-09-01':'2015-12-31'].idxmax()) #Printing the max value from September, 2015 to December, 2015
mb.head(10)
pclass1_survived = df_titanic.loc[df_titanic['pclass'] == 1, 'survived'] $ print(pclass1_survived.value_counts()) $ pclass1_survived.value_counts().plot(kind='pie', autopct='%.2f', fontsize=20, figsize=(6, 6))
q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1) $ q1.get_shape
pd.DataFrame(random_integers, index=[3, 2, 1, 0])
age_hist.reset_index(level=0, inplace=True)
i = 1125 $ 'Real: {:.3f}, Predicted: {:.3f}'.format(overdue_duration[i], rfr.predict(features_regress_vect[i])[0])
df=pd.read_csv('data/df_new_comma_2.csv') $ df.head()
retweets['id'].groupby(pandas.to_datetime(retweets['created_at']).dt.date).count().mean() # 2.86
df_test.shape
n = df.query('group == "treatment" & landing_page != "new_page"').count()[0] $ m = df.query('group == "control" & landing_page != "old_page"').count()[0] $ n+m
obs_diff=df2.query("landing_page=='new_page'").converted.mean()-df2.query("landing_page=='old_page'").converted.mean() $ (obs_diff<p_diffs).mean() $
tweet_archive_df.rating_denominator.value_counts().sort_index()
for x,y in zip(predY.split('\n'),trueY.split('\n')): $     print(x.strip(), "\t\t\t",y.strip()) 
import pandas as pd $ def one_hot_encode(x): $     print(len(x)) $     return np.eye(10)[x] $ tests.test_one_hot_encode(one_hot_encode)
msftAC.shift(1, freq='S')
user_summary_df[user_summary_df.tweets_in_dataset == 0].count()
df2.loc['2016-09-18', ['GrossIn', 'NetIn']]
to_be_predicted_Day2 = 21.28236846 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
results_m.summary()
tweets_df.entities.describe()
Val_eddyFlux = Plotting(hs_path + '/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc')
temp = date_df[['weekday', 'duration(min)']] $ temp.head()
df_archive_clean = df_archive_clean.rename(columns = {'timestamp':'tweet_date'})
X = df.text $ y = df.label
archive_df.info()
MATTHEW_gb_user.get_group('BeauSadity').text.values
df.drop_duplicates(inplace=True)
sessions = pd.read_csv('./airbnb firstdestinations/sessions.csv')
training_X_scaled = scaler_M7.transform(training_X) $ holdout_X_scaled = scaler_M7.transform(holdout_X) $ len(training_X_scaled),len(holdout_X_scaled)
unsorted_df.sort_values(by=['col1'],ascending=False,kind="heapsort")
len(df2.query("group == 'treatment'"))/df2.shape[0]
regr2.score(X2, y)  # when we fit all of the data points
df1['user_id'].nunique()
social_disorder = pd.DataFrame(social_disorder.groupby("Census Tract").mean().mean(axis=1)) $ social_disorder.columns = ["Social Disorder"] $ social_disorder
for idx, row in df_trips.iterrows(): $     df_trips.loc[idx, "trip_ended"] = row["trip_started"] + np.random.randint(60 * 5, 60 * 60)
plt.scatter(df_elect['Polarity'], df_elect['Subjectivity'], alpha=0.5, color='darkred') $ plt.title('Tweet #Election2018, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
active = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                order_by(func.count(Measurement.tobs).desc()).all() $ print(active)
QLESQ = QLESQ.dropna(axis=1, how='all') $ QLESQ.columns
results_list = [] $ for y in results: $     results_list.append(y)
df.head(7)
if not os.path.exists('new_data_files'): $     os.mkdir('new_data_files') $ records3.to_csv('new_data_files/Q3B.csv')
(~autos["registration_year"].between(1900,2016)).sum() / autos.shape[0]
Logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','UK','US']]) $ results = Logit_mod.fit() $ print(results.summary()) $ print(np.exp(-0.0149),np.exp(0.0506),np.exp(0.0408))
ranking_table = p_stats.groupby(['season', 'PLAYER_ID', "PLAYER_NAME"])[["MIN", "PTS", "AST", "BLK", "REB", "STL"]].mean().reset_index() $ games_played = p_stats.groupby(['season', 'PLAYER_ID', "PLAYER_NAME"])['GAME_ID'].count().reset_index()
MATTHEWKW['SOCIAL_NETWORKS'] = MATTHEWKW.text.apply(lambda text: pd.Series([x in text for x in SOCIAL_NETWORKS]).any()) $ MATTHEWKW['DECISION_MAKING']  = MATTHEWKW.text.apply(lambda text: pd.Series([x in text for x in DECISION_MAKING]).any()) $ MATTHEWKW['ADAPTIVE_CAPACITY']  = MATTHEWKW.text.apply(lambda text: pd.Series([x in text for x in ADAPTIVE_CAPACITY]).any())
snow.upload_dataframe(df, "st_rvo_me_ref")
bd = birth_dates.set_index("Name").head(3) $ bd
stock = stock.fillna(0)
df.iloc[99,3]
user_corrs = df.groupby('user_id')[['user_answer', 'question_answer']].corr() $ user_corrs = user_corrs.iloc[0::2, -1].reset_index(level=[1])['question_answer'] $
from urllib.request import urlopen, Request $ from urllib.parse import urlencode $ from lxml import etree
df.head()
for row in spark_df.take(2): $     print row
sys.getsizeof(data2017_tup)
matrix_product = tf.matmul(tf_mat1, tf_mat2) $ matrix_sum = tf.add(tf_mat1, tf_mat2)
print(telemetry_feat.count())
import datetime $ data['yyyymm'] = data['Created Date'].apply(lambda x:datetime.datetime. $                                            strftime(x,'%Y%m')) $ data['yyyymm']
merged_complaints_per_tract_capita.describe()
subwaydf.iloc[81762:81768] #this low number seems to be because entries and exits resets.
df.iloc[::-1].plot.barh(title="station observation") $ plt.tight_layout() $ plt.show()
result.summary()
word_count = Counter() $ for sent in df_links[df_links['link.domain'] == 'amzn.to']['tweet.text']: $     word_count.update([w for w in sent.split() if w not in stopwords.words('English')]) $ word_count.most_common(10)
properati[['state_name','price_per_m2','price_usd_per_m2','price_aprox_usd']].groupby('state_name').agg(['mean','sum'])\ $         .sort_values(('price_usd_per_m2','sum'),ascending=False)
All_tweet_data_v2.info()
reddit.describe()
transactions_items.head(2)
tweet_json_df.loc['id'] = data['tweets'][0]['id'] $ tweet_json_df['id']
from tensorforce.agents import PPOAgent $ from tensorforce.core.networks import LayeredNetwork, layers, Network, network
from collections import Counter $ c = Counter([int(stats.coleman_liau(x['cdescr'])) for x in df.to_dict(orient='records')]) $
inputNetwork.add_nodes(**filter_models['inputFilter'])
e.instance_method
grouped_by_date_df.to_csv('interest_level_by_date_df.csv',index=False)
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
train_downsampled.cache() $ testing.cache() $
clean_measure.info()
filename = 'turnstile_180303.csv' $ subwaydf = pd.read_csv(filename)
data2017_list.append(33)
def next_nearest_id(row): $     this_date = pair.DATETIME[row.pair_id] $     next_id = nearest_id(this_date + np.timedelta64(1, 'D')) $     next_date = pair.DATETIME[next_id] $     return next_id if ((next_date - this_date) > np.timedelta64(20, 'h')) else None $
df2.user_id.nunique(), countries_df.user_id.nunique()
df = df[(df.Opened > '01/01/2015') & (df.Opened < '01/01/2018')]
active_user_column = ['Asked', 'Answered'] $ for column in active_user_column: $     print('Number of one-time-users that %s before leaving for good:' % column.lower(), $           one_day_users.loc[users[column].notnull()].shape[0])
soup_b = BeautifulSoup(body_txt, "html.parser") $ latest_news = soup_b.find_all("p")[1] $ latest_news = latest_news.get_text() $ latest_news
twitter_archive_df_clean.columns
p_diffs=[] $ for i in range(10000): $     new_page_converted=np.random.choice([1,0],size=nnew,p=[pnew,1-pnew]) $     old_page_converted=np.random.choice([1,0],size=nold,p=[pold,1-pold]) $     p_diffs.append(new_page_converted.mean()-old_page_converted.mean()) $
file_name = str(time.strftime("%m-%d-%y")) + "-tweets.csv" $ tweet_df.to_csv("analysis/" + file_name, mode = 'w',encoding="utf-8",index = False) $
min(SCN_BDAY_qthis.start_date)
graph.run(tweet_query,param=list_to_merge[23000::]) $
header = loadRetailData.first() $ loadRetailData = loadRetailData.filter(lambda line: line != header).\ $                             map(lambda l: l.split(",")) $ for row in loadRetailData.take(5): $     print row
from sklearn.cross_validation import train_test_split $ X_train, X_test, Y_train, Y_test = train_test_split(description_2017,train_label,test_size=0.25)
n_old = control_df.shape[0] $ print('The number of individuals in the treatment group is n_old = {}.'.format(n_old))
theft.iloc[4]
num_uniqueid = df.user_id.nunique() $ print('No. of unique user ids: ' + str(num_uniqueid)) $
calls_nocontact.street_address.value_counts()
officer_citations = pd.DataFrame(df['badge_#'].value_counts()) $ fig2 = officer_citations['badge_#'][:10].plot('barh', title='Top Citations by Officer Badge Number') $
import statsmodels.api as sm; $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
edge_types_DF = pd.read_csv(edge_types_file, sep = ' ') $ edge_types_DF
df.drop(6, inplace=True)
df2.query('landing_page=="new_page"').user_id.nunique()/df2.user_id.count()
def get_user_id_from_file(f): $     return f.split('url_csvs/')[-1].replace('_urls.csv', '') $ df_link_yt['twitter_id'] = df_link_yt['filename'].apply(get_user_id_from_file)
callbacks = [EarlyStopping('val_loss', patience=2), ModelCheckpoint('movie_weights.h5', save_best_only=True)]
df[['control', 'treatment']] = pd.get_dummies(df['group'])
df.groupby('episode_id')['character_id'].nunique().agg(['min', 'mean', 'max'])
calls_df.pivot_table(["length_in_sec"],["user"],aggfunc="mean").sort_values("length_in_sec",ascending=False)
rng = pd.date_range('1/1/2018',periods=100, freq='M')  # it can also be 'M' $ rng
!head data/countries.csv
model3.summary()
small_ratings_data.take(3)
[p.endswith("a") for p in points.index]
df['timestamp']=pd.to_datetime(df['timestamp'],format='%Y-%m-%d')
old_page_converted = np.random.binomial(old_n, old_p) $ old_page_converted
df_course_after.columns
details.dropna(subset=['Genres', 'Released'], inplace = True)
file_names = [] $ for name in glob.glob('serverdata/*'): $     name = name.split('/')[-1] $     file_names.append(name)
df.nd_key_formatted.unique()
liquor2016_q1.Profit = (liquor2016_q1.StateBottleRetail - liquor2016_q1.StateBottleCost) * liquor2016_q1.BottlesSold $ liquor2016_q1_profit = liquor2016_q1.Profit.groupby(liquor2016_q1.StoreNumber).agg(['sum']) $ liquor2016_q1_profit.columns = ['Profit'] $ liquor2016_q1_profit.tail()
learner.model.load_state_dict(wgts)
df_tte_all.to_csv('tte_may.csv')
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','UK']]) $ results = logit_mod.fit() $ results.summary()
grouped_publications_by_author[grouped_publications_by_author['authorName'] == 'Lunulls A. Lima Silva']
with open('op_attr.pkl', 'rb') as picle: $     op_attr = pickle.load(picle) $ sub_df = pd.concat([sub_df, op_attr], axis=1)
def save_frame_to_CSV(dataframe, name_of_file): $     print("Begin saving dataframe into a csv...\n") $     name = name_of_file + ".csv" $     dataframe.to_csv(name, sep=',', encoding='utf-8', index=False) $     print("Finish and saved into " + name + "\n")
np.shape(temp_fine)
conn = sqlite3.connect("/Users/nicksteil/Desktop/FPA_FOD_20170508.sqlite") $ wildfires_df = pd.read_sql_query("select FIRE_Year, FIRE_SIZE, STATE from Fires;" , conn) $
df2_unique = df2.user_id.nunique() $ print('The number of unique user_ids in df2 is {}.'.format(df2_unique))
df.sample(10)
print(df_new['slot'].value_counts()) $ df_new.loc[:,'slot_top']=np.nan $ df_new.loc[df_new['slot']=='RHS','slot_top']=0 $ df_new.loc[df_new['slot']=='Top','slot_top']=1 $ print(df_new['slot_top'].value_counts())
gene_count_df = chromosome_gene_count.to_frame(name='gene_count').reset_index() $ merged_df = gdf.merge(gene_count_df, on='seqid') $ merged_df
from sklearn.model_selection import train_test_split
tweet_lang_hist.sum()['count']
type(tag_df.stack().index)
print('len(abc.index) is the number of total DRGs when considering ALL years',len(abc.index))
!python extract_dl1.py -f refdata/Run17473_r1.tio
plt = sns.boxplot(data=df, x="race_desc", y="time_detained", hue="race_desc", dodge=False);
swinbounds.head()
n_old= df2['landing_page'].value_counts()["old_page"] $ n_old
tt_final.groupby(['all_p'])['favorite_count'].mean().sort_values(ascending=False).head(10) $
dd.plot(sample_field='Subject',gui='jupyter')
xprep = df_upsampled.copy() $ xprep.drop(['irlco', 'Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True) $ xprep.columns.values[2] = '1hr' $ xprep.reset_index()
player_stats.tail(3)
journalists_mention_summary_df[journalist_mention_summary_fields].sort_values(['mentioning_count', 'mention_count'], ascending=False).head(25)
twitter_data.index=pd.to_datetime(twitter_data['created_at']) $ hourly = twitter_data.groupby(pd.Grouper(freq="H")).count()['_id'].to_frame() $ hourly.columns = ['Count'] $ hourly.plot(figsize=(16,6))
df2.head()
datetime.strptime('01/Aug/1995:09:22:01 -0400',DATETIME_PARSE_PATTERN).weekday()
df_ad_airings_5.isnull().any()
rain_2017_df = rain_df.set_index("date") $ rain_2017_df.head() $
for i in ['Shipped Created diff','Updated Shipped diff']: $     print df[df[i]>500].shape
s = select([employee]) $ result = conn.execute(s) $ row = result.fetchall() $ print(row)
print('Out of {} organisations, {} were matched as companies.'.format( $         len(df_rand), $         len(df_rand[df_rand.is_ch_company == True])))
datetime. #need to execute last declaretion at first
retweet_relation.to_csv("../output/retweet_relation.csv",encoding="utf-8")
y_t.size
X.head(2)
def p_diff(): $     new_page_converted = np.random.choice([1, 0], size=n_new, p=[p0, (1-p0)]) $     old_page_converted = np.random.choice([1, 0], size=n_old, p=[p0, (1-p0)]) $     return np.mean(new_page_converted) - np.mean(old_page_converted) $ p_diffs = np.array([p_diff() for _ in range(10000)])
query_result1.spatial_reference
df = pd.read_sql('SELECT * FROM booking_amounts', con=conn_b) $ df
filename = "data/temperatures/annual.land_ocean.90S.90N.df_1901-2000mean.dat" $ full_globe_temp = pd.read_table(filename) $ full_globe_temp
for info in zf_train.infolist(): $     print("File Name         -> {}".format(info.filename)) $     print("Compressed Size   -> {:.2f} {}".format(info.compress_size/(1024*1024), "MB")) $     print("UnCompressed Size -> {:.2f} {}".format(info.file_size/(1024*1024), "MB"))
ts[pd.Timestamp(2018, 1, 3)]
dftouse=pickledf.copy()
key = "VQ9pfbdfDxTMbF6jdHm6" $ aapl = quandl.get("WIKI/AAPL", start_date="2006-12-25", api_key = key) $ aapl.head()
podcast_descriptions = [] $ for idx in max_vals_idx: $     podcast_descriptions.append(data[idx]) $ podcast_descriptions[:20]
autos.columns
timezones = DataSet['userTimezone'].value_counts()[:20] $ print(timezones)
output = pd.DataFrame(data={"id":test["id"], "sentiment":result,})# "probs":result_prob[:,1]}) $ output.to_csv(os.path.join(outputs,'Word2Vec_AverageVectors.csv'), index=False, quoting=3)
tweets_prediction = pd.merge(archive_clean, images_clean, how='left', on='tweet_id')
s = pd.Series([False, False, True, True, True]) $ s
bnbA.date_first_booking.head()
import statsmodels.api as sm $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit() $
list(zip(feature_cols,lr.coef_))
df.shape
data['screen_resolution'].value_counts()
train_x = encodedlist $ print np.asarray(train_x).shape
all_text = pd.DataFrame(X.toarray(), columns=vect.get_feature_names()) $ all_text.index = fashion.index $ all_text.shape
df_out = pd.merge(df_userid,df_Tran,how='outer',on="Key")[['UserID','ProductID']]
merged_df = pd.merge(left=g,right=p,how='inner',left_on='customer',right_on='customer',copy=True) $ print(merged_df.shape) $ print(merged_df['validOrders'].sum()) $ merged_df.head() $
ci = set(test[test['srch_ci'].isnull()].index) $ co = set(test[test['srch_co'].isnull()].index) $ print(ci,co)
facts_metrics = pd.read_sql_query('select * from "facts_metrics"',con=engine)
topic_word = model.topic_word_  # model.components_ also works $ vocab = tf_vectorizer.get_feature_names()
%%time $ dask_df['new_col'] = dask_df.body.apply(clean_func) $ dask_df['token_body'] = dask_df.new_col.apply(tokenizer)
autos['seller'].value_counts()
data[:] = np.float64(data[:]) $ data.info()
Train = TrainData_ForLogistic.copy() $ Test = TestData_ForLogistic.copy()
df.head(2)
pd.DataFrame({'population': population, 'area': area})
contractor_merge.rename(index=str, columns={"state_abbrev" :"state_code"}, inplace =True)
three = df.pop('three')
data[['returns', 'strategy']].mean() * 252
grouped = tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['dog_stage']!='None'].groupby(['dog_stage'])['favorite_count']
logodds.drop_duplicates().sort_values(by=['count']).plot(kind='barh')
merged1['Provider'].isnull().sum(), merged1['Provider'].notnull().sum()
records['State'].value_counts().plot.bar() $ plt.title("Students per State", fontdict={'fontsize': 14}); $ plt.savefig('images/barplot_state.png')
model.load_weights(MODEL_WEIGHTS_FILE) $ loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0) $ print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))
classifier.fit(X_train,y_train)
twitter_archive.head()
taxiData.shape
os.chdir("..") $ os.chdir("..")
np.exp(0.0506) #UK $
merged2 = pd.DataFrame.merge(merged,omdb_df,on="movie",how="inner") $ merged2.head()
!du -h train.zip
c_denom = [0, 2, 7, 11, 15, 16] $ archive_df[archive_df.rating_numerator.isin(c_denom)][['text', 'rating_numerator', 'rating_denominator']]
sorted_trigram = sorted(trigrams_fd.items(), reverse = True, key=lambda x: x[1]) $
ix = ((iris.Species == "setosa") | (iris.Species == "versicolor")) & (iris["Sepal.Length"]>6.5) $ iris.loc[ix.values,iris.columns[:2]]
df_ind_site.groupby(['id_num', 'year'])['drg3'].agg([np.sum, np.mean, np.max]) $
df = pandas.DataFrame(clean_words, columns=['Words']) $ df['Words'].value_counts()
log_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA', 'US','CA_ind_ab_page','US_ind_ab_page']]) $ results = log_mod.fit() $ results.summary()
bands = questions['bands'].str.get_dummies(sep="'")
dff = dff.reset_index().set_index('Datetime') $ dff = dff[dff['Incident_number'].index.notnull()] $ dff.loc[:, 'index'] = 1
print(clean_madrid.info()) $ clean_madrid.head()
overallTotRmsAbvGrd = pd.get_dummies(dfFull.TotRmsAbvGrd)
s.str.replace('@','$')
df_unique_users_control = df2[df2['group'] == 'control'] $ n_unique_users_control = len(df_unique_users_control) $ n_conversion_control = len(df_unique_users_control[df_unique_users_control['converted'] == 1]) $ probability_control = n_conversion_control/n_unique_users_control $ print ("The probability of an individual converting from the control group: {:.4f}".format(probability_control))
con = sqlite3.connect('db.sqlite') $ con.close() $ df.head()
plt.rcParams['figure.figsize'] = [15, 10]
wrong_control_new_page = df.query('landing_page == "new_page" & group == "control"').shape[0] $ wrong_treatment_old_page = df.query('landing_page == "old_page" & group == "treatment"').shape[0] $ wrong_page_group = wrong_control_new_page + wrong_treatment_old_page $ print('New page and treatment don\'t line up in {} rows'.format(wrong_page_group))
output_file_path = "tweets.ftvec" $ with open(output_file_path, "r") as f: $     lines = f.readlines() $ lines = [l.split(" ")[:-1] for l in lines]
if page.isRedirectPage(): $     page = pb.Page(commons_site, 'File:Parlament Europeu.jpg').getRedirectTarget() $     print(page.title())
r_payments = ft.Relationship(es['loans']['loan_id'], $                                       es['payments']['loan_id']) $ es = es.add_relationship(r_payments) $ es
%matplotlib inline $ tweets_master_df.groupby(tweets_master_df["timestamp"].apply(lambda x: x.month))['timestamp'].count().plot(kind="bar");
'dcf' in payment_plans_combined.columns
filteredPingsDF.take(10)
total_control = df2[(df2['landing_page'] == "old_page")].count() $ print(total_control)
model.predict(numpy.array([[3, 92.6, 109.3, 2, 12, 26],[2, 10.4, 43.5, 3, 26, 5]]))
potential_accounts_buildings_info_tbrr = pd.merge(accounts, potential_accounts_buildings_info,\ $          on=['Account ID'],\ $          how='inner')
ti_tmall.rename(columns={'review_image':'image_url','harvest_product_description':'product_description','retailer_product_code':'rpc','user_id':'username'}, inplace=True) $ ti_tmall['store'] = 'Tmall' $ ti_tmall = ti_tmall[ti_clm] $ ti_tmall.shape
df[(df.group == 'treatment') & (df.landing_page != 'new_page')].shape[0]
contribs.amount
stop_words_update.append('star') $ pipeline.set_params(posf__stop_words=stop_words_list, cv__stop_words=stop_words_update)
wrd_clean['rating_numerator'] = wrd_clean['text'].str.extract(r'(\d+\.?\d+?/\d+\.?\d+?)',expand=False) $ wrd_clean['rating_denominator'] = wrd_clean['text'].str.extract(r'(\d+\.?\d+?/\d+\.?\d+?)',expand=False)
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
consumerKey = 'XXXXXXXXXXXXXXXXXXXXXXX' $ consumerSecret = 'XXXXXXXXXXXXXXXXXXXXXXX' $ auth = tweepy.OAuthHandler(consumer_key=consumerKey, $     consumer_secret=consumerSecret) $ api = tweepy.API(auth)
df.head
proj_df['Project Need Statement'].str.startswith('My students').value_counts()
(p_diffs < obs_diff).mean() + (p_diffs > (np.mean(p_diffs) + np.mean(p_diffs) - obs_diff)).mean()
cityID = '944c03c1d85ef480' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Fresno.append(tweet) 
clinton_client = clinton_df[(clinton_df['source'] == 'Twitter Web Client') | (clinton_df['source'] == 'Twitter for iPhone')] $ clinton_client.head()
np.arange(0, 10, 2)  #Like range, but creates an array instead of a list.
subject_annotation_df = pd.merge(subject_df, final_annotations_df, how='outer', left_on='classification_id', right_on='classification_id') $ subject_annotation_df.sample(20)
responses = pd.read_json('/Users/thomasmulhern/Downloads/pitchesDataJson/response.json')
ccc = td.columns
plt.scatter(bnbAx.age, bnbAx.target) $ plt.xlabel('age') $ plt.ylabel('target')
rfc = RandomForestClassifier(max_depth = 10, n_estimators=5, random_state=42) $ rfc.fit(X_train, y_train) $ rfc.score(X_test, y_test)       # scoring the  on test Data
len([premieScn for premieScn in SCN_BDAY_qthis.scn_age if premieScn < 0])/SCN_BDAY_qthis.scn_age.count()
df.text[343]
from sklearn.linear_model import LogisticRegression $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $
autos['registration_year'].value_counts()
plt.figure(figsize=(12,6)) $ sns.countplot(x='sourceID', data=articles)
shopping_carts.shape
datetime.datetime.strptime(bitcoin_price.iloc[0]['Date'],'%m/%d/%y %H:%M')
df_unique.to_csv('./data/unique_feature_list.csv', encoding='utf-8')
n_customers = data['customer_id'].unique().size
paired_df.head(2)
testheadlines = test["text"] $ advancedtest = advancedvectorizer.transform(testheadlines) $ advpredictions = advancedmodel.predict(advancedtest)
autos.info()
nba_df['Date'] = pd.to_datetime(nba_df['Date'])
obs_diff = np.mean(new_page_converted)-np.mean(old_page_converted) $ obs_diff
deciles = np.arange(0, 1., 0.05) $ sdsw['WAGE_RATE_EST'].quantile(deciles)
dftype = pd.get_dummies(cp311['complaint_type'])
BTC['Balance'] = BTC['BTC_amount'].cumsum() $ BTC['BTC_balance_GBP'] = (BTC['BTC_price_EUR'] / BTC['EUR_GBP']) * BTC['Balance'] $ BTC['BTC_balance_EUR'] = BTC['BTC_price_EUR'] * BTC['Balance'] $ BTC = BTC.sort_values('Timestamp')
df['date'] = df.start_date.dt.date
maxi = stock.cummax()
hdf.loc[(slice('adult', 'child'), slice('Alcoholic Beverage', 'Choc/Cocoa Prod')), :].head()
set(sakhalin_data_in_bbox.id.values) - set(sakhalin_filtered.id.values)
pd.value_counts(preds).plot.bar()
df.count().tail()
fig, ax = subplots() $ data.groupby([pd.Grouper(key='date', freq='D'), 'airline_sentiment']).size().unstack().plot(figsize=(15,7), \ $   color = ['#FF860C', 'grey', '#0080FF'], linewidth = 2, ax = ax) $ ax.xaxis.set_major_locator(mdates.DayLocator(interval = 3)) $ ax.set_ylabel("number of tweets") $
control_converted = df[df['group'] == 'control']['converted'].mean() $ treatment_converted = df[df['group'] == 'treatment']['converted'].mean() $ diff = treatment_converted - control_converted $ (diff < np.array(p_diffs)).mean()
total.print_xs()
transactions.merge(users, how='left', on=['UserID'])
cohens_d = stms.dCohen(df_measures_users[df_measures_users['ms']==1][score_variable], $                        df_measures_users[df_measures_users['ms']==0][score_variable]).effect_size() $ print('Cohen`s d-value: {}'.format(round(cohens_d,3)))
a = master_file.iloc[:, 28:-1].count(axis=0) $ a = len(master_file.index) - a $ print(a)
gen = encod.inverse_transform(siim['gender']) $ pre = encod.inverse_transform(siim['teacher_prefix']) $
proj_df['proj_type'] = proj_df['Project Type'].astype('category') $ pd.concat((proj_df['proj_type'], proj_df['proj_type'].cat.codes), axis=1)
df['converted'].sum()
trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy')) $ val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))
frame.index.names = ['key1', 'key2'] $ frame.columns.names = ['state', 'color'] $ frame
df_train.loc[:, 'is_attributed'] = df_train.loc[:, 'is_attributed'].astype(np.int8) $ df_valid.loc[:, 'is_attributed'] = df_valid.loc[:, 'is_attributed'].astype(np.int8) $ df_test.loc[:, 'is_attributed'] = df_test.loc[:, 'is_attributed'].fillna(0).astype(np.int8)
store_items.fillna(method='ffill', axis=1)
pdiff = (new_page_converted.mean()) - (old_page_converted.mean()) $
plt.hist(house_data['price'])
for i in range(0, 5): $     fig = plt.figure() $     plt.imshow(adjmat_load.data[i, :, :].squeeze(), cmap='jet') $     plt.colorbar()
intersections_irr = pd.concat([intersections_irr, x_normalized], axis=1, sort=False)
clean_df = pd.DataFrame() $ clean_df["id"] = df["id"] $ clean_df["summary_ids"] = df["summary_ids"].apply(lambda x : x[:-1]) $ clean_df["summary_count"] = df["summary_ids"].apply(lambda x : len(x[:-1].split(","))) $ clean_df.head()
df2 = pd.read_csv('ab_edited.csv')
S_distributedTopmodel.decision_obj.groundwatr.options, S_distributedTopmodel.decision_obj.groundwatr.value
a = 5.6 $ s = str(a) $ print(s)
non_grad_GPA_mean = records3[records3['Graduated'] == 'No']['GPA'].mean() $ non_grad_GPA_mean
to_be_predicted_Day5 = 36.48429159 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
season15 = ALL[(ALL.index >= '2015-09-10') & (ALL.index <= '2016-02-07')]
frames1 = [first_values, last_values, count] $ result1 = pd.concat(frames1, axis=1) $
! unzip -l {path_to_zips}On_Time_On_Time_Performance_2015_1.zip
converted = df.query('converted == 1').count() $ converted_prop = converted/total $ converted_prop[0]
churned_ordered_end_date = [churned_ordered.loc[bid,'canceled_at'][-1] if churned_df.loc[bid,'cancel_at_period_end'][-1] == False else churned_ordered.loc[bid,'current_period_end'][-1] if churned_ordered.loc[bid,'cancel_at_period_end'][-1]==True else None for bid in churned_ordered.index]
pd.crosstab(aqi['AQI Category_eug'], aqi['AQI Category_cg'], normalize='columns', margins=True)
testheadlines = test["text"].values $ basictest = basicvectorizer.transform(testheadlines) $ predictions = basicmodel.predict(basictest)
footfall.head(2)
from smart_open import smart_open $ class MyCorpus(object): $     def __iter__(self): $         for line in smart_open('B01NAJGGA2.csv', 'rb'): $             yield dictionary.doc2bow(line.lower().split())
inv_count,inv_bin=np.histogram(investor_fut_bucket_30360_xirr_orig,bins=np.linspace(-.5,.5,101)) $
users_orders = users_orders.groupby('id_partner', as_index=False).sum().sort_values('Cost_14') $ users_orders.Cost_14 = users_orders.Cost_14/users_orders.Order_14 $ users_orders.Cost_30 = users_orders.Cost_30/users_orders.Order_30 $ users_orders.Cost_60 = users_orders.Cost_60/users_orders.Order_60 $ users_orders
slicer.apply(np.mean, axis=1)
s3.reindex(np.arange(0,7), method='bfill')
df2[df2['landing_page']=='new_page'].user_id.nunique()/df2.user_id.nunique()
e.map_rain(basemap=True)
pd.DataFrame(df['price'].describe())
df.to_csv('twitter_archive_master.csv', index=False)
mileages_by_brand = {} $ for b in brands: $     mileages_by_brand[b] = autos.loc[autos['brand']==b, 'kilometer'].mean() $ mileages_by_brand
obs_diff = treatment['converted'].sum() / treatment.shape[0] - control['converted'].sum() / control.shape[0] $ obs_diff
df.plot(x_compat=True, color='#003399') $ plt.xticks(rotation='45') $ plt.tight_layout() $ plt.savefig('rain_station.png')
tl_2030 = pd.read_csv('input/data/trans_2030_ls.csv', encoding='utf8', index_col=0)
noise['AFFGEOID'] = noise['AFFGEOID'].astype(str)
with open('new_reddit_topics.pkl', 'rb') as pick: $     submissions = pickle.load(pick)
! unzip -p {path_to_zips}On_Time_On_Time_Performance_2015_1.zip | head -n 2
longitudinal = activeDF.groupBy("cid")\ $                        .agg({"ssd": "collect_list", $                              "num_ssd": "collect_list"})\ $                        .withColumnRenamed("collect_list(ssd)","ssd")\ $                        .withColumnRenamed("collect_list(num_ssd)", "num_ssd")
df2.query('group == "treatment"')['user_id'].nunique() / total_users
data2.drop('date', axis = 1, inplace = True)
a.append([3,4]) $ a
parsed_email_data['institution'] = parsed_email_data['address'].str.split("@").str.get(1) $ parsed_email_data.head(10)
convert_rate = df2.converted.mean() $ convert_rate
df.groupby('Clus_km').mean()
df_western.groupby(['release_decade'], as_index = False)['id'].count()
data = {'date': ['2014-05-01 18:47:05.069722', '2014-05-01 18:47:05.119994', '2014-05-02 18:47:05.178768', '2014-05-02 18:47:05.230071', '2014-05-02 18:47:05.230071', '2014-05-02 18:47:05.280592', '2014-05-03 18:47:05.332662', '2014-05-03 18:47:05.385109', '2014-05-04 18:47:05.436523', '2014-05-04 18:47:05.486877'], $         'battle_deaths': [34, 25, 26, 15, 15, 14, 26, 25, 62, 41]} $ df = pd.DataFrame(data, columns = ['date', 'battle_deaths']) $ df
gender.ix[[166,262,441,657,976,1260]]=1
LabelsReviewedByDate = wrangled_issues_df.groupby(['created_at','DetectionPhase']).created_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
df2.query('landing_page=="new_page"')['user_id'].count()
from gensim.models import word2vec $ print("Training model...") $ model = word2vec.Word2Vec(sentences, workers=num_workers, \ $             size=num_features, min_count = min_word_count, \ $             window = context, sample = downsampling)
np.shape(rhum_fine)
data.drop(['ceil_10min'], axis=1,inplace=True)
df[['product_type','price_doc']].groupby('product_type').aggregate(np.median)
train_data.isnull().sum()
df['org_twitter'].nunique()
df_byzone.to_sql(zone.lower() + '_table', engine, index = False, if_exists='replace')
dummy_var_df.head()
loan_requests_indebtedness.to_csv('loan_requests_indebtedness_all_user_{}.csv'.format(tod),encoding='UTF-8')
mean_diff = dfNiwot["TDIFF"].mean() $ print("Mean Temp Diff = {:.3f}".format(mean_diff))
(obs_diff-np.mean(p_diffs))/np.std(p_diffs)
df = pd.read_csv("/Users/peterjost/Downloads/12-classwork/classwork-12-311/data/311_Service_Requests_from_2010_to_Present.csv", nrows=50000) $ df.head()
op_ed_articles.head() $ op_ed_articles.to_csv('for_paper.csv')
signals[signals.positions > 0]
season07["InorOff"] = "In-Season" # Again, this assigns an identifier as to whether this is inseason or offseason.
biblical_language = load_from_txt("/Users/jeriwieringa/Dissertation/drafts/data/word-lists/kjv_bible_wordlist.txt")
alg5 = GaussianNB() $ alg5.fit(X_train, y_train) $ probs = alg5.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
random_numbers['2015-09-01':'2015-12-31'].idxmax() # for third 4 months Sep to December
dfRegMet2016 = dfRegMet[dfRegMet.index.year == 2016 or dfRegMet.index.year == 2015 or dfRegMet.index.year == 2014 or dfRegMet.index.year == 2013]
mlb_national_west_league_id = 9 $ url = form_url(f'leagues/{mlb_national_west_league_id}/teams', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
df_license_activities.head(2)
active_with_type.head()
df.columns # get the list of columns names
predict = 0.5
cityID = '7f061ded71fdc974' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Montgomery.append(tweet) 
chefdf = pd.merge(chefdf, chef11df,  how='left', left_on=['name', 'user'], $                   right_on = ['name', 'user'], suffixes=('','_11'))
ldamodel_Tesla= models.ldamodel.LdaModel(corpus_Tesla, num_topics=3, id2word = dictionary, passes=20)
data['Language'] = ['German', 'Danish', 'Dutch']  #Add a new column from a list.
df.mean()
pc_cz_fl.index.values
df_test.isnull().any()
details.drop(['budget', 'original_language', 'overview', 'production_companies', 'production_countries', 'spoken_languages', 'status', 'tagline', $               'production_companies_number', 'production_countries_number', 'revenue', 'spoken_languages_number', 'imdb_id', $               'original_title' $              ], axis=1, inplace=True)
import keras.backend as K $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
data_tickers = data.set_index('date').pivot(columns='ticker') $ data_tickers.head()
logit_mod = sm.Logit(df5['converted'], df5[[ 'CA', 'US']]) $ results = logit_mod.fit() $
pm_data = pd.read_csv('../data/predictive_maintenance_dataset.csv') $ print(pm_data.shape) $ pm_data.head()
df2[df2.duplicated('user_id')]
loans_df = loans_df.query('home_ownership != "ANY"')
clean_liverp = pd.read_csv('clean_liverp.csv') $ clean_liverp.head()
text1 = textract.process(datapath3 / 'textimage.png').decode('utf-8') $ print(text1[0:900])
recent_prcp_data = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date >= YrFrombd).\ $     order_by(Measurement.date).all() $ print(recent_prcp_data) $
grid_search.best_estimator_.score(test, test['Visits'])
scores = pd.read_csv('./data/fandango_score_comparison.csv')
to_plot.plot.hist(by='Days Between Int', bins=100); $ plt.title("Actual number of days between purchases") $
submission_full.proba.sum()
df_2009['bank_name'] = df_2009.bank_name.str.split(",").str[0] $
nba_df.values[0:3]
pair.plot(y=['ann_return', 'ann_' + stock1, 'ann_' + stock2]) $ plt.title(stock1 + '-' + stock2 + 'Trading Strategy Cumulative Returns');
plt.boxplot(cc.close_ratio) $ plt.title("Close Ratio") $ plt.show()
df_predictions['p1_dog'].value_counts()
df2.country.unique()
tweets_csv=load_csv('Twitter.csv') $ tweets_csv.head()
for idx in featuresToPreprocessByMean: $     store[idx] = store[idx].fillna(np.mean(store[idx]))
cpq_business.head()
df.head()
df2.groupby('group')['converted'].mean()
sns.distplot((df['kilometer']),hist=False, color="g",kde_kws={"shade": True}) $ plt.xlabel("kilometer " , fontsize=20) $ plt.ylabel("percentage" , fontsize=20) $ plt.title('Vehicle kilometers', fontsize=20)
len(pres_df['subjects'][0].split())
brand_dict = {} $ for brand in top_5_percent.index: $     price_mean = autos_pr[autos_pr['brand'] == brand]['price'].mean() $     brand_dict[brand] = price_mean $ print(brand_dict)
tweets_rt = pd.DataFrame(ndarray_rt)
to_be_predicted_Day3 = 38.27978917 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
S.decision_obj.stomResist.value = 'BallBerry' $ S.decision_obj.stomResist.value
rule_one_above = df[(df['X'] > x_chart_ucl)] $ for i in range(0, rule_one_above.shape[0], 10): $     display_html(rule_one_above.iloc[i:i+10].T)
n_old = df2.query('landing_page == "old_page"')['user_id'].nunique() $ n_old
feature_names = ('sepal length', 'sepal width', 'petal length', 'petal width') $ sfs1 = sfs1.fit(X, y, custom_feature_names=feature_names) $ sfs1.subsets_
uber_15["month"] = uber_15["Pickup_date"].apply(lambda x: int(x[5:7])) $ uber_15["day_of_month"] = uber_15["Pickup_date"].apply(lambda x: int(x[8:10])) $ uber_15["day_of_year"] = uber_15["Pickup_date"].apply(lambda x: x.split(" ")[0]) $ uber_15.head()
df['Clean Tweets'] = np.array([clean_tweet(tweet) for tweet in df['Tweets']])
dta.join((visit_num.reset_index(level=[0, 1], drop=True) $  .to_frame() $  .rename( $     columns={'inspection_date': 'num_fails'} $ )))
from pandas_datareader.nasdaq_trader import get_nasdaq_symbols $ symbols = get_nasdaq_symbols() $ print(symbols.ix['IBM'])
labels = (tdf.smoker == 'Yes').astype(float) $ numerical = ['total_bill', 'tip', 'size'] $ num_data = tdf[numerical] $ num_data.sample(5)
hm = pd.read_csv('Resources/hawaii_measurements.csv') $ hs = pd.read_csv('Resources/hawaii_stations.csv')
pgh_311_data['REQUEST_TYPE'].value_counts(ascending=True).plot.barh(figsize=(10,50))
soft_outliers_fare = outlier_detection.outlier_detection_serie_1d('fare',cutoff_params=outlier_detection.basic_cutoff) $ strong_outliers_fare = outlier_detection.outlier_detection_serie_1d('fare',cutoff_params=outlier_detection.strong_cutoff)
contractor[contractor.contractor_id.duplicated() == True]
MNB = SklearnClassifier(MultinomialNB()) $ MNB.train(train_set)
routes = json.loads(requests.get('{0}index/agencies/{1}/{2}/routes'.format(base_url, '1', '1')).text) $ print(routes)
n_old = df2[ df2['landing_page'] == 'old_page' ]['user_id'].count() $ n_old
%timeit re.sub('[^a-z ]+','','hello 1923worl 234 shad54awo')
print(train['first_browser'].unique())
end_of_period = pd.Timestamp("2018-07-30") $ datediff = lambda x: (end_of_period - x) $ df_r["Recency (1)"] = (df_r.groupby("CustID")["BookingDate"].transform(datediff)).dt.days $ df_r["Recency (3)"] = df_r["Recency (1)"]
sample.head()
recommend.head()
df[df.sentiment == 0].index
df_archive["rating_denominator"].value_counts()
campaigns['is_click'].sort_values(ascending = False)[0:5]
run txt2pdf.py -o "OROVILLE HOSPITAL  Sepsis.pdf"   "OROVILLE HOSPITAL  Sepsis.txt"
times=[] $ for df in dfs: $     times.append(df['Time'][df['Outside Temperature'] == df['Outside Temperature'].max()].values[0]) $ print (times)    
ac['Monitoring Start'].describe()
df.count()
genre_most_favorite = list(sales.groupby(['Genre'])['Global_Sales'].sum().sort_values(ascending = False)[:7].index) $ sales_genre_year = sales.groupby(['Genre','Year_of_Release'])['Global_Sales'].sum() $ sales_genre_year = sales_genre_year.reset_index().set_index('Year_of_Release')
df2.converted.mean() $ df2.describe().loc['mean'].converted
malebyphase = malemoon.groupby(['Moon Phase']).sum().reset_index() $ malebyphase
csv = 'clean_tweet.csv' $ my_df = pd.read_csv(csv,index_col=0) $ my_df.head()
bacteria_data.value[5] = 1130
daily_returns = calc_daily_returns(closes) $ daily_returns.plot(figsize=(8,6));
grouped_dpt_city.aggregate(np.mean) # means based on two groups
def filter_col(df, cols): $     return df.drop(df.columns[[cols]], axis=1) $ ltc = filter_col(ltc, [0,2,3,4,5,6,7,8,9,11,12,13,14,15,16,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,39]) $ eth = filter_col(eth, [0,2,3,4,5,6,7,8,9,11,12,13,14,15,16,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,39]) $ xrp = filter_col(xrp, [0,2,3,4,5,6,7,8,9,11,12,13,14,15,16,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,39]) $
airports_by_city = df2.groupby('OriginCityName')['Origin'].unique()
proc = processor(hueristic_pct_padding=.7, keep_n=5000) $ vecs = proc.fit_transform(target_docs)
random.sample(labels.items(), 25)
autos["odometer_km"].value_counts()
warnings.filterwarnings("ignore", 'This pattern has match groups') $ faulty_rating_id = archive_clean[archive_clean.text.str.contains( r"(\d+\.?\d*\/\d+\.?\d*\D+\d+\.?\d*\/\d+\.?\d*)")].tweet_id $
from sklearn import preprocessing $ X = preprocessing.StandardScaler().fit(X).transform(X) $ X[0:5]
df2['user_id'].index.name == countries_df['user_id'].index.name
evaluator.get_metrics('phrase_level_results')
df.head()
annotations_df['HOWMANY'] = annotations_df.HOWMANY.astype(float)
data.sample(5)
seed = 2210 $ (train1, dev1, modeling2) = (modeling1 $                              .randomSplit([0.4, 0.1, 0.5], seed=seed))
goo.info()
df = pd.read_sql_query("select * from track limit 3 offset 5 ;", conn) $ df
inputs = sagemaker_session.upload_data(path='namesdata', key_prefix='namesdata')
avg_longest_interval = interval_c.mean() $ res_c_i.iloc[:, -1] = res_c_i.iloc[:,-1].fillna(avg_longest_interval + res_c_i.iloc[:, -2])
pprint.pprint(treaties.find_one({"reinsurer": "AIG"}))
extract_all.loc[(extract_all.APPLICATION_DATE_short>=datetime.date(2018,6,1)) $                &(extract_all.DEC_ALL_DECLINE_REASONS.isin([1000000000])) $                &(extract_all.DEC_FINAL_DECISION.isin(['D']))].sort_values('app_branch_state').groupby('app_branch_state').size()
y = df_train['is_churn'] $ X = df_train.loc[:, df_train.columns != 'is_churn']
df_bkk['place'] = df_bkk['place'].apply(lambda x: translate(x,'en')) $ df_bkk['text'] = df_bkk['text'].apply(lambda x: translate(x,'en'))
1/np.exp(results_countries.params[1])
from sklearn.linear_model import LogisticRegression
basedate = data.Date.sort_values()[0] $ data['DaysSinceGame0'] = list(map(lambda d: (d - basedate).days, data.Date)) $ data.drop(['Date'], axis=1, inplace=True) $ data.head()
twitter_archive_df.columns
df3['ab_UK'] = df3['ab_page']*df3['UK'] $ df3['ab_CA'] = df3['ab_page']*df3['CA'] $ new_model = sm.Logit(df3['converted'], df3[['intercept','ab_UK','ab_CA','ab_page','CA','UK']]) $ new_result = new_model.fit() $ new_result.summary()
dblight["ViolationStreetName"] = dblight["ViolationStreetName"].apply( $      lambda x: re.sub("( ST\.?| DR\.?| RD\.?| BLVD\.?)","",x).strip() $ ) $ blight_gb = dblight.groupby(["ViolationStreetNumber","ViolationStreetName"],as_index=False).sum() $ print "Before aggregation", len(dblight),"rows. After aggregation",len(blight_gb),"rows"
job_a_requirements = pd.DataFrame(requirements, columns = ['Job_A']) $ job_a_requirements
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results = log_mod.fit() $ results.summary() $
from keras.models import Sequential $ from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout $ from keras.optimizers import SGD $ from keras.utils import np_utils
sox.ix[(pd.to_datetime(sox.date) >= dt.datetime(2013,10,4)) & (pd.to_datetime(sox.date) < dt.datetime(2014,1,1)),'playoff'] = 1
a_tags = soup.find_all('a') $ for link in a_tags: $     print(link.get('href')) $
rankings_USA.plot(y ='rank', x = 'rank_date')
meeting_status = pd.read_csv('./data/MeetingStatus.csv')
data = {'Integers' : [1,2,3], $         'Floats' : [4.5, 8.2, 9.6]} $ df = pd.DataFrame(data) $ df
tweets.loc[tweets['sentiment_score'] == 0,'sentiment'] = 'neutral' $ tweets.loc[tweets['sentiment_score'] > 0,'sentiment'] = 'positive' $ tweets.loc[tweets['sentiment_score'] < 0,'sentiment'] = 'negative'
correlation_table = df_return[['.FTSE', 'GB 10Y GILT', 'GB 7Y GILT', 'GB 2Y GILT', 'GB 3M T-BILL','GBPEUR', 'GBPUSD', 'Dummy Variables']] $ correlation_table = correlation_table.corr() $ correlation_table = np.round(correlation_table, decimals = 3) $ correlation_table $
test_pd = test.pivot_table(values = 'Visits', index = 'Page', columns = 'date')
df['day'] = df.date.dt.dayofweek
prediction_clean = prediction.copy() $ prediction_clean['p1'] = prediction_clean['p1'].apply(lambda x: x.lower()) $ prediction_clean['p2'] = prediction_clean['p2'].apply(lambda x: x.lower()) $ prediction_clean['p3'] = prediction_clean['p3'].apply(lambda x: x.lower())
max_ch_ol1 = max(abs(v.close-next(islice(o_data.values(), i+1, i+2)).close) for i, v in enumerate(o_data.values()) if i < len(o_data)-1) $ print('A one liner using islice: {:.2f}'.format(max_ch_ol1))
over_thresh_dict = {} $ over_winnings_dict = {} $ for thresh in np.linspace(.5, .66, 16): $     over_thresh_dict[thresh] = sum([x[1] > thresh for x in pred_probas_over_fm]) $     over_winnings_dict[thresh] = np.mean(y_test_over[[x[1] > thresh for x in pred_probas_over_fm]]) $
f_lr_hash_modeling2 = (prediction_modeling2 $                        .withColumn('f_lr_hash_inter2_2p18_noip', get_pred_udf(col('probability'))) $                        .select('id', 'f_lr_hash_inter2_2p18_noip')) $ f_lr_hash_modeling2.show(3)
shows[['release_date', 'first_year']].isnull().sum()
p_new_act = df2.query('group == "treatment"')['converted'].mean() $ p_old_act = df2.query('group == "control"')['converted'].mean() $ p_diff_act = p_new_act - p_old_act $ (np.array(p_diffs) > p_diff_act).mean()
impressions_products.head(2)
cursor = conn.cursor()
clfgtb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x, y) $ print(clfgtb.score(x_new, y_new)) $ feature_importances = np.stack([new_feature_cols, clfgtb.feature_importances_], axis=1) $ print(np.sort(feature_importances, axis=0)[::-1])
old_page_converted = np.random.choice(2,size=n_old,p=[1-p_old,p_old])
from quantopian.pipeline.factors import AverageDollarVolume, SimpleMovingAverage
pd.set_option('display.float_format', lambda x: '%.0f' % x)
results_ball_rootDistExp, output_ball_rootDistExp = S.execute(run_suffix="ball_rootDistExp", run_option = 'local')
toy['encrypted_customer_id'].nunique()
actual.head()
submit.to_csv("properati_dataset_sample_submision3.csv", index = False)
from nltk.tokenize import word_tokenize
move_3_union = sale_lost(breakfastlunchdinner.iloc[3, 1], 20) $ adjustment_2 = move_1_union + move_2_union $ print('Adjusted total for route: ' + str(move_34p14u14u - move_3_union))
datetime_df.Time.dt.weekday_name.head()
api = tweepy.API(auth)
train_session.shape
log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit()
df.mean(axis='columns')
water_body = 'SE654470-222700' $ temp_df = w.get_filtered_data(step = 2, subset = subset_uuid, indicator = 'din_winter', water_body = water_body)[['SDATE','MONTH', 'WATER_BODY_NAME', 'VISS_EU_CD', 'DIN','SALT_CTD', 'SALT_BTL']].dropna(thresh=7) $ print('Waterbodys left: {}'.format(temp_df.loc[temp_df['VISS_EU_CD'].isin([water_body.strip('SE')])]['WATER_BODY_NAME'].unique())) $ temp_df.loc[temp_df['VISS_EU_CD'].isin([water_body])] $
df_page = pd.get_dummies(df2['group']) $ df_page.drop('control', axis=1, inplace=True) $ df2['ab_page'] = df_page $ df2.head()
[doc['id'] for doc in resp.json()]
from scipy import sparse # Need this to create a sparse array $ scalingDF_sparse = sparse.csr_matrix(scalingDF)
df_raw['rank_this_week'] = pd.to_numeric(df_raw.rank_this_week) $ df_raw['rank_last_week'] = pd.to_numeric(df_raw.rank_last_week, errors='coerce')
outfilename="namesAndFavColors.parquet" $ !rm -rf $dir/$outfilename $ df2.write.save(dir+"/"+outfilename) $ !ls -ld $dir/$outfilename
gas_df = gas_df.set_index('MONTH') $ gas_df.head()
Xtr_scale = np.hstack((Xtr_scale,Xtr[:,7:])) $ Xts_scale = np.hstack((Xts_scale,Xts[:,7:]))
lists = pd.io.json.json_normalize(data['results']['lists'])
df.plot() $ plt.show()
shoppingList = pd.Series(["apple", "cherry", "banana", "cucumber"]) $ priceCategory = {"apple": "$3", "cherry": "$20", "banana": "$1.5", "cucumber": "$3"}
cur = sqlc.cursor()
len(train_data[(train_data.vehicleType == 'kleinwagen') & (train_data.gearbox == 'manuell')])
sampleDF.info()
data['funny'].shape
X_train, X_test, y_train, y_test = train_test_split(stock.iloc[915:-1].drop(['target', 'target_class'], 1), stock.iloc[915:-1]['target_class'], test_size=0.3, random_state=42)
gene_df['gene_id'].unique().shape
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
import statsmodels.api as sm $ convert_old = df2.query('group =="control"')['converted'].sum() $ convert_new = df2.query('group =="treatment"')['converted'].sum() $ n_old = len(df2.query('group == "control"')) $ n_new = len(df2.query('group =="treatment"'))
doc_topic = model.doc_topic_ 
print(list(output.simulations[0]['thermo'].keys()))
train = energy.copy()[energy.index < valid_start_dt][['load']]
csvData[csvData['street'].str.match('.*West.*')]['street']
yc_new1.rename(columns={'incomePC':'income_departure'}, inplace=True) $ yc_new1.columns
print(bigdf.shape) $ print(bigdf_read.shape)
ben_dummy = ben_final.loc[:,['diffs','userid','pagetitle']]
tweets_df.entities.value_counts()
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
f_df = new_df[features_col]   # features $ t_df = new_df['meantempm']    # target $ mu = f_df.mean() $ sigma = f_df.std() $ n_f_df = (f_df - mu) / sigma $
def pratio(x, y): $     if isinstance(x, float) or isinstance(y, float): $         return None $     return fuzz.ratio(x, y)
import os $ cwd = os.getcwd()
new_user = len(df2.query("group == 'treatment'")) $ users=df2.shape[0] $ new_user_p = new_user/users $ print(new_user_p)
top_supporters.plot.barh().set_yticklabels(top_supporters.contributor_cleanname) $ plt.xlabel("amount") $ plt.ylabel("contributor name") $ plt.title("Contribution Amount")
snt = analyser.polarity_scores(s) # polarity_scores(s) $ snt
sel_df[sel_df.BlockRange.isnull()].head()
df_users_6.loc[df_users_6['created']<'2017-07-01','DSA_account_created_before_or_after']='Before'
S.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
mars_weather = last_tweet.find_by_tag('p').text
plt.plot(x, y) $ X=np.linspace(1,len(x),len(x)) $ plt.plot(X, t0+t1*X, "r") $ plt.show()
if 0 == go_no_go:   $     pyLDAvis.display(lda_vis_serialized)
req = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY) $ afxdata = req.json() $ afxdata
plt.plot(Q1) $ plt.plot(T1)
len(testObjDocs.outDF)  # current length of DF
exiftool -csv -createdate -modifydate ciscih8/CISCIH8_cycle1.mp4 ciscih8/CISCIH8_cycle2.mp4 ciscih8/CISCIH8_cycle3.mp4 ciscih8/CISCIH8_cycle4.mp4 ciscih8/CISCIH8_cycle5.mp4 ciscih8/CISCIH8_cycle6.mp4 > ciscih8.csv
print("Number of Mitigations in Mobile ATT&CK") $ mitigations = lift.get_all_mobile_mitigations() $ print(len(mitigations)) $ df = json_normalize(mitigations) $ df.reindex(['matrix', 'mitigation', 'mitigation_description', 'url'], axis=1)[0:5]
deaths_liberia =dropped_liberia.loc['Total death/s in confirmed, probable, suspected cases'] $
df2.shape
plt.figure(figsize=(10,5)) $ plt.plot(df['datetime'], df['distance']) $ plt.xlabel('time') $ plt.ylabel('distance') $ plt.show()
df_pivot.shape
print("5 recent tweets:\n") $ for tweet in tweets[:5]: $     print(tweet.text, '\n')
df_clean.columns
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = df2.query("landing_page == 'old_page'").count()[0] $ n_new = df2.query("landing_page == 'new_page'").count()[0]
image_predictions_clean = image_predictions_clean[(image_predictions_clean.p1_dog == True) | $                                                   (image_predictions_clean.p2_dog == True) | $                                                   (image_predictions_clean.p3_dog == True)]
df.sort_values(by=['title','num_comments'],inplace =True) $ df.head()
result['type'].value_counts()
recentd =[] $ recentd =session.query(Measurement.prcp).\ $     filter(Measurement.date > '2016-08-24').\ $     order_by(Measurement.date).all()\ $
print('Linear Support Vector Classifier Accuracy:', nltk.classify.accuracy(LSVC, test_set) * 100, '%')
e_p_b_two.rename(columns = {0:"Count"},inplace=True)
df8_lunch[df8_lunch > 9].count() $
df_test.head()
csv2 = pd.read_csv(csv_files[1]) $ print(csv2.head())
chinadata.index
logit_mod3=sm.Logit(df_new['converted'],df_new[['intercept','UK','US','new_page','new_page_UK','new_page_US']]) $ results3=logit_mod3.fit() $ results3.summary2()
p_null = df2['converted'].mean() $ p_null
srs_target_counts = df_train.is_attributed.value_counts() $ print('{:0.3f} % of rows are attributed (target = 1)'.format(srs_target_counts[1] / srs_target_counts[0] * 100))
bucket.upload_dir('data/city-util/raw', 'city-util/raw', clear_dest_dir=True)
plt.hist(full_clean_df['type']) $ plt.ylabel('Number of Doggies') $ plt.xlabel('Dog Type') $ plt.title('Which Dog Type is the Most Common')
with open('map_data.json', 'w') as the_file: $     for item in coordinates: $         the_file.write("{}".format(item)) $ pd.DataFrame(coordinates).to_json('map_data.json', orient='records')
n_new = df2.query("landing_page=='new_page'").shape[0] $ n_new
del data_scrapped['index'] $ data_scrapped.shape
pattern = re.compile('AA') $ print(pattern.match('AAbc')) $ print(pattern.match('bcAA'))
sumToRG = toRG.sum()
lr2 = LinearRegression() $ lr2.fit(train_data, train_labels)
all_df.dtypes
v_invoice_sat.shape
van15_fin["stiki_mean"] = van_scores.groupby('userid')['stiki_score'].mean()
btime3 = ph.phimage(dirname) $
tablename='payments' $ pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False).head(10)
df.select(a / 100).show(5)
for c in classes: $     for i in df[c].iteritems(): $         if math.isnan(i[1]): $             df[c][i[0]] = j $         j = i[1] # prev number
cpi_sdmx.names['Dimension'][1]
ogdata=pd.read_csv('datafinal.csv') $ ogxprep = ogdata.copy() $ ogxprep.drop(['irlco', 'Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True) $ ogxprep.columns.values[2] = '1hr'
sf = dataframe.groupby(['year','month']).daily_worker_count.sum()
tmp = API.GetStatus(df_tweets.index[0])
wgts['0.encoder.weight'] = T(new_w) $ wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w)) $ wgts['1.decoder.weight'] = T(np.copy(new_w))
zscore, pvalue = sm.stats.proportions_ztest([convert_old, convert_new], [nold, nnew], alternative='smaller') $ print(zscore, pvalue)
X_test = test_df.drop(['target', 'true_grow'], 1) $ y_test = test_df.true_grow
f_channel_hour_clicks.show(1)
finaldf.to_csv('irl_votes.csv')
sum(clean_rates.name.str.contains(r'^[a-z]'))
instance_details = client.service_instance.get_details() $ print(json.dumps(instance_details, indent=2))
df2[df2['group'] == 'control']['converted'].mean() #going directly by mean unlike below step
reddit['Evening/Night Hours'] = reddit['Hours'].apply(lambda x: 1 if x<10 and x>5 else 0)
tweet_archive_clean = tweet_archive_clean[(tweet_archive_clean.in_reply_to_status_id.isnull()) & $                                           (tweet_archive_clean.retweeted_status_id.isnull())] 
c_numer = [0, 420, 666, 1776] $ archive_df[archive_df.rating_numerator.isin(c_numer)][['text', 'rating_numerator', 'rating_denominator']]
df_train.columns[np.array(list(map(lambda x: sum(df_train[x] == value), df_train.columns))) != 0]
grid.fit(Xtrain, ytrain)
model_arima121 = ARIMA(AAPL_array, (1,2,1)).fit() $ print(model_arima121.params)
weekdays_avg.reset_index(inplace=True) $ weekdays_count.reset_index(inplace=True) $ weekends_avg.reset_index(inplace=True) $ weekends_count.reset_index(inplace=True)
plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101_d[df_nd101_d['ud120']>0].ud120)
data2['date'] = pd.to_datetime(data2['date'],unit='s')
!curl -XGET 'http://elasticsearch:9200/logstash-2018.03.01/_mapping?pretty=true'
loans_act_arrears_xirr=cashflows_act_arrears_investor.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
pmean = np.mean([p_new,p_old]) $ round (pmean, 4)
Difference=ControlConverted-TreatmentConverted $ print("The propotion of p_diffs greater than actual differnce observed equals",(null_values > Difference).mean())
to_be_predicted_Day3 = 34.57440189 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
spp['season'] = spp.index.str.split('.').str[0] $ spp['term'] = spp.index.str.split('.').str[1]
returns.tail()
from sklearn.cross_validation import train_test_split $ x_train, x_test, y_train, y_test = train_test_split(x_final,y_final,test_size = 0.2,random_state = 0) # Do 80/20 split
classifier = train_classifier(articles_train, authors_train, articles_test, authors_test, $                               SVC(kernel='linear')) $ print(classifier.score(test_features_tokenized, authors_test))
len(test5result.columns)
climate_df.describe() $
df2.loc[df2['user_id'] == 773192]
plt.savefig('News Mood Analysis.png')
df_grp = df.groupby('group') $ df_grp.describe()
X_train = houses_train.loc[:, houses_train.columns != "SalePrice"].values # convert to np.array $ y_train = houses_train.loc[:, houses_train.columns == "SalePrice"].values.reshape(-1, ) $ X_test = houses_test.loc[:, houses_train.columns != "SalePrice"].values # convert to np.array
temp_err = temp_table["Max-Temp"]-temp_table["Min-Temp"] $ max_temp = session.query(func.max(Measurements.tobs)).all() $ temp_upper_lim = int(pd.to_numeric(max_temp[0])) + 20 $ temp_lower_lim = temp_table["Min-Temp"].sum()-10 $ temp_avg = temp_table["Avg-Temp"].sum()
data.dropna().describe()
df2 = df2.drop(2893) $ df2[df2.user_id.duplicated(keep = False)]
output.count() $ output.printSchema() $ output.show(2)
idxslice = pd.IndexSlice
data = users.merge(df_first, left_on=['UserID'], right_on=['UserID'], how='left' ) $ data.drop(['key'], axis = 1, inplace = True)  # Column 'key' was added in a previous step that is now dropped $ data
X = bikedataframe[names] $ X.shape
data_spd['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_spd['tweets'] ])
quiet = open("quiet.json").read() $ print(type(quiet),"\n") $ print(quiet)
from sklearn.model_selection import train_test_split $ from sklearn.linear_model import LogisticRegression $ from sklearn import linear_model $ from sklearn.metrics import accuracy_score
autos.info() $ print(autos.head(10))
base_df = sqlContext.read.text(log_file_path) $ base_df.printSchema()
df['edition'].value_counts()
mpbrand_series = pd.Series(mean_price_by_brand).sort_values(ascending=False) $ mpbrand_series
pd.date_range('2015-07-03', periods=9)
temp_df2 = temp_df.drop_duplicates()
fb_day_time_gameless = fb_day_time[fb_day_time.service_day.isin(all_game_dates) == False]
df.landing_page.value_counts()
sorted(data.date.tolist())[-1], sorted(data.date.tolist())[-1][0:4]
weather.head()
y_train = pd.DataFrame(y_train, columns=['country'], index=X_train.index)
sns.factorplot('lgID',hue='divID',kind='count',data=wcPerf1_df)
df_compare.head()
lastYear = dt.datetime.today() - dt.timedelta(days=365)
merged.amount.sum()
HAMD = HAMD[HAMD["level"]=="Enrollment"] $ HAMD = HAMD.dropna(axis=1, how='all') $ HAMD.columns
hr[hr["icustay_id"]==14882].plot(x="new charttime", $                                  y=["chart delta"])
clients = pd.read_csv('data/clients.csv', index_col=0) $ clients
atdist_noinfo_opp_dist_tabledata = atdist_opp_dist_noinfo_count_prop_byloc.reset_index() $ create_study_table(atdist_noinfo_opp_dist_tabledata, 'locationType', 'emaResponse', $                    location_remapping, atdist_noinfo_response_list)
%%time $ in_degree_centrality = convert_dictionary_to_sorted_list(nx.in_degree_centrality(network_friends))
pickup_coords = data.loc[:,['pickup_lat','pickup_lon']] $ dropoff_coords = data.loc[:,['dropoff_lat','dropoff_lon']] $ ride_coords = data.loc[:,['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon']]
ga4,ga3,gsc,cred = ga.build_service(code)
tbl_ = df.groupby('msno').is_cancel.sum().to_frame() $ tbl_.columns = ['cancel_total_count'] $ tbl_.reset_index(inplace = True)
df_small = df.loc[(df['doggo'] != 'None') | (df['floofer'] != 'None') | (df['pupper'] != 'None') | (df['puppo'] != 'None')]
conn.dropcaslib('research')
df.Date.dtype
goodWeather = barcelona.ix[datetime(2014,7,1) : datetime(2014,9,30)]
holdout_idx = math.ceil(.15*len(X_main))
s = Series(randn(len(rng)), rng) $ s.head()
s1.iloc[1]
TEXT.numericalize([md.trn_ds[0].text[:12]], device=-1)
ad_group_performance.loc[5]
df.info() $ df.isnull().sum() $
tweets_df_clean[tweets_df_clean['id'].duplicated()]
batch_size = tf.shape(X)[0] $ W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name="W_tiled")
temps_df.columns
tweets['rating_denominator'].nunique() $
log_mod = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']])
seconds = station_distance['Trip Duration'].tolist() $ seconds_len = (len(seconds)) $ minutes_list = [] $ def get_min(): $     return seconds[i] / 60
data.isnull().sum()
df.head()
!more "files/approved users.txt"
dr_num_new_patients = dr_new['id'].resample('W-MON', lambda x: x.nunique()) $ dr_num_existing_patients = dr_existing['id'].resample('W-MON', lambda x: x.nunique())
arr = bg_df.values.reshape(7, 3) # reshape our 21 values into 3 columns; becomes ndarray $ bg_df2 = pd.DataFrame(arr) # convert back to DataFrame $ bg_df2
temps2  = pd.Series([70,75,83,79,77, 69], index = dates) $ temps_diffs = temps1 - temps2 $ temps_diffs
classes.head(1)
autos = autos.replace({"num_doors": numbers, "num_cylinders": numbers}) $ autos.head()
ks_goal_failed = ks_goals.drop(ks_goals.index[ks_goals.state != 'failed']) $ ks_goal_failed = ks_goal_failed.drop(ks_goal_failed.index[ks_goal_failed.counts < 300]) $ ks_goal_failed = ks_goal_failed.sort_values(by = ['goal'], ascending = True) $ ks_goal_failed.set_index('goal', inplace=True)
%matplotlib notebook $ pnls.plot()
validation.analysis(observation_data, simple_resistance_simulation_0_5)
data.loc[(80, slice(None), 'put'), :].iloc[0:5, 0:4]
temperature_sensors_df['state'][ $     temperature_sensors_df['entity_id'] == 'sensor.darksky_sensor_temperature'].hist( $     bins=50); $ plt.title("Outside temperature"); $ plt.xlabel("Temperature $^\circ$C");
csvData['street'] = csvData['street'].str.replace(' Northeast', ' NE') $ csvData[csvData['street'].str.match('.*North.*')]['street']
clf = MultinomialNB() $ clf.fit(train_data, train_labels) $
b_df = final_.copy()
target_pf.iloc[-1]['date']
df2=df2[df2.index!=2893]
index = pd.DatetimeIndex(dates)
analyzer.polarity_scores("The book was not good.")
with tf.Session() as session: $     session.run(model) $     print(session.run(y))
html = browser.html $ twitter_news = bs(html, 'html.parser')
import os $ import zipfile $ filenames = os.listdir(path_to_zips) $ zip_file = zipfile.ZipFile(path_to_zips + filenames[0])
ideas.duplicated().sum()  # Checking for duplicated data 
logit = sm.Logit(df2['converted'],df2[['intercept','ab_page','CA', 'UK']]) $ results = logit.fit() $ results.summary()
join_d.where(F.col(''))
sessions.tail()
graffiti['created_year'] = graffiti['created_year'].astype(int)
obs_diff=exp-con $ np.array(p_diffs) $ null_valls=np.random.normal(0,np.std(p_diffs),10000) $ plt.hist(null_valls) $ plt.axvline(obs_diff,c="red")
TrainData_ForLogistic.head()
processed_tweets.sample(4)
bus['text'].iloc[0]
file4=file4.dropna() $ file4.count()
sns.set_style("darkgrid") $ plt.figure(figsize = (10,5)) $ sns.barplot(months.index, months.values, palette= "GnBu_d").set_title("Rides per Month")
(v1,v2,v3,v4,v5,v6)=gid.get_repo_version(git_location = GIT_LOCATION) $
a = tips.loc[:,"tip"] $ a.head()
blob = TextBlob(tweets[0]) $ blob.sentences[:10]
preg.columns[10:30]
log_mod2 = sm.Logit(df2['converted'], df2[['intercept', 'CA', 'UK']]) $ results2 = log_mod2.fit() $ results2.summary()
freeways = gis.content.get('91c6a5f6410b4991ab0db1d7c26daacb') $ freeways
newuser = len(df2.query("group == 'treatment'")) $ user=df2.shape[0] $ newuserprobability = newuser/user $ print(newuserprobability)
df.iloc[0]
from collections import Counter $ x = analyze_set['name'] $ count = Counter(x) $ count.most_common(5)
crimes.columns
page.oldest_revision['timestamp'].strftime("%Y-%m-%d")
print "Mean Time for closing a ticket: %f hours" % (time2close.mean()/3600.0) $ print "Median Time for closing a ticket: %f hours" % (time2close.median()/3600.0) $ print "Quantiles: " $ print time2close.quantile([0.25, 0.5, 0.75])
pold = df2['converted'].mean() $ print(pold) #creating the mean converted values as pnew
import matplotlib.pyplot as plt $ df.drop('Volume', axis=1).plot(figsize=(10,7)) $
X.columns
pd.Series([1, np.nan, 2, None])
X_test, X_val, y_test, y_val = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=23) $ X_train.shape, X_test.shape, X_val.shape $ print("Training instances   {}, Training features   {}".format(X_train.shape[0], X_train.shape[1])) $ print("Validation instances {}, Validation features {}".format(X_val.shape[0], X_val.shape[1])) $ print("Testing instances    {}, Testing features    {}".format(X_test.shape[0], X_test.shape[1]))
merged[merged.committee_position=="OPPOSE"].sort_values("amount", ascending=False)
df.iloc[18248]['AveragePrice']
(autos["ad_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_values() $         )
notus.loc[condition, 'country'] = notus.loc[condition, 'cityOrState']
table1.head(3)
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=30000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
train = pd.read_csv("wikipedia_train3.csv") $ test = pd.read_csv("wikipedia_test3.csv")
c.loc[c>0.7]
old_page_converted = df2.query('converted == "1"').user_id.nunique()
brand_color = le.transform(list(set(df[:1000]['brand'])))
movies_df = pd.read_csv('movies.csv') $ ratings_df = pd.read_csv('ratings.csv') $ movies_df.head()
dfBill.head(10)
alg4 =AdaBoostClassifier() $ alg4.fit(X_train, y_train) $ probs = alg4.predict_proba(X_test) $ score = log_loss(y_test, probs) $ print(score)
prec_group.index = pd.DatetimeIndex(prec_group.index) $ prec_group = prec_group.drop([pd.Timestamp('2014-11-02 01:00:00'), pd.Timestamp('2015-11-01 01:00:00'), pd.Timestamp('2016-11-06 01:00:00')]) $ prec_group.index = prec_group.index.tz_localize('America/New_York')
hmeq_card = cassession.CASTable('hmeq_card', where='_NMISS_ > 0') $ df_hmeq_card = hmeq_card.fetch().Fetch $ df_hmeq_card $
type(df_concat_2.date_series) $ df_concat_2["time"] = df_concat_2.date_series.dt.time.astype(str) $ df_concat_2["time"].head()
nypd_count_df = nypd_df.groupby(['create_date'], as_index=False)['Agency'].count() $ nypd_count_df.columns = ['create_date', 'sidewalk_parking_count']
plt.savefig('Sentiment_fantastic ms.png')
(df2['landing_page'] == 'old_page').sum()
print stations.installation_date.min() $ print stations.installation_date.max()
sherpa = current.loc[df["By Name"] ==  "Sherpa "] $ sherpa
q_specific = c.submit_query(PTOQuerySpec().time("2018-06-10", "2018-06-12") $                                              .target("170.218.212.80"))
%pylab inline $ stackTaskCompleted = gDate_vProject.plot.bar(stacked=True, figsize=(15, 15)) $ stackTaskCompleted.set_ylabel("Tasks Completed") $ stackTaskCompleted.set_title("Weekly Task Completion")
folds = 5 $ n = training_data.shape[0] $ kf = KFold(n,folds,random_state=123)
gp = df[['placeId', 'hashtags']].groupby('placeId')
low[low < myIP[2]].max() $
comments = df.groupby(['subreddit'])['body'].count()
taxi_hourly_df.loc[index_missing, :]
df.query_string.value_counts()
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'}) $ station_count = station[['Count']] $ station_count 
y_train.value_counts()
state_party_df.info()
n_new = df_treatment.nunique()['user_id']
tweets_raw["date"] = tweets_raw["date"].apply(lambda d: parse(d, ignoretz = True))
db_user_scenario_load =pd.read_sql_query( "SELECT * FROM user_scenario_load",conn) $ db_user_scenario_update =pd.read_sql_query( "SELECT * FROM user_scenario_update",conn) $ db_user_scenario_action =pd.read_sql_query( "SELECT * FROM user_scenario_action",conn)
print(imgp_clean[(imgp_clean.p1_dog == True) | (imgp_clean.p2_dog == True) | (imgp_clean.p3_dog == True)].shape) $ print(imgp_clean.shape)
fig, ax = plt.subplots() $ ffr.rolling(window=7).max().plot(ax=ax) $ ffr.rolling(window=7).min().plot(ax=ax) $ ax.legend(["max", "min"])
r.json()['dataset_data']['data']
if not os.path.exists('new_data_files'): $     os.mkdir('new_data_files') $ records2.to_csv('new_data_files/Q3A.csv')
conf_gain = x.loc[x['pred'] > 0.75] $ conf_gain = pd.DataFrame(conf_gain, columns=['pred','pred_std', 'Gain +1d']) $ len(conf_gain.loc[conf_gain['Gain +1d'] == True])/len(conf_gain)
ac['Dispute Resolution Start'].describe()
df_data.head()
weather_yvr_dt.dtypes
df2_no_outliers = df2.copy() $ df2_no_outliers['y'] = np.log(df2_no_outliers['y'])
locations = session.query(Station).group_by(Station.station).count() $ print(locations)
walking_df = df_summary[df_summary['Activity']=='walking'].groupby('Date').sum( $ ).reset_index().sort_values(by='Date', ascending=1) $ walking_df = walking_df.filter(items=['Date', 'Distance', 'Steps']) $ walking_df.rename(columns={'Distance': 'ttl_wal_distance', 'Steps': 'ttl_steps'}, inplace=True)
countries_df = pd.read_csv('./countries.csv') $ countries_df.head() $
df.isnull().sum().max()
plt.hist(last_year_temp, bins="auto", label="tobs") $ plt.legend(loc="upper left") $ plt.ylabel("Frequencey") $ plt.tight_layout()
or_list1 = df.query("group=='control'| landing_page=='old_page'").index
filtered_tweets_df = tweetdf.loc[~tweetdf.latlng.isin(badgeo_df.latlng)]
rf = RandomForestClassifier(n_estimators = 10) $ rf = rf.fit(train_data_features, y_train)
df.mean() #mean 
from pyspark.sql.functions  import to_date $ df = df.withColumn("post_creation_date", to_date(df["post_creation_date"],'yyyy-MM-dd'))
car_data.describe(include='all')
plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('frequency') $ plt.title('Plot of 10,000 simulated p_diffs');
df2.head()
classification_data = pd.merge(classification_df,classification_orgs,left_on = 'company_uuid',right_on = 'uuid', $                               how = 'left')
crimes['date']=crimes['Date'].dt.date $ crimes['date']=crimes['date'].astype(str) $ crimes.head()
from scipy.stats import norm $ norm.cdf(z_score)
ols_model.fvalue
t = datetime.time(1,30,5,2) $ print(t)
%%time $ entity_id_list =  list(engine.execute("SELECT entity_id, COUNT(*) FROM states \ $ GROUP BY entity_id ORDER by 2 DESC"))
modelslist = autodf['model'].value_counts() $ top_models = list(modelslist.index) $ sns.factorplot('model',data=autodf,kind='count',size=50,x_order = top_models) $ p.set_xticklabels(rotation=90) $
len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)
R_weather.info()
df3 = df.copy() $ df3 ['E'] = ['q', 'w', 'e', 'r', 't', 'y', 'g', 'j', 'k', 'l', 'f', 's'] $ df3
def random_coord(): $     xrange = 50 $     cities_df["Lat"] = [np.random.uniform(-90,90) for x in range(xrange)] $     cities_df["Lon"] = [np.random.uniform(-180,180) for x in range(xrange)]
print(autos.price.describe(percentiles=[0.10, 0.90])) #Find the 10th percentile and 90th percentile $ autos = autos[(autos["price"] > 500 ) & (autos["price"] < 14000 )] #Remove outliers using the percentile values
df_archive_clean["name"] = df_archive_clean["name"].replace("a","None").replace("the","None").replace("an","None")
print("Pixel values:\n{}".format(image)) $ print("Value of the pixel at (10,10): {}".format(image[10, 10]))
label_rating = df_twitter.groupby('dog_label')['score_rating'].mean()[1:]
weights, sentiments = sentiment_analyzer(df19.Tweet) $ df19["Senitments"] = sentiments $ df19["Weights"] = weights
ca_all.loc[(~ca_all.Account_ID.isnull())].sample().Account_ID
w_aux=[] $ for i in range(0,l2): $     if i not in seq: $         w_aux.append(w_counts[i]) $ col.append(np.array(w_aux))
s519397_df["prcp"].sum()
import matplotlib.pyplot as plt $ plt.style.use('ggplot')
df.filter(regex='\d')
officers = pd.read_csv('data/outputs/active_officers.csv') $ officers.company_number = officers.company_number.astype(str)
r = requests.get(url) $ json_data = r.json()
%%time $ lr2 = LogisticRegression(random_state=20, max_iter=10000, C= 1, multi_class='ovr', solver='saga') $ lr2.fit(X_tfidf, y_tfidf) $ lr2.score(X_tfidf_test, y_tfidf_test)
au.find_some_docs(uso17_qual_coll,limit=3)
mentions_df.head()
n_old = df2[df2.group == 'control'].count()[0] $ n_old
data.L2.unique()
autos['price'].round(4) $ print(autos['price'].describe()) $ print(autos['price'].value_counts().sort_index().head()) $ print(autos['price'].value_counts().sort_index().head()) $
users_conv = df.query('converted == 1').user_id.nunique() $ print('Proportion of users converted : {:.3f} %'.format(100.0*users_conv/unique_users_num))
result=lm.fit() $ result.summary()
medals_data = medals_data.assign(log_population=np.log(medals.population))
ax = df.plot(title="Adj Close",fontsize=2) $ ax.set_xlabel('Time') $ ax.set_ylabel('Price') $ plt.show()
tmp_cov_1 = tmp_cov.reset_index(level=0) $ del tmp_cov_1['price_date'] $ tmp_cov_1
rdd = sc.parallelize(range(10))
auth_client = gdax.AuthenticatedClient(KEY, $                                        B64SECRET, $                                        PASSPHRASE)
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status','OriginationPhase']).created_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['orange','green', 'blue', 'red', 'black'], grid=False) $
pmol.df['atom_type'].value_counts().plot(kind='bar') $ plt.xlabel('atom type') $ plt.ylabel('count') $ plt.show()
loan_requests[loan_requests.id_loan_request==122466]
p_old_null = df.converted.mean() $ p_old_null
X, y = createDataPoints([[4,3], [2,-1], [-1,4]] , 1500, 0.5)
tweet_df["date"] = pd.to_datetime(tweet_df["date"]) $
people = pd.DataFrame(np.random.randn(5,5),columns=['a','b','c','d','e'], $                      index=['Joe','Steve','Wes','Jim','Travis']) $ people
table_1c.drop(pn_qty[pn]['1cstoreinfo'])
import statsmodels.api as sm
merge[merge.columns[13]].value_counts()
df['ts_year']=df['timestamp'].apply(lambda x:x.year)
RDDTestScorees.groupByKey().collect()
shiny.head()
print('Total records {}'.format(len(non_na_df))) $ print('Start / End : {}, {}'.format(non_na_df.index.min(), non_na_df.index.max()))
d311 = pd.read_csv(dic_inp["detroit-311.csv"], quotechar='"',converters={"lng":float,"lat":float}) $ d311.drop_duplicates(subset="ticket_id") $ d311.rename(columns={"lng":"long"},inplace=True)
vip_df['Date'] = "" $ for i in range(0, len(vip_df)): $     vip_df.loc[i, 'Date'] = datetime.strptime(vip_df.loc[i, 'Time'], "%m/%d/%Y %H:%M:%S").date() $     vip_df.loc[i, 'Time'] = datetime.strptime(vip_df.loc[i, 'Time'], "%m/%d/%Y %H:%M:%S").time()
rate_pnew_null = sum(df2['converted'])/df2.shape[0] $ rate_pnew_null
df['Website'] = df['Website'].str.rstrip('/')
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False) $
X_mice_dummify, _ = custom_dummify(X_mice, 0.01)
def normalize_event_properties_column(df): $     df_event_properties = json_normalize(data=df.event_properties) $     df_event_properties = df_event_properties.set_index(df.index) $     standardize_column_names(df_event_properties) $     return df.merge(df_event_properties, left_index=True, right_index=True) $
prob_ind_conv = df2[df2["converted"] == 1].shape[0]/df2.shape[0] $ prob_ind_conv
co_buildings_latlong.head()
Station = Base.classes.station $ stations = session.query(Station).count() $ print(f'There are {stations} stations in the database')
plt.style.use('default')
trunc_df.description = trunc_df.description.astype(str) $ vectorizer = TfidfVectorizer(max_df=0.5, max_features=1000, min_df=2, stop_words='english',use_idf=True) $ X = vectorizer.fit_transform(trunc_df.description)
spark.sql("select count(*) from ufo_data").toPandas()
t_retweets = pd.Series(data=data['RTs'].values, index=data['Date'])
from matplotlib.pyplot import figure $ figure(num=None, figsize=(14, 3), dpi=80, facecolor='w', edgecolor='k') $ names = list(age_hist.age) $ values = list(age_hist.age_freq) $ plt.plot(names, values, linewidth=1.0, marker='o', linestyle='-')
stock['target'] = stock.daily_gain.shift(-1)
data = get_fremont_data() $ pivoted = data.pivot_table('Total', index=data.index.time, columns=data.index.date)
cols = ['id_partner', 'Cost_14', 'Cost_30', 'Cost_60'] $ users_orders = users_orders[cols] $ cost_per_user_per_partner_path =  output_path + '\\cost_per_user_per_partner.csv' $ users_orders.to_csv(cost_per_user_per_partner_path)
conn = sqlite3.connect(os.path.join(outputs,'example.db')) $ c = conn.cursor()
print pd.pivot_table(data=cust_demo, index='Location', columns='Gender', values='age',aggfunc='mean' )
myzip.filename
df2 = df2.set_index('user_id') $ df2.head()
df2.query('landing_page == "new_page"').user_id.nunique() / df2.user_id.nunique()
print(array_masked.mean(axis=0))
temp_bar_chart = (calc_temps('2016-08-10', '2016-08-23')) $ labels = ['TMIN', 'TAVE', 'TMAX'] $ df = pd.DataFrame.from_records(temp_bar_chart, columns=labels)
charge_counts = grouped['cause_num'].count()
all_count = df_aggregate.shape[0] $ broken_count = df_aggregate[df_aggregate.iloc[:, 1] >= 1].shape[0] $ print ('number of disks : ',all_count) $ print ('number of failed disks: ',broken_count) $ print ('percentage of broken disks: ',broken_count/all_count*100,'%' )
plt.bar(scale, recscale) $ plt.title("Number of people by likelihood", weight = 'bold', fontsize = 15) $ plt.xlabel("Recommendation Likelihood") $ plt.ylabel("Number of People") $ plt.grid(False)
dbdata['id_os'] = dbdata['id_os'].astype('datetime64[ns]') $ dbdata = dbdata.rename(index=str, columns={"id_os": "Date"}) $ dbdata
struct_v1_0 = sp_df_test.schema $ emptyDF = spark.createDataFrame(sc.emptyRDD(), struct_v1_0) $ emptyDF.write.parquet(os.path.join(comb_fldr, "flight_v1_0.pq"))
ol = cb.organization('origami-logic')
sns.heatmap(temp_fine)
logistic = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ model = logistic.fit()
df.dtypes
df2017=data[data.year == 2017] $ df2017 $
likes.shape
a.keys() & b.keys()
cp = nltk.RegexpParser(grammar) $ result = cp.parse(sentence) $ print(result)
model.fit(X_tr, y_tr) $ preds = model.predict(X_val)
df_campaigns.info()
newdf['Date'] = newdf['Date'].apply(lambda x: x.split('-')[0] + x.split('-')[1])
all_sets.cards = all_sets.cards.apply(lambda x: pd.read_json(json.dumps(x), orient = "records"))
for df in alldfs_list: $     print(df.tail(1))
Conversion_Rate=df2.loc[df['converted']==1,].shape[0]/df2.shape[0] $ print("The convert Rate for p(new) under the null is ",Conversion_Rate)
data = data[data['processing_time']>=datetime.timedelta(0,0,0)]
store_items.isnull()
df['label'] = df[forecast_col].shift(-forecast_out)
import re $ pattern = re.compile('\$\d*\.\d{2}') $ result = pattern.match('$17.89') $ print(result) $ print(bool(result))
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=25000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
value = 152000  # Deletion threshold  (52: 152000) $ index_drop = []  # ['MNO(WT%)', 'ND(PPM)', 'RB(PPM)']  # Deleted elements $ index_keep = []  # ['TA(PPM)', 'HF(PPM)', 'SC(PPM)', 'TH(PPM)', 'CE(PPM)', 'ZR(PPM)', 'NB(PPM)', 'Y(PPM)']  # Preserved elements $ index_ = (a > value) & ~ a.index.isin(index_keep) | a.index.isin(index_drop)
df_columns.groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False]) $
dist = np.sum(train_data_features, axis=0) $ print (dist) $ for tag, count in zip(vocab, dist): $     print (count, tag) $
noLatLon = datatest[datatest.lat.isnull()].groupby('place_name').agg(np.size) $ noLatLon
litecoin_github_issues_url = blockchain_projects_github_issues_urls[3] $ litecoin_github_issues_df  = pd.read_json(get_http_json_response_contents(litecoin_github_issues_url))
from sqlalchemy import create_engine $ cnx = create_engine('postgresql://emilygeller:p@54.173.47.58:5432/emilygeller')
calls_df.describe()
ixs = [ix for ix,b in enumerate([i in dfs[4].index for i in dfs[3].index]) if b==True] $ print(ixs[0:3]) $ dfs[3] = dfs[3][0:ixs[2]] $ df_all = pd.concat(dfs) $ df_all.head() $
df_archive_clean.head()
import pandas as pd $ typ = 'resampled' $ file_name = "data/survey_Brussels_NonResidentialElectricity_wbias_projected_dynamic_resampled_10000_{}.csv" $ sample_survey = pd.read_csv(file_name.format(2016), index_col=0) $ sample_survey.head()
dftest.head()
hrefs = soup.find_all('a', href='http://www.iana.org/domains/example') $ hrefs
df4[['CA','UK','US']] = pd.get_dummies(df4['country']) $ df4.head()
df = pd.merge(releases, bookings, on='hash_id', how='left')
train.age.value_counts().head()
dframe_team.dtypes
num_authors_by_project = authors_grouped_by_id_saved.groupBy("project_name").agg(F.count("Author").alias("author_count")) $ num_authors_by_project.cache() $ num_authors_by_project.show()
from scipy import stats $ results = logit.fit() $ results.summary()
pop.index.names = ['states', 'year'] $ pop
! python -m xport {data_dir}paxraw_d.xpt > {data_dir}paxraw_d.csv
r2_score(y_test, best_model['model'].predict(X_test))
df.toordinal() - datetime(1989, 5, 17).toordinal()
fields_node = columns['fields'] $ fields_node.keys()
df[df.num_comments == max(trumpint.num_comments)]
staff.drop(["Fulltime"], axis=1)
df_total.info()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     f.remove_node(where=f.root.gr1.gr2, name='some_array')
file200601_1dir = os.path.join(filepath, '2006') $ file200601_1name = os.path.join('01Jan', '2.xls') $ file200601_1path = os.path.join(file200601_1dir, file200601_1name)
so = pd.read_csv('../../data/stackoverflow_qa.csv') $ so.head()
merged2.to_pickle("data/merged_2018.pkl")
len(pd.Series(y).unique())
sn.distplot(train_binary_dummy['obs_count'])
unique_title_artist = pd.unique(list(zip(df.loc[:,'title'], df.loc[:,'artist']))) $ len_unique_title_artist = len(unique_title_artist) $ print(len_unique_title_artist) $ unique_title_artist[:10] $ batch_size = 1000
model.score(all_text, all_text.index)
train_preprocessed = train_preprocessed.fillna(-1) $ train_preprocessed.to_csv('train_preprocessed.csv')
ben_fin["stiki_mean"] = ben_scores.groupby('userid')['stiki_score'].mean()
df_wo_slope = df.loc[:,[i for i in list(df) if not "slope" in i]] $ df_wo_slope_sig = (df_wo_slope.iloc[:, 1:(df_wo_slope.shape[1]-6)] > 7.301).sum(axis=1) $ df_wo_slop_less = df_wo_slope.loc[df_wo_slope_sig>0]
article_divs = [item.find('div',{'class':'article--container'}) for item in soups]
text = df_pr[df_pr.item==1].iloc[0].body $ text
crime_wea.head(10)
import statsmodels.api as sm $ convert_old = 17489 $ convert_new = 17264 $ n_old = 145310 $ n_new = 145274
df = pd.DataFrame(bmp_series, columns=["mean_price"]) $ df["mean_mileage"] = bmm_series $ df.head()
url_p1 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&api_key=' $ url = url_p1 + API_KEY $ r = requests.get(url) $ data = r.json()
df_new[df_new['group']=='treatment'].head()
srctake.to_sql(con=engine, name='sourceuse', if_exists='replace', flavor='mysql',chunksize=10000)
a400hz = a400hz.distinct()
tweet.text
df_predictions['img_num'].value_counts()
df.min()  # default axis=0, column-wise
mod = sm.tsa.statespace.SARIMAX(stock.close, trend='n', order=(1,1,3), seasonal_order=(0,1,1,80)) $ results = mod.fit() $ results.summary()
columns = inspector.get_columns(db_tables[0]) $ columns
df_new['US_ab_page'] = df_new['US'] * df_new['ab_page'] $ df_new['CA_ab_page'] = df_new['CA'] * df_new['ab_page'] $ df_new.head()
c_rate_null = (df2["converted"] == 1).mean() ### assumed that is  pnew  and  pold  are equal
avi_data.head(10)
DF.show()
import statsmodels.api as sm $ convert_old = df2[(df2['landing_page'] == 'old_page') & (df2['converted'] == 1)].shape[0] $ convert_new = df2[(df2['landing_page'] == 'new_page') & (df2['converted'] == 1)].shape[0] $ n_old = df2[df2['landing_page'] == 'old_page'].shape[0] $ n_new = df2[df2['landing_page'] == 'new_page'].shape[0]
total_driver=pd.DataFrame(dfa_1) $ driver_count=total_driver.reset_index() $ driver_count
nl=list(chain.from_iterable(ll)) $ nl
autos["brand"].value_counts()
autos.loc[autos["registration_year"]>2017, "registration_year" ].shape
norms_df.plot(kind='area', x_compat=True, alpha=.5, stacked=False) $ plt.tight_layout() $ plt.title("Daily Norms") $ plt.savefig('images/dailynorms.png') $ plt.show()
weeklyTotals = tweets['text'].groupby(tweets['weekNumber']).size()
prophet_df.sample(4)
baseball.reindex(baseball.index[::-1]).head()
df_usage = pd.read_csv(path + "train_usage_data.csv")
lmdict = pd.read_excel('https://www3.nd.edu/~mcdonald/Word_Lists_files/LoughranMcDonald_MasterDictionary_2014.xlsx')
goals_df[['PKG', 'PKA']] = goals_df['PKG/A'].str.split('/', expand=True) $ goals_df.drop('PKG/A', axis=1, inplace=True)
temp = zip(dfUsers['userFromId'],dfUsers['userToId']) $ G.add_edges_from(temp)
df_characters.head(10)
df_ad_state_metro_2.set_index('state', inplace=True) $ df_state_victory_margins.set_index('state', inplace=True)
image_file.head()
defaults = np.seterr(all="ignore") $ Z = np.ones(1) / 0 $ _ = np.seterr(**defaults) $ with np.errstate(divide='ignore'): $     Z = np.ones(1) / 0
df.apply(lambda x: x.max() - x.min())
obs_diff = new_page_converted.mean() - old_page_converted.mean() $ obs_diff
logit = sm.Logit(data['converted'], data[['intercept', 'ab_page', 'UK', 'CA']]) $ result=logit.fit()
room_temp.__delattr__('temperature')
nyt = trump.loc[trump['text'].str.contains("nyt"),:] $ fox = trump.loc[trump['text'].str.contains("fox"),:] $ ax = sns.kdeplot(nyt['polarity'], label="nyt") $ ay = sns.kdeplot(fox['polarity'], label="fox") $
total_users = df2['user_id'].nunique() $ converted_users = df2[df2['converted'] == 1].count() $ conversion_prob = converted_users/total_users $ print(conversion_prob)
p_range = max(v.high-v.low for v in trade_data_dict.values()) $ p_range_date = [v for v in trade_data_dict.values() if v.high-v.low == p_range][0][0] $ print('The largest trading range in 2017 was: {:.2f} on {}'.format(p_range, p_range_date))
austin.drop(3572, inplace=True)
more_recs.to_sql('samples', con, if_exists='append', index=False)
print(df.std()) $ print(df.mean())
bars.open.at_time('9:30')
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.25,random_state=102)
most_active_df = history_df.loc[history_df['station'] == 'USC00519281'] $ max_val = most_active_df['tobs'].max() $ min_val = most_active_df['tobs'].min() $ avg_val = most_active_df['tobs'].mean() $ print(f'Max:{max_val} | Min:{min_val} | Average:{avg_val}')
df.groupby("pickup_year")["cancelled"].mean()
df["web_booking"] = df["booking_application"].isin(["web", "web-desktop", "web-mobile", "web-tablet"]).astype(int) $ df["iphone"] = df["booking_application"].isin(["iphone-appstore"]).astype(int) $ df["android"] = df["booking_application"].isin(["android"]).astype(int)
ndf = ndf.assign(Norm_reviewText = normalize_corpus(ndf.reviewText))
two_day_sample['date'] = two_day_sample.timestamp.dt.date
import datetime $ today = datetime.datetime.now() $ diff = datetime.timedelta(weeks = 5, days = 2) $ result = today + diff $ print(result)
a,b=0,train $ x,y=X[a:b],Y[a:b] $ scores=model.evaluate(x,y) $ print ("\n%s: %.2f%%" %(model.metrics_names[1],scores[1]*100))
logit_control_3 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','CA_int_ab_page', 'US_int_ab_page']]) $ result_control_3 = logit_control_3.fit() $ result_control_3.summary()
active_with_return.info()
group_2 = df.groupby(['STATION', pd.Grouper(freq='D',key='DATETIME')]).sum().reset_index() $ print (len(group_2)) $ group_2.head()
df2[df2['E'].isin(['two','four'])]
data.info()
df.Close.pct_change().median()
winpct.head()
mailing_list_posts_mbox_df_saved.schema
mismatch_grp1 = df.query("group == 'treatment' and landing_page == 'old_page'") $ print("times the treatment group user lands incorrectly on old_page is :: {}".format(len(mismatch_grp1))) $ mismatch_grp2 = df.query("group == 'control' and landing_page == 'new_page'") $ print("times the control group user incorrectly lands on new_page is :: {}".format(len(mismatch_grp2))) $ print("times the new_page and treatment don't line up is :: {}".format(len(mismatch_grp1) + len(mismatch_grp2)))
plt.hist(holdout_residuals)
ax = perf.prices.to_drawdown_series().plot()
opt_fn = partial(optim.Adam, betas=(0.7, 0.99)) # changing down to 70
db_grade = pd.read_sql_query("SELECT * FROM current_scenario", conn)
journalists_retweeted_by_male_summary_df = journalist_retweet_summary(journalists_retweet_df[journalists_retweet_df.gender == 'M']) $ journalists_retweeted_by_male_summary_df.to_csv('output/journalists_retweeted_by_male_journalists.csv') $ journalists_retweeted_by_male_summary_df[journalist_retweet_summary_fields].head(25)
f_ip_device_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_device_hour_clicks"), header=True) $ print('Found %d observations.' %f_ip_device_hour_clicks.count())
result['createdAtUTC'] = pd.to_datetime(result['createdAt'], unit='ms')
model_df = topics_data.drop(['body', 'comms_num', 'id', 'title'], axis=1)
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_under_null, p_under_null]) $ print(new_page_converted)
(df2['landing_page'] == 'new_page').mean() $
yc_new2.describe()
cvecdata = cvec.fit_transform(X_train) $ cvec_df  = pd.DataFrame(cvecdata.todense(), $              columns=cvec.get_feature_names())
data.fillna(0).describe()
num_RNPA_new = RNPA_new['Provider'].resample('W-MON', lambda x: x.nunique()) $ num_RNPA_existing = RNPA_existing['Provider'].resample('W-MON', lambda x: x.nunique()) $ num_RNPA_new = num_RNPA_new[:'2018-04-30'] $ num_RNPA_existing = num_RNPA_existing[:'2018-04-30'] $ num_RNPA_existing = num_RNPA_existing[1:]
sensor.key
loans_plan_20150430_xirr=cashflows_plan_investor_20150430.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
plt.plot(dataframe.groupby('month').daily_worker_count.mean()) $ plt.show()
null_vals = np.random.normal(0, np.std(p_diffs), 10000) $ plt.hist(null_vals);
dt_year = toDate('07 15, 2014') $ new_df = review_df[review_df.reviewDate > dt_year] $ new_total = len(new_df) $ print (new_total)
df2.head(1)
honeypot_df.head()
new_page_converted = np.random.choice([0, 1], size=n_new, p=[1-p_new_null, p_new_null])
newdf['date'] = newdf.index
base_df.count()
house_data.info()
learn.fit(lr, 3, cycle_len=3)
from sklearn.feature_extraction.text import CountVectorizer $ tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=10000, $                                 stop_words='english') $ tf = tf_vectorizer.fit_transform(text[mask][INDEX])
results.summary()
df.to_excel('data/analysis2/Dow2.xlsx') $ df.tail(3)   
titanic.pivot_table('survived', index='class', columns='sex')
my_columns = list(Users_first_tran.columns) $ print(my_columns)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/adult.data.TAB.txt" $ mydata = pd.read_csv(path, sep= '\t') $ mydata.head(5)
df_concat.drop(["likes.data", 'likes.summary.can_like','likes.summary.has_liked'], 1, inplace = True)
dfX.head()
x = K.placeholder() $ target = K.placeholder() $ w = K.variable(np.random.rand()) $ b = K.variable(np.random.rand())
ins['type'].value_counts()
df_h1b_mv_ft.pw_1.describe()
threeoneone_geo['fix_time']=threeoneone_geo['closed_date']-threeoneone_geo['created_date'] $ threeoneone_geo['fix_time_sec']= [x / np.timedelta64(1, 's') for x in threeoneone_geo['fix_time']]
experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)
tripduration_minutes = pd.Series(minutes_list)
us[us['cityOrState'].isin(states) == False]['cityOrState'].value_counts(dropna=False)
elms_all = elms_sl.append(elms_pl, ignore_index=True) $ elms_all = elms_all.append(elms_tl, ignore_index=True) $ elms_all['SSN'] = [float(x) for x in elms_all.SSN.values]
new_df['waiting_days'] = new_df.apply(lambda _: '', axis=1) $ print("Adding new column 'waiting_days'") $ new_df.head(5)
implied_therapy = ['Therapy', 'New Patient Therapy', ] $ implied_doctor = ['Therapy Telepsychiatry','Follow up Telepsychiatry', 'New Patient Therapy Telepsychiatry',\ $                   'New Patient MD Adult', 'New Patient MD Adult Telepsychiatry'] $ merged1['Specialty'].loc[merged1['ReasonForVisitName'].isin(implied_therapy)] = 'therapist' $ merged1['Specialty'].loc[merged1['ReasonForVisitName'].isin(implied_doctor)] = 'doctor'
versions = {} $ with open('obj/version_dict.pkl', 'rb') as handle: $     versions = pickle.load(handle) $
pd.date_range('2015-07-03', periods=8, freq='H')
def top_gender(data): $     return pd.DataFrame(data.groupby('candidate_gender').count()['candidate_id'])
df_new['intercept'] =1 $ log_mod2= sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ result2 = log_mod2.fit() $ result2.summary()
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     my_group = f.get_node(where='/my group') $     print(my_group) $     my_group = f.get_node(where='/', name='my group') $     print(my_group)
old_page_converted = np.random.choice([1,0],size=n_old,p=[p_mean,(1-p_mean)]) $ old_page_converted.mean()
new_dems.newDate.tail()
import operator $ nl=reduce(operator.add,ll) $ nl
c.index
autos_p['ad_created'].str[:10].value_counts(normalize = True, dropna = False).sort_index(ascending = True).head(100)
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'],df2[['intercept','ab_page']])
proxy = xmlrpc.client.ServerProxy(url) $ try: $     print(proxy.AuthCheck(auth)) $ except Exception as e: $     print(f"OOPS, something wrong with {type(e)} - {e}")
sns.set()
accepted_answer_ids = list(set(posts_df['AcceptedAnswerId'])) $ posts_df['post_label'] = [get_post_label(post_id,parent_id,accepted_answer_ids) for post_id,parent_id in zip(posts_df['Id'],posts_df['ParentId'])] $ posts_df['post_label'].value_counts()
movies.sample(5)
json_tweets = pd.DataFrame(API_tweet_data, columns = ['tweet_id','retweet_count', 'favorite_count', $                                                'text','retweeted', 'date_time']) $ json_tweets.to_csv('tweet_json.txt', encoding = 'utf-8', index=False)
tweet_archive_enhanced_clean.loc[277,'text']
for c in ccc: $     vwg[c] = vwg[vwg.columns[vwg.columns.str.contains(c)==True]].sum(axis=1)
zip_counts = bus.groupby("postal_code").count() $ zip_counts = zip_counts.iloc[:, 0:1] $ zip_counts.columns = ["count"] $ zip_counts
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK_ind_ab_page', "UK","ab_page"]]) $ results = log_mod.fit() $ results.summary()
rating_error_rows =[] $ for i in range(len(twitter_archive)): $     if str(twitter_archive['rating_numerator'][i]) not in twitter_archive['text'][i]: $         rating_error_rows.append(i) $ print(rating_error_rows)
pretty_dates = [] $ for date in date_list: $     new_date = date.strftime("%Y-%m-%d") $     pretty_dates.append(new_date) $ print(pretty_dates) $
get_raw_data_script_file = os.path.join(os.path.pardir,'src','data','get_raw_data.py')
segDataGrouped = segmentData.groupby(['lead_mql_status','opportunity_month_year', 'opportunity_stage']).opportunity_stage.count() $ oppstagepct = segDataGrouped.groupby(level=[0,1]).apply(lambda x: x / float(x.sum())); oppstagepct.head()
evaluator.get_metrics('entity_level_results')
X = pd.merge(X, meals[['id','cook_id']], left_on='meal_id', right_on='id', how='inner') $ del X['id'] $ X = pd.merge(X, cooks[['id', 'is_cooking_reason_meet', 'is_cooking_reason_brand', 'is_cooking_reason_money']], left_on='cook_id', right_on='id', how='inner') $ del X['id'] $ del X['cook_id']
hi_conf_gain = x.loc[(x['pred'] - x['pred_std']) > 0.5] $ len(hi_conf_gain)
from scipy import stats $ stats.chisqprob = lambda chisq, df2: stats.chi2.sf(chisq, df2) $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ log_mod_results = log_mod.fit()
a.find(':')
from pandas.plotting import parallel_coordinates $ parallel_coordinates(P,'prediction')
new_page = gb.loc['new_page'][0] $ old_page = gb.loc['old_page'][0] $ p = new_page/(new_page + old_page) $ p
empDf.createOrReplaceTempView("employees") $ SpSession.sql("select * from employees where salary > 4000").show()
df_protest.loc[:, 'start_date'] = pd.to_datetime(df_protest.loc[:, 'start_date']) $ df_protest.loc[df_protest.towncity_name=='Johannesburg', 'start_date'].min()
pp.pprint(r.json())
twitter_ar['name1'] = twitter_ar.text.apply(lambda row: findname(row))
paradasColectivos.loc[distancia((-58.460934, -34.704528), (paradasColectivos["lat"], paradasColectivos["lon"])) <= 300]
v_aux=[] $ for i in range(0,l2): $     if i not in seq: $         v_aux.append(votes[i]) $ col.append(np.array(v_aux))
df2.groupby(['group'])['converted'].mean()
transactions.head(2)
education_dummies = pd.get_dummies(df.education) $ purpose_dummies = pd.get_dummies(df.purpose) $ data = pd.concat([df.submitted,education_dummies,purpose_dummies],axis=1)
import statsmodels.api as sm $ old_page_converted = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ new_page_converted = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = nold $ n_new = nnew
calls_df=calls_df.drop(["list_id","processed"],axis=1)
com311['Created Date']=com311['Created Date'].dt.date $ com311['Closed Date']=com311['Closed Date'].dt.date
TEXT.vocab.itos[1003]
com_grp.agg({'Age':np.mean, 'Salary': [np.min,np.max]})
classifier.fit(x_train, y_train, steps = 100)
output= "SELECT * from user where followers>150000" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','user_name','followers','following','tweet_count'])
prior_orders=orders[orders['eval_set']=='prior'] $ priors = pd.merge(order_products_prior, prior_orders, on='order_id',how ='right') $ priors.head()
Meter1.EndAutoRun()
taxi_hourly_df.loc[taxi_hourly_df.isnull().all(1) == True, "missing_dt"] = True $ taxi_hourly_df.loc[~(taxi_hourly_df.missing_dt == True), "missing_dt"] = False
cohort_retention_sum = pd.Series([sum(cohort_retention_df[col]) for col in cohort_retention_df.columns],index=cohort_retention_df.columns,name='Total')
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ old_page_converted
df['month'] = df.date.dt.month
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 3 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
fig, ax = plt.subplots(figsize=(12, 4)) $ births_by_date.plot(ax=ax)
df_enhanced['rating_10_scaled'].dtypes
sum(df2.converted==1) / df2.shape[0]
b = a $ a = np.append(a, 4)  # Note that this makes a copy; the original array is not affected $ print(b) $ print(a)
tips.head(3) # default 5 $
for sentence in sentences: $         vs = analyzer.polarity_scores(sentence) $         print("{:-<65} {}".format(sentence, str(vs)))
final.index=range(final.shape[0])
prop.info()
parser.BadLine
test_df_01 = test[:, ['label', 'imagePath']].cbind(preds['predict']).as_data_frame() $ test_df_01.head()
pivot_table_pos_count = pd.pivot_table(bb_df, values = ['Wt'], index = ['Pos'], aggfunc = 'count') $ pivot_table_pos_count['pct'] = pivot_table_pos_count.Wt / pivot_table_pos_count.Wt.sum() $ pivot_table_pos_count.sort_values(by = ['pct'], ascending = False)
notnaindex=tweets["retweet_status_user_name"].notna()
import matplotlib.pyplot as plt $ plt.style.use('ggplot') $
df2[['old_page', 'ab_page']] = pd.get_dummies(df['group'])
X_cvec_synos.shape
to_be_predicted_Day2 = 50.82048625 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
tableWebExtract = restaurantsExcelFile.parse(sheetname="WebExtract");
autos['price'].value_counts().sort_index(ascending=False).head(20)
x = np.random.randn(10) $ df9 = pd.DataFrame(index = range(100), columns = range(10)) $ for i in range(df9.shape[0]): $     df9.iloc[i] = x $ df9
unique_usrs =(df.user_id.unique()) #unique users in df dataset $ len(unique_usrs) #number of unique users in the df dataset
stock_data.loc[stock_data['close'] > 80]
calls_df.groupby(["call_time"])["length_in_sec"].mean()
y_train = train_shifted[[y_col]].as_matrix() $ X_train = train_shifted[X_cols].as_matrix()
noise_graf = noise_graf.drop('geometry',axis=1)
svd_matrixwashpark = svdtrun.fit_transform(washpark_matrixdf)
vocabulary_expression = pd.DataFrame(svdtrun.components_, $                                      index=components_names, $                                      columns=tfidf_vectorizer.get_feature_names()).T $
df_weekly.to_csv('trump_twitter_data_mar14.csv') $ df.to_csv('trump_twitter_data_all_mar14.csv')
df.shape[0]-(df.query("landing_page=='new_page' and group=='treatment'").shape[0])-df.query("landing_page=='old_page' and group=='control'").shape[0]
df_mas['rating_numerator'] = df_mas['rating_numerator'].astype(str) $ df_mas['rating_denominator'] = df_mas['rating_denominator'].astype(str)
null_vals=np.random.normal(0, p_diffs.std(), p_diffs.size)
biased_train_15.to_csv('biased_train_15.csv',sep=',') $ biased_train_33.to_csv('biased_train_33.csv',sep=',') $ biased_train_60.to_csv('biased_train_60.csv',sep=',') $ biased_train_05.to_csv('biased_train_05.csv',sep=',')
print(kelvin_to_celsius(273.15))
years = list() $ for x, i in enumerate(unprocessed_list): $     years.append(unprocessed_list[x][3])
poverty.drop([391, 392, 393, 394, 395, 396, 397], inplace=True)
print('{0:.2f}%'.format((scores[:1.625].sum() / total) * 100))
import pandas as pd $ from math import sqrt $ import numpy as np $ import matplotlib.pyplot as plt $ %matplotlib inline
kickstarter.describe()
Aussie_df["userTimezone"].value_counts()
weather_data = pd.concat([weather_data1, weather_data2, weather_data3], ignore_index= True); weather_data.head()
display(data.head())
np.exp(0.0408)
data['orientation'].value_counts()
n_new=df2[df2['group']=='treatment'].shape[0] $ print(n_new)
top_supports.groupby("contributor_cleanname").amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
stores = pd.read_csv("DataSets/stores.csv") $ stores
communities=[int(i[0:i.index('.')]) for i in labels]
data['date'] = pd.to_datetime(data['Timestamp'],unit='s').dt.date $ group = data.groupby('date') $ Daily_Price = group['Weighted_Price'].mean() $ Daily_Price.head()
b.tail(10)
df_eng_mult_visits.info()
tweet_data = pd.read_csv('tweet_json.txt', encoding = 'utf-8')
to_be_predicted_Day4 = 14.52772653 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
def zscore(series): $     return (series-series.mean())/series.std()
events.toPandas().head()
import pandas.io.sql as psql
df.describe()
foo = events.groupBy(["user_id", "event_date"]).count()#.agg( count("user_id"), count("event_date") )
plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS,  color='blue') $ plt.xlabel("Engine size") $ plt.ylabel("Emission") $ plt.show()
wrd_clean['expanded_urls'].isnull().sum()
df1=pd.read_csv('countries.csv', header=0) $ df1.head() $ df3=df2.set_index('user_id').join(df1.set_index('user_id')) $ df3.head()
ab_df.head()
<br> $ **Getting a Database** $     - A single instance of MongoDB can support multiple independent databases. $     - When working with PyMongo you access databases using attribute style access on MongoClient instances. $     - Database name can not use attribute style access (ie. test-data), but "test_data" is okay
train['feature_list'] = train['features'].map(lambda x: ','.join(x)).str.lower()
usa = ['USA','US','United States', 'United States of America'] $ us.loc[us['country'].str.contains('US', case=False) | us['country'].str.contains('United States', case=False), ['country']] = 'USA' $ us[(us['cityOrState'].isin(usa) == True) & (us['country'].isin(usa) == False)]['country'].value_counts(dropna=False)
df_filtered = df[(df["Keyword"].str.contains("bmw|audi", regex=True)==True)] $ df_filtered.head(5)
sales_df.groupby('Country').sum()['Quantity'].sort_values(ascending=False)
i = pd.read_sql_query(QUERY, conn) $ i
images.info()
data.head(10)
print(data.info())
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ print(z_score, p_value)
hrefs = soup.find_all('a') $ hrefs
from imblearn.over_sampling import SMOTE
DummyDataframe2 = DummyDataframe2.apply(lambda x: calPercentage(x, "Token_Count", ["Positiv", "Negativ"]), axis=1) $ DummyDataframe2
%%time $ ent_count = defaultdict(int) # reset defaultdict $ for doc in nlp.pipe(texts, batch_size=100, disable=['parser','tagger','ner']): $     matcher(doc) # match on your text $ print(ent_count)
cust_data[["MonthlyIncome","ID"]].head(5)
start_date = dt.date(2018,7,11) $ end_date = dt.date(2018,7,21) $ previous_year = dt.timedelta(days=365) $ diff_dates_temp = (calc_temps((start_date-previous_year), (end_date-previous_year))) $ diff_dates_temp $
for i, row in breakfastlunchdinner.iterrows(): $     breakfastlunchdinner.loc[i, 'day_sales'] = sum(row[1:]) * .002 $ breakfastlunchdinner
df_top_cat_2015=df.groupby(df_2015.category_name,as_index=False).agg({ $         "sale_dollars": np.sum, $         "category_name": lambda x: x.iloc[0] $                 }).sort_values(by='sale_dollars', ascending=False)
to_be_predicted_Day4 = 48.60616693 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
dt.time()
X = preprocessing.scale(X) $
print("--- %s seconds ---" % (time.time() - start_time))
from scipy.stats import entropy $ sp.stats.entropy(df['prob'])
df['Descriptor'].tail()
s4g.groupby('Symbol')
data_full = pd.read_csv('invoices_full.csv')
df_mes = df_mes[(df_mes['average_speed'] != np.inf) & (df_mes['average_speed'] < 100)] $ df_mes.shape[0]
leadPerMonth.tail()
from sklearn.cross_validation import cross_val_score $ scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10) $ print scores $ print scores.mean() $
df.tweet[0]
digits.data
df['fileSizeMB'].describe()
"We have {0} tweets.".format(len(D1))
from pandas_datareader import data $ goog = data.DataReader('GOOG', start='2004', end='2016', $                        data_source='google') $ goog.head()
train['title_len'] = train.title.apply(len) $ train.boxplot('title_len', by='popular')
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(total['text'], total['tag'], random_state=42, test_size=0.2, shuffle=True)
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA_ind_ab_page', "CA","ab_page"]]) $ results = log_mod.fit() $ results.summary()
pd.options.display.max_colwidth = 130 $ ttarc.expanded_urls.head(16)
%timeit ' '.join(map(stemmer,regex.sub('','The vintage 1930 beaded earrings are still calling to me.').split()))
webmap = gis.map('Meades Ranch, KS') $ webmap
state_grid = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(10, 10)) $ state_grid
map_values = {'N0(i-)': 'N0(i_minus)', 'N0(i+)': 'N0(i_plus)'} $ df_categorical['latest_n'] = df_categorical['latest_n'].replace(map_values) $ df_categorical['latest_n'].unique()
print(len(df_bud)) $ df_bud.head()
pgh_311_data = pd.read_csv("311.csv", $                            index_col="CREATED_ON", $                            parse_dates=True) $ pgh_311_data.head()
df.groupby("userid").sum().sort_values(by="postcount",ascending=False)[:10]
print(df2.query('converted==1').converted.count()) $ print(df2.converted.count()) $ print(df2.query('converted==1').converted.count()/df2.converted.count())
df = pd.read_sql('SELECT p.customer_id, p.amount FROM payment p WHERE p.amount > (SELECT AVG(p.amount) FROM payment p);', con=conn) $ df $
close_series = stock_data['close'] $ display(close_series.head(5)) 
df2.all()
forecast_df['white_pop'] = 0 $ for ind, row in SANDAG_ethnicity_df[SANDAG_ethnicity_df['ETHNICITY'] == 'White'].iterrows(): $     forecast_df.loc[ind, 'white_pop'] = row['POPULATION'] $ forecast_df.head()
df_pca.describe()
df.head()
archive_copy['name'].loc[archive_copy['name'].str.islower()] = 'None'
lag_plot(dataSeries) $ pyplot.show()
df.head()
df[df['gathering'].isnull()]
act.append(141182021)
valid_scores.groupby('tx_TacticId').apply(slice_metrics_2)
v_invoice_hub.columns[~v_invoice_hub.columns.isin(invoice_hub.columns)]
BASEBALL_SWING_ACTION_TYPE_ID = 1 $ url = form_url(f'actionTypes/{BASEBALL_SWING_ACTION_TYPE_ID}/metricTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
ts[pd.datetime(2011, 12, 25):]
autos['ad_created'].str[:10].head()
autos['brand'].describe()
l=[(x[6]) for x in json_dict] $ avg_vol = sum(l)/len(l) $ print('Average volume for all 2017 is {:.2f}'.format(avg_vol)) $
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
test_prediction = clf.predict_proba(test_df[features])
len(df.loc[df['matrix'] == 'mitre-attack'])
sample_dic.keys()
time_stamp = "2017-01-09 05:37:58.781806" $ df2 = df2[df2.timestamp != time_stamp] $ df2.info()
joined = joined[joined.Sales!=0]
all_tables_df.info()
season_type_groups.aggregate(np.mean)
!head -5 data-new.csv
s = pd.Series(['Tom', 'William Rick', 'John', 'Alber@t', np.nan, '1234','SteveSmith']) $ s
recent = pd.DataFrame(list(teams)) $ recent["GoalRate"] = 0 $ recent["ConcedeRate"] = 0 $ recent["OverallWin%"] = 0 $ recent["Opp Toughness"] = 0
extract_all.loc[(extract_all.APP_SSN==29392513), $                ['APP_APPLICATION_ID','system.created_at','APP_FIRST_NAME','APP_LAST_NAME']]
%pycat main.py
min(close, key=close.get)
pivot = df_R.pivot_table(values='Sales_Volume' , index='Month', columns='Year')
prop = props[props.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
stopword = nltk.corpus.stopwords.words('english')
df[b'unique_key'].plot()
val_df.reset_index(drop=True, inplace=True) $ val_y.reset_index(drop=True, inplace=True) $ test_df.reset_index(drop=True, inplace=True)
token_sendReceiveAvg_month.columns = ["ID","sendCntMean_mon","receiveCntMean_mon"]
autos.info()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page" & converted == 1').shape[0] $ convert_new = df2.query('landing_page == "new_page" & converted == 1').shape[0] $ n_old = df2.query("landing_page == 'old_page'").shape[0] $ n_new = df2.query("landing_page == 'new_page'").shape[0]
STATION_traffic_weektotals = (SCP_ENTRY_weektotals + SCP_EXIT_weektotals).groupby(['STATION']).sum() $ STATION_traffic_weektotals.sort_values(ascending=False).head(10)
tweets_j=json.loads(tweets_csv.to_json(orient='records')) $ tweets_j[0]
results2 = [] $ for tweet in tweepy.Cursor(api.search, q='%23puppy').items(2000): $     results2.append(tweet.text)
u = numpy.fft.rfft(Q1 - Qbar) $ y = numpy.fft.rfft(T1 - numpy.mean(T1))
doc_term_matrix = [dictionary.doc2bow(doc) for doc in tweets_list]
combined_df3['vo_propdescrip'].str.upper();
old_page_converted = np.random.choice([0,1],n_old,p=(p_old,1-p_old))
reddit_df.columns
INDEX_FROM = 3 $ print('Loading data...') $ (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, index_from=INDEX_FROM) $ print(len(x_train), 'train sequences') $ print(len(x_test), 'test sequences')
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ z_score, p_value 
df_repeat=df2[df2['user_id'].duplicated()]['user_id'].values $ print("Repeated user_id is "+str(df_repeat[0]))
df2[df2.landing_page=="new_page"].count()[0]/df2.count()[0]
bands.columns = ['BAND_'+str(col) for col in bands.columns]
predictions=model.predict(X) $ rounded =[round(x[0]) for x in predictions] $ print(rounded)
n_old= df2[df2["landing_page"] == "old_page"].count() $ n_old = n_old[0] $ n_old
df_columns[df_columns['Complaint Type']=='Street Condition']['Descriptor'].value_counts().head() $
df.Standard_Plat.value_counts()
shows['plot'].head()
(details['Number of Ratings'] == 0).value_counts()
tips_analysisDF = pd.read_csv('text_preparation/yelp_tips_prepared.csv', index_col=False, encoding='latin-1')
df2[df2.converted==1].count()[0]/df2.count()[0]
autos["price"].median()
details.head()
tweets = pd.read_json('tweet_json.txt')
training_set = sentim_analyzer.apply_features(train['splitText'].tolist()) $ test_set = sentim_analyzer.apply_features(tweets['splitText'].tolist())
TensorBoard.stop(327)
df.isnull().any() # there is no any missing value inside the dataset
df_clean3[df_clean3.rating_numerator <= df_clean3.rating_denominator].sample(10)
sortedprecip_12mo_df=precip_12mo_df.sort_values('date',ascending=True) $ sortedprecip_12mo_df.head()
X_testset.shape $ y_testset.shape
df_ids = pd.DataFrame(np.column_stack([['carlahome80' for i in range(len(ids))], ids]),\ $                       columns = ['account_owner_screen_name', 'id'])
temp=pd.DataFrame(most_retweeted_tweeps_sum)
has_null_value = blame.isnull().any(axis=1) $ dropped_entries = blame[has_null_value] $ blame = blame[~has_null_value] $ dropped_entries
os.environ["Zgs_key"] = "gs://resource-watch-public/" + s3_key_edit $ os.environ["Zs3_key"] = "s3://wri-public-data/" + s3_key_edit
author_data['reputation_score'] = author_data.apply(lambda row: reputation_score(row['reputation']), axis=1)
bild = bild[bild.message != "NaN"] $ spon = spon[spon.message != "NaN"]
data.info()
lat = temp_nc.variables['lat'][:] $ lon = temp_nc.variables['lon'][:] $ time = temp_nc.variables['time'][:] $ temp = temp_nc.variables['air'][1,:,:] $ np.shape(temp)
df.iloc[1]
X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = model_selection.train_test_split(X, $                                                                                         train["sentiment"], $                                                                                         test_size=0.2, $                                                                                         random_state=0)
locations = session.query(Measurement).group_by(Measurement.station).count() $ print("There are {} stations.".format(locations))
f = np.sin(np.linspace(0,np.pi,100)) $ print("f: ", f)
df.loc[df['user_id'] == 63]
np.c_[np.random.random(5),np.random.random(5)]
df = pd.concat([df_en,df_other])
pos_threshold=0.1 $ neg_threshold=-0.1
api_copy.rename(columns = {'id': 'tweet_id'} ,  inplace= True) $ api_copy['tweet_id'] = api_copy['tweet_id']. astype('str') $ type(archive_copy['tweet_id'].iloc[0]) $
list(r_close.keys())[98]
images.head()
df[df.funding_rounds < 10].hist(column = 'funding_rounds',bins = 10)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)
df2_con=df2[df2['group']=="control"] $ prob1 = df2_con[df2_con['converted']==1].shape[0]/df2_con.shape[0] $ prob1
contract_history.isnull().sum()
sub1 = sub1.drop_duplicates()
prec_group.head()
pf_mus, pf_sigmas = create_random_portfolios(returns.values, n_portfolios=3000) $ pf_sigmas
full_orig.groupby(['MonthAdmit'])['<=30Days'].mean().plot(kind='bar',figsize=(12,4)) $ plt.ylabel('readmit rate') $ plt.title('Readmit Rate based on {}'.format(feat1),size=15) $ plt.axhline(y=pop_avg, color='black', linestyle='--') $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
data = allocate_equities(allocs=[  0.25 , 0.25,  0.25 , 0.25],dates=dates) $ data.plot(), $ plt.show() $
df2.query('landing_page == "old_page"').shape[0]
zstat1_score, p_value_twosided = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='two-sided') $ print("The Pvalue for Two sided for alternate Hypothesis is {}".format(p_value_twosided))
df_person.set_index("id", inplace=True) $ df_grades.set_index("person_id", inplace=True)
logreg.fit(X, y) $ new_pred_prob = logreg.predict_proba(X_new)[:, 1] $ metrics.roc_auc_score(new.popular, new_pred_prob)
%matplotlib inline $ AAPL[['close','MA20']].plot()
print pd.pivot_table(data=df, $                      index='date', $                      columns='item', $                      values='status', $                      aggfunc=np.sum)
na_df.fillna("NEW")
new_action = action.drop_duplicates(subset=['article_id', 'project_id', 'user_id', 'user_type'], keep='first') $ print(new_action.shape) $ new_action.head() 
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth)
female_journalists_retweet_summary_df[['retweet_count']].describe()
data2.SA = data2.SA.astype(str)
%%time $ X_train_term  = vectorizer.fit_transform(X_train['text'])
df = pd.DataFrame(rows, columns=cols)
to_be_predicted_Day5 = 43.11698497 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df.index = pd.to_datetime(df.index)
im = Imputer(strategy='median') $ im.fit(X_train)
tweets_streamedDF.full_text[0]
trigrams = trigram_converter.get_feature_names() $ len(trigrams)
df_birth.columns
grammar = '' $ cp = nltk.RegexpParser(grammar) $ result = cp.evaluate(train_trees) $ print(result)
archive_clean.drop(['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'],axis=1,inplace=True) $
Base.classes.keys()
knn.fit(train[['property_type', 'lat', 'lon']], train['surface_covered_in_m2'])
places = extractor.geo_search(query="Philippines", granularity="country") $ place_id = places[0].id $ print("Philippines id is:", place_id)
complete_df = pd.concat([parsed_guinea_df, parsed_liberia_df, parsed_sierra_df]) $ complete_df.reset_index(inplace=True, drop=True)
df_delta = df[df['Difference'] >= pd.Timedelta(seconds=1)].dropna()
type(df2.created_time.loc[0])
df[df.num_shares == max(trumpint.num_shares)]
top_5_percent.sum() / autos_pr['brand'].value_counts().sum()
print ('Summary:') $ print (summarize(text2, ratio=0.2))
df['timestamp'] = pd.to_datetime(df.timestamp) $ df['ts_year'] = df['timestamp'].dt.year $ df['ts_dayofweek'] = df['timestamp'].dt.weekday_name $ df18 = df.loc[0:5,['timestamp','ts_year','ts_dayofweek']] $ df18
from gensim.models import word2vec as word2vec $ models = gensim.models.Word2Vec(nltk_Tokenize, size=32, window=5, min_count=1, workers=4)
RMSE_list[W.index(12)]
train_bf.summary()
X_test.shape
test = test.drop(columns="id") $ (train.shape, test.shape, submit.shape)
LMsigLoans_path = cwd+'\\LeadGen\\Ad hoc\\SubID\\LM Sig Loans.xlsx' $ LMsigLoans = pd.read_excel(LMsigLoans_path, sheetname='lm_data') $ phones_ssns = pd.read_excel(LMsigLoans_path, sheetname='phone') $ print(LMsigLoans.shape) $ print(phones_ssns.shape)
prod_ch.columns.values
Pnew = df2['converted'].mean() $ Pnew
grouped_dpt["Revenue"].filter(lambda x: x.sum() > 1000)
dat[out_columns].to_csv(save_pth, float_format='%g', date_format=date_format) #save data
matchresult.groups()
tf.unique()
print soup.prettify()
rest = datatest[datatest.surface_total_in_m2.isnull()] $ train = datatest[datatest.surface_total_in_m2.notnull()] $ test = train[train['covered/total'].isnull()] $ train = train[train['covered/total'].notnull()]
from pandas import read_csv $ df = read_csv("tweets.csv")
top_supporters = merged.groupby(["contributor_firstname", "contributor_lastname"]).amount.sum().reset_index().sort_values("amount", ascending=False).head(10)
tweets_p1_month = tweets_p1_month.drop(['Month', 'Year'], axis=1) $ tweets_p1_month.head()
import json $ with open('network_data_twitter.json', 'w') as outfile: $     json.dump(data, outfile)
sentiment.iloc[309402]['datetime']
module_id = 'cl_Jx8qzYJh' $ res = ml.classifiers.classify(module_id, textList, sandbox=False)
cursor=db.bookings.aggregate([{"$group" : {"_id" : "$status", "num_status" : {"$sum" : 1}}}]) $ df =  pd.DataFrame(list(cursor)) $ df $
loan_stats["loan_result"].head(rows=2)
df_M7_2016 = pd.DataFrame({'DATE':dfM.DATE[-12:],'ACTUAL':[i[0] for i in M7_actual_2016], $                       'M7':[i[0] for i in M7_pred_2016], $                      'BASELINE':dfM.t[-12:]}) $
autos = autos[autos["price"].between(100,1000000)]
df_temperature = pd.read_csv('temperature_nyc.csv') $ df_precipitation = pd.read_csv('precipitation_nyc.csv')
precip_df = pd.DataFrame(precip, columns=['date', 'prcp']) $ precip_df.set_index('date', inplace=True, ) $ precip_df.head()
parcel_in = parcels.apply(within_area,axis=1) $ parcels = parcels.loc[parcel_in] $ print "Number of parcels wuthin area",sum(parcel_in),"out of",len(parcel_in) $ del parcel_ins $ parcels["grid"] = parcels.apply(build_grid_index,axis=1)
mortraffic_byday = (targettraffic.groupby(('STATION', 'weekdayname')) $                      .mean() $                      .reset_index())
df[(df.amount == 0)].amount_initial.unique()
data=[] $ for js in yelppath: $     with open(js) as json_file: $         data.append(json.load(json_file))
countries.info()
fraud_df['signup_time'] = fraud_df['signup_time']+pd.to_timedelta(fraud_df['gmt_hour_avg'],unit='h') $ fraud_df['purchase_time'] = fraud_df['purchase_time']+pd.to_timedelta(fraud_df['gmt_hour_avg'],unit='h')
dr_2018.shape
df['Month'] = df['Date'].dt.month
df.sum(0)
s.rank(ascending=False)
df_clean.source.head(100)
n_old = df2[df2['landing_page']=='old_page']['user_id'].nunique() $ n_old 
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x, np.dot(x3, ridge2.coef_) + ridge2.intercept_, color ='fuchsia', linewidth = '5') $ plt.plot(x, np.dot(x3, model3.coef_) + model3.intercept_, color = 'dimgrey', linewidth = '3.5')
df_combined['intercept'] = 1 $ lm = sm.Logit(df_combined['converted'], df_combined[['intercept','CA','US','ab_page']]) $ results = lm.fit() $ results.summary()
sortHighThenVol = AAPL.sort_values(by=["high", "volume"], ascending=False) $ sortHighThenVol.head()
print("regarding attrition:") $ print("Yes: " + str(ibm_hr.filter(ibm_hr['Attrition'] == "Yes").count())) $ print("No: " + str(ibm_hr.filter(ibm_hr['Attrition'] == "No").count()))
(p_diffs > actualdiff).mean()
re_cid = re.compile('cid') $ et_al_cid = re.compile('et al') $ for pape in all_papers: $     pape['article'] = re.sub(re_cid, ' ', pape['article']) $     pape['article'] = re.sub(et_al_cid, '', pape['article'])
myplot_parts = [go.Scatter(x=person_counts["stamp"],y=person_counts["count"],mode="line")] $ mylayout = go.Layout(autosize=False, width=1000,height=500) $ myfigure = go.Figure(data = myplot_parts, layout = mylayout) $ iplot(myfigure,filename="crisis")
true_file = pd.read_csv(filepath_or_buffer=os.path.join(edfDir, 'pt1sz2_eeg.csv'), header=None) $ test_file = pd.read_csv(filepath_or_buffer=outputData, header=None)
missing_conditions = conditions_m.isnull() $ missing_conditions
namesList[3] # citigroup in no.3
df_bill_data= df_bill_data.drop('bill_id', axis=1) $ df_bill_data = df_bill_data.groupby(['patient_id','date_of_admission']).sum().reset_index() $ df_bill_data.to_csv('df_bill_data.csv')
df_regression.rename(columns={'treatment': 'ab_page'}, inplace=True)
col = ['Team', 'Executive', 'Tenure'] $ dframe_team[col] = dframe_team[col].ffill() $ dframe_team = dframe_team[((dframe_team['Draft_year'] >= 1976) & (dframe_team['Draft_year'] <= 2016))] $ dframe_team.head(10)
active_users.shape[0] / users.shape[0] # Only a third of users signed to the site were active
np.mean((Y_train - np.mean(Y_train))**2)
dem_in = dem.apply(within_area,axis=1) $ dem.loc[dem_in,["date","type"]].groupby("type").agg(["min","max","count"])
most_common_dog.plot(kind = 'barh', figsize = (8, 8)) $ plt.title ('Top 5 most rated dog breeds') $ plt.xlabel ('Amount of dogs') $ plt.ylabel ('Dog breed');
df6.mean() # mean of lunch values $
team_attributes._get_numeric_data().columns.tolist()
full.duplicated().sum()
numerical = condo_6[list(set(condo_6.columns) - set(['SSL', 'SALEDATE', 'FULLADDRESS', 'UNITNUM']))] $ numerical.info()
brand_mean_prices = {} $ for brand in brands: $     mean_price = autos.loc[autos["brand"]==brand,"price"].mean() $     brand_mean_prices[brand] = mean_price $ brand_mean_prices
run txt2pdf.py -o '2018-06-25 Sepsis.pdf'  '2018-06-25 Sepsis.txt' $
def analyze_tweet(tweet): $     tweet = tweet._json $     score = analyzer.polarity_scores(tweet["text"]) $     return score
df.head()
subwaydf.iloc[153302:153306] # this one seems to be a correct high number and I assume ones below this are also accurate
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df3[df3['STATION']=='103 ST'].groupby('WEEK').sum()
u235_scatter_xs = fuel_xs.get_values(nuclides=['(U235 / total)'], $                                 scores=['(scatter / flux)']) $ print(u235_scatter_xs)
total_treatment = df2[(df2['landing_page'] == "new_page")].count() $ print(total_treatment)
import pandas as pd $ import numpy as np $ from sklearn.ensemble import RandomForestRegressor $ from sklearn.model_selection import train_test_split
table1.head(3)
def get_data(): $     return pd.DataFrame.from_csv("../../../data/mbta.csv").reset_index() $ gatecount_raw = date.init(get_data())
All_tweet_data_v2.retweeted_status_timestamp=All_tweet_data_v2.retweeted_status_timestamp.str.replace(' \+0000','') $ All_tweet_data_v2.retweeted_status_timestamp $ All_tweet_data_v2.retweeted_status_timestamp.dropna().head()
print ('Binarized Mean: \n',training_active_listing_dummy.mean())
foo.isnull()
graffiti2 = gpd.sjoin(p, g_geo, how='left')
verif=X_test.copy() $ verif['yhat']=Yhat_test $ verif['y']=y_test $
celtics = pd.read_csv('../../../data/celtics/home.csv')
files8= files8.drop('EndDate',axis=1) $ files8= files8.drop('StartDate',axis=1) $ files8.head() $
austin= austin[austin['distance_travelled']>0] $ austin.shape
com_luz_sp = ((df_sp.PERIDOOCORRENCIA_TIPO==1) | (df_sp.PERIDOOCORRENCIA_TIPO==2)) $ sem_luz_sp = ((df_sp.PERIDOOCORRENCIA_TIPO==3) | (df_sp.PERIDOOCORRENCIA_TIPO==4)) $ print('Com Luz Solar: ', com_luz_sp.sum()) $ print('Sem Luz Solar: ', sem_luz_sp.sum())
response = requests.get(url)
pct.head()
autos['odometer'] = (autos['odometer'] $                  .str.replace('km','') $                  .str.replace(',','') $                   .astype(float) $                  )
%run returns.py $
model_df.weekday = model_df.timestamp.dt.weekday_name #assign the day of the week from 'timestamp' columns to the 'weekday' column
stories.dropna(thresh=9, axis=1).shape
autos.groupby('brand')['price'].mean().sort_values(ascending = False).head(20)
fs_url = 'http://sampleserver3.arcgisonline.com/ArcGIS/rest/services/SanFrancisco/311Incidents/FeatureServer' $ sanfran = FeatureLayerCollection(fs_url)
inputPath2 = "graphdata/imagelabels.csv" $ edges = sqlContext.read.options(header='true', inferSchema='true').csv(inputPath2) $ edges.show(5)
stocks.columns  # Inspecting progress
print("The minimum donation amount is: $", donations['Donation Amount'].min()) $ print("The maximum donation amount is: $", donations['Donation Amount'].max()) $ print("The average donation amount is: $", donations['Donation Amount'].mean()) $ df_mode = donations['Donation Amount'].mode() #note the mode outputs a dataframe where each row would identify the most common element in the case of ties. $ print("The most common donation amount is: $", df_mode.iloc[0]) $
beginDT = '2017-02-01T00:00:00.000Z' $ endDT = '2017-09-01T00:00:00.000Z' $
pandas_df.drop(['post_id','user_id'],axis=1).describe()
conf_matrix = confusion_matrix(y_tfidf_test, svc.predict(X_tfidf_test), labels=[1,2,3,4,5]) $ conf_matrix
sample = pd.read_csv('../../../data/talking/train_sample.csv')
us.loc[us['country'].str.len() == 2, 'country'] = us.loc[us['country'].str.len() == 2, 'country'].str.upper() $ us.loc[us['country'].str.len() == 2, 'country'].value_counts(dropna=False)
    ufo_pandas.info()
assert df_total.isna().any().any() == False
random_features = bag_of_words_vectorizer.transform(random_corpus)
wrd_tidy.head(3)
email_age_unique = discover_data[['email', 'request.ageRange']].drop_duplicates(keep='last') $ age_range_breakdown = email_age_unique.groupby('request.ageRange').count() #apply(lambda x: 100 * x / x.sum()) $ age_range_breakdown['Percentage'] =age_range_breakdown.apply(lambda x: 100 * x / x.sum()) $ age_range_breakdown # age_range_breakdown['Percentage'] = age_range_breakdown['email'].map(lambda x: 100 * x / x.sum())
category_dummies.to_csv('./data/category_dummies.csv')
from sklearn.model_selection import cross_val_score, StratifiedKFold $ from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier
pd.to_datetime([1, 3.14], unit='s')
y = np.array(df['label'])
for layer in model.layers: $     weights = layer.get_weights() # list of n $ weights
mydata = np.load('my_filename2.npz') $ print(mydata['a']) $ print(mydata['b'])
autos = autos.drop(["num_photos", "seller", "offer_type"], axis=1)
df_ad_airings_5.shape
sp500.loc[['MMM', 'MSFT']]
status.head()
sum(image_clean['jpg_url'].duplicated())
scowl4 = [x.lower() for x in scowl3]
fld = 'SchoolHoliday' $ df = df.sort_values(['Store', 'Date']) $ get_elapsed(fld, 'After') $ df = df.sort_values(['Store', 'Date'], ascending=[True, False]) $ get_elapsed(fld, 'Before')
pd.DataFrame(list(zip(tx,ty))).plot.scatter( $     x=0, y=1, alpha=.4);
f = pd.merge(d, e, how='left', on='msno') $ f[f.msno == '29V0Jm3Xli1dy9UFeEL/BH2EMOr62DgeGLeKAKfE07k=']
df = pd.read_csv("BaseFinale",index_col=0)
dti = pd.to_datetime(['Aug 1,2014','2014-08-2','2014.8.3',None]) $ for l in dti: print(l)
sl_train= sl.loc[sl.second_measurement==0] $ sl_holdout= sl.loc[sl.second_measurement==1] $ print("training rows",sl_train.shape[0]) $ print('holdout_rows', sl_holdout.shape[0]) $ print('sanity check: full sl shape', sl.shape[0], 'combination of train and holdout',sl_train.shape[0] + sl_holdout.shape[0]) 
cnn_fox_data = cnn_fox_data.reindex(np.random.permutation(cnn_fox_data.index))
trimmed = prune_below_degree(tweetnet, 100) $ nx.info(trimmed)
p_diffs = [] $ for _ in range(10000) : $     new_page_converted = np.random.binomial(n_new,p_new,1)/n_new $     old_page_converted = np.random.binomial(n_old,p_old,1)/n_old $     p_diffs.append(new_page_converted - old_page_converted)     
df.head()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY $ r = requests.get(url)
dtypes={'item_nbr': np.int64,'family':np.str,'class':np.int64,'perishable':np.int64} $ items = pd.read_csv('items.csv',dtype=dtypes) # opens the csv file $ print("Rows and columns:",items.shape) $ pd.DataFrame.head(items)
np.exp(rmse_CBoE)
dict1 = {'one': pd.Series([1, 2], index=['a', 'b']), $          'two': pd.Series(['t1','t2','t3'], index=['a', 'b', 'c'])} $ df2 = pd.DataFrame(dict1) $ df2
new_page_converted = np.random.choice(2, size = n_new, p=(p_new,1-p_new)) $ new_page_converted
d = {'col1': red_test[:,0], 'col2': red_test[:,1]} $ red_test_data = pd.DataFrame(data=d) $ test_float['PCA1'] = red_test_data['col1'] $ test_float['PCA2'] = red_test_data['col2'] $ test_float.head()
inv_hist=pd.DataFrame({'bin':inv_bin[:-1],'count':inv_count})
df.drop(['text_split'], axis=1, inplace=True)
autos = autos[autos["price"].between(100, 500000)]
archive_clean.info()
from pandas_datareader import DataReader $ from datetime import date $ from dateutil.relativedelta import relativedelta $ goog = DataReader("GOOG", "morningstar", date.today() + relativedelta(months=-3)) $ goog.tail()
result = results[0] $ result.keys()
demoji = emoji.demojize(text, delimiters=('__','__')) $ print(demoji)
conn.columninfo(table=dict(name='iris_db', caslib='casuser'))
learner= md.get_model(opt_fn, em_sz, nh, nl, $     dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4]) $ learner.metrics = [accuracy] $ learner.freeze_to(-1)
pax_raw.groupby('seqn').paxcal.mean().value_counts()
svm_predicted = svm_model.predict(X_final)
import plotly.plotly as py $ import plotly.graph_objs as go
from sklearn.cross_validation import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(data[features], data["OutcomeType"], stratify=data["OutcomeType"], random_state=100) $ X_train.head()
df_final['recency'] = (now - df_final['last_trx']).dt.days
rf = RandomForestRegressor(n_estimators=100, random_state=1) $ rf.fit(X_train, y_train)
main_deduped = dedupe(main_subset.filter(lambda p: p.get("clientId") in all_clients), "id") $ main_deduped_count = main_deduped.count()
autos["registration_year"].between(1900,2016).sum()
cercanasAfuerteApacheEntre75Y100mts = cercanasAfuerteApache.loc[(cercanasAfuerteApache['surface_total_in_m2'] >= 75) & (cercanasAfuerteApache['surface_total_in_m2'] < 100)] $ cercanasAfuerteApacheEntre75Y100mts.loc[:, 'Distancia a Fuerte Apache'] = cercanasAfuerteApacheEntre75Y100mts.apply(descripcionDistancia2, axis = 1) $ cercanasAfuerteApacheEntre75Y100mts.loc[:, ['price', 'Distancia a Fuerte Apache']].groupby('Distancia a Fuerte Apache').agg(np.mean)
dr_test_data = dr_test_data.groupby(dr_test_data.index.date)['Hours_Spent'].sum() $ RN_PA_test_data = RN_PA_test_data.groupby(RN_PA_test_data.index.date)['Hours_Spent'].sum() $ therapist_test_data = therapist_test_data.groupby(therapist_test_data.index.date)['Hours_Spent'].sum()
issue_comments.head()
agency_borough = data.groupby(['Agency','Borough']) $ agency_borough.size().plot(kind='bar')
np.exp(-0.0149),np.exp(0.0099),np.exp(-0.0408)
obs_diff = (df2.query("landing_page=='new_page'").converted.mean() - df2.query("landing_page=='old_page'").converted.mean()) $ obs_diff
measure.head()
dfWeek = dfDay.copy(deep=True)
p_new = df2['converted'].mean()
tweets = pd.read_pickle('cleaned_tweets_data.pkl')
df.join(d, how='outer')
df2.nunique() #checking the number of unqiue ids
manager.image_df.loc[0,'p_hash']
from nltk.corpus import stopwords $ print(stopwords.words('english'))
ti_mar = pd.concat([ti_jd, ti_tmall, ti_suning]) $ ti_mar.shape
session.query(Measurement.date).\ $ filter(Measurement.station == 'USC00519281').\ $ order_by(Measurement.date.desc()).first()
engine.execute('SELECT * FROM measurement LIMIT 10').fetchall() $
gta = df[df.Name =='Grand Theft Auto V'] $ gta[['Platform','Standard_Plat','Name','Global_Sales','Critic_Score','Release_Date','Year_of_Release','_merge']]
dateset.drop('index',inplace=True,axis=1)
user_status_model.possibly_sensitive
datatest.loc[datatest.place_name == "Prados del Oeste",'lat'] = -34.594077 $ datatest.loc[datatest.place_name == "Prados del Oeste",'lon'] = -58.829508
df = pd.read_excel('PPB_gang_records_UPDATED_100516.xlsx')
bobby_ols = ols('opening_gross ~ star_avg',dftouse).fit() $ bobby_ols.summary()
from scipy.stats import norm $ norm.cdf(z_score) $
pd.datetime.now()
to_be_predicted_Day2 = 21.38060307 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
val=17.5 $ size = np.exp(val)/(1024*1024) $ percentage = 100*df.loc[df.Size < np.exp(val)].Size.count()/len(df) $ print('Setting mark for size at {0:.2f} MB will still give us {1:.2f} % of data objects'.format(size, percentage))
monthlyDF = byMonth['MeanFlow_cfs'].mean()
autos.loc[autos.price>350000.0,'price']=np.nan
import locale $ locale.setlocale(locale.LC_ALL, 'en_US.UTF-8') $ new_users = users[:] $ new_users['CUSTOMER_ID'] = users['CUSTOMER_ID'].apply(locale.atoi) $ new_users.head()
datasets_ref.groupby('id')['slug'].count().value_counts()
pysqldf("select * from genes where genomic_accession = 'LT906474.1' and start > 3252780 and end < 3302485")
pd.read_pickle(folderint + 'interactions-2016-12-18.pkl')
df_daily3 = df_daily.groupby(["C/A", "UNIT", "STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index() $ df_daily3.head(5)
df.query('landing_page == "new_page"').query('group == "control"').count() + df.query('landing_page == "old_page"').query('group == "treatment"').count()
df2.head()
df.columns
temp_df = pd.DataFrame(data = us_temp) $ temp_df.columns = ts.dt.date[:843]
dicttagger_service = DictionaryTagger(['service.yml'])
high_rev_acc_opps_net.shape
preds = learn.predict(is_test=True)
tree_tunned.best_params_
ds = remote_data.sel(time='2014-08-19').sum('time') $ print ds
del (reduced_df, chunks_list) $ gc.collect()
summary.head()
logging.disabled = True
pd_data=pd_data.fillna('') $ pd_data.loc[pd_data['paidList']=='',['paidList']]=pd_data.paymentSerial.map(lambda x:[x] if x else [])
df['Year'] = df['created_date'].dt.strftime('%Y') $ df['YearMonth'] = df['created_date'].dt.strftime('%Y/%m') $ df['Month'] = df['created_date'].dt.strftime('%b')
subset=data.iloc[0:10] $ subset
df_pivot = df_cryptdex.pivot(index='date',columns='symbol',values='close').reset_index()
news_df = (news_df.groupby(['topic']) $               .filter(lambda x: len(x) >= 25))
results_lumpedTopmodel, output_LT = S_lumpedTopmodel.execute(run_suffix="lumpedTopmodel_hs", run_option = 'local')
master_list = pd.DataFrame( $     {'Word': vocab, $      'Count': dist, $     })
print(diagnosis_DXSUM_PDXCONV_ADNIALL.columns.values) $ print(diagnosis_DXSUM_PDXCONV_ADNIALL['DXCURREN'].unique()) $ dfv = diagnosis_DXSUM_PDXCONV_ADNIALL['DXCURREN'].value_counts(dropna=False) $ print(dfv) $ print(diagnosis_DXSUM_PDXCONV_ADNIALL['DXCURREN'].shape[0])
twitter_archive_master = pd.concat([df, image,tweet_text], axis=1, join='inner')
tmp_df[tmp_df.columns[tmp_df.dtypes == 'category']] = tmp_df[tmp_df.columns[tmp_df.dtypes == 'category']].astype(str) $ tmp_df['Judgment.In.Favor.Of.Plaintiff'] = tmp_df['Judgment.In.Favor.Of.Plaintiff'].astype(int) $ tmp_df['Case.Duration'] = tmp_df['Case.Duration'].dt.days
precip_data_df1=precip_data_df.copy() $ precip_data_df1.reset_index(inplace=True,drop=False) $ precip_data_df1.head(5)
my_df.columns = ['text', 'polarity'] $ my_df.head() $
logmodel.coef_
zone_train = df_run[df_run['zone'].notnull()] $ zone_test = df_run[df_run['zone'].isnull()] $ zone_train.head()
sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)
fig,axes = plt.subplots(1, 2, figsize = (16,4), sharey= True) $ axes[0].plot_date(x=obama.created_at, y = obama.n_words,linestyle = '-',marker='None') $ axes[1].plot_date(x=trump.created_at, y = trump.n_words,linestyle='solid',marker='None') $ plt.savefig("fig/n_word_comparison.png")
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old]) $ alpha = norm.ppf(1-(0.05/2)) $ print('z-score: ' + str(z_score)) $ print('p_value: ' + str(p_value)) $ print('alpha: ' + str(alpha))
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0) & (jobs.Group == 'p_meiler')][['Memory','Wait']].groupby('Memory').agg(['mean', 'median','count']).reset_index()
df_joined.groupby(['country','ab_page_new_page']).converted.count()
dictionary = Dictionary(messages_clean) $ corpus = [dictionary.doc2bow(text) for text in messages_clean]
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score, p_value)
importances.sort_values(by='Gini-importance', ascending=False)[0:50]
print (autos['price'].value_counts().head(10)) $ print(autos['price'].value_counts().tail(10) )
len(pres_df['hour_aired'].value_counts()) # sanity check - should be 24
pred_clean.drop(['jpg_url','img_num'], axis=1,inplace=True);
full[full['Readmitted'] == 1].groupby(['Patient']).diff()['AdmitDate'].tail()
m1.df
import PairsTrading.api.PairsTrading as pt $
grid_df.tail()
random.shuffle(porn_ids) $ porn_bots = porn_ids[:6000]
autos[["price","odometer"]].head()
X.shape
data['Sales'].diff(periods=1).head()
tweets.describe(include='all')
tt_json.info()
engine = create_engine("sqlite:///Resources/hawaii.sqlite")
z_Score,p_score = sm.stats.proportions_ztest([convert_old,convert_new],[n_old,n_new]) $ z_Score
twitter_archive_df_clean[~pd.isnull(twitter_archive_df_clean['retweeted_status_id'])]
S.decision_obj.stomResist.value = 'simpleResistance' $ S.decision_obj.stomResist.value
station_distance['Sex'] = station_distance.Gender.map({0:'unknown', 1:'male', 2:'female'})
coinbase_btc_eur['Timestamp'] = coinbase_btc_eur["Time"].apply(lambda row: unix_to_datetime(row))
plt.figure(figsize = (10,5)) $ sns.heatmap(df, cmap="PuBuGn").set_title("Month & Day of Week")
for c in ccc: $     for i in vhd[vhd.columns[vhd.columns.str.contains(c)==True]].columns: $         vhd[i] /= vhd[i].max()
churned_unordered.head()
INQ2018.Create_Date.dt.month.value_counts().sort_index()
rf.score(X_train_all, y_train)
df_train = pd.DataFrame({'text': train_texts, 'labels': [0] * len(train_texts)}, columns=col_names) $ df_val = pd.DataFrame({'text': val_texts, 'labels': [0] * len(val_texts)}, columns=col_names) $ df_train.to_csv(LM_PATH / 'train.csv', header=False, index=False) $ df_val.to_csv(LM_PATH / 'test.csv', header=False, index=False)
duration_df.dropna(axis=0, inplace=True)
df.groupby("cancelled")["weekend_pickup"].mean()
plt.figure(figsize=(8, 5)) $ plt.scatter(prepared_train.favs_lognorm, prepared_train.views); $ plt.title('The distribution of the favs_lognorm and number of the views without outliers');
df.shape
nar3=nar2.merge(df_loan2[['fk_loan',0]].rename(columns={0:'irr'}),on='fk_loan')
autos_VW = autos[autos["brand"] == "volkswagen"] $ autos_VW["price"].describe()
grouped2 = df.groupby(['year','month']) $ cntdf2 = pd.DataFrame({'count' : grouped2.size()}).reset_index() $ piv2 = cntdf2.pivot('year','month','count') $ sns.heatmap(piv2, cmap="YlGnBu").set_title('Tweet Frequency')
sum(df2.converted)/len(df2.converted)
old_page_converted = np.random.binomial(n_old,p_old)
for row_index,row in df.iterrows(): $    print(row_index,row)
tipsDF = tipsDF.drop(tipsDF.columns[0], axis = 1) $ tipsDF.head()
df.managers[:10]
S_lumpedTopmodel.initial_cond.filename
new_page_converted = np.random.choice([0, 1], size = n_new, p = [p_new, 1 - p_new])
walmart.start_time # The start of Q1 is 1st Feb rather then the normal 1 st Jan
np.exp(-0.0335 + 0.0877)
pd.crosstab(df_protest.loc[:, 'Violent_or_non_violent'], df_protest.loc[:, 'Type'])
airbnb_df.filter(regex='fee|price|deposit', axis=1).head()
plt.hist(inches, 40);
from imblearn.over_sampling import SMOTE
NameEvents.head(1)
from scipy.stats import norm $ critical_val = norm.ppf(1-(0.05/2)) $ critical_val
df_variables = pd.merge(df_variables,df_features,how="left",on="CustID",indicator=True)
read_table("adj_close_stock_data_yahoo_2005_2010.txt", sep="\s+", parse_dates=[[0,1,2]], na_values=["-"])
InfinityWars_Predictions.head()
dep.get_ages(exclude='ing-diba')
cm = metrics.confusion_matrix(y_val, res_val) $ print(cm) $ print(classification_report(y_pred=res_val,y_true=y_val)) $ print(np.round(f1_score(y_pred=res_val,y_true=y_val),3))
np.datetime64('2015-07-04 12:00')
print('RF score: {:.4f}'.format(scores))
old_page_converted = np.random.binomial(1, p_old, n_old) $ p_old_sample = old_page_converted.mean() $ p_old_sample $
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
df.head()
new_page = df2.query('landing_page == "new_page"') $ new_page.landing_page.count()
dfstationmeandinner[['STATION','totals']].nlargest(10, ['totals'])
plt.pie(fare_city, labels=type_city,explode=explode, colors=colors, $         autopct="%1.1f%%", shadow=True, startangle=140) $ plt.axis("equal")
k = pd.concat([j, c], axis=1, join_axes=[a.index])
print ("Number of unique users:",df.nunique()['user_id'])
all_lum.iloc[0:1]
vacancies['created'] = vacancies['created'].apply(lambda x: dateutil.parser.parse(x))
m3 = md2.get_model(opt_fn, 1500, bptt, emb_sz=em_sz, n_hid=nh, n_layers=nl, $            dropout=0.1, dropouti=0.4, wdrop=0.5, dropoute=0.05, dropouth=0.3) $ m3.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ m3.load_encoder(f'adam3_10_enc')
df.to_html('resources/html_table_marsfacts.html')
text = "Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers." $ print(text)
kimanalysis.getfile(result, 'results.edn', contentformat='edn')
endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url') $ print endpoint_published_models
TestData[['DOB_clean', 'Lead_Creation_Date_clean']].describe()
val_idx = np.flatnonzero( $     (df.index<=datetime.datetime(2014,9,17)) & (df.index>=datetime.datetime(2014,8,1)))
df = pd.concat(map(pd.read_csv, glob.glob("*.csv")))
old_crs = np.array(old_crs) $ old_crs.mean()
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://image.slidesharecdn.com/tdscspeech-151114010319-lva1-app6891/95/science-in-text-mining-8-638.jpg?cb=1447463123 ")
commits = git_log[['additions', 'deletions', 'filename']]\ $     .join(git_log[['sha', 'timestamp', 'author']]\ $     .fillna(method='ffill'))\ $     .dropna() $ commits.head()
df_mod = countries_df.set_index('user_id').join(df.set_index('user_id'), how='inner') $ df_mod.head()
june_acj_data.head()
print 'Read data from the web about Microsoft in 2012 - 2013' $ from pandas_datareader import data, wb $ msft = data.DataReader('MSFT', data_source='google', start='2012-01-01', end='2013-12-30')
test2 = vec2.fit_transform(df.message[1]) $ print(vec2.get_feature_names()) $ test2.toarray()
np.mean(df2.days_repayment)
lv_workspace.cfg['indicators']
crimes_wea=pd.merge(crimes, weather, on='date', how='left') $ crimes_wea.tail()
df_CLEAN1C.head()
%%time $ turnstile_timeseries_list = [] $ for key, turnstile in turnstiles: $     turnstile_timeseries_list.append(get_time_series(turnstile)) $ print('Aggregated data for {} turnstiles.'.format( len(turnstile_timeseries_list)))
!./container/run_local_server.sh $(image)
bus_sf = bus[bus['postal_code_5'].isin(sf_dense_zip)] $ num_missing_in_each_zip = bus_sf.groupby('postal_code_5')['longitude'].agg(lambda x: sum(x.isnull())).astype('int').sort_values(ascending =False)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
records2.loc[records2['Age'].isnull(), 'Age'] = records2['Age'].mean() $ records2.loc[records2['GPA'].isnull(), 'GPA'] = records2['GPA'].mean() $ records2.loc[records2['Days_missed'].isnull(), 'Days_missed'] = records2['Days_missed'].mean()
from sklearn.ensemble import RandomForestRegressor $ model = RandomForestRegressor() $ print ('Random forest') $ RandomForestRegressor_model = reg_analysis(model,X_train, X_test, y_train, y_test)
trump.describe()
sandwich_counts.head(3)
tweet_json.retweeted_status.value_counts()
human_genome_length = gdf.length.sum() $ print("The length of the human genome is {} bases long.".format(human_genome_length))
sessions =pd.read_csv('https://raw.githubusercontent.com/ben519/DataWrangling/master/Data/sessions.csv') $ sessions.head()
annual_precip = (session $                 .query(Measurement.date, Measurement.prcp) $                 .filter(and_(Measurement.date <= '2017-12-31', Measurement.date >= '2017-01-01')) $                 .all()) $ annual_precip
for ix, file in enumerate(s3_files): $     os.environ["gs_key"] = "gs://resource-watch-public/" + s3_key_origs[ix] $     os.environ["asset_id"] = "users/resourcewatch/" + file[:-4] $     !earthengine upload image --asset_id=$asset_id $gs_key
!cp ../../drive/ColabNotebooks/AV_innoplexus_html/sample_submission.csv .
colnames = ['username','userid','blocked_time','blocked_reason','type'] $ type(colnames) $ users=pd.read_table('users.tsv',sep="\t",header=None,names=colnames) $ users=pd.DataFrame(users) $ users.head()
print Y_train.shape $ print X_train.shape
MAE_score = metrics.mean_absolute_error(test_target, test_preds) $ MSE_score = metrics.mean_squared_error(test_target, test_preds) $ RMSE_score = np.sqrt(metrics.mean_squared_error(test_target, test_preds)) $ R2_score = lr.score(test_features, test_target)
import numpy as np $ import pandas as pd $ import matplotlib.pyplot as plt $ %matplotlib inline
data = data.sort_values(['order','lastAttacked'],ascending=[0,1]) $ data.head()
df_predictions_clean.p2 = df_predictions_clean.p2.str.title()
w_change = w - w.shift(1) $ w_change[coins_infund].plot(legend=False) $ plt.show()
df_concat_2["message_likes_dummy"] = np.where(df_concat_2.message_likes_rel > 500, 1, 0)
mb.xs(1, level='Patient')
fed_reg_data = r'data/fed_reg_data.pickle' $ final_df.to_pickle(fed_reg_data)
X_train = df.loc[:25000, 'review'].values $ y_train = df.loc[:25000, 'sentiment'].values $ X_test = df.loc[25000:, 'review'].values $ y_test = df.loc[25000:, 'sentiment'].values
inspector = inspect(engine) $ inspector.get_table_names()
print(" the probability of an individual converting regardless of the page they receive = ", df2.converted.mean())
df2.converted.mean()
file = 'https://raw.githubusercontent.com/mwentzWW/petrolpy/master/Zeus/Sample_Production/Sample_Prod_Data.xlsx' $ well_data = pd.read_excel(file) $ well_data.columns = [c.lower() for c in well_data.columns] $ well_data.info()
a = np.arange(1, 5); b = np.arange(5, 9); a, b, a+b, a-b, a/b.astype(float)
df4[df4['country_US']==1]['converted'].mean()
log_model = sm.Logit(y, X)
data.loc[(80, slice('20150117','20150417'), $              'put'), :].iloc[:, 0:4]
df2.drop('group',inplace=True, axis=1) $ df2.head(2)
learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)
plt.figure(figsize=(15, 5)) $ sns.barplot(x="purpose", y="default", data=train,estimator=np.mean) $ plt.xticks(rotation='vertical');
models = [train(hits_df.loc[:date]) for date in model_dates]
opener_lexicon = pd.read_csv('https://raw.githubusercontent.com/opener-project/public-sentiment-lexicons/master/propagation_lexicons/es/es.lemma.sy.an.hypo.rels.maxdepth5.seed500.maj.gold.csv', sep=';', header=None) $ opener_lexicon.columns = ['x','pos', 'polarity', 'xx','word','xxx'] $ opener_lexicon.head()
findname('This is a Dasani Kingfisher from Maine. His name is Daryl. Daryl doesnt like being swallowed by a panda.  ')
dfEPEXbase.corr()
%%time $ dfHashtags = pd.DataFrame(sentences)
df_h1b_ft_US_Y.pw_1.describe()
def days_since_last_order(d): $     timedelta = pd.to_datetime('2017-10-17') - d.max() $     return timedelta.days
X_train.iloc[0:5,:12]
binary_columns = genres $ real_columns = ['Year', 'Body_Count'] $ Bernoulli = pd.DataFrame(data=np.zeros((2,len(binary_columns))), columns=binary_columns, index=['theta_0', 'theta_1']) $ Gaussian = pd.DataFrame(data=np.zeros((4,len(real_columns))), columns=real_columns, index=['mu_0', 'sigma2_0', 'mu_1', 'sigma2_1'])
df.dropna(axis="columns")
guinea = concatenated.loc[concatenated['country'] == 'guinea'] $
np.exp(result1.params)
df2['intercept'] =1 $ mapper = {'treatment':1, 'control':0} $ df2['ab_page'] = df2.group.map(mapper)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser()) $ news_source = ["FoxNews", "CNN", "BBCWorld", "CBSNews", "nytimes"]
s = pd.Series([False, False, True, True, True], index=[2, 3, 4, 5, 6]) $ s
_ = ok.grade('q02a') $ _ = ok.backup()
sb.heatmap(components3)
autos["num_photos"].value_counts()
print(team.shape) $ team.head()
data_scrapped['datePublished'] = data_scrapped['datePublished'].apply(lambda x:datetime.strptime(x,'%Y-%m-%d'))
X_mice.head()
log_mod_int = sm.Logit(df_new['converted'], df_new[['intercept', 'treatment', 'UK', 'US']]) $ results_int = log_mod_int.fit() $ results_int.summary()
plt.plot(data['Year'], data['Body_Count'], 'rx')
sets.head(12)
wrd_clean['rating_denominator'].describe()
use_columns=['total fuel (mmbtu)', 'generation (MWh)', $                                                'elec fuel (mmbtu)'] $ eia_extra = (eia_total_monthly.loc[idx[:,:,:], use_columns] - $              eia_facility_fuel.loc[idx[:,:,:], use_columns]) $ eia_extra.loc[idx[['HPS', 'DPV'],:,:], use_columns] = eia_total_monthly.loc[idx[['HPS', 'DPV'],:,:], use_columns] $
total_number_of_events = len(new_events) $ print('There are {} events to export.'.format(total_number_of_events))
pickle.dump(bow_df,open('../proxy/dataset3_textb','wb'))
df[df['Complaint Type'].str.contains('Nois')]['Complaint Type'].value_counts()
sys.stderr.write("This is stderr text\n") $ sys.stderr.flush() $ sys.stdout.write("This is stdout text\n") $ print(sys.argv) $
df2[df2['user_id'].duplicated(keep = False)]
foo.sort( foo['count'].desc() ).show()
lda_tf2 = models.LdaModel.load(os.path.join(outputs, 'model_tf2.lda')) $ corpus_lda_tf2 = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf2.mm'))
logistic = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logistic.fit()
clf.score(X_test, y_test)
df['UP'].sum()/df['UP'].count()
df_new['US_ab_page'] = df_new['ab_page']*df_new['US'] $ df_new['UK_ab_page'] = df_new['ab_page']*df_new['UK'] $ log_mod_con = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page','US_ab_page', 'UK_ab_page', 'US', 'UK']]) $ results = log_mod_con.fit() $ results.summary()
sentiments_pd.count()
results_sim_rootDistExp, output_sim_rootDistExp = S.execute(run_suffix="sim_rootDistExp", run_option = 'local')
ab_data.query('converted == 1').shape[0] / ab_data.shape[0]
test_df[["id", "labels"]].to_csv("submission_8.27.csv", index=False)
sub_set = c_df[['DEVICE_MODEL','CUSTOMER_ID_CAT']] $ sub_set = sub_set.drop_duplicates(keep='first') $ sub_set.groupby('DEVICE_MODEL').size().reset_index(name='counts')
with open('topic_list_5_29.pkl', 'wb') as piccle4: $     pickle.dump(topic_list, piccle4)
df.head()
data.loc[data['hired']==1].groupby('category').hourly_rate.std()
support.amount.sum() / merged.amount.sum()
y2 = df3['converted'] $ X2 = df3[['intercept', 'new_page', 'UK', 'US']] $ X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20, random_state=0)
tvec = TfidfVectorizer(stop_words='english') $ X_train_matrix = tvec.fit_transform(X_train['title']) $ X_test_matrix = tvec.transform(X_test['title'])
obs_diffs = df[df['group'] == 'treatment']['converted'].mean() - df[df['group'] == 'control']['converted'].mean() $ obs_diffs
n_new = df2.query("landing_page == 'new_page'")['landing_page'].count() $ n_new
((loans.originated_since_date<datetime.date(2015,2,28)) & (loans.payback_state=='payback_complete')).sum()
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv', nrows=10)
save_combined_df( donald_combined_df, 'realDonaldTrump' ) $ !ls ../../data/
result3.summary2()
autos.drop(["num_pictures","seller","offer_type"],axis = 1,inplace = True)
users_target = tweet_df.rename(columns={'id':'tweet_id'})[tweet_df.rename(columns={'id':'tweet_id'})['tweet_id'].isin(users_target['tweet_id'])].set_index('tweet_id').join(users_target[users_target['tweet_id'].isin(tweet_df.rename(columns={'id':'tweet_id'})['tweet_id'])].set_index('tweet_id'), lsuffix='', rsuffix='_right', how='inner')[['username', 'personality_type']].set_index('username')
author_name_id = xml_in_sample.drop_duplicates(subset=['authorId','authorName'], keep='first')[['authorId','authorName']] $ df_movies = pd.merge(df, author_name_id, left_on=['movieId'], right_on=['authorId'],  how='left')[['movieId', 'authorName']] $ df_movies.head() $ df_movies.drop_duplicates(subset=None, keep='first', inplace=True)
bwd = df[['Store'] + columns].sort_index().groupby("Store").rolling(7, min_periods=1).sum()
from nltk.corpus import stopwords $ from sklearn.feature_extraction.text import CountVectorizer
datatest.loc[datatest.place_name == 'Bs.As. G.B.A. Zona Oeste', 'lat'] = datatest.loc[datatest.state_name == 'Bs.As. G.B.A. Zona Oeste', 'lat'].mean() $ datatest.loc[datatest.place_name == 'Bs.As. G.B.A. Zona Oeste', 'lon'] = datatest.loc[datatest.state_name == 'Bs.As. G.B.A. Zona Oeste', 'lon'].mean()
s519281_df = pd.DataFrame(s519281) $ print(len(s519281_df.index)) $ s519281_df.info() $ s519281_df.head(5)
df['Picture_Location'].replace('NA', np.NaN, inplace = True)
start_end_points_df = pd.DataFrame(start_end_points, columns = ['Lat0', 'Lat1', 'Lon0', 'Lon1']) $ start_end_points_df.head()
artistYearGenreDF = pd.DataFrame({'Artist': artist, 'Years': years, 'Genre': genre})
def usersAndMisspelled(artistID): $     print("MisspelledArtistID =", artistID, " -  Name =", artistDF[artistDF.artistID==artistID].take(1)[0][1]) $     standardArtistID = artistAliasDF[artistAliasDF.MisspelledArtistID == artistID].take(1)[0][1] $     print("StandardArtistID =", standardArtistID, " - Real name =", artistDF[artistDF.artistID==standardArtistID].take(1)[0][1]) $     userArtistDF[userArtistDF.artistID == artistID].show()
cryptos.percent_change_7d > 25
print 'Total walking distance covered during the trip: %.2f km \nTotal steps covered during the trip: \t\t%d steps' % ( $     sum(walking_df['ttl_wal_distance']),sum(walking_df['ttl_steps']))
plt.show()
predicted_outcome_first_measure.head()
max_value = rankings['total_points'].idxmax() #idxmax() returns the index of the max value from column total_points $ rankings.iloc[max_value] #iloc[] looks up the observation when given an index number
RMSE, predictions = workflow(working_data, get_split, train_model, get_rmse, n_train = 600,n_test = 60) $ print('Test GRU model RMSE: %.3f' % RMSE)
old_page_converted = df2.query('landing_page == "old_page"').sample(n_old, replace=True)
data_rec = data.view(np.recarray) $ data_rec.age
df_cal.boxplot(column='is_all_day')
weather_max = weather_all.groupby('Station Name').max() $ weather_max.sort_values('Wind Spd (km/h)', ascending=False)
piv_train = df_train.shape[0] $ df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True) $ df_all = df_all.drop(['id', 'date_first_booking'], axis=1) $ df_all = df_all.fillna(-1)
vect = TfidfVectorizer(ngram_range=(2,4), stop_words='english') $ summaries = "".join(amanda_tweets['text']) $ ngrams_summaries = vect.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
pandas.Series({'a': 12, 'b': 42})
autos['price'].value_counts().sort_index(ascending=False).head(10)
from sklearn.metrics import jaccard_similarity_score $ jaccard_similarity_score(y_test, yhat)
autos.columns
df.reset_index(inplace=True) $ df=df.merge(bwd, 'left', ['date', 'store_nbr'], suffixes=['', '_bw']) $ df=df.merge(fwd, 'left', ['date', 'store_nbr'], suffixes=['', '_fw'])
ad_group_performance.drop(columns=['TheAnswer']) $ ad_group_performance
url = "https://raw.githubusercontent.com/miga101/course-DSML-101/master/pandas_class/TSLAday.csv" $ tesla_days = pd.read_csv(url, index_col=0, parse_dates=True) $ tesla_days
print(pd.value_counts(train_df['channel'])[:20])
y_pred = rf.predict_proba(X_test)[:, 1] $ fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred)
dta.violations.head()
trip_data_sub["lpep_pickup_datetime"] = trip_data_sub.lpep_pickup_datetime.apply(lambda x: (pd.to_datetime(x) -  pd.datetime(2015, 9, 1))/ np.timedelta64(1, 's')) $ trip_data_sub["Lpep_dropoff_datetime"] = trip_data_sub.Lpep_dropoff_datetime.apply(lambda x: (pd.to_datetime(x) -  pd.datetime(2015, 9, 1))/ np.timedelta64(1, 's'))
df_jse.loc[:, 'Open'].resample('W').mean().head()
df["text"] = df["extended_tweet"].combine_first(df["text"])
[x.text for x in html.find_all('a', {'class':'next '})]
open_prs.aggregations
lmscore.summary() ##I think I can use any one of the two summary types
offices.head()
liberiaDf.index = ['-'.join([date.split("/")[2], date.split("/")[0].zfill(2), date.split("/")[1]]) $                      for date in liberiaDfOld.index] $ liberiaDf.index.name = 'Date' $ liberiaDf.head()
np.exp(-0.0149), 1/np.exp(-0.0149)
print("Number of rows : " + str(df.shape[0]))
lr.fit(X_train, y_train) $ lr.score(X_test, y_test)
numbers_df = pd.DataFrame(numbers, index = ['number_1', 'number_2','number_3']) $ numbers_df
DataSet_sorted = DataSet.sort_values('prediction', ascending=False)
df.date
df[df.genre.isnull()]
print("LED off") $ GPIO.output(18,GPIO.LOW) $ time.sleep(1)
cr_control = df2[df2['group'] == 'control']['converted'].mean() $ cr_control $
pp.head()
all_sets["setSize"] = all_sets.apply(lambda x: x.cards.shape[0], axis = 1)
%load_ext rpy2.ipython $
X = trip_data_sub.ix[:,0:trip_data_sub.shape[1]].values $ y = trip_data_sub.ix[:,trip_data_sub.shape[1]-1].values $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
df.to_html('table.html')
mod.predict_proba(a)
lq2015_combined.columns.values
new_page_converted = np.random.choice([0, 1], size=n_new, p=[(1-p_new), p_new])
print(str(autos["price_euro"].unique().shape[0]) + " unique values") $ print(autos["price_euro"].describe())
a[a.find(':') + 1:].strip()
authors['mean'].head()
melted_total = pd.melt(total_df, id_vars = ['Res_id','Name', 'Neighbourhood','Url','Review_count', 'Rating'], $                         value_vars = ['cat1','cat2','cat3'], value_name = 'Categories').drop('variable',axis = 1)
pipeline = Pipeline([('posf', PartOfSpeechFilter()), $                      ('cv', CountVectorizer()) $                    ]) $ pipeline.set_params(cv__lowercase=True,cv__max_df=0.95, cv__min_df=0.01, cv__stop_words='english')
new_df.groupby('created_time').sum()
users_converted = df.query('converted == 1').user_id.count() $ users_conv_uniq = df.query('converted == 1').user_id.nunique() $ users_total = df.shape[0] $ users_unique = df.user_id.nunique()
print(autos["seller"].value_counts()) $ print(autos["offer_type"].value_counts())
new_reps.newDate[new_reps.Trump.isnull()]
new_texas_city.columns = ["Measurement_date", "Compaction"] $ new_texas_city.loc[546, "Measurement_date"] = "2015-07-23 00:00:00" $ new_texas_city.tail(10)
twitter_goodreads_users_df.head(5)
data2.head()
X_train_dummify, _ = custom_dummify(X_train, 0.01) $ df_imput = MICE(n_imputations=200, impute_type='col', verbose=False).complete(X_train_dummify.as_matrix())
top_10 = scores.loc[168].argsort()[::-1][:11] $ trunc_df.loc[list(top_10)]
print('Best score for data:',np.mean(forest_clf.feature_importances_))
df_merged['link.domain_resolved'].value_counts().head(25)
autos['date_crawled'] = autos['date_crawled'].str[:10].str.replace("-","").astype(int) $ autos['ad_created'] = autos['ad_created'].str[:10].str.replace("-","").astype(int) $ autos['last_seen'] = autos['last_seen'].str[:10].str.replace("-","").astype(int)
os.path.exists("classB.csv")
train_df['parcelid'].nunique(dropna=False)
le.fit(df2['tag']) $ df2["tag_num"] = le.fit_transform(df2['tag'])
print('number of deaths in 2014:',df['2014']['battle_deaths'].sum())
import os $ import matplotlib.pyplot as plt
df2=df2.drop(treat_old.index) $ df2=df2.drop(control_new.index) $ df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
auth = tweepy.OAuthHandler(consumer_key, consumer_secret, callback_url)
df = pd.merge(train, $ membership_loyalty[(membership_loyalty.transaction_date < datetime.strptime('2017-03-01', '%Y-%m-%d'))], $ on=['msno'], $ how='left')
r['A'].aggregate([np.sum,np.mean])
yelp_dataframe.to_csv("yelp_reviews.csv", index=False, encoding="UTF-8")
Base.classes.keys()
new_page_converted = np.random.binomial(n_new,p_new,10000)/n_new $ print (new_page_converted)
print(cbow_m1.wv.similarity('love', 'like')) $ print(cbow_m2.wv.similarity('love', 'like')) $ print(cbow_m3.wv.similarity('love', 'like')) $ print(cbow_m4.wv.similarity('love', 'like')) $ print(cbow_m5.wv.similarity('love', 'like'))
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-pold, pold]) $ print(old_page_converted)
funded = train.loc[train.final_status == 1, :] $ failed = train.loc[train.final_status == 0, :]
repos_rank = repos_users.reindex_axis(['full_name', 'rank'], axis=1) $ repos_rank.to_csv('data/2017/repos-ranks.csv', index=False)
nitrodata['ActivityStartDate'] = pd.to_datetime(nitrodata['ActivityStartDate'],format="%Y-%m-%d")
df['messages'].value_counts()
df_new['uk_intercept'] = df_new['country'].replace(('US','UK','CA'),(0,1,0)) $ lm = sm.OLS(df_new['converted'],df_new[['intercept','uk_intercept']]) $ lm.fit().summary()
total1 = pd.read_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_1/target_winter2017_night2.csv',\ $                     keep_default_na=False, na_values=[""]) $ total1[80:]
df_clean=df_clean.drop(['favorited','truncated'],axis=1) $
r.loc['20170228':'20170307', ['share']]
p_old = df2[df2['converted'] == 1].user_id.nunique() / df2.user_id.nunique() $ p_old
cc['logmarket'] = np.log1p(cc['market']) $ plt.hist(cc['logmarket']) $ plt.show()
new_cases_liberia_concat.columns = ["Date", "New Case/s (Suspected)","country","Date1","New Case/s (Probable)","country","Date2","New case/s (confirmed)","country","Total_new_cases_liberia"] $ Total_new_cases_liberia =new_cases_liberia_concat[['Date','Total_new_cases_liberia']] $ Total_new_cases_liberia.head() $
df.to_json("json_data_format_split.json", orient="split") $ !cat json_data_format_split.json
df2.plot.pie(subplots=True,figsize=(10,10), autopct='%.1f', legend='best') $ plt.title('IOOS Percent of total messages to the GTS') $
first_value_greater_50 = np.min([i for i,v in enumerate(cum_sum_percentage_payments) if v > 50]) $ first_value_greater_50
df[df.Target == 5]
feature_imp=pd.DataFrame(list(zip(features,xgb_model.feature_importances_))) $ column_names= ['features','XGB_imp'] $ feature_imp.columns= column_names
df2[df2.duplicated('user_id')]
gc.collect()
new_page_converted = np.random.binomial(1,p_new,n_new)
from pyspark.sql.types import DoubleType $ lookup_user_means = ... $ def zero_mean(user_id, rating): $     pass $ df_centered = df_selected.withColumn('ratings_centered', df_selected.stars - F.avg('stars').over(w2))
df_postgres = psql.read_sql('SELECT classification, origin, destination FROM cable LIMIT 5;', con=conn) $ df_postgres.head()
(df[df.converted==1].count())/df.count()
np.count_nonzero(df.isnull().values)
data = sh.flatten_event(raw)
df_twitter_copy = df_twitter_copy.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis = 1)
gs = GridSearchCV(lg,param_grid=lg_params,verbose=1,n_jobs = 3) $ gsmodel= gs.fit(X_train,y_train)
clean_en_test_df = pd.concat([time_df, clean_en_test_df], axis=1)
crimes_by_yr_month.reset_index(inplace=True) $ crimes_by_yr_month.columns = ["year","month","crime_count"]
to_be_predicted_Day3 = 17.68195036 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
class_merged.dtypes
gpCreditCard.Trip_distance.describe()
house_data.info()
concat_3 = pd.concat([df1, df2, df5], axis=1) $ concat_3
date_df = pd.DataFrame({'date': date, 'weekday': weekdays, 'count': count, 'duration(min)': duration}) $ date_df.head()
m1 = VAR(queries=['de','het'], timefield = 'META.ADDED', granularity = 'day')
volume_weekly = vol.groupby([vol.index.year, vol.index.week]).sum() $ volume_weekly
gbm = GradientBoostingClassifier(max_depth = 6, n_estimators= 400, max_features = 0.3) $ gbm.fit(X, y) $ y_score = gbm.predict_proba(X)[:,1] $ metrics.roc_auc_score(y, y_score)   # 0.77
convert_old = df2.query('landing_page=="old_page"').query('converted==1').shape[0] $ convert_new = df2.query('landing_page=="new_page"').query('converted==1').shape[0] $ n_old = df2.query('landing_page=="old_page"').shape[0] $ n_new = df2.query('landing_page=="new_page"').shape[0]
df_cal.head()
def upvotes(result): $     upvotes = [x.text for x in soup.find_all('div', {'class':'score unvoted'})]
apr_jun_2018 = df.ix['2018-04-01 00:00:00':'2018-06-01 00:00:00'] $ apr_jun_2018.head()
p_t=df2.query("group=='treatment'")['converted'].mean() $ p_t
df.to_csv('superbowl_playbyPlay.csv')
df2=pd.read_csv("https://s3.amazonaws.com/tripdata/201707-citibike-tripdata.csv.zip")  #July 2017
rf = RandomForestClassifier() $ rf.fit(X_train_all, y_train) $ rf.score(X_test_all, y_test)
df_melt.drop(['Result Types for Jan  1, 2017', 'Result Types for Sep  1, 2017'], axis=1, inplace=True)
df2[((df2['group'] == 'treatment') ==(df2['landing_page'] == 'new_page')) == False].shape[0]
df.to_json("json_data_format_index.json", orient="index") $ !cat json_data_format_index.json
df_dd.shape $ df_dd.head()
learner.save('lm_last_ft')
df_Sessions = df_Sessions.drop(['latt','long','country_code','rt'],axis=1)
((pf.cost.sum()/100)/(max(pf.day)-min(pf.day)).days)*365
len(df[~(df.user_properties == {})])
taxiData.Trip_distance.describe()
implied_therapy = ['Therapy', 'New Patient Therapy', 'Therapy Telepsychiatry'] $ implied_doctor = ['Follow up Telepsychiatry', 'New Patient Therapy Telepsychiatry',\ $                   'New Patient MD Adult', 'New Patient MD Adult Telepsychiatry'] $ merged1['Specialty'].loc[merged1['ReasonForVisitName'].isin(implied_therapy)] = 'therapist' $ merged1['Specialty'].loc[merged1['ReasonForVisitName'].isin(implied_doctor)] = 'doctor'
for title, artist in unique_title_artist[current_len:min(current_len+batch_size, len_unique_title_artist)]: $     youtubeurl = urllib.parse.quote_plus(YOUTUBE_URL_TEMPLATE.format(gc.search(title +' '+artist)), safe='/:?=') $     youtube_urls[str((title, artist))] = youtubeurl
dfOther.head(80)
(df.loc[df['margin_val'] < 0]).head()
g = df[(df.genre.isin(df.Genre)==False)] $ g[['genre','Genre','Name','_merge','Platform','Standard_Plat']].head()
df.groupby("cancelled")["awards_referral_bonus"].mean()
import statsmodels.api as sm $ convert_old = df2.query('landing_page == "old_page"').converted.sum() $ convert_new = df2.query('landing_page == "new_page"').converted.sum() $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
users_converted = df.query('converted == 1').count()[0] / (df.query('converted == 1').count()[0] + df.query('converted == 0').count()[0]) $ users_converted_percentage = users_converted * 100 $ print('The proporation of converted users are ' + str(users_converted_percentage) + '%')
for df in (joined,joined_test): $     df["CompetitionOpenSince"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, $                                                      month=df.CompetitionOpenSinceMonth, day=15)) $     df["CompetitionDaysOpen"] = df.Date.subtract(df.CompetitionOpenSince).dt.days
dfRegMet2016 = dfRegMet[dfRegMet.index.year == 2016]
tweet_clean.rename({'id': 'tweet_id'}, axis=1, inplace=True)
df_ab_raw.info()
cat_outcomes['outcome_subtype'].value_counts()
train.StateHoliday = train.StateHoliday!='0' $ test.StateHoliday = test.StateHoliday!='0'
southern_sea_level.year == northern_sea_level.year
df = pd.merge(tweets, users[['screenName', 'lang']], how='left', on='screenName')
cur = conn.cursor() $ cur.execute('ALTER TABLE actor ADD COLUMN middle_name varchar(45);') $ df = pd.read_sql('SELECT * FROM actor', con=conn) $ df.head()
time_of_day = faa_data_pandas['TIME_OF_DAY'].value_counts() $ print(time_of_day)
treat_convert = df2[df2['group']== 'treatment'].converted.mean() $ print(treat_convert)
within_sakhalin_request_url = HERBARIUM_SEARCH_URL + '?' + '&'.join(map(lambda x: x[0] + '=' + quote(x[1].strip()), query_sakhalin_bbox))
df_clean['tweet_id'] = df_clean['tweet_id'].astype(str)
xml_in.head()
image_clean.info()
df.max()
tips.set_index(["sex","day"]).head(5)
print('Method 1: ', '\n') $ for i in set(csv_df['names']): $     print(i, " :", len(set(csv_df['date'][csv_df['names']==i]))) $ print('\n', 'Method 2: ') $ csv_df.groupby(['names'])['date'].nunique()
type(room_temp).__dict__['temperature'].__get__(room_temp,Celsius)
open('test_data//write_test.txt', mode='w').close()
tfv = TfidfVectorizer(ngram_range=(1,4), max_features=2000) $ X1 = tfv.fit_transform(clean_text1).todense() $ print X1.shape
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==835246439529840640,'rating_numerator']=13 $ twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==835246439529840640,'rating_denominator']=10
df_onc = pd.read_csv(r'F:\Projects\Pfizer_mCRPC\Data\pre_modelling\EMR_Oncology\Pfizer_mCRPC_ONCEMR_update.csv')
df_btc['created_at'] = pd.to_datetime(df_btc['Date'])
import os $ import tempfile $ TEMP_FOLDER = tempfile.gettempdir() $ print('Folder "{}" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35}, $           {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5}] $ store_items = pd.DataFrame(items2) $ store_items
faa = re.findall(r'([A-Z]{3}) .+ ([A-Z]{2})\n', data[1:])
dictionary = corpora.Dictionary(lemmatized_texts) $ terms_matrix = [dictionary.doc2bow(doc) for doc in lemmatized_texts]
y_pred_mdl = mdl.predict(X_test) $ y_train_pred_mdl=mdl.predict(X_train) $ print("Accuracy of logistic regression classifier on on test set: {:0.5f}".format(mdl.score(X_test, y_test)))
female = crime.loc[crime['Sex']=='F'] $ female.head(3)
for x in cat_attribs: $     print('-------------') $     print(ac_tr[x].value_counts())
df3['us_new'] = df3['country_US']*df3['ab_page'] $ df3['uk_new'] = df3['country_UK']*df3['ab_page'] $ df3.head(3)
df_a = df.loc[(df['landing_page'] == 'new_page') & (df['group'] == 'treatment')] $ df_b = df.loc[(df['landing_page'] == 'old_page') & (df['group'] == 'control')] $ frames = [df_a, df_b] $ df2 = pd.concat(frames)
bow_vectorizer, bow_features = bow_extractor(sdf.Norm_reviewText) $ features_bow = bow_features.todense() $ feature_names = bow_vectorizer.get_feature_names() $ print (feature_names)
html = requests.get('https://www.google.com/search?source=hp&ei=OXPhWviWDKqJmwWThrmgBA&q=%D0%BF%D0%BE%D0%B3%D0%BE%D0%B4%D0%B0+%D0%B2+%D0%BC%D0%BE%D1%81%D0%BA%D0%B2%D0%B5&oq=%D0%BF%D0%BE%D0%B3%D0%BE%D0%B4%D0%B0+%D0%B2+%D0%BC%D0%BE%D1%81%D0%BA%D0%B2%D0%B5&gs_l=psy-ab.3..0i203k1l10.1990.5682.0.5795.19.14.1.0.0.0.550.1687.0j1j3j1j0j1.6.0....0...1c.1.64.psy-ab..13.6.1167.0..0j35i39k1.0.hbVCA6n3Um0').text
df_archive_clean.rating_numerator = df_archive_clean.rating_numerator.astype(int)
optimizer_col= optim.Adam(model.parameters(),lr=0.001)
[h for h, v in opener.addheaders]
drug_counts = park.named_fda_drug.apply(lambda x: pd.Series(x).value_counts()).sum()
data.head()
print('Proportion of converted users:',format(100*df.converted.mean(),'.3f'),'%')
corpora.MmCorpus.serialize('experiment/textCorpus.mm', corpus)
station_distance.info()
to_be_predicted_Day5 = 17.79353423 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
team_groups.first()
n_old = len(df2.query("group == 'control'")) $ print('The n_old is: {}.'.format(n_old))
weather_yvr_dt.head()
extract[['rating_numerator', 'rating_denominator']] = pd.DataFrame(extract.rating.values.tolist(), index = extract.index)
posts_answers_df.head()
agg = c.aggregate([{'$group': {'_id': '$born', $                                'people_count': {'$sum': 1}}}])
portfolio_df.reset_index(inplace=True) $ adj_close_acq_date = pd.merge(adj_close, portfolio_df, on='Ticker') $ adj_close_acq_date.head()
pd.DataFrame ([{"name":"Yong", "id":1},{"name":"Gan","id":2}], columns=("MyID","MyName"))
import statsmodels.api as sm; $ convert_old = df2[df2['group']=='control'].converted.sum() $ convert_new = df2[df2['group']=='treatment'].converted.sum() $ n_old = df2[df2['group']=='control'].shape[0] $ n_new = df2[df2['group']=='treatment'].shape[0] $
pickle.dump(lsa_cv_df, open('iteration1_files/epoch3/lsa_cv_df.pkl', 'wb'))
cityID = '3df0e3eb1e91170b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Columbus.append(tweet) 
ftrs = vectorizer.get_feature_names()
doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list] $ corpora.MmCorpus.serialize('corpus.mm', doc_term_matrix) $ print 'length of the matrix: {}'.format(len(doc_term_matrix)) $ print 'verctorized ticket #20: {}'.format(doc_term_matrix[20])
dsg.to_dataframe().head()
food.drop("created_datetime", axis=1, inplace=True)
plt.scatter(high['longitude'],high['latitude'])
base_dict = collections.defaultdict(int, {k: d1[k] + d2[k] + d3[k] for k in set(d1) | set(d2)| set(d3)})
rdd = sc.parallelize([random() for _ in range(10)]) $ rdd.collect()
archive_copy['timestamp'].dtype
props.info()
groceries.iloc[[2, 3]]
df_pol[df_pol['pol_id']=='Conservative']['domain'].value_counts().head(20).plot(kind='bar');
results = pp.get_results()
total_number_of_usage_per_feature = df_usage['feature_name'].value_counts() $ total_number_of_usage_per_feature.shape
cr = df2.converted.sum()/df2.converted.count() $ cr
import os $ os.environ['PATH'] += ':/usr/local/cuda/bin'
plt.hist(p_diffs); $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('simulated 10,000 values in p_diffs'); $ plt.axvline(x=act_diff,color ='red')
from scipy.stats import norm $ norm.cdf(z_score),norm.ppf(1-(0.05/2)) $
old_page_converted = np.random.choice([0,1], size=n_old, p=[1-p_old, p_old]) $ old_page_converted.mean()
autos.hist(column = 'price', bins = 100)
autos.columns
data=users.merge(transactions,how='left',on='UserID').groupby(['UserID']).min() $ data
dfd.describe()
p_old = df2['converted'].mean() $ print('Convert rate for p_old under the null :: ',p_old)
PUNCTUATION = [':', '.', ')', '('] $ PUNCTUATION_AND_URL = PUNCTUATION + ['URL'] $ LEMMATIZER_POS = ['n', 'v', 'a', 'r'] $ TWITTER_SYMBOLS = ['#', '@'] $ URL_PATTERN = re.compile("(https?:\/\/)?([\da-z\.-]+)\.([a-z\.]{2,6})([\/\w \.-]*)*\/?")
kochdf = pd.merge(kochdf, koch02df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_02'))
labels_np = labels_np[:70000,:] $ print(labels_np.shape) $ np.save('Participant_7_label.npy', labels_np)
with codecs.open(review_txt_filepath, encoding='utf_8') as f: $     sample_review = list(it.islice(f, 8, 9))[0] $     sample_review = sample_review.replace('\\n', '\n') $ print sample_review
filtered_df = classification_df $ plt.figure() $ plt.title('Reward in Dollar') $ plt.scatter(filtered_df['rewardInDollar'], filtered_df['best'], marker= 'o', s=20) $ plt.show()
plt.scatter(mario_game.NA_Sales, mario_game.JP_Sales) $ print('The pearson correlation between the NA sales and JP sales is {}' \ $       .format(pearsonr(mario_game.NA_Sales, mario_game.JP_Sales)[0]))
pd.read_csv("Data/microbiome_missing.csv", na_values=['?', -99999]).head(20)
delays_by_origin = grouped_by_origin['DepDelay'].agg(['mean', 'count']) $ delays_by_origin.sort_values('mean', ascending=False).head(10)
f_counts_week_app.show(1)
df['statement_type'].value_counts()
classifier.get_variable_value('dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights').shape
df['source_new'].value_counts()
recommendation_df = recommendation.to_dataframe()
tweets.head()
X=data2[['open','high','low','close','volume']] $ y=data2['target'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
df1=files4.pivot_table(index='jobcandidate', columns='assessmentname', values='assessmentscore') $ df1=df1.fillna(0) $ df1new = pd.DataFrame(df1.to_records()) $ df3=pd.merge(df2,df1new,on='jobcandidate',how='left') $ df3.shape
test = word_counts.to_frame()
beirut.head()
total_ctrl = (df2['group']=='control').sum() $ prob_ctrl = total_ctrl/unique_users $ df2[(df2['group']=='control') & (df2['converted']== 1)].sum() $
train.date.value_counts().shape
education_data.reset_index(drop=True, inplace=True)
df.head() $
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[n_new, n_old], alternative = 'larger') $ print("Z-score: ",z_score) $ print("P_value: ",p_value)
old_page_converted = np.random.binomial(1, p_old, n_old) $ old_page_converted
USvideos['trending_date'] = pd.to_datetime(USvideos['trending_date'],format='%y.%d.%m') $ USvideos['time_to_trend'] = (USvideos.trending_date - USvideos.publish_time).astype('timedelta64[ms]')/3600000 $ USvideos.info()
print("p_new under the null is: %.4f\np_old under the null is: %.4f" %(p_new, p_old))
from scipy.stats import norm $ z_critical_value = norm.ppf(1-0.05) $ print('critcal value of z-score at 5% alpha is: {}'.format(z_critical_value))
day_ahead_price_df.columns = ['Date','DA-price', 'PTE'] $ DA_power_df = day_ahead_price_df.merge(solar_wind_df, on = ['Date', 'PTE'], how = 'inner') $ DA_power_df.head()
sort_df = df.sort_values(by='date',ascending=True) $ sort_df.head(10)
from sklearn.cluster import KMeans $ num_clusters = 50 $ km = KMeans(n_clusters=num_clusters) $ km.fit(model.docvecs) $ clusters = km.labels_.tolist()
noise_graf['AFFGEOID'] = noise_graf['AFFGEOID'].astype(str)
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page'))== False].describe()
pct_accepted = {} $ for i in trainwdummies.columns[-8:-1]: $     accepted = trainwdummies[(trainwdummies.project_is_approved == 1) & (trainwdummies[i] == 1)].shape[0] $     total_applied = trainwdummies[trainwdummies[i] == 1].shape[0] $     pct_accepted[i] = np.round(accepted/total_applied*100, 2)
pd.read_html('https://en.wikipedia.org/wiki/Python_(programming_language)', header=0)[1]
filtered = vwap.between_time('10:00', '16:00') $ filtered.head(20)
query_potholes = pgh_311_data_merged['REQUEST_TYPE'] == "Potholes" $ query_bloomfield = pgh_311_data_merged['NEIGHBORHOOD'] == "Bloomfield" $ bloomfield_pothole_data = pgh_311_data_merged[query_potholes & query_bloomfield] $ print(bloomfield_pothole_data.shape) $ bloomfield_pothole_data.head()
df_usnpl['Website'] = df_usnpl['Website'].apply(remove_www) $ df_usnpl_one_hot = pd.get_dummies(df_usnpl[['Website', 'Medium']], columns=['Medium'])
glm_multi_v3.hit_ratio_table(valid=True)
obs_old = df2.query('group == "control" and converted == 1').count()/df2[df2['group'] == "control"].count() $ obs_old[0]
ticker = 'AAPL' $ source = 'google' $ start = '2015-01-01' $ end = '2017-12-31' $ aapl_goo = web.DataReader(ticker, source, start, end)
df_daily = df_daily.reset_index() $ df_daily.head()
null_mean=0 $ null_vals=np.random.normal(null_mean,np.std(p_diffs),10000) $ plt.hist(null_vals);
heights = [59.0, 65.2, 62.9, 65.4, 63.7, 65.7, 64.1] $ data = {'height':heights,'sex': 'M'} $ results = pd.DataFrame(data) $ print(results)
df[(df['outcome']=='Exposed to Potential Harm') | (df['outcome']=='No Negative Outcome')].count()[0]
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
print('start one-hot encoding') $ kick_projects_ip = pd.get_dummies(kick_projects, prefix = [ 'category', 'main_category', 'currency','country'], $                              columns = [ 'category', 'main_category', 'currency','country']) $ print('ADS dummy columns made')
commit_nova = pandas_ds[(pandas_ds["current_status"] == 'MERGED') & (pandas_ds["gerrit_tracker"].str.contains("/nova"))]; $ commit_nova
pd.pivot_table(df,index= ['class'],columns= ['year'],values= ['hwy', 'cty'],aggfunc= [np.mean, np.max],margins= True)
df2.drop_duplicates(subset="user_id", inplace = True)
df['screen_name'].value_counts()[:6]
n_new,n_old = df2.landing_page.value_counts() $ print(n_new,n_old)
result = %sql select query_id, state from runtime.queries limit 1
grid_clf = GridSearchCV(rdf_clf, param_grid, cv=10) $ grid_clf.fit(X_final[columns], y_final)
df[ ((df['group']!='treatment') & (df['landing_page']=='new_page')) | ((df['group']=='treatment') & (df['landing_page']=='old_page')) ].shape[0]
lm = sm.Logit(df3['converted'], df3[['intercept', 'US', 'UK']]) $ results = lm.fit() $ results.summary()
data.info()
from scipy.stats import chisquare
hpd["2015-01":"2015-02"]['Complaint Type'].value_counts().head(5)
(df.query('landing_page=="new_page"').query('group!="treatment"')['user_id'].count())+(df.query('landing_page!="new_page"').query('group=="treatment"')['user_id'].count()) $
company_vacancies.count()
save_model('model_svm_v1.mod', svc_grid)
stops["color"].value_counts()
date_news = valid_news.copy() $ date_news.dates = pd.to_datetime(date_news.dates) $ date_news.head()
sentences = [nltk.word_tokenize(s) for s in sentences_text] $ print(sentences[10])
print(ind.size, ind.shape, ind.ndim, ind.dtype)
to_be_predicted_Day4 = 14.80705454 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
team_groups.aggregate(np.sum).sort_values(by = "Tm.TRB", ascending = False)
fb.info()
stmt = session.query(Measurement).\ $     order_by(Measurement.station.desc()).statement $ df2 = pd.read_sql_query(stmt, session.bind) $ df2.head(10)
ux.html_utils.get_webpage_title(urls[0])
retention_10_per = retention_10.iloc[:,1:].div(retention_10['Week 0'], axis=0)
df.replace({'city': {'': np.nan}, 'state': {'': np.nan}}, inplace=True) $ df.set_index(['zipcode'], inplace=True) $ zipcodes.set_index(['zipcode'], inplace=True) $ df.update(zipcodes, join='left', overwrite=False, raise_conflict=False)
autos.odometer_km.describe()
Maindf['Month Change'] = Maindf['Month'] - Maindf['Month'].shift(1) #shift 1 is to shift down 
cc.drop(columns=['logspread','logclose']) $ cc.head()
breed_predict_df_clean.drop(['jpg_url', 'img_num'], axis=1, inplace=True)
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df['group']) $ df2.drop(['control'], axis=1, inplace=True) $ df2.head()
len(df.query('group != "treatment"') $     .query('landing_page == "new_page"').index)+len( $     df.query('group == "treatment"').query('landing_page != "new_page"').index)
sample.asfreq('H')
%timeit df.pivot(index='date', columns='item', values='status')
df_agg_notop_rand.head()
data = originaldata.copy() $ data = data.reset_index() $ del data["index"]
full_dataset.to_pickle('rolling_dataset.pkl')
from nltk.tokenize.casual import TweetTokenizer $ from nltk.corpus import stopwords $ from nltk.stem import PorterStemmer $ import string
df2.user_id.value_counts()
df = pd.read_csv("CSV/Iowa_Liquor_sales_sample_10pct.csv") $ print df.columns $ df.head()
df2.head()
monthly_portfolio_var = round(np.dot(stock_weights.T,np.dot(monthly_cov_matrix, stock_weights)),2) $ monthly_portfolio_var
submissions.head()
lat = prec_nc.variables['lat'][:] $ lon = prec_nc.variables['lon'][:] $ time = prec_nc.variables['time'][:] $ prec = prec_nc.variables['pr_wtr'][1,:,:] $ np.shape(prec)
to_be_predicted_Day2 = 48.54345518 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
controldf=df2[df2.group=="control"] $ controldf[controldf.converted==1].count()[0]/controldf.count()[0]
df.irlco.sum()
dftouse.head()
df = pd.merge(df, tran_time_diff, on='msno', how='left').drop_duplicates(['msno','order_number']) $ df = df.dropna()
pool = SimpleProcessPoolExecutor(4) $ for seq in range(1, 1000): $     pool.submit(interpolate_acs_file, process_year, seq) $ pool.shutdown() $ None
n_new = df2[df2['group'] == 'treatment']['user_id'].count() $ print('Elements of treatment group n_new: ',n_new)
column_name = data.columns[3] $ column_name.split(SEPARATOR)
data.ix[:3, :'pop']
URL = 'https://api.blockchain.info/charts/transactions-per-second?timespan=5weeks&rollingAverage=8hours&format=json'
temp_series_freq_15min = temp_series.resample("15Min").interpolate(method="cubic") $ temp_series_freq_15min.head(n=10)
pd.datetime(2014,8,1)
lm_us = sm.OLS(df_new['converted'], df_new[['intercept_us', 'ab-page']]) $ results_us = lm_us.fit() $ results_us.summary()
TrainData_ForLogistic.to_csv('training_logistic.csv')
soup(lambda tag: tag.has_attr('alt'))
pd.DataFrame(dict(VMM=VMMmeans))
plt.hist(p_diffs);
accuracy_score(clf.predict(x_train),y_train)
full_data.to_csv('examples/simple/data/varying_data.csv', index=False)
df = df.drop_duplicates()
import statsmodels.api as sm $ logit_mod = sm.Logit(df2_dummy.converted, df2_dummy[['intercept','ab_page_new_page']]) $
ys_df = valid_scores[['point_diff']] $ ys_df.head()
print len(hpdpro) $ data = pd.merge(data, hpdpro, how='outer', on='ComplaintID') $ print len(data)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print("z-score: {}".format(z_score)) $ print("p-value: {}".format(p_value))
pd_train_filtered.sample(10)
first_commit_timestamp = pd.to_datetime('2005-04-16 22:20:36') $ last_commit_timestamp = pd.to_datetime('today') $ corrected_log = git_log[(git_log['timestamp']>first_commit_timestamp) & (git_log['timestamp']<last_commit_timestamp)] $ print(corrected_log.describe())
get_items_purchased('Shirley_a_louis@yahoo.com', product_train, customers_arr, products_arr, item_lookup)
y_pred=knn3.predict(X_test)
SCR_PLANS_df = USER_PLANS_df.drop(free_mo_churns)
AFX_url_str = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?' $ AFX_date_str = 'start_date=2017-01-01&end_date=2017-12-31' $ AFX_APIkey_str = '&api_key='+API_KEY $ AFX_X_2017_r = requests.get(AFX_url_str+AFX_date_str+AFX_APIkey_str) $
bow_df.columns = ['col'+ str(x) for x in bow_df.columns] $ tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]
session.query(Actor.actor_id, Actor.first_name, Actor.last_name).filter_by(first_name='Joe').frame()
index = pd.to_datetime(non_na_df['created_at']) $ non_na_df.index = index
missing_info = list(Users_first_tran.columns[Users_first_tran.isnull().any()]) $ missing_info
print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_) $ clf = gs_lr_tfidf.best_estimator_ $ print('Test Accuracy: %.3f' % clf.score(X_test, y_test))
pd.DataFrame(features['MEAN(payments.payment_amount)'].head())
for zc in weather.zip_code.unique(): $     print weather[weather.zip_code == zc].isnull().sum() $     print
df2.head()
df1.shape, df2.shape
test_df.head()
lr.head()
np.zeros((3,4),dtype=int)
df_loc = df_location[['country','name','state']] $ df_loc = df_loc.rename(columns={'country':'loc_country','name':'loc_name','state':'loc_state'}) $
list_of_genre_1990s_clean = list() $ for x in list_of_genre_1990s_1: $     if(x != ''): $         list_of_genre_1990s_clean.append(x)
df.boxplot(figsize=(20, 10)) $ plt.show()
del vol['Volume'] $ vol.head()
np.mean(y_test > 0)
df.iloc[1,]
df.text[279]
print(len(set(intervention_train.INSTANCE_ID))) $ print(len(set(intervention_test.INSTANCE_ID))) $ print(len(set(intervention_history.INSTANCE_ID)))
tobs_data = [] $ for row in temperature_data: $     tobs_data.append(row[0]) $ tobs_data $
prcp_analysis_df = df.rename(columns={0: "Date", 1: "precipitation"}) $ prcp_analysis_df.head()
dt.day
with open('../ChildBookProject/100Books.txt') as f: $     childbooks = f.read() $ childbooks
df3[df3['STATION'] == '103 ST'].groupby(['DATE']).sum().plot(figsize=(10,3))
key_name = 'aws-master.pem' $ pem_path = '/Users/Ev/.ssh' + '/' + key_name $ dns = 'ec2-52-37-101-84.us-west-2.compute.amazonaws.com' $ user = 'ubuntu' $ print 'ssh -X -i {} {}@{}'.format(pem_path, user, dns)
fps.pyldavis_fp
rshelp.query("SELECT COUNT(*) FROM (SELECT * FROM postgres_public.ratings_amenities ORDER BY RANDOM() LIMIT 100) WHERE ev_charging;")
dfM['COUNT_s1'] = dfM['COUNT'].shift(1) $ dfM['diff_1'] = df.COUNT - dfM.COUNT_s1 $ dfM.head(5)
nps = nps.to_crs({'init': 'epsg:3857'}) $ nps.crs
pres_df.ix[362865]['subjects'] # how to get row info by index number and column
all_columns = pd.Series(list(df_twitter_archive) + list(df_img_predictions) + list(df_tweet_data)) $ all_columns[all_columns.duplicated()]
modal_model.compile(optimizer=optimizers.Nadam(lr=0.0001), loss='cosine_proximity', metrics=['accuracy']) $ batch_size = 1200 $ epochs = 20 $ history = modal_model.fit([train_source_emb], train_target_emb, $                           batch_size=batch_size, epochs=epochs, validation_split=0.1)
data.tail()
seller = autos.groupby("seller").mean() $ seller['price_$']
np.exp(-2.0397)
ride_percity=original_merged_table["type"].value_counts() $
twitter_archive_df_clean.name.value_counts()[:20]
n_new = new.count() $ n_new
logit_model = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit_model.fit()
from pyspark.sql import Row $ RowedRDD = RandomOneRDD.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2])) $ print RowedRDD.collect()                                               
naimp.get_overlapping_matrix()
sns.regplot(x=data['avg_c_gap'], y=data['ltv'])
df_b = df[(df.landing_page =="old_page") & (df.group =="treatment")]; $ len(df_b)
print('POW(X,2) = \n', np.power(fruits, 2))
df = pd.read_csv('https://query.data.world/s/kw3bkr2haxalgzcit3snux4yrtqmtf')
df.groupby('Borough').count().sort(desc("count")).show(10)
log_mod = pickle.load(open('../data/model_data/log_pred_mod.sav', 'rb'))
logreg.predict_proba(X_test)[:10]
fb.sort_index(inplace=True)
from pandas.tseries.offsets import *
births.index = pd.to_datetime(10000 * births.year + $                              100 * births.month + $                              births.day, format='%Y%m%d') $ births['dayofweek'] = births.index.dayofweek
df = df[df["time"].dt.day != 23] $ temp_inst = df.iloc[:,1:].mean(axis=1)
liberia_data.columns
x-m  #the trailing dimension matches, and m is stretched to match the 2 rows of x.
predCBoE=model_CBoE.predict()
pd.merge(transactions, transactions, on='UserID')
y_knn_predicted = knn_model.predict(X_test)
npd.node_names()
df_groups.city.unique()
table = dynamodb_resource.Table('demographics') $ table.delete_item(Key={'pmcid': '2842549'})
summary.columns
first_minute = walk['2014-08-01 00:00'] $ pd.rolling_mean(first_minute,5).plot() $ first_minute.plot() $ plt.legend(labels=['Rolling Mean','Raw']);
dates=pd.date_range(start='1/Oct/2020', periods=5, freq='M') $ print(dates)
average_polarity.fillna('No Data',inplace=True) $ count_polarity.fillna('No Data',inplace=True)
tickerdata = tickerdata.reindex(all_weekdays)
statezips = csvData['statezip'].value_counts().reset_index() $ statezips.columns = ['statezip', 'count'] $ statezips[statezips['count'] < 5]
closeSeriesP.nlargest(5)
census_tracts_df.head()
idx = pd.period_range('2014-07-01 09:00', periods=5, freq='H')
y_pred = model.predict(X_test) $ utils.metrics(y_test, y_pred)
predictions_new = predictions - mean_rmse $ RMSE_new = sqrt(mean_squared_error(Y_test2_inverse, predictions_new)) $ print('Test GRU model RMSE_new: %.3f' % RMSE_new)
fashion['date_time'] = pd.to_datetime(fashion.created) $ fashion.drop(labels='created', axis=1, inplace=True) $ fashion.info()
n_old = (df2[df2['landing_page']=='old_page']).count()[0] $ n_old
df.tail(1)
guineaDf.index.is_unique
df[["first_rental", "new_customer", "repeat_customer"]] = pd.get_dummies(df["reservation_frequency"])
print(pandas_list_2d.head(2))
df.iloc[3:6]
print(new_seen_and_click.shape) $ new_seen_and_click.head()
df.shape
cols=pd.Series(tmp_df.columns, index=tmp_df.columns) $ tmp_df = tmp_df.drop(cols[cols.str.match(r'^(?:Second.)?(?:Defendant|Plaintiff)')], axis=1) $ tmp_df = tmp_df.drop(['Case.Type', 'Case.Subtype', 'Nature.of.Claim','Judgment.Against','Judgment.In.Favor.Of', $                       'Next.Hearing.Date', 'Next.Hearing.Desc', 'Next.Hearing.Time', 'Style.Of.Case' $                      ], axis=1)
month_year_crimes.hist()
ids = df2["user_id"] $ df2[ids.isin(ids[ids.duplicated()])]
store_items = store_items.append(new_store, sort=True) $ store_items
sns.regplot(x="totqlesq", y="y", data=psy_native).set_title("Quality of Life Enjoyment and Satisfaction") $
consumer_key = apikeys.TWITTER_CONS_KEY $ consumer_secret = apikeys.TWITTER_CONS_SECRET $ access_token = apikeys.TWITTER_ACC_TOKE $ access_token_secret = apikeys.TWITTER_ACC_TOKE_SECRET
acs_df = pd.read_csv('./Datasets/ACS_Cleaned.csv').drop('Unnamed: 0', axis=1) $ acs_df.head()
df_u=df.user_id.nunique() $ print("Number of unique users in the dataset is :{}".format(df_u))
twitter_df_clean['timestamp'].sample(5)
nan_values = data.isnull().sum() $ print('Porcentaje de valores faltantes por columna:') $ (nan_values[nan_values > 0] / len(data)) *100
out['display_address'] = out['display_address'].map(lambda x: x.replace('\r','')) $ out['street_address'] = out['street_address'].map(lambda x: x.replace('\r',''))
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','country_UK','UK_page','country_US','US_page']]) $ results = logit_mod.fit() $ results.summary()
implied_therapy = ['Therapy', 'New Patient Therapy'] $ implied_doctor = ['Therapy Telepsychiatry','Follow up Telepsychiatry', 'New Patient Therapy Telepsychiatry',\ $                   'New Patient MD Adult', 'New Patient MD Adult Telepsychiatry'] $ merged1['Specialty'].loc[merged1['ReasonForVisitName'].isin(implied_therapy)] = 'therapist' $ merged1['Specialty'].loc[merged1['ReasonForVisitName'].isin(implied_doctor)] = 'doctor'
np.shape(glons)
data = pd.read_csv('barbsList99.csv')
pd.Series(zip(labels, y)).value_counts()
df_c_merge.groupby('country')['converted'].mean()
tmp1 = (x > 0.5) $ tmp2 = (y < 0.5) $ mask = tmp1 & tmp2
groupby_breed = df_master.groupby('dog_breed').sum().reset_index() $ groupby_breed.sort_values('rating_numerator',inplace=True,ascending=False) $
df_yt_resolved = pd.DataFrame(meta)
fig, ax = plt.subplots(1,1, figsize=(16,8)) $ colors =[None, 'black', 'red', 'blue'] $ for key, vals in scoresdf.items(): $     plt.plot(vals['min_df'], vals['score'], color=colors[key], label=key) $ plt.legend(fontsize=18)
print("The no of Unique user_id in dataframe are",df2['user_id'].nunique())
df.columns.values
df2.query("landing_page == 'new_page'").count()[0] / len(df2)
df_index_demo = df_protest.iloc[0:3, 0:3]
d3 = pd.DataFrame({'place': ['Boston', 'New York', 'Seattle'], 'area': [48.42, 468.48, 142.5]}) $ pd.merge(d1, d3, left_on='city', right_on='place')
df_new = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') #joining countries_df with df2 $ df_new.head()#displays first 5 rows of df_new
X = X.rename(columns = {'C(occupation)[T.2.0]':'occ_2','C(occupation)[T.3.0]':'occ_3','C(occupation)[T.4.0]':'occ_4','C(occupation)[T.5.0]':'occ_5','C(occupation)[T.6.0]':'occ_6','C(occupation_husb)[T.2.0]':'occ_husb_2','C(occupation_husb)[T.3.0]':'occ_husb_3','C(occupation_husb)[T.4.0]':'occ_husb_4','C(occupation_husb)[T.5.0]':'occ_husb_5','C(occupation_husb)[T.6.0]':'occ_husb_6'})
countries = wb.get_countries() $ countries.iloc[0:10].ix[:,['name','capitalcity','iso2c']]
DA_power_df.to_excel(data_folder_path + '/temp/day_ahead_merge_power.xlsx', index = False)
engine = create_engine('sqlite:///tweets.db')
df.to_csv("sentiment_dataframe.csv")
tree.fit(X,y)
df_symbols = nasdaq.get_ipo_list(start_date) $ print('symbols', df_symbols.shape[0])
os.chdir(os.path.expanduser('~/PycharmProjects/chat-room-recommendation/')) $ lines = open('cornell-movie-dialogs-corpus/movie_lines.txt','r').read().split('\n') $ movie_metadata = open('cornell-movie-dialogs-corpus/movie_titles_metadata.txt','r').read().split('\n')
datetime.strptime('20091031', '%Y%m%d')
results = log_mod.fit() $ results.summary()
tweet = result[0] #in JSON format with lots of unnecessary info $ for item in dir(tweet): $     if not item.startswith("_"): $         print("%s : %s\n" % (item, eval('tweet.'+item))) $
df_sentiment = df.withColumn( $     "user_age", user_age_udf("created_at", "user_since") $ ).withColumn("sentiment", get_sentiment_udf(array(SENTIMENT_KEYS)))
excelDF.groupby('Ship Mode').Quantity.max()
np.exp(results.params)
z_score , p_value = sm.stats.proportions_ztest(np.array([convert_old,convert_new]), $                                                np.array([n_old,n_new]),value=0.0 , alternative = 'larger') $ print('z-score = {} \n p_value = {}'.format(z_score , p_value))
langdata = df['lang'].value_counts()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'UK', 'US']]) $ results = logit_mod.fit() $ results.summary() $
feature_set = {} $ for features in df_train['features']: $     for feature in features: $         feature_set[feature.lower()] = feature_set.get(feature.lower(), 0) + 1 $
group.mean()
experience.to_csv('../data/experience.csv')
bp[bp["icustay_id"]==14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"]) $ bp[bp["icustay_id"]!=14882].plot(x="new charttime", $                                  y=["systolic", "diastolic"])
sub1['time'] = sub1.groupby(['hacker_id', 'challenge_id'])['created_at'].transform('max')
f = open(path_play + "/all_demo_train","w",encoding="utf-8") $ for line in all_doc: $     f.write(line) $ f.close()
BallBerry_resistance_simulation_1 = BallBerry_ET_Combine['BallBerry(Root Exp = 1.0)'] $ BallBerry_resistance_simulation_0_5 = BallBerry_ET_Combine['BallBerry(Root Exp = 0.5)'] $ BallBerry_resistance_simulation_0_25 = BallBerry_ET_Combine['BallBerry(Root Exp = 0.25)']
elms_all_0611.iloc[1048575:].to_excel(cwd+'\\ELMS-DE backup\\elms_all_0611_part2.xlsx', index=False)
print df.set_index(['date', 'item']).unstack() $
total_users_marketing = clean_users['enabled_for_marketing_drip'].sum() $ SD_marketing = np.sqrt(2*total_users_marketing/total_users * (1-total_users_marketing/total_users) / total_users)
df_enhanced = df_enhanced.drop('rating_denominator', axis=1)
random_numbers.groupby(by=[random_numbers.index.month]).mean()#here only mean values for each calender month with only month no.
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'facilitator'
rt_max = data['RTs'].max() $ rt_tweet = data[data['RTs']==rt_max].index $ print("The tweet with more retweets is: \n{}".format(data['Tweets'][rt_tweet])) $ print("Number of retweets: {}".format(rt_max)) $ print("{} characters.\n".format(data['len'][rt_tweet]))
random_integers.max()
df_new['us_page'] = df_new['us'] * df_new['ab_page'] $ df_new['uk_page'] = df_new['uk'] * df_new['ab_page'] $ df_new.head()
page.is_filepage()
df2['ab_CA'] = df2['ab_page']*df2['CA'] $ df2['ab_UK'] = df2['ab_page']*df2['UK'] $ df2['ab_US'] = df2['ab_page']*df2['US'] $ df2.head()
matching = pd.DataFrame({'idx': pd.Series(address_indexes_arr[matched_df.reset_index()['Key'].values])}) $ ad_ix_df = pd.DataFrame({'idx': address_indexes_arr[matched_df.reset_index()['Key']]}, index=matched_df.index) $ join_ds = ad_ix_df.apply(lambda x: pd.Series(x['idx']), axis=1).stack().reset_index(level=1, drop=True).astype('int64') $ join_ds.head(20)
df_train_tokenized = [] $ for tweet in df_train: $     tweet_tokenized = regexp_tok.tokenize(tweet) $     df_train_tokenized.append(tweet_tokenized)
print(f'We eliminated {b_cal.shape[0]-b_cal_q1.shape[0]} listings')
manager.syncs_df
s.name = 'Justice League ages' $ s.index = ['bruce', 'selina', 'kara', 'clark'] $ s
df.drop_duplicates(subset = ['last_name'], keep = 'last', inplace = False)
df.state.unique()
df2.loc[df2['user_id'] == 773192] #Verify that drop happened
propiedades.loc[:, 'state_name'].value_counts()
job_status = sqlClient.wait_for_job(jobId) $ print("Job " + jobId + " terminated with status: " + job_status) $ if job_status == 'failed': $     details = sqlClient.get_job(jobId) $     print("Error: {}\nError Message: {}".format(details['error'], details['error_message']))
for key, grp in part1_flt.groupby('campaign_id'): $     print "{0: <20}{1}".format(key, percent_charged(grp.charged.values))
ip.head(20)
reddit = praw.Reddit(client_id='OPV1wsaqbio8yA', $                      client_secret='xM_zLZjDaqwex_rH4vcfpoEvCqc', $                      password='capstone', $                      user_agent='testscript by /u/capstone33', $                      username='capstone33')
val.drop('overdue', axis=1, inplace=True)
!cat /datalab/run.sh
df_archive_clean["doggo"].value_counts()
def find_gh_email(df): $     user_gh_dict={} $     for index, row in df.iterrows(): $         user_gh_dict[row['id']]=str(row['email']) $     return user_gh_dict
df = pd.DataFrame() $ df = df.append(pd.read_csv('rorypul_tweets.csv')) $ df
spark.sql(query).collect()
df_ad_airings_5.dropna(subset= ['location'], inplace=True)
df = pd.DataFrame(list('abcd'), index = [pd.Period('2017-01'), pd.Period('2017-02'), $                                          pd.Period('2017-03'), pd.Period('2017-05')]) $ df
bst.get_fscore()
rand_word = random.randint(0, len(d)) $ rand_word, d[rand_word]
to_be_predicted_Day5 = 21.29192796 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
len(df.user_id.unique()) $
!!from time import sleep $ for i in range(0, 100): $     print(i) $     sleep(0.1) $ assert False
result2.summary2()
df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], $              index=['Mon', 'Tues', 'Wed'], $              columns=['A', 'B', 'C'])
Lab7_Equifax.head() $
df.withColumn("sentiment", get_sentiment_udf(array(SENTIMENT_KEYS))).head()
poverty=pd.read_csv('data/crime/final_poverty.csv')
df2[pd.get_dummies(df2['dow']).columns]=pd.get_dummies(df2['dow'])
train.head()
means = df_lg_hubs.mean() $ default_use = [means[str(i)] for i in range(1, 13)] $ default_use
speakers.sort(columns = 'id',inplace = True)
data2['date'] = data2['dates'].dt.date $ data2['day'] = data2['dates'].dt.day $ data2['month'] = data2['dates'].dt.month $ data2['year'] = data2['dates'].dt.year
pd.DataFrame ([[101,'Alice',40000,2017], $                [102,'Bob',  24000, 2017], $                [103,'Charles',31000,2017]] )
df = pytrends.get_historical_interest(kw_list, year_start=2018, month_start=4, day_start=22, hour_start=0, $                                       year_end=2018, month_end=5, day_end=31, hour_end=0, $                                       cat=0, geo='', gprop='', sleep=0) $ df.to_csv('./data/google.csv') $
likes_max = np.max(data['Likes']) $ print(likes_max) $ retweet_max  = np.max(data['RTs']) $ print(retweet_max)
exploration_airbnb.structure()
train.head()
'Mean ROC_AUC score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_autocorrelation(therapist_duration.diff()[1:], params=params, lags=30, alpha=0.05, \ $     title='Weekly Therapist Hours First Difference Autocorrelation')
df['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['UK', 'US']]) $ result = log_mod.fit() $ result.summary()
old_page_converted = np.random.choice([0,1],n_old, p=(1-p_old, p_old))
df_t1["time_hrs"] = df_delta.apply(lambda td: td.total_seconds() / 3600)
conn_helloDB = create_engine("mysql+pymysql://{user}:{pw}@localhost".format(user="few", pw="123456")) $
grades = df['Grades'].astype('category', $                              categories=['D', 'D+', 'C-', 'C', 'C+', 'B-', 'B', 'B+', 'A-', 'A', 'A+'], $                              ordered=True) $ grades.head()
df2.query('landing_page == "new_page"').user_id.nunique() / df2.shape[0]
twitter_archive_df_clean.drop(['doggo', 'puppo', 'pupper', 'floofer'], axis=1, inplace=True)
from sklearn.metrics import accuracy_score $ accuracy_score(y_test, y_predict)
response_json = response.json() $ for part in response_json.keys(): $     print part
plt.close()
glp1 = df.query('group == "control" and landing_page == "new_page"').count() $ glp2 = df.query('group == "treatment" and landing_page == "old_page"').count() $ print(glp1 + glp2)
sns.boxplot(x=tmdb_movies_production_countries_revenue['production_countries'], y=tmdb_movies_production_countries_revenue['revenue'], showmeans=True) $ plt.show()
df_dates_final.to_csv('medium_urls_dates_unique.csv',sep=',',index_label='url',header=['date'])
full_data['order_date'] = pd.to_datetime(full_data.order_date,format= '%Y-%m-%d')
centers_lim.shape
lm = sm.OLS(df_new['converted'],df_new[['a/b_page','us_intercept']]) $ result = lm.fit() $ result.summary()
dfWords.index = pd.DatetimeIndex(dfWords["date"]) $
print('---averages---') $ print(df_usa.mean()) $ print('---SD---') $ print(df_usa.std())
lm=sm.Logit(lr['converted'],lr[['intercept','ab_page']])
tagged_df[tagged_df['is_rebalance']]['date'].unique()
df.weekday()[2]
for file in s3_key_edits: $     os.environ["s3_key"] = "s3://wri-public-data/" + file $     os.environ["gs_key"] = "gs://resource-watch-public/" + file $     !gsutil cp $s3_key $gs_key
sample.head(1)
unique_domains.sort_values('num_authors', ascending=False)[['domain', 'num_authors']][0:50]
autos = autos[autos['price'].between(1, 400000)]
snow.select (" select * from RWD_DB.RWD.PELICAN_DIAGNOSIS limit 3")
df.dropna(axis=1, thresh=1000).shape == df2.shape
tm_week_df = df_table.groupby(df_table['Datetime'].dt.weekday_name).count() $ print(tm_week_df['Incident_number'])
import pyspark.sql.functions as F $ df_business.createOrReplaceTempView("business") $ df_restaurants = sqlc.sql("select * from business where array_contains(categories, 'Restaurants')")
df[df['converted']==1]['user_id'].nunique()
afreq = np.logspace( -5, 0, 30 ) $ Sconf = gwt.lisa_Sconf(3) $ plt.loglog( afreq, np.sqrt( Sconf(afreq) ), 'b-' ) $ plt.ylim([1e-21, 1e-14]) $ plt.grid(True) $
sns.distplot(master_list['Count'])
sns.set_style("darkgrid") $ plt.figure(figsize=(8, 4)) $ failures['failure'].value_counts().plot(kind='bar') $ plt.ylabel('Count')
high = out_df.copy() $ high['low']=0 $ high['medium']=0 $ high['high']=1 $ high.to_csv('all_high.csv',index=False)
twitter_data['CleanTweets'] = np.array([clean_tweet(tweet.text) for tweet in tweets]) $ twitter_data['Sentiment'] = np.array([analyze_sentiment(tweet.text) for tweet in tweets]) $ display(twitter_data[-10:])
uniqueCategoryID = list(set(df['CategoryID'].tolist()))
grid_search.fit(X_train,y_train)
mention_df['mention_user_id'].unique().size
from sklearn.metrics import recall_score $ recall_score(y_test, y_pred, average='binary')
readFromFieldsDB() $ c.close() $ connect.close()
len(tweets_df.tweet_source.unique())
model_brand_combo = autos['model'] + ' ' + autos['brand'] $ model_brand_combo.value_counts().head(20)
conn = engine.connect() $ measure_df = pd.read_sql("SELECT * FROM Measurement", conn)
stats.norm.cdf((active_referred-inactive_referred)/SD_referred)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page', 'UK', 'US']]) $ results = logit_mod.fit()
pd.Period('2005', 'AS')
df_predictions_clean['p1'] = df_predictions_clean['p1'].str.replace('_', ' ') $ df_predictions_clean['p2'] = df_predictions_clean['p2'].str.replace('_', ' ') $ df_predictions_clean['p3'] = df_predictions_clean['p3'].str.replace('_', ' ') $
df['car_type'].value_counts()
df.rating_numerator.value_counts()
to_be_predicted_Day5 = 34.13616486 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df1_clean[df1_clean.name == 'None']
perceptron = Perceptron() $ perceptron.fit(X_train, Y_train) $ Y_pred = perceptron.predict(X_test) $ acc_perceptron = round(perceptron.score(X_test, Y_test) * 100, 2) $ acc_perceptron
crimes.resample('D').size().idxmax()
dat_hcad = pd.read_table(dir_hcad+"real_acct.txt", sep='\t', encoding = "ISO-8859-1")
learn.fit(lr, 3, cycle_len=1, cycle_mult=2)
ts.shift(3)
df[df['Complaint Type'] == 'Street Condition']['Descriptor'].value_counts()
serhc.describe()
plt.hist(review_length_biname, bins=50) $ plt.title('Distribution of review\'s length from two-word username') $ plt.show()
tweet_data.describe()
xml_in_merged['authorId'].nunique()
df_concat_2.head()
df.shape $
new_page_converted = np.random.choice(2, size = n_new, p=[0.8805, 0.1195]) $
convo_frame.iloc[rsi]['a']
p_value = scipy.stats.chi2_contingency(full_contingency)[1] $ print(p_value)
timelog.to_csv("data/toggl-detailed-logs-full-export.csv")
bus[bus.isnull()["postal_code_5"]]
def generate_returns(close): $     return None $ project_tests.test_generate_returns(generate_returns)
data.describe()
Events.head(3)
classification_data.head(2)
All_tweet_data_v2.name[All_tweet_data_v2.name.str.len() < 2]='None'
test = adjmats $ print adjmats.
grid_tfidf.best_params_
df4["intercept"] = 1 $ logit_mod = sm.Logit(df4["converted"], df4[["intercept","UK","CA","old_page"]]) $ results = logit_mod.fit() $ results.summary()
tobs_results = session.query(Measurement.station, Measurement.tobs).filter(Measurement.date.between('2017-08-01', '2018-07-31')).all() $ tobs_results
vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', lowercase=False, $                              tokenizer=lambda text: text) $ spmat = vectorizer.fit_transform(x_tokens) $
sample.dtypes
austin.isnull().sum()
criteria = {"strategy": "RandomDiscrete", $             "stopping_rounds": 10, $             "stopping_tolerance": 0.00001, $             "stopping_metric": "MSE"}
tweet_archive_clean.tweet_id  = tweet_archive_clean.tweet_id.astype(str)
events.sort_values('speed', ascending = False).tail()
prcp_df.describe()
df2.set_value(index=df2[df2['group']=='treatment'].index, col='ab_page', value=1) $ df2[['intercept', 'ab_page']] = df2[['intercept', 'ab_page']].astype(int)
timelog.tail()
joined.loc[(joined['NewsDesk'] == 'Foreign') & (joined['SectionName'].isnull())].head()
df_r1 = df_r.groupby(["CustID"], as_index=False).agg({"Recency (1)": np.min, $                                                       "Recency (3)": np.mean, $                                                       "Counter":np.max}) $ df_r1["Recency (3)"] = np.round(df_r1["Recency (3)"], decimals = 2)
pc_gt100.index.values
labeled_features = final_feat.merge(failures, on=['datetime', 'machineID'], how='left') $ labeled_features = labeled_features.fillna(method='bfill', limit=7, axis=1) # fill backward up to 24h $ labeled_features = labeled_features.fillna('none') $ labeled_features.head()
raw_large_grid_df.iloc[1:2]
git_blame.info(memory_usage='deep')
vectorizer.get_feature_names()
print('We half the amount of apples and oranges:\n', fruits.loc[['apples', 'oranges']] / 2)
p = getpass.getpass() $ try: $     conn = psycopg2.connect("dbname='cablegate' user='rsouza' host='localhost' password='{}'".format(p)) $ except: $     print("I am unable to connect to the database")
df2.head() $
pd.date_range(pd.datetime(2016, 1, 1), pd.datetime(2016, 7, 1), freq="W")
gdp = web.DataReader('GDP', 'fred', $                     datetime.date(2012, 1, 1), $                     datetime.date(2014, 1, 27)) $ gdp
popCon[popCon.content == 'album'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
R_trip.head()
cat_pizza.head(2)
collection = db.test_collection
lemmatizer = WordNetLemmatizer() $ X = [" ".join([lemmatizer.lemmatize(word, 'v') for word in string.split(" ")]) for string in X] $ X = [" ".join([lemmatizer.lemmatize(word, 'n') for word in string.split(" ")]) for string in X] $ X = [" ".join([lemmatizer.lemmatize(word, 'a') for word in string.split(" ")]) for string in X]
df = pd.read_csv('../data/rawdata/HPD_Heat_HotWater.csv' $                   , dtype = {'Master SR #': str, 'SR #': str})
pd.crosstab(test["rise_nextday"], advpredictions, rownames=["Actual"], colnames=["Predicted"])
event_num = suspects_data.groupby('imsi')['event_type'].unique().apply(len)
p_new = df2.converted.mean() $ p_new # displaying the convart rate for the treatment
print(np.nan is np.nan) $ print(np.isnan(np.nan))
df = pd.read_csv('all_data.csv', parse_dates=True, infer_datetime_format=True, header=0) $ df
import os $ os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "/content/gcp_credentials.json" $ !echo $GOOGLE_APPLICATION_CREDENTIALS
season11 = ALL[(ALL.index >= '2011-09-08') & (ALL.index <= '2012-02-05')]
Grouping_Year_DRG_discharges_payments.groupby(['year']).get_group(2011)[:4]
n_old = df2.query('landing_page == "old_page"').converted.count() $ n_old
s.index
idx = payments_total_yrs[ payments_total_yrs['disc_times_pay']<1000000].index.tolist() $ print('There are',len(idx),'sites with < $1,000,000 in Total Payments') $
obj = pd.Series(range(4), index=['d', 'a', 'b', 'c'])
barplot = yeardf.plot(x=yeardf.index, y=['TSRetAnn'], kind='bar', figsize=(16,10)) $ barplot.set_ylabel('Annual Retrun %')
cust_data2.reset_index(inplace=True) #create variable
model = model_list[1] $ pprint(model.show_topics(formatted=False))
print('Highest opening price - {} \nLowest opening price - {}'.format(max(opening_price), min(opening_price)))
sub_df['time_from_deadline'] = [(x-imp_date).total_seconds() for x in sub_df['date_created']]
output= " select tag_id, sum(retweets) as crt from tweet_details as td inner join tweet_tags as tt where td.tweet_id=tt.tweet_id group by tag_id order by crt desc limit 3; " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['tag_id','Sum'])
json.dumps(letters)[:1000]
raw.location.values[0], raw.location.values[17]
xml_in_sample = xml_in[xml_in['authorName'].isin(random_authors_final)]
print(len(all_complaints)) $ all_complaints.head()
large_df.head()
post_number = len( niners[niners['Jimmy'] == 'yes']['GameID'].unique() ) $ print post_number
data.loc[data.rooms > 12, 'rooms'] = np.NaN
df.groupby(['product_type', 'state']).get_group(('Investment',1.0))
plt.figure(4) $ df['avgtone'].plot.hist(bins=(20),legend=True) #looks Guassian $ plt.figure(5) $ df['avgtone'].plot.kde(legend=True)
import matplotlib.pyplot as plt $ %matplotlib inline
df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5], $                    [np.nan, np.nan], [0.75, -1.3]], $                  index=['a', 'b', 'c', 'd'], $                  columns=['one', 'two'])
archive_df.rating_denominator.value_counts(sort=False)
input_sequence = test_docs[random.randint(0, len(test_docs))] $ print('input sequence: ', input_sequence, '\n\nhidden states:\n') $ vec = proc.transform([input_sequence])[:,1:] $ embedding_model.predict(vec)
df2.drop('drop_it', axis=1, inplace=True) $ df2.head()
save_filepath = os.path.expanduser("~")
train['age'] = train['age'].apply(lambda x: ageTransform(x))
tweet_json.describe()
StockData.tail()
dr_new_8_to_16wk_arimax = dr_new_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted_Hours', 'Predicted_Num_Providers']] $ dr_new_8_to_16wk_arimax.index = dr_new_8_to_16wk_arimax.index.date
print(test_df[test_df.author.isnull()].shape[0]) $ print(test_df.shape[0])
sp_500_adj_close = sp500[['Adj Close']].reset_index()
model_filename = 'models/finalized_traffic_flow_prediction_model.sav' $ loaded_traffic_flow_prediction_model = pickle.load(open(model_filename, 'rb')) $
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=15000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
pgh_311_data.resample("Q").mean()
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old,(1-p_old)]) $ old_page_converted.mean()
FileLink("xgb_unconverged.csv")
price2017 = price2017.drop(['Date','Time'], axis=1)
df["date"] = df["creation_date"] + " " + df["creation_time"] $ df["date"] = pd.to_datetime(df["date"]) $ df2 = df.set_index("date") $ df2.sort_index(inplace=True) $ df2.head()
raw_df.head(20)
pgh_311_data['REQUEST_TYPE'].value_counts()
ns_raw =  root.tag.split('{')[1].split('}')[0] $ ns = '{'+ ns_raw + '}' $ namespaces['af'] = ns_raw $ namespaces
print ('RMSE of age imputation via h2o model is {}'.format(np.sqrt(((y_age_predict - y_age_valid)**2).mean())))
my_date = parser.parse("<your date string here") $ my_date
times.ncattrs()[0]+': '+times.units
df_final['name_islower'] = map(lambda x: x.islower(), df_final['name']) $ df_final.loc[df_final['name_islower'] == True] $
dfRegMet.index = pd.DatetimeIndex(dfRegMet["created_at"])
staff_df = staff_df.reset_index() $ student_df = student_df.reset_index() $ pd.merge(staff_df, student_df, how='left', left_on='Name', right_on='Name')
p_newpage=df2.query('landing_page=="new_page"').shape[0]/df2.shape[0] $ p_newpage
df.loc[df['ruling']=='Full Flop']
msft_cum_ret.resample("M").mean()
Xforadding = np.expand_dims(X, axis=2) $ Xforadding_v = np.expand_dims(VX, axis=2)
store_items.fillna(method='backfill', axis=0) $
ds[4].head() $
url_df.head()
get_url = 'https://staging.app.wikiwatershed.org/api/jobs/{job}/'.format
clicks.info()
z_score, p_value = sm.stats.proportions_ztest((convert_new, convert_old), (n_new, n_old), alternative='larger') $ z_score, p_value
fig,ax = plt.subplots() $ geo_df.plot(ax = ax)
df.loc[((df['group']=='treatment')& (df['landing_page']!='new_page')) | ((df['group']!='treatment')& (df['landing_page']=='new_page'))].count()
df.loc['Mon', ['A', 'B']]
weather_yvr = pd.read_csv('data/weather_yvr.csv')
times = len(y_test) $ vals = np.random.normal(u, sigma, times) $
np.any(x < 0)
classification_df = encoded_df.query('(id == 5 or id == 14) and best != 0') $ classification_df.best.describe()
import numpy as np $ from sklearn.linear_model import LogisticRegression
ftp.login("anonymous", "anonymous")
ab_file.nunique()
pd.describe_option("display.max_rows")
autos[(autos.registration_year > 2016)|(autos.registration_year < 1900)].shape
idx = df_providers[ (df_providers['drg3']==39) &(df_providers['year']==2011) ].index.tolist() $ print('length of IDX',len(idx))
from sklearn.linear_model import LinearRegression $ lm = LinearRegression() $ lm.fit(X,y) $ predictions = lm.predict(X) $ plt.scatter(predictions,y)
click_condition_meta = click_condition_meta[click_condition_meta.dvce_type != 'Unknown'] # google bot
twitter_goodreads_users_df.info()
all_data_long = pd.concat([all_data_as_dfs['pagecounts_desktop-site'], all_data_as_dfs['pagecounts_mobile-site'], $                            all_data_as_dfs['pageviews_desktop'], pageviews_mobile_df]) $ len(all_data_long)
merged_NNN.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False)
about.html
tr_lnd = df.query('group == "treatment" and landing_page != "new_page"') $ tr_lnd2 = df.query('group != "treatment" and landing_page == "new_page"') $ tr_lnd.count() + tr_lnd2.count() $ bad_df = pd.concat([tr_lnd, tr_lnd2]) $ bad_df.count()
list_of_game_ids = list(set(re.findall(r'0021600\d{3}', str(soup))))
!sed -i -e 's/if self.max_leaf_nodes == "None":/if self.max_leaf_nodes == "None" or not self.max_leaf_nodes:/' \ $   /usr/local/lib/python3.5/dist-packages/autosklearn/pipeline/components/regression/gradient_boosting.py
all_text.info()
clean_words=[x.lower() for x in clean_words] $ clean_words.sort() $ clean_words.sort(key=len) $ clean_words
treatment_rate = df2[df2['group'] == 'treatment']['converted'].value_counts()[1]/df2[df2['group'] == 'treatment']['converted'].count() $ treatment_rate
topUserItemDocs.columns=['item_index_corpus','score','user_id','source_item_id','score_weight','item_id'] $ topUserItemDocs.head()
t.reset_index(inplace=False)
%sql select * from mysql.user; $
pokemon.drop(["id"],inplace=True,axis=1) $ pokemon.drop(["Type 2"],inplace=True,axis=1) $ pokemon.head()
StockData.tail()  # pandas.DataFrame.tail(n) shows the last n rows
facilities.head()
df2['ab_page*CA'] = df2.ab_page*df2.CA $ df2['ab_page*US'] = df2.ab_page*df2.US
sample_x1.shape,sample_y1.shape,sample_value.shape
np.random.seed(123456) $ ts = pd.Series(np.random.randn(2), dates) $ type(ts.index)
grouped2.size().unstack().plot(kind="bar", stacked=True, figsize=(8,6)) $ plt.show()
df1 = df1.drop_duplicates(subset = 'Title', keep = False) $
format = lambda x: '%.2f' % x
store = join_df(store, store_states, "Store") $ len(store[store.State.isnull()])
((df[df['group'] == 'treatment']['converted'].mean() -  df[df['group'] == 'control']['converted'].mean())< np.array(p_diffs)).mean()
data_df = df.sort_values(by = "date", ascending=True) $ data_df.fillna(0) $ data_df.head()
campaigns['click_given_open'].sort_values(ascending = False)[0:5]
df2['converted'].mean()
tran_time_diff.info()
tweet_df_clean['retweeted_status'].unique()
a = pd.DataFrame(np.random.random((5, 2)), columns=['one', 'two']) $ a.iloc[1, 1] = np.nan $ a
prob_new_page = df2.query('landing_page == "new_page"').user_id.count() / df2.user_id.count() $
pd.read_csv?
new_page_users = df2.query('landing_page == "new_page"')['user_id'].count() $ total_users = df2.shape[0] $ p_new_page = new_page_users/total_users $ p_new_page
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', usecols=['Created Date', 'Closed Date', 'Agency Name', 'Complaint Type', 'Descriptor'])
highest_temp_station_no = active_stations[0][0] $ highest_temp_obs = active_stations[0][1] $ print(f"The station with the highest number of temperature observations is {highest_temp_station_no} with total observations of {highest_temp_obs}.")
df.groupby('Year').agg(np.size)
articles['pair_id'] = articles.apply(lambda row: nearest_id(row.created_at.tz_convert(None)), axis=1)
pd_train_filtered.sample(3)
df.groupby(['date']).price.mean()
localFile = open("loan_train.csv", "wb") $ localFile.write(rawdata) $ localFile.close()
regression = regression.fit() $ regression.summary()
type(full_globe_temp.values)
log_mod=sm.Logit(result_df['converted'],result_df[['intercept','ab_page','UK','US','UK_page','US_page']]) $ results=log_mod.fit()
result_2 = pd.concat([df1, df3], axis = 1, join_axes=[df1.index]) # concatenate one dataframe on another along columns $ result_2
for x in range(len(twitter_df['Date'])): $     date = convert_time(twitter_df.loc[x,'Date']) $     twitter_df.loc[x, 'Date'] = date $ twitter_df.head()
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
bikedataframe.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)
autos = autos[autos["price"].between(1, 351000)] $ autos["price"].describe()
merged_df.tail(3)
staff.loc[2]
parser.HHParser.regexp.match(aSingleLine)
try: $     df.select(col('nonsenseColumnName')) $ except: $     print("Doesn't exist, friendo")
SGD = SklearnClassifier(SGDClassifier()) $ SGD.train(train_set)
num_base_twitters = df1.shape[0] + df2.shape[0] + df3.shape[0]
n_old = df2[df2['group']== 'control'].shape[0] $ n_old
m3.freeze_to(-1) $ m3.fit(lrs/2, 1, metrics=[accuracy])
autos.loc[autos["registration_year"]>2016, "registration_year" ].shape
sox.to_csv('../../../../../CS 171/cs171-gameday/data/sox.csv', index=False) $ bruins.to_csv('../../../../../CS 171/cs171-gameday/data/bruins.csv', index=False) $ celtics.to_csv('../../../../../CS 171/cs171-gameday/data/celtics.csv', index=False)
datacamp.head()
df.shape[0] $
for type_area in typeA_list: $     w.apply_indicator_data_filter(step = 2, $                               subset = subset_uuid, $                               indicator = 'din_winter', $                               type_area = type_area)
df7.plot(x='Date', y='BG') $ plt.title('Blood Glucose over 600 Values starting Dec. 22, 2017', $           color='Green') $ plt.show()
target_vs_created = history.loc[~history.target.isnull(), ['target', 'target_test']]
tree_tunned.fit(X_train, y_train)
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','treatment']]) $ results = logit.fit() $ results.summary()
tweets_df.created_at.describe()
so[so['score'] >= 10].head()
df3['country'].value_counts() $ dummy_country = pd.get_dummies(df3['country'], prefix='country') $ dummy_country.head()
SMOOTH.plot_catchup_amplitudes(smooth)+expand_limits(y = 0)
(p_diffs > p_diff_observed).mean()
likes.groupby(by=['year','month']).count().plot(kind="line")
tmpdata = raw_large_grid_df.groupby(['eyetracker','subject','posx', 'posy'],as_index=False).mean()
for col in my_df_small_T.iteritems(): $     print(col)
from google.colab import files $ train_char = files.upload('train_char.npy') $ train_word = files.upload('train_word.npy') $ train_labels = files.upload('train_labels.npy')
df_clean.info()
n_old = df2[df2.landing_page == 'old_page'].shape[0] $ n_old
(p_diffs > obs_diff).mean()
df['cleaned_description'] = cleaned_description $ cleaned_description = df.cleaned_description.apply(lambda x: ' '.join(x))
df_data.count()
w.get_step_object(step = 3, subset = subset_uuid).calculate_indicator_status(subset_unique_id = subset_uuid, indicator_list = ['din_winter'])
shows['release_monthday'] = shows['release_date'].dropna().apply(lambda x: x.strftime('%d')) $ shows['release_monthday'] = shows['release_monthday'].dropna().apply(lambda x: str(x)) $ shows['release_monthday'] = shows['release_monthday'].dropna().apply(lambda x: int(x))
print(autos["price"].describe()) $ print(autos.loc[autos["price"]<300,"price"].shape) $ print(autos.loc[autos["price"]<300,"price"].value_counts(ascending = True).sort_index())
pivoted = Fremont.pivot_table('Total', index=Fremont.index.time, columns=Fremont.index.date) $ pivoted.plot(legend=False, alpha=0.01);
plt.bar(left=range(1, len(per_var) + 1), height = per_var, tick_label=labels) $ plt.show()
Twitter_map = folium.Map([45.955263, 8.935129], tiles='cartodbdark_matter', zoom_start = 5) $ Twitter_map
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df['group']) $ df2['ab_page'] = df['treatment'] $ df2.drop(['control', 'treatment'], axis = 1)
plt.figure(figsize=(12,8)) $ sns.barplot(x='DATE',y='TOTALTRAFFIC',data=station,color='blue') $ plt.xlabel("Date") $ plt.title("Total Entries for a Station") $ plt.ylabel("Count of Total Entries")
cols_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'splitseed'] $ X_train.drop(cols_to_drop, axis=1, inplace=True) $ X_age_notnull.drop(cols_to_drop, axis=1, inplace=True)
autos = pd.read_csv('autos.csv', encoding='Latin-1') $ autos = pd.read_csv('autos.csv', encoding='Windows-1251') $
salesdec.Platform.value_counts()
tfidf = models.TfidfModel(terms_matrix) $ corpus_tfidf = tfidf[terms_matrix] $ corpus_lda = ldamodel[corpus_tfidf] $ total_topics = 5
df_categories.tail()
tesla.tail()
van15_fin['reverted_mode'] = van_final.groupby(['userid']).agg({'isReverted': lambda x:stats.mode(x)[0]})
tweet_archive_clean.tweet_id = tweet_archive_clean.tweet_id.astype('str') $ tweet_archive_clean.timestamp = pd.to_datetime(tweet_archive_clean.timestamp)
park.date = pd.to_datetime(park.date)
regr = linear_model.LinearRegression()
train[train['is_weekend'] == 1].head(3)
cols=df.columns.tolist() $ cols.remove('date') $ cols.remove('store_nbr')
( temp_cat.min(), temp_cat.max(), temp_cat.mode() )
print train_data['features'].apply(len).describe()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
male_journalists_mention_summary_df = journalists_mention_summary_df[journalists_mention_summary_df.gender == 'M'] $ male_journalists_mention_summary_df.to_csv('output/male_journalists_mentioned_by_journalists.csv') $ male_journalists_mention_summary_df[journalist_mention_summary_fields].head(25)
def beta(stock,market_returns): $     beta_a = (stock.pct_change()).cov(market_returns.pct_change())/((market_returns.pct_change()).var()) $     return float(beta_a)
df2['timestamp'] = pd.to_datetime(df2['timestamp'], format='%d/%b/%Y:%H:%M:%S +0000', utc=True)
writers.groupby("Country").apply(sum)
df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopMcMrNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TMcM_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0)) $ df_all_wells_wKNN_DEPTHtoDEPT['cat_isTopPalNearby_known']=df_all_wells_wKNN_DEPTHtoDEPT['diff_TPal_Pick_v_DEPT'].apply(lambda x: 100 if x==0 else ( 95 if (-0.5 < x and x <0.5) else 60 if (-5 < x and x <5) else 0)) $
images_predictions.p1.value_counts()
df=pd.read_csv('ab_data.csv') $ df.head()
new_page_converted=np.random.binomial(n_new,p_new) $ new_page_converted
tweets_df["created_at"] = pd.to_datetime(tweets_df.created_at)
times = pd.DatetimeIndex(df_en['date']) $ grouped = df_en.groupby([times.hour, times.minute])['text'].agg(['count'])
train.head(3)
jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 2) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
print(' the probability of an individual converting regardless of the page they receive is {}'.format(round(df2['converted'].mean(),4)))
expected_index = pd.DatetimeIndex(start=raw.index[0], end=raw.index[-1], freq='0.25H') $ ideal_index_df = pd.DataFrame(index=expected_index) $ print(ideal_index_df.head()) $ len(raw.index), len(expected_index)
paired_df_grouped.n_best_co_occurence.hist(bins=20)
ab_df2.query('group == "control"')['converted'].mean()
url_df_full=url_df[url_df['url'].isnull()==False]
station = pd.DataFrame(hawaii_measurement_df.groupby('Station').count()).rename(columns={'Date':'Count'}) $ station_count = station[['Count']] $ station_count $
first_qtr = df['time_open'].quantile(q=0.25) $ print(first_qtr)
df['zone'].value_counts()
df_pol['text']=df_pol['title'].str.replace('\d+', '') $
ctc = pd.DataFrame(columns=ccc, index=ccc)
STUDENT_EMAIL = 'skooch@gmail.com' $ STUDENT_TOKEN = 'SY1bBNISrYHigVui' $ grader.status()
store_items.insert(4, 'shoes', [8,5,0]) $ store_items
df_enhanced.head(1)
flattened_df.head()
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Loud Music/Party') ]['Day of the week'].value_counts() $
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ (z_score,p_value)
pd.Timestamp(pd.datetime(2012, 5, 1))
plot_model(training_model, to_file='./model.png', show_shapes=True)
plt.scatter(vx, vy)
bloomfield_pothole_data['REQUEST_ID'].resample("M").count().plot(figsize=(10,6))
k = k.groupby('project_is_approved').sum().transpose() $ k['avg'] = k[1]/(k[1] + k[0]) $ k['count'] = (k[1] + k[0])
df_new = df.query('landing_page == "new_page"') $ p_new = df_new[df_new['converted'] == 1]['converted'].count() / df_new[['converted']].count() $ print (p_new)
releases.info()
print('Most dril:', tweets_pp[tweets_pp.handle == 'wint'].sort_values( $     'dril_pp', ascending=False).text.values[0]) $ print('Least dril:', tweets_pp[tweets_pp.handle == 'wint'].sort_values( $     'dril_pp', ascending=True).text.values[0])
df.shape[0]
Measurements = Base.classes.measurement $ Stations = Base.classes.station
intersections.info()
df.shape
stock = stock.set_index("Date")
flight.crosstab('start_date','from_city_name').show() $
tweet_archive_clean =tweet_archive_clean[tweet_archive_clean['retweeted_status_user_id'].isnull() == True]
X_train.shape, y_train.shape
bb_pivot_table_min = pd.pivot_table(bb_df, values = ["Inches", "Wt"], index = ['From'], aggfunc = np.min) $ bb_pivot_table_max = pd.pivot_table(bb_df, values = ["Inches", "Wt"], index = ['From'], aggfunc = np.max) $ bb_pivot_table_2 = pd.merge(bb_pivot_table_min, bb_pivot_table_max, left_index=True, right_index=True)
source_counts = clinton_df['source'].value_counts() $ sns.set_style("whitegrid") $ sns.set_palette("deep") $ source_counts.plot.barh()
df_columns[df_columns['Complaint Type']=='Noise - Residential']['Descriptor'].value_counts().head() $
Meter1.Data
rf_yhat = knn.predict(X_test)
df.info()
predicted_2017 = text_clf.predict(test_2017_cleaned) $ print('Accuracy of the model is:',accuracy_score(Y_test,predicted_2017)) $ print('Precision of the model is:',precision_score(Y_test,predicted_2017)) $ print('Recall of the model is:',recall_score(Y_test,predicted_2017)) $ print('confusion Matrix is:',confusion_matrix(Y_test,predicted_2017))
plt.xlabel('Votes') $ plt.ylabel('Number of Movies') $ plt.title('Distribution of Votes') $ exp_budget_vote.hist(histtype = 'stepfilled', label = 'Rates of the Most Expensive Movies')
git_log['timestamp']=pd.to_datetime(git_log['timestamp'], unit='s') $ print(git_log['timestamp'].describe())
result.summary2() # result.summary() wasn't working for some reason, but this one does
prcp_df.plot() $ plt.xlabel("Date") $ plt.ylabel("Precipitation") $ plt.savefig("precipitation_analysis.png") $ plt.show()
aSL.shape, eSL.shape
apple = apple.loc[apple.index.sort_values(),:] $ apple.head()
logit_new_countries = sm.Logit(df_countries['converted'], df_countries[['intercept','new_page','CA_new','UK_new','CA','UK']]) $ result_countries_new = logit_new_countries.fit() $ result_countries_new.summary()
n_old = len(df2.query('group == "control"')) $ n_old
from dotce.report import (most_informative_features, $                           ascii_confusion_matrix)
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     earray = f.root.my_earray $     earray.append(sequence=matrix[1001:5000, :])
df3.country.unique()
df_TempIrregular['timeStamp'] = pd.to_datetime(df_TempIrregular.pubTimeStamp) $
plt.hist(taxiData.Trip_distance, bins = 50, range = [3, 7]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
fwd = df[['Store']+columns].sort_index(ascending=False $                                       ).groupby("Store").rolling(7, min_periods=1).sum()
inputs = feables.drop('match_api_id', axis = 1) $ inputs['label'] = inputs.label.apply(lambda x: 1 if x =='Win' else 0) $ labels = inputs['label'] $ features = inputs.drop(['label'], axis = 1)
df3 = df2.set_index('user_id').join(df_new.set_index('user_id'))
X =Quandl_DF.where(Quandl_DF['Date'] >= '20100101').groupby(['Year','Month'])['GBP_to_HKD'].mean().plot()
public_tweets = extractor.home_timeline('709071172839714818', count=300) $ for tweet in public_tweets[:8]: $     print(tweet.text) $     print()
company = pd.read_csv('data/company.csv') $ company.head()
df = pd.read_pickle("./Rat_Sightings.pkl") $ print(df.shape) $ df.head()
doneMult = slicer.pipe(multi,2) $ doneMult
mapping_dict = {} $ for row in sponsors_df.index: $     mapping_dict[row] = sponsors_df['Sponsor_Classification'][row] $ mapping_dict
trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy') $ val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')
pd.DataFrame(data, columns=['year','state', 'pop'])
data.tail(6)
test.to_csv('update/agg-users.csv', index=False) $ candSplit.to_csv('update/agg-users-by-candidate.csv', index=False)
print('There are',len(list_of_py),'files that are','py','files') $ for k in range(len(list_of_py)): $     print(list_of_py[k]) $     shutil.copy(os.path.abspath(list_of_py[k]), holding_file_name) # target filename is /dst/dir/file.ext $
health_data_row.xs(key=(2013 , 1), level=('year', 'visit'))
p_new = df2.query('converted == 1').user_id.count()/df2.user_id.count() $ p_new
df['source'].value_counts()
with pd.option_context('float_format', '{:.2f}'.format): print(df_mes.loc[:,['travel_time','average_speed']].describe()) #10.294.628 cases
pulledTweets_df['emoji_enc_text'] = pulledTweets_df['Processed_tweet'].apply(declump_emojis_in_text) $ pulledTweets_df['emoji_enc_text'] = pulledTweets_df['emoji_enc_text'].apply(encode_emojis) $ pulledTweets_df['emojis'] = pulledTweets_df['Processed_tweet'].apply(extract_emojis)
1-0.190/2
import os $ if not os.path.exists("models/{}".format(model_name)): $     os.mkdir("models/{}".format(model_name))
for dataset in combine: $     dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int) $ train_df.head()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=8000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
wrd_clean['name'] = wrd_clean['name'].apply(lambda x: 'NaN' if x == "None" else x) $ wrd_clean['doggo'] = wrd_clean['doggo'].apply(lambda x: 'NaN' if x == "None" else x) $ wrd_clean['floofer'] = wrd_clean['floofer'].apply(lambda x: 'NaN' if x == "None" else x) $ wrd_clean['pupper'] = wrd_clean['pupper'].apply(lambda x: 'NaN' if x == "None" else x) $ wrd_clean['puppo'] = wrd_clean['puppo'].apply(lambda x: 'NaN' if x == "None" else x)
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] $ cdf.head(9)
df_country = pd.read_csv('countries.csv')
itos.insert(0, '_pad_') $ itos.insert(0, '_unk_')
jobs.loc[(jobs.FAIRSHARE == 2) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result = logit.fit()
dtm_df['news'].sum()
duration_df.shape
(df.location_id.nunique(), $  df.raw_location_text.nunique(), $  df.raw_location_text.str.lower().nunique())
df2[df2.duplicated(subset='user_id', keep=False) == True]
twitter_archive_master.groupby('has_name')['rating'].mean()
df_station.drop('zipcheck', axis=1, inplace=True )
df_R['Month'] = df_R['Date'].apply(lambda s:s.split('-')[1])
connection = sqlite3.connect("projectTwit.db") $ cursor = connection.cursor() $ cursor2= connection.cursor()
pos_tweets.shape
find_emoji = emoji_pattern.findall(text) $ print(find_emoji)
df_all.age.fillna(value=0, inplace=True) $ df_all.age[df_all.age > 116] = 0 $ df_all.age[df_all.age < 15] = 0 $
import numpy as np $ import matplotlib.pyplot as plt $ import mlai
d + pd.tseries.offsets.Week()
r.json() 
print login_response.status_code $ login_page = BeautifulSoup(login_response.content, 'lxml')
df_tweet_json_clean['created_at'].head() $
data['age']=(data['Week Ending Date']-data['temp']).dt.days $ data.head(10) $
%matplotlib inline $ ventas_por_tipo.plot('bar', title = 'Ventas por tipo por anio', figsize=(15,10))
print("Output of percentage missing data\n") $ for col in missing_info: $     percent_missing = df_users_first_transaction[df_users_first_transaction[col].isnull() \ $                          == True].shape[0]/df_users_first_transaction.shape[0] $     print('percent missing for column {}: {}'.format(col, percent_missing))
df_index.head()
frame3.values
df_new.groupby('country')['converted'].mean()
for i in TrainData_ForLogistic.columns: $     if i not in TestData_ForLogistic.columns: $         print i
p_new =df2[df2['landing_page'] == "new_page"].converted.mean() $ p_new
df.sort_index(axis=1,ascending=False)
diff1 = treatment_convert - control_convert $ diff1 $ plt.hist(p_diffs) $ plt.axvline(x=diff1,color ='red') $ plt.show
pd_train.sample(10)
train['business_day'] = train.date.isin(business_days) $ train['holiday'] = train.date.isin(holidays)
import re $ import datetime $ m = re.search('(?<=abc)def', 'abcdef') $ print(m.group(0)) $ print('This was last run on: {0}'.format(datetime.datetime.now()))
Image(url= "https://pbs.twimg.com/media/C2tugXLXgAArJO4.jpg")
log_mod = sm.Logit(df2.converted,df2[['intercept','ab_page']]) $ result_1 = log_mod.fit()
sdf = Item(gis, item_id).layers[0].query(out_sr={'wkid': 4326}).df $ sdf.head()
rf_v1.hit_ratio_table(valid=True)
q_derived_count.results()["count"].sum()
mp2013 = pd.period_range('1/1/2013', '12/31/2013', freq='M') $ mp2013
print('Best score:', gs.best_score_)
Measurements = Base.classes.measurement $ Stations = Base.classes.station $ Hawaii = Base.classes.hawaii
df_loc10 = pd.read_csv('/project/stack_overflow/Data/geocoded/geocoded_QueryResults_(10).csv')
set_themes.head()
education_data=education.iloc[education_data_rows, :]
reddit.head(50)
ann_ret_SP500[9].describe()
autos['vehicle_type'].value_counts()
merged_data['inv_creation_day'] = merged_data['invoices_creation_date'].dt.day $ merged_data['inv_creation_month'] = merged_data['invoices_creation_date'].dt.month $ merged_data['inv_creation_year'] = merged_data['invoices_creation_date'].dt.year
for ix, row in df.iterrows(): $     if row.FlightNumber != row.FlightNumber: $         df.at[ix, 'FlightNumber'] = 10045 + ix * 10 $ df['FlightNumber'] = df.FlightNumber.astype(np.int64) $ df
df2[df2.duplicated(['user_id'],keep=False)]
df2['intercept']=1 $ df2['ab_page']=pd.get_dummies(df2['group'])['treatment']
lm = sm.Logit(df['converted'], df[['intercept', 'ab_page']]) $ results = lm.fit()
df.describe()
corrected_log.head()
df.head()
df2['date'] = pd.to_datetime(df2['timestamp']) $ df2['date'] = pd.DatetimeIndex(df2.date).normalize()
All_tweet_data_v2.timestamp=pd.to_datetime(All_tweet_data_v2.timestamp) $ All_tweet_data_v2.timestamp.head()
df2[df2.duplicated('user_id',keep=False)] $
treatment = df2[df2['group']=='treatment']['converted'].mean() $ treatment
df.drop(df.query('group == "treatment" and landing_page == "old_page"').index, inplace=True) $ df.drop(df.query('group == "control" and landing_page == "new_page"').index, inplace=True) $ df2=df
print("Times new_page and treatment don't line up is {}".format(len(gr_e1) + len(gr_e2)))
result = pd.concat([df1, df2]) # concatenate one dataframe on another along rows $ result
df_tick_clsfd_sent.head(10)
order.describe()
(grades + bonus_points).fillna(0)
df2.drop_duplicates()
def on_base_percentage(x): $     nominator = x['h']+ x['X2b']+x['hbp'] $     denominator = x['ab']+x['X2b']+x['hbp']+x['sf']+1e-6 $     return nominator/denominator $ baseball.apply(on_base_percentage, axis=1).round(3).sort_values()
session.query(measurement.date).order_by(measurement.date).first()
party_type_crosstab = by_party_type.set_index( $     ["election_year", "party_type"] $ ).unstack(1).reset_index().fillna(0)
lst = data_after_first_filter.WATER_BODY_NAME.unique() $ print('Waterbodies in workspace dataset:\n{}'.format('\n'.join(lst)))
breed_predict_df.sample(20)
data.plot(title='AAPL stock price | 50 & 200 days SMAs', figsize=(10, 6))
weather.head()
live_weights.plot.hist(rwidth=.9, bins=np.arange(0,16,1)) $ plt.xlabel('weight')
df[df['status_type']=='link'].groupby('dayofweek').status_id.count()
final_ = sample.merge(b_df,on="Date",how="outer")
clf=clf.fit(X_train, y_train) # perform training
data.loc[data.L2 == ' ', ['L2']] = np.nan $ data.L2.unique()
test_process('ADV19000601-V02-06-page13.txt')
ps[pd.datetime(2011, 12, 25):]
own_star.sort_values(by=['user_id', 'repo_id']).head(10)
pumaBB = pd.merge(pumaBB.iloc[:,:-1],pumaPP[['B28002_001E','NAME']],on='NAME') $ pumaBB['pcBB'] = pumaBB.B28002_004E/pumaBB.B28002_001E * 100
tweets_clean.puppo.value_counts()
random_locs = random.choices(list(non_empty_crash_data_df['Location']), k=sample_size)
for i in range(10,len(myIP)): $     print table2[table2['lower_bound_ip_address']== low[low < myIP[i]].max()]['country'] $
plt.hist(np.log(threeoneone_census_complaints[threeoneone_census_complaints['avg_fix_time_sec']>0]['avg_fix_time_sec']),bins=100) $ plt.show()
pickle.dump(best_dt, open('xgb_model.pkl', 'wb'))
folder_name = '/home/workspace' $ if not os.path.exists(folder_name): $     os.makedirs(folder_name)
df = pd.read_sql('SELECT a.address_id, a.city_id, c.city FROM address a RIGHT JOIN city c ON a.city_id = c.city_id ORDER BY a.address_id DESC;', con=conn) $ df
remove_dog = np.intersect1d(dog1, dog2)
events = events.withColumn('event_date', events['event_date'].cast('date'))
dsDir = "data/" $ bus = pd.read_csv("data/businesses.csv", encoding='ISO-8859-1') $ ins = pd.read_csv("data/inspections.csv") $ vio = pd.read_csv("data/violations.csv")
url = form_url(f'organizations/{org_id}') $ response = requests.delete(url, headers=headers) $ print_status(response)
df.info()
r_start_date = '2017-01-01' $ r_end_date = '2017-12-31' $ r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key='\ $                  +API_KEY+'&start_date='+r_start_date+'&end_date='+r_end_date)
type_of_compaint = 'Rodent' $ general_volume=general_volume_types[general_volume_types.complaint_type == type_of_compaint] $ general_volume.head() $ general_volume.columns=['ds','complaint_type','y']
bucket.upload_dir('data/wx/tmy3/raw/', 'wx/tmy3/raw', clear_dest_dir=True)
sel2=[Station.name, $      func.count(Station.name)] $ station_count=session.query(*sel2).group_by(Station.name).all() $ nb_st=len(station_count) $ print(f"The total numbers of available statations is {nb_st} ") $
caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled, $                             name="caps2_predicted")
users_not_odu.groupby(users['Answered'] > 0).mean()
question_2_dataframe_in_top_zips.\ $     reset_index().\ $     sort_values(['incident_zip','count'], ascending=False).\ $     set_index(['incident_zip','complaint_type'])
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://pbs.twimg.com/media/CmgBZ7kWcAAlzFD.jpg")
autos["unrepaired_damage"].value_counts()
n_new=df2[df2['landing_page']=='new_page']['converted'].count() $ n_new
def split2(elem): $     elem = elem.replace('POINT (', '') $     elem = elem.replace(')', '') $     return elem.split(' ')[1]
Counter(weather_df["weather_main"].values)
df2.query('group=="treatment"')['converted'].mean()
import xgboost as xgb $ from xgboost import XGBClassifier $ xgb_train = xgb.DMatrix(X, y, feature_names=X.columns) $ num_rounds = 200
auto.Price.mean()
data = pd.DataFrame(session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > '2016-08-23').all()) $ data.head()
pp_dict(collection.find_one())
df = df.set_index('booking_dt') $ df.head()
rfmTable.head()
df1.info()
highest_obs_temps = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $     filter(Measurement.date > '2016-08-023').\ $     filter(Measurement.station == highestobs_station).\ $     order_by(Measurement.date).all() $ highest_obs_temps
print(my_tree_one.feature_importances_) $ print(my_tree_one.score(features_one, target))
0.5
df.text[279]
import torchtext $ from torchtext import vocab, data $ from torchtext.datasets import language_modeling
pd.DataFrame(cats['SexuponOutcome'].value_counts()).plot(kind='bar')
data2=pd.read_csv('data/intraday_60min_CTL (1).csv') $ data2['timestamp']=pd.to_datetime(data2['timestamp'])
adopted_cats.loc[adopted_cats['Color']=='Black/Black','Color'] = 'Black' $ adopted_cats.loc[adopted_cats['Color']=='Tortie/Black','Color'] = 'Tortie Black' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Black','Color'] = 'Calico Black' $ adopted_cats.loc[adopted_cats['Color']=='Tortie/Black Smoke','Color'] = 'Tortie Black Smoke'
df.A > 0
autos['registration_year'].value_counts(normalize=True).head(10)
least = pd.DataFrame(data.groupby('tasker_id').hired.sum()) $ least.loc[least['hired']==0]
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
merged.sort_values("amount", ascending=False)
np.isnan(StockData).sum()
venues_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/venues.csv')['Body'].read().decode('utf-8') $ venues = pd.read_csv(StringIO(venues_csv_string), header=0)
1/np.exp(-0.0149), 1/np.exp(-0.0408), np.exp(0.0099)
df_twitter_archive_master.info()
sub1 = pd.merge(sub1, sub2, on = ['hacker_id', 'challenge_id'], how = 'left')
os.chdir(path)
sl_data3=sl_data3.fillna(0)
close = dataset["Close"] $ diff = [] $ for i in range(len(dataset["Close"])-1): $     diff.append(close[i+1] - close[i])
df2['tweet_id'][(df2['predict_3_breed'] == True)].count()/2075 $
print(dfd.in_pwr_47F_min.describe()) $ dfd.in_pwr_47F_min.hist()
df2 = df2.drop("treatment", axis=1)
from sklearn.model_selection import train_test_split
d311_in = d311.apply(within_area,axis=1) $ d311["inc_311"] = 1 $ d311_gb = d311.loc[d311_in,["long","lat","inc_311"]].groupby(["long","lat"],as_index=False).sum()
df_yt_resolved.head(2)
intervention_history.sort_values(['INSTANCE_ID', 'CRE_DATE_GZL', 'INCIDENT_NUMBER'], inplace=True)
last_id = trump[0]._json['id'] $ print('tweet url: https://twitter.com/i/web/status/{}'.format(last_id)) $ retweets = [x._json for x in api.retweets(id=last_id,count=100)] $ print('got {} retweets!'.format(len(retweets))) # ... unreliable, won't get exactly 100. oh well!
dictionary.save('/home/david/Desktop/guardian.dict')
quarters = pd.period_range('2016Q1', periods=8, freq='Q') $ quarters
user=tweet.author. $ for param in dir(user): $     if not param.startswith("_"): $         print "%s : %s" % (param, eval("user." + param))
autos["odometer"] = autos["odometer"].str.replace("km","") $ autos["odometer"] = autos["odometer"].str.replace(",","") $ autos.odometer = autos.odometer.astype(float) $ autos.rename({"odometer":"odometer_km"}, axis =1, inplace = True)
df.query("landing_page=='new_page' and group=='treatment'").shape[0]
people[np.array([True, False, True])]
data['age']=data['Week Ending Date']-data['temp'] $ data['age'].head(10) $
groupby_regiment = df['score'].groupby([df['author'],df['com_id']]) $ q=groupby_regiment.max() $ q.head()
autos["odometer_km"].describe()
for res_key, df in entso_e.items(): $     entso_e[res_key], nan_tables[res_key + ' ENTSO-E'], overviews[res_key + ' ENTSO-E'] = find_nan( $         df, res_key, headers, patch=True)
af = pd.DataFrame(allfiles())
labels = df['fan_of_team_playing'].values $ docs = df['comment_body'].values $ vocab_size = 100000 $ encoded_docs = [one_hot(str(d), vocab_size) for d in docs]
xgb_learner.fit_best_model(dtrain)
r.json()
df_inds.columns.value_counts()[:5]
columns = inspector.get_columns('Station') $ for c in columns: $     print(c['name'], c["type"])
df_concensus_uaa.groupby('metric').tail(3) #last three records of consensus, how many records to we need before the final release?
conn_b.commit()
betting_sets = bets.to_records(index=False)
df_uro_dd_dummies_no_sparse.shape $ df_uro_dd_dummies_no_sparse.head()
dfWeek.set_index('Date').to_csv('PipelineInventorySales_Weekly.csv')
print("Service URL: {}".format(web_service_full._service_url)) $ print("Service Key: {}".format(web_service_full._api_key))
c = watershed.unary_union.centroid # GeoPandas heavy lifting $ m = folium.Map(location=[c.y, c.x], tiles='CartoDB positron', zoom_start=12)
pd.read_sql(f'explain {sql_on}', engine).head()
%timeit pd.read_sql(f'explain {sql}', engine).head()
combined_df['intercept'] = 1 $ logistic_model = sm.Logit(combined_df['converted'], combined_df[['intercept', 'CA', 'US']]) $ result = logistic_model.fit()
kick_projects_ip_scaled_ftrs = pd.DataFrame(preprocessing.normalize(kick_projects_ip[features])) $ kick_projects_ip_scaled_ftrs.columns=list(kick_projects_ip[features])
df.shape, df.size
df2.shape[0]
df2 = pd.DataFrame() $ df2['X'] = norm.rvs(size=N)
plt.hist(real_runtimes['fixed_runtime'].dropna()) $ plt.title('Distribution of Runtime') $ plt.ylabel('Frequency') $ plt.xlabel('Runtime')
new_df.head()
WorldBankLimited.toPandas()
df1.join(df2)
numeric_df.agg(*corr_aggs).show()
taxiData.columns
evaluator.get_metrics('micro_avg_accuracy')
intForMaxTS=interactionsData[(interactionsData['interaction_type']>0) & (interactionsData['interaction_type']!=4)] $ intForMaxTS=intForMaxTS.groupby(['user_id','item_id'])['created_at'].max() $ intForMaxTS=intForMaxTS.to_frame()
np.vstack((a, a))[(0,1,2),:]
tweet_df.head() $
donations['Donation Amount'].value_counts().head(10)
np.savetxt(outputs / 'data.dat', data)
df2_dup = df2[df2.duplicated(['user_id'], keep = False)] $ df2_dup
locations = session.query(Measurement).group_by(Measurement.station).count() $ print(locations)
y = df.groupby('Journal')['Title'].count() $ df_s = df[df['Journal'].apply(lambda x: y[x]>15)] $ avg_pd = df_s.groupby('Journal').agg({'PubDays':'mean', 'Publisher':'first'}) $ avg_pd = avg_pd.reset_index() $ avg_pd['Journal']=avg_pd['Journal'].str.lower()
raw_data.drop(null_desc.index, axis=0, inplace=True)
gen_aranjuez = pagegenerators. \ $                      CategorizedPageGenerator(pb.Category(commons_site, "Aranjuez"), recurse=True) $ images_aranjuez = list( $     set([page.title(withNamespace=True) for page in gen_aranjuez if page.is_filepage()]) $ )
google_request.content[0:500]
print('reduce memory') $ utils.reduce_memory(tran_time_diff) $ tran_time_diff.info()
ab_data_nr_rows = ab_data.shape[0] $ ab_data_nr_rows
aTL.shape, eTL.shape
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.country.unique()
relevant_pings.count()
big_append = df1.append([df2,df3,df4], sort=True) $ tabulation = pd.crosstab(big_append['Temperature (F)'], columns='count', margins=True) $ tabulation.columns.name = 'Freq Tabulation' $ tabulation
liberia_data1 = liberia_data[liberia_data.Description.isin(['Total new cases registered so far', $                                                          'New deaths registered'])] 
sl.loc[sl.wpdx_id=='wpdx-00063117']
df.all(axis=1)
train_msk = ((train.click_timeDay == 8) & (train.click_timeHour >= 9)) | \ $ ((train.click_timeDay == 9) & (train.click_timeHour <= 8)) $ val_msk = ((train.click_timeDay == 9) & (train.click_timeHour >= 9) & (train.click_timeHour <= 15))
df2.drop(1404, inplace=True) $ df2.shape
domain_list = ["wsj.com", "nytimes.com", "techcrunch.com", "washingtonpost.com", "cnn.com", "fox-news"]
df_2018.dropna(subset=['Specialty'], how='all', inplace=True)
pothole = df[df['Descriptor'] == 'Loud Music/Party'] $ pothole.groupby(pothole.index.weekday)['Created Date'].count().plot()
insert.compile().params #
print((fit.shape, fit1_test.shape)) $ print((fit2.shape, fit2_test.shape)) $ print((fit3.shape, fit3_test.shape))
is_attributed = np.exp(preds)[:, 1]
from nltk.corpus import stopwords $ stop = stopwords.words('english') $ df['body'] = df['body'].apply(lambda x: " ".join(x for x in x.split() if x not in stop)) $ df.body.head()
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=400000, usecols=['Created Date', 'Closed Date', 'Agency', 'Complaint Type', 'Descriptor'])
im_clean["p1"] = im_clean["p1"].str.lower() $ im_clean["p2"] = im_clean["p2"].str.lower() $ im_clean["p3"] = im_clean["p3"].str.lower()
%matplotlib inline $ AAPL.plot()
from bs4 import BeautifulSoup#Beautiful Soup is reading the HTML and making sense of its structure.
data.columns
control_df = df2[df2['group']=='control'] $ n_control = control_df.shape[0] $ 1.0*sum(control_df['converted']==1)/ n_control
diff= new_page_converted.mean() - old_page_converted.mean() $ diff
conn.columninfo(table=dict(name='myclass', caslib='casuser'))
for c in ccc: $     if not os.path.isdir('output/' + c): $         os.makedirs('output/' + c)
pd.set_option('display.max_rows', 500)
df_clean.rating_denominator = df_clean.rating_denominator.str.replace('(\d*)', '10')
df.select('c', c_secondDigit3).show(5)
creations.groupby("creator_autoconfirmed").size()
print(contrib_state.sort_values(['amount'], ascending=False).set_index('contributor_state'))
try: $     ValidHHFile.parser.parseLine('41B0044210100320100101-9999-9999   25   24') $ except parser.BadLine as err: $     print(err)
preprocessor.print_infos('consistency')
soup = BeautifulSoup(r.text, 'html.parser')  
df2_treatment = df2[df2['group'] == 'treatment']['converted'].mean() $ print('The probability that an individual in the treatment group converted is: {}'.format(round(df2_treatment, 4)))
pd.crosstab(df_result.launched_year, df_result.State).plot()
response = Query(github_index).get_terms("user_org")\ $                                        .fetch_aggregation_results() $ buckets = response['aggregations']['0']['buckets'] $ organizations = pd.Series([item['key'] for item in buckets]) $ print(organizations.head()) $
stock_data = pd.concat([data_1, data_2])
import matplotlib as mpl $ import matplotlib.pyplot as plt $ from IPython.display import Image $ import seaborn as sns
barcelona['GMT'] = to_datetime(barcelona['GMT']) $ barcelona = barcelona.sort('GMT')
series.index $
sensors_num_df = detective.NumericalSensors(db.master_df)
cassession.caslibinfo()
backup = df2.copy()
df.groupby('episode_id')['id'].nunique().head()
autos = autos[autos['price'].between(1,1300000)] $ autos['price'].describe()
from urllib.parse import quote $ from pprint import pprint
twitter_archive_master['rating_numerator'] = twitter_archive_master['full_text'].str.extract('([0-9][0-9]?\.?[0-9]?)\/\d\d\d?[^\/]').astype(float) $ twitter_archive_master['rating_denominator'] = twitter_archive_master['full_text'].str.extract('[0-9][0-9]?\.?[0-9]?\/(\d\d\d?)[^\/]').astype(float)
PredClass.df_model.shape
df = df.reset_index(drop=True)
users['invited'] = [True if i else False for i in users.invited_by_user_id]
gr2_success = pd.merge(user_group2,success_order, how='inner', left_on=['CUSTOMER_ID'], right_on =['CUSTOMER_ID']) $ len(gr2_success)
dfMeta[:2000].to_html('Metasubset.html')
np.exp(-0.0408)
data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=["Tweets"]) $ display(data.tail(10))
station_df = pd.read_sql("SELECT * FROM station", conn)
print("The maximum change in one day is {}. The minimum change in one day is {}".format(max(change_values), min(change_values)))
control_new.index
df.drop(34867,inplace=True)
import urllib.request $ post_url = 'https://api.instagram.com/oembed?url=' + df.iloc[0]['link_to_post'] + '/' $ response = urllib.request.urlopen(post_url) $ html = json.load(response)   $ print(html['html'])
b = np.loadtxt('myfile.csv', delimiter=',')  #Load data into `b`. $ os.remove('myfile.csv')
bool_list = crime_data["OFNS_DESC"].isin(['VEHICLE AND TRAFFIC LAWS', 'ALCOHOLIC BEVERAGE CONTROL LAW']) == False $ crime_data = crime_data[bool_list] $
n_new, n_old = df2.landing_page.value_counts() $ print('n_new is:',n_new)
def wins(team, month, day, year): $     team = teamtables[team] $     team = team[team["Date"]<datetime.datetime(year,month,day)] $     return team["Wins"][len(team["Wins"])-1]
P['prediction']=P.prediction.astype(str) $ P.dtypes
Z = np.zeros(10) $ Z[4] = 1 $ print(Z)
dt.strftime('%m/%d/%Y %H:%M')
df_predictions_clean.p1_dog.value_counts()
autos["price"].value_counts().sort_index(ascending=False).head(20)
params = dict(limit = 100, offset = 0) $ response = requests.get(url_speaker,params = params)
result['sourceId'].nunique()
def sharpe(returnseries): $     mean = 12*np.mean(returnseries) $     vol = np.sqrt(12)*np.std(returnseries) $     return np.round(mean/vol, 3)
rf.fit(X, y)
len(column_datasets['train'].y)
missing_values_count = building_pa_prc.isnull().sum() $ missing_values_count[:]
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG_max =(grouped.aggregate(np.size)['id_num']) $ grouped_by_year_DRG_max.head()
datatest.info()
df.show()
na_df.isnull() # check elements that are missing 
staff['Fulltime'] = True $ staff.head()
uniquehedgefundnames = hedgeprop1[['CIK','Company Name']].set_index(['CIK']).drop_duplicates() $ uniquehedgefundnames
smith_df = smith[['text', 'label']] $ skip_df = skip[['text', 'label']] $ train = pd.concat( [smith_df, skip_df] ).reset_index()
df.of_fielding_alignment.value_counts()
cityID = 'e41805d7248dbf1e' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Modesto.append(tweet) 
df_new['US_ab'] = np.multiply(df_new['ab_page'],df_new['US']) $ df_new['CA_ab'] = np.multiply(df_new['ab_page'],df_new['CA']) $ df_new['UK_ab'] = np.multiply(df_new['ab_page'],df_new['UK']) $ df_new.head(10) $
bus = cX_test.loc[p_flag & t_flag,['name', 'text', 'city', 'prob']].copy() $ bus = bus.sort_values(by='prob', axis=0, ascending=False) $ bus.head()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK', 'ab_page', 'CA_ab_page', 'UK_ab_page']]) $ results = logit_mod.fit() $ results.summary2()
headline_unique_date_count = headline_date_count.groupby("headline").size() $ headline_unique_date_count[headline_unique_date_count > 1]
X = pivoted.fillna(0).T.values
third_qtr = df['time_open'].quantile(q=0.75) $ print(third_qtr)
print("Number of rows in the sheet:"), $ print(sheet.nrows)
tier3 = getcrimesby_tier(store1,Violent) $ tier3_df = tier3.copy()
np.exp(results_new.params)
test1.to_csv('../data/negTweetsFinal.csv')
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2], keys=["df1", 'df2'])
indicator= 'din_winter' $ w.get_step_object(step = 3, subset = subset_uuid).get_indicator_data_filter_settings(indicator)
sns.regplot(x=df['score'], y=df["comms_num"], $             line_kws={"color":"r","alpha":0.7,"lw":5});
df2.reset_index(inplace=True, drop=True);
order_data.head()
users_chanel = pd.merge(Users, Relations, how = 'outer', on=['id_partner', 'name']) $ users_chanel.head()
len(df[~(df.data == {})])
lin_svc_clf.fit(X_train_cont_doc, Y_train_lab)
image.sample(20)
theft.iloc[0:10]
fig, ax = plt.subplots(figsize=(8,4)) $ Ralston.hist(column="TMAX", ax=ax); $
known_places = list(processed_tweets[["location", "lat", "lon"]].drop_duplicates('location')['location'])
%%time $ df.to_hdf('store.h5', key='losses')
df_all=pd.DataFrame(all_prcp,columns=['date','Prcp']) $ df_all.head()
df2['intercept']=1 $ df2['ab_page'] = 1 $ df2['ab_page'][df['group'] =='control'] = 0 $ df2['ab_page'][df['group'] =='treatment'] = 1 $ df2.info()
dates = [random_date("1/1/2010", pd.datetime.now()) for _ in range(1000)] $ df_date = pd.DataFrame(dates, columns=['date']) $ df_date.head()
twitter_archive_full['retweet_count'] = twitter_archive_full.retweet_count.fillna(0.0).astype(int) $ twitter_archive_full['favorite_count'] = twitter_archive_full.favorite_count.fillna(0.0).astype(int) $
mean_dist = reddit['Above_Below_Mean'].value_counts()/len(reddit) $ mean_dist
df_uro = df_uro.rename(columns = {'METAC_SITE_NM1': 'METAC_SITE'})
unique_id = df_providers['name'].unique()
trips_data['bike_id'].hist(bins=20) $ plt.show()
tmdb_movies_production_countries_revenue.head()
print(train_data.isnull().sum()) $ print(test_data.isnull().sum())
so.loc[(so['score'] >= 5) & (so['ans_name'] == 'Scott Boston')]
data.loc[3] $ data.head() $ data.tail(3) $ data.shape
statetotals = pd.DataFrame(locations['State'].value_counts()) $ statetotals.head(n=10)
mmileage_series = pd.Series(mean_mileage_by_brand).sort_values(ascending=False) $ mmileage_series
import statsmodels.api as sm $ convert_old = control_df.converted.sum() $ convert_new = treatment_df.converted.sum() $ n_old = control_df.shape[0] $ n_new = treatment_df.shape[0]
known_shorteners += ['youtube.com']
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05)))
ds_train.head()
data.corr()['Order_Qty']
hour = vol.index.hour $ hourly_volume = vol.groupby(hour).mean()
n_old = df2.query('landing_page == "old_page"').shape[0] $ print('Number of occurrences for landing_page == "old_page": {}'.format(n_old))
filter_df['start_time'].min(), filter_df['start_time'].max() $
season1 = pd.date_range("2016-10-01", "2017-06-30").to_series() $ season2 = pd.date_range("2017-10-01", "2018-06-30").to_series()
loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)
final_topbike.head(0)
bird_data['timestamp'] = pd.Series(timestamps, index = bird_data.index) $ print(bird_data.timestamp.head())
largest_collection_size = df_meta['collection_size_bytes'].max() $ largest_collection_size
cityID = '28ace6b8d6dbc3af' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chula_Vista.append(tweet) 
tvec = TfidfVectorizer(stop_words=stopwords.words('english'), $                                  lowercase=True,max_features=500) $ X_train_matrix = tvec.fit_transform(X_train['text']) $ X_test_matrix = tvec.transform(X_test['text'])
buff =np.mean(test, axis=1, keepdims=True) $ print(buff) $ print(test - buff)
tweet_archive_clean.dog_stage = tweet_archive_clean.dog_stage.where(tweet_archive_clean.dog_stage != 'None')
df.info()
df2 = df.copy() $ df2 = df2.drop('ENTRIES',1) $ df2 = df2.drop('EXITS',1) $
df3[df3['group'] == 'treatment'].head()
data = df_norm(data[39000:]) $ data.plot()
from helper_code import mlplots as ml $ ml.confusion(y_test.reshape(y_test.shape[0]), $              predicted, ['No Failure', 'Failure'], 2, 'O-Ring Thermal Distress')
x_min=np.around(np.amin(windfield_matched_array),decimals=-1)-10 $ x_max=np.around(np.amax(windfield_matched_array),decimals=-1)+10 $ x_num= np.around(np.amax(windfield_matched_array)-np.amin(windfield_matched_array))
party_crosstab = by_party.set_index([ $     "election_year", "party_name" $ ]).unstack(1).reset_index().fillna(0)
df['line_up'] = np.where((df['landing_page']=='new_page')!=(df['group']=='treatment'), 0, 1) $ df['line_up'].value_counts()
print 'Not date looked crazy so i just format them' $ df_cleaned['ImportDate'] = pd.to_datetime(df_cleaned['ImportDate']) $ df_cleaned['yearImport'] = df_cleaned['ImportDate'].apply(lambda d: d.year) $ df_cleaned['yearMonthImport'] = df_cleaned['ImportDate'].apply(lambda d: dt.datetime(d.year, d.month,1))
tweetsF17 = pd.read_sql_query("SELECT created_at, extracted FROM tweets_info;", connF17, parse_dates=['created_at'] ) $ tweetsF17['created_at'] = tweetsF17['created_at'].dt.tz_localize("UTC").dt.tz_convert("Europe/Berlin") $ print("Number of Tweets: %s" %len(tweetsF17)) $ tweetsF17.head()
f_close_clicks_os_train = spark.read.csv(os.path.join(mungepath, "f_close_clicks_os_train"), header=True) $ f_close_clicks_os_test = spark.read.csv(os.path.join(mungepath, "f_close_clicks_os_test"), header=True) $ print('Found %d observations in train.' %f_close_clicks_os_train.count()) $ print('Found %d observations in test.' %f_close_clicks_os_test.count())
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
n_new=df2.query('landing_page=="new_page"').count()[0] $ n_new $
outcome.info()
stock_data.Date
ts.head()
df.loc[6:10]
df_click = pd.read_csv('data/clicking_data.csv') $ df_click.head()
width, height = 18, 6 $ plt.figure(figsize=(width, height)) $ jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 4) & (jobs.GPU == 0) & (jobs.Memory % 4000 >= 0)].groupby('Memory').Wait.median().plot(kind = 'bar', logy = False) $
df_M7 = pd.DataFrame({'DATE':dfM.DATE[16:-12],'ACTUAL':[i[0] for i in M7_actual], $                       'M7':[i[0] for i in M7_pred], $                      'BASELINE':dfM.t[16:-12]})
clustered_hashtags.filter(clustered_hashtags.hashtag == 'brexit').show()
for k in range(10,20): $     file_name = States[k] + '-2018-07-12-2015-871 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS W MCC' $     print('run txt2pdf.py -o ' +"'" +file_name + '.pdf'  +"'  '" +file_name + '.txt'  +"'" ) $
stemmed_dict_list = list(stanford_word_list.Word_stem)
filepath2 = "data/kickstarter-projects/" $ df2 = pd.read_csv(filepath2 + "ks-projects-201801.csv", encoding = 'ISO-8859-1', low_memory=False)
df.groupby('state')['ID'].count()
data.describe()
def clean(text): $     text=text.lower() $     text=re.sub(r'[^a-zA-Z]'," ",text) $     text="".join([i for i in text if i not in punch]) $     return text.split()
df2.converted.mean() $ print("Probability of individual converting:", df2.converted.mean())
autos['price'].value_counts().head(10)
data_filename = 'Red_Light_Camera_Violations.csv' $ data = pd.read_csv(data_filename, parse_dates=True)
start = dt.datetime(1982,1,1) $ end = dt.date.today() $ compTicker = 'AAPL' $ dataFrame = readData(start,end,compTicker) $ dataFrame.head() $
dfFull.GarageArea = dfFull.GarageArea.fillna(dfFull.GarageArea.mean())
(~autos["price"].between(1,350000)).sum() / autos.shape[0]
df1.info() $
Train.head()
db = sqlite3.connect('data.db') $ df_tweets = pd.read_sql_query("SELECT * FROM clean", db) $ df_mentions = pd.read_sql_query("SELECT * FROM mentions", db) $ df_hashtags = pd.read_sql_query("SELECT * FROM hashtags", db) $ df_tweets
df_links['link.domain'].value_counts().head(25)
df_protest.loc[0, 'start_date'].weekday()
commas = xmlData.loc[89, 'address'].count(',') $ print commas
merged1.index
if cl_ca.loc[(cl_ca.duplicated('APP_APPLICATION_ID')==True)].shape[0]==0: $     ca_de_xml = ca_de.copy() $     ca_de_xml = ca_de_xml.merge(cl_ca, how='left', on='APP_APPLICATION_ID') $ else: $     print("Duplicated application id in XML data")
all_tables_df.iloc[2:4, 1:]
accepted = train[train['project_is_approved'] == 1] $ for i in check_cols: $     print(accepted[i].value_counts()) $     print('')
df2['intercept'] = 1 $ df2[['ab_page_reverse', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop('ab_page_reverse', axis =1, inplace=True);
Guinea_deaths_Mean = pd.DataFrame({'August':August_deaths_Guinea, $                      'September':September_deaths_Guinea, $                      'October':October_deaths_Guinea }, index=['Guinea_deaths_Mean']) $ Guinea_deaths_Mean
autos["offer_type"].value_counts()
c_pd.join(j_pd,'party_id_orig')['party_id_orig','max(aggregated_prediction)','predicted']
new_page_converted=np.random.choice([1,0],size=n_new,p=[p_new,(1-p_new)]) $ new_page_converted.mean()
resultJson = result.to_json(orient='records')
print("Probability of control group converting:", df2[df2['group']=='control']['converted'].mean())
len(nt.catfathername.unique())
h5 = pd.HDFStore('/root/pyalgocourse/final_project/data.h5','r')
mgxs_lib.domain_type = 'cell' $ mgxs_lib.domains = geometry.get_all_material_cells().values()
df_links['link.domain'].value_counts().head(15)
overall_topics.to_csv("web/Analysis/overall_topics.csv", index=False)
tweet_archive_clean = pd.merge(tweet_archive_clean, images, on = 'tweet_id', how = 'inner')
new_converted_simulation = np.random.binomial(n_new, p_new, 10000)/float(n_new) $ old_page_simulation = np.random.binomial(n_old, p_old, 10000)/float(n_old) $ p_diffs = new_converted_simulation - old_page_simulation $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative="smaller") $ print(f"z-score: {z_score}") $ print(f"p-value: {p_value}")
log_regression = sm.Logit(combined_df['converted'], combined_df[['UK', 'CA']]) $ output = log_regression.fit() $ output.summary()
summary['leverage_ratio'] = summary.bookmaker_odds / summary.predicted_odds
logit_countries2 = sm.Logit(df3['converted'], $                            df3[['ab_page', 'country_UK', 'country_US', 'intercept']]) $ result2 = logit_countries2.fit()
df_sig_features = pd.read_csv('./data/significant_features.csv')
merged_df_flight_cancels.to_csv("../clean_data/FinalData_for_Models.csv")
reddit_info.to_csv('reddit_data_2.csv', encoding = 'utf-8', index = False)
station_count = session.query(Measurement.station).distinct().count() $ print('There are',station_count,'stations')
house_data.corr()
df.loc[df['last_name'] == 'Copening', 'dob'] = df.loc[df['last_name'] == 'Copening', 'dob'].apply(lambda x: get_year_month(pd.to_datetime('1963-01'))) $ df.loc[df['last_name'] == 'Copening']
df['harvey160'] = df.text.apply(lambda text: pd.Series([x in text for x in HARVEY_WORDS_160]).any()) $ df['harvey92']  = df.text.apply(lambda text: pd.Series([x in text for x in HARVEY_WORDS_92]).any())
lm(Combined_df.Ad_Cost, Combined_df.Total_Sales)
from google.colab import files $ !cp gts*.csv drive/NBA_Data_Hackathon
wd = 1e-7 $ wd = 0 $ learn.load_encoder('lm5_enc')
convo4 = companyNeg[companyNeg.group_id == 'group_100179' ]
temp_df = pd.DataFrame(year_temp) $ index_temp_df = temp_df.set_index('date') $ index_temp_df.head()
retweets = pd.read_csv('LaManada_new/tblretweets.csv',sep=SEP) $ retweets.shape
tweets_df.info()
scores['test_accuracy'].mean() $
from fastai.fastai.structured import * $ from fastai.fastai.column_data import *
import pandas as pd $ flight_pd=pd.read_csv("/home/ubuntu/parquet/flight_pd.csv", delimiter='\t')
precip_df = pd.DataFrame(year_precip) $ index_date_df = precip_df.set_index('date') $ index_date_df.head()
newcols = np.array(list(set(df.columns).union(zipcodesdetail.columns)))
pd.read_html(table.get_attribute('innerHTML').strip(), header=0)[2]
autos_real['registration_year'].value_counts(normalize=True)
lyra_lightcurve_fullday = lc.LYRALightCurve.create('2011-06-07') $ lyra_lightcurve = lyra_lightcurve_fullday.truncate('2011-06-07 06:00', '2011-06-07 08:00')
n_control = df2.query('group == "control"').shape[0] $ n_control
df.shape
clients.merge(bankID, on='BankID')
(trainingData, testData) = data.randomSplit([0.7, 0.3]) $
knn.fit(X_train_scaled, y_train) $ y_pred_class = knn.predict(X_test_scaled) $ print metrics.accuracy_score(y_test, y_pred_class) # This is the accuracy
df2 = df.copy() $ df2['E'] = ['one', 'one','two', 'three','three','four', 'four'] $ df2
celtics['season'] = (pd.DatetimeIndex(celtics.date) - np.timedelta64(9,'M')).year
tweet_df['created_at'] = pd.to_datetime(tweet_df['created_at']) $ tweet_df['date'] = tweet_df['created_at'].dt.date $ tweet_df['time'] = tweet_df['created_at'].dt.hour $ tweet_df.drop('created_at', axis=1, inplace=True)
print(len(complaints2016)) $ complaints2016.head()
to_be_predicted_Day3 = 17.78535457 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
example1_df = spark.read.json("./world_bank.json.gz")
INC.dtypes $
df = pd.read_csv("song_count.csv",index_col=0) $ plt.bar(range(len(df.Count)),df.Count,align="center") $ plt.xticks(range(len(df.Count)),df.index,rotation=70) $ plt.figure(figsize=(20,10)) $
px = px.asfreq('B').fillna(method='pad') $ rets = px.pct_change() $ cumulative_returns = ((1 + rets).cumprod() - 1)
import nltk $ from nltk.util import ngrams $ from pylab import * $ from nltk.corpus import stopwords $ import re
print('Examples of tweets (with only text and hashtag column):') $ twh[['text', 'hashtag']].head(3)
print(df2.user_id.duplicated())
twitter_merged_data.hist(column='rating'); $ plt.title('Rating Histogram') $ plt.xlabel('Rating') $ plt.ylabel('Count');
dict(list(r_ord.items())[0:3])
plot_confusion_matrix(cm_knn, classes=['COLLECTION', 'PAIDOFF'],normalize=False, title="Confusion matrix for knn", cmap=plt.cm.Blues)
df1 = pd.read_csv('E:/Keylogs/Typing_Participants/Participant_1_20171207/acc_right/ar2.csv',header=None) $ df2 = pd.read_csv('E:/Keylogs/Typing_Participants/Participant_1_20171207/acc_right/ar3.csv',header=None) $ df3 = pd.concat([df1,df2])
cnxn = pyodbc.connect(conn_str)
nbScore = MNB.score(x_test, y_test)#TODO $ print ("Model Accuracy: %f" % nbScore)
questions = questions.reset_index(drop=True)
tc1 = tc[~tc['ISBN RegEx'].isin(bad_tc_isbns)] $ tc1['ISBN RegEx'].size
res.summary2()
aqmdata = pd.read_csv('data-new.csv') $ aqmdata.head()
dta.violations.unique().shape
sns.distplot((y_test-predictions),bins=50);
pd.Timestamp('2017-01-10 8:05AM')
cityID = 'e67427d9b4126602' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Madison.append(tweet) 
!wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/002/079/225/GCA_002079225.1_ASM207922v1/GCA_002079225.1_ASM207922v1_feature_table.txt.gz
df_urls = pd.read_csv('ny_times_url_dataframe.csv', encoding = 'utf-8') $ df_urls.shape $
autos["price"].value_counts().sort_index(ascending=False)
train, test = data.randomSplit([0.8,0.2], seed=6) $ train.cache() $ test.cache()
grad_GPA_mean = records3[records3['Graduated'] == 'Yes']['GPA'].mean() $ grad_GPA_mean
user.info()
df_goog.index = df_goog.index.to_datetime() $ df_goog.index.dtype
df2.query("landing_page == 'new_page'").count()[0]/df2.shape[0]
yhat_knn = neigh.predict(X_test_knn) $ yhat_knn[0:5]
df.columns=['name','gender','age'] $ df
wordsX = [] $ for i in X: $     words = i.split() $     wordsX.extend([word for word in words if word not in stop ])
type(students['weight'].values)
image = soup.find("img", class_="thumb")["src"] $ featured_image_url = "https://www.jpl.nasa.gov" + image $ print(featured_image_url)
start_date = '2014-10-20' $ end_date = (now + datetime.timedelta(days=180)).strftime('%Y-%m-%d') $ model_dates = [today, '2017-01-01', '2016-01-01']
df.shape
cityID = 'dc62519fda13b4ec' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tampa.append(tweet) 
df_archive_clean.info()
cityID = 'b004be67b9fd6d8f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Norfolk.append(tweet) 
le_indicators = wb.search("life expectancy") $ le_indicators.iloc[:3,:2]
print (df.loc[100,'year1'])
df_full = pd.DataFrame() $ df_list = []
df["screen_name"].value_counts().head()
breaches.set_index('BreachDate').resample('3M').sum().PwnCount
a = df4.groupby('placeId').count()
p_new = df2['converted'].mean() $ print("The converted rate for P(new) = ", p_new)
newdf.head()
actuals = pd.DataFrame(y_test.data.numpy(), columns=['tavg_norm']) $ actuals['series'] = 'actual'
groceries.drop('apples')
test_df.info()
wellTrans = pd.merge(pumpT, scrnInt, on='WIN', how='left') $ wellTrans.dropna(subset=['trans'],inplace=True) $ wellTrans = wellTrans[(wellTrans.Drawdown_ft > 0)]
knnmodel.fit(XX_train, yy_train) $ knnmodel_predictionsX = knnmodel.predict(XX_test) $ num_of_knnmodel_predX = collections.Counter(knnmodel_predictionsX) $ num_of_knnmodel_predX
cust_demo.size
df2[df2.user_id.duplicated()]
df[df['id']==41000425].to_csv('BikeID41000425.csv')
top_supporters.head(10)
index = (geo['cityOrState'].isin(places + states + states_full) | geo['country'].isin(places + states + states_full))
print "word    count" $ print "----------------------------\n" $ !hdfs dfs -cat /user/koza/hw3/3.2/issues/wordCount_part3/* | head -n 10
df.source.value_counts()
response.headers["content-type"]
df_low_temps.describe() $
control_group = df2.query('group == "control"') $ converted_control_group = converted_group.query('group == "control"') $ print(converted_control_group.user_id.nunique() / control_group.user_id.nunique())
import graphlab $ import matplotlib.pyplot as plt $ %matplotlib inline
for num, sentence in enumerate(parsed_review.sents): $     print 'Sentence {}:'.format(num + 1) $     print sentence $     print ''
df.index
US_cases[US_cases.Merkmal_typ=='hauptmerkmal'].Merkmalcode.unique()
bus["postal_code"].value_counts(dropna=False).sort_values(ascending = False).head(15)
flightv1_1.select('trip').distinct().show()
kickstarter["state"].unique()
results_1 = pd.DataFrame(list(results1), columns=['tags', 'values'])#, index=[1,2,3,4]) $ results_1.head(20)
total_new_page = (df2['landing_page'] == 'new_page').sum() $ print(total_new_page / total)
df_summary['year'] = df_summary.index.year # e.g. 2013, 2014 $ df_summary['month'] = pd.to_datetime(df_summary.index.month, format='%m') # e.g. 1900-01-01, 1900-02-01 $ df_summary['str_date'] = df_summary.index.strftime('%b-%Y') # e.g. Jan-2013, Feb-2013 $ df_summary['month_name'] = df_summary.index.strftime('%b') # e.g. Jan, Feb $ df_summary.head()
import subprocess $ output  = subprocess.call("ls",shell=True) $ output  = subprocess.check_output("ls",shell=True) $ print(output.decode('UTF-8'))
e = driver.find_element_by_id('date_arrival')
df_archive_clean.sample(5)
FREEVIEW.plot_fixation_durations(raw_freeview_df, option='facet_subjects')
"23l4uyv2.35v324".isalnum()
df.drop(df.query("group == 'treatment' & landing_page == 'old_page'").index, inplace=True) $ df.drop(df.query("group == 'control' & landing_page == 'new_page'").index, inplace=True) $ df2 = df
data = pd.Series([0.25, 0.5, 0.75, 1.0], $                  index=[2, 5, 3, 7]) $ data
tweet_data =  pd.merge(tweet_archive_clean, tweet_image_clean, how= 'inner', on= 'tweet_id') $ tweet_data =  pd.merge(tweet_data, tweet_df_clean, how= 'inner', on= 'tweet_id') $ tweet_data.head(2)
df2.groupby('landing_page').count()/df2.count()
autos['price'].value_counts().head(20)
to_be_predicted_Day2 = 22.39137095 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
f = open('..\\Output\\GenreString.csv','w') $ f.write(GenresString) #Give your csv text here. $ f.close()
logit_mod = sm.Logit(df_total.converted,df_total[['intercept','US_new','UK_new']]) $ results = logit_mod.fit() $ results.summary()
pivoted_data.plot(figsize=(10,10))
dat['NY.GDP.PCAP.KD'].groupby(level=0).mean()
from sklearn.feature_selection import SelectFromModel
a = a.reshape(4, 5) $ a
cols_filt=building_pa_prc.columns[filt_cols] $ cols_filt
with open('./data/processed/X_train.pkl', 'rb') as picklefile: $     X_train = pickle.load(picklefile)
libraries_query.tostring()[:1000]
observCt_results = session.query(func.count(Measurements.date)) $ observCt_results
dr_existing_patient_8_to_16wk_arima = dr_existing_patient_data_plus_forecast['2018-06-25':'2018-08-26'][['Predicted Number of Patients']] $ dr_existing_patient_8_to_16wk_arima.index = dr_existing_patient_8_to_16wk_arima.index.date
keep_RNPA_cols = ['follow up Telepsyche', 'Follow up', 'follow', 'Returning Patient'] $ RNPA_existing = RNPA_existing[RNPA_existing['ReasonForVisitName'].isin(keep_RNPA_cols)] $ pd.value_counts(RNPA_existing['ReasonForVisitName'])
from collections import Counter $ from imblearn.over_sampling import SMOTE $ X_resampled, y_resampled = SMOTE().fit_sample(X, y) $ print(sorted(Counter(y_resampled).items()))
with open('image-predictions.tsv', 'w', encoding='utf-8') as file: $     file.write(str(r.text))
sheets = {last_word(sheet_name): data_file.parse(sheet_name) $           for sheet_name in cam_sheet_names}
retweeted_columns = ['retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp'] $ twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['retweeted_status_id'].isnull()] $ twitter_archive_clean = twitter_archive_clean.drop(columns=retweeted_columns)
rfc_bet_under = [x[1] > .54 for x in pred_probas_rfc_under]
train.shape
df_titanic = pd.read_excel(DATA_FOLDER + "/titanic.xls", converters={'pclass': np.int, 'survived': np.int, 'age': np.float}) $ print(df_titanic.head())
to_be_predicted_Day1 = 82.55 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new
click_condition_meta.os_timezone.unique()
run txt2pdf.py -o '2018-06-22  2012 872 discharges.pdf'  '2018-06-22  2012 872 discharges.txt'
bills_votes['congress'].value_counts()
grouped_grades = final_grades.groupby("hobby") $ grouped_grades
df_subset['Initial Cost'].head()
ratings=pd.read_csv('..\\Data\\ml-20m\\ml-20m\\ratings.csv')
for i in range(len(df.dtypes)): $     print (df.dtypes.index[i], df.dtypes[i])
bp.rename(columns={"value1num":"systolic","value2num":"diastolic"}, inplace=True)#inplace: overwrite new label in old label
np.savez('my_filename2', a=a, b=b) # The second variable name is the $
cols=["brand", "price", "odometer_km", "registration_year"] $ brand_stats = autos[cols].groupby("brand").mean() $ brand_stats.sort_values(by=["price", "odometer_km"],ascending=False) $
xml_in_sample1.shape
Historical_Raw_Data.set_index("Date_Monday")["2017"].groupby([pd.TimeGrouper('M'),"Product_Motor"]).sum().unstack(level = 0)
users_visits = users_visits.assign(user=1) $ users_visits = pd.merge(users_visits, Relations, how='outer', on=['name', 'id_partner']) $ users_visits = users_visits[['visits', 'user', 'chanel']] $ users_visits.head()
gr3_success = pd.merge(user_group3,success_order, how='inner', left_on=['CUSTOMER_ID'], right_on =['CUSTOMER_ID']) $ len(gr3_success)
print(cc['ranknow'].describe())
new_df = pd.DataFrame(x_dict)
month.to_csv('../data/month.csv')
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'treatment']])
np.exp(0.0506)
a_sevenPointSeven = df['a'] == 7.7
r.groupby(level=0).sum()['pf_ret']
df = df.set_index('date') $ month_plot = df.resample('d', how=sum).plot() $ plt.show()
ffrM = ffrM_resample.first() $ df_info(ffrM)
recipes.description.str.contains('[Bb]reakfast').sum()
sales_change = pd.DataFrame(over_time_2015['Sale (Dollars)']).reset_index() $ sales_change1 = sales_change[sales_change['month'] == 1] $ sales_change2 = sales_change[sales_change['month']==12] $
rng2 = pd.date_range(start='4/7/2018', end='4/8/2018', freq='3H') $ rng2 $
df2.query('landing_page == "new_page"').user_id.count()/df2.user_id.count()
old_page_converted = np.random.binomial(n, p_old, n_old) $ old_page_converted
control_df=df2.query('group=="control"') $ control_df.head()
import pandas_datareader.data as dr $ from pandas_datareader import data $ import pandas as pd $ import datetime $
bigram_model = prep.get_bigram_model(from_scratch=False):
data.head()
likes['date'] = likes['timestamp'].apply( lambda x : dt.datetime.fromtimestamp(x))
data[data.title == 'Collaborative Filtering and Embeddings']
df['youtube_url'] = df.apply(lambda x: youtube_urls[str((x['title'], x['artist']))], axis=1) $
borough_complaints = question_3_dataframe.\ $     groupby('borough')['created_date'].\ $     count() $ borough_complaints
cust_demo.Location.value_counts().plot(kind='barh', color='R', alpha=0.5)
a = np.array([[1., 2.], [3., 4.]]) $ a[0, 0] #Element [row, column]. Equivalent to a[0][0].
for k in range(3,15): $     knn = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train) $     yhat = knn.predict(X_test) $     print("k = " + str(k) + " -> f1 score = " + str(f1_score(y_test, yhat, average = "weighted")))
filter_df.shape
print('Logistic Regression Accuracy:', nltk.classify.accuracy(logistic, test_set) * 100, '%')
lm=sm.OLS(df2_by_day['new_rate'], df2_by_day[['intercept', 'day']]) $ results=lm.fit() $ results.summary()
mean_mileage_by_brand = {} $ for b in sel_brand: $     mean_mileage = autos.loc[autos["brand"] == b, "odometer_km"].mean() $     mean_mileage_by_brand[b] = int(mean_mileage) $ mean_mileage_by_brand
deck = pd.DataFrame(titanic3['cabin'].dropna().str[0]) $ deck.columns = ['deck']  # Get just the deck column $ sns.factorplot('deck', data=deck, kind='count')
users_usage_summaries = pd.pivot_table(df_usage[['id', 'feature_name']], index=['id'], columns=['feature_name'], aggfunc=len, fill_value=0) $ accepted_rate = df_usage.groupby(['id'])['accepted'].mean().to_frame() $ churned = joined_df.groupby(['id'])['churned'].mean().to_frame() $ users_usage_summaries = users_usage_summaries.join(accepted_rate, how='left').join(churned, how='left') $ users_usage_summaries.head(10) $
print(r.json()['dataset']['data'][0])
rfc = RandomForestClassifier(n_estimators= 600, max_depth = 8) $ rfc.fit(X, y) $ y_score_rfc = rfc.predict_proba(X)[:,1] $ metrics.roc_auc_score(y, y_score_rfc)
print(df5['new_id'].value_counts())
score_100.shape[0] / score.shape[0]
DF = sqlContext.createDataFrame(records,['name_pair','count']).withColumn("word1", F.col('name_pair'))#.withColumn("x4", lit(1))
sorted(problem_combos.map(lambda c: (c[0], 1)).countByKey().iteritems(), key=lambda x: x[1], reverse=True)
temp_long_df.head()
move_34p14u14u = (breakfastlunchdinner.iloc[3, 1] $                + breakfastlunchdinner.iloc[5, 2] $                + breakfastlunchdinner.iloc[5, 3]) * 0.002 $ move_34p14u14u
tickets_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/tickets.csv')['Body'].read().decode('utf-8') $ tickets = pd.read_csv(StringIO(tickets_csv_string), header=0, delimiter='|') $ tickets['meal_created_date'] = pd.to_datetime(tickets['meal_created_date']) $ tickets['meal_date'] = pd.to_datetime(tickets['meal_date'])
 df2.query('landing_page == "new_page"').user_id.nunique() / df2.user_id.nunique()
dates_by_tweet_count['Volume'].corr(dates_by_tweet_count['Percent_change'])
decoder_model_loaded = load_model('decoder_model_inference.h5')
ControlGroup = df2.query('group == "control"')['user_id'].count() $ ControlGroupConverted = df2.query('converted == 1 and group=="control"')['user_id'].count() $ print("Probability of Control Group Converted: ",(ControlGroupConverted/ControlGroup)) $ ctrl_conv_rt = (ControlGroupConverted/ControlGroup)
tweet_list = api.search(q='#%23mallorca')
d = docx.Document(downloadIfNeeded(example_docx, example_docx_save, mode = 'rb')) $ for paragraph in d.paragraphs[:7]: $     print(paragraph.text)
hk = pd.read_csv(path+"datas/28hse_HongKong.csv")
tmp = pd.Series.from_array(y_test)
techmeme['sources'] = techmeme.extra_sources.copy() $ for i, list_ in enumerate(techmeme.sources): $     list_.append(techmeme.original_source[i])
_ = df_trips.trip_requested.plot(kind="hist", bins=11)
df_new[['CA','UK','US']]= pd.get_dummies(df_new['country']) #create dummy variables for three countries
k = 10 $ top_k_features_churned = joined_df[joined_df['churned']==True]['feature_name'].value_counts().index[:k] $ top_k_features_not_churned = joined_df[joined_df['churned']==False]['feature_name'].value_counts().index[:k]
my_gempro.get_sequence_properties()
df, y, nas, mapper = proc_df(joined_samp, 'priceClose', do_scale=True) $ yl = np.log(y)
aux = image_clean.merge(dog_rates.loc[dog_rates.cuteness == 'doggo,puppo'], $                                                            how='inner', on='tweet_id') $ aux[['jpg_url','grade']].sort_values(['grade'], ascending=False).iloc[:,0].values[:3]
weather_mean.iloc[[4, 2], 3:7]
forecast = pd.DataFrame(forecast,index = future_dates_df.index,columns=['Forecast'])
user.query("screen_name == 'Trump'")
gifs = [".gifv", ".gif", "gfycat"] $ images = ["i.redd", "https://imgur.com"]
msft = pd.read_csv("../../data/msft.csv", dtype={'Volume': np.float64}) $ msft.dtypes
sales_vs_targets = pd.merge(sales, targets) $ print(sales_vs_targets)
index_missin_hr0to6_before2016.shape
run txt2pdf.py -o"2018-06-19 2015 OROVILLE HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2015 OROVILLE HOSPITAL Sorted by discharges.txt"
hist_cats['DaysInShelter'].hist(bins=10,color='c') $ plt.axvline(hist_cats.DaysInShelter.mean(), color='b', linestyle='dashed', linewidth=2)
lower = re.compile(r'^([a-z]|_)*$') $ lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_|:)*$') $ problemchars = re.compile(r'[=\+/&<>;\'"\?%#$@\,\. \t\r\n]') $ keys = {"lower": 0, "lower_colon": 0, "problemchars": 0, "other": 0} $ def countKeyType(element): $
bool(np.nan)
bnbA['date_first_booking'] = bnbA.date_first_booking.fillna(0)
segmentData = segmentData[segmentData.opportunity_stage != 'Closed - Duplicate Record']
df = pd.DataFrame(data, columns=["phenomenonTime", "name", "result"]) $ df.index = pd.to_datetime(df["phenomenonTime"]) $ df = df.drop("phenomenonTime", axis=1) $ print(df.shape) $ df.head()
df_main.to_csv('twitter_archive_master.csv')
data['c_employment_job_start'].min()
csvname = "Examplepath.csv" $ df.to_csv(csvname,index=False)
df_2002.dropna(inplace=True) $ df_2002
!hdfs dfs -put text.3.2.txt {HDFS_DIR}/3.2/input/text.3.2.txt
inputstrings =  ['Python2.7', 'Cpython', 'Python3.4', 'Perbl5.0', 'Lua', 'Python3.6', 'Powershell'] $ outputstrings = ["Hello, " + item for item in inputstrings if 'python' in item.lower()] $ outputstrings
stations = pd.read_csv('../hawaii_stations.csv') $ measure = pd.read_csv('../hawaii_measurements.csv', parse_dates=['date'])
store_items.pop('new watches')
conn.fileinfo('data')
google_stock.corr()
with open("Twitter_Keys.json", "r") as file: $     keys = json.load(file) $ API_KEY = keys["API_KEY"] $ API_SECRET = keys["API_SECRET"]
names = [name.replace(' ', '_').lower() for name in r_json['dataset']['column_names']] $ Data = namedtuple('data', names) $ data = [Data._make(observation) for observation in r_json['dataset']['data']] $ data = sorted(data, key=lambda observation: observation.date)
r = q_multi.results() $ r
def internship(x): $     if 'Intern' in x: $         return 1 $     return 0 $ df_more['Intern'] = df_more['Title'].apply(internship)
df_tweet_clean=df_tweet.copy() $
elms_all_0604.shape, elms_all_0611.shape
datatest.loc[datatest.expenses.str.isdigit() == False,'expenses'] = np.NaN
df = df.drop('Unnamed: 0', axis = 1)
df_pivot2['id_ndaj1'] = df_pivot2.index $ df_pivot2.head()
median = df_more['Average'].median() $ print ('The median salary for our data set is $' + str(median))
stock_id_ls = [] $ stock_name_ls = [] $ for id in stock_ids: $     stock_id_ls.append(str(id).strip('"<>/a').split('=')[-1].split('">')[0]) $     stock_name_ls.append(str(id).strip('"<>/a').split('=')[-1].split('">')[1])
print(categoricosx.max()) $ print(categoricosx.min()) $ print(categoricosx.mean())    $ print(categoricosx.median()) $
result_control.summary()
pt_weekly.drop(pt_weekly.index[[3, 5, 8]], inplace=True) $ pt_weekly.head(7)
autos.loc[autos["registration_year"]<1900, "registration_year" ]
t1 = datetime.datetime(2018, 1, 21) $ t2 = datetime.datetime(2017, 11, 15)
import numpy as np $ a = np.random.randn(100,100) $ print(a.shape) # matrix of 100 by 100 $ print(a) $ %timeit np.dot(a,a)
autos['no_of_pictures'].value_counts()
df.zone.unique()
cities = cities * 0.5 $ print(cities)
scores  = movies[['grossRank', 'scoreRank']]#creating an array of our gross rank and gross score $ movies['HarMean'] = scores.apply(stats.hmean, axis=1)#loading in the array and apply the hmean method to the data creating a harmonic mean.
list_of_values = [ $     '1 series bmw', $     'ford deals' $ ] $ df_filtered = df[df["Keyword"].isin(list_of_values)]
selected_features=selected.index $ X_train_new=X_train[selected_features] $ X_test_new=X_test[selected_features]
exchanges = get_exchanges_list() $ EXCHANGE_DB = pd.DataFrame.from_dict(e, orient='index') $ print(EXCHANGE_DB.head())
wkrng = pd.date_range('2012-10-25', periods=3, freq='W') $ wkrng
def count_description(df): $     words=[] $     for tweet in df['text']: $      words.append(text_process(tweet)) $     return  words $
t.hour
daily_sales=pd.merge(daily_sales,oil,on=['date'],how='left') $ print("Rows and columns:",daily_sales.shape) $ pd.DataFrame.head(daily_sales)
print str(loadedModelArtifact.name)
conv_all = (df2['converted'] == 1).sum() / len(df2.index) $ print(conv_all)
for column in trip_data_sub.columns: $     print column, ":", trip_data_sub[column].dtype
index_ts.head()
airports_df[airports_df['city'].str.lower() =='new york']
git_project_data_df.schema
profit_calculator(stock.iloc[1640:], 'predict_grow', -1)
plt.figure() $ fitA = SMOOTH.plot_single_trial(etsamples,etmsgs,None,'VP3','el',10,4,smooth_stanmodel) # trial 1, block 1 $ fitB = SMOOTH.plot_single_trial(etsamples,etmsgs,None,'VP3','pl',10,4,smooth_stanmodel) # trial 1, block 1
fullDf[fullDf.country=='NP'].levelIndices.value_counts()
df.duplicated().sum()
EmEnc_model_results_gs_df[EmEnc_model_results_gs_df.dataset=='lemmatized']
opt_fn = partial(optim.Adam, betas=(0.7, 0.99))
last_date = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ print(last_date) $
filter_iphone(orig_tweets)[filter_iphone(orig_tweets)['text'].str.contains("\?")]
op_after = op_after.loc[at_snapshot].reset_index() $ op_a2 = op_after.loc[op_after.groupby(['id_container', 'time_in'])['time_move'].idxmin()] $ op_a2.head()
new_page_converted=np.random.choice([1,0],p=(P_new,1-P_new),size=n_new) $ len(new_page_converted)
df_predictions = pd.read_csv('image-predictions.tsv', sep='\t')
df2.head(8)
df.isnull().sum()
mydata = mydata.fillna(method="bfill")
df = pd.DataFrame(data) $ df['keyword'] = searchQuery[1:] $ print(df.head()) $ df.to_csv('Tweets_ss_24.csv', index = False)
df1 = pd.DataFrame(np.random.randn(6,3),columns=["Col1","Col2","Col3"]) $ df1
df_result['diff_abs_logprob_final_tote_morning_line'] = abs(df_result['diff_logprob_final_tote_morning_line']/df_result['num_starters']) $ df_result['diff_sum_logprob_final_tote_morning_line'] = df_result.groupby('race_id')['diff_abs_logprob_final_tote_morning_line'].transform(lambda x:sum(x)) $ df_result.head()
cursor.execute(sq74) $ cursor.execute(sq75) $ results3 = cursor.fetchall() $ results3
plt.hist(null_vals)
plt.scatter(y_test,predictions)
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page']=='old_page')&(df2['converted']==1)]) $ convert_new = len(df2[(df2['landing_page']=='new_page')&(df2['converted']==1)]) $ n_old = len(df2.query('group == "control"')) $ n_new = len(df2.query('group == "treatment"'))
fwd.ix['gbp curncy','fwd curve']
df.loc[:,['A','C']]
df_ad_airings_5['state'].unique()
df.shape
df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page == "new_page")')['user_id'].count() $
glm_multi_v1.hit_ratio_table(valid=True)
y = df['custcat'].values $ y[0:5]
dftouse = flattened_df[~flattened_df['review_count'].map(np.isnan)] $ dftouse.shape
results = mod.fit()     $ results.summary()
print('Best Score: {}'.format(XGBClassifier.best_ntree_limit))
df_proj[df_proj.duplicated(subset=['FMSID'], keep=False)]
tweet_df_clean = tweet_df_clean[tweet_df_clean['retweeted_status'].isnull() == True]
from IPython.display import Image $ Image(filename='/Users/benjarman/Development/SparkIpython/data/original_schema.jpg')
my_gempro.prep_itasser_modeling('~/software/I-TASSER4.4', '~/software/ITLIB/', runtype='local', all_genes=False)
pd.merge(df1, df3)
tweet_archive_enhanced_clean['rating_numerator'] = tweet_archive_enhanced_clean['text'].str.extract('([0-9]+[.]*[0-9]*)\/(10)',expand = True)[0].astype('float')
pl.hist(yc_new3['tipPC'], bins=30) $ pl.ylabel('N') $ pl.xlabel('tipPC') $ pl.title('Distribution of Tips as a percentage of Fare, under 100%') $ print('The first moment is 4.67 and the second moment is 9.61')
main_tables = dict() $ for i in files_to_manage: $     print(download_file_from_dropbox(account, '{}_2017.csv'.format(i), folder="nba games/test_ws_workflow/")) $     main_tables[i] = pd.read_csv('{}_2017.csv'.format(i))
df_reg2['US_ind_ab_page'] = df_reg2['country_US']*df_reg2['ab_page'] $ df_reg2['UK_ind_ab_page'] = df_reg2['country_UK']*df_reg2['ab_page'] $ logit_co_2 = sm.Logit(df_reg2['converted'], df_reg2[['intercept', 'ab_page', 'country_US', 'country_UK', 'US_ind_ab_page', 'UK_ind_ab_page']]) $ result_co_2 = logit_co_2.fit()
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv")
bacteria = pd.Series([632, 1638, 569, 115], $     index=['Firmicutes', 'Proteobacteria', 'Actinobacteria', 'Bacteroidetes']) $ bacteria
np.random.seed(42) $ tf.set_random_seed(42)
print(well_data.loc[[3], ('gas_mcf')])
vid_list_return = list(rsp.json()['data'].keys())
dfL.boxplot(figsize=(1150,1150), rot=50 , return_type='axes');
pd.merge?
powerConsumptionsPerDay = powerConsumptions.groupby([pd.Grouper(freq='D', level=0), 'meter_id']).sum() $ powerConsumptionsPerDay
df1.info(), df2.info(), df3.info()
%matplotlib inline $ import matplotlib.pyplot as plt $ import numpy as np
test_features = ['goal','Cat-Nums','City-Nums','launched_atYM','Length_of_kick','Days_spent_making_campign','City_Pop','staff_pick'] $ X = df_select[test_features].copy() $ y = df_select[['status']].copy() $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)
(p_diffs - diff).mean()
tweetid_series = ttarc.tweet_id
appl["20d"] = np.round(appl["Close"].rolling(window = 20, center = False).mean(), 2) $ pandas_candlestick_ohlc(appl.loc['2016-01-19':'2017-01-19',:], otherseries = "20d")
top20 = autos['brand'].value_counts().head(20).index
from sklearn.ensemble import RandomForestClassifier $ from sklearn.grid_search import GridSearchCV $ from sklearn.datasets import make_classification
plt.hist(p_diffs);
future = m.make_future_dataframe(periods=90) $ future.tail()
permits_df.dropna(inplace=True)
SS = StandardScaler() $ train[num_features] = SS.fit_transform(train[num_features]) $ if (kaggle | sim): test[num_features] = SS.transform(test[num_features])
df.loc[ [100,103] ]  # by row labels
newdf.head()
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True).hist()
fNames = dfX.columns
[tweet.lang for tweet in tweet_list]
startups_USA = startups_USA.drop(['name','homepage_url', 'category_list', 'region', 'city', 'country_code'], 1) $ cols = list(startups_USA) $ cols.append(cols.pop(cols.index('status'))) $ startups_USA = startups_USA.ix[:, cols]
df_wm['Polarity'] = df_wm['cleaned_text'].apply(polarity_calc) $ df_wm['Subjectivity'] = df_wm['cleaned_text'].apply(subjectivity_calc)
teams_sorted_by_wins = df.sort_values(by=['wins', 'teamid'], ascending=False) $ teams_sorted_by_wins.head(10)
df_TempJams.info()
dbData.head(10)  # NaN's show up in the field with no data!  Want to drop these from the dataframe.
tweets_df.sample(3)
ab_data.user_id.nunique()
bacteria2.isnull()
twitter_archive_master = twitter_archive_master.drop(['rating_numerator','rating_denominator'],axis = 1)
df.groupby('user_gender')['user_gender'].apply(len)
to_be_predicted_Day3 = 21.29160417 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
prob_group = df2.query("group == 'control'")["converted"].mean() $ print("In the 'control' group the probability they converted is {}.".format(prob_group))
trip_data['start_city'].unique()
active_users.agg([np.mean])
vow.describe()
datapath = "../datasets/Kaggle/BOWPopcorn/" $ outputs = "../outputs/"
from functions import et_helper $ et_helper.plot_around_event(etsamples_grid,etmsgs_grid,etevents_grid,raw_large_grid_df.query("eyetracker=='pl'&subject=='VP4'&block==1").iloc[5],plusminus=(-2,5))
my_data.collect()
dti = pd.date_range('2014-08-29','2014-09-05',freq='B') $ dti.values
pd.read_pickle('data/city-util/proc/utility.pkl', compression='bz2').head()
df_t_not_n = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_not_t_n = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ mismatch= len(df_t_not_n) + len(df_not_t_n) $ mismatch_df = pd.concat([df_t_not_n, df_not_t_n]) $ mismatch
feedex.shape
trumpTweets=api.GetUserTimeline('25073877', include_rts=False, count=200) $ print(trumpTweets[0]) $ print(len(trumpTweets))
rsp.json()['data']
y_hat = model.predict(X_test)
pd.DataFrame(records3.describe().loc['mean', ['Age', 'GPA', 'Days_missed']])
train.shape, validation.shape
print(pd.DataFrame(train_matrix).head())
writer = pd.ExcelWriter('NaN_table.xlsx') $ for res_key, df in nan_sets.items(): $     df.to_excel(writer, res_key) $ writer.save()
siteGroup = nitrodata.groupby('MonitoringLocationIdentifier')['Year'] $ siteInfo = siteGroup.agg(['min','max']) $ siteInfo.columns = ['StartYear','EndYear'] $ siteInfo['Range'] = siteInfo['EndYear'] - siteInfo['StartYear'] $ siteInfo
parts_sets.head()
n_old = (df2.landing_page == 'old_page').sum() $ n_old
zp = zip(list(o_data.values()),list(o_data.values())[1:]) $ (mx, st_d, ed_d) = max([(abs(t.close-p.close), t.date, p.date) for t, p in zp]) $ print('The largest intraday price change was: {:.2f}'.format(mx)) $ print('Between trading days {} and {}'.format(st_d, ed_d))
Meter1.Errors
df_MC_least_Convs = pd.concat([year_month.transpose(), df_MC_leastConvs], axis=1) $ print 'DataFrame df_MC_least_Convs: ', df_MC_least_Convs.shape $ df_MC_least_Convs
np.log(0.00025/0.0028)
tw_creds = tw_consumer_key + ":" + tw_consumer_secret $ tw_creds_encoded = base64.b64encode(tw_creds.encode()).decode()
autos['price'].value_counts().sort_index()
knn_10.score(X_test, y_test)
from scipy.stats import norm $ print("The significance z-score (p-value) is: %.4f" %norm.cdf(z_score)) $ print("The critical value at 95%% confidence is: %.4f" %(norm.ppf(1-(0.05/2))))
LS_loans.loc[0,'APP_APPLICATION_ID']
df1=df.toPandas()
X.shape, y.shape
df.query('group == "treatment" & landing_page != "new_page"|group == "control" & landing_page != "old_page"').count()
ip_clean.tweet_id = ip_clean.tweet_id.astype(str)
sunspots = pd.read_csv(filepath,sep=';', $                        usecols=[0,1,2,3,4,7], $                        header = None) $ sunspots.iloc[10:20, :] $
tsne = TSNE(n_components=2, learning_rate=150, verbose=2).fit_transform(one_hot_domains_questionable)
train.columns
s = df_Providers['year_name'].value_counts() $ print('sum of (value_counts > 1): ',sum(s>1), '    (0 indicates that there are no dups)') $
print(d.headers)
aligned_results_1 = df[(df['group'] == 'treatment') & (df['landing_page'] == "new_page")] $ aligned_results_2 = df[(df['group'] != 'treatment') & (df['landing_page'] != "new_page")] $ df2 = pd.concat([aligned_results_1,aligned_results_2]) $ df2.head()
reverseAAPL = AAPL.sort_index(ascending=False) $ reverseAAPL.head()
segments.seg_length.hist(bins=100)
df2.head(3) $
stars.created_at = pd.to_datetime(stars.created_at) $ stars['starred'] = 2
df_con_control = df_con1.query('group =="control"') $ x_control = df_con_control["user_id"].count() $ x_control $
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ df_vow = pd.read_csv('../datasets/vow.csv')
weather.to_csv(os.path.join(path, 'data_out.csv'))
df3.head(5)
by_time.loc['Weekday']
df.D
random_non_crashes_df.shape
data_final.shape
from sklearn.dummy import DummyClassifier
train_embedding=graf_train['DETAILS3'].apply(lambda words: np.mean([model[w] for w in words if w in model], axis=0))
df = pd.read_sql_query(sql_query,con) $
data_l2_begin = tmpdf.index[tmpdf[tmpdf.isin(DATA_L2_HDR_KEYS)].notnull().any(axis=1)].tolist() $ data_l2_begin
userArtistDF.describe().show()
scores = cross_val_score(knn5, Xs, y, cv=skf) $ print 'Cross-validated KNN5 scores based on words:', scores $ print 'Mean score:', scores.mean()
df.to_csv("../../data/msft_piped.txt",sep='|') $ !head -n 5 ../../data/msft_piped.txt
loans = pd.read_csv('lending-club-loan-data/loan.csv') $ loans.head(2)
df = pd.read_sql('SELECT actor_id, first_name, last_name FROM actor WHERE first_name ilike \'Joe\'', con=conn) $ df
P_new=df2['converted'].mean() $ print("convert rate for P_new under the null is {}" .format(P_new))
giss_temp.index
modeldriver = 'EAM_Dynamo__MD_120291908751_004' $ print(kimanalysis.extendedid(modeldriver)) $ print(kimanalysis.shortid(modeldriver)) $ print(kimanalysis.shortcode(modeldriver))
weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend') $ by_time = data.groupby([weekend, data.index.time]).mean()
predictions = client.deployments.score(scoring_url, scoring_data) $ print("Scoring result: " + str(predictions))
to_be_predicted_Day5 = 21.30655088 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
txt.reset_index() $ txt['Date'] = pd.to_datetime(txt['Time'] , unit = 's').dt.date $
(df[df['Complaint Type'].str.contains('noise', case=False)] $  ['Descriptor'] $  .value_counts() $  .sort_values() $  .plot(kind='barh'))
purchase_history = pd.merge(orders_subset, $                             order_details_prior, $                             on=['order_id'])[['user_id', 'product_id']]
item.metadata
month.head(5)
tag_degrees = tweetnet.degree()
lgb_cv.fit(X_reduced.values,y_train.values.flatten())
testheadlines = test["text"].values $ basictest = basicvectorizer.transform(testheadlines) $ predictions = basicmodel.predict(basictest)
New_df = Type_date.groupby(['New_or_Returning']).get_group('New').reset_index() $ New_df.head()
result = pd.merge(df, ser, on = 'key').drop('key', axis = 1) $ result
proj_df.shape
adjClose_pyo = pd.DataFrame(data=df_jArr, columns=etfSymbols) $ corr = adjClose_pyo[:corr]()
_ = ok.grade('q05')
for c in ccc: $     vwg[c] /= vwg[c].max()
retweet_per_doggy.plot(kind='bar') $ plt.ylabel('Number of Retweets') $ plt.xlabel('Dog Type') $ plt.title('Average Retweets for Dog Type')
new_page_user = len(df.query("group =='treatment'")) $ users = df.shape[0] $ new_user_prob = new_page_user/users $ print('The probabiity an individual landed on a new page is {}'.format(round(new_user_prob, 4)))
tweets.shape
pickle.dump(df_final_,open("kicks_modeling_data.pkl",'wb')) $
def rest_to_json(row): $     pass $ rest = ... $ dict_rest = {k: v for d in rest for k, v in d.items()}
lq.columns = lq.columns.str.replace(' ','')
with open('dropbox/github/Thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_continent-90m.csv', 'rU') as inputFile: $     header = next(inputFile) #iterates to the next value) $     for line in inputFile: $         print(line)
from test_package.print_hello_class_container import Print_hello_class # Import the class $ print_hello_instance = Print_hello_class()                             # Instantiate the class $ print(print_hello_instance.name)                                       # Print a property of the instance $ print_hello_instance.print_hello_method_within_class()                 # Call a method of the instance 
import pandas as pd $ disaster_tables = pd.read_html("https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll", header = 0)
df.mean()
n_old = df2[df2.landing_page == 'old_page'].user_id.count() $ print(n_old)
ben_final.shape
%%R $ flightsDB$ARR_HOUR <- trunc(flightsDB$CRS_ARR_TIME/100) # Cuts off the minutes, essentially. $ flightsDB$DEP_HOUR <- trunc(flightsDB$CRS_DEP_TIME/100)
base_col = 't' $ df.rename(columns={target_column: base_col}, inplace=True)
df.head()
Numerator=df2.loc[(df2['group']=='treatment') & (df2['converted']==1),].shape[0] $ Denominator=df2.loc[df2['group']=='treatment',].shape[0] $ TreatmentConverted=Numerator/Denominator $ print("The probability is", Numerator/Denominator)
title_pp = processor(append_indicators=True, keep_n=4500, $                      padding_maxlen=12, padding ='post') $ train_title_vecs = title_pp.fit_transform(train_title_raw)
df2 = df.query('(landing_page == "new_page" & group == "treatment") | \ $                 (landing_page == "old_page" & group == "control")') $ df2.groupby(by=['landing_page', 'group']).count()
from nltk.corpus import stopwords $ clean_para = [word for word in no_punc.split() if word not in stopwords.words('english')] $ clean_para
url_base = 'https://ajax.googleapis.com/ajax/services/search/news' $ params = dict(q="Donald Trump", v=1.0, ned="us", rsz=8) $ rsp = requests.get(url_base, params=params) $ print rsp.url $ results=simplejson.loads(rsp.text)
list(legos.keys())
print('Example of the original tweets for the word "{}" that was detected as an event:'.format(new_events.iloc[0].hashtag)) $ dictionary[new_events.iloc[0].hashtag].head(10)
all_rows = engine.execute('SELECT * FROM station LIMIT 10').fetchall() $ print(all_rows)
total.load_from_statepoint(sp) $ absorption.load_from_statepoint(sp) $ scattering.load_from_statepoint(sp)
joblib.dump(sentiment_word_dic, "resources/sentiment_word_dic.dmp", compress=3)
df.Date = pd.to_datetime(df.Date) $ df.info() $ print('\n','>'*40) $ df.head()
s.str.isnumeric()
with open('topic_list_5_29.pkl', 'rb') as pikkle4: $     topic_list = pickle.load(pikkle4)
petropavlovsk_freq = petropavlovsk_filtered.genus.value_counts() / len(petropavlovsk_filtered) $ petropavlovsk_freq
df = pd.read_sql_query(prcp.statement, engine) $ df = df.set_index('date') $ df.groupby('date') $ df.head() $
age_median = train_users["age"].median() $ age_median
print('Average Daily trading volume during 2017 is :',np.median(dtunit))
len(df['user_id'].unique())
session.query( $     Actor.actor_id, Actor.first_name, Actor.last_name $ ).filter( $     Actor.last_name.like('%GEN%') $ ).frame()
tmax_day_2018.tmax[100].plot();
sample = df.sample(n=1000, replace=False)
print(autos.columns)
sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger')
new_page_converted = pd.DataFrame(np.random.choice([0,1],n_new,p=[1-CRnew,CRnew])) $
shift_entries.head(10)[['SE_ID', 'SE_LOCATION', 'SE_START', 'SE_TIMESTAMP','TimeElapsed', 'SE_SHIFTACTIVITY']]
df_wna.sort_values(by='n_faults')[['n_amb', 'n_mcu', 'n_temp'\ $                                   ]].median(axis=1).head(24).sort_values(ascending=False)
cr_treatment = df2[df2['group'] == 'treatment']['converted'].mean() $ cr_treatment $
members_info_to_acq = set(unique_member_ids).union(set(unique_speaker_id)) $ print(len(members_info_to_acq))
len(set(b_list['id'].unique()) - set(b_cal_q1['listing_id'].unique()))
USER_PLANS_df['start_date'] = pd.to_datetime(USER_PLANS_df['scns_created'].apply(lambda x:x[np.argmin(x)])).dt.strftime('%Y-%m')
(autos["date_crawled"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().head() $ )
speeches_cleaned.reset_index(inplace = True)
pd.concat([train_data['lifetime_revenue'], train_data.drop(['lifetime_revenue'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False) $ pd.concat([validation_data['lifetime_revenue'], validation_data.drop(['lifetime_revenue'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger') $ z_score,p_value
import qgrid $ qgrid.set_grid_option('forceFitColumns', False) $ qgrid.set_grid_option('editable', False)
mod = sm.tsa.statespace.SARIMAX(model_df.close, trend='n', order=(1,1,3), seasonal_order=(0,1,1,80)) $ results = mod.fit() $ results.summary()
autos["price_euro"].value_counts(normalize = True).sort_index(ascending = False).head(20)
f = e.instance_method $ e.instance_var = 'e\'s instance var' $ f()
tweets_df.language.value_counts() $
test = test.drop('teacher_id',axis=1)
df_new[['ab_page', 'ab_page_2']] = pd.get_dummies(df_new['landing_page']) $ df_new = df_new.drop(columns = ['ab_page_2'], axis = 1) $ lm = sm.Logit(df_new['converted'], df_new[['US', 'CA','ab_page', 'intercept']]) $ results = lm.fit() $ results.summary()
INT.head(1)
msft.loc['2012-01-01':'2012-01-05'] #slicing
%%time $ ent_count = defaultdict(int) $ for doc in nlp.pipe(texts, batch_size=100, disable=['parser','ner']):  $     matcher(doc) # match on your text $ print(ent_count)
print("no. of converted users = ",(df["converted"].mean()*100),'%')
tables = pd.read_html('https://en.wikipedia.org/wiki/List_of_accidents_and_disasters_by_death_toll', header=0)
model = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'UK', 'interaction_ab_CA', 'interaction_ab_UK']]).fit()
airline_df['sentiment'] = airline_df['tweet'].apply(lambda tweet: NBClassifier.classify(extract_features(getFeatureVector(processTweet2(tweet))))) $
%%writefile -a butler/trainer/model.py $ def dict_queries(): $     return query $ def en_queries(): $     return query
posts.count()
saved_model = ml_repository_client.models.save(model_artifact)
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_index()
news_df.sample(5)
URL = "https://raw.githubusercontent.com/feststelltaste/software-analytics/master/demos/dataset/git_demo_timestamp_linux.gz" $ git_log = pd.read_csv(URL, compression="gzip") $ git_log.head()
n_new = df2[df2['group'] == 'treatment'].shape[0] $ n_new
out = conn.addtable(table='banklist', caslib='casuser', $                     **htmldmh.args.addtable) $ out
df["date"] = df['created_at'].map(lambda x: datetime.date(x)) $ df["time"] = df['created_at'].map(lambda x: datetime.time(x)) $ df["hour"] = df['time'].map(lambda x: x.hour) $ df['day'] = df['date'].map(lambda x: x.day) $ df.head()
df_ad_state_metro_2.plot(x='state', y='ad_duration_secs', kind='bar') $ df_state_victory_margins.plot(x='state', y='Percent margin', kind='bar') $
obama_speech_save = 'obama_speech_8.html' $ with open(obama_speech_save, mode='w', encoding='utf-8') as f: $     f.write(obamaSpeechRequest.text)
resolved_links = ux.multithread_expand(urls_to_shorten,  $                                        n_workers=2, $                                        return_errors=False)
df_unique = df.user_id.nunique() $ print("There are {} unique users in the dataset.".format(df_unique))
primary_temp_null_indx=dat[primary_temp_column].isnull()
device = train_data.groupby(['device.browser',"device.operatingSystem","device.isMobile"]).agg({'totals.transactionRevenue': 'sum'}) $ device = device.groupby(level=[0]).apply(lambda x:100 * x / float(x.sum())) $ device #.reset_index()
df[ $     (df['datetime'] > datetime(2016, 1, 1)) & $     (df['datetime'] < datetime(2017, 1, 1)) $ ].set_index('datetime').resample('1w').aggregate('count').plot()
df = df.dropna(how='any',subset=['segment_id']) $ df['segment_id'] = df.segment_id.astype('int64') $ rdf['segment_id'] = rdf.segment_id.astype('int64') $ rdf['station_id'] = rdf.station_id.astype('int64') # Weather statiion ID
contract_histo.info(memory_usage='deep')
S_distributedTopmodel.decision_obj.bcLowrSoiH.options, S_distributedTopmodel.decision_obj.bcLowrSoiH.value
gdf.sample(10)
df = pd.DataFrame(d)  # convert data to a pandas data frame
import statsmodels.api as sm $ convert_old = df.query("landing_page == 'old_page'")['converted'].sum() $ convert_new = df.query("landing_page == 'new_page'")['converted'].sum() $ n_old = df.query("landing_page == 'old_page'")['user_id'].count() $ n_new = df.query("landing_page == 'new_page'")['user_id'].count() $
trump['num_char']=trump['num_char'].apply(lambda x: np.log(x))
((df['group'] == 'treatment') == (df['landing_page'] != 'new_page')).sum()
tweet_json[tweet_json.duplicated('id')]
tweets_streamedDF.dtypes
first_values = grouped_months.first() $ first_values=first_values.rename(columns = {'Totals':'First_v_T'}) $
list(data.columns.values)
data.info()
np.exp(fruits)
print(df_pat.shape) $ df_pat.head()
df.dtypes
shots_df = pickle.load(open('shots_df.pickle', 'rb'))
df_master[df_master.rating_numerator==1776].jpg_url
df.query("group == 'treatment' and landing_page != 'new_page'").shape[0] + df.query("group != 'treatment' and landing_page == 'new_page'").shape[0]
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key=' + API_KEY + '&start_date=2017-1-1&end_date=2017-12-31') $
merged_NNN.head()
twitter_archive_master = twitter_archive_master.drop(['in_reply_to_status_id','in_reply_to_user_id', $                                                      'retweeted_status_id','retweeted_status_user_id', $                                                     'retweeted_status_timestamp'],axis = 1)
dictionary = corpora.Dictionary(tweets_list)
data = r.json()
s3 = pd.Series(['A','B']) $ s4 = pd.Series(['C','D']) $ pd.DataFrame([s3,s4])
obs_diff=df2[(df2.group == 'treatment')].converted.mean() - df2[(df2.group == 'control')].converted.mean() $ (p_diffs > obs_diff).mean()
sns.regplot(x=reddit['Upvotes'], y=reddit['Comments'], $             line_kws={"color":"r","alpha":0.7,"lw":5}) 
train.question_score.corr(train.answer_score)
df.rename(columns={'Created Date': 'created_at'})
convers_con.time_difference.value_counts(dropna=False)
df.duration /= 60
import spacy $ nlp = spacy.load('en_core_web_md')
austin.loc[austin['miles'].argmax()] $
rf.fit(X, y) $ pp = rf.predict_proba(X)
titanic.isnull().any()
crimes.columns
df_onc_no_metac.head()
_ = ok.submit()
import pickle $ output = open('speeches_all_bill_data.pkl', 'wb') $ pickle.dump(speeches_all_bill_data, output, protocol=4) $ output.close()
ddf = dd.read_csv('test-data/output/sample-xls-case-multisheet.xlsx-*.csv') $ ddf.compute().head()
treatment = df2[df2['group'] == 'treatment'] $ size_treatment = treatment.shape[0] $ prop_conv_treatment = treatment[treatment['converted'] == 1].shape[0] / treatment.shape[0] $ prop_conv_treatment
token_sendReceiveAvg_month = pd.merge(token_sendavg,token_receiveavg,on="ID",how="outer")
type(archive_df_clean['timestamp']) $ archive_df_clean.head(10)
sl['two_measures'] = np.where((sl.mindate!=sl.maxdate),1,0)
def datetimemdyampm2datetime(text): $     try: $         return  pd.to_datetime(text,format='%m-%d-%Y %I:%M:%S %p') $     except AttributeError: $         return text 
data[['Sales']].resample('d').mean().rolling(window=15).mean().diff(1).sort_values(by='Sales').head() $
wb.search('cell').iloc[:5,:2]
df2['timestamp'] = pd.to_datetime(df2['timestamp'])
brand_counts = autos.brand.value_counts(normalize=True) $ brand_counts
pd.pivot_table(ins,index=['business_id'],columns=['date'],values='score',aggfunc=lambda x:len(np.unique(x)),fill_value=0.0)
converted_prob = (df2['converted'] == 1).sum() / df2.user_id.count()
df_pol_matrix_df = pd.DataFrame(df_pol_matrix.todense(), $                          columns=tvec.get_feature_names(), $                          index=df_pol_t.index)
col_analysis = pd.DataFrame $
utility_patents_df.number_of_claims.describe()
experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)
df[df.location_id>0].raw_location_text.value_counts().head()
calls.info()
order_drop = order.drop(Imputation_columns,axis=1)
final_topbikes.to_csv('final_top_bikes.csv')
data1_new.head()
AAPL_pred=model.predict()
mb = pd.read_csv("Data/microbiome.csv", index_col=['Patient','Taxon']) $ mb.head()
df['created_at'].head()
print 'Total negative amount is: ', df[df.amount < 0].amount.sum()
merged = pd.merge(prop, contribs, on="calaccess_committee_id")
jail_census.loc["2017-02-01"]
from pysumma.Plotting import Plotting $ from jupyterthemes import jtplot $ import matplotlib.pyplot as plt $ import pandas as pd $ jtplot.figsize(x=10, y=10)
df.filter(b_threeish & a_sevenPointSeven).show(5)
learn.save("dnn80")
dfSummary[4:9].plot(kind='bar', $                     figsize=(20,6), $                     title="Data Summaries: Quantiles");
df2.drop_duplicates('user_id')
token_sendReceiveAvg_month.level =  token_sendReceiveAvg_month.level.fillna("null_level")
twitter_data.rating_denominator.value_counts()
from sklearn import model_selection $ skf = model_selection.StratifiedKFold(n_splits=5) $ skf.get_n_splits(X, y) $ folds = [(tr,te) for (tr,te) in skf.split(X, y)]
df.all()  # checks if all the elements in each column are non-null
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $ api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())
cur.execute('INSERT INTO experiments (material_id) VALUES ("EVA")')  # use defaults, $ conn.commit()
! wget http://vrf.wpengine.netdna-cdn.com/wp-content/uploads/2014/03/sonkey13.jpeg $ !./darknet detect cfg/yolov3.cfg yolov3.weights sonkey13.jpeg
dat_hcad['blk_lower'] = dat_hcad['0'] - dat_hcad['0'] % 100 $ dat_hcad['blk_upper'] = dat_hcad['blk_lower'] + 99 $ dat_hcad['blk_range'] = dat_hcad['blk_lower'].map(str)+'-'+dat_hcad['blk_upper'].map(str)+' '+dat_hcad['COMMERCE'].map(str)
from sklearn.model_selection import GridSearchCV $ from sklearn.tree import DecisionTreeClassifier
df2.loc[df2.user_id.duplicated()] 
df.index.name = 'week_ending'
ggplot(raw_large_grid_df.query("subject=='VP4'"),aes(x="duration"))+geom_histogram(binwidth=0.1)+coord_cartesian(xlim=(0,3))+facet_grid("~eyetracker")
preds_train= aml.leader.predict(htrain) $ h2o_train=df_train[['air_store_id','visit_date']]
station_count = session.query(func.count(distinct(Measurement.station))).all() $ print ("There are" + " " + str(station_count[0][0]) + " " + "unique stations")
df[0].plot()
df_questionable[df_questionable['political'] == 1]['link.domain_resolved'].value_counts(25).head(25)
rate_change=df_master[['date', 'retweet_count', 'favorite_count', 'numerator', 'rating_denominator']].copy()
cityID = '44d207663001f00b' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Mesa.append(tweet) 
db_connection = ittconnection('prodcopy') $ query = (" SELECT * FROM signal_signal WHERE `signal` = 'ANN_simple' ") $ signals_df = pd.read_sql(query, con=db_connection) $ signals_df.tail(6)
df['Temperature']   = df['Temperature'].astype(float) $ df['Precipitation'] = df['Precipitation'].astype(float)
print 'A Series with minute-level intervals.' $ minute_interval = pd.date_range('2014-08-01', '2014-10-29 23:59:00', freq='T') $ bymin = pd.Series(np.arange(0, len(minute_interval)), minute_interval) $ bymin
params = result.params # get coefficients $ print(np.exp(params)) # odds ratio of the coefficients
df2.head()
van15_fin = van_dummy.groupby('userid').nth(1)
pd.read_sql_query("SELECT * FROM person, grades WHERE person.id = grades.person_id" , conn)
result = pd.merge(df_pivot2,df_pat, how='inner', on=['id_ndaj1']) $ result.shape
%%time $ grid.fit(X, y)
def trip_end_date(x): $     return re.findall(r'\d{4}-\d{2}-\d{2}', x)[-1]
dfCountry = pd.read_csv('countries.csv') $ dfCountry.head()
print('Null Values for Donations.csv: ', donations.isnull().sum()) $ print('Null Values for Donors.csv: ', donors.isnull().sum()) $
get_adjacency_matrix(1253).head()
images.head()
prediction.info()
eligible_by_week = {} $ for key in set([x['ama.week.diff'] for x in ama_guests.values()]): $     eligible_by_week[key] = [x['author'] for x in ama_guests.values() if $                              x['ama.week.diff'] == key]
!rm SIGHTINGS.csv -f $ !wget https://www.quandl.com/api/v3/datasets/NUFORC/SIGHTINGS.csv
table_1c.reset_index(drop=True)
precision = float(precision_score(cy, rf.predict(cX))) $ recall = float(recall_score(cy, rf.predict(cX))) $ print "Car searches model -" $ print("The precision is {:.1f}% and the recall is {:.1f}%.".format(precision * 100, recall * 100))
prop_ca = df_full.query('country == "CA"')['converted'].mean() $ prop_uk = df_full.query('country == "UK"')['converted'].mean() $ prop_us = df_full.query('country == "US"')['converted'].mean() $ print("Proportion of conversions\nCA: {:.5f}\nUK: {:.5f}\nUS: {:.5f}"\ $       .format(prop_ca, prop_uk, prop_us))
mean_old=df2[df2['converted']==1].count()[0]/df2.shape[0] $ mean_old
temp_df2['timestamp'].max() - temp_df2['timestamp'].min()
data[(data['author_flair'] == 'Bears') & (data['win_differential'] >= 0.9)].comment_body.head(15) $
from sklearn.metrics import confusion_matrix $ %matplotlib inline
for p in mp2013: $   print("{0} {1} {2} {3}".format(p, p.freq,  p.start_time, p.end_time))
autos['gearbox'].unique()
ts=pd.Series(data=np.arange(72),index=rng)
assert np.isclose(trump.loc[690171032150237184]['hour'], 8.93639)
df4[["CA","UK","US"]] = pd.get_dummies(df4["country"]) $ df4.head()
tweet_info.tail()
numUsers = firstWeekUserMerged[['userid']].drop_duplicates().count() $ print("The number of users is :",int(numUsers))
stocks.groupby(['ticker', 'dow']).mean()
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old]) $ old_page_converted.mean() $ print(old_page_converted)
archive_clean = archive_clean[archive_clean['in_reply_to_status_id'].isnull()] $ archive_clean = archive_clean[archive_clean['retweeted_status_id'].isnull()]
fare = pd.qcut(titanic['fare'], 2) $ titanic.pivot_table('survived', ['sex', age], [fare, 'class'])
autos['date_crawled'].str[:10].value_counts(normalize = True,dropna = False).sort_index()
df2['dow']=df2['datetime'].dt.weekday_name
feeds = db.get_feeds() $ feeds.head()
def session_cleaner(session): $     session_v1 = session.dropna(axis = 0, subset = ["user_id"])            $     return session_v1
np.exp(result.params)
top_active = session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $ most_active_id = session.query(Measurement.station).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).first() $ top_active $
print(mars_hemispheres.prettify())
detroit_census2.head(1)
logit_mod_3 = sm.Logit(merged['converted'], merged[['intercept', 'UK_page', 'CA_page']]) $ results_3 = logit_mod_3.fit() $ results_3.summary()
importances = randfor.feature_importances_ $ importanceDF= pd.DataFrame() $ importanceDF['features']= test.ix[:, test.columns != 'class'].columns $ importanceDF['importance']= importances $ importanceDF
na_df.notna() # check elements that are not missing
os.listdir('..')
df['gathering'].fillna(0, inplace = True)
female_journalists_retweet_df = journalists_retweet_df[journalists_retweet_df.gender == 'F'] $ female_journalists_retweet_by_gender_df = pd.merge(user_summary_df[user_summary_df.gender == 'F'], female_journalists_retweet_df.groupby(['user_id', 'retweet_gender']).size().unstack(), how='left', left_index=True, right_index=True)[['F', 'M']] $ female_journalists_retweet_by_gender_df.fillna(0, inplace=True) $ female_journalists_retweet_by_gender_df['all'] = female_journalists_retweet_by_gender_df.F + female_journalists_retweet_by_gender_df.M $ female_journalists_retweet_by_gender_df.describe()
all_cities = pd.merge(left=city_loc, right=city_pop, on="city", how="outer") $ all_cities
plt.figure(figsize = (7,7)) $ plt.scatter(x_9d[:,0],x_9d[:,1], c='goldenrod',alpha=0.5) $ plt.ylim(-10,30) $ plt.show()
data = pd.read_csv("datosSinNan.csv") $ test = pd.read_csv("datatestSinNan.csv")
atdist_4x_valid_responses = ['Yes! This info is useful, I\'m going now.'] $ atdist_4x_valid_count_prop_byuser = compute_valid_count_prop_byuser(atdist_4x, users_4x, 'vendorId', 'emaResponse', $                                                                     atdist_4x_valid_responses) $ atdist_4x_valid_count_prop_byuser.head()
df = pd.read_csv('sieve_data.csv')
indA = pd.Index([1,3,5,7,9]) $ indB = pd.Index([2,3,5,7,11])
X_train_dtm = vect.transform(X_train) $ X_test_dtm = vect.transform(X_test)
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old],[n_new,n_old], alternative='larger') $ print(z_score,p_value)
graf_train['DETAILS3']=graf_train['DETAILS'].apply(review_to_wordlist) $ graf_test['DETAILS3']=graf_test['DETAILS'].apply(review_to_wordlist) $
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.drop('onpromotion') $
sub.to_csv('./submission/submission_2017-04-21_r1.csv', index=False)
import test_package.package_within_package.print_bye_function_container $ import test_package.package_within_package.print_bye_class_container $ import test_package.package_within_package.print_bye_direct
all_tweets.shape
march_2016 = pd.Period('2016-03', freq='M') $ print march_2016.start_time $ print march_2016.end_time
a.
sentiment_overall = news_sentiments.groupby('News Source').agg({'Compound': np.mean}).reset_index() $ sentiment_overall
pd.to_datetime(['04-01-2012 10:00'], dayfirst=True)
metadata['data_ignore_value'] = float(refldata.attrs['Data_Ignore_Value']) $ metadata
math.exp(6000/1000)
pd.crosstab(df_result.launched_year, df_result.State).plot.bar()
df = {} $ for RunID in RunIDs.values(): $     db.call_non_select_stored_proc(DBConnections.FRAME_USER, '[Frame_User].[FinFC].[USP_HHdataVarAnalysis]', params=(RunID, '2017-10-01', '2018-01-01', 'Forecast', 'all'), print_sql=True) $     df[RunID] = db.simple_query(sql, DBConnections.FRAME_USER)
pickle.dump(lda_tfidf_data, open('iteration1_files/epoch3/lda_tfidf_data.pkl', 'wb'))
reviews.groupby('taster_twitter_handle').taster_twitter_handle.count()
week48 = week47.rename(columns={336:'336'}) $ stocks = stocks.rename(columns={'Week 47':'Week 48','329':'336'}) $ week48 = pd.merge(stocks,week48,on=['336','Tickers']) $ week48.drop_duplicates(subset='Link',inplace=True)
X_train, X_test, y_train, y_test = train_test_split(features, labels, $                                                     test_size = 0.2, $                                                     random_state = 88, $                                                     stratify = labels)
m.fit(lr, 3, metrics=[exp_rmspe])
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_6208.values.squeeze(), lags=40, ax=ax1)
baseball1_df.loc[baseball1_df['ageAtFinal'].idxmax()]
df_merged.nlargest(10, 'p1_conf').loc[df_merged['p1_dog'] == True]
age.iloc[[True, False, True, False, True, False]]
for df in (train,test): $     df["CompetitionOpenSince"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, $                                                      month=df.CompetitionOpenSinceMonth, day=15)) $     df["CompetitionDaysOpen"] = df.Date.subtract(df.CompetitionOpenSince).dt.days
texts = ['It always seems impossible until its done.', $          'In order to succeed, we must first believe that we can.', $          'Life is 10% what happens to you and 90% how you react to it.', $          'Start where you are. Use what you have. Do what you can.',] $ cur.executemany('INSERT INTO test.test_table (text) VALUES (%s)', texts) $
dftop2.sort_values(['days_top_complaint'], ascending = 0, inplace = True)
p_old  = (df2['converted']).mean() $ print(p_old) $
df.shape 
tokens = pd.DataFrame({'token':X_train_tokens, 'one_star':one_star_token_count, 'five_star':five_star_token_count}).set_index('token')
my_stream.disconnect()
import pandas as pd  $ df = pd.DataFrame(records, columns=['date', 'lie', 'explanation', 'url']) 
data3.head()
df.loc[:,'A']
data[data['processing_time']>datetime.timedelta(148,0,0)]
old_page_converted = np.random.choice([0, 1], size=nold, p=[1-poldnull, poldnull])
RandomForestReg.fit(X_train, y_train) $ RandomForestReg_score = cross_val_score(RandomForestReg , X_train, y_train, cv=3, scoring='r2').mean() $ RandomForestReg_mse = cross_val_score(RandomForestReg , X_train, y_train, cv=3, scoring='neg_mean_squared_error').mean() $ print("RandomForestRegressor R2 ScoreR2 Score:",RandomForestReg_score) $ print("RandomForestRegressor mean squared error:",RandomForestReg_mse)
writer.save()
sales = graphlab.SFrame('Data/kc_house_data.gl/') $ sales.head()
vect = CountVectorizer(stop_words="english", min_df=10) $ X = vect.fit_transform(text) $ X = pd.DataFrame(X.toarray(), columns = vect.get_feature_names()) $ model = Ridge(alpha = 1) $ model.fit(X, y)
size_c = df2.query("group=='control'").count()[0] $ old_page_converted = np.random.choice([0,1], size=(size_c,1), p=[1-po,po]) $
df2=df.drop(indexes)
dinw_filter_set.settings.df
f = netCDF4.Dataset('../OISST/OISST_1982-2010.nc', 'r') $ print(f) 
plot_helpers.hist_in_range(postings.creation_date, datetime.datetime(2014, 8, 1));
df= pd.merge(df, df_weather_origin, on=['ORIGIN','YEAR','MONTH','DAY_OF_MONTH','DEP_HOUR'], how='left') $ df = pd.merge(df, df_weather_dest, on=['DEST','YEAR','MONTH','DAY_OF_MONTH','ARR_HOUR'], how='left')
by_year.set_index("election_year").plot.bar(figsize=(20, 10))
temp = ['low','high','medium','high','high','low','medium','medium','high'] $ temp_cat = pd.Categorical(temp) $ temp_cat_removed = temp_cat.remove_categories('low') $ temp_cat_removed
model.predict_proba(np.array([0,0,1,0,0,0,0,0,50,50,50]))
pd.read_csv("ign.csv",skiprows=[0]).head()
clean_appt_df[['Age','No-show']].boxplot(by='No-show')
df2['ab_page'] = pd.get_dummies(df['group']) ['treatment']
train_norm.head(3)
transactions.merge(users,how="left",on="UserID")
df.groupby('Team').filter(lambda x: len(x) >= 3)
treatment_conversion = df2[df2['group'] == 'treatment']['converted'].mean() $ treatment_conversion
import time $ t0 = time.time() $ rnd_clf.fit(X_train, y_train) $ t1 = time.time()
ax = mains.plot() $ co.steady_states['active average'].plot(style='o', ax = ax); $ plt.ylabel("Power (W)") $ plt.xlabel("Time"); $
for col in var_cat: $     taxi_sample[col] = taxi_sample[col].astype(np.int64)
raw.event.values[0], raw.event.values[25], raw.event.values[17]
list[festivals[['latitude', 'longitude']].itertuples(index=False, name=None)] $ list(df[['lat', 'long']].itertuples(index=False, name=None)) $
login_form = get_login_form(login_page, 'YOUR USERNAME', 'YOUR PASSWORD') $ login_form $ logged_in = s.post(login_url, data=login_form, headers=headers) $ logged_in.status_code
for char in a: $     if char == 's': $         l.append(char)
df.head(5)
y_pred = pipeline.predict(X_test) $ print('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100)) $
save_n_load_df(joined, 'joined_30_day_oil.pkl')
plt.figure(figsize=(10,5)) $ plt.plot(df.index, df['distance']) $ plt.xlabel('time') $ plt.ylabel('distance') $ plt.show()
countries = pd.read_csv('countries.csv') $ countries.head()
preds = aml.leader.predict(test)
df_ = pd.DataFrame(op_)
t1 = pd.Series(list('abc'), [pd.Timestamp('2016-09-01'), pd.Timestamp('2016-09-02'), pd.Timestamp('2016-09-03')]) $ t1
df.isnull().sum()
df.columns = ['Hospital', 'Provider ID', 'State', 'Period', 'Claim Type', 'Avg Spending Hospital', 'Avg Spending State', 'Avg Spending Nation', 'Percent Spending Hospital', 'Percent Spending State', 'Percent Spending Nation'] $ df.columns
print('A_fault =', uc.get_in_units(results_dict['A_fault'], area_unit), area_unit)
statetotals = pd.DataFrame(locations['State'].value_counts()) $ top20 = statetotals[0:20]
print('Density distribution of the error') $ sb.distplot(errors) $ plt.xlabel('error')
x = content_performance_bytime.groupby(['document_type', pd.Grouper(freq='M')])['pageviews'].sum()
result = model.model(q, T, n_iterations, top_bc, bot_bc)
df2 = df.query('group == "treatment" and landing_page == "new_page"').append(df.query('group == "control" and landing_page == "old_page"')) $ df2.head()
p = len(train_att)/len(train) $ print(len(train_att)) $ print('The percentage of converted clicks is {num:.10%}'.format(num=p))
pos_lda = models.LdaModel(pos_bow, id2word=pos_dic, num_topics=3, chunksize=10000, passes=4, iterations=100) $ pos_lda.show_topics(formatted=False)
words = [w for w in words if not w in stopwords.words('english')] $ print(words[:100])
df.hist(bins=50, figsize=(15,15)); $
df_clean.info()
complete_solar_df = pd.read_excel(weather_folder + '/compete_solar_df.xlsx') $ complete_wind_df = pd.read_excel(weather_folder + '/compete_wind_df.xlsx') $ solar_wind_df = complete_solar_df.merge(complete_wind_df, on=['DateTime', 'Date', 'PTE'], how='inner') $ solar_wind_df.to_excel(weather_folder + '/complete_solar_wind.xlsx', index = False)
avg_price_by_brand = dict.fromkeys(brands) $ for name in brands: $     brand_prices = autos.loc[autos["brand"] == name, "price"] $     brand_avg_price = round(brand_prices.mean(), 2) $     avg_price_by_brand[name] = brand_avg_price
archive_copy.to_csv('twitter_archive_master.csv')
lab_choices = ['[High school level laboratory]','[First year undergraduate physics laboratory]','[First year undergraduate chemistry laboratory]','[Higher level physics labs]','[Higher level chemistry labs]'] $ p = [] $ p.append(lab_choices) $ p.append([len(pre_analyzeable[pre_analyzeable['[prior_lab] What lab courses are you presently taking or have taken in the past? Check all that apply. '+choice]==1]) for choice in lab_choices]) $ print tabulate(p) $
import statsmodels.api as sm $ z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'larger') $ z_score, p_value
df['Unique Key', 'Created Date', 'Closed Date'].show(truncate = False)
df_samples = pd.concat([df_failed_samples, df_ok_samples]) $ df_samples = df_samples.reset_index(drop=True)
df_columns.head() $
convert_old = df2.query('landing_page == "old_page" & converted == "1"').count()[0] $ convert_new = df2.query('landing_page == "new_page" & converted == "1"').count()[0] $ n_old = df2.query('landing_page == "old_page"').count()[0] $ n_new = df2.query('landing_page == "new_page"').count()[0] $ convert_old, convert_new, n_old, n_new
print (soup.prettify())
n_old = len(df2.query("group == 'control'")) $ n_old
df_archive_clean.shape
crime_data = crime_data[crime_data["DATE_TIME"].dt.year == 2017] $ crime_data.shape
forecast = m.predict(future)
cust_data = cust_data.drop("MonthlySavings1",axis = 1).head(3) $ cust_data.drop("MonthlySavings1",axis = 1,inplace = True).head(3)
np.save('Participant_7_label.npy', labels_np)
freqs = numpy.logspace(-2.8, -2, 5)
data['team'] $ data.team
cities = pd.DataFrame({ 'City name': city_names, 'Population': population }) $ cities['Area square miles'] = pd.Series([46.87, 176.53, 97.92]) $ cities['Population density'] = cities['Population'] / cities['Area square miles'] $ cities['Is wide and has saint name'] = (cities['Area square miles'] > 50) & cities['City name'].apply(lambda name: name.startswith('San')) $ cities
def filterStopWords(doc): $     return [x for x in doc if x not in stoplist] $ data["Improvements_lemma"] = data["Improvements_lemma"].apply(filterStopWords)
Aust_result.to_csv("Australia_Tweets.csv")
dfrecent.head(10) #looks like winter days have the highest counts
df_A.groupby('Gender').get_group('F')
from sklearn.ensemble import RandomForestClassifier $ import math
with model: $     observation = pm.Poisson("obs", lambda_, observed = summary['event'])
%matplotlib inline $ california_house_dataframe.hist('median_income')
new_trump_df = trump_df.text.apply(clean_tweet)
with open("email_hash_list_gh.csv", "w") as outfile: $     for entries in user_gh_list_hash: $         outfile.write(str(entries)) $         outfile.write("\n")
stanford_word_list = pd.read_csv('dictionary_stanford.txt', header=None) $ stanford_word_list.columns=['Word']
texts = df[df['section_text'].str.contains('fees')][['filename','section_text']].values $ len(texts)
df3 = df2 $ df3.head()
expanding_mean = lambda x: pd.rolling_mean(x,len(x),min_periods=1) $ hlw.plot() $ pd.expanding_mean(hlw).plot() $ plt.legend(labels=['Expanding Mean','Raw']);
wheels = autos[['make', 'drive_wheels']] $ wheels.head()
lons, lats = np.meshgrid(lon_us, lat_us) $ plt.plot(lons, lats, marker='.', color='k', linestyle='none') $ plt.show()
train_col = train.Trainer(model,column_dataloaders['train'],column_dataloaders['val'],optimizer_col)
print (Ralston.TMAX.mean(), Ralston.TMAXc.mean())
print(airquality_dup.shape) $ print(airquality_melt.shape) $ print(airquality_dup.head()) $ print(airquality_melt.head())
df = target_pf[target_pf['date'] == inception_date].copy()
financial_crisis.ndim
countries_count = countries_df['country'].value_counts() $ countries_count
df = pd.DataFrame({'A': rng.rand(5), $                   'B': rng.rand(5)}) $ df
df["TOTAL_PAYMENT"].plot()
data_df.info()
PROCESSED_DATA_DIR = 'processed_data' $ clean_appt_df = pd.read_csv(PROCESSED_DATA_DIR + "/clean_appt_df.csv", $                            parse_dates=['AppointmentDay', 'ScheduledDay']) $ clean_appt_df['No-show'].value_counts() / len(clean_appt_df)
scipy.stats.pearsonr(df_day_pear['tripduration'],df_night_pear['tripduration']) $
stem_porter.concordance('damn')
df2.query('group == "treatment"').converted.sum()/df2.query('group == "treatment"').user_id.nunique()
year_ago = datetime.date(2017, 8, 23) - datetime.timedelta(days = 365) $ print(year_ago)
Test.SetFlowStats(Ch1State=True, Ch2State=False, Ch3State=True)
india = ['India', 'india', 'INDIA', 'India.', 'New Delhi', 'Mumbai', $          'Bengaluru', 'Pune', 'Hyderabad', 'Kolkata', 'Chennai', 'Bangalore', 'Delhi'] $ notus.loc[notus['country'].isin(india), 'country'] = 'India' $ notus.loc[notus['cityOrState'].isin(india), 'country'] = 'India' $ notus.loc[notus['country'] == 'India', 'cityOrState'].value_counts(dropna=False)
new_dems.Sanders.describe()
lm_multi = sm.OLS(df_new['converted'], df_new[['intercept', 'US', 'UK', 'ab_page']]) $ multi_results = lm_multi.fit() $ multi_results.summary()
logit_mod=sm.Logit(df['converted'],df[['intercept','treatment']])
data=pd.DataFrame({'points':[632, 1638, 569, 115], $                    'team':['India','China','Pakistan','Bhutan'], $                    'captain':['Suresh','Ramesh','Kamlesh','Dharmesh'] $                   }) $ print(data)
df2.head()
df['Source'].unique()
import pprint $ pprint.pprint(vars(example_tweets[0]))
plt.plot(top_bc, label ='Top BC') $ plt.plot(bot_bc, label = 'Bot BC') $ plt.legend();
commiters.to_csv('author_commits.csv', columns=['author', 'total_commits'], index=False) $ commiters_by_month.to_csv('commiters_by_month.csv')
np.dtype([('name', 'S10'), ('age', 'i4'), ('weight', 'f8')])
n_old = df2.query('landing_page == "old_page"').user_id.count() $ n_old
a = np.array([1, 2, 3]) $ a
df_users_test = df_users.iloc[:2, :] $ df_users_test.iloc[1, -1] = '2017-09-20' $ df_users_test
from sklearn.metrics import jaccard_similarity_score $ from sklearn.metrics import f1_score $ from sklearn.metrics import log_loss
mask = (nullCity["creationDate"] > '2015-01-01') & (nullCity["creationDate"]<= '2015-12-31') $ nullCity2015 = (nullCity.loc[mask]) $ nullCity2015.head()
df = pd.read_sql('SELECT * from booking', con=conn_b) $ df
out_train = out_df $ train = train_df $ train["interest_level"] = out_train[["high","medium","low"]].idxmax(axis=1).apply(lambda x: target[x]) $ train[["high","medium","low"]] = out_train[["high","medium","low"]]
[random_date(pd.datetime.now() - pd.offsets.Day(10), pd.datetime.now()) for _ in range(10)]
failures_spark = spark.createDataFrame(failures, $                                        verifySchema=False) $ failures_spark.printSchema()
learner.load_encoder('adam1_20_enc')
print("Probability an individual recieved new page:", $       ab_file2['landing_page'].value_counts()[0]/len(ab_file2))
import datetime $ print(datetime.date.today()) $ armageddon = datetime.date(2012, 12, 21) $ print(armageddon.strftime('%A %d %b %Y'))
pd.pivot_table(expenses_df, values = "Amount", index = "Day", aggfunc = np.sum)
Mars_df = Mars_tables[0] $ Mars_df.columns = ['0', '1'] $ Mars_df.rename(columns = {'0':'Mars Information', '1': 'Facts'}, inplace = True) $ Mars_df
df['month'] = df.order_date.dt.month.T $ df['day'] = df.order_date.dt.day.T $ df['year'] = df.order_date.dt.year.T
dfs.head()
ls -l *.csv
weather_df.info()
pd.Series(bnb.first_affiliate_tracked).isnull().sum()
df2_treatment = df2.query("group == 'treatment'")
shows2 = pd.read_csv('shows_good_data.csv') $
def normalize(series): $     return series / 2 * series.mean() $
res = requests.get(BASE) $ status_object = res.json() $ print(json.dumps(status_object, indent=4))
calls.head()
from sklearn.linear_model import LinearRegression $ from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score $ import numpy as np
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].count()
browser.quit()
df_prep14 = df_prep(df14) $ df_prep14_ = pd.DataFrame({'date':df_prep14.index, 'values':df_prep14.values}, index=pd.to_datetime(df_prep14.index))
df_new = df_new.join(pd.get_dummies(df_new['country'])) $ df_new.head()
cols = df_os.columns.tolist() + df_usnpl_one_hot_state.columns.tolist() $ media_classes = [c for c in cols if c not in ['domain', 'notes']] $ breakdown = df_questionable_3[media_classes].sum(axis=0) $ breakdown.sort_values(ascending=False).head(20)
tweet_source_hist = pd.crosstab(index=tweets_df["tweet_source"], columns="count") $ tweet_source_hist['source_freq'] = tweet_source_hist['count'] * 100 / tweet_source_hist.sum()['count'] $ tweet_source_hist = tweet_source_hist.sort_values('source_freq', ascending=False) $ tweet_source_hist.head(15)
dog_ratings.dog_breed = dog_ratings.dog_breed.str.replace('_',' ') $ dog_ratings.dog_breed = dog_ratings.dog_breed.str.lower()
open_list = [] $ for i in data2: $     if i[1] != None: $         open_list.append(i[1])
min_open = [] $ for entry in d["dataset_data"]["data"]: $     min_open.append(entry[1]) $ min_open = filter(None, min_open) $ print("The lowest opening price was $"+str(min(min_open)))
STEPS = 365*10 $ random_steps = pd.Series(np.random.randn(STEPS), index=pd.date_range('2000-01-01', periods=STEPS))
df = df.loc[(df['ruling']!='Full Flop')&(df['ruling']!='Half Flip')]
v_invoice_link.drop(v_invoice_link_dropper, axis=1, inplace=True) $ invoice_link.drop(invoice_link_dropper, axis = 1, inplace=True)
train_texts, val_texts = sklearn.model_selection.train_test_split(np.concatenate([train_texts, val_texts]), test_size=0.1)
draft_df.shape
trips.isnull().any()
df2_treatment = df2.query('group == "treatment"') $ agg_df2_treatment = df2_treatment.query('converted == "1"').user_id.nunique() / df2_treatment.user_id.nunique() $ agg_df2_treatment
details.sort_values(by='Released')
import pandas as pd $ datos=pd.read_csv('data_thinkspeak.csv') $ df = pd.DataFrame(datos) $ df.head() $
df.loc['20180101':'20180103', ['A','B']]
print(kmeans.cluster_centers_) $ print(kmeans.labels_)  $
loans_act_20150430_repaid_xirr=cashflows_act_investor_20150430_repaid.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
autos["registration_year"].describe()
df_questionable_3[df_questionable_3['state_AZ'] == 1]['link.domain_resolved'].value_counts()
data_full.head()
df_group_by = df_group_by.reset_index() $
barcelona.index = barcelona['GMT']
df2.drop(df2.index[1899])
recommendation_df = pd.merge(recommendation_df, challenge_contest, on = 'challenge_id', how = 'left')
%%time $ result = db.Operations.find(query, $                             projection=proj, $                             sort=sort) $ fvL = list(result)
train_users_pd['gender'].unique()
dict1 = {"a":1,"b":2} $ dict2 = {"c":3,"b":4,"a":5} $ list_of_dict = [dict1,dict2] $ df2 = pd.DataFrame(list_of_dict) $ print(df2)
hundred = convRate.loc[convRate['rate']==1] $ hundred
tfav_k1.plot(figsize=(16,4), label="Likes", legend=True) $ tret_k1.plot(figsize=(16,4), label="Retweets", legend=True);
mod.predict(a)
df.index
ac['Eligibility Date'].describe()
corrplot(fin_r.loc[start_date:end_date].corr(), annot=True) $ plt.title('Correlation matrix - daily data \n from ' + start_date + ' to ' + end_date) $ plt.show()
output_folder = '../data/sync/'+experiment_name+'/event_list/' $ checkDirectory(output_folder) $ event_list['protocol_name']= '' $ event_list['repetition_name']= '' $ event_list.to_csv(output_folder+'event_list_'+experiment_name+'_.csv',index=False)
beirut.dtypes
autos.info()
y = final_df["ground_truth_adjusted"].values  #Target $ X = final_df[[col for col in final_df.columns if "ground_truth" not in col]] $ X.to_csv("data/X.csv", index=False) $ final_df["ground_truth_crude"].to_csv("data/y_crude.csv", index=False) $ final_df["ground_truth_adjusted"].to_csv("data/y_adjusted.csv", index=False)
from sklearn.feature_extraction.text import CountVectorizer $ count_vect = CountVectorizer(analyzer=text_process) $ bow_transformer = count_vect.fit(X) # Bag of Words Transformer
sdof_resp()  # arguments aren't necessary to use the defaults.
pd.datetime.today() - pd.Timestamp('1/1/1970')
1/np.exp(-0.0149),1/np.exp(-0.0408),np.exp(0.0099)
df2.loc[df2['user_id'].duplicated(keep = "first"), :]
df.A
urllib.parse.urlparse(url) # better to use urlsplit $ split_url = urllib.parse.urlsplit(url) $ split_url
plt.figure(figsize=(12,6)) $ sns.barplot(x='lead_source', y= 'leadConversionPercent', data=leadConvpct) $ plt.xticks(rotation=90) $ plt.title('Lead to Opportunity Percentage');
to_be_predicted_Day4 = 55.24901111 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
import math $ print('{:.10f}'.format(math.pi)) $ print('{:0g}'.format(2e10)) $ count, total = 8500, 9000 $ print("Accuracy for {} words: {:.4%}".format(total, count / total))
Results_rf1000.to_csv('soln_rf1000.csv', index=False)
df.to_csv(dataurl+'train200th.csv', sep=',', encoding='utf-8', index=False)
archive.pupper.value_counts()
def index_level_dtypes(df): $     return [f"{df.index.names[i]}: {df.index.get_level_values(n).dtype}" $             for i, n in enumerate(df.index.names)] $ index_level_dtypes(read_multi_df)
m=learner.model $
trips_sorted_pilot = df_trips.sort_values(["pilot", "trip_requested"])
df.head(5)
hour_of_day15 = uber_15["hour_of_day"].value_counts().sort_index().to_frame()
_ = df.query('(group == "treatment" and landing_page == "old_page") or (group == "control" and landing_page == "new_page")')['user_id'].nunique() $
sim_diff = new_page_converted.mean() - old_page_converted.mean() $ print('The difference in the simulated conversion rates is {}.'.format(round(sim_diff,4))) $
p_diffs = [] $ new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
aru_df.info()
df2.keys()
dat.head(3)
frame2['eastern'] = frame2.state == 'Ohio'  # Can not be created with frame2.eastern
Queens_gdf = newYork_gdf[newYork_gdf.boro_name == 'Queens'] $ Queens_gdf.crs = {'init': 'epsg:4326'}
p5_result = p2_table.sort_values('Profit', ascending=True) $ p5_result.head()
data4 = gpd.GeoDataFrame(data3, geometry='geometry')
cluster_label_group.ix[:,0:21]
job_requirements.columns
d = pd.Period('2016-02-28', freq='D') $ d
df2 = df.groupby(['month','account_id']).count() $ df2 = df2.reset_index('account_id') $
class_perishables=pd.DataFrame(group['perishable'].agg('sum')) $ pd.DataFrame.head(class_perishables)
(df.isnull().sum() / df.shape[0]).sort_values(ascending=False)   # credit Ben shaver
tweet_hour.count()
glm_binom_v1.accuracy()
from gensim.summarization import summarize, keywords
imsi_with_1T = event_num[event_num == 1].index.values
g['created_year'].unique()
lm.score(x_test, y_test)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df.loc[0, 'review'][:-500]
autos["price"].head()
(events.groupby('client_uuid') $      .map(lambda x: x.total_seconds()) $      .hist(bins=50)) # KABOOM !
full_nan.isnull().sum() / full_nan.shape[0]
pd.to_datetime(['2009/07/31', 'asd'], errors='coerce')
pageviews.head(2)
cur.execute('SELECT EmpGradeRank, count(EmpGradeRank) FROM demotabl WHERE EmpGradeRank="SENIOR";') $ cur.fetchone()
df_twitter_copy.info()
print("file saved as :",OUT_FOLDER +output_file_name) $ df_raw.to_csv(OUT_FOLDER +output_file_name, index=True)
age_category1 = df_titanic_temp.loc[df_titanic_temp['age'] <= age_median, 'age'] $ age_category2 = df_titanic_temp.loc[df_titanic_temp['age'] > age_median, 'age'] $ print(len(age_category1)) $ print(len(age_category2))
df = pd.DataFrame(results, columns=['Station', 'Temperature']) $ df.head()
(p_diffs > np.array(obs_diff)).mean()
airlines_day_unstacked = airlines_day_unstacked[(airlines_day_unstacked != 0).all(1)]
os.chdir(str(today))
saved2 = save_dictionary(short_dict, 'gs://{}/dictionary'.format(BUCKET))
df = sean['Event Type Name'].value_counts() $ df_sean = pd.Series.to_frame(df) $ df_sean.columns = ['Sean H'] $ df_sean
print(w.get_filtered_data(step = 2, subset = subset_uuid, type_area = type_area, indicator = 'din_winter').MONTH.unique()) $ print(w.get_filtered_data(step = 2, subset = subset_uuid, type_area = type_area, indicator = 'din_winter').DEPH.min(), $ w.get_filtered_data(step = 2, subset = subset_uuid, type_area = type_area, indicator = 'din_winter').DEPH.max()) $ w.get_filtered_data(step = 2, subset = subset_uuid, type_area = type_area).WATER_TYPE_AREA.unique()
donors['Donor Zip'].value_counts().head()
ctr = pd.read_csv("ctr_graph.csv")
with open('test.txt', 'a') as f: $     VidDumpSingle(['1','2','3','4','5','6'], f)
df_afc_champ2018['playnumber'] = df_afc_champ2018['playId'].apply(lambda x: int(str(x).replace('400999172', ' ')))
df.head()
own_star.rename(columns={'created_at_star': 'created_at'}, inplace=True) $ own_star.head(10)
for t in tables: display(t.head())
df['twoM'] = np.nan $ df['fiveM'] = np.nan $ df['tenM'] = np.nan $ df['thirtyM'] = np.nan $ df.head(2)
df['intercept'] = 1 $ df[['treatment', 'control']] = pd.get_dummies(df['group'])
users.email = users.email.apply(code_email) $ users.creation_source = users.creation_source.apply(code_creation)
fig = acc.plot_history(startdate='01.08.2016', enddate='23.09.2016', what='cumsum')
cs.head()
s.plot(figsize=(12,10), label="Actual") $ s_median.plot(label="Median") $ plt.legend();
guipyter.notebook_tools.notebook_current_convert("html")
cog_simband_times[cog_simband_times.index == 'LW-OTS BHC0048-1']
twitter_ar.drop(['doggo','floofer','pupper','puppo'],axis=1,inplace=True)
shots_df.describe()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)
recommend.dtypes # Returns the dtypes of a Dataframee
train_sample = pd.read_csv("train_sample.csv") $ train_sample.head()
f0 = lv_workspace.get_data_filter_object(step=0) 
df[df['Descriptor'] == 'Loud Music/Party']['Unique Key'].groupby(df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(kind='bar')
print('There are {} Sites'.format(len(df.index)))
from IPython.display import Image $ from IPython.core.display import HTML $ Image(url= "https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2017/03/join-types-merge-names.jpg")
tweet_archive_df.rating_numerator.value_counts().sort_index()
tweet_archive_master_name = 'twitter_archive_master.csv' $ tweet_archive_master_path = os.path.join(folder_name, tweet_archive_master_name) $ tweet_archive_master.to_csv(tweet_archive_master_path)
user["favourites_count"] = user["favourites_count"].apply(np.exp2) $ user.head(3)
k_means_cluster_centers = k_means.cluster_centers_ $ k_means_cluster_centers
df_temp_by_station = engine.execute("select min(tobs), max(tobs), avg(tobs) from measurement where station = 'USC00519281';").fetchall() $ df_temp_by_station
twitter_final['date'] = pd.to_datetime(twitter_final['date'])
crimes.PRIMARY_DESCRIPTION.value_counts()
calls_df.loc[(calls_df["time"]>=4) & (calls_df["time"]<10),["call_time"]]="Morning" $ calls_df.loc[(calls_df["time"]>=10) & (calls_df["time"]<16),["call_time"]]="Noon" $ calls_df.loc[(calls_df["time"]>=16) & (calls_df["time"]<22),["call_time"]]="Evening" $ calls_df.loc[(calls_df["time"]>=22) & (calls_df["time"]<4),["call_time"]]="Night" $ calls_df["call_time"].unique()
DataSet_sorted[['tweetText', 'prediction']].tail(10)
def wei_to_ether(wei): $     return 1.0 * wei / 10**18
kickstarter.info()
pc_order.index.values
nt_price.head()
datascience_tweets[datascience_tweets['text'].str.contains("RT")]['text'].count() # 322
pred3 = nba_pred_modelv1.predict(g3) $ prob3 = nba_pred_modelv1.predict_proba(g3) $ print(pred3) $ print(prob3)
diff_weekly_mean = day_counts.select('week','hashtag', diff_weekly_mean_comp.alias('sq_diff'))
plt.plot(all_scores)
ab_dataframe.shape
com_grp.groups  # return Dictionary
all_noms[all_noms["agency"] != "Foreign Service"]["nom_count"].sum() + additional_june_noms
plot_data(389) $ digits.target[389]
joined = load_df('joined_transactions.pkl')
df_master = df_master.dropna() $ df_master.shape
tweets.dtypes
row_df.head()
data1 = pd.read_csv("TrendingTags.csv", sep=',') $ data1.head()
from pandas import DataFrame $ cf = DataFrame.from_csv("hillaryclinton_statusesfacebook.csv") $ ccf=cf.drop(cf.columns[[0,1,2,3,5]], axis=1) $ cliint = ccf[(ccf['status_published'] > '2016-07-26') & (ccf['status_published'] < '2016-11-08')]
fb = cb.organization('facebook')
pregnancies.data.loc[0]
obs = df_gp_hr.groupby('level_0').mean() $ observation_data = obs['Observation (aspen)']
final_word_df = grouped[grouped.Word_stem.isin(stemmed_dict_list)]
S_1dRichards.decision_obj.groundwatr.options, S_1dRichards.decision_obj.groundwatr.value
usersDf[['favourites_count','followers_count','friends_count','listed_count']].describe()
df.resample('D', how='mean')
tt_json_clean.id = tt_json_clean.id.astype(str)
all_df.columns
import datetime $ date = datetime.date(year=2015,month=1,day=8) $ df[df['date'] > date]
df['bg_ratio'] = df['backers'] / df['goal'] $ df['bg_avg'] = df[['backers', 'goal']].mean(axis=1) $ df['bg_ratio_date'] = (df['backers'] / df['goal']) * df['date_diff']
n_new_page = len(df2[df2['landing_page']=='new_page']) $ probabiility_newpage = n_new_page / n_unique_users $ print ("The probability that an individual received the new page is: {:.4f}".format(probabiility_newpage))
old_page_converted = np.random.choice([1, 0], size=nold, p=[pold, (1-pold)])
ad_group_performance.drop( $     columns=['TheAnswer'], inplace=True $ ) $ ad_group_performance
sns.heatmap(data_numeric.corr().abs().round(2),annot=True)
sub_df.head()
sum(tweetsDF.time_zone.value_counts())
house_id = os.listdir('house_image')
from sklearn.metrics import jaccard_similarity_score $ from sklearn.metrics import log_loss
twitter_api = {} $ twitter_api_filename = 'twitter_api_keys.json' $ with open('./{}'.format(twitter_api_filename)) as twitter_api_keys: $     twitter_api = json.load(twitter_api_keys)   
y = new["Purchased"] $ x = new.drop("Purchased",axis=1) $ Y = new_scaled["Purchased"] $ X = new_scaled.drop("Purchased",axis=1)
def num_repeated_songs(x): $     x['num_repeated_songs'] = (x.num_100 + x.num_985 + x.num_75) / x.num_unq $     return x $ user_logs = user_logs.apply(num_repeated_songs, axis = 1)
df = pd.read_csv('https://raw.githubusercontent.com/jackiekazil/data-wrangling/master/data/chp3/data-text.csv') $ df.head(2)
from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report $ from sklearn.model_selection import GridSearchCV, StratifiedKFold $ from sklearn import preprocessing
tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3)) $ full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist()) $ train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist()) $ test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())
vol = vol.div(vol['Volume'], axis=0) $ vol.head()
medals = pd.read_table('../data/olympics.1996.txt', sep='\t', $                        index_col=0, $                        header=None, names=['country', 'medals', 'population']) $ medals.head()
df['duration'].sum()
treatment_group = df2.group == 'treatment' $ treatment_group_and_converted = treatment_group & (df2.converted == 1) $ len(df2[treatment_group_and_converted]) / len(df2[treatment_group])
to_be_predicted_Day4 = 50.98147755 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
df_tweet.info()
corn.mean()
counts_by_campaign_date.loc['Sport']
Z = np.tile( np.array([[0,1],[1,0]]), (4,4)) $ print(Z)
import statsmodels.api as sm $ log_reg = sm.Logit(df2['converted'], df2[['intercept','treatment']]) $ res = log_reg.fit()
print("%d events have been duplicated in this dataset." % 2512)
merged_df['frequency'] = merged_df['validOrders'] - 1 $ merged_df.head()
%%writefile 1st_flask_app_2/static/style.css $ body { $     font-size: 2em; $ }
cityID = 'b49b3053b5c25bf5' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Denver.append(tweet) 
Incid = pd.read_csv('Data/Incidents.csv') $ Incid.head() $
pd.to_datetime('20170109',format='%Y%d%m')
plt.hist(s2) $ plt.show()
df.groupby(['userid', 'website'])['price'].sum()
DataSet['text_clean'] = DataSet['tweetText'].apply(remove_punctuation) $ DataSet_matrix = vectorizer.transform(DataSet['text_clean']) $ Predictions = sentiment_model.predict_proba(DataSet_matrix)
ser.iloc[:1]
len(data_2017_subset[data_2017_subset.incident_zip.str.match('([^0-9])', na=False)])
treatment = df2.query('group == "treatment"') $ treatment_converted = treatment.query('converted == 1') $ p_treatment_convert = treatment_converted.count()[0]/treatment.count()[0] $ p_treatment_convert
search2.tail(5)
train_dum_clean.head()
%matplotlib inline $ import matplotlib.pyplot as plt, numpy as np
doctype_by_day = doctype_grouped.unstack('document_type', fill_value=0)
twitter_final.head(2)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ (z_score, p_value)
df = pd.read_csv('../../datasets/crypto-index-fund/crypto_data/CryptoDataWide.csv', parse_dates=True)
df_new['country'].value_counts()
query = ' SELECT count(id) FROM nodes_tags WHERE key = "tourism" AND value = "hotel"' $ c.execute(query) $ results = c.fetchall() $ print "Hotels in Leiden database:", results[0][0]
h.end_time # Shows us the end of that hour
pd.to_datetime([1349720105100, 1349720105200, 1349720105300, 1349720105400, 1349720105500 ], unit='ms')
y = df.num_comments
positive_deaths_per_year = defaultdict(int) $ for death,trend in zip(df_valid["Died"],df_valid["Trends"]): $     if trend >= 1: $         positive_deaths_per_year[death.year] += 1 $ positive_deaths_sorted = OrderedDict(sorted(positive_deaths_per_year.items()))
ids = [1,3,56,83,34634,536352,45745] $ user = 34 $ df = pd.DataFrame({'repo_id': ids, 'user_id': [user for i in range(len(ids))]}) $ df.head()
sp.info()
unique_users = df.user_id.nunique() $ print('The dataset includes {} unique users.'.format(unique_users))
plt.hist(p_diffs) $ plt.axvline(x=df2.converted[df2.group == "treatment"].mean() - df2.converted[df2.group == "control"].mean(), color = "red" )
station_splits = pd.read_csv('../../../data/StationSplits.csv') $ station_splits.fillna(0.0, inplace=True) $ station_splits
obs_diff = df2['converted'][df2['group'] == 'treatment'].mean() - df2['converted'][df2['group'] == 'control'].mean()
type(my_town.sentiment.polarity)
import matplotlib.pyplot as plt $ %matplotlib inline
most_active_stations = session.query(Measurement.station).\ $                                      group_by(Measurement.station).order_by(func.count(Measurement.prcp).desc()).limit(1).scalar() $ print ("Station: " + str(most_active_stations), "has the highest number of observations")
def get_rental_concession_2(description): $     tags = {k for k, p in PATTERNS2.items() if re.search(p, description)} $     if tags: $         return tags
df_twitter.text.loc[822]
s_dates.set_index('date', inplace=True)
miss_grp2 = df.query("group =='control' and landing_page =='new_page'") $ print('The number of times a user from the control group lands on the new page is {}'.format(len(miss_grp2)))
number_of_commits = len(git_log) $ number_of_authors = len(git_log["author"].dropna().unique()) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df_goog.index = df_goog['Date'] $ df_goog.index = df_goog.index.to_datetime() $ df_goog.head()
tweets_clean.set_index(tweets_clean.columns[0], inplace = True) $ tweets_clean.head()
autos=autos.drop('abtest',1)
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US', 'ab_page']]) $ results =lm.fit() $ results.summary() $
titanic_clean = preprocessor.basic_cleaning()
from sklearn.linear_model import LogisticRegression $ logit = LogisticRegression(max_iter=200) $ logit.fit(train[simple_features], y_train)
active = session.query(Measurement.station,Measurement.tobs) $ df2=pd.DataFrame(active[:],columns=["Stations","TOBS"]) $ activestations=df2.groupby("Stations").count().sort_values('TOBS',ascending=False) $ activestations
df2[df2['landing_page']=='old_page'].count()-1
lda_tfidf.show_topics(formatted=False, num_words=20)[0:2]
token_receivecnt = token.groupby(["receiver","month"]).size().reset_index(name= "receivecount")
grouped_dpt["Revenue"].filter(lambda x: len(x) < 5)
def connect(db_name, user_id, password): $     dsn = cx_Oracle.makedsn(host='hippo.its.monash.edu', port='1521', sid=db_name) # pass data source name $     conn = cx_Oracle.connect(user=user_id, password=password, dsn=dsn) # connect to database $     cur = conn.cursor() # cursor() methods opens a cursor for statements to use. $     return cur, conn #return cursor and connection
MA_final.to_csv('attarction_rate_2017_5to5.csv')
geo_TempIrrs.head()
print(autos['price_$'].unique().shape) $ print(autos['price_$'].describe()) $ print(autos['price_$'].value_counts().sort_index(ascending=True)) 
sh_max_df.describe(include="all")
treatment=df2.query('group=="treatment"') $ exp=df2.query('group=="treatment" and converted==1').count()[0]/treatment.count()[0] $ exp
plt.hist(b56[~np.isnan(b56)],50); #50 signifies the # of bins
for i in inv: $     a1=repaid_loans_cash[(repaid_loans_cash.fk_loan==36) & (repaid_loans_cash.fk_user_investor==i)] $     a2=loan_principals[(loan_principals.fk_loan==36) & (loan_principals.fk_user_investor==i)] $     print i,xirr([a1,a2]) $
import statsmodels.api as sm $ convert_old = df2.query('group=="control"')['converted'].sum() $ convert_new = df2.query('group=="treatment"')['converted'].sum() $ n_old = df2.query('group=="control"').shape[0] $ n_new = df2.query('group=="treatment"').shape[0]
print(df.shape) $ for col_x in lag_list: $     for iter_x in np.arange(lookback_window)+1: $         df['{0}-{1}'.format(col_x,   str(iter_x).zfill(2)      )] = df[col_x].shift(iter_x) $ print(df.shape)
tm_2020 = pd.read_csv('input/data/trans_2020_m.csv', encoding='utf8', index_col=0)
cdum=pd.get_dummies(df_new['country']) $ df_new=df_new.join(cdum)
bnbA.date_first_booking.head(10)
plt.figure() $ stockP['apple'].plot(x='total_hours', y='price')
import statsmodels.api as sm $ convert_old = df2_control.converted.sum() $ convert_new = df2_treatment.converted.sum() $ n_old = df2_control.converted.count() $ n_new = df2_treatment.converted.count()
count_bldg_opps = opportunities.groupby(['Account ID','On Zayo Network Status'])['Building ID'].count() $ count_bldg_opps = count_bldg_opps.reset_index()
print('Slope: Efthymiou vs experiment: {:0.2f}'.format(popt_axial_brace_crown[2][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_crown[2]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
sl_pf_v2.shape
prods_user2=prods_user2.fillna(0) $ test_orders_prod3=pd.merge(test_orders,prods_user2, on='user_id') $ test_orders_prod3.head()
p_treatment_obs = df2.query('group == "treatment"')['converted'].mean() $ p_treatment_obs
np.array([1,0,0,0,0])
df_new['intercept_country'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept_country','US', 'UK']]) $ results = log_mod.fit() $ results.summary()
Base.classes.keys()
sum(df.Category.isnull())
cityID = '946ccd22e1c9cda1' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Pittsburgh.append(tweet) 
outcome=[0,1] $ old_page_converted=np.random.choice(outcome,n_old, p=(1-p_old,p_old))        $ old_page_converted
autos['price'].sort_index(ascending=False).head(9)
_index = 'SPX' $ index_ts = pd.read_sql("select * from daily_price where ticker='SPX' and instrument_type='index'", engine) $
print train_data['photos'].apply(len).describe()
returns.corrwith(volumn)
subwaydf['4HR_Entries'] = subwaydf['4HR_Entries'].where(subwaydf['SCP'] == subwaydf['SCP'].shift(1)) $ subwaydf['4HR_Exits'] = subwaydf['4HR_Exits'].where(subwaydf['SCP'] == subwaydf['SCP'].shift(1))
table = [] $ for pn in pn_qty.iterkeys(): $     table.append(data_from_1c.where(data_from_1c['Part no.'] == pn))
staff.index
twitter_Archive.describe()
temp_cat.set_categories(['low','medium','sederhana','susah','senang'])
zip_1_df.head()
brand_price = {} $ for brand in brands: $     mean_price = autos['price'][autos['brand']==brand].mean() $     brand_price[brand] = int(mean_price) $ brand_price
tweets.head()
tweets_df.iloc[3, 10]
table_rows = driver.find_elements_by_tag_name("tbody")[22].find_elements_by_tag_name("tr") $
reqs.count()
data1=data1.interpolate()
countries_df = pd.read_csv('countries.csv')
!hdfs dfs -put Consumer_Complaints.csv Consumer_Complaints.csv
sns.regplot(x=data['days_active'], y=data['ltv'])
from sklearn.linear_model import LogisticRegression $ from sklearn.model_selection import train_test_split $ from sklearn.metrics import accuracy_score, roc_auc_score,f1_score $ from sklearn.metrics import confusion_matrix $ from sklearn.grid_search import GridSearchCV $
date_agency = data.groupby(['yyyymm','Agency']) $ date_agency.size().unstack().plot(kind='bar',figsize=(15,15))
merged_data['cs_creation_day'] = merged_data['customer_creation_date'].dt.day $ merged_data['cs_creation_month'] = merged_data['customer_creation_date'].dt.month $ merged_data['cs_creation_year'] = merged_data['customer_creation_date'].dt.year
liberia_data.Description
df["grade"] = df["raw_grade"].astype("category") $ df
webmap.zoom = 4
df.info()
autos.sort_values("price_dollars",ascending=False).head(10)
position_df = pd.read_excel('../data_gathering/data' +'/position/nordzee-wind-pnl.xlsx', encoding = 'ISO-8859-1')
X_train, X_test, y_train, y_test = data.get_X_y(data.simple_features, transformed_df)
lists=record['Post'].tolist()
df_2015.dropna(inplace=True) $ df_2015
train.isnull().sum()
authors_with_name.select("parsed_info.first", "parsed_info.title").show()
tia['duration_int'] = tia['duration'].apply(lambda x: (x/np.timedelta64(1, 'D')).astype(int))
df.head()
ss.fit(X)
df_mes['travel_time'] = df_mes['tpep_dropoff_datetime'] - df_mes['tpep_pickup_datetime'] $ df_mes['travel_time'] = df_mes['travel_time'].dt.total_seconds()
np.median(df2.days_active)
df.head()
con = sqlite3.connect('db.sqlite') $ con.execute("CREATE TABLE tbl(wikipedia TEXT, topic TEXT, year INTEGER, month INTEGER, pageviews INTEGER);") $ con.commit() $ con.close()
df.to_csv('comma_delim_clean.csv', sep=',')
writer.save()
import statsmodels.api as sm $ convert_old = df2[df2['landing_page'] == 'old_page']['converted'].sum() $ convert_new = df2[df2['landing_page'] == 'new_page']['converted'].sum() $ print(convert_old, convert_new, n_old, n_new)
overlapping_trip_pilot_ids = trips_sorted_pilot[trips_sorted_pilot["overlap"] == 1]["id"].values $ df_trips = df_trips.drop(overlapping_trip_pilot_ids)
combined_df4['general_paymeth']=combined_df4['paymeth_code'].apply(lambda x: 'DD' if 'DD' in x else 'CSM') $ combined_df4[['paymeth_code','general_paymeth']]
format8 = lambda x: '%.2f' %x $ print df.applymap(format8) $ print df.apply(lambda x: x.max() - x.min())
pr("Droping rows with NA values (location and creation date).") $ tw1 = twh.dropna(axis=0, how='any', subset=['createdAt']) $ tw1.dropna(subset=['longitude'], inplace=True) $ tw1.dropna(subset=['latitude'], inplace=True) $ pr('The data have been reduced from {} tweets to {} tweets.'.format(strNb(len(twh)), strNb(len(tw1))))
lr=0.2
image_predictions_clean = pd.concat([image_predictions_clean, dog_breeds.rename('dog_breed')], axis=1)
ao18_coll, db = au.get_coll("ausopen18") $ ao18_qual_coll, _ = au.get_coll("ausopen18_qual")
rush_hours = [5,6,7,8,15,16,17,18] $ train['is_rushhour'] = train['start_timestamp'].map(lambda x: 1 if x.hour in rush_hours else 0)                      $ test['is_rushhour'] = test['start_timestamp'].map(lambda x: 1 if x.hour in rush_hours else 0)
X_c=X_d.drop(['title','text'],axis=1)
sns.violinplot(autos["price"]);
print("beggining inner CV...") $ params = {'learning_rate':[0.05,.1], 'max_depth':[3,5,7], 'n_estimators':[1000, 2500]} $ model_inner = model_selection.GridSearchCV(XGBClassifier(scoring='logloss', reg_alpha=0, reg_lambda=1.0,colsample_bylevel=1.0, $                                         colsample_bytree=0.5,gamma=0.01,min_child_weight=1.0, max_delta_step=0.0 ), params) $ model_inner.fit(X_train, y_train )
from pprint import pprint $ def show_author(name): $     print('\n%s' % name) $     print('Topics:') $     pprint([(topic[0], model.show_topic(topic[0])) for topic in sorted(model[name], key=lambda x:x[1])])
print('Groups: {}'.format(df['group'].unique())) $ print('Landing pages: {}'.format(df['landing_page'].unique())) $ print('Converted: {}'.format(df['converted'].unique()))
with pd.option_context("display.max_rows",10): $    print(pd.get_option("display.max_rows")) $    print(pd.get_option("display.max_rows"))
flights_by_Airline = flights.groupby(["AIRLINE"]) $ num_flights_airline = flights_by_Airline.size() $ num_flights_airline
with open('nmf_doc_top_5_29.pkl', 'wb') as piccle: $     pickle.dump(nmf_doc_top, piccle)
people.assign( $     body_mass_index = people["weight"] / (people["height"] / 100) ** 2, $     has_pets = people["pets"] > 0 $ )
train_session_v2['index'].mean()
bldg_data_0 = bldg_data[bldg_data['255_elec_use']==0] $ bldg_data_0.groupby([bldg_data_0.index.year,bldg_data_0.index.month]).agg('count').head(100) $
autos["last_seen"].str[:10].\ $ value_counts(normalize=True,dropna=False).\ $ sort_index(ascending=True)
temperature = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $     filter(Measurement.station == most_active).\ $     filter(Measurement.date > last_year).\ $     order_by(Measurement.date).all()
exclude_year = [1985, 1986, 1987] $ lv_workspace.get_subset_object('A').set_data_filter(step=1, filter_type='exclude_list', filter_name='YEAR', data=exclude_year) 
old_page_converted = old_page_df.query('converted == True').user_id.nunique() / old_page_df.query('converted == False').user_id.nunique()
nba_df.head(3)
HARVEY_gb_user.get_group('2015gardener').text.values
article_titles = [item.find('h1').get_text() for item in article_divs]
score_a = score[(score["score"] < 100) & (score["score"] >= 90)] $ score_a.shape[0]
compound_final = compound_sub1.append(compound_sub2)
wordfreq = FreqDist(words_sk) $ print('The 100 most frequent terms, including special terms: ', wordfreq.most_common(100))
autos['price'].describe(percentiles=[.10, .90])
lsi = models.LsiModel(corpus_tfidf, id2word=bag, num_topics=10) $ corpus_lsi = lsi[corpus_tfidf]
points.loc['r0', :]  # .loc() is label-based indexing
df["pickup_end_of_month"] = df["pickup_week"].isin([4, 5])
from svpol import parser $ try: $     SVPOL(data/'trialdata'/'loremipsum.txt').to_dataframe().head() $ except parser.BadFile as err: $     print(err)
px = px.asfreq('B').fillna(method='pad')
pd.DataFrame(data = np.random.rand(50, 4), columns=['a', 'b', 'c', 'd']) $ plot_test.plot(kind='scatter', x='a', y='b')
pump_data_path = os.path.join('..', 'data', 'raw', 'pumps_train_values.csv') $ df = pd.read_csv(pump_data_path) $ df.head(1)
index_df.head()
act_diff = df[df["group"]=='treatment']["converted"].mean() - df[df["group"]=='control']["converted"].mean() $ act_diff
dcrime = pd.read_csv(dic_inp["detroit-crime.csv"]) $ dcrime.rename(columns = {"LON":"long","LAT":"lat"},inplace=True)
MergeWeek['Date'] = MergeWeek['Date'].dt.to_period("W").dt.start_time
df_combined['intercept'] = 1 $ lm = sm.Logit(df_combined['converted'], df_combined[['intercept','ab_page','US','CA','US_ind_ab_page','CA_ind_ab_page']]) $ results = lm.fit() $ results.summary()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
dem = pd.read_table("dm01.txt", skiprows = [1], na_values= "-7")
from sklearn.metrics import mean_squared_error,make_scorer
dsg.axes('Z')
adj_close_acq_date_modified = adj_close_acq_date[adj_close_acq_date['Date Delta']>=0] $ adj_close_acq_date_modified.head()
print date_now.isoweekday() $ print date_now.weekday()
names = top_allocs[:10].index.tolist()
network_simulation.info()
c_counts=[] $ for t in t_open_resol: $     i=t['index'][1] $     c_counts.append(data_issues_transitions['comment_count'][i])
merged_NNN.groupby("committee_name_x")
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_key, access_secret) $ api = tweepy.API(auth, retry_count=2, retry_delay=5, $                  wait_on_rate_limit=True, $                  wait_on_rate_limit_notify=True)
output= "SELECT date, sum(retweets) as srt from tweet_details group by date order by srt desc limit 10 " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['Date','Sum Of Retweets'])
tstLoc1 = gMapAddrDat.getGeoAddr(40.699100, -73.703697)  ## default: test=False $ print(type(tstLoc1))                                     ## when called internally to build the address field for $ tstLoc1                                                  ## for outDF, it just returns an address string
model = sm.Logit(reg_df['converted'], reg_df[['intercept', 'ab_page', 'is_US', 'is_UK']])
print 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))
df_image = pd.read_csv('./WeRateDogs_data/image-predictions.tsv', delimiter='\t')
df = pd.DataFrame(X) $ df['labels'] = pd.Series(y) $
mcap_mat.iloc[-2,:].sort_values(ascending=True).tail(c).plot.barh() $ plt.title('Top ' + str(c) + ' today') $ plt.xlabel('Marketcap') $ plt.show()
autos['nr_of_pictures'].head(10)
d_education=detroit_census2.drop(detroit_census2.index[36:]) $ d_education=d_education.drop(d_education.index[:34]) $ d_education.head()
df.columns
df_temp = df.query('landing_page == "new_page" & group == "treatment"') $ df2 = df.query('landing_page == "old_page" & group == "control"') $ df2 = df2.append(df_temp)
to_be_predicted_Day1 = 25.18 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
lm = smf.ols(formula='vFA ~ IP', data=fx).fit() $ lm.params $ lm.summary()
question_1_dataframe = data_2017_subset[data_2017_subset['complaint_type'].isin(top_10_complaint_types.index)] $ len(question_1_dataframe)
last_12_precip = session.query(Measurement.date, Measurement.prcp).\ $ filter(Measurement.date >= '2016-08-24').filter(Measurement.date <= '2017-08-23').order_by(Measurement.date).all() $ last_12_precip
countdf = pd.DataFrame(kochbar03) $ countdf = countdf.drop_duplicates(subset=['name', 'user']) $ countdf.info()
toRG = rGraphData.groupby(['to', 'from']) $ toRG.sum().head()
c.index = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j"] $ c
sel = [Measurement.station, func.count(Measurement.station)] $ stations_and_tobs = session.query(*sel).group_by(Measurement.station).order_by(-func.count(Measurement.station)).all() $ stations_and_tobs
import pandas as pd $ url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.xlsx' $ df = pd.read_excel(url, sheet_name=0, header=1) $ df.head(2)
ax = cell_df[cell_df['Class'] == 4][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='DarkBlue', label='malignant'); $ cell_df[cell_df['Class'] == 2][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='Yellow', label='benign', ax=ax); $ plt.show()
os.chdir('object_detection')
Image('https://secure.gravatar.com/avatar/f85b7564fd35d5c86054b95090052d94.jpg?s=192&d=https%3A%2F%2Fa.slack-edge.com%2F7fa9%2Fimg%2Favatars%2Fava_0025-192.png')
match[match.iloc[:,55 :66].notnull().any(axis=1)].iloc[:5,55 :66] # get rows with non-nulls in columns 55 :66
flux = sp.get_tally(name='flux') $ flux = flux.get_slice(filters=[openmc.CellFilter], filter_bins=[(fuel_cell.id,)]) $ fuel_rxn_rates = sp.get_tally(name='fuel rxn rates') $ mod_rxn_rates = sp.get_tally(name='moderator rxn rates')
closePrice.nlargest()[0]
df.groupby([df.index.month, df.index.day]).size().plot() $ plt.show()
model = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA', 'US']]) $ sol = model.fit() $ sol.summary()
state_list = list(cand_date_df['state'].unique()) $ state_list
import csv $ data = list(csv.reader(open("test_data//my_data.csv")))
house = pd.merge(legHouse.reset_index(), openHouse.reset_index(), how='left', on=['first_name', 'last_name'])
df_final.head(5)
datatest.loc[datatest.floor > 100, 'floor'] = np.NaN
df_repub = df[df.party == 'Republican']
twitter_archive_clean.head(1)
df_new.loc[ df_new['followers'] >= 200 , 'score_followers_1'] = 15 $ df_new.loc[ (df_new['followers'] < 200) & (df_new['followers'] >= 50) , 'score_followers_1'] = 10 $ df_new.loc[ (df_new['followers'] < 50) & (df_new['followers'] >= 30) , 'score_followers_1'] = 5 $ df_res['score_followers'] = df_new['score_followers_1'] $ df_res $
station_query = session.query(Measurement.station).group_by(Measurement.station) $ station_result = pd.read_sql(station_query.statement, station_query.session.bind).count() $ print("There are ", station_result.station , 'station')
law = tc_final $ law['YBP sub-account'].replace("195099", "590099", inplace= True) $ law
(autos["date_crawled"] $  .str[:10] $  .value_counts(normalize=True, dropna=False) $  .sort_index().tail() $ )
df = pd.DataFrame(likes.groupby(by='title').size()) $ df.columns = ['counts'] $ df.sort_values(by='counts', ascending=False)
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK','ab_page']]) $ results = mod.fit() $ results.summary()
train_target_vecs = proc.transform(target_docs) $ hidden_states = embedding_model.predict(train_target_vecs[:, 1:])
df.query('converted == "1"').user_id.nunique() / len(df['converted'])
trump['num_punc']= trump['num_punc'].apply(lambda x: np.log(x))
f_model_pr = sm.Logit(df_new["converted"],df_new[["intercept","uk","us","treatment","page_country"]]) $ f_result_pr = f_model_pr.fit() $ f_result_pr.summary2() $
pp.barplot(df=df, filters={'technology': tecs, 'variable': ['CAP_NEW']}, $            title='CAP_NEW - light')
training_set.info()
df['y'].plot.hist(bins=15)
model = gensim.models.doc2vec.Doc2Vec(size=200, min_count=3, iter=200) $ model.build_vocab(tag_tokenized) $ model.train(tag_tokenized, total_examples=model.corpus_count, epochs=model.iter) $ print(model.docvecs[10])
dfFull['BsmtFinSF1Norm'] = dfFull['BsmtFinSF1']/dfFull['BsmtFinSF1'].max()
df_2018 = df[df.year==2018] $ predicted_talk_indexes = predicted_talks_vector.nonzero()[0] + len(df[df.year==2017]) $ df_2018_talks = df_2018.loc[predicted_talk_indexes]
print ("Data Frame with Forward Fill:") $ df2.reindex_like(df1,method='ffill')
df4.describe()
fav_max = ... $ fav_tweet = ... $ print("The tweet with more likes is: \n{}".format(data['Tweets'][fav_tweet])) $ print("Number of likes: {}".format(fav_max)) $ print("{} characters.\n".format(data['len'][fav_tweet]))
top_songs['Position'].isnull().sum()
basicmodel = LogisticRegression() $ basicmodel = basicmodel.fit(basictrain, train["rise_nextday"])
joblib.load(X_train, 'X_train_glove.pkl')
ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'], $     as_index=False $ ).count()
p0_old = df2.query("converted==1 & group=='control'").shape[0]/df2.query("group=='control'").shape[0]
os.getcwd()
n_old = ab_file2[ab_file2['landing_page']=='old_page'].shape[0] $ print(n_old)
breed_predict_df_clean = breed_predict_df_clean[breed_predict_df.p1_dog == True]
obj.unique()
B4JAN16['Contact_ID'].value_counts().sum() $
tweet_json_clean.info()
count_cols = [col_str for col_str in list(merged_feature_df.columns) if col_str.endswith('_count')] $ merged_feature_df[count_cols] = merged_feature_df[count_cols].fillna(0)
Line_Treatement=df.loc[(df['landing_page']=='new_page') & (df['group']=="treatment"),].shape[0] #Find no of entries where they do line up $ Line_Control=df.loc[(df['landing_page']=='old_page') & (df['group']=="control"),].shape[0] $ print("The total no of times new page and treatment dont line up are",Total_entried-Line_Upentries-Line_Control)
adopted_cats['Neutered'] = adopted_cats['Sex upon Outcome'].str.split().str[0] $ adopted_cats['Female'] = adopted_cats['Sex upon Outcome'].str.split().str[1]
datetime.now().toordinal() - lesson_date.toordinal()
ts.index.dtype
for v in contin_vars: $     joined[v] = joined[v].fillna(0).astype('float32') $     joined_test[v] = joined_test[v].fillna(0).astype('float32')
html_table = df.to_html() $ html_table
df1['K_lbs-Site1'] = df1["AnnualFlow_MGD"] * df1['TotalN'] * 8.34 / 100000 $ df2['K_lbs-Site2'] = df2["AnnualFlow_MGD"] * df2['TotalN'] * 8.34 / 100000 $ df3['K_lbs-Site3'] = df3["AnnualFlow_MGD"] * df3['TotalN'] * 8.34 / 100000
most_retweeted = tweets.loc[tweets.snspostid.isin(grouped.iloc[:100,:].parentPost.astype(np.str).values)] $ most_retweeted.head(2)
tweet_scores.favorite_count
del merged_portfolio_sp_latest_YTD_sp['Date'] $ merged_portfolio_sp_latest_YTD_sp.rename(columns={'Adj Close': 'SP Start Year Close'}, inplace=True) $ merged_portfolio_sp_latest_YTD_sp['Share YTD'] = merged_portfolio_sp_latest_YTD_sp['Ticker Adj Close'] / merged_portfolio_sp_latest_YTD_sp['Ticker Start Year Close'] - 1 $ merged_portfolio_sp_latest_YTD_sp['SP 500 YTD'] = merged_portfolio_sp_latest_YTD_sp['SP 500 Latest Close'] / merged_portfolio_sp_latest_YTD_sp['SP Start Year Close'] - 1 $ merged_portfolio_sp_latest_YTD_sp.head()
a = description.values.tolist()
test_features = ['usd_goal_real', 'Main_Cat_Nums', 'Sub_Cat_Nums', 'City-Nums', 'State-Nums', 'month_launched', 'Length_of_kick', 'Days_spent_making_campign', 'City_Pop','staff_pick'] $ X = df_master_select[test_features].copy() $ y = df_master_select[['Status']].copy() $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=324)
df['rating_numerator'].value_counts()
df_goog.Close
csv_files = s3.ls(s3_path + 'url_csvs/*.csv')
print(results.summary())
df_user[ $     df_user['collection'] == 'us_media_accounts_2016' $ ]['user.name'].tolist()[:5]
y_predict=[round(ii[0]) for ii in model.predict(x)] $ deviate=[0.5 for aa,bb in zip(y,y_predict) if aa==bb] $ plt.figure() $ plt.plot(y,marker='s',linestyle='') $ plt.plot(deviate,marker='h',markersize=1,linestyle='',color='k') $
res_noise = df[df['Complaint Type'] == 'Noise - Residential'] $ res_noise['Descriptor'].value_counts()
print(click_condition_meta.platform.unique()) # one unique value for the entire dataframe $ click_condition_meta.drop(['platform'], axis = 1, inplace = True)
newpage = (df2.query('landing_page == "new_page"').count()[0]) $ total_page = df2.shape[0] $ print ("The probability of receiving the new page is {}".format(float(newpage) /  float(total_page)))
pd.date_range(start, periods=10, freq='2h20min')
df = pd.read_csv('data/test1.csv', parse_dates=['date']) $ df
joined=load_df('joined_elapsed_events.pkl')
hist(df2.tripduration, bins = 20, label = "Male", normed=1) $ plt.xlabel("Trip Duration in seconds", fontsize=12) $ plt.ylabel("Amount of trips", fontsize=12) $ plt.title("Trip Duration Histogram male users", weight='bold', fontsize=14) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
all_enterprise = lift.get_all_enterprise()
old_page_converted= np.random.choice([1, 0], size=n_old, p=[p_mean, (1-p_mean)]) $ old_page_converted.mean()
np.mean(comps_count['competitor_count'])
import pandas as pd $ dfjson = pd.read_json('itemlist.json') $ dfjson $
sess = tf.Session() $ print(sess.run(result))
autos.info() $ autos.head()
skafos.engine.save(schema, data_out).result()
goo2.index[:3]  
xpdraft1 = pd.merge(xpdraft, orgName, left_on='orgId', right_on='orgId', how='left') $ xpdraft1.head()
twitter_data[twitter_data.tweet_id.duplicated()]
np_places = np.array(countries)
p_diff_sim = np.mean(new_page_converted) - np.mean(old_page_converted) $ p_diff_sim
fuel_mgxs = mgxs_lib.get_mgxs(fuel_cell, 'nu-fission')
rng.tz
snow.select("select count(distinct patient_id) from nk_albatross_ftd where left(patient_id, 5) != 'XXX -' and patient_id in (select distinct patient_id from nk_albatross_psp)")
weblogs.count()
prob_control = df2.query('group == "control"').converted.mean() $
sns.barplot(words, importance)
twitter_df.to_csv("news_mood_df.csv", encoding="utf-8")
testObj.buildOutDF(tst_lat_lon_df[600:])  ## end of the df added in $
p_old_actual=df2.query('group=="control"').converted.mean() $ p_new_actual=df2.query('group=="treatment"').converted.mean() $ actual_diff=(p_new_actual - p_old_actual) $ actual_diff
auth = tweepy.OAuthHandler(consumerKey, consumerSecret) $ api = tweepy.API(auth)
print("Number of Groups in ATT&CK") $ groups = lift.get_all_groups() $ print(len(groups)) $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
sns.barplot(x="MILLESIME", y="target", hue="PROBLEM_CODE", data=df)
td_norm = td ** (10/11) $ td_norm = td_norm.round(1)
pd.DataFrame(data.target_names)
df_precep_dates_sorted_12mo = df_precep_dates_12mo.sort_values(by=['date']) $ df_precep_dates_sorted_12mo
def sentlex_analysis(text): $     SWN = sentlex.SWN3Lexicon() $     classifier = sentlex.sentanalysis.BasicDocSentiScore() $     classifier.classify_document(text, tagged=False, L=SWN, a=True, v=True, n=False, r=False, negation=False, verbose=False) $     return classifier.resultdata
X = pj.convert_to_df(scaling=False, filtered=True)
df.head()
df.isnull().any(axis=1).any()
engine.execute('SELECT * FROM Station LIMIT 5').fetchall()
acc.find(description='DeScription 12')
merged1.columns
url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest' $
emotion_big_df.head()
autos['price'] = autos.price.str.replace('$','').str.replace(',','').astype(float)
for p in paragraphs: $     print(etree.tostring(p, pretty_print=True, method='html'))  
from dask.diagnostics import ProgressBar $ with ProgressBar(): $     d.compute()
lm=sm.Logit(df_new['converted'],df_new[['intercept','ab_page','country_UK','country_US']]) $ r=lm.fit()
from sklearn.preprocessing import StandardScaler  $ scaler = StandardScaler() $ scaler.fit(x_train)  $ x_train = scaler.transform(x_train) $ x_test = scaler.transform(x_test)
df[df['Duration'] < 60*45]['Duration'].plot.hist(bins=30)
ca = categorical_cv(feature_params['category_nfold']) $ train_df,test_df,fea_categorical = ca.transform(train_df,test_df)
top10_no_go_areas = mean_age_per_component.sort_values('mean', ascending=False).head(10) $ top10_no_go_areas
latestTimeByUser = firstWeekUserMerged.groupby('userid')['time_stamp2'].max().reset_index(name="recentTime") $ latestTimeByUser.head(5)
df_con.shape
data = raw.copy() $ data = data.div((data['Close']/data['Adj Close']), axis=0) $ del data['Adj Close'] $ del data['Volume'] $ data.head()
df = PredClass.df_model $ print df.dtypes.loc[df.dtypes == 'object']
new_page_converted = np.random.choice([0,1],size = nnew,p=[1-rate_pnew_null,rate_pnew_null]) $ print(len(new_page_converted))  
tweets_df.favorite_count.describe()
Q3.plot()
notes.isnull().sum()
BID_PLANS_df.loc['e08b8a17']
clean_users['creation_source'].value_counts()
!g++ test_cpp_cuda_codes/hello-world.c $ !./a.out
hits = baseball_newind.h $ hits
old_page_converted = np.random.binomial(n_old,p_old)
df.query('(group=="treatment" and landing_page!="new_page") or (group!="treatment" and landing_page=="new_page")').count()
print (r.json())
df['EST'] = pd.to_datetime(df['EST'])
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
taxi_hourly_df = taxi_hourly_df.drop([pd.Timestamp('2014-11-02 01:00:00'), pd.Timestamp('2015-11-01 01:00:00'), pd.Timestamp('2016-11-06 01:00:00'), pd.Timestamp('2017-03-12 02:00:00')]) $ taxi_hourly_df.index = taxi_hourly_df.index.tz_localize('America/New_York')
k = 4 $ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train) $ neigh
np.exp(log_mod_interact_results.params)
StockData = StockData.set_index('Date') $ StockData.head()
prob_kNN500x = kNN500.predict_proba(Test_extra) $ prob1_kNN500x = [x[1] for x in prob_kNN500x] $ Results_kNN500x = pd.DataFrame({'ID': Test.index, 'Approved': prob1_kNN500x}) $ Results_kNN500x = Results_kNN500x[['ID', 'Approved']] $ Results_kNN500x.head()
join_e = join_d.join(join_b,'party_id_orig')['party_id_orig',"aggregated_prediction",'highest_match','predicted']
cityID = '161d2f18e3a0445a' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Raleigh.append(tweet) 
cats_df['age at death'] = cats_df['age at death'].apply(lambda x: np.nan if x == 0 else x) $ cats_df[cats_df['age at death'].isnull()]
id_range = range(baseball.index.values.min(), baseball.index.values.max()) $ baseball.reindex(id_range).head()
def getStockPrice(tickr, startYear, startMonth, startDay, endYear, endMonth, endDay): $     start = datetime.datetime(startYear, startMonth, startDay) $     end = datetime.datetime(endYear, endMonth, endDay) $     readData = dr.DataReader(tickr, "yahoo",start,end) $     return pd.DataFrame(readData) $
summed.fillna(summed.mean())  # each column received its respective mean. The NaN column is untouched.
BroncosBillsPct.head()
df2.query("user_id == {}".format(duplicated_user.iloc[0]["user_id"])) $
writing_commit_df = commit_df.query("(characters_added > 0 or characters_deleted > 0) and merge == 0") $ stats['manuscript_commits'] = len(writing_commit_df)
x_train_cat = snowshoe_df[msk] $ snowshoe_prob = x_train_cat[['month','snowshoes']].groupby(['month']).mean() $ test_month_list = snowshoe_df.month[~msk].tolist() $ y_pred_baseline = snowshoe_prob.snowshoes[test_month_list]
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page','UK','US']]) $ results = lm.fit() $ results.summary()
cats_df[cats_df['date of last vet visit'].isnull() == True]
house_data.tail(20)
import pandas as pd $ pd.read_html("HistoricalNASDAQ.htm",parse_dates=True,header=0,index_col=0)[0]
weather = pd.read_csv('weather.csv', sep=',', parse_dates=['date'], $                       infer_datetime_format=True,low_memory=False)
num_treatment_oldpage = len(df.query('group == "treatment" and landing_page == "old_page"')) $ print('Number of times users of the treatment group landed on the old page: ',num_treatment_oldpage) $ num_control_newpage = len(df.query('group == "control" and landing_page == "new_page"')) $ print('Number of times users of the control group landed on the new page: ',num_control_newpage) $ print('Number of times treatment and new_page do not line up: ',num_treatment_oldpage+num_control_newpage)
plt.hist(shows['imdb_rating'].dropna()) $ plt.title('Distribution of IMDB Scores') $ plt.ylabel('Frequency') $ plt.xlabel('Score')
pd.concat([s1, s2, s3], axis=0, keys=['one', 'two', 'thr'])
saver.restore(sess,tf.train.latest_checkpoint('/tmp/testing/stock_prediction_12_21/'))
from PutModelData import get_game_results, get_boxscores, get_gamesheets, get_season_game_stats $ GameResults = get_game_results() $ GameResults.head()
final_data.to_pickle('D:/CAPSTONE_NEW/jobs_data_final.pkl') $ gc.collect()
all_data.reindex(columns=['date', 'country', 'variable', 'totals']).head()
sdf = feature_set.df $ sdf.head()
for v in contin_vars: $     joined[v] = joined[v].astype('float32') $     joined_test[v] = joined_test[v].astype('float32')
tfidf_vect = TfidfVectorizer(stop_words="english") $ tfidf_mat = tfidf_vect.fit_transform(PYR['soup'].values)
df.to_sql(name='coindesk',con=engine,dtype=None)
donations["Donation Received Date"] = donations["Donation Received Date"].astype("datetime64[ns]")
USER_PLANS_df.loc[str(np.array(['e08b8a17', 'f6dd6544']))]
tips.sex = tips.sex.astype('category') $ tips.smoker = tips.smoker.astype('category') $ print(tips.info()) $
df = pd.read_feather(f'{PATH}df')
idx = pd.isnull(geocoded_df[['Case.File.Date','Judgment.Date']] $                ).apply(lambda x: not (x['Case.File.Date'] or x['Judgment.Date']), axis=1)
pi = 22/7 $ f = np.linspace(0.0, pi, num=100, endpoint = True)    #creating array of 100 equally spaced numbers between o and pi $ f = np.sin(f)      #printing sin of the values $ print("f: ", f)
plt.figure(figsize=(20,6)); $ plt.plot(df['datetime'],df['MeanFlow_mps'],linewidth=0.5,color='orange') $ plt.ylim((0,700)) $ plt.show;
pd.NaT
tweet_df_raw = pd.read_json('tweet_json.txt', lines = 'True') $ tweet_df = tweet_df_raw[['id', 'retweet_count', 'favorite_count']]
rating_and_retweet = twitter_archive_clean.copy() $ rating_and_retweet = rating_and_retweet.sort_values(by='score') $ rating_and_retweet = rating_and_retweet[:int(len(rating_and_retweet)*0.9)]
groupedvalues=jobs.loc[(jobs.FAIRSHARE == 1) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'p_meiler')][['Memory','Wait']].groupby('Memory').agg(['mean', 'median','count']).reset_index()
data_issues.head()
seats_per_hour.fillna(0, inplace=True)
df_merge = archive_clean.merge(image_clean,how='left',on = 'tweet_id',) $
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', usecols=['Unique Key','Created Date','Closed Date','Agency','Agency Name','Complaint Type','Descriptor','City'])
(temp_df.email == '\\N').sum() + (temp_df.email != '\\N').sum()
opening_price = [] $ for ele in r.json()['dataset']['data']: $     if ele[1] != None: $      opening_price.append(ele[1]) $ print(opening_price)
df_tick = df_tick.loc['2017-08-24':'2017-08-30']
es.indices.delete(index='index_basket', ignore=[400, 404]) $
print(np.info(np.random.random)
feature_importances(model, feature_matrix.columns, n=25)
allNames = list(df['sn'].unique())
X = firstWeekUserMerged.groupby('userid')["action"].count().reset_index(name="totalAct") $ print(X.head(5)) $ print(X.shape) $ assert X.shape[0], numUsers
raw = raw[raw.find("I--DOWN THE RABBIT-HOLE"):raw.rfind("End of the Project Gutenberg EBook")] $ raw.find("I--DOWN THE RABBIT-HOLE") #Trimmed book text
print("The largest change in one day is %3.3f."%(abs(df.High - df.Low).max()))
df_l_s.head()
extract_deduped.loc[(extract_deduped.APP_PROMO_CD=='SFL418PA') $                    &(extract_deduped.app_branch_state=='CA')].groupby('APPLICATION_DATE_short').size()
df.info()
new_page_converted = np.random.binomial(n_new, p_new) $ print(new_page_converted)
from ncbi_remap.io import remove_chunk, add_table
token = token[~ token.receiver.isin([i for i in token.receiver.unique() if bool(re.search("[^0-9]", str(i)))])] $ token = token[~ token.sender.isin([i for i in token.sender.unique() if bool(re.search("[^0-9]", str(i)))])] $ token = token.dropna() $ token.index = range(len(token)) $
coefs = pd.DataFrame(gs.best_estimator_.coef_[0], index=X_train_all.columns, columns=['coef']) $ coefs['coef'] = np.exp(coefs['coef']) $ coefs = coefs.sort_values(by='coef', ascending=False) $ coefs.head()
todrop2 = df.loc[(df['group'] == 'control') & (df['landing_page'] == 'new_page')].index
most = dfrecent.head(50)
sns.set(font_scale=1.5) $ serfeatures = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=True) $ _ = serfeatures.plot(kind='barh') $ _ = plt.title('Relative Feature Importance')
df.index[0]
print(model_arima121.aic,model_arima121.bic,model_arima121.hqic)
import pandas as pd $ import numpy as np $ rng = np.random.RandomState(42) $ ser = pd.Series(rng.randint(0, 10, 4)) $ ser
airline_df.to_csv('unitedAIRLINES_classified.csv', index=False)
len(feat_imp), X_train.shape
gMapAddrDat.set_endRow_OutDf_caching(3)
countries = pd.read_csv(r"D:\courses\Nanodegree\term1\statistics\AnalyzeABTestResults\countries.csv") $ df4 = pd.merge(df3,countries, how= 'inner',left_on="user_id" , right_on= "user_id" ) $
prices.fillna(method='ffill', inplace=True) $ prices.iloc[:, 0:10].head()
df_no_cat = df_onc_no_metac.drop(columns = ls_other_columns)
session_v2.head()
fps = DirFileMgr(dr_mod_id_str) $ fps.create_all_modeling_fps(dr_mod_id_str) $ fps.add_fp('pyLDAvis')
BID_PLANS_df.loc[aliases]
a = np.array([[1., 2.], [3., 4.]]); a
df_3.keys()
m.sched.plot(100)
df[['Principal','terms','age','Gender','education']].head()
print(len(distance_list)) $ station_distance.shape
users.dtypes
monthlyDF.plot(figsize=(20,6),legend=True);
station_count.sort_values(['Count'],ascending=False, inplace=False, kind='quicksort', na_position='last')
df_prep7 = df_prep(df7) $ df_prep7_ = pd.DataFrame({'date':df_prep7.index, 'values':df_prep7.values}, index=pd.to_datetime(df_prep7.index))
creations.ix[null_reg.index, "creator_autoconfirmed"] = True $ creations[ creations["user_registration"].isnull() ]["creator_autoconfirmed"]
full_globe_temp.plot()
subwaydf.iloc[10874:10880] #this low number seems to be because entries and exits resets
output.to_csv("Word2Vec_svm.csv", index=False, quoting=3)
model.add(Flatten()) $ model.add(Dropout(rate=0.25))
df.info()
week40 = week39.rename(columns={280:'280'}) $ stocks = stocks.rename(columns={'Week 39':'Week 40','273':'280'}) $ week40 = pd.merge(stocks,week40,on=['280','Tickers']) $ week40.drop_duplicates(subset='Link',inplace=True)
rate = survey[survey.question.str.contains('Please rate this assessment from one to five, five being the highest')].copy()
twitter.head(4)
ufos_df2 = spark_df.withColumn("year", extract_year(spark_df['Reports']))
word_freq_df.tail()
cust_data.loc[1:10,['ID', 'DebtRatio']]
doesnt_meet_credit_policy = loan_stats['loan_status'].grep(pattern = "Does not meet the credit policy.  Status:", $                                                      output_logical = True)
sample = rng.choice(sizes, sample_size, replace=True) $ print(f'Mean (one sample) = {np.mean(sample):5.3f}') $ print(f'Standard deviation (one sample) = {np.std(sample):5.3f}')
df = df.assign(hmax = lambda row: hg+row.ht) $ df = df.assign(hmax_sim = lambda row: row.hmax/row.h0) $ df = df.assign(cv_sim = lambda row: row.cv*1.175) $ df['W'] = 33 $ df['dt'] = 0.1
tdList = table.findAll('td')
StockData.head()
print(df.shape) $ df.dtypes
sel = [Measurements.date, Measurements.prcp] $ initial_date = format_latest_date - timedelta(days=365) # This will be start date from 12 months before final date of 8/23/17 $ prcp_data = session.query(*sel).\ $     filter((Measurements.date >= initial_date)).all() $ prcp_data[:10] # Display the records
df_final_edited.info()
access_logs_df.registerTempTable("access_logs_df") $ new_access_logs_df = spark.sql('SELECT dateTime, endpoint FROM access_logs_df WHERE responseCode = 403').show(20,False)
from pyspark.sql.types import FloatType $ floatCol = a.cast(FloatType()) $ df.select(floatCol / 100).show(5)
df2['is_duplicated'] = df2.duplicated('user_id') $ df2.query('is_duplicated==True')['user_id']
project_size=pd.value_counts(ac['Project'].values, sort=True, ascending=False) $ project_size.head(20)
!du -h test.csv
to_be_predicted_Day2 = 26.32 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
X_train_feature_counts = count_vect.fit_transform(train['feature_list'])
df.columns.values
s = pd.Series(np.random.randn(4)) $ s
cols_to_keep = ['Churn','gender', 'SeniorCitizen', 'Partner','Dependents','tenure','PhoneService','PaperlessBilling','MonthlyCharges'] $ data = df[cols_to_keep].join(dummy_IntServ.loc[:, 'IntServ_Fiber Optic':]) $ data = data.join(dummy_Contract.loc[:, 'Contract_One year':]) $ data = data.join(dummy_PayMethod.loc[:, 'PayMethod_Credit card (automatic)':]) $ print(data.head())
preds = model.predict(x_test)
SCN_BDAY['start_date'] = pd.to_datetime(SCN_BDAY['scns_created'].apply(lambda x:x[np.argmin(x)])).dt.strftime('%Y-%m-%d')
call.loc['2018-03-16']
autos.describe(include='all')
yhat = LR_model.predict(X_test) $ yhat
dic = {} $ for field in Fields: $     dic[field] = Fields_df.groupby(['Field']).get_group(field)
print('Rows: ', df.shape[0], '\nColumns: ', df.shape[1])
converted_treatment = df2.query('group == "treatment"')['converted'].mean() $ print("Given that an individual was in the control group, the probability they converted is{0: .4} ".format(converted_treatment))
accuracy_np(*m3.predict_with_targs())
potential_name = '1999--Mishin-Y--Ni--LAMMPS--ipr1' $ potential_file = os.path.join(librarydir, 'potential_LAMMPS', potential_name) + '.json' $ potential_dir = os.path.join(librarydir, 'potential_LAMMPS', potential_name) $ potential = lmp.Potential(potential_file, potential_dir) $ print('Successfully loaded potential', potential)
index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it
df.drop(df.loc[(df["group"]!="treatment") & (df["landing_page"]=="new_page") ,  ["group","landing_page"]].index, inplace=True) $ df.drop(df.loc[(df["group"]=="treatment") & (df["landing_page"]!="new_page") ,  ["group","landing_page"]].index, inplace=True)
definition_url = client.repository.get_definition_url(definition_details) $ definition_uid = client.repository.get_definition_uid(definition_details) $ print(definition_url)
dfleavetimes.info()
n_old = len(df2.loc[~new_page]) $ n_old
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page','UK','US']]) $ results = logit_mod.fit() $ results.summary()
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)])
model = gl.item_similarity_recommender.create(train_data, user_id = 'hacker_id', item_id= 'challenge_id', similarity_type='jaccard')
temp_pred.dtypes
df.groupby('Park Borough').count().sort(desc("count")).show(10)
utility_patents_subset_df['number-of-figures'].describe()
cluster_label_group = df.groupby("cluster_labels").mean()
1/np.exp(result2.params)
grouped = data[['float_time','Agency']].groupby('Agency') $ grouped.mean().sort_values('float_time',ascending=False)
tweets_predictions_all['source'].value_counts()
print("dfDay = ",dfDay['Contract Value (Daily)'].sum(), "dfDay Project Count = ", dfDay['Project Name'].nunique())
lm=sm.Logit(df2['converted'],df2[['Intercept','ab_page']]) $ results=lm.fit()
result[result['uid'].notnull()].head()
%%time $ model = GradientBoostingClassifier() $ model.fit(train_x, train_y)
def day_of_week(date): $     days_of_week = {0: 'monday', 1: 'tuesday', 2: 'wednesday', 3: 'thursday', 4: 'friday', 5: 'saturday', 6: 'sunday'} $     return days_of_week[date.weekday()]
res=driver.find_elements_by_css_selector('div.conference-item')
crimes.drop('LOCATION', axis=1, inplace=True)
X_train = train.drop('label', axis=1) $ Y_train = train['label'] $ X_test = test.drop('label', axis=1) $ Y_test = test['label']
bwd.head(20)
datatest.loc[datatest.place_name == "Altos de Hudson I",'lat'] = -34.862623 $ datatest.loc[datatest.place_name == "Altos de Hudson I",'lon'] = -58.168622
inter1[(inter1.user_id == 806282) & (inter1.item_id == 387909)].sort_values(by = 'created_at_date')
'Mean ROC_AUC score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
cr_under_null = df2['converted'].mean()    # Same as above: under the null, use both groups. $ cr_under_null $
archive_clean[archive_clean['retweeted_status_id'].notnull()]
close = panel_data.ix['Close'] $ all_weekdays = pd.date_range(start=start_date, end=end_date, freq='B') $ close = close.reindex(all_weekdays) $ close.tail(10)
df2.query('user_id == 773192')
import autosklearn.regression $ import sklearn.model_selection $ import sklearn.metrics $ X_train, X_test, y_train, y_test = \ $     sklearn.model_selection.train_test_split(features.as_matrix(), labels.as_matrix(), random_state=1) $
weather.head()
metrics.confusion_matrix(y_test, y_pred_class)
predictions.printSchema()
twitter_archive_clean[twitter_archive_clean.expanded_urls.isnull()]
autos['last_seen'].str[:10].value_counts().sort_index()
capa2017offshore.head()
%run -i 'graph.py'
aggregated_data = pd.DataFrame(mean_mileage,columns=['mean_mileage'])
strategy.evaluate_returns(plot = False);
images_df.head()
users['verified'].value_counts(dropna=False)
age.iloc[0:3]
ts_split_under = TimeSeriesSplit(n_splits=3).split(X_train)
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
sample.asfreq("H", method="ffill")
df2.sort_values('No.', ascending=False)
df.iloc[1:3, [0, 2]]  # Selecting by position
log_reg_over.score(X_test, y_test_over)
brand_counts = autos['brand'].value_counts(normalize=True) $ brand_counts
p_diffs = np.array(p_diffs) $ p_diffs
station_df = pd.read_sql(session.query(Measurement.tobs).filter(Measurement.station == 'USC00519281', Measurement.date >= '2016-08-23').statement,session.bind) $ station_df.head()
plots.proto_distribution(traffic_type = 1,scale = "log") #Malicious traffic log scale
for row in selfharmm_topic_names_df.iloc[1]: $     print(row)
BNB = SklearnClassifier(BernoulliNB()) $ BNB.train(train_set)
df_yt['yt_id'] = df_yt['url'].apply(yt.strip_video_id_from_url)
y_tr = target_data['Prediction'].values $ plt.hist(y_tr,bins=2000) $ plt.xlim((0,10000))
remove_list = archive_clean[(archive_clean['rating_numerator'] == 0) | (archive_clean['rating_numerator'] == 1)].tweet_id $ for i in remove_list: $     print(i) $
suspects_27_with_1_loc = (suspects_with_1T_27.groupby('imsi')['lat'].unique().apply(len) == 1).index.values
Helper().which_parameters_does_on_the_fly_selector_have()
missing_values = missing_values_table(perf_test) $ missing_values.head(20)
df_test_index = pd.DataFrame() $
print("Score:", metrics.r2_score(stock.iloc[:-1].target, stock.iloc[:-1].forecast)) $ print("MSE:", metrics.mean_squared_error(stock.iloc[:-1].target, stock.iloc[:-1].forecast))
prices.drop(cols_to_drop, axis=1, inplace=True) $ prices.iloc[:, 0:10].head()
def getjson(url): $     req = requests.get(url) $     specification = req.json() $     return specification
unique_desc_reg=class_merged_hol['description_regional_hol'].unique() $ print (unique_desc_reg)
data.text.notnull().resample("1T").sum().plot()
so.iloc[a, [5, 10, 11]].head()
clean_fb_tokens = [word.lower() for word in fb_tokens if word not in string.punctuation]
pd.read_pickle('data/city-util/proc/city.pkl', compression='bz2').head()
import requests $ response = requests.get("http://api.open-notify.org/iss-now.json") $ print(response.status_code)
october = sentiments[(sentiments.created_at < '2017-11-01')] $ november = sentiments[(sentiments.created_at > '2017-10-31') & (sentiments.created_at <= '2017-11-30')] $ december = sentiments[(sentiments.created_at > '2017-11-30')]
%load_ext sql $ %config SqlMagic.autocommit=False
print('Number of unique users in the dataset is {}.'.format(df.user_id.nunique()))
for index, row in im_clean.iterrows(): $     assert(any(letter.isupper() for letter in row["p1"]) == False) $     assert(any(letter.isupper() for letter in row["p2"]) == False) $     assert(any(letter.isupper() for letter in row["p3"]) == False)
anomaly_df.apply(lambda row: store_news_1day(row.Ticker, row.Date, stored_list=news_list, verbose=True), axis=1 )
ibm_hr_final2 = ibm_hr_final.join(ibm_hr.select("Attrition")) $ ibm_hr_final2.printSchema()
df_master = pd.merge(archive_copy, images_copy, how = 'left', on = ['tweet_id'] ) $ df_master = pd.merge(df_master, api_copy, how = 'left', on = ['tweet_id']) $ df_master.info()
os.environ.get('FOO')
df_never_moved.to_csv('never_moved.csv')
data.shape
print("Percentage of merchants not churning: %s %%" % "{0:.3}".format(100*start/(start+trial)))
Baseline_training_RMSE = mean_squared_error(df_M7.ACTUAL,df_M7.BASELINE) ** 0.5
tweet_image_prediction_url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(tweet_image_prediction_url) $ with open('image_predictions.tsv', mode = 'wb') as file: $     file.write(response.content)
fullDF = pd.concat([tweets_streamedDF, tweetsDF], ignore_index = True)
mobile = nvidia.filter(lambda p: devices[int(p['adapter']['deviceID'], 16)].endswith("M")) $ mobile.map(lambda p: len(p['adapters'])).countByValue()
commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count() $ commits_per_year.columns = ['commits'] $ commits_per_year.head()
A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')]) $ A
df.index
df.iloc[0:5,:12]
label_encoder = preprocessing.LabelEncoder() $ df1['loan_status'] = label_encoder.fit_transform(df1['loan_status']) $ y_loadtest = df1['loan_status'].values
new_page_converted = np.random.binomial(1, prop_users_converted, n_new)
pytz.timezone('America/Chicago')
from oauth2client.client import GoogleCredentials $ creds = GoogleCredentials.get_application_default()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=4000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df_pageviews_mobile_app.head()
soup.find_all('div', class_='schedule-container')[0].select('.disabled')
r1.keys()
speeches_df3['text'] = [text.replace('\n', '').replace('  ', '') for text in speeches_df3['text']]
from sklearn.linear_model import LogisticRegression $ from sklearn.ensemble import RandomForestClassifier $ clf_A = LogisticRegression(random_state = 42) $ clf_A.fit(X_train, Y_train) $ print(clf_A.score(X_test, Y_test))
s = pd.Series(np.random.randn(10000)) $ s.plot(kind='kde', color='b') 
grouped.size().unstack().fillna(0).plot(kind="bar", stacked=True, figsize=(10,6)).legend(loc='center left', bbox_to_anchor=(1, 0.5)) $ plt.show()
response = requests.get("http://api.open-notify.org/astros.json") $ pd.read_json(response.content)
ratings.groupBy("movieId", "userId").count().filter("count != 1").show()
turnaround_planes_df=turnaround_planes.toPandas() $ turnaround_planes_df
n_old = df2[df2.group == "control"].count()[0] $ print("The population of user under treatment group: %d" %n_old)
print('atoms.atype ->', atoms.atype) $ print('atoms.pos[2] ->', atoms.pos[2])
vol.resample('D', how='sum')
constructor = pd.DataFrame(bmp_series,columns=['mileage'])
plt.show()                                                                   # displays plot
result.head()
sum(insertid_freq.values())
van_final['revtime'] = pd.to_datetime(van_final['revtime'])
params = {'figure.figsize': [4,4],'axes.grid.axis': 'both', 'axes.grid': True, 'axes.labelsize': 'Medium', 'font.size': 12.0, \ $ 'lines.linewidth': 2} $ plot_partial_autocorrelation(series=dr_num_new_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of dr number of new patients')) $ plot_partial_autocorrelation(series=dr_num_existing_patients.diff()[1:], params=params, lags=30, alpha=0.05, title='PACF {}'.format('first difference of dr number of existing patients'))
options_frame.info()
mask_with_labels = tf.placeholder_with_default(False, shape=(), $                                                name="mask_with_labels")
trading.hedge_ratio_ols(window=30)
ind.head(2)
df.groupby("cancelled")[["days_to_pickup", "trip_duration"]].mean()
all_states = pd.merge(states, pd.DataFrame(df.state.unique(), columns=['state']), on='state', how='right') $ invalid_states = all_states[pd.isnull(all_states.id)].state
contractor_clean.shape 
np.exp(-.0099), np.exp(-.0507)
twitter_archive_df_clean['stage'].unique()
devices_list = [entity[0] for entity in entity_id_list if entity[0].split('.')[0] == 'device_tracker'] # Print only the sensors $ devices_list
df_train["num_photos"] = df_train["photos"].apply(len) $ df_train["num_features"] = df_train["features"].apply(len) $ df_train["num_description_words"] = df_train["description"].apply(lambda x: len(x.split(" ")))
df_user[df_user['user.name'].str.contains('marco rubio', case=False)]
df.groupby(by=['landing_page', 'group']).count() $
df = fuel_mgxs.get_pandas_dataframe() $ df
raw_data[['charged', 'objective']].loc[raw_data.charged > 0].groupby('objective').mean()
from sklearn.ensemble import VotingClassifier $ clf_vot_tf = VotingClassifier(estimators=[('rf', clf_RF_tf), $                                           ('lr', clf_LR_tf), $                                           ('rgf', clf_RGF_tf)], voting='soft').fit(X_traincv_tf, y_traincv_tf)
shopping_carts = pd.DataFrame(items) $ shopping_carts
date = [div.find('p').text for div in soup.find_all('div', {'class': 'event-description-dev-metabox'})] $ time = [div.find('p').findNext('p').text for div in soup.find_all('div', {'class': 'event-description-dev-metabox'})] $ location = [div.find('p').findNext('p').findNext('p').text for div in $             soup.find_all('div', {'class': 'event-description-dev-metabox'})]
z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new], nobs=[n_old, n_new], alternative="smaller") $
combined_df['intercept'] = 1 $ logistic_model = sm.Logit(combined_df['converted'], combined_df[['intercept','ab_page', 'CA', 'US']]) $ result = logistic_model.fit()
autos["ad_created"].str[:10].value_counts(normalize = True, dropna = False).sort_values()
local_sample_fractions = sample_fractions.select( $     sample_fractions.project_name, sample_fractions.sample_fraction).collect()
data_full.describe()
len(cvec.get_feature_names()) $
f['converted'].mean()
plt.scatter(X[:, 0], X[:, 1], marker='.')
df = pd.read_csv('twitter_archive_master.csv') $ df['timestamp'] = pd.to_datetime(df['timestamp']) $ df.set_index('timestamp', inplace=True)
df.head()
df_archive["timestamp"].sample(5)
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit.fit()
random_crashes_upsample_df = pd.DataFrame(random.choices(random_crashes_df.values, k=sample_size), columns=list(random_crashes_df.columns)) $ random_crashes_upsample_df
df.columns = ['C/A', 'UNIT', 'SCP', 'STATION', 'LINENAME','DIVISION', 'DATE', 'TIME', 'DESC', 'ENTRIES', 'EXITS']
day_of_year15.to_excel(writer, index=True, sheet_name="2015")
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
train_session_v2 = pd.merge(user_actions,train_session, on='user_id', how='inner')
all_sets.cards["XLN"].columns
total.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2017_3/target_winter2017_night2.csv',index=False)
%matplotlib inline $ sns.violinplot(data=october, inner="box", orient = "h", bw=.03) $
df.sort_values(by='B', ascending=False)
print "Directed Graph Edges: " + str(len(G.edges())) $ print "Directed Multi Graph Edges: " + str(len(multiG.edges())) $ print "Directed Graph Nodes:" + str(len(G.nodes())) $ print "Directed Multi Graph Nodes:" + str(len(multiG.nodes()))
models_dict = visual_recognition.list_classifiers(verbose=True); models_dict
X_new.shape $
drugTree.fit(X_trainset,y_trainset)
destinationZip = destinationZip[['Unnamed: 0', 'ZIPCODE']] $ destinationZip.rename(columns={'ZIPCODE':'zip_dest'}, inplace=True) $ destinationZip.head()
deletes["YBP sub-account"].replace(195099, 590099, inplace= True) $ deletes
nf = 1e-1 $ loss = tf.reduce_mean(tf.squared_difference(Ypred*nf,Y*nf)) $
print('Unique number of users notified: {}'.format(len(atloc_opp_loc['vendorId'].unique())))
trainheadlines = train["text"].values $ basicvectorizer = CountVectorizer() $ basictrain = basicvectorizer.fit_transform(trainheadlines) $ print(basictrain.shape)
print(f"{urls[1]} returns:") $ ux.is_short(urls[1])
store.replace({"PromoInterval": {np.nan: 0}}, inplace=True)
indexed_df = openLastOnly.set_index('TLO_id_x')
cast_data = pd.read_csv('../datasets/mad-men-cast-show-data.csv')
lasso=Lasso(alpha=0.1)
import pandas as pd $ Google_stock = pd.read_csv('./goog-1.csv') $ print('Google_stock is of type:', type(Google_stock)) $ print('Google_stock has shape:', Google_stock.shape)
customer_visitors.DateCol.dt.dayofweek
lgreg = LogisticRegression() $ lgreg.fit(train_data, train_labels)
df.head()
df_group=df2.groupby('group') $ df_group.describe() $ a=0.120386 $ b=0.118808 $
for idx, row in enumerate(mentioned_bills_all): $     if len(mentioned_bills_all.loc[idx, 'cosponsors_by_party'].keys()) > 1: $         mentioned_bills_all.loc[idx, 'cosponsored_by_mt1_party'] = 1 $     else: $         mentioned_bills_all.loc[idx, 'cosponsored_by_mt1_party'] = 0
date_cols = ['CRE_DATE', 'UPD_DATE', 'DATE_RESILIATION', 'DATE_DEBUT', 'DATE_FIN'] $ contract_history = pd.read_csv(data_repo + 'contract_history.csv', dtype={'NUM_CAMPAGNE': object}, parse_dates=date_cols, **import_params)
idx = pd.IndexSlice $ health_data.loc[idx[:, 1], idx[:, 'HR']]
data.filter(data['Height'] > 161).select(data['id']).show()
X = my_data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']].values $ X[0:5]
df_users_products.groupby(["UserID","ProductID"])["Quantity"].sum().reset_index()
def convert_time (time): $     new_time = datetime.datetime.strptime(time, "%Y-%m-%d %H:%M:%S") $     new_time = new_time.strftime('%Y-%m-%d') $     return new_time
train_view.sort_values(by=2, ascending=False)[0:10]
tweet_image.head()
txns.tail()
swPos = companyPos[companyPos['author_id'].str.contains('SouthwestAir') | $                        companyPos['text'].str.contains('SouthwestAir')]
df.insert(2,"Com Rate %", 5) $ df.head(3)
active_ordered = ordered_df.loc[~churned_ord] $
LR = LogisticRegression(C=0.01).fit(X_train,y_train) $ LR
data.shape
tweetsF03 = pd.read_sql_query("SELECT created_at, extracted FROM tweets_info;", connF03, parse_dates=['created_at'] ) $ tweetsF03['created_at'] = tweetsF03['created_at'].dt.tz_localize("UTC").dt.tz_convert("Europe/Berlin") $ print("Number of Tweets: %s" %len(tweetsF03)) $ tweetsF03.head()
p_diffs = [] $ for _ in range(int(1e4)): $     new_pc = np.random.binomial(nnew, p = pnew) $     old_pc = np.random.binomial(nold, p = pold) $     p_diffs.append(new_pc / nnew - old_pc / nold)
num_shutdown_dupes = sum([len(filter(lambda e: e == 'shutdown', t[1])) for t in collected_clientid_reasons]) $ print("Duplicate 'shutdown' pings percentage (by client id): {:.2f}%"\ $       .format(pct(num_shutdown_dupes, ping_count)))
fianl = pd.read_csv('tweets.csv', encoding='ISO-8859-1') $ t = fianl
old_page_converted = np.random.choice([0,1], n_old, p=[1-pold,pold]) $ old_page_converted.mean()
class_merged=pd.merge(class_merged,oil_interpolation,on=['date'],how='left') $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
df_DRGs.to_csv('Unique 592 DRGs.csv')
tm_2050 /= 1000 $ tm_2050_norm = tm_2050 ** (10/11) $ tm_2050_norm = tm_2050_norm.round(1) $ tm_2050_alpha = tm_2050 ** (1/3) $ tm_2050_alpha = tm_2050_alpha / tm_2050_alpha.max().max()
learner.save_encoder('lm5_enc')
predictions = client.deployments.score(scoring_url, scoring_data) $ print("Scoring result: " + str(predictions))
autos["brand"].value_counts(normalize=True).plot(kind="bar", title="Brand", cmap="Blues_r") $ plt.axhline(0.05, c="green")
temperature_df.plot.hist(bins=12)
(df2.groupby('converted')['converted'].count().iloc[1])/len(df2)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4) $ print ('Train set:', X_train.shape,  y_train.shape) $ print ('Test set:', X_test.shape,  y_test.shape)
pd.options.display.max_colwidth = 400 $ data_df[['clean_desc','sent_pola','sent_subj', 'tone']][data_df.tone == 'impolite']
store_items.interpolate(method='linear', axis=1)
df_perhour = data_df.groupby(data_df["date"].dt.hour).mean()
df_unit.head()
print(automl.leader)
import re $ soup.find_all(href = re.compile("/item/main.nhn?"))
embeddings_matrix = sess.run(embeddings)    $
meanLat = datatest[['place_name','lat']].groupby('place_name').agg(np.mean) $ meanLon = datatest[['place_name','lon']].groupby('place_name').agg(np.mean) $
twitter_df_clean['date'] = twitter_df_clean['timestamp'].apply(lambda time: time.strftime('%d-%m-%Y')) $ twitter_df_clean['time'] = twitter_df_clean['timestamp'].apply(lambda time: time.strftime('%H:%M:%S'))
import statsmodels.api as sm $ convert_old = df2.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new = df2.query("landing_page == 'new_page' and converted == 1").shape[0] $ print(convert_old, convert_new, n_old, n_new)
kNN5.fit(X, y)
pd.Series(sales, index=pd.date_range(start='2018-01-01', periods=len(sales), freq='T'))
df4.head(3)
scipy.stats.kruskal(df2["tripduration"], df3["tripduration"])
csv_full = "../Data Files/urinal-data-28-nov_clean.csv" $ if not os.path.isfile(csv_full): $     print("Please Download the csv file ")
results.head()
len(df2.query('converted == 1'))/len(df2)
scaler = StandardScaler() $ X_train_scaled = scaler.fit_transform(X_train.astype(np.float32)) $ X_test_scaled = scaler.transform(X_test.astype(np.float32))
query_geographic = feature_layer.query(where='POP2010 > 1000000', out_sr='4326') $ query_geographic.features[0].geometry
np.exp(stats.coef)
mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys())) $ y_train = mlb.fit_transform(y_train) $ y_val = mlb.fit_transform(y_val)
stat_data['female_percent'] = float(gender_top.iloc[0] / sum(gender_top))
not_lined_up = df_ab[(df_ab['group']=='treatment') & (df_ab['landing_page']!= 'new_page')|(df_ab['group']=='control') & (df_ab['landing_page']!= 'old_page')] $ not_lined_up.count()[0] $
duration_test = flight2.select("stop_duration1") $ duration_test.show() $ duration_test.withColumn('duration_h', when(duration_test.stop_duration1.isNull(), None) $                           .otherwise(hour(unix_timestamp(duration_test.stop_duration1,"HH'h'mm'm'").cast("timestamp")))).show(20)          $
ridgereg = Ridge(alpha=0, normalize=True) $ ridgereg.fit(X_train, y_train) $ y_pred = ridgereg.predict(X_test) $ print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
well_data.drop(columns=['api number', 'water_bbl'], inplace=True) $ well_data.head()
df2['intercept'] = 1 $ logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_test']]) $ results = logit_mod.fit() $ results.summary()
df2[df2.user_id == 773192]
z_values, labels = create_latent(nn_aae.nn_enc, test_loader) $ print(z_values[:,0].mean(), z_values[:,0].std()) $ print(z_values[:,1].mean(), z_values[:,1].std()) $ plt.scatter(z_values[:,0], z_values[:,1], s=1)
pd.date_range(start = '10/23/2016 17:00:10', periods = 13, freq = '%w', normalize =True )
top_supporters['contributor_cleanname'] = top_supporters.apply(combine_names, axis=1)
sns.countplot(x="deck", data=titanic, palette="Greens_d");
dfgts_sorted = dfgts.sort_values(by=['date', 'team'], ascending=True).reset_index()
X2 = PCA(2).fit_transform(X)
print(submission.title)  # Output: the submission's title $ print(submission.score)  # Output: the submission's score $ print(submission.id)     # Output: the submission's ID $ print(submission.url)    # Output: the URL the submission points to           
major_cities_l1_features[0].attributes
train, test = pd.read_csv('train.csv'), pd.read_csv('test.csv')
nlp = spacy.load('en') $ op_ed_articles['full_text_tokenized'] = op_ed_articles['full_text'].apply(lambda x: nlp(x)) $
history = import_all(data_repo + 'intervention_history.csv', history=True)
female_journalists_mention_summary_df = journalists_mention_summary_df[journalists_mention_summary_df.gender == 'F'] $ female_journalists_mention_summary_df.to_csv('output/female_journalists_mentioned_by_journalists.csv') $ female_journalists_mention_summary_df[journalist_mention_summary_fields].head(25)
csv_fn=f'{PATH}tmp/sub.csv'
df.loc[df['edition']=='NBC']
atdist_info_4x_tabledata = atdist_4x_count_prop_byloc.reset_index() $ create_study_table(atdist_info_4x_tabledata, 'locationType', 'emaResponse', $                    location_remapping, atdist_info_response_list)
iris_df = h2o.import_file(path=iris_data_path)
full['<=30Days'] = (full['DaysSinceAdmission'] <= pd.Timedelta('30 days')).astype(np.int)
df = pd.DataFrame(rng.rand(1000, 3), columns=['A', 'B', 'C']) $ df.head() $
hr.rename(columns ={"value1num":"systolic", "value2num" :"diastolic"}, $          inplace=True)
df2.any()
weather = weather.merge(events, left_index = True, right_index = True)
new_article_project.to_csv('data_table/article_project.csv', index = False)
stars = dataset.groupby('rating').mean() $ stars.corr()
now.weekday(), now.isoformat()
plt.hist(p_diffs) $ plt.axvline(x=o_diff, color='red');
people.eval("weight / (height/100) ** 2 > 25")
tweet_info.head()
jail_census.groupby('Race')['Age at Booking'].mean()
url_hemispheres = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(url_hemispheres)
is_business_id_unique = all([id for id in bus['business_id'].value_counts()==1])
from jira import JIRA $ jira = JIRA(cred['host_jira'], basic_auth=(cred['email'], cred['password_jira'])) $ issues = jira.search_issues('issuetype = "Bug de Tracking"')
df[df['Created At'] > df['Shipped At']].shape
print (train["review"][0]) $ print ("\n", example1.get_text())
df3.head()
new_page_user_count = df2[df2['landing_page'] == 'new_page']['user_id'].count() $ new_page_user_count / total_users $
few_recs = pd.read_csv("../data/microbiome/microbiome.csv", nrows=4) $ few_recs
ts = pd.Series(np.random.randn(3), dates)
score_0.shape[0] / score.shape[0]
df.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis = 1, inplace = True)
datafile = "2013_ERCOT_Hourly_Load_Data.xls"
(df2.query("group == 'treatment'")['converted'] == 1).mean()
df_categorical = df_EMR.select_dtypes(exclude=['float64', 'datetime64', 'int64'])
amps.to('seconds')  # DimensionalityError
time_series.describe()
autos.drop(["seller", "offer_type","nr_of_pictures"], axis = 1, inplace = True)
cursor.execute(sq7) $ cursor.execute(sq71) $ results1 = cursor.fetchall() $ results1
adf_check(dfs['Stock First Difference'].dropna())
cpi_all = abs_to_df(response_cpi_all.json(),name_only=False) $ cpi_all.head(10)
for item in result_set: $     print(item.index,item.education)
df['weekend']= df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0) $ df.head()
df2_new_page = len(df2.query("landing_page == 'new_page'")) / df2.shape[0] $ print('The probability that an individual received the new page is: {}.'.format(round(df2_new_page, 4)))
tb/tb.loc['All']
fit.summary()
steps = 24 $ t_selected = spp_plot.index.unique()[0::steps] $ t_marker = np.arange(0, len(t), steps) $
import pandas as pd $ enrollments['content_availability'] = pd.to_datetime(enrollments.content_availability)
x = tags['Count'][0:20] $ y = tags['TagName'][0:20] $ sns.barplot(x, y, color = 'g')
baseball_newind.query('ab > 500')
df_estimates_true.groupby('metric').count()
page = requests.get(url, timeout=5)
logodds.to_csv('logodds2.csv')
df.groupby('Cat')['Size'].count().plot(kind='bar')
cityID = '095534ad3107e0e6' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Louisville.append(tweet) 
sp500.at['MMM', 'Price']
temp = pd.read_table('vader_lexicon.txt', names=('word', 'polarity', 'idk', 'idk1')) $ sent = pd.DataFrame({'polarity':temp['polarity']}) $ sent.index = temp['word'] $
model = gensim.models.Word2Vec(sentences, size=200)
contribs.info()
df_display = df_target.nlargest(20, 'BlockChainPredictions').sort_values('BlockChainPredictions', ascending=False) $ pd.set_option('display.max_colwidth', -1) $ display(df_display.drop("text", axis=1)) $
pold = df2[df2['landing_page']=='old_page']['converted'].mean() $ print("the convert rate for  pold = ",pold)
states_table = page_soup.find_all("table") $ states_table
d['pasttweets_text']=d['pasttweets'].apply(lambda x: ', '.join(x))
df.tail(1).index
my_df["week_day"] = my_df["timestamp"].dt.dayofweek $ print(my_df["week_day"].head())
plt.hist(p_diffs) $ plt.axvline(differ, color = 'red');
dup_user = df2[df2['user_id'].duplicated()] $ dup_user
station = session.query(Station.station, Station.name, func.count(Measurement.tobs)).\ $ group_by(Measurement.station).filter(Station.station ==  Measurement.station).group_by(Measurement.station).\ $ order_by(func.count(Measurement.tobs).desc()).all() $ station
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK', 'ab_page']]) $ results = lm.fit() $ results.summary()
dog_stage_stats = tweet_archive_master[['dog_stage', 'rating_numerator', 'favorite_count', 'retweet_count']].groupby('dog_stage').mean() $
def check_wait_corr(): $     mini_df = data[['Order_Qty','wait']] $     mini_df['wait'] = pd.to_numeric(mini_df['wait']) $     return mini_df $ check_wait_corr().corr()
from sklearn.metrics import f1_score $ f1_score(y_test, y_pred_lgr, average='macro')  
appleNegs.groupby('group_id').tweet_id.count().reset_index() $
df2 = df2.join(df3.set_index('user_id'), on='user_id') $ df.isnull().sum()
data = pd.read_csv('Dumping.csv', delimiter = ',', skiprows=0, squeeze=False, skip_blank_lines=True, index_col=None) $
grouped_df = df.groupby('Date')
googletrend.head()
Df_X, Df_y = train_test_split(df, random_state=24) $ Df_X.head()
extract_all.loc[(extract_all.APPLICATION_DATE_short>=datetime.date(2018,6,1)) $                &(extract_all.APP_PRODUCT_TYPE.isin(['TL','RL'])) $                &(extract_all.CLA_RiskScore==1) $                &(extract_all.app_branch_state=='GA')].groupby('DEC_LOAN_AMOUNT1').size()
by_hour = sample.resample("H") $ by_hour
df2.query('landing_page == "new_page"').shape[0]/df2['landing_page'].shape[0]
last_year = dt.date(2017,8,23)-dt.timedelta(days=365) $ last_12_months = session.query(Measurement.date,Measurement.prcp).\ $     filter(Measurement.date > last_year).all() $ last_12_months
ts = type(merged_data.iloc[0, :][4])
import statsmodels.api as sm $ from sklearn.metrics import mean_absolute_error
df.applymap(lambda x:x*100)
re.sub("[^a-zA-Z]", " ", df.text[175])
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2, columns=['group'])['group_treatment']
week8 = week7.rename(columns={56:'56'}) $ stocks = stocks.rename(columns={'Week 7':'Week 8','49':'56'}) $ week8 = pd.merge(stocks,week8,on=['56','Tickers']) $ week8.drop_duplicates(subset='Link',inplace=True)
( diff_between_group < p_diffs).mean()
df = pd.DataFrame({"id":[1,2,3,4,5,6], "raw_grade":['a', 'b', 'b', 'a', 'a', 'e']}) $ df
list_tokens = [] $ for idx in range(len(message)): $     list_tokens.append([str(x) for x in message[idx].split(' ')])
(p_diffs > p_observed).mean()
twitter_df_clean.date.sample(2)
forest_model = RandomForestClassifier() $ forest_model.fit(X_train,y_train) $ print(cross_val_score(forest_model,X_train,y_train,cv=3,scoring='recall'))
p_diffs.mean()
sc.stop()
keff = res_esc * fast_fiss * therm_util * eta * p_fnl * p_tnl $ keff.get_pandas_dataframe()
df = df.dropna()
open('data/wx/tmy3/proc/700197.csv').readlines()[:6]
search_category = (categorised_df['word']) $ search_category.head(5)
all_data = {ticker: web.get_data_yahoo(ticker) $            for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']}
df.to_csv('ab_updated.csv', index=False)
crimes_by_yr_month_type = pd.DataFrame(datAll.groupby([ $     datAll['year'],datAll['month'],datAll['Offense Type']]).agg({'Offense_count':'sum'}))
df2.info()
Test.CorectionFactorSetup(MF1Gas='xenon', MF2Gas='Krypton', MF3Gas='Argon')
df.to_csv('train_new.csv')
print ("Propotion of users converted:",df.converted.mean())
df_categories = pd.read_csv('categories.csv') $ df_categories.head()
df['created_at'] = pd.to_datetime(df['Created Date'], format="%m/%d/%Y %I:%M:%S %p")
holiday_forecast_exp = np.exp(holiday_forecast[['yhat','yhat_lower','yhat_upper']]) $ holiday_forecast_exp.index = holiday_forecast['ds'] $ holiday_error = holiday_forecast_exp['yhat'] - df_orig['y'] $ MAPE_holiday_model = (holiday_error/df_orig['y']).abs().sum()/n *100 $ print ("Holidays model error: ",round(MAPE_holiday_model,2))
norm.ppf(1-(0.05/2)) # Tells us what our critical value at 95% confidence is
X.head() #to check the columns name of X featuers
for res_key, df in data_sets.items(): $     df.to_pickle('patched_'+res_key+'.pickle')
df2[df2.duplicated(subset=['user_id'], keep='first')] $ print(df2.query('user_id == 773192'))
import statsmodels.api as sm $ log = sm.Logit(df['converted'], df[['intercept', 'treatment']])
df2 = df.drop(df.query('(group == "treatment" and landing_page != "new_page") or (group != "treatment" and landing_page =="new_page") or (group == "control" and landing_page !="old_page") or (group !="control" and landing_page == "old_page")').index)
df = df.replace('712-2', '51529') $ df['Zip Code'] = df['Zip Code'].astype(int) $ df.dtypes
conn = sqlite3.connect("geo.db") $
questions.drop(['vip'], axis=1, inplace=True)
np.exp(rmse_ebay)
tryDF = joined_df[['id','end_use_date', 'usage_duration']].groupby(['id']).sum() $ tryDF.head()
logit_mod = sm.Logit(df2_copy['converted'], df2_copy[['intercept', 'ab_page_treatment']]) $ results = logit_mod.fit()
news_df['topic'].value_counts()
fashion.info()
tweets_clean.floofer.value_counts()
token_sendreceiveCnt["sendReceiveCnt"] = token_sendreceiveCnt.sendcount + token_sendreceiveCnt.receivecount
git_blame.timestamp = pd.to_datetime(git_blame.timestamp) $ git_blame.info(memory_usage='deep')
df_example4 = df_example3.join(schemaExample, schemaExample["ID"] == df_example3["id"] ) $ for row in df_example4.take(5): $     print row
df.loc[df['Sold_to_Party'] == '0000101663'].sample(10)['SalesOffice']
results2 = model_selection.cross_val_score(gnb, X_test, Y_test, cv=loocv) $ results2.mean()
plt.hist(taxiData.Trip_distance, bins = 50, range = [10,20]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
tweet_archive_clean.loc[tweet_archive_clean['new'].apply(find_dot) == True,'rating_numerator'] = new_df['rating_numerator'].tolist() $ tweet_archive_clean.loc[tweet_archive_clean['new'].apply(find_dot) == True,'rating_denominator'] = new_df['rating_denominator'].tolist()
soup.img
random_sample = dataset_test.sample(8, random_state=123) $ headlines = random_sample["name"]
dfn['goal_log'] = np.log10(dfn['goal'].values)
autos['odometer_km'].unique().shape
lmscore.predict(X)
year = pd.get_dummies(auto_new.CarYear) $ year = year.ix[:, ["2014", "2011","2010", "2012", "2013", "2015", "2016", "2008", "2009", "2007", "2017"]] $ year.head()
for ra in df['rb'].unique()[25::100]: $     temp = df[df['rb'] == ra] $     plt.scatter(temp['B_aver_N'], temp['Chernoff_Bound'], s=0.1) $ plt.ylim(0.4986, 0.5)
prcp_analysis_df = prcp_analysis_df.loc[prcp_analysis_df["date"]>=pa_min_date]
bikedataframe = bikedataframe.join(pd.Series(1, index=bike_events, name='bike_events')) $ bikedataframe['bike_events'].fillna(0, inplace=True)
wgts['0.encoder.weight'] = T(new_w) $ wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w)) $ wgts['1.decoder.weight'] = T(np.copy(new_w))
twitter_archive_master[twitter_archive_master.name == 'None'].head()
lancaster = nltk.LancasterStemmer() $ print([lancaster.stem(t) for t in tokens[550:600]])
n_new = (df2.landing_page == 'new_page').sum() $ n_new
diffSD = np.sqrt(active_sd**2/active_count + inactive_sd**2/inactive_count) $ diffSD
def sentiment_analysis(text): $     return TextBlob(text).sentiment
print ("Test Accuracy :: ",accuracy_score(test_dep[response], xgb_model.predict(test_ind[features]))) $ print ("Train Accuracy :: ",accuracy_score(train_dep[response], xgb_model.predict(train_ind[features]))) $ print ("Complete Accuracy  :: ",accuracy_score(kick_projects_ip[response], xgb_model.predict(kick_projects_ip_scaled_ftrs))) $ print (" Confusion matrix of complete data is", confusion_matrix(kick_projects_ip[response],kick_projects_ip["Pred_state_XGB"]))
order_data.set_index('userId', inplace=True)
month_zero = autos.loc[autos["registration_month"] == 0, :].index $ autos = autos.drop(index = month_zero)
education.drop(rows_todrop_1, inplace=True)
temps_df.loc['2018-05-05']
dfDay['Estimated Start'] = pd.to_datetime(dfDay['Estimated Start']) $ dfDay['Estimated End'] = pd.to_datetime(dfDay['Estimated End'])
df['user_id'][df['converted'] == 1].count() / df['user_id'].count()
keywords = ['earthquake', 'quake', 'magnitude', 'epicenter', 'magnitude', 'aftershock'] $ search_results = api.search(q=' OR '.join(keywords), count=100)
tweet_vector = CountVectorizer(analyzer='word',stop_words=stopWords).fit(tweet_hour['tweet_text'])
k.head()
df.query('group == "treatment" and landing_page != "new_page"').nunique()['user_id'] + df.query('group == "control" and landing_page != "old_page"').nunique()['user_id']
with open(home_dir + '/general_warehouse_key.json', 'rb') as a: $     Config = json.load(a) $ Historical_Raw_Data = Connect_Sql_Warehouse( $     Config, Historical_Demand_SG, "Historical Demand")
date_str = loc_date_str.split(',')[1].strip() + ', ' + loc_date_str.split(',')[2][:5].strip() $ date_str
click_condition_meta.head(3)
import pandas as pd $ df = pd.read_csv("data1.csv") $ print(df)
Shoal_Ck_hr = Shoal_Ck_15min.resample('h', on=str('DateTime')).mean() $ Shoal_Ck_hr.head(10) $
git_blame['knowing'] = git_blame.timestamp >= six_months_ago $ git_blame.head()
full['Age'].plot(kind='box',figsize=(12,4),vert=False) $ plt.xlabel('yrs old') $ plt.title('Distribution of Age',size=15)
dfs = pd.read_html('http://www.contextures.com/xlSampleData01.html', header=0) $ dfs[0].head()
trip_result.plot.area(alpha=0.2, stacked=False) $ plt.xticks(rotation=45) $ plt.show() $ plt.rcParams["figure.figsize"] = [6.4, 4.8] $
plt.rcParams['axes.unicode_minus'] = False $ dta_692.plot(figsize=(15,5)) $ plt.show()
df2[df2.groupby('user_id')['user_id'].transform('size') > 1]
index_values=df.query('group=="treatment" and landing_page!="new_page" or group=="control" and landing_page!="old_page"').index.values $ index_values
fig = plt.figure(figsize=(10,4)) $ ax = fig.add_subplot(111) $ ax = resid_6201.plot(ax=ax);
data_overdue_dur[data_overdue_dur > 0].median()
joined_data = gameids.set_index(['TEAM_ABBREVIATION','GAME_ID']).join(adv_stats.set_index(['TEAM_ABBREVIATION','GAME_ID']), $                                                        lsuffix='_L',rsuffix='_R') $ joined_data.reset_index(inplace=True) $ joined_data
loanlr = LogisticRegression(C=0.01, solver='liblinear').fit(X,y) $ loanlr
c_df.rename(columns={'TimeCreate':'Date'},inplace=True)
typesub2017['Solar'].sum() 
data = data[(data['Latitude'].notnull())& $             (data['Longitude'].notnull())& $             (data['Closed Date'].notnull())]
df[df.transaction_date > datetime.strptime('2017-02-01', '%Y-%m-%d')]
liberia_data4 = liberia_data3.set_index(['Description', 'Date']).sort_index() $ liberia_data4.columns = ['liberia'] $ liberia_data4
dfs['Stock First Difference'] = dfs['Close'] - dfs['Close'].shift(1)
offseason10 = ALL[(ALL.index > '2010-02-07') & (ALL.index < '2010-09-09')]
lr = LogisticRegression() $ lr.fit(X_train_dtm, y_train) $ lr.predict(X_test_dtm)
day_of_year14 = uber_14["day_of_year"].value_counts().sort_index().to_frame() $ day_of_year14.head()
newfile.rename(index=str, columns= $                {"Customer Service Rep Z( - Name": "CSCA Name", $                 "Customer Service Rep-Z(": "CSCA"}, $                 inplace=True)
pd.merge(df_a, df_b, right_index = True, left_index = True) # merge based on indexes 
cust_demo.Martial_Status.value_counts().plot(kind='bar', color='R', alpha=0.5)
from sklearn.feature_extraction.text import TfidfVectorizer $ from sklearn.metrics.pairwise import cosine_similarity
grouped = df_providers.groupby(['year','drg3']) $ grouped_by_year_DRG_max =(grouped.aggregate(np.max)['disc_times_pay']) $ grouped_by_year_DRG_max.head()
ls -n data.*
print("Train before", train['StateHoliday'].unique()) $ train.replace({"StateHoliday": {"0": 0}}, inplace=True) $ print("Train Before", train['StateHoliday'].unique())
t.isnull().values.any()
a.items() & b.items()
display(df_anomalies.head(15))
pd_builder.sample()
tech=sess.get_historical_data(['msft us equity','aapl us equity'],'px last', format=bp.data_frame('date', 'Security')) $ tech.head()
tweet_archive_clean.shape $
listings['amenities'] = listings['amenities'].map( $     lambda amns: "|".join([amn.replace("}", "").replace("{", "").replace('"', "")\ $                            for amn in amns.split(",")])) $ amenities = np.unique(np.concatenate(listings['amenities'].map(lambda amns: amns.split("|"))))[1:] $ np.array([listings['amenities'].map(lambda amns: amn in amns) for amn in amenities[0:43]])
score_fs.shape[0] / score.shape[0]
bar[['close','high','low']].plot()
(temp_df.email == '\\N').sum()
SraMTMmvt = delta(df_new, df_old, ['SraMTM']).rename('SraMTMmvt') $ PremiumMvt = delta(df_new, df_old, ['OtherPremium']).rename('PremiumMvt') $ ManAdjMvt = delta(df_new, df_old, ['FixedManualAdj']).rename('ManAdjMvt')
country_dummy = pd.get_dummies(fraud_data_updated['country']) $ fraud_data_updated = pd.concat([fraud_data_updated,country_dummy],axis=1)
df_variables.loc[df_variables["CustID"].isin([customer])]
grid_df = pd.DataFrame(data = us_grid) $ grid_df.columns = ['glon', 'glat'] $ grid_df = pd.concat([grid_id, grid_df], axis=1) $ grid_df.head() $
df_json.info()
df2 = df.query('group == "control" & landing_page == "old_page" | group == "treatment" & landing_page == "new_page"')
df.query('converted == "1"').user_id.nunique() / df['user_id'].nunique() $
rtc = pd.read_excel('input/data/ExogenousTransmissionCapacity.xlsx', $                     pars_cols='B:R', $                     header=3)
type(AAPL.columns)  # returns a pandas index  ( b/c each col is a series, therfore it has its own index?)
from pyspark.sql.functions import collect_list $ dict_visited_by_user = ...
deaths_XX_century = deaths_by_decade.reset_index()[mask_XX_century] $ deaths_XX_century.set_index('decade')
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.contains('^[(a-z)]'))]='None'
np.shape(prec_fine)
df_min_max.head()
df[df.date != 'Jan 10']
np.var(p_diffs)
fraud_data.head()
twitter_archive_enhanced.head()
y_pred = lgb_model.predict(X_test)
full_df.created.min(), full_df.created.max()
station_list = session.query(Measurement.station).distinct().all() $ station_list
df2[df2['landing_page'] == "new_page"].count()[1]/df2.shape[0]
df_exp.head(1)
db.limit(load_buf_array, 30)[:]
TEXT = dill.load(open(f'{PATH}models/TEXT.pkl', 'rb'))
pprint.pprint(parser.HHParser.parseLine(aSingleLine))
missing_info = list(data.columns[data.isnull().any()]) $ missing_info
guido_text = soup.text $ print(guido_text[:500])
b.append(5) $ print a
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth) $     return api
sanders_df = pd.DataFrame(sanders) $ sanders_df.head()
race_cols = race_vars.columns.tolist()
px.head()
) $ pd.read_sql('desc actor', engine)
all_market_data_as_df.resample('120S').agg({'bid_size': 'sum', 'bid_price': 'mean', 'ask_price': 'mean','ask_size':'sum'})[["bid_size", "bid_price", "ask_price", "ask_size"]]
soup.find("span", {"class":'next-button'}).a['href'][-9:]
rainfall_sql=[Measurement.station, $               func.sum(Measurement.prcp)] $ rainfal_station=session.query(*rainfall_sql).filter(Measurement.date>='2016-08-05').filter(Measurement.date<='2017-08-05').group_by(Measurement.station).all() $ rainfal_station
dfFull.BsmtFinSF2 = dfFull.BsmtFinSF2.fillna(dfFull.BsmtFinSF2.mean())
class_merged.isnull().sum()
obamaSpeechSoup = bs4.BeautifulSoup(obamaSpeechRequest.text, 'html.parser') $ print(obamaSpeechSoup.text[:200])
number_string = 'aaaXXaaaXXaaa' $ number_string.count('XX')
retweets = lead[~lead["retweeted_screen_name"].isnull()] $ retweet_pairs = retweets[["id","screen_name","retweeted_screen_name"]].groupby(["screen_name","retweeted_screen_name"]).agg({"id":"count"}).rename(columns={"id":"Weight"}) $ retweet_pairs.reset_index(inplace=True) $ retweet_pairs.head()
from scipy import stats $ stats.describe(RandomPercentage)
n_old = df2.query('landing_page == "old_page"')['user_id'].shape[0] $ n_old
test_df = data.iloc[-10:].drop(columns=['Adj. Open','Adj. High','Adj. Low','Adj. Volume','Adj. Close', 'Ex-Dividend', 'Split Ratio'])
df_subset['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=False, logy=False) $ plt.show()
team_id = 'eefcfb58-4d42-490d-9aea-4d4fd97964ce' $ url = form_url(f'teams/{team_id}') $ response = requests.get(url, headers=headers) $ print_body(response, skip_audit_info=True)
p_old = df2.converted.sum() / df2.shape[0] $ p_old
df_new =df_country.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
vol = df['Close'] $ vol.head()
g1 = df.groupby('end_station_name').count() #sorting by end station names $ g_sorted1 = g1.tripduration.sort_values(ascending=False) #arranging in descending order
pd.DataFrame({'count':user_summary_df[user_summary_df.gender == 'M'].verified.value_counts(), 'percentage':user_summary_df[user_summary_df.gender == 'M'].verified.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})
display((data['Tweets'][fav])) $ print("Number of likes: {}".format(fav_max)) $ print("{} characters.\n".format(data['len'][fav]))
ah_followers = list(t.user_lookup(user_ids=ah[:10])) $ for a in ah_followers: $     print(a["description"]) $     print("---"*10)
r = q_mine.results() $ r
df2['user_id'].duplicated() $ sum(df2['user_id'].duplicated())
import scipy.io as spio $ help(spio.matlab)
display_code('models.py',[234,238])
trump_observed_tvd = tvd(trump_pivoted.iloc[:,0], trump_pivoted.iloc[:,2]) $ trump_observed_tvd
df.iat[6,2] = 666 $ df
new_log_mod = sm.Logit(df3['converted'], df3[['intercept', 'new_page', 'UK', 'US']]) $ new_results = new_log_mod.fit() $ new_results.summary()
union_set = set() $ set_cols = all_sets.cards.map(lambda x: set(x.columns)) $ for setname in set_cols.index: $     union_set = union_set | set_cols[setname] $ print(union_set)
for c, v in zip(logreg_sentiment.fit(X, y).coef_[0].round(3), ['neg','pos','neu','compound','len']): $     print v, c
import pandas as pd  $ style.use('fivethirtyeight') $ %config InlineBackend.figure_format = 'retina'
rng_dateutil = pd.date_range('3/6/2012 00:00', periods=10, freq='D', tz='dateutil/Europe/London')
a400hz.count()
display(data.head(n=3))
fundvolume_binary.index
visit_num.head(n=15)
sns.lmplot(x="Debt", y="Income", data=training, x_estimator=np.mean, order=1)
building_pa_prc_shrink.to_csv("buildding_01.csv",index=False) $
df_archive_clean["dog_stage"] = df_archive_clean["doggo"].add(df_archive_clean["floofer"], $                               fill_value="").add(df_archive_clean["pupper"], $                                                  fill_value="").add(df_archive_clean["puppo"],fill_value="")
for inst in idx_set: $     with open('../notebooks/subsample_idx_{}.json'.format(inst), 'w') as fd: $         json.dump(list(idx_set[inst]), fd, indent=2) $
full_df['ymd'] = full_df['created'].map(lambda x: x.strftime('%Y-%m-%d'))
merge = pd.merge(df1, df2, left_index=True, $                 right_index=True, how="outer", $                 suffixes=("", "_y")) $ merge.to_csv("{}MN_17_3616_merge.csv".format(out_dir), $             float_format='%g')
print(lyra_lightcurve.meta)
xmlData.head()
l = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result=l.fit()
df4[["old_page","new_page"]] = pd.get_dummies(df4["landing_page"]) $ df4.head()
round((timelog.seconds.sum() / 60 / 60), 1)
lims_query = "SELECT ephys_roi_results.id, specimens.id, specimens.name \ $ FROM ephys_roi_results JOIN specimens ON specimens.ephys_roi_result_id = ephys_roi_results.id" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
today = datetime.now() $ print(today)
pd_excel = pd.read_excel(datapath / datafile) $ pd_excel.head()
list_2d = [('Jack', 1, 'NY'), ('Sally', 2, 'CA'), ('Chris', 3, 'FL')] $ pandas_list_2d = pd.DataFrame(list_2d) $ print(pandas_list_2d)
response = urllib.urlopen(url) $ data = response.read() $ data_json = json.loads(data)   #change to json format $ data_json
sns.distplot(master_list[master_list['Count'] < 5]['Count'])
print('UPDATED DATAFRAME, QUESTION 3 FINDINGS:') $ print('- The number of unique user_ids in df2 is {}.'.format(df2_unique)) $ print('- The repeated user_id was {}'.format(user_repeated))
import time $ time.strftime('%Y-%m-%d %H:%M:%S')
driver = webdriver.Edge(executable_path = "C:/libs/web_driver_for_crawl/MicrosoftWebDriver.exe") $ driver.wait = WebDriverWait(driver, 10)
df['sent_label'] = 'neutral' $ df.loc[df['sentiment'] > 0.1, 'sent_label'] = 'positive' $ df.loc[df['sentiment'] < -0.1, 'sent_label'] = 'negative' $ df.head(3) $
null_vals = np.random.normal(0, np.std(diffs), 10000) # Here are 10000 draws from the sampling distribution under the null
reframed_rip = series_to_supervised(scaled_rip, 1, 1) $ print(reframed_rip.shape) $ print(ripple_market_info.shape)
df = pd.get_dummies(df, columns = ["max_dog_size", "min_dog_size", "requester_gender", "provider_gender", "experience"]) $ df.head(5)
refl_shape = serc_reflArray.shape $ print('SERC Reflectance Data Dimensions:',refl_shape)
df_trips = pd.merge(df_trips, pilot_planets, how="left", on="pilot")
df_plot = df_items.groupby(['family','perishable']).agg({'item_nbr':'count'}).reset_index() $ plt.figure(figsize=(15,6)) $ plt.title('Perishable Items by Family', fontsize=20) $ sns.barplot(data=df_plot, y='item_nbr', x='family', hue='perishable') $ plt.xticks(rotation=75, fontsize=12)
import matplotlib.pyplot as plt $ def plot_data(n): $     print "scan of %s"%digits.target[n] $     show(plt.matshow(digits.images[n], interpolation="nearest", cmap=plt.cm.Greys_r))
data["Improvements"] = data["Improvements"].apply(filtering)
def get_cycle(dt): $     if dt.year % 2 == 0: $         return dt.year $     else: $         return dt.year +1
joined_hist.joined = pd.to_datetime(joined_hist.joined) $ joined_hist = joined_hist.sort_values(ascending=True, by='joined') $ joined_hist.head() $
likes = likes.drop(['timestamp', 'actor', 'title'],axis=1) $ likes.head(10)
pd.crosstab(df_concat_2["message_likes_dummy"],df_concat_2["page"],margins=True)
feature_importances.iloc[:15,:].plot.bar() $ plt.ylabel('Importance') $ plt.title('Feature Importance Plot') $ plt.show()
from sklearn.model_selection import train_test_split
sys.getsizeof(data2017_list)
fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat']) $ for word in sorted(fdist): $     print(word, '->', fdist[word], end='; ')
positive = '/Users/EddieArenas/desktop/Capstone/positive-words.txt' $ positive = pd.read_table('/Users/EddieArenas/desktop/Capstone/positive-words.txt')
tweets['retweeted'].value_counts()
nmf_cv, nmf_cv_data, nmf_tfidf, nmf_tfidf_data, lsa_cv, lsa_cv_data, lsa_tfidf, lsa_tfidf_data, lda_cv, lda_cv_data, lda_tfidf, lda_tfidf_data, combo_models_list = mf.gen_vectorizer_model_combos(cv_fitted, cv_data, tfidf_fitted, tfidf_data, n_topics=7)
keep_vars = set(no_hyph.value_counts().head(12).index)
loan_stats["issue_d"].types
class_merged.columns=['date','store_nbr','class','family','sum_unit_sales','no_items','no_perishable_items','items_onpromotion'] $ pd.DataFrame.head(class_merged)
category = "men", "women", "mixed", "men", "women", "mixed" $ course = "long", "long", "long", "short", "short", "short" $ record_tables = [t for t in tables if "Event" in t.columns] $ len(record_tables)
archive[archive.duplicated]
plt.hist(p_diffs) #same histogram as above $ plt.axvline(x=obs_diff, color='red'); #adding in the line for the actual/observed difference since we are about to calculate p-value
df_complete.head(1)
autos["gearbox"].value_counts()
df['year_built'].isnull().sum()
train.isnull().sum()
grades_cat = df['Grades'].astype(dtype= 'category') # Series object $ grades_cat
print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())
X_test.shape
train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=1729), [int(0.7 * len(df)), int(0.9 * len(df))]) 
fashion.head(2)
year_data=year.json()['dataset']
calls_df=calls_df.drop(["call_date","time"],axis=1)
cols = ["has_image", "has_map", "num_bed", "where", "hour", "dayofweek", "area"] $ X_num = pd.get_dummies(data[cols]) $ y = data["price"]
for elem in table3['browser'].unique(): $     table3[str(elem)] = table3['browser'] == elem $ table3.head(3)
def get_4hour(entries_or_exits): $     return entries_or_exits - entries_or_exits.shift(1)
top_songs['Date'].max()
treatment_not_new = df.query('group == "treatment"').query('landing_page != "new_page"') $ treatment = df.query('group == "treatment"').query('landing_page == "new_page"') $ not_treatment_new = df.query('group != "treatment"').query('landing_page == "new_page"') $ treatment_not_new.count()[0] + not_treatment_new.count()[0]
likes.describe().T
user1.plot.scatter(x='TowerLon', y='TowerLat', c='gray', alpha=0.1, title='Call Locations') $ plt.show()
station_distance.insert(loc=11, column='Distance(Miles)', value=distance)
index = pd.DatetimeIndex(['2014-07-04', '2014-08-04', '2015-07-04', '2015-08-04']) $ data = pd.Series([0, 1, 2, 3], index=index) $ data
df_test = loaded_text_classifier.predict(df_test) $ evaluator = loaded_text_classifier.evaluate(df_test)          $ evaluator.plot_confusion_matrix() $ evaluator.get_metrics("macro_f1")
to_cat_codes = ['gender', 'race', 'resident_status'] $ for c in to_cat_codes: $     df[c] = df[c].astype('category') $     df[c+'_cat'] = df[c].cat.codes        
merkmale.to_clipboard()
print(sample_data.json())
print(mnb_gd.best_params_, mnb_gd.best_score_)
randomdata1[(randomdata1 >= 1).any(axis=1)]
pred_vals = list(scaler.inverse_transform(model.predict(tmp_input)).reshape(1,-1)[0])
posts_questions_df.head()
station.isnull().any()
(df.set_index('STNAME').groupby(level=0)['POPESTIMATE2010','POPESTIMATE2011'] $     .agg({'avg': np.average, 'sum': np.sum}))
df.sort_values(by='hash_id')[['hash_id', 'cause_num']].head(15)
extract_deduped.APPLICATION_DATE_short.max()
station_bounds[station_bounds.station_id == choice].index[0]
df1 = df.drop(a_mismatch.index) $ df2 = df1.drop(b_mismatch.index)
df3 = pd.read_csv('countries.csv') $ df4 = df3.set_index('user_id').join(df2.set_index('user_id')) $ df4.head()
show_in_notebook('./rendered.html')
history = model.fit(X_conv_train, y_train, epochs=30, validation_split=0.2, batch_size=128)
top_score = df.rating_score.max() $ print('The highest rating is a {} out of 5.'.format(top_score))
merged_data.drop(['customer_creation_date', 'invoices_creation_date', 'last_payment_date'], axis=1, inplace=True)
query = { $     'type' : 'fill_vesting_withdraw', $     'timestamp' : {'$gte': dt.now() - datetime.timedelta(days=360)}} $ proj = {'deposited.amount': 1, 'withdrawn.amount': 1, 'timestamp': 1, 'from_account': 1, 'to_account':1, '_id': 0} $ sort = [('timestamp', -1)]
errors = model.anomaly(mtcars_filtered) $ errors.describe()
old_page_converted = np.array(df2.sample(received_old_page, replace=True).converted)
autos.head()
c_aux=[] $ for i in range(0,l2): $     if i not in seq: $         c_aux.append(c_counts[i]) $ col.append(np.array(c_aux))
temp_df.count()
tweet_df['place'].value_counts()
All_tweet_data_v2=All_tweet_data.copy()
tf_feature_names = vectorizer.get_feature_names()
tweets_df.tail()
tz_dog = timedog_df.groupby('userTimezone')[['tweetRetweetCt', 'tweetFavoriteCt']].mean() $ tz_dog.head()
countries_df = pd.read_csv('./countries.csv') $ countries_df.head() $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['search'])
session = Session() $ query = session.query(articles_raw).limit(100) $ df = pd.read_sql(query.statement, query.session.bind) $ session.close()
print(parquet_file) $ df = sqlContext.read.load(parquet_file) $ df.show()
tw_clean = tw_clean.drop(tw_clean[tw_clean.rating_denominator != 10].index)
question_2_dataframe = question_2_dataframe.merge(population_by_zip, how='left', on=['incident_zip']) $ question_2_dataframe.head(5)
a+'.'
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
mydata.describe()
data['new_claps'] = buckets $ data.head()
datetime.now().date()
print(ozzy.age)
result = 'TE_320860761056_001-and-MO_800509458712_001-1525993487-tr' $ print(kimanalysis.shortcode(result)) $ print(kimanalysis.shortid(result)) $ print(kimanalysis.extendedid(result))
signedup_cstmrs = c_df[c_df['CUSTOMER_ID'].str.contains('@')].shape[0] $ signedup_cstmrs
store_items.interpolate(method = 'linear', axis = 1)
df.plot();
w_train, w_test=train_test_split(df3, test_size=.33, random_state=42)
df_userid = pd.DataFrame({"UserID":users["UserID"]}) $ df_Tran = pd.DataFrame({"ProductID":products["ProductID"]}) $ df_userid['Key'] = 1 $ df_Tran['Key'] = 1 $ df_out = pd.merge(df_userid,df_Tran,how='outer',on="Key")[['UserID','ProductID']]
df_onc.columns $ df_uro.columns
def find_prediction(savm_id,data_id): $     return 1 if savm_id==data_id else 0 $ find_prediction_udf = F.udf(find_prediction,types.IntegerType()) $ join_b = join_b.withColumn("predicted", find_prediction_udf(join_b["sales_acct_id_orig"],join_b["sales_acct_id"]))#.select(['endcustomerlinefixed_data','predicted'])
new_df_left['parcelid'].sort_values() $ new_df_left[new_df_left['parcelid'].duplicated(keep=False)].reset_index()
trump.dtypes
print('Number of rows before duplicated value removed, {}.'.format(df2.shape[0])) $ df2.drop_duplicates(['user_id'], inplace=True) $ df2 = df2.drop('duplicated', axis=1) $ print('Number of rows after duplicated value removed, {}.'.format(df2.shape[0]))
reddit_master['Class_comments'].value_counts()/reddit_master.shape[0]
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'],unit='s') $ git_log.describe()
try: $     result = db.tweets.drop() $     print ("analytics tweets dropped") $ except: $     pass
s=random_numbers[random_numbers.index.weekday==2].sum()
sum(df_h1b_nyc_ft.pw_1*df_h1b_nyc_ft.total_workers)/sum(df_h1b_nyc_ft.total_workers)
data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} $ df = pd.DataFrame(data) $ print(df)
a = "abc" $ print(a.__str__())  # Equivalent to str(a) $ print(a.__repr__())
import matplotlib.pyplot as plt $ from matplotlib.ticker import FuncFormatter $ import seaborn as sns $ from itertools import chain $ def explore_data(df): $
X[col_with_na].describe()
twitter_archive.rating_numerator.describe()
df_questionable_3 = pd.merge(left= df_questionable, left_on= 'link.domain_resolved', $                              right= df_usnpl_one_hot_state, right_on= 'domain', how= 'left')
Features = iris.values[:, :4] $ species = iris.values[:, 4]
active_list_pending_ratio = x_train[['Active Listing Count ', 'Pending Ratio']].values $ ss_scaler.fit(active_list_pending_ratio) $ active_list_pending_ratio_transform = ss_scaler.transform(active_list_pending_ratio) $ active_list_pending_ratio_transform[0:5,:]
series1 = pd.Series(np.random.randn(1000)) $ series2 = pd.Series(np.random.randn(1000))
fld_sci = scidat['We will match you with a pen pal who has expressed an interest in at least one of the following subjects. Which topic is most relevant to your work?'] $ print(np.array(np.unique(fld_sci))) $ print(scidat['Date Created']) $
yhat = loansvm.predict(X_test) $ yhat [0:5]
height.fillna(-1)
df['hour'] = df['tweet_created'].dt.hour $ df['date'] = df['tweet_created'].dt.date $ df['dow'] = df['tweet_created'].dt.dayofweek
df = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']) $ df
import re $ p = re.compile('\d+(\.\d+)?') $ strs = '---' $ cflot = (0.0 if p.match(strs) == None else float(strs)) $ print (cflot) $
for remove in map(lambda r: re.compile(re.escape(r)), $                   [",", ":", "\"", "=", "&", ";", "%", "$", "@", "%", "^", "*", "(", ")", "{", "}", $                    "[", "]", "|", "/", "\\", ">", "<", "-", "!", "?", ".", "'", "--", "---", "#", "..."] $                  ): $     test_sentence.replace(remove, " ", inplace=True)
house_data.head()
obs_diff = prob_treat - prob_contr $ (p_diffs > obs_diff).mean() $
ffr.resample("MS").last().head()
df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])[['CA','UK','US']] $ df_new['country'].astype(str).value_counts() $ df_new.head()
archive_clean.to_csv('twitter_archive_master.csv')
user_table = events[['user_id', 'login']].drop_duplicates() $ user_table.to_sql(name='users', con=con, if_exists='append', index=False)
car_data.head()
ebola_tidy = pd.concat([ebola_melt_1, status_country], axis = 1) $ print(ebola_tidy.shape) $ print(ebola_tidy.head()) $
df_loan2.set_index('fk_loan',inplace=True)
test['price_usd'] = pd.Series(predictions)
sales_location = pd.DataFrame(true_sales.ix[0:,'City':]) $ sales_city = sales_location.groupby(by='City',as_index=False).sum() $ city_margin_2015_mean = sales_city.sort_values(by='2015 Margin mean', ascending=False) $ top_city_sales = city_margin_2015_mean.head(10) $ top_city_sales $
sns.factorplot('monthOfRegistration',data=autodf,kind='count')
churned_ix = USER_PLANS_df[churned_bool].index
print(df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')].shape[0] + $       df[(df['landing_page'] == 'new_page') & (df['group'] != 'treatment')].shape[0])
Image("/Users/jamespearce/repos/dl/data/dogscats/train/dog.5959.jpg")
df2[(df2['group'] == 'treatment')]['converted'].mean()
witf = open("latlong_test3.txt","w", encoding="utf-8") $ for i in range(len(test_kyo3)): $     witf.write("{location: new google.maps.LatLng(%f, %f)},\n" % (test_kyo3['ex_lat'][i],test_kyo3['ex_long'][i])) $ witf.close()
subwaydf.columns = subwaydf.columns.str.strip()
df = pd.get_dummies(df, columns = ['new_home', 'hail_resistant_roof'], $                     drop_first=True)
click_condition_meta['os_timezone'] = np.where(click_condition_meta['os_timezone'].isin(europe), 'Europe', click_condition_meta.os_timezone) $ click_condition_meta['os_timezone'] = np.where(click_condition_meta['os_timezone'].isin(america), 'America', click_condition_meta.os_timezone) $ click_condition_meta['os_timezone'] = np.where(click_condition_meta['os_timezone'].isin(asia), 'Asia', click_condition_meta.os_timezone) $ click_condition_meta['os_timezone'] = np.where(click_condition_meta['os_timezone'].isin(australia), 'Australia', click_condition_meta.os_timezone) $ click_condition_meta['os_timezone'] = np.where(click_condition_meta['os_timezone'].isin(africa), 'Africa', click_condition_meta.os_timezone)
df['name'].apply(capitalizer)
def printFile(filePath): $     with open(filePath,'r') as f: $         data = pd.read_csv(f) $         print "=================",filePath,"=================" $         print data.head(),'\n' $
vsim = 0.00544
X.shape[1] == test.shape[1]
start = time.clock() $ Google_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True) $ print '{:.2f}s'.format(time.clock() - start)
giss_temp.dropna(how="any").tail()
(df_cprc.isnull()).sum()
norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True)) $ normalized_embeddings = embeddings / norm $ valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset) $ similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True) $
autos['price'].head(10)
sum(users_visits.visits)
p_value = (p_diff < ab_data_diff).mean() $ p_value $
print("There have been {} tweets, with an average of {} and std {} remaining".format(thisWeek['text'].count() - 1, $                                                                                      hourlyRates.loc[hourlyRates['hourNumber'] == thisWeek['hourNumber'][0],'meanRemainingTweets'].iloc[0], $                                                                                      hourlyRates.loc[hourlyRates['hourNumber'] == thisWeek['hourNumber'][0],'stdRemainingTweets'].iloc[0]))
ann_ret_SP500[0].describe()
Amazon.loc[Amazon['Close'] > Amazon['Open'], ['Close', 'Open']].index.day.value_counts()
scoreListOfList[3] # Citigroup in no.3
n_new = df2.query('group == "treatment"')['user_id'].nunique() $ n_new
a['SA'] = np.array([ analyze_sentiment(tweet) for tweet in a['Tweets'] ]) $ b['SA'] = np.array([ analyze_sentiment(tweet) for tweet in b['Tweets'] ])
np.sort(top_songs['Country'].unique())
df.head()
plt.hist(p_diffs); $ plt.ylabel('Frequency', fontsize=12); $ plt.xlabel('p_diffs', fontsize=12); $ plt.title('10,000 simulated p_diffs', fontsize=14);
null_values = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist( null_values) $ plt.axvline(obs_diff, color = 'red');
df_arch_clean["source"].value_counts() $
tokens = nltk.word_tokenize(content) $ fdist = nltk.FreqDist(tokens) $ print(fdist)
appl["50d"] = np.round(appl["Close"].rolling(window = 50, center = False).mean(), 2) $ appl["200d"] = np.round(appl["Close"].rolling(window = 200, center = False).mean(), 2) $ pandas_candlestick_ohlc(appl.loc['2016-01-19':'2017-01-19',:], otherseries = ["20d", "50d", "200d"]) $
tobs_df = pd.DataFrame(tobs_data, columns=['Date', 'tobs']) $ tobs_df.set_index('Date', inplace=True ) $ tobs_df
for i in labels: $     tops = np.argsort(dist(result['cluster_centers'][i], reduced))[:10] $     print('cluster #%i:\n' % i, df.loc[tops]['title'].values.tolist(), '\n')
gdax_trans_btc = gdax_trans.loc[gdax_trans['Account_name'] == 'BTC',:]
n_new = df2[df2['landing_page']=='new_page'].user_id.count() $ n_new
print 'Number of nulls in the data set: \n', df.isnull().sum(),'\n\n' $
n_old = ab_df2.query('landing_page == "old_page"').shape[0] $ n_old
print("The probability of an individual converting is: {}".format(df['converted'].mean()))
response = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv') $ with open('image_preview.tsv', 'wb') as file: $     file.write(response.content)
sm.graphics.plot_partregress('Lottery', 'Wealth', ['Region', 'Literacy'], $                               data=df, obs_labels=False)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ results.summary() $
%bash $ mkdir sample $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" sample/train.csv $ gsutil cp "gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-*" sample/valid.csv
autos["last_seen"].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
slFileList = glob.glob("Data/ebola/sl_data/*.csv") $ slFrameList = [pd.read_csv(file,usecols=['variable','date','National'],index_col=['variable','date']) for file in slFileList] $ len(slFileList)
print cust_data1.columns
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted'])
df2.head()
word_docx = 'https://dl.dropboxusercontent.com/s/tlbu2ao89bmzjiw/transcripci%C3%B3n%20Kathya%20Araujo.docx'
yhat = loanTree.predict(X_test) $ print (yhat[0:5]) $ yhat
df.loc["a"] #using labels 
cat_feats = ['Company'] $ features = pd.get_dummies(features, columns=cat_feats, drop_first=True) $ features.head()
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_713.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_713, lags=40, ax=ax2)
ab = fb['2012':].resample('W')['share_count'].sum().sort_values(ascending=False)[:5].index
for info in zf_test.infolist(): $     print("File Name         -> {}".format(info.filename)) $     print("Compressed Size   -> {:.2f} {}".format(info.compress_size/(1024*1024), "MB")) $     print("UnCompressed Size -> {:.2f} {}".format(info.file_size/(1024*1024), "MB"))
ccl = pd.read_csv("CCL.csv", sep="\t") $ ccl.head()
user_list = list(submissions['hacker_id'].unique())
autos['year_of_registration'].describe()
googletrend['Date']=googletrend['week'].str.split(' - ',expand=True)[0] $ googletrend['State']=googletrend['file'].str.split('_',expand=True)[2] $ add_datepart(googletrend,"Date",drop=False) $ trend_de=googletrend[googletrend['file']=='Rossmann_DE']
fuel_type = autos.groupby("fuel_type").mean() $ fuel_type['price_$']
data['returns'] = np.log(data['AAPL'] / data['AAPL'].shift(1)) $ data['returns'].hist(bins=35)
from sklearn.metrics import confusion_matrix $ cmat=confusion_matrix(y_test,y_pred) $ print(cmat)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','ab_page']])
reddit_comments_data.groupby('parent_id').count().orderBy('count', ascending = False).show(100, truncate = False)
young.join(logs, logs.userId == users.userId, "left_outer")
import nltk $ from nltk.corpus import stopwords $ print(stopwords.words('english'))
Grouping_Year_DRG_discharges_payments.xs(2, level='drg3')
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth) $     return api
bag_of_words_vectorizer = CountVectorizer(min_df=2) $ bow_feature_vector = bag_of_words_vectorizer.fit_transform(corpus)
rf_reg.score(x_test,y_test)
print('Change the column Indicator to Indicator_id and dispaly (first 2 records)') $ df.rename(columns = {'Indicator':'Indicator_id'}).head(2)
repos = (pd.DataFrame(result['data']['organization']['repositories']['nodes'], columns=['name','issues'])).copy()
with open(meritocracy_save, mode='w', encoding='utf-8') as f: $     f.write(wikiMeritRequest.text)
top_songs['Streams'].isnull().sum()
atl_data = pd.read_csv('../data/external/atlanta_demographics.csv', index_col=0, header=None).transpose() $ atl_data.head()
len(train_data[train_data.notRepairedDamage == 'nein'])
df.isnull().any()
df[df.msno == '+++hVY1rZox/33YtvDgmKA2Frg/2qhkz12B9ylCvh8o=']
msftAC.shift(1, freq="S")[:5]
fig, axs = plt.subplots(1, 3, sharey=True) $ data.plot(kind='scatter', x='TV', y='sales', ax=axs[0], figsize=(16, 8)) $ data.plot(kind='scatter', x='radio', y='sales', ax=axs[1]) $ data.plot(kind='scatter', x='newspaper', y='sales', ax=axs[2])
from pyspark.sql import Row $ rdd_example3 = rdd_example2.map(lambda x: Row(id=x[0], val1=x[1], val2=x[2])) $ print rdd_example3.collect() $
train_small_data.to_feather("../../../data/talking/train_small_data.csv.feather") $ val_small_data.to_feather("../../../data/talking/val_small_data.feather")
calls_df.shape
df = pd.DataFrame(daily_norm, columns = ["date","min_temp","avg_temp","max_temp"]) $ df["date"] = pd.to_datetime(df["date"]).dt.date $ df.set_index(["date"], inplace = True)
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 1) & $        (finals["reb_l"] == 1) & (finals["stl_l"] == 0), 'type'] = 'inside_gamers'
datatest['expenses'] = datatest['expenses'].apply(lambda x : float(x))
np.exp(log_mod_dweek_results.params)
data_archie.isnull().sum()
df.merge?
dfss.head()
if 0 == 1: $     news_titles_csv_file = os.path.join(config.HR_DIR, 'news_titles.csv') $     news_period_df = pd.read_pickle(config.NEWS_PERIOD_DF_PKL) $     news_period_df.to_csv(path_or_buf=news_titles_csv_file, columns=['news_collected_time', 'news_title'], sep='\t', header=True, index=True)
twitter_archive_clean.text = twitter_archive_clean.text.str.replace('&amp;','&')
stores_tran_nulls=pd.DataFrame(class_merged[pd.isnull(class_merged.transactions)]) $ stores_with_nulls=len(stores_tran_nulls['store_nbr'].unique()) $ all_stores=len(stores['store_nbr'].unique()) $ stores_with_nulls/all_stores
remove_cols = [ 'img_num','p1', 'p1_conf', 'p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog' ] $ images_copy.drop (remove_cols, axis =1 , inplace= True) $ images_copy.tail()
df2.user_id.nunique()
Pold = df2.query('converted == 1').user_id.nunique()  / df2.user_id.count() $ Pold
df=json_normalize(data["dataset"], "data") $ df.columns = col_names $ df.head(3)
filtered_greater_100 = english_df.groupBy('hashtag').count().filter('count > 100')
print(len(df5['andrew_id_hash'].value_counts().where(df5['andrew_id_hash'].value_counts() > 0).dropna())) $ print(len(df5['andrew_id_hash'].value_counts().where(df5['andrew_id_hash'].value_counts() > 1).dropna())) $ print(len(df5['andrew_id_hash'].value_counts().where(df5['andrew_id_hash'].value_counts() > 20).dropna()))
np.sum(sample_x1[0],axis=-1).shape
tweets_list = [] $ for i,row in df_clean.iterrows(): $     tweet = api.get_status(row['tweet_id'],tweet_mode='extended') $     tweets_list.append(tweet._json)
education.drop([702, 703, 704, 705, 706], inplace=True)
%%time $ reg = xgb.XGBRegressor().fit(X_train, y_train)
df_labeled['subject_id'].unique()
distinct_authors_with_gh = authors_to_github_username_saved.withColumn( $     "new_unique_id", $     F.when(F.col("github_username") != "", $          F.col("github_username")).otherwise( $         F.col("email"))) $
!miniasm -f Archivos_sim_ont/reads_lambda_albacore.fastq Archivos_sim_ont/Ensamble/ovlp_fl.paf > Archivos_sim_ont/Ensamble/fago_lambda_raw.gfa
ml.ConfusionMatrix.from_csv( $   input_csv='./evalme/predict_results_eval.csv', $   schema_file='./evalme/predict_results_schema.json' $ ).plot()
clf.fit(X_train, y_train)
cur.execute(get_first_ten)
dfWords.to_pickle(data + "dfWordsRegMet.p") $
df_final.head()
df2.head(5)
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].head()
with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f: $     gr1 = f.create_group(where='/', name='gr1') $     gr2 = f.create_group(where=f.root.gr1, name='gr2') $     f.create_array(where='/gr1/gr2',  name='some_array', obj=[0, 1, 2, 3]) $     f.create_hard_link(where='/', name='hard_link', target='/gr1/gr2/some_array')
RN_PA_duration.head()
ab_file2.drop(labels=2862, inplace=True) $ ab_file2[ab_file2['user_id']==773192]
col_desc = pd.DataFrame( $     {'name': ["my_col{}".format(c) for c in range(0, cols)], $      'comment': ["You could add some comment for column {}".format(c) for c in range(0, cols)] $     }) $ col_desc.head()
df_dem = df[df.party == 'Democrat']
full_data = pd.concat([train_small_sample[usecols], val_small_sample[usecols], test[usecols]])
list(c.find({}, {'_id': 0}))
with open(url.split('/')[-1], mode='wb') as file: $     file.write(response.content)
twitter_Archive.to_csv('twitter_archive_master.csv', index=False)
def train(df): $     ar_model = sm.tsa.AR(df, freq='D') $     ar_model_res = ar_model.fit(ic='bic') $     return ar_model_res
autos['kilometer'].unique().shape[0]
df_1 = df[(df['group']=='control') & (df['landing_page'] != 'new_page')] $ df_2 = df[(df['group']=='treatment') & (df['landing_page'] != 'old_page')] $ df2 = df_2.append(df_1, ignore_index=True) $ print(df.shape[0] - df2.shape[0])
%run indebtedness.py
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
lr_mod_UK_US = sm.Logit(df_country_join['converted'], df_country_join[['intercept', 'UK', 'US', 'ab_page']]) $ lr_mod_UK_US = lr_mod_UK_US.fit() $ print (lr_mod_UK_US.summary())
df.rolling(window=3,min_periods=2).aggregate(np.sum) $
not_in_dsi.head(3)
tipsDF.head()
lat_list = list(top_bike['Lat']) $ last_elem = lat_list[-1] $ lat_list.append(last_elem) $ lat_list.pop(0) $ top_bike['newLat'] = lat_list
session.query(Measurements.date).order_by(Measurements.date).first()
centers_df.shape
df.resample('D', how='sum')
def get_weights(event): $     for h in event.group_by('hours'): $         mean_error = h['regression'].mean() - h['real_min'].mean() $         h['weight'] = 1 - mean_error $     return event $
!cat crossref-by-doi/*.json \ $   | jq -r '.message | select(has("alternative-id") | not) | .DOI' \ $   | wc -l
full_clean_df = pd.merge(df1_clean, df3, on='tweet_id')
forecast_set = clf.predict(X_lately)
parties['Unique Key'].groupby(by= parties.index.dayofweek).count().sort_values(ascending = False).head(1)
fb['created_time'] = pd.to_datetime(fb['created_time']) $ fb.set_index('created_time', inplace=True) $ fb.sort_index(inplace=True)
df.tail(3)
df_h1b_nyc_ft.lca_case_employer_name.nunique()
extract_all.DEC_LOAN_AMOUNT1
dtc = DecisionTreeClassifier() $ dtc.fit(X_train, y_train) $ predictions = dtc.predict(X_test) $ accuracy_score(y_true = y_test, y_pred = predictions)
print("The probability of converted:", df.converted.mean())
print(r.json()['dataset_data']['data'][:4])
sponsors_df.values[0:5]
df2[df2['converted']==1].count()[0]/df2.count()[0]
df_final_edited['p1'].value_counts() $
model.predict_proba(X_train)[:,0]>0.5
df_t_best = df_t[df_t['Shipping Method name'].isin([271,326,323])].groupby(['Place Name','State']).filter(lambda x: len(x) >= 5)
compound_final.set_index(['Date'], inplace=True) $ compound_final.head()
geocoded_df.groupby('Plaintiff.Name')['Plaintiff.Name'].count().sort_values(ascending=False).head(50)
def get_day(df): $     df['timestamp'] = pd.to_datetime(df['timestamp']) $     df['day'] = df['timestamp'].dt.day
len([1 for h in heap if len(h.tweets) == 2])/len(heap)
valid_probs = pd.DataFrame(columns=Y_valid_df.columns, index=Y_valid_df.index)
from nltk.corpus import stopwords $ stop = stopwords.words('english') $ [w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] $ if w not in stop]
print 'Most VICE-like:', tweets_pp[tweets_pp.handle == 'VICE'].sort_values('VICE_Prob', ascending=False).text.values[0] $ print 'Least VICE-like:', tweets_pp[tweets_pp.handle == 'VICE'].sort_values('VICE_Prob', ascending=True).text.values[0]
export1[['place_value', 'placeLatitude', 'placeLongitude', 'topic', 'chi_squared']].to_csv("event_detection.csv")
lda = prep.get_lda_model(from_scratch=False)
todays_datetimes = [datetime.datetime.today() for i in range(100)]
obs_diff = df2.query('group=="treatment"').converted.mean() - df2.query('group=="control"').converted.mean() $ sim_diff = np.array(p_diffs).mean() $ obs_diff, sim_diff
print 'A data structure containing month-level frequencies for the year 2013' $ mp2013 = pd.period_range('1/1/2013', '12/31/2013', freq='M') $ mp2013
actual_diff = df2.converted[df2.group == "treatment"].mean() - df2.converted[df2.group == "control"].mean() $ (np.array(p_diffs) > actual_diff).mean()
MODEL_NAME = 'basic_features_hyperparameter_tuning' $ os.environ['MODEL_NAME'] = MODEL_NAME
r.close()
print(reddit.user.me())
merged_df['recency'] = (merged_df['last'] - merged_df['first']).astype('timedelta64[W]') #D for days, W for weeks etc $ print(merged_df.dtypes) $ merged_df.head() $ merged_df[merged_df['recency'] <0] 
dfX = data.drop(['pickup_lat','pickup_lon','dropoff_lat','dropoff_lon','created_at','date','ooCost','ooIdleCost','corrCost','floor_date','floor_10min'], axis=1) $ dfY = data['corrCost']
vlvar = grp2.createVariable('phony_vlen_var', vlen_t, ('time','lat','lon'))
autos['unrepaired_damage'].unique()
bmp_dict = {} $ for b in top_brands: $     bmp_dict[b] = (autos $                    .loc[ autos["brand"]==b, "price"] $                    .mean().round(2))
age_median = df_titanic_temp['age'].median() $ print(age_median) $
client.experiments.get_status(experiment_run_uid)
reddit_comments_data.select(max("score")).show(truncate=False)
test = np.array(([3,3],[9,1])) $ print(test) $ print(test.shape) $ print(test.shape())
myTweets = pd.read_sql("SELECT * FROM polished_tweets", Database().myDB, index_col='tweet_id')
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.len() < 3) & (All_tweet_data_v2.name.str.contains('^[(a-z)]'))]='None'
prcp_results= session.query(func.strftime("%Y-%m-%d",Measurement.date), func.sum(Measurement.prcp))\ $             .filter(Measurement.date.between('2016-08-01','2017-07-31')).group_by(Measurement.date)\ $             .order_by(Measurement.date).all() $ prcp_results   
p_value = (null_vals > obs_mean).mean() $ p_value
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 6 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
extract_all.loc[(extract_all.APP_SSN.isin([570810796])),['APP_SSN','app_source_v2']]
df.columns
plot_series_save_fig(series=therapist_duration, figsize=(12,6), xlabel='Date', ylabel='Appointment Time (hours)',\ $                      plot_name='Therapists', figname='./images/therapists_weekly_time_series.png')
print("Train set:") $ print(train_set['No-show'].value_counts() / len(train_set)) $ print("Test set:") $ print(test_set['No-show'].value_counts() / len(test_set))
train_df[["Sex", "Survived"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)
loan_stats['loan_status'].table()
df_oldlen = len(df2.query("group =='control'")) $ df_oldlen
df.query("group == 'control' and landing_page != 'old_page'").shape[0] + df.query("group != 'control' and landing_page == 'old_page'").shape[0]
tree.fit(X_train, y_train)
plt.figure(figsize = (10, 8)) $ sns.kdeplot(perf_train.loc[perf_train['Default']==0,'PERIODID_MY'], label = 'Default == 0') $ sns.kdeplot(perf_train.loc[perf_train['Default']==1,'PERIODID_MY'], label = 'Default == 1') $ plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');
question_1_dataframe = question_1_dataframe.size().to_frame(name='count') $ question_1_dataframe
df_raw = df_raw.loc[df_raw.artist.notnull(),:]
links_lst=[] $ for k in users_retweeted_dict: $     if users_retweeted_dict[k]: $         for u in users_retweeted_dict[k]: $             links_lst.append({"source":k,"target":u}) $
S_2017.to_csv('2017_summer.csv') $ S_2016.to_csv('2016_summer.csv') $ S_2015.to_csv('2015_summer.csv')
len(pd.unique(df.Twitter_Handle.ravel()))
resultvalue_df.head()
siteds = dsdf[(dsdf['DataSetCode'].str.contains(siteCode))]
damage = crimes[crimes['PRIMARY_DESCRIPTION']=='CRIMINAL DAMAGE'] $ damage.head()
assert b.T.sum().median() == c $
logit_mod5 = sm.Logit(df2['converted'], df2[['intercept', 'ab_page', 'US', 'ab_US']]) $ results5 = logit_mod5.fit() $ results5.summary()
import scipy.stats as stats
sme = SMOTEENN() $ X_train, y_train = sme.fit_sample(X_train, y_train) $ print(X_train.shape) $ print(y_train.shape) $ unique(y_train, return_counts=True)
df.var()
df2.converted[df2.group == 'treatment'].mean()
featureIndexer = VectorIndexer(inputCol="features", $                                outputCol="indexedFeatures", $                                maxCategories=10).fit(df) $
df['launched'] = pd.to_datetime(df.launched) $ df['deadline'] = pd.to_datetime(df.deadline) $
for user in session.query(User).\ $ ...          filter(User.name=='ed').\ $ ...          filter(User.fullname=='Ed Jones'): $ ...    print(user)
df[df['price'] == df['price'].max()]
pold = df2[df2['landing_page']=='old_page']["converted"].mean() $ pold
df.dtypes
import pyspark.sql.functions as func $ f_lr_hash_test.groupBy().agg(func.max(col('id'))).show()
gdf["color"] = gdf.apply(lambda feat: 'green' if feat['SiteTypeCV'] == 'Stream' else 'red', axis=1)
indeed.dropna(subset=['summary'], inplace=True) $ indeed.isnull().sum()
print("n_bytes = ", reader.n_bytes * 1E-9, "GB") $ print("n_rows = ", reader.n_rows) $ print("columns = ", reader.columns)
pd.Timestamp('2017-02-13') + pd.Timedelta('2D 3H')
train_commits, test_commits = train_test_split(commits_per_repo, test_size=0.33)
df_new_log['UK_ind_ab_page'] = df_new_log['country_UK']*df_new_log['group_treatment'] #add new column UK_ind_ab_page $ df_new_log['US_ind_ab_page'] = df_new_log['country_US']*df_new_log['group_treatment'] #add new column US_ind_ab_page $ logit_modC = sm.Logit(df_new_log['converted'], df_new_log[['intercept', 'group_treatment', 'country_US', 'country_UK', 'UK_ind_ab_page', 'US_ind_ab_page']]) $ resultsC = logit_modC.fit() $ resultsC.summary()
df=pd.read_csv("dataset_quora/info_test.csv")
df['currency'].unique()
joined.to_feather(f'{PATH}joined')
appointments.info()
sns.barplot(y = X.columns, x = clf.feature_importances_)
df_R['Year']=df_R['Date'].str.slice(0,4) $
train.StateHoliday = train.StateHoliday != '0' $ test.StateHoliday = test.StateHoliday != '0'
wrd_clean['expanded_urls'] = wrd_clean['tweet_id'].apply(lambda x: 'https://twitter.com/dog_rates/status/'+str(x)+'/photo/1')
engine.execute(create_new_table) $
corn.first()
def create_df_grouped_by_date( tweets_df ): $     return tweets_df.groupby( Grouper( 'date' )).mean()
df_tsv.head()
dates=pd.date_range(start='1/Oct/2020', periods=5, freq='3M') $ print(dates)
sum(df2.landing_page=='new_page') / df2.shape[0]
df2 = df.query('group == "control" & landing_page == "old_page" |' $                'group == "treatment" & landing_page == "new_page"') $ df2.groupby(['group', 'landing_page']).count()
X_test=test[features_to_use] $ X_test=sm.add_constant(X_test) $ X_test_pred=lmscore.predict(X_test) $ y_test=test.readingScore
twitter_data[twitter_data.expanded_urls.duplicated()].size
y_pred_class = nb.predict(X_test_dtm) $ from sklearn import metrics $ metrics.accuracy_score(y_test, y_pred_class)
result2.summary()
print grid.best_score_ $ print grid.best_params_ $ print grid.best_estimator_
brackets = [] $ for b in range(1,len(priceData)): $     brackets.append(int(priceData['TickerSymbol'][b].split(".")[0]))
df = df[['geoid','index_col','t10walk']] $ df = df.merge(pred, on='index_col', how='left').merge(geog, on='geoid', how='left') $ df.to_csv(OUTPUT_FOLDER + 'beh_nyc_walkability_prediction.csv', index=False)
random_numbers=pd.Series(np.random.randn(len(ind_date)),index=ind_date) # indexed dates with normal distributed random numbers $ random_numbers
y_pred = model.predict(X_test) $ y_pred = np.exp(y_pred) - 1 # undo the log we took earlier
sns.jointplot(x='auc', $               y='delta_recall', $               kind="reg", $               data=combined_df)
country_df = pd.read_csv('countries.csv') $ country_df.head() $ country_df.country.unique()
first_result.find('strong')
autos['registration_month'].value_counts()
data['popular'].shape
merged.groupby("committee_name_x").amount.sum().reset_index().sort_values("amount", ascending=False)
df_vow.plot() $ df_vow[['Open','Close','High','Low']].plot()
old_page_converted = np.random.binomial(1, p=p_old, size=n_old)
weather.columns = weather.columns.str.strip().str.upper().str.replace(' ','_') $ weather.dtypes
df[(df['outcome']=='Exposed to Potential Harm') | $    (df['outcome']=='No Negative Outcome')].count()[0]/df[df['public']=='offline'].count()[0]*100
print(news_titles_sr.iloc[0]) $
df[['total_spend','month','day','year','cust_id']].groupby(['year','cust_id','month','day','total_spend']).agg('sum').reset_index() $
cnf_matrix = confusion_matrix(y_test, yhat_knn, labels=['PAIDOFF','COLLECTION']) $ np.set_printoptions(precision=2) $ plt.figure() $ plot_confusion_matrix(cnf_matrix, classes=['PAIDOFF','COLLECTION'],normalize= False,  title='Confusion matrix')
scenario_dates = df_exp.groupBy('date').sum() $ var_rdd = scenario_dates.map(lambda r: (r[0], r[1], float(var(np.array(r[2:]) - r[1])))) $ df_var = sql.createDataFrame(var_rdd, schema=['date', 'neutral', 'var'])
log_reg_under.score(X_test, y_test_under)
print(df.index, '\n') $ print(df.columns, '\n') $ print(df.values,'\n')  #as numpy array $ print(df.values.shape)
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True)
pattern = re.compile(' +') $ print(pattern.split('AA   bc')) $ print(pattern.split('bcA A'))
print "Queen", sentiDf['2015-04-16' == sentiDf.index].mean().mean() # queen $ print "Space", sentiDf['2015-09-02' == sentiDf.index].mean().mean() # dane in space $ print "Gold OL", sentiDf['2016-08-15' == sentiDf.index].mean().mean() # Swimmer wins gold
all_gen2_verse=[] $ for verse in gen2: $     gen2verse = sent_tokenize(verse) $     all_gen2_verse.extend(gen2verse) $ print(all_gen2_verse)
stem_freq_dict = final_word_df.set_index('Word_stem')['Frequency'].to_dict()
df.printSchema()
loans_df['credit_line_days'] = (loans_df['issue_d'] - loans_df['earliest_cr_line']).dt.days
url = 'http://stat.data.abs.gov.au/sdmx-json/data/CPI/1.1.40055+115524.10.Q/all?detail=DataOnly&dimensionAtObservation=AllDimensions&startPeriod=2015-Q2' $ response = requests.get(url)
clean_appt_df = clean_appt_df.rename(index=str, $                      columns = {"Hipertension": "Hypertension", $                       "Handcap": "Handicap"}) $ clean_appt_df = clean_appt_df.drop(clean_appt_df[clean_appt_df['Age'] < 0].index) $ clean_appt_df.describe()
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
df.groupby('Year').get_group(2014)
tweet_archive_clean = tweet_archive.copy() $ info_clean = info.copy() $ images_clean = images.copy()
np.exp(-2.0375)
from scipy.stats import norm $ norm.cdf(z_score),norm.ppf(1-(0.05/2)) $
min_lbl = trn_labels.min() $ trn_labels -= min_lbl $ val_labels -= min_lbl $ c=int(trn_labels.max())+1
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key={}'.format(API_KEY))
users.gender.value_counts()
tweet_archive_enhanced_clean.loc[200]
USERNAME = '' $ TOKEN = '' $ API_BASE_URL = 'https://ooinet.oceanobservatories.org/api/m2m/12576/sensor/inv/'
df.info()
df_r = df2.copy()
(apple.tail(1).index[0] - apple.head(1).index[0]).days
plt.plot(weather['created_date'], weather['max']) $ plt.xticks(rotation='vertical')
plt.figure(figsize=(8, 5)) $ train_df.flow.value_counts().plot.bar(); $ plt.title('Number of #hab by flow') $ plt.xticks(rotation='horizontal'); $
sf_small_grouped = sf_small.groupby('start_date').agg({'duration':'size','Rain':'mean'})
data = data[(data['Date'].dt.month == 6)]
impressions_products.info()
master_df.loc[master_df.favorite_count > 0, ['favorite_count']].boxplot();
merged_df["T"] = (pd.datetime.now().date() - merged_df['first']).astype('timedelta64[W]') $ merged_df.head()
grouped = complete_df.groupby(by=[complete_df.Country, complete_df.Description, $                                   [x.month for x in complete_df.Date]])
fb['2012':].resample('W')['share_count'].sum()[ab]
staff = staff.reset_index() $ staff
DC_features = pd.read_csv('Historical_Work_Sunlight_Data.csv',index_col=0,parse_dates=True)
calls_df.loc[(calls_df["call_type"]=="Not Interested") | (calls_df["call_type"]=="Not Eligible"),"phone number"].nunique()
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \ $       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY $ req = requests.get(url)
zipcode = "nyc_zip.geojson" $ zc = gpd.read_file(zipcode) $ zc.shape
historicFollowers = pd.read_csv('Data/todaysFollowers_all.csv',sep = ';') #load historic Followers
ix_cape_town = df_protest.TownCity_Name=='Cape Town' $ df_protest.loc[ix_cape_town].head()
ocsvm_stemmed_tfidf.fit(trump_stemmed_tfidf, y = y_true_stemmed_tfidf) $ prediction_stemmed_tfidf = ocsvm_stemmed_tfidf.predict(test_stemmed_tfidf) $ prediction_stemmed_tfidf
import matplotlib.pyplot as plt
old_page_converted = np.random.choice([0,1], size = n_old, replace=True,p=[1-p_old, p_old])
df_state_votes[df_state_votes.State.isin([ $     "Alaska", "Alabama", "Iowa", $     "Delaware", "Vermont", "Hawaii" $ ])]
building_pa_prc_shrink[building_pa_prc_shrink.columns[0:5]].head(10)
_ = plt.scatter(df[df.amount > 5000].amount.values, df[df.amount > 5000].donation_date.values) $ plt.show()
data[["Class","V1"]].groupby(["Class"]).count()
result[(result['age']<18)&(result['dt_deces'].notnull())].shape
ds_valid = FileDataStream.read_csv(valid_file, collapse=False, header=False, names=columns, numeric_dtype=np.float32, sep='\t', na_values=[''], keep_default_na=False)
print("The maximum value of userID:") $ userArtistDF.agg(max("userID")).show()
lr.intercept_
lr = LogisticRegression(C=3.0, random_state=42)
train.age.value_counts().shape
titanic.pclass.value_counts(sort=False).plot(kind='bar')
len(set(ioDF.group_id_x))
df.head(5)
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Base.classes.keys()
first_result.contents[1][1:-2]
print('Largest High-Low range =', '{:.2f}'.format(max(highlow_range)))
Counter(tag_df.values.ravel()).most_common(5)
highest_scores = fcc_nn.loc[fcc_nn['score'] >= 10000].reset_index().drop(['level_0'], axis=1) $ highest_scores
print(len(df.index)) $ len(df[(df.group=="treatment") & (df.landing_page!='new_page')])+len(df[(df.group=="control") & (df.landing_page!='old_page')])
for o in ['Before', 'After']: $     for p in ['SchoolHoliday', 'StateHoliday', 'Promo']: $         a = o+p $         train[a] = train[a].fillna(0).astype(int) $         test[a] = test[a].fillna(0).astype(int)
df_merge.plot(title='retweet_count over favorite_count',x='favorite_count',y='retweet_count',kind='scatter',alpha=0.1);
builder.select('endcustomerlinefixed_data').distinct().show(10,False)
grades - grades.values.mean() # substracts the global mean (8.00) from all grades
knn = KNeighborsClassifier(n_neighbors = 3) $ knn.fit(X_train, Y_train) $ Y_pred = knn.predict(X_test) $ acc_knn = round(knn.score(X_test, Y_test) * 100, 2) $ acc_knn
station_281 = session.query(Measurement.date, Measurement.tobs).filter(Measurement.station == 'USC00519281')\ $ .filter(Measurement.date >= '2016-08-23').all()    
archive_copy['id_str'] = archive_copy['tweet_id'].astype(str)
df.sort_values('Shipped Created diff',ascending=False).head()
plt = sns.boxplot(data=df, x="race_desc", y="log_time_detained", hue="race_desc", dodge=False) $ plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) $ plt.set_xticklabels(plt.get_xticklabels(), rotation=45);
stoplist = stopwords.words('english') $ texts = [[word for word in pape.split() if word not in stoplist] for pape in all_text]
df_A.to_csv("classA.csv")
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 4 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
inspector = inspect(engine) $ inspector.get_table_names()
autos['date_crawled'].str[:10].value_counts(normalize=True, $                                            dropna=False).sort_index()
y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled) $ mse = mean_squared_error(y_test, y_pred) $ np.sqrt(mse)
users = pd.read_csv('../data_20180120/customer.csv').fillna('') $ users.head()
"issid" in df_protest.columns
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs) $ plt.xlabel('p_diffs') $ plt.ylabel('Frequency') $ plt.title('Simulated Difference of new_page & old_page converted under the Null');
df['num_comments'].describe()
b_list.shape
weather_missing.sum()
plt.hist(null_vals); $ plt.axvline(x=ob_dif, color='red');
twitter_df_clean['timestamp'] = twitter_df_clean['timestamp'].str[:-5]
qualification['opportunity_qualified_date'] = pd.to_datetime(qualification.opportunity_qualified_date)
soups = [soup(requests.get(url).text, 'html.parser') for url in article_urls]
autos= autos.drop('seller',1)
url = "https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?" + \ $       "&start_date=2017-01-01&end_date=2017-12-31&api_key=" + API_KEY $ req = requests.get(url)
b = pd.read_sql_query(QUERY,conn) $ b $
session.query(Stations.station,Stations.name,Stations.latitude, Stations.longitude,Stations.elevation).all() $
"my string".find('ng')
Recent_Measurements.describe()
festivals.head(3)
t1= twitter.copy() $ t2 = pred.copy() $ t3 = tweet_df.copy()
df_twitter_archive.source.value_counts()
ws.delete_cols(1)
df.info()
information_ratio * 1.2
data_311['CREATED_ON'] = pd.to_datetime(data_311['CREATED_ON']) $ highlandpark_potholes.index = highlandpark_potholes['CREATED_ON'] $ highlandpark_potholes['REQUEST_ID'].resample("M").count().plot(title="Highland Park Potholes", figsize=(10,6))
d.info()
df.dtypes
small_frame = loan_stats[1:3,['loan_amnt','installment']] $ single_col_frame = loan_stats[1:3,'grade'] $ small_frame.cbind(single_col_frame) 
rng = pd.date_range('1/1/2011', periods=72, freq='H') $ rng[:5]
vip_crosstab_percentage = vip_crosstab.apply(lambda x: x/x.sum(), axis=1) $ vip_crosstab_percentage
pred_probas_over_fm = gs_from_model.predict_proba(X_test) $ fm_bet_over = [x[1] > .6 for x in pred_probas_over_fm]
bigdf.shape
op_ed_articles = pd.read_json('nytimes_oped_articles.json') $ op_ed_articles = op_ed_articles.reset_index(drop=True) $ op_ed_articles.head()
flight_phase = faa_data_pandas['PHASE_OF_FLT'].value_counts() $ print(flight_phase)
import statsmodels.api as sm $ convert_old = df2.query('group == "control"').converted.sum() $ convert_new = df2.query('group == "treatment"').converted.sum() $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'"))
with open('turnstile_160305.txt') as f: $     reader = csv.reader(f) $     rows = [[cell.strip() for cell in row] for row in reader]
borough_group = data.groupby('Borough') $ borough_group.size().plot(kind='bar') $
session.query(Measurement.date).order_by(Measurement.date.desc()).first() $
temps_df.iloc[1]
sess = tf.Session() $ sess.run(tf.global_variables_initializer())
newdf["Hour_of_day"] = taxiData.lpep_pickup_datetime.apply(getHour)
df.groupby('episode_id')['id'].nunique().min()
twitter_df_clean.info()
df.head() $ df.iloc[:,[16,21,20, 0,1,2,5,6,7,8,9,10,22,14,15]]
merge_DA_imb_power_df.to_excel(data_folder_path + '/temp/merged_DA_imb_power_df.xlsx', index = False)
plt.hist(testdata.rating) $ plt.title("Testing Data Star to Own Distribution");
for name, in session.query(User.name).\ $ ...             filter_by(fullname='Ed Jones'): $ ...    print(name)
print("{:.2f} GB".format(df.fileSizeMB.sum() / 1024))
for i in check_cols: $     print(train[i].value_counts()) $     print('')
df2['intercept'] = 1 $ df2[['ab_page2', 'ab_page']] = pd.get_dummies(df2['group']) $ df2 = df2.drop('ab_page2', axis = 1) $ df2.head()
youthUser4['creationDate'] = pd.to_datetime(youthUser4['creationDate']) $ youthUser4['year'] = youthUser4['creationDate'].dt.year $ youthUser4['month'] = youthUser4['creationDate'].dt.month $ youthUser4.to_csv('lrng/youthUser4.csv')
df1[df1['dev_state']=='.'][['io_state','dev_state']]
n_old=len(df2.query('group=="control"')) $ print("n_old : {}".format(n_old))
vectorizer.vocabulary_
np.exp(-0.015)
pred_hacker_list = list(recommendation_df['hacker_id'].unique())
twitter_df_clean.loc[191]
num_users = df.user_id.nunique() $ num_users
IQR = scores_thirdq - scores_firstq $ print('The IQR of the ratings is {}.'.format(IQR))
B2_NTOT_WINTER_SETTINGS = lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'] $ lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'].allowed_variables $ lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'].settings.get_value('EK G/M', 22) $
df_anomalies = pd.read_csv('anomalies_vw.csv') $ df_anomalies = df_anomalies.drop('Unnamed: 0', 1) $ df_anomalies['period_start'] = pd.to_datetime(df_anomalies['period_start']) $ display(df_anomalies.head())
y = np.log(df['avgPrice']).values $ X = df.loc[:,['km','year','powerPS']].values
pipe_knn_2 = make_pipeline(tvec, knn) $ pipe_knn_2.fit(X_train, y_train) $ pipe_knn_2.score(X_test, y_test)
unzipfile("test.csv.zip","data") $ unzipfile("train.csv.zip","data")
merged_data.head(3)
sensor.unit
from scipy.stats import norm $ print("cdf: {}".format(norm.cdf(z_score))) $ print("ppf: {}".format(norm.ppf(1-(0.05/2)))) $
print(df.shape)
week20 = week19.rename(columns={140:'140'}) $ stocks = stocks.rename(columns={'Week 19':'Week 20','133':'140'}) $ week20 = pd.merge(stocks,week20,on=['140','Tickers']) $ week20.drop_duplicates(subset='Link',inplace=True)
ncTest.variables['tvar']
stock.iloc[-2]['next_day_open'] - stock.iloc[1640]['next_day_open']
df_new['ab_CA'] = df_new['ab_page']*df_new['CA'] $ df_new['ab_UK'] = df_new['ab_page']*df_new['UK'] $ lm = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA', 'UK','ab_CA','ab_UK']]) $ results3 = lm.fit() $ results3.summary()
data[data.state == 'open'].sort_values('updated_at')
def generate_returns(prices): $     returns = (prices - prices.shift(1))/prices.shift(1) $     return returns $ project_tests.test_generate_returns(generate_returns)
row = 0 $ df.loc[row,:]
... $ display(Markdown(fake_news_answer))
df[df['RT']==False][['op','text','time','ufol']].sort_values(by='ufol',ascending=False).head()
log.info("starting job") $ response = client.run_job(body=req_body) $ log.info("done with job")
psy_prepro = psy $ psy_prepro.to_csv("psy_prepro.csv")
data.iloc[140:170,]
autos[["date_crawled", "ad_created", "last_seen", "registration_month", "registration_year"]].head()
helpthispersonpls12 = df5['HT'].where(df5['andrew_id_hash'] == '8062624b2a0e63429aec70cb1f934fee704435ef').dropna() $ plt.hist(helpthispersonpls12, bins=20)
similarities = index[tfidf[new_vec]] $ recomendations = list(enumerate(similarities)) $ top_recomendations = sorted(recomendations, key=lambda rating: rating[1], reverse=True) $ top_users = top_recomendations[:10] $ repos_langs.iloc[[tr[0] for tr in top_users], 0]
os.environ['PROJECT'] = PROJECT $ os.environ['BUCKET'] = BUCKET $ os.environ['REGION'] = REGION
df2.drop(1899, axis=0, inplace = True) $ df2.query('user_id == 773192').index.get_values()
data.groupby('Agency').size().sort_values(ascending=False).plot(kind='bar', figsize=(20,4))
potential_accounts_buildings_info_tbrr = pd.DataFrame(potential_accounts_buildings_info_tbrr\ $                                                       [potential_accounts_buildings_info_tbrr[' Total BRR '] > 0]) $ potential_accounts_buildings_info_tbrr.drop([' AnnualRevenue ','NumberOfEmployees',\ $                                              ' DandB Revenue ', 'DandB Total Employees'], axis=1, inplace=True)
btc_price_df.sort_index(inplace=True)
f_yt_raw = '/scratch/olympus/projects/ideology_scaling/congress/youtube_links_raw.csv' $ df_yt.to_csv(f_yt_raw, index=False) $ shutil.chown(f_yt_raw, group='smapp')
for x in tweets_clean.dog_class.unique(): $     print('Mean retweets for ' + str(x) + ' class:' + str(tweets_clean[tweets_clean.dog_class == x].retweet_count.mean()))
logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']]) $ results = logit_mod.fit()
'Ohio' in frame3.columns
df.dtypes
df = pd.read_csv("detected.csv") $ df['META_b_cov'] = df['META_b'].where(df['metaREF']!=df['glmmREF'].str.lower(), -df['META_b'])
dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015', $                        '2015-Jul-6', '07-07-2015', '20150708']) $ dates
e_neg = df_elect[df_elect['Polarity'] < 0] $ e_neg.shape
cohort_retention_df = cohort_retention_df.append(cohort_retention_sum)
df_EMR_with_dummies.dtypes.unique()
stemmed_tokens = [] $ for t in clean_tokens: $     t = nltk.PorterStemmer().stem(t) $     stemmed_tokens.append(t) $ stemmed_tokens[:10]
dfRegMet.shape
df_parsed.plot();
df_agg= df_merge.groupby('start_date').agg({'id_x':'count','mean_temperature_f':'mean','cloud_cover':'mean'}).reset_index() $ df_filter = df_agg[(df_agg['start_date']<='2013-09-30')&(df_agg['start_date']>='2013-09-01')] $ df_filter['start_date'] = pd.to_datetime(df_filter['start_date'])
def ceil_dt(dt): $     delta = timedelta(minutes=15) $     return (dt + (datetime.min - dt) % delta) + timedelta(hours=2)
url = 'https://mars.nasa.gov/news/' $ browser.visit(url)
pr1=len(df2.query("group=='treatment' & converted == 1")) $ prob1 = pr1/length $ prob1
X.shape
non_rle_pscs = active_psc_records[active_psc_records.non_rle_country == True] $ len(non_rle_pscs.company_number.unique())
merged1.drop('id_y', axis=1, inplace=True)
unitech_df.tail(5)
print_feature_importance(vectorizer.feature_names_, rfc.feature_importances_)
(turnstiles_df $  .groupby(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"]) $  .ENTRIES.count() $  .reset_index() $  .sort_values("ENTRIES", ascending=False)).head(5)
df.apply(func8, axis=1)
df.loc['b']
data['Closed Date'] = data['Closed Date'].apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%y %H:%M'))
b = conn.get_bucket('ev.chicagoviolencepredictor') $ b.get_all_keys()
prediction_clean = prediction.copy() $ prediction_clean['tweet_id'] = prediction_clean['tweet_id'].astype('str')
top_20_breed_archive = tweet_archive_master[tweet_archive_master.dog_breed.isin(top_20_breeds)]
pre_strategy_users = people_person[pre_strategy] $ pre_strategy_users.head()
n_new = len(df2[df2.group == 'treatment']) $ n_new
libraries_df = libraries_df.merge(libraries_metadata_df, on="asset_id")
american_ratings.head(3)
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = .33, random_state= 7779311)
sn.distplot(train_binary_dummy['avg_time'])
c.insert_one(bowie)
day_of_week = pd.DatetimeIndex(pivoted.columns).dayofweek
result_concat.loc[3]
left_cols = ['Reg_date', 'id_partner', 'name'] $ right_cols = ['date_created', 'id_partner', 'campaign'] $ users_cost = pd.merge(users_cost, Costs, how = 'left', left_on=left_cols, right_on=right_cols) $ users_cost.head()
train['smart_author'] = train.author.isin(smart_authors.index).astype(int) $ train.groupby('smart_author').popular.mean()
data['pickup_cluster'] = id_pickup_label.astype(str) $ data['dropoff_cluster'] = id_dropoff_label.astype(str) $ data['ride_cluster'] = id_ride_label.astype(str)
viscorr = viscorr.drop(columns = ['BTC Price', 'BTC Price Change', 'BTC Volume', 'ETH Price', $                                   'ETH Price Change', 'ETH Volume']) $ viscorr = viscorr.drop(viscorr.index[-4:]) $ viscorr $
os.chdir(root_dir + "data/") $ df_fda_drugs_reported = pd.read_csv("filtered_fda_drug_reports.csv", header=0)
X_test.info()
new_log_m2 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'US_ab_page', 'CA_ab_page', 'US', 'CA']]) $ new_results2 = new_log_m2.fit() $ new_results2.summary()
activation = K.softmax(y) # Softmax
(fe.bs.SPXmean, fe.bs.SPXsigma)
N_samples = 25 $ x=np.linspace(-2,2,N_samples)
user_tweets= [] $ for tweet in tweepy.Cursor(api.user_timeline, screen_name=screen_name).items(): $     user_tweets.append(tweet._json) $ len(user_tweets)
clean_archive.rename(columns = {'tweet_id': 'id_str', $             'timestamp': 'created_at', $             'text': 'full_text', $             'expanded_urls': 'expanded_url'}, inplace=True) $ clean_predictions.rename(columns = {'tweet_id': 'id_str'}, inplace=True)
diff = (new_page_converted.mean() - old_page_converted.mean()) $ diff
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
len(train_data[train_data.gearbox == 'automatik'])
p = pd.read_sql_query(QUERY, conn) $ p
pop_rec = graphlab.popularity_recommender.create(sf_stars, $                                                 user_id = 'user_id', $                                                 item_id = 'business_id', $                                                 target = 'stars')
c.execute('SELECT Count(*) FROM weather') $ print(c.fetchall())
countries_df = pd.read_csv('countries.csv') $ df_joined = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_joined.head()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
sn.distplot(train['age'])
df1.info()
Meter1.RemoteSetup()
k150_bets_over = [x[1]>.6 for x in pred_probas_over_k150]
df_group_by2[columns_to_scale] = mms.fit_transform(df_group_by2[columns_to_scale])
df.dropna(axis=1, thresh=1000).shape
import numpy as np $ print("The median trading volume during this year is ",np.median(df["Traded Volume"])) $
dfChile = df[df["latitude"] > -56.000000]
pd.DataFrame(l_norm,columns=['score']).plot.hist() #1 user has 14.000 stars $ plt.xlabel('scores', fontsize=14) $ plt.ylabel('number of tests', fontsize=14) $ plt.title('UR scores test', fontsize=17) $ plt.savefig('ur_score_test.png')
pc_cz = pd.DataFrame(tt1_ok.groupby('CUISINECODE').size()) $ pc_cz.columns=['count'] $ pc_cz_fl = pc_cz.sort('count', ascending = False) $ pc_cz_fl['CUISINE_PROB'] = pc_cz_fl['count'] / sum(pc_cz_fl['count']) $ pc_cz_fl
filtered = []   # new list $ for w in words:  #storing only the words from the list 'words' which are not in the stopwords list, thus eliminating the stopwords $     if w not in stop_words: $         filtered.append(w)
testObj.buildOutDF(tst_lat_lon_df[0:600])
taxiData.Trip_distance.size
df_new[['UK', 'US', 'CA']] = pd.get_dummies(df_new['country']) $ df_new=df_new.drop('UK',axis=1) $
df_merged.head()
petitions=extract_field(tmp["results"]) $ petitions.to_csv("petitions.csv")
dr.get('https://ya.ru')
dog = shelter_cleaned_df.loc[shelter_cleaned_df['AnimalType'] == 'Dog'] $ dog.describe()
country_data.head(3)
clean_rates.shape
df = df.drop("a") $ df
df_protest.columns.size
df['converted'].mean() #. Overall mean of the whole column
print(df.loc[sd:ed].head()) #more pythonic? $ print(df[sd:ed].head())
total_sales.ratio.max()
sns.barplot(y = X.columns, x = list(logr.coef_.reshape(-1))*clf.feature_importances_)
df2 = df.query("group == 'treatment' and landing_page == 'new_page' or group == 'control' and landing_page == 'old_page'")
tweets_per_hour_df.plot.line(x='hour',y='number_of_tweets',figsize=(15,8),lw=1)
df['closed_at'] = pd.to_datetime(df['Closed Date'], format= '%m/%d/%Y %I:%M:%S %p')
try: $     import test_package.print_hello_function_container.print_hello_function $ except: $     print("Functions cannot be imported directly")
df, y, nas, mapper = proc_df(joined_samp, 'Sales', do_scale=True) $ yl = np.log(y)
df.info()
print('Before removing reactivations:',df.shape) $ df = df[df.Injury != 0] $ print('With only placements onto the Disabled List:',df.shape)
!wget http://files.fast.ai/data/dogscats.zip && unzip dogscats.zip -d data/
plt.bar(df['title'], df['sum(quantity)'], align='center') $ plt.show()
plt.figure(figsize=(20, 5)) $ distplot = sns.distplot(df.user_answer, bins=101, color='darkblue', kde=False)
for row in session.query(User.name.label('name_label')).all(): $ ...    print(row.name_label)
x = search.cv_results_["param_n_neighbors"].data $ score = search.cv_results_["mean_test_score"] $ yerr = search.cv_results_["std_test_score"] $ fig, ax = plt.subplots(figsize=(12,8)) $ ax.errorbar(x, score, yerr = yerr);
for c in ccc: $     for i in vwg[vwg.columns[vwg.columns.str.contains(c)==True]].columns: $         vwg[i] /= vwg[i].max()
top20_brands = autos.brand.value_counts().head(20).index
player['event'] = np.where((player['events'] == 'home_run'), #| $                            1, 0) $ player['event'].sum()
week39 = week38.rename(columns={273:'273'}) $ stocks = stocks.rename(columns={'Week 38':'Week 39','266':'273'}) $ week39 = pd.merge(stocks,week39,on=['273','Tickers']) $ week39.drop_duplicates(subset='Link',inplace=True)
filtered.to_csv('update/filtered-users.csv', index=False)
visit_num = grouper.apply(lambda df: (df.shift(1).results == 'Fail').cumsum())
row_0 = train.iloc[0]
from datetime import datetime $ print (datetime.fromtimestamp(1474416000))
def get_historical_price_timestamp(coin, to_curr=CURR, timestamp=time.time(), exchange=EXCHANGE, **kwargs): $
print(json.dumps(experiment_details, indent=2))
df.rename(columns={'Year_of_Release':'Year_of_Release_Sales'}, inplace=True) $
StockData.head(10)
%time df_columns['created_at'] = pd.to_datetime(df_columns['Created Date'], format="%m/%d/%Y %H:%M:%S %p") $
Inspection_duplicates = data_FCInspevnt_latest.loc[data_FCInspevnt_latest['brkey'].isin(vals)] $ Inspection_duplicates
t.to_csv('count.csv')
fm.head()
pd.set_option('mode.chained_assignment', None)
linkNYC.head()
to_be_predicted_Day4 = 22.24313147 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
archive_df.rating_numerator.value_counts()
len(df[df['converted']==1]['user_id'].unique())/ (1.0*n_unique_users)
from swat import * $ cassession = CAS(server, 5570, authinfo='~/.authinfo', caslib="casuser") $ cassession.loadactionset(actionset="table")
df = pd.read_pickle('data/df.pkl')
content = [item.find_all(['p','h2']) for item in article_divs] $
archive_copy = archive_copy.merge(predictions_copy, on='tweet_id', how='inner')
condos.shape
r= requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json") $ json_data = r.json() $ json_data["dataset_data"]["data"][0]
election_data = pd.read_excel("/Users/nicksteil/Desktop/election1216.xlsx") #reading in an excel file
m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars), $                    0.04, 1, [1000,500], [0.001,0.01], y_range=y_range) $ lr = 1e-3
df_anomalies.loc[:, 'tweets'] = df_anomalies.apply(lambda row: tweets_in_period(row['period_start']), axis=1) $ df_anomalies.loc[:, 'len_tweets'] = df_anomalies.apply(lambda row: len(row['tweets']), axis=1) $ display(df_anomalies.head())
test_portfolio.rename(columns={'metric_date': 'date'}, inplace=True)
df.count()
timestart = time.time() $ with Pool(1) as p: $     result  = p.map( np.sin, range(1,100) ) $ timeend = time.time() $ print(timeend-timestart)
url = 'https://mars.nasa.gov/news/'
X_new = pd.DataFrame({'TV': [40]}) $ X_new.head()
pnew = df2.converted.mean() $ print(pnew)
autos["registration_year"].describe()
tsla_tuna_neg = mapped.filter(lambda row: (row[3] < 0 and row[4] < 0)) $
len(model.author2doc['Microsoft'])
df3 = pd.read_csv('countries.csv') $ df3.head()
df_joined[['CA','UK','US']] = pd.get_dummies(df_joined['country'])
n_final_participants = pax_raw.seqn.nunique() $ print(f'Total participants remaining in analysis set: {n_final_participants}')
data.loc[data.expenses < 150, 'expenses'] = np.NaN
mars_html_table = mars_html_table.replace("\n", "") $ mars_html_table
fig = plt.figure(figsize=(11,11)) $ ax = fig.add_subplot(111) $ ax.axis('off') $ pumashplc.assign(cl=LinkNYCpLagQ10.yb).plot(column='cl',edgecolor='grey',ax=ax,legend=True,cmap='hot_r',categorical=True)
p_old = df2['converted'].mean() $ print("The converted rate for P(old) = ", p_old)
data = check_y(df, delta_change=-3.0, start = 560, end = 600) 
suspects_with_25_1['in_cp'] = suspects_with_25_1.apply(in_checkpoint, axis=1)
linked = pd.Series(list(set(gh_hashlist["emailhash"]).intersection(set(so_hashlist["emailhash"]))))
df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'uk', 'us']]) $ results = logit_mod.fit() $ results.summary()
points2.isnull()
prob_convert_c = df2_control.converted.mean() $ prob_convert_c
df.head()
t1 = datetime.datetime(2016,12,1) $ t2 = datetime.datetime(2016,10,1, tzinfo=timezone('UTC'))
movies.info()
omega = numpy.logspace(-3, -2) $ s = omega*1j
obs_diff = df2.query('group == "treatment"')['converted'].mean() - df2.query('group == "control"')['converted'].mean() $ (obs_diff < p_diffs).mean()
vip_df = pd.read_csv("/Users/erikgregorywebb/Downloads/VIPKID_Raw.csv") $ vip_df.head()
so.loc[so['viewcount'] > 500000, col_bools]
oppose=merge[merge.committee_position=="OPPOSE"]
df.head()
%%R $ numericDB <- select(flightsDB, -c(CRS_DEP_TIME, CRS_ARR_TIME)) $ write.csv(numericDB, 'FinalFlightsNumeric.csv')
GBR.score(X_test,Y_test)
users_nan = (users.isnull().sum() / users.shape[0]) *100 $ users_nan
pd.DataFrame(sorted([[n, len(g)] for n,g in pd.groupby(topics, by=topics.name)], key=lambda x: x[1])[-9:], columns=["name", "count"])
sumTable = tips.groupby(["sex","day"]).mean() $ sumTable
autos['odometer']=autos['odometer'].str.replace('km','').str.replace(',','').astype(int)
pageviews_mobile_df = (all_data_as_dfs['pageviews_mobile-web']['views'] + all_data_as_dfs['pageviews_mobile-app']['views']).to_frame() $ pageviews_mobile_df['access'] = 'mobile' $ pageviews_mobile_df['api'] = 'pageviews' $ pageviews_mobile_df[:3]
ans = df.groupby(['A','B']).sum() $ ans
facts_metrics.groupby('dimensions_item_id').size().describe()
df2[df2.converted == 1].shape[0]/df2.shape[0]
df2.drop(2893,inplace=True) $
df_parcel = pd.DataFrame(json.loads(response.content)) $ df_parcel.head()
soup.li.parent.text
person = df[df["screen_name"]==person] $ person.head(100)
grouped = writers.groupby(lambda x: age_groups(writers,x,'Age')) $ grouped.groups
right = pd.DataFrame({'key' : ['bar', 'foo'], 'rval': [4,5]}) $ right
df = pd.DataFrame(recent_prcp_data).dropna() $ df['date'] = pd.to_datetime(df['date']) $ df.set_index('date', inplace=True) $ df.head()
pulledTweets_df.head(2)
response = requests.get('https://api.github.com/repos/greenelab/deep-review') $ result = response.json() $ stats['github_stars'] = result['stargazers_count'] $ stats['github_forks'] = result['forks_count']
sl_pf_v2.sample()
df_final.to_csv('twitter_archive_master.csv', encoding='utf-8')
data = pd.read_csv(datafile,low_memory=False) $ data
for column in all_df.columns: $     print ("{:<20s} {:>6.0f}".format(column, all_df[column].nunique())) $
pattern = r'^[a-z]' $ mask = df.name.str.contains(pattern) $ mask.value_counts()
(df2.query('group == "control"').converted).mean()
%%timeit $ scipy.optimize.broyden1(globals()[function_name], 2, f_tol=1e-14)
for iter_x in np.arange(lookforward_window)+1: $     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
lst_dates = list(cov_df.index.get_level_values(0).unique()) $ lst_dates
if df.isnull().values.any() == False: $     print('No rows have missing values') $ else : print(df.info())
df_R.head()
df.index  # Display the index only
data = [1,2,3,4,5] $ df = pd.DataFrame(data) $ print(df)
df2.shape
group_software = lift.get_software_used_by_group('APT12')
Merge = pd.merge(Project, Pipeline, how='outer', on=['Date'])
print(cc['slug'].describe()) $ print(cc['symbol'].describe())
vader_df.head()
prcp_data_dict= prcp_data_df.to_dict(orient='records')
tweep_group=tizibika.groupby('user') $ tweep_most_likes=tweep_group['likes'].sum() $ tweep_most_likes_df_temp=pd.DataFrame(tweep_most_likes) $ tweep_most_likes_df=pd.DataFrame(tweep_most_likes_df_temp['likes']) $
df.describe()
files = [local_orig, local_edit] $ for file in files: $     with rio.open(file, 'r') as src: $         profile = src.profile $         print(profile)
y_pred = regr.predict(X_test) $ print("Mean squared error: %.2f" $       % mean_squared_error(y_test, y_pred)) $ print('Variance score: %.2f' % r2_score(y_test, y_pred))
import numpy as np $ change = [] $ for entry in d["dataset_data"]["data"]: $     change.append(entry[4]) $ print("The largest change in a given day was $"+str(max(np.diff(change))))
df.data.head()
x = pd.Series([0.0, 0.1, 0.2], index=['a', 'b', 'c'])
bins = [5, 25, 50, 100, 200, 300, 400, 500, 1000, 5000] $ df3_freq = df3['Donation Amount'].value_counts(bins=bins).sort_index() $ df3_freq.plot(x='Donation Amount', y='Donation Amount count', kind= 'line') $ plt.show() $
autos["price"]= autos["price"].str.replace("$",'')
df = pd.read_csv('data/bigmac.csv') $ df.head()
df_day = endog.to_frame() $ df_day.rename(columns={"BikeID" : "Count"}, inplace=True)
start = datetime.now() $ modelxg_lr1 = xgb.XGBClassifier(max_depth=15, learning_rate=.1, n_estimators=250, n_jobs=-1) $ modelxg_lr1.fit(Xtr.toarray(), ytr) $ print(modelxg_lr1.score(Xte.toarray(), yte)) $ print((datetime.now() - start).seconds)
index_df.rename(columns={'adj_close_price':'SPX'}, inplace=True)
elms_all.ORIG_DATE.min()
train_data['abtest_int'] = train_data['abtest'].apply(get_integer1) $ test_data['abtest_int'] = test_data['abtest'].apply(get_integer1) $ del train_data['abtest'] $ del test_data['abtest']
p_val = (null_vals > p_convert_obs_diff).mean() $ p_val
def print_rmse(model, name, input_fn): $   metrics = model.evaluate(input_fn=input_fn, steps=1) $   print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['loss'])) $ print_rmse(model, 'validation', make_input_fn(df_valid))
first_votes = first_movie.find('span', attrs = {'name':'nv'}) $ first_votes
convert_new = df2.loc[(df2.landing_page == "new_page") & (df2.converted == 1)].user_id.nunique() $ convert_new
df1["date"]= pd.to_datetime(df1["date"], format='%Y-%m-%d', errors='coerce') $ df1.set_index("date", inplace=True)
df = pd.read_sql_query("SELECT * FROM Indiana.sesiones_page_screen_w_categL1 where platform = 'iOS' and date >='20170701' and date <= '20170712' and page = '/VIP/ITEM/MAIN/' and site = 'MLA' and site = 'MLA'limit 100", cnx) $ df.tail(10)
for df in (joined, joined_test): $   df['Promo2Since']=pd.to_datetime(df.apply(lambda x: Week(x['Promo2SinceYear'],x['Promo2SinceWeek']).monday(),axis=1).astype(pd.datetime)) $   df['Promo2Days']=df['Date'].subtract(df['Promo2Since']).dt.days
country_dummies = pd.get_dummies(df_new['country']) $ df_new = df_new.join(country_dummies)
store_items = pd.DataFrame(items2, index = ['store 1', 'store 2']) $ store_items
wmf.hive.run( $ )
sub_dataset['SectionName'].value_counts(sort=True, normalize=True)
autos['last_seen'].str[:10].value_counts()
lc_review = pd_review["text"][0].lower() $
cityID = '53b67b1d1cc81a51' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Birmingham.append(tweet) 
len(topUserItemDocs['item_id'].unique())
def from_pickle(filename): $     with open(filename, 'rb') as f: $         return pickle.load(f)
df['site_admin'].unique()
Google_stock.head()
crimes.describe(include=['O'])
df_users.iloc[:,4] = pd.to_datetime(df_users.iloc[:,4], format='%Y-%m-%d %H:%M:%S') $ df_events.iloc[:,5] = pd.to_datetime(df_events.iloc[:,5], format='%Y-%m-%d %H:%M:%S') $ df_events.iloc[:,7] = pd.to_datetime(df_events.iloc[:,7], format='%Y-%m-%d %H:%M:%S') $ df_log.iloc[:,2] =  pd.to_datetime(df_log.iloc[:,2], format='%Y-%m-%d %H:%M:%S') $ unknown_users.iloc[:,2] = pd.to_datetime(unknown_users.iloc[:,2], format='%Y-%m-%d %H:%M:%S')
temp_df = df.copy() $ temp_df.index = df.index.set_names('Desc.', level='Description') $ temp_df.head(3)
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(1, p_null, n_new) $     old_page_converted = np.random.binomial(1, p_null, n_old) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())
obs_diff = df2.query("group == 'treatment'")['converted'].mean() - df2.query("group == 'control'")['converted'].mean()   $ obs_diff
result_df = sqlClient.get_result(jobId) $ print("OK, we have a dataframe for the SQL result that has been stored by SQL Query in " + sqlClient.get_job(jobId)['resultset_location'])
restaurants.rename(columns={"VIOLATION CODE": "VIOLATION"}, inplace = True)
noNulls.show(5)
wb.search('cell').head()
height.dropna()
rng = pd.date_range('1/1/2012', periods=5, freq='M')
stations = session.query(Measurement).group_by(Measurement.station).count() $ print(stations)
contributions_by_day = sorted_by_date.groupby(['contb_receipt_dt']).sum().reset_index() $ contributions_by_day.contb_receipt_dt = pd.to_datetime(contributions_by_day.contb_receipt_dt) $ contributions_by_day.sort_values('contb_receipt_dt', inplace=True, ascending=True) $ contributions_by_day
amenities = class_data.iloc[:, [3] + [14] + list(range(19,41))] $ amenities.head()
uber_15["day_of_week"].value_counts()
df.head()
Final_question_df = pd.read_csv('/Users/Gian/GitHub/Giancarlo-Programming-for-Analytics/Assignments/financial_table.csv')
pca = sklearnPCA(n_components=1) $ transf = pca.fit_transform(X_cols) $ transf_train = transf[mask] $ transf_test = transf[~mask] $ print transf_train[:5]
m3.freeze_to(-1) $ m3.fit(lrs/2, 1, metrics=[accuracy]) $ m3.unfreeze() $ m3.fit(lrs, 1, metrics=[accuracy], cycle_len=1)
nba_df = pd.read_csv("NBA_GameLog_2010_2017.csv")
r = requests.get("https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2017-01-01&end_date=2017-01-03&api_key="", auth=('')) $
obj4.isnull()
print(z_score) $ print(p_value) $ print( norm.ppf(1-(0.05/2)))
df.drop(['MINUTE'], axis = 1, inplace=True)
excelDF.iat[1,0]
df.text[226]
pageRankPlot = prDf.head(33).plot.barh(title="PageRank score distribution", figsize=(14,7)) $ pageRankPlot = pageRankPlot.get_figure() $ pageRankPlot.savefig("plots/pageRankPlot.png")
df = pd.read_csv("btime.csv", index_col = 0) $ df
finalDf.loc[:0:]
try: $     print("[+] 1337/0 = " + str(1337/0)) $ except: $     print("[-] Error. ") $
reorder = options_data.reindex(columns=['MATURITY', 'STRIKE', 'PRICE', 'IMP_VOL'])
pd.DataFrame ([[101,'Alice',40000,2017], $                [102,'Bob',  24000, 2017], $                [103,'Charles',31000,2017]], columns = ['empID','name','salary','year'])
obs_diff = new_page_converted.mean() - old_page_converted.mean() $ print(obs_diff)
archive_clean.info()
print(df_aggregate.head())
a = [1,2,0,0,4,0] $ nonz = np.nonzero(a) $ print(nonz)
print(data[data['volume'] == 0].shape[0]) $ data[data['volume'] == 0]
model = Sequential() $ model.add(Dense(LATENT_DIM, activation="relu", input_shape=(2*T,))) $ model.add(Dense(HORIZON))
rs.best_params_
embedding_save_path = os.path.join(pathModel, "data.npy")
df_2014['bank_name'] = df_2014.bank_name.str.split(",").str[0] $
tia.shape
(data_archie['cur_sell_price'] == 0.0).sum()
coin_data.columns
log_r = sm.Logit(df2["converted"], df2[["intercept", "ab_page"]]) $ res = log_r.fit()
df_final.head(3)
frame = pd.DataFrame(np.arange(9).reshape((3, 3)), index = ['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])
df_example3 = rdd_example3.toDF() $ df_example3.registerTempTable("df_example3") $ print type(df_example3)
fullData['change'].value_counts().keys()
pct = pd.read_csv('170418_3_pp_w_foursquare_prep.csv') $ pct = pct.drop('Unnamed: 0',axis=1) $ pct.head(2)
entries = entries.json() $
twitter_archive.sample(10)
df_master = pd.merge(df_clean, image_clean, on ='tweet_id', how= 'inner' ) $ df_master = pd.merge(df_master, tweet_clean, on = 'tweet_id', how = 'inner' )
new_page_converted = np.random.choice([0, 1], $                                       p=[1. - p_convert_treatment, p_convert_treatment], $                                       size=n_treatment, $                                       replace=True) $ new_page_converted.mean()
for tag,val in tag_cloud.items(): $     print(tag, " ", val)
df_master[df_master.favorite_count == [df_master['favorite_count'].max()]]
to_be_predicted_Day5 = 14.52788976 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
print(16 in squares) $ print('square of 4' in squares)  # calls .__contains__()
df3 = df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')] $ df4 = df[(df['group'] != 'treatment') & (df['landing_page'] == 'new_page')] $ df3.shape[0] + df4.shape[0]
db_file = '/home-assistant_v2.db' $ DB_URL = 'sqlite:////' + os.getcwd() + db_file $ DB_URL
day_of_month14.to_excel(writer, index=True, sheet_name="2014")
import pandas as pd $ import numpy as np $ import math $ import matplotlib.pyplot as plt $ %matplotlib inline
def remove_punctuation(text): $     import string $     map = text.maketrans('', '', string.punctuation) $     return text.lower().translate(map) $ sample['text_clean'] = sample['text'].apply(remove_punctuation)
to_be_predicted_Day2 = 38.20895047 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
ave_ratings_over_time = tweet_archive_master[['timestamp', 'rating_numerator']].groupby(pd.Grouper(key='timestamp', freq='M')).mean() $ ave_ratings_over_time.index = tweet_archive_by_month.index.map(strip_date)
df['intercept']=1 $ df[['control', 'treatment']] = pd.get_dummies(df['group']) $ df['ab_page'] = df['treatment'] $ df = df.drop(['control', 'treatment'], axis=1) $ df.head()
liquor["Margin"] = (liquor["State Bottle Retail"] - liquor["State Bottle Cost"]) * liquor["Bottles Sold"] $ liquor["Price per Liter"] = liquor["Sale (Dollars)"] / liquor["Volume Sold (Liters)"] $ liquor.head()
import pandas as pd $ import numpy as np $ pd.options.mode.chained_assignment = None  # default='warn' $ import glob as glob
plate_appearances = plate_appearances.merge(events, on='atbat_pk', suffixes=['_ignore', ''])
users.loc[users['Age'] > 90, 'Age'] = np.nan
tweets_by_user = pd.read_sql_query(query, conn, parse_dates=['created_at']) $ tweets_by_user.head()
temporal_group = 'weekly' $ df = pd.read_csv('../data/historical_data_{0}.csv'.format(temporal_group)) $ df['date'] = pd.to_datetime(df['date']) $ df = df.set_index('date')
df2['user_id'][(df2['user_id'].duplicated() == True)]
print gs_2.best_params_ $ print gs_2.best_score_
SSE=((lmscore.predict(X_test)-y_test)**2).sum() $ SSE
doctors.head()
chart.set_yticklabels(top_supporters.contributor_lastname)
df2.converted.sum()/df2.user_id.nunique()
result['createdBy'].value_counts()
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit()
outcome=[0,1] $ new_page_converted=np.random.choice(outcome,n_new, p=(1-p_new,p_new)) $ new_page_converted
print(np.mean(new_page_converted)-np.mean(old_page_converted))
retweets.head(1)
train_frame = train_frame.reset_index() $ df_series = pd.Series(train_frame["values"]) $
p_diffs = np.array(p_diffs) $ plt.hist(p_diffs);
from sklearn.linear_model import LogisticRegression $ from sklearn.ensemble import RandomForestClassifier $ from sklearn.metrics import confusion_matrix, f1_score $ from sklearn.cross_validation import cross_val_score
with_condition_heatmap_quer5.add_child(plugins.HeatMap(list_query_5[:40000], radius=15)) $ with_condition_heatmap_quer5
print df.nrOfPictures.sum(), '\n' $ print df.seller.value_counts(), '\n' $ print df.offerType.value_counts(), '\n' $ df.drop(['nrOfPictures', 'seller', 'offerType' ], axis=1, inplace=True)
features_df = afl_feature_creation_v2.prepare_afl_features(afl_data=afl_data, match_results=match_results, odds=odds)
print('Logreg intercept:', log_reg.intercept_) $ print('Logreg coef(s):', log_reg.coef_)
predictions_clean.info()
train_users.country_destination.value_counts()
measure_nan.count()
notlive_live_woc = live_woutcome[(live_woutcome['state'] == 'failed') | (live_woutcome['state'] == 'successful')] $ notlive_live_woc = notlive_live_woc[['name','category_name','blurb','blurb_count','goal_USD','backers_count',\ $                                'launched_at','state_changed_at','days_to_change','state']] $ print len(notlive_live_woc) $ notlive_live_woc.head()
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05/2))) $
df_ = df.groupby('msno').apply(within_n_days, T, n = 14).reset_index(drop = True) $ tbl = df_[df_.date_diff == 1].groupby('msno').date_diff.size().to_frame() $ tbl.columns = ['listen_music_in_a_row_count_during_t_14'] $ tbl['listen_music_in_a_row_ratio_during_t_14'] = tbl.listen_music_in_a_row_count_during_t_14 / df_.groupby('msno').date_diff.apply(len) $ tbl.reset_index(inplace = True)
df_freq_users = df[df["user_id"].duplicated()]
df_con.head()
latest_df = raw_df[raw_df.workflow_version == 47.58].copy()
help(scipy.optimize.root)
print("zscore: {}, pvalue: {}".format(z_score, p_value));
results.summary()
converted = df2.query('converted == 1').count()[0]/df2['landing_page'].count() $ converted
for tz in pytz.all_timezones: $     print(tz)
df_goog['Closed_Higher'] = df_goog.Open > df_goog.Close $ df_goog['Closed_Higher'] = pd.get_dummies(df_goog.Open > df_goog.Close).values
np.exp(0.0506)
df_new['timestamp'].min(), df_new['timestamp'].max()
Ralston.loc[:,"T_DELTA"] = Ralston.loc[:,"TMAX"] - Ralston.loc[:,"TMIN"] $ Ralston.head()
import sqlite3 $ sqlite_file = 'mydb' $ cnx = sqlite3.connect(sqlite_file) $ c = cnx.cursor()
lm=sm.Logit(df2['converted'],df2[['intercept','new','ab_page']]) $ results=lm.fit() $ results.summary()
df.shape, len(pd.unique(df.createdTime))
active_station_df = pd.DataFrame(active_station[:], columns=['station','tobs',]) $ active_station_df.sort_values("tobs", ascending=[False], inplace=True) $ active_station_df.set_index('station', inplace=True) $ active_station_df.head()
pt=pd.pivot_table(data=cust_demo, index=['Location','Martial_Status'], columns=['Gender', 'Own_House'], values='age',aggfunc='mean' )
column_list1 = ['DewPoint'] $ df[column_list1].plot() $ plt.show()
Xt=pd.DataFrame(Xt)
group1 = df.query("group == 'treatment' and landing_page == 'old_page'") $ group2 = df.query("group == 'control' and landing_page == 'new_page'") $ Number_times = len(group1) + len(group2) $ Number_times
fig, axes = plt.subplots(3, figsize=(10, 8)) $ plot_series_and_differences(series=RN_PA_duration, ax=axes, num_diff=2, title='RN/PA') $ fig.tight_layout()
pd.value_counts(no_specialty['ReasonForVisitName'])
test_doc = doc_id_list[['test' in doc_id for doc_id in doc_id_list]] $ print("test_doc is created with following document names: {} ...".format(test_doc[0:5]))
df_mes['tpep_pickup_hour'] = pd.DatetimeIndex(df_mes['tpep_pickup_datetime']).hour $ df_mes['tpep_dropoff_hour'] = pd.DatetimeIndex(df_mes['tpep_dropoff_datetime']).hour
X = pd.get_dummies(df['subreddit']) $ y = df['HIGH_LOW'] $ from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
bd.index
Z = np.arange(10) $ np.add.reduce(Z)
customer_visitors_new = customer_visitors.groupby([customer_visitors.DateCol.dt.year, customer_visitors.DateCol.dt.dayofweek]).mean() $ customer_visitors_new
students.iloc[0:3,[0,2]]
df_never_moved.info() #remove Time
github_data.isnull().sum()
df2 = pd.DataFrame()   # create a new dataframe object $ df2['cust_avail_v3'] = cust_avail_v3   # apply the random column to the new dataframe $ df2.head()
df3 = df2.drop(['FlightDate', 'DepTime', 'DepTimeStr'], axis=1)
token.date = token.date.map(lambda x : datetime.datetime.strptime(str(x)[:10],"%Y-%m-%d"))
high_hba1c.limit(10).toPandas()
twitter_merged_data.describe()
BroncosBillsPct.loc[25441]['text30']
from nltk import pos_tag, ne_chunk $ from nltk.tokenize import word_tokenize $ tree = ne_chunk(pos_tag(word_tokenize("Antonio joined Udacity Inc. in California."))) $ IPython.display.display(tree)
containers[0].find("li", {"class":"name"}).a['title']
size_t = df2.query("group=='treatment'").count()[0] $ size_t
X_features = features.iloc[:, 1:8].as_matrix() $ y_features = features.iloc[:, 8].as_matrix() $ y_features = y_features * 5 $
plt.figure() $ df.T.describe().T["unique"].plot()
p_old = df2['converted'].mean() $ print("The convert rate for p_old under the null: ",p_old)
data = pd.Series([1, "quick", "brown", "fox"], name="Fox") $ data
control_df = df2.query('group=="control"') $ converted_control = control_df.query('converted == 1') $ p_old_act = float(converted_control.user_id.nunique()/control_df.user_id.nunique()) $ print('Prob. old page actual {}'.format(p_old_act))
new_page_converted = np.random.choice([0, 1], size=df_n.shape[0], p=[(1-p_new), p_new])
m_s_df = stops_per_crime_per_month.groupby(stops_per_crime_per_month.index.month).mean().reset_index() $ m_s_df['month'] = ['Jan','Feb','Mar','Apr','May','Jun','July','Aug','Sep','Oct','Nov','Dec'] $ m_s_df.index = m_s_df.month $ del m_s_df['month']
df.shape
results = pd.read_csv("../01-data/processed/curieuzeneuzen-results.csv")
for ind, column in enumerate(reddit.columns): $     print(ind, column)
for tweet in collect.get_iterator(): $     print(json.dumps(tweet, indent=2)) $     break
plt.figure(figsize=(16, 5)) $ plt.plot(news.date, news.title.apply(len));
df.head()
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
url = "http://www.fifeweather.co.uk/cowdenbeath/200606.csv"
df_G = pd.DataFrame({'key': list('bbacccb'), $                  'val': np.random.randn(7) }).round(2) $ df_G $ print df_G,'\n' $ print df_G.join(pd.get_dummies(df_G['key'], prefix='dummy')).drop('key', axis=1).drop('dummy_c', axis=1)
testurl = 'https://42hire.com/20-reasons-why-working-from-home-is-way-better-than-working-in-an-office-with-people-7590d54980c8' $ df_test = df_merged[df_merged[df_merged.columns[0]]==testurl] $ df_test.head()
df_positive = data[data['SA']>0].copy() $ df_positive.sort_values(by='Likes',ascending=False,inplace=True) $ df_positive.head()
iowa.shape
print(mbti_text_collection_filler.drop('text', axis=1)[mbti_text_collection_filler.text_count<50])
for index, row in anomaly_df.iterrows(): $     show_news_1day(row.Ticker, row.Date)
df.shape $ df.columns.values
df_usa['PD2'] = np.around(df_usa['Total population']/df_usa['Area'], decimals = 2) $ df_usa
pd.merge(pd.concat([df_a, df_b]), df_c, left_on = "mathdad_id", right_on = "mathdad_id") # same as above 
import folium $ from folium import plugins $ print(folium.__version__)
!rm output.json $ !aws sagemaker-runtime invoke-endpoint --endpoint-name tensorboard-names-2018-03-20-22-40-47-154 --body '{"name":"barnam"}' --content-type "application/json" output.json $ ! cat output.json
dep.add_account(yah) $ dep.add_account(stx) $ dep.account_infos
complete_df['group_country'] = complete_df['group'] + '_' + complete_df['country'] $ complete_df.head()
mb.sortlevel('Patient', ascending=False).head()
df_adjusted[['adjusted_numerator', 'rating_numerator_x']].iloc[np.r_[0:5, -10:0]]
len(df['lang'].value_counts())
integratedData.head(10)
r_client_previous = ft.Relationship(es['clients']['client_id'], $                                     es['loans']['client_id']) $ es = es.add_relationship(r_client_previous)
plt.figure(figsize=(16,8)) $ dendrogram(features['news24']['linkage'], orientation='top', $           p=300, truncate_mode='lastp', no_labels=True, color_threshold=0) $ plt.axes().get_yaxis().set_visible(False) $ plt.show()
lm = sm.OLS(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ result3 = lm.fit() $ result3.summary()
results=[] $ for tweet in tweepy.Cursor(api.search,q="Dreamers").items(10): $     results.append(tweet)
evaluator = MulticlassClassificationEvaluator( $     labelCol="label", predictionCol="prediction", metricName="weightedPrecision") $ precision = evaluator.evaluate(predictions) $ print("Precision = %g " % (precision))
df_new[(df_new['group'] == 'treatment')]['converted'].mean()
plt.figure(figsize=(12,8)) $ sns.violinplot(x=trump['source'], y= 'num_punc', data= trump) $ plt.xlabel(' Source of Tweet', fontsize=12) $ plt.ylabel('Amount of Punctuation', fontsize=12) $ plt.title('Amount of Punctuation per Tweet by Source', fontsize=15)
firsts = live[live.birthord == 1].birthwgt_lb.dropna() $ others = live[live.birthord != 1].birthwgt_lb.dropna()
train.sort_values('num_points', ascending=False).head()
def logtorec(x): $     ypred = list(zip(x, list(range(100)))) $     ypred5 = ' '.join([str(t[1]) for t in sorted(ypred, reverse=True)][:5]) $     return ypred5
print(weights_A.shape,end='\n') $ print(weights_A)
p_mean = np.mean([p_new, p_old])  
aviation = disaster_tables[2] # reading from html $ aviation.head()
nb.fit(X_train,y_train) $ nb_pred = nb.predict_proba(X_test)
import ssbio.core.io $ my_saved_gempro = ssbio.core.io.load_json('/tmp/mtuberculosis_gp_atlas/model/mtuberculosis_gp_atlas.json', decompression=False)
import datetime $ day = lambda x: x.split(' ')[0].replace('-',',') $ emotion_big_df['date']=emotion_big_df['createdAt'].apply(day)
archive_clean['timestamp'] =pd.to_datetime(archive_clean.timestamp)
conn.addtable(table='myclass', caslib='casuser', replace=True, $               **mydmh.args.addtable)
recip_treatment = round(1/np.exp(-0.0150), 2) $ print('For every 1 unit decrease in conversion, treatment is {} times as likely holding all else constant.'.format(round(recip_treatment, 2)))
treatment = df2['group'] == 'treatment' $ obs_conv_new = df2[treatment]['converted'].mean() $ print(obs_conv_new)
weeks = pd.date_range(start=datetime.date(2016,1,1), $                       end=datetime.datetime(2016,12,31), $                       freq='W') $ weeks
print(min_max_dict_model.name, min_max_dict_model.description, min_max_dict_model.version, sep = '\t')
xml_in_merged = pd.merge(xml_in_sample, grouped_authors_by_publication, on=['publicationKey'], how='left')
df.num_comments = df.num_comments.apply(lambda x: x.replace(' comment', ''))
hgt_str = bb_df.Ht.values $ hgt_str = [x.split("-") for x in hgt_str] $ hgt_in = [(int(x[0]) * 12) + int(x[1]) for x in hgt_str] $ se = pd.Series(hgt_in) $ bb_df.insert(loc = 8, column= 'Inches', value = se)
sentencelist = map(lambda text: list(jieba.cut(text,cut_all=True)),kate_df.text) $ reducedwordlist = reduce(lambda x,y:x+y,sentencelist)
print(df.shape) $ print(df.describe())
autos["vehicle_type"].value_counts()
learner.model.load_state_dict(wgts)
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB.json?start_date=2014-01-01&end_date=2014-01-03&api_key=HL_EJeRkuQ-GFYyb_sVd')
((loans.originated_since_date<datetime.date(2015,2,28))).sum()
from sklearn.model_selection import GridSearchCV, KFold
tweet_archive_enhanced_clean['p1_dog'].value_counts()[True]/tweet_archive_enhanced_clean['p1_dog'].count()
df2_control = df2.query('group == "control"').converted.mean() $ df2_control
average_polarity.to_csv('polarity_results_LexiconBased/weekly/polarity_avg_weekly_2012_2016.csv', index=None) $ count_polarity.to_csv('polarity_results_LexiconBased/weekly/polarity_count_weekly_2012_2016.csv', index=None)
abstract = df.at[3,'abstract'] $ abstract
urban_summary_table = pd.DataFrame({"Average Fare": urban_avg_fare, $                                    "Total Rides": urban_ride_total}) $ urban_summary_table.head()
season_groups.first() # first row of each group 
week28 = week27.rename(columns={196:'196'}) $ stocks = stocks.rename(columns={'Week 27':'Week 28','189':'196'}) $ week28 = pd.merge(stocks,week28,on=['196','Tickers']) $ week28.drop_duplicates(subset='Link',inplace=True)
np.dot(returns, weights)
quadratic = [[x ** 2, x, 1] for x in np.arange(0, len(y))] 
details.head()
for dataset in combine: $     dataset['Age*Class'] = dataset.Age * dataset.Pclass $ train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)
number_of_rows = df['user_id'].count() $ print('number of rows in the dataset is {} '.format(number_of_rows))
df.head()
Test.shape
len(df.query('(group == "treatment" & landing_page == "old_page") or (group == "control" & landing_page == "new_page")'))
dump["country"].value_counts() / dump["country"].value_counts().sum() $
USvideos[['category_name', 'views']].groupby('category_name').describe()
import pandas as pd $ data_cols = json_data["dataset_data"]["column_names"] $ data_rows = json_data["dataset_data"]["data"] $ df = pd.DataFrame(data_rows, columns=data_cols) $ df.head()
print 'Total new data = {}'.format(len(data_archie))
regressor2 = sm.Logit(df_new['converted'], df_new[['country_US', 'country_UK', 'intercept']]) $ result2 = regressor2.fit()
from sklearn.feature_extraction.text import CountVectorizer
weather_mean.loc['HALIFAX':'Ottawa', ['Pressure (kPa)', 'Temp (deg C)']]
urls_to_shorten = [link for link in urls if ux.is_short(link)] $ urls_to_shorten
taxi_hourly_df.head()
lsi_tf = models.LsiModel.load(os.path.join(outputs, 'model_tf.lsi')) $ corpus_lsi_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tf.mm')) $ lsi_tfidf = models.LsiModel.load(os.path.join(outputs, 'model_tfidf.lsi')) $ corpus_lsi_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tfidf.mm'))
row_sums = np.sum(desparse, axis=1) $ normed = desparse/row_sums[:,None] $ dtm_df = pd.DataFrame(columns=word_list, data=normed) $ dtm_df
releases = pd.read_csv('../input/jail_releases.csv') $ bookings = pd.read_csv('../input/jail_bookings.csv')
pred = predict_class(np.array(theta), X_test_1) $ print ('Test Accuracy: %f' % ((y_test[(pred == y_test)].size / float(y_test.size)) * 100.0))
bigdf['comment_body'] = bigdf['comment_body'].replace('/r/r',' ')
df.groupby('Agency').count().sort(desc("count")).show(10)
len([doc['id'] for doc in resp.json()])
df.loc[df['offerType'] != 'Angebot','offerType'].values
embedding_matrix2 = np.zeros((vocab_size, 300)) $ for word, i in t.word_index.items(): $     embedding_vector = embeddings_index2.get(word) $     if embedding_vector is not None: $         embedding_matrix2[i] = embedding_vector
datatest.loc[datatest.rooms>16,'rooms'] = np.NaN
stats = pd.DataFrame([i[1:] for i in summary.tables[1].data[1:]], $                      columns=summary.tables[1].data[0][1:], $                      index=[i[0] for i in summary.tables[1].data[1:]]) $ stats = stats[stats.columns].astype(float) $ stats
new_page_converted = np.random.binomial(1,p_new,n_new) $ new_page_converted.mean()
weather_yvr = pd.read_csv('data/weather_yvr.csv') $ weather_yvr['Relative Humidity (fraction)'] = weather_yvr['Relative Humidity (%)'] / 100 $ weather_yvr['Temperature (F)'] = 1.8 * weather_yvr['Temperature (C)'] + 32
p_diffs=[] $ for _ in range(10000): $     new_page_converted=np.random.binomial(n_new,p_new) $     old_page_converted=np.random.binomial(n_old,p_old) $     p_diffs.append((new_page_converted/n_new)-(old_page_converted/n_old))
Results_kNN100 = Results_kNN100[['ID', 'Approved']] $ Results_kNN100.head()
old_page_converted = np.random.choice(2, n_new, p=[1-p_new,p_new]) $ old_page_converted
merge_event = df_event.merge(df_user, on="user_id", how = 'inner') $ merge_event = merge_event.loc[(merge_event.event_name == "login")]
data_compare['SA_google_translate'].mean()
plt.scatter(tbl['mkt_ret'],tbl['port_ret']) $ plt.plot(tbl['mkt_ret'], result2.fittedvalues,'g')
df_user.rename(columns={"id_str":"id_user_str"}, inplace=True)
autos.loc[(autos.price >100000) | (autos.price < 1000),'price'] = np.nan
grouped_dpt.last() # last row of each group 
glm_multi_v2.confusion_matrix(valid)
vecs = pd.concat([dfv, df['subreddit']], axis=1)
zero_rev_acc_opps.sort_values(by='Total Buildings', ascending=False, inplace=True)
df[df.pct_chg_opencls < -0.98]
en_df = large_post_df.loc[large_post_df.language == 'en', :] $ print('Found {} English posts'.format(len(en_df)))
londonDFSubset.head()
all_data.shape
cust.iloc[:8, 6:]
thisDir = os.getcwd() $ csvDir = thisDir + '/../dbases/'
print("Jaccard  Similarity Score: ", metrics.accuracy_score(y_test, yhat)) $ print("F1-score Accuracy: ",  metrics.f1_score(y_test, yhat, average='weighted')) 
preproc_reviews = pipeline.fit_transform(review_body) $ pipe_cv = pipeline.named_steps['cv']
np.exp(ser)
california_house_dataframe.head()
run txt2pdf.py -o '2018-07-09-2015-872 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS Without MCC.pdf'  '2018-07-09-2015-872 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS Without MCC.txt'
df2 = df[df['date']>'2017-07-15  22:00:00'] $ df2 = df2[df2['date']<'2017-07-16  00:00:00'] $ positive = df2[df2['sentiment']=='positive']['date'].count() $ negative = df2[df2['sentiment']=='negative']['date'].count() $ round(positive*100/(positive+negative)) $
print(len(df_clean)) $ df_clean = df_clean.dropna(subset=['expanded_urls'])
pd.Series({2:'a', 1:'b', 3:'c'}, index=[3, 2])
twitter_archive_enhanced[ $     (twitter_archive_enhanced['doggo'] == 'None') & $     (twitter_archive_enhanced['floofer'] == 'None') & $     (twitter_archive_enhanced['pupper'] == 'None') & $     (twitter_archive_enhanced['puppo'] == 'None')].text.head()
print("The probability of converted in control group:", df2.query('group == "control"')['converted'].mean())
joined_samp.head(2)
props.prop_name.value_counts().reset_index()
check_wait_corr().groupby('wait').var()
url_BAL = "https://ravens.seasonticketrights.com/Images/Teams/BaltimoreRavens/SalesData/Baltimore-Ravens-Sales-Data-PSLs.xls"
n=13 $ df['Close'].pct_change(n).rolling(21).mean()
regr = RidgeCV(cv=5, alphas=[0.0001, 0.001, 0.01, 0.1, 1]) $ regr.fit(experiment_X, experiment_y)
series = pd.Series(list) $ series
tfidf.fit(text)
zip_2_df.head()
rGraphData.head(20)
plt.hist(taxiData.Trip_distance, bins = 20, range = [50,150]) $ plt.xlabel('Traveled Trip Distance') $ plt.ylabel('Counts of occurrences') $ plt.title('Histogram of Trip_distance') $ plt.grid(True)
df_new.drop('CA', axis=1, inplace=True) $ df_new.head()
lq.columns = lq.columns.str.replace('(','') $ lq.columns = lq.columns.str.replace(')','') $ lq.columns.values
np.mean(investors_df.investment_count)
df['timestamp'].dt
findM = re.compile(r'wom[ae]n', re.IGNORECASE) $ for i in range(0, len(postsDF)): $ 	print(findM.findall(postsDF.iloc[i,0]))
import statsmodels.api as sm $ convert_old = df[(df.landing_page == 'old_page') & (df.converted == 1)].count()[0] $ convert_new = df[(df.landing_page == 'new_page') & (df.converted == 1)].count()[0] $ n_old = df[df.landing_page == 'old_page'].count()[0] $ n_new = df[df.landing_page == 'new_page'].count()[0]
Data.groupby('to_account')['deposited', 'withdrawn'].sum().head()
modeling2 = modeling1.join(f_lr_hash_modeling2.select('id'), on='id', how='inner') $ print("modeling2 size: ", modeling2.count())
training.StateHoliday.replace(to_replace='0',value=0,inplace=True) $ testing.StateHoliday.replace(to_replace='0',value=0,inplace=True)
n_new= df2['landing_page'].value_counts()["new_page"] $ n_new
df_actuals = io.cache.prince.gas.demand.query_segment_demand(start_date, end_date, region='WA') $ actuals = df_actuals[df_actuals.Customer_Type=='MM'].groupby('Gas_Date').Daily.sum().reset_index()
twitter_Archive.head()
input_data.head()
merged_df_prec = pd.concat([merged_df, prec_group], axis=1)
festivals['Date'] = pd.to_datetime(festivals['Date'], dayfirst=True, errors='corerce')
breaches = pd.read_csv('breaches.csv', parse_dates=['BreachDate', 'AddedDate', 'ModifiedDate']) $ breaches.head()
test_orders_prodfill.head()
predictions.select("label", "prediction", "probability").show()
df_clean = df_clean.dropna()
desc_stats.transpose()
texts = df[df['section_text'].str.contains('fees')]['section_text'].values[0:5]
analyze_set[analyze_set['tweet_id']==744234799360020481].jpg_url
train['diff_lng'] = train['diff_lng'].map(lambda x: abs(x)) $ train['diff_lat'] = train['diff_lat'].map(lambda x: abs(x)) $ test['diff_lng'] = test['diff_lng'].map(lambda x: abs(x)) $ test['diff_lat'] = test['diff_lat'].map(lambda x: abs(x))
df2 = df.query("(group == 'control' and landing_page == 'old_page') or (group == 'treatment' and landing_page == 'new_page')")
tsvData=autoData.map(lambda x : x.replace(",","\t")) $ print (autoData.take(5)) $ print ("\n") $ print (tsvData.take(5)) $
data['Gender'].value_counts()
md = ColumnarModelData.from_data_frame(PATH, val_idx, df, yl.astype(np.float32), cat_flds=cat_vars, bs=128) $
merkmale.xs(96439,level='id')
df.head(2)
suspects_with_25_2['in_cp'] = suspects_with_25_2.apply(in_checkpoint, axis=1)
closePrice.sort_values()[-1] $ closePrice.sort_values(ascending=False)[0]
import matplotlib.pyplot as plt $ import matplotlib.dates as mdates $ from matplotlib.ticker import ScalarFormatter, FormatStrFormatter $ monthly = df.set_index('Created') $ monthly.info()
model = sm.Logit(df_new['converted'], df_new[['intercept', 'morning', 'night']]) $ sol = model.fit() $ sol.summary()
df2_curr=pd.get_dummies(df['TRANSACTION_CURRENCY'], prefix_sep='currency') $ df2_curr.head() $ df2=df2.drop(['TRANSACTION_CURRENCY','VOL'], axis=1)
overallBedroomAbvGr = pd.get_dummies(dfFull.BedroomAbvGr)
ser.loc[:1]
df.describe()
conditions = ((autos.registration_year >2016) $               |(autos.registration_year <1920)) $ print(autos.loc[conditions,'registration_year'].value_counts() $                                                .sort_index())
last_date = session.query(Measurements.date).order_by(Measurements.date.desc()).first() $ print(last_date)
import lightgbm as lgb $ mdl =lgb.LGBMClassifier(boosting_type ='gbdt',objective ='binary', num_leaves =30) $ mdl.fit(X_train,y_train)
ttarc[ttarc.text.str.contains('(\d+(\.\d+))\/(\d+)')] $
df_subset['Total Est. Fee'].describe()
df_A.loc['s1':'s2']
start = dt.strptime("2018-01-01", "%Y-%m-%d") $ day_list = [start + relativedelta(days=x) for x in range(0,365)] $ future_day = pd.DataFrame(index=day_list, columns= df_day.columns) $ df_day = pd.concat([df_day, future_day])
duration_df.info()
temp_series_paris_naive.tz_localize("Europe/Paris", ambiguous="infer")
giss_temp = pd.read_table("data/temperatures/GLB.Ts+dSST.txt", sep="\s+", skiprows=7, $                           skip_footer=11, engine="python") $ giss_temp
df2 = pd.read_csv('2002.csv')
aux = image_clean.merge(dog_rates.loc[dog_rates.cuteness == 'doggo'], $                                                            how='inner', on='tweet_id') $ aux[['jpg_url','grade']].sort_values(['grade'], ascending=False).iloc[:,0].values[:3]
len(df2.loc[new_page]) / len(df2)
results_tobs = session.query(Measurement.date, Measurement.tobs).filter(Measurement.date >='2016-09-01').\ $ filter(Measurement.tobs != None).\ $ filter(Measurement.station == 'USC00519281').\ $ order_by(Measurement.date.desc()).all() $ results_tobs
print(questions.shape) $ print(questions.dtypes) $ print(questions.head(5))
TC_adds = "TC-adds.txt" $ TC_deletes = "TC-deletes.txt"
ncTest.ncattrs()
reddit.head()
df.batter = df.batter.astype(int) $ df = df.merge(names, left_on='batter', right_on = 'key_mlbam', suffixes=('_old', ''))
merkmale.columns
dsDir = Path('data') $ bus = pd.read_csv(dsDir/'businesses.csv',encoding='ISO-8859-1') $ ins = pd.read_csv(dsDir/'inspections.csv') $ vio = pd.read_csv(dsDir/'violations.csv')
months = months.dropna(subset = ['birth year']) $ print(months.shape) $ print(months.head(5))
lq.head(1)
cat_col_index = loan_stats.columns_by_type(coltype='categorical') $ loan_stats[cat_col_index].head(rows=1)
n_old = df2.query('landing_page=="old_page"').shape[0] $ n_old
date_agg=pd.DataFrame(merged['date'].value_counts()) $ date_agg.columns=['frequency'] $ date_agg['date']=date_agg.index $ pd.DataFrame.head(date_agg)
records3.loc[(records3['Graduated'] == 'Yes') & (records3['GPA'].isnull()), 'GPA'] = grad_GPA_mean $ records3.loc[(records3['Graduated'] == 'No') & (records3['GPA'].isnull()), 'GPA'] = non_grad_GPA_mean
df2.describe()
df_concat_2.boxplot(column="message_likes_rel", by="page")
from nltk.tokenize import sent_tokenize $ sentence = sent_tokenize(text) $ print(sentence)
outcome['OutcomeDate'] = pd.to_datetime(outcome['DateTime'],format ='%m/%d/%Y %I:%M:%S %p' ) $ outcome['Date of Birth'] = pd.to_datetime(outcome['Date of Birth'],format ='%m/%d/%Y' ) $ outcome = outcome.drop(['DateTime','MonthYear','Name','Breed','Color'],axis=1)
old_page_converted = np.random.choice(2, n_old, p= [1-p_old,p_old]) $ old_page_converted
with open("Valid_Politician_and_Events","wb") as f: $     pickle.dump(ValidNameEvents,f)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ results = log.fit() $ results.summary()
Lab7_Redesign.to_csv('Lab7_AllTicker.csv',index=False)
dateset.reset_index(inplace=True)
impressions.info()
temp_company_numbers = non_rle_pscs.company_number $ most_common_addresses_for_non_rle_pscs = active_companies[active_companies.CompanyNumber.isin(temp_company_numbers)].groupby(['first_and_postcode'])['CompanyNumber']\ $         .agg(lambda x: len(x.unique())).sort_values(ascending=False) $ most_common_addresses_for_non_rle_pscs.head(20)
trans.head()
srp_url = 'https://pythonprogramming.net/parsememcparseface/' $ src = urllib.request.urlopen(srp_url).read() $ soup = bs.BeautifulSoup(src,'lxml') $ for para in soup.find_all('p'): $     a.append(para.get_text()) $
mit.groupby(mit['date'].map(lambda x: x.year)).num_commits.sum()
cust_data1.dtypes $ cust_data1['No_of_30_59_DPD'] = cust_data1['No_of_30_59_DPD'].astype('str')
archive_clean['stage'].replace('', np.nan, inplace=True)
header = flowerData.first() $ flowerKV= flowerData.filter(lambda line: line != header) $ print (flowerKV.collect())
rsv = df_temp_diff_redL $ rsv = rsv.append(df_temp_diff_redM) $ rsv = rsv.append(df_temp_diff_redR) $ rsv.sort_index(inplace=True) $ rsv.sample(5)
df2=df2.join(df_c.set_index('user_id'),on='user_id') $ df2.head()
df.head(1)
%bash $ model_dir=$(ls ${PWD}/taxi_trained/export/exporter) $ gcloud ml-engine local predict \ $   --model-dir=${PWD}/taxi_trained/export/exporter/${model_dir} \ $   --json-instances=/tmp/test.json
autos["price"].describe()
!head -5 /home/ubuntu/data/restaurant.csv
sim_ET_Combine = pd.concat([simResist_rootDistExp_1, simResist_rootDistExp_0_5, simResist_rootDistExp_0_25], axis=1) $ sim_ET_Combine.columns = ['simResist(Root Exp = 1.0)', 'simResist(Root Exp = 0.5)', 'simResist(Root Exp = 0.25)']
top_supporters['contributor_fullname'] = top_supporters.contributor_firstname + " " + top_supporters.contributor_lastname $
df.plot();
%time aod550 = collocate(emep.EXT_550nm, ec550) $ aod550
source_indices_L23fs_L23exc = np.where(np.array(conn_L23exc_L23fs.i)==29) $ target_indices_L23fs_L23exc = np.array(conn_L23fs_L23exc.j)[source_indices_L23fs_L23exc]
df2 = df2.drop(2893) $ df2.info()
talks.text[0]
com_grp['Age'].sum()
data_donald_replies = pd.read_csv("compared_sentiments.csv", index_col=0, dtype={'reply_id':str}) $ data_donald_replies.head()
X = np.hstack((np.ones_like(x), x)) $ print(X)
1 + np.nan
bnb.fit(X_train, y_train) $ y_pred = bnb.predict(X_train) $ print('Training set score:', bnb.score(X_train, y_train)) $ print('\nTest set score:', bnb.score(X_test, y_test)) $ print()
databreach_2017.head()
merged1['Specialty'].isnull().sum(), merged1['Specialty'].notnull().sum()
countries = pd.get_dummies(df_new['country']) $ df_new = df_new.join(countries) $ df_new.head()
body = pd.get_dummies(auto_new.Body_Type) $ body.head()
decision_tree = DecisionTreeClassifier() $ decision_tree.fit(X_train, Y_train) $ Y_pred = decision_tree.predict(X_test) $ acc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2) $ acc_decision_tree
df.groupby('userid').sum()
dfX_hist.head()
Z = np.dot(np.ones((5,3)), np.ones((3,2))) $ print(Z) $
(p_diffs > actual_converted_diff).mean() $
pd.Series(bnb.first_affiliate_tracked).fillna('untracked', inplace=True)
active_sample_sizes = non_blocking_df_save_or_load_csv( $     active_raw_sample_sizes, $     "{0}/active_sample_sizes_13".format(fs_prefix))
s.loc[s > 100].head()
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=7000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
leadConvDataGrouped = segmentData.groupby(['lead_source', 'opportunity_conversion']).opportunity_conversion.count() $ leadConvpct = leadConvDataGrouped.groupby(level=[0]).apply(lambda x: 100* x / float(x.sum())); 
with open('data/model2.pkl', 'wb') as f: $     pickle.dump(model2, f)
old = len(df2.query("landing_page == 'old_page'")) $ old
pd.Period('2012-1-1 19:00', freq='5H')
import pandas as pd $ tweets=pd.read_csv('tweets.csv')
autos['price'].value_counts()
corpusDF.head(2)
from ast import literal_eval $ df = pd.read_csv('https://raw.githubusercontent.com/satkuma/cs109B_public/master/tmdb60years.csv') $ df = df.drop('Unnamed: 0', 1) $ df['genre_ids'] = df['genre_ids'].apply(literal_eval) $ df.tail()
soup.find_all('p')
data = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key="+API_KEY)
dr.drop('Specialty', axis=1, inplace=True) $ RNPA.drop('Specialty', axis=1, inplace=True) $ ther.drop('Specialty', axis=1, inplace=True)
afl_data = afl_data.append(next_week_df).reset_index(drop=True) $ match_results = afl_data_cleaning_v2.get_cleaned_match_results().append(next_week_df) $ odds = (afl_data_cleaning_v2.get_cleaned_odds().pipe(lambda df: df.append(next_week_df[df.columns])) $        .reset_index(drop=True))
tweets = extractor.user_timeline(screen_name="DARRENHARDY", count=200) $ print("number of tweets extracted: {}.\n".format(len(tweets)))
act_dif = new_page_converted.mean() - old_page_converted.mean() $ act_dif
temp_df2['titles'] = temp_df2['titles'].astype(str)
100.0*(1-df_outcomes).sum()/len(df_outcomes)
print perf.display()
trunc_df.description = trunc_df.description.astype(str)
groupby_example.groupby('key').sum()
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2016-09-28', '2016-10-05'))
autos["price"].describe()
sum_df = full_clean_df.groupby('type').sum()['retweet_count'] $ count_df = full_clean_df.groupby('type').count()['tweet_id'] $ retweet_per_doggy = sum_df/count_df
height.isnull()
segments.dtypes
cityID = '55b4f9e5c516e0b6' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Orlando.append(tweet) 
treatment_converted = df2[df2['group'] == 'treatment']['converted'].mean() $ treatment_converted
print(full_df.columns)
df_arch_clean['source'] = df_arch_clean['source'].astype('category')
x=df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].index.values
transactions = sql_context.read.json('data/transactions.ndjson')
grouped_by_plot_and_sex = merged_data.groupby(['plot_id','sex','taxa'])['record_id'].count() $ grouped_by_plot_and_sex
trump_tfidf.shape, trump_cleaned_tfidf.shape, trump_stemmed_tfidf.shape
df_users_2=pd.merge(df_users_1,pt_download[['uid','no_of_downloads','downloaded_or_not']],left_on='uid',right_on='uid',how='left') $ df_users_2.loc[df_users_2['downloaded_or_not'].isnull(),'downloaded_or_not']='Not downloaded'
news_sentiment_df = pd.DataFrame.from_dict(news_sentiment)
final_grades_clean = final_grades.dropna(how="all") $ final_grades_clean
total_df.drop_duplicates(subset = 'Res_id', keep = 'first', inplace = True)
hmeq = cassession.CASTable('hmeq') $ hmeq.head() $
import ffn $ df_portfolio_value = data['cumreturns'] $ perf = df_portfolio_value.calc_stats() $ perf.plot()
copy.head()
metrics.f1_score(y_valid, y_pred)
def calculate_dividend_weights(ex_dividend): $     return None $ project_tests.test_calculate_dividend_weights(calculate_dividend_weights)
mvrs = ratings.groupby('movieId').size().sort_values(ascending=False) $ tmp_ratings = ratings.ix[mvrs[mvrs > rating_count].index].dropna()
text_dataset = grouped.join(users_target, how='inner') $ X = text_dataset['full_text'] $ y = text_dataset['personality_type'].apply(lambda x: pd.Series(list(x.replace('-', '')), $                                                                index=['I/E', 'N/S', 'F/T', 'P/J', 'A/T'])).replace(replace)
ddf[20].sort_values(ascending=False)
def useDiffColNamesToFillInNA(dataframeOfWells,colReplaceList): $     for each in colReplaceList: $         print("each",each) $         dataframeOfWells[each[0]].fillna(dataframeOfWells[each[1]], inplace=True) $     return dataframeOfWells
print(speeches_cleaned.shape) $ print(len(speeches_cleaned['text'].unique())) $ unique_speeches = speeches_cleaned['text'].unique()
print(city_names.index) $ print(cities.index)
df['LOT'] = pd.to_numeric(df['LOT'], errors='coerce')
autos = autos[autos['registration_year'].between(1900, 2016)] $ autos['registration_year'].min()
resampled1_groups.boxplot(by = 'Field', column='Sales_in_CAD', figsize=(25,10)) $ plt.ylim(0, 40000) $ plt.title('Sales to Customers according to Field (Years: 2010 - 2015)') $ plt.xlabel('Field of Customer') $ plt.ylabel('Sales in CAD') $
df_new.country.unique()
print "Training set:\n{}".format(df_train.priority.value_counts())
dfBTCPrice.corr()
lr = LogisticRegression() $ lr.fit(X_train,y_train)
pres_df['subjects'].describe()
! ls  | grep --fixed-strings --file samples_with_signatures.txt -v | grep log | xargs tail -n 20 | head -n 50
dfPlot = pd.DataFrame() $ dfPlot['ScoreLR', 'scoresRF']=0 $ scoresLR=[] $ scoresRF=[]
parsed_locations[parsed_locations.country.isnull()]
df2.loc['2016-01-01':'2016-12-31', ['GrossOut', 'GrossIn']].plot()
conn.caslibinfo()
logit_mod = sm.Logit(df3['converted'], df3[['intercept','ab_page', 'CA', 'UK']]) $ results = logit_mod.fit() $ results.summary()
df_new[['CA','UK', 'US']] = pd.get_dummies(df_new['country'])
discrp['started_on'] = pd.to_datetime(discrp['started_on'], format= '%Y-%m-%d %H:%M:%S') $ discrp['completed_on'] = pd.to_datetime(discrp['completed_on'], format= '%Y-%m-%d %H:%M:%S')
daily_ret = calc_daily_ret(closes) $ daily_ret.plot(figsize=(8,6));
features.iloc[:, 4:].head()
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x, np.dot(x3, ridge3.coef_) + ridge3.intercept_, color ='orchid', linewidth = '5') $ plt.plot(x, np.dot(x3, model3.coef_) + model3.intercept_, color = 'dimgrey', linewidth = '3.5')
git_log = git_log.sort_index() $ git_log[git_log['author'] == 'Linus Torvalds'].head(10)
prop.head()
lab2.info()
metadata['bad_band_window1'] = refl.attrs['Band_Window_1_Nanometers'] $ metadata['bad_band_window1']
convert_mean = df2['converted'].mean() $ convert_mean
YS1517.Close.plot(label='Yahoo') $ moving_average.plot(label='Moving average') $ plt.legend() $ plt.gcf().set_size_inches(15,5)
y_test_hat = model.predict(X_test) $ y_test_hat = squeeze(y_test_hat) $ test_stats = CheckAccuracy(y_test, y_test_hat)
def historical_data(ticker, outsize = "full"): $     alphavantage_link = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={0}&apikey=NXY0VT9AHBRYGKKC&datatype=csv&outputsize={1}'.format(ticker, outsize) $     df = pd.read_csv(alphavantage_link) $     return df $
data_df.describe()
artists_info = [sp.artist(artist_id[i]) for i in range(0,num_songs)]
ab_df.query('(group == "control" and landing_page == "new_page") or (group == "treatment" and landing_page == "old_page")').count() 
highmeans.sort_values(ascending = False)
plt.scatter(cc['high'],cc['spread']) $ plt.title("High vs Spread Price") $ plt.xlabel("High Value") #Change x and y axis scale $ plt.ylabel("Spread") $ plt.show()
importances = {} $ for team in teamtables.keys(): $     x_filtered = pd.concat([X[(X["Team1"]==team)],X[X["Team2"]==team]]) $     y_filtered = pd.concat([Y[(X["Team1"]==team)],Y[X["Team2"]==team]]) $     importances[team] = predictions(x_filtered,y_filtered,X_test,True)[2] $
y_test_log_pred = XGB_model.predict_proba(X_test)
bedrooms = train_df['bedrooms'].value_counts() $ x = bedrooms.index $ y = bedrooms.values $ sns.barplot(x, y )
firstWeekUserMerged.isnull().sum()
df_lm.filter(regex='q_lvl_0|last_month|q_lvl_0_c').boxplot(by='last_month', figsize=(10,10),showfliers=False)
n_old=df2[df2['landing_page']=='old_page']['user_id'].count() $ n_old $
df.to_pickle("dfSentences.p")
import pandas as pd $ entries = [el for el in vcf_reader] $ print(pd.DataFrame(entries).head().iloc[:,:7])
Addition = df.loc['Total']['AveragePrice'] + df.iloc[18248]['AveragePrice'] $ print(Addition)
plt.figure(figsize=(15, 7)) $ df.public_repos.hist(log=True, bins=80);
house_data['bedrooms'].unique()
df['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['CA', 'US']]) $ results = log_mod.fit() $ results.summary()
f=open(path_database+'tf_idf_lesk_table_20.csv') $ all_feature=f.readline().replace("'",'').split(',')[1:] $ print (all_feature) $ print (len(all_feature))
from sklearn.feature_extraction.text import CountVectorizer $ count_vect = CountVectorizer() $ X_train_counts = count_vect.fit_transform(tweet_train_data) $ X_train_counts.shape
remove_index = treat_oldp.append(ctrl_newp).index $ remove_index.shape
tweets_predictions_all = pd.merge(tweets_prediction, popularity_clean, how='left', on='tweet_id')
for ids in faulty_rating_id: $     txt = archive_clean[archive_clean.tweet_id == ids].text $     rating_list = txt.str.findall(r"\d+\.?\d*\/\d+\.?\d*\D+(\d+\.?\d*)\/\d+\.?\d*") $     archive_clean.loc[archive_clean.tweet_id == ids,'rating_numerator'] = rating_list.values[0] $     archive_clean.loc[archive_clean.tweet_id == ids,'rating_denominator'] = 10  
ice_df.head(10)
words_sum = preproc_reviews.sum(axis=0) $ counts_per_word = list(zip(pipe_cv.get_feature_names(), words_sum.A1)) $ sorted(counts_per_word, key=lambda t: t[1], reverse=True)[:20]
from IPython.display import Image $ Image("/Users/jamespearce/repos/dl/data/dogscats/train/cat.2150.jpg")
np.exp(result4.params)
bnb[bnb['age']>1000].plot(kind='hist', y='age', bins=20)
%matplotlib inline $ fig= date_df.groupby(date_df.created_time.dt.day).count().plot(kind="bar",title="posts par jour")
print(train_data.fuelType.isnull().sum()) $ print(test_data.fuelType.isnull().sum())
gi = convert_game_value(testset.filelist[ind],testset.feature_list,pgn2value)
k = p.join(train.reset_index(drop=True).project_is_approved)
sqlContext.sql("select * from RandomTwo").toPandas()
df_questionable_3[df_questionable_3['state_CA'] == 1]['link.domain_resolved'].value_counts()
tweet1.author
trump_tfidf = vect_tfidf.fit_transform(df_train) $ trump_cleaned_tfidf = vect_cleaned_tfidf.fit_transform(df_cleaned_train) $ trump_stemmed_tfidf = vect_stemmed_tfidf.fit_transform(df_stemmed_train)
B = pd.DataFrame(np.random.randint(0, 10, (3, 3)), $                  columns=list('BAC')) $ B
area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297, $              'Florida': 170312, 'Illinois': 149995} $ area = pd.Series(area_dict) $ area
df2[['country_ca', 'country_uk', 'country_us']] = pd.get_dummies(df2['country']) $ df2 = df2.drop(['country_ca'], axis=1) $ df2.head()
loan_requests_indebtedness_web=loan_requests_indebtedness.merge(df2.reset_index(),on='id_loan_request')
import pandas as pd $ distance_from_sun = [149.6, 1433.5, 227.9, 108.2, 778.6] $ planets = ['Earth','Saturn', 'Mars','Venus', 'Jupiter'] $ dist_planets = pd.Series(data = distance_from_sun, index = planets) $ time_light = dist_planets / 18 
df2 = df_ab.query('(group == "treatment" & landing_page == "new_page")|(group == "control" & landing_page == "old_page")') $
df_goog['Closed_Higher'] = df_goog.Close > df_goog.Open
nb2 = MultinomialNB() $ %time nb2 = nb2.fit(train_4_reduced, y_train) $ %time y_hat_nb2 = nb2.predict(train_4_reduced) $ print(roc_auc_score(y_train, y_hat_nb2)) $ print(log_loss(y_train, y_hat_nb2))
grouped_by_origin = df3.groupby('Origin') $ type(grouped_by_origin)
df_merge.groupby(['school_name', 'grade']).math_score.mean().unstack()
n_new = df2.query('landing_page == "new_page"') $ len(n_new)
top5_days.plot(kind='barh')
bool_regyear = autos["registration_year"].between(1900, 2016) $ autos = autos[bool_regyear]
data.head()
fwd.head()
df.reset_index(drop=False, inplace=True) $ zipcodes.reset_index(drop=False, inplace=True)
users.created_at = pd.to_datetime(users.created_at)
dat_wkd.vgplot.line(value_name='Hospital mortality rate')
date_info.head()
with open(datapath / 'simple.yaml', 'r') as stream: $     try: $         print(yaml.load(stream)) $     except yaml.YAMLError as exc: $         print(exc)
df2.query('group=="treatment"').converted.mean()
userByCountry_dfMax =userByCountry_df.loc[userByCountry_df.groupby(["channel_title"])["views"].idxmax()] $ userByCountry_dfMax
print('Unique number of users notified: {}'.format(len(atdist_opp_dist[~atdist_opp_dist['infoIncluded']]['vendorId'].unique())))
df_all_wells_basic.head()
df2_copy.head()
summed.fillna(method='pad', limit=1)  # No more than one padded NaN in a row
sigma_est = sim_closes_hist.iloc[-1].std() $ (call_hist.iloc[-1].Prima-4.5*sigma_est*np.exp(-r*ndays)/np.sqrt(nscen),call_hist.iloc[-1].Prima+4.5*sigma_est*np.exp(-r*ndays)/np.sqrt(nscen))
frame.loc[['a', 'b', 'c', 'd'], ['Texas', 'Utah', 'California']]
linkNYC['days'] = (datetime.datetime.now().date()-linkNYC.days).apply(lambda x: x.days)
soup.name
pickling_on = open("df1.pickle","wb") $ pickle.dump(df, pickling_on) $ pickling_on.close()
evaluator.get_metrics('macro_avg_accuracy')
data.columns
doglist = dogs.dropna(how='all') $ doglist
high = max(v.Open for v in data.values() ) $ low = min(v.Open for v in data.values()) $ print('=>The high and low opening prices for this stock in 2017 were {:.2f} and {:.2f} '.format(high, low) + 'respectively.')
print 'See correlation with actual: ',test_case.select('address1').take(1) $ actual_acct_id.select('address1').distinct().show(10,False)
fires.head() # For some reason, we get no output from this command
df['rating'] = df.rating_numerator/df.rating_denominator $ df['rating_category'] = pd.cut(df.rating, bins = [0.0, np.percentile(df.rating,25), np.percentile(df.rating,50), np.percentile(df.rating,75), np.max(df.rating)],labels=['Low','Below_average','Above_average','High']) $ df.drop(['rating_numerator','rating_denominator'], axis=1, inplace=True)
df2.head()
gbm_regressor = H2OGradientBoostingEstimator(distribution="gaussian",ntrees=10, max_depth=3, min_rows=2, learn_rate=0.2)
df_trends.plot(kind='bar', x='name',y='tweet_volume',figsize=(100,100), $                                                            fontsize=100)
autos['odometer_km'].value_counts().sort_index()
words_only_scrape = [term for term in words_scrape if not term.startswith('#') and not term.startswith('@')] $ print('The number of words only (no hashtags, no mentions): ', len(words_only_scrape))
order_data.head()
import numpy as np $ import matplotlib.pyplot as plt $ country_comp = free_data.groupby('country')['y'] $ print('Most people in',country_comp.sum().idxmin(), 'feel they are more free than repressed.')
df_combined = pd.merge(df, df_btc, on='created_at', how='inner')
twitter_archive_master.stage.value_counts().plot(kind='bar');
_ = ok.grade('q08c') $ _ = ok.backup()
winpct.loc[winpct['text'].apply(lambda x: any(re.findall('Santos',x)))][['playId','homeWinPercentage','playtext','date']]
y_pred = tuned_forest_model.predict(X_test) $ confusion_matrix(y_test,y_pred)
n_new=df2.query('landing_page == "new_page"').shape[0] $ n_new
transformed_df = data.transform_data(raw_df) $ transformed_sample_df = transformed_df.sample(4, random_state=42) $ transformed_sample_df[['Name', 'Rating'] + data.simple_features].T
working_data = [df_train, df_test] $ working_data = pd.concat(working_data) $ working_data = working_data.reset_index() $ working_data['date'] = pd.to_datetime(working_data['date']) $ working_data = working_data.set_index('date')
learn.save("dnn20")
df_q = pd.read_sql(query, conn, index_col='Date_ID') $ df_q.head(5)
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result=logit.fit()
df13 = pd.read_csv('2013.csv')
dfFull['TotalBsmtSFNorm'] = dfFull.TotalBsmtSF/dfFull.TotalBsmtSF.max()
logit = sm.Logit(df3['converted'], df3[['ab_page', 'intercept']]) $ result=logit.fit()
local_sea_level_stations.columns
df_u= df_vu.groupby(["landing_page","group"]).count() $ df_u $
tw_clean.sort_values('retweet_count').tail(1)
to_be_predicted_Day2 = 22.32749815 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
dfFull.TotalBsmtSF = dfFull.TotalBsmtSF.fillna(dfFull.TotalBsmtSF.mean())
df2 = df2.drop_duplicates(subset='user_id', keep='first') $ df2[df2['user_id'] == 773192] $
twitter_archive_df_clean['expanded_urls'].isnull().any()
os.chdir(out_path) $ os.getcwd()
openstates["last_name"] = openstates["last_name"].apply(substitute) $ openstates["full_name"] = openstates["full_name"].apply(substitute)
df_treat = df2.query('group=="treatment"') $ y_treat = df_treat["user_id"].count() $
new_w = np.zeros((vs, em_sz), dtype=np.float32) $ for i, w in enumerate(itos): $     r = stoi2[w] $     new_w[i] = enc_weights[r] if r >= 0 else row_m
merged1.isnull().sum()
df_numbers = pd.DataFrame({'Number':np.arange(5)}) $ df_numbers
def get_historical_price_day(coin, to_curr=CURR, timestamp=time.time(), exchange=EXCHANGE, allData='false', **kwargs): $     return get_historical_price_day(*args, **kwargs, limit=1)
tc2 = tc1[~tc1['ISBN RegEx'].isin(bad_tc_e_isbns)] $ tc2['ISBN RegEx'].size
store_items.size
dfss.price.mean()
A = np.random.randint(0,5,(8,3)) $ B = np.random.randint(0,5,(2,2)) $ C = (A[..., np.newaxis, np.newaxis] == B) $ rows = np.where(C.any((3,1)).all(1))[0] $ print(rows)
!wget -nv https://www.py4e.com/code3/mbox.txt -O mbox.txt
df = pd.read_csv("contact.csv", index_col=None) 
any_of_these = [10029,280060,280040,280077] $ idx = all_sites_with_unique_id_nums_and_names[all_sites_with_unique_id_nums_and_names['id_num'].isin(any_of_these)].index.tolist() $ all_sites_with_unique_id_nums_and_names.loc[idx]
tweets2 = pd.concat([josh_tweets, matt_tweets], axis=0) $ tweets2.shape
feature_names = list(train_features.limit(1).toPandas().columns) $ feature_names = [f for f in feature_names if f not in ['ip','os','device','app','channel', $                                                        'day', $                                                        'id','is_attributed']]
result['uid'].notnull().sum()
df2.groupby('OriginCityName')['Origin'].unique().tail(15) # number of airports code per City
nflx= web.DataReader("NFLX", 'yahoo', start, end)[['Adj Close']].pct_change()['2012-07-20':'2013-07-23']*100 $ gspc_nflx = web.DataReader("^GSPC", 'yahoo', start, end)[['Adj Close']].pct_change()['2012-07-20':'2013-07-23']*100 $ tbl4 = pd.concat([nflx,gspc_nflx], axis = 1, join='outer') $ tbl4.columns=['NFLX','GSPC'] $ tbl4.head()
sum(df_h1b_mv_ft.pw_1*df_h1b_mv_ft.total_workers)/sum(df_h1b_mv_ft.total_workers)
models = [fit_model() for i in range(11)]#was range(6) 5 iterations, now 10 iterations.
adopted_cats['Y'].value_counts()
energy_cpi.dtypes
total4=total.ix[(total['RA0']<50) & (total['RA0']>15)] $ total4.to_csv('/Users/taweewat/Dropbox/Documents/MIT/Observation/2016_1/objs_miss_fall2016_ra_gt_1.csv',index=False)
df_tweet_clean2.info()
maxTemp = beirut['Max TemperatureC'].plot(grid=True, figsize=(10,5), title="Daily Maximum Temperature in Beirut, 2015") $ maxTemp.set(xlabel="", ylabel="Temperature, C")
old_page_converted = np.random.binomial(n_old,p_old) $ old_page_converted
tips["sex"].index
resultvalue_df.plot(y='datavalue', x='valuedtoff')
mnb = MultinomialNB() $ mnb.fit(X_train, y_train) $ print('Training set score:', mnb.score(X_train, y_train)) $ print('\nTest set score:', mnb.score(X_test, y_test)) $ print('\nCross Val score:',cross_val_score(mnb, X_test, y_test, cv=5))
Nnew = df2.query('landing_page == "new_page"').user_id.count() $ Nnew
geocoded_df.loc[idx,'Case.Duration'].dt.days.plot()
df2.drop([1899],inplace=True)
autos["brand"].value_counts(normalize=True) * 100
dr_new.columns $
df6.unstack()
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_axial_brace_saddle[0][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_saddle[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
states.mean(level='location')
sum(df2.converted == 1) / 290584
saveToFile = os.path.join(PROCESSED_PATH, 'Current_Accounts_Untapped_Buildings_On_Net.csv') $ potential_accounts_buildings_info_tbrr_temp.to_csv(saveToFile, index = False)
cashflows_plan_investor_20150430.head()
g_geo = g_geo.to_crs({'init':'epsg:4269'})
september = goodWeather.ix[datetime(2014,9,1) : datetime(2014,9,30)] $ september[['Precipitationmm', 'Apparent Mean Temp C', 'Apparent Max Temp C']].plot(grid=True, figsize=(10,5))
data_archie.created_at = pd.to_datetime(data_archie['created_at']).dt.date $ data_archie.updated_at = pd.to_datetime(data_archie['updated_at']).dt.date $ data_archie.created_at.head(5)
shows = pd.read_csv("ismyshowcancelled_tmp_1.csv",index_col=0)
nar5.to_clipboard()
plt.xlabel('Votes') $ plt.ylabel('Number of Cheapest Movies') $ plt.title('Distribution of Votes') $ cheap_budget_vote.hist(label = 'Rates of the Cheapest Movies')
df = pd.DataFrame(tweet_dict) $ df
y_pred_cont_norm.sum().sum()
df_birth[df_birth.population > 1000000000] $
raw_full_df.shape
norm.ppf(1-(0.05/2))
education_data.drop(columns_to_drop, axis=1, inplace=True)
print("# of files in unsubscribed :", filecount(os.fspath(master_folder + lists + "unsubscribed/"))) $ print("# of files in members :", filecount(os.fspath(master_folder + lists + "members/"))) $ print("# of files in cleaned :", filecount(os.fspath(master_folder + lists + "cleaned/")))
percent_unique_convert=df.query('converted=="1"').user_id.nunique()/df["user_id"].nunique()*100
telecom3.shape
sns.distplot(dfz.retweet_count, color = 'blue', label = 'Retweets')
import numpy as np $ mars_facts_df.to_html('mars_table.html') $
data_vi.index
dci = indices(dcaggr, 'text', 'YearWeek') $ dci.head()
y.to_csv('target', index=False)
(~autos["registration_year"].between(1900,2016)).sum()/autos.shape[0]
df['author_lower'] = df['author'].apply(lambda x: x.lower())
fin_r_monthly.shape
us_tax_recent = quandl.get("OECD/REV_NES_TOTALTAX_TAXUSD_USA", start_date="2000-01-01") $ df_info(us_tax_recent)
price.describe()
articles['id'].unique()
vio.describe()
for n in [tup[0] for tup in france_tops]: $     print (lda.print_topic(n)) $     print ("")
df.head()
to_be_predicted_Day4 = 31.34447701 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
calls = pd.read_csv("311_Calls__2012-Present_.csv")
all_texts = [ $     normalize_links(msg['text']) $     for msg in ods.messages $     if msg['type'] == 'message' and msg.get('subtype') is None $ ]
openmoc_geometry = get_openmoc_geometry(mgxs_lib.geometry)
keep_types = [u'WWW', u'WND', u'WAS', u'SUN', 'DPV', u'NUC', u'NG', $        u'PEL', u'PC', u'OTH', u'COW', u'OOG', u'HPS', u'HYC', u'GEO'] $ keep_cols = ['generation (MWh)', 'total fuel (mmbtu)', 'elec fuel (mmbtu)', $              'all fuel CO2 (kg)', 'elec fuel CO2 (kg)'] $ eia_total_monthly = eia_total.loc[(eia_total['type'].isin(keep_types))].groupby(['type', 'year', 'month'])[keep_cols].sum()
features_std = [35, 36, 37] $ for f in features_std: $     feature_sc = StandardScaler() $     X[:,f] = feature_sc.fit_transform(X[:,f].reshape(-1, 1)).reshape(X.shape[0])
np.median(shows['first_year'].dropna())
cp = nltk.RegexpParser(grammar) $ pr_ht['chunks'] = pr_ht['pos body'].map(lambda sentences: [cp.parse(sentence) for sentence in sentences]) $ pd.options.display.max_colwidth=1000 $ print(pr_ht['body'].iloc[2]) $ print(pr_ht['chunks'].iloc[2])
classify_df.shape
surveys_df = pd.concat([surveys2001_df, surveys2002_df], axis=0, sort=False) $ surveys_df = surveys_df.reset_index(drop=True)
def next_weekday(d): $     day_of_week = d.weekday() $     if day_of_week !=0: $         d = d + datetime.timedelta(7 - day_of_week) $     return d.strftime('%Y-%m-%d')
n_old = (df2[df2['landing_page'] == 'old_page']).shape[0] $ n_old
log_mod=sm.Logit(result_df['converted'],result_df[['intercept','ab_page','UK','US']]) $ results=log_mod.fit()
bgf_test = BetaGeoFitter(penalizer_coef=0.0) $ bgf_test.fit(m_test['frequency'], m_test['recency'], m_test['T']) $ print(bgf_test)
autos.price.max()  # we have a high max probably an outlier. 
pm_data['time_stamp'] = pd.to_datetime(pm_data.time_stamp) $ pm_data.sort_values(['unit_number','time_stamp'], inplace=True) $ pm_data.reset_index(inplace=True)
from keras.utils import Sequence
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2017-01-01&end_date=2017-12-31&apikey='+API_KEY)
print type(RandomOneRDD) $ print RandomOneRDD.collect()
sub_df2=df2.query('dow=="Friday" | dow=="Monday"' )
for dim in d.dimensions: $     print('%s:\t %s'% (dim, d.dimensions[dim]))
exiftool -csv -createdate -modifydate cisuabn14/cisuabn14_cycle2.MP4 cisuabn14/cisuabn14_cycle3.MP4 cisuabn14/cisuabn14_cycle4.MP4 cisuabn14/cisuabn14_cycle5.MP4 cisuabn14/cisuabn14_cycle6_part1.MP4 cisuabn14/cisuabn14_cycle6_part2.MP4 > cisuabn14.csv
df.loc["Equifax",:]
df1.describe()
avg_per_seat_price_seasonsandteams["2013 Offseason", "BAL"] # This is the average price of PSLs after BAL won the SuperBowl.
vhd = pd.read_excel('input/Data.xlsm', sheet_name='52', usecols='A:AO', header=6, skipfooter=16)
old_page_converted = np.random.choice([1,0], size = nOld, p=[pMean,oneMinusP]) $ old_page_converted.mean()
pres_df['split_location_tmp'] = pres_df['location'].map(lambda x: x.split(',')) $ pres_df.head(2)
df = pd.concat(dfs[4:12]) $ df
train.ORIGINE_INCIDENT.unique()
complete_solar_df = pd.read_excel(weather_folder + '/compete_solar_df.xlsx') $ complete_wind_df = pd.read_excel(weather_folder + '/compete_wind_df.xlsx') $ solar_wind_df = pd.read_excel(weather_folder + '/complete_solar_wind.xlsx')
X_test_term.shape
git_blame.path.nunique()
b.visit("https://www.skyscanner.net/transport/flights/tpet/cts/180118/180130?adults=1&children=0&adultsv2=1&childrenv2=&infants=0&cabinclass=economy&rtn=1&preferdirects=false&outboundaltsenabled=false&inboundaltsenabled=false&qp_prevProvider=ins_month&qp_prevCurrency=GBP&qp_prevPrice=178&pricesourceid=b3ms-SG1-2#results")
len(train_data[train_data.fuelType == 'lpg'])
df_twitter_copy['jpg_url'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['jpg_url']) $ df_twitter_copy['p1'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['p1']) $ df_twitter_copy['p1_conf'] = df_twitter_copy['tweet_id'].map(image_predictions_copy.set_index('tweet_id')['p1_conf'])
Grouping_Year_DRG_discharges_payments_for_specific_year = \ $    Grouping_Year_DRG_discharges_payments.loc[the_year] $ Grouping_Year_DRG_discharges_payments_for_specific_year.head()
df_a = df_graph.drop(['ordered_0','ordered_1','user_id'],1)
twitter_archive_df_clean['timestamp'] = twitter_archive_df_clean['timestamp'].map(lambda x: pd.to_datetime(x).date())
plt.figure(figsize=(16,8)) $ fb['2012':].resample('W').count()['message'].plot()
oil_interpolation=daily_sales[['date','dcoilwtico']] $ pd.DataFrame.head(oil_interpolation)
usersDf.hist(column=['favourites_count'],bins=50) $
s = '' $ for row in temp1[::-1]: $     s += ''.join(element.rjust(5) for element in row) + '\n' $ print(s)
df_log.num_of_people.value_counts()
test_df.head()
w = 'kike' $ model.wv.most_similar (positive = w)
ffr.resample("MS").first().head()
breakdown[breakdown != 0].sort_values().plot( $     kind='bar', title='Russian Trolls Number of Links per Topic' $ );
tweet_archive_clean.dtypes
cc['logopen'] = np.log(cc['open']) $ plt.hist(cc['logopen']) $ plt.show()
from helper_code import mlplots as ml $ ml.confusion(l_test.reshape(l_test.shape[0]), $              y_pred, labels, 3, $              'Decision Tree Classification')
reddit_info.titles.drop_duplicates(inplace = True)
t_likes = pd.Series(data=data['Likes'].values, index=data['Date']) $ type(t_likes)
archive_copy['source'] = archive_copy['source'].str.replace('<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>', 'Twitter for iPhone') $ archive_copy['source'] = archive_copy['source'].str.replace('<a href="http://twitter.com" rel="nofollow">Twitter Web Client</a>', 'Twitter Web Client') $ archive_copy['source'] = archive_copy['source'].str.replace('<a href="https://about.twitter.com/products/tweetdeck" rel="nofollow">TweetDeck</a>', 'TweetDeck')
autos["odometer_km"].value_counts().sort_index(ascending=True)
temp_df2.sort_values(by='titles').head()
autos.head()
df_users.user_id.nunique()
agg_stats.tail(3)
sorted(content_values.iteritems(), key=lambda x: -x[1])[0:20]
len(b_cal['price'].unique())
aa2 = aa.replace('\n ',',').replace(' ',',') $ aa2
UK_control_conversion = df_c_merge[(df_c_merge['group']== 'control') & (df_c_merge['country'] == 'UK')]['converted'].mean() $ UK_control_conversion
count_non_null(geocoded_df, 'Disposition.Desc')
test_preds_df = pd.DataFrame(test_preds,index=test_target.index,columns=['kwh_pred']) $
twitter_df_clean.name.value_counts()[0:5]
df_raw['gender']=df_raw['gender'].replace({'Male': 0, 'Female': 1}) $ df_raw['InternetService']=df_raw['InternetService'].replace({'DSL': 0, 'Fiber optic': 1, 'No': 2}) $ df_raw['Contract']=df_raw['Contract'].replace({'Month-to-month': 0, 'One year': 1, 'Two year': 2}) $ df_raw['PaymentMethod']=df_raw['PaymentMethod'].replace({'Electronic check': 0, 'Mailed check': 1, 'Bank transfer (automatic)': 2, 'Credit card (automatic)':3})
tweets_df.info()
clf_RF_tf.predict_proba(X_testcv_tf)
qqqq = pd.merge(c,d , on =['msno','transaction_date']) $ qqqq
ac['Issues'].describe()
sns.pairplot(df, x_vars=['Bottles Sold','Volume Sold (Liters)'], y_vars='Sale (Dollars)', size=7, aspect=0.7, kind='reg')
data.info()
bug_count = df_bug[u'Service Location'].value_counts() $ mdid_lst = list(df_bthlst[df_bthlst["booth_id"].isin(bug_count.index.values)]["md_id"]) $ indx = ['mdb' + str(v) for v in mdid_lst] $ df_selparams = pd.DataFrame(index=indx, columns=['n_faults', 'n_amb', 'n_door', 'n_relay', 'n_mcu', 'n_temp'])
avg_per_seat_price_inoroffseason = inoroffseason["Per Seat Price"].mean() # This takes the average of each of the 2 categories. $ avg_per_seat_price_inoroffseason
df_cs.head(2)
index = pd.DatetimeIndex(['2014-03-04', '2014-08-04', $                           '2015-04-04', '2015-09-04', $                           '2016-01-01', '2016-02-16']) $ data = pd.Series([0, 1, 2, 3, 4, 5], index=index) $ data
p = getpass.getpass() $ conn = pymysql.connect(host='localhost', port=3306, user='root', passwd=p,)# db='example') $ cur = conn.cursor()
logit_mod = sm.Logit(df2['converted'], df2[['intercept','new_page']]) $ results = logit_mod.fit()
np.exp(-0.0149), np.exp(0.0506), np.exp(0.0408)
new_cust_without_discover = post_launch_emails[post_launch_emails['Time To Buy'] < pd.Timedelta('00:00:00')] $ new_cust_without_discover
pystore.list_stores()
merged['onpromotion'].value_counts(normalize=True,dropna=False)
df.doggo.value_counts()
csvData.drop('condition', axis = 1, inplace = True)
collection.delete_item('AAPL')
merged.committee_position.value_counts().reset_index()
!cp ../submissions/svc.csv ../../drive/ColabNotebooks/AV_innoplexus_html
lm = sm.OLS(df2['converted'],df2[['intercept','a/b_page']]) $ result = lm.fit()
plt.title('Gucci ngram', fontsize=18) $ gucci_ng.plot(kind='barh', figsize=(20,16)); $ plt.savefig('../visuals/gucci_ngram.jpg')
import seaborn as sns $ neg_class = pd.DataFrame(lr_balanced_predp)[0] $ pos_class = pd.DataFrame(lr_balanced_predp)[1] $ plt.hist([neg_class,pos_class], color=['r','b'], alpha=0.5)
conn.columninfo(table=dict(name='iris_sql', caslib='casuser'))
ET_Combine = pd.concat([hour_1dRichards, hour_lumpedTopmodel, hour_distributedTopmodel_average], axis=1) $ ET_Combine.columns = ["Baseflow = 1D Richards'", 'Baseflow = Topmodel(lumped)', 'Baseflow = Topmodel(distributed)']
c_sort = country_sort.merge(sale_country_sort,how='left', left_on='Product', right_on='Product') $ c_sort['SalePrice'] = c_sort['SalePrice'].fillna(0) $ c_sort=c_sort.rename(columns = {'Country_x':'Country'}) $ c_sort.drop(['Country_y'], axis = 1, inplace = True) $ c_sort
conn.commit()
print("5 recent tweets:\n") $ for tweet in tweets[:5]: $     print(tweet.text) $     print()
html_table_marsfacts = html_table_marsfacts.replace('\n', ' ') $ html_table_marsfacts
ekos.load_workspace(user_id, alias = 'lena_newdata')
users_conditions = df_experiment.copy() $ users_conditions = users_conditions[-users_conditions.user_id.isin(have_seen_two_versions)] # drop user who have seen more than one version
fixed_bonus_points.insert(0, "sep", 0) $ fixed_bonus_points
results_1.summary()
baseball = pd.read_csv("Data/baseball.csv", index_col='id') $ baseball.head()
sorted(problem_combos.map(lambda c: (c, 1)).countByKey().iteritems(), key=lambda x: x[1], reverse=True)[:20]
data.plot()
p.head(2)
containers[0].find("div",{"class":"key"}).a['title'].split()[0].replace(',',"")
from sklearn.svm import SVC $ from sklearn.utils import class_weight
totals.sort_values('total',ascending=False)
authors_count = db.get_sql(sql)
!head -n 2 evalme/predict_results_eval.csv
wb = openpyxl.load_workbook('most_excellent.xlsx') $ wb.sheetnames
with open('D:/CAPSTONE_NEW/indeed_com-jobs_us_deduped_n_merged_20170817_113502269355122.json', encoding="utf-8-sig") as data_file: $     json_data2 = j.load(data_file)
reflRaw = refl['Reflectance_Data'].value $ reflRaw
news6=('OPEC and its allies were close to an agreement to extend their oil-production cuts for another nine months as they seek to prop up prices and revive their economies.' $ 'While ministers gathering in Vienna still planned to discuss other options -- a shorter deal for six months or curbs lasting for the whole of next year -- consensus was building around an agreement that runs through March 2018.' $ 'The Organization of Petroleum Exporting Countries and 11 non-members agreed last year to cut output by as much as 1.8 million barrels a day. The supply reductions were initially intended to last six months from January, but the slower-than-expected decline in surplus fuel inventories prompted the group to consider an extension. The most influential members of the deal, including Russia and Saudi Arabia and Iraq, have publicly backed supply curbs lasting until March 2018 to finally clear the glut.' $ 'A committee of six OPEC and non-OPEC nations charged with ensuring successful implementation of the cuts will meet on Wednesday morning to study the merits of a 12-month extension, in addition to the six and nine-month durations already discussed publicly, according to delegates familiar with the matter. No participant in the agreement has publicly backed another 12 months.' $ 'If OPEC maintains its April crude production of 31.8 million barrels a day throughout the rest of the year, the decline in oil stockpiles will accelerate, according to the International Energy Agency.' $
print('Screenshot from the Mathematica file for filtering, data from 20180201_092937.csv.') $ print('For comparison with below .dropna() Pandas version.') $ from IPython.display import Image $ Image(filename = 'filteringMathematica.png')
df_transactions.head()
c.find_one({'born': {'$lt': datetime(2000, 1, 1)}, # AND $             'born': {'$gt': datetime(1900, 1, 1)}})
kickstarter = kickstarter[kickstarter["usd pledged"].notnull()] $ assert not kickstarter["usd pledged"].isnull().values.any() $ kickstarter["country"].unique()
twitter_archive_clean.loc[twitter_archive_clean['tweet_id']==810984652412424192]
df_all = pd.concat(dfs[2:], keys=selected_reporting_dates[2:])
display(sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new],alternative='larger'),sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new],alternative='smaller')) $
check=compiled_data[['lang','quote_count']] $ check=check.rename(columns = {'quote_count':'count'}) $ check.groupby(['lang'], sort=True, as_index=False).count().sort_values(by='count', ascending=False)
print(deep_triplet_model.summary())
km.labels_
print(type(x), "\n") $ print(x.upper()) # string method applies to bytes type object. $ print(type(x))
df.landing_page.unique()
df = df.dropna()
selected_features=selected.index $ x_train_new=x_train[selected_features] $ x_test_new=x_test[selected_features]
(call.iloc[-1].Prima-2.6*sim_closes.iloc[-1].std()*np.exp(-r*ndays)/np.sqrt(nscen),call.iloc[-1].Prima+2.6*sim_closes.iloc[-1].std()*np.exp(-r*ndays)/np.sqrt(nscen))
from urllib import request $ req = request.Request('https://python.org') $ res = request.urlopen(req) $ print(res.read().decode('utf-8'))
gbm_model.varimp_plot() $ print(gbm_model.confusion_matrix(valid = True))
for i in files_to_manage: $     main_tables[i] = main_tables[i].append(new_data[i])
grouped_sentiments_pd = sentiments_pd.groupby('Media Source') $ sentiment_avg_pd = grouped_sentiments_pd.mean() $ sentiment_avg_pd.sort_values(by=['Compound'], ascending=False, inplace=True) $ sentiment_avg_pd = sentiment_avg_pd.reset_index() $ sentiment_avg_pd
df_out = df.iloc[:, 1:(df.shape[1]-6)] $ df_out.index = df['V1'] + "[" + df['FXNCLASS'] + "] " + df['SYMBOL'] $ fig=plt.figure(figsize=(15, 80), dpi= 80, facecolor='w', edgecolor='k') $ sns.heatmap(df_out,cmap='RdBu_r', vmin=0, center=1.301, vmax=7.301)
plt.plot(df.index, df['Price'])
log_mod = sm.Logit(df3['converted'], df3[['intercept', 'ab_page' , 'CA' , 'UK']]) $ results = log_mod.fit() $ results.summary() $
mp2013 = pd.period_range('1/1/2017','12/31/2017',freq='M') $ mp2013
history_with_target = intervention_history.join(intervention_train['target'], how='left')
users_jobs = customers.merge(jobs, how='left', left_on='id', right_on='customer_id') $ users_jobs_occ = users_jobs.merge(occurrences, how='left', left_on='id_y', right_on='job_id') $ users_jobs_occ_zones = users_jobs_occ.merge(zones, how='left', left_on='zone_id', right_on='id') $ users_jobs_occ_zones_cities = users_jobs_occ_zones.merge(cities, how='left', left_on='city_id', right_on='id') $ customer = users_jobs_occ_zones_cities
selectedComponents = [x for x,y in zip(range(len(pca.explained_variance_)),pca.explained_variance_) if y>=0.1]
ids = df2['user_id'] $ (df2[ids.isin(ids[ids.duplicated()])])
X2 = PCA(2, svd_solver='full').fit_transform(X) $ X2.shape
liberia_data.columns = ['Bomi County', 'Bong County', 'Date', 'Gbarpolu County', 'Grand Bassa', $        'Grand Cape Mount', 'Grand Gedeh', 'Grand Kru', 'Lofa County', $        'Margibi County', 'Maryland County', 'Montserrado County', 'Totals', $        'Nimba County', 'River Gee County', 'RiverCess County', 'Sinoe County', $        'Unnamed: 18', 'Description']
n_old = df2.query('group == "control"').count()[0] $ n_old
sub = pd.read_csv('../input/sample_submission.csv') $ sub['Tag'] = final_test_pred_nbsvm1 $ sub.to_csv('../submissions/nvsvm.csv', index=False)
X_train.shape
twitter_final['length'] = twitter_final.text.apply(len)
print('Shape of Train data:', train.shape) $ print('Shape of Test data:', test.shape)
def sql_to_df(sql_query): $     df=pd.read_sql(sql_query,db) $     return df
news = ['cnn', 'fox', 'new york times', 'washington post'] $ news_dict = {} $ for entity in news: $     news_dict.update({entity: df.text.str.extractall(r'({})'.format(entity), re.IGNORECASE).size}) $ news_dict = dict(sorted(news_dict.items(), key= lambda x: x[1], reverse=True))
df2['converted'].mean() $
subtes.head(10)
df.groupby('episode_id')
run txt2pdf.py -o"2018-06-18  2015 853 disc_times_pay.pdf"  "2018-06-18  2015 853 disc_times_pay.txt"
highest_obs = session.query(Measurement.station, Measurement.tobs).\ $     filter(Measurement.date > '2017-08-023').\ $     group_by(Measurement.station).\ $     order_by(func.count(Measurement.tobs).desc()).all() $ highest_obs
import statsmodels.api as sm $ convert_old = df2[df2.landing_page == 'old_page'].converted.sum() $ convert_new = df2[df2.landing_page == 'new_page'].converted.sum() $ n_old = df2[df2.landing_page == 'old_page'].count()['user_id'] $ n_new = df2[df2.landing_page == 'new_page'].count()['user_id'] $
!head -5 "data_sql_input.csv"
df['time_created'] = pd.to_datetime(df['time_created'])
last_year = stops[stops["date"] > datetime.datetime(year=2017, month=3, day=28)]
convert_old = df2.loc[(df2.landing_page == "old_page") & (df2.converted == 1)].user_id.nunique() $ convert_old
(autos['price'] $  .value_counts() $  .head() $  .sort_index(ascending=False))
pd.date_range('1/1/2016', '12/1/2016', freq='bm')
df3 = df3.join(dummy) $ df3.head()
print("\nThe columns in the data have the following data types:\n{}".format(df.dtypes))
dummy_PayMethod = pd.get_dummies(df['PaymentMethod'], prefix='PayMethod') $ print(dummy_PayMethod.head())
train_data.head()
import pandas as pd $ speeches_df4 = speeches_df3.loc[:,~speeches_df3.columns.duplicated()]
sl[sl.status_binary==0][(sl.today_preds==1)].shape[0]*.57
p_new = new_page_converted.mean() $ p_old = old_page_converted.mean() $ p_new-p_old
merge[merge.columns[40:]].head()
pd.options.display.max_colwidth = 2000 $ data_df[data_df.nwords > 800]['clean_desc']
tweet_archive_clean.text.head()
run txt2pdf.py -o '2018-07-09-2015-870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS.pdf'  '2018-07-09-2015-870 - SEPTICEMIA OR SEVERE SEPSIS W MV 96+ HOURS.txt'
journalists_mention_summary_df[['mention_count']].describe()
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href']
df = pd.read_csv("AppleTweets19.csv")
len(df[(df.landing_page=='new_page') & (df.group=='control')]) + len(df[(df.landing_page=='old_page')& (df.group=='treatment')])
social_disorder = scale_values(social_disorder, list(range(1,10))) $ social_disorder.head()
expectancy_for_least_country = le_data.min(axis=0) $ expectancy_for_least_country
props=pd.read_csv("http://www.firstpythonnotebook.org/_static/committees.csv")
calls_df.loc[calls_df["status"]=="INCALL","call_time"].value_counts()
print("LED on") $ GPIO.output(18,GPIO.HIGH) $ time.sleep(1)
merged2.dropna(subset=['Specialty'], how='all', inplace=True)
round(df2[df2['group'] == 'treatment']['converted'].mean(), 4)
click_condition_meta.reset_index(inplace = True, drop = True) $ click_condition_meta.info()
index = pd.MultiIndex.from_product([['a', 'c', 'b'], [1,2]]) $ data = pd.Series(np.random.rand(6), index=index) $ data.index.names = ['char', 'int'] $ data
%matplotlib inline $ import matplotlib.pyplot as plt, numpy as np
billstargs.drop(([408], [416], [456], [718], [757], [811], [928], [968], [1075], [1204], [1463], [1464], [1466], [1467], [1468], [1469], [1470], [1497], [1552], [1555], [1576], [1577], [1655], [1801], [2125], [2126], [2127], [2128], [2129], [2130], [2131], [2260], [2428], [2599]), inplace=True) $
users.head()
data = pd.DataFrame(r.json()['dataset']['data'], columns=['Date', $    'Open','High','Low','Close','Change','Traded Volume','Turnover', $    'Last Price of the Day','Daily Traded Units','Daily Turnover'])
def normalizer(mat): $     a = np.array(mat) $     b = np.zeros_like(a) $     b[np.arange(len(a)), a.argmax(1)] = True $     return b
ins['new_date'] = pd.to_datetime(ins['date'], format='%Y%m%d') $ ins.head(5)
clean_df = pd.DataFrame(clean_tweet_texts,columns=['text']) $ clean_df['target'] = df.sentiment $ clean_df.head()
for i in ['Updated Shipped diff']: $     t = df[i].hist(bins=500) $     t.set_xlim((300,8000)) $     t.set_ylim((0,500))
1/np.exp(-0.0149), 1/np.exp(-0.0408), np.exp(0.0099)
new_page_converted = [] $ sim_new = np.random.choice([0, 1], size=n_new, p=[1-p_new, p_new]) $ new_page_converted.append(sim_new) $ p_new_sim = sim_new.sum()/n_new $ p_new_sim
days_to_30 = (spencer_bday + thirty_years - today).days $ print("Spencer will be 30 in {} days".format(days_to_30))
X = [html.unescape(string) for string in X]
c.find_one({'albums.released': {'$gt': 1980}})
the_drg_number = 66 $ idx = df_providers[ (df_providers['year']==2011) & \ $                   (df_providers['drg3']==the_drg_number)].index.tolist() $ print('There are',len(idx),'sites for DRG',the_drg_number) $ print('Max payment:',np.max( df_providers.loc[idx,'medicare_payment'] ))
save_n_load_df(promo_df, 'promo_df5.pkl')
df = pd.merge(df_demo, df_clinic, on='patient_id', how='left') $ df.shape
changes = sp_companies[1] $ changes.iloc[7:12]
segmentData['opportunity_won_amount'] = segmentData.apply(stageWonMap, axis=1)
simm.sort_values('test',ascending=False)
click.shape
cashflows_act_investor[cashflows_act_investor.id_loan==1545]
df['y'].plot(marker='o', markersize=5)
print topUserItemDocs.shape $ topUserItemDocs=topUserItemDocs.join(targetItemData['id'],on='item_index_corpus',how='left') $ print topUserItemDocs.shape $ topUserItemDocs.head()
with tb.open_file(filename='data/my_pytables_file.h5', mode='w') as f: $     f.create_group(where='/', name='my group')
assert isinstance(sent, pd.DataFrame) $ assert sent.shape == (7517, 1) $ assert list(sent.index[5000:5005]) == ['paranoids', 'pardon', 'pardoned', 'pardoning', 'pardons'] $ assert np.allclose(sent['polarity'].head(), [-1.5, -0.4, -1.5, -0.4, -0.7])
df_madrid['clean_text'] = [tweet_cleaner(t) for t in df_madrid.text]
sorted(Counter(reducedwordlist).items(),key=lambda wordtuple: wordtuple[1],reverse=True)[:30]
df2 = df $ treat_no = df[(df['group']=='treatment') & (df['landing_page']=='old_page')] $ ctrl_no = df[(df['group']=='control') & (df['landing_page']=='new_page')] $ no_lineup = pd.concat([treat_no, ctrl_no]) $ df2 = df2.drop(no_lineup.index)
df_time['tweetRetweetCt'].plot(kind = 'bar') $ plt.xlabel('Time Zone') $ plt.ylabel('Number of Retweets') $ plt.show()
autos["odometer"]= autos["odometer"].replace("[,km]",'',regex=True) $ autos["odometer"]
df2.groupby([str.lower, mapping]).mean()
df_twitter_archive_copy.gender.dtype
orgs = [] $ for act in actions: $     if act['Method']['Organization']['OrganizationCode'] not in orgs: $         orgs.append(act['Method']['Organization']['OrganizationCode'])
clf.fit(X, y)
movies['year']=movies['title'].str.extract('(\(\d\d\d\d\)$)',expand=True) $ movies['year'] = movies['year'].str.replace('(', '') $ movies['year'] = movies['year'].str.replace(')', '')
def create_validation(df, start_date): $     return df.loc[(df['date'] >= pd.to_datetime(start_date) - relativedelta(days=0)) & \ $                   (df['date'] <  pd.to_datetime(start_date) + relativedelta(months=6))].index, \ $            df.loc[(df['date'] >= pd.to_datetime(start_date) + relativedelta(months=6)) & \ $                   (df['date'] <  pd.to_datetime(start_date) + relativedelta(months=12))].index
n_old = df2[df2['landing_page'] == 'old_page']['user_id'].count() $ print('n_old: ', n_old)
perceptron = Perceptron() $ perceptron.fit(X_train, Y_train) $ Y_pred = perceptron.predict(X_test) $ acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2) $ acc_perceptron
cryptos.name  # Ready out loud "cryptos dot name"
df_twitter_copy = df_twitter_copy[df_twitter_copy['retweeted_status_id'].isnull()]
df['CurrentRatio'] = df['CurrentRatio'].map(transform1)
heatmap(ddf) $ plt.show()
plotdf.loc[plotdf.index[-1], 'forecast'] = hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'meanRemainingTweets'].iloc[0] + plotdf['currentCount'].max() $ plotdf.loc[plotdf.index[-1], 'forecastPlus'] = plotdf.loc[plotdf.index[-1], 'forecast'] + hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0] $ plotdf.loc[plotdf.index[-1], 'forecastMinus'] = plotdf.loc[plotdf.index[-1], 'forecast'] - hourlyRates.loc[hourlyRates['hourNumber'] == thisWeekHourly['hourNumber'].max(),'stdRemainingTweets'].iloc[0]
top_supports.head(5).amount.plot.barh()
weekend_grid['predict'] = adj_glm_int.predict(weekend_grid[['inday_icu_wkd','admission_type']]) $ weekend_grid['log_odds'] = prob2logodds(weekend_grid['predict']) $ weekend_grid.set_index(['inday_icu_wkd','admission_type'], inplace=True) $ weekend_grid
from sklearn.feature_extraction.text import TfidfVectorizer $ tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=2, max_df=0.5, stop_words=portuguese_stop_words)
pd.DataFrame(rows, columns=np.transpose(table_info)[1])
SCR_PLANS_df['start_date'] = pd.to_datetime(SCR_PLANS_df['scns_created'].apply(lambda x:x[np.argmin(x)])).dt.strftime('%Y-%m')
price2017.head()
hdaysDF = trainDF[['MONTH', 'DAY_OF_MONTH', 'HDAYS']].drop_duplicates()
c.execute(query) $ results = c.fetchall() $ df = pd.DataFrame(results) $ df = df.rename(columns={0:"Type",1: "Number"}) $ df
def norm_by_data2(x): $     x['data1'] /= x['data2'].sum() $     return x $ display('df', "df.groupby('key').apply(norm_by_data2)")
df_new['intercept'] = 1 $ lm = sm.OLS(df_new['converted'], df_new[['intercept','CA', 'US']]) $ lm.fit().summary()
plot_data = df['amount_tsh'] $ sns.kdeplot(plot_data, bw = 1000) $ plt.show()
d.sum()
to_match = pd.Series(['c', 'a', 'b', 'b', 'c', 'a'])
final = sample.merge(c_df,on="Date",how="outer")
movies2000to2014['year']=pd.to_numeric(movies2000to2014['year'],errors='coerce')
stocks.write.format("parquet").save("stocks.parquet")
print(stockdf.columns.get_loc('PCF')) $ print(stockdf.columns.get_loc('Prediction Feature 5'))
top_supports['contributor_fullname'] = top_supports.contributor_firstname + " " + top_supports.contributor_lastname
USER_PLANS_df.head()
sample_diff = new_page_converted.mean() - old_page_converted.mean() $ sample_diff
test_data = pd.read_csv('data/kaggle_data/test-val.csv', header=None, sep=" ", names=list_feature_names) $ test_data.head(5)
sorted_budget_biggest.groupby('original_title')['vote_average'].mean()
bool_sel = autos['registration_year'] > 2016 $ autos.loc[autos['registration_year'] > 2016,'registration_year'].value_counts() $
news_df = news_df.loc[news_df['topic'] != 'commentisfree',:]
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=38000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
X2.head() #original label encoded data for Sierra Leone
comments_per_user = scores.groupby('author')['score'].count() $ comments_per_user.sort_values(ascending=False)
writer.save() $ writer.close()
df_c = df.query('landing_page!= "old_page"')  $ df_cb = df_c.query('group == "control"') $ df_cb.nunique() $
dfss.price.max()
df_log.head()
tweets_clean = tweets.copy()
tweets.head() $ tweets.dtypes
df_protest.shape
df_questionable_2.head(3)
much_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26)) $ large_df = pd.DataFrame(much_data, columns=list("ABCDEFGHIJKLMNOPQRSTUVWXYZ")) $ large_df[large_df % 16 == 0] = np.nan $ large_df.insert(3,"some_text", "Blabla") $ large_df
tweet_scores_clean=tweet_scores_clean[tweet_scores_clean.retweet_count!='failed']
all_df = pd.read_pickle('data/all_political_ads.pickle')
shows.head()
n_old = len(df2.query("landing_page == 'old_page'")) $ print('N_old is {}'.format(n_old))
df.head()
qualConvpct = qualConvpct.filter(like='convertedQual').rename('qualConversionPercent').to_frame().reset_index()
import urllib.request $ google = urllib.request.urlopen("http://google.com") $ print(google.read()[:300])
offsets = list(pd.timedelta_range(start, periods=52, freq= '7D')) # Compiling list of offsets for same range $ offsets # Inspecting progress
data.head()
logit_mod = sm.Logit(df_new['converted'], $ df_new[['intercept', 'ab_page', 'US', 'new_US', 'CA', 'new_CA']]) $ results_log = logit_mod.fit() $ results_log.summary()
with model: $     idx = np.arange(n_count_data) $     lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)
tipsDF.hist(figsize=(10, 10)) $ plt.savefig('text_preparation/likes_target_feature.png')
from sklearn.linear_model import LinearRegression
xml_in[xml_in['publicationKey'].isnull()].count()
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).head(10)
species_count = faa_data_pandas['SPECIES'].value_counts() $ print(species_count)
unsolve_hacker_list = list(sub1['hacker_id'].unique())
joined = pd.read_feather(f'{PATH}joined') $ joined_test = pd.read_feather(f'{PATH}joined_test')
autos['price'].dtype
raw.fillna(0, inplace=True)
drace_df.dropna(inplace=True)
stats = prices.describe().transpose() $ cols_to_drop = list(stats[stats['count'] < min_days_traded].index) $ len(cols_to_drop)
prices.tail()
image_predictions[image_predictions.duplicated('tweet_id')]
df.rename(columns={'Closed Date': 'closed_at'})
df - 0.5*df
california = california.query("FIRE_SIZE > 321")
ex=x.mean() $ ey=y.mean() $ print(ex) $ print(ey)
miss_loans=set(loans.id_loan) ^ set(payment_plans.fk_loan) # symmetric difference
tokendata = tokendata.applymap(lambda x:round(x,2))
ip[ip.duplicated()]
run txt2pdf.py -o '2018-06-22 2015 FLORIDA HOSPITAL Sorted by payments.pdf'  '2018-06-22 2015 FLORIDA HOSPITAL Sorted by payments.txt'
X = pd.merge(X, menu_about_latent_features, left_on='master_menu_id', right_on='menu_id', how='left') $ del X['menu_id']
tmax_day_2018.attrs
sub_mean = df.groupby('subreddit').agg({'num_comments': 'mean'}) $ top_com = sub_mean.sort_values('num_comments', ascending = False).head() $ top_com
tips.groupby(["sex","size"]).mean().loc[:,"total_bill"].loc["Female",3:5]
tweets_predictions_all.to_csv('twitter_archive_master.csv', index=False)
df_h1b_nyc[df_h1b_nyc.pw_1>300000][['pw_1','lca_case_wage_rate_from','lca_case_wage_rate_to']]
df_raw[df_raw.list_date.isnull()]
with open('clean_tweets_sample.pkl', 'wb') as picklefile: # wb: write, binary $     pickle.dump(sampleDF, picklefile) #dump data into pickle file
train_view.sort_values(by=7, ascending=False)[0:10]
service = discovery.build('calendar', 'v3', credentials=credentials)
def calc_temps(start_date, end_date): $     return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2012-02-28', '2012-03-05'))
new_page_converted = [] $ for _ in range(n_new): $     a = np.random.binomial(1, p_new) $     new_page_converted.append(a) $ new_page_converted = np.asarray(new_page_converted)
crimes.head()
df_count_clean.shape
p_new = (df2['converted'] == 1).mean() $ p_new $
dfJobs.ix[4009]
random_integers.as_matrix()
so_score_10_or_more = so[criteria] $ so_score_10_or_more.head()
X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.3, $                                                     random_state=42)
adopted_cats.loc[adopted_cats['Color']=='Orange Tabby/Orange','Color'] = 'Orange Tabby' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Orange Tabby','Color'] = 'Calico Orange Tabby' $ adopted_cats.loc[adopted_cats['Color']=='Tortie/Orange','Color'] = 'Tortie Orange'
tweet_archive_clean = tweet_archive_clean[tweet_archive_clean.tweet_id != 855862651834028034]
grouped.describe()
sqlContext.sql(query).toPandas()
sites.dtypes
top_words = pd.DataFrame(X.toarray()).sum().sort_values(ascending=False).head(10) $ for word in top_words.index: $     print 'Feature: {}, Token: {}'.format(word, tfidf.get_feature_names()[word])
data = df.copy()
analyze_set.loc[analyze_set['favorites']==162332]
sns.lmplot(x="age", y="satisfied", data=training, x_estimator=np.mean, order=1)
df_airbnb.loc[index_outliers_age,:]
yc_new.head()
All_tweet_data_v2.to_csv('twitter_archive_master.csv', sep=";", encoding='utf-8')
df2[['test', 'ab_page']] = pd.get_dummies(df2['group']) $ df2['intercept'] = 1 $ df2 = df2.drop(['test'], axis = 1) $ df2.head()
df[['favorites', 'rating', 'retweets']].corr(method='pearson')
brand_counts = autos["brand"].value_counts(normalize=True).head(7) $ brands = brand_counts.index $ brands
session.query(Measurement.tobs).order_by(Measurement.tobs.desc()).first()
inp = Input(shape=(train_source_emb.shape[1],)) $ x = Dense(train_target_emb.shape[1], use_bias=False)(inp) $ modal_model = Model([inp], x) $ modal_model.summary()
onehot = pd.get_dummies(wheels['drive_wheels']).head() $ onehot
df_tsv.drop_duplicates() $ df_archive_csv.drop_duplicates() $ df_json_tweets.drop_duplicates()
ac.groupby(['IAM', 'Bank'])['Complaint Name'].count()
df2.head(5)
n_old = df2.query('landing_page == "old_page"')['user_id'].count() $ n_old
pd.Timestamp('9/3/2016')-pd.Timestamp('9/1/2016')
archive_copy.info()
lm = sm.Logit(df_new['converted'],df_new[['intercept','CA','US','ab_page']]) $ result = lm.fit() $ result.summary()
data_stations=session.query(Measurement.station, func.count(Measurement.tobs)).\ $ group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).all() $ data_stations $
airlines_day_unstacked = airlines_day.unstack().reset_index() $ airlines_day_unstacked.rename(columns={'level_0': 'airline', 0: 'count'}, inplace=True)
tweets_wo_dublicates = tweet_df[~tweet_df['username'].isin(dublicated_users)]
tsd_df.dtypes
tweets_original.head()
gold = IMDB_df.groupby("movie_name") $ review_count = gold.movie_name.count() $ star_avg = gold.stars.mean() $ positive = gold.positive.mean()
model.fit(team_names[predictor_cols],team_names.regular_occurrences)
df_germany = pd.read_csv('data/Y_Germany.csv') # Pre-created data for Germany vs South Korea
SMOOTH.plot_init_latency(smoothresult,option="")
%%bash $ tar -zxf aclImdb_v1.tar.gz $ ls
taxi_hourly_df.index.max()
no_images=[] $ for i in range(len(tweet_json)): $     if 'media' not in list(tweet_json['entities'][i].keys()): $         no_images.append(i) $ print("{} tweets do not have an image".format(len(no_images)))
full_act_data.to_csv(os.path.join(data_dir, 'bbradshaw_fbml_data.csv'))
auto_new = auto_new[auto_new.Price > 1000]
first_movie.div
print k_var.dtypes
actor = pd.read_sql_query('select * from country where country in ("Afghanistan", "Bangladesh", "China")', engine) $ actor.head()
portfolio_df.set_index(['Ticker'], inplace=True) $ portfolio_df.head()
mask = df2.user_id.duplicated(keep=False) $ df2[mask] #found this technique on Stack Overflow - nice!? $
import pandas as pd $ df = pd.read_csv('/data/measurements/C47C8D65CB0F.csv', $                  names=['time', 'moisture', 'temperature', 'conductivity', 'light']) $ print(df.to_string())
classifier.fit(X_train, y_train) # fitting the model using train set
database.keys()
data.sort_index(inplace=True) $ data.head(5)
df_h1b = df_data[df_data.visa_class=='H-1B'].drop('visa_class',axis=1)
len(merged2[(merged2.dir_nmovies == 0)])# | (merged2.act_gross == 0)])
converted = df2.query('converted == 1') $ converted.user_id.nunique()/df2.user_id.nunique()
cabs_df_rsmpld = cabs_df_byday.resample('1M')['passenger_count'].count() $ cabs_df_rsmpld.head()
transactions.join(users.set_index('UserID'), on='UserID', how = 'inner') $
pd.read_csv("Data/microbiome.csv", skiprows=[3,4,6]).head()
fundvolume_binary.T.sum().plot(logy=True) $ plt.title('Fund') $ plt.ylabel('Volume') $ plt.show()
fulldf.head()
df[df.duplicated(subset='user_id', keep=False)].groupby(['group', 'landing_page']).count()
department_df.groupby("Department", as_index = False).sum() # if we want the group keys as a column and not index 
ser5 = pd.Series(5,index=[0,1]) $ ser5
users = df.nunique()["user_id"] $ print("Number of unique users - {}".format(users))
import pandas as pd $ weather = pd.DataFrame(weather_json['data']) $ weather['date']  = pd.to_datetime(weather['date'])
len(" ".join(tm['text']).split(" "))
old_page_converted = np.random.binomial(1,pold,nold)
Maindf['Year'] = Maindf.index.year $ Maindf['Month'] = Maindf.index.month $
stop = stopwords.words('english') $ locationing['Text'] = locationing['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) $ pd.Series(' '.join(locationing['Text']).lower().split()).value_counts()[:50]
y_id = test['listing_id'].astype('O')
package.descriptor['name'] = 'winemag-reviews' $ package.descriptor['title'] = 'Winemag wine reviews dataset' $ package.descriptor
max_fwers = df.loc[df.followers.idxmax()] $ name = max_fwers['name'] if max_fwers['name'] is not None else max_fwers['login'] $
train_small_data = pd.read_feather("../../../data/talking/train_small_data.feather") $ val_small_data = pd.read_feather("../../../data/talking/val_small_data.feather") $ test = pd.read_feather("../../../data/talking/test_small_data.feather")
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ res = lm.fit() $ res.summary2()
pd.Series(master_file.axes[1])
np.arange(10,25,5)
a = df_2015.sales_jan_mar.sum() $ a
tweet_data_clean.index
dates = pd.date_range('2016-04-01', '2016-04-06') $ dates
feeds = json.loads(response) $ print(type(feeds)) $ print(feeds)
logit_mod = sm.Logit(df_total.converted, df_total[['intercept','UK','US']]); $ results = logit_mod.fit() $ results.summary()
rest_2017["Tm.3PA"].mean()
tips.columns
closes = s4p.pivot(index='Date',columns='Symbol',values='Adj Close') $ closes[:4]
import statsmodels.api as sm $ df2.head(5) $
print('Saving processed file to the path:',processed_file_path) $ df_final.to_csv(processed_file_path, index=False)
df2.country.value_counts()
chambers.reset_index(inplace = True)
df.cdescr.head()
last_year = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ print(last_year)
stories.dtypes
b_new_order_item.to_csv('../2_data/explored/order_item.csv', index =False)
type(twitter_archive_full.timestamp[0])
autos.head()
myplot_parts = [go.Scatter(x=counts["stamp"],y=counts["count"],mode="line",name="Total"), $                 go.Scatter(x=person_counts["stamp"],y=person_counts["count"],mode="line",name="KianMcIan")] $ mylayout = go.Layout(autosize=False, width=1000,height=500) $ myfigure = go.Figure(data = myplot_parts, layout = mylayout) $ iplot(myfigure,filename="crisis")
season_groups.ngroups
df3.sample(5)
lines = access_logs_df.count() $ lines
conn.commit()
aux = image_clean.merge(dog_rates.loc[dog_rates.cuteness == 'puppo'], $                                                            how='inner', on='tweet_id') $ aux[['jpg_url','grade']].sort_values(['grade'], ascending=False).iloc[:,0].values[:3]
type(status.user)
out_train.columns = ["high", "medium", "low"] $ out_train.head()
autos['registration_year'].describe()
d={'c1':['A','B','C','D'],'c2':np.random.randint(0,4,4)} $ pd.DataFrame(d)
all_preds.shape
df.info()
b_cal_q1 = pd.DataFrame(b_cal[b_cal['available'] == 't'])
data.info()
crimes_all.to_csv('data/crime/crimes_all.csv')
p_diffs = np.array(p_diffs) $
style0 = xlwt.easyxf('font: name Times New Roman, color-index red, bold on', num_format_str='#,##0.00') $ style1 = xlwt.easyxf(num_format_str='D-MMM-YY')
R=Meter1.ErrorReadAct(); print(R)
percipitation_2017_df.plot() $ plt.show()
learning_rate = 0.01 $ with tf.name_scope("train"): $     optimizer = tf.train.GradientDescentOptimizer(learning_rate) $     training_op = optimizer.minimize(loss)
data['timestamp'] = data['timestamp'].dt.strftime('%Y-%m-%d')
obj2 = pd.Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])
dfd = dfs.drop_duplicates(subset = 'text', inplace=False)
raw_df = raw_df.replace('\B(@[\w]+)', '', regex=True) $ raw_df = raw_df.replace('\B(https//[\S\w]+)', '', regex=True)
cust_data2=cust_data1.set_index("ID") $
(final_rf_predictions['predict']==test['Cover_Type']).as_data_frame(use_pandas=True).mean()
height.iloc[0] = float('nan')
z_score, p_value = sm.stats.proportions_ztest([17489, 17264], [145274, 145310], alternative='smaller') $ z_score, p_value $
bikedataframe = pd.concat([dfbikes, snow_weather, df_wu, dfsunrise], axis=1) $ bikedataframe.head()
df_enhanced.info() $ df_enhanced[df_enhanced['in_reply_to_status_id'] != 'nan'].head(3)
tweets = pd.read_csv("dataframe_terror.csv", parse_dates=[0]) $ terror = pd.read_csv('attacks.csv', parse_dates=[0]) $ terror["type"] = terror["type"].fillna("UnkownType")
new_page = df2[df2["landing_page"] == 'new_page'] $ new_page_prob = new_page.shape[0]/df2.shape[0] $ new_page_prob
pd.ols(y=mr, x=lagged)
store_items = store_items.drop(['watches', 'shoes'], axis = 1) $ store_items
params = {'figure.figsize': [6,6],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_partial_autocorrelation(doc_duration.diff()[1:], params=params, lags=30, alpha=0.05, \ $     title='Weekly Doctor Hours First Difference Partial Autocorrelation')
%matplotlib inline $ import matplotlib.pyplot as plt # this imports the plotting library in python}
result['uid'].nunique()
cursor.execute("INSERT INTO person VALUES (7, 'Moore', 'Andrew');") $ pd.read_sql_query("SELECT * from person;", conn, index_col="id")
df_twitter['tweet_id'] = df_twitter['tweet_id'].astype(str)
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $
pd.set_option('display.max_columns', None)  
calories_df.head()
df_sentiment.withColumn( $         "value_count", get_log_digitized_udf(np.arange(-1.25,7.25,0.5), add_1=False)("followers_count") $     ).head()
df_total = pd.concat([weekday_df, month_df, df_total], axis=1)
grouped_bias = up_bias_x20.groupby(['country_destination'],as_index=False) $ grouped_bias.count()
print('Unique number of users notified: {}'.format(len(enroute_4x['vendorId'].unique())))
data.to_csv("skincare.csv", encoding='utf-8', index=False)
p_new = df2.converted.sum() / df2.shape[0] $ p_new
cityID = '7c01d867b8e8c494' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Garland.append(tweet) 
plt.figure(4) $ nvda_plot = plt.subplot() $ nvda_plot.plot(nvda['Close'],color='purple') $ plt.legend(['Nvidia Close Value'],loc="upper left") $ plt.title('Valores de Cierre Nvidia')
education_data.head(10)
n_old=df2[df2['landing_page']=="old_page"].shape[0] $ n_old
%matplotlib inline $ df.plot();
lm = smf.ols(formula='y ~ x', data=df).fit() $ lm.params
print(arma_res.summary())
stc.checkpoint("checkpoint12")
to_be_predicted_Day3 = 21.39121221 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
print('looking for the duplicate entry') $ df2[df2.duplicated(['user_id'], keep=False)]['user_id']
accuracies.mean()
l = [i for i in dummy_features if i not in test.columns.tolist()] $ print('%d dummy_features in train not in test.'%len(l))
ttarc.expanded_urls.str.extract('(photo)').count()
feature_cols = ['TV', 'radio'] $ X = data[feature_cols] $ print(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')).mean())
!git clone https://github.com/tensorflow/models.git
plt.hist(p_diffs) $ plt.xlabel("p_diffs") $ plt.ylabel("frequency") $ plt.title("plot of simualted p_diffs")
df_cal.is_all_day.value_counts()
questions = pd.concat([questions.drop(['rate_experience'], axis=1), experience], axis=1)
df.groupby('Hour').Avg_speed.mean()
def create_validation(df, start_date, gap_size, train_size, test_size): $     train = np.arange(start_date, start_date+train_size) $     validate = np.arange(start_date+train_size+gap_size, start_date+train_size+gap_size+test_size) $     return train, validate
min(TestData.DOB_clean), max(TestData.DOB_clean)
properati.info()
meta = [] $ for chunk in tqdm(chunks(video_ids, 50)): $     data = yt.get_video_metadata(chunk, key, verbose=1) $     meta.extend(data) $     time.sleep(.1)
print(multi.shape) $ print(pp.shape)
!find {TRN} -name '*.txt' | xargs cat | wc -w
date = datetime.datetime.strptime(   ) $ mask = 
df.query("landing_page=='old_page' and group=='treatment'").count()[0] + df.query("group =='control' and landing_page=='new_page'").count()[0] 
xml_in[xml_in['venueName'].isnull()].count()
filtered_store_stuff = joined_store_stuff.filter("store_level > 2").sort( joined_store_stuff['count'].desc() ) $ filtered_store_stuff.show()
bus['text'].iloc[7]
result = c.update_one({'name.last': 'Bowie'}, $                       {'$set': {'albums': []}})
dict_category['slug'].split('/')[0] # parent category
df.groupby(['character_id', 'raw_character_text']).size().reset_index().head(10)
df3.fillna(0, inplace=True) $ df3.head() $ df3.isnull().sum()
sl_train= sl.loc[sl.two_measures==0] $ sl_holdout= sl.loc[sl.two_measures==1] #completely remove both measurements $ print("training rows",sl_train.shape[0]) $ print('holdout_rows', sl_holdout.shape[0]) $ print('sanity check: full sl shape', sl.shape[0], 'combination of train and holdout',sl_train.shape[0] + sl_holdout.shape[0]) 
infinity.head()
def grad_computation(theta, X, y): $     hx = sigmoid(np.dot(X, theta)) $     error = hx - y # difference between label and prediction $     grad = np.dot(error, X) / y.size # gradient vector $     return grad
plt.scatter(cdf.CYLINDERS, cdf.CO2EMISSIONS,  color='blue') $ plt.xlabel("Cylinder") $ plt.ylabel("Emission") $ plt.show()
df_repub.iloc[2984]
all_sites_with_unique_id_nums_and_names.shape $
df2_treatment = df2.query('group == "treatment"') $ p_treatment = df2_treatment['converted'].sum() / df2_treatment.shape[0] $ p_treatment
final_topbike = top_bike[top_bike['Distance'] != 0.000000]
counts_df.info()
index_to_change = df3[df3['group'] == 'treatment'].index $ df3.set_value(index = index_to_change, col = 'ab_page', value = 1) $ df3.set_value(index = df3.index, col = 'intercept', value = 1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
data.columns
df_2010['bank_name'] = df_2010.bank_name.str.split(",").str[0] $
pd.set_option('display.mpl_style', 'default') $ plt.rcParams['figure.figsize'] = (15, 5)
forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 1 $ forecast_df['unemployment'] = 100 - (forecast_df['employed'] / (forecast_df['pop'] - forecast_df['under_20']) * 100) $ forecast_df.loc[forecast_df['pop'] == 0, 'pop'] = 0 $ forecast_df.head()
df_2018_talks.shape
frame3.values
for i in range(-5, 0, 1) : $     data[f'Close {i}d'] = data['Close'].shift(-i) $ data = data.dropna() $ data.head()
cryptos[cryptos.percent_change_7d > 25]
bymin.resample("S").bfill()
contractor_final.to_csv('contractor_final.csv')
test = pd.read_csv('Input/test.csv') $ train  = pd.read_csv('Input/train.csv')
df_old = df.query('landing_page == "old_page"') $ p_old = df_old[df_old['converted'] == 1]['converted'].count() / df_old[['converted']].count() $ print (p_old)
df_tot = df_stock.merge(df_index, left_on='price_date', right_on='price_date') $ df_tot.rename(columns={'variable':'ticker'},inplace=True) $ df_tot.set_index(['price_date', 'ticker'],inplace=True) $ df_tot.rename(columns={'Return': 'Returns'},inplace=True)
%%timeit -n 10 $ for state in df['STNAME'].unique(): $     avg = np.average(df.where(df['STNAME']==state).dropna()['CENSUS2010POP']) $     print('Counties in state ' + state + ' have an average population of ' + str(avg))
databreach_2017.head()
round(len(df_twitter[df_twitter.dog_label == 'doggo']) / len(df_twitter.dog_label), 2)
autos['price'].unique().shape
os.getcwd()
fashion[fashion.index == 'gucci'].sort_values("PRADA-proba", ascending=False).head(10)
df2['DepTimeStr'].head()
todaysTweets.head()
%time nb.fit(train_4, y_train)
import pandas as pd $ the_dict = pd.read_clipboard().to_dict('records') $ the_dict
model_preds.sort_values('prob_off').head(100)
isinstance(r.json(), dict)
data = data[data['Processing Time']>=datetime.timedelta(0,0,0)]
np.mean(score)
import datetime $ rng = pd.date_range('2015-12-01 20:00', periods=100, freq='S') $ df = DataFrame({'sales':np.random.randint(0, 500, len(rng))}, index=rng) $ df.head()
IncBC = (delta(df_new, df_old, ['BCDemandMW','BC_TP'], Aggregate=True, ConvertToMWH=True) - df_new['RealPrice'] * delta(df_new, df_old, ['BCDemandMW'], ConvertToMWH=True)).rename('IncBC') $ IncSwaps = (df_new['RealPrice'] * delta(df_new, df_old, ['SwapMW'], ConvertToMWH=True) - delta(df_new, df_old, ['SwapMW','VWSwapStrike'], Aggregate=True, ConvertToMWH=True)).rename('IncSwap') $ IncSwaptions = (df_new['RealPrice'] * delta(df_new, df_old, ['ManExercOptionsMW'], ConvertToMWH=True) - delta(df_new, df_old, ['ManExercOptionsMW','IntrinsicStrike'], Aggregate=True, ConvertToMWH=True)).rename('IncSwaption') $ IncCaps = (df_new['CapCurve'] * delta(df_new, df_old, ['CapMW'], ConvertToMWH=True) + delta(df_new, df_old, ['CapPremium'])).rename('IncCaps')
df_usa = df1[(df1['Area'] == "United States of America")].set_index('Year') $ df_usa.head(5)
email_bool = [email in good_emails for email in users.email]
cars.loc[cars.price > 160000].count()['name'] $
full['WillBe<=30Days'].value_counts()#.mean()
calls_df.pivot_table(["length_in_sec"],["status"],aggfunc="mean").sort_values("length_in_sec",ascending=False)
items2 = [{'bikes': 20, 'pants': 30, 'watches': 35, 'shirts': 15, 'shoes':8, 'suits':45}, $ {'watches': 10, 'glasses': 50, 'bikes': 15, 'pants':5, 'shirts': 2, 'shoes':5, 'suits':7}, $ {'bikes': 20, 'pants': 30, 'watches': 35, 'glasses': 4, 'shoes':10}] $ store_items = pd.DataFrame(items2, index = ['store 1', 'store 2', 'store 3']) $ store_items
(p_diffs > prob_convert_given_treatment- prob_convert_given_control).mean() $
df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp']) $ df_clean['date'] = df_clean['timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ df_clean['time'] = df_clean['timestamp'].apply(lambda time: time.strftime('%H:%M'))
df.dtypes
var_list = ["Mkt_Cap2GDP", "Credit_2NonFinSec","Deposits_2GDP", "Household_Credit", "GDP", "Non_Financial_Credit_Ratio"] $ for var in var_list: $      new_name = var + "_growth" $      chinadata[new_name] = 100*chinadata[var].pct_change() $ chinadata.dropna(inplace = True)
df_mod[['US','UK']] = pd.get_dummies(df_mod['country'])[['US','UK']] # 'CA' being the reference $ df_mod['country'].value_counts()
hp['som'] = pd.DatetimeIndex(np.array(hp.listdate, dtype='datetime64[M]'), freq=None) #  finding start of the month
sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)] $ sheet_data[:2]
for num in range(16): $     l.append(num**2)
client.query('select * from cpu_load_short', database='example')
polpages = ['christian.kern.spoe', 'matthias.strolz','hcstrache', $             'sebastiankurz.at','peterpilz', $             'diegruenen', 'NeosDasNeueOesterreich', 'Sozialdemokratie', $             'Volkspartei', 'listepilz.at', 'fpoe', 'wernerkogler'] $ graph = GraphAPI(access_token)
agent.load_weights(save_path)
merged = df2.merge(dfCountry, on='user_id') $ merged.head()
train.isnull().sum()
df2['entry_time'] = pd.to_datetime(df2['entry_time']) $ df2['help_time'] = pd.to_datetime(df2['help_time']) $ df2['exit_time'] = pd.to_datetime(df2['exit_time']) $ df2['topic_id'] = df2['topic_id'].astype(int)
wd=1e-7 $ bptt=70 $ bs=52 $ opt_fn = partial(optim.Adam, betas=(0.8, 0.99))
temps_df.Missouri
my_person.__str__()
df_sale_price=df_sale_price.loc[(df_sale_price.RegionName=='Santa Clara'), ] $ df_sale_price
movie_rating=pd.merge(top100ratings,movies, on='movieId', how='inner')
mean = np.mean(data['len']) $ print("The length's average in tweets: {}", format(mean)) $
y_pred = rf_pred $ print('precision: {:.2f}\nrecall: {:.2f}\naccuracy: {:.2f}'.format(precision_score(y_test,y_pred), $                                                        recall_score(y_test,y_pred), $                                                        accuracy(y_test,y_pred)))
food["created_datetime"].head()
df2 = df2.drop(['start station id', 'start station latitude', $        'start station longitude', 'end station id', 'end station latitude', $        'end station longitude', 'bikeid', 'birth year', 'gender', 'ageM', 'ageF'], axis=1) $ df2.head(3)
max(image_predictions['tweet_id'].value_counts())
print('\nMinimum Close value:', google_stock['Close'].min())
from sklearn.svm import LinearSVC
df_transactions['not_auto_renew'] = df_transactions.is_auto_renew.apply(lambda x: 1 if x == 0 else 0)
df_new['intercept'] = 1 $ lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ result = lm.fit() $ result.summary()
autos = autos[autos["price"].between(200, 500000)]
datatest.loc[datatest.place_name == "Haras San Pablo",'lat'] = -34.606195 $ datatest.loc[datatest.place_name == "Haras San Pablo",'lon'] = -59.038684
store_items = store_items.rename(columns={'pants': 'hats'})
dicttagger_location = DictionaryTagger(['location.yml'])
inputPath = "train/embeddings.csv" $ embeddings = sqlContext.read.options(header='true', inferSchema='true').csv(inputPath) $ embeddings.show(5)
posts.find_one()
jobs.loc[(jobs.FAIRSHARE == 180) & (jobs.ReqCPUS == 1) & (jobs.GPU == 0) & (jobs.Group == 'mass_spec')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
def get_rating(row): $     ratings = re.findall(r'(\d\d?\d?.?\d*\/\d*\d*\d*\d)', row.text) $     final_rate = str(ratings[-1]) $     return final_rate
bres = opener.open(uri) $ tres = opener.open(uri + "?%s" % exportparams)
image_predictions.shape
df2 = df.drop(df[(df.group== "treatment")&(df.landing_page != "new_page")].index) $ df2.drop(df2[(df2.group != "treatment") & (df2.landing_page== "new_page")].index, inplace=True)
!du -h sample_submission.csv
%%time $ df_from_hdf = pd.read_hdf('store.h5', key='losses')
index = pd.Index(dates)
old_compiled_data.iloc[0]
old_page_converted = np.random.choice([1, 0], size=n_old, p=[p_old, (1-p_old)]) $ p_old_page_converted = old_page_converted.mean() $ print(p_old_page_converted)
to_be_predicted_Day1 = 33.10 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
start_date=dt.date(2017,8,1) $ end_date=dt.date(2017,8,10)
df = df.sort_values('label', ascending = True) $ df.head()
print('Out of {} organisations, {} were matched as companies.'.format( $         len(df[(df.company == False) & (df.other == False)]), $         len(df[(df.is_ch_company == True) & (df.company == False) & (df.other == False)])))
for cell in openmc_cells: $     for rxn_type in xs_library[cell.id]: $         xs_library[cell.id][rxn_type].load_from_statepoint(sp)
wine_reviews = pd.read_csv("D:\kagglelearn\kaggledatasets\winemag-data-130k-v2.csv", index_col=0) $ wine_reviews.head()
tweet_archive_clean.tweet_id = tweet_archive_clean.tweet_id.astype(str) $ tweet_archive_clean.in_reply_to_status_id = tweet_archive_clean.in_reply_to_status_id.astype(str) $ tweet_archive_clean.in_reply_to_user_id = tweet_archive_clean.in_reply_to_user_id.astype(str)
from nltk.sentiment.vader import SentimentIntensityAnalyzer
tickers = portfolio_df['Ticker'].unique() $ tickers
twitter_archive_full[twitter_archive_full.name.isin(invalid_name_list)][['tweet_id','name','text']]
location = loc_date_str.split(',')[0].strip() $ location
run txt2pdf.py -o '2018-06-22 2015 FLORIDA HOSPITAL Sorted by discharges.pdf'  '2018-06-22 2015 FLORIDA HOSPITAL Sorted by discharges.txt'
import findspark $ findspark.init('/usr/local/spark/') $ import pyspark
all_data = sales.merge(targets).merge(men_women) $ print(all_data)
c.execute("UPDATE cities SET state='Californ-I-A' WHERE state='CA'") $ conn.commit() $ print(c.fetchall())
r = regex.compile('('+a+'){e<=3}') $ if r.match(b): $     print "match" $ else: $     print "don't match"
df_1 = pd.read_csv('bitcoin_data_one_minute_2016_2017.csv', parse_dates = ['timestamp']).set_index('timestamp')
pre_discover_sales = sales_data_clean[sales_data_clean['Created at']< "2017-09-09"] $ post_discover_sales = sales_data_clean[sales_data_clean['Created at']>= "2017-09-09"] $ only_users_from_post_discover_sales = post_discover_sales[post_discover_sales['Email'].isin(discover_first['email'].unique())] $ only_users_from_post_discover_sales
df = tables[0] $ df.columns = ['Mars_planet_profile', 'Value'] $ df
mean = np.mean(df['len']) $ print("The lenght's average in tweets: {}".format(mean))
pax_raw[pax_raw.paxstep > step_threshold].tail(20)
cursor = connection.cursor() $ cursor.execute(query) $ results = cursor.fetchall() $ pd.DataFrame.from_records(results)
X_test.head()
df['age'] = df['fetched time'] - df['created_utc'] $ df['age'] = df['age'].astype('timedelta64[m]') $ df.head()
cp311["created_date"] = pd.to_datetime(cp311['created_date'],infer_datetime_format=True)
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','ca','uk']]) $ results = logit_mod.fit()
s2 = s.reindex(['a', 'c', 'e', 'g']) $ s2['a'] = 0 $ s2
dill.dump(TEXT, open(f'{PATH}models/TEXT.pkl', 'wb'))
tweet_data.head()
!h5ls -r 'data/NYC-yellow-taxis-100k.h5'
df_test_index = pd.merge(df_test_index[event_list['event_start_at'] > df_test_user['created_on']], $                             log_user1[event_list['event_start_at'] > df_test_user['created_on']], on='event_id', how='left') $ df_test_index
cols = cols[1:] + cols[:1] $ cols $ tweet_json_df = tweet_json_df[cols] $ tweet_json_df.head(2)
with TablesBigMatrixReader(output_path) as reader: $     row_indices = reader.get_row_indices('id == {}'.format(some_id)) $     data_tmp = reader[row_indices, :] $ data_tmp
feature_set = layer.query(out_sr={'wkid': 4326}) $
(df2['user_id']==290584).sum()
new_page = df2[df2.landing_page =='new_page'].count()['user_id'] $ prob_new = new_page/df2.count()['user_id'] $ prob_new
total_user_count = ab_df['user_id'].nunique() $ converted_user_count = ab_df.query('converted == 1')['user_id'].count() $ converted_proportion = converted_user_count / total_user_count $
y_fit = rf.predict_proba(test_data_features) $ report_score(y_test, y_fit, threshold = 0.2, proba=True)
for article in articles: $     footer = article.find('footer') $     featured_image_url = 'https://www.jpl.nasa.gov' + footer.a['data-fancybox-href'] $     print (featured_image_url)
Raw_Forecast["ID"] =  Raw_Forecast.Date_Monday.astype(str)[:] + "/" + Raw_Forecast.Product_Motor + "/" + Raw_Forecast.Part_Number.str[:] $ Raw_Forecast.head(20)
test_bow.shape, test_tfidf.shape
with open('youtube_urls.json', 'r') as fp: $     youtube_urls = json.load(fp)
if not os.path.exists('new_data_files'): $     os.mkdir('new_data_files') $ records3.to_csv('new_data_files/Q3C.csv')
print(df.Opened.min()) $ print(df.Opened.max())
key = 'recordio-pb-data' $ boto3.resource('s3').Bucket(bucket).Object(os.path.join(dist, key)).upload_fileobj(buf) $ s3_train_data = 's3://{}/{}{}'.format(bucket, dist, key) $ print('uploaded training data location: {}'.format(s3_train_data))
ad_groups_zero_impr.groupby( $     ['CampaignName', 'Date'] $ )
df.tail()
autos['year_of_registration'].value_counts(normalize = True).sort_index()
adopted_cats.loc[adopted_cats['Point']==1,'mix_color'] = 1
all_data_merge.shape
tree_labels = [] $ for tree in train_trees: $     for n in tree: $         if isinstance(n, nltk.tree.Tree): $             tree_labels.append(n.label())
import requests $ from collections import namedtuple, OrderedDict, Counter $ import pprint $ pp = pprint.PrettyPrinter(indent=2)
yt.get_video_comments(video_id[0])[:2]
df_vow['Year'] = df_vow.index.year $ df_vow['Month'] = df_vow.index.month $ df_vow['Day'] = df_vow.index.day
frame2 = pd.DataFrame(data, columns=['year','state', 'pop', 'debt'], $                      index=['one', 'two', 'three', 'four','five', 'six'])
df2['converted'].mean() $ print("{:.2%}".format(df2['converted'].mean()))
!dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb
import numpy as np $ print(np.corrcoef(ages,weights)) $ print(np.corrcoef(ages,weights)[0,1])
import exampleModule # This has one function called ex $ exampleModule.ex([4,5,6])
!wget --load-cookies /tmp/cookies.txt "https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?id=1LAiELvs_FQhZuldX7JPMutfX00NzhonF&export=download' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1LAiELvs_FQhZuldX7JPMutfX00NzhonF" -O train.zip && rm -rf /tmp/cookies.txt
print (train[train.age < 15].age.shape) $ print (train[train.age > 80].age.shape) $ print (test[test.age < 15].age.shape) $ print (test[test.age > 80].age.shape)
img_title = hems_soup.find_all("h3") $ img_titles = [] $ for image in img_title: $     img_titles.append(image.get_text()) $ img_titles
data.text.str.contains("sandwich").resample("1T").sum().plot()
df.set_index(['Date', 'Store', 'Category', 'Subcategory', 'Description'], inplace=True) $ df.head(3)
results = nfl.interest_over_time() $
result = cur.fetchall() $
model.most_similar("economy")
experimentp = df2[df2['group'] == 'treatment']['converted'].mean() $ experimentp
train.sample(5)
psy_native = psy_df5 $ psy_native = psy_native.set_index('subjectkey', verify_integrity=True)
twitter_Archive.head(15)
pd.isnull(obj4)
pd.Timestamp('2012-05-01')
stock['target'] = stock.close.shift(-1)
df_clean.head(1)
top_songs['Country'].isnull().sum()
print(json.dumps(experiment_details, indent=2))
data = df[columns] $ data = data.reset_index(drop=True)
mod.model.n_features
model_sm.summary()
cp311.head(3)
data.name.isnull().sum()
fm_confident_under = fm_confident_under.rename('bet_won_under_pred')
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger') $ 'Z-score: {}, p-value: {}'.format(z_score, p_value)
f.dimensions
pd.merge(df1,df3,how='right')
model = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ results = model.fit() $ results.summary()
assert pd.notnull(ebola).all().all()
merged2.index
list = [] $ lat = (festivals.at[1,'latitude']) $ long = (festivals.at[1,'longitude']) $ list.append((lat, long)) $ len(list)
print(d['entries'][0]['title']) $ print(d.entries[0]['link'])
day_access = log_with_day.groupBy('dayOfWeek').count() $ day_access.show()
city_data1 = "city_data.csv" $ ride_data1 = "ride_data.csv" $ city_data_df = pd.read_csv(city_data1) $ ride_data_df = pd.read_csv(ride_data1) $ ride_data_df.head(5) $
df.head()
Results_kNN500x.to_csv('soln_kNN500_extra.csv', index=False)
times = pd.to_datetime(weather_yvr['Datetime'])
df_vow['Date'] $ type(df_vow['Date'].loc[0])
sel = [Measurements.tobs] $ temperature_data = session.query(*sel).\ $     filter(Measurements.date >= initial_date).\ $     filter(Measurements.station == highest_station).all() $
df_NOTCLEAN1A['AGE'].head(20)
autos = autos[autos["registration_year"].between(1900,2016)] $ autos["registration_year"].value_counts(normalize=True)
merged2 = pd.merge(dataset, doi_pid, how="left", left_on="fixed_doi", right_on="DOI") $ merged2.head(3).T
wrd_clean['expanded_urls'].str.contains('https://gofundme.com/').sum()
df_new['intercept'] = 1 $ lm = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','CA','US']]) $ results = lm.fit() $ results.summary()
most_common_dog = df_merge.p1.value_counts().head() $ most_common_dog
cold_tweets.head()
d_stream_keywords = pd.read_json('Twitter_SCRAPING/keyword_tweets.json', lines=True) $ tweets_l_stream_keywords = d_stream_keywords['text'].tolist() # create a list from 'text' column in d dataframe $ print(tweets_l_stream_keywords[-1:])
df_kws = df.set_index('datetime').resample('1m').aggregate('sum')
outlier_detection.basic_cutoff
taxiData.head(10)
print(np.shape(a)) $ print(np.size(a))
users.merge(sessions, how='inner', left_on=['UserID','Registered'],right_on=['UserID','SessionDate'])
combined_df.to_csv('kaggle_results.csv', index=False)
le_data_all = wb.download(indicator="SP.DYN.LE00.IN", $                          country = countries['iso2c'], $                          start='1980', $                          end='2012') $ le_data_all
pd.merge(df_a, df_b, on='mathdad_id', how='outer')
print example1_df.printSchema()
import seaborn as sns $ import matplotlib.pyplot as plt $ %matplotlib inline
archive_clean.to_csv('archive_clean.csv') $ images_clean.to_csv('images_clean.csv') $ popularity_clean.to_csv('popularity_clean.csv')
df4.fillna(value = 6)
df_clean.info()
miss_grp1 = df.query("group =='treatment' and landing_page == 'old_page'") $ print('The number of times a user from the treatment group lands on the old page is {}'.format(len(miss_grp1)))
df2[(df2['group']=='treatment') & (df2['converted']==1)].converted.sum()/df2[df2['group']=='treatment' ].group.count()
beijing['WindDirDegrees'] = beijing['WindDirDegrees'].str.rstrip('<br />')
datafile = "mlab_dtp_data_mlabnetdb.csv" $ df.to_csv(datafile)
weather.hail.value_counts()
len(calls_nocontact.location.unique())
df2.columns
%%time $ grid_svc.fit(X, y)
p_control = df2.query('group == "control"').count()['user_id']/df2.count()['user_id'] $ p_control_converted = df2.query('group == "control" & converted == 1').count()['user_id']/df2.query('group == "control"').count()['user_id'] $ print('The probability of someone converting given that an individual was in the control group, disregaring the page he/she visited is {:2f}'.format(p_control_converted))
tweet_data_df.to_csv("tweet_data_df.csv",sep =",")
tweets["created_at"] = tweets["created_at"].astype("datetime64")
dataframe.columns
from sklearn.model_selection import train_test_split
orders_subset = orders[orders.user_id.isin(selected_users)]
df['duration'] = np.round((df['deadline'] - df['launched']).dt.days / 7)
print(train['first_device_type'].unique())
plt.plot(ticks[train_size+seq_len:], test_plot)
df['Complaint Type'].resample('M').count().plot()
digits.images[200], digits.target[200]
! cat readme.html | wc -l
std_for_each_weekday['day_order'] = pd.Series(lst, index=std_for_each_weekday.index) $ std_for_each_weekday
logit_mod = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']]) $ results = logit_mod.fit()
emoji_tokens = word_tokenize(text.lower()) $ print(emoji_tokens)
image_predictions_df[image_predictions_df['tweet_id'].duplicated()]
twitter_archive_enhanced.describe()
pyautogui.FAILSAFE=True
data = pd.concat([data, df_cat], axis=1)
dcAutos=cleanedAutos.dropna() $ dcAutos.isnull().sum()
z=loan_fundings1 $ z[(z.fk_loan==3) & (z.fk_user==38)]
fulldata_copy = pd.DataFrame(fulldata_copy)
house_data['price'].hist(bins = 20)
import types $ print([f for f in globals().values() if type(f) == types.FunctionType])
df = pd.read_csv('Crimes_-_2001_to_present.csv')
data_test.head()
IMDB_df = train_df.append(test_df)
len(df.user_id.unique())
video_id = df['video_id'].tolist() $ video_id[:2]
information_ratio[['Manager_A', 'Manager_B']] * 1.5
WHO_Region = np.array(df['WHO Region']) $ WHO_Region
%time df.to_hdf(target, '/data')
import seaborn as sns $ import matplotlib as mpl $ import matplotlib.pyplot as plt $ %matplotlib inline
data_archie = data_archie[data_archie.cur_purchase_price != 0.0]
objects = Graph(user = 'neo4j', password = 'hello') $ objects.delete_all()
def proportion(col): $     return col / col.sum() $ clinton_pivoted['aides_proportion'] = proportion(clinton_pivoted['False']) $ clinton_pivoted['clinton_proportion'] = proportion(clinton_pivoted['True']) $ clinton_pivoted[['aides_proportion', 'clinton_proportion']].plot.barh()
Grouping_Year_DRG_discharges_payments.xs(2011, level='year').head()
df_master_select = df_master.copy() $ df_master_select = df_master_select[merge_features]
lr2.score(new_x, new_y) # the result hasn't improved
df2[df2.user_id.duplicated(keep=False)]
train_view.sort_values(by=1, ascending=False)[0:10]
np.random.lognormal(mean=0, sigma=1, size=5)
musk = pd.read_csv('elonmusk_tweets.csv')
sean = relevant_data[relevant_data['User Name'] == 'Sean Hegarty'] $ sean['Event Type Name'].value_counts().plot(kind='barh')
dod_change = [] $ for i in range(len(data) - 1): $     dod_change.append(abs(data[i][4] - data[i+1][4])) $ largest_change = round(max(dod_change),2) $ print('Largest day-over-day change: ', largest_change)
Google_stock['Open'].min()
naive_bayes_classifier.show_most_informative_features(15)
support.head()
sorted_m3 = np.sort(m3,axis=None)[::-1].reshape(m3.shape) $ print("sorted m3: ", sorted_m3)
dfEPEXpeak = dfEPEXbase[(dfEPEXbase.index.hour >= 8) & (dfEPEXbase.index.hour < 20)] $ dfEPEXpeak.head(15) # verify extracted data only contains 09:00 to 20:00
df.groupby(['drg3','year']).quantile([0,.25,.5,.75,1]) $ abc = df.groupby(['drg3','year']).describe()
def clean_tweet(tweet): $     return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())
from keras.preprocessing.text import Tokenizer $ import numpy as np
ax = commits_per_hour.plot.bar() $ ax.set_title("Commits per Hour") $ ax.set_xlabel("Hour of Day") $ ax.set_ylabel("Number of Commits")
df.groupby("cancelled")["local_rental"].mean()
tokens = [] $ for t in nltk.regexp_tokenize(text.lower(), '[a-z]+'): $     if t not in sr: $         tokens.append(t) $ tokens[:10]
for col in ['Partner','Dependents','PhoneService','PaperlessBilling','Churn']: $      df_raw[col].replace({'Yes': 1, 'No': 0}, inplace=True)
tran_time_diff[tran_time_diff.msno == '++1Wu2wKBA60W9F9sMh15RXmh1wN1fjoVGzNqvw/Gro=']
time_utc = [] $ for i in range(0,l): $     yourdate = dateutil.parser.parse(time[i]) $     time_utc.append(yourdate.timestamp() - 7200) #timezone correction for 2 hours $ time_utc = np.array(time_utc) * 1000
data.to_json('nytimes_oped_articles.json')
glm_perf = glm_model.model_performance(valid = True) $ glm_perf.plot()
autos["price"].unique().shape
count = grouped_months.count() $ count=count.rename(columns = {'Totals':'count_v_T'}) $
len(df[~(df.group_properties == {})])
%%bash $ gsutil cat "gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*" | head
df.loc['Alabama']
training_data[numericFeatures].head()
tweetering['polarity'] = tweetering.apply(lambda x: TextBlob(x['Text']).sentiment.polarity, axis=1) $ tweetering['subjectivity'] = tweetering.apply(lambda x: TextBlob(x['Text']).sentiment.subjectivity, axis=1) $ tweetering = tweetering.sort_values('Text', ascending=False).drop_duplicates(['Text']) $ tweetering.head()
tmp_df.columns.sort_values().values
df_clean = df.copy()
country = pd.read_sql_query('select * from Country', conn)  # don't forget to specify the connection $ print(country.shape) $ country.head()
g_sorted.head(10)
Train.fillna(0, inplace=True) $ Test.fillna(0, inplace=True)
sub_df.to_csv("xgb_unconverged.csv", index=False)
d = pd.datetime(2018, 2, 27, 9, 0)
countries = pd.read_csv('countries.csv') $ new = df2.set_index('user_id').join(countries.set_index('user_id'))
constants = pd.read_csv('control_chart_constants.csv') $ subgroup_size = len(df.columns) - 1 $ d_two = constants['d2'][constants['n'] == subgroup_size].values[0] $ d_three = constants['d3'][constants['n'] == subgroup_size].values[0]
b = 2  # a file level variable $ def my_function(): $     b = 20  # local commands cannot change variables from higher environments. $ my_function() $ print("file_b: " + str(b))
import inspect $ sig=inspect.signature(tag) $ str(sig)
df.to_json("json_data_format_values.json", orient="values") $ !cat json_data_format_values.json
old_page_converted = df_control_and_old_page.sample(no_of_samples_control) $ p_old = old_page_converted.converted.mean() $ p_old
austin['driver_rating'] = austin['driver_rating'].apply(lambda x: round(x)) $ driver_rat = austin['driver_rating'].value_counts().sort_index() $ print(driver_rat)
zipped = zip(selected_feature_updated[1:], model.feature_importances_[1:] ) $ zipped_dict = dict(sorted(zipped, key=lambda x: -x[1])[:3]) $ list(zipped_dict.keys())
df.num_comments.max()
conn = sqlite3.connect('daily_discharge.db') $ cur = conn.cursor()
for c in df_test_supplement.columns: $     print(c, df_test_supplement[c].dtype)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
df.rename(columns={'Indicator':'Indicator_ID'},inplace=True) $ df.info()
df2['as_date'] = pd.to_datetime(df2['timestamp'])
aapl = pd.io.data.Options('AAPL', 'yahoo') $ data = aapl.get_all_data() $ data.iloc[0:6, 0:4]
n_old = df2.query('group == "control"').shape[0] $ n_old
result1 = (df1 < df2) & (df2 <= df3) * (df3 != df4) $ result2 = pd.eval('df1 <df2 <= df3 != df4') $ np.allclose(result1, result2)
plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101_d_b[df_nd101_d_b['ud120']>0].ud120)
DataSet = DataSet[DataSet.userTimezone.notnull()] $ len(DataSet)
reddit.user.me() $ subreddit = reddit.subreddit('tifu') $ top_subreddit = subreddit.top(limit=1000)
rsskb_gbm_best.feature_importances_
averagerank = rankings_USA.groupby('year')['rank'].mean() $ averagerank.plot(y='rank', x='year') $
data_final = ben_out.append(van_out)
autos["price"] = autos["price"].str.replace(",","").str.replace("$","").astype(int) $ autos.rename({"price":"price_dollars"},axis=1,inplace=True) $ autos["odometer"]=autos["odometer"].str.replace(",","").str.replace("km","").astype(int) $ autos.rename({"odometer":"odometer_km"},axis=1,inplace=True)
plt.hist(null_vals); $ plt.axvline(x=obs_diff, color='red')
df.info()
dep = 'priceClose' $ joined = joined[cat_vars+contin_vars+[dep, 'timePeriodStart']].copy() $
forest = RandomForestClassifier(max_depth = 10, n_estimators=5, random_state=42) $ forest.fit(X_train, y_train) $ forest.score(X_test, y_test)
raw_data = '/s3/three-word-weather/hack/3ww-all-raw.csv'
control = df2.query('group == "control"') $ converted = control.query('converted == 1') $ p_control_convert = converted.count()[0]/control.count()[0] $ p_control_convert
tmp = df.corr(method = 'pearson')[['meantempm']] $
fh_2 = FeatureHasher(num_features=uniques.iloc[1, 1], input_type='string', non_negative=True) $ %time fit2 = fh_2.fit_transform(train.device_id)
trainheadlines = train["text"].values $ basicvectorizer = CountVectorizer() $ basictrain = basicvectorizer.fit_transform(trainheadlines) $ print(basictrain.shape)
h = pd.Period('2017-08-15 23:00:00',freq='H') $ h
print(r.json()['dataset_data']['column_names'])
df_survival.sort_values(by=['Donor ID', 'Donation Received Date'], inplace=True) $ df_survival.loc[:,'Time between donations'] = df_survival.groupby('Donor ID')['Donation Received Date'].diff().fillna(0)
old_page_converted = np.random.binomial(1,p_old,n_old) $ old_page_converted.mean()
plot_df = telemetry.loc[(telemetry['machineID'] == 1) & $                         (telemetry['datetime'] > pd.to_datetime('2015-01-01')) & $                         (telemetry['datetime'] < pd.to_datetime('2015-02-01')), ['datetime', 'volt']]
plt.rcParams['figure.figsize'] = (15, 5) $ tzs.plot(kind='bar') $ plt.xlabel('Timezones') $ plt.ylabel('Tweet Count') $ plt.title('Top 10 Timezones tweeting #puravida')
loansvm = svm.SVC(kernel='rbf') $ loansvm.fit(X, y) 
cm10 = ConfusionMatrix(y10_test, predicted10) $ cm10.print_stats()
treatment_df = df2.query('group == "treatment"') $ treat_cr = treatment_df.converted.mean() $ treat_cr
properati['place_name'].value_counts(dropna=False)
df_new = pd.merge(df2,countries,on = 'user_id') $ df_new.head()
brands = brand_counts[brand_counts > 0.05].index $ brands
datetime.now().strftime('It is %H:%M, on %h %d, %Y.')
n_old=df2.landing_page.value_counts()[1] $ n_old
%time train = pd.read_csv("../assets/trainaa") $ train.head(1)
customer_visitors_new.index.levels
df_group = df.groupby(['Ranking URL on Sep  1, 2017', 'Market']).agg({ $     'Keyword': 'count', $     'Rank': 'mean' $ }).reset_index()
df.shape
df_pr.sort_values('sentiment', ascending=False).iloc[:10]
def unique(df): $     return df.id.nunique() $ print(unique(ltc)) $ print(unique(eth)) $ print(unique(xrp))
X = X[feature_names] $ X = X.apply(pd.to_numeric)
auth = tweepy.OAuthHandler(credentials.api_key, credentials.api_secret) $ auth.set_access_token(credentials.token_key, credentials.token_secret) $ api = tweepy.API(auth)
squares.values 
pd.Series(data=predicted10).value_counts()
countries = wb.get_countries() $ countries.iloc[0:10].ix[:,['name', 'capitalCity', 'iso2c']]
autos["registration_year"].describe()
country = pd.read_csv('countries.csv') $ country.head()
lagged = mr.shift(1) $ lagged.head()
aliases = [b for b in BID_PLANS_df.index if '\n' in b]
rGraphData.groupby(['from', 'to']).sum().tail()
from sklearn.preprocessing import MinMaxScaler $ scaler = MinMaxScaler() $ scaler.fit(X_train) $ scaled_X_train = scaler.transform(X_train) $ X_train = pd.DataFrame(scaled_X_train, columns=X_train.columns)
time.mktime(first_time.timetuple())
brazil.plot()
def is_clinton(tweet): $     text = extract_text(tweet) $     return text.endswith("-H") $ clinton_df["is_personal"] = [is_clinton(clinton_tweets[i]) for i in range(len(clinton_tweets))] $ clinton_df
def drawTrainingPlot(study_area, scene_num, covertype, scene_picks_arr): $     ax1, scene_picks_arr = drawTrainingScene(study_area, scene_num, covertype, scene_picks_arr) $     plt.draw() $     return scene_picks_arr
top_5_percent = autos_pr['brand'].value_counts()[autos_pr['brand'].value_counts(normalize = True) > 0.05]
containers[0].find("div",{"class":"key"}).a['title'].split()[1].replace(',',"")
info.describe()
temp_freq_df.plot(x="date",y="Precipitation",kind="bar",ax=None,legend=True, $                      title="Hawaii - Temperature  vs Frequency ") $ plt.show()
greater_than_10 = twitter_archive[twitter_archive['rating_denominator']>10] $ for idx in range(len(greater_than_10)): $     observation = greater_than_10.iloc[idx] $     print("tweet id: {}\n rating: {}/{}\n text: {}".format(observation['tweet_id'],observation['rating_numerator'],observation['rating_denominator'],observation['text']))
df_loan2 = df_loan1.merge(loans[['fk_loan','payback_state']], $                                           on='fk_loan')
plt.plot(scores); plt.title("Scores");
week43 = week42.rename(columns={301:'301'}) $ stocks = stocks.rename(columns={'Week 42':'Week 43','294':'301'}) $ week43 = pd.merge(stocks,week43,on=['301','Tickers']) $ week43.drop_duplicates(subset='Link',inplace=True)
X_train = X_train.loc[:,[col for col in X_train.columns if col not in ['Footnote','CountyName','Month']]]
daily = data.set_index('created_at').resample('D').size() $ monthly_mean = daily.resample('M').mean() $ monthly_mean.index = monthly_mean.index.strftime('%Y/%m') $ iplot(monthly_mean.iplot(asFigure=True, dimensions=(750, 500), vline=['2017/09', '2017/03', '2016/09', '2016/03', '2015/09', '2014/12', '2014/04', '2013/10']))
v_invoice_sat.drop(v_invoice_sat_dropper, axis=1, inplace=True) $ invoice_sat.drop(invoice_sat_dropper, axis = 1, inplace=True)
corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'corpus.mm'))
df1.fillna(-999999,inplace=True)
Celsius._temperature
df_2017.shape
ben_dummy.groupby(['userid'])['diffs'].mean()
df_likes = df_comment.drop('liked_by', axis=1)[['author_id']].join(s).dropna().reset_index()
df2 = df2.drop(df.index[[2893]]) $ df2.shape
fin_pivot_table = pd.merge(fin_pivot_table_tr, fin_pivot_table_ni, left_index=True, right_index=True) $ fin_pivot_table['Avg Profit Margin'] = fin_pivot_table['Avg Net Income']/fin_pivot_table['Avg Total Revenue'] $ fin_pivot_table.sort_values(by = ['Avg Profit Margin'], ascending = False)
learn.load('clas_1')
a.zipcode = a.zipcode.astype("str")
!open resources/html_table_marsfacts.html
df_valid["Died"] = pd.to_datetime(df_valid['Died'], unit="ms") $ df_valid = df_valid[df_valid["Died"] < datetime.strptime("2018-01-01", "%Y-%m-%d")]
mp2013 = pd.period_range('1/1/2013','12/31/2013',freq='M') $ mp2013
list(db.osm.aggregate([{"$match":{"amenity":{"$exists":1},"amenity":"place_of_worship"}},\ $                         {"$group":{"_id":"$religion", "count":{"$sum":1}}}]))
iris_fromUrl.head(6)
len(df2['user_id']) - len(df2['user_id'].unique())
bldg_data_0.groupby([bldg_data_0.index.month]).agg('count').head(12)
df2.drop(mismatch_treatment_to_old_pg.index, inplace=True) $ df2.drop(mismatch_control_to_new_page.index, inplace=True)
pipe = pc.PipelineControl(data_path='examples/simple/data/varying_data.csv', $                           prediction_path='examples/simple/data/predictions.csv', $                           retraining_flag=True) $ pipe.runPipeline()
categories = df['main_category'].value_counts() $ categories.plot.bar(title='Number of projects by main category'); $
movies.describe()
size_t = df2.query("group=='treatment'").count()[0] $ new_page_converted =  np.random.choice([0,1], size=(size_t,1), p=[1-pn,pn]) $
output.coalesce(2).write.parquet("/home/ubuntu/parquet/output.parquet/output.parquet")
y_test_over[fm_bet_over].mean()
tweets_master_df.info()
lm_ca = sm.OLS(df_new['converted'], df_new[['intercept_ca', 'ab-page']]) $ results_ca = lm_ca.fit() $ results_ca.summary()
monthly_medication_df.first_month_adherence.mean()
old_page_conv = np.random.binomial(n_old,p_old) $ old_page_conv
sns.set_style("darkgrid") $ plt.figure(figsize=(8, 4)) $ errors['errorID'].value_counts().plot(kind='bar') $ plt.ylabel('Count')
X_train.shape[1]
val requests = my_stream $     .map(line => (line.split(" ")(2), 1)) $     .reduceByKey((x, y) => x + y)
df["three_day_reminder_profile_incomplete_sent"].value_counts()
groceries.loc[['eggs', 'apples']]
df['SOCIAL_NETWORKS'] = df.text.apply(lambda text: pd.Series([x in text for x in SOCIAL_NETWORKS]).any()) $ df['DECISION_MAKING']  = df.text.apply(lambda text: pd.Series([x in text for x in DECISION_MAKING]).any()) $ df['ADAPTIVE_CAPACITY']  = df.text.apply(lambda text: pd.Series([x in text for x in ADAPTIVE_CAPACITY]).any())
time_df = pd.DataFrame(en_test_df[['created_at']]) $ time_df = time_df.reset_index(drop=True) $ time_df.head()
train.info()
word_freq_df = word_freq_df.reset_index()
autos['last_seen'].str[:10].value_counts(normalize=True, dropna=False).head(10)
results.summary() 
df.info() #.Additional check to note the null values. 
btc_google = my_cryptory.get_google_trends(kw_list=['bitcoin']).merge(my_cryptory.extract_coinmarketcap('bitcoin')[['date','close']], on='date', how='inner') $ btc_google
retweets = megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")] $ megmfurr_tweets[megmfurr_tweets['text'].str.contains("RT")]['text'].count() # 1,633
speeches_metadata.reset_index(inplace = True)
print('The proportion of users converted:',df['converted'].mean())
repos = pd.read_pickle('data/pickled/new_subset_repos.pkl')
df_tsv.info()
logmod = sm.Logit(ab_df2['converted'], ab_df2[['intercept','ab_page']]) $ results = logmod.fit()
rdg2 = Ridge(alpha=5) $ rdg2.fit(train_data, train_labels)
inter = mr * vol $ inter = inter.between_time('9:30', '16:00') $ lagged_inter = inter.tshift(1, 'min').between_time('9:30', '16:00') $ pd.ols(y=mr, x=lagged_inter)
goog.index = goog.index.to_datetime()
data2.dtypes
dataset["article_publisher_id"].unique().size - 1 # -1 to remove the empty
len(segmentData.opportunity_owner_id.unique())
serc_pixel_df.head(5)
pd.value_counts(ac['If No DR, Why?'])
logit_country = sm.Logit(df3['converted'], df3[['intercept','country_US', 'country_UK']]) $ results2 = logit_country.fit()
from sklearn.cluster import DBSCAN
X = zone_train.drop('zone', axis=1) $ y = zone_train['zone'] $ X_train, X_test, y_train, y_test = train_test_split(X, y, $                                                     test_size= 0.10, $                                                     random_state=1)
mobile.filter(lambda p: len(p['adapters'])> 1).take(1)
test_stemmed = test_cleaned.apply(lambda sequence: ' '.join(stemmer.stem(word) for word in sequence.lower().split()))
df['state'] = df['state'].str.capitalize() $ df.groupby('state')['ID'].count()
conn_helloDB = create_engine("mysql+pymysql://{user}:{pw}@localhost/{db}".format(user="few", pw="123456", db="HelloDB"))
df2.max()
p_new=df2.query("converted==1").shape[0]/df2.shape[0]
np.exp(result.params)
pax_raw.loc[pax_raw.paxstep > step_threshold, 'paxstep'] = np.nan $ pax_raw['paxstep'] = pax_raw.groupby('seqn').paxstep.transform(pd.DataFrame.interpolate, method='linear')
e.krige(basemap=True)
vectorizer = TfidfVectorizer(max_df=1.0, min_df=0.005, sublinear_tf=True, $                              ngram_range=(1,3), stop_words=stoplist) $ X = vectorizer.fit_transform(train) $ X.shape
mean_mileage_by_brand = {} $ for brand in brands: $     mean_mileage_by_brand[brand] = autos.loc[autos["brand"] == brand, 'odometer_km'].mean() $ mean_mileage_by_brand
all_colnames = [clean_string(colname) for colname in df_total.columns] $ df_total.columns = all_colnames $ df_total.head()
print('Data entries befor dropping:', data.shape[0]) $ data.dropna(subset=odds_W, thresh=2, axis=0, inplace=True) $ data.dropna(subset=odds_L, thresh=2, axis=0, inplace=True) $ print("-" * 40) $ print('Data entries after dropping:', data.shape[0])
cohort['percentage'] = cohort['users'] / cohort['users'].sum() $ cohort['cdf'] = cohort['percentage'].cumsum() $ cohort.head(25)
z = np.arange(10, 16) $ s = pd.Series(z, index=list('abcdef')) $
print(16 in squares.values)
nba_df["Points_Range"] = team_groups["Tm.Pts"].transform('max') - team_groups["Tm.Pts"].transform('min') $ nba_df.iloc[1:5, ]
def hours_of_daylight(date, axis=23.44, latitude=47.61): $
df.shape $
pro_table = profit_table_simple.groupby(['Year', "Month Name"]).Profit.sum().reset_index() $ pro_table
sample_new = np.random.binomial(n_new, p_new, 10000)/n_new $ sample_old = np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs = sample_new - sample_old
unique_domains.sort_values('mentions', ascending=False).head()
top50_airport = pd.read_csv('data/top50airports.csv')['IATA'].tolist() $ df = df[df['ORIGIN'].isin(top50_airport)] $ df = df[df['DEST'].isin(top50_airport)]
field_stream_occurances_df.tail()
df.to_csv('../output/releases_with_demographics.csv')
df_year_avg = pd.DataFrame(year_averages) $ df_year_avg.set_index(df_year_avg['date'], inplace=True) $ df_year_avg.head() $
d_dates = data_with_dates.groupby('created_time').sum()
gas_df = gas_df.rename(columns={'Unnamed: 0':'MONTH'}) $ gas_df.columns
df2.plot.bar()
g8_groups.mean()
data['Created Date'].tail()
breakdown[breakdown != 0].sort_values().plot( $     kind='bar', title='Russian Trolls Number of Links per Topic' $ );
sh50_df = pd.read_csv('../data/sh50.csv') $
print(twitter_archive_df.columns) $ twitter_archive_df.head()
cpq_business = pd.merge(cpq, buildings, how='inner')
pd.value_counts(doctors['AppointmentDuration'])
df2.converted.count()
band_names = [1975, 1990, 2000, 2014] $ os.environ["bandnames"] = str(band_names) $ !earthengine asset set -p "(string)bandname=$bandnames" users/resourcewatch/cit_014_built_up_areas_nodata_set_to_zero
set(stories.columns) - set(stories.dropna(thresh=9, axis=1).columns) $
df2c = df.query('group == "control" and landing_page == "old_page"')
nobel['Birth Country'] = nobel['Birth Country'].replace( $     ['Guadeloupe Island', 'Trinidad', 'Northern Ireland', 'Scotland', 'Taiwan'], $     ['France', 'Trinidad and Tobago', 'United Kingdom', 'United Kingdom', 'China'] $ )
p_mean = np.mean([p_new, p_old]) $ print("Probability of conversion udner null hypothesis (p_mean):", p_mean)
traindf['inspection_result'].value_counts()
pred7 = nba_pred_modelv1.predict(g7) $ prob7 = nba_pred_modelv1.predict_proba(g7) $ print(pred7) $ print(prob7)
def losses(team, month, day, year): $     team = teamtables[team] $     team = team[team["Date"]<datetime.datetime(year,month,day)] $     return team["Losses"][len(team["Losses"])-1]
p_treatment_converted = df2[df2['group'] == 'treatment']['converted'].mean() $ print('The probability of an individual in the treatment group converting: ', p_treatment_converted)
continentdictallcap = {key.upper():continentdict[key] for key in continentdict} $ continentdictallcap
Y, W = dmatrices('Y ~ W', data=df2) $ mod4 = sm.OLS(Y, W) $ res4 = mod4.fit() $ res4.summary2()
sb.heatmap(components1)
columns = inspector.get_columns('measurement') $ for column in columns: $     print(column["name"], column["type"])
df.Date
pd.date_range('2014-08-01 12:10:01',freq='S',periods=10)
primitives[primitives['type'] == 'transform'].head(10)
from biopandas.mol2 import PandasMol2 $ pmol = PandasMol2() $ pmol.read_mol2('./data/1b5e_1.mol2') $ pmol.df.tail(10)
cfg_fnames = list(glob.glob('test-data/excel_adv_data/sample-xls-case-multifile*.xlsx'))
doc_duration = doc_duration.resample('W-MON').sum() $ RN_PA_duration = RN_PA_duration.resample('W-MON').sum() $ therapist_duration = therapist_duration.resample('W-MON').sum()
perc = lambda x: x/x.max() * 100 $ perc_df = cryptos.set_index('name')[['24h_volume_usd', 'market_cap_usd', 'price_btc']].apply(perc) $ perc_df.head()
train.head()
from IPython.display import IFrame $ IFrame('readme.html', width=800, height=450)
year_ago = dt.date(2017,8,23)-dt.timedelta(days=365) $ year_ago 
c.execute('SELECT city FROM weather where cold_month = "January" LIMIT 2') $ print(c.fetchall())
airport_count = faa_data_pandas['AIRPORT'].value_counts() $ print(airport_count)
df_a.join(df_b, on = "employee_num") # join on index and column 
replace_set = stopwords.words('english') $ demo = demo.str.split(' ').apply(lambda x: ' '.join(k for k in x if k not in replace_set)) # texts with no stop-words $ demo
from sklearn import linear_model $ from sklearn.model_selection import GridSearchCV $ logistic = linear_model.LogisticRegression() $ print('LogisticRegression score: %f' $       % logistic.fit(x_train[:,:,0], y_train).score(x_val[:,:,0], y_val)) $
% run Dropbox/Python/jupyter-blog/content/tweet_dumper.py
total_rows_in_control = (df2['group'] == 'control').sum() $ rows_converted = len((df2[(df2['group'] == 'control') & (df2['converted'] == 1)])) $ rows_converted/total_rows_in_control
print ('Polynomial Mean:\n\n',training_pending_ratio.mean())
sample = pd.Series(weather.TMAX.sample(frac=0.1, replace=False)) $ sample.mean()
df[['on_1b', 'on_2b', 'on_3b', 'man_on_first', 'man_on_second', 'man_on_third', 'men_on_base']].head()
reduced_trips_data.shape # not much outliers removed as data is not normally distributed 
with pd.option_context("use_inf_as_null", True, $                        "max.rows", 15): $     print(ratio.dropna())
all_preds = np.stack([m.predict(X_test, batch_size=256) for m in models])
old_converted.mean() - new_converted.mean()
assert trump['text'].loc[884740553040175104] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'
final_grades["hobby"] = ["Biking", "Dancing", np.nan, "Dancing", "Biking"] $ final_grades
np.exp(result.params)
git_log['timestamp'] = pd.to_datetime(git_log['timestamp'], unit='s') $ git_log['timestamp'].describe()
twitter_archive_master = twitter_archive_master.drop(twitter_archive_master[twitter_archive_master['rating_numerator'] <= 10].index)
pd.melt(df, id_vars=['A'], value_vars=['B'])
!wget https://images.pexels.com/photos/635529/pexels-photo-635529.jpeg
q_mine = c.submit_query(PTOQuerySpec().time("2018-06-10", "2018-06-12").set_id(0x6) $                                       .condition("ecn.negotiation.*") $                                       .group_by_condition())
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=870) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
stocks.write.format("json").save("stocks.json")
Station.__table__
with open('dropbox/github/thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_codebook.\ $ csv', 'rU') as inputFile: $     for line in inputFile: $         line = line.rstrip().split(',') $         print(len(line))
df_links['link.domain'].value_counts().head(40)
df_DOT = df[df['Agency Name'].isin(['Department of Transportation', 'DOT'])]
df.columns.values
days = ['day1', 'day2'] $ nocol=pd.DataFrame(index=days) $ nocol
ab_file2[ab_file2.duplicated('user_id')]
News_outlets_df.head()
prop_grthan_p_diffs = len(p_diffs[np.where(p_diffs > calc_diff)]) / len(p_diffs) $ prop_grthan_p_diffs
c.execute('SELECT sum(average_high) FROM weather') $ print(c.fetchall())
new_page_converted = np.random.choice([1,0], size=n_new, p=[p_new, 1-p_new]).mean() $ new_page_converted
df_health = pd.concat(frames)
os.mkdir('code')
baseball.drop(['ibb','hbp'], axis=1)
df = pd.read_csv('data/winemag-data-130k-v2.csv')
new_dems.newDate.describe() $
pythonistas = {'hugo':'browne-anderson', 'francis':'castrp'} $ for key, value in pythonistas.items(): $     print(key, value)
m, n = X_train.shape
proj_df['Project Subject Category Tree'].value_counts()
plt.plot(ds_issm['time'],ds_issm['met_salsurf'],'r.') $ plt.title('CP03ISSM, Sea Surface Salinity') $ plt.ylabel('Salinity') $ plt.xlabel('Time') $ plt.show()
pgrid = ParameterGrid({'learning_rate': [0.01, 0.1, 0.2, 0.4] , 'n_estimators': [100, 200, 300], 'random_state': [1] $                       })
df_clean[df_clean['in_reply_to_status_id'].notnull()]
!head twitter.csv
active_station = session.query(Measurements.station,func.count(Measurements.date)) \ $              .group_by(Measurements.station)
gs = GridSearchCV(KNeighborsRegressor(), parameters, cv = 5, scoring = 'neg_mean_squared_error') $ gs.fit(X_train, y_train)
newdf.head()
filename_output = f'en-wikipedia_traffic_{PAGECOUNTS_START[:6]}-{PAGEVIEWS_END[:6]}.csv' $ filename_output
ls = pd.to_datetime([obs['ResultDateTime'] for obs in results]) $ print('Start Date: {0} | End Date: {1}'.format(ls.min().isoformat(), ls.max().isoformat()))
sales.head()
writers.groupby("Country").groups
classified = pd.read_csv("compared_sentiments_classified.csv", index_col=0) $ classified.head()
df_with_predictions = pd.DataFrame(np.c_[X_test, y_test, y_pred]) $ df_with_predictions.columns = list(acs_df.columns) + ['homeval_pred'] $ df_with_predictions.head()
nx.write_gexf(graph, 'user_graph.gexf')
df.head()
unique_users = df.user_id.nunique() $ unique_users
emails_dataframe['address'].str.split("@")
crimes['2011-06-15']['TYPE'].value_counts().head(5)
dframe_team['end_cut'] = pd.to_datetime(dframe_team['end_cut'], infer_datetime_format=True, errors='coerce') $ dframe_team['start_cut'] = pd.to_datetime(dframe_team['start_cut'], errors='coerce') $ dframe_team['Tenure'] = dframe_team['End'] - dframe_team['Start'] $ dframe_team
selected=features[features.importance>0.0001] $ selected.sort_values(by="importance", ascending=False)
talks_train.reset_index(drop = True, inplace = True)
nbar_clean.red
state_party_df['National_R_neg_ratio']['2016-08-01':'2016-08-07'].sum() / 7
twitter_df_merged.to_csv('twitter_archive_master.csv')
tweets_raw = tweets_raw.drop(axis= 1, labels=  ["id", "user_name"])
pd.DataFrame(recommended_vids)
df_CLEAN1A['AGE'].max()
import sklearn.linear_model as lm
adf = df.reset_index() $ adf['Date'] = pd.Series({0: 'December 1', 2: 'mid-May'}) $ adf
frame = pd.DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=list('dabc'))
df.head()
test_orders_prodfill_finale.to_csv("submission2.csv", encoding='utf-8',index=False)
results = Counter() $ before['text'].str.lower().str.split().apply(results.update) $ for w in stop_words: $     del results[w] $ results.most_common()
data.head()
p_old=df2.converted.mean() $ p_old
df2_after_df1 = df1.append(df2) $ df2_after_df1
import pandas as pd $ url = 'http://www.sv.uio.no/english/research/phd/summer-school/courses-2017/' $ courses = pd.read_html(url)
data2['len'] = np.array([len(tweet.text) for tweet in lista]) $ data2['Likes'] = np.array([tweet.favorite_count for tweet in lista]) $ data2['RTs'] = np.array([tweet.retweet_count for tweet in lista]) $ data2.head()
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Mea = Base.classes.measurements $ Sta = Base.classes.stations
print('sorting an already sorted list:') $ %time L.sort()
stop_words = set(stopwords.words('english')) $ stop_words.update(['-', '=', '+', '*','.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) $ for doc in document: $     list_of_words = [i.lower() for i in wordpunct_tokenize(doc) if i.lower() not in stop_words] $ stop_words.update(list_of_words)
z_score, p_val = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative = 'smaller') $ ( 'p-value:', p_val, 'z-score:', z_score)
Successful_model_log = LogisticRegression(C=1.0, solver='newton-cg', n_jobs=-1) $ Successful_model_log.fit(X, y_encode)
df.groupby('status')['MeanFlow_cms'].describe(percentiles=[0.1,0.25,0.75,0.9])
model_128_id = models_df.loc[models_df['name'] == model_128_name, 'classifier_id'].values[0] $ model_129_id = models_df.loc[models_df['name'] == model_129_name, 'classifier_id'].values[0] $ model_130_id = models_df.loc[models_df['name'] == model_130_name, 'classifier_id'].values[0]
sns.countplot(data=data, x='OutcomeType',hue='SexuponOutcome', palette="Set3")
X.shape
query = session.query(Measurement) $ rain = query.order_by(Measurement.date.desc()).all() $ print(rain[0].date)
print(autos.columns)
temp = df.groupby('street_block')['street_block'].count() $ parking_count = pd.DataFrame({'Block': np.array(temp.keys()), 'parkings_by_block': temp.values}).set_index('Block') $ parking_count.head()
print('Number of rows in joined = {}'.format(joined.CustomerID.count())) $ joined.head()
plt.figure(figsize=(12,4)) $ sb.boxplot("share_count", data=fb) $ plt.xlim(0,1000)
distinct_user_values = df.nunique()['user_id'] $ print("Unique users are : {}".format(distinct_user_values))
logit_stats = sm.Logit(df['converted'],df[['intercept','treatment']])
train[ ['CompetitionOpenSinceYear','CompetitionDaysOpen','CompetitionMonthsOpen','CompetitionOpenSinceMonth']].sort_values('CompetitionOpenSinceYear')
df.resample('3D').mean()
print(data['class'].mean()) $ print(data.shape)
b.loc[1:7]
stocks_happiness=pd.merge(happiness_df,stocks_df,on='dates',how='outer')
merged_df.fillna('unknown', inplace=True) $ merged_df.rename(columns={'GROUP':'Group'}, inplace=True) $ merged_df.head()
clf = svm.SVC(kernel='linear', C = 1.0) $ clf.fit(X, y)
df_repub['created_at'].dtypes
df_ad_airings_4['ad_duration_secs'] = (df_ad_airings_4['end_time'] - df_ad_airings_4['start_time']).map(duration)
with pd.option_context('max.rows', 15): $     print(result["Fail"].sort_values(ascending=False)) $
chefdf.to_csv('exports/trend_data_chefkoch.csv')
summary = Logit(y, X).fit().summary() $ summary
clean_stations = pd.concat([stations, stat_info_merge], axis=1)
df2['intercept']= 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']
x = ['monetary', 'frequency', 'recency','total_cust', 'join_days', 'total_profit'] $ data = df_final[x] $ data.head()
lm.summary()
p_diff_obs=new_page_converted.mean() - old_page_converted.mean() $ p_diff_obs
from geopy.geocoders import Nominatim $ geolocator = Nominatim() $ location = geolocator.reverse("40.735806, -73.985437") $ print(location)
baseball.mean()
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ results = mod.fit() $ results.summary()
df.tail()
df_clean3.loc[189, 'text']
tmp = possible_bots.groupby(['followers_count']).size().reset_index() $ tmp.rename(columns={0:'nusers'},inplace=True) $ ax = sns.scatterplot(data=tmp,y='nusers',x='followers_count') $ ax.set_yscale('log')
students.tail()
keras_entity_recognizer.get_step_params_by_name("learner")
p_new = df2.query('landing_page == "new_page"').converted.mean() $ p_old = df2.query('landing_page == "old_page"').converted.mean() $ p_mean = np.mean([p_new, p_old]) $ p_mean
aapl_opt.loc[:, 'Expiry']
old_converted_simulation = np.random.binomial(n_old, p_old, 10000)/n_old $ old_converted_simulation.mean()
throwaway_df = sqlContext.createDataFrame([('Anthony', 10), ('Julia', 20), ('Fred', 5)], ('name', 'count')) $
df_twitter_archive_copy.stage.value_counts()
session.query(measurement.station, func.count(measurement.tobs))\ $ .group_by(measurement.station)\ $ .order_by(func.count(measurement.tobs).desc()).all() $
diff = new_page_converted.mean()- old_page_converted.mean() $ diff
autos = pd.read_csv('autos.csv',encoding = 'Latin-1')
twitter_master1.shape
ts.index
df2 = df2.drop_duplicates(subset='user_id') $ df2.shape[0]
input_data['ordered_weekday'] = input_data.week_day.apply(reorder_weekday)
combined_array = np.concatenate([training_active_listing_dummy, training_pending_ratio],axis=1) $ print ('Shape of the Combined Array: \n', combined_array.shape)
bacteria_data.ix[3]
smart_beta_tracking_error = helper.tracking_error(index_weighted_cumulative_returns, etf_weighted_cumulative_returns) $ helper.plot_tracking_error(smart_beta_tracking_error, 'Smart Beta Tracking Error')
baseball_newind.loc['gonzalu01ARI2006', ['h','X2b', 'X3b', 'hr']]
print (df.iloc[1,2])
print df2['# Of Positions'].unique() $
df_birth.population.value_counts(dropna=False).head()
df_mas['rating_numeric']=df_mas.rating_numerator/df_mas.rating_denominator
df_merge.columns
driver = webdriver.Chrome(executable_path="./chromedriver")
df[(df.landing_page == 'new_page') & (df.group != 'treatment')].user_id.count() + df[(df.landing_page != 'new_page') & (df.group == 'treatment')].user_id.count()
dic_mars_valles = { $     "title": "Valles Marineris Hemisphere", $     "img_url": img_url_valles $ } $ hemisphere_image_urls.append(dic_mars_valles)
merged=pd.merge(merged,transactions,on=['date','store_nbr'], how='left') $ print("Rows and columns:",merged.shape)
click_condition_meta['refr_source'] = np.where((pd.isnull(click_condition_meta['refr_source'])), $                                                'refr_source not available', click_condition_meta.refr_source)
dict_wells_df_and_Nofeatures_20180707 = dict_of_well_df $ pickle.dump(dict_wells_df_and_Nofeatures_20180707, open( "dict_of__wells_df_No_features_class3_20180707.p", "wb" ) )
data_donald_replies.head()
store_items = store_items.rename(index = {'store 2': 'last store'}) $ store_items
temp_series_paris = temp_series_ny.tz_convert("Europe/Paris") $ temp_series_paris
tweets.columns
df.at[dates[2],'A']
df2['sales'] = df2.new_sales + random_differences $ df2 = df2.round({'sales':2}) $ df2.head()
np.datetime64('2015-04-09') + 12
def get_ARIMA_number_patient_predictions(forecast_df): $     forecast_df.columns = ['Number of Patients', 'Predicted Number of Patients'] $     forecast_df['Predicted Number of Patients'] = round(forecast_df['Predicted Number of Patients'], 1)   $     return forecast_df
df2.query("group=='control' & converted==1").count()[0]/df2.query("group=='control'").count()[0]
df_c_merge['country'].astype(str).value_counts()
plt.plot(total['field4']) $ plt.show()
tmp_3 = linear_model.BayesianRidge() $ tmp_3.fit(X_training_3,y_training) $ y_pred_no_f = tmp_3.predict(X_final_test_3) $ r2_score(y_final_test, y_pred_no_f)
fashion.text[0]
ordered_df.head()
df1.head()
gs_rfc_over = GridSearchCV(pipe_rfc, param_grid=params_rfc, cv=ts_split, scoring='roc_auc') $ gs_rfc_over.fit(X_train, y_train_over)
train_commits[['user_id', 'repo_id', 'log10_commits']].to_csv('data/new_subset_data/train_commits.csv', index=False) $ test_commits[['user_id', 'repo_id', 'log10_commits']].to_csv('data/new_subset_data/test_commits.csv', index=False)
aggregation = {'tweet_id':'count','rating':'median', 'favorite_count':'median', 'retweet_count':'median'} $ stage_summary = pd.DataFrame(tweet_data[tweet_data['stage']!= ''].groupby('stage').agg(aggregation).to_records()) $ stage_summary.columns = ['stage', 'count', 'median_rating','median_favoriate_count','median_retweet_count'] $ stage_summary $
print(autos['ad_created'].str[:10].unique().shape) $ autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index()
pickle.dump(lsa_cv, open('iteration1_files/epoch3/lsa_cv.pkl', 'wb'))
ab_data.isnull().sum()
total_Visits_Convs_month_byMC = pd.concat([totalVisits_month_byMC[['year', 'month', 'day', 'marketing_channel', 'user_visits']], totalConvs_month_byMC.conversions], axis=1) $ total_Visits_Convs_month_byMC['conversion_rate'] = total_Visits_Convs_month_byMC['conversions'] / total_Visits_Convs_month_byMC['user_visits'] $ total_Visits_Convs_month_byMC.head()
total = 0 $ for chunk in pd.read_csv('data.csv', chuncksize = 1000): $     total += sum(chunk['x']) $ print(total)
timeslot = 120 $ y2_train = y_train.apply(lambda x : x // timeslot) $ print(y_train.head()) $ print(y2_train.head())
fraud_data_updated = fraud_data_updated.drop(['sex','browser','source','country'],axis=1)
print(convert_old, convert_new, n_old, n_new) $ z_score, pval = sm.stats.proportions_ztest([convert_new, convert_old],[n_new, n_old], alternative='larger') $ print('Zscore: {:.3f}\np-val: {:.4f}'.format(z_score, pval))
df1_clean = df1_clean[~(df1_clean.doggo.isnull() & df1_clean.floofer.isnull() & df1_clean.pupper.isnull() & $           df1_clean.puppo.isnull())]
tables = pd.read_html(url) $ tables
startDate = dateutil.parser.parse("2012-01-01") $ startDateStock = startDate - dateutil.relativedelta.relativedelta(months=2) $ endDate = dateutil.parser.parse("2017-12-31")
plt.hist(autos_VW["price"], bins=20) $ plt.title("Price for VW cars")
cityID = '389e765d4de59bd2' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Glendale.append(tweet) 
events.category.iloc[0]
from sklearn.naive_bayes import MultinomialNB $ model_count_NB = MultinomialNB() $ model_tfidf_NB = MultinomialNB() $ model_count_NB.fit(X_train_count, y_train_count) $ model_tfidf_NB.fit(X_train_tfidf, y_train_tfidf)
previous_month_date = end_date - timedelta(days=30) $ PR = PullRequests(github_index).is_closed().get_percentiles("time_to_close_days")\ $                                .since(start=previous_month_date)\ $                                .until(end=end_date) $ get_aggs(PR)
monte.str.split().str.get(-1)
logit_control_1 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA_int_ab_page','UK_int_ab_page', 'US_int_ab_page']]) $ result_control_1 = logit_control_1.fit()
trans = trans.loc[trans['membership_expire_date'] >= trans['transaction_date']]
!tar -xvf ./train-jpg.tar $
final_topbikes.groupby(by=final_topbikes.index.hour)['id'].count().plot(kind='bar', figsize=(12,6))
import json $ df.to_json('dataframe.json',date_format='iso') #specify date format so that "read_json" command can retrive datatime
print(dfd.capacity_5F_max.describe()) $ dfd.capacity_5F_max.hist()
all_years_by_DRG = all_years_by_DRG.reset_index() $ all_years_by_DRG.head()
df_new[['CA', 'UK', 'US']]=pd.get_dummies(df_new["country"])
df4.country.unique()
def get_residual_sum_of_squares(model, data, outcome): $     predictions = model.predict(data) $     residuals = outcome - predictions $     RSS = (residuals ** 2).sum() $     return(RSS)
pandas_df = day_access.sort('dayOfWeek').select('count').toPandas() $ pandas_df
df.loc[mask,:]
data.index = pd.to_datetime(data.index)
autos["odometer_km"].value_counts()
S.decision_obj.stomResist.options
df_new.groupby('country').mean().converted
df.shape
df_numbers['Number**2'] = df_numbers['Number'].map(lambda inset: inset**2) $ df_numbers
total = df2.shape[0] $ received_new_page = df2[df2['landing_page'] == 'new_page'].shape[0] $ received_new_page/total
tokendata.sendReceiveAvg = tokendata.sendReceiveCntAvg_mon.map(lambda x: round(x,2))
df_y_pred = np.exp(y_age_predict.as_data_frame()) $ df_y_actual = np.exp(y_age_valid.as_data_frame())
purchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00), $              ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00), $              ('2006-04-06', 'SELL', 'IBM', 500, 53.00), $             ] $ c.executemany('INSERT INTO stocks VALUES (?,?,?,?,?)', purchases)
eur_rate = pd.read_csv('eur_rate.csv', header=0) $ eur_rate['Date'] = pd.to_datetime(eur_rate['Date'], format="%m/%Y") $ eur_rate['Year'] = eur_rate['Date'].dt.year $ eur_rate['Month'] = eur_rate['Date'].dt.month $ gdax_trans_btc = pd.merge(gdax_trans_btc, eur_rate.iloc[:,1:], on=['Year','Month'], how="left")
old_page_converted = np.random.choice([1, 0], size = n_old, p = [p_mean, (1-p_mean)], replace = True)
df_afc_champ2018['awayWinPercentage'] = df_afc_champ2018['homeWinPercentage'].apply(lambda x: 1-x)
df_B2=pd.read_csv("classB.csv") $ df_B2.shape
model_yaml = model.to_yaml() $ with open("model_lstm_{}.yaml".format(n_epoch), "w") as yaml_file: $     yaml_file.write(model_yaml) $ model.save_weights("model_lstm_{}.h5".format(n_epoch)) $ print("Saved model to disk")
new_page_converted = np.random.choice([0, 1], size = n_new, p = [(1 - p_new), p_new])
percent_top_dollar = 0.2 $ high_volume_symbols = helper.large_dollar_volume_stocks(df, 'adj_close', 'adj_volume', percent_top_dollar) $ df = df[df['ticker'].isin(high_volume_symbols)]
sum(tw.duplicated(subset=['expanded_urls']))
nconvert = len(df2[(df2.converted==1)]) $ ntot = len(df2.index) $ prob = nconvert/ntot $ print(prob)
repos_users = pd.merge(repos, users, on='user', how='left') $ print('Shape repos:', repos.shape) $ print('Shape users:', users.shape) $ print('Shape repos_users:', repos_users.shape) $ repos_users.head()
and_list1 = df.query("group=='control'& landing_page=='old_page'").index
ax = sns.countplot(x="num_comments", data=reddit)
pd.concat([d1, d3])
max_open = [] $ for entry in d["dataset_data"]["data"]: $     max_open.append(entry[1]) $ print("The highest opening price was $"+str(max(max_open)))
evaluator = text_classifier.evaluate(df_test)          $ evaluator.plot_confusion_matrix()
ttest_ind(monthly_medication_df.first_month_adherence, monthly_medication_df.second_month_adherence)
precipitation_df.index=pd.to_datetime(precipitation_df.index)
df_new[['CA','US','UK']] = pd.get_dummies(df_new['country'])[['CA','US','UK']]
trainFIC = train $ y = trainFIC.Purchase $ x = trainFIC.drop(['User_ID', 'Product_ID', 'City_Category', 'Purchase'], axis=1)
expenses_df.drop(expenses_df.index[-1], inplace = True) $ expenses_df
import os $ print(os.getcwd())                   # print current working directory $ print(os.listdir(os.getcwd()))       # print contents of the current working directory $ print(os.path.split(os.getcwd())[0]) # print the parent directory. Repeat as needed to get to higher directories.
df_nullcount = df_closed.isnull().apply(np.count_nonzero) $ print df_nullcount
str(1).rjust(5)
df_control = df2.query('group == "control"') $ cont_convert = df_control.query('converted == 1').shape[0] / df_control.shape[0] $ cont_convert
b = 2. * np.random.randn(*a.shape) + 1. $ b.shape
tree_features_df.loc[0,'p_hash']
LARGE_GRID.plot_accuracy_be(raw_large_grid_df)+ggtitle("Median-Block of Mean-Element Accuracy")
s_filled = s.fillna(0) $ s_filled
!head -n 20 ../data/microbiome/microbiome_missing.csv
latest_df = latest_df.apply(id_annotation, axis=1)
df_loudparties = df[df['Descriptor'] == 'Loud Music/Party'] $ df_loudparties.groupby(df_loudparties.index.hour).apply(lambda x: len(x)).plot()
df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'], $                    'supervisor': ['Carly', 'Guido', 'Steve']}) $ display('df3', 'df4', 'pd.merge(df3, df4)')
dates.to_period('D')
df3 = df2.join(df_countries.set_index('user_id'), on='user_id')
word = 'convolution' $ fig, ax = w2v.create_3d_tsne_plot(word, number_closest_words=25)
yerr =(last_year[2] - last_year[0]) $ plt.boxplot(last_year) $ plt.title('Trip Average Temperature') $ plt.ylabel("Temperature", size = 10) $ plt.show()
filtered_df = data_df.drop(labels=['id', 'fst_subject', 'fst_tutor_type'], axis=1)
sweden = ['Sverige', 'Sweden', 'Stockholm', 'Gothenburg', 'Schweden', 'Sweden, Earth', 'Sweden, Universe'] $ notus.loc[notus['country'].isin(sweden), 'country'] = 'Sweden' $ notus.loc[notus['cityOrState'].isin(sweden), 'country'] = 'Sweden' $ notus.loc[notus['country'] == 'Sweden', 'cityOrState'].value_counts(dropna=False)
df2[['Begining', 'End', 'Middle']] = pd.get_dummies(df2['dategroup']) $ df2.head()
print('Retweet tweets: ' + str(len(tweets_clean.query('retweeted_status_id == retweeted_status_id')))) $ print('Total tweets: ' + str(len(tweets_clean)))
?pd.read_csv()
a = [0,1] $ ps_new = [p_new, 1-p_new] $ new_page_converted = np.random.choice(a, size=n_new, p=ps_new)
cust_demo.columns
df1.head()
metrics.precision_score(y_valid, y_pred)
filtered.ix['2011-11-03':'2011-11-04'].head(20)
All_tweet_data.head(5)
top_songs.info()
session_user = pd.crosstab(session_top_subset['user_id'], session_top_subset['action']) $ session_user.head()
top_supporters.apply(combine_names, axis=1)
import json $ import pprint $ with open('config.json') as config: $     config_file = json.load(config) $ pprint.pprint(config_file)
complete_df[complete_df[DEFAULT_NAME_COLUMN_TOTAL].isnull()].head()
print(datetime.datetime.now())
from sklearn.model_selection import KFold $ cv = KFold(n_splits=10, random_state=None, shuffle=True) $ estimator = Ridge(alpha=3000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
liquor2016_fy = liquor2016.SaleDollars.groupby(liquor.StoreNumber).agg(['sum']) $ liquor2016_fy.columns = ['Total_Sales'] $ liquor2016_fy.tail()
offseason18 = ALL[(ALL.index > '2018-02-07')] # Finally, this says that everything after this past Superbowl is part of $
train.pivot_table(values = 'Fare', index = 'Gender', aggfunc=np.mean)
df2['intercept'] = 1 $ mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = mod.fit()
trip_output_df  = pd.DataFrame({"Minimum Temperature": [tmin], "Average Temperature":[tavg], "Maximum Temperature":[tmax]}) $ trip_output_df
index = pd.date_range(start='2018-01-01', periods=len(sales), freq='D') $ index
selection = spice_df.query('parsley & tarragon') $ len(selection)
df_potholes = df[df['Descriptor'] == 'Pothole'] $ df_potholes.groupby(df_potholes.index.hour).apply(lambda x: len(x)).plot()
%%time $ max_key = max( r_dict.keys(), key = get_nextday_chg ) $ print('largest change in price between any two days: '+ str( get_nextday_chg(max_key) ) )
pd.value_counts(ac['Non-Compliance Found']).head()
for i in forecast_set: $     next_date = datetime.datetime.fromtimestamp(next_unix) $     next_unix += 86400 $     df.loc[next_date] = [np.nan for _ in range(len(df.columns) - 1)] + [i]
df_msg.fillna(0).groupby(['type', 'subtype'])['dt'].count()
mask = df_journey['name'] == (line['stop']) $ index = df_journey[mask].index[0] $ df_journey = df_journey.loc[index:] $ df_journey['arrDate'].iloc[0] = np.NaN $ df_journey['arrTime'].iloc[0] = np.NaN $
df_loc = pd.read_csv('/project/stack_overflow/Data/geocoded/geocoded_QueryResults_(9).csv') $ df_loc2 = pd.read_csv('/project/stack_overflow/Data/geocoded/geocoding_locations.csv') $ df_loc = pd.concat([df_loc,df_loc2,df10]).reset_index(drop=True)
from keras import regularizers $ from keras.models import Model, Sequential $ from keras.layers import Dense $ from keras.callbacks import EarlyStopping, ModelCheckpoint
autos = autos[autos['registration_year'] <= 2016] $ autos['registration_year'].value_counts(normalize=True).sort_index(ascending=True).hist(bins=20) $
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(y='Agency')
df2_treat = df2.query('group == "treatment"') $ df2_treat_conv = df2_treat[df2_treat['converted'] == 1].count() $ total = df2_treat['converted'].count() $ prop_treat = (df2_treat_conv['converted'] / total) $ print(prop_treat)                   $
n_old = df2.query('landing_page=="old_page"').count()[1] $ n_old
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=35000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
df.shape
from pandas.tools.plotting import autocorrelation_plot $ autocorrelation_plot(dataSeries) $ pyplot.show()
df.drop(bad_indices, inplace=True)
A = int(dframe_team.iloc[0]['Draft_year']) # grabs the first year of 'Draft_year' $ dframe_team.set_index('Draft_year').reindex(range(A,2016)) # Sets a new index, starting with the first year of the team $ dframe_team = dframe_team.set_index('Draft_year').reindex(range(A,2016)).reset_index() $ dframe_team.head(10)
df = df.drop_duplicates(["tweet_id"])
test_data_features = vectorizer.transform(clean_test_reviews)
data = pd.get_dummies(data,columns=['purpose', 'addr_state']) $ data['issue_d'] = data.issue_d.astype(int) $ data = data.drop('zip_code', axis=1) $ data.head()
train['question_dt']=pd.to_datetime(train['question_utc'], unit='s') $ test['question_dt']=pd.to_datetime(test['question_utc'], unit='s') $ train['answer_dt']=pd.to_datetime(train['answer_utc'], unit='s') $ test['answer_dt']=pd.to_datetime(test['answer_utc'], unit='s')
X_train, X_test, y_train, y_test = model_selection.train_test_split(X_sl_training, Y_sl_training, $                                                                     test_size=.2, random_state=7, $                                                                     stratify=Y_sl_training) $
details.isnull().sum()
engine = create_engine("sqlite:///./Resources/hawaii.sqlite", echo=False)
sample_submission = pd.read_csv("sample_submission.csv") $ sample_submission.head()
pd.groupby(vlc, by=vlc.visited.dt.year).size()
df_from_json = pd.read_json("msft.json") $ df_from_json.head(5)
sessions_summary = pd.DataFrame(data=sessions_sample.groupby(["user_id"])["action"].apply(list))
features = df[list(feature_dict.keys())] $ feat_type = list(feature_dict.values()) $ feature_types = feat_type $ labels = df[label_name]
D = 2 $ q_mu = np.float32([ 1., 4.]) $ p_mu = np.float32([-3., 2.]) $ q_sigma = np.ones(D).astype('float32') $ p_sigma = 2.5*np.ones(D).astype('float32')
engine.execute("select * from station limit 5").fetchall()
from sklearn.metrics import accuracy_score $ y_pred = grid_search.predict(X_test) $ accuracy_score(y_test, y_pred)
volume.sort() $ median_index = int(round(len(volume)/2,0)-1) $ median_volume = volume[int(median_index)] $ print('The median volume was: '+str(median_volume))
plt.figure(figsize=(20,6)); $ plt.plot(df['datetime'],df['MeanFlow_cms'],linewidth=0.5,color='orange') $ plt.ylim((0,700)) $ plt.show;
c.head()
mb2 = pd.read_excel('../data/microbiome/MID2.xls', sheetname='Sheet 1', header=None) $ mb2.head()
rfc_feat_sel.fit(blurbs_to_vect, y_encode)
import dotce $ stats = dotce.LanguageStats()
np.exp(0.0099)
separated_dog_stage = twitter_archive_clean[['doggo','floofer','pupper','puppo']].copy()
joined = joined[joined.priceClose!=0]
pm_data.dropna(inplace=True) $ pm_data.status.value_counts()
def clean_special_chars(text): $     text = re.sub(r'[^\w\s]', ' ', text) $     text = re.sub(r"$\d+\W+|\b\d+\b|\W+\d+$", "", text) $     text_with_no_special_chars = re.sub("\s+", " ", text) $     return text_with_no_special_chars
df['user_id'].nunique()
s_hr_min = dfss.index.get_level_values('time').min() $ s_hr_max = dfss.index.get_level_values('time').max()
tweets['created_at'] = tweets['created_at'].dt.tz_localize('GMT').dt.tz_convert('US/Eastern')
twitter_master2.head(2)
review_array = np.array(["Most disgusting and vile math instructor at Umass. His classes are a complete waste of time. Avoid like the plague. The worst of the worst."]) $ review_vector = vectorizer.transform(review_array) $ print(clf.predict(review_vector))
file = 'https://assets.datacamp.com/production/course_2023/datasets/tips.csv' $ tips = pd.read_csv(file) $ tips.info()
archive_clean.info()
ab_new[['CA','US','UK']] = pd.get_dummies(ab_new['country'])[['CA','US','UK']] #making dummy values for CA and US $ ab_new
smpl.select('end_customer_party_ssot_party_id_int_sav_party_id').distinct().count()
msft = pd.read_csv("../../data/msft.csv") $ msft.head()
group=merged.groupby(['date','store_nbr','class','family'],as_index=False) $ class_onpromotion=pd.DataFrame(group['onpromotion'].agg('sum')) $ pd.DataFrame.head(class_onpromotion)
print('The data in Groceries is:', groceries.values) $ print('The index of Groceries is:', groceries.index)
svm_classifier = GridSearchCV(estimator=estimator, cv=kfold, param_grid=svm_parameters) $ svm_classifier.fit(X_train, Y_train)
df.loc[:,"message"] = df.message.apply(lambda x : str.lower(x)) #Lower capital letters.
list(df.columns.values)
mgxs_lib = openmc.mgxs.Library.load_from_file(filename='mgxs', directory='mgxs')
name_dataframe = pd.DataFrame(autos["name"].str.split("_", expand = True)) $ name_dataframe
US_coeff = 0.0408 $ UK_coeff = 0.0506 $ np.exp(US_coeff), np.exp(UK_coeff)
precip_df.describe()
trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn]) $ val_lm = np.array([[stoi[o] for o in p] for p in tok_val])
df_final.shape
bacteria2.mean()
url = form_url(f'actions/{action_id}/metricSeries') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
print("Probability of control group:", $       df2[df2['group']=='control']['converted'].mean())
df4 = df3.copy() $ df4 = df4.drop('DAY',1) $ df4 = df4.drop('WEEK',1)
utility_patents_subset_df['figure_density'] = utility_patents_subset_df['number-of-figures'] / utility_patents_subset_df['number-of-drawing-sheets'] $ utility_patents_subset_df['figure_density'].describe()
sl.second_measurement.sum()
challange_1.shape
dask_dataframe = dd.read_csv(nyc_311_data_filename, dtype='str', parse_dates=['Created Date']) $ dask_dataframe.head(2)
country_result.summary()
df_main.text.sample(50)
print(df_subset.info())
learner.save('lm5')
area = np.pi * ( X[:, 1])**2  $ plt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(np.float), alpha=0.5) $ plt.xlabel('Age', fontsize=18) $ plt.ylabel('Income', fontsize=16) $ plt.show() $
files1= files1.loc[files1['Shortlisted']==1] $ files1.shape
z=loan_fundings[['fk_loan','amount','investment_yield']].copy() $ z['amount_yield']=z['amount']*z['investment_yield'] $ avg_yield=z.groupby('fk_loan').sum() $ avg_yield['avg_investment_yield']=avg_yield['amount_yield']/avg_yield['amount'] $ avg_yield['avg_investment_apr']=np.power(1+avg_yield['avg_investment_yield']/1200,12)-1
bnbA.shape
data_after_subset_filter = lv_workspace.get_filtered_data(level=1, subset='A') # level=0 means first filter $ print('{} rows mathing the filter criteria'.format(len(data_after_subset_filter))) $ data_after_subset_filter.head()
res.text
X_today.head()
df_regression.head()
mortraffic_byday.head()
store_items = store_items.append(new_store) $ store_items
twitter_archive_full['timestamp'] = pd.to_datetime(twitter_archive_full['timestamp'])
df2 = df.dropna(how = 'any') $ df2
df.pivot_table(values='(kW)', index='YEAR', columns='Make', aggfunc=np.mean)
num_reviews = train["review"].size $ clean_train_reviews = [] $ for i in range( 0, num_reviews ): $     clean_train_reviews.append( review_to_words( train["review"][i] ) )
df_measures_users = pd.read_csv('../data/interim/df_measures_users.csv', encoding="utf-8")
dfLikes = pd.DataFrame(l)[["id", "name", "created_time", "link", "description", "category", "fan_count"]]
click_condition_meta[pd.isnull(click_condition_meta['refr_source'])].shape[0]
tipsDFslice.tail()
scoring.columns
df2.query('converted == True')['converted'].sum()/ df2.shape[0]
store_items.fillna(method='ffill', axis=1) # filled with previous value from that row
dfLikes["date"] = dfLikes["created_time"].apply(lambda d: datetime.strptime(d, '%Y-%m-%dT%H:%M:%S+%f').strftime('%Y%m%d'))
S_lumpedTopmodel.decision_obj.simulStart.value, S_lumpedTopmodel.decision_obj.simulFinsh.value
df_subset.plot(kind='scatter', x='Initial Cost', y='Total Est. Fee', rot=70) $ plt.show()
user_df.head()
min(df2.timestamp), max(df2.timestamp)
df_twitter_archive.name.value_counts()
from sklearn.preprocessing import Imputer $ testDataVecs = Imputer().fit_transform(testDataVecs) $ print(np.isfinite(testDataVecs).all())
temperature_sensors_df = sensors_df[sensors_df['entity_id'].isin(temperature_sensors_list)] $ temperature_sensors_df.head()
scaler = MinMaxScaler() $ X_train = w2v_to_features(w2v, list_tokens) $ X_train = scaler.fit_transform(X_train)
train = train.drop(['timestamp_first_active','date_first_booking', 'timestamp_first_active', 'date_first_booking'], axis=1)
URL = "http://www.reddit.com"
col.head(1)
np.savetxt(r'text_preparation/tips.txt',tipsDFslice['text'], fmt='%s')
old_page_converted = np.random.choice([1,0], size=sample_size_old_page, p=[p_old_null, 1-p_old_null])
((df['group'] == 'treatment') != (df['landing_page'] == 'new_page')).sum()
sales_change_growth = (sales_diff['Sale (Dollars)_y'].values - sales_diff['Sale (Dollars)_x'].values)/(sales_diff['Sale (Dollars)_x'].values) $ len(sales_change_growth) $ total_sales['sales_change_growth'] = sales_change_growth $
title_tokens.head()
sns.jointplot(x = "positive_ratio", y = "wow_ratio", data = news_df)
ebola_dirs = !ls ../data/ebola/ $ ebola_dirs
movies.isnull().sum() #checking for missing values
(ggplot(all_lum.query("subject=='VP3'"),aes(x="td",y="gx",color="subject"))+stat_summary(geom="line"))
confidence = clf.score(X_test, y_test) $ print(confidence)
tweet_archive = pd.read_csv('twitter-archive-enhanced.csv') $
model_df.groupby('flair_text').size() 
Lab7['YEAR'] = Lab7['YEAR'].replace([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14], $ [2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2017])
p_treatment = df2[df2['group'] == 'treatment']['converted'].mean() $ p_treatment
stories.submitter_user.iloc[3]
users_visits.visits = 1 * (users_visits.visits > 0) $ users_visits = users_visits.assign(regs=1) $ users_visits.head()
today = datetime.datetime.today() $
new_page_converted = np.random.choice([0, 1], size = n_new, p = [1 - p_new, p_new]) $ p_new_sim = new_page_converted.mean()
np.dtype('S10,i4,f8')
issubclass(parser.BadLine, Exception)
boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv') $ boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')
newParser = parser.SVPOLParser(newFormat) $ newParser
score_df = pd.merge(info_cut, sc_cut, how='inner', on='parkid') $ score_df.head()
data.info()
preproc_reviews = pipeline.fit_transform(review_body) $ pipe_cv = pipeline.named_steps['cv']
np.exp(df3_results.params)
scaler = preprocessing.StandardScaler().fit(X_train) $ X_train_scaled = scaler.transform(X_train) $ X_test_scaled = scaler.transform(X_test)
members = [] $ for m in data: $     for i in m: $         members.append(i)
s = pd.Series(np.random.randint(0,7,size=10))  #low,high,size $ print(s,'\n') $ s.value_counts()
from sklearn.decomposition import TruncatedSVD
rows = session.query(Adultdb).filter_by(education="9th").all() $ print("-"*100) $ print("Count of rows having education as '9th' before delete: ",len(rows)) $ print("-"*100)
df2['intercept'] = 1 $ lm = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = lm.fit()
keras_entity_recognizer.set_step_params_by_name("learner", $                                                  model__recurrence_type = 'RNN', $                                                  model__bidirectional= True, #False, $                                                  batch_size = 50, $                                                  n_epochs = 5) $
tsne_input = word_vectors.drop(spacy.en.STOPWORDS, errors=u'ignore') $ tsne_input = tsne_input.head(5000)
a = 5; b = 4.5 $ isinstance(a, (int, float)) $ isinstance(b, (int, float))
session.query(Measurement.station, func.count(Measurement.station))\ $     .group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
df2['landing_page'].value_counts()
% matplotlib inline $ tweets_clean[tweets_clean.rating_numerator < 20].rating_numerator.hist(figsize = (8,8));
sns.lmplot('FLOW','REVENUE',df3) $ plt.show()
uber_15["day_of_week"] = uber_15["Pickup_date"].apply(lambda x: getDayofWeek(x, 2015)) $ uber_15.head()
text = 'value' #@param $ date_input = '2018-03-22' #@param {type:"date"} $ number_slider = 0 #@param {type:"slider", min:-1, max:1, step:0.1} $ dropdown = '1st option' #@param ["1st option", "2nd option", "3rd option"] $
Base = automap_base() $ Base.prepare(engine, reflect=True)
reddit['Class_comments'] = reddit.apply(lambda x: 'High' if x['num_comments'] > median_comments else 'Low', axis = 1) $ reddit.head()
result.summary()
tweets.head()
model.predict_proba(np.array([1,50]))
import statsmodels $ import statsmodels.api as sm $ statsmodels.__version__
df4.groupby(df4.index).median().plot()
df_example3.registerTempTable('df_example3')
df = actuals.merge(backcast, on='Gas_Date')
display('df5', 'df6', $        "pd.concat([df5, df6], join='inner')")
per_tweet_archive_by_month = tweet_counts_by_month.copy() $ per_tweet_archive_by_month['favorite_count'] /= per_tweet_archive_by_month['tweet_count'] $ per_tweet_archive_by_month['retweet_count'] /= per_tweet_archive_by_month['tweet_count'] $ per_tweet_archive_by_month.drop('tweet_count', axis=1, inplace=True) $ per_tweet_archive_by_month.columns = ['favorite_count_per_tweet', 'retweet_count_per_tweet']
calls_df["time"]=calls_df["call_date"].dt.hour $ calls_df["call_time"]="NULL"
LSI_model.evaluate(corpus[-100])
trend = pd.merge(df.reset_index(), topic_geo_infomation, on='date')
autos = autos.rename(columns={'odometer':'odometer_km'})
d_stream_profiles = pd.read_json('Twitter_SCRAPING/profile_tweets.json', lines=True) $ tweets_l_stream_profiles = d_stream_profiles['text'].tolist() # create a list from 'text' column in d dataframe $ print(tweets_l_stream_profiles[-1:])
autos.head()
joy_pos = df_joy[df_joy['Polarity'] >= 0] $ print(joy_pos['Polarity'].count(), joy_pos['Polarity'].count())
nexts = torch.topk(res[-1], 10)[1] $ [TEXT.vocab.itos[o] for o in to_np(nexts)]
transactions[~transactions["UserID"].isin(users["UserID"])] $
from bs4 import BeautifulSoup $ example1 = BeautifulSoup(train["comment_text"][0], "html.parser")
pl.hist(yc_new2['tipPC'], bins=100) $ pl.ylabel('N') $ pl.xlabel('tipPC') $ pl.title('Distribution of Tips as a percentage of Fare') $ print('The first moment is 4.89 and the second moment is 15.22')
mmbyp=[ l.split('	') for l in mmbyp.split('\n') if len(l)>0] $ bypdf=pd.DataFrame(mmbyp) $ bypdf.columns=['sDate','Event'] $ bypdf['Date']=pd.to_datetime(bypdf['sDate'])
df = df_providers[['drg3','disc_times_pay','year']].sort_values(['drg3','year'],ascending=[True,True]) $ df.head() $
df[(df.state == 'YY') & (df.amount >= 45000)]
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
dl = df.groupby('converted').count() $ dl $
pokemon['Name'] = pokemon['Name'].fillna('Primeape') $ pokemon['Type 1'] = pokemon['Type 1'].replace('Fighting', 'Fight') $ pokemon['Type 2'] = pokemon['Type 2'].replace('Fighting', 'Fight') $ pokemon_df = pokemon.copy() $ pokemon.head() $
tg_u= df[df['group']=='treatment'] $ tg_u_prob_of_conv = tg_u['converted'].mean() $ print('The probability of a treatment group user converting is:  ' + str(tg_u_prob_of_conv)) $
weather_yvr.plot?
twitter_archive_full[(twitter_archive_full.text.str.contains('doggos'))][['tweet_id','stage','text']]
tizibika[tizibika['retweets']==tizibika['retweets'].max()]
for col in ['MultipleLines']: $      df_raw[col].replace({'Yes': 1, 'No': 0, 'No phone service': 2}, inplace=True)
tweet_df.shape
data = pd.Series([0.25, 0.5, 0.75, 1.0]) $ data
configure['run']['duration'] = 0.5
for i, num in enumerate(data2017_list): $     print("{}: {}".format(i, num))
bus['postal_code_5'] = bus['postal_code_5'].str.replace("94602","94102") $ bus['postal_code'] = bus['postal_code_5'].str.replace("94602","94102") $
tweet_df_clean.reset_index(inplace=True) $ tweet_df_clean.drop('index', axis=1, inplace=True) $
RMSE_list[W.index(15)]
sns.factorplot('repo', 'is_core', data=df.ix[df['date'] > cutoff], kind='bar', col='org')
for row in range(len(data_new)): $     print(float(row)/len(data_new)) $     data_new['user_id'].iloc[row] = y[row] $ data_new.to_csv('senti_labeled.csv')
Daily_WRFraw_1950_2010 = ogh.getDailyWRF_salathe2014(homedir, mappingfile)
claps = pd.Series(data['claps']) $ plt.boxplot(claps,0,'rs') $ plt.hlines(y = claps.quantile(0.90), label = '99.5 percentile', xmin = 0,xmax = 100, linestyles='dashed' ) $
tmpdf_md = tmpdf_md.loc[md_idx,:] $ tmpdf_md.dropna(axis=1,how='all',inplace=True) $ tmpdf_md.dropna(axis=0,how='all',inplace=True) $ tmpdf_md.columns = list(range(0,tmpdf_md.shape[1])) $ tmpdf_md
df.tail(3)
users_visits = users_visits.groupby('chanel', as_index = False).mean() $ retention_lifetime_path =  output_path + '\\retention_lifetime.csv' $ users_visits.to_csv(retention_lifetimel_path)
tweet_archive_enhanced_clean[tweet_archive_enhanced_clean['favorite_count']==tweet_archive_enhanced_clean['favorite_count'].max()]
plt.figure(figsize=(12,4)) $ plt.hist(station_df['D_ENTRIES'].tolist(), bins=40);
g = sns.plt.title('Top Complaint Types on the 360 Days with The Most Complaints') $ g = sns.barplot(x = 'days_top_complaint', y='complaint_type', data = dftop2, color = "b" ) $ g.set(xlabel='Days of Top Complaint', ylabel='complaint type') $ plt.tight_layout() $ plt.savefig('Top_complaints_days_most_volume.png')
data.to_csv("csvs/datosFiltrados.csv", index = False)
nc_file.setncattr(name="description", value="this is just a netCDF smaple file.") $ nc_file.setncattr("created time", str(date.today())) $ nc_file.version = "1.0" $ print(nc_file)
print result.summary()
df_grouped.drop(['passing_reading', 'passing_math'], axis = 1, inplace = True) $ df_grouped.columns
df['converted'].sum() / df.shape[0]
p_diffs = np.asarray(p_diffs) $ (p_diffs > actual_diff).mean() #In how many cases the calculated differences (from the simulations) were higher than the actual differences?
tweet_json_filename = 'tweet_json.txt'
sum(list(d.values()))/len(list(d.values()))
df2_conv = df2[df2['converted'] == 1].count() $ total = df2['converted'].count() $ prop2 = (df2_conv['converted'] / total) $ print(prop2)
loans_df.shape
df_new.groupby(['country'], as_index=False).mean()
y_test_over[bet_over].mean()
local_mar.groupby('store').size()
print(df2.query('landing_page == "new_page"').shape[0])
from keras.models import Sequential, Model $ from keras.layers import LSTM, Dense, Conv1D, Input, Dropout, AvgPool1D, Reshape, Concatenate
dfJobs.ix[40]
import twitter $ api = twitter.Api(consumer_key='XXXXXXXXXXXXXXXXXXXXX', $   consumer_secret='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', $   access_token_key='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', $   access_token_secret='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')
from pyspark.sql.functions import desc $ df.groupby('Park Facility Name').count().sort(desc("count")).show()
msftAC / msftAC.shift(1) - 1
len(email_age_unique[pd.isnull(email_age_unique['request.ageRange']) == True])
raw_test_df.shape
merged_df['Hour'] = merged_df.index.hour
user.loc["realDonaldTrump": "LaraLeaTrump"]
data.describe()
autos1_df= pd.DataFrame({"price_comp":price_series, "odom_comp":odom_series}) $ autos_df
df_main.info()
pd.cut(tips.tip, np.r_[0, 1, 5, np.inf]).sample(10)
cleaned2['tc_sentiment'] = cleaned2['text'].apply(lambda t : tc.get_tweet_sentiment(t))
tweet_full_df = df_merge.merge(tweet_clean,how='left',on = 'tweet_id')
condition = (notus['cityOrState'].isna() == False) & (notus['country'].isna() == True) $ notus[condition]
grouped_dpt.head(1)
tm_2020 /= 1000 $ tm_2020_norm = tm_2020 ** (10/11) $ tm_2020_norm = tm_2020_norm.round(1) $ tm_2020_alpha = tm_2020 ** (1/3) $ tm_2020_alpha = tm_2020_alpha / tm_2020_alpha.max().max()
logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ results = logit_mod.fit() $ results.summary2()
for col in temp_columns: $     print(col) $     dat.loc[:,col]=dat[col].interpolate(method='linear', limit=3)
train.head()
from bmtk.analyzer import nodes_table $ input_nodes_DF = nodes_table(nodes_file, 'FridayHarborBiophysics') $ input_nodes_DF[:5]
dfRegMet2015 = dfRegMet[dfRegMet.index.year == 2015]
from sklearn.naive_bayes import GaussianNB $ gnb = GaussianNB() $ gnb.fit(X_train, Y_train)
autotomyDict = {False:'intact',True:'autotomized'} $ df.loc[:,'autotomized'] = df.loc[:,'autotomized'].map(autotomyDict) $ df.autotomized.unique()
git_log.timestamp = pd.to_datetime(git_log.timestamp, unit='s') $ git_log.timestamp.describe()
tweets['id'].groupby(pandas.to_datetime(tweets['created_at']).dt.date).count().mean()
y_pred_mdl7 = mdl.predict(test_orders_prodfill)
train.ORIGINE_INCIDENT.value_counts()
rng = pd.date_range(start='1/1/2017', periods=72, freq='B') $ rng $
from datetime import datetime $ datetime.now().strftime('%Y-%m-%d %H:%M:%S')
airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading') $ print(airquality_pivot.head())
menu_course_counts_csv_string = s3.get_object(Bucket='braydencleary-data', Key='feastly/cleaned/menu_course_counts.csv')['Body'].read().decode('utf-8') $ menu_course_counts = pd.read_csv(StringIO(menu_course_counts_csv_string), header=0, delimiter='|')
print(trump.axes) $ print(trump.shape)
percipitation_year = session.query(Measurements.date, func.avg(Measurements.prcp)) \ $     .filter(Measurements.date >= '2016-05-01').filter(Measurements.date <= '2017-06-10') \ $     .group_by(Measurements.date).all()
deaths_Sl_concat['Total_deaths_Sl'] = deaths_Sl_concat['Total_deaths_Sl'].astype(int) $ deaths_Sl_concat.columns = ["date", "death_suspected","country","date1","death_probable","country","date2","death_confirmed","country","Total_deaths_Sl"] $ Total_deaths_Sl =deaths_Sl_concat[['date','Total_deaths_Sl']] $ Total_deaths_Sl.head()
user = pd.read_csv('users.csv')
df.info()
%run -i 'image_retraining/retrain.py' --image_dir 'train'
h6.close()
miss = df.isnull().values.any() $ print ("Missing Values : {}".format(miss))
test_pixelData = df_test.as_matrix() $ print ("test_pixelData.shape:", test_pixelData.shape, test_pixelData.shape[0]) $
import statsmodels.api as sm $ convert_old = old.query('converted == 1').user_id.count() $ convert_new = new.query('converted == 1').user_id.count() $ n_old = df2.query('landing_page == "old_page"').user_id.count() $ n_new = df2.query('landing_page == "new_page"').user_id.count() $
df['Duration'].plot.hist()
movies.show(5)
tweet = api.get_status(id_list[0],tweet_mode='extended') $ tweet._json
print("dataframe df1 row names as follows : ") $ df1.index.values
s=table.find(text='Survivors').find_next('td').text $ survivors=re.search(r'\d+', s).group() $ survivors
df2 = pd.read_csv('ab_data2.csv') $ df2['user_id'].nunique()
dfNiwot["TMAX"].mean()
filename = url.split("/")[-1]
from sklearn.model_selection import train_test_split $ from sklearn import metrics $ from sklearn.metrics import classification_report, confusion_matrix
print("Jaccard  Similarity Score: ", metrics.accuracy_score(y_test, yhat)) $ print("F1-score Accuracy: ",  metrics.f1_score(y_test, yhat, average='weighted')) 
data.sample(4)
msk = np.random.rand(len(df_raw_tweet)) < 0.9 $ df_train = df_raw_tweet[msk] $ df_test = df_raw_tweet[~msk] $ print('Training set size: ' + str(len(df_train))) $ print('Test set size: ' + str(len(df_test)))
names.str.capitalize() $
output_variables = S.modeloutput_obj.read_variables_from_file() $ output_variables
plt.show()
df_new[['CA', 'UK']] = df_new[['CA', 'UK']].mul(df_new['new_page'], axis=0)
average_polarity.to_csv('polarity_results_LexiconBased/monthly/polarity_avg_monthly_2012_2016.csv', index=None) $ count_polarity.to_csv('polarity_results_LexiconBased/monthly/polarity_count_monthly_2012_2016.csv', index=None)
alphas = gb.sum().values                 # number of successes $ betas = (gb.count() - gb.sum()).values   # failures == total - successes $ beta_dist = np.empty(shape=(mcmc_iters, n_bandits))   # matrix to contain simulated Beta distributions $ for n in range(n_bandits): $     beta_dist[:, n] = np.random.beta(np.asscalar(alphas[n]), np.asscalar(betas[n]), size=(mcmc_iters))
convert_old = df2.query('group == "control" & converted == 1')['converted'].count() $ convert_new = df2.query('group == "treatment" & converted == 1')['converted'].count() $ convert_old, convert_new, n_old, n_new
df_prep15 = df_prep(df15) $ df_prep15_ = pd.DataFrame({'date':df_prep15.index, 'values':df_prep15.values}, index=pd.to_datetime(df_prep15.index))
plt.scatter("obs_date","status", data=pm_final,marker='o', alpha = 0.25) $ plt.xlabel("Cumulative date for readings") $ plt.ylabel("Status") $ plt.title('Failure over time') $ plt.show()
ad_group_performance.loc[ $     [5, 6, 7, 8, 15, 25] $ ]
df.query('converted==1').nunique()/df.user_id.nunique()
print(df.shape)
predicted_table_T = predicted_table.T
from scipy import stats $ stats.chisqprob = lambda chisq, df2: stats.chi2.sf(chisq, df2) # https://github.com/statsmodels/statsmodels/issues/3931 $ mod = sm.Logit(df2['ab_page'], df2[['converted','intercept']]) $ res = mod.fit() $ res.summary()
print "The probability of an individual converting in the treatment is {0:.4f}%".format(float(treat_con[0])/sum(df2['group']=='treatment')*100)
n_new=len(df2.query('group=="treatment"')) $ print("n_new :{}".format(n_new))
print(df2.query('landing_page == "new_page"').shape[0] / df2.shape[0]) $ obs_diff = treat_convert - cont_convert $ print(obs_diff)
sum(cm.diagonal())/sum(cm.flatten())
result = clf_RF_CT.predict(test_centroids) $ output = pd.DataFrame(data={"id":test["id"], "sentiment":result}) $ output.to_csv(os.path.join(outputs,"BagOfCentroids.csv"), index=False, quoting=3) $ print("Wrote BagOfCentroids.csv")
wells_df_new_cleaned_plus_nn_wNoNulls.head()
(english_df.groupBy('hashtag') $            .count() $            .sort('count', ascending=False) $            .show(5) $ )
print sqlContext.sql("select count(*) from ufo_sightings limit 10").collect()
X_train['building_id'].factorize()[0].max(), X_train['building_id'].factorize()[0].min()
autos["unrepaired_damage"].unique()
workspaces.to_csv('data/toggl-workspaces.csv')
elon.text = elon.text.str.lstrip('b') $ elon.created_at = pd.to_datetime(elon.created_at) $ elon['date'] = elon.created_at.apply(lambda x: x.date())
scoring_url = json.loads(response_online.text).get('entity').get('scoring_url') $ print scoring_url
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv', nrows=1000000)
print(daily_returns['SPY'].kurtosis()) $ print(daily_returns['XOM'].kurtosis())
events.loc['2017-01-10':'2017-01-11'].head(4)
df['date'] = pd.to_datetime(df['date'],infer_datetime_format=True)
dfleavetimes.head(5)
week29 = week28.rename(columns={203:'203'}) $ stocks = stocks.rename(columns={'Week 28':'Week 29','196':'203'}) $ week29 = pd.merge(stocks,week29,on=['203','Tickers']) $ week29.drop_duplicates(subset='Link',inplace=True)
df2= pd.read_csv('https://query.data.world/s/bmpdbk2e2ags6f5mkrvup3tir7xuax')
spp_plot = spp[(spp.season.isin(s)) & (spp.term.isin(t))] $ ved_plot = ved[(ved.season.isin(s)) & (ved.term.isin(t))] $ vhd_plot = vhd[(vhd.season.isin(s)) & (vhd.term.isin(t))] $ vwg_plot = vwg[(vwg.season.isin(s)) & (vwg.term.isin(t))]
hdb = pd.read_json(json.dumps(res_json['result']['records']))
nt = nt[(~nt["catfathername"].isin(np.append(hk["catfathername"].unique(),kl["catfathername"].unique())))]
df.iloc[2893]
S_1dRichards.decision_obj.hc_profile.options, S_1dRichards.decision_obj.hc_profile.value
date = dt.datetime.today().strftime("%m-%d-%Y") $ dframe_team.replace(['present'], [date], inplace=True)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
col='Case.Status' $ set(tmp_df.loc[np.logical_not(tmp_df[col].isnull()),col])
grouped.sum()
na_df.notnull() # check elements that are not missing
train.columns
dates = url.split('_')[-1].split(".")[-2].split("to")
data.loc[data.surface_covered_in_m2 < 1, 'surface_covered_in_m2'] = np.NaN
df.rolling(window = 2, min_periods = 1).sum() # rolling sum with window length of 2 (sum of every 2 values) - adds if there is at least 1 value in the window
data = [['Alex',10],['Bob',12],['Clarke',13]] $ df = pd.DataFrame(data,columns=['Name','Age'],dtype=float) $ print(df)
sns.countplot(calls_df["status"],hue=calls_df["call_time"])
for col in df.select_dtypes(include='datetime64').columns: $     print_time_range(col)
from gensim import models, similarities $ lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=100)
Imagenes_data[(Imagenes_data['p1_dog'] == False) & (Imagenes_data['p2_dog'] ==False ) & (Imagenes_data['p3_dog'] == False )].shape
from sklearn.manifold import LocallyLinearEmbedding $ t0 = time.time() $ X_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X_train) $ t1 = time.time() $ print("LLE took {:.1f}s.".format(t1 - t0)) $
temp = pd.Series(28 + 10*np.random.randn(10)) $ print(temp.describe())
yt.get_video_metadata(video_id[0], key, parser=P.default)
plt.figure(figsize=(15,5)) $ plt.title("Number of mentions per day") $ plt.plot(num_of_mentions_by_day.values) $ plt.xticks(range(len(num_of_mentions_by_day)),num_of_mentions_by_day.index,rotation='vertical') $ plt.show()
(df2.landing_page == 'new_page').sum() / df2.shape[0]
df_master.info()
header = cylData.first() $ cylHPData= cylData.filter(lambda line: line != header) $ print (cylHPData.collect())
p = pd.Period('2005', 'A') $ p
rfecv.support_
varcode = 'TN' $ siteds_dct = siteds[siteds['DataSetCode'].str.contains(varcode)].reset_index().iloc[0].to_dict() $ siteds_dct
pumaBB['public use microdata area'] = pumaBB['public use microdata area'].astype('str')
X_train = pd.get_dummies(columns=['term', 'home_ownership', 'verification_status', 'purpose'], data=X_train)
price = 100.50
MATTHEW_92_USERS_AC.sample(10)
import json $ with open('jallikattu.json', 'r') as f: $     line = f.readline() # read only the first tweet/line $     tweet = json.loads(line) # load it as Python dict $     print(json.dumps(tweet, indent=4)) # pretty-print
sumAll = df['MeanFlow_cfs'].describe(percentiles=[0.1,0.25,0.75,0.9]) $ sumAll
autos = autos[autos["price"].between(1,350000)] $ autos["price"].describe()
to_be_predicted_Day4 = 38.494209 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
for col in b_list.columns: $     if len(b_list[col].unique()) < 2: $         b_list.drop(col,axis=1,inplace=True) $         print(f"Dropped {col}")
new_page_converted=np.random.binomial(n_new, p_null)
forest = RandomForestClassifier(max_depth = 10, n_estimators=5, random_state=42) $ forest.fit(X_train, y_train) $ forest.score(X_test, y_test)    
p__new = df2['converted'].mean() $ p__new
parsedDF.head()
todaysFollowers[todaysFollowers[date].isnull()] #check nulls
    for path in modules.index: modules.loc[path] = file_to_module(path)
levels = approved.purpose.unique() $ sns.countplot(y="purpose", data=approved) $ plt.show() $ levels
print(df.seller.unique()) $ print(df.offerType.unique()) $ print(df.abtest.unique()) $ print(df.nrOfPictures.unique())
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'], $                   'data': range(6)}, columns=['key', 'data']) $ df
df.index.values
df2.query('converted == 1')['user_id'].nunique() / total_users
cassession.loadactionset('datastep') $
df.query('landing_page == "new_page"').query('group != "treatment"').info()
weather = weather.sort_values(by='created_date') $ weather.head()
efs1.subsets_
model.most_similar("best")
df_final_edited.head(1)
stackedCloses = closes.stack() $ stackedCloses
np.exp(-0.0149), np.exp(-0.0099), np.exp(-0.0506)
top_10_authors = git_log.groupby('author').count().apply(lambda x: x.sort_values(ascending=False)).head(10) $ top_10_authors
df2[df2.duplicated(subset="user_id", keep=False)] 
unique_vals = pd.Series(['c', 'b', 'a'])
londonDFSubsetWithCounts['centroid'] = londonDFSubsetWithCounts['geometry'].centroid $ londonDFSubsetWithCounts['lng'] = londonDFSubsetWithCounts['centroid'].map(lambda x: x.xy[0][0]) $ londonDFSubsetWithCounts['lat'] = londonDFSubsetWithCounts['centroid'].map(lambda x: x.xy[1][0])
new_page_converted = np.random.choice(2,n_new,p=[1-p_new,p_new]) $ print(new_page_converted)
predictions.select("label", "prediction", "probability").show()
df_tweets.head()
team_names = pd.read_csv('player_stats/team_names.csv') $ playoff_dict = {0:'DNQ',1:'First Round',2:'Second Round',3:'Conference Finals',4:'NBA Finals',5:'Champion'}  # Decoding $ team_names['playoff_round'] = team_names.Playoffs.apply(lambda x : playoff_dict[x]) $ names = list(team_names['Name'])
%writefile /tmp/test.json $ {"dayofweek": "Sun", "hourofday": 17, "pickuplon": -73.885262, "pickuplat": 40.773008, "dropofflon": -73.987232, "dropofflat": 40.732403, "passengers": 2}
prophet_model = Prophet(interval_width=0.95)  #default==0.8
train_words.shape
hawaii_measurement_df.head(10)
table_rows = driver.find_elements_by_tag_name("tbody")[30].find_elements_by_tag_name("tr") $
fig, ax = plt.subplots() $ sns.stripplot(x='State', y='Aver_N', data=df_one, ax=ax) $ ax.set_ylim(-0.2, 2.5) $ ax.set_ylabel('Average Photon Number') $ plt.savefig('../output/g_nbar_vs_states_basic.pdf', bbox_inches='tight')
lowest_open=df['Open'].min()
def tokenizer(text): $     return text.split()
ranked_variance = (weekly_variance.select('week','hashtag', 'variance', weekly_rank) $             .filter('rank <= 5') $             .sort('week','rank') $             .cache()) $
sqlite> SELECT value, COUNT(*) as sum $    ...> FROM nodes_tags $    ...> WHERE key = 'state_id' $    ...> GROUP BY value $    ...> ;
df = pd.read_excel(excel_file_Path, sheet_name = 'UP') $ df.head(100)
def last_id(soup): $     last = soup.find("span", {"class":'next-button'}).a['href'][-9:] $     return last
autos["odometer_km"].unique().shape
len(train_data[train_data.fuelType == 'elektro'])
df2['intercept'] = 1 $ df2['ab-page'] = df2['group'].replace(('treatment', 'control'), (1, 0)) $ df2.head()
xs = np.array([1,2,3,4,5], dtype=np.float64) $ ys = np.array([5,4,6,5,6], dtype=np.float64)
run txt2pdf.py -o '2018-07-09-2015-871 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS W MCC.pdf'  '2018-07-09-2015-871 - SEPTICEMIA OR SEVERE SEPSIS Without MV 96+ HOURS W MCC.txt'
import os $ import subprocess $ from subprocess import check_output, run $ string_to_do ="sleep 1; echo hello" $ out = run(string_to_do,shell=True,stdout=subprocess.PIPE)
pickle.dump(lda_tfidf_df, open('iteration1_files/epoch3/lda_tfidf_df.pkl', 'wb'))
tweet_json.info()
from sklearn.linear_model import LogisticRegression $ LR = LogisticRegression(C=0.01).fit(X_train,y_train) $ LR
plt.bar(range(24),hours.values(),width=0.8) $ plt.xlim(0,24) $ plt.xlabel('Hour of Day') $ plt.ylabel('Tweets per Hour')
from functools import reduce $ dfs = [df_CLEAN1A, df_CLEAN1B, df_CLEAN1C] # lift of the dataframes $ data = reduce(lambda left,right: pd.merge(left,right,on='MATCHKEY', how='inner'), dfs) $
to_be_predicted_Day3 = 81.92415603 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
details["Popularity"] = pd.to_numeric(details.Popularity)
tran_train = pd.read_csv('transactions_train.csv') $ print('Training data shape: ', tran_train.shape) $ tran_train.head(10)
new_page_converted = np.random.choice([1, 0], size=nnew, p=[np.mean([pnew, pold]), (1-np.mean([pnew, pold]))]).mean() $ print(new_page_converted)
users.head()
plt.scatter(y=dftouse_seven.opening_gross, x=dftouse_seven.star_avg,s=dftouse_seven.review_count)
df_sample = df.loc[idx_set['accordion']]
ben_final['diffs'] = ben_final['diffs'].fillna(0)
brandValues.mapValues(lambda x: int(x[0])/int(x[1])). \ $     collect() $
rodelar.groups()
print(results.summary())
wrd_prediction.to_csv('twitter_archive_master.csv')
status = status.retweet() $ status.retweeted, status.retweet_count
df.dtypes
data_for_model.head()
sss = list(spp.season.unique())
plt.hist(p_diffs); $ plt.axvline(x=p_diff_actual, color='r');
my_data = sc.textFile("file:/path/*")
red.columns
df_agg.sort_values(by=['revenue'], ascending = False).head()
%matplotlib notebook $ data_imported_nonan.groupby("main_category").count().state.plot(kind='bar', figsize=(10, 10))
df.columns
dfData['rqual_score5'].hist()
treaties.find_one({"_id": treaty_id_as_str}) # No result
click_condition_meta['os_name'] = np.where(click_condition_meta['os_name'].isin(ios), 'iOS', click_condition_meta.os_name) $ click_condition_meta['os_name'] = np.where(click_condition_meta['os_name'].isin(android), 'Android', click_condition_meta.os_name) $ click_condition_meta['os_name'] = np.where(click_condition_meta['os_name'].isin(windows), 'Windows', click_condition_meta.os_name) $ click_condition_meta['os_name'] = np.where(click_condition_meta['os_name'].isin(other), 'Other', click_condition_meta.os_name)
mars_html = mars_html.replace('\n', '') $ mars_html
match_results.tail(3)
events.loc[events.swimstyle == "breaststroke", ["distance", "swimstyle", "speed", "time", "category", "course"]].sort_values('speed')
stories[['hardware', 'score']].corr()
rng = np.random.RandomState(42) $ ser = pd.Series(rng.rand(5)) $ ser
urb_pop_reader = pd.read_csv(file_wb, chunksize = 1000) $ df_urb_pop = next(urb_pop_reader) $ print(df_urb_pop.head())
text = nltk.corpus.gutenberg.raw('bryant-stories.txt') $ sents = nltk.sent_tokenize(text) $ pp.pprint(sents[79:89])
df2.shape
twitter_archive_master.info()
ac['Date Closed'].describe()
start_df.info()
sns.factorplot(x='Year', data=affordability, y='Ratio')
twitter_mean.head()
df = pd.read_pickle("all_news.pkl") $ len(df)
target_pf.head()
columns = inspector.get_columns('stations') $ for c in columns: $     print(c['name'], c["type"]) $
q = [50, 85, 90, 95, 99] $ p = np.percentile(simulated_total, q) $ print(p)
from sklearn import metrics $ print metrics.accuracy_score(y_test, predicted) $
top_tracks.plot.bar() $ plt.xticks(np.array(range(0,5)),top_tracks.track_name ) $ plt.title('Top Tracks by Artist Followers')
xt = np.arange(0,23) $ tweets.plot.scatter(x='hour', y='chars_count',xticks = xt)
hspop = todf((hs * 100.00) / pop)
right(np.array([0, 1]), np.array([1, 0]))
df_archive_clean.drop(["retweeted_status_id","retweeted_status_user_id","retweeted_status_timestamp"], axis =1, inplace = True)
ps = PorterStemmer() #initialize Porter Stemmer object $ ps_stems = [] $ for w in test_post_words: $     ps_stems.append(ps.stem(w)) $ print(' '.join(ps_stems)) # add all the stemmed words to one string
collection.count()
crimes_all=crimes[~crimes['Latitude'].isnull()]
autos['offer_type'].unique()
x.plot.bar() $ plt.title("Temperature for Vacation dates") $ plt.xlabel("Vacation Dates") $ plt.ylabel("Temperature") $ plt.show()
posts.set_index('date_week',inplace=True)
df_subset['Initial Cost'] = pd.to_numeric(df_subset['Initial Cost'].str.replace('$',''), $                                           errors = 'coerce') $ df_subset['Total Est. Fee'] = pd.to_numeric(df_subset['Total Est. Fee'].str.replace('$',''), $                                           errors = 'coerce') $ df_subset.info()
url = 'https://dms2203.cartodb.com/api/v2/sql?format=csv&q=SELECT geoid, osm_id, latitude, longitude FROM new_york_new_york_points_int_ct10&api_key='+APIKEY $ dl_file = urllib.URLopener() $ dl_file.retrieve(url, PROCESSING_FOLDER+'/new_york_new_york_points_int_ct10.csv')
tweet_table = tweet_table.join(got_sentiment_df) $ tweet_table.head()
experiment_run_uid = client.experiments.get_run_uid(experiment_run_details) $ print(experiment_run_uid)
print(rdf.columns.tolist())
print(autos['last_seen'].str[:10].value_counts(normalize = True, dropna = False).sort_index()) $ print(autos['last_seen'].str[:10].value_counts(normalize = True, dropna = False).shape)
csvfile = "beatles-diskography.csv" $ datafile = datapath / csvfile
from sklearn.linear_model import LogisticRegression $ log_clf = LogisticRegression(multi_class="multinomial", solver="lbfgs", random_state=42) $ t0 = time.time() $ log_clf.fit(X_train, y_train) $ t1 = time.time()
df_joined=df_countries.join(df2_dummy) $ df_joined.tail(1)
from sklearn.mixture import GaussianMixture $ gmm = GaussianMixture(2)
import requests $ r = requests.get('https://en.wikipedia.org/wiki/Python_(programming_language)')
Imagenes_data.tweet_id.duplicated().value_counts()
soup.find('p', attrs={'class' : 'article-location-publishdate'})
%time train_4_reduced = tsvd.transform(train_4)
valid_news = news.loc[((news.messages.str.contains('http')) | (news.messages.str.contains('ht'))) & (~news.messages.str.contains('twitter.com'))] $ valid_news.shape
product_title=[preprocessor.remove_parentheses(value) for value in tqdm.tqdm(data_for_embedding["Product Title"])]
gh_hashlist.dropna(axis=0, how='all')
f = ['salt', 'water', 'fish', 'fishing'] $ fishing = wk_output[wk_output.explain.str.contains('|'.join(f))] $ fishing.shape $ fishing.to_csv('fishing_feedback.csv')
troll_users.head()
tweets_timeline = tweets.groupby(by=['hour', 'day_of_week'])['full_text'].count()
tlen.plot(figsize=(16,4), color='r');
 print(r.text)
%matplotlib inline $ import matplotlib.pyplot as plt, numpy as np
df_group_by.columns = df_group_by.columns.droplevel(1)
dfTemp=transactions.merge(users, how='inner',left_on='UserID',right_on='UserID') $ dfTemp
import sys $ sys.version
print('Accuracy of Logistic Regression algorithm using log loss: {}'.format(log_loss(y_test, yhat_prob)))
y_pred = rnd_reg.predict(X_test)
high12 = session.query(Measurement.tobs).\ $ filter(Measurement.station == "USC00519281", Measurement.station == Station.station, Measurement.date >="2016-08-23", Measurement.date <="2017-08-23").\ $ all()
from sqlalchemy.orm import sessionmaker $ Session = sessionmaker(bind=engine) $ session = Session()
import urllib3 $ dir(urllib3.exceptions)
n_new = df2.loc[(df2.landing_page == "new_page")].user_id.nunique() $ n_new
df.plot(kind = "hist", alpha = 0.5, bins = 20) $ plt.xlim([0, 1.0]) $ plt.yscale("log", nonposy = 'clip') $ plt.show()
n_new = df2.query('landing_page == "new_page"')['user_id'].nunique() $ n_new
feature_layer.properties.capabilities
local_cor = numeric_df.agg(*corr_aggs).collect()
df[['rating_numerator', 'favorite_count', 'retweet_count']].describe()
teams_df.head()
prcp_df.plot() $ plt.show()
witf = open("latlong_test2.txt","w", encoding="utf-8") $ for i in range(len(test_kyo2)): $     witf.write("{location: new google.maps.LatLng(%f, %f)},\n" % (test_kyo2['ex_lat'][i],test_kyo2['ex_long'][i])) $ witf.close()
to_be_predicted_Day1 = 38.13 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
seen_and_click = pd.merge(seen, click,  how='outer', left_on=['article_id','user_type','user_id','project_id'], right_on = ['article_id','user_type','user_id','project_id'])
sub.to_csv("first_sub.csv", index=False)
df.groupby('landing_page')['group'].value_counts()['new_page']['control'] + df.groupby('landing_page')['group'].value_counts()['old_page']['treatment']
dfa = df.query('landing_page == "new_page"') $ dfb = df.query('group == "treatment"') $ sum(dfa['group'] == 'control') + sum(dfb['landing_page'] == 'old_page')
df['price_doc'].values
df = pd.read_csv("msft.csv", $                 skip_footer = 2, $                 engine = 'python') $ df
ac['Compliance Review Start'].groupby([ac['Compliance Review Start'].dt.year]).agg('count')
import statsmodels.api as sm $ convert_old = len(df2[(df2['landing_page']=='old_page') & (df2['converted']==1)]) $ convert_new = len(df2[(df2['landing_page']=='new_page') & (df2['converted']==1)]) $ n_old = n_old $ n_new = n_new
df2_conversion = df2[df2['converted'] == 1] $ probability = (len(df2_conversion)/n_unique_users) $ print ("The probability of an individual converting regardless of the page is: {:.4f}".format(probability))
total.build_hdf5_store(filename='mgxs', append=True) $ absorption.build_hdf5_store(filename='mgxs', append=True) $ scattering.build_hdf5_store(filename='mgxs', append=True)
df.shape
df_first = transactions.groupby(["UserID"]).first() $ df_first['UserID'] = df_first.index # Assign a column that is equal to the index created from the Groupby $ df_first
x = B4JAN16['Contact_ID'].value_counts().sum() $ y = B4JAN17['Contact_ID'].value_counts().sum() $ z = B4JAN18['Contact_ID'].value_counts().sum()
frequency = defaultdict(int) $ for text in documents: $     for token in text.split(): $         frequency[token] += 1 $ frequency
archive_df.source.value_counts()
new_df=df[['Principal','terms','age','Gender','weekend']] $ new_df=pd.concat([new_df,pd.get_dummies(df['education'])], axis=1) $ new_df.drop(['Master or Above'], axis = 1,inplace=True)
A = pd.DataFrame(rng.randint(0, 20, (2,2)), columns=list('AB')) $ A
close_series.plot()  # plots Series directly $ plt.show()
import geopy.distance
import matplotlib.pyplot as plt $ plt.boxplot(y) $ plt.show()
data_2017_12_14_iberia_negative.head()
df.drop(df2.index, axis=0,inplace=True) $ df.drop(df3.index, axis=0,inplace=True) $ df2 = df
data = pandas.concat(sheets, names=["Worksheet Name","Worksheet Row"])
engine = create_engine('postgres://%s@localhost/%s'%(username,dbname)) $ print(engine.url)
knn_grid.best_estimator_
sns.lmplot(x='language_english', y='target', data=bnbAx, ci=None, logistic=True, x_jitter=0.05, y_jitter=0.1)
data.head(2)
confidence  = clf.score(X_test, y_test) $ print("Confidence our SVR classifier is: ", confidence)
p_tags = sample_soup.find_all("p") $ for p in p_tags: $     print(p.text)
twitter_archive_clean = twitter_archive.copy()
y=train.readingScore $ X=train[features_to_use] $ X=sm.add_constant(X) $ lmscore=sm.OLS(y,X) $ lmscore=lmscore.fit()
MergeMonth.head()
d = {'Ahmedabad': 1000, 'Kolkata': 1300, 'Mumbai': 900, 'Chennai': 1100, $      'Delhi': 450, 'Jaipur': None} $ cities = pd.Series(d) $ cities
from sklearn.linear_model import LogisticRegression
print("Proportion of users converted = {:.5}%".format(converted_count / total))
print("P-I-New_Page:", $       df2['landing_page'].value_counts()[0]/len(df2))
def capitalize_every_word(string): $     return string.title() $ capitalize_every_word('hello world!')
df.dropna(axis='rows', thresh=3)
%time X_train_dtm = stfvect.fit_transform(X_train)
min_req_wear_time = 60*10 $ keep_days = pax_raw[pax_raw.paxinten > 0].groupby(['seqn', 'paxday']).paxinten.count() >= min_req_wear_time $ keep_days = keep_days.reset_index() $ keep_days = keep_days.loc[keep_days.paxinten==True, ['seqn', 'paxday']]
store.info()
target_date.to_pydatetime()
import pandas as pd $ url = "https://space-facts.com/mars/" $ tables = pd.read_html(url) $ tables
pyLDAvis.save_html(vis, f"output/lda_{topics}topics_score{round(coherence_lda,4)}.html")
auto.loc[auto.Price.isnull(), 'Price'] = auto.groupby(['CarModel', 'CarYear']).Price.transform('mean') #excelov el stugeci, chishta hashvel
dill_file = open('model.dill', 'rb') $ model = dill.load(dill_file)
from sagemaker.predictor import csv_serializer, json_deserializer $ rcf_inference.content_type = 'text/csv' $ rcf_inference.serializer = csv_serializer $ rcf_inference.accept = 'application/json' $ rcf_inference.deserializer = json_deserializer
guardian_data.info()
np.exp(-0.0149), np.exp(-0.0408), np.exp(0.0099)
dict_location = json.loads(df.loc[row,'location']) $ pprint(dict_location)
data[['Sales']].resample('D').mean().rolling(window=10, center=True).mean().plot()
np.sum(~( (inches <= 0.5) | (inches >= 1) ))
pgh_311_data['REQUEST_ID'].resample("M").count().plot()
pd.merge(df_a, df_b, on='mathdad_id', how='left')
titanic_df[['cabin', 'cabin_floor']].head()
session.query(func.count(distinct(Measurement.station))).all()
df_new.head()
print(''.join(open("vader_lexicon.txt").readlines()[:10]))
n_new = df2['landing_page'].value_counts()[0] $ n_new
y_train.shape
print len(data[data['ComplaintID'].notnull()])
!wget -nv -O /resources/data/PierceCricketData.csv https://ibm.box.com/shared/static/reyjo1hk43m2x79nreywwfwcdd5yi8zu.csv $ df = pd.read_csv("/resources/data/PierceCricketData.csv") $ df.head()
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata = pd.read_csv(path, sep ='\s+', header=None) $ mydata.head(5)
data = fat.add_ema_columns(data, 'Close', [3,6,9,12,16,20,26,35,50]) $ data.tail()
df.head(1)
graf_counts2.reset_index(inplace=True)
ax1.set_xticks([5, 15, 25, 35, 45]) $ ax1.set_title('This is a placeholder Plot Title') $ ax1.set_xlabel('Values of X') $ ax1.set_ylabel('Values of Y') $ f
autos['date_crawled'].str[:10].value_counts(normalize = True, dropna = False).sort_index()
to_be_predicted_Day3 = 21.38157719 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
driver = webdriver.Chrome("./util/chromedriver")
(details['Average Rating'] == 0.0).value_counts()
predict.predict_score('Water_fluoridation')
last_hours = quarterly_revenue.to_timestamp(how="end", freq="H") $ last_hours
loans_df.shape
model_artifact = MLRepositoryArtifact(model, training_data=train.select('ATM_POSI','POST_COD_Region','DAY_OF_WEEK','TIME_OF_DAY_BAND','FRAUD'),\ $                                       name="Predict ATM Fraud") $ model_artifact.meta.add("authorName", "Data Scientist");
cityID = '300bcc6e23a88361' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Seattle.append(tweet) 
"Slow.it.down"
combats.Winner[combats.Winner == combats.First_pokemon] = 0 $ combats.Winner[combats.Winner == combats.Second_pokemon] = 1 $ print(combats.head(5))
one_week_in_seconds = 60*60*24*7 $ current_time = datetime.datetime.utcnow() $ for guest_id, guest in ama_guests.items(): $     guest['ama.week.diff'] = int((current_time - guest['date']).total_seconds() / $                                              one_week_in_seconds) $
judgeTool = judgeTools() $ testResults = judgeTool.judgeModel(dataDir, basicWeeklyMovePredict, resDir)
dem.shape
print clintondf.datetime.max() $ print clintondf.datetime.min()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
test = pd.Series(list(set(aggregates['campaign_name']))).apply(lambda x: format_cust_campaign(x))
def strip_singlequote_and_tolowercase(text): $     try: $         return text.strip('\'').lower() $     except AttributeError: $         return text 
plt.hist(taxiData.Trip_distance, bins = 100, range = [7, 10])
X_train_all.head()
print d.variables['trajectory'][0] $ print ''.join(d.variables['trajectory'][0])
In the logistic reegression the null hypothesis is that there is no relation between the variables and convertion rate. $ In the Alternate hyothesis is that there is a relationship between them
df = pd.read_sql("select sum(quantity),title from order_details a inner join store_items_sizes b on a.order_item_size_id = b.order_item_size_id GROUP BY title",conn)
{2:6} {3:6} {4:6} {5:6,.1f} 
xmlData['state'].value_counts() $ xmlData['state'].replace({' Washington':'WA'}, inplace = True)
Z = np.arange(9) $ print(Z.reshape((3, 3)))
cred = '' $ with open("credentials/localhost/jessica.txt") as credfile: $     cred=credfile.read().strip() $ cnx = create_engine('postgres://{}@localhost:5432/rpred'.format(cred), isolation_level='AUTOCOMMIT') $ conn = cnx.connect() $
full_data.reset_index(drop=True, inplace=True)
hellotrans_request = requests.get('https://hellotranslations.wordpress.com/2015/01/30/dou-po-cang-qiong-chapter-1/') $ hellotrans_soup = bs4.BeautifulSoup(hellotrans_request.text, 'html.parser') $ hellotrans_ptags = hellotrans_soup.article.findAll('p') $ for h_ptag in hellotrans_ptags: $     print(h_ptag.text)
S_distributedTopmodel.basin_par.filename
gb.agg(['sum', 'count'])    $
def capitalize(string,lower_rest=False): $     return string[:1].upper() + (string[1:].lower() if lower_rest else string[1:]) $ capitalize('fooBar')
raw.education.head()
model_data = pd.read_pickle('D:/CAPSTONE_NEW/jobs_data_final.pkl')
nx.set_node_attributes(G,  offer_group_desc_dict, 'offer_group') $ nx.set_node_attributes(G, first_genre_dict, 'genre') $ nx.set_node_attributes(G, entity_type_dict, 'entity_type')
print myIP[0] $ print low[low > myIP[0]].min() $ print high[high < myIP[0]].max() $ print low[low > myIP[0]].min() $
plate_appearances.loc[plate_appearances.batter_name=='javier baez',].tail(10)
df2['intercept'] = 1 $ df2[['ab_page','B']] = pd.get_dummies(df2['landing_page']) $ df2 = df2.drop('B',axis = 1) $ df2.head()
pipe_lr_3 = make_pipeline(tvec, lr) $ pipe_lr_3.fit(X_train, y_train) $ pipe_lr_3.score(X_test, y_test)
merged2.head()
df2.shape
r = r.json() $ type(r)
w.apply_data_filter(subset = subset_uuid, step = 1)
lr = 0.0005 $ lrs = lr
print(df2[df2['group'] == 'treatment']['converted'].mean())
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_ipb_brace_crown[0][0])) $ perr = np.sqrt(np.diag(pcov_ipb_brace_crown[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df2.query('group == "treatment" and converted == 1').count()['user_id']/df2.query('group == "treatment"').count()['user_id']
n_new = df2.query('landing_page == "new_page"').group.count() $ n_new
!jupyter nbconvert index.ipynb --to html $ bucket.upload_file('index.html', 'index.html') $ bucket.upload_file('index.ipynb', 'index.ipynb')
year7 = driver.find_elements_by_class_name('yr-button')[6] $ year7.click()
sentiments['created_at'] =  pd.to_datetime(sentiments['created_at'])
df.query('group == "treatment" and landing_page == "old_page"').shape[0] + df.query('group == "control" and landing_page == "new_page"').shape[0]
question_feature_names = ["text_length","number_of_links","code_percentage"] $ for tag in pos_tags: $     question_feature_names.append("%s_count"%tag)
s_n_s_epb_one = s_n_s_df[s_n_s_df.Beat=='5K02'] $ s_n_s_epb_one = s_n_s_epb_one.groupby(s_n_s_epb_one.Date).size().reset_index() $ s_n_s_epb_two = s_n_s_df[s_n_s_df.Beat=='5G02'] $ s_n_s_epb_two = s_n_s_epb_two.groupby(s_n_s_epb_two.Date).size().reset_index()
df.head(5)
to_be_predicted_Day2 = 25.11647691 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
vectorizer3 = TfidfVectorizer(use_idf=True, ngram_range=(1, 2))  $ transformed3 = vectorizer3.fit_transform(cleaned_description.values) $ features3 = vectorizer3.get_feature_names()
print(my_df["timestamp"].describe())
station_count = session.query(Station.id).count() $ print(" There are {} stations within the data set".format(station_count))
print('Stochastic Gradient Descent Accuracy:', nltk.classify.accuracy(SGD, test_set) * 100, '%')
metadata['epsg'] = int(refl['Metadata']['Coordinate_System']['EPSG Code'].value) $ metadata['epsg']
print(ds_issm.geospatial_lat_max) $ print(ds_issm.geospatial_lon_max)
trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt) $ val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt) $ md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)
features = pd.merge(features, features_rolling_averages, on=['game', 'team'])
weekly_variance.show(10)
pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
learn.save('clas_2')
1 / np.exp(-0.0149)
cols = ['chanel', 'logins'] $ users_visits = users_visits[cols] $ logins_per_user_per_chanel_path =  output_path + '\\logins_per_user_per_chanel.csv' $ users_visits.to_csv(logins_per_user_per_chanel_path)
z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new, n_old],alternative='larger') $ p_value
s1.dropna
cp311.info()
theta = opt.fmin_bfgs(cost_computation, theta_0, fprime=grad_computation, args=(X_train_1, y_train))
val = rng.values
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
loan_requests[loan_requests.id==96439]
inactive_count = len(clean_users[clean_users['active']==0].dropna(how='any')) $ inactive_mean = clean_users[clean_users['active']==0].dropna(how='any')['account_life'].dt.days.mean() $ inactive_sd = clean_users[clean_users['active']==0].dropna(how='any')['account_life'].dt.days.std()
ab_df.user_id.nunique()
cv = CountVectorizer(stop_words=stop_words) $ dtm = cv.fit_transform(no_urls_all_engl) $ tt = TfidfTransformer(norm='l1',use_idf=False) $ dtm_tf = tt.fit_transform(dtm)
hillary_data = data[data.cand_nm == 'Clinton, Hillary Rodham'] $ sorted_by_date = hillary_data.sort_values("contb_receipt_dt", ascending=True)
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results = lm.fit() $ results.summary()
confederations.head()
client.experiments.delete(experiment_run_id)
cp = nltk.RegexpParser(grammar) $ result = cp.evaluate(train_trees) $ print(result)
day_of_year14["avg_day_of_month"] = day_of_year14.index.map(lambda x: int(x.split("/")[1])) $ day_of_year14.head()
df.columns
p_old = df2["converted"].mean() $ print("Convert rate for P-old is: {}".format(p_old))
simm = pd.DataFrame()
pd.merge(left, right, on = 'key')
plt.subplot(1,2,1) $ master_df.loc[master_df.rating_numerator_normal < 1776, ['rating_numerator_normal']].boxplot() $ plt.subplot(1,2,2) $ outlier_df[['rating_numerator_normal']].boxplot();
idx = 0 $ festivals.insert(loc=idx, column='latitude', value=[41.9028805]) $ idx = 0 $ festivals.insert(loc=idx, column='longitude' value=[-87.7035663]) $ head.festivals(5)
tweet_length.plot(figsize=(20,5),color='r')
len([earlyPr for earlyPr in BDAY_PAIR_df.pair_age if earlyPr < 3])/BDAY_PAIR_df.pair_age.count()
eve_new= len(df_aft.query('ab_page==1')) $ eve_old= len(df_aft.query('ab_page==0')) $ eve_prob =df_aft.converted.mean()
ibm_train = ibm_hr_final.join(ibm_hr_target.select("Attrition_numerical")) $ ibm_train.printSchema()
pd.merge(total, total2, how='inner', on=['name'])
media_classes = [c for c in df_os.columns if c not in ['domain', 'notes']] $ media_classes
df_inter_2.interpolate(method='polynomial', order=2)
content_performance_bytime.to_csv('../DATA/content_performance_bytime', compression='gzip')
new_page_converted = np.random.binomial(1, p_new, size = n_new)
output= "SELECT count(*) from tweet where tweet_content like '%a%' " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['Count'])
results.query('home_score > @min_home_score')
mtcars.head()
nmf2 = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(transformed2)
base_dict_by_place = pd.DataFrame(c.apply(lambda x: [i for l in x for i in l], axis=1), columns=['hashtags'])
df = pd.read_csv('loan_train.csv') $ df.head()
data = monthly_sales['item_cnt_month'] $ sns.distplot(data.sample(1000)) $ plt.title('monthly sales distribution') $ plt.show() $ data.describe()
All_tweet_data_v2.name[(All_tweet_data_v2.name.str.len() < 3) & (All_tweet_data_v2.name.str.contains('^[(a-z)]'))]
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ p_new
tia1 = tia[['company','location','skills','title','salary_clean','category','duration_int','summary_clean']] $ tia1.shape
print(diff.days)
delta_tickets = create_delta_tickets_time(resolved_df) $ ax = plt.figure(figsize=(15, 8)) $ ax = sns.barplot(x="assigned_group", y="delta", hue="created_at", data=delta_tickets)
first_commit_timestamp = git_log[git_log['author']=='Linus Torvalds'].sort_values(by='timestamp').head(1).reset_index(drop=True).iloc[0,0] $ last_commit_timestamp =  pd.to_datetime('today') $ corrected_log = git_log[(git_log['timestamp'] >= first_commit_timestamp) & (git_log['timestamp'] <= last_commit_timestamp)] $ corrected_log['timestamp'].describe()
image_predictions_df.img_num.value_counts()
pivot_discover_first.sort_values(['email', 'step_no'], inplace=True) $ discover_first.columns[-10:-1] $ discover_first[discover_first['email'] == "yuleandra21@gmail.com"] $
df2 = df2[['date', 'cust_avail_v3', 'css_count', 'css_score', 'orders', 'new_conv', 'new_sales', 'new_sales_perc', $           'sales', 'calls_per_day']] $ df2.head(1)
week25 = week24.rename(columns={175:'175'}) $ stocks = stocks.rename(columns={'Week 24':'Week 25','168':'175'}) $ week25 = pd.merge(stocks,week25,on=['175','Tickers']) $ week25.drop_duplicates(subset='Link',inplace=True)
ad_source = ad_source.drop(['[', ']'], axis=1) $ ad_source = ad_source.drop(ad_source.columns[0], axis=1)
grouped_df.nsmallest(3, 'Sentiment')
df.plot()
secondary_temp_dat=dat[secondary_temp_columns].copy() $ secondary_temps_exist= not secondary_temp_dat.dropna(how='all').empty $
df.drop( columns='year1') # drop single column
import os $ sc.addPyFile(os.path.expanduser('~/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar')) $ from graphframes import *
P_newpage = (df.landing_page=='new_page').mean() $ P_newpage
X.shape
total = read_csv('mmenv01.csv') 
pd.Series(pd.DatetimeIndex(pivoted.T[labels==1].index).strftime('%a')).value_counts().plot(kind='bar');
print(pd.DataFrame(test_bow).head())
import pandas as pd $ df = pd.read_csv('movie_data.csv', encoding='utf-8') $ df.head(3)
df_subset2 = df_subset[df_subset['Total Est. Fee'] <= 1000000]
tweet_image_clean.info()
action_id = 'd68585ed-b922-4fd7-86bf-d78e22ccbc8d' $ url = form_url(f'actions/{action_id}') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
import pandas as pd $ stops = pd.DataFrame(data, columns=good_columns)
print(df['Site Fill'].value_counts(dropna=False))
import statsmodels.api as sm $ logit = sm.Logit(df['converted'],df[['intercept','control']]) $ results = logit.fit() $ results.summary()
appt_mat.shape
prob_t= df2.query('group == "treatment"').converted.mean() $ prob_t
df2.head(5)
dset = dset.sel(lon=slice(100, 300), lat=slice(50, -50))
users.loc[users.age < 18,'age'] = np.nan $ users.loc[users.age > 150, 'age'] = 2014 - users.age $ users.loc[users.age > 122, 'age'] = np.nan
bigrams = bigram_converter.get_feature_names() $ len(bigrams)
df2.query("user_id == @duplicated_user_id")
df2.query('landing_page == "new_page"').landing_page.count()/df2.landing_page.count()
temp = pd.concat([X_train.manager_id,pd.get_dummies(y_train)], axis = 1).groupby('manager_id').mean() $ temp.columns = ['high_frac','low_frac', 'medium_frac'] $ temp['count'] = X_train.groupby('manager_id').count().iloc[:,1] $ print(temp.tail(10))
stepwise_model.fit(train)
ti_mar.groupby('store').size()
df.shape
get_descendant_frame(observations_ext_node, data)[observations_ext_node['number'][DATA].notnull()].head(14)
query_results=small_df $ births = [] $ for i in range(0,query_results.shape[0]): $     births.append(dict(account=query_results.iloc[i]['account'], likes=query_results.iloc[i]['likes'], link_to_post=query_results.iloc[i]['link_to_post'])) $ births
def textblob_tokenizer(str_input): $     blob = TextBlob(str_input.lower()) $     tokens = blob.words $     return words
import matplotlib.pyplot as plt $ %matplotlib inline
pd.date_range(start, end, freq='W')
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results = mod.fit() $ results.summary()
X_trainfinaltemp= X_trainfinal.copy() $ X_trainfinaltemp = X_trainfinaltemp.rename(columns={'fit': 'fit_feat'})
sdf.createOrReplaceTempView("tempTable") $ res.show()
plt.hist(counts, bins=100)
subred_num_tot = reddit[['subreddit','num_comments']].groupby(by='subreddit', sort=True, as_index=False).sum().sort_values(by='num_comments',ascending=False)
state_party_df = state_party_df.rolling(7).mean()
val_idx = np.flatnonzero( $     (df.index<=datetime.datetime(2018,4,3)) & (df.index>=datetime.datetime(2018,3,1))) $
dfn.News.iloc[0] 
poparr = fe.bs.csv2ret('boots_ndl_d4spx_1957-2018.csv.gz', $                        mean=fe.bs.SPXmean, sigma=fe.bs.SPXsigma, $                        yearly=256)
pumashp.head()
np.sum(my_df.isnull().any(axis=1))
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
for row in c.execute('SELECT * FROM stocks ORDER BY price'): $         print(row)
stream.filter(track=['clinton','trump','sanders','cruz'])
df = df.drop('log_price', axis=1)
turnstiles_df.sort_values(["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True, ascending=False) $ turnstiles_df.drop_duplicates(subset=["C/A", "UNIT", "SCP", "STATION", "DATE_TIME"], inplace=True)
df2['date'] = df.date
rng = pd.date_range('2015-12-01', periods=100, freq='S') $ rng[:5]
RF_feature_impt = pd.DataFrame({'features':features.columns, 'importance':model_rf.feature_importances_}).sort_values('importance', ascending=False) $ RF_feature_impt.head(20)
projects = pd.read_csv('data_old/sc_report_projects.csv') $ print(projects.shape) $ projects
!wc -l /home/ubuntu/kmer-hashing/sourmash/lung_cancer/samples.csv
clean_measure.to_csv('../clean_measure.csv', index=False)
clean_predictions.info()
def get_integer3(s): $     return gearbox_list.index(s)
prob1_rf = pd.Series(x[1] for x in prob_rf) $ Results_rf10 = pd.DataFrame({'ID': Test.index, 'Approved': prob1_rf}) $ Results_rf10 = Results_rf10[['ID', 'Approved']] $ Results_rf10.head()
stop_word = r'\b(?:{})\b'.format('|'.join(stop)) $ data_df['clean_desc_rsw'] = data_df['clean_desc'].str.lower() $ data_df['clean_desc_rsw'] = data_df['clean_desc_rsw'].str.replace(stop_word, '') $ data_df['clean_desc_rsw'] = data_df['clean_desc_rsw'].str.replace(r'\s+', ' ')
df.to_csv('trump_state_of_union_2018.csv.gz', index=False, compression='gzip')
df.iloc[  0:3  ]    # by row number range
conv_control_users = df2.query('converted == 1 and group == "control"').shape[0] $ users_Control_group= df2.query('group == "control"').shape[0] $ p2 = float(conv_control_users / users_Control_group) $ print("Given that an individual was in the control group, the probability they converted is {:.4f}".format(p2))
parameters = dict(n_estimators=[1000, 1500, 2000], criterion=['gini', 'entropy']) $ RFC_grid = GridSearchCV(RFC_model_grid, param_grid=parameters, cv=5, verbose=3)#, scoring='f1') $ RFC_grid.fit(X_train, y_train)
len(data.groupby(['longitude', 'latitude']))
df[~df.index.isin([5,12,23,56])].head(13)
loaded_keras_entity_recognizer = KerasEntityExtractor.load(pipeline_zip_file) $ predictions = loaded_keras_entity_recognizer.predict(df_test) $ evaluator2 = loaded_keras_entity_recognizer.evaluate(predictions) $ evaluator2.get_metrics('phrase_level_results')[['Recall', 'Prec.', 'F-score']]
countries_path = '../../Data/countries.csv' $ countries = pd.read_csv(countries_path) $ print(len(countries)) $ countries
nold = (df2['landing_page'] == 'old_page').sum() $ print(nold)
wrsids[np.where(ids=='105001')[0][0]] $
temp_df = active_psc_records[active_psc_records.company_number.isin(secret_corporate_pscs.company_number)] $ len(temp_df[temp_df.groupby('company_number').secret_base.transform(all)].company_number.unique())
causes = data['Cause'].value_counts() $ causes = pd.DataFrame(data['Cause'].value_counts()) $ causes.plot(kind='bar') $ plt.show()
loan_stats = loan_stats[meets_credit_policy] $ loan_stats['loan_status'].table()
assert mcap_mat.shape == price_mat.shape
apple_SA = apple.groupby('SA').agg({'SA':'count'}).rename(columns=({'SA': 'Count'})) $ apple_SA ['% of total'] = apple_SA['Count']/apple_SA['Count'].sum() $ apple_SA ['Sentiment'] = ['Negative', 'Neutral', 'Positive'] $ apple_SA = apple_SA.reindex(columns = ['Sentiment', 'Count','% of total']) $ apple_SA
import mysql.connector $ from sqlalchemy import create_engine $ engine = create_engine('mysql+mysqlconnector://admin:geotwitter@geotwitter.uncg.edu:3306/geotwitter', echo=False) $ tweetsIRMA.to_sql(name='tweetIrmaTimes', con=engine, if_exists = 'append', index=False)
import pandas as pd $ df = pd.read_csv('movie_data.csv', encoding='utf-8') $ df.head(3)
classifier = OneVsRestClassifier(LinearSVC(random_state=42)) $ classifier.fit(train_matrix, train_labels_bin) $ y_pred = classifier.predict(test_matrix) $ y_pred
with tb.open_file(filename='data/NYC-yellow-taxis-100k.h5', mode='a') as f: $     table = f.get_node(where='/yellow_taxis_2017_12') $     table.cols.trip_distance.remove_index() $     table.cols.passenger_count.remove_index()
merge.sort_values("amount", ascending=False)
reddit.corr()
siteNo = '02087500' #Neuse R. Near Clayton $ pcode = '00060'     #Discharge (cfs) $ scode = '00003'     #Daily mean
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ print(z_score, p_value) $
HAMD = pd.read_table("hrsd01.txt", skiprows = [1], na_values= "-9")
merge.head(1)
print(experience.sum(axis=0).sum()) $ print() $ print(experience.sum(axis=0))
missing_values = df.isnull().sum() $ missing_values
pubs = db.get_publications(limit=None)  # limit sets a limitation for sql commands $ pubs.head()
twitter_archive_master[(twitter_archive_master.iloc[:,8:12].sum(axis=1) == 0) & (twitter_archive_master['has_stage'] == 1)]
X = dataset[dataset.columns[1:]] $ Y = np.array(dataset["label"])
one_df = df.query("group == 'treatment' & landing_page == 'new_page'") $ other_df = df.query("group == 'control' & landing_page == 'old_page'") $ df2= one_df.append(other_df) $ df2.head()
c.head()
model.doesnt_match("paris berlin london austria".split())
def plot_mentions(symbol_data, symbol): $     symoccurances_df = pd.DataFrame(symbol_data) $     symoccurances_df.plot(kind= 'bar' , x = 'time', y = 'sentiment', color='blue')
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth) $     return api
df.plot(column = "Cluster_Labels_GaussianMixture", cmap = "Reds", legend = True) $ plt.title("Clusters: Gaussian Mixture")
autos.price.hist(bins = 30)
%matplotlib inline $ from IPython.display import Image $ from IPython.core.display import HTML
rejected['approved'] = False $ rejected.head()
summary_hc = df_measures_users[df_measures_users['ms']==0][score_variable].describe() $ print('Average Score of HC Group: {} (SD {})'.format( $     round(summary_hc.loc['mean'],2), round(summary_hc.loc['std'],2)))  
gr_e1 = df.query("group == 'treatment' and landing_page == 'old_page'")
from test_package.print_hello_function_container import print_hello_function $ print_hello_function()
from sklearn.preprocessing import MinMaxScaler $ scaler = MinMaxScaler() $ train['load'] = scaler.fit_transform(train) $ train.head(10)
rent_db3.price.hist(bins=50)
avg_sale_table = data_df[['ID','Segment','Country','Product','Sale Price']].copy() $ avg_sale_table.head()
df.fine_amount.unique()
total = len(df2.index) $ total_converted = (df2['converted'] == 1).sum() $ print(total_converted / total)
grouped_dpt.get_group("HR") # get one group 
with open('./data/in/dm.xpt', 'rb') as f: $     _xport_dm = xport.to_columns(f) $     _xport_dm2 = pd.DataFrame(_xport_dm) $ _xport_dm2.head()
s_cal = pd.read_csv('seattle/calendar.csv') $ s_list = pd.read_csv('seattle/listings.csv') $ s_rev = pd.read_csv('seattle/reviews.csv')
from sklearn.feature_extraction.text import CountVectorizer $ from sklearn.model_selection import train_test_split
df.drop(df.query("group== 'control' and landing_page=='new_page'").index,inplace=True) $ df.drop(df.query("group== 'treatment' and landing_page=='old_page'").index,inplace=True) $ df.info()
decades = [1960, 1970, 1980, 1990, 2000, 2010, 2020] $ decade_names = ['1960', '1970', '1980', '1990', '2000', '2010'] $ df_western['release_decade'] = pd.cut(df['release_year'], decades, labels=decade_names) $ df_western.head() $
min_mean_km = price_vs_km["mean_odometer_km"].min() $ price_vs_km.loc[price_vs_km["mean_odometer_km"] == min_mean_km, :]
pd.read_csv?
idx = df_providers[ (df_providers['id_num']==110130) & \ $                   (df_providers['year']==2012)].index.tolist() $ len(idx) $ np.sum(df_providers.loc[idx,'disc_times_pay']) $ (df_providers.loc[idx[:5],:])
%matplotlib inline $ commits_per_year.plot(figsize=(12,7), kind='bar', legend=None, title='Commits per year')
df2.head()
app_pivot["Percent"] = [(row[0]/row[1]) for row in app_pivot[['Application', 'Total']].values] $ app_pivot
df_list_rand.columns
np.random.seed(123456) $ dates = pd.date_range('8/1/2014', periods=10) $ s1 = pd.Series(np.random.randn(10), dates) $ s1[:10]
from pandas import DataFrame $ labels = DataFrame(answer.Sentiment.map(dict(positive=1, negative=0)))
haw = pd.merge(left=dfAnnualN,right=dfAnnualMGD,how='inner',left_index=True,right_index=True) $ haw.columns
combined_df4['bin_label']=combined_df4['bill_bal'].apply(lambda x: 1 if float(x)<0 else 0) $ combined_df4['bin_label'].value_counts()
datetimes = ['timestamp', 'created_at'] $ for field in datetimes: $     archive_copy[field] = archive_copy[field].astype('datetime64[ns]')
print(month.sum(axis=0).sum()) $ print() $ print(month.sum(axis=0))
intervention_history['target_test'] = intervention_history['time_delta'].between(pd.Timedelta('1 days'), pd.Timedelta('182 days')) \ $                                         & (intervention_history['next_incident_type'] != 'Entretien') \ $                                         & intervention_history['MOTIF_ANNULATION_CODE'].isnull()
x_minor_ticks = 10 # Note that this doesn't work for datetime axes. $ y_minor_ticks = 10
autos["price"] = autos["price"].str.split('$',expand=True).iloc[:,1].str.replace(',','').astype(float) $ autos["odometer"] = autos["odometer"].str.replace("km",'').str.replace(',','').astype(float) $ autos.rename(columns={'odometer':'odometer_km'}, inplace=True) $ autos[["price","odometer_km"]].describe() $
monthly_cov_matrix = monthly_gain_summary.cov()
cc['logspread'] = np.log1p(cc['spread']) $ plt.hist(cc['logspread']) $ plt.show()
brands = [] $ for key, value in brand_pct.items(): $     if value >= 0.05: $         brands.append(key[:])
talks.head()
shots_df.to_pickle('shots_df.pkl')
df.to_csv('Tableau-CitiBike/TripData_2018_Winter.csv', index=False)
trn_lm = np.load(LM_PATH / 'tmp' / 'train_ids.npy') $ val_lm = np.load(LM_PATH / 'tmp' / 'val_ids.npy') $ itos = pickle.load(open(LM_PATH / 'tmp' / 'itos.pkl', 'rb'))
prob_treatment = (df2['group'] == 'treatment').sum()/unique_users_2 $ prob_treatment_and_converted = df2[  (df2['group'] == 'treatment')& (df2['converted'] == 1)].shape[0]/unique_users_2 $ prob_convert_given_treatment = prob_treatment_and_converted/prob_treatment $ prob_convert_given_treatment
df_os['domain'] = df_os['domain'].apply(remove_www) $ df_os.head()
import pyLDAvis $ import pyLDAvis.gensim $ pyLDAvis.enable_notebook() $ pyLDAvis.gensim.prepare(lda_tf, corpus_tf, dictionary)
archive_copy.puppo.unique()
sns.lmplot(x="Income", y="Age", data=training, x_estimator=np.mean, order=1)
pandas.DataFrame(globalCitySentences,columns=['content'])
sdsw['WAGE_RATE_EST'] = sdsw[['WAGE_RATE_OF_PAY_FROM', 'WAGE_RATE_OF_PAY_TO']].max(axis=1).fillna(method='bfill') $ len(sdsw)
for c in ccc: $     ved[c] /= ved[c].max()
treatment_group_user_count = df2[df2['group'] == 'treatment']['user_id'].count() $ converted_treatment_user_count = df2[(df2['group'] == 'treatment') & (df2['converted'] == True)]['user_id'].count() $ p_treatment_converted = converted_treatment_user_count / treatment_group_user_count $
tweets['retweeted'] = tweets['retweeted_status'].notna() $ tweets = tweets.drop(columns=['retweeted_status'])
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'")['converted']) $ convert_new = sum(df2.query("group == 'treatment'")['converted']) $ n_old = len(df2.query("group == 'control'")) $ n_new = len(df2.query("group == 'treatment'")) $
pos_sent = open("../data/positive_words.txt", encoding='utf-8').read() $ neg_sent = open("../data/negative_words.txt", encoding='utf-8').read() $ print(pos_sent[:101])
file = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', sep=',').load('treas_parking_payments_2017_datasd_parsed.csv') $ file.show(3)
grouped_by_bedrooms_df = full_df.groupby('bedrooms')['listing_id'].count().reset_index().copy() $ grouped_by_bedrooms_df.columns = ['bedrooms','count_of_listings'] $ grouped_by_bedrooms_df
df.rename(columns={'Indicator':'Indicator_id'}).head(2)
intr = df[y_col].value_counts(1).head(50) $ plot = intr.plot(kind='bar', figsize=(16,8)); $ plot.set_xticklabels(intr.index, {'rotation' : 90});
url_speaker = 'https://api.ted.com/v1/speakers.json?api-key=ynw2u8e4h9sk8c2htp7vutxq'
weather_cols = ['Weather', 'TempF', 'TempC', 'Humidity', 'WindSpeed', 'WindDirection', 'Pressure', 'Precip', ] $ weather_df = weather_df[weather_cols] $ weather_df.head()
file_t = open('tweets_terror2.txt','w') $ for item in terrorism: $     file_t.write("%s\n" % item)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
twitter_archive[twitter_archive.tweet_id==782021823840026624] $
with open('all_data_vectorized.dpkl', 'wb') as f: $     dpickle.dump(all_data_vectorized, f)
overallYrSold = pd.get_dummies(dfFull.YrSold)
print("Print content for df_CLEAN1A:",'\n', df_CLEAN1A.isnull().sum()) $ print('\n') $ print("Print content for df_CLEAN1B:",'\n', df_CLEAN1B.isnull().sum()) $ print('\n') $ print("Print content for df_CLEAN1C:",'\n', df_CLEAN1C.isnull().sum())
causes = pd.DataFrame(data['Cause'].value_counts()) $ causes
SGDC = SGDClassifier() $ model2 = SGDC.fit(x_train, y_train)
start = timer() $ partition_to_labels(50) $ end = timer() $ print(f'{round(end - start)} seconds elapsed.')
yt.get_video_comments(video_id[0], key)[:2]
df_train.hist(figsize=(5,3),color = 'y') $ plt.xlabel('is_Churn') $ plt.ylabel('Count') $ plt.show()
pd.to_datetime([1349720105, 1349806505, 1349892905, 1349979305, 1350065705], unit='s')
aqmdata['location'].value_counts()
res = requests.get(url, headers=headers)    # Sending an HTTP request with URL and the Requesting agent as header
req = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-7-04&end_date=2017-07-04&api_key=" + API_KEY) $ data = req.json() $ data
my_model_q6_b = SuperLearnerClassifier(clfs=[clf_base_knn,clf_base_svc, clf_base_nb], stacked_clf=clf_stack_lr, training='label', randomN=4) $ my_model_q6_b.fit(X_train, y_train) $ y_pred = my_model_q6_b.predict(X_test) $ accuracy = metrics.accuracy_score(y_test, y_pred) $ print("Accuracy: " +  str(accuracy))
props.prop_name.value_counts()
LT906474 = pd.read_table("GCA_900186905.1_49923_G01_feature_table.txt.gz", compression="infer") $ CP020543 = pd.read_table("GCA_002079225.1_ASM207922v1_feature_table.txt.gz", compression="infer")
tree_pred = model_tree.predict(x_test) $ logit_pred = model_logit.predict(x_test) $ rf_pred = model_rf.predict(x_test) $ gb_pred = model_gb.predict(x_test) $ nn_pred = model_nn.predict(x_test)
mars_facts=df.to_dict('records') $ mars_facts
taxi_hourly_df = taxi_hourly_df.reindex(dateindex)
mod_model.scen2xls(version=None)
loans_df.head()
cat_sz = [ $     (c, len(joined_samp[c].cat.categories) + 1) for c in cat_vars]
conn.columninfo(table=dict(name='iris', caslib='casuser'))
display('df2', 'df2.groupby(str.lower).mean()')
df_uro.dtypes.unique()
next_day_pf = ch_portfolio.Ch_Portfolio('test_monthly', engine)
df1 = pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
print('raw time value: {}'.format(plan['plan']['date'])) $ print('datetime formatted: {}'.format(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(plan['plan']['date']/1000))))
t0, t1 = lin_reg.intercept_[0], lin_reg.coef_[0][0] $ t0, t1
predictions['p1'].value_counts()
submissions.head()
total_daily_sales = data[['Sales']].resample('D').sum() $ total_daily_sales.expanding().sum()['2014-12'].head() $
plt.scatter(master_copy.rating_numerator, master_copy.favorite_count) $ plt.xlim(0,25) $ plt.xlabel('Rating Numerator') $ plt.ylabel('Favorites') $ plt.title('Favoriting based on numerator score')
ftr_imp=zip(features,xgb_model.feature_importances_)
A = pd.Series([2,4,6], index=[0,1,2]) $ B = pd.Series([1,3,5], index=[1,2,3]) $ A + B
df1 = tweets_gametitle.groupby('Game Title Date')['date','source'].count()
data_archie['cur_sell_price'].min()
retweet_pairs[["FromType","FromName","Edge","ToType","ToName","Weight"]][retweet_pairs["Weight"]>1].to_csv("For_Graph_Commons.csv",index=False)
np.ones((3,4),dtype=np.int)
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep $ lee_train_file = test_data_dir + 'lee_background.cor'
vis = points.copy() $ vis.set_index(["Team"], inplace = True) $ odds = vis["Wins"].plot(kind = "bar", figsize = (25, 5), title = "Number of wins in the world cup", rot = 90, legend = True) $ odds.set_ylabel("Number of Wins", fontsize = 15) $ plt.show()
data.head(100)
df_bkk.head()
p_education=portland_census2.drop(portland_census2.index[39:]) $ p_education=p_education.drop(p_education.index[:37]) $ p_education.head()
DummyDataframe = DummyDataframe.set_index("Date").sort_index() $ DummyDataframe = DummyDataframe.groupby("Date").sum()
cityID = '00ab941b685334e3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Nashville.append(tweet) 
qnt(100)   # it accepts just a scalar i.e a single value as an argument
display(base_accuracy_comparison) $ plt.xlim(0, 1.0) $ _ = plt.barh(range(len(base_accuracy_comparison)), list(base_accuracy_comparison.values()), align='center') $ _ = plt.yticks(range(len(base_accuracy_comparison)), list(base_accuracy_comparison.keys()))
feature_data = pd.read_csv('data/kaggle_data/features.txt', header=None, sep="  ", names=['feature_names', 'feature_description']) $ feature_data.head(5)
median_trading_volume = statistics.median([day[6] for day in data]) $ print ('Median trading volume for 2017:', median_trading_volume)
df.iloc[7:11][['Genre','genre','Name','Release_Date','_merge','Standard_Plat','Platform','Year_of_Release']]
df2.plot(x='station_name',y='observ_ct',kind = 'bar',title = 'Observation Frequency by Station') $ plt.show()
df =  prepareDF(dfL, makeCopy=True) $ display(dfL.head(), "==>" ,df.head() ) $ df = df.sample(frac=1).reset_index(drop=True) $
plt.hist(null_vals); #Here is the sampling distribution of the difference under the null $ plt.ylabel('Frequency', fontsize = 18); $ plt.xlabel(' Difference in Mean', fontsize = 18); $ plt.title('Null Values Plot', fontsize = 18); $
newsorgs_bar = mean_newsorg_sentiment["News Organization"] $ compound_bar = mean_newsorg_sentiment["Compound"] $ x_axis = np.arange(0, len(compound_bar), 1)
image_df.head(5)
tweet_data = pd.read_csv(r'./tweetCoords.csv',header=None,names=columns,parse_dates=[1],infer_datetime_format=True)
MostHourlyEntries = subway3_df.nlargest(100,'Hourly_Entries')
B = lv_workspace.get_subset_object('B') $ print(B.get_step_list())
daily_trading_volume = [] $ for lis in answer1: $     daily_trading_volume.append(lis[6])
CIN = pd.read_excel(url_CIN, $                     skiprows = 8)
daterange = pd.date_range(scn_genesis,datetime.today(),freq='1M')
titanic.fare.hist()
combined_df2.keys()
sorted_agg_churned_plans_counts = sorted(agg_churned_plans_counts,key=lambda x:x[1].tolist(),reverse=True) 
df2[(df2['landing_page']=='old_page')].count()[0]
pnew = df2[df2['landing_page']=='new_page']['converted'].mean() $ pnew
date + pd.to_timedelta(np.arange(12), 'D')
df_archive_clean = df_archive.copy() $ df_image_clean = df_image.copy() $ df_count_clean = df_count.copy()
df = df[df.powerPS != 0] $ df.dropna(subset=['powerPS'],axis=0, how='any', thresh=None, inplace=False) $ df.shape
messy = pd.read_csv('data/weather_yvr_messy.csv') $ messy.head()
result = vocabulary_expression.sort(['components_1':'component_8'], ascending=False).head(10)
df.bound_at.isnull().sum()
TEXT.vocab.itos[:12]
dfData['rqual_score10'].hist()
xmlData['build_year'].value_counts()
btc_wallet.head()
df[df['abuse_number'].str.contains('ES147799')]
import cv2 $ cv2.__version__
df_cont.shape
mars_facts_url = "https://space-facts.com/mars/" $ browser.visit(mars_facts_url)
df = df.drop(columns=['index'])
notus.isna().sum()
df[df['converted']==1]['user_id'].count() / df.shape[0]
test.shape
mentionsfreq = FreqDist(words_mention_sp) $ print("20 most common mentions: ", mentionsfreq.most_common(20)) $ hashfreq = FreqDist(words_hash_sp) $ print("20 most common hashtags: ", hashfreq.most_common(20))
yhat_SVM = clf.predict(X_test)
orig_ct = len(dfd) $ dfd = dfd.query('in_pwr_47F_min >= 0.05 and in_pwr_47F_min <= 2.0') $ print(len(dfd) - orig_ct, 'eliminated')
data.head()
!ls ../data/imsa-cbf/ | grep _b.cbf | wc -l
n_bins = 10 $ bin_fxn = lambda y: pd.qcut(y,q=n_bins,labels = range(1,n_bins+1)) $ features['f15'] = prices.volume.groupby(level='symbol').apply(bin_fxn) $
df_t = pd.merge(left=df,right=df_updated_stats,how='left',left_on='Updated Shipped ranges',right_on='Updated Shipped ranges',validate="many_to_one")
from datetime import date $ goodTargetUserItemInt['weeknum']=[date.fromtimestamp(i).isocalendar()[1] for i in goodTargetUserItemInt['created_at']] $ print sorted(goodTargetUserItemInt['weeknum'].unique())
def custome_roune(stock_price): $     return int(stock_price/100.0) * 100
print(autos["price"].describe()) #See the statistics of the price
daily_revenue=file4.groupby('date').agg({'trans_amt':'sum'}).sort(desc("date")) $ daily_revenue.show() $ daily_revenue.count() #43 days of record 
old_pid_php_xml_output = get_scielo_php("S1414-32832016005024105") $ dict(old_pid_php_xml_output.xpath("//ARTICLE")[0].attrib)
c = 2 # NUMBER OF CLASSES $ m = get_rnn_classifier(bptt, 20*bptt, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1, $           layers=[em_sz*3, 50, c], drops=[dps[4], 0.1], $           dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3]) $
name = ['Alice', 'Bob', 'Cathy', 'Doug'] $ age = [25, 45, 37, 19] $ weight = [55.0, 85.5, 68.0, 61.5]
image_predictions_clean.p1=image_predictions_clean.p1.astype('category') $ image_predictions_clean.p2=image_predictions_clean.p2.astype('category') $ image_predictions_clean.p3=image_predictions_clean.p3.astype('category')
vals = bacteria_data.value $ vals
inputs = tf.placeholder(tf.int32, [None]) $ one_hot_inputs = tf.one_hot(inputs, len(vocab)) $ transformed = tf.Session().run(one_hot_inputs, {inputs: word_ids}) $ transformed
move_34p34h34h = (breakfastlunchdinner.iloc[1, 1] $                + breakfastlunchdinner.iloc[2, 2] $                + breakfastlunchdinner.iloc[2, 3]) * 0.002 $ move_34p34h34h
sentdata_beginning.plot(kind='area')
df.loc['Monday'] # select by row label 'Monday'
negative_examples.to_csv('training_data/utah_negative_examples.csv')
support_NNN = merged_NNN[merged_NNN.committee_position == "SUPPORT"]
to_be_predicted_Day1 = 48.45 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
y_pred = m.predict(X_valid) $ cnf_matrix = metrics.confusion_matrix(y_valid, y_pred) $ cnf_matrix
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0] $ df2.shape
df7 = df[df['hired'] ==1] $ df7.groupby('category')['position','hourly_rate','num_completed_tasks'].agg({'median','mean','min','max'}).reset_index() \ $     .rename(columns={'category': 'category', 'position': 'position_stats_hired','hourly_rate':'hourly_rate_stats_hired', \ $                  'num_completed_tasks':'num_completed_tasks_stats_hired'}) $
LARGE_GRID.make_table_accuracy(raw_large_grid_df)
df['has_logo'].unique()
history = model.fit([X_dense_train, X_conv_train], y_train, epochs=30, validation_split=0.2, batch_size=128)
check_all('2017-01-17').head()
data_df.groupby('topic')['ticket_id'].nunique()
df.resample('H').mean()
grouped = writers.groupby(["Country","Gender"]) $ grouped.groups
corpus = st.CorpusFromParsedDocuments(df, $                                       category_col='category', $                                       parsed_col='parse').build()
len(df_questions["Capsule id"].value_counts())
if not os.path.isdir('output/electricity_demand'): $     os.makedirs('output/electricity_demand')
quotes = yahoo_finance.download_quotes(["GILD", "IBM"]) $ print quotes
logins.info()
sql = "SELECT * FROM paudm.photoz_bcnz as bcnz limit 5 " $ df1 = pd.read_sql(sql,engine)
df_test3_promotions.head().T
titanic['is_adult'] = titanic.age >= 18 $ titanic.head()
rf_v1.score_history()
from scipy.integrate import odeint $ def cap_continua(C, t, r): $     return r * C
r = requests.get(url) $ json_data = r.json() $ print(json_data)
print('{} outliers were removed from the training dataset'.format(train_df.shape[0] - prepared_train.shape[0]))
import requests $ import quandl $ quandl.ApiConfig.api_key = 'AnxQsp4CdfgzKqwfNbg8'
c['landmark'].value_counts().head(10)
brand_counts = autos["brand"].value_counts(normalize=True) $ common_brands = brand_counts[brand_counts > 0.05].index $ print(common_brands)
lower_case = letters_only.lower()        # Convert to lower case $ words = lower_case.split()               # Split into words
results = session.query(func.max(Station.latitude)).all() $ for result in results: $     print(result)
crimes['month'] = crimes.DATE__OF_OCCURRENCE.map(lambda x: x.month) $
print(automl.show_models())
pd.merge(df1, df2, how='left', $          right_index=True, left_index=True)
stop = stopwords.words('english') $ tweetering['Text'] = tweetering['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) $ pd.Series(' '.join(tweetering['Text']).lower().split()).value_counts()[:50]
mismatch_all = pd.concat([no_mismatch_newpage, no_mismatch_oldpage])
print('Collecting Todays Followers') $ todaysFollowers[date] = todaysFollowers['Account'].apply(get_NumberofFollowers)
np.histogram(noaa_data[(noaa_data.index >= '2018-05-29') & (noaa_data.index < '2018-05-30')].loc[:,'AIR_TEMPERATURE'])
test1.shape, test2.shape
grouped = events.groupBy("user_id").agg( count("user_id") ) $ grouped = grouped.withColumnRenamed("count(user_id)", "count") $ grouped.schema
f, ax = plt.subplots() $ ax.set_ylim(ymax=150); $ ax.set_xlabel('Shift duration [minutes]'); $ ax.set_ylabel('Count'); $ df['duration'].astype('timedelta64[m]').hist(bins=75, ax=ax);
df2.query('group == "treatment"').shape[0]/df2.shape[0]
talks = pd.read_csv("talks.tsv", sep="\t", header=0) $ talks
grouped_dpt.ngroups # number of groups 
rat_data.head()
print("Number of Groups in Enterprise ATT&CK") $ print(len(all_enterprise['groups'])) $ df = all_enterprise['groups'] $ df = json_normalize(df) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
df_new_page = df.query("landing_page == 'new_page'") $ df_new_page.count()['user_id'] / df['landing_page'].count()
all_tweets = pd.read_csv('all_tweets.csv', sep=',', error_bad_lines=False, index_col=False, dtype='unicode')
df = pd.read_csv(input_path) $ df.dropna(subset=['Recipient Email Address'], inplace=True) $ df.ix[:5, :5]
m3 = np.dot(m2.T,m) $ print("m3: ", m3)
output.printSchema() $ output2 = output.select('label', 'features') $
import urllib.request $ import pandas as pd $ pd.set_option('display.max_colwidth', -1)
print(np.info(np.subtract.outer))
twitter_count = pd.DataFrame(twitter_count, columns=['base_twitter_count'])
dfp.groupby("congress").bill_type.count().plot(kind='bar') $ plt.xlabel('Bills per Session')
data.columns
data.info()
patient_times.columns = ['patient', 'start_time', 'stop_time']
df_inds.to_csv('../datasets/technical_indicators.csv')
X.drop(["userid","recentTime","createdtm"], axis=1, inplace=True)
pdf.loc['2016-1-1':'2016-3-31'].plot() $
nfl.interest_over_time()
dfWordsEn = package.run('WordsEn.dprep', dataflow_idx=0) $ dfFirstNames = package.run('FirstNames.dprep', dataflow_idx=0) $ dfBlackListWords = package.run('WordsBlacklist.dprep', dataflow_idx=0)
1/np.exp(result4.params)
ab_data.converted.value_counts()
df2.info()
print(len(X.columns)) $ print(len(y_resampled))
for k in range(len(random_hospitals)): $     print("{0:3}  {1:3} ".format(random_hospitals[k],list_of_hospitals[random_hospitals[k]])) $     bubble_single_site_single_year((list_of_hospitals[random_hospitals[k]]),year,871,'svg') $
potential_accounts_buildings_info_tbrr.head()
f_ip_app_clicks = spark.read.csv(os.path.join(mungepath, "f_ip_app_clicks"), header=True) $ print('Found %d observations.' %f_ip_app_clicks.count())
tweet1.user.description
fairshare.drop(columns = 'GROUP', inplace = True) $ fairshare.rename(columns = {'ACCOUNT': 'Group'}, inplace = True)
s = requests.Session() $ s.get(url, headers=headers)
infinity.head()
for (user_id, (user_info, count)) in user_actions_with_profiles.take(10): $     print user_id, count, user_info
autos['registration_month'].describe()
close_day2 = [] $ for day in data: $     close_day2.append(day[4])   $ close_day2.pop(0)  $
sumvalues = youthUser3.cityName.value_counts() $ sumvalues.to_csv('lrng/sumvalues.csv') $ type(sumvalues)
data.loc[:'Utah', 'two']
results = log_reg.fit() $
files8.EndDate=pd.to_datetime(files8.EndDate) $ files8.StartDate=pd.to_datetime(files8.StartDate) $ files8.head()
orgs[orgs.primary_role == 'company'].shape
import matplotlib.pyplot as plt $ import seaborn as sns $ sns.set_style('whitegrid') $ %matplotlib inline
(df.where(df['SUMLEV']==50) $     .dropna() $     .set_index(['STNAME','CTYNAME']) $     .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))
close_prices = df['Close'] $ close_prices.head()
with open('data/chefkoch_03.json') as data_file:    $     chef03 = json.load(data_file) $ clean_new(chef03) $ chef03df = convert(chef03) $ chef03df.info()
results = pd.DataFrame(rf_gridcv.cv_results_) $ results.sort_values(by='rank_test_score', ascending=False, inplace=True) $ results.head()
MyList5 = np.array(MyList) $ %timeit MyList6 = np.sort(MyList5) $ MyList6 = np.sort(MyList5) $ MyList6[:5]
df_sites = df_sites.sort_values(['disc_times_pay'], ascending=[True]) $ df_sites = df_sites.reset_index(drop=True) $ df_sites.head()
total = sessions_sample.shape[0] $ top_actions = pd.DataFrame(data=sessions_sample.action.value_counts()) $ top_actions = top_actions.reset_index().rename(columns={"index":"action", "action":"count"}) $ top_actions["cumul_pct"] = top_actions["count"].cumsum() / total $ list_top_actions = top_actions.loc[0:19,"action"].tolist()
m.reset_index() $ m.head()
treatment_converted = df2[(df2.group == "treatment") & (df2.converted == 1)].shape[0] $ total_treatment = df2[(df2.group == "treatment")].shape[0] $ p_treatment_convert = treatment_converted/total_treatment $ p_treatment_convert
clicking_conditions = pd.merge(left = df_click, right = users_conditions, left_on = 'user_session', right_on = 'user_id') $ clicking_conditions.drop(['user_session', 'experiment_id'], axis = 1, inplace = True) # to-do: can also remove action_label, action_type
z_score,p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],alternative = "smaller") $ print(z_score, p_value)
df_merged.hist(column='rating_numerator',bins=[0,1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]);
np.min(lon), np.max(lon)
hdf.sort_index(inplace=True, $               level=[0, 1]) $ hdf.index.lexsort_depth # both levels are sorted against
groupby_charge = pmol.df.groupby(['atom_type'])['charge'] $ groupby_charge.mean().plot(kind='bar', yerr=groupby_charge.std()) $ plt.ylabel('charge') $ plt.show()
l=test_scores.select('score').rdd.map(lambda x: x.score).collect()
tips.ndim $ len(tips) $ tips.shape
std_df = choose_local_df('STD') $ std_df.loc[std_df['Sold_to_Party']=='0000101348'][['Sold_to_Party','Sales_document','Material_Description','Unit_Price','Document_Date']]
wrd_clean['floofer'].value_counts()[:10]
pd.DataFrame(lostintranslation.credits()['crew'])[["job", "name", "id"]].head()
sm = morning_rush.iloc[:5][['longitude', 'latitude']] $ sm
plt.hist(p_diffs) $ plt.axvline(x=actual_diff, color='r', linewidth=2);
com_grp.head(2)
output.head()
calls_nocontact_simp = calls_nocontact.drop(columns=['ticket_id', 'issue_description', 'city', 'state', 'location', 'geom']) $ calls_nocontact_simp.head()
dict(list(result.items())[:20])
s2.loc["bob"]
df2.query("landing_page == 'new_page'")['landing_page'].count() / df2.shape[0]
data.loc[((data.surface_total_in_m2 > 70000) & (data.property_type == "apartment")), 'surface_total_in_m2'] = np.NaN
co.steady_states.tail()
(0.190/2)
df.tail(50)
dataset=scaler.inverse_transform(dataset) $ close_day0_actual=dataset[len(dataset)-1] $ close_day30_predicted=testPredict[len(testPredict)-1] $ return_predict=((close_day30_predicted-close_day0_actual)/close_day0_actual)*100 $ print(return_predict)
treatment_mean = df2.query('group == "treatment"')['converted'].mean() $ print(treatment_mean)
to_be_predicted_Day3 = 52.42539737 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
((df-df_from_csv)**2 < 1e-25).all()
pd.DataFrame(data['data']['children'])    # will give us the kind of data as well as the subreddit id
intersections_final_for_update.info()
twitter_dataset=twitter_dataset.drop(columns=['stage'])
user_extract.describe()
tweets["created_at"] = pd.to_datetime(tweets["created_at"]) $ tweets.head()
print(rf_gd.best_params_, rf_gd.best_score_)
preds = m.predict(val) $ m.score(trn, y_trn), m.score(val, y_val), m.oob_score_, exp_rmspe(preds, y_val)
events = df.sort_values(['game_date', 'at_bat_number'], ascending=True).groupby(['atbat_pk']).last().reset_index() $ events = events[['atbat_pk', 'events', 'woba_value']]
pk_planes = pk_planes.distinct()
logit_mod = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'CA', 'UK']]) $ results = logit_mod.fit() $ results.summary()
subwaydf['4HR_Entries'].nlargest(20)
guineaDf = pd.concat([guineaCases,guineaDeaths],axis=1) $ guineaDf.index.name = 'Date' $ guineaDf.head()
plt.scatter(pca_df['PC1'],pca_df['PC2'], c = day_of_week, cmap='rainbow'); $ plt.colorbar();
df2[df2.duplicated(['user_id'],keep=False)]
george = relevant_data[relevant_data['User Name'] == 'George Liu'] $ df = george['Event Type Name'].value_counts() $ df_george = pd.Series.to_frame(df) $ df_george.columns = ['George L'] $ df_george
df.query('group=="control"').query('landing_page=="new_page"')['user_id'].nunique()+df.query('group=="treatment"').query('landing_page=="old_page"')['user_id'].nunique()
from sklearn.preprocessing import StandardScaler $ ss = StandardScaler() $ Xs = ss.fit_transform(X) $ type(Xs) $ Xs
print(jan_2015_frame.shape) $ print(jan_2015_groupby.shape)
df.index
rural_ride_total = rural_type_df.groupby(["city"]).count()["ride_id"] $ rural_ride_total.head()
merge[merge.columns[38]].value_counts()
print(norm.ppf(1-(0.05))) # Tells us what our critical value at 95% confidence is
sqlContext.sql("select count(person) from pcs").show()
df['seqid'].unique()
liberia_data3=liberia_data3.fillna(0)
plt.pie(count_driver, labels=type_driver,explode=explode, colors=colors, $         autopct="%1.1f%%", shadow=False, startangle=140) $
tweets = df["text"].values
top_10_authors = git_log['author'].value_counts().head(10).to_frame() $ top_10_authors
txt = txt.set_index('Date') $ txt = txt.sort_index()
classify_df = classify_df.drop(['Neutered','Female','Intake Type_Euthanasia Request','Intake Type_Public Assist'],axis=1) $ classify_df = classify_df.drop(['Intake Condition_Aged','Intake Condition_Feral','Intake Condition_Other'],axis=1) $ classify_df = classify_df.drop(['Intake Condition_Sick','Intake Condition_Pregnant'],axis=1)
import datalab.bigquery as bq $ import numpy as np $ import shutil
df.head()
pd.read_sql_query("SELECT id,last_name FROM person WHERE id > 2;", conn, index_col="id")
df_h1b_mv[df_h1b_mv.pw_1>700000][['pw_1','lca_case_wage_rate_from','lca_case_wage_rate_to']]
def drop_table(cur, table_name): $     cur.execute(sql)
df2 = df.drop(remove_index) $ print(df2.shape)  # This should be 294478 - 3893 $ df2.head()
subwaydf.iloc[122449:122454] #this low number seems to be because entries and exits resets
p1_true=twitter_archive_master[twitter_archive_master['p1_dog']==True] $ p1_true.p1.value_counts().plot(kind='pie');
df2.converted.mean()
nnew = df2.query('landing_page == "new_page"').shape[0] $ nnew
X_expense= pd.DataFrame(df[['Total_Operating_Expenses','Maintenance_Expense','Taxes_Expense', $                             'Insurance_Expense','Utilities_Expense','Payroll_Expense']]) $ col_corr =X_expense.corr() $ sns.heatmap(col_corr, annot=True) $ sns.plt.title('Heatmap of Correlation Matrix')
X_train, X_test, y_train, y_test = train_test_split(X_s, y_sc, test_size=0.5, random_state=42)
austin= austin.set_index('RIDE_ID')
ekos.import_default_data(user_id, workspace_alias = workspace_uuid)
def save_model(model, country): $     filename = '{}_pickle.sav'.format(country) $     pickle.dump(model, open(filename, 'wb')) $     print('saved as', filename)
df.head()
df_clean.drop(df_clean[df_clean['retweeted_status_id'].notnull()].index,inplace=True)
uniqueArtists = userArtistDF.select("artistID").distinct().count() $ print("Total n. of artists: ", uniqueArtists) $
xml_in_sample.shape
datetime.datetime.strptime(df_TempJams['timeStamp'][0],"%Y-%m-%d %H:%M:%S")
new_page_converted = np.random.choice([1, 0], n_new, (p_new, 1-p_new))
df2 = df2.drop(df[(df.group == 'treatment') & (df.landing_page == 'old_page')].index)
new_obj = obj.drop('c')
df_final = titanic_class_sex_age.survived.mean() $ print(df_final) $ print(df_final.index.is_unique)
df.age.describe()
grid_clf.best_estimator_.feature_importances_
grouped_newsorgs = news_sentiment_df.groupby(["News Organization"], as_index = False)
ada_predict = adaboost.predict(testx)
from scipy.stats import norm $ z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $ z_score, norm.ppf(1-(0.05/2)), p_value
fig, ax = plt.subplots(figsize=(14, 6)) $ varlabel = ts_df.columns[0] $ ts_df[varlabel].plot(style='-', ax=ax) $ ax.set_ylabel(varlabel) $ ax.set_title("{} | {}".format(dataset['id'], dataset['title']));
expiry = datetime.date(2015, 1, 17) $ aapl_calls = aapl.get_call_data(expiry=expiry) $ aapl_calls.iloc[0:5, 0:4]
my_gempro.set_representative_sequence() $ print('Missing a representative sequence: ', my_gempro.missing_representative_sequence) $ my_gempro.df_representative_sequences.head()
dump_svmlight_file(X=train_data.drop(['y_no', 'y_yes'], axis=1), y=train_data['y_yes'], f='train.libsvm') $ dump_svmlight_file(X=validation_data.drop(['y_no', 'y_yes'], axis=1), y=validation_data['y_yes'], f='validation.libsvm') $ dump_svmlight_file(X=test_data.drop(['y_no', 'y_yes'], axis=1), y=test_data['y_yes'], f='test.libsvm') $ boto3.Session().resource('s3').Bucket(bucket).Object(prefix + '/train/train.libsvm').upload_file('train.libsvm') $ boto3.Session().resource('s3').Bucket(bucket).Object(prefix + '/validation/validation.libsvm').upload_file('validation.libsvm')
conn.columninfo(table=dict(name='banklist', caslib='casuser'))
1/np.exp(-0.0150),np.exp(0.0506),np.exp(0.0408) $
lm = smf.ols(formula='sales ~ TV + radio + newspaper + TV*radio', data=data).fit() $ lm.params
plt.hist(deaths['deaths'], bins=20, range=(1000, 3000), edgecolor='black') $ plt.xlabel('Deaths in Month') $ plt.ylabel('Number of Months') $ plt.show()
rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]
df2 = pd.DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))
g8_groups.agg(['mean', 'std'])
features = [col for col in train.columns if not col == 'Close'] $ X = train[features] $ y = train['Close'] $ etr = RandomForestRegressor() $ cross_val_score(etr, X, y).mean()
df_clean.columns
import numpy as np; np.random.seed(8) $ mean, cov = [4, 6], [(1.5, .7), (.7, 1)] $ x, y = np.random.multivariate_normal(mean, cov, 80).T $ ax = sb.regplot(x=x, y=y, color="g")
active_listing_count_bin = Binarizer(10000) $ active_listing_count_bin.fit(x_train['Active Listing Count '].values.reshape(-1,1)) $ training_active_listing_dummy = active_listing_count_bin.transform(x_train['Active Listing Count '].values.reshape(-1,1))
from lifetimes.plotting import plot_probability_alive_matrix $ plot_probability_alive_matrix(bgf)
historicalPriceC = pdr.DataReader('C', 'yahoo', "2005-01-01", "2015-10-06")
pd.isna(df1)
df.index.day
psy_df = dem.merge(QUIDS_wide, on='subjectkey', how='right') # I want to keep all Ss from QUIDS_wide $ psy_df.shape
df.registerTempTable("dataframe_name")
crsr.columns(table="diet")
index = pd.MultiIndex.from_product([['a', 'c', 'b'], [1, 2]]) $ data = pd.Series(np.random.rand(6), index=index) $ data.index.names = ['char', 'int'] $ data
df['DAY_OF_WEEK'] = df['DATE'].dt.dayofweek
dataset = pd.read_csv('fashion-mnist_train.csv') $ dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly $ num_classes = 10 $ classes = {0: "T-shirt/top", 1:"Trouser", 2: "Pullover", 3:"Dress", 4:"Coat", 5:"Sandal", 6:"Shirt", 7:"Sneaker", 8:"Bag", 9:"Ankle boot"} $ display(dataset.head())
sanfran.tables
df_protest.info()
print(f'total_open_users: {len(open_users)}') $ print(f'total_pass: {len(open_users_test[open_users_test["TEST"]])}') $ print(f'total_fail: {len(open_users_test[~open_users_test["TEST"]])}') $ print(f'failed_names: {open_users_test[~open_users_test["TEST"]]["USERNAME"].values}')
stars.value_counts().sort_index()
df_goog.index + timedelta(hours=1) $ df_goog.index + timedelta(days=3) $ df_goog.index - timedelta(days=4380, seconds=43)
scr_activated_df.to_csv('activation_paid_518.csv')
LetsGetMeta.__class__
dc.head()
churn_df.size
df.groupby(['landing_page', 'group']).count()
speakers.reset_index(inplace = True)
df2.query('group == "control" and landing_page != "old_page"').count()
df2[df2.user_id == int(ruid)] 
for temp_col in temp_columns: $     dat[temp_col]=LVL1.hampel(dat[temp_col], k=7) #remove outliers with hampel filter $     dat[temp_col]=LVL1.basic_median_outlier_strip(dat[temp_col], k=8, threshold=4, min_n_for_val=3) #remove remaining outliers; spikes where val exceeds 2-hr rolling median by 4C    
pd.Timestamp('2018-01-01') + Hour(3)
all_cards.sample(10)
df_train.head()
writers.groupby('Country').last()
b56 = serc_reflArray[:,:,55].astype(float) $ print('b56 type:',type(b56)) $ print('b56 shape:',b56.shape) $ print('Band 56 Reflectance:\n',b56)
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
print(lyra_lightcurve.data.keys())
plt.show()
discConvpct = discConvpct.filter(like='Closed Won').rename('discConversionPercent').to_frame().reset_index()
os.listdir('../input')
import statsmodels.api as sm $ convert_old = control_df.query('converted == 1').user_id.nunique() $ convert_new = treatment_df.query('converted == 1').user_id.nunique() $ n_old = control_df.user_id.nunique() $ n_new = treatment_df.user_id.nunique()
a = np.random.randn(50, 600, 100) $ a.shape
pd.crosstab(calls_df['call_time'], calls_df['call_type'], margins=True)
github_data = pd.concat([pd.read_csv(filename) for filename in glob.glob('../data/raw/github_data/*')])
y_test_under[k150_bets_under].mean()
af.to_csv('../data/assigned_files.csv')
records3 = records.copy()
tvec.fit(X_train) $ tvec_df  = pd.DataFrame(tvec.transform(X_train).todense(), $                    columns=tvec.get_feature_names(), $                    index=[X_train])
from scipy.stats import norm $ norm.cdf(z), norm.ppf(.95)
pprint.pprint(treaties.find_one({"reinsurer": "AIG"}))
df.to_pickle(data_file_path + 'convo_df.pkl') $ print("... saved as pickle")
game_info.head(3)
seasons_and_teams = ALLbyseasons.groupby(["Category", "Team"]) # Groups by specific season/offseason, then by team
basic_model = Sequential() $ basic_model.add(LSTM(500, input_shape=(X_train_vals.shape[1], X_train_vals.shape[2]))) $ basic_model.add(Dense(500)) $ basic_model.compile(loss='mean_absolute_error', optimizer='adam')
df.to_json(data_file_path+'convo_df.json') $ print("... saved as json")
data_df.info()
tweets.describe()
details.shape
df2[df2['landing_page']=='new_page']['converted'].mean()
lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])
df[df['Descriptor'] == 'Loud Music/Party']['Created Date'].groupby(df[df['Descriptor'] == 'Loud Music/Party'].index.dayofweek).count().plot(kind='bar')
df_usa =df1[df1['Area']=='United States of America'] $ df_usa=df_usa.set_index(df_usa['Year']) $ df_usa.head()
df2_new['intercept']=1 $ logit2=sm.Logit(df2_new['converted'],df2_new[['intercept','ab_page','CA','UK']]) $ result2=logit2.fit() $ result2.summary()
twitter_archive.info()
autos['price'] = autos['price'].str.replace(',','').str.replace('$','').astype(int) $ autos['odometer'] = autos['odometer'].str.replace(',','').str.replace('km','').astype(int) $ autos.rename({"odometer": "odometer_km"}, axis=1, inplace=True) $ autos.head() $
%%sql mysql://admin:admin@172.20.101.81 $ drop user if exists 'user1'@'%';
en_es = pd.read_csv(BASE + "/en-es-clean.txt", sep=" ") $ en_es.head()
!mkdir ./data
len(train_data.id.unique())
print("The number of unique users in the dataset is:  " + str(df['user_id'].nunique()))
logistic_mod_pagecountry = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK','US']]) $ results_pagecountry = logistic_mod_pagecountry.fit() $ results_pagecountry.summary()
posGroups = list(pos_tweets.group_id_x) $ num_convos = len(set(posGroups)) $ print(f'Working with {num_convos} conversations') $ companyPos = filtered[filtered.group_id.isin(posGroups)]
df_tweet = pd.DataFrame(list_tweets_je) $ df_tweet.head(10)
sum(contractor.city.isnull())
date.strftime('%A')
days_w_beer = len(df.created_at.dt.date.value_counts()) $ days_total = df.created_at.dt.date.max() - df.created_at.dt.date.min() $ days_wo_beer = days_total.days - days_w_beer # currently unused $ print('Out of a total {} days using Untappd, I have had at least one beer {} days.'.format( $                                             days_total.days, days_w_beer))
pd.Timestamp(datetime(2018, 1, 1))
freq = Counter(p for o in tok_trn for p in o) $ freq.most_common(25)
stocks_pca_m=stocks_pca_m[~np.isnan(stocks_pca_m).any(axis=1)]
m.plot_components(forecast);
oldPageTreat=((df["landing_page"]=="old_page") & (df["group"]=="treatment")) $ newPageCont=((df["landing_page"]=="new_page") & (df["group"]=="control")) $ display((np.sum(oldPageTreat))+(np.sum(newPageCont)))#total number of mismatches
df.reset_index(inplace=True)
temp["time"] = pd.to_datetime(temp["time"])
events_df['event_hour_with_utcoffset'] = events_df.apply(lambda r: (r['event_time'] + datetime.timedelta(seconds=int(r['utc_offset']))).hour,axis=1)
df_new['UK_and_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new['intercept'] = 1 $ logit_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK', 'ab_page', 'UK_and_ab_page']]) $ results = logit_mod.fit() $ results.summary()
new_df = pd.merge(tweets_df, stock_data, on='Time')
sns.heatmap(rhum_us)
ex_index=df.index.isin([5,12,23,56]) $ df[~ex_index].head(10)
n_old = df2.query('landing_page == "old_page"') $ len(n_old)
w_change = w - w.shift(1) $ w_change[coins_infund].T.sum().plot() $ plt.title('Total weight change per month') $ plt.show()
predictions = forest.predict(test_data_features) $ output = pd.DataFrame( data={"id":test["id"], "rating":predictions} )
df.groupby("two_day_reminder_profile_incomplete_sent")["cancelled"].mean()
d = feedparser.parse('https://oglobo.globo.com/rss.xml')
ethPrice['close'].plot(figsize=(16, 12))
df.head()
print (df_country.head(), df2.head())
train_data = prepare_data(intervention_train_df).drop(drop_columns, axis=1)
logit = sm.Logit(df2['converted'], df2[['intercept', 'treatment']]) $ results = logit.fit() $
df_joined.country.value_counts()
result=logit.fit() $ result.summary2() 
Sort2 = stores.sort_values(by = ["Location","TotalSales"])
url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ base_url = "https://astrogeology.usgs.gov" $ hemisphere_image_urls = []
xgb_learner.best_params
df.isnull().any()
x_data, y_data = (df["Year"].values, df["Value"].values) $ plt.plot(x_data, y_data, 'ro') $ plt.ylabel('GDP') $ plt.xlabel('Year') $ plt.show()
retweets_of_me = api.retweets_of_me()
sns.heatmap(ndvi_coarse, vmin = -0.1, vmax = 0.703)
df_G.val = np.random.rand(7).round(1) $ df_G $ pd.get_dummies(pd.cut(df_G['val'], 3, labels=list('XYZ')), prefix='dummy')
giss_temp.index = giss_temp.index.astype(np.int32)
varieties = dataset['variety'].tolist() $ wine_count = Counter(varieties) $ nr_reviews = len(varieties) $ threshold = 200
sum(twitter_archive_clean['expanded_urls'].isnull())
absorption.tallies
df = spark.read.csv('../NYC311/nyc311_sample.csv', inferSchema=False, header=True) $ df.printSchema()
oppstage = segmentData[['lead_mql_status', 'opportunity_month_year', 'opportunity_stage']].pivot_table( $                 index=['lead_mql_status', 'opportunity_month_year'], $                 columns='opportunity_stage', aggfunc=len, fill_value=0 $             )
price_dict = price_data.to_dict()
df['price'].max()
import re $ times = re.findall(r"\d\d:\d\d", questions_text) $ times = [str(x) for x in times] $ times.sort() $ print(times)
X_train, X_test, y_train, y_test = train_test_split(X_svd, $                                                     y_encode, $                                                     test_size=0.2, $                                                     random_state=42) $
autos = autos[autos["price_$"].between(100,1000000)]
n_old = df2.query('landing_page == "old_page"').landing_page.count()
df2.fillna("",inplace=True)
quandl.ApiConfig.api_key = API_KEY $ quandl.get('FSE/AFX_X', start_date='2018-08-08', end_date='2018-08-08')
(df2['landing_page'] == 'new_page').sum()/unique_users_2 $
y_preds = svc_grid.best_estimator_.predict(X_test) $ svc_scores = show_model_metrics('SVM', svc_grid, y_test, y_preds)
index_missin_hr0to6_before2016 = taxi_hourly_df.loc[(taxi_hourly_df.missing_dt == True) & $                                                     (taxi_hourly_df.index.hour.isin((0, 1, 2, 3, 4, 5, 6))), : $                                                    ].index
pipeline = Pipeline(stages = [dt]) $ model = pipeline.fit(training_data)
graffiti['created_year'] = graffiti['created_year'].replace(2015, 1) $ graffiti['created_year'] = graffiti['created_year'].replace(2014, 1) $ graffiti['created_year'] = graffiti['created_year'].replace(2013, 1) $ graffiti['graffiti_count'] = graffiti['created_year']
del room_temp.temperature
profit_calculator(stock.iloc[1640:], 'true_grow',-1)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old,n_new], alternative='smaller') $ print(z_score, p_value)
stories.head()
predictions = knn.predict(test[['expenses', 'floor', 'lat', 'lon', 'property_type',\ $                                 'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']])
trips1 = df1.pivot("day","city_id","trips") #pivot trips to set cities to columns $ trips1.reset_index(inplace=True) #collapse index $ trips2 = df2 $ trips2.columns=['day','rouk'] #rename column appropriately $ trips3 = pd.merge(trips1, trips2, on='day', how='left') #join data
df_schools.describe()
treatment = df2[df2["group"] == 'treatment'] $ treatment_conv = treatment[treatment["converted"] == 1] $ treatment_conv_prob = treatment_conv.shape[0]/treatment.shape[0] $ treatment_conv_prob
df_concat_2["message_likes_rel"] = (df_concat_2.likes_total/df_concat_2.page_likes)*1000000 $ df_concat_2.head()
    idx = payments_total_yrs[ (payments_total_yrs['id_num']==330101)].index.tolist()[0] $     idx $     payments_total_yrs.loc[0,'id_num'] $
pd.Timestamp('now', tz='UTC')
cat_features = ['NTACode', 'manager_id'] $ from sklearn import preprocessing $ for cat in cat_features: $     cat_le = preprocessing.LabelEncoder() $     train[cat] = cat_le.fit_transform(train[cat])
df_protest.groupby('type')
learner.load_encoder('adam3_20_enc')
df_list_rand.head()
df2['duplicated'] = df2.user_id.duplicated() $ df2.query('duplicated == True')[:-1]
pd.concat([cust_demo.head(3),cust_new.head(3)]).fillna('')
t = 4 $ m_test['predicted_purchases'] = bgf_test.conditional_expected_number_of_purchases_up_to_time(t, m_test['frequency'], m_test['recency'], m_test['T']) $ m_test.sort_values(by='predicted_purchases').tail(15)
print("Percentage of positive tweets: {}%".format(len(pos_tweets)*100/len(data['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets)*100/len(data['Tweets']))) $ print("Percentage of negative tweets: {}%".format(len(neg_tweets)*100/len(data['Tweets'])))
df.shape
users.loc[users['verified'] == True, 'verified'] = 1 $ users.loc[users['verified'] == False, 'verified'] = 0 $ users.loc[users['verified'] == 'True', 'verified'] = 1 $ users.loc[users['verified'] == 'False', 'verified'] = 0
train_holiday_oil_store_transaction_item = train_holiday_oil_store_transaction.join(items, 'item_nbr', 'left_outer') $
to_be_predicted_Day1 = 81.95 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new
df3.groupby(['STATION', 'DATE']).sum()
url_NOTCLEAN1A = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/NOTCLEAN1A.csv" $ df_NOTCLEAN1A = pd.read_csv(url_NOTCLEAN1A,sep=',')
data.dtypes
ds = web.DataReader("5_Industry_Portfolios", "famafrench") $ print(ds['DESCR'])
stationCount = func.count(Measurement.station) $ session.query(Measurement.station, stationCount).group_by(Measurement.station).order_by(stationCount.desc()).all() $
run txt2pdf.py -o "YALE-NEW HAVEN HOSPITAL  Sepsis.pdf"   "YALE-NEW HAVEN HOSPITAL  Sepsis.txt"
predictSVR = svr_rbf.predict(np.concatenate((X_train,X_validation,X_test),axis=0)) $ predictANN = ANNModel.predict(np.concatenate((X_train,X_validation,X_test),axis=0))#one can ignore this line if ANN is $
es = elasticsearch.Elasticsearch(es_url)
df.corr()   # checking the correlation of the features
to_be_predicted_Day5 = 38.29379229 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df_pol_d.shape
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_axial_chord_crown[1][0])) $ perr = np.sqrt(np.diag(pcov_axial_chord_crown[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
bp.rename(columns={"value1num":"systolic", "value2num":"diastolic"},inplace=True) $ bp.head()
bymin = pd.Series(np.arange(0, 90*60*24), pd.date_range('2014-08-01', '2014-10-29 23:59:00',freq='T')) $ bymin
def get_integer6(s): $     return notRepairedDamage_list.index(s)
train_dum = pd.get_dummies(TrainData, columns=['Source', 'Customer_Existing_Primary_Bank_Code'], $                           dummy_na=True, sparse=True, drop_first=True) $ test_dum = pd.get_dummies(TestData, columns=['Source', 'Customer_Existing_Primary_Bank_Code'], $                           dummy_na=True, sparse=True, drop_first=True)
minDimension = min(min(dataSet1.shape), min(dataSet2.shape)) $ randU, randS, randVt = randomized_svd(fullProduct, n_components=minDimension)
save_model('model_random_forest_v1.mod', random_forests_grid)    
has_missing_values = df.isnull().values.any() $ print("Yes, It has" if has_missing_values else "No, It has not")
train_df9 = train_df8.copy() $ bathroom_dummy = pd.get_dummies(train_df9.bath_categorical) $ train_df9 = pd.concat([train_df9, bathroom_dummy], axis = 1) $ del train_df9['bath_categorical'] $ train_df9.head()
recortados = [recortar_tweet(t) for t in tweets_data_all] $ tweets = pd.DataFrame(recortados)
df.to_csv(r'C:\Users\LW130003\cgv.csv', index=False)
import os $ os.sys.path.append(os.path.abspath('.')) $ %reload_ext autoreload $ %autoreload 2
model.compile(optimizer='RMSprop', loss='mse')
result['UTC_create_date'] = result['CreateDate'].apply(str_to_utc) $ result['UTC_modify_date'] = result['ModifyDate'].apply(str_to_utc)
df['reviw'] = df['review'].apply(preprocessor)
findNum = re.compile(r'\d+') $ for i in range(0, len(postsDF)): $ 	print(findNum.findall(postsDF.iloc[i,0]))
ideas = categorical.join(ideas)
team_names['regular_occurrences'] = [c[name.lower()] for name in names] $ team_names.sort_values(['regular_occurrences'],ascending=False).head()
CARDINALITY_SINGLE_VALUED = 1 $ CARDINALITY_MULTI_VALUED = 2 $ metric_series_type_ids = [metric['id'] for metric in response.json()['items'] $                           if metric['cardinality']['id'] == CARDINALITY_MULTI_VALUED]
logit_mod = sm.Logit(df_new2['converted'], df_new2[['intercept', 'ab_page', 'UK_pages']]) $ results = logit_mod.fit() $ results.summary()
greater_than_p_diff = [i for i in p_diffs if i > p_diff] $ less_than_zero = [i for i in p_diffs if i < 0] $ print("number of elements less than zero is {} of the total {}".format(len(less_than_zero), len(p_diffs)))
(details['Average Rating'] == 0).value_counts()
height.apply(np.sqrt)
X_test = X_test.loc[:,[col for col in X_test.columns if col not in ['Footnote','CountyName','Month']]]
v = voters[['E1_110816', 'E2_060716', 'E3_110414','E4_060314', 'E5_110612', 'E6_060512']] $ v.columns = pd.MultiIndex.from_product([v.columns, ['vote']]) $ v = v.stack(0) $ v.index.set_names('election', level=1, inplace=True)
Precipitation,=plt.plot(df_d['date'], df_d['Prcp'],label='precipitation') $ plt.tight_layout() $ plt.show()
extract_all['system.created_by'].values
df.loc[d[1]]
response_cpi_all = requests.get(url_cpi_all)
df2[df2['group']=='control']['converted'].mean()
hc.sql('create table asm_wspace.final_400hz_2017q4 as select * from tmp_400hz_usage_final')
df_daily = df_daily.reset_index() $ df_daily.head()
df_predictions_clean.p2_dog.value_counts()
train[train.id.isin(session.id.unique())].country_destination.value_counts()
sub_dataset.groupby(["NewsDesk", "Popular"]).size()
model.userFeatures().first()
tweet_archive_clean.drop(['stages'], axis=1, inplace=True)
source_paths = { $     'medical_conditions_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/MCQ_D.XPT', $     'weight_history_questionnaire': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/WHQ_D.XPT', $     'demographics_with_sample_weights': 'https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT' $ }
mean_for_each_weekday = delineate_in_weekdays.mean().reset_index() $ mean_for_each_weekday
before = np.array(tweets_df[tweets_df['Date'] == "2017-11-12"]['Sentiment']) $ during = np.array(tweets_df[tweets_df['Date'] == "2017-11-13"]['Sentiment']) $ after = np.array(tweets_df[tweets_df['Date'] == "2017-11-14"]['Sentiment'])
os.system("mv %s %s"%("departureZIP.csv", os.getenv("PUIDATA"))) $ os.system("mv %s %s"%("destinationZIP.csv", os.getenv("PUIDATA"))) $ departureZip = pd.read_csv('http://cosmo.nyu.edu/~fb55/data/departureZIP.csv') $ destinationZip = pd.read_csv('http://cosmo.nyu.edu/~fb55/data/destinationZIP.csv')
subway2_df['datetime'] = subway2_df[['DATE','TIME']].apply(lambda x: ' '.join(x), axis=1) $ subway2_df['datetime'] = pd.to_datetime(subway2_df['datetime'])
station_availability_df['dock_id'].plot(kind='hist', logx = True, logy=True) $ plt.show() $ station_availability_df = station_availability_df.groupby('dock_id').filter(lambda x: len(x) >= 1440) $ station_availability_df['dock_id'].plot(kind='hist', logx = True, logy=True) $ plt.show()
conn.close()
df.head(1)
ax = sns.barplot(x= "countPublications", y = "countPublications", data = data_final, palette='rainbow', $                  estimator = lambda countPublications: len(countPublications) / len(data_final) * 100) $ ax.set(ylabel="% of authors with a given number of publications") $ ax.set(xlabel="Number of publication by author") $ ax.set(xlim=(0, 20))
test_bow = vect_bow.transform(test) $ test_tfidf = vect_tfidf.transform(test)
df_proj_agg = df_proj[['ProjectId','DivisionName','UnitName','Borough','Sponsor','PhaseName', $  'ProjectType', 'Priority', 'DesignContractType', 'ConstructionContractType', $  'SourceSystem', 'MultipleFMSIds', 'DesignFiscalYear','ConstructionFiscalYear']].drop_duplicates()
documents[0] $ dataset.filenames
from statsmodels.stats.diagnostic import acorr_ljungbox $
round_to_1_decimal = lambda x:round( x,1) $ cum_sum_percentage_payments = list(np.cumsum(percent_of_total_list)) $ cum_sum_percentage_payments = list(map(round_to_1_decimal, cum_sum_percentage_payments)) $ cum_sum_percentage_payments[:10]
image_predictions_clean[image_predictions_clean.jpg_url=='https://pbs.twimg.com/media/CkjMx99UoAM2B1a.jpg'] $
from scipy.stats import norm $ z_crit = norm.ppf(0.95) $ print(z_crit)
df1= pd.read_csv('Monthly_data_cmo.csv') $ df2=pd.read_csv('CMO_MSP_Mandi.csv')
ioDF.columns.tolist()
aru_url = "https://opendata.arcgis.com/datasets/c3c0ae91dca54c5d9ce56962fa0dd645_68.csv" $ aru_file = os.path.join(DATA_DIR, "address_residential_unit.csv")
country_df = c_df[['COUNTRY','CUSTOMER_ID_CAT']] $ country_df = country_df.drop_duplicates(keep='first') $ country_df.groupby('COUNTRY').size().reset_index(name='counts')
tizibika['date'] = [d.date() for d in tizibika['time_date']] $ tizibika['time'] = [d.time() for d in tizibika['time_date']] $ tizibika['hour']= tizibika.time_date.dt.hour $
matchinglist = getmatchinglist() $ matchinglist.head()
train_groupped.loc[page].iloc[np.arange(0,10)]
df = pd.read_csv('./data/train.csv') $ cats = df.project_subject_categories $ cats.sample(10)
import nltk $ from nltk.corpus import stopwords
hr.plot(x="realtime", y="value1num")
merged2['Specialty'].isnull().sum(), merged2['Specialty'].notnull().sum(), merged2.shape
filter_android(orig_tweets)[filter_android(orig_tweets)['text'].str.contains("\?")]
Test_extra = Test.merge(test_dum_clean, right_on='ID', left_index=True)
df_stars_count = df_stars.groupby('stars').size() $ print(df_stars_count)
def select_source_cells(sources, target, N_syn): $     target_id = target.node_id $     source_ids = [s.node_id for s in sources] $     nsyns_ret = [N_syn]*len(source_ids) $     return nsyns_ret
import pandas as pd $ commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq='AS')).count() $ print(commits_per_year.head())
def twitter_setup(): $     auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET) $     auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) $     api = tweepy.API(auth) $     return api
plt.rcParams['figure.figsize'] = [15, 5]
ctb_model.fit(X, y) $ submissionctb =  pd.DataFrame(  ctb_model.predict_proba(test_X)[:,1], index = test['comment_id'] , columns = ["is_fake"] ) $ submissionctb.to_csv('submissionctb_Cat.csv')
s_nonulls = s.dropna() $ s_nonulls
author_data = pd.DataFrame.from_csv(AUTHOR_DATA_FILE_PATH) $ author_data.head()
df = pd.DataFrame(['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D'], $                   index=['excellent', 'excellent', 'excellent', 'good', 'good', 'good', 'ok', 'ok', 'ok', 'poor', 'poor']) $ df.rename(columns={0: 'Grades'}, inplace=True) $ df
room_temp.__repr__ = lambda : 'lol2'
df['word_count'] = df['word_count'].round(2)
Vycnn = np.array(Vy)
print("Actual difference is" , p_diff) $ p_greater_than_diff = len(greater_than_diff) / len(p_diffs) $ print('Proportion greater than actual difference is', p_greater_than_diff) $ print('That is {}%'.format(p_greater_than_diff*100))
date.strftime("%A")
%%bash $ head md_traffic.json
print(cbow_m1.wv.similarity('men', 'women')) $ print(cbow_m3.wv.similarity('men', 'women')) $ print(cbow_m4.wv.similarity('men', 'women')) $ print(cbow_m5.wv.similarity('men', 'women'))
f1 = w.get_data_filter_object(subset = subset_uuid, step=1) $ f1.include_list_filter
top_apps2 = df_nona.groupby('app_id').district_id.nunique() $ top_apps2.sort(inplace=True, ascending=False) $ top_apps2.head(10)
top = final.groupby('text').agg({'tweetCount':'sum', 'latitude':'first', 'longitude':'first', 'text':'first'}) $ top = top.sort_values('tweetCount', ascending=False)[0:50]
pizza_poor_train,pizza_poor_test = train_test_split(pizza_poor_reviews,0.9)
speakers = pd.DataFrame(speakers)
valid_scores = valid_scores[valid_scores['home team 41 game win%'].notnull() & $                 valid_scores['away team 41 game win%'].notnull()] $ print(valid_scores.shape) $ valid_scores.head()
dti2 = pd.to_datetime(['Aug 1, 2014', 'foo']) $ type(dti2)
np.exp(-0.0150)
*/30 * * * * python3 api_checks.py
tweets_df.text.describe()
pd.Series(data=y10_train).value_counts()
df_2013.dropna(inplace=True) $ df_2013
df[df['Complaint Type'] == 'Illegal Fireworks'].sort_index().resample('M').count().plot(y='Complaint Type')
[ [ rsp.json()['data'][ vid ]['stat'][fid] for fid in fields_stat] for vid in vid_list_return] 
flight_hex = h2o.H2OFrame(flight_pd)
sessions_summary.columns
offset.rollback(d)
!wget https://raw.githubusercontent.com/jacubero/ColabEnv/master/awk_netstat.sh -O awk_netstat.sh $ !chmod +x awk_netstat.sh $ !./awk_netstat.sh
df2.user_id.count()
len(purchase_history.product_id.unique())
hourly_dat=pd.DataFrame()
a = np.random.randn(300, 3)  #Create a 300x3 matrix of standard normal variates. $ a.std(axis=0)  #or np.std(a, axis=0)
x_test = test_data.values
import time $ from sklearn.model_selection import RandomizedSearchCV
plt.figure(figsize=(15,8)) $ ax = sns.boxplot(x = 'Type 1', y = 'Total', data = pokemon ) $ plt.show()
bad_tc_e_isbns = pd.read_table("C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/TC electronic ISBNs to exclude.txt") $ bad_tc_e_isbns = bad_tc_e_isbns['ISBN'] $ bad_tc_e_isbns.size
data.groupby('SA')['RTs'].sum().plot(kind = 'barh')
department_df_sub.apply(np.sum, axis = 0) # apply function to each column 
df = pd.DataFrame({ $     'Date': $         ['2017-05-18','2017-05-18','2017-05-22','2017-05-22','2018-05-19','2017-05-24','2017-05-23','2017-05-21','2017-05-23','2017-05-23'], $     'News':[news1, news2 , news3 ,  news4 , news5 , news6 , news7,  news8, news9, news10], 'Autor':['Tsvetana Paraskova', 'Zainab Calcuttawala' , 'Tsvetana Paraskova' ,  'Zainab Calcuttawala' , 'Tsvetana Paraskova' , 'Laura Hurst , Nayla Razzouk , and Grant Smith' , 'Catherine Traywick  and Jennifer A Dlouhy',  'Javier Blas  and Jack Farchy', 'Kadhim Ajrash  and Khalid Al Ansary', 'Zainab Calcuttawala']})
Y_train.shape
enc_wgts = to_np(wgts['0.encoder.weight']) $ row_m = enc_wgts.mean(0)
with shelve.open('result/vars1') as db: $     db['obama'] = Obama_raw[['created_at', 'text']] $     db['trump'] = Trump_raw[['created_at', 'text']] $     db['obama_raw'] = Obama_raw[['created_at', 'text','replies', 'retweets', 'favorites']] $     db['trump_raw'] = Trump_raw[['created_at', 'text','replies', 'retweets', 'favorites']]
for key in sorted(eligible_by_week.keys()): $     print("Week age {0}: {1} authors ".format(key, len(eligible_by_week[key])))
max = session.query(func.max(Measurement.tobs)).\ $     filter(Measurement.station == "USC00519281").all() $ print(f"Highest Temp: {max}")
image_pred = pd.read_csv('image-predictions/image-predictions.tsv',sep='\t') $ image_pred.head()
logit_mod = sm.Logit(df2.converted, df2[['intercept','ab_page']])
print d.variables['time']
df_train = pd.read_csv('data/df_1.csv') $ df_test = pd.read_csv('data/df_1_test.csv')
df['times_contacted'].value_counts()
@udf(returnType=StringType()) $ def main_endpoint(endpoint): $     return '/'.join(endpoint.split('/')[:2])
pickup_demand.head()
validation.analysis(observation_data, BallBerry_resistance_simulation_0_5)
watch_table = watch_table.rename(columns={'created_at':'date_starred', 'login':'user_login'}) $ watch_table['date_starred'] = watch_table['date_starred'].apply(pd.to_datetime)
usecols=[0,6,21,30,31,32,33,34,58,59,60] $ nitrogen = nitrogen.iloc[:,usecols] $ nitrogen.columns
churned_unordered_end_date = [churned_unordered.loc[bid,'canceled_at'][np.argmax(churned_unordered.loc[bid,'scns_created'])] if churned_unordered.loc[bid,'cancel_at_period_end'][np.argmax(churned_unordered.loc[bid,'scns_created'])] == False else churned_unordered.loc[bid,'current_period_end'][np.argmax(churned_unordered.loc[bid,'scns_created'])] if churned_unordered.loc[bid,'cancel_at_period_end'][np.argmax(churned_unordered.loc[bid,'scns_created'])]==True else None for bid in churned_unordered.index]
neg = [x['neg'] for x in blah] $ pos = [x['pos'] for x in blah] $ neu = [x['neu'] for x in blah] $ snt = [x['compound'] for x in blah] $ op_sent = pd.DataFrame({'usr_neg': neg, 'usr_pos': pos, 'usr_neu': neu, 'usr_snt': snt})
unique_users = ab_data["user_id"].nunique() $ unique_users
s[s.between(11, 14)].head()
psy_native.shape
os.path.exists(filepath)
df.head(5)
baseball_newind[baseball_newind.ab>500]
ex4.dropna(axis = 1)
joined_samp['AfterStateHoliday']=joined_samp['AfterStateHoliday'].fillna(0) $ joined_samp['BeforeStateHoliday']=joined_samp['BeforeStateHoliday'].fillna(0)
sl_data1 = sl_data[sl_data.Description.isin(['Total new cases registered so far', $                                                          'New deaths registered'])] 
to_be_predicted_Day1 = 48.57 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
students.weight.value_counts(ascending = True)
autos['brand'].value_counts(normalize=True)
reddit_comments_data.groupBy('created_date').count().orderBy('created_date').show(100)
targetarticles_files_list = glob.glob('/home/dave/datapubmed/targetarticles/PMC*.nxml') $ targetarticles_files_list = [i[40:] for i  in targetarticles_files_list  ] $ targetarticles_files_list = [i[:-5] for i  in targetarticles_files_list  ] $ print(targetarticles_files_list[:5]) $ print(len(targetarticles_files_list))
print("Number of Groups in Enterprise ATT&CK") $ groups = lift.get_all_enterprise_groups() $ print(len(groups)) $ df = json_normalize(groups) $ df.reindex(['matrix', 'group', 'group_aliases', 'group_id', 'group_description'], axis=1)[0:5]
mention_df = base_mention_df.join(user_summary_df['gender'], on='user_id') $ mention_df.count()
print(df_weather.shape,col_totals.shape,df_weather1.shape)
b_cal_q1.shape
conn.getsessopt('caslib')
fdist.hapaxes()
to_be_predicted_Day4 = 43.44784915 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
!wc -l *.csv
area=df_usa['Area']*10 $ df_usa['Area']=area $ df_usa.head()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
df['event_life'] = df['event_life'].map(lambda x: 0 if np.isnan(x) else x) $ df.info()
violation_counts = restaurants["VIOLATION DESCRIPTION"].value_counts(); $ violation_counts[0:10]
big_df_avg.head()
paired_df_grouped['best_co_occurence'] = paired_df_grouped.apply(lambda x: x['all_co_occurence'][:rule_of_thumb(x['top10'])], axis=1) $ del paired_df_grouped['top10'] $
rfc = ensemble.RandomForestClassifier() $ rfc.fit(X_train, y_train) $ print('Training set score:', rfc.score(X_train, y_train)) $ print('\nTest set score:', rfc.score(X_test, y_test)) $ print('\nCross Val score:',cross_val_score(rfc, X_test, y_test, cv=5))
test['date'].min(), test['date'].max(), train['date'].min(), train['date'].max()
df2.shape
df_DRGs.loc[46,'DRG_name']
df_average = df.groupby(['business_id']).mean() $ df_average.head()
categorised_df.head(5)
1/np.exp(-0.0149)
pd.merge(staff_df, student_df, how='inner', left_index=True, right_index=True)
broadcast_list_sc = sc.broadcast(broadcast_list)
prob = df2['converted'].mean() $ print("The probability of an individual converting is - {}".format(prob))
df.to_csv(file_name, index=False,header=False)
df_l.groupby(["landing_page","group"]).count() #verifying the values in the new dataset
print(plan.keys())
%config SqlMagic.autopandas=True
import scipy $ p_values_one_sided = scipy.stats.norm.cdf(abs(zstat_score)) #cumulative distribution function $ p_values_two_sided = scipy.stats.norm.sf(abs(zstat_score))*2 #twosided $ critical_value = scipy.stats.norm.ppf(1-(0.05)) $ print("The Pvalue for Two sided for alternate Hypothesis is {}".format(p_values_two_sided))
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23Australia').items(5000): $     results.append(tweet) $ print(len(results))
merged_df_flight_cancels.head()
class_test =  class_test.iloc[19:, :]
df['word_count'] = df['body'].apply(lambda x: len(str(x).split(" "))) $ df[['body','word_count']].head()
print(">>> Printing the vector of 'inc': {} ...".format(model['inc'][:10])) $ print(">>> Printing the similarity between 'inc' and 'love': {}"\ $       .format(model.wv.similarity('inc', 'love'))) $ print(">>> Printing the similarity between 'inc' and 'company': {}"\ $       .format(model.wv.similarity('inc', 'company')))
old=df2.query('landing_page=="old_page"') $ old_samp=old.sample(old.shape[0],replace=True) $ old_page_converted=old_samp.query('converted==1')['user_id'].count()/old_samp['user_id'].count() $ old_page_converted
ax = by_day['category_name'].count().plot() $ ax.set_xlabel('Time') $ ax.set_ylabel('Weekly average of new job postings')
with open("train_df_dict.json", "r") as fd: $     train_df_dict = json.load(fd) $ with open("test_df_dict.json", "r") as fd: $     test_df_dict = json.load(fd)
n_old=df2.query('landing_page=="old_page"').count()[0] $ n_old
quarters.asfreq("A")
pd.set_option('display.max_colwidth', -1)
autos["registration_year"].describe()
fh_3 = FeatureHasher(num_features=uniques.iloc[0, 1], input_type='string', non_negative=True) $ %time fit3 = fh_3.fit_transform(train.device_ip)
df.values
print(scoresdf['time_taken'].sum())
df_1 = pd.read_csv('tweet_csvs/realDonaldTrump_tweets.csv', index_col = None, header = 0, $                      parse_dates=['created_at'], infer_datetime_format = True, dayfirst = True)
segmented_rfm = rfmTable $ segmented_rfm.head()
cFrame.reset_index(inplace=True)  $ cFrame.head(20)
data['strategy'] = data['position'].shift(1) * data['returns'] $ data[['returns', 'strategy']].sum()
pd.read_sql(f'explain {sql_sub}', engine).head()
df2 = df2.drop_duplicates(subset='user_id', keep="last")
series + pandas.Series({'a': 2})
dtypes={'date':np.str,'store_nbr':np.int64,'class':np.str,'family':np.str,'sum_unit_sales': np.float64,'no_items':np.float64,'no_perishable_items':np.float64,'items_onpromotion':np.float64,'dcoilwtico':np.float64,'city':np.str,'state':np.str,'type':np.str,'cluster':np.str,'transactions':np.float64} $ parse_dates=['date'] $ class_merged = pd.read_csv('class_merged_excl_hol.csv', names=['date','store_nbr','class','family','sum_unit_sales','no_items','no_perishable_items','items_onpromotion','dcoilwtico','city','state','type','cluster','transactions'],skiprows=1) # opens the csv file $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
f_app_hour_clicks = spark.read.csv(os.path.join(mungepath, "f_app_hour_clicks"), header=True) $ print('Found %d observations.' %f_app_hour_clicks.count())
item_hub.drop(item_hub_dropper, axis =1 , inplace = True)
daily_returns.head()
ol.uuid
df2[df2['landing_page'] == 'new_page'].count()/df2.shape[0]
sessions_summary["language"].fillna("unk").value_counts(dropna=False)
print('admit: {} rows and {} columns'.format(*admit.shape)) $ print('claims: {} rows and {} columns'.format(*claims.shape))
for row in my_df_large.itertuples(): $     pass $
match.iloc[:,:11].head()
result = api.search(q='%23wonderbread') #%23 is used to specify '#' $ len(result)
close = df.reset_index().pivot(index='ticker', columns='date', values='adj_close') $ volume = df.reset_index().pivot(index='ticker', columns='date', values='adj_volume') $ ex_dividend = df.reset_index().pivot(index='ticker', columns='date', values='ex-dividend')
movies = movies.dropna(axis=1)
p_diffs=[] $ p_diffs = np.random.binomial(n_new, p_null, 10000)/n_new - np.random.binomial(n_old, p_null, 10000)/n_old   
toy['encrypted_customer_id'].unique()[0:10]
empty_sample.iloc[np.r_[0:10, len(empty_sample)-10:len(empty_sample)]]
df.head(2)
test.iloc[40:100]
df_treat = df2[df2['group'] == 'treatment'] $ df_treat['converted'].mean()
mean = daily_returns['SPY'].mean() $ print(mean) $ std = daily_returns['SPY'].std() $ print(std)
%timeit [name.endswith("bacteria") and value > 1000 for (name,value) in zip(data.phylum.values, data.value.values)]
upper = df2_no_outliers.mean() + df2_no_outliers.std()*3 $ lower = df2_no_outliers.mean() - df2_no_outliers.std()*3 $ df2_no_outliers.loc[df2_no_outliers['y'] > upper['y'], 'y'] = None $ df2_no_outliers.loc[df2_no_outliers['y'] < lower['y'], 'y'] = None
ax = normals_df.plot(kind='area', stacked=False, title='Vacation Weather') $ ax.set_ylabel('Temperature (F)') $ plt.xticks(np.arange(len(dates_range)), dates_range, rotation=20) $ plt.savefig("Resources/vacation_weather.png") $ plt.show()
print "# All done."
print(testObjDocs.__doc__)  # note: formatting is messed up if you do not use print() on the doc string
prec_long_df.describe()
purchases.to_csv('./cleaned/purchases.csv', header=False, index=False)
dat['local_time']=pd.to_datetime(dat[Local_time_column_name], format= date_format) $ dat=dat.set_index('local_time') #set this local time as the index
data = pd.DataFrame(data=[tweet.text for tweet in results], $                     columns=['Tweets'])
plans_set = set() $ plans,counts = np.unique([(['Free-Month-Trial'] + p if p[0] != 'Free-Month-Trial' else p) for p in BID_PLANS_df['scns_array']  ],return_counts = True)
score, acc = model.evaluate(x_test, $                             y_test, $                             batch_size=batch_size) $ print('Test score:', score) $ print('Test accuracy:', acc)
from pixiedust.display import * $ display(result_df)
datAll['zip'] = datAll.blk_rng.map(dat_hcad_zip.set_index('blk_range').zip)
df.to_pickle('r_politic.pickle')
news_tweets_pd = news_tweets_pd[["User", "Tweet Count", "Tweet Text", "Date", "Compound", "Positive", "Neutral", "Negative"]] $ news_tweets_pd.head()
Multiplication = df.loc['Total']['AveragePrice'] * df.iloc[18248]['AveragePrice'] $ print(Multiplication)
sum_ratio = absorption_to_total + scattering_to_total $ sum_ratio.get_pandas_dataframe()
tab.insert_one(json.loads(a.T.to_json()))
with open('/Users/bellepeng/Desktop/Metis/Projects/Project_AirBNB/data/sentiments.pkl', 'rb') as file: $     sentiments = pickle.load(file)
df_new[['CA','US']] = pd.get_dummies(df_new['country'])[['CA','US']] $
excelDF['Ship Mode'].count()
exiftool -csv -createdate -modifydate cisnwe5/Cisnwe5_cycle4.MP4 > cisnwe5.csv
tweet_archive_master = tweet_archive_clean.merge(image_predictions_clean, on='tweet_id').merge(tweet_json_clean, on='tweet_id')
df2.info()
users['Registered'] = pd.to_datetime(users['Registered']) $ users['Cancelled'] = pd.to_datetime(users['Cancelled']) $ sessions['SessionDate'] = pd.to_datetime(sessions['SessionDate']) $ transactions['TransactionDate'] = pd.to_datetime(transactions['TransactionDate'])
max_change_1day = max((v.High - v.Low) for v in data.values()) $ print('=>The maximum change in a day was {:.2f} ' .format(max_change_1day))
house = elec['fridge'] #only one meter so any selection will do $ df = house.load().next() #load the first chunk of data into a dataframe $
archive_clean.drop(['in_reply_to_status_id','in_reply_to_user_id','expanded_urls'],axis=1,inplace=True )
a.shape
cursor = fs.find({"filename" : "scansmpl.pdf"}).limit(3) $ print "Found", cursor.count(), "documents" $ for doc in cursor: $     print doc.uploadDate
autos['registration_year'].describe()
ben_final['isReverted'].value_counts()
from azureml.core import Workspace $ subscription_id = "b1395605-1fe9-4af4-b3ff-82a4725a3791" $ resource_group = "meetup_aml_rg" $ workspace_name = "meetup_aml_workspace" $ workspace_region = 'eastus2' # or eastus2euap
train['Title'] = train.apply(lambda row: row[0].split(" ")[0], axis = 1) $ train['Gender'] = train.apply(lambda row: row[0].split(" ")[1].split("G")[0], axis = 1) $ test['Title'] = test.apply(lambda row: row[0].split(" ")[0], axis = 1) $ test['Gender'] = test.apply(lambda row: row[0].split(" ")[1].split("G")[0], axis = 1) $ train.head(3)
round(1/np.exp(-0.0099), 2), round(1/np.exp(-0.0507), 2)
git_blame = pd.read_csv("C:/Temp/linux_blame.gz") $ git_blame.head()
result = pd.DataFrame(list(results), columns=['tags', 'values']) $ result
database = "pidata"                # e.g. "pidata" $ hostname = "172.20.101.81"         # e.g.: "mydbinstance.xyz.us-east-1.rds.amazonaws.com" $ port = 3306                        # e.g. 3306 $ uid = "pilogger"                   # e.g. "user1" $ pwd = "foobar"                     # e.g. "Password123"
tables = pd.read_html(facts_url) $ df = tables[0] $ df.head()
df['Date'] = pd.to_datetime(df['Date']) $ df.set_index('Date', inplace=True)
print("State space samples:") $ print(np.array([env.observation_space.sample() for i in range(10)]))
new_converted = np.random.choice([1, 0], size=nnew, p=[pmean, (1-pmean)]) $ new_converted.mean()
fin_df['totalAssets'].corr(fin_df['totalRevenue'])
stack_score.mean()
df.query(('group=="control"' and 'landing_page=="new_page"') and ('group=="treatment"' and 'landing_page=="old_page"')).shape[0]
dataset.head(3)
ocsvm_cleaned_tfidf.fit(trump_cleaned_tfidf, y = y_true_cleaned_tfidf) $ prediction_cleaned_tfidf = ocsvm_cleaned_tfidf.predict(test_cleaned_tfidf) $ prediction_cleaned_tfidf
print(list(autos["vehicle_type"].unique())) $ print(list(autos["gearbox"].unique())) $ print(list(autos["fuel_type"].unique()))
df_Left_trans_users = pd.merge(transactions,users,how="left",on="UserID") $ df_Left_trans_users
pd.Timestamp('Jan 1 2018')
prob_temp = df2.query("group == 'control'")["converted"].mean() * 100 $ print("Probability if user in control group: {}%".format(round(prob_temp, 2)))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key='+API_KEY) $ fse = r.json() $ fse
testmx2.sum(axis = 0 )
tz_pytz = pytz.timezone('Europe/London')
test_data = load_df('data/test.csv')
url= "https://mars.nasa.gov/api/v1/news_items/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest" $ news = requests.get(url) $ news = news.json() $ most_recent = news.get('items')[0] $ most_recent_b = most_recent['body'] $
sentiment_df.plot(x="Tweets Ago", y='Compound',title='News Mood Analysis', kind='scatter') $ plt.show()
print('The dataset contains {} tweets.'.format(strNb(len(tw))))
lr = 1e-3 $ learn.fit(lr, 20, cycle_len=1, use_clr=(10,10))
max(dif_dict, key=dif_dict.get) 
new_page_convert_rate=df2.query('converted==1 and landing_page=="new_page"').user_id.nunique()/n_new $ old_page_convert_rate=df2.query('converted==1 and landing_page=="old_page"').user_id.nunique()/n_old $ obs_diff=new_page_convert_rate-old_page_convert_rate $ print(obs_diff)
yhat = clf.predict_proba(X_transform)[:,1] $ auc = metrics.roc_auc_score(y, yhat) $ print('{:0.3f} - AUROC of model (training set).'.format(auc))
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ mydata = pd.read_csv(path, sep ='\s+', na_values=['.']) $ mydata.head(5)
df_enhanced.groupby('name')['name'].size()
station_count.loc[(station_count['Count'] >= 2000)]
import gensim $ import numpy as np $ numpy_matrix = np.random.randint(10, size=[5,2]) $ corpus = gensim.matutils.Dense2Corpus(numpy_matrix) $ numpy_matrix_dense = gensim.matutils.corpus2dense(corpus, num_terms=10)
train.head(10)
df_goog.head()
timestamp = tweets['created_at'].values $ ts = [pd.Timestamp(t).tz_convert('US/Eastern') for t in timestamp] $ tweets.index = ts
!wget https://www.hydroshare.org/django_irods/download/2474e3c1f33b4dc58e0dfc0824c72a84/data/contents/ogh_meta.json $ !wget https://www.hydroshare.org/django_irods/download/2474e3c1f33b4dc58e0dfc0824c72a84/data/contents/ogh.py
X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.3, random_state=62)
speakers.to_json('speakers.json')
df_clean.info()
ttDaily = (ttTimeEntry.groupby(mainkey+['DATE']) $            .ENTRIES.first().reset_index()) $ ttDaily.head()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
df2.rx_requested.value_counts()
archive_df_clean = archive_df.copy()
if not os.path.exists('new_data_files'): $     os.mkdir('new_data_files') $ records.to_csv('new_data_files/Q2.csv')
weekdays = [] $ for date in logins['datetime']: $     weekdays.append(date.weekday()) $ logins['weekday'] = weekdays
locationing.corr()
test = datatest[datatest['floor'].isnull()] $ train = datatest[datatest['floor'].notnull()]
most_confident_predictions.head()
soup = BeautifulSoup(data.text, 'html.parser') $ records = soup.find_all('span', attrs={'class':'short-desc'}) $ print(f'{len(records)} found')
taxi_hourly_df.index = pd.DatetimeIndex(taxi_hourly_df.index)
df["2015-06":"2015-08"]['Complaint Type'].value_counts().head(5)
lq = pd.read_csv('../Assets/Iowa_Liquor_sample.csv',parse_dates=['Date'],infer_datetime_format=True)
df_user_extract_copy.info()
mention_pairs = mentions[["id","screen_name","mention"]].groupby(["screen_name","mention"]).agg({"id":"count"}).rename(columns={"id":"Weight"}) $ mention_pairs.reset_index(inplace=True) $ mention_pairs.head()
df_test_attempt.to_csv("BaseTestAttemptsPSE")
df_group_by2 = copy.copy(df_group_by)
column = ['US'] $ df_new2 = df_new.drop(column, axis=1) $ df_new2.head()
(dta.violations $  .str.extract("(\d+)(?=\.)", expand=False) $  .astype(int))
m = merged_df[['customer','frequency','recency','T']].set_index('customer') $ print(m.shape) $ m.head() $
data.fillna(method='bfill')
article_urls = ['http://www.nhm.ac.uk/discover/the-cannibals-of-goughs-cave.html','http://www.nhm.ac.uk/discover/how-we-became-human.html','http://www.nhm.ac.uk/discover/the-origin-of-our-species.html'] $
sns.tsplot(time=cum_cont.date, data=cum_cont.cum_contr)
grid_id_name_rand = 'gbm_random' $ gbm_hyperparams_rand = {'learn_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1], $                 'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10], $                 'ntrees': [50, 100, 200, 500, 1000]}
consumer_key = config.consumer_key $ consumer_secret = config.consumer_secret $ access_token = config.access_token $ access_token_secret = config.access_token_secret
def wind_chillf(temp, wind, round_n=2): $     wc = 35.74 + 0.6215 * temp - 35.75 * (wind ** 0.16) + 0.4275 * temp * (wind ** 0.16) $     return round(wc, round_n)
austin[austin['distance_travelled'].isnull()].index.tolist()
plt.ylabel("Price of BTC") $ plt.xlabel("Date") $ plt.xticks(x0[::150]) $ plt.show()
a = dat['police district'].unique() $ a.sort() $ a
print('\nThe current directory is:\n' + color.RED + color.BOLD + os.getcwd() + color.END) $ os.chdir(Base_Directory)
p_old = df2.converted.mean() $ print('The convert rate for old_page under the null is: {}.'.format(round(p_old, 4)))
conn = engine.connect() $ conn.execute("select * from stations ").fetchall()
wb_list = df_step1.VISS_EU_CD.unique() $ print('number of waterbodies in step 1: {}'.format(len(wb_list))) $ typeA_list = [row.split('-')[0].strip().lstrip('0') for row in df_step1.WATER_TYPE_AREA.unique()] $ print('number of type areas in step 1: {}'.format(len(typeA_list))) $
autos["registration_year"].describe()
from sklearn.svm import LinearSVR $ lin_svr = LinearSVR(random_state=42) $ lin_svr.fit(X_train_scaled, y_train)
a_list = [1.0, 'one', '1', 100, 1, 1.0, 1, 'two', 2, 2.0]
!head data/goog.csv $
df, err = ek.get_data(['GOOG.O','MSFT.O', 'FB.O'], $                       [ 'TR.Revenue','TR.GrossProfit']) $ df
df_o = df2.query('landing_page == "old_page"') $ df_o.shape[0]
import matplotlib.pyplot as plt $ import matplotlib.image as mpimg $ img=mpimg.imread('C://Files//man.png') $ plt.axis("off") $ imgplot = plt.imshow(img) $
import sklearn $ from sklearn.svm import LinearSVC $ classifier = LinearSVC(verbose=1) $ classifier.fit(X_train, y_train)
print df.shape $ just_apps = df[df.is_application == 'Application'] $ print just_apps.shape $ just_apps.head(5)
result['timestampCorrected'].describe()
countries['country'].value_counts()
print('Total number of error records: {}'.format(len(errors.index))) $ errors.head()
import numpy as np $ Serial_No = [] $ for i in range(len(tweets)): $     Serial_No.append(i) $ tweets['Serial'] = np.array(Serial_No)
sample.head()
C = pd.merge(A,B, on = 'team', how = 'left').drop_duplicates(['pts','ast']) $ C[C.team.duplicated()]
sp.sort_values('Date', inplace = True) $ sp.reset_index(drop= True, inplace = True)
ridge1 = linear_model.Ridge(alpha = 0) $ ridge1.fit(x3, y) $ (ridge1.coef_, ridge1.intercept_)
MICROSACC.plot_densities(microsaccades)
df['price'] = df['price'].astype(float) $ df.info()
ABT_tip.to_csv('text_preparation/abt_text_analysis_with_tips.csv')
pizza_counts.head()
for index, row in im_clean.iterrows(): $     assert("-" not in row["p1"]) $     assert("-" not in row["p2"]) $     assert("-" not in row["p3"])
df.head()
df_rand = unclassified(df).sample(1000) $ df_rand = df_rand[df_rand.postcodes.notnull()] $ num_pc_entries = len(df_rand) $ print("This run has {} random entries containing postcode information.".format(num_pc_entries))
stops_window.head()
df_clean.rating_denominator = df_clean.rating_denominator.astype(int)
%%file setup.py $ from setuptools import setup $ setup(name='my_package', $       version='0.0.1')
lm3 = sm.Logit(ab_df_new['converted'], ab_df_new[['intercept','CA','UK','treatment_US','treatment_CA']]) $ result3 = lm3.fit() $ result3.summary2()
df_2015['sales_jan_mar']=[y if ((x.month >=1) & (x.month <=3)) else 0.0 for x, y in zip(df['Date'],df['sale_dollars'])]
new_user_project = user_project.drop_duplicates(subset=['user_id', 'project_id'], keep='first') $ new_user_project.groupby(['user_id','project_id']).count().head()
legos['inventory_parts'].shape, lego_color_cat.shape
def pairwise_squared_distances(y_i: torch.Tensor): $     squared_distances_ijk = (y_i[:, None, :] - y_i) ** 2 $     pairwise_squared_distances_ij = squared_distances_ijk.sum(2)    $     return pairwise_squared_distances_ij $ pairwise_squared_distances(torch.from_numpy(y_i))
df = pd.DataFrame([tweet.text for tweet in tweets], columns=['Tweets'])
df.loc[df.sex.isin(sex2NA),'sex']=np.nan $ df.loc[df.sex.isin(sex2m),'sex']='m' $ print(df.sex.loc[df.sex.isin(sex2NA)==True].count()) $ print(df.sex.loc[df.sex.isin(sex2m)==True].count())
n_conversion = df[df['converted'] == 1] # number of conversions $ unique_users_converted = n_conversion['user_id'].nunique() # number of unique users converted $ proportion_converted = ((unique_users_converted/n_unique_users)*100) $ print ("The proportion of users converted is: {:.2f}%".format(proportion_converted))
twitter_ar.head(2)
prices = pd.DataFrame(index=df.index)
twoWordReviews["Improvements_lemma"].head(20)
elms_all.ORIG_DATE.max()
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2], keys=["df1", 'df2'])
countries_df = pd.read_csv(r'C:\Users\Hank2\Desktop\Code_Nano\Python\ABtest\AnalyzeABTestResults 2./countries.csv') $
with open('df_3.out','w') as outFile: $     pickle.dump(fullDf,outFile)
stocksRdd = sc.textFile("stocks") $ stocksRdd.cache().count()
df.head(2)
LR_yhat = LR_model.predict(test_X) $ LR_yhat_prob = LR_model.predict_proba(test_X) $ print("LR Jaccard index: %.2f" % jaccard_similarity_score(test_y, LR_yhat)) $ print("LR F1-score: %.2f" % f1_score(test_y, LR_yhat, average='weighted') ) $ print("LR LogLoss: %.2f" % log_loss(test_y, LR_yhat_prob))
cond = ((datatest.surface_covered_in_m2.isnull()) & (datatest.surface_total_in_m2.isnull())) $ rest = datatest[cond == False] $ train = rest[rest.surface_covered_in_m2.notnull()] $ rest = rest[rest.surface_covered_in_m2.isnull()] $ test = datatest[cond]
dir2df(cachedir, fnpat='\.ifc$', addcols=['mtime'])
fin_r_monthly = fin_r_monthly.iloc[:-1]
prices.head()
df.head(3)
vocab = {v: k for k, v in vectorizer.vocabulary_.items()} $ vocab
import pickle $ output = open('think_tanks.pkl', 'wb') $ pickle.dump(think_tank_party_dict, output) $ output.close()
sales = sales.sort(['sqft_living', 'price'])
df.head()
combined_city_df = pd.merge(city_data_df, ride_data_df, $                                  how='outer', on='city') $ combined_city_df.head(5)
tw_clean.to_csv("twitter_archive_master.csv")
merged.amount.sum()
pn_and_qty.columns
data_df.head()
df.to_json("json_data_format_columns.json", orient="columns") $ !cat json_data_format_columns.json
hr[hr["icustay_id"]==14882].plot(x="new charttime", $                                  y="value1num") $ hr[hr["icustay_id"]!=14882].plot(x="new charttime", $                                  y="value1num", color='red') #this is time series
groceries.loc['eggs']  # explicity using a labeled index
plt.figure() $ qplt.contourf(temperature_cube, brewer_cmap.N, cmap=brewer_cmap) $ plt.gca().coastlines() $ plt.show()
purchases = pd.read_csv('./raw_data/purchases.csv') $ purchases['Number of Add Ons'].fillna(0.0, inplace=True) $ purchases['Add On Amount'].fillna(0.0, inplace=True)
clf = RandomForestClassifier(n_estimators=10) $ clf = clf.fit(X_train,y_train) $ y_test_hat = clf.predict(X_test)
token_lemma = [token.lemma_ for token in parsed_review] $ token_shape = [token.shape_ for token in parsed_review] $ pd.DataFrame(zip(token_text, token_lemma, token_shape), $              columns=['token_text', 'token_lemma', 'token_shape'])
Jarvis_ET_Combine_Graph = Jarvis_ET_Combine.plot(color=['blue', 'green', 'orange']) $ Jarvis_ET_Combine_Graph.invert_yaxis() $ Jarvis_ET_Combine_Graph.scatter(xvals, df_gp_hr['Observation (aspen)'], color='black') $ Jarvis_ET_Combine_Graph.set(xlabel='Time of day (hr)', ylabel='Total evapotranspiration (mm h-1) ') $ Jarvis_ET_Combine_Graph.legend()
model_uid = client.repository.get_model_uid(model_details) $ print(model_uid)
data[data.author == 'Scaryclouds'].comment_body.head()
df['review'] = df['review'].apply(preprocessor)
autos.loc[autos['price'].between(0, 1e5), 'price'].describe()
train_geo.head()
pre_strategy_google.tail(1)
dfSummary = pd.concat([sumAll,sumPre,sumPost],axis=1) $ dfSummary.columns = ("all","before","after")
messy.to_csv('data/weather_yvr_cleaned.csv', index=False)
compulsory_fields = w.data_handler.physical_chemical.filter_parameters['compulsory_fields'] $ display_columns = compulsory_fields + use_parameters $ print(display_columns)
def get_integer4(s): $     return fuelType_list.index(s)
tcga_target_gtex_expression_hugo_tpm.head()
cig_data_SeriesCO.dtype
featured_image_url = soup.select_one("figure.lede img").get("src") $ print(featured_image_url)
facts_url = 'https://space-facts.com/mars'
df_h1b_mv.pw_1.describe()
conn.commit()
ttarc_clean[(ttarc_clean.doggo == 3) & (ttarc_clean.pupper == 2)].head() $
new_doc = "Human computer interaction" $ new_vec = dictionary.doc2bow(new_doc.lower().split()) $ print(new_vec)  # the word "interaction" does not appear in the dictionary and is ignored
df_model = pd.concat([df_h1b_ft_US_Y.pw_1,df_sector,df_subsector,df_industry,df_decision_days,df_employment_length_days,df_states],axis=1)
ts.index
values = [4, 56, 2, 45.6, np.nan, 23] # np.nan returns a null object (Not a Number) $ s = pd.Series(values) $ s
abc.head()
df.columns
final.isnull().any()
iter_visits = pd.read_csv(visits_path, iterator=True, chunksize=1000000) $ users_visits = Users.assign(visits=lambda x: pd.Timedelta(0, unit='d')) $ users_visits = last_visit(iter_visits, users_visits, 0)
to_be_predicted_Day2 = 31.33310125 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
features['f12'] = features['f07'].dropna().groupby(level='date').rank(pct=True) $
conn.rollback()
hours[5:8].start.plot() $ sns.lmplot(x='hour', y='start', data=hours[5:8], aspect=1.5, scatter_kws={'alpha':0.5})
geo_collect = collect.set_custom_filter(is_getlocated)
tweet_counts_by_hour = tweet_archive_master[['timestamp', 'favorite_count', 'retweet_count']] $ tweet_counts_by_hour.timestamp = tweet_counts_by_hour.timestamp.map(lambda x: x.hour)
autos['date_created'].str[:10].value_counts(normalize=True, dropna=False).sort_index(ascending=True)
df[0].replace(np.arange(3, 6), method='bfill')
import statsmodels.api as sm $ convert_old = df2.query('converted == 1 & group == "control"').count()[0] $ convert_new = df2.query('converted == 1 & group == "treatment"').count()[0] $ n_old = df2.query('group == "control"').shape[0] $ n_new = df2.query('group == "treatment"').shape[0]
van15_fin['reverted_mode'].value_counts()
closing = [] $ for day in dict_17: $     closing.append(dict_17[day]['Close']) $ largest_change = max(closing) - min(closing) $ largest_change
print(r.text)
noise['AFFGEOID'].dtypes
df_vow.head()
features = pd.merge(ind, noise_graf, on='AFFGEOID')
df_archive_clean.info()
squares.keys()
images.tail()
f, ax = plt.subplots() $ ax.set_ylim(ymax=1200); $ ax.set_xlabel('Shift duration [h]'); $ ax.set_ylabel('Count'); $ df['duration'].astype('timedelta64[h]').hist(bins=75, ax=ax);
dates_59th = dates_location.iloc[0:27] $ entries_59th = entries_location.iloc[0:27] $ plt.figure(figsize = (12,3)) # I made this bigger so I could read the x-axis labels better $ plt.plot(dates_59th, entries_59th);
df.loc['r1':'r2']
import statsmodels.api as sm $ convert_old = sum(df_control['converted']) $ convert_new = sum(df_treatment['converted']) $ n_old = df_control.shape[0] $ n_new = df_treatment.shape[0] $
tweets_df_clean.head()
daily_returns.hist(bins=20) $ plt.show()
data.columns
closing_values["2017-17"].corr()
ufos_df = spark_df.map(lambda x: Row(**dict(x.asDict(), year=int(x.Reports[0:4]))))
data.shape
df2.loc[df2['user_id'].duplicated()[df2['user_id'].duplicated() == True].index[0]]['user_id']
salesdec = pd.read_csv ("Video_Games_Sales_as_at_22_Dec_2016.csv") $ salesdec.head() $
user_table = df[['user_id','user_screen_name','user_created_at','user_location','user_description','user_verified','user_followers_count','user_statuses_count']] $ user_table.sort_values(by=['user_statuses_count'],ascending=False) $ user_table_1 = user_table.drop_duplicates(['user_screen_name'],keep='first') $ user_table_1.count()
tweets_master_df.iloc[tweets_master_df['favorite_count'].nlargest(10).index, :]
comments_by_club = [] $ for club in clubs: $     comments_by_club.append((club, words[words.subreddit == club]['body'])) $ comments_by_club[0]
new_page_converted= np.random.binomial(1, p_new, n_new)
preproc_reviews.shape # we have 761 reviews, which contain 189 words in total after preprocessing
full['Age'].hist(bins=26,figsize=(12,4)) $ plt.title('Distribution of Age',size=15) $ plt.xlabel('yrs old') $ plt.ylabel('count')
dfall['Sentiment_score'] = 0.0 $ dfall['weighted_sentiment_score'] = 0.0 $ dfall.info() $ df = dfall
df.head()
model = Sequential() $ model.add()
sum(df.Category.value_counts().unique())
dataset.head(20)
planets.head()
df_dd.shape $ df_dd.head()
random_walk.plot();
df_merged[['US', 'UK']] = pd.get_dummies(df_merged['country'])[['US','UK']] $ df_merged.head()
most_active = stations[0][0] $ record = [func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)] $ session.query(*record).filter(Measurement.station == most_active).first()
smpl_new = smpl_join.select("party_id").rdd\ $   .map(lambda x: predict_acct_id(x[0],x[1],x[2],x[3],x[4],x[5]))\ $   .distinct()
tweets = tweets.drop(axis= 1, labels=  ["lan"]) $ tweets.to_csv("dataframe_terror.csv", index=False)
content_performance_bytime['date'] = pd.to_datetime(content_performance_bytime['dimensions_date_id'])
df = pd.DataFrame({'temp':pd.Series(28 + 10*np.random.randn(10)), $                 'rain':pd.Series(100 + 50*np.random.randn(10)), $              'location':list('AAAAABBBBB')}) $ print(df.info())
news_outlets_handles = ['BBCNorthAmerica', 'CBSNews', 'CNN', 'FoxNews', 'nytimes'] $ news_tweets = [] $ for news_outlet in news_outlets_handles: $     for status in tweepy.Cursor(api.user_timeline, id=news_outlet).items(100): $         news_tweets.append(status)
print('min wavelength:', np.amin(wavelengths),'nm') $ print('max wavelength:', np.amax(wavelengths),'nm')
keys = list(specsJson.keys()) $ len(keys) $ print(specsJson[keys[1]])
pre_analyzeable.columns
import statsmodels.api as sm $ convert_old = df2.query('group == "control" and converted == 1').user_id.nunique() $ convert_new = df2.query('group == "treatment" and converted == 1').user_id.nunique() $ n_old = df2.query('group == "control"')['user_id'].nunique() $ n_new = df2.query('group == "treatment"')['user_id'].nunique()
qbfile = open("/home/midhu/DataScience/nlp/kaggle/all_question_pairs_train.csv","r") $ for aline in qbfile.readlines(): $     line=aline.strip().split(',') $     tq1.append(line[3]) $     tq2.append(line[4]) $
features.head(2)
dr_new = doctors[doctors['ReasonForVisitDescription'].str.contains('New')] $ dr_existing = doctors[~doctors['ReasonForVisitDescription'].str.contains('New')]
twitter_archive.rating_numerator.value_counts()
treatment_df = df2.query('group == "treatment"') $ treatment_pro = treatment_df.query('converted == 1').user_id.nunique() / treatment_df.user_id.nunique() $ treatment_pro
oil_pandas.isnull().sum()
from sklearn import linear_model $ from sklearn.linear_model import LogisticRegression $ logit = linear_model.LogisticRegression() $ logit.set_params(C = 1)
scoring.to_csv('scoring_{}.csv'.format(tod),encoding='UTF-8') $ loan_requests1.to_csv('loan_requests1_{}.csv'.format(tod),encoding='UTF-8') $ merkmale.to_csv('merkmale_{}.csv'.format(tod),encoding='UTF-8') $ loan_requests_indebtedness.to_csv('loan_requests_indebtedness_all_user_{}.csv'.format(tod),encoding='UTF-8')
cell = openmc.Cell(cell_id=1, name='cell') $ cell.region = +min_x & -max_x & +min_y & -max_y $ cell.fill = inf_medium
df_EMR_with_dummies = pd.concat([df_not_categorical, df_dummies], axis=1)
sentiments_pd.to_csv("Twitter_News_Mood.csv", index=False)
print(date_price.index)
BroncosBillsTweets.to_csv('BB_Tweets.csv')
df['PRCP'].plot(kind='hist', figsize=(15,8), alpha=0.5); $ plt.xlabel('Precipitation (in)') $ plt.ylabel('Number of Days')
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
df_app_prev['NAME_CONTRACT_STATUS'].value_counts()
df_c=pd.read_csv('countries.csv') $ df_c.head()
a.A /= a.A.max()
for df in (train,test): $     df["CompetitionMonthsOpen"] = df["CompetitionDaysOpen"]//30 $     df.loc[df.CompetitionMonthsOpen>24, "CompetitionMonthsOpen"] = 24 $ train.CompetitionMonthsOpen.unique()
df2 = df2.drop(dup_user.index)
ac['Monitoring Start'].groupby([ac['Monitoring Start'].dt.year]).agg('count')
target = wages['wage_per_hour'].values $ print(target.shape) $ target[0:6] $
convert_old = control[control['converted']==1].user_id.nunique() $ convert_new = treatment[treatment['converted']==1].user_id.nunique() $ n_old = treatment['user_id'].nunique() $ n_new = control['user_id'].nunique()
df_clean.info()
gMapAddrDat.getGeoAddr(40.699100, -73.703697, test=True)  ## function called by buildOutDF() $
p_old = df2['converted'].mean() $ printmd("**P*old***: {:0.4}".format(p_old))
tweets_original.info()
gdfNps.plot(color='green')
country_size=pd.value_counts(ac['Country'].values, sort=True, ascending=False) $ country_size.head()
ordered.head()
full['LOS'].plot(kind='box',figsize=(12,4),vert=False) $ plt.xlabel('days') $ plt.title('Distribution of length of stay',size=15)
nufission = xs_library[fuel_cell.id]['nu-fission'] $ nufission.print_xs(xs_type='micro', nuclides=['U235', 'U238'])
for date in trueDate: $     for comp in company: $         df[comp][date] = HMM(comp, date-timedelta(211), date-timedelta(1)) $ df.head(10)
genre = list() $ for x, i in enumerate(unprocessed_list): $     genre.append(unprocessed_list[x][5])
(fit.shape, fit2.shape, fit3.shape, fit4.shape)
from sklearn.tree import export_graphviz $ lt = ["low","medium","high"] $ predict = out_train[lt].idxmax(axis=1) $ target = {'high':0, 'medium':1, 'low':2} $ predict= np.array(predict.apply(lambda x: target[x]))
twitter_final.head(2)
contribs.groupby("committee_name")
clean_appt_df = appt_df.copy() $ clean_appt_df.describe()
df.head(2)
pp = rf.predict_proba(X_train) $ pp = pd.DataFrame(pp, columns=['The_Onion_Prob', 'VICE_Prob', 'GoldenStateWarriors_Prob'])
df3['ab_pageUs']=df3['ab_page']*df3['US'] #DFirst Interaction Term $ df3['ab_pageUK']=df3['ab_page']*df3['UK'] #Second Interaction Term $ lm=sm.Logit(df3['converted'],df3[['Intercept','ab_page','UK','US','ab_pageUs','ab_pageUK']]) $ results=lm.fit() $ results.summary2() $
plt.plot(x)
dfBTCVol.corr()
df_selparams.to_csv('../../data/raw/booth_sc.csv')
merged_visits = visited.merge(dta) $ merged_visits.head()
data['Incident Zip'] = data['Incident Zip'].apply(fix_zip)
df.head(10)
for m in data.media: $     if m == None: $         data["media"] = 'ext_link'
total_df['Rating'].plot.hist(bins = 8) $ plt.title('Overall Rating Distribution') $ plt.ylabel("Frequency") $ plt.xlabel("Rating")
stories.dropna(axis=1).shape
baseball.hr.rank()
df_2003.dropna(inplace=True) $ df_2003
df[['country','currency']].head()
conn.rollback() #After an erronious call, the connection needs to be rolled back before use again.
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ df['intercept'] = 1 $ df[['control','treatment']] = pd.get_dummies(df['group'])
np.std(p_diffs) $
old_page_converted = np.random.binomial(1, p_old, n_old) $ old_page_converted[:5]
rf_yhat = knn.predict(X_test)
learner= md.get_model(opt_fn, em_sz, nh, nl, $     dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4]) $ learner.metrics = [accuracy] $ learner.freeze_to(-1) # why are we unfreezing this one? Don't we want to unfreeze FIRST layer? $
contract_history.OPTION.fillna("NO", inplace=True)
full[['WillBe<=30Days']] = full[full['Readmitted'] == 1].groupby('Patient').shift(-1)[['<=30Days']].fillna(0).astype(np.int) $
ALL.shape # The final sample we are working with contains 14,395 transactions across eight teams.
resp = requests.get(stats_url)
type(my_town.polarity)
month_year_crimes = crimes.groupby(['year', 'month']).size() $ month_year_crimes
maxtobs = max(tobsDateDFclean.tobs) $ tobsDateDFclean.hist(bins=12)
expenses_df.pivot(index = "Type", columns = "Buyer", values = "Amount")
faa_data_destroyed_damage_pandas = faa_data_pandas[faa_data_pandas['DAMAGE'] == "D"] $ print(faa_data_destroyed_damage_pandas.shape) $ faa_data_destroyed_damage_pandas
autos["price"].unique().shape[0]
all_tweets.columns
full_weight_dict = {} $ for key, value in co_occurence_on_top50_slug.items(): $     for tup in value: $         full_weight_dict[key]={tup[0]: {'weight':tup[1]}}
X = pd.merge(X, latestTimeByUser, on="userid") $ X = pd.merge(X, uniqueCreatedTimeByUser , on ="userid") $ X.head(5)
acs_df = acs_df.set_index(['year', 'tract']).sort_index().reindex(crime_and_permits_df.index) $ acs_df['crime_count'] = crime_and_permits_df['crime_count'] $ acs_df['permit_count'] = crime_and_permits_df['permit_count'] $ acs_df.head()
df1_clean.sample(1)
!pip install wget --upgrade $ link = 'https://s3.amazonaws.com/img-datasets/mnist.npz'
edge_types_file = directory_name + 'edge_types.csv'   # Contains info. about every edge type $ edges_file = directory_name + 'edges.h5'             # Contains info. about every edge created
df_usnpl_one_hot_state = pd.get_dummies(df_usnpl[['domain', 'state_level_media', 'state']], columns=['state'])
display(stock_data.iloc[200]) $ print("The close value of the first entry is: ", stock_data.iloc[0, 3])
print location $ print location.split(', ')[-6]
giss_temp.to_excel(writer, sheet_name="GISS temp data") $ full_globe_temp.to_excel(writer, sheet_name="NASA temp data")
btc_price_df['mean'].plot(figsize = (15,7))
df.index
df_data_1['DATE'] = pd.to_datetime(df_data_1['DATE']) $ df_data_1['ID'] = df_data_1['ID'].astype('category') $ df_data_1.dtypes
st_columns = inspector.get_columns('stations') $ for c in st_columns: $     print(c['name'], c["type"]) $
df.tail(3) # See the bottom 3 rows
hobbies_learned.findall(r"<\w*> <\w*> <or> <other> <\w*> <\w*>")
water.head()
halfrow = df.iloc[0, ::2] $ halfrow
file_name = str(time.strftime("%m-%d-%y")) + "-NewsMoodTweets.csv" $ sentiments_pd.to_csv(file_name, encoding="utf-8")
bmp_series = pd.Series(brand_mean_prices).sort_values(ascending=False)
session.query(Station.station).all()
the_data = tmp_df.applymap(lambda x: 1 if x > 3 else 0).as_matrix() $ print(the_data.shape)
train_ind, test_ind, train_dep, test_dep = train_test_split(kick_projects_ip_scaled_ftrs, kick_projects_ip[response], test_size=0.3, random_state=0)
clip_names = [os.path.abspath(tiff[:-4]+"_clip"+".tif") for tiff in tiff_files] $ full_tif_files = [os.path.abspath("./"+tiff) for tiff in tiff_files] $
import statsmodels.api as sm $ convert_old = df.query("landing_page == 'old_page' and converted == 1").shape[0] $ convert_new =  df.query("landing_page == 'new_page' and converted == 1").shape[0] $ n_old = df[df['group'] == 'control'].shape[0] $ n_new = df[df['group'] == 'treatment'].shape[0]
autos['vehicle_type'].unique()
df.mean(1)
df = pd.read_sql('SELECT actor_id, first_name, last_name FROM actor WHERE last_name ilike \'%LI%\' ORDER BY last_name, first_name', con=conn) $ df
%timeit [name.endswith('bacteria') for name in data.phylum] and [val > 1000 for val in data.value]
site_valsdf['valuedatetime'] = site_valsdf.apply(lambda x: pd.to_datetime(x['valuedatetime']).tz_localize(dateutil.tz.tzoffset(None, $                                                                                                                                int(x['valuedatetimeutcoffset'])*60*60)).astimezone(pytz.UTC), axis=1)
cc['logclose']=np.log1p(cc['close']) $ plt.hist(cc['logclose']) $ plt.show()
goog['closedHigher'] = goog.Close > goog.Open
tweets_df['Sentiment'] = tweets_df['Text'].apply(lambda x: sa.get_sentiment_intensity(x)) $ tweets_df
all_data_df = pd.read_csv('github_issues.csv') $ all_data_bodies = all_data_df['body'].tolist()
tweets.retweet_count
yx = [w for w in cfd_index if re.search('^[yx]+$', w)] $ display({k:cfd_index[k] for k in yx if k in cfd_index})
df = pd.read_sql('SELECT * FROM country WHERE country IN (\'Afghanistan\',\'Bangladesh\', \'China\')', con=conn) $ df
t2 = StringIndexer(inputCol="user_id", outputCol="user_idn").fit(df_city_reviews) $ df_city_reviews = t2.transform(df_city_reviews)
aa = np.where( dbData.pl_hostname == 'PSR J1719-1438') $ print(aa,aa[0]) $ print( dbData.iloc[aa] )
giss_temp = giss_temp.drop("Year") $ giss_temp
df.message_likes_dummy.value_counts()
max_vals_idx =  a_sort.flatten()[::-1][:100]
unique_quality_str = df_new.Quality.unique() $ print(unique_quality_str)
lostintranslation = tmdb.Movies(153) $ lostintranslation.info()
engine.execute( $     "SELECT tc.constraint_name,  kcu.column_name FROM  information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name where tc.TABLE_NAME =  'contractor'").fetchall()
fruits.iloc[0] + 1
df_twitter_copy = df_twitter_copy[df_twitter_copy.retweet_count.notnull()]
results_Jarvis, out_file3 = S.execute(run_suffix="Jarvis_hs", run_option = 'local')
clustering_df = classification_data[['company_uuid','company_name','company_category_list','category_group_list', $                                     'short_description','description','founded_on']].copy() $ clustering_df.drop_duplicates('company_uuid',inplace = True)
val_small_data.head(1)
plt.plot(windfield_matched_array) $ plt.show()
import statsmodels.api as sm $ convert_old = df2[(df2["landing_page"] == "old_page") & (df2["converted"] == 1)]["user_id"].count() $ convert_new = df2[(df2["landing_page"] == "new_page") & (df2["converted"] == 1)]["user_id"].count() $ n_old = df2[df2['group'] == 'control'].shape[0] $ n_new = df2[df2['group'] == 'treatment'].shape[0]
for i in c.find().limit(10): $     print i
final_rf_predictions = rf_v2.predict(test[:-1])
import pandas as pd $ url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/titanic.csv' $ df = pd.read_csv(url) $ df.head(5)
itemTable = pd.read_csv('itemTable.csv', index_col = 0) $ itemTable.head(10)
dr1 = meta_file['dailymet_livneh2013']['date_range'] $ dr2 = meta_file['dailywrf_salathe2014']['date_range'] $ dr = ogh.overlappingDates(tuple([dr1['start'],dr1['end']]), tuple([dr2['start'],dr2['end']])) $ dr
print(data.json())
training_data,holdout = train_test_split(liquor2015_combined,shuffle=True,test_size=0.10,random_state=123)
df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit = 's') $ df.head()
df.set_index('created_at', inplace= True)
cp_age = sorted(train_data_df ['created_year']) $ pdf = stats.norm.pdf(cp_age, np.mean(cp_age), np.std(cp_age)) $ plt.plot(cp_age, pdf) $ plt.hist(cp_age,normed=True)
prcp_data_df.head()
new_fp = 'data/brain2body_headers.txt' $ b2b_df.to_csv(new_fp, index=False) # Setting index to False will drop the index integers, which is ok in this case
summaries = "".join(df.title) $ ngrams_summaries = cvec_1.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(10)
regr.fit(X_train, y_train)
logit_new = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'UK']]) $ results_new = logit_new.fit() $ results_new.summary()
df.shape
city_ride_data = clean_combined_city_df.groupby(["type"]).sum()["fare"] $ city_ride_data.head() $
captions = yt.get_captions(vid, verbose=False)
new = df.groupby(['Year','Month','tweet_ats'])['tweet_ats'].agg({'no':'count'}) # do a groupby and count each hashtag $ new.reset_index(level=[0,1,2], inplace=True) # reset the indexes $ new['datetime'] = pd.to_datetime((new.Year*10000+new.Month*100+1).apply(str),format='%Y%m%d') # convert back to a datetime with a default day of 1
hyperparam =[0.01, 0.1, 1, 10, 100] $
error_count = pd.get_dummies(errors.set_index('datetime')).reset_index() $ error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5'] $ error_count = error_count.groupby(['machineID', 'datetime']).sum().reset_index() $ error_count.head(13)
df = h2o.H2OFrame({'A': [1, 2, 3],'B': ['a', 'b', 'c'],'C': [0.1, 0.2, 0.3]})
lossprob = fe.bs.smallsample_loss(2560, poparr, yearly=256, repeat=500, level=0.90, inprice=1.0) $
ptgeom = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])] $ sites = gpd.GeoDataFrame(df, geometry=ptgeom, crs={'init': 'epsg:4326'}) $ sites.head(5)
corn_vege.agg(["sum","mean"])
census_zip = df_station.merge(wealthy, left_on=['zipcode'], right_on=['zipcode'],  how='left') $ nan_rows = census_zip[census_zip['betw150kand200k'].isnull()] $ census_zip.dropna(inplace = True)
metadata_documents = [x.name() for x in query.element("asset/meta").elements() if x.name() != "mf-revision-history"] $ metadata_documents
sts_model.pred_table()
for i in range(100,110): $     print reddit.title.values[i], analyzer.polarity_scores(reddit.title.values[i]) $     print 'Length: ', len(reddit.title.values[i].split())
data.plot(x='Date')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
joined.dtypes.filter(items=['Frequency_score'])
df_results = pd.DataFrame(results) $ df_results = df_results[['Model', 'Accuracy', 'Precision', 'Recall']] $ df_results
df_EMR_dd_dummies_no_sparse.head()
df = pd.read_csv('tweet_json.txt', encoding = 'utf-8') $ df.set_index('tweet_id', inplace = True) $ df.tail()
portfolio_df.head()
df = pd.read_csv('ZILLOW-Z49445_ZRISFRR.csv',index_col=0) $ df.columns=['Price'] # Changing the name of the column. (Index is not treated as a column so in our df we have only 1 column) $ df.head()
bookings.head()
closest_id = sim.get(df.iloc[-100]['skills']) $ print('Intersection:', sim.df_new.iloc[closest_id].sum(), 'tags')
sns.barplot(x=top_com['num_comments'], y=top_com.index) # challenge: annotate values in the plot $ plt.xlabel("mean number of comments") $ plt.axvline(df['num_comments'].mean(), color='b', linestyle='dashed', linewidth=2) $ plt.axvline(top_com['num_comments'].mean(), color='r', linestyle='dashed', linewidth=2) $ plt.title("Top 5 active subreddits by mean # of comments");
new_page_convert.mean() - old_page_convert.mean()
Base = automap_base() $ Base.prepare(engine, reflect=True) $ Base.classes.keys() # Retrieve the names of the tables in the database
pd.read_sql(f'select * from {view}', engine)
tokens = reddit2.apply(process) $ dict_tokens = corpora.Dictionary(tokens) $ doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]
fig=plt.figure(figsize=[5,8]) $ ax = fig.add_subplot(111) $ ax.bar(injuries_hour.date_time[:200], injuries_hour.injuries[:200],width=0.1) $ ax.xaxis_date()
dfEtiquetas.groupby("city", as_index=False).agg({"id": "count"}).sort_values(by="id", ascending=False)
dayofweek.get_group('Tuesday')
from sklearn.svm import SVR $ model = SVR(kernel='rbf', C=1e3, gamma=0.1) $ print ('SVR') $
ff3.set_index('Date', inplace=True)
tweets_original = pd.DataFrame(ndarray_original)
results=pd.read_csv("results.csv") $ results.tail() $
drop_columns_list = ['date','serial_number','failure','fails_soon','seq_id','max_work_day','final_failure'] $ X_train = df_train.drop(columns=drop_columns_list) $ X_test = df_test.drop(columns=drop_columns_list) $ Y_train = df_train.iloc[:,5] $ Y_test = df_test.iloc[:,5]
def read_tweet(status_object): $     import pprint $     pprint.pprint(vars(status_object)) $ read_tweet(clinton_tweets[0])
df_train.head()
df[pd.unique(['Country'] + df.columns.values.tolist()).tolist()].head()
better_bonus_points = bonus_points.copy() $ better_bonus_points.insert(0, "sep", 0) $ better_bonus_points.loc["alice"] = 0 $ better_bonus_points = better_bonus_points.interpolate(axis=1) $ better_bonus_points
diff_actual = df[df['group'] == 'treatment']['converted'].mean() - df[df['group'] == 'control']['converted'].mean() $ print('The actual difference in the original dataset: ',diff_actual) $ p_diffs = np.array(p_diffs) $ p_val = (diff_actual > p_diffs).mean() $ print('P-Value: ', p_val)
df['2018-05-21'] # Try to understand the error ... Its searching in columns !!! $
percentage_click_by_no_emails = train.groupby('no_of_emails')['is_click'].agg(np.mean) $ percentage_click_by_no_emails
weighted_predictions = tf.multiply(routing_weights, caps2_predicted, $                                    name="weighted_predictions") $ weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True, $                              name="weighted_sum")
s.groupby(['group','ID']).size().unstack()
!ls -lh losses.csv
df = df.dropna(subset=['ACTION REQUESTED'])
new_df['post_creation_date'] = list(map((lambda x: pd.to_datetime(x)), new_df['post_creation_date'])) $ new_df['date'] = new_df['post_creation_date'].dt.strftime('%Y-%m')
data.plot()
people['Id'] = pd.to_numeric(people['Id'], errors='coerce')
total_newpage = df2[df2.landing_page == 'new_page'].count()['user_id'] $ total_pages = df2.shape[0] $ total_newpage / total_pages #0.50006194422266881 $
pos_freq = {k: pos_tfidf.dfs.get(v) for v, k in pos_dic.id2token.items()} $ sorted(pos_freq.items(), key=lambda x: x[1], reverse=True)
morning_rush.iloc[:5,6:8]
x = np.arange(10) $ (x > 4) & (x < 8)
output_fn = "News_Sentiment_Analysis.csv" $ sentiment_df.to_csv(output_fn)
index_to_change = df3[df3['group']=='treatment'].index $ df3.set_value(index=index_to_change, col='ab_page', value=1) $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3[['intercept', 'ab_page']] = df3[['intercept', 'ab_page']].astype(int) $ df3 = df3[['user_id', 'timestamp', 'group', 'landing_page', 'ab_page', 'intercept', 'converted']]
df.sample(10)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
def update_hosted_service_feature(source_label, source_camera_id, source_image_url, source_image_localpath, source_datetime, $                                   target_service, target_label_field, target_camera_id_field, target_source_image_field, $                                   overwrite_attachment=True, layer_index=0): $     target_lyr = target_service.layers[layer_index]   $
kochdf = pd.merge(kochdf, koch01df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_01'))
%matplotlib inline $ sns.violinplot(data=november, inner="box", orient="h", bw=.03)
qqqq = qqqq.groupby('msno').apply(days_since_the_last_expiration) $ qqqq = qqqq.groupby('msno').apply(days_since_the_last_subscription) $ qqqq = qqqq.groupby('msno').apply(is_subscribe_early) $ qqqq = qqqq.groupby('msno').apply(creat_loyalty_trend) $
newdf.set_index('Date', inplace=True)
y2 = tweets2['handle'].map(lambda x: 1 if x == 'mattjpfmcdonald' else 0).values $ print max(pd.Series(y2).value_counts(normalize=True))
def flatten(x_tensor): $     return tf.layers.flatten(x_tensor) $ tests.test_flatten(flatten)
df2.head()
tsla_tuna_neg_cnt = tsla_tuna_neg.count()*100 $ print "{:,} users were only active on {} ({:.2%} of DAU)"\ $       .format(tsla_tuna_neg_cnt, D0.isoformat(), tsla_tuna_neg_cnt*1./dau)
df_c2['intercept']= 1 $ lom2 = sm.Logit(df_c2['converted'], df_c2[['intercept','UK', 'US', 'ab_page']]) $ results2 = lom2.fit() $ results2.summary()
train['dot_edu'] = train.url.str.contains('.edu', case=False, na=False, regex=False).astype(int) $ train.groupby('dot_edu').popular.mean()
df.shape
result =[] $ for i,j in top_features: $     result.append(i)
result_df.shape
import dill as dpickle $ with open('ppm_body.dpkl', 'rb') as file: $     ppm_body = dpickle.load(file) $ with open('ppm_title.dpkl', 'rb') as file: $     ppm_title = dpickle.load(file)
pumashp = pumashp.merge(pumaPop,on='public use microdata area')
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $     filter(Measurement.station == 'USC00519281').all()
lons,lats = f.variables['X'], f.variables['Y'] $ print(lons, lats)                 
pred_labels = lr.predict(test_data) $ print("Training set score: {:.2f}".format(lr.score(train_data, train_labels))) $ print("Test set score: {:.2f}".format(lr.score(test_data, test_labels)))
pd.options.display.max_columns = 60 $ df_protest.head()
top_supporters["contributor_fullname"] = top_supporters.contributor_firstname + " " + top_supporters.contributor_lastname
aqmdata2 =  pd.read_html("https://en.wikipedia.org/wiki/List_of_mobile_telephone_prefixes_by_country", match="Afghanistan") $ data = aqmdata2[0] $ data.head() $
pd.to_datetime(pd.Series(['Jul 31, 2009', '2010-01-10', None]))
af.sort_values(by=['length'], ascending=False).head()
df[df.msno == '++1G0wVY14Lp0VXak1ymLhPUdXPSFJVBnjWwzGxBKJs=']
imgp.shape
(autos["last_seen"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $ )
import matplotlib.pyplot as plt $ df[['Date','AveCoalPrice']].set_index('Date').plot()
set(df.funding_rounds)
auth = tweepy.OAuthHandler(hidden.consumer_key, hidden.consumer_secret) $ auth.set_access_token(hidden.token_key, hidden.token_secret) $ api = tweepy.API(auth)
from sklearn.ensemble import RandomForestRegressor $
cust_data.loc[:10,'ID']
val = pd.Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])
print('Features Engineering completed at : ', datetime.datetime.now())
import pandas as pd $ reviews=pd.read_csv("ign.csv") $ reviews.head()
df_final['Scorepoints'] = df_final.R + df_final.F + df_final.M + df_final.P + df_final.J
df2.query('landing_page=="old_page"')['user_id'].count()
df1 = pd.merge(left=dfWQ_annual,right=dfQ1_annual,how='inner',left_index=True,right_index=True) $ df2 = pd.merge(left=dfWQ_annual,right=dfQ2_annual,how='inner',left_index=True,right_index=True) $ df3 = pd.merge(left=dfWQ_annual,right=dfQ3_annual,how='inner',left_index=True,right_index=True)
query_date = dt.date(2017, 8, 23) - dt.timedelta(days=365) $ print(f"Going back one year, the date was {query_date}.")
results.summary()
print 'Python Version ' + sys.version $ print 'Pandas Version ' + pd.__version__
customer_visitors_new = customer_visitors_new.unstack(1) $ customer_visitors_new.columns = customer_visitors_new.columns.droplevel(0) $ customer_visitors_new = customer_visitors_new.reindex_axis(sorted(customer_visitors_new.columns, key= lambda x: list(calendar.day_name).index(x)), axis=1) $ customer_visitors_new
pd.date_range('2017', periods=4, freq='QS-FEB')  # 4 quarters starting from beginning of February
df2.duplicated('user_id') $
tweet_th.describe()
!cat "json_example.json"
get_topic_distribution(american_train_model, american_dictionary, american_test.iloc[2]['reviews_without_rare_words'])
df.pct_chg_opencls.describe()
x_axis2 = [] $ y_axis2 = [] $ for item in output2: $     x_axis2.append(item[0]) $     y_axis2.append(item[1])
extraRecordsForLag.dtypes
df = pd.DataFrame([2, 3, 1, 4, 3, 5, 2, 6, 3]) $ df.quantile(q=[0.25, 0.75])
Conversion_No1=df2.loc[df2['landing_page']=="old_page",].shape[0] $ print("The no of observations regarding the old page equals",Conversion_No)
sns.set_style("whitegrid") $ ax=sns.distplot(compiled_data['botometer'], kde=False) $ ax.set_xlabel("BotOMeter Score") $ ax.set_ylabel("Count")
total_data_rows = len(df.index) $ df.dropna(subset = ['UNIQUE_CARRIER','ORIGIN','DEST','CRS_DEP_TIME','CRS_ARR_TIME','ARR_DELAY','CRS_ELAPSED_TIME','DISTANCE'],inplace=True) $ data_retained = len(df.index)/total_data_rows $ print('Data Retained: '+str(round(data_retained*100,2))+' %')
arima11= ARIMA(dta_713,[1,1,0],freq='Q').fit() $ arima11.summary()
print("Number of Techniques in PRE-ATT&CK") $ print(len(all_pre['techniques'])) $ df = all_pre['techniques'] $ df = json_normalize(df) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'detectable_by_common_defenses'], axis=1)[0:5]
raw['parse_date'] = pd.to_datetime(raw['parse_date'])
mean_bed_df = full_df.groupby(['bedrooms', 'bathrooms'])['price'].mean().reset_index().copy() $ mean_bed_df
p_new = np.count_nonzero(new_page_converted)/len(new_page_converted) $ p_old = np.count_nonzero(old_page_converted)/len(old_page_converted) $ a = p_new - p_old $ print("%.5f" % a)
print("GLM AUC on training = " + str(glm_model.auc(train = True)) + " and GLM AUC on validation = " + str(glm_model.auc(valid = True))) $ print("GBM AUC on training = " + str(gbm_model.auc(train = True)) + " and GBM AUC on validation = " + str(gbm_model.auc(valid = True)))
actual_diff = df[df['group'] == 'treatment']['converted'].mean() - df[df['group'] == 'control']['converted'].mean()
(~autos["registration_year"].between(1970,2016)).sum() / autos.shape[0]
n_old=df2.query("landing_page =='old_page'").shape[0] $ n_old
StockData.count()
terror.index
preproc_titles = pipeline.fit_transform(review_title) $ pipe_cv = pipeline.named_steps['cv']
data_df = pd.read_csv('dropbox/github/thinkful/unit1/data/lecz-urban-rural-population-land-area-estimates_continent-90m.\ $ csv', encoding='windows-1252') 
image_predictions_df.sample(3)
s.resample('30D').head(10)
import statsmodels.api as sm # think this is already imported, so not sure why we are being asked to repeat $ logit = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit.fit()
hpdpro = pd.read_csv('../data/rawdata/Complaints20170201/Problem20170131.csv', $                      dtype = {'ProblemID':str, 'ComplaintID':str}, parse_dates = ['StatusDate'])
old_page_converted = np.random.choice([0,1], size = n_old, p=[1-p_old, p_old]) $ old_page_converted.mean()
dftemp = df1[df1['Area'].isin(['Iceland'])]    #storing the rows where area is iceland $ dftemp
data.drop('two', axis=1)
cityID = '3b77caf94bfc81fe' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Los_Angeles.append(tweet) 
new_n = df2[df2['group'] == 'treatment'].shape[0] $ new_n
complaints2016_geodf = gp.GeoDataFrame(complaints2016_filtered_cat_wlocations)
tweet_archive_enhanced_clean['retweeted_status_id'].value_counts()
child.head()
df = pd.DataFrame({'ImageId': imageids, 'Label': category}) $ print (df.shape) $ print (df.head()) $ print (df.tail())
h4 = qb.History(spy.Symbol, 360, Resolution.Daily) $
d = d.query('group != "treatment" or landing_page != "new_page"') $ d.head()
print( gwt.ggPM(4,0.0) )
combine_Xtrain_ytrain = pd.concat([X,y],axis=1) $ combine_Xtrain_ytrain = combine_Xtrain_ytrain.drop(['date','subjects','word_count','full_text'],axis=1) $ combine_Xtrain_ytrain.head()
twitter_archive_full[twitter_archive_full.retweet_count == 77458][ $     ['tweet_id', 'stage','retweet_count']]
data.describe()
all_complaints.info()
obj = pd.Series(np.arange(4.), index=['a', 'b', 'c', 'd'])
complete_wind_df.head()
def analyse_correlation(df, dep_variable, indep_variable): $     for v in indep_variable: $          sb.regplot(x=v, y=dep_variable, data=df) $          plt.show() $ analyse_correlation(cats_df.dropna(), 'age at death', ['hair length', 'height', 'number of vet visits', 'weight'])
df[df['Descriptor'] == 'Loud Music/Party']['Unique Key'].groupby(df[df['Descriptor'] == 'Loud Music/Party'].index.hour).count().plot()
df_new=df2.join(df_country.set_index('user_id'),on='user_id') #join the country colum
sns.jointplot(x = "positive_ratio", y = "negative_ratio", data = news_df)
con_df['country'].value_counts()
Z = np.random.random(10) $ Z.sort() $ print(Z)
df['Month'].head()
a= bus[bus["postal_code_5"].isin(all_sf_zip_codes)==False] $ weird_zip_code_businesses=a[a.notnull()["postal_code_5"]] $
input_edge_types_DF = pd.read_csv(input_edge_types_file, sep = ' ') $ input_edge_types_DF
today = dt.date.today() $ year_ago = today - dt.timedelta(days=365) $ print(today) $ print(year_ago) $
results2.summary()
import matplotlib.pyplot as plt $ plt.hist(p_diffs) $ plt.axvline(x = prob_convert_given_treatment-prob_convert_given_control,color='red') $
cig_data_SeriesCO.shape
null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size)
len(df2_treatment.index)/len(df2.index)
X_train_df = pd.DataFrame(X_train_matrix.todense(), $                          columns=tvec.get_feature_names(), $                          index=X_train.index)
autos = autos[autos['registration_year'].between(1900,2016)] $ autos['registration_year'].value_counts(normalize=True)
df[df.dataset=="test"].head()
print(g_test.shape) $ print(p_test.shape) $ merged_test = pd.merge(left=g_test,right=p_test,how='inner',left_on='customer',right_on='customer',copy=True) $ print(merged_test.shape) $ merged_test.head()
tweets_kyoto.head()
n_net3 = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5000,2500,1000,500,250,100,50), $                       random_state=1, verbose = True) $ n_net3.fit(x_train,y_train)
engine.execute('alter table actor drop column middle_name') $ pd.read_sql('desc actor', engine)
reviews.groupby('variety').price.agg([min,max]).sort_values(by=['min','max'], ascending=False)
tests = pd.read_csv("tests.csv") $ combats = pd.read_csv("combats.csv")
p_value_ztest_lt * 2 == pval_2_sided # 0.18988337448195103
pd.DataFrame.query?
tweet_archive_enhanced_clean['rating_denominator'].sum()
lr = 5e-4 $ learn.fit(lr, 10, cycle_len=1, use_clr=(10,10))
from biopandas.mol2 import PandasMol2 $ pmol = PandasMol2().read_mol2('./data/1b5e_1.mol2')
titanic = pd.read_excel('Data/titanic.xls') $ print(titanic.head)
messages_with_dates_ext = messages.apply(find_dates_in_row,axis=1)
def upper_bound(QCB, M): $     return 0.5 * QCB ** M $ def lower_bound(tr, M): $     return (1 - np.sqrt(1 - tr ** (2 * M))) / 2
rets = solar_df.pct_change() $ print(rets)
firstWeekUserMerged["day_time_stamp2"] = firstWeekUserMerged.time_stamp2.dt.day
def calc_daily_returns(closes): $     return np.log(closes/closes.shift(1))[1:]
jobPostDF.month.value_counts()
data.sort_values('TMED', inplace=True, ascending=False) $ data.head()
coef_uk = np.exp(0.0099) $ coef_ca = np.exp(-0.0408) $ print("UK: {0}, CA: {1}".format(coef_uk, coef_ca))
weather.head()
dog_stage_stats['rating_numerator'].sort_values(ascending=False).plot(kind='bar', ylim=[10,13])
result.summary()
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df3.set_index('user_id'), how='inner') $ df_new.head()
print(data.json()['dataset']['column_names'])
!h5dump 'data/my_pytables_file.h5'
density = 1.32
RatingSampledf.to_csv("..\\Output\\SampleRatings.csv")
diff_simulated = new_page_converted.mean() - old_page_converted.mean() $ diff_simulated
newdf = newdf.join(ff3, how='inner')
pred_labels = elnet2.predict(test_data) $ print("Training set score: {:.2f}".format(elnet2.score(train_data, train_labels))) $ print("Test set score: {:.2f}".format(elnet2.score(test_data, test_labels))) $ print("Number of features used: {}".format(np.sum(elnet2.coef_ != 0)))
from scipy.stats import norm $ parameters = norm.fit(sample)
lm = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US', 'ab_page']]) $ results = lm.fit() $ results.summary()
df_goog['6 mo offset'] = df_goog.index + relativedelta(months=+6)
print("Probability of individual user converting is", df2.converted.mean())
df2.head(5)
old_page_converted = np.random.binomial(1, control_cnv, control_num) $ old_page_converted.mean()
log_with_day = access_logs_df.withColumn('dayOfWeek',weekday('dateTime'))
plt.figure(figsize=(12,10)) $ plt.plot(cluster_label_group.ix[:,0:21].T) $ plt.title("Cluster Centers Over Time: K Means") $ plt.legend([0,1,2])
autos["model_brand"] = autos["model"] + " " + autos["brand"] $ autos["model_brand"].head()
sampleDF.full_text[0]
print(df.count()) $ print('There are no missing values, because the number of rows and the numbers from the count function are the same.')
results = soup.find_all('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text')
import statsmodels.formula.api as smf $ logit_model=smf.Logit(y,X) $ result=logit_model.fit()
df2 = pd.DataFrame({ $     'E' : ["test","train","test","train"], $     'F' : 'foo', $     'D' : np.array([3] * 4,dtype='int32') $     })
keywords = corpus.groupby(['Article','Stem','Word']).count() $ keywords = keywords.reset_index().sort_values(['Article','Dummy'], ascending=False).groupby('Article').head(20) $ keywords.rename(columns={'Dummy':'Count'}, inplace=True) $ keywords $
documents = np.array(df['body'])
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value $
archive_copy.info()
recentMediaResponse = requests.get("https://api.instagram.com/v1/users/self/media/recent/",params = {"access_token": access_token}) $ recentMediaJson = json.loads(recentMediaResponse.text) $ pprint.pprint(recentMediaJson)
df_likes.head()
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, (1-p_old)])
df = pd.read_sql_query('SELECT Agency, Descriptor FROM data LIMIT 3', disk_engine) $ df.head()
investors.groupby('investor_type').size().plot(kind = 'bar',figsize = (20,8))
user_url = 'http://nodeassets.nbcnews.com/russian-twitter-trolls/users.csv' $ df_users = pd.read_csv(user_url, dtype={'id' : str}) $ print("N = {}".format(len(df_users))) $ df_users.head(3)
result_co_2.summary()
gpCreditCard.Tip_amount.describe()
nold= df2[df2["landing_page"] == "old_page"].count() $ nold = nold[0] $ nold
archive_df_clean.info() $ archive_df_clean.rating_denominator.value_counts()
qty_left_1c_store = table_store.get_value(0, u'Quantity Txn Part')
train_data["totals.transactionRevenue"] = train_data["totals.transactionRevenue"].fillna(0.0) $ train_data["totals.transactionRevenue"] = np.log1p(train_data["totals.transactionRevenue"])
p_diff_act=new_page_converted.mean()-old_page_converted.mean() $ p_diff_act
df.head()
model_json = model.to_json() $ with open("500topics_1_8_15_tenepochs.json", "w") as json_file: $     json_file.write(model_json) $ model.save_weights("500topics_1_8_15_tenepochs.h5") $ print("Saved model to disk") $
df.isnull().sum()
mentions = api.GetMentions() $ print([m.text for m in mentions])
cool = [ [cars[1].Model, "Cool"] $         if cars[1].hp >= 200 else [cars[1].Model, "Boring"] $         for cars in cars.iterrows()] $ coolCars = pd.DataFrame(cool, columns=['Model', 'Rating'])
c_new = df2['converted'].mean() $ print(c_new)
df.info()
cond = df_full['index'].isin(data_l2_end + data_other) $ df_full.loc[cond,'school_name'] = df_full[cond]['school_code'].map(DATA_SUM_DICT) $ df_full.loc[cond,'school_code'] = INVALID_SCHOOL_CODE $ cond = df_full['index'].isin(data_other) $ df_full.loc[cond,'school_type'] = ALL_SCHOOL_TYPE
subred_num_avg = reddit[['subreddit','num_comments']].groupby(by='subreddit', sort=True, as_index=False).mean().round().sort_values(by='num_comments',ascending=False)
df.loc[['a','b','f','h'],['A','C']]
reframed_eth = series_to_supervised(scaled_eth, 1, 1) $ print(reframed_eth.shape) $ print(eth_market_info.shape)
tweet_df.iloc[:,1:15].sample(5)
sqlContext = SQLContext(sc) $ sdf = sqlContext.read.csv('production.csv', header=True, inferSchema=True) # requires spark 2.0 or later $ print ('Number of rows: ' , sdf.count()) $ sdf.printSchema() $ sdf.show()
airbnb_df.info(memory_usage='deep')
dedup.shape
aml.leader
discounts_table.Segment.unique()
tfav_ps = pd.Series(data=data_ps['Likes'].values, index=data_ps['Date']) $ tret_ps = pd.Series(data=data_ps['RTs'].values, index=data_ps['Date']) $ tfav_ps.plot(figsize=(16,4), label="Likes", legend=True) $ tret_ps.plot(figsize=(16,4), label="Retweets", legend=True);
df_final.last_trx.max()
df_prep98 = df_prep(df98) $ df_prep98_ = pd.DataFrame({'date':df_prep98.index, 'values':df_prep98.values}, index=pd.to_datetime(df_prep98.index))
block_geoids_2010 = json.load(open('block_geoids_2010.json')) $ print 'There are', len(block_geoids_2010), 'blocks' $ assert(len(block_geoids_2010) + 1 == len(block_populations))
reg_mod = sm.OLS(df2['converted'], df2[['intercept', 'ab_page']]) $ model = reg_mod.fit()
joined_df.groupby(['id', 'churned'])[['accepted']].mean().boxplot(by='churned')
n_old = df2[df2.group=='control']['converted'].shape[0] $ n_old
autos[~autos["registration_year"].between(1900,2016)]
sf_small = sf_subset.iloc[1:100000]
X_train, X_validate, y_train, y_validate = train_test_split(X.values, y)
import math $ from keras.models import Sequential $ from keras.layers import LSTM $ from keras.layers.core import Dense, Activation, Dropout
import pandas as pd $ rpd = pd.read_csv(fl) $ df = pd.DataFrame(rpd) $ df.head()
new_cases['Date'] = new_cases['Date'].apply(lambda d: datetime.strptime(d, '%Y-%m-%d')) $ grouped_months_new = new_cases.groupby(new_cases['Date'].apply(lambda x: x.month)) $ new_cases['Total_new_cases_guinea'] = new_cases['Total_new_cases_guinea'].astype(int)
log_CA = sm.Logit(new['converted'],new[['intercept','country_CA']]) $ r = log_CA.fit() $ r.summary()
df_twitter.describe()
df['AQI Category'].cat.categories = ['Good', 'Moderate', 'Unhealthy', 'Somewhat Unhealthy'] $ df['AQI Category'] = df['AQI Category'].cat.add_categories(['Very Unhealthy']) $ df['AQI Category'] = df['AQI Category'].cat.reorder_categories(['Good', 'Moderate', 'Somewhat Unhealthy', 'Unhealthy', 'Very Unhealthy'], ordered=True)
rowmeans = slicer.mean(axis=1) $ rowmeans
print(autos['price'].unique().shape)
p_diffs = np.array(p_diffs) $ (act_diff < p_diffs).mean() $
most_active_station_tobs = session.query(Measurement.tobs).\ $ filter(Measurement.station == most_active_station, Measurement.station == Station.station,\ $        Measurement.date >="2017-08-01", Measurement.date <="2018-07-31").all()
sess.run(embedded, {inputs: [0, 2]})
r.json()
punctuations = list(string.punctuation) $ df['body_tokens'] = df['body_tokens'].apply(lambda x: [word for word in x if word not in punctuations]) $ print(df['body_tokens'])
selected_train_df = train_df[['Q0_RELEVANT', 'Q1_mood_of_speaker']].reset_index(drop=True)
output_bot.to_csv("outputbot.csv", sep=',', encoding='utf-8')
import os $ REGION = 'us-central1' # Choose an available region for Cloud MLE from https://cloud.google.com/ml-engine/docs/regions. $ BUCKET = 'qwiklabs-gcp-771407dbf531ec59' # REPLACE WITH YOUR BUCKET NAME. Use a regional bucket in the region you selected. $ PROJECT = 'qwiklabs-gcp-771407dbf531ec59'    # CHANGE THIS
cohort_active_activated_df = pd.DataFrame(index=daterange,columns=daterange)
!ls -l columncache/acs2015_5year_tract2010/B08006_002.float32
wrd_clean['expanded_urls'].str.contains('https://www.gofundme.com/').sum()
new_page_converted = np.random.choice([0,1], size=n_new, p=[p_new, (1-p_new)])
cityID = 'b046074b1030a44d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Jersey_City.append(tweet) 
submit = pd.read_csv('data/sample_submission.csv')
pipeline = Pipeline([('posf', PartOfSpeechFilter()), $                      ('cv', CountVectorizer()) $                    ]) $ pipeline.set_params(cv__lowercase=True,cv__max_df=0.95, cv__min_df=0.01, cv__stop_words='english') $ pipeline.set_params(posf__stop_words=stop_words_list, cv__stop_words=stop_words_update)
df_gt[u'Equipment Type'].value_counts()
jobs.Submit = pd.to_datetime(jobs.Submit) $ jobs.Start = pd.to_datetime(jobs.Start) $ jobs['Wait'] = jobs.Start - jobs.Submit $ jobs['Wait'] = jobs['Wait'].dt.seconds
WholeDf.created_at $ dateparse = lambda x: WholeDf.created_at.strptime(x, '%Y-%m-%d %H:%M:%S') $ datecreated = WholeDf $ datecreated = pd.to_datetime(WholeDf.created_at, format='%Y-%m-%d')
pd.Series(np.r_[1,2,9])
party = candidate_df.iloc[0]['party_full'] $ print(party)
contractor_clean['zip_part1'] = contractor_clean.zipcode.str.split('-').str[0] $ contractor_clean['zip_part2'] = contractor_clean.zipcode.str.split('-').str[1]
vol_list = [value for key, value in price_dict['Traded Volume'].items()] $ ave_vol = round(sum(vol_list)/Len, 0) $ print('The average daily trading volume during this year is: ' + str(ave_vol))
conn_string = "host='localhost' dbname='qua-kit'" $ print ("Connecting to database\n	->%s" % (conn_string)) $ conn = psycopg2.connect(conn_string) $ print ("Connected!\n")
p_diffs = [] $ for _ in range(10000): $     new_page_converted = np.random.binomial(1,p_new,n_new)  # bootstrapping $     old_page_converted = np.random.binomial(1,p_old,n_old) # bootstrapping $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())
pageviews = pd.read_sql_query("SELECT * from pageviews "+RetSqlLimit("pageviews",sqlLimit), conn) $ pageviews_tags = pd.read_sql_query("SELECT * from pageviews_tags "+RetSqlLimit("pageviews",sqlLimit), conn) $ pageviews['group'] = pageviews['ab'].apply(lambda x: x[:1]) $ pageviews['session'] = pageviews['ab'].apply(lambda x: x[2:])
data['subreddit_other'] = [subred if (data['subreddit'] == subred).sum() > 3 else 'other' for subred in data['subreddit']]
vocab = {} $ for word in _dict_word_count: $     if (not (word in stopwords) and _dict_word_count[word] > 5 and word.isalnum()): $         vocab[word] = _dict_word_count[word]
df.sub(s,axis='index')
print('The biggest change in price was: ' + '${:,.2f}'.format(max([(sublist[2]-sublist[3]) for sublist in inputlist])))
bwd.drop('Store',1,inplace=True) $ bwd.reset_index(inplace=True)
from statsmodels.tsa.stattools import adfuller as ADF $
stories.shape
transit_df.info()
print('The maximum change between two days is {}'.format(max(change)))
key = get_excel_range(input_data, "Key", "A1:E54") $ key.head()
year_temp = session.query(Measurement.station, Measurement.date, Measurement.tobs).\ $         filter(Measurement.date.between('2016-01-01' , '2017-01-01')).\ $         filter(Measurement.station == most_active).all() $ year_temp
from plotly import __version__ $ from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot $ import plotly.offline as offline
weather.Humidity_avg_percent =pd.to_numeric(weather.Humidity_avg_percent,errors = "coerce")
data_overdue_dur = (data['last_payment_date'] - data['due']).astype('timedelta64[D]') $ data_overdue_dur[data_overdue_dur > 0].hist(bins=200, figsize=(20, 10))
n_old = len(df2[df2['landing_page'] == 'old_page']) $ print ("n_old is: {}".format(n_old))
control_num = df2.query('group=="control"').shape[0] $ control_num
merged_311_data.groupby("Category").size().sort_values(ascending=False)
trans_data.head()
cell_df = pd.read_csv("cell_samples.csv") $ cell_df.head()
train = energy.copy()[energy.index < valid_start_dt][['load', 'temp']]
G = K_p/(tau_p*s + 1)*numpy.exp(-theta*s)
df_archive_clean.drop(["doggo","pupper","floofer","puppo"], axis =1, inplace = True)
final_topbikes.groupby(by=final_topbikes.index.weekday)['Distance'].count().plot(kind='bar', figsize=(12,6))
df_2009.dropna(inplace=True) $ df_2009
X_d = pd.get_dummies(X, columns=['domain_d'], drop_first=True)
print ("The original dtype of dateCrawled is " + str(rawautodf.dateCrawled.dtype)) # checked datatype $
observed_mean_treatment = df2.query('group=="treatment"')['converted'].mean() $ print(observed_mean_treatment)
raw_file = parser.from_file(path_list[0])['content']
import statsmodels.api as sm
auth = tp.OAuthHandler(consumer_key=key, consumer_secret=secret) $ api = tp.API(auth)
autos["registration_year"].value_counts(normalize = True).sort_index()
df_new.rename(columns={"treatment":"ab_page"}, inplace=True) $ df_new.head(1)
(null_vals > actual_diff).mean()
y_pred = knn.predict(X_test) $ print(accuracy_score(y_test, y_pred)*100)
precip_data_df2=precip_data_df1[["date","Precipitation"]] $ precip_data_df2.describe()
taxi_hourly_df.head()
df_mes[(df_mes['improvement_surcharge']!=0.3)].shape[0]
y_test_pred = gs.predict(X_test) $ print('Root Mean Sequare Error on the Test Data:') $ print(np.sqrt(mean_squared_error(y_test_pred, y_test)))
import matplotlib.pyplot as plt $ plt.style.use('seaborn') $ df1.plot(x='avexpr', y='logpgp95', kind='scatter') $ plt.show()
femalenew = femalebyphase.rename(columns={"Offense":"Female"}) $ femalenew.head(3)
temp_stats = session.query(func.min(Measurement.tobs),func.max(Measurement.tobs),func.avg(Measurement.tobs)).\ $ filter(Measurement.station=='USC00519281').all() $ temp_stats $
logit = sm.Logit(df['converted'], df[['ab_page', 'intercept']]) $ result=logit.fit()
k_means_labels = k_means.labels_ $ k_means_labels
tx, ty = tsne[:,0], tsne[:,1] $ tx = (tx-np.min(tx)) / (np.max(tx) - np.min(tx)) $ ty = (ty-np.min(ty)) / (np.max(ty) - np.min(ty))
msftA = msft[['Adj Close']] $ closes = pd.concat([msftA, aaplA], axis=1) $ closes[:3]
import statsmodels.api as sm $ convert_old = np.random.binomial(size_treatment, converted_prob, 10000).mean() $ convert_new = np.random.binomial(size_control, converted_prob, 10000).mean() $ n_old = size_control $ n_new = size_treatment
from sklearn.preprocessing import LabelEncoder $ le = LabelEncoder() $ y_train = le.fit_transform(train[target])
ffr2008.plot()
df.source.value_counts().plot(kind='bar');
df_twitter_archive_copy = df_twitter_archive.copy() $ df_img_predictions_copy = df_img_predictions.copy() $ df_tweet_data_copy = df_tweet_data.copy()
vectorizers = cvec.fit_transform(xprep.billtext) $ df_vec = pd.DataFrame(vectorizers.todense(), columns = cvec.get_feature_names()) $
(own_star[~own_star['starred'].isnull() & ~own_star['owned'].isnull()]).shape
requests.get(wikipedia_meritocracy)
occurrences[occurrences['job_id'] == 68551].to_csv('abcd.csv')
y = X.sold $ del X['sold']
senti = df.groupby('subreddit')['SA'].mean()
l = LogisticRegression() $ l.fit(x_train,y_train) $ y_pred = l.predict_proba(x_test)
autos.info() $ autos.head()
metrics.accuracy_score(rf.predict(x_test), y_test)
df2[df2.user_id.duplicated()]
print(len(terror)) $ terror.head()
df = pd.read_json('data/data.json')
unique_domains.query('mentions >= 10').sort_values('avg_payout', ascending=False).head()
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)
cheap_budget_vote = sorted_budget_cheapest.groupby('original_title')['vote_average'].mean()
tweet_json_clean['followers_count'] = tweet_json_clean.user.map(lambda x: x['followers_count'])
ks_cat_failed = ks_categories.drop(ks_categories.index[ks_categories.state != 'failed']) $ ks_cat_failed.set_index('main_category', inplace=True) $ ks_cat_failed
park.named_drug.notnull().sum()
highest_confidence = stacked_image_predictions.groupby(['tweet_id'])['confidence'].max().reset_index()
sns.pairplot(iris, hue='species');
(scores $        .pipe((sns.factorplot,'data'), x='max_features', y='mean_', hue='max_depth', col='min_samples_split') $  )
cancer_data = pd.read_csv('data/small_Endometrium_Uterus.csv', sep=",")  # load data $ X = cancer_data.drop(['ID_REF', 'Tissue'], axis=1).values $ y = pd.get_dummies(cancer_data['Tissue']).values[:,1] $ print X.shape, y.shape
archive.rating_numerator.value_counts()
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y, $                                                     test_size=0.5)
root_universe = openmc.Universe(name='root universe', cells=[cell])
vc = data.loc[:,'C3 Complaint Type'].value_counts() $ vc.head(n=10)
path =r'Data/ebola/sl_data' $ sl =read_data(path, 'variable', 'sl') $ frames = [guinea, liberia, sl] $ concatenated = pd.concat(frames) $
print(90*'_') $ print("\nCount of players in each cluster") $ print(90*'_') $ pd.value_counts(model.labels_, sort=False)
date_info['visit_date'] = pd.to_datetime(date_info['visit_date']) $ date_info['day_of_week'] = lbl.fit_transform(date_info['day_of_week']) $ date_info['visit_date'] = date_info['visit_date'].dt.date
df.head()
test = Plot['mLayerVolFracWat'].data $ dates = Plot['time'].data $ test = np.squeeze(test) $ df = pd.DataFrame(data = test, index=dates) $ df.replace(to_replace=-9999.0, value = 0, inplace=True)
csvData.head(5)
mach = open("mach.json").read() $ print(type(mach),"\n") $ print(mach)
columns = ['date','favorites','id','retweets','source','text','user_flwr_count','user_name'] $ data = pd.read_csv(file_name,header = None,names = columns,parse_dates=['date'],dtype={'id':str,'favorites':str,'retweets':str,'user_flwr_count':str}) $ new_df = data.dropna(subset=['id','favorites','retweets','user_flwr_count']) $ new_df.to_csv(file_name, index=False,header=False) $ data = pd.read_csv(file_name,header = None,names = columns,parse_dates=['date'],dtype={'id':str,'favorites':int,'retweets':int,'user_flwr_count':int})
just_team_year_and_wins = df[['teamid', 'year', 'wins']] $ just_team_year_and_wins.head(10)
coinbase_btc_eur_min['Timestamp'] = pd.to_datetime(coinbase_btc_eur_min['Timestamp'], format="%Y/%m/%d %H:%M")
df_predictions_clean.head()
new_p_mean = new_page_converted.mean() $ old_p_mean = old_page_converted.mean() $ print(new_p_mean) $ print(old_p_mean) $ print(new_p_mean - old_p_mean)
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new,n_old],alternative='larger') $ print("z-score",z_score) $ print("p-value",p_value)
churn_df = pd.read_csv("ChurnData.csv") $ churn_df.head()
calls_nocontact.neighborhood_district.value_counts()
data_libraries_df.to_csv("output/data_libraries_{}.tsv".format(date), index=False, sep="\t", na_rep="NA")
query = session.query(Station).all() $ print(len(query))
daily_averages['2014'].head()
print('Column array using a variable') $ WHO_Region=df['WHO Region'] $ np.array(WHO_Region)
qrt = closePrice.resample('Q').mean()
stat_info_st = stat_info[0].apply(fix_space) $ print(stat_info_st)
full_dataset.loc[full_dataset.team_A_name == 'NRG']
games_to_bet.head()
!open table.html
autos["price_dollars"].value_counts().sort_index(ascending=False)
births['day'] = births['day'].astype(int)
df["user_name"] = df["user"].apply(lambda x: x["screen_name"])
ab_file2[((ab_file2['group'] == 'treatment') == (ab_file2['landing_page'] == 'new_page')) == False].shape[0]
session.query(Measurement.date, Measurement.prcp).filter(Measurement.date > '2016-08-23').all() $
to_be_predicted_Day2 = 21.38782431 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
X_train.values
to_be_predicted_Day2 = 43.14093082 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
labels.columns = ['id','label']
null_columns = nba_df.columns[nba_df.isnull().any()] $ nba_df[null_columns].isnull().sum()
dul_isbns = dul['ISBN RegEx'] $ dul_isbns.size
All_tweet_data_v2.name[All_tweet_data_v2.name.str.len() < 3].value_counts()
raw_data = raw_data.sort_values(by=["datetime", "campaign_id", "campaign_spend"]) $ part1_flt = raw_data.loc[raw_data.campaign_id.map(lambda x: x in [1,2])] $ part1_flt[:5]
d_utc = d.tz_localize('UTC') $ d_utc
learner.save_encoder('lm1_enc') # We've got a good one saved. 8.24
df_inventory_santaclara['Year']=(df_inventory_santaclara['Date'].str.split('-').str[0]) $ df_inventory_santaclara['Month']=(df_inventory_santaclara['Date'].str.split('-').str[1]) $ df_inventory_santaclara
autos['kilometer'].describe()
from quilt.data.dsdb import processing_test $ processing_test
autos["ad_created"].str[:10].value_counts(normalize=True, dropna=False).sort_index().plot(kind="bar", title="Ad_created (All)", colormap="Blues_r")
df.to_csv('cleaned_politifact_df',encoding='utf-8')
aux.loc[~ (aux.cuteness_y == 'nan'),:].head()
from jira.client import JIRA $ options = {'server': 'https://intertec.atlassian.net'} $ jp = JIRA(options=options, basic_auth = ('Carlos.Sell', 'Nadia#663s494'))
final_topbikes = final_topbike.head(0)
df.sort_values(by=['userid', 'price'])
df_train.head()
np.random.seed(123456) $ dates = ['2014-08-01', '2014-08-02'] $ ts = pd.Series(np.random.randn(2), dates) $ ts
models.save('models')
df_goog.dtypes
from datetime import datetime $ x = [datetime.strptime(d, '%Y-%m-%d %H:%M:%S.%f') for d in df['time']] $ y = df['moisture']
for t in nocol.itertuples(): $     print(t.day)
train, test = train_test_split(df, test_size=0.3)
df[df.speaking_line].groupby('episode_id')['id'].nunique().head()
df_tweet_clean.rename(columns={'id': 'tweet_id'}, inplace=True)
df_t = df.groupby(['user_id','nd_key_formatted'])['course_key'].apply(lambda x: ', '.join(x)).reset_index() $ df_t
y.T-30
github_data.head(2)
unseen_predictions = rsskb_gbm_best.predict(X_unseen[possible_features]) $ print 'Accuracy: ', accuracy_score(y_unseen, unseen_predictions) $ print classification_report(y_unseen, unseen_predictions)
f_counts_week_channel.show(1)
%%time $ M_NB_model.fit(X_train_term, y_train)
focount.to_csv('followers.csv', index=False)
archive_clean.loc[archive_clean['rating_denominator'] != 10, 'rating_denominator'] = 10
def manager(x): $     if 'Manager' in x: $         return 1 $     return 0 $ df_more['Manager'] = df_more['Title'].apply(manager)
soup = BeautifulSoup(html, 'lxml') $ titles = soup.findAll("a", {"data-event-action": "title"})
df_clean['tweet_id'] = df_clean['tweet_id'].astype('str') $ image_clean['tweet_id'] = image_clean['tweet_id'].astype('str') $ tweet_clean['tweet_id'] = tweet_clean['tweet_id'].astype('str')
Nold = df2.query('group == "control"')['user_id'].nunique() $ print (Nold)
to_be_predicted_Day5 = 21.37348327 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
new_page_converted = np.random.choice([0,1], size=n_new, p=[p_new, 1-p_new])
dates = pd.date_range('10-01-2016', periods=9, freq='2W-SUN') $ dates
images.set_index('tweet_id', inplace = True) $ df2 = pd.merge(left=archive, right=images, left_index=True, right_index=True, how='left') $ df2 = pd.merge(left=df2, right=df, left_index=True, right_index=True, how='left') $ df2.to_csv('df2copy.csv', encoding = 'utf-8')
import os $ print('\n'.join(os.environ['PATH'].split(':')))
mydate = datetime.strptime("12-03-97", "%m-%d-%y") $ print('Year: ', mydate.strftime("%Y"))
sqlClient.get_cos_summary(targeturl)
train_df = pd.read_csv('../input/train_cont.csv') $ test_df = pd.read_csv('../input/test_cont.csv')
df_input_clean = df_input_clean.withColumn("isLate", (df_input_clean.Resp_time > 1).cast('integer'))
p_old = sum(df2.converted == 1) / 290584 $ p_old
result = linear_predictor.predict(train_set[0][30:31]) $ print(result)
samp311 = cp311.sample(frac = .2,random_state=757859 )
globalCityContent = readPDF(globalCityBytes) $ globalCitySentences = globalCityContent.replace('\n','').split('.') $ type(globalCitySentences)
to_be_predicted_Day3 = 31.23086009 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
blahblah = 'This string is stored in the variable on the left.' $ blahblah
beds = 1 $ for i in range(3): $     bed_df.set_value(i, 'bedrooms', beds) $     beds +=1 $ print(bed_df)
loans_df.isnull().sum()
m3 = np.around(m3,2) $ print("m3: ", m3)
top_200.head()
df2[df2['group']=='treatment']['converted'].mean()
elon['nlp_text'] = elon.text.apply(lambda x: tokenizer.tokenize(x.lower())) $ elon.nlp_text = elon.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) $ elon.nlp_text = elon.nlp_text.apply(lambda x: ' '.join(x))
control_new_page = df[(df.group == 'control') & (df.landing_page == 'new_page')] $ treatment_old_page = df[(df.group == 'treatment') & (df.landing_page == 'old_page')] $ control_new_page.shape[0] + treatment_old_page.shape[0]
t0.shape, t1.shape
grouped_df = surveys_df.groupby(['sex','year'])['wgt'].mean().unstack() $ grouped_df.plot(kind="bar") $ plt.xlabel("Sex") $ plt.ylabel("Average Weight") $ plt.show()
names = pd.Series(data) $ names
train_df.head()
plot = sb.boxplot(x=dfEPEXbase.index.hour, y='Price', data=dfEPEXbase) # plot distribution of hourly prices $ plot.set_title('Distribution of Hourly Prices')
df = pd.read_csv('data_textmining.csv', index_col=0, encoding='utf-8-sig') $ df = df.drop([ 'currency', 'deadline', 'goal', $        'launched', 'pledged', 'state', 'backers', 'date_diff', 'rtdate', 'rate', 'goal_usd', 'pledged_usd'], axis=1) $ dfname = df.copy() $ dfname.head()
print("Number of Mitigations in ATT&CK") $ mitigations = lift.get_all_mitigations() $ print(len(mitigations)) $ df = json_normalize(mitigations) $ df.reindex(['matrix', 'mitigation', 'mitigation_description', 'url'], axis=1)[0:5]
(p_diffs>p_diff).mean()
df['HIGH_LOW']=(df['num_comments']>df['num_comments'].median()).astype(int) $ print(df.shape) $ df.head()
plt.figure(figsize=(20,8)) $ sns.swarmplot(x='Type 1', y='Total', data=pokemon, hue='Legendary') $ plt.axhline(pokemon['Total'].mean(), color='red', linestyle='dashed') $ plt.show()
s3 = pd.Series(np.arange(12, 14), index=[1, 2]) $ pd.DataFrame({'c1': s1, 'c2': s2, 'c3': s3})
err.subject.unique()
import statsmodels.api as sm $ logit = sm.Logit(df2['converted'],df2[['intercept' ,'treatment']]) $ results = logit.fit()
train.shape
df_new['intercept'] = 1 $ lm = sm.OLS(df_new['converted'], df_new[['intercept', 'CA', 'UK']]) $ results = lm.fit() $ results.summary()
pokemon['Total']= pokemon['HP']+pokemon['Attack']+pokemon['Defense']+pokemon['Sp. Atk']+pokemon['Sp. Def']+pokemon['Speed'] $ pokemon.head()
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_axial_brace_crown[0][0])) $ perr = np.sqrt(np.diag(pcov_axial_brace_crown[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df_total = pd.concat([df_uro_no_cat, df_dummies], axis=1)
part_of_site_name = input('What are some of the letters in site name?') $ part_of_site_name = part_of_site_name.upper() $ matching = [s for s in Site_names if (part_of_site_name in s )] $
print(regexResults.group(0))
workbook = xlrd.open_workbook(datapath / datafile) $ sheet = workbook.sheet_by_index(0)
df_us.shape[0], df_ca.shape[0], df_uk.shape[0]
Ntree = 500 $ rfc = RandomForestClassifier(n_estimators=Ntree) $ y_pred = cross_validation.cross_val_predict(rfc, X, y, cv=5) $ print(metrics.accuracy_score(y, y_pred))
merge_event.loc[(merge_event.user_type == 1)].groupby('nweek')['user_id'].nunique().plot(label="user 1") $ merge_event.loc[(merge_event.user_type == 2)].groupby('nweek')['user_id'].nunique().plot(label="user 2") $ merge_event.loc[(merge_event.user_type == 3)].groupby('nweek')['user_id'].nunique().plot(label="user 3") $ plt.legend() $ plt.show()
w = 'jew' $ model.wv.most_similar (positive = w)
my_model_q4 = SuperLearnerClassifier(clfs=clf_base_default, stacked_clf=clf_stack_dt, training='label') $ my_model_q4.fit(X_train, y_train) $ y_pred = my_model_q4.predict(X_test) $ accuracy = metrics.accuracy_score(y_test, y_pred) $ print("Accuracy: " +  str(accuracy))
p = sns.factorplot('vehicleType',data=autodf,kind='count') $ p.set_xticklabels(rotation=30) #letitbe
data2 = r.json()['dataset_data']['data']
testObj.outDF.tail()  ## final records in the output
data[data['Processing Time']>datetime.timedelta(148,0,0)]
bday = datetime(1986, 3, 6).toordinal() $ now = datetime.now().toordinal() $ now - bday
baseball.hr.cov(baseball.X2b)
for y in range(2014, 2018): $     print(y, it_df[it_df["bidDeadlineYear"] == y].price.sum())
vect = CountVectorizer() $ vect.fit(X_train)
data = pd.merge(data, pickup_demand, left_on= ['floor_15min', 'pickup_cluster'], right_on=['ceil_15min', 'pickup_cluster'], how='left') $ data = pd.merge(data, dropoff_demand, left_on= ['floor_15min', 'dropoff_cluster'], right_on=['ceil_15min', 'dropoff_cluster'], how='left') $ data = pd.merge(data, ride_demand, left_on= ['floor_15min', 'ride_cluster'], right_on=['ceil_15min', 'ride_cluster'], how='left')
rmse =((np.sum(y_error**2))/len(y_error))**0.5 $ print(rmse)
jobs.loc[(jobs.FAIRSHARE == 3) & (jobs.ReqCPUS == 8) & (jobs.GPU == 0)].groupby(['Group']).JobID.count().sort_values(ascending = False)
svm_classifier.score(X_test, Y_test)
df2.converted.sum()/df2.shape[0]
sum(trumpint.num_shares),sum(cliint.num_shares)
df_new[['US','UK','CA']] = pd.get_dummies(df_new['country']) $ df_new.drop('CA', axis = 1, inplace = True) $ df_new.head()
autos["brand"].unique().shape
p_new = df2.query('landing_page == "new_page"').shape[0]/df2.shape[0] $ p_new
df_summary = pd.DataFrame(index=pd.date_range(start=start_date, end=end_date, freq='M')) $ df_summary['tubes'] = temp_df.set_index('date').resample('M').sum().num_tubes $ df_summary.fillna(inplace=True, value=0) $ df_summary=df_summary.join(df_summary.groupby(df_summary.index.year).cumsum(), rsuffix='_cumul') $ df_summary.head()
stories.head()
fav_max = np.max(data['Likes']) $ rt_max = np.max(data['RTs']) $ fav = data[data.Likes == fav_max].index[0] $ rt  = data[data.RTs == rt_max].index[0]
fig = plt.figure(figsize=(12,8)) $ ax = fig.add_subplot(111) $ fig = qqplot(resid_713, line='q', ax=ax, fit=True)
tweet_archive_clean.stage = tweet_archive_clean.stage.apply(lambda x:x.replace('None','')) $ tweet_archive_clean.stage = tweet_archive_clean.stage.apply(lambda x:x.replace('doggopuppo','doggo')) $ tweet_archive_clean.stage = tweet_archive_clean.stage.apply(lambda x:x.replace('doggofloofer','doggo')) $ tweet_archive_clean.stage = tweet_archive_clean.stage.apply(lambda x:x.replace('doggopupper','doggo')) $ tweet_archive_clean.drop(['doggo', 'floofer','pupper', 'puppo'],  axis=1, inplace=True)
df2.query('landing_page == "new_page"').landing_page.count()/df2.shape[0]
vect = TfidfVectorizer(ngram_range=(2,5), stop_words='english') $ summaries = "".join(una_tweets['text']) $ ngrams_summaries = vect.build_analyzer()(summaries) $ Counter(ngrams_summaries).most_common(20)
df['WHO Region'].values
logit.fit().summary()
Train_extra = Train.merge(train_dum_clean, right_on='ID', left_index=True) $ Train_extra.head()
df['rating_denominator'].value_counts()
df.columns = ['id', 'username', 'posted_datetime', 'comments']
Precipitation_DF.head(10)
df_users.invited_by_user_id.isnull().values.ravel().sum()
MATTHEWKW.head()
old_page_converted = np.random.choice([1,0], size=n_old, p=[p_old, 1-p_old]) $ old_page_converted
epoch3_df.shape
beta_dist[np.arange(mcmc_iters), betas_argmax] /= 10000
results=logistic_model2.fit()
from pandas import DataFrame, read_csv $ import matplotlib.pyplot as plt $ import pandas as pd $ import sys $ %matplotlib inline
plt.figure(3) $ appl_plot = plt.subplot() $ appl_plot.plot(appl['Close'],color='red') $ plt.legend(['Apple Close Value'],loc="upper left") $ plt.title('Valores de Cierre Apple')
goog.head()
X_train = train_set[:,lookforward_window:].copy() $ y_train = train_set[:,:lookforward_window].copy() $ X_test = test_set[:,lookforward_window:].copy() $ y_test = test_set[:,:lookforward_window].copy()
df_h1b_nyc_ft.groupby(['lca_case_employer_name'])['total_workers'].sum()
import test_package.print_hello_class_container $ my_instance = test_package.print_hello_class_container.Print_hello_class() $ my_instance
print('Answer 2.A: ', '\n') $ print(csv_df['names'].value_counts(), '\n') $ for i in set(csv_df['names']): $     print(i, ": ", len(set(csv_df['date'][csv_df['names']==i])))
td_wdth = td_norm * 1.5 $ td_alph = td_alpha $ td[:] = td[:].astype(int)
twitter_archive_master['favorite_count'].corr(twitter_archive_master['retweet_count'])
import statsmodels.api as sm $ df2['intercept'] = 1 $ logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit() $ results.summary()
X.head(10)
crimes_wea.to_csv('data/crime/crimes_weather.csv')
googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]
m.plot(forecast,xlabel="Timestamp",ylabel="Value");
inputs = tf.placeholder(tf.int32, [None]) $ targets = tf.placeholder(tf.int32, [None])
volume_m = volumes.resample('M').sum()
tweet = "Wed Aug 27 13:08:45 +0000 2008" $ time_info = datetime.strptime(tweet,'%a %b %d %H:%M:%S +%f %Y') $ print('Day of week: ', time_info.strftime("%a")) $ print('Hour: ', time_info.strftime("%H")) $ print('Year: ', time_info.strftime("%Y"))
X2.time_since_meas_years.hist()
students.loc['Ryan']['weight']
reg_df = df2.copy() $ reg_df['intercept'] = 1 $ reg_df['ab_page'] = (reg_df['group'] == 'treatment')*1 $ reg_df.head()
dupID = '2017-01-09 05:37:58.781806' $ df2 = df2[df2.timestamp != dupID]
ebay["2017-01"].plot()
new_page_converted = np.random.choice([0, 1], size= new, p=[(1-x), x]) $ new_page_converted.mean() $
data.groupby("center_name").count()
learning_rate = .01 $ embed_size = 300 $ batch_size = 64 $ steps = 1000000
ndvi_mean_change =ndvi_change.mean(dim=('x','y')) $ percentage_ndvi_mean_change= (ndvi_mean_change /(ndvi_of_interest.mean(dim=('x','y'))))*100 $ print('Average difference in NDVI: ' +str(ndvi_mean_change.values)[0:5]) $ print('percentage change of NDVI: ' +str((percentage_ndvi_mean_change.values))[0:5]+'%')
from datetime import datetime, timedelta $ df_gene = df[df['action']=="click_genetic_rec"] $ df_gene['created_at'] = pd.to_datetime(df_gene["created_at"]) $ df_gene['created_atc']=df_gene['created_at'].dt.tz_localize('UTC').dt.tz_convert('Asia/Bangkok') $
df_tte_all.columns
s = pd.Series([1,2,3,4,5],index=['a','b','c','d','e']) $ s
autos["registration_year"].value_counts(normalize = True, dropna = False).sort_index()
uber_14["day_of_week"].value_counts()
liquor2016_q1.Whisky = liquor2016_q1.CategoryName.str.contains('WHISK') * liquor2016_q1.SaleDollars $ liquor2016_q1_whisky = liquor2016_q1.Whisky.groupby(liquor2016_q1.StoreNumber).agg(['sum']) $ liquor2016_q1_whisky.columns = ['Whisky'] $ liquor2016_q1_whisky.tail()
re.sub(rpt_regex, rpt_repl, "Reppppeated characters in wordsssssssss" )
microsoft = MovingAverage('msft', msft, 40, 100) $ print(microsoft.generate_signals())
meta = pd.read_excel("./Data/microbiome/metadata.xls", index_col="BARCODE").fillna(value="Na")
ssc.checkpoint("dir")
a = range(1, 6) $ b = range(7, 12) $ np.stack((a, b), axis=1)
co_occurence_on_top50_slug = {datasets_id_slug.get(k): [(datasets_id_slug.get(el[0]), el[1]) for el in v] for k, v in co_occurence_on_top50.items()}
df_2004['bank_name'] = df_2004.bank_name.str.split(",").str[0] $
from sklearn.model_selection import RandomizedSearchCV
cols_to_export = ["epoch","src","trg","src_str","src_screen_str","trg_str","trg_screen_str"] $ mentions_df.to_csv("/mnt/idms/fberes/network/usopen/data/uso17_mentions_with_names.csv",columns=cols_to_export,sep="|",index=False)
df_sched2.dtypes
bucket.upload_dir('data/city-util/proc', 'city-util/proc', clear_dest_dir=True)
df.quantile(.75) - df.quantile(.25)
stocks = stocks.rename(columns = {'id' : 'stock_code', 'name' : 'company'})
df[df['Agency']=='NYPD']['Complaint Type'].value_counts()
go_no_go_times = go_no_go.groupby('subject_id').Time.unique() $ simp_rxn_time_times = simp_rxn_time.groupby('subject_id').Time.unique() $ proc_rxn_time_times = proc_rxn_time.groupby('subject_id').Time.unique()
glm_binom_feat_3.accuracy(valid=True)
executable_path = {'executable_path': '/usr/local/bin/chromedriver'} $ browser = Browser('chrome', **executable_path, headless=False) $ url = "https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars" $ browser.visit(url) $ hemisphere_image_urls = []
autos = autos.rename(columns={"odometer": "odometer_km"})
no_newpage_treatment = df[(((df.group == 'treatment') & (df.landing_page != 'new_page')) | ((df.group != 'treatment') & (df.landing_page == 'new_page')))] $ no_newpage_treatment.shape[0]
for df in (joined,joined_test): $   df['CompetitionOpenSince']=pd.to_datetime(dict(year=df['CompetitionOpenSinceYear'], $                                                 month=df['CompetitionOpenSinceMonth'], $                                                 day=15)) $   df['CompetitionDaysOpen']=df['Date'].subtract(df['CompetitionOpenSince']).dt.days
display(merged_data.iloc[10000:10010])
tweet_archive.shape
PCA(2).fit(X)
plt.scatter(e_pos['Polarity'], e_pos['Subjectivity'], alpha=0.3, color='purple') $ plt.title('Positive tweets #Election2018, Subjectivity on Polarity') $ plt.ylabel('Subjectivity') $ plt.xlabel('Polarity') $ plt.show()
tweets = api.user_timeline('BTS_SkyTrain', count=200) $ for i in range(15): $     tweets += api.user_timeline('BTS_SkyTrain', count=200, max_id=tweets[-1].id)
df["Borough"].value_counts()
df_joined[['CA', 'UK', 'US']] = pd.get_dummies(df_joined.country) $ df_joined = df_joined.drop(['CA'], axis=1) $ df_joined.head()
temp.shape
rtitle = [x.text for x in soup.find_all('a', {'data-event-action':'title'})] $ rtitle.pop(0) $ subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})] $ rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})] $ comments = [x.text for x in soup.find_all('a', {'class':'bylink comments may-blank'})]
plot_confusion_matrix(cm_dt, classes=['COLLECTION', 'PAIDOFF'],normalize=False, title="Confusion matrix for decision tree", cmap=plt.cm.Blues)
logit3 = sm.Logit(df_new['converted'], df_new[['intercept','ab_page','CA','US']]) $ r3 = logit3.fit() $ r3.summary()
talks_train.columns
df2[df2['landing_page'] == 'new_page'].shape[0] / df2.shape[0]
random_integers.values
count_liberia = grouped_months_liberia.count() $ count_liberia=count_liberia.rename(columns = {'National':'count_v_T'}) $
import statsmodels.api as sm $ convert_old = df2.query('group == "control"')['converted'].sum() $ convert_new = df2.query('group == "treatment"')['converted'].sum() $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
df, y, nas, mapper = structured.proc_df(joined_samp, 'Sales', do_scale=True)
run txt2pdf.py -o "FLORIDA HOSPITAL  Sepsis.pdf"   "FLORIDA HOSPITAL  Sepsis.txt"
run txt2pdf.py -o"2018-06-19 2012 FLORIDA HOSPITAL Sorted by payments.pdf"  "2018-06-19 2012 FLORIDA HOSPITAL Sorted by payments.txt"
final = cryptoprice.join(txt, how = 'outer')
x.drop(["sum"], axis=1, inplace=True) $ x
df["new_customer"] = (df["new_customer"] == 1).astype(int)
y_size.describe()
cityID = 'cb74aaf709812e0f' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tulsa.append(tweet) 
iris.loc[:,"Species"] = iris.loc[:,"Species"].astype("category")
pd.concat([df,s3,s1],axis=1)
import pandas as pd $ import numpy as np $ topUserItemDocs=pd.read_pickle('datasets/topUserItemDocsCB_27Jun_vFULL300.pkl')
X['is_cat'] = X['title'].map(lambda x: 1 if 'cat' in x else 0) $ X['is_funny'] = X['title'].map(lambda x: 1 if 'funny' in x else 0)
sortedRemovedWordsList = np.sort(removedWordsList) $ dfx = pd.DataFrame(sortedRemovedWordsList) $ dfx.to_csv("removed_words.csv", index=False, index_label=False)
affordability = SCC_med_inc.merge(SCC_MSP_year, left_on = 'Year', right_on='Year') $ affordability
joined['Retire'].dtypes
df_sale_price=pd.read_csv('Sale_Prices.csv',index_col=0) $ df_sale_price.head(5)
df.head()
dont_align = df.query('(group != "treatment" and landing_page == "new_page") or (group == "treatment" and landing_page != "new_page")') $ dont_align.shape[0]
df['Grades'].astype('category').head()
df.sample(3)
station_cluster = (turnstile_cluster $                      .groupby(['STATION','DATE'])['DAILY_ENTRY'] $                      .sum() $                      .reset_index() $                      )
fuel_mgxs.print_xs()
X_traincvWV, X_testcvWV, y_traincvWV, y_testcvWV = model_selection.train_test_split(trainDataVecs, $                                                                                     train["sentiment"], $                                                                                     test_size=0.2, $                                                                                     random_state=0)
dfETHVol.corr()
rows = session.query(Adultdb).filter_by(occupation="?").all() $ print("-"*100) $ print("Count of rows having occupation '?' after delete: ",len(rows)) $ print("-"*100)
params = {'figure.figsize': [8, 8],'axes.labelsize': 'Medium', 'font.size': 12.0, 'lines.linewidth': 2} $ plot_decomposition(doc_duration, params=params, freq=31, title='Doctors Decomposition')
X_train, X_test, y_train, y_test = train_test_split(X_s_n, y_sc, test_size=0.5, random_state=42)
city_names = pd.Series(['San Francisco', 'San Jose', 'Sacramento']) $ population = pd.Series([852469, 1015785, 485199]) $ pd.DataFrame({ 'City name': city_names, 'Population': population })
journalists_retweeted_by_female_summary_df = journalist_retweet_summary(journalists_retweet_df[journalists_retweet_df.gender == 'F']) $ journalists_retweeted_by_female_summary_df.to_csv('output/journalists_retweeted_by_female_journalists.csv') $ journalists_retweeted_by_female_summary_df[journalist_retweet_summary_fields].head(25)
plt.rcParams['axes.unicode_minus'] = False $ dta_59.plot(figsize=(15,5)) $ plt.show()
act_diffs = df2.query('group == "treatment"').converted.mean() - df2.query('group == "control"').converted.mean() $ act_diffs
result4 = sm.ols(formula="HPr_RF ~ Mkt_RF + SMB + HML", data=tbl3).fit() $ result4.summary()
mismatch.head(15)
predictMostListened(allData).take(2)
plt.hist(MaxPercentage, bins=50,alpha=0.5) $ plt.hist(RandomPercentage, bins=50,alpha=0.5) $ plt.ylabel("Frequency") $ plt.xlabel("percentual change used for profit") $ plt.show()
pd.crosstab(test_df_01.label, test_df_01.predict)
zf_train = zipfile.ZipFile('train.csv.zip', mode='r') $ zf_test = zipfile.ZipFile('test.csv.zip', mode='r')
reg.intercept_
cursor = connection.cursor() $
staff.Name 
import sys $ src_dir = os.path.join('..', 'src') $ sys.path.append(src_dir)
pizza_ratings.head()
'Remember when we learned about split?'.split()
slices = dataset.Sentiment.value_counts() $ activities = ['Neutral', 'Positive', 'Negative']
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2012 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2012 Percentiles.txt"
sex2NA=['adult','juv'] $ sex2m=['unm'] $ df.loc[df.sex.isin(sex2NA)==True] $ print(df.sex.loc[df.sex.isin(sex2NA)==True].count()) $ print(df.sex.loc[df.sex.isin(sex2m)==True].count())
r1.json()
ethPrice = pd.read_csv("btc.csv", sep=',',parse_dates=['date']) $ ethPrice.dtypes
titanic.describe()
data = check_y(df_1, delta_change=-3.0, start = 480, end = 600) 
plt.scatter(range(len(train_df)), train_df.price)
recommendation_df.shape
cbow_m5 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=100)
rf = RandomForestClassifier(n_estimators = 30) $ rf.fit(X_train, y_train) $ rf.score(X_test, y_test)
df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
M = prob1-prob $ p_diffs = np.array(p_diffs) $ (p_diffs > M).mean() $
df2[df2['user_id'].duplicated()==True]
twitter_archive_clean[twitter_archive_clean.expanded_urls.isnull()]
us.loc[us['country'].str.len() == 2, 'country'].value_counts(dropna=False)
A = np.random.randint(0,10,(3,4,3,4)) $ sum = A.sum(axis=(-2,-1)) $ print(sum) $ sum = A.reshape(A.shape[:-2] + (-1,)).sum(axis=-1) $ print(sum)
print(ozzy.name + ' is ' + str(ozzy.age) + 'year(s) old.')
reddit.shape
table = df[['text', 'timestamp_ms', 'coordinates', 'id', 'user_location']].set_index('id')
dr_num_providers = doctors['Provider'].resample('W-MON', lambda x: x.nunique())
old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_under_null, p_under_null]) $ print(old_page_converted)
from scipy.stats import norm $ norm.cdf(z_score) # Tells us how significant our z-score is
conv = df2[df2['converted']==1]['user_id'].unique().shape[0] $ total = df2['user_id'].unique().shape[0] $ pold = pnew = conv/total $ pnew
tweet_json_clean = tweet_json_df.copy()
twitter_data.name.value_counts()
df.track_popularity.nunique()
to_be_predicted_Day2 = 81.77623296 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
clean_rates.created_at = pd.to_datetime(clean_rates.created_at, format='%a %b %d %H:%M:%S +%f %Y')
from sklearn import preprocessing $ le = preprocessing.LabelEncoder() $ le.fit(list(set(df[:1000]['brand']))) $
automl_feat.fit(X_train, y_train, $            dataset_name='psy_native', $            feat_type=feat_type)
hist.transpose()
session.query(Station).count() $
print(dataframe1.tail()) $ y=dataframe1['SMA_30'] $ plt.plot(y) $ plt.show()
sp_re_df.columns = models
target_column = 'DGS30' $ col_list.remove(target_column)
x = np.arange(1,101) $ y = np.tile(x,5) $ news_df['Tweet No.'] = y $ news_df = news_df[['Tweet No.', 'Source Acc.', 'Tweet', 'Date', 'Compound Score', 'Pos Score', 'Neu Score', 'Neg Score']] $ news_df.head()
summary = records.describe(include='all') $ summary $
print(str(out.stdout.decode('utf-8')))
df_clean['dog_nick'].value_counts()
voc=vectorizer.vocabulary_
control_group = df2.group == 'control' $ control_group_and_converted = control_group & (df2.converted == 1) $ len(df2[control_group_and_converted]) / len(df2[control_group])
tweet_archive_clean.rating_denominator.value_counts()
grades["dec"] = np.nan $ final_grades = grades + better_bonus_points $ final_grades
learner.load_encoder('adam1_enc')
from google.colab import files $ files.upload()
pred.head()
df_clean.loc[df_clean['expanded_urls'].str.extract(pat=r'(photo)', expand=False).isnull(), :]
ebola_0 = ebola.fillna(value=0) $ ebola_0.info()
(p_diffs > actual).mean()
df2.query( $     '(landing_page=="new_page" & group=="control") or (landing_page=="old_page" & group=="treatment")' $     ).count()['user_id']
close_prices.groupby([close_prices.index.year, close_prices.index.month]).min()
psa_proudlove = pd.read_csv('psa_proudlove.csv', parse_dates=True, index_col='Date') $ psa_perry = pd.read_csv('psa_perry.csv', parse_dates=True, index_col='Date') $ psa_all = pd.read_csv('psa_all.csv', parse_dates=True, index_col='Date')
building_pa_prc_shrink=building_pa_prc.drop(columns=cols_filt) $ building_pa_prc_shrink.head()
totalInputs = reduce((lambda x, y: x + y), inAny).sort_values() $ plt.plot(df['id'], totalInputs, 'ro') $ plt.xlabel('Mechanism ID') $ plt.ylabel('totalInputs') $ plt.show()
gender_vars.columns = gender_vars.columns.str.lower()
df.head()
treatment_ctr=treatment_df.query('converted==1').user_id.nunique()/treatment_df.query('converted==0').user_id.nunique() $
labels['Key_id'].iloc[40]
guineaCases = guineaCases.rename(columns = {'Totals':'New cases'}) $ guineaDeaths = guineaDeaths.rename(columns = {'Totals':'New deaths'})
df_train.max(axis=0)
conn = hive.Connection(host=cred['host_hive'], database="default", port=10000, auth="LDAP", username=cred['username'], password=cred['password_hive'])
z_score, p_value = sm.stats.proportions_ztest(count=[convert_new, convert_old], nobs=[Nnew, Nold], alternative='larger') $ print("z-score:", z_score,",p-value:", p_value)
bacteria_data.value
roll_test_df.columns
df.Genre.value_counts() #GENRE!!!
small_big_coeffs = pd.concat([coeffs.sort_values().head(20), $                               coeffs.sort_values().tail(20)])
!ls -lh store.h5
y = df.groupby('Journal')['Title'].count() $ df_less = df[df['Journal'].apply(lambda x: y[x]>50)]
lat = temp_nc.variables['lat'][16:26] $ lon = temp_nc.variables['lon'][94:118]-360 $ time = temp_nc.variables['time'][833:846] $ temp = temp_nc.variables['air'][833:846,16:26,94:118] $ np.shape(temp)
sites_no_net.head()
snow.select ("select * from st_alb_angina limit 100")
providers_schedules.info()
pd.concat(g for _, g in df2.groupby("user_id") if len(g) > 1)
liquor2016 = liquor[liquor.Date.dt.year == 2016]
tweet_full_df['tweet_id']=tweet_full_df['tweet_id'].astype(str)
pd.to_datetime([1349720105, 1349806505, 1349892905, 1349979305, 1350065705], unit='s')
sc.uiWebUrl
ax=temp_df.plot.area(stacked=False, alpha=0.2) $ ax.xaxis.set_tick_params(rotation=30) $ plt.tight_layout()
pd.DataFrame(authors_test)[0].value_counts()
df_batch6_sec['Date'] = df_batch6_sec.index
yhat = model.predict_classes(x, batch_size=32) $ acc = sum(yhat==ynum) $ print("Accuracy = ",acc/len(ynum)) $ from sklearn.metrics import confusion_matrix $ confusion_matrix(yhat,ynum)
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ Generate_code_to_generate_PDF_files_from_TXT()
df.pipe(adder,2)
full_globe_temp = pd.read_table(filename, sep="\s+") $ full_globe_temp
nrows, ncols = 100000, 100 $ rng = np.random.RandomState(42) $ df1, df2, df3, df4 = (pd.DataFrame(rng.rand(nrows, ncols)) for i in range(4)) $ df1, df2, df3, df4
borough_population_complaints = pd.concat([borough_population, borough_complaints], axis=1) $ borough_population_complaints.columns = ['population', 'complaints'] $ borough_population_complaints['per_cap_complaints'] = borough_population_complaints['complaints'] / borough_population_complaints['population'] $ borough_population_complaints.sort_values(by='per_cap_complaints', ascending=False)
spon.head(3)
df['date'].min()
gg.ggplot(gg.aes(x='day', y='tavg', color='series'), $           data=eval_data) + gg.geom_line() + gg.ggtitle("Predicted vs Actual High Temp")
sum(tweet_archive_clean['jpg_url'].isnull())
df_CLEAN1A.info()
from sklearn.ensemble import RandomForestClassifier $ rf = RandomForestClassifier()
sns.boxplot(x=df.author, y=df.created_at.dt.hour); $ plt.xticks(np.arange(10), ('collins', 'hclinton', 'hirono', 'hoeven', 'mccain', 'obama', 'ryan', 'sanders', 'schwarzenegger', 'trump'), rotation = 50);
df.mean(axis=1, skipna=False)
model = Sequential() $ model.add(Dense(500, input_shape=(len(nn_X_train[0]),), activation='relu')) $ model.add(Dense(750, activation='relu')) $ model.add(Dense(len(nn_y_train[0]), activation='softmax')) $ model.compile(optimizer="Adagrad", loss='categorical_crossentropy',metrics=['accuracy'])
ans = reviews.loc[reviews.country.notnull() & reviews.variety.notnull()] $ ans = ans.apply(lambda srs: srs.country+'-'+srs.variety, axis='columns') $ ans.value_counts()
stocks.tail()
index_max = max(range(len(values)), key=values.__getitem__)
print dt $ print dt.replace(minute=0, second=0)
autos['price'] = autos['price'].str.replace('$', '').str.replace(',', '').astype(int)
pd.value_counts(merged1[merged1['AppointmentDuration'] > 90]['ReasonForVisitName'])
df['province'] = map(lambda x: int(x),df.province) $ df['city'] = map(lambda x: int(x),df.city)
timeseries.rolling(50).mean().plot(label='50 day Rolling Mean', figsize = (12,8)) $ timeseries.rolling(50).std().plot(label='50 day Month Rolling Std') $ timeseries.plot() $ plt.legend()
run txt2pdf.py -o "CEDARS-SINAI MEDICAL CENTER  Sepsis.pdf"   "CEDARS-SINAI MEDICAL CENTER  Sepsis.txt"
treatment_group_sample = df2[df2.group == 'treatment'] $ num_of_converted_in_treatment = len(treatment_group_sample[treatment_group_sample.converted == 1]) $ p_treatment_converted = num_of_converted_in_treatment/len(treatment_group_sample) $ p_treatment_converted
y_test_under[k150_bets_under].sum()
Y_df = pd.DataFrame(train_df12.iloc[:,0]) $ Y_df.head()
url = 'http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/img/knn/cos.png' $ Image(url,width=300, height=500)
df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f', 'h'], $                   columns=['one', 'two', 'three']) $ df
tweets.head()
!python download_dataset.py
model3 = linear_model.LinearRegression() $ model3.fit(x3, y) $ (model3.coef_, model3.intercept_)
trans_data["date"] = pd.to_datetime(trans_data["date"]).dt.strftime('%Y-%m-%d')
autos.info() $ autos.head()
goodreads_users_df.head(3)
path = "C:\\Users\\Alvaro\\Documents\\Python_Projects\\Pandas\\Pandas-Project2\\car_data.txt" $ df = pd.read_table(path, sep ='\s+', na_values=['.']) $ df.head(5)
with open('../../api_modules/mlxtend.feature_selection/SequentialFeatureSelector.md', 'r') as f: $     s = f.read() $ print(s)
dataBPL.tail(20)
print 'See correlation with actual: ',test_case.select('party_name').take(1) $ actual_acct_id.select('party_name').distinct().show(10,False)
df.describe()
dayofweek = pd.DatetimeIndex(pivoted.columns).dayofweek
baseball_swing_action_type = 1 $ url = form_url(f'actionTypes/{baseball_swing_action_type}') $ response = requests.get(url, headers=headers) $ print_body(response)
open_copy = opendf.copy() $ open_copy.drop([1, 8], inplace = True) $ corr2 = np.corrcoef(open_copy['repair_time'], open_copy['2017_homeAverage']) $ corr2
shelter_df_idx = model.transform(shelter_df) $ shelter_df_idx.show()
fig, ax = plt.subplots(1, figsize=(12,4)) $ plot_with_moving_average(ax, 'Seasonal AVG RN/PAs', RN_PA_duration, window=52)
new_page_count = df2[df2.landing_page == 'new_page'].user_id.count() $ new_page_count / df2.shape[0]
w=(Window.partitionBy('user_id').orderBy(col('score').desc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)) $ predictions_train=predictions_train.withColumn('reccomendations',F.count('user_id').over(w))
alice_top_vec = all_top_vecs[bks.alice[0]] $ hod_top_vec = all_top_vecs[bks.heart_darkness[0]] $ sum_vec2 = vec_lst_add_subtract(hod_top_vec, alice_top_vec, mode='add') $ find_most_similar(sum_vec2, all_top_vecs, title_lst, vec_in_corp='N')
df2.query('user_id == ' + str(repeated_id[0])) $
result = logit.fit() $ result.summary2()
wrong_date_format = cats_df[cats_df['date of last vet visit'].apply(lambda c: pd.to_datetime(c, errors='coerce')).isnull()] $ print(wrong_date_format) $ cats_df['date of last vet visit'] = cats_df['date of last vet visit'].apply(lambda c: pd.to_datetime(c, errors='coerce')) $ cats_df['remove'].iloc[wrong_date_format.index] = True $ del wrong_date_format
finals['type'].value_counts()
df.head()
guardian_data.head()
df.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis=1, inplace=True)
noaa_data.loc[:,'PRECIPITATION'].groupby(pd.Grouper(freq='W')).sum()
session.query(Measurement.id, func.min(Measurement.tobs)).filter(Measurement.station == 'USC00519281').all()
print(-result.fun)
walk = (np.random.randint(0, 200, size=N) - 100) * 0.25
uk, us, abs_pg = 1/np.exp(0.0506), 1/np.exp(0.0408), 1/np.exp(-0.0150) $ uk,us, abs_pg
raw = pd.read_excel( param.weather_folder + '/wind-forecast.xlsx') $ raw['TIMESTAMP_UTC'] = pd.to_datetime(raw['TIMESTAMP_UTC']) $ raw = raw.set_index('TIMESTAMP_UTC') $ raw.head()
at_menu_id_granularity = menu_dishes_about_latent_features.groupby('menu_id').sum().reset_index()
import plotly $ import plotly.figure_factory as ff $ import seaborn as sns
df2.groupby('group')['converted'].describe()
mse = mean_squared_error(y_test, grbreg.predict(X_test)) $ print("MSE: %.4f" % mse)
twitter_archive.describe()
count = df.groupby('text').count()
data[['Sales']].resample('D').mean().rolling(window=4, center=True).mean().head()
df_test = pd.read_csv("C:/Users/ajayc/Desktop/ACN/2_Spring2018/ML/Project/WSDM/DATA/sample_submission_v2.csv")
gaussian = GaussianNB() $ gaussian.fit(X_train, Y_train) $ Y_pred = gaussian.predict(X_test) $ acc_gaussian = round(gaussian.score(X_test, Y_test) * 100, 2) $ acc_gaussian
portal_pth = "../data"
print(["1" "2" "abc" "Two words."]) $ print(["1", "2", "abc", "Two words."])
df_weather.loc[:, "precipitation_inches"] = df_weather.precipitation_inches.apply(lambda x: "0.005" if x == "T" else x) $ df_weather.loc[:, "precipitation_inches"] = df_weather.loc[:, "precipitation_inches"].astype(np.float) $ np.sort(df_weather.precipitation_inches.unique())
feed=pd.read_csv("feed.csv")
recommendation = model.recommend(users = user_list, k = 50)
print(nx.info(G)) #print basic info about the newly create graph
test = import_all(data_repo + 'intervention_test.csv')
tags['Count'].agg(['count', np.mean, np.std, np.median])
df2.shape
df2['intercept'] = 1 $ df2.head() $
crimes.PRIMARY_DESCRIPTION.head()
scraped_batch6_top['Date'] = scraped_batch6_top['Date'].str.split('/')
print(member['duration_sec'].describe())
results_BallBerry, out_file2 = S.execute(run_suffix="BallBerry_hs", run_option = 'local')
station_count_stats = session.query(Measurement.station, func.min(Measurement.tobs),func.max(Measurement.tobs),\ $ func.avg(Measurement.tobs)).group_by(Measurement.station).filter(Measurement.station == 'USC00519281').all() $ station_count_stats
df_html_extract = pd.read_csv('data/df_html_extract.csv') $ df_html_extract_copy = df_html_extract.copy() $ df_html_extract_copy.head()
full_data = pd.DataFrame({'cust_id':data_cus_id,'order_date':data_dates})
rdf = arcgis.SpatialDataFrame().from_featureclass(road_features_path)
a=[0,1] $ old_page_converted = np.random.choice(a,145274,p=[0.8804,0.1196]) $ print(old_page_converted.mean())
new_page_converted = new_page_converted[:145274] $ new_page_converted.mean() - old_page_converted.mean()
data_df.iloc[535]
nba_df.columns = ['Rk', 'Pk', 'Tm','Player','College', 'Yrs','G', 'MP', 'PTS','TRB','AST','FG%', $                     '3P%', 'FT%', 'MP', 'PTS', 'TRB', 'AST', 'WS', 'WS/48', 'BPM', 'VORP'] $ nba_df.head()
cluster_label_group_1 = df.groupby("Cluster_Labels_GaussianMixture").mean()
Meter1.ModeSet('Res4W')
p_new = df2[df2['landing_page'] == 'new_page']['converted'].mean() $ print("Probability of conversion for new page is {}".format(p_new))
election_data.groupby('st')['labor_force_13'].mean() $
test_sentence = 'To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.' $ test = ['Come on and kill Kenny!', $         'Make America great again!', $         'Beer... Beeeeer... Beeeeeeeeer... WOO-HOO!']
from CHECLabPy.core.io import DL1Reader $ reader = DL1Reader("refdata/Run17473_dl1.h5") $ reader.monitor.load_entire_table()
question_2_dataframe_in_top_zips = question_2_dataframe_in_top_zips.size().to_frame(name='count')
Val_eddyFlux = Plotting('/glade/u/home/ydchoi/summaTestCases_2.x/testCases_data/validationData/ReynoldsCreek_eddyFlux.nc') $
tfidf_matrix = tfidf.fit_transform(df_videos['video_desc']) $ tfidf_matrix.toarray()
df_selparams.head()
users.to_csv('Data/users3.csv', index=False, encoding='utf-8')
import numpy as np $ npscore = np.array(scores) $ npscore.reshape(7,20).mean(1)
lda = np.linspace(130, 140, 300) $ dist = [f(a) for a in lda] $ plt.figure(figsize=(10, 5)) $ _ = plt.plot(lda, dist)
bucket_name = "i-agility-212104.appspot.com"
def slice_metrics_2(group): $     return { $             'precision' : sk_metrics.precision_score(group.Label, group.PredictedLabel), $             'recall' : sk_metrics.recall_score(group.Label, group.PredictedLabel) $            }
shows['primary_genre'] = shows['genre'].str.split(' / ').apply(lambda x: x[0]) $ shows['secondary_genre'] = shows['genre'].str.split(' / ').apply(lambda x: x[1] if len(x)>1 else np.nan)
cars['dateCreatedMod'] = (pd.to_datetime(cars['dateCreated'])) $ cars['lastSeenMod'] = (pd.to_datetime(cars['lastSeen'])) $ cars['Ad_age']=(cars['lastSeenMod']-cars['dateCreatedMod']).astype("timedelta64[D]")
freqs = [(word, X_train_feature_counts.getcol(idx).sum()) for word, idx in count_vect.vocabulary_.items()] $ print sorted(freqs, key = lambda x: -x[1])[:20]
df[ $     (df.u_state != "USA") $   & (~df.u_state.isnull()) $   & (df.t_n_urls > 0) $ ].assign(state_color=lambda x: x.u_state.map(STATE_COLORS)).groupby("state_color").t_n_urls.count()
tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]
appleInitialNegs.to_pickle('../data/appleInitial.pkl')
probs = model.predict_proba(X_live) $ probs
a = np.arange(1, 11) $ a
df_2013['bank_name'] = df_2013.bank_name.str.split(",").str[0] $
time_local = (np.array(time_utc) - time_utc[0]) $ key_press = np.ones((l,1))
if 'http' not in url.lower(): $     url='http://'+ url $ print url $ page = requests.get(url, timeout=5)
active_df.head()
sims = gensim.similarities.docsim.Similarity( 'shard', tf_idf[corpus], $                                              num_features=len(dictionary), num_best= 10) $ print(sims)
!echo ".env" > .gitignore
graffiti2['created_year'] = graffiti2['created_year'].replace(2015, 1) $ graffiti2['created_year'] = graffiti2['created_year'].replace(2014, 1) $ graffiti2['created_year'] = graffiti2['created_year'].replace(2013, 1) $ graffiti2['graffiti_count'] = graffiti2['created_year']
print(np.exp(res.params))
to_be_predicted_Day5 = 34.57075049 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
df3[['CA', 'US']] = pd.get_dummies(df3['country'])[['CA','US']] $ df3['country'].astype(str).value_counts()
unrepaired_damage_dict = {"nein": "no", "ja": "yes"} $ autos["unrepaired_damage"].replace(unrepaired_damage_dict,inplace=True) $ autos["unrepaired_damage"].fillna("not_specified", inplace=True)
response = requests.get(url)
validation_features = spark.read.csv(os.path.join(mungepath,"model_data/20180504/rf_lr_lasso_inter2_noip/validation_features/*"), header=True) $ print("Number of observations in validation :", validation_features.count())
train_df['num_photos']=train_df['photos'].apply(len) $ num_photos = train_df['num_photos'].value_counts() $ x = num_photos.index $ y = num_photos.values $ sns.barplot(x, y )
df_user_prod_quant = pd.merge(df_out,transactions,how='left',on=['UserID','ProductID']) $ df_user_quantity = df_user_prod_quant.groupby(['UserID','ProductID'])['Quantity'].sum() $ df_user_quantity.reset_index().fillna(0)
datetime_count_times = {turnstile: [[rows[i][0], $                                      rows[i+1][1] - rows[i][1], $                                      rows[i+1][0] - rows[i][0]] $                                     for i in range(len(rows) - 1)] $                         for turnstile, rows in datetime_cumulative.items()}
df_t.sort_values(by='timestamp',ascending=False).head(2)
user.where(user["friends_count"] > 200).head(8)
lista = api.search('assholes',count=100) $ len(lista)
df_survival_by_donor = df_survival[['Donor ID', 'Donation Received Date']].groupby('Donor ID') $ mean_time_diff = df_survival_by_donor.apply(lambda df: df['Donation Received Date'].diff().mean()) $
df_all.to_clipboard()
multi_col_lvl_df = df.unstack('Store') $ multi_col_lvl_df.sample(10)
grid_id = pd.DataFrame(grid_id_flat).astype('str') $ grid_id.columns = ['grid_id'] $ print(grid_id.head(), grid_id.tail())
total_treatment = (df2['group'] == 'treatment').sum() $ treatment_converted = len((df2[(df2['group'] == 'treatment') & (df2['converted'] == 1)])) $ print((treatment_converted / total_treatment))
cityID = 'f995a9bd45d4a867' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Memphis.append(tweet) 
df.loc[102]  # by single row label
cleaner_tweets = [] $ for tweet in tweets: $     cleaner_tweets.append({'id': tweet.id, 'text': tweet.text, 'created_at': tweet.created_at, 'profile': tweet.user.screen_name}) $
%timeit functools.reduce( (lambda x,y:x+' '+y), L)
from sklearn.model_selection import StratifiedKFold , cross_val_score , GridSearchCV $ skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=42 )
ax = hits_df.plot(title="GitHub search hits for {} days sans outliers".format(len(hits_df)), $                   figsize=figsize) $ ax.set_xlabel('Date') $ _ = ax.set_ylabel('# of ipynb files')
tesla['nlp_text'] = tesla.text.apply(lambda x: tokenizer.tokenize(x.lower())) $ tesla.nlp_text = tesla.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) $ tesla.nlp_text = tesla.nlp_text.apply(lambda x: ' '.join(x))
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X?start_date=2018-08-09&end_date=2018-08-09&api_key=' + key) $ r.headers['content-type'] $ r.json()
y_pred = final_xgb.predict(dval) $ y_pred
cvecdata1 =cvec.fit_transform(X_test)
archive_df_clean.head(10)
contribs = pd.read_csv("http://www.firstpythonnotebook.org/_static/contributions.csv") $ contribs.info()
geo_db.isnull().sum()
autos["price"].value_counts().sort_index(ascending=True).head(20)
X_test.columns.values
print("Data Types: \n%s" % (excel_data.dtypes))
All_tweet_data_v2.name[All_tweet_data_v2.name.str.len() < 2].value_counts()
cust, c = generate_labels('5Jp8Y1PqL2sVd9rhoeMiAP836V5xlll3y+MOj2bN8pM=', trans, $                           label_type = 'MS', churn_period = 30, return_cust = True) $ c.iloc[15:20] $ cust.iloc[:, 6:]
df = cs.loc[cs['SUMLEV'] == 50].groupby('STNAME').agg({'CTYNAME' : "count"}) $ df.head()
frame.applymap(format)
sns.distplot(temp_df[temp_df.total_companies > 100].proportion_no_psc)
y_2_pred = rnd_reg_2.predict(X_future)
r = pd.read_csv('price.csv')
train, valid, test = covtype_df.split_frame([0.7, 0.15], seed=1234) $ covtype_X = covtype_df.col_names[:-1]     #last column is Cover_Type, our desired response variable \n", $ covtype_y = covtype_df.col_names[-1]    
to_be_predicted_Day1 = 14.48 $ predicted_new = ridge.predict(to_be_predicted_Day1) $ predicted_new 
y_col = ["state"] $ X_train, X_test, y_train, y_test = train_test_split(X, $                                                     y_encode, $                                                     test_size=0.2, $                                                     random_state=42) $
x = store_items.isnull().sum().sum() $ print(x)
import statsmodels.api as sm $ convert_old = df_old['converted'].sum() $ convert_new = df_new['converted'].sum() $ n_old = df_old.shape[0] $ n_new = df_new.shape[0]
sale_prod_sort = sale_average[sale_average.Product == 'Amarilla'] $ sale_prod_sort
df_mes['travel_time'] = df_mes['travel_time'].astype('timedelta64[s]').dt.total_seconds()
df_characters = df.groupby( $     ['character_id', 'raw_character_text'])['id'].nunique().reset_index().rename( $     columns={'id': 'num_lines'})
red_4.drop(['created_utc', 'time fetched'], axis = 1, inplace = True)
import re $ a= '2010-06-28' $ m=re.search('\d+',a) $ m.group() $
with open('data.json', 'w') as f: $     json.dump(r.json(), f)
df_loudparties = df[df['Descriptor'] == 'Loud Music/Party'] $ df_loudparties.groupby(df_loudparties.index.weekday).apply(lambda x: len(x)).plot(kind='bar')
turnstiles = df.groupby(['STATION','C/A','UNIT','SCP']) $ print('There are {} unique turnstiles.'.format(len(turnstiles))) $
pmol.df[pmol.df['atom_type'] == 'O.2']
geo_db = rent_db3[['latitude','longitude', 'interest_level']].copy()
print 'Most GoldenStateWarriors-like:', tweets_pp[tweets_pp.handle == 'warriors'].sort_values('GoldenStateWarriors_Prob', ascending=False).text.values[0] $ print 'Least GoldenStateWarriors-like:', tweets_pp[tweets_pp.handle == 'warriors'].sort_values('GoldenStateWarriors_Prob', ascending=True).text.values[0]
active_station = session.query(Measurement.station, func.count(Measurement.tobs)).group_by(Measurement.station).\ $                order_by(func.count(Measurement.tobs).desc()).all() $ active_station
twitter_archive_clean.head()
df_test.head(2)
tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]
new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating)) $ new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD) $ new_user_recommendations_rating_title_and_count_RDD.take(3)
pr('Grouping by hastag.') $ tw6['numberOfTweets'] = 1 ## We make a column number of tweets that will be useful later $ gp = tw6.groupby('hashtag') $ pr('Done')
met = get_metrics(STAMP, y, y_pred, 0.5)
_ = ok.grade('q03b') $ _ = ok.backup()
df['category_name'] = [json.loads(x)['name'] for x in df['category']] $ df['creator_name'] = [json.loads(x)['name'] for x in df['creator']] $ df['blurb_len2'] = df['blurb'].str.lower().str.split() $ df['blurb_length'] = df['blurb_len2'].apply(count_list_items)
%%time $ df['scrubbed'] = df.description.apply(lambda x: scrub(x)) $ df['tokens'] = df.scrubbed.apply(lambda x: tokenize(x))
amznnews_matrix = count_vectorizer.transform(df_amznnews.headline_text) $ amznnews_pred = nb_classifier.predict(amznnews_matrix)
df_group.sort_values(['Keyword'], ascending=False)
nri = dftemp[(dftemp['Variable Name'] == "National Rainfall Index (NRI)")] $ nri = nri[(nri.Value>950) |(nri.Value<900) ] $ nri.head(10)
wb.search('cell.*%').iloc[:,:3]
states = pd.read_csv('in/state_table.csv') $ states.rename(columns={'abbreviation': 'state'}, inplace=True)
my_df["user_create"] = df_user.groupby('nweek_create')['user_id'].nunique() $ my_df["user_active"] = df_user.groupby('nweek_active')['user_id'].nunique()
df1 = pd.read_csv("mindbody_twitter_scores_monday.csv") $ df2 = pd.read_csv("mindbody_twitter_scores_tuesday.csv")
 new_page_converted = np.random.choice([0,1], n_new, [(1-p_new), p_new])
terror.loc[138]
G = nx.read_gpickle('email_prediction.txt') $ print(nx.info(G))
cc.low.describe()
len(train_data[train_data.offerType == 'Angebot'])
knn3 = KNeighborsClassifier( n_neighbors=3,  $                            weights='uniform') $ scores = cross_val_score(knn3,  X_test, y_test,  cv=5) $ np.mean(scores), np.std(scores)
flightv1_1.where(flightv1_1.trip == 1).show(2)
dataCopy = house_data.copy()
out_test['display_address'] = out_test['display_address'].map(lambda x: x.replace('\r','')) $ out_test['street_address'] = out_test['street_address'].map(lambda x: x.replace('\r',''))
pd.read_csv('data/cleaned_data.csv').shape
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['search_results'])
df.info()
df[('2016-09-01' < df['Observation Date']) & (df['Observation Date'] < '2017-08-31')]['AQI'].mean()
X_sample = pd.DataFrame(X.loc[4232].values[None,:], columns=X.columns) $ print('Predicted bin = ' + str(rf.predict(X_sample)))
probly = pd.read_csv('probly.csv') $ ttest(probly["Likely"], probly["Unlikely"])
data_set.to_csv("earthquake nov 10.csv")
df['timestamp'] = pd.to_datetime(df['timestamp']) df['timestamp'].dt.strftime('%m/%d/%Y') $ df['timestamp'].dt.strftime('%m/%d/%Y') 
plt.hist(test_residuals['255_elec_use'])
autos['registration_year'].value_counts(normalize=True).sort_values(ascending=False)
vc.head()
p_diffs = [] $ for _ in range(10000): $     old_page_converted = np.random.binomial(n_old,p_old) $     new_page_converted=np.random.binomial(n_new,p_new) $     p_diffs.append((new_page_converted/n_new) - (old_page_converted/n_old))
def datetimestring2datetime(text): $     try: $         return  pd.to_datetime(text,format='%d-%m-%Y %H:%M') $     except AttributeError: $         return text 
df.plot(x='homeWinPercentage', y='startyardsToEndzone', kind='scatter')
tweet = api.user_timeline("FoxNews", page=0) $
a=contractor_clean.groupby(['contractor_bus_name'])['contractor_number'].nunique().sort_values(ascending=True) $ ax=a.plot(kind='barh', title ="Bar Chart for Frequency of unique contractor number by contractor original name", figsize=(15, 10), legend=True, fontsize=12) $ ax.set_xlabel("Contractor Name", fontsize=12) $ ax.set_ylabel("Count of Contractor Number", fontsize=12) $ plt.show()
data_mean = health_data.mean(level='year') $ data_mean
df = pd.read_csv('twitter_archive_master.csv')
for c in text: $     if ord(c) > 127: # Skips single characters $         print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))
feat_names = X_final.columns.values
np.vstack((a, a))[0:3,:]
lr = LinearRegression() $ lr.fit(train_data, train_labels)
(autos['registration_year'] $  .between(0,1900) $  .value_counts() $  .sort_index() $ )
tweets_df[tweets_df.possibly_sensitive == True]
df_day_pear=df_day.sample(n=774000).sort_values('tripduration') $ df_night_pear=df_night.sort_values('tripduration')
model_1 = graphlab.linear_regression.create(train_data, target = 'price', $                                                   features = model_1_features, $                                                   validation_set = None)
ttDaily['DailyEntry'] = ttDaily.ENTRIES.diff()
MICROSACC.plot_mainsequence(microsaccades)+coord_cartesian(xlim=(-3,1))
cp311 = pd.concat([cp311,dfstatus],axis=1,join='inner').copy()
print('Most positive tweets:') $ for t in trump.sort_values('polarity', ascending=False).head()['text']: $     print('\n  ', t)
vectorizer = CountVectorizer() $ vectorizer.fit(articles_train) $ train_features = vectorizer.transform(articles_train)
data.iloc[1:10]
import re $ letters_only = re.sub('[^a-zA-Z]',              # the pattern to search for $                              " ",                          # the pattern to replace it with $                              example1.get_text())   # the text to search $ print(letters_only[:100])  
missing_val_df=pd.DataFrame(pd.isnull(calls_df).sum()).reset_index() $ missing_val_df=missing_val_df.rename(columns={"index":"Columns",0:"Missing Percentage"}) $ missing_val_df["Missing Percentage"]=missing_val_df["Missing Percentage"]/len(calls_df)*100 $ missing_val_df=missing_val_df.sort_values("Missing Percentage",ascending=False).reset_index(drop=True) $ missing_val_df.head(10)
nbart_allsensors =nbart_allsensors.sortby('time')
df2.drop_duplicates(subset = ['user_id'], inplace = True)
df_new['country'].unique()
df_image_tweet = pd.merge(df_tweet_clean2, df_image_clean2, on='tweet_id', how='left')
consistently_delayed = delays_by_origin[delays_by_origin['count'] > 14] $ highly_delayed = consistently_delayed.sort_values('mean', ascending=False).head(100) $ highly_delayed.head(10)
conn = sqlite3.connect("D:\kagglelearn\kaggledatasets\FPA_FOD_20170508.sqlite")
!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.119 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-GB,en-US;q=0.9,en;q=0.8" "https://storage.googleapis.com/kaggle-competitions-data/kaggle/8076/test.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1519109659&Signature=CKW8rJJNahupAjFgWS%2FVIlGs2tj0CugO9%2FNrFpMz953KGQCxweyHuwLWm2Fl%2FxQEsyIJJFQYvvfl94gk9NAr4OnMen2UVebjZ98%2BIeGSljiwYdTiUGyH9QPk9CwMKlrBouuTVKmZELo1eQX4FmiD7a1Iy1oI0jbJKWZqrCqq2%2BrkMzmtAT91CyGUUxhhrIW47mE8Ff%2Bx4%2BIta%2Fcpi3HFJ6a0bZAR6jlJynJ4MBxtUjmOPTdAoJ6VNmdo0tgnn5xO%2Bkn5e1H6vk6M0QY0qA6j1uZJOr3gjKxBZroWDatDR8liiEuGdGSCkxTJ2X3HvpHe2D7A7%2Bq%2B21SeigLc4Hma9A%3D%3D" -O "test.csv.zip" -c
html_page = browser.html $ JPL_soup = bs(html_page, "lxml")
autos = autos[autos['registration_year'].between(1886, 2016)]
common_keywords = vectorized.columns
old_page_converted = np.random.choice([1, 0], size = n_old, p=[p_old, (1-p_old)]).mean() $ old_page_converted
rounds_df.shape[0] * 0.15
tokens = word_tokenize(raw) $ text = nltk.Text(tokens) $ text.collocations() $ pp.pprint(text.collocations())
to_be_predicted_Day3 = 26.69278823 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
url = form_url(f'organizations/{org_id}/memberTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_enumeration(response)
old_page_converted = np.random.choice(a=[0, 1], $                                       p=[1-p_old, p_old], $                                       size=n_old) $ old_page_converted
twitter_df.nunique()
data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0) $ data.head()
duplicated_user = df2[df2.user_id.duplicated()] $ duplicated_user
op_add_comms.drop('body', axis=1, inplace=True)
loading_scores = pd.DataFrame(pca.components_[0], index = pivoted.index.values) $ sorted_loading_scores = loading_scores.abs().sort_values( by = [0],ascending=False) $ top_ten_genes = sorted_loading_scores[0:10].index $ loading_scores[loading_scores.index.isin(top_ten_genes)].sort_values(by=0,ascending=False)
sqlClient.logon() $ jobId = sqlClient.submit_sql(sql) $ print("SQL query submitted and running in the background. jobId = " + jobId)
for child in get_grandchild_nodes(observations_node).values(): $     summarize_node(child)
correct_win = [np.sign(y_test[i])==np.sign(lin_pred[i]) for i in range(len(y_test))] $ np.mean(correct_win)
n_old=df2[df2['group']=='control'].user_id.nunique() $ n_old
VAA = ['AGG', 'EFA', 'EEM'] $ Maindf = historical_data("SPY") $ Maindf.index = pd.to_datetime(Maindf.timestamp) #set the index as the timestamp $ Maindf = Maindf[['timestamp','close']] $ Maindf.columns = ["timestamp","SPY"] $
stats_str = json.dumps(stats, indent=2) $ print(stats_str)
data = requests.get ("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?api_key="+API_KEY ).json()
preds = lm.predict(X_new) $ preds
eia_total['type'].unique()
training.to_csv(processed_path+"train_set.csv",index=False) $ testing.to_csv(processed_path+"test_set.csv",index=False)
df.head(5)
bacteria_data.loc[3]
df2.query('landing_page=="new_page"').count()/df2.shape[0]
proc.document_length_stats
is_unpaid.plot.scatter(x='score', y='paid_status', alpha=0.3, figsize=(20, 10))
output= "SELECT Distinct user_id  from tweet limit 10 " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id'])
data.index[0] = 15
mismatch_1 = df.query('group == "treatment" and landing_page != "new_page"').count()[0] $ mismatch_2 = df.query('group != "treatment" and landing_page == "new_page"').count()[0] $ mismatch_1 + mismatch_2
dd_df['notRepairedDamage'].fillna('nRD_unknown', inplace=True)
import pandas as pd $ df=pd.DataFrame(top50[0]['trends']) $ df
assert pd.notnull(ebola_0).all().all()
temperature_sensors_df['state'][ $     temperature_sensors_df['entity_id'] != 'sensor.darksky_sensor_temperature'].hist( $     bins=50); $ plt.title("Inside temperature"); $ plt.xlabel("Temperature $^\circ$C");
dfEtiquetas = pd.DataFrame(l)[["id", "created_time", "place"]]
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2], axis="col")
train_data['gearbox_int'] = train_data['gearbox'].apply(get_integer3) $ test_data['gearbox_int'] = test_data['gearbox'].apply(get_integer3) $ del train_data['gearbox'] $ del test_data['gearbox']
actual_payments.loc[actual_payments.fk_loan==loan_test,['iso_date','actual_amount_month','in_arrears_since']]
qry_url = '{}index/feeds'.format(base_url) $ response = requests.get(qry_url) $ response = response.text $ print(response)
sp500.iat[0, 1]
with open(link.split('/')[-1], mode = 'wb') as outfile: $     outfile.write(r.content)
warm.sum()
jan_2015_groupby.head()
eia_total_monthly.head()
to_pickle('Data files/cats_merge.p',cats_merge) $ to_pickle('Data files/adopted_cats.p',adopted_cats)
df4['Date'] = pd.to_datetime(df4['Date']) $ df4.dtypes
links_df=url_df_full[url_df_full['url'].str.contains('http')] $ print(links_df.head())
import pandas as pd $ import numpy as np $ pd.set_option('display.max_columns', 150) $ pd.set_option('display.max_colwidth', -1)
titanic.deck.unique()
average_daily_sales = data[['Sales', 'Open']].resample('D').mean() $ average_daily_sales['DiffVsLastWeek'] = average_daily_sales[['Sales']].diff(periods=7) $ average_daily_sales.sort_values(by='DiffVsLastWeek').head() $ average_daily_sales[average_daily_sales.Open == 1].sort_values(by='DiffVsLastWeek').head() $
import pandas as pd $ import numpy as np $ import time $ finalGoodTargetUserItemInt=pd.read_pickle('datasets/goodNonColdTargetUserInt.pkl')
log_mod_int = sm.Logit(df_new['converted'],df_new[['intercept','US','UK','ab_page','US_ab_page','UK_ab_page']]) $ results_int = log_mod_int.fit() $ results_int.summary()
recv_npage = (df2.landing_page == 'new_page').mean() $ print("The probability that an individual received the new page is {0: .4} ".format(recv_npage))
year_labeled=2017 $ year_predict=2018 $ description_labeled = df[df.year==year_labeled]['description'] $ description_predict = df[df.year==year_predict]['description']
import re $ df.loc[:,"message"] = df.message.apply(lambda x : " ".join(re.findall('[\w]+',x))) $
len(df_proj.ProjectId.unique())
len(df.select_dtypes(include=[np.number]).columns.tolist())
df = pd.read_csv('kickstarter-projects/ks-projects-201801.csv', $                  parse_dates=['deadline', 'launched'], $                  encoding = "ISO-8859-1")
tt1_ok_indices = [val in vi_ok['VIOLATIONCODE'].values for val in tt1['VIOLCODE'].values]
train.columns
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     print(f.root.hard_link[:])
df_mes = df_mes[df_mes['tolls_amount']>=0] $ df_mes.shape[0]
df.query('landing_page == "old_page" and group == "treatment"').user_id.size #1965 $ df.query('landing_page == "new_page" and group == "control"').user_id.nunique() #1928
clinton_deck = clinton_df[clinton_df['source'] == 'TweetDeck'] $ clinton_deck.head()
df["DateTime"] = pd.to_datetime(df["DATE"]+" "+df["TIME"],format="%m/%d/%Y %H:%M:%S")
mod = sm.Logit(df_new['converted'], df_new[['UK', 'US', 'ab_page',"intercept"]]) $ results = mod.fit() $ results.summary()
tweet_archive_enhanced_clean['expanded_urls'] = "https://twitter.com/dog_rates/status/" + tweet_archive_enhanced_clean['tweet_id'].astype(str)
date = pd.to_datetime("4th of July, 2017") $ date
df2 = df2.drop('control',axis = 1) $ df2 = df2.rename(columns={'treatment': 'ab_page'}) $ df2.head()
print() $ print('Number of non-NaN values in the columns of our DataFrame:\n', store_items.count())
user.type.value_counts()
clean_rates.sample()
print("recent 10 tweeets") $ for tweet in tweets[:10]: $     print(tweet.text) $     print()
from sklearn.neighbors import KNeighborsClassifier $ knn_model = KNeighborsClassifier(n_neighbors=6) $ knn_model.fit(X_train, y_train)
apple['Date'] = pd.to_datetime(apple['Date']) $ apple.dtypes
pd_train_filtered = pd_train_filtered.reset_index(drop=True)  #we reset the index $ y_labels = pd_train_filtered['unit_sales'] $ X_train_filtered = pd_train_filtered.drop(['unit_sales'], axis = 1) $ print('Shape X :', X_train_filtered.shape) $ print('Shape y :', y_labels.shape)
def get_ord_from_str(string): $     ord_dic = {'nan':0, 'S':1, 'Q':2, 'C':3,} $     stripped_string = str(string).strip().strip('\n') $     return ord_dic.get(stripped_string, -1)
%%time $ with open('df_3.out','r') as inFile: $     fullDf=pickle.load(inFile)
p_newpage = df2.query('landing_page == "new_page"').shape[0]/df2.shape[0] $ p_newpage
crimes = pd.read_csv('./data/chicago_past_year_crimes.csv')
tweet_image_clean['tweet_id'].isin(tweet_archive_clean['tweet_id']).value_counts() $
bd.reset_index(drop=True)
html = browser.html $ soup = bs(html, 'html.parser') $ browser.find_by_css("div.carousel_container div.carousel_items a.button").first.click() $ time.sleep(3)  #allow time for page to load $
df_new_dummy = pd.get_dummies(data=df_countries, columns=['country']) $ df_latest = df_new_dummy.merge(df_regression, on='user_id') $ df_latest.head(1) $ df_latest[['user_id', 'country_CA', 'country_UK', 'country_US','converted','intercept','ab_page']]=df_latest[['user_id', 'country_CA', 'country_UK', 'country_US','converted','intercept','ab_page']].astype(int) $
merged = merged[merged.date>'2014-03-31']
print('The proportion of users converted is {}%'.format(round(df.converted.mean()*100,2)))
import statsmodels.api as sm $ logit_mod3 = sm.Logit(df4['converted'], df4[['intercept','ab_page','UK','US']]) $ results3 = logit_mod3.fit()
learn.sched.plot()
retweet_ids = archive_copy[pd.isnull(archive_copy.retweeted_status_id) == False ].tweet_id $ retweets = archive_copy[pd.isnull(archive_copy.retweeted_status_id) == False ]['tweet_id'] .index $ archive_copy.drop (retweets, inplace = True) $ archive_copy.info()
loglikA = np.reshape(np.array([fitA.llnull, fitA_Cli1.llf, fitA_Cli2.llf, fitA.llf]), (4,1)) $ loglik = np.reshape(np.array([-152.763, -139.747, -149.521, -134.178]), (4,1)) $ lldf = pd.DataFrame(loglik, index=['Const','Const+li1','Const+li2','All'], columns=['Loglikelihood'])
df.nunique().user_id
from src.pipeline import pipeline_json $ pj = pipeline_json(my_json) $ X = pj.convert_to_df(scaling=True, filtered=True) $ y = pj.output_labelarray()
prob_treatment = prob.mean()['converted']['treatment'] $ print("The probability of an individual converted, being in the 'treatment' group is - {}".format(prob_treatment))
autos["odometer_km"].value_counts().sort_index(ascending = True)
tweet_data.info()
print('Probability of an individual converting:',df2['converted'].mean())
pd.set_option('display.max_rows', 20) $ pd.set_option('display.max_seq_items', 20)
Base.prepare(engine, reflect=True) $ Base.classes.keys()
run txt2pdf.py -o"2018-06-19 2015 UNIVERSITY OF MICHIGAN HEALTH SYSTEM Sorted by discharges.pdf"  "2018-06-19 2015 UNIVERSITY OF MICHIGAN HEALTH SYSTEM Sorted by discharges.txt"
%time df_columns['closed_at'] = pd.to_datetime(df_columns['Closed Date'], format="%m/%d/%Y %H:%M:%S %p") $
1/np.exp(-0.0211) $
df_vow['Closed_Higher'] = df_vow.Open > df_vow.Close $ df_vow['Closed_Higher'] = pd.get_dummies(df_vow.Open > df_vow.Close).values
df2['user_id'].value_counts().head() $
conditions_m = messy['Conditions'] $ conditions_m.value_counts()
house_data['grade'].nunique()
df.describe()
df2['new_page_uk'] = df2['new_page'] * df2['country_uk'] $ df2['new_page_us'] = df2['new_page'] * df2['country_us'] $ df2.head()
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new.drop(columns = ['UK'], axis = 1, inplace = True) $ lm = sm.Logit(df_new['converted'], df_new[['US', 'CA', 'intercept']]) $ results = lm.fit() $ results.summary()
corrmat = gain_df.corr() $ f, ax = plt.subplots(figsize=(6, 6)) $ sn.heatmap(corrmat, vmax=.8, square=True, annot=True) $ plt.show()
data_country = [] $ for cntry in free_data.country.unique(): $     data_country.append(free_data.query("country == '" + cntry + "'")) $ type(data_country[0])   $
w.get_filtered_data(step = 1)[['DIN', 'SALT_CTD']].apply(pd.to_numeric).dropna(thresh=2)
rt_count_1 = df_rt[['keyword']].groupby(['keyword']).size().reset_index() $ rt_count_1.columns = ['keyword', 'count'] $ rt_count_1.sort_values(by = ['count'], ascending = False, inplace = True)
from gensim.models import Word2Vec $ model = Word2Vec.load("300features_40minwords_10context")
df_weather.head()
count_by_Confidence = grpConfidence['MeanFlow_cfs'].count() $ count_by_Confidence.plot(kind='pie');
df1['Description'].apply(returnName).value_counts().head(15)                                   
print(df.user_answer.nunique()) $ print(sorted(df.user_answer.unique())[:5])
motion_at_home_df = binary_sensors_df[binary_sensors_df['entity_id']=='binary_sensor.motion_at_home'] $ motion_at_home_df = motion_at_home_df[motion_at_home_df['state']==True] # Since on/off are always paired for motion, drop false $ motion_at_home_df.head()
ll ../input/
df['Descriptor'].value_counts().head()
data = data[data['Borough']!='Unspecified'] $ data.info()
df.to_csv('311_week_number.csv', encoding = 'utf-8', index = False)
posts.columns
df.query ('group == "control" and landing_page != "old_page"')
rankings_USA.query("rank <= 10")['rank_date'].count() $
df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'], $                    'salary': [70000, 80000, 120000, 90000]}) $ display('df1', 'df3', 'pd.merge(df1, df3, left_on="employee", right_on="name")')
miss_grp_sum = len(miss_grp1) + len(miss_grp2) $ print('The total number of misalinged cases is {}'.format(miss_grp_sum))
six = [] $ for cat in range(len(four)): $     four[cat]['age_cat']=cat $     six.append(four[cat]['age_cat'])
df_columns.index.month.value_counts().sort_index().plot(kind='barh') $
test_embedding=pd.DataFrame(test_embedding)
humans = TextBlob("I love humans") $ humans.sentiment
train = pd.read_excel('./Data/sentimentTrain.xlsx') $ train['splitText'] = train['SentimentText'].apply(lambda t : str(t).strip().split(' ')) $ train['trainTuple'] = train.apply(lambda row : (row['splitText'], row['Sentiment']), axis=1)
with tf.Session() as session: $     result = session.run(slice,feed_dict={my_image: input_image}) $     print(result.shape)
temp_df = proportion_and_count(active_psc_records,'nationality',len(active_psc_records[active_psc_records.kind == 'individual-person-with-significant-control'])) $ temp_df['count'].head(5).to_csv('data/viz/nationalities.csv',index=False) $ temp_df.head(5)
desc_stats.columns = ['Count', 'Mean', 'Std. Dev.', 'Min.', '25th Pct.', 'Median', '75th Pct.', 'Max'] $ desc_stats
data['float_time'].hist(bins=50)
b = np.array([[1,2,3], $               [4,5,6], $               [7,8,9]]) $ b
feats_dict = dict() $ for i, name in enumerate(dummies_df.keys()): $     feats_dict[i] = name $
model.load_weights(MODEL_WEIGHTS_FILE) $ loss, accuracy = model.evaluate([q1_data1, q2_data2], labels_, verbose=0) $ print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))
X_test = combine_X_data(test_projection, test_df.length, test_df.paragraphs, $                        test_df.num_words, test_df.spelling_errors, $                        test_df.average_sentence_length, test_df.sentence_length_variance) $ y_test = test_df.reward.values.astype(float) $ X_test,y_test
TERM2017 = INT.loc[(INT.Term == 'Fall 2017')] $ TERM2018 = INT.loc[(INT.Term == 'Fall 2018')] $ TERM2019 = INT.loc[(INT.Term == 'Fall 2019')] $
import pandas as pd $ import csv $ pd.set_option('display.max_colwidth', -1) # do not truncate comments row, need to see this for cleaning
import copy $ newFormat = copy.deepcopy(formats.defaultFormat) $ newFormat['recorddefinition']['granularity'] = '1H' $ newFormat['recorddefinition']['minrecordcount'] = newFormat['recorddefinition']['maxrecordcount'] = 24 $ pprint.pprint(newFormat['recorddefinition'])
print("PASS: ", pass_students.ATAR.std()) $ print("FAIL: ", fail_students.ATAR.std())
df_all = df_submissions.append(df_comments) $ df_all['comment'] = np.where(df_all['comment_id'].isnull(),0,1) $ df_all['submission'] = np.where(df_all['comment']==0,1,0) $ print(df_all.shape) $ df_all.columns
commits_per_day_cumulative = commits_per_day.cumsum() $ commits_per_day_cumulative.head()
autos.ad_created_month.value_counts(normalize=True) $
print(autos["odometer_km"].value_counts().sort_index()) $ print(autos["odometer_km"].unique().shape) $ print(autos["odometer_km"].describe())
grid = qgrid.show_grid(measurements_df) $ grid
url = "https://en.wikipedia.org/wiki/List_of_world_records_in_swimming" $ tables = pd.read_html(url, header = 0, encoding='utf-8') $ tables[0].head(5)
df2.query('group=="control" & converted==1').user_id.count() / df2.query('group=="control"').user_id.count()
ideas.dtypes  # Inspecting datatypes
amazon_review = pd.read_csv('amazon_cells_labelled.tsv',sep='\t') $ amazon_review
dfChile = pd.read_pickle("dfAllGPSTweetsFilterChile.p")
print ("Jaccard Similarity Score", jaccard_similarity_score(y_test, yhat)) $ print("F1-score Accuracy: ",  metrics.f1_score(y_test, yhat, average='weighted')) 
pd.DataFrame.to_csv(merged)
df2['intercept']=1 $ df2[['control','treatment',]]=pd.get_dummies(df2['group']) $
all_data_wide.loc[all_data_wide.index > pd.to_datetime(PAGECOUNTS_END, format='%Y%m%d%H'), 'pagecount_all_views'] = np.nan $ all_data_wide.loc[all_data_wide.index < pd.to_datetime(PAGEVIEWS_START, format='%Y%m%d%H'), 'pageview_all_views'] = np.nan
print(len(tweets)) $ tweets.head()
pd.datetime.now()
epsg = {'code':'4269'} $ expected_geom_type = 'Point' $ source_uri = 'https://www.sciencebase.gov/catalog/item/5af6219be4b0da30c1b5faad' $ outfile_name = 'gnis_20180601'
'we only rate dogs' in twitter_ar.text
print(rf.attrs['crs'])
pd.merge(df_a, df_b, on='mathdad_id', how='right')
access_logs_df = access_logs_parsed.toDF()
test = pandas.read_csv('test.csv')
df1 = pd.DataFrame(np.random.randn(6,3),columns=['col1','col2','col3']) $ df1
dfjoined.columns = [['created_date', 'count_complaints_day', 'dayofweek','complaint_type', 'count_type_day']]
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS $ custom_stop_words = list(ENGLISH_STOP_WORDS)
df_goog.Open.asfreq('M',method = 'ffill')
new_page_converted = np.random.choice([0,1],size=n_new, p=(cr_new_null,1-cr_new_null)) 
rankings_USA = rankings[rankings['country_full'] == "USA"].reset_index(drop=True) $
df1 = pd.DataFrame({"A":["A1", "A2"], $                     "B":["B1","B2"]},index=[1,2]) $ df2 = pd.DataFrame({"A":["A3", "A4"], $                     "B":["B3","B4"]},index=[3,4]) $ pd.concat([df1,df2], axis=1)
df2.query("landing_page=='new_page'").count()[0]/len(df2)
df.head(2)
df_sb.isDuplicated.value_counts() $ df_sb.drop(['Unnamed: 0','longitude','favorited','truncated','latitude','id','isDuplicated','replyToUID'],axis=1,inplace=True) $
long_microseconds = datetime.datetime(2015, 12, 31, 23, 59, 12, 999999).strftime("%f") $ print("long", long_microseconds) $ short_microseconds = datetime.datetime(2015, 12, 31, 23, 59, 12, 999999).strftime("%f")[:-3] $ print("short", short_microseconds)
to_fix = ['p1', 'p2', 'p3'] $ for col in to_fix: $     ip_clean[col] = ip_clean[col].str.replace('_', ' ').str.capitalize()
trump_tweets[0].text
treat_prob = df2[df2['group'] == 'treatment']['converted'].mean() $ treat_prob
count_polarity=pd.concat([count_polarity_2012,count_polarity_2013,count_polarity_2014 $                            ,count_polarity_2015,count_polarity_2016], axis=1)
df_pr.head()
data['VolRatio'] = (data['ATR'] / data['ATR5']) $ data.tail()
p_mean = np.mean([p_new, p_old]) $ print('The mean conversion rate of p_new and p_old:',p_mean)
pulledTweets_df = gu.read_pickle_obj(processed_dir+'pulledTweetsProcessedAndClassified_df')
ks_name_failed = ks_name_failed.sort_values(by = ['counts'], ascending = False) $ ks_name_failed.head(10)
autos['registration_year'].describe()
df_goog['year'] = df_goog.index.year $ df_goog['month'] = df_goog.index.month $ df_goog['weekday'] = df_goog.index.strftime('%A') $ df_goog.head()
Elec  = transition_rate(0, 0.4, default_start=2016, default_end=2025) $ Water = transition_rate(0, 0.2, default_start=2016, default_end=2025) $ pr    = transition_rate(0, 0.3, default_start=2016, default_end=2025)
csvData[csvData['street'].str.match('.*East.*')]['street']
lifetime=[] $ for t in t_open_resol: $     a=t['lifetime'] $     lifetime.append(a)
financial_crisis.loc['Tulip Mania']
df_times = pd.DataFrame(times, columns=['Time of daily highs'])
X_train_tokens = stfvect.get_feature_names() $ len(X_train_tokens)
providers_schedules = pd.read_csv('./data/ProvidersSchedulesLastest.csv')
df = pd.read_csv("../../data/msft_with_footer.csv",skipfooter=2,engine='python') $ df
def isNotBlank (myString): $     if myString and myString.strip(): $         return True $     return False $ isNotBlank(' ')
DATE = '2017-10-11' $ df_save = pd.read_hdf('walltimejobs_'+DATE+'.h5', 'df_save') $ result = get_result(df_save) $ display(result.head(5)) $ display(result.shape)
f_counts_minute_ip.show(1)
destination=table.find(text='Destination').find_next('td').text $ destination
df2[df2['group']=='treatment']['converted'].mean()
m.fit(lr, 1, metrics=[exp_rmspe])
for key, value in close.items(): $     if value == max_dif_Q5: $         print key, value
twitter_archive_with_json[twitter_archive_with_json.tweet_id == 835246439529840640][['tweet_id','rating_denominator']]
year11 = driver.find_elements_by_class_name('yr-button')[10] $ year11.click()
df_loudparties = df[df['Descriptor'] == 'Loud Music/Party'] $ df_loudparties.groupby(df_loudparties.index.weekday).apply(lambda x: len(x)).plot()
df_archive = pd.read_csv("twitter-archive-enhanced.csv",sep=",")
load_weigths_into_target_network(agent, target_network) $ sess.run([tf.assert_equal(w, w_target) for w, w_target in zip(agent.weights, target_network.weights)]); $ print("It works!")
ab_df2 = ab_df2.drop(labels=2893)
fitted = lr.fit(X_train, y_train)
results.shape
df = pd.read_csv('msft.csv', $                    names = ['open', 'high', 'low', $                            'close', 'volume', 'adjclose']) $ df.head()
n_new = len(df2[df2['landing_page'] == 'new_page']) $ print ("n_new is: {}".format(n_new))
results3.summary()
bnbx = bnb $ bnbx.shape
sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative='larger') $
date= plt.plot_date(data=tweets_df, x="created_at", y="retweet_count", fmt="go") $ plt.xticks(rotation=60) $
catalog_df.to_csv('catalog.csv', encoding='utf8')
embedding_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)
new_page_converted = np.random.choice([0,1],size=n_new,p= [1-p_new,p_new])
t.reindex([103,102,101,104,105])
date = np.array('2017-10-21', dtype=np.datetime64) $ date = date + np.arange(189)
res = vectorizer.fit_transform(answer['tweet'].values)
twitter_archive_df['name'].value_counts(ascending = False)[:20]
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept','action_new_page','UK','US','CA']]) $ res = log_mod.fit() $ res.summary2()
most_retweeted = grouped.iloc[:5,:] $ most_retweeted
model2 = LogisticRegression() $ model2.fit(x_train, y_train)
!!from time import sleep $ for i in range(0, 100): $     print(i) $     sleep(0.1)
df_model = df_model.drop('C1') $ df_model.describe()
normal,malicous = plots.proto_proportions() $ print(normal) $ print(malicous)
print('The number of rows in the dataset is {}'.format(df.shape[0]))
tweet_archive_clean.to_csv('twitter_archive_master.csv')
df[df.converted==1].shape[0]/unique_users
pd.concat([city_loc.set_index("city"), city_pop.set_index("city")], axis=1)
jobs.loc[(jobs.FAIRSHARE == 10) & (jobs.ReqCPUS == 1) & (jobs.GPU == 1) & (jobs.Group == 'h_fabbrilab')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
print(tweets.loc[300074]["content"]) $ tweets.loc[300074]
k_means.fit(X)
autos['price'].value_counts().sort_index()
predict_column = set(['tip_pct']) $ feature_columns = set(taxi_sample.columns) - set(['tip_amount', 'tip_pct']) $ var_num = set(['passenger_count', 'trip_distance', 'temp', 'total_amount']) $ var_cat = set(['payment_type', 'pickup_boroname', 'dropoff_boroname', 'color']) $ var_cat_num = set(['pickup_dow', 'pickup_hour'])
pipeline = Pipeline([ $        OneHotVectorizer(columns=columns[catefeatures_index].values.tolist()), $        LightGbmBinaryClassifier(feature=columns[features_index].values.tolist(), num_boost_round=200) $ ]) $ pipeline.fit(ds_train, 'Label')
T.set_subtensor(y_true[])
joined = joined.merge(tr_roll, 'left', ['date', 'store_nbr'])
unique_domains.query('num_authors >= 10').sort_values('num_authors', ascending=False)[['domain', 'num_authors']][0:50]
rob = df.loc['lossyrob'].resample('D').size() # we can drop the users level with access by .loc and a username $
which_files=[10,12,14] $ for k in range(len(which_files)): $     print('run txt2pdf.py -o ' + '"' + list_of_files[which_files[k]][:-4] + '.pdf' + '"  "' +\ $          list_of_files[which_files[k]] + '"') $
df.query("(group == 'treatment' and landing_page == 'old_page') or (group == 'control' and landing_page == 'new_page')" ).shape[0]
trump_bow = vect_bow.fit_transform(df_train) $ trump_cleaned_bow = vect_cleaned_bow.fit_transform(df_cleaned_train) $ trump_stemmed_bow = vect_stemmed_bow.fit_transform(df_stemmed_train)
df.sentiment.value_counts()
pd.set_option('display.max_columns', 100)
plt.hist(p_diffs) $ plt.axvline(x=obs_diff, color='red');
conv_cont_prob = df2[df2['group']=='control']['converted'].mean() * 100 $ output = round(conv_cont_prob, 2) $ print("The probability of the control group individual converting regardless of the page they receive is: {}%".format(output))
t = ('RHAT',) #tuple with just one element $ c.execute('SELECT * FROM stocks WHERE symbol=?', t) $ print(c.fetchone())
pd.set_option("display.max_rows",80) $ pd.get_option("display.max_rows")
datetime.date(2007, 12, 5) + datetime.timedelta(days=-30)
from sklearn.model_selection import KFold $ cv = KFold(n_splits=250, random_state=None, shuffle=True) $ estimator = Ridge(alpha=32000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
df.location_id.shift(-1).head()
autos[['date_crawled','ad_created','last_seen']].head(5)
xml_in_sample['authorId'].nunique()
clean_liverp.to_csv('clean_liv.csv')
LabelsReviewedByDate = wrangled_issues_df.groupby(['Status', 'OriginationPhase']).created_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue', 'green', 'red', 'yellow'], grid=False) $
df_new.country.unique()
tweets['created'] = pd.to_datetime(tweets['created'])
df2=df_a $ df2=df2.append(df_b, ignore_index=True) $ df2.info()
test_df.head()
run txt2pdf.py -o"2018-06-19 2015 CLEVELAND CLINIC Sorted by discharges.pdf"  "2018-06-19 2015 CLEVELAND CLINIC Sorted by discharges.txt"
import numpy as np $ import statsmodels.formula.api as smf $ mod = smf.ols("cellphone ~ np.log(gdp)", dat).fit() $ print(mod.summary())
df2[(df2.landing_page == 'new_page')].shape[0] / df2.shape[0]
print cust_data.drop_duplicates(keep='last').head(3) $ print cust_data.drop_duplicates(keep=False).head(3)
print(model.similarity('human', 'party')) $ print(model.similarity('tree', 'murder'))
ripple_github_issues_url = blockchain_projects_github_issues_urls[4] $ ripple_github_issues_df  = pd.read_json(get_http_json_response_contents(ripple_github_issues_url))
df2=pd.read_csv('edited_ab_data.csv')
print(pd.value_counts(train_df['device'])[:20])
flight_delays["ARR_DATETIME"] = pd.DatetimeIndex(flight_delays.ARR_DATETIME) $ flight_delays.set_index("ARR_DATETIME", inplace=True) $ flight_delays = flight_delays.drop([pd.Timestamp('2014-03-09 02:00:00'), pd.Timestamp('2014-11-02 01:00:00'), pd.Timestamp('2015-03-08 02:00:00'), pd.Timestamp('2015-11-01 01:00:00'), pd.Timestamp('2016-03-13 02:00:00'), pd.Timestamp('2016-11-06 01:00:00'), pd.Timestamp('2017-03-12 02:00:00')]) $ flight_delays.index = flight_delays.index.tz_localize('America/New_York')
X = df_train[features_to_use] $ y = df_train["interest_level"] $ X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)
f_close_clicks_app_train.show(3)
infinity = InfinityWars.loc[InfinityWars['lang'] == 'en']
file.to_dataframe().head()
import sklearn $ import matplotlib $ import matplotlib.pyplot as plt $
first_status = re_json.get('statuses', [{}])[0] $ print first_status['text']
predictions = [] $ for array in np.array_split(test_set[0], 100): $     result = linear_predictor.predict(array) $     predictions += [r['predicted_label'] for r in result['predictions']] $ predictions = np.array(predictions)
import statsmodels.api as sm $ from scipy import stats $ from scipy.stats import norm $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
conv_rate = df2['converted'].mean() $ print('The probability of a conversion is {}'.format(round(conv_rate, 4)))
df.subtract(df['R'], axis=0)
menu_dishes_about_latent_features = pd.DataFrame(menu_dishes_about_latent_features_with_menu_ids, columns=menu_dishes_about_latent_features_column_names)
inconvertible = dep.show_overview() $ inconvertible
pd.date_range('2017-01', '2017-12', freq='M')  # This gives us 12 dates, one for each month, on the last day of each month
print('Data type of each column: ') $ data.dtypes
df_arch_clean['source'].value_counts() $
first_result.find('a').text[1:-1]
100 * ab_data[ab_data.converted==1].shape[0] / ab_data.shape[0]
train = train().merge(pd.DataFrame(x.toarray(), $                          columns=["subject_cat_"+nm for nm in ctv.get_feature_names()]) $             ,left_index=True, $             right_index=True) $ del ctv, x ,project_cats, train['project_subject_categories']
xt = pd.to_datetime(df.created_at) $ df.index = xt $ del df['created_at'] $ df.index.names = ['Date']
unique_desc_loc=class_merged_hol['description_local_hol'].unique() $ print (unique_desc_loc)
df['Complaint Type'].groupby(by=df.index.month).count()
url=("/Users/maggiewest/Projects/cpi3.xlsx") $ cpi = pd.read_excel(url)
fdist.plot(100, cumulative=True)
pd.Period('2012', freq='A')
encoder_model_inference.save('encoder_model_inference.h5')
test = talks_train.ix[23,'speaker_ids']
temp_df2['timestamp'] = pd.to_datetime(temp_df2['timestamp'],infer_datetime_format=True)
LONG_MAX = -86.1010 $ LONG_MIN = -118.2360 $ LAT_MAX = 33.5791 $ LAT_MIN = 12.37
requests.delete(BASE + 'networks/' + str(sample_network_suids[0]))
date_null_agg=pd.DataFrame(nulls['date'].value_counts()) $ date_null_agg.columns=['frequency'] $ date_null_agg['date']=date_null_agg.index $ pd.DataFrame.head(date_null_agg)
autos['odometer_km'].value_counts()
total.index = DatetimeIndex(total['created_at'])
df = pd.merge(train.to_frame(), hsi["rise_nextday"].to_frame(), left_index=True, right_index=True)
import datetime $ data['Created Date'] = data['Created Date'].apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%y %H:%M'))
df2 = pd.read_csv('ab_edited.csv', sep=',')
from sklearn.linear_model import Lasso $ lassoreg = Lasso(alpha=0.001, normalize=True) $ lassoreg.fit(X_train, y_train) $ print(lassoreg.coef_)
df_495 = pd.read_sql(sql_495,conn_laurel) $ df_495.groupby(['accepted','paid','preview_clicked','preview_watched','preview_finished'])['applicant_id'].count().unstack().fillna(0)
motion_df.tail()
from bs4 import BeautifulSoup $ soup=BeautifulSoup(r.text,"lxml")
new_DUL_file = folder + "\Duluth-all.txt" $ new_DUL_file
iowa['state_bottle_cost'] = iowa['state_bottle_cost'].str.replace('$', '').astype('float64') $ iowa['state_bottle_retail'] = iowa['state_bottle_retail'].str.replace('$', '').astype('float64') $ iowa['sale_dollars'] = iowa['sale_dollars'].str.replace('$', '').astype('float64')
df['user_id'].nunique() $
df1.sort_values(by=['date'], ascending=True, inplace=True)
df.resample('H').count().head(10)
data_vi_week = data_vi.groupby('Weekday').sum()
sheets_with_bad_column_names = {sheet_name: sheet for sheet_name, sheet in sheets.items() $                                 if any(len(column.split()) > 1 for column in sheet.columns)}
class Spider(Bug): $     def __init__(self): $         super().__init__(legs=8, name='spider') $ Spider()
grp = dta.groupby(dta.results.str.contains("Pass")) $ grp.groups.keys()
frame3.T
from nltk.stem import porter $ from nltk.stem import LancasterStemmer $ import re
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new]) $     old_page_converted = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old]) $     p_diffs.append(new_page_converted.mean() - old_page_converted.mean())
usage_400hz_filter_pd.to_csv("400HZ_gefilterd_Q4_2017.csv")
status_keys = list(status._json.keys()) $ len(status_keys)
y_preds = xgb_grid.best_estimator_.predict(X_test) $ xgb_scores = show_model_metrics('XGBoost', xgb_grid, y_test, y_preds)
scr_churned_df.to_csv('churned_paid_518.csv')
print(sorted(df_mes['RatecodeID'].unique()))
print requests.__package__+':'+requests.__version__ $ print etree.__package__+':'+etree.VERSION $ print pd.__package__+':'+pd.__version__ $ print matplotlib.__package__+':'+matplotlib.__version__
non_na_df = df.dropna()
A = np.arange(3).reshape(3,1) $ B = np.arange(3).reshape(1,3) $ it = np.nditer([A,B,None]) $ for x,y,z in it: z[...] = x + y $ print(it.operands[2])
sandwich_train,sandwich_test = train_test_split(cat_sandwich,0.9)
np.mean(yhat==y_test)
datetime_df = pd.DataFrame([now], columns=['Time'])
last_date = dt.date(2017, 8, 23) $ last_date
bnb.groupby('affiliate_provider').count()
wikiMarvelSoup = bs4.BeautifulSoup(wikiMarvelRequest.text, 'html.parser') $ print(wikiMarvelSoup.text[:200])
Josh = miner.mine_user_tweets(user="JNkappers") $ Matt = miner.mine_user_tweets(user="mattjpfmcdonald")
unique_users = df['user_id'].nunique() $ unique_users
df.filter(df.column_name>10)
df_filter = df.copy() $ df_filter = df_filter[df_filter['currency'] == 'USD'] $ features = ['id','goal','status','Categories','City','State','launched_atYM','Length_of_kick','Days_spent_making_campign','City_Pop','staff_pick'] $ df_select = df_filter.copy()[features] $ df_select.head()
from scipy.stats import norm $ norm.cdf(z_score) $
X_train_pca = pca.fit_transform(X_train_sc) $ X_test_pca = pca.transform(X_test_sc)
log_with_day.join(gold_customers,'ipAddress','inner').groupBy('ipAddress', 'dayOfWeek').count().sort(log_with_day.ipAddress, log_with_day.dayOfWeek).show(30)
from pandas.tseries.holiday import USFederalHolidayCalendar $ from pandas.tseries.offsets import CustomBusinessDay
S_distributedTopmodel.decision_obj.simulStart.value, S_distributedTopmodel.decision_obj.simulFinsh.value
perf_train['Default'].astype(int).plot.hist();
my_stop_words = frozenset(list(term_freq_df.sort_values(by='total', ascending=False).iloc[:10].index))
Results_ZeroFill = Results_ZeroFill[['ID', 'Approved']] $ Results_ZeroFill.head()
from subprocess import call $ call(['python', '-m', 'nbconvert', 'Analyze_ab_test_results_notebook.ipynb'])
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head(8)
new_page_converted = np.random.choice([0,1], size = n_new, p=[1-p_new, p_new]) $ new_page_converted.mean()
tweet_th['text'].apply(lambda x: "I'm at" in x).value_counts()
df2.drop([1899], inplace= True) $ df2.info()
df = pd.read_csv(io.StringIO(response_clean), $                  skiprows=rowsToSkip,     #Skip metadta and data spec lines $                  delimiter='\t',          #Set to tab delimited $                  dtype={'site_no':'str'}) #Set site_no to a string datatype
df2 = df.drop(index = control_new_page.index) $ df2 = df2.drop(index = treatment_old_page.index)
red_4['num_comments'].max()
gdf.plot();
df = train $ df.head()
sns.distplot(approved.loan_amnt[:400000], label='approved') $ sns.distplot(rejected.loan_amnt[:400000], label='rejected') $ plt.legend();
ibm_hr_cat = ibm_hr_no_numerical.select(categorical_no_target) $ ibm_hr_cat.show(3)
from nltk.corpus import stopwords
33*0.07*0.0054/(0.1*0.5)/3.54*4*24
so_hashlist = pd.read_csv('email_hash_list_so_2012.csv', header=None)
hours = df4['Date'].dt.hour $ hours $
import matplotlib.pyplot as plt $ import numpy as np $ y = df.price.values $ plt.plot(np.sort(y),'.') $ plt.show()
plt.plot(newdf['20131':'201312'])
df2['intercept']=1 $ df2[['control', 'ab_page']]=pd.get_dummies(df2['group']) $ df2.drop(labels=['control'], axis=1, inplace=True) $ df2.head(10)
 print(deep_triplet_model.summary())
writer = pd.ExcelWriter("../visualizations/uber_hour_of_day.xlsx")
plt.subplots(figsize=(6, 4)) $ sn.barplot(train_session_v2['isNDF'],train_session_v2['similar_listings'])
data.head()
df["statuses_count"].head()
words_only_scrape_freq = FreqDist(words_only_scrape) $ print('The 20 most frequent terms (terms only): ', words_only_scrape_freq.most_common(20))
new_samp = df2.sample(df2.shape[0], replace=True) $ new_page_converted = new_samp.query('landing_page == "new_page"').converted.mean() $
str(type(x[0].get('attr-mov')))
df.dtypes
upper_region = (p_diffs > full_diff).mean() $ p_value = upper_region $ p_value
df_image_clean[df_image_clean['img_num'] == 2].sample().image_url
start_idx = pd.datetime.strptime(start, '%Y-%m-%d').date() $ end_idx = pd.datetime.strptime(end, '%Y-%m-%d').date()
api = tweepy.API(auth, wait_on_rate_limit= True ,wait_on_rate_limit_notify= True)
df.head()
print(data_df.shape) $ print(individuals_df.shape) $ print(libraries_df.shape)
rnd_clf2 = RandomForestClassifier(random_state=42) $ t0 = time.time() $ rnd_clf2.fit(X_train_reduced, y_train) $ t1 = time.time()
df_all_wells_basic.astype(bool).sum(axis=0)
helper.clean_folder
Results_rf10.to_csv('soln_rf10.csv', index=False)
predictions_copy.info()
weather.TMAX.mean()
tweet_ids=[x.id for x in results] $ orig_tweet_ids=set(tweet_ids) $ len(tweet_ids) == len(orig_tweet_ids) $ len(tweet_ids)
holdout.head(1)
dummy_var_df.shape
import numpy as np $ import pandas as pd $ from numpy import genfromtxt $ filename = 'data/allnames.txt' $ df=pd.read_csv(filename, sep=',', names = ["Name", "Gender", "Count"])
suspects_with_25_2['day'].value_counts().sort_index()
p_percprofit = 1 - stats.norm.cdf(0.005,loc = monthly_portfolio_average,scale = np.sqrt(monthly_portfolio_var)) $ p_percprofit
joined_samp.head(2)
df = pd.DataFrame([]) $ for i in range(0,1000000): $     x = next(meta) #next item in the generator $     df2 = pd.DataFrame.from_dict(x,orient='index').transpose() $     df = df.append(df2,ignore_index=True)
bnbAx.gender.value_counts()
lr_saga_predicted = lr_model_saga.predict(X_test)
train_dum_clean.shape
page_logit = sm.Logit(df2['converted'],df2[['intercept','ab_page','UK','US']]) $ result = page_logit.fit() $ result.summary()
tags.Tags.value_counts()
lm = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = lm.fit()
r[['optimized_weight', 'weight', 'capital']].head(100)
 def clean_tweet(tweet): $         return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split()) $
print(cc.volume.describe())
pd.DataFrame( $     rng.randn(6, 3), $     index = pd.date_range('20180101', periods = 6), $     columns = ['C', 'B', 'A'] $ )
df2.query('group == "treatment"').converted.sum() / df2.query('group == "treatment"').shape[0]
from tensorflow.examples.tutorials.mnist import input_data $ mnist = input_data.read_data_sets("/tmp/data/")
df_TempIrregular.describe()
df_mes[(df_mes['extra']!=0)&(df_mes['extra']!=0.5)&(df_mes['extra']!=1)&(df_mes['extra']!=1.5)].extra.unique()
so.shape
html_table.replace('\n', '')
pres_date_df.set_index('start_time', inplace=True, drop=False) # could have dropped, but in this case kept it $ pres_date_df.head(20)
df_train = df_train.drop_duplicates(subset='features') $ display(df_train.describe())
country_df = pd.read_csv('countries.csv') $ country_df.head() $
twitter_df_clean.drop(twitter_df_clean[twitter_df_clean.in_reply_to_status_id.notnull()].index, inplace=True)
new_page_converted.mean()-old_page_converted.mean()
new_p_c.mean(0) - old_p_c.mean()
sales.dtypes
tree_features_df[tree_features_df['p_hash'] == manager.image_df.loc[1,'p_hash']] #Yes, it's in both, with the same name
scores[scores.IMDB == max_IMDB]
data = raw_data[['goal', 'country', 'currency', 'backers_count', $                  'deadline', 'launched_at']] $ data.head()
sns_plot = sns.lmplot(x='score',y='retweet_count',data=rating_and_retweet, fit_reg=False,scatter_kws={'alpha':0.05}) $ sns_plot.savefig("score_vs_retweet.jpg")
posts_groupby.mean().num_comments.plot(kind='barh', figsize=[8,8])
results = [] $ for tweet in tweepy.Cursor(api.search, q="'insomnia', 'can't sleep'").items(100): $     results.append(tweet) $ print( len(results))
def assistant(x): $     if 'Assistant' in x: $         return 1 $     return 0 $ df_more['Assistant'] = df_more['Title'].apply(assistant)
closeSeriesR.head()
print('Scores') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.score),np.median(df.score))) $ print('Number of coments') $ print("Mean is {:0.2f} and the Median is {:0.2f}".format(np.mean(df.comms_num),np.median(df.comms_num))) $
merged.to_csv(r'X:\data_report\01.Result\jobsid\result.csv', encoding='utf-8', index=False)
nba_df.info()
%run -i 'label_image/label_image.py' --graph='/tmp/output_graph.pb' --labels='/tmp/output_labels.txt' --input_layer='Mul' --output_layer='final_result' --input_mean=128 --input_std=128 --image='test/Tiger_Woods.jpg'
dfFull['MSSubClassNorm'] = dfFull.MSSubClass/dfFull.MSSubClass.max()
m.fit(lr, 5, metrics=[exp_rmspe], cycle_len=1)
KKK = str(time.strftime("%m-%d-%y")) + "-Output.csv" $ df.to_csv("output/" + KKK, encoding="utf-8")
training_active_listing=Binarizer(10000) $ training_active_listing.fit(X_train['Active Listing Count '].values.reshape(-1,1)) $ training_active_listing_dummy=training_active_listing.transform( $     X_train['Active Listing Count '].values.reshape(-1,1))
df_predictions_clean['p1_dog'].unique()
archive_copy = archive.copy() $ predictions_copy = predictions.copy() $ tweet_data_copy = tweet_data.copy()
df.loc[df.index[df.handle == 'Donald J. Trump'], 'label'] = 1
stores.drop("GrandTotalSales",axis = 1,inplace=True) $
data.columns
conf_no_gain = x.loc[x['pred'] < 0.25] $ conf_no_gain = pd.DataFrame(conf_no_gain, columns=['pred','pred_std', 'Gain +1d']) $ len(conf_no_gain.loc[conf_no_gain['Gain +1d'] == False])/len(conf_no_gain)
top3station = target_by_station.STATION.tolist()[0:3]
df.to_csv('movie_data.csv', index=False, encoding='utf-8')
tc_final = tc2.rename(columns={'ISBN RegEx':'ISBN'}) $ tc_final.to_csv(folder + "\\" + 'TC-all.txt', sep="\t", index=False)
p_diffs = np.array(p_diffs) $ null_vals = np.random.normal(0, p_diffs.std(), p_diffs.size) $ plt.hist(null_vals);
all_data_long['access'].value_counts(dropna=False)
d = datetime.strptime(c, '%m/%d/%Y%H:%M:%S')
all_within_3A = pmol.df[pmol.df['distances'] <= 3.0] $ all_within_3A.tail()
df = pd.read_sql('SELECT actor_id, first_name, last_name FROM actor WHERE last_name ilike \'%GEN%\'', con=conn) $ df
df.filter(like="FLG_", axis=1)
p_new_diff = (new_page_converted.sum() / len(new_page_converted)) $ p_new_diff
display(train_r_order.sort_values("Total Orders", ascending=False).head()) $ display(test_r_order.sort_values("Total Orders", ascending=False).head())
statuses = [] $ def status_class(css_class): $     return css_class in ['status-can','status-conc','status-onair','status-new'] $ for status in soup.find_all("span",class_=status_class): $     statuses.append(status.string)
print(today.strftime("???"))
ts.tshift(3)
ab_diffs = df2[df2['group'] == 'treatment']['converted'].mean() -  df2[df2['group'] == 'control']['converted'].mean() $ print(ab_diffs)
print list(label_encoder.inverse_transform([0,1])) $ model.predict_proba(np.array([0,0,1,0,0,0,0,0,0,5,0])) 
All_tweet_data_v2.Stage.value_counts()
conn = sqlite3.connect('twitter_testing.sqlite') $ cur = conn.cursor() $
parsed_sierra_df.head()
sns.jointplot(x='number-of-figures', y='prosecution_period', data=utility_patents_subset_df, kind="kde", color="red") $ plt.show()
print("Information of the duplicate entry.") $ df2[df2.duplicated('user_id', keep=False)]
df2[['ab_page', 'drop_it']] = pd.get_dummies(df2['landing_page']) $ df2.head()
def get_url_parts(url): $     parsed_url = urlparse(url) $     ordered_values = tuple(value for key,value in parsed_url._asdict().items() if key != "params") $     return ordered_values
len(train_data[train_data.vehicleType == 'kleinwagen'])
bmp_series = pandas.Series(carprice) $ bmm_series = pandas.Series(carmileage) $ df_price_mil = pandas.DataFrame(bmp_series, columns=['mean_price']) $ df_price_mil['mean_mileage']=pandas.Series(carmileage) $ print(df_price_mil)
import statsmodels.api as sm $ logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']])
counter_trump = Counter() $ mrtrumpdf.tags.apply(lambda s: counter_trump.update(s))
StockData.set_index(['Date'], inplace=True)
cols = ['RequestType', 'RequestSource', 'Address', 'Latitude' , 'Longitude', 'CD', 'Created Date', 'Created Time', 'Week Number'] $ data = pd.read_csv("311_week_number.csv", usecols = cols, low_memory = False) $ data
mu = ret_aapl.mean().AAPL $ sigma = ret_aapl.std().AAPL $ r = 0.0160/252 $ mu, sigma, r
store_info.info()
convert=df2.query('converted=="1"').user_id.nunique()/df["user_id"].nunique() $ convert
lda_corpus = model[corpus] $ results = [] $ for i in lda_corpus: $     results.append(i) $ results[:1]
plt.rcParams['axes.unicode_minus'] = False $ dta_690.plot(figsize=(15,5)) $ plt.show()
dfHashtags.index = pd.DatetimeIndex(dfHashtags["date"])
df.isnull().sum()
tmp.created_at_in_seconds
a400hz.head()
rt = pd.merge(ranking_table, games_played, on=['season', 'PLAYER_ID', 'PLAYER_NAME']) $ rt.rename(columns={"GAME_ID":"games_played"}, inplace=True)
for df in (joined, joined_test): $   df['CompetitionOpenSinceMonth']= df['CompetitionOpenSinceMonth'].fillna(1).astype(np.int32) $   df['CompetitionOpenSinceYear']= df['CompetitionOpenSinceYear'].fillna(1900).astype(np.int32) $   df['Promo2SinceWeek']=df['Promo2SinceWeek'].fillna(1).astype(np.int32) $   df['Promo2SinceYear']=df['Promo2SinceYear'].fillna(2009).astype(np.int32)
twitter_archive_clean.head()
date.strftime("%A")
irradiance_clear_df = pd.read_csv(irradiance_clear_file_path, index_col='Date', parse_dates=True, encoding='latin1') $ irradiance_cloudy_df = pd.read_csv(irradiance_cloudy_file_path, index_col='Date', parse_dates=True, encoding='latin1')
if os.path.isfile(pickle_full): $     print("You've already pickled!") $ else: $     sorted_stays.to_pickle(pickle_full)
zip_1_sns =  s_n_s_df[s_n_s_df.Zip==70117] $ zip_2_sns  = s_n_s_df[s_n_s_df.Zip==70119] $ zip_1_sns = zip_1_sns.groupby(zip_1_sns.Date).size().reset_index() $ zip_2_sns = zip_2_sns.groupby(zip_2_sns.Date).size().reset_index()
np.dtype({'names':('name', 'age', 'weight'), $           'formats':('U10', 'i4', 'f8')})
final_df = en_df.dropna() $ print('Final data set has {} entries'.format(len(final_df))) $ final_df.head()
df4.isnull().sum()
import statsmodels.api as sm $ Log_model = sm.Logit(df2['converted'], df2[['intercept','treatment']])
intervention_train.dtypes
prob_convert = df2[df2["converted"] == 1].count() / (df2[df2["converted"] == 0].count() + df2[df2["converted"] == 1].count()) $ prob_convert = prob_convert[0] $ prob_convert
df.source[:5]
joined['oil_30_day_change']=joined['oil_30_day_change'].astype(np.float32) $ joined['transactions']=joined['transactions'].astype(np.float32)
gDate_vEnergy = gDateEnergy_content.count().unstack() $ gDate_vEnergy = gDate_vEnergy.fillna(0)
talks.text.shape
df = pd.DataFrame(recentd, columns=['prcp']) $ df.head(20)
establecimientos_educativos.head(5)
print("modelType: " + saved_model.meta.prop("modelType")) $ print("runtime: " + saved_model.meta.prop("runtime")) $ print("creationTime: " + str(saved_model.meta.prop("creationTime"))) $ print("modelVersionHref: " + saved_model.meta.prop("modelVersionHref"))
df_treatment = df2[(df2['group'] == 'treatment')] # create new DF with only treatment group $ df_treatment['converted'].mean() # for the treatment group, probablility of conversion is 0.11880806551510564
list(trump_tweets[0].__dict__.keys())
df4.query('four < 3')
df["ARRIVIAL_DELAYED"] =  df["ARR_DELAY"].apply(lambda x: "YES" if x > 8 else "NO")
abc = abc.reset_index(level=[0,1]) $ abc.columns
datetime.now().strftime('%A')
print 'Number of booths which are analysed:', df_t1.Task.unique().size
autos = pd.read_csv("autos.csv",encoding="Latin-1")
rfe.fit(x_train,y_train) $ rfe.score(x_test,y_test)
old_page_sim = (np.random.choice([1, 0], size=n_old, p=[p_mean/100, (1-p_mean/100)])).mean() $ output2 = round(old_page_sim, 4) * 100 $ print(output2,'%')
print(client.version)
print("Percentage of positive tweets: {}%".format(len(pos_tweets2)*100/len(data2['Tweets']))) $ print("Percentage of neutral tweets: {}%".format(len(neu_tweets2)*100/len(data2['Tweets']))) $ print("Percentage de negative tweets: {}%".format(len(neg_tweets2)*100/len(data2['Tweets'])))
html = browser.html $ soup = bs(html, 'html.parser') $ current_weather_info = soup.find_all('p', class_="TweetTextSize TweetTextSize--normal js-tweet-text tweet-text") $ current_weather_info
io2.shape
train.head()
len(finnal_data["event"].unique())
tweet_full_df['img_num']=tweet_full_df['img_num'].astype(int)
All_tweet_data_v2=All_tweet_data_v2[All_tweet_data_v2.rating_numerator<=30]
speakers = pd.read_json('speakers.json')
for i in c.find(): $     print i
intervention_history.dtypes
print(odometer_high['price'].mean()) $ print(odometer_mid['price'].mean()) $ print(odometer_low['price'].mean())
testheadlines = test["text"] $ advancedtest = advancedvectorizer.transform(testheadlines) $ advpredictions = advancedmodel.predict(advancedtest)
grouped_dpt.agg(np.sum) # note: we can also just write agg instead of aggregate
pr('Extracting hashtags... (2 min)') $ tw['hashtag'] = tw.text.apply(lambda x: extract_hashtags(str(x))) # Getting list of hashtag into new column $ twh = tw.ix[tw.hashtag.apply(lambda x: len(x) != 0)] # droping the rows (tweets) that contains no hashtags. $ pr('We have extracted {} rows with hashtags out of the {} rows of our initial dataframe.'.format(strNb(len(twh)),strNb(len(tw))))
destination_frame_name = 'loan_modeling_table.hex' $ loan_stats = h2o.import_file(path="../data/LoanStats3a.csv", destination_frame=destination_frame_name)
X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]) $ X[0:5]
summer = ghana.ix[datetime(2014,6,1) : datetime(2014,8,31)]
df.head(1)
scores[scores.RottenTomatoes == scores.RottenTomatoes.min()]
subwaydf['datetime'] = subwaydf[['DATE','TIME']].apply(lambda x: ' '.join(x), axis=1) $ subwaydf['datetime'] = pd.to_datetime(subwaydf['datetime'])
qtrclosePrice.head()
conn = pg8000.connect(user = 'dot_student', database='training', port=5432, host='training.c1erymiua9dx.us-east-1.rds.amazonaws.com', password='qgis')
msft_cum_ret['1986-03'].mean()
out_train['display_address'] = out_train['display_address'].map(lambda x: x.replace('\r','')) $ out_train['street_address'] = out_train['street_address'].map(lambda x: x.replace('\r',''))
to_be_predicted_Day4 = 21.37347759 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
control_new_pageUU = df2.loc[(df2['group'] == 'control')] $ len(control_new_pageUU[(control_new_pageUU['converted']==1)] )/ control_new_pageUU.shape[0]
smith.head(4)
url = form_url(f'actionTypes/{baseball_swing_action_type_id}/metricTypes', orderBy='name asc') $ response = requests.get(url, headers=headers) $ print_status(response)
source = pd.crosstab(tweet_table.source_cleaned_2,tweet_table.lang) $ source
row = df3.query('group == "treatment"').index $ df3.set_value(index=df3.index, col='intercept', value=1) $ df3.set_value(index=row, col='ab_page', value=1) $ df3[['intercept','ab_page']] = df3[['intercept','ab_page']].astype(int)
obj = pd.Series(range(5), index=['a', 'a', 'b', 'b', 'c'])
pokemon.drop(['Generation'],inplace=True,axis=1) $ pokemon.rename(columns={'#':'id'},inplace=True) $ pokemon['Type 2'] = pokemon['Type 2'].fillna('None') $ pokemon.info()
np.save('Participant_3_label.npy', labels_np)
for x,y in zip(X_train_dtm.columns, mnnb.feature_log_prob_): $     print x,y
joined = pats_chiefs_nov8_plays.join(pats_chiefs_nov8_tweets, how='outer', rsuffix='tweets')
df = pd.DataFrame(np.random.randn(periods,4), index=dates, columns=list('ABCD')) $ df['A'].head(5)
%%time $ binary_sensors_df['robin'] = binary_sensors_df['last_changed'].apply( $     lambda x: get_device_state(parsedDF, 'device_tracker.robins_iphone', x))
times = bird_data.timestamp[bird_data.bird_name == "Eric"] $ elapsed_time = [time-times[0] for time in times] $ print(elapsed_time[:10])
df_DRGs.head(2)
train.head()
old_page_converted = np.random.choice([0,1], size = (145274), p = [0.88, 0.12]) $ p_old = (old_page_converted == 1).mean() $
future_df.tail()
features, feature_names = ft.dfs(entityset = es, target_entity = 'clients', $                                  agg_primitives = ['mean', 'max', 'percent_true', 'last'], $                                  trans_primitives = ['years', 'month', 'subtract', 'divide'])
convert_old = df2.query('group == "control" and converted == 1').shape[0] $ convert_new = df2.query('group == "treatment" and converted == 1').shape[0] $ n_old = n_control $ n_new = n_treatment
y_train = y_train.iloc[:nb_samples]
def apply_cats(df, trn): $     for n,c in df.items(): $         if (n in trn.columns) and (trn[n].dtype.name=='category'): $             df[n] = pd.Categorical(c, categories=trn[n].cat.categories, ordered=True) $
s = pd.to_datetime(df['dnatestactivationdayid']) - pd.to_datetime(df['ordercreatedate']) $ df['week_diff'] = s.astype('timedelta64[W]') $
df2 = t $ m_i = c_t.index $ df2 = df2.drop(m_i)
(~autos["registration_year"].between(1910,2016)).sum() / autos.shape[0]
df_movies.shape
legHouse = legHouse.fillna(np.nan)
data['e'] = 1.25 $ data
base2 = df2[['placeId', 'hashtags']].groupby('placeId').aggregate(lambda x: [i for l in x for i in l ])
url='http://www.ign.com/' $ html=requests.get(url).content $ soup=BeautifulSoup(html,'lxml')
from sklearn.neighbors import KNeighborsRegressor $ neigh = KNeighborsRegressor(n_neighbors=30) $ x = np.float32(x) $ x = x.reshape((len(x),1)) $ neigh.fit(x,y) 
goodTargetUserItemInt['weeknum_rank']=[(i+(52-44))%52 for i in goodTargetUserItemInt['weeknum']] $ print sorted(goodTargetUserItemInt['weeknum_rank'].unique())
df2['Open'].apply(qnt)
p_value = (p_diffs > obs_diff[0]).mean() $ p_value $
dfFull.GarageCars = dfFull.GarageCars.fillna(dfFull.GarageCars.mean().round())
df2[['ab_page2','ab_page']]=pd.get_dummies(df2['group']) $ df2 = df2.drop('ab_page2', axis=1) $ df2['intercept'] = 1
games_2017 = nba_df.loc[(nba_df.index.year == 2017), ] $ pd.pivot_table(games_2017, values = "Tm.Pts", index = "Team", aggfunc = np.mean).sort_values(by = "Tm.Pts", ascending = False).head(5)
flight_cancels["SCHED_DEP_DATETIME"] = pd.DatetimeIndex(flight_cancels.SCHED_DEP_DATETIME) $ flight_cancels.set_index("SCHED_DEP_DATETIME", inplace=True) $ flight_cancels = flight_cancels.drop([pd.Timestamp('2014-03-09 02:00:00'), pd.Timestamp('2014-11-02 01:00:00'), pd.Timestamp('2015-03-08 02:00:00'), pd.Timestamp('2015-11-01 01:00:00'), pd.Timestamp('2016-03-13 02:00:00'), pd.Timestamp('2016-11-06 01:00:00'), pd.Timestamp('2017-03-12 02:00:00')]) $ flight_cancels.index = flight_cancels.index.tz_localize('America/New_York')
autos.registration_year.describe() $
autos.columns = ['date_crawled', 'name', 'seller', 'offer_type', 'price', 'abtest', $        'vehicle_type', 'registration_year', 'gearbox', 'power_ps', 'model', $        'odometer', 'registration_month', 'fuel_type', 'brand', $        'unrepaired_damage', 'ad_created', 'nr_of_pictures', 'postal_code', $        'last_seen'] $
corn_vege.agg({"price": ["max","min"], "sold": "size"}).rename( $             columns={"max": "most expensive", "min": "cheapest", "size":"number of sold" })
salesfull.dropna(how='any',inplace=True)
sampled_contirbutors_human_agg_by_gender_and_proj_df = non_blocking_df_save_or_load_csv( $     group_by_project_count_gender(sampled).repartition(1), $     "{0}/sampled_contirbutors_human_agg_by_gender_and_proj_3c".format(fs_prefix)).alias("sampled")
df.shape  # tuple
trump.index
df.loc[df.index[df.handle == 'Jim Cramer'], 'label'] = 2
plt.pie(typeby_city, labels=typeof_city,explode=explode, colors=colors, $         autopct="%1.1f%%", shadow=True, startangle=45) $ plt.axis("equal")
lan_sfn_query = baseball['team'].isin(['LAN', 'SFN'])
batcmd = thecmd $ result = subprocess.check_output(batcmd, shell=True)
run txt2pdf.py -o"2018-06-14-1513 FLORIDA HOSPITAL - 2013 Percentiles.pdf"  "2018-06-14-1513 FLORIDA HOSPITAL - 2013 Percentiles.txt"
archive_clean.text.str.extractall( r"(\d+\.?\d*\/\d+\.?\d*\D+\d+\.?\d*\/\d+\.?\d*)") $
print("\nThere are {} data points in our data set.".format(df.shape[0]))
df.index = df['open']
results = soup.find_all('span', attrs={'class':'short-desc'}) 
logmodel.fit(XX_train, yy_train) $ logmodel_predictionsX = logmodel.predict(XX_test) $ num_of_logmodel_predX = collections.Counter(logmodel_predictionsX) $ num_of_logmodel_predX
[column.name for column in get_child_descendant_column_data(observations_node)]
clusters.loc[clusters == clusters.value_counts().idxmax()]
dta.select_dtypes(['category']).describe()
dat_missing_zip = datAll[pd.isnull(datAll['zip'])]
len(df2[(df2['landing_page'] == 'new_page')])/df2.shape[0]
GLOBAL_VARS.MODEL_NAME = placeholder_model_name.format(best_svm_epoch) $ doc2vec_model = Doc2Vec.load(os.path.join(doc2vec_model_save_location, GLOBAL_VARS.MODEL_NAME, MODEL_PREFIX))
def mean(x): $     return x.mean() $ mean_age_per_component = git_blame.groupby('component').age.agg([mean, 'count']) $ mean_age_per_component.head()
pipeline = Pipeline(stages=stages_with_random_forest) $ model = pipeline.fit(trainingData) $ predictions = model.transform(testData)
parse_dict['category'].head(5) $
train['question_dt'].describe()
sl.loc[sl.mindate!= sl.maxdate].head()
summary.loc['missing'] = len(records) - summary.loc['count'] $ summary
holidays_df.loc[holidays_df['date']==chk.head()['date'].iloc[0]].head()
changes['date'] = changes['date'].map(lambda x: datetime.strptime(x, '%B %d, %Y'))
data = pd.read_csv('./fake_company.csv') $ data
top_songs['Track Name'].dtype
raw_annotations_df = pd.io.json.json_normalize(latest_df.annotations, record_path='value', meta='classification_id') $ raw_annotations_df.tail(5)
ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) $ ts.head()
df.printSchema()
print("Probability of user converting:", df2.converted.mean())
pr_item = (2017, 1) $ url = 'https://www.sec.gov/news/press-release/' + str(pr_item[0]) + '-' + str(pr_item[1]) $ page = urlopen(url)
p_diffs = np.array(p_diffs)
dup = df2[df2.duplicated(['user_id'], keep=False)] $ dup_id = dup['user_id'].unique() $ print("The repeated ID is", dup_id)
df.set_index('created_at', inplace=True)
bp["new charttime"]=bp["charttime"]-time_delta $ bp["new realtime"]=bp["realtime"]-time_delta $ bp.head()
tweet_archive_enhanced_clean['rating_denominator'].value_counts()
data2 = np.loadtxt(outputs / 'data.dat') $ print(data2)
html = browser.html $ jpl_soup = bs(html, 'html.parser') $ image_url = jpl_soup.find('a', {'id': 'full_image', 'data-fancybox-href': True}).get('data-fancybox-href') $ image_url
scaled_score = scaler.fit_transform(stories[['score']])
pd.merge(left=users,right=sessions,left_on=["UserID","Registered"],right_on=["UserID","SessionDate"]) $
s = pd.Series([1, 2, 3, 4, 5, 6]) $ s
%%time $ df.to_csv('losses.csv')
df_freq_users.user_location_longitude.isnull().sum()
from pandas_datareader import wb $ wb.search('gdp.*capita.*const').iloc[:,:2]
site_dct = site.iloc[0].to_dict()
classify_df.head()
df.first_affiliate_tracked.value_counts()
new_page_converted=np.random.choice([1,0],size=n_new,p=[pnew,(1-pnew)]) $ new_page_converted.mean()
data['battery_level'].value_counts().sort_index().plot(kind='bar', figsize=[25, 5])
cnn_df = constructDF("@CNN") $ display(constructDF("@CNN").head())
embeddings = tf.Variable(tf.random_uniform((len(vocab), embed_size), -1, 1)) $ embed = tf.nn.embedding_lookup(embeddings, inputs)
n_topics = 10 $ nmf = NMF(n_components=n_topics, random_state=0, init="nndsvd") $ nmf = nmf.fit(tfidf)
week_num=[] $ for i in range(len(one_station)): $     week_num.append("week"+str(math.ceil((i+1)/7))) $ week_num[:14]
plt.scatter(X,y2) $ plt.plot(X, np.dot(X_20, linear.coef_) + linear.intercept_, c='yellow') $ plt.xlabel('Hour of Day') $ plt.ylabel('Count')
plt.hist(p_diffs);
by_agency = df2.groupby('Agency') $ by_agency['# Of Positions'].sum().sort_values(ascending=False)
df_2015.shape, df_2016.shape
df['label'] = df[forecast_column].shift(-forecast_out)
df_parsed.head(3)
archive_clean.rating_numerator = archive_clean.rating_numerator.astype(float) $ archive_clean.rating_denominator = archive_clean.rating_denominator.astype(float)
weather[weather['snow_depth'] > 0].date.count()
transit_df['EXITS'] = pd.to_numeric(transit_df['EXITS'],errors='coerce') $ transit_df = transit_df.set_index('DATETIME') $ transit_df.info()
crimes.shape
mask = df["cancelled"] == 1 $ df["booking_application"][mask].value_counts() / df["booking_application"][mask].count()
encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy') $ decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')
'this is {0} number {1}'.format( "string", "1")
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ df = pd.read_csv('datasets/vow.csv')
users.head()
sns.kdeplot(pm_final['obs_count'], shade=True).set_title("Distribution of reading counts for turbines")
another_set = {2, 2.0, 'two', 'Two', 100, 'one hundred'}
ffinal = pd.merge(p_stats, finals[['TEAM_ID', "season", "PLAYER_ID", 'pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l']], $                   on=['TEAM_ID', 'PLAYER_ID', "season"], how='left') $ ffinal[['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l']] = ffinal[['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l']].fillna(0) $ ffinal['season'] = ffinal['season'] + 1 $ ffinal.groupby(['GAME_ID', "TEAM_CITY", 'TEAM_ID'])['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l'].sum().reset_index().head()
with open("a_movie.json") as json_file: $     json_data = json.load(json_file) $ for k in json_data.keys(): $     print(k + ': ', json_data[k]) $
s_pacific = s_eastern.tz_convert("US/Pacific") $ s_pacific
dataset = pd.read_csv("NYTimesBlogTrain.csv")
print np.mean(low_polarity['polarity'])*len(low_polarity) $ print np.mean(high_polarity['polarity'])*len(high_polarity)
conn.fetch(table=dict(name='iris_sql', caslib='casuser'), to=5)
one_row = date_df.iloc[0] $ print(one_row) $ print(type(one_row['count'])) $ print(type(one_row['date'])) $ print(type(one_row['duration']))
df[df.handle == 'Donald J. Trump'].head()
df_all_wells_wKNN_DEPTHtoDEPT.info()
plt.hist(np.log(house_data['price']))
train.shape
irr_df.to_clipboard()
bnbAx.head(5)
loadedModelArtifact = ml_repository_client.models.get(saved_model.uid)
s = pd.Series([7, 'AmarAkbar', 3.14, -1789710578, 'Sholay']) $ s
kmeans = pd.DataFrame(km_res.cluster_centers_) $ kmeans.columns = finalDf.columns[:-1] $ kmeans.head(2)
data_vi = data_v.set_index(pd.DatetimeIndex(data_v['VIOLATION DATE']))
test_case = contest_cr.where(F.col('party_id')== test[0][0]) $ test_case.take(1) $
np.asarray(wfn.Fa())
linreg = LinearRegression(normalize=True) $ linreg.fit(X_train, y_train)
treatment = df2[df2['group'] == 'treatment'] $ treatment_converted = treatment.converted.sum() $ prob_treatment_conversion = treatment_converted / treatment.shape[0] $ print(prob_treatment_conversion)
dates=pd.date_range('1-Sep-2017',periods=15,freq='D') $ print(dates)
%matplotlib inline $ commits_per_year.plot(kind='bar',title='Linux Commits by Year',legend=False)
nba_df.loc["2017-01-01", "Referee1":]
df.query('group!="treatment" and landing_page=="new_page"').count()[0]+\ $ df.query('group=="treatment" and landing_page!="new_page"').count()[0]
sns.countplot(y="purpose", data=rejected) $ plt.show() $ rejected.purpose.unique()
pst.instruction_files
df_coun = pd.read_csv("countries.csv") $ df_coun.head() $ df_coun.count()
active_with_return.iloc[:,1] = pd.to_datetime(active_with_return.iloc[:,1])
file_attrs_string = str(list(hdf5_file.items())) $ file_attrs_string
df['days'] = pd.to_datetime(df['timestamp']) 
stx = ha.accounts.manual_current('dbxtrack_stoxx_50', path=os.path.join('data', 'manual_accounts'), $                                  currency='LU0380865021') $ stx.add_transaction('13.08.2013', 'me', 'buy', 139, t_type='buying rate: 34.52 EUR')
%%time $ dfRegMet["sentences"] = dfRegMet["tweet"].str.lower()
df[df['public']=='offline'].count()[0]/df.count()[0]*100
more_100_df = en_tweets_df[en_tweets_df['username'].isin(more_100)].copy()
def transmogrifier(x): $     new_val = '- ' + str(x ** 3) + ' -' $     return new_val $ s4.apply(transmogrifier) $
df2['tweet_id'][(df2['predict_2_breed'] == True)].count()/2075 $
len(np.unique(reddit['subreddit']))
df1 = pd.read_csv("C:/Users/cvalentino/Desktop/UB/Project/data/tweets_publics_ext_all.csv", encoding='ANSI', $                  index_col='tweet_id', $                  sep=',') $
from sklearn.model_selection import train_test_split $ labels = df[df.year == 2017]['label'] $ test_size= 0.3 $ X_train, X_test, y_train, y_test = train_test_split(vectorized_text_labeled, labels, test_size=test_size, random_state=1)
injury_df.drop(['Acquired','Injury'], axis=1, inplace=True) $ injury_df.drop(injury_df[injury_df['Player_Full_Name'] == ' '].index, inplace=True)
vio.count()
to_be_predicted_Day2 = 52.30367575 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df.tail(5)
reg_mod_ca = sm.OLS(df_all['converted'], df_all[['CA_int', 'ab_page']]) $ analysis_ca = reg_mod_ca.fit() $ analysis_ca.summary()
df1.loc[0:8,['SerNo', 'By']]
def Show_Row(dataframe, column, name, list_columns): $     return dataframe.loc[column == name, list_columns]
data.info()
frame2.values
df_bill_data = pd.merge(df_bill_id, df_bill, on='bill_id') $ df_bill_data[df_bill_data['patient_id'] == sample_repeat]
pd_aux2=pd.DataFrame(building_pa_prc_fix_issued[['permit_creation_date','issued_date']])
P_old=df2['converted'].mean() $ print("convert rate for P_old under the null is {}" .format(P_old))
model_x.summary2() # For categorical X.
journalists_mentioned_by_female_summary_df = journalist_mention_summary(journalists_mention_df[journalists_mention_df.gender == 'F']) $ journalists_mentioned_by_female_summary_df.to_csv('output/journalists_mentioned_by_female_journalists.csv') $ journalists_mentioned_by_female_summary_df[journalist_mention_summary_fields].head(25)
plt.figure(figsize=(8, 5)) $ plt.title("Distribution of hash tags in the dataset") $ hashtags_dist.plot(40) # the most common 40 hashtags $ plt.show()
Image.open('sign_up.png')
df.info()
df = pd.get_dummies(model_data, columns = ['creation_source','org_id']) $ df.fillna(0, inplace = True) $ df
X = stock.drop(['target', 'true_grow', 'predict_grow'], 1) $ y = stock.true_grow
imagelist =['BARNES JEWISH HOSPITAL 260032.png']
store_items = store_items.rename(columns = {'bikes': 'hats'}) $ store_items
hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')
df2a = df.loc['FY16 Total'] $ print(df2a)
[v for v in vessels.type.unique() if v.find('/')==-1] $
df_cont.head()
subrtvec  = pd.DataFrame(tvecdata.todense(), $                    columns=tvec.get_feature_names(), $                          index=[X_train,y_train]) $ subrtvec.shape
ind = ind.rename(columns={'AFFGEOID_x':'AFFGEOID', 'GEOID_x':'GEOID'})
df2.query('landing_page=="new_page"').shape[0] / df2.shape[0]
jd = r1.json() $ print(jd)
joined_train_df = joined[joined.unit_sales.notnull()]
from pandas_datareader import data $ goog = data.DataReader('GOOG', start='2004', end='2016', data_source='morningstar') $ goog.head()
from scipy.stats import norm $ print(norm.ppf(1-(0.05)))
df_goog.head()
param_test2 = {'max_depth':range(1,16,2), 'min_samples_split':range(200,1001,200)} $ gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=80, max_features='sqrt',subsample=0.8),\ $                         param_grid = param_test2, scoring='log_loss',n_jobs=4,iid=False, cv=5) $ gsearch2.fit(drace_df[feats_used],y)
cityID = '0a0de7bd49ef942d' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Scottsdale.append(tweet) 
autos["price"].sort_index().head() #having multiple functions on the same line is known as method chaining. $
df.head(10)
z_score,p_value = sm.stats.proportions_ztest(count=[convert_old,convert_new],nobs=[n_old,n_new],alternative='smaller') $ z_score,p_value
cig_data_SeriesCO[cig_data_SeriesCO > cig_data_SeriesCO.median()]
price_bool1 = autos["price"].between(300, 100000, inclusive = True) $ autos = autos[price_bool1] $ print(autos.shape)
prob = df2.converted.mean() $
df_new['page_country'] = df_new['ab_page']*df_new['US'] $ df_new['page_country2'] = df_new['ab_page']*df_new['UK'] $ logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','US','UK','page_country','page_country2']]) $ result = logit_mod.fit() $ result.summary()
loans_df.term.value_counts()
p=df2.query('landing_page=="new_page"').describe() $ p
finals[finals.season==2016].head()
df.groupby('key')
google['high'].apply(custome_roune).nunique()
Measurement = Base.classes.measurement $ Station = Base.classes.station
import nltk $ parser = nltk.ChartParser(my_grammar) $ sentence = word_tokenize("I shot an elephant in my pajamas") $ for tree in parser.parse(sentence): $     print(tree)
trump.drop('id_str', axis=1, inplace=True) $ trump.drop('in_reply_to_user_id_str', axis=1, inplace=True)
heehee = pd.DataFrame() $ heehee['y'] = y_test $ heehee['y_pred'] = predi
grouped_grades.mean()
b1[b1.base_twitter_count > 100000].shape
inv=repaid_loans_cash[(repaid_loans_cash.fk_loan==36)].fk_user_investor.unique()
apple.resample('M').mean().plot(grid=True)
daily_deltas = (hits_df.hits - hits_df.hits.shift()).fillna(0)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_NearTop =  pd.read_pickle('df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p')
with open(djb_save) as f: $     djb_soup = bs4.BeautifulSoup(f, 'html.parser') $ dj_content = djb_soup.body.findAll('article') $ for x in dj_content: $ 	print(x.text)
lo = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ result = lo.fit()
from sklearn.cluster import AgglomerativeClustering
age_gender.head()
def lm (x, y): $     lm = sm.formula.ols(formula = 'y ~ x', data = data).fit() $     x_new = pd.DataFrame({'newdata' : range(1,len(x)+1)}) $     y_preds = lm.predict(x_new) $     print(lm.summary())
merged = price2017.merge(typesub2017,how='inner',left_on='DateTime',right_on='DateTime')
crime_df.head()
mig_l12.agg([np.mean, np.std])
full_monte = pd.DataFrame({'name': monte, $                           'info': ['B|C|D', 'B|D', 'A|C', $                                   'B|D', 'B|C', 'B|C|D']}) $ full_monte
fig = plt.figure(figsize=(18, 6)) $ ax1 = fig.add_subplot(111) $ bins =[0,1,5,10,60,120,180,240,300,600,1200,1800,2400,3000,3600,7200,10800,21600,43200] $ counts, bins, fig = ax1.hist(train.duration, bins=bins)
autos["brand"].value_counts().head(20)
from sklearn.model_selection import train_test_split $ from sklearn import svm $ from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error
import pandas_datareader as pdr $ import datetime $ aapl = pdr.get_data_yahoo('AAPL', $                           start=datetime.datetime(2014, 1, 1), $                           end=datetime.datetime(2017, 3, 1))
knnmodel.fit(X_train, y_train) $ knnmodel_predictions = knnmodel.predict(X_test) $ num_of_knnmodel_pred = collections.Counter(knnmodel_predictions) $ num_of_knnmodel_pred
data.columns
import json $ import pandas as pd $ import math $ with open('./data/go-original.cyjs', 'r') as f: $     gotree = json.load(f)
data.head(4)
owns[['user_id', 'repo_id', 'created_at']].to_csv('data/new_subset_data/new_subset_owns.csv', sep='\t', index=False)
df2.converted.sum()/df2.converted.count()
def add_to_list(word_list, dictionary): $     for each in word_list: $         dictionary.append(each)
autos['registration_year'].value_counts().sort_index(ascending=True).head(20)
releases['jurisdiction_cd'].unique()
stocks = pd.concat([AAPL, GOOGL], keys=['Apple', 'Google']) $ stocks
reviews.points.dtype
df = pd.read_csv('./reddit_data_2.csv')
with open(sources_yaml_path, 'r') as f: $     sources = yaml.load(f.read()) $ for k, v in sources.items(): $     print(yaml.dump({k: list(v.keys())}, default_flow_style=False))
returns = data[tickers] / data[tickers].shift(1) - 1 $ returns = returns.fillna(method='ffill').dropna() $ returns.plot()
dfs = amzn.loc['AMZN']
a.alias('sepal_width')
stops.head()
accounts[' Total BRR '] = accounts[' Total BRR '].map(lambda tbrr: float(tbrr.split('$')[1] $                                                                          .replace(',','') $                                                                          .replace('-','0.0'))) $ accounts[' Total BRR '] = accounts[' Total BRR '].astype(float)
inspector =inspect(engine) $ inspector.get_table_names()
df['acct_type'].unique().tolist()
new_page_converted = np.random.binomial(1, p_new, n_new) $ new_page_converted
y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled) $ mse = mean_squared_error(y_train, y_pred) $ np.sqrt(mse)
prop.head()
pred6 = nba_pred_modelv1.predict(g6) $ prob6 = nba_pred_modelv1.predict_proba(g6) $ print(pred6) $ print(prob6)
taxiData2.loc[taxiData2.Tip_amount < 0, "Tip_amount"] = 0
from datetime import datetime $ date = nc.num2date(time, 'hours since 1800-01-01 00:00:0.0') $ ts = pd.Series(date, index = date) $ ts.head()
for k in range(initial_year,final_year + 1): $     idx = df_providers[ (df_providers['year']==k)].index.tolist() $     drgs_that_year = df_providers.loc[idx,'drg3'].unique() $     drgs_that_year.sort() $     print(k,len(drgs_that_year ))
data.columns
oecd = pd.read_html(oecd_site, header=0)[1][['Country', 'Date']] $ oecd.head()
result.acknowledged
plt.figure(figsize=(10,3)) $ plt.plot(weekly['weekday'],weekly['DAILY_ENTRIES'])
MATTHEWKW.head().text.values
cr_old_null = len(df2.query('converted == 1')) / len(df2['converted']) $ print('The convert rate for P_old under the null is {}'.format(cr_old_null))
roundedDF=np.around(overallDF, decimals=2) $ roundedDF["Compounded Score"]
save_model('model_xgb_v1.mod', xgb_grid) 
hours = bikes.groupby('hour_of_day').agg('count') $ hours['hour'] = hours.index $ hours.start.plot() $
nt = nt[nt["sq_price_value"] != 0.0]
df[df['Complaint Type'] == 'Homeless Encampment'].index.month.value_counts().sort_index().plot()
donations['Donation Received Date'].dt.month.value_counts().sort_index().plot(kind='bar') $ plt.title('Donations Received by Month') $ plt.xlabel('Month') $ plt.ylabel('Counts')
BDAY_PAIR_df['pair_age'] = ((pd.to_datetime(BDAY_PAIR_df['created_at']) - pd.to_datetime(BDAY_PAIR_df['birthdate']))/np.timedelta64(1,'M'))
print("P converting:", df2.converted.mean())
print(who_purchased.sum(axis=0).sum()) $ print() $ print(who_purchased.sum(axis=0))
x = tf.constant(1, name='x') $ y = tf.Variable(x+10, name='y') $ print(y)
x = np.array([1,2,3,4,5], dtype=np.float64)
speeches_metadata.columns
def get_trip_data(x): $     return suspects_with_25_1.loc[:x].tail(2) $ map(get_trip_data, trip_index_25_1)
noaa_month = noaa_data.loc["2018-07-01":"2018-08-01"] $ noaa_month.loc[:,'AIR_TEMPERATURE'].groupby(noaa_month.index.hour).mean()
complete_df[complete_df['Totals'].apply(lambda x: int(x)) < 0]
may_acj_data.head()
df_image_clean.nlargest(5, 'img_num')[['Frist_pred_conf', 'Second_pred_conf', 'Third_pred_conf']]
%%bash $ grep -E '^ {2}"' md_traffic.json
tesla['Close'].describe() # the describe function will give us a statistical summary
session.query(Measurement.station, func.count(Measurement.station)).\ $     group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all()
df['body'] = df['body'].apply(lambda x: x.lower());
gscv.fit(x_train, y_train)
train_df.columns[train_df.isnull().any()].tolist()
df.loc['20180101':'20180103',['A','C']]
print("Probability of user converting is :", ab_file2.converted.mean())
sns.regplot(x = np.arange(-len(my_tweet_df[my_tweet_df["tweet_source"] == "BBC News (World)"]), 0, 1),y=my_tweet_df[my_tweet_df["tweet_source"] == "BBC News (World)"]["tweet_vader_score"],fit_reg=False,marker = "o",scatter_kws={"color":"darkred","alpha":0.8,"s":100}) $ ax = plt.gca() $ ax.set_title("Sentiment Analysis (BBC)",fontsize = 12) $ plt.savefig('Sentiment_BBC.png')
import test_package.print_hello_direct
sns.barplot(x='user',y='number_of_retweets',data=most_retweeted_tweeps_sorted.tail(5))
X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)
status = client.experiments.get_status(experiment_run_uid)
plt.xticks(rotation=25) $ plt.plot(data['created_at'],data['rating'],'b.',alpha=0.5) $ plt.plot(data['created_at'],data['timestamp']*slope + intercept,'r-', linewidth=3) $ plt.show()
n_new = df2.query('group == "treatment"').shape[0] $ n_new
with open('data/recipeitems-latest.json') as f: $     line = f.readline() $     line = line.split('\n') $     line = eval(line[0]) $ line $
pd.date_range(start = '08/19/2018', end = '08/29/2018', freq = myc)
tlen.plot(figsize=(16,4), color='r');
y_train = train['any_spot'].values $ X_train = train.drop(['Temperature_Departure','Temperature_Avg','year','parkings_by_hour','index','date','any_spot','Real.Spots','datetime','month','DOW','Street','From','To','Block','Street.Length'], axis=1) $ new_val = val.dropna() $ y_valid = new_val['any_spot'].values $ X_valid = new_val.drop(['Temperature_Departure','Temperature_Avg','year','parkings_by_hour','index','date','any_spot','Real.Spots','datetime','month','DOW','Street','From','To','Block','Street.Length'], axis=1)
twitter_archive_master[(twitter_archive_master['rating_denominator'] == 20) & (twitter_archive_master['rating_numerator'] == 4)].iloc[0].name
print("P(converted) = %.4f" %df2.converted.mean())
X.columns
for model_name in nbsvm_models.keys(): $     valid_probs[model_name] = nbsvm_models[model_name].predict_proba(X_valid_cont_doc)
all_tables_df.iloc[0]
vip_reason.columns = ['VIP_'+str(col) for col in vip_reason.columns]
dr_num_patients.index
start2 = datetime.datetime.strptime('2014-05-01', "%Y-%m-%d").timestamp() $ end2 = datetime.datetime.strptime('2015-12-31', "%Y-%m-%d").timestamp() $ stockdftest = getStockDataframe(stocklist, start2, end2, True) 
month_df = pd.get_dummies(df_total.index.month, prefix='month') $ month_df.index =  df_total.index
multiG = nx.MultiDiGraph() $ loadInNodes(multiG) $ loadInEdges(multiG)
pd.to_datetime(segments.st_time[:])
np.exp(reg_lm2.params)
stroh = am.defect.Stroh(C=C, burgers=burgers, axes=np.array([x_axis, y_axis, z_axis])) $ K_tensor = stroh.K_tensor $ print(uc.get_in_units(K_tensor, 'GPa'))
obj.rank()
dbData.head(7)  # NaN's show up when the field has no data.  Need both masses, eccentricity, semimajor axis, $
austin.shape
df['np_counts'] = df['clean_text'].apply(lambda x: get_np_counts(x, df['profile']))
from sklearn.feature_extraction.text import CountVectorizer
plt.figure(figsize=(16,8)) $ dendrogram(features['bbc-news']['linkage'], orientation='top', $           p=300, truncate_mode='lastp', no_labels=True, color_threshold=0) $ plt.axes().get_yaxis().set_visible(False) $ plt.show()
y_pred = classifier.predict( X_test ) $ report = sklearn.metrics.classification_report( y_test , y_pred ) $ print(report)
train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1) $ test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1) $ combine = [train_df, test_df] $ train_df.head()
essential_genes = pd.read_excel("http://mbio.asm.org/content/9/1/e02096-17/DC7/embed/inline-supplementary-material-7.xlsx", header=1)
sns.set_color_codes("pastel") $ sns.despine(offset=10, trim=True)
df_archive_clean["name"].value_counts()[0:15]
pd.merge(user_products, transactions, how='left', on=['UserID', 'ProductID']).groupby(['UserID', 'ProductID']).apply(lambda x: pd.Series(dict(Quantity=x.Quantity.sum()))).reset_index().fillna(0)
df_state_victory_margins.sort_values('Percent margin', ascending=False, inplace=True)
stopwords.words("english")
financial_crisis.size
df.min(axis=1) # axis=1, row-wise
df_customers['number of customers'] = np.exp(df_customers['log number of customers'])
df_onc_no_metac['ONC_LATEST_N'] = df_onc_no_metac['ONC_LATEST_N'].replace({'N0(i-)': 'N0(i_minus)', 'N0(i+)': 'N0(i_plus)'}) $ df_onc_no_metac['ONC_LATEST_N'].nunique()
df.iloc[0, [0, 1]]
autos = autos.drop(index = price_outliers)
number_of_commits = len(git_log['timestamp']) $ number_of_authors = len(pd.unique(git_log['author'].dropna())) $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
optimizer = tf.train.AdamOptimizer() $ training_op = optimizer.minimize(loss, name="training_op")
bible_top_vec = all_top_vecs[bks.kjv_bible[0]] $ find_most_similar(bible_top_vec, all_top_vecs, title_lst, vec_in_corp='Y')
df2['tweet_id'][(df2['predict_1_breed'] == True)].count()/2075 $
df2.query('landing_page == "new_page"').shape[0] / df2.shape[0]
N_new = df2.query('landing_page == "new_page"')['user_id'].nunique() $ N_new 
vow.set_index('Date', inplace=True)
import matplotlib.pyplot as plt $ import seaborn as sns
nb = MultinomialNB() $ nb.fit(X_train, y_train) $ nb.score(X_test, y_test)
own_star.drop('uniqueID', axis=1, inplace=True)
example['hour (sin)'].plot() $ example['hour (cos)'].plot()
result2.summary()
Z1 = np.random.randint(0,10,10) $ print (Z1) $ Z2 = np.random.randint(0,10,10) $ print (Z2) $ print(np.intersect1d(Z1,Z2))
df[(df['landing_page'] == 'new_page') & (df['group'] != 'treatment')].landing_page.count()
p_diffs = [] $ for _ in range(10000): $     new_page_converted1 = np.random.binomial(1,P_new,n_new) $     old_page_converted1 = np.random.binomial(1, P_old, n_old) $     p_diffs.append(new_page_converted1.mean()-old_page_converted1.mean())
auth = tweepy.OAuthHandler(consumer_key=consumerKey, consumer_secret=consumerSecret) $ api = tweepy.API(auth)
autos["brand"].value_counts(normalize=True).head(20)*100
s3_upload.upload_file(local_orig1, s3_bucket, s3_key_orig1, $                          Callback=ProgressPercentage(local_orig1)) $ s3_upload.upload_file(local_orig2, s3_bucket, s3_key_orig2, $                          Callback=ProgressPercentage(local_orig2)) $
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new], alternative = 'smaller') $ z_score, p_value
favorite_count_3q = master_df.favorite_count.quantile(0.75) $ favorite_count_iqr = master_df.favorite_count.quantile(0.75) - master_df.favorite_count.quantile(0.25) $ outlier_df = master_df.loc[master_df.favorite_count >= (favorite_count_3q+(1.5*favorite_count_iqr))]
%%time $ df.head()
df2.head()
yc_new2['Tip_Amt'] = yc_new['Tip_Amt'] / yc_new['Fare_Amt'] * 100 $ yc_new2.head()
maxForRow = ddf.max(axis=0) $ globalMax = maxForRow.max() $ print("Max Sharpe Ratio is: {0:.2f}".format(globalMax)) $ ddf[ddf.values==globalMax]
archive_df = pd.read_csv('twitter-archive-enhanced.csv', sep = ',')
for k in range(len(the_hospitals)): $     idx = df_providers[ (df_providers['id_num']==the_hospitals[k])].index.tolist() $     print("'" + df_providers.loc[idx[0],'name'] + "'," ,end='') $
merged2.shape $
DATADIC = pd.read_csv('DATADIC.csv') $ list(DATADIC['FLDNAME'].unique()) $ DATADIC[['FLDNAME', 'TEXT']].head()
titanic3['home.dest'].str.upper().head()
df_usage_test = pd.read_csv("test_usage_data.csv") $ df_usage_test.head()
try: $     open('test_data//write_test.txt', mode='w').read() $ except Exception as error_message: $     print(error_message)
p_value=(1-0.19/2) $ p_value
p_diff = new_page_converted.mean() - old_page_converted.mean() $ p_diff
bins = [400, 2000, 3500, 5000] $ bin_names = ['Small', 'Medium', 'Large'] $ df_grouped['size_bins'] = pd.cut(df_grouped['size'], bins, labels = bin_names) $ df_grouped.head()
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
assert (ebola_0 >= 0).all().all()
subwaydf.iloc[174568:174572] #this high number seems to be because entries and exits messes up.
sqlContext.sql("select * from pcs where count > 1").show()
logit_mod3=sm.Logit(df_new['converted'],df_new[['ab_page','intercept', 'CA', 'UK', 'CanadaNew','UKNew']]) $ fit3=logit_mod3.fit() $ fit3.summary()
autos["registration_year"].value_counts(normalize=True).sort_index(ascending=False).head(10)
p_old = df2['converted'].mean() $ print("Convert rate for p_old :", p_old)
datapath='/Users/paulp/GoogleDrive/projects/PeaIslandBeachMonitoring/data/' $ fn='BiologicalCounts.txt' #'PeaIsland_BioCounts.txt' $ latest_survey='201708'  # most recent survey - use for labeling plot graphics... $ plotpath='/Users/paulp/GoogleDrive/projects/PeaIslandBeachMonitoring/plots/PlotGroup6/'
dd2=cfs.diff_abundance('Subject','Control','Patient', transform=None, random_seed=2018)
df_new = countries.set_index ( 'user_id' ) .join (df2.set_index ('user_id') , how='inner' )
pd.options.mode.chained_assignment = None  # default='warn'
df['Category']=df['Memo'].apply(returnCategory) $ df['Single Name']=df['Name'].apply(returnName) $ df.head()
df1 = df1.rename(columns={"0": 'created_at', "1": "tweet"}) $ df2 = df2.rename(columns={"0": 'created_at', "1": "tweet"})
dfstatus = pd.get_dummies(cp311['status'])
ob_diffs = df2.query('group == "treatment"').converted.mean() - df2.query('group == "control"').converted.mean()
raw_full_df.building_id_iszero.value_counts()
from pysumma.Plotting import Plotting $ from jupyterthemes import jtplot $ import matplotlib.pyplot as plt $ import pandas as pd $ jtplot.figsize(x=10, y=10)
%%time $ df["sentences"] = df["tweet"].str.lower()
x = [[1, 2], $     [3, 4]] $ np.concatenate([x, x], axis=1)
first_row = session.query(Station).first() $ first_row.__dict__
df[df.donor_id == '_1D50SWTKX'].sort_values(by='activity_date').tail()
df2.query('group == "control"').converted.sum() $
StockData.count().sum()
nt_price = nt.groupby(pd.Grouper(freq='W'))["sq_price_value"].mean().to_frame().rename(index=str, columns={"sq_price_value":"nt_sq_price_value"})
print(artistAliasDF[artistAliasDF.StandardArtistID == 1000010].count(), "MisspelledArtistID point to Aerosmith.") $ print("The old dataframe contains the artistID 1000010:", userArtistDF[userArtistDF.artistID == 1000010].count(), "times.") $ print("The new dataframe contains the artistID 1000010:", newUserArtistDF[newUserArtistDF.artistID == 1000010].count(), "times.")
index = pd.DatetimeIndex(['2014-07-04', '2014-08-04', $                          '2015-07-04', '2015-08-04']) $ data = pd.Series([0, 1, 2, 3], index=index) $ data
temp_df['reorder_interval_group'].isnull().any()
data.isnull().sum() # column name and number of nulls 
os.chdir('/Users/Vigoda/Knivsta/Capstone project/Adding_2015_IPPS') $ Number_Types_Files_Current_Directory()
ALLbyseasons.shape # This matches the height of our previous sample, confirming that we didn't lose any transactions.
P_new = ab_df2['converted'].mean() $ print(P_new)
agency_group = data.groupby('Agency') $ agency_group.size().plot(kind='bar')
df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'], $                    'rank': [1, 2, 3, 4]}) $ df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'], $                    'rank': [3, 1, 4, 2]}) $ display('df8', 'df9', 'pd.merge(df8, df9, on="name")')
df2['timestamp']=pd.to_datetime(df['timestamp'],format='%Y-%m-%d %H:%M:%S') $ duration = df2['timestamp'].sort_values() $ duration[0] - duration[len(duration)-1]
sel_shopping_cart = pd.DataFrame(items, index = ['pants', 'book']) $ sel_shopping_cart
pets = pd.read_pickle('petpickle.pkl') $ pets
print(df_van_asn.shape, $ df_tor_asn.shape, $ df_mtl_asn.shape)
import re $ import string $ punch=set(string.punctuation)
for key, grp in raw_data.groupby('objective'): $     print "{0: <20}{1}".format(key, percent_charged(grp.charged.values))
counts_df[counts_df.tweet_id.duplicated()]
MNB = MultinomialNB() $ model3 = MNB.fit(x_train, y_train)
start_time = datetime.datetime.now() $ print('start_time:', start_time)
twitter_df_clean.time.sample(2)
%%time $ validation_docs_iterator = DocumentBatchGenerator(validation_preprocessed_files_prefix, $                                                   validation_preprocessed_docids_files_prefix, batch_size=None) $ pool = ThreadPool(16) $ threaded_reps = pool.map(infer_one_doc, validation_docs_iterator)
f=table.find(text='Fatalities').find_next('td').text $ fatalities=re.search(r'\d+', f).group() $ fatalities
twitter_archive_master.head()
d = pd.read_csv('twitter_archive_master.csv')
first_result.find('a').text[1:-1]
xirrs_all['rating_base'].to_clipboard()
precipitation_df.head()
np.eye(5)
df = df_raw.filter(items=['year','list_date','rank_this_week','rank_last_week','song_title', $                           'artist','peak_to_date','weeks_on_chart']) $ df.columns = ['year','date','tw','lw','title','artist','peak_to_date','weeks_on_chart'] $ df = df.sort_values(by=['year','date','tw']) $ df
print(afx_x_oneday.json())
rfr = RandomForestRegressor(random_state=0) $ cv_score = cross_val_score(rfr, features_regress_vect, overdue_duration, scoring='neg_median_absolute_error', cv=5) $ 'MedianAE score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
df2.timestamp.min(), df2.timestamp.max()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
np.random.seed(0) $ def generate_data(case, sparse=False): $     Benchmark influence of :changing_param: on both MSE and latency. $     Plot influence of model complexity on both accuracy and latency. $
db.limit(db.arrays.KG_VARIANT_EXAMPLE, 10)[:]
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
pd.DataFrame(not_missing_values_pd['SexuponOutcome'].value_counts()).plot(kind='bar')
m=sm.Logit(f_new['converted'],f_new[['intercept','US','CA']]) $ results=m.fit() $ results.summary()
def filtering(s): $     s = s.lower() $     s = ''.join([c for c in s if c in string.printable]) # Get rid of non ascii whitespace chars, e.g. japanese $     s = s.strip() # Get rid of whitespace AFTER removing chars $     return s
train[simple_features].columns[2], train[simple_features].columns[14], train[simple_features].columns[10]
multi_index=pd.MultiIndex.from_tuples([(i, j, k) for i , j, k in zip(label_index, community_index, $                                                                      education_data['Category'].tolist())])
iris.iloc[iris.iloc[:,1].between(3.5, 3.6).values,1]
csvData[csvData['street'].str.match('.*Drive.*')]['street']
hits_df.loc[outliers] = np.NaN
p_control = df2.query('group == "control"').converted.mean() $ p_control
if not os.path.isdir('output'): $     os.makedirs('output')
print cust_data.drop_duplicates().head(3) $
df.start_date = pd.to_datetime(df.start_date, format='%m/%d/%Y %H:%M')
my_data.take(10)
Celsius.temperature.__doc__
df2.loc['2016-03-01':'2016-03-31', ['GrossOut', 'NetOut']].sum()
(taxiData2.Fare_amount <= 0).any() # This Returns False, proving we have successfully changed the values with no negative $
import statsmodels.api as sm $ convert_old = df2[(df2['landing_page'] == 'old_page') & (df2['converted'] == 1)].user_id.count() $ convert_new = df2[(df2['landing_page'] == 'new_page') & (df2['converted'] == 1)].user_id.count() $ n_old = df2[df2['landing_page'] == 'old_page'].user_id.count() $ n_new = df2[df2['landing_page'] == 'new_page'].user_id.count()
week1 = opening.rename(columns={7:'7'}) $ stocks = stocks.rename(columns={'Opening Price':'Week 1','Time':'7'}) $ week1 = pd.merge(stocks,week1,on=['7','Tickers']) $ week1.drop_duplicates(subset='Link',inplace=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)
import datetime $ data['Created Date'] = data['Created Date'].apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))
from test_package.print_hello_function_container import * 
results.summary() #Summary of our test results.
from sklearn.metrics import auc, roc_curve
%%time $ def reconsitute_full_df(timeseries_list): $     full_df = pd.concat(timeseries_list, ignore_index=True) $     return full_df $ all_turnstiles = reconsitute_full_df(turnstile_timeseries_list)
tallies_file = openmc.Tallies() $ tallies_file += total.tallies.values() $ tallies_file += absorption.tallies.values() $ tallies_file += scattering.tallies.values() $ tallies_file.export_to_xml()
bus.describe()
df_prep12 = df_prep(df12) $ df_prep12_ = pd.DataFrame({'date':df_prep12.index, 'values':df_prep12.values}, index=pd.to_datetime(df_prep12.index))
df.head()
p_new=df2['converted'].mean() $ print(p_new)
prop = props[props.prop_name == 'PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE.']
smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name, $                                                HyperParameterTuningJobConfig = tuning_job_config, $                                                TrainingJobDefinition = training_job_definition)
grouped_city = department_df.groupby('City') $ grouped_city # groupby object 
flattened_df = pd.concat([indexed_df, review_count], axis=1, join_axes=[indexed_df.index]) $ flattened_df.rename(columns={'movie_name': 'review_count'}, inplace=True) $ flattened_df = pd.concat([flattened_df, star_avg], axis=1, join_axes=[indexed_df.index]) $ flattened_df.rename(columns={'stars': 'star_avg'}, inplace=True) $ flattened_df = pd.concat([flattened_df, positive], axis=1, join_axes=[indexed_df.index])
print(json.dumps(tweets._json, indent=4))
df_sched.head()
mean_weekday.iloc[[1,3]]
df.tail(3)
date_splits = sorted(list(mentions_df["date"].unique())) $
df2.corr()
pairs_1 = [] $ for num1 in range(0,2): $     for num2 in range(6,8): $         pairs_1.append((num1, num2)) $ print(pairs_1)
MultData.groupby('to_account').mean()
alice_sel_shopping_cart = pd.DataFrame(items, index = ['glasses', 'bike'], columns = ['Alice']) $ alice_sel_shopping_cart
corpus_tf = [dictionary.doc2bow(sentence) for sentence in all_sentences] $ print('corpus tf done') $ tfidf = models.TfidfModel(corpus_tf) $ corpus_tfidf = tfidf[corpus_tf] $ print('corpus tfidf done')
def roundup_10(x): $     return float(math.ceil(x / 10.0)) * 10 $ df['year_built'] = df['year_built'].iloc[:, 0] $ df['year_built'] $ df['year_built'] = df['year_built'].iloc[:, 0].apply(lambda row: roundup_10(row)) $
msft.insert(0, 'Symbol', 'MSFT') $ aapl.insert(0, 'Symbol', 'AAPL') $ combined = pd.concat([msft, aapl]).sort_index()
grouped.to_csv('tweetcount.csv')
daily_transaction=file4.groupby('date').agg({'uuid':'count','trans_amt':'avg'}).sort(desc("date")) $ daily_transaction.show()
%%time $ df = pd.read_csv('data/311_Service_Requests_from_2010_to_Present.csv')
data.head(3)
closePrice = aapl['Close'] $ closePrice.head(5)
tweets1 = pd.concat([una_tweets, amanda_tweets], axis=0) $ tweets1.shape
df_train.min(axis=0)
data['retained'] = pd.to_datetime('2014-07-01') - pd.to_datetime(data.last_trip_date) <= pd.Timedelta('30 days')
type(s), s.dtype, len(s), s.shape
df.head()
print(clf.feature_importances_)
df2[df2['landing_page']=='new_page']['landing_page'].count()/df2['landing_page'].count()
log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'uk', 'us']]) $ result = log_mod.fit() $ result.summary()
df.index = df.datetime
stock['next_day_open'] = stock.open.shift(-1) $ stock['true_grow'] = stock.target.apply(lambda x: 1 if x >= 0 else 0)
file_t = open('tweets_terror3.txt','w') $ for item in terrorism: $     file_t.write("%s\n" % item)
df = tables[0] $ df.columns = ['', 'Mars'] $ df.set_index('', inplace=True) $ df
data = pd.read_csv('dump.csv')
all_data.index.is_unique
merge[merge.columns[18]].value_counts().sort $
zero_rev_acc_opps.head()
df2['user_id'].value_counts()
tweets.text[0]
from biopandas.mol2 import PandasMol2 $ import pandas as pd $ pd.set_option('display.width', 600) $ pd.set_option('display.max_columns', 8)
clients = pd.DataFrame() $ for i in list(range(0, len(user_clients))): $     clients_df_temp = pd.DataFrame.from_dict(user_clients) $     clients = pd.concat([clients_df_temp, clients])
stock_return = stocks.apply(lambda x: x / x[0]) $ stock_return.head()
p_new = df2.converted.sum()/len(df2) $ p_new
train.raceeth.value_counts()
df0 = pd.read_csv('../../datasets/crypto-index-fund/crypto_data/CryptoData.csv', parse_dates=True)
get_time_delta = lambda s: s.diff(1).shift(-1)
(autos["last_seen"] $  .str[:10] $  .value_counts(normalize=True,dropna=False) $  .sort_index() $ )
df.info()
image_predictions_df.describe()
crime_data = crime_data[crime_data["DATE_TIME"].dt.year >= 2016] $ print(crime_data["DATE_TIME"].dt.year.value_counts()) $ print(crime_data["BORO_NM"].value_counts())
temp_df['reorder_interval_group'] = temp_df['reorder_interval_group'].astype(float)
B2_NTOT_WINTER_SETTINGS = lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'] $ lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'].allowed_variables $ lv_workspace.get_subset_object('B').get_step_object('step_2').indicator_ref_settings['ntot_winter'].settings.get_value('EK G/M', 22) $
potential = lmp.Potential('2015--Pascuet-M-I--Al--LAMMPS--ipr1.json')
pids_df = TensorBoard.list() $ if not pids_df.empty: $     for pid in pids_df['pid']: $         TensorBoard().stop(pid) $         print 'Stopped TensorBoard with pid {}'.format(pid)
training_features, test_features, \ $ training_target, test_target, = train_test_split(X, y, test_size = 0.33, random_state=12)
station_availability_df.head()
p_diffs = [] $ for _ in range(10000): $     new_page_converted=np.random.choice([0,1],size=nnew[0],p=[pnew,1-pnew]).mean() $     old_page_converted=np.random.choice([0,1],size=nold[0],p=[pold,1-pold]).mean() $     p_diffs.append((new_page_converted/nnew)-(old_page_converted/nold))
from sklearn.feature_extraction.text import CountVectorizer
sum_row=df[["Dividend Yield","Closing Price"]].sum() $ sum_row
pd.Series(np.array(['a','b','c','d','e']))  # from np.array
new_page_converted = np.random.normal(0, p_new, n_new)
month['PERIOD ENTRIES'] = month['ENTRIES'].diff() $ month['PERIOD ENTRIES'] = month['PERIOD ENTRIES'].shift(periods = -1) $ month
image_predictions = pd.read_csv('image-predictions.tsv', sep ='\t') $
red_inter_recr.head()
df_weather.precipitation_inches.unique()
van_final['isReverted'].value_counts()
keep_types = [u'WWW', u'WND', u'WAS', u'TSN', u'NUC', u'NG', $        u'PEL', u'PC', u'OTH', u'COW', u'OOG', u'HPS', u'HYC', u'GEO'] $ eia_total_annual = eia_total_monthly.reset_index().groupby('year').sum() $ eia_total_annual['index (g/kWh)'] = eia_total_annual['elec fuel CO2 (kg)'] / eia_total_annual['generation (MWh)']
r.headers['content-type']
prediction_frame=pd.DataFrame(columns=(predictions['fields'])) $ prediction_frame
old_page_converted=np.random.binomial(1,Conversion_Rate,Conversion_No1)
tweet_full_df.info()
df_new.drop(['us'], axis=1, inplace=True)
linreg = linear_model.LinearRegression() # call the sklearn.linear_model function $ linreg.fit(quadratic, y) # the fit() function creates the best fit line
pats_chiefs_nov8_tweets = pats_chiefs_nov8_tweets.reset_index().drop('tweet_id', axis=1)
daily_returns.corr(method='pearson')
price_df.to_csv('price2.csv')
temp_list = DataSet.loc[DataSet['userFollowerCt'] == max_followers] $ print(temp_list['tweetText'])
df4.describe() # basic stats all from one method $
support = merged[merged.committee_position == "SUPPORT"]
df2.head(2)
import matplotlib.pyplot as plt $ from matplotlib import style $ style.use('ggplot') $ Y=df1[forcast_col] $ x=range(len(df1[forcast_col]))
f, axes = plt.subplots(1, 1, figsize=(10, 6)) $ sns.distplot(df['polarity'], kde=False, bins=20) $ plt.ylabel('count') $ plt.title('Polarity for "#Trump"') $
Xtr_m = np.mean(Xtr[:,:7],axis=0) $ Xtr_std = np.std(Xtr[:,:7],axis=0) $ Xtr_scale = (Xtr[:,:7]-Xtr_m)/Xtr_std $ Xts_scale = (Xts[:,:7]-Xtr_m)/Xtr_std
ocsvm_cleaned_bow.fit(trump_cleaned_bow, y = y_true_cleaned_bow) $ prediction_cleaned_bow = ocsvm_cleaned_bow.predict(test_cleaned_bow) $ prediction_cleaned_bow
! python  keras-yolo3/yolo.py ../sonkey13.jpeg
def show_distribution(df): $     n, bins, patches = plt.hist(df) $     plt.show()
test_portfolio[test_portfolio['date'] == test_portfolio['date'].min()]
print(df2.loc[[2893]]) $ df2.drop_duplicates(subset='user_id', inplace=True)
soup.findAll(attrs={'class':'yt-uix-tile-link'})[0]['href'][9:]
merged.loc[merged['state'].isnull(), 'state/region'].unique()
new_comments_df = pd.read_csv('input/test.csv') # Replace 'test.csv' with your dataset $ X_test = test["comment_text"].str.lower() # Replace "comment_text" with the label of the column containing your comments
n_new = df2.query('landing_page == "new_page"').nunique()['user_id'] $ n_new $
automl.leaderboard
val = val.join(street_freq, on="Block", rsuffix='_fre')
web.DataReader("A576RC1A027NBEA","fred",datetime.date(1929,1,1),datetime.date(2013,1,1))
df[df['Descriptor'] == 'Pothole'].count()
print(airquality.head()) $ airquality_melt = pd.melt( $     airquality, $     id_vars=['Month', 'Day']) $ print(airquality_melt.head()) $
converted_old=  df2.query('landing_page == "old_page"')['converted'].sum() $ converted_new= df2.query('landing_page == "new_page"')['converted'].sum() $ n_old = df2.query('landing_page == "old_page"').shape[0] $ n_new = df2.query('landing_page == "new_page"').shape[0]
df_archive_clean["retweeted_status_timestamp"] = pd.to_datetime(df_archive_clean["retweeted_status_timestamp"], $                                                                 format='%Y-%m-%d %H:%M:%S ',utc = True)
df_new[['CA','UK','US']] = pd.get_dummies(df_new['country']) $ df_new = df_new.drop('CA',axis=1) #Making the CA column the baseline for country $ df_new.head()
topten.loc[:, (topten != 0).any(axis=0)].columns
data['year'] = data.order_date.dt.year
dfRegMet2013 = dfRegMet[dfRegMet.index.year == 2013]
pizza_poor_reviews.head(2)
average_chart_upper_control_limit = average_of_averages + 3 * d_three * average_range / \ $                                     (d_two * math.sqrt(subgroup_size))
treatment = pd.Series([0]*4 + [1]*2) $ treatment
predicted_probs_first_measure = holdout_results[holdout_results.second_measurement==0].\ $ groupby('wpdx_id').cv_probabilities.sum() $ predicted_probs_first_measure.head()
from sklearn.neighbors import KNeighborsClassifier
for i in [203106, 203490, 201985, 201228, 200839, 101236]: $     print p_stats[p_stats.PLAYER_ID==i].PLAYER_NAME.unique()
! unzip ./data/train.zip
print 'See correlation with actual: ',test_case.select('street_name').take(1) $ actual_acct_id.select('street_name').distinct().show(10,False)
naimp.data_isna.corr()
cityID = 'af2a75dbeb10500' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Lincoln.append(tweet) 
print(f"Loaded {len(tw)} of {bininfo['number-selected-tweets']} tweets.")
df = pd.read_csv('ab_data.csv') $ df.head()
ideas.isnull().sum() # Checking for missing data
df.groupby(['episode_id'])['location_id'].shift().head()
ndays = 360 $ ntraj=10 $ dates=pd.date_range('20170101',periods=ndays) $ simret = pd.DataFrame(sigma*np.random.randn(ndays,ntraj)+mu,index=dates) $ simret
autos.columns = final_new_columns_names
final_topbikes.groupby(by=final_topbikes.index.hour)['Distance'].median().plot(kind='bar', figsize=(12,6))
journalists_mention_df.head()
df_usa=df_usa.pivot(columns='Variable Name',values='Value')  $ df_usa.head()
coefs.loc['age', :]
np.argmax(sp_rec)
conn = sqlite3.connect(os.path.join(outputs,'example.db')) $ df = pd.read_sql_query("SELECT * from stocks ORDER BY price", conn) $ print(df.head()) $ conn.close()
print(public_tweets[4].source_url) $ print(public_tweets[4].geo)
tweets_df_clean.columns
df.describe() # will describe the only numerical variable present in the dataset
ls_other_columns = df_with_metac_with_onc.loc[:, ls_both].columns
plt.scatter(df.Longitude, df.Latitude) $ plt.xlabel('Longitude') $ plt.ylabel('Latitude') $ plt.show()
data.year.fillna(2013, inplace=True) $ data
total_base_dict = base_dict.copy()
hour.apply(pd.Timestamp('2014-01-01 23:00'))
dfLikes.head()
bg3.columns = (['TBD1', 'Date', 'TBD2', 'BG']) # set new column headers $ bg3.columns
movie_ratings.describe().loc[['min', 'max'], ['imdb', 'metascore']]
df.isnull()
tweet_list = api.search(q='#%23mallorca', count = 1000) #tweet_list = api.search(q='#%23datascience', count = 100) $ len(tweet_list)
list(df2.columns)
df_prep11 = df_prep(df11) $ df_prep11_ = pd.DataFrame({'date':df_prep11.index, 'values':df_prep11.values}, index=pd.to_datetime(df_prep11.index))
FREEVIEW.plot_number_of_fixations(raw_fix_count_df, option=None)
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK','US']] $ df_new.head()
Quandl_DF['Month'] = Quandl_DF['Date'].dt.month $ Quandl_DF['Year'] = Quandl_DF['Date'].dt.year $ Quandl_DF['WeekNo'] = Quandl_DF['Date'].dt.week
def get_integer5(s): $     return brand_list.index(s)
autos["odometer_km"].value_counts().sort_index(ascending=True)
r.agg([np.sum,np.mean])
for col in totals_columns: $     games[col+'_HOME'].hist(label=col, alpha=0.25) $ plt.legend()
a=np.random.normal(0,50,size=(5,20)) $ plt.plot(a) $ plt.show()
plt.scatter(x,y, color = 'lightgreen') $ plt.plot(x, np.dot(x2, model2.coef_) + model2.intercept_, color = 'midnightblue', linewidth = '5.0')
weather['dateShort'] = pd.to_datetime(weather['EST']) $ print weather.ix[:, 'dateShort'].head()
weather_data2.columns = weather_data1.columns.values; weather_data2.head()
print("Predicted classes:", y_pred) $ print("Actual classes:   ", mnist.test.labels[:20])
rec_budg = budg[budg['Department Code'] == 'REC'] $ rec_budg.groupby(['Fiscal Year'])[['Amount']].sum()
print('Before deleting out of bounds game rows:',injury_df.shape) $ injury_df = injury_df[(injury_df['Date'] > '2000-03-28') & (injury_df['Date'] < '2016-10-03')] $ print('After deleting out of bounds game rows:',injury_df.shape)
url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ r = requests.get(url) $
score = pd.DataFrame(y_test) $ score['pred_prob_rf'] = y_pred_rf $ score['pred_prob_lr'] = y_pred_lr $ score['pred_rf'] = np.where(score['pred_prob_rf']>=.25, 1, 0) $ score['pred_lr'] = np.where(score['pred_prob_lr']>=.25, 1, 0)
from urllib import FancyURLopener $ class MyOpener(FancyURLopener): $     version = 'My new User-Agent' $ MyOpener.version
df_new['intercept'] = 1 $ log_mod_con = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'US', 'UK']]) $ results = log_mod_con.fit() $ results.summary()
p4_result = p1_table.sort_values('Profit', ascending=True) $ p4_result.head()
df['logViewsPercentChange'].hist()
df_usage_test = pd.read_csv("test_usage_data.csv") $ users_usage_summaries_test = pd.pivot_table(df_usage_test[['id', 'feature_name']], index=['id'], columns=['feature_name'], aggfunc=len, fill_value=0) $ accepted_rate_test = df_usage_test.groupby(['id'])['accepted'].mean().to_frame() $ users_usage_summaries_test = users_usage_summaries_test.join(accepted_rate_test, how='left') $ X_test = users_usage_summaries_test.values
imgp_clean.shape
plt.plot(x,fitted_pdf,"red",label="Fitted normal dist",linestyle="dashed", linewidth=2) $ plt.hist(sample,normed=1,color="cyan",alpha=.3) #alpha, from 0 (transparent) to 1 (opaque) $ plt.title("Normal distribution fitting") $ plt.legend() $ plt.show()
data['processing_time'] =  data['Closed Date'].subtract(data['Created Date'])
stemmer = porter.PorterStemmer() $ stemmerL = LancasterStemmer()
iter_visits = pd.read_csv(visits_path, iterator=True, chunksize=1000000) $ cols = ['id', 'id_partner', 'name', 'visits'] $ users_visits = Users.assign(visits=lambda x: 0) $ users_visits[cols] $ users_visits = count_visits(iter_visits, users_visits, 0)
total_df.head()
import os $ import sagemaker $ from sagemaker import get_execution_role $ sagemaker_session = sagemaker.Session() $ role = get_execution_role()
data2010.set_index('state', inplace=True) $ density = data2010['population'] / data2010['area (sq. mi)']
max(tweet_json['id_str'].value_counts())
grouped_by_dow_df.to_csv('grouped_by_day_of_week_df.csv',index=False)
df['lead_mgr_counts'] = df.groupby(['lead_mgr'])['lead_mgr'].transform('count') $
tub=tub.drop(['Scoops/Tub','Proportion','Scoop'],1).rename(columns={'level_6':'Scoop',0:'Flavour'}) $ tub
train['answer_dt'].describe()
df['language'] = [np.nan if l == 'C' else l for l in df.language]
image_predictions.info()
re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', cfd_index['pr'][13])
offset = pd.tseries.offsets.BMonthEnd()
concat.groupby('store').size()
scores.iloc[0]
focount.columns = ['name', 'weight']
temps_df.iloc[1].index
brand_list = list(set(train_data.brand))
gender_dummies=pd.get_dummies(df.gender,prefix='gender').iloc[:,0:2] $ gender_dummies
for i in range(0, 10000): $     dfs_quantile[i] = dfs_morning.ENTRIES_MORNING.quantile(i/10000) $ plt.plot(range(0,10000), dfs_quantile) $ plt.axis([9900, 10000, None, None])
! rm -rf ~/s3/comb/flight_v1_0.pq
train['hour'] = train.created_at.dt.hour $ train.groupby('hour').popular.mean().plot()
print('The Jupyter notebook stores information in the "Kernel".\ $       \nRestart the Kernel to clear noteook memory.')
cnx = mysql.connector.connect(user=cred['username'], password=cred['password_indiana'], $                               host=cred['host_indiana'],port='6612', $                               database='')
import pandas as pd $ date = pd.to_datetime("4th of July, 2015") $ date
adjectives.most_common(10)  # most frequent adjectives
Test.SetFlowStats(Ch1State=True, Ch2State=True, Ch3State=True)
austin[['distance_travelled', 'miles', 'driver_rating', 'rider_rating', 'total_fare', 'base_fare', 'tip']].describe()
df.query('group != "treatment" and landing_page == "new_page"') $ df.query('group == "treatment" and landing_page != "new_page"') $
unique_desc=class_merged_hol['description'].unique() $ print (unique_desc)
df_goog['Date'].unique()
print('Largest change (based on closing prices) =', '{:.2f}'.format(max(diff_close))) $ print('Largest absolute change (based on closing prices) =', '{:.2f}'.format(max(map(abs,diff_close))))
autos["price"].value_counts().sort_index(ascending=True).head(80)
df_modified.groupby("cancelled")[["modified", "modified_time"]].mean()
dfbrexits = dfstationmeanbreak[['exits','STATION']] $ dfbrexits.nlargest(10, ['exits'])
tweet_df.info()
data.head()
plt.rcParams['figure.figsize'] = [16,4] $ plt.plot(pd.to_datetime(mydf3.datetime),mydf3.fuelVoltage, 'g.', markersize = 2); $ plt.xlim(datetime.datetime(2017,11,15),datetime.datetime(2018,3,28))
df = df.query('breed != "Not identified"')
print('Under the null hypothesis p_new is the same conversion rate regardless of the page: {}.'.format(conversion_rate_all_pages))
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
df[pd.isnull(df.insert_id)].head()
dic_mars_cerberus = { $     "title": "Cerberus Hemisphere", $     "img_url": img_url_cerberus $ } $ hemisphere_image_urls.append(dic_mars_cerberus)
for col in [col for col in full.columns if full[col].dtype == 'object' and 'Patient' not in col]: $     dummies = pd.get_dummies(full[col],prefix=col) $     full.drop(col,axis=1,inplace=True) $     full = pd.concat([full,dummies],axis=1)    
plot_ARIMA_model_save_fig(data=dr_num_existing_patients, order=(2,1,1), start=start_date,\ $                 end=end_pred, title='Dr number existing patients AR2/MA1', xlabel='Time', ylabel='Number of New Patients',\ $                               figname='./images/dr_number_existing_patients_AR[2]IMA[1].png')
smpl_join_r = smpl_join.select(*(F.col(x).alias(x + '_orig') for x in smpl_join.columns)) $ join_a = smpl_join_r.join(contest_savm,F.col('postal_code_orig')==F.col('postal_code')).where(F.col('decision_date_time_orig')<=F.col('start_date'))
autos['price'].head()
xml_in['authorName'].shape
df2.drop(df2[df2.duplicated('user_id')==True].index,inplace=True) $ df2.shape
sorted(users.age.unique())
sns.heatmap(data.isnull().T)
g.get_group('106.152.115.161').head(10)
df_apps[df_apps.package_name.str.contains('com.facebook.katana')].count()
tablename='postshells' $ pd.read_csv(read_inserted_table(dumpfile, tablename), $             delimiter=",", $             error_bad_lines=False).head(10)
kick_projects = pd.merge(kick_projects, ks_ppb, on = ['category', 'launched_year','goal_cat_perc'], how = 'left')
tobs_stn_281_df = pd.DataFrame.from_records(tobs_stn_281, columns=('Date','Station','Tobs')) $ tobs_stn_281_df.head()
s_tobs_cnt = session.query(weather.station, func.count(weather.tobs)).\ $     filter(weather.date.between('2015-01-01','2015-12-31')).\ $     group_by(weather.station).order_by(func.count(weather.tobs).desc()).all() $ s_tobs_cnt
old_page_converted = np.random.choice([0,1], n_old, p=[conversion_rate_all_pages, 1-conversion_rate_all_pages])
import pickle $ with open('house_regressor.pkl', 'wb') as f: $     pickle.dump(automl, f) $
ab_file.drop(ab_file.query("group == 'treatment' and landing_page == 'old_page'").index, inplace=True) $ ab_file.drop(ab_file.query("group == 'control' and landing_page == 'new_page'").index, inplace=True)
f_app_hour_clicks.show(1)
au.save_df(df_city, 'data/city-util/proc/city') $ au.save_df(df_util, 'data/city-util/proc/utility') $ au.save_df(misc_info, 'data/city-util/proc/misc_info')  # this routine works with Pandas Series as well
van15_fin.head()
print('Predicted bin = ' + str(rf.predict(new_df)[0]))
my_data = pd.read_csv("drug200.csv", delimiter=",") $ my_data[0:5]
data_archie[data_archie['cur_sell_price'] == 0.0].head()
stocks.isnull().sum() # Checking for missing data
plt.figure(figsize=(14,12)) $ plt.title("Correlation of Features", y=1, size=15) $ sns.heatmap(data.astype(float).corr().abs(), linewidths=0.1,vmax=0.2, square=True, cmap=plt.cm.RdBu, linecolor="white", annot=True)
for bike in Del_list: $     id_list.remove(bike)
print('This is a code cell.')
def formatEmails(emails): $     escaped_emails = [email.replace("'", "''") for email in emails] $     email_string = ", ".join(["'" + email + "'" for email in escaped_emails]) $     return email_string
total_users_control = df2.query('group == "control"')['user_id'].nunique() $ user_control_conv = df2.query('group == "control" & converted == 1')['user_id'].nunique() / total_users_control $ user_control_conv
tt1.head()
investors_df = investors[investors.investor_type.isin(accepted_types)].copy()
apple["Signal"].plot(ylim =(-2, 2))
from scipy.stats import norm $ print(norm.cdf(z_score)) # significance of our z-score $ print(norm.ppf(1-(0.05))) # critical value at 95% confidence for one-tail test
store_items = store_items.set_index('pants') $ store_items
for country in list(df_joined['country'].unique()): $     df_joined['it_'+country] = df_joined.apply(lambda row: row['ab_page']*row[country], axis=1) $ df_joined.head()
df2.head(2)
data = fat.add_ema_columns(data, 'Close', [3, 50])
rf_yhat = rf.predict(X_test)
new .head(5)
ts = dfs[name][dfs[name].index > startDate][['v_sentiment','f_sentiment']] $ ts_count = ts['v_sentiment'].resample('D', how='count').rename('count').resample(sampling, how='mean')
carrierDF.head()
plt.figure(figsize=(15, 7)) $ df.following.hist(log=True, bins=80);
tNormal = len(df[ (df['bmi'] > 18.5) & (df['bmi'] < 25.0) ]) $ tNormal
tweetsIRMA = pd.read_sql("SELECT tc.tweet_id, i.DateTime, tc.text, tc.longitude as 'tweet_long', tc.latitude as 'tweet_lat', i.lon as 'irma_long', i.lat as 'irma_lat' FROM tweetIrmaTimes ti JOIN irmaFeatures i on ti.irmaTime = i.DateTime JOIN tweetCoords tc on tc.tweet_id = ti.tweet_id",Database().myDB)
convert = df[df['converted']==1].converted.count() $ convert $
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     current_mark = f.get_current_mark() $ current_mark
knn_reg.predict([x_test.loc[278100]])[0] > 0
df['time_elapsed'] = current_date - df['dateCreated'] $ display(df['time_elapsed'].head())
precip_df = precip_df.sort_index(ascending = True) $ precip_df.head()
df = pd.read_sql('SELECT hotel_name, contact_email FROM hotel WHERE hotel_name=\'Hyatt Regency Sydney\'', con=conn_a) $ df
data_df.to_pickle('cleaned_data.pkl') $ fraud_df['class'].to_pickle('label.pkl')
ts3.index = pd.to_datetime(ts3.index) $ ts3
result_merged.summary2()
pd.DataFrame(df_comment[df_comment.authorName==u'The Zeus']['date'].value_counts()).head()
X_test_2 = input_data[input_data.year >= 2016][['year_day', 'ordered_weekday', 'max_temp', 'min_temp', 'rain', 'ice_pellets']].values #no real need to scale this for a random forest $ Y_cat_test = input_data[input_data.year >= 2016]['categories'] $ grid_search.score(X_test_2, Y_cat_test)
frame.apply(f, axis='columns')
join_e.show(5)
free_data.groupby(["sex"]).mean() $ free_data.groupby(["country", "sex"]).mean() $ free_data.groupby(["educ", "sex"]).mean()
df_first_published_at = df_events.copy()
entities.value_counts()[:10]
((df.group=='treatment') & (df.landing_page=='old_page')).sum()+ ((df.group=='control') & (df.landing_page=='new_page')).sum()
from tqdm import tqdm $ tqdm.pandas()
states_log = np.log(states) $ states_log.plot(kind='bar') $ plt.title('Log-Counts for States of All Kickstarter Campaigns');
cityID = 'bced47a0c99c71d0' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Durham.append(tweet) 
p_diffs = np.array(p_diffs) $ prop_pdiffs_greater = ((p_diffs > actual_diff).sum()) / len(p_diffs) $ print('The proportion of the p_diffs that are greater than the actual difference observed in ab_data.csv is {}'.format(prop_pdiffs_greater))
df.head()
results=lm.fit() $ results.summary()
p_diffs = np.array(p_diffs) $ p_diffs
pickup_demand = pickup_orders.groupby(['ceil_15min','pickup_cluster'], as_index=False).apply(my_agg).reset_index() $ dropoff_demand = dropoff_orders.groupby(['ceil_15min','dropoff_cluster'], as_index=False).apply(my_agg).reset_index() $ ride_demand = ride_orders.groupby(['ceil_15min','ride_cluster'], as_index=False).apply(my_agg).reset_index()
cryptos[ $     (cryptos.percent_change_7d > 25) & $     (cryptos.market_cap_usd > 200000000) $ ]
%%HTML $ <blockquote class="twitter-tweet" data-lang="en"><p lang="und" dir="ltr">Hey <a href="https://twitter.com/HillaryClinton?ref_src=twsrc%5Etfw">@HillaryClinton</a> <a href="https://twitter.com/hashtag/ReleaseTheMemo?src=hash&amp;ref_src=twsrc%5Etfw">#ReleaseTheMemo</a> <a href="https://twitter.com/hashtag/pleasedontsuicideanymore?src=hash&amp;ref_src=twsrc%5Etfw">#pleasedontsuicideanymore</a><a href="https://twitter.com/hashtag/Obamagate?src=hash&amp;ref_src=twsrc%5Etfw">#Obamagate</a> <a href="https://twitter.com/hashtag/maga?src=hash&amp;ref_src=twsrc%5Etfw">#maga</a><a href="https://twitter.com/hashtag/SchumerShutdown?src=hash&amp;ref_src=twsrc%5Etfw">#SchumerShutdown</a> <a href="https://twitter.com/hashtag/LockHerUp?src=hash&amp;ref_src=twsrc%5Etfw">#LockHerUp</a> <a href="https://twitter.com/hashtag/LockThemAllUp?src=hash&amp;ref_src=twsrc%5Etfw">#LockThemAllUp</a> <a href="https://twitter.com/hashtag/Gitmo?src=hash&amp;ref_src=twsrc%5Etfw">#Gitmo</a></p>&mdash; Q.UIETMUMBLES (@quietmumbles) <a href="https://twitter.com/quietmumbles/status/954503731852267521?ref_src=twsrc%5Etfw">January 19, 2018</a></blockquote> $ <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
eval_RGF_tfidf_tts = clf_RGF_tfidf.score(X_testcv_tfidf, y_testcv_tfidf) $ print(eval_RGF_tfidf_tts)
print("Number of unique users : " + str(df.user_id.nunique()))
df1=df[df.Trip_distance>=100] $ df1.loc[:,['Trip_distance','Trip_duration']]
tweet_model = models.Word2Vec(sentences=tweet_hour['tweet_text'].apply(tweet_tokenizer.tokenize))
f = {'total_cost':['sum','count'], 'date_of_birth':['first']} $ total_spending = df.groupby(['uid'])['total_cost','date_of_birth'].agg(f) $
fig, ax = plt.subplots(figsize=(15.0, 10.0)) $ actual_train_results_df = pd.DataFrame(actual_train_results) $ ax = sns.distplot(actual_train_results_df["prediction_status"]) $ ax.get_figure().text(0.90, 0.01, "AntiNex - v1", va="bottom", fontsize=8, color="#888888") $ plt.show()
not_in_misk = data_science_immersive_df.merge(dsi_me_1_df, how='left', on='name')
p_converted2 = df2['converted'].mean() $ print('The probability of an individual converting: ', p_converted2)
import random $ def create_dataset(hm,variance,step=2,correlation=False): $     return np.array(xs, dtype=np.float64),np.array(ys,dtype=np.float64)
pd.read_csv("data.csv", index_col=[0, 1, 2, 3, 4, 5], skipinitialspace=True, parse_dates=['Date']).head(3)
import time $ ctime = obj['openTime']/1000 $ time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ctime))
two_digits = lambda x: '%.2f' % x $ perc_df.applymap(two_digits).head()
df_daily = df_daily.reset_index() $ df_daily.drop('SCP', axis=1, inplace=True) $ df_daily.head()
ps = ts.to_period()
df.shape
biased_train_33 = modern_train.drop(modern_train[modern_train['country_destination'] == 'NDF'].sample(frac=0.33).index) $ biased_train_60 = modern_train.drop(modern_train[modern_train['country_destination'] == 'NDF'].sample(frac=0.60).index) $ biased_train_15 = modern_train.drop(modern_train[modern_train['country_destination'] == 'NDF'].sample(frac=0.15).index) $ biased_train_05 = modern_train.drop(modern_train[modern_train['country_destination'] == 'NDF'].sample(frac=0.05).index) $
pop.index=['DK', 'DE', 'NL']
datatest = datatest.reset_index(drop = True)
sagemaker.Session().delete_endpoint(rcf_inference.endpoint)
pipe_lr = make_pipeline(cvec, lr) $ pipe_lr.fit(X_train, y_train) $ pipe_lr.score(X_test, y_test)
df = pd.read_sql('SELECT p.customer_id, p.amount FROM payment p WHERE p.amount > 4.2006673312979002;', con=conn) $ df $
np.random.normal(size=(2,3))
dfRegMet =pd.read_pickle(data + "dfRegMet.p")
df6 = df4.where( (hours > 10) & (hours < 13)) # show lunch data rows only $ df6 = df6[df6['BG'].notnull()] $ df6 # got same data as previous technique
stat_data['job_hopings'] = jh_data['employer'].mean() $ stat_data
class_merged=class_merged[class_merged.date>'2014-03-31']
logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','US','UK']]) $ result = logit_mod.fit() $ result.summary()
import datetime $ def time_to_date(row): $     return datetime.datetime.fromtimestamp(row['deadline']).strftime('%Y-%m-%d %H:%M:%S') $ df_us_['deadline_dt'] = df_us_.apply(time_to_date, axis =1)
station_cnts_asc = session.query(Measurement.station,func.count(Measurement.station).label("scount")).\ $                     group_by(Measurement.station).\ $                     order_by("scount").all() $ station_cnts_asc          
at_mentions_pattern = re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-\.]))@([A-Za-z0-9_]+)')
titanic.groupby(['sex', 'class'])['survived'].mean().unstack()
combined_df3.Ward.value_counts()
gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_
len(df.user_name.unique())  # number of authors of these tweets
ps_old = [p_old, 1-p_old] $ old_page_converted = np.random.choice(a, size=n_old, p=ps_old)
train.head()
print('{} / {} '.format(data['Voluntary Soft-Story Retrofit'].notna().sum(), len(data['Voluntary Soft-Story Retrofit']))) $ print('{:.3f} % '.format(data['Voluntary Soft-Story Retrofit'].notna().sum() * 100 / len(data['Voluntary Soft-Story Retrofit']))) $ data[data['Voluntary Soft-Story Retrofit'].notna()]['Voluntary Soft-Story Retrofit']
from pylab import rcParams $ rcParams['figure.figsize'] = 10, 10 $ ax= df_a.plot(style='.-', markevery=5,figsize = (10,7))
now = dt.datetime.now() $ past_yr = now.replace(year=2017) $ results = session.query(Measurement.date, Measurement.prcp).\ $             filter(Measurement.date > past_yr).all() $
reddit_comments_data.show(10)
score = score.dropna() # drops null values in the score df and reassigns it to score
df_all_columns['datetime'] = pd.to_datetime(df_all_columns.Timestamp, unit='s') $ df = df_all_columns[['datetime','Volume_(BTC)','Weighted_Price']] $ df = df.rename(index=str, columns={"datetime": 'timestamp', 'Volume_(BTC)':'value', 'Weighted_Price':'price'}) $ print("row count: " + str(len(df.index))) $ df.head() $
grouped = data[['processing_time','Borough']].groupby('Borough') $ grouped.describe()
from repository.mlrepositoryclient import MLRepositoryClient $ from repository.mlrepositoryartifact import MLRepositoryArtifact $ from repository.mlrepository import MetaProps, MetaNames
post = {"author": "Renato", $         "text": "My first blog post!", $         "tags": ["mongodb", "python", "pymongo"], $         "date": datetime.datetime.utcnow()}
df_predictions.sample(15)
dreamophone_url = 'https://dreamophone.com/dream/2186'
df['From Country'].unique()  # From country is also constant so let's also delete this
prop = props[props.prop_name == "PROPOSITION 062- DEATH PENALTY. INITIATIVE STATUTE."]
for f in zip(X_.columns, rfecv.support_): $     print(f)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_token_secret) $
gbdt_grid.fit(X_train, y_train) $
df_ad_airings_5 = pd.read_pickle('./TV_AD_AIRINGS_FILTER_DATASET_3.pkl')
actual_holidays=holidays_events[(holidays_events.transferred==False)] $ print("Rows and columns:",actual_holidays.shape) $ pd.DataFrame.head(actual_holidays)
df2[df2.duplicated(subset = ['user_id']) == True]['user_id']
pd.read_csv('../data/ebola/guinea_data/2014-09-02.csv').head()
df2[df2['user_id'].duplicated()]['user_id']
df["DATE_TIME"] = pd.to_datetime(df.DATE + " " + df.TIME, format = "%m/%d/%Y %H:%M:%S") $ df
import locale $ print('Current locale: {}'.format(locale.getdefaultlocale())) $ locale.setlocale(locale.LC_TIME, '') $ time_string = time.strftime('%A %d %b %Y', time.localtime()) $ print(time_string.encode('latin1').decode('cp1251')) $
clean_madrid['clean_text'].replace('', np.nan, inplace=True) $ print(clean_madrid['clean_text'].isna().sum()) $ df_madrid.iloc[clean_madrid[clean_madrid.isna().any(axis=1)].index,:].head()
newmod = X_dunno $ newmod['y_pred'] = yhat_summore $ topten = newmod.sort_values('y_pred', ascending=False).head(10) $ topten.loc[:, (topten != 0).any(axis=0)] $
depthdf.set_index('max_depth')['score'].plot(figsize=(16,8))
lr_model_saga = LogisticRegression(C=0.1, class_weight=None, max_iter=125, solver='saga')
Final_question_df.describe()
for n in range(20): $     ypred = model.predict(np.random.random((1, 95,1))) $     int_to_char = {v:k for k,v in char_to_int.items()} $     results = int_to_char[np.argmax(ypred)] $     print(np.argmax(ypred),results)
donors_c = donors.copy()
import datetime $ day = lambda x: x.split(' ')[0].replace('-',',') $ emotion_big_df['date']=emotion_big_df['createdAt'].apply(day)
pets = pd.read_csv('pets.csv', index_col = 0) $ pets
_ = ok.grade('q05a') $ _ = ok.backup()
html = browser.html $ soup = bs(html, 'html.parser') $ weathers = soup.find_all('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text') $ weather = weathers[0].text $ print(weather) $
for key in sorted(eligible_by_week.keys()): $     print("Week age {0}: {1} authors ".format(key, len(eligible_by_week[key])))
import keras $ from keras import regularizers, optimizers $ from keras.models import Sequential $ from keras.layers import Dense,Dropout $ from keras.callbacks import EarlyStopping, Callback
from scipy.stats import norm $ norm.cdf(z_score) $ norm.ppf(1-(0.05/2)) $
ved['season'] = ved.index.str.split('.').str[0] $ ved['term'] = ved.index.str.split('.').str[1]
nba_df.info()
autos["price"].value_counts().sort_index(ascending=True)
tubes = results[results['SetId'].isin(setup['SetId']).tolist()]
rawData = image_predictions.content $ image_predictions_df = pd.read_csv(io.StringIO(rawData.decode('utf-8')), sep='\t')
session.query(Measurement.station, func.count(Measurement.station)).group_by(Measurement.station).order_by(func.count(Measurement.station).desc()).all() $
planevisits_df.to_excel('temp_planevisits_df1.xlsx', index=False)
trgt_aux=[] $ for i in range(0,l2): $     if i not in seq: $         trgt_aux.append(target[i]) $ Y=np.array(trgt_aux)
autos["price"].value_counts().head(20) #Print the number count of each price point
df.info()
mlp_df.plot(figsize=(10, 10)) $ plt.show()
num_unique_users = df.nunique()['user_id'] $ print("There are {} many unique users ".format(num_unique_users))
df.to_csv('tab_delim_clean.csv', sep='\t')
lr = LogisticRegression() $ lr.fit(X, y)
df2.query('group == "control"').converted.sum()/df2.query('group == "control"')['user_id'].count()
num_df = pd.get_dummies(fraud_df[columns_convert],prefix = columns_convert) $ columns_num = ['delay_time','purchase_value','age','device_id_count','ip_address_count','signup_hour','signup_week','purchase_hour','purchase_week'] $ data_df = fraud_df[columns_num].join(num_df) $ feature_columns = columns_num + num_df.columns.tolist() $ target_column = 'class'
tweet_json_df.info()
ax=df_series.plot(title="Original Data") $ ax.set_ylabel("Value") $ plt.show()
client.deployments.list()
nfl['Date'] = pd.to_datetime(nfl['Date'])
temp_series_freq_15min = temp_series.resample("15Min").mean() $ temp_series_freq_15min.head(n=10) # `head` displays the first n values
clf.score(X_tr[0:len(X_train)-40-1],y_ls)
datacamp['publishyymm'] = datacamp['publishdate'].dt.strftime("%Y-%b") $ datacamp["posts"] = 1
p = subprocess.Popen('ls', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) $ for line in p.stdout.readlines(): $     print(line) $ retval = p.wait()
data['love'].shape
twitter_df_clean.stage_name.isnull().sum()
tables = [pd.read_csv(f'{PATH}{fname}.csv', low_memory=False) for fname in table_names]
df_merged = pd.merge(df_merged, pred_clean, on='tweet_id', how='inner') $ df_merged.info()
autos['registration_year'].describe()
plot_autocorrelation(series=dr_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr hours new patients')) $ plot_autocorrelation(series=dr_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of dr hours existing patients'))
week35 = week34.rename(columns={245:'245'}) $ stocks = stocks.rename(columns={'Week 34':'Week 35','238':'245'}) $ week35 = pd.merge(stocks,week35,on=['245','Tickers']) $ week35.drop_duplicates(subset='Link',inplace=True)
session.query(measurement.date).\ $     filter(measurement.station == 'USC00519281').\ $     order_by(measurement.date).first()
null_reg = creations[ creations["user_registration"].isnull() ] $ null_reg
data = pd.DataFrame(data=[tweet.text for tweet in public_tweets], columns=['Tweets']) $ display(data.head(10))
g_live_kd915_filt_df = live_kd915_filt_df.groupby(['blurb']) $ live_woutcome = g_live_kd915_filt_df.filter(lambda x: len(x) > 1).sort_values(['blurb','launched_at']) $ live_woutcome[['name','category_name','blurb','blurb_count','goal_USD','backers_count','launched_at',\ $                'state_changed_at','days_to_change','state']]
noHandReliableData = reliableData[reliableData['ID'] != 'Hand'] $ noHandReliableData['ID'].unique()
age.loc[['Alice', 'Bob']]
tweet_ids_twitter_archive = twitter_archive['tweet_id'].unique() $ tweet_ids_image_predictions = image_predictions['tweet_id'].unique()
plt.bar([1,2,3,4,5,6,7,8,9,10,11,12],monthly_residuals) $ plt.xlabel('Month of 2017') $ plt.ylabel('Monthly Residuals')
train.head()
df.index = df['created_date'] $ df.head(2)
df_cat.head()
df_list_rand = pd.read_csv(master_folder + lists +  "unsubscribed/" + files_lists[0] ,\ $                   low_memory=False)
table_rows = driver.find_elements_by_tag_name("tbody")[12].find_elements_by_tag_name("tr") $
data.shape
merge.head(20)
monte.str.len()
import requests $ import collections $ mydata = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=8F4u-pRdtQsDCn-fuBZh")
station_obs_df = pd.DataFrame(sq.station_obs(), columns = ["Station name", "Observation counts"]) $ station_obs_df
sklearn.metrics.mean_absolute_error(y_true= y_test,y_pred=lin_pred)
from sklearn.metrics import mean_absolute_error, mean_squared_error $ from math import sqrt $ from pysumma.Validation import validation
[print('{0} importance = {1:.2f}'.format(X_train.columns.tolist()[x], XGBClassifier.feature_importances_[x])) for x in range(len(X_train.columns))]
new_page_converted = np.random.binomial(n_new, p_new, 10000)/n_new $ old_page_converted = np.random.binomial(n_old, p_old, 10000)/n_old $ p_diffs = new_page_converted - old_page_converted $
twitter_df.to_csv('../Data_sources/twitter_df.csv')
rank = cc.groupby(['ranknow', 'slug'])['market'].mean() $ rank
cityID = '60e2c37980197297' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         St_Paul.append(tweet) 
ensemble_2 = api.create_ensemble(train_dataset, { \ $     "balance_objective" : True, $     "excluded_fields": ["DEP_HOUR","DEP_TIME","DEP_DELAY"]}) $ api.ok(ensemble_2)
x_tr.shape
print("dfWeek = ",dfWeek['Contract Value (Weekly)'].sum(), "dfWeek Project Count = ", dfWeek['Project Name'].nunique()) $ print("dfDay = ",dfDay['Contract Value (Daily)'].sum(), "dfDay Project Count = ", dfDay['Project Name'].nunique())
s = pd.Series([1, 2], index=["1", "2"]) $ print(s["1"])  # matches index type; use explicit $ print(s[1])  # integer doesn't match index type; use implicit positional
test_data['Survived'] = 1.23
from scipy.stats import norm $ norm.cdf(z_score), norm.ppf(1 - 0.05)
allVars = read.getVariables() $ variables_df = pd.DataFrame.from_records([vars(variable) for variable in allVars], index='VariableID') $ relatedSite = read.getRelatedSamplingFeatures(sfid=26, relationshiptype='Was Collected at')[0]
df['lead_mgr'].head()
X2 = PCA(2, svd_solver='full').fit_transform(X)
k = 7 $ neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train_knn,y_train_knn) $ neigh
serieschallenge5 = data.groupby(['KEY2','DATE'])['EntriesDifference'].agg(pd.np.sum) $
n_topics = 10 $ nmf = NMF(n_components=n_topics, random_state=0, init="nndsvd") $ nmf = nmf.fit(tfidf)
tweets.favorite_count
all_df_name='new_full_df.pkl' $ save_n_load_df(all_df, all_df_name)
list(zip(df.columns, [type(x) for x in df.ix[0,:]]))
print(np.min(temp), np.mean(temp), np.max(temp))
first_row = session.query(Measurement).first() $ first_row.__dict__.get('prcp') $ one_year_prcp = session.query(Measurement.station, Measurement.date, Measurement.prcp, Measurement.tobs)\ $ .filter(Measurement.date >= '2016-08-23').all() $ one_year_prcp[0]
import test_package.print_hello_function_container $ test_package.print_hello_function_container.print_hello_function()
recipes.head()
con.call_type.unique()
plt.figure() $ plt.hist(temp_hist_data['tobs'], bins=12)
m2 = m[:,1:3].reshape(5,2) $ print("m2: ", m2)
All_tweet_data=pd.merge(twitter_data_v2, tweet_data_v2, on='tweet_id') $ All_tweet_data.shape
url_payouts = grouped['total_payout_value'].agg({'total_payout': 'sum', 'avg_payout': 'mean'})
repeated = df2[df2.user_id.duplicated()] $ print("The repeated 'user_id' in df2 is:\n {}".format(repeated))
df2 = df2[df2.timestamp != '2017-01-14 02:55:59.590927']
train = pd.merge(train, hpg_reserve, how='left', on=['air_store_id','visit_date']) $ test = pd.merge(test, hpg_reserve, how='left', on=['air_store_id','visit_date'])
df = pd.merge(train.to_frame(), ccl["rise_in_next_week"].to_frame(), left_index=True, right_index=True)
combined_df2.columns
minval = data[["TMIN"]].min().values[0] $ maxval = data[["TMAX"]].max().values[0] $ minval, maxval
daily = data.set_index('created_at').resample('D').size() $ monthly_mean = daily.resample('M').mean() $ monthly_mean.index = monthly_mean.index.strftime('%Y/%m')
X = X.apply(lambda x: x.fillna(x.median()),axis=0)
df_clean[['retweeted_status_id', 'retweeted_status_user_id', $          'retweeted_status_timestamp']].notnull().sum()
int_ab_page = 1/np.exp(-0.0150) $ int_ab_page
frame3 = pd.DataFrame(pop)
log_mod = sm.Logit(df2['converted'],df2[['intercept' ,'ab_page']]) $ results = log_mod.fit()
print(autos['price'].max()) $ print(autos['price'].min())
next(iter(md.trn_dl))
a.capitalize()
df_arch_clean.info() $
df_master.shape
sim_val = new_page_converted/n_new - old_page_converted/n_old $ print('Simulated vaues are {}.'.format(sim_val)) $ print('Simulated vaues are {}.'.format(round(sim_val, 4)))
y_pred_class = lr.predict(X_test_dtm) $ metrics.accuracy_score(y_test, y_pred_class)
df1 = pd.read_csv('https://raw.githubusercontent.com/kjam/data-wrangling-pycon/master/data/berlin_weather_oldest.csv') $ df1.head(2)
regime_orig = apple.ix[-1, "Regime"] $ apple.ix[-1, "Regime"] = 0 $ apple["Signal"] = np.sign(apple["Regime"] - apple["Regime"].shift(1)) $ apple.ix[-1, "Regime"] = regime_orig $ apple.tail()
gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_
data = pd.merge(data, pickup_demand, left_on= ['floor_10min', 'pickup_cluster'], right_on=['ceil_10min', 'pickup_cluster'], how='left') $ data = pd.merge(data, dropoff_demand, left_on= ['floor_10min', 'dropoff_cluster'], right_on=['ceil_10min', 'dropoff_cluster'], how='left') $ data = pd.merge(data, ride_demand, left_on= ['floor_10min', 'ride_cluster'], right_on=['ceil_10min', 'ride_cluster'], how='left')
users['Registered']=pd.to_datetime(users['Registered']) $ users['Cancelled']=pd.to_datetime(users['Cancelled']) $ users
trip_arrive = dt.date(2018, 4, 1) $ trip_leave = dt.date(2018, 4, 15) $ last_year = dt.timedelta(days=365) $ temp_avg_lst_year = (calc_temps((trip_arrive-last_year), (trip_leave-last_year))) $ print(temp_avg_lst_year)
df_training.cache()
lr_scheduler_col = lr_sched.myCosineAnnealingLR(optimizer_col,879,cycle_mult=1.0) $ train_col.train_model(num_epochs=3,scheduler=lr_scheduler_col)
partitions = community.best_partition(Gun, weight='weight', partition=politiciansPartyDict) $ print community.modularity(partitions, Gun, weight='weight')
url='https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' $ url_api=url+API_KEY $ r = requests.get(url_api)
All_tweet_data_v2[['rating_denominator','rating_numerator']][All_tweet_data_v2.rating_denominator!=10]
bikedataframe[['count', 'predicted']].tail(30)
replace_name = dict() $ for i in [203106, 203490, 201985, 201228, 200839, 101236]: $     replace_name[i] = str(player_details[player_details.PLAYER_ID==i].DISPLAY_FIRST_LAST.unique()[0]) $ for i in [203106, 203490, 201985, 201228, 200839, 101236]: $     p_stats.loc[p_stats["PLAYER_ID"]==i, "PLAYER_NAME"] = replace_name[i]
df.to_csv('ab.csv',index=False) #creating a new dataset file
import statsmodels.api as sm $ convert_old = len(df2[((df2['landing_page'] == 'old_page') & (df2['converted'] == 1))]) $ convert_new = len(df2[((df2['landing_page'] == 'new_page') & (df2['converted'] == 1))]) $ n_old = len(df2[df2['landing_page']=='old_page']) $ n_new = len(df2[df2['landing_page']=='new_page'])
train_df.isnull().sum()
missings = messy.isnull() $ missings
submission_full['proba'].head()
price_data = price_data.iloc[1:]
3.2500
train[train.age>1000].age.value_counts()
df.fillna(method='ffill', axis=1) $
merged_portfolio_sp_latest = pd.merge(merged_portfolio_sp, sp_500_adj_close, left_on='Latest Date', right_on='Date') $ merged_portfolio_sp_latest.head()
fin_pivot_table_ni = pd.pivot_table(fin_df, values = 'netIncome', index = ['symbol'], aggfunc = np.mean) $ fin_pivot_table_ni = fin_pivot_table_ni.rename(index=str, columns={"netIncome": "Avg Net Income"}) $ fin_pivot_table_ni.sort_values(by = ['Avg Net Income'], ascending = False)
class_merged_hol.to_csv('class_merged_hol.csv',sep=',')
brand_counts = autos['brand'].value_counts(normalize=True) $ common_brands = brand_counts[brand_counts > .05].index $ print(common_brands)
Xs = pd.get_dummies(df.subreddit, drop_first = True)
df_ad_airings_6 = pd.read_pickle('./data/TV_AD_AIRINGS_STATE_METRO_AREA_5.pkl')
df2.user_id.value_counts().sort_values(ascending=False).head(1)
%matplotlib inline $ git_blame['knowing'].value_counts().plot.pie(label="") $ print(git_blame['knowing'].mean())
df2 = pd.read_csv('ab_data_mod.csv')
df_daily2 = df_daily.groupby(["C/A", "UNIT", "STATION", "DATE"]).DAILY_ENTRIES.sum().reset_index()
AAPL.iloc[:,0:4].plot()
taxi_hourly_df.loc[index_missin_hr0to6_before2016, "num_pickups"] = 0 $ taxi_hourly_df.loc[index_missin_hr0to6_before2016, "num_passengers"] = 0 $ taxi_hourly_df.loc[index_missin_hr0to6_before2016, "missing_dt"] = False
print("Rows: {}".format(train.shape[0])) $ print("Columns: {}".format(train.shape[1]))
logit_country = sm.Logit(merged['converted'],merged[['intercept','UK', 'US']]) $ result_merged = logit_country.fit()
s = np.array([3, 2, 4, 1, 5]) $ s[s > np.mean(s)]  # Get the values above the mean
dta.describe()
imgp_clean.tweet_id = imgp_clean.tweet_id.astype(str)
sudptable = data.pivot_table(index=["author"], values=["score", "ups", "downs"], aggfunc=sum).reset_index() $ sudptable.sort_values(inplace=True, by="score", ascending=False) $ sudptable[0:9]
def get_child_column_names(node): $     return [column.name for column in get_child_column_data(node)]
pmol.df.head(3)
xirr_actual['payout_quarter'].head()
r = requests.get('https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?start_date=2018-03-27&end_date=2018-03-27&api_key='+API_KEY)
text0 = textract.process(datapath2 / onlyfiles[0]).decode('utf-8') $ print(text0[0:1000])
data[['Sales']].resample('D').mean().head()
data.columns
df_birth.population = pd.to_numeric(df_birth.population.str.replace(',',''))
ltdate = pd.to_datetime(voters.LTDate.map(lambda x: x.replace(' 0:00', ''))) $ print(ltdate.describe()) $ ltdate.value_counts(dropna=False).head(5)
new_page_converted = np.random.choice([1, 0], size = n_new, p=[p_new, (1-p_new)]).mean() $ new_page_converted
df_weather.isnull().sum()
import statsmodels.api as sm $ from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ log_model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ result = log_model.fit()
tweets_df.describe()
kNN500x = KNeighborsClassifier(n_neighbors=500) $ kNN500x.fit(X_extra, y_extra)
(pd.Series(list_tables_with_insert(dumpfile)).value_counts()).sort_index()
temps_df = pd.DataFrame({'Missouri': temps1, $                          'Philadelphia':temps2}) $ temps_df
new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new $ old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old $ p_diffs = new_converted_simulation - old_converted_simulation
shows[shows.synopsis.isnull()]
consumer_key = os.environ.get('TWEEPY_API_KEY') $ consumer_secret = os.environ.get('TWEEPY_API_SECRET') $ access_key = os.environ.get('TWEEPY_ACCESS_TOKEN') $ access_secret = os.environ.get('TWEEPY_TOKEN_SECRET')
def send_sine_wave(t, lab): $     print(f'\rTime: {t} Last sleep: {labtime.lastsleep} ', end='') $     step = numpy.max(numpy.nonzero(switch_times <= t)) $     lab.Q1(Qbar + A*numpy.sin(freqs[step]*(t - switch_times[step])))
tip_sample.to_csv('text_prepartion/yelp_tips_prepared.csv')
us_cities = pd.read_csv("us_cities_states_counties.csv", sep="|") $ us_cities.head() $
%%bash $ sort temp/pheno.txt > temp/pheno_sort.txt $ cut -f 2- data/PPMI.pca.eigenvec | sort | join -t$'\t' temp/pheno_sort.txt -  > temp/pheno_PC5_temp.txt $ grep 'IID' temp/pheno_PC5_temp.txt > temp/pheno_PC5_colnames.txt $ grep -v 'IID' temp/pheno_PC5_temp.txt | cat temp/pheno_PC5_colnames.txt - > temp/pheno_PC5.txt
df3 = df3.assign(ROI=df3['Amount'] / df3['Costs'] - 1) $ df3.info()
print(All_tweet_data.shape) $ All_tweet_data.head()
y_size = fire_size['SIZECLASS'] $ x_climate = fire_size.loc[:, 'glon':'rhum_perc_lag12']
deaths=dropped.loc['Total deaths (confirmed + probables + suspects)'] $
group1 = df[((df['group'] == 'treatment') & (df['landing_page'] == 'old_page'))] $ group2 = df[((df['group'] == 'control') & (df['landing_page'] == 'new_page'))] $ print('"new_page" and "treatment" do not line up {} times.'.format(len(group1 + group2)))
df2_dummy.set_index('user_id', inplace=True)
samsung = tweets[tweets['samsung'] == True] $ samsung.head()
df['date'] = pd.to_datetime(df['date']) $ df.index = df['date'] $ del df['date'] $ df
print(df.shape) $ df.groupby('state').nunique()
from sklearn.preprocessing import MinMaxScaler $ y_scaler = MinMaxScaler() $ y_scaler.fit(train[['load']]) $ X_scaler = MinMaxScaler() $ train[['load', 'temp']] = X_scaler.fit_transform(train)
images.describe()
twitter_archive_clean=twitter_archive_clean[twitter_archive_clean.rating_numerator<15]
test1.columns
classif_varieties = set(ndf[y_col].unique()) $ label_map = {val: idx for idx, val in enumerate(ndf[y_col].unique())}
rank_meters['evening_17_0'].most_common(11)[1:]
ebola_melt['str_split'] = ebola_melt.type_country.str.split('_') $ ebola_melt.head()
offset1 = timedelta(hours=1) $ offset2 = timedelta(days=3) $ offset3 = timedelta(days= ((365*12)+3), hours=1, seconds=43) $ df_goog.index + offset3
display(trip_data.tail())
print('Best score for data:', svm_classifier.best_score_) $ print('Best C:',svm_classifier.best_estimator_.C) $ print('Best Kernel:',svm_classifier.best_estimator_.kernel) $ print('Best Gamma:',svm_classifier.best_estimator_.gamma)
X.drop('title', axis=1, inplace=True) $ X = pd.get_dummies(X, drop_first=True)
sessions.to_csv('sessions_cleaned.csv')
def f_loan_amt(these_accounts,pconn): $     df = pd.read_sql(query,pconn) $     return df
df2.query('converted == 1').count()['user_id']/df2.shape[0]
s.iloc[1]
(df['landing_page'] == "new_page").mean()
df = pd.DataFrame({'Char':chars,'Target':y_true})
eth.to_csv('ethh.csv', sep=',')
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
full = full.replace('@NA',np.NaN) # replace @NA $ full = full.replace('',np.NaN) # didn't find any empty strings $ full = full.replace(np.NaN,'')
tree_features_df['p_hash'].isin(manager.image_df['p_hash']).describe() $
model.add(Dropout(rate=0.25))
now = Time.now() $ print(now) $ print(now.mjd)
treatment_df2 = df2.query('group == "treatment"') $ treatment_conversion = treatment_df2['converted'].mean() $ treatment_conversion
token["month"] = token.date.map(lambda x : x.year*12 + x.month)
rfr.fit(data[['expenses', 'floor', 'lat', 'lon', 'property_type', \ $               'rooms', 'surface_covered_in_m2', 'surface_total_in_m2']], \ $         data[['price_aprox_usd']].values.ravel())
df_protest.loc[176].to_frame()
autos['date_crawled'].str[:10].describe()
tcp.columns
df.loc[~df["ORIGINE_INCIDENT"].isin(origines), "ORIGINE_INCIDENT"] = "NAN"
pd.get_dummies(cust_demo['Martial_Status'], prefix="D").head(10)
r.json()['dataset_data']
df.eval('D = (A + B) / C', inplace=True) $ df.head()
allqueryDF.loc[0]['last_changed']
df3 = df2 $ df3['intercept'] = 1 $ df3['ab_page'] = pd.get_dummies(df['group'])['treatment'] $ df3.head()
df_merged.describe()
from sklearn.linear_model import LogisticRegression $ log_model = LogisticRegression() $ log_model = log_model.fit(X=X_train, y=y_train) $ y_pred = log_model.predict(X_test) $ print(classification_report(y_test, y_pred)) $
!du -h html_data.csv
climate_df = pd.DataFrame(climate_data, columns=['date', 'Precipitation']) $ climate_df = climate_df.sort_values('date', ascending=True) $ climate_df.set_index('date', inplace=True) $ climate_df.head()
table_rows = driver.find_elements_by_tag_name("tbody")[20].find_elements_by_tag_name("tr") $
x =  store_items.isnull().sum().sum() $ print('Number of NaN values in our DataFrame:', x)
dfjson = pd.read_json('quoteresult.json') $ dfjson
print(age.min()) $ print(age.max())
%%time $ tokenized_tweets = [] $ for tweet in df_train: $     tokenized = regexp_tok.tokenize(tweet) $     tokenized_tweets.append(tokenized)
parks_score_PSA = parks_sc.groupby(['PSA','FQ'])[['Score']].mean() $ parks_score_quar = parks_sc.groupby(['FQ','PSA'])[['Score']].mean() $ parks_score_quar.plot(kind='hist', bins=30);
popCon[popCon.content == 'video'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
df2['intercept'] = 1 $ df2[['old_page', 'new_page']] = pd.get_dummies(df2['landing_page']) $ df2.head() # I needed two passages to create the ab_page, although I see it's just what I labeled old_page
X_train = mnist.train.images $ X_test = mnist.test.images $ y_train = mnist.train.labels.astype("int") $ y_test = mnist.test.labels.astype("int")
gc.collect()
import random $ metric_series_type_id = random.choice(metric_series_type_ids) $ url = form_url(f'metricTypes/{metric_series_type_id}') $ response = requests.get(url, headers=headers) $ print_body(response, max_array_components=3)
TensorBoard().stop(23002) $ print 'stopped TensorBoard' $ TensorBoard().list()
sns.heatmap(X.corr()) $ plt.show()
tmp = tmp[(tmp['longitude']<-73.95)&(tmp['longitude']>-74.05)&(tmp['latitude']<41)&(tmp['latitude']>40.7)]
df.resample('A').mean()
devices_df['state'].unique()
loud_parties = df[df['Descriptor'] == 'Loud Music/Party'] $ loud_parties.groupby(loud_parties.index.hour)['Created Date'].count().plot()
plt.show()                                                                   # displays plot
imdb_df = pd.read_html("http://www.imdb.com/chart/top?ref_=nv_wl_img_3")[0] $ imdb_df.head()
twitter_archive_enhanced_clean.to_csv('twitter_archive_master.csv') $ tweets_clean.to_csv('tweets_stats.csv')
preds_test.head()
old_page_converted = np.random.choice([0,1],size=n_old,p= [1-p_old,p_old])
d.groupby(d['pasttweets'].str.len())['tweet_id'].count()
nypd_unspecified = data[(data['Borough']=='Unspecified')& $                        (data['Agency']=='NYPD')]['Borough'].count() $ nypd_unspecified
td_alpha = td ** (1/3) $ td_alpha = td_alpha / td_alpha.max().max()
print("Number of rows in slice", reviewsDFslice.shape)
from shapely.geometry import Point $ data3['geometry'] = data3.apply(lambda x: Point((float(x.lon), float(x.lat))), axis=1)
train.tail()
bacteria_data = pd.DataFrame({'value':[632, 1638, 569, 115, 433, 1130, 754, 555], $                      'patient':[1, 1, 1, 1, 2, 2, 2, 2], $                      'phylum':['Firmicutes', 'Proteobacteria', 'Actinobacteria', $     'Bacteroidetes', 'Firmicutes', 'Proteobacteria', 'Actinobacteria', 'Bacteroidetes']}) $ bacteria_data
dayofweek.size()
students.info()
archive_clean.drop(['retweeted_status_id', $                     'retweeted_status_user_id', $                     'retweeted_status_timestamp'], $                   axis=1, inplace=True)
df[['state_cost','state_retail','sale']] = df[['state_cost','state_retail','sale']].apply(pd.to_numeric) $ df[['store_number','vendor_number','item_number',]]=df[['store_number','vendor_number','item_number']].astype(object) $ df['date']= pd.to_datetime(df['date'], format="%m/%d/%Y")
p_old = df2['converted'].mean() $ print("Actual values of Probability of conversion for old page (p_old):", p_old)
merged[merged['population'].isnull()].head()
cur.execute('SELECT * FROM id LIMIT 5;') $ cur.fetchall()
pd.Series({2:'a',1:'b',3:'c'}, index=[3,2])
def slugging(x): $     bases = x['h']-x['X2b']-x['X3b']-x['hr'] + 2*x['X2b'] + 3*x['X3b'] + 4*x['hr'] $     ab = x['ab']+1e-6 $     return bases/ab $ baseball.apply(slugging, axis=1).round(3)
def True_positive(prediction,test): $     predict = pd.DataFrame(prediction.unstack()).groupby(['CUSTOMER_ID','MATNR']).sum().reset_index() $     predicted_item = predict[predict[0]==1] $     result = len(pd.merge(test, predicted_item, how='inner', on=['CUSTOMER_ID', 'MATNR'])) $     return result
n_new=df2[df2['group']=='treatment'].user_id.nunique() $ n_new
OldPage = np.random.choice([1, 0], size=old, p=[p, (1-p)]) $ old_avg = OldPage.mean() $ print(old_avg)
dfstationmeanbreak = station_mean(dfbreakfast) $ dfstationmeanbreak.head(2)
input_data = np.array([3, 5]) $ weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}
wrd.query('name == "an"')['text']
invoice_sat.columns.tolist()
df.iloc[1] # select row by integer location 1 $
print("Number of Relationships in ATT&CK") $ relationships = lift.get_all_relationships() $ print(len(relationships)) $ df = json_normalize(relationships) $ df.reindex(['id','relationship', 'relationship_description', 'source_object', 'target_object'], axis=1)[0:5]
cust_data1.dtypes
autos["price"].unique().shape
d.join(df).head(10)
autos = autos[autos['price_dollars'].between(0,350000)]
from rgf.sklearn import RGFClassifier, FastRGFClassifier $ clf_RGF_tf = RGFClassifier(max_leaf=240, $                            algorithm="RGF_Sib", $                            test_interval=100, $                            verbose=False,).fit(X_traincv_tf, y_traincv_tf)
facts_res = requests.get(facts_url)
data.head()
pts_df = points.unstack() $ pts_df
display(df['Market'].head(1)) $ display(df['Market'].head(2))
ts.tshift(-1,freq="H")
df2.head()
diff = [] $ for ele in r.json()['dataset']['data']: $     change = ele[2]-ele[3] $     diff.append(change) $ print('The largest change in any one day based on High and Low price: ' + str( max(diff)))
for x in prop.columns: $     print (x,prop[x].nunique(dropna = False))
ts.shift(5, freq=pd.datetools.bday)
from sklearn.naive_bayes import GaussianNB
cercanasA1_11_14Entre75Y100mts = cercanasA1_11_14.loc[(cercanasA1_11_14['surface_total_in_m2'] >= 75) & (cercanasA1_11_14['surface_total_in_m2'] < 100)] $ cercanasA1_11_14Entre75Y100mts.loc[:, 'Distancia a 1-11-14'] = cercanasA1_11_14Entre75Y100mts.apply(descripcionDistancia, axis = 1) $ cercanasA1_11_14Entre75Y100mts.loc[:, ['price', 'Distancia a 1-11-14']].groupby('Distancia a 1-11-14').agg(np.mean)
df_2016.dropna(inplace=True) $ df_2016
df_a.join(df_b)
df[df['price'] < 10].head(1)
from matplotlib.patches import Rectangle
list(rain_period.index)
condo_6.drop(['full_address',  'MAR_MATCHADDRESS', 'MAR_XCOORD', 'MAR_YCOORD', 'MAR_LATITUDE', 'MAR_LONGITUDE', $               'MAR_WARD', 'MAR_ZIPCODE', 'MARID', 'MAR_ERROR', 'MAR_SCORE', 'MAR_SOURCEOPERATION', $               'MAR_IGNORE'], axis=1, inplace=True)
AAPL['MA20'] = AAPL['close'].rolling(20).mean() $ AAPL.head()
all_39s_from_2011 = Grouping_Year_DRG_discharges_payments.loc[(slice(2011), slice(39)),:] $ len(all_39s_from_2011)
avg_per_seat_price = total["Per Seat Price"].mean() # This takes the mean of each our eight teams.
df.show()
messages_with_dates_ext.head(100)
df2.tail(2)
len(t.user_id.unique())
save_n_load_df(joined, 'joined_transactions.pkl')
autos["price"].sort_values(ascending=False).head(50)
laurel = open('conn_laurel.txt','r') $ hardy = open('conn_hardy.txt','r') $ conn_laurel = psycopg2.connect(laurel.read()) $ conn_hardy = psycopg2.connect(hardy.read())
stocks['Date'] = stocks['Date'].dt.week
print('97.5th quantile: ', officer_citations['badge_#'].quantile(0.85)) $ print('Two standard deviations above the median, and the cutoff income is $175,000')
from mpl_toolkits.basemap import Basemap
english_df.show(5)
pnew = new_page_converted.sum()/len(new_page_converted) $ pold = old_page_converted.sum()/len(old_page_converted) $ diff = pnew - pold
df = df[(((df.Longitude > min_lon) & (df.Longitude < max_lon)) & \ $          ((df.Latitude  > min_lat) & (df.Latitude  < max_lat)))]
df_t['day'] = df_t['timestamp'].dt.day $ df_t['day'] = df_t['day']-1
df.describe()
predicted_outcome_first_measure = holdout_results[holdout_results.second_measurement==0].\ $ groupby('wpdx_id').cv_predictions.sum()
old_page_converted = np.random.choice([1, 0], p=[p_old, 1-p_old], size=n_old) $ old_page_converted.mean()
test_predictions = model.transform(df_test) $ test_rmse = evaluator.evaluate(test_predictions) $ print(test_rmse)
df_train['b1'] = df_train.EXT_SOURCE_1.map(lambda x: 1 if pd.isnull(x) else 0) $ df_train['b2'] = df_train.EXT_SOURCE_2.map(lambda x: 1 if pd.isnull(x) else 0) $ df_train['b3'] = df_train.EXT_SOURCE_2.map(lambda x: 1 if pd.isnull(x) else 0)
LOC_outstanding[LOC_outstanding.Merkmalcode=='KG']
autos = autos.drop(index = impossible_year)
quantiles = df_final[features].quantile(q=[0.25,0.5,0.75]) $ quantiles = quantiles.to_dict() $ quantiles
FREEVIEW.plot_heatmap(raw_freeview_df,raw_fix_count_df)
for v in cat_vars: joined[v] = joined[v].astype('category').cat.as_ordered()
import statsmodels.api as sm $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_ipb_chord_crown[0][0])) $ perr = np.sqrt(np.diag(pcov_ipb_chord_crown[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
sampled_authors_grouped_by_author_id_flattened.schema
S_distributedTopmodel.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
pd.date_range('11-Sep-2017', '17-Sep-2017', freq='2D')
weather_all.groupby('Station Name').agg(['mean', 'std'])
dog_ratings.source.value_counts()
autos.shape
etf_weights = calculate_dividend_weights(ex_dividend) $ helper.plot_weights(etf_weights, 'ETF Weights')
autos = autos[autos['registration_year'].between(1910,2016)] $ (autos['registration_year'] $ .value_counts(normalize=True) $ .sort_index())
total_tokens_sk = len(all_tokens_sk) $ corpus_tweets_streamed_keyword.append(('total tokens', total_tokens_sk)) # update corpus comparison $ print('Total number of words (including mentions, hashtags and links) in the collection: ', total_tokens_sk)
train['dot_gov'] = train.url.str.contains('.gov', case=False, na=False, regex=False).astype(int) $ train.groupby('dot_gov').popular.mean()
stemmer = nltk.stem.porter.PorterStemmer() $ %timeit articles['tokens'] = articles['tokens'].map(lambda s: [stemmer.stem(w) for w in s])
df.name.value_counts()
train.MARQUE_LIB.value_counts()
sakhalin_shp.bbox
df2.drop(2893, inplace = True) $ df2.query('user_id == 773192')
facts_metrics.groupby(['dimensions_date_id', 'dimensions_item_id', 'id']).sum()
converted_new = df2.query("group == 'treatment' and converted == '1'").count()[0]/df2.query("group == 'treatment'").count()[0] $ converted_new
notus.loc[notus['cityOrState'] == 'Londo', 'cityOrState'] = 'London' $ notus['cityOrState'] = notus['cityOrState'].str.strip('\n') $ notus['cityOrState'] = notus['cityOrState'].str.strip()
grid_search.best_score_
df_imputed_mean_NOTCLEAN1A.head(5)
g_all = pd.read_csv('https://assets.datacamp.com/production/course_2023/datasets/gapminder.csv') $ print(g_all.shape) $ print(g_all.head())
data1[0]=pd.to_datetime(data1[0],unit='s') $ data1=data1.set_index(0,drop=True) $ data1_new= data1[1].resample('60Min', how='ohlc') $ data1_new['volume'] = data1[2].resample('60Min').sum() $
tweet_mentions = extractor.mentions_timeline(['942535103099502592',200]) 
df_goog.dtypes
data.sample(5)
tl_2020 = pd.read_csv('input/data/trans_2020_ls.csv', encoding='utf8', index_col=0)
df_city_reviews.cache()
ttarc.info()
commits_per_hour.plot.bar();
prop.plot(kind='bar') $ plt.title('Anonymous location, Is the Donor a Teacher?') $ plt.show()
model.score(X_test,y_test)
kl = pd.read_csv(path+"datas/kowloon.csv", index_col=0)
corpus = [dictionary.doc2bow(text) for text in texts] $ corpora.MmCorpus.serialize('/home/david/Desktop/guardian.mm', corpus)  # store to disk, for later use $
    test_size = 3 $     X = clean_prices.copy().loc[:,'EMA_ratio':'5dayvol'] $     y = clean_prices.copy().loc[:, ['5_day_target']]
proj_df.info()
df_cs['blobw'] = df_cs['cleaned_text'].apply(TextBlob)
df2 = df2.drop_duplicates(subset='user_id', keep='last')
df[df['Descriptor'] == 'Loud Music/Party'].groupby(by=df[df['Descriptor'] == 'Loud Music/Party'].index.hour).count().plot(y='Agency')
df2 = pd.read_csv('ab_updated_data.csv') $ df2.head()
geocoded_df['Judgment.In.Favor.Of.Plaintiff'] = for_plaintiff
df_full[['CA', 'UK', 'US']] = pd.get_dummies(df_full['country']) $ logit_mod = sm.Logit(df_full['converted'], df_full[['intercept', 'ab_page', 'US', 'CA' ]]) $ results = logit_mod.fit() $ results.summary()
pres_date_df = pres_df.copy() $ pres_date_df.head(2)
obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])
df_all.shape
(autos["ad_created"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $ )
data.describe()
figs_std_dev = utility_patents_subset_df['number-of-figures'].std() $ figs_median = utility_patents_subset_df['number-of-figures'].median() $ utility_patents_subset_df = utility_patents_subset_df[utility_patents_subset_df['number-of-figures'] <= (figs_median + 3*figs_std_dev)] $ sns.distplot(utility_patents_subset_df['number-of-figures'], color="green") $ plt.show()
train['month_account_created'] = train['date_account_created'].apply(lambda x: x.strftime('%m'))
y = df['comments'] $ X = df[['subreddit', 'title']].copy(deep=True) 
search_url = 'https://api.twitter.com/1.1/search/tweets.json'
fb.index = fb.index.tz_localize('utc').tz_convert('Asia/Singapore')
contractor_clean['last_updated'].head() $
session.query(func.min(Measurement.tobs), func.max(Measurement.tobs), func.avg(Measurement.tobs)).\ $     filter(Measurement.station == most_active_id[0]).all()
X = reddit_master['title'] $ y = reddit_master['Class_comments'].apply(lambda x: 1 if x == 'High' else 0)
authors_grouped_by_id = distinct_authors_with_gh.groupBy("project_name", "new_unique_id").agg( $     collect_set(F.col("email")).alias("emails"), $     F.last(F.col("Author")).alias("Author"), $     F.first("github_username").alias("github_username"), $     F.max("latest_commit").alias("latest_commit")) $
session_top_subset[session_top_subset.user_id == '00023iyk9l']
ways[(ways['timestamp'].dt.year == 2017) | $       (ways['timestamp'].dt.year == 2016)]['user'].value_counts().head(10)
autos["ad_created"].str[:10].value_counts(dropna = False, normalize = True).sort_index()
save_n_load_df(df, 'elapsed_events_df.pkl')
pivoted_data.resample("M").sum().plot(figsize=(10,10))
df_enhanced = df_enhanced.query('retweeted_status_id == "NaN"')
pickledf = pd.read_pickle('cleandf')
import os $ print(os.listdir('.'))
df_users_6_mvp.to_csv('/Users/nikhil.mogare/Desktop/DSA_Reporting/Week3_Sep19/CSVs/historical_mvp_users.csv') $ df_users_6_after_mvp.to_csv('/Users/nikhil.mogare/Desktop/DSA_Reporting/Week3_Sep19/CSVs/after_july_2017_mvp_users.csv')
aus = ['Sydney', 'New South Wales', 'Australia', 'Melbourne', 'Brisbane', 'Queensland', 'Canberra', 'Perth'] $ notus.loc[notus['country'].isin(aus), 'country'] = 'Australia' $ notus.loc[notus['cityOrState'].isin(aus), 'country'] = 'Australia' $ notus.loc[notus['country'] == 'Australia', 'cityOrState'].value_counts(dropna=False)
se = pd.Series(list1) $ data['cleanedtext'] = se.values
from IPython.display import Image $ Image('/Users/jdchipox/Desktop/SV40table.png')
autos['price'].describe()
kick_projects['duration']=(kick_projects['deadline_date']-kick_projects['launched_date']).dt.days $ kick_projects['launched_quarter']= kick_projects['launched_date'].dt.quarter $ kick_projects['launched_month']= kick_projects['launched_date'].dt.month $ kick_projects['launched_year']= kick_projects['launched_date'].dt.year
collection = store.collection('NASDAQ.EOD') $ collection
fm_confident_over = fm_confident_over.rename('bet_won_over_pred')
df2.drop([2893], inplace=True)
print("The minimum value of userID:") $ userArtistDF.agg(min("userID")).show()
dashdata=ml.merge(clean_prices.groupby(['name']).tail(1),left_index=True,right_index=True,how='left')
rt_count_1.to_csv('retweets_by_keywords.csv')
print "Mean time for closing a ticket in 2012: %f hours" % (time2close_2012.mean()/3600.0) $ print "Median time for closing a ticket in 2012: %f hours" % (time2close_2012.median()/3600.0) $ print "Quantiles: " $ print time2close_2012.quantile([0.25, 0.5, 0.75])
crimes.to_csv('../data/processed/crimes_le.csv', index=False)
ctc = ctc.fillna(0)
df[df['Descriptor'] == 'Pothole'].resample('M').count().head()
x = df[ (df['group'] == 'treatment') & (df['landing_page'] != 'new_page')].shape[0] $ y= df[ (df['group'] != 'treatment') & (df['landing_page'] == 'new_page')].shape[0] $ x+y
df2[(df2['group']=='control') & (df2['converted']==1)].converted.sum()/df2[df2['group']=='control' ].group.count()
train.head()
X.isnull().sum()
response = requests.get(mars_news_url)
convert_rate_old = np.mean([p_new, p_old]) $ convert_rate_old
df2 = PredClass.date_cut(df, '2017-10-01') $ df2 = df $ X, y = make_x_y(df2)
data_df.clean_desc[22]
df.select(a == 0).show(5)
import matplotlib.pyplot as plt
df2['landing_page'].value_counts()
def yearMarkers(axis_obj, x_pos, **kwargs): $     axis_obj.axvline(x_pos, linestyle='--', color='w', alpha=.4, **kwargs) $ years = np.arange(0,len(grouped), 51)
data = pd.read_csv('csvs/datosFiltrados.csv', low_memory=False)
df_img_predictions.info()
commiters_by_month = commiters['first_commit']\ $                      .groupby(commiters.first_commit.dt.to_period('M'))\ $                      .agg('count') $ pprint (commiters_by_month)
FREEVIEW.plot_number_of_fixations(raw_fix_count_df, option='facet_subjects')
ax = sns.regplot(x="trip_distance", y="fare_amount", ci=None, truncate=True, data=trips) $ ax.set(xlim=(-1,45)) $ ax.set(ylim=(-1, 220))
dates = ['2017-July-02', 'Aug 03 1985', '3/17/19'] $ df = pd.DataFrame(list('abc'), index = pd.to_datetime(dates) ) $ df.index
enroute_4x_tabledata = enroute_4x_count_prop_byloc.reset_index() $ create_study_table(enroute_4x_tabledata, 'locationType', 'remappedResponses', $                    location_remapping, atloc_response_list)
results[-1].retweet_count
pivoted.plot(legend=False, alpha = 0.1);
df_protest.loc[df_protest.towncity_name=='Johannesburg', 'start_date'].dtype
filtered_file = master_file.copy(); $ filtered_file.drop(a[index_].index.values, axis=1, inplace=True) $ filtered_file.dropna(subset=a[~ index_].index.values, inplace=True) $ filtered_file = filtered_file.reset_index(drop=True)  # Removing data messes with the indexing
twitter_archive[~twitter_archive.retweeted_status_id.isnull()].head()
df = pd.read_csv('data/df_new.csv').drop('Unnamed: 0', 1) $ print df.info() $
title = soup.title.contents[0][10:] $ title
cbg = gpd.read_file('cb_2015_36_bg_500k.shp')
parse_dict['category']['category_'] = parse_dict['category']['slug'].str.split('/') $ parse_dict['category']['category'] =parse_dict['category']['category_'].str[0] $ parse_dict['category']['subcategory'] = parse_dict['category']['category_'].str[1] $ df_cat = parse_dict['category'][['category','subcategory']]
titanic = sns.load_dataset('titanic')
week50 = week49.rename(columns={350:'350'}) $ stocks = stocks.rename(columns={'Week 49':'Week 50','343':'350'}) $ week50 = pd.merge(stocks,week50,on=['350','Tickers']) $ week50.drop_duplicates(subset='Link',inplace=True)
lims_query = "SELECT err.id, s.id AS cell_id, s.name \ $ FROM ephys_roi_results err \ $ JOIN specimens s ON s.ephys_roi_result_id = err.id" $ lims_df = get_lims_dataframe(lims_query) $ lims_df.tail()
dfmean = df.groupby('Single Name').mean()[df.groupby('Single Name').count()['Name']>6] $ dfmean.reset_index() #turn index into a column
df.index = entities $ df.loc['Citigroup',:]
feature_matrix_duplicated.shape
precision = float(precision_score(hy, rf.predict(hX))) $ recall = float(recall_score(hy, rf.predict(hX))) $ print "Hotel searches model -" $ print("The precision is {:.1f}% and the recall is {:.1f}%.".format(precision * 100, recall * 100))
df_mes[(df_mes['average_speed'] == np.inf) | (df_mes['average_speed'] > 100)]
df['date'] = df.date.astype(str)
filtered.groupby('group_id').tweet_id.count()
pickle.dump(cv_data, open('iteration1_files/epoch3/cv_data.pkl', 'wb'))
date_df = pd.DataFrame({'date': date, 'count': count, 'duration': duration}) $ date_df.head()
df_new['CA_ab_page'] = df_new['CA'] * df_new['ab_page'] $ df_new['UK_ab_page'] = df_new['UK'] * df_new['ab_page'] $ df_new.head()
clf = RandomForestClassifier(n_estimators=300).fit(X, y)
_ = ok.grade('q16') $ _ = ok.backup()
df_archive_clean.text = df_archive_clean.text.apply(lambda row: row[:-24])
df.sort_values(['operator', 'part'], inplace=True)
mbti_text_collection_filler.to_csv('Reddit_mbti_data_filler.csv',encoding='utf-8')
("Tue, 11 Feb 2014 15:18:55").split()
sample_survey.loc[:, 'i_Education'].unique()
df_data=pd.DataFrame({'time':(times+utcoffset).value[:-sa],'chips':final.chips.values}) $
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ p_new
store_size_2015 = iowa_2015.groupby('Store Number')[['Volume Sold (Gallons)', 'Profit']].sum().reset_index() $ store_size_2015['Profit Per Gallon'] = store_size_2015['Profit'] / store_size_2015['Volume Sold (Gallons)'] $ mpl.pyplot.figure(figsize=(10,7)) $ sb.regplot(data=store_size_2015, x='Volume Sold (Gallons)', y='Profit Per Gallon')
auth = tp.AppAuthHandler(API_KEY, API_SECRET) $ api = tp.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
df_archive_clean[["timestamp", "retweeted_status_timestamp"]] = df_archive_clean[[ "timestamp", "retweeted_status_timestamp"]].replace("\+0000","",regex = True)
scaled.describe()
from sklearn.model_selection import GridSearchCV
g1800s.columns
print(model_ADP.aic,model_ADP.bic,model_ADP.hqic)
x3 = poly3.fit_transform(x)
df['MeanFlow_cfs'].describe()
final_train_users = final_train_users.ix[np.logical_and(final_train_users['age'] > 10, final_train_users['age'] < 80)] $
plot_series_save_fig(series=RN_PA_duration, figsize=(12,6), xlabel='Date', ylabel='Appointment Time (hours)',\ $                      plot_name='RN/PAs', figname='./images/RNPA_weekly_time_series.png')
with open('sample_data.json') as json_data: $     d = json.load(json_data) $ json_data.close()
ac['Dispute Resolution End'].describe()
df_new = ct_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_new.head()
exploration_titanic.findupcol() $
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print(z_score) $ print(p_value)
goodreads_users_df = pd.read_csv(goodreads_users_filename)
tweets_df[~(tweets_df.in_reply_to_screen_name.isnull())].count()
positive_words=pos_sent.split('\n') $ negative_words=neg_sent.split('\n') $ print(positive_words[:10]) $ print(negative_words[:10])
appointments = pd.read_csv('./data/AppointmentsSince2015.csv')
autos.rename({"odometer": "odometer_km"},axis=1,inplace = True)
p_old_real = df2.query('landing_page == "old_page"')['converted'].mean() $ p_old = df2.query('converted == 1').count()[0]/df2.count()[0] $ p_old
news_df.to_csv('news_analysis.csv')
snowshoe_prob = snowshoe_df[['month','snowshoes']].groupby(['month']).mean() $ snowshoe_prob.plot.bar() $ demo.apply_plot_settings()
convRate = pd.concat([hired,shown],axis=1) $ convRate['rate'] = convRate['hired']/convRate['tasker_id']
lm.summary()
old_page_converted = np.random.binomial(n_old,p_old) $ print('new_page_converted :: ',old_page_converted)
df.describe(include="all")
nba_df.sort_values(by = ["Tm.STL"], ascending = False)
ss = StandardScaler() $ X_train = ss.fit_transform(X_train) $ X_test = ss.transform(X_test)
patterns_sex="m|f|NA" $ non_matches=df.sex.loc[df.sex.str.match(patterns_sex)!=True] $ print("\nThere are {} entries for sex which do not match the patterns {}:"\ $       .format(non_matches.shape[0],patterns_sex.split("|"))) $ non_matches.value_counts()
df_sites = (df_providers.groupby(['id_num','name','year'])[['disc_times_pay']].sum()) $ df_sites = df_sites.reset_index() $ df_sites.head()
pd.DataFrame(X.toarray()).info()
my_list = ["a", "b", "c", "d"] $ print(my_list[0])   # <-- prints "a" $ print(my_list[3])   #  Read '[3]' as: '4th element.                     # $ print(my_list[0:3]) #  Read '[0:3]' as: 'From 1st to 3rd element.       # $
unwanted= df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False] $ unwanted.shape[0]
rdg = Ridge(alpha=5) $ rdg.fit(train_data, train_labels)
import matplotlib.pyplot as plt $ fig = plt.figure(figsize=(11,8)) $ ax1 = fig.add_subplot(111) $ ax1.plot(crimes_by_yr_month['year'], crimes_by_yr_month['crime_count'], label=1)
wrd.info()
test = Plot['iLayerHeight'].data $ dates = Plot['time'].data $ test = np.squeeze(test) $ df = pd.DataFrame(data = test, index=dates) $ df.replace(to_replace=-9999.0, value = 0, inplace=True)
arrPriceList = closingPrices[closingPrices.sort_values(ascending = False)]
posts['ViewCount'].agg([np.mean, np.median])
os.chdir('movieclassifier/')
load2017 = load2017.dropna()
X.shape
df = pd.read_csv("ab_data.csv") $ df["intercept"] = 1 $ df[["control","treatment"]] = pd.get_dummies(df["group"]) $ df.head()
active_station = str(session.query(Measurement.station).group_by(Measurement.station).\ $                        order_by(func.count(Measurement.id).desc()).first())
index_weights = generate_dollar_volume_weights(close, volume) $ helper.plot_weights(index_weights, 'Index Weights')
client = MongoClient(config.get('Mongo', 'host'), int(config.get('Mongo', 'port')))
lbl = LabelEncoder() $ train['air_store_id2'] = lbl.fit_transform(train['air_store_id']) $ test['air_store_id2'] = lbl.transform(test['air_store_id'])
df2['intercept']=1 $ df2[['control','ab_page']]=pd.get_dummies(df2['group']) $ df2 = df2.drop('control', axis = 1)
gs_pca_under.score(X_test_pca, y_test_under)
df_clean['rating_numerator'].value_counts()
df_h1b_nyc_ft.pw_1.describe()
df.head()
merged_data = pd.merge(left=surveys_df,right=species_df, how='inner', left_on='species_id', right_on='species_id') $ merged_data.head()
df = pd.read_csv(CSV_FILE_PATH,delim_whitespace=True)
station_histo_df = max_calc = df.loc[df["station"] == "USC00519397"] $ plt.figure() $ plt.hist(station_histo_df["tobs"], bins = 12) $ plt.show()
s.groupby(['ID','group']).size().unstack().plot(kind='bar', stacked=True); $ plt.xlabel('Number of sms\'s') $ plt.ylabel('Number of people targeted') $ plt.title('Frequency of sms each person received');
bwd.head()
userMovies = userMovies.reset_index(drop=True) $ userGenreTable = userMovies.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1) $ userGenreTable
df[df.index.month.isin([1,2,3])].head()
train_col.plot_lr()
data['win_differential'] = abs(data.homeWinPercentage - data.awayWinPercentage) $ data['win_team'] = np.where(data.awayWinPercentage >= data.homeWinPercentage, 'away', 'home') $ data['game_state'] = np.where(data.win_differential < 0.6, 'close', 'notclose') $
from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS $ stop_words = ENGLISH_STOP_WORDS $ cv = CountVectorizer(stop_words=stop_words) $ dtm = cv.fit_transform(no_urls_all_engl) $ dtm
obj = pd.Series([4, 6, -1, 2])
y_pred = [randint(0, 2) for i in range(1000)] $ y_true = [randint(0, 2) for i in range(1000)] $ di = {0: 'actif', 1: 'churn', 2: 'lost'} $ y_pred = [di.get(n,n) for n in y_pred] $ y_true = [di.get(n,n) for n in y_true]
with open('sfmin.template', 'w') as f: $
date_df=pd.DataFrame(pd.to_datetime(df2.created_time))
X = endometrium_data.drop(['ID_REF', 'Tissue'], axis=1).values $ y = pd.get_dummies(endometrium_data['Tissue']).values[:,1]
trump = trump.drop(["id_str"], axis = 1)
wildfires_df['FIRE_YEAR'].unique()
df.groupby('episode_id')['id'].nunique().mean()
user.set_index("screen_name").head(3)
for tweet in tweets: $     print(tweet.author.name) $     print(tweet.text)
psy_df4 = PDSQ.merge(psy_df3, on='subjectkey', how='right') # I want to keep all Ss from psy_df $ psy_df4.shape
n_old = len(df2.query("landing_page == 'old_page'"))           $ print('n_old :: ', n_old)
df_id_num_name_year = df_providers[['id_num','name','year']] $ df_id_num_name_year = df_id_num_name_year.drop_duplicates() $
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23H2P').items(100): $     results.append(tweet) $ len(results)
a = [1, 2, 3, "cat"] $ print(a) $ print(len(a))  # len() gives the length of the list $ print(a[1])  # [] can be used to index in to the list; implemented by list.__getitem__; assignment uses list.__setitem__ $ print(a[-1])  # negative indices can be used to index from the end of the list (-1 for last element)
archive_clean['stage'].value_counts()
a = df.user_id.unique() $ print(' The number of unique users in the dataset is {}'.format(len(a))) $
jobs.loc[(jobs.FAIRSHARE == 3) & (jobs.ReqCPUS == 8) & (jobs.GPU == 0) & (jobs.Group == 'rathmell_lab')][['Memory','Wait']].groupby(['Memory']).agg(['mean', 'median','count']).reset_index()
from sklearn.feature_extraction.text import CountVectorizer
for c in ccc: $     ved[c] = ved[ved.columns[ved.columns.str.contains(c)==True]].sum(axis=1)
sub_df = pd.DataFrame({"click_id":test.click_id ,"is_attributed":preds})
s3.dropna? $
titanic.fillna({'fare': 0}, inplace=True) $ titanic[~filter.fare]
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("Probability of conversion for new page (p_new):", p_new)
df.head()
df_new_conv = df_newpage.query('converted == "1"') $ x_new_conv = df_new_conv["user_id"].count() $
stock.columns
autos["odometer_km"].unique().shape
located_data.tail(100)
preds = gbm.predict(T_test_xgb)
to_be_predicted_Day4 = 48.64924294 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
model.save('model')
res3_a = df_test3_promotions \ $     .agg({'PromotionRuleCount':'sum', 'AwardAmount':'sum', 'ShopperID':'nunique'}) $ res3_a['AwardAmount'] = res3_a['AwardAmount'] / 100 $ res3_a
X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2, random_state = 2)
staff.columns
corn.get_group("beef")
import pandas as pd $ from pandas.io import gbq
print s1.json()
predictions_final=predictions_final.select(col('user_id').alias('p_user_id'),col('repo_id').alias('p_repo_id'),'score') $ test_forks=test_set.filter(test_set.event=='fork') $ test_join=test_forks.join(predictions_final,(test_forks.user_id==predictions_final.p_user_id)\ $                    & (test_forks.repo_id==predictions_final.p_repo_id),'left')
df_new['country'].value_counts() $ df_new[["CA","US"]] = pd.get_dummies(df_new['country'])[["CA","US"]] $ df_new.head()
git_log.author.head()
X, Y = X_test, np.array(Y_test_labels) $ predictions = model.predict(x = X.reshape(shapeX)) $ confidence = predictionConfidence(predictions) $ prediction_barcode = predictionGetBarcodeLabel(predictions) $ selection =  (Y != prediction_barcode) $
new_data.head(1)
wikiContentSoup = bs4.BeautifulSoup(wikiContentRequest.text, 'html.parser') $ print(wikiContentSoup.text[:800])
p_new_page = df2.query('landing_page == "new_page"').shape[0] / df2.shape[0] $ print('Probability of an individual received the new page: {}'.format(p_new_page))
myseries=pd.Series(np.random.randn(6),index=multiIndex) $ print(myseries)
onlyfiles = [f for f in os.listdir(datapath2) if os.path.isfile(datapath2 / f) and f.endswith('.pdf')] $ onlyfiles.sort() $ print('Files in the folder:') $ for i, w in enumerate(onlyfiles): $     print(i+1, '--' ,w)
re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', cfd_index['rt'][10])
sum(tweet_archive_clean['expanded_urls'].isnull())
wrd_full.groupby(['year'])['favorite'].mean()
y = to_categorical(digits.iloc[:,0]) $ y
print_tweet_text(tweet_archive_df[(tweet_archive_df.rating_denominator % 10).astype(bool)])
from sklearn.decomposition import PCA
def string_to_datetime(string): $     return datetime.strptime(string,'%Y-%m-%d %H:%M:%S %z') $ tweet_archive_clean.timestamp = tweet_archive_clean.timestamp.map(string_to_datetime)
noaa_data.head()
df = pd.read_csv('https://raw.githubusercontent.com/mbdata/coding_problems/master/python/data/Medicare_Hospital_Spending_by_Claim.csv') $ df = df.drop(['Measure Start Date', 'Measure End Date'], axis = 'columns') $ df
dutchPhoneNumber = re.compile(r'(^\+[0-9]{2}|^\+[0-9]{2}\(0\)|^\(\+[0-9]{2}\)\(0\)|^00[0-9]{2}|^0)([0-9]{9}$|[0-9\-\s]{10}$)') $ for number,quantity in phoneNumbers: $     strippedNumber = number.replace(" ", "") $     if not dutchPhoneNumber.match(strippedNumber): $         print strippedNumber
finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 0) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 1), 'type'] = 'defenders'
(new_page_converted.mean()) - (old_page_converted.mean())
!unzip train_cont.zip
def calc_temps(start_date, end_date): $      return session.query(func.min(Measurement.tobs), func.avg(Measurement.tobs), func.max(Measurement.tobs)).\ $         filter(Measurement.date >= start_date).filter(Measurement.date <= end_date).all() $ print(calc_temps('2012-02-28', '2012-03-05'))
pd.merge(d1, d3, left_on='city', right_on='place').drop('place', axis=1)
noise_graf = pd.merge(noise, graf_counts, on='AFFGEOID')
print(training_active_listing_dummy[0:5],training_active_listing_dummy.mean())
train = pd.read_csv("../data/wikipedia_train3.csv") $ test = pd.read_csv("../data/wikipedia_test3.csv")
df_transactions['amount_per_day'].unique()
result_df.head(10)
print(norm.cdf(z_score))
closingprice_list = [] $ for item in data2: $     if item[4]!= None: $         closingprice_list.append(item[4]) $
F.mse_loss(input=pred1, target=target1)
tst_lat_lon_df.describe()
counts = df.groupby(Grouper(key="stamp",freq='30min')).agg({"id":"count"}).rename(columns={"id":"count"}) $ counts.reset_index(inplace=True) $ counts.head()
np.allclose(df1 + df2 + df3 + df4, pd.eval('df1 + df2 + df3 + df4'))
import pickle $ pkl_file = open('mentioned_bills.pkl', 'rb') $ mentioned_bills_all = pickle.load(pkl_file)
week30 = week29.rename(columns={210:'210'}) $ stocks = stocks.rename(columns={'Week 29':'Week 30','203':'210'}) $ week30 = pd.merge(stocks,week30,on=['210','Tickers']) $ week30.drop_duplicates(subset='Link',inplace=True)
train_downsampled.write.mode('overwrite').parquet(S3_HOME + '/train_downsampled.parquet') $ testing.write.mode('overwrite').parquet(S3_HOME + '/testing.parquet') $
VX.head(1)
a.strip('"<>/a').split('=')[-1].split('">')
df2.head(2)
tweet_archive_enhanced_clean.info()
commit_nova_2015 = pandas_ds[(pandas_ds["gerrit_closing_date"] >= '2011-01-01') & (pandas_ds["gerrit_closing_date"] <= '2012-01-01') & (pandas_ds["current_status"] == 'MERGED') & (pandas_ds["gerrit_tracker"].str.contains("/tempest"))]; $ commit_nova_2015
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=8000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
store_items = store_items.rename(index = {'store 3': 'last store'}) $ store_items
print("Number of Tools in Enterprise ATT&CK") $ print(len(all_enterprise['tools'])) $ df = all_enterprise['tools'] $ df = json_normalize(df) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
tokendata = pd.merge(keydf,tokendata,on="ID",how="left") $ len(tokendata)
print("Number of Software in ATT&CK") $ software = lift.get_all_software() $ print(len(software)) $ df = json_normalize(software) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
inspector = inspect(engine) $ columns = inspector.get_columns('measurement') $ for c in columns: $     print(c['name'], c["type"])
inter_day0 = pd.read_pickle(folderint + 'interactions-2016-11-06.pkl')
sns.distplot(train_df.Log_price.values, bins=100, kde=True)
df.isnull().sum().any()
grid_clf.best_estimator_
import sqlalchemy.engine
weather_df["weather_main"] = weather_df.weather_main.str.replace("drizzle", "rain") $ weather_df["weather_description"] = weather_df.weather_description.str.replace("drizzle", "light rain") $ weather_df["weather_description"] = weather_df.weather_description.str.replace("heavy intensity drizzle", "moderate rain") $ weather_df["weather_description"] = weather_df.weather_description.str.replace("light intensity drizzle", "light rain")
p_converted_treatment_user2 = df2.query('converted==1 and group=="treatment"').user_id.nunique()/df2.query('group=="treatment"').user_id.nunique() $ p_converted_treatment_user2
date_price.index[0]
p_old = df2.converted.mean() $ p_old
X_train, X_test, y_train, y_test = train_test_split(trains_fe2_x, trains_fe2_y, test_size=0.2) $ print(X_train.shape) $ print(y_train.shape) $ print(X_test.shape) $ print(y_test.shape)
dtm_pos['pos_count'] = dtm_pos.sum(axis=1) $ dtm_pos['pos_count']
print(data.last_trip_date.iloc[0], type(data.last_trip_date.iloc[0]))
ind = pd.date_range(start= '01-07-2017',periods= 9,freq= '2W-MON') $ ind
scaled = scaled.reset_index() $ scaled.head()
!hdfs dfs -cat /user/koza/hw3/3.2.1/productFrequencies/* | wc -l
r = requests.get('https://{}/api/v3/orgs/{}/repos'.format(baseurl, source_org), \ $          headers=header, params={ 'type': 'all' , 'page': 4 })
sns_plot = sns.barplot(x=stages,y=retweets,color='lightblue',order=['puppo','doggo','floofer','pupper'],ci=None) $ plt.ylabel('Median Retweet Count') $ plt.xlabel('Dog Stages') $ sns_plot.figure.savefig('dog_stages_vs_retweet.jpg')
flight.select("arr_time").show(2, truncate=False)
sql.registerDataFrameAsTable(df, 'scen')
drace_df = drace_df.fillna(mean_values)
breed_ratings = df_twitter.groupby('p1')['score_rating'].mean()
twitter_ar.head(2)
extract_all.loc[(extract_all.APP_DOB.isin(['1961/08/09','19610809','08/09/1961'])), $                 ['APP_APPLICATION_ID','APPLICATION_DATE_short','APP_PRODUCT_TYPE','APP_LOGIN_ID', $                 'APP_FIRST_NAME','APP_MIDDLE_NAME','APP_LAST_NAME','APP_SSN', $                 'APP_DOB','APP_CELL_PHONE_NUMBER','DEC_LOAN_AMOUNT1']].sort_values('DEC_LOAN_AMOUNT1', ascending=False)
autos["brand"].value_counts()
df.loc[df['lead_mgr'].str.contains('Stanl'), 'lead_mgr'] = 'Morgan Stanley' $
from sklearn.linear_model import LogisticRegression $ logreg= LogisticRegression(class_weight = 'balanced') $ logreg.fit(preprocessing.scale(train.ix[:, train.columns != 'class']), train['class']) $ logpred= forfit.predict(preprocessing.scale(test.ix[:, test.columns != 'class']))
average_reading_score = df_students['reading_score'].mean() $ average_reading_score
set_names_trans.describe()
leadDollarsClosedPerMonth.mean().plot.bar(figsize={12,6}) $ plt.title('Average Opportunity Revenue per Month');
pd.DataFrame(rows)
n = 3652 $ phi1 = -2 $ phi2 = 2 $ y = np.zeros(n) $ y[1:n] = phi1*x[0:(n-1)] + phi2*z[0:(n-1)] + r[1:n]
plt.plot(x, x ** 2)
tallies_file = openmc.Tallies() $ mgxs_lib.add_to_tallies_file(tallies_file, merge=True)
df_archive_clean.sample(5)
df_uniq_ads = pd.read_csv('./data/Unique_ads.csv') $ df_uniq_ads.head(5)
df.loc[df.toes.str.match(pattern1)==True]
autos[autos.registration_year < 1920]
type(df_vow['Date'].loc[0])
type(df_train_time.fulldate2[0])
lsi_out.head()
p_diffs = np.random.binomial(nnew, pnew, 10000)*1.0/nnew - np.random.binomial(nold, pold, 10000)*1.0/nold $
t0 = time() $ allData = userArtistDataRDD.map(lambda r: ((r[0],r[1]),r[2])).reduceByKey(_+_).map(lambda r: Rating(r[0][0], r[0][1], r[1])).repartition(8).cache() $ t1 = time() $ print('Time taken = ', t1-t0) $ allData.take(5)
airbnb_df['day_of_week'] = airbnb_df['date_listed'].dt.dayofweek  # Number, Monday == 0 $ airbnb_df.loc[0:5, ['date_listed', 'listed_year_month', 'day_of_week']]
plt.plot(i[0:-1], d[2]) $ plt.show()
for i in cpi_all['Adjustment Type'].cat.categories.tolist(): $     print i    
data_tickers = data_tickers.resample(sampling, how='last') $ data_tickers.head()
np.exp(results_new2.params)
plt.figure(figsize=(10,3)) $ plt.plot(df_daily2['DATE'],df_daily2['DAILY_ENTRIES'])
pods.notebook.display_plots('olympic_LM_polynomial_num_basis{num_basis:0>3}.svg', $                             directory='../slides/diagrams/ml', $                             num_basis=IntSlider(1,1,27,1))
from  scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05))) $
soup.img['src']
j = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?&start_date=2014-01-01&end_date=2014-01-02&api_key=' + API_KEY) $
eth = pd.read_csv('data/eth-price.csv', parse_dates=True, index_col=0) $ print(eth.info()) $ eth.head()
n_old = len(df2.query("group == 'control'")) $ print(n_old)
x_train, x_test, y_train, y_test = split_data(x, y) $ x_train.shape, y_train.shape, x_test.shape, y_test.shape $
rows - df.shape[0]
df2_with_country['intercept'] = 1 $ log_reg_with_country = sm.Logit(df2_with_country['converted'], df2_with_country[['intercept','new_page','CA', 'US']]) $ results = log_reg_with_country.fit() $ results.summary()
print 'number of records with 0 conversions: ', np.size(zeroConvs), '\n' $ print 'number of records with 0 conversions by datestamp: ', '\n', df_zeroConvs.groupby('datestamp').size(), '\n' $ print 'number of records with 0 conversions by country: ', '\n', df_zeroConvs.groupby('country_code').size(), '\n' $ print 'number of records with 0 conversions by marketing_channel: ', '\n', df_zeroConvs.groupby('marketing_channel').size(), '\n'
head = pd.Timestamp('20151101') $ tail = pd.Timestamp('20151104') $ df = hp.get_data(sensortype='gas', head=head,tail=tail, diff=True, unit='kW') $ charts.plot(df, stock=True, show='inline')
dates = pd.pivot_table(df, index="Store Number", values="Date", aggfunc=(min, max)) $ df = df.merge(dates, left_on='Store Number', right_index=True) $ df
active_users[active_users['LastAccessDate'] > pd.to_datetime('01-01-2017')].shape # more than half of active users were active in the last year
pd.period_range('11-Sep-2017', '17-Sep-2017', freq='M')
ts = pd.to_datetime('2015-01-15 08:30') $ ts
template = '{0:.2f} {1:s} are worth US${2:d}'
df_t.head()
favourite_max = np.max(data['Likes']) $ favourite = data[data.Likes == favourite_max].index[0] $ print("Tweet with most number of likes is: \n{}".format(data['Tweets'][favourite])) $ print("NUmber of likes: {}".format(favourite_max)) $ print("{} characters.\n".format(data['len'][favourite]))
dates = [pd.datetime(2012, 5, 1), pd.datetime(2012, 5, 2), pd.datetime(2012, 5, 3)]
data = {'Bob' : pd.Series(data = [245, 25, 55]), $          'Alice' : pd.Series(data = [40, 110, 500, 45])} $ df = pd.DataFrame(data) $ df
reg = sm.Logit(df3['converted'], df3[['intercept', 'US', 'CA']])
df.head(2)
tempsApr = Series([29, 30, 28, 31, 32], index = pd.date_range('2018-04-20', '2018-04-24')) $ tempsMay = Series([26, 24, 22, 22, 19], index = pd.date_range('2018-05-20', '2018-05-24')) $ tempsMay - tempsApr
dfAnnualMGD = dfHaw_Discharge.groupby('Year')['flow_MGD'].agg(['sum','count']) $ dfAnnualMGD = dfAnnualMGD[dfAnnualMGD['count'] > 350] $ dfAnnualMGD.columns = ['AnnualFlow_MGD','Count']
tree_c_features_test = c_features_test.apply(class_le.fit_transform)
unitech_df.head(5)
df_arch_clean = df_arch_clean.drop('doggo', axis=1) $ df_arch_clean = df_arch_clean.drop('floofer', axis=1) $ df_arch_clean = df_arch_clean.drop('pupper', axis=1) $ df_arch_clean = df_arch_clean.drop('puppo', axis=1)
df.head(1)
df2.query('user_id == 773192')
autos['price'].describe()
cols=[c for c in nar2.columns if c not in \ $           [ u'in_arrears_since',u'in_arrears_since_days', u'in_arrears_since_days_30360',  u'bucket', u'bucket_pd',  u'payback_state']]
df1 = pd.read_csv('/Users/vikranth/Typing_Participants/Participant_3_20171208/acc_right/ar3.csv',header=None) $ df2 = pd.read_csv('/Users/vikranth/Typing_Participants/Participant_3_20171208/acc_right/ar4.csv',header=None) $ df3 = pd.concat([df1,df2])
1/np.exp(-0.0150)
strs = 'NOTE: This event is EVERY FRIDAY!! Signup is a' $ result = re.split(r'[^0-9A-Za-z]+',strs) $ print(result)
np.exp(results_3.params)
dedup = dedup.drop_duplicates(subset='hash');
wrd_clean['pupper'].value_counts()[:10]
cov_matrix = daily_ret.cov() $ cov_matrix
lda.get_document_topics(dictionary.doc2bow(texts[3]))
%matplotlib inline $ import matplotlib $ import matplotlib.pyplot as plt $ matplotlib.rcParams['figure.figsize'] = (15.0, 8.0) $ bt.plot_equity()
%writefile /tmp/test.json $ {"age":"29.0","activity":3.0}
def clean_dataset(x): $     if isinstance(x, str): $         return str.lower(x.replace(" ", "").replace("&", ",").replace("/",",")) $     else: $         return ''
health_data.iloc[:2, :2]
bottom_views = doctype_by_day.loc[:,doctype_by_day.max() < 10] $ ax = bottom_views.plot() $ ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
archive_df[archive_df.name.str.islower()]
new_page_converted = np.random.binomial(1,pnew,nnew) $ plt.hist(new_page_converted)
tweets_clean = tweets_clean[tweets_clean['name'].str.istitle()] $ not_dogs = tweets_clean[tweets_clean['text'].str.contains("only rate")] $ tweets_clean = tweets_clean.drop(not_dogs.index, axis = 0) $ tweets_clean.info() $
sampled_authors_saved.show()
gender.shape
len(combined_df.columns)
pres_df['subject_count_tmp'].mean(), pres_df['subject_count_tmp'].median()
data.describe(include=['O'])
input_col = ['msno','payment_plan_days','transaction_date', 'membership_expire_date',] $ transactions = utils.read_multiple_csv('../../input/preprocessed_data/transactions',input_col)
walk.resample("1Min", closed="right")
from sklearn.mixture import GaussianMixture $ gmm = GaussianMixture(2) $ gmm.fit(scaled_data) $ labels = gmm.predict(scaled_data) $ labels
base_search_url = 'https://music.youtube.com/search?q=' $ urllib.parse.quote_plus(base_search_url + ' '.join(df.loc[0:0,'title'] + df.loc[0:0,'artist']), safe='/:?=')
bigdf.to_csv('Combined_Comments-Fixed2.csv')
mentions = read_csv("mentions.csv") $ mentions.head()
to_be_predicted_Day2 = 38.62079858 $ predicted_new = ridge.predict(to_be_predicted_Day2) $ predicted_new 
df.dtypes
week34 = week33.rename(columns={238:'238'}) $ stocks = stocks.rename(columns={'Week 33':'Week 34','231':'238'}) $ week34 = pd.merge(stocks,week34,on=['238','Tickers']) $ week34.drop_duplicates(subset='Link',inplace=True)
df['in_reply_to_user_id'].value_counts()
team_names.Conference = team_names.Conference.apply(lambda x : 0 if x=="E" else 1)  # Numeric Encoding
randomdata2 = randomdata1[(randomdata1 >=3) | (randomdata1 <=-3)] $ randomdata2.describe() $
session.query(func.count(Station.id)).all()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) $ regr = LinearRegression() $ regr.fit(X_train, y_train)
open('test_data//open_close_test.txt')
scenario = model.model2db() $ scenario.solve(model='MESSAGE', case='MESSAGE_GHD')
avg_stops_window_crimes__ =  int(np.average(stops_month_window__['sum_window_crimes'])) $ avg_monthly_crimes__ = int(np.average(crime_month_window__['sum_crimes'])) $ avg_bi_weekly_crimes__ = int(np.average(crime_two_week_window__['sum_crimes'])) $ avg_weekly_crimes__ = int(np.average(crime_week_window__['sum_crimes'])) $ avg_daily_crimes__ = int(np.average(crime_day_window__['sum_crimes']))
print("Number of Techniques in Mobile ATT&CK") $ print(len(all_mobile['techniques'])) $ df = all_mobile['techniques'] $ df = json_normalize(df) $ df.reindex(['matrix', 'tactic', 'technique', 'technique_id', 'tactic_type'], axis=1)[0:5]
volmeans = cc.groupby(['name'])['volume'].mean() $ volmeans.sort_values(ascending = False)
print(merged.info())
austin['miles']= austin['distance_travelled']*.000621371
df = pd.DataFrame(list('abcd'), index = [pd.Timestamp('2017-01-01'), pd.Timestamp('2017-01-08'), $                                          pd.Timestamp('2017-01-15'), pd.Timestamp('2017-01-22')]) $ df
run txt2pdf.py -o '2014 Snapshot.pdf' '2014 Snapshot.txt' $
firstday = (vader_df['created_at'] > '2017-10-28') & (vader_df['created_at'] < '2017-10-31') $ firstday_df = df[firstday] $ firstday_df = firstday_df.sort_values(by="sentiment", ascending = False) $ firstday_df.to_csv("firstday.csv") $ firstday_df
weight = live.birthwgt_lb
new_page_converted = np.random.choice([1, 0], size=n_new, p=[p_new, (1-p_new)]) $ print(new_page_converted)
plt.hist(shows['first_year'].dropna()) $ plt.title('Distribution of Release Years') $ plt.ylabel('Frequency') $ plt.xlabel('Year Released')
and_list = df.query("group=='treatment'& landing_page=='new_page'").index
df_test.head() $ Feature_test=df_test[['Principal','terms','age','Gender','weekend']] $ Feature_test=pd.concat([Feature_test,pd.get_dummies(df_test['education'])], axis=1) $ Feature_test.drop(['Master or Above'], axis = 1,inplace=True) $ Feature_test.head()
response = requests.get(url) $ response
archive_copy = archive_copy.drop('id', axis=1)
df.drop(treatment_mismatch.index, inplace=True) $ df.drop(control_mismatch.index, inplace=True) $ df.to_csv('AB_test_edited.csv', index=False) $ df2 = pd.read_csv('AB_test_edited.csv', index_col='timestamp', parse_dates=True) $ df2.head(3)
print('Get subset data of rows no. 11, 24, 37') $ df.loc[[11,24,37]] 
positiveIntMore=positiveIntMore.join(userData.set_index('id')[['discipline_id','industry_id','career_level','country','region']],on='user_id') $ print positiveIntMore.shape $ positiveIntMore.head()
countries_df.shape
tt4Dict = ttTimeEntry.iloc[:,[0,1,2,3,7,6]]
distinct_authors_latest_commit = non_blocking_df_save_or_load( $     raw_distinct_authors_latest_commit, $     "{0}distinct_authors_latest_commit_4".format(fs_prefix))
df2.reindex_like(df1) #Padding Nan
soup.find('div', class_="poster-section left").find('img')['src']
def sigmoid(z):  $     return 1 / (1 + np.exp(-z))
autos['price'].head()
s = pd.Series([7, 'AmarAkbar', 3.14, -1789710578, 'Sholay'], $               index=['int', 'Movie1', 'Pi', 'LongInt', 'Movie2']) $ s
df = pd.read_sql('SELECT booking_id, cust_id, total_amount, payment_status FROM booking WHERE booking_id=8', con=conn_b) $ df
n_new = len(df2.query("group == 'treatment'")) $ print(n_new)
probarr2 = fe.toar(lossprob2) $ fe.plotn(fe.np.sort(probarr2), title="tmp-SORTED-prob")
plt.plot?
act_irr.shape
mars_fact_url = "https://space-facts.com/mars/" $ mars_facts = pd.read_html(mars_fact_url) $ mars_facts_df=pd.DataFrame(mars_facts[0]) $ mars_facts_df.columns=["description", "value"] $ mars_facts_df
image_predictions_df.head()
train['Age'] = train.apply(lambda row: int(row[6].split("-")[0]) - int(row[1].split("-")[0]), axis = 1) $ test['Age'] = test.apply(lambda row: int(row[6].split("-")[0]) - int(row[1].split("-")[0]), axis = 1)
mars_url = "https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars" $ browser.visit(mars_url)
df.groupby('key')
commodity=df1.Commodity.unique() $ apmc=df1.APMC.unique()
p_diffs=[] $ for _ in range(10000): $     new_page_converted = np.random.binomial(n_new,Pnew) $     old_page_converted = np.random.binomial(n_old,Pold) $     p_diffs.append(new_page_converted/n_new - old_page_converted/n_old) $
year3 = driver.find_elements_by_class_name('yr-button')[2] $ year3.click()
run txt2pdf.py -o"2018-06-14 2148 CLEVELAND CLINIC Sorted by Discharges.pdf"  "2018-06-14 2148 CLEVELAND CLINIC Sorted by Discharges.txt"
first_commit_timestamp = git_log.iloc[-1, 0] $ last_commit_timestamp = pd.to_datetime('now') $ corrected_log = git_log[(git_log['timestamp']>=first_commit_timestamp)\ $                         & (git_log['timestamp']<=last_commit_timestamp)] $ corrected_log['timestamp'].describe()
url_NYG = "https://nygiants.strmarketplace.com/Images/Teams/NewYorkGiants/SalesData/New-York-Giants-Sales-Data.xls"
with tf.Session() as sess: $     prod_res = sess.run(matrix_product) $     sum_res = sess.run(matrix_sum) $     det_res = sess.run(matrix_det)
nexts = torch.topk(res[-1], 10)[1] $ [TEXT.vocab.itos[o] for o in to_np(nexts)]
tf_idf = tfidf_vectorizer.fit_transform(df['lemma'])
df2.drop(labels=1899, axis=0, inplace=True) #dropping one of the duplicated entry
y = datay.values.astype(int) $ X = data_adv.values
reddit_comments_data.select('author').distinct().count()
df['log_CBoE']=np.log(df['NASDAQ.CBOE']) $ CBoE_array=df["log_CBoE"].dropna().as_matrix() $ df['diff_log_CBoE']= df["log_CBoE"]-df["log_CBoE"].shift(periods=-1) $ model_CBoE = ARIMA(CBoE_array, (2,2,1)).fit() $ predCBoE=model_CBoE.predict()
win_paths = glob.glob(raw_windows_path+'*/*.csv') $ print("Found {} windows".format(len(win_paths)))
(elms_all.shape, elms_all_0611.shape)
s.str.contains(' ')
clf.predict_proba(raw_text)
num_of_converted = df2[df2.converted == 1] $ p_old = len(num_of_converted)/len(df2) $ p_old
summed.fillna('missing')  # changed dtype to "object"
df2 = df2.drop(index=df2[df2['dupe']==True].index).drop(columns='dupe')
hr_total[hr_total.notnull()]
apple.set_index(rng, inplace = True) # This sets our index to the range that we just created $ apple.head() # And we have our datetime index 
print("Number of Relationships in Mobile ATT&CK") $ relationships = lift.get_all_mobile_relationships() $ print(len(relationships)) $ df = json_normalize(relationships) $ df.reindex(['id','relationship', 'relationship_description', 'source_object', 'target_object'], axis=1)[0:5]
t.days
print(global_mean) $ app_ver_map.head() $ app_ver = joined_df.groupby(['id'])['initial_app_version'].first().to_frame() $ app_ver = app_ver.applymap(lambda x: app_ver_map[x] if x in app_ver_map.index else global_mean) $ app_ver.head()
prcp_analysis_df.set_index("date", inplace=True) $ prcp_analysis_df.describe(include="all")
price = autos['price'] $ ax = sns.boxplot(y= price) $
y_train.head()
new = zc.merge(data3, on='zipcode') $ new.head()
pd_data.payChannel = pd_data.payChannel.map(lambda x: x.split(','))
datAll['Offense_count'] = np.where(~np.isnan(datAll['# Of']),datAll['# Of'],np.nan) $ datAll['Offense_count'] = np.where(~np.isnan(datAll['# Of Offenses']),datAll['# Of Offenses'],datAll['Offense_count']) $ datAll['Offense_count'] = np.where(~np.isnan(datAll['# Offenses']),datAll['# Offenses'],datAll['Offense_count']) $ datAll['Offense_count'] = np.where(~np.isnan(datAll['# offenses']),datAll['# offenses'],datAll['Offense_count']) $ datAll['Offense_count'] = np.where(~np.isnan(datAll['Offenses']),datAll['Offenses'],datAll['Offense_count'])
df.to_excel("../../data/stocks_msft.xlsx", sheet_name='MSFT')
local_path = 'data/clean_boulder_weather.csv' $ web_path   = 'https://raw.githubusercontent.com/chrisketelsen/csci3022/master/inclass-notebooks/data/clean_boulder_weather.csv' $ file_path = web_path $ df = pd.read_csv(file_path)
diff_between_group = mean_per_group.treatment - mean_per_group.control $ diff_between_group
tweet=results[2] $ for param in dir(tweet): $     if not param.startswith("_"): $         print ("%s : %s" % (param, eval("tweet." + param)))
df2=df2.set_index('user_id').join(df_country.set_index('user_id'))
y = api.GetUserTimeline(screen_name="berniesanders", count=20, max_id=935706980643147777, include_rts=False) $ y = [_.AsDict() for _ in y]
%%bash $ p2o.py --enrich --index github_raw --index-enrich github -e http://localhost:9200 \ $ --no_inc --debug github grimoirelab perceval -t '47fb3a29d189e294e3fdc83ecfffb13f737fb684' --sleep-for-rate
df = df[df.Category.isin(df.Category.value_counts()[:8].index)]
f_lr_hash_modeling2 = f_lr_hash_modeling2.withColumn('id', col('id').cast('long')) $ f_lr_hash_test = f_lr_hash_test.withColumn('id', col('id').cast('long'))
sox = sox[pd.to_datetime(sox.date).isin(bad_dates) == False] $ sox = sox[pd.to_datetime(sox.date) >= dt.datetime(2013,1,1)] $ sox.reset_index(drop=True, inplace=True)
old_page_converted = np.random.binomial(n_old, p_old)
df.head()
rounds_df[(rounds_df.announced_on >= '2015-08-01')].shape[0]
with open('Al-fcc.poscar', 'w') as f: $
a_result_1 = df1.append([df3]) # same as option 1 above $ a_result_1
df['car_age'] =  (last_seen.year - df.yearOfRegistration).apply(lambda x: int(x))
np.median([len(h.tweets) for h in heap])
metadata = pd.read_excel('PPB_gang_records_UPDATED_100516.xlsx',header=3,sheetname=1)
data.plot()
print(f'standard deviation of tracking error: {np.std(pipe.tracking_error)}')
air_reserve.head() $
train_df.info()
cercanasAfuerteApache = dataLatLon.loc[(dataLatLon['distanciaAfuerteApache'] > 1000) & (dataLatLon['distanciaAfuerteApache'] < 2000), :] $ cercanasAfuerteApache.loc[:, 'price'] = cercanasAfuerteApache['price_usd_per_m2'] * cercanasAfuerteApache['surface_total_in_m2'] $ cercanasAfuerteApache
df = pd.read_csv("en-wikipedia_traffic_200801-201709.csv", sep='\t')
tweets3.text[0]
tweet_image_clean.head(3)
rtitle = [x.text for x in soup.find_all('a', {'data-event-action':'title'})] $ rtitle.pop(0)
for v in data.values(): $     if v['answers']['Q1'] == 'yes': $         v['answers']['Q1A'] = 0
clf.score(X, y)
old_page_converted = np.random.choice([1,0], size = n_old, p = [p_old,(1-p_old)])
clean_madrid.dropna(inplace=True) $ clean_madrid.reset_index(drop=True,inplace=True)
num_clusters = 3 $ km = KMeans(n_clusters=num_clusters, random_state=0).fit(tfidf_matrix)#TODO $ clusters = km.labels_.tolist()
K = 1000 $ N = K * 23 $ BALANCE = 1.0 $ QUANTILE = 0.9
time_diff = merged_data['invoices_creation_date'].max() - merged_data['invoices_creation_date'].min() 
train.shape, train.is_attributed.mean()
C_Group = df.query('group == "control" and landing_page == "new_page"').count()[0] $ T_Group = df.query('group == "treatment" and landing_page == "old_page"').count()[0] $ print ("Number of times not lined up is {}".format(C_Group+T_Group))
avg_day_of_month14.to_excel(writer, index=True, sheet_name="2014")
import pytz $ mountain_tz = pytz.timezone("US/Mountain") $ eastern_tz = pytz.timezone("US/Eastern") $ mountain_tz.localize(now),eastern_tz.localize(now)
maxCol=lambda x: max(x.min(), x.max(), key=abs) $ journeys['acceleration'] = journeys[['x', 'y', 'z']].apply(maxCol,axis=1) $
np.random.seed(123) $ a=np.random.randint(-100,100,size=(5,20)) $ plt.plot(a) $ plt.show()
tweet_info = df.filter(['id','favorite_count','retweet_count'], axis=1) $ tweet_info.head()
soup.find('div', class_='movie-add-info left').find_all('li')
df = pd.DataFrame(precipitation_2yearsago, columns = ["prcp","date"]) $ df = df.set_index("date") $ print(df)
airlines_day_unstacked["date"] = airlines_day_unstacked["date"].apply(lambda x: x.strftime('%Y-%m-%d'))
df["numerized_tokens"].iloc[0][:20]
print 'There are far less non smokers and the mean cholesterol of the population is different' $ print 'Mean chol. smoker: {}'.format(yes.chol.mean()) $ print 'Mean chol. non smoker: {}'.format(no.chol.mean())
df_input_clean.filter("`Resp_time` <= 0").groupBy("Resp_time").count().sort(desc("count")).show(50)
df_h1b_mv_ft.pw_1.hist(bins=20,figsize=(8,8))
Project = pd.read_csv('ProjectInventorySales_Daily.csv', encoding='ISO-8859-1') $ Project = Project.copy(deep=True)
joined_test[dep] = 0 $ joined_test = joined_test[cat_vars+contin_vars+[dep, 'Date', 'Id']].copy()
df = pd.read_csv('YearPredictionMSD.txt', sep=",", header=None) $ df.head()
text = [] $ for ind, row in talks.iterrows(): $     text.append(' '.join([row['description'],row['keywords'],row['title']]))
automl.fit(X_train, y_train, dataset_name='psy_prepro')
autos.loc[autos["registration_year"].between(1950, 2018), "registration_year" ].shape
dates_with_nulls=len(nulls['date'].unique()) $ all_dates=len(merged['date'].unique()) $ dates_with_nulls/all_dates
np.mean(df.query('group == "control"')['converted'])
sum(tweet_image_clean.duplicated())
def find_DRGs_in_given_year(year): $     idx = df_providers[(df_providers['year']==year)].index.tolist() $     list_DRG_year = df_providers.loc[idx,'drg'].unique() $     return list_DRG_year
from sklearn.model_selection import GridSearchCV
thisWeek = tweets.loc[tweets['created_at'] > endDay] $ tweets = tweets.loc[tweets['created_at'] < endDay]
rcn_in_proj = h2020_proj["projectRCN"].unique() $ missing_rcn = h2020_part.loc[~h2020_part["projectRCN"].isin(rcn_in_proj),"projectRCN"]
rfc = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=0) $ cv_score = cross_val_score(rfc, features_class_norm, overdue_transf, scoring='roc_auc', cv=5) $ 'Mean ROC_AUC score: {:.3f}, std: {:.3f}'.format(np.mean(cv_score), np.std(cv_score))
df.head(5)
print '%d tweets after quake out of %d by users active before quake (%.2f%%)' % \ $ (beforeUsersAfter.shape[0],fullDf[fullDf.index>pd.datetime(2015,4,25)].shape[0],float(beforeUsersAfter.shape[0])/fullDf[fullDf.index>pd.datetime(2015,4,25)].shape[0])
clf.fit(random_features,random_labels)
factors = web.DataReader("Global_Factors","famafrench") $ factors
[ print(c, " - ", str(len(california_house_dataframe[c].unique()))) for c in california_house_dataframe.columns]
count_np_not_treat = df.query('landing_page == "new_page" & group != "treatment"') $ count_op_not_ctrl = df.query('landing_page == "old_page" & group != "control"') $ count_np_not_treat.shape[0] + count_op_not_ctrl.shape[0]
lfiles.sort(key=sort_files)
df.loc[df.toes.str.match(pattern2)==True]
from bs4 import BeautifulSoup $ soup = BeautifulSoup(resp.text, 'lxml')
autos['price'].value_counts().sort_index(ascending = False).head(15)
against.groupby(['contributor_firstname','contributor_lastname'])['amount'].sum().reset_index().sort_values('amount', ascending = False)
bucket.upload_dir('data/heat-pump/proc/', 'heat-pump/proc', clear_dest_dir=True)
(etsamples_engbert,etmsgs_engbert,etevents_engbert) = be_load.load_data(algorithm='') $ raw_large_grid_df_engbert = condition_df.get_condition_df(data=(etsamples_engbert,etmsgs_engbert,etevents_engbert),condition='LARGE_GRID')
embeddings_matrix.shape
new_reps.Cruz.astype("float64").describe()
tmp_df = geocoded_df.copy() $ tmp_df[[x for x in tmp_df.columns if x.endswith('Date')]] = tmp_df[[x for x in tmp_df.columns if x.endswith('Date')]].astype('str') $ tmp_df['Judgment.Date'].head()
plots.duration_analysis(scale = "log") #Distribution analysis for Normal and Malicious traffic in log scale
BPL_electric.tail()
df.sort_values('prob_off').head()
prob_control = (df2.query('group=="control"')['converted']==1).mean() $ print('Probability converted given that individual in control group: ' + str(prob_control))
import pandas_datareader.data as web $ from datetime import datetime
test.head()
plt.rcParams['axes.unicode_minus'] = False $ dta_51.plot(figsize=(15,5)) $ plt.show()
plot_correlation_map(labeled) $ labeled.corr()
import pickle $ output = open('speeches_cleaned_2016.pkl', 'wb') $ pickle.dump(speeches_df4, output) $ output.close()
print(ts.month) $ print(ts.day)
control_convert = df2.query('group =="control"').converted.mean() $ print("Probability of control group converting is :", control_convert)
portfolio_df.info()
one_station=df_daily5.loc[df_daily5['STATION']=='1 AV'] $ one_station.head()
ks_projects['duration'] = ks_projects['deadline'] - ks_projects['launched'] $ ks_projects['duration'] = ks_projects['duration'].apply(lambda x: x.days) $ ks_projects.set_index('name', inplace=True) $ ks_projects.head(5)
print(autos['odometer_km'].unique().shape) $ autos['odometer_km'].describe()
test.head()
old_compiled_data.describe()
logreg = LogisticRegression() $ logreg.fit(X_train,y_train)
actual_old = df2.query('converted == 1 and landing_page=="old_page"').count()[0]/n_old
log_mod3 = sm.Logit(df3['converted'], df3[['intercept', 'US', 'UK']]) $ results3 = log_mod3.fit()
resampled_groups = Exchange_df.groupby(['Field', 'Sales_in_CAD']).sum() $ resampled_groups.head()
df.head(10)
to_be_predicted_Day3 = 31.34422603 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
joinresult = data.join(meta, on="BARCODE") $ joinresult.reset_index(inplace=True)
for obj in cos.Bucket(buckets[0]).objects.all(): $     print('Object key: {}'.format(obj.key)) $     print('Object size (kb): {}'.format(obj.size/1024))
people['a'].loc['Joe']
results2.summary()
final=final[(final.dec<30)&((final.ra<195.)|(final.ra>330.))] $ print final.shape, final.groupby('priority',axis=0).get_group('A').shape, final.groupby('priority',axis=0).get_group('B').shape,\ $ final.groupby('priority',axis=0).get_group('C').shape, final.groupby('priority',axis=0).get_group('D').shape $
goo2.head()
mySql = spark.sql('SELECT dateTime, endpoint FROM AccessLog WHERE responseCode = 403') $ mySql.show() $
polarity_avg.rename(columns={'index':'canton'}, inplace=True) $ polarity_count.rename(columns={'index':'canton'}, inplace=True)
results = logit_mod.fit() $ results.summary2()
sess.get_data('ibm us equity',['px last','px open','px high','px low'])
df_json_tweets.duplicated()
status.shape
pd.set_option('display.max_columns', 100)
joined = join_df(df1, df2) $ len(joined[joined.Close.isnull()])
groupby_example = pandas.DataFrame({'key': ['a', 'b', 'a', 'b'], $                                     'value': [1,2,1,2]})
session.query(Measurements.date, Measurements.station, Measurements.prcp, Measurements.tobs).\ $       filter(Measurements.date == query_date).all()
autos['date_created'].describe()
dd=cfs.diff_abundance('Subject','Control','Patient', random_seed=2018)
data_compare['SA_mix'].plot() $ data_compare['SA_google_translate'].plot() $ data_compare['SA_textblob_de'].plot()
pop_df.stack()
theft.iloc[0:5]
abc3 =prediction['vova']*0.5 + prediction['rf']*0.2 + prediction['lgb_s']*0.3
cog_simband_times.drop(labels=['CC-OTS BHC0109-1', 'DH-OTS BHC0242-1','JA-OST BHC0258-1'], inplace=True)
inspector = inspect(engine) $ columns = inspector.get_columns('measurement') $ for column in columns: $     print(column["name"], column["type"])
xpdraft2 = xpdraft1.groupby("name").status.value_counts() $ xpdraft2.head()
df_ad_state_metro_1['sponsors'].value_counts()
titanic.deck.value_counts()
sub_df = pd.DataFrame({"click_id":test["click_id"], "is_attributed":is_attributed})
giss_temp.fillna(value=0).tail()
deaths.head()
stop_words = nltk.corpus.stopwords.words('english') $ print(stop_words) $ print('Count: {}'.format(len(stop_words)))
import pandas as pd $ url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.json' $ df = pd.read_json(url, orient='columns') $ df.head(2) $
station_count = station_df['Station ID'].nunique() $ print(station_count)
(autos["brand"].value_counts(normalize=True)*100).head(20)
xgb = XGBClassifier(objective='binary:logistic') $ xgb.fit(X_train, y_train) $ test_predictions = xgb.predict(X_test) $ eval_sklearn_model(y_test, test_predictions, model=xgb, X=X_test)
Results_kNN2500.to_csv('soln_kNN2500.csv', index=False)
to_be_predicted_Day4 = 36.4833672 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
extension=(results.split("url('",1)[1]).rstrip("');")
print("Number of unique users: {}".format(df.user_id.nunique()))
MergeWeek.set_index('Date').to_csv('Sales_WeeklySummary.csv')
soup.body
real_new = len(df2.query('group == "treatment" & converted == 1'))/len(df2.query('group == "treatment"')) $ print(real_new)
pd.set_option('display.max_columns', 65) $ df.head() $
(autos["date_crawled"] $         .str[:10] $         .value_counts(dropna=False) $         .sort_values() $         )
predictions = loadedModelArtifact.model_instance().transform(predict_data)
weights = w.sort_index().values
ts / ts.shift(1)
weather.set_index((weather['date'] - day_zero).apply(lambda d: d.days), inplace=True) $ weather.sort_index(inplace=True)
feature_imp_RF= feature_imp_RF.sort_values('RF_imp',ascending=False) $ feature_imp_RF[:15]
df.isnull().user_id.sum()
df_25year=df[df['date']>'1991-02-24'] #last 25 years $ df_25year.groupby('origin')['description'].agg('count').sort_values(ascending=False).head(1)
from pandas.plotting import autocorrelation_plot $ autocorrelation_plot(CH_electric['Total_Demand_KW'])
testdata = np.load(outputFile) $ data = pd.read_csv(inputFile, delimiter=',', header=None)
corn_vege.sum()
duplicated = df2[df2.duplicated(['user_id'], keep=False)] $ duplicated['user_id']
week36 = week35.rename(columns={252:'252'}) $ stocks = stocks.rename(columns={'Week 35':'Week 36','245':'252'}) $ week36 = pd.merge(stocks,week36,on=['252','Tickers']) $ week36.drop_duplicates(subset='Link',inplace=True)
autos["last_seen"].str[:10].value_counts(dropna = False, normalize = True).sort_index()
Grouping_Year_DRG_discharges_payments.groupby('drg3').head()
df3.groupby(['TUPLEKEY2', 'DATE']).sum()
c['location'] = c['location'].str.upper() $ c['location'].value_counts(dropna=False)
print(np.shape(log_S)) $ plt.plot(log_S.T, "*") $ plt.show() $ print(9144/394) $ print(sr)
lr.fit(pscore, btc_price)
rng = pd.date_range('3/6/2017 00:00', periods=5, freq='M') $ ts1 = pd.Series(np.random.randn(len(rng)), rng) $ ts1
df_tot.sample(10)
names = open("../data/other_data/first_names.txt").read().split("\n") $ names_pattern = re.compile(r'\b(?:{})\b'.format('|'.join(names)))
archive_df_clean.head(10)
co_buildings_latlong = pd.merge(CO_profit, cpq_status, on='Building ID', how='inner') $ tx_buildings_latlong = pd.merge(TX_profit, cpq_status, on='Building ID', how='inner') $ ga_buildings_latlong = pd.merge(GA_profit, cpq_status, on='Building ID', how='inner')
bobby_ols = ols('opening_gross ~ star_avg',dftouse_seven).fit() $ bobby_ols.summary()
df1['Adj. Close'].plot() $ df1['forcast'].plot() $ plt.legend(loc=4) $ plt.xlabel('Date') $ plt.ylabel('Price') $
merge.to_csv('Data/Incidents.csv') $
pd.read_csv('myfile.csv', index_col=0)
list_DRGs_common_ALL_years = DRGs_common_all_years() $ list_DRGs_common_ALL_years.sort() $ list_DRGs_common_ALL_years[:5]
twosample_sub = scipy.stats.ttest_ind(locationing.subjectivity, tweetering.subjectivity) $ twosample_sub
duplicated_user_id = df2.set_index('user_id').index.get_duplicates() $ duplicated_user_id
newdf.to_csv('bitcoin_semscore.csv')
unemp.quarter = pd.to_datetime(unemp.quarter.values)
images_predictions.nunique()
cust_demo.age.hist(bins=50, color='R')
df1_clean = df1_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis=1)
data.head()
finals.to_csv("player_season_statistics.csv", index=False)
talks_train.to_json('train2.json')
tweets_df.in_reply_to_status_id.describe()
join_e.select('party_Id_orig').distinct().count()
train_view.sort_values(by=0, ascending=False)[0:10]
tokens['one_star'] = tokens.one_star / nb.class_count_[0] $ tokens['five_star'] = tokens.five_star / nb.class_count_[1]
df.dtypes
df2.dropna(inplace=True)
data.loc['Colorado', ['two', 'three']]
price2017.dtypes
model.doesnt_match("france england germany berlin".split())
print max((df['Announced At'] - df['Created At']).astype('timedelta64[h]')) $ print min((df['Announced At'] - df['Created At']).astype('timedelta64[h]'))
old_page_converted = np.random.choice([0, 1], size=old, p=[(1-x), x]) $ old_page_converted.mean()
def oppConversion(x): $     p = '' $     if type(x.lead_converted_date) == pd.tslib.Timestamp: p = 'convertedLead' $     return p
morning_rush.iloc[:1000][['longitude', 'latitude']].get_values()[0]
df['month'] = df.month.map({1: 'jan', 2: 'feb', 3: 'mar', 4:'apr', 5:'may', 6:'jun', 7:'jul', 8: 'aug', 9: 'sept', 10: 'oct', 11: 'nov', 12: 'dec'})
ip['p3_conf'].describe()
file3=file2.filter(file2.trans_start!=file2.meter_expire) $ file3.show(3)
kk[kk.user_id==4]
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=31000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
unique_users=df['user_id'].nunique() $ print("The total number of unique users are : {}".format(unique_users))
details.sort_values(by='Released')
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Pothole') ]['Day of the week'].value_counts().head() $
msft.dtypes
daily_df.reset_index(inplace=True)
def get_parent(acctId): $     matches = df[df['acctId'] == acctId] $     return matches[:1]['account_id'].to_string(index=False) $ get_parent(766)
credentials_file = 'oauth.json' $ yfs2 = yfs_test(credentials_file)
X_train, X_test, y_train, y_test = train_test_split(X_d, y, test_size=0.5, random_state=42) $
s519397_df["prcp"].mean()
tweet_archive_clean['new'] = tweet_archive_clean.text.str.extract('(?P<new>(\d+).(\d+)+/\d+)', expand=False)['new']
assortativity = nx.attribute_assortativity_coefficient(multiG, 'Block') $ print assortativity
daily_cases.unstack().head()
rng.is_month_end
top_songs['Streams'].dtype
resp = json.loads(r.content)['response'] $ pd.DataFrame(resp)
Image("/Users/jamespearce/repos/dl/data/dogscats/train/dog.7223.jpg")
Precipitation_DF.plot(rot=45,title="Precipitation from %s to %s"%(start_date,end_date),figsize=(8,5),grid=None,colormap="PuOr_r") $ plt.show()
print(np.exp(res.params))
total.head()
from sklearn.metrics import classification_report, confusion_matrix $ print(classification_report(y_test, rf_yhat))
r = requests.get("https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=", auth=('', '')) $
Returning_df = Type_date.groupby(['New_or_Returning']).get_group('Returning ').reset_index() $ Returning_df.head()
data.to_csv("raw/1_cleaning_translated.csv", encoding="utf-8")
odometer.unique().shape
print(X_resampled.shape, y_resampled.shape)
df.converted.sum()/df_length
df[df['Agency'] == 'NYPD']['Unique Key'].resample('M').count().plot() $ df[df['Agency'] == 'DOT']['Unique Key'].resample('M').count().plot()
pt_all.sort_values(by=['uid'])
predictions.printSchema()
vader_df.tail()
df1.device_id[0],df2.device_id[0],df3.device_id[0],df4.device_id[0],df5.device_id[0],df6.device_id[0]
dfClean = df['2004':'2017'].copy() $ byYear = dfClean.resample('y').count()['id'] $ activeByYear = dfClean[(dfClean['stats.activeUsers']>=1)&(dfClean['users_1']>0)].resample('y').count()['id'] $ activeByYear
wrd_clean.query('name == "a"')
order_data.head()
fan_zip = [zipcode.isequal(str(zc)) for zc in questions['zipcode']] $ questions['zipcode'] = fan_zip
s_mean_df = pd.DataFrame(station_mean, columns=["date", "avg_prcp"]) $ print(len(s_mean_df.index)) $ s_mean_df.info() $ s_mean_df.head(5)
%sql \ $ SELECT twitter.tag_text, count(*) AS count \ $ FROM twitter \ $ WHERE twitter_day = 9 \ $ GROUP BY tag_text ORDER BY count DESC LIMIT 1;
print('number of breeds: {}'.format(len(tweet_archive_master['dog_breed'].value_counts())))
from scipy.stats import norm $ print(norm.cdf(z_score)) $ print(norm.ppf(1-(0.05))) $
n_new_page = len(df2.query("group == 'treatment'")) $ print(n_new_page)
MetaMetaclass.__call__(MetaClass,'Example', (), {}) $
train_ratio = 0.75 $ train_size = int(samp_size * train_ratio); print(train_size) $ val_idx = list(range(train_size, len(df)))
app_pivot['Total'] = [0 for i in range(len(app_pivot))] $ app_pivot['Total'] = app_pivot.Application + app_pivot['No Application'] $ app_pivot
with open(households_yaml_path, 'r') as f: $     households = yaml.load(f.read()) $ for k, v in households.items(): $     print(yaml.dump({k: list(v['feeds'].keys())}, default_flow_style=False))
print(" \n |".join(list(tweets.loc[300086:300094]["content"])))
counts = tokaise.groupby(Grouper(key="stamp",freq='3H')).agg({"id":"count"}).rename(columns={"id":"count"}) $ counts.head()
test_collection.find({"account": "deluxetattoochicago"}).count()
print('Most dril: {}'.format(tweets_pp[tweets_pp.handle == 'wint'].sort_values('dril_pp', ascending=False).text.values[0])) $ print('Least dril: {}'.format(tweets_pp[tweets_pp.handle == 'wint'].sort_values('dril_pp', ascending=True).text.values[0]))
df_B.to_csv("classB.csv",index=None)
newdf['Date'] = newdf['Date'].apply(lambda x: pd.to_datetime(x, format= '%Y%m'))
['state'] = df_ad_airings_5['location'].map(lambda x: x.split(","))
from sklearn import preprocessing $ le = preprocessing.LabelEncoder()
y = pd.Series(data.target) $ y
df = df.append(pd.DataFrame([{'age':20,'gender':'F','name':'qoo'}]),ignore_index=True)
df_chapters_read.head(3)
repos.groupby(repos.created_at.dt.year).size().plot(kind='bar')
df_train.columns.values[np.where(df_train.columns.values != 'date_first_booking')]
import gym.wrappers $ env_monitor = gym.wrappers.Monitor(make_env(),directory="videos",force=True) $ sessions = [evaluate(env_monitor, agent, n_games=1) for _ in range(100)] $ env_monitor.close()
freq = Counter(p for o in df.tokenized_text for p in o) $ freq.most_common(25)
df_rt = df[df.text.str.contains('^RT')] $ df_rt.head()
md_keys = ['   ' + s + ':' for s in METADATA_KEYS] $ md_idx = tmpdf_md.index[tmpdf_md[tmpdf_md.isin(md_keys)].notnull().any(axis=1)].tolist() $ md_idx
unsorted_df.sort_values(by=['col2','col1'],ascending=False)
user_clients = toggl.request("https://www.toggl.com/api/v8/clients")
pivot = (pd.pivot_table(data_predict,index=['store','item'],columns=[freq],values=['sales', 'predict', 'variance'],aggfunc=np.sum,margins=True, margins_name='Total') $          .swaplevel(axis=1) $          .sortlevel(0, axis=1, sort_remaining=False) $         ) $ pivot
learner.clip = 0.3
liberiaDf = pd.concat([liberiaCases, liberiaDeaths],axis=1) $ liberiaDf.index.name = 'Date' $ liberiaDf.head()
All_tweet_data_v2.name[All_tweet_data_v2.name.str.len() < 3].value_counts()
table = df[['retweet_count', 'favorite_count']].groupby(by=df.source).agg(np.median) $ table.sort_values(by = 'retweet_count')
raw['hash'] = raw['title'] + raw['company'] + raw['location'] + raw['parse_date'].dt.strftime('%Y%U')
! pip install pandas=0.19.1 --yes $ ! conda install pandas=0.19.2 --yes
records3.loc[(records3['Graduated'] == 'Yes') & (records3['Age'].isnull()), 'Age'] = grad_age_mean $ records3.loc[(records3['Graduated'] == 'No') & (records3['Age'].isnull()), 'Age'] = non_grad_age_mean
from datetime import datetime $ date_lis = [datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in test_df.created]
apple.sort_index(inplace=True) $ apple.head()
for v in squares.iteritems(): $     print(v)
autos.price.value_counts().sort_index().head(10)
train.created.max(), train.created.min()
pres_df['state'] = pres_df['split_location_tmp'].map(lambda x: x[1]) $ pres_df['state'].head()
resampled1_groups = resampled_groups.reset_index()
rain_result = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date > year_ago).\ $     order_by(Measurement.date).all() $ rain_result
sum(data.c1_date.isnull())
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_6203.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_6203, lags=40, ax=ax2)
autos["brand"].value_counts(normalize = True).head(4)
a = 5; b = 4.5
df2[df2.duplicated(['user_id'], keep=False)]['user_id']
test_words = cv.transform(fb_test.message)
train.created.dtype
cur.execute(sql_all_tables) $ all_tables_df = pd.DataFrame( $     cur.fetchall(), columns=[rec[0] for rec in cur.description])
old_page_converted = np.random.binomial(n_old,p_old) $
len(df[~(df.event_properties == {})])
lv_workspace.get_subset_object('B').get_step_object('step_1').show_settings() $ lv_workspace.get_subset_object('B').get_data_filter_object(step=1).exclude_list_filter
lat_25_1 = (suspects_with_25_1['lat'] - 1)*100 $ lng_25_1 = (suspects_with_25_1['lng'] - 103) * 100 $ minute_25_1 = (suspects_with_25_1['timestamp'].dt.day-1)*24*60 + suspects_with_25_1['timestamp'].dt.hour*60 + suspects_with_25_1['timestamp'].dt.minute $ sec_25_1 = minute_25_1*60 + suspects_with_25_1['timestamp'].dt.second $ day_25_1 = suspects_with_25_1['day']
df.groupby('Year').agg({'Points' : np.sum,'Rank' : np.mean})
so.shape
pgh_311_data[today.strftime('%Y-%m-%d')]
import pandas as pd $ url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies' $ df = pd.read_html(url, index_col=None,skiprows=1)[0] $ df.columns=['Ticker symbol','Security','SEC filings','GICS Sector','GICS Sub Industry','Address of Headquarters','Date first added','CIK'] $ df.head()
df1=data.rename(columns={'SA':'Polarity'}) $ df1.head()
dc2015 = dc[(dc['created_at'] >= '2015-03-30') & (dc['created_at'] <= '2015-05-07')] $ dcelse = dc[(dc['created_at'] < '2015-03-30') | (dc['created_at'] > '2015-05-07')] $ if ((dc2015.shape[0] + dcelse.shape[0]) == dc.shape[0] ): $     print("Ok")
joined['PubDate'] = pd.to_datetime(joined['PubDate']) $ joined['Weekday'] = joined['PubDate'].dt.weekday $ joined['Hour'] = joined['PubDate'].dt.hour
len(df[df.location_id == df.prev_location_id])
londonDFSubsetWithCounts.head()
autos.columns
df_t.sort_values(by='timestamp').head(2)
pd.Series(baseball_newind.index).value_counts()
from scipy.stats import norm $ norm.cdf(z_score)
users = pd.read_csv('./data_airbnb firstdestinations/train_users.csv')
df_clean2['tweet_id'] = df_clean2['tweet_id'].astype(str)
df3[['CA', 'US']] = pd.get_dummies(df3['country'])[['CA', 'US']]
gensim.models.Word2Vec
autos['brand'].value_counts(normalize = True)
baby_scn_postgen.to_csv('Paired_Activated_postgen_61218.csv')
print(players.shape) $ players.head()
print store_preds_2016['PredictedSales'].sum() $ store_preds_2016['PredictedSales'].head()
image_predictions_df.tail(3)
df_imputed_median_NOTCLEAN1A.head(5)
DataAPI.write.update_factors(factors=['GROWTH'],trading_days=trading_days, override=True, log=False)
movies.head(5)
!jupyter nbconvert --to html "simple examples.ipynb"
kick_projects = pd.merge(kick_projects, ks_particpants, on = ['category', 'launched_year', 'launched_quarter','goal_cat_perc'], how = 'left')
df_pageviews_desktop.head()
df[df.hash.duplicated(keep=False)]
joined_samp.head(2)
c = df2[df2['group']=='treatment']['converted'].mean() $ b = df2[df2['group']=='control']['converted'].mean() $ actual_diff =c-b; $ actual_diff
from sklearn.model_selection import KFold $ cv = KFold(n_splits=200, random_state=None, shuffle=True) $ estimator = Ridge(alpha=10000) $ plot_learning_curve(estimator, "Ridge Regression", X_std, y, cv=cv, train_sizes=np.linspace(0.2, 1.0, 10))
chefdf = pd.merge(chefdf, chef01df,  how='left', left_on=['name','user'], $                   right_on = ['name','user'], suffixes=('','_01'))
bruins.reset_index(drop=False, inplace=True) $ bruins.rename(columns={'index':'game_id'}, inplace=True)
full_globe_temp = pd.read_table(filename, sep="\s+", names=["year", "mean temp"], $                                 index_col=0, parse_dates=True) $ full_globe_temp
pd.DataFrame(stadium_arr.groupby(['home_team','away_team'])['arrests'].sum().sort_values(ascending=False).reset_index())[:10] $
cityID = '013379ee5729a5e6' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Tucson.append(tweet) 
tweet_en['text'].apply(lambda x: "I'm at" in x).value_counts()
print('Train...') $ model.fit(x_train, y_train, $           batch_size=batch_size, $           epochs=n_epoch, $           validation_data=(x_test, y_test))
transactions.merge(users, how='inner', on=['UserID'])
new_read = pd.merge(is_read, articles[['id','read_time']], how='inner', left_on=['article_id'], right_on = ['id']) $ print(new_read.shape) $ new_read.head() 
import statsmodels.api as sm $ df3['intercept'] = 1 $ log = sm.Logit(df3['converted'], df3[['intercept', 'US', 'CA']]) $ results = log.fit() $ results.summary()
df.plot() $ plt.show()
d = np.array([0, -1]) $ print(a * d) $ for i in range(2): $     print(a[i] * d)
season13 = ALL[(ALL.index >= '2013-09-05') & (ALL.index <= '2014-02-02')]
df_usertran = pd.merge(users,transactions,how='left',on='UserID') $ df_ = df_usertran.drop_duplicates(subset='UserID') $ df_ = df_.reset_index(drop=True) $ df_
url = 'https://mars.nasa.gov/news/'
education_data.drop('Category', axis=1, inplace=True)
duplicate_rows = df2[df2.user_id == repeated_user_id] $ duplicate_rows
liquor2016_q1_features = liquor2016_q1_profit.merge(liquor2016_q1_whisky,left_index=True,right_index=True)\ $ .merge(liquor2016_q1_volume,left_index=True,right_index=True)\ $ .merge(liquor2016_q1_days,left_index=True,right_index=True) $ liquor2016_q1_features.head()
np.mean(df.query('group == "treatment"')['converted'])
user1 = df[df.In == people[0]] $
cv_fitted, cv_data, tfidf_fitted, tfidf_data = mf.vectorize_both_ways(epoch3_df, 'cleaned_text')
token_send_add_receiveAvg_month = token_sendreceiveCnt.groupby("ID").agg({"sendReceiveCnt":mean_except_outlier}).reset_index()
ntp_epoch = datetime.datetime(1900, 1, 1) $ unix_epoch = datetime.datetime(1970, 1, 1) $ ntp_delta = (unix_epoch - ntp_epoch).total_seconds() $ def ntp_seconds_to_datetime(ntp_seconds): $     return datetime.datetime.utcfromtimestamp(ntp_seconds - ntp_delta).replace(microsecond=0)
y_test.shape
raw_data.head()
df2.converted.mean()
df_combined['country'].unique()
df_EMR_dd_dummies_no_sparse = df_EMR_dd_dummies.drop(columns=ls_sparse_cols)
ek.get_data(['USGDPF=ECI'],['GN_TXT16_4']) $ ek.get_timeseries(['aCNFRTRRAW'], interval='monthly')
to_be_predicted_Day4 = 21.39121267 $ predicted_new = ridge.predict(to_be_predicted_Day4) $ predicted_new 
import json $ tweets = api.search(q = "#modi", count = 100) $ for tweet in tweets: $     posts.insert_one(tweet._json)
style.use('ggplot')
print "# of rows: {0} \n# of columns: {1}".format(trip_data.shape[0], trip_data.shape[1])
round_count['count'].sum()
df_goog.head(3)
tree_c_features = c_features.apply(class_le.fit_transform)
liveonly_live_woc.head()
learn.fit(lr,3,cycle_len=1,cycle_mult=2)
mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page', 'CA', 'UK', 'interaction_ca_ab_page', 'interaction_uk_ab_page']]) $ results = mod.fit() $ results.summary()
!ls ../src
indexed = df.ix['2017-06-01 00:00:00':'2018-06-01 00:00:00'] $ indexed.head()
companies = ["WIKI/ATVI.11","WIKI/ADBE.11","WIKI/AKAM.11","WIKI/ALXN.11","WIKI/GOOGL.11","WIKI/AMZN.11","WIKI/AAL.11","WIKI/AMGN.11","WIKI/ADI.11","WIKI/AAPL.11","WIKI/AMAT.11","WIKI/ADSK.11","WIKI/ADP.11","WIKI/BIDU.11","WIKI/BIIB.11","WIKI/BMRN.11","WIKI/CA.11","WIKI/CELG.11","WIKI/CERN.11","WIKI/CHKP.11","WIKI/CTAS.11","WIKI/CSCO.11","WIKI/CTXS.11","WIKI/CTSH.11","WIKI/CMCSA.11","WIKI/COST.11","WIKI/CSX.11","WIKI/XRAY.11","WIKI/DISCA.11","WIKI/DISH.11","WIKI/DLTR.11","WIKI/EBAY.11","WIKI/EA.11","WIKI/EXPE.11","WIKI/ESRX.11","WIKI/FAST.11","WIKI/FISV.11","WIKI/GILD.11","WIKI/HAS.11","WIKI/HSIC.11","WIKI/HOLX.11","WIKI/IDXX.11","WIKI/ILMN.11","WIKI/INCY.11","WIKI/INTC.11","WIKI/INTU.11","WIKI/ISRG.11","WIKI/JBHT.11","WIKI/KLAC.11","WIKI/LRCX.11","WIKI/LBTYA.11","WIKI/MAR.11","WIKI/MAT.11","WIKI/MXIM.11","WIKI/MCHP.11","WIKI/MU.11","WIKI/MDLZ.11","WIKI/MSFT.11","WIKI/MNST.11","WIKI/MYL.11","WIKI/NFLX.11","WIKI/NVDA.11","WIKI/ORLY.11","WIKI/PCAR.11","WIKI/PAYX.11","WIKI/PCLN.11","WIKI/QCOM.11","WIKI/REGN.11","WIKI/ROST.11","WIKI/STX.11","WIKI/SIRI.11","WIKI/SWKS.11","WIKI/SBUX.11","WIKI/SYMC.11","WIKI/TSLA.11","WIKI/TXN.11","WIKI/TSCO.11","WIKI/TMUS.11","WIKI/FOX.11","WIKI/ULTA.11","WIKI/VRSK.11","WIKI/VRTX.11","WIKI/VIAB.11","WIKI/VOD.11","WIKI/WBA.11","WIKI/WDC.11","WIKI/WYNN.11","WIKI/XLNX.11"]#"WIKI/PCLN.11",
with_condition_heatmap_query4= folium.Map([41.90293279, -87.70769386], $                zoom_start=11) $ with_condition_heatmap_query4.add_child(plugins.HeatMap(final_location_ll[:40000], radius=15)) $ with_condition_heatmap_query4
df_blacklist = pd.read_csv(read_inserted_table(dumpfile, tablename),delimiter=",",error_bad_lines=False) $ df_blacklist.rename(inplace=True, columns={'ID':'id', $                                           'IP':'ip'}) $ df_blacklist.head(10)
conn.execute(sql) $ conn.execute(sql) $ conn.execute(sql)
df.to_csv('ab_edited.csv', index=False)
users[users.id == 28]
legos['colors'].head()
rshelp.query("SELECT COUNT(*) FROM postgres_public.parking_spot WHERE ev_charging IS NOT NULL;")
print(page_soup.prettify())
df_variables = pd.read_csv("../01_data preprocessing/data new/variables.csv",encoding="utf-8",sep=",")
listings_unique_ids = list(listings['id'].unique()) $ len(listings_unique_ids) $
len(pd.unique(tag_df.values.ravel()))
print(ozzy.buddy.name) $ print(ozzy.buddy.age)
df.plot(x='observ_time',y='observ_prcp',kind = 'bar',title = 'Precipitation Over Time') $ plt.show()
tfav.plot(figsize=(16,4), label="Likes", legend=True)
history_with_target.loc[~history_with_target.target.isnull(), ['target', 'target_test','time_delta', 'next_incident_type', 'MOTIF_ANNULATION_CODE', 'ACTUAL_START_DATE', 'ACTUAL_END_DATE', 'INCIDENT_TYPE_NAME', 'SCHEDULED_START_DATE', 'SCHEDULED_END_DATE',  'INCIDENT_STATUS_NAME', 'TYPE_BI', 'MOTIF_ANNULATION_CODE', 'MOTIF_ANNULATION_DESC']][50:100]
n_new = df2.query('landing_page == "new_page"')['user_id'].count() $ print(n_new)
tweet_info.info()
pyLDAvis.sklearn.prepare(lda_tfidf, X, vectorizer, R=15)
df1['io_state']= df1.io_state.apply(lambda x: x.zfill(8)) $
nold = df2.query('group == "control"').user_id.nunique() $ nold
p_new = df2['converted'].mean() $ print(" Actual values of Probability of conversion for new page (p_new):", p_new)
fulldata_copy.head()
most_recent = session.query(Measurements.date).order_by(Measurements.date.desc()).first() $ most_recent_list = most_recent[0].split("-")#split on "-" $ most_recent_list#check
df1.head()
df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize) $ df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize)
scores, metrics = pipeline.test(ds_train, 'Label') $ print("Performance metrics on training set: ") $ display(metrics)
pivoted_table.plot(legend = False, alpha = 0.01)
export_path=cwd+'\\ca_simu_from_python.csv' $ ca_de.to_csv(export_path, index=False)
fda_drugs = pd.read_table('../../static/Products.txt', usecols = ['Form', 'Strength','DrugName', 'ActiveIngredient'])
get_dt_next = lambda ts: ts.diff().shift(1) $ train['from_prev_comment'] = train.groupby('user_id').date_created.apply(get_dt_next).dt.total_seconds() // 60 $ test['from_prev_comment'] = test.groupby('user_id').date_created.apply(get_dt_next).dt.total_seconds() // 60 $ train['from_prev_comment'].fillna(2500000, inplace=True) $ test['from_prev_comment'].fillna(2500000, inplace=True)
df2[df2['group'] == 'treatment'].converted.mean()
y = df2[df2.group == 'treatment'] $ y["converted"].mean() * 100 $
twitter_archive_enhanced.head()
full.info()
df.rename = df $ df
start = datetime.datetime.strptime("2012-09-09", "%Y-%m-%d") $ date_list = [start + relativedelta(weeks=x) for x in range(0,len(df.sales))] $ df['index'] =date_list $ df.set_index(['index'], inplace=True) $ df.index.name=None
autos['odometer'] = autos['odometer'].str.replace("km","").str.replace(',', '') $ autos['odometer'] = autos['odometer'].astype(int) $ autos.rename({"odometer": "odometer_km"}, axis=1, inplace=True) $ autos["odometer_km"].head()
df_new['intercept'] = 1 $ mod = sm.Logit(df_new['converted'], df_new[['intercept', 'US', 'CA']]) $ results = mod.fit() $ results.summary()
active_with_return.dropna(inplace = True)
print('Number of unique values in... :') $ for attr in titanic: $     print("   {attr}: {u}".format(attr=attr, u=len(titanic[attr].unique())))
print("Mean squared error: %.2f" $       % mean_squared_error(y_test, y_pred))
neighborhoods = pd.read_csv('../final_project/neighborhoods.csv') $ neighborhoods.head()
pd.Period('2007-1-1', 'B')
session.query(Measurements.station, func.avg(Measurements.prcp) ).\ $         filter(Measurements.date >= Pre_start_date).\ $            filter(Measurements.date <= Pre_end_date).\ $         group_by(Measurements.station).all() $
print(1/np.exp(-1.9888)) $ print(1/np.exp(-0.0150))
new_model = gensim.models.Word2Vec.load(temp_path)  
from sklearn.ensemble import RandomForestClassifier
time_length.plot(figsize = (16, 4), color = 'r') $ time_fav.plot(figsize = (16,4), color = 'g') $
sf_business = graphlab.SFrame(df_business.reset_index())
list_of_files = [i for i in os.listdir() $              if i.endswith("Sepsis.pdf") and not i.startswith(today) ] $ list_of_files
image.info()
from sklearn.decomposition import PCA as sklearnPCA $ sklearn_pca = sklearnPCA(n_components=3) $ sklearn_transf = sklearn_pca.fit_transform(X)
pd.isnull(df)  # Get the boolean mask where values are NaN
weather_parquet = join(weather_dir, zip_file[:-3]+'parquet') $ print(weather_parquet) $ df = sqlContext.read.load(weather_parquet) $ df.show(1)
from sklearn.utils import resample $ np.random.seed(1) $ err = np.std([model.fit(*resample(X, y)).coef_ $               for i in range(1000)], 0)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')
hashtag_df = pd.DataFrame.from_dict(list(dict(counts).items())) $ hashtag_df.columns = ['keyword', 'count'] $ sorted_hashtag_df = hashtag_df.sort_values(by='count', ascending=False)
approved.isnull().sum()
controlg = df2.query('group == "control" & converted ==1').user_id.count() $ controlttl = df2.query('group == "control"').user_id.count() $ control_convert = controlg/controlttl $ control_convert
ip_orig.to_csv('image-predictions.csv')
top10 = twitter_final[twitter_final.dog_species != ' '].groupby('dog_species')['tweet_id'].size().reset_index(name="Count").sort_values(by='Count', ascending=False).head(10) $ top10.plot.bar('dog_species')
d.describe()
train_geo.shape
raw.shape
import quandl $ quandl.ApiConfig.api_key  = 'r4mgK1hxD11kLoFtRsso'
X = [string.replace('\n', ' ') for string in X]
pca_data = pca.transform(scaled_data)
m = pd.Period('2017-12', freq = 'M') # This creates a monthly time period $ m # You might be able to leave off the freq argument
dfDay['Likelihood of Win'].unique()
c_df = e_p_b_one.groupby(e_p_b_one.index).size().reset_index() $ c_df.index = c_df.TimeCreate $ c_df.rename(columns={0:"Crime Count in 5k02"},inplace=True)
urban_avg_fare = urban_type_df.groupby(["city"]).mean()["fare"] $ urban_avg_fare.head()
so[no_answer].head()
base_dict_by_place['count'] = base_dict_by_place['hashtags'].apply(lambda x: len(x))
df_customers['number of customers'].median()
nameHasMaxScoreList = scoreListOfList.apply(getHighest)
df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept', 'CA', 'US']]) $ results = log_mod.fit() $ results.summary()
import numpy as np $ da['Sentiment'] = np.array(labels)
df.describe()
data.describe()
pd.to_datetime([1])
tweets_master_df.iloc[tweets_master_df['retweet_count'].nlargest(10).index, :]
melted_total.groupby('Categories')['Neighbourhood'].value_counts().ix[top10_categories.index].unstack().plot.bar(legend=True,figsize=(10, 5))
df = pd.DataFrame([['Daniel','-',42], $                    ['John', 12, 31], $                    ['Jane', 24, 27]]) $ df.columns = ['name','treatment a','treatment b'] $ print(df)
formatted_sample_pq_saved = non_blocking_df_save_or_load( $     formatted_sample, $     "{0}/formatted_sample_pq_11".format(fs_prefix)) $
n_old = df2.query('landing_page == "old_page"').count()[0] $ n_old
df2[df2.duplicated(['user_id'],keep = False)]
fig, ax = plt.subplots() $ df[["tmin","tavg","tmax"]].plot.area(stacked=False, ax=ax, alpha=0.3) $ ax.set_xticklabels("2017-"+df["date"], rotation=30) $ plt.xlabel("date") $ plt.tight_layout()
pop_result.print_rows(num_rows=20)
col_names = autos.columns
df.replace({0: 3}, {0: 99})
scaled = scaled.dropna() $ scaled.head().T
payments_all_yrs = payments_all_yrs.loc[:, :].sort_values(['disc_times_pay','25'], ascending=[False,True]) $ payments_all_yrs = payments_all_yrs.reset_index(drop=True) $ payments_all_yrs.tail()
loans_act_arrears_latest_paid_repaid_xirr=cashflows_act_arrears_latest_paid_investor_repaid.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf)) $ loans_act_origpd_latest_paid__repaid_xirr=cashflows_act_origpd_latest_paid_investor_repaid.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
xpdraft = pd.DataFrame() $ xpdraft =xp[['status','orgId']].copy() $ xpdraft.head()
print(d['feed']['title']) $ print(d['feed']['link']) $ print(d.feed.subtitle) $ print(len(d['entries']))
genes.head()
tfv = TfidfVectorizer(ngram_range=(1,3), max_features=2500) $ X_train, X_test = tfv.fit_transform(X_train['clean_text']), tfv.transform(X_test['clean_text'])
df_protest.ward.head()
plot_df['WEEK_OF_YEAR'] = plot_df['DATE'].apply(lambda x: x.isocalendar()[1])
!ls ../data/imsa-cbf/ | grep _peace.cbf | wc -l
loans_xirr[loans_xirr.id_loan==350]
gb = df.groupby('gender') $ gb.agg        gb.boxplot    gb.cummin     gb.describe   gb.filter     gb.get_group  gb.height   $ gb.last       gb.median     gb.ngroups    gb.plot       gb.rank       gb.std        gb.transform $ gb.aggregate  gb.count      gb.cumprod    gb.dtype      gb.first      gb.groups     gb.hist       gb.max        gb.min        gb.nth        gb.prod       gb.resample   gb.sum        gb.var $ gb.apply      gb.cummax     gb.cumsum     gb.fillna     gb.gender     gb.head       gb.indices    gb.mean       gb.name       gb.ohlc       gb.quantile   gb.size       gb.tail       gb.weight $
countdf = pd.DataFrame(chef03) $ countdf = countdf.drop_duplicates(subset=['name', 'user']) $ countdf.info()
df_group_by2.head()
local = mngr.connect(dsdb.LOCAL) $ local
s_median = s.resample('5BM').median() $ s_median
from src.pipeline import pipeline_json $ pj = pipeline_json(s1) $
df2 = df2.drop(df2.query('user_id == 773192').index[0])
files_lists = [f for f in os.listdir(master_folder + lists +  "unsubscribed/")]
treat_old = len(df.query("group == 'treatment' & landing_page == 'old_page'")) $ control_new = len(df.query("group == 'control' & landing_page == 'new_page'")) $ print('The number of times the new_page and treatment don"t line up is {}'.format(treat_old + control_new))
lots2-lots.ix[-1]
elms_all.ORIG_DATE.max()
vote_sort = to_plot_df.sort_values(by=['vote_count'], ascending=False) $ pt = vote_sort['vote_count'].plot(kind='bar', figsize=(20,10), rot=0, fontsize=20) $ plt.xlabel(pt.get_xlabel(), fontsize=22) $ plt.ylabel('Average of vote_count', fontsize=22) $ plt.title('Average of vote_count by original_language', fontsize=30)
new_page_converted = np.random.binomial(n_new,p_new) $ print('new_page_converted :: ',new_page_converted)
learn.sched.plot_loss() # Loss is still going down. We could keep training this guy if necessary
rGraphData.groupby(['from', 'to']).sum().head()
first_result.find('a')
airports = pd.read_csv('flights/airports.csv') $ airports.head(3)
print(dtm_df.sum().sort_values(ascending=False))
mean_price_by_brand = {} $ for b in sel_brand: $     mean_price = autos.loc[autos["brand"] == b, "price"].mean() $     mean_price_by_brand[b] = int(mean_price) $ mean_price_by_brand
a = df.groupby(['group', 'landing_page']).count() $ print(a) $ print('\n Total rows/items dont line up is {}'.format(a.iat[0,1] + a.iat[3,1]))
oneDayData['likes'] = oneDayData['likes'].map(lambda x: x['count']) $ oneDayData['locationID'] = oneDayData['location'].map(lambda x: str(x['id'])) $ oneDayData['lat'] = oneDayData['location'].map(lambda x: x['latitude']) $ oneDayData['lng'] = oneDayData['location'].map(lambda x: x['longitude']) $ oneDayData['hourOfDay'] = oneDayData['timestamp'].map(lambda x: x.hour)
sdsw = sd[(sd['JOB_TITLE'].str.contains('SOFTWARE')) | (sd['JOB_TITLE'].str.contains('PROGRAMMER')) | (sd['WAGE_UNIT_OF_PAY'] == 'Year')] $ sdsw.sample(50)
df.head()
df.iloc[[1,2,4],[0,2]]
dates = QTU_pipeline.index.levels[0] $ grouped = QTU_pipeline.groupby(level=0).count() $ num_securities = grouped['AverageDollarVolume200'].values $ plt.plot(dates, num_securities) $ plt.title('Number of securities in tradeable universe')
train.isnull().sum()
start_time = time.time()
counts_by_campaign_date.loc[('Sport', '2018-03-18')]
df[df['RT']==False][['op','text','time','fcount']].sort_values(by='fcount',ascending=False).head()
df2.query('group=="treatment"').query('converted==1')['user_id'].count()/df2.query('group=="treatment"').shape[0]
trigram_reviews_filepath = paths.trigram_reviews_filepath
data.iloc[0, 2] = 90 $ data
print("Converted users proportion is {}%".format((df['converted'].mean())*100))
mean_newsorg_sentiment = grouped_newsorgs['Compound'].mean() $ mean_newsorg_sentiment.head()
articles['tokens'] = articles['tokens'].map(lambda s: [w for w in s if len(w)>1])
bb['low'].plot()
response = requests.get('https://www.jconline.com/search/gun%20control/') $ soupresults2 = BeautifulSoup(response.text,'lxml') $
import json $ emp_records_json_str = json.dumps(EmployeeRecords) $ df = pd.read_json(emp_records_json_str, orient='records', convert_dates=['DOJ']) $ print(df)
p_old = (df2['converted']==1).mean() $ p_old
list(t2.p1)
Elec  = transition_rate(0, 0.4, start=2016) $ Water = transition_rate(0, 0.2, start=2016) $ pr    = transition_rate(0, 0.3, start=2016)
altitude_only[(altitude_only.altitude > 1000) & (altitude_only.collection_started>deadline)]
url = "https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/09_Time_Series/Apple_Stock/appl_1980_2014.csv" $
autos['registration_year'].describe()
fund_nr_coinswitches['2014':].plot() $ plt.title('Nr of coin switches in the fund') $ plt.ylabel('Coin switches') $ plt.show()
feature_vectors = vectorizer.fit_transform(df_train['features']) $ lsh_forest.fit(feature_vectors)
aux = df2[df2.group == 'treatment'] $ aux[aux.converted == 1].shape[0] / aux.shape[0]
p_new = p_old = df2['converted'].mean() $ p_new
df_t.head()
df.rating_denominator.value_counts()
result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4) $ result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)') $ np.allclose(result1, result2)
merged_portfolio = pd.merge(portfolio_df, adj_close_latest, left_index=True, right_index=True) $ merged_portfolio.head()
adopted_cats.loc[adopted_cats['Color']=='Brown Tabby/Tortie','Color'] = 'Brown Tabby Tortie' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Brown','Color'] = 'Calico Brown' $ adopted_cats.loc[adopted_cats['Color']=='Brown Tabby/Brown','Color'] = 'Brown Tabby' $ adopted_cats.loc[adopted_cats['Color']=='Calico/Brown Tabby','Color'] = 'Calico Brown Tabby' $ adopted_cats.loc[adopted_cats['Color']=='Agouti/Brown Tabby','Color'] = 'Brown Tabby'
df_twitter_archive_master.sort_values(by=['favorite_count'],ascending=False).head(5)
data3.to_csv('data3.csv')
G.add_path([0,1,2,3,4,5,6,7,8,9]) $ H = G.subgraph([0,1,2,3,4,5,6,7,8,9]) $ H.edges()           
tweet_df[tweet_df['lang'] == 'en'].groupby('username')['lang']
grid_svc.cv_results_['mean_test_score']
control_rate = df2[df2['group'] == 'control']['converted'].value_counts()[1]/df2[df2['group'] == 'control']['converted'].count() $ control_rate
model.add(Dense(1024, activation='relu')) $ model.add(Dropout(rate=0.25))
X_train.shape
waterlevels.to_sql(con=engine, name='waterlevels', if_exists='replace', flavor='mysql',index=False)
pd.Series(Counter(missing_hours)).plot(kind="bar")
new_nums = [] $ for num in nums: $     new_nums.append(num + 1) $ print(new_nums)
data.columns
y, x = dmatrices('encoded_state ~ backers_count', kick_data_state, return_type="dataframe") $ x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training, 30% test $ x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5) # 15% test, 15% validation
flights_month_airline = flights.groupby(["AIRLINE", "MONTH"]) $ num_flights_month_airline = flights_month_airline.size() $ num_flights_month_airline.head()
fig = acc.plot_history(what='amount')
order_num = pd_data.groupby(['appCode','orderType','year','month'])['id'].count()
station_bounds = pd.read_csv('data_raw_cb/deployment_bounds.csv') $ station_bounds.drop(['Unnamed: 0'],axis=1,inplace=True) $ station_bounds[station_bounds.station_id == choice]
baseball_newind.loc[:'myersmi01NYA2006', 'hr']
train_df[['user_id', 'repo_id', 'rating']].to_csv('data/new_subset_data/final_train_data.csv', index=False) $ test_df[['user_id', 'repo_id', 'rating']].to_csv('data/new_subset_data/final_test_data.csv', index=False)
df.plot(y ='rating', ylim=[0,14], style = '.', alpha = 0.4) $ plt.title('Rating with Time') $ plt.xlabel('Date') $ plt.ylabel('Rating');
predict_house_price = stats.linregress(house_data['sqft_living'], house_data['price'])
print "Eval set:\n{}".format(df_eval.priority.value_counts())
plot_acf(model.resid, lags=30)
print meat.head()
data.to_csv("data.csv",index=False, encoding='utf-8')
model.fit(x_train, y_train, $           batch_size=batch_size, $           epochs=epochs, $           validation_data=(x_test, y_test))
p_diffs = [] $ for i in tqdm(range(1,10000)): $     new_converted = np.random.choice([1, 0], size=len(df_new), p=[P_mean, (1-P_mean)]) $     old_converted = np.random.choice([1, 0], size=len(df_old), p=[P_mean, (1-P_mean)]) $     p_diffs.append(new_converted.mean() - old_converted.mean())
rcParams['figure.figsize']= 5,4 $ sb.set_style('whitegrid')
no_of_rows_in_the_data_set = df.shape[0] $ print("no: of rows in the data set = {}".format(no_of_rows_in_the_data_set))
df.drop('four', axis = 'columns') $ df
df.isnull().sum() #there are no missing values
apple.index.duplicated().sum()
url_mars_facts = "https://space-facts.com/mars/" $ browser.visit(url_mars_facts)
create_all_topic_vecs=True $ if create_all_topic_vecs: $     all_top_vecs = [lda.get_document_topics(serial_corp[n], minimum_probability=0) \ $                     for n in range(len(serial_corp))]
random_numbers['2015-05-01':'2015-08-31'].idxmax()# for second 4 months May to August
techmeme['nlp_text'] = techmeme.titles.apply(lambda x: tokenizer.tokenize(x.lower())) $ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x]) $ techmeme.nlp_text = techmeme.nlp_text.apply(lambda x: ' '.join(x))
grpConfidence.count()
station_tobs = session.query(Measurement.tobs).filter(Measurement.station == 'USC00519281', Measurement.date >= '2016-08-23').all() $
X = stock.iloc[915:-1].drop(['target', 'news_sources', 'news_text', 'tesla_tweet', 'elon_tweet'], 1) $ y = stock.iloc[915:-1].target
tweets_df.userLocation.value_counts()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == True].shape[0]
df.head(5)
n_new = df2.query('landing_page == "new_page"').shape[0] $ print('Number of occurrences for landing_page == "new_page": {}'.format(n_new))
grouped_by_year_DRG.groupby(level=0) $
mismatch_grp1 = df.query("group == 'treatment' and landing_page == 'old_page'") $ print("Times treatment group user lands incorrectly on old_page is {}".format(len(mismatch_grp1))) $ mismatch_grp2 = df.query("group == 'control' and landing_page == 'new_page'") $ print("Times control group user incorrectly lands on new_page is {}".format(len(mismatch_grp2))) $ print("Times new_page and treatment don't line up is {}".format(len(mismatch_grp1) + len(mismatch_grp2)))
sample_groups_sorted_by_events = sample_groups.sort_values([ 'MSA_CODE', 'past_event_count' ], ascending=False) $ sample_groups_sorted_by_events.groupby([ 'MSA_NAME' ]).apply(lambda x: x.head(5))
archive_clean['doggo'].replace('None', np.nan, inplace=True) $ archive_clean['floofer'].replace('None', np.nan, inplace=True) $ archive_clean['pupper'].replace('None', np.nan, inplace=True) $ archive_clean['puppo'].replace('None', np.nan, inplace=True)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
import datetime as dt $ from datetime import datetime $ from dateutil.relativedelta import relativedelta $ import numpy as np $
tweets_df.describe()
shows['keywords'] = shows['keywords'].apply(remove_punctuation)
donors['Donor State'].value_counts().head(20).plot(kind='barh') $ plt.title('Top 20 States with most Donors') $ plt.xlabel('Number of Donors') $ plt.gca().invert_yaxis()
vals = data.value.copy() $ vals[5] = 1000 $ vals $ data
%%bash $ ssh volcano@waihaha "cat /home/volcano/programs/omi/logs/so2_summary.csv" > so2_summary.csv
gps_df = full_df[['latitude','longitude']].copy() $ gps_df.head(5)
dayofweek = pd.DatetimeIndex(pivoted.columns).dayofweek
breakfastlunchdinner.plot(x="STATION", y=["breakfast", "lunch + brexits", "dinner"], $                           figsize = (20,9), kind="line", rot = 30, fontsize=13, ylim = (0,90000), $                           title="Average Daily Weekday Traffic for Top 10 Stations at B/L/D"); $ plt.xticks([0, 1, 2, 3,4, 5, 6, 7, 8, 9], ['GRND CNTRL-42ST', '34ST-HERALD SQ', '34ST-PENN STA', '14ST-UNION SQ', '23ST', 'TIMES SQ-42ST', '47-50 STS ROCK','86ST', '59 ST COLUMBUS','59ST', ]); $
transactions = pd.DataFrame([2, 4, 5], $                            index=['Mon', 'Wed', 'Thu']) $ customers = pd.DataFrame([2, 2, 3, 2], $                         index=['Sat', 'Mon', 'Tue', 'Thu']) $ transactions / customers
logit_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = logit_mod.fit()
number_of_commits = len(git_log.index) $ number_of_authors = git_log.author.nunique() $ print("%s authors committed %s code changes." % (number_of_authors, number_of_commits))
df_fireworks = df[(df['Complaint Type'] == 'Illegal Fireworks')] $ df_fireworks.groupby(df_fireworks.index.month).apply(lambda x: len(x)).plot(kind='bar')
train['has_clicked'].describe()
enrollments['cohort_availability'] = enrollments.content_availability.apply(lambda ts: ts.date() - datetime.timedelta((ts.date().isoweekday() + 4) % 7))
question_2_dataframe_in_top_zips = question_2_dataframe[question_2_dataframe['incident_zip'].isin(top_10_zipcodes_by_population)]
wrd_api.info()
df = create_df('data/epl_data.csv')
fit = np.polyfit(ages, weights,1) $ fit_fn = np.poly1d(fit) $ plt.plot(ages, weights,'.', ages, fit_fn(ages))
z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger') $ print(z_score) $ print(p_value)
combined_df = session_df.merge(classifier_df, how='left') $ combined_df.head()
token = pd.merge(token,empInfo[["sender","senderlevel"]],how="left",on = "sender") $ token = pd.merge(token,empInfo[["receiver","receiverlevel"]],how="left",on = "receiver")
stat, p, med, tbl = scipy.stats.median_test(df3["tripduration"], df4["tripduration"]) $ print p $ print tbl
data["bd"].describe()
run txt2pdf.py -o"2018-06-18  2015 871 disc_times_pay.pdf"  "2018-06-18  2015 871 disc_times_pay.txt"
goog.sort_values(by='Date', inplace=True)
tweets_df_clean.drop_duplicates('id', inplace=True)
tag_pairs["FromType"] = "Person" $ tag_pairs["ToType"] = "Hashtag" $ tag_pairs["Edge"] = "Mentioned" $ tag_pairs.rename(columns={"screen_name":"FromName","hashtag":"ToName"},inplace=True) $ tag_pairs.head()
for i in forcast_set:    # iterating through the forcast set $     next_date = datetime.datetime.fromtimestamp(next_unix) # find the next date for the data $     next_unix+=one_day  #increment for finding next date $     df1.loc[next_date] =[np.nan for item in range(len(df1.columns)-1)]+[i]  #settinf up the date as a $
grades - grades.mean()  # equivalent to: grades - [7.75, 8.75, 7.50]
to_pickle('Data files/predictors.p',X_df) $ to_pickle('Data files/target.p',Y_df) $ to_pickle('Data files/classification_df.p',classify_df)
titanic3 = pd.read_excel('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls') $ titanic3.head()
birth_dates["BirthDate_dt"] = pd.to_datetime(birth_dates["BirthDate"], format="%d.%m.%Y") $ birth_dates.head()
df2.groupby(df2['landing_page']=='old_page').size().reset_index()[0].iloc[1]/df2['landing_page'].count()
df.head()
pd.to_datetime(df['time'])
logistic_file = '../data/model_data/log_pred_mod.sav' $ pickle.dump(logreg, open(logistic_file, 'wb'))
concat = pd.concat([ti_mar[ti_mar['store'] != 'Suning'],local_mar]).drop_duplicates(subset = ['review_id'],keep = 'first') $ concat.shape
input_data['year'] = input_data.date.dt.year
df2['intercept'] = 1 $ df2[['control', 'treatment']] = pd.get_dummies(df2['group'])
fit_result.summary2()
ts.shift(1,freq="B")
df2['converted'].mean()
twitter_df[['rating_numerator','rating_denominator']].describe()
cal_map = { $     1:1, # Keep 1 as meaning "calibrated" $     2:0 # Set 2 to 0 to mean "not calibrated" $ } $ pax_raw['paxcal'] = pax_raw.paxcal.map(cal_map)
help(requests.post)
autos.loc[expensive, "price"].count()
img1 = img1.resize((256, 256),pil_image.NEAREST) $ x1 = np.asarray(img1, dtype='float32') $ img2 = img2.resize((256, 256),pil_image.NEAREST) $ x2 = np.asarray(img2, dtype='float32')
train.readingScore[train.male==0].mean()
print('Cells can be executed multiple times, edited, copied/pasted, and re-ordered.')
p_new = df2.converted.sum()/df2.converted.count() $ new_page_converted = np.random.choice([0,1],new_page.landing_page.count(), p=(p_new,1-p_new))
support.amount.sum()/merged.amount.sum()
financial_crisis.drop('Spain defaults 7x') $
pred_labels = rdg2.predict(test_data) $ print("Training set score: {:.2f}".format(rdg2.score(train_data, train_labels))) $ print("Test set score: {:.2f}".format(rdg2.score(test_data, test_labels)))
prec_df = pd.DataFrame(data = us_prec) $ prec_df.columns = ts.dt.date
print('Accuracy Metrics for Logistic Regression') $ score[['is_false_positive', 'is_false_negative', 'is_true_positive', 'is_true_negative']].mean()
g = sns.jointplot(x='longitude', y='latitude', data=data, kind='kde', stat_func=None) $ plt.show()
sns.set_style("darkgrid") $ sns.set_context(rc={"figure.figsize": (8,4)}) $ sns.barplot(weekday.index, weekday.values, palette="Blues").set_title("Rides per Weekday")
tweets.tail()
df_world_map = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) $
unique_user_conversions = df.user_id[(df.converted == 1)].nunique() $ prop_conversions = unique_user_conversions / unique_users $ prop_conversions
from sklearn.model_selection import train_test_split
yt.get_video_metadata(video_id[0], key, parser=P.parse_video_metadata)
new_reps.shape[0]
shows.shape
twitter_archive.info()
diffs=np.array(p_diffs) $ null_values=np.random.normal(0,diffs.std(),diffs.size) $ plt.hist(null_values) #Simulation from the null $ plt.axvline(ControlConverted-TreatmentConverted,color="red") $
zero_rev_acc_opps.columns
dfETHPrice.corr()
px = pd.read_csv(dwld_key + '-hold-pricing.csv', index_col='Date', parse_dates=True)
twitter[twitter.name==twitter.name.str.lower()].name.value_counts()
df.select_dtypes(include='bool').head()
records4.loc[age1[age1['Age'].isnull()].index, 'Age'] = age1_mean $ records4.loc[age2[age2['Age'].isnull()].index, 'Age'] = age2_mean $ records4.loc[age3[age3['Age'].isnull()].index, 'Age'] = age3_mean $ records4.loc[age4[age4['Age'].isnull()].index, 'Age'] = age4_mean
esp.to_csv('spanish_translated_feedback.csv')
AAPL.to_csv('/home/hoona/Python/mpug/directFromPandaDataFrame.csv')
p_diffs=[] $ new_convert=np.random.binomial(Nnew, p_new, 10000)/Nnew $ old_convert=np.random.binomial(Nold, p_old, 10000)/Nold $ p_diffs=new_convert-old_convert
plt.hist(p_diffs) $ plt.axvline(x=obs_diff_conversion, color = 'red'); 
sns.heatmap(data_for_model.corr(),cmap='coolwarm',annot=True)
%matplotlib inline $ import warnings $ warnings.filterwarnings('ignore')
eval_vot_tfidf_tts = clf_vot_tfidf.score(X_testcv_tfidf, y_testcv_tfidf) $ print(eval_vot_tfidf_tts)
df_vow.describe()
log_mod = sm.Logit(df_new['converted'], df_new[['CA', 'US']]) $ results = log_mod.fit() $ results.summary()
autos.head()
clf = LinearRegression()  # create a classifire object $ clf.fit(xtrain,ytrain) # train data related with fir() method $ accuracy=clf.score(xtest,ytest) # test data related with score() method $ print "the accuracy is "+str(accuracy)
df_Providers = df_Providers.sort_values(['name','year'], ascending=[True,True]) $ todays_date = time.strftime("%b %d %Y %H%M") $ print('Date: ', todays_date) $ print('Dimensions of df_Providers:', df_Providers.shape) $ print(df_Providers.head()) $
plt.hist(ins2016["num_vio"].values)
sentiments_pd.head()
most_active_station = record[0][0] $ most_active_station_name = session.query(Station.name).filter(Station.station == record[0][0]).first()[0] $ most_active_station_name
baseball_newind.loc['wickmbo012007']
from pyspark.sql.functions import monotonically_increasing_id $ test = (test $         .withColumn('id', monotonically_increasing_id()) $         .drop('click_time')) $ print("test size: ", test.count())
df.head()
print(house_data['date'][0]) $ print(house_data['date'][0].year) $ print(house_data['date'][0].month) $ print(house_data['date'][0].day)
DataSet_sorted[['tweetText', 'prediction']].head(10)
scores = pd.read_csv(mypath+'/tfidf_cosine.csv', index_col = 0)
transactions['items_total'].describe()
df_outcomes = np.array(pd.get_dummies(df_h1b_ft_US_Y.status).CERTIFIED)
rng = np.random.RandomState(0) $ x = rng.randint(10, size=(3, 4)) $ x
df['closed_at'].head()
beirut['WindDirDegrees'] = beirut['WindDirDegrees'].astype('float64') $ beirut['EET'] = to_datetime(beirut['EET'])
questions.shape
trump['hours'] = trump.index.hour $ trump['weekday'] = trump.index.weekday_name
df_new['updated_at'] = df_new['updated_at'].apply(pd.to_datetime) $ df_new['updated_at'] $ df_new['updated_at'] = df_new['updated_at'].dt.date $ df_new['updated_at'] = df_new['updated_at'].apply(pd.to_datetime) $ df_new['updated_at']
ax = pdf.plot() $ s_etf.plot(ax=ax, legend='right')
(data['Traded Volume']).mean()
p_old=df2['converted'].mean() $ print(p_old)
df_new[['country_CA','country_UK','country_US']] = pd.get_dummies(df_new['country']) $ df_new.head()
res=geo.geoLocate('Pokhara') $ assert res[0][3]=='NP' $ assert hitDetector.isInCountry((res[0][2],res[0][1])) $ assert hitDetector.getRegion((res[0][2],res[0][1]))==(5,'West')
with open('/Users/annalisasheehan/Dropbox/Climate_India/Data/data_child/lat_long_younglives_AS_cut.csv') as f: $     latlon = f.read().splitlines()
print(full_df.shape)
(np.cumsum(df.Category.value_counts())/len(df)).head(20)
df_everything_about_DRGs.loc[0:5]['discharges']['count']
pd.crosstab(test["rise_in_next_week"], advpredictions, rownames=["Actual"], colnames=["Predicted"])
csvData['street'] = csvData['street'].str.replace(' Avenue', ' Ave') $ csvData['street'] = csvData['street'].str.replace(' Court', ' Ct') $ csvData['street'] = csvData['street'].str.replace(' Drive', ' Dr')
for tweet in collect.get_iterator(): $     print(json.dumps(tweet, indent=4)) $     break
df_copy['date'] = df_copy['timestamp'].apply(lambda time: time.strftime('%m-%d-%Y')) $ df_copy['time'] = df_copy['timestamp'].apply(lambda time: time.strftime('%H:%M')) $ df_copy=df_copy.drop(columns=['timestamp'])
epa.loc[:,'CO2_MASS (kg)'].fillna(0, inplace=True)
pax_raw.paxstep.describe()
obs_diff = df2[df2.landing_page == 'new_page'].converted.mean() - df2[df2.landing_page == 'old_page'].converted.mean() $ obs_diff
pca_full = PCA() $ pca_full.fit(crosstab) ## note: This takes 1:20 minutes to complete 20,000 records
from sklearn.metrics.pairwise import cosine_similarity $ print('Similarity:') $ print('   computer to mouse =', cosine_similarity(vectors.loc[['computer']], vectors.loc[['mouse']])[0][0]) $ print('   cat to mouse =', cosine_similarity(vectors.loc[['cat']], vectors.loc[['mouse']])[0][0]) $ print('   dog to mouse =', cosine_similarity(vectors.loc[['dog']], vectors.loc[['mouse']])[0][0])
p_diffs = np.array(p_diffs) $ (p_diffs > diff).mean()
emails_dataframe['address'].str.split("@").str.get(1)
mgxs_lib.build_hdf5_store(filename='mgxs.h5', directory='mgxs')
from sklearn.neural_network import MLPClassifier $ n_net = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(1000, 500, 250, 100, 50), $                       random_state=1, verbose = True) $ n_net.fit(x_train,y_train > 0)
import statsmodels.api as sm $ logit_model=sm.Logit(y_all,X) $ result=logit_model.fit() $ print(result.summary2())
s + pd.tseries.offsets.DateOffset(months=2)
df.shape[0] #finding number of rows in a dataset
cat_outcomes.head()
vif = pd.DataFrame() $ vif["VIF Factor"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])] $ vif["features"] = X.columns $ vif
plt.figure(figsize=(10,10)) $ g = sns.boxplot(x='dog_stages',y='retweet_count',data= df_master,palette='rainbow') $ g.axes.set_title('Boxplot between dog stages and retweet', fontsize=14);
examples = active_companies[active_companies.CompanyNumber.isin(trading_exemption_records.company_number)] $ examples.to_csv('data/for_further_investigation/trading_exemptions.csv') $ examples[['CompanyNumber','CompanyName','URI']].head(5)
nodes['is_lab'] = nodes['user'].apply(lambda x: islab(x)) $ ways['is_lab'] = ways['user'].apply(lambda x: islab(x)) $ lab = nodes[nodes['is_lab']==1] $ lab2 = ways[ways['is_lab']==1]
lda = LatentDirichletAllocation( $     n_topics=n_topics, learning_method="online", random_state=0) $ lda = lda.fit(tf)
tweets.head()
print('Best estimator :{}'.format(gridSearchModel.best_estimator_)) $ print('\nScore on train data :{}'.format(np.abs(gridSearchModel.score(df_train, tsne_train_output)))) $ print('\nScore on test data :{}'.format(np.abs(gridSearchModel.score(df_test, tsne_test_output))))
conn = pymysql.connect(host='localhost', port=3306, user='root', password='1234')
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split( $     X, y, test_size=0.3, random_state=42)
inv_hist.to_clipboard()
df["in_reply_to_screen_name"].value_counts()
column_list2 = ['Temperature','DewPoint'] $ df[column_list2].plot() $ plt.show()
scowl2 = [x for x in scowl if not regex_1.match(x)] $ len(scowl2)
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['NewWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'] $ df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['LastBitWell'] = df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'].shift(-1) != df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM['UWI'] $
distinct_authors_latest_commit.schema
data[data.index.duplicated()]
import requests $ res = requests.get("https://www.skyscanner.net/transport/flights/tpet/cts/180118/180130?adults=1&children=0&adultsv2=1&childrenv2=&infants=0&cabinclass=economy&rtn=1&preferdirects=false&outboundaltsenabled=false&inboundaltsenabled=false&qp_prevProvider=ins_month&qp_prevCurrency=GBP&qp_prevPrice=178&pricesourceid=b3ms-SG1-2#results")
p_old = df2.converted.sum()/df2.converted.count() $ old_page_converted = np.random.choice([0,1],old_page.landing_page.count(), p=(p_old,1-p_old))
sql_usedb = 'use HelloDB;' $ cur_helloDB.execute(sql_usedb)
'this is {} number {}'.format( "string", "1")
commits = Query(git_index).get_cardinality("hash") $ print("Total commits: ", get_aggs(commits)) $ all_commits = commits.fetch_results_from_source("hash", "commit_date", dataframe=True) $ print("All commits: ", all_commits.head())
obs_diff = (new_page/df2.query('landing_page == "new_page"').shape[0]) - (old_page/df2.query('landing_page == "old_page"').shape[0]) $ obs_diff.mean()
print(df_release.shape) $ df_release.columns
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK', 'US']]) $ res = mod.fit() $ res.summary()
df.user_id.nunique() #return number of unique rows
analyser = SentimentIntensityAnalyzer() $ df_en['polarity_vader'] = df_en['text'].apply(lambda tweet: analyser.polarity_scores(tweet)['compound'])
df_period = df_period.loc[df_period.time_id < '2014-09-01']
b_rev.shape
All_tweet_data_v2=All_tweet_data_v2[All_tweet_data_v2.Stage!='None']
name = df.index == 2127 $ df[name]['text']
from sklearn.ensemble import RandomForestRegressor $ from sklearn.ensemble import ExtraTreesRegressor $ from sklearn.tree import DecisionTreeRegressor $ import xgboost as xgb $ from sklearn.model_selection import GridSearchCV
autos['registration_year'].min()
df_mes = df_mes[df_mes['total_amount']>=0] $ df_mes.shape[0]
logit_mod = sm.Logit(df_merge['converted'], df_merge[['intercept', 'ab_page', 'UK', 'US']]) $ res = logit_mod.fit() $ res.summary()
from sklearn.linear_model import LogisticRegression $ from sklearn.model_selection import cross_val_score $ classifier = LogisticRegression(C =10 , solver ='sag' , n_jobs =-1) 
session.head()
features_scores ={} $ for c, v in zip(logreg_words.fit(X_words, y).coef_[0].round(3)[-4:], ['Curse','Fun','Happy','Vice']): $     features_scores[v] = c $ sent_coef = pd.DataFrame(features_scores, index=['Coefficient']).T.sort_values(by='Coefficient', ascending=False) $ sent_coef
userID = np.array(users["id"]) $ my_solution = pd.DataFrame(my_prediction, userID, columns = ["country_destination"]) $ print(my_solution)
df['logViewsPercentChange']=df['Views-PercentChange'].apply(np.log)
bnbAx.head(5)
appleinbounds = appleinbounds[(appleinbounds['tweet_num'] > 2) & (appleinbounds['all_sent'] >= 0.0)]
df_course.shape
segmentData.opportunity_source.value_counts()
speaker_dummies = pd.get_dummies(speeches_metadata_evidence['speaker_bioguide'], prefix = 'speaker_id') $ speeches_metadata_evidence = pd.concat([speeches_metadata_evidence,speaker_dummies], axis = 1) $ speeches_metadata_evidence2 = speeches_metadata_evidence.drop_duplicates(subset = ['bill_id'])
S_1dRichards.initial_cond.filename
data1=pd.read_csv('data/allcoinUSD.csv',header=None)
movies2000to2014=movies.loc[(movies['year']>=2000) & (movies['year']<=2014)]
df = pd.DataFrame(np.random.randint(0, 50, 32).reshape(8, 4), columns=list('WXYZ'), index=list('abcdefgh')) $ df1 = df.ix[2:, ['W', 'X']] $ df2 = df.ix[:5, ['Y', 'Z']] $ print df, '\n\n', df1, '\n\n', df2
print(df["YEAR"].dtype,df["YEAR"].dtype,df["DAY_OF_MONTH"].dtype,df["DEP_HOUR"].dtype) $ print(df_weather_origin["YEAR"].dtype,df_weather_origin["YEAR"].dtype,df_weather_origin["DAY_OF_MONTH"].dtype,df_weather_origin["DEP_HOUR"].dtype)
tfav = pd.Series(data=data['Likes'].values, index=data['Date']) $ tret = pd.Series(data=data['RTs'].values, index=data['Date']) $ tfav.plot(figsize=(16,4), label="Likes", legend=True) $ tret.plot(figsize=(16,4), label="Retweets", legend=True)
p_diffs = np.array(p_diffs) $ (p_diffs > ab_diffs).mean()
tweets_clean.drop(columns = ['doggo','floofer','pupper','puppo'], inplace = True) $ tweets_clean.head()
df.groupby(['character_id', 'raw_character_text'])['id'].nunique().reset_index().head(10)
import datetime $ df_date['date'] = map(lambda x: datetime.datetime(x.year,x.month,x.day,0,0), df_date.created_at)
with open('tweet_json.txt', 'a') as outfile: $     json.dump(tweets_list, outfile)
model2 = linear_model.LinearRegression() $ model2.fit(x2, y) $ (model2.coef_, model2.intercept_)
df_columns[ ~df_columns['Descriptor'].isnull() & df_columns['Descriptor'].str.contains('Pothole') ].groupby(["Hour of day","AM|PM"]).size().reset_index().sort_values([0], ascending=[False]) $
data.values
X =Quandl_DF.where(Quandl_DF['Date'] >= '20150101').groupby(['Year','WeekNo'])['GBP_to_HKD'].mean().plot()
res_tuples = [] $ for url in urls: $     res_tuples.append(dict2tuple(get_detail(url))) $ res_tuples
for row in nocachedf.itertuples(): $     do_ifcformant(row, indir=srcdir, outdir=cachedir)    
sns.set_style("whitegrid") $ sns.barplot(x="sex", y="survived", data=titanic)
for row in conn.execute("SELECT * FROM beacon_hit"): $     print(row)
gb_model.feature_importances_
infered_relevant_info.groupBy(infered_relevant_info.genderize_gender).agg(F.count("*")).show()
pd.DataFrame(d, index=['d', 'b', 'a']) # uses d, not df, as data input
linear = linear_model.LinearRegression() $ linear.fit(X_20, y2) $ (linear.coef_, linear.intercept_)
p_new = df2.query('group == "treatment"').shape[0] / df2.shape[0] $ p_new
img_url_cerberus = soup.find('div', 'downloads').a['href']
dfOld = dfOld[['title','popdate','url','userid','username','highlight','nlikes','ncomments','ntags','origdb','tags','text','npar']] $ dfOld.index.name = 'postid' $ print(dfOld.shape) $ dfOld.head()
n_new = df2.query('group == "treatment"')['user_id'].count() $ n_new
result = api.search(q='%23Australia')  # "%23" == "#" $ len(result)
df['score'] = df['rating_numerator'] / df['rating_denominator'] $ df['score'].describe()
results = (calc_temps('2017-07-22', '2017-07-29')) $ results_df = pd.DataFrame(results, columns=['Min','Ave' ,'Max']) $ results_df.head()
from pyspark.sql.functions import col $ file2=file.where((col('trans_start_month')==7) | (col('trans_start_month')==8)) $ file2.show(3)
df2['converted'].value_counts()[1]/len(df2)
prices.plot(figsize=(12, 6))
date_df = date_df[['count', 'date', 'weekday', 'duration(min)']] $ date_df.head()
prob_treatment = df2[df2.group == 'treatment']['converted'].mean() $ prob_treatment 
tweets_streamedDF[tweets_streamedDF.user == 'bleedingheartmx']
countries_df = pd.read_csv('countries.csv') $ countries_df.head()
print  [v for v in pre_analyzeable['prior_lab'].values if str(v) != 'nan']
train['month_of_first_booking'] =  train['date_first_booking'].map(lambda x: x.strftime('%m') if pd.notnull(x) else '')
base_dict_by_place.head()
test_post = test_post.lower()
df = df[df.launched_year != 1970]
grouped_months_liberia = Total_new_cases_liberia.groupby(Total_new_cases_liberia['Date'].apply(lambda x: x.month)) $ new_cases_liberia_concat['Total_new_cases_liberia'] = new_cases_liberia_concat['Total_new_cases_liberia'].astype(int)
X_test_dtm = pd.DataFrame(cvec.transform(X_test.title).todense(), columns=cvec.get_feature_names())
nitrodata['Month'].value_counts().sort_index()
from time import strptime $ thr_date_str = 'January 1, 2014' $ thr_date = strptime(thr_date_str, "%B %d, %Y") $ vi_ok_indices = vi['ENDDATE'] > thr_date_str $ vi_ok = vi.loc[vi_ok_indices,['VIOLATIONCODE']]
old_page_converted = np.random.choice([0, 1], size = n_old, p = [p_old, 1 - p_old])
string_punctuation = '!"#$%&\'()*,-./:;<=>?@[\\]^_`{|}~' $ dict_punc = {key:' ' for key in string_punctuation} $ negtags = ['not', 'no', 'none', 'neither', 'never', 'nobody'] $ negtag_regex = '(' + '|'.join(negtags) + ') '
all_sets.cards["XLN"].shape
MEW_TOKENS = getJSON('https://raw.githubusercontent.com/MyEtherWallet/ethereum-lists/master/tokens/tokens-eth.json') $ MEW_TOKENS = pd.DataFrame.from_dict(MEW_TOKENS) $ MEW_TOKENS.head()
result.to_csv('recommendation.csv', header = False, index = False) 
test_set.info()
tw_clean.drop_duplicates(subset='expanded_urls', inplace=True)
autos[autos["brand"].isin(common_brands)].groupby(autos["brand"])["price"].std().sort_values()
session.query(func.min(measurement.tobs),func.max(measurement.tobs),func.avg(measurement.tobs)).\ $     filter(measurement.station == 'USC00519281').all()
mars_fact_url = "https://space-facts.com/mars/" $ mars_facts = pd.read_html(mars_fact_url) $ mars_facts[0]
soup = bs(html.text, 'html.parser')
fake_today = parse("2017-08-14") $ dt = relativedelta(years=1) $ year_ago = fake_today - dt $ year_ago_str = year_ago.strftime("%Y-%m-%d") $ print(year_ago_str)
df['price_log'] =  np.log10(df['price']) $ df['powerPS_log'] =  np.log10(df['powerPS'])                           
hpdvio.dtypes
print(ad_source.sum(axis=0).sum()) $ print() $ print(ad_source.sum(axis=0))
top_tracks = df_track.merge(df_artist, on = ['artist_id','artist_name']).sort_values( $     by = 'artist_followers', $     ascending = False)[['track_name','artist_name','artist_followers']].head(5) $ top_tracks
activity = session.query(Stations.station, Stations.name, Measurements.station, func.count(Measurements.tobs)).filter(Stations.station == Measurements.station).group_by(Measurements.station).order_by(func.count(Measurements.tobs).desc()).all()
my_list = (list(set(list_payments_total_yrs).intersection(test_df_ids))) $ my_list[:10]
X = vectorizer.fit_transform(clean_train_reviews)
crosstab_transformed = pca.transform(crosstab)
df.Day_of_week.value_counts().plot(kind='bar')
columns.keys()
c_df['CUSTOMER_ID_CAT'].value_counts()
df_daily2.sort_values(by=['DATE'], inplace=True)
df_archive["tweet_id"].value_counts()[0:5]
tables = pd.read_html(facts_url) $ tables
mw = pd.read_csv('mw.csv', sep='~') $ mw.rename(columns={'body':'content', 'dst':'slug'}, inplace=True) $ mw['created'] = pd.to_datetime(mw.timestamp, utc=True, unit = 's') $ mw = mw[['title','slug','created','content']] $ mw.info()
components1= pd.DataFrame(pca.components_, columns=['SP500','DJIA','NASDAQ','happiness'])
from src.environments.portfolio import PortfolioEnv $
print(mi_diccionario.keys(), mi_diccionario.values())
d + pd.tseries.offsets.YearEnd()
num_words=["roussia","jayin","page","chance"] $ print (words_df[ words_df['Word'].isin(num_words) ])
index = pd.date_range(start='2016-09-01-00:00',periods=30*24,freq='h') $ index
countries_df = pd.read_csv('./countries.csv') $ df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
nitrogen['ActivityMediaSubdivisionName'].unique()
sqlContext.registerFunction("simple_function", simple_function)
donors[donors['Donor Zip'] == '606' ]['Donor State'].value_counts().head()
len(Data.groupby('to_account')['deposited', 'withdrawn'].sum())
metadata_df = pd.read_html(DATA_FOLDER+'/titanic.html', header=0)
f, ax = plt.subplots() $ ax.set_ylim(ymax=400); $ ax.set_xlabel('Shift duration [h]'); $ ax.set_ylabel('Count'); $ df['duration'].astype('timedelta64[h]').hist(bins=75, ax=ax);
df_test = pd.read_csv('../Data/sector_joined_closes_test.csv', index_col='Date_Time') $ display(DataFrameSummary(df_test).summary())
psy_prepro = pd.read_csv("psy_prepro.csv") $ psy_prepro = psy_prepro.set_index('subjectkey', verify_integrity=True) $
tweets_df = tweets_df[['id_str', 'retweet_count', 'favorite_count']].set_index('id_str')
net_loans=match_id(merkmale, merkmale.Merkmalcode.isin(['KR','ML'])) $ net_loans_exclude_US=drop_id(net_loans,net_loans.Merkmalcode=='US') $ net_loans_exclude_US_outstanding=drop_paid_off(net_loans_exclude_US) $
pivoted.columns
df2.shape
twitter_Archive.info()
logs.name.tail(13)
treatment_df = df2.query('group == "treatment"') $ treatment_df.user_id.nunique()
df['count'].plot()
model3 = sm.Logit(df_new.converted, df_new[['intercept','US', 'CA', 'US_new_page', 'CA_new_page', 'new_page']]) $ result3 = model3.fit() $ result3.summary()
lmscore.pvalues[lmscore.pvalues >0.05]
import datetime $ data['Created Date'] = data['Created Date'].apply(lambda x:datetime. $                                                   datetime. $                                                   strptime(x,'%m/%d/%Y %H:%M'))
df2.query('landing_page == "old_page"').converted.count()
%bash $ model_dir=$(ls $PWD/hallelujah-effect_trained/export/exporter/) $ gcloud ml-engine local predict \ $     --model-dir=./hallelujah-effect_trained/export/exporter/${model_dir} \ $     --json-instances=/tmp/test.json
agg_stats.tail()
df2.query('user_id==773192')
sess.delete_endpoint(predictor.endpoint)
max_div_stock=df.iloc[df["Dividend Yield"].idxmax()] $ max_div_stock $ print("The stock with the max dividend yield is %s with yield %s" % (max_div_stock['Company Name'],max_div_stock['Dividend Yield']))
Celsius._temperature = 'spag'
max(temp[temp.columns.values[0]])
lastyear_day = session.query(Measurement.date).order_by(Measurement.date.desc()).first() $ lastyear_day $
next_day_pf.df
model.wv.similarity('feeding', 'son')
df.to_sql('coindesk_df', engine)
sp = s.str.split('_') $ sp
from sklearn.metrics import mean_squared_error as mse $ from sklearn.metrics import mean_absolute_error as mae $ from sklearn.metrics import median_absolute_error as medae $ from sklearn.metrics import explained_variance_score as evs $ from sklearn.metrics import r2_score
countries_df.head()
def checkCustomer(cust_id): $     if cust_id in set(top5_best_fan.keys()): $         return True $     else: $         return False
import pandas as pd $ df = pd.DataFrame(records, columns=['date', 'lie', 'explanation', 'url'])
from patsy import dmatrices $ from statsmodels.stats.outliers_influence import variance_inflation_factor
count = np.array(range(1,190))
facts_url = 'http://space-facts.com/mars/'
BASE = 'http://' + IP +  ':' + str(PORT_NUMBER) + '/v1/' $ HEADERS = {'Content-Type': 'application/json'} $ requests.delete(BASE + 'session') $ def pp(json_data): $     print(json.dumps(json_data, indent=4))
plt.plot(spks[:, 0], spks[:, 1], '.k') $ plt.ylim(-0.5, 7.5) $ plt.xlabel('Time (s)') $ plt.ylabel('Neuron Number')
plt.plot(x, y) $ X=np.linspace(1,len(x),len(x)) $ plt.plot(x, Y, "r") $ plt.show()
brand_counts = autos["brand"].value_counts(normalize=True) $ common_brands = brand_counts[brand_counts > .05].index $ print(common_brands) $
raw_df.workflow_version.value_counts()
grouped_dpt.aggregate(np.sum) # aggregate sum based on one group
new_array = np.concatenate((training_active_listing_dummy,training_pending_ratio),axis=1)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ results = log_mod.fit() $ results.summary()
player_id = baseball.player + baseball.year.astype(str) $ baseball_newind = baseball.copy() $ baseball_newind.index = player_id $ baseball_newind.head()
d = pd.date_range('20170101', periods = 12) $ d
blame = pd.concat([pd.DataFrame(first_line).T, blame]) $ blame.head()
df3['WEEK'] = df3['DATE'].dt.week
retweets = tweets["retweet_count"] $ print (retweets.dtype) $ print (retweets.mean()) # calculate mean $ print (retweets.drop(1).head()) #drop using index $ print (tweets["screen_name"].drop_duplicates().head()) #drop duplicated names
def combine_names(row): $     if row.contributor_fullname.startswith("SEAN PARKER"): $         return "SEAN PARKER" $     return row.contributor_fullname
import random $ L=[random.random() for i in range(100000)] $ %timeit L.sort()
openLastOnly = pd.merge(legLastOnly, openHouse.reset_index(), how='left', on=['last_name'])
df_ad_airings_4.head(1)
LabelsReviewedByDate = wrangled_issues_df.groupby(['Priority','DetectionPhase']).created_at.count() $ dateLabelsFig = LabelsReviewedByDate.unstack().plot(kind='bar',stacked=True,  color=['blue','yellow', 'purple', 'red', 'green'], grid=False)
dfb_test.describe()
donors_c[donors_c['Donor Zip'] == '606' ]['Donor State'].value_counts().head()
park.groupby(by = 'named_drug').count().abstract
df_exp = pd.read_csv("BTime_exps_staszeq.csv")
res4 = df_test4_promotions.groupby(['AwardType']).agg({'PromotionRuleCount':'sum', 'AwardAmount':'sum', 'ShopperID':'nunique'}) $ res4['AwardAmount'] = res4['AwardAmount'] / 100 $ res4 $
sakhalin_filtered = sakhalin_data_in_bbox[[sakhalin_poly.contains(Point(x,y)) for x,y in zip(sakhalin_data_in_bbox.longitude, sakhalin_data_in_bbox.latitude)]]
baby_scn_postgen['ACTIVATED_ELAPSED'].describe()
sqlstr = "SELECT ID, Name, DOB, SSN \ $                FROM SAMPLE.PERSON \ $                WHERE Name %STARTSWITH ?" $ samples_query.execute_sql(sqlstr,'A') $ samples_query.display_records(5)
from sklearn.model_selection import train_test_split $ X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1) # 30% for testing, 70% for training $ X_train.sample(5)
url = "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv" $ response = requests.get(url) $ with open(url.split("/")[-1], mode="wb") as file: $     file.write(response.content)
for ind, sex in gender.iteritems(): $     if type(sex)==list: $         print ind,sex
%time df_hdf['Total Profit'].sum().compute()
max(results.argsort())
display(df.head(),(df.dtypes))
print(qa_data.head())
df['timestamp'] = pd.to_datetime(df['timestamp']) $ df['EST'] = df['timestamp'] - pd.Timedelta(hours=5) #Convert to EST
df_eve =df3.query('evening==1')
bacteria_data.treatment
mod = sm.Logit(df3['converted'], df3[['intercept', 'US', 'CA']])
logistic_mod_country = sm.Logit(df3['converted'], df3[['intercept', 'UK','US']]) $ results_country = logistic_mod_country.fit() $ results_country.summary()
pd.Series([6,5,4,3,2,1], index=10 * np.arange(6)) # index par default : 0, 1 .. $
[endpoint_deployments] = [x.get('entity').get('deployments').get('url') for x in json.loads(response_get.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid] $ print endpoint_deployments
print('Metadata of the First Dataframe (created from data-text.csv)\n') $ df.info() $ print('\n\nMetadata of the Second Dataframe (created from berlin_weather_oldest.csv)\n') $ df1.info()
df_archive['name'].value_counts()
import sys $ if not sys.warnoptions: $     import warnings $     warnings.simplefilter("ignore")
h2020.columns == fp7.columns
closingPrices.head()
print('Slope FEA/1 vs experiment: {:0.2f}'.format(popt_opb_chord_saddle[0][0])) $ perr = np.sqrt(np.diag(pcov_opb_chord_saddle[0]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
df2.to_csv("msft_temp.csv", index_label='date')
newmodmod = X_know $ newmodmod['y_pred'] = yhaty $ newmoddie = pd.concat([newmod, newmodmod]) $ newmoddie.head()
df_final.columns
sns.barplot(x="tweet_source",y="tweet_vader_score",data= my_tweet_df,estimator=np.mean)
dfs = pd.read_html('http://www.contextures.com/xlSampleData01.html')
new_TC_file = folder + "\TC-all.txt" $ new_TC_file
df_germany = df_germany.drop(df_germany.columns[0], axis=1)
keys=[] $ for t in t_open_resol: $     keys.append(t['key']) $
af.index.names = ['id']
np.exp(fruits)
filenames = os.listdir(path) $ filenames
sensor = hp.find_sensor('d5a747b86224834f745f4c9775d70241')
states.columns
INC['dateShort'] = pd.to_datetime(INC['date_incid']) $ print INC.ix[:, 'dateShort'].head() $
query_result1 = feature_layer.query(where='POP2010>1000000', $                                     out_fields='WHITE,BLACK,MULT_RACE,HISPANIC') $ len(query_result1.features)
overallBsmtFullBath = pd.get_dummies(dfFull.BsmtFullBath)
plt.hist(p_diffs) $ plt.xlabel('Conversion rate difference') $ plt.ylabel('Total number') $ plt.title('Sampling Distribution from the Null vs Observed Difference') $ plt.axvline(obs_diff, c='red');
mismatch_treatment_to_old_pg = df.query("group=='treatment' & landing_page =='old_page'") $ mismatch_control_to_new_page = df.query("group=='control' & landing_page =='new_page'")
p_diffs = [] $ for i in range(10000): $     new_page_converted = np.random.binomial(1, p_new,size = n_new) $     old_page_converted = np.random.binomial(1, p_old,size = n_old) $     p_diffs.append (new_page_converted.mean() - old_page_converted.mean())
!ls ../data/imsa-cbf/ | grep _w.cbf | wc -l
nameHasMaxScoreList[3] # Citigroup in no.3
random_times = random.choices(list(non_empty_crash_data_df['Rounded Date and Time']), k=sample_size)
df['TMAX'] = df['TMAX'] * 9/5 + 32 $ df['TMIN'] = df['TMIN'] * 9/5 + 32 $ df['TOBS'] = df['TOBS'] * 9/5 + 32 $ df['PRCP'] = df['PRCP'] * 0.0393701
df.head(1)
run txt2pdf.py -o '2018-06-22 2013 FLORIDA HOSPITAL Sorted by discharges.pdf'  '2018-06-22 2013 FLORIDA HOSPITAL Sorted by discharges.txt'
import time $ start = time.time() $ trips = pd.read_csv('trip.csv', sep=',', parse_dates=['start_date','end_date'], $                       infer_datetime_format=True,low_memory=False) $ print 'Time elapsed: ' + str(time.time() - start)
dfs.set_index('DATE_TIME', inplace=True) $ dfs_resample = dfs.groupby(['C/A', 'UNIT', 'STATION', 'SCP'], as_index = False).apply(lambda df1: df1.resample('6H') $                                                               .first() $                                                              .interpolate()) $
logit = sm.Logit(y, X) $ result = logit.fit() $ result.summary()
archive_df.head()
gatecount_station_line[gatecount_station_line.name == 'South Station'].head()
my_df = get_lims_dataframe(my_query) $ my_df.head()
print('Scanning for all greetings:') $ for key, row in table.scan(): $     print('\t{}: {}'.format(key, row[column_name.encode('utf-8')])) $
details.head(5)
chosen_disk_model = 'ST4000DM000' $ merged_file_path = merged_data_dir+'/merged_data_2015_2017.csv'
s.str.endswith('t')
drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7 # We should probably update these to match below? Or tune mult upwards
iris.head()
not_in_dsi = not_in_dsi.copy()[not_in_dsi.copy()['full_name_y'].isnull()][['name', 'forks_url_x', 'forks_count_x', 'created_at_x']] \ $     .rename(columns={'forks_url_x': 'forks_url', 'forks_count_x': 'forks_count', 'created_at_x': 'created_at'})
df_d.describe()
df2['intercept'] = 1 $ df2[['to_drop', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(['to_drop'], axis=1, inplace=True) $ df2.head()
grouped_by_bedrooms_df.to_csv('grouped_by_num_of_bedrooms_df.csv',index=False)
pd.set_option('display.max_columns', 1844278)
df=pd.read_json('tweet_json.txt', lines=True) $
bacteria_data.treatment = 1 $ bacteria_data
archive_clean.name.value_counts()
labels.head()
raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1], $                        dtype=np.float32, name="raw_weights")
url='https://dzone.com/articles/top-5-data-science-and-machine-learning-course-for'
archive_clean.info()
df2 = ab_dataframe.query(' (group=="treatment" & landing_page == "new_page") | ( group =="control" & landing_page == "old_page") ') $ print(df2.shape) $
data_libraries_df.to_excel(writer, "data_libraries", index=False, na_rep="NA")
df2.groupby(['TUPLEKEY', 'DATE']).sum()
%sql select * from mysql.user;
db_criterion =pd.read_sql_query( "SELECT * FROM criterion",conn) $ db_vote =pd.read_sql_query( "SELECT * FROM vote",conn) $ db_peer_review =pd.read_sql_query( "SELECT * FROM review",conn) $ db_expert_review = pd.read_sql_query("SELECT * FROM  expert_review",conn) $ db_voter_score = pd.read_sql_query("SELECT * FROM vote_rating", conn)
tall.all()
stocks = (spark $           .read $           .option("header", True) $           .option("inferSchema", True) $           .csv("stocks"))
rec_new_prob = (df2['landing_page'] == 'new_page').sum() / len(df2.index) $ print(rec_new_prob)
rfr = RandomForestRegressor() $ rfr.fit(x_train1, y_train1) $ rfr.score(x_test1, y_test1)
n_new = df2[df2.group=='treatment']['converted'].shape[0] $ n_new
fb.message = fb.message.fillna("")
(msftAC / msftAC.shift(1) - 1) [:5]
loans_plan_origpd_noshift_xirr=cashflows_plan_origpd_noshift_all.groupby('id_loan').apply(lambda x: xirr(x.payment,x.dcf))
tf.reduce_sum(tf.distributions.kl_divergence( $     tf.distributions.Normal(loc=q_mu, scale=q_sigma), $     tf.distributions.Normal(loc=p_mu, scale=p_sigma)), axis=-1).eval()
week7 = week6.rename(columns={49:'49'}) $ stocks = stocks.rename(columns={'Week 6':'Week 7','42':'49'}) $ week7 = pd.merge(stocks,week7,on=['49','Tickers']) $ week7.drop_duplicates(subset='Link',inplace=True)
p_treat_conv = df2.query('group == "treatment"')['converted'].mean() $ p_treat_conv
df['Week']=pd.DatetimeIndex(df['Lpep_dropoff_datetime']).week $ df['Week']=df.Week.astype('category') $ df['Week'].describe()
print watch_table.shape $ watch_table.head()
rng = pd.date_range(start = '7/1/2017', end = '7/21/2017', freq = 'B') $ rng
df = pd.DataFrame([ {'id': result.id, 'created_at': result.created_at, 'user': '@'+result.user.name, 'text': result.text } for result in  search_results])[['id', 'created_at', 'user', 'text']] $ display(df.head())
tSVD2 = TruncatedSVD(n_components=300) $ blurb_SVD2 = tSVD2.fit_transform(blurbs_to_vect) $ print(np.sum(tSVD2.explained_variance_ratio_))
autos = autos[autos['registration_year'].between(1900,2018)] $ autos['registration_year'].value_counts(normalize=True).head(15)
dtc = DecisionTreeClassifier(max_leaf_nodes=1002) $ dtc.fit(X_train, y_train) $ predictions = dtc.predict(X_test) $ accuracy_score(y_true = y_test, y_pred = predictions)
search1 = search.merge(user1, how='left', left_on='user_uuid', right_on='uuid')
x.loc[:,"B":"C"]
df_user_extract_copy.head()
(autos['registration_year'].between(1900,2016)).sum() / autos.shape[0]
df_en['PosOrNeg'] = df_en['polarity_vader'] $ df_en.PosOrNeg[df_en.PosOrNeg == 0.0] = None $ df_en.PosOrNeg[df_en.PosOrNeg > 0.0] = 1.0 $ df_en.PosOrNeg[df_en.PosOrNeg < 0.0] = 0.0 $
autos["last_seen"].str[:10].\ $ value_counts(normalize=True,dropna=False).\ $ sort_index(ascending=True).describe()
1/np.exp(model_new.params)
train = train.drop(columns=["hour"]) $ test = test.drop(columns=["hour"])
df.T.to_csv('../outputs/retrospective/CSFV2_outlook_weekly_90th_per_summary_table_from_{:%Y%m%d}.csv'.format(dates_range[0]))
df = pd.read_excel('SampleData.xlsx', sheet_name='SalesOrders')
def list_dataset(name,node): $     if isinstance(node, h5py.Dataset): $         print(name) $ f.visititems(list_dataset)
fraud_data_updated.shape
scores_median = np.median(sorted(raw_scores)) $ print('The median is {}.'.format(scores_median))
re.findall('mailto:(.*?) so', text)
words_df.head()
top_supporters.amount.plot.barh().set_yticklabels(top_supporters.contributor_fullname)
weather_df = weather_df.fillna(method = "ffill")
cutoff_times = pd.read_csv('s3://customer-churn-spark/p1/SMS-14_labels.csv') $ cutoff_times.head() $ cutoff_times.tail()
print(pd.value_counts(train_df['app'])[:20])
bigdf_read = pd.read_csv('Combined_Comments-Fixed.csv', index_col=0)
p_diffs_actual =-0.1203 + 0.1188 $ greater_than_diff = [i for i in p_diffs if i > (p_diffs_actual)] $ print('Proportion greater than actual difference:', len(greater_than_diff)/len(p_diffs))
against.sort_values('amount', ascending= False).head(10)
sorted(modern_combos.map(lambda c: (c, 1)).countByKey().iteritems(), key=lambda x: x[1], reverse=True)[:20]
data = data.reset_index(drop = True)
df_andorra = df[df['Country'] =='Andorra'] $ df_andorra = df_andorra[['Indicator_id','Country','Year','WHO Region','Publication Status']] $ df_andorra = df_andorra.sort_values(['Indicator_id','Country','Year','WHO Region'],ascending=[False,False,True,False]).drop_duplicates(keep="first") $ df_andorra =df_andorra.reset_index(drop=True) $ df_andorra.head(3)
cities = set(us_cities['City'].str.lower()) $ states = set(us_cities['State full'].str.lower()) $ counties = set(us_cities['County'].str.lower())
plt.style.use('seaborn') $ breed_ax = breed_ratings[1:51].plot(kind='bar', figsize=(20,8)) $ breed_ax.set_title("Highest Rated Dog Breeds on WeRateDogs", fontsize = 24) $ breed_ax.set_xlabel("") $ plt.savefig('plots/breed_ratings.png', bbox_inches='tight')
print(regular_df.head()) $ print(learner_df.head())
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
demographics.head()
data.groupby('SA').size() / 200
autos["registration_year"].describe()
weather_yvr.to_csv('data/weather_yvr_extra.csv', index=False)
filtered = pd.read_pickle('../data/filtered.pkl')
qualConvDataGrouped = qualification.groupby(['lead_source', 'qual_conversion']).qual_conversion.count() $ qualConvpct = qualConvDataGrouped.groupby(level=[0]).apply(lambda x: 100* x / float(x.sum())); 
df.sort_values(by='B', ascending=True)  # Sort by values
day = pd.tseries.offsets.Day(normalize=True)
merged_portfolio_sp_latest_YTD_sp_closing_high = pd.merge(merged_portfolio_sp_latest_YTD_sp, adj_close_pivot_merged $                                              , on=['Ticker', 'Acquisition Date']) $ merged_portfolio_sp_latest_YTD_sp_closing_high.rename(columns={'Adj Close': 'Closing High Adj Close', 'Date': 'Closing High Adj Close Date'}, inplace=True) $ merged_portfolio_sp_latest_YTD_sp_closing_high['Pct off High'] = merged_portfolio_sp_latest_YTD_sp_closing_high['Ticker Adj Close'] / merged_portfolio_sp_latest_YTD_sp_closing_high['Closing High Adj Close'] - 1 $ merged_portfolio_sp_latest_YTD_sp_closing_high
df2 = df2.drop(df.query('landing_page=="old_page"').query('group=="treatment"').index) 
test_df = pd.read_csv('house/test.csv')[list(feature_dict.keys())] $ test_df
print(autos["odometer_km"].unique().shape) $ autos["odometer_km"].describe()
pulledTweets_df.columns
autos['price'].value_counts().sort_index(ascending = True).head(20)
injury_df.iloc[:40,:]
questions['years_attend'].value_counts()
deleted = [] $ for i, x in enumerate(data.date): $     if int(x[:4]) >= 9000: $         deleted.append(i) $         data.drop(data.index[i], inplace=True)
def filter_func(x): $     return x['data2'].std() > 4 $ display('df', "df.groupby('key').std()", "df.groupby('key').filter(filter_func)")
df99 = pd.read_csv('1999.csv')
week21 = week20.rename(columns={147:'147'}) $ stocks = stocks.rename(columns={'Week 20':'Week 21','140':'147'}) $ week21 = pd.merge(stocks,week21,on=['147','Tickers']) $ week21.drop_duplicates(subset='Link',inplace=True)
es = es.entity_from_dataframe(entity_id = 'loans', dataframe = loans, $                               variable_types = {'repaid': ft.variable_types.Categorical}, $                               index = 'loan_id', $                               time_index = 'loan_start')
le.classes_
raw_data[["campaign_id", "campaign_budget", "objective", "bid"]].drop_duplicates()
ip_clean = ip.copy()
fig, axes = plt.subplots(3, figsize=(10, 8)) $ plot_series_and_differences(series=therapist_duration, ax=axes, num_diff=2, title='Therapists') $ fig.tight_layout()
res.summary()
%%bash $ model_dir=$(ls ${PWD}/taxi_trained/export/exporter) $ gcloud ml-engine local predict \ $   --model-dir=${PWD}/taxi_trained/export/exporter/${model_dir} \ $   --json-instances=/tmp/test.json
talks.drop('name',axis = 1,inplace = True)
print(well_data.head())
df['acct_type'].unique()
sorted(domain_mentions_unique_comments[1].items(), key=lambda x: x[1], reverse=True)
import statsmodels.api as sm $ convert_old = df2.query('group=="control"').converted.mean() $ convert_new = df2.query('group=="treatment"').converted.mean() $ n_old = len(df2.query('group == "control"')) $ n_new = len(df2.query('group == "treatment"'))
plt.figure(2) $ goog_plot = plt.subplot() $ goog_plot.plot(goog['Close'],color='blue') $ plt.legend(['Google Close Value'],loc="upper left") $ plt.title('Valores de Cierre Google')
data = brazil.join(biology, lsuffix='_brazil', rsuffix='_biology') $ data.plot()
convert_rate_p_new = df2.converted.mean() $ print('Convert rate of p_new under the null is:{}'.format(convert_rate_p_new))
results.summary()
week2 = week1.rename(columns={14:'14'}) $ stocks = stocks.rename(columns={'Week 1':'Week 2','7':'14'}) $ week2 = pd.merge(stocks,week2,on=['14','Tickers']) $ week2.drop_duplicates(subset='Link',inplace=True)
df_2018 = df[df.year==2018] $ predicted_talk_indexes = predicted_talks_vector.nonzero()[0] + len(df[df.year==2017]) $ df_2018_talks = df_2018.loc[predicted_talk_indexes] $ print df_2018_talks.count() $ print df_2018_talks
slDf = pd.concat([slCases, slDeaths],axis=1) $ slDf.index.name = 'Date' $ slDf.head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ print("The z-score is: %.4f\nThe p-value is: %.4f" %(z_score, p_value))
df2[df2['user_id'].duplicated() == True] $ df2.query('user_id == 773192')
train.business_day = train.business_day.map(lambda x: 1 if x == True else 0) $ train.holiday = train.holiday.map(lambda x: 1 if x == True else 0)
data[data['lat-lon'] == '-34.3402525,-58.7849434']
ratings.count()
conn_a.commit()
df4.head()
df.drop('gender',axis=1,inplace=True)
sns.violinplot(california.FIRE_SIZE)
min_train, max_train = pd_train_filtered['unit_sales'].min(), pd_train_filtered['unit_sales'].max()
popCon[popCon.content == 'post'].sort_values(by='counts', ascending=False).head(10).plot(kind='bar')
df.head()
first_result.find('strong').text[0:-1]
lv_workspace.load_indicators(subset = 'B')
data['Processing Time'] = data['Closed Date'].subtract(data['Created Date']) $ data['Processing Time'].describe()
import statsmodels.api as sm $ df_new['intercept'] = 1 $ log_mod = sm.Logit(df_new['converted'], df_new[['intercept','UK','US']]) $ res = log_mod.fit() $ res.summary2()
df = pd.DataFrame()  #If in doubt, start with an empty DataFrame. $ df['SP500'] = np.log(p['Adj Close']) - np.log(p['Adj Close']).shift(1)  #Make sure there's no ^ in the column name. $ p = web.DataReader("^VIX", 'yahoo', start, end) $ df['VIX'] = np.log(p['Adj Close'])-np.log(p['Adj Close']).shift(1) $ df.tail()
data1=data1_new.loc[:69,:]
new_reps.drop(new_reps.index[0], inplace = True) $ new_reps.reset_index() $ new_reps.head()
pd.read_sql('desc actor', engine)
til_today = pd.date_range(hits_df.index[0], hits_df.index[-1])
evaluator.plot_confusion_matrix(type='entity_level', normalize = True)
with open('filename.pickle', 'rb') as handle: $     b = pickle.load(handle)
weights = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)
perf_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)
a,b,c=31.88052,26.55708,10.63995
dates_with_nulls=len(stores_tran_nulls['date'].unique()) $ all_dates=len(class_merged['date'].unique()) $ dates_with_nulls/all_dates
df.groupby('character_id').raw_character_text.unique().head()
autos["price"].value_counts().sort_index(ascending = False).head(20)
sns.distplot(dfz.favorite_count, color = 'red', label = 'Favorites') $
wrd_api.sample(3)
df_treatment_old = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_control_new = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ misaligned = len(df_treatment_old) + len(df_control_new) $ misaligned_df = pd.concat([df_treatment_old, df_control_new]) $ misaligned
trump['hour'] = trump['est_time'].dt.hour + (trump['est_time'].dt.minute)/60 + (trump['est_time'].dt.second)/(60**2) $ sns.distplot(trump['hour']) $ plt.show() $
with open("data/treatments_twitter.json", 'w') as fh: $     json.dump(treatment_counts, fh)
print(norm.cdf(z_score)) #value to test significance $ print(norm.ppf(1-(0.05))) # critical value for one sided test $
row_inf = df2[df2.duplicated(["user_id"], keep=False)] $ print("The row information for the repeated 'user_id' is:\n {}".format(row_inf))
date = pd.to_datetime(date_str) $ date
tall.any()
df_goog['month'] = df_goog.index.month
df_date_vs_count.head()
pt_weekly['WoW'] = pt_weekly['num_listings'].diff(periods=1) $ pt_weekly.head(10)
def relevant_ping(p): $     parent = p.get('payload', {}).get('keyedHistograms', {}).get('IPC_MESSAGE_SIZE', {}).get('PLayerTransaction::Msg_Update') $     content = p.get('payload', {}).get('processes', {}).get('content', {}).get('keyedHistograms', {}).get('IPC_MESSAGE_SIZE', {}).get('PLayerTransaction::Msg_Update') $     return parent is not None and content is not None and parent['sum'] == content['sum'] $ relevant_pings = pings.filter(relevant_ping)
contractor_final.to_sql('contractor', con=engine,if_exists='append',index = False)
table = soup.find_all('table', id="tablepress-mars")
math_data = results_dict['math_summaries'][0] $ print("Data is saved as a {} object.\nSubject answer {}% math problems correctly".format(type(math_data), $                                                                                           math_data.percent_correct))
temp_us_full = temp_nc.variables['air'][:, lat_li:lat_ui, lon_li:lon_ui]
import sqlite3 $ con = sqlite3.connect(":memory:") $ tbl = data.to_sql("stock", con, index=False)
fe.stat(lossprob2)
autos.rename({'odometer':'odometer_km'},axis = 1, inplace=True)
stores.toPandas().head()
pca.components_
print(means_bb.to_string())
n2 = len(df.query('landing_page=="old_page"').query('group=="treatment"')) $ n2
autos["num_pictures"].value_counts()
df.to_csv('integratedData.csv')
df.drop(['parsed', 'github_id', 'node_id'], axis=1, inplace=True)
fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True) $ sns.boxplot(x="frcar", y="y", data = psy_prepro, ax=ax1).set_title("Anxiety driving/riding in a car") $ sns.boxplot(x="jmp2w", y="y", data = psy_prepro, ax=ax2).set_title("Feel jumpy and restless in past 2 weeks") $
df[treatment]['converted'].mean()
for e in _td: $     print e.text
df2['intercept'] = 1 $ log_mod = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = log_mod.fit() $ results.summary()
df_merged.to_csv('twitter_archive_master.csv', index=False)
access_logs_df.
os.listdir("./")
P_Treatment_Converted = (df2.query('converted == 1')['group'] == 'treatment').mean() $ P_Treatment = (df2.group=='treatment').mean() $ P_Converted_Treatment = (P_Treatment_Converted * P_Converted) / P_Treatment $ P_Converted_Treatment
list_users = df2['user_id'].value_counts() $ list_users.head()
locations = DataSet['userLocation'].value_counts()[:10] $ print(locations)
val_utf8 = val.encode('utf-8') $ val_utf8 $ type(val_utf8)
%%ml batch_predict --model train/evaluation_model --output evalme $ format: csv $ prediction_data: $     csv: './eval.csv'
df[['retweet_count', 'favorite_count']].describe()
df_clean3.loc[1202, 'rating_numerator'] = 11 $ df_clean3.loc[1202, 'rating_denominator'] = 10
df['datetime'] = df['datetime'].map(convert_a)
df2['ltv_bin_median'] = df2['ltv'].map(lambda x: 1 if x > 29 else 0) $ df2['ltv_bin_mean'] = df2['ltv'].map(lambda x: 1 if x > 42 else 0)
df = pd.merge(releases, demographics, on='hash_id', how='left')
extract_all.loc[(extract_all.APPLICATION_DATE_short>=datetime.date(2018,6,1)) $                &(extract_all.APP_PRODUCT_TYPE.isin(['TL','RL'])) $                &(extract_all.CLA_RiskScore==1)].groupby('app_branch_state').size()
all_data_long['api'].value_counts(dropna=False)
raw_weights_round_2 = tf.add(raw_weights, agreement, $                              name="raw_weights_round_2")
cleansed_search_df.to_csv('./output/ganalytics_search_products.txt', sep='\t', index=False, quoting=csv.QUOTE_NONE)
df_concensus_uaa = df_concensus_uaa.set_index(['latest_consensus_created_date']) $ df_concensus_uaa
own_star['created_at_star'] = own_star['created_at_star'].fillna(own_star['created_at_own'])
dataframe.head(20)
all_metrics = client.experiments.get_metrics(experiment_run_uid)
joined=join_df(joined,weather,["State","Date"]) $ joined_test=join_df(joined_test,weather,["State","Date"]) $ sum(joined['Max_TemperatureC'].isnull()),sum(joined_test['Max_TemperatureC'].isnull())
dfrecent = dfdaycounts[dfdaycounts['created_date']> pd.to_datetime('2012-12-31')]
for tweet in public_tweets: $     print(json.dumps(tweet, sort_keys=True, indent=4))
import tweepy $ trump_tweets = get_tweets_with_cache("realDonaldTrump", "keys.json") $ trump_df = make_dataframe(trump_tweets)
unique_urls.query('num_authors >= 10').sort_values('avg_votes', ascending=False)[0:50][['url', 'avg_votes']]
num_rows = df2.shape[0] $ new_rows = df2.query("landing_page == 'new_page'").shape[0] $ new_rows/num_rows
from sklearn.externals import joblib $ date_txt = '0402' $ joblib.dump(gbm, "gbm_"+ date_txt + '.pkl') $ joblib.dump(rfc, "rfc_"+ date_txt + '.pkl') $ feature_imp.to_csv("Feature importance 0420.csv", index = False)
output= "Select * from ABC limit 10" $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['user_id','Tweet Content','Retweets'])
active_unordered = unordered_df.loc[~churned_unord]
p_diffs = [] $ for _ in range (10000): $     sample = (np.random.binomial(1, p_new, n_new).mean()) - (np.random.binomial(1, p_old, n_old).mean()) $     p_diffs.append(sample.mean())
df.drop_duplicates(['title'],keep= 'first',inplace =True)
df.loc[:,"message"] = df.message.str.replace('\d+', '') #Remove numbers.
top_car_brands = pd.DataFrame(top_brands_mean_prices, columns=['top_brands_mean_prices']) $ top_car_brands['top_brands_mean_mileages'] = top_brands_mean_mileages $ top_car_brands
plt.plot(tfidf_svd_v2.explained_variance_ratio_);
spanair = df[df["text_3"].str.contains("spanair", case = False)] $ spanair.text_3.str.split(expand=True).stack().value_counts().head(10).reset_index()
calls_df.loc[(calls_df["call_type"]=="Important"),"phone number"].value_counts()[0:9]
weekday_data = pd.get_dummies(train_data['weekday'], prefix='weekday', drop_first=True) $ other_data = train_data.drop(['weekday'], axis=1) $ training_data = pd.concat([weekday_data, other_data], axis=1) $ training_data.head(5)
mydata.to_csv("Data-5year-2012-20180617.csv")
dul_final = dul1.rename(columns={'ISBN RegEx':'ISBN'}) $ dul_final
client.repository.delete(experiment_uid)
df1 = pd.read_csv('captures/botnet-capture-20110810-neris.csv')
rfmTable = df.groupby('cust_id').agg({'order_date': lambda x: (NOW - x.max()).days, 'count': lambda x: len(x), 'total_spend': lambda x: x.sum()}) $ rfmTable['order_date'] = rfmTable['order_date'].astype(int) $ rfmTable.rename(columns={'order_date': 'Recency', $                          'count': 'Frequency', $                          'total_spend': 'Monetary_value'}, inplace=True)
df_arch_clean.to_csv('twitter_archive_master.csv') $ df_img_algo_clean.to_csv('df_img_algo_clean') $ df_tweet_json_clean.to_csv('df_tweet_json_clean') $
from statsmodels.api import Logit $ from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)
taxi_hourly_df.loc[(taxi_hourly_df.missing_dt == True) & $                     (taxi_hourly_df.index.hour.isin((0, 1, 2, 3, 4, 5, 6))) & $                     (taxi_hourly_df.num_pickups.isnull()) & $                     (taxi_hourly_df.num_passengers.isnull()), : $                    ]
my_data.filter(lambda line: ".txt" in line)
%matplotlib inline $ from keras.models import Model $ from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization $ from keras import optimizers
model.add(Conv1D(filters=30,kernel_size=30, $                  input_shape=(527,1), $                  activation='relu', $                  padding='same'))
nnew=df2[df2.landing_page=="new_page"].count()[0] $ nnew
%matplotlib inline $ import matplotlib.pyplot as plt $ import seaborn as sns
train.isnull().sum()
%%time $ model = KeyedVectors.load_word2vec_format('..'+sep+"Data" + sep +"word2vec"+sep+ "wiki.es.vec", $                                            binary=False) 
for col in data_df.columns: $     if np.unique(data_df[col].dropna().astype(str)).shape[0] <= 1: $         print(col)
new_page_converted = np.random.choice([0,1],size=n_new,p=[(1-p_new),p_new])
df.dist_km.describe() #1.87 million trips in Sept 2017, with average trip distance of 1.88km.
sp500.loc['MMM']
dti1 = pd.to_datetime(['8/1/2014']) $ dti2 = pd.to_datetime(['1/8/2014'], dayfirst=True) $ dti1[0], dti2[0]
df['DAY'] = 1 $ df['DATE'] = pd.to_datetime(df[['YEAR','MONTH','DAY']])
tweets_clean.doggo.value_counts()
f, ax = plt.subplots(figsize=(10, 10)) $ correlation_df = df_train.corr() $ sns.heatmap(correlation_df, mask=np.zeros_like(correlation_df, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True)) $ plt.show()
raw_data.drop(null_name.index, axis=0, inplace=True)
driver.find_element_by_xpath('//*[@id="middleContainer"]/ul[1]/li[3]/a').click()
sqlc = sqlite3.connect('iris.db')
df.sample(5)
print "Probability of control group converting:",df2.groupby(['group'], as_index = False)['converted'].mean().iloc[0,1]
date_ini = str(date(2013, 1, 1)) $ date_end = str(date.today()) $ df = get311NoiseComplaints(date_ini, date_end)
%pylab inline $ pandas_ds["time2close"].plot(kind="hist")
counts_df.sample(20)
ws = Workspace(workspace_name = workspace_name, $                subscription_id = subscription_id, $                resource_group = resource_group) $ ws.write_config()
from statsmodels.tsa.stattools import adfuller as ADF $ print() $ print(ADF(resid_701.values)[0:4]) $ print() $ print(ADF(resid_701.values)[4:7])
x = np.random.normal(size=4)
validation_data.groupby(['last_pymnt_d', 'loan_status']).agg({'member_id':lambda x:x.nunique(0)})
df = pd.DataFrame(cleaner_tweets) $ df.tail()
print(tweets.info()) $ tweets.describe()
log_loss(y_val, val_pred_probs, labels=rf.classes_)
SVPOL(data/'realdata'/'DD.dat').to_dataframe().head()
df.drop(df.query("group == 'treatment' and landing_page == 'old_page'").index, inplace = True) $ df.drop(df.query("group == 'control' and landing_page == 'new_page'").index, inplace = True)
for i in range(len(df)): $     print(df.iloc[i]['account'])
plotdf['forecast'] = plotdf['forecast'].interpolate() $ plotdf['forecastPlus'] = plotdf['forecastPlus'].interpolate() $ plotdf['forecastMinus'] = plotdf['forecastMinus'].interpolate()
votes=[] $ for t in t_open_resol: $     i=t['index'][1] $     votes.append(data_issues_transitions['vote_count'][i])
cityID = 'fef01a8cb0eacb64' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Akron.append(tweet)   
diversity=[] $ for a in who_counts: $     s=set(a) $     number=len(s) $     diversity.append(number) $
import pprint as pp $ pp.pprint(lds.describe())
spacy_tok = spacy.load('en')
df.head()
monte.str.split()
first_cluster.fit(docs)
def returnCategory(x): $     return x.split(' ~ Category: ')[1] $ def returnName(x): $     return x.split(' ')[0]
ts = pd.Series(np.random.randn(3), index=dr)
station_count = session.query(func.count(Station.id)).all() $ station_count
s = pd.Series([2,-1,3,5]) $ s
td = pd.read_excel('input/data/TransmissionSampleData.xlsx', $                     parse_cols='B:F', $                     skiprows=[16], $                     header=11, $                     index_col=0)
time1 = datetime.strptime(dfnychead["Created Date"][1],'%M/%d/%Y %H:%m:%S AM') $ print "time1 =", time1 $ print "time1.year =", time1.year
p_new=df2['converted'].sum()/len(df2) $ p_new
dfRegMet2015.shape
del_list = [] $ del_list = df_arch_clean[df_arch_clean['retweeted_status_timestamp'].notnull()]['tweet_id'] $ del_index = del_list.index $ df_arch_clean = df_arch_clean.drop(del_index) $
df2[['US','UK','CA']]=pd.get_dummies(df2['country']) $ df2.head()
signal = get_long_short(close, lookback_high, lookback_low) $ project_helper.plot_signal( $     close[apple_ticker], $     signal[apple_ticker], $     'Long and Short of {} Stock'.format(apple_ticker))
tl_2030 /= 1000 $ tl_2030_norm = tl_2030 ** (10/11) $ tl_2030_norm = tl_2030_norm.round(1) $ tl_2030_alpha = tl_2030 ** (1/3) $ tl_2030_alpha = tl_2030_alpha / tl_2030_alpha.max().max()
mydata.head(3)
joined_test_df = joined[joined.unit_sales.isnull()]
data = pd.DataFrame( $     {'time_step': time_step, $      'valuea': values, $     })
data = pd.DataFrame(Rationales,columns=["TweetID","Rationales"])
df.to_csv("msft_temp2.txt", sep="|") $ ! head -5 msft_temp2.txt
wine_reviews.head()
closed_pr = PullRequests(github_index).is_closed().get_cardinality("id_in_repo").by_period(period="quarter") $ print("Trend for quarter: ", get_trend(get_timeseries(closed_pr)))
Test.SetFlowValues(8.5, 10.5, 0.25)
df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f','h'],columns=['one', 'two', 'three']) $ df
trading.backtest()
dmh.PandasDataFrame.__subclasses__()
autos['date_crawled'].str[:10].value_counts(normalize=True, dropna=False).sort_index()
df['Date'] = pd.to_datetime(df['Date'])
raw.rename(columns = lambda x: x.lower().replace(' ', ''), inplace = True) $ raw.drop(['ref', 'unnamed:2'], axis = 1, inplace = True, errors='ignore') $ raw.head(2)
import test_package.print_hello_function_container #import module $ test_package.print_hello_function_container.print_hello_function() # call function from the imported copy of library $ import test_package.print_hello_function_container $ print_hello = test_package.print_hello_function_container.print_hello_function $ print_hello()
celtics.rename(columns={'Playoff':'playoff','Opponent':'opponent'}, inplace=True)
trans = joined[['date', 'store_nbr','transactions']].drop_duplicates() $ trans.set_index('date', inplace=True)
pax_raw.paxstep.max() <= step_threshold
trunc_df = trunc_df.drop(trunc_df.columns[[0]],1)
eta = therm_fiss_rate / fuel_therm_abs_rate $ eta.get_pandas_dataframe()
pd.__version__
zp = zip(list(trade_data_dict.values()),list(trade_data_dict.values())[1:]) $ (mx, st_d, ed_d) = max([(abs(t.close-p.close), t.date, p.date) for t, p in zp]) $ print('The largest intraday price change was: {:.2f}'.format(mx)) $ print('Between trading days {} and {}'.format(st_d, ed_d))
df4.tail() # last 5 rows
df_new['intercept'] =1 $ df_new = df_new.join(pd.get_dummies(df_new.country)) $ df_new = df_new.join(pd.get_dummies(df_new.landing_page))
fullProduct.shape
learner.fit(lrs / 2, 1, wds=wd, use_clr=(32, 2), cycle_len=1)
testObjDocs.reindex_OutDF() $ testObjDocs.outDF[984:991]
p_diffs=np.array(p_diffs) $ plt.hist(p_diffs) $ plt.axvline(d, color='red') 
min_avail = df.cust_avail_v3.min() $ max_avail = df.cust_avail_v3.max() $ size = df.cust_avail_v3.size $ cust_avail_v3 = np.random.uniform(min_avail, max_avail, size) $ cust_avail_v3
df_A.groupby('Gender').get_group('M')
html = browser.html $ soup = bs(html, 'html.parser') $ current_img_link = soup.find_all('article', class_="carousel_item") $ current_img_link
pd.options.mode.chained_assignment = None # This just stops Python from popping up an annoying warning $ dframe_team.drop(dframe_team.columns[[6,7]], inplace=True, axis=1) # dropping 'cut_year' and 'end_cut' columns, the 'tenure' column could be interesting
significant_features = df_sig_features.xfeature.tolist()
X.head()
test_sentence = pd.Series(test_sentence) $ test_sentence[0]
for col in missing_info: $     num_missing = Users_first_tran[Users_first_tran[col].isnull() == True].shape[0] $     print('number missing for column {}: {}'.format(col, num_missing))
scores['point_diff'] = scores['home pts']- scores['away pts'] $ scores.head()
plt.figure(figsize=(16,10)) $ plt.clf() $ sns.boxplot(x='Journal',y='PubDays',hue='Publisher',data=df_less) $ plt.xticks(rotation=90)
import string $ pd.options.mode.chained_assignment = None $ from datetime import datetime
arma_mod40 = sm.tsa.ARMA(dta_713, (4,0)).fit(disp=False) $ print(arma_mod40.params)
listings = pd.read_csv('../final_project/listings.csv') $ listings.head()
p.flatten()
xgb_model = xgb.XGBRegressor(random_state=1)
data_sample['endDate'] = pd.Series([datetime.datetime.strptime(i, fmt) for i in data_sample['endDate']])
df_cities = pd.read_csv('cities.csv') $ df_cities
for filename in ("my_df.csv", "my_df.html", "my_df.json"): $     print("#", filename) $     with open(filename, "rt") as f: $         print(f.read()) $         print() $
results.summary() $
len(reddit['title'].unique())
target_var = 'win_differential' $ y_data = use_data.loc[:, target_var]
post_req = requests.post(post_url, data=json_dat, headers=headers) $ json_out = json.loads(post_req.content)
df3.country.value_counts()
from biopandas.mol2 import split_multimol2 $ mol2_id, mol2_cont = next(split_multimol2('./data/40_mol2_files.mol2')) $ print('Molecule ID:\n', mol2_id) $ print('First 10 lines:\n', mol2_cont[:10])
ds_complete_temp_CTD_1988 = xr.Dataset.from_dataframe(df_complete_temp_CTD_1988)
df1 = transactions.groupby(['UserID','ProductID']).sum() $ df1
autos["brand"].value_counts(normalize=True) $
to_be_predicted_Day3 = 21.37332097 $ predicted_new = ridge.predict(to_be_predicted_Day3) $ predicted_new 
ratings_matrix = df_ratings.pivot_table(index=['user_id'], columns=['video_id'], values=['overall_rating_value']) $ ratings_matrix                                  
model = learn.model $ model.reset() $ model.eval()
data.head()
df['sentiment'].value_counts()
grouped_by_plot = merged_data.groupby(['plot_id','taxa'])['record_id'].count() $ grouped_by_plot.head()
full_data = pd.concat([train_data,test_data], ignore_index=True) $
cols = ['sunspots', 'definite'] $ sunspots = sunspots[cols] $ sunspots.iloc[10:20, :]
total_trt = (df2['group']=='treatment').sum() $ prob_trt = total_trt/unique_users $ df2[(df2['group']=='treatment') & (df2['converted']== 1)].sum() 
social_disorder = class_data.iloc[:, 3:13] $ social_disorder.head()
df.iloc[4,1]
df['time_online'] = (df.lastSeen - df.dateCreated).apply(lambda x: int(x.days))
TestData_ForLogistic = dum.drop(['City_Code', 'Employer_Code', 'Customer_Existing_Primary_Bank_Code', 'Source', $                                  'DOB', 'Lead_Creation_Date'], axis=1)
tweet_df.columns
(details.Runtime == 0).value_counts()
np.random.seed(123456) $ df = pd.DataFrame(np.random.randn(5, 4), columns=['A', 'B', 'C', 'D']) $ df
sim_p_old = old_page_converted.mean() $ sim_p_new = new_page_converted.mean()
X = preprocessing.StandardScaler().fit(X).transform(X.astype(float)) $ X[0:5]
df_archive["puppo"].value_counts()
plt.scatter(X2[:, 0], X2[:, 1]);
with open(os.path.join(url.split('/')[-1]), mode='wb') as file: $     file.write(response.content)
user_unique= df.nunique()['user_id'] $ len(df.user_id.unique()) $ print("Number of unique users is : {}".format(user_unique))
print(df.apply(lambda x: abs(x)),'\n')  # example of element-by-element lambda fx $ print(df.abs()) # same result (to confirm)
for file in local_orig_keys: $     with rio.open(file) as src: $         print(src.profile)
df5 = df4.set_index(pd.DatetimeIndex(df4['Date'])) $ df5 = df5[['BG']].copy() $ df5
mlb_id = int(input("Please enter the mlbam id to retrieve player name: ")) $ player_ids = [mlb_id] $ data = pyb.playerid_reverse_lookup(player_ids, key_type='mlbam') $ data
vocabulary_expression['component_3'].sort_values(ascending=False).head(7) $
donors_c.loc[donors_c['Donor Zip']== 'n19', :]
pivoted.T[labels==1].T.plot(legend=False, alpha = 0.1);
g_kick_data = kick_data_state.groupby(['blurb']) $ g_filtered = g_kick_data.filter(lambda x: len(x) > 1).sort_values(['blurb','launched_at']) $ len(g_filtered)
trips.describe()
hits_df = pd.read_csv('ipynb_counts.csv', index_col=0, header=0, parse_dates=True) $ hits_df.reset_index(inplace=True) $ hits_df.drop_duplicates(subset='date', inplace=True) $ hits_df.set_index('date', inplace=True) $ hits_df.sort_index(ascending=True, inplace=True)
print('NLTK Naive Bayes Accuracy:', nltk.classify.accuracy(naive_bayes_classifier, test_set) * 100, '%')
with open('components/pop_models/excitatory_pop.json') as exc_data: $     exc_prs = json.load(exc_data) $ pprint.pprint(exc_prs)
churned_ordered['start_date'] = pd.to_datetime(churned_ordered['scns_created'].apply(lambda x:x[0])).dt.strftime('%Y-%m')
tzs = DataSet['userTimezone'].value_counts()[:10] $ print(tzs)
df.shape
data.groupby('bandit')['expid', 'visibility'].mean()
random.sample(words.items(), 10)
sub_df['sub_len'] = [len(x.split(' ')) for x in sub_df['text']]
inter1 = pd.read_pickle(folderData + 'interactions1Data.pkl') $ inter1.shape
for tweet in df_tweets: $     print(TextBlob(tweet).sentiment) $     print(TextBlob(tweet).sentiment.subjectivity) $
df['comments'] = df['comments'].str.replace('\\', '')
plt.scatter(sing_fam.baths.values, sing_fam.rp1lndval.values);
df_birth['Continent'].value_counts(dropna=False)
df.dtypes
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = plot_acf(dta_690.values.squeeze(), lags=40, ax=ax1)
df['campaign'].value_counts()
random_labels = random_sample['labels'].tolist()
feature_imp = pd.DataFrame({"imp": Model.feature_importances_,"col": features}) $ feature_imp = feature_imp.sort_values("imp", ascending=False) $ feature_imp.to_csv('feature_imp.csv')
X_test.columns #age_well is 10
os.environ["asset_id"] = "users/resourcewatch/ene_018_wind_energy_potential" $ !earthengine upload image --asset_id=%asset_id% %Zgs_key%
WorldBankdf = sqlContext.read.json("world_bank.json.gz")
ng_m6 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=100, sg=1)
ohlc = pybacktest.load_from_yahoo('SPY') $ ohlc.tail()
df.price_doc.hist(bins=100) $
df_h1b_nyc_ft = df_h1b_nyc[df_h1b_nyc.full_time_pos=='Y'] $ print('There are {:.0f} visa applications for full-time jobs located in NYC in this dataset.'.format(df_h1b_nyc_ft.shape[0])) $
df_by_donor = donations[['Donor ID','Donation ID', 'Donation Amount', 'Donation Received Date']].groupby('Donor ID', as_index=False).agg({'Donation ID': 'count', 'Donation Received Date': 'max', 'Donation Amount': ['min', 'max', 'mean', 'sum']})
for topic in ldamodel.show_topics(num_topics=10, formatted=False, num_words=10): $     print("Top terms in topic {}: ".format(topic[0])) $     words = [w for (w, val) in topic[1]] $     print(words)
afx = r.json()
pop_df = pop.unstack() $ pop_df
df_test.dtypes $
df_location = pd.DataFrame.from_dict(df_us_.location.to_dict()).transpose() $ df_location.head(5) $
comments.describe()
(pd.Series([list(chain(*user_slice[c])) for c in user_slice.columns],index=USER_PLANS_df.columns))
lr=3e-3 $ lrm = 2.6 $ lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])
df.status.value_counts()
print(triplet_model.summary())
Train.DOB_clean.value_counts()[1]
ra_uni=total['ra'].values $ dec_uni=total['dec'].values $ num=total['chips'].values $ ra_l=[Angle(i, u.deg).to_string(unit=u.hr, sep=':', precision=1, pad=True) for i in ra_uni] $ dec_l=[Angle(i, u.deg).to_string(unit=u.degree, sep=':', precision=1, alwayssign=True, pad=True) for i in dec_uni] $
df = df.groupby(['Year', 'Month']).agg({'Temperature': np.mean, 'Precipitation': np.mean, 'Description': 'count'}).reset_index()
print('Most chuuch:', tweets_pp[tweets_pp.handle == 'chuuch'].sort_values( $     'ch000ch_pp', ascending=False).text.values[0]) $ print('Least chuuch:', tweets_pp[tweets_pp.handle == 'chuuch'].sort_values( $     'ch000ch_pp', ascending=True).text.values[0])
df2['DepTimeStr'].count() == df2['DepTime'].count()
model = sm.OLS(df_new.converted, df_new[['UK', 'US', 'ab_test', 'intercept']]) $ results = model.fit()
talks = pd.merge(talks_train[['description', $                              'id', 'keywords', 'name']],labels,on = 'id')
by_area = df.groupby('Reporting Area') $ by_area.describe()
rural_summary_table = pd.DataFrame({"Average Fare": rural_avg_fare, $                                    "Total Rides": rural_ride_total}) $ rural_summary_table.head()
pred = rfbestgini.predict(cvecogXfinaltemp) $ print classification_report(pred, ogy) $ print confusion_matrix(ogy, pred) $ print "cross val score accuracy :"+str(sum(cross_val_score(rfbestgini, cvecogXfinaltemp, ogy))/3) $ print 'roc_auc score :'+str(sum(cross_val_score(rfbestgini, cvecogXfinaltemp, ogy, scoring='roc_auc'))/3) $
underlying_symbol = 'IBM' $ options_obj = pd.Options('IBM', 'yahoo') $ options_frame_live = options_obj.get_all_data() $ options_frame_live.to_pickle('options_frame.pickle')
print("Converted proportion : " + str(df.converted.mean()))
country_with_least_expectancy = le_data.idxmin(axis=0) $ country_with_least_expectancy
users = mydata[['user.id','user.screen_name','user.location','user.verified','user.followers_count','user.friends_count','user.statuses_count','user.created_at']] $ users = users.drop_duplicates(subset = ['user.id']) $ users['klout_score'] = [0.0] * len(users) $ users
gdp = web.DataReader("GDP","fred",datetime.date(2012,1,1),datetime.date(2014,1,27)) $ gdp
pivoted.T[labels == 0].T.plot(legend=False, alpha=0.1)
from dask.diagnostics import visualize $ from bokeh.io import output_notebook $ from bokeh.resources import CDN $ output_notebook(CDN, hide_banner=True) $ visualize([prof, rprof])
dfCat = df.groupby(['Category','Day of Week']).sum() $ dfCatUnstacked = dfCat.unstack()['Amount'] $
tweet_data.columns
n_new = df2.query('landing_page == "new_page"').user_id.count() $ n_new
twitter_archive.info()
station_count.iloc[:,0].idxmax() $
subwaydf[subwaydf['C/A'] == 'A025'].nsmallest(20, '4HR_Entries')
archive_copy.head(1)
apple['2017-07']['Close'].mean()
import statsmodels.api as sm $ convert_old = df2.query('group=="control"')['converted'].sum() $ convert_new = df2.query('group=="treatment"')['converted'].sum() $ n_old = len(df2.query('group=="control"')) $ n_new = len(df2.query('group=="treatment"'))
train_data.groupby(['device.browser']).agg({'totals.transactionRevenue': 'mean'}).reset_index().set_index("device.browser",drop=True).plot.bar()
ypred.shape
path = os.path.join('Final emission factors.csv') $ ef = pd.read_csv(path, index_col=0)
print ts.groupby('year').sum().tail(5)
learn.freeze_to(-2)
print('Slope FEA/2 vs experiment: {:0.2f}'.format(popt_opb_brace_saddle[1][0])) $ perr = np.sqrt(np.diag(pcov_opb_brace_saddle[1]))[0] $ print('One standard deviation error on the slope: {:0.2f}'.format(perr))
for link in soup.find_all('a'): $     print(link.get('href'))
from IPython.core.interactiveshell import InteractiveShell $ InteractiveShell.ast_node_interactivity = "all" $ import platform $ platform.python_version() $
X_train, y_train = train['title'].values, train['tags'].values $ X_val, y_val = validation['title'].values, validation['tags'].values $ X_test = test['title'].values
url_CLEAN1A = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1A.csv" $ url_CLEAN1B = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1B.csv" $ url_CLEAN1C = "https://raw.githubusercontent.com/sb0709/bootcamp_KSU/master/Data/CLEAN1C.csv" $
obj.sort_values()
new_page_converted = np.random.binomial(1, p_new, n_new) $ new_page_converted[:5]
autos["odometer_km"].value_counts() $
TEXT = data.Field(lower=True, tokenize='spacy')
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer $ analyser = SentimentIntensityAnalyzer()
np.bincount(y_train)
df = pd.DataFrame(predict_result, columns = ["prob_0", "prob_1"])
features_to_use = ["latitude", "longitude", "price", $                    "num_photos", "num_features", "num_description_words" $                    ] $ features_to_use.append('manager_id')
summer = beirut.ix[datetime(2015,6,1) : datetime(2015,9,30)] $ summer.columns.values
query_result1.fields
iris_fromUrl.describe()
np.set_printoptions(precision=2, suppress=True)
tweets_master_df.columns
s = pd.Series(np.random.randint(0,10,size=10)) $ s
crime['Sex'].value_counts()
pats_chiefs_nov8_tweets = pbptweets.loc[(pbptweets['screen_name'] == 'patriots_pbp') | $                                         (pbptweets['screen_name'] == 'chiefs_pbp')]
print(df,'\n') $ print(df.apply(lambda x: x.max()-x.min()))
save_n_load_df(promo_df, 'promo_df2.pkl')
baseurl = 'https://gender-api.com/get?name=' $ key='oxxyvSjsHaSKyuLUFF'
reconstruction_mask_reshaped = tf.reshape( $     reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1], $     name="reconstruction_mask_reshaped")
df3[df3['group'] == 'treatment'].head()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])
df_json[df_json['id'].duplicated(keep=False)]
source_list = df1_clean.source.unique() $ source_list
def get_historical_price_minute(coin, to_curr=CURR, exchange=EXCHANGE, toTs=time.time(), **kwargs): $
autos[autos.price=='$0']
Numerator=df2.loc[(df2['group']=='control') & (df2['converted']==1),].shape[0] $ Denominator=df2.loc[df2['group']=='control',].shape[0] $ ControlConverted= Numerator/Denominator $ print("The probability is", Numerator/Denominator)
users.schema
bitcoin_github_issues_url = blockchain_projects_github_issues_urls[0] $ bitcoin_github_issues_df = pd.read_json(get_http_json_response_contents(bitcoin_github_issues_url))
a.max(1)
plot_autocorrelation(series=RNPA_new_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of RNPA hours new patients')) $ plot_autocorrelation(series=RNPA_existing_hours.diff()[1:], params=params, lags=30, alpha=0.05, title='ACF {}'.format('first difference of RNPA hours new patients'))
a[a.find('t')]
months = original_months $ months.head(5) $ print(months.shape)
tmi['weekFriendliness'].max()
p_sort=p_sort.rename(columns = {'Product_x':'Product'}) $ p_sort.drop(['Product_y'], axis = 1, inplace = True) $ p_sort
store_items.interpolate(method = 'linear', axis = 0)
import pandas as pd $ from datetime import timedelta $ %matplotlib inline $ unemp = pd.read_csv('../10.02-intro_to_timeseries-lesson/datasets/seasonally-adjusted-quarterly-us.csv')
df['user_type'].nunique()
stop_words_update = list(pipe_cv.get_stop_words()) $ stop_words_update.append('pron') $ stop_words_update.append('aa')
girls_shuffled = girls_by_name.sort_values('Year') $ girls_shuffled.head()
df2 = df2.drop_duplicates(['user_id'], keep='first') $ df2[df2['user_id'] == 773192] 
ts = pd.Series(np.random.randint(0,500,len(rng)), index=rng) $ ts5min = ts.resample('20S') $ print(ts5min,'\n')  # Resampler object $ print(ts5min.groups,'\n') # dict $ print(ts5min.sum(), '\n') # series of sum() to each 1min bin $
priceData = [] $ for i in range(0, len(page)): $     data = pd.DataFrame.from_dict(page.Contracts[i],  orient='index').transpose() $     priceData.append(data) $ priceData = pd.concat(priceData, axis=0, ignore_index=True)
country.name.unique()
data3.groupby(['year']).sum().reset_index()
pd.scatter_matrix(df);
len(df2.query('group=="treatment"').query('converted==1'))/len(df2.query('group=="treatment"'))
reddit['title'].value_counts()/reddit.shape[0]
top_topics = (news_df['topic'] $               .value_counts() $               .sort_values(ascending = False) $               .head(10)) $ top_topics
BBC_df = news_sentiments.loc[news_sentiments["News Source"] == "BBC",:] $ CBC_df = news_sentiments.loc[news_sentiments["News Source"] == "CBC",:] $ FoxNews_df = news_sentiments.loc[news_sentiments["News Source"] == "FoxNews",:] $ CNN_df = news_sentiments.loc[news_sentiments["News Source"] == "CNN",:] $ Nytimes_df = news_sentiments.loc[news_sentiments["News Source"]=="nytimes",:]
input =  df_MC_most_Convs.MC_mostConvs.tolist() $ c = Counter(input) $ df_MC_most_Convs = pd.DataFrame(c.items()) $ df_MC_most_Convs.rename(columns={1:'number_months'}, inplace=True) $ df_MC_most_Convs
planevisits_df.count()
X = pd.merge(X, meals[['id','menu_id']], left_on='meal_id', right_on='id', how='inner') $ del X['id'] $ X = pd.merge(X, menu_course_counts[['menu_id', 'count_of_first_courses', 'count_of_second_courses', 'count_of_third_courses', 'count_of_appetizers', 'count_of_desserts', 'count_of_small_plates', 'count_of_entrees', 'count_of_beverages']], on='menu_id', how='inner') $ del X['menu_id']
meals.to_csv('./cleaned/meals.csv', header=False, index=False)
list_of_tables = pd.read_html("https://simple.wikipedia.org/wiki/List_of_U.S._state_capitals") $ states_df = list_of_tables[0] $ states_df.head()
import statsmodels.api as sm
pros_std_dev = utility_patents_subset_df.prosecution_period.std() $ pros_median = utility_patents_subset_df.prosecution_period.median() $ utility_patents_subset_df = utility_patents_subset_df[utility_patents_subset_df.prosecution_period <= (pros_median + 3*pros_std_dev)] $ sns.distplot(utility_patents_subset_df.prosecution_period, color="green") $ plt.show()
df_new[['old_page','new_page']]=pd.get_dummies(df_new['landing_page']) $ df_new=df_new.drop('old_page',axis=1)
r.groupby(level=0)['net'].sum().plot()
c_secondDigit3 = (df['c'].cast('string').substr(3, 1) == '3')
print('train: {} rows and {} columns'.format(*train.shape)) $ print('test: {} rows and {} columns'.format(*test.shape))
gen2str = '\n'.join(all_gen2_verse)
tweets_pp = pd.concat([multi.reset_index(), pp.reset_index()], axis=1) $ tweets_pp.head(2)
df_new["page_country"] = df_new["treatment"]*df_new["us"] $ df_new.head()
z_score, p_value = sm.stats.proportions_ztest(count=[convert_old, convert_new], nobs=[n_old, n_new], alternative='smaller' ) $ z_score, p_value
unique_users = len(df.user_id.unique()) $ unique_users
from keras.datasets import mnist $ (x_train, y_train), (x_test, y_test) = mnist.load_data()
print train.shape $ print weather.shape
df2['intercept'] = 1 $ df2[['control','treatment']] = pd.get_dummies(df['group']) $ df2['ab_page'] = df2['treatment'] $
rows_with_missing_response = loan_stats['loan_status'].isna() $ no_missing_values_response = rows_with_missing_response.logical_negation()
median_comments = reddit['num_comments'].median() $ median_comments
df = pd.read_excel('Financial_Sample.xlsx')
station_distance['Start Station Latitude'] = station_distance['Start Station Latitude'].astype(str) $ station_distance['Start Station Longitude'] = station_distance['Start Station Longitude'].astype(str) $ station_distance['End Station Latitude'] = station_distance['End Station Latitude'].astype(str) $ station_distance['End Station Longitude'] = station_distance['End Station Longitude'].astype(str) $ station_distance.info()
fx.head()
business_days = pd.to_datetime(business_days, format='%Y/%m/%d').date $ holidays = pd.to_datetime(holidays, format='%Y/%m/%d').date
AAPL.merge(GOOGL)
obs_diff_newpage = df2.query('landing_page == "new_page"')['user_id'].count() / df2.shape[0] $ obs_diff_newpage
train = df[df.index < '2017-01-01'] $ test = df[df.index > '2016-12-31']
new_p = df2['converted'].mean() $ new_p
observations_ext_node['number'][DATA].fillna(0, inplace=True) $ sets_node['date'][DATA].fillna(method='pad', inplace=True) $ observations = get_descendant_frame(sets_node)
date_price = b_cal_q1.groupby('date').mean()['price']
weather_df.groupby(["weather_main", "weather_description"]).sum()
df.groupby("pickup_dow")["booked_price"].mean()
lm = sm.OLS(df2['converted'], df2[['intercept', 'ab-page']]) $ results = lm.fit()
import sys $ sys.path.append('/content/models/research/') $ sys.path.append('/content/models/research/slim/')
current_img = current_img_link[0]['style'] $ print(current_img.find("('")) $ print(current_img.find("')"))
highRate = movies['Your Rating'] >= 4.0
class_merged_hol=pd.merge(class_merged,national_hol,on=['date'],how='left') $ print("Rows and columns:",class_merged_hol.shape) $ pd.DataFrame.head(class_merged_hol)
print(type(df.groupby("grade").size()))  # as series (series values is count of each category) $ df.groupby("grade").size()
returns.MSFT.cov(returns.IBM)
plt.hist(null_vals) $ plt.axvline(obs_diff, c='red');
tweets.head()
Senate = pd.read_csv('~/Desktop/Senate.csv', index_col='state')
def row_split(line, cols, index): $     label, *vector = line.split() $     index.append(label) $     for col_num, new_value in enumerate(vector): $         cols[col_num].append(new_value) $
sanders = miner.mine_user_tweets(user="berniesanders", max_pages=5) $ donald = miner.mine_user_tweets(user="realDonaldTrump", max_pages=5)
payload_scoring = {"fields": ["GENDER","AGE","MARITAL_STATUS","PROFESSION"],"values": [["M",23,"Single","Student"],["M",55,"Single","Executive"]]} $ response_scoring = requests.post(scoring_url, json=payload_scoring, headers=header) $ print response_scoring.text
lm=sm.Logit(df3['converted'],df3[['Intercept','ab_page','UK','US']]) $ results=lm.fit() $ results.summary2()
df_A3=pd.read_csv("classA.csv",index_col=0) $ df_A3.index $ df_A3
manual_uniprot_dict = {'Rv1755c': 'P9WIA9', 'Rv2321c': 'P71891', 'Rv0619': 'Q79FY3', 'Rv0618': 'Q79FY4', 'Rv2322c': 'P71890'} $ my_gempro.manual_uniprot_mapping(manual_uniprot_dict) $ my_gempro.df_uniprot_metadata.tail(4)
feat_imp_index = [f[1:] for f in feat_imp.index]
terrorism = text_file.filter(lambda t: is_interesting(t,bds)).take(10)
autos["price"].value_counts().head(10)
largest_change=(df['High']-df['Low']).max()
df2['converted'].value_counts()[1]/df2['converted'].count() 
df.loc[[101,103,105], ['name' , 'age']]
new_page_converted = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new])
h2o.cluster().shutdown()
f, ax = plt.subplots(figsize=(15, 5)) $ sns.countplot(x="complaint_type", data = samp311);
c = sites.unary_union.centroid # GeoPandas heavy lifting $ m = folium.Map(location=[c.y, c.x], tiles='CartoDB positron', zoom_start=8)
pd.DataFrame(data)
doc_duration = doctors.groupby(doctors.index.date)['Hours_Spent'].sum() $ RN_PA_duration = RN_PA.groupby(RN_PA.index.date)['Hours_Spent'].sum() $ therapist_duration = therapists.groupby(therapists.index.date)['Hours_Spent'].sum()
data['inday_icu_wkd'] = np.where(data.intime_icu.dt.weekday <= 4, $                                  'weekday','weekend') $ data['inday_icu_wkd'].value_counts()
autos[["date_crawled","date_created","last_seen"]].describe()
df_test = pd.read_json('data/raw_data.json') $ df_orig = pd.read_csv('data/cleaned_data.csv') # must read in dummy $ df_merge = pd.concat((df_test, df_orig)) $ df_merge_clean = clean_data(df_merge)
 df.sum(axis=1) #means across the column
dictionary=pd.read_csv("dictionary.csv") $ with open("IMDB_dftouse_dict.json", "r") as fd: $     IMDB = json.load(fd) $ IMDB_df = pd.DataFrame(IMDB)
plt.figure(figsize=(8, 5)) $ sbr.heatmap(train_df[['comments', 'favs', 'views', 'views_lognorm', 'comments_lognorm', 'favs_lognorm']].corr());
conn.fetch(table=dict(name='banklist', caslib='casuser'), $            sastypes=False, to=3)
df_B=pd.DataFrame({"Student_height":heights_B,"Student_weight":weights_B})
dates = pd.date_range('1950-01', '2013-03', freq='M'); dates
testObjDocs.outDF.tail()  ## note: all records are in here now but indices will be different from input DF
treatment = df2[df2['group'] == 'treatment'] $ treatment[treatment['converted'] == 1].shape[0] / treatment.shape[0]
df_group = df2["group"] $ prob_new_page = (df_group == "treatment").mean() $ prob_new_page
now = dt.datetime.now() $ print("type of now:", type(now)) $ now
import seaborn as sns $ %matplotlib inline $ from matplotlib import pyplot as plt $ plt.figure(figsize=(10,10)) $ sns.distplot(df_nd101[(df_nd101['ud120']>=0)&(df_nd101['ud120']<=1)].ud120)
mode = "overwrite" $ url = "jdbc:postgresql://localhost:5432/postgres" $ properties = {"user": "benjarman","password": "","driver": "org.postgresql.Driver"} $ for dat in data_sets: $     dat[1].write.jdbc(url=url, table=dat[0], mode=mode, properties=properties)
MultData.head()
def get_body_media(the_posts): $     list_of_body_media = [] $     for i in list_Media_ID: $         list_of_body_media.append("https://www.instagram.com/p/" + i) $     return list_of_body_media
df_DRGs.head()
train.describe()
ward_sample = tfidf_matrix[:5000].toarray() $ ward_distances = pairwise_distances(ward_sample, metric='cosine')
grouped_dpt = department_df.groupby('Department') $ grouped_dpt # groupby object 
fb.founded_on.year
df_all_repaid_latest=pd.concat(dfs,keys=reporting_dates) $ df_all_repaid_latest.columns=['irr'] $ df_all_repaid_latest.to_clipboard() $
Pipeline.head()
plt.plot(data['time_step'][0:800],data['valuea'][0:800])
n_cols = 3 $ layout = (n_cols, 1+ int( (1+states.shape[1])/n_cols)) $ states.columns = ['ofi', 'vol_mb', 'macd', 'mid_std', 'possession', 'cap_pct'] $ states.plot(subplots=True, figsize=(15,10), layout=layout);
pd.cut(df['CTYNAME'],bins= 5,include_lowest= True).sort_values() # series object
apple["20d"] = np.round(apple["close"].rolling(window = 20, center = False).mean(), 2) $ apple.index = apple['date'] $ import datetime as dt $ pandas_candlestick_ohlc(apple.loc[dt.date(2016,8,7):dt.date(2016,1,4),:], otherseries = "20d")
new_page = df2[((df2['landing_page'] == 'new_page') )].count()[0] $ all_page = df2['landing_page'].count() $ prob_new_page = round(new_page/ (all_page),2) $ print('the probability that an individual received the new page {}'.format(prob_new_page)) $
engine = create_engine('postgresql://mengeling:mengeling@localhost:5432/silvercar') $ df = pd.read_sql_table("reservations", engine)
np.all(x == 6)
new_df = pd.DataFrame({'content':df.iloc[:,dict_num_colnames['description']],    $                        'content2':df.iloc[:,dict_num_colnames['itunes_summary']], $                        'name': df.iloc[:,dict_num_colnames['name']], $                        'website_url': df.iloc[:,dict_num_colnames['website_url']] $                       })
ifcdf = pd.concat([ifc2df(row) for row in cachedf.itertuples()]) $ ifcdf
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv' $ response = requests.get(url) $ response
sample_df['entity_type'].unique()
access_logs_df = spark.createDataFrame(access_logs_parsed)
data = pd.Series([0.25,0.5,0.75,1]) $ data
news5 = ('The oil and gas industry of Texas continued to recover in April, with strong oil production growth last month, the Federal Reserve Bank of Dallas said in its Energy Indicators monthly release.' $ 'The Permian Basin continued to be the driver of the U.S. rig count growth. Rig counts in the Permian increased from 310 in March to 337 in April, while rig counts in the Eagle Ford rose from 80 in March to 89 in April, the Dallas Fed said.' $ 'In job figures for a month lagging production figures, total Texas oil and gas employment rose in March by 3,500 jobs to around 211,700 jobs, with oil and gas extraction employment up slightly to 92,500 jobs, and payrolls in support activities for mining rising to 119,200. March was the third consecutive month of increases in total Texas oil and gas employment, the Dallas Fed said.' $ 'Earlier this month, the Dallas Fed said in its Texas Economic Update that positive job growth and rising rig counts indicate an ongoing energy sector recovery.')
X = np.array(df.drop(['label'], 1)) $ y = np.array(df['label'])
testObj.buildOutDF(tst_lat_lon_df[-25:])  ## redo end of DF .. should be entirely blank since we're out of licenses $
grouped_months_Sl = Total_deaths_Sl.groupby(Total_deaths_Sl['date'].apply(lambda x: x.month)) $
july = summer.loc[datetime(2014,7,1) : datetime(2014,7,31)] $ july[['Mean TemperatureC', 'Precipitationmm']].plot(grid=True, figsize=(10,5))
full_globe_temp.index
hdf.reset_index().tail()
filteredTweets.groupby(['DateTime']).mean()['Has_IRMA']
target_pf['is_rebalance'] = target_pf['date'].map(lambda x: True if x in lst_date_to_set else False)
uber_14["day_of_month"].value_counts().head()
!cp "empty_db.sqlite" "db.sqlite" 
np.random.shuffle(whitelistedArticles) $ np.random.shuffle(whitelistedArticles)
r_lol = r.json()['dataset']['data']
test_article = 'Spanish-speaking audiences didnt turn out for Los Monlogos de la Vagina, forcing the quick closure of Eve Enslers play after 15 previews and 33 regular performances. Producers hoped that importing a translated Vagina Monologues that has been playing 14 years in Mexico City, and bringing in Spanish-speaking actors with followings from stage and television, would be a recipe for renewed New York success. (The play initially ran for 1,300 performances.)  They described the production as the first open commercial run of a Spanish-language play Off Broadway. But that came to a close with Mondays announcement that the Sunday night performance had been the shows last. Even with positive notices and enthusiastic audiences, the show is not attracting the numbers needed to make this economically viable,  the producers said in a statement. We hope to announce plans to bring the production to other U. S. cities.'
old_page_converted = np.random.choice([1,0], size=df_oldlen, p=[pold,(1-pold)]) $
lgb_model.fit(X_train, y_train)
contain_geo_search = collection_reference.find({'geo' : { '$ne' : None}}) # want docs where 'geo' key is 'not equal' to none $ contain_geo_search # Cursor object
df.to_pickle('/Users/emilytew/Documents/rwanda/Avocado_changed.pkl')
pprint.pprint(treaties.find_one({"reinsurer": "ACE"}))
year_prcp_df.set_index('date', inplace=True) $ year_prcp_df.head()
svc = SVC(random_state=20) $ param_grid = { 'C': [1, 0.5, 5, 10,100], 'decision_function_shape':['ovo', 'ovr'], 'kernel':['linear', 'rbf']} $ grid_svc = GridSearchCV(svc, param_grid=param_grid, cv=10, n_jobs=-1)
cpi_sdmx.lookup_code('housing',position=2)
offseason13 = ALL[(ALL.index > '2013-02-03 ') & (ALL.index < '2013-09-05')]
df2 = df2.drop(labels=1899) $ df2[df2['user_id']==773192]
df_new[['UK', 'US']] = pd.get_dummies(df_new['country'])[['UK','US']] $ df_new.head()
x_normalized.info()
df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')].shape[0] + \ $ df[(df['group'] != 'treatment') & (df['landing_page'] == 'new_page')].shape[0]
for v in d.variables: $     print(v)
tweet_clean.info()
smalldf = df.query("fileSizeMB < 100") $ print("{:.2f} GB".format(smalldf.fileSizeMB.sum() / 1024)) $ smalldf.shape $ for ref in df['ref']: $     print('https://www.kaggle.com/c/{}/rules'.format(ref))
df_breed = pd.read_csv('image-predictions.tsv', sep = '\t') $ df_breed.head(3)
train_labels = perf_train['Default'] $ perf_train, perf_test = perf_train.align(perf_test, join = 'inner', axis = 1) $ perf_train['Default'] = train_labels $ print('Training Features shape: ', perf_train.shape) $ print('Testing Features shape: ', perf_test.shape)
df_pr.sort_values('sentiment').iloc[:10]
print cbg.crs $ print g_geo.crs
ser5.loc[["a","b"]]
autos["registration_year"].value_counts().sort_index(ascending=True)
cursor.execute("SHOW TABLES") $ cursor.fetchall()
twitter_data_v2[~twitter_data_v2.tweet_id.isin(tweet_data_v2.tweet_id)]
prng = pd.period_range('2005', periods=7, freq='A') $ prng
model = gensim.models.Word2Vec(sentences, min_count=10)
tweet_json.describe()
import time $ filename = time.asctime() +'_res.csv' $ out_df.to_csv(filename, index=False)
plt.bar(x_axis1,y_axis1) $ plt.xticks(rotation=90) $ plt.show()
ts = df15.groupby('day_of_week').agg({'sale':['sum']}) $ ts.plot()
df_imputed_median_NOTCLEAN1A = df_NOTCLEAN1A.fillna(df_NOTCLEAN1A.median())
df.head()
prob_group2 = df2.query("group == 'treatment'")["converted"].mean() $ print("In the 'treatment' group the probability they converted is {}.".format(prob_group2))
print(store_items.dropna(axis=0)) $ print(store_items.dropna(axis=1)) $
pd.unique(df_users.creation_source.values)
activity_df = pd.DataFrame(joined_data, index=pd.date_range('20171229', periods=6), columns=['Walking','Cycling']); $ print(activity_df)
tweet_tokenizer.tokenize(tweet_hour.loc[1340,'tweet_text'])
print('{}index/agencies/{}'.format(base_url, '1')) $ print(json.loads(requests.get('{}index/agencies/{}'.format(base_url, '1')).text))
print('Full:') $ print('Polarity:', fullDF.Polarity.mean()) $ print('Subjectivity:', fullDF.Subjectivity.mean())
df.head() $ list1=list(df) $ df.head()
from PutIDFiles import put_team_ids, put_game_ids, put_all_matchups $ Teams = put_team_ids() $ Teams.head() $
pd.merge(staff_df, student_df, how='right', left_index=True, right_index=True)
fig, ax = plt.subplots(figsize=(16, 9)) $ df_t1.boxplot(column='time_hrs', by='Task', ax=ax) $ ax.set_xticklabels([]) $
pumashp.head()
df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]
plt.scatter(tbl['mr_rf'],tbl['pr_rf']) $ plt.plot(tbl['mr_rf'], result.fittedvalues,'g')
lsi_tfidf.show_topic(1)
test['Popular'] = (0.6*probsGBC+0.4*probsRF) $ test['UniqueID'] = test['UniqueID'].astype(int) $ test.to_csv('preds.csv', columns=['UniqueID', 'Popular'], header=['UniqueID', 'Probability1'], index=False)
top_supports.apply(combine_names, axis=1)
import statsmodels.api as sm $ convert_old = sum(df2.query("group == 'control'").converted) $ convert_new = sum(df2.query("group == 'treatment'").converted) $ n_old = df2.query("group == 'control'").shape[0] $ n_new = df2.query("group == 'treatment'").shape[0]
df3 = tier1_df.reset_index() $ df3 = df3.rename(columns={'Date':'ds', 'Incidents':'y'}) $ df_orig = df3['y'].to_frame() $ df_orig.index = df3['ds'] $ n = np.int(df_orig.count())
to_be_predicted_Day5 = 56.18025818 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
labeled.drop(['name_feat'], axis='columns', inplace=True)
tag_df = stories.tags.apply(pd.Series) $ tag_df.head()
x2 = poly2.fit_transform(x)
rm_indices = events['type'][events['type']== 'type'].index $ events = events.drop(events.index[[rm_indices]])
def get_neg_convo(groupID,df=companyNeg): $     convo = df[df.group_id == groupID ] $     print(convo.head()) $     return convo
learner.save_encoder('adam1_20_enc')
rf_v1.cross_validation_metrics_summary()
f_lr_hash_modeling2.show(3)
store_items.dropna(axis=1)
weekday=weekday.drop(columns=['day_of_the_week']) $ weekend=weekend.drop(columns=['day_of_the_week'])
domain = 'twitter.com'
pd.to_datetime(['14-01-2012', '01-14-2012'], dayfirst=True)
for i in top_tracks: $     if not top_tracks[i][0].split(",")[0].isdigit(): $         del top_tracks[i] $
country_dummies = pd.get_dummies(df_new['country']) $ df_new = df_new.join(country_dummies) $ df_new.head()
scr_churned_df.head()
order_with_cust = pd.merge(cust_age,order_item, how='inner', left_on=['CUSTOMER_ID'], right_on =['CUSTOMER_ID']) $ order_with_cust.head()
year_prcp = session.query(Measurement.date, Measurement.prcp).\ $     filter(Measurement.date > '2017-07-02').\ $     order_by(Measurement.date).all() $ year_prcp
print('reduce memory') $ utils.reduce_memory(transactions) $ transactions = transactions[transactions.msno == '/nPDKKeCsp3ifkeonrSdvdJrtSadkJRnwVTrV8RV/rk='] $ transactions['membership_expire_date']  = transactions.membership_expire_date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d')) $ transactions['transaction_date']  = transactions.transaction_date.apply(lambda x: datetime.strptime(x, '%Y-%m-%d')) $
fig, ax = plt.subplots(1, figsize=(10,3)) $ plot_linear_trend(ax, title='RN/PAs', series= RNPA_detrended) $ plt.title('RN/PAs data, linearly detrended') $ plt.tight_layout()
df.groupby('Week').Avg_speed.mean()
counts = Counter(word_list) $ pprint(counts.most_common(10))
df[df['Descriptor'] == 'Loud Music/Party'].index.weekday.value_counts()
s.str.upper()
rodelar = pb.User(commons_site, "Rodelar")
df2.shape, op_before.shape, op_after.shape,  op_a2.shape $
train_holiday_oil_store_transaction_item_test_004 = train_holiday_oil_store_transaction_item_test_004.drop('city', 'state', 'store_type', 'cluster') $
df.isnull().values.ravel().sum()
pm_data.dropna(inplace = True) $ pm_data.shape
aru_df = pd.read_csv(aru_file)
sns.distplot(dfz.favorite_count.apply(np.log), color = 'red', label = 'Favorites') $ sns.distplot(dfz.retweet_count.apply(np.log), color = 'blue', label = 'Retweets')
ST = sim_closes.tail(1) $ ind = np.digitize(ST.values[0],np.array([0, K, ST.values.max()+1])) $ freq = (np.array([sum(ind==1),sum(ind==2)])/nscen)*100
with open("Events_eps0.7_5days_500topics","rb") as fp: $     Events = pickle.load(fp)
iberia = df[df["text_3"].str.contains("iberia", case = False)] $ iberia.text_3.str.split(expand=True).stack().value_counts().head(10).reset_index()
rng[5].tz_localize('Asia/Shanghai')
(p_diffs > obs_diff).mean()
df = pd.read_csv("Z:00_ETL/CustomerBehaviour/rawdata2.txt", sep = "\t", encoding = "ISO-8859-1")
stock_dict = r.json()['dataset'] $ type(stock_dict)
df.columns = ['ID', 'name', 'category', 'Main category', 'currency', 'deadline', $               'goal', 'launched', 'pledged', 'State', 'Backers', 'country', $               'usd pledged', 'Pledged (USD)', 'Goal (USD)']
df_all.head()
df_lm.filter(regex='tch_view_assig|last_month').boxplot(by='last_month', figsize=(10,10),showfliers=False)
lostintranslation_imdb = response_imdb[0] $ ia.update(lostintranslation_imdb) $ print "Title:", lostintranslation_imdb["title"] $ print "Director:", lostintranslation_imdb["director"][0] $ print "Genre:", lostintranslation_imdb["genre"][0]
print(type(df.groupby("grade").count())) # as data frame ('id' column and 'raw_grade' column both contained) $ df.groupby("grade").count()
dates_range = pd.date_range(start=dates[0], freq='7d', periods=12)
test.plot()
df_arch_clean['rating_denominator'] = df_arch_clean['rating_denominator'].astype('float') $ df_arch_clean['rating_numerator'] = df_arch_clean['rating_numerator'].astype('float') $
zip_1_df.TimeCreate = zip_1_df.TimeCreate.apply(lambda x:x.date()) $ zip_2_df.TimeCreate = zip_2_df.TimeCreate.apply(lambda x:x.date())
x / 100.0 * 30
import re $ def getHours(x): $   return re.match('([0-9]+(?=h))', x) $ temp = flight.select("duration").rdd.map(lambda x:getHours(x[0])).toDF() $ temp.select("duration").show()
finals = pd.concat([finals, pd.get_dummies(finals['type'])], axis=1) $ finals.head()
shuffled = data.sample(frac=1) $
k.dtypes
html = browser.html $ mars_hemispheres = bs(html, 'html.parser')
import pyLDAvis.gensim
print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
tweet_scores = tweet_scores[['id', 'retweet_count','favorite_count']] $ tweet_scores.head()
lst_tickers = stocks_ts['ticker'].values
commits_per_hour = git_log.timestamp.dt.hour.value_counts(sort=False) $ commits_per_hour.head()
autos.loc[(autos.registration_year > 2016)|(autos.registration_year < 1900),'registration_year'] = np.nan
df_clean.source.head()
received_new_page = len(df2.query('landing_page == "new_page"')) $ received_new_page
shift_entries.query('TimeElapsed > 32 or TimeElapsed < 0')[ $     ['SE_ID', 'SE_LOCATION', 'SE_START', 'SE_TIMESTAMP','TimeElapsed', 'SE_SHIFTACTIVITY']]
country=pd.get_dummies(df3['country']) $ country.tail()
excel_data[['Postal Code']] = excel_data[['Postal Code']].astype(object) $ print("Data Types: \n%s" % (excel_data.dtypes))
sns.distplot(autodf.kilometer,rug=True)
trn_df.head(1)
brand_dict = {} $ for car in car_brands: $     brand_subset = autos[autos["brand"]==car] $     brand_dict[car] = brand_subset['price'].mean() $ brand_dict
engine = create_engine('postgres://%s@localhost/%s'%(username,dbname)) $ print engine.url
my_gempro.genes.get_by_id('Rv1295').protein.representative_structure $ my_gempro.genes.get_by_id('Rv1295').protein.representative_structure.get_dict()
temp_df.info()
input_edge_types_DF = pd.read_csv('network/source_input/edge_types.csv', sep = ' ') $ input_edge_types_DF
age.loc[age == 29]
X_train.shape
s4g.groupby(['Symbol', 'Year', 'Month'],as_index=False).agg(np.mean)[:5]
df5 = pd.DataFrame({'group': ['Accounting', 'Accounting', $                               'Engineering', 'Engineering', 'HR', 'HR', 'Librarian'], $                     'skills': ['math', 'spreadsheets', 'coding', 'linux', $                                'spreadsheets', 'organization', 'nunchucks']}) $ df5
rng.asi8[0]
y_series = pd.Series(y) $ y_series.head()
interests = [] $ for i in vlc.topics: $     for j in i: $         interests.append(j)
fish.weight $
uber = pd.read_csv('https://assets.datacamp.com/production/course_2023/datasets/nyc_uber_2014.csv') $ print(uber.shape) $ print(uber.head())
regr_M7 = linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)
print(r.json()['dataset']['column_names'])
full_data = full_data.replace(np.nan, 'null') $ one_hot = pd.get_dummies(full_data['gender']) $ full_data = full_data.drop('gender', axis =1) $ full_data = full_data.join(one_hot)
movie_mask = [(movie in BOM_movie_list) for movie in IMDB_df.movie_name]
old_page_converted = np.random.binomial(1, p_old,n_old)
conn.upload(df, casout=dict(name='iris_df', caslib='casuser'))
df2.head()
totalshooterswithcauses = causes.sum() $ result = 68 / totalshooterswithcauses * 100 $ print("The percentage of shooters that have a mental health problems is %f percent" % (result)) $
net_loans_exclude_US_outstanding_user.xs()
number_unpaids[number_unpaids['paid_status'] > 1].size / n_customers
df = df.loc[(df.created_at >= "2017-12-04") & (df.created_at <= "2018-06-30")]
max_fwing = df.loc[df.following.idxmax()] $ name = max_fwing['name'] if max_fwing['name'] is not None else max_fwing['login'] $
fig,ax=plt.subplots(1,2,figsize=(15,3)) $ ax[0].boxplot(joined['Promo2SinceYear'],vert=False) $ ax[1].boxplot(joined['Promo2Days'],vert=False)
def plot_contingency(df, feature_1, feature_2): $     ct = pd.crosstab(df[feature_1], df[feature_2]) $     ct.plot.barh(stacked=True) $     print("p-value for independence to be true : ", chi2_contingency(ct)[1])
hpd['Complaint Type'].value_counts().head(5)
all_pre = lift.get_all_pre()
pd.DataFrame({'like_plaintiff': like_plaintiff.sample(100)}).join(pd.DataFrame({'like_defendant':like_defendant})).join(geocoded_df[['Plaintiff.Name','Defendant.Name','Judgment.In.Favor.Of']])
predict, model = runXGB(train_X, train_y, train_X, num_rounds=400) $ out_train = pd.DataFrame(predict)
df[df['price'] > 1e6].head()
import matplotlib.pyplot as plt
hashtags = read_csv("hashtags.csv") $ hashtags.head()
mod=sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'CA','US']]) $ results=mod.fit() $ results.summary()
from sklearn.metrics import roc_auc_score $ print(roc_auc_score(y_test, y_pred))
device = train_data.groupby(["device.browser","device.operatingSystem","device.isMobile"]).agg({'totals.transactionRevenue': 'sum'}) $ device.sort_values(by = ["device.browser","totals.transactionRevenue"],ascending=False)
np.exp(0.0408) #US $
pd.date_range('2015-07-03', periods=8)
dot.attr('edge', fontcolor='darkred')
with open('faved_tweets.df', 'wb') as handle: $     pickle.dump(df, handle)
conditions.unique()
model._model_json['output']
ABT_tip.head()
autos = autos[autos["price"].between(200,250000)]
P = pd_centers(featuresUsed=select5features, centers=model.cluster_centers_) $ P
order.shape
vc.plot(kind='bar', x='day', y='count', figsize=(20,5), fontsize=25)
df.iloc[0]['date_added']
df_enhanced['rating_10_scaled'] = df_enhanced['rating_10_scaled'].astype('float')
data.datetime.min(), data.datetime.max()
archive_df.head(5)
df2_clean = df2_clean[~(df2_clean.jpg_url.duplicated())]
discasto = pb.User(pb.Site('es', 'wikipedia'), 'Discasto') $ discasto.isBlocked()
series_of_converted_ages = age_converter(test_clean["Age no Nans"]) $ list_of_test_features.append(("Age", series_of_converted_ages))
class MyOpener(FancyURLopener): $     version = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/601.2.7 (KHTML, like Gecko) Version/9.0.1 Safari/601.2.7' $ MyOpener.version
segments.st_time.dt.month.head()
releases.iloc[0]
df.resample('M').mean()
archive[archive['rating_denominator'] != 10].sample(5)
auto.tail(15)
labeled_features = final_feat.merge(failures, on=['datetime', 'machineID'], how='left') $ labeled_features = labeled_features.fillna(method='bfill', limit=7) # fill backward up to 24h $ labeled_features = labeled_features.fillna('none') $ labeled_features.head()
%load -s regression_contour teaching_plots.py
new_page_users2 = float(df2.query('landing_page == "new_page"')['user_id'].nunique()) $ Newpage_p2 = new_page_users2/float(df2.shape[0]) $ print("The probability that an individual received the new page is {0:.2%}".format(Newpage_p2)) $
date_retweets.created_at.value_counts().plot() $ plt.xticks(rotation=60) $
df_episodes[~df_episodes.id.isin(df.episode_id)]
df = pd.read_pickle(pretrain_data_dir+'/pretrain_data_03.pkl') $ df = df.sort_values(by=['seq_id','work_day'],axis=0,ascending=[True, False])
cur = con.cursor() $ data_ls = cur.fetchall()
df2.drop([1899], inplace = True) $ df2.duplicated(['user_id']).sum()
backup = clean_rates.copy() $ rating_numbers = clean_rates.text.str.extract(r'(?P<numerator>\d+(\.\d+)?)/(?P<denominator>\d+(\.\d+)?)', expand=True) $ clean_rates['rating_numerator'] = rating_numbers['numerator'] $ clean_rates['rating_denominator'] = rating_numbers['denominator']
header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} $ response_get = requests.get(endpoint_published_models, headers=header) $ print(response_get) $ print(response_get.text)
doc_duration.index
people.iloc[2]
red_4['age'] = red_4['age'].astype('timedelta64[h]') $ red_4.head()
from sklearn.model_selection import train_test_split #need to conduct a Train/Test split before creating features $ X_train, X_test, y_train, y_test = train_test_split(pd.get_dummies(reddit['Subreddits']), reddit['Above_Below_Median'], test_size=0.3, random_state=42)
theft = crimes[crimes['PRIMARY_DESCRIPTION']=='THEFT'] $ theft.head()
ls = ['license', 'renew', 'apply', 'permit', 'business', 'certificate','form','check'] $ license_stop = eng_stop $ for w in ls: $     license_stop.append(w)
df['price_doc'].hist(bins=100)
temp = pd.read_csv(mappingfile).sort_values('ELEV') $ temp.head(10)
df.to_csv(LM_PATH/'df.csv', index=False)
digits.data.shape
mentions_df["lang"].value_counts()
Fraud_Data['purchase_value'].nunique()
a[2, :] = np.array([[0,0,0]]) $ a
df.loc[['2018-05-21','2018-05-25'],['Open','Volume']]
MergeMonth = MergeMonth.groupby(['Date'])[['Contract Value', 'Pipeline Contract Value', 'Pipeline Weighted Value']].sum().reset_index()
predicted_talks_vector = classifier.predict( ... )
df['Long'] = df['2017-09-07 23:00:02'].apply(split1) $ df['Lat'] = df['2017-09-07 23:00:02'].apply(split2) $ df['Timestamp'] = '2017-09-07 23:00:02' $ del df['2017-09-07 23:00:02']
got_data.head()
autos = autos[autos["price"].between(350,155000)]
log_mod = sm.Logit(df3.converted,df3[['intercept','ab_page','country_UK','country_US']]) $ result_3 = log_mod.fit() $ result_3.summary()
new_page_converted = np.random.binomial(1, p_new, n_new) $ print('binomial', new_page_converted.mean()) $ new_page_converted = np.random.choice([1, 0], n_new, p=(p_new,1-p_new)) $ print('random choice', new_page_converted.mean())
df.ndim
rdf_clf.fit(X_final[columns], y_final)
s.values
hc.sql('select * from tweet_table').toPandas()
compound_sub2 = compound_wdate_df2.append(compound_wdate_df3)
cpi_sdmx.names['Dimension']
df.columns
transactions[~transactions.UserID.isin(users.UserID)]
pol_tweets.head()
odds = vis["Trophies"].plot(kind = "bar", figsize = (25, 5), title = "Number of trophies won in the world cup", rot = 90, legend = True) $ odds.set_ylabel("Trophies", fontsize = 15) $ plt.show()
driver_sum = merged_df_nodup.groupby("type").sum().driver_count $ plt.pie(driver_sum, explode = explode, labels = labels, colors = colors, autopct = "%.1f%%", shadow = True, startangle= 150) $ plt.title("% of Total Drivers by City Type") $ plt.savefig("totaldrivers.png")
sql_createdb = 'create database HelloDB3;' $ conn_helloDB.execute(sql_createdb)
from sklearn.linear_model import ElasticNet, LinearRegression as lr $ from sklearn.ensemble import GradientBoostingRegressor as gbr, RandomForestRegressor as rfr $ from xgboost import XGBRegressor
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new], alternative='smaller') $ z_score, p_value
ax = users.created_at.hist(bins=12) $ ax.set_xlabel('Date') $ ax.set_ylabel('# Users') $ ax.set_title("Users' account creation per year")
print(len(train_df['channel'].unique()))
Merge = Merge.rename(columns= {'Contract Value (Daily)_x': 'Contract Value', $                         'Contract Value (Daily)_y': 'Pipeline Contract Value', $                         'Weighted Value': 'Pipeline Weighted Value' $                          }) $
chinadata.shape
!echo "foo foo quux labs foo bar quux" > p32_input.txt $ !hdfs dfs -put p32_input.txt {HDFS_DIR}/p32_input.txt
df_with_metac_with_onc = pd.concat([df_with_metac1, df_oncstage_dummies], axis=1)
diffs = np.array(p_diffs) $ p_val=(diffs > obs_diff).mean() $ print('The p-value for the differences of conversion rates is {}.'.format(round(p_val,4)))
feature_names = [col for col in training_df if col.startswith('f_')] $ le = LabelEncoder() $ train_y = le.fit_transform(training_df.result) $ train_x = training_df[feature_names]
session.query(func.count(Stations.station)).all() $
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     f.undo(mark='after b')
w.get_available_indicators(subset= 'A', step=2)
temp = train.groupby("hour")["any_spot"].mean() $ train['hour_more_parking'] = train['hour'].isin(temp[temp>0.4].keys()) $ val['hour_more_parking'] = val['hour'].isin(temp[temp>0.4].keys())
df2.groupby(df2['group']=='treatment')['converted'].mean()[1]
print("Check the number of records") $ print("Number of records: ", train.shape[0], "\n") $ print("Null analysis") $ empty_sample = train[train.isnull().any(axis=1)] $ print("Number of records contain 1+ null: ", empty_sample.shape[0], "\n")
selectfromcols = X_train.columns[gs_from_model_under.best_estimator_.named_steps['frommodel'].get_support()] $ selectfrom_coef = gs_from_model_under.best_estimator_.named_steps['logreg'].coef_ $ pd.DataFrame(selectfrom_coef, columns=selectfromcols).T.sort_values(0, ascending=False).head(10)
scoredf = printsummarydf(score1,score2,scoreagg,comments,istweet=False).sort_values('General Score',ascending=False) $ scoredf
popCon = pd.DataFrame(likes.groupby(by=['friend','content']).size()) $ popCon.columns = ['counts'] $ popCon = popCon.reset_index('content') $ popCon.sort_values(by='counts', ascending=False).head(10)
y_pred = pipeline.predict(X_test)
ncfile = netCDF4.Dataset('../OISST/new.nc','w',format='NETCDF4') $ print(ncfile)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_secret) $ api = tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
aapl.plot() $ plt.yscale('log')  # logarithmic scale on vertical axis $ plt.show()
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller') $ z_score, p_value
six_months_ago = pd.Timestamp('now') - pd.DateOffset(months=6) $ six_months_ago
df.isnull().any(axis=1).sum()
plot_data(df[~df.index.isin(df.query('state == "YY" and amount > 5000').index)])
new_converted_simulation = np.random.binomial(n_new, p_new, 10000)/n_new $ new_converted_simulation.mean()
c.find_one({'name.last': 'Bowie'})
model2 = sm.Logit(df_new['converted'], df_new[['intercept', 'ab_page', 'UK', 'US']]) $ results2 = model2.fit()
test_data.loc[(test_data.gearbox == 'automatik') & (test_data.vehicleType.isnull()), 'vehicleType'] = 'limousine'
multiple_party_votes_all.shape
np.unique(val_small_sample.click_timeDay), np.unique(val_small_sample.click_timeHour)
files8['Tenure'] = files8['Tenure'].fillna(0)
BOM_movie_list = movie_df.title.values.tolist()
new_page_prob = (df2['landing_page'] == "new_page").mean() $ print('The probability that an individual received the new page is {}.'.format(new_page_prob))
from keras.models import Model, Sequential $ from keras.layers import Dense, Activation $ import keras.backend as K
lr = LogisticRegressionCV(n_jobs=3) $ lr.fit(X_train[['avg_shifted_against', 'def_shift_pct']], y_train)
pumashplc["linkNYCp100p"].describe()
s4.value_counts()
BMonthEnd().rollforward(datetime(2014,9,15))
d - pd.tseries.offsets.Week(normalize=True)
print(type(texas_city.index)) $ print(type(addicks.index)) $
short_window = 40 $ long_window = 100 $ signals = pd.DataFrame(index=aapl.index) $ signals['signal'] = 0.0
autos.unrepaired_damage.unique()     
(training_data, testing_data) = assembled.randomSplit([0.8, 0.2], seed = 13234)
cpi_all.loc[:,'Time'] = cpi_all.loc[:,'Time'].map(lambda i: i.split(':')[1]) $ cpi_all.loc[:,'Time'] = cpi_all.loc[:,'Time'].map(lambda t: pd.to_datetime(t,format='%b-%Y')) $ cpi_all.Time.cat.categories
df_ab_converted_portion = df_ab_raw.groupby(['converted']).count() $ df_ab_converted_portion.reset_index(inplace = True) $ df_ab_converted_portion['porportion'] = df_ab_converted_portion['user_id'] / df_ab_converted_portion['user_id'].sum()
counts = combined_df5['vo_propdescrip'].value_counts() $ repl = counts[counts <= threshold].index $ combined_df5['vo_propdescrip']=combined_df5['vo_propdescrip'].replace(repl, 'Other') $
mod_model = ModifyModel(run_config='config/run_config.yml', model='MESSAGE_GHD', scen='hospitals baseline', $                         xls_dir='scen2xls', $                         file_name='data.xlsx', verbose=False)
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) $ auth.set_access_token(access_token, access_secret) $ api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)
query_sakhalin_bbox = tuple(zip(['lonl', 'latl', 'lonu', 'latu'], map(str, sakhalin_shp.bbox))) $ print(query_sakhalin_bbox)
data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, num_workers=4) $ learn = ConvLearner.pretrained(arch, data, precompute=True)
print(df['Confidence'].unique())
p_null = np.mean(df2['converted']) $ p_null
ebay.index = pd.to_datetime(ebay.index) $ ebay.index
if not os.path.exists(os.path.join('data', 'energy.csv')): $     %run common/extract_data.py $ energy = load_data() $ energy.head()
Tradeday = namedtuple('Tradeday', 'date open high low close change traded_vol turn_over last_price d_trad_unit d_turnover') $ data = {d[0] : Tradeday(*d) for d in r.json()['dataset_data']['data']} $ data2 = defaultdict(list) $ for d in r.json()['dataset_data']['data']: $     data2[d[0]] = Tradeday(*d) $
test_df = df[df["dataset"]=="test"]; test_df.shape
from sklearn.linear_model import LogisticRegression $ log_reg = LogisticRegression(C = 0.0001) $ log_reg.fit(train, train_labels)
df = pandas.DataFrame([[user,daysPassed,dream]], columns=['user','time','dream']) $ print(df) $
x_normalized = intersections[for_normalized_columns].values.astype(float)
maxitems = 10 $ print "London tweets retrieve testing" $ print '----------------------------------' $ for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_L).items(maxitems): $     print tweet.text
df2_t = df2.query('group == "treatment"') $ df2_t.query('converted == 1').user_id.count()/df2_t.user_id.count()
df.iloc[0,0]   # returns element of first row and first column as per zero indexing
print('summa package summary:') $ print(summarizer.summarize(matt1str, words=200))
views_data_clean = pd.read_csv('data/views_data_clean.csv')
data.fillna(0)
sc.stop()
session.query(Measurement.station, func.count(Measurement.tobs)).filter(Measurement.tobs!=None).group_by(Measurement.station).order_by(func.count(Measurement.tobs).desc()).first()
bonus_points.interpolate(axis=1)
cluster = df.ix[:,:21]
df.groupby(['A','B']).sum()
usage_400hz_filter.registerTempTable('tmp_400hz_usage_final')
result = cur.fetchall() $
%%capture $ casActionsets = ['cardinality','dataPreprocess','varReduce','clustering','pca','sampling','decisionTree','dataStep','neuralNet','svm','astore','fedsql','percentile'] $ [cassession.loadactionset(actionset=i) for i in casActionsets] $ cassession.actionsetinfo(all=False)
flight_delays = flight_delays.reindex(taxi_hourly_df.index) $ flight_delays.fillna(0, inplace=True)
df.loc[sq_index,'life_sq'] = np.NaN
df2 = df2.drop(index=[1899], axis = 0)
%%time $ df['closed_at'] = pd.to_datetime(df['Closed Date'], format="%m/%d/%Y %X %p")
test_embedding=graf_test['DETAILS3'].apply(lambda words: np.mean([model[w] for w in words if w in model], axis=0)) $
for col in train.columns: $     print(col,':',train[col].dtype)
users[users['LastAccessDate'] < (pd.Timestamp.today() - pd.DateOffset(years=1))].shape
normalize_links("[{'id': 1, 'title': 'Show Me Shiny', 'fallback': 'Show Me Shiny', 'title_link': 'http://www.showmeshiny.com/', 'text': 'Gallery of R Web Apps', 'from_url': 'http://www.showmeshiny.com/'}]")
content_size_summary_df = logs_df.describe(['content_size']) $ content_size_summary_df.show()
complaints2016_geodf.crs
cityID = '9a974dfc8efb32a0' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Kansas_City.append(tweet) 
plt.scatter(reddit['Comments'], reddit['Upvotes']) $ plt.xlabel('Upvotes',fontsize='large') $ plt.ylabel('Comments',fontsize='large') $ plt.title('Upvotes and Comments have a positive relationship!', fontsize='large') $ plt.show()
for post in d.entries: $     print(post.title + ": " + post.link + "\n")
tweets.to_pickle('cleaned_tweets_data.pkl')
open_correction = (5 * 10) / (7 * 24) $ print(f"Opening hours are {open_correction:.2%} of total hours")
len(non_rle_pscs[non_rle_pscs.secret_base == True]) / len(non_rle_pscs)
high_ranked_answers_threshold = np.percentile(answers_scores, 90) $ high_ranked_answers_threshold
df_merged.hist(column='favorite_count',bins=100);
%%time $ ds_complete_temp_CTD_1988 = ds_complete_temp_CTD_1988.where((ds_complete_temp_CTD_1988['time.year'] > 1979),drop=True)
from scipy.stats import norm $ norm.ppf(1-(0.05/2))
moviesRdd.filter(lambda line: len(line.split(",")) != 3).take(10)
stock.iloc[915:].shape
lmdict.head()
people.index
lr.fit(X_train, y_train)
gc_cur = gc_conn.cursor() $ excelurl_textstream.seek(0) $ gc_cur.copy_from(excelurl_textstream, 'jpstat_excel_urls', null="", sep='\t') # null values become '' $ gc_conn.commit() $ logging.info('Goole Cloud SQL Excel URL database table updated.')
df['edition_type'].value_counts()
pd.Timestamp(d)
res.head(1)
plt.plot(data,color='k',linestyle='--',lw=1.25) $ plt.plot(purple_line, color='purple',lw=2.0,alpha=0.85) $ plt.plot(green_line,color='green',lw=1.75) $ plt.grid(True) $ plt.legend(['actual','pred','train'], loc='best') $
data = data.withColumnRenamed("Est Income", "EstIncome").withColumnRenamed("Car Owner","CarOwner") $ data.toPandas().head()
injuries_hour=pd.merge(df_weather,col_totals,left_on="date_time",right_on="date_time",how="left")
hired = data.loc[data['hired']==1].tasker_id.value_counts() $ hired[:5]
ser5.index
n_unique_users = len(df['user_id'].unique()) $ n_unique_users
year13 = driver.find_elements_by_class_name('yr-button')[12] $ year13.click()
yt.get_featured_channels(channel_id, key)
df2 = df2.drop_duplicates(subset='user_id', keep='first') $ df2[df2.duplicated(subset='user_id')==True]
with ZipFile('{0}.zip'.format(datapath / zipfile), 'r') as myzip: $     myzip.extractall(datapath)
5/9*(86.53-32)
soup.ul.children
lin_clf = LinearSVC(random_state=42) $ lin_clf.fit(X_train_scaled, y_train)
betas_argmax = np.zeros(shape=mcmc_iters) $ betas_argmax = beta_dist.argmax(1)
df = pd.DataFrame() $ for (name, sex) in a_list: $     get_all_tweets(name, sex) $     df = df.append(pd.read_csv('%s_tweets.csv' % name))
adaboost.fit(trainx,trainy)
df_master.info()
my_person.lastname
tweets_rt['full_text'] = tweets_rt['retweeted_status'].map(lambda value: value[0].decode('utf-8')) $ tweets_rt['created_at'] = tweets_rt['created_at'].str.decode('utf-8')
sorted_budget_cheapest = df.sort_values(by=['budget_adj'], ascending = True).head(200)
df_new['UK'] = pd.get_dummies(df_new['country'])['UK']
list(data.dropna(thresh=int(data.shape[0] * .9), axis=1).columns) #set threshold to drop NAs
for o in ['Before', 'After']: $     for p in columns: $         a = o+p $         df[a] = df[a].fillna(0).astype(int)
tweet_image_clean.columns
df3=df2.join(df2_curr)
len(df.user_id.unique()) $
df_batch6_top['Date'] = df_batch6_top.index
yc_new3.describe()
saved_model.meta.available_props()
dfjoined.sort_values(['count_complaints_day', 'count_type_day'], ascending = [0,0], inplace = True)
from google.colab import files $ files.upload()
dates = [pd.Timestamp('2014-03-04').date(), pd.Timestamp('2013-05-23').date(), pd.Timestamp('2014-01-15').date()] $ df_modified = df[~df["updated_at"].dt.date.isin(dates)]
sox.head()
response = urllib.request.urlopen("https://api.iextrading.com/1.0/stock/nati/quote") $ str_response = response.read().decode('utf-8') $ obj = json.loads(str_response) $ obj
df_countries = pd.read_csv('countries.csv') $ df_joined = df_countries.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df_joined.head()
image_clean.sample(5)
visual_df['team'] = visual_df.index
cityID = '1d9a5370a355ab0c' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Chicago.append(tweet) 
group = grouper.get_group(group_key)
r.json()['dataset_data']['column_names']
df_countries = pd.read_csv('countries.csv') $ df_countries.head()
from sklearn.model_selection import train_test_split $ labels = df[df.year == 2017]['label'] $ test_size= ... $ X_train, X_test, y_train, y_test = train_test_split(vectorized_text_labeled, labels, test_size=test_size, random_state=1)
TempObs = pd.DataFrame(Active_TempObs, columns=["Date", "Temperature Observations"]) $ TempObs.head()
prop = donors.loc[donors['Donor Zip'].isnull(), 'Donor Is Teacher'].value_counts(normalize=True) $ prop
len(machin[(machin.user_id == 1881755) & (machin.item_id == 1928479) & (machin.interaction_type == 5)])
dummy_var_df = ce.fit_transform(data['subreddit_other'].values.reshape(-1, 1))
data["Improvements_split"] = data["Improvements"].apply(splitcol)
gbm_model.plot()
applications['application_created_at'] = pd.to_datetime(applications.application_created_at) $ applications['submitted_at'] = pd.to_datetime(applications.submitted_at) $ applications['submitted'] = np.where(applications['submitted_at'].isnull(), 0, 1) $ applications.groupby('submitted').agg({'applicant_id': lambda x: len(set(x))}).reset_index()
fullDF.info()
df.describe()
text = ' '.join(most_retweeted.text.values.tolist()) $ for word in all_stopwords: $     text = text.lower().replace(' {} '.format(word),'') $ text = text.replace('?','').replace('.','').replace(',','').replace(':','').replace(';','')
x = np.linspace(0, 10, 100) $ df = pd.DataFrame({"y":np.sin(x), "z":np.cos(x)}, index=x) $ df.head()
ids[pd.notnull(ids)].iloc[0]
run txt2pdf.py -o"2018-06-14 2148 CLEVELAND CLINIC Sorted by Payments.pdf"  "2018-06-14 2148 CLEVELAND CLINIC Sorted by Payments.txt"
raw_full_df.bedrooms.value_counts()
odds = (pd.read_csv('data/weekly_epl_odds.csv') $            .replace({ $                 'Man Utd': 'Man United', $                 'C Palace': 'Crystal Palace'}))
assert trn_df.shape[0] == trn_y.shape[0] $ assert val_df.shape[0] == val_y.shape[0]
import brunel $ df = data.toPandas() $ %brunel data('df') bar x(CHURN) y(EstIncome) mean(EstIncome) color(LocalBilltype) stack tooltip(EstIncome) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 
extract_deduped_with_elms_v2.shape, extract_deduped_with_elms.shape
data_kb['SA'] = np.array([ analize_sentiment(tweet) for tweet in data_kb['Tweets'] ]) $ display(data_kb.head)
prec_group = prec_group.reindex(taxi_weather_df.index)
def clean_tweet(tweet): $     return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split()) $
try: $     my_df.to_excel("my_df.xlsx", sheet_name='People') $ except ImportError as e: $     print(e)
df[df['Agency Name'].isin(['Department of Transportation', 'DOT'])].count()
df_change_count.sort_values(by='Count', ascending=False).head(10)
len(df2.query('landing_page == "new_page"'))/len(df2.landing_page)
def join_df(left, right, left_on, right_on=None, suffix='_y'): $     if right_on is None: right_on = left_on $     return left.merge(right, how='left', left_on=left_on, right_on=right_on, $                       suffixes=("", suffix))
obs_diff = new_page_converted.mean() - old_page_converted.mean() $ obs_diff
df2_treatment = df2.query('group == "treatment"') $ treatment_prob = df2_treatment.converted.mean() $ print(treatment_prob)
y_pred = pipe_nb.predict(pulledTweets_df.emoji_enc_text) $ y_proba = pipe_nb.predict_proba(pulledTweets_df.emoji_enc_text) $ pulledTweets_df['sentiment_predicted_nb']=[classes[y_pred[i]] for i in range(len(y_pred))]
from pyspark.sql import SQLContext $ sqlContext = SQLContext(sc)
plt.figure() $ plt.plot(word_count)
system = system.supersize(2, 2, 2) $ print(system)
new_df.shape
tweets.isnull().sum()
df.isnull().sum()
clean_rates.cuteness.value_counts()
optimizer_col= optim.Adam(model.parameters(),lr=0.0005) $ lr_scheduler_col = lr_sched.myCosineAnnealingLR(optimizer_col,400,cycle_mult=1.5) $ train_col.train_model(num_epochs=3,optimizer=optimizer_col,scheduler=lr_scheduler_col)
df.dropna(axis='columns', how='all')
days_predicted = np.arange(conversion_data.shape[0]-20,conversion_data.shape[0]) $ days_predicted
old_page_converted = np.random.choice([0, 1], size=n_old, p=[(1-p_old), p_old]) $ (old_page_converted == 1).sum()
title_tokens.reset_index(inplace=True, drop=True)
tweet_df = tweet_df[tweet_df.place != 'Myanmar'] $ tweet_df.describe()
Quantile_95_disc_times_pay.head(5)
lr_stack.fit(stack_X_test,stack_y_test) $ lr_stack_pred = lr_stack.predict(stack_X_test)
attend_with = attend_with.drop(['[', ']'], axis=1) $ attend_with = attend_with.drop(attend_with.columns[0], axis=1)
reddit[['Titles', 'Subreddits']].duplicated().sum() 
autos['registration_year'].value_counts().sort_index(ascending=False).head(20)
new_texas_city.set_index("Measurement_date", inplace = True) $ texas_city = new_texas_city $ print(texas_city.head()) $ print(type(texas_city.index))
dftouse_four.head()
df = pd.read_pickle("dfSentences.p")
bikedataframe.dtypes
week1 = trip_data_q5[trip_data_q5.weeks == 1].avgSpeed.values $ week2 = trip_data_q5[trip_data_q5.weeks == 2].avgSpeed.values $ week3 = trip_data_q5[trip_data_q5.weeks == 3].avgSpeed.values $ week4 = trip_data_q5[trip_data_q5.weeks == 4].avgSpeed.values $ week5 = trip_data_q5[trip_data_q5.weeks == 5].avgSpeed.values
minute_return = bars.close / bars.open - 1 $ minute_return.describe()
calendar = USFederalHolidayCalendar() $ holidays = calendar.holidays(start=train.date.min(), end=train.date.max())
df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726 = test5result $ pickle.dump(df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM__NearTop_CurveF_20180726, open( "df_all_wells_wKNN_DEPTHtoDEPT_KNN1PredTopMcM_20180724.p", "wb" ) )
from sklearn.ensemble import GradientBoostingRegressor
bacteria_dict = {'Firmicutes': 632, 'Proteobacteria': 1638, 'Actinobacteria': 569, 'Bacteroidetes': 115} $ bact = pd.Series(bacteria_dict)
prec_group.precip_in.fillna(method='bfill', inplace=True)
nypd_df['create_date'] = nypd_df['create_date_time'].dt.date $ nypd_df['day_of_week'] = nypd_df.create_date.apply(lambda x: x.weekday()) $ nypd_df['weekend'] = 0 $ nypd_df['weekend'][(nypd_df['day_of_week'] == 4) | (nypd_df['day_of_week'] == 5)] = 1 $ nypd_df['hour_of_day'] = nypd_df.create_date_time.apply(lambda x: x.hour())
df[(df.company == True) & (df.other == True)]
avg_tweek_stops_window_crimes_ =  int(np.average(stops_two_week_window_['sum_window_crimes'])) $ avg_tweek_tweek_crimes_ = int(np.average(crime_tweek_tweek_window_['sum_crimes'])) $ avg_week_tweek_crimes_ = int(np.average(crime_week_tweek_window_['sum_crimes'])) $ avg_datily_tweek_crimes_ = int(np.average(crime_day_tweek_window_['sum_crimes']))
df_change_count['Count'].mean()
basic_plot_generator("mention_count", "Saving an Image Graph" ,DummyDataframe.index, DummyDataframe,saveImage=True, fileName="dummyGraph")
df.groupby('landing_page')['group'].value_counts()
databreach_2017.to_csv('databreach_2017.csv')
df['created_at'] = df['created_at'].map(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))
open_weekly = open_prices.groupby([open_prices.index.year, open_prices.index.week]).mean() $ open_weekly
p_diffs = [] $ for _ in range(10000): $     npcon = np.random.choice([0,1],size = n_new, p = [1-p_new,p_new]).mean() $     opcon = np.random.choice([0,1],size = n_new, p = [1-p_old,p_old]).mean() $     p_diffs.append(npcon - opcon)
display('x', 'y', 'pd.concat([x, y], ignore_index=True)')
retweets_100 = api.retweets(deep_learning_tweet_id, count=100) $ len(retweets_100)
df = pd.read_csv(cama_file)
pd.read_csv('super.json')
knn_yhat = knn.predict(test_X) $ print("KNN Jaccard index: %.2f" % jaccard_similarity_score(test_y, knn_yhat)) $ print("KNN F1-score: %.2f" % f1_score(test_y, knn_yhat, average='weighted') )
yc = T.set_subtensor(y_true[T.eq(y_true,0).nonzero()], 100)
first_result.find('strong').text[0:-1] + ', 2017'  
archive_copy = archive_df.copy()
yc_new2 = yc_new1[['Fare_Amt', 'tripDurationHours', 'Trip_Distance', $        'Tip_Amt', 'income_departure', 'income_dest']] $ yc_new2.head()
merged2 = merged2.fillna(0)
countries_path = '../../Data/age_gender_bkts.csv' $ countries = pd.read_csv(countries_path) $ print(len(countries)) $ countries[countries['country_destination']=='CA']
oppose = merged[merged.committee_position == "OPPOSE"]
trans_data.describe()
df_total.to_csv("en-wikipedia_traffic_200801-201709.csv", sep='\t', index=False)
props.head()
to_be_predicted_Day5 = 38.49420904 $ predicted_new = ridge.predict(to_be_predicted_Day5) $ predicted_new 
fb.head()
dataset = os.path.join('..\\','Data', 'ml-20m') $ ml_20m_path = os.path.join(dataset, 'ml-20m.zip') $ ml_1m_path = os.path.join(dataset, 'ml-latest-small.zip')
df_enhanced = df_enhanced.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis=1)
stocks = sp_companies[0] $ stocks.head()
1-0.090999999999999998
appointments['AppointmentCreated'] = pd.to_datetime(appointments['AppointmentCreated'], errors='coerce') $ appointments['AppointmentDate'] = pd.to_datetime(appointments['AppointmentDate'], errors='coerce')
logit_mod_joined_result = logit_mod_joined.fit() $ logit_mod_joined_result.summary()
raw.name.values[0], raw.name.values[10]
df_concat["date_series"] = pd.to_datetime(df_concat["created_time"]) $ df_concat["date_series"].head()
transactions.head()
df = df.join(cluster.cluster_labels)
autos['ad_created'].str[:10].describe()
df2 = df2.drop(146678) $ df2 = df2.reset_index(drop=True) $ df2.iloc[146678]
DataSet = ds[ds.userTimezone.notnull()] $ len(DataSet)
filter_df.shape[0] - all_df.shape[0], ((filter_df.shape[0] - all_df.shape[0]) / all_df.shape[0]) * 100
%%time $ reps = [] $ doc2vec_model.random = np.random.RandomState(DOC2VEC_SEED) $ for doc in doc_contents: $     reps.append((doc[0], doc2vec_model.infer_vector(doc[1])))
pd.DataFrame({'count':user_summary_df[user_summary_df.gender == 'F'].verified.value_counts(), 'percentage':user_summary_df[user_summary_df.gender == 'F'].verified.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})
(new_page_converted.mean() - old_page_converted.mean())
unique = df['user_id'].unique() $ len(unique)
test_scores = run(q_agent_new, env, num_episodes=100, mode='test') $ print("[TEST] Completed {} episodes with avg. score = {}".format(len(test_scores), np.mean(test_scores))) $ _ = plot_scores(test_scores)
type(df_vow['Date'].loc[0])
tmp_df = tmp_ratings.pivot(index='userId', columns='movieId', values='rating')
autos['odometer'] = autos['odometer'].str.replace("km","") $ autos['odometer'] = autos['odometer'].str.replace(",","") $ autos['odometer'] = autos['odometer'].astype(float) $ autos.rename({'odometer':'odometer_km'}, axis = 1,inplace = True)
week37 = week36.rename(columns={259:'259'}) $ stocks = stocks.rename(columns={'Week 36':'Week 37','252':'259'}) $ week37 = pd.merge(stocks,week37,on=['259','Tickers']) $ week37.drop_duplicates(subset='Link',inplace=True)
xgb_learner.best_model.best_iteration
result1 = df2.T[0] + df3.iloc[1] $ result2 = pd.eval('df2.T[0] + df3.iloc[1]') $ np.allclose(result1, result2)
df.filter(items=["MES", "CODIGOCLIENTE"], axis=1)
ser6.mean()
jobPostDFSample = jobPostDFSample.reset_index()
days_range = pd.date_range(start=min_date, end=max_date, freq='D') $ idx_days = [str(s)[:10] for s in days_range]
sox['date'] = sox.START_DATE.str.slice(0,10) $ sox.rename(columns={'OPPONENT':'opponent'}, inplace=True) $ sox['playoff'] = 0 $ sox['late'] = 0 $ sox['day_of_week'] = pd.DatetimeIndex(sox.date).weekday
user_retention[['2016-12', '2017-01', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07' ]].plot(figsize = (10, 5)) $ plt.title('Cohorts: User Retention') $ plt.xticks(np.arange(1,12.1, 1)) $ plt.xlim(1,12) $ plt.ylabel('% of Cohort Purchasing');
df_test.shape
autos.describe(include='all')
tweetsIn22Mar.head() $ tweetsIn1Apr.head() $ tweetsIn2Apr.head()
finals['type'] = "normal" $ finals.loc[(finals["pts_l"] == 0) & (finals["ast_l"] == 1) & (finals["blk_l"] == 0) & $        (finals["reb_l"] == 0) & (finals["stl_l"] == 0), 'type'] = 'facilitator'
fin_p.dropna(inplace=True)
model = pipeline.fit(train)
data_sets[0] = ("users", users.withColumn("active", expr('cast(active as int)')))
help(soup.find_all)
c = news.comments.fillna(0) $ plt.figure(figsize=(16, 5)); $ c.hist(bins=len(np.unique(c))); $ plt.title('Distributon of the number of comments per news article');
NoOfConverted = df.query('converted == 1')['user_id'].count() $ ProportionConverted = (NoOfConverted/NoOfRows)*100 $ ProportionConverted
rating_and_retweet['score'].corr(rating_and_retweet['favorite_count'])
ipAddress_to_country.info()
df.head()
df_geo = pd.DataFrame(sub_data["id_str"]).reset_index(drop=True) $ df_geo["geo_code"] = geo_code $ df_geo.head()
df = panel_data.to_frame() $ df.head()
data.iloc[[1, 2], [3, 0, 1]]
def lemmatize(text): $     text = ' '.join(lemma.lemmatize(word) for word in text.split()) $     return(text)
!wget -O FuelConsumption.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv
first_date_pageviews = pageviews['timestamp'].min() $ last_date_pageviews = pageviews['timestamp'].max()
df2[['control', 'treatment']] = pd.get_dummies(df['group']) $ df2['intercept'] = 1
tweets_rt['retweeted'].value_counts()
df_geo_unique.shape
print(len(df_DRGs.drg_description.unique())) $ print('So all DRGs in df_DRGs are unique')
from sklearn.linear_model import Ridge $ ridge = Ridge(alpha=32000) $ ridge.fit(X_train_std, y_train) $ print("MSE: %.2f" % np.mean((ridge.predict(X_test_std) - y_test) ** 2)) $ print("R Square:",ridge.score(X_test_std, y_test))
results.reindex(results.index[::-1]).head()
hp.dtypes
pd.read_sql_query("SELECT * from person;", conn, index_col="id")
df_big.info()
tags = df.loc[:, 'tags --> ...':].apply(lambda x: x.dropna().tolist(), axis=1) $ df = df.drop(df.columns.to_series()['tags --> ...':], axis=1) $ tags = tags.apply(lambda x: list(map(lambda y: '_'.join(y.lower().strip().split()), x))) $ df['tags'] = tags
rootDistExp = Plotting(S.setting_path.filepath+S.para_trial.value)
df_raw[df_raw.song_title.isnull()] # look at rows without title
prop = props[props.prop_name == "PROPOSITION 064- MARIJUANA LEGALIZATION. INITIATIVE STATUTE."]
output= "SELECT date, count(*) as srt from tweet_details group by date order by srt desc limit 10 " $ cursor.execute(output) $ pd.DataFrame(cursor.fetchall(), columns=['Tweet_id','Count of Tweets'])
df2['intercept'] = 1 $ df2[['control', 'ab_page']] = pd.get_dummies(df2['group']) $ df2.drop(['control'], axis = 1, inplace = True) $ df2.head(5)
import math $ import numpy as np $ train_df['Log_price']=np.log(train_df['price'].astype(float))
df4.info()
twitter_df_clean = twitter_df_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis=1)
days_traded = len(r) $ n_years = days_traded / 365 $ cagr = (end_value / start_value) ** (1 / n_years) $ print("Compound annual growth rate (CAGR) = {:5.4}%".format((cagr - 1) * 100))
df = df[pd.notnull(df['Consumer complaint narrative'])] $ df.head()[['Product', 'Consumer complaint narrative']]
resp_dict = r.json() $ type(resp_dict)
df_clean.info()
about.find('a')[0].html
station_count = session.query(func.distinct(Measurement.station)).count() $ station_count $
df_customers.head() $
unique_cols=pd.unique(["Country"]+df.columns.values.tolist()) $ unique_cols
google_stock.tail() $ google_stock.tail(10)
closemeans = cc.groupby(['name'])['close'].mean()
pd.DataFrame({'count':user_summary_df.verified.value_counts(), 'percentage':user_summary_df.verified.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})
(grades > 5).all(axis = 1)
s = pd.Series(['Tom', ' William Rick ', ' John', 'Alber@t ', np.nan, '1234','SteveSmith']) $ s
df['Date'] = pd.to_datetime(df['Date']) $ df.set_index('Date', inplace=True) $
df.drop(['id','date','query_string','user'],axis=1,inplace=True)
PRE_PATH = PATH/'models'/'wt103' $ PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'
weather.head()
prcp_analysis_df.plot(figsize = (18,8), color='blue', rot = 340 ) $ plt.show() $
tw_clean['rating_numerator'].plot.hist(bins=3000, xlim=(0,15), title='Rating Numerator Histogram');
new_page_converted = np.random.choice([0,1],n_new, p=(1-p_new, p_new))
sns.swarmplot(x="embark_town", y="age", hue="sex", data=titanic)
pandas_profiling.ProfileReport(r6s)
rf = RandomForestClassifier(n_estimators=50, max_depth=50, n_jobs=4) $ rf.fit(X = clim_train, y = size_train)
df.lead_mgr.unique()[:5]
twitter_df_merged.head(5)
bigdf.dtypes
plt.rcParams["figure.figsize"] = [20,30] $ country_size.plot.barh().invert_yaxis()
IP = '137.110.137.158'
print df_t_best['Shipped At'].apply(lambda x: x.date()).min() $ print df_t_best['Shipped At'].apply(lambda x: x.date()).max()
ben_fin['FirstMeta'] = ben_final.groupby('userid')['pagetitle'].first().str.contains('/')
n_old = df2[df2['landing_page'] == "old_page"]['converted'].count() $ n_old
ts = pd.Series(['a', 'b', 'c', 'd', 'e'], index=pd.date_range(start='3/1/18', periods=5))
CumSums = Lags.groupby(level=1).cumsum() $ CumSums.columns = ['Cum'+c[:-4] for c in CumSums.columns] $ CumSums.head()
df.loc[df['seller'] != 'privat'].shape
df2 = df2.append(df3)
plt.hist(p_diffs) $ plt.title("sample distribution for the conversion rate") $
date_ny['date'] = pd.to_datetime(date_ny.year*10000+date_ny.month*100+date_ny.day,format='%Y%m%d') $ date_ny['shannon'] = date_ny['count']
sum(df2.converted == 1)/df2.shape[0]
X = digits.drop('5', axis = 1).values $ X
n_old = (df2.query('group == "control"')['converted'] >= 0).sum() $ n_old
potential_accounts_buildings_info_tbrr_temp = potential_accounts_buildings_info_tbrr[potential_accounts_buildings_info_tbrr['# Buildings not on Net'] > 0] $ potential_accounts_buildings_info_tbrr_temp.sort_values(by='# Buildings on Net', ascending=False, inplace=True)
sns.set(color_codes=True) $ for column in ['bottles_sold','volume_sold_lt']: $     plt.figure(figsize=(4,4)) $     sns.regplot(x=column, y='sale_dollars', data=df_2015)
df4 = pd.DataFrame(q4_results,columns=['tobs']) $ df4.columns $
train_history=model.fit(Xcnn, y_label_train_OneHot, $                         validation_split=0.2, $                         epochs=8, batch_size=20, verbose=1)       
def join_df(left, right, left_on, right_on=None, suffix="_y"): $   if right_on is None: right_on=left_on $   return left.merge(right, left_on=left_on, right_on=right_on, suffixes=("",suffix))
c.execute('SELECT city FROM weather where warm_month = "July" and cold_month != "January" ') $ print(c.fetchall())
visits = visits.sort_values(["address", "dba_name", "inspection_date"])
kickstarter['main_category'].unique()
train.head()
df2[df2.duplicated("user_id")]
posts = pd.read_csv('data/posts.csv', index_col = 0, parse_dates = [5, 6, 7]) $ posts.columns
import mglearn $ pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o', $                            hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)
techmeme.columns = ['date', 'original_source', 'original_title', 'extra_sources', $        'extra_titles', 'date_time', 'news_sources', 'titles', 'news_text']
df2["intercept"] = 1 $ df2[["not_ab", "ab_page"]] = pd.get_dummies(df2["group"]) $ df2 = df2.drop(["not_ab"], axis=1) $ df2.head(1) $
df_parsed = pd.read_json(df.to_json(orient="records"))
inactive_org_id = clean_users[clean_users['active']==0][['org_id','active']].groupby('org_id').count() $ inactive_org_id.columns = ['inactive']
doc_top_mat = pd.DataFrame(doc_top) $ doc_top_mat['TopicNum'] = doc_top_mat.apply(np.argmax, axis=1)
n_new = len(df2.query("group == 'treatment'")); $ n_new
y = df['target'] $ X = df['subreddits'] $ cvec = CountVectorizer(stop_words = 'english') $ X  = pd.DataFrame(cvec.fit_transform(X).todense(), $              columns=cvec.get_feature_names())
retweets.columns
resultsfff = session.query(Measurement.date, func.avg(Measurement.tobs), func.max(Measurement.tobs), func.min(Measurement.tobs)).\ $         filter(Measurement.date == '2017-08-07').all() $ resultsfff        
tweets_df = pd.read_csv(tweets_filename, $                         converters={'tweet_place': extract_country_code, $                                     'tweet_source': extract_tweet_source})
preg.head()
frame.sort_index(axis='columns')
train=pd.read_csv('../input/train.csv', sep='\t') $ test=pd.read_csv('../input/test.csv', sep='\t')
xmlData.drop('date', axis = 1, inplace = True)
from keras.models import Sequential $ from keras.layers import SimpleRNN, Dense,Activation, Conv1D, MaxPool1D
%timeit -n1 -r1 pd.read_csv(dataurl+'test.csv.gz', sep=',', compression='gzip') $ %timeit -n1 -r1 pd.read_csv(dataurl+'test.csv', sep=',')
q_derived_count = c.submit_query( $                     ptoclient.PTOQuerySpec().time("2014-01-01","2018-01-01") $                                      .condition("ecn.stable.*") $                                      .condition("ecn.multipoint.*") $                                      .time_series_year()) $
df.groupBy("column_name").count()
pd_aux2['diff']=pd_aux2.issued_date-pd_aux2.permit_creation_date
df.head()
from bmtk.builder.networks import NetworkBuilder $ net = NetworkBuilder('V1')
date = '2017-03' $ fund_vs_coins(date, ['Bitcoin', 'Ethereum']) $ plt.title('$100 investment a year ago') $ plt.ylabel('Indexed price \n(start at $100)') $ plt.show()
df2.query('landing_page == "old_page"').user_id.nunique()
model.set_parameter(par=demand.append(new_demand), name='demand')
results=logit_mod.fit() $ results.summary()
%bash $ model_dir=$(ls ${REPO}/taxi_trained/export/Servo) $ gcloud ml-engine local predict \ $     --model-dir=${REPO}/taxi_trained/export/Servo/${model_dir} \ $     --json-instances=./test.json
print(df.shape) $ print(df1.shape)
outcome.head()
theft.loc[12]
raw_full_df.head()
mod = sm.Logit(df_new['converted'], df_new[['intercept', 'UK','CA']]) $ results = mod.fit()
prcp_12monthsDf.head()
wrd_clean['tweet_id'] = wrd_clean['tweet_id'].astype('str') $ wrd_clean['in_reply_to_status_id'] = wrd_clean['in_reply_to_status_id'].astype('str') $ wrd_clean['in_reply_to_user_id'] = wrd_clean['in_reply_to_user_id'].astype('str') $ wrd_clean['retweeted_status_id'] = wrd_clean['retweeted_status_id'].astype('str') $ wrd_clean['retweeted_status_user_id'] = wrd_clean['retweeted_status_user_id'].astype('str')
pipeline.steps[1][1].coef_
model_props = {client.repository.ModelMetaNames.NAME: "XGBoost_model_for_Retail_Churn"} $ model_details = client.repository.store_model(model, model_props)
cur.execute("SELECT name FROM sqlite_master WHERE type='table';") $ print(cur.fetchall())
df1 = pd.DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))
import requests $ r = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&output=csv') $ data = r.content
baseball.drop([89525, 89526])
weekly_cases.cumsum().plot()
pandas_ufos_withyears = sqlContext.sql(query).toPandas() $ pandas_ufos_withyears.plot(kind='bar', x='year', y='count', figsize=(12, 5))
df=pd.read_json('dataframe.json') #check the last row $ df.loc[1057,:]
authors_grouped_by_id_saved.schema
bacteria_data.values
autos['registration_year'].describe()
data[data.name == 'New EP/Music Development'].head()
null_diff = new_page_converted.mean() - old_page_converted.mean() $ print(null_diff)
p_old = df2['converted'].sum() / df2.shape[0] $ p_old
import astropy.units as u $ from astropy.coordinates.sky_coordinate import SkyCoord
import textstat.textstat
df = ayush['Event Type Name'].value_counts() $ df_ayush = pd.Series.to_frame(df) $ df_ayush.columns = ['Ayush J'] $ df_ayush
manager.image_df['p_hash'].isin(tree_features_df['p_hash']).describe()
from bmtk.utils.spike_trains import SpikesGenerator $ sg = SpikesGenerator(nodes='network/mthalamus_nodes.h5', t_max=3.0) $ sg.set_rate(10.0) $ sg.save_csv('thalamus_spikes.csv', in_ms=True)
df2.drop_duplicates(['user_id'] , inplace = True)
%matplotlib inline $ import matplotlib.pyplot as plt $ import pandas as pd $ print("pandas version {}".format(pd.__version__))
stock["rise_in_next_day"] = stock["Close"].astype("float").shift(-1)/stock["Close"].astype("float") >=1
subred_counts.head()
df_imgs.info()
df_DOT['Complaint Type'].value_counts().head(1)
dates = pd.date_range('20180114', periods=7) $ dates
Mars_soup = BeautifulSoup(html, 'html.parser')
time_range = pd.DatetimeIndex( $     start=modifications_per_authors_over_time.index.min(), $     end=modifications_per_authors_over_time.index.max(), $     freq=TIME_FREQUENCY) $ time_range
os.listdir()
joblib.dump(rf_gridcv, '../../data/shelter cats/models/random_forests_grid_model2.pkl') 
dfn.to_csv('News.csv')
for i in cpi_all['Index'].cat.categories.tolist(): $     print i    
df_movies.to_csv('/Users/aj186039/projects/PMI_UseCase/git_data/pmi2week/UseCase2/Transforming/movies_v1.csv', sep=',', encoding='utf-8', header=True)
rfc = RandomForestClassifier() $ scores = cross_val_score(rfc,  X_test, y_test,  cv=5) $ np.mean(scores), np.std(scores)    # scoring on my Testing Data under performs also
df[pd.datetime(2014, 5, 3):]
df = pd.DataFrame({'Counts': [10, 31, 42, 5, 1, np.nan]}) $ df
diff_grid = weekend_grid.loc['weekend']['log_odds'] - weekend_grid.loc['weekday']['log_odds'] $ np.exp(diff_grid)
x_scaled.shape
dogs= pd.DataFrame(index=['Rex', 'Rover', 'Polly'], columns=['breed', 'gender', 'weight']) $ dogs
from sklearn.model_selection import cross_val_score $ from sklearn.ensemble import GradientBoostingClassifier $ forest_clf.score(X_test, Y_test) # test socre $
mostactivestation=df2.loc[df2['Stations']==activestations.index[0]] $ mostactivestation $ results1=[mostactivestation["TOBS"].min(),mostactivestation["TOBS"].max(),mostactivestation["TOBS"].mean()] $ results1
full_dataset.loc[(full_dataset.player_team_name=='NRG')&(full_dataset.player_team_opponent=='Luminosity')].to_excel('NRG_v_lumin_only.xlsx')
plt.show()
df[(df.year==2017) & (df.label==1)]['description']
Grouping_Year_DRG_discharges_payments.xs(2015, level='year').head()
tweet_json.head()
df_drug_counts['year'] = df_drug_counts.index # dash likes values from columns, not from indexes
X_test.fillna(X_train.mean(), inplace=True)
df_mes[df_mes['PULocationID'].astype('int64') >= 266]
for row in cursor.columns(table='TBL_FCBridge'): $     print(row.column_name)
df3 = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner') $ df3.head()
df_TempIrrs.info()
contractor_final.info()
executable_path = {'executable_path': 'chromedriver.exe'} $ browser = Browser('chrome', **executable_path, headless=False)
data.head()
np.exp(-0.0674), np.exp(0.0175), np.exp(0.0118), np.exp(0.0469), np.exp(0.0783)
x_y_shuffled = train_df.values $ np.random.shuffle(x_y_shuffled) $ train_df = train_df.iloc[:,:-1] $ Y = x_y_shuffled[:,-1] $ X = x_y_shuffled[:,:82]
lr = LogisticRegression(C=0.01, solver='liblinear') $ lr.fit(train_x, train_y) $ predicted_probs = lr.predict_proba(production_df[feature_names]) $ predicted_odds = 1 / predicted_probs
recipes['ingredients'].str.len().idxmax()
client.venues.search(params={'near': 'New York, NY','query': 'coffee'})
sns.distplot(master_list[master_list['Count'] >= 5]['Count'])
unique_urls[unique_urls.domain == domain].head()
tokenizer = nltk.casual.TweetTokenizer()
learn.save("dnn40")
df['incident_zip'] = df.incident_zip.astype(int) $ df = df[df['incident_zip'] < 12000 ]
print('reduce memory') $ utils.reduce_memory(user_logs) $ utils.reduce_memory(train)
tfidf_vect = TfidfVectorizer(analyzer = clean_text) $ X_tfidf = tfidf_vect.fit_transform(tweets_1['text']) $ X_features = tfidf_vect.fit_transform(tweets_1['text'])
pre_strategy_google.head()
print(stock_data.dtypes)
df_train.info()
%%time $ df['created_at'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %X %p') $ df['closed_at'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %X %p')
X = df[[x for x in df.columns if x != 'bet_won_over' and x != 'bet_won_under']] $ y_over = df[['bet_won_over', 'game_date']] $ y_under = df[['bet_won_under', 'game_date']]
maxitems = 10 $ print("London tweets retrieve testing") $ print('----------------------------------') $ for tweet in tweepy.Cursor(api.search, q="place:%s" % place_id_L).items(maxitems): $     print(tweet.text)
cohort_retention_df.to_csv('retention_cohorts_518.csv')
agency_borough.head()
sum(abs(removing_features_validate[:, 2] - clf.predict_proba(X_validate)[:, 1]))
data_active = data_activ[data_activ['new_date']==day.strftime('%Y/%B')].reset_index().get('id')[0]
model.doesnt_match("france england germany berlin".split())
df.tail(5)
learn=ConvLearner.pretrained(f_model,data,metrics=metrics)
train = train.replace(['Business', 'Economy'], [1, 0]) $ test = test.replace(['Business', 'Economy'], [1, 0])
autos['num_photos'].value_counts()
df_new.groupby(['country','landing_page']).converted.mean()
df[df.isnull().any(axis=1)] # returns the rows with missing valuies
frame.sub(series3, axis='index') # or axis=0
def add_dummies_to_list(column_to_dummy, target_list): $     dummies_df = pd.get_dummies(column_to_dummy) $     for column in dummies_df: $         target_list.append((column, dummies_df[str(column)])) $
csvDF = pd.read_csv("C://customer.csv",header=None,names=['custid','fname','lname','age','job'])
df_NOTCLEAN1A.shape
model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']]) $ results = model.fit()
df.shape
persort.head()
autos["price_dollars"].value_counts().sort_index(ascending=False)
data_grid1, err = ek.get_data(["IBM", "GOOG.O", "MSFT.O"], ["TR.PriceClose", "TR.Volume", "TR.PriceLow"]) $ data_grid2, err = ek.get_data("IBM", ['TR.Employees', {'TR.GrossProfit':{'params':{'Scale': 6, 'Curn': 'EUR'},'sort_dir':'asc'}}]) $ fields = [ek.TR_Field('tr.revenue'),ek.TR_Field('tr.open',None,'asc',1),ek.TR_Field('TR.GrossProfit',{'Scale': 6, 'Curn': 'EUR'},'asc',0)] $ data_grid3, err = ek.get_data(["IBM","MSFT.O"],fields)
import pandas as pd $ import numpy as np $ import datetime $ import matplotlib.pyplot as plt
df.head()
train['has_opened'].describe()
city_pop2 = city_pop.copy() $ city_pop2.columns = ["population", "name", "state"] $ pd.merge(left=city_loc, right=city_pop2, left_on="city", right_on="name")
own_star.fillna(0, inplace=True) $ own_star.head(20)
unique_animals.remove("Dog")
nvidia.take(1)
df.info()
weather_df_rsmpld = weather_df_byday.resample('1M').mean() $ weather_df_rsmpld.head()
len(van_final['userid'].unique())
time_sentiment = data_sentiment.groupby(["created_at"])["sentiment_score"].mean() $ time_sentiment = time_sentiment.to_frame().reset_index() $ time_sentiment["created_at"] = time_sentiment["created_at"].apply(lambda t: datetime.combine(d, t)) $ sentiment_plot_data = time_sentiment.groupby(pd.Grouper(key='created_at', freq='300s'))["sentiment_score"].mean()
X.head()
z_score , p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='larger') $ z_score , p_value
contribs.median()
autos['price'].value_counts().sort_index(ascending=False).head(20)
baseball_newind.iloc[:5, 5:8]
driver.get('https://submissions.scholasticahq.com/reviewers/mobilization?page=2&per_page=10&query=&sort_column=most_recent_invitation_sent_at&sort_order=desc&journal_id=mobilization')
knowledge_per_component = git_blame.groupby('component').knowing.mean() $ knowledge_per_component.head()
df_archive[df_archive.text.duplicated()]
readerASN = geoip2.database.Reader(GEOLITE_ASN_PATH) $ readerCITY = geoip2.database.Reader(GEOLITE_CITY_PATH)
new_page_converted = df2.query('landing_page == "new_page"').sample(n_new, replace=True)
delineate_in_weekdays = temp.groupby(['weekday'])
import statsmodels.api as sm $ convert_old = 0.119659 $ convert_new = 0.119659 $ n_old = 145274 $ n_new = 145310
sp = openmc.StatePoint('statepoint.20.h5')
n_new = df2.query('group == "treatment"').count()[0] $ n_new
f = "../data/airbnb/sessions.csv" $ num_lines = sum(1 for l in open(f))
a = set(X_train.columns) $ b = set(X_test.columns) $ print(a.difference(b))
new_scores = [x[1] - x[0] for x in scores_pairs_by_business["score_pair"]] $ plt.hist(new_scores, bins = np.arange(-25,30,2))
journalists_mention_df = mention_df.join(user_summary_df['gender'], how='inner', on='mention_user_id', rsuffix='_mention') $ journalists_mention_df.rename(columns = {'gender_mention': 'mention_gender'}, inplace=True) $ journalists_mention_df.count()
tweet_archive_enhanced_clean.loc[1202,'text']
trump['source'] = trump['source'].str.replace('<[^>]*>', '') $
reviews.loc[(reviews.points/reviews.price).idxmax()].title
predictions = pipeline.predict(fb_test.message)
lines  = df.rdd
pd.Series([42, 13, 2, 69])
countries_df=countries_df.set_index('user_id')
msk = np.random.rand(len(df)) < 0.8 $ train = cdf[msk] $ test = cdf[~msk]
weather.isnull().sum()
conn_b.commit()
tweets.head()
y_log = np.log(y)
sns.factorplot(y='Average_Housing_permits',x='Date',data=df_newhouse,kind='bar',aspect=7)
if(not(os.path.exists("model/{}".format(version)))): $     os.makedirs("model/{}".format(version))
grouped_months_liberia = deaths_liberia.groupby(deaths_liberia['Date'].apply(lambda x: x.month)) $ deaths_liberia['National'] = deaths_liberia['National'].astype(int) $
data_tickers = data_tickers[data_tickers.index >= startDateStock] $ data_tickers['adj_close'][ticker].plot() $ plt.title(ticker + ' share price') $ plt.xlabel('') $ plt.show()
print (repeated_user) $ print (repeated_user.index)
%run process_twitter2tokens.py -i ../data/test_promotion_people.csv -ot ../data/test_promotion_people.txt -oc ../data/test_promotion_people_tokenized.csv -co text
problemData=df.loc[df.lizardNumber\ $                    .isin(unprocessed)\ $                    ,['date','year','svl','sex','meters','new.recap','location','toes','lizardNumber','species']] $ print("\nSo far {} rows could not be processed.".format(problemData.shape[0])) $ problemData
gdfNps.head()
from flask import Flask, jsonify, request $ from urllib.parse import urlparse
youthUserNov2017.cityName.value_counts()
b = df2['landing_page'] == 'old_page' $ n_old = df2[b] $ n_old = n_old.converted.count() $ print('Number of rows in which landing page is aligned with new page is:{}'.format(n_old))
log_columns=['FLOW','REVENUE'] $ log_df3 = df3.copy() $ log_df3[log_columns] = log_df3[log_columns].apply(np.log10)
df2['converted'].mean()
count_non_null(geocoded_df, 'Disposition.Date')
pgh_311_data_merged = pgh_311_data.merge(pgh_311_codes, left_on="REQUEST_TYPE", right_on="Issue") $ pgh_311_data_merged.sample(10)
collect.set_custom_filter(exclude_retweets)
df1[df1['day_of_week']== 'Monday']['tweet_id'].count()
y.name
df.groupby('episode_id').size().head()
temperature_sensors_df['state'].hist(bins=10);  # Plot histogram of all temperature sensors $ plt.xlabel("Temperature $^\circ$C");
def removeHttp(myString): $     if 'http' in myString: $         k = myString.index('http') $         myString = myString[0:k] $     return myString
%matplotlib inline $ import matplotlib.pyplot as plt $ sns.set() # use seaborn styles $ births.pivot_table('births', index='year', columns='gender', aggfunc='sum').plot() $ plt.ylabel('total births per year')
OldConverted = df2.query('converted == 1')['user_id'].count() $ HNull = (OldConverted/NewTotalUser) $ print("Null Convert Rate: ",HNull)
sh_results = session.query(Measurements.date,Measurements.tobs).\ $             filter(Measurements.date >= pa_min_date).\ $             filter(Measurements.station==station_max).all()
gb = experiment_df.groupby('treatment_group_key')
results.summary()
df4[df4['country_UK']==1]['converted'].mean()
df['TOTAL_PAYMENT'].describe()
data_volumn.shape
recipes.ingredients.str.contains('[Cc]innamon').sum()
ratings.show(5)
com_grp[['Age','Salary']].sum()
wic.to_excel('wic.xlsx', sheet_name='Total Women') # This didn't work in code as openpyxl was supposedly missing!
df2.columns[0] $ df2.drop('comments.data' , 1 , inplace=True) $ df2.drop( df2.columns[[0,1,5,6,7]] , 1 , inplace=True)
%timeit ' '.join(L)
cols_to_split = data.view_count_by_category
archive_clean.drop(['expanded_urls'], axis=1, inplace = True) $ archive_clean.drop(['doggo'], axis=1, inplace = True) $ archive_clean.drop(['floofer'], axis=1, inplace = True) $ archive_clean.drop(['pupper'], axis=1, inplace = True) $ archive_clean.drop(['puppo'], axis=1, inplace = True)
data = pd.read_json('challenge.json')
p_new = df2[df2['landing_page']=='new_page']['converted'].mean() $ print("The probability of conversion for the new page is: " + str(p_new))
round(df1['converted'].mean(), 4)
new_page_converted = np.random.choice([0,1], n_new, [1-p_new, p_new]) $
df[df['Complaint Type']=='Noise - Residential']['Descriptor'].value_counts()
flight_delays.head()
len(tweet_archive_df[tweet_archive_df.name.isin(['a', 'an', 'None'])][['tweet_id', 'name']])
df.drop('date_account_created',axis=1,inplace=True)
fundmean = fundret.mean()*12 $ fundvol = fundret.std()*np.sqrt(12) $ fundmean / fundvol
autos["brand"].value_counts(normalize=True)
df = df[(df.powerPS > 50) & (df.powerPS < 1000) ]
n_new = (df2.query('group == "treatment"')['converted'] >= 0).sum() $ n_new
joined.reset_index(inplace=True) $ joined_test.reset_index(inplace=True)
first_result.find('a')['href']
hr["chart delta"] = \ $ hr.apply(lambda x: (x["charttime"] - $                         x["realtime"]).total_seconds(), axis=1) $
Image(url='https://tf-curricula-prod.s3.amazonaws.com/curricula/b04b6f653b9d364d5612ac767527458a/DATA-001/v2/assets2/1.1.3_Get_To_Know_Pythons_Data_Types/Data_types_2.png')
occurrences = np.asarray(vectorized_text_labeled.sum(axis=0)).ravel() $ terms = vectorizer.get_feature_names() $ counts_df = pd.DataFrame({'terms': terms, 'occurrences': occurrences}).sort_values('occurrences', ascending=False) $ counts_df
df2[df2.duplicated('user_id') == True]['user_id']
result_new.summary2()
full_nan = full.replace('@NA',np.NaN) # replace @NA $ full_nan = full_nan.replace('',np.NaN) # didn't find any empty strings
df2.describe() $
cur.execute("select s.id, s.storage_directory from specimens s where s.name ilike %s", (cell,)) $ result = cur.fetchone()
zip_counts = bus.fillna("MISSING").groupby(["postal_code"])["name"].count().to_frame() $ zip_counts = zip_counts.rename(columns={'name': 'count'}) $ zip_counts
counts.plot(figsize=(20, 10));
nba_df.iloc[1001:1005, ]
ssc.awaitTermination()
df.groupby(['education'])['loan_status'].value_counts(normalize=True)
df = es_rdd.map(lambda l: Row(**dict(l))).toDF()
users.head()
lookforward_window = 1 $ for iter_x in np.arange(lookforward_window)+1: $     df['y+{0}'.format(str(iter_x))] = df[base_col].shift(-iter_x)
res = df_imgs.tweet_id.value_counts() > 1 $ res.value_counts()
new.head()
fig, ax = plt.subplots(figsize=(10,8)) $ fig = arma_res.plot_predict(start=0, end=25000, ax=ax) $ legend = ax.legend(loc='upper left') $ plt.title("Time Series Forecasting using the ARIMA model")
before_date=str(fc_clean.sel(time =start_of_event, method = 'pad').time.values)[0:10] $ after_date=str(fc_clean.sel(time =start_of_event, method = 'backfill').time.values)[0:10]
X_mice['age'] = np.log(X_mice['age'])
length_of_new = (df2[df2['landing_page'] == "new_page"].user_id.count()) $ length_of_new 
train.dropna(inplace=True) $ test.dropna(inplace=True)
print('The probability of receiving new page:', (df2['landing_page'] == 'new_page').mean())
print(df_data.PERIDOOCORRENCIA_TIPO.value_counts()) $ df_data.PERIDOOCORRENCIA_TIPO.hist(bins=[0,1,2,3,4,5])
bacteria2.index = bacteria.index
with open('test_data//open_close_test.txt') as my_file: $     print (my_file.read()[0:100])
train = pd.read_csv(train_file, header=0, sep='\t', na_values=[''], keep_default_na=False)
file_attrs_string_split = file_attrs_string.split("'") $ file_attrs_string_split
from matplotlib.pyplot import figure $ figure(num=None, figsize=(17, 4), dpi=80, facecolor='w', edgecolor='k') $ names = list(tweet_lang_hist.index) $ values = list(tweet_lang_hist.lang_freq) $ plt.bar(names, values)
df3 = df2.join(df_country.set_index('user_id'), on = 'user_id')
plt.plot(data.moving_avg)
_ = us[us['cityOrState'].isin(states) == False]['cityOrState'].value_counts(dropna=False).index.tolist()[1:40] $ _
print(site.text)
highest_tobs = session.query(Measurement.tobs).filter(Measurement.date > query_date_2yearsago).filter(Measurement.station == most_active).all() $ df_highest_tobs = pd.DataFrame(highest_tobs, columns = ["tobs"]) $ df_highest_tobs $ df_highest_tobs.plot(kind="hist")
sns.heatmap(shows.corr())
train_topics_df.head()
scn_genesis = pd.to_datetime(min(USER_PLANS_df['scns_created']))
af.length.sum() / 1e9
pd.value_counts(ac['If No Eligibility, Why?']).head(20)
df.head()
twitter_archive[twitter_archive.text.str.contains(r"(\d+\.\d*\/\d+)")] $
print("Number of Malware in ATT&CK") $ print(len(all_attack['malware'])) $ malware = all_attack['malware'] $ df = json_normalize(malware) $ df.reindex(['matrix', 'software', 'software_labels', 'software_id', 'software_description'], axis=1)[0:5]
twitter_archive_enhanced.info()
df.head(2)
twitter_archive_clean.timestamp=pd.to_datetime(twitter_archive_clean.timestamp)
len([x for x in eligible_posts if x['previous.posts']==0]) / len(eligible_posts)
jan_2015_frame.head()
with tb.open_file(filename='data/my_pytables_file.h5', mode='r') as f: $     print(f.root.dangling_link[...])
dup_user = df2['user_id'][df2['user_id'].duplicated()] $ dup_user
logreg = LogisticRegression() $ logreg.fit(X_train, Y_train) $ Y_pred = logreg.predict(X_test) $ acc_log = round(logreg.score(X_train, Y_train) * 100, 2) $ acc_log
cols = df_train.columns.values $ dfTemp = df_train.dropna(how='any') $ print(dfTemp.shape) $ dfTemp.head(10)
field_streams = list(field_stream_occurances_df.field_stream.unique())
trigrams_fd = nltk.FreqDist() $ trigram_words = [ ','.join(map(str,tg)) for tg in nltk.trigrams(wordsX) ] $ trigrams_fd.update(trigram_words) $
country_sort = averages[averages.Country == 'Canada'] $ country_sort
p_diff = np.array(diffs) $ plt.hist(p_diff); $ plt.axvline(x = ab_data_diff, color = 'r');
weather_mean.iloc[[4, 2], [6, 4, 5]]
a,b=train,(train+validation) $ x,y=X[a:b],Y[a:b] $ scores_cv=model.evaluate(x,y) $ print ("\n%s: %.2f%%" %(model.metrics_names[1],scores_cv[1]*100))
first_date = dt.date(2017,8, 23) - dt.timedelta(days=365) $ one_year_precipitation = session.query(Measurement.date, Measurement.prcp).\ $                         filter(Measurement.date > first_date).\ $                         order_by(Measurement.date).all() $ print(one_year_precipitation)
treatment_conv_prob = df2.loc[(df2["group"] == "treatment"), "converted"].mean() $ treatment_conv_prob
cum_cont = pd.DataFrame(data={"cum_contr":np.cumsum(sorted_by_date.contb_receipt_amt),"date":get_date(sorted_by_date.contb_receipt_dt)}) $ cum_cont.head()
articles.min()
df2 = df.copy() $ df2 = df2[((df.group == 'control') & (df.landing_page == 'old_page')) | $           ((df.group == 'treatment') & (df.landing_page == 'new_page'))]
assert X_train.index.all() == X_train_df.index.all()
plt.figure(figsize=(10,10)) $ plt.plot(building_pa_prc_shrink['zipcode'],building_pa_prc_shrink['supervisor_district'],'o') $ plt.grid(True) $ plt.ylabel('supervisor_district') $ plt.xlabel('zipcode')
df_t_best_means.shape
sns.factorplot('pclass', data=titanic3, hue='sex', kind='count')
token.sender = token.sender.astype(int) $ token.receiver = token.receiver.astype(int)
df.sort_index(inplace=True) $ df.head()
cb.organization('matter-io').description
weather.isnull().any()
sqlContext.registerFunction("TimesTen", TimesTen)
ts.asfreq('B').fillna(0)
sim_closes.plot(figsize=(8,6));
indices = np.argsort(vectorizer.idf_)[::-1] $ top_n = 100 $ top_features = [feature_names[i] for i in indices[:top_n]] $ print(top_features)
m['predicted_purchases'].sum()
top_songs['Month'] = top_songs['Date'].dt.month
msft = pd.read_csv("../../data/msft.csv", index_col=0) $ msft.head()
y_t = knn_model.predict(X_train)
X_train.set_index('id', inplace=True)
X_train, X_test, y_train, y_test = train_test_split(df['full_text'], y_retweet, test_size=0.3, random_state=0) $ vectorizer = TfidfVectorizer(stop_words='english',token_pattern = r'\b[a-zA-Z]{3,}\b',lowercase = True) $ X_train_tfidf = vectorizer.fit_transform(X_train) 
from pandas.tseries.offsets import * $ d + BusinessDay()
print(pd.crosstab(classes, resultados, rownames = ["Real"], colnames=["Predito"], margins=True))
print(props.head())
cashflows_plan_origpd_noshift_all[(cashflows_plan_origpd_noshift_all.id_loan==574) & (cashflows_plan_origpd_noshift_all.fk_user_investor==31192)].to_clipboard()
images_predictions.info()
round(max(multi.handle.value_counts(normalize=True)),4)
import statsmodels $ import statsmodels.api as sm $ import statsmodels.formula.api as smf
archive_df_clean['tweet_id'] = archive_df_clean['tweet_id'].astype(str) $ status_df['tweet_id'] = status_df['tweet_id'].astype(str) $
os.chdir("polarity_season_data_LexiconBased")
with tb.open_file(filename='data/my_pytables_file.h5', mode='a') as f: $     earray = f.root.my_earray $     earray.append(sequence=matrix[0:1000, :])
1/np.exp(result_c.params)
autos["price"].head(3)
df_categorical = df_categorical.applymap(clean_string)
stmt1=employee.delete().where(employee.c.id==1) $ stmt2=employee.delete().where(employee.c.id==2 and employee.c.sex=='Male') $ conn.execute(stmt1) $ conn.execute(stmt2)
calls_nocontact.zip_code.value_counts()
df=pd.DataFrame([['frank','M',29],['mary','F',23],['tom','M',35],['ted','M',33],['jean','F',21],['lisa','F',20]],columns=['name','gender','age']) $ df
beginning_date = df3['Donation Received Date'].max() - dt.timedelta(days=365) $ end_date = df3['Donation Received Date'].max() $ print("The focus period begins at:", beginning_date) $ print("The focus period ends at:", end_date)
train_frame = temp_frame[0 : int(0.7*len(temp_frame))] $ test_frame = temp_frame[int(0.7*len(temp_frame)) : ]
retention_10.to_csv('retention.csv')
twitter_archive_master.describe()
df_predictions_clean = df_predictions_clean.sort_values('tweet_id', ascending = True)
pd.DatetimeIndex(pivoted.T[labels==1].index)[Tue_index]
[endpoint_deployments] = [x.get('entity').get('deployments').get('url') for x in json.loads(response_get.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid] $ print(endpoint_deployments)
z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old , n_new], $                                              alternative='smaller') $
archive.rating_denominator.value_counts()
from collections import defaultdict $ word_cnts = defaultdict(int) $ for tweet in orig_tweets['text']: $     for word in nlp(tweet): $         word_cnts[word.text] += 1
import pandas as pd $ df = pd.read_csv('realdonaldtrump.csv', parse_dates=[5]) $ df.head()
print(1/np.exp(-0.0408)) $ print(1/np.exp(0.0099))
help(datasets[0]) $ print(datasets[0].__dict__)
df_actor[df_actor.last_name.str.contains('GEN')]
contract = import_contract_history('../data/contract_history.csv')
df_not_treat_new = df[((df['group'] == 'treatment') & (df['landing_page'] == 'new_page')) == True] $ df_not_cntrl_old = df[((df['group'] == 'control') & (df['landing_page'] == 'old_page')) == True] $ df2 = pd.concat([df_not_treat_new, df_not_cntrl_old]) $
commits_per_year1 = corrected_log["timestamp"].groupby(corrected_log["timestamp"].dt.year).count() $ commits_per_year = corrected_log.groupby(pd.Grouper(key='timestamp', freq="AS")).count() $ print(commits_per_year1.head(5)) $ commits_per_year.head(5)
bobby_ols = ols('opening_gross ~ star_avg',dftouse_four).fit() $ bobby_ols.summary()
nold = df2.query('landing_page == "old_page"').shape[0] $ nold
fine_xs = xs_library[fuel_cell.id]['transport'] $ condensed_xs = fine_xs.get_condensed_xs(coarse_groups)
np.sqrt(metrics.mean_squared_error(y_val, preds)) $
cand_date_df = pd.read_pickle('data/pres_sorted_with_sponsors_and_party.pickle') $ cand_date_df.head()
pd.DataFrame.to_csv(pd.DataFrame(file_lines),"Fireeye_output.csv",index = False)
df2 = df_imputed_mean_NOTCLEAN1A.copy()
df3[df3['STATION'] == '103 ST'].groupby(['DATE']).sum()
bets.drop('bet_either', 1, inplace=True)
np.allclose(df1 + df2 + df3 + df4, $            pd.eval('df1 + df2 + df3 + df4'))
temp_df2.shape
df.titles.shape
np.sum(rfc_feat_sel.feature_importances_ > 0)
train_data['price'], Lambda = stat.boxcox(train_data['price'].clip(1475,13000)) $ Lambda
run txt2pdf.py -o '2018-06-22  2013 872 discharges.pdf'  '2018-06-22  2013 872 discharges.txt'
from datetime import datetime $ plot2 = df[datetime(2018, 7, 3,18):datetime(2018, 7, 3,21)].plot(y='lux', marker='.', linestyle="--")
pd.concat([test, train]).plot(y=['PJME', 'ElasticNet_Prediction'], figsize=(15,5))
import quandl $ with open("quandl_apkey.txt", "r") as keyfile: $     key = keyfile.read() $ quandl_key = key.rstrip() $ quandl.ApiConfig.api_key = quandl_key # "evWfebtKvTVN_dxvWqau"
log_with_day.select('dateTime', 'dayOfWeek').show(10)
median = reviews.price.median() $ reviews.price.map(lambda v: v-median)
path = "https://raw.githubusercontent.com/arqmain/Python/master/Pandas/Project2/car_data.txt" $ df = pd.read_table(path, sep ='\s+', na_values=['.']) $ df.head(5)
for df in (joined,joined_test): $     df.loc[df.CompetitionDaysOpen<0, "CompetitionDaysOpen"] = 0 $     df.loc[df.CompetitionOpenSinceYear<1990, "CompetitionDaysOpen"] = 0
stationActive_df = pd.read_sql("SELECT station, count(*) as `Station Count` FROM measurement group by station order by `Station Count` DESC", conn) $ stationActive_df
np.sum(x < 6)
client.get_list_database()
df_final.loc_country.value_counts()
if 'LAC' in team_list: team_list.remove('LAC') $ if 'ORL' in team_list: team_list.remove('ORL')  $ len(team_list)
f_counts_minute_ip = spark.read.csv(os.path.join(mungepath, "f_counts_minute_ip"), header=True) $ print('Found %d observations.' %f_counts_minute_ip.count())
lr = LogisticRegression(random_state=20, max_iter=10000) $ param_grid = { 'C': [1, 0.5, 5, 10,100], 'multi_class' : ['ovr', 'multinomial'], 'solver':['saga','newton-cg', 'lbfgs']} $ grid_tfidf = GridSearchCV(lr, param_grid=param_grid, cv=10, n_jobs=-1)
new_page_group = df2.query('landing_page == "new_page"') $ print(treatment_group.user_id.nunique() / df2.user_id.nunique()) $ print(new_page_group.user_id.nunique() / df2.user_id.nunique())
es = EarlyStopping(patience=10) $ history = model2.fit([X_train_text, X_train_uid], y_train, validation_split=0.25, $                         epochs=100, verbose=1, callbacks=[es])
res4 = df_test4_promotions.agg({'PromotionRuleCount':'sum', 'AwardAmount':'sum', 'ShopperID':'nunique'}) $ res4['AwardAmount'] = res4['AwardAmount'] / 100 $ res4 $
test_tweet = "I won't be playing rolandgarros this year. I'll miss my fans in France, but appreciate your support." $ w=analize_sentiment(test_tweet) $ print(w)
gamma.E_gsf_line_plot(vect=[0.5, 0.5, -1], length_unit=length_unit, $                       energyperarea_unit=energyperarea_unit) $ plt.show()
states = pd.DataFrame({'population': population, $                        'area': area}) $ states
from IPython.display import display $ pd.options.display.max_columns = None
recs.ix['ibm us equity','best analyst recs bulk'].ix[:,'chg pct ytd'].mean()
daily_trading_volume.sort()
prng.to_timestamp(how='e')
lgbm_train=train[['air_store_id','visit_date']] $ lgbm_train['visitors'] = lgbmrstcv.predict(train[col].values) $ lgbm_train.to_csv('stacking_input/lgbm_tscv_train_s2.csv',index=False)
flight.where(flight.trip == 1).show(2)
df_day['Forecast'] = bound_prediction(sarima_mod.predict()) $ df_day $ df_day.plot(figsize=(14, 6));
data = pd.merge(irl, billstargs, left_on='bill_id', right_on='bill_id') $ data.drop_duplicates(['bill_id'], keep='last') $
load2017 = load2017.dropna() 
dfY.head()
input_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])] $ weights = { $     'node_0': np.array([2, 4]), $     'node_1': np.array([ 4, -5]), $     'output': np.array([2, 7])}
class_merged=pd.merge(class_merged,stores,on=['store_nbr'],how='left') $ print("Rows and columns:",class_merged.shape) $ pd.DataFrame.head(class_merged)
import statsmodels.api as sm $ log_mod = sm.Logit(df3['converted'], df3[['intercept', 'ab_page']]) $ results1 = log_mod.fit() $
WorldBankdf.registerTempTable("worldbank")
print('This is the list of folders in your directory for this HydroShare resource.') $ test = [each for each in os.listdir(homedir) if os.path.isdir(each)] $ print(test)
data_jsn=[x for x in tweets_data.find({}, $                      {'favorite_count':1,'retweet_count':1,'created_at':1,'entities.hashtags':1,'entities.urls':1, $                       'entities.media':1,'_id':0})]
top_85_important_features = [] $ for b in range(0,len(fit.ranking_)): $     if fit.ranking_[b] == 1: $         top_85_important_features.append(b) $         print b,data_adv.columns[b]
timelog.describe()
temp_df2.info()
test_data.dtypes
p_new_page = df2.query('landing_page == "new_page"').shape[0] / df2.shape[0] $ p_new_page
def remove_by_regex(tweets, regexp): $         tweets.loc[:, "text"].replace(regexp, " ", inplace = True) $         return tweets
alto = df.loc[0,'field1'] $ df.loc[0,'dfield1']=0 $ alto,type(alto)
tropical = reviews.description.map(lambda r: 'tropical' in r).value_counts() $ fruity = reviews.description.map(lambda r : 'fruity' in r).value_counts() $ pd.Series([tropical[True],fruity[True]],index=['tropical','fruity'])
stock_data = pd.DataFrame(list(iex_coll_reference.find())) $ stock_data.head()
labmt.head()
Class_frame.head()
df_pivot1 = df_pivot[df_pivot[10102].notnull() & df_pivot[10120].notnull()] $ df_pivot1.shape
words = latest_tweet['full_text'].split(' ')
goodTargetUserItemInt.head()
from sklearn import linear_model $ lin_reg = linear_model.LinearRegression() $ lin_reg.fit(x_train,y_train)
train_dum_clean = train_dum[['ID'] + [col for col in train_dum.columns if 'Source_' in col or $                       'Customer_Existing_' in col]]
fig, ax = plt.subplots() $ weekday_agg.plot(kind='scatter',x='end_station_id', y= 'duration', c='weekday', s= 50,cmap=plt.cm.Blues, ax=ax) $ plt.show() $
df['gender']= df['gender'].astype(str) #converting the numerical field of gender values (0,1,2) to strings $ grouped_genders = df.groupby('gender').count() #grouping by gender $ sorted_gender = (grouped_genders.tripduration.sort_values(ascending = False)).head()#and arranging in descending order $ sorted_gender
df1.join([df2, df3, df4]).fillna('')
pods.notebook.display_plots('quadratic_function{num_function:0>3}.svg', $                             directory='../slides/diagrams/ml', $                             num_basis=IntSlider(0,0,2,1))
k1.head()
tweets_clean['user'] = user_info['id'] $ tweets_clean.head()
stock['volatility'] = stock.high - stock.low
df = df.merge(nycshp, on="ZIPCODE")
station_count = session.query(Stations.station).group_by(Stations.station).count()
query_result1.spatial_reference
emails = re.findall('mailto:(.*?) so', text) $
save_combined_df( donald_combined_df, 'realDonaldTrump' ) $ !ls ./data 
validation.analysis(observation_data, simple_resistance_simulation_1)
SANDAG_age_df.loc[(2012, '1'), 'POPULATION'].sum()
logit_mod.fit().summary()
test_im = X_train[154] $ plt.imshow(test_im.reshape(28,28), cmap='viridis', interpolation='none') $ plt.show()
filename = 'nyc_subway_weather.csv' $ subway_df = pd.read_csv(filename)
df = pd.read_csv("../../data/msft2.csv",skiprows=[0,2,3]) $ df
df2[df2['converted']==1].user_id.nunique()/df2.user_id.nunique()
cur.fetchall()
fig = plt.figure(figsize=(12,8)) $ ax1 = fig.add_subplot(211) $ fig = sm.graphics.tsa.plot_acf(resid_701.values.squeeze(), lags=40, ax=ax1) $ ax2 = fig.add_subplot(212) $ fig = sm.graphics.tsa.plot_pacf(resid_701, lags=40, ax=ax2)
squares.values
df.drop(df[df.donation_date == '1899-12-31'].index, axis=0, inplace=True)
df_apps[df_apps.package_name.str.contains('com.instagram.android')].count()
results["units"]["unit_annotation_score"].head()
df.groupby('dog_type')['rating'].describe()
df.select('label').describe().show() $
tp = dataframe.groupby(['User']).size().reset_index(name='no_of_tweet_counts') $ tp = tp.sort_values(by = tp.columns[1],ascending = False) $ tp.to_csv('topNusers.csv') $ tp.head()
yuniq = df['hotel_cluster'].unique() $ if list(range(len(yuniq)))==sorted(list(yuniq)): $     print('Hotel cluster classes span range of {0} to {1}'.format(min(yuniq), max(yuniq)))
df_new[['new_page', 'old_page']] = pd.get_dummies(df_new.landing_page)
nconvert = len(df2[(df2.converted==1)]) $ ntot = len(df2.index) $ pnew=nconvert/ntot $ prob
features = cv.get_feature_names() $ feature_importances = model.feature_importances_ $ features_df = pd.DataFrame({'Features': features, 'Importance Score': feature_importances}) $ features_df.sort_values('Importance Score', inplace=True, ascending=False)
df_methods.head()
pd.set_option('display.max_colwidth', -1) $ result = df[:200] $ result['class'] = pred_y_df
csvData[csvData['street'].str.match('.*North.*')]['street']
image_predictions_df[image_predictions_df.p1_dog == False][0:5]
RMSE_list = [] $ W =[i for i in range(n_training)[3::3]] $ for w in W: $     RMSE_w = ts_kfold(w,w,regr_M7) $     RMSE_list.append(RMSE_w)
test_data.head()
batting_df.tail(30)
df2['intercept'] = 1 $ df2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] $ df2.head()
STD_reorder_stats.head(10)
RegO = pd.to_datetime(voters.RegDateOriginal.map(lambda x: x.replace(' 0:00', ''))) $ Reg = pd.to_datetime(voters.RegDate.map(lambda x: x.replace(' 0:00', ''))) $ datecomp = pd.DataFrame({'OrigRegDate':RegO, 'RegDate':Reg})
df = df.withColumn('geolocation', concat(df.latitude, lit(','),df.longitude)) $ df = df.drop('latitude').drop('longitude') $ hz.addTransformDescr('geolocation','geolocation variable created from latitude and longitude variables') $ print("Generated Harmonized geolocation variable, and dropped original latitude and longitude variables")
S.executable = "/media/sf_pysumma/summa-master/bin/summa.exe"
df_mes = df_mes[df_mes['mta_tax']>=0] $ df_mes.shape[0]
x_test, y_test = x[int(0.6*len(x)):], y[int(0.6*y.size):]
msk = np.random.rand(len(df)) < 0.8 $ train = cdf[msk] $ test = cdf[~msk]
observed_tvd = tvd(clinton_pivoted.iloc[:,0], clinton_pivoted.iloc[:,1]) $ observed_tvd
acc.find(bdate='20.08.2016')
np.exp(0.0506), np.exp(0.0408)
data_libraries_df = pd.merge(left=libraries_df, right=tmp, on="asset_name", how="outer")
weather_mean.iloc[:, 2:5]
import kipoi_veff $ from kipoi_veff import VcfWriter $ vcf_path = "example_data/clinvar_donor_acceptor_chr22.vcf" $ out_vcf_fpath = vcf_path[:-4] + "%s.vcf"%model_name.replace("/", "_") $ writer = VcfWriter(model, vcf_path, out_vcf_fpath)
volume = [] $ for entry in d["dataset_data"]["data"]: $     volume.append(entry[6]) $ answer = reduce(lambda x, y: x + y, volume) / len(volume) $ print("The average daily trading volume was $"+str(answer))
q_all_pathdep.results()
(df_merged['p1'].iloc[228], df_merged['p1_conf'].iloc[228],df_merged['p2'].iloc[228],df_merged['p2_conf'].iloc[228],df_merged['p3'].iloc[228],df_merged['p3_conf'].iloc[228])
ac['Description'].describe()
staff.drop([1])
co_sum = CO_profit['Profit Including Build Cost'].sum() $ buildings_co = len(CO_profit)
tweet_text = mars_weather['text'] $ date = mars_weather['created_at'] $ tweet_date = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(date,'%a %b %d %H:%M:%S +0000 %Y')) $
Lda = gensim.models.ldamodel.LdaModel $ ldamodel = Lda(corpus=doc_term_matrix, id2word=dictionary, num_topics=topics)
p_old_obs = df2.query('group == "control"').converted.sum() / df2.query('group == "control"').shape[0] $ p_new_obs = df2.query('group == "treatment"').converted.sum() / df2.query('group == "treatment"').shape[0] $ diff_obs = p_new_obs - p_old_obs $ diff_obs
dates = most['created_date'].tolist()
df_clean['rating_numerator'].value_counts()
airbnb_df.info(memory_usage='deep')
autos['price'].unique()
df_combined['won'] = df_combined['won'].astype(int)
df_clean.text.sample().tolist()
record = session.query(Measurement.station, func.count(Measurement.date)).group_by(Measurement.station).\ $             order_by(func.count(Measurement.date).desc()).all() $ record
lat_dim = ncfile.createDimension('lat', 73)     # latitude axis $ lon_dim = ncfile.createDimension('lon', 144)    # longitude axis $ time_dim = ncfile.createDimension('time', None) # unlimited axis (can be appended to).
tw.shape
autos.loc[max_price, "price"].count()
results.keys()
p_new = df2.converted.mean() $ print(p_new)
week14 = week13.rename(columns={98:'98'}) $ stocks = stocks.rename(columns={'Week 13':'Week 14','91':'98'}) $ week14 = pd.merge(stocks,week14,on=['98','Tickers']) $ week14.drop_duplicates(subset='Link',inplace=True)
pylab.figure() $ data.groupby(pandas.TimeGrouper(key="Visit Date", freq="1D") $             ).count()["Visit Type"].plot()
dumpfile='pandabooter.sql'
for tweet in results: $     print(tweet.text, '\n') $     print(tweet.created_at, '\n')
recSets = data.groupby('recommendation_id')
from xml.dom import minidom $ from lxml import etree as ET #Supports xpath syntax $ from collections import defaultdict $ from pprint import pprint
data = requests.get("https://listado.mercadolibre.com.pe/sonic#D[A:sonic]")
owns.to_pickle('data/pickled/new_subset_owns.pkl')
df_tweets = pd.read_pickle("../tweets_extended.pkl").set_index("tid")
import pydotplus $ from sklearn import tree $ dot_data = tree.export_graphviz(clf, out_file=None,feature_names= features,class_names = ["high","medium","low"]) $ graph = pydotplus.graph_from_dot_data(dot_data)   $
mb = pd.read_excel('../data/microbiome/MID2.xls', sheetname='Sheet 1', header=None) $ mb.head()
print(train_df.columns.values)
finals[finals.PLAYER_NAME.str.contains("Durant")]
suspects_file_path = '../../data/outbound/sample_219.txt' $ suspects_data = pd.read_csv(suspects_file_path, sep='|', header=None,names=header)
run txt2pdf.py -o"2018-06-19 2014 FLORIDA HOSPITAL Sorted by discharges.pdf"  "2018-06-19 2014 FLORIDA HOSPITAL Sorted by discharges.txt"
corpus_Tesla = [dictionary.doc2bow(text) for text in sws_removed_all_tweets]
df_h1b_nyc_ft.pw_1.hist(bins=20,figsize=(8,8))
fwd = df[['Store']+columns].sort_index(ascending=False $                                       ).groupby("Store").rolling(7, min_periods=1).sum()
wqYear = dfHawWQ.groupby('Year')['TotalN'].mean() $ dfAnnualN = pd.DataFrame(wqYear)
model = GradientBoostingRegressor(verbose=True) $ model.fit(X_train, y_train) $ model.score(X_test, y_test) # This is the R^2 value of the prediction
old_page_converted = np.random.choice(2, size = n_old, p=[0.8805, 0.1195]) $
output= "Update user SET following=50 where user_id='@Pratik'" $ cursor.execute(output) $
df2['intercept'] = 1 $ df2[['ab_page', 'ab_page_2']] = pd.get_dummies(df2['landing_page']) $ df2 = df2.drop(columns = ['ab_page_2'], axis = 1) $ df2.head() $
active_distinct_authors_latest_commit.show() $ print(active_distinct_authors_latest_commit.count()) $ active_distinct_authors_latest_commit.take(5)
sp_re_df.boxplot() $ plt.xlabel('Model') $ plt.ylabel('Score')
selected=features[features.importance>0.01] $ selected.sort_values(by="importance", ascending=False)
no_of_stations = engine.execute('SELECT count(*) FROM station').fetchall() $ print("No_of_stations", no_of_stations)
sorted_posts = gram_collection.find({"contains_tattoo": 1}).sort([("likes", pymongo.DESCENDING)])
noaa_data.loc["2018-06-10 22:00:00":"2018-06-10",'PRECIPITATION']
merged2.index.date
def read_file(file_path): $     raw_data = sc.textFile(file_path) $     raw_data_header = raw_data.take(1)[0]   $     return raw_data, raw_data_header
old_page_converted = np.random.binomial(1, cr_under_null, size=n_old) $ print(len(old_page_converted)) $ old_page_converted
with open('test_data//open_close_test.txt') as input_file: $     with open('test_data//write_test.txt', mode='w') as output_file: $         output_file.write(input_file.read()[0:25]) $ with open('test_data//write_test.txt') as output_file: $     print(output_file.read())
author_commits = git_log.groupby('author').count().sort_values(by='timestamp', ascending=False) $ top_10_authors = author_commits.head(10) $ top_10_authors
df_final = df_grouped.merge(df_schoo11, on='school_name', how='left') $ df_final.drop(['budget_x', 'size_x', 'School ID','size_y', 'budget_y'], axis = 1, inplace=True) $ df_final
df_titanic.head()
B2.allowed_indicator_settings_steps
doc_duration.shape, RN_PA_duration.shape, therapist_duration.shape
evaluator.get_metrics("macro_f1")
reg_mod_uk = sm.OLS(df_all['converted'], df_all[['intercept', 'UK']]) $ analysis_uk = reg_mod_uk.fit() $ analysis_uk.summary()
weather_mean.iloc[2, 6]
tweet_df = tweet_df.reset_index()[['created_at','id','favorite_count','favorited','retweet_count','retweeted','retweeted_status','text']]
len(train_data[train_data.fuelType == 'benzin'])
z_score, p_value = sm.stats.proportions_ztest([convert_old,convert_new], [n_old,n_new],value=None, alternative='smaller', prop_var=False) $ print(z_score,p_value) $
def localize_time(api_time, myTimeZone): $     api_time = datetime.datetime.strptime(api_time, "%a %d %b %Y %H:%M:%S %z") $     return api_time.astimezone(myTimeZone)
df_new[['UK','US']] = pd.get_dummies(df_new['country'])[['UK','US']] $ df_new.head()
df_.plot(kind='barh', title="Num of State-Level Media Outlets");
df2.drop('drop_me', axis = 1, inplace = True) $ df2.head()
print ("For the tweet = ", train_.tweetText.values[10] ) $ print (" ") $ print ("The following features has been created:") $ print (" ") $ print (v_train[0][0])
df_archive["name"].value_counts()
pkl_file = open('speeches_metadata_evidence.pkl', 'rb') $ speeches_metadata_evidence = pickle.load(pkl_file) $ speeches_all_bill_data = speeches_metadata_evidence.merge(bills_votes, left_on = ['bill_id', 'congress'], right_on = ['bill_slug', 'congress'], how = 'inner')
mandelaSpeechText0 = readPDF(infoExtractionBytes)[1187:6549] $ mandelaSpeechText1 = re.sub(r'\n', ' ', mandelaSpeechText0) $ mandelaSpeechText2 = re.sub(r'\s\s', ' ', mandelaSpeechText1) $ mandelaSpeechText = re.sub(r'\x0c', '', mandelaSpeechText2)  
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=42)
cutoff_times = generate_labels('5fPXqLcScoC93rH/gCPK+5Soj+XdNMXX9S3LhV5dJjM=', trans, $                                label_type = 'SMS', churn_period = 14) $ cutoff_times.head()
area = pd.Series({'Califonia':23454523, 'Texas': 23445, 'New York': 23890490, 'Florida': 23890450, 'Illinois': 2348035890}) $ pop = pd.Series({'Califonia':254523, 'Texas': 234453, 'New York': 238904, 'Florida': 290450, 'Illinois': 2348035}) $ data = pd.DataFrame({'area':area, 'pop': pop}) $ data
import quandl, math $ import numpy as np $ import pandas as pd $ from sklearn import preprocessing, cross_validation, svm $ from sklearn.linear_model import LinearRegression
learn.unfreeze()
X_train['Text_Tokenized'] = X_train.Text.apply(process) $ X_train.head()
with open('data/kochbar_12.json') as data_file:    $     kochbar12 = json.load(data_file) $ koch12df = preprocess(kochbar12) $ koch12df.info()
iplot(data.groupby('assignee.login').size().sort_values(ascending=False)[:20].iplot(asFigure=True, dimensions=(750, 500), kind='bar'))
(autos["ad_crawled"] $         .str[:10] $         .value_counts(normalize=True, dropna=False) $         .sort_index() $         )
data.columns
CSV_URL = ('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')
df.iloc[8]['Profile_Pic_URL']
trial_df = df.loc[(df.label==0)] $ trial_df['label'] = trial_df.label.apply(lambda a: int(a))
result.to_csv('/home/student/data_analytics/bus_lines/result_of_66-dir1.csv')
client.experiments.list_training_runs(experiment_run_uid)
store_items.fillna(method='backfill', axis=0)
epsg = {'code':'4269'} $ expected_geom_type = 'Point' $ source_uri = 'https://www.sciencebase.gov/catalog/item/5af6219be4b0da30c1b5faad' $ outfile_name = 'gnis/gnis_20180601'
dfa.head()
data = r.json()
df[df['SchoolHoliday'] == 0].head()
new_converted_simulation = np.random.binomial(treatment_num, treatment_cnv,  10000)/treatment_num $ old_converted_simulation = np.random.binomial(control_num, control_cnv,  10000)/control_num $ p_diffs2 = new_converted_simulation - old_converted_simulation
autos["odometer_km"].unique().shape[0]
filtered_df[numeric_cols].dtypes
def emb_init(x): $     x = x.weight.data $     sc = 2/(x.size(1)+1) $     x.uniform_(-sc,sc)
stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' '))) $ lbl = LabelEncoder() $ for i in range(10): $     stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else '')) $ stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])
logit_mod=sm.Logit(df3['converted'],df3[['intercept','ab_page']])
print(' '.join(TEXT.vocab.itos[int(w)] for w in batch[0][:,0]))
parse_dict['creator'].head(5)
p_new = .1196
org_id_table = active_org_id.join(inactive_org_id) $ org_id_table['active_proportion'] = org_id_table['active'] / (org_id_table['active']+org_id_table['inactive'])
all_accounts = proxy.GetPersons(auth) $ print(f"We have {len(all_accounts)} accounts")
%matplotlib inline $ import matplotlib $ import matplotlib.pyplot as plt
offseason07 = ALL[(ALL.index < '2007-09-06')] # This means every transaction before 9-6-07 belongs to the 2007 Offseason.
common_names = df_merge.name.value_counts().head(6) $ common_names
df_anomalies.loc[:, 'novel_tweets'] = df_anomalies.apply(lambda row: novel_tweets(row['tweets']), axis=1) $ display(df_anomalies['novel_tweets'].head())
df2_rows_removed = df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0] $ print('Now, the number of rows where the landing_page and group columns don\'t align is {}.'.format(df2_rows_removed))
net_loans_exclude_US_outstanding.loc[KR_select & (net_loans_exclude_US_outstanding.Ratenart=='J')]
df_new[['action_new_page','action_old_page']] = pd.get_dummies(df_new['landing_page']) $ df_new = df_new.drop('action_old_page', axis=1) $ df_new.head()
!wget https://github.com/ipeirotis/data/raw/master/origin-of-species.txt
daily_averages['Month'] = daily_averages.index.month
festivals.columns = festivals.iloc[0] $ list(festivals.columns.values)
dummy_cols.remove('idx_id') $ df_new.drop(dummy_cols, axis=1, inplace=True)
xnames = xd.columns $ xarr = np.array(xd)
list(data.items())
print('No missing values \nIsnull: {}'.format(df.isnull().values.any())) $
df_t_not_n = df[(df['group'] == 'treatment') & (df['landing_page'] == 'old_page')] $ df_not_t_n = df[(df['group'] == 'control') & (df['landing_page'] == 'new_page')] $ mismatch= len(df_t_not_n) + len(df_not_t_n) $ mismatch_df = pd.concat([df_t_not_n, df_not_t_n]) $ mismatch
df_pr['sentiment'] = df_pr['body'].apply(compute_sentiment)
df_=data[['QG_mult','QG_ptD','QG_axis2']].copy() $ df_['target']=data['isPhysG'] $ df_.head()
df1 = df1.rename(columns={'date': 'Tweet_Count'})
new['country'].unique()
a,b=validation,(validation+test) $ x,y=X[a:b],Y[a:b] $ scores_test=model.evaluate(x,y) $ print ("\n%s: %.2f%%" %(model.metrics_names[1],scores_test[1]*100))
jobs.loc[(jobs.GPU == 1) & (jobs.FAIRSHARE == 1)].groupby('ReqCPUS').JobID.count().sort_values(ascending= False)
df.drop("water_year2",axis='columns',inplace=True)
trt_total = df2.query('group == "treatment"').nunique() $ pct_conv_trt = df2.query('group == "treatment" and converted == 1').user_id.nunique() $ print(pct_conv_trt/trt_total['user_id'])
d = pd.datetime(2016, 11, 15, 13, 0)
from scipy import stats $ stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) $ logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ res = logit_mod.fit()
series2 = pd.Series(range(3), index=['b', 'e', 'f'])
df_daily = df_daily.groupby(['C/A', 'UNIT','STATION','DATE']).sum() $ df_daily.head()
adj_close.head()
X_train_matrix = tvec.fit_transform(X_train) $ X_test_matrix = tvec.transform(X_test) $ forest.fit(X_train_matrix, y_train) $ forest.score(X_test_matrix, y_test)
df2=df.copy()
prices.plot()
df2['intercept'] = 1 $ df2['ab_page'] = np.where(df2['group'] =='control',0,1)
new_cols_cust.to_csv('tags_cust_proc') $ new_cols_prosp.to_csv('tags_prosp_proc')
le.classes_
any(stanford_word_list.Word=='aa')
X_clust = matutils.corpus2dense(lsi_corpus, num_terms=topics).transpose() $ X_clust = normalize(X_clust)
converted = df2[df2.converted==1] $ prob_conveted = converted.shape[0]/df2.shape[0] $ prob_conveted
seq2seq_model.load_weights('kdd_seq2seq_weights.h5')
infinity.head(10)
trump_fb = trump.source == "Facebook"
idx1 = (df['group'] == 'treatment') & (df['landing_page'] == 'new_page') $ idx2 = (df['group'] == 'control') & (df['landing_page'] == 'old_page') $ df2 = df[idx1 | idx2] $ print('df2 rows', len(df2)) $ print('{} rows removed'.format(df.shape[0] - df2.shape[0]))
for (name, sex) in c_list: $     get_all_tweets(name, sex) $     df = df.append(pd.read_csv('%s_tweets.csv' % name))
model.add(Dropout(0.25))
print 'mean: {} std: +/- {}'.format(np.mean(score), np.std(score))
p_new = df2["converted"].mean() $ print("Convert rate for P-new is: {}".format(p_new))
y_train.value_counts()
df1['forcast'].head() #allthe data will be nan $
df_group_by = pd.merge(df_group_by,df_members, on = 'msno', how = 'left')
myTimeZone = pytz.timezone('US/Eastern') $ itemTable["Date"] = itemTable["Date"].apply(localize_time, args=(myTimeZone,))
df.sample(5)
0.1196
cityID = 'e444ecd51bd16ff3' $ for tweet in tweepy.Cursor(api.search, q=q, contained_within=cityID, wait_on_rate_limit=True, lang="en").items(500):    $         Cincinnati.append(tweet) 
df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].shape[0]
from pyspark.sql.functions import concat, col, lit $ file4 = file4.withColumn('date', concat(col('trans_start_year'), $                                         col('trans_start_month'), $                                        col('trans_start_day'))) $ file4.show(3)
image_copy['p1']=image_copy['p1'].str.title() $ image_copy['p2']=image_copy['p2'].str.title() $ image_copy['p3']=image_copy['p3'].str.title()
pgh_311_data.info()
bc.head()
print(festivals.dtypes) $ print(festivals.info()) $
tm_2040 = pd.read_csv('input/data/trans_2040_m.csv', encoding='utf8', index_col=0)
from sklearn.grid_search import GridSearchCV $ param_test1 = {'n_estimators':range(20,81,10)} $ gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8),\ $                         param_grid = param_test1, scoring='log_loss',n_jobs=4,iid=False, cv=5) $ gsearch1.fit(drace_df[feats_used],y)
sample_text = "Hey there! This is a sample review, which happens to contain punctuations." $ print(text_process(sample_text))
val_prediction = model.predict(validation_data)
hm_clean.to_sql("measurements", conn, index=False, if_exists='replace')
comments = pd.read_csv('data/comments.csv', index_col = 0, parse_dates = [2]) $ comments.columns
log_mod_countries = sm.Logit(df_new['converted'],df_new[['intercept','US','UK']]) $ results_countries = log_mod_countries.fit() $ results_countries.summary()
from fastai.fastai.rnn_reg import * $ from fastai.fastai.rnn_train import * $ from fastai.fastai.nlp import * $ from fastai.fastai.lm_rnn import * $ import dill
autos['last_seen'].str[:10].head()
print(df_B.columns) $ print(df_B)
model.evaluate_word_pairs(test_data_dir + 'wordsim353.tsv')
pivoted_table = data.pivot_table('Total', index = data.index.time, columns = data.index.date) $ pivoted_table.head()
n_old = (df2['landing_page'] == 'old_page').sum(); $ n_old
df = data[(data['Shop_Existence_Days']>=30) & (data['Shop_Status']=='disabled') | (data['Merchant_Plan_Offer']=='Start')] $ trial, start = df['Merchant_Plan_Offer'].value_counts() $ print('Interest merchant plan offer distribution: ') $ print('Trial: '+str(trial)) $ print('Start: '+str(start))
import html $ D1 = [html.unescape(t) for t in D1]
churned_bool = pd.Series([USER_PLANS_df.loc[uid]['status'][np.argmax(USER_PLANS_df.loc[uid]['scns_created'])] =='canceled' for uid in USER_PLANS_df.index],index=USER_PLANS_df.index)
data_l2_end = tmpdf.index[tmpdf[tmpdf.isin(DATA_SUM1_KEYS)].notnull().any(axis=1)].tolist() $ data_l2_end
data2 = data.set_index('Date')
s = pd.Series([95, 61, 17, np.nan, 87, 13]) $ s
parsed_email_data['institution'].value_counts()
def getJson(name): $     response = urlopen("http://d.yimg.com/autoc.finance.yahoo.com/autoc?query="+'%20'.join(name.split(' '))+"&region=1&lang=en").read().decode('utf-8') $     responseJson = json.loads(response) $     return responseJson $
df_clean.head(1)
data = pd.read_sql("SELECT * FROM session_privs",xedb) $ print(data) $
print(type(blurb_SVD)) $ print(np.sum(tSVD.explained_variance_ratio_))
df2 = df.drop(not_lined_up_df.index)
import numpy as np $ incr = pd.DataFrame(np.arange(10), columns=['IncreasingSequence']) $ incr['RunningSum'] = incr['IncreasingSequence'].expanding().sum() $ incr
d_salary=detroit_census2.drop(detroit_census2.index[:46]) $ d_salary=d_salary.drop(d_salary.index[50:]) $ d_salary.head(4)
autos = autos[autos["price_euro"].between(0,351000)] $ autos["price_euro"].describe()
conn.upload('/u/username/data/iris.csv', casout=dict(name='iris2', caslib='casuser'))
train_embedding=pd.DataFrame(train_embedding)
calls_df.columns
conn.fetch(table=dict(name='myclass', caslib='casuser'), to=5)
for line in autoData.collect(): $     print (len(line))
p_new_0 = df_treatment.query('converted == 1').user_id.nunique() / df_treatment.user_id.nunique() $ p_old_0 = df_control.query('converted == 1').user_id.nunique() / df_control.user_id.nunique() $ obs_diff_0 = p_new_0 - p_old_0 $ np.mean(p_diffs > obs_diff_0)
second_comb.head(5)
json_data = req.json()
df.index[-5:]
tickers = companies['tweet_ticker'].tolist() $
import matplotlib.pyplot as plt $ import pylab as plt $ import seaborn as sb $ from pylab import rcParams
hsi = pd.read_csv("HSI.csv") $ hsi.head()
recsys_annoyobj.save('recsys_annoyobj.pkl')
df32.count()
obj.rank(method='first')
stock['model_predict'] = best_dt.predict(stock.drop(['target', 'true_grow', 'predict_grow'], 1))
cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']] $ cdf.head(9)
access_logs_df.
reddit.info()
df_uro_no_cat.tail()
old_page_converted = np.random.choice([0,1], size=145274, p=[1-0.1196, 0.1196]); old_page_converted
dictionary = corpora.Dictionary(texts) $ dictionary
from sklearn.model_selection import train_test_split
cpq_CO = cpq_status.loc[cpq_status['State'] == 'CO'] $ cpq_TX = cpq_status.loc[cpq_status['State'] == 'TX'] $ cpq_GA = cpq_status.loc[cpq_status['State'] == 'GA']
df.info()
from statsmodels.tsa.stattools import adfuller as ADF $ print() $ print(ADF(resid_6203.values)[0:4]) $ print() $ print(ADF(resid_6203.values)[4:7])
df_final.to_csv('twitter_archive_master.csv')
DOT_df = df[df['Agency'] == 'DOT'] $ DOT_df['Complaint Type'].value_counts().head() $
df['range'] = (df.max(axis='columns') - df.min(axis='columns'))
temp = train.groupby("DOW")["any_spot"].mean().sort_values(ascending=True) $ DOW_rank = pd.DataFrame({'DOW': temp.reset_index(drop=False).DOW.values, 'DOW_ranking':range(7)}).set_index('DOW') $ train = train.join(DOW_rank, on="DOW", rsuffix='_ranking') $ val = val.join(DOW_rank, on="DOW", rsuffix='_ranking')
users_num = Users[['Reg_date', 'id_partner', 'name']].assign(num_users=lambda x: 1) $ users_num = users_num.groupby(['id_partner', 'name', 'Reg_date'], as_index=False).sum() $ users_cost = pd.merge(Users, users_num, how = 'left', on=['Reg_date', 'id_partner', 'name']) $ del users_num
wash_park_matrix.todense()
pickle.dump(comments, open('data/comments_with_post_ids.dat', 'wb'))
ecemaml = pb.User(commons_site, 'Ecemaml') $ ecemaml.registration()
S_distributedTopmodel.decision_obj.thCondSoil.options, S_distributedTopmodel.decision_obj.thCondSoil.value
submission_full = pd.concat([submission2, submission1], axis=0) $ submission_full.to_csv('lgbm.csv', index=False)
new_page_converted.mean()-old_page_converted.mean() $
sb.pairplot(df_new[['converted','group']])
exog.info()
df.head()
prob1_ZeroFill = pd.Series(x[1] for x in prob_ZeroFill) $ prob1_ZeroFill.head()
out = conn.addtable(table='iris_sql', caslib='casuser', $                     **sqldmh.args.addtable) $ out
p_diff_simulated = new_converted.mean() - old_converted.mean() $ print(p_diff_simulated)
tbl_detail = conn.get_table_details("nyctaxi") $ pd.DataFrame(tbl_detail)
n_old=df2.query('landing_page == "old_page"').user_id.nunique() $ n_old
dummy = pd.get_dummies(df3['country'], prefix='country_code') $ dummy.head()
pax_raw = pax_raw[pax_raw.paxcal==1].copy() # Keep only users with a calibrated device $ pax_raw.drop(columns='paxcal', inplace=True) # Drop the calibration column, we no longer need it
tweet_table.head()
nypd_count_df[['create_date', 'sidewalk_parking_count']].plot()
b = splinter.Browser('chrome')
acc.find(amount=1500)
df_users.last_session_creation_time = df_users.last_session_creation_time.astype(int)
precip_12mo_df=pd.DataFrame(precip_12mo) $ precip_12mo_df.head() $ mod1precip_12mo_df=precip_12mo_df.set_index('date') $ mod1precip_12mo_df.head()
fig, ax = plt.subplots(figsize=(6, 5), dpi = 75) $ f1 = runs['Rusher'].value_counts().plot(kind='barh') $ f1.set(title = "Players with the most Yards from Rushing", xlabel='Count', ylabel='Player Name') $ plt.show()
df_predictions_clean['tweet_id'] = df_predictions_clean.tweet_id.astype(str)
temp = temp.reset_index()
df.sort_index(axis=1, ascending=False)  # Sort by column
!python ~/neuroscience/utils/create_dataset.py ~/neuroscience/data/processed/dataset.csv
transit_df['DELEXITS']= transit_df['EXITS'].diff() $
model_df['is_top_user'] = False $ model_df.is_top_user[model_df.author.isin(top_25_users)] = True $ model_df = model_df.drop(['author'], axis=1)
print('The total number of tweets is: ', all_tweets.shape[0], 'tweets')
data_new = pd.read_csv('../data/no_label.csv') $ data_new.head(10)
df3 = df2.join(countries_df.set_index('user_id'), on='user_id') $ df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country']) $ df3.drop('CA', axis =1, inplace=True); $ df3.head()
df = pd.read_csv('CPSC-LOA-DATA.csv') $ df.head()
df_concat_2.head()
r = requests.get('https://www.quandl.com/api/v3/datasets/FSE/EON_X.json?api_key=&start_date=2017-01-01&end_date=2017-12-31') $ print(r.json())
doctors.columns
p_diffs = [] $ for _ in range(10000): $     new_page = np.random.choice([0,1], size = n_new, p = [1-p_new, p_new]) $     old_page = np.random.choice([0,1], size = n_old, p = [1-p_old, p_old]) $     p_diffs.append(new_page.mean() - old_page.mean())
start_val=1000000 $ pos_vals=alloced*start_val $ pos_vals.head()
print train.shape
coefs.T.sort_values(0).tail().T
SANDAG_age_df[SANDAG_age_df['SEX'] == 'Male'].loc[(2012, '1')]
giss_temp["Jan"].astype("float32")
df2.query("group=='control'").count()
label_and_pred = dtModel.transform(testData).select('label', 'prediction') $ label_and_pred.rdd.zipWithIndex().countByKey() $
tweets['retweeted'].value_counts(dropna=False) $ tweets['favorited'].value_counts(dropna=False)
S_data = session.query(Station).first() $ S_data.__dict__
import statsmodels.api as sm $ log_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']]) $ results = log_mod.fit() $
merged2[["article_publisher_id", "PID"]] \ $     [(merged2["article_publisher_id"] != "") & $     (merged2["article_publisher_id"] != merged2["PID"])] \ $     .drop_duplicates()
app_pivot = app_counts.pivot(columns="is_application",index="ab_test_group",values="last_name").reset_index()
sns.barplot(x="alone", y="survived", hue="sex", data=titanic)
plt.rcParams['figure.figsize'] = [16,4] $ plt.plot(pd.to_datetime(mydf2.datetime),mydf2.fuelVoltage, 'g.'); $ plt.xlim(datetime.datetime(2018,2,8),datetime.datetime(2018,3,25))
str(int(internet.iloc[0][0]))+"-12-31"+"T00:00:00Z"
data.sort_values(by='Likes',ascending=False,inplace=True) $ data.head()
cutoff_times = pd.read_csv('s3://customer-churn-spark/p1/MS-30_labels.csv') $ cutoff_times.head() $ cutoff_times.tail()
traindata = pd.read_csv('data/new_subset_data/final_train_data.csv') $ testdata = pd.read_csv('data/new_subset_data/final_test_data.csv')
new_df =  pd.value_counts(IKEA_data.user_location).reset_index() $ new_df.columns = ['user_location', 'frequency'] $ top50_city = new_df.head(50) $ new_df.head(10)
s.loc[['b','c']]
weather_mean.loc['CHARLOTTETOWN', 'Wind Spd (km/h)']
df_2018 = df[df.year==2018] $ predicted_talk_indexes = predicted_talks_vector.nonzero()[0] + len(df[df.year==2017]) $ df_2018_talks = df_2018.loc[predicted_talk_indexes] $
df = df.groupby(["C/A", "UNIT", "SCP", "STATION", "DATE"]).ENTRIES.first().reset_index() $ df.head() $
model.fit(X_tr, y_tr) $ preds = model.predict(X_val) $ np.sqrt(metrics.mean_squared_error(y_val, preds)) $
device = hp.find_device('FL03001552') $ device
addingprediction=model.predict_classes(Xforadding) $ addingprediction_v=model.predict_classes(Xforadding_v) $ Events.insert(loc=1, column='prediction', value=addingprediction) $ Valid_events.insert(loc=1, column='prediction', value=addingprediction_v)
df_all_loans = pd.concat(dfs_loan, keys=selected_reporting_dates)
data = pd.read_csv('data.csv', $                     parse_dates={'Timestamp': ['Date', 'Time']}, $                     index_col='Timestamp') $ data
 date_retweets = tweets_df[(tweets_df.created_at < "2018-04-23") & (~tweets_df.retweet_count.isnull())]
normal_forecast_exp = np.exp(normal_forecast[['yhat','yhat_lower','yhat_upper']]) $ normal_forecast_exp.index = normal_forecast['ds'] $ normal_model_error = normal_forecast_exp['yhat'] - df_orig['y'] $ MAPE_for_normal_model = (normal_model_error/df_orig['y']).abs().sum()/n *100 $ print ("Normal model error: ",round(MAPE_for_normal_model,2))
c = pd.concat(days_since_the_last_transactions, ignore_index = True) $ c.info()
target_pf.shape
import matplotlib.pyplot as plt $ plt.imshow(input_image) $ plt.show()
dfUsers = pd.DataFrame() $ dfUsers['userFromName'] =[] $ dfUsers['userFromId'] =[] $ dfUsers['userToId'] = [] $ count = 0 
pickup_demand = pickup_orders.groupby(['ceil_10min','pickup_cluster'], as_index=False).apply(my_agg).reset_index() $ dropoff_demand = dropoff_orders.groupby(['ceil_10min','dropoff_cluster'], as_index=False).apply(my_agg).reset_index() $ ride_demand = ride_orders.groupby(['ceil_10min','ride_cluster'], as_index=False).apply(my_agg).reset_index()
pd.Series(5, index=[100, 200, 300])
clean_appt_df['DaysAhead'] = (clean_appt_df['AppointmentDay'] - clean_appt_df['ScheduledDay'])\ $     .dt.days $ clean_appt_df[['DaysAhead','No-show']].boxplot(by='No-show')
data.head(10)
df2 = pd.DataFrame(q2_results,columns=['station_name','observ_ct']).sort_values(by = 'observ_ct') $ df2
instance.example() $ print('\nContextid: {}'.format(contextid))
guineaDeaths1 = guineaFullDf.loc['New deaths registered'] $ guineaDeaths2 = guineaFullDf.loc['New deaths registered today'] $ guineaDeaths = pd.concat([guineaDeaths1, guineaDeaths2]) $ guineaDeaths.head()
pd.to_datetime(food["created_date"], infer_datetime_format=True)
test_ind["Pred_state_RF"] = trained_model_RF.predict(test_ind[features]) $ train_ind["Pred_state_RF"] = trained_model_RF.predict(train_ind[features]) $ kick_projects_ip["Pred_state_RF"] = trained_model_RF.predict(kick_projects_ip_scaled_ftrs)
df=df.drop('SEC filings',axis=1)
log_reg_under.score(X_train, y_train_under)
frames=[NameEvents,ValidNameEvents] $ TotalNameEvents = pd.concat(frames)
n_cols = predictors.shape[1]
sales_pivot = df.pivot_table(index=['Company','Product'], $                              values=['Quantity','Price'], $                              aggfunc=[np.sum,np.mean], $                              fill_value=0) $ sales_pivot.head()
X_train, X_test, y_train, y_test = train_test_split(X.titles, y, test_size=0.33, random_state=100)
weather_mean = weather_all.groupby('Station Name').mean() $ weather_mean.head()
df_concat.head()
dta = sentiments.head(100) # testdata $ dta['item'] = dta.index $ dta.head()
dfss.stamp[dfss.price == dfss.price.max()]
df_new['US'] = pd.get_dummies(df_new['country'])['US']
max(close, key=close.get)
result = pd.merge(result2,result3, how='inner', on=['id_ndaj1']) $ result.shape
plt.hist(p_diffs); $ plt.axvline(obs_diff, c='red')
learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn) $ learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1) $ learn.clip=25. $ learn.metrics = [accuracy]
knn = KNeighborsClassifier(n_neighbors=5) $ knn.fit(X_train_total, y_train) $ knn.score(X_test_total_checked, y_test)
X_train, X_test, y_train, y_test = train_test_split(stock.drop(['target', 'close', 'open'], 1), stock['target'], test_size=0.3, random_state=42)
z_score, p_value = sm.stats.proportions_ztest([old_page_converted, new_page_converted], [n_old, n_new], alternative='smaller') $ z_score, p_value
cX_test = X_test.copy()
print(result.summary())
data_issues_transitions=pd.read_csv('/Users/JoaoGomes/Dropbox/Xcelerated/assessment/data/avro-transitions.csv')
data_numeric = auto_new.select_dtypes(exclude="object")
writtens = df5['HT'].where(df5['new_id'] == 'written').dropna() $ plt.hist(writtens, bins=50)
house_data = house_data.set_index('listing_id')
results = [] $ for tweet in tweepy.Cursor(api.search, q='%23Trump').items(2000): $     results.append(tweet.text) $ print len(results)
aux.loc[(aux.cuteness_y == 'nan'),:].sample(5)
pre_strategy_google = pre_strategy_users[pre_strategy_users['channel'] == "Google"].sort_values(by='date')
race_vars = pd.get_dummies(df['race_desc']) $ gender_vars = pd.get_dummies(df['gender_desc'])
pprint.pprint(posts.find_one({"reinsurer": "AIG"}))
org_id_table.sort_values('active_proportion', ascending = False)
baseball[['r','h','hr']].rank(ascending=False).head()
df_main.p1.head(3)
readme = processing_test.README() $ readme = open(readme, "r") $ print(readme.read())
for param in dir(tweet): $     if not param.startswith("_"): $         print("%s: %s\n" % (param, eval('tweet.'+param)))
titanic[titanic.survived & (titanic.sex == 'female') & (titanic.age > 50)].head()
%%time $ X_df = df.drop(['vegFriendly', 'Vegan', 'Vegetarian'], axis=1) $ y_df = df['vegFriendly']
kf = cross_validation.KFold(len(data), n_folds=5, shuffle=True)
ad_created_count_norm.describe()
autos = pd.read_csv('autos.csv', encoding='Latin-1')
tweetsIn22Mar = tweets22Mar[tweets22Mar.lang == 'in'] $ tweetsIn1Apr = tweets1Apr[tweets1Apr.lang == 'in'] $ tweetsIn2Apr = tweets2Apr[tweets2Apr.lang == 'in']
austin[['distance_travelled', 'miles', 'driver_rating', 'rider_rating', 'total_fare', 'base_fare', 'tip']].describe()
df_ser_dict.pop("Four") $ df_ser_dict
df_twitter_copy['tweet_id'] = df_twitter_copy['tweet_id'].astype('str')
df = pd.read_csv("AAPL.csv",na_values = ["null"]) $ df.dtypes
df['intercept'] = 1 $ df[['control', 'treatment']] = pd.get_dummies(df['group'])
predictions.info()
preg.columns
ab_file.to_csv('ab_edited.csv', index=False)
validation.analysis(observation_data, Jarvis_resistance_simulation_0_5)
df.expanding(min_periods=2).sum()
